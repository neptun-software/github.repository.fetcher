{
  "metadata": {
    "timestamp": 1736565523617,
    "page": 396,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "onnx/onnx-tensorrt",
      "stars": 2992,
      "defaultBranch": "10.7-GA",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.18359375,
          "content": "onnx2trt\n\n# Compiled files\n*.so\n*.o*\n*.lo\n*.la\n*.a\n.deps/\n\n# Backup files\n*~\n*.bak*\n\n# Log files\n*.log\n*.prof\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n\n# Build\nbuild"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.111328125,
          "content": "[submodule \"third_party/onnx\"]\n\tpath = third_party/onnx\n\turl = https://github.com/onnx/onnx.git\n\tbranch = v1.17.0\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 6.013671875,
          "content": "# SPDX-License-Identifier: Apache-2.0\n\ncmake_minimum_required(VERSION 3.13)\nproject(onnx2trt LANGUAGES CXX C)\n\nset(ONNX2TRT_ROOT ${PROJECT_SOURCE_DIR})\n# Set C++17 as standard for the whole project, as required by ONNX 1.16\nset(CMAKE_CXX_STANDARD 17)\n\n# Enable compiler warnings\nif (CMAKE_COMPILER_IS_GNUCC)\n    set(CMAKE_CXX_FLAGS  \"${CMAKE_CXX_FLAGS} -Wall -Wno-deprecated-declarations -Wno-unused-function\")\nendif()\nif (MSVC)\n    set(CMAKE_CXX_FLAGS  \"${CMAKE_CXX_FLAGS} /W4\")\nendif()\n\n# Build the libraries with -fPIC\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\n\nset(PARSER_LINKER_SCRIPT  ${ONNX2TRT_ROOT}/libnvonnxparser.version)\n\n# Find length of source directory used to pad filename in Status.hpp\nstring(LENGTH \"${CMAKE_SOURCE_DIR}/\" SOURCE_LENGTH)\nadd_definitions(\"-DSOURCE_LENGTH=${SOURCE_LENGTH}\")\n\n#--------------------------------------------------\n# Version information\n#--------------------------------------------------\nset(ONNX2TRT_MAJOR 10)\nset(ONNX2TRT_MINOR 7)\nset(ONNX2TRT_PATCH 0)\nset(ONNX2TRT_VERSION \"${ONNX2TRT_MAJOR}.${ONNX2TRT_MINOR}.${ONNX2TRT_PATCH}\" CACHE STRING \"ONNX2TRT version\")\n\n#--------------------------------------------------\n# Build configurations, global to all projects\n#--------------------------------------------------\n\nset(IMPORTER_SOURCES\n  NvOnnxParser.cpp\n  ModelImporter.cpp\n  ModelRefitter.cpp\n  onnxOpImporters.cpp\n  ImporterContext.cpp\n  importerUtils.cpp\n  ShapedWeights.cpp\n  ShapeTensor.cpp\n  LoopHelpers.cpp\n  RNNHelpers.cpp\n  OnnxAttrs.cpp\n  onnxErrorRecorder.cpp\n  ConditionalHelpers.cpp\n  bfloat16.cpp\n  onnxOpCheckers.cpp\n  onnxProtoUtils.cpp\n  weightUtils.cpp\n  WeightsContext.cpp\n  TensorOrWeights.cpp\n  errorHelpers.cpp\n)\n\nif (BUILD_ONNXIFI)\n  set(ONNXIFI_SOURCES onnx_trt_backend.cpp)\nendif()\n\nset(API_TESTS_SOURCES\n  getSupportedAPITest.cpp\n  ModelImporter.cpp\n)\n\n# Find protobuf if it's not a target.\nif (NOT TARGET protobuf::libprotobuf)\n  FIND_PACKAGE(Protobuf REQUIRED)\nendif()\n\n# Set protobuf libraries between full / lite.\nif (ONNX_USE_LITE_PROTO)\n  add_definitions(\"-DUSE_LITE_PROTOBUF=1\")\n  set(PROTOBUF_LIBRARY \"protobuf::libprotobuf-lite\")\nelse()\n  set(PROTOBUF_LIBRARY \"protobuf::libprotobuf\")\nendif()\n\nif(NOT TARGET onnx_proto)\n  # Note: This avoids libprotobuf.so complaining about name collisions at runtime\n  if(NOT ONNX_NAMESPACE)\n    set(ONNX_NAMESPACE \"onnx2trt_onnx\")\n  endif()\n  add_definitions(\"-DONNX_NAMESPACE=${ONNX_NAMESPACE}\")\n  add_subdirectory(third_party/onnx EXCLUDE_FROM_ALL)\nendif()\n\n# CUDA\nif (NOT CUDA_TOOLKIT_ROOT_DIR)\n  set(CUDA_TOOLKIT_ROOT_DIR /usr/local/cuda)\nendif()\nfind_path(CUDA_INCLUDE_DIR cuda_runtime.h\n  HINTS ${CUDA_TOOLKIT_ROOT_DIR}\n  PATH_SUFFIXES include\n)\nMESSAGE(STATUS \"Found CUDA headers at ${CUDA_INCLUDE_DIR}\")\n\n# TensorRT\nfind_path(TENSORRT_INCLUDE_DIR NvInfer.h\n  HINTS ${TENSORRT_ROOT} ${CUDA_TOOLKIT_ROOT_DIR}\n  PATH_SUFFIXES include)\nMESSAGE(STATUS \"Found TensorRT headers at ${TENSORRT_INCLUDE_DIR}\")\n\n# TensorRT Python Headers\nfind_path(TENSORRT_PYTHON_INCLUDE_DIR plugin.h\n  HINTS ${TENSORRT_ROOT}\n  PATH_SUFFIXES python/include/impl)\nmessage(NOTICE \"Found TensorRT Python headers at ${TENSORRT_PYTHON_INCLUDE_DIR}\")\n\n# Output dynamic library names depends on platform:\nif (MSVC)\n    set(nvonnxparser_lib_name \"nvonnxparser_${ONNX2TRT_MAJOR}\")\nelse()\n    set(nvonnxparser_lib_name \"nvonnxparser\")\nendif()\n# Output static library name is the same cross-platform.\nset(nvonnxparser_lib_name_static \"nvonnxparser_static\")\n\n# --------------------------------\n# Importer library\n# --------------------------------\nadd_library(${nvonnxparser_lib_name} SHARED ${IMPORTER_SOURCES})\ntarget_include_directories(${nvonnxparser_lib_name} PUBLIC ${ONNX_INCLUDE_DIRS} ${TENSORRT_INCLUDE_DIR} ${TENSORRT_PYTHON_INCLUDE_DIR} ${CUDA_INCLUDE_DIR})\ntarget_link_libraries(${nvonnxparser_lib_name} PUBLIC onnx_proto ${PROTOBUF_LIBRARY})\nset_target_properties(${nvonnxparser_lib_name} PROPERTIES\n  VERSION   ${ONNX2TRT_VERSION}\n  SOVERSION ${ONNX2TRT_MAJOR}\n  LINK_DEPENDS ${PARSER_LINKER_SCRIPT}\n  LINK_FLAGS \"-Wl,--version-script=${PARSER_LINKER_SCRIPT}\"\n  ARCHIVE_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\"\n  LIBRARY_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\"\n  RUNTIME_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\"\n)\nadd_library(${nvonnxparser_lib_name_static} STATIC ${IMPORTER_SOURCES})\ntarget_include_directories(${nvonnxparser_lib_name_static} PUBLIC ${ONNX_INCLUDE_DIRS} ${TENSORRT_INCLUDE_DIR} ${TENSORRT_PYTHON_INCLUDE_DIR} ${CUDA_INCLUDE_DIR})\ntarget_link_libraries(${nvonnxparser_lib_name_static} PUBLIC onnx_proto ${PROTOBUF_LIBRARY})\nset_target_properties(${nvonnxparser_lib_name_static} PROPERTIES\n  ARCHIVE_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\"\n  LIBRARY_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\"\n  RUNTIME_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\"\n)\n# --------------------------------\n# Onnxifi library\n# --------------------------------\nif(BUILD_ONNXIFI)\n  add_library(trt_onnxify SHARED ${ONNXIFI_SOURCES})\n  target_include_directories(trt_onnxify PUBLIC ${CUDA_INCLUDE_DIR} ${ONNX_INCLUDE_DIRS} ${TENSORRT_INCLUDE_DIR} ${TENSORRT_PYTHON_INCLUDE_DIR})\n  target_link_libraries(trt_onnxify PUBLIC ${nvonnxparser_lib_name_static} ${CMAKE_THREAD_LIBS_INIT} ${CMAKE_DL_LIBS})\nendif()\n\n# --------------------------------\n# API Tests\n# --------------------------------\nif (BUILD_API_TEST)\n  add_executable(getSupportedAPITest ${API_TESTS_SOURCES})\n  target_include_directories(getSupportedAPITest PUBLIC ${ONNX_INCLUDE_DIRS} ${CUDNN_INCLUDE_DIR})\n  target_link_libraries(getSupportedAPITest PUBLIC ${PROTOBUF_LIB} ${nvonnxparser_lib_name_static} ${CMAKE_THREAD_LIBS_INIT} ${CMAKE_DL_LIBS})\nendif()\n\n# --------------------------------\n# Installation\n# --------------------------------\ninstall(TARGETS\n                ${nvonnxparser_lib_name}\n                ${nvonnxparser_lib_name_static}\n        LIBRARY DESTINATION lib\n        ARCHIVE DESTINATION lib\n)\n\ninstall(FILES ${HEADERS}\n  DESTINATION include\n)\n\nSET(CPACK_GENERATOR \"DEB\")\nSET(CPACK_DEBIAN_PACKAGE_MAINTAINER \"NVIDIA\") #required\nSET(CPACK_PACKAGE_NAME \"onnx-trt-dev\")\nSET(CPACK_PACKAGE_VERSION \"0.5.9\")\nSET(CPACK_PACKAGE_VERSION_MAJOR \"0\")\nSET(CPACK_PACKAGE_VERSION_MINOR \"5\")\nSET(CPACK_PACKAGE_VERSION_PATCH \"9\")\n\nINCLUDE(CPack)\n"
        },
        {
          "name": "ConditionalHelpers.cpp",
          "type": "blob",
          "size": 6.7119140625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"ConditionalHelpers.hpp\"\n#include \"ModelImporter.hpp\"\n#include \"importerUtils.hpp\"\n#include \"toposort.hpp\"\n\nnamespace onnx2trt\n{\n\n// Search for a network Layer name in a SubgraphPortsMap.\nSubgraphPortsMap::const_iterator findLayer(const SubgraphPortsMap& inputs, const std::string layerName)\n{\n    return std::find_if(\n        inputs.begin(), inputs.end(), [&](const auto& item) { return layerName == item.first->getName(); });\n}\n\n// Add an ConditionalInputLayer between `layer` and its inputs.\n// I.e. input[inIdx] -> layer ==> input[inIdx] -> ConditionalInputLayer -> layer.\nvoid addConditionalInputLayer(ImporterContext* ctx, nvinfer1::IIfConditional* conditional, InputsMap& inputsMap,\n    nvinfer1::ILayer& layer, int32_t inIdx, ::ONNX_NAMESPACE::NodeProto const* node)\n{\n    auto input = layer.getInput(inIdx);\n    if (input == nullptr)\n    {\n        // Phantom input (an input that is really constant weights).\n        return;\n    }\n\n    if (layer.getType() == nvinfer1::LayerType::kCONDITIONAL_OUTPUT)\n    {\n        return;\n    }\n\n    auto const name = input->getName();\n    auto it = inputsMap.find(name);\n    nvinfer1::IIfConditionalInputLayer* inputLayer = nullptr;\n    if (it == inputsMap.end())\n    {\n        inputLayer = N_CHECK(conditional->addInput(*input));\n        inputsMap[name] = inputLayer;\n        const std::string inputLayerName(name);\n        ctx->registerLayer(inputLayer, inputLayerName + \"_InputLayer\", node);\n        // Note: Since multiple conditionals may use the same external tensor, check unique names for output tensors of\n        // IfConditionalInputLayers to avoid tensor name duplication.\n        ctx->registerTensor(\n            TensorOrWeights{N_CHECK(inputLayer->getOutput(0))}, inputLayerName + \"_InputLayer_output\", /*checkUniqueName*/ true);\n    }\n    else\n    {\n        // An InputLayer may in the inputsMap if it has several consumers.\n        inputLayer = it->second;\n    }\n    auto ifOutput = N_CHECK(inputLayer->getOutput(0));\n    layer.setInput(inIdx, *ifOutput);\n};\n\n// Take a snapshot of the network before and after parsing the subgraph and return a list\n// of newly added network layers.\nvoid importSubgraph(ImporterContext* ctx, ::ONNX_NAMESPACE::GraphProto const& subgraph,\n    std::vector<nvinfer1::ILayer*>& newLayers, std::vector<TensorOrWeights>& subgraphTensors)\n{\n    auto net = ctx->network();\n    int32_t beforeSubgraph = net->getNbLayers();\n\n    // Establish scope for names local to the subgraph.\n    NameScope nameScope(*ctx);\n\n    std::vector<Status> errors{};\n    onnx2trt::parseGraph(ctx, subgraph, errors);\n\n    for (int32_t i = 0; i < subgraph.output_size(); ++i)\n    {\n        std::string name = subgraph.output(i).name();\n        subgraphTensors.push_back(ctx->tensors().at(name));\n    }\n\n    for (int32_t i = beforeSubgraph; i < net->getNbLayers(); i++)\n    {\n        newLayers.push_back(net->getLayer(i));\n    }\n}\n\n// Add an IConditionalInputLayer to `layer`'s inputs, if they don't already exist.\nvoid addConditionalInputIfNeeded(ImporterContext* ctx, nvinfer1::IIfConditional* conditional, InputsMap& inputsMap,\n    nvinfer1::ILayer& layer, SubgraphPortsMap subgraphInputsMap, ::ONNX_NAMESPACE::NodeProto const* node)\n{\n    // Return all of the layer's inputs that are external to the subgraph that\n    // that the layer belongs to.\n    auto getLayerExternalInputs = [&](std::string const& layerName) {\n        std::set<int32_t> inIndices;\n        auto iter = findLayer(subgraphInputsMap, layerName);\n        if (iter != subgraphInputsMap.end())\n        {\n            const auto& indicesSet = iter->second;\n            inIndices.insert(indicesSet.begin(), indicesSet.end());\n        }\n\n        return inIndices;\n    };\n\n    const auto inIndices = getLayerExternalInputs(layer.getName());\n    for (auto inIdx : inIndices)\n    {\n        LOG_VERBOSE(\"Adding Input layer for \" << layer.getName());\n        addConditionalInputLayer(ctx, conditional, inputsMap, layer, inIdx, node);\n    }\n}\n\n// Add IConditionalInputLayers to `layer`'s inputs.\nvoid addIfInputLayers(ImporterContext* ctx, nvinfer1::IIfConditional* conditional, InputsMap& inputsMap,\n    const std::vector<nvinfer1::ILayer*>& newLayers, ::ONNX_NAMESPACE::NodeProto const* node)\n{\n    // Find all of the tensors entering the subgraph.\n    SubgraphPortsMap externalInputs;\n    getSubgraphInputs(newLayers, externalInputs);\n\n    // Add a ConditionalInputLayer in front of each input that is external to the subgraph.\n    for (const auto& layer : newLayers)\n    {\n        addConditionalInputIfNeeded(ctx, conditional, inputsMap, *layer, externalInputs, node);\n    }\n}\n\n// Given a subgraph, find all of its external inputs (tensors entering the subgraph).\nvoid getSubgraphInputs(const std::vector<nvinfer1::ILayer*>& newLayers, SubgraphPortsMap& externalInputs)\n{\n    using PortIndex = int32_t;\n    using TensorsSet = std::unordered_set<nvinfer1::ITensor*>;\n    TensorsSet outputTensors;\n    TensorsSet inputTensors;\n\n    // To determine which tensors are entering or exiting the given graph, we first collect the sets of all input and\n    // output tensors. Then we categorize the tensors according to this logic:\n    //  Entering tensors := {inputs} - {outputs}\n    //  Exiting tensors := {outputs} - {inputs}\n\n    // Collect all input and output tensors belonging to nodes in the graph.\n\n    auto getTensors = [](nvinfer1::ILayer const* l, bool const input, auto inserter) {\n        auto const count = input ? l->getNbInputs() : l->getNbOutputs();\n        for (int32_t i = 0; i < count; i++)\n        {\n            inserter(input ? l->getInput(i) : l->getOutput(i));\n        }\n    };\n\n    for (const auto& l : newLayers)\n    {\n        getTensors(l, false, [&](nvinfer1::ITensor* t) { outputTensors.insert(t); });\n        getTensors(l, true, [&](nvinfer1::ITensor* t) { inputTensors.insert(t); });\n    }\n\n    using TensorsVec = std::vector<nvinfer1::ITensor*>;\n    auto getInputs = [&](nvinfer1::ILayer const* l, TensorsVec& res) {\n        getTensors(l, true, [&](nvinfer1::ITensor* t) { res.emplace_back(t); });\n    };\n\n    // Retrieve the list of tensors either exiting or entering the subgraph.\n    auto filterTensors = [&](TensorsSet const& tensors, auto getNodeAccessor) {\n        for (nvinfer1::ILayer const* l : newLayers)\n        {\n            PortIndex i = 0;\n\n            TensorsVec nodeAccessor;\n            getNodeAccessor(l, nodeAccessor);\n            for (const auto& tensor : nodeAccessor)\n            {\n                if (tensor == nullptr)\n                {\n                    continue;\n                }\n                if (tensors.count(tensor) == 0)\n                {\n                    externalInputs[l].insert(i);\n                }\n                i++;\n            }\n        }\n    };\n\n    filterTensors(outputTensors, getInputs);\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "ConditionalHelpers.hpp",
          "type": "blob",
          "size": 1.5712890625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n *\n * Helper functions used for importing the ONNX If-operator follow below.\n *\n */\n\n#pragma once\n\n#include \"ImporterContext.hpp\"\n#include \"Status.hpp\"\n#include <NvInfer.h>\n#include <set>\n#include <string>\n#include <unordered_map>\n#include <vector>\n\nnamespace onnx2trt\n{\n\nusing NodeName = std::string;\nusing LayerName = std::string;\nusing InputIndex = int32_t;\n\n// A SubgraphPortsMap maps inputs' ports of each layer in an ONNX graph.\nusing SubgraphPortsMap = std::unordered_map<const nvinfer1::ILayer*, std::unordered_set<InputIndex>>;\n\n// Given a subgraph, find all of its external inputs (tensors entering the subgraph).\nvoid getSubgraphInputs(const std::vector<nvinfer1::ILayer*>& newLayers, SubgraphPortsMap& externalInputs);\n\n// Take a snapshot of the network before and after parsing the subgraph and return a list\n// of newly added network layers.\nvoid importSubgraph(ImporterContext* ctx, ::ONNX_NAMESPACE::GraphProto const& subgraph,\n    std::vector<nvinfer1::ILayer*>& newLayers, std::vector<TensorOrWeights>& subgraphTensors);\n\n// An InputsMap tracks which IIfConditionalInputLayer we've added to a layer's inputs,\n// so that we can reuse them if needed.\nusing InputsMap = std::unordered_map<LayerName, nvinfer1::IIfConditionalInputLayer*>;\n\n// Add IIfConditionalInputLayers to the inputs of the subgraph indicated by `subgraph`.\nvoid addIfInputLayers(ImporterContext* ctx, nvinfer1::IIfConditional* conditional, InputsMap& inputsMap,\n    const std::vector<nvinfer1::ILayer*>& newLayers, ::ONNX_NAMESPACE::NodeProto const* node);\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "ImporterContext.cpp",
          "type": "blob",
          "size": 9.0576171875,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"ImporterContext.hpp\"\n#include \"NvInferVersion.h\"\n#include \"importerUtils.hpp\"\n#include \"weightUtils.hpp\"\n#include <sstream>\n\n#if !defined(_WIN32)\n#include <dlfcn.h>\n#if defined(__linux__)\n#include <link.h>\n#endif\n#else // defined(_WIN32)\n#include <windows.h>\n#endif // !defined(_WIN32)\n\n#define RT_ASSERT(cond)                                                                                                \\\n    do                                                                                                                 \\\n    {                                                                                                                  \\\n        if (!(cond))                                                                                                   \\\n        {                                                                                                              \\\n            throw std::runtime_error(\"Assertion \" #cond \" failed!\");                                                   \\\n        }                                                                                                              \\\n    } while (0)\n\nnamespace onnx2trt\n{\n\nvoid ImporterContext::pushBaseNameScope()\n{\n    mBaseNameScopeStack.push_back({});\n}\n\nvoid ImporterContext::popBaseNameScope()\n{\n    auto& tensorMap = tensors();\n    for (auto& binding : mBaseNameScopeStack.back())\n    {\n        if (binding.second.first)\n        {\n            tensorMap.erase(binding.first);\n        }\n        else\n        {\n            tensorMap.at(binding.first) = std::move(binding.second.second);\n        }\n    }\n    mBaseNameScopeStack.pop_back();\n}\n\nvoid ImporterContext::registerTensor(TensorOrWeights tensor, std::string const& basename, bool const checkUniqueName)\n{\n    // TRT requires unique tensor names.\n    std::string const& uniqueName = generateUniqueName(mTensorNames, mSuffixCounter, basename);\n\n    if (tensor)\n    {\n        if (tensor.is_tensor())\n        {\n            tensor.tensor().setName(uniqueName.c_str());\n            // Logging macro refers to ctx.\n            auto* ctx = this;\n            LOG_VERBOSE(\"Registering tensor: \" << uniqueName << \" for ONNX tensor: \" << basename);\n        }\n        else if (tensor.is_weights())\n        {\n            // It may be possible for nested subgraphs to have different values for the same initializer.\n            // For multiple name scopes - use unique name to keep track of weights.\n            if (!mBaseNameScopeStack.empty())\n            {\n                tensor.weights().setName(uniqueName.c_str());\n            }\n            else\n            {\n                tensor.weights().setName(basename.c_str());\n            }\n        }\n    }\n\n    std::string const& nameToCheck = checkUniqueName ? uniqueName : basename;\n\n    auto const p = this->tensors().emplace(nameToCheck, TensorOrWeights{});\n    bool nameIsDuplicate = false;\n    if (!mBaseNameScopeStack.empty())\n    {\n        // Remember original binding so it can be restored when scope is popped.\n        auto const q\n            = mBaseNameScopeStack.back().emplace(nameToCheck, std::make_pair(p.second, std::move(p.first->second)));\n        // Check that scope did not already have a binding for basename.\n        nameIsDuplicate = !q.second;\n    }\n    else\n    {\n        // The condition here accounts for ModelImporter::importModel reserving\n        // output names by registering null tensors.\n        nameIsDuplicate = !p.second && !p.first->second.isNullTensor();\n    }\n    if (nameIsDuplicate)\n    {\n        throw std::runtime_error(\"ONNX graph has duplicate tensor name: \" + nameToCheck);\n    }\n    p.first->second = std::move(tensor);\n}\n\nvoid ImporterContext::registerLayer(nvinfer1::ILayer* layer, std::string const& basename, ::ONNX_NAMESPACE::NodeProto const* node)\n{\n    // No layer will be added for Constant nodes in ONNX.\n    if (layer)\n    {\n        std::string const name = basename.empty() ? layer->getName() : basename;\n        std::string const& uniqueName = generateUniqueName(mLayerNames, mSuffixCounter, basename);\n\n        auto* ctx = this; // To enable logging.\n        if (node != nullptr)\n        {\n            LOG_VERBOSE(\"Registering layer: \" << uniqueName << \" for ONNX node: \" << basename);\n        }\n        else\n        {\n            LOG_VERBOSE(\"Registering layer: \" << uniqueName << \" required by ONNX-TRT\");\n        }\n\n        layer->setName(uniqueName.c_str());\n        if (layer->getType() == nvinfer1::LayerType::kCONSTANT)\n        {\n            if (basename != uniqueName && mConstantLayers.find(uniqueName) != mConstantLayers.end())\n            {\n                LOG_ERROR(\"Constant layer: \" << uniqueName << \" can be a duplicate of: \" << basename);\n                assert(!\"Internal error: duplicate constant layers for the same weights\");\n            }\n            mConstantLayers.insert({uniqueName, static_cast<nvinfer1::IConstantLayer*>(layer)});\n        }\n    }\n    // Set metadata only if the layer is associated with an ONNX node.\n    // Skip constant layers because constants are represented as initializers in ONNX and should not be associated\n    // with any ONNX node.\n    if (node != nullptr && layer != nullptr && layer->getType() != nvinfer1::LayerType::kCONSTANT)\n    {\n        processMetadata(this, *node, layer);\n    }\n}\n\nvoid ImporterContext::registerLayer(nvinfer1::ILayer* layer, ::ONNX_NAMESPACE::NodeProto const& node)\n{\n    std::string const& basename = getNodeName(node);\n    registerLayer(layer, basename, &node);\n}\n\nnamespace\n{\n\n//! Translates a \"logical\" library name into an OS-dependent DSO or DLL name\nstd::string getOSLibraryName(char const* logicalName)\n{\n    std::stringstream libName;\n#if defined(_WIN32)\n    libName << logicalName << \".dll\";\n#else\n    libName << \"lib\" << logicalName << \".so.\" << NV_TENSORRT_MAJOR;\n#endif\n    return libName.str();\n}\n\n//! Platform-agnostic wrapper around dynamic libraries.\nclass DynamicLibrary\n{\npublic:\n    explicit DynamicLibrary(std::string const& name)\n        : mLibName{name}\n    {\n#if defined(_WIN32)\n        mHandle = LoadLibraryA(name.c_str());\n#else  // defined(_WIN32)\n        int32_t flags{RTLD_LAZY};\n        mHandle = dlopen(name.c_str(), flags);\n#endif // defined(_WIN32)\n\n        if (mHandle == nullptr)\n        {\n            std::string errorStr{};\n#if !defined(_WIN32)\n            errorStr = std::string{\" due to \"} + std::string{dlerror()};\n#endif\n            throw std::runtime_error(\"Unable to open library: \" + name + errorStr);\n        }\n    }\n\n    DynamicLibrary(DynamicLibrary const&) = delete;\n    DynamicLibrary(DynamicLibrary const&&) = delete;\n\n    ~DynamicLibrary()\n    {\n        try\n        {\n#if defined(_WIN32)\n            RT_ASSERT(static_cast<bool>(FreeLibrary(static_cast<HMODULE>(mHandle))));\n#else\n            RT_ASSERT(dlclose(mHandle) == 0);\n#endif\n        }\n        catch (...)\n        {\n            std::cerr << \"Unable to close library: \" << mLibName << std::endl;\n        }\n    }\n\n    std::string getFullPath() const\n    {\n        RT_ASSERT(mHandle != nullptr);\n#if defined(__linux__)\n        link_map* linkMap = nullptr;\n        auto const err = dlinfo(mHandle, RTLD_DI_LINKMAP, &linkMap);\n        RT_ASSERT(err == 0 && linkMap != nullptr && linkMap->l_name != nullptr);\n        return std::string{linkMap->l_name};\n#elif defined(_WIN32)\n        constexpr int32_t kMAX_PATH_LEN{4096};\n        std::string path(kMAX_PATH_LEN, '\\0'); // since C++11, std::string storage is guaranteed to be contiguous\n        auto const pathLen = GetModuleFileNameA(static_cast<HMODULE>(mHandle), &path[0], kMAX_PATH_LEN);\n        RT_ASSERT(GetLastError() == ERROR_SUCCESS);\n        path.resize(pathLen);\n        path.shrink_to_fit();\n        return path;\n#else\n        RT_ASSERT(!\"Unsupported operation: getFullPath()\");\n#endif\n    }\n\nprivate:\n    std::string mLibName{}; //!< Name of the DynamicLibrary\n    void* mHandle{};        //!< Handle to the DynamicLibrary\n};\n\n//! Translates an OS-dependent DSO/DLL name into a path on the filesystem\nstd::string getOSLibraryPath(std::string const& osLibName)\n{\n    DynamicLibrary lib{osLibName};\n    return lib.getFullPath();\n}\n\n} // namespace\n\nvoid ImporterContext::addUsedVCPluginLibrary(\n    ::ONNX_NAMESPACE::NodeProto const& node, char const* pluginName, char const* pluginLib)\n{\n    auto* ctx = this; // For logging\n    auto osPluginLibName = getOSLibraryName(pluginLib);\n    LOG_VERBOSE(\"Node \" << getNodeName(node) << \" requires plugin \" << pluginName << \" which is provided by \"\n                        << osPluginLibName);\n    mLogicalVCPluginLibraries.insert(osPluginLibName);\n}\n\nstd::vector<std::string> ImporterContext::getUsedVCPluginLibraries()\n{\n    auto* ctx = this; // For logging\n#if defined(_WIN32) || defined(__linux__)\n    std::vector<std::string> ret;\n    ret.reserve(mLogicalVCPluginLibraries.size());\n    for (auto const& l : mLogicalVCPluginLibraries)\n    {\n        auto osLibPath = getOSLibraryPath(l);\n        LOG_VERBOSE(\"Library \" << l << \" located on filesystem as \" << osLibPath);\n        ret.emplace_back(std::move(osLibPath));\n    }\n    return ret;\n#else\n    LOG_WARNING(\"getUsedVCPluginLibraries not implemented on platform!\");\n    return {};\n#endif\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "ImporterContext.hpp",
          "type": "blob",
          "size": 12.6875,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include \"NvOnnxParser.h\"\n#include \"ShapedWeights.hpp\"\n#include \"Status.hpp\"\n#include \"TensorOrWeights.hpp\"\n#include \"onnxErrorRecorder.hpp\"\n#include \"WeightsContext.hpp\"\n#include <fstream>\n#include <functional>\n#include <list>\n#include <onnx/onnx_pb.h>\n#include <unordered_map>\n#include <unordered_set>\n#include <vector>\n\nnamespace onnx2trt\n{\n\ntemplate <typename T>\nusing StringMap = std::unordered_map<std::string, T>;\n\nclass ErrorRecorderWrapper\n{\npublic:\n    ErrorRecorderWrapper(nvinfer1::INetworkDefinition* network, nvinfer1::ILogger* logger)\n        : mNetwork(network)\n        , mLogger(logger)\n    {\n        if (mNetwork)\n        {\n            mUserErrorRecorder = mNetwork->getErrorRecorder();\n            mOnnxErrorRecorder = ONNXParserErrorRecorder::create(logger, mUserErrorRecorder);\n            if (mOnnxErrorRecorder)\n            {\n                if (mUserErrorRecorder)\n                {\n                    mUserErrorRecorder->incRefCount();\n                }\n                mNetwork->setErrorRecorder(mOnnxErrorRecorder);\n            }\n        }\n    }\n\n    ~ErrorRecorderWrapper()\n    {\n        if (mNetwork && mOnnxErrorRecorder)\n        {\n            if (mUserErrorRecorder)\n            {\n                mNetwork->setErrorRecorder(mUserErrorRecorder);\n                mUserErrorRecorder->decRefCount();\n            }\n            ONNXParserErrorRecorder::destroy(mOnnxErrorRecorder);\n        }\n    }\n\n    bool hasError() const\n    {\n        return mOnnxErrorRecorder != nullptr && mOnnxErrorRecorder->getNbErrors() != 0;\n    }\n\n    //! Return recorder used by hasError().\n    nvinfer1::IErrorRecorder* getErrorRecorder() const\n    {\n        return mOnnxErrorRecorder ? mOnnxErrorRecorder : nullptr;\n    }\nprivate:\n    nvinfer1::INetworkDefinition* mNetwork{nullptr};\n    nvinfer1::ILogger* mLogger{nullptr};\n    ONNXParserErrorRecorder* mOnnxErrorRecorder{nullptr};\n    nvinfer1::IErrorRecorder* mUserErrorRecorder{nullptr};\n};\n\nclass ImporterContext\n{\n    nvinfer1::INetworkDefinition* mNetwork;\n    nvinfer1::ILogger* mLogger;\n    //! WeightsContext object to hold ownership of ONNX weights and any temporary weights created by the Parser.\n    WeightsContext mWeightsContext;\n    StringMap<int64_t> mOpsets;\n    //! All tensors in the graph mapped to their names.\n    StringMap<TensorOrWeights> mTensors;\n    StringMap<nvinfer1::TensorLocation> mTensorLocations;\n    StringMap<float> mTensorRangeMins;\n    StringMap<float> mTensorRangeMaxes;\n    StringMap<nvinfer1::DataType> mLayerPrecisions;\n    //! Set to keep track of how many times a tensor name shows up, to avoid duplicate naming in TRT.\n    std::set<std::string> mTensorNames;\n    //! Set to keep track of how many times a tensor name shows up, to avoid duplicate naming in TRT.\n    std::set<std::string> mLayerNames;\n    //! An increasing suffix counter used to uniquify layer names.\n    int64_t mSuffixCounter{0};\n    //! Set to keep track of how many times a batch norm weight name shows up,\n    //! to avoid duplicate naming in TRT.\n    std::set<std::string> mBatchNormWeightNames;\n    //! An increasing suffix counter used to uniquify batch norm weight names.\n    int64_t mBatchNormWeightSuffixCounter{0};\n    //! Set to hold output tensor names of layers that produce shape tensor outputs but do not\n    //! natively support them.\n    std::unordered_set<std::string> mUnsupportedShapeTensors;\n    //! Container to map subgraph tensors to their original outer graph names.\n    StringMap<std::string> mLoopTensors;\n    //! Error recorder to control TRT errors.\n    std::unique_ptr<ErrorRecorderWrapper> mErrorWrapper;\n    StringMap<nvinfer1::IConstantLayer*> mConstantLayers;\n    bool mConvertINT64Logged{false};\n    bool mConvertINT64OutOfBoundsLogged{false};\n    bool mConvertDoubleLogged{false};\n    bool mConvertDoubleOutOfBoundsLogged{false};\n    //! OnnxParserFlags specified by the parser.\n    nvonnxparser::OnnxParserFlags mOnnxParserFlags;\n    StringMap<std::vector<nvinfer1::ITensor const*>> mNodeNameToTensor;\n\n    //! Logical library names for VC plugin libraries. This gets translated to library paths\n    //! when getUsedVCPluginLibraries() is called.\n    std::set<std::string> mLogicalVCPluginLibraries;\n\n    //! Stack of names defined by nested ONNX graphs, with information about how to\n    //! restore their associated values when popping back to the surrounding scope.\n    //!\n    //! The stack is empty when processing the top-level ONNX graph.\n    //! back() corresponds to the innermost ONNX graph being processed.\n    //!\n    //! For each entry {name, {bool, TensorOrWeights}}:\n    //!\n    //! * If the bool is true, the name was newly introduced by the scope.\n    //!\n    //! * If the bool is false, the name shadows a name in a surrounding scope,\n    //!   and TensorOrWeights was the name's value before being shadowed.\n    //!\n    std::vector<StringMap<std::pair<bool, TensorOrWeights>>> mBaseNameScopeStack;\n\n    //! Map holding FunctionProtos\n    StringMap<::ONNX_NAMESPACE::FunctionProto> mLocalFunctions;\n\n    //! Data type to keep track of a local function in mLocalFunctionStack.\n    //! It is a tuple of three elements: (1) function name (2) node name and (3) function attributes.\n    struct LocalFunctionMetadata\n    {\n        std::string functionName;\n        std::string nodeName;\n        StringMap<::ONNX_NAMESPACE::AttributeProto const*> attrs;\n    };\n\n    //! Vector to hold current local function names and attributes\n    std::vector<LocalFunctionMetadata> mLocalFunctionStack;\n\n    //! Vector to hold the local function names at each error\n    std::vector<std::vector<std::string>> mLocalFunctionErrors;\n\n    //! Vector to hold expected graph outputs\n    std::vector<::ONNX_NAMESPACE::ValueInfoProto> mGraphOutputNames;\n\npublic:\n    ImporterContext(nvinfer1::INetworkDefinition* network, nvinfer1::ILogger* logger)\n        : mNetwork(network)\n        , mLogger(logger)\n        , mWeightsContext(WeightsContext(logger))\n        , mErrorWrapper(std::make_unique<ErrorRecorderWrapper>(mNetwork, logger))\n    {\n    }\n    nvinfer1::INetworkDefinition* network()\n    {\n        assert(mNetwork != nullptr);\n        return mNetwork;\n    }\n    WeightsContext& getWeightsContext()\n    {\n        return mWeightsContext;\n    }\n    StringMap<TensorOrWeights>& tensors()\n    {\n        return mTensors;\n    }\n    StringMap<nvinfer1::TensorLocation>& tensorLocations()\n    {\n        return mTensorLocations;\n    }\n    StringMap<float>& tensorRangeMins()\n    {\n        return mTensorRangeMins;\n    }\n    StringMap<float>& tensorRangeMaxes()\n    {\n        return mTensorRangeMaxes;\n    }\n    StringMap<nvinfer1::DataType>& layerPrecisions()\n    {\n        return mLayerPrecisions;\n    }\n    std::unordered_set<std::string>& unsupportedShapeTensors()\n    {\n        return mUnsupportedShapeTensors;\n    }\n    StringMap<std::string>& loopTensors()\n    {\n        return mLoopTensors;\n    }\n    // Pass file location down to WeightsContext as all external weight handling logic is done in that class.\n    void setOnnxFileLocation(std::string location)\n    {\n        mWeightsContext.setOnnxFileLocation(location);\n    }\n    void pushBaseNameScope();\n\n    void popBaseNameScope();\n\n    // This actually handles weights as well, but is named this way to be consistent with the tensors()\n    void registerTensor(TensorOrWeights tensor, std::string const& basename, bool const checkUniqueName = false);\n\n    void registerLayer(nvinfer1::ILayer* layer, std::string const& basename, ::ONNX_NAMESPACE::NodeProto const* node);\n    void registerLayer(nvinfer1::ILayer* layer, ::ONNX_NAMESPACE::NodeProto const& node);\n\n    nvinfer1::ILogger& logger()\n    {\n        return *mLogger;\n    }\n\n    // Register an unique name for the created weights\n    ShapedWeights createNamedTempWeights(ShapedWeights::DataType type, nvinfer1::Dims shape, bool batchNormNode = false)\n    {\n        if (batchNormNode)\n        {\n            return mWeightsContext.createNamedTempWeights(\n                type, shape, mBatchNormWeightNames, mBatchNormWeightSuffixCounter, /*batchNormNode=*/true);\n        }\n        return mWeightsContext.createNamedTempWeights(type, shape, mTensorNames, mSuffixCounter);\n    }\n\n    void clearOpsets()\n    {\n        mOpsets.clear();\n    }\n    void addOpset(std::string domain, int64_t version)\n    {\n        mOpsets.emplace(domain, version);\n    }\n    int64_t getOpsetVersion(const char* domain = \"\") const\n    {\n        if (mOpsets.empty())\n        {\n            return 1;\n        }\n        else if (mOpsets.size() == 1)\n        {\n            return mOpsets.begin()->second;\n        }\n        else if (mOpsets.count(domain))\n        {\n            return mOpsets.at(domain);\n        }\n        else\n        {\n            domain = \"ai.onnx\";\n            assert(mOpsets.count(domain));\n            return mOpsets.at(domain);\n        }\n    }\n    bool hasError() const noexcept\n    {\n        return mErrorWrapper != nullptr && mErrorWrapper->hasError();\n    }\n\n    nvinfer1::IErrorRecorder* getErrorRecorder() const noexcept\n    {\n        return mErrorWrapper ? mErrorWrapper->getErrorRecorder() : nullptr;\n    }\n    nvinfer1::IConstantLayer* getConstantLayer(const char* name) const\n    {\n        if (name == nullptr)\n        {\n            return nullptr;\n        }\n        auto const iter = mConstantLayers.find(name);\n        if (iter == mConstantLayers.end())\n        {\n            return nullptr;\n        }\n        return iter->second;\n    }\n\n    void setFlags(nvonnxparser::OnnxParserFlags const& onnxParserFlags)\n    {\n        mOnnxParserFlags = onnxParserFlags;\n    }\n    nvonnxparser::OnnxParserFlags getFlags() const\n    {\n        return mOnnxParserFlags;\n    }\n\n    virtual void addUsedVCPluginLibrary(\n        ::ONNX_NAMESPACE::NodeProto const& node, char const* pluginName, char const* pluginLib);\n\n    virtual std::vector<std::string> getUsedVCPluginLibraries();\n\n    bool isConvertINT64Logged()\n    {\n        return mConvertINT64Logged;\n    }\n    void setConvertINT64Logged(bool logged)\n    {\n        mConvertINT64Logged = logged;\n    }\n    bool isConvertINT64OutOfBoundsLogged()\n    {\n        return mConvertINT64OutOfBoundsLogged;\n    }\n    void setConvertINT64OutOfBoundsLogged(bool logged)\n    {\n        mConvertINT64OutOfBoundsLogged = logged;\n    }\n    bool isConvertDoubleLogged()\n    {\n        return mConvertDoubleLogged;\n    }\n    void setConvertDoubleLogged(bool logged)\n    {\n        mConvertDoubleLogged = logged;\n    }\n    bool isConvertDoubleOutOfBoundsLogged()\n    {\n        return mConvertDoubleOutOfBoundsLogged;\n    }\n    void setConvertDoubleOutOfBoundsLogged(bool logged)\n    {\n        mConvertDoubleOutOfBoundsLogged = logged;\n    }\n    StringMap<::ONNX_NAMESPACE::FunctionProto>& localFunctions()\n    {\n        return mLocalFunctions;\n    }\n    std::vector<LocalFunctionMetadata>& localFunctionStack()\n    {\n        return mLocalFunctionStack;\n    }\n    std::vector<std::vector<std::string>>& localFunctionErrors()\n    {\n        return mLocalFunctionErrors;\n    }\n    std::vector<::ONNX_NAMESPACE::ValueInfoProto>& getGraphOutputNames()\n    {\n        return mGraphOutputNames;\n    }\n    nvinfer1::ITensor const* findLayerOutputTensor(std::string name, int64_t i)\n    {\n        auto it = mNodeNameToTensor.find(name);\n        if (it == mNodeNameToTensor.end())\n        {\n            return nullptr;\n        }\n        auto tensors = it->second;\n        return i < static_cast<int64_t>(tensors.size()) ? tensors.at(i) : nullptr;\n    }\n    void addLayerOutputTensors(std::string name, std::vector<TensorOrWeights> const& outputs)\n    {\n        if (mNodeNameToTensor.find(name) != mNodeNameToTensor.end())\n        {\n            auto* ctx = this; // For logging\n            LOG_WARNING(\n                \"A node named \" << name\n                                << \" already exists, the output tensors of this new instance will not be queryable.\");\n            return;\n        }\n        for (auto const& output : outputs)\n        {\n            if (output.is_tensor())\n            {\n                mNodeNameToTensor[name].push_back(static_cast<nvinfer1::ITensor const*>(&(output.tensor())));\n            }\n        }\n    }\n    size_t getNestedDepth()\n    {\n        return mBaseNameScopeStack.size();\n    }\n\n    // Returns if the underlying network was created with the KSTRONGLY_TYPED flag.\n    bool const isStronglyTyped()\n    {\n        assert(mNetwork != nullptr);\n        return mNetwork->getFlag(nvinfer1::NetworkDefinitionCreationFlag::kSTRONGLY_TYPED);\n    }\n};\n\ntypedef std::vector<TensorOrWeights> NodeOutputs;\ntypedef std::function<NodeOutputs(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights>& inputs)>\n    NodeImporter;\n\ntypedef std::function<void(\n    ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, std::vector<Status>& errors, size_t const nodeIndex)>\n    OpStaticErrorChecker;\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 12.3134765625,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 NVIDIA Corporation\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n\n   PORTIONS LICENSED AS FOLLOWS\n\n   > ieee_half.h\n   > half.h\n\n     The MIT License\n\n     Copyright (c) 2012-2017 Christian Rau <rauy@users.sourceforge.net>\n\n     Permission is hereby granted, free of charge, to any person obtaining a\n     copy of this software and associated documentation files (the \"Software\"),\n     to deal in the Software without restriction, including without limitation\n     the rights to use, copy, modify, merge, publish, distribute, sublicense,\n     and/or sell copies of the Software, and to permit persons to whom the\n     Software is furnished to do so, subject to the following conditions:\n\n     The above copyright notice and this permission notice shall be included\n     in all copies or substantial portions of the Software.\n\n     THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n     IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n     FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n     THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n     LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n     DEALINGS IN THE SOFTWARE.\n\n\n"
        },
        {
          "name": "LoopHelpers.cpp",
          "type": "blob",
          "size": 0.8291015625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"LoopHelpers.hpp\"\n#include \"importerUtils.hpp\"\n\nnamespace onnx2trt\n{\n\nnvinfer1::ITensor* addLoopCounter(ImporterContext* ctx, nvinfer1::ILoop* loop, int64_t initial)\n{\n    nvinfer1::ITensor* initialTensor\n        = addConstantScalar(ctx, initial, ::ONNX_NAMESPACE::TensorProto::INT64, nvinfer1::Dims{1, {1}})->getOutput(0);\n    nvinfer1::ITensor* one = addConstantScalar(ctx, static_cast<int64_t>(1), ::ONNX_NAMESPACE::TensorProto::INT64,\n        nvinfer1::Dims{1, {1}})->getOutput(0);\n\n    auto counter = N_CHECK(loop->addRecurrence(*initialTensor));\n    nvinfer1::ITensor* addOne = getElementWiseResult(ctx, *N_CHECK(counter->getOutput(0)), *one, nvinfer1::ElementWiseOperation::kSUM);\n    counter->setInput(1, *addOne);\n    return N_CHECK(counter->getOutput(0));\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "LoopHelpers.hpp",
          "type": "blob",
          "size": 0.2568359375,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include <NvInfer.h>\n\n#include \"ImporterContext.hpp\"\n\nnamespace onnx2trt\n{\n\nnvinfer1::ITensor* addLoopCounter(ImporterContext* ctx, nvinfer1::ILoop* loop, int64_t initial = 0);\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "ModelImporter.cpp",
          "type": "blob",
          "size": 37.1953125,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"ModelImporter.hpp\"\n#include \"OnnxAttrs.hpp\"\n#include \"Status.hpp\"\n#include \"errorHelpers.hpp\"\n#include \"importerUtils.hpp\"\n#include \"onnxProtoUtils.hpp\"\n#include \"toposort.hpp\"\n\n#include <google/protobuf/io/coded_stream.h>\n#include <google/protobuf/io/zero_copy_stream_impl.h>\n#include <google/protobuf/text_format.h>\n\n#include <functional>\n#include <limits>\n#include <sys/stat.h>\n#include <unordered_set>\n\nnamespace onnx2trt\n{\n\n// Helper class and object to shutdown protobuf library upon library unload.\nclass ProtobufShutter\n{\npublic:\n    ~ProtobufShutter()\n    {\n        google::protobuf::ShutdownProtobufLibrary();\n    }\n};\n\nstatic ProtobufShutter protobufShutter;\n\n// Helper for deserializing INetwork\nvoid setTensorLocations(\n    ImporterContext* ctx, std::vector<std::string> const& tensors, std::vector<std::string> const& locations)\n{\n    ONNXTRT_CHECK((tensors.size() >= locations.size())\n            && \"The size of tensors misaligns with the size of the attribute trt_outputs_loc.\",\n        nvonnxparser::ErrorCode::kINVALID_GRAPH);\n    for (size_t i = 0; i < locations.size(); ++i)\n    {\n        std::string tensor = tensors.at(i);\n        std::string location = locations.at(i);\n        nvinfer1::TensorLocation loc\n            = location == \"device\" ? nvinfer1::TensorLocation::kDEVICE : nvinfer1::TensorLocation::kHOST;\n\n        if (ctx->tensorLocations().count(tensor) > 0)\n        {\n            ONNXTRT_CHECK((ctx->tensorLocations()[tensor] == loc) && \"The tensor location cannot be changed.\",\n                nvonnxparser::ErrorCode::kINVALID_GRAPH);\n        }\n        else\n        {\n            ctx->tensorLocations()[tensor] = loc;\n        }\n    }\n}\n\n// Helper for deserializing INetwork\ntemplate <typename T>\nvoid setStringMap(\n    ImporterContext* ctx, std::vector<std::string> const& tensors, std::vector<T> const& data, StringMap<T>& map)\n{\n    ONNXTRT_CHECK((tensors.size() >= data.size())\n            && \"The size of tensors misaligns with the size of the attribute trt_outputs_range_min/max.\",\n        nvonnxparser::ErrorCode::kINVALID_GRAPH);\n    for (size_t i = 0; i < data.size(); ++i)\n    {\n        std::string name = tensors.at(i);\n        T dataName = data.at(i);\n        if (map.count(name) > 0)\n        {\n            ONNXTRT_CHECK( (map[name] == dataName) && \"The order of tensorRangeMin/Max in context misaligns with the order of the attribute trt_outputs_range_min/max.\", nvonnxparser::ErrorCode::kINVALID_GRAPH);\n        }\n        else\n        {\n            map[name] = dataName;\n        }\n    }\n}\n\n//! Make error explanation from TensorRT error recorder.\nstatic std::string makeErrorExplanation(ImporterContext* ctx, std::string const& nodeName)\n{\n    std::ostringstream result;\n    result << \"Invalid Node - \" << nodeName;\n    if (auto* errorRecorder = ctx->getErrorRecorder())\n    {\n        // Append information that might help the user understand the error.\n        int32_t const nbErrors = errorRecorder->getNbErrors();\n        for (int32_t i = 0; i < nbErrors; ++i)\n        {\n            result << \"\\n\" << errorRecorder->getErrorDesc(i);\n        }\n    }\n    return result.str();\n}\n\n//! Make error explanation from an exception.\nstatic std::string makeErrorExplanation(std::exception const& e, std::string const& nodeName)\n{\n    std::ostringstream result;\n    result << \"Exception occurred in - \" << nodeName << \"\\n\" << e.what();\n    return result.str();\n}\n\nbool isNodeInPluginRegistry(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node)\n{\n    OnnxAttrs attrs(node, ctx);\n    std::string const pluginVersion{attrs.get<std::string>(\"plugin_version\", \"1\")};\n    std::string const pluginNamespace{attrs.get<std::string>(\"plugin_namespace\", \"\")};\n    LOG_INFO(\"Checking if node can be treated as plugin: \" << node.op_type() << \", plugin_version: \" << pluginVersion\n                                                           << \", plugin_namespace: \" << pluginNamespace);\n    nvinfer1::IPluginCreatorInterface* creator\n        = importPluginCreator(ctx, node.op_type(), pluginVersion, pluginNamespace);\n    return creator;\n}\n\nvoid parseNode(\n    ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx, bool deserializingINetwork)\n{\n    // For nodes that contain subgraphs (Ifs, Loops, Scans, LocalFunctions), ensure that the recursion depth is\n    // limited to a set amount. Recursion depth is tracked by the size of ctx->mBaseNameScopeStack().\n    size_t const kMAX_NESTED_SUBGRAPHS = 24;\n    if (ctx->getNestedDepth() > kMAX_NESTED_SUBGRAPHS)\n    {\n        ONNXTRT_THROW(MAKE_ERROR(\"ONNX graph contains nested structures that exceed the maximum allowed by TensorRT!\",\n            ErrorCode::kUNSUPPORTED_GRAPH));\n    }\n    StringMap<NodeImporter> const& opImporters = getBuiltinOpImporterMap();\n    std::string const& nodeName = getNodeName(node);\n    std::string const& nodeType = node.op_type();\n    LOG_VERBOSE(\"Parsing node: \" << nodeName << \" [\" << nodeType << \"]\");\n\n    // Assemble node inputs. These may come from outside the subgraph.\n    std::vector<TensorOrWeights> nodeInputs;\n    std::ostringstream ssInputs{};\n    ssInputs << nodeName << \" [\" << nodeType << \"] inputs: \";\n    for (auto const& inputName : node.input())\n    {\n        // Empty input names indicate optional inputs which have not been supplied.\n        if (inputName.empty())\n        {\n            // Push back null input as place holder.\n            nodeInputs.emplace_back(nullptr);\n            ssInputs << \"[optional input, not set], \";\n        }\n        else\n        {\n            LOG_VERBOSE(\"Searching for input: \" << inputName);\n            ONNXTRT_CHECK_NODE((ctx->tensors().count(inputName)), \"Node input was not registered.\", node, nodeIdx,\n                ErrorCode::kINVALID_GRAPH);\n            nodeInputs.push_back(ctx->tensors().at(inputName));\n            ssInputs << \"[\" << inputName << \" -> \" << nodeInputs.back().shape() << \"[\" << nodeInputs.back().getType()\n                     << \"]\"\n                     << \"], \";\n        }\n    }\n    LOG_VERBOSE(ssInputs.str());\n\n    // Dispatch to appropriate converter.\n    NodeImporter const* importFunc{nullptr};\n    if (opImporters.count(nodeType))\n    {\n        importFunc = &opImporters.at(nodeType);\n    }\n    else if (ctx->localFunctions().count(nodeType))\n    {\n        // Let plugin take precedence over local function. So first check if this can be dispatched to a plugin.\n        if (isNodeInPluginRegistry(ctx, node))\n        {\n            LOG_INFO(\"Found registered plugin: \" << nodeType << \". Importing local function as a plugin.\");\n            importFunc = &opImporters.at(\"FallbackPluginImporter\");\n        }\n        else\n        {\n            LOG_INFO(\"Found registered local function: \" << nodeType << \". Importing as a local function.\");\n            importFunc = &opImporters.at(\"LocalFunctionImporter\");\n        }\n    }\n    else\n    {\n        LOG_INFO(\"No importer registered for op: \" << nodeType << \". Attempting to import as plugin.\");\n        importFunc = &opImporters.at(\"FallbackPluginImporter\");\n    }\n\n    std::vector<TensorOrWeights> outputs;\n    try\n    {\n        outputs = (*importFunc)(ctx, node, nodeIdx, nodeInputs);\n    }\n    catch (OnnxTrtException& e)\n    {\n        throw e;\n    }\n    catch (std::exception& e)\n    {\n        ONNXTRT_THROW(MAKE_NODE_ERROR(makeErrorExplanation(ctx, nodeName), ErrorCode::kINVALID_NODE, node, nodeIdx));\n    }\n\n    ctx->addLayerOutputTensors(nodeName, outputs);\n    for (auto const& output : outputs)\n    {\n        if (output.is_tensor())\n        {\n            // check that we can resolve output dims\n            // in the future we may have a network/layer.validate() which will help with that as well\n            output.tensor().getDimensions();\n\n            // If output dimensions cannot be resolved the error will be captured by the ErrorRecorder.\n            if (ctx->hasError())\n            {\n                ONNXTRT_THROW(\n                    MAKE_NODE_ERROR(makeErrorExplanation(ctx, nodeName), ErrorCode::kINVALID_NODE, node, nodeIdx));\n            }\n        }\n    }\n\n    if (deserializingINetwork)\n    {\n        OnnxAttrs attrs(node, ctx);\n\n        // Tensor locations, dynamic ranges and layer precisions will be set after parsing the network\n        std::vector<std::string> outputsLocation = attrs.get<std::vector<std::string>>(\"trt_outputs_loc\", {});\n        std::vector<std::string> outputsVec(node.output().begin(), node.output().end());\n        std::vector<std::string> layerName{nodeName};\n        setTensorLocations(ctx, outputsVec, outputsLocation);\n\n        auto outputsRangeMin = attrs.get<std::vector<float>>(\"trt_outputs_range_min\", {});\n        setStringMap<float>(ctx, outputsVec, outputsRangeMin, ctx->tensorRangeMins());\n        auto outputsRangeMax = attrs.get<std::vector<float>>(\"trt_outputs_range_max\", {});\n        setStringMap<float>(ctx, outputsVec, outputsRangeMax, ctx->tensorRangeMaxes());\n\n        if (attrs.count(\"trt_layer_precision\"))\n        {\n            std::vector<nvinfer1::DataType> layerPrecision{attrs.get<nvinfer1::DataType>(\"trt_layer_precision\")};\n            setStringMap<nvinfer1::DataType>(ctx, layerName, layerPrecision, ctx->layerPrecisions());\n        }\n    }\n\n    int32_t nonEmptyOutputs\n        = std::count_if(node.output().begin(), node.output().end(), [](std::string const& str) { return !str.empty(); });\n    ONNXTRT_CHECK_NODE(nonEmptyOutputs == static_cast<int32_t>(outputs.size()),\n        \"Node has more output tensors than TRT expected, expected output size is \"\n            << outputs.size() << \", actual output size is \" << nonEmptyOutputs << \".\",\n        node, nodeIdx, ErrorCode::kINVALID_GRAPH);\n\n    // Set output names and register outputs with the context.\n    std::ostringstream ssOutputs{};\n    ssOutputs << nodeName << \" [\" << node.op_type() << \"] outputs: \";\n    for (int32_t i = 0, trtCnt = 0; i < node.output().size(); ++i)\n    {\n        auto const& outputName = node.output(i);\n        // Empty strings denote null-tensor outputs. Ignore these.\n        if (outputName.empty())\n        {\n            continue;\n        }\n        auto& output = outputs.at(trtCnt);\n        ssOutputs << \"[\" << outputName << \" -> \" << output.shape() << \"[\" << output.getType() << \"]\"\n                  << \"], \";\n        // Note: This condition is to allow ONNX outputs to be ignored\n        // Always register output weights (even empty ones) as it may be mapped to an unused input\n        if ((output || output.is_weights()) && !outputName.empty())\n        {\n            ctx->registerTensor(std::move(output), outputName);\n        }\n        // UINT8 is only allowed as network inputs and outputs. Therefore any node that produces an UINT8-typed\n        // output that is not also a graph output is unsupported.\n        if (output.getType() == \"UINT8\")\n        {\n            bool legalUINT8 = false;\n            for (auto const& graphOutput : ctx->getGraphOutputNames())\n            {\n                if (graphOutput.name() == outputName)\n                {\n                    legalUINT8 = true;\n                }\n            }\n            ONNXTRT_CHECK_NODE(legalUINT8, \"TensorRT does not support UINT8 types for intermediate tensors!\", node,\n                nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        }\n        trtCnt++;\n    }\n    LOG_VERBOSE(ssOutputs.str());\n}\n\nvoid parseNodeStaticCheck(\n    ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, std::vector<Status>& errors, size_t const nodeIndex)\n{\n    StringMap<OpStaticErrorChecker> const& opCheckers = getOpStaticErrorCheckerMap();\n    StringMap<NodeImporter> const& opImporters = getBuiltinOpImporterMap();\n    std::string const& nodeName = getNodeName(node);\n    std::string const& nodeType = node.op_type();\n    LOG_VERBOSE(\"Static check for parsing node: \" << nodeName << \" [\" << nodeType << \"]\");\n\n    // Dispatch to appropriate static error checker.\n    OpStaticErrorChecker const* checkerFunc{nullptr};\n    if (opImporters.count(nodeType))\n    {\n        if (!opCheckers.count(nodeType))\n        {\n            std::string errorMsg = \"No static checker was found for \" + nodeType;\n            errors.push_back(MAKE_NODE_ERROR(errorMsg, ErrorCode::kINTERNAL_ERROR, node, nodeIndex));\n            return;\n        }\n        checkerFunc = &opCheckers.at(nodeType);\n    }\n    else if (opCheckers.count(nodeType))\n    {\n        checkerFunc = &opCheckers.at(nodeType);\n    }\n    else if (ctx->localFunctions().count(nodeType))\n    {\n        // Let plugin take precedence over local function. So first check if this can be dispatched to a plugin.\n        if (isNodeInPluginRegistry(ctx, node))\n        {\n            LOG_INFO(\"Found registered plugin: \" << nodeType << \". Importing local function as a plugin.\");\n            checkerFunc = &opCheckers.at(\"FallbackPluginImporter\");\n        }\n        else\n        {\n            LOG_INFO(\"Found registered local function: \" << nodeType << \". Importing as a local function.\");\n            checkerFunc = &opCheckers.at(\"LocalFunctionImporter\");\n        }\n    }\n    else\n    {\n        LOG_INFO(\"No checker registered for op: \" << nodeType << \". Attempting to check as plugin.\");\n        checkerFunc = &opCheckers.at(\"FallbackPluginImporter\");\n    }\n    (*checkerFunc)(ctx, node, errors, nodeIndex);\n}\n\nvoid parseGraph(ImporterContext* ctx, ::ONNX_NAMESPACE::GraphProto const& graph, std::vector<Status>& errors,\n    bool deserializingINetwork, int* currentNode)\n{\n    // Import initializers.\n    try\n    {\n        for (::ONNX_NAMESPACE::TensorProto const& initializer : graph.initializer())\n        {\n            LOG_VERBOSE(\"Importing initializer: \" << initializer.name());\n            ShapedWeights weights;\n            ONNXTRT_CHECK(\n                ctx->getWeightsContext().convertOnnxWeights(initializer, &weights) && \"Failed to import initializer.\",\n                ErrorCode::kUNSUPPORTED_NODE);\n            ctx->registerTensor(TensorOrWeights{std::move(weights)}, initializer.name());\n        }\n    }\n    catch (const std::exception& e)\n    {\n        ONNXTRT_THROW(MAKE_ERROR(\"Failed to import initialzer\", ErrorCode::kINVALID_GRAPH));\n    }\n\n    // Keep track of graph outputs in the context to validate UINT8 nodes\n    for (const auto& output : graph.output())\n    {\n        ctx->getGraphOutputNames().push_back(output);\n    }\n\n    std::vector<size_t> topoOrder;\n    ONNXTRT_CHECK(\n        toposort(graph.node(), &topoOrder) && \"Failed to sort the model topologically.\", ErrorCode::kINVALID_GRAPH);\n\n    for (auto const& nodeIndex : topoOrder)\n    {\n        if (currentNode)\n        {\n            *currentNode = nodeIndex;\n        }\n        parseNodeStaticCheck(ctx, graph.node(nodeIndex), errors, nodeIndex);\n        if (errors.size() == 0)\n        {\n            // At most one dynamic error will be returned.\n            parseNode(ctx, graph.node(nodeIndex), nodeIndex, deserializingINetwork);\n        }\n    }\n\n    // Static check still reports error through the error vector by design\n    if (errors.size() != 0)\n    {\n        auto result = errors.back();\n        errors.pop_back(); // this error will be added back to the list in ModelImporter::parseWithWeightDescriptors.\n\n        ONNXTRT_THROW(result);\n    }\n}\n\n// Still returns a vector<Status> since CHECK_INPUT doesn't immediately return\nstd::vector<Status> importInput(ImporterContext* ctx, ::ONNX_NAMESPACE::ValueInfoProto const& input,\n    nvinfer1::ITensor** tensor, std::vector<NamedDimension>& namedDims)\n{\n    std::vector<Status> errorList{};\n    auto const& onnxDtype = input.type().tensor_type();\n    nvinfer1::DataType trtDtype{nvinfer1::DataType::kFLOAT};\n    CHECK_INPUT(\n        convertDtype(onnxDtype.elem_type(), &trtDtype) && \"Failed to convert ONNX date type to TensorRT data type.\",\n        ErrorCode::kUNSUPPORTED_NODE, input.name(), errorList);\n    nvinfer1::Dims trt_dims;\n    size_t const oldNbNamedDimensions = namedDims.size();\n    CHECK_INPUT(convertOnnxDims(onnxDtype.shape().dim(), trt_dims, namedDims)\n            && \"Failed to convert ONNX dimensions to TensorRT dimensions.\",\n        ErrorCode::kUNSUPPORTED_GRAPH, input.name(), errorList);\n\n    LOG_VERBOSE(\n        \"Adding network input: \" << input.name() << \" with dtype: \" << trtDtype << \", dimensions: \" << trt_dims);\n    if (errorList.empty())\n    {\n        *tensor = ctx->network()->addInput(input.name().c_str(), trtDtype, trt_dims);\n        CHECK_INPUT(\n            *tensor && \"Failed to add input to the network.\", ErrorCode::kUNSUPPORTED_NODE, input.name(), errorList);\n    }\n\n    // Fill in field `tensor` for any dimensions that had names in the ONNX.\n    for (auto i = oldNbNamedDimensions; i < namedDims.size(); ++i)\n    {\n        namedDims[i].tensor = *tensor;\n    }\n    return errorList;\n}\n\nstatic void setDimensionNames(ImporterContext* ctx, std::vector<NamedDimension>& namedDims)\n{\n    for (auto const& namedDim : namedDims)\n    {\n        namedDim.tensor->setDimensionName(namedDim.index, namedDim.dimParam.c_str());\n    }\n}\n\nvoid importInputs(ImporterContext* ctx, ::ONNX_NAMESPACE::GraphProto const& graph, StringMap<TensorOrWeights>* tensors,\n    std::vector<Status>& errors)\n{\n    // The weights come from the Initializer list in onnx graph\n    // Initializers are not really network inputs, so they need to be excluded.\n    std::unordered_set<std::string> initializers{};\n    for (::ONNX_NAMESPACE::TensorProto const& initializer : graph.initializer())\n    {\n        initializers.emplace(initializer.name());\n    }\n\n    std::vector<NamedDimension> namedDims;\n    std::vector<Status> statusList{};\n    for (::ONNX_NAMESPACE::ValueInfoProto const& input : graph.input())\n    {\n        TensorOrWeights tensor;\n        if (!initializers.count(input.name()))\n        {\n            nvinfer1::ITensor* tensor_ptr{nullptr};\n            std::vector<Status> status = importInput(ctx, input, &tensor_ptr, namedDims);\n            statusList.insert(statusList.end(), status.begin(), status.end());\n            tensor = tensor_ptr;\n            if (statusList.empty() && tensor_ptr->getType() == nvinfer1::DataType::kINT64)\n            {\n                LOG_WARNING(\"Make sure input \" << input.name() << \" has Int64 binding.\");\n            }\n        }\n        ctx->registerTensor(std::move(tensor), input.name());\n    }\n    if (!statusList.empty())\n    {\n        errors.insert(errors.end(), statusList.begin(), statusList.end());\n        return;\n    }\n    setDimensionNames(ctx, namedDims);\n}\n\nvoid importLocalFunctions(ImporterContext* ctx, ::ONNX_NAMESPACE::ModelProto const& model)\n{\n    for (auto const& localFunction : model.functions())\n    {\n        ctx->localFunctions().insert({localFunction.name(), localFunction});\n    }\n}\n\nstd::pair<bool, ModelImporter::SubGraphSupportVector_t> ModelImporter::doSupportsModel(\n    void const* serialized_onnx_model, size_t serialized_onnx_model_size, char const* model_path)\n{\n    ::ONNX_NAMESPACE::ModelProto model;\n    deserializeOnnxModel(serialized_onnx_model, serialized_onnx_model_size, &model);\n\n    if (model_path)\n    {\n        mImporterCtx.setOnnxFileLocation(model_path);\n    }\n\n    bool allSupported{true};\n\n    // Parse the graph and see if we hit any parsing errors\n    allSupported = parse(serialized_onnx_model, serialized_onnx_model_size);\n\n    int32_t error_node = -1;\n    std::string input_node{};\n\n    if (!allSupported)\n    {\n        int32_t nerror = getNbErrors();\n        for (int32_t i = 0; i < nerror; ++i)\n        {\n            nvonnxparser::IParserError const* error = getError(i);\n            if (error->node() != -1)\n            {\n                error_node = error->node();\n                allSupported = false;\n            }\n            // The node that we failed on is one of the input nodes (-1). Get the name of the input node\n            // that we failed on and remove all nodes that spawn out of it.\n            else\n            {\n                // Node name is extracted through error->file as all errors thrown on input nodes are wrapped\n                // around MAKE_INPUT_ERROR.\n                input_node = error->file();\n            }\n        }\n    }\n    auto* ctx = &mImporterCtx;\n    auto checkForInput = [&input_node, &ctx](::ONNX_NAMESPACE::NodeProto const& node) {\n        for (auto input : node.input())\n        {\n            if (input_node == input || ctx->loopTensors()[input_node] == input)\n            {\n                return true;\n            }\n        }\n        return false;\n    };\n\n    bool newSubGraph(true);\n    // Sort and partition supported subgraphs\n    std::vector<size_t> topological_order;\n    if (!toposort(model.graph().node(), &topological_order))\n    {\n        LOG_VERBOSE(\"Failed to sort model topologically, exiting ...\");\n        return std::make_pair<bool, SubGraphSupportVector_t>(false, {});\n    }\n\n    SubGraphSupportVector_t supportVector;\n    for (int32_t node_idx : topological_order)\n    {\n        ::ONNX_NAMESPACE::NodeProto const& node = model.graph().node(node_idx);\n        // Add the node to the subgraph if:\n        //     1. It is not directly connected to an unsupported input\n        //     2. The importer function did not throw an assertion\n        bool unsupportedInput = (input_node.empty()) ? false : checkForInput(node);\n        bool unsuccessfulParse = node_idx == error_node;\n        if (!unsupportedInput && !unsuccessfulParse)\n        {\n            if (newSubGraph)\n            {\n                // If it is the beginning of a new subGraph, we start a new vector\n                supportVector.emplace_back();\n                // Mark all new graphs as \"unknown\"\n                supportVector.back().second = false;\n                newSubGraph = false;\n            }\n            // We add the new node to the last graph\n            supportVector.back().first.emplace_back(node_idx);\n        }\n        else\n        {\n            // This is not a supported node, reset newSubGraph\n            newSubGraph = true;\n            allSupported = false;\n        }\n    }\n\n    // Only mark the subgraph as supported if there is one supported subgraph.\n    if (allSupported)\n    {\n        supportVector.back().second = true;\n    }\n    return std::make_pair(allSupported, std::move(supportVector));\n}\n\nbool ModelImporter::supportsModel(void const* serialized_onnx_model, size_t serialized_onnx_model_size,\n    SubGraphCollection_t& sub_graph_collection, char const* model_path) noexcept\n{\n    ONNXTRT_TRY\n    {\n        std::pair<bool, SubGraphSupportVector_t> result\n            = doSupportsModel(serialized_onnx_model, serialized_onnx_model_size, model_path);\n        bool supports = result.first;\n        SubGraphSupportVector_t supportVector = result.second;\n\n        sub_graph_collection.clear();\n\n        // SubGraphCollection uses size_t, while SubGraphSupportVector_t uses int64_t\n        for (const auto& pair : supportVector)\n        {\n            bool subgraphSupports = pair.second;\n\n            std::vector<int64_t> const& subgraphNodes = pair.first;\n            std::vector<size_t> subgraphNodesRet(subgraphNodes.begin(), subgraphNodes.end());\n\n            // Create a new pair and add it to vector b\n            sub_graph_collection.push_back(std::make_pair(subgraphNodesRet, subgraphSupports));\n        }\n\n        return supports;\n    }\n    ONNXTRT_CATCH_RECORD\n    return false;\n}\n\nbool ModelImporter::supportsModelV2(\n    void const* serialized_onnx_model, size_t serialized_onnx_model_size, char const* model_path) noexcept\n{\n    ONNXTRT_TRY\n    {\n        std::pair<bool, SubGraphSupportVector_t> result\n            = doSupportsModel(serialized_onnx_model, serialized_onnx_model_size, model_path);\n        bool supports = result.first;\n        SubGraphSupportVector_t supportVector = result.second;\n\n        mSubGraphSupportVector.resize(supportVector.size());\n        std::copy(supportVector.begin(), supportVector.end(), mSubGraphSupportVector.begin());\n\n        return supports;\n    }\n    ONNXTRT_CATCH_RECORD\n    return false;\n}\n\nint64_t ModelImporter::getNbSubgraphs() noexcept\n{\n    ONNXTRT_TRY\n    {\n        return mSubGraphSupportVector.size();\n    }\n    ONNXTRT_CATCH_RECORD\n    return 0;\n}\n\nbool ModelImporter::isSubgraphSupported(int64_t const index) noexcept\n{\n    ONNXTRT_TRY\n    {\n        std::ostringstream errorMessage;\n        errorMessage << \"Query index \" << index\n                     << \" exceeds subgraph support vector (size = \" << mSubGraphSupportVector.size()\n                     << \"). Have you called supports_model_v2?\";\n        ONNXTRT_CHECK(mSubGraphSupportVector.size() > static_cast<uint64_t>(index) && errorMessage.str().c_str(),\n            ErrorCode::kINVALID_VALUE);\n        return mSubGraphSupportVector[index].second;\n    }\n    ONNXTRT_CATCH_RECORD\n    return false;\n}\n\nint64_t* ModelImporter::getSubgraphNodes(int64_t const index, int64_t& subgraphLength) noexcept\n{\n    ONNXTRT_TRY\n    {\n        std::ostringstream errorMessage;\n        errorMessage << \"Query index \" << index\n                     << \" exceeds subgraph support vector (size = \" << mSubGraphSupportVector.size()\n                     << \"). Have you called supports_model_v2?\";\n        ONNXTRT_CHECK(mSubGraphSupportVector.size() > static_cast<uint64_t>(index) && errorMessage.str().c_str(),\n            ErrorCode::kINVALID_VALUE);\n        subgraphLength = mSubGraphSupportVector[index].first.size();\n        return mSubGraphSupportVector[index].first.data();\n    }\n    ONNXTRT_CATCH_RECORD\n\n    subgraphLength = 0;\n    return nullptr;\n}\n\nbool ModelImporter::supportsOperator(char const* op_name) const noexcept\n{\n    ONNXTRT_TRY\n    {\n        return _op_importers.count(op_name);\n    }\n    ONNXTRT_CATCH_RECORD\n\n    return false;\n}\n\nbool ModelImporter::parseWithWeightDescriptors(\n    void const* serialized_onnx_model, size_t serialized_onnx_model_size) noexcept\n{\n    ONNXTRT_TRY\n    {\n        mCurrentNode = -1;\n        // TODO: This function (and its overload below) could do with some cleaning,\n        //       particularly wrt error handling.\n        // Note: We store a copy of the model so that weight arrays will persist\n        mONNXModels.emplace_back();\n        ::ONNX_NAMESPACE::ModelProto& model = mONNXModels.back();\n        deserializeOnnxModel(serialized_onnx_model, serialized_onnx_model_size, &model);\n        importModel(model);\n        return true;\n    }\n    ONNXTRT_CATCH_RECORD\n\n    return false;\n}\n\nbool ModelImporter::parse(\n    void const* serialized_onnx_model, size_t serialized_onnx_model_size, const char* model_path) noexcept\n{\n    ONNXTRT_TRY\n    {\n        auto* const ctx = &mImporterCtx;\n\n        if (ctx->network()->getNbLayers() > 0)\n        {\n            LOG_ERROR(\"Parse was called with a non-empty network definition\");\n            return false;\n        }\n        if (model_path)\n        {\n            mImporterCtx.setOnnxFileLocation(model_path);\n        }\n        return this->parseWithWeightDescriptors(serialized_onnx_model, serialized_onnx_model_size);\n    }\n    ONNXTRT_CATCH_RECORD\n\n    return false;\n}\n\nvoid ModelImporter::importModel(::ONNX_NAMESPACE::ModelProto const& model)\n{\n    auto* ctx = &mImporterCtx;\n    mImporterCtx.clearOpsets();\n    // Add domain import limit for security reasons\n    int32_t const MAX_DOMAINS = 1024;\n    ONNXTRT_CHECK(model.opset_import().size() <= MAX_DOMAINS\n            && \"Model contains more than 1024 domains! Parsing will halt for security reasons.\",\n        ErrorCode::kUNSUPPORTED_GRAPH);\n    for (int32_t i = 0; i < model.opset_import().size(); ++i)\n    {\n        std::string domain = model.opset_import(i).domain();\n        int64_t version = model.opset_import(i).version();\n        // TensorRT requires an ONNX graph to be generated with at least ai.onnx version 7.\n        // ONNX spec says that the default domain is either an empty string or is \"ai.onnx\".\n        if ((domain.empty() || domain == \"ai.onnx\") && version < 7)\n        {\n            LOG_WARNING(\n                \"TensorRT supports ONNX graphs generated with at least opset 7. Models using older opsets are not \"\n                \"guaranteed to work.\");\n        }\n        mImporterCtx.addOpset(domain, version);\n    }\n    ::ONNX_NAMESPACE::GraphProto const& graph = model.graph();\n    // Create a dummy tensors so that we can reserve output names. If the output names are encountered elsewhere\n    // in the graph, the ctx will know to make the names unique.\n    for (::ONNX_NAMESPACE::ValueInfoProto const& output : graph.output())\n    {\n        mImporterCtx.registerTensor(TensorOrWeights{}, output.name());\n    }\n\n    // Import LocalFunctions\n    importLocalFunctions(&mImporterCtx, model);\n\n    // Propagate OnnxParserFlags down to the importer context.\n    mImporterCtx.setFlags(getFlags());\n\n    mCurrentNode = -1;\n    importInputs(&mImporterCtx, graph, &mImporterCtx.tensors(), mErrors);\n    parseGraph(&mImporterCtx, graph, mErrors, model.producer_name() == \"TensorRT\", &mCurrentNode);\n\n    mCurrentNode = -1;\n    // Mark outputs defined in the ONNX model (unless tensors are user-requested)\n    for (::ONNX_NAMESPACE::ValueInfoProto const& output : graph.output())\n    {\n        ONNXTRT_CHECK((mImporterCtx.tensors().count(output.name())) && \"The output tensor was not registered.\",\n            ErrorCode::kINVALID_GRAPH);\n        nvinfer1::ITensor* output_tensor_ptr\n            = &convertToTensor(mImporterCtx.tensors().at(output.name()), &mImporterCtx);\n        LOG_VERBOSE(\"Marking \" << output_tensor_ptr->getName() << \" as output: \" << output.name());\n        output_tensor_ptr->setName(output.name().c_str());\n\n        if (output_tensor_ptr->isNetworkInput())\n        {\n            // HACK WAR for TRT not allowing input == output\n            // TODO: Does this break things by changing the name of the input tensor?\n            output_tensor_ptr->setName((\"__\" + output.name()).c_str());\n            output_tensor_ptr = &identity(&mImporterCtx, output_tensor_ptr).tensor();\n            ONNXTRT_CHECK(output_tensor_ptr && \"Failed to add an Identity layer.\", ErrorCode::kUNSUPPORTED_NODE);\n            output_tensor_ptr->setName(output.name().c_str());\n        }\n\n        mImporterCtx.network()->markOutput(*output_tensor_ptr);\n        nvinfer1::DataType output_trt_dtype;\n\n        ONNXTRT_CHECK(convertDtype(output.type().tensor_type().elem_type(), &output_trt_dtype)\n                && \"Failed to convert ONNX date type to TensorRT data type.\",\n            ErrorCode::kUNSUPPORTED_NODE);\n        // For INT32 data type, output type must match tensor type\n        ONNXTRT_CHECK((output_tensor_ptr->getType() != nvinfer1::DataType::kINT32\n                          || output_trt_dtype == nvinfer1::DataType::kINT32)\n                && \"For INT32 tensors, the output type must also be INT32.\",\n            ErrorCode::kUNSUPPORTED_NODE);\n        // Note: Without this, output type is always float32\n        output_tensor_ptr->setType(output_trt_dtype);\n        if (output_trt_dtype == nvinfer1::DataType::kINT64)\n        {\n            LOG_WARNING(\"Make sure output \" << output.name() << \" has Int64 binding.\");\n        }\n    }\n\n    if (model.producer_name() == \"TensorRT\")\n    {\n        // iterate over all tensors in the network and add them to \"tensors\" map\n        StringMap<nvinfer1::ITensor*> tensors;\n        StringMap<nvinfer1::ILayer*> layers;\n        for (int32_t idx = 0; idx < mImporterCtx.network()->getNbInputs(); ++idx)\n        {\n            nvinfer1::ITensor* tensor = mImporterCtx.network()->getInput(idx);\n            if (tensor != nullptr)\n            {\n                tensors[tensor->getName()] = tensor;\n            }\n        }\n        for (int32_t idx = 0; idx < mImporterCtx.network()->getNbOutputs(); ++idx)\n        {\n            nvinfer1::ITensor* tensor = mImporterCtx.network()->getOutput(idx);\n            if (tensor != nullptr)\n            {\n                tensors[tensor->getName()] = tensor;\n            }\n        }\n        for (int32_t layerIdx = 0; layerIdx < mImporterCtx.network()->getNbLayers(); ++layerIdx)\n        {\n            nvinfer1::ILayer* layer = mImporterCtx.network()->getLayer(layerIdx);\n            for (int32_t idx = 0; idx < layer->getNbInputs(); ++idx)\n            {\n                nvinfer1::ITensor* tensor = layer->getInput(idx);\n                if (tensor != nullptr)\n                {\n                    tensors[tensor->getName()] = tensor;\n                }\n            }\n            for (int32_t idx = 0; idx < layer->getNbOutputs(); ++idx)\n            {\n                nvinfer1::ITensor* tensor = layer->getOutput(idx);\n                if (tensor != nullptr)\n                {\n                    tensors[tensor->getName()] = tensor;\n                }\n            }\n            layers[layer->getName()] = layer;\n        }\n\n        // Set locations for all tensors\n        for (auto const& tensor : ctx->tensorLocations())\n        {\n            ONNXTRT_CHECK((tensors.count(tensor.first) > 0) && \"The tensor does not have an assigned location.\",\n                nvonnxparser::ErrorCode::kINVALID_GRAPH);\n            tensors.at(tensor.first)->setLocation(tensor.second);\n        }\n        // Set dynamic range for all tensors\n        for (auto const& tensor : ctx->tensorRangeMins())\n        {\n            // if there's a min range, there must be a max range as well\n            ONNXTRT_CHECK((tensors.count(tensor.first) > 0) && \"The tensor does not have an assigned location.\",\n                nvonnxparser::ErrorCode::kINVALID_GRAPH);\n            if (!std::isnan(tensor.second))\n            {\n                tensors.at(tensor.first)->setDynamicRange(tensor.second, ctx->tensorRangeMaxes().at(tensor.first));\n            }\n        }\n        // Avoid setting layer precision if graph is strongly typed.\n        if (!ctx->network()->getFlag(nvinfer1::NetworkDefinitionCreationFlag::kSTRONGLY_TYPED))\n        {\n            // Set precisions for all layers.\n            for (auto const& layer : ctx->layerPrecisions())\n            {\n                ONNXTRT_CHECK((layers.count(layer.first) > 0) && \"The layer does not have an assigned precision.\",\n                    nvonnxparser::ErrorCode::kINVALID_GRAPH);\n                layers.at(layer.first)->setPrecision(layer.second);\n            }\n        }\n    }\n\n    // Regenerate the plugin library list\n    mPluginLibraryList = ctx->getUsedVCPluginLibraries();\n    mPluginLibraryListCStr.clear();\n    mPluginLibraryListCStr.reserve(mPluginLibraryList.size());\n    for (auto const& s : mPluginLibraryList)\n    {\n        mPluginLibraryListCStr.push_back(s.c_str());\n    }\n}\n\nbool ModelImporter::parseFromFile(char const* onnxModelFile, int32_t verbosity) noexcept\n{\n    ONNXTRT_TRY\n    {\n        auto* ctx = &mImporterCtx;\n\n        // Define S_ISREG macro for Windows\n#if !defined(S_ISREG)\n#define S_ISREG(mode) (((mode) & S_IFMT) == S_IFREG)\n#endif\n\n        struct stat sb;\n        if (stat(onnxModelFile, &sb) == 0 && !S_ISREG(sb.st_mode))\n        {\n            LOG_ERROR(\"Input is not a regular file: \" << onnxModelFile);\n            return false;\n        }\n\n        GOOGLE_PROTOBUF_VERIFY_VERSION;\n\n        // Own the ONNX model for weights to persist.\n        mONNXModels.emplace_back();\n        ::ONNX_NAMESPACE::ModelProto& onnxModel = mONNXModels.back();\n        bool const fileLoadSuccess = ParseFromFileAsBinary(&onnxModel, onnxModelFile);\n        if (!fileLoadSuccess)\n        {\n            LOG_ERROR(\"Failed to parse ONNX model from file: \" << onnxModelFile << \"!\");\n            return false;\n        }\n\n        // Keep track of the absolute path to the ONNX file.\n        mImporterCtx.setOnnxFileLocation(onnxModelFile);\n\n        int64_t const opset_version = (onnxModel.opset_import().size() ? onnxModel.opset_import(0).version() : 0);\n        LOG_INFO(\"----------------------------------------------------------------\");\n        LOG_INFO(\"Input filename:   \" << onnxModelFile);\n        LOG_INFO(\"ONNX IR version:  \" << onnxIRVersionAsString(onnxModel.ir_version()));\n        LOG_INFO(\"Opset version:    \" << opset_version);\n        LOG_INFO(\"Producer name:    \" << onnxModel.producer_name());\n        LOG_INFO(\"Producer version: \" << onnxModel.producer_version());\n        LOG_INFO(\"Domain:           \" << onnxModel.domain());\n        LOG_INFO(\"Model version:    \" << onnxModel.model_version());\n        LOG_INFO(\"Doc string:       \" << onnxModel.doc_string());\n        LOG_INFO(\"----------------------------------------------------------------\");\n\n        // Set currentNode count to -1\n        mCurrentNode = -1;\n\n        // Prevent failure of importModel from early-exiting\n        try\n        {\n            this->importModel(onnxModel);\n        }\n        catch (OnnxTrtException& e)\n        {\n            mErrors.push_back(e.getStatus());\n        }\n        catch (std::exception& e)\n        {\n            mErrors.push_back(MAKE_ERROR(e.what(), ErrorCode::kINTERNAL_ERROR));\n        }\n\n        int32_t const numErrors = getNbErrors();\n        for (int32_t i = 0; i < numErrors; ++i)\n        {\n            nvonnxparser::IParserError const* error = getError(i);\n            if (error->node() != -1)\n            {\n                ::ONNX_NAMESPACE::NodeProto const& node = onnxModel.graph().node(error->node());\n                LOG_ERROR(\"While parsing node number \" << error->node() << \" [\" << node.op_type() << \" -> \\\"\"\n                                                       << node.output(0) << \"\\\"\"\n                                                       << \"]:\");\n                LOG_ERROR(\"--- Begin node ---\" << \"\\n\" << node);\n                LOG_ERROR(\"--- End node ---\");\n            }\n            LOG_ERROR(\"ERROR: \" << error->file() << \":\" << error->line() << \" In function \" << error->func() << \":\\n\"\n                                << \"[\" << static_cast<int>(error->code()) << \"] \" << error->desc());\n        }\n        return numErrors == 0;\n    }\n    ONNXTRT_CATCH_RECORD\n    return false;\n}\n\nchar const* const* ModelImporter::getUsedVCPluginLibraries(int64_t& nbPluginLibs) const noexcept\n{\n    nbPluginLibs = mPluginLibraryListCStr.size();\n    return (nbPluginLibs > 0) ? mPluginLibraryListCStr.data() : nullptr;\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "ModelImporter.hpp",
          "type": "blob",
          "size": 5.06640625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include \"ImporterContext.hpp\"\n#include \"NvInferPlugin.h\"\n#include \"NvOnnxParser.h\"\n#include \"errorHelpers.hpp\"\n#include \"onnxOpCheckers.hpp\"\n#include \"onnxOpImporters.hpp\"\n#include <stdexcept>\n\nnamespace onnx2trt\n{\n\nvoid parseNode(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx,\n    bool deserializingINetwork = false);\n\nvoid parseNodeStaticCheck(\n    ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, std::vector<Status>& errors, size_t const nodeIndex);\n\nvoid parseGraph(ImporterContext* ctx, ::ONNX_NAMESPACE::GraphProto const& graph, std::vector<Status>& errors,\n    bool deserializingINetwork = false, int32_t* currentNode = nullptr);\n\nclass ModelImporter : public nvonnxparser::IParser\n{\n    using SubGraphSupport_t = std::pair<std::vector<int64_t>, bool>;\n    using SubGraphSupportVector_t = std::vector<SubGraphSupport_t>;\n\nprotected:\n    StringMap<NodeImporter> _op_importers;\n    virtual void importModel(::ONNX_NAMESPACE::ModelProto const& model);\n\nprivate:\n    ImporterContext mImporterCtx;\n    std::vector<std::string> mPluginLibraryList; // Array of strings containing plugin libs\n    std::vector<char const*>\n        mPluginLibraryListCStr; // Array of C-strings corresponding to the strings in mPluginLibraryList\n    std::list<::ONNX_NAMESPACE::ModelProto> mONNXModels; // Needed for ownership of weights\n    SubGraphSupportVector_t mSubGraphSupportVector;\n    int mCurrentNode;\n    mutable std::vector<Status> mErrors; // Marked as mutable so that errors could be reported from const functions\n    nvonnxparser::OnnxParserFlags mOnnxParserFlags{\n        1U << static_cast<uint32_t>(\n            nvonnxparser::OnnxParserFlag::kNATIVE_INSTANCENORM)}; // kNATIVE_INSTANCENORM is ON by default.\n    std::pair<bool, SubGraphSupportVector_t> doSupportsModel(\n        void const* serialized_onnx_model, size_t serialized_onnx_model_size, char const* model_path = nullptr);\n\npublic:\n    ModelImporter(nvinfer1::INetworkDefinition* network, nvinfer1::ILogger* logger) noexcept\n        : _op_importers(getBuiltinOpImporterMap())\n        , mImporterCtx(network, logger)\n    {\n    }\n    bool parseWithWeightDescriptors(\n        void const* serialized_onnx_model, size_t serialized_onnx_model_size) noexcept override;\n    bool parse(void const* serialized_onnx_model, size_t serialized_onnx_model_size,\n        const char* model_path = nullptr) noexcept override;\n\n    bool supportsModel(void const* serialized_onnx_model, size_t serialized_onnx_model_size,\n        SubGraphCollection_t& sub_graph_collection, const char* model_path = nullptr) noexcept override;\n    bool supportsModelV2(void const* serialized_onnx_model, size_t serialized_onnx_model_size,\n        char const* model_path = nullptr) noexcept override;\n\n    int64_t getNbSubgraphs() noexcept override;\n    bool isSubgraphSupported(int64_t const index) noexcept override;\n    int64_t* getSubgraphNodes(int64_t const index, int64_t& subgraphLength) noexcept override;\n\n    bool supportsOperator(const char* op_name) const noexcept override;\n\n    void setFlags(nvonnxparser::OnnxParserFlags onnxParserFlags) noexcept override\n    {\n        mOnnxParserFlags = onnxParserFlags;\n    }\n    nvonnxparser::OnnxParserFlags getFlags() const noexcept override\n    {\n        return mOnnxParserFlags;\n    }\n\n    void clearFlag(nvonnxparser::OnnxParserFlag onnxParserFlag) noexcept override\n    {\n        ONNXTRT_TRY\n        {\n            mOnnxParserFlags &= ~(1U << static_cast<uint32_t>(onnxParserFlag));\n        }\n        ONNXTRT_CATCH_RECORD\n    }\n\n    void setFlag(nvonnxparser::OnnxParserFlag onnxParserFlag) noexcept override\n    {\n        ONNXTRT_TRY\n        {\n            mOnnxParserFlags |= 1U << static_cast<uint32_t>(onnxParserFlag);\n        }\n        ONNXTRT_CATCH_RECORD\n    }\n\n    bool getFlag(nvonnxparser::OnnxParserFlag onnxParserFlag) const noexcept override\n    {\n        ONNXTRT_TRY\n        {\n            auto flag = 1U << static_cast<uint32_t>(onnxParserFlag);\n            return static_cast<bool>(mOnnxParserFlags & flag);\n        }\n        ONNXTRT_CATCH_RECORD\n        return false;\n    }\n\n    int32_t getNbErrors() const noexcept override\n    {\n        return mErrors.size();\n    }\n    nvonnxparser::IParserError const* getError(int32_t index) const noexcept override\n    {\n        ONNXTRT_TRY\n        {\n            return &mErrors.at(index);\n        }\n        ONNXTRT_CATCH_RECORD\n        return nullptr;\n    }\n    void clearErrors() noexcept override\n    {\n        mErrors.clear();\n    }\n\n    nvinfer1::ITensor const* getLayerOutputTensor(char const* name, int64_t i) noexcept override\n    {\n        ONNXTRT_TRY\n        {\n            if (!name)\n            {\n                throw std::invalid_argument(\"name is a nullptr\");\n            }\n            return mImporterCtx.findLayerOutputTensor(name, i);\n        }\n        ONNXTRT_CATCH_RECORD\n        return nullptr;\n    }\n\n    bool parseFromFile(char const* onnxModelFile, int32_t verbosity) noexcept override;\n\n    virtual char const* const* getUsedVCPluginLibraries(int64_t& nbPluginLibs) const noexcept override;\n};\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "ModelRefitter.cpp",
          "type": "blob",
          "size": 15.849609375,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"ModelRefitter.hpp\"\n#include \"ShapedWeights.hpp\"\n#include \"onnxProtoUtils.hpp\"\n#include \"toposort.hpp\"\n\n#include <google/protobuf/io/coded_stream.h>\n#include <google/protobuf/io/zero_copy_stream_impl.h>\n#include <google/protobuf/text_format.h>\n\n#include <algorithm>\n#include <sys/stat.h>\n#include <unordered_map>\n#include <vector>\n\nnamespace onnx2trt\n{\nnamespace\n{\nvoid deserializeOnnxModelFile(char const* onnxModelFile, ::ONNX_NAMESPACE::ModelProto& onnx_model)\n{\n    // Define S_ISREG macro for Windows\n#if !defined(S_ISREG)\n#define S_ISREG(mode) (((mode) & S_IFMT) == S_IFREG)\n#endif\n\n    struct stat sb;\n    ONNXTRT_CHECK(!(stat(onnxModelFile, &sb) == 0 && !S_ISREG(sb.st_mode))\n            && \"Failed to parse the ONNX model; input is not a regular file.\",\n        ErrorCode::kMODEL_DESERIALIZE_FAILED);\n\n    GOOGLE_PROTOBUF_VERIFY_VERSION;\n\n    bool const fileLoadSuccess = ParseFromFileAsBinary(&onnx_model, onnxModelFile);\n    ONNXTRT_CHECK(fileLoadSuccess && \"Failed to parse the ONNX model!\", ErrorCode::kMODEL_DESERIALIZE_FAILED);\n}\n} // anonymous namespace\n\nstd::unordered_set<std::string> ModelRefitter::getRefittableWeights()\n{\n    int32_t numWeights = mRefitter->getAllWeights(0, nullptr);\n    std::vector<char const*> weightNames{static_cast<size_t>(numWeights)};\n    mRefitter->getAllWeights(numWeights, weightNames.data());\n    return std::unordered_set<std::string>{weightNames.begin(), weightNames.end()};\n}\n\ntemplate <typename T, typename TConvertFunc>\nsize_t ModelRefitter::batchnormWeightRefitter(\n    ::ONNX_NAMESPACE::NodeProto const& node, std::vector<ShapedWeights>& inputs, TConvertFunc&& f)\n{\n    auto const& scale = inputs.at(0);\n    auto const& bias = inputs.at(1);\n    auto const& mean = inputs.at(2);\n    auto const& variance = inputs.at(3);\n\n    T const* const scaleValues = f(scale);\n    T const* const biasValues = f(bias);\n    T const* const meanValues = f(mean);\n    T const* const varianceValues = f(variance);\n\n    T eps = static_cast<T>(1e-5f);\n\n    for (auto const& attr : node.attribute())\n    {\n        if (attr.name() == \"epsilon\")\n        {\n            eps = static_cast<T>(attr.f());\n            break;\n        }\n    }\n\n    // Fold the weights together into a single bias and scale\n    int32_t const nbChannels = scale.shape.d[0];\n    ShapedWeights::DataType weightType = typeid(T).hash_code() == typeid(BFloat16).hash_code()\n        ? ::ONNX_NAMESPACE::TensorProto::BFLOAT16\n        : (typeid(T).hash_code() == typeid(half_float::half).hash_code() ? ::ONNX_NAMESPACE::TensorProto::FLOAT16\n                                                                         : ::ONNX_NAMESPACE::TensorProto::FLOAT);\n\n    ShapedWeights combinedScale = mWeightsContext.createNamedTempWeights(\n        weightType, scale.shape, mBatchNormWeightNames, mBatchNormWeightSuffixCounter, /*batchNormNode=*/true);\n    ShapedWeights combinedBias = mWeightsContext.createNamedTempWeights(\n        weightType, bias.shape, mBatchNormWeightNames, mBatchNormWeightSuffixCounter, /*batchNormNode=*/true);\n\n    // Validate that all the weights have the same amount of values\n    bool allSame = scale.count() == bias.count() && mean.count() == scale.count() && variance.count() == scale.count()\n        && combinedScale.count() == scale.count() && combinedBias.count() == scale.count();\n    ONNXTRT_CHECK(allSame && \"Inputs to BatchNormalization must have the same shape!\", ErrorCode::kREFIT_FAILED);\n\n    for (int32_t i = 0; i < nbChannels; ++i)\n    {\n        combinedScale.at<T>(i) = scaleValues[i] / sqrtf(varianceValues[i] + eps);\n        combinedBias.at<T>(i) = biasValues[i] - meanValues[i] * combinedScale.at<T>(i);\n    }\n    size_t successfullyRefittedWeights = 0;\n    if (refittableWeights.count(combinedScale.name))\n    {\n        refittableWeights.erase(combinedScale.name);\n        ONNXTRT_CHECK(\n            mRefitter->setNamedWeights(combinedScale.name, std::move(combinedScale)) && \"Failed to set named weights\",\n            ErrorCode::kREFIT_FAILED);\n        ++successfullyRefittedWeights;\n    }\n    if (refittableWeights.count(combinedBias.name))\n    {\n        refittableWeights.erase(combinedBias.name);\n        ONNXTRT_CHECK(\n            mRefitter->setNamedWeights(combinedBias.name, std::move(combinedBias)) && \"Failed to set named weights\",\n            ErrorCode::kREFIT_FAILED);\n        ++successfullyRefittedWeights;\n    }\n    return successfullyRefittedWeights;\n}\n\n//! Functor for extracting weights from ShapedWeights via cheap pointer cast to T*.\ntemplate <typename T>\nclass QuickCast\n{\npublic:\n    T const* operator()(ShapedWeights const& w) const\n    {\n        return static_cast<T const*>(w.values);\n    };\n};\n\nvoid ModelRefitter::refitOnnxWeights(::ONNX_NAMESPACE::ModelProto const& onnx_model)\n{\n    nestedDepth = 0;\n    successfullyRefittedWeights = 0;\n    size_t const numberOfWeightsToRefit = refittableWeights.size();\n    refitOnnxGraph(onnx_model.graph());\n    ONNXTRT_CHECK(successfullyRefittedWeights == numberOfWeightsToRefit && \"Failed to refit all the weights.\",\n        ErrorCode::kREFIT_FAILED);\n}\n\nvoid ModelRefitter::refitOnnxGraph(::ONNX_NAMESPACE::GraphProto const& graph)\n{\n    for (::ONNX_NAMESPACE::TensorProto const& initializer : graph.initializer())\n    {\n        if (!refittableWeights.count(initializer.name()))\n        {\n            continue;\n        }\n        // Remove the weight name from the set as some initializers\n        // might have the same name across different nested constructs (e.g. IF nodes);\n        // the assumption is that those weights would have the same value\n        refittableWeights.erase(initializer.name());\n        if (refittedWeights.count(initializer.name()))\n        {\n            LOG_REFITTER_WARNING(\"Duplicate initializer name (\"\n                << initializer.name() << \") was found when processing the graph (\" << graph.name()\n                << \"). The refit process would only work properly if both initializers have the same values.\");\n        }\n        else\n        {\n            refittedWeights.insert(initializer.name());\n        }\n        ShapedWeights weights;\n        ONNXTRT_CHECK(mWeightsContext.convertOnnxWeights(initializer, &weights, /*ownAllWeights=*/true)\n                && \"Failed to import initializer.\",\n            ErrorCode::kUNSUPPORTED_NODE);\n        ONNXTRT_CHECK(\n            mRefitter->setNamedWeights(initializer.name().c_str(), std::move(weights)) && \"Failed to set named weights\",\n            ErrorCode::kREFIT_FAILED);\n        ++successfullyRefittedWeights;\n    }\n\n    std::vector<size_t> topoOrder;\n    ONNXTRT_CHECK(\n        toposort(graph.node(), &topoOrder) && \"Failed to sort the model topologically.\", ErrorCode::kINVALID_GRAPH);\n\n    for (auto const& nodeIdx : topoOrder)\n    {\n        ::ONNX_NAMESPACE::NodeProto const& node = graph.node(nodeIdx);\n        refitOnnxNode(node, graph);\n    }\n}\n\nvoid ModelRefitter::refitOnnxNode(::ONNX_NAMESPACE::NodeProto const& node, ::ONNX_NAMESPACE::GraphProto const& graph)\n{\n    // For nodes that contain subgraphs (Ifs, Loops, Scans),\n    // ensure that the recursion depth is limited to a set amount.\n    ++nestedDepth;\n    static size_t const MAX_NESTED_SUBGRAPHS = 24;\n    ONNXTRT_CHECK((nestedDepth <= MAX_NESTED_SUBGRAPHS)\n            && \"ONNX graph contains nested structures that exceed the maximum allowed by TensorRT!\",\n        ErrorCode::kUNSUPPORTED_GRAPH);\n\n    if (node.op_type() == \"Constant\")\n    {\n        refitOnnxConstantNode(node, graph.name());\n    }\n    else if (node.op_type() == \"BatchNormalization\")\n    {\n        refitOnnxBatchNormNode(node, graph);\n    }\n    else if (node.op_type() == \"If\")\n    {\n        refitOnnxIfNode(node);\n    }\n    else if (node.op_type() == \"Loop\")\n    {\n        refitOnnxLoopNode(node);\n    }\n    else if (node.op_type() == \"Scan\")\n    {\n        refitOnnxScanNode(node);\n    }\n    --nestedDepth;\n}\n\nvoid ModelRefitter::refitOnnxConstantNode(::ONNX_NAMESPACE::NodeProto const& node, std::string const& graphName)\n{\n    if (!refittableWeights.count(node.output(0)))\n    {\n        return;\n    }\n    refittableWeights.erase(node.output(0));\n    if (refittedWeights.count(node.output(0)))\n    {\n        LOG_REFITTER_WARNING(\"Duplicate weight name name (\"\n            << node.output(0) << \") was found when processing the graph (\" << graphName\n            << \"). The refit process would only work properly if both weights have the same values.\");\n    }\n    else\n    {\n        refittedWeights.insert(node.output(0));\n    }\n    ShapedWeights weights;\n    ::ONNX_NAMESPACE::AttributeProto const& nodeAttribute = node.attribute(0);\n    if (nodeAttribute.name() == \"value_float\")\n    {\n        weights = mWeightsContext.createTempWeights(::ONNX_NAMESPACE::TensorProto::FLOAT, {0, {}});\n        float value = nodeAttribute.f();\n        ONNXTRT_CHECK(weights.count() == 1 && \"Failed to import Constant node.\", ErrorCode::kUNSUPPORTED_NODE);\n        std::memcpy(weights.values, &value, sizeof(float));\n    }\n    else if (nodeAttribute.name() == \"value_floats\")\n    {\n        std::vector<float> values{nodeAttribute.floats().begin(), nodeAttribute.floats().end()};\n        int64_t valueSize = values.size();\n        weights = mWeightsContext.createTempWeights(::ONNX_NAMESPACE::TensorProto::FLOAT, {1, {valueSize}});\n        ONNXTRT_CHECK(\n            weights.count() == values.size() && \"Failed to import Constant node.\", ErrorCode::kUNSUPPORTED_NODE);\n        std::memcpy(weights.values, values.data(), weights.count() * sizeof(float));\n    }\n    else if (nodeAttribute.name() == \"value_int\")\n    {\n        weights = mWeightsContext.createTempWeights(::ONNX_NAMESPACE::TensorProto::INT64, {0, {}});\n        int64_t value = nodeAttribute.i();\n        ONNXTRT_CHECK(weights.count() == 1 && \"Failed to import Constant node.\", ErrorCode::kUNSUPPORTED_NODE);\n        std::memcpy(weights.values, &value, sizeof(int64_t));\n    }\n    else if (nodeAttribute.name() == \"value_ints\")\n    {\n        std::vector<int64_t> values{nodeAttribute.ints().begin(), nodeAttribute.ints().end()};\n        int64_t valueSize = values.size();\n        weights = mWeightsContext.createTempWeights(::ONNX_NAMESPACE::TensorProto::INT64, {1, {valueSize}});\n        ONNXTRT_CHECK(\n            weights.count() == values.size() && \"Failed to import Constant node.\", ErrorCode::kUNSUPPORTED_NODE);\n        std::memcpy(weights.values, values.data(), weights.count() * sizeof(int64_t));\n    }\n    else\n    {\n        ::ONNX_NAMESPACE::TensorProto const& onnx_weights_tensor = nodeAttribute.t();\n        ONNXTRT_CHECK(\n            mWeightsContext.convertOnnxWeights(onnx_weights_tensor, &weights) && \"Failed to import Constant node.\",\n            ErrorCode::kUNSUPPORTED_NODE);\n    }\n    ONNXTRT_CHECK(\n        mRefitter->setNamedWeights(node.output(0).c_str(), std::move(weights)) && \"Failed to set named weights\",\n        ErrorCode::kREFIT_FAILED);\n    ++successfullyRefittedWeights;\n}\n\nvoid ModelRefitter::refitOnnxBatchNormNode(\n    ::ONNX_NAMESPACE::NodeProto const& node, ::ONNX_NAMESPACE::GraphProto const& graph)\n{\n    ONNXTRT_CHECK(\n        node.input().size() == 5 && \"BatchNorm node does not have five required inputs.\", ErrorCode::kINVALID_NODE);\n    std::vector<ShapedWeights> batchNormInputs;\n    // The following looping construct is due to the fact that some tensors\n    // might be shared among the BatchNorm's inputs\n    std::vector<std::string> const inputNames(node.input().begin() + 1, node.input().end());\n    for (size_t inputIdx = 0; inputIdx < inputNames.size(); ++inputIdx)\n    {\n        for (::ONNX_NAMESPACE::TensorProto const& initializer : graph.initializer())\n        {\n            if (inputNames.at(inputIdx) == initializer.name())\n            {\n                ShapedWeights weights;\n                ONNXTRT_CHECK(\n                    mWeightsContext.convertOnnxWeights(initializer, &weights) && \"Failed to import initializer.\",\n                    ErrorCode::kUNSUPPORTED_NODE);\n                weights.name = initializer.name().c_str();\n                batchNormInputs.push_back(std::move(weights));\n                break;\n            }\n        }\n    }\n\n    // If some of the inputs to the BN node were not actual initializers,\n    // the weight folding logic from Parser is no longer applicable and\n    // we must have already refitted the weights directly in refitOnnxGraph()\n    if (batchNormInputs.size() < 4)\n    {\n        return;\n    }\n    size_t batchnormRefittedWeights{0};\n    auto const scaleType = batchNormInputs.at(0).type;\n    bool const typesEqual = scaleType == batchNormInputs.at(1).type && scaleType == batchNormInputs.at(2).type\n        && scaleType == batchNormInputs.at(3).type;\n    if (typesEqual && scaleType == ::ONNX_NAMESPACE::TensorProto::FLOAT16)\n    {\n        batchnormRefittedWeights\n            = batchnormWeightRefitter<half_float::half>(node, batchNormInputs, QuickCast<half_float::half>());\n    }\n    else if (typesEqual && scaleType == ::ONNX_NAMESPACE::TensorProto::BFLOAT16)\n    {\n        batchnormRefittedWeights = batchnormWeightRefitter<BFloat16>(node, batchNormInputs, QuickCast<BFloat16>());\n    }\n    else\n    {\n        // Do calculations in FP32, possibly promoting/demoting arithmetic types of some operands.\n        batchnormRefittedWeights = batchnormWeightRefitter<float>(\n            node, batchNormInputs, [this](ShapedWeights const& w) { return mWeightsContext.getFP32Values(w); });\n    }\n    successfullyRefittedWeights += batchnormRefittedWeights;\n}\n\nvoid ModelRefitter::refitOnnxIfNode(::ONNX_NAMESPACE::NodeProto const& node)\n{\n    size_t thenGraphOutputSize{};\n    size_t elseGraphOutputSize{};\n    for (auto const& attr : node.attribute())\n    {\n        if (attr.name() == \"then_branch\")\n        {\n            ::ONNX_NAMESPACE::GraphProto const& thenGraph = static_cast<::ONNX_NAMESPACE::GraphProto const&>(attr.g());\n            refitOnnxGraph(thenGraph);\n            thenGraphOutputSize = thenGraph.output_size();\n        }\n        else if (attr.name() == \"else_branch\")\n        {\n            ::ONNX_NAMESPACE::GraphProto const& elseGraph = static_cast<::ONNX_NAMESPACE::GraphProto const&>(attr.g());\n            refitOnnxGraph(elseGraph);\n            elseGraphOutputSize = elseGraph.output_size();\n        }\n    }\n\n    // Number of outputs are the same between the two branches.\n    ONNXTRT_CHECK(thenGraphOutputSize == elseGraphOutputSize\n            && \"then/else subgraphs within the IF node should have the same number of outputs\",\n        ErrorCode::kREFIT_FAILED);\n}\n\nvoid ModelRefitter::refitOnnxLoopNode(::ONNX_NAMESPACE::NodeProto const& node)\n{\n    ::ONNX_NAMESPACE::GraphProto const& body = static_cast<::ONNX_NAMESPACE::GraphProto const&>(node.attribute(0).g());\n    refitOnnxGraph(body);\n}\n\nvoid ModelRefitter::refitOnnxScanNode(::ONNX_NAMESPACE::NodeProto const& node)\n{\n    for (auto const& attr : node.attribute())\n    {\n        if (attr.name() == \"body\")\n        {\n            ::ONNX_NAMESPACE::GraphProto const& body = static_cast<::ONNX_NAMESPACE::GraphProto const&>(attr.g());\n            refitOnnxGraph(body);\n            break;\n        }\n    }\n}\n\nbool ModelRefitter::refitFromBytes(\n    void const* serializedOnnxModel, size_t serializedOnnxModelSize, char const* modelPath) noexcept\n{\n    ONNXTRT_TRY\n    {\n        if (modelPath)\n        {\n            // Keep track of the absolute path to the ONNX file.\n            mWeightsContext.setOnnxFileLocation(modelPath);\n        }\n\n        deserializeOnnxModel(serializedOnnxModel, serializedOnnxModelSize, &onnx_model);\n\n        refittableWeights = getRefittableWeights();\n        refitOnnxWeights(onnx_model);\n        return true;\n    }\n    ONNXTRT_CATCH_LOG(mLogger)\n    return false;\n}\n\nbool ModelRefitter::refitFromFile(char const* onnxModelFile) noexcept\n{\n    ONNXTRT_TRY\n    {\n        // Keep track of the absolute path to the ONNX file.\n        mWeightsContext.setOnnxFileLocation(onnxModelFile);\n\n        deserializeOnnxModelFile(onnxModelFile, onnx_model);\n        refittableWeights = getRefittableWeights();\n        if (!refittableWeights.empty())\n        {\n            refitOnnxWeights(onnx_model);\n        }\n        return true;\n    }\n    ONNXTRT_CATCH_LOG(mLogger)\n\n    return false;\n}\n} // namespace onnx2trt\n"
        },
        {
          "name": "ModelRefitter.hpp",
          "type": "blob",
          "size": 4.2568359375,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include \"NvInferRuntime.h\"\n#include \"Status.hpp\"\n#include \"WeightsContext.hpp\"\n#include \"errorHelpers.hpp\"\n#include <onnx/onnx_pb.h>\n#include <string>\n#include <unordered_set>\n#include <vector>\n\n// Logging macros\n#define LOG_REFITTER(msg, severity)                                                                                    \\\n    do                                                                                                                 \\\n    {                                                                                                                  \\\n        std::ostringstream ss{};                                                                                       \\\n        if (severity <= nvinfer1::ILogger::Severity::kWARNING)                                                         \\\n            ss << __FILENAME__ << \":\" << __LINE__ << \": \";                                                             \\\n        ss << msg;                                                                                                     \\\n        mLogger->log(severity, ss.str().c_str());                                                                      \\\n    } while (0)\n\n#define LOG_REFITTER_WARNING(msg) LOG_REFITTER(msg, nvinfer1::ILogger::Severity::kWARNING)\n\nnamespace onnx2trt\n{\nclass ModelRefitter : public nvonnxparser::IParserRefitter\n{\nprivate:\n    nvinfer1::IRefitter* mRefitter;\n    nvinfer1::ILogger* mLogger;\n\n    //! WeightsContext object to hold ownership of ONNX weights and any temporary weights created by the refitter.\n    WeightsContext mWeightsContext;\n\n    //! ONNX ModelProto object to hold ownership of ONNX weights whenever a data type conversion is not needed.\n    ::ONNX_NAMESPACE::ModelProto onnx_model;\n\n    //! Counter to limit the recursion depth to a set amount for nodes containing subgraphs.\n    size_t nestedDepth{0};\n\n    //! Set to keep track of how many times a batch norm weight name shows up, to avoid duplicate naming in TRT.\n    std::set<std::string> mBatchNormWeightNames;\n    //! An increasing suffix counter used to uniquify batch norm weight names.\n    int64_t mBatchNormWeightSuffixCounter{0};\n\n    size_t successfullyRefittedWeights{};\n    std::unordered_set<std::string> refittableWeights;\n    std::unordered_set<std::string> refittedWeights;\n\n    mutable std::vector<Status> mErrors;\n\n    std::unordered_set<std::string> getRefittableWeights();\n\n    //! T is the working type.\n    //! TConvertFunc is a functor for converting ShapedWeights to an array of type T.\n    //! It should return a T*.\n    template <typename T, typename TConvertFunc>\n    size_t batchnormWeightRefitter(\n        ::ONNX_NAMESPACE::NodeProto const& node, std::vector<ShapedWeights>& inputs, TConvertFunc&& f);\n\n    void refitOnnxWeights(::ONNX_NAMESPACE::ModelProto const& onnx_model);\n    void refitOnnxGraph(::ONNX_NAMESPACE::GraphProto const& graph);\n    void refitOnnxNode(::ONNX_NAMESPACE::NodeProto const& node, ::ONNX_NAMESPACE::GraphProto const& graph);\n    void refitOnnxConstantNode(::ONNX_NAMESPACE::NodeProto const& node, std::string const& graphName);\n    void refitOnnxBatchNormNode(::ONNX_NAMESPACE::NodeProto const& node, ::ONNX_NAMESPACE::GraphProto const& graph);\n    void refitOnnxIfNode(::ONNX_NAMESPACE::NodeProto const& node);\n    void refitOnnxLoopNode(::ONNX_NAMESPACE::NodeProto const& node);\n    void refitOnnxScanNode(::ONNX_NAMESPACE::NodeProto const& node);\n\npublic:\n    ModelRefitter(nvinfer1::IRefitter* refitter, nvinfer1::ILogger* logger)\n        : mRefitter{refitter}\n        , mLogger{logger}\n        , mWeightsContext{WeightsContext{logger}}\n    {\n    }\n\n    bool refitFromBytes(void const* serializedOnnxModel, size_t serializedOnnxModelSize,\n        char const* modelPath = nullptr) noexcept override;\n    bool refitFromFile(char const* onnxModelFile) noexcept override;\n\n    int32_t getNbErrors() const noexcept override\n    {\n        return mErrors.size();\n    }\n\n    nvonnxparser::IParserError const* getError(int32_t index) const noexcept override\n    {\n        ONNXTRT_TRY\n        {\n            return &mErrors.at(index);\n        }\n        ONNXTRT_CATCH_LOG(mLogger)\n        return nullptr;\n    }\n\n    void clearErrors() noexcept override\n    {\n        mErrors.clear();\n    }\n};\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "NvOnnxParser.cpp",
          "type": "blob",
          "size": 0.8212890625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"NvOnnxParser.h\"\n#include \"ModelImporter.hpp\"\n#include \"ModelRefitter.hpp\"\n#include \"NvInferRuntime.h\"\n\nextern \"C\" void* createNvOnnxParser_INTERNAL(void* network_, void* logger_, int version) noexcept\n{\n    auto network = static_cast<nvinfer1::INetworkDefinition*>(network_);\n    auto logger = static_cast<nvinfer1::ILogger*>(logger_);\n    return new onnx2trt::ModelImporter(network, logger);\n}\n\nextern \"C\" void* createNvOnnxParserRefitter_INTERNAL(void* refitter_, void* logger_, int32_t version) noexcept\n{\n    auto refitter = static_cast<nvinfer1::IRefitter*>(refitter_);\n    auto logger = static_cast<nvinfer1::ILogger*>(logger_);\n    return new onnx2trt::ModelRefitter(refitter, logger);\n}\n\nextern \"C\" int getNvOnnxParserVersion() noexcept\n{\n    return NV_ONNX_PARSER_VERSION;\n}\n"
        },
        {
          "name": "NvOnnxParser.h",
          "type": "blob",
          "size": 17.84375,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#ifndef NV_ONNX_PARSER_H\n#define NV_ONNX_PARSER_H\n\n#include \"NvInfer.h\"\n#include <stddef.h>\n#include <string>\n#include <vector>\n\n//!\n//! \\file NvOnnxParser.h\n//!\n//! This is the API for the ONNX Parser\n//!\n\n#define NV_ONNX_PARSER_MAJOR 0\n#define NV_ONNX_PARSER_MINOR 1\n#define NV_ONNX_PARSER_PATCH 0\n\nstatic constexpr int32_t NV_ONNX_PARSER_VERSION\n    = ((NV_ONNX_PARSER_MAJOR * 10000) + (NV_ONNX_PARSER_MINOR * 100) + NV_ONNX_PARSER_PATCH);\n\n//!\n//! \\typedef SubGraph_t\n//!\n//! \\brief The data structure containing the parsing capability of\n//! a set of nodes in an ONNX graph.\n//!\ntypedef std::pair<std::vector<size_t>, bool> SubGraph_t;\n\n//!\n//! \\typedef SubGraphCollection_t\n//!\n//! \\brief The data structure containing all SubGraph_t partitioned\n//! out of an ONNX graph.\n//!\ntypedef std::vector<SubGraph_t> SubGraphCollection_t;\n\n//!\n//! \\namespace nvonnxparser\n//!\n//! \\brief The TensorRT ONNX parser API namespace\n//!\nnamespace nvonnxparser\n{\n\ntemplate <typename T>\nconstexpr inline int32_t EnumMax() noexcept;\n\n//!\n//! \\enum ErrorCode\n//!\n//! \\brief The type of error that the parser or refitter may return\n//!\nenum class ErrorCode : int\n{\n    kSUCCESS = 0,\n    kINTERNAL_ERROR = 1,\n    kMEM_ALLOC_FAILED = 2,\n    kMODEL_DESERIALIZE_FAILED = 3,\n    kINVALID_VALUE = 4,\n    kINVALID_GRAPH = 5,\n    kINVALID_NODE = 6,\n    kUNSUPPORTED_GRAPH = 7,\n    kUNSUPPORTED_NODE = 8,\n    kUNSUPPORTED_NODE_ATTR = 9,\n    kUNSUPPORTED_NODE_INPUT = 10,\n    kUNSUPPORTED_NODE_DATATYPE = 11,\n    kUNSUPPORTED_NODE_DYNAMIC = 12,\n    kUNSUPPORTED_NODE_SHAPE = 13,\n    kREFIT_FAILED = 14\n};\n\n//!\n//! Maximum number of flags in the ErrorCode enum.\n//!\n//! \\see ErrorCode\n//!\ntemplate <>\nconstexpr inline int32_t EnumMax<ErrorCode>() noexcept\n{\n    return 14;\n}\n\n//!\n//! \\brief Represents one or more OnnxParserFlag values using binary OR\n//! operations, e.g., 1U << OnnxParserFlag::kNATIVE_INSTANCENORM\n//!\n//! \\see IParser::setFlags() and IParser::getFlags()\n//!\nusing OnnxParserFlags = uint32_t;\n\nenum class OnnxParserFlag : int32_t\n{\n    //! Parse the ONNX model into the INetworkDefinition with the intention of using TensorRT's native layer\n    //! implementation over the plugin implementation for InstanceNormalization nodes.\n    //! This flag is required when building version-compatible or hardware-compatible engines.\n    //! This flag is set to be ON by default.\n    kNATIVE_INSTANCENORM = 0\n};\n\n//!\n//! Maximum number of flags in the OnnxParserFlag enum.\n//!\n//! \\see OnnxParserFlag\n//!\ntemplate <>\nconstexpr inline int32_t EnumMax<OnnxParserFlag>() noexcept\n{\n    return 1;\n}\n\n//!\n//! \\class IParserError\n//!\n//! \\brief an object containing information about an error\n//!\nclass IParserError\n{\npublic:\n    //!\n    //!\\brief the error code.\n    //!\n    virtual ErrorCode code() const = 0;\n    //!\n    //!\\brief description of the error.\n    //!\n    virtual char const* desc() const = 0;\n    //!\n    //!\\brief source file in which the error occurred.\n    //!\n    virtual char const* file() const = 0;\n    //!\n    //!\\brief source line at which the error occurred.\n    //!\n    virtual int line() const = 0;\n    //!\n    //!\\brief source function in which the error occurred.\n    //!\n    virtual char const* func() const = 0;\n    //!\n    //!\\brief index of the ONNX model node in which the error occurred.\n    //!\n    virtual int node() const = 0;\n    //!\n    //!\\brief name of the node in which the error occurred.\n    //!\n    virtual char const* nodeName() const = 0;\n    //!\n    //!\\brief name of the node operation in which the error occurred.\n    //!\n    virtual char const* nodeOperator() const = 0;\n    //!\n    //!\\brief A list of the local function names, from the top level down, constituting the current\n    //!             stack trace in which the error occurred. A top-level node that is not inside any\n    //!             local function would return a nullptr.\n    //!\n    virtual char const* const* localFunctionStack() const = 0;\n    //!\n    //!\\brief The size of the stack of local functions at the point where the error occurred.\n    //!             A top-level node that is not inside any local function would correspond to\n    //              a stack size of 0.\n    //!\n    virtual int32_t localFunctionStackSize() const = 0;\n\nprotected:\n    virtual ~IParserError() {}\n};\n\n//!\n//! \\class IParser\n//!\n//! \\brief an object for parsing ONNX models into a TensorRT network definition\n//!\n//! \\warning If the ONNX model has a graph output with the same name as a graph input,\n//!          the output will be renamed by prepending \"__\".\n//!\n//! \\warning Do not inherit from this class, as doing so will break forward-compatibility of the API and ABI.\n//!\nclass IParser\n{\npublic:\n    //!\n    //! \\brief Parse a serialized ONNX model into the TensorRT network.\n    //!         This method has very limited diagnostics. If parsing the serialized model\n    //!         fails for any reason (e.g. unsupported IR version, unsupported opset, etc.)\n    //!         it the user responsibility to intercept and report the error.\n    //!         To obtain a better diagnostic, use the parseFromFile method below.\n    //!\n    //! \\param serialized_onnx_model Pointer to the serialized ONNX model\n    //! \\param serialized_onnx_model_size Size of the serialized ONNX model\n    //!        in bytes\n    //! \\param model_path Absolute path to the model file for loading external weights if required\n    //! \\return true if the model was parsed successfully\n    //! \\see getNbErrors() getError()\n    //!\n    virtual bool parse(\n        void const* serialized_onnx_model, size_t serialized_onnx_model_size, const char* model_path = nullptr) noexcept\n        = 0;\n\n    //!\n    //! \\brief Parse an onnx model file, which can be a binary protobuf or a text onnx model\n    //!         calls parse method inside.\n    //!\n    //! \\param onnxModelFile name\n    //! \\param verbosity Level\n    //!\n    //! \\return true if the model was parsed successfully\n    //!\n    //!\n    virtual bool parseFromFile(const char* onnxModelFile, int verbosity) noexcept = 0;\n\n    //! [DEPRECATED] Deprecated in TensorRT 10.1. See supportsModelV2.\n    //!\n    //! \\brief Check whether TensorRT supports a particular ONNX model.\n    //!        If the function returns True, one can proceed to engine building\n    //!        without having to call \\p parse or \\p parseFromFile.\n    //!\n    //! \\param serialized_onnx_model Pointer to the serialized ONNX model\n    //! \\param serialized_onnx_model_size Size of the serialized ONNX model\n    //!        in bytes\n    //! \\param sub_graph_collection Container to hold supported subgraphs\n    //! \\param model_path Absolute path to the model file for loading external weights if required\n    //! \\return true if the model is supported\n    //!\n    TRT_DEPRECATED virtual bool supportsModel(void const* serialized_onnx_model, size_t serialized_onnx_model_size,\n        SubGraphCollection_t& sub_graph_collection, const char* model_path = nullptr) noexcept = 0;\n\n    //!\n    //!\\brief Parse a serialized ONNX model into the TensorRT network\n    //! with consideration of user provided weights\n    //!\n    //! \\param serialized_onnx_model Pointer to the serialized ONNX model\n    //! \\param serialized_onnx_model_size Size of the serialized ONNX model\n    //!        in bytes\n    //! \\return true if the model was parsed successfully\n    //! \\see getNbErrors() getError()\n    //!\n    virtual bool parseWithWeightDescriptors(\n        void const* serialized_onnx_model, size_t serialized_onnx_model_size) noexcept\n        = 0;\n\n    //!\n    //!\\brief Returns whether the specified operator may be supported by the\n    //!         parser.\n    //!\n    //! Note that a result of true does not guarantee that the operator will be\n    //! supported in all cases (i.e., this function may return false-positives).\n    //!\n    //! \\param op_name The name of the ONNX operator to check for support\n    //!\n    virtual bool supportsOperator(const char* op_name) const noexcept = 0;\n\n    //!\n    //!\\brief Get the number of errors that occurred during prior calls to\n    //!         \\p parse\n    //!\n    //! \\see getError() clearErrors() IParserError\n    //!\n    virtual int getNbErrors() const noexcept = 0;\n\n    //!\n    //!\\brief Get an error that occurred during prior calls to \\p parse\n    //!\n    //! \\see getNbErrors() clearErrors() IParserError\n    //!\n    virtual IParserError const* getError(int index) const noexcept = 0;\n\n    //!\n    //!\\brief Clear errors from prior calls to \\p parse\n    //!\n    //! \\see getNbErrors() getError() IParserError\n    //!\n    virtual void clearErrors() noexcept = 0;\n\n    virtual ~IParser() noexcept = default;\n\n    //!\n    //! \\brief Query the plugin libraries needed to implement operations used by the parser in a version-compatible\n    //! engine.\n    //!\n    //! This provides a list of plugin libraries on the filesystem needed to implement operations\n    //! in the parsed network.  If you are building a version-compatible engine using this network,\n    //! provide this list to IBuilderConfig::setPluginsToSerialize to serialize these plugins along\n    //! with the version-compatible engine, or, if you want to ship these plugin libraries externally\n    //! to the engine, ensure that IPluginRegistry::loadLibrary is used to load these libraries in the\n    //! appropriate runtime before deserializing the corresponding engine.\n    //!\n    //! \\param[out] nbPluginLibs Returns the number of plugin libraries in the array, or -1 if there was an error.\n    //! \\return Array of `nbPluginLibs` C-strings describing plugin library paths on the filesystem if nbPluginLibs > 0,\n    //! or nullptr otherwise.  This array is owned by the IParser, and the pointers in the array are only valid until\n    //! the next call to parse(), supportsModel(), parseFromFile(), or parseWithWeightDescriptors().\n    //!\n    virtual char const* const* getUsedVCPluginLibraries(int64_t& nbPluginLibs) const noexcept = 0;\n\n    //!\n    //! \\brief Set the parser flags.\n    //!\n    //! The flags are listed in the OnnxParserFlag enum.\n    //!\n    //! \\param OnnxParserFlag The flags used when parsing an ONNX model.\n    //!\n    //! \\note This function will override the previous set flags, rather than bitwise ORing the new flag.\n    //!\n    //! \\see getFlags()\n    //!\n    virtual void setFlags(OnnxParserFlags onnxParserFlags) noexcept = 0;\n\n    //!\n    //! \\brief Get the parser flags. Defaults to 0.\n    //!\n    //! \\return The parser flags as a bitmask.\n    //!\n    //! \\see setFlags()\n    //!\n    virtual OnnxParserFlags getFlags() const noexcept = 0;\n\n    //!\n    //! \\brief clear a parser flag.\n    //!\n    //! clears the parser flag from the enabled flags.\n    //!\n    //! \\see setFlags()\n    //!\n    virtual void clearFlag(OnnxParserFlag onnxParserFlag) noexcept = 0;\n\n    //!\n    //! \\brief Set a single parser flag.\n    //!\n    //! Add the input parser flag to the already enabled flags.\n    //!\n    //! \\see setFlags()\n    //!\n    virtual void setFlag(OnnxParserFlag onnxParserFlag) noexcept = 0;\n\n    //!\n    //! \\brief Returns true if the parser flag is set\n    //!\n    //! \\see getFlags()\n    //!\n    //! \\return True if flag is set, false if unset.\n    //!\n    virtual bool getFlag(OnnxParserFlag onnxParserFlag) const noexcept = 0;\n\n    //!\n    //!\\brief Return the i-th output ITensor object for the ONNX layer \"name\".\n    //!\n    //! Return the i-th output ITensor object for the ONNX layer \"name\".\n    //! If \"name\" is not found or i is out of range, return nullptr.\n    //! In the case of multiple nodes sharing the same name this function will return\n    //! the output tensors of the first instance of the node in the ONNX graph.\n    //!\n    //! \\param name The name of the ONNX layer.\n    //!\n    //! \\param i The index of the output. i must be in range [0, layer.num_outputs).\n    //!\n    virtual nvinfer1::ITensor const* getLayerOutputTensor(char const* name, int64_t i) noexcept = 0;\n\n    //!\n    //! \\brief Check whether TensorRT supports a particular ONNX model.\n    //!            If the function returns True, one can proceed to engine building\n    //!            without having to call \\p parse or \\p parseFromFile.\n    //!            Results can be queried through \\p getNbSubgraphs, \\p isSubgraphSupported,\n    //!            \\p getSubgraphNodes.\n    //!\n    //! \\param serializedOnnxModel Pointer to the serialized ONNX model\n    //! \\param serializedOnnxModelSize Size of the serialized ONNX model in bytes\n    //! \\param modelPath Absolute path to the model file for loading external weights if required\n    //! \\return true if the model is supported\n    //!\n    virtual bool supportsModelV2(\n        void const* serializedOnnxModel, size_t serializedOnnxModelSize, char const* modelPath = nullptr) noexcept = 0;\n\n    //!\n    //! \\brief Get the number of subgraphs. Calling this function before calling \\p supportsModelV2 results in undefined\n    //! behavior.\n    //!\n    //!\n    //! \\return Number of subgraphs.\n    //!\n    virtual int64_t getNbSubgraphs() noexcept = 0;\n\n    //!\n    //! \\brief Returns whether the subgraph is supported. Calling this function before calling \\p supportsModelV2\n    //! results in undefined behavior.\n    //!\n    //!\n    //! \\param index Index of the subgraph.\n    //! \\return Whether the subgraph is supported.\n    //!\n    virtual bool isSubgraphSupported(int64_t const index) noexcept = 0;\n\n    //!\n    //! \\brief Get the nodes of the specified subgraph. Calling this function before calling \\p supportsModelV2 results\n    //! in undefined behavior.\n    //!\n    //!\n    //! \\param index Index of the subgraph.\n    //! \\param subgraphLength Returns the length of the subgraph as reference.\n    //!\n    //! \\return Pointer to the subgraph nodes array. This pointer is owned by the Parser.\n    //!\n    virtual int64_t* getSubgraphNodes(int64_t const index, int64_t& subgraphLength) noexcept = 0;\n};\n\n//!\n//! \\class IParserRefitter\n//!\n//! \\brief An interface designed to refit weights from an ONNX model.\n//!\n//! \\warning Do not inherit from this class, as doing so will break forward-compatibility of the API and ABI.\n//!\nclass IParserRefitter\n{\npublic:\n    //!\n    //! \\brief Load a serialized ONNX model from memory and perform weight refit.\n    //!\n    //! \\param serializedOnnxModel Pointer to the serialized ONNX model\n    //! \\param serializedOnnxModelSize Size of the serialized ONNX model\n    //!        in bytes\n    //! \\param modelPath Absolute path to the model file for loading external weights if required\n    //! \\return true if all the weights in the engine were refit successfully.\n    //!\n    //! The serialized ONNX model must be identical to the one used to generate the engine\n    //! that will be refit.\n    //!\n    virtual bool refitFromBytes(\n        void const* serializedOnnxModel, size_t serializedOnnxModelSize, char const* modelPath = nullptr) noexcept\n        = 0;\n\n    //!\n    //! \\brief Load and parse a ONNX model from disk and perform weight refit.\n    //!\n    //! \\param onnxModelFile Path to the ONNX model to load from disk.\n    //!\n    //! \\return true if the model was loaded successfully, and if all the weights in the engine were refit successfully.\n    //!\n    //! The provided ONNX model must be identical to the one used to generate the engine\n    //! that will be refit.\n    //!\n    virtual bool refitFromFile(char const* onnxModelFile) noexcept = 0;\n\n    //!\n    //!\\brief Get the number of errors that occurred during prior calls to \\p refitFromBytes or \\p refitFromFile\n    //!\n    //! \\see getError() IParserError\n    //!\n    virtual int32_t getNbErrors() const noexcept = 0;\n\n    //!\n    //!\\brief Get an error that occurred during prior calls to \\p refitFromBytes or \\p refitFromFile\n    //!\n    //! \\see getNbErrors() IParserError\n    //!\n    virtual IParserError const* getError(int32_t index) const noexcept = 0;\n\n    //!\n    //!\\brief Clear errors from prior calls to \\p refitFromBytes or \\p refitFromFile\n    //!\n    //! \\see getNbErrors() getError() IParserError\n    //!\n    virtual void clearErrors() = 0;\n\n    virtual ~IParserRefitter() noexcept = default;\n};\n\n} // namespace nvonnxparser\n\nextern \"C\" TENSORRTAPI void* createNvOnnxParser_INTERNAL(void* network, void* logger, int version) noexcept;\nextern \"C\" TENSORRTAPI void* createNvOnnxParserRefitter_INTERNAL(\n    void* refitter, void* logger, int32_t version) noexcept;\nextern \"C\" TENSORRTAPI int getNvOnnxParserVersion() noexcept;\n\nnamespace nvonnxparser\n{\n\nnamespace\n{\n\n//!\n//! \\brief Create a new parser object\n//!\n//! \\param network The network definition that the parser will write to\n//! \\param logger The logger to use\n//! \\return a new parser object or NULL if an error occurred\n//!\n//! Any input dimensions that are constant should not be changed after parsing,\n//! because correctness of the translation may rely on those constants.\n//! Changing a dynamic input dimension, i.e. one that translates to -1 in\n//! TensorRT, to a constant is okay if the constant is consistent with the model.\n//! Each instance of the parser is designed to only parse one ONNX model once.\n//!\n//! \\see IParser\n//!\ninline IParser* createParser(nvinfer1::INetworkDefinition& network, nvinfer1::ILogger& logger) noexcept\n{\n    try\n    {\n        return static_cast<IParser*>(createNvOnnxParser_INTERNAL(&network, &logger, NV_ONNX_PARSER_VERSION));\n    }\n    catch (std::exception& e)\n    {\n        logger.log(nvinfer1::ILogger::Severity::kINTERNAL_ERROR, e.what());\n    }\n\n    return nullptr;\n}\n\n//!\n//! \\brief Create a new ONNX refitter object\n//!\n//! \\param refitter The Refitter object used to refit the model\n//! \\param logger The logger to use\n//! \\return a new ParserRefitter object or NULL if an error occurred\n//!\n//! \\see IParserRefitter\n//!\ninline IParserRefitter* createParserRefitter(nvinfer1::IRefitter& refitter, nvinfer1::ILogger& logger) noexcept\n{\n    try\n    {\n        return static_cast<IParserRefitter*>(\n            createNvOnnxParserRefitter_INTERNAL(&refitter, &logger, NV_ONNX_PARSER_VERSION));\n    }\n    catch (std::exception& e)\n    {\n        logger.log(nvinfer1::ILogger::Severity::kINTERNAL_ERROR, e.what());\n    }\n\n    return nullptr;\n}\n\n} // namespace\n\n} // namespace nvonnxparser\n\n#endif // NV_ONNX_PARSER_H\n"
        },
        {
          "name": "OnnxAttrs.cpp",
          "type": "blob",
          "size": 10.998046875,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"OnnxAttrs.hpp\"\n#include \"ShapedWeights.hpp\"\n#include \"importerUtils.hpp\"\n#include <onnx/onnx_pb.h>\n\nbool isExternalAttribute(std::string const& key, onnx2trt::ImporterContext* ctx)\n{\n    return !key.empty() && !ctx->localFunctionStack().empty() && ctx->localFunctionStack().back().attrs.count(key);\n}\n\ntemplate <>\nfloat OnnxAttrs::get<float>(std::string const& key) const\n{\n    std::string extName = this->at(key)->ref_attr_name();\n    bool isExtAttr = isExternalAttribute(extName, mCtx);\n    return isExtAttr ? mCtx->localFunctionStack().back().attrs.at(extName)->f() : this->at(key)->f();\n}\n\ntemplate <>\nint32_t OnnxAttrs::get<int32_t>(std::string const& key) const\n{\n    std::string extName = this->at(key)->ref_attr_name();\n    bool isExtAttr = isExternalAttribute(extName, mCtx);\n    return isExtAttr ? mCtx->localFunctionStack().back().attrs.at(extName)->i() : this->at(key)->i();\n}\n\ntemplate <>\nint64_t OnnxAttrs::get<int64_t>(std::string const& key) const\n{\n    std::string extName = this->at(key)->ref_attr_name();\n    bool isExtAttr = isExternalAttribute(extName, mCtx);\n    return isExtAttr ? mCtx->localFunctionStack().back().attrs.at(extName)->i() : this->at(key)->i();\n}\n\ntemplate <>\nbool OnnxAttrs::get<bool>(std::string const& key) const\n{\n    std::string extName = this->at(key)->ref_attr_name();\n    bool isExtAttr = isExternalAttribute(extName, mCtx);\n    int64_t value = isExtAttr ? mCtx->localFunctionStack().back().attrs.at(extName)->i() : this->at(key)->i();\n    assert(value == bool(value));\n    return static_cast<bool>(value);\n}\n\ntemplate <>\nstd::string OnnxAttrs::get<std::string>(std::string const& key) const\n{\n    std::string extName = this->at(key)->ref_attr_name();\n    bool isExtAttr = isExternalAttribute(extName, mCtx);\n    return isExtAttr ? mCtx->localFunctionStack().back().attrs.at(extName)->s() : this->at(key)->s();\n}\n\ntemplate <>\nstd::vector<int32_t> OnnxAttrs::get<std::vector<int32_t>>(std::string const& key) const\n{\n    std::string extName = this->at(key)->ref_attr_name();\n    bool isExtAttr = isExternalAttribute(extName, mCtx);\n    auto attr = isExtAttr ? mCtx->localFunctionStack().back().attrs.at(extName)->ints() : this->at(key)->ints();\n    return std::vector<int32_t>(attr.begin(), attr.end());\n}\n\ntemplate <>\nstd::vector<int64_t> OnnxAttrs::get<std::vector<int64_t>>(std::string const& key) const\n{\n    std::string extName = this->at(key)->ref_attr_name();\n    bool isExtAttr = isExternalAttribute(extName, mCtx);\n    auto attr = isExtAttr ? mCtx->localFunctionStack().back().attrs.at(extName)->ints() : this->at(key)->ints();\n    return std::vector<int64_t>(attr.begin(), attr.end());\n}\n\ntemplate <>\nstd::vector<float> OnnxAttrs::get<std::vector<float>>(std::string const& key) const\n{\n    std::string extName = this->at(key)->ref_attr_name();\n    bool isExtAttr = isExternalAttribute(extName, mCtx);\n    auto attr = isExtAttr ? mCtx->localFunctionStack().back().attrs.at(extName)->floats() : this->at(key)->floats();\n    return std::vector<float>(attr.begin(), attr.end());\n}\n\ntemplate <>\nnvinfer1::Dims OnnxAttrs::get<nvinfer1::Dims>(std::string const& key) const\n{\n    auto values = this->get<std::vector<int32_t>>(key);\n    nvinfer1::Dims dims;\n    dims.nbDims = values.size();\n    if (dims.nbDims > nvinfer1::Dims::MAX_DIMS)\n    {\n        throw std::runtime_error{\"Number of dimensions values exceed the maximum amount supported by TensorRT!\"};\n    }\n    std::copy(values.begin(), values.end(), dims.d);\n    // Note: No dimension type information is included\n    return dims;\n}\n\ntemplate <>\nnvinfer1::DimsHW OnnxAttrs::get<nvinfer1::DimsHW>(std::string const& key) const\n{\n    nvinfer1::Dims dims = this->get<nvinfer1::Dims>(key);\n    assert(dims.nbDims == 2);\n    return nvinfer1::DimsHW(dims.d[0], dims.d[1]);\n}\n\ntemplate <>\nnvinfer1::Permutation OnnxAttrs::get<nvinfer1::Permutation>(std::string const& key) const\n{\n    auto values = this->get<std::vector<int32_t>>(key);\n    nvinfer1::Permutation perm;\n    if (values.size() > nvinfer1::Dims::MAX_DIMS)\n    {\n        throw std::runtime_error{\"Number of permutations values exceed the maximum amount supported by TensorRT!\"};\n    }\n    std::copy(values.begin(), values.end(), perm.order);\n    // Fill unused values with identity permutation\n    for (int32_t i = values.size(); i < nvinfer1::Dims::MAX_DIMS; ++i)\n    {\n        perm.order[i] = i;\n    }\n    return perm;\n}\n\ntemplate <>\nonnx2trt::ShapedWeights OnnxAttrs::get<onnx2trt::ShapedWeights>(std::string const& key) const\n{\n    // Check for reference attribute in parent function\n    std::string extName = this->at(key)->ref_attr_name();\n    bool isExtAttr = isExternalAttribute(extName, mCtx);\n\n    ::ONNX_NAMESPACE::TensorProto const& onnxTensor = isExtAttr ? mCtx->localFunctionStack().back().attrs.at(extName)->t() : this->at(key)->t();\n    onnx2trt::ShapedWeights weights;\n    bool success = mCtx->getWeightsContext().convertOnnxWeights(onnxTensor, &weights, true);\n    if (!success)\n    {\n        throw std::runtime_error{\"Unable to convert ONNX weights\"};\n    }\n    return weights;\n}\n\ntemplate <>\nnvinfer1::DataType OnnxAttrs::get<nvinfer1::DataType>(std::string const& key) const\n{\n    ::ONNX_NAMESPACE::TensorProto::DataType onnx_dtype\n        = static_cast<::ONNX_NAMESPACE::TensorProto::DataType>(this->at(key)->i());\n    nvinfer1::DataType dtype{};\n    if (!onnx2trt::convertDtype(onnx_dtype, &dtype))\n    {\n        dtype = static_cast<nvinfer1::DataType>(-1);\n    }\n    return dtype;\n}\n\ntemplate <>\nstd::vector<nvinfer1::DataType> OnnxAttrs::get<std::vector<nvinfer1::DataType>>(std::string const& key) const\n{\n    auto attr = this->at(key)->ints();\n    auto onnx_dtypes = std::vector<int64_t>(attr.begin(), attr.end());\n    std::vector<nvinfer1::DataType> dtypes{};\n    for (auto onnx_dtype : onnx_dtypes)\n    {\n        nvinfer1::DataType dtype{};\n        if (!onnx2trt::convertDtype(static_cast<int32_t>(onnx_dtype), &dtype))\n        {\n            dtype = static_cast<nvinfer1::DataType>(-1);\n        }\n        dtypes.push_back(dtype);\n    }\n    return dtypes;\n}\n\ninline nvinfer1::ActivationType activationStringToEnum(std::string const& type)\n{\n    if (type == \"Relu\")\n    {\n        return nvinfer1::ActivationType::kRELU;\n    }\n    if (type == \"Tanh\")\n    {\n        return nvinfer1::ActivationType::kTANH;\n    }\n    if (type == \"Sigmoid\")\n    {\n        return nvinfer1::ActivationType::kSIGMOID;\n    }\n    if (type == \"LeakyRelu\")\n    {\n        return nvinfer1::ActivationType::kLEAKY_RELU;\n    }\n    if (type == \"ThresholdedRelu\")\n    {\n        return nvinfer1::ActivationType::kTHRESHOLDED_RELU;\n    }\n    if (type == \"ScaledTanh\")\n    {\n        return nvinfer1::ActivationType::kSCALED_TANH;\n    }\n    if (type == \"HardSigmoid\")\n    {\n        return nvinfer1::ActivationType::kHARD_SIGMOID;\n    }\n    if (type == \"Elu\")\n    {\n        return nvinfer1::ActivationType::kELU;\n    }\n    if (type == \"Softsign\")\n    {\n        return nvinfer1::ActivationType::kSOFTSIGN;\n    }\n    if (type == \"Softplus\")\n    {\n        return nvinfer1::ActivationType::kSOFTPLUS;\n    }\n    throw std::runtime_error(\"Unknown activation type: \" + type);\n}\n\ntemplate <>\nnvinfer1::ActivationType OnnxAttrs::get<nvinfer1::ActivationType>(std::string const& key) const\n{\n    const std::string type = this->get<std::string>(key);\n    return activationStringToEnum(type);\n}\n\ntemplate <>\nstd::vector<nvinfer1::ActivationType> OnnxAttrs::get<std::vector<nvinfer1::ActivationType>>(\n    std::string const& key) const\n{\n    const auto strings = this->at(key)->strings();\n    std::vector<nvinfer1::ActivationType> actTypes;\n    for (const auto& str : strings)\n    {\n        actTypes.emplace_back(activationStringToEnum(str));\n    }\n    return actTypes;\n}\n\ntemplate <>\nconst ::ONNX_NAMESPACE::GraphProto& OnnxAttrs::get<const ::ONNX_NAMESPACE::GraphProto&>(std::string const& key) const\n{\n    return this->at(key)->g();\n}\n\ntemplate <>\nstd::vector<std::string> OnnxAttrs::get<std::vector<std::string>>(std::string const& key) const\n{\n    auto attr = this->at(key)->strings();\n    return std::vector<std::string>(attr.begin(), attr.end());\n}\n\ntemplate <>\nnvinfer1::ScaleMode OnnxAttrs::get<nvinfer1::ScaleMode>(std::string const& key) const\n{\n    std::string s = this->get<std::string>(key);\n    if (s == \"uniform\")\n    {\n        return nvinfer1::ScaleMode::kUNIFORM;\n    }\n    if (s == \"channel\")\n    {\n        return nvinfer1::ScaleMode::kCHANNEL;\n    }\n    if (s == \"elementwise\")\n    {\n        return nvinfer1::ScaleMode::kELEMENTWISE;\n    }\n    throw std::runtime_error(\"Unknown ScaleMode: \" + s);\n}\n\ntemplate <>\nnvinfer1::MatrixOperation OnnxAttrs::get<nvinfer1::MatrixOperation>(std::string const& key) const\n{\n    std::string s = this->get<std::string>(key);\n    if (s == \"none\")\n    {\n        return nvinfer1::MatrixOperation::kNONE;\n    }\n    if (s == \"transpose\")\n    {\n        return nvinfer1::MatrixOperation::kTRANSPOSE;\n    }\n    if (s == \"vector\")\n    {\n        return nvinfer1::MatrixOperation::kVECTOR;\n    }\n    throw std::runtime_error(\"Unknown MatrixOperation: \" + s);\n}\n\ntemplate <>\nnvinfer1::InterpolationMode OnnxAttrs::get<nvinfer1::InterpolationMode>(std::string const& key) const\n{\n    const auto& mode = this->get<std::string>(key);\n    if (mode == \"nearest\")\n    {\n        return nvinfer1::InterpolationMode::kNEAREST;\n    }\n    if (mode == \"linear\" || mode == \"bilinear\")\n    {\n        return nvinfer1::InterpolationMode::kLINEAR;\n    }\n    throw std::runtime_error(\"Unknown InterpolationMode: \" + mode);\n}\n\ntemplate <>\nnvinfer1::ResizeCoordinateTransformation OnnxAttrs::get<nvinfer1::ResizeCoordinateTransformation>(\n    std::string const& key) const\n{\n    const auto& transformation = this->get<std::string>(key);\n    if (transformation == \"align_corners\")\n    {\n        return nvinfer1::ResizeCoordinateTransformation::kALIGN_CORNERS;\n    }\n    if (transformation == \"asymmetric\")\n    {\n        return nvinfer1::ResizeCoordinateTransformation::kASYMMETRIC;\n    }\n    if (transformation == \"half_pixel\")\n    {\n        return nvinfer1::ResizeCoordinateTransformation::kHALF_PIXEL;\n    }\n    throw std::runtime_error(\"Unknown ResizeCoordinateTransformation: \" + transformation);\n}\n\ntemplate <>\nnvinfer1::ResizeSelector OnnxAttrs::get<nvinfer1::ResizeSelector>(std::string const& key) const\n{\n    const auto& selector = this->get<std::string>(key);\n    if (selector == \"formula\")\n    {\n        return nvinfer1::ResizeSelector::kFORMULA;\n    }\n    if (selector == \"upper\")\n    {\n        return nvinfer1::ResizeSelector::kUPPER;\n    }\n    throw std::runtime_error(\"Unknown ResizeSelector: \" + selector);\n}\n\ntemplate <>\nnvinfer1::ResizeRoundMode OnnxAttrs::get<nvinfer1::ResizeRoundMode>(std::string const& key) const\n{\n    const auto& roundMode = this->get<std::string>(key);\n    if (roundMode == \"half_up\")\n    {\n        return nvinfer1::ResizeRoundMode::kHALF_UP;\n    }\n    if (roundMode == \"half_down\")\n    {\n        return nvinfer1::ResizeRoundMode::kHALF_DOWN;\n    }\n    if (roundMode == \"floor\")\n    {\n        return nvinfer1::ResizeRoundMode::kFLOOR;\n    }\n    if (roundMode == \"ceil\")\n    {\n        return nvinfer1::ResizeRoundMode::kCEIL;\n    }\n    throw std::runtime_error(\"Unknown ResizeRoundMode: \" + roundMode);\n}\n"
        },
        {
          "name": "OnnxAttrs.hpp",
          "type": "blob",
          "size": 1.3583984375,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include <NvInfer.h>\n#include <onnx/onnx_pb.h>\n#include <unordered_map>\n#include <vector>\n\n#include \"ImporterContext.hpp\"\n\nclass OnnxAttrs\n{\n    template <typename T>\n    using string_map = std::unordered_map<std::string, T>;\n    typedef string_map<::ONNX_NAMESPACE::AttributeProto const*> AttrMap;\n    AttrMap _attrs;\n    onnx2trt::ImporterContext* mCtx;\n\npublic:\n    explicit OnnxAttrs(::ONNX_NAMESPACE::NodeProto const& onnx_node, onnx2trt::ImporterContext* ctx)\n        : mCtx{ctx}\n    {\n        for (auto const& attr : onnx_node.attribute())\n        {\n            _attrs.insert({attr.name(), &attr});\n        }\n    }\n\n    bool count(std::string const& key) const\n    {\n        return _attrs.count(key);\n    }\n\n    ::ONNX_NAMESPACE::AttributeProto const* at(std::string key) const\n    {\n        if (!_attrs.count(key))\n        {\n            throw std::out_of_range(\"Attribute not found: \" + key);\n        }\n        return _attrs.at(key);\n    }\n\n    ::ONNX_NAMESPACE::AttributeProto::AttributeType type(std::string const& key) const\n    {\n        return this->at(key)->type();\n    }\n\n    template <typename T>\n    T get(std::string const& key) const;\n\n    template <typename T>\n    T get(std::string const& key, T const& default_value) const\n    {\n        return _attrs.count(key) ? this->get<T>(key) : default_value;\n    }\n};\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.033203125,
          "content": "<!--- SPDX-License-Identifier: Apache-2.0 -->\n\n# TensorRT Backend For ONNX\n\nParses ONNX models for execution with [TensorRT](https://developer.nvidia.com/tensorrt).\n\nSee also the [TensorRT documentation](https://docs.nvidia.com/deeplearning/tensorrt/).\n\nFor the list of recent changes, see the [changelog](docs/Changelog.md).\n\nFor a list of commonly seen issues and questions, see the [FAQ](docs/faq.md).\n\nFor business inquiries, please contact researchinquiries@nvidia.com\n\nFor press and other inquiries, please contact Hector Marinez at hmarinez@nvidia.com\n\n## Supported TensorRT Versions\n\nDevelopment on the this branch is for the latest version of [TensorRT 10.7](https://developer.nvidia.com/nvidia-tensorrt-download) with full-dimensions and dynamic shape support.\n\nFor previous versions of TensorRT, refer to their respective branches.\n\n## Supported Operators\n\nCurrent supported ONNX operators are found in the [operator support matrix](docs/operators.md).\n\n# Installation\n\n### Dependencies\n\n - [Protobuf >= 3.0.x](https://github.com/google/protobuf/releases)\n - [TensorRT 10.7](https://developer.nvidia.com/tensorrt)\n - [TensorRT 10.7 open source libaries] (https://github.com/NVIDIA/TensorRT/)\n\n### Building\n\nFor building within docker, we recommend using and setting up the docker containers as instructed in the main [TensorRT repository](https://github.com/NVIDIA/TensorRT#setting-up-the-build-environment) to build the onnx-tensorrt library.\n\nOnce you have cloned the repository, you can build the parser libraries and executables by running:\n\n    cd onnx-tensorrt\n    mkdir build && cd build\n    cmake .. -DTENSORRT_ROOT=<path_to_trt> && make -j\n    # Ensure that you update your LD_LIBRARY_PATH to pick up the location of the newly built library:\n    export LD_LIBRARY_PATH=$PWD:$LD_LIBRARY_PATH\n\nNote that this project has a dependency on CUDA. By default the build will look in `/usr/local/cuda` for the CUDA toolkit installation. If your CUDA path is different, overwrite the default path by providing `-DCUDA_TOOLKIT_ROOT_DIR=<path_to_cuda_install>` in the CMake command.\n\nTo build with `protobuf-lite` support, add `-DUSE_ONNX_LITE_PROTO=1` to the end of the `cmake` command.\n\n### InstanceNormalizaiton Performance\n\nThere are two implementations of InstanceNormalization that may perform differently depending on various parameters. By default, the parser will use the native TensorRT implementation of InstanceNorm. Users that want to benchmark using the plugin implementation of InstanceNorm can unset the parser flag `kNATIVE_INSTANCENORM` prior to parsing the model. Note that the plugin implementation cannot be used for building version compatible or hardware compatible engines, and attempting to do so will result in an error.\n\nC++ Example:\n\n    // Unset the kNATIVE_INSTANCENORM flag to use the plugin implementation.\n    parser->unsetFlag(nvonnxparser::OnnxParserFlag::kNATIVE_INSTANCENORM);\n\nPython Example:\n\n    // Unset the NATIVE_INSTANCENORM flag to use the plugin implementation.\n    parser.clear_flag(trt.OnnxParserFlag.NATIVE_INSTANCENORM)\n\n## Executable Usage\n\nThere are currently two officially supported tools for users to quickly check if an ONNX model can parse and build into a TensorRT engine from an ONNX file.\n\nFor C++ users, there is the [trtexec](https://github.com/NVIDIA/TensorRT/tree/main/samples/opensource/trtexec) binary that is typically found in the `<tensorrt_root_dir>/bin` directory. The basic command of running an ONNX model is:\n\n`trtexec --onnx=model.onnx`\n\nRefer to the link or run `trtexec -h` for more information on CLI options.\n\nFor Python users, there is the [polygraphy](https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy) tool. The basic command for running an onnx model is:\n\n`polygraphy run model.onnx --trt`\n\nRefer to the link or run `polygraphy run -h` for more information on CLI options.\n\n### Python Modules\n\nPython bindings for the ONNX-TensorRT parser are packaged in the shipped `.whl` files.\n\nTensorRT 10.7 supports ONNX release 1.17.0. Install it with:\n\n    python3 -m pip install onnx==1.17.0\n\nThe ONNX-TensorRT backend can be installed by running:\n\n    python3 setup.py install\n\n## ONNX-TensorRT Python Backend Usage\n\nThe TensorRT backend for ONNX can be used in Python as follows:\n\n```python\nimport onnx\nimport onnx_tensorrt.backend as backend\nimport numpy as np\n\nmodel = onnx.load(\"/path/to/model.onnx\")\nengine = backend.prepare(model, device='CUDA:1')\ninput_data = np.random.random(size=(32, 3, 224, 224)).astype(np.float32)\noutput_data = engine.run(input_data)[0]\nprint(output_data)\nprint(output_data.shape)\n```\n\n## C++ Library Usage\n\nThe model parser library, libnvonnxparser.so, has its C++ API declared in this header:\n\n    NvOnnxParser.h\n\n### Tests\n\nAfter installation (or inside the Docker container), ONNX backend tests can be run as follows:\n\nReal model tests only:\n\n    python onnx_backend_test.py OnnxBackendRealModelTest\n\nAll tests:\n\n    python onnx_backend_test.py\n\nYou can use `-v` flag to make output more verbose.\n\n## Pre-trained Models\n\nPre-trained models in ONNX format can be found at the [ONNX Model Zoo](https://github.com/onnx/models)\n"
        },
        {
          "name": "RNNHelpers.cpp",
          "type": "blob",
          "size": 8.2529296875,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"RNNHelpers.hpp\"\n#include \"LoopHelpers.hpp\"\n#include \"importerUtils.hpp\"\n#include <array>\n\nnamespace onnx2trt\n{\n\nnvinfer1::ITensor* addRNNInput(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, nvinfer1::ILoop* loop,\n    std::vector<TensorOrWeights>& inputs, const std::string& direction)\n{\n    // In the forward/reverse cases, we only use a single iterator. In the bidirectional case, a forward and reverse\n    // iterator must be concatenated.\n    // Input dimensions: [1, B, E]\n    nvinfer1::ITensor* iterationInput{nullptr};\n    nvinfer1::ITensor* input = &convertToTensor(inputs.at(0), ctx);\n\n    const int sequenceLenIndex = 4;\n    bool isRagged = inputs.size() > sequenceLenIndex && inputs.at(sequenceLenIndex);\n\n    if (direction == \"forward\")\n    {\n        iterationInput = unsqueezeTensor(ctx, *N_CHECK(loop->addIterator(*input)->getOutput(0)), std::vector<int>{0});\n\n        if (isRagged)\n        {\n            nvinfer1::ITensor* seqLens = &convertToTensor(inputs.at(sequenceLenIndex), ctx);\n            auto maxLen = getAxisLength(ctx, input, 0);\n            iterationInput = clearMissingSequenceElements(ctx, node, loop, seqLens, iterationInput, maxLen);\n        }\n    }\n    else if (direction == \"reverse\")\n    {\n        nvinfer1::IIteratorLayer* reverseIterator = N_CHECK(loop->addIterator(*input));\n        reverseIterator->setReverse(true);\n        auto reverseIteratorOutput = N_CHECK(reverseIterator->getOutput(0));\n        iterationInput = unsqueezeTensor(ctx, *reverseIteratorOutput, std::vector<int>{0});\n        if (isRagged)\n        {\n            nvinfer1::ITensor* seqLens = &convertToTensor(inputs.at(sequenceLenIndex), ctx);\n            auto maxLen = getAxisLength(ctx, input, 0);\n            iterationInput = clearMissingSequenceElements(ctx, node, loop, seqLens, iterationInput, maxLen, true);\n        }\n    }\n    else if (direction == \"bidirectional\")\n    {\n        nvinfer1::IIteratorLayer* forward = N_CHECK(loop->addIterator(*input));\n        nvinfer1::IIteratorLayer* reverse = N_CHECK(loop->addIterator(*input));\n        reverse->setReverse(true);\n\n        auto forwardInput = unsqueezeTensor(ctx, *N_CHECK(forward->getOutput(0)), std::vector<int>{0});\n        auto reverseInput = unsqueezeTensor(ctx, *N_CHECK(reverse->getOutput(0)), std::vector<int>{0});\n        if (isRagged)\n        {\n            nvinfer1::ITensor* seqLens = &convertToTensor(inputs.at(sequenceLenIndex), ctx);\n            auto counter = addLoopCounter(ctx, loop);\n            auto maxLen = getAxisLength(ctx, input, 0);\n            forwardInput = clearMissingSequenceElements(ctx, node, loop, seqLens, forwardInput, maxLen, false, counter);\n            reverseInput = clearMissingSequenceElements(ctx, node, loop, seqLens, reverseInput, maxLen, true, counter);\n        }\n\n        // Stack on the 0th axis to create a (numDirections, B, E) tensor.\n        std::array<nvinfer1::ITensor*, 2> tensors{{forwardInput, reverseInput}};\n        nvinfer1::IConcatenationLayer* concat = N_CHECK(ctx->network()->addConcatenation(tensors.data(), 2));\n        concat->setAxis(0);\n        iterationInput = N_CHECK(concat->getOutput(0));\n    }\n    if (iterationInput)\n    {\n        LOG_VERBOSE(\"Input shape: \" << iterationInput->getDimensions());\n    }\n    return iterationInput;\n}\n\nnvinfer1::ITensor* clearMissingSequenceElements(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node,\n    nvinfer1::ILoop* loop, nvinfer1::ITensor* seqLens, nvinfer1::ITensor* toMask, nvinfer1::ITensor* maxLen,\n    bool reverse, nvinfer1::ITensor* counter)\n{\n    nvinfer1::ITensor* zero\n        = addConstantScalar(ctx, 0.f, ::ONNX_NAMESPACE::TensorProto::FLOAT, nvinfer1::Dims3(1, 1, 1))->getOutput(0);\n    nvinfer1::ITensor* seqMask = getRaggedMask(ctx, node, loop, seqLens, maxLen, reverse, counter);\n    auto selectLayer = N_CHECK(ctx->network()->addSelect(*seqMask, *toMask, *zero));\n    return N_CHECK(selectLayer->getOutput(0));\n}\n\nnvinfer1::ITensor* maskRNNHidden(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, nvinfer1::ILoop* loop,\n    nvinfer1::ITensor* seqLens, nvinfer1::ITensor* prevH, nvinfer1::ITensor* Ht, nvinfer1::ITensor* maxLen,\n    bool reverse, nvinfer1::ITensor* counter)\n{\n    // maxLen must be provided if reverse is true\n    // Forwards previous hidden state if invalid\n    nvinfer1::ITensor* valid = getRaggedMask(ctx, node, loop, seqLens, maxLen, reverse, counter);\n    auto selectLayer = N_CHECK(ctx->network()->addSelect(*valid, *Ht, *prevH));\n    return N_CHECK(selectLayer->getOutput(0));\n}\n\nnvinfer1::ITensor* maskBidirRNNHidden(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node,\n    nvinfer1::ILoop* loop, nvinfer1::ITensor* seqLens, nvinfer1::ITensor* maxLen, nvinfer1::ITensor* Ht1,\n    nvinfer1::ITensor* Ht, nvinfer1::ITensor* singlePassShape)\n{\n    // Splits hidden state into forward and backward states, masks each accordingly, then concatenates\n\n    nvinfer1::ITensor* forwardStart = addConstant(ctx, std::vector<int32_t>{0, 0, 0},\n        ::ONNX_NAMESPACE::TensorProto::INT32,\n        nvinfer1::Dims{1, {3}})->getOutput(0);\n    nvinfer1::ITensor* reverseStart = addConstant(ctx, std::vector<int32_t>{1, 0, 0},\n        ::ONNX_NAMESPACE::TensorProto::INT32,\n        nvinfer1::Dims{1, {3}})->getOutput(0);\n\n    nvinfer1::ISliceLayer* HtForwardLayer\n        = N_CHECK(ctx->network()->addSlice(*Ht, nvinfer1::Dims3{0, 0, 0}, nvinfer1::Dims3{0, 0, 0}, nvinfer1::Dims3{1, 1, 1}));\n    HtForwardLayer->setInput(1, *forwardStart);\n    HtForwardLayer->setInput(2, *singlePassShape);\n\n    nvinfer1::ISliceLayer* HtBackwardLayer\n        = N_CHECK(ctx->network()->addSlice(*Ht, nvinfer1::Dims3{0, 0, 0}, nvinfer1::Dims3{0, 0, 0}, nvinfer1::Dims3{1, 1, 1}));\n    HtBackwardLayer->setInput(1, *reverseStart);\n    HtBackwardLayer->setInput(2, *singlePassShape);\n\n    nvinfer1::ISliceLayer* Ht1ForwardLayer\n        = N_CHECK(ctx->network()->addSlice(*Ht1, nvinfer1::Dims3{0, 0, 0}, nvinfer1::Dims3{0, 0, 0}, nvinfer1::Dims3{1, 1, 1}));\n    Ht1ForwardLayer->setInput(1, *forwardStart);\n    Ht1ForwardLayer->setInput(2, *singlePassShape);\n\n    nvinfer1::ISliceLayer* Ht1BackwardLayer\n        = N_CHECK(ctx->network()->addSlice(*Ht1, nvinfer1::Dims3{0, 0, 0}, nvinfer1::Dims3{0, 0, 0}, nvinfer1::Dims3{1, 1, 1}));\n    Ht1BackwardLayer->setInput(1, *reverseStart);\n    Ht1BackwardLayer->setInput(2, *singlePassShape);\n\n    auto forwardHt = N_CHECK(HtForwardLayer->getOutput(0));\n    auto backwardHt = N_CHECK(HtBackwardLayer->getOutput(0));\n    auto forwardHt1 = N_CHECK(Ht1ForwardLayer->getOutput(0));\n    auto backwardHt1 = N_CHECK(Ht1BackwardLayer->getOutput(0));\n\n    auto counter = addLoopCounter(ctx, loop, 0);\n    forwardHt = maskRNNHidden(ctx, node, loop, seqLens, forwardHt1, forwardHt, maxLen, false, counter);\n    backwardHt = maskRNNHidden(ctx, node, loop, seqLens, backwardHt1, backwardHt, maxLen, true, counter);\n    std::array<nvinfer1::ITensor*, 2> tensors{{forwardHt, backwardHt}};\n    nvinfer1::IConcatenationLayer* concat = N_CHECK(ctx->network()->addConcatenation(tensors.data(), 2));\n    concat->setAxis(0);\n    return N_CHECK(concat->getOutput(0));\n}\n\nnvinfer1::ITensor* getRaggedMask(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, nvinfer1::ILoop* loop,\n    nvinfer1::ITensor* seqLens, nvinfer1::ITensor* maxLen, bool reverse, nvinfer1::ITensor* counter)\n{\n    // Returns a bool tensor which is true where the elements are valid (within the sequence) and false when outside the\n    // sequence.\n    // maxLen must be provided if reverse is true\n    assert(!reverse || maxLen);\n\n    if (!counter)\n    {\n        counter = addLoopCounter(ctx, loop, 0);\n    }\n\n    // ONNX spec currently requires seqLens to be int32\n    counter = castHelper(ctx, counter, nvinfer1::DataType::kINT32);\n\n    // Create Mask\n    nvinfer1::ITensor* seqMask;\n    if (reverse)\n    {\n        counter = getElementWiseResult(\n            ctx, *unsqueezeTensor(ctx, *maxLen, {0}), *counter, nvinfer1::ElementWiseOperation::kSUB);\n        seqMask = getElementWiseResult(ctx, *seqLens, *counter, nvinfer1::ElementWiseOperation::kLESS);\n        seqMask = getUnaryResult(ctx, *seqMask, nvinfer1::UnaryOperation::kNOT);\n    }\n    else\n    {\n        seqMask = getElementWiseResult(ctx, *counter, *seqLens, nvinfer1::ElementWiseOperation::kLESS);\n    }\n    return unsqueezeTensor(ctx, *seqMask, std::vector<int>{0, 2});\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "RNNHelpers.hpp",
          "type": "blob",
          "size": 1.8525390625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include <NvInfer.h>\n#include <string>\n#include <vector>\n\n#include \"TensorOrWeights.hpp\"\n#include \"ImporterContext.hpp\"\n\nnamespace onnx2trt\n{\n\nnvinfer1::ITensor* addRNNInput(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, nvinfer1::ILoop* loop,\n    std::vector<TensorOrWeights>& inputs, const std::string& direction);\n\n// Zeros out invalid timesteps in toMask. maxLen must be provided if reverse is true\nnvinfer1::ITensor* clearMissingSequenceElements(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node,\n    nvinfer1::ILoop* loop, nvinfer1::ITensor* seqLens, nvinfer1::ITensor* toMask, nvinfer1::ITensor* maxLen,\n    bool reverse = false, nvinfer1::ITensor* counter = nullptr);\n\n// Returns a bool tensor which is true during valid timesteps\nnvinfer1::ITensor* getRaggedMask(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, nvinfer1::ILoop* loop,\n    nvinfer1::ITensor* seqLens, nvinfer1::ITensor* maxLen = nullptr, bool reverse = false,\n    nvinfer1::ITensor* counter = nullptr);\n\n// Selects between prevH and Ht to forward previous hidden state through invalid timesteps\nnvinfer1::ITensor* maskRNNHidden(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, nvinfer1::ILoop* loop,\n    nvinfer1::ITensor* seqLens, nvinfer1::ITensor* prevH, nvinfer1::ITensor* Ht, nvinfer1::ITensor* maxLen = nullptr,\n    bool reverse = false, nvinfer1::ITensor* counter = nullptr);\n\n// Splits a bidirectional hidden state into forward and reverse passes, masks each using maskRNNHidden, then\n// concatenates\nnvinfer1::ITensor* maskBidirRNNHidden(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node,\n    nvinfer1::ILoop* loop, nvinfer1::ITensor* seqLens, nvinfer1::ITensor* maxLen, nvinfer1::ITensor* Ht1,\n    nvinfer1::ITensor* Ht, nvinfer1::ITensor* singlePassShape);\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "ShapeTensor.cpp",
          "type": "blob",
          "size": 18.96875,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"ShapeTensor.hpp\"\n#include \"Status.hpp\"\n#include \"TensorOrWeights.hpp\"\n#include \"importerUtils.hpp\"\n#include <algorithm>\n#include <functional>\n\nnamespace onnx2trt\n{\n\nShapeTensor::ShapeTensor(int32_t rank_, std::vector<int64_t>&& values_)\n    : mDepth(0)\n    , mAllValuesKnown(true)\n    , mRank(rank_)\n    , mSize(values_.size())\n    , mValues(std::move(values_))\n{\n    assert((rank_ == 0 || rank_ == 1) && \"shape tensor must have rank 0 or 1\");\n    assert(rank_ > 0 || mValues.size() == 1);\n}\n\n//!\n//! Construct a shape tensor representation for float values.\n//! This is a hack to get shape tensor working with float values with minimal changes.\n//! The constructed ShapeTensor has the following properties:\n//! 1. mAllValuesKnown == false\n//! 2. mValues.size() == 0\n//! 3. mIsFloat == true\n//!\n//! Float shape tensor does not have any constant folding in parser as available to int32_t shape tensors.\n//! Instead, rely on builder for constant folding and build-time simplifications.\n//!\nShapeTensor::ShapeTensor(int32_t rank_, std::vector<float>&& values_)\n    : mDepth(0)\n    , mAllValuesKnown(false)\n    , mRank(rank_)\n    , mSize(values_.size())\n    , mValues({})\n    , mIsFloat{true}\n{\n    assert((rank_ == 0 || rank_ == 1) && \"shape tensor must have rank 0 or 1\");\n    assert(mValues.size() == 0 && \"non-empty floating-point shape tensor with known values not supported\");\n}\n\nShapeTensor::ShapeTensor(ImporterContext* ctx, TensorOrWeights& t)\n    : mDepth(0)\n    , mIsFloat(t.isFp32())\n{\n    if (t.is_tensor())\n    {\n        *this = ShapeTensor(t.tensor());\n    }\n    else\n    {\n        const nvinfer1::Dims d = t.shape();\n        auto const& weights = t.weights();\n        if (isFloat())\n        {\n            *this = ShapeTensor(convertToTensor(t, ctx));\n            return;\n        }\n        assert(0 <= d.nbDims);\n        assert(d.nbDims <= 1 && \"shape tensor must be 0D or 1D\");\n        mRank = d.nbDims;\n        mSize = d.nbDims == 0 ? 1 : d.d[0];\n\n        weightsToVector(weights, &mValues);\n        mAllValuesKnown = true;\n    }\n}\n\nstatic bool hasAllNonNegativeValues(const std::vector<int64_t>& values)\n{\n    return std::all_of(values.begin(), values.end(), [](int x) { return x >= 0; });\n}\n\nShapeTensor::ShapeTensor(nvinfer1::ITensor& t, int depth)\n    : mDepth(depth)\n    , mRank(1)\n    , mTensor(&t)\n    // The check for depth == 0 is needed because when depth > 0, it means *this represents the shape of mTensor, and\n    // shapes always have integral type.\n    , mIsFloat(depth == 0 && t.getType() == nvinfer1::DataType::kFLOAT)\n{\n    const nvinfer1::Dims dims = t.getDimensions();\n    assert((!isFloat() || mDepth == 0) && \"floating-point shape tensor must have depth == 0\");\n\n    switch (mDepth)\n    {\n    case 0:\n        assert(t.getType() == nvinfer1::DataType::kINT32 || t.getType() == nvinfer1::DataType::kINT64\n            || t.getType() == nvinfer1::DataType::kFLOAT);\n        mRank = dims.nbDims;\n        if (mRank == 0)\n        {\n            mSize = 1;\n        }\n        else if (mRank == 1)\n        {\n            mSize = dims.d[0];\n        }\n        else\n        {\n            assert(mRank == -1);\n        }\n        break;\n\n    case 1:\n        if (dims.nbDims >= 0)\n        {\n            mSize = dims.nbDims;\n            mValues.resize(dims.nbDims);\n            std::copy_n(dims.d, dims.nbDims, mValues.begin());\n            mAllValuesKnown = hasAllNonNegativeValues(mValues);\n        }\n        break;\n\n    case 2:\n        mSize = 1;\n        if (dims.nbDims >= 0)\n        {\n            mValues = {dims.nbDims};\n            mAllValuesKnown = hasAllNonNegativeValues(mValues);\n        }\n        break;\n\n    case 3:\n        // Applying IShapeLayer three times always yields a 1D vector containing 1.\n        mDepth = 0;\n        mSize = 1;\n        mValues = {1};\n        mAllValuesKnown = true;\n        mTensor = nullptr;\n        break;\n\n    default:\n        // Though depths greater than 3 could be handled the same as 3, they are\n        // likely a sign of a problem.  Depths less than 0 make no sense.\n        assert(0);\n        break;\n    }\n}\n\nShapeTensor shapeVector(int64_t value)\n{\n    return ShapeTensor(1, std::vector<int64_t>({value}));\n}\n\nShapeTensor shapeScalar(int64_t value)\n{\n    return ShapeTensor(0, std::vector<int64_t>({value}));\n}\n\nbool ShapeTensor::valueKnown(int k) const\n{\n    assert(0 <= k);\n    assert(k < mSize);\n    return allValuesKnown() || (mValues.size() == static_cast<size_t>(mSize) && mValues[k] >= 0);\n}\n\nbool ShapeTensor::isAll(int64_t x) const\n{\n    assert(mDepth >= 0 && \"undefined tensor\");\n    return allValuesKnown() && std::all_of(begin(), end(), [x](int64_t y) { return x == y; });\n}\n\nnvinfer1::ITensor& ShapeTensor::tensor(ImporterContext* ctx) const\n{\n    assert(mDepth >= 0 && \"undefined tensor\");\n    assert(mDepth <= 2);\n    if (!mTensor || mDepth != 0)\n    {\n        // Need to create an ITensor representing *this.\n        if (allValuesKnown())\n        {\n            // Create constant\n            const nvinfer1::Dims dims{rank(), {size()}};\n            // Must create temp weights in the parser in order to keep ownership of weights.\n            auto shapeWeights = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto::INT64, dims);\n            std::memcpy(shapeWeights.values, mValues.data(), mValues.size() * sizeof(int64_t));\n            auto* constLayer = N_CHECK(ctx->network()->addConstant(dims, shapeWeights));\n            ctx->registerLayer(constLayer, \"ONNXTRT_ShapeTensorFromDims\", nullptr);\n            mTensor = N_CHECK(constLayer->getOutput(0));\n            mDepth = 0;\n        }\n        else\n        {\n            assert(mTensor);\n            for (; mDepth > 0; --mDepth)\n            {\n                auto* shapeLayer = N_CHECK(ctx->network()->addShape(*mTensor));\n                ctx->registerLayer(shapeLayer, \"ONNXTRT_ShapeTensor\", nullptr);\n                mTensor = N_CHECK(shapeLayer->getOutput(0));\n                mTensor = castHelper(ctx, mTensor, nvinfer1::DataType::kINT64);\n            }\n        }\n    }\n    return *mTensor;\n}\n\nShapeTensor iotaShapeVector(int32_t n)\n{\n    std::vector<int64_t> values(n);\n    std::iota(values.begin(), values.end(), 0);\n    return ShapeTensor(1, std::move(values));\n}\n\nShapeTensor similar(ImporterContext* ctx, const ShapeTensor& exemplar, int64_t value)\n{\n    return fillShapeVector(ctx, value, shapeOf(exemplar));\n}\n\nShapeTensor fillShapeVector(ImporterContext* ctx, int64_t value, const ShapeTensor& count)\n{\n    assert(count.rank() == 1 && \"implementation assumes 1D size\");\n    assert(count.size() == 1 && \"implementation assumes 1D size of known size\");\n    if (count.allValuesKnown())\n    {\n        return ShapeTensor(1, std::vector<int64_t>(count[0], value));\n    }\n    else\n    {\n        nvinfer1::ISliceLayer* slice\n            = addSlice(ctx, shapeVector(value).tensor(ctx), shapeVector(0), count, shapeVector(0));\n        ctx->registerLayer(slice, \"ONNXTRT_FillShapeVector\", nullptr);\n        auto* sliceOutput = N_CHECK(slice->getOutput(0));\n        return ShapeTensor(*sliceOutput);\n    }\n}\n\nusing nvinfer1::ElementWiseOperation;\n\n//! Helper that implements an elementwise operations on two shape tensors x and y.\n//! f must implement the operation on a pair of int64_t.\n//! commutes should be true f is commutative.\n//! rightIdentity should be the right identity value for f.\nstatic ShapeTensor op(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y, ElementWiseOperation operation,\n    bool commutative, int64_t rightIdentity, const std::function<int64_t(int64_t, int64_t)>&& f)\n{\n    assert(!x.rankKnown() || !y.rankKnown() || x.rank() == y.rank());\n    // Return early for empty-vector operands -- when present, the result is empty too.\n    // Returning early for empty-vector operands simplifies subsequent logic, which\n    // can assume the size of the result is the max of the sizes of the operands.\n    if (x.isEmpty())\n    {\n        return x;\n    }\n    if (y.isEmpty())\n    {\n        return y;\n    }\n    if (x.sizeKnown() && y.sizeKnown())\n    {\n        assert(x.size() == 1 || y.size() == 1 || x.size() == y.size());\n        if (y.isAll(rightIdentity) && y.size() <= x.size())\n        {\n            return x;\n        }\n        if (commutative && x.isAll(rightIdentity) && x.size() <= y.size())\n        {\n            return y;\n        }\n    }\n    if (x.allValuesKnown() && y.allValuesKnown())\n    {\n        assert(!x.isFloat() && !y.isFloat());\n        std::vector<int64_t> values(std::max(x.size(), y.size()));\n        for (size_t i = 0; i < values.size(); ++i)\n        {\n            // The % simulates broadcast rules.\n            values[i] = f(x[i % x.size()], y[i % y.size()]);\n        }\n        return ShapeTensor(x.rank(), std::move(values));\n    }\n    auto* elemLayer = N_CHECK(ctx->network()->addElementWise(x.tensor(ctx), y.tensor(ctx), operation));\n    ctx->registerLayer(elemLayer, \"ONNXTRT_ShapeElementWise\", nullptr);\n    auto* elemOutput = N_CHECK(elemLayer->getOutput(0));\n    return ShapeTensor(*elemOutput, 0);\n}\n\nShapeTensor add(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y)\n{\n    return op(ctx, x, y, ElementWiseOperation::kSUM, true, 0, std::plus<int64_t>());\n}\n\nShapeTensor sub(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y)\n{\n    return op(ctx, x, y, ElementWiseOperation::kSUB, false, 0, std::minus<int64_t>());\n}\n\nShapeTensor mul(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y)\n{\n    return op(ctx, x, y, ElementWiseOperation::kPROD, true, 1, std::multiplies<int64_t>());\n}\n\nShapeTensor min(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y)\n{\n    return op(ctx, x, y, ElementWiseOperation::kMIN, true, std::numeric_limits<int64_t>::max(),\n        [](int64_t x, int64_t y) { return std::min(x, y); });\n}\n\nShapeTensor max(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y)\n{\n    return op(ctx, x, y, ElementWiseOperation::kMAX, true, std::numeric_limits<int64_t>::min(),\n        [](int64_t x, int64_t y) { return std::max(x, y); });\n}\nShapeTensor floorDiv(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y)\n{\n    return op(ctx, x, y, ElementWiseOperation::kFLOOR_DIV, false, 1, [](int64_t x, int64_t y) {\n        assert(y != 0 && \"divisor must be non-zero\");\n        const int64_t d = x / y;\n        return d * y == x ? d : d - ((x < 0) ^ (y < 0));\n    });\n}\n\nShapeTensor broadcast(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y)\n{\n    // max(x,y) works unless x or y is 0.\n    // min(x,y,1) yields 0 if x or y is 0, and 1 otherwise.\n    // So compute max(x,y)*min(x,y,1).\n    return mul(ctx, max(ctx, x, y), min(ctx, x, min(ctx, y, similar(ctx, y, 1))));\n}\n\nShapeTensor product(ImporterContext* ctx, const ShapeTensor& x, int first, int last, int rank)\n{\n    assert(first <= last);\n    ShapeTensor z(rank, std::vector<int64_t>(1, 1));\n    for (int i = first; i < last; ++i)\n    {\n        z = mul(ctx, z, gather(ctx, x, ShapeTensor(rank, std::vector<int64_t>(1, i))));\n    }\n    return z;\n}\n\nShapeTensor concat(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y)\n{\n    assert(!x.rankKnown() || x.rank() == 1);\n    assert(!y.rankKnown() || y.rank() == 1);\n    if (x.sizeKnown() && x.size() == 0)\n    {\n        return y;\n    }\n    if (y.sizeKnown() && y.size() == 0)\n    {\n        return x;\n    }\n    if (x.allValuesKnown() && y.allValuesKnown())\n    {\n        std::vector<int64_t> values(x.size() + y.size());\n        auto p = std::copy(x.begin(), x.end(), values.begin());\n        std::copy(y.begin(), y.end(), p);\n        return ShapeTensor(1, std::move(values));\n    }\n\n    nvinfer1::ITensor* const args[2] = {&x.tensor(ctx), &y.tensor(ctx)};\n    auto* concatLayer = N_CHECK(ctx->network()->addConcatenation(args, 2));\n    ctx->registerLayer(concatLayer, \"ONNXTRT_ShapeConcat\", nullptr);\n    auto* concatOutput = N_CHECK(concatLayer->getOutput(0));\n    return ShapeTensor(*concatOutput);\n}\n\nShapeTensor gather(ImporterContext* ctx, const ShapeTensor& data, const ShapeTensor& indices)\n{\n    assert(data.rank() == 1);\n    if (indices.allValuesKnown()\n        && std::all_of(indices.begin(), indices.end(), [&data](int i) { return data.valueKnown(i); }))\n    {\n        std::vector<int64_t> z(indices.size());\n        std::transform(indices.begin(), indices.end(), z.begin(), [&data](int64_t i) {\n            assert(0 <= i);\n            assert(i < data.size());\n            return data[i];\n        });\n        return ShapeTensor(indices.rank(), std::move(z));\n    }\n    auto* gatherLayer = ctx->network()->addGather(data.tensor(ctx), indices.tensor(ctx), 0);\n    ctx->registerLayer(gatherLayer, \"ONNXTRT_ShapeGather\", nullptr);\n    auto* gatherOutput = N_CHECK(gatherLayer->getOutput(0));\n    return ShapeTensor(*gatherOutput);\n}\n\nShapeTensor castToInt32(ImporterContext* ctx, ShapeTensor const& x)\n{\n    nvinfer1::ILayer* cast = N_CHECK(ctx->network()->addCast(x.tensor(ctx), nvinfer1::DataType::kINT32));\n    ctx->registerLayer(cast, \"ONNXTRT_ShapeCastToInt32\", nullptr);\n    auto castOutput = N_CHECK(cast->getOutput(0));\n    return ShapeTensor(*castOutput);\n}\n\nShapeTensor castToInt64(ImporterContext* ctx, ShapeTensor const& x)\n{\n    nvinfer1::ILayer* cast = N_CHECK(ctx->network()->addCast(x.tensor(ctx), nvinfer1::DataType::kINT64));\n    ctx->registerLayer(cast, \"ONNXTRT_ShapeCastToInt64\", nullptr);\n    auto castOutput = N_CHECK(cast->getOutput(0));\n    return ShapeTensor(*castOutput);\n}\n\nShapeTensor shapeOf(nvinfer1::ITensor& tensor)\n{\n    return ShapeTensor(tensor, 1);\n}\n\nShapeTensor shapeOf(TensorOrWeights& t)\n{\n    if (t.is_tensor())\n    {\n        return shapeOf(t.tensor());\n    }\n    const nvinfer1::Dims& d = t.weights().shape;\n    return ShapeTensor(1, std::vector<int64_t>(d.d, d.d + d.nbDims));\n}\n\nShapeTensor shapeOf(const ShapeTensor& t)\n{\n    assert(t.mDepth >= 0);\n    if (t.mTensor)\n    {\n        return ShapeTensor(*t.mTensor, t.mDepth + 1);\n    }\n    assert(t.rankKnown());\n    assert(t.sizeKnown());\n    // ShapeTensor is either a scalar or vector.\n    // shape of a scalar is an empty tensor.\n    // shape of a vector is a one-element tensor containing the length of the vector.\n    return t.rank() == 0 ? ShapeTensor(0, std::vector<int64_t>{}) : ShapeTensor(1, std::vector<int64_t>{t.size()});\n}\n\nShapeTensor convertTo1D(ImporterContext* ctx, const ShapeTensor& tensor)\n{\n    assert(tensor.rank() == 0);\n    assert(tensor.size() == 1);\n    if (tensor.valueKnown(0))\n    {\n        return shapeVector(tensor[0]);\n    }\n    return ShapeTensor(*N_CHECK(addShuffle(ctx, tensor.tensor(ctx), shapeVector(1))->getOutput(0)));\n}\n\nShapeTensor convertTo0D(ImporterContext* ctx, const ShapeTensor& tensor)\n{\n    if (tensor.size() != 1)\n    {\n        throw std::runtime_error(\"Cannot convert a tensor with size > 1 to a scalar!\");\n    }\n    if (tensor.valueKnown(0))\n    {\n        return shapeScalar(tensor[0]);\n    }\n    auto* layer = N_CHECK(ctx->network()->addShuffle(tensor.tensor(ctx)));\n    layer->setReshapeDimensions(nvinfer1::Dims{0});\n    return ShapeTensor(*N_CHECK(layer->getOutput(0)));\n}\n\n//! If all values of x are known, return Dims with those values,\n//! but throw exception if any value is outside specified bounds.\n//! Otherwise return Dims with zeros.\n//!\n//! The string that should describe the context of the dimensions,\n//! e.g. \"reshape\" or \"fill output\".\nnvinfer1::Dims shapeTensorToDims(const ShapeTensor& x, const char* what, int32_t minAllowed, int32_t maxAllowed)\n{\n    nvinfer1::Dims d{-1, {}};\n    if (x.sizeKnown())\n    {\n        d.nbDims = x.size();\n        if (x.allValuesKnown())\n        {\n            assert(x.size() <= nvinfer1::Dims::MAX_DIMS);\n            for (const auto& dim : x)\n            {\n                if (dim < minAllowed || dim > maxAllowed)\n                {\n                    std::ostringstream msg;\n                    msg << what << \" dimensions have value \" << dim << \" beyond allowed bounds.\" << std::endl;\n                    throw std::runtime_error(msg.str());\n                }\n            }\n            std::copy(x.begin(), x.end(), d.d);\n        }\n    }\n    return d;\n}\n\n//! If not all values in x are known, set layer input specifed by inputIndex\n//! to tensor with value of x.\nstatic void setShapeInputIfDynamic(ImporterContext* ctx, nvinfer1::ILayer* layer, int inputIndex, const ShapeTensor& x)\n{\n    if (!x.allValuesKnown())\n    {\n        layer->setInput(inputIndex, x.tensor(ctx));\n    }\n}\n\nbool operator==(const ShapeTensor& x, const ShapeTensor& y)\n{\n    if (x.allValuesKnown() && y.allValuesKnown())\n    {\n        return x.mValues == y.mValues;\n    }\n    assert(x.mTensor || y.mTensor);\n    return x.mTensor == y.mTensor && x.mDepth == y.mDepth;\n}\n\nnvinfer1::ITensor& reshape(ImporterContext* ctx, nvinfer1::ITensor& data, const ShapeTensor& newShape)\n{\n    const ShapeTensor oldShape = shapeOf(data);\n    if (newShape == oldShape)\n    {\n        return data;\n    }\n    return *N_CHECK(addShuffle(ctx, data, newShape)->getOutput(0));\n}\n\nnvinfer1::IShuffleLayer* addShuffle(\n    ImporterContext* ctx, nvinfer1::ITensor& data, const ShapeTensor& reshapeDims, bool zeroIsPlaceholder)\n{\n    nvinfer1::IShuffleLayer* shuffle = N_CHECK(ctx->network()->addShuffle(data));\n    if (reshapeDims.allValuesKnown())\n    {\n        shuffle->setReshapeDimensions(\n            shapeTensorToDims(reshapeDims, \"reshape\", -1, std::numeric_limits<int32_t>::max()));\n    }\n    else\n    {\n        shuffle->setInput(1, reshapeDims.tensor(ctx));\n    }\n    shuffle->setZeroIsPlaceholder(zeroIsPlaceholder);\n    ctx->registerLayer(shuffle, \"ONNXTRT_ShapeShuffle\", nullptr);\n    return shuffle;\n}\n\nnvinfer1::ISliceLayer* addSlice(ImporterContext* ctx, nvinfer1::ITensor& data, const ShapeTensor& starts,\n    const ShapeTensor& sizes, const ShapeTensor& strides)\n{\n    constexpr int32_t minDim = std::numeric_limits<int32_t>::min();\n    constexpr int32_t maxDim = std::numeric_limits<int32_t>::max();\n    nvinfer1::ISliceLayer* slice = N_CHECK(ctx->network()->addSlice(data,\n        shapeTensorToDims(starts, \"slice start\", minDim, maxDim), shapeTensorToDims(sizes, \"slice size\", 0, maxDim),\n        shapeTensorToDims(strides, \"slide strides\", minDim, maxDim)));\n    setShapeInputIfDynamic(ctx, slice, 1, starts);\n    setShapeInputIfDynamic(ctx, slice, 2, sizes);\n    setShapeInputIfDynamic(ctx, slice, 3, strides);\n    ctx->registerLayer(slice, \"ONNXTRT_ShapeSlice\", nullptr);\n    return slice;\n}\n\nnvinfer1::IFillLayer* addFill(ImporterContext* ctx, const ShapeTensor& shape, nvinfer1::FillOperation op)\n{\n    nvinfer1::IFillLayer* fill = N_CHECK(\n        ctx->network()->addFill(shapeTensorToDims(shape, \"fill output\", 0, std::numeric_limits<int32_t>::max()), op,\n            nvinfer1::DataType::kFLOAT));\n    setShapeInputIfDynamic(ctx, fill, 0, shape);\n    ctx->registerLayer(fill, \"ONNXTRT_ShapeFill\", nullptr);\n    return fill;\n}\n\nstd::ostream& operator<<(std::ostream& stream, const ShapeTensor& x)\n{\n    stream << \"(\";\n    for (int i = 0, e = x.size(); i < e; ++i)\n    {\n        stream << (i ? \", \" : \"\");\n        if (x.valueKnown(i))\n        {\n            stream << x[i];\n        }\n        else\n        {\n            stream << \"_\";\n        }\n    }\n    if (x.size() == 1 && x.rank() == 1)\n    {\n        // Use Python convention to distinguish 1-element vector from a scalar.\n        stream << \",\";\n    }\n    return stream << \")\";\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "ShapeTensor.hpp",
          "type": "blob",
          "size": 8.54296875,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include <NvInfer.h>\n#include <cassert>\n#include <iosfwd>\n#include <vector>\n\nnamespace onnx2trt\n{\n\nclass ImporterContext;\nclass TensorOrWeights;\n\n//! Represents a 0D or 1D tensor of int64_t.\nclass ShapeTensor\n{\npublic:\n    //! Create undefined ShapeTensor.\n    ShapeTensor() = default;\n\n    //! Create ShapeTensor with known rank and int64_t values.\n    ShapeTensor(int32_t rank_, std::vector<int64_t>&& values_);\n\n    //! Create ShapeTensor with known rank and float values.\n    ShapeTensor(int32_t rank_, std::vector<float>&& values_);\n\n    //! Create ShapeTensor representing value of TensorOrWeights.\n    ShapeTensor(ImporterContext* ctx, TensorOrWeights& t);\n\n    //! Construct ShapeTensor equivalent to applying IShapeLayer depth times.\n    //! The depth may be in [0,3].\n    explicit ShapeTensor(nvinfer1::ITensor& t, int depth = 0);\n\n    //! True if rank is known.\n    bool rankKnown() const\n    {\n        return mRank != kRANK_UNKNOWN;\n    }\n\n    //! Number of dimensions.  Always 0 or 1.\n    int32_t rank() const\n    {\n        assert(rankKnown());\n        return mRank;\n    }\n\n    //! True if number of elements in tensor is known.\n    bool sizeKnown() const\n    {\n        return mSize != kSIZE_UNKNOWN;\n    }\n\n    //! Number of elements in the tensor.  Asserts that sizeKnown()==true.\n    int32_t size() const\n    {\n        assert(sizeKnown());\n        return mSize;\n    }\n\n    //! True if tensor is known to be an empty vector.\n    bool isEmpty() const\n    {\n        // No need to check rank because if rank is 0, then mSize==1,\n        // and if rank is unknown, mSize = kSIZE_UNKNOWN.\n        return mSize == 0;\n    }\n\n    //! True if all element values are known.\n    bool allValuesKnown() const\n    {\n        return mAllValuesKnown;\n    }\n\n    //! True if all element values equal the given value.\n    bool isAll(int64_t value) const;\n\n    //! True if floating-point shape tensor.\n    bool isFloat() const\n    {\n        return mIsFloat;\n    }\n\n    using const_iterator = std::vector<int64_t>::const_iterator;\n\n    //! Iterator pointing to beginning of sequence of element values.\n    //! Requires that allValuesKnown() is true.\n    const_iterator begin() const\n    {\n        assert(mAllValuesKnown);\n        return mValues.begin();\n    }\n\n    //! Iterator pointing to end of sequence of element values.\n    //! Requires that allValuesKnown() is true.\n    const_iterator end() const\n    {\n        assert(mAllValuesKnown);\n        return mValues.end();\n    }\n\n    //! True if operator[](k) is valid.\n    bool valueKnown(int k) const;\n\n    //! Return kth value.\n    //! For a 0D tensor, k must be 0.\n    //! Requires that valueKnown(k) is true.\n    int64_t operator[](int k) const\n    {\n        assert(valueKnown(k));\n        return mValues[k];\n    }\n\n    //! Return true if x and y always have the same value.\n    friend bool operator==(const ShapeTensor& x, const ShapeTensor& y);\n    friend ShapeTensor shapeOf(const ShapeTensor& t);\n\n    //! Get TensorRT tensor representation.\n    nvinfer1::ITensor& tensor(ImporterContext* ctx) const;\n\nprivate:\n    //! Number of IShapeLayer to apply to mTensor to get ITensor representing value of *this.\n    //! -1 for undefined *this, a value in [0,2] otherwise.\n    //! 0: *this represents value of the tensor (always 0D or 1D)\n    //! 1: *this represents shape of mTensor (always 1D)\n    //! 2: *this represents rank of mTensor (always 1D tensor of length 1)\n    mutable int8_t mDepth{-1};\n\n    //! True if all values are known.\n    bool mAllValuesKnown{false};\n\n    static constexpr int kRANK_UNKNOWN = -1;\n    static constexpr int kSIZE_UNKNOWN = -1;\n\n    //! Rank of *this.\n    //! Always -1, 0 or 1.\n    int8_t mRank{kRANK_UNKNOWN};\n\n    //! Number of elements in the tensor, or -1 if unknown.\n    int32_t mSize{kSIZE_UNKNOWN};\n\n    //! Must be non-null if mAllValuesKnown.\n    mutable nvinfer1::ITensor* mTensor{nullptr};\n\n    //! Values of elements if some might be known.\n    //! mValues.size() is always zero or equal to mSize.\n    //! When mAllValuesKnown==true, all the values in mValues are correct\n    //! and mValues.size() == mSize.\n    //! When mAllValuesKnown==false, only the non-negative values in mValues\n    //! are guaranteed to be correct, and only so if mValues.size() == mSize.\n    std::vector<int64_t> mValues{};\n\n    bool mIsFloat{false};\n};\n\n//! Print ShapeTensor.  Unknown values are printed as _.\nstd::ostream& operator<<(std::ostream& stream, const ShapeTensor& x);\n\n//! Create 1D ShapeTensor of length n filled with value.\n//! count must be 1D ShapeTensor of size 1.\nShapeTensor fillShapeVector(ImporterContext* ctx, int64_t value, const ShapeTensor& count);\n\n//! Create 1D ShapeTensor of length 1 containing given value.\nShapeTensor shapeVector(int64_t value);\n\n//! Create 0D ShapeTensor containing the given value.\nShapeTensor shapeScalar(int64_t value);\n\n//! Create 1D ShapeTensor containing [0,n).\nShapeTensor iotaShapeVector(int32_t n);\n\n//! Create ShapeTensor filled with value that has same shape as exemplar.\n//! The exemplar must be 1D.\nShapeTensor similar(ImporterContext* ctx, const ShapeTensor& exemplar, int64_t value);\n\n//! Elementwise addition\nShapeTensor add(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y);\n\n//! Elementwise subtraction\nShapeTensor sub(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y);\n\n//! Elementwise multiplication\nShapeTensor mul(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y);\n\n//! Elementwise min\nShapeTensor min(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y);\n\n//! Elementwise max\nShapeTensor max(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y);\n\n//! Elementwise floor division\nShapeTensor floorDiv(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y);\n\n//! Elementwise f, for a partial function f defined by:\n//! f(x,x) = x\n//! f(1,x) = x\n//! f(x,1) = x\n//! Undefined otherwise or if x < 0.\nShapeTensor broadcast(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y);\n\n//! Return product of x[i] for i in [first..last), as 0D or one-element 1D tensor of given rank.\nShapeTensor product(ImporterContext* ctx, const ShapeTensor& x, int first, int last, int rank);\n\n//! Gather where data is 1D tensor and indices can be 0D or 1D\nShapeTensor gather(ImporterContext* ctx, const ShapeTensor& data, const ShapeTensor& indices);\n\n//! Concatenation of two 1D tensors\nShapeTensor concat(ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y);\n\n//! Cast to int32_t shape tensor.\nShapeTensor castToInt32(ImporterContext* ctx, ShapeTensor const& x);\n\n//! Cast to int64_t shape tensor.\nShapeTensor castToInt64(ImporterContext* ctx, ShapeTensor const& x);\n\n//! Return gather(concat(x,y),subscripts)\ninline ShapeTensor interlace(\n    ImporterContext* ctx, const ShapeTensor& x, const ShapeTensor& y, const ShapeTensor& subscripts)\n{\n    return gather(ctx, concat(ctx, x, y), subscripts);\n}\n\n//! Return shape of a tensor.\nShapeTensor shapeOf(nvinfer1::ITensor& tensor);\nShapeTensor shapeOf(const ShapeTensor& tensor);\nShapeTensor shapeOf(TensorOrWeights& t);\n\n//! Reshape 0D tensor to 1D tensor.\nShapeTensor convertTo1D(ImporterContext* ctx, const ShapeTensor& tensor);\n\n//! Reshape single value 1D tensor to a 0D tensor.\nShapeTensor convertTo0D(ImporterContext* ctx, const ShapeTensor& tensor);\n\n//! Convert ShapeTensor to Dims, with bounds checking.\nnvinfer1::Dims shapeTensorToDims(const ShapeTensor& x, const char* what, int32_t minAllowed, int32_t maxAllowed);\n\n//! Add an ISliceLayer.\nnvinfer1::ISliceLayer* addSlice(ImporterContext* ctx, nvinfer1::ITensor& data, const ShapeTensor& starts,\n    const ShapeTensor& sizes, const ShapeTensor& strides);\n\n//! Add an IShuffleLayer.\n//! If the result does not need to have its parameters changed, and\n//! optimizing the no-op case away is okay, use function reshape instead.\n//!\n//! In general the default zeroIsPlaceholder=false should be used so\n//! that reshaping to empty tensors works correctly.  Calling with\n//! zeroIsPlaceholder=true should happen only when replicating the\n//! semantics of the ONNX Reshape operator.\nnvinfer1::IShuffleLayer* addShuffle(\n    ImporterContext* ctx, nvinfer1::ITensor& data, const ShapeTensor& reshapeDims, bool zeroIsPlaceholder = false);\n\n//! Add an IFillLayer.\nnvinfer1::IFillLayer* addFill(ImporterContext* ctx, const ShapeTensor& shape, nvinfer1::FillOperation op);\n\n//! Reshape a tensor.\n//!\n//! Treats any zeros in newShape as dimensions, not placeholders.\n//! Implementation note: does not insert shuffle if it's a no-op.\nnvinfer1::ITensor& reshape(ImporterContext* ctx, nvinfer1::ITensor& data, const ShapeTensor& newShape);\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "ShapedWeights.cpp",
          "type": "blob",
          "size": 1.5263671875,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"ShapedWeights.hpp\"\n#include \"importerUtils.hpp\"\n#include <cstdint>\n#include <cstring>\n#include <limits>\n\nnamespace onnx2trt\n{\n\nsize_t ShapedWeights::count() const\n{\n    assert(shape.nbDims >= 0);\n    size_t c = 1;\n    for (int32_t i = 0; i < this->shape.nbDims; ++i)\n    {\n        if (shape.d[i] == 0)\n        {\n            c = 0;\n            break;\n        }\n        if (c > std::numeric_limits<size_t>::max() / shape.d[i])\n        {\n            throw std::runtime_error(\"Count of weights exceeds maximum!\");\n        }\n        c *= this->shape.d[i];\n    }\n    return c;\n}\n\nShapedWeights ShapedWeights::empty(DataType type)\n{\n    return ShapedWeights(type, nullptr, nvinfer1::Dims{1, {0}});\n}\n\nShapedWeights::ShapedWeights(DataType type_, void* values_, nvinfer1::Dims shape_)\n    : type(type_)\n    , values(values_)\n    , shape(shape_)\n{\n    // Note: this->shape.type[] is not used\n}\n\nsize_t ShapedWeights::size_bytes() const\n{\n    return getTensorOrWeightsSizeBytes(this->count(), this->type);\n}\n\nShapedWeights::operator bool() const\n{\n    return (bool) this->values;\n}\n\nShapedWeights::operator nvinfer1::Weights() const\n{\n    nvinfer1::Weights w{};\n    w.values = this->values;\n    bool supported_type = convertDtype(this->type, &w.type);\n    (void) supported_type;\n    assert(supported_type);\n    w.count = this->count();\n    return w;\n}\n\nconst char* ShapedWeights::getName() const\n{\n    return this->name;\n}\n\nvoid ShapedWeights::setName(const char* n)\n{\n    this->name = n;\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "ShapedWeights.hpp",
          "type": "blob",
          "size": 1.31640625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include <NvInfer.h>\n#include <onnx/onnx_pb.h>\n\nnamespace onnx2trt\n{\n\nclass ShapedWeights\n{\npublic:\n    using DataType = int32_t;\n\n    //! Create 1D zero-length ShapedWeights of given type, count()==0, and values=nullptr.\n    static ShapedWeights empty(DataType type);\n\n    //! Construct ShapedWeights that is not expected to be usable,\n    //! except with `operator=` and method `setName()`.\n    ShapedWeights() = default;\n\n    explicit ShapedWeights(DataType type, void* values, nvinfer1::Dims shape_);\n\n    size_t count() const;\n\n    size_t size_bytes() const;\n\n    const char* getName() const;\n\n    void setName(const char* name);\n\n    //! True if values exist.\n    explicit operator bool() const;\n\n    operator nvinfer1::Weights() const;\n\n    template <typename T>\n    T& at(size_t index)\n    {\n        assert(values && index >= 0 && index < count());\n        return static_cast<T*>(values)[index];\n    }\n\n    template <typename T>\n    const T& at(size_t index) const\n    {\n        assert(values && index >= 0 && index < count());\n        return static_cast<const T*>(values)[index];\n    }\n\npublic:\n    DataType type{static_cast<DataType>(-1)};\n    void* values{nullptr};\n    nvinfer1::Dims shape{-1, {}};\n    const char* name{};\n};\n\nclass ImporterContext;\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "Status.hpp",
          "type": "blob",
          "size": 16.4228515625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include \"NvOnnxParser.h\"\n\n#include <algorithm>\n#include <iterator>\n#include <cassert>\n#include <sstream>\n#include <string>\n\n#ifndef ENABLE_STD_PLUGIN\n#define ENABLE_STD_PLUGIN 1\n#endif // ENABLE_STD_PLUGIN\n\n#ifndef ENABLE_SAFE_PLUGIN\n#define ENABLE_SAFE_PLUGIN 0\n#endif // ENABLE_SAFE_PLUGIN\n\n#ifndef USE_LITE_PROTOBUF\n#define USE_LITE_PROTOBUF 0\n#endif // USE_LITE_PROTOBUF\n\n// Used to strip out build path information from debug prints\n#if defined(SOURCE_LENGTH)\n#define __FILENAME__ (__FILE__ + SOURCE_LENGTH)\n#else\n#define __FILENAME__ (__FILE__)\n#endif\n\n// Logging macros\n#define LOG(msg, severity)                                                                                             \\\n    do                                                                                                                 \\\n    {                                                                                                                  \\\n        std::stringstream ss{};                                                                                        \\\n        if (severity <= nvinfer1::ILogger::Severity::kWARNING)                                                         \\\n            ss << __FILENAME__ << \":\" << __LINE__ << \": \";                                                             \\\n        ss << msg;                                                                                                     \\\n        ctx->logger().log(severity, ss.str().c_str());                                                                 \\\n    } while (0)\n\n#define LOG_VERBOSE(msg) LOG(msg, nvinfer1::ILogger::Severity::kVERBOSE)\n#define LOG_INFO(msg) LOG(msg, nvinfer1::ILogger::Severity::kINFO)\n#define LOG_WARNING(msg) LOG(msg, nvinfer1::ILogger::Severity::kWARNING)\n#define LOG_ERROR(msg) LOG(msg, nvinfer1::ILogger::Severity::kERROR)\n\n#define MAKE_ERROR(desc, code) onnx2trt::Status((code), (desc), __FILENAME__, __LINE__, __func__)\n\n#define ASSERT(condition, error_code)                                                                                  \\\n    do                                                                                                                 \\\n    {                                                                                                                  \\\n        if (!(condition))                                                                                              \\\n        {                                                                                                              \\\n            return MAKE_ERROR(\"Assertion failed: \" #condition, (error_code));                                          \\\n        }                                                                                                              \\\n    } while (0)\n\n#define MAKE_NODE_ERROR(desc, code, node, index)                                                                       \\\n    onnx2trt::Status((code), (desc), __FILENAME__, __LINE__, __func__, (index), (node.name()), (node.op_type()))\n\n#define ASSERT_NODE(condition, msg, node, index, error_code)                                                           \\\n    do                                                                                                                 \\\n    {                                                                                                                  \\\n        if (!(condition))                                                                                              \\\n        {                                                                                                              \\\n            std::stringstream error;                                                                                   \\\n            error << \"Assertion failed: \" << #condition << \": \" << msg;                                                \\\n            return MAKE_NODE_ERROR((error.str()), (error_code), node, index);                                          \\\n        }                                                                                                              \\\n    } while (0)\n\n#define MAKE_STATIC_ERROR(desc, code, node, index)                                                                     \\\n    onnx2trt::Status((code), (desc), __FILENAME__, __LINE__, __func__, (index), (node.name()), (node.op_type()))\n\n#define ADD_STATIC_ERROR(desc, code, node, index, error_list)                                                          \\\n    do                                                                                                                 \\\n    {                                                                                                                  \\\n        size_t stackSize = ctx->localFunctionStack().size();                                                           \\\n        std::vector<std::string> localFunctionStackString{};                                                           \\\n        std::vector<char const*> localFunctionStackChar{};                                                             \\\n        for (size_t i = 0; i < stackSize; i++)                                                                         \\\n        {                                                                                                              \\\n            auto const& func = ctx->localFunctionStack()[i];                                                           \\\n            localFunctionStackString.push_back(func.nodeName + \" (\" + func.functionName + \")\");                        \\\n        }                                                                                                              \\\n        ctx->localFunctionErrors().push_back(localFunctionStackString);                                                \\\n        for (size_t i = 0; i < stackSize; i++)                                                                         \\\n        {                                                                                                              \\\n            localFunctionStackChar.push_back(ctx->localFunctionErrors().back()[i].c_str());                            \\\n        }                                                                                                              \\\n        error_list.push_back(onnx2trt::Status((code), (desc), __FILENAME__, __LINE__, __func__, (index),               \\\n            (node.name()), (node.op_type()), localFunctionStackChar));                                                 \\\n    } while (0)\n\n#define STATIC_CHECK(condition, error_code, node, error_list, index)                                                   \\\n    do                                                                                                                 \\\n    {                                                                                                                  \\\n        if (!(condition))                                                                                              \\\n        {                                                                                                              \\\n            ADD_STATIC_ERROR(#condition, (error_code), node, index, error_list);                                       \\\n        }                                                                                                              \\\n    } while (0)\n\n#define MAKE_INPUT_ERROR(desc, code, name) Status((code), (desc), name, __LINE__, __func__)\n\n#define CHECK_INPUT(condition, error_code, name, error_list)                                                           \\\n    do                                                                                                                 \\\n    {                                                                                                                  \\\n        if (!(condition))                                                                                              \\\n        {                                                                                                              \\\n            error_list.push_back(MAKE_INPUT_ERROR(\"Assertion failed: \" #condition, (error_code), (name)));             \\\n        }                                                                                                              \\\n    } while (0)\n\n#define ASSERT_C(condition, error_code)                                                                                \\\n    do                                                                                                                 \\\n    {                                                                                                                  \\\n        if (!(condition))                                                                                              \\\n        {                                                                                                              \\\n            return error_code;                                                                                         \\\n        }                                                                                                              \\\n    } while (0)\n\n#define GET_VALUE(value_or_error_, result_ptr)                                                                         \\\n    do                                                                                                                 \\\n    {                                                                                                                  \\\n        auto const& value_or_error = value_or_error_;                                                                  \\\n        if (value_or_error.is_error())                                                                                 \\\n        {                                                                                                              \\\n            return value_or_error.error();                                                                             \\\n        }                                                                                                              \\\n        else                                                                                                           \\\n        {                                                                                                              \\\n            *result_ptr = value_or_error.value();                                                                      \\\n        }                                                                                                              \\\n    } while (0)\n\n#define CHECK_STATUS(call)                                                                                             \\\n    do                                                                                                                 \\\n    {                                                                                                                  \\\n        onnx2trt::Status status = call;                                                                                \\\n        if (!status.is_success())                                                                                      \\\n        {                                                                                                              \\\n            return status;                                                                                             \\\n        }                                                                                                              \\\n    } while (0)\n\n// Nullptr check for added layers and tensors. All added layers and their output tensors\n// should be non-null, so throw an exception here if it is null. This exception\n// will be caught by the parseNode function.\ntemplate <typename T>\nT* N_CHECK(T* inputPtr)\n{\n    if (!inputPtr)\n    {\n        throw std::runtime_error(\"Internal Error!\");\n    }\n    return inputPtr;\n}\n\n// Overloads of operator<< on TensorRT types must be defined inside nvinfer1\n// so that argument-dependent lookup works as expected. Declared static to\n// avoid symbol clashing when statically linking with other TensorRT libraries\nnamespace nvinfer1\n{\n\ntemplate <typename T>\nstatic std::ostream& printSequence(std::ostream& stream, const T* begin, int count)\n{\n    stream << \"(\";\n    if (count > 0)\n    {\n        std::copy_n(begin, count - 1, std::ostream_iterator<T>(stream, \", \"));\n        stream << begin[count - 1];\n    }\n    stream << \")\";\n    return stream;\n}\n\nstatic std::ostream& operator<<(std::ostream& stream, nvinfer1::Dims const& shape)\n{\n    return printSequence(stream, shape.d, shape.nbDims);\n}\n\nstatic std::ostream& operator<<(std::ostream& stream, nvinfer1::Permutation const& perm)\n{\n    return printSequence(stream, perm.order, nvinfer1::Dims::MAX_DIMS);\n}\n\nstatic std::ostream& operator<<(std::ostream& stream, nvinfer1::DataType const& dtype)\n{\n    switch (dtype)\n    {\n    case nvinfer1::DataType::kFLOAT: return stream << \"float32\";\n    case nvinfer1::DataType::kHALF: return stream << \"float16\";\n    case nvinfer1::DataType::kBF16: return stream << \"bfloat16\";\n    case nvinfer1::DataType::kINT8: return stream << \"int8\";\n    case nvinfer1::DataType::kUINT8: return stream << \"uint8\";\n    case nvinfer1::DataType::kINT32: return stream << \"int32\";\n    case nvinfer1::DataType::kINT64: return stream << \"int64\";\n    case nvinfer1::DataType::kBOOL: return stream << \"bool\";\n    case nvinfer1::DataType::kFP8: return stream << \"float8\";\n    case nvinfer1::DataType::kINT4: return stream << \"int4\";\n\n    default: throw std::runtime_error(\"Unknown dtype\");\n    }\n}\n\n} // namespace nvinfer1\n\nnamespace onnx2trt\n{\n\nusing nvonnxparser::ErrorCode;\n\nclass Status : public nvonnxparser::IParserError\n{\n    ErrorCode _code;\n    std::string _desc;\n    std::string _file;\n    int _line;\n    std::string _func;\n    int _node;\n    std::string _nodeName;\n    std::string _nodeOperator;\n    std::vector<char const*> _localFunctionStack;\n\npublic:\n    static Status success()\n    {\n        return Status(ErrorCode::kSUCCESS);\n    }\n    Status() {}\n    explicit Status(ErrorCode code, std::string desc = \"\", std::string file = \"\", int line = 0, std::string func = \"\",\n        int node = -1, std::string nodeName = \"\", std::string nodeOperator = \"\",\n        std::vector<char const*> localFunctionStack = {})\n        : _code(code)\n        , _desc(desc)\n        , _file(file)\n        , _line(line)\n        , _func(func)\n        , _node(node)\n        , _nodeName(nodeName)\n        , _nodeOperator(nodeOperator)\n        , _localFunctionStack(localFunctionStack)\n    {\n    }\n    ErrorCode code() const override\n    {\n        return _code;\n    }\n    char const* desc() const override\n    {\n        return _desc.c_str();\n    }\n    char const* file() const override\n    {\n        return _file.c_str();\n    }\n    int line() const override\n    {\n        return _line;\n    }\n    char const* func() const override\n    {\n        return _func.c_str();\n    }\n    int node() const override\n    {\n        return _node;\n    }\n    bool is_error() const\n    {\n        return _code != ErrorCode::kSUCCESS;\n    }\n    bool is_success() const\n    {\n        return _code == ErrorCode::kSUCCESS;\n    }\n    void setNode(int node)\n    {\n        _node = node;\n    }\n    char const* nodeName() const override\n    {\n        return _nodeName.c_str();\n    }\n    char const* nodeOperator() const override\n    {\n        return _nodeOperator.c_str();\n    }\n    char const* const* localFunctionStack() const override\n    {\n        return _localFunctionStack.data();\n    }\n    int32_t localFunctionStackSize() const override\n    {\n        return _localFunctionStack.size();\n    }\n};\n\ntemplate <typename T>\nclass ValueOrStatus\n{\n    bool _is_error;\n    T _value;\n    Status _error;\n\npublic:\n    ValueOrStatus(T const& value)\n        : _is_error(false)\n        , _value(value)\n        , _error(Status::success())\n    {\n    }\n    ValueOrStatus(T&& value)\n        : _is_error(false)\n        , _value(value)\n        , _error(Status::success())\n    {\n    }\n    ValueOrStatus(Status const& error)\n        : _is_error(true)\n        , _error(error)\n    {\n    }\n    ValueOrStatus(Status&& error)\n        : _is_error(true)\n        , _error(error)\n    {\n    }\n    bool is_error() const\n    {\n        return _is_error;\n    }\n    T const& value() const\n    {\n        assert(!_is_error);\n        return _value;\n    }\n    T& value()\n    {\n        assert(!_is_error);\n        return _value;\n    }\n    Status const& error() const\n    {\n        assert(_is_error);\n        return _error;\n    }\n};\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "TensorOrWeights.cpp",
          "type": "blob",
          "size": 3.89453125,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"TensorOrWeights.hpp\"\n#include <cassert>\n\nnamespace onnx2trt\n{\n\nstd::string TensorOrWeights::getType() const\n{\n    if (is_tensor())\n    {\n        switch (tensor().getType())\n        {\n        case nvinfer1::DataType::kFLOAT: return \"FLOAT\";\n        case nvinfer1::DataType::kHALF: return \"HALF\";\n        case nvinfer1::DataType::kBF16: return \"BF16\";\n        case nvinfer1::DataType::kINT8: return \"INT8\";\n        case nvinfer1::DataType::kUINT8: return \"UINT8\";\n        case nvinfer1::DataType::kINT32: return \"INT32\";\n        case nvinfer1::DataType::kINT64: return \"INT64\";\n        case nvinfer1::DataType::kBOOL: return \"BOOL\";\n        case nvinfer1::DataType::kFP8: return \"FP8\";\n        case nvinfer1::DataType::kINT4: return \"INT4\";\n        }\n    }\n    else\n    {\n        switch (weights().type)\n        {\n        // Demote double to float.\n        case ::ONNX_NAMESPACE::TensorProto::DOUBLE: return \"FLOAT\";\n        case ::ONNX_NAMESPACE::TensorProto::FLOAT: return \"FLOAT\";\n        case ::ONNX_NAMESPACE::TensorProto::INT8: return \"INT8\";\n        case ::ONNX_NAMESPACE::TensorProto::UINT8: return \"UINT8\";\n        case ::ONNX_NAMESPACE::TensorProto::FLOAT16: return \"HALF\";\n        case ::ONNX_NAMESPACE::TensorProto::BFLOAT16: return \"BF16\";\n        case ::ONNX_NAMESPACE::TensorProto::BOOL: return \"BOOL\";\n        case ::ONNX_NAMESPACE::TensorProto::INT32: return \"INT32\";\n        case ::ONNX_NAMESPACE::TensorProto::INT64: return \"INT64\";\n        case ::ONNX_NAMESPACE::TensorProto::FLOAT8E4M3FN: return \"FP8\";\n        case ::ONNX_NAMESPACE::TensorProto::INT4: return \"INT4\";\n        }\n    }\n    return \"UNKNOWN TYPE\";\n}\n\nnvinfer1::DataType TensorOrWeights::convertONNXDataType(ShapedWeights::DataType datatype) const\n{\n    switch (datatype)\n    {\n        case ::ONNX_NAMESPACE::TensorProto::DOUBLE: return nvinfer1::DataType::kFLOAT;\n        case ::ONNX_NAMESPACE::TensorProto::FLOAT: return nvinfer1::DataType::kFLOAT;\n        case ::ONNX_NAMESPACE::TensorProto::INT8: return nvinfer1::DataType::kINT8;\n        case ::ONNX_NAMESPACE::TensorProto::UINT8: return nvinfer1::DataType::kUINT8;\n        case ::ONNX_NAMESPACE::TensorProto::FLOAT16: return nvinfer1::DataType::kHALF;\n        case ::ONNX_NAMESPACE::TensorProto::BFLOAT16: return nvinfer1::DataType::kBF16;\n        case ::ONNX_NAMESPACE::TensorProto::BOOL: return nvinfer1::DataType::kBOOL;\n        case ::ONNX_NAMESPACE::TensorProto::INT32: return nvinfer1::DataType::kINT32;\n        case ::ONNX_NAMESPACE::TensorProto::INT64: return nvinfer1::DataType::kINT64;\n        case ::ONNX_NAMESPACE::TensorProto::FLOAT8E4M3FN: return nvinfer1::DataType::kFP8;\n        case ::ONNX_NAMESPACE::TensorProto::INT4: return nvinfer1::DataType::kINT4;\n        }\n        assert(false && \"Unknown datatype\");\n        return nvinfer1::DataType::kFLOAT;\n}\n\nShapedWeights::DataType TensorOrWeights::convertTRTDataType(nvinfer1::DataType datatype) const\n{\n    switch (datatype)\n    {\n        case nvinfer1::DataType::kFLOAT: return ::ONNX_NAMESPACE::TensorProto::FLOAT;\n        case nvinfer1::DataType::kINT8: return ::ONNX_NAMESPACE::TensorProto::INT8;\n        case nvinfer1::DataType::kUINT8: return ::ONNX_NAMESPACE::TensorProto::UINT8;\n        case nvinfer1::DataType::kHALF: return ::ONNX_NAMESPACE::TensorProto::FLOAT16;\n        case nvinfer1::DataType::kBF16: return ::ONNX_NAMESPACE::TensorProto::BFLOAT16;\n        case nvinfer1::DataType::kBOOL: return ::ONNX_NAMESPACE::TensorProto::BOOL;\n        case nvinfer1::DataType::kINT32: return ::ONNX_NAMESPACE::TensorProto::INT32;\n        case nvinfer1::DataType::kINT64: return ::ONNX_NAMESPACE::TensorProto::INT64;\n        case nvinfer1::DataType::kFP8: return ::ONNX_NAMESPACE::TensorProto::FLOAT8E4M3FN;\n        case nvinfer1::DataType::kINT4: return ::ONNX_NAMESPACE::TensorProto::INT4;\n        }\n        assert(false && \"Unknown datatype\");\n        return ::ONNX_NAMESPACE::TensorProto::FLOAT;\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "TensorOrWeights.hpp",
          "type": "blob",
          "size": 4.4951171875,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include \"ShapedWeights.hpp\"\n#include <NvInfer.h>\n#include <cassert>\n#include <stdexcept>\n\nnamespace onnx2trt\n{\n\n//! Abstract representation of a tensor, which might be a nvinfer1::ITensor or ShapedWeights.\nclass TensorOrWeights\n{\n    union\n    {\n        nvinfer1::ITensor* _tensor;\n        ShapedWeights _weights;\n    };\n    enum\n    {\n        NODE_TENSOR,\n        NODE_WEIGHTS\n    } _variant;\n\npublic:\n    //! Represents \"null tensor\", which is used to denote \"missing tensor\".\n    TensorOrWeights()\n        : _tensor(nullptr)\n        , _variant(NODE_TENSOR)\n    {\n    }\n    TensorOrWeights(nvinfer1::ITensor* tensor)\n        : _tensor(tensor)\n        , _variant(NODE_TENSOR)\n    {\n    }\n    TensorOrWeights(ShapedWeights const& weights)\n        : _weights(weights)\n        , _variant(NODE_WEIGHTS)\n    {\n    }\n    bool is_tensor() const\n    {\n        return _variant == NODE_TENSOR;\n    }\n    bool is_weights() const\n    {\n        return _variant == NODE_WEIGHTS;\n    }\n    bool isNullTensor() const\n    {\n        return is_tensor() && _tensor == nullptr;\n    }\n    nvinfer1::ITensor& tensor()\n    {\n        if (is_weights() || isNullTensor())\n        {\n            throw std::runtime_error(\"Trying to access weights or a null tensor!\");\n        }\n        return *_tensor;\n    }\n    nvinfer1::ITensor const& tensor() const\n    {\n        if (is_weights() || isNullTensor())\n        {\n            throw std::runtime_error(\"Trying to access weights or a null tensor!\");\n        }\n        return *_tensor;\n    }\n    ShapedWeights& weights()\n    {\n        if (is_tensor())\n        {\n            throw std::runtime_error(\"Trying to access a null weights!\");\n        }\n        return _weights;\n    }\n    ShapedWeights const& weights() const\n    {\n        if (is_tensor())\n        {\n            throw std::runtime_error(\"Trying to access a null weights!\");\n        }\n        return _weights;\n    }\n    nvinfer1::Dims shape() const\n    {\n        return is_tensor() ? tensor().getDimensions() : weights().shape;\n    }\n    explicit operator bool() const\n    {\n        return is_tensor() ? _tensor != nullptr : static_cast<bool>(_weights);\n    }\n    bool isFp32() const\n    {\n        return is_tensor() ? tensor().getType() == nvinfer1::DataType::kFLOAT\n                           : weights().type == ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT;\n    }\n    bool isFp16() const\n    {\n        return is_tensor() ? tensor().getType() == nvinfer1::DataType::kHALF\n                    : weights().type == ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT16;\n    }\n    bool isBFp16() const\n    {\n        return is_tensor() ? tensor().getType() == nvinfer1::DataType::kBF16\n                    : weights().type == ::ONNX_NAMESPACE::TensorProto_DataType_BFLOAT16;\n    }\n    bool isInt32() const\n    {\n        return is_tensor() ? tensor().getType() == nvinfer1::DataType::kINT32\n                           : weights().type == ::ONNX_NAMESPACE::TensorProto_DataType_INT32;\n    }\n    bool isInt64() const\n    {\n        return is_tensor() ? tensor().getType() == nvinfer1::DataType::kINT64\n                           : weights().type == ::ONNX_NAMESPACE::TensorProto_DataType_INT64;\n    }\n    bool isInt8() const\n    {\n        return is_tensor() ? tensor().getType() == nvinfer1::DataType::kINT8\n                           : weights().type == ::ONNX_NAMESPACE::TensorProto_DataType_INT8;\n    }\n    bool isBool() const\n    {\n        return is_tensor() ? tensor().getType() == nvinfer1::DataType::kBOOL : weights().type == ::ONNX_NAMESPACE::TensorProto_DataType_BOOL;\n    }\n    bool isFp8() const\n    {\n        return is_tensor() ? tensor().getType() == nvinfer1::DataType::kFP8 : weights().type == ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT8E4M3FN;\n    }\n    std::string getName() const\n    {\n        return is_tensor() ? tensor().getName() : weights().getName();\n    }\n    std::string getType() const;\n\n    nvinfer1::DataType convertONNXDataType(ShapedWeights::DataType datatype) const;\n\n    ShapedWeights::DataType convertTRTDataType(nvinfer1::DataType datatype) const;\n\n    nvinfer1::DataType getDataType() const\n    {\n        if (is_tensor())\n        {\n            return tensor().getType();\n        }\n        else\n        {\n            return convertONNXDataType(weights().type);\n        }\n    }\n\n    ShapedWeights::DataType getONNXDataType() const\n    {\n        if (is_tensor())\n        {\n            return convertTRTDataType(tensor().getType());\n        }\n        else\n        {\n            return weights().type;\n        }\n    }\n};\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "WeightsContext.cpp",
          "type": "blob",
          "size": 18.49609375,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"WeightsContext.hpp\"\n#include <algorithm>\n#include <cstdlib>\n#include <cstring>\n#include <fstream>\n#include <limits>\n\nnamespace onnx2trt\n{\n\nvoid* WeightsContext::ownWeights(\n    void const* weightValues, const ShapedWeights::DataType dataType, nvinfer1::Dims const& shape, const size_t nBytes)\n{\n    void* reservedWeights{createTempWeights(dataType, shape).values};\n    std::memcpy(reservedWeights, weightValues, nBytes);\n    return reservedWeights;\n}\n\nint32_t* WeightsContext::convertUINT8(uint8_t const* weightValues, nvinfer1::Dims const& shape)\n{\n    int64_t const nbWeights = volume(shape);\n    int32_t* int32Weights{\n        static_cast<int32_t*>(createTempWeights(::ONNX_NAMESPACE::TensorProto::INT32, shape).values)};\n\n    for (int64_t i = 0; i < nbWeights; i++)\n    {\n        int32Weights[i] = static_cast<int32_t>(weightValues[i]);\n    }\n    return int32Weights;\n}\n\nfloat* WeightsContext::convertDouble(double const* weightValues, nvinfer1::Dims const& shape)\n{\n    auto* ctx = this; // For logging macros.\n    int64_t const nbWeights = volume(shape);\n    float* floatWeights{\n        static_cast<float*>(createTempWeights(::ONNX_NAMESPACE::TensorProto::FLOAT, shape).values)};\n\n    bool outOfBounds{false};\n    double const floatMax = static_cast<double>(std::numeric_limits<float>::max());\n    double const floatMin = static_cast<double>(std::numeric_limits<float>::lowest());\n    for (int64_t i = 0; i < nbWeights; i++)\n    {\n        if (weightValues[i] > floatMax || weightValues[i] < floatMin)\n        {\n            floatWeights[i] = static_cast<float>(std::max(std::min(weightValues[i], floatMax), floatMin));\n            LOG_WARNING(\"Weight at index \" << i << \": \" << weightValues[i]\n                                        << \" is out of range. Clamping to: \" << floatWeights[i]);\n            outOfBounds = true;\n        }\n        else\n        {\n            floatWeights[i] = static_cast<float>(weightValues[i]);\n        }\n    }\n    if (outOfBounds)\n    {\n        LOG_WARNING(\"One or more weights outside the range of FLOAT was clamped\");\n    }\n\n    return floatWeights;\n}\n\nuint8_t* WeightsContext::convertPackedInt32Data(\n    int32_t const* weightValues, nvinfer1::Dims const& shape, size_t nbytes, int32_t onnxdtype)\n{\n    uint8_t* newWeights{static_cast<uint8_t*>(createTempWeights(onnxdtype, shape).values)};\n\n    for (size_t i = 0; i < nbytes; i++)\n    {\n        newWeights[i] = static_cast<uint8_t>(weightValues[i]);\n    }\n    return newWeights;\n}\n\n// Helper function to validate size_t multiplications will not overflow\nbool multiplicationWillOverflow(size_t const a, size_t const b)\n{\n    if (b == 0)\n    {\n        return false;\n    }\n    if (a > std::numeric_limits<size_t>::max() / b)\n    {\n        return true;\n    }\n    return false;\n}\n\n// Helper function to ensure that a ONNX initializer is supportable by TensorRT.\nbool validateOnnxInitializer(::ONNX_NAMESPACE::TensorProto const& onnxTensor)\n{\n    // Validate type.\n    auto onnxDtype = onnxTensor.data_type();\n    auto typeSize = getDtypeSizeBits(onnxDtype);\n    if (typeSize == -1 || typeSize == 0)\n    {\n        return false;\n    }\n    // Validate rank.\n    auto nbDims = onnxTensor.dims().size();\n    if (nbDims > nvinfer1::Dims::MAX_DIMS)\n    {\n        return false;\n    }\n    // Validate volume is within bounds.\n    size_t vol = 1;\n    for (int32_t i = 0; i < nbDims; i++)\n    {\n        auto dimVal = onnxTensor.dims().Get(i);\n        if (dimVal == 0)\n        {\n            vol = 0;\n            break;\n        }\n        if (vol > std::numeric_limits<size_t>::max() / dimVal)\n        {\n            return false;\n        }\n        vol = vol * dimVal;\n    }\n    // Validate size in bytes is within bounds.\n    if (vol > std::numeric_limits<size_t>::max() / typeSize)\n    {\n        return false;\n    }\n\n    return true;\n}\n\n// Function to read bytes from an external file and return the data in a buffer.\nbool WeightsContext::parseExternalWeights(\n    std::string const& file, int64_t offset, int64_t length, std::vector<char>& weightsBuf, size_t& size)\n{\n    auto* ctx = this; // For logging macros.\n    // Accessing parent directories (i.e. ../) is not allowed. Normalize path first.\n    auto path = mOnnxFileLocation;\n    std::string normalizedFile = normalizePath(file);\n    bool illegalDir{false};\n#ifdef _MSC_VER\n    illegalDir |= normalizedFile.find(\"..\\\\\") != std::string::npos;\n#endif\n    illegalDir |= normalizedFile.find(\"../\") != std::string::npos;\n\n    if (illegalDir)\n    {\n        LOG_ERROR(\"Relative paths to parent (../) are not allowed in ONNX external weights! Normalized path is: \"\n            << normalizedFile);\n        return false;\n    }\n    // The weight paths in the ONNX model are relative paths to the main ONNX file.\n#ifdef _MSC_VER\n    size_t slash = path.rfind(\"\\\\\");\n    // When using WSL path can have \"\\\" or \"/\". Need to check both options here.\n    if (slash == std::string::npos)\n    {\n        slash = path.rfind(\"/\");\n    }\n#else\n    size_t slash = path.rfind(\"/\");\n#endif\n    if (slash != std::string::npos)\n    {\n        path.replace(slash + 1, path.size() - (slash + 1), normalizedFile);\n    }\n    else\n    {\n        path = normalizedFile;\n    }\n    LOG_VERBOSE(\"Reading weights from external file: \" << path);\n    std::ifstream relPathFile(path, std::ios::binary | std::ios::ate);\n    if (!relPathFile)\n    {\n        LOG_ERROR(\"Failed to open file: \" << path);\n        return false;\n    }\n    std::streamsize fileSize = relPathFile.tellg();\n    relPathFile.seekg(offset, std::ios::beg);\n    int64_t weightsBufSize = length == 0 ? fileSize : length;\n    weightsBuf.resize(weightsBufSize);\n    if (!relPathFile.read(weightsBuf.data(), weightsBuf.size()))\n    {\n        LOG_ERROR(\"Failed to read weights from external file: \" << path);\n        return false;\n    }\n    size = weightsBuf.size();\n    return true;\n}\n\n// Function to read data from an ONNX Tensor and move it into a ShapedWeights object. Handles external weights as well.\nbool WeightsContext::convertOnnxWeights(\n    ::ONNX_NAMESPACE::TensorProto const& onnxTensor, ShapedWeights* weights, bool ownAllWeights)\n{\n    auto* ctx = this; // For logging macros.\n\n    // Sanity check for onnxTensors\n    if (!validateOnnxInitializer(onnxTensor))\n    {\n        LOG_ERROR(\"ONNX initializer \" << onnxTensor.name() << \" cannot be imported into TensorRT!\");\n        return false;\n    }\n\n    void* dataPtr{nullptr};\n    size_t nbytes{0};\n    auto onnxDtype = onnxTensor.data_type();\n\n    nvinfer1::Dims shape{};\n    shape.nbDims = onnxTensor.dims().size();\n    std::copy_n(onnxTensor.dims().begin(), shape.nbDims, shape.d);\n    // ONNX weight values can be stored in either the TensorProto itself, or in an external file in the case\n    // of large models. Check for this here.\n    auto dataLocation = onnxTensor.data_location();\n    // External Data\n    if (dataLocation == 1)\n    {\n        std::string location{\"\"};\n        int64_t offset{0};\n        int64_t length{0};\n\n        // onnxTensor.external_data() is a String : String map that holds metadata about how to read from an external\n        // file\n        for (auto onnxMapEntry : onnxTensor.external_data())\n        {\n            auto keyName = onnxMapEntry.key();\n            if (keyName == \"location\")\n            {\n                location = onnxMapEntry.value();\n            }\n            else if (keyName == \"offset\")\n            {\n                offset = std::atoll(onnxMapEntry.value().c_str());\n            }\n            else if (keyName == \"length\")\n            {\n                length = std::atoll(onnxMapEntry.value().c_str());\n            }\n            // Not used at the moment\n            else if (keyName == \"checksum\")\n            {\n                continue;\n            }\n            else\n            {\n                LOG_ERROR(\"Key value of: \" << keyName << \" was not expected!\");\n                return false;\n            }\n        }\n\n        // Buffer to hold the data read from the file\n        std::vector<char> dataBuf;\n        // Will update dataBuf and nbytes by reference.\n        if (!parseExternalWeights(location, offset, length, dataBuf, nbytes))\n        {\n            return false;\n        }\n\n        // For weights parsed from external files, createTempWeights is necessary to keep them in scope\n        ShapedWeights externalWeights;\n        dataPtr = dataBuf.data();\n\n        // Cast non-native TRT types to their corresponding proxy types\n        if (onnxDtype == ::ONNX_NAMESPACE::TensorProto::UINT8)\n        {\n            // Cast UINT8 weights to INT32.\n            dataPtr = convertUINT8(reinterpret_cast<uint8_t const*>(dataPtr), shape);\n            size_t const sizeOffset = sizeof(int32_t) / sizeof(uint8_t);\n            if (multiplicationWillOverflow(nbytes, sizeOffset))\n            {\n                return false;\n            }\n            nbytes = nbytes * sizeOffset;\n            onnxDtype = ::ONNX_NAMESPACE::TensorProto::INT32;\n        }\n        else if (onnxDtype == ::ONNX_NAMESPACE::TensorProto::DOUBLE)\n        {\n            // Cast DOUBLE weights to FLOAT.\n            dataPtr = convertDouble(reinterpret_cast<double const*>(dataPtr), shape);\n            nbytes = nbytes / (sizeof(double) / sizeof(float));\n            onnxDtype = ::ONNX_NAMESPACE::TensorProto::FLOAT;\n        }\n\n        // Create the holder for external weights.\n        externalWeights = createTempWeights(onnxDtype, shape);\n\n        // Check if the size of external weights is as expected.\n        if (externalWeights.size_bytes() != nbytes)\n        {\n            LOG_ERROR(\"Unexpected size for the external weights! Expected size: \"\n                << externalWeights.size_bytes() << \" bytes (shape = \" << shape << \"). Actual size: \" << nbytes\n                << \" bytes.\");\n            return false;\n        }\n\n        // Copy the weight values into externalWeights.\n        std::memcpy(externalWeights.values, dataPtr, nbytes);\n\n        *weights = externalWeights;\n        return true;\n    }\n\n    // Weights information is within the TensorProto itself\n\n    // Cast non-native TRT types to their corresponding proxy types\n    if (onnxDtype == ::ONNX_NAMESPACE::TensorProto::UINT8)\n    {\n        onnxDtype = ::ONNX_NAMESPACE::TensorProto::INT32;\n        if (onnxTensor.raw_data().size() > 0)\n        {\n            dataPtr = convertUINT8(reinterpret_cast<uint8_t const*>(onnxTensor.raw_data().data()), shape);\n            size_t const sizeOffset = (sizeof(int32_t) / sizeof(uint8_t));\n            if (multiplicationWillOverflow(nbytes, sizeOffset))\n            {\n                return false;\n            }\n            nbytes = onnxTensor.raw_data().size() * sizeOffset;\n        }\n        else if (onnxTensor.int32_data().size() > 0)\n        {\n            dataPtr = (void*) onnxTensor.int32_data().data();\n            if (multiplicationWillOverflow(nbytes, sizeof(int32_t)))\n            {\n                return false;\n            }\n            nbytes = onnxTensor.int32_data().size() * sizeof(int32_t);\n            if (ownAllWeights)\n            {\n                dataPtr = ownWeights(dataPtr, onnxDtype, shape, nbytes);\n            }\n        }\n    }\n    else if (onnxDtype == ::ONNX_NAMESPACE::TensorProto::DOUBLE)\n    {\n        if (onnxTensor.raw_data().size() > 0)\n        {\n            dataPtr = convertDouble(reinterpret_cast<double const*>(onnxTensor.raw_data().data()), shape);\n            nbytes = onnxTensor.raw_data().size() / (sizeof(double) / sizeof(float));\n        }\n        else if (onnxTensor.double_data().size() > 0)\n        {\n            dataPtr = convertDouble(onnxTensor.double_data().data(), shape);\n            if (multiplicationWillOverflow(nbytes, sizeof(float)))\n            {\n                return false;\n            }\n            nbytes = onnxTensor.double_data().size() * sizeof(float);\n        }\n        onnxDtype = ::ONNX_NAMESPACE::TensorProto::FLOAT;\n    }\n\n    // Check for supported types that can be found in the int32_data field in the TensorProto\n    // https://github.com/onnx/onnx/blob/609282efe8d4871f620141223139bbb99bdbe9f6/onnx/onnx.proto#L567\n    else if (onnxDtype == ::ONNX_NAMESPACE::TensorProto::INT32 || onnxDtype == ::ONNX_NAMESPACE::TensorProto::INT64\n        || onnxDtype == ::ONNX_NAMESPACE::TensorProto::FLOAT16 || onnxDtype == ::ONNX_NAMESPACE::TensorProto::BFLOAT16\n        || onnxDtype == ::ONNX_NAMESPACE::TensorProto::INT8 || onnxDtype == ::ONNX_NAMESPACE::TensorProto::BOOL\n        || onnxDtype == ::ONNX_NAMESPACE::TensorProto::INT4)\n    {\n        if (onnxTensor.raw_data().size() > 0)\n        {\n            dataPtr = (void*) (onnxTensor.raw_data().data());\n            nbytes = onnxTensor.raw_data().size();\n            if (ownAllWeights)\n            {\n                dataPtr = ownWeights(dataPtr, onnxDtype, shape, nbytes);\n            }\n        }\n        else\n        {\n            nbytes = getTensorOrWeightsSizeBytes(onnxTensor.int32_data().size(), onnxDtype);\n            switch (onnxDtype)\n            {\n            case ::ONNX_NAMESPACE::TensorProto::INT32:\n                dataPtr = (void*) (onnxTensor.int32_data().data());\n                if (ownAllWeights)\n                {\n                    dataPtr = ownWeights(dataPtr, onnxDtype, shape, nbytes);\n                }\n                break;\n            case ::ONNX_NAMESPACE::TensorProto::INT64:\n                nbytes = getTensorOrWeightsSizeBytes(onnxTensor.int64_data().size(), onnxDtype);\n                dataPtr = (void*) (onnxTensor.int64_data().data());\n                if (ownAllWeights)\n                {\n                    dataPtr = ownWeights(dataPtr, onnxDtype, shape, nbytes);\n                }\n                break;\n            case ::ONNX_NAMESPACE::TensorProto::FLOAT16:\n            case ::ONNX_NAMESPACE::TensorProto::BFLOAT16:\n                dataPtr = convertInt32Data<uint16_t>(onnxTensor.int32_data().data(), shape, onnxDtype);\n                break;\n            case ::ONNX_NAMESPACE::TensorProto::INT8:\n                dataPtr = convertInt32Data<int8_t>(onnxTensor.int32_data().data(), shape, onnxDtype);\n                break;\n            case ::ONNX_NAMESPACE::TensorProto::BOOL:\n                dataPtr = convertInt32Data<uint8_t>(onnxTensor.int32_data().data(), shape, onnxDtype);\n                break;\n            case ::ONNX_NAMESPACE::TensorProto::INT4:\n                // int4 data is packed, each int32 element contains one byte (two int4 nibbles)\n                nbytes = onnxTensor.int32_data().size();\n                dataPtr = convertPackedInt32Data(onnxTensor.int32_data().data(), shape, nbytes, onnxDtype);\n                break;\n            default:\n                LOG_ERROR(\"Found unsupported datatype (\" << onnxDtype\n                                                         << \") when importing initializer: \" << onnxTensor.name());\n                break;\n            }\n        }\n    }\n    else if (onnxDtype == ::ONNX_NAMESPACE::TensorProto::FLOAT)\n    {\n        if (onnxTensor.raw_data().size() > 0)\n        {\n            dataPtr = (void*) (onnxTensor.raw_data().data());\n            nbytes = onnxTensor.raw_data().size();\n        }\n        else\n        {\n            dataPtr = (void*) (onnxTensor.float_data().data());\n            if (multiplicationWillOverflow(nbytes, sizeof(float)))\n            {\n                return false;\n            }\n            nbytes = onnxTensor.float_data().size() * sizeof(float);\n        }\n        if (ownAllWeights)\n        {\n            dataPtr = ownWeights(dataPtr, onnxDtype, shape, nbytes);\n        }\n    }\n    else if (onnxDtype == ::ONNX_NAMESPACE::TensorProto::FLOAT8E4M3FN)\n    {\n        if (onnxTensor.raw_data().size() > 0)\n        {\n            dataPtr = (void*) (onnxTensor.raw_data().data());\n            nbytes = onnxTensor.raw_data().size();\n        }\n        else\n        {\n            dataPtr = (void*) (onnxTensor.int32_data().data());\n            nbytes = onnxTensor.int32_data().size();\n        }\n        if (ownAllWeights)\n        {\n            dataPtr = ownWeights(dataPtr, onnxDtype, shape, nbytes);\n        }\n    }\n    else\n    {\n        LOG_ERROR(\"Found unsupported datatype (\" << onnxDtype << \") when importing initializer: \" << onnxTensor.name());\n        return false;\n    }\n    onnx2trt::ShapedWeights trt_weights(onnxDtype, dataPtr, shape);\n    // Sanity check that weights were converted properly\n    if (trt_weights.size_bytes() != nbytes)\n    {\n        LOG_ERROR(\"Size mismatch when importing initializer: \" << onnxTensor.name() << \". Expected size: \" << nbytes\n                                                            << \" , actual size: \" << trt_weights.size_bytes());\n        return false;\n    }\n    *weights = trt_weights;\n    return true;\n}\n\nfloat* WeightsContext::convertFP16Data(void* weightValues, nvinfer1::Dims const& shape)\n{\n    int64_t const nbWeights = volume(shape);\n    float* newWeights{static_cast<float*>(createTempWeights(::ONNX_NAMESPACE::TensorProto::FLOAT, shape).values)};\n\n    half_float::half* tempValues = static_cast<half_float::half*>(weightValues);\n\n    for (int64_t i = 0; i < nbWeights; i++)\n    {\n        newWeights[i] = tempValues[i];\n    }\n    return newWeights;\n}\n\nfloat* WeightsContext::getFP32Values(ShapedWeights const& w)\n{\n    assert((w.type == ::ONNX_NAMESPACE::TensorProto::FLOAT || w.type == ::ONNX_NAMESPACE::TensorProto::FLOAT16)\n        && \"Conversion only valid from FLOAT or FLOAT16\");\n    return (w.type == ::ONNX_NAMESPACE::TensorProto::FLOAT) ? static_cast<float*>(w.values)\n                                                            : convertFP16Data(w.values, w.shape);\n}\n\nShapedWeights WeightsContext::createNamedTempWeights(ShapedWeights::DataType type, nvinfer1::Dims const& shape,\n    std::set<std::string>& namesSet, int64_t& suffixCounter, bool batchNormNode)\n{\n    std::string const& name\n        = generateUniqueName(namesSet, suffixCounter, batchNormNode ? \"tmp_batch_norm_weight\" : \"tmp_weight\");\n    return createNamedWeights(type, shape, name);\n}\n\nShapedWeights WeightsContext::createTempWeights(ShapedWeights::DataType type, nvinfer1::Dims const& shape)\n{\n    ShapedWeights weights(type, nullptr, shape);\n    int64_t nbBytes = weights.size_bytes();\n    // For empty weights, keep the values as nullptr.\n    if (nbBytes == 0)\n    {\n        return weights;\n    }\n    void* ptr = operator new(nbBytes);\n    std::memset(ptr, 0, nbBytes);\n    mWeightBuffers.push_back(BufferPtr{ptr});\n    weights.values = ptr;\n    return weights;\n}\n\nShapedWeights WeightsContext::createNamedWeights(ShapedWeights::DataType type, nvinfer1::Dims const& shape,\n    std::string const& name, std::set<std::string>* bufferedNames)\n{\n    ShapedWeights weights = createTempWeights(type, shape);\n    if (bufferedNames)\n    {\n        bufferedNames->insert(name);\n        weights.setName((*bufferedNames->find(name)).c_str());\n    }\n    else\n    {\n        weights.setName(name.c_str());\n    }\n    return weights;\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "WeightsContext.hpp",
          "type": "blob",
          "size": 3.9033203125,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include \"ShapedWeights.hpp\"\n#include \"Status.hpp\"\n#include \"weightUtils.hpp\"\n#include <string>\n#include <vector>\n\nnamespace onnx2trt\n{\n\n// Class reponsible for reading, casting, and converting weight values from an ONNX model and into ShapedWeights\n// objects. All temporary weights are stored in a buffer owned by the class so they do not go out of scope.\n\nclass WeightsContext\n{\n    struct BufferDeleter\n    {\n        void operator()(void* ptr)\n        {\n            operator delete(ptr);\n        }\n    };\n\n    using BufferPtr = std::unique_ptr<void, BufferDeleter>;\n\n    nvinfer1::ILogger* mLogger;\n\n    // Vector of hunks to maintain ownership of weights.\n    std::vector<BufferPtr> mWeightBuffers;\n\n    // Keeps track of the absolute location of the file in order to read external weights.\n    std::string mOnnxFileLocation;\n\npublic:\n    WeightsContext(nvinfer1::ILogger* logger)\n        : mLogger(logger){};\n\n    int32_t* convertUINT8(uint8_t const* weightValues, nvinfer1::Dims const& shape);\n\n    float* convertDouble(double const* weightValues, nvinfer1::Dims const& shape);\n\n    template <typename DataType>\n    DataType* convertInt32Data(int32_t const* weightValues, nvinfer1::Dims const& shape, int32_t onnxdtype);\n\n    uint8_t* convertPackedInt32Data(\n        int32_t const* weightValues, nvinfer1::Dims const& shape, size_t nbytes, int32_t onnxdtype);\n\n    // Function to create an internal buffer to own the weights without any type conversions.\n    void* ownWeights(void const* weightValues, ShapedWeights::DataType const dataType, nvinfer1::Dims const& shape,\n        size_t const nBytes);\n\n    // Function to read bytes from an external file and return the data in a buffer.\n    bool parseExternalWeights(\n\n        std::string const& file, int64_t offset, int64_t length, std::vector<char>& weightsBuf, size_t& size);\n\n    // Function to read data from an ONNX Tensor and move it into a ShapedWeights object.\n    // Handles external weights as well.\n    bool convertOnnxWeights(\n        ::ONNX_NAMESPACE::TensorProto const& onnxTensor, ShapedWeights* weights, bool ownAllWeights = false);\n\n    // Helper function to convert weightValues' type from fp16 to fp32.\n    float* convertFP16Data(void* weightValues, nvinfer1::Dims const& shape);\n\n    // Helper function to get fp32 representation of fp16 or fp32 weights.\n    float* getFP32Values(ShapedWeights const& w);\n\n    // Register an unique name for the created weights.\n    ShapedWeights createNamedTempWeights(ShapedWeights::DataType type, nvinfer1::Dims const& shape,\n        std::set<std::string>& namesSet, int64_t& suffixCounter, bool batchNormNode = false);\n\n    // Create weights with a given name.\n    ShapedWeights createNamedWeights(ShapedWeights::DataType type, nvinfer1::Dims const& shape, std::string const& name,\n        std::set<std::string>* bufferedNames = nullptr);\n\n    // Creates a ShapedWeights object class of a given type and shape.\n    ShapedWeights createTempWeights(ShapedWeights::DataType type, nvinfer1::Dims const& shape);\n\n    // Sets the absolute filepath of the loaded ONNX model in order to read external weights.\n    void setOnnxFileLocation(std::string location)\n    {\n        mOnnxFileLocation = location;\n    }\n\n    // Returns the absolutate filepath of the loaded ONNX model.\n    std::string getOnnxFileLocation()\n    {\n        return mOnnxFileLocation;\n    }\n\n    // Returns the logger object.\n    nvinfer1::ILogger& logger()\n    {\n        return *mLogger;\n    }\n};\n\ntemplate <typename DataType>\nDataType* WeightsContext::convertInt32Data(int32_t const* weightValues, nvinfer1::Dims const& shape, int32_t onnxdtype)\n{\n    size_t const nbWeights = volume(shape);\n    DataType* newWeights{static_cast<DataType*>(createTempWeights(onnxdtype, shape).values)};\n\n    for (size_t i = 0; i < nbWeights; i++)\n    {\n        newWeights[i] = static_cast<DataType>(weightValues[i]);\n    }\n    return newWeights;\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "bfloat16.cpp",
          "type": "blob",
          "size": 0.9375,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"bfloat16.hpp\"\n#include <cstring>\n\nnamespace onnx2trt\n{\n\nBFloat16::operator float() const\n{\n    static_assert(sizeof(uint32_t) == sizeof(float), \"\");\n    float val{0.F};\n    auto bits = static_cast<uint32_t>(mRep) << 16;\n    std::memcpy(&val, &bits, sizeof(uint32_t));\n    return val;\n}\n\nBFloat16::BFloat16(float x)\n{\n    static_assert(sizeof(uint32_t) == sizeof(float), \"\");\n    uint32_t bits{0};\n    std::memcpy(&bits, &x, sizeof(float));\n\n    // FP32 format: 1 sign bit, 8 bit exponent, 23 bit mantissa\n    // BF16 format: 1 sign bit, 8 bit exponent, 7 bit mantissa\n\n    // Mask for exponent\n    constexpr uint32_t exponent = 0xFFU << 23;\n\n    // Check if exponent is all 1s (NaN or infinite)\n    if ((bits & exponent) != exponent)\n    {\n        // x is finite - round to even\n        bits += 0x7FFFU + (bits >> 16 & 1);\n    }\n\n    mRep = static_cast<uint16_t>(bits >> 16);\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "bfloat16.hpp",
          "type": "blob",
          "size": 0.478515625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include <cstdint>\n\nnamespace onnx2trt\n{\n\n//! Implements \"Brain Floating Point\": like an IEEE FP32,\n//! but the significand is only 7 bits instead of 23 bits.\nclass BFloat16\n{\npublic:\n    BFloat16()\n        : mRep(0)\n    {\n    }\n\n    // Rounds to even if there is a tie.\n    BFloat16(float x);\n\n    operator float() const;\n\nprivate:\n    //! Value stored in BFloat16 representation.\n    uint16_t mRep;\n};\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "common.hpp",
          "type": "blob",
          "size": 4.5634765625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n#pragma once\n\n#include <onnx/onnx_pb.h>\n#include <memory>\n#include <fstream>\n#include <iostream>\n#include <ctime>\n#include <fcntl.h> // For ::open\n#include <limits>\n#include <google/protobuf/io/coded_stream.h>\n#include <google/protobuf/io/zero_copy_stream_impl.h>\n#include <google/protobuf/text_format.h>\n\n// Namespace for common functions used throughout onnx-trt\nnamespace common\n{\n  struct InferDeleter {\n      template<typename T>\n      void operator()(T* obj) const {\n  \tif( obj ) {\n  \t    obj->destroy();\n  \t}\n      }\n  };\n\n  template<typename T>\n  inline std::shared_ptr<T> infer_object(T* obj) {\n      if( !obj ) {\n  \tthrow std::runtime_error(\"Failed to create object\");\n      }\n      return std::shared_ptr<T>(obj, InferDeleter());\n  }\n\n  // Logger for TensorRT info/warning/errors\n  class TRT_Logger : public nvinfer1::ILogger {\n    nvinfer1::ILogger::Severity _verbosity;\n    std::ostream* _ostream;\n  public:\n    TRT_Logger(Severity verbosity=Severity::kWARNING,\n               std::ostream& ostream=std::cout)\n      : _verbosity(verbosity), _ostream(&ostream) {}\n    void log(Severity severity, const char* msg) noexcept override {\n      if( severity <= _verbosity ) {\n        time_t rawtime = std::time(0);\n        char buf[256];\n        strftime(&buf[0], 256,\n                 \"%Y-%m-%d %H:%M:%S\",\n                 std::gmtime(&rawtime));\n        const char* sevstr = (severity == Severity::kINTERNAL_ERROR ? \"    BUG\" :\n                              severity == Severity::kERROR          ? \"  ERROR\" :\n                              severity == Severity::kWARNING        ? \"WARNING\" :\n                              severity == Severity::kINFO           ? \"   INFO\" :\n                              \"UNKNOWN\");\n        (*_ostream) << \"[\" << buf << \" \" << sevstr << \"] \"\n                    << msg\n                    << std::endl;\n      }\n    }\n  };\n\n  inline bool ParseFromFile_WAR(google::protobuf::Message* msg,\n                         const char*                filename) {\n    int fd = ::open(filename, O_RDONLY);\n    google::protobuf::io::FileInputStream raw_input(fd);\n    raw_input.SetCloseOnDelete(true);\n    google::protobuf::io::CodedInputStream coded_input(&raw_input);\n  #if GOOGLE_PROTOBUF_VERSION >= 3011000\n    // Starting Protobuf 3.11 accepts only single parameter.\n    coded_input.SetTotalBytesLimit(std::numeric_limits<int>::max());\n  #else\n    // Note: This WARs the very low default size limit (64MB)\n    coded_input.SetTotalBytesLimit(std::numeric_limits<int>::max(),\n                                   std::numeric_limits<int>::max()/4);\n  #endif\n    return msg->ParseFromCodedStream(&coded_input);\n  }\n\n  inline bool MessageToFile(const google::protobuf::Message* msg,\n                         const char*                filename) {\n    int fd = ::open(filename, O_WRONLY | O_CREAT | O_TRUNC, 0644);\n    google::protobuf::io::FileOutputStream raw_output(fd);\n    raw_output.SetCloseOnDelete(true);\n    google::protobuf::io::CodedOutputStream output(&raw_output);\n\n    // Write the size.\n    const int size = msg->ByteSize();\n\n    uint8_t* buffer = output.GetDirectBufferForNBytesAndAdvance(size);\n    if (buffer != NULL) {\n      // Optimization:  The msg fits in one buffer, so use the faster\n      // direct-to-array serialization path.\n      msg->SerializeWithCachedSizesToArray(buffer);\n    } else {\n      // Slightly-slower path when the msg is multiple buffers.\n      msg->SerializeWithCachedSizes(&output);\n      if (output.HadError()) return false;\n    }\n\n    return true;\n  }\n\n  inline bool ParseFromTextFile(google::protobuf::Message* msg,\n                         const char*                filename) {\n    int fd = ::open(filename, O_RDONLY);\n    google::protobuf::io::FileInputStream raw_input(fd);\n    raw_input.SetCloseOnDelete(true);\n    return google::protobuf::TextFormat::Parse(&raw_input, msg);\n  }\n\n  inline std::string onnx_ir_version_string(int64_t ir_version=::ONNX_NAMESPACE::IR_VERSION) {\n    int onnx_ir_major = ir_version / 1000000;\n    int onnx_ir_minor = ir_version % 1000000 / 10000;\n    int onnx_ir_patch = ir_version % 10000;\n    return (std::to_string(onnx_ir_major) + \".\" +\n            std::to_string(onnx_ir_minor) + \".\" +\n            std::to_string(onnx_ir_patch));\n  }\n\n  inline void print_version() {\n    std::cout << \"Parser built against:\" << std::endl;\n    std::cout << \"  ONNX IR version:  \" << onnx_ir_version_string(::ONNX_NAMESPACE::IR_VERSION) << std::endl;\n    std::cout << \"  TensorRT version: \"\n         << NV_TENSORRT_MAJOR << \".\"\n         << NV_TENSORRT_MINOR << \".\"\n         << NV_TENSORRT_PATCH << std::endl;\n  }\n} // namespace common\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "errorHelpers.cpp",
          "type": "blob",
          "size": 1.3955078125,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n#include \"errorHelpers.hpp\"\n\nnamespace onnx2trt\n{\nOnnxTrtException::OnnxTrtException(Status status)\n    : mStatus(status)\n{\n}\nStatus OnnxTrtException::getStatus() const noexcept\n{\n    return mStatus;\n}\nchar const* OnnxTrtException::what() const noexcept\n{\n    if (mMessage.empty())\n    {\n        mMessage = parserErrorStr(&mStatus);\n    }\n    return mMessage.c_str();\n}\n\nnvinfer1::ErrorCode errorCodeToTrtCode(ErrorCode const code)\n{\n    switch (code)\n    {\n    case ErrorCode::kSUCCESS: return nvinfer1::ErrorCode::kSUCCESS;\n\n    case ErrorCode::kINTERNAL_ERROR:\n    case ErrorCode::kMODEL_DESERIALIZE_FAILED:\n    case ErrorCode::kREFIT_FAILED:\n    {\n        return nvinfer1::ErrorCode::kINTERNAL_ERROR;\n    }\n\n    case ErrorCode::kMEM_ALLOC_FAILED:\n    {\n        return nvinfer1::ErrorCode::kFAILED_ALLOCATION;\n    }\n\n    case ErrorCode::kINVALID_VALUE:\n    case ErrorCode::kINVALID_GRAPH:\n    case ErrorCode::kINVALID_NODE:\n    case ErrorCode::kUNSUPPORTED_GRAPH:\n    case ErrorCode::kUNSUPPORTED_NODE:\n    case ErrorCode::kUNSUPPORTED_NODE_ATTR:\n    case ErrorCode::kUNSUPPORTED_NODE_INPUT:\n    case ErrorCode::kUNSUPPORTED_NODE_DATATYPE:\n    case ErrorCode::kUNSUPPORTED_NODE_DYNAMIC:\n    case ErrorCode::kUNSUPPORTED_NODE_SHAPE:\n    {\n        return nvinfer1::ErrorCode::kINVALID_ARGUMENT;\n    }\n    }\n    return nvinfer1::ErrorCode::kINTERNAL_ERROR;\n}\n} // namespace onnx2trt\n"
        },
        {
          "name": "errorHelpers.hpp",
          "type": "blob",
          "size": 6.3662109375,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n#pragma once\n\n#include \"Status.hpp\"\n#include <NvInferRuntime.h>\n#include <exception>\n#include <sstream>\n#include <stdexcept>\n\n#define ONNXTRT_TRY try\n\n#define ONNXTRT_CATCH_RECORD                                                                                           \\\n    catch (OnnxTrtException & e)                                                                                       \\\n    {                                                                                                                  \\\n        Status status = e.getStatus();                                                                                 \\\n        mImporterCtx.getErrorRecorder()->reportError(errorCodeToTrtCode(status.code()), e.what());                     \\\n        mErrors.push_back(status);                                                                                     \\\n    }                                                                                                                  \\\n    catch (std::exception & e)                                                                                         \\\n    {                                                                                                                  \\\n        mImporterCtx.getErrorRecorder()->reportError(nvinfer1::ErrorCode::kUNSPECIFIED_ERROR, e.what());               \\\n        mErrors.push_back(Status{ErrorCode::kINTERNAL_ERROR, e.what()});                                               \\\n    }\n\n#define ONNXTRT_CATCH_LOG(logger)                                                                                      \\\n    catch (OnnxTrtException & e)                                                                                       \\\n    {                                                                                                                  \\\n        Status status = e.getStatus();                                                                                 \\\n        (logger)->log(nvinfer1::ILogger::Severity::kINTERNAL_ERROR, e.what());                                         \\\n        mErrors.push_back(status);                                                                                     \\\n    }                                                                                                                  \\\n    catch (std::exception & e)                                                                                         \\\n    {                                                                                                                  \\\n        (logger)->log(nvinfer1::ILogger::Severity::kINTERNAL_ERROR, e.what());                                         \\\n        mErrors.push_back(Status{ErrorCode::kINTERNAL_ERROR, e.what()});                                               \\\n    }\n\n#define ONNXTRT_THROW(status) throw OnnxTrtException(status)\n\n#define ONNXTRT_CHECK(cond, code)                                                                                      \\\n    if (!(cond))                                                                                                       \\\n    {                                                                                                                  \\\n        std::ostringstream ss;                                                                                         \\\n        ss << \"Assertion failed: \" << #cond;                                                                           \\\n        ONNXTRT_THROW(MAKE_ERROR(ss.str(), (code)));                                                                   \\\n    }\n\n#define ONNXTRT_CHECK_NODE(cond, desc, node, nodeIdx, code)                                                            \\\n    if (!(cond))                                                                                                       \\\n    {                                                                                                                  \\\n        std::ostringstream ss;                                                                                         \\\n        ss << \"Assertion failed: \" << #cond << \": \" << desc;                                                           \\\n        ONNXTRT_THROW(MAKE_NODE_ERROR((ss.str()), (code), (node), (nodeIdx)));                                         \\\n    }\n\nnamespace onnx2trt\n{\ninline char const* errorCodeStr(ErrorCode code)\n{\n    switch (code)\n    {\n    case ErrorCode::kSUCCESS: return \"SUCCESS\";\n    case ErrorCode::kINTERNAL_ERROR: return \"INTERNAL_ERROR\";\n    case ErrorCode::kMEM_ALLOC_FAILED: return \"MEM_ALLOC_FAILED\";\n    case ErrorCode::kMODEL_DESERIALIZE_FAILED: return \"MODEL_DESERIALIZE_FAILED\";\n    case ErrorCode::kINVALID_VALUE: return \"INVALID_VALUE\";\n    case ErrorCode::kINVALID_GRAPH: return \"INVALID_GRAPH\";\n    case ErrorCode::kINVALID_NODE: return \"INVALID_NODE\";\n    case ErrorCode::kUNSUPPORTED_GRAPH: return \"UNSUPPORTED_GRAPH\";\n    case ErrorCode::kUNSUPPORTED_NODE: return \"UNSUPPORTED_NODE\";\n    case ErrorCode::kUNSUPPORTED_NODE_ATTR: return \"UNSUPPORTED_NODE_ATTR\";\n    case ErrorCode::kUNSUPPORTED_NODE_INPUT: return \"UNSUPPORTED_NODE_INPUT\";\n    case ErrorCode::kUNSUPPORTED_NODE_DATATYPE: return \"UNSUPPORTED_NODE_DATATYPE\";\n    case ErrorCode::kUNSUPPORTED_NODE_DYNAMIC: return \"UNSUPPORTED_NODE_DYNAMIC\";\n    case ErrorCode::kUNSUPPORTED_NODE_SHAPE: return \"UNSUPPORTED_NODE_SHAPE\";\n    case ErrorCode::kREFIT_FAILED: return \"REFIT_FAILED\";\n    }\n    return \"UNKNOWN\";\n};\n\ninline std::string const parserErrorStr(nvonnxparser::IParserError const* error)\n{\n    std::string const nodeInfo = \"In node \" + std::to_string(error->node()) + \" with name: \" + error->nodeName()\n        + \" and operator: \" + error->nodeOperator() + \" \";\n    std::string const errorInfo\n        = std::string(\"(\") + error->func() + \"): \" + errorCodeStr(error->code()) + \": \" + error->desc();\n    if (error->code() == ErrorCode::kMODEL_DESERIALIZE_FAILED || error->code() == ErrorCode::kREFIT_FAILED)\n    {\n        return errorInfo.c_str();\n    }\n    return (nodeInfo + errorInfo).c_str();\n}\n\nnvinfer1::ErrorCode errorCodeToTrtCode(ErrorCode const code);\n\nclass OnnxTrtException : public std::exception\n{\n    Status mStatus;\n    mutable std::string mMessage;\n\npublic:\n    OnnxTrtException(Status status);\n\n    Status getStatus() const noexcept;\n\n    virtual char const* what() const noexcept override;\n\n    virtual ~OnnxTrtException() {}\n};\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "getSupportedAPITest.cpp",
          "type": "blob",
          "size": 5.357421875,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include <iostream>\n#include <fstream>\n#include <unistd.h> // For ::getopt\n#include <string>\n#include \"NvOnnxParser.h\"\n#include \"NvInferPlugin.h\"\n#include \"onnx_utils.hpp\"\n#include \"common.hpp\"\n\nusing std::cout;\nusing std::cerr;\nusing std::endl;\n\nvoid print_usage() {\n  cout << \"This program will determine whether or not an ONNX model is compatible with TensorRT. \"\n       << \"If it isn't, a list of supported subgraphs and unsupported operations will be printed.\" << endl;\n  cout << \"Usage: getSupportedAPITest -m onnx_model.pb\" << endl;\n  cout << \"Optional argument: -e TRT_engine\" << endl;\n}\n\nvoid printSubGraphs(SubGraphCollection_t& subGraphs, ::ONNX_NAMESPACE::ModelProto onnx_model)\n{\n    if (subGraphs.size() != 1)\n    {\n        cout << \"The model contains unsupported Nodes. It has been partitioned to a set of supported subGraphs.\" << endl;\n        cout << \"There are \"<< subGraphs.size() << \" supported subGraphs: \" << endl;\n        cout << \"NOTE: Due to some limitations with the parser, the support of specific subgraphs may not have been determined.\"\n        << \" Please refer to the printed subgraphs to see if they are truly supported or not.\" << endl;\n    }\n    else\n    {\n        cout << \"The model is fully supported by TensorRT. Printing the parsed graph:\" << endl;\n    }\n\n    for (auto subGraph: subGraphs)\n    {\n        cout << \"\\t{\";\n        for (auto idx: subGraph.first) cout << \"\\t\" << idx << \",\" <<onnx_model.graph().node(idx).op_type();\n        cout << \"\\t}\\t - \";\n        if (subGraph.second)\n        {\n            cout << \"Fully supported\" << endl;\n        }\n        else\n        {\n            cout << \"UNKNOWN whether this is fully supported.\" << endl;\n        }\n    }\n}\n\n\nint main(int argc, char* argv[]) {\n\n    GOOGLE_PROTOBUF_VERIFY_VERSION;\n\n    std::string engine_filename;\n    std::string text_filename;\n    std::string full_text_filename;\n    std::string onnx_filename;\n    int c;\n    size_t max_batch_size = 32;\n    size_t max_workspace_size = 1 << 30;\n    int verbosity = (int)nvinfer1::ILogger::Severity::kWARNING;\n    while ((c = getopt (argc, argv, \"m:e:\")) != -1)\n    {\n        switch(c)\n        {\n            case 'm':\n                    onnx_filename = optarg;\n                    break;\n            case 'e':\n                    engine_filename = optarg;\n                    break;\n        }\n    }\n\n    if (onnx_filename.empty())\n    {\n        print_usage();\n        return -1;\n    }\n\n    common::TRT_Logger trt_logger((nvinfer1::ILogger::Severity)verbosity);\n\n    auto trt_builder = common::infer_object(nvinfer1::createInferBuilder(trt_logger));\n    auto trt_network = common::infer_object(trt_builder->createNetworkV2(1U << static_cast<uint32_t>(nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH)));\n    auto trt_parser  = common::infer_object(nvonnxparser::createParser(*trt_network, trt_logger));\n\n    initLibNvInferPlugins(&trt_logger, \"\");\n\n    cout << \"Parsing model: \" << onnx_filename << endl;\n\n    std::ifstream onnx_file(onnx_filename.c_str(),\n                            std::ios::binary | std::ios::ate);\n    std::streamsize file_size = onnx_file.tellg();\n    onnx_file.seekg(0, std::ios::beg);\n    std::vector<char> onnx_buf(file_size);\n\n    if( !onnx_file.read(onnx_buf.data(), onnx_buf.size()) ) {\n        cerr << \"ERROR: Failed to read from file \" << onnx_filename << endl;\n        return -1;\n    }\n\n    ::ONNX_NAMESPACE::ModelProto onnx_model;\n    if (!common::ParseFromFile_WAR(&onnx_model, onnx_filename.c_str()))\n    {\n        cout << \"Failure while parsing ONNX file\" << endl;\n        return -1;\n    }\n\n    SubGraphCollection_t SubGraphCollection;\n\n    // supportsModel() parses the graph and returns a list of supported subgraphs.\n    if (!trt_parser->supportsModel(onnx_buf.data(), onnx_buf.size(), SubGraphCollection))\n    {\n        cout << \"Model cannot be fully parsed by TensorRT!\" << endl;\n        printSubGraphs(SubGraphCollection, onnx_model);\n        return -1;\n    }\n\n    printSubGraphs(SubGraphCollection, onnx_model);\n\n    // If -e was specified, create and save the TensorRT engine to disk.\n    // Note we do not call trt_parser->parse() here since it's already done above in parser->supportsModel()\n    if( !engine_filename.empty() ) {\n        trt_builder->setMaxBatchSize(max_batch_size);\n        auto builder_config = common::infer_object(trt_builder->createBuilderConfig());\n        builder_config->setMaxWorkspaceSize(max_workspace_size);\n\n        cout << \"input name: \" << trt_network->getInput(0)->getName() << endl;\n        cout << \"output name: \" << trt_network->getOutput(0)->getName() << endl;\n        cout << \"num layers: \" << trt_network->getNbLayers() << endl;\n        cout << \"outputs: \" << trt_network->getNbOutputs() << endl;\n\n        auto trt_engine = common::infer_object(trt_builder->buildEngineWithConfig(*trt_network.get(), *builder_config.get()));\n\n        if( verbosity >= (int)nvinfer1::ILogger::Severity::kWARNING ) {\n            cout << \"Writing TensorRT engine to \" << engine_filename << endl;\n        }\n        auto engine_plan = common::infer_object(trt_engine->serialize());\n        std::ofstream engine_file(engine_filename.c_str(), std::ios::binary);\n        engine_file.write(reinterpret_cast<const char*>(engine_plan->data()), engine_plan->size());\n        engine_file.close();\n    }\n\n    if( verbosity >= (int)nvinfer1::ILogger::Severity::kWARNING ) {\n        cout << \"All done\" << endl;\n    }\n    return 0;\n}\n"
        },
        {
          "name": "half.h",
          "type": "blob",
          "size": 1.220703125,
          "content": "/*\n * SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n * SPDX-License-Identifier: Apache-2.0\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n//\n// Custom wrapper around external half-precision header\n//\n// Header has some \"extra parentheses\" warnings when different rounding modes are used.\n\n#if defined(__GNUC__)\n#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored \"-Wparentheses\"\n#endif\n\n\n#if defined(__clang__)\n#pragma clang diagnostic push\n#pragma clang diagnostic ignored \"-Wmismatched-tags\"\n#endif\n\n#include \"ieee_half.h\"\ntypedef half_float::half float16;\n\n#if defined(__clang__)\n#pragma clang diagnostic pop\n#endif\n\n#if defined(__GNUC__)\n#pragma GCC diagnostic pop\n#endif\n"
        },
        {
          "name": "ieee_half.h",
          "type": "blob",
          "size": 143.904296875,
          "content": "/*\n * SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n * SPDX-License-Identifier: Apache-2.0\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// half - IEEE 754-based half-precision floating point library.\n//\n// Copyright (c) 2012-2017 Christian Rau <rauy@users.sourceforge.net>\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation \n// files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, \n// modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the \n// Software is furnished to do so, subject to the following conditions:\n//\n// The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE \n// WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR \n// COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, \n// ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n// Version 1.12.0\n\n/// \\file\n/// Main header file for half precision functionality.\n\n#ifndef HALF_HALF_HPP\n#define HALF_HALF_HPP\n\n/// Combined gcc version number.\n#define HALF_GNUC_VERSION (__GNUC__*100+__GNUC_MINOR__)\n\n//check C++11 language features\n#if defined(__clang__)\t\t\t\t\t\t\t\t\t\t//clang\n\t#if __has_feature(cxx_static_assert) && !defined(HALF_ENABLE_CPP11_STATIC_ASSERT)\n\t\t#define HALF_ENABLE_CPP11_STATIC_ASSERT 1\n\t#endif\n\t#if __has_feature(cxx_constexpr) && !defined(HALF_ENABLE_CPP11_CONSTEXPR)\n\t\t#define HALF_ENABLE_CPP11_CONSTEXPR 1\n\t#endif\n\t#if __has_feature(cxx_noexcept) && !defined(HALF_ENABLE_CPP11_NOEXCEPT)\n\t\t#define HALF_ENABLE_CPP11_NOEXCEPT 1\n\t#endif\n\t#if __has_feature(cxx_user_literals) && !defined(HALF_ENABLE_CPP11_USER_LITERALS)\n\t\t#define HALF_ENABLE_CPP11_USER_LITERALS 1\n\t#endif\n\t#if (defined(__GXX_EXPERIMENTAL_CXX0X__) || __cplusplus >= 201103L) && !defined(HALF_ENABLE_CPP11_LONG_LONG)\n\t\t#define HALF_ENABLE_CPP11_LONG_LONG 1\n\t#endif\n/*#elif defined(__INTEL_COMPILER)\t\t\t\t\t\t\t\t//Intel C++\n\t#if __INTEL_COMPILER >= 1100 && !defined(HALF_ENABLE_CPP11_STATIC_ASSERT)\t\t????????\n\t\t#define HALF_ENABLE_CPP11_STATIC_ASSERT 1\n\t#endif\n\t#if __INTEL_COMPILER >= 1300 && !defined(HALF_ENABLE_CPP11_CONSTEXPR)\t\t\t????????\n\t\t#define HALF_ENABLE_CPP11_CONSTEXPR 1\n\t#endif\n\t#if __INTEL_COMPILER >= 1300 && !defined(HALF_ENABLE_CPP11_NOEXCEPT)\t\t\t????????\n\t\t#define HALF_ENABLE_CPP11_NOEXCEPT 1\n\t#endif\n\t#if __INTEL_COMPILER >= 1100 && !defined(HALF_ENABLE_CPP11_LONG_LONG)\t\t\t????????\n\t\t#define HALF_ENABLE_CPP11_LONG_LONG 1\n\t#endif*/\n#elif defined(__GNUC__)\t\t\t\t\t\t\t\t\t\t//gcc\n\t#if defined(__GXX_EXPERIMENTAL_CXX0X__) || __cplusplus >= 201103L\n\t\t#if HALF_GNUC_VERSION >= 403 && !defined(HALF_ENABLE_CPP11_STATIC_ASSERT)\n\t\t\t#define HALF_ENABLE_CPP11_STATIC_ASSERT 1\n\t\t#endif\n\t\t#if HALF_GNUC_VERSION >= 406 && !defined(HALF_ENABLE_CPP11_CONSTEXPR)\n\t\t\t#define HALF_ENABLE_CPP11_CONSTEXPR 1\n\t\t#endif\n\t\t#if HALF_GNUC_VERSION >= 406 && !defined(HALF_ENABLE_CPP11_NOEXCEPT)\n\t\t\t#define HALF_ENABLE_CPP11_NOEXCEPT 1\n\t\t#endif\n\t\t#if HALF_GNUC_VERSION >= 407 && !defined(HALF_ENABLE_CPP11_USER_LITERALS)\n\t\t\t#define HALF_ENABLE_CPP11_USER_LITERALS 1\n\t\t#endif\n\t\t#if !defined(HALF_ENABLE_CPP11_LONG_LONG)\n\t\t\t#define HALF_ENABLE_CPP11_LONG_LONG 1\n\t\t#endif\n\t#endif\n#elif defined(_MSC_VER)\t\t\t\t\t\t\t\t\t\t//Visual C++\n\t#if _MSC_VER >= 1900 && !defined(HALF_ENABLE_CPP11_CONSTEXPR)\n\t\t#define HALF_ENABLE_CPP11_CONSTEXPR 1\n\t#endif\n\t#if _MSC_VER >= 1900 && !defined(HALF_ENABLE_CPP11_NOEXCEPT)\n\t\t#define HALF_ENABLE_CPP11_NOEXCEPT 1\n\t#endif\n\t#if _MSC_VER >= 1900 && !defined(HALF_ENABLE_CPP11_USER_LITERALS)\n\t\t#define HALF_ENABLE_CPP11_USER_LITERALS 1\n\t#endif\n\t#if _MSC_VER >= 1600 && !defined(HALF_ENABLE_CPP11_STATIC_ASSERT)\n\t\t#define HALF_ENABLE_CPP11_STATIC_ASSERT 1\n\t#endif\n\t#if _MSC_VER >= 1310 && !defined(HALF_ENABLE_CPP11_LONG_LONG)\n\t\t#define HALF_ENABLE_CPP11_LONG_LONG 1\n\t#endif\n\t#define HALF_POP_WARNINGS 1\n\t#pragma warning(push)\n\t#pragma warning(disable : 4099 4127 4146)\t//struct vs class, constant in if, negative unsigned\n#endif\n\n//check C++11 library features\n#include <utility>\n#if defined(_LIBCPP_VERSION)\t\t\t\t\t\t\t\t//libc++\n\t#if defined(__GXX_EXPERIMENTAL_CXX0X__) || __cplusplus >= 201103\n\t\t#ifndef HALF_ENABLE_CPP11_TYPE_TRAITS\n\t\t\t#define HALF_ENABLE_CPP11_TYPE_TRAITS 1\n\t\t#endif\n\t\t#ifndef HALF_ENABLE_CPP11_CSTDINT\n\t\t\t#define HALF_ENABLE_CPP11_CSTDINT 1\n\t\t#endif\n\t\t#ifndef HALF_ENABLE_CPP11_CMATH\n\t\t\t#define HALF_ENABLE_CPP11_CMATH 1\n\t\t#endif\n\t\t#ifndef HALF_ENABLE_CPP11_HASH\n\t\t\t#define HALF_ENABLE_CPP11_HASH 1\n\t\t#endif\n\t#endif\n#elif defined(__GLIBCXX__)\t\t\t\t\t\t\t\t\t//libstdc++\n\t#if defined(__GXX_EXPERIMENTAL_CXX0X__) || __cplusplus >= 201103\n\t\t#ifdef __clang__\n\t\t\t#if __GLIBCXX__ >= 20080606 && !defined(HALF_ENABLE_CPP11_TYPE_TRAITS)\n\t\t\t\t#define HALF_ENABLE_CPP11_TYPE_TRAITS 1\n\t\t\t#endif\n\t\t\t#if __GLIBCXX__ >= 20080606 && !defined(HALF_ENABLE_CPP11_CSTDINT)\n\t\t\t\t#define HALF_ENABLE_CPP11_CSTDINT 1\n\t\t\t#endif\n\t\t\t#if __GLIBCXX__ >= 20080606 && !defined(HALF_ENABLE_CPP11_CMATH)\n\t\t\t\t#define HALF_ENABLE_CPP11_CMATH 1\n\t\t\t#endif\n\t\t\t#if __GLIBCXX__ >= 20080606 && !defined(HALF_ENABLE_CPP11_HASH)\n\t\t\t\t#define HALF_ENABLE_CPP11_HASH 1\n\t\t\t#endif\n\t\t#else\n\t\t\t#if HALF_GNUC_VERSION >= 403 && !defined(HALF_ENABLE_CPP11_CSTDINT)\n\t\t\t\t#define HALF_ENABLE_CPP11_CSTDINT 1\n\t\t\t#endif\n\t\t\t#if HALF_GNUC_VERSION >= 403 && !defined(HALF_ENABLE_CPP11_CMATH)\n\t\t\t\t#define HALF_ENABLE_CPP11_CMATH 1\n\t\t\t#endif\n\t\t\t#if HALF_GNUC_VERSION >= 403 && !defined(HALF_ENABLE_CPP11_HASH)\n\t\t\t\t#define HALF_ENABLE_CPP11_HASH 1\n\t\t\t#endif\n\t\t#endif\n\t#endif\n#elif defined(_CPPLIB_VER)\t\t\t\t\t\t\t\t\t//Dinkumware/Visual C++\n\t#if _CPPLIB_VER >= 520\n\t\t#ifndef HALF_ENABLE_CPP11_TYPE_TRAITS\n\t\t\t#define HALF_ENABLE_CPP11_TYPE_TRAITS 1\n\t\t#endif\n\t\t#ifndef HALF_ENABLE_CPP11_CSTDINT\n\t\t\t#define HALF_ENABLE_CPP11_CSTDINT 1\n\t\t#endif\n\t\t#ifndef HALF_ENABLE_CPP11_HASH\n\t\t\t#define HALF_ENABLE_CPP11_HASH 1\n\t\t#endif\n\t#endif\n\t#if _CPPLIB_VER >= 610\n\t\t#ifndef HALF_ENABLE_CPP11_CMATH\n\t\t\t#define HALF_ENABLE_CPP11_CMATH 1\n\t\t#endif\n\t#endif\n#endif\n#undef HALF_GNUC_VERSION\n\n//support constexpr\n#if HALF_ENABLE_CPP11_CONSTEXPR\n\t#define HALF_CONSTEXPR\t\t\tconstexpr\n\t#define HALF_CONSTEXPR_CONST\tconstexpr\n#else\n\t#define HALF_CONSTEXPR\n\t#define HALF_CONSTEXPR_CONST\tconst\n#endif\n\n//support noexcept\n#if HALF_ENABLE_CPP11_NOEXCEPT\n\t#define HALF_NOEXCEPT\tnoexcept\n\t#define HALF_NOTHROW\tnoexcept\n#else\n\t#define HALF_NOEXCEPT\n\t#define HALF_NOTHROW\tthrow()\n#endif\n\n#include <algorithm>\n#include <iostream>\n#include <limits>\n#include <climits>\n#include <cmath>\n#include <cstring>\n#if HALF_ENABLE_CPP11_TYPE_TRAITS\n\t#include <type_traits>\n#endif\n#if HALF_ENABLE_CPP11_CSTDINT\n\t#include <cstdint>\n#endif\n#if HALF_ENABLE_CPP11_HASH\n\t#include <functional>\n#endif\n\n\n/// Default rounding mode.\n/// This specifies the rounding mode used for all conversions between [half](\\ref half_float::half)s and `float`s as well as \n/// for the half_cast() if not specifying a rounding mode explicitly. It can be redefined (before including half.hpp) to one \n/// of the standard rounding modes using their respective constants or the equivalent values of `std::float_round_style`:\n///\n/// `std::float_round_style`         | value | rounding\n/// ---------------------------------|-------|-------------------------\n/// `std::round_indeterminate`       | -1    | fastest (default)\n/// `std::round_toward_zero`         | 0     | toward zero\n/// `std::round_to_nearest`          | 1     | to nearest\n/// `std::round_toward_infinity`     | 2     | toward positive infinity\n/// `std::round_toward_neg_infinity` | 3     | toward negative infinity\n///\n/// By default this is set to `-1` (`std::round_indeterminate`), which uses truncation (round toward zero, but with overflows \n/// set to infinity) and is the fastest rounding mode possible. It can even be set to `std::numeric_limits<float>::round_style` \n/// to synchronize the rounding mode with that of the underlying single-precision implementation.\n/// For GIE-1275, changing it to 1 (to nearest)\n#ifndef HALF_ROUND_STYLE\n\t#define HALF_ROUND_STYLE\t1\t\t\t// = std::round_to_nearest\n#endif\n\n/// Tie-breaking behaviour for round to nearest.\n/// This specifies if ties in round to nearest should be resolved by rounding to the nearest even value. By default this is \n/// defined to `0` resulting in the faster but slightly more biased behaviour of rounding away from zero in half-way cases (and \n/// thus equal to the round() function), but can be redefined to `1` (before including half.hpp) if more IEEE-conformant \n/// behaviour is needed.\n#ifndef HALF_ROUND_TIES_TO_EVEN\n\t#define HALF_ROUND_TIES_TO_EVEN\t0\t\t// ties away from zero\n#endif\n\n/// Value signaling overflow.\n/// In correspondence with `HUGE_VAL[F|L]` from `<cmath>` this symbol expands to a positive value signaling the overflow of an \n/// operation, in particular it just evaluates to positive infinity.\n#define HUGE_VALH\tstd::numeric_limits<half_float::half>::infinity()\n\n/// Fast half-precision fma function.\n/// This symbol is only defined if the fma() function generally executes as fast as, or faster than, a separate \n/// half-precision multiplication followed by an addition. Due to the internal single-precision implementation of all \n/// arithmetic operations, this is in fact always the case.\n#define FP_FAST_FMAH\t1\n\n#ifndef FP_ILOGB0\n\t#define FP_ILOGB0\t\tINT_MIN\n#endif\n#ifndef FP_ILOGBNAN\n\t#define FP_ILOGBNAN\t\tINT_MAX\n#endif\n#ifndef FP_SUBNORMAL\n\t#define FP_SUBNORMAL\t0\n#endif\n#ifndef FP_ZERO\n\t#define FP_ZERO\t\t\t1\n#endif\n#ifndef FP_NAN\n\t#define FP_NAN\t\t\t2\n#endif\n#ifndef FP_INFINITE\n\t#define FP_INFINITE\t\t3\n#endif\n#ifndef FP_NORMAL\n\t#define FP_NORMAL\t\t4\n#endif\n\n\n/// Main namespace for half precision functionality.\n/// This namespace contains all the functionality provided by the library.\nnamespace half_float\n{\n\tclass half;\n\n#if HALF_ENABLE_CPP11_USER_LITERALS\n\t/// Library-defined half-precision literals.\n\t/// Import this namespace to enable half-precision floating point literals:\n\t/// ~~~~{.cpp}\n\t/// using namespace half_float::literal;\n\t/// half_float::half = 4.2_h;\n\t/// ~~~~\n\tnamespace literal\n\t{\n\t\thalf operator \"\" _h(long double);\n\t}\n#endif\n\n\t/// \\internal\n\t/// \\brief Implementation details.\n\tnamespace detail\n\t{\n\t#if HALF_ENABLE_CPP11_TYPE_TRAITS\n\t\t/// Conditional type.\n\t\ttemplate<bool B,typename T,typename F> struct conditional : std::conditional<B,T,F> {};\n\n\t\t/// Helper for tag dispatching.\n\t\ttemplate<bool B> struct bool_type : std::integral_constant<bool,B> {};\n\t\tusing std::true_type;\n\t\tusing std::false_type;\n\n\t\t/// Type traits for floating point types.\n\t\ttemplate<typename T> struct is_float : std::is_floating_point<T> {};\n\t#else\n\t\t/// Conditional type.\n\t\ttemplate<bool,typename T,typename> struct conditional { typedef T type; };\n\t\ttemplate<typename T,typename F> struct conditional<false,T,F> { typedef F type; };\n\n\t\t/// Helper for tag dispatching.\n\t\ttemplate<bool> struct bool_type {};\n\t\ttypedef bool_type<true> true_type;\n\t\ttypedef bool_type<false> false_type;\n\n\t\t/// Type traits for floating point types.\n\t\ttemplate<typename> struct is_float : false_type {};\n\t\ttemplate<typename T> struct is_float<const T> : is_float<T> {};\n\t\ttemplate<typename T> struct is_float<volatile T> : is_float<T> {};\n\t\ttemplate<typename T> struct is_float<const volatile T> : is_float<T> {};\n\t\ttemplate<> struct is_float<float> : true_type {};\n\t\ttemplate<> struct is_float<double> : true_type {};\n\t\ttemplate<> struct is_float<long double> : true_type {};\n\t#endif\n\n\t\t/// Type traits for floating point bits.\n\t\ttemplate<typename T> struct bits { typedef unsigned char type; };\n\t\ttemplate<typename T> struct bits<const T> : bits<T> {};\n\t\ttemplate<typename T> struct bits<volatile T> : bits<T> {};\n\t\ttemplate<typename T> struct bits<const volatile T> : bits<T> {};\n\n\t#if HALF_ENABLE_CPP11_CSTDINT\n\t\t/// Unsigned integer of (at least) 16 bits width.\n\t\ttypedef std::uint_least16_t uint16;\n\n\t\t/// Unsigned integer of (at least) 32 bits width.\n\t\ttemplate<> struct bits<float> { typedef std::uint_least32_t type; };\n\n\t\t/// Unsigned integer of (at least) 64 bits width.\n\t\ttemplate<> struct bits<double> { typedef std::uint_least64_t type; };\n\t#else\n\t\t/// Unsigned integer of (at least) 16 bits width.\n\t\ttypedef unsigned short uint16;\n\n\t\t/// Unsigned integer of (at least) 32 bits width.\n\t\ttemplate<> struct bits<float> : conditional<std::numeric_limits<unsigned int>::digits>=32,unsigned int,unsigned long> {};\n\n\t\t#if HALF_ENABLE_CPP11_LONG_LONG\n\t\t\t/// Unsigned integer of (at least) 64 bits width.\n\t\t\ttemplate<> struct bits<double> : conditional<std::numeric_limits<unsigned long>::digits>=64,unsigned long,unsigned long long> {};\n\t\t#else\n\t\t\t/// Unsigned integer of (at least) 64 bits width.\n\t\t\ttemplate<> struct bits<double> { typedef unsigned long type; };\n\t\t#endif\n\t#endif\n\n\t\t/// Tag type for binary construction.\n\t\tstruct binary_t {};\n\n\t\t/// Tag for binary construction.\n\t\tHALF_CONSTEXPR_CONST binary_t binary = binary_t();\n\n\t\t/// Temporary half-precision expression.\n\t\t/// This class represents a half-precision expression which just stores a single-precision value internally.\n\t\tstruct expr\n\t\t{\n\t\t\t/// Conversion constructor.\n\t\t\t/// \\param f single-precision value to convert\n\t\t\texplicit HALF_CONSTEXPR expr(float f) HALF_NOEXCEPT : value_(f) {}\n\n\t\t\t/// Conversion to single-precision.\n\t\t\t/// \\return single precision value representing expression value\n\t\t\tHALF_CONSTEXPR operator float() const HALF_NOEXCEPT { return value_; }\n\n\t\tprivate:\n\t\t\t/// Internal expression value stored in single-precision.\n\t\t\tfloat value_;\n\t\t};\n\n\t\t/// SFINAE helper for generic half-precision functions.\n\t\t/// This class template has to be specialized for each valid combination of argument types to provide a corresponding \n\t\t/// `type` member equivalent to \\a T.\n\t\t/// \\tparam T type to return\n\t\ttemplate<typename T,typename,typename=void,typename=void> struct enable {};\n\t\ttemplate<typename T> struct enable<T,half,void,void> { typedef T type; };\n\t\ttemplate<typename T> struct enable<T,expr,void,void> { typedef T type; };\n\t\ttemplate<typename T> struct enable<T,half,half,void> { typedef T type; };\n\t\ttemplate<typename T> struct enable<T,half,expr,void> { typedef T type; };\n\t\ttemplate<typename T> struct enable<T,expr,half,void> { typedef T type; };\n\t\ttemplate<typename T> struct enable<T,expr,expr,void> { typedef T type; };\n\t\ttemplate<typename T> struct enable<T,half,half,half> { typedef T type; };\n\t\ttemplate<typename T> struct enable<T,half,half,expr> { typedef T type; };\n\t\ttemplate<typename T> struct enable<T,half,expr,half> { typedef T type; };\n\t\ttemplate<typename T> struct enable<T,half,expr,expr> { typedef T type; };\n\t\ttemplate<typename T> struct enable<T,expr,half,half> { typedef T type; };\n\t\ttemplate<typename T> struct enable<T,expr,half,expr> { typedef T type; };\n\t\ttemplate<typename T> struct enable<T,expr,expr,half> { typedef T type; };\n\t\ttemplate<typename T> struct enable<T,expr,expr,expr> { typedef T type; };\n\n\t\t/// Return type for specialized generic 2-argument half-precision functions.\n\t\t/// This class template has to be specialized for each valid combination of argument types to provide a corresponding \n\t\t/// `type` member denoting the appropriate return type.\n\t\t/// \\tparam T first argument type\n\t\t/// \\tparam U first argument type\n\t\ttemplate<typename T,typename U> struct result : enable<expr,T,U> {};\n\t\ttemplate<> struct result<half,half> { typedef half type; };\n\n\t\t/// \\name Classification helpers\n\t\t/// \\{\n\n\t\t/// Check for infinity.\n\t\t/// \\tparam T argument type (builtin floating point type)\n\t\t/// \\param arg value to query\n\t\t/// \\retval true if infinity\n\t\t/// \\retval false else\n\t\ttemplate<typename T> bool builtin_isinf(T arg)\n\t\t{\n\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\treturn std::isinf(arg);\n\t\t#elif defined(_MSC_VER)\n\t\t\treturn !::_finite(static_cast<double>(arg)) && !::_isnan(static_cast<double>(arg));\n\t\t#else\n\t\t\treturn arg == std::numeric_limits<T>::infinity() || arg == -std::numeric_limits<T>::infinity();\n\t\t#endif\n\t\t}\n\n\t\t/// Check for NaN.\n\t\t/// \\tparam T argument type (builtin floating point type)\n\t\t/// \\param arg value to query\n\t\t/// \\retval true if not a number\n\t\t/// \\retval false else\n\t\ttemplate<typename T> bool builtin_isnan(T arg)\n\t\t{\n\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\treturn std::isnan(arg);\n\t\t#elif defined(_MSC_VER)\n\t\t\treturn ::_isnan(static_cast<double>(arg)) != 0;\n\t\t#else\n\t\t\treturn arg != arg;\n\t\t#endif\n\t\t}\n\n\t\t/// Check sign.\n\t\t/// \\tparam T argument type (builtin floating point type)\n\t\t/// \\param arg value to query\n\t\t/// \\retval true if signbit set\n\t\t/// \\retval false else\n\t\ttemplate<typename T> bool builtin_signbit(T arg)\n\t\t{\n\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\treturn std::signbit(arg);\n\t\t#else\n\t\t\treturn arg < T() || (arg == T() && T(1)/arg < T());\n\t\t#endif\n\t\t}\n\n\t\t/// \\}\n\t\t/// \\name Conversion\n\t\t/// \\{\n\n\t\t/// Convert IEEE single-precision to half-precision.\n\t\t/// Credit for this goes to [Jeroen van der Zijp](ftp://ftp.fox-toolkit.org/pub/fasthalffloatconversion.pdf).\n\t\t/// \\tparam R rounding mode to use, `std::round_indeterminate` for fastest rounding\n\t\t/// \\param value single-precision value\n\t\t/// \\return binary representation of half-precision value\n\t\ttemplate<std::float_round_style R> uint16 float2half_impl(float value, true_type)\n\t\t{\n\t\t\ttypedef bits<float>::type uint32;\n\t\t\tuint32 bits;// = *reinterpret_cast<uint32*>(&value);\t\t//violating strict aliasing!\n\t\t\tstd::memcpy(&bits, &value, sizeof(float));\n/*\t\t\tuint16 hbits = (bits>>16) & 0x8000;\n\t\t\tbits &= 0x7FFFFFFF;\n\t\t\tint exp = bits >> 23;\n\t\t\tif(exp == 255)\n\t\t\t\treturn hbits | 0x7C00 | (0x3FF&-static_cast<unsigned>((bits&0x7FFFFF)!=0));\n\t\t\tif(exp > 142)\n\t\t\t{\n\t\t\t\tif(R == std::round_toward_infinity)\n\t\t\t\t\treturn hbits | 0x7C00 - (hbits>>15);\n\t\t\t\tif(R == std::round_toward_neg_infinity)\n\t\t\t\t\treturn hbits | 0x7BFF + (hbits>>15);\n\t\t\t\treturn hbits | 0x7BFF + (R!=std::round_toward_zero);\n\t\t\t}\n\t\t\tint g, s;\n\t\t\tif(exp > 112)\n\t\t\t{\n\t\t\t\tg = (bits>>12) & 1;\n\t\t\t\ts = (bits&0xFFF) != 0;\n\t\t\t\thbits |= ((exp-112)<<10) | ((bits>>13)&0x3FF);\n\t\t\t}\n\t\t\telse if(exp > 101)\n\t\t\t{\n\t\t\t\tint i = 125 - exp;\n\t\t\t\tbits = (bits&0x7FFFFF) | 0x800000;\n\t\t\t\tg = (bits>>i) & 1;\n\t\t\t\ts = (bits&((1L<<i)-1)) != 0;\n\t\t\t\thbits |= bits >> (i+1);\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tg = 0;\n\t\t\t\ts = bits != 0;\n\t\t\t}\n\t\t\tif(R == std::round_to_nearest)\n\t\t\t\t#if HALF_ROUND_TIES_TO_EVEN\n\t\t\t\t\thbits += g & (s|hbits);\n\t\t\t\t#else\n\t\t\t\t\thbits += g;\n\t\t\t\t#endif\n\t\t\telse if(R == std::round_toward_infinity)\n\t\t\t\thbits += ~(hbits>>15) & (s|g);\n\t\t\telse if(R == std::round_toward_neg_infinity)\n\t\t\t\thbits += (hbits>>15) & (g|s);\n*/\t\t\tstatic const uint16 base_table[512] = { \n\t\t\t\t0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, \n\t\t\t\t0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, \n\t\t\t\t0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, \n\t\t\t\t0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, \n\t\t\t\t0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, \n\t\t\t\t0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, \n\t\t\t\t0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0001, 0x0002, 0x0004, 0x0008, 0x0010, 0x0020, 0x0040, 0x0080, 0x0100, \n\t\t\t\t0x0200, 0x0400, 0x0800, 0x0C00, 0x1000, 0x1400, 0x1800, 0x1C00, 0x2000, 0x2400, 0x2800, 0x2C00, 0x3000, 0x3400, 0x3800, 0x3C00, \n\t\t\t\t0x4000, 0x4400, 0x4800, 0x4C00, 0x5000, 0x5400, 0x5800, 0x5C00, 0x6000, 0x6400, 0x6800, 0x6C00, 0x7000, 0x7400, 0x7800, 0x7C00, \n\t\t\t\t0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, \n\t\t\t\t0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, \n\t\t\t\t0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, \n\t\t\t\t0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, \n\t\t\t\t0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, \n\t\t\t\t0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, \n\t\t\t\t0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, \n\t\t\t\t0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, \n\t\t\t\t0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, \n\t\t\t\t0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, \n\t\t\t\t0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, \n\t\t\t\t0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, \n\t\t\t\t0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, \n\t\t\t\t0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8001, 0x8002, 0x8004, 0x8008, 0x8010, 0x8020, 0x8040, 0x8080, 0x8100, \n\t\t\t\t0x8200, 0x8400, 0x8800, 0x8C00, 0x9000, 0x9400, 0x9800, 0x9C00, 0xA000, 0xA400, 0xA800, 0xAC00, 0xB000, 0xB400, 0xB800, 0xBC00, \n\t\t\t\t0xC000, 0xC400, 0xC800, 0xCC00, 0xD000, 0xD400, 0xD800, 0xDC00, 0xE000, 0xE400, 0xE800, 0xEC00, 0xF000, 0xF400, 0xF800, 0xFC00, \n\t\t\t\t0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, \n\t\t\t\t0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, \n\t\t\t\t0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, \n\t\t\t\t0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, \n\t\t\t\t0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, \n\t\t\t\t0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, \n\t\t\t\t0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00 };\n\t\t\tstatic const unsigned char shift_table[512] = { \n\t\t\t\t24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, \n\t\t\t\t24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, \n\t\t\t\t24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, \n\t\t\t\t24, 24, 24, 24, 24, 24, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, \n\t\t\t\t13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, \n\t\t\t\t24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, \n\t\t\t\t24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, \n\t\t\t\t24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 13, \n\t\t\t\t24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, \n\t\t\t\t24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, \n\t\t\t\t24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, \n\t\t\t\t24, 24, 24, 24, 24, 24, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, \n\t\t\t\t13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, \n\t\t\t\t24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, \n\t\t\t\t24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, \n\t\t\t\t24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 13 };\n\t\t\tuint16 hbits = base_table[bits>>23] + static_cast<uint16>((bits&0x7FFFFF)>>shift_table[bits>>23]);\n\t\t\tif(R == std::round_to_nearest)\n\t\t\t\thbits += (((bits&0x7FFFFF)>>(shift_table[bits>>23]-1))|(((bits>>23)&0xFF)==102)) & ((hbits&0x7C00)!=0x7C00)\n\t\t\t\t#if HALF_ROUND_TIES_TO_EVEN\n\t\t\t\t\t& (((((static_cast<uint32>(1)<<(shift_table[bits>>23]-1))-1)&bits)!=0)|hbits)\n\t\t\t\t#endif\n\t\t\t\t;\n\t\t\telse if(R == std::round_toward_zero)\n\t\t\t\thbits -= ((hbits&0x7FFF)==0x7C00) & ~shift_table[bits>>23];\n\t\t\telse if(R == std::round_toward_infinity)\n\t\t\t\thbits += ((((bits&0x7FFFFF&((static_cast<uint32>(1)<<(shift_table[bits>>23]))-1))!=0)|(((bits>>23)<=102)&\n\t\t\t\t\t((bits>>23)!=0)))&(hbits<0x7C00)) - ((hbits==0xFC00)&((bits>>23)!=511));\n\t\t\telse if(R == std::round_toward_neg_infinity)\n\t\t\t\thbits += ((((bits&0x7FFFFF&((static_cast<uint32>(1)<<(shift_table[bits>>23]))-1))!=0)|(((bits>>23)<=358)&\n\t\t\t\t\t((bits>>23)!=256)))&(hbits<0xFC00)&(hbits>>15)) - ((hbits==0x7C00)&((bits>>23)!=255));\n\t\t\treturn hbits;\n\t\t}\n\n\t\t/// Convert IEEE double-precision to half-precision.\n\t\t/// \\tparam R rounding mode to use, `std::round_indeterminate` for fastest rounding\n\t\t/// \\param value double-precision value\n\t\t/// \\return binary representation of half-precision value\n\t\ttemplate<std::float_round_style R> uint16 float2half_impl(double value, true_type)\n\t\t{\n\t\t\ttypedef bits<float>::type uint32;\n\t\t\ttypedef bits<double>::type uint64;\n\t\t\tuint64 bits;// = *reinterpret_cast<uint64*>(&value);\t\t//violating strict aliasing!\n\t\t\tstd::memcpy(&bits, &value, sizeof(double));\n\t\t\tuint32 hi = bits >> 32, lo = bits & 0xFFFFFFFF;\n\t\t\tuint16 hbits = (hi>>16) & 0x8000;\n\t\t\thi &= 0x7FFFFFFF;\n\t\t\tint exp = hi >> 20;\n\t\t\tif(exp == 2047)\n\t\t\t\treturn hbits | 0x7C00 | (0x3FF&-static_cast<unsigned>((bits&0xFFFFFFFFFFFFF)!=0));\n\t\t\tif(exp > 1038)\n\t\t\t{\n\t\t\t\tif(R == std::round_toward_infinity)\n\t\t\t\t\treturn hbits | 0x7C00 - (hbits>>15);\n\t\t\t\tif(R == std::round_toward_neg_infinity)\n\t\t\t\t\treturn hbits | 0x7BFF + (hbits>>15);\n\t\t\t\treturn hbits | 0x7BFF + (R!=std::round_toward_zero);\n\t\t\t}\n\t\t\tint g, s = lo != 0;\n\t\t\tif(exp > 1008)\n\t\t\t{\n\t\t\t\tg = (hi>>9) & 1;\n\t\t\t\ts |= (hi&0x1FF) != 0;\n\t\t\t\thbits |= ((exp-1008)<<10) | ((hi>>10)&0x3FF);\n\t\t\t}\n\t\t\telse if(exp > 997)\n\t\t\t{\n\t\t\t\tint i = 1018 - exp;\n\t\t\t\thi = (hi&0xFFFFF) | 0x100000;\n\t\t\t\tg = (hi>>i) & 1;\n\t\t\t\ts |= (hi&((1L<<i)-1)) != 0;\n\t\t\t\thbits |= hi >> (i+1);\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tg = 0;\n\t\t\t\ts |= hi != 0;\n\t\t\t}\n\t\t\tif(R == std::round_to_nearest)\n\t\t\t\t#if HALF_ROUND_TIES_TO_EVEN\n\t\t\t\t\thbits += g & (s|hbits);\n\t\t\t\t#else\n\t\t\t\t\thbits += g;\n\t\t\t\t#endif\n\t\t\telse if(R == std::round_toward_infinity)\n\t\t\t\thbits += ~(hbits>>15) & (s|g);\n\t\t\telse if(R == std::round_toward_neg_infinity)\n\t\t\t\thbits += (hbits>>15) & (g|s);\n\t\t\treturn hbits;\n\t\t}\n\n\t\t/// Convert non-IEEE floating point to half-precision.\n\t\t/// \\tparam R rounding mode to use, `std::round_indeterminate` for fastest rounding\n\t\t/// \\tparam T source type (builtin floating point type)\n\t\t/// \\param value floating point value\n\t\t/// \\return binary representation of half-precision value\n\t\ttemplate<std::float_round_style R,typename T> uint16 float2half_impl(T value, false_type) \n\t\t{\n\t\t\tuint16 hbits = static_cast<unsigned>(builtin_signbit(value)) << 15;\n\t\t\tif(value == T())\n\t\t\t\treturn hbits;\n\t\t\tif(builtin_isnan(value))\n\t\t\t\treturn hbits | 0x7FFF;\n\t\t\tif(builtin_isinf(value))\n\t\t\t\treturn hbits | 0x7C00;\n\t\t\tint exp;\n\t\t\tstd::frexp(value, &exp);\n\t\t\tif(exp > 16)\n\t\t\t{\n\t\t\t\tif(R == std::round_toward_infinity)\n\t\t\t\t\treturn hbits | (0x7C00 - (hbits>>15));\n\t\t\t\tif(R == std::round_toward_neg_infinity)\n\t\t\t\t\treturn hbits | (0x7BFF + (hbits>>15));\n\t\t\t\treturn hbits | (0x7BFF + (R!=std::round_toward_zero));\n\t\t\t}\n\t\t\tif(exp < -13)\n\t\t\t\tvalue = std::ldexp(value, 24);\n\t\t\telse\n\t\t\t{\n\t\t\t\tvalue = std::ldexp(value, 11-exp);\n\t\t\t\thbits |= ((exp+13)<<10);\n\t\t\t}\n\t\t\tT ival, frac = std::modf(value, &ival);\n\t\t\thbits += static_cast<uint16>(std::abs(static_cast<int>(ival)));\n\t\t\tif(R == std::round_to_nearest)\n\t\t\t{\n\t\t\t\tfrac = std::abs(frac);\n\t\t\t\t#if HALF_ROUND_TIES_TO_EVEN\n\t\t\t\t\thbits += (frac>T(0.5)) | ((frac==T(0.5))&hbits);\n\t\t\t\t#else\n\t\t\t\t\thbits += frac >= T(0.5);\n\t\t\t\t#endif\n\t\t\t}\n\t\t\telse if(R == std::round_toward_infinity)\n\t\t\t\thbits += frac > T();\n\t\t\telse if(R == std::round_toward_neg_infinity)\n\t\t\t\thbits += frac < T();\n\t\t\treturn hbits;\n\t\t}\n\n\t\t/// Convert floating point to half-precision.\n\t\t/// \\tparam R rounding mode to use, `std::round_indeterminate` for fastest rounding\n\t\t/// \\tparam T source type (builtin floating point type)\n\t\t/// \\param value floating point value\n\t\t/// \\return binary representation of half-precision value\n\t\ttemplate<std::float_round_style R,typename T> uint16 float2half(T value)\n\t\t{\n\t\t\treturn float2half_impl<R>(value, bool_type<std::numeric_limits<T>::is_iec559&&sizeof(typename bits<T>::type)==sizeof(T)>());\n\t\t}\n\n\t\t/// Convert integer to half-precision floating point.\n\t\t/// \\tparam R rounding mode to use, `std::round_indeterminate` for fastest rounding\n\t\t/// \\tparam S `true` if value negative, `false` else\n\t\t/// \\tparam T type to convert (builtin integer type)\n\t\t/// \\param value non-negative integral value\n\t\t/// \\return binary representation of half-precision value\n\t\ttemplate<std::float_round_style R,bool S,typename T> uint16 int2half_impl(T value)\n\t\t{\n\t\t#if HALF_ENABLE_CPP11_STATIC_ASSERT && HALF_ENABLE_CPP11_TYPE_TRAITS\n\t\t\tstatic_assert(std::is_integral<T>::value, \"int to half conversion only supports builtin integer types\");\n\t\t#endif\n\t\t\tif(S)\n\t\t\t\tvalue = -value;\n\t\t\tuint16 bits = S << 15;\n\t\t\tif(value > 0xFFFF)\n\t\t\t{\n\t\t\t\tif(R == std::round_toward_infinity)\n\t\t\t\t\tbits |= 0x7C00 - S;\n\t\t\t\telse if(R == std::round_toward_neg_infinity)\n\t\t\t\t\tbits |= 0x7BFF + S;\n\t\t\t\telse\n\t\t\t\t\tbits |= 0x7BFF + (R!=std::round_toward_zero);\n\t\t\t}\n\t\t\telse if(value)\n\t\t\t{\n\t\t\t\tunsigned int m = value, exp = 24;\n\t\t\t\tfor(; m<0x400; m<<=1,--exp) ;\n\t\t\t\tfor(; m>0x7FF; m>>=1,++exp) ;\n\t\t\t\tbits |= (exp<<10) + m;\n\t\t\t\tif(exp > 24)\n\t\t\t\t{\n\t\t\t\t\tif(R == std::round_to_nearest)\n\t\t\t\t\t\tbits += (value>>(exp-25)) & 1\n\t\t\t\t\t\t#if HALF_ROUND_TIES_TO_EVEN\n\t\t\t\t\t\t\t& (((((1<<(exp-25))-1)&value)!=0)|bits)\n\t\t\t\t\t\t#endif\n\t\t\t\t\t\t;\n\t\t\t\t\telse if(R == std::round_toward_infinity)\n\t\t\t\t\t\tbits += ((value&((1<<(exp-24))-1))!=0) & !S;\n\t\t\t\t\telse if(R == std::round_toward_neg_infinity)\n\t\t\t\t\t\tbits += ((value&((1<<(exp-24))-1))!=0) & S;\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn bits;\n\t\t}\n\n\t\t/// Convert integer to half-precision floating point.\n\t\t/// \\tparam R rounding mode to use, `std::round_indeterminate` for fastest rounding\n\t\t/// \\tparam T type to convert (builtin integer type)\n\t\t/// \\param value integral value\n\t\t/// \\return binary representation of half-precision value\n\t\ttemplate<std::float_round_style R,typename T> uint16 int2half(T value)\n\t\t{\n\t\t\treturn (value<0) ? int2half_impl<R,true>(value) : int2half_impl<R,false>(value);\n\t\t}\n\n\t\t/// Convert half-precision to IEEE single-precision.\n\t\t/// Credit for this goes to [Jeroen van der Zijp](ftp://ftp.fox-toolkit.org/pub/fasthalffloatconversion.pdf).\n\t\t/// \\param value binary representation of half-precision value\n\t\t/// \\return single-precision value\n\t\tinline float half2float_impl(uint16 value, float, true_type)\n\t\t{\n\t\t\ttypedef bits<float>::type uint32;\n/*\t\t\tuint32 bits = static_cast<uint32>(value&0x8000) << 16;\n\t\t\tint abs = value & 0x7FFF;\n\t\t\tif(abs)\n\t\t\t{\n\t\t\t\tbits |= 0x38000000 << static_cast<unsigned>(abs>=0x7C00);\n\t\t\t\tfor(; abs<0x400; abs<<=1,bits-=0x800000) ;\n\t\t\t\tbits += static_cast<uint32>(abs) << 13;\n\t\t\t}\n*/\t\t\tstatic const uint32 mantissa_table[2048] = { \n\t\t\t\t0x00000000, 0x33800000, 0x34000000, 0x34400000, 0x34800000, 0x34A00000, 0x34C00000, 0x34E00000, 0x35000000, 0x35100000, 0x35200000, 0x35300000, 0x35400000, 0x35500000, 0x35600000, 0x35700000, \n\t\t\t\t0x35800000, 0x35880000, 0x35900000, 0x35980000, 0x35A00000, 0x35A80000, 0x35B00000, 0x35B80000, 0x35C00000, 0x35C80000, 0x35D00000, 0x35D80000, 0x35E00000, 0x35E80000, 0x35F00000, 0x35F80000, \n\t\t\t\t0x36000000, 0x36040000, 0x36080000, 0x360C0000, 0x36100000, 0x36140000, 0x36180000, 0x361C0000, 0x36200000, 0x36240000, 0x36280000, 0x362C0000, 0x36300000, 0x36340000, 0x36380000, 0x363C0000, \n\t\t\t\t0x36400000, 0x36440000, 0x36480000, 0x364C0000, 0x36500000, 0x36540000, 0x36580000, 0x365C0000, 0x36600000, 0x36640000, 0x36680000, 0x366C0000, 0x36700000, 0x36740000, 0x36780000, 0x367C0000, \n\t\t\t\t0x36800000, 0x36820000, 0x36840000, 0x36860000, 0x36880000, 0x368A0000, 0x368C0000, 0x368E0000, 0x36900000, 0x36920000, 0x36940000, 0x36960000, 0x36980000, 0x369A0000, 0x369C0000, 0x369E0000, \n\t\t\t\t0x36A00000, 0x36A20000, 0x36A40000, 0x36A60000, 0x36A80000, 0x36AA0000, 0x36AC0000, 0x36AE0000, 0x36B00000, 0x36B20000, 0x36B40000, 0x36B60000, 0x36B80000, 0x36BA0000, 0x36BC0000, 0x36BE0000, \n\t\t\t\t0x36C00000, 0x36C20000, 0x36C40000, 0x36C60000, 0x36C80000, 0x36CA0000, 0x36CC0000, 0x36CE0000, 0x36D00000, 0x36D20000, 0x36D40000, 0x36D60000, 0x36D80000, 0x36DA0000, 0x36DC0000, 0x36DE0000, \n\t\t\t\t0x36E00000, 0x36E20000, 0x36E40000, 0x36E60000, 0x36E80000, 0x36EA0000, 0x36EC0000, 0x36EE0000, 0x36F00000, 0x36F20000, 0x36F40000, 0x36F60000, 0x36F80000, 0x36FA0000, 0x36FC0000, 0x36FE0000, \n\t\t\t\t0x37000000, 0x37010000, 0x37020000, 0x37030000, 0x37040000, 0x37050000, 0x37060000, 0x37070000, 0x37080000, 0x37090000, 0x370A0000, 0x370B0000, 0x370C0000, 0x370D0000, 0x370E0000, 0x370F0000, \n\t\t\t\t0x37100000, 0x37110000, 0x37120000, 0x37130000, 0x37140000, 0x37150000, 0x37160000, 0x37170000, 0x37180000, 0x37190000, 0x371A0000, 0x371B0000, 0x371C0000, 0x371D0000, 0x371E0000, 0x371F0000, \n\t\t\t\t0x37200000, 0x37210000, 0x37220000, 0x37230000, 0x37240000, 0x37250000, 0x37260000, 0x37270000, 0x37280000, 0x37290000, 0x372A0000, 0x372B0000, 0x372C0000, 0x372D0000, 0x372E0000, 0x372F0000, \n\t\t\t\t0x37300000, 0x37310000, 0x37320000, 0x37330000, 0x37340000, 0x37350000, 0x37360000, 0x37370000, 0x37380000, 0x37390000, 0x373A0000, 0x373B0000, 0x373C0000, 0x373D0000, 0x373E0000, 0x373F0000, \n\t\t\t\t0x37400000, 0x37410000, 0x37420000, 0x37430000, 0x37440000, 0x37450000, 0x37460000, 0x37470000, 0x37480000, 0x37490000, 0x374A0000, 0x374B0000, 0x374C0000, 0x374D0000, 0x374E0000, 0x374F0000, \n\t\t\t\t0x37500000, 0x37510000, 0x37520000, 0x37530000, 0x37540000, 0x37550000, 0x37560000, 0x37570000, 0x37580000, 0x37590000, 0x375A0000, 0x375B0000, 0x375C0000, 0x375D0000, 0x375E0000, 0x375F0000, \n\t\t\t\t0x37600000, 0x37610000, 0x37620000, 0x37630000, 0x37640000, 0x37650000, 0x37660000, 0x37670000, 0x37680000, 0x37690000, 0x376A0000, 0x376B0000, 0x376C0000, 0x376D0000, 0x376E0000, 0x376F0000, \n\t\t\t\t0x37700000, 0x37710000, 0x37720000, 0x37730000, 0x37740000, 0x37750000, 0x37760000, 0x37770000, 0x37780000, 0x37790000, 0x377A0000, 0x377B0000, 0x377C0000, 0x377D0000, 0x377E0000, 0x377F0000, \n\t\t\t\t0x37800000, 0x37808000, 0x37810000, 0x37818000, 0x37820000, 0x37828000, 0x37830000, 0x37838000, 0x37840000, 0x37848000, 0x37850000, 0x37858000, 0x37860000, 0x37868000, 0x37870000, 0x37878000, \n\t\t\t\t0x37880000, 0x37888000, 0x37890000, 0x37898000, 0x378A0000, 0x378A8000, 0x378B0000, 0x378B8000, 0x378C0000, 0x378C8000, 0x378D0000, 0x378D8000, 0x378E0000, 0x378E8000, 0x378F0000, 0x378F8000, \n\t\t\t\t0x37900000, 0x37908000, 0x37910000, 0x37918000, 0x37920000, 0x37928000, 0x37930000, 0x37938000, 0x37940000, 0x37948000, 0x37950000, 0x37958000, 0x37960000, 0x37968000, 0x37970000, 0x37978000, \n\t\t\t\t0x37980000, 0x37988000, 0x37990000, 0x37998000, 0x379A0000, 0x379A8000, 0x379B0000, 0x379B8000, 0x379C0000, 0x379C8000, 0x379D0000, 0x379D8000, 0x379E0000, 0x379E8000, 0x379F0000, 0x379F8000, \n\t\t\t\t0x37A00000, 0x37A08000, 0x37A10000, 0x37A18000, 0x37A20000, 0x37A28000, 0x37A30000, 0x37A38000, 0x37A40000, 0x37A48000, 0x37A50000, 0x37A58000, 0x37A60000, 0x37A68000, 0x37A70000, 0x37A78000, \n\t\t\t\t0x37A80000, 0x37A88000, 0x37A90000, 0x37A98000, 0x37AA0000, 0x37AA8000, 0x37AB0000, 0x37AB8000, 0x37AC0000, 0x37AC8000, 0x37AD0000, 0x37AD8000, 0x37AE0000, 0x37AE8000, 0x37AF0000, 0x37AF8000, \n\t\t\t\t0x37B00000, 0x37B08000, 0x37B10000, 0x37B18000, 0x37B20000, 0x37B28000, 0x37B30000, 0x37B38000, 0x37B40000, 0x37B48000, 0x37B50000, 0x37B58000, 0x37B60000, 0x37B68000, 0x37B70000, 0x37B78000, \n\t\t\t\t0x37B80000, 0x37B88000, 0x37B90000, 0x37B98000, 0x37BA0000, 0x37BA8000, 0x37BB0000, 0x37BB8000, 0x37BC0000, 0x37BC8000, 0x37BD0000, 0x37BD8000, 0x37BE0000, 0x37BE8000, 0x37BF0000, 0x37BF8000, \n\t\t\t\t0x37C00000, 0x37C08000, 0x37C10000, 0x37C18000, 0x37C20000, 0x37C28000, 0x37C30000, 0x37C38000, 0x37C40000, 0x37C48000, 0x37C50000, 0x37C58000, 0x37C60000, 0x37C68000, 0x37C70000, 0x37C78000, \n\t\t\t\t0x37C80000, 0x37C88000, 0x37C90000, 0x37C98000, 0x37CA0000, 0x37CA8000, 0x37CB0000, 0x37CB8000, 0x37CC0000, 0x37CC8000, 0x37CD0000, 0x37CD8000, 0x37CE0000, 0x37CE8000, 0x37CF0000, 0x37CF8000, \n\t\t\t\t0x37D00000, 0x37D08000, 0x37D10000, 0x37D18000, 0x37D20000, 0x37D28000, 0x37D30000, 0x37D38000, 0x37D40000, 0x37D48000, 0x37D50000, 0x37D58000, 0x37D60000, 0x37D68000, 0x37D70000, 0x37D78000, \n\t\t\t\t0x37D80000, 0x37D88000, 0x37D90000, 0x37D98000, 0x37DA0000, 0x37DA8000, 0x37DB0000, 0x37DB8000, 0x37DC0000, 0x37DC8000, 0x37DD0000, 0x37DD8000, 0x37DE0000, 0x37DE8000, 0x37DF0000, 0x37DF8000, \n\t\t\t\t0x37E00000, 0x37E08000, 0x37E10000, 0x37E18000, 0x37E20000, 0x37E28000, 0x37E30000, 0x37E38000, 0x37E40000, 0x37E48000, 0x37E50000, 0x37E58000, 0x37E60000, 0x37E68000, 0x37E70000, 0x37E78000, \n\t\t\t\t0x37E80000, 0x37E88000, 0x37E90000, 0x37E98000, 0x37EA0000, 0x37EA8000, 0x37EB0000, 0x37EB8000, 0x37EC0000, 0x37EC8000, 0x37ED0000, 0x37ED8000, 0x37EE0000, 0x37EE8000, 0x37EF0000, 0x37EF8000, \n\t\t\t\t0x37F00000, 0x37F08000, 0x37F10000, 0x37F18000, 0x37F20000, 0x37F28000, 0x37F30000, 0x37F38000, 0x37F40000, 0x37F48000, 0x37F50000, 0x37F58000, 0x37F60000, 0x37F68000, 0x37F70000, 0x37F78000, \n\t\t\t\t0x37F80000, 0x37F88000, 0x37F90000, 0x37F98000, 0x37FA0000, 0x37FA8000, 0x37FB0000, 0x37FB8000, 0x37FC0000, 0x37FC8000, 0x37FD0000, 0x37FD8000, 0x37FE0000, 0x37FE8000, 0x37FF0000, 0x37FF8000, \n\t\t\t\t0x38000000, 0x38004000, 0x38008000, 0x3800C000, 0x38010000, 0x38014000, 0x38018000, 0x3801C000, 0x38020000, 0x38024000, 0x38028000, 0x3802C000, 0x38030000, 0x38034000, 0x38038000, 0x3803C000, \n\t\t\t\t0x38040000, 0x38044000, 0x38048000, 0x3804C000, 0x38050000, 0x38054000, 0x38058000, 0x3805C000, 0x38060000, 0x38064000, 0x38068000, 0x3806C000, 0x38070000, 0x38074000, 0x38078000, 0x3807C000, \n\t\t\t\t0x38080000, 0x38084000, 0x38088000, 0x3808C000, 0x38090000, 0x38094000, 0x38098000, 0x3809C000, 0x380A0000, 0x380A4000, 0x380A8000, 0x380AC000, 0x380B0000, 0x380B4000, 0x380B8000, 0x380BC000, \n\t\t\t\t0x380C0000, 0x380C4000, 0x380C8000, 0x380CC000, 0x380D0000, 0x380D4000, 0x380D8000, 0x380DC000, 0x380E0000, 0x380E4000, 0x380E8000, 0x380EC000, 0x380F0000, 0x380F4000, 0x380F8000, 0x380FC000, \n\t\t\t\t0x38100000, 0x38104000, 0x38108000, 0x3810C000, 0x38110000, 0x38114000, 0x38118000, 0x3811C000, 0x38120000, 0x38124000, 0x38128000, 0x3812C000, 0x38130000, 0x38134000, 0x38138000, 0x3813C000, \n\t\t\t\t0x38140000, 0x38144000, 0x38148000, 0x3814C000, 0x38150000, 0x38154000, 0x38158000, 0x3815C000, 0x38160000, 0x38164000, 0x38168000, 0x3816C000, 0x38170000, 0x38174000, 0x38178000, 0x3817C000, \n\t\t\t\t0x38180000, 0x38184000, 0x38188000, 0x3818C000, 0x38190000, 0x38194000, 0x38198000, 0x3819C000, 0x381A0000, 0x381A4000, 0x381A8000, 0x381AC000, 0x381B0000, 0x381B4000, 0x381B8000, 0x381BC000, \n\t\t\t\t0x381C0000, 0x381C4000, 0x381C8000, 0x381CC000, 0x381D0000, 0x381D4000, 0x381D8000, 0x381DC000, 0x381E0000, 0x381E4000, 0x381E8000, 0x381EC000, 0x381F0000, 0x381F4000, 0x381F8000, 0x381FC000, \n\t\t\t\t0x38200000, 0x38204000, 0x38208000, 0x3820C000, 0x38210000, 0x38214000, 0x38218000, 0x3821C000, 0x38220000, 0x38224000, 0x38228000, 0x3822C000, 0x38230000, 0x38234000, 0x38238000, 0x3823C000, \n\t\t\t\t0x38240000, 0x38244000, 0x38248000, 0x3824C000, 0x38250000, 0x38254000, 0x38258000, 0x3825C000, 0x38260000, 0x38264000, 0x38268000, 0x3826C000, 0x38270000, 0x38274000, 0x38278000, 0x3827C000, \n\t\t\t\t0x38280000, 0x38284000, 0x38288000, 0x3828C000, 0x38290000, 0x38294000, 0x38298000, 0x3829C000, 0x382A0000, 0x382A4000, 0x382A8000, 0x382AC000, 0x382B0000, 0x382B4000, 0x382B8000, 0x382BC000, \n\t\t\t\t0x382C0000, 0x382C4000, 0x382C8000, 0x382CC000, 0x382D0000, 0x382D4000, 0x382D8000, 0x382DC000, 0x382E0000, 0x382E4000, 0x382E8000, 0x382EC000, 0x382F0000, 0x382F4000, 0x382F8000, 0x382FC000, \n\t\t\t\t0x38300000, 0x38304000, 0x38308000, 0x3830C000, 0x38310000, 0x38314000, 0x38318000, 0x3831C000, 0x38320000, 0x38324000, 0x38328000, 0x3832C000, 0x38330000, 0x38334000, 0x38338000, 0x3833C000, \n\t\t\t\t0x38340000, 0x38344000, 0x38348000, 0x3834C000, 0x38350000, 0x38354000, 0x38358000, 0x3835C000, 0x38360000, 0x38364000, 0x38368000, 0x3836C000, 0x38370000, 0x38374000, 0x38378000, 0x3837C000, \n\t\t\t\t0x38380000, 0x38384000, 0x38388000, 0x3838C000, 0x38390000, 0x38394000, 0x38398000, 0x3839C000, 0x383A0000, 0x383A4000, 0x383A8000, 0x383AC000, 0x383B0000, 0x383B4000, 0x383B8000, 0x383BC000, \n\t\t\t\t0x383C0000, 0x383C4000, 0x383C8000, 0x383CC000, 0x383D0000, 0x383D4000, 0x383D8000, 0x383DC000, 0x383E0000, 0x383E4000, 0x383E8000, 0x383EC000, 0x383F0000, 0x383F4000, 0x383F8000, 0x383FC000, \n\t\t\t\t0x38400000, 0x38404000, 0x38408000, 0x3840C000, 0x38410000, 0x38414000, 0x38418000, 0x3841C000, 0x38420000, 0x38424000, 0x38428000, 0x3842C000, 0x38430000, 0x38434000, 0x38438000, 0x3843C000, \n\t\t\t\t0x38440000, 0x38444000, 0x38448000, 0x3844C000, 0x38450000, 0x38454000, 0x38458000, 0x3845C000, 0x38460000, 0x38464000, 0x38468000, 0x3846C000, 0x38470000, 0x38474000, 0x38478000, 0x3847C000, \n\t\t\t\t0x38480000, 0x38484000, 0x38488000, 0x3848C000, 0x38490000, 0x38494000, 0x38498000, 0x3849C000, 0x384A0000, 0x384A4000, 0x384A8000, 0x384AC000, 0x384B0000, 0x384B4000, 0x384B8000, 0x384BC000, \n\t\t\t\t0x384C0000, 0x384C4000, 0x384C8000, 0x384CC000, 0x384D0000, 0x384D4000, 0x384D8000, 0x384DC000, 0x384E0000, 0x384E4000, 0x384E8000, 0x384EC000, 0x384F0000, 0x384F4000, 0x384F8000, 0x384FC000, \n\t\t\t\t0x38500000, 0x38504000, 0x38508000, 0x3850C000, 0x38510000, 0x38514000, 0x38518000, 0x3851C000, 0x38520000, 0x38524000, 0x38528000, 0x3852C000, 0x38530000, 0x38534000, 0x38538000, 0x3853C000, \n\t\t\t\t0x38540000, 0x38544000, 0x38548000, 0x3854C000, 0x38550000, 0x38554000, 0x38558000, 0x3855C000, 0x38560000, 0x38564000, 0x38568000, 0x3856C000, 0x38570000, 0x38574000, 0x38578000, 0x3857C000, \n\t\t\t\t0x38580000, 0x38584000, 0x38588000, 0x3858C000, 0x38590000, 0x38594000, 0x38598000, 0x3859C000, 0x385A0000, 0x385A4000, 0x385A8000, 0x385AC000, 0x385B0000, 0x385B4000, 0x385B8000, 0x385BC000, \n\t\t\t\t0x385C0000, 0x385C4000, 0x385C8000, 0x385CC000, 0x385D0000, 0x385D4000, 0x385D8000, 0x385DC000, 0x385E0000, 0x385E4000, 0x385E8000, 0x385EC000, 0x385F0000, 0x385F4000, 0x385F8000, 0x385FC000, \n\t\t\t\t0x38600000, 0x38604000, 0x38608000, 0x3860C000, 0x38610000, 0x38614000, 0x38618000, 0x3861C000, 0x38620000, 0x38624000, 0x38628000, 0x3862C000, 0x38630000, 0x38634000, 0x38638000, 0x3863C000, \n\t\t\t\t0x38640000, 0x38644000, 0x38648000, 0x3864C000, 0x38650000, 0x38654000, 0x38658000, 0x3865C000, 0x38660000, 0x38664000, 0x38668000, 0x3866C000, 0x38670000, 0x38674000, 0x38678000, 0x3867C000, \n\t\t\t\t0x38680000, 0x38684000, 0x38688000, 0x3868C000, 0x38690000, 0x38694000, 0x38698000, 0x3869C000, 0x386A0000, 0x386A4000, 0x386A8000, 0x386AC000, 0x386B0000, 0x386B4000, 0x386B8000, 0x386BC000, \n\t\t\t\t0x386C0000, 0x386C4000, 0x386C8000, 0x386CC000, 0x386D0000, 0x386D4000, 0x386D8000, 0x386DC000, 0x386E0000, 0x386E4000, 0x386E8000, 0x386EC000, 0x386F0000, 0x386F4000, 0x386F8000, 0x386FC000, \n\t\t\t\t0x38700000, 0x38704000, 0x38708000, 0x3870C000, 0x38710000, 0x38714000, 0x38718000, 0x3871C000, 0x38720000, 0x38724000, 0x38728000, 0x3872C000, 0x38730000, 0x38734000, 0x38738000, 0x3873C000, \n\t\t\t\t0x38740000, 0x38744000, 0x38748000, 0x3874C000, 0x38750000, 0x38754000, 0x38758000, 0x3875C000, 0x38760000, 0x38764000, 0x38768000, 0x3876C000, 0x38770000, 0x38774000, 0x38778000, 0x3877C000, \n\t\t\t\t0x38780000, 0x38784000, 0x38788000, 0x3878C000, 0x38790000, 0x38794000, 0x38798000, 0x3879C000, 0x387A0000, 0x387A4000, 0x387A8000, 0x387AC000, 0x387B0000, 0x387B4000, 0x387B8000, 0x387BC000, \n\t\t\t\t0x387C0000, 0x387C4000, 0x387C8000, 0x387CC000, 0x387D0000, 0x387D4000, 0x387D8000, 0x387DC000, 0x387E0000, 0x387E4000, 0x387E8000, 0x387EC000, 0x387F0000, 0x387F4000, 0x387F8000, 0x387FC000, \n\t\t\t\t0x38000000, 0x38002000, 0x38004000, 0x38006000, 0x38008000, 0x3800A000, 0x3800C000, 0x3800E000, 0x38010000, 0x38012000, 0x38014000, 0x38016000, 0x38018000, 0x3801A000, 0x3801C000, 0x3801E000, \n\t\t\t\t0x38020000, 0x38022000, 0x38024000, 0x38026000, 0x38028000, 0x3802A000, 0x3802C000, 0x3802E000, 0x38030000, 0x38032000, 0x38034000, 0x38036000, 0x38038000, 0x3803A000, 0x3803C000, 0x3803E000, \n\t\t\t\t0x38040000, 0x38042000, 0x38044000, 0x38046000, 0x38048000, 0x3804A000, 0x3804C000, 0x3804E000, 0x38050000, 0x38052000, 0x38054000, 0x38056000, 0x38058000, 0x3805A000, 0x3805C000, 0x3805E000, \n\t\t\t\t0x38060000, 0x38062000, 0x38064000, 0x38066000, 0x38068000, 0x3806A000, 0x3806C000, 0x3806E000, 0x38070000, 0x38072000, 0x38074000, 0x38076000, 0x38078000, 0x3807A000, 0x3807C000, 0x3807E000, \n\t\t\t\t0x38080000, 0x38082000, 0x38084000, 0x38086000, 0x38088000, 0x3808A000, 0x3808C000, 0x3808E000, 0x38090000, 0x38092000, 0x38094000, 0x38096000, 0x38098000, 0x3809A000, 0x3809C000, 0x3809E000, \n\t\t\t\t0x380A0000, 0x380A2000, 0x380A4000, 0x380A6000, 0x380A8000, 0x380AA000, 0x380AC000, 0x380AE000, 0x380B0000, 0x380B2000, 0x380B4000, 0x380B6000, 0x380B8000, 0x380BA000, 0x380BC000, 0x380BE000, \n\t\t\t\t0x380C0000, 0x380C2000, 0x380C4000, 0x380C6000, 0x380C8000, 0x380CA000, 0x380CC000, 0x380CE000, 0x380D0000, 0x380D2000, 0x380D4000, 0x380D6000, 0x380D8000, 0x380DA000, 0x380DC000, 0x380DE000, \n\t\t\t\t0x380E0000, 0x380E2000, 0x380E4000, 0x380E6000, 0x380E8000, 0x380EA000, 0x380EC000, 0x380EE000, 0x380F0000, 0x380F2000, 0x380F4000, 0x380F6000, 0x380F8000, 0x380FA000, 0x380FC000, 0x380FE000, \n\t\t\t\t0x38100000, 0x38102000, 0x38104000, 0x38106000, 0x38108000, 0x3810A000, 0x3810C000, 0x3810E000, 0x38110000, 0x38112000, 0x38114000, 0x38116000, 0x38118000, 0x3811A000, 0x3811C000, 0x3811E000, \n\t\t\t\t0x38120000, 0x38122000, 0x38124000, 0x38126000, 0x38128000, 0x3812A000, 0x3812C000, 0x3812E000, 0x38130000, 0x38132000, 0x38134000, 0x38136000, 0x38138000, 0x3813A000, 0x3813C000, 0x3813E000, \n\t\t\t\t0x38140000, 0x38142000, 0x38144000, 0x38146000, 0x38148000, 0x3814A000, 0x3814C000, 0x3814E000, 0x38150000, 0x38152000, 0x38154000, 0x38156000, 0x38158000, 0x3815A000, 0x3815C000, 0x3815E000, \n\t\t\t\t0x38160000, 0x38162000, 0x38164000, 0x38166000, 0x38168000, 0x3816A000, 0x3816C000, 0x3816E000, 0x38170000, 0x38172000, 0x38174000, 0x38176000, 0x38178000, 0x3817A000, 0x3817C000, 0x3817E000, \n\t\t\t\t0x38180000, 0x38182000, 0x38184000, 0x38186000, 0x38188000, 0x3818A000, 0x3818C000, 0x3818E000, 0x38190000, 0x38192000, 0x38194000, 0x38196000, 0x38198000, 0x3819A000, 0x3819C000, 0x3819E000, \n\t\t\t\t0x381A0000, 0x381A2000, 0x381A4000, 0x381A6000, 0x381A8000, 0x381AA000, 0x381AC000, 0x381AE000, 0x381B0000, 0x381B2000, 0x381B4000, 0x381B6000, 0x381B8000, 0x381BA000, 0x381BC000, 0x381BE000, \n\t\t\t\t0x381C0000, 0x381C2000, 0x381C4000, 0x381C6000, 0x381C8000, 0x381CA000, 0x381CC000, 0x381CE000, 0x381D0000, 0x381D2000, 0x381D4000, 0x381D6000, 0x381D8000, 0x381DA000, 0x381DC000, 0x381DE000, \n\t\t\t\t0x381E0000, 0x381E2000, 0x381E4000, 0x381E6000, 0x381E8000, 0x381EA000, 0x381EC000, 0x381EE000, 0x381F0000, 0x381F2000, 0x381F4000, 0x381F6000, 0x381F8000, 0x381FA000, 0x381FC000, 0x381FE000, \n\t\t\t\t0x38200000, 0x38202000, 0x38204000, 0x38206000, 0x38208000, 0x3820A000, 0x3820C000, 0x3820E000, 0x38210000, 0x38212000, 0x38214000, 0x38216000, 0x38218000, 0x3821A000, 0x3821C000, 0x3821E000, \n\t\t\t\t0x38220000, 0x38222000, 0x38224000, 0x38226000, 0x38228000, 0x3822A000, 0x3822C000, 0x3822E000, 0x38230000, 0x38232000, 0x38234000, 0x38236000, 0x38238000, 0x3823A000, 0x3823C000, 0x3823E000, \n\t\t\t\t0x38240000, 0x38242000, 0x38244000, 0x38246000, 0x38248000, 0x3824A000, 0x3824C000, 0x3824E000, 0x38250000, 0x38252000, 0x38254000, 0x38256000, 0x38258000, 0x3825A000, 0x3825C000, 0x3825E000, \n\t\t\t\t0x38260000, 0x38262000, 0x38264000, 0x38266000, 0x38268000, 0x3826A000, 0x3826C000, 0x3826E000, 0x38270000, 0x38272000, 0x38274000, 0x38276000, 0x38278000, 0x3827A000, 0x3827C000, 0x3827E000, \n\t\t\t\t0x38280000, 0x38282000, 0x38284000, 0x38286000, 0x38288000, 0x3828A000, 0x3828C000, 0x3828E000, 0x38290000, 0x38292000, 0x38294000, 0x38296000, 0x38298000, 0x3829A000, 0x3829C000, 0x3829E000, \n\t\t\t\t0x382A0000, 0x382A2000, 0x382A4000, 0x382A6000, 0x382A8000, 0x382AA000, 0x382AC000, 0x382AE000, 0x382B0000, 0x382B2000, 0x382B4000, 0x382B6000, 0x382B8000, 0x382BA000, 0x382BC000, 0x382BE000, \n\t\t\t\t0x382C0000, 0x382C2000, 0x382C4000, 0x382C6000, 0x382C8000, 0x382CA000, 0x382CC000, 0x382CE000, 0x382D0000, 0x382D2000, 0x382D4000, 0x382D6000, 0x382D8000, 0x382DA000, 0x382DC000, 0x382DE000, \n\t\t\t\t0x382E0000, 0x382E2000, 0x382E4000, 0x382E6000, 0x382E8000, 0x382EA000, 0x382EC000, 0x382EE000, 0x382F0000, 0x382F2000, 0x382F4000, 0x382F6000, 0x382F8000, 0x382FA000, 0x382FC000, 0x382FE000, \n\t\t\t\t0x38300000, 0x38302000, 0x38304000, 0x38306000, 0x38308000, 0x3830A000, 0x3830C000, 0x3830E000, 0x38310000, 0x38312000, 0x38314000, 0x38316000, 0x38318000, 0x3831A000, 0x3831C000, 0x3831E000, \n\t\t\t\t0x38320000, 0x38322000, 0x38324000, 0x38326000, 0x38328000, 0x3832A000, 0x3832C000, 0x3832E000, 0x38330000, 0x38332000, 0x38334000, 0x38336000, 0x38338000, 0x3833A000, 0x3833C000, 0x3833E000, \n\t\t\t\t0x38340000, 0x38342000, 0x38344000, 0x38346000, 0x38348000, 0x3834A000, 0x3834C000, 0x3834E000, 0x38350000, 0x38352000, 0x38354000, 0x38356000, 0x38358000, 0x3835A000, 0x3835C000, 0x3835E000, \n\t\t\t\t0x38360000, 0x38362000, 0x38364000, 0x38366000, 0x38368000, 0x3836A000, 0x3836C000, 0x3836E000, 0x38370000, 0x38372000, 0x38374000, 0x38376000, 0x38378000, 0x3837A000, 0x3837C000, 0x3837E000, \n\t\t\t\t0x38380000, 0x38382000, 0x38384000, 0x38386000, 0x38388000, 0x3838A000, 0x3838C000, 0x3838E000, 0x38390000, 0x38392000, 0x38394000, 0x38396000, 0x38398000, 0x3839A000, 0x3839C000, 0x3839E000, \n\t\t\t\t0x383A0000, 0x383A2000, 0x383A4000, 0x383A6000, 0x383A8000, 0x383AA000, 0x383AC000, 0x383AE000, 0x383B0000, 0x383B2000, 0x383B4000, 0x383B6000, 0x383B8000, 0x383BA000, 0x383BC000, 0x383BE000, \n\t\t\t\t0x383C0000, 0x383C2000, 0x383C4000, 0x383C6000, 0x383C8000, 0x383CA000, 0x383CC000, 0x383CE000, 0x383D0000, 0x383D2000, 0x383D4000, 0x383D6000, 0x383D8000, 0x383DA000, 0x383DC000, 0x383DE000, \n\t\t\t\t0x383E0000, 0x383E2000, 0x383E4000, 0x383E6000, 0x383E8000, 0x383EA000, 0x383EC000, 0x383EE000, 0x383F0000, 0x383F2000, 0x383F4000, 0x383F6000, 0x383F8000, 0x383FA000, 0x383FC000, 0x383FE000, \n\t\t\t\t0x38400000, 0x38402000, 0x38404000, 0x38406000, 0x38408000, 0x3840A000, 0x3840C000, 0x3840E000, 0x38410000, 0x38412000, 0x38414000, 0x38416000, 0x38418000, 0x3841A000, 0x3841C000, 0x3841E000, \n\t\t\t\t0x38420000, 0x38422000, 0x38424000, 0x38426000, 0x38428000, 0x3842A000, 0x3842C000, 0x3842E000, 0x38430000, 0x38432000, 0x38434000, 0x38436000, 0x38438000, 0x3843A000, 0x3843C000, 0x3843E000, \n\t\t\t\t0x38440000, 0x38442000, 0x38444000, 0x38446000, 0x38448000, 0x3844A000, 0x3844C000, 0x3844E000, 0x38450000, 0x38452000, 0x38454000, 0x38456000, 0x38458000, 0x3845A000, 0x3845C000, 0x3845E000, \n\t\t\t\t0x38460000, 0x38462000, 0x38464000, 0x38466000, 0x38468000, 0x3846A000, 0x3846C000, 0x3846E000, 0x38470000, 0x38472000, 0x38474000, 0x38476000, 0x38478000, 0x3847A000, 0x3847C000, 0x3847E000, \n\t\t\t\t0x38480000, 0x38482000, 0x38484000, 0x38486000, 0x38488000, 0x3848A000, 0x3848C000, 0x3848E000, 0x38490000, 0x38492000, 0x38494000, 0x38496000, 0x38498000, 0x3849A000, 0x3849C000, 0x3849E000, \n\t\t\t\t0x384A0000, 0x384A2000, 0x384A4000, 0x384A6000, 0x384A8000, 0x384AA000, 0x384AC000, 0x384AE000, 0x384B0000, 0x384B2000, 0x384B4000, 0x384B6000, 0x384B8000, 0x384BA000, 0x384BC000, 0x384BE000, \n\t\t\t\t0x384C0000, 0x384C2000, 0x384C4000, 0x384C6000, 0x384C8000, 0x384CA000, 0x384CC000, 0x384CE000, 0x384D0000, 0x384D2000, 0x384D4000, 0x384D6000, 0x384D8000, 0x384DA000, 0x384DC000, 0x384DE000, \n\t\t\t\t0x384E0000, 0x384E2000, 0x384E4000, 0x384E6000, 0x384E8000, 0x384EA000, 0x384EC000, 0x384EE000, 0x384F0000, 0x384F2000, 0x384F4000, 0x384F6000, 0x384F8000, 0x384FA000, 0x384FC000, 0x384FE000, \n\t\t\t\t0x38500000, 0x38502000, 0x38504000, 0x38506000, 0x38508000, 0x3850A000, 0x3850C000, 0x3850E000, 0x38510000, 0x38512000, 0x38514000, 0x38516000, 0x38518000, 0x3851A000, 0x3851C000, 0x3851E000, \n\t\t\t\t0x38520000, 0x38522000, 0x38524000, 0x38526000, 0x38528000, 0x3852A000, 0x3852C000, 0x3852E000, 0x38530000, 0x38532000, 0x38534000, 0x38536000, 0x38538000, 0x3853A000, 0x3853C000, 0x3853E000, \n\t\t\t\t0x38540000, 0x38542000, 0x38544000, 0x38546000, 0x38548000, 0x3854A000, 0x3854C000, 0x3854E000, 0x38550000, 0x38552000, 0x38554000, 0x38556000, 0x38558000, 0x3855A000, 0x3855C000, 0x3855E000, \n\t\t\t\t0x38560000, 0x38562000, 0x38564000, 0x38566000, 0x38568000, 0x3856A000, 0x3856C000, 0x3856E000, 0x38570000, 0x38572000, 0x38574000, 0x38576000, 0x38578000, 0x3857A000, 0x3857C000, 0x3857E000, \n\t\t\t\t0x38580000, 0x38582000, 0x38584000, 0x38586000, 0x38588000, 0x3858A000, 0x3858C000, 0x3858E000, 0x38590000, 0x38592000, 0x38594000, 0x38596000, 0x38598000, 0x3859A000, 0x3859C000, 0x3859E000, \n\t\t\t\t0x385A0000, 0x385A2000, 0x385A4000, 0x385A6000, 0x385A8000, 0x385AA000, 0x385AC000, 0x385AE000, 0x385B0000, 0x385B2000, 0x385B4000, 0x385B6000, 0x385B8000, 0x385BA000, 0x385BC000, 0x385BE000, \n\t\t\t\t0x385C0000, 0x385C2000, 0x385C4000, 0x385C6000, 0x385C8000, 0x385CA000, 0x385CC000, 0x385CE000, 0x385D0000, 0x385D2000, 0x385D4000, 0x385D6000, 0x385D8000, 0x385DA000, 0x385DC000, 0x385DE000, \n\t\t\t\t0x385E0000, 0x385E2000, 0x385E4000, 0x385E6000, 0x385E8000, 0x385EA000, 0x385EC000, 0x385EE000, 0x385F0000, 0x385F2000, 0x385F4000, 0x385F6000, 0x385F8000, 0x385FA000, 0x385FC000, 0x385FE000, \n\t\t\t\t0x38600000, 0x38602000, 0x38604000, 0x38606000, 0x38608000, 0x3860A000, 0x3860C000, 0x3860E000, 0x38610000, 0x38612000, 0x38614000, 0x38616000, 0x38618000, 0x3861A000, 0x3861C000, 0x3861E000, \n\t\t\t\t0x38620000, 0x38622000, 0x38624000, 0x38626000, 0x38628000, 0x3862A000, 0x3862C000, 0x3862E000, 0x38630000, 0x38632000, 0x38634000, 0x38636000, 0x38638000, 0x3863A000, 0x3863C000, 0x3863E000, \n\t\t\t\t0x38640000, 0x38642000, 0x38644000, 0x38646000, 0x38648000, 0x3864A000, 0x3864C000, 0x3864E000, 0x38650000, 0x38652000, 0x38654000, 0x38656000, 0x38658000, 0x3865A000, 0x3865C000, 0x3865E000, \n\t\t\t\t0x38660000, 0x38662000, 0x38664000, 0x38666000, 0x38668000, 0x3866A000, 0x3866C000, 0x3866E000, 0x38670000, 0x38672000, 0x38674000, 0x38676000, 0x38678000, 0x3867A000, 0x3867C000, 0x3867E000, \n\t\t\t\t0x38680000, 0x38682000, 0x38684000, 0x38686000, 0x38688000, 0x3868A000, 0x3868C000, 0x3868E000, 0x38690000, 0x38692000, 0x38694000, 0x38696000, 0x38698000, 0x3869A000, 0x3869C000, 0x3869E000, \n\t\t\t\t0x386A0000, 0x386A2000, 0x386A4000, 0x386A6000, 0x386A8000, 0x386AA000, 0x386AC000, 0x386AE000, 0x386B0000, 0x386B2000, 0x386B4000, 0x386B6000, 0x386B8000, 0x386BA000, 0x386BC000, 0x386BE000, \n\t\t\t\t0x386C0000, 0x386C2000, 0x386C4000, 0x386C6000, 0x386C8000, 0x386CA000, 0x386CC000, 0x386CE000, 0x386D0000, 0x386D2000, 0x386D4000, 0x386D6000, 0x386D8000, 0x386DA000, 0x386DC000, 0x386DE000, \n\t\t\t\t0x386E0000, 0x386E2000, 0x386E4000, 0x386E6000, 0x386E8000, 0x386EA000, 0x386EC000, 0x386EE000, 0x386F0000, 0x386F2000, 0x386F4000, 0x386F6000, 0x386F8000, 0x386FA000, 0x386FC000, 0x386FE000, \n\t\t\t\t0x38700000, 0x38702000, 0x38704000, 0x38706000, 0x38708000, 0x3870A000, 0x3870C000, 0x3870E000, 0x38710000, 0x38712000, 0x38714000, 0x38716000, 0x38718000, 0x3871A000, 0x3871C000, 0x3871E000, \n\t\t\t\t0x38720000, 0x38722000, 0x38724000, 0x38726000, 0x38728000, 0x3872A000, 0x3872C000, 0x3872E000, 0x38730000, 0x38732000, 0x38734000, 0x38736000, 0x38738000, 0x3873A000, 0x3873C000, 0x3873E000, \n\t\t\t\t0x38740000, 0x38742000, 0x38744000, 0x38746000, 0x38748000, 0x3874A000, 0x3874C000, 0x3874E000, 0x38750000, 0x38752000, 0x38754000, 0x38756000, 0x38758000, 0x3875A000, 0x3875C000, 0x3875E000, \n\t\t\t\t0x38760000, 0x38762000, 0x38764000, 0x38766000, 0x38768000, 0x3876A000, 0x3876C000, 0x3876E000, 0x38770000, 0x38772000, 0x38774000, 0x38776000, 0x38778000, 0x3877A000, 0x3877C000, 0x3877E000, \n\t\t\t\t0x38780000, 0x38782000, 0x38784000, 0x38786000, 0x38788000, 0x3878A000, 0x3878C000, 0x3878E000, 0x38790000, 0x38792000, 0x38794000, 0x38796000, 0x38798000, 0x3879A000, 0x3879C000, 0x3879E000, \n\t\t\t\t0x387A0000, 0x387A2000, 0x387A4000, 0x387A6000, 0x387A8000, 0x387AA000, 0x387AC000, 0x387AE000, 0x387B0000, 0x387B2000, 0x387B4000, 0x387B6000, 0x387B8000, 0x387BA000, 0x387BC000, 0x387BE000, \n\t\t\t\t0x387C0000, 0x387C2000, 0x387C4000, 0x387C6000, 0x387C8000, 0x387CA000, 0x387CC000, 0x387CE000, 0x387D0000, 0x387D2000, 0x387D4000, 0x387D6000, 0x387D8000, 0x387DA000, 0x387DC000, 0x387DE000, \n\t\t\t\t0x387E0000, 0x387E2000, 0x387E4000, 0x387E6000, 0x387E8000, 0x387EA000, 0x387EC000, 0x387EE000, 0x387F0000, 0x387F2000, 0x387F4000, 0x387F6000, 0x387F8000, 0x387FA000, 0x387FC000, 0x387FE000 };\n\t\t\tstatic const uint32 exponent_table[64] = { \n\t\t\t\t0x00000000, 0x00800000, 0x01000000, 0x01800000, 0x02000000, 0x02800000, 0x03000000, 0x03800000, 0x04000000, 0x04800000, 0x05000000, 0x05800000, 0x06000000, 0x06800000, 0x07000000, 0x07800000, \n\t\t\t\t0x08000000, 0x08800000, 0x09000000, 0x09800000, 0x0A000000, 0x0A800000, 0x0B000000, 0x0B800000, 0x0C000000, 0x0C800000, 0x0D000000, 0x0D800000, 0x0E000000, 0x0E800000, 0x0F000000, 0x47800000, \n\t\t\t\t0x80000000, 0x80800000, 0x81000000, 0x81800000, 0x82000000, 0x82800000, 0x83000000, 0x83800000, 0x84000000, 0x84800000, 0x85000000, 0x85800000, 0x86000000, 0x86800000, 0x87000000, 0x87800000, \n\t\t\t\t0x88000000, 0x88800000, 0x89000000, 0x89800000, 0x8A000000, 0x8A800000, 0x8B000000, 0x8B800000, 0x8C000000, 0x8C800000, 0x8D000000, 0x8D800000, 0x8E000000, 0x8E800000, 0x8F000000, 0xC7800000 };\n\t\t\tstatic const unsigned short offset_table[64] = { \n\t\t\t\t   0, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, \n\t\t\t\t   0, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024 };\n\t\t\tuint32 bits = mantissa_table[offset_table[value>>10]+(value&0x3FF)] + exponent_table[value>>10];\n//\t\t\treturn *reinterpret_cast<float*>(&bits);\t\t\t//violating strict aliasing!\n\t\t\tfloat out;\n\t\t\tstd::memcpy(&out, &bits, sizeof(float));\n\t\t\treturn out;\n\t\t}\n\n\t\t/// Convert half-precision to IEEE double-precision.\n\t\t/// \\param value binary representation of half-precision value\n\t\t/// \\return double-precision value\n\t\tinline double half2float_impl(uint16 value, double, true_type)\n\t\t{\n\t\t\ttypedef bits<float>::type uint32;\n\t\t\ttypedef bits<double>::type uint64;\n\t\t\tuint32 hi = static_cast<uint32>(value&0x8000) << 16;\n\t\t\tint abs = value & 0x7FFF;\n\t\t\tif(abs)\n\t\t\t{\n\t\t\t\thi |= 0x3F000000 << static_cast<unsigned>(abs>=0x7C00);\n\t\t\t\tfor(; abs<0x400; abs<<=1,hi-=0x100000) ;\n\t\t\t\thi += static_cast<uint32>(abs) << 10;\n\t\t\t}\n\t\t\tuint64 bits = static_cast<uint64>(hi) << 32;\n//\t\t\treturn *reinterpret_cast<double*>(&bits);\t\t\t//violating strict aliasing!\n\t\t\tdouble out;\n\t\t\tstd::memcpy(&out, &bits, sizeof(double));\n\t\t\treturn out;\n\t\t}\n\n\t\t/// Convert half-precision to non-IEEE floating point.\n\t\t/// \\tparam T type to convert to (builtin integer type)\n\t\t/// \\param value binary representation of half-precision value\n\t\t/// \\return floating point value\n\t\ttemplate<typename T> T half2float_impl(uint16 value, T, ...)\n\t\t{\n\t\t\tT out;\n\t\t\tint abs = value & 0x7FFF;\n\t\t\tif(abs > 0x7C00)\n\t\t\t\tout = std::numeric_limits<T>::has_quiet_NaN ? std::numeric_limits<T>::quiet_NaN() : T();\n\t\t\telse if(abs == 0x7C00)\n\t\t\t\tout = std::numeric_limits<T>::has_infinity ? std::numeric_limits<T>::infinity() : std::numeric_limits<T>::max();\n\t\t\telse if(abs > 0x3FF)\n\t\t\t\tout = std::ldexp(static_cast<T>((abs&0x3FF)|0x400), (abs>>10)-25);\n\t\t\telse\n\t\t\t\tout = std::ldexp(static_cast<T>(abs), -24);\n\t\t\treturn (value&0x8000) ? -out : out;\n\t\t}\n\n\t\t/// Convert half-precision to floating point.\n\t\t/// \\tparam T type to convert to (builtin integer type)\n\t\t/// \\param value binary representation of half-precision value\n\t\t/// \\return floating point value\n\t\ttemplate<typename T> T half2float(uint16 value)\n\t\t{\n\t\t\treturn half2float_impl(value, T(), bool_type<std::numeric_limits<T>::is_iec559&&sizeof(typename bits<T>::type)==sizeof(T)>());\n\t\t}\n\n\t\t/// Convert half-precision floating point to integer.\n\t\t/// \\tparam R rounding mode to use, `std::round_indeterminate` for fastest rounding\n\t\t/// \\tparam E `true` for round to even, `false` for round away from zero\n\t\t/// \\tparam T type to convert to (buitlin integer type with at least 16 bits precision, excluding any implicit sign bits)\n\t\t/// \\param value binary representation of half-precision value\n\t\t/// \\return integral value\n\t\ttemplate<std::float_round_style R,bool E,typename T> T half2int_impl(uint16 value)\n\t\t{\n\t\t#if HALF_ENABLE_CPP11_STATIC_ASSERT && HALF_ENABLE_CPP11_TYPE_TRAITS\n\t\t\tstatic_assert(std::is_integral<T>::value, \"half to int conversion only supports builtin integer types\");\n\t\t#endif\n\t\t\tunsigned int e = value & 0x7FFF;\n\t\t\tif(e >= 0x7C00)\n\t\t\t\treturn (value&0x8000) ? std::numeric_limits<T>::min() : std::numeric_limits<T>::max();\n\t\t\tif(e < 0x3800)\n\t\t\t{\n\t\t\t\tif(R == std::round_toward_infinity)\n\t\t\t\t\treturn T(~(value>>15)&(e!=0));\n\t\t\t\tif(R == std::round_toward_neg_infinity)\n\t\t\t\t\treturn -T(value>0x8000);\n\t\t\t\treturn T();\n\t\t\t}\n\t\t\tunsigned int m = (value&0x3FF) | 0x400;\n\t\t\te >>= 10;\n\t\t\tif(e < 25)\n\t\t\t{\n\t\t\t\tif(R == std::round_to_nearest)\n\t\t\t\t\tm += (1<<(24-e)) - (~(m>>(25-e))&E);\n\t\t\t\telse if(R == std::round_toward_infinity)\n\t\t\t\t\tm += ((value>>15)-1) & ((1<<(25-e))-1U);\n\t\t\t\telse if(R == std::round_toward_neg_infinity)\n\t\t\t\t\tm += -(value>>15) & ((1<<(25-e))-1U);\n\t\t\t\tm >>= 25 - e;\n\t\t\t}\n\t\t\telse\n\t\t\t\tm <<= e - 25;\n\t\t\treturn (value&0x8000) ? -static_cast<T>(m) : static_cast<T>(m);\n\t\t}\n\n\t\t/// Convert half-precision floating point to integer.\n\t\t/// \\tparam R rounding mode to use, `std::round_indeterminate` for fastest rounding\n\t\t/// \\tparam T type to convert to (buitlin integer type with at least 16 bits precision, excluding any implicit sign bits)\n\t\t/// \\param value binary representation of half-precision value\n\t\t/// \\return integral value\n\t\ttemplate<std::float_round_style R,typename T> T half2int(uint16 value) { return half2int_impl<R,HALF_ROUND_TIES_TO_EVEN,T>(value); }\n\n\t\t/// Convert half-precision floating point to integer using round-to-nearest-away-from-zero.\n\t\t/// \\tparam T type to convert to (buitlin integer type with at least 16 bits precision, excluding any implicit sign bits)\n\t\t/// \\param value binary representation of half-precision value\n\t\t/// \\return integral value\n\t\ttemplate<typename T> T half2int_up(uint16 value) { return half2int_impl<std::round_to_nearest,0,T>(value); }\n\n\t\t/// Round half-precision number to nearest integer value.\n\t\t/// \\tparam R rounding mode to use, `std::round_indeterminate` for fastest rounding\n\t\t/// \\tparam E `true` for round to even, `false` for round away from zero\n\t\t/// \\param value binary representation of half-precision value\n\t\t/// \\return half-precision bits for nearest integral value\n\t\ttemplate<std::float_round_style R,bool E> uint16 round_half_impl(uint16 value)\n\t\t{\n\t\t\tunsigned int e = value & 0x7FFF;\n\t\t\tuint16 result = value;\n\t\t\tif(e < 0x3C00)\n\t\t\t{\n\t\t\t\tresult &= 0x8000;\n\t\t\t\tif(R == std::round_to_nearest)\n\t\t\t\t\tresult |= 0x3C00U & -(e>=(0x3800+E));\n\t\t\t\telse if(R == std::round_toward_infinity)\n\t\t\t\t\tresult |= 0x3C00U & -(~(value>>15)&(e!=0));\n\t\t\t\telse if(R == std::round_toward_neg_infinity)\n\t\t\t\t\tresult |= 0x3C00U & -(value>0x8000);\n\t\t\t}\n\t\t\telse if(e < 0x6400)\n\t\t\t{\n\t\t\t\te = 25 - (e>>10);\n\t\t\t\tunsigned int mask = (1<<e) - 1;\n\t\t\t\tif(R == std::round_to_nearest)\n\t\t\t\t\tresult += (1<<(e-1)) - (~(result>>e)&E);\n\t\t\t\telse if(R == std::round_toward_infinity)\n\t\t\t\t\tresult += mask & ((value>>15)-1);\n\t\t\t\telse if(R == std::round_toward_neg_infinity)\n\t\t\t\t\tresult += mask & -(value>>15);\n\t\t\t\tresult &= ~mask;\n\t\t\t}\n\t\t\treturn result;\n\t\t}\n\n\t\t/// Round half-precision number to nearest integer value.\n\t\t/// \\tparam R rounding mode to use, `std::round_indeterminate` for fastest rounding\n\t\t/// \\param value binary representation of half-precision value\n\t\t/// \\return half-precision bits for nearest integral value\n\t\ttemplate<std::float_round_style R> uint16 round_half(uint16 value) { return round_half_impl<R,HALF_ROUND_TIES_TO_EVEN>(value); }\n\n\t\t/// Round half-precision number to nearest integer value using round-to-nearest-away-from-zero.\n\t\t/// \\param value binary representation of half-precision value\n\t\t/// \\return half-precision bits for nearest integral value\n\t\tinline uint16 round_half_up(uint16 value) { return round_half_impl<std::round_to_nearest,0>(value); }\n\t\t/// \\}\n\n\t\tstruct functions;\n\t\ttemplate<typename> struct unary_specialized;\n\t\ttemplate<typename,typename> struct binary_specialized;\n\t\ttemplate<typename,typename,std::float_round_style> struct half_caster;\n\t}\n\n\t/// Half-precision floating point type.\n\t/// This class implements an IEEE-conformant half-precision floating point type with the usual arithmetic operators and \n\t/// conversions. It is implicitly convertible to single-precision floating point, which makes artihmetic expressions and \n\t/// functions with mixed-type operands to be of the most precise operand type. Additionally all arithmetic operations \n\t/// (and many mathematical functions) are carried out in single-precision internally. All conversions from single- to \n\t/// half-precision are done using the library's default rounding mode, but temporary results inside chained arithmetic \n\t/// expressions are kept in single-precision as long as possible (while of course still maintaining a strong half-precision type).\n\t///\n\t/// According to the C++98/03 definition, the half type is not a POD type. But according to C++11's less strict and \n\t/// extended definitions it is both a standard layout type and a trivially copyable type (even if not a POD type), which \n\t/// means it can be standard-conformantly copied using raw binary copies. But in this context some more words about the \n\t/// actual size of the type. Although the half is representing an IEEE 16-bit type, it does not neccessarily have to be of \n\t/// exactly 16-bits size. But on any reasonable implementation the actual binary representation of this type will most \n\t/// probably not ivolve any additional \"magic\" or padding beyond the simple binary representation of the underlying 16-bit \n\t/// IEEE number, even if not strictly guaranteed by the standard. But even then it only has an actual size of 16 bits if \n\t/// your C++ implementation supports an unsigned integer type of exactly 16 bits width. But this should be the case on \n\t/// nearly any reasonable platform.\n\t///\n\t/// So if your C++ implementation is not totally exotic or imposes special alignment requirements, it is a reasonable \n\t/// assumption that the data of a half is just comprised of the 2 bytes of the underlying IEEE representation.\n\t#if defined(__clang__)\n\t/* this is a WAR, after nvcc's process,\n\t * `friend class std::numeric_limits<half>;` in the following code, becomes\n\t * `friend class numeric_limits<half>;`, namespsace `std` is removed, which results compilation error in clang.\n\t * tested on nvcc V10.0.95, and clang 5.0.300080 in ndk 16b\n\t */\n\tusing std::numeric_limits;\n\tusing std::hash;\n\t#endif\n\tclass half\n\t{\n\t\tfriend struct detail::functions;\n\t\tfriend struct detail::unary_specialized<half>;\n\t\tfriend struct detail::binary_specialized<half,half>;\n\t\ttemplate<typename,typename,std::float_round_style> friend struct detail::half_caster;\n\t\tfriend class std::numeric_limits<half>;\n\t#if HALF_ENABLE_CPP11_HASH\n\t\tfriend struct std::hash<half>;\n\t#endif\n\t#if HALF_ENABLE_CPP11_USER_LITERALS\n\t\tfriend half literal::operator \"\" _h(long double);\n\t#endif\n\n\tpublic:\n\t\t/// Default constructor.\n\t\t/// This initializes the half to 0. Although this does not match the builtin types' default-initialization semantics \n\t\t/// and may be less efficient than no initialization, it is needed to provide proper value-initialization semantics.\n\t\tHALF_CONSTEXPR half() HALF_NOEXCEPT : data_() {}\n\n\t\t/// Copy constructor.\n\t\t/// \\tparam T type of concrete half expression\n\t\t/// \\param rhs half expression to copy from\n\t\thalf(detail::expr rhs) : data_(detail::float2half<round_style>(static_cast<float>(rhs))) {}\n\n\t\t/// Conversion constructor.\n\t\t/// \\param rhs float to convert\n\t\texplicit half(float rhs) : data_(detail::float2half<round_style>(rhs)) {}\n\t\n\t\t/// Conversion to single-precision.\n\t\t/// \\return single precision value representing expression value\n\t\toperator float() const { return detail::half2float<float>(data_); }\n\n\t\t/// Assignment operator.\n\t\t/// \\tparam T type of concrete half expression\n\t\t/// \\param rhs half expression to copy from\n\t\t/// \\return reference to this half\n\t\thalf& operator=(detail::expr rhs) { return *this = static_cast<float>(rhs); }\n\n\t\t/// Arithmetic assignment.\n\t\t/// \\tparam T type of concrete half expression\n\t\t/// \\param rhs half expression to add\n\t\t/// \\return reference to this half\n\t\ttemplate<typename T> typename detail::enable<half&,T>::type operator+=(T rhs) { return *this += static_cast<float>(rhs); }\n\n\t\t/// Arithmetic assignment.\n\t\t/// \\tparam T type of concrete half expression\n\t\t/// \\param rhs half expression to subtract\n\t\t/// \\return reference to this half\n\t\ttemplate<typename T> typename detail::enable<half&,T>::type operator-=(T rhs) { return *this -= static_cast<float>(rhs); }\n\n\t\t/// Arithmetic assignment.\n\t\t/// \\tparam T type of concrete half expression\n\t\t/// \\param rhs half expression to multiply with\n\t\t/// \\return reference to this half\n\t\ttemplate<typename T> typename detail::enable<half&,T>::type operator*=(T rhs) { return *this *= static_cast<float>(rhs); }\n\n\t\t/// Arithmetic assignment.\n\t\t/// \\tparam T type of concrete half expression\n\t\t/// \\param rhs half expression to divide by\n\t\t/// \\return reference to this half\n\t\ttemplate<typename T> typename detail::enable<half&,T>::type operator/=(T rhs) { return *this /= static_cast<float>(rhs); }\n\n\t\t/// Assignment operator.\n\t\t/// \\param rhs single-precision value to copy from\n\t\t/// \\return reference to this half\n\t\thalf& operator=(float rhs) { data_ = detail::float2half<round_style>(rhs); return *this; }\n\n\t\t/// Arithmetic assignment.\n\t\t/// \\param rhs single-precision value to add\n\t\t/// \\return reference to this half\n\t\thalf& operator+=(float rhs) { data_ = detail::float2half<round_style>(detail::half2float<float>(data_)+rhs); return *this; }\n\n\t\t/// Arithmetic assignment.\n\t\t/// \\param rhs single-precision value to subtract\n\t\t/// \\return reference to this half\n\t\thalf& operator-=(float rhs) { data_ = detail::float2half<round_style>(detail::half2float<float>(data_)-rhs); return *this; }\n\n\t\t/// Arithmetic assignment.\n\t\t/// \\param rhs single-precision value to multiply with\n\t\t/// \\return reference to this half\n\t\thalf& operator*=(float rhs) { data_ = detail::float2half<round_style>(detail::half2float<float>(data_)*rhs); return *this; }\n\n\t\t/// Arithmetic assignment.\n\t\t/// \\param rhs single-precision value to divide by\n\t\t/// \\return reference to this half\n\t\thalf& operator/=(float rhs) { data_ = detail::float2half<round_style>(detail::half2float<float>(data_)/rhs); return *this; }\n\n\t\t/// Prefix increment.\n\t\t/// \\return incremented half value\n\t\thalf& operator++() { return *this += 1.0f; }\n\n\t\t/// Prefix decrement.\n\t\t/// \\return decremented half value\n\t\thalf& operator--() { return *this -= 1.0f; }\n\n\t\t/// Postfix increment.\n\t\t/// \\return non-incremented half value\n\t\thalf operator++(int) { half out(*this); ++*this; return out; }\n\n\t\t/// Postfix decrement.\n\t\t/// \\return non-decremented half value\n\t\thalf operator--(int) { half out(*this); --*this; return out; }\n\t\n\tprivate:\n\t\t/// Rounding mode to use\n\t\tstatic const std::float_round_style round_style = (std::float_round_style)(HALF_ROUND_STYLE);\n\n\t\t/// Constructor.\n\t\t/// \\param bits binary representation to set half to\n\t\tHALF_CONSTEXPR half(detail::binary_t, detail::uint16 bits) HALF_NOEXCEPT : data_(bits) {}\n\n\t\t/// Internal binary representation\n\t\tdetail::uint16 data_;\n\t};\n\n#if HALF_ENABLE_CPP11_USER_LITERALS\n\tnamespace literal\n\t{\n\t\t/// Half literal.\n\t\t/// While this returns an actual half-precision value, half literals can unfortunately not be constant expressions due \n\t\t/// to rather involved conversions.\n\t\t/// \\param value literal value\n\t\t/// \\return half with given value (if representable)\n\t\tinline half operator \"\" _h(long double value) { return half(detail::binary, detail::float2half<half::round_style>(value)); }\n\t}\n#endif\n\n\tnamespace detail\n\t{\n\t\t/// Wrapper implementing unspecialized half-precision functions.\n\t\tstruct functions\n\t\t{\n\t\t\t/// Addition implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\return Half-precision sum stored in single-precision\n\t\t\tstatic expr plus(float x, float y) { return expr(x+y); }\n\n\t\t\t/// Subtraction implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\return Half-precision difference stored in single-precision\n\t\t\tstatic expr minus(float x, float y) { return expr(x-y); }\n\n\t\t\t/// Multiplication implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\return Half-precision product stored in single-precision\n\t\t\tstatic expr multiplies(float x, float y) { return expr(x*y); }\n\n\t\t\t/// Division implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\return Half-precision quotient stored in single-precision\n\t\t\tstatic expr divides(float x, float y) { return expr(x/y); }\n\n\t\t\t/// Output implementation.\n\t\t\t/// \\param out stream to write to\n\t\t\t/// \\param arg value to write\n\t\t\t/// \\return reference to stream\n\t\t\ttemplate<typename charT,typename traits> static std::basic_ostream<charT,traits>& write(std::basic_ostream<charT,traits> &out, float arg) { return out << arg; }\n\n\t\t\t/// Input implementation.\n\t\t\t/// \\param in stream to read from\n\t\t\t/// \\param arg half to read into\n\t\t\t/// \\return reference to stream\n\t\t\ttemplate<typename charT,typename traits> static std::basic_istream<charT,traits>& read(std::basic_istream<charT,traits> &in, half &arg)\n\t\t\t{\n\t\t\t\tfloat f;\n\t\t\t\tif(in >> f)\n\t\t\t\t\targ = f;\n\t\t\t\treturn in;\n\t\t\t}\n\n\t\t\t/// Modulo implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\return Half-precision division remainder stored in single-precision\n\t\t\tstatic expr fmod(float x, float y) { return expr(std::fmod(x, y)); }\n\n\t\t\t/// Remainder implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\return Half-precision division remainder stored in single-precision\n\t\t\tstatic expr remainder(float x, float y)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::remainder(x, y));\n\t\t\t#else\n\t\t\t\tif(builtin_isnan(x) || builtin_isnan(y))\n\t\t\t\t\treturn expr(std::numeric_limits<float>::quiet_NaN());\n\t\t\t\tfloat ax = std::fabs(x), ay = std::fabs(y);\n\t\t\t\tif(ax >= 65536.0f || ay < std::ldexp(1.0f, -24))\n\t\t\t\t\treturn expr(std::numeric_limits<float>::quiet_NaN());\n\t\t\t\tif(ay >= 65536.0f)\n\t\t\t\t\treturn expr(x);\n\t\t\t\tif(ax == ay)\n\t\t\t\t\treturn expr(builtin_signbit(x) ? -0.0f : 0.0f);\n\t\t\t\tax = std::fmod(ax, ay+ay);\n\t\t\t\tfloat y2 = 0.5f * ay;\n\t\t\t\tif(ax > y2)\n\t\t\t\t{\n\t\t\t\t\tax -= ay;\n\t\t\t\t\tif(ax >= y2)\n\t\t\t\t\t\tax -= ay;\n\t\t\t\t}\n\t\t\t\treturn expr(builtin_signbit(x) ? -ax : ax);\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Remainder implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\param quo address to store quotient bits at\n\t\t\t/// \\return Half-precision division remainder stored in single-precision\n\t\t\tstatic expr remquo(float x, float y, int *quo)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::remquo(x, y, quo));\n\t\t\t#else\n\t\t\t\tif(builtin_isnan(x) || builtin_isnan(y))\n\t\t\t\t\treturn expr(std::numeric_limits<float>::quiet_NaN());\n\t\t\t\tbool sign = builtin_signbit(x), qsign = static_cast<bool>(sign^builtin_signbit(y));\n\t\t\t\tfloat ax = std::fabs(x), ay = std::fabs(y);\n\t\t\t\tif(ax >= 65536.0f || ay < std::ldexp(1.0f, -24))\n\t\t\t\t\treturn expr(std::numeric_limits<float>::quiet_NaN());\n\t\t\t\tif(ay >= 65536.0f)\n\t\t\t\t\treturn expr(x);\n\t\t\t\tif(ax == ay)\n\t\t\t\t\treturn *quo = qsign ? -1 : 1, expr(sign ? -0.0f : 0.0f);\n\t\t\t\tax = std::fmod(ax, 8.0f*ay);\n\t\t\t\tint cquo = 0;\n\t\t\t\tif(ax >= 4.0f * ay)\n\t\t\t\t{\n\t\t\t\t\tax -= 4.0f * ay;\n\t\t\t\t\tcquo += 4;\n\t\t\t\t}\n\t\t\t\tif(ax >= 2.0f * ay)\n\t\t\t\t{\n\t\t\t\t\tax -= 2.0f * ay;\n\t\t\t\t\tcquo += 2;\n\t\t\t\t}\n\t\t\t\tfloat y2 = 0.5f * ay;\n\t\t\t\tif(ax > y2)\n\t\t\t\t{\n\t\t\t\t\tax -= ay;\n\t\t\t\t\t++cquo;\n\t\t\t\t\tif(ax >= y2)\n\t\t\t\t\t{\n\t\t\t\t\t\tax -= ay;\n\t\t\t\t\t\t++cquo;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn *quo = qsign ? -cquo : cquo, expr(sign ? -ax : ax);\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Positive difference implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\return Positive difference stored in single-precision\n\t\t\tstatic expr fdim(float x, float y)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::fdim(x, y));\n\t\t\t#else\n\t\t\t\treturn expr((x<=y) ? 0.0f : (x-y));\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Fused multiply-add implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\param z third operand\n\t\t\t/// \\return \\a x * \\a y + \\a z stored in single-precision\n\t\t\tstatic expr fma(float x, float y, float z)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH && defined(FP_FAST_FMAF)\n\t\t\t\treturn expr(std::fma(x, y, z));\n\t\t\t#else\n\t\t\t\treturn expr(x*y+z);\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Get NaN.\n\t\t\t/// \\return Half-precision quiet NaN\n\t\t\tstatic half nanh() { return half(binary, 0x7FFF); }\n\n\t\t\t/// Exponential implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr exp(float arg) { return expr(std::exp(arg)); }\n\n\t\t\t/// Exponential implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr expm1(float arg)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::expm1(arg));\n\t\t\t#else\n\t\t\t\treturn expr(static_cast<float>(std::exp(static_cast<double>(arg))-1.0));\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Binary exponential implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr exp2(float arg)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::exp2(arg));\n\t\t\t#else\n\t\t\t\treturn expr(static_cast<float>(std::exp(arg*0.69314718055994530941723212145818)));\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Logarithm implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr log(float arg) { return expr(std::log(arg)); }\n\n\t\t\t/// Common logarithm implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr log10(float arg) { return expr(std::log10(arg)); }\n\n\t\t\t/// Logarithm implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr log1p(float arg)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::log1p(arg));\n\t\t\t#else\n\t\t\t\treturn expr(static_cast<float>(std::log(1.0+arg)));\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Binary logarithm implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr log2(float arg)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::log2(arg));\n\t\t\t#else\n\t\t\t\treturn expr(static_cast<float>(std::log(static_cast<double>(arg))*1.4426950408889634073599246810019));\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Square root implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr sqrt(float arg) { return expr(std::sqrt(arg)); }\n\n\t\t\t/// Cubic root implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr cbrt(float arg)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::cbrt(arg));\n\t\t\t#else\n\t\t\t\tif(builtin_isnan(arg) || builtin_isinf(arg))\n\t\t\t\t\treturn expr(arg);\n\t\t\t\treturn expr(builtin_signbit(arg) ? -static_cast<float>(std::pow(-static_cast<double>(arg), 1.0/3.0)) : \n\t\t\t\t\tstatic_cast<float>(std::pow(static_cast<double>(arg), 1.0/3.0)));\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Hypotenuse implementation.\n\t\t\t/// \\param x first argument\n\t\t\t/// \\param y second argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr hypot(float x, float y)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::hypot(x, y));\n\t\t\t#else\n\t\t\t\treturn expr((builtin_isinf(x) || builtin_isinf(y)) ? std::numeric_limits<float>::infinity() : \n\t\t\t\t\tstatic_cast<float>(std::sqrt(static_cast<double>(x)*x+static_cast<double>(y)*y)));\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Power implementation.\n\t\t\t/// \\param base value to exponentiate\n\t\t\t/// \\param exp power to expontiate to\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr pow(float base, float exp) { return expr(std::pow(base, exp)); }\n\n\t\t\t/// Sine implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr sin(float arg) { return expr(std::sin(arg)); }\n\n\t\t\t/// Cosine implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr cos(float arg) { return expr(std::cos(arg)); }\n\n\t\t\t/// Tan implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr tan(float arg) { return expr(std::tan(arg)); }\n\n\t\t\t/// Arc sine implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr asin(float arg) { return expr(std::asin(arg)); }\n\n\t\t\t/// Arc cosine implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr acos(float arg) { return expr(std::acos(arg)); }\n\n\t\t\t/// Arc tangent implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr atan(float arg) { return expr(std::atan(arg)); }\n\n\t\t\t/// Arc tangent implementation.\n\t\t\t/// \\param x first argument\n\t\t\t/// \\param y second argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr atan2(float x, float y) { return expr(std::atan2(x, y)); }\n\n\t\t\t/// Hyperbolic sine implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr sinh(float arg) { return expr(std::sinh(arg)); }\n\n\t\t\t/// Hyperbolic cosine implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr cosh(float arg) { return expr(std::cosh(arg)); }\n\n\t\t\t/// Hyperbolic tangent implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr tanh(float arg) { return expr(std::tanh(arg)); }\n\n\t\t\t/// Hyperbolic area sine implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr asinh(float arg)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::asinh(arg));\n\t\t\t#else\n\t\t\t\treturn expr((arg==-std::numeric_limits<float>::infinity()) ? arg : static_cast<float>(std::log(arg+std::sqrt(arg*arg+1.0))));\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Hyperbolic area cosine implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr acosh(float arg)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::acosh(arg));\n\t\t\t#else\n\t\t\t\treturn expr((arg<-1.0f) ? std::numeric_limits<float>::quiet_NaN() : static_cast<float>(std::log(arg+std::sqrt(arg*arg-1.0))));\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Hyperbolic area tangent implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr atanh(float arg)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::atanh(arg));\n\t\t\t#else\n\t\t\t\treturn expr(static_cast<float>(0.5*std::log((1.0+arg)/(1.0-arg))));\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Error function implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr erf(float arg)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::erf(arg));\n\t\t\t#else\n\t\t\t\treturn expr(static_cast<float>(erf(static_cast<double>(arg))));\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Complementary implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr erfc(float arg)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::erfc(arg));\n\t\t\t#else\n\t\t\t\treturn expr(static_cast<float>(1.0-erf(static_cast<double>(arg))));\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Gamma logarithm implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr lgamma(float arg)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::lgamma(arg));\n\t\t\t#else\n\t\t\t\tif(builtin_isinf(arg))\n\t\t\t\t\treturn expr(std::numeric_limits<float>::infinity());\n\t\t\t\tif(arg < 0.0f)\n\t\t\t\t{\n\t\t\t\t\tfloat i, f = std::modf(-arg, &i);\n\t\t\t\t\tif(f == 0.0f)\n\t\t\t\t\t\treturn expr(std::numeric_limits<float>::infinity());\n\t\t\t\t\treturn expr(static_cast<float>(1.1447298858494001741434273513531-\n\t\t\t\t\t\tstd::log(std::abs(std::sin(3.1415926535897932384626433832795*f)))-lgamma(1.0-arg)));\n\t\t\t\t}\n\t\t\t\treturn expr(static_cast<float>(lgamma(static_cast<double>(arg))));\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Gamma implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return function value stored in single-preicision\n\t\t\tstatic expr tgamma(float arg)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::tgamma(arg));\n\t\t\t#else\n\t\t\t\tif(arg == 0.0f)\n\t\t\t\t\treturn builtin_signbit(arg) ? expr(-std::numeric_limits<float>::infinity()) : expr(std::numeric_limits<float>::infinity());\n\t\t\t\tif(arg < 0.0f)\n\t\t\t\t{\n\t\t\t\t\tfloat i, f = std::modf(-arg, &i);\n\t\t\t\t\tif(f == 0.0f)\n\t\t\t\t\t\treturn expr(std::numeric_limits<float>::quiet_NaN());\n\t\t\t\t\tdouble value = 3.1415926535897932384626433832795 / (std::sin(3.1415926535897932384626433832795*f)*std::exp(lgamma(1.0-arg)));\n\t\t\t\t\treturn expr(static_cast<float>((std::fmod(i, 2.0f)==0.0f) ? -value : value));\n\t\t\t\t}\n\t\t\t\tif(builtin_isinf(arg))\n\t\t\t\t\treturn expr(arg);\n\t\t\t\treturn expr(static_cast<float>(std::exp(lgamma(static_cast<double>(arg)))));\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Floor implementation.\n\t\t\t/// \\param arg value to round\n\t\t\t/// \\return rounded value\n\t\t\tstatic half floor(half arg) { return half(binary, round_half<std::round_toward_neg_infinity>(arg.data_)); }\n\n\t\t\t/// Ceiling implementation.\n\t\t\t/// \\param arg value to round\n\t\t\t/// \\return rounded value\n\t\t\tstatic half ceil(half arg) { return half(binary, round_half<std::round_toward_infinity>(arg.data_)); }\n\n\t\t\t/// Truncation implementation.\n\t\t\t/// \\param arg value to round\n\t\t\t/// \\return rounded value\n\t\t\tstatic half trunc(half arg) { return half(binary, round_half<std::round_toward_zero>(arg.data_)); }\n\n\t\t\t/// Nearest integer implementation.\n\t\t\t/// \\param arg value to round\n\t\t\t/// \\return rounded value\n\t\t\tstatic half round(half arg) { return half(binary, round_half_up(arg.data_)); }\n\n\t\t\t/// Nearest integer implementation.\n\t\t\t/// \\param arg value to round\n\t\t\t/// \\return rounded value\n\t\t\tstatic long lround(half arg) { return detail::half2int_up<long>(arg.data_); }\n\n\t\t\t/// Nearest integer implementation.\n\t\t\t/// \\param arg value to round\n\t\t\t/// \\return rounded value\n\t\t\tstatic half rint(half arg) { return half(binary, round_half<half::round_style>(arg.data_)); }\n\n\t\t\t/// Nearest integer implementation.\n\t\t\t/// \\param arg value to round\n\t\t\t/// \\return rounded value\n\t\t\tstatic long lrint(half arg) { return detail::half2int<half::round_style,long>(arg.data_); }\n\n\t\t#if HALF_ENABLE_CPP11_LONG_LONG\n\t\t\t/// Nearest integer implementation.\n\t\t\t/// \\param arg value to round\n\t\t\t/// \\return rounded value\n\t\t\tstatic long long llround(half arg) { return detail::half2int_up<long long>(arg.data_); }\n\n\t\t\t/// Nearest integer implementation.\n\t\t\t/// \\param arg value to round\n\t\t\t/// \\return rounded value\n\t\t\tstatic long long llrint(half arg) { return detail::half2int<half::round_style,long long>(arg.data_); }\n\t\t#endif\n\n\t\t\t/// Decompression implementation.\n\t\t\t/// \\param arg number to decompress\n\t\t\t/// \\param exp address to store exponent at\n\t\t\t/// \\return normalized significant\n\t\t\tstatic half frexp(half arg, int *exp)\n\t\t\t{\n\t\t\t\tint m = arg.data_ & 0x7FFF, e = -14;\n\t\t\t\tif(m >= 0x7C00 || !m)\n\t\t\t\t\treturn *exp = 0, arg;\n\t\t\t\tfor(; m<0x400; m<<=1,--e) ;\n\t\t\t\treturn *exp = e+(m>>10), half(binary, (arg.data_&0x8000)|0x3800|(m&0x3FF));\n\t\t\t}\n\n\t\t\t/// Decompression implementation.\n\t\t\t/// \\param arg number to decompress\n\t\t\t/// \\param iptr address to store integer part at\n\t\t\t/// \\return fractional part\n\t\t\tstatic half modf(half arg, half *iptr)\n\t\t\t{\n\t\t\t\tunsigned int e = arg.data_ & 0x7FFF;\n\t\t\t\tif(e >= 0x6400)\n\t\t\t\t\treturn *iptr = arg, half(binary, arg.data_&(0x8000U|-(e>0x7C00)));\n\t\t\t\tif(e < 0x3C00)\n\t\t\t\t\treturn iptr->data_ = arg.data_ & 0x8000, arg;\n\t\t\t\te >>= 10;\n\t\t\t\tunsigned int mask = (1<<(25-e)) - 1, m = arg.data_ & mask;\n\t\t\t\tiptr->data_ = arg.data_ & ~mask;\n\t\t\t\tif(!m)\n\t\t\t\t\treturn half(binary, arg.data_&0x8000);\n\t\t\t\tfor(; m<0x400; m<<=1,--e) ;\n\t\t\t\treturn half(binary, static_cast<uint16>((arg.data_&0x8000)|(e<<10)|(m&0x3FF)));\n\t\t\t}\n\n\t\t\t/// Scaling implementation.\n\t\t\t/// \\param arg number to scale\n\t\t\t/// \\param exp power of two to scale by\n\t\t\t/// \\return scaled number\n\t\t\tstatic half scalbln(half arg, long exp)\n\t\t\t{\n\t\t\t\tunsigned int m = arg.data_ & 0x7FFF;\n\t\t\t\tif(m >= 0x7C00 || !m)\n\t\t\t\t\treturn arg;\n\t\t\t\tfor(; m<0x400; m<<=1,--exp) ;\n\t\t\t\texp += m >> 10;\n\t\t\t\tuint16 value = arg.data_ & 0x8000;\n\t\t\t\tif(exp > 30)\n\t\t\t\t{\n\t\t\t\t\tif(half::round_style == std::round_toward_zero)\n\t\t\t\t\t\tvalue |= 0x7BFF;\n\t\t\t\t\telse if(half::round_style == std::round_toward_infinity)\n\t\t\t\t\t\tvalue |= 0x7C00 - (value>>15);\n\t\t\t\t\telse if(half::round_style == std::round_toward_neg_infinity)\n\t\t\t\t\t\tvalue |= 0x7BFF + (value>>15);\n\t\t\t\t\telse\n\t\t\t\t\t\tvalue |= 0x7C00;\n\t\t\t\t}\n\t\t\t\telse if(exp > 0)\n\t\t\t\t\tvalue |= (exp<<10) | (m&0x3FF);\n\t\t\t\telse if(exp > -11)\n\t\t\t\t{\n\t\t\t\t\tm = (m&0x3FF) | 0x400;\n\t\t\t\t\tif(half::round_style == std::round_to_nearest)\n\t\t\t\t\t{\n\t\t\t\t\t\tm += 1 << -exp;\n\t\t\t\t\t#if HALF_ROUND_TIES_TO_EVEN\n\t\t\t\t\t\tm -= (m>>(1-exp)) & 1;\n\t\t\t\t\t#endif\n\t\t\t\t\t}\n\t\t\t\t\telse if(half::round_style == std::round_toward_infinity)\n\t\t\t\t\t\tm += ((value>>15)-1) & ((1<<(1-exp))-1U);\n\t\t\t\t\telse if(half::round_style == std::round_toward_neg_infinity)\n\t\t\t\t\t\tm += -(value>>15) & ((1<<(1-exp))-1U);\n\t\t\t\t\tvalue |= m >> (1-exp);\n\t\t\t\t}\n\t\t\t\telse if(half::round_style == std::round_toward_infinity)\n\t\t\t\t\tvalue -= (value>>15) - 1;\n\t\t\t\telse if(half::round_style == std::round_toward_neg_infinity)\n\t\t\t\t\tvalue += value >> 15;\n\t\t\t\treturn half(binary, value);\n\t\t\t}\n\n\t\t\t/// Exponent implementation.\n\t\t\t/// \\param arg number to query\n\t\t\t/// \\return floating point exponent\n\t\t\tstatic int ilogb(half arg)\n\t\t\t{\n\t\t\t\tint abs = arg.data_ & 0x7FFF;\n\t\t\t\tif(!abs)\n\t\t\t\t\treturn FP_ILOGB0;\n\t\t\t\tif(abs < 0x7C00)\n\t\t\t\t{\n\t\t\t\t\tint exp = (abs>>10) - 15;\n\t\t\t\t\tif(abs < 0x400)\n\t\t\t\t\t\tfor(; abs<0x200; abs<<=1,--exp) ;\n\t\t\t\t\treturn exp;\n\t\t\t\t}\n\t\t\t\tif(abs > 0x7C00)\n\t\t\t\t\treturn FP_ILOGBNAN;\n\t\t\t\treturn INT_MAX;\n\t\t\t}\n\n\t\t\t/// Exponent implementation.\n\t\t\t/// \\param arg number to query\n\t\t\t/// \\return floating point exponent\n\t\t\tstatic half logb(half arg)\n\t\t\t{\n\t\t\t\tint abs = arg.data_ & 0x7FFF;\n\t\t\t\tif(!abs)\n\t\t\t\t\treturn half(binary, 0xFC00);\n\t\t\t\tif(abs < 0x7C00)\n\t\t\t\t{\n\t\t\t\t\tint exp = (abs>>10) - 15;\n\t\t\t\t\tif(abs < 0x400)\n\t\t\t\t\t\tfor(; abs<0x200; abs<<=1,--exp) ;\n\t\t\t\t\tuint16 bits = (exp<0) << 15;\n\t\t\t\t\tif(exp)\n\t\t\t\t\t{\n\t\t\t\t\t\tunsigned int m = std::abs(exp) << 6, e = 18;\n\t\t\t\t\t\tfor(; m<0x400; m<<=1,--e) ;\n\t\t\t\t\t\tbits |= (e<<10) + m;\n\t\t\t\t\t}\n\t\t\t\t\treturn half(binary, bits);\n\t\t\t\t}\n\t\t\t\tif(abs > 0x7C00)\n\t\t\t\t\treturn arg;\n\t\t\t\treturn half(binary, 0x7C00);\n\t\t\t}\n\n\t\t\t/// Enumeration implementation.\n\t\t\t/// \\param from number to increase/decrease\n\t\t\t/// \\param to direction to enumerate into\n\t\t\t/// \\return next representable number\n\t\t\tstatic half nextafter(half from, half to)\n\t\t\t{\n\t\t\t\tuint16 fabs = from.data_ & 0x7FFF, tabs = to.data_ & 0x7FFF;\n\t\t\t\tif(fabs > 0x7C00)\n\t\t\t\t\treturn from;\n\t\t\t\tif(tabs > 0x7C00 || from.data_ == to.data_ || !(fabs|tabs))\n\t\t\t\t\treturn to;\n\t\t\t\tif(!fabs)\n\t\t\t\t\treturn half(binary, (to.data_&0x8000)+1);\n\t\t\t\tbool lt = ((fabs==from.data_) ? static_cast<int>(fabs) : -static_cast<int>(fabs)) < \n\t\t\t\t\t((tabs==to.data_) ? static_cast<int>(tabs) : -static_cast<int>(tabs));\n\t\t\t\treturn half(binary, from.data_+(((from.data_>>15)^static_cast<unsigned>(lt))<<1)-1);\n\t\t\t}\n\n\t\t\t/// Enumeration implementation.\n\t\t\t/// \\param from number to increase/decrease\n\t\t\t/// \\param to direction to enumerate into\n\t\t\t/// \\return next representable number\n\t\t\tstatic half nexttoward(half from, long double to)\n\t\t\t{\n\t\t\t\tif(isnan(from))\n\t\t\t\t\treturn from;\n\t\t\t\tauto lfrom = static_cast<long double>(from);\n\t\t\t\tif(builtin_isnan(to) || lfrom == to)\n\t\t\t\t\treturn half(static_cast<float>(to));\n\t\t\t\tif(!(from.data_&0x7FFF))\n\t\t\t\t\treturn half(binary, (static_cast<detail::uint16>(builtin_signbit(to))<<15)+1);\n\t\t\t\treturn half(binary, from.data_+(((from.data_>>15)^static_cast<unsigned>(lfrom<to))<<1)-1);\n\t\t\t}\n\n\t\t\t/// Sign implementation\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\return composed value\n\t\t\tstatic half copysign(half x, half y) { return half(binary, x.data_^((x.data_^y.data_)&0x8000)); }\n\n\t\t\t/// Classification implementation.\n\t\t\t/// \\param arg value to classify\n\t\t\t/// \\retval true if infinite number\n\t\t\t/// \\retval false else\n\t\t\tstatic int fpclassify(half arg)\n\t\t\t{\n\t\t\t\tunsigned int abs = arg.data_ & 0x7FFF;\n\t\t\t\treturn abs ? ((abs>0x3FF) ? ((abs>=0x7C00) ? ((abs>0x7C00) ? FP_NAN : FP_INFINITE) : FP_NORMAL) :FP_SUBNORMAL) : FP_ZERO;\n\t\t\t}\n\n\t\t\t/// Classification implementation.\n\t\t\t/// \\param arg value to classify\n\t\t\t/// \\retval true if finite number\n\t\t\t/// \\retval false else\n\t\t\tstatic bool isfinite(half arg) { return (arg.data_&0x7C00) != 0x7C00; }\n\n\t\t\t/// Classification implementation.\n\t\t\t/// \\param arg value to classify\n\t\t\t/// \\retval true if infinite number\n\t\t\t/// \\retval false else\n\t\t\tstatic bool isinf(half arg) { return (arg.data_&0x7FFF) == 0x7C00; }\n\n\t\t\t/// Classification implementation.\n\t\t\t/// \\param arg value to classify\n\t\t\t/// \\retval true if not a number\n\t\t\t/// \\retval false else\n\t\t\tstatic bool isnan(half arg) { return (arg.data_&0x7FFF) > 0x7C00; }\n\n\t\t\t/// Classification implementation.\n\t\t\t/// \\param arg value to classify\n\t\t\t/// \\retval true if normal number\n\t\t\t/// \\retval false else\n\t\t\tstatic bool isnormal(half arg) { return ((arg.data_&0x7C00)!=0) & ((arg.data_&0x7C00)!=0x7C00); }\n\n\t\t\t/// Sign bit implementation.\n\t\t\t/// \\param arg value to check\n\t\t\t/// \\retval true if signed\n\t\t\t/// \\retval false if unsigned\n\t\t\tstatic bool signbit(half arg) { return (arg.data_&0x8000) != 0; }\n\n\t\t\t/// Comparison implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\retval true if operands equal\n\t\t\t/// \\retval false else\n\t\t\tstatic bool isequal(half x, half y) { return (x.data_==y.data_ || !((x.data_|y.data_)&0x7FFF)) && !isnan(x); }\n\n\t\t\t/// Comparison implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\retval true if operands not equal\n\t\t\t/// \\retval false else\n\t\t\tstatic bool isnotequal(half x, half y) { return (x.data_!=y.data_ && ((x.data_|y.data_)&0x7FFF)) || isnan(x); }\n\n\t\t\t/// Comparison implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\retval true if \\a x > \\a y\n\t\t\t/// \\retval false else\n\t\t\tstatic bool isgreater(half x, half y)\n\t\t\t{\n\t\t\t\tint xabs = x.data_ & 0x7FFF, yabs = y.data_ & 0x7FFF;\n\t\t\t\treturn xabs<=0x7C00 && yabs<=0x7C00 && (((xabs==x.data_) ? xabs : -xabs) > ((yabs==y.data_) ? yabs : -yabs));\n\t\t\t}\n\n\t\t\t/// Comparison implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\retval true if \\a x >= \\a y\n\t\t\t/// \\retval false else\n\t\t\tstatic bool isgreaterequal(half x, half y)\n\t\t\t{\n\t\t\t\tint xabs = x.data_ & 0x7FFF, yabs = y.data_ & 0x7FFF;\n\t\t\t\treturn xabs<=0x7C00 && yabs<=0x7C00 && (((xabs==x.data_) ? xabs : -xabs) >= ((yabs==y.data_) ? yabs : -yabs));\n\t\t\t}\n\n\t\t\t/// Comparison implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\retval true if \\a x < \\a y\n\t\t\t/// \\retval false else\n\t\t\tstatic bool isless(half x, half y)\n\t\t\t{\n\t\t\t\tint xabs = x.data_ & 0x7FFF, yabs = y.data_ & 0x7FFF;\n\t\t\t\treturn xabs<=0x7C00 && yabs<=0x7C00 && (((xabs==x.data_) ? xabs : -xabs) < ((yabs==y.data_) ? yabs : -yabs));\n\t\t\t}\n\n\t\t\t/// Comparison implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\retval true if \\a x <= \\a y\n\t\t\t/// \\retval false else\n\t\t\tstatic bool islessequal(half x, half y)\n\t\t\t{\n\t\t\t\tint xabs = x.data_ & 0x7FFF, yabs = y.data_ & 0x7FFF;\n\t\t\t\treturn xabs<=0x7C00 && yabs<=0x7C00 && (((xabs==x.data_) ? xabs : -xabs) <= ((yabs==y.data_) ? yabs : -yabs));\n\t\t\t}\n\n\t\t\t/// Comparison implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\retval true if either \\a x > \\a y nor \\a x < \\a y\n\t\t\t/// \\retval false else\n\t\t\tstatic bool islessgreater(half x, half y)\n\t\t\t{\n\t\t\t\tint xabs = x.data_ & 0x7FFF, yabs = y.data_ & 0x7FFF;\n\t\t\t\tif(xabs > 0x7C00 || yabs > 0x7C00)\n\t\t\t\t\treturn false;\n\t\t\t\tint a = (xabs==x.data_) ? xabs : -xabs, b = (yabs==y.data_) ? yabs : -yabs;\n\t\t\t\treturn a < b || a > b;\n\t\t\t}\n\n\t\t\t/// Comparison implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\retval true if operand unordered\n\t\t\t/// \\retval false else\n\t\t\tstatic bool isunordered(half x, half y) { return isnan(x) || isnan(y); }\n\n\t\tprivate:\n\t\t\tstatic double erf(double arg)\n\t\t\t{\n\t\t\t\tif(builtin_isinf(arg))\n\t\t\t\t\treturn (arg<0.0) ? -1.0 : 1.0;\n\t\t\t\tdouble x2 = arg * arg, ax2 = 0.147 * x2, value = std::sqrt(1.0-std::exp(-x2*(1.2732395447351626861510701069801+ax2)/(1.0+ax2)));\n\t\t\t\treturn builtin_signbit(arg) ? -value : value;\n\t\t\t}\n\n\t\t\tstatic double lgamma(double arg)\n\t\t\t{\n\t\t\t\tdouble v = 1.0;\n\t\t\t\tfor(; arg<8.0; ++arg) v *= arg;\n\t\t\t\tdouble w = 1.0 / (arg*arg);\n\t\t\t\treturn (((((((-0.02955065359477124183006535947712*w+0.00641025641025641025641025641026)*w+\n\t\t\t\t\t-0.00191752691752691752691752691753)*w+8.4175084175084175084175084175084e-4)*w+\n\t\t\t\t\t-5.952380952380952380952380952381e-4)*w+7.9365079365079365079365079365079e-4)*w+\n\t\t\t\t\t-0.00277777777777777777777777777778)*w+0.08333333333333333333333333333333)/arg + \n\t\t\t\t\t0.91893853320467274178032973640562 - std::log(v) - arg + (arg-0.5) * std::log(arg);\n\t\t\t}\n\t\t};\n\n\t\t/// Wrapper for unary half-precision functions needing specialization for individual argument types.\n\t\t/// \\tparam T argument type\n\t\ttemplate<typename T> struct unary_specialized\n\t\t{\n\t\t\t/// Negation implementation.\n\t\t\t/// \\param arg value to negate\n\t\t\t/// \\return negated value\n\t\t\tstatic HALF_CONSTEXPR half negate(half arg) { return half(binary, arg.data_^0x8000); }\n\n\t\t\t/// Absolute value implementation.\n\t\t\t/// \\param arg function argument\n\t\t\t/// \\return absolute value\n\t\t\tstatic half fabs(half arg) { return half(binary, arg.data_&0x7FFF); }\n\t\t};\n\t\ttemplate<> struct unary_specialized<expr>\n\t\t{\n\t\t\tstatic HALF_CONSTEXPR expr negate(float arg) { return expr(-arg); }\n\t\t\tstatic expr fabs(float arg) { return expr(std::fabs(arg)); }\n\t\t};\n\n\t\t/// Wrapper for binary half-precision functions needing specialization for individual argument types.\n\t\t/// \\tparam T first argument type\n\t\t/// \\tparam U first argument type\n\t\ttemplate<typename T,typename U> struct binary_specialized\n\t\t{\n\t\t\t/// Minimum implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\return minimum value\n\t\t\tstatic expr fmin(float x, float y)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::fmin(x, y));\n\t\t\t#else\n\t\t\t\tif(builtin_isnan(x))\n\t\t\t\t\treturn expr(y);\n\t\t\t\tif(builtin_isnan(y))\n\t\t\t\t\treturn expr(x);\n\t\t\t\treturn expr(std::min(x, y));\n\t\t\t#endif\n\t\t\t}\n\n\t\t\t/// Maximum implementation.\n\t\t\t/// \\param x first operand\n\t\t\t/// \\param y second operand\n\t\t\t/// \\return maximum value\n\t\t\tstatic expr fmax(float x, float y)\n\t\t\t{\n\t\t\t#if HALF_ENABLE_CPP11_CMATH\n\t\t\t\treturn expr(std::fmax(x, y));\n\t\t\t#else\n\t\t\t\tif(builtin_isnan(x))\n\t\t\t\t\treturn expr(y);\n\t\t\t\tif(builtin_isnan(y))\n\t\t\t\t\treturn expr(x);\n\t\t\t\treturn expr(std::max(x, y));\n\t\t\t#endif\n\t\t\t}\n\t\t};\n\t\ttemplate<> struct binary_specialized<half,half>\n\t\t{\n\t\t\tstatic half fmin(half x, half y)\n\t\t\t{\n\t\t\t\tint xabs = x.data_ & 0x7FFF, yabs = y.data_ & 0x7FFF;\n\t\t\t\tif(xabs > 0x7C00)\n\t\t\t\t\treturn y;\n\t\t\t\tif(yabs > 0x7C00)\n\t\t\t\t\treturn x;\n\t\t\t\treturn (((xabs==x.data_) ? xabs : -xabs) > ((yabs==y.data_) ? yabs : -yabs)) ? y : x;\n\t\t\t}\n\t\t\tstatic half fmax(half x, half y)\n\t\t\t{\n\t\t\t\tint xabs = x.data_ & 0x7FFF, yabs = y.data_ & 0x7FFF;\n\t\t\t\tif(xabs > 0x7C00)\n\t\t\t\t\treturn y;\n\t\t\t\tif(yabs > 0x7C00)\n\t\t\t\t\treturn x;\n\t\t\t\treturn (((xabs==x.data_) ? xabs : -xabs) < ((yabs==y.data_) ? yabs : -yabs)) ? y : x;\n\t\t\t}\n\t\t};\n\n\t\t/// Helper class for half casts.\n\t\t/// This class template has to be specialized for all valid cast argument to define an appropriate static `cast` member \n\t\t/// function and a corresponding `type` member denoting its return type.\n\t\t/// \\tparam T destination type\n\t\t/// \\tparam U source type\n\t\t/// \\tparam R rounding mode to use\n\t\ttemplate<typename T,typename U,std::float_round_style R=(std::float_round_style)(HALF_ROUND_STYLE)> struct half_caster {};\n\t\ttemplate<typename U,std::float_round_style R> struct half_caster<half,U,R>\n\t\t{\n\t\t#if HALF_ENABLE_CPP11_STATIC_ASSERT && HALF_ENABLE_CPP11_TYPE_TRAITS\n\t\t\tstatic_assert(std::is_arithmetic<U>::value, \"half_cast from non-arithmetic type unsupported\");\n\t\t#endif\n\n\t\t\tstatic half cast(U arg) { return cast_impl(arg, is_float<U>()); };\n\n\t\tprivate:\n\t\t\tstatic half cast_impl(U arg, true_type) { return half(binary, float2half<R>(arg)); }\n\t\t\tstatic half cast_impl(U arg, false_type) { return half(binary, int2half<R>(arg)); }\n\t\t};\n\t\ttemplate<typename T,std::float_round_style R> struct half_caster<T,half,R>\n\t\t{\n\t\t#if HALF_ENABLE_CPP11_STATIC_ASSERT && HALF_ENABLE_CPP11_TYPE_TRAITS\n\t\t\tstatic_assert(std::is_arithmetic<T>::value, \"half_cast to non-arithmetic type unsupported\");\n\t\t#endif\n\n\t\t\tstatic T cast(half arg) { return cast_impl(arg, is_float<T>()); }\n\n\t\tprivate:\n\t\t\tstatic T cast_impl(half arg, true_type) { return half2float<T>(arg.data_); }\n\t\t\tstatic T cast_impl(half arg, false_type) { return half2int<R,T>(arg.data_); }\n\t\t};\n\t\ttemplate<typename T,std::float_round_style R> struct half_caster<T,expr,R>\n\t\t{\n\t\t#if HALF_ENABLE_CPP11_STATIC_ASSERT && HALF_ENABLE_CPP11_TYPE_TRAITS\n\t\t\tstatic_assert(std::is_arithmetic<T>::value, \"half_cast to non-arithmetic type unsupported\");\n\t\t#endif\n\n\t\t\tstatic T cast(expr arg) { return cast_impl(arg, is_float<T>()); }\n\n\t\tprivate:\n\t\t\tstatic T cast_impl(float arg, true_type) { return static_cast<T>(arg); }\n\t\t\tstatic T cast_impl(half arg, false_type) { return half2int<R,T>(arg.data_); }\n\t\t};\n\t\ttemplate<std::float_round_style R> struct half_caster<half,half,R>\n\t\t{\n\t\t\tstatic half cast(half arg) { return arg; }\n\t\t};\n\t\ttemplate<std::float_round_style R> struct half_caster<half,expr,R> : half_caster<half,half,R> {};\n\n\t\t/// \\name Comparison operators\n\t\t/// \\{\n\n\t\t/// Comparison for equality.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\retval true if operands equal\n\t\t/// \\retval false else\n\t\ttemplate<typename T,typename U> typename enable<bool,T,U>::type operator==(T x, U y) { return functions::isequal(x, y); }\n\n\t\t/// Comparison for inequality.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\retval true if operands not equal\n\t\t/// \\retval false else\n\t\ttemplate<typename T,typename U> typename enable<bool,T,U>::type operator!=(T x, U y) { return functions::isnotequal(x, y); }\n\n\t\t/// Comparison for less than.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\retval true if \\a x less than \\a y\n\t\t/// \\retval false else\n\t\ttemplate<typename T,typename U> typename enable<bool,T,U>::type operator<(T x, U y) { return functions::isless(x, y); }\n\n\t\t/// Comparison for greater than.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\retval true if \\a x greater than \\a y\n\t\t/// \\retval false else\n\t\ttemplate<typename T,typename U> typename enable<bool,T,U>::type operator>(T x, U y) { return functions::isgreater(x, y); }\n\n\t\t/// Comparison for less equal.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\retval true if \\a x less equal \\a y\n\t\t/// \\retval false else\n\t\ttemplate<typename T,typename U> typename enable<bool,T,U>::type operator<=(T x, U y) { return functions::islessequal(x, y); }\n\n\t\t/// Comparison for greater equal.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\retval true if \\a x greater equal \\a y\n\t\t/// \\retval false else\n\t\ttemplate<typename T,typename U> typename enable<bool,T,U>::type operator>=(T x, U y) { return functions::isgreaterequal(x, y); }\n\n\t\t/// \\}\n\t\t/// \\name Arithmetic operators\n\t\t/// \\{\n\n\t\t/// Add halfs.\n\t\t/// \\param x left operand\n\t\t/// \\param y right operand\n\t\t/// \\return sum of half expressions\n\t\ttemplate<typename T,typename U> typename enable<expr,T,U>::type operator+(T x, U y) { return functions::plus(x, y); }\n\n\t\t/// Subtract halfs.\n\t\t/// \\param x left operand\n\t\t/// \\param y right operand\n\t\t/// \\return difference of half expressions\n\t\ttemplate<typename T,typename U> typename enable<expr,T,U>::type operator-(T x, U y) { return functions::minus(x, y); }\n\n\t\t/// Multiply halfs.\n\t\t/// \\param x left operand\n\t\t/// \\param y right operand\n\t\t/// \\return product of half expressions\n\t\ttemplate<typename T,typename U> typename enable<expr,T,U>::type operator*(T x, U y) { return functions::multiplies(x, y); }\n\n\t\t/// Divide halfs.\n\t\t/// \\param x left operand\n\t\t/// \\param y right operand\n\t\t/// \\return quotient of half expressions\n\t\ttemplate<typename T,typename U> typename enable<expr,T,U>::type operator/(T x, U y) { return functions::divides(x, y); }\n\n\t\t/// Identity.\n\t\t/// \\param arg operand\n\t\t/// \\return uncahnged operand\n\t\ttemplate<typename T> HALF_CONSTEXPR typename enable<T,T>::type operator+(T arg) { return arg; }\n\n\t\t/// Negation.\n\t\t/// \\param arg operand\n\t\t/// \\return negated operand\n\t\ttemplate<typename T> HALF_CONSTEXPR typename enable<T,T>::type operator-(T arg) { return unary_specialized<T>::negate(arg); }\n\n\t\t/// \\}\n\t\t/// \\name Input and output\n\t\t/// \\{\n\n\t\t/// Output operator.\n\t\t/// \\param out output stream to write into\n\t\t/// \\param arg half expression to write\n\t\t/// \\return reference to output stream\n\t\ttemplate<typename T,typename charT,typename traits> typename enable<std::basic_ostream<charT,traits>&,T>::type\n\t\t\toperator<<(std::basic_ostream<charT,traits> &out, T arg) { return functions::write(out, arg); }\n\n\t\t/// Input operator.\n\t\t/// \\param in input stream to read from\n\t\t/// \\param arg half to read into\n\t\t/// \\return reference to input stream\n\t\ttemplate<typename charT,typename traits> std::basic_istream<charT,traits>&\n\t\t\toperator>>(std::basic_istream<charT,traits> &in, half &arg) { return functions::read(in, arg); }\n\n\t\t/// \\}\n\t\t/// \\name Basic mathematical operations\n\t\t/// \\{\n\n\t\t/// Absolute value.\n\t\t/// \\param arg operand\n\t\t/// \\return absolute value of \\a arg\n//\t\ttemplate<typename T> typename enable<T,T>::type abs(T arg) { return unary_specialized<T>::fabs(arg); }\n\t\tinline half abs(half arg) { return unary_specialized<half>::fabs(arg); }\n\t\tinline expr abs(expr arg) { return unary_specialized<expr>::fabs(arg); }\n\n\t\t/// Absolute value.\n\t\t/// \\param arg operand\n\t\t/// \\return absolute value of \\a arg\n//\t\ttemplate<typename T> typename enable<T,T>::type fabs(T arg) { return unary_specialized<T>::fabs(arg); }\n\t\tinline half fabs(half arg) { return unary_specialized<half>::fabs(arg); }\n\t\tinline expr fabs(expr arg) { return unary_specialized<expr>::fabs(arg); }\n\n\t\t/// Remainder of division.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\return remainder of floating point division.\n//\t\ttemplate<typename T,typename U> typename enable<expr,T,U>::type fmod(T x, U y) { return functions::fmod(x, y); }\n\t\tinline expr fmod(half x, half y) { return functions::fmod(x, y); }\n\t\tinline expr fmod(half x, expr y) { return functions::fmod(x, y); }\n\t\tinline expr fmod(expr x, half y) { return functions::fmod(x, y); }\n\t\tinline expr fmod(expr x, expr y) { return functions::fmod(x, y); }\n\n\t\t/// Remainder of division.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\return remainder of floating point division.\n//\t\ttemplate<typename T,typename U> typename enable<expr,T,U>::type remainder(T x, U y) { return functions::remainder(x, y); }\n\t\tinline expr remainder(half x, half y) { return functions::remainder(x, y); }\n\t\tinline expr remainder(half x, expr y) { return functions::remainder(x, y); }\n\t\tinline expr remainder(expr x, half y) { return functions::remainder(x, y); }\n\t\tinline expr remainder(expr x, expr y) { return functions::remainder(x, y); }\n\n\t\t/// Remainder of division.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\param quo address to store some bits of quotient at\n\t\t/// \\return remainder of floating point division.\n//\t\ttemplate<typename T,typename U> typename enable<expr,T,U>::type remquo(T x, U y, int *quo) { return functions::remquo(x, y, quo); }\n\t\tinline expr remquo(half x, half y, int *quo) { return functions::remquo(x, y, quo); }\n\t\tinline expr remquo(half x, expr y, int *quo) { return functions::remquo(x, y, quo); }\n\t\tinline expr remquo(expr x, half y, int *quo) { return functions::remquo(x, y, quo); }\n\t\tinline expr remquo(expr x, expr y, int *quo) { return functions::remquo(x, y, quo); }\n\n\t\t/// Fused multiply add.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\param z third operand\n\t\t/// \\return ( \\a x * \\a y ) + \\a z rounded as one operation.\n//\t\ttemplate<typename T,typename U,typename V> typename enable<expr,T,U,V>::type fma(T x, U y, V z) { return functions::fma(x, y, z); }\n\t\tinline expr fma(half x, half y, half z) { return functions::fma(x, y, z); }\n\t\tinline expr fma(half x, half y, expr z) { return functions::fma(x, y, z); }\n\t\tinline expr fma(half x, expr y, half z) { return functions::fma(x, y, z); }\n\t\tinline expr fma(half x, expr y, expr z) { return functions::fma(x, y, z); }\n\t\tinline expr fma(expr x, half y, half z) { return functions::fma(x, y, z); }\n\t\tinline expr fma(expr x, half y, expr z) { return functions::fma(x, y, z); }\n\t\tinline expr fma(expr x, expr y, half z) { return functions::fma(x, y, z); }\n\t\tinline expr fma(expr x, expr y, expr z) { return functions::fma(x, y, z); }\n\n\t\t/// Maximum of half expressions.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\return maximum of operands\n//\t\ttemplate<typename T,typename U> typename result<T,U>::type fmax(T x, U y) { return binary_specialized<T,U>::fmax(x, y); }\n\t\tinline half fmax(half x, half y) { return binary_specialized<half,half>::fmax(x, y); }\n\t\tinline expr fmax(half x, expr y) { return binary_specialized<half,expr>::fmax(x, y); }\n\t\tinline expr fmax(expr x, half y) { return binary_specialized<expr,half>::fmax(x, y); }\n\t\tinline expr fmax(expr x, expr y) { return binary_specialized<expr,expr>::fmax(x, y); }\n\n\t\t/// Minimum of half expressions.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\return minimum of operands\n//\t\ttemplate<typename T,typename U> typename result<T,U>::type fmin(T x, U y) { return binary_specialized<T,U>::fmin(x, y); }\n\t\tinline half fmin(half x, half y) { return binary_specialized<half,half>::fmin(x, y); }\n\t\tinline expr fmin(half x, expr y) { return binary_specialized<half,expr>::fmin(x, y); }\n\t\tinline expr fmin(expr x, half y) { return binary_specialized<expr,half>::fmin(x, y); }\n\t\tinline expr fmin(expr x, expr y) { return binary_specialized<expr,expr>::fmin(x, y); }\n\n\t\t/// Positive difference.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\return \\a x - \\a y or 0 if difference negative\n//\t\ttemplate<typename T,typename U> typename enable<expr,T,U>::type fdim(T x, U y) { return functions::fdim(x, y); }\n\t\tinline expr fdim(half x, half y) { return functions::fdim(x, y); }\n\t\tinline expr fdim(half x, expr y) { return functions::fdim(x, y); }\n\t\tinline expr fdim(expr x, half y) { return functions::fdim(x, y); }\n\t\tinline expr fdim(expr x, expr y) { return functions::fdim(x, y); }\n\n\t\t/// Get NaN value.\n\t\t/// \\return quiet NaN\n\t\tinline half nanh(const char*) { return functions::nanh(); }\n\n\t\t/// \\}\n\t\t/// \\name Exponential functions\n\t\t/// \\{\n\n\t\t/// Exponential function.\n\t\t/// \\param arg function argument\n\t\t/// \\return e raised to \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type exp(T arg) { return functions::exp(arg); }\n\t\tinline expr exp(half arg) { return functions::exp(arg); }\n\t\tinline expr exp(expr arg) { return functions::exp(arg); }\n\n\t\t/// Exponential minus one.\n\t\t/// \\param arg function argument\n\t\t/// \\return e raised to \\a arg subtracted by 1\n//\t\ttemplate<typename T> typename enable<expr,T>::type expm1(T arg) { return functions::expm1(arg); }\n\t\tinline expr expm1(half arg) { return functions::expm1(arg); }\n\t\tinline expr expm1(expr arg) { return functions::expm1(arg); }\n\n\t\t/// Binary exponential.\n\t\t/// \\param arg function argument\n\t\t/// \\return 2 raised to \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type exp2(T arg) { return functions::exp2(arg); }\n\t\tinline expr exp2(half arg) { return functions::exp2(arg); }\n\t\tinline expr exp2(expr arg) { return functions::exp2(arg); }\n\n\t\t/// Natural logorithm.\n\t\t/// \\param arg function argument\n\t\t/// \\return logarithm of \\a arg to base e\n//\t\ttemplate<typename T> typename enable<expr,T>::type log(T arg) { return functions::log(arg); }\n\t\tinline expr log(half arg) { return functions::log(arg); }\n\t\tinline expr log(expr arg) { return functions::log(arg); }\n\n\t\t/// Common logorithm.\n\t\t/// \\param arg function argument\n\t\t/// \\return logarithm of \\a arg to base 10\n//\t\ttemplate<typename T> typename enable<expr,T>::type log10(T arg) { return functions::log10(arg); }\n\t\tinline expr log10(half arg) { return functions::log10(arg); }\n\t\tinline expr log10(expr arg) { return functions::log10(arg); }\n\n\t\t/// Natural logorithm.\n\t\t/// \\param arg function argument\n\t\t/// \\return logarithm of \\a arg plus 1 to base e\n//\t\ttemplate<typename T> typename enable<expr,T>::type log1p(T arg) { return functions::log1p(arg); }\n\t\tinline expr log1p(half arg) { return functions::log1p(arg); }\n\t\tinline expr log1p(expr arg) { return functions::log1p(arg); }\n\n\t\t/// Binary logorithm.\n\t\t/// \\param arg function argument\n\t\t/// \\return logarithm of \\a arg to base 2\n//\t\ttemplate<typename T> typename enable<expr,T>::type log2(T arg) { return functions::log2(arg); }\n\t\tinline expr log2(half arg) { return functions::log2(arg); }\n\t\tinline expr log2(expr arg) { return functions::log2(arg); }\n\n\t\t/// \\}\n\t\t/// \\name Power functions\n\t\t/// \\{\n\n\t\t/// Square root.\n\t\t/// \\param arg function argument\n\t\t/// \\return square root of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type sqrt(T arg) { return functions::sqrt(arg); }\n\t\tinline expr sqrt(half arg) { return functions::sqrt(arg); }\n\t\tinline expr sqrt(expr arg) { return functions::sqrt(arg); }\n\n\t\t/// Cubic root.\n\t\t/// \\param arg function argument\n\t\t/// \\return cubic root of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type cbrt(T arg) { return functions::cbrt(arg); }\n\t\tinline expr cbrt(half arg) { return functions::cbrt(arg); }\n\t\tinline expr cbrt(expr arg) { return functions::cbrt(arg); }\n\n\t\t/// Hypotenuse function.\n\t\t/// \\param x first argument\n\t\t/// \\param y second argument\n\t\t/// \\return square root of sum of squares without internal over- or underflows\n//\t\ttemplate<typename T,typename U> typename enable<expr,T,U>::type hypot(T x, U y) { return functions::hypot(x, y); }\n\t\tinline expr hypot(half x, half y) { return functions::hypot(x, y); }\n\t\tinline expr hypot(half x, expr y) { return functions::hypot(x, y); }\n\t\tinline expr hypot(expr x, half y) { return functions::hypot(x, y); }\n\t\tinline expr hypot(expr x, expr y) { return functions::hypot(x, y); }\n\n\t\t/// Power function.\n\t\t/// \\param base first argument\n\t\t/// \\param exp second argument\n\t\t/// \\return \\a base raised to \\a exp\n//\t\ttemplate<typename T,typename U> typename enable<expr,T,U>::type pow(T base, U exp) { return functions::pow(base, exp); }\n\t\tinline expr pow(half base, half exp) { return functions::pow(base, exp); }\n\t\tinline expr pow(half base, expr exp) { return functions::pow(base, exp); }\n\t\tinline expr pow(expr base, half exp) { return functions::pow(base, exp); }\n\t\tinline expr pow(expr base, expr exp) { return functions::pow(base, exp); }\n\n\t\t/// \\}\n\t\t/// \\name Trigonometric functions\n\t\t/// \\{\n\n\t\t/// Sine function.\n\t\t/// \\param arg function argument\n\t\t/// \\return sine value of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type sin(T arg) { return functions::sin(arg); }\n\t\tinline expr sin(half arg) { return functions::sin(arg); }\n\t\tinline expr sin(expr arg) { return functions::sin(arg); }\n\n\t\t/// Cosine function.\n\t\t/// \\param arg function argument\n\t\t/// \\return cosine value of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type cos(T arg) { return functions::cos(arg); }\n\t\tinline expr cos(half arg) { return functions::cos(arg); }\n\t\tinline expr cos(expr arg) { return functions::cos(arg); }\n\n\t\t/// Tangent function.\n\t\t/// \\param arg function argument\n\t\t/// \\return tangent value of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type tan(T arg) { return functions::tan(arg); }\n\t\tinline expr tan(half arg) { return functions::tan(arg); }\n\t\tinline expr tan(expr arg) { return functions::tan(arg); }\n\n\t\t/// Arc sine.\n\t\t/// \\param arg function argument\n\t\t/// \\return arc sine value of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type asin(T arg) { return functions::asin(arg); }\n\t\tinline expr asin(half arg) { return functions::asin(arg); }\n\t\tinline expr asin(expr arg) { return functions::asin(arg); }\n\n\t\t/// Arc cosine function.\n\t\t/// \\param arg function argument\n\t\t/// \\return arc cosine value of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type acos(T arg) { return functions::acos(arg); }\n\t\tinline expr acos(half arg) { return functions::acos(arg); }\n\t\tinline expr acos(expr arg) { return functions::acos(arg); }\n\n\t\t/// Arc tangent function.\n\t\t/// \\param arg function argument\n\t\t/// \\return arc tangent value of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type atan(T arg) { return functions::atan(arg); }\n\t\tinline expr atan(half arg) { return functions::atan(arg); }\n\t\tinline expr atan(expr arg) { return functions::atan(arg); }\n\n\t\t/// Arc tangent function.\n\t\t/// \\param x first argument\n\t\t/// \\param y second argument\n\t\t/// \\return arc tangent value\n//\t\ttemplate<typename T,typename U> typename enable<expr,T,U>::type atan2(T x, U y) { return functions::atan2(x, y); }\n\t\tinline expr atan2(half x, half y) { return functions::atan2(x, y); }\n\t\tinline expr atan2(half x, expr y) { return functions::atan2(x, y); }\n\t\tinline expr atan2(expr x, half y) { return functions::atan2(x, y); }\n\t\tinline expr atan2(expr x, expr y) { return functions::atan2(x, y); }\n\n\t\t/// \\}\n\t\t/// \\name Hyperbolic functions\n\t\t/// \\{\n\n\t\t/// Hyperbolic sine.\n\t\t/// \\param arg function argument\n\t\t/// \\return hyperbolic sine value of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type sinh(T arg) { return functions::sinh(arg); }\n\t\tinline expr sinh(half arg) { return functions::sinh(arg); }\n\t\tinline expr sinh(expr arg) { return functions::sinh(arg); }\n\n\t\t/// Hyperbolic cosine.\n\t\t/// \\param arg function argument\n\t\t/// \\return hyperbolic cosine value of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type cosh(T arg) { return functions::cosh(arg); }\n\t\tinline expr cosh(half arg) { return functions::cosh(arg); }\n\t\tinline expr cosh(expr arg) { return functions::cosh(arg); }\n\n\t\t/// Hyperbolic tangent.\n\t\t/// \\param arg function argument\n\t\t/// \\return hyperbolic tangent value of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type tanh(T arg) { return functions::tanh(arg); }\n\t\tinline expr tanh(half arg) { return functions::tanh(arg); }\n\t\tinline expr tanh(expr arg) { return functions::tanh(arg); }\n\n\t\t/// Hyperbolic area sine.\n\t\t/// \\param arg function argument\n\t\t/// \\return area sine value of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type asinh(T arg) { return functions::asinh(arg); }\n\t\tinline expr asinh(half arg) { return functions::asinh(arg); }\n\t\tinline expr asinh(expr arg) { return functions::asinh(arg); }\n\n\t\t/// Hyperbolic area cosine.\n\t\t/// \\param arg function argument\n\t\t/// \\return area cosine value of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type acosh(T arg) { return functions::acosh(arg); }\n\t\tinline expr acosh(half arg) { return functions::acosh(arg); }\n\t\tinline expr acosh(expr arg) { return functions::acosh(arg); }\n\n\t\t/// Hyperbolic area tangent.\n\t\t/// \\param arg function argument\n\t\t/// \\return area tangent value of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type atanh(T arg) { return functions::atanh(arg); }\n\t\tinline expr atanh(half arg) { return functions::atanh(arg); }\n\t\tinline expr atanh(expr arg) { return functions::atanh(arg); }\n\n\t\t/// \\}\n\t\t/// \\name Error and gamma functions\n\t\t/// \\{\n\n\t\t/// Error function.\n\t\t/// \\param arg function argument\n\t\t/// \\return error function value of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type erf(T arg) { return functions::erf(arg); }\n\t\tinline expr erf(half arg) { return functions::erf(arg); }\n\t\tinline expr erf(expr arg) { return functions::erf(arg); }\n\n\t\t/// Complementary error function.\n\t\t/// \\param arg function argument\n\t\t/// \\return 1 minus error function value of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type erfc(T arg) { return functions::erfc(arg); }\n\t\tinline expr erfc(half arg) { return functions::erfc(arg); }\n\t\tinline expr erfc(expr arg) { return functions::erfc(arg); }\n\n\t\t/// Natural logarithm of gamma function.\n\t\t/// \\param arg function argument\n\t\t/// \\return natural logarith of gamma function for \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type lgamma(T arg) { return functions::lgamma(arg); }\n\t\tinline expr lgamma(half arg) { return functions::lgamma(arg); }\n\t\tinline expr lgamma(expr arg) { return functions::lgamma(arg); }\n\n\t\t/// Gamma function.\n\t\t/// \\param arg function argument\n\t\t/// \\return gamma function value of \\a arg\n//\t\ttemplate<typename T> typename enable<expr,T>::type tgamma(T arg) { return functions::tgamma(arg); }\n\t\tinline expr tgamma(half arg) { return functions::tgamma(arg); }\n\t\tinline expr tgamma(expr arg) { return functions::tgamma(arg); }\n\n\t\t/// \\}\n\t\t/// \\name Rounding\n\t\t/// \\{\n\n\t\t/// Nearest integer not less than half value.\n\t\t/// \\param arg half to round\n\t\t/// \\return nearest integer not less than \\a arg\n//\t\ttemplate<typename T> typename enable<half,T>::type ceil(T arg) { return functions::ceil(arg); }\n\t\tinline half ceil(half arg) { return functions::ceil(arg); }\n\t\tinline half ceil(expr arg) { return functions::ceil(arg); }\n\n\t\t/// Nearest integer not greater than half value.\n\t\t/// \\param arg half to round\n\t\t/// \\return nearest integer not greater than \\a arg\n//\t\ttemplate<typename T> typename enable<half,T>::type floor(T arg) { return functions::floor(arg); }\n\t\tinline half floor(half arg) { return functions::floor(arg); }\n\t\tinline half floor(expr arg) { return functions::floor(arg); }\n\n\t\t/// Nearest integer not greater in magnitude than half value.\n\t\t/// \\param arg half to round\n\t\t/// \\return nearest integer not greater in magnitude than \\a arg\n//\t\ttemplate<typename T> typename enable<half,T>::type trunc(T arg) { return functions::trunc(arg); }\n\t\tinline half trunc(half arg) { return functions::trunc(arg); }\n\t\tinline half trunc(expr arg) { return functions::trunc(arg); }\n\n\t\t/// Nearest integer.\n\t\t/// \\param arg half to round\n\t\t/// \\return nearest integer, rounded away from zero in half-way cases\n//\t\ttemplate<typename T> typename enable<half,T>::type round(T arg) { return functions::round(arg); }\n\t\tinline half round(half arg) { return functions::round(arg); }\n\t\tinline half round(expr arg) { return functions::round(arg); }\n\n\t\t/// Nearest integer.\n\t\t/// \\param arg half to round\n\t\t/// \\return nearest integer, rounded away from zero in half-way cases\n//\t\ttemplate<typename T> typename enable<long,T>::type lround(T arg) { return functions::lround(arg); }\n\t\tinline long lround(half arg) { return functions::lround(arg); }\n\t\tinline long lround(expr arg) { return functions::lround(arg); }\n\n\t\t/// Nearest integer using half's internal rounding mode.\n\t\t/// \\param arg half expression to round\n\t\t/// \\return nearest integer using default rounding mode\n//\t\ttemplate<typename T> typename enable<half,T>::type nearbyint(T arg) { return functions::nearbyint(arg); }\n\t\tinline half nearbyint(half arg) { return functions::rint(arg); }\n\t\tinline half nearbyint(expr arg) { return functions::rint(arg); }\n\n\t\t/// Nearest integer using half's internal rounding mode.\n\t\t/// \\param arg half expression to round\n\t\t/// \\return nearest integer using default rounding mode\n//\t\ttemplate<typename T> typename enable<half,T>::type rint(T arg) { return functions::rint(arg); }\n\t\tinline half rint(half arg) { return functions::rint(arg); }\n\t\tinline half rint(expr arg) { return functions::rint(arg); }\n\n\t\t/// Nearest integer using half's internal rounding mode.\n\t\t/// \\param arg half expression to round\n\t\t/// \\return nearest integer using default rounding mode\n//\t\ttemplate<typename T> typename enable<long,T>::type lrint(T arg) { return functions::lrint(arg); }\n\t\tinline long lrint(half arg) { return functions::lrint(arg); }\n\t\tinline long lrint(expr arg) { return functions::lrint(arg); }\n\t#if HALF_ENABLE_CPP11_LONG_LONG\n\t\t/// Nearest integer.\n\t\t/// \\param arg half to round\n\t\t/// \\return nearest integer, rounded away from zero in half-way cases\n//\t\ttemplate<typename T> typename enable<long long,T>::type llround(T arg) { return functions::llround(arg); }\n\t\tinline long long llround(half arg) { return functions::llround(arg); }\n\t\tinline long long llround(expr arg) { return functions::llround(arg); }\n\n\t\t/// Nearest integer using half's internal rounding mode.\n\t\t/// \\param arg half expression to round\n\t\t/// \\return nearest integer using default rounding mode\n//\t\ttemplate<typename T> typename enable<long long,T>::type llrint(T arg) { return functions::llrint(arg); }\n\t\tinline long long llrint(half arg) { return functions::llrint(arg); }\n\t\tinline long long llrint(expr arg) { return functions::llrint(arg); }\n\t#endif\n\n\t\t/// \\}\n\t\t/// \\name Floating point manipulation\n\t\t/// \\{\n\n\t\t/// Decompress floating point number.\n\t\t/// \\param arg number to decompress\n\t\t/// \\param exp address to store exponent at\n\t\t/// \\return significant in range [0.5, 1)\n//\t\ttemplate<typename T> typename enable<half,T>::type frexp(T arg, int *exp) { return functions::frexp(arg, exp); }\n\t\tinline half frexp(half arg, int *exp) { return functions::frexp(arg, exp); }\n\t\tinline half frexp(expr arg, int *exp) { return functions::frexp(arg, exp); }\n\n\t\t/// Multiply by power of two.\n\t\t/// \\param arg number to modify\n\t\t/// \\param exp power of two to multiply with\n\t\t/// \\return \\a arg multplied by 2 raised to \\a exp\n//\t\ttemplate<typename T> typename enable<half,T>::type ldexp(T arg, int exp) { return functions::scalbln(arg, exp); }\n\t\tinline half ldexp(half arg, int exp) { return functions::scalbln(arg, exp); }\n\t\tinline half ldexp(expr arg, int exp) { return functions::scalbln(arg, exp); }\n\n\t\t/// Extract integer and fractional parts.\n\t\t/// \\param arg number to decompress\n\t\t/// \\param iptr address to store integer part at\n\t\t/// \\return fractional part\n//\t\ttemplate<typename T> typename enable<half,T>::type modf(T arg, half *iptr) { return functions::modf(arg, iptr); }\n\t\tinline half modf(half arg, half *iptr) { return functions::modf(arg, iptr); }\n\t\tinline half modf(expr arg, half *iptr) { return functions::modf(arg, iptr); }\n\n\t\t/// Multiply by power of two.\n\t\t/// \\param arg number to modify\n\t\t/// \\param exp power of two to multiply with\n\t\t/// \\return \\a arg multplied by 2 raised to \\a exp\n//\t\ttemplate<typename T> typename enable<half,T>::type scalbn(T arg, int exp) { return functions::scalbln(arg, exp); }\n\t\tinline half scalbn(half arg, int exp) { return functions::scalbln(arg, exp); }\n\t\tinline half scalbn(expr arg, int exp) { return functions::scalbln(arg, exp); }\n\n\t\t/// Multiply by power of two.\n\t\t/// \\param arg number to modify\n\t\t/// \\param exp power of two to multiply with\n\t\t/// \\return \\a arg multplied by 2 raised to \\a exp\t\n//\t\ttemplate<typename T> typename enable<half,T>::type scalbln(T arg, long exp) { return functions::scalbln(arg, exp); }\n\t\tinline half scalbln(half arg, long exp) { return functions::scalbln(arg, exp); }\n\t\tinline half scalbln(expr arg, long exp) { return functions::scalbln(arg, exp); }\n\n\t\t/// Extract exponent.\n\t\t/// \\param arg number to query\n\t\t/// \\return floating point exponent\n\t\t/// \\retval FP_ILOGB0 for zero\n\t\t/// \\retval FP_ILOGBNAN for NaN\n\t\t/// \\retval MAX_INT for infinity\n//\t\ttemplate<typename T> typename enable<int,T>::type ilogb(T arg) { return functions::ilogb(arg); }\n\t\tinline int ilogb(half arg) { return functions::ilogb(arg); }\n\t\tinline int ilogb(expr arg) { return functions::ilogb(arg); }\n\n\t\t/// Extract exponent.\n\t\t/// \\param arg number to query\n\t\t/// \\return floating point exponent\n//\t\ttemplate<typename T> typename enable<half,T>::type logb(T arg) { return functions::logb(arg); }\n\t\tinline half logb(half arg) { return functions::logb(arg); }\n\t\tinline half logb(expr arg) { return functions::logb(arg); }\n\n\t\t/// Next representable value.\n\t\t/// \\param from value to compute next representable value for\n\t\t/// \\param to direction towards which to compute next value\n\t\t/// \\return next representable value after \\a from in direction towards \\a to\n//\t\ttemplate<typename T,typename U> typename enable<half,T,U>::type nextafter(T from, U to) { return functions::nextafter(from, to); }\n\t\tinline half nextafter(half from, half to) { return functions::nextafter(from, to); }\n\t\tinline half nextafter(half from, expr to) { return functions::nextafter(from, to); }\n\t\tinline half nextafter(expr from, half to) { return functions::nextafter(from, to); }\n\t\tinline half nextafter(expr from, expr to) { return functions::nextafter(from, to); }\n\n\t\t/// Next representable value.\n\t\t/// \\param from value to compute next representable value for\n\t\t/// \\param to direction towards which to compute next value\n\t\t/// \\return next representable value after \\a from in direction towards \\a to\n//\t\ttemplate<typename T> typename enable<half,T>::type nexttoward(T from, long double to) { return functions::nexttoward(from, to); }\n\t\tinline half nexttoward(half from, long double to) { return functions::nexttoward(from, to); }\n\t\tinline half nexttoward(expr from, long double to) { return functions::nexttoward(from, to); }\n\n\t\t/// Take sign.\n\t\t/// \\param x value to change sign for\n\t\t/// \\param y value to take sign from\n\t\t/// \\return value equal to \\a x in magnitude and to \\a y in sign\n//\t\ttemplate<typename T,typename U> typename enable<half,T,U>::type copysign(T x, U y) { return functions::copysign(x, y); }\n\t\tinline half copysign(half x, half y) { return functions::copysign(x, y); }\n\t\tinline half copysign(half x, expr y) { return functions::copysign(x, y); }\n\t\tinline half copysign(expr x, half y) { return functions::copysign(x, y); }\n\t\tinline half copysign(expr x, expr y) { return functions::copysign(x, y); }\n\n\t\t/// \\}\n\t\t/// \\name Floating point classification\n\t\t/// \\{\n\n\n\t\t/// Classify floating point value.\n\t\t/// \\param arg number to classify\n\t\t/// \\retval FP_ZERO for positive and negative zero\n\t\t/// \\retval FP_SUBNORMAL for subnormal numbers\n\t\t/// \\retval FP_INFINITY for positive and negative infinity\n\t\t/// \\retval FP_NAN for NaNs\n\t\t/// \\retval FP_NORMAL for all other (normal) values\n//\t\ttemplate<typename T> typename enable<int,T>::type fpclassify(T arg) { return functions::fpclassify(arg); }\n\t\tinline int fpclassify(half arg) { return functions::fpclassify(arg); }\n\t\tinline int fpclassify(expr arg) { return functions::fpclassify(arg); }\n\n\t\t/// Check if finite number.\n\t\t/// \\param arg number to check\n\t\t/// \\retval true if neither infinity nor NaN\n\t\t/// \\retval false else\n//\t\ttemplate<typename T> typename enable<bool,T>::type isfinite(T arg) { return functions::isfinite(arg); }\n\t\tinline bool isfinite(half arg) { return functions::isfinite(arg); }\n\t\tinline bool isfinite(expr arg) { return functions::isfinite(arg); }\n\n\t\t/// Check for infinity.\n\t\t/// \\param arg number to check\n\t\t/// \\retval true for positive or negative infinity\n\t\t/// \\retval false else\n//\t\ttemplate<typename T> typename enable<bool,T>::type isinf(T arg) { return functions::isinf(arg); }\n\t\tinline bool isinf(half arg) { return functions::isinf(arg); }\n\t\tinline bool isinf(expr arg) { return functions::isinf(arg); }\n\n\t\t/// Check for NaN.\n\t\t/// \\param arg number to check\n\t\t/// \\retval true for NaNs\n\t\t/// \\retval false else\n//\t\ttemplate<typename T> typename enable<bool,T>::type isnan(T arg) { return functions::isnan(arg); }\n\t\tinline bool isnan(half arg) { return functions::isnan(arg); }\n\t\tinline bool isnan(expr arg) { return functions::isnan(arg); }\n\n\t\t/// Check if normal number.\n\t\t/// \\param arg number to check\n\t\t/// \\retval true if normal number\n\t\t/// \\retval false if either subnormal, zero, infinity or NaN\n//\t\ttemplate<typename T> typename enable<bool,T>::type isnormal(T arg) { return functions::isnormal(arg); }\n\t\tinline bool isnormal(half arg) { return functions::isnormal(arg); }\n\t\tinline bool isnormal(expr arg) { return functions::isnormal(arg); }\n\n\t\t/// Check sign.\n\t\t/// \\param arg number to check\n\t\t/// \\retval true for negative number\n\t\t/// \\retval false for positive number\n//\t\ttemplate<typename T> typename enable<bool,T>::type signbit(T arg) { return functions::signbit(arg); }\n\t\tinline bool signbit(half arg) { return functions::signbit(arg); }\n\t\tinline bool signbit(expr arg) { return functions::signbit(arg); }\n\n\t\t/// \\}\n\t\t/// \\name Comparison\n\t\t/// \\{\n\n\t\t/// Comparison for greater than.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\retval true if \\a x greater than \\a y\n\t\t/// \\retval false else\n//\t\ttemplate<typename T,typename U> typename enable<bool,T,U>::type isgreater(T x, U y) { return functions::isgreater(x, y); }\n\t\tinline bool isgreater(half x, half y) { return functions::isgreater(x, y); }\n\t\tinline bool isgreater(half x, expr y) { return functions::isgreater(x, y); }\n\t\tinline bool isgreater(expr x, half y) { return functions::isgreater(x, y); }\n\t\tinline bool isgreater(expr x, expr y) { return functions::isgreater(x, y); }\n\n\t\t/// Comparison for greater equal.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\retval true if \\a x greater equal \\a y\n\t\t/// \\retval false else\n//\t\ttemplate<typename T,typename U> typename enable<bool,T,U>::type isgreaterequal(T x, U y) { return functions::isgreaterequal(x, y); }\n\t\tinline bool isgreaterequal(half x, half y) { return functions::isgreaterequal(x, y); }\n\t\tinline bool isgreaterequal(half x, expr y) { return functions::isgreaterequal(x, y); }\n\t\tinline bool isgreaterequal(expr x, half y) { return functions::isgreaterequal(x, y); }\n\t\tinline bool isgreaterequal(expr x, expr y) { return functions::isgreaterequal(x, y); }\n\n\t\t/// Comparison for less than.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\retval true if \\a x less than \\a y\n\t\t/// \\retval false else\n//\t\ttemplate<typename T,typename U> typename enable<bool,T,U>::type isless(T x, U y) { return functions::isless(x, y); }\n\t\tinline bool isless(half x, half y) { return functions::isless(x, y); }\n\t\tinline bool isless(half x, expr y) { return functions::isless(x, y); }\n\t\tinline bool isless(expr x, half y) { return functions::isless(x, y); }\n\t\tinline bool isless(expr x, expr y) { return functions::isless(x, y); }\n\n\t\t/// Comparison for less equal.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\retval true if \\a x less equal \\a y\n\t\t/// \\retval false else\n//\t\ttemplate<typename T,typename U> typename enable<bool,T,U>::type islessequal(T x, U y) { return functions::islessequal(x, y); }\n\t\tinline bool islessequal(half x, half y) { return functions::islessequal(x, y); }\n\t\tinline bool islessequal(half x, expr y) { return functions::islessequal(x, y); }\n\t\tinline bool islessequal(expr x, half y) { return functions::islessequal(x, y); }\n\t\tinline bool islessequal(expr x, expr y) { return functions::islessequal(x, y); }\n\n\t\t/// Comarison for less or greater.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\retval true if either less or greater\n\t\t/// \\retval false else\n//\t\ttemplate<typename T,typename U> typename enable<bool,T,U>::type islessgreater(T x, U y) { return functions::islessgreater(x, y); }\n\t\tinline bool islessgreater(half x, half y) { return functions::islessgreater(x, y); }\n\t\tinline bool islessgreater(half x, expr y) { return functions::islessgreater(x, y); }\n\t\tinline bool islessgreater(expr x, half y) { return functions::islessgreater(x, y); }\n\t\tinline bool islessgreater(expr x, expr y) { return functions::islessgreater(x, y); }\n\n\t\t/// Check if unordered.\n\t\t/// \\param x first operand\n\t\t/// \\param y second operand\n\t\t/// \\retval true if unordered (one or two NaN operands)\n\t\t/// \\retval false else\n//\t\ttemplate<typename T,typename U> typename enable<bool,T,U>::type isunordered(T x, U y) { return functions::isunordered(x, y); }\n\t\tinline bool isunordered(half x, half y) { return functions::isunordered(x, y); }\n\t\tinline bool isunordered(half x, expr y) { return functions::isunordered(x, y); }\n\t\tinline bool isunordered(expr x, half y) { return functions::isunordered(x, y); }\n\t\tinline bool isunordered(expr x, expr y) { return functions::isunordered(x, y); }\n\n\t\t/// \\name Casting\n\t\t/// \\{\n\n\t\t/// Cast to or from half-precision floating point number.\n\t\t/// This casts between [half](\\ref half_float::half) and any built-in arithmetic type. The values are converted \n\t\t/// directly using the given rounding mode, without any roundtrip over `float` that a `static_cast` would otherwise do. \n\t\t/// It uses the default rounding mode.\n\t\t///\n\t\t/// Using this cast with neither of the two types being a [half](\\ref half_float::half) or with any of the two types \n\t\t/// not being a built-in arithmetic type (apart from [half](\\ref half_float::half), of course) results in a compiler \n\t\t/// error and casting between [half](\\ref half_float::half)s is just a no-op.\n\t\t/// \\tparam T destination type (half or built-in arithmetic type)\n\t\t/// \\tparam U source type (half or built-in arithmetic type)\n\t\t/// \\param arg value to cast\n\t\t/// \\return \\a arg converted to destination type\n\t\ttemplate<typename T,typename U> T half_cast(U arg) { return half_caster<T,U>::cast(arg); }\n\n\t\t/// Cast to or from half-precision floating point number.\n\t\t/// This casts between [half](\\ref half_float::half) and any built-in arithmetic type. The values are converted \n\t\t/// directly using the given rounding mode, without any roundtrip over `float` that a `static_cast` would otherwise do.\n\t\t///\n\t\t/// Using this cast with neither of the two types being a [half](\\ref half_float::half) or with any of the two types \n\t\t/// not being a built-in arithmetic type (apart from [half](\\ref half_float::half), of course) results in a compiler \n\t\t/// error and casting between [half](\\ref half_float::half)s is just a no-op.\n\t\t/// \\tparam T destination type (half or built-in arithmetic type)\n\t\t/// \\tparam R rounding mode to use.\n\t\t/// \\tparam U source type (half or built-in arithmetic type)\n\t\t/// \\param arg value to cast\n\t\t/// \\return \\a arg converted to destination type\n\t\ttemplate<typename T,std::float_round_style R,typename U> T half_cast(U arg) { return half_caster<T,U,R>::cast(arg); }\n\t\t/// \\}\n\t}\n\n\tusing detail::operator==;\n\tusing detail::operator!=;\n\tusing detail::operator<;\n\tusing detail::operator>;\n\tusing detail::operator<=;\n\tusing detail::operator>=;\n\tusing detail::operator+;\n\tusing detail::operator-;\n\tusing detail::operator*;\n\tusing detail::operator/;\n\tusing detail::operator<<;\n\tusing detail::operator>>;\n\n\tusing detail::abs;\n\tusing detail::fabs;\n\tusing detail::fmod;\n\tusing detail::remainder;\n\tusing detail::remquo;\n\tusing detail::fma;\n\tusing detail::fmax;\n\tusing detail::fmin;\n\tusing detail::fdim;\n\tusing detail::nanh;\n\tusing detail::exp;\n\tusing detail::expm1;\n\tusing detail::exp2;\n\tusing detail::log;\n\tusing detail::log10;\n\tusing detail::log1p;\n\tusing detail::log2;\n\tusing detail::sqrt;\n\tusing detail::cbrt;\n\tusing detail::hypot;\n\tusing detail::pow;\n\tusing detail::sin;\n\tusing detail::cos;\n\tusing detail::tan;\n\tusing detail::asin;\n\tusing detail::acos;\n\tusing detail::atan;\n\tusing detail::atan2;\n\tusing detail::sinh;\n\tusing detail::cosh;\n\tusing detail::tanh;\n\tusing detail::asinh;\n\tusing detail::acosh;\n\tusing detail::atanh;\n\tusing detail::erf;\n\tusing detail::erfc;\n\tusing detail::lgamma;\n\tusing detail::tgamma;\n\tusing detail::ceil;\n\tusing detail::floor;\n\tusing detail::trunc;\n\tusing detail::round;\n\tusing detail::lround;\n\tusing detail::nearbyint;\n\tusing detail::rint;\n\tusing detail::lrint;\n#if HALF_ENABLE_CPP11_LONG_LONG\n\tusing detail::llround;\n\tusing detail::llrint;\n#endif\n\tusing detail::frexp;\n\tusing detail::ldexp;\n\tusing detail::modf;\n\tusing detail::scalbn;\n\tusing detail::scalbln;\n\tusing detail::ilogb;\n\tusing detail::logb;\n\tusing detail::nextafter;\n\tusing detail::nexttoward;\n\tusing detail::copysign;\n\tusing detail::fpclassify;\n\tusing detail::isfinite;\n\tusing detail::isinf;\n\tusing detail::isnan;\n\tusing detail::isnormal;\n\tusing detail::signbit;\n\tusing detail::isgreater;\n\tusing detail::isgreaterequal;\n\tusing detail::isless;\n\tusing detail::islessequal;\n\tusing detail::islessgreater;\n\tusing detail::isunordered;\n\n\tusing detail::half_cast;\n}\n\n\n/// Extensions to the C++ standard library.\nnamespace std\n{\n\t/// Numeric limits for half-precision floats.\n\t/// Because of the underlying single-precision implementation of many operations, it inherits some properties from \n\t/// `std::numeric_limits<float>`.\n\ttemplate<> class numeric_limits<half_float::half> : public numeric_limits<float>\n\t{\n\tpublic:\n\t\t/// Supports signed values.\n\t\tstatic HALF_CONSTEXPR_CONST bool is_signed = true;\n\n\t\t/// Is not exact.\n\t\tstatic HALF_CONSTEXPR_CONST bool is_exact = false;\n\n\t\t/// Doesn't provide modulo arithmetic.\n\t\tstatic HALF_CONSTEXPR_CONST bool is_modulo = false;\n\n\t\t/// IEEE conformant.\n\t\tstatic HALF_CONSTEXPR_CONST bool is_iec559 = true;\n\n\t\t/// Supports infinity.\n\t\tstatic HALF_CONSTEXPR_CONST bool has_infinity = true;\n\n\t\t/// Supports quiet NaNs.\n\t\tstatic HALF_CONSTEXPR_CONST bool has_quiet_NaN = true;\n\n\t\t/// Supports subnormal values.\n\t\tstatic HALF_CONSTEXPR_CONST float_denorm_style has_denorm = denorm_present;\n\n\t\t/// Rounding mode.\n\t\t/// Due to the mix of internal single-precision computations (using the rounding mode of the underlying \n\t\t/// single-precision implementation) with the rounding mode of the single-to-half conversions, the actual rounding \n\t\t/// mode might be `std::round_indeterminate` if the default half-precision rounding mode doesn't match the \n\t\t/// single-precision rounding mode.\n\t\tstatic HALF_CONSTEXPR_CONST float_round_style round_style = (std::numeric_limits<float>::round_style==\n\t\t\thalf_float::half::round_style) ? half_float::half::round_style : round_indeterminate;\n\n\t\t/// Significant digits.\n\t\tstatic HALF_CONSTEXPR_CONST int digits = 11;\n\n\t\t/// Significant decimal digits.\n\t\tstatic HALF_CONSTEXPR_CONST int digits10 = 3;\n\n\t\t/// Required decimal digits to represent all possible values.\n\t\tstatic HALF_CONSTEXPR_CONST int max_digits10 = 5;\n\n\t\t/// Number base.\n\t\tstatic HALF_CONSTEXPR_CONST int radix = 2;\n\n\t\t/// One more than smallest exponent.\n\t\tstatic HALF_CONSTEXPR_CONST int min_exponent = -13;\n\n\t\t/// Smallest normalized representable power of 10.\n\t\tstatic HALF_CONSTEXPR_CONST int min_exponent10 = -4;\n\n\t\t/// One more than largest exponent\n\t\tstatic HALF_CONSTEXPR_CONST int max_exponent = 16;\n\n\t\t/// Largest finitely representable power of 10.\n\t\tstatic HALF_CONSTEXPR_CONST int max_exponent10 = 4;\n\n\t\t/// Smallest positive normal value.\n\t\tstatic HALF_CONSTEXPR half_float::half min() HALF_NOTHROW { return half_float::half(half_float::detail::binary, 0x0400); }\n\n\t\t/// Smallest finite value.\n\t\tstatic HALF_CONSTEXPR half_float::half lowest() HALF_NOTHROW { return half_float::half(half_float::detail::binary, 0xFBFF); }\n\n\t\t/// Largest finite value.\n\t\tstatic HALF_CONSTEXPR half_float::half max() HALF_NOTHROW { return half_float::half(half_float::detail::binary, 0x7BFF); }\n\n\t\t/// Difference between one and next representable value.\n\t\tstatic HALF_CONSTEXPR half_float::half epsilon() HALF_NOTHROW { return half_float::half(half_float::detail::binary, 0x1400); }\n\n\t\t/// Maximum rounding error.\n\t\tstatic HALF_CONSTEXPR half_float::half round_error() HALF_NOTHROW\n\t\t\t{ return half_float::half(half_float::detail::binary, (round_style==std::round_to_nearest) ? 0x3800 : 0x3C00); }\n\n\t\t/// Positive infinity.\n\t\tstatic HALF_CONSTEXPR half_float::half infinity() HALF_NOTHROW { return half_float::half(half_float::detail::binary, 0x7C00); }\n\n\t\t/// Quiet NaN.\n\t\tstatic HALF_CONSTEXPR half_float::half quiet_NaN() HALF_NOTHROW { return half_float::half(half_float::detail::binary, 0x7FFF); }\n\n\t\t/// Signalling NaN.\n\t\tstatic HALF_CONSTEXPR half_float::half signaling_NaN() HALF_NOTHROW { return half_float::half(half_float::detail::binary, 0x7DFF); }\n\n\t\t/// Smallest positive subnormal value.\n\t\tstatic HALF_CONSTEXPR half_float::half denorm_min() HALF_NOTHROW { return half_float::half(half_float::detail::binary, 0x0001); }\n\t};\n\n#if HALF_ENABLE_CPP11_HASH\n\t/// Hash function for half-precision floats.\n\t/// This is only defined if C++11 `std::hash` is supported and enabled.\n\ttemplate<> struct hash<half_float::half> //: unary_function<half_float::half,size_t>\n\t{\n\t\t/// Type of function argument.\n\t\ttypedef half_float::half argument_type;\n\n\t\t/// Function return type.\n\t\ttypedef size_t result_type;\n\n\t\t/// Compute hash function.\n\t\t/// \\param arg half to hash\n\t\t/// \\return hash value\n\t\tresult_type operator()(argument_type arg) const\n\t\t\t{ return hash<half_float::detail::uint16>()(static_cast<unsigned>(arg.data_)&-(arg.data_!=0x8000)); }\n\t};\n#endif\n}\n\n\n#undef HALF_CONSTEXPR\n#undef HALF_CONSTEXPR_CONST\n#undef HALF_NOEXCEPT\n#undef HALF_NOTHROW\n#ifdef HALF_POP_WARNINGS\n\t#pragma warning(pop)\n\t#undef HALF_POP_WARNINGS\n#endif\n\n#endif\n"
        },
        {
          "name": "importerUtils.cpp",
          "type": "blob",
          "size": 96.6240234375,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"importerUtils.hpp\"\n#include \"OnnxAttrs.hpp\"\n#include \"Status.hpp\"\n#include \"bfloat16.hpp\"\n#include \"errorHelpers.hpp\"\n#include <ctype.h>\n#include <ostream>\n#include <regex>\n#include <set>\n\nnamespace onnx2trt\n{\n\nvoid PluginDeleter::operator()(nvinfer1::IPluginV2* t)\n{\n    t->destroy();\n}\n\nStatus notInvalidType(TensorOrWeights const& input, std::vector<std::string> const& invalidTypes,\n    ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx)\n{\n    bool invalid = std::any_of(invalidTypes.begin(), invalidTypes.end(),\n        [&](std::string invalidType) { return input.getType() == invalidType; });\n    if (invalid)\n    {\n        ASSERT_NODE(\n            false, \"Found unsupported input type of \" << input.getType(), node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    }\n    return Status::success();\n}\n\nvoid checkNotInvalidType(TensorOrWeights const& input, std::vector<std::string> const& invalidTypes,\n    ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx)\n{\n    Status status = notInvalidType(input, invalidTypes, node, nodeIdx);\n    ONNXTRT_CHECK_NODE(status.is_success(), \"Found unsupported input type of \" << input.getType(), node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n}\n\nNodeOutputs activationHelper(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights>& inputs, nvinfer1::ActivationType op, float* alpha, float* beta)\n{\n    checkNotInvalidType(inputs.at(0), {\"INT32\", \"BOOL\", \"UINT8\"}, node, nodeIdx);\n    nvinfer1::ITensor& input = convertToTensor(inputs.at(0), ctx);\n    nvinfer1::IActivationLayer* layer = N_CHECK(ctx->network()->addActivation(input, op));\n    ONNXTRT_CHECK_NODE(layer, \"Failed to add activation layer!\", node, nodeIdx, ErrorCode::kINTERNAL_ERROR);\n    if (alpha)\n    {\n        layer->setAlpha(*alpha);\n    }\n    if (beta)\n    {\n        layer->setBeta(*beta);\n    }\n    ctx->registerLayer(layer, node);\n    auto* output = N_CHECK(layer->getOutput(0));\n    return {{output}};\n}\n\nnvinfer1::ITensor* addClip(ImporterContext* ctx, nvinfer1::ITensor* input, float clip)\n{\n    if (clip >= 0.f)\n    {\n        nvinfer1::IActivationLayer* layer\n            = N_CHECK(ctx->network()->addActivation(*input, nvinfer1::ActivationType::kCLIP));\n        layer->setAlpha(-clip);\n        layer->setBeta(clip);\n        return N_CHECK(layer->getOutput(0));\n    }\n    return input;\n};\n\nNodeOutputs argMinMaxHelper(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights>& inputs, nvinfer1::TopKOperation op)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    nvinfer1::ITensor* tensor = &convertToTensor(inputs.at(0), ctx);\n\n    // Get attributes.\n    OnnxAttrs attrs(node, ctx);\n    int32_t keepdims = attrs.get(\"keepdims\", 1);\n    int32_t axis = attrs.get(\"axis\", 0);\n    int32_t selectLastIndex = attrs.get<int32_t>(\"select_last_index\", 0);\n\n    // Insert a TopK layer with k set to 1.\n    int32_t nbDims = tensor->getDimensions().nbDims;\n    convertAxis(axis, nbDims, node, nodeIdx);\n    uint32_t axisMask = 1 << axis;\n    nvinfer1::ITopKLayer* layer;\n\n    // New attribute added to Opset-12\n    // Whether to select the last index or the first index if the {name} appears in multiple indices, default is False\n    // (first index).\n    if (selectLastIndex)\n    {\n        // Need to flip the data input along the given axis using the Slice operator\n        auto const dims = shapeOf(*tensor);\n        ShapeTensor starts = shapeVector(-1);\n        ShapeTensor ends = shapeVector(static_cast<int64_t>(INT_MIN));\n        ShapeTensor axes = shapeVector(axis);\n        ShapeTensor steps = shapeVector(-1);\n\n        if (axes.size() < dims.size())\n        {\n            // axes specify a subset of the dimensions, or out of order.\n            // Convert starts/ends/steps to complete in-order form.\n            ShapeTensor const subscripts{axesToInterlaceSubscripts(axes, dims.size())};\n            starts = interlace(ctx, similar(ctx, dims, 0), starts, subscripts);\n            ends = interlace(ctx, dims, ends, subscripts);\n            steps = interlace(ctx, similar(ctx, dims, 1), steps, subscripts);\n        }\n        decodeOnnxStartsAndEnds(ctx, dims, steps, starts, ends);\n        // TensorRT uses sizes of the output dimensions instead of ends.\n        ShapeTensor const sizes = computeSliceSizes(ctx, starts, ends, steps, dims);\n\n        nvinfer1::ISliceLayer* slice = addSlice(ctx, *tensor, starts, sizes, steps);\n        auto flippedTensor = N_CHECK(slice->getOutput(0));\n        layer = N_CHECK(ctx->network()->addTopK(*flippedTensor, op, 1, axisMask));\n    }\n    else\n    {\n        layer = N_CHECK(ctx->network()->addTopK(*tensor, op, 1, axisMask));\n    }\n\n    ctx->registerLayer(layer, node);\n\n    // We don't care about the TopK values, just the indices.\n    nvinfer1::ITensor* indices = N_CHECK(layer->getOutput(1));\n    indices = castHelper(ctx, indices, nvinfer1::DataType::kINT64);\n\n    // If selectLastIndex is true, the TopK operation was performed on reversed data on the provided axis.\n    // Convert reversed indices back to forward indices by calculating the following:\n    // indices = shape(tensor)[axis] - indices - 1\n    if (selectLastIndex)\n    {\n        // Use shapeTensor semantics to support dynamic shapes\n        auto const dims = shapeOf(*tensor);\n        auto const indicesDims = shapeOf(*indices);\n        auto const axisTensor = shapeVector(axis);\n        auto const dimOnAxis = gather(ctx, dims, axisTensor);\n\n        // Create constant of shape indicesDims with values tensor.shape[axis]\n        auto const tensorDimOnAxis = constantOfShape(ctx, &dimOnAxis.tensor(ctx), &indicesDims.tensor(ctx));\n\n        // Create constant of shape indicesDims with values of 1\n        auto const ones = constantOfShape(ctx, &shapeVector(1).tensor(ctx), &indicesDims.tensor(ctx));\n\n        std::vector<TensorOrWeights> newInputs{tensorDimOnAxis, indices, ones};\n        std::vector<TensorOrWeights> indicesUpdate\n            = elementwiseHelper(ctx, node, nodeIdx, newInputs, nvinfer1::ElementWiseOperation::kSUB);\n        indices = &convertToTensor(indicesUpdate.at(0), ctx);\n    }\n    // The default behavior of the TopK layer is to keepdims.\n    // Otherwise, we need to squeeze the axis dimension.\n    if (!keepdims)\n    {\n        std::vector<int32_t> axes{axis};\n        indices = squeezeTensor(ctx, *indices, axes);\n    }\n    // TensorRT doesn't support int64 for TopK indices\n    indices = castHelper(ctx, indices, nvinfer1::DataType::kINT64);\n    return {{indices}};\n}\n\nvoid broadcastTensor(ImporterContext* ctx, nvinfer1::ITensor*& t, int const nbDims)\n{\n    ONNXTRT_CHECK(\n        ctx->getOpsetVersion() >= 7 && \"Pre-opset 7 broadcasting is unsupported in this version of the ONNX parser\",\n        ErrorCode::kUNSUPPORTED_NODE);\n    auto const inputDims = shapeOf(*t);\n    int const nbInputDims = inputDims.size();\n    ONNXTRT_CHECK((nbInputDims <= nbDims) && \"Cannot broadcast a higher rank tensor to a lower rank tensor.\",\n        ErrorCode::kUNSUPPORTED_NODE);\n    if (nbInputDims < nbDims)\n    {\n        nvinfer1::IShuffleLayer* reshape\n            = addShuffle(ctx, *t, concat(ctx, fillShapeVector(ctx, 1, shapeVector(nbDims - nbInputDims)), shapeOf(*t)));\n        ctx->registerLayer(reshape, \"ONNXTRT_Broadcast\", nullptr);\n        t = N_CHECK(reshape->getOutput(0));\n    }\n}\n\nvoid broadcastTensors(ImporterContext* ctx, nvinfer1::ITensor*& t1, nvinfer1::ITensor*& t2)\n{\n    int const t1Dims = t1->getDimensions().nbDims;\n    int const t2Dims = t2->getDimensions().nbDims;\n\n    if (t1Dims == t2Dims)\n    {\n        return;\n    }\n\n    if (t1Dims > t2Dims)\n    {\n        return broadcastTensor(ctx, t2, t1Dims);\n    }\n    return broadcastTensor(ctx, t1, t2Dims);\n}\n\nvoid broadcastTensors(ImporterContext* ctx, nvinfer1::ITensor*& t1, nvinfer1::ITensor*& t2, nvinfer1::ITensor*& t3)\n{\n    int const maxDims = std::max({t1->getDimensions().nbDims, t2->getDimensions().nbDims, t3->getDimensions().nbDims});\n    broadcastTensor(ctx, t1, maxDims);\n    broadcastTensor(ctx, t2, maxDims);\n    broadcastTensor(ctx, t3, maxDims);\n}\n\n// Helper functions for calculateBias:\nint32_t getBias(std::vector<int32_t> const& dimension_count, std::vector<int32_t> const& pitches, int32_t axis)\n{\n    int32_t result{0};\n    for (int32_t i = 0; i < static_cast<int32_t>(dimension_count.size()); i++)\n    {\n        if (i != axis)\n        {\n            result += dimension_count[i] * pitches[i];\n        }\n    }\n    return result;\n}\n\nvoid incrementOuterDimension(std::vector<int32_t>& dimensionCount, nvinfer1::Dims idxDims)\n{\n    // Start at [x,x,0]. Increment starting from the outer dimension.\n    int32_t rank = dimensionCount.size();\n\n    for (int32_t i = rank - 1; i >= 0; i--)\n    {\n        int dimLimit = idxDims.d[i];\n        // If we're not at the limit, increment current axis and return\n        if (++dimensionCount[i] != dimLimit)\n        {\n            break;\n        }\n        // Else, we increment on the next dimension and reset current one\n        dimensionCount[i] = 0;\n    }\n}\n\nstd::vector<int32_t> calculateBias(\n    nvinfer1::Dims const& daDims, nvinfer1::Dims const& idxDims, std::vector<int32_t> const& pitches, int32_t axis)\n{\n    std::vector<int32_t> biasVector;\n    std::vector<int32_t> dimensionCount(daDims.nbDims, 0);\n    int64_t total = volume(idxDims);\n\n    for (int64_t i = 0; i < total; i++)\n    {\n        int32_t bias = getBias(dimensionCount, pitches, axis);\n        biasVector.push_back(bias);\n        incrementOuterDimension(dimensionCount, idxDims);\n    }\n    return biasVector;\n}\n\nstd::vector<int32_t> calculatePitches(nvinfer1::Dims const& inputDims)\n{\n    int32_t pitch = 1;\n    int32_t nbDims = inputDims.nbDims;\n    std::vector<int32_t> pitches(nbDims);\n    pitches[nbDims - 1] = pitch;\n    for (int32_t i = nbDims - 2; i >= 0; i--)\n    {\n        pitch *= inputDims.d[i + 1];\n        pitches[i] = pitch;\n    }\n    return pitches;\n}\n\nbool canUseNDResize(size_t const scaleSize, float const* scaleFactors, size_t const n)\n{\n    // Linear resize supports up to 3D resize on the outermost dimensions (n = 3).\n    if (scaleSize > n)\n    {\n        for (size_t i = 0; i < scaleSize - n; i++)\n        {\n            if (scaleFactors[i] != 1)\n            {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n\nnvinfer1::ITensor* castHelper(ImporterContext* ctx, nvinfer1::ITensor* input, nvinfer1::DataType dtype)\n{\n    nvinfer1::ICastLayer* cast = N_CHECK(ctx->network()->addCast(*input, dtype));\n    ctx->registerLayer(cast, \"ONNXTRT_castHelper\", nullptr);\n    return N_CHECK(cast->getOutput(0));\n}\n\nnvinfer1::ITensor* constantOfShape(ImporterContext* ctx, nvinfer1::ITensor* constant, nvinfer1::ITensor* shape)\n{\n    ShapeTensor shapeT{*shape};\n    ShapeTensor zeros = similar(ctx, shapeT, 0);\n    // `constant` must be broadcasted to the same rank as `shape`.\n    ShapeTensor broadcastedShape = similar(ctx, shapeT, 1);\n    constant = &reshape(ctx, *constant, broadcastedShape);\n    auto* l = N_CHECK(addSlice(ctx, *constant, zeros, shapeT, zeros));\n    return N_CHECK(l->getOutput(0));\n}\n\nvoid convertAxis(int32_t& axis, int32_t const nbDims, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx)\n{\n    // Support negative indexing\n    if (axis < 0)\n    {\n        axis += nbDims;\n    }\n    // Support nbDims as a valid axis for QuantDequantLinearHelper\n    ONNXTRT_CHECK_NODE((axis >= 0 && axis <= nbDims),\n        \"Axis must be in the range [0, nbDims (\" << nbDims << \")]. Provided axis is: \" << axis, node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n}\n\nbool convertDtype(int32_t onnx_dtype, nvinfer1::DataType* trt_dtype)\n{\n    switch (onnx_dtype)\n    {\n    case ::ONNX_NAMESPACE::TensorProto::DOUBLE: *trt_dtype = nvinfer1::DataType::kFLOAT; break;\n    case ::ONNX_NAMESPACE::TensorProto::FLOAT: *trt_dtype = nvinfer1::DataType::kFLOAT; break;\n    case ::ONNX_NAMESPACE::TensorProto::INT8: *trt_dtype = nvinfer1::DataType::kINT8; break;\n    case ::ONNX_NAMESPACE::TensorProto::UINT8: *trt_dtype = nvinfer1::DataType::kUINT8; break;\n    case ::ONNX_NAMESPACE::TensorProto::FLOAT16: *trt_dtype = nvinfer1::DataType::kHALF; break;\n    case ::ONNX_NAMESPACE::TensorProto::BFLOAT16: *trt_dtype = nvinfer1::DataType::kBF16; break;\n    case ::ONNX_NAMESPACE::TensorProto::BOOL: *trt_dtype = nvinfer1::DataType::kBOOL; break;\n    case ::ONNX_NAMESPACE::TensorProto::INT32: *trt_dtype = nvinfer1::DataType::kINT32; break;\n    case ::ONNX_NAMESPACE::TensorProto::INT64: *trt_dtype = nvinfer1::DataType::kINT64; break;\n    case ::ONNX_NAMESPACE::TensorProto::FLOAT8E4M3FN: *trt_dtype = nvinfer1::DataType::kFP8; break;\n    case ::ONNX_NAMESPACE::TensorProto::INT4: *trt_dtype = nvinfer1::DataType::kINT4; break;\n    default:\n        std::cerr << \"Unsupported ONNX data type: \" << getDtypeName(onnx_dtype) << \" (\" << std::to_string(onnx_dtype)\n                  << \")\" << std::endl;\n        return false;\n    }\n    return true;\n}\n\nbool convertOnnxPadding(ImporterContext* ctx, int32_t nbInputDims, std::vector<int64_t> const& onnxPadding,\n    nvinfer1::ITensor*& startTensor, nvinfer1::ITensor*& totalPaddingTensor)\n{\n    std::vector<int64_t> start;\n    std::vector<int64_t> totalPadding;\n    if (onnxPadding.size() % 2U != 0)\n    {\n        return false;\n    }\n    auto const diff = nbInputDims - static_cast<int32_t>(onnxPadding.size() / 2U);\n    if (diff < 0)\n    {\n        return false;\n    }\n    start.resize(nbInputDims, 0);\n    totalPadding.resize(nbInputDims, 0);\n\n    for (int32_t i = diff; i < nbInputDims; i++)\n    {\n        auto const idx = i - diff;\n        auto const pre = onnxPadding[idx];\n        auto const post = onnxPadding[onnxPadding.size() / 2U + idx];\n        if (pre < 0 || post < 0)\n        {\n            return false;\n        }\n\n        start[i] = -pre;\n        totalPadding[i] = pre + post;\n    }\n    auto* startLayer\n        = N_CHECK(addConstant(ctx, start, ::ONNX_NAMESPACE::TensorProto::INT64, nvinfer1::Dims{1, {nbInputDims}}));\n    startTensor = N_CHECK(startLayer->getOutput(0));\n\n    auto* totalPaddingLayer = N_CHECK(\n        addConstant(ctx, totalPadding, ::ONNX_NAMESPACE::TensorProto::INT64, nvinfer1::Dims{1, {nbInputDims}}));\n    totalPaddingTensor = N_CHECK(totalPaddingLayer->getOutput(0));\n    return startTensor && totalPaddingTensor;\n}\n\nbool shiftIsAllZeros(ShapedWeights const& shift)\n{\n    // Check if all of the values in the shift tensor are zeros. Shift dtype is one of [INT8, UINT8, INT4, UINT4]\n    auto const* v = static_cast<int8_t const*>(shift.values);\n    size_t const count = shift.size_bytes();\n    auto allZeros = std::all_of(v, v + count, [](int8_t x) { return x == 0; });\n    return allZeros;\n}\n\nonnx2trt::ShapedWeights createZeroShifts(onnx2trt::ShapedWeights const& shiftInt, int32_t type, ImporterContext* ctx)\n{\n    if (!shiftIsAllZeros(shiftInt))\n    {\n        LOG_WARNING(\"TensorRT currenly supports only zero shifts values for QuatizeLinear/DequantizeLinear ops\");\n    }\n    auto shift = ctx->createNamedTempWeights(type, shiftInt.shape);\n    float* sh = static_cast<float*>(shift.values);\n    for (int i = 0, n = shift.count(); i < n; i++)\n    {\n        sh[i] = 0.0f;\n    }\n    return shift;\n}\n\nnvinfer1::ITensor* createZeroTensor(ImporterContext* ctx, nvinfer1::ITensor* data)\n{\n    auto shape = shapeOf(*data);\n    auto* zeros = N_CHECK(addConstantScalar(ctx, 0.0F, ::ONNX_NAMESPACE::TensorProto::FLOAT)->getOutput(0));\n    zeros = castHelper(ctx, zeros, data->getType());\n    return constantOfShape(ctx, zeros, &shape.tensor(ctx));\n}\n\nnvinfer1::ITensor* convertToScalar(ImporterContext* ctx, nvinfer1::ITensor* inpTensor)\n{\n    if (inpTensor->getDimensions().nbDims == 0)\n    {\n        return inpTensor;\n    }\n    auto const tensorVolume = volume(inpTensor->getDimensions());\n    if (tensorVolume != 1)\n    {\n        LOG_VERBOSE(\"Cannot convert tensor to scalar. Note: Tensor dimensions were: \"\n            << inpTensor->getDimensions() << \", with volume: \" << tensorVolume);\n        return nullptr;\n    }\n    nvinfer1::IShuffleLayer* reshape = N_CHECK(ctx->network()->addShuffle(*inpTensor));\n    reshape->setReshapeDimensions(nvinfer1::Dims{0});\n    ctx->registerLayer(reshape, \"ONNXTRT_convertToScalar\", nullptr);\n    // Do not need to call setZeroIsPlaceholder, since reshape dimensions are empty.\n    return N_CHECK(reshape->getOutput(0));\n}\n\nnvinfer1::ITensor& convertToTensor(TensorOrWeights& input, ImporterContext* ctx)\n{\n    if (input.is_tensor())\n    {\n        return input.tensor();\n    }\n    // Handle non-tensor indices input by adding a new constant layer to the network.\n    ShapedWeights& weights = input.weights();\n\n    auto const existingConstantLayer = ctx->getConstantLayer(weights.getName());\n    if (existingConstantLayer != nullptr)\n    {\n        return *N_CHECK(existingConstantLayer->getOutput(0));\n    }\n    auto* constantLayer = N_CHECK(ctx->network()->addConstant(weights.shape, weights));\n    // Register layer and constant name (if set) into RefitMap:\n    if (weights.getName())\n    {\n        ctx->registerLayer(constantLayer, weights.getName(), nullptr);\n        ctx->network()->setWeightsName(weights, weights.getName());\n    }\n\n    auto* output = N_CHECK(constantLayer->getOutput(0));\n\n    return *output;\n}\n\nnvinfer1::ITensor* convertToScalar(TensorOrWeights& input, ImporterContext* ctx)\n{\n    if (input.is_tensor())\n    {\n        return convertToScalar(ctx, &input.tensor());\n    }\n    ShapedWeights& weights = input.weights();\n    if (volume(weights.shape) != 1)\n    {\n        LOG_VERBOSE(\"Cannot convert weights to scalar. Note: Tensor dimensions were: \"\n            << weights.shape << \", with volume: \" << volume(weights.shape));\n        return nullptr;\n    }\n    auto* scalarLayer = N_CHECK(ctx->network()->addConstant(nvinfer1::Dims{0, {0}}, weights));\n    return N_CHECK(scalarLayer->getOutput(0));\n}\n\nnvinfer1::ITensor* convertScalarToVector(ImporterContext* ctx, nvinfer1::ITensor* input)\n{\n    if (input->getDimensions().nbDims != 0)\n    {\n        return input;\n    }\n    std::vector<int32_t> axes{0};\n    return unsqueezeTensor(ctx, *input, axes);\n}\n\nint divCeil(int n, int d)\n{\n    return (n - 1) / d + 1;\n}\n\nstd::string getTrtDtypeName(nvinfer1::DataType TrtDtype)\n{\n    switch (TrtDtype)\n    {\n    case nvinfer1::DataType::kFLOAT: return \"FLOAT\";\n    case nvinfer1::DataType::kHALF: return \"HALF\";\n    case nvinfer1::DataType::kINT8: return \"INT8\";\n    case nvinfer1::DataType::kINT32: return \"INT32\";\n    case nvinfer1::DataType::kBOOL: return \"BOOL\";\n    case nvinfer1::DataType::kUINT8: return \"UINT8\";\n    case nvinfer1::DataType::kFP8: return \"FP8\";\n    case nvinfer1::DataType::kBF16: return \"BF16\";\n    case nvinfer1::DataType::kINT64: return \"INT64\";\n    case nvinfer1::DataType::kINT4: return \"INT4\";\n    default: return \"<UNKNOWN>\";\n    }\n}\n\nstd::string getElementWiseOpName(nvinfer1::ElementWiseOperation op)\n{\n    switch (op)\n    {\n    case nvinfer1::ElementWiseOperation::kSUM: return \"SUM\";\n    case nvinfer1::ElementWiseOperation::kPROD: return \"PROD\";\n    case nvinfer1::ElementWiseOperation::kMAX: return \"MAX\";\n    case nvinfer1::ElementWiseOperation::kMIN: return \"MIN\";\n    case nvinfer1::ElementWiseOperation::kSUB: return \"SUB\";\n    case nvinfer1::ElementWiseOperation::kDIV: return \"DIV\";\n    case nvinfer1::ElementWiseOperation::kPOW: return \"POW\";\n    case nvinfer1::ElementWiseOperation::kFLOOR_DIV: return \"FLOOR_DIV\";\n    case nvinfer1::ElementWiseOperation::kAND: return \"AND\";\n    case nvinfer1::ElementWiseOperation::kOR: return \"OR\";\n    case nvinfer1::ElementWiseOperation::kXOR: return \"XOR\";\n    case nvinfer1::ElementWiseOperation::kEQUAL: return \"EQUAL\";\n    case nvinfer1::ElementWiseOperation::kGREATER: return \"GREATER\";\n    case nvinfer1::ElementWiseOperation::kLESS: return \"LESS\";\n    default: return \"<UNKNOWN>\";\n    }\n}\n\nvoid elementwiseCheck(std::vector<TensorOrWeights> const& inputs, const nvinfer1::ElementWiseOperation op,\n    ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx)\n{\n    switch (op)\n    {\n    // These operations only support boolean inputs\n    case nvinfer1::ElementWiseOperation::kAND:\n    case nvinfer1::ElementWiseOperation::kOR:\n    case nvinfer1::ElementWiseOperation::kXOR:\n        ONNXTRT_CHECK_NODE(\n            std::all_of(inputs.begin(), inputs.end(), [](TensorOrWeights const& input) { return input.isBool(); }),\n            \"Elementwise layer only supports operator \" + getElementWiseOpName(op)\n                + \" and the given inputs with type BOOL.\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        break;\n    // These operations do not support boolean types\n    case nvinfer1::ElementWiseOperation::kDIV:\n    case nvinfer1::ElementWiseOperation::kFLOOR_DIV:\n    case nvinfer1::ElementWiseOperation::kGREATER:\n    case nvinfer1::ElementWiseOperation::kLESS:\n    case nvinfer1::ElementWiseOperation::kMAX:\n    case nvinfer1::ElementWiseOperation::kMIN:\n    case nvinfer1::ElementWiseOperation::kPROD:\n    case nvinfer1::ElementWiseOperation::kSUB:\n    case nvinfer1::ElementWiseOperation::kSUM:\n        ONNXTRT_CHECK_NODE(\n            !std::any_of(inputs.begin(), inputs.end(), [](TensorOrWeights const& input) { return input.isBool(); }),\n            \"Elementwise layer does not support operator \" + getElementWiseOpName(op)\n                + \" and the given inputs with type BOOL.\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        break;\n    // Pow does not support bool or integer types\n    case nvinfer1::ElementWiseOperation::kPOW:\n        ONNXTRT_CHECK_NODE(\n            !std::any_of(inputs.begin(), inputs.end(),\n                [](TensorOrWeights const& input) { return input.isBool() || input.isInt32() || input.isInt64(); }),\n            \"Elementwise layer does not support operator POW with boolean or integer types.\", node, nodeIdx,\n            ErrorCode::kUNSUPPORTED_NODE);\n        break;\n    // Equal supports all types.\n    case nvinfer1::ElementWiseOperation::kEQUAL: break;\n    }\n}\n\nNodeOutputs elementwiseHelper(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights> const& inputs, nvinfer1::ElementWiseOperation binary_op)\n{\n    ONNXTRT_CHECK_NODE((!inputs.empty()), \"Inputs vector is empty.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    std::vector<nvinfer1::ITensor*> inputTensors;\n    int maxNbDims = -1;\n    for (auto input : inputs)\n    {\n        maxNbDims = std::max(maxNbDims, input.shape().nbDims);\n    }\n\n    for (auto input : inputs)\n    {\n        auto* tensor_ptr = &convertToTensor(input, ctx);\n\n        // Broadcast all input tensors to size of maxNbDims\n        broadcastTensor(ctx, tensor_ptr, maxNbDims);\n        ONNXTRT_CHECK_NODE(tensor_ptr->getDimensions().nbDims == maxNbDims,\n            \"The number of dimensions should remain the same adding inputs: \" << tensor_ptr->getDimensions().nbDims\n                                                                              << \" != \" << maxNbDims << \".\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        inputTensors.push_back(tensor_ptr);\n    }\n    elementwiseCheck(inputs, binary_op, node, nodeIdx);\n\n    // Use the first tensor input as the base for the elementwise operation\n    nvinfer1::ITensor* combined = inputTensors.at(0);\n    if (inputTensors.size() == 1)\n    {\n        // Note: Single input must be wrapped in identity to avoid messing up network outputs\n        return {{identity(ctx, combined)}};\n    }\n    for (size_t i = 1; i < inputTensors.size(); ++i)\n    {\n        nvinfer1::ITensor* tensor = inputTensors.at(i);\n        ONNXTRT_CHECK_NODE((tensor->getDimensions().nbDims == combined->getDimensions().nbDims),\n            \"The number of dimensions should remain the same adding inputs: \"\n                << tensor->getDimensions().nbDims << \" != \" << combined->getDimensions().nbDims << \".\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        auto* layer = N_CHECK(ctx->network()->addElementWise(*combined, *tensor, binary_op));\n        ctx->registerLayer(layer, node);\n        combined = N_CHECK(layer->getOutput(0));\n    }\n    return {{combined}};\n}\n\nnvinfer1::ITensor* flattenTensor(\n    ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, nvinfer1::ITensor& tensor, int axis, bool regLayer)\n{\n    auto const dims = shapeOf(tensor);\n    auto const d0 = product(ctx, dims, 0, axis, 1);\n    auto const d1 = product(ctx, dims, axis, dims.size(), 1);\n\n    // ShuffleLayer here interprets dim extent 0 as empty dim to support empty tensor\n    nvinfer1::IShuffleLayer* flattenLayer\n        = N_CHECK(addShuffle(ctx, tensor, concat(ctx, d0, d1), /*zeroIsPlaceholder=*/false));\n    if (regLayer)\n    {\n        ctx->registerLayer(flattenLayer, node);\n    }\n    else\n    {\n        ctx->registerLayer(flattenLayer, \"ONNXTRT_flattenTensor\", nullptr);\n    }\n    return N_CHECK(flattenLayer->getOutput(0));\n}\n\nnvinfer1::ITensor* extractDimension(ImporterContext* ctx, nvinfer1::ITensor* shapeTensor, int dim, nvinfer1::Dims shape)\n{\n    // Comparing with gather, slice is more flexible. It does not need to convert dim into a constant.\n    // It is important for refit as when add an additional constant, this gather may not be optimized out.\n    auto* slice = N_CHECK(ctx->network()->addSlice(\n        *shapeTensor, nvinfer1::Dims{1, {dim}}, nvinfer1::Dims{1, {1}}, nvinfer1::Dims{1, {1}}));\n    ctx->registerLayer(slice, \"ONNXTRT_extractDimension\", nullptr);\n    if (shape != nvinfer1::Dims{1, {1}})\n    {\n        auto* reshape = N_CHECK(ctx->network()->addShuffle(*slice->getOutput(0)));\n        reshape->setReshapeDimensions(shape);\n        ctx->registerLayer(reshape, \"ONNXTRT_extractDimensionReshape\", nullptr);\n        return N_CHECK(reshape->getOutput(0));\n    }\n    return N_CHECK(slice->getOutput(0));\n}\n\n// Helper function to generate padding values for convTranspose\nvoid generatePadding(nvinfer1::Dims inputShape, nvinfer1::Dims outputShape, nvinfer1::Dims kernelSize,\n    nvinfer1::Dims strides, nvinfer1::Dims dilations, int const nbSpatialDims, nvinfer1::Dims& begPadding,\n    nvinfer1::Dims& endPadding, nvinfer1::Dims& outputPadding, nvinfer1::PaddingMode paddingMode)\n{\n    nvinfer1::Dims totalPadding{nbSpatialDims, {}};\n    // Pre and post padding calculated as per https://github.com/onnx/onnx/blob/master/docs/Operators.md#ConvTranspose\n    // Note that output shape is inconsistent in the spec - can either be in full dimensions form (i.e. NCHW) or just\n    // spatial dimensions form (i.e. HW). Calculate potential offset here.\n    auto const outputOffset = outputShape.nbDims - nbSpatialDims;\n    for (int32_t i = 0; i < nbSpatialDims; i++)\n    {\n        totalPadding.d[i] = strides.d[i] * (inputShape.d[2 + i] - 1) + outputPadding.d[i]\n            + ((kernelSize.d[i] - 1) * dilations.d[i] + 1) - outputShape.d[outputOffset + i];\n        // Same upper is calculated differently\n        if (paddingMode != nvinfer1::PaddingMode::kSAME_UPPER)\n        {\n            begPadding.d[i] = totalPadding.d[i] / 2;\n            endPadding.d[i] = totalPadding.d[i] - (totalPadding.d[i] / 2);\n        }\n        else\n        {\n            begPadding.d[i] = totalPadding.d[i] - (totalPadding.d[i] / 2);\n            endPadding.d[i] = (totalPadding.d[i] / 2);\n        }\n    }\n}\n\nfloat getActivationDefaultAlpha(nvinfer1::ActivationType type)\n{\n    switch (type)\n    {\n    case nvinfer1::ActivationType::kCLIP: return 0.f;\n    case nvinfer1::ActivationType::kELU: return 1.0f;\n    case nvinfer1::ActivationType::kGELU_ERF: return 0.f;\n    case nvinfer1::ActivationType::kGELU_TANH: return 0.f;\n    case nvinfer1::ActivationType::kHARD_SIGMOID: return 0.2f;\n    case nvinfer1::ActivationType::kLEAKY_RELU: return 0.01f;\n    case nvinfer1::ActivationType::kRELU: return 0.f;\n    case nvinfer1::ActivationType::kSCALED_TANH: return 1.0f;\n    case nvinfer1::ActivationType::kSELU: return 1.67326319217681884765625f;\n    case nvinfer1::ActivationType::kSIGMOID: return 0.f;\n    case nvinfer1::ActivationType::kSOFTPLUS: return 0.f;\n    case nvinfer1::ActivationType::kSOFTSIGN: return 0.f;\n    case nvinfer1::ActivationType::kTANH: return 0.f;\n    case nvinfer1::ActivationType::kTHRESHOLDED_RELU: return 1.0f;\n    }\n    throw std::runtime_error{\"Unrecognized activation type\"};\n}\n\nfloat getActivationDefaultBeta(nvinfer1::ActivationType type)\n{\n    switch (type)\n    {\n    case nvinfer1::ActivationType::kCLIP: return 0.f;\n    case nvinfer1::ActivationType::kELU: return 0.f;\n    case nvinfer1::ActivationType::kGELU_ERF: return 0.f;\n    case nvinfer1::ActivationType::kGELU_TANH: return 0.f;\n    case nvinfer1::ActivationType::kHARD_SIGMOID: return 0.5f;\n    case nvinfer1::ActivationType::kLEAKY_RELU: return 0.f;\n    case nvinfer1::ActivationType::kRELU: return 0.f;\n    case nvinfer1::ActivationType::kSCALED_TANH: return 1.0f;\n    case nvinfer1::ActivationType::kSELU: return 1.05070102214813232421875f;\n    case nvinfer1::ActivationType::kSIGMOID: return 0.f;\n    case nvinfer1::ActivationType::kSOFTPLUS: return 0.f;\n    case nvinfer1::ActivationType::kSOFTSIGN: return 0.f;\n    case nvinfer1::ActivationType::kTANH: return 0.f;\n    case nvinfer1::ActivationType::kTHRESHOLDED_RELU: return 0.f;\n    }\n    throw std::runtime_error{\"Unrecognized activation type\"};\n}\n\nnvinfer1::ITensor* getAxisLength(ImporterContext* ctx, nvinfer1::ITensor* inpTensor, int32_t axis, nvinfer1::Dims shape)\n{\n    // Let TRT handle the shape tensor optimization.\n    auto* shapeLayer = N_CHECK(ctx->network()->addShape(*inpTensor));\n    nvinfer1::ITensor* inpShape = N_CHECK(shapeLayer->getOutput(0));\n    // TRT-22536 - remove the cast and fix clients of getAxisLength to use 64-bit lengths.\n    auto* castLayer = N_CHECK(ctx->network()->addCast(*inpShape, nvinfer1::DataType::kINT32));\n    inpShape = N_CHECK(castLayer->getOutput(0));\n    return extractDimension(ctx, inpShape, axis, shape);\n}\n\nnvinfer1::ITensor* getAxisLengthInt64(\n    ImporterContext* ctx, nvinfer1::ITensor* inpTensor, int axis, nvinfer1::Dims shape)\n{\n    auto* shapeLayer = N_CHECK(ctx->network()->addShape(*inpTensor));\n    nvinfer1::ITensor* inpShape = N_CHECK(shapeLayer->getOutput(0));\n    return extractDimension(ctx, inpShape, axis, shape);\n}\n\nnvinfer1::ITensor* getElementWiseResult(\n    ImporterContext* ctx, nvinfer1::ITensor& lhs, nvinfer1::ITensor& rhs, nvinfer1::ElementWiseOperation op)\n{\n    auto* elemLayer = N_CHECK(ctx->network()->addElementWise(lhs, rhs, op));\n    return N_CHECK(elemLayer->getOutput(0));\n}\n\nnvinfer1::ITensor* getUnaryResult(ImporterContext* ctx, nvinfer1::ITensor& input, nvinfer1::UnaryOperation op)\n{\n    auto* unaryLayer = N_CHECK(ctx->network()->addUnary(input, op));\n    return N_CHECK(unaryLayer->getOutput(0));\n}\n\nvoid getKernelParams(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, nvinfer1::Dims* kernelSize,\n    nvinfer1::Dims* strides, nvinfer1::Dims* begPadding, nvinfer1::Dims* endPadding, nvinfer1::PaddingMode& paddingMode,\n    bool& countExcludePadding, nvinfer1::Dims* dilations, nvinfer1::Dims* outputPadding, bool const poolingCeilMode)\n{\n    int32_t const nbSpatialDims = kernelSize->nbDims;\n    OnnxAttrs attrs(node, ctx);\n    if (attrs.count(\"kernel_shape\"))\n    {\n        auto const* onnxKernelSize = attrs.at(\"kernel_shape\");\n        setAttr(kernelSize, onnxKernelSize, nbSpatialDims, 1);\n    }\n    if (attrs.count(\"strides\"))\n    {\n        auto const* onnxStrides = attrs.at(\"strides\");\n        setAttr(strides, onnxStrides, nbSpatialDims, 1);\n    }\n    if (dilations && attrs.count(\"dilations\"))\n    {\n        auto const* onnxDilations = attrs.at(\"dilations\");\n        setAttr(dilations, onnxDilations, nbSpatialDims, 1);\n    }\n    if (attrs.count(\"count_include_pad\"))\n    {\n        auto const* includePad = attrs.at(\"count_include_pad\");\n        int32_t val = includePad->i();\n        val == 1 ? countExcludePadding = false : countExcludePadding = true;\n    }\n    // For ConvTranspose Layer\n    if (attrs.count(\"output_padding\"))\n    {\n        auto const* onnxOutputPadding = attrs.at(\"output_padding\");\n        setAttr(outputPadding, onnxOutputPadding, nbSpatialDims, 0);\n    }\n\n    paddingMode\n        = poolingCeilMode ? nvinfer1::PaddingMode::kEXPLICIT_ROUND_UP : nvinfer1::PaddingMode::kEXPLICIT_ROUND_DOWN;\n    auto onnxAutoPad = attrs.get(\"auto_pad\", std::string(\"NOTSET\"));\n    if (onnxAutoPad != \"SAME_LOWER\" && onnxAutoPad != \"SAME_UPPER\")\n    {\n        if (attrs.count(\"pads\"))\n        {\n            auto onnxPadding = attrs.get<std::vector<int32_t>>(\"pads\");\n            int32_t ndim = onnxPadding.size() / 2;\n            for (int32_t i = 0; i < nbSpatialDims; ++i)\n            {\n                if (i < ndim)\n                {\n                    begPadding->d[i] = onnxPadding.at(i);\n                    endPadding->d[i] = onnxPadding.at(i + ndim);\n                }\n                else\n                {\n                    begPadding->d[i] = 0;\n                    endPadding->d[i] = 0;\n                }\n            }\n        }\n        if (onnxAutoPad == \"EXPLICIT_ROUND_UP\")\n        {\n            paddingMode = nvinfer1::PaddingMode::kEXPLICIT_ROUND_UP;\n        }\n    }\n    else\n    {\n        // If auto_pad is SAME_LOWER or SAME_UPPER, input padding should be calculated\n        // \"pads\" attribute should not be specified\n        ONNXTRT_CHECK(!attrs.count(\"pads\")\n                && \"Pads attribute should not be specified with SAME_LOWER or SAME_UPPER auto padding!\",\n            ErrorCode::kINVALID_NODE);\n        // Note: ONNX is always NCHW ordering\n        if (onnxAutoPad == \"SAME_LOWER\")\n        {\n            paddingMode = nvinfer1::PaddingMode::kSAME_LOWER;\n        }\n        else if (onnxAutoPad == \"SAME_UPPER\")\n        {\n            paddingMode = nvinfer1::PaddingMode::kSAME_UPPER;\n        }\n        else\n        {\n            ONNXTRT_THROW(MAKE_ERROR(\"invalid autopad attribute was set!\", ErrorCode::kINVALID_NODE));\n        }\n    }\n}\n\nfloat getSingleValueAsFloat(ShapedWeights const& weights)\n{\n    assert(weights.count() == 1 && \"Expected weights to contain only a single value\");\n    switch (weights.type)\n    {\n    case ::ONNX_NAMESPACE::TensorProto::FLOAT: return static_cast<float const*>(weights.values)[0];\n    case ::ONNX_NAMESPACE::TensorProto::FLOAT16:\n        return static_cast<float>(static_cast<half_float::half const*>(weights.values)[0]);\n    case ::ONNX_NAMESPACE::TensorProto::BFLOAT16:\n        return static_cast<float>(static_cast<BFloat16 const*>(weights.values)[0]);\n    default: assert(false && \"Unsupported type!\"); return 0.F;\n    }\n    return 0.F;\n}\n\nnvinfer1::ITensor* globalPoolingHelper(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node,\n    nvinfer1::ITensor& tensor, nvinfer1::ReduceOperation op)\n{\n    nvinfer1::Dims dims = tensor.getDimensions();\n    // Generate a bitmask of all 1s except the last 2 bits (N and C axes)\n    uint32_t reduceAxes = ((1 << dims.nbDims) - 1) & ~0b11;\n    auto* layer = N_CHECK(ctx->network()->addReduce(tensor, op, reduceAxes, /*keepDimensions=*/true));\n    ctx->registerLayer(layer, node);\n    return N_CHECK(layer->getOutput(0));\n}\n\nNodeOutputs greaterLessOrEqual(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx,\n    nvinfer1::ITensor* inputA, nvinfer1::ITensor* inputB, bool greater)\n{\n    nvinfer1::ElementWiseOperation op\n        = greater ? nvinfer1::ElementWiseOperation::kGREATER : nvinfer1::ElementWiseOperation::kLESS;\n    std::vector<TensorOrWeights> firstOpResults = elementwiseHelper(ctx, node, nodeIdx, {inputA, inputB}, op);\n    std::vector<TensorOrWeights> equalsResult\n        = elementwiseHelper(ctx, node, nodeIdx, {inputA, inputB}, nvinfer1::ElementWiseOperation::kEQUAL);\n    return elementwiseHelper(\n        ctx, node, nodeIdx, {firstOpResults.at(0), equalsResult.at(0)}, nvinfer1::ElementWiseOperation::kOR);\n}\n\nnvinfer1::IPluginCreatorInterface* importPluginCreator(ImporterContext* ctx, std::string const& pluginName,\n    std::string const& pluginVersion, std::string const& pluginNamespace)\n{\n    nvinfer1::IPluginCreatorInterface* creator = nullptr;\n\n#if ENABLE_STD_PLUGIN\n    auto& pluginRegistry = ctx->network()->getBuilder().getPluginRegistry();\n    creator = pluginRegistry.getCreator(pluginName.c_str(), pluginVersion.c_str(), pluginNamespace.c_str());\n#endif // ENABLE_STD_PLUGIN\n\n    // Do not perform a N_CHECK here as a plugin not being found is a valid case. It is up to the callers to handle the\n    // nullptr correctly.\n    return creator;\n}\n\nstd::unique_ptr<nvinfer1::IPluginV2, PluginDeleter> createPlugin(std::string const& name,\n    std::string const& /* pluginNamespace */, nvinfer1::IPluginCreator* pluginCreator,\n    std::vector<nvinfer1::PluginField> const& pluginFields)\n{\n    if (!pluginCreator)\n    {\n        return nullptr;\n    }\n\n    nvinfer1::PluginFieldCollection fc;\n    fc.nbFields = pluginFields.size();\n    fc.fields = pluginFields.data();\n\n    return std::unique_ptr<nvinfer1::IPluginV2, PluginDeleter>{pluginCreator->createPlugin(name.c_str(), &fc)};\n}\n\nnamespace\n{\nconstexpr char const* kV1_CREATOR_IFACE_KIND = \"PLUGIN CREATOR_V1\";\nconstexpr char const* kV3_CREATOR_ONE_IFACE_KIND = \"PLUGIN CREATOR_V3ONE\";\nconstexpr char const* kV3_CREATOR_QUICK_IFACE_KIND = \"PLUGIN CREATOR_V3QUICK\";\n\nbool isKind(nvinfer1::InterfaceInfo const& info, char const* kind)\n{\n    ONNXTRT_CHECK(\n        info.kind != nullptr && \"Invalid plugin creator interface with NULL kind.\", ErrorCode::kUNSUPPORTED_NODE);\n    return std::strcmp(info.kind, kind) == 0;\n}\n\n} // namespace\n\nCreatorVersion getPluginCreatorVersion(nvinfer1::IPluginCreatorInterface const* pluginCreator)\n{\n    ONNXTRT_CHECK(pluginCreator != nullptr && \"Null plugin creator.\", ErrorCode::kINTERNAL_ERROR);\n    auto const ifaceInfo = pluginCreator->getInterfaceInfo();\n    if (isKind(ifaceInfo, kV1_CREATOR_IFACE_KIND))\n    {\n        return CreatorVersion::kV1;\n    }\n    if (isKind(ifaceInfo, kV3_CREATOR_ONE_IFACE_KIND))\n    {\n        return CreatorVersion::kV3ONE;\n    }\n    if (isKind(ifaceInfo, kV3_CREATOR_QUICK_IFACE_KIND))\n    {\n        return CreatorVersion::kV3QUICK;\n    }\n    ONNXTRT_CHECK(false && \"Unknown plugin creator version.\", ErrorCode::kINTERNAL_ERROR);\n}\n\nstd::unique_ptr<nvinfer1::IPluginV3> createPlugin(std::string const& name, std::string const& pluginNamespace,\n    nvinfer1::IPluginCreatorInterface* pluginCreator, std::vector<nvinfer1::PluginField> const& pluginFields)\n{\n    if (!pluginCreator)\n    {\n        return nullptr;\n    }\n\n    auto const creatorVersion = getPluginCreatorVersion(pluginCreator);\n\n    ONNXTRT_CHECK((creatorVersion == CreatorVersion::kV3ONE || creatorVersion == CreatorVersion::kV3QUICK)\n            && \"Only IPluginCreatorV3One and IPluginCreatorV3Quick are supported for V3 plugin imports.\",\n        ErrorCode::kUNSUPPORTED_NODE);\n\n    nvinfer1::PluginFieldCollection fc;\n    fc.nbFields = pluginFields.size();\n    fc.fields = pluginFields.data();\n\n    if (creatorVersion == CreatorVersion::kV3ONE)\n    {\n        return std::unique_ptr<nvinfer1::IPluginV3>{\n            static_cast<nvinfer1::IPluginCreatorV3One*>(pluginCreator)\n                ->createPlugin(name.c_str(), &fc, nvinfer1::TensorRTPhase::kBUILD)};\n    }\n    else if (creatorVersion == CreatorVersion::kV3QUICK)\n    {\n        return std::unique_ptr<nvinfer1::IPluginV3>{\n            static_cast<nvinfer1::IPluginCreatorV3Quick*>(pluginCreator)\n                ->createPlugin(name.c_str(), pluginNamespace.c_str(), &fc, nvinfer1::TensorRTPhase::kBUILD)};\n    }\n    ONNXTRT_CHECK(false && \"Found invalid creator version when creating a V3 plugin.\", ErrorCode::kINTERNAL_ERROR);\n}\n\nNodeOutputs staticSliceImporter(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights>& inputs, nvinfer1::ITensor& data)\n{\n\n    auto const nbInputs = inputs.size();\n    nvinfer1::Dims inputDims = data.getDimensions();\n    auto const nbDims = inputDims.nbDims;\n\n    // Create start, sizes, and steps with default values\n    nvinfer1::Dims starts{nbDims, {}};\n    nvinfer1::Dims sizes{nbDims, {}};\n    nvinfer1::Dims steps{nbDims, {}};\n\n    for (int32_t i = 0; i < nbDims; i++)\n    {\n        starts.d[i] = 0;\n        sizes.d[i] = inputDims.d[i];\n        steps.d[i] = 1;\n    }\n\n    // Default axes / steps values\n    std::vector<int32_t> defaultAxes(nbDims);\n    std::iota(defaultAxes.begin(), defaultAxes.end(), 0);\n    std::vector<int32_t> defaultSteps(nbDims, 1);\n\n    // Get int32 pointer representation of ONNX provided values\n    int32_t* startVals = static_cast<int32_t*>(inputs.at(1).weights().values);\n    int32_t* endVals = static_cast<int32_t*>(inputs.at(2).weights().values);\n    int32_t* axesVals = nbInputs > 3 ? static_cast<int32_t*>(inputs.at(3).weights().values) : defaultAxes.data();\n    int32_t* stepVals = nbInputs > 4 ? static_cast<int32_t*>(inputs.at(4).weights().values) : defaultSteps.data();\n\n    // Handle non-standard values for slice\n    // Start values must in range of [0, dims.d[i]] for + steps, [0, dims.d[i] - 1] for - steps\n    auto convertStarts = [](int32_t start, int32_t upper, int32_t stepSign) {\n        int32_t newStarts = start < 0 ? start + upper : start;\n        newStarts = std::min(std::max(newStarts, 0), upper + stepSign);\n        return newStarts;\n    };\n\n    // End values must in range of [0, dims.d[i]] for + steps, [-1, dims.d[i]] for - steps\n    auto convertEnds = [](int32_t end, int32_t upper, int32_t stepSign) {\n        int32_t newEnds = end < 0 ? end + upper : end;\n        newEnds = std::min(std::max(newEnds, stepSign), upper);\n        return newEnds;\n    };\n    // Axes values must in range of [0, nbDims]\n    auto convertAxes = [&nbDims](int32_t axis) { return axis < 0 ? axis + nbDims : axis; };\n\n    // Since axes can be sparse, get the expected number of provided values\n    auto const nbValues = inputs.at(1).shape().d[0];\n\n    for (int32_t i = 0; i < nbValues; i++)\n    {\n        auto axesIndex = convertAxes(axesVals[i]);\n        // Modify starts\n        int32_t stepSign = stepVals[i] < 0 ? -1 : 0;\n        starts.d[axesIndex] = convertStarts(startVals[i], inputDims.d[axesIndex], stepSign);\n        // Modify ends\n        int32_t modifiedEnds = convertEnds(endVals[i], inputDims.d[axesIndex], stepSign);\n        steps.d[axesIndex] = stepVals[i];\n        // Perform ceil integer division of (ends - starts) / steps to compute sizes.\n        // Note ceil(x/y) = (x+y-1) / y for postive x & y, and ceil(x/y) = (x+y+1)/y for negative x&y\n        // Negative sizes indicates an empty slice, so clamp to 0\n        sizes.d[axesIndex] = std::max<int64_t>(\n            (modifiedEnds - starts.d[axesIndex] + steps.d[axesIndex] - (steps.d[axesIndex] > 0 ? 1 : -1))\n                / steps.d[axesIndex],\n            0);\n    }\n\n    auto* slice = N_CHECK(ctx->network()->addSlice(data, starts, sizes, steps));\n    ctx->registerLayer(slice, node);\n    auto output = N_CHECK(slice->getOutput(0));\n    return {{output}};\n}\n\nbool isDynamic(nvinfer1::Dims const& shape)\n{\n    return std::any_of(shape.d, shape.d + shape.nbDims, [](int dim) { return dim < 0; });\n}\n\nNodeOutputs modulatedDeformableConvPluginHelper(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node,\n    size_t const nodeIdx, std::vector<TensorOrWeights>& inputs)\n{\n    nvinfer1::ITensor* inputXPtr = &convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor* weightPtr = &convertToTensor(inputs.at(1), ctx);\n    nvinfer1::ITensor* offsetPtr = &convertToTensor(inputs.at(2), ctx);\n    int32_t nbDims = inputXPtr->getDimensions().nbDims;\n    ONNXTRT_CHECK_NODE(nbDims >= 3 && nbDims <= 4, \"TensorRT only supports DeformConv on 3D, or 4D tensors!\", node,\n        nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    bool const needToExpandDims = (nbDims == 3);\n    if (needToExpandDims)\n    {\n        // Expand spatial dims from 1D to 2D\n        std::vector<int32_t> const axes{3};\n        inputXPtr = unsqueezeTensor(ctx, *inputXPtr, axes);\n        weightPtr = unsqueezeTensor(ctx, *weightPtr, axes);\n        offsetPtr = unsqueezeTensor(ctx, *offsetPtr, axes);\n        ONNXTRT_CHECK(inputXPtr && \"Failed to unsqueeze the input tensor.\", ErrorCode::kUNSUPPORTED_NODE);\n        ONNXTRT_CHECK(weightPtr && \"Failed to unsqueeze the weight tensor.\", ErrorCode::kUNSUPPORTED_NODE);\n        ONNXTRT_CHECK(offsetPtr && \"Failed to unsqueeze the offset tensor.\", ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    // Parse attributes\n    OnnxAttrs attrs(node, ctx);\n    int32_t nbSpatialDims = nbDims - 2;\n    if (attrs.count(\"kernel_shape\"))\n    {\n        ONNXTRT_CHECK(nbSpatialDims == attrs.at(\"kernel_shape\")->ints().size()\n                && \"The attribute kernel_shape misaligns with the shape of the weight tensor.\",\n            ErrorCode::kUNSUPPORTED_NODE);\n        ONNXTRT_CHECK_NODE(((nbSpatialDims == 1 && needToExpandDims) || nbSpatialDims == 2),\n            \"The attribute kernel_shape misaligns with the shape of the input tensor.\", node, nodeIdx,\n            ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    nvinfer1::Dims dilations = makeDims(nbSpatialDims, /*Default value of dilations*/ 1);\n    if (attrs.count(\"dilations\"))\n    {\n        auto const* onnxDilations = attrs.at(\"dilations\");\n        setAttr(&dilations, onnxDilations, nbSpatialDims, 1);\n    }\n\n    nvinfer1::Dims kernelShape = makeDims(nbSpatialDims, 0);\n    if (attrs.count(\"kernel_shape\"))\n    {\n        auto const* onnxKernelShape = attrs.at(\"kernel_shape\");\n        setAttr(&kernelShape, onnxKernelShape, nbSpatialDims, 0);\n    }\n    else\n    {\n        auto weightTensorShape = inputs.at(1).shape();\n        for (int32_t i = 0; i < nbSpatialDims; i++)\n        {\n            kernelShape.d[i] = weightTensorShape.d[2 + i];\n        }\n    }\n\n    nvinfer1::Dims begPadding = makeDims(nbSpatialDims, /*Default value of pads*/ 0);\n    nvinfer1::Dims endPadding = makeDims(nbSpatialDims, /*Default value of pads*/ 0);\n\n    if (attrs.count(\"pads\"))\n    {\n        auto onnxPadding = attrs.get<std::vector<int32_t>>(\"pads\");\n        int32_t ndim = onnxPadding.size() / 2;\n        ONNXTRT_CHECK(ndim == nbSpatialDims\n                && \"The given pads attribute mismatch with the spatial dimensions of the weight tensor.\",\n            ErrorCode::kUNSUPPORTED_NODE);\n        for (int32_t i = 0; i < nbSpatialDims; ++i)\n        {\n            begPadding.d[i] = onnxPadding.at(i);\n            endPadding.d[i] = onnxPadding.at(i + ndim);\n        }\n    }\n\n    ONNXTRT_CHECK(begPadding == endPadding\n        && \"TensorRT only support the pads attribute of the DeformConv operator where the same number of pixels are added to the beginning and the end of the corresponding axis.\", ErrorCode::kUNSUPPORTED_NODE);\n\n    nvinfer1::Dims strides = makeDims(nbSpatialDims, /*Default value of strides*/ 1);\n    if (attrs.count(\"strides\"))\n    {\n        auto const* onnxStrides = attrs.at(\"strides\");\n        setAttr(&strides, onnxStrides, nbSpatialDims, 1);\n    }\n\n    int32_t group = attrs.get(\"group\", 1);\n    int32_t offset_group = attrs.get(\"offset_group\", 1);\n\n    // Populate instanceNormalization plugin properties.\n    std::string const pluginName = \"ModulatedDeformConv2d\";\n    std::string const pluginVersion = \"1\";\n    std::vector<nvinfer1::PluginField> f;\n\n    // Unsqueeze the list attributes if necessary\n    int32_t listAttrSize = nbSpatialDims == 1 ? 2 : nbSpatialDims;\n    std::vector<int32_t> dilationValues(listAttrSize, /*Default value of dilations*/ 1);\n    std::vector<int32_t> strideValues(listAttrSize, /*Default value of strides*/ 1);\n    std::vector<int32_t> paddingValues(listAttrSize, /*Default value of pads*/ 0);\n\n    for (int32_t i = 0; i < nbSpatialDims; i++)\n    {\n        dilationValues[i] = static_cast<int32_t>(dilations.d[i]);\n        strideValues[i] = static_cast<int32_t>(strides.d[i]);\n        paddingValues[i] = static_cast<int32_t>(begPadding.d[i]);\n    }\n\n    f.emplace_back(\"group\", &group, nvinfer1::PluginFieldType::kINT32, 1);\n    f.emplace_back(\"deformable_group\", &offset_group, nvinfer1::PluginFieldType::kINT32, 1);\n    f.emplace_back(\"stride\", strideValues.data(), nvinfer1::PluginFieldType::kINT32, listAttrSize);\n    f.emplace_back(\"padding\", paddingValues.data(), nvinfer1::PluginFieldType::kINT32, listAttrSize);\n    f.emplace_back(\"dilation\", dilationValues.data(), nvinfer1::PluginFieldType::kINT32, listAttrSize);\n\n    // Create plugin from registry\n    auto const plugin = createPlugin(pluginName, kTRT_STD_PLUGIN_NAMESPACE,\n        static_cast<nvinfer1::IPluginCreator*>(importPluginCreator(ctx, pluginName, pluginVersion)), f);\n\n    ONNXTRT_CHECK_NODE(plugin != nullptr, \"ModulatedDeformConv2d plugin was not found in the plugin registry!\", node,\n        nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    nvinfer1::ITensor* biasPtr = nullptr;\n    nvinfer1::ITensor* maskPtr = nullptr;\n\n    // Create the default mask input if not provided.\n    // The mask input is optional in ONNX but is required by the ModulatedDeformConv plugin.\n    if (inputs.size() > 4)\n    {\n        // Add the optional Mask tensor input.\n        maskPtr = &convertToTensor(inputs.at(4), ctx);\n        if (needToExpandDims)\n        {\n            // Expand spatial dims from 1D to 2D\n            std::vector<int32_t> const axes{3};\n            maskPtr = unsqueezeTensor(ctx, *maskPtr, axes);\n            ONNXTRT_CHECK(maskPtr && \"Failed to unsqueeze the mask tensor.\", ErrorCode::kUNSUPPORTED_NODE);\n        }\n    }\n    else\n    {\n        // Create the default mask input as a tensor of ones.\n        // The offset and mask inputs have the same shape.\n        nvinfer1::ITensor& maskShape = shapeOf(*offsetPtr).tensor(ctx);\n        ShapedWeights defaultMaskWeights\n            = ctx->createNamedTempWeights(inputs.at(0).getONNXDataType(), nvinfer1::Dims{1, {1}});\n\n        if (inputs.at(0).getDataType() == nvinfer1::DataType::kHALF)\n        {\n            static_cast<half_float::half*>(defaultMaskWeights.values)[0] = 1.0;\n            auto maskTensor = TensorOrWeights{defaultMaskWeights};\n            maskPtr = constantOfShape(ctx, &convertToTensor(maskTensor, ctx), &maskShape);\n        }\n        else\n        {\n            static_cast<float*>(defaultMaskWeights.values)[0] = 1.F;\n            auto maskTensor = TensorOrWeights{defaultMaskWeights};\n            maskPtr = constantOfShape(ctx, &convertToTensor(maskTensor, ctx), &maskShape);\n        }\n    }\n\n    if (inputs.size() > 3)\n    {\n        // Add the optional Bias tensor input.\n        biasPtr = &convertToTensor(inputs.at(3), ctx);\n    }\n\n    std::vector<nvinfer1::ITensor*> inputTensorsPtrs = {inputXPtr, offsetPtr, maskPtr, weightPtr};\n    if (biasPtr != nullptr)\n    {\n        inputTensorsPtrs.push_back(biasPtr);\n    }\n\n    auto* layer = N_CHECK(ctx->network()->addPluginV2(inputTensorsPtrs.data(), inputTensorsPtrs.size(), *plugin));\n    ctx->registerLayer(layer, node);\n    nvinfer1::ITensor* outputPtr = N_CHECK(layer->getOutput(0));\n\n    if (needToExpandDims)\n    {\n        // Un-expand spatial dims back to 1D\n        std::vector<int32_t> const axes{3};\n        outputPtr = squeezeTensor(ctx, *outputPtr, axes);\n        ONNXTRT_CHECK_NODE(outputPtr, \"Failed to squeeze tensor.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    return {{outputPtr}};\n}\n\nNodeOutputs instanceNormPluginHelper(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node,\n    size_t const nodeIdx, std::vector<TensorOrWeights>& inputs)\n{\n    // Scales and biases must be initializers\n    ONNXTRT_CHECK_NODE(inputs.at(1).is_weights(), \"The scale tensor is required to be an initializer.\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n    ONNXTRT_CHECK_NODE(inputs.at(2).is_weights(), \"The bias tensor is required to be an initializer.\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n    nvinfer1::ITensor* tensorPtr = &convertToTensor(inputs.at(0), ctx);\n    int32_t nbDims = tensorPtr->getDimensions().nbDims;\n    ONNXTRT_CHECK_NODE(nbDims >= 3 && nbDims <= 5,\n        \"TensorRT only supports InstanceNormalization on 3D, 4D, or 5D tensors!\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n\n    bool const needToExpandDims = (nbDims == 3);\n    if (needToExpandDims)\n    {\n        // Expand spatial dims from 1D to 2D\n        std::vector<int32_t> const axes{3};\n        tensorPtr = unsqueezeTensor(ctx, *tensorPtr, axes);\n        ONNXTRT_CHECK(tensorPtr && \"Failed to unsqueeze tensor.\", ErrorCode::kUNSUPPORTED_NODE);\n    }\n    auto scaleWeights = inputs.at(1).weights();\n    auto biasWeights = inputs.at(2).weights();\n    OnnxAttrs attrs(node, ctx);\n    float epsilon = attrs.get(\"epsilon\", 1e-5F);\n    int32_t const relu{0};  // the ONNX instance norm op does not use the relu parameter\n    float const alpha{0.F}; // the ONNX instance norm op does not use the alpha parameter\n\n    // Populate instanceNormalization plugin properties.\n    std::string const pluginName = \"InstanceNormalization_TRT\";\n    std::string const pluginVersion = \"3\";\n    std::vector<nvinfer1::PluginField> f;\n\n    // get the values of constant inputs and cast them to float32\n    float const* scaleValues = ctx->getWeightsContext().getFP32Values(scaleWeights);\n    float const* biasValues = ctx->getWeightsContext().getFP32Values(biasWeights);\n\n    f.emplace_back(\"epsilon\", &epsilon, nvinfer1::PluginFieldType::kFLOAT32, 1);\n    f.emplace_back(\"scales\", scaleValues, nvinfer1::PluginFieldType::kFLOAT32, scaleWeights.count());\n    f.emplace_back(\"bias\", biasValues, nvinfer1::PluginFieldType::kFLOAT32, biasWeights.count());\n    f.emplace_back(\"relu\", &relu, nvinfer1::PluginFieldType::kINT32, 1);\n    f.emplace_back(\"alpha\", &alpha, nvinfer1::PluginFieldType::kFLOAT32, 1);\n\n    // Create plugin from registry\n    auto const plugin = createPlugin(getNodeName(node), kTRT_STD_PLUGIN_NAMESPACE,\n        static_cast<nvinfer1::IPluginCreatorV3One*>(importPluginCreator(ctx, pluginName, pluginVersion)), f);\n\n    ONNXTRT_CHECK_NODE(plugin != nullptr, \"InstanceNormalization plugin was not found in the plugin registry!\", node,\n        nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    auto* layer = N_CHECK(ctx->network()->addPluginV3(&tensorPtr, 1, nullptr, 0, *plugin));\n    ctx->registerLayer(layer, node);\n    tensorPtr = N_CHECK(layer->getOutput(0));\n\n    if (needToExpandDims)\n    {\n        // Un-expand spatial dims back to 1D\n        std::vector<int32_t> const axes{3};\n        tensorPtr = squeezeTensor(ctx, *tensorPtr, axes);\n        ONNXTRT_CHECK_NODE(tensorPtr, \"Failed to squeeze tensor.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    return {{tensorPtr}};\n}\n\nnvinfer1::ITensor* iota(ImporterContext* ctx, ShapeTensor iotaDims, int32_t axis)\n{\n    std::vector<int32_t> deltaVals(iotaDims.size(), 0);\n    deltaVals[axis] = 1;\n    auto* iota\n        = N_CHECK(ctx->network()->addFill({0, {0}}, nvinfer1::FillOperation::kLINSPACE, nvinfer1::DataType::kINT32));\n    auto* alphaLayer = N_CHECK(addConstantScalar(ctx, static_cast<int32_t>(0), ::ONNX_NAMESPACE::TensorProto::INT32));\n    auto* alpha = N_CHECK(alphaLayer->getOutput(0));\n    auto* deltaLayer\n        = N_CHECK(addConstant(ctx, deltaVals, ::ONNX_NAMESPACE::TensorProto::INT32, {1, {iotaDims.size()}}));\n    auto* delta = N_CHECK(deltaLayer->getOutput(0));\n    iota->setInput(0, iotaDims.tensor(ctx));\n    iota->setInput(1, *alpha);\n    iota->setInput(2, *delta);\n    ctx->registerLayer(iota, \"ONNXTRT_iota\", nullptr);\n    return castHelper(ctx, N_CHECK(iota->getOutput(0)), nvinfer1::DataType::kINT64);\n}\n\nTensorOrWeights identity(ImporterContext* ctx, TensorOrWeights input)\n{\n    if (input.is_weights())\n    {\n        return input;\n    }\n    else\n    {\n        auto* layer = N_CHECK(ctx->network()->addIdentity(input.tensor()));\n        ctx->registerLayer(layer, \"ONNXTRT_identity\", nullptr);\n        return N_CHECK(layer->getOutput(0));\n    }\n}\n\nnvinfer1::Dims makeDims(int nbDims, int val)\n{\n    // Zero all the dimensions, so that unused dimensions are deterministic even if accidentally used.\n    nvinfer1::Dims dims{nbDims, {}};\n    std::fill_n(dims.d, nbDims, val);\n    return dims;\n}\n\nNodeOutputs normalizationHelper(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights>& inputs)\n{\n    auto* input = &convertToTensor(inputs.at(0), ctx);\n    auto* scale = &convertToTensor(inputs.at(1), ctx);\n    auto* bias = &convertToTensor(inputs.at(2), ctx);\n\n    OnnxAttrs attrs(node, ctx);\n    float epsilon = attrs.get(\"epsilon\", 1e-5f);\n    int32_t nbGroups = attrs.get(\"num_groups\", 1);\n\n    auto nbDims = input->getDimensions().nbDims;\n    ONNXTRT_CHECK_NODE(nbDims >= 3,\n        \"Input to normalization should be at least 3D, the actual number of dimensions is \" << nbDims << \".\", node,\n        nodeIdx, ErrorCode::kINVALID_NODE);\n\n    // Need to broadcast scale and bias to the input shape. Note that normal broadcasting rules cannot be applied\n    // as scale and bias are 1D and need to be broadcasted to shape [1, S, 1, 1, ...].\n    uint32_t axesMask{0};\n    std::vector<int32_t> unsqueezeAxes;\n\n    for (int32_t i = 0; i < nbDims; i++)\n    {\n        if (i == 1)\n        {\n            continue;\n        }\n        // Axes should correspond to the spatial dimensions\n        if (i >= 2)\n        {\n            axesMask |= 1 << i;\n        }\n        unsqueezeAxes.push_back(i);\n    }\n\n    scale = unsqueezeTensor(ctx, *scale, unsqueezeAxes);\n    bias = unsqueezeTensor(ctx, *bias, unsqueezeAxes);\n\n    auto* layer = N_CHECK(ctx->network()->addNormalization(*input, *scale, *bias, axesMask));\n    layer->setEpsilon(epsilon);\n    layer->setNbGroups(nbGroups);\n    ctx->registerLayer(layer, node);\n    auto* output = N_CHECK(layer->getOutput(0));\n    return {{output}};\n}\n\nvoid normalizeAxes(ShapeTensor& axes, int32_t const rank)\n{\n    ONNXTRT_CHECK(axes.allValuesKnown() && \"Axes should not contain unknown values.\", ErrorCode::kINTERNAL_ERROR);\n    std::vector<int64_t> newAxes;\n    newAxes.reserve(axes.size());\n    for (int64_t axis : axes)\n    {\n        ONNXTRT_CHECK((-rank <= axis && axis < rank) && \"Axis must be in the range of [-rank, rank-1].\",\n            ErrorCode::kINVALID_VALUE);\n        // \"Negative value means counting dimensions from the back.\"\n        if (axis < 0)\n        {\n            axis += rank;\n        }\n        newAxes.push_back(axis);\n    }\n    axes = ShapeTensor(1, std::move(newAxes));\n}\n\nnvinfer1::Dims insertDimension(nvinfer1::Dims const& dims, int const axis, int const value)\n{\n    if (axis >= nvinfer1::Dims::MAX_DIMS || dims.nbDims >= nvinfer1::Dims::MAX_DIMS)\n    {\n        throw std::invalid_argument(\"Cannot insert a dimension past Dims::MAX_DIMS!\");\n    }\n    nvinfer1::Dims newDims{};\n    newDims.nbDims = dims.nbDims + 1;\n    std::copy(dims.d, dims.d + axis, newDims.d);\n    newDims.d[axis] = value;\n    std::copy(dims.d + axis, dims.d + dims.nbDims, newDims.d + axis + 1);\n    return newDims;\n}\n\nstd::vector<float> parseLSTMActivationValues(std::vector<nvinfer1::ActivationType> const& activationTypes,\n    std::vector<float> const& activationValues, bool isAlpha)\n{\n    size_t actIndex{0};\n    std::vector<float> tmpActs{};\n    for (size_t i = 0; i < activationTypes.size(); ++i)\n    {\n        float defaultVal\n            = isAlpha ? getActivationDefaultAlpha(activationTypes[i]) : getActivationDefaultBeta(activationTypes[i]);\n        if (defaultVal == 0.f || actIndex == activationValues.size())\n        {\n            tmpActs.push_back(defaultVal);\n        }\n        else\n        {\n            tmpActs.push_back(activationValues[actIndex]);\n            actIndex++;\n        }\n    }\n    return tmpActs;\n}\n\nNodeOutputs poolingHelper(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights>& inputs, nvinfer1::PoolingType type)\n{\n    nvinfer1::ITensor* tensorPtr = &convertToTensor(inputs.at(0), ctx);\n    nvinfer1::Dims dims = tensorPtr->getDimensions();\n    bool needToExpandDims = (dims.nbDims == 3);\n    if (needToExpandDims)\n    {\n        // Expand spatial dims from 1D to 2D\n        std::vector<int32_t> axes{3};\n        tensorPtr = unsqueezeTensor(ctx, *tensorPtr, axes);\n        ONNXTRT_CHECK(tensorPtr && \"Failed to unsqueeze tensor.\", ErrorCode::kUNSUPPORTED_NODE);\n        dims = tensorPtr->getDimensions();\n    }\n\n    OnnxAttrs attrs(node, ctx);\n    int nbSpatialDims = attrs.at(\"kernel_shape\")->ints().size();\n    ONNXTRT_CHECK_NODE(((nbSpatialDims == 1 && needToExpandDims) || nbSpatialDims == 2 || nbSpatialDims == 3),\n        \"The attribute kernel_shape misaligns with the shape of the input tensor.\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n\n    nvinfer1::Dims kernelSize = makeDims(nbSpatialDims, 1);\n    nvinfer1::Dims strides = makeDims(nbSpatialDims, 1);\n    nvinfer1::Dims begPadding = makeDims(nbSpatialDims, 0);\n    nvinfer1::Dims endPadding = makeDims(nbSpatialDims, 0);\n    nvinfer1::PaddingMode paddingMode;\n\n    bool exclude_padding(true);\n\n    // Ceiling-mode output padding and dilations added in opset 10\n    bool ceilMode(false);\n    if (ctx->getOpsetVersion() >= 10)\n    {\n        ceilMode = static_cast<bool>(attrs.get<int>(\"ceil_mode\", 0));\n    }\n\n    getKernelParams(ctx, node, &kernelSize, &strides, &begPadding, &endPadding, paddingMode, exclude_padding, nullptr,\n        nullptr, ceilMode);\n\n    if (needToExpandDims)\n    {\n        kernelSize = insertDimension(kernelSize, nbSpatialDims, 1);\n        strides = insertDimension(strides, nbSpatialDims, 1);\n        begPadding = insertDimension(begPadding, nbSpatialDims, 0);\n        endPadding = insertDimension(endPadding, nbSpatialDims, 0);\n    }\n\n    nvinfer1::IPoolingLayer* poolingLayer = N_CHECK(ctx->network()->addPoolingNd(*tensorPtr, type, kernelSize));\n    poolingLayer->setStrideNd(strides);\n    // This member is ignored in maxpooling\n    poolingLayer->setAverageCountExcludesPadding(exclude_padding);\n    poolingLayer->setPaddingMode(paddingMode);\n    poolingLayer->setPrePadding(begPadding);\n    poolingLayer->setPostPadding(endPadding);\n\n    ctx->registerLayer(poolingLayer, node);\n    tensorPtr = N_CHECK(poolingLayer->getOutput(0));\n    dims = tensorPtr->getDimensions();\n    if (needToExpandDims)\n    {\n        // Un-expand spatial dims back to 1D\n        std::vector<int32_t> axes{3};\n        tensorPtr = squeezeTensor(ctx, *tensorPtr, axes);\n    }\n    return {{tensorPtr}};\n}\n\nbool IsReduceNoOp(\n    ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, std::vector<TensorOrWeights> const& inputs)\n{\n    OnnxAttrs attrs(node, ctx);\n    return (attrs.get(\"noop_with_empty_axes\", 0) == 1) && (!attrs.count(\"axes\")) && (inputs.size() == 1);\n}\n\nNodeOutputs reduceTensor(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx,\n    TensorOrWeights input, nvinfer1::ReduceOperation operation, TensorOrWeights inputAxes)\n{\n    // TensorRT does not support reduction on Bool or UINT8 tensors.\n    checkNotInvalidType(input, {\"BOOL\", \"UINT8\"}, node, nodeIdx);\n\n    OnnxAttrs attrs(node, ctx);\n    nvinfer1::ITensor& tensor = convertToTensor(input, ctx);\n    bool keepdims = attrs.get(\"keepdims\", 1);\n    int32_t ndim = tensor.getDimensions().nbDims;\n    std::vector<int32_t> axes;\n    if (attrs.count(\"axes\"))\n    {\n        axes = attrs.get<std::vector<int32_t>>(\"axes\");\n    }\n    else if (!inputAxes.isNullTensor())\n    {\n        ONNXTRT_CHECK_NODE(\n            inputAxes.is_weights(), \"Axis input must be an initializer!\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        weightsToVector<int32_t>(inputAxes.weights(), &axes);\n    }\n    // It's possible that the axes tensor, axes initializer, or axes attribute was empty. Handle such cases here.\n    if (axes.empty())\n    {\n        // Fast return path for no-op case.\n        if (attrs.get(\"noop_with_empty_axes\", 0) == 1)\n        {\n            TensorOrWeights output = identity(ctx, input);\n            return {{output}};\n        }\n        axes.resize(ndim);\n        std::iota(axes.begin(), axes.end(), 0);\n    }\n\n    uint32_t axisMask = 0;\n    for (int32_t axis : axes)\n    {\n        convertAxis(axis, ndim, node, nodeIdx);\n        axisMask |= 1 << axis;\n    }\n\n    auto* layer = N_CHECK(ctx->network()->addReduce(tensor, operation, axisMask, keepdims));\n    ctx->registerLayer(layer, node);\n    auto output = N_CHECK(layer->getOutput(0));\n    return {{output}};\n}\n\nnvinfer1::ITensor* reshapeTensor(ImporterContext* ctx, nvinfer1::ITensor& tensor, nvinfer1::Dims shape)\n{\n    if (shape == tensor.getDimensions())\n    {\n        return &tensor;\n    }\n    nvinfer1::IShuffleLayer* layer = N_CHECK(ctx->network()->addShuffle(tensor));\n    layer->setReshapeDimensions(shape);\n    layer->setZeroIsPlaceholder(false);\n    ctx->registerLayer(layer, \"ONNXTRT_reshapeTensor\", nullptr);\n    return N_CHECK(layer->getOutput(0));\n}\n\nNodeOutputs scaleHelper(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx,\n    nvinfer1::ITensor& tensor_, nvinfer1::ScaleMode mode, nvinfer1::Weights const& shift,\n    nvinfer1::Weights const& scale, nvinfer1::Weights const& power, char const* shiftName, char const* scaleName)\n{\n    nvinfer1::ITensor* tensorPtr = &tensor_;\n    const ShapeTensor origShape = shapeOf(*tensorPtr);\n\n    // TensorRT scale layers support 4D(NCHW) or 5D(NCDHW) input.\n    // For input other than 4D or 5D will be expanded or squeezed to 4D.\n    bool needToReshape = (origShape.size() != 4 && origShape.size() != 5);\n    if (needToReshape)\n    {\n        if (origShape.size() < 4)\n        {\n            std::vector<int> expandAxes(4 - origShape.size());\n            std::iota(expandAxes.begin(), expandAxes.end(), origShape.size());\n            tensorPtr = unsqueezeTensor(ctx, *tensorPtr, expandAxes);\n        }\n        else\n        {\n            // Collapse trailing dimensions if origShape.size() > 5\n            const ShapeTensor collapsedDim = product(ctx, origShape, 3, origShape.size(), 1);\n            const ShapeTensor collapsedShape = concat(ctx, gather(ctx, origShape, iotaShapeVector(3)), collapsedDim);\n            tensorPtr = &reshape(ctx, *tensorPtr, collapsedShape);\n        }\n    }\n\n    auto* layer = N_CHECK(ctx->network()->addScaleNd(*tensorPtr, mode, shift, scale, power, 1));\n    // Register layer name, and shift and scale weight names for the refit map.\n    ctx->registerLayer(layer, node);\n    ctx->network()->setWeightsName(shift, shiftName);\n    ctx->network()->setWeightsName(scale, scaleName);\n\n    tensorPtr = N_CHECK(layer->getOutput(0));\n\n    if (needToReshape)\n    {\n        tensorPtr = &reshape(ctx, *tensorPtr, origShape);\n    }\n    return {{tensorPtr}};\n}\n\nvoid setAttr(nvinfer1::Dims* trtAttr, ::ONNX_NAMESPACE::AttributeProto const* onnxAttr, int32_t nbSpatialDims,\n    int32_t defaultVal)\n{\n    assert(trtAttr->nbDims == nbSpatialDims);\n    int32_t ndim = onnxAttr->ints().size();\n    for (int32_t i = 0; i < nbSpatialDims; ++i)\n    {\n        if (i < ndim)\n        {\n            trtAttr->d[i] = onnxAttr->ints(i);\n        }\n        else\n        {\n            trtAttr->d[i] = defaultVal;\n        }\n    }\n}\n\nnvinfer1::ITensor* sliceAcrossAxis(\n    ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, nvinfer1::ITensor* data, int32_t const axis)\n{\n    ShapeTensor starts, sizes, strides;\n    ShapeTensor axisLength = ShapeTensor(*getAxisLengthInt64(ctx, data, axis, {1, {1}}));\n    int32_t const nbDims = data->getDimensions().nbDims;\n\n    std::vector<int64_t> values(nbDims, 0);\n    starts = ShapeTensor(1, std::move(values));\n    sizes = axis == 0 ? shapeVector(1) : ShapeTensor(*getAxisLengthInt64(ctx, data, 0, {1, {1}}));\n    strides = axis == 0 ? axisLength : shapeVector(1);\n\n    // On axis dimension, set strides = lengthOfDim and sizes = 1\n    for (int32_t i = 1; i < nbDims; i++)\n    {\n        if (i == axis)\n        {\n            strides = concat(ctx, strides, axisLength);\n            sizes = concat(ctx, sizes, shapeVector(1));\n        }\n        else\n        {\n            ShapeTensor currLength = ShapeTensor(*getAxisLengthInt64(ctx, data, i, {1, {1}}));\n            strides = concat(ctx, strides, shapeVector(1));\n            sizes = concat(ctx, sizes, currLength);\n        }\n    }\n    auto* sliceLayer = N_CHECK(addSlice(ctx, *data, starts, sizes, strides));\n    return N_CHECK(sliceLayer->getOutput(0));\n}\n\nnvinfer1::ITensor* squeezeTensor(ImporterContext* ctx, nvinfer1::ITensor& tensor, std::vector<int32_t> const& axes)\n{\n    auto* axesTensor\n        = N_CHECK(addConstant(ctx, axes, ::ONNX_NAMESPACE::TensorProto::INT32, {1, {static_cast<int64_t>(axes.size())}})\n                      ->getOutput(0));\n    auto* squeezeLayer = N_CHECK(ctx->network()->addSqueeze(tensor, *axesTensor));\n    auto* squeezedTensor = N_CHECK(squeezeLayer->getOutput(0));\n    LOG_VERBOSE(\"Original shape: \" << shapeOf(tensor) << \", squeezing to: \" << shapeOf(*squeezedTensor));\n    ctx->registerLayer(squeezeLayer, \"ONNXTRT_squeezeTensor\", nullptr);\n    return squeezedTensor;\n}\n\nnvinfer1::ITensor* transposeTensor(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node,\n    nvinfer1::ITensor& tensor, nvinfer1::Permutation const& perm)\n{\n    nvinfer1::IShuffleLayer* layer = N_CHECK(ctx->network()->addShuffle(tensor));\n    ctx->registerLayer(layer, node);\n    layer->setFirstTranspose(perm);\n    return N_CHECK(layer->getOutput(0));\n}\n\n::ONNX_NAMESPACE::TensorProto_DataType trtDataTypeToONNX(nvinfer1::DataType dt)\n{\n    switch (dt)\n    {\n    case nvinfer1::DataType::kFLOAT: return ::ONNX_NAMESPACE::TensorProto::FLOAT;\n    case nvinfer1::DataType::kHALF: return ::ONNX_NAMESPACE::TensorProto::FLOAT16;\n    case nvinfer1::DataType::kBF16: return ::ONNX_NAMESPACE::TensorProto::BFLOAT16;\n    case nvinfer1::DataType::kINT32: return ::ONNX_NAMESPACE::TensorProto::INT32;\n    case nvinfer1::DataType::kINT64: return ::ONNX_NAMESPACE::TensorProto::INT64;\n    case nvinfer1::DataType::kINT8: return ::ONNX_NAMESPACE::TensorProto::INT8;\n    case nvinfer1::DataType::kBOOL: return ::ONNX_NAMESPACE::TensorProto::BOOL;\n    case nvinfer1::DataType::kUINT8: return ::ONNX_NAMESPACE::TensorProto::UINT8;\n    case nvinfer1::DataType::kFP8: return ::ONNX_NAMESPACE::TensorProto::FLOAT8E4M3FN;\n    case nvinfer1::DataType::kINT4: return ::ONNX_NAMESPACE::TensorProto::INT4;\n    }\n    return ::ONNX_NAMESPACE::TensorProto_DataType_UNDEFINED;\n}\n\nstd::string getUnaryOpName(nvinfer1::UnaryOperation op)\n{\n    switch (op)\n    {\n    case nvinfer1::UnaryOperation::kEXP: return \"EXP\";\n    case nvinfer1::UnaryOperation::kLOG: return \"LOG\";\n    case nvinfer1::UnaryOperation::kSQRT: return \"SQRT\";\n    case nvinfer1::UnaryOperation::kRECIP: return \"RECIP\";\n    case nvinfer1::UnaryOperation::kABS: return \"ABS\";\n    case nvinfer1::UnaryOperation::kNEG: return \"NEG\";\n    case nvinfer1::UnaryOperation::kSIN: return \"SIN\";\n    case nvinfer1::UnaryOperation::kCOS: return \"COS\";\n    case nvinfer1::UnaryOperation::kTAN: return \"TAN\";\n    case nvinfer1::UnaryOperation::kSINH: return \"SINH\";\n    case nvinfer1::UnaryOperation::kCOSH: return \"COSH\";\n    case nvinfer1::UnaryOperation::kASIN: return \"ASIN\";\n    case nvinfer1::UnaryOperation::kACOS: return \"ACOS\";\n    case nvinfer1::UnaryOperation::kATAN: return \"ATAN\";\n    case nvinfer1::UnaryOperation::kASINH: return \"ASINH\";\n    case nvinfer1::UnaryOperation::kACOSH: return \"ACOSH\";\n    case nvinfer1::UnaryOperation::kATANH: return \"ATANH\";\n    case nvinfer1::UnaryOperation::kCEIL: return \"CEIL\";\n    case nvinfer1::UnaryOperation::kFLOOR: return \"FLOOR\";\n    case nvinfer1::UnaryOperation::kERF: return \"ERF\";\n    case nvinfer1::UnaryOperation::kNOT: return \"NOT\";\n    case nvinfer1::UnaryOperation::kSIGN: return \"SIGN\";\n    case nvinfer1::UnaryOperation::kROUND: return \"ROUND\";\n    case nvinfer1::UnaryOperation::kISINF: return \"ISINF\";\n    default: return \"<UNKNOWN>\";\n    }\n}\n\nNodeOutputs unaryHelper(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx,\n    TensorOrWeights& input, nvinfer1::UnaryOperation op)\n{\n    nvinfer1::ITensor* tensorPtr = &convertToTensor(input, ctx);\n    auto const inputType = tensorPtr->getType();\n\n    bool validUnaryType = true;\n    switch (op)\n    {\n    // TRT only supports BOOL types for the NOT operation\n    case nvinfer1::UnaryOperation::kNOT:\n    {\n        validUnaryType = inputType == nvinfer1::DataType::kBOOL;\n        break;\n    }\n    // ABS and SIGN supports everything except BOOL and UINT8.\n    case nvinfer1::UnaryOperation::kABS:\n    case nvinfer1::UnaryOperation::kSIGN:\n    {\n        validUnaryType = (inputType != nvinfer1::DataType::kBOOL && inputType != nvinfer1::DataType::kUINT8);\n        break;\n    }\n    case nvinfer1::UnaryOperation::kNEG:\n    {\n        // WAR: NEG can work with INT32 types via ElementWise Layer: (0 - x)\n        if (inputType == nvinfer1::DataType::kINT32)\n        {\n            // Calculate the rank of the input, and set all size to one and rely on broadcasting\n            auto* zeroLayer\n                = N_CHECK(addConstant(ctx, std::vector<int32_t>{0}, ::ONNX_NAMESPACE::TensorProto::INT32, {0, {1}}));\n            nvinfer1::ITensor* zeroTensor = N_CHECK(zeroLayer->getOutput(0));\n            broadcastTensors(ctx, zeroTensor, tensorPtr);\n            std::vector<TensorOrWeights> layerInputs = {zeroTensor, tensorPtr};\n            return elementwiseHelper(ctx, node, nodeIdx, layerInputs, nvinfer1::ElementWiseOperation::kSUB);\n        }\n        validUnaryType = (inputType != nvinfer1::DataType::kBOOL && inputType != nvinfer1::DataType::kUINT8);\n        break;\n    }\n    default:\n    {\n        // By default TRT does not support BOOL, INT32, INT64, and UINT8 types for Unary operations.\n        validUnaryType = (inputType != nvinfer1::DataType::kBOOL && inputType != nvinfer1::DataType::kINT32\n            && inputType != nvinfer1::DataType::kINT64 && inputType != nvinfer1::DataType::kUINT8);\n    }\n    }\n\n    ONNXTRT_CHECK_NODE(validUnaryType,\n        \"This version of TensorRT does not support the given operator \" + getUnaryOpName(op)\n            + \" with the given input data type \" + getTrtDtypeName(inputType) + \".\",\n        node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    nvinfer1::IUnaryLayer* layer = N_CHECK(ctx->network()->addUnary(*tensorPtr, op));\n    ctx->registerLayer(layer, node);\n    tensorPtr = N_CHECK(layer->getOutput(0));\n\n    return {{tensorPtr}};\n}\n\nNodeOutputs convMultiInput(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights>& inputs)\n{\n    ONNXTRT_CHECK(inputs.size() >= 2 && \"Convolution require at least 2 inputs.\", ErrorCode::kUNSUPPORTED_NODE);\n    nvinfer1::ITensor* input = &convertToTensor(inputs.at(0), ctx);\n    nvinfer1::Dims dims = input->getDimensions();\n    bool needToExpandDims = (dims.nbDims == 3);\n    if (needToExpandDims)\n    {\n        // Expand spatial dims from 1D to 2D\n        std::vector<int32_t> const axes{3};\n        input = unsqueezeTensor(ctx, *input, axes);\n        dims = input->getDimensions();\n    }\n    auto const nbSpatialDims = dims.nbDims - 2;\n\n    nvinfer1::Dims kernelDims;\n    kernelDims.nbDims = nbSpatialDims;\n\n    // Populate spatial dims from the shape of the convolution weights.\n    for (int32_t i = 1; i <= nbSpatialDims; ++i)\n    {\n        kernelDims.d[nbSpatialDims - i] = inputs.at(1).shape().d[inputs.at(1).shape().nbDims - i];\n    }\n\n    nvinfer1::Dims strides = makeDims(nbSpatialDims, 1);\n    nvinfer1::Dims begPadding = makeDims(nbSpatialDims, 0);\n    nvinfer1::Dims endPadding = makeDims(nbSpatialDims, 0);\n    nvinfer1::Dims dilations = makeDims(nbSpatialDims, 1);\n    nvinfer1::PaddingMode paddingMode;\n    bool excludePadding{false};\n    getKernelParams(\n        ctx, node, &kernelDims, &strides, &begPadding, &endPadding, paddingMode, excludePadding, &dilations);\n    auto const nChannel = dims.d[1];\n    auto const K = inputs.at(1).shape().d[0];\n    auto const C = inputs.at(1).shape().d[1];\n\n    auto kernelWeights = ShapedWeights::empty(::ONNX_NAMESPACE::TensorProto::FLOAT);\n    auto biasWeights = ShapedWeights::empty(::ONNX_NAMESPACE::TensorProto::FLOAT);\n\n    auto const checkSpatialDims = [&nbSpatialDims, &kernelDims](nvinfer1::Dims const& dims) {\n        // Check that the number of spatial dimensions and the kernel shape matches up.\n        if (nbSpatialDims != dims.nbDims - 2)\n        {\n            return false;\n        }\n\n        return std::equal(kernelDims.d, kernelDims.d + nbSpatialDims, dims.d + dims.nbDims - nbSpatialDims);\n    };\n\n    nvinfer1::ITensor* kernelTensor{nullptr};\n    nvinfer1::ITensor* biasTensor{nullptr};\n    if (inputs.at(1).is_tensor())\n    {\n        kernelTensor = &convertToTensor(inputs.at(1), ctx);\n        if (needToExpandDims)\n        {\n            // Expand spatial dims from 1D to 2D\n            std::vector<int32_t> const axes{3};\n            kernelTensor = unsqueezeTensor(ctx, *kernelTensor, axes);\n            ONNXTRT_CHECK(kernelTensor && \"Failed to unsqueeze tensor.\", ErrorCode::kUNSUPPORTED_NODE);\n        }\n        ONNXTRT_CHECK(checkSpatialDims(kernelTensor->getDimensions())\n                && \"The input tensor shape misaligns with the input kernel shape.\",\n            ErrorCode::kUNSUPPORTED_NODE);\n    }\n    else\n    {\n        kernelWeights = inputs.at(1).weights();\n        if (needToExpandDims)\n        {\n            kernelWeights.shape.nbDims = 4;\n            kernelWeights.shape.d[3] = 1;\n        }\n        ONNXTRT_CHECK_NODE(checkSpatialDims(kernelWeights.shape),\n            \"The input tensor shape misaligns with the input kernel shape.\", node, nodeIdx,\n            ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    if (inputs.size() == 3)\n    {\n        if (inputs.at(2).is_weights())\n        {\n            biasWeights = inputs.at(2).weights();\n        }\n        else\n        {\n            biasTensor = &convertToTensor(inputs.at(2), ctx);\n        }\n    }\n\n    OnnxAttrs attrs(node, ctx);\n    int32_t ngroup = attrs.get(\"group\", 1);\n    ONNXTRT_CHECK_NODE((nChannel == -1 || C * ngroup == nChannel),\n        \"The attribute group and the kernel shape misalign with the channel size of the input tensor. \", node, nodeIdx,\n        ErrorCode::kINVALID_NODE);\n\n    nvinfer1::IConvolutionLayer* layer\n        = N_CHECK(ctx->network()->addConvolutionNd(*input, K, kernelDims, kernelWeights, biasWeights));\n    ONNXTRT_CHECK_NODE(layer, \"Failed to add the Convolution layer.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    layer->setStrideNd(strides);\n    layer->setPaddingMode(paddingMode);\n    layer->setPrePadding(begPadding);\n    layer->setPostPadding(endPadding);\n    layer->setDilationNd(dilations);\n    layer->setNbGroups(ngroup);\n\n    // Set dynamic weights\n    if (kernelTensor)\n    {\n        layer->setInput(1, *kernelTensor);\n    }\n    if (biasTensor)\n    {\n        layer->setInput(2, *biasTensor);\n    }\n    ctx->registerLayer(layer, node);\n    if (kernelWeights)\n    {\n        ctx->network()->setWeightsName(kernelWeights, inputs.at(1).getName().c_str());\n    }\n    if (biasWeights && inputs.size() == 3)\n    {\n        ctx->network()->setWeightsName(biasWeights, inputs.at(2).getName().c_str());\n    }\n\n    nvinfer1::ITensor* outputTensor = N_CHECK(layer->getOutput(0));\n    if (needToExpandDims)\n    {\n        // Un-expand spatial dims back to 1D\n        std::vector<int32_t> const axes{3};\n        outputTensor = squeezeTensor(ctx, *outputTensor, axes);\n    }\n\n    return {{outputTensor}};\n}\n\nnvinfer1::ITensor* unsqueezeTensor(ImporterContext* ctx, nvinfer1::ITensor& tensor, std::vector<int32_t> const& axes)\n{\n    auto* axesTensor\n        = N_CHECK(addConstant(ctx, axes, ::ONNX_NAMESPACE::TensorProto::INT32, {1, {static_cast<int64_t>(axes.size())}})\n                      ->getOutput(0));\n    auto* unsqueezeLayer = N_CHECK(ctx->network()->addUnsqueeze(tensor, *axesTensor));\n    auto* unsqueezedTensor = N_CHECK(unsqueezeLayer->getOutput(0));\n    LOG_VERBOSE(\"Original shape: \" << shapeOf(tensor) << \", unsqueezing to: \" << shapeOf(*unsqueezedTensor));\n    ctx->registerLayer(unsqueezeLayer, \"ONNXTRT_unsqueezeTensor\", nullptr);\n    return N_CHECK(unsqueezeLayer->getOutput(0));\n}\n\nnvinfer1::ITensor* resizeShapeTensor(ImporterContext* ctx, nvinfer1::ITensor& input, TensorOrWeights& scales)\n{\n    // Create below subnetwork for processing resize scale tensor or weights.\n    // clang-format off\n    // scale weights (convert to tensor) or scale tensor -> elementwise mul -> transformation(floor, ceil, round) -> identity (cast to int) -> resize shape tensor\n    //      input -> shapeof -> identity (cast to float) ->\n    // clang-format on\n    auto* floatCast = N_CHECK(ctx->network()->addCast(shapeOf(input).tensor(ctx), nvinfer1::DataType::kFLOAT));\n    ctx->registerLayer(floatCast, \"ONNXTRT_resizeShapeTensor_floatCast\", nullptr);\n    auto* inputShapeTensor = N_CHECK(floatCast->getOutput(0));\n\n    auto& scaleTensor = convertToTensor(scales, ctx);\n    auto* prodLayer = N_CHECK(\n        ctx->network()->addElementWise(scaleTensor, *inputShapeTensor, nvinfer1::ElementWiseOperation::kPROD));\n    ctx->registerLayer(prodLayer, \"ONNXTRT_resizeShapeTensor_prod\", nullptr);\n\n    auto* prod = N_CHECK(prodLayer->getOutput(0));\n    auto* floorLayer = N_CHECK(ctx->network()->addUnary(*prod, nvinfer1::UnaryOperation::kFLOOR));\n    ctx->registerLayer(floorLayer, \"ONNXTRT_resizeShapeTensor_floor\", nullptr);\n\n    auto* floor = N_CHECK(floorLayer->getOutput(0));\n    auto* intCast = N_CHECK(ctx->network()->addCast(*floor, nvinfer1::DataType::kINT32));\n    ctx->registerLayer(intCast, \"ONNXTRT_resizeShapeTensor_intCast\", nullptr);\n    return N_CHECK(intCast->getOutput(0));\n}\n\nstd::string const getNodeName(::ONNX_NAMESPACE::NodeProto const& node)\n{\n    if (node.name().empty() && (node.output_size() != 0))\n    {\n        return \"node_of_\" + node.output(0);\n    }\n    else\n    {\n        return node.name();\n    }\n}\n\n//! Return ShapeTensor representing x clamped to closed interval [lowerBound,upperBound].\nstatic ShapeTensor clamp(\n    ImporterContext* ctx, ShapeTensor const& x, ShapeTensor const& lowerBound, ShapeTensor const& upperBound)\n{\n    return min(ctx, max(ctx, x, lowerBound), upperBound);\n}\n\n//! Return ShapeTensor representing indices < 0 ? inputDims + indices : indices\nstatic ShapeTensor bumpIfNegative(ImporterContext* ctx, ShapeTensor const& inputDims, ShapeTensor const& indices)\n{\n    auto const signs = clamp(ctx, indices, shapeVector(-1), shapeVector(0));\n    return sub(ctx, indices, mul(ctx, signs, inputDims));\n}\n\nvoid decodeOnnxStartsAndEnds(ImporterContext* ctx, ShapeTensor const& inputDims, ShapeTensor const& steps,\n    ShapeTensor& starts, ShapeTensor& ends)\n{\n    //! The ONNX specification is unreliable (https://github.com/onnx/onnx/issues/3063)\n    //! thus the logic here is designed to match that in\n    //! https://github.com/onnx/onnx/blob/master/onnx/defs/tensor/defs.cc .\n\n    // Set stepSign to step < 0 ? -1 : 0.\n    auto const stepSign = clamp(ctx, steps, shapeVector(-1), shapeVector(0));\n\n    // Update starts.\n    starts = bumpIfNegative(ctx, inputDims, starts);\n    starts = clamp(ctx, starts, shapeVector(0), add(ctx, inputDims, stepSign));\n\n    // Update ends\n    ends = bumpIfNegative(ctx, inputDims, ends);\n    ends = clamp(ctx, ends, stepSign, inputDims);\n}\n\nShapeTensor axesToInterlaceSubscripts(ShapeTensor const& axes, int nbDims)\n{\n    std::vector<int64_t> subscripts(nbDims);\n    std::iota(subscripts.begin(), subscripts.end(), 0);\n    for (int32_t i = 0; i < axes.size(); ++i)\n    {\n        subscripts[axes[i]] = nbDims + i;\n    }\n    return ShapeTensor(1, std::move(subscripts));\n}\n\nShapeTensor computeSliceSizes(ImporterContext* ctx, ShapeTensor const& starts, ShapeTensor const& ends,\n    ShapeTensor const& steps, ShapeTensor const& dims)\n{\n    if (steps.isAll(1))\n    {\n        // The general formula in the else is correct,\n        // but creates much debris for this common case.\n        return sub(ctx, ends, starts);\n    }\n    // \"If a negative value is passed for step, it represents slicing backward.\"\n    // Compute ceil((end-start)/step) using only operations available on ShapeTensor,\n    // using the identity ceil(x) = -floor(-x).\n    return sub(ctx, similar(ctx, dims, 0), floorDiv(ctx, sub(ctx, starts, ends), steps));\n}\n\nnvinfer1::ITensor* addSoftmax(\n    ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx, nvinfer1::ITensor& input)\n{\n    OnnxAttrs attrs(node, ctx);\n    // \"axis : int (default is opset specific)\"\n    int32_t const defaultAxis = (ctx->getOpsetVersion() >= 13) ? -1 : 1;\n    int32_t axis = attrs.get(\"axis\", defaultAxis);\n\n    // \"Negative value means counting dimensions from the back.\n    // Accepted range is [-r, r-1] where r = rank(input).\"\n    auto const rank = shapeOf(input).size();\n    convertAxis(axis, rank, node, nodeIdx);\n\n    nvinfer1::ISoftMaxLayer* softMax{nullptr};\n    if (ctx->getOpsetVersion() >= 13)\n    {\n        softMax = N_CHECK(ctx->network()->addSoftMax(input));\n        softMax->setAxes(1 << axis);\n    }\n    else\n    {\n        // \"The input does not need to explicitly be a 2D vector; rather, it will be coerced into one.\"\n        auto* flattened = flattenTensor(ctx, node, input, axis);\n        softMax = N_CHECK(ctx->network()->addSoftMax(*flattened));\n        // ONNX softmax is always on second dimension.\n        softMax->setAxes(1 << 1);\n    }\n    ctx->registerLayer(softMax, node);\n    return N_CHECK(softMax->getOutput(0));\n}\n\nNodeOutputs addScatterLayer(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights>& inputs, nvinfer1::ScatterMode mode, int32_t axis)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    nvinfer1::ITensor& data = convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor& indices = convertToTensor(inputs.at(1), ctx);\n    nvinfer1::ITensor& updates = convertToTensor(inputs.at(2), ctx);\n\n    // Validate input dimensions\n    if (mode == nvinfer1::ScatterMode::kELEMENT)\n    {\n        auto const dataDims = data.getDimensions();\n        auto const indicesDims = indices.getDimensions();\n        auto const updatesDims = updates.getDimensions();\n\n        // Ranks must all be the same\n        ONNXTRT_CHECK_NODE(dataDims.nbDims == indicesDims.nbDims && dataDims.nbDims == updatesDims.nbDims,\n            \"Input dimensions to ScatterElements must have the same rank! data rank =  \"\n                << dataDims.nbDims << \", indices rank = \" << indicesDims.nbDims\n                << \", updates rank = \" << updatesDims.nbDims << \".\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n        // Corresponding dimensions of indices and updates must be <= data\n        for (int32_t i = 0; i < dataDims.nbDims; ++i)\n        {\n            if (indicesDims.d[i] != -1 && dataDims.d[i] != -1)\n            {\n                ONNXTRT_CHECK_NODE(indicesDims.d[i] <= dataDims.d[i],\n                    \"Indices dimensions must be less than data dimensions! indices dimension = \"\n                        << indicesDims.d[i] << \", data dimension = \" << dataDims.d[i] << \" on index \" << i << \".\",\n                    node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n            }\n            if (updatesDims.d[i] != -1 && dataDims.d[i] != -1)\n            {\n                ONNXTRT_CHECK_NODE(updatesDims.d[i] <= dataDims.d[i],\n                    \"Updates dimensions must be less than data dimensions! updates dimension = \"\n                        << updatesDims.d[i] << \", data dimension = \" << dataDims.d[i] << \" on index \" << i << \".\",\n                    node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n            }\n        }\n    }\n\n    // TRT doesn't support int64 for indices\n    auto* cast = N_CHECK(ctx->network()->addCast(indices, nvinfer1::DataType::kINT32));\n    auto* indicesInt32 = N_CHECK(cast->getOutput(0));\n\n    auto* layer = N_CHECK(ctx->network()->addScatter(data, *indicesInt32, updates, mode));\n    layer->setAxis(axis);\n    ctx->registerLayer(layer, node);\n    auto output = N_CHECK(layer->getOutput(0));\n    return {{output}};\n}\n\n//! Helper function to calculate mod(A, B)\nnvinfer1::IElementWiseLayer* modWithIntegerInputs(\n    ImporterContext* ctx, nvinfer1::ITensor* input0, nvinfer1::ITensor* input1, bool fmod)\n{\n    using eOp = nvinfer1::ElementWiseOperation;\n    auto divOp = fmod ? eOp::kDIV : eOp::kFLOOR_DIV;\n\n    // input0 - (input1 * divOp(input0, input1))\n    auto rhs = getElementWiseResult(ctx, *input1, *getElementWiseResult(ctx, *input0, *input1, divOp), eOp::kPROD);\n    return N_CHECK(ctx->network()->addElementWise(*input0, *rhs, eOp::kSUB));\n}\n\nnvinfer1::IElementWiseLayer* modWithFPInputs(ImporterContext* ctx, nvinfer1::ITensor* input0, nvinfer1::ITensor* input1,\n    nvinfer1::ITensor* divResult, bool sameSign)\n{\n    using eOp = nvinfer1::ElementWiseOperation;\n    using uOp = nvinfer1::UnaryOperation;\n    // divResult need to be round towards 0\n    // When inputs have the same sign, round down (input0 / input1), else round up.\n    auto roundOp = sameSign ? uOp::kFLOOR : uOp::kCEIL;\n\n    // input0 - (input1 * round_towards_0(input0/ input1))\n    auto rhs = getElementWiseResult(ctx, *input1, *getUnaryResult(ctx, *divResult, roundOp), eOp::kPROD);\n    return N_CHECK(ctx->network()->addElementWise(*input0, *rhs, eOp::kSUB));\n}\n\nstd::string truncateString(std::string const& s, int64_t limit)\n{\n    if (static_cast<int64_t>(s.size()) <= limit)\n    {\n        return s;\n    }\n    return s.substr(0, limit / 2U) + \" ... \" + s.substr(s.size() - limit / 2U);\n}\n\nvoid processMetadata(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, nvinfer1::ILayer* layer)\n{\n    // Create a docstring that that holds node metadata and assign it to the corresponding TRT layer.\n    // The format of the string is as follows:\n    // [ONNX Layer: <name> | property1 | property2 | property3 ...]\n\n    std::string metadata = \"[ONNX Layer: \" + getNodeName(node);\n\n    // Generate local function stack string.\n    for (auto it = ctx->localFunctionStack().crbegin(); it < ctx->localFunctionStack().crend(); ++it)\n    {\n        metadata += \" | \" + it->nodeName + \" (\" + it->functionName + \")\";\n    }\n\n    metadata += \"]\";\n\n    // Truncate very long metadata since TRT API has a limit.\n    constexpr int64_t kMETADATA_LIMIT{4000};\n    layer->setMetadata(truncateString(metadata, kMETADATA_LIMIT).c_str());\n}\n\n//! Parse einsum equation into a vector of input strings and an output string.\nvoid parseEinsumEquation(\n    std::string& equation, std::vector<std::string>& inputSubscriptsVec, std::string& outputSubscripts)\n{\n    //! remove spaces\n    equation.erase(std::remove(equation.begin(), equation.end(), ' '), equation.end());\n\n    auto const& arrowIndex = equation.find(\"->\");\n    std::string left{};\n    if (arrowIndex != std::string::npos)\n    {\n        constexpr uint32_t kARROW_SIZE = 2;\n        left = equation.substr(0, arrowIndex);\n        outputSubscripts = equation.substr(arrowIndex + kARROW_SIZE);\n    }\n    else\n    {\n        left = equation;\n        outputSubscripts.clear();\n    }\n    left.push_back(','); // Correctly handle trailing scalars in equations like \",\"\n    std::regex const regex(\",\");\n    std::sregex_token_iterator begin(left.begin(), left.end(), regex, -1);\n    std::copy(begin, std::sregex_token_iterator(), std::back_inserter(inputSubscriptsVec));\n}\n\n//! replace ellipsis with the same subscripts for each input/output subscript string.\nvoid replaceEllipsis(nvinfer1::ITensor* const inputTensor, bool const isInput,\n    std::map<char, int64_t> const& subscriptCount, std::string& substitution, std::string& subscripts)\n{\n    auto const& ellipsisIndex = subscripts.find(\"...\");\n    if (ellipsisIndex != std::string::npos)\n    {\n        constexpr uint32_t kELLIPSIS_SIZE = 3;\n        std::string const& left = subscripts.substr(0, ellipsisIndex);\n        std::string const& right = subscripts.substr(ellipsisIndex + kELLIPSIS_SIZE);\n        if (substitution.empty() && isInput && inputTensor != nullptr) // First-time update substitution\n        {\n            nvinfer1::Dims inputDim = inputTensor->getDimensions();\n            int64_t const ellipsisDim = inputDim.nbDims - left.size() - right.size();\n            char c = 'a';\n            while (static_cast<int64_t>(substitution.size()) < ellipsisDim && c <= 'z')\n            {\n                if (!subscriptCount.count(c))\n                {\n                    substitution += c;\n                }\n                c++;\n            }\n        }\n        subscripts = left + substitution + right;\n    }\n}\n\n//! Rebuild einsum equation from input and output subscripts\nstd::string rebuildEinsumEquation(\n    std::vector<std::string> const& inputSubscriptsVec, std::string const& outputSubscripts)\n{\n    std::string equation{};\n    for (auto& s : inputSubscriptsVec)\n    {\n        equation += s;\n        equation += ',';\n    }\n    if (!equation.empty())\n    {\n        equation.pop_back();\n    }\n    equation += \"->\";\n    equation += outputSubscripts;\n    return equation;\n}\n\nvoid processEllipsisAndImplicitOutput(\n    std::vector<nvinfer1::ITensor*> const& inputTensors, std::string& equation, bool const withEllipsis)\n{\n    std::vector<std::string> inputSubscriptsVec{};\n    std::string outputSubscripts{};\n\n    parseEinsumEquation(equation, inputSubscriptsVec, outputSubscripts);\n\n    //! count subscripts\n    std::map<char, int64_t> subscriptCount;\n    for (auto& s : inputSubscriptsVec)\n    {\n        for (auto& c : s)\n        {\n            if (isalpha(c))\n            {\n                subscriptCount[c]++;\n            }\n        }\n    }\n\n    //! For implicit einsum, infer and write its outputSubscripts in equation\n    if (equation.find(\"->\") == std::string::npos)\n    {\n        if (withEllipsis)\n        {\n            outputSubscripts\n                = \"...\"; // In implicit mode, the ellipsis dimensions are set to the beginning of the output.\n        }\n        for (auto& subscript : subscriptCount)\n        {\n            if (subscript.second == 1)\n            {\n                //! Implicitly, output subscripts are set to the alphabetically sorted sequence.\n                //! Here we use a sorted map of subscript to achieve it.\n                outputSubscripts += subscript.first;\n            }\n        }\n    }\n\n    //! Replace ellipsis with new subscripts.\n    if (withEllipsis)\n    {\n        std::string substitution{};\n        int64_t const inputSize = inputTensors.size();\n        for (int64_t i = 0; i < inputSize; ++i)\n        {\n            replaceEllipsis(inputTensors[i], true, subscriptCount, substitution, inputSubscriptsVec[i]);\n        }\n        replaceEllipsis(nullptr, false, subscriptCount, substitution, outputSubscripts);\n    }\n\n    //! Rebuild einsum equation.\n    equation = rebuildEinsumEquation(inputSubscriptsVec, outputSubscripts);\n}\n\n//! Infer hiddent output subscripts when transforming einsum layer with more than 2 inputs into multiple 2-input einsum\n//! layers.\nstd::string inferHiddenOutputSubscripts(std::vector<std::string> const& inputSubscriptsVec)\n{\n    std::map<char, int64_t> subscriptCount;\n    std::string outputSubscripts{};\n    for (auto const& s : inputSubscriptsVec)\n    {\n        for (auto const& c : s)\n        {\n            if (isalpha(c))\n            {\n                subscriptCount[c]++;\n            }\n        }\n    }\n    for (auto const& subscript : subscriptCount)\n    {\n        outputSubscripts += subscript.first;\n    }\n    return outputSubscripts;\n}\n\nnvinfer1::IEinsumLayer* parseGraphWithMoreInputs(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node,\n    std::vector<nvinfer1::ITensor*> const& inputs, int64_t const nbInputs, std::string equation)\n{\n    assert(nbInputs > 0);\n    assert(inputs.size() == static_cast<size_t>(nbInputs));\n    assert(ctx != nullptr);\n    std::vector<std::string> inputSubscriptsVec{};\n    std::string outputSubscripts{};\n\n    parseEinsumEquation(equation, inputSubscriptsVec, outputSubscripts);\n\n    std::string leftSubscripts = inputSubscriptsVec[0];\n    nvinfer1::ITensor* leftInput = inputs[0];\n    assert(leftInput != nullptr);\n\n    for (int64_t i = 1; i < nbInputs - 1; ++i)\n    {\n        std::vector<nvinfer1::ITensor*> inputTensors{leftInput, inputs[i]};\n        std::vector<std::string> inputSubscripts{leftSubscripts, inputSubscriptsVec[i]};\n        std::string hiddenOutputSubscripts = inferHiddenOutputSubscripts(inputSubscripts);\n        std::string hiddenEquation = rebuildEinsumEquation(inputSubscripts, hiddenOutputSubscripts);\n\n        nvinfer1::IEinsumLayer* einsumLayer\n            = N_CHECK(ctx->network()->addEinsum(inputTensors.data(), 2, hiddenEquation.c_str()));\n        ctx->registerLayer(einsumLayer, node);\n\n        leftSubscripts = hiddenOutputSubscripts;\n        leftInput = N_CHECK(einsumLayer->getOutput(0));\n    }\n\n    assert(inputs[nbInputs - 1] != nullptr);\n    std::vector<nvinfer1::ITensor*> finalInputTensors{leftInput, inputs[nbInputs - 1]};\n    std::string finalEquation\n        = rebuildEinsumEquation({leftSubscripts, inputSubscriptsVec[nbInputs - 1]}, outputSubscripts);\n    nvinfer1::IEinsumLayer* einsumLayer\n        = N_CHECK(ctx->network()->addEinsum(finalInputTensors.data(), 2, finalEquation.c_str()));\n    ctx->registerLayer(einsumLayer, node);\n\n    return einsumLayer;\n}\n\nnvinfer1::ITensor* generateWindow(ImporterContext* ctx, nvinfer1::ITensor* N)\n{\n    auto shapeOfN = ShapeTensor(*N, 0);\n    nvinfer1::IFillLayer* layer = N_CHECK(addFill(ctx, convertTo1D(ctx, shapeOfN), nvinfer1::FillOperation::kLINSPACE));\n    layer->setAlpha(0.0F);\n    layer->setBeta(1.0F);\n    auto* fillOutput = N_CHECK(layer->getOutput(0));\n    return fillOutput;\n}\n\nnvinfer1::ITensor* windowHelper(ImporterContext* ctx, float numerator, nvinfer1::ITensor* n, nvinfer1::ITensor* N,\n    nvinfer1::UnaryOperation op, int32_t periodic)\n{\n    auto* numeratorTensor = N_CHECK(addConstantScalar(ctx, numerator, ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT,\n        nvinfer1::Dims{1, {1}})->getOutput(0));\n    auto numeratorLayer\n        = N_CHECK(ctx->network()->addElementWise(*numeratorTensor, *n, nvinfer1::ElementWiseOperation::kPROD));\n    auto numeratorOutput = N_CHECK(numeratorLayer->getOutput(0));\n\n    // If periodic is 0, subtract 1 from the denominator (N)\n    if (periodic == 0)\n    {\n        auto* one = N_CHECK(addConstantScalar(ctx, 1, ::ONNX_NAMESPACE::TensorProto_DataType_INT32)->getOutput(0));\n        one = castHelper(ctx, one, N->getType());\n        auto minusOne = N_CHECK(ctx->network()->addElementWise(*N, *one, nvinfer1::ElementWiseOperation::kSUB));\n        N = N_CHECK(minusOne->getOutput(0));\n    }\n\n    auto NFloat = N_CHECK(castHelper(ctx, N, nvinfer1::DataType::kFLOAT));\n    broadcastTensors(ctx, n, NFloat);\n    auto divLayer\n        = N_CHECK(ctx->network()->addElementWise(*numeratorOutput, *NFloat, nvinfer1::ElementWiseOperation::kDIV));\n    auto divOutput = N_CHECK(divLayer->getOutput(0));\n\n    auto trigLayer = N_CHECK(ctx->network()->addUnary(*divOutput, op));\n    auto trigOutput = N_CHECK(trigLayer->getOutput(0));\n\n    return N_CHECK(trigOutput);\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "importerUtils.hpp",
          "type": "blob",
          "size": 24.19140625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include \"ImporterContext.hpp\"\n#include \"OnnxAttrs.hpp\"\n#include \"ShapeTensor.hpp\"\n#include \"ShapedWeights.hpp\"\n#include \"Status.hpp\"\n#include \"errorHelpers.hpp\"\n#include \"weightUtils.hpp\"\n\n#include \"plugin.h\"\n#include <NvInfer.h>\n\n#include \"bfloat16.hpp\"\n#include \"half.h\"\n#include <cstring> // For std::memcpy\n#include <iostream>\n#include <limits>\n#include <numeric>\n#include <sstream>\n#include <typeindex>\n#include <unordered_map>\n\nnamespace onnx2trt\n{\n\nconstexpr char const* kTRT_STD_PLUGIN_NAMESPACE = \"\";\n\nstruct PluginDeleter\n{\n    void operator()(nvinfer1::IPluginV2* t);\n};\n\n// Helper function to add a single constant value into TensorRT\ntemplate <typename ScalarType>\nnvinfer1::IConstantLayer* addConstantScalar(\n    ImporterContext* ctx, ScalarType scalar, ShapedWeights::DataType type, nvinfer1::Dims shape = nvinfer1::Dims{0})\n{\n    assert(getShapedWeightsDataType<ScalarType>() == type);\n    assert(volume(shape) == 1 && \"Cannot add constant scalar with a shape that has volume > 1\");\n    ShapedWeights scalarWeights = ctx->createNamedTempWeights(type, shape);\n    static_cast<ScalarType*>(scalarWeights.values)[0] = static_cast<ScalarType>(scalar);\n    nvinfer1::IConstantLayer* l = N_CHECK(ctx->network()->addConstant(scalarWeights.shape, scalarWeights));\n    ctx->network()->setWeightsName(scalarWeights, scalarWeights.getName());\n    return l;\n}\n\n// Helper function to create a tensor given a vector of values and a shape.\ntemplate <typename ScalarType>\nnvinfer1::IConstantLayer* addConstant(\n    ImporterContext* ctx, std::vector<ScalarType> const& values, ShapedWeights::DataType type, nvinfer1::Dims shape)\n{\n    assert(getShapedWeightsDataType<ScalarType>() == type);\n    assert(volume(shape) == static_cast<int64_t>(values.size()) && \"Shape does not match number of values provided\");\n    auto const sizeInBits = getDtypeSizeBits(type);\n    assert(sizeInBits % 8 == 0); // TRT-22989: handle sub-byte size and shape checks\n    assert(sizeof(ScalarType) == sizeInBits / 8 && \"ONNX dtype does not have the same size as the value type\");\n    (void) sizeInBits;\n    ShapedWeights weights = ctx->createNamedTempWeights(type, shape);\n    std::memcpy(weights.values, values.data(), values.size() * sizeof(ScalarType));\n    nvinfer1::IConstantLayer* l = N_CHECK(ctx->network()->addConstant(weights.shape, weights));\n    ctx->network()->setWeightsName(weights, weights.getName());\n    return l;\n}\n\n// Helper overloads for comparisons between dimensions.\ninline bool operator==(nvinfer1::Dims const& a, nvinfer1::Dims const& b)\n{\n    if (a.nbDims != b.nbDims)\n    {\n        return false;\n    }\n    for (int32_t i = 0; i < a.nbDims; ++i)\n    {\n        if (a.d[i] != b.d[i])\n        {\n            return false;\n        }\n    }\n    return true;\n}\n\ninline bool operator!=(nvinfer1::Dims const& a, nvinfer1::Dims const& b)\n{\n    return !(a == b);\n}\n\nenum ScaleOp\n{\n    kSHIFT,\n    kSCALE,\n    kPOWER,\n};\n\n// Helper function to import ONNX activation nodes into TRT\nNodeOutputs activationHelper(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights>& inputs, nvinfer1::ActivationType op, float* alpha = nullptr, float* beta = nullptr);\n\n// Add clipping to a tensor if clip is a valid value.\nnvinfer1::ITensor* addClip(ImporterContext* ctx, nvinfer1::ITensor* input, float clip);\n\n// Helper function to import ArgMax and ArgMin nodes into TRT\nNodeOutputs argMinMaxHelper(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights>& inputs, nvinfer1::TopKOperation op);\n\n//! If t has rank less than nbDims, reshape it to have nbDims by prepending ones to its dimensions.\n//! Assert failure if t has rank greater than nbDims.\nvoid broadcastTensor(ImporterContext* ctx, nvinfer1::ITensor*& t, int const nbDims);\n\n// Helper function to broadcast two tensors to the larger one's shape\nvoid broadcastTensors(ImporterContext* ctx, nvinfer1::ITensor*& t1, nvinfer1::ITensor*& t2);\n\n// Helper function to broadcast three tensors to the largest one's shape\nvoid broadcastTensors(ImporterContext* ctx, nvinfer1::ITensor*& t1, nvinfer1::ITensor*& t2, nvinfer1::ITensor*& t3);\n\n// Helper function to calculate the bias tensor for GatherElements.\nstd::vector<int32_t> calculateBias(\n    nvinfer1::Dims const& daDims, nvinfer1::Dims const& idxDims, std::vector<int32_t> const& pitches, int32_t axis);\n\n// Helper function to check that linear/cubic resize can be used\nbool canUseNDResize(size_t const scaleSize, float const* scaleFactors, size_t const n);\n\n// Helper function to calculate and return a vector representation of the pitches of a given shape\nstd::vector<int32_t> calculatePitches(nvinfer1::Dims const& inputDims);\n\n// Helper function to add a Cast layer in the network\nnvinfer1::ITensor* castHelper(ImporterContext* ctx, nvinfer1::ITensor* input, nvinfer1::DataType dtype);\n\n// Helper function for constantOfShape operator. Input shape must be a shape tensor\nnvinfer1::ITensor* constantOfShape(ImporterContext* ctx, nvinfer1::ITensor* constant, nvinfer1::ITensor* shape);\n\n// Helper function to convert an ONNX axis into a TRT axis\nvoid convertAxis(int32_t& axis, int32_t const nbDims, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx);\n\n// Helper function to convert an ONNX datatype into a TRT datatype\nbool convertDtype(int32_t onnx_dtype, nvinfer1::DataType* trt_dtype);\n\n// Helper function to convert ONNX padding into TRT padding. Will update startTensor and totalPaddingTensor by reference\nbool convertOnnxPadding(ImporterContext* ctx, int32_t nbInputDims, std::vector<int64_t> const& onnxPadding,\n    nvinfer1::ITensor*& startTensor, nvinfer1::ITensor*& totalPaddingTensor);\n\n// Helper function to check if all of the values in the shift tensor are zeros\nbool shiftIsAllZeros(ShapedWeights const& shiftInt8);\n\n// Helper function to create zero shifts for QuantizeLinear/DequantizeLinear ops\nonnx2trt::ShapedWeights createZeroShifts(onnx2trt::ShapedWeights const& shiftInt8, int32_t type, ImporterContext* ctx);\n\n// Helper function to create a tensor of all zeros with the same shape as a data tensor\nnvinfer1::ITensor* createZeroTensor(ImporterContext* ctx, nvinfer1::ITensor* data);\n\n// Helper function to convert multi input convolution\nNodeOutputs convMultiInput(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights>& inputs);\n\n// Helper function to convert a 1D tensor into a scalar\nnvinfer1::ITensor* convertToScalar(ImporterContext* ctx, nvinfer1::ITensor* inpTensor);\n\n// Helper function to convert a ShapedWeights object into a tensor\nnvinfer1::ITensor& convertToTensor(TensorOrWeights& input, ImporterContext* ctx);\n\n// Helper function to convert a ShapedWeights object into a scalar\nnvinfer1::ITensor* convertToScalar(TensorOrWeights& input, ImporterContext* ctx);\n\n// Helper function to convert a scalar into a vector.\nnvinfer1::ITensor* convertScalarToVector(ImporterContext* ctx, nvinfer1::ITensor* input);\n\n// Helper function to provide a ceiling-rounding division between two integers\nint divCeil(int n, int d);\n\n// Helper function to check that the input data types for an elementwise operation are supported\nvoid elementwiseCheck(std::vector<TensorOrWeights> const& inputs, const nvinfer1::ElementWiseOperation op,\n    ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx);\n\n// Helper function to import an ONNX elementwise op into TRT\nNodeOutputs elementwiseHelper(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights> const& inputs, nvinfer1::ElementWiseOperation binary_op);\n\n// Helper function to flatten a tensor on a given axis\nnvinfer1::ITensor* flattenTensor(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node,\n    nvinfer1::ITensor& tensor, int axis = 0, bool regLayer = false);\n\n// Slice out the specified dimension from a shape tensor. e.g. extractDimension(shape=(7, 6, 5), dim=2) would return 5.\n// shape specifies the shape of the returned Tensor. Must have a volume of 1.\nnvinfer1::ITensor* extractDimension(\n    ImporterContext* ctx, nvinfer1::ITensor* shapeTensor, int32_t dim, nvinfer1::Dims shape);\n\n// Helper function to generate padding values for convTranspose\nvoid generatePadding(nvinfer1::Dims inputShape, nvinfer1::Dims outputShape, nvinfer1::Dims kernelSize,\n    nvinfer1::Dims strides, nvinfer1::Dims dilations, int const nbSpatialDims, nvinfer1::Dims& begPadding,\n    nvinfer1::Dims& endPadding, nvinfer1::Dims& outputPadding, nvinfer1::PaddingMode paddingMode);\n\n// Helper function to get default ONNX activation alpha values\nfloat getActivationDefaultAlpha(nvinfer1::ActivationType type);\n\n// Helper function to get default ONNX activation beta values\nfloat getActivationDefaultBeta(nvinfer1::ActivationType type);\n\n// Helper function to get the length of the specified axis\nnvinfer1::ITensor* getAxisLength(\n    ImporterContext* ctx, nvinfer1::ITensor* inpTensor, int32_t axis, nvinfer1::Dims shape = nvinfer1::Dims{0});\n\n// Helper function to return the result tensor from an elementwise layer with nullptr checking.\nnvinfer1::ITensor* getElementWiseResult(\n    ImporterContext* ctx, nvinfer1::ITensor& lhs, nvinfer1::ITensor& rhs, nvinfer1::ElementWiseOperation op);\n\n// Helper function to return the result tensor from an unary layer with nullptr checking.\nnvinfer1::ITensor* getUnaryResult(ImporterContext* ctx, nvinfer1::ITensor& input, nvinfer1::UnaryOperation op);\n\n// Helper function to get kernel attributes for various ONNX nodes\nvoid getKernelParams(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, nvinfer1::Dims* kernelSize,\n    nvinfer1::Dims* strides, nvinfer1::Dims* begPadding, nvinfer1::Dims* endPadding, nvinfer1::PaddingMode& paddingMode,\n    bool& countExcludePadding, nvinfer1::Dims* dilations = nullptr, nvinfer1::Dims* outputPadding = nullptr,\n    bool const poolingCeilMode = false);\n\n// Helper function to get the scaling mode for TRT's scale layer\nnvinfer1::ScaleMode getScaleMode(nvinfer1::Dims const& weights_shape, nvinfer1::Dims const& tensor_shape);\n\n// Helper function to get a float representation of weights containing a single value.\nfloat getSingleValueAsFloat(ShapedWeights const& weights);\n\n// Helper function to map ONNX Global Pooling ops into TensorRT.\nnvinfer1::ITensor* globalPoolingHelper(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node,\n    nvinfer1::ITensor& tensor, nvinfer1::ReduceOperation op);\n\n// Helper function to create a greaterOrEqual or lessOrEqual operation. Provide `greater=true` for greaterOrEqual,\n// `greater=false` for lessOrEqual\nNodeOutputs greaterLessOrEqual(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx,\n    nvinfer1::ITensor* inputA, nvinfer1::ITensor* inputB, bool greater);\n\n// Helper function to determine if a shape contains dynamic dimensions\nbool isDynamic(nvinfer1::Dims const& shape);\n\n// Helper function to use modulatedDeformableConv2D plugin\nNodeOutputs modulatedDeformableConvPluginHelper(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node,\n    size_t const nodeIdx, std::vector<TensorOrWeights>& inputs);\n\n// Helper function to use optimized 3D instanceNorm plugin\nNodeOutputs instanceNormPluginHelper(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node,\n    size_t const nodeIdx, std::vector<TensorOrWeights>& inputs);\n\n// Helper fucntion to create an iota fill given a set of dimensions and an axis\nnvinfer1::ITensor* iota(ImporterContext* ctx, ShapeTensor iotaDims, int32_t axis);\n\n// Helper function to load a creator from the registry\nnvinfer1::IPluginCreatorInterface* importPluginCreator(ImporterContext* ctx, std::string const& pluginName,\n    std::string const& pluginVersion, std::string const& pluginNamespace = kTRT_STD_PLUGIN_NAMESPACE);\n\n// Helper function to get a plugin from the PluginRegistry\nstd::unique_ptr<nvinfer1::IPluginV2, PluginDeleter> createPlugin(std::string const& name,\n    std::string const& pluginNamespace, nvinfer1::IPluginCreator* pluginCreator,\n    std::vector<nvinfer1::PluginField> const& pluginFields);\n\n// Helper function to get a V3 plugin from the PluginRegistry\nstd::unique_ptr<nvinfer1::IPluginV3> createPlugin(std::string const& name, std::string const& pluginNamespace,\n    nvinfer1::IPluginCreatorInterface* pluginCreator, std::vector<nvinfer1::PluginField> const& pluginFields);\n\n// Helper function to return the identity of a TensorOrWeights\nTensorOrWeights identity(ImporterContext* ctx, TensorOrWeights input);\n\n// Helper function to create and fill a Dims object with defined values\nnvinfer1::Dims makeDims(int nbDims, int val);\n\n// Helper function to create normalization layers for GroupNorm and InstanceNorm\nNodeOutputs normalizationHelper(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights>& inputs);\n\n// Given a list of axes in the range of [-rank, rank-1], where rank is the rank\n// of the corresponding data tensor, normalize to [0, rank-1].\nvoid normalizeAxes(ShapeTensor& axes, int32_t const rank);\n\n// Helper function to parse activation values for LSTM nodes\nstd::vector<float> parseLSTMActivationValues(std::vector<nvinfer1::ActivationType> const& activationTypes,\n    std::vector<float> const& activationValues, bool isAlpha);\n\n// Helper function to map various ONNX pooling ops into TensorRT.\nNodeOutputs poolingHelper(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights>& inputs, nvinfer1::PoolingType type);\n\n// Helper function to check if reduce op equals No-op\nbool IsReduceNoOp(\n    ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, std::vector<TensorOrWeights> const& inputs);\n\n// Helper function to import reduce ops into TRT\nNodeOutputs reduceTensor(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx,\n    TensorOrWeights input, nvinfer1::ReduceOperation operation, TensorOrWeights inputAxes = TensorOrWeights());\n\n// Helper function to shape a Tensor given a new shape\nnvinfer1::ITensor* reshapeTensor(ImporterContext* ctx, nvinfer1::ITensor& tensor, nvinfer1::Dims shape);\n\n// Helper function to map attributes to a TRT scale layer\nNodeOutputs scaleHelper(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx,\n    nvinfer1::ITensor& tensor_, nvinfer1::ScaleMode mode, nvinfer1::Weights const& shift,\n    nvinfer1::Weights const& scale, nvinfer1::Weights const& power, char const* shiftName, char const* scaleName);\n\n// Helper function to set an ONNX attribute\nvoid setAttr(nvinfer1::Dims* trtAttr, ::ONNX_NAMESPACE::AttributeProto const* onnxAttr, int32_t nbSpatialDims,\n    int32_t defaultVal);\n\n// Helper function to slice away elements on a given axis dimension\nnvinfer1::ITensor* sliceAcrossAxis(\n    ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, nvinfer1::ITensor* data, int32_t const axis);\n\n// Helper function to squeeze a tensor on a given set of axes\nnvinfer1::ITensor* squeezeTensor(ImporterContext* ctx, nvinfer1::ITensor& tensor, std::vector<int32_t> const& axes);\n\n// Helper function to transpose a tensor given a permutation\nnvinfer1::ITensor* transposeTensor(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node,\n    nvinfer1::ITensor& tensor, nvinfer1::Permutation const& perm);\n\n::ONNX_NAMESPACE::TensorProto_DataType trtDataTypeToONNX(nvinfer1::DataType dt);\n\n// Helper function to import ONNX unary ops into TRT\nNodeOutputs unaryHelper(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx,\n    TensorOrWeights& input, nvinfer1::UnaryOperation op);\n\n// Helper function to unsqueeze tensors on a given set of axes\nnvinfer1::ITensor* unsqueezeTensor(ImporterContext* ctx, nvinfer1::ITensor& tensor, std::vector<int32_t> const& axes);\n\n// Helper function to calculate and return the expected output shape of a resize given the resize scale weights or scale\n// tensor.\nnvinfer1::ITensor* resizeShapeTensor(ImporterContext* ctx, nvinfer1::ITensor& input, TensorOrWeights& scales);\n\n// Helper function to convert a ShapedWeights object into a vector\ntemplate <typename WeightType>\nvoid weightsToVector(TensorOrWeights weights, std::vector<WeightType>* weightVector)\n{\n    ONNXTRT_CHECK(weights.is_weights(), ErrorCode::kUNSUPPORTED_NODE);\n    ONNXTRT_CHECK((weights.weights().type == ::ONNX_NAMESPACE::TensorProto::INT32)\n            || (weights.weights().type == ::ONNX_NAMESPACE::TensorProto::INT64)\n            || (weights.weights().type == ::ONNX_NAMESPACE::TensorProto::BOOL)\n            || (weights.weights().type == ::ONNX_NAMESPACE::TensorProto::FLOAT),\n        ErrorCode::kINVALID_NODE);\n    weightVector->resize(weights.weights().count());\n    if (weights.weights().type == ::ONNX_NAMESPACE::TensorProto::INT64)\n    {\n        auto array_start = static_cast<int64_t*>(weights.weights().values);\n        std::copy(array_start, array_start + weights.weights().count(), weightVector->begin());\n    }\n    else if (weights.weights().type == ::ONNX_NAMESPACE::TensorProto::INT32)\n    {\n        auto array_start = static_cast<int32_t*>(weights.weights().values);\n        std::copy(array_start, array_start + weights.weights().count(), weightVector->begin());\n    }\n    else if (weights.weights().type == ::ONNX_NAMESPACE::TensorProto::BOOL)\n    {\n        auto array_start = static_cast<bool*>(weights.weights().values);\n        std::copy(array_start, array_start + weights.weights().count(), weightVector->begin());\n    }\n    else if (weights.weights().type == ::ONNX_NAMESPACE::TensorProto::FLOAT)\n    {\n        auto array_start = static_cast<float*>(weights.weights().values);\n        std::copy(array_start, array_start + weights.weights().count(), weightVector->begin());\n    }\n}\n\nNodeOutputs staticSliceImporter(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights>& inputs, nvinfer1::ITensor& data);\n\n// Helper function to convert ONNX node name. If no node name, using name of first output.\nstd::string const getNodeName(::ONNX_NAMESPACE::NodeProto const& node);\n\n//! Decode in place the starts and ends indices according to ONNX Slice rules.\nvoid decodeOnnxStartsAndEnds(ImporterContext* ctx, ShapeTensor const& inputDims, ShapeTensor const& steps,\n    ShapeTensor& starts, ShapeTensor& ends);\n\n//! Return ShapeTensor representing size of result of Slice.\n//! starts and ends should first be decoded by decodeOnnxStartsAndEnds.\nShapeTensor computeSliceSizes(ImporterContext* ctx, ShapeTensor const& starts, ShapeTensor const& ends,\n    ShapeTensor const& steps, ShapeTensor const& dims);\n\n//! Return subscripts such that gather(concat(x,y),subscripts)\n//! will return x with x[subcripts[i]] replaced by y[i].\nShapeTensor axesToInterlaceSubscripts(ShapeTensor const& axes, int nbDims);\n\n//! Helper function to add SoftMax layer.\nnvinfer1::ITensor* addSoftmax(\n    ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx, nvinfer1::ITensor& input);\n\n//! Helper function to import ONNX scatter nodes into TRT\nNodeOutputs addScatterLayer(ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, size_t const nodeIdx,\n    std::vector<TensorOrWeights>& inputs, nvinfer1::ScatterMode mode, int32_t axis = 0);\n\n//! Helper function to calculate mod(A, B), A & B are integers\nnvinfer1::IElementWiseLayer* modWithIntegerInputs(\n    ImporterContext* ctx, nvinfer1::ITensor* input0, nvinfer1::ITensor* input1, bool fmod);\n\n//! Helper function to calculate mod(A, B), A & B are floating point numbers\nnvinfer1::IElementWiseLayer* modWithFPInputs(ImporterContext* ctx, nvinfer1::ITensor* input0, nvinfer1::ITensor* input1,\n    nvinfer1::ITensor* divResult, bool sameSign);\n\n//! RAII wrapper for ImporterContext::pushBaseNameScope() and popBaseNameScope().\nclass NameScope\n{\npublic:\n    NameScope(ImporterContext& context)\n        : mContext(context)\n    {\n        mContext.pushBaseNameScope();\n    }\n    ~NameScope()\n    {\n        mContext.popBaseNameScope();\n    }\n\nprivate:\n    ImporterContext& mContext;\n};\n\n// Helper function to validate input types for an ONNX node\nStatus notInvalidType(TensorOrWeights const& input, std::vector<std::string> const& invalidTypes,\n    ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx);\n\nvoid checkNotInvalidType(TensorOrWeights const& input, std::vector<std::string> const& invalidTypes,\n    ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx);\n\nvoid processMetadata(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, nvinfer1::ILayer* layer);\n\n//! Helper function to process ellipsis and implicit output in Einsum\n//!\n//! \\param inputTensors Vector of input tensors\n//! \\param equation String of equation in Einsum. It will be modified in this function.\n//! \\param withEllipsis Bool indicating whether the equation contains ellipsis.\n//!\n//! \\brief For an Einsum equation with ellipsises or implicit output, this function does the following steps:\n//!        1. parse the equation into a vector of input strings and an output string;\n//!        2. infer and write output string if the equation has implicit output;\n//!        3. replace ellipsis with new subscripts for each input/output string when the equation contains ellipsis;\n//!        4. rebuild the einsum equation string with explicit output.\n//!\nvoid processEllipsisAndImplicitOutput(\n    std::vector<nvinfer1::ITensor*> const& inputTensors, std::string& equation, bool const withEllipsis);\n\n//! Helper function to parse the Einsum layer with more than 2 inputs as a graph with multiple 2-input Einsum layers.\n//!\n//! \\param equation It is intended to be a copy instead of a const reference.\n//!        It cannot be a const as it will be further edited in parseEinsumEquation() which requires string& equation.\n//!        It cannot be a reference as like an output of this function which it is not.\n//!\nnvinfer1::IEinsumLayer* parseGraphWithMoreInputs(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node,\n    std::vector<nvinfer1::ITensor*> const& inputs, int64_t const nbInputs, std::string equation);\n\n// Helper function to convert TensorRT datatype enum into a human-readable string.\nstd::string getTrtDtypeName(nvinfer1::DataType TrtDtype);\n\n// Helper fucntion to generate a Window tensor for Window operations (HannWindow, HammingWindow, BlackmanWindow).\nnvinfer1::ITensor* generateWindow(ImporterContext* ctx, nvinfer1::ITensor* N);\n\n// Helper function to handle Window generation ops. Calculates TrigOp(numerator*n / N) and returns the output tensor.\nnvinfer1::ITensor* windowHelper(ImporterContext* ctx, float numerator, nvinfer1::ITensor* n, nvinfer1::ITensor* N,\n    nvinfer1::UnaryOperation op, int32_t periodic);\n\n//! Describes occurrence of a named dimension.\nclass NamedDimension\n{\npublic:\n    //! TensorRT tensor.\n    nvinfer1::ITensor* tensor;\n\n    //! Index of tensor dimension to be named.\n    int32_t index;\n\n    //! ONNX \"dim param\" that is the name of the dimension.\n    std::string dimParam;\n\n    //! Construct a NamedDimension where the tensor will be filled in later.\n    NamedDimension(int32_t index_, std::string const& dimParam_)\n        : tensor(nullptr)\n        , index(index_)\n        , dimParam(dimParam_)\n    {\n    }\n};\n\ntemplate <typename OnnxDims>\nbool convertOnnxDims(OnnxDims const& onnxDims, nvinfer1::Dims& trtDims, std::vector<NamedDimension>& namedDims)\n{\n    if (onnxDims.size() > nvinfer1::Dims::MAX_DIMS)\n    {\n        return false;\n    }\n    std::vector<int32_t> onnxDimsVec;\n    for (auto const& onnxDim : onnxDims)\n    {\n        // For empty dimensions, the ONNX specification says it's a dynamic dimension\n        if (!onnxDim.has_dim_value() && !onnxDim.has_dim_param())\n        {\n            onnxDimsVec.emplace_back(-1);\n        }\n        else\n        {\n            if (!onnxDim.dim_param().empty())\n            {\n                namedDims.emplace_back(static_cast<int32_t>(onnxDimsVec.size()), onnxDim.dim_param());\n            }\n            const int32_t dim = onnxDim.dim_param() == \"\" ? (onnxDim.dim_value() >= 0 ? onnxDim.dim_value() : -1) : -1;\n            onnxDimsVec.emplace_back(dim);\n        }\n    }\n    trtDims.nbDims = onnxDimsVec.size();\n    std::copy(onnxDimsVec.begin(), onnxDimsVec.end(), trtDims.d);\n    return true;\n}\n\n//! Helper enum for TRT plugin creator versions\nenum class CreatorVersion : int32_t\n{\n    kV1,\n    kV3ONE,\n    kV3QUICK\n};\n\n//! Evaluate CreatorVersion given a pointer to a TensorRT plugin creator\nCreatorVersion getPluginCreatorVersion(nvinfer1::IPluginCreatorInterface const* pluginCreator);\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "libnvonnxparser.version",
          "type": "blob",
          "size": 0.1845703125,
          "content": "{\n  global:\n    createNvOnnxParser_INTERNAL;\n    createNvOnnxParserRefitter_INTERNAL;\n    getNvOnnxParserVersion;\n    extern \"C++\" {\n      vtable*nvonnxparser::*;\n    };\n  local:\n    *;\n};\n"
        },
        {
          "name": "onnx2trt_common.hpp",
          "type": "blob",
          "size": 1.775390625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include <NvInfer.h>\n#include <memory>\n\n#if NV_TENSORRT_MAJOR < 4\nnamespace nvinfer1\n{\n\nenum class PluginFormat : uint8_t\n{\n    kNCHW = 0,   //!< NCHW\n    kNC2HW2 = 1, //!< NCHW with 2-element packed channels\n    kNHWC8 = 2   //!< NHWC with 8-element packed channels (C must be a multiple of 8)\n};\n// from NvInfer.h\nclass IPluginExt : public IPlugin\n{\npublic:\n    virtual int getTensorRTVersion() const noexcept\n    {\n        return NV_TENSORRT_VERSION;\n    }\n    virtual bool supportsFormat(DataType type, PluginFormat format) const noexcept = 0;\n    virtual void configureWithFormat(const Dims* inputDims, int nbInputs, const Dims* outputDims, int nbOutputs,\n        DataType type, PluginFormat format, int maxBatchSize) noexcept\n        = 0;\n\nprotected:\n    void configure(\n        const Dims* inputDims, int nbInputs, const Dims* outputDims, int nbOutputs, int maxBatchSize) noexcept final\n    {\n        try\n        {\n            DataType type = nvinfer1::DataType::kFLOAT;\n            PluginFormat format = nvinfer1::PluginFormat::kLINEAR;\n            return this->configureWithFormat(inputDims, nbInputs, outputDims, nbOutputs, type, format, maxBatchSize);\n        }\n        catch (const std::exception& e)\n        {\n            nvinfer1::getLogger()->log(nvinfer1::ILogger::Severity::kERROR, e.what().c_str());\n        }\n    }\n    virtual ~IPluginExt()\n    {\n    }\n};\n\n} // namespace nvinfer1\n#endif\n\nnamespace onnx2trt\n{\n\nstruct IOwnable\n{\n    virtual void destroy() = 0;\n\nprotected:\n    virtual ~IOwnable()\n    {\n    }\n};\n\nstruct OwnableDeleter\n{\n    void operator()(IOwnable* obj) const\n    {\n        obj->destroy();\n    }\n};\n\nusing UniqueOwnable = std::unique_ptr<IOwnable, OwnableDeleter>;\nclass Plugin;\nclass PluginV2;\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "onnx2trt_runtime.hpp",
          "type": "blob",
          "size": 0.2197265625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include \"onnx2trt_common.hpp\"\n\nnamespace onnx2trt\n{\n\ntypedef Plugin* (*plugin_deserializer)(const void* serialData, size_t serialLength);\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "onnxErrorRecorder.cpp",
          "type": "blob",
          "size": 2.5048828125,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"onnxErrorRecorder.hpp\"\n#include <exception>\n\nnamespace onnx2trt\n{\n\n\nONNXParserErrorRecorder* ONNXParserErrorRecorder::create(\n    nvinfer1::ILogger* logger, nvinfer1::IErrorRecorder* otherRecorder)\n{\n    try\n    {\n        auto recorder = new ONNXParserErrorRecorder(logger, otherRecorder);\n        if (recorder)\n        {\n            recorder->incRefCount();\n        }\n        return recorder;\n    }\n    catch (const std::exception& e)\n    {\n        logError(logger, e.what());\n        return nullptr;\n    }\n}\n\nvoid ONNXParserErrorRecorder::destroy(ONNXParserErrorRecorder*& recorder)\n{\n    if (recorder)\n    {\n        recorder->decRefCount();\n        recorder = nullptr;\n    }\n}\n\nvoid ONNXParserErrorRecorder::logError(nvinfer1::ILogger* logger, const char* str)\n{\n    if (logger)\n    {\n        logger->log(ILogger::Severity::kERROR, str);\n    }\n}\n\nONNXParserErrorRecorder::ONNXParserErrorRecorder(\n    nvinfer1::ILogger* logger, nvinfer1::IErrorRecorder* otherRecorder)\n    : mUserRecorder(otherRecorder)\n    , mLogger(logger)\n{\n    if (mUserRecorder)\n    {\n        mUserRecorder->incRefCount();\n    }\n}\n\nONNXParserErrorRecorder::~ONNXParserErrorRecorder() noexcept\n{\n    if (mUserRecorder)\n    {\n        mUserRecorder->decRefCount();\n    }\n}\n\nvoid ONNXParserErrorRecorder::clear() noexcept\n{\n    try\n    {\n        // grab a lock so that there is no addition while clearing.\n        std::lock_guard<std::mutex> guard(mStackLock);\n        mErrorStack.clear();\n    }\n    catch (const std::exception& e)\n    {\n        logError(mLogger, e.what());\n    }\n};\n\nbool ONNXParserErrorRecorder::reportError(\n    nvinfer1::ErrorCode val, nvinfer1::IErrorRecorder::ErrorDesc desc) noexcept\n{\n    try\n    {\n        std::lock_guard<std::mutex> guard(mStackLock);\n        mErrorStack.push_back(errorPair(val, desc));\n        if (mUserRecorder)\n        {\n            mUserRecorder->reportError(val, desc);\n        }\n        else\n        {\n            logError(mLogger, desc);\n        }\n    }\n    catch (const std::exception& e)\n    {\n        logError(mLogger, e.what());\n    }\n    // All errors are considered fatal.\n    return true;\n}\n\nnvinfer1::IErrorRecorder::RefCount ONNXParserErrorRecorder::incRefCount() noexcept\n{\n    // Atomically increment or decrement the ref counter.\n    return ++mRefCount;\n}\n\nnvinfer1::IErrorRecorder::RefCount ONNXParserErrorRecorder::decRefCount() noexcept\n{\n    auto newVal = --mRefCount;\n    if (newVal == 0)\n    {\n        delete this;\n    }\n    return newVal;\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "onnxErrorRecorder.hpp",
          "type": "blob",
          "size": 3.150390625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include \"NvInferRuntime.h\"\n#include <atomic>\n#include <cstdint>\n#include <exception>\n#include <mutex>\n#include <string>\n#include <vector>\n\nnamespace onnx2trt\n{\n\n//!\n//! A simple implementation of the IErrorRecorder interface for\n//! use by ONNX importer.\n//! ONNX-importer Error recorder is based on a vector that pairs the error\n//! code and the error string into a single element. It also uses\n//! standard mutex and atomics in order to make sure that the code\n//! works in a multi-threaded environment.\n//!\nclass ONNXParserErrorRecorder : public nvinfer1::IErrorRecorder\n{\n    using RefCount       = nvinfer1::IErrorRecorder::RefCount;\n    using ErrorDesc      = nvinfer1::IErrorRecorder::ErrorDesc;\n    using ErrorCode      = nvinfer1::ErrorCode;\n    using IErrorRecorder = nvinfer1::IErrorRecorder;\n    using ILogger        = nvinfer1::ILogger;\n\n    using errorPair      = std::pair<ErrorCode, std::string>;\n    using errorStack     = std::vector<errorPair>;\n\npublic:\n    static ONNXParserErrorRecorder* create(\n        ILogger* logger, IErrorRecorder* otherRecorder = nullptr);\n\n    static void destroy(ONNXParserErrorRecorder*& recorder);\n\n    void     clear()       noexcept final;\n    RefCount incRefCount() noexcept final;\n    RefCount decRefCount() noexcept final;\n    bool     reportError(ErrorCode val, ErrorDesc desc) noexcept final;\n\n    int32_t getNbErrors() const noexcept final\n    {\n        return mErrorStack.size();\n    }\n\n    ErrorCode getErrorCode(int32_t errorIdx) const noexcept final\n    {\n        return invalidIndexCheck(errorIdx) ? ErrorCode::kINVALID_ARGUMENT : (*this)[errorIdx].first;\n    }\n\n    ErrorDesc getErrorDesc(int32_t errorIdx) const noexcept final\n    {\n        return invalidIndexCheck(errorIdx) ? \"errorIdx out of range.\" : (*this)[errorIdx].second.c_str();\n    }\n\n    bool hasOverflowed() const noexcept final\n    {\n        // This class can never overflow since we have dynamic resize via std::vector usage.\n        return false;\n    }\n\nprotected:\n    ONNXParserErrorRecorder(ILogger* logger, IErrorRecorder* otherRecorder = nullptr);\n\n    virtual ~ONNXParserErrorRecorder() noexcept;\n\n    static void logError(ILogger* logger, const char* str);\n\n    // Simple helper functions.\n    const errorPair& operator[](size_t index) const noexcept\n    {\n        return mErrorStack[index];\n    }\n\n    bool invalidIndexCheck(int32_t index) const noexcept\n    {\n        // By converting signed to unsigned, we only need a single check since\n        // negative numbers turn into large positive greater than the size.\n        size_t sIndex = index;\n        return sIndex >= mErrorStack.size();\n    }\n    // Mutex to hold when locking mErrorStack.\n    std::mutex mStackLock;\n\n    // Reference count of the class. Destruction of the class when mRefCount\n    // is not zero causes undefined behavior.\n    std::atomic<int32_t> mRefCount{0};\n\n    // The error stack that holds the errors recorded by TensorRT.\n    errorStack mErrorStack;\n\n    // Original error recorder (set by user)\n    IErrorRecorder* mUserRecorder{nullptr};\n\n    // logger\n    ILogger* mLogger{nullptr};\n}; // class ONNXParserErrorRecorder\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "onnxOpCheckers.cpp",
          "type": "blob",
          "size": 35.7529296875,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"onnxOpCheckers.hpp\"\n#include \"ConditionalHelpers.hpp\"\n#include \"LoopHelpers.hpp\"\n#include \"ModelImporter.hpp\"\n#include \"NvInfer.h\"\n#include \"NvInferPlugin.h\"\n#include \"NvInferRuntime.h\"\n#include \"OnnxAttrs.hpp\"\n#include \"RNNHelpers.hpp\"\n#include \"ShapeTensor.hpp\"\n#include \"bfloat16.hpp\"\n#include \"half.h\"\n#include \"importerUtils.hpp\"\n\n#include <array>\n#include <iostream>\n#include <iterator>\n#include <tuple>\n\nnamespace onnx2trt\n{\n\nStringMap<OpStaticErrorChecker>& getOpStaticErrorCheckerMap()\n{\n    static StringMap<OpStaticErrorChecker> error_checkers;\n    return error_checkers;\n}\n\nnamespace\n{\n\nusing nvinfer1::DataType;\n\n#define IGNORE_UNUSED_GLOBAL(x)                                                                                        \\\n    static void _ignore_unused2_##x();                                                                                 \\\n    static void _ignore_unused1_##x()                                                                                  \\\n    {                                                                                                                  \\\n        (void) _ignore_unused2_##x;                                                                                    \\\n        (void) x;                                                                                                      \\\n    }                                                                                                                  \\\n    static void _ignore_unused2_##x()                                                                                  \\\n    {                                                                                                                  \\\n        (void) _ignore_unused1_##x;                                                                                    \\\n    }                                                                                                                  \\\n    struct SwallowSemicolon##x                                                                                         \\\n    {                                                                                                                  \\\n    }\n\n#define DECLARE_OP_CHECKER(op)                                                                                         \\\n    void check##op(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, std::vector<Status>& errors,         \\\n        size_t const nodeIndex)\n\n#define DEFINE_OP_CHECKER(op)                                                                                          \\\n    void check##op(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, std::vector<Status>& errors,         \\\n        size_t const nodeIndex);                                                                                       \\\n    static bool const op##_registered_op_checker = registerOpStaticErrorChecker(#op, check##op);                       \\\n    IGNORE_UNUSED_GLOBAL(op##_registered_op_checker);                                                                  \\\n    void check##op(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, std::vector<Status>& errors,         \\\n        size_t const nodeIndex)\n\n#define DEFINE_OP_EMPTY_CHECKER(op)                                                                                    \\\n    DEFINE_OP_CHECKER(op) {}\n\nbool registerOpStaticErrorChecker(std::string op, OpStaticErrorChecker const& checker)\n{\n    bool inserted = getOpStaticErrorCheckerMap().insert({op, checker}).second;\n    assert(inserted);\n    return inserted;\n}\n\nvoid argMinMaxCheckHelper(\n    ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, std::vector<Status>& errors, size_t const nodeIndex)\n{\n    OnnxAttrs attrs(node, ctx);\n    int32_t selectLastIndex = attrs.get<int32_t>(\"select_last_index\", 0);\n    STATIC_CHECK((!selectLastIndex || (selectLastIndex && ctx->getOpsetVersion() >= 12))\n            && \"Per-opset 12 ONNX does not support the select_last_index attribute.\",\n        ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nvoid poolingCheckHelper(\n    ImporterContext* ctx, const ::ONNX_NAMESPACE::NodeProto& node, std::vector<Status>& errors, size_t const nodeIndex)\n{\n    OnnxAttrs attrs(node, ctx);\n    if (ctx->getOpsetVersion() >= 10)\n    {\n        auto const dilations = attrs.get<std::vector<int32_t>>(\"dilations\", std::vector<int32_t>(2, 1));\n        for (size_t i = 0; i < dilations.size(); i++)\n            STATIC_CHECK((dilations[i] == 1) && \"This version of TensorRT does not support dilations other than 1.\",\n                ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n    }\n}\n\nvoid randomUniformCheckHelper(\n    ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, std::vector<Status>& errors, size_t const nodeIndex)\n{\n    OnnxAttrs attrs(node, ctx);\n\n    // Set datatype of output:\n    //      RandomUniform: dype is required and defaults to 1\n    //      RandomUniformLike: dtype is optional and defaults to the same type as the input\n    if (attrs.count(\"dtype\"))\n    {\n        auto dtype = attrs.get<int32_t>(\"dtype\", 1);\n        if (dtype != ::ONNX_NAMESPACE::TensorProto::FLOAT && dtype != ::ONNX_NAMESPACE::TensorProto::FLOAT16)\n        {\n            ADD_STATIC_ERROR(\n                \"Unsupported data type in randomUniform\", ErrorCode::kINVALID_VALUE, node, nodeIndex, errors);\n        }\n    }\n}\n\nvoid randomNormalCheckHelper(\n    ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, std::vector<Status>& errors, size_t const nodeIndex)\n{\n    OnnxAttrs attrs(node, ctx);\n\n    // Set datatype of output:\n    //      RandomNormal: dype is required and defaults to 1\n    //      RandomNormalLike: dtype is optional and defaults to the same type as the input\n    if (attrs.count(\"dtype\"))\n    {\n        auto dtype = attrs.get<int32_t>(\"dtype\", 1);\n        if (dtype != ::ONNX_NAMESPACE::TensorProto::FLOAT && dtype != ::ONNX_NAMESPACE::TensorProto::FLOAT16)\n        {\n            ADD_STATIC_ERROR(\n                \"Unsupported data type in randomNormal\", ErrorCode::kINVALID_VALUE, node, nodeIndex, errors);\n        }\n    }\n}\n\nvoid emptyOutputChecker(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, std::vector<Status>& errors,\n    size_t const nodeIndex, int32_t numSupportedOutputs)\n{\n    int32_t numOutputs = node.output().size();\n    for (int32_t i = 0; i < numOutputs; i++)\n    {\n        if (i < numSupportedOutputs)\n        {\n            continue;\n        }\n        if (!node.output(i).empty())\n        {\n            std::ostringstream ssMsg{};\n            ssMsg << \"This version of TensorRT doesn't support mode than \" << numSupportedOutputs << \" outputs for \"\n                  << node.op_type() << \" nodes!\";\n            ADD_STATIC_ERROR(ssMsg.str(), ErrorCode::kUNSUPPORTED_NODE, node, nodeIndex, errors);\n        }\n    }\n}\n\nDEFINE_OP_EMPTY_CHECKER(Abs)\n\nDEFINE_OP_EMPTY_CHECKER(Acos)\n\nDEFINE_OP_EMPTY_CHECKER(Acosh)\n\nDEFINE_OP_EMPTY_CHECKER(And)\n\nDEFINE_OP_EMPTY_CHECKER(Asin)\n\nDEFINE_OP_EMPTY_CHECKER(Asinh)\n\nDEFINE_OP_EMPTY_CHECKER(Atan)\n\nDEFINE_OP_EMPTY_CHECKER(Atanh)\n\nDEFINE_OP_EMPTY_CHECKER(Add)\n\nDEFINE_OP_CHECKER(ArgMax)\n{\n    argMinMaxCheckHelper(ctx, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(ArgMin)\n{\n    argMinMaxCheckHelper(ctx, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(AveragePool)\n{\n    poolingCheckHelper(ctx, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(BatchNormalization)\n{\n    OnnxAttrs attrs(node, ctx);\n    auto const isTraining = attrs.get<int32_t>(\"training_mode\", 0);\n    STATIC_CHECK(!isTraining && \"This version of TensorRT does not support training_mode == 1 in BatchNormalization.\",\n        ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n    // Also check the number of outputs. TRT only supports the first output.\n    emptyOutputChecker(ctx, node, errors, nodeIndex, 1);\n}\n\nDEFINE_OP_EMPTY_CHECKER(BlackmanWindow)\n\nDEFINE_OP_CHECKER(Cast)\n{\n    OnnxAttrs attrs(node, ctx);\n    // Get data type to cast to.\n    auto onnxType = attrs.get<int32_t>(\"to\");\n    DataType newType{DataType::kFLOAT};\n    STATIC_CHECK(convertDtype(onnxType, &newType) && \"Unsupported data type for the Cast operator!\",\n        ErrorCode::kINVALID_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_EMPTY_CHECKER(CastLike)\n\nDEFINE_OP_EMPTY_CHECKER(Ceil)\n\nDEFINE_OP_EMPTY_CHECKER(Celu)\n\nDEFINE_OP_EMPTY_CHECKER(Clip)\n\nDEFINE_OP_EMPTY_CHECKER(Concat)\n\nDEFINE_OP_CHECKER(Constant)\n{\n    OnnxAttrs attrs(node, ctx);\n    // Having the trt_outputs_range_min attributes means it's from\n    // serialized iNetworkDefinition which does not have this check.\n    if (attrs.get<std::vector<float>>(\"trt_outputs_range_min\", {}).empty())\n    {\n        STATIC_CHECK((!attrs.count(\"sparse_value\")) && (!attrs.count(\"value_string\")) && (!attrs.count(\"value_strings\"))\n            && \"This version of TensorRT does not support the sparse_value, value_string and value_strings attributes.\",\n        ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n    }\n}\n\nDEFINE_OP_CHECKER(ConstantOfShape)\n{\n    OnnxAttrs attrs(node, ctx);\n    ShapedWeights zeroWeights\n        = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto_DataType_FLOAT, nvinfer1::Dims{1, {1}});\n    static_cast<float*>(zeroWeights.values)[0] = 0.f;\n    auto valueWeights = TensorOrWeights{attrs.get(\"value\", zeroWeights)};\n    STATIC_CHECK(notInvalidType(valueWeights, {\"UINT8\"}, node, nodeIndex).is_success()\n            && \"Invalid input type for ConstantOfShape\",\n        ErrorCode::kUNSUPPORTED_NODE_DATATYPE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_EMPTY_CHECKER(Conv)\n\nDEFINE_OP_EMPTY_CHECKER(ConvTranspose)\n\nDEFINE_OP_EMPTY_CHECKER(Cos)\n\nDEFINE_OP_EMPTY_CHECKER(Cosh)\n\nDEFINE_OP_EMPTY_CHECKER(CumSum)\n\nDEFINE_OP_EMPTY_CHECKER(DeformConv)\n\nDEFINE_OP_EMPTY_CHECKER(DepthToSpace)\n\nDEFINE_OP_EMPTY_CHECKER(QuantizeLinear)\n\nDEFINE_OP_EMPTY_CHECKER(DequantizeLinear)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_FP8QuantizeLinear)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_FP8DequantizeLinear)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_INT4QuantizeLinear)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_INT4DequantizeLinear)\n\n\nDECLARE_OP_CHECKER(Mul);\n\nDEFINE_OP_EMPTY_CHECKER(Div)\n\nDEFINE_OP_CHECKER(Dropout)\n{\n    // TensorRT does not support the Dropout operator with training mode.\n    if (ctx->getOpsetVersion() <= 6)\n    {\n        OnnxAttrs attrs(node, ctx);\n        int32_t isTestingMode = attrs.get<int32_t>(\"is_test\", 1);\n        STATIC_CHECK(isTestingMode && \"TensorRT does not support the Droupout operator with training mode.\",\n            ErrorCode::kUNSUPPORTED_NODE_ATTR, node, errors, nodeIndex);\n    }\n}\n\nDEFINE_OP_CHECKER(Einsum)\n{\n    OnnxAttrs attrs(node, ctx);\n    std::string equation = attrs.get<std::string>(\"equation\");\n    std::string invalidCharacters;\n    for (char c : equation)\n    {\n        if ((c < 'a' || c > 'z') && c != '-' && c != '>' && c != '.' && c != ',' && c != ' ')\n        {\n            invalidCharacters.push_back(c);\n            invalidCharacters.push_back(',');\n        }\n    }\n    if (!invalidCharacters.empty())\n    {\n        invalidCharacters.pop_back();\n        ADD_STATIC_ERROR(\"Invalid character(s) in Einsum equation: \" + invalidCharacters, ErrorCode::kINVALID_NODE,\n            node, nodeIndex, errors);\n    }\n}\n\nDEFINE_OP_EMPTY_CHECKER(Elu)\n\nDEFINE_OP_EMPTY_CHECKER(Equal)\n\nDEFINE_OP_EMPTY_CHECKER(Erf)\n\nDEFINE_OP_EMPTY_CHECKER(Exp)\n\nDEFINE_OP_EMPTY_CHECKER(Expand)\n\nDEFINE_OP_EMPTY_CHECKER(EyeLike)\n\nDEFINE_OP_EMPTY_CHECKER(Flatten)\n\nDEFINE_OP_EMPTY_CHECKER(Floor)\n\nDEFINE_OP_EMPTY_CHECKER(Gather)\n\nDEFINE_OP_EMPTY_CHECKER(GatherElements)\n\nDEFINE_OP_EMPTY_CHECKER(GatherND)\n\nDEFINE_OP_EMPTY_CHECKER(Gelu)\n\nDEFINE_OP_EMPTY_CHECKER(Gemm)\n\nDEFINE_OP_EMPTY_CHECKER(GlobalAveragePool)\n\nDEFINE_OP_EMPTY_CHECKER(GlobalLpPool)\n\nDEFINE_OP_EMPTY_CHECKER(GlobalMaxPool)\n\nDEFINE_OP_EMPTY_CHECKER(Greater)\n\nDEFINE_OP_EMPTY_CHECKER(GreaterOrEqual)\n\nDEFINE_OP_EMPTY_CHECKER(GroupNormalization)\n\nDEFINE_OP_CHECKER(GRU)\n{\n    using trtAct = nvinfer1::ActivationType;\n\n    OnnxAttrs attrs{node, ctx};\n\n    std::string const direction = attrs.get<std::string>(\"direction\", \"forward\");\n    int32_t const numDirections = (direction == \"bidirectional\") ? 2 : 1;\n\n    constexpr int32_t NUM_ACTIVATIONS = 2;\n    std::vector<trtAct> defaultActs{trtAct::kSIGMOID, trtAct::kTANH};\n    if (numDirections == 2)\n    {\n        defaultActs.insert(defaultActs.end(), {trtAct::kSIGMOID, trtAct::kTANH});\n    }\n    std::vector<trtAct> activations = attrs.get<std::vector<trtAct>>(\"activations\", defaultActs);\n\n    std::vector<float> activationAlphas = attrs.get<std::vector<float>>(\"activation_alpha\", std::vector<float>{});\n    std::transform(activations.begin() + activationAlphas.size(), activations.end(),\n        std::back_inserter(activationAlphas), &getActivationDefaultAlpha);\n\n    std::vector<float> activationBetas = attrs.get<std::vector<float>>(\"activation_beta\", std::vector<float>{});\n    std::transform(activations.begin() + activationBetas.size(), activations.end(), std::back_inserter(activationBetas),\n        &getActivationDefaultBeta);\n\n    // TODO: Support cases where in bidirectional GRUs, activations of reverse iteration do not match forward pass.\n    // TODO: This will require splitting the input tensor in the loop when applying activations.\n    if (numDirections == 2)\n    {\n        STATIC_CHECK(std::equal(activations.begin(), activations.begin() + NUM_ACTIVATIONS, activations.begin() + NUM_ACTIVATIONS)\n            && \"The parser does not currently support cases where activations for the reverse pass of the GRU do not match the forward pass.\", ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n        STATIC_CHECK(std::equal(activationAlphas.begin(), activationAlphas.begin() + NUM_ACTIVATIONS, activationAlphas.begin() + NUM_ACTIVATIONS)\n            && \"The parser does not currently support cases where activations for the reverse pass of the GRU do not match the forward pass.\", ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n        STATIC_CHECK(std::equal(activationBetas.begin(), activationBetas.begin() + NUM_ACTIVATIONS, activationBetas.begin() + NUM_ACTIVATIONS)\n            && \"The parser does not currently support cases where activations for the reverse pass of the GRU do not match the forward pass.\", ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n    }\n}\n\nDEFINE_OP_EMPTY_CHECKER(HammingWindow)\n\nDEFINE_OP_EMPTY_CHECKER(HannWindow)\n\nDEFINE_OP_EMPTY_CHECKER(Hardmax)\n\nDEFINE_OP_EMPTY_CHECKER(HardSigmoid)\n\nDEFINE_OP_EMPTY_CHECKER(Identity)\n\nDEFINE_OP_CHECKER(If)\n{\n    OnnxAttrs attrs(node, ctx);\n    ::ONNX_NAMESPACE::GraphProto const& thenGraph = attrs.get<::ONNX_NAMESPACE::GraphProto const&>(\"then_branch\");\n    ::ONNX_NAMESPACE::GraphProto const& elseGraph = attrs.get<::ONNX_NAMESPACE::GraphProto const&>(\"else_branch\");\n\n    // Number of outputs are the same between the two branches.\n    STATIC_CHECK(thenGraph.output_size() == elseGraph.output_size()\n            && \"then/else subgraphs should have the same number of outputs.\",\n        ErrorCode::kUNSUPPORTED_NODE_ATTR, node, errors, nodeIndex);\n}\n\nDEFINE_OP_EMPTY_CHECKER(ImageScaler)\n\nDEFINE_OP_EMPTY_CHECKER(InstanceNormalization)\n\nDEFINE_OP_EMPTY_CHECKER(IsInf)\n\nDEFINE_OP_EMPTY_CHECKER(IsNaN)\n\nDEFINE_OP_CHECKER(LayerNormalization)\n{\n    // TRT only expects one valid output. Other outputs are training artifacts that should've been removed for inference graphs.\n    emptyOutputChecker(ctx, node, errors, nodeIndex, 1);\n}\n\nDEFINE_OP_EMPTY_CHECKER(LeakyRelu)\n\nDEFINE_OP_EMPTY_CHECKER(Less)\n\nDEFINE_OP_EMPTY_CHECKER(LessOrEqual)\n\nDEFINE_OP_EMPTY_CHECKER(Log)\n\nDEFINE_OP_EMPTY_CHECKER(LogSoftmax)\n\nDEFINE_OP_EMPTY_CHECKER(Loop)\n\nDEFINE_OP_EMPTY_CHECKER(LRN)\n\nDEFINE_OP_CHECKER(LSTM)\n{\n    using trtAct = nvinfer1::ActivationType;\n\n    OnnxAttrs attrs{node, ctx};\n    std::string const direction = attrs.get<std::string>(\"direction\", \"forward\");\n    int32_t const numDirections = (direction == \"bidirectional\") ? 2 : 1;\n    int32_t const inputForget = attrs.get(\"input_forget\", 0);\n\n    STATIC_CHECK(inputForget == 0 && \"Coupled input/forget is unsupported in the LSTM converter\",\n        ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n\n    constexpr int32_t NUM_ACTIVATIONS = 3;\n    std::vector<trtAct> defaultActs{trtAct::kSIGMOID, trtAct::kTANH, trtAct::kTANH};\n    if (numDirections == 2)\n    {\n        defaultActs.insert(defaultActs.end(), {trtAct::kSIGMOID, trtAct::kTANH, trtAct::kTANH});\n    }\n    std::vector<trtAct> activations = attrs.get<std::vector<trtAct>>(\"activations\", defaultActs);\n\n    std::vector<float> activationAlphas = attrs.get<std::vector<float>>(\"activation_alpha\", std::vector<float>{});\n    activationAlphas = parseLSTMActivationValues(activations, activationAlphas, true);\n\n    std::vector<float> activationBetas = attrs.get<std::vector<float>>(\"activation_beta\", std::vector<float>{});\n    activationBetas = parseLSTMActivationValues(activations, activationBetas, false);\n\n    // TODO: Support cases where in bidirectional LSTMs, activations of reverse iteration do not match forward pass.\n    // TODO: This will require splitting the input tensor in the loop when applying activations.\n    if (numDirections == 2)\n    {\n        STATIC_CHECK(std::equal(activations.begin(), activations.begin() + NUM_ACTIVATIONS, activations.begin() + NUM_ACTIVATIONS)\n            && \"The parser does not currently support cases where activations for the reverse pass of the LSTM do not match the forward pass.\", ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n        STATIC_CHECK(std::equal(activationAlphas.begin(), activationAlphas.begin() + NUM_ACTIVATIONS, activationAlphas.begin() + NUM_ACTIVATIONS)\n            && \"The parser does not currently support cases where activation alphas for the reverse pass of the LSTM do not match the forward pass.\", ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n        STATIC_CHECK(std::equal(activationBetas.begin(), activationBetas.begin() + NUM_ACTIVATIONS, activationBetas.begin() + NUM_ACTIVATIONS)\n            && \"The parser does not currently support cases where activation betas for the reverse pass of the LSTM do not match the forward pass.\", ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n    }\n}\n\nDEFINE_OP_CHECKER(LpNormalization)\n{\n    OnnxAttrs attrs(node, ctx);\n    int32_t p = attrs.get<int32_t>(\"p\", 2);\n\n    STATIC_CHECK((p == 1 || p == 2) && \"Only L1 and L2 normalization are supported.\", ErrorCode::kUNSUPPORTED_NODE,\n        node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(LpPool)\n{\n    OnnxAttrs attrs(node, ctx);\n    int32_t p = attrs.get<int32_t>(\"p\", 2);\n\n    STATIC_CHECK((p == 1 || p == 2) && \"Only L1 and L2 normalization are supported.\", ErrorCode::kUNSUPPORTED_NODE,\n        node, errors, nodeIndex);\n    poolingCheckHelper(ctx, node, errors, nodeIndex);\n}\n\nDEFINE_OP_EMPTY_CHECKER(MatMul)\n\nDEFINE_OP_EMPTY_CHECKER(Max)\n\nDEFINE_OP_CHECKER(MaxPool)\n{\n    // TRT only expects one valid output. `Indices` output is unsupported.\n    emptyOutputChecker(ctx, node, errors, nodeIndex, 1);\n    poolingCheckHelper(ctx, node, errors, nodeIndex);\n}\n\nDEFINE_OP_EMPTY_CHECKER(Mean)\n\nDEFINE_OP_EMPTY_CHECKER(MeanVarianceNormalization)\n\nDEFINE_OP_EMPTY_CHECKER(Min)\n\nDEFINE_OP_EMPTY_CHECKER(Mul)\n\nDEFINE_OP_EMPTY_CHECKER(Mod)\n\nDEFINE_OP_EMPTY_CHECKER(Neg)\n\nDEFINE_OP_EMPTY_CHECKER(NonMaxSuppression)\n\nDEFINE_OP_EMPTY_CHECKER(Not)\n\nDEFINE_OP_EMPTY_CHECKER(OneHot)\n\nDEFINE_OP_EMPTY_CHECKER(Or)\n\nDEFINE_OP_EMPTY_CHECKER(Pad)\n\nDEFINE_OP_EMPTY_CHECKER(ParametricSoftplus)\n\nDEFINE_OP_EMPTY_CHECKER(Pow)\n\nDEFINE_OP_EMPTY_CHECKER(PRelu)\n\nDEFINE_OP_CHECKER(RandomUniform)\n{\n    randomUniformCheckHelper(ctx, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(RandomUniformLike)\n{\n    randomUniformCheckHelper(ctx, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(RandomNormal)\n{\n    randomNormalCheckHelper(ctx, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(RandomNormalLike)\n{\n    randomNormalCheckHelper(ctx, node, errors, nodeIndex);\n}\n\nDEFINE_OP_EMPTY_CHECKER(Range)\n\nDEFINE_OP_EMPTY_CHECKER(Reciprocal)\n\nDEFINE_OP_EMPTY_CHECKER(ReduceL1)\n\nDECLARE_OP_CHECKER(ReduceSum);\n\nDEFINE_OP_EMPTY_CHECKER(ReduceLogSum)\n\nDEFINE_OP_EMPTY_CHECKER(ReduceLogSumExp)\n\nDECLARE_OP_CHECKER(ReduceSumSquare);\n\nDEFINE_OP_EMPTY_CHECKER(ReduceL2)\n\nDEFINE_OP_EMPTY_CHECKER(ReduceMax)\n\nDEFINE_OP_EMPTY_CHECKER(ReduceMean)\n\nDEFINE_OP_EMPTY_CHECKER(ReduceMin)\n\nDEFINE_OP_EMPTY_CHECKER(ReduceProd)\n\nDEFINE_OP_EMPTY_CHECKER(ReduceSum)\n\nDEFINE_OP_EMPTY_CHECKER(ReduceSumSquare)\n\nDEFINE_OP_EMPTY_CHECKER(Relu)\n\nDEFINE_OP_EMPTY_CHECKER(Sign)\n\nDEFINE_OP_EMPTY_CHECKER(Round)\n\nDEFINE_OP_CHECKER(Resize)\n{\n    OnnxAttrs attrs(node, ctx);\n\n    auto mode = attrs.get<std::string>(\"mode\", \"nearest\");\n    if (mode != \"cubic\" || mode != \"linear\")\n    {\n        STATIC_CHECK((mode == \"cubic\" || mode == \"linear\" || mode == \"nearest\") && \"Invalid Resize mode\",\n            ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n    }\n\n    // set transformation\n    std::string transformationMode = \"half_pixel\";\n\n    if (ctx->getOpsetVersion() >= 11)\n    {\n        // Check for TRT-supported resize attributes\n        transformationMode = attrs.get<std::string>(\"coordinate_transformation_mode\", \"half_pixel\");\n        auto const nearest_mode = attrs.get<std::string>(\"nearest_mode\", \"round_prefer_floor\");\n\n        STATIC_CHECK((transformationMode != \"tf_half_pixel_for_nn\" || nearest_mode == \"round_prefer_floor\")\n                && \"This version of TensorRT only support round_prefer_floor nearest mode in tf_half_pixel_for_nn!\",\n            ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n\n        STATIC_CHECK((transformationMode == \"align_corners\" || transformationMode == \"tf_half_pixel_for_nn\" || transformationMode == \"pytorch_half_pixel\"\n                || transformationMode == \"half_pixel\" || transformationMode == \"asymmetric\")\n                && \"TensorRT only supports half_pixel, pytorch_half_pixel, tf_half_pixel_for_nn, asymmetric and \"\n                \"align_corners transformation modes!\", ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n    }\n\n    // Antialiasing is not supported currently.\n    auto const antialias = attrs.get<int32_t>(\"antialias\", 0);\n    STATIC_CHECK((antialias == 0) && \"Antialiasing is not supported currently.\", ErrorCode::kUNSUPPORTED_NODE_ATTR,\n        node, errors, nodeIndex);\n\n    // Only stretch keep_aspect_ratio_policy is supported currently.\n    auto const keep_aspect_ratio_policy = attrs.get<std::string>(\"keep_aspect_ratio_policy\", \"stretch\");\n    STATIC_CHECK((keep_aspect_ratio_policy == \"stretch\")\n            && \"Only `stretch` is supported currently as `keep_aspect_ratio_policy`.\",\n        ErrorCode::kUNSUPPORTED_NODE_ATTR, node, errors, nodeIndex);\n\n    // Axes provided must be unique.\n    auto const resizeAxes = attrs.get<std::vector<int32_t>>(\"axes\", std::vector<int32_t>());\n    STATIC_CHECK((std::unordered_set<int32_t>(resizeAxes.begin(), resizeAxes.end()).size() == resizeAxes.size())\n            && \"The input axes must have unique elements.\",\n        ErrorCode::kINVALID_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_EMPTY_CHECKER(Reshape)\n\nDEFINE_OP_CHECKER(ReverseSequence)\n{\n    OnnxAttrs attrs{node, ctx};\n    int32_t const batchAxis = attrs.get<int32_t>(\"batch_axis\", 1);\n    int32_t const sequenceAxis = attrs.get<int32_t>(\"time_axis\", 0);\n    STATIC_CHECK((batchAxis != sequenceAxis) && \"batch_axis and time_axis cannot be the same\",\n        ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(RNN)\n{\n    OnnxAttrs attrs{node, ctx};\n\n    const std::string direction = attrs.get<std::string>(\"direction\", \"forward\");\n    const int32_t numDirections = (direction == \"bidirectional\") ? 2 : 1;\n\n    constexpr int32_t NUM_ACTIVATIONS = 1;\n    std::vector<nvinfer1::ActivationType> defaultActs{nvinfer1::ActivationType::kTANH};\n    if (numDirections == 2)\n    {\n        defaultActs.insert(defaultActs.end(), {nvinfer1::ActivationType::kTANH});\n    }\n    std::vector<nvinfer1::ActivationType> activations\n        = attrs.get<std::vector<nvinfer1::ActivationType>>(\"activations\", defaultActs);\n\n    std::vector<float> activationAlphas = attrs.get<std::vector<float>>(\"activation_alpha\", std::vector<float>{});\n    std::transform(activations.begin() + activationAlphas.size(), activations.end(),\n        std::back_inserter(activationAlphas), &getActivationDefaultAlpha);\n\n    std::vector<float> activationBetas = attrs.get<std::vector<float>>(\"activation_beta\", std::vector<float>{});\n    std::transform(activations.begin() + activationBetas.size(), activations.end(), std::back_inserter(activationBetas),\n        &getActivationDefaultBeta);\n\n    // TODO: Support cases where in bidirectional RNNs, activations of reverse iteration do not match forward pass.\n    // TODO: This will require splitting the input tensor in the loop when applying activations.\n    if (numDirections == 2)\n    {\n        STATIC_CHECK(std::equal(activations.begin(), activations.begin() + NUM_ACTIVATIONS, activations.begin() + NUM_ACTIVATIONS)\n            && \"The parser does not currently support cases where activations for the reverse pass of the RNN do not match the forward pass.\", ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n        STATIC_CHECK(std::equal(activationAlphas.begin(), activationAlphas.begin() + NUM_ACTIVATIONS, activationAlphas.begin() + NUM_ACTIVATIONS)\n            && \"The parser does not currently support cases where activations for the reverse pass of the RNN do not match the forward pass.\", ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n        STATIC_CHECK(std::equal(activationBetas.begin(), activationBetas.begin() + NUM_ACTIVATIONS, activationBetas.begin() + NUM_ACTIVATIONS)\n            && \"The parser does not currently support cases where activations for the reverse pass of the RNN do not match the forward pass.\", ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n    }\n}\n\nDEFINE_OP_CHECKER(RoiAlign)\n{\n    OnnxAttrs attrs(node, ctx);\n    auto mode = attrs.get(\"mode\", std::string(\"avg\"));\n    STATIC_CHECK((mode == \"avg\" || mode == \"max\") && \"Mode must be avg or max!\", ErrorCode::kINVALID_NODE, node, errors,\n        nodeIndex);\n\n    auto samplingRatio = attrs.get<int32_t>(\"sampling_ratio\", 0);\n    STATIC_CHECK(\n        samplingRatio >= 0 && \"Sampling ratio cannot be negative!\", ErrorCode::kINVALID_NODE, node, errors, nodeIndex);\n\n    // Opset 16 attributes\n    if (ctx->getOpsetVersion() >= 16)\n    {\n        auto ctm = attrs.get(\"coordinate_transformation_mode\", std::string(\"half_pixel\"));\n        STATIC_CHECK((ctm == \"half_pixel\" || ctm == \"output_half_pixel\")\n                && \"Coordinate transformation mode must be half_pixel or output_half_pixel!\",\n            ErrorCode::kINVALID_NODE, node, errors, nodeIndex);\n    }\n}\n\nDEFINE_OP_EMPTY_CHECKER(ScaledTanh)\n\nDEFINE_OP_EMPTY_CHECKER(Scan)\n\nDEFINE_OP_EMPTY_CHECKER(GridSample)\n\nDEFINE_OP_EMPTY_CHECKER(ScatterND)\n\nDEFINE_OP_EMPTY_CHECKER(ScatterElements)\n\nDEFINE_OP_EMPTY_CHECKER(Scatter)\n\nDEFINE_OP_EMPTY_CHECKER(Selu)\n\nDEFINE_OP_EMPTY_CHECKER(Shape)\n\nDEFINE_OP_EMPTY_CHECKER(Sigmoid)\n\nDEFINE_OP_EMPTY_CHECKER(Sin)\n\nDEFINE_OP_EMPTY_CHECKER(Sinh)\n\nDEFINE_OP_EMPTY_CHECKER(Size)\n\nDEFINE_OP_CHECKER(Slice)\n{\n    int32_t const nbInputs = node.input().size();\n\n    if (ctx->getOpsetVersion() >= 10)\n    {\n        STATIC_CHECK((nbInputs >= 3 && nbInputs <= 5) && \"Post-opset 10 Slice operator requires 3 - 5 inputs.\",\n            ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n    }\n}\n\nDEFINE_OP_EMPTY_CHECKER(Softmax)\n\nDEFINE_OP_EMPTY_CHECKER(Softsign)\n\nDEFINE_OP_EMPTY_CHECKER(Softplus)\n\nDEFINE_OP_EMPTY_CHECKER(SpaceToDepth)\n\nDEFINE_OP_EMPTY_CHECKER(Split)\n\nDEFINE_OP_EMPTY_CHECKER(Sqrt)\n\nDEFINE_OP_EMPTY_CHECKER(Squeeze)\n\nDEFINE_OP_EMPTY_CHECKER(Sub)\n\nDEFINE_OP_EMPTY_CHECKER(Sum)\n\nDEFINE_OP_EMPTY_CHECKER(Tan)\n\nDEFINE_OP_EMPTY_CHECKER(Tanh)\n\nDEFINE_OP_EMPTY_CHECKER(ThresholdedRelu)\n\nDEFINE_OP_EMPTY_CHECKER(Tile)\n\nDEFINE_OP_CHECKER(TopK)\n{\n    OnnxAttrs attrs(node, ctx);\n\n    if (ctx->getOpsetVersion() < 10)\n    {\n        STATIC_CHECK(\n            (attrs.count(\"k\")) && \"Attribute k is missing.\", ErrorCode::kINVALID_NODE, node, errors, nodeIndex);\n    }\n}\n\nDEFINE_OP_EMPTY_CHECKER(Transpose)\n\nDEFINE_OP_EMPTY_CHECKER(Trilu)\n\nDEFINE_OP_EMPTY_CHECKER(Unsqueeze)\n\nDEFINE_OP_CHECKER(Upsample)\n{\n    OnnxAttrs attrs(node, ctx);\n    auto mode = attrs.get<std::string>(\"mode\", \"nearest\");\n    STATIC_CHECK((mode == \"nearest\" || mode == \"linear\" || mode == \"bilinear\")\n            && \"The attribute mode can only be nearest, linear, or bilinear.\",\n        ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_EMPTY_CHECKER(Where)\n\nDEFINE_OP_EMPTY_CHECKER(Xor)\n\nDEFINE_OP_EMPTY_CHECKER(Shrink)\n\nDEFINE_OP_EMPTY_CHECKER(HardSwish)\n\nDEFINE_OP_EMPTY_CHECKER(NonZero)\n\nDEFINE_OP_EMPTY_CHECKER(Mish)\n\n// Any ops that are not supported will attempt to import as plugins.\nDEFINE_OP_CHECKER(FallbackPluginImporter)\n{\n    OnnxAttrs attrs(node, ctx);\n    std::string const pluginName{node.op_type()};\n    std::string const pluginVersion{attrs.get<std::string>(\"plugin_version\", \"1\")};\n    std::string const pluginNamespace{attrs.get<std::string>(\"plugin_namespace\", \"\")};\n\n    nvinfer1::IPluginCreatorInterface* creator = importPluginCreator(ctx, pluginName, pluginVersion, pluginNamespace);\n    STATIC_CHECK(creator && \"Plugin not found, are the plugin name, version, and namespace correct?\",\n        nvonnxparser::ErrorCode::kINVALID_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(LocalFunctionImporter)\n{\n    auto function = ctx->localFunctions().at(node.op_type());\n    STATIC_CHECK(node.input().size() == function.input().size(), ErrorCode::kINVALID_NODE, node, errors, nodeIndex);\n\n    // Create attribute map for the local function instance. Attributes can have default values (from the parent\n    // FunctionProto) or local values (from the NodeProto instance of the Function).\n\n    StringMap<::ONNX_NAMESPACE::AttributeProto const*> attrMap;\n    // Add local values first as they override any default values.\n    for (auto const& attr : node.attribute())\n    {\n        attrMap.insert({attr.name(), &attr});\n    }\n    // Add default values\n    for (auto const& attr : function.attribute_proto())\n    {\n        attrMap.insert({attr.name(), &attr});\n    }\n\n    // Push current function name to top of stack in order to properly set layer metadata and track attributes\n    ctx->localFunctionStack().push_back({node.op_type(), getNodeName(node), attrMap});\n\n    for (auto const& node : function.node())\n    {\n        onnx2trt::parseNodeStaticCheck(ctx, node, errors, nodeIndex);\n    }\n\n    // Pop the current function name from stack\n    ctx->localFunctionStack().pop_back();\n}\n\nDEFINE_OP_EMPTY_CHECKER(TRT_Scale)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_Shuffle)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_TopK_Min)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_MatMul)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_RNNv2)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_RaggedSoftmax)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_FullyConnected)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_MaxAverageBlendPool)\n\n#if ENABLE_STD_PLUGIN\nDEFINE_OP_EMPTY_CHECKER(TRT_PluginV2)\n#endif // ENABLE_STD_PLUGIN\n\nDEFINE_OP_EMPTY_CHECKER(TRT_Gather)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_Slice)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_Resize)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_FloorDiv)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_Conv)\n\nDEFINE_OP_EMPTY_CHECKER(TRT_Deconv)\n\nDEFINE_OP_CHECKER(TRT_MaxPool)\n{\n    checkMaxPool(ctx, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(TRT_AveragePool)\n{\n    checkAveragePool(ctx, node, errors, nodeIndex);\n}\n\n// Define unsupported node checkers\nDEFINE_OP_CHECKER(BitShift)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(BitwiseAnd)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(BitwiseNot)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(BitwiseOr)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(BitwiseXor)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(Col2Im)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(Compress)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(ConcatFromSequence)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(ConvInteger)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(DFT)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(Det)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(ImageDecoder)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(MatMulInteger)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(MaxRoiPool)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(MaxUnpool)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(MelWeightMatrix)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(Multinomial)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(Optional)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(OptionalGetElement)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(OptionalHasElement)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(QLinearConv)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(QLinearMatMul)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(RegexFullMatch)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_EMPTY_CHECKER(STFT)\n\nDEFINE_OP_CHECKER(SequenceAt)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(SequenceConstruct)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(SequenceEmpty)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(SequenceErase)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(SequenceInsert)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(SequenceLength)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(SplitToSequence)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(StringConcat)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(StringNormalizer)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(StringSplit)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(TfIdfVectorizer)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(Unique)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(AffineGrid)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(Bernoulli)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(CenterCropPad)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(DynamicQuantizeLinear)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(NegativeLogLikelihoodLoss)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(SequenceMap)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\nDEFINE_OP_CHECKER(SoftmaxCrossEntropyLoss)\n{\n    STATIC_CHECK(false, ErrorCode::kUNSUPPORTED_NODE, node, errors, nodeIndex);\n}\n\n} // namespace\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "onnxOpCheckers.hpp",
          "type": "blob",
          "size": 0.1982421875,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include \"ImporterContext.hpp\"\n\nnamespace onnx2trt\n{\n\nStringMap<OpStaticErrorChecker>& getOpStaticErrorCheckerMap();\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "onnxOpImporters.cpp",
          "type": "blob",
          "size": 316.212890625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#if defined(_MSC_VER)\n#define _USE_MATH_DEFINES\n#endif\n#include <cmath>\n\n#include \"ConditionalHelpers.hpp\"\n#include \"LoopHelpers.hpp\"\n#include \"ModelImporter.hpp\"\n#include \"NvInfer.h\"\n#include \"NvInferPlugin.h\"\n#include \"NvInferRuntime.h\"\n#include \"OnnxAttrs.hpp\"\n#include \"RNNHelpers.hpp\"\n#include \"ShapeTensor.hpp\"\n#include \"bfloat16.hpp\"\n#include \"errorHelpers.hpp\"\n#include \"half.h\"\n#include \"importerUtils.hpp\"\n#include \"onnxOpImporters.hpp\"\n\n#include <algorithm> // For std::min, std::max\n#include <array>\n#include <cstring> // For std::memcpy, std::memset\n#include <iostream>\n#include <iterator>\n#include <numeric> // For std::iota\n#include <sstream>\n#include <tuple>\n#include <unordered_set>\n\nnamespace onnx2trt\n{\n\nStringMap<NodeImporter>& getBuiltinOpImporterMap()\n{\n    static StringMap<NodeImporter> builtin_op_importers;\n    return builtin_op_importers;\n}\n\nnamespace\n{\n\nusing nvinfer1::DataType;\n\n#define IGNORE_UNUSED_GLOBAL(x)                                                                                        \\\n    static void _ignore_unused2_##x();                                                                                 \\\n    static void _ignore_unused1_##x()                                                                                  \\\n    {                                                                                                                  \\\n        (void) _ignore_unused2_##x;                                                                                    \\\n        (void) x;                                                                                                      \\\n    }                                                                                                                  \\\n    static void _ignore_unused2_##x()                                                                                  \\\n    {                                                                                                                  \\\n        (void) _ignore_unused1_##x;                                                                                    \\\n    }                                                                                                                  \\\n    struct SwallowSemicolon##x                                                                                         \\\n    {                                                                                                                  \\\n    }\n\n#define DECLARE_BUILTIN_OP_IMPORTER(op)                                                                                \\\n    NodeOutputs import##op(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx,        \\\n        std::vector<TensorOrWeights>& inputs)\n\n#define DEFINE_BUILTIN_OP_IMPORTER(op)                                                                                 \\\n    NodeOutputs import##op(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx,        \\\n        std::vector<TensorOrWeights>& inputs);                                                                         \\\n    static bool const op##_registered_builtin_op = registerBuiltinOpImporter(#op, import##op);                         \\\n    IGNORE_UNUSED_GLOBAL(op##_registered_builtin_op);                                                                  \\\n    NodeOutputs import##op(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const nodeIdx,        \\\n        std::vector<TensorOrWeights>& inputs)\n\n#define RETURN_FIRST_OUTPUT(layer, node, nodeIdx)                                                                      \\\n    do                                                                                                                 \\\n    {                                                                                                                  \\\n        nvinfer1::ILayer* layer_ptr = layer;                                                                           \\\n        ONNXTRT_CHECK_NODE(layer_ptr, \"Input layer is null.\", node, nodeIdx, ErrorCode::kINVALID_NODE);                \\\n        auto* output = N_CHECK(layer->getOutput(0));                                                                   \\\n        return {{output}};                                                                                             \\\n    } while (0)\n\n#define RETURN_IDENTITY(input, node, nodeIdx)                                                                          \\\n    do                                                                                                                 \\\n    {                                                                                                                  \\\n        TensorOrWeights output = identity(ctx, input);                                                                 \\\n        ONNXTRT_CHECK_NODE(output, \"Failed to add an identity layer.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);   \\\n        return {{output}};                                                                                             \\\n    } while (0)\n\n#define RETURN_ALL_OUTPUTS(layer, node, nodeIdx)                                                                       \\\n    do                                                                                                                 \\\n    {                                                                                                                  \\\n        nvinfer1::ILayer* layer_ptr = layer;                                                                           \\\n        ONNXTRT_CHECK_NODE(layer_ptr, \"The input layer is null.\", node, nodeIdx, ErrorCode::kINVALID_NODE);            \\\n        std::vector<TensorOrWeights> outputs;                                                                          \\\n        for (int i = 0; i < layer_ptr->getNbOutputs(); ++i)                                                            \\\n            outputs.push_back(N_CHECK(layer_ptr->getOutput(i)));                                                       \\\n        return {outputs};                                                                                              \\\n    } while (0)\n\nvoid assertIsWeights(TensorOrWeights const& input, std::string const& specificMsg)\n{\n    if (!input.is_weights())\n    {\n        std::ostringstream msg;\n        msg << specificMsg;\n        msg << \" Try applying constant folding on the model using Polygraphy: \"\n               \"https://github.com/NVIDIA/TensorRT/tree/master/tools/Polygraphy/examples/cli/surgeon/\"\n               \"02_folding_constants\";\n        throw std::runtime_error(msg.str());\n    }\n}\n\nbool registerBuiltinOpImporter(std::string op, NodeImporter const& importer)\n{\n    bool inserted = getBuiltinOpImporterMap().insert({op, importer}).second;\n    assert(inserted);\n    return inserted;\n}\n\nbool onlySupportInt32TRTPlugin(std::string const& pluginName)\n{\n    // TRT plugins that doesn't support INT64 as inputs, but support INT32.\n    static std::vector<std::string> const names = {\n        \"CustomQKVToContextPluginDynamic\",\n        \"EfficientNMS_TRT\",\n        \"EfficientNMS_ONNX_TRT\",\n        \"EfficientNMS_Implicit_TF_TRT\",\n        \"EfficientNMS_Explicit_TF_TRT\",\n        \"VoxelGeneratorPlugin\",\n        \"ScatterND\",\n        \"ROIAlign_TRT\",\n        \"PillarScatterPlugin\",\n        \"MultiscaleDeformableAttnPlugin_TRT\",\n        \"CustomEmbLayerNormPluginDynamic\",\n    };\n    return std::find(names.begin(), names.end(), pluginName) != names.end();\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Abs)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kABS);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Acos)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kACOS);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Acosh)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kACOSH);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(And)\n{\n    return elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kAND);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Asin)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kASIN);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Asinh)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kASINH);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Atan)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kATAN);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Atanh)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kATANH);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Add)\n{\n    return elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kSUM);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(ArgMax)\n{\n    return argMinMaxHelper(ctx, node, nodeIdx, inputs, nvinfer1::TopKOperation::kMAX);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(ArgMin)\n{\n    return argMinMaxHelper(ctx, node, nodeIdx, inputs, nvinfer1::TopKOperation::kMIN);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(AveragePool)\n{\n    return poolingHelper(ctx, node, nodeIdx, inputs, nvinfer1::PoolingType::kAVERAGE);\n}\n\nNodeOutputs batchnormFallback(\n    ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t nodeIdx, std::vector<TensorOrWeights>& inputs)\n{\n    using eOp = nvinfer1::ElementWiseOperation;\n    using uOp = nvinfer1::UnaryOperation;\n\n    nvinfer1::ITensor& input = convertToTensor(inputs.at(0), ctx);\n    int32_t const rank = input.getDimensions().nbDims;\n\n    std::array<nvinfer1::ITensor*, 4> tensors = {\n        &convertToTensor(inputs.at(1), ctx),\n        &convertToTensor(inputs.at(2), ctx),\n        &convertToTensor(inputs.at(3), ctx),\n        &convertToTensor(inputs.at(4), ctx),\n    };\n\n    // Alias tensors for convenience\n    auto& [scale, bias, mean, variance] = tensors;\n\n    // Reshape batchnorm weights from [C] to [N, C, ...] for elementwise operations.\n    bool const needsExpandDims = rank > 1;\n    if (needsExpandDims)\n    {\n        std::vector<int32_t> axes(rank - 1);\n        axes[0] = 0;\n        std::iota(axes.begin() + 1, axes.end(), 2);\n        for (auto*& t : tensors)\n        {\n            t = unsqueezeTensor(ctx, *t, axes);\n        }\n    }\n\n    OnnxAttrs attrs(node, ctx);\n    float eps = attrs.get<float>(\"epsilon\", 1e-5F);\n\n    nvinfer1::Dims scalarShape{rank};\n    std::fill(scalarShape.d, scalarShape.d + scalarShape.nbDims, 1);\n\n    auto varType = variance->getType();\n    nvinfer1::IConstantLayer* epsLayer;\n    if (varType == DataType::kHALF)\n    {\n        epsLayer = addConstantScalar(\n            ctx, static_cast<half_float::half>(eps), ::ONNX_NAMESPACE::TensorProto::FLOAT16, scalarShape);\n    }\n    else if (varType == DataType::kBF16)\n    {\n        epsLayer\n            = addConstantScalar(ctx, static_cast<BFloat16>(eps), ::ONNX_NAMESPACE::TensorProto::BFLOAT16, scalarShape);\n    }\n    else\n    {\n        epsLayer = addConstantScalar(ctx, eps, ::ONNX_NAMESPACE::TensorProto::FLOAT, scalarShape);\n    }\n    nvinfer1::ITensor* epsilon = N_CHECK(epsLayer->getOutput(0));\n\n    // For stronglyTyped networks, cast BatchNormalization parameters to the same type as the input type.\n    if (ctx->isStronglyTyped())\n    {\n        LOG_VERBOSE(\"Casting BatchNormalization parameters to the same type as input for StronglyTyped networks.\");\n        for (auto*& t : tensors)\n        {\n            t = castHelper(ctx, t, input.getType());\n        }\n        epsilon = castHelper(ctx, epsilon, input.getType());\n    }\n\n    // batchnorm = scale * (input - mean) / sqrt(variance + epsilon) + bias\n    // The WAR is split the single c++ code line into 3 to avoid the sequence swap by compiler.\n    nvinfer1::ITensor* divisor\n        = getUnaryResult(ctx, *getElementWiseResult(ctx, *variance, *epsilon, eOp::kSUM), uOp::kSQRT);\n    nvinfer1::ITensor* dividend = getElementWiseResult(ctx, input, *mean, eOp::kSUB);\n    auto intermediateResult\n        = getElementWiseResult(ctx, *scale, *getElementWiseResult(ctx, *dividend, *divisor, eOp::kDIV), eOp::kPROD);\n    nvinfer1::IElementWiseLayer* layer = N_CHECK(ctx->network()->addElementWise(*intermediateResult, *bias, eOp::kSUM));\n\n    ctx->registerLayer(layer, node);\n\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\ntemplate <typename T>\nNodeOutputs batchnormWeightHelper(\n    ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t nodeIdx, std::vector<TensorOrWeights>& inputs)\n{\n    auto const scale = inputs.at(1).weights();\n    auto const bias = inputs.at(2).weights();\n    auto const mean = inputs.at(3).weights();\n    auto const variance = inputs.at(4).weights();\n\n    T const* scaleValues = static_cast<T*>(scale.values);\n    T const* biasValues = static_cast<T*>(bias.values);\n    T const* meanValues = static_cast<T*>(mean.values);\n    T const* varianceValues = static_cast<T*>(variance.values);\n\n    nvinfer1::ITensor* tensorPtr = &convertToTensor(inputs.at(0), ctx);\n\n    OnnxAttrs attrs(node, ctx);\n    T eps = static_cast<T>(attrs.get<float>(\"epsilon\", 1e-5f));\n\n    // Fold the weights together into a single bias and scale\n    int32_t const nbChannels = scale.shape.d[0];\n    ShapedWeights::DataType weightType = typeid(T).hash_code() == typeid(BFloat16).hash_code()\n        ? ::ONNX_NAMESPACE::TensorProto::BFLOAT16\n        : (typeid(T).hash_code() == typeid(half_float::half).hash_code() ? ::ONNX_NAMESPACE::TensorProto::FLOAT16\n                                                                         : ::ONNX_NAMESPACE::TensorProto::FLOAT);\n    auto combinedScale = ctx->createNamedTempWeights(weightType, scale.shape, /*batchNormNode=*/true);\n    auto combinedBias = ctx->createNamedTempWeights(weightType, bias.shape, /*batchNormNode=*/true);\n\n    // Validate that all the weights have the same amount of values\n    bool allSame = scale.count() == bias.count() && mean.count() == scale.count() && variance.count() == scale.count()\n        && combinedScale.count() == scale.count() && combinedBias.count() == scale.count();\n    ONNXTRT_CHECK_NODE(\n        allSame, \"Inputs to BatchNormalization must have the same shape!\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    for (int32_t i = 0; i < nbChannels; ++i)\n    {\n        combinedScale.at<T>(i) = scaleValues[i] / sqrtf(varianceValues[i] + eps);\n        combinedBias.at<T>(i) = biasValues[i] - meanValues[i] * combinedScale.at<T>(i);\n    }\n\n    return scaleHelper(ctx, node, nodeIdx, *tensorPtr, nvinfer1::ScaleMode::kCHANNEL, combinedBias, combinedScale,\n        ShapedWeights::empty(weightType), combinedBias.getName(), combinedScale.getName());\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(BatchNormalization)\n{\n    ONNXTRT_CHECK_NODE((inputs.at(1).shape().nbDims == 1), \"The shape of the scale input must be (C, )\", node, nodeIdx,\n        ErrorCode::kINVALID_NODE);\n    ONNXTRT_CHECK_NODE((inputs.at(2).shape().nbDims == 1), \"The shape of the bias input must be (C, )\", node, nodeIdx,\n        ErrorCode::kINVALID_NODE);\n    ONNXTRT_CHECK_NODE((inputs.at(3).shape().nbDims == 1), \"The shape of the mean input must be (C, )\", node, nodeIdx,\n        ErrorCode::kINVALID_NODE);\n    ONNXTRT_CHECK_NODE((inputs.at(4).shape().nbDims == 1), \"The shape of the var input must be (C, )\", node, nodeIdx,\n        ErrorCode::kINVALID_NODE);\n\n    OnnxAttrs attrs(node, ctx);\n\n    bool const allInputsWeights = inputs.at(1).is_weights() && inputs.at(2).is_weights() && inputs.at(3).is_weights()\n        && inputs.at(4).is_weights();\n\n    // If any input is not an initializer, use fallback method implementing batchnorm as a combination of elementwise\n    // layers.\n    if (!allInputsWeights)\n    {\n        LOG_VERBOSE(\"Found BatchNormalization node with non-initializer inputs, using Elementwise fallback\");\n        return batchnormFallback(ctx, node, nodeIdx, inputs);\n    }\n\n    // If all inputs are weights of the same type, then combine the weights into a single scale layer.\n    auto tensorType = inputs.at(0).getType();\n    bool allWeightsSameType = tensorType == inputs.at(1).getType() && tensorType == inputs.at(2).getType()\n        && tensorType == inputs.at(3).getType() && tensorType == inputs.at(4).getType();\n    if (allWeightsSameType)\n    {\n        LOG_VERBOSE(\n            \"Found BatchNormalization node with conforming initializer types. Combining into a single scale node.\");\n        if (tensorType == \"FLOAT\")\n        {\n            return batchnormWeightHelper<float>(ctx, node, nodeIdx, inputs);\n        }\n        if (tensorType == \"HALF\")\n        {\n            return batchnormWeightHelper<half_float::half>(ctx, node, nodeIdx, inputs);\n        }\n        if (tensorType == \"BF16\")\n        {\n            return batchnormWeightHelper<BFloat16>(ctx, node, nodeIdx, inputs);\n        }\n        ONNXTRT_CHECK_NODE(false, \"Invalid data type provided for BatchNormalization\", node, nodeIdx,\n            ErrorCode::kUNSUPPORTED_NODE_DATATYPE);\n    }\n    // With weights of different types, use fallback method to cast weights to the input type for stronglyTyped\n    // networks.\n    else if (ctx->isStronglyTyped())\n    {\n        return batchnormFallback(ctx, node, nodeIdx, inputs);\n    }\n\n    // For weakly-typed networks, cast everything to FP32 for consistency.\n    LOG_VERBOSE(\n        \"Found BatchNormalization node with non-conforming initializer types. Casting parameters to FP32 and combining \"\n        \"into a single scale node.\");\n    auto const scale = inputs.at(1).weights();\n    auto const bias = inputs.at(2).weights();\n    auto const mean = inputs.at(3).weights();\n    auto const variance = inputs.at(4).weights();\n\n    // In the case of mixed precision, cast all values to FLOAT.\n    float const* scaleValues = ctx->getWeightsContext().getFP32Values(scale);\n    float const* biasValues = ctx->getWeightsContext().getFP32Values(bias);\n    float const* meanValues = ctx->getWeightsContext().getFP32Values(mean);\n    float const* varianceValues = ctx->getWeightsContext().getFP32Values(variance);\n\n    nvinfer1::ITensor* tensorPtr = &convertToTensor(inputs.at(0), ctx);\n\n    float eps = attrs.get<float>(\"epsilon\", 1e-5f);\n\n    // Fold the weights together into a single bias and scale\n    int32_t const nbChannels = scale.shape.d[0];\n    auto combinedScale\n        = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto::FLOAT, scale.shape, /*batchNormNode=*/true);\n    auto combinedBias\n        = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto::FLOAT, bias.shape, /*batchNormNode=*/true);\n\n    // Validate that all the weights have the same amount of values\n    bool allSame = scale.count() == bias.count() && mean.count() == scale.count() && variance.count() == scale.count()\n        && combinedScale.count() == scale.count() && combinedBias.count() == scale.count();\n    ONNXTRT_CHECK_NODE(\n        allSame, \"Inputs to BatchNormalization must have the same shape!\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    for (int32_t i = 0; i < nbChannels; ++i)\n    {\n        combinedScale.at<float>(i) = scaleValues[i] / sqrtf(varianceValues[i] + eps);\n        combinedBias.at<float>(i) = biasValues[i] - meanValues[i] * combinedScale.at<float>(i);\n    }\n\n    return scaleHelper(ctx, node, nodeIdx, *tensorPtr, nvinfer1::ScaleMode::kCHANNEL, combinedBias, combinedScale,\n        ShapedWeights::empty(::ONNX_NAMESPACE::TensorProto::FLOAT), combinedBias.getName(), combinedScale.getName());\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(BlackmanWindow)\n{\n\n    /***\n\n    Operation returns a window vector, where\n\n    Y[n] = 0.42 - 0.5cos(2pi*n / N) + 0.08cos(4pi*n / N)\n\n    Where N is the window length, and n is each element in the window.\n\n    Note that if `periodic == 0`, the denominator becomes N - 1.\n\n    This can be represented by creating a range 'n' from 0 -> N, and performing the operations elementwise.\n\n    ***/\n\n    OnnxAttrs attrs(node, ctx);\n    int32_t outputDtype = attrs.get<int32_t>(\"output_datatype\", 1);\n    int32_t periodic = attrs.get<int32_t>(\"periodic\", 1);\n    ONNXTRT_CHECK_NODE(outputDtype == 1, \"Output must be float32-type!\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    constexpr float alpha = 0.42F;\n    constexpr float beta = 0.5F;\n    constexpr float gamma = 0.08F;\n\n    auto* N = &convertToTensor(inputs.at(0), ctx);\n    ONNXTRT_CHECK_NODE(\n        N->getDimensions().nbDims == 0, \"Window length must be a scalar!\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    auto* window = generateWindow(ctx, N);\n\n    auto lhsCosOutput = windowHelper(ctx, 2.F * M_PI, window, N, nvinfer1::UnaryOperation::kCOS, periodic);\n\n    auto betaTensor = N_CHECK(addConstantScalar(ctx, beta, ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT,\n        nvinfer1::Dims{1, {1}})->getOutput(0));\n    auto betaLayer\n        = N_CHECK(ctx->network()->addElementWise(*betaTensor, *lhsCosOutput, nvinfer1::ElementWiseOperation::kPROD));\n    auto betaOutput = N_CHECK(betaLayer->getOutput(0));\n\n    auto rhsCosOutput = windowHelper(ctx, 4.F * M_PI, window, N, nvinfer1::UnaryOperation::kCOS, periodic);\n    auto gammaTensor = N_CHECK(addConstantScalar(ctx, gamma, ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT,\n        nvinfer1::Dims{1, {1}})->getOutput(0));\n    auto gammaLayer\n        = N_CHECK(ctx->network()->addElementWise(*gammaTensor, *rhsCosOutput, nvinfer1::ElementWiseOperation::kPROD));\n    auto gammaOutput = N_CHECK(gammaLayer->getOutput(0));\n\n    auto alphaTensor = N_CHECK(addConstantScalar(ctx, alpha, ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT,\n        nvinfer1::Dims{1, {1}})->getOutput(0));\n    auto alphaMinusBeta\n        = N_CHECK(ctx->network()->addElementWise(*alphaTensor, *betaOutput, nvinfer1::ElementWiseOperation::kSUB));\n    auto alphaMinusBetaTensor = N_CHECK(alphaMinusBeta->getOutput(0));\n\n    auto plusGamma = N_CHECK(\n        ctx->network()->addElementWise(*alphaMinusBetaTensor, *gammaOutput, nvinfer1::ElementWiseOperation::kSUM));\n    RETURN_FIRST_OUTPUT(plusGamma, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Cast)\n{\n    // Get input node.\n    nvinfer1::ITensor& tensor = convertToTensor(inputs.at(0), ctx);\n    OnnxAttrs attrs(node, ctx);\n    // Get data type to cast to. Ignore \"saturate\" attribute as TRT will reject casts to FP8.\n    auto onnxType = attrs.get<int32_t>(\"to\");\n    DataType newType{DataType::kFLOAT};\n    LOG_VERBOSE(\"Casting to type: \" << newType);\n    ONNXTRT_CHECK_NODE(convertDtype(onnxType, &newType), \"Unsupported cast!\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    // Add the layer.\n    nvinfer1::ICastLayer* layer = N_CHECK(ctx->network()->addCast(tensor, newType));\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(CastLike)\n{\n    // Get input tensor to cast\n    nvinfer1::ITensor& tensor = convertToTensor(inputs.at(0), ctx);\n    // Get datatype to cast to, extracted from the second input tensor. Ignore \"saturate\" attribute as TRT will reject\n    // casts to FP8.\n    auto type = convertToTensor(inputs.at(1), ctx).getType();\n    nvinfer1::ICastLayer* layer = N_CHECK(ctx->network()->addCast(tensor, type));\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Ceil)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kCEIL);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Celu)\n{\n\n    using eOp = nvinfer1::ElementWiseOperation;\n    using uOp = nvinfer1::UnaryOperation;\n    using eOpInstuctor = std::tuple<int, int, const nvinfer1::ElementWiseOperation>;\n\n    ONNXTRT_CHECK_NODE((!inputs.empty()), \"Inputs vector is empty.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    OnnxAttrs attrs(node, ctx);\n    TensorOrWeights input = inputs.at(0);\n    float alpha = attrs.get<float>(\"alpha\", 1.0);\n\n    TensorOrWeights weightsOfZero = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto::FLOAT, {0, {}});\n    ShapedWeights weightsOfOnes = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto::FLOAT, {0, {}});\n    std::vector<float> ones{1};\n    std::memcpy(weightsOfOnes.values, ones.data(), weightsOfOnes.count() * sizeof(float));\n    ShapedWeights weightsOfAlpha = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto::FLOAT, {0, {}});\n    std::vector<float> alphas{alpha};\n    std::memcpy(weightsOfAlpha.values, alphas.data(), weightsOfAlpha.count() * sizeof(float));\n\n    // Variable name -> index in inputTensors\n    // x -> 0\n    // 0 -> 1\n    // 1 -> 2\n    // alpha -> 3\n    std::vector<TensorOrWeights> newInputs{input, weightsOfZero, weightsOfOnes, weightsOfAlpha};\n\n    std::vector<nvinfer1::ITensor*> inputTensors;\n    int32_t maxNbDims = -1;\n    for (auto i : newInputs)\n    {\n        maxNbDims = std::max(maxNbDims, i.shape().nbDims);\n    }\n\n    for (auto i : newInputs)\n    {\n        auto* tensor_ptr = &convertToTensor(i, ctx);\n\n        // Broadcast all input tensors to size of maxNbDims\n        broadcastTensor(ctx, tensor_ptr, maxNbDims);\n        ONNXTRT_CHECK_NODE(tensor_ptr->getDimensions().nbDims == maxNbDims, \"Failed to broadcast tensors elementwise!\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        inputTensors.push_back(tensor_ptr);\n    }\n\n    // Calculate (x/alpha)\n    std::vector<TensorOrWeights> tempInputs{newInputs[0], newInputs[3]};\n    elementwiseCheck(tempInputs, eOp::kDIV, node, nodeIdx);\n    nvinfer1::ITensor* combined = inputTensors.at(0);\n    auto* divLayer = N_CHECK(ctx->network()->addElementWise(*combined, *inputTensors.at(3), eOp::kDIV));\n    ctx->registerLayer(divLayer, node);\n    combined = N_CHECK(divLayer->getOutput(0));\n\n    // Calculate exp(x/alpha) -> 4\n    nvinfer1::IUnaryLayer* uLayer = N_CHECK(ctx->network()->addUnary(*combined, uOp::kEXP));\n    ctx->registerLayer(uLayer, node);\n    combined = N_CHECK(uLayer->getOutput(0));\n    inputTensors.push_back(combined);\n\n    std::vector<eOpInstuctor> operations{\n        // max(0,x) -> 5\n        eOpInstuctor(0, 1, eOp::kMAX),\n        // (exp(x/alpha)-1)) -> 6\n        eOpInstuctor(4, 2, eOp::kSUB),\n        // alpha*(exp(x/alpha)-1) -> 7\n        eOpInstuctor(3, 6, eOp::kPROD),\n        // min(0,alpha*(exp(x/alpha)-1)) -> 8\n        eOpInstuctor(1, 7, eOp::kMIN),\n        // max(0,x) + min(0,alpha*(exp(x/alpha)-1)) -> 9\n        eOpInstuctor(5, 8, eOp::kSUM),\n    };\n\n    for (auto it : operations)\n    {\n        nvinfer1::ITensor* firstTensor = inputTensors.at(std::get<0>(it));\n        nvinfer1::ITensor* secondTensor = inputTensors.at(std::get<1>(it));\n        eOp const op = std::get<2>(it);\n        tempInputs = {firstTensor, secondTensor};\n        elementwiseCheck(tempInputs, op, node, nodeIdx);\n        ONNXTRT_CHECK_NODE((firstTensor->getDimensions().nbDims == secondTensor->getDimensions().nbDims),\n            \"The rank of operands should be the same adding inputs. First tensor rank is \"\n                << firstTensor->getDimensions().nbDims << \", but second tensor rank is \"\n                << secondTensor->getDimensions().nbDims << \".\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        auto* layer = N_CHECK(ctx->network()->addElementWise(*firstTensor, *secondTensor, op));\n        ctx->registerLayer(layer, node);\n        inputTensors.push_back(N_CHECK(layer->getOutput(0)));\n    }\n    return {{inputTensors.back()}};\n}\n\n// Helper function to perform clip through elementwise operations\ntemplate <typename ScalarType>\nNodeOutputs elementwiseClipHelper(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node,\n    std::vector<TensorOrWeights>& inputs, size_t numInputs, int32_t onnxType)\n{\n    OnnxAttrs attrs(node, ctx);\n    auto* input = &convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor* alphaT{nullptr};\n    nvinfer1::ITensor* betaT{nullptr};\n    ScalarType alpha = std::numeric_limits<ScalarType>::lowest();\n    ScalarType beta = std::numeric_limits<ScalarType>::max();\n    if (numInputs == 1)\n    {\n        alphaT = N_CHECK(addConstantScalar(ctx, alpha, onnxType)->getOutput(0));\n        betaT = N_CHECK(addConstantScalar(ctx, beta, onnxType)->getOutput(0));\n    }\n    else if (numInputs == 2)\n    {\n        alphaT = &convertToTensor(inputs.at(1), ctx);\n        betaT = N_CHECK(addConstantScalar(ctx, beta, onnxType)->getOutput(0));\n    }\n    else if (numInputs == 3)\n    {\n        // \"min\" can be optional if \"max\" is specified. Check for this case here\n        if (!inputs.at(1).isNullTensor())\n        {\n            alphaT = &convertToTensor(inputs.at(1), ctx);\n        }\n        else\n        {\n            alphaT = N_CHECK(addConstantScalar(ctx, alpha, onnxType)->getOutput(0));\n        }\n        if (!inputs.at(2).isNullTensor())\n        {\n            betaT = &convertToTensor(inputs.at(2), ctx);\n        }\n        else\n        {\n            betaT = N_CHECK(addConstantScalar(ctx, beta, onnxType)->getOutput(0));\n        }\n    }\n\n    // Now that we have alphaT and betaT, do the elementwise calculation\n    using eOp = nvinfer1::ElementWiseOperation;\n    broadcastTensors(ctx, input, alphaT);\n    broadcastTensors(ctx, input, betaT);\n    auto* lowerClipLayer = N_CHECK(ctx->network()->addElementWise(*input, *alphaT, eOp::kMAX));\n    auto* lowerClip = N_CHECK(lowerClipLayer->getOutput(0));\n    auto* upperClipLayer = N_CHECK(ctx->network()->addElementWise(*lowerClip, *betaT, eOp::kMIN));\n    auto* upperClip = N_CHECK(upperClipLayer->getOutput(0));\n    return {{upperClip}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Clip)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    // For INT32 and multi-input clips, use elementwise operators instead.\n    size_t numInputs = inputs.size();\n    bool elementwiseClip = inputs.at(0).isInt32() || inputs.at(0).isInt64();\n    for (size_t i = 1; i < numInputs; i++)\n    {\n        elementwiseClip |= inputs.at(i).is_tensor();\n    }\n    if (elementwiseClip)\n    {\n        auto type = convertToTensor(inputs.at(0), ctx).getType();\n        ONNXTRT_CHECK_NODE((type == DataType::kFLOAT || type == DataType::kHALF || type == DataType::kBF16\n                               || type == DataType::kINT32 || type == DataType::kINT64),\n            \"This version of TensorRT only supports floating-point, INT32, or INT64 inputs for Clip! The current input \"\n            \"type is \"\n                + getTrtDtypeName(type) + \".\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        if (type == DataType::kHALF)\n        {\n            return elementwiseClipHelper<half_float::half>(\n                ctx, node, inputs, numInputs, ::ONNX_NAMESPACE::TensorProto::FLOAT16);\n        }\n        if (type == DataType::kBF16)\n        {\n            return elementwiseClipHelper<BFloat16>(\n                ctx, node, inputs, numInputs, ::ONNX_NAMESPACE::TensorProto::BFLOAT16);\n        }\n        if (type == DataType::kFLOAT)\n        {\n            return elementwiseClipHelper<float>(ctx, node, inputs, numInputs, ::ONNX_NAMESPACE::TensorProto::FLOAT);\n        }\n        if (type == DataType::kINT64)\n        {\n            return elementwiseClipHelper<int64_t>(ctx, node, inputs, numInputs, ::ONNX_NAMESPACE::TensorProto::INT64);\n        }\n        return elementwiseClipHelper<int32_t>(ctx, node, inputs, numInputs, ::ONNX_NAMESPACE::TensorProto::INT32);\n    }\n\n    // Activation path only supports float/half initializers\n    OnnxAttrs attrs(node, ctx);\n    // beta is the upper bound\n    float alpha = std::numeric_limits<float>::lowest();\n    float beta = std::numeric_limits<float>::max();\n\n    if (ctx->getOpsetVersion() >= 11)\n    {\n        // Handle \"min\" node input.\n        if (numInputs == 2)\n        {\n            ONNXTRT_CHECK_NODE(inputs.at(1).is_weights(), \"Clip min value must be an initializer!\", node, nodeIdx,\n                ErrorCode::kINVALID_NODE);\n            auto min = inputs.at(1).weights();\n            alpha = getSingleValueAsFloat(min);\n        }\n        // Handle both \"min\" and \"max\" node inputs\n        else if (numInputs == 3)\n        {\n            // \"min\" can be optional if \"max\" is specified. Check for this case here\n            if (!inputs.at(1).isNullTensor())\n            {\n                ONNXTRT_CHECK_NODE(inputs.at(1).is_weights(), \"Clip min value must be an initializer!\", node, nodeIdx,\n                    ErrorCode::kINVALID_NODE);\n                auto min = inputs.at(1).weights();\n                alpha = getSingleValueAsFloat(min);\n            }\n\n            if (!inputs.at(2).isNullTensor())\n            {\n                ONNXTRT_CHECK_NODE(inputs.at(2).is_weights(), \"Clip max value must be an initializer!\", node, nodeIdx,\n                    ErrorCode::kINVALID_NODE);\n                auto max = inputs.at(2).weights();\n                beta = getSingleValueAsFloat(max);\n            }\n        }\n    }\n    else\n    {\n        alpha = attrs.get(\"min\", std::numeric_limits<float>::lowest());\n        beta = attrs.get(\"max\", std::numeric_limits<float>::max());\n    }\n\n    return activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kCLIP, &alpha, &beta);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Concat)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    std::vector<nvinfer1::ITensor*> tensors;\n    for (auto& input : inputs)\n    {\n        auto* tensorPtr = &convertToTensor(input, ctx);\n        tensors.push_back(tensorPtr);\n    }\n    OnnxAttrs attrs(node, ctx);\n    int32_t axis = attrs.get<int32_t>(\"axis\");\n    int32_t nbDims = inputs.at(0).shape().nbDims;\n    convertAxis(axis, nbDims, node, nodeIdx);\n    auto* layer = N_CHECK(ctx->network()->addConcatenation(tensors.data(), tensors.size()));\n    ctx->registerLayer(layer, node);\n    layer->setAxis(axis);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Constant)\n{\n    OnnxAttrs attrs(node, ctx);\n\n    // Having the trt_outputs_range_min attributes means it's from\n    // serialized iNetworkDefinition.\n    if (!attrs.get<std::vector<float>>(\"trt_outputs_range_min\", {}).empty())\n    {\n        // just create a constant layer here for 1-1 mapping during network deserialization\n        auto weights = attrs.get<ShapedWeights>(\"value\");\n        auto* layer = N_CHECK(ctx->network()->addConstant(weights.shape, weights));\n        ctx->network()->setWeightsName(weights, weights.getName());\n        RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n    }\n\n    if (ctx->getOpsetVersion() >= 12)\n    {\n        if (attrs.count(\"value_float\"))\n        {\n            ShapedWeights convertedWeights = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto::FLOAT, {0, {}});\n            float value = attrs.get<float>(\"value_float\");\n            std::memcpy(convertedWeights.values, &value, convertedWeights.count() * sizeof(float));\n            return {{convertedWeights}};\n        }\n\n        if (attrs.count(\"value_floats\"))\n        {\n            std::vector<float> values = attrs.get<std::vector<float>>(\"value_floats\");\n            int32_t valueSize = values.size();\n            ShapedWeights convertedWeights\n                = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto::FLOAT, {1, {valueSize}});\n            std::memcpy(convertedWeights.values, values.data(), convertedWeights.count() * sizeof(float));\n            return {{convertedWeights}};\n        }\n        if (attrs.count(\"value_int\"))\n        {\n            ShapedWeights convertedWeights = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto::INT64, {0, {}});\n            int64_t value = attrs.get<int64_t>(\"value_int\");\n            std::memcpy(convertedWeights.values, &value, convertedWeights.count() * sizeof(int64_t));\n            return {{convertedWeights}};\n        }\n\n        if (attrs.count(\"value_ints\"))\n        {\n            std::vector<int64_t> values = attrs.get<std::vector<int64_t>>(\"value_ints\");\n            int32_t valueSize = values.size();\n            ShapedWeights convertedWeights\n                = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto::INT64, {1, {valueSize}});\n            std::memcpy(convertedWeights.values, values.data(), convertedWeights.count() * sizeof(int64_t));\n            return {{convertedWeights}};\n        }\n    }\n    attrs.get<ShapedWeights>(\"value\");\n\n    return {{attrs.get<ShapedWeights>(\"value\")}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(ConstantOfShape)\n{\n    OnnxAttrs attrs(node, ctx);\n    nvinfer1::ITensor* shape = &convertToTensor(inputs.at(0), ctx);\n\n    ShapedWeights zeroWeights\n        = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto_DataType_FLOAT, nvinfer1::Dims{1, {1}});\n    static_cast<float*>(zeroWeights.values)[0] = 0.f;\n    auto valueWeights = TensorOrWeights{attrs.get(\"value\", zeroWeights)};\n    nvinfer1::ITensor* value = &convertToTensor(valueWeights, ctx);\n    return {{constantOfShape(ctx, value, shape)}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Conv)\n{\n    if (inputs.at(1).is_tensor() || (inputs.size() > 2 && inputs.at(2).is_tensor()))\n    {\n        // Handle dynamic weights convolution\n        return convMultiInput(ctx, node, nodeIdx, inputs);\n    }\n\n    nvinfer1::ITensor* tensorPtr = &convertToTensor(inputs.at(0), ctx);\n\n    auto kernelWeights = inputs.at(1).weights();\n\n    nvinfer1::Dims dims = tensorPtr->getDimensions();\n    LOG_VERBOSE(\"Convolution input dimensions: \" << dims);\n    ONNXTRT_CHECK_NODE(dims.nbDims >= 0, \"TensorRT could not compute output dimensions of Conv\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n\n    bool const needToExpandDims = (dims.nbDims == 3);\n    if (needToExpandDims)\n    {\n        // Expand spatial dims from 1D to 2D\n        std::vector<int32_t> axes{3};\n        tensorPtr = unsqueezeTensor(ctx, *tensorPtr, axes);\n        ONNXTRT_CHECK_NODE(tensorPtr, \"Failed to unsqueeze tensor.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        dims = tensorPtr->getDimensions();\n    }\n    if (kernelWeights.shape.nbDims == 3)\n    {\n        kernelWeights.shape.nbDims = 4;\n        kernelWeights.shape.d[3] = 1;\n    }\n\n    int32_t const nbSpatialDims = dims.nbDims - 2;\n    // Check that the number of spatial dimensions and the kernel shape matches up.\n    ONNXTRT_CHECK_NODE((nbSpatialDims == kernelWeights.shape.nbDims - 2),\n        \"The number of spatial dimensions and the kernel shape doesn't match up for the Conv operator. Number of \"\n        \"spatial dimensions = \"\n            << nbSpatialDims << \", number of kernel dimensions = \" << kernelWeights.shape.nbDims << \".\",\n        node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    nvinfer1::Weights biasWeights;\n    if (inputs.size() == 3)\n    {\n        assertIsWeights(inputs.at(2), \"The bias tensor is required to be an initializer for the Conv operator.\");\n        auto shapedBiasWeights = inputs.at(2).weights();\n        // Unsqueeze scalar weights to 1D\n        if (shapedBiasWeights.shape.nbDims == 0)\n        {\n            shapedBiasWeights.shape = {1, {1}};\n        }\n        ONNXTRT_CHECK_NODE((shapedBiasWeights.shape.nbDims == 1), \"The bias tensor is required to be 1D.\", node,\n            nodeIdx, ErrorCode::kINVALID_NODE);\n        ONNXTRT_CHECK_NODE((shapedBiasWeights.shape.d[0] == kernelWeights.shape.d[0]),\n            \"The shape of the bias tensor misaligns with the weight tensor. Shape of bias weights = \"\n                << shapedBiasWeights.shape.d[0] << \", shape of kernel weights = \" << kernelWeights.shape.d[0] << \".\",\n            node, nodeIdx, ErrorCode::kINVALID_NODE);\n        biasWeights = shapedBiasWeights;\n    }\n    else\n    {\n        biasWeights = ShapedWeights::empty(kernelWeights.type);\n    }\n    nvinfer1::Dims kernelSize;\n    kernelSize.nbDims = nbSpatialDims;\n    for (int32_t i = 1; i <= nbSpatialDims; ++i)\n    {\n        kernelSize.d[nbSpatialDims - i] = kernelWeights.shape.d[kernelWeights.shape.nbDims - i];\n    }\n    nvinfer1::Dims strides = makeDims(nbSpatialDims, 1);\n    nvinfer1::Dims begPadding = makeDims(nbSpatialDims, 0);\n    nvinfer1::Dims endPadding = makeDims(nbSpatialDims, 0);\n    nvinfer1::Dims dilations = makeDims(nbSpatialDims, 1);\n    nvinfer1::PaddingMode paddingMode;\n    bool excludePadding;\n    getKernelParams(\n        ctx, node, &kernelSize, &strides, &begPadding, &endPadding, paddingMode, excludePadding, &dilations);\n\n    for (int32_t i = 1; i <= nbSpatialDims; ++i)\n    {\n        ONNXTRT_CHECK_NODE((kernelSize.d[nbSpatialDims - i] == kernelWeights.shape.d[kernelWeights.shape.nbDims - i]),\n            \"The size of spatial dimension and the size of kernel shape are not equal for the Conv operator. \"\n            \"Size of spatial dimensions = \"\n                << kernelSize.d[nbSpatialDims - i]\n                << \", size of kernel dimensions = \" << kernelWeights.shape.d[kernelWeights.shape.nbDims - i] << \".\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    int32_t nchan = dims.d[1];\n    int32_t noutput = kernelWeights.shape.d[0];\n    nvinfer1::IConvolutionLayer* layer\n        = N_CHECK(ctx->network()->addConvolutionNd(*tensorPtr, noutput, kernelSize, kernelWeights, biasWeights));\n\n    layer->setStrideNd(strides);\n    layer->setPaddingMode(paddingMode);\n    layer->setPrePadding(begPadding);\n    layer->setPostPadding(endPadding);\n    layer->setDilationNd(dilations);\n    OnnxAttrs attrs(node, ctx);\n    int32_t ngroup = attrs.get(\"group\", 1);\n    ONNXTRT_CHECK_NODE((nchan == -1 || kernelWeights.shape.d[1] * ngroup == nchan),\n        \"Kernel weight dimension failed to broadcast to input.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    layer->setNbGroups(ngroup);\n    // Register layer name as well as kernel weights and bias weights (if any)\n    ctx->registerLayer(layer, node);\n    ctx->network()->setWeightsName(kernelWeights, inputs.at(1).weights().getName());\n    if (inputs.size() == 3)\n    {\n        ctx->network()->setWeightsName(biasWeights, inputs.at(2).weights().getName());\n    }\n    tensorPtr = N_CHECK(layer->getOutput(0));\n    dims = tensorPtr->getDimensions();\n\n    if (needToExpandDims)\n    {\n        // Un-expand spatial dims back to 1D\n        std::vector<int32_t> axes{3};\n        tensorPtr = squeezeTensor(ctx, *tensorPtr, axes);\n        ONNXTRT_CHECK_NODE(tensorPtr, \"Failed to squeeze tensor.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    LOG_VERBOSE(\"Using kernel: \" << kernelSize << \", strides: \" << strides << \", prepadding: \" << begPadding\n                                 << \", postpadding: \" << endPadding << \", dilations: \" << dilations\n                                 << \", numOutputs: \" << noutput << \", nbGroups: \" << ngroup);\n    LOG_VERBOSE(\"Convolution output dimensions: \" << dims);\n\n    return {{tensorPtr}};\n}\n\n// TRT only supports 2D or 3D deconvolutions (Layout: [N,C,D1,D2,(D3)])\n// Inputs should be of dimension 4 or 5.\n// When input.nbDims = 3, we expand it to 4D\nDEFINE_BUILTIN_OP_IMPORTER(ConvTranspose)\n{\n    // Expand spatial dims from 1D to 2D, return true if reshaped activation\n    auto const NCWtoNCHW = [&ctx, &node](nvinfer1::ITensor*& tensor, nvinfer1::Dims& tensorShape) {\n        if (tensor && tensor->getDimensions().nbDims == 3)\n        {\n            std::vector<int32_t> const axes{3};\n            tensor = unsqueezeTensor(ctx, *tensor, axes);\n            tensorShape = tensor->getDimensions();\n            return true;\n        }\n        // for initializer, just change the shape by appending 1\n        if (tensorShape.nbDims == 3)\n        {\n            tensorShape.nbDims = 4;\n            tensorShape.d[3] = 1;\n        }\n        return false;\n    };\n\n    ONNXTRT_CHECK_NODE(\n        inputs.size() >= 2, \"deconvolution require at least 2 inputs.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    nvinfer1::ITensor* tensorPtr = &convertToTensor(inputs.at(0), ctx);\n    auto inputType = tensorPtr->getType();\n    nvinfer1::ITensor* kernelTensorPtr = inputs.at(1).is_tensor() ? &convertToTensor(inputs.at(1), ctx) : nullptr;\n    nvinfer1::ITensor* biasTensorPtr\n        = inputs.size() > 2 && inputs.at(2).is_tensor() ? &convertToTensor(inputs.at(2), ctx) : nullptr;\n\n    nvinfer1::Dims dims = tensorPtr->getDimensions();\n    // Deconvolution input must be at least 3D and at most 5D.\n    ONNXTRT_CHECK_NODE(dims.nbDims >= 3 && dims.nbDims <= 5,\n        \"Deconvolution input must be at least 3D and at most 5D! The current input is rank \" << dims.nbDims << \".\",\n        node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    // Kernel weights have layout [C, M/group, k1, k2, (k3)]\n    auto kernelShape = inputs.at(1).shape();\n\n    bool needReshapeBack = NCWtoNCHW(tensorPtr, dims);\n    NCWtoNCHW(kernelTensorPtr, kernelShape);\n\n    int32_t const nbSpatialDims = dims.nbDims - 2;\n    // Check that the number of spatial dimensions and the kernel shape matches up.\n    ONNXTRT_CHECK_NODE((nbSpatialDims == kernelShape.nbDims - 2),\n        \"The number of spatial dimensions and the kernel shape doesn't match up. Number of spatial dimensions = \"\n            << nbSpatialDims << \", number of kernel dimensions = \" << kernelShape.nbDims << \".\",\n        node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    // Get all attributes\n    OnnxAttrs attrs(node, ctx);\n    nvinfer1::Dims outputShape;\n    nvinfer1::Dims outputPadding = makeDims(nbSpatialDims, 0);\n    nvinfer1::Dims kernelSize;\n    nvinfer1::Dims strides = makeDims(nbSpatialDims, 1);\n    nvinfer1::Dims begPadding = makeDims(nbSpatialDims, 0);\n    nvinfer1::Dims endPadding = makeDims(nbSpatialDims, 0);\n    nvinfer1::Dims dilations = makeDims(nbSpatialDims, 1);\n    nvinfer1::PaddingMode paddingMode;\n    bool excludePadding = false;\n\n    int32_t ngroup = attrs.get(\"group\", 1);\n    int32_t noutput = kernelShape.d[1] * ngroup; // Note: Weights order is CKRS\n\n    // Get static bias weights\n    nvinfer1::Weights staticBiasWeights;\n    if (inputs.size() > 2 && biasTensorPtr == nullptr)\n    {\n        auto shapedBiasWeights = inputs.at(2).weights();\n        // ONNX requires shapedBiasWeights to be 1D\n        ONNXTRT_CHECK_NODE(shapedBiasWeights.shape.nbDims == 1,\n            \"The bias tensor is required to be 1D. Provided bias has rank \" << shapedBiasWeights.shape.nbDims << \".\",\n            node, nodeIdx, ErrorCode::kINVALID_NODE);\n        ONNXTRT_CHECK_NODE((shapedBiasWeights.shape.d[0] == noutput),\n            \"The number of the bias weights does not align with the number of output maps. Number of bias weights = \"\n                << shapedBiasWeights.shape.d[0] << \", number of output maps = \" << noutput << \".\",\n            node, nodeIdx, ErrorCode::kINVALID_NODE);\n        staticBiasWeights = shapedBiasWeights;\n    }\n    else\n    {\n        staticBiasWeights = ShapedWeights::empty(trtDataTypeToONNX(inputType));\n    }\n\n    // Kernel shape either comes from the attributes or extracted from the kernel weights shape\n    kernelSize.nbDims = nbSpatialDims;\n    for (int32_t i = 1; i <= nbSpatialDims; ++i)\n    {\n        kernelSize.d[nbSpatialDims - i] = kernelShape.d[kernelShape.nbDims - i];\n    }\n\n    getKernelParams(ctx, node, &kernelSize, &strides, &begPadding, &endPadding, paddingMode, excludePadding, &dilations,\n        &outputPadding);\n\n    for (int32_t i = 1; i <= nbSpatialDims; ++i)\n    {\n        ONNXTRT_CHECK_NODE((kernelSize.d[nbSpatialDims - i] == kernelShape.d[kernelShape.nbDims - i]),\n            \"Attribute kernel_shape misaligns with the dimensions of the weight tensor. Number of spatial dimensions = \"\n                << kernelSize.d[nbSpatialDims - i]\n                << \", number of kernel dimensions = \" << kernelShape.d[kernelShape.nbDims - i] << \".\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    // Set padding. ONNX ConvTranspose supports many different padding modes. Order of priority for padding:\n    // 1. Output shape is specified - calculate expected pre and post padding.\n    // 2. AUTO_PAD != NOTSET: ignore all other padding values and set padding mode with layer->setPaddingMode.\n    //    Pad the resulting output vector with values from output_padding.\n    // 3. Use specified \"pads\" values from the node. Pad the resulting output vector with values from output_padding.\n\n    auto autoPadMode = attrs.get(\"auto_pad\", std::string(\"NOTSET\"));\n    if (attrs.count(\"output_shape\") && autoPadMode == std::string(\"NOTSET\"))\n    {\n        outputShape = attrs.get<nvinfer1::Dims>(\"output_shape\");\n\n        // This function takes references to begPadding, endPadding and outputPadding and will update them with correct\n        // values.\n        generatePadding(dims, outputShape, kernelSize, strides, dilations, nbSpatialDims, begPadding, endPadding,\n            outputPadding, paddingMode);\n\n        // NOTE: it is possible for generatePadding to produce negative values for pre and post padding, which usually\n        // happens when output_shape is provided but output_padding is not. Any negative values generated for\n        // post-padding can be translated into outputPadding to pad the output tensor post deconvolution. Any negative\n        // values for pre-padding are unsupported.\n\n        for (int32_t i = 0; i < nbSpatialDims; i++)\n        {\n            ONNXTRT_CHECK_NODE(begPadding.d[i] >= 0,\n                \"TensorRT does not support negative pre-padding in the ConvTranspose operator!\", node, nodeIdx,\n                ErrorCode::kUNSUPPORTED_NODE);\n            // Update outputPadding with any negative values in endPadding, and set the corresponding value to 0.\n            if (endPadding.d[i] < 0)\n            {\n                outputPadding.d[i] = endPadding.d[i] * -1;\n                endPadding.d[i] = 0;\n            }\n        }\n    }\n\n    // When there is output_padding, if postPadding is larger than outputPadding, just adjust postPadding\n    // Or reduce outputPadding as minimum as possible.\n    bool hasOutputPadding = false;\n    if (outputPadding != makeDims(nbSpatialDims, 0) && autoPadMode == std::string(\"NOTSET\"))\n    {\n        for (int32_t i = 0; i < nbSpatialDims; ++i)\n        {\n            if (endPadding.d[i] - outputPadding.d[i] >= 0)\n            {\n                endPadding.d[i] -= outputPadding.d[i];\n                outputPadding.d[i] = 0;\n            }\n            else\n            {\n                // Reduce outputPadding as possible.\n                outputPadding.d[i] -= endPadding.d[i];\n                endPadding.d[i] = 0;\n                hasOutputPadding = true;\n            }\n        }\n    }\n\n    auto const emptyBiasWeights = ShapedWeights::empty(trtDataTypeToONNX(inputType));\n    auto const kernelWeights = kernelTensorPtr ? nvinfer1::Weights{inputType, nullptr, 0} : inputs.at(1).weights();\n    auto const biasWeights = biasTensorPtr ? nvinfer1::Weights{inputType, nullptr, 0} : staticBiasWeights;\n    // Create a deconvolution layer and set known attributes - strides,ngroups, and dilations\n    // If there is still output padding, remove the bias weights. Bias will be added below.\n    auto* layer = N_CHECK(ctx->network()->addDeconvolutionNd(\n        *tensorPtr, noutput, kernelSize, kernelWeights, hasOutputPadding ? emptyBiasWeights : biasWeights));\n    ONNXTRT_CHECK_NODE(layer, \"Failed to create layer\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    layer->setStrideNd(strides);\n    layer->setNbGroups(ngroup);\n    layer->setDilationNd(dilations);\n    if (kernelTensorPtr)\n    {\n        layer->setInput(1, *kernelTensorPtr);\n    }\n    else\n    {\n        ctx->network()->setWeightsName(kernelWeights, inputs.at(1).weights().getName());\n    }\n    if (biasTensorPtr)\n    {\n        layer->setInput(2, *biasTensorPtr);\n    }\n\n    layer->setPaddingMode(paddingMode);\n    layer->setPrePadding(begPadding);\n    layer->setPostPadding(endPadding);\n\n    LOG_VERBOSE(\"Running deconvolution with: \" << \"\\n\"\n                                               << \"Padding mode: \" << autoPadMode << \"\\n\"\n                                               << \"Pre-padding: \" << begPadding << \"\\n\"\n                                               << \"Post-padding: \" << endPadding);\n\n    // Register layer, along with refittable kernel weights and bias weights (if any)\n    ctx->registerLayer(layer, node);\n    tensorPtr = N_CHECK(layer->getOutput(0));\n    dims = tensorPtr->getDimensions();\n\n    // There is still output padding. Add a padding layer to handle it.\n    if (hasOutputPadding)\n    {\n        LOG_VERBOSE(\"Padding output deconvolution tensor with: \" << outputPadding);\n\n        // Add padding layer\n        nvinfer1::ITensor* start{};\n        nvinfer1::ITensor* totalPadding{};\n        std::vector<int64_t> combinePadding{};\n        for (int32_t i = 0; i < outputPadding.nbDims; ++i)\n        {\n            combinePadding.insert(combinePadding.begin(), 0);\n            combinePadding.push_back(outputPadding.d[i]);\n        }\n        ONNXTRT_CHECK_NODE(convertOnnxPadding(ctx, dims.nbDims, combinePadding, start, totalPadding),\n            \"Failed to convert padding!\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        auto const size = getElementWiseResult(\n            ctx, shapeOf(*tensorPtr).tensor(ctx), *totalPadding, nvinfer1::ElementWiseOperation::kSUM);\n        auto const stride = makeDims(dims.nbDims, 1);\n        auto const& dummy = stride;\n        auto* sliceLayer = N_CHECK(ctx->network()->addSlice(*tensorPtr, dummy, dummy, stride));\n        sliceLayer->setInput(1, *start);\n        sliceLayer->setInput(2, *size);\n        sliceLayer->setMode(nvinfer1::SampleMode::kFILL);\n        tensorPtr = N_CHECK(sliceLayer->getOutput(0));\n\n        // This bias is not handled by deconv. Use an elementwise to handle it.\n        if (biasWeights.count != 0)\n        {\n            // Set C dimension to weights count and set other dimensions to 1 to enable broadcast\n            auto constantDims = makeDims(dims.nbDims, 1);\n            constantDims.d[dims.nbDims - nbSpatialDims - 1] = biasWeights.count;\n            auto biasConstant = N_CHECK(ctx->network()->addConstant(constantDims, biasWeights));\n            tensorPtr = getElementWiseResult(\n                ctx, *tensorPtr, *N_CHECK(biasConstant->getOutput(0)), nvinfer1::ElementWiseOperation::kSUM);\n        }\n    }\n\n    if (inputs.size() > 2 && biasTensorPtr == nullptr)\n    {\n        ctx->network()->setWeightsName(biasWeights, inputs.at(2).weights().getName());\n    }\n\n    if (needReshapeBack)\n    {\n        std::vector<int32_t> axes{3};\n        tensorPtr = squeezeTensor(ctx, *tensorPtr, axes);\n        ONNXTRT_CHECK_NODE(tensorPtr, \"Failed to squeeze tensor.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    return {{tensorPtr}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Cos)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kCOS);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Cosh)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kCOSH);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(CumSum)\n{\n    OnnxAttrs attrs(node, ctx);\n    int32_t const exclusive = attrs.get<int32_t>(\"exclusive\", 0);\n    int32_t const reverse = attrs.get<int32_t>(\"reverse\", 0);\n\n    nvinfer1::ITensor* input = &convertToTensor(inputs.at(0), ctx);\n    auto dims = input->getDimensions();\n\n    assertIsWeights(inputs.at(1), \"Axis input for CumSum must be an initializer!\");\n    ShapedWeights axisWeights = inputs.at(1).weights();\n    int32_t axis = static_cast<int32_t*>(axisWeights.values)[0];\n    convertAxis(axis, dims.nbDims, node, nodeIdx);\n\n    // Create \"inputSliced\" tensor that is sliced on dimension[axis] to length 1\n    auto inputSliced = sliceAcrossAxis(ctx, node, input, axis);\n\n    /* For exclusive CumSums, it is equivalent as a non-exclusive CumSum on a modified input tensor\n\n        Forward summations:\n            concat(0, data[0:length-1:1])\n\n        Reverse summations:\n            concat(data[1:length:1], 0)\n\n    */\n    if (exclusive)\n    {\n        auto zero = createZeroTensor(ctx, inputSliced);\n        std::vector<nvinfer1::ITensor*> concatTensors = reverse == 1 ? std::vector<nvinfer1::ITensor*>{input, zero}\n                                                                     : std::vector<nvinfer1::ITensor*>{zero, input};\n\n        auto* concat = N_CHECK(ctx->network()->addConcatenation(concatTensors.data(), concatTensors.size()));\n        concat->setAxis(axis);\n        input = N_CHECK(concat->getOutput(0));\n\n        if (reverse == 0)\n        {\n            ShapeTensor const subscripts{axesToInterlaceSubscripts(shapeVector(axis), dims.nbDims)};\n            ShapeTensor starts = fillShapeVector(ctx, 0, shapeVector(dims.nbDims));\n            ShapeTensor sizes = interlace(ctx, shapeOf(*input),\n                sub(ctx, gather(ctx, shapeOf(*input), shapeVector(axis)), shapeVector(1)), subscripts);\n            ShapeTensor strides = fillShapeVector(ctx, 1, shapeVector(dims.nbDims));\n            input = N_CHECK(addSlice(ctx, *input, starts, sizes, strides)->getOutput(0));\n        }\n        else\n        {\n            ShapeTensor const subscripts{axesToInterlaceSubscripts(shapeVector(axis), dims.nbDims)};\n            ShapeTensor starts\n                = interlace(ctx, fillShapeVector(ctx, 0, shapeVector(dims.nbDims)), shapeVector(1), subscripts);\n            ShapeTensor sizes = interlace(ctx, shapeOf(*input),\n                sub(ctx, gather(ctx, shapeOf(*input), shapeVector(axis)), shapeVector(1)), subscripts);\n            ShapeTensor strides = fillShapeVector(ctx, 1, shapeVector(dims.nbDims));\n            input = N_CHECK(addSlice(ctx, *input, starts, sizes, strides)->getOutput(0));\n        }\n    }\n\n    // Scan through each slice across summation axis and add it to the running sum\n    auto loop = N_CHECK(ctx->network()->addLoop());\n    nvinfer1::ITensor* tripLimit = getAxisLength(ctx, input, axis);\n    loop->addTripLimit(*tripLimit, nvinfer1::TripLimit::kCOUNT);\n    auto iterator = loop->addIterator(*input, axis, reverse);\n    auto data = N_CHECK(iterator->getOutput(0));\n\n    // Squeeze inputSliced down to same shape as `data`\n    inputSliced = squeezeTensor(ctx, *inputSliced, {axis});\n    auto zeroTensor = createZeroTensor(ctx, inputSliced);\n    auto runningSum = loop->addRecurrence(*zeroTensor);\n    auto runningSumTensor = N_CHECK(runningSum->getOutput(0));\n\n    auto curSum\n        = N_CHECK(ctx->network()->addElementWise(*data, *runningSumTensor, nvinfer1::ElementWiseOperation::kSUM));\n    auto* curSumOutput = N_CHECK(curSum->getOutput(0));\n    runningSum->setInput(1, *curSumOutput);\n\n    auto reverseFlag = reverse == 1 ? nvinfer1::LoopOutput::kREVERSE : nvinfer1::LoopOutput::kCONCATENATE;\n    nvinfer1::ILoopOutputLayer* loopOut = loop->addLoopOutput(*curSumOutput, reverseFlag, axis);\n    loopOut->setInput(1, *tripLimit);\n\n    RETURN_FIRST_OUTPUT(loopOut, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(DeformConv)\n{\n    auto inputDataType = inputs.at(0).getDataType();\n    auto weightDataType = inputs.at(1).getDataType();\n    auto offsetDataType = inputs.at(2).getDataType();\n\n    ONNXTRT_CHECK_NODE((inputDataType == DataType::kFLOAT || inputDataType == DataType::kHALF),\n        \"Inputs must be either FLOAT or FLOAT16. Input type is \" + getTrtDtypeName(inputDataType) + \".\", node, nodeIdx,\n        ErrorCode::kINVALID_NODE);\n\n    ONNXTRT_CHECK_NODE((inputDataType == weightDataType && inputDataType == offsetDataType),\n        \"Inputs must be either all FLOAT or all FLOAT16. Input type = \" + getTrtDtypeName(inputDataType)\n            + \", weight type = \" + getTrtDtypeName(weightDataType)\n            + \", offset type = \" + getTrtDtypeName(offsetDataType) + \".\",\n        node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    if (inputs.size() > 3)\n    {\n        auto biasDataType = inputs.at(3).getDataType();\n        ONNXTRT_CHECK_NODE((inputDataType == biasDataType),\n            \"Inputs must be either all FLOAT or all FLOAT16. Input type = \" + getTrtDtypeName(inputDataType)\n                + \", bias type = \" + getTrtDtypeName(biasDataType) + \".\",\n            node, nodeIdx, ErrorCode::kINVALID_NODE);\n    }\n\n    if (inputs.size() > 4)\n    {\n        auto maskDataType = inputs.at(4).getDataType();\n        ONNXTRT_CHECK_NODE((inputDataType == maskDataType),\n            \"Inputs must be either all FLOAT or all FLOAT16. Input type = \" + getTrtDtypeName(inputDataType)\n                + \", mask type = \" + getTrtDtypeName(maskDataType) + \".\",\n            node, nodeIdx, ErrorCode::kINVALID_NODE);\n    }\n\n    return modulatedDeformableConvPluginHelper(ctx, node, nodeIdx, inputs);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(DepthToSpace)\n{\n    checkNotInvalidType(inputs.at(0), {\"BOOL\", \"UINT8\"}, node, nodeIdx);\n    // Input tensor is in NCHW format\n    ONNXTRT_CHECK_NODE((inputs.at(0).shape().nbDims == 4), \"The input tensor must be in NCHW format.\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n    nvinfer1::ITensor* tensorPtr = &convertToTensor(inputs.at(0), ctx);\n\n    // Extract attributes\n    OnnxAttrs attrs(node, ctx);\n    auto blockSize = attrs.get<int>(\"blocksize\");\n    auto mode = attrs.get<std::string>(\"mode\", \"DCR\");\n\n    // Useful constants\n    auto const inputShape = shapeOf(*tensorPtr);\n    auto const N = gather(ctx, inputShape, shapeVector(0));\n    auto const C = gather(ctx, inputShape, shapeVector(1));\n    auto const H = gather(ctx, inputShape, shapeVector(2));\n    auto const W = gather(ctx, inputShape, shapeVector(3));\n    auto const blockSizeTensor = shapeVector(blockSize);\n    auto const C_2 = floorDiv(ctx, C, mul(ctx, blockSizeTensor, blockSizeTensor));\n    auto const H_2 = mul(ctx, H, blockSizeTensor);\n    auto const W_2 = mul(ctx, W, blockSizeTensor);\n    int32_t const DCRPerm[6] = {0, 3, 4, 1, 5, 2};\n    int32_t const CRDPerm[6] = {0, 1, 4, 2, 5, 3};\n\n    ShapeTensor firstShape;\n    nvinfer1::Permutation perm{};\n\n    if (mode == \"DCR\")\n    {\n        // First reshape to {N, blockSize, blockSize, C / (blockSize * blockSize), H, W}\n        firstShape = concat(\n            ctx, N, concat(ctx, blockSizeTensor, concat(ctx, blockSizeTensor, concat(ctx, C_2, concat(ctx, H, W)))));\n        std::copy(std::begin(DCRPerm), std::end(DCRPerm), std::begin(perm.order));\n    }\n    else\n    {\n        // First reshape to {N, C / (blockSize * blockSize), blockSize, blockSize, H, W}\n        firstShape = concat(\n            ctx, N, concat(ctx, C_2, concat(ctx, blockSizeTensor, concat(ctx, blockSizeTensor, concat(ctx, H, W)))));\n        std::copy(std::begin(CRDPerm), std::end(CRDPerm), std::begin(perm.order));\n    }\n\n    auto* firstShuffle = addShuffle(ctx, *tensorPtr, firstShape);\n    firstShuffle->setSecondTranspose(perm);\n    ctx->registerLayer(firstShuffle, node);\n    tensorPtr = N_CHECK(firstShuffle->getOutput(0));\n\n    // Finally reshape to {N, C / (blockSize * blockSize), H * blockSize, W * blockSize};\n    auto secondShape = concat(ctx, N, concat(ctx, C_2, concat(ctx, H_2, W_2)));\n    auto* secondShuffle = addShuffle(ctx, *tensorPtr, secondShape);\n    tensorPtr = N_CHECK(secondShuffle->getOutput(0));\n\n    return {{tensorPtr}};\n}\n\n// Backward traverse the graph to retrieve the input weights from the constant node. We allow skipping all cast/identity\n// nodes until reaching the constant node.\nShapedWeights getWeightsFromIdentityOrConstant(nvinfer1::INetworkDefinition& network, nvinfer1::ITensor* input)\n{\n    // Const node output -> const node mapping.\n    std::unordered_map<nvinfer1::ITensor*, nvinfer1::IConstantLayer*> constNodeToOutputMap;\n    // Identity node output -> identity/cast node mapping.\n    std::unordered_map<nvinfer1::ITensor*, nvinfer1::ILayer*> identityCastNodeToOutputMap;\n\n    // Collect all the constant, identity nodes from network.\n    int32_t nbLayers = network.getNbLayers();\n    for (int32_t i = 0; i < nbLayers; ++i)\n    {\n        nvinfer1::ILayer* layer = N_CHECK(network.getLayer(i));\n        if (layer->getType() == nvinfer1::LayerType::kCONSTANT)\n        {\n            constNodeToOutputMap[layer->getOutput(0)] = static_cast<nvinfer1::IConstantLayer*>(layer);\n        }\n        else if ((layer->getType() == nvinfer1::LayerType::kIDENTITY)\n            || (layer->getType() == nvinfer1::LayerType::kCAST))\n        {\n            identityCastNodeToOutputMap[layer->getOutput(0)] = layer;\n        }\n    }\n    // Skip all the cast/identity nodes before current node.\n    auto findIdenityIter = identityCastNodeToOutputMap.find(input);\n    while (findIdenityIter != identityCastNodeToOutputMap.end())\n    {\n        input = findIdenityIter->second->getInput(0);\n        findIdenityIter = identityCastNodeToOutputMap.find(input);\n    }\n    // Find out the weights from constant node.\n    auto findConstIter = constNodeToOutputMap.find(input);\n    if (findConstIter != constNodeToOutputMap.end())\n    {\n        auto weights = findConstIter->second->getWeights();\n        return ShapedWeights(\n            trtDataTypeToONNX(weights.type), const_cast<void*>(weights.values), findConstIter->second->getDimensions());\n    }\n    // Return empty weights when not found.\n    return ShapedWeights{};\n}\n\n// This is a helper function for QuantizeLinear/DequantizeLinear\nNodeOutputs QuantDequantLinearHelper(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t nodeIdx,\n    std::vector<TensorOrWeights>& inputs, bool isDQ, bool isCustomOp, DataType customOpType = DataType::kFP8)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n\n    // For QuantizeLinear, the output type (and thus quantization type) is dependent on the second input (zero point).\n    if (!isDQ && inputs.size() >= 3)\n    {\n        checkNotInvalidType(inputs.at(2), {\"UINT8\"}, node, nodeIdx);\n    }\n    auto addConstantLayer\n        = [ctx, node](nvinfer1::INetworkDefinition& network, ShapedWeights const& weights) -> nvinfer1::ITensor* {\n        nvinfer1::IConstantLayer* constLayer = N_CHECK(network.addConstant(weights.shape, weights));\n        ctx->registerLayer(constLayer, weights.getName(), &node);\n        network.setWeightsName(weights, weights.getName());\n        return N_CHECK(constLayer->getOutput(0));\n    };\n\n    auto newConstantInput = [&](int32_t i) {\n        return inputs.at(i).is_weights() && (ctx->getConstantLayer(inputs.at(i).weights().getName()) == nullptr);\n    };\n\n    // Read the optional quantization axis attribute. Set it to the rank of the input tensor if not provided\n    ONNXTRT_CHECK_NODE((inputs.size() >= 2),\n        \"This version of TensorRT requires at least 2 inputs for the QuantizeLinear/DequantizeLinear operator.\", node,\n        nodeIdx, nvonnxparser::ErrorCode::kINVALID_NODE);\n\n    std::string nodeName = getNodeName(node);\n    // Input 0 is the data to quantize or dequantize.\n    nvinfer1::ITensor* dataInput = &convertToTensor(inputs.at(0), ctx);\n\n    // Input 1 initializes the layer's scale weights.\n    nvinfer1::ITensor* scaleInput = nullptr;\n    if (newConstantInput(1))\n    {\n        // Scale is concrete so verify it now.\n        auto scale = inputs.at(1).weights();\n        ONNXTRT_CHECK_NODE(\n            scale.count() > 0, \"Cannot have scale with no coefficients.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n        bool scaleAllPositive = false;\n        if (inputs.at(1).isFp32())\n        {\n            auto const* scaleVal = static_cast<float const*>(scale.values);\n            scaleAllPositive = std::all_of(scaleVal, scaleVal + scale.count(), [](float x) { return x > 0; });\n        }\n        else if (inputs.at(1).isFp16())\n        {\n            auto const* scaleVal = static_cast<half_float::half const*>(scale.values);\n            scaleAllPositive\n                = std::all_of(scaleVal, scaleVal + scale.count(), [](half_float::half x) { return x > 0; });\n        }\n        else if (inputs.at(1).isBFp16())\n        {\n            auto const* scaleVal = static_cast<BFloat16 const*>(scale.values);\n            scaleAllPositive = std::all_of(scaleVal, scaleVal + scale.count(), [](BFloat16 x) { return x > 0; });\n        }\n        ONNXTRT_CHECK_NODE(\n            scaleAllPositive, \"Scale coefficients must all be positive\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n        // If the scale is concrete weights, then add a ConstantLayer that will be an input which\n        // will initialize the scale weights.\n        scaleInput = addConstantLayer(*ctx->network(), scale);\n    }\n    else\n    {\n        scaleInput = &convertToTensor(inputs.at(1), ctx);\n    }\n    auto const& inputDims = dataInput->getDimensions();\n    auto const& scaleDims = scaleInput->getDimensions();\n    auto const& scaleType = scaleInput->getType();\n\n    auto const& scaleSize = isDynamic(scaleDims) ? 0 : volume(scaleDims);\n\n    // Input 2 initializes the layer's zero-point.\n    nvinfer1::ITensor* zeroPointInput = nullptr;\n    // ONNX default is UINT8, TRT will default to INT8 as TRT doesn't allow UINT8 quantization\n    // When importing CustomOp FP8/INT4 Q/DQ, default to FP8/INT4\n    DataType chosenDataType = isCustomOp ? customOpType : DataType::kINT8;\n    ONNXTRT_CHECK_NODE(!isCustomOp || customOpType == DataType::kFP8 || customOpType == DataType::kINT4,\n        \"Custom QDQ ops are available only for FP8 and INT4\", node, nodeIdx, ErrorCode::kINTERNAL_ERROR);\n\n    OnnxAttrs attrs(node, ctx);\n    DataType outputDtype;\n    auto const outputDTypeOnnx = attrs.get<int32_t>(\"output_dtype\", ::ONNX_NAMESPACE::TensorProto::UNDEFINED);\n    bool isOutputDtypeSet = (outputDTypeOnnx != ::ONNX_NAMESPACE::TensorProto::UNDEFINED);\n    if (isOutputDtypeSet)\n    {\n        isOutputDtypeSet = convertDtype(outputDTypeOnnx, &outputDtype);\n        ONNXTRT_CHECK_NODE(isOutputDtypeSet,\n            \"Attribute output_dtype specifies an unsupported data type \" << outputDtype << \".\", node, nodeIdx,\n            nvonnxparser::ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    if (inputs.size() > 2)\n    {\n        // ONNX spec definition is that when zero point is set, use its datatype for quantization\n        DataType zeroPointDataType = inputs.at(2).getDataType();\n        ONNXTRT_CHECK_NODE(!isOutputDtypeSet || outputDtype == zeroPointDataType,\n            \"Mismatch between attribute output_dtype \" << outputDtype << \" and zero-point data type \"\n                                                       << zeroPointDataType << \".\",\n            node, nodeIdx, nvonnxparser::ErrorCode::kINVALID_NODE);\n        if (zeroPointDataType == DataType::kFP8 || zeroPointDataType == DataType::kINT8\n            || zeroPointDataType == DataType::kINT4)\n        {\n            chosenDataType = zeroPointDataType;\n        }\n        else\n        {\n            // If zero point is set to UINT8, default to INT8.\n            LOG_WARNING(\n                \"TensorRT doesn't support QuantizeLinear/DequantizeLinear with UINT8 zero_point. TensorRT will use \"\n                \"INT8 instead.\");\n            chosenDataType = DataType::kINT8;\n        }\n\n        if (chosenDataType != DataType::kFP8)\n        {\n            // For patterns \"const\" -> Q/DQ, the zero point constant can be shared between different Q/DQ. For pattern\n            // like \"const\" -> \"identity\" -> Q/DQ. We have to create new constant because its type is INT8 in ONNX but\n            // TRT expect FP32 zero point. To handle both case, we always create new constant for zero point.\n            auto& zeroPtInput = inputs.at(2);\n            ShapedWeights zeroPoint{};\n            if (zeroPtInput.is_tensor())\n            {\n                // Look backward to find out the original weights in \"Constant\" node.\n                zeroPoint = getWeightsFromIdentityOrConstant(*ctx->network(), &zeroPtInput.tensor());\n            }\n            else\n            {\n                zeroPoint = zeroPtInput.weights();\n                ONNXTRT_CHECK_NODE(zeroPoint.values,\n                    \"QuantizeLinear/DequantizeLinear operator must contains all zeros values.\", node, nodeIdx,\n                    nvonnxparser::ErrorCode::kINVALID_NODE);\n            }\n            if (!zeroPoint.values)\n            {\n                // Cannot static analysis the zero point values from Q/DQ, fallback to use the activation input.\n                zeroPointInput = &convertToTensor(inputs.at(2), ctx);\n            }\n            else\n            {\n                // Create new constant for zero input.\n                ONNXTRT_CHECK_NODE(shiftIsAllZeros(zeroPoint),\n                    \"TensorRT only supports symmetric quantization. The zero point for the \"\n                    \"QuantizeLinear/DequantizeLinear operator must be all zeros.\",\n                    node, nodeIdx, nvonnxparser::ErrorCode::kINVALID_NODE);\n\n                // Convert the zero-point to float because TRT uses float for zero-point. Note this zero-point is not\n                // refittable because refit need the same data type as builder time.\n                auto fpZeroPoint = createZeroShifts(zeroPoint, ::ONNX_NAMESPACE::TensorProto::FLOAT, ctx);\n                zeroPointInput = addConstantLayer(*ctx->network(), fpZeroPoint);\n            }\n\n            if (zeroPointInput && !isDynamic(scaleDims))\n            {\n                auto const zeroPointSize = volume(zeroPointInput->getDimensions());\n                // ONNX may represent a scalar using either 0-D or 1-D, so compare sizes instead of shapes.\n                ONNXTRT_CHECK_NODE(zeroPointSize == scaleSize,\n                    \"The scale and zero point must have the same volume. Size of zero point = \"\n                        << zeroPointSize << \", size of the scale = \" << scaleSize << \".\",\n                    node, nodeIdx, nvonnxparser::ErrorCode::kINVALID_NODE);\n            }\n        }\n    }\n    else if (isOutputDtypeSet)\n    {\n        ONNXTRT_CHECK_NODE(\n            outputDtype == DataType::kFP8 || outputDtype == DataType::kINT8 || outputDtype == DataType::kINT4,\n            \"Attribute output_dtype specifies an invalid data type \" << outputDtype << \".\", node, nodeIdx,\n            nvonnxparser::ErrorCode::kINVALID_NODE);\n        chosenDataType = outputDtype;\n    }\n\n    int32_t axis = attrs.get<int32_t>(\"axis\", inputDims.nbDims);\n    convertAxis(axis, inputDims.nbDims, node, nodeIdx);\n\n    if (scaleSize != 1)\n    {\n        // Per-Channel Quantization.\n        // We assume this is weight-quantization with dimensions KCRS (K is # output channels).\n        // Activations-quantization does not support per-axis quantization.\n        if (axis == inputDims.nbDims)\n        {\n            axis = 0;\n        }\n        if (scaleDims.nbDims == 1 && !isDynamic(scaleDims))\n        {\n            // Ensure that number of scale-coefficients is equal to the number of output channels.\n            int64_t const K = dataInput->getDimensions().d[axis];\n            ONNXTRT_CHECK_NODE(K == scaleSize,\n                \"The number of scales is not equal to the number of output channels. Number of output channels = \"\n                    << K << \", number of scales = \" << scaleSize << \".\",\n                node, nodeIdx, nvonnxparser::ErrorCode::kINVALID_NODE);\n        }\n        else if (scaleDims.nbDims == inputDims.nbDims)\n        {\n            // Exactly one dimension is blocked, other should have the same dimension as the input\n            if (!isDynamic(inputDims) && !isDynamic(scaleDims))\n            {\n                int32_t rank = inputDims.nbDims;\n                std::vector<int32_t> blockDims(rank);\n                std::transform(\n                    inputDims.d, inputDims.d + rank, scaleDims.d, blockDims.begin(), std::divides<int32_t>());\n\n                auto equals_one = [](int32_t i) { return i == 1; };\n                ONNXTRT_CHECK_NODE(std::count_if(blockDims.begin(), blockDims.end(), equals_one) == rank - 1,\n                    \"Only a single blocking dimension is allowed.\", node, nodeIdx,\n                    nvonnxparser::ErrorCode::kINVALID_NODE);\n\n                auto const inputSize = volume(inputDims);\n                ONNXTRT_CHECK_NODE(inputSize % scaleSize == 0,\n                    \"Inferred block size is not an integer. Input volume = \" << inputSize\n                                                                             << \", scale volume = \" << scaleSize << \".\",\n                    node, nodeIdx, nvonnxparser::ErrorCode::kINVALID_NODE);\n            }\n        }\n        else\n        {\n            ONNXTRT_CHECK_NODE(false, \"Invalid rank for the scale tensor. Rank = \" << scaleDims.nbDims << \".\", node,\n                nodeIdx, nvonnxparser::ErrorCode::kINVALID_NODE);\n        }\n    }\n    else\n    {\n        // Per-Tensor Quantization.\n        // Currently axis is ignored by TRT, but it is required here by addScaleNd (for computing nbSpatialDims). Set to\n        // a sane default depending on rank the input tensor.\n        axis = inputDims.nbDims <= 1 ? 0 : 1;\n    }\n\n    // TRT does not support scalar data input for Q/DQ layers, convert 0-D tensor to 1-D first.\n    if (inputDims.nbDims == 0)\n    {\n        dataInput = reshapeTensor(ctx, *dataInput, nvinfer1::Dims{1, {1}});\n    }\n\n    // INT4 requires an even last-dimension due to packing restrictions\n    if (!isDynamic(inputDims) && (chosenDataType == DataType::kINT4))\n    {\n        auto const inputSize = volume(inputDims);\n        ONNXTRT_CHECK_NODE((inputSize % 2 == 0), \"4-bit quantization requies an even number of elements.\", node,\n            nodeIdx, nvonnxparser::ErrorCode::kINVALID_NODE);\n    }\n\n    nvinfer1::ILayer* layer = nullptr;\n    ONNXTRT_CHECK_NODE(\n        (chosenDataType == DataType::kINT8 || chosenDataType == DataType::kFP8 || chosenDataType == DataType::kINT4),\n        \"TensorRT only allows FP8, INT8, and INT4 quantization. The requested quantization type is\"\n            + getTrtDtypeName(chosenDataType) + \".\",\n        node, nodeIdx, nvonnxparser::ErrorCode::kINVALID_NODE);\n\n    bool stronglyTyped = ctx->isStronglyTyped();\n    if (isDQ)\n    {\n        // Add and configure a DequantizeLayer.\n        if (stronglyTyped)\n        {\n            // Input type is inferred. Layer output type is specified with scaleType.\n            nvinfer1::IDequantizeLayer* dq = N_CHECK(ctx->network()->addDequantize(*dataInput, *scaleInput, scaleType));\n            dq->setAxis(axis);\n            layer = dq;\n        }\n        else\n        {\n            // Use legacy API for weakly typed network.\n            nvinfer1::IDequantizeLayer* dq = N_CHECK(ctx->network()->addDequantize(*dataInput, *scaleInput));\n            dq->setAxis(axis);\n            layer = dq;\n            // Type constraint for layer output type.\n            layer->setOutputType(0, scaleType);\n        }\n    }\n    else\n    {\n        // Add and configure a QuantizeLayer.\n        if (stronglyTyped)\n        {\n            if (ctx->getOpsetVersion() < 19 && scaleInput->getType() != dataInput->getType())\n            {\n                // Ensure that Q scale type matches input type.\n                auto* scaleCastLayer = N_CHECK(ctx->network()->addCast(*scaleInput, dataInput->getType()));\n                scaleInput = N_CHECK(scaleCastLayer->getOutput(0));\n            }\n            // Input type is inferred. Layer output type is specified with chosenDataType.\n            nvinfer1::IQuantizeLayer* q = N_CHECK(ctx->network()->addQuantize(*dataInput, *scaleInput, chosenDataType));\n            q->setAxis(axis);\n            layer = q;\n        }\n        else\n        {\n            // Use legacy API for weakly typed network.\n            nvinfer1::IQuantizeLayer* q = N_CHECK(ctx->network()->addQuantize(*dataInput, *scaleInput));\n            q->setAxis(axis);\n            layer = q;\n            // This implictly sets layer input type.\n            layer->setPrecision(scaleType);\n            // Type constraint for layer output type.\n            layer->setOutputType(0, chosenDataType);\n        }\n    }\n\n    layer->setName(nodeName.c_str());\n    if (zeroPointInput)\n    {\n        layer->setInput(2, *zeroPointInput);\n    }\n\n    // Register the Q/DQ layer.\n    ctx->registerLayer(layer, node);\n\n    // Return layer output\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\n\nDEFINE_BUILTIN_OP_IMPORTER(QuantizeLinear)\n{\n    return QuantDequantLinearHelper(ctx, node, nodeIdx, inputs, false /*isDQ*/, false /*isCustomOp*/);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(DequantizeLinear)\n{\n    return QuantDequantLinearHelper(ctx, node, nodeIdx, inputs, true /*isDQ*/, false /*isCustomOp*/);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_FP8QuantizeLinear)\n{\n    return QuantDequantLinearHelper(\n        ctx, node, nodeIdx, inputs, false /*isDQ*/, true /*isCustomOp*/, DataType::kFP8 /*customOpType*/);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_FP8DequantizeLinear)\n{\n    return QuantDequantLinearHelper(\n        ctx, node, nodeIdx, inputs, true /*isDQ*/, true /*isCustomOp*/, DataType::kFP8 /*customOpType*/);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_INT4QuantizeLinear)\n{\n    return QuantDequantLinearHelper(\n        ctx, node, nodeIdx, inputs, false /*isDQ*/, true /*isCustomOp*/, DataType::kINT4 /*customOpType*/);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_INT4DequantizeLinear)\n{\n    return QuantDequantLinearHelper(\n        ctx, node, nodeIdx, inputs, true /*isDQ*/, true /*isCustomOp*/, DataType::kINT4 /*customOpType*/);\n}\n\nDECLARE_BUILTIN_OP_IMPORTER(Mul);\nDEFINE_BUILTIN_OP_IMPORTER(Div)\n{\n    return elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kDIV);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Dropout)\n{\n    // TensorRT does not support the Dropout operator with training mode.\n    // The source of training mode information comes from :\n    // 1. Pre-opset 6: attribute is_test = 0\n    // 2. Post-opset 12: input[2] training_mode = true.\n    //      We can deal with the cases where training_mode is an initializer.\n    if (ctx->getOpsetVersion() >= 12 && node.input().size() == 3)\n    {\n        ONNXTRT_CHECK_NODE(inputs.at(2).is_weights(),\n            \"This Version of TensorRT only supports the training_mode input as an initializer.\", node, nodeIdx,\n            ErrorCode::kUNSUPPORTED_NODE);\n        std::vector<int64_t> trainingMode;\n        weightsToVector<int64_t>(inputs.at(2).weights(), &trainingMode);\n        ONNXTRT_CHECK_NODE(!trainingMode[0], \"TensorRT does not support the Dropout operator in training mode.\", node,\n            nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    size_t noutputs = node.output().size();\n    if (noutputs == 1)\n    {\n        RETURN_IDENTITY(inputs.at(0), node, nodeIdx);\n    }\n    else\n    {\n        // Add identity layer twice for both Dropout outputs: (output + mask)\n        std::vector<TensorOrWeights> outputs;\n        outputs.push_back(identity(ctx, inputs.at(0)));\n\n        // Add mask tensor, which is the same shape as the input tensor\n        auto& inputTensor = inputs.at(0).tensor();\n        nvinfer1::ITensor* maskTensor{nullptr};\n        // Post opset 12 the mask tensor contains all 1s. Prior to opset 12 the mask tensor contains all 0s.\n        if (ctx->getOpsetVersion() >= 12)\n        {\n            maskTensor = getElementWiseResult(ctx, inputTensor, inputTensor, nvinfer1::ElementWiseOperation::kEQUAL);\n        }\n        else\n        {\n            maskTensor = getElementWiseResult(ctx, inputTensor, inputTensor, nvinfer1::ElementWiseOperation::kLESS);\n        }\n        outputs.push_back(TensorOrWeights(maskTensor));\n        return outputs;\n    }\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Einsum)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    OnnxAttrs attrs(node, ctx);\n    std::string equation = attrs.get<std::string>(\"equation\");\n\n    ONNXTRT_CHECK_NODE((!inputs.empty()), \"Inputs vector is empty.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    std::vector<nvinfer1::ITensor*> inputTensors;\n\n    for (auto input : inputs)\n    {\n        auto* tensor_ptr = &convertToTensor(input, ctx);\n        inputTensors.push_back(tensor_ptr);\n    }\n    auto nbInputs = static_cast<int64_t>(inputTensors.size());\n\n    bool withEllipsis{false};\n    if (equation.find(\"...\") != std::string::npos)\n    {\n        withEllipsis = true;\n    }\n\n    if (withEllipsis || nbInputs > 2)\n    {\n        LOG_VERBOSE(\"Equation before preprocessing ellipsis and output: \" << equation);\n        processEllipsisAndImplicitOutput(inputTensors, equation, withEllipsis);\n        LOG_VERBOSE(\"Equation after preprocessing ellipsis and output: \" << equation);\n    }\n\n    nvinfer1::IEinsumLayer* einsumLayer{nullptr};\n    if (nbInputs > 2)\n    {\n        einsumLayer = parseGraphWithMoreInputs(ctx, node, inputTensors, nbInputs, equation);\n    }\n    else\n    {\n        einsumLayer = N_CHECK(ctx->network()->addEinsum(inputTensors.data(), nbInputs, equation.c_str()));\n        ctx->registerLayer(einsumLayer, node);\n    }\n\n    RETURN_FIRST_OUTPUT(einsumLayer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Elu)\n{\n    OnnxAttrs attrs(node, ctx);\n    float alpha = attrs.get<float>(\"alpha\", 1.f);\n    return activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kELU, &alpha);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Equal)\n{\n    return elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kEQUAL);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Erf)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kERF);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Exp)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kEXP);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Expand)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    // \"Broadcast the input tensor following the given shape and the broadcast rule.\"\n    nvinfer1::ITensor& inputTensor = convertToTensor(inputs.at(0), ctx);\n    auto const inputDims = shapeOf(inputTensor);\n    auto const inputRank = shapeOf(inputDims);\n\n    // \"A 1-D tensor indicates the shape you want to expand to, following the broadcast rule\"\n    ONNXTRT_CHECK_NODE((inputs.at(1).shape().nbDims == 1), \"The shape tensor is required to be 1D.\", node, nodeIdx,\n        ErrorCode::kINVALID_VALUE);\n    ShapeTensor shape{ctx, inputs.at(1)};\n    auto const shapeLength = shapeOf(shape);\n\n    ShapeTensor const newRank = max(ctx, shapeLength, inputRank);\n    // \"Dimensions are right alignment;...\"\n    ShapeTensor const newDims = concat(ctx, fillShapeVector(ctx, 1, sub(ctx, newRank, inputRank)), inputDims);\n    nvinfer1::ITensor& newInputTensor = reshape(ctx, inputTensor, newDims);\n\n    // \", or the shape.ndim < input.shape.ndim\"\n    ShapeTensor newShape = concat(ctx, fillShapeVector(ctx, 1, sub(ctx, newRank, shapeLength)), shape);\n\n    ShapeTensor const starts = similar(ctx, newDims, 0);\n    // Do the broadcast rule.\n    ShapeTensor const sizes = broadcast(ctx, newDims, newShape);\n    // Compute (x > 1 ? 1 : 0) for x in newDims, assuming positive x, using only TensorRT operations.\n    ShapeTensor const one = shapeVector(1);\n    ShapeTensor const strides = min(ctx, one, sub(ctx, newDims, one));\n\n    nvinfer1::ISliceLayer* sliceLayer = addSlice(ctx, newInputTensor, starts, sizes, strides);\n    ctx->registerLayer(sliceLayer, node);\n\n    RETURN_FIRST_OUTPUT(sliceLayer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(EyeLike)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    // Get input node.\n    nvinfer1::ITensor& tensor = convertToTensor(inputs.at(0), ctx);\n    OnnxAttrs attrs(node, ctx);\n    int32_t k = attrs.get(\"k\", 0);\n\n    // \"Only 2D tensors are supported, i.e. input T1 must be of rank 2...\"\n    nvinfer1::Dims dims = tensor.getDimensions();\n    ONNXTRT_CHECK_NODE(dims.nbDims == 2, \"Only 2D tensors are supported. Input must be of rank 2.\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n\n    // The data type can be specified by the 'dtype' argument\n    DataType dtype = tensor.getType();\n    if (attrs.count(\"dtype\"))\n    {\n        auto onnxType = attrs.get<int32_t>(\"dtype\");\n        ONNXTRT_CHECK_NODE(\n            convertDtype(onnxType, &dtype), \"Unsupported cast!\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n        LOG_VERBOSE(\"Casting to type: \" << dtype);\n    }\n\n    // Create weights and constant layer\n    ONNXTRT_CHECK_NODE(!isDynamic(dims), \"Eyelike does not work for dynamically shaped tensors.\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n    int totalWeights = dims.d[0] * dims.d[1];\n    std::vector<int> values(totalWeights);\n    for (int32_t r = 0; r < dims.d[0]; ++r)\n    {\n        for (int32_t c = 0; c < dims.d[1]; ++c)\n        {\n            values[r * dims.d[1] + c] = 0;\n            if (c - r == k)\n            {\n                values[r * dims.d[1] + c] = 1;\n            }\n        }\n    }\n\n    ShapedWeights tempWeights = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto::INT32, dims);\n    std::memcpy(tempWeights.values, values.data(), values.size() * sizeof(int));\n    auto* layer = N_CHECK(ctx->network()->addConstant(dims, tempWeights));\n    ctx->registerLayer(layer, node);\n    auto* layerOutput = N_CHECK(layer->getOutput(0));\n\n    if (dtype != DataType::kINT32)\n    {\n        return {{castHelper(ctx, layerOutput, dtype)}};\n    }\n    return {{layerOutput}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Flatten)\n{\n    OnnxAttrs attrs(node, ctx);\n    nvinfer1::ITensor* tensorPtr = &convertToTensor(inputs.at(0), ctx);\n    int32_t nbDims = tensorPtr->getDimensions().nbDims;\n    int32_t axis = attrs.get(\"axis\", 1);\n    convertAxis(axis, nbDims, node, nodeIdx);\n\n    // No-op Flatten: (a, b) => Flatten(axis = 1) => (a, b)\n    // Add identity layer to avoid name mangling of engine bindings\n    // For rest of configurations, we must flatten.\n    if (nbDims == 2 && axis == 1)\n    {\n        RETURN_IDENTITY(inputs.at(0), node, nodeIdx);\n    }\n\n    tensorPtr = flattenTensor(ctx, node, *tensorPtr, axis, true);\n    ONNXTRT_CHECK_NODE(tensorPtr, \"Failed to flatten the tensor.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    return {{tensorPtr}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Floor)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kFLOOR);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Gather)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    nvinfer1::ITensor& data = convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor* indices = &convertToTensor(inputs.at(1), ctx);\n    OnnxAttrs attrs(node, ctx);\n    int32_t axis = attrs.get<int32_t>(\"axis\", 0);\n    int32_t nbDims = inputs.at(0).shape().nbDims;\n    convertAxis(axis, nbDims, node, nodeIdx);\n    LOG_VERBOSE(\"Using Gather axis: \" << axis);\n\n    if (inputs.at(0).getType() != \"INT64\" && inputs.at(1).getType() == \"INT64\")\n    {\n        // Int64 indices only supported for Int64 data\n        indices = castHelper(ctx, indices, DataType::kINT32);\n    }\n    auto* layer = N_CHECK(ctx->network()->addGather(data, *indices, axis));\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(GatherElements)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    nvinfer1::ITensor& data = convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor* indices = &convertToTensor(inputs.at(1), ctx);\n    nvinfer1::Dims const& dataDims = data.getDimensions();\n\n    OnnxAttrs attrs(node, ctx);\n    int32_t axis = attrs.get<int32_t>(\"axis\", 0);\n    int32_t const dataNbDims = dataDims.nbDims;\n    convertAxis(axis, dataNbDims, node, nodeIdx);\n    LOG_VERBOSE(\"Using Gather axis: \" << axis);\n\n    if (inputs.at(0).getType() != \"INT64\")\n    {\n        // Int64 indices only supported for Int64 data\n        indices = castHelper(ctx, indices, DataType::kINT32);\n    }\n    auto* layer = N_CHECK(ctx->network()->addGatherV2(data, *indices, nvinfer1::GatherMode::kELEMENT));\n    layer->setGatherAxis(axis);\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(GatherND)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    nvinfer1::ITensor& data = convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor* indices = &convertToTensor(inputs.at(1), ctx);\n\n    OnnxAttrs attrs(node, ctx);\n    auto const nbElementWiseDims = attrs.get<int32_t>(\"batch_dims\", 0);\n\n    if (inputs.at(0).getType() != \"INT64\")\n    {\n        // Int64 indices only supported for Int64 data\n        indices = castHelper(ctx, indices, DataType::kINT32);\n    }\n    auto* layer = ctx->network()->addGatherV2(data, *indices, nvinfer1::GatherMode::kND);\n    ONNXTRT_CHECK_NODE(layer, \"Failed to create layer\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    layer->setNbElementWiseDims(nbElementWiseDims);\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Gelu)\n{\n    OnnxAttrs attrs(node, ctx);\n    auto const approximate = attrs.get(\"approximate\", std::string(\"none\"));\n    if (approximate == std::string(\"none\"))\n    {\n        return activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kGELU_ERF);\n    }\n    else if (approximate == std::string(\"tanh\"))\n    {\n        return activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kGELU_TANH);\n    }\n    else\n    {\n        ONNXTRT_CHECK_NODE(false, \"Invalid value provided for the Gelu \\'approximate\\' attribute: \" << approximate,\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE_ATTR);\n    }\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Gemm)\n{\n    checkNotInvalidType(inputs.at(0), {\"INT32\", \"INT64\"}, node, nodeIdx);\n    OnnxAttrs attrs(node, ctx);\n    float alpha = attrs.get(\"alpha\", 1.f);\n    float beta = attrs.get(\"beta\", 1.f);\n    bool transA = attrs.get(\"transA\", false);\n    bool transB = attrs.get(\"transB\", false);\n    nvinfer1::ITensor& inputA = convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor& inputB = convertToTensor(inputs.at(1), ctx);\n    // Validate inputs\n    ONNXTRT_CHECK_NODE(inputA.getDimensions().nbDims == 2 && inputB.getDimensions().nbDims == 2,\n        \"GEMM must have 2D inputs! inputA has rank \" << inputA.getDimensions().nbDims << \", inputB has rank \"\n                                                     << inputB.getDimensions().nbDims << \".\",\n        node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    auto const getMatrixOp = [](nvinfer1::ITensor const& input, bool transpose) {\n        if (input.getDimensions().nbDims == 1)\n        {\n            return nvinfer1::MatrixOperation::kVECTOR;\n        }\n        if (transpose)\n        {\n            return nvinfer1::MatrixOperation::kTRANSPOSE;\n        }\n        return nvinfer1::MatrixOperation::kNONE;\n    };\n\n    nvinfer1::MatrixOperation opA = getMatrixOp(inputA, transA);\n    nvinfer1::MatrixOperation opB = getMatrixOp(inputB, transB);\n\n    LOG_VERBOSE(\"Using opA: \" << static_cast<int>(opA) << \" opB: \" << static_cast<int>(opB));\n\n    nvinfer1::IMatrixMultiplyLayer* matmul = N_CHECK(ctx->network()->addMatrixMultiply(inputA, opA, inputB, opB));\n    ctx->registerLayer(matmul, node);\n\n    nvinfer1::ITensor* matmulTensor = N_CHECK(matmul->getOutput(0));\n\n    // Scale A*B if needed.\n    if (alpha != 1.f)\n    {\n        nvinfer1::IConstantLayer* alphaConstant\n            = addConstantScalar(ctx, alpha, ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT);\n        nvinfer1::ITensor* alphaConstantTensor = N_CHECK(alphaConstant->getOutput(0));\n        broadcastTensors(ctx, alphaConstantTensor, matmulTensor);\n        nvinfer1::IElementWiseLayer* scaledMatmul = N_CHECK(\n            ctx->network()->addElementWise(*alphaConstantTensor, *matmulTensor, nvinfer1::ElementWiseOperation::kPROD));\n        matmulTensor = N_CHECK(scaledMatmul->getOutput(0));\n    }\n\n    // In opset 11, the bias tensor is an optional input\n    if (inputs.size() > 2)\n    {\n        nvinfer1::ITensor* biasTensor = &convertToTensor(inputs.at(2), ctx);\n\n        // Scale C if needed\n        if (beta != 1.f)\n        {\n            nvinfer1::IConstantLayer* betaConstant\n                = addConstantScalar(ctx, beta, ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT);\n            nvinfer1::ITensor* betaConstantTensor = N_CHECK(betaConstant->getOutput(0));\n            broadcastTensors(ctx, betaConstantTensor, biasTensor);\n            nvinfer1::IElementWiseLayer* scaledBias = N_CHECK(ctx->network()->addElementWise(\n                *betaConstantTensor, *biasTensor, nvinfer1::ElementWiseOperation::kPROD));\n            biasTensor = N_CHECK(scaledBias->getOutput(0));\n        }\n        broadcastTensors(ctx, matmulTensor, biasTensor);\n        nvinfer1::IElementWiseLayer* biasAdd\n            = N_CHECK(ctx->network()->addElementWise(*matmulTensor, *biasTensor, nvinfer1::ElementWiseOperation::kSUM));\n        auto output = N_CHECK(biasAdd->getOutput(0));\n        return {{output}};\n    }\n\n    return {{matmulTensor}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(GlobalAveragePool)\n{\n    LOG_VERBOSE(\"GlobalAveragePool operators are implemented via Reduce layers rather than Pooling layers\");\n    return {{globalPoolingHelper(ctx, node, convertToTensor(inputs.at(0), ctx), nvinfer1::ReduceOperation::kAVG)}};\n}\n\n// GlobalLpPool: pow(reduce_sum(pow(x, p)), 1./p)\nDEFINE_BUILTIN_OP_IMPORTER(GlobalLpPool)\n{\n    auto& tensor = convertToTensor(inputs.at(0), ctx);\n    auto inputType = tensor.getType();\n    ONNXTRT_CHECK_NODE((inputType == DataType::kFLOAT || inputType == DataType::kHALF),\n        \"Only FLOAT and HALF are supported in GlobalLpPool. The current type = \" + getTrtDtypeName(inputType) + \".\",\n        node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    nvinfer1::Dims dims = tensor.getDimensions();\n\n    OnnxAttrs attrs{node, ctx};\n    float p = static_cast<float>(attrs.get(\"p\", 2));\n\n    // Add constants for p and 1/p\n    nvinfer1::Dims scalarDims{dims.nbDims};\n    std::fill(scalarDims.d, scalarDims.d + scalarDims.nbDims, 1);\n    nvinfer1::IConstantLayer* pLayer;\n    nvinfer1::IConstantLayer* pInvLayer;\n    if (inputType == DataType::kHALF)\n    {\n        pLayer = addConstantScalar(\n            ctx, static_cast<half_float::half>(p), ::ONNX_NAMESPACE::TensorProto::FLOAT16, scalarDims);\n        pInvLayer = addConstantScalar(\n            ctx, static_cast<half_float::half>(1.F / p), ::ONNX_NAMESPACE::TensorProto::FLOAT16, scalarDims);\n    }\n    else\n    {\n        pLayer = addConstantScalar(ctx, p, ::ONNX_NAMESPACE::TensorProto::FLOAT, scalarDims);\n        pInvLayer = addConstantScalar(ctx, 1.F / p, ::ONNX_NAMESPACE::TensorProto::FLOAT, scalarDims);\n    }\n\n    // firstPow = pow(x, p)\n    auto* firstPow\n        = getElementWiseResult(ctx, tensor, *N_CHECK(pLayer->getOutput(0)), nvinfer1::ElementWiseOperation::kPOW);\n    // reduced = reduce_sum(firstPow)\n    auto* reduced = globalPoolingHelper(ctx, node, *firstPow, nvinfer1::ReduceOperation::kSUM);\n    // finalPow = pow(reduced, 1./p)\n    auto* finalPow\n        = getElementWiseResult(ctx, *reduced, *N_CHECK(pInvLayer->getOutput(0)), nvinfer1::ElementWiseOperation::kPOW);\n    return {{finalPow}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(GlobalMaxPool)\n{\n    LOG_VERBOSE(\"GlobalMaxPool operators are implemented via Reduce layers rather than Pooling layers\");\n    return {{globalPoolingHelper(ctx, node, convertToTensor(inputs.at(0), ctx), nvinfer1::ReduceOperation::kMAX)}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Greater)\n{\n    return elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kGREATER);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(GreaterOrEqual)\n{\n    return greaterLessOrEqual(ctx, node, nodeIdx, &convertToTensor(inputs.at(0), ctx),\n        &convertToTensor(inputs.at(1), ctx),\n        /*greater*/ true);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(GroupNormalization)\n{\n    return normalizationHelper(ctx, node, nodeIdx, inputs);\n}\n\n// singlePassShape is the shape of the output from a single pass.\nnvinfer1::ITensor* concatenateRNNOutputs(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node,\n    nvinfer1::ILoop* loop, nvinfer1::ITensor* singlePassShape, nvinfer1::ITensor* sequenceLength,\n    nvinfer1::ITensor* concatenatedOutput, int numDirections, std::vector<TensorOrWeights>& inputs,\n    bool reverse = false)\n{\n    nvinfer1::ITensor* yOutput{nullptr};\n    if (numDirections == 2)\n    {\n        nvinfer1::ITensor* forwardStart = addConstant(ctx, std::vector<int32_t>{0, 0, 0},\n            ::ONNX_NAMESPACE::TensorProto::INT32,\n            nvinfer1::Dims{1, {3}})->getOutput(0);\n        nvinfer1::ITensor* reverseStart = addConstant(ctx, std::vector<int32_t>{1, 0, 0},\n            ::ONNX_NAMESPACE::TensorProto::INT32,\n            nvinfer1::Dims{1, {3}})->getOutput(0);\n\n        LOG_VERBOSE(\"Concatenated output shape: \" << concatenatedOutput->getDimensions());\n        nvinfer1::ISliceLayer* HtForwardLayer = N_CHECK(ctx->network()->addSlice(\n            *concatenatedOutput, nvinfer1::Dims3{0, 0, 0}, nvinfer1::Dims3{0, 0, 0}, nvinfer1::Dims3{1, 1, 1}));\n        auto forwardHt = N_CHECK(HtForwardLayer->getOutput(0));\n        LOG_VERBOSE(\"Forward pass shape: \" << forwardHt->getDimensions());\n        HtForwardLayer->setInput(1, *forwardStart);\n        HtForwardLayer->setInput(2, *singlePassShape);\n\n        nvinfer1::ISliceLayer* HtBackwardLayer = N_CHECK(ctx->network()->addSlice(\n            *concatenatedOutput, nvinfer1::Dims3{0, 0, 0}, nvinfer1::Dims3{0, 0, 0}, nvinfer1::Dims3{1, 1, 1}));\n        auto backwardHt = N_CHECK(HtBackwardLayer->getOutput(0));\n        LOG_VERBOSE(\"Reverse pass shape: \" << backwardHt->getDimensions());\n        HtBackwardLayer->setInput(1, *reverseStart);\n        HtBackwardLayer->setInput(2, *singlePassShape);\n\n        if (inputs.size() > 4 && inputs.at(4))\n        {\n            nvinfer1::ITensor* seqLens = &convertToTensor(inputs.at(4), ctx);\n            forwardHt = clearMissingSequenceElements(ctx, node, loop, seqLens, forwardHt, sequenceLength);\n            backwardHt\n                = clearMissingSequenceElements(ctx, node, loop, seqLens, backwardHt, sequenceLength, /*reverse=*/true);\n        }\n\n        nvinfer1::ILoopOutputLayer* forwardOutput\n            = N_CHECK(loop->addLoopOutput(*forwardHt, nvinfer1::LoopOutput::kCONCATENATE, 0));\n        forwardOutput->setInput(1, *sequenceLength);\n        nvinfer1::ILoopOutputLayer* reverseOutput\n            = N_CHECK(loop->addLoopOutput(*backwardHt, nvinfer1::LoopOutput::kREVERSE, 0));\n        reverseOutput->setInput(1, *sequenceLength);\n\n        auto fTensor = N_CHECK(forwardOutput->getOutput(0));\n        auto rTensor = N_CHECK(reverseOutput->getOutput(0));\n\n        std::array<nvinfer1::ITensor*, 2> passes{{fTensor, rTensor}};\n        nvinfer1::IConcatenationLayer* concat = ctx->network()->addConcatenation(passes.data(), passes.size());\n        concat->setAxis(1);\n        yOutput = N_CHECK(concat->getOutput(0));\n    }\n    else\n    {\n        if (inputs.size() > 4 && inputs.at(4))\n        {\n            nvinfer1::ITensor* seqLens = &convertToTensor(inputs.at(4), ctx);\n            concatenatedOutput\n                = clearMissingSequenceElements(ctx, node, loop, seqLens, concatenatedOutput, sequenceLength, reverse);\n        }\n        nvinfer1::ILoopOutputLayer* scanOut = N_CHECK(loop->addLoopOutput(\n            *concatenatedOutput, (reverse ? nvinfer1::LoopOutput::kREVERSE : nvinfer1::LoopOutput::kCONCATENATE), 0));\n        scanOut->setInput(1, *sequenceLength);\n        yOutput = N_CHECK(scanOut->getOutput(0));\n    }\n    return yOutput;\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(GRU)\n{\n    using nvinfer1::Dims;\n    using nvinfer1::Dims3;\n    using mOp = nvinfer1::MatrixOperation;\n    using eOp = nvinfer1::ElementWiseOperation;\n    using trtAct = nvinfer1::ActivationType;\n    nvinfer1::INetworkDefinition* net = ctx->network();\n\n    OnnxAttrs attrs{node, ctx};\n    constexpr int32_t NUM_GATES = 3;\n    std::string const direction = attrs.get<std::string>(\"direction\", \"forward\");\n    int32_t const numDirections = (direction == \"bidirectional\") ? 2 : 1;\n    int32_t const hiddenSize = attrs.get<int32_t>(\"hidden_size\");\n    int32_t const linearBeforeReset = attrs.get<int32_t>(\"linear_before_reset\", 0);\n    float const clip = attrs.get(\"clip\", -1.f); // Clipping cannot be negative, so -1.0 is a good sentinel value.\n\n    // The input is in SBE format\n    nvinfer1::ITensor* input = &convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor& weights = convertToTensor(inputs.at(1), ctx);\n    nvinfer1::ITensor& recurrenceWeights = convertToTensor(inputs.at(2), ctx);\n\n    std::vector<trtAct> defaultActs{trtAct::kSIGMOID, trtAct::kTANH};\n    if (numDirections == 2)\n    {\n        defaultActs.insert(defaultActs.end(), {trtAct::kSIGMOID, trtAct::kTANH});\n    }\n    std::vector<trtAct> activations = attrs.get<std::vector<trtAct>>(\"activations\", defaultActs);\n\n    std::vector<float> activationAlphas = attrs.get<std::vector<float>>(\"activation_alpha\", std::vector<float>{});\n    std::transform(activations.begin() + activationAlphas.size(), activations.end(),\n        std::back_inserter(activationAlphas), &getActivationDefaultAlpha);\n\n    std::vector<float> activationBetas = attrs.get<std::vector<float>>(\"activation_beta\", std::vector<float>{});\n    std::transform(activations.begin() + activationBetas.size(), activations.end(), std::back_inserter(activationBetas),\n        &getActivationDefaultBeta);\n\n    // Need to split weights/biases into ZR gates and H gate, because h(t) computations depend on z(t) and r(t).\n    nvinfer1::ITensor* numDirectionsTensor\n        = addConstantScalar(ctx, numDirections, ::ONNX_NAMESPACE::TensorProto::INT32, Dims{1, {1}})->getOutput(0);\n    nvinfer1::ITensor* hiddenSizeTensor\n        = addConstantScalar(ctx, hiddenSize, ::ONNX_NAMESPACE::TensorProto::INT32, Dims{1, {1}})->getOutput(0);\n    nvinfer1::ITensor* hiddenSizeDoubledTensor\n        = addConstantScalar(ctx, 2 * hiddenSize, ::ONNX_NAMESPACE::TensorProto::INT32, Dims{1, {1}})->getOutput(0);\n    nvinfer1::ITensor* eDimTensor = getAxisLength(ctx, input, 2, Dims{1, {1}});\n\n    nvinfer1::ITensor* weightsZRStart = addConstant(ctx, std::vector<int32_t>{0, 0, 0},\n        ::ONNX_NAMESPACE::TensorProto::INT32,\n        Dims{1, {3}})->getOutput(0);\n    auto* weightsZRSizeLayer = N_CHECK(net->addConcatenation(\n        std::array<nvinfer1::ITensor*, 3>{{numDirectionsTensor, hiddenSizeDoubledTensor, eDimTensor}}.data(), 3));\n    nvinfer1::ITensor* weightsZRSize = N_CHECK(weightsZRSizeLayer->getOutput(0));\n    nvinfer1::ISliceLayer* weightsZRLayer = N_CHECK(net->addSlice(weights, Dims{3}, Dims{3}, Dims3{1, 1, 1}));\n    ONNXTRT_CHECK_NODE(weightsZRLayer, \"Failed to create layer\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    weightsZRLayer->setInput(1, *weightsZRStart);\n    weightsZRLayer->setInput(2, *weightsZRSize);\n    nvinfer1::ITensor* weightsZR = N_CHECK(weightsZRLayer->getOutput(0));\n    LOG_VERBOSE(\"Weights for ZR gates shape is: \" << weightsZR->getDimensions());\n\n    nvinfer1::ITensor* weightsHStart = addConstant(ctx, std::vector<int32_t>{0, 2 * hiddenSize, 0},\n        ::ONNX_NAMESPACE::TensorProto::INT32,\n        Dims{1, {3}})->getOutput(0);\n    auto weightsHSizeLayer = N_CHECK(net->addConcatenation(\n        std::array<nvinfer1::ITensor*, 3>{{numDirectionsTensor, hiddenSizeTensor, eDimTensor}}.data(), 3));\n    nvinfer1::ITensor* weightsHSize = N_CHECK(weightsHSizeLayer->getOutput(0));\n    nvinfer1::ISliceLayer* weightsHLayer = N_CHECK(net->addSlice(weights, Dims{3}, Dims{3}, Dims3{1, 1, 1}));\n    weightsHLayer->setInput(1, *weightsHStart);\n    weightsHLayer->setInput(2, *weightsHSize);\n    nvinfer1::ITensor* weightsH = N_CHECK(weightsHLayer->getOutput(0));\n    LOG_VERBOSE(\"Weights for H gate shape is: \" << weightsH->getDimensions());\n\n    auto recurrenceWeightsZRLayer = N_CHECK(net->addSlice(\n        recurrenceWeights, Dims3{0, 0, 0}, Dims3{numDirections, 2 * hiddenSize, hiddenSize}, Dims3{1, 1, 1}));\n    nvinfer1::ITensor* recurrenceWeightsZR = N_CHECK(recurrenceWeightsZRLayer->getOutput(0));\n    LOG_VERBOSE(\"Recurrence weights for ZR gates shape is: \" << recurrenceWeightsZR->getDimensions());\n    auto recurrenceWeightsHLayer = N_CHECK(net->addSlice(\n        recurrenceWeights, Dims3{0, 2 * hiddenSize, 0}, Dims3{numDirections, hiddenSize, hiddenSize}, Dims3{1, 1, 1}));\n    nvinfer1::ITensor* recurrenceWeightsH = N_CHECK(recurrenceWeightsHLayer->getOutput(0));\n    LOG_VERBOSE(\"Recurrence weights for H gate shape is: \" << recurrenceWeightsH->getDimensions());\n\n    // bias/recurrenceBias will have shape (numDirections, NUM_GATES * hiddenSize)\n    nvinfer1::ITensor* biasZR{nullptr};\n    nvinfer1::ITensor* biasH{nullptr};\n    nvinfer1::ITensor* recurrenceBiasZR{nullptr};\n    nvinfer1::ITensor* recurrenceBiasH{nullptr};\n    if (inputs.size() > 3 && inputs.at(3))\n    {\n        // ONNX bias is a concatenation of Wb and Rb on the second axis, so has shape (numDirections, 2 * NUM_GATES *\n        // hiddenSize)\n        // Unsqueeze to (numDirections, 1, 2 * NUM_GATES * hiddenSize) so we can broadcast later\n        nvinfer1::ITensor* concatenatedBias = &convertToTensor(inputs.at(3), ctx);\n        nvinfer1::IShuffleLayer* unsqueeze = N_CHECK(net->addShuffle(*concatenatedBias));\n        unsqueeze->setReshapeDimensions(Dims3{numDirections, 1, 2 * NUM_GATES * hiddenSize});\n        unsqueeze->setZeroIsPlaceholder(false);\n        concatenatedBias = N_CHECK(unsqueeze->getOutput(0));\n\n        auto biasZRLayer = N_CHECK(\n            net->addSlice(*concatenatedBias, Dims3{0, 0, 0}, Dims3{numDirections, 1, 2 * hiddenSize}, Dims3{1, 1, 1}));\n        biasZR = N_CHECK(biasZRLayer->getOutput(0));\n        LOG_VERBOSE(\"Bias for ZR gates shape is: \" << biasZR->getDimensions());\n        auto biasHLayer = N_CHECK(net->addSlice(\n            *concatenatedBias, Dims3{0, 0, 2 * hiddenSize}, Dims3{numDirections, 1, hiddenSize}, Dims3{1, 1, 1}));\n        biasH = N_CHECK(biasHLayer->getOutput(0));\n        LOG_VERBOSE(\"Bias for H gate shape is: \" << biasH->getDimensions());\n\n        auto recurrenceBiasZRLayer = N_CHECK(net->addSlice(*concatenatedBias, Dims3{0, 0, NUM_GATES * hiddenSize},\n            Dims3{numDirections, 1, 2 * hiddenSize}, Dims3{1, 1, 1}));\n        recurrenceBiasZR = N_CHECK(recurrenceBiasZRLayer->getOutput(0));\n        LOG_VERBOSE(\"Recurrence bias for ZR gates shape is: \" << recurrenceBiasZR->getDimensions());\n        auto recurrenceBiasHLayer = N_CHECK(net->addSlice(*concatenatedBias, Dims3{0, 0, (NUM_GATES + 2) * hiddenSize},\n            Dims3{numDirections, 1, hiddenSize}, Dims3{1, 1, 1}));\n        recurrenceBiasH = N_CHECK(recurrenceBiasHLayer->getOutput(0));\n        LOG_VERBOSE(\"Recurrence bias for H gate shape is: \" << recurrenceBiasH->getDimensions());\n    }\n\n    // Get a shape tensor containing: (numDirections, batchSize, hiddenSize)\n    auto const initialStateShape = [&ctx, &numDirections, &hiddenSize, &input, &net]() -> nvinfer1::ITensor* {\n        // Get batchSize from input shape\n        nvinfer1::ITensor* numDirectionsTensor\n            = addConstantScalar(ctx, numDirections, ::ONNX_NAMESPACE::TensorProto_DataType_INT32, Dims{1, {1}})\n                  ->getOutput(0);\n        LOG_VERBOSE(\"numDirections is: \" << numDirections\n                                         << \", numDirections Tensor shape: \" << numDirectionsTensor->getDimensions());\n        nvinfer1::ITensor* hiddenSizeTensor\n            = addConstantScalar(ctx, hiddenSize, ::ONNX_NAMESPACE::TensorProto_DataType_INT32, Dims{1, {1}})\n                  ->getOutput(0);\n        LOG_VERBOSE(\n            \"hiddenSize is: \" << hiddenSize << \", hiddenSizeTensor shape: \" << hiddenSizeTensor->getDimensions());\n        nvinfer1::ITensor* batchSizeTensor = getAxisLength(ctx, input, 1, Dims{1, {1}});\n        LOG_VERBOSE(\"batchSizeTensor shape: \" << batchSizeTensor->getDimensions());\n\n        nvinfer1::IConcatenationLayer* concatenatedShape = N_CHECK(net->addConcatenation(\n            std::array<nvinfer1::ITensor*, 3>{{numDirectionsTensor, batchSizeTensor, hiddenSizeTensor}}.data(), 3));\n        return N_CHECK(concatenatedShape->getOutput(0));\n    };\n    nvinfer1::ITensor* gateOutputShape = initialStateShape();\n    LOG_VERBOSE(\"Gate output rank (equal to initial hidden/cell state rank): \" << gateOutputShape->getDimensions());\n\n    LOG_VERBOSE(\"Entering Loop\");\n    // Scan over the S dimension of the input\n    auto loop = N_CHECK(net->addLoop());\n    nvinfer1::ITensor* tripLimit = getAxisLength(ctx, input, 0);\n    loop->addTripLimit(*tripLimit, nvinfer1::TripLimit::kCOUNT);\n\n    // Add X(t)\n    nvinfer1::ITensor* iterationInput = addRNNInput(ctx, node, loop, inputs, direction);\n\n    // H(t-1)\n    auto const getInitialInputValue = [&ctx, &gateOutputShape, &inputs, &node](size_t inputIdx) -> nvinfer1::ITensor* {\n        if (inputs.size() > inputIdx && inputs.at(inputIdx))\n        {\n            return &convertToTensor(inputs.at(inputIdx), ctx);\n        }\n        return constantOfShape(ctx,\n            addConstantScalar(ctx, 0.f, ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT, Dims{1, {1}})->getOutput(0),\n            gateOutputShape);\n    };\n\n    nvinfer1::ITensor* initialHidden = getInitialInputValue(5);\n    LOG_VERBOSE(\"Initial hidden state shape: \" << initialHidden->getDimensions());\n\n    nvinfer1::IRecurrenceLayer* Ht1 = N_CHECK(loop->addRecurrence(*initialHidden));\n    ctx->registerLayer(Ht1, node);\n    auto Ht1Output = N_CHECK(Ht1->getOutput(0));\n    LOG_VERBOSE(\"Hidden state shape: \" << Ht1Output->getDimensions());\n\n    // Compute stackedZR(t) = f(X(t) * W[zr]^T + H(t-1) * R[zr]^T + (Wb[zr] + Rb[zr])). stackedZR(t) has shape\n    // (numDirections, batchSize, 2 * hiddenSize)\n    auto xtWTZRLayer = N_CHECK(net->addMatrixMultiply(*iterationInput, mOp::kNONE, *weightsZR, mOp::kTRANSPOSE));\n    nvinfer1::ITensor* xtWTZR = N_CHECK(xtWTZRLayer->getOutput(0));\n    LOG_VERBOSE(\"X(t) * W[zr]^T -> \" << xtWTZR->getDimensions());\n\n    auto ht1RTLayer = N_CHECK(net->addMatrixMultiply(*Ht1Output, mOp::kNONE, *recurrenceWeightsZR, mOp::kTRANSPOSE));\n    nvinfer1::ITensor* ht1RT = N_CHECK(ht1RTLayer->getOutput(0));\n    LOG_VERBOSE(\"H(t-1) * R[zr]^T -> \" << ht1RT->getDimensions());\n\n    auto stackedZRtSumLayer = N_CHECK(net->addElementWise(*xtWTZR, *ht1RT, eOp::kSUM));\n    nvinfer1::ITensor* stackedZRt = N_CHECK(stackedZRtSumLayer->getOutput(0));\n    if (biasZR && recurrenceBiasZR)\n    {\n        auto biasSumLayer = N_CHECK(net->addElementWise(*stackedZRt, *biasZR, eOp::kSUM));\n        stackedZRt = N_CHECK(biasSumLayer->getOutput(0));\n        auto recSumLayer = N_CHECK(net->addElementWise(*stackedZRt, *recurrenceBiasZR, eOp::kSUM));\n        stackedZRt = N_CHECK(recSumLayer->getOutput(0));\n    }\n\n    nvinfer1::IActivationLayer* stackedZRtLayer\n        = N_CHECK(net->addActivation(*addClip(ctx, stackedZRt, clip), activations.at(0)));\n    stackedZRtLayer->setAlpha(activationAlphas.at(0));\n    stackedZRtLayer->setBeta(activationBetas.at(0));\n    stackedZRt = N_CHECK(stackedZRtLayer->getOutput(0));\n    LOG_VERBOSE(\"stackedZR(t) -> \" << stackedZRt->getDimensions());\n\n    auto const isolateGate = [&ctx, &hiddenSize, &gateOutputShape, &net](\n                                 nvinfer1::ITensor* gates, int32_t gateIndex) -> nvinfer1::ITensor* {\n        nvinfer1::ISliceLayer* isolateGate\n            = N_CHECK(net->addSlice(*gates, Dims3{0, 0, 0}, Dims3{0, 0, 0}, Dims3{1, 1, 1}));\n        isolateGate->setInput(1,\n            *addConstant(ctx, std::vector<int32_t>{0, 0, gateIndex * hiddenSize},\n                ::ONNX_NAMESPACE::TensorProto_DataType_INT32, Dims{1, {3}})\n                ->getOutput(0));                    // Start\n        isolateGate->setInput(2, *gateOutputShape); // Size\n        return N_CHECK(isolateGate->getOutput(0));\n    };\n\n    // zt = stackedZRt[:, :. 0:H]\n    nvinfer1::ITensor* zt = isolateGate(stackedZRt, 0);\n    LOG_VERBOSE(\"z(t) -> \" << zt->getDimensions());\n\n    // rt = stackedZRt[:, :. H:2H]\n    nvinfer1::ITensor* rt = isolateGate(stackedZRt, 1);\n    LOG_VERBOSE(\"r(t) -> \" << rt->getDimensions());\n\n    // Compute h(t)\n    nvinfer1::ITensor* ht{nullptr};\n    // xtWTH = X(t) * (W[h]^T)\n    auto xtWTHLayer = N_CHECK(net->addMatrixMultiply(*iterationInput, mOp::kNONE, *weightsH, mOp::kTRANSPOSE));\n    nvinfer1::ITensor* xtWTH = N_CHECK(xtWTHLayer->getOutput(0));\n    if (linearBeforeReset == 0)\n    {\n        // h(t) = g(xtWTH + (r(t) . H(t-1)) * (R[h]^T) + Rb[h] + Wb[h])\n        // rtHt1 = (r(t) . H(t-1))\n        nvinfer1::ITensor* rtHt1 = getElementWiseResult(ctx, *rt, *Ht1Output, eOp::kPROD);\n        // rtHt1Rh = (r(t) . H(t-1)) * (R[h]^T)\n        auto rtHt1RhLayer = N_CHECK(net->addMatrixMultiply(*rtHt1, mOp::kNONE, *recurrenceWeightsH, mOp::kTRANSPOSE));\n        nvinfer1::ITensor* rtHt1Rh = N_CHECK(rtHt1RhLayer->getOutput(0));\n\n        // (xtWTH + rtHt1Rh) + (Rb[h] + Wb[h])\n        nvinfer1::ITensor* actInput = getElementWiseResult(ctx, *xtWTH, *rtHt1Rh, eOp::kSUM);\n\n        // If bias is defines, both recurrence and normal bias must be present\n        if (recurrenceBiasH && biasH)\n        {\n            nvinfer1::ITensor* secondSum = getElementWiseResult(ctx, *recurrenceBiasH, *biasH, eOp::kSUM);\n            actInput = getElementWiseResult(ctx, *actInput, *secondSum, eOp::kSUM);\n        }\n\n        nvinfer1::IActivationLayer* htLayer\n            = N_CHECK(net->addActivation(*addClip(ctx, actInput, clip), activations.at(1)));\n        htLayer->setAlpha(activationAlphas.at(1));\n        htLayer->setBeta(activationBetas.at(1));\n        ht = N_CHECK(htLayer->getOutput(0));\n    }\n    else\n    {\n        // h(t) = g(xtWTH + (r(t) . (H(t-1) * (R[h]^T) + Rb[h])) + Wb[h])\n        // ht1Rh = H(t-1) * (R[h]^T)\n        auto ht1RhLayer = N_CHECK(net->addMatrixMultiply(*Ht1Output, mOp::kNONE, *recurrenceWeightsH, mOp::kTRANSPOSE));\n        nvinfer1::ITensor* ht1Rh = N_CHECK(ht1RhLayer->getOutput(0));\n\n        // rtHtRhRbh = r(t) . (ht1Rh + Rb[h])\n        if (recurrenceBiasH)\n        {\n            ht1Rh = getElementWiseResult(ctx, *ht1Rh, *recurrenceBiasH, eOp::kSUM);\n        }\n        nvinfer1::ITensor* rtHtRhRbh = getElementWiseResult(ctx, *rt, *ht1Rh, eOp::kPROD);\n\n        // h(t) = g(xtWTH + rtHtRhRbh + Wb[h])\n        if (biasH)\n        {\n            rtHtRhRbh = getElementWiseResult(ctx, *rtHtRhRbh, *biasH, eOp::kSUM);\n        }\n        nvinfer1::IActivationLayer* htLayer = N_CHECK(net->addActivation(\n            *addClip(ctx, getElementWiseResult(ctx, *xtWTH, *rtHtRhRbh, eOp::kSUM), clip), activations.at(1)));\n        ONNXTRT_CHECK_NODE(htLayer, \"Failed to create layer\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        htLayer->setAlpha(activationAlphas.at(1));\n        htLayer->setBeta(activationBetas.at(1));\n        ht = htLayer->getOutput(0);\n    }\n    LOG_VERBOSE(\"h(t) -> \" << ht->getDimensions());\n\n    // H(t) = (1 - z(t)) . h(t) + (z(t) . H(t-1))\n    // Constant `1` needs to be the same type as the inputs, either FP16 or FP32.\n    auto* constOne = zt->getType() == nvinfer1::DataType::kHALF\n        ? N_CHECK(addConstantScalar(\n              ctx, static_cast<half_float::half>(1), ::ONNX_NAMESPACE::TensorProto::FLOAT16, Dims3{1, 1, 1})\n                      ->getOutput(0))\n        : N_CHECK(addConstantScalar(ctx, 1.f, ::ONNX_NAMESPACE::TensorProto::FLOAT, Dims3{1, 1, 1})->getOutput(0));\n    nvinfer1::ITensor* Ht = getElementWiseResult(ctx,\n        *getElementWiseResult(ctx, *getElementWiseResult(ctx, *constOne, *zt, eOp::kSUB), *ht, eOp::kPROD),\n        *getElementWiseResult(ctx, *zt, *Ht1Output, eOp::kPROD), eOp::kSUM);\n\n    // singlePassShape = (1, batchSize, hiddenSize)\n    nvinfer1::ITensor* singlePassShape = getElementWiseResult(ctx, *gateOutputShape,\n        *addConstant(ctx, std::vector<int32_t>{numDirections, 1, 1}, ::ONNX_NAMESPACE::TensorProto_DataType_INT32,\n            nvinfer1::Dims{1, {3}})\n             ->getOutput(0),\n        nvinfer1::ElementWiseOperation::kDIV);\n    if (inputs.size() > 4 && inputs.at(4))\n    {\n        nvinfer1::ITensor* seqLens = &convertToTensor(inputs.at(4), ctx);\n        auto maxLen = getAxisLength(ctx, input, 0);\n        Ht = numDirections == 2\n            ? maskBidirRNNHidden(ctx, node, loop, seqLens, maxLen, Ht1Output, Ht, singlePassShape)\n            : maskRNNHidden(ctx, node, loop, seqLens, Ht1Output, Ht, maxLen, direction == \"reverse\");\n    }\n    Ht1->setInput(1, *Ht);\n    LOG_VERBOSE(\"H(t) -> \" << Ht->getDimensions());\n\n    std::vector<TensorOrWeights> outputs{};\n\n    // Add outputs. In ONNX, all GRU outputs are optional, so check for them here.\n    auto shouldAddOutput = [&node](int32_t index) { return index < node.output().size() && !node.output(index).empty(); };\n\n    // Y = concatenation of all H(t) for each element of the sequence\n    if (shouldAddOutput(0))\n    {\n        outputs.emplace_back(concatenateRNNOutputs(ctx, node, loop, singlePassShape, getAxisLength(ctx, input, 0), Ht,\n            numDirections, inputs, direction == \"reverse\"));\n    }\n    // Yh = last value of H(t)\n    if (shouldAddOutput(1))\n    {\n        outputs.emplace_back(N_CHECK(loop->addLoopOutput(*Ht1Output, nvinfer1::LoopOutput::kLAST_VALUE)->getOutput(0)));\n    }\n    return {{outputs}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(HammingWindow)\n{\n    /***\n\n    Operation returns a window vector, where:\n\n    Y[n] = alpha - beta * cos(2*pi*n / N)\n\n    Where N is the window length, and n is each element in the window.\n\n    Note that if `periodic == 0`, the denominator becomes N - 1.\n\n    This can be represented by creating a range 'n' from 0 -> N, and performing the operations elementwise.\n\n    Note that in the ONNX op definition alpha and beta are not provided. We will use the default values defined in ONNX:\n\n        alpha = 25/46\n        beta = 1 - alpha\n\n    ***/\n\n    OnnxAttrs attrs(node, ctx);\n    int32_t outputDtype = attrs.get<int32_t>(\"output_datatype\", 1);\n    int32_t periodic = attrs.get<int32_t>(\"periodic\", 1);\n    ONNXTRT_CHECK_NODE(outputDtype == 1, \"Output must be float32-type!\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    constexpr float alpha = 25.F / 46.F;\n    constexpr float beta = 1.F - alpha;\n\n    auto* N = &convertToTensor(inputs.at(0), ctx);\n    ONNXTRT_CHECK_NODE(\n        N->getDimensions().nbDims == 0, \"Window length must be a scalar!\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    auto* window = generateWindow(ctx, N);\n\n    auto* cosOutput = windowHelper(ctx, 2.F * M_PI, window, N, nvinfer1::UnaryOperation::kCOS, periodic);\n\n    auto betaTensor = N_CHECK(addConstantScalar(ctx, beta, ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT,\n        nvinfer1::Dims{1, {1}})->getOutput(0));\n    auto betaLayer\n        = N_CHECK(ctx->network()->addElementWise(*betaTensor, *cosOutput, nvinfer1::ElementWiseOperation::kPROD));\n    auto betaOutput = N_CHECK(betaLayer->getOutput(0));\n\n    auto alphaTensor = N_CHECK(addConstantScalar(ctx, alpha, ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT,\n        nvinfer1::Dims{1, {1}})->getOutput(0));\n    auto alphaLayer\n        = N_CHECK(ctx->network()->addElementWise(*alphaTensor, *betaOutput, nvinfer1::ElementWiseOperation::kSUB));\n\n    RETURN_FIRST_OUTPUT(alphaLayer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(HannWindow)\n{\n    /***\n\n    Operation returns a window vector, where:\n\n    Y[n] = sin^2(pi*n / N)\n\n    Where N is the window length, and n is each element in the window.\n\n    Note that if `periodic == 0`, the denominator becomes N - 1.\n\n    This can be represented by creating a range 'n' from 0 -> N, and performing the operations elementwise.\n\n    ***/\n\n    OnnxAttrs attrs(node, ctx);\n    int32_t outputDtype = attrs.get<int32_t>(\"output_datatype\", 1);\n    int32_t periodic = attrs.get<int32_t>(\"periodic\", 1);\n    ONNXTRT_CHECK_NODE(outputDtype == 1, \"Output must be float32-type!\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    auto* N = &convertToTensor(inputs.at(0), ctx);\n    ONNXTRT_CHECK_NODE(\n        N->getDimensions().nbDims == 0, \"Window length must be a scalar!\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    auto* window = generateWindow(ctx, N);\n\n    auto sinOutput = windowHelper(ctx, M_PI, window, N, nvinfer1::UnaryOperation::kSIN, periodic);\n\n    auto sinSquaredLayer\n        = N_CHECK(ctx->network()->addElementWise(*sinOutput, *sinOutput, nvinfer1::ElementWiseOperation::kPROD));\n\n    RETURN_FIRST_OUTPUT(sinSquaredLayer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Hardmax)\n{\n    checkNotInvalidType(inputs.at(0), {\"INT64\", \"INT32\", \"INT8\", \"UINT8\", \"BOOL\"}, node, nodeIdx);\n    OnnxAttrs attrs(node, ctx);\n    nvinfer1::ITensor* values = &convertToTensor(inputs.at(0), ctx);\n    auto originalDims = values->getDimensions();\n\n    int32_t axis = attrs.get(\"axis\", ctx->getOpsetVersion() < 13 ? 1 : -1);\n    convertAxis(axis, originalDims.nbDims, node, nodeIdx);\n    if (ctx->getOpsetVersion() < 13)\n    {\n        // Reshape into 2D tensor\n        values = flattenTensor(ctx, node, *values, axis, true);\n        axis = 1;\n    }\n    uint32_t axisMask = 1 << axis;\n\n    auto* topKLayer = N_CHECK(ctx->network()->addTopK(*values, nvinfer1::TopKOperation::kMAX, /* k */ 1, axisMask));\n\n    auto* squeezedIndices = squeezeTensor(ctx, *topKLayer->getOutput(1), {axis});\n    auto* zeroOneTensor = N_CHECK(addConstant(ctx, std::vector<int32_t>{0, 1},\n        ::ONNX_NAMESPACE::TensorProto_DataType_INT32,\n        nvinfer1::Dims{1, {2}})->getOutput(0));\n    auto* depth = getAxisLength(ctx, values, axis, nvinfer1::Dims{0});\n    auto* oneHotLayer = N_CHECK(ctx->network()->addOneHot(*squeezedIndices, *zeroOneTensor, *depth, axis));\n    auto* oneHotOutput = N_CHECK(oneHotLayer->getOutput(0));\n\n    if (ctx->getOpsetVersion() < 13)\n    {\n        oneHotOutput = reshapeTensor(ctx, *oneHotOutput, originalDims);\n    }\n    auto* output = castHelper(ctx, oneHotOutput, values->getType());\n    return {{output}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(HardSigmoid)\n{\n    OnnxAttrs attrs(node, ctx);\n    float alpha = attrs.get<float>(\"alpha\", 0.2f);\n    float beta = attrs.get<float>(\"beta\", 0.5f);\n    return activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kHARD_SIGMOID, &alpha, &beta);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Identity)\n{\n    auto* layer = N_CHECK(ctx->network()->addIdentity(convertToTensor(inputs.at(0), ctx)));\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(If)\n{\n    OnnxAttrs attrs(node, ctx);\n    auto cond = inputs.at(0);\n\n    // Push current scope name to top of stack in order to properly set layer metadata and print error message.\n    ctx->localFunctionStack().push_back({node.op_type(), getNodeName(node), {}});\n\n    ::ONNX_NAMESPACE::GraphProto const& thenGraph = attrs.get<::ONNX_NAMESPACE::GraphProto const&>(\"then_branch\");\n    ::ONNX_NAMESPACE::GraphProto const& elseGraph = attrs.get<::ONNX_NAMESPACE::GraphProto const&>(\"else_branch\");\n\n    // Number of outputs are the same between the two branches.\n    ONNXTRT_CHECK_NODE(thenGraph.output_size() == elseGraph.output_size(),\n        \"then/else subgraphs should have the same number of outputs: then outputs = \"\n            << thenGraph.output_size() << \", else outputs = \" << elseGraph.output_size() << \".\",\n        node, nodeIdx, ErrorCode::kINVALID_NODE);\n    int32_t const nbOutputs = thenGraph.output_size();\n    std::vector<TensorOrWeights> graphOutputs;\n\n    // For constant conditions, parse only the selected subgraph\n    if (cond.is_weights() && cond.weights().count() == 1)\n    {\n        // Boolean weights are stored as uint8_t\n        auto const value = *(static_cast<uint8_t*>(cond.weights().values));\n        ::ONNX_NAMESPACE::GraphProto const& body = value == 1 ? thenGraph : elseGraph;\n\n        // Establish scope for names local to the subgraph.\n        NameScope nameScope(*ctx);\n\n        std::vector<Status> errors{};\n        onnx2trt::parseGraph(ctx, body, errors);\n        for (int32_t i = 0; i < nbOutputs; i++)\n        {\n            graphOutputs.emplace_back(ctx->tensors().at(body.output(i).name()));\n        }\n        return {graphOutputs};\n    }\n\n    //\n    // The condition is not a build-time constant. Construct an if-conditional construct.\n    //\n\n    // The `condition` tensor must be a scalar boolean.\n    auto* condTensor = convertToScalar(ctx, &convertToTensor(cond, ctx));\n    ONNXTRT_CHECK_NODE(\n        condTensor, \"Failed to convert the input cond to a scalar.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    auto conditional = N_CHECK(ctx->network()->addIfConditional());\n    conditional->setName(getNodeName(node).c_str());\n    conditional->setCondition(*condTensor);\n\n    std::vector<nvinfer1::ILayer*> thenLayers, elseLayers;\n    std::vector<TensorOrWeights> thenSubgraphTensors;\n    std::vector<TensorOrWeights> elseSubgraphTensors;\n\n    ctx->localFunctionStack().push_back({\"then_branch\", thenGraph.name(), {}});\n    importSubgraph(ctx, thenGraph, thenLayers, thenSubgraphTensors);\n    ctx->localFunctionStack().pop_back();\n\n    ctx->localFunctionStack().push_back({\"else_branch\", elseGraph.name(), {}});\n    importSubgraph(ctx, elseGraph, elseLayers, elseSubgraphTensors);\n    ctx->localFunctionStack().pop_back();\n\n    using InputsMap = std::unordered_map<std::string, nvinfer1::IIfConditionalInputLayer*>;\n    InputsMap inputsMap;\n    ctx->localFunctionStack().push_back({\"then_branch\", thenGraph.name(), {}});\n    addIfInputLayers(ctx, conditional, inputsMap, thenLayers, &node);\n    ctx->localFunctionStack().pop_back();\n    ctx->localFunctionStack().push_back({\"else_branch\", elseGraph.name(), {}});\n    addIfInputLayers(ctx, conditional, inputsMap, elseLayers, &node);\n    ctx->localFunctionStack().pop_back();\n\n    ONNXTRT_CHECK_NODE(thenSubgraphTensors.size() == elseSubgraphTensors.size(),\n        \"Found different number of output tensors in If conditional subgraphs! then outputs = \"\n            << thenSubgraphTensors.size() << \", else outputs = \" << elseSubgraphTensors.size() << \".\",\n        node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    ctx->localFunctionStack().pop_back();\n\n    for (size_t i = 0; i < thenSubgraphTensors.size(); i++)\n    {\n        auto* thenOut = &convertToTensor(thenSubgraphTensors[i], ctx);\n        auto* elseOut = &convertToTensor(elseSubgraphTensors[i], ctx);\n        auto* outputLayer = N_CHECK(conditional->addOutput(*thenOut, *elseOut));\n        ctx->registerLayer(outputLayer, std::string(conditional->getName()) + \"_OutputLayer\", &node);\n        graphOutputs.emplace_back(N_CHECK(outputLayer->getOutput(0)));\n    }\n\n    return {graphOutputs};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(ImageScaler)\n{\n    nvinfer1::ITensor& tensor = convertToTensor(inputs.at(0), ctx);\n    OnnxAttrs attrs{node, ctx};\n    // Shift the input by a per-channel bias value.\n    std::vector<float> biases = attrs.get<std::vector<float>>(\"bias\");\n    nvinfer1::Dims dims{1, {static_cast<int32_t>(biases.size())}};\n    ShapedWeights shiftWeights = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto_DataType_FLOAT, dims);\n    std::copy(biases.begin(), biases.end(), static_cast<float*>(shiftWeights.values));\n    // Scale is applied to every element of the input, but we need to duplicate it over every channel.\n    float scale = attrs.get<float>(\"scale\", 1.0f);\n    ShapedWeights scaleWeights = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto_DataType_FLOAT, dims);\n    std::fill(static_cast<float*>(scaleWeights.values), static_cast<float*>(scaleWeights.values) + scaleWeights.count(),\n        scale);\n    // Finally add the scale layer.\n    auto layer = N_CHECK(ctx->network()->addScale(\n        tensor, nvinfer1::ScaleMode::kCHANNEL, shiftWeights, scaleWeights, nvinfer1::Weights{}));\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(InstanceNormalization)\n{\n    auto inputDataType = inputs.at(0).getDataType();\n    auto scaleDataType = inputs.at(1).getDataType();\n    auto biasDataType = inputs.at(2).getDataType();\n\n    ONNXTRT_CHECK_NODE((inputDataType == scaleDataType && scaleDataType == biasDataType),\n        \"Inputs must be either all FLOAT or all FLOAT16. Input type = \" + getTrtDtypeName(inputDataType)\n            + \", scale type = \" + getTrtDtypeName(scaleDataType) + \", bias type = \" + getTrtDtypeName(biasDataType)\n            + \".\",\n        node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    // Choose plugin implementation if requested by the user. kNATIVE_INSTANCENORM is ON by default.\n    auto flags = ctx->getFlags();\n    uint32_t nativeInstanceNormFlag = 1U << static_cast<uint32_t>(nvonnxparser::OnnxParserFlag::kNATIVE_INSTANCENORM);\n    if (flags & nativeInstanceNormFlag)\n    {\n        return normalizationHelper(ctx, node, nodeIdx, inputs);\n    }\n    ONNXTRT_CHECK_NODE((inputDataType == DataType::kFLOAT || inputDataType == DataType::kHALF),\n        \"Inputs to InstanceNorm plugin must be either FLOAT or FLOAT16. Input type is \" + getTrtDtypeName(inputDataType)\n            + \".\",\n        node, nodeIdx, ErrorCode::kINVALID_NODE);\n    return instanceNormPluginHelper(ctx, node, nodeIdx, inputs);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(IsInf)\n{\n    OnnxAttrs attrs{node, ctx};\n    int32_t const detectNegative = attrs.get<int32_t>(\"detect_negative\", 1);\n    int32_t const detectPositive = attrs.get<int32_t>(\"detect_positive\", 1);\n\n    if (detectNegative && detectPositive)\n    {\n        return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kISINF);\n    }\n\n    auto& input = convertToTensor(inputs.at(0), ctx);\n    auto inputDims = input.getDimensions();\n    nvinfer1::Dims scalarDims{inputDims.nbDims};\n    std::fill(scalarDims.d, scalarDims.d + scalarDims.nbDims, 1);\n    auto& zeroTensor = *addConstantScalar(ctx, 0.F, ::ONNX_NAMESPACE::TensorProto::FLOAT, scalarDims)->getOutput(0);\n\n    if (detectNegative)\n    {\n        auto* isNegLayer\n            = N_CHECK(ctx->network()->addElementWise(input, zeroTensor, nvinfer1::ElementWiseOperation::kLESS));\n        auto* isNeg = N_CHECK(isNegLayer->getOutput(0));\n        auto* isInfLayer = N_CHECK(ctx->network()->addUnary(input, nvinfer1::UnaryOperation::kISINF));\n        auto* isInf = N_CHECK(isInfLayer->getOutput(0));\n        auto* finalLayer = ctx->network()->addElementWise(*isNeg, *isInf, nvinfer1::ElementWiseOperation::kAND);\n        RETURN_FIRST_OUTPUT(finalLayer, node, nodeIdx);\n    }\n    if (detectPositive)\n    {\n        auto* isPosLayer\n            = N_CHECK(ctx->network()->addElementWise(input, zeroTensor, nvinfer1::ElementWiseOperation::kGREATER));\n        auto* isPos = N_CHECK(isPosLayer->getOutput(0));\n        auto* isInfLayer = N_CHECK(ctx->network()->addUnary(input, nvinfer1::UnaryOperation::kISINF));\n        auto* isInf = N_CHECK(isInfLayer->getOutput(0));\n        auto* finalLayer = ctx->network()->addElementWise(*isPos, *isInf, nvinfer1::ElementWiseOperation::kAND);\n        RETURN_FIRST_OUTPUT(finalLayer, node, nodeIdx);\n    }\n    // In this case, always return false.\n    auto* isPosLayer\n        = N_CHECK(ctx->network()->addElementWise(input, zeroTensor, nvinfer1::ElementWiseOperation::kGREATER));\n    auto* isPos = N_CHECK(isPosLayer->getOutput(0));\n    auto* isNegLayer\n        = N_CHECK(ctx->network()->addElementWise(input, zeroTensor, nvinfer1::ElementWiseOperation::kLESS));\n    auto* isNeg = N_CHECK(isNegLayer->getOutput(0));\n    auto* finalLayer = ctx->network()->addElementWise(*isPos, *isNeg, nvinfer1::ElementWiseOperation::kAND);\n    RETURN_FIRST_OUTPUT(finalLayer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(IsNaN)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kISNAN);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(LayerNormalization)\n{\n    auto* input = &convertToTensor(inputs.at(0), ctx);\n\n    auto dt = input->getType();\n    nvinfer1::IConstantLayer* scaleLayer;\n    nvinfer1::IConstantLayer* biasLayer;\n    if (dt == DataType::kHALF)\n    {\n        scaleLayer = addConstantScalar(ctx, static_cast<half_float::half>(1), ::ONNX_NAMESPACE::TensorProto::FLOAT16);\n        biasLayer = addConstantScalar(ctx, static_cast<half_float::half>(0), ::ONNX_NAMESPACE::TensorProto::FLOAT16);\n    }\n    else if (dt == DataType::kBF16)\n    {\n        scaleLayer = addConstantScalar(ctx, static_cast<BFloat16>(0), ::ONNX_NAMESPACE::TensorProto::BFLOAT16);\n        biasLayer = addConstantScalar(ctx, static_cast<BFloat16>(0), ::ONNX_NAMESPACE::TensorProto::BFLOAT16);\n    }\n    else\n    {\n        scaleLayer = addConstantScalar(ctx, static_cast<float>(0), ::ONNX_NAMESPACE::TensorProto::FLOAT);\n        biasLayer = addConstantScalar(ctx, static_cast<float>(0), ::ONNX_NAMESPACE::TensorProto::FLOAT);\n    }\n    auto* scale = inputs.at(1).isNullTensor() ? N_CHECK(scaleLayer->getOutput(0)) : &convertToTensor(inputs.at(1), ctx);\n    auto* bias = (inputs.size() == 3 && !inputs.at(2).isNullTensor()) ? &convertToTensor(inputs.at(2), ctx)\n                                                                      : N_CHECK(biasLayer->getOutput(0));\n\n    OnnxAttrs attrs(node, ctx);\n    float epsilon = attrs.get(\"epsilon\", 1e-5f);\n    int32_t axis = attrs.get(\"axis\", -1);\n    nvinfer1::DataType computeType = nvinfer1::DataType::kFLOAT;\n    convertDtype(attrs.get<int32_t>(\"stash_type\", 1), &computeType);\n\n    int32_t const nbDims = input->getDimensions().nbDims;\n    convertAxis(axis, nbDims, node, nodeIdx);\n    uint32_t axesMask{0};\n\n    // Populate axesMask with axis values\n    for (int32_t i = axis; i < nbDims; i++)\n    {\n        axesMask |= 1 << i;\n    }\n\n    // Broadcast scale and bias to input size\n    broadcastTensors(ctx, input, scale);\n    broadcastTensors(ctx, input, bias);\n\n    auto* layer = N_CHECK(ctx->network()->addNormalization(*input, *scale, *bias, axesMask));\n    layer->setEpsilon(epsilon);\n    auto const stronglyTyped = ctx->isStronglyTyped();\n    if (!stronglyTyped)\n    {\n        layer->setComputePrecision(computeType);\n    }\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(LeakyRelu)\n{\n    OnnxAttrs attrs(node, ctx);\n    float alpha = attrs.get<float>(\"alpha\", 0.01f);\n    return activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kLEAKY_RELU, &alpha);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Less)\n{\n    return elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kLESS);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(LessOrEqual)\n{\n    return greaterLessOrEqual(ctx, node, nodeIdx, &convertToTensor(inputs.at(0), ctx),\n        &convertToTensor(inputs.at(1), ctx),\n        /*greater*/ false);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Log)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kLOG);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(LogSoftmax)\n{\n    auto& input = convertToTensor(inputs.at(0), ctx);\n    // Don't use softmax converter since it adds a shuffle layer\n    // which prevents the builder to fuse softmax and log operations.\n    auto* softmax = addSoftmax(ctx, node, nodeIdx, input);\n    nvinfer1::IUnaryLayer* unaryLayer = N_CHECK(ctx->network()->addUnary(*softmax, nvinfer1::UnaryOperation::kLOG));\n    auto unaryOutput = N_CHECK(unaryLayer->getOutput(0));\n    // Reshape back to original shape\n    auto* reshapeLayer = addShuffle(ctx, *unaryOutput, shapeOf(input));\n    RETURN_FIRST_OUTPUT(reshapeLayer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Loop)\n{\n    constexpr int32_t NB_NON_STATE_INPUTS = 2; // First 2 inputs are trip count and condition respectively.\n    constexpr int32_t NB_DISCARDED_OUTPUTS\n        = 1; // First output is the updated value of the condition, and is ignored by the outer loop node.\n    constexpr int32_t DUMMY_SCAN_OUTPUT_LENGTH = 1024;\n    ONNXTRT_CHECK_NODE((inputs.size() >= 2),\n        \"The Loop operator requires at least 2 inputs. The current number of inputs = \" << inputs.size() << \".\", node,\n        nodeIdx, ErrorCode::kINVALID_NODE);\n    OnnxAttrs attrs(node, ctx);\n    int32_t const nbInputs = node.input().size();\n    // The number of state variables on the input and output is the same.\n    int32_t const nbStateVars = nbInputs - NB_NON_STATE_INPUTS;\n\n    ::ONNX_NAMESPACE::GraphProto const& body = attrs.get<::ONNX_NAMESPACE::GraphProto const&>(\"body\");\n\n    auto loop = N_CHECK(ctx->network()->addLoop());\n    loop->setName(getNodeName(node).c_str());\n\n    // Establish scope for names local to the subgraph.\n    NameScope nameScope(*ctx);\n\n    // Trip count and condition are optional inputs.\n    nvinfer1::ITensor* tripLimit{nullptr};\n    if (inputs[0])\n    {\n        // Some convertors will use INT_MAX to signify \"use cond input as loop termination\". From TRT's perspective,\n        // we can just treat these cases as an empty tripLimit.\n        bool const isMaxTripCount = inputs[0].is_weights() && inputs[0].isInt64()\n            && static_cast<int64_t*>(inputs[0].weights().values)[0]\n                >= static_cast<int64_t>(std::numeric_limits<int32_t>::max());\n        if (!isMaxTripCount)\n        {\n            tripLimit = convertToScalar(ctx, &convertToTensor(inputs[0], ctx));\n            tripLimit = castHelper(ctx, tripLimit, DataType::kINT32);\n            ONNXTRT_CHECK_NODE(tripLimit, \"Failed to convert the trip-count input to a scalar.\", node, nodeIdx,\n                ErrorCode::kINVALID_NODE);\n            ctx->loopTensors()[body.input(0).name()] = node.input(0);\n            loop->addTripLimit(*tripLimit, nvinfer1::TripLimit::kCOUNT);\n            // First graph input is iteration_num, so create a loop counter\n            auto counter = convertToScalar(ctx, addLoopCounter(ctx, loop, 0));\n            ctx->registerTensor(counter, body.input(0).name());\n        }\n    }\n    nvinfer1::ITensor* cond{nullptr};\n    if (inputs[1])\n    {\n        cond = convertToScalar(ctx, &convertToTensor(inputs[1], ctx));\n        ONNXTRT_CHECK_NODE(\n            cond, \"Failed to convert the input cond to a scalar.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n        ctx->loopTensors()[body.input(1).name()] = node.input(1);\n        ctx->registerTensor(cond, body.input(1).name());\n    }\n    // Add initial state inputs using recurrent layers.\n    std::vector<nvinfer1::IRecurrenceLayer*> stateVars{};\n    for (size_t i = 2; i < inputs.size(); ++i)\n    {\n        stateVars.emplace_back(N_CHECK(loop->addRecurrence(convertToTensor(inputs[i], ctx))));\n        ctx->loopTensors()[body.input(i).name()] = node.input(i);\n        ctx->registerTensor(TensorOrWeights{N_CHECK(stateVars.back()->getOutput(0))}, body.input(i).name());\n        LOG_VERBOSE(\"Mapped Loop node input \" << node.input(i) << \" to loop body input \" << body.input(i).name());\n    }\n\n    // Loop body\n    std::vector<Status> errors{};\n    onnx2trt::parseGraph(ctx, body, errors);\n\n    if (cond)\n    {\n        // Add recurrence for loop condition\n        auto recurrence = N_CHECK(loop->addRecurrence(*cond));\n        auto const& bodyOutputName = body.output(0).name();\n        auto condOutput = convertToScalar(ctx, &convertToTensor(ctx->tensors().at(bodyOutputName), ctx));\n        recurrence->setInput(1, *condOutput);\n        auto recurrenceOutput = N_CHECK(recurrence->getOutput(0));\n        loop->addTripLimit(*recurrenceOutput, nvinfer1::TripLimit::kWHILE);\n    }\n\n    // Set final values of state variables.\n    std::vector<TensorOrWeights> nodeOutputs{};\n    for (int32_t i = 0; i < nbStateVars; ++i)\n    {\n        // The first output of the body graph is the updated condition, which is ignored by the Loop node.\n        int32_t const index = i + NB_DISCARDED_OUTPUTS;\n        auto const& bodyOutputName = body.output(index).name();\n        auto& stateOutput = convertToTensor(ctx->tensors().at(bodyOutputName), ctx);\n        LOG_VERBOSE(\"For state variable output: \" << bodyOutputName\n                                                  << \", found matching tensor: \" << stateOutput.getName()\n                                                  << \", with shape: \" << stateOutput.getDimensions());\n        stateVars.at(i)->setInput(1, stateOutput);\n        // Each state variable is also a loop output\n        auto stateVarOutput = N_CHECK(stateVars.at(i)->getOutput(0));\n        auto stateOutputLayer = N_CHECK(loop->addLoopOutput(*stateVarOutput, nvinfer1::LoopOutput::kLAST_VALUE));\n        auto stateOutputTensor = N_CHECK(stateOutputLayer->getOutput(0));\n        nodeOutputs.emplace_back(stateOutputTensor);\n    }\n    int32_t const nbOutputs = body.output_size();\n    // Finally, set up scan outputs if there are any\n    for (int32_t i = nbStateVars + NB_DISCARDED_OUTPUTS; i < nbOutputs; ++i)\n    {\n        auto const& bodyOutputName = body.output(i).name();\n        auto& scanOutput = convertToTensor(ctx->tensors().at(bodyOutputName), ctx);\n        LOG_VERBOSE(\"For scan output: \" << bodyOutputName << \", found matching tensor: \" << scanOutput.getName()\n                                        << \", with shape: \" << scanOutput.getDimensions());\n        nvinfer1::ILoopOutputLayer* trtScanOut\n            = N_CHECK(loop->addLoopOutput(scanOutput, nvinfer1::LoopOutput::kCONCATENATE, 0));\n        // If trip limit is set, we can set the loop output to the tripLimit, otherwise, set to some dummy constant\n        // value.\n        // In the latter case, the scan outputs must not be used in the rest of the model.\n        if (tripLimit)\n        {\n            trtScanOut->setInput(1, *tripLimit);\n        }\n        else\n        {\n            trtScanOut->setInput(1,\n                *N_CHECK(addConstantScalar(ctx, DUMMY_SCAN_OUTPUT_LENGTH, ::ONNX_NAMESPACE::TensorProto_DataType_INT32)\n                             ->getOutput(0)));\n        }\n        nodeOutputs.emplace_back(N_CHECK(trtScanOut->getOutput(0)));\n    }\n\n    return {nodeOutputs};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(LRN)\n{\n    nvinfer1::ITensor& tensor = convertToTensor(inputs.at(0), ctx);\n    OnnxAttrs attrs(node, ctx);\n    int size = attrs.get<int>(\"size\");\n    float alpha = attrs.get<float>(\"alpha\", 0.0001f);\n    float beta = attrs.get<float>(\"beta\", 0.75f);\n    float bias = attrs.get<float>(\"bias\", 1.0f);\n    auto* layer = N_CHECK(ctx->network()->addLRN(tensor, size, alpha, beta, bias));\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(LSTM)\n{\n    using trtAct = nvinfer1::ActivationType;\n    using eOp = nvinfer1::ElementWiseOperation;\n\n    OnnxAttrs attrs{node, ctx};\n    constexpr int32_t NUM_GATES = 4;\n    std::string const direction = attrs.get<std::string>(\"direction\", \"forward\");\n    int32_t const numDirections = (direction == \"bidirectional\") ? 2 : 1;\n    int32_t const hiddenSize = attrs.get<int>(\"hidden_size\");\n    float const clip = attrs.get(\"clip\", -1.f); // Clipping cannot be negative, so -1.0 is a good sentinel value.\n\n    // The input is in SBE format\n    nvinfer1::ITensor* input = &convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor* weights = &convertToTensor(inputs.at(1), ctx);\n    nvinfer1::ITensor* recurrenceWeights = &convertToTensor(inputs.at(2), ctx);\n\n    std::vector<trtAct> defaultActs{trtAct::kSIGMOID, trtAct::kTANH, trtAct::kTANH};\n    if (numDirections == 2)\n    {\n        defaultActs.insert(defaultActs.end(), {trtAct::kSIGMOID, trtAct::kTANH, trtAct::kTANH});\n    }\n    std::vector<trtAct> activations = attrs.get<std::vector<trtAct>>(\"activations\", defaultActs);\n\n    std::vector<float> activationAlphas = attrs.get<std::vector<float>>(\"activation_alpha\", std::vector<float>{});\n    activationAlphas = parseLSTMActivationValues(activations, activationAlphas, true);\n\n    std::vector<float> activationBetas = attrs.get<std::vector<float>>(\"activation_beta\", std::vector<float>{});\n    activationBetas = parseLSTMActivationValues(activations, activationBetas, false);\n\n    // Roll Rb into Wb (and RBb into WBb). Bias is in the form  [Wb[iofc], Rb[iofc], WBb[iofc], RBb[iofc]].\n    // So reshape such that we can perform a reduction to add Wb and Rb.\n    nvinfer1::ITensor* combinedBias{nullptr};\n    if (inputs.size() > 3 && inputs.at(3))\n    {\n        nvinfer1::ITensor* bias = &convertToTensor(inputs.at(3), ctx);\n        LOG_VERBOSE(\"Bias shape is: \" << bias->getDimensions());\n        // Reshape to [[Wb[iofc], Rb[iofc]], [WBb[iofc], RBb[iofc]]]\n        nvinfer1::IShuffleLayer* reshapeBias = N_CHECK(ctx->network()->addShuffle(*bias));\n        reshapeBias->setReshapeDimensions(nvinfer1::Dims3{numDirections, 2, NUM_GATES * hiddenSize});\n        reshapeBias->setZeroIsPlaceholder(false);\n        auto* reshapeBiasOut = N_CHECK(reshapeBias->getOutput(0));\n        LOG_VERBOSE(\"Reshaping bias to: \" << reshapeBiasOut->getDimensions());\n        auto reduceLayer\n            = N_CHECK(ctx->network()->addReduce(*reshapeBiasOut, nvinfer1::ReduceOperation::kSUM, /*axis=*/0b010,\n                /*keepDimensions=*/true));\n        combinedBias = N_CHECK(reduceLayer->getOutput(0));\n        LOG_VERBOSE(\"After reduction, bias shape is: \" << combinedBias->getDimensions());\n    }\n\n    // Get a shape tensor containing: (numDirections, batchSize, hiddenSize)\n    auto const initialStateShape = [&ctx, &numDirections, &hiddenSize, &input]() -> nvinfer1::ITensor* {\n        // Get batchSize from input shape\n        nvinfer1::ITensor* numDirectionsTensor = addConstantScalar(\n            ctx, numDirections, ::ONNX_NAMESPACE::TensorProto_DataType_INT32, nvinfer1::Dims{1, {1}})\n                                                     ->getOutput(0);\n        LOG_VERBOSE(\"numDirectionsTensor shape: \" << numDirectionsTensor->getDimensions());\n        nvinfer1::ITensor* hiddenSizeTensor\n            = addConstantScalar(ctx, hiddenSize, ::ONNX_NAMESPACE::TensorProto_DataType_INT32, nvinfer1::Dims{1, {1}})\n                  ->getOutput(0);\n        LOG_VERBOSE(\"hiddenSizeTensor shape: \" << hiddenSizeTensor->getDimensions());\n        nvinfer1::ITensor* batchSizeTensor = getAxisLength(ctx, input, 1, nvinfer1::Dims{1, {1}});\n        LOG_VERBOSE(\"batchSizeTensor shape: \" << batchSizeTensor->getDimensions());\n\n        std::array<nvinfer1::ITensor*, 3> tensors{{numDirectionsTensor, batchSizeTensor, hiddenSizeTensor}};\n        nvinfer1::IConcatenationLayer* concatenatedShape = N_CHECK(ctx->network()->addConcatenation(tensors.data(), 3));\n        return N_CHECK(concatenatedShape->getOutput(0));\n    };\n    nvinfer1::ITensor* gateOutputShape = initialStateShape();\n    LOG_VERBOSE(\"Gate output rank (equal to initial hidden/cell state rank): \" << gateOutputShape->getDimensions());\n\n    auto const getInitialInputValue = [&ctx, &gateOutputShape, &inputs, &node](size_t inputIdx) -> nvinfer1::ITensor* {\n        if (inputs.size() > inputIdx && inputs.at(inputIdx))\n        {\n            return &convertToTensor(inputs.at(inputIdx), ctx);\n        }\n        return constantOfShape(ctx,\n            addConstantScalar(ctx, 0.f, ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT, nvinfer1::Dims{1, {1}})\n                ->getOutput(0),\n            gateOutputShape);\n    };\n\n    nvinfer1::ITensor* initialHidden = getInitialInputValue(5);\n    LOG_VERBOSE(\"Initial hidden state shape: \" << initialHidden->getDimensions());\n\n    nvinfer1::ITensor* initialCellState = getInitialInputValue(6);\n    LOG_VERBOSE(\"Initial cell state shape: \" << initialCellState->getDimensions());\n\n    LOG_VERBOSE(\"Entering Loop\");\n    // Scan over the S dimension of the input\n    auto loop = N_CHECK(ctx->network()->addLoop());\n    nvinfer1::ITensor* tripLimit = getAxisLength(ctx, input, 0);\n    loop->addTripLimit(*tripLimit, nvinfer1::TripLimit::kCOUNT);\n\n    // Add X(t)\n    nvinfer1::ITensor* iterationInput = addRNNInput(ctx, node, loop, inputs, direction);\n    ONNXTRT_CHECK_NODE(iterationInput, \"Failed to add RNN input.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    // H(t-1)\n    nvinfer1::IRecurrenceLayer* Ht1 = N_CHECK(loop->addRecurrence(*initialHidden));\n    ctx->registerLayer(Ht1, node);\n    auto Ht1Output = N_CHECK(Ht1->getOutput(0));\n    LOG_VERBOSE(\"Hidden state shape: \" << Ht1Output->getDimensions());\n\n    // C(t-1)\n    nvinfer1::IRecurrenceLayer* Ct1 = loop->addRecurrence(*initialCellState);\n    auto Ct1Output = N_CHECK(Ct1->getOutput(0));\n    LOG_VERBOSE(\"Cell state shape: \" << Ct1Output->getDimensions());\n\n    // Compute intermediate(t) = (X(t) * W^T + H(t-1) * R^T + (Wb + Rb)). intermediate(t) has shape (numDirections,\n    // batchSize, 4 * hiddenSize)\n    nvinfer1::ITensor* xtWT = ctx->network()\n                                  ->addMatrixMultiply(*iterationInput, nvinfer1::MatrixOperation::kNONE, *weights,\n                                      nvinfer1::MatrixOperation::kTRANSPOSE)\n                                  ->getOutput(0);\n    LOG_VERBOSE(\"X(t) * W^T -> \" << xtWT->getDimensions());\n\n    nvinfer1::ITensor* ht1RT = ctx->network()\n                                   ->addMatrixMultiply(*Ht1Output, nvinfer1::MatrixOperation::kNONE, *recurrenceWeights,\n                                       nvinfer1::MatrixOperation::kTRANSPOSE)\n                                   ->getOutput(0);\n    LOG_VERBOSE(\"H(t-1) * R^T -> \" << ht1RT->getDimensions());\n\n    nvinfer1::ITensor* intermediatet = getElementWiseResult(ctx, *xtWT, *ht1RT, eOp::kSUM);\n    if (combinedBias)\n    {\n        intermediatet = getElementWiseResult(ctx, *intermediatet, *combinedBias, eOp::kSUM);\n    }\n    LOG_VERBOSE(\"intermediate(t) -> \" << intermediatet->getDimensions());\n\n    // Gate shape is (numDirections, batchSize, hiddenSize)\n    auto const isolateGate\n        = [&ctx, &hiddenSize, &gateOutputShape](nvinfer1::ITensor* gates, int32_t gateIndex) -> nvinfer1::ITensor* {\n        nvinfer1::ISliceLayer* isolate = N_CHECK(ctx->network()->addSlice(\n            *gates, nvinfer1::Dims3{0, 0, 0}, nvinfer1::Dims3{0, 0, 0}, nvinfer1::Dims3{1, 1, 1}));\n        ;\n        isolate->setInput(1,\n            *addConstant(ctx, std::vector<int32_t>{0, 0, gateIndex * hiddenSize},\n                ::ONNX_NAMESPACE::TensorProto_DataType_INT32, nvinfer1::Dims{1, {3}})\n                 ->getOutput(0));               // Start\n        isolate->setInput(2, *gateOutputShape); // Size\n        return N_CHECK(isolate->getOutput(0));\n    };\n\n    // Compute peephole connections\n    nvinfer1::ITensor* peephole{nullptr};\n    if (inputs.size() > 7 && inputs.at(7))\n    {\n        peephole = &convertToTensor(inputs.at(7), ctx);\n    }\n\n    auto const addPeephole = [&ctx, &node, &hiddenSize, &numDirections, &peephole](nvinfer1::ITensor* gate,\n                                 nvinfer1::ITensor* cellState, int32_t gateIndex) -> nvinfer1::ITensor* {\n        nvinfer1::ISliceLayer* isolatePeephole\n            = N_CHECK(ctx->network()->addSlice(*peephole, nvinfer1::Dims2{0, gateIndex * hiddenSize},\n                nvinfer1::Dims2{numDirections, hiddenSize}, nvinfer1::Dims2{1, 1}));\n        auto* peepholeWeights = unsqueezeTensor(ctx, *isolatePeephole->getOutput(0), std::vector<int32_t>{1});\n        LOG_VERBOSE(\"Peephole weight for gate: \" << gateIndex << \" shape: \" << peepholeWeights->getDimensions());\n\n        return getElementWiseResult(\n            ctx, *gate, *getElementWiseResult(ctx, *peepholeWeights, *cellState, eOp::kPROD), eOp::kSUM);\n    };\n\n    // NOTE: . represents a hadamard product\n    nvinfer1::ITensor* itGate = isolateGate(intermediatet, 0);\n\n    if (peephole)\n    {\n        // i(t) (w/ peephole) =  i(t) + Pi . C(t-1)\n        itGate = addPeephole(itGate, Ct1Output, 0);\n    }\n\n    nvinfer1::IActivationLayer* itGateAct\n        = N_CHECK(ctx->network()->addActivation(*addClip(ctx, itGate, clip), activations.at(0)));\n    itGateAct->setAlpha(activationAlphas.at(0));\n    itGateAct->setBeta(activationBetas.at(0));\n    itGate = N_CHECK(itGateAct->getOutput(0));\n\n    nvinfer1::ITensor* ftGate = isolateGate(intermediatet, 2);\n\n    if (peephole)\n    {\n        // f(t) (w/ peephole) =  f(t) + Pf . C(t-1)\n        ftGate = addPeephole(ftGate, Ct1Output, 2);\n    }\n\n    nvinfer1::IActivationLayer* ftGateAct\n        = N_CHECK(ctx->network()->addActivation(*addClip(ctx, ftGate, clip), activations.at(0)));\n    ftGateAct->setAlpha(activationAlphas.at(0));\n    ftGateAct->setBeta(activationBetas.at(0));\n    ftGate = N_CHECK(ftGateAct->getOutput(0));\n\n    // c(t) = g(intermediate(t)[:, :, 3H:4H])\n    nvinfer1::IActivationLayer* ctAct\n        = N_CHECK(ctx->network()->addActivation(*addClip(ctx, isolateGate(intermediatet, 3), clip), activations.at(1)));\n    ctAct->setAlpha(activationAlphas.at(1));\n    ctAct->setBeta(activationBetas.at(1));\n\n    nvinfer1::ITensor* ctGate = N_CHECK(ctAct->getOutput(0));\n    LOG_VERBOSE(\"c(t) -> \" << ctGate->getDimensions());\n\n    // C(t) = f(t) . C(t - 1) + i(t) . c(t)\n    nvinfer1::ITensor* operandIC = getElementWiseResult(ctx, *itGate, *ctGate, eOp::kPROD);\n    nvinfer1::ITensor* operandFC = getElementWiseResult(ctx, *ftGate, *Ct1Output, eOp::kPROD);\n    nvinfer1::ITensor* Ct = getElementWiseResult(ctx, *operandFC, *operandIC, eOp::kSUM);\n\n    nvinfer1::ITensor* singlePassShape = getElementWiseResult(ctx, *gateOutputShape,\n        *addConstant(ctx, std::vector<int>{numDirections, 1, 1}, ::ONNX_NAMESPACE::TensorProto_DataType_INT32,\n            nvinfer1::Dims{1, {3}})\n             ->getOutput(0),\n        eOp::kDIV);\n\n    if (inputs.size() > 4 && inputs.at(4))\n    {\n        nvinfer1::ITensor* seqLens = &convertToTensor(inputs.at(4), ctx);\n        auto maxLen = getAxisLength(ctx, input, 0);\n        Ct = numDirections == 2\n            ? maskBidirRNNHidden(ctx, node, loop, seqLens, maxLen, Ct1Output, Ct, singlePassShape)\n            : maskRNNHidden(ctx, node, loop, seqLens, Ct1Output, Ct, maxLen, direction == \"reverse\");\n    }\n\n    Ct1->setInput(1, *Ct);\n    LOG_VERBOSE(\"C(t) -> \" << Ct->getDimensions());\n\n    nvinfer1::ITensor* otGate = isolateGate(intermediatet, 1);\n\n    if (peephole)\n    {\n        // o(t) (w/ peephole) =  o(t) + Po . C(t)\n        otGate = addPeephole(otGate, Ct, 1);\n    }\n\n    nvinfer1::IActivationLayer* otGateAct\n        = N_CHECK(ctx->network()->addActivation(*addClip(ctx, otGate, clip), activations.at(0)));\n    otGateAct->setAlpha(activationAlphas.at(0));\n    otGateAct->setBeta(activationBetas.at(0));\n    otGate = N_CHECK(otGateAct->getOutput(0));\n\n    // H(t) = o(t) . h(C(t))\n    nvinfer1::IActivationLayer* hAct\n        = N_CHECK(ctx->network()->addActivation(*addClip(ctx, Ct, clip), activations.at(2)));\n    hAct->setAlpha(activationAlphas.at(2));\n    hAct->setBeta(activationBetas.at(2));\n    auto hActTensor = N_CHECK(hAct->getOutput(0));\n\n    nvinfer1::ITensor* Ht = getElementWiseResult(ctx, *otGate, *hActTensor, eOp::kPROD);\n    if (inputs.size() > 4 && inputs.at(4))\n    {\n        nvinfer1::ITensor* seqLens = &convertToTensor(inputs.at(4), ctx);\n        auto maxLen = getAxisLength(ctx, input, 0);\n        Ht = numDirections == 2\n            ? maskBidirRNNHidden(ctx, node, loop, seqLens, maxLen, Ht1Output, Ht, singlePassShape)\n            : maskRNNHidden(ctx, node, loop, seqLens, Ht1Output, Ht, maxLen, direction == \"reverse\");\n    }\n    Ht1->setInput(1, *Ht);\n    LOG_VERBOSE(\"H(t) -> \" << Ht->getDimensions());\n\n    std::vector<TensorOrWeights> outputs{};\n\n    // Add outputs. In ONNX, all LSTM outputs are optional, so check for them here.\n    auto shouldAddOutput = [&node](int32_t index) { return index < node.output().size() && !node.output(index).empty(); };\n\n    // Y = concatenation of all H(t) for each element of the sequence\n    // singlePassShape = (1, batchSize, hiddenSize)\n    if (shouldAddOutput(0))\n    {\n        outputs.emplace_back(concatenateRNNOutputs(ctx, node, loop, singlePassShape, getAxisLength(ctx, input, 0), Ht,\n            numDirections, inputs, direction == \"reverse\"));\n    }\n    // Yh = last value of H(t)\n    if (shouldAddOutput(1))\n    {\n        auto yhLayer = N_CHECK(loop->addLoopOutput(*Ht1Output, nvinfer1::LoopOutput::kLAST_VALUE));\n        outputs.emplace_back(N_CHECK(yhLayer->getOutput(0)));\n    }\n    // Yc = last value of C(t)\n    if (shouldAddOutput(2))\n    {\n        auto ycLayer = N_CHECK(loop->addLoopOutput(*Ct1Output, nvinfer1::LoopOutput::kLAST_VALUE));\n        outputs.emplace_back(N_CHECK(ycLayer->getOutput(0)));\n    }\n\n    return {{outputs}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(LpNormalization)\n{\n    using eOp = nvinfer1::ElementWiseOperation;\n    using uOp = nvinfer1::UnaryOperation;\n    using rOp = nvinfer1::ReduceOperation;\n\n    OnnxAttrs attrs(node, ctx);\n    nvinfer1::ITensor* input = &convertToTensor(inputs.at(0), ctx);\n    auto inputType = input->getType();\n    int32_t axis = attrs.get<int32_t>(\"axis\", -1);\n    int32_t p = attrs.get<int32_t>(\"p\", 2);\n    int32_t nbDims = input->getDimensions().nbDims;\n    DataType dt = input->getType();\n    ONNXTRT_CHECK_NODE((dt == DataType::kFLOAT || dt == DataType::kHALF),\n        \"Only float inputs/outputs supported in LpNormalization. The current data type = \" + getTrtDtypeName(dt) + \".\",\n        node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    convertAxis(axis, nbDims, node, nodeIdx);\n\n    nvinfer1::ITensor* norm{nullptr};\n    TensorOrWeights zeros = ctx->createNamedTempWeights(trtDataTypeToONNX(inputType), {0, {}});\n    nvinfer1::ITensor* zerosTensor = &convertToTensor(zeros, ctx);\n    broadcastTensor(ctx, zerosTensor, nbDims);\n\n    if (p == 1)\n    {\n        // abs(x)\n        nvinfer1::IUnaryLayer* absLayer = N_CHECK(ctx->network()->addUnary(*input, uOp::kABS));\n        ctx->registerLayer(absLayer, node);\n        norm = N_CHECK(absLayer->getOutput(0));\n\n        // norm coeff = sum(abs(x)) along axis dimension\n        nvinfer1::IReduceLayer* reduceLayer = N_CHECK(ctx->network()->addReduce(*norm, rOp::kSUM, 1 << axis, true));\n        ctx->registerLayer(reduceLayer, node);\n        norm = N_CHECK(reduceLayer->getOutput(0));\n    }\n    else if (p == 2)\n    {\n        // x^2\n        auto* sqrLayer = N_CHECK(ctx->network()->addElementWise(*input, *input, eOp::kPROD));\n        ctx->registerLayer(sqrLayer, node);\n        norm = N_CHECK(sqrLayer->getOutput(0));\n\n        // sum(x^2) along axis dimension\n        nvinfer1::IReduceLayer* reduceLayer = N_CHECK(ctx->network()->addReduce(*norm, rOp::kSUM, 1 << axis, true));\n        ctx->registerLayer(reduceLayer, node);\n        norm = N_CHECK(reduceLayer->getOutput(0));\n\n        // norm coeff = sqrt(sum(x^2))\n        nvinfer1::IUnaryLayer* sqrtLayer = N_CHECK(ctx->network()->addUnary(*norm, uOp::kSQRT));\n        ctx->registerLayer(sqrtLayer, node);\n        norm = N_CHECK(sqrtLayer->getOutput(0));\n    }\n\n    // norm coeff |= 1 (change 0s to 1s, leave all other values same)\n    nvinfer1::IElementWiseLayer* maskLayer = N_CHECK(ctx->network()->addElementWise(*norm, *zerosTensor, eOp::kEQUAL));\n    ctx->registerLayer(maskLayer, node);\n    nvinfer1::ITensor* mask = N_CHECK(maskLayer->getOutput(0));\n    mask = castHelper(ctx, mask, dt);\n    auto* combinedLayer = N_CHECK(ctx->network()->addElementWise(*norm, *mask, eOp::kSUM));\n    ctx->registerLayer(combinedLayer, node);\n    norm = N_CHECK(combinedLayer->getOutput(0));\n\n    // x/(norm coeff)\n    // norm tensor is broadcast along axis dimension to match shape of input\n    auto* layer = N_CHECK(ctx->network()->addElementWise(*input, *norm, eOp::kDIV));\n    ctx->registerLayer(layer, node);\n\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(LpPool)\n{\n    using eOp = nvinfer1::ElementWiseOperation;\n    using uOp = nvinfer1::UnaryOperation;\n    using pType = nvinfer1::PoolingType;\n\n    OnnxAttrs attrs(node, ctx);\n    nvinfer1::ITensor* input = &convertToTensor(inputs.at(0), ctx);\n    int32_t p = attrs.get<int32_t>(\"p\", 2);\n    int32_t nbDims = input->getDimensions().nbDims;\n    int32_t nbSpatialDims = attrs.get<nvinfer1::Dims>(\"kernel_shape\").nbDims;\n\n    DataType dt = input->getType();\n    ONNXTRT_CHECK_NODE((dt == DataType::kFLOAT || dt == DataType::kHALF),\n        \"Only float inputs/outputs supported in LpPool. The current data type = \" + getTrtDtypeName(dt) + \".\", node,\n        nodeIdx, ErrorCode::kINVALID_NODE);\n\n    nvinfer1::Dims kernelShape = makeDims(nbSpatialDims, 1);\n    nvinfer1::Dims strides = makeDims(nbSpatialDims, 1);\n    nvinfer1::Dims begPadding = makeDims(nbSpatialDims, 0);\n    nvinfer1::Dims endPadding = makeDims(nbSpatialDims, 0);\n    nvinfer1::PaddingMode paddingMode;\n    bool excludePadding(false);\n    bool ceilMode = static_cast<bool>(attrs.get<int32_t>(\"ceil_mode\", 0));\n    getKernelParams(ctx, node, &kernelShape, &strides, &begPadding, &endPadding, paddingMode, excludePadding, nullptr,\n        nullptr, ceilMode);\n\n    nvinfer1::Dims scalarDims = makeDims(nbDims, 1);\n    float kernelSz{1.0F};\n    for (int32_t i = 0; i < kernelShape.nbDims; i++)\n    {\n        kernelSz *= kernelShape.d[i];\n    }\n\n    nvinfer1::IConstantLayer* kernelSzLayer;\n    if (dt == DataType::kHALF)\n    {\n        kernelSzLayer = addConstantScalar(\n            ctx, static_cast<half_float::half>(kernelSz), ::ONNX_NAMESPACE::TensorProto::FLOAT16, scalarDims);\n    }\n    else\n    {\n        kernelSzLayer = addConstantScalar(ctx, kernelSz, ::ONNX_NAMESPACE::TensorProto::FLOAT, scalarDims);\n    }\n\n    nvinfer1::ITensor* output{nullptr};\n    if (p == 1)\n    {\n        // x' = abs(x)\n        nvinfer1::IUnaryLayer* absLayer = N_CHECK(ctx->network()->addUnary(*input, uOp::kABS));\n        ctx->registerLayer(absLayer, node);\n        output = N_CHECK(absLayer->getOutput(0));\n    }\n    else if (p == 2)\n    {\n        // x' = x^2\n        auto* sqrLayer = N_CHECK(ctx->network()->addElementWise(*input, *input, eOp::kPROD));\n        ctx->registerLayer(sqrLayer, node);\n        output = N_CHECK(sqrLayer->getOutput(0));\n    }\n\n    // pool_avg(x')\n    nvinfer1::IPoolingLayer* poolLayer = N_CHECK(ctx->network()->addPoolingNd(*output, pType::kAVERAGE, kernelShape));\n    poolLayer->setPaddingMode(paddingMode);\n    poolLayer->setPrePadding(begPadding);\n    poolLayer->setPostPadding(endPadding);\n    poolLayer->setStrideNd(strides);\n    poolLayer->setAverageCountExcludesPadding(excludePadding);\n    ctx->registerLayer(poolLayer, node);\n    output = N_CHECK(poolLayer->getOutput(0));\n\n    // pool_sum = pool_avg(x')*kernel_size\n    auto* correctedSumLayer\n        = N_CHECK(ctx->network()->addElementWise(*output, *kernelSzLayer->getOutput(0), eOp::kPROD));\n    ctx->registerLayer(correctedSumLayer, node);\n    output = correctedSumLayer->getOutput(0);\n\n    // if p == 1, output = pool_sum\n    // if p == 2, output = sqrt(pool_sum)\n    if (p == 2)\n    {\n        nvinfer1::IUnaryLayer* sqrtLayer = N_CHECK(ctx->network()->addUnary(*output, uOp::kSQRT));\n        ctx->registerLayer(sqrtLayer, node);\n        output = N_CHECK(sqrtLayer->getOutput(0));\n    }\n    return {{output}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(MatMul)\n{\n    checkNotInvalidType(inputs.at(0), {\"INT32\", \"INT64\"}, node, nodeIdx);\n    nvinfer1::ITensor* inputA = &convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor* inputB = &convertToTensor(inputs.at(1), ctx);\n\n    bool needSqueezeHead = false;\n    bool needSqueezeTail = false;\n    int32_t const t1Dims = inputA->getDimensions().nbDims;\n    int32_t const t2Dims = inputB->getDimensions().nbDims;\n    if (t1Dims > t2Dims && t2Dims == 1)\n    {\n        // The second input is 1-D vector, promote to matrix by appending 1 in shape.\n        std::vector<int32_t> axes{1};\n        inputB = unsqueezeTensor(ctx, *inputB, axes);\n        needSqueezeTail = true;\n    }\n    else if (t1Dims < t2Dims && t1Dims == 1)\n    {\n        // The first argument is 1-D, promote to matrix by prepending a 1 in shape.\n        // This is done in broadcast extra dimensions.\n        needSqueezeHead = true;\n    }\n    broadcastTensors(ctx, inputA, inputB);\n\n    auto const getMatrixOp = [](nvinfer1::ITensor const& input) {\n        return (input.getDimensions().nbDims == 1) ? nvinfer1::MatrixOperation::kVECTOR\n                                                   : nvinfer1::MatrixOperation::kNONE;\n    };\n\n    nvinfer1::MatrixOperation opA = getMatrixOp(*inputA);\n    nvinfer1::MatrixOperation opB = getMatrixOp(*inputB);\n\n    nvinfer1::IMatrixMultiplyLayer* matmul = N_CHECK(ctx->network()->addMatrixMultiply(*inputA, opA, *inputB, opB));\n    ctx->registerLayer(matmul, node);\n\n    auto outputTensor = N_CHECK(matmul->getOutput(0));\n    if (needSqueezeHead)\n    {\n        // After MM we need remove the prepended 1.\n        std::vector<int32_t> axes{0};\n        outputTensor = squeezeTensor(ctx, *outputTensor, axes);\n    }\n    if (needSqueezeTail)\n    {\n        // After MM we need remove the appended 1.\n        std::vector<int32_t> axes{outputTensor->getDimensions().nbDims - 1};\n        outputTensor = squeezeTensor(ctx, *outputTensor, axes);\n    }\n\n    return {{outputTensor}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Max)\n{\n    return elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kMAX);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(MaxPool)\n{\n    return poolingHelper(ctx, node, nodeIdx, inputs, nvinfer1::PoolingType::kMAX);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Mean)\n{\n    std::vector<TensorOrWeights> sumResult\n        = elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kSUM);\n    auto sum_input = sumResult.at(0);\n    nvinfer1::ITensor& sum_tensor = convertToTensor(sum_input, ctx);\n\n    int32_t ndim = sum_tensor.getDimensions().nbDims;\n    float scale_value = 1.f / inputs.size();\n    auto scale_dtype = sum_input.isBFp16()\n        ? ::ONNX_NAMESPACE::TensorProto::BFLOAT16\n        : (sum_input.isFp16() ? ::ONNX_NAMESPACE::TensorProto::FLOAT16 : ::ONNX_NAMESPACE::TensorProto::FLOAT);\n    auto scale_shape = nvinfer1::Dims{ndim, {1, 1, 1, 1, 1, 1, 1, 1}};\n    auto scale_weights = ctx->createNamedTempWeights(scale_dtype, scale_shape);\n    static_cast<float*>(scale_weights.values)[0] = scale_value;\n    auto* constant_layer = N_CHECK(ctx->network()->addConstant(scale_weights.shape, scale_weights));\n    ctx->network()->setWeightsName(scale_weights, scale_weights.getName());\n    nvinfer1::ITensor& scale_constant = *constant_layer->getOutput(0);\n    auto* outputLayer\n        = ctx->network()->addElementWise(sum_tensor, scale_constant, nvinfer1::ElementWiseOperation::kPROD);\n    RETURN_FIRST_OUTPUT(outputLayer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(MeanVarianceNormalization)\n{\n    // Previous: stdDev = sqrt(E(x^2) - E(x)^2)\n    // Current: stdDev = sqrt(E((x-E(x))^2))\n    // The current formula avoids (E(x^2) - E(x)^2) < 0 caused by float point precision errors\n    using eOp = nvinfer1::ElementWiseOperation;\n    using uOp = nvinfer1::UnaryOperation;\n    using rOp = nvinfer1::ReduceOperation;\n\n    OnnxAttrs attrs(node, ctx);\n    nvinfer1::ITensor* input = &convertToTensor(inputs.at(0), ctx);\n    auto const dims = input->getDimensions();\n    DataType const dt = input->getType();\n\n    ONNXTRT_CHECK_NODE((dt == DataType::kFLOAT || dt == DataType::kHALF || dt == DataType::kBF16),\n        \"Only float32/float16/bfloat16 inputs/outputs supported in MeanVarianceNormalization. The current data type = \"\n            + getTrtDtypeName(dt) + \".\",\n        node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE_DATATYPE);\n\n    // convert axes vector to bitmask\n    std::vector<int32_t> const defaultAxes = {0, 2, 3};\n    auto const axes = attrs.get<std::vector<int32_t>>(\"axes\", defaultAxes);\n    uint32_t axesMask = 0;\n\n    for (int32_t axis : axes)\n    {\n        convertAxis(axis, dims.nbDims, node, nodeIdx);\n        axesMask |= 1 << axis;\n    }\n\n    // mean(x) along axes direction\n    auto* reduceLayer = N_CHECK(ctx->network()->addReduce(*input, rOp::kAVG, axesMask, true));\n    ctx->registerLayer(reduceLayer, node);\n    auto* meanX = N_CHECK(reduceLayer->getOutput(0));\n\n    // numerator: x-mean(x)\n    auto* numSubLayer = N_CHECK(ctx->network()->addElementWise(*input, *meanX, eOp::kSUB));\n    ctx->registerLayer(numSubLayer, node);\n    auto* numerator = N_CHECK(numSubLayer->getOutput(0));\n\n    // (x-mean(x))^2\n    auto* sqrLayer = N_CHECK(ctx->network()->addElementWise(*numerator, *numerator, eOp::kPROD));\n    ctx->registerLayer(sqrLayer, node);\n    auto* sqrNumerator = N_CHECK(sqrLayer->getOutput(0));\n\n    // mean((x-mean(x))^2)\n    auto* meanLayer = N_CHECK(ctx->network()->addReduce(*sqrNumerator, rOp::kAVG, axesMask, true));\n    ctx->registerLayer(meanLayer, node);\n    auto* variance = N_CHECK(meanLayer->getOutput(0));\n\n    // sqrt(mean((x-mean(x))^2))\n    nvinfer1::IUnaryLayer* sqrtLayer = N_CHECK(ctx->network()->addUnary(*variance, uOp::kSQRT));\n    ctx->registerLayer(sqrtLayer, node);\n    auto* stdDev = N_CHECK(sqrtLayer->getOutput(0));\n\n    // denominator: avoid division by zero\n    nvinfer1::Dims scalarShape{dims.nbDims};\n    std::fill(scalarShape.d, scalarShape.d + scalarShape.nbDims, 1);\n    auto* epsilonTensor\n        = addConstantScalar(ctx, 1e-9f, ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT, scalarShape)->getOutput(0);\n    auto* addEpsLayer = N_CHECK(ctx->network()->addElementWise(*stdDev, *epsilonTensor, eOp::kSUM));\n    ctx->registerLayer(addEpsLayer, node);\n    stdDev = N_CHECK(addEpsLayer->getOutput(0));\n\n    // division numerator/standard-deviation\n    auto* divLayer = N_CHECK(ctx->network()->addElementWise(*numerator, *stdDev, eOp::kDIV));\n    ctx->registerLayer(divLayer, node);\n    RETURN_FIRST_OUTPUT(divLayer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Min)\n{\n    return elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kMIN);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Mul)\n{\n    return elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kPROD);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Mod)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    using eOp = nvinfer1::ElementWiseOperation;\n    OnnxAttrs attrs(node, ctx);\n    int32_t const fmod = attrs.get(\"fmod\", 0);\n    nvinfer1::ITensor* input0 = &convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor* input1 = &convertToTensor(inputs.at(1), ctx);\n    broadcastTensors(ctx, input0, input1);\n\n    if (fmod == 0)\n    {\n        // fmod = 0, inputs can only be integers\n        ONNXTRT_CHECK_NODE((input0->getType() == DataType::kINT32 || input0->getType() == DataType::kINT64),\n            \"The fmod attribute is set to 0. Inputs cannot be of floating point types. The current input type is \"\n                + getTrtDtypeName(input0->getType()) + \".\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE_DATATYPE);\n        // Result = input0 - (input1 * floorDiv(input0, input1))\n        nvinfer1::IElementWiseLayer* resultLayer = modWithIntegerInputs(ctx, input0, input1, false);\n\n        ctx->registerLayer(resultLayer, node);\n        RETURN_FIRST_OUTPUT(resultLayer, node, nodeIdx);\n    }\n    // Fmod with integer inputs\n    else if (input0->getType() == DataType::kINT32 || input0->getType() == DataType::kINT64)\n    {\n        // Result = input0 - (input1 * Div(input0, input1))\n        nvinfer1::IElementWiseLayer* resultLayer = modWithIntegerInputs(ctx, input0, input1, true);\n        ctx->registerLayer(resultLayer, node);\n        RETURN_FIRST_OUTPUT(resultLayer, node, nodeIdx);\n    }\n    // Fmod with floating point inputs\n    else\n    {\n        // Calculate input0 / input1\n        std::vector<TensorOrWeights> divResult = elementwiseHelper(ctx, node, nodeIdx, {input0, input1}, eOp::kDIV);\n        auto* divResultTensor = &convertToTensor(divResult.at(0), ctx);\n\n        // Calculate input0 - (input1 * floor(input0 / input1))\n        nvinfer1::IElementWiseLayer* layerWithDivFloor = modWithFPInputs(ctx, input0, input1, divResultTensor, true);\n\n        // Calculate input0 - (input1 * ceil(input0 / input1))\n        nvinfer1::IElementWiseLayer* layerWithDivCeil = modWithFPInputs(ctx, input0, input1, divResultTensor, false);\n\n        auto* zero = createZeroTensor(ctx, divResultTensor);\n        std::vector<TensorOrWeights> greaterOrEqualResult\n            = greaterLessOrEqual(ctx, node, nodeIdx, divResultTensor, zero, true);\n        auto* condition = &convertToTensor(greaterOrEqualResult.at(0), ctx);\n        auto* outputWithDivFloor = layerWithDivFloor->getOutput(0);\n        auto* outputWithDivCeil = layerWithDivCeil->getOutput(0);\n\n        // If (input0 / input1) >= 0, result = input0 - (input1 * floor(input0 / input1))\n        // Else result = input0 - (input1 * ceil(input0 / input1))\n        auto* result = N_CHECK(ctx->network()->addSelect(*condition, *outputWithDivFloor, *outputWithDivCeil));\n        ctx->registerLayer(result, node);\n        RETURN_FIRST_OUTPUT(result, node, nodeIdx);\n    }\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Neg)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kNEG);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(NonMaxSuppression)\n{\n    // max_output, iou_threshold and score_threshold are optional\n    ONNXTRT_CHECK_NODE(inputs.size() >= 2 && inputs.size() <= 5,\n        \"The node requires between 2-5 inputs. The actual input size is \" << inputs.size() << \".\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n\n    // Input: boxes\n    nvinfer1::ITensor* boxesTensorPtr = &convertToTensor(inputs.at(0), ctx);\n    ONNXTRT_CHECK_NODE(boxesTensorPtr->getDimensions().nbDims == 3,\n        \"The boxes tensor must be 3D. The actual rank is \" << boxesTensorPtr->getDimensions().nbDims << \".\", node,\n        nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    // Input: scores\n    nvinfer1::ITensor* scoresTensorPtr = &convertToTensor(inputs.at(1), ctx);\n    ONNXTRT_CHECK_NODE(scoresTensorPtr->getDimensions().nbDims == 3,\n        \"The scores tensor must be 3D. The actual rank is \" << scoresTensorPtr->getDimensions().nbDims << \".\", node,\n        nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    int32_t const maxOutputBoxesPerClassDefault = 0;\n    nvinfer1::ITensor* maxOutputBoxesPerClassTensorPtr = nullptr;\n    nvinfer1::ITensor* iouThresholdTensorPtr = nullptr;\n    nvinfer1::ITensor* scoreThresholdTensorPtr = nullptr;\n\n    // Input: max_output_boxes_per_class (default = 0)\n    if (inputs.size() >= 3 && !inputs.at(2).isNullTensor())\n    {\n        maxOutputBoxesPerClassTensorPtr = convertToScalar(inputs.at(2), ctx);\n        // Consider when user chooses int64 max as input, which is reasonable. We need to convert it to int32 max first.\n        nvinfer1::ITensor* int32Max = addConstantScalar(\n            ctx, static_cast<int64_t>(std::numeric_limits<int32_t>::max()), ::ONNX_NAMESPACE::TensorProto::INT64)\n                                          ->getOutput(0);\n        maxOutputBoxesPerClassTensorPtr\n            = ctx->network()\n                  ->addElementWise(*maxOutputBoxesPerClassTensorPtr, *int32Max, nvinfer1::ElementWiseOperation::kMIN)\n                  ->getOutput(0);\n        maxOutputBoxesPerClassTensorPtr = castHelper(ctx, maxOutputBoxesPerClassTensorPtr, DataType::kINT32);\n        ONNXTRT_CHECK_NODE(maxOutputBoxesPerClassTensorPtr != nullptr,\n            \"The max_output_boxes_per_class tensor must be 0D\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    }\n    else\n    {\n        auto* constantLayer = N_CHECK(ctx->network()->addConstant(\n            nvinfer1::Dims{0, {}}, nvinfer1::Weights{DataType::kINT32, &maxOutputBoxesPerClassDefault, 1}));\n        ONNXTRT_CHECK_NODE(constantLayer != nullptr, \"Failed to add in constant for default max_output_boxes_per_class\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        maxOutputBoxesPerClassTensorPtr = N_CHECK(constantLayer->getOutput(0));\n    }\n\n    // Input: iou_threshold (default = 0)\n    if (inputs.size() >= 4 && !inputs.at(3).isNullTensor())\n    {\n        iouThresholdTensorPtr = convertToScalar(inputs.at(3), ctx);\n        ONNXTRT_CHECK_NODE(iouThresholdTensorPtr != nullptr, \"The iou_threshold tensor must be 0D\", node, nodeIdx,\n            ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    // Input: score_threshold (default = 0)\n    if (inputs.size() >= 5 && !inputs.at(4).isNullTensor())\n    {\n        scoreThresholdTensorPtr = convertToScalar(inputs.at(4), ctx);\n        ONNXTRT_CHECK_NODE(scoreThresholdTensorPtr != nullptr, \"The score_threshold tensor must be 0D\", node, nodeIdx,\n            ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    // Transpose scores tensor from [batch, classes, bounding_boxes] to [batch, bounding_boxes, classes]\n    nvinfer1::Permutation perm{0, 2, 1};\n    nvinfer1::ITensor* transposedScoresTensorPtr = transposeTensor(ctx, node, *scoresTensorPtr, perm);\n    ONNXTRT_CHECK_NODE(transposedScoresTensorPtr, \"Failed to transpose the scores input.\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n\n    // Create the NMS layer\n    auto* layer = N_CHECK(\n        ctx->network()->addNMS(*boxesTensorPtr, *transposedScoresTensorPtr, *maxOutputBoxesPerClassTensorPtr));\n    ctx->registerLayer(layer, node);\n\n    // Handle the optional threshold inputs\n    if (iouThresholdTensorPtr != nullptr)\n    {\n        layer->setInput(3, *iouThresholdTensorPtr);\n    }\n    if (scoreThresholdTensorPtr != nullptr)\n    {\n        layer->setInput(4, *scoreThresholdTensorPtr);\n    }\n\n    // Attribute: center_point_box (default = 0)\n    int32_t const centerPointBox = OnnxAttrs{node, ctx}.get(\"center_point_box\", 0);\n    nvinfer1::BoundingBoxFormat fmt;\n    switch (centerPointBox)\n    {\n    case 0: fmt = nvinfer1::BoundingBoxFormat::kCORNER_PAIRS; break;\n    case 1: fmt = nvinfer1::BoundingBoxFormat::kCENTER_SIZES; break;\n    default:\n        ONNXTRT_CHECK_NODE(false, \"Invalid value provided for the center_point_box attribute\", node, nodeIdx,\n            ErrorCode::kUNSUPPORTED_NODE_ATTR);\n    }\n    layer->setBoundingBoxFormat(fmt);\n    auto* indices = N_CHECK(layer->getOutput(0));\n    indices = castHelper(ctx, indices, DataType::kINT64);\n\n    return {{indices}};\n};\n\nDEFINE_BUILTIN_OP_IMPORTER(Not)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kNOT);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(OneHot)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    checkNotInvalidType(inputs.at(1), {\"UINT8\"}, node, nodeIdx);\n    checkNotInvalidType(inputs.at(2), {\"UINT8\"}, node, nodeIdx);\n    ONNXTRT_CHECK_NODE(node.input_size(),\n        \"OneHot must have exactly 3 inputs. Number of inputs = \" << node.input_size() << \".\", node, nodeIdx,\n        ErrorCode::kINVALID_NODE);\n\n    nvinfer1::ITensor* values = &convertToTensor(inputs.at(2), ctx);\n\n    nvinfer1::ITensor* indices = &convertToTensor(inputs.at(0), ctx);\n    if (!inputs.at(0).isInt32())\n    {\n        indices = castHelper(ctx, indices, DataType::kINT32);\n    }\n    nvinfer1::ITensor* depth = &convertToTensor(inputs.at(1), ctx); // tensor #1 in ONNX\n    if (!inputs.at(1).isInt32())\n    {\n        depth = castHelper(ctx, depth, DataType::kINT32);\n    }\n    depth = convertToScalar(ctx, depth);\n    ONNXTRT_CHECK_NODE(depth, \"Failed to convert the depth to a scalar.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    OnnxAttrs attrs(node, ctx);\n    auto axis = attrs.get<int32_t>(\"axis\", -1);\n    auto nbDims = indices->getDimensions().nbDims;\n    convertAxis(axis, nbDims + 1, node, nodeIdx);\n\n    auto* layer = N_CHECK(ctx->network()->addOneHot(*indices, *values, *depth, axis));\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Or)\n{\n    return elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kOR);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Pad)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    nvinfer1::ITensor* tensorPtr = &convertToTensor(inputs.at(0), ctx);\n    int32_t const nbDims = tensorPtr->getDimensions().nbDims;\n    ShapeTensor const tensorDims = shapeOf(*tensorPtr);\n\n    OnnxAttrs attrs(node, ctx);\n    auto const mode = attrs.get<std::string>(\"mode\", \"constant\");\n    float value{0.F};\n    nvinfer1::ITensor* valuePtr = nullptr;\n    std::vector<int64_t> onnxPadding;\n\n    if (ctx->getOpsetVersion() < 11)\n    {\n        value = attrs.get<float>(\"value\", 0.F);\n        auto padding = attrs.get<std::vector<int32_t>>(\"pads\");\n        onnxPadding = std::vector<int64_t>(padding.begin(), padding.end());\n        if (onnxPadding.empty())\n        {\n            LOG_VERBOSE(\"Found no-op pad in node: \" + getNodeName(node));\n            RETURN_IDENTITY(inputs.at(0), node, nodeIdx);\n        }\n    }\n    else\n    {\n        // In opset >= 11, padding indicies and values moved from attributes to inputs\n        if (inputs.at(1).is_weights())\n        {\n            weightsToVector<int64_t>(inputs.at(1).weights(), &onnxPadding);\n        }\n        if (inputs.size() >= 3 && !inputs.at(2).isNullTensor())\n        {\n            bool isValueSet = false;\n            if (inputs.at(2).is_weights())\n            {\n                auto const padWeight = inputs.at(2).weights();\n                ONNXTRT_CHECK_NODE((padWeight.count() == 1), \"The input constant_value is required to be a scalar.\",\n                    node, nodeIdx, ErrorCode::kINVALID_NODE);\n                switch (padWeight.type)\n                {\n                case ::ONNX_NAMESPACE::TensorProto::FLOAT:\n                    value = static_cast<float const*>(padWeight.values)[0];\n                    isValueSet = true;\n                    break;\n                case ::ONNX_NAMESPACE::TensorProto::FLOAT16:\n                    value = static_cast<half_float::half const*>(padWeight.values)[0];\n                    isValueSet = true;\n                    break;\n                case ::ONNX_NAMESPACE::TensorProto::BFLOAT16:\n                    value = static_cast<BFloat16 const*>(padWeight.values)[0];\n                    isValueSet = true;\n                    break;\n                default:\n                    // we use trt constant layer to do the data type convertion\n                    break;\n                }\n            }\n            if (!isValueSet)\n            {\n                valuePtr = &convertToTensor(inputs.at(2), ctx);\n            }\n        }\n    }\n\n    auto padAxes\n        = inputs.size() == 4 && !inputs.at(3).isNullTensor() ? ShapeTensor(ctx, inputs.at(3)) : iotaShapeVector(nbDims);\n\n    ShapeTensor beginPads;\n    ShapeTensor endPads;\n    int32_t const padAxesSize = padAxes.size();\n    if (!onnxPadding.empty() && padAxes.allValuesKnown())\n    {\n        // The pads is from initializer or attributes.\n        // Passthrough path for no-op padding.\n        if (std::all_of(onnxPadding.begin(), onnxPadding.end(), [](int64_t i) { return i == 0; }))\n        {\n            LOG_VERBOSE(\"Found no-op pad in node: \" + getNodeName(node));\n            RETURN_IDENTITY(inputs.at(0), node, nodeIdx);\n        }\n\n        // Sanity check.\n        ONNXTRT_CHECK_NODE(static_cast<int32_t>(onnxPadding.size()) == padAxesSize * 2,\n            \"Length of pads input must be twice the length of axes input.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n        std::vector<int64_t> beginPadsVec(onnxPadding.begin(), onnxPadding.begin() + padAxesSize);\n        std::vector<int64_t> endPadsVec(onnxPadding.begin() + padAxesSize, onnxPadding.end());\n        beginPads = ShapeTensor(1, std::move(beginPadsVec));\n        endPads = ShapeTensor(1, std::move(endPadsVec));\n    }\n    else\n    {\n        nvinfer1::ITensor* onnxPaddingPtr = &convertToTensor(inputs.at(1), ctx);\n        ONNXTRT_CHECK_NODE((onnxPaddingPtr->getDimensions().nbDims == 1),\n            \"The padding input must be 1D. The rank of padding input = \" << onnxPaddingPtr->getDimensions().nbDims\n                                                                         << \".\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        ONNXTRT_CHECK_NODE(onnxPaddingPtr->getDimensions().d[0] == padAxesSize * 2,\n            \"pads should be twice the length of input axes i.e. \"\n                << 2 * padAxesSize << \", actual length is: \" << onnxPaddingPtr->getDimensions().d[0],\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n        // onnxPaddingPtr is of the format [x1_begin, x2_begin, ..., x1_end, x2_end,...].\n        ShapeTensor const paddingLen = gather(ctx, shapeOf(*onnxPaddingPtr), shapeVector(0));\n        ShapeTensor const halfPaddingLen = floorDiv(ctx, paddingLen, shapeVector(2));\n        // Obtain begins [x1_begin, x2_begin, ...,].\n        nvinfer1::ISliceLayer* beginSliceLayer\n            = addSlice(ctx, *onnxPaddingPtr, shapeVector(0), halfPaddingLen, shapeVector(1));\n        ctx->registerLayer(beginSliceLayer, node);\n        beginPads = ShapeTensor{*(beginSliceLayer->getOutput(0))};\n        // Obtain ends [x1_end, x2_end, ...].\n        nvinfer1::ISliceLayer* endSliceLayer\n            = addSlice(ctx, *onnxPaddingPtr, halfPaddingLen, halfPaddingLen, shapeVector(1));\n        ctx->registerLayer(endSliceLayer, node);\n        endPads = ShapeTensor{*(endSliceLayer->getOutput(0))};\n    }\n\n    if (padAxes.allValuesKnown())\n    {\n        // gather() requires indices to be normalized if their values are known\n        normalizeAxes(padAxes, nbDims);\n    }\n    auto axesDims = gather(ctx, tensorDims, padAxes);\n    ShapeTensor const zeros = similar(ctx, beginPads, 0);\n    ShapeTensor start = sub(ctx, zeros, beginPads);\n    ShapeTensor size = add(ctx, axesDims, add(ctx, beginPads, endPads));\n    ShapeTensor const stride = similar(ctx, start, 1);\n\n    auto* layer = N_CHECK(addSlice(ctx, *tensorPtr, start, size, stride));\n    ONNXTRT_CHECK_NODE(layer, \"Could not create padding layer\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    if (padAxes.allValuesKnown())\n    {\n        layer->setAxes(shapeTensorToDims(padAxes, \"slice axes\", -nbDims, nbDims - 1));\n    }\n    else\n    {\n        layer->setInput(5, convertToTensor(inputs.at(3), ctx));\n    }\n    if (mode == \"constant\")\n    {\n        layer->setMode(nvinfer1::SampleMode::kFILL);\n\n        if (valuePtr)\n        {\n            layer->setInput(4, *valuePtr);\n        }\n        else if (value != 0.F)\n        {\n            // constant_value must have the same data type as the input tensor\n            nvinfer1::ITensor* fillValue = nullptr;\n            switch (tensorPtr->getType())\n            {\n            case DataType::kHALF:\n                fillValue = addConstantScalar(\n                    ctx, static_cast<half_float::half>(value), ::ONNX_NAMESPACE::TensorProto::FLOAT16)\n                                ->getOutput(0);\n                break;\n            case DataType::kBF16:\n                fillValue\n                    = addConstantScalar(ctx, static_cast<BFloat16>(value), ::ONNX_NAMESPACE::TensorProto::BFLOAT16)\n                          ->getOutput(0);\n                break;\n            case DataType::kFLOAT:\n            case DataType::kINT8:\n                fillValue = addConstantScalar(ctx, value, ::ONNX_NAMESPACE::TensorProto::FLOAT)->getOutput(0);\n                break;\n            default:\n                fillValue = addConstantScalar(ctx, static_cast<int32_t>(value), ::ONNX_NAMESPACE::TensorProto::INT32)\n                                ->getOutput(0);\n                break;\n            }\n            ONNXTRT_CHECK_NODE(\n                fillValue, \"Could not create layer for constant_value\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n            layer->setInput(4, *fillValue);\n        }\n    }\n    else if (mode == \"reflect\")\n    {\n        layer->setMode(nvinfer1::SampleMode::kREFLECT);\n    }\n    else if (mode == \"edge\")\n    {\n        layer->setMode(nvinfer1::SampleMode::kCLAMP);\n    }\n    else if (mode == \"wrap\")\n    {\n        layer->setMode(nvinfer1::SampleMode::kWRAP);\n    }\n    else\n    {\n        ONNXTRT_THROW(MAKE_ERROR(\"Unsupported pad mode\", ErrorCode::kUNSUPPORTED_NODE));\n    }\n\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(ParametricSoftplus)\n{\n    OnnxAttrs attrs(node, ctx);\n    float alpha = attrs.get<float>(\"alpha\");\n    float beta = attrs.get<float>(\"beta\");\n    return activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kSOFTPLUS, &alpha, &beta);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Pow)\n{\n    return elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kPOW);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(PRelu)\n{\n    checkNotInvalidType(inputs.at(0), {\"INT32\", \"INT64\"}, node, nodeIdx);\n    checkNotInvalidType(inputs.at(1), {\"INT32\", \"INT64\"}, node, nodeIdx);\n    ONNXTRT_CHECK_NODE((inputs.size() == 2),\n        \"The PRelu operator requires exactly 2 inputs. Current input size = \" << inputs.size() << \".\", node, nodeIdx,\n        ErrorCode::kINVALID_NODE);\n    nvinfer1::ITensor* input = &convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor* slopes = &convertToTensor(inputs.at(1), ctx);\n    broadcastTensors(ctx, input, slopes);\n    auto* layer = N_CHECK(ctx->network()->addParametricReLU(*input, *slopes));\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nNodeOutputs randomHelper(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t nodeIdx,\n    ShapeTensor const& inputShape, OnnxAttrs const& attrs, DataType const& inputDType, nvinfer1::FillOperation op)\n{\n    auto* fillLayer = addFill(ctx, inputShape, op);\n    ONNXTRT_CHECK_NODE(fillLayer, \"Failed to create layer\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    ctx->registerLayer(fillLayer, node);\n\n    bool const isUniform = op == nvinfer1::FillOperation::kRANDOM_UNIFORM;\n\n    // Set datatype of output:\n    //      RandomUniform / RandomNormal: dtype is required and defaults to 1\n    //      RandomUniformLike / RandomNormalLike: dtype is optional and defaults to the same type as the input\n    if (attrs.count(\"dtype\"))\n    {\n        auto dtype = attrs.get<int32_t>(\"dtype\", 1);\n        switch (dtype)\n        {\n        case ::ONNX_NAMESPACE::TensorProto::FLOAT: fillLayer->setToType(DataType::kFLOAT); break;\n        case ::ONNX_NAMESPACE::TensorProto::FLOAT16: fillLayer->setToType(DataType::kHALF); break;\n        default: ONNXTRT_THROW(MAKE_ERROR(\"Unsupported data type\", ErrorCode::kINVALID_VALUE));\n        }\n    }\n    else\n    {\n        fillLayer->setToType(inputDType);\n    }\n\n    std::string const alphaName = isUniform ? \"low\" : \"mean\";\n    std::string const betaName = isUniform ? \"high\" : \"scale\";\n    auto alpha = attrs.get<float>(alphaName, 0.F);\n    auto beta = attrs.get<float>(betaName, 1.F);\n\n    fillLayer->setAlpha(alpha);\n    fillLayer->setBeta(beta);\n\n    // TensorRT does not support \"seed\" field now. The support will be added in future versions.\n    if (attrs.count(\"seed\"))\n    {\n        LOG_WARNING(\n            \"TensorRT currently ignores the \\\"seed\\\" field in RandomUniform or RandomNormal op. Random seeds will be \"\n            \"used.\");\n    }\n\n    RETURN_FIRST_OUTPUT(fillLayer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(RandomUniform)\n{\n    OnnxAttrs attrs(node, ctx);\n    auto const shapeAsIntList = attrs.get<std::vector<int64_t>>(\"shape\");\n    ShapeTensor const inputShape{1, std::vector<int64_t>(shapeAsIntList.begin(), shapeAsIntList.end())};\n\n    return randomHelper(\n        ctx, node, nodeIdx, inputShape, attrs, DataType::kFLOAT, nvinfer1::FillOperation::kRANDOM_UNIFORM);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(RandomUniformLike)\n{\n    ONNXTRT_CHECK_NODE((inputs.size() == 1),\n        \"The RandomUniformLike operator requires exactly 1 input. Current input size = \" << inputs.size() << \".\", node,\n        nodeIdx, ErrorCode::kINVALID_NODE);\n    ONNXTRT_CHECK_NODE((inputs.at(0).is_tensor()), \"The input tensor cannot be an initializer.\", node, nodeIdx,\n        nvonnxparser::ErrorCode::kUNSUPPORTED_NODE);\n    auto& input = inputs.at(0).tensor();\n    auto const inputShape = shapeOf(input);\n    OnnxAttrs const attrs(node, ctx);\n    auto const dType = input.getType();\n\n    return randomHelper(ctx, node, nodeIdx, inputShape, attrs, dType, nvinfer1::FillOperation::kRANDOM_UNIFORM);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(RandomNormal)\n{\n    OnnxAttrs attrs(node, ctx);\n    auto const shapeAsIntList = attrs.get<std::vector<int64_t>>(\"shape\");\n    ShapeTensor const inputShape{1, std::vector<int64_t>(shapeAsIntList.begin(), shapeAsIntList.end())};\n\n    return randomHelper(\n        ctx, node, nodeIdx, inputShape, attrs, DataType::kFLOAT, nvinfer1::FillOperation::kRANDOM_NORMAL);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(RandomNormalLike)\n{\n    ONNXTRT_CHECK_NODE((inputs.size() == 1),\n        \"The RandomNormalLike operator requires exactly 1 input. Current input size = \" << inputs.size() << \".\", node,\n        nodeIdx, ErrorCode::kINVALID_NODE);\n    ONNXTRT_CHECK_NODE((inputs.at(0).is_tensor()), \"The input tensor cannot be an initializer.\", node, nodeIdx,\n        nvonnxparser::ErrorCode::kUNSUPPORTED_NODE);\n    auto& input = inputs.at(0).tensor();\n    auto const inputShape = shapeOf(input);\n    OnnxAttrs const attrs(node, ctx);\n    auto const dType = input.getType();\n\n    return randomHelper(ctx, node, nodeIdx, inputShape, attrs, dType, nvinfer1::FillOperation::kRANDOM_NORMAL);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Range)\n{\n    ONNXTRT_CHECK_NODE(\n        (inputs.at(0).getType() == inputs.at(1).getType() && inputs.at(0).getType() == inputs.at(2).getType()),\n        \"For range operator types for start, limit, and delta must be identical. Type of start = \"\n            + inputs.at(0).getType() + \", type of limit = \" + inputs.at(1).getType()\n            + \", type of delta = \" + inputs.at(2).getType() + \".\",\n        node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    bool const isInt32 = inputs.at(0).isInt32();\n    bool const isInt64 = inputs.at(0).isInt64();\n    bool const isFp32 = inputs.at(0).isFp32();\n    ONNXTRT_CHECK_NODE((isInt32 || isInt64 || isFp32),\n        \"This version of TensorRT only supports int32, int64, and float input types for Range!\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n\n    // \"start : T\n    //     Scalar. First entry for the range of output values.\n    //  limit : T\n    //     Scalar. Exclusive upper limit for the range of output values.\n    //  delta : T\n    //     Scalar. Value to step by.\"\n    ShapeTensor start{};\n    ShapeTensor limit{};\n    ShapeTensor delta{};\n    if (isFp32)\n    {\n        start = ShapeTensor{ctx, inputs.at(0)};\n        limit = ShapeTensor{ctx, inputs.at(1)};\n        delta = ShapeTensor{ctx, inputs.at(2)};\n    }\n    else\n    {\n        nvinfer1::ITensor* input0 = castHelper(ctx, &convertToTensor(inputs.at(0), ctx), DataType::kINT64);\n        nvinfer1::ITensor* input1 = castHelper(ctx, &convertToTensor(inputs.at(1), ctx), DataType::kINT64);\n        nvinfer1::ITensor* input2 = castHelper(ctx, &convertToTensor(inputs.at(2), ctx), DataType::kINT64);\n        start = ShapeTensor{*input0};\n        limit = ShapeTensor{*input1};\n        delta = ShapeTensor{*input2};\n    }\n\n    // \"number_of_elements = max( ceil( (limit - start) / delta ) , 0 )\"\n    //\n    // To implement this in TensorRT using only operations allowed on\n    // shape tensors, rewrite as:\n    //      \"number_of_elements = max(0 - floor((start - limit) / delta), 0)\n    //\n    ShapeTensor zero{};\n    ShapeTensor fQuotient{};\n    ShapeTensor quotient{};\n    ShapeTensor numberOfElements{};\n\n    zero = shapeScalar(0);\n    fQuotient = floorDiv(ctx, sub(ctx, start, limit), delta);\n    quotient = (isFp32 || isInt32) ? castToInt64(ctx, fQuotient) : fQuotient;\n    numberOfElements = max(ctx, sub(ctx, zero, quotient), zero);\n\n    nvinfer1::IFillLayer* layer = addFill(ctx, convertTo1D(ctx, numberOfElements), nvinfer1::FillOperation::kLINSPACE);\n    ONNXTRT_CHECK_NODE(layer, \"Failed to create layer\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    ctx->registerLayer(layer, node);\n\n    // TensorRT requires that alpha and beta both be dynamic or both be static.\n    if (start.allValuesKnown() && delta.allValuesKnown() && !isInt64)\n    {\n        layer->setAlpha(start[0]);\n        layer->setBeta(delta[0]);\n    }\n    else if (inputs.at(0).is_weights() && inputs.at(2).is_weights() && isInt32)\n    {\n        // For constant int32 start and delta, we can set to layer params directly.\n        // This might not be required if TRT-20829 is done.\n        ONNXTRT_CHECK_NODE(inputs.at(0).weights().count() == 1, \"Start must only be a single value!\", node, nodeIdx,\n            ErrorCode::kINVALID_NODE);\n        ONNXTRT_CHECK_NODE(inputs.at(2).weights().count() == 1, \"Delta must only be a single value!\", node, nodeIdx,\n            ErrorCode::kINVALID_NODE);\n        layer->setAlpha(inputs.at(0).weights().at<int32_t>(0));\n        layer->setBeta(inputs.at(2).weights().at<int32_t>(0));\n    }\n    else\n    {\n        layer->setInput(1, start.tensor(ctx));\n        auto* delta1D = &convertTo1D(ctx, delta).tensor(ctx);\n        layer->setInput(2, *delta1D);\n    }\n\n    if (isInt32)\n    {\n        layer->setToType(DataType::kINT32);\n    }\n    else if (isInt64)\n    {\n        layer->setToType(DataType::kINT64);\n    }\n\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Reciprocal)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kRECIP);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(ReduceL1)\n{\n    if (IsReduceNoOp(ctx, node, inputs))\n    {\n        RETURN_IDENTITY(inputs.at(0), node, nodeIdx);\n    }\n    std::vector<TensorOrWeights> absResult\n        = unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kABS);\n\n    return reduceTensor(ctx, node, nodeIdx, absResult.at(0), nvinfer1::ReduceOperation::kSUM,\n        inputs.size() >= 2 ? inputs.at(1) : TensorOrWeights());\n}\nDECLARE_BUILTIN_OP_IMPORTER(ReduceSum);\nDEFINE_BUILTIN_OP_IMPORTER(ReduceLogSum)\n{\n    if (IsReduceNoOp(ctx, node, inputs))\n    {\n        RETURN_IDENTITY(inputs.at(0), node, nodeIdx);\n    }\n\n    auto sum_result = importReduceSum(ctx, node, nodeIdx, inputs);\n    TensorOrWeights sum_input = sum_result.at(0);\n    return unaryHelper(ctx, node, nodeIdx, sum_input, nvinfer1::UnaryOperation::kLOG);\n}\nDEFINE_BUILTIN_OP_IMPORTER(ReduceLogSumExp)\n{\n    if (IsReduceNoOp(ctx, node, inputs))\n    {\n        RETURN_IDENTITY(inputs.at(0), node, nodeIdx);\n    }\n\n    std::vector<TensorOrWeights> expResult\n        = unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kEXP);\n\n    return importReduceLogSum(ctx, node, nodeIdx, expResult);\n}\nDECLARE_BUILTIN_OP_IMPORTER(ReduceSumSquare);\nDEFINE_BUILTIN_OP_IMPORTER(ReduceL2)\n{\n    if (IsReduceNoOp(ctx, node, inputs))\n    {\n        RETURN_IDENTITY(inputs.at(0), node, nodeIdx);\n    }\n\n    auto sum_sqr_result = importReduceSumSquare(ctx, node, nodeIdx, inputs);\n    TensorOrWeights sum_sqr = sum_sqr_result.at(0);\n    return unaryHelper(ctx, node, nodeIdx, sum_sqr, nvinfer1::UnaryOperation::kSQRT);\n}\nDEFINE_BUILTIN_OP_IMPORTER(ReduceMax)\n{\n    return reduceTensor(ctx, node, nodeIdx, inputs.at(0), nvinfer1::ReduceOperation::kMAX,\n        inputs.size() >= 2 ? inputs.at(1) : TensorOrWeights());\n}\nDEFINE_BUILTIN_OP_IMPORTER(ReduceMean)\n{\n    return reduceTensor(ctx, node, nodeIdx, inputs.at(0), nvinfer1::ReduceOperation::kAVG,\n        inputs.size() >= 2 ? inputs.at(1) : TensorOrWeights());\n}\nDEFINE_BUILTIN_OP_IMPORTER(ReduceMin)\n{\n    return reduceTensor(ctx, node, nodeIdx, inputs.at(0), nvinfer1::ReduceOperation::kMIN,\n        inputs.size() >= 2 ? inputs.at(1) : TensorOrWeights());\n}\nDEFINE_BUILTIN_OP_IMPORTER(ReduceProd)\n{\n    return reduceTensor(ctx, node, nodeIdx, inputs.at(0), nvinfer1::ReduceOperation::kPROD,\n        inputs.size() >= 2 ? inputs.at(1) : TensorOrWeights());\n}\nDEFINE_BUILTIN_OP_IMPORTER(ReduceSum)\n{\n    return reduceTensor(ctx, node, nodeIdx, inputs.at(0), nvinfer1::ReduceOperation::kSUM,\n        inputs.size() >= 2 ? inputs.at(1) : TensorOrWeights());\n}\nDEFINE_BUILTIN_OP_IMPORTER(ReduceSumSquare)\n{\n    if (IsReduceNoOp(ctx, node, inputs))\n    {\n        RETURN_IDENTITY(inputs.at(0), node, nodeIdx);\n    }\n\n    nvinfer1::ITensor& tensor = convertToTensor(inputs.at(0), ctx);\n    auto* sqr_layer = N_CHECK(ctx->network()->addElementWise(tensor, tensor, nvinfer1::ElementWiseOperation::kPROD));\n    nvinfer1::ITensor* sqr_tensorPtr = N_CHECK(sqr_layer->getOutput(0));\n    return reduceTensor(ctx, node, nodeIdx, sqr_tensorPtr, nvinfer1::ReduceOperation::kSUM,\n        inputs.size() >= 2 ? inputs.at(1) : TensorOrWeights());\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Relu)\n{\n    return activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kRELU);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Sign)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kSIGN);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Round)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kROUND);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Resize)\n{\n    checkNotInvalidType(inputs.at(0), {\"BOOL\", \"UINT8\"}, node, nodeIdx);\n    nvinfer1::ITensor& input = convertToTensor(inputs.at(0), ctx);\n    int32_t const inputRank = input.getDimensions().nbDims;\n    ShapeTensor const inputDims = shapeOf(input);\n    ONNXTRT_CHECK_NODE(\n        (inputRank > 0), \"The input tensor cannot be a scalar.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    // Add resize layer\n    nvinfer1::IResizeLayer* layer = N_CHECK(ctx->network()->addResize(input));\n    ctx->registerLayer(layer, node);\n    OnnxAttrs attrs(node, ctx);\n\n    auto mode = attrs.get<std::string>(\"mode\", \"nearest\");\n    auto interpolationMode = nvinfer1::InterpolationMode::kNEAREST;\n\n    if (mode == \"cubic\")\n    {\n        interpolationMode = nvinfer1::InterpolationMode::kCUBIC;\n    }\n    else if (mode == \"linear\")\n    {\n        interpolationMode = nvinfer1::InterpolationMode::kLINEAR;\n    }\n\n    // Obtain axes, if provided. Axes must be unique and in the range [-inputRank, inputRank-1].\n    auto resizeAxes = attrs.get<std::vector<int32_t>>(\"axes\", std::vector<int32_t>());\n    bool isCompleteIota = (static_cast<int32_t>(resizeAxes.size()) == inputRank);\n    int32_t counter = 0;\n    for (int32_t& axis : resizeAxes)\n    {\n        convertAxis(axis, inputRank, node, nodeIdx);\n        isCompleteIota &= (axis == counter++);\n    }\n    bool const axesInterlacingNeeded = !resizeAxes.empty() && !isCompleteIota;\n    // Note: This check is done after the conversion of axes to be in range [0, inputRank - 1] to make sure there are no\n    // duplicates.\n    ONNXTRT_CHECK_NODE(std::unordered_set<int32_t>(resizeAxes.begin(), resizeAxes.end()).size() == resizeAxes.size(),\n        \"The input axes must have unique elements.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    // set transformation\n    std::string transformationMode = \"half_pixel\";\n\n    layer->setSelectorForSinglePixel(nvinfer1::ResizeSelector::kFORMULA);\n    layer->setNearestRounding(nvinfer1::ResizeRoundMode::kHALF_DOWN);\n    if (ctx->getOpsetVersion() >= 11)\n    {\n        // Check for TRT-supported resize attributes\n        transformationMode = attrs.get<std::string>(\"coordinate_transformation_mode\", \"half_pixel\");\n        auto nearest_mode = attrs.get<std::string>(\"nearest_mode\", \"round_prefer_floor\");\n\n        // clang-format off\n        // The existence of a fourth input means a shape was passed as the resize parameter\n        // For ONNX resize with the \"sizes\", TensorRT's resize maps to ONNX's in the following ways:\n        // Nearest&Linear&Cubic:\n        //     align_corners        -> ResizeCoordinateTransformation::kALIGN_CORNERS ResizeSelector::kFORMULA ResizeRoundMode::kFLOOR\n        //     half_pixel           -> ResizeCoordinateTransformation::kHALF_PIXEL    ResizeSelector::kFORMULA ResizeRoundMode::kFLOOR\n        //     asymmetric           -> ResizeCoordinateTransformation::kASYMMETRIC    ResizeSelector::kFORMULA ResizeRoundMode::kFLOOR\n        //     pytorch_half_pixel   -> ResizeCoordinateTransformation::kHALF_PIXEL    ResizeSelector::kUPPER   ResizeRoundMode::kFLOOR\n        //     tf_half_pixel_for_nn -> ResizeCoordinateTransformation::kHALF_PIXEL    ResizeSelector::kFORMULA ResizeRoundMode::kFLOOR\n        // clang-format on\n\n        if (transformationMode == \"align_corners\")\n        {\n            layer->setCoordinateTransformation(nvinfer1::ResizeCoordinateTransformation::kALIGN_CORNERS);\n        }\n        else if (transformationMode == \"tf_half_pixel_for_nn\")\n        {\n            // `tf_half_pixel_for_nn` has been deprecated after Resize-11. Leaving it as is for backward compatibility.\n            layer->setNearestRounding(nvinfer1::ResizeRoundMode::kCEIL);\n            layer->setCoordinateTransformation(nvinfer1::ResizeCoordinateTransformation::kHALF_PIXEL);\n        }\n        else if (transformationMode == \"pytorch_half_pixel\")\n        {\n            layer->setSelectorForSinglePixel(nvinfer1::ResizeSelector::kUPPER);\n            layer->setCoordinateTransformation(nvinfer1::ResizeCoordinateTransformation::kHALF_PIXEL);\n        }\n        else if (transformationMode == \"half_pixel\")\n        {\n            layer->setCoordinateTransformation(nvinfer1::ResizeCoordinateTransformation::kHALF_PIXEL);\n        }\n        else if (transformationMode == \"asymmetric\")\n        {\n            layer->setCoordinateTransformation(nvinfer1::ResizeCoordinateTransformation::kASYMMETRIC);\n        }\n        else\n        {\n            // NOTE: Currently `half_pixel_symmetric` and `tf_crop_and_resize` are not supported. `extrapolation_value`\n            // attribute and `roi` input are relevant only for `tf_crop_and_resize` and hence, aren't supported.\n            ONNXTRT_CHECK_NODE(false, \"Unsupported coordinate transformation mode \" << transformationMode, node,\n                nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        }\n\n        if (transformationMode != \"tf_half_pixel_for_nn\")\n        {\n            if (nearest_mode == \"floor\")\n            {\n                layer->setNearestRounding(nvinfer1::ResizeRoundMode::kFLOOR);\n            }\n            else if (nearest_mode == \"ceil\")\n            {\n                layer->setNearestRounding(nvinfer1::ResizeRoundMode::kCEIL);\n            }\n            else if (nearest_mode == \"round_prefer_floor\")\n            {\n                layer->setNearestRounding(nvinfer1::ResizeRoundMode::kHALF_DOWN);\n            }\n            else if (nearest_mode == \"round_prefer_ceil\")\n            {\n                layer->setNearestRounding(nvinfer1::ResizeRoundMode::kHALF_UP);\n            }\n\n            // set exclude_outside, only support after opset 11.\n            auto excludeOutside = static_cast<bool>(attrs.get<int32_t>(\"exclude_outside\", 0));\n            layer->setExcludeOutside(excludeOutside);\n\n            // set bicubic, only support after opset 11.\n            if (interpolationMode == nvinfer1::InterpolationMode::kCUBIC)\n            {\n                auto cubicCoeff = attrs.get<float>(\"cubic_coeff_a\", -0.75F);\n                layer->setCubicCoeff(cubicCoeff);\n            }\n\n            if (inputs.size() == 4 && !inputs.at(3).isNullTensor())\n            {\n                if (inputs.at(3).is_weights())\n                {\n                    ONNXTRT_CHECK_NODE((inputs.at(3).weights().shape.nbDims == 1),\n                        \"The sizes input must be 1D. Sizes rank = \" << inputs.at(3).weights().shape.nbDims << \".\", node,\n                        nodeIdx, ErrorCode::kINVALID_NODE);\n                    std::vector<int64_t> sizesVec;\n                    weightsToVector<int64_t>(inputs.at(3).weights(), &sizesVec);\n                    if (axesInterlacingNeeded)\n                    {\n                        ONNXTRT_CHECK_NODE(sizesVec.size() == resizeAxes.size(),\n                            \"Length of sizes input must be same as length of axes attribute.\", node, nodeIdx,\n                            ErrorCode::kINVALID_NODE);\n                        std::vector<int64_t> tempVec(inputDims.begin(), inputDims.end());\n                        for (size_t idx = 0; idx < resizeAxes.size(); idx++)\n                        {\n                            int32_t const currAxis = resizeAxes[idx];\n                            tempVec[currAxis] = sizesVec[idx];\n                        }\n                        sizesVec = std::move(tempVec);\n                    }\n                    ONNXTRT_CHECK_NODE((static_cast<int32_t>(sizesVec.size()) == inputRank),\n                        \"The shape of weights must align with input data. Length of sizes = \"\n                            << sizesVec.size() << \", rank of input = \" << inputRank << \".\",\n                        node, nodeIdx, ErrorCode::kINVALID_NODE);\n                    nvinfer1::Dims resizeShape{inputRank, {}};\n                    for (int32_t i = 0; i < inputRank; i++)\n                    {\n                        resizeShape.d[i] = static_cast<int32_t>(sizesVec[i]);\n                    }\n                    layer->setOutputDimensions(resizeShape);\n                }\n                else\n                {\n                    auto* resizeShape = &convertToTensor(inputs.at(3), ctx);\n                    if (axesInterlacingNeeded)\n                    {\n                        ONNXTRT_CHECK_NODE(resizeShape->getDimensions().d[0] == static_cast<int64_t>(resizeAxes.size()),\n                            \"sizes input tensor should be of the same length as axes attribute i.e. \"\n                                << resizeAxes.size() << \", actual length is: \" << resizeShape->getDimensions().d[0],\n                            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n                        std::vector<int64_t> axesLongInt(resizeAxes.begin(), resizeAxes.end());\n                        ShapeTensor const subscripts{\n                            axesToInterlaceSubscripts(ShapeTensor(1, std::move(axesLongInt)), inputRank)};\n                        ShapeTensor const orderedShapeTensor\n                            = interlace(ctx, inputDims, ShapeTensor(*resizeShape), subscripts);\n                        resizeShape = &orderedShapeTensor.tensor(ctx);\n                    }\n                    layer->setInput(1, *resizeShape);\n                }\n                layer->setResizeMode(interpolationMode);\n                RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n            }\n        }\n    }\n    // For opset 10 resize, the only supported mode is asymmetric resize with scales. Nearest resizes use floor\n    // rounding.\n    else\n    {\n        transformationMode = \"asymmetric\";\n        layer->setCoordinateTransformation(nvinfer1::ResizeCoordinateTransformation::kASYMMETRIC);\n        if (mode == \"nearest\")\n        {\n            layer->setNearestRounding(nvinfer1::ResizeRoundMode::kFLOOR);\n        }\n    }\n\n    // Resizes that use scale factors have the same import logic between opsets\n    auto scales = ctx->getOpsetVersion() >= 11 ? inputs.at(2) : inputs.at(1);\n\n    if (scales.is_weights())\n    {\n        // TRT-15340: Remove this and use else path when safety support nbDims == 1.\n        ONNXTRT_CHECK_NODE((scales.weights().shape.nbDims == 1),\n            \"The scales input must be 1D. Scales rank = \" << scales.weights().shape.nbDims << \".\", node, nodeIdx,\n            ErrorCode::kINVALID_NODE);\n        int32_t const scaleSize = scales.weights().shape.d[0];\n\n        std::vector<float> scalesVec;\n        weightsToVector<float>(scales.weights(), &scalesVec);\n        if (axesInterlacingNeeded)\n        {\n            ONNXTRT_CHECK_NODE(scalesVec.size() == resizeAxes.size(),\n                \"Length of scales input must be same as length of axes attribute.\", node, nodeIdx,\n                ErrorCode::kINVALID_NODE);\n            std::vector<float> tempVec(inputRank, 1.0);\n            for (size_t idx = 0; idx < resizeAxes.size(); idx++)\n            {\n                int32_t const currAxis = resizeAxes[idx];\n                tempVec[currAxis] = scalesVec[idx];\n            }\n            // Update scalesVec to hold the ordered information.\n            scalesVec = std::move(tempVec);\n        }\n        ONNXTRT_CHECK_NODE((static_cast<int32_t>(scalesVec.size()) == inputRank),\n            \"The shape of weights must align with input data. Length of scales = \"\n                << scalesVec.size() << \", rank of input = \" << inputRank << \".\",\n            node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n        // check resize dims\n        if (interpolationMode == nvinfer1::InterpolationMode::kLINEAR)\n        {\n            ONNXTRT_CHECK_NODE(canUseNDResize(scaleSize, scalesVec.data(), 3),\n                \"This version of TensorRT only supports linear resizing on the outermost 3 dimensions.\", node, nodeIdx,\n                ErrorCode::kUNSUPPORTED_NODE);\n        }\n        else if (interpolationMode == nvinfer1::InterpolationMode::kCUBIC)\n        {\n            ONNXTRT_CHECK_NODE(canUseNDResize(scaleSize, scalesVec.data(), 2),\n                \"This version of TensorRT only supports cubic resizing on the outermost 2 dimensions.\", node, nodeIdx,\n                ErrorCode::kUNSUPPORTED_NODE);\n        }\n        layer->setScales(scalesVec.data(), inputRank);\n    }\n    else\n    {\n        // Currently, interlacing of `scales` tensor with `axes` is not supported because interlacing needs a\n        // `ShapeTensor` of scales (float values) and a `ShapeTensor` holding float values isn't supported yet.\n        ONNXTRT_CHECK_NODE(!axesInterlacingNeeded,\n            \"Currently, `axes` attribute is supported with `scales` tensor only when it's trivial i.e. it's an \"\n            \"iota vector of same length as input rank.\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        nvinfer1::ITensor* resizeShape = resizeShapeTensor(ctx, input, scales);\n        layer->setInput(1, *resizeShape);\n    }\n\n    layer->setResizeMode(interpolationMode);\n\n    LOG_VERBOSE(\"Running resize layer with: \\n\"\n        << \"Transformation mode: \" << transformationMode << \"\\n\"\n        << \"Resize mode: \" << mode << \"\\n\");\n\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Reshape)\n{\n    // \"data : T\n    // An input tensor\"\n    nvinfer1::ITensor& data = convertToTensor(inputs.at(0), ctx);\n\n    // The attribute allowzero was introduced in opset 14, but as an extension\n    // recognize it for opset >= 5.\n    int32_t allowZero = 0;\n    if (ctx->getOpsetVersion() >= 5)\n    {\n        OnnxAttrs attrs{node, ctx};\n        if (attrs.count(\"allowzero\"))\n        {\n            allowZero = attrs.get<int32_t>(\"allowzero\");\n            if (ctx->getOpsetVersion() < 14)\n            {\n                LOG_WARNING(\n                    getNodeName(node) << \": Using attribute allowzero with opset < 14 is a TensorRT extension.\");\n            }\n        }\n    }\n\n    ShapeTensor shape;\n    if (ctx->getOpsetVersion() >= 5)\n    {\n        // \"shape : tensor(int64)\n        // Specified shape for output.\"\n        shape = ShapeTensor{ctx, inputs.at(1)};\n    }\n    else\n    {\n        // \"Reshape-1\n        // ...\n        // shape : list of ints\n        // New shape\"\n        OnnxAttrs attrs{node, ctx};\n        auto const shapeAsIntList = attrs.get<std::vector<int32_t>>(\"shape\");\n        shape = ShapeTensor(1, std::vector<int64_t>(shapeAsIntList.begin(), shapeAsIntList.end()));\n    }\n\n    // \"A dimension could also be 0, in which case the actual dimension\n    // value is unchanged (i.e. taken from the input tensor).\"\n    nvinfer1::IShuffleLayer* layer = addShuffle(ctx, data, shape, /*zeroIsPlaceholder=*/!allowZero);\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(ReverseSequence)\n{\n    ONNXTRT_CHECK_NODE((inputs.size() == 2),\n        \"ReverseSequence expects two input tensors: input and sequence_lens. Current input size = \" << inputs.size()\n                                                                                                    << \".\",\n        node, nodeIdx, ErrorCode::kINVALID_NODE);\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    nvinfer1::ITensor* input = &convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor* sequenceLens = &convertToTensor(inputs.at(1), ctx);\n    auto const inputDims = input->getDimensions();\n    auto const sequenceLensDims = sequenceLens->getDimensions();\n    ONNXTRT_CHECK_NODE((inputDims.nbDims >= 2),\n        \"Rank of input must be at least two. Current rank of inputs = \" << inputDims.nbDims << \".\", node, nodeIdx,\n        ErrorCode::kINVALID_NODE);\n    ONNXTRT_CHECK_NODE((sequenceLensDims.nbDims == 1),\n        \"Rank of sequence_lens must be one. Current rank of sequence lens = \" << sequenceLensDims.nbDims << \".\", node,\n        nodeIdx, ErrorCode::kINVALID_NODE);\n\n    OnnxAttrs attrs{node, ctx};\n    int32_t const batchAxis = attrs.get<int32_t>(\"batch_axis\", 1);\n    int32_t const sequenceAxis = attrs.get<int32_t>(\"time_axis\", 0);\n    ONNXTRT_CHECK_NODE((batchAxis >= 0 && batchAxis <= inputDims.nbDims), \"Invalid batch_axis\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE_ATTR);\n    ONNXTRT_CHECK_NODE((sequenceAxis >= 0 && sequenceAxis <= inputDims.nbDims), \"Invalid time_axis\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE_ATTR);\n\n    auto layer = N_CHECK(ctx->network()->addReverseSequence(*input, *sequenceLens));\n    ctx->registerLayer(layer, node);\n    ONNXTRT_CHECK_NODE(layer, \"Failed to add ReverseSequence layer.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    layer->setBatchAxis(batchAxis);\n    layer->setSequenceAxis(sequenceAxis);\n\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(RNN)\n{\n    OnnxAttrs attrs{node, ctx};\n\n    const std::string direction = attrs.get<std::string>(\"direction\", \"forward\");\n    const int32_t numDirections = (direction == \"bidirectional\") ? 2 : 1;\n    const int32_t hiddenSize = attrs.get<int32_t>(\"hidden_size\");\n\n    float const clip = attrs.get(\"clip\", -1.f); // Clipping cannot be negative, so -1.0 is a good sentinel value.\n\n    // The input is in SBE format\n    nvinfer1::ITensor* input = &convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor* weights = &convertToTensor(inputs.at(1), ctx);\n    nvinfer1::ITensor* recurrenceWeights = &convertToTensor(inputs.at(2), ctx);\n\n    std::vector<nvinfer1::ActivationType> defaultActs{nvinfer1::ActivationType::kTANH};\n    if (numDirections == 2)\n    {\n        defaultActs.insert(defaultActs.end(), {nvinfer1::ActivationType::kTANH});\n    }\n    std::vector<nvinfer1::ActivationType> activations\n        = attrs.get<std::vector<nvinfer1::ActivationType>>(\"activations\", defaultActs);\n\n    std::vector<float> activationAlphas = attrs.get<std::vector<float>>(\"activation_alpha\", std::vector<float>{});\n    std::transform(activations.begin() + activationAlphas.size(), activations.end(),\n        std::back_inserter(activationAlphas), &getActivationDefaultAlpha);\n\n    std::vector<float> activationBetas = attrs.get<std::vector<float>>(\"activation_beta\", std::vector<float>{});\n    std::transform(activations.begin() + activationBetas.size(), activations.end(), std::back_inserter(activationBetas),\n        &getActivationDefaultBeta);\n\n    // Roll Rb into Wb (and RBb into WBb). Bias is in the form  [Wb[iofc], Rb[iofc], WBb[iofc], RBb[iofc]].\n    // So reshape such that we can perform a reduction to add Wb and Rb.\n    nvinfer1::ITensor* combinedBias{nullptr};\n    if (inputs.size() > 3 && inputs.at(3))\n    {\n        nvinfer1::ITensor* bias = &convertToTensor(inputs.at(3), ctx);\n        LOG_VERBOSE(\"Bias shape is: \" << bias->getDimensions());\n        // Reshape to [[Wb[iofc], Rb[iofc]], [WBb[iofc], RBb[iofc]]]\n        nvinfer1::IShuffleLayer* reshapeBias = N_CHECK(ctx->network()->addShuffle(*bias));\n        reshapeBias->setReshapeDimensions(nvinfer1::Dims3{numDirections, 2, hiddenSize});\n        reshapeBias->setZeroIsPlaceholder(false);\n        auto reshapeBiasOutput = N_CHECK(reshapeBias->getOutput(0));\n        LOG_VERBOSE(\"Reshaping bias to: \" << reshapeBiasOutput->getDimensions());\n        auto reduceLayer\n            = N_CHECK(ctx->network()->addReduce(*reshapeBiasOutput, nvinfer1::ReduceOperation::kSUM, /*axis=*/0b010,\n                /*keepDimensions=*/true));\n        combinedBias = N_CHECK(reduceLayer->getOutput(0));\n        LOG_VERBOSE(\"After reduction, bias shape is: \" << combinedBias->getDimensions());\n    }\n\n    // Get a shape tensor containing: (numDirections, batchSize, hiddenSize)\n    auto const initialStateShape = [&ctx, &numDirections, &hiddenSize, &input]() -> nvinfer1::ITensor* {\n        // Get batchSize from input shape\n        nvinfer1::ITensor* numDirectionsTensor = N_CHECK(\n            addConstantScalar(ctx, numDirections, ::ONNX_NAMESPACE::TensorProto_DataType_INT32, nvinfer1::Dims{1, {1}})\n                ->getOutput(0));\n        LOG_VERBOSE(\"numDirectionsTensor shape: \" << numDirectionsTensor->getDimensions());\n        nvinfer1::ITensor* hiddenSizeTensor = N_CHECK(\n            addConstantScalar(ctx, hiddenSize, ::ONNX_NAMESPACE::TensorProto_DataType_INT32, nvinfer1::Dims{1, {1}})\n                ->getOutput(0));\n        LOG_VERBOSE(\"hiddenSizeTensor shape: \" << hiddenSizeTensor->getDimensions());\n        nvinfer1::ITensor* batchSizeTensor = getAxisLength(ctx, input, 1, nvinfer1::Dims{1, {1}});\n        LOG_VERBOSE(\"batchSizeTensor shape: \" << batchSizeTensor->getDimensions());\n\n        std::array<nvinfer1::ITensor*, 3> tensors{{numDirectionsTensor, batchSizeTensor, hiddenSizeTensor}};\n        nvinfer1::IConcatenationLayer* concatenatedShape = N_CHECK(ctx->network()->addConcatenation(tensors.data(), 3));\n        return N_CHECK(concatenatedShape->getOutput(0));\n    };\n\n    auto const getInitialInputValue\n        = [&ctx, &initialStateShape, &inputs, &node](size_t inputIdx) -> nvinfer1::ITensor* {\n        if (inputs.size() > inputIdx && inputs.at(inputIdx))\n        {\n            return &convertToTensor(inputs.at(inputIdx), ctx);\n        }\n        return constantOfShape(ctx,\n            N_CHECK(addConstantScalar(ctx, 0.f, ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT, nvinfer1::Dims{1, {1}})\n                        ->getOutput(0)),\n            initialStateShape());\n    };\n\n    nvinfer1::ITensor* initialHidden = getInitialInputValue(5);\n    LOG_VERBOSE(\"Initial hidden state shape: \" << initialHidden->getDimensions());\n\n    LOG_VERBOSE(\"Entering Loop\");\n    // Scan over the S dimension of the input\n    auto loop = N_CHECK(ctx->network()->addLoop());\n    nvinfer1::ITensor* tripLimit = getAxisLength(ctx, input, 0);\n    loop->addTripLimit(*tripLimit, nvinfer1::TripLimit::kCOUNT);\n\n    // Add X(t)\n    nvinfer1::ITensor* iterationInput = addRNNInput(ctx, node, loop, inputs, direction);\n    ONNXTRT_CHECK_NODE(iterationInput, \"Failed to add RNN input.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    // H(t-1)\n    nvinfer1::IRecurrenceLayer* hiddenState = loop->addRecurrence(*initialHidden);\n    ctx->registerLayer(hiddenState, node);\n    LOG_VERBOSE(\"Hidden state shape: \" << hiddenState->getOutput(0)->getDimensions());\n\n    // Compute intermediate(t) = (X(t) * W^T + H(t-1) * R^T + (Wb + Rb)).\n    auto xtWTLayer = N_CHECK(ctx->network()->addMatrixMultiply(\n        *iterationInput, nvinfer1::MatrixOperation::kNONE, *weights, nvinfer1::MatrixOperation::kTRANSPOSE));\n    nvinfer1::ITensor* xtWT = N_CHECK(xtWTLayer->getOutput(0));\n    LOG_VERBOSE(\"X(t) * W^T -> \" << xtWT->getDimensions());\n\n    auto ht1RTLayer = N_CHECK(ctx->network()->addMatrixMultiply(*hiddenState->getOutput(0),\n        nvinfer1::MatrixOperation::kNONE, *recurrenceWeights, nvinfer1::MatrixOperation::kTRANSPOSE));\n    nvinfer1::ITensor* ht1RT = N_CHECK(ht1RTLayer->getOutput(0));\n    LOG_VERBOSE(\"H(t-1) * R^T -> \" << ht1RT->getDimensions());\n\n    nvinfer1::ITensor* intermediatet = getElementWiseResult(ctx, *xtWT, *ht1RT, nvinfer1::ElementWiseOperation::kSUM);\n    if (combinedBias)\n    {\n        intermediatet = getElementWiseResult(ctx, *intermediatet, *combinedBias, nvinfer1::ElementWiseOperation::kSUM);\n    }\n\n    // H(t) = f(intermediate(t))\n    nvinfer1::IActivationLayer* hAct\n        = N_CHECK(ctx->network()->addActivation(*addClip(ctx, intermediatet, clip), activations.at(0)));\n    hAct->setAlpha(activationAlphas.at(0));\n    hAct->setBeta(activationBetas.at(0));\n    nvinfer1::ITensor* Ht = N_CHECK(hAct->getOutput(0));\n\n    // singlePassShape = (1, batchSize, hiddenSize)\n    nvinfer1::ITensor* singlePassShape = getElementWiseResult(ctx, *initialStateShape(),\n        *N_CHECK(addConstant(ctx, std::vector<int>{numDirections, 1, 1}, ::ONNX_NAMESPACE::TensorProto_DataType_INT32,\n            nvinfer1::Dims{1,\n                {3}})->getOutput(0)),\n        nvinfer1::ElementWiseOperation::kDIV);\n\n    if (inputs.size() > 4 && inputs.at(4))\n    {\n        nvinfer1::ITensor* seqLens = &convertToTensor(inputs.at(4), ctx);\n        auto maxLen = getAxisLength(ctx, input, 0);\n        Ht = numDirections == 2\n            ? maskBidirRNNHidden(ctx, node, loop, seqLens, maxLen, hiddenState->getOutput(0), Ht, singlePassShape)\n            : maskRNNHidden(ctx, node, loop, seqLens, hiddenState->getOutput(0), Ht, maxLen, direction == \"reverse\");\n    }\n\n    hiddenState->setInput(1, *Ht);\n    LOG_VERBOSE(\"H(t) -> \" << Ht->getDimensions());\n\n    std::vector<TensorOrWeights> outputs{};\n\n    // Add outputs. In ONNX, all RNN outputs are optional, so check for them here.\n    auto shouldAddOutput = [&node](int32_t index) { return index < node.output().size() && !node.output(index).empty(); };\n\n    // Y = concatenation of all H(t) for each element of the sequence\n    if (shouldAddOutput(0))\n    {\n        outputs.emplace_back(concatenateRNNOutputs(ctx, node, loop, singlePassShape, getAxisLength(ctx, input, 0), Ht,\n            numDirections, inputs, direction == \"reverse\"));\n    }\n    // Yh = last value of H(t)\n    if (shouldAddOutput(1))\n    {\n        outputs.emplace_back(\n            loop->addLoopOutput(*hiddenState->getOutput(0), nvinfer1::LoopOutput::kLAST_VALUE)->getOutput(0));\n    }\n\n    return {{outputs}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(RoiAlign)\n{\n    nvinfer1::ITensor* tensorPtr = &convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor* roisPtr = &convertToTensor(inputs.at(1), ctx);\n    nvinfer1::ITensor* batchIndicesPtr = &convertToTensor(inputs.at(2), ctx);\n    batchIndicesPtr = castHelper(ctx, batchIndicesPtr, DataType::kINT32);\n\n    // Sanity checking\n    auto roiDims = roisPtr->getDimensions();\n    ONNXTRT_CHECK_NODE(roiDims.nbDims == 2 && roiDims.d[1] == 4,\n        \"Found incorrect dimensions for ROIs input! Rank of ROI input = \" << roiDims.nbDims\n                                                                          << \", roiDims.d[1] = \" << roiDims.d[1] << \".\",\n        node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    OnnxAttrs attrs(node, ctx);\n\n    int32_t coordinateTransformationMode{};\n\n    if (ctx->getOpsetVersion() >= 16)\n    {\n        coordinateTransformationMode\n            = attrs.get(\"coordinate_transformation_mode\", std::string(\"half_pixel\")) == \"half_pixel\" ? 1 : 0;\n    }\n    else\n    {\n        // RoiAlign-10 does not support coordinate_transformation_mode\n        // Fall-back to 'output_half_pixel' in RoiAlign-16\n        coordinateTransformationMode = 0;\n    }\n\n    int32_t mode = attrs.get(\"mode\", std::string(\"avg\")) == \"avg\" ? 1 : 0;\n    int32_t outputHeight = attrs.get(\"output_height\", 1);\n    int32_t outputWidth = attrs.get(\"output_width\", 1);\n    int32_t samplingRatio = attrs.get(\"sampling_ratio\", 1);\n    float spatialScale = attrs.get(\"spatial_scale\", 1.0F);\n\n    // Populate RoiAlign plugin properties.\n    std::string const pluginName = \"ROIAlign_TRT\";\n    std::string const pluginVersion = \"2\";\n    std::vector<nvinfer1::PluginField> f;\n    f.emplace_back(\n        \"coordinate_transformation_mode\", &coordinateTransformationMode, nvinfer1::PluginFieldType::kINT32, 1);\n    f.emplace_back(\"mode\", &mode, nvinfer1::PluginFieldType::kINT32, 1);\n    f.emplace_back(\"output_height\", &outputHeight, nvinfer1::PluginFieldType::kINT32, 1);\n    f.emplace_back(\"output_width\", &outputWidth, nvinfer1::PluginFieldType::kINT32, 1);\n    f.emplace_back(\"sampling_ratio\", &samplingRatio, nvinfer1::PluginFieldType::kINT32, 1);\n    f.emplace_back(\"spatial_scale\", &spatialScale, nvinfer1::PluginFieldType::kFLOAT32, 1);\n\n    // Create plugin from registry\n    auto const plugin = createPlugin(getNodeName(node), kTRT_STD_PLUGIN_NAMESPACE,\n        static_cast<nvinfer1::IPluginCreatorV3One*>(importPluginCreator(ctx, pluginName, pluginVersion)), f);\n\n    ONNXTRT_CHECK_NODE(plugin != nullptr, \"ROIAlign plugin was not found in the plugin registry!\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n\n    nvinfer1::ITensor* const inputTensorsPtr[3] = {tensorPtr, roisPtr, batchIndicesPtr};\n    auto* layer = N_CHECK(ctx->network()->addPluginV3(inputTensorsPtr, 3, nullptr, 0, *plugin));\n    ctx->registerLayer(layer, node);\n\n    // ROIAlign requires nvinfer_vc_plugin when using VC.\n#if defined(_WIN32)\n    ctx->addUsedVCPluginLibrary(\n        node, pluginName.c_str(), (\"nvinfer_vc_plugin_\" + std::to_string(NV_TENSORRT_MAJOR)).c_str());\n#else\n    ctx->addUsedVCPluginLibrary(node, pluginName.c_str(), \"nvinfer_vc_plugin\");\n#endif\n\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(ScaledTanh)\n{\n    OnnxAttrs attrs(node, ctx);\n    float alpha = attrs.get<float>(\"alpha\");\n    float beta = attrs.get<float>(\"beta\");\n    return activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kSCALED_TANH, &alpha, &beta);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Scan)\n{\n    OnnxAttrs attrs(node, ctx);\n\n    // In opset 8, the scan node is defined differently than in later opsets.\n    //     1. It has an optonal input `sequence_lens`\n    //     2. The scan input/output axis are always set to 1\n    const int32_t opset8Offset = ctx->getOpsetVersion() == 8 ? 1 : 0;\n    if (opset8Offset == 1)\n    {\n        ONNXTRT_CHECK_NODE(inputs.at(0).isNullTensor(), \"TensorRT doesn't support sequence_lens input for this node!\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    }\n    int32_t const nbInputs = node.input().size() - opset8Offset;\n    int32_t const nbScanInputs = attrs.get<int>(\"num_scan_inputs\");\n    // The number of state variables on the input and output is the same.\n    int32_t const nbStateVars = nbInputs - nbScanInputs;\n    int32_t const nbScanOutputs = node.output().size() - nbStateVars;\n\n    // Populate scan input axis\n    std::vector<int32_t> defaultScanInputArgs(nbScanInputs);\n    std::fill(defaultScanInputArgs.begin(), defaultScanInputArgs.end(), opset8Offset);\n    std::vector<int32_t> scanInputAxes(attrs.get(\"scan_input_axes\", defaultScanInputArgs));\n\n    // Populate scan input directions\n    std::vector<int32_t> defaultInputScanDirection(nbScanInputs);\n    std::fill(defaultInputScanDirection.begin(), defaultInputScanDirection.end(), 0);\n    std::vector<int32_t> const scanInputDirections(attrs.get(\"scan_input_directions\", defaultInputScanDirection));\n\n    // Populate scan output axis\n    std::vector<int32_t> defaultScanOutputArgs(nbScanOutputs);\n    std::fill(defaultScanOutputArgs.begin(), defaultScanOutputArgs.end(), opset8Offset);\n    std::vector<int32_t> scanOutputAxes(attrs.get(\"scan_output_axes\", defaultScanOutputArgs));\n\n    // Populate scan ouput directions\n    std::vector<int32_t> defaultOutputScanDirection(nbScanOutputs);\n    std::fill(defaultOutputScanDirection.begin(), defaultOutputScanDirection.end(), 0);\n    std::vector<int32_t> const scanOutputDirections(attrs.get(\"scan_output_directions\", defaultOutputScanDirection));\n\n    ::ONNX_NAMESPACE::GraphProto const& body = attrs.get<::ONNX_NAMESPACE::GraphProto const&>(\"body\");\n\n    // Support possible negative axis for input and output axes:\n    for (auto& axis : scanInputAxes)\n    {\n        convertAxis(axis, nvinfer1::Dims::MAX_DIMS, node, nodeIdx);\n    }\n\n    for (auto& axis : scanOutputAxes)\n    {\n        convertAxis(axis, nvinfer1::Dims::MAX_DIMS, node, nodeIdx);\n    }\n\n    auto loop = N_CHECK(ctx->network()->addLoop());\n    // When multiple scan inputs are present, scan behaves like zip, so it is sufficient\n    // to use only one scan input to determine trip limit.\n    nvinfer1::ITensor* tripLimit = getAxisLength(ctx, &convertToTensor(inputs.back(), ctx), scanInputAxes.back());\n    loop->addTripLimit(*tripLimit, nvinfer1::TripLimit::kCOUNT);\n\n    // Establish scope for names local to the subgraph.\n    NameScope nameScope(*ctx);\n\n    // Add initial state inputs using recurrent layers, and scan inputs using iterators.\n    std::vector<nvinfer1::IRecurrenceLayer*> stateVars{};\n    for (int32_t i = 0; i < nbStateVars; ++i)\n    {\n        stateVars.emplace_back(N_CHECK(loop->addRecurrence(convertToTensor(inputs.at(i + opset8Offset), ctx))));\n        ctx->registerTensor(TensorOrWeights{N_CHECK(stateVars.back()->getOutput(0))}, body.input(i).name());\n    }\n    ctx->registerLayer(stateVars.at(0), node);\n\n    for (int32_t i = 0; i < nbScanInputs; ++i)\n    {\n        const int32_t index = nbStateVars + i; // Scan Inputs are after the state variables.\n        nvinfer1::IIteratorLayer* scanInput\n            = N_CHECK(loop->addIterator(convertToTensor(inputs.at(index + opset8Offset), ctx)));\n        scanInput->setAxis(scanInputAxes.at(i));\n        scanInput->setReverse(scanInputDirections.at(i) == 1);\n        ctx->registerTensor(TensorOrWeights{N_CHECK(scanInput->getOutput(0))}, body.input(index).name());\n    }\n\n    // Loop Body. This is handled by dispatching to other op converters.\n    std::vector<Status> errors{};\n    onnx2trt::parseGraph(ctx, body, errors);\n\n    // Set up recurrence outputs (first N body graph outputs).\n    std::vector<TensorOrWeights> nodeOutputs{};\n    for (int32_t i = 0; i < nbStateVars; ++i)\n    {\n        auto const& bodyOutputName = body.output(i).name();\n        auto& stateOutput = convertToTensor(ctx->tensors().at(bodyOutputName), ctx);\n        LOG_VERBOSE(\"For state variable output: \" << bodyOutputName\n                                                  << \", found matching tensor: \" << stateOutput.getName()\n                                                  << \", with shape: \" << stateOutput.getDimensions());\n        stateVars.at(i)->setInput(1, stateOutput);\n        // Each state variable is also a loop output\n        auto output = N_CHECK(stateVars.at(i)->getOutput(0));\n        auto outputLayer = N_CHECK(loop->addLoopOutput(*output, nvinfer1::LoopOutput::kLAST_VALUE));\n        auto outputTensor = N_CHECK(outputLayer->getOutput(0));\n        nodeOutputs.emplace_back(outputTensor);\n    }\n    // Finally, set up scan outputs.\n    for (int32_t i = 0; i < nbScanOutputs; ++i)\n    {\n        int32_t const index = nbStateVars + i;\n        auto const& bodyOutputName = body.output(index).name();\n        auto& scanOutput = convertToTensor(ctx->tensors().at(bodyOutputName), ctx);\n        // For scanOutputDirections, 0 indicates appending, and 1, prepending.\n        auto const scanDirection\n            = (scanOutputDirections.at(i) == 0) ? nvinfer1::LoopOutput::kCONCATENATE : nvinfer1::LoopOutput::kREVERSE;\n        auto const scanAxis = scanOutputAxes.at(i);\n        LOG_VERBOSE(\"For scan output: \" << bodyOutputName << \", found matching tensor: \" << scanOutput.getName()\n                                        << \", with shape: \" << scanOutput.getDimensions() << \". Using scan direction: \"\n                                        << static_cast<int32_t>(scanDirection) << \", and scan axis: \" << scanAxis);\n        nvinfer1::ILoopOutputLayer* trtScanOut = N_CHECK(loop->addLoopOutput(scanOutput, scanDirection, scanAxis));\n        trtScanOut->setInput(1, *tripLimit);\n        nodeOutputs.emplace_back(N_CHECK(trtScanOut->getOutput(0)));\n    }\n\n    return {nodeOutputs};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(GridSample)\n{\n    checkNotInvalidType(inputs.at(0), {\"BOOL\", \"UINT8\"}, node, nodeIdx);\n    checkNotInvalidType(inputs.at(1), {\"BOOL\", \"UINT8\"}, node, nodeIdx);\n    ONNXTRT_CHECK_NODE(\n        (inputs.size() == 2), \"TRT expects two input tensors: grid and input\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    nvinfer1::ITensor& input = convertToTensor(inputs.at(0), ctx);\n    int32_t const inputRank = input.getDimensions().nbDims;\n    ONNXTRT_CHECK_NODE(\n        (inputRank > 0), \"The input tensor cannot be a scalar.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    nvinfer1::ITensor& grid = convertToTensor(inputs.at(1), ctx);\n    int32_t const gridRank = grid.getDimensions().nbDims;\n    ONNXTRT_CHECK_NODE(\n        (gridRank > 0), \"The grid tensor cannot be a scalar.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    ONNXTRT_CHECK_NODE((gridRank == inputRank),\n        \"The input tensor and the grid tensor must have the same rank. Rank of grid tensor = \"\n            << gridRank << \", rank of input = \" << inputRank << \".\",\n        node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    // Add grid sample layer\n    nvinfer1::IGridSampleLayer* layer = N_CHECK(ctx->network()->addGridSample(input, grid));\n    ctx->registerLayer(layer, node);\n    OnnxAttrs attrs(node, ctx);\n\n    auto paddingMode = attrs.get<std::string>(\"padding_mode\", \"zeros\");\n    nvinfer1::SampleMode sampleMode{nvinfer1::SampleMode::kFILL};\n    if (paddingMode == \"zeros\")\n    {\n        sampleMode = nvinfer1::SampleMode::kFILL;\n    }\n    else if (paddingMode == \"border\")\n    {\n        sampleMode = nvinfer1::SampleMode::kCLAMP;\n    }\n    else if (paddingMode == \"reflection\")\n    {\n        sampleMode = nvinfer1::SampleMode::kREFLECT;\n    }\n\n    auto mode = attrs.get<std::string>(\"mode\", \"bilinear\");\n    nvinfer1::InterpolationMode interpolationMode{nvinfer1::InterpolationMode::kNEAREST};\n    if (mode == \"nearest\")\n    {\n        interpolationMode = nvinfer1::InterpolationMode::kNEAREST;\n    }\n    else if (mode == \"bilinear\")\n    {\n        interpolationMode = nvinfer1::InterpolationMode::kLINEAR;\n    }\n    else if (mode == \"bicubic\")\n    {\n        interpolationMode = nvinfer1::InterpolationMode::kCUBIC;\n    }\n\n    bool const alignCorners{attrs.get<int32_t>(\"align_corners\", 0) == 1};\n\n    ONNXTRT_CHECK_NODE(\n        layer->setSampleMode(sampleMode), \"Failed to set sample mode!\", node, nodeIdx, ErrorCode::kINVALID_VALUE);\n    layer->setAlignCorners(alignCorners);\n    layer->setInterpolationMode(interpolationMode);\n\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(ScatterND)\n{\n    OnnxAttrs attrs(node, ctx);\n    ONNXTRT_CHECK_NODE(!attrs.count(\"reduction\"), \"Attribute reduction is not supported.\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE_ATTR);\n    return addScatterLayer(ctx, node, nodeIdx, inputs, nvinfer1::ScatterMode::kND);\n}\n\nNodeOutputs scatterPluginHelper(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t nodeIdx,\n    std::vector<TensorOrWeights>& inputs, int32_t axis, std::string const& reduction)\n{\n    // Populate scatter plugin properties.\n    std::string const pluginName = \"ScatterElements\";\n    std::string const pluginVersion = \"2\";\n    std::vector<nvinfer1::PluginField> f;\n\n    // populate fields axis, reduction type\n\n    f.emplace_back(\"axis\", &axis, nvinfer1::PluginFieldType::kINT32, 1);\n    f.emplace_back(\"reduction\", reduction.c_str(), nvinfer1::PluginFieldType::kCHAR, reduction.size());\n\n    // Create plugin from registry\n    auto const plugin = createPlugin(getNodeName(node), kTRT_STD_PLUGIN_NAMESPACE,\n        static_cast<nvinfer1::IPluginCreatorV3One*>(importPluginCreator(ctx, pluginName, pluginVersion)), f);\n\n    ONNXTRT_CHECK_NODE(plugin != nullptr, \"ScatterReduction plugin was not found in the plugin registry!\", node,\n        nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    // Create vector of inputs\n    std::vector<nvinfer1::ITensor*> pluginInputs{};\n    for (auto& input : inputs)\n    {\n        pluginInputs.emplace_back(&convertToTensor(input, ctx));\n    }\n\n    auto* layer = N_CHECK(ctx->network()->addPluginV3(pluginInputs.data(), pluginInputs.size(), nullptr, 0, *plugin));\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(ScatterElements)\n{\n    OnnxAttrs attrs(node, ctx);\n    int32_t axis = attrs.get<int>(\"axis\", 0);\n    int32_t nbDims = inputs.at(0).shape().nbDims;\n    convertAxis(axis, nbDims, node, nodeIdx);\n\n    auto reduction = attrs.get<std::string>(\"reduction\", \"none\");\n    if (reduction != \"none\")\n    {\n        return scatterPluginHelper(ctx, node, nodeIdx, inputs, axis, reduction);\n    }\n\n    return addScatterLayer(ctx, node, nodeIdx, inputs, nvinfer1::ScatterMode::kELEMENT, axis);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Scatter)\n{\n    // Scatter was deprecated in Opset 11 and replaced by ScatterElements\n    if (ctx->getOpsetVersion() >= 11)\n    {\n        LOG_WARNING(\"Scatter was deprecated in Opset 11. Node: \\\"\" << getNodeName(node)\n                                                                   << \"\\\" will be converted to ScatterElements.\");\n    }\n\n    return importScatterElements(ctx, node, nodeIdx, inputs);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Selu)\n{\n    OnnxAttrs attrs(node, ctx);\n    float alpha = attrs.get(\"alpha\", 1.6732f);\n    float beta = attrs.get(\"gamma\", 1.0507f);\n    return activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kSELU, &alpha, &beta);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Shape)\n{\n    OnnxAttrs attrs(node, ctx);\n    nvinfer1::ITensor& input = convertToTensor(inputs.at(0), ctx);\n    auto* layer = N_CHECK(ctx->network()->addShape(input));\n    ctx->registerLayer(layer, node);\n    auto* layerOutput = N_CHECK(layer->getOutput(0));\n\n    nvinfer1::ICastLayer* cast = N_CHECK(ctx->network()->addCast(*layerOutput, DataType::kINT64));\n\n    auto const rank = input.getDimensions().nbDims;\n    auto start = attrs.get<int32_t>(\"start\", 0);\n    auto end = attrs.get<int32_t>(\"end\", rank);\n    auto const getInRank = [](int32_t x, int32_t rank) {\n        if (x < 0)\n        {\n            x += rank;\n        }\n        x = std::min(std::max(0, x), rank);\n        return x;\n    };\n    start = getInRank(start, rank);\n    end = getInRank(end, rank);\n    if (start == 0 && end == rank)\n    {\n        RETURN_FIRST_OUTPUT(cast, node, nodeIdx);\n    }\n\n    auto const size = std::max(0, end - start);\n    auto castOutput = N_CHECK(cast->getOutput(0));\n    auto* slice = ctx->network()->addSlice(*castOutput, makeDims(1, start), makeDims(1, size), makeDims(1, 1));\n    RETURN_FIRST_OUTPUT(slice, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Sigmoid)\n{\n    return activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kSIGMOID);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Sin)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kSIN);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Sinh)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kSINH);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Size)\n{\n    // \"data : T\n    // An input tensor.\"\n    auto const shape = shapeOf(inputs.at(0));\n\n    // \"outputs a int64 scalar that equals to the total number of elements of the input tensor.\"\n    ShapeTensor const size = product(ctx, shape, 0, shape.size(), /*rank=*/0);\n\n    return {{&size.tensor(ctx)}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Slice)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    int32_t const nbInputs = node.input().size();\n    // \"...it uses this information to slice the input data tensor.\"\n    nvinfer1::ITensor& data = convertToTensor(inputs.at(0), ctx);\n    auto const dims = shapeOf(data);\n\n    // \"Slices uses starts, ends, axes and steps inputs to specify the start and\n    // end dimension and step for each axis in the list of axes...\"\n    ShapeTensor starts;\n    ShapeTensor ends;\n    ShapeTensor axes;\n    ShapeTensor steps;\n\n    // If opset version >= 10 slice parameters are weights instead of attributes.\n    if (ctx->getOpsetVersion() >= 10)\n    {\n        auto isWeightsOrEmpty\n            = [&inputs, &nbInputs](int32_t index) { return nbInputs <= index || inputs.at(index).is_weights(); };\n\n        auto isInt32 = inputs.at(1).isInt32();\n\n        // Fast path for all INT32 constants. Required for safety engines that do not support INT64.\n        if (isInt32 && isWeightsOrEmpty(1) && isWeightsOrEmpty(2) && isWeightsOrEmpty(3) && isWeightsOrEmpty(4)\n            && !isDynamic(data.getDimensions()))\n        {\n            return staticSliceImporter(ctx, node, nodeIdx, inputs, data);\n        }\n\n        nvinfer1::ITensor* input1 = castHelper(ctx, &convertToTensor(inputs.at(1), ctx), DataType::kINT64);\n        nvinfer1::ITensor* input2 = castHelper(ctx, &convertToTensor(inputs.at(2), ctx), DataType::kINT64);\n        starts = ShapeTensor{*input1};\n        ends = ShapeTensor{*input2};\n        // \"If axes are omitted, they are set to [0, ..., ndim-1].\"\n        axes = nbInputs > 3 ? ShapeTensor(ctx, inputs.at(3)) : iotaShapeVector(dims.size());\n        ONNXTRT_CHECK_NODE((starts.size() == axes.size()),\n            \"The shape of input starts misaligns with the shape of input axes. Shape of input starts = \"\n                << starts.size() << \", shape of input axes = \" << axes.size() << \".\",\n            node, nodeIdx, ErrorCode::kINVALID_NODE);\n        ONNXTRT_CHECK_NODE(ends.size() == axes.size(),\n            \"The shape of input ends misaligns with the shape of input axes. Shape of input ends = \"\n                << ends.size() << \", sahpe of input axes = \" << axes.size() << \".\",\n            node, nodeIdx, ErrorCode::kINVALID_NODE);\n        // \"If steps are omitted, they are set to [1, ..., 1] of length len(starts).\"\n        steps = inputs.size() > 4 ? ShapeTensor(ctx, inputs.at(4)) : similar(ctx, starts, 1);\n    }\n    else\n    {\n        OnnxAttrs attrs(node, ctx);\n        starts = ShapeTensor(1, attrs.get<std::vector<int64_t>>(\"starts\"));\n        ends = ShapeTensor(1, attrs.get<std::vector<int64_t>>(\"ends\"));\n        // \"It's optional. If not present, will be treated as [0, 1, ..., len(starts) - 1].\"\n        axes = attrs.count(\"axes\") ? ShapeTensor(1, attrs.get<std::vector<int64_t>>(\"axes\"))\n                                   : iotaShapeVector(starts.size());\n        steps = similar(ctx, starts, 1);\n    }\n\n    if (axes.allValuesKnown())\n    {\n        // gather() requires indices to be normalized if their values are known\n        normalizeAxes(axes, dims.size());\n    }\n    // Get dimensions of dims that correspond to axes for the computation of sizes\n    auto const axesDims = gather(ctx, dims, axes);\n\n    // ONNX has a bunch of rules for converting out of bounds starts/ends\n    // indices into the actual indices to use.\n    decodeOnnxStartsAndEnds(ctx, axesDims, steps, starts, ends);\n\n    // TensorRT uses sizes of the output dimensions instead of ends.\n    ShapeTensor sizes = computeSliceSizes(ctx, starts, ends, steps, axesDims);\n\n    // Negative sizes signifies an empty slice, so clamp sizes to 0\n    ShapeTensor const zeros = similar(ctx, axesDims, 0);\n    sizes = max(ctx, zeros, sizes);\n\n    nvinfer1::ISliceLayer* slice = addSlice(ctx, data, starts, sizes, steps);\n    if (axes.allValuesKnown())\n    {\n        auto const nbDims = data.getDimensions().nbDims;\n        slice->setAxes(shapeTensorToDims(axes, \"slice axes\", -nbDims, nbDims - 1));\n    }\n    else\n    {\n        slice->setInput(5, convertToTensor(inputs.at(3), ctx));\n    }\n\n    ctx->registerLayer(slice, node);\n\n    RETURN_FIRST_OUTPUT(slice, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Softmax)\n{\n    auto& input = convertToTensor(inputs.at(0), ctx);\n    auto* softmax = addSoftmax(ctx, node, nodeIdx, input);\n    ONNXTRT_CHECK_NODE(softmax, \"Failed to create softmax layer\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    // Reshape back to original shape\n    auto* reshapeLayer = addShuffle(ctx, *softmax, shapeOf(input));\n    ONNXTRT_CHECK_NODE(reshapeLayer, \"Failed to create reshape layer\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    RETURN_FIRST_OUTPUT(reshapeLayer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Softsign)\n{\n    return activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kSOFTSIGN);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Softplus)\n{\n    return activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kSOFTPLUS);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(SpaceToDepth)\n{\n    checkNotInvalidType(inputs.at(0), {\"BOOL\", \"UINT8\"}, node, nodeIdx);\n    // Input tensor is in NCHW format\n    ONNXTRT_CHECK_NODE((inputs.at(0).shape().nbDims == 4), \"The input tensor must be in the NCHW format.\", node,\n        nodeIdx, ErrorCode::kINVALID_NODE);\n    nvinfer1::ITensor* tensorPtr = &convertToTensor(inputs.at(0), ctx);\n\n    // Extract attributes\n    OnnxAttrs attrs(node, ctx);\n    auto blockSize = attrs.get<int32_t>(\"blocksize\");\n\n    nvinfer1::Permutation const perm{0, 3, 5, 1, 2, 4};\n\n    auto inputShape = shapeOf(*tensorPtr);\n\n    auto const N = gather(ctx, inputShape, shapeVector(0));\n    auto const C = gather(ctx, inputShape, shapeVector(1));\n    auto const H = gather(ctx, inputShape, shapeVector(2));\n    auto const W = gather(ctx, inputShape, shapeVector(3));\n    auto const blockSizeTensor = shapeVector(blockSize);\n\n    auto const C_2 = mul(ctx, C, mul(ctx, blockSizeTensor, blockSizeTensor));\n    auto const H_2 = floorDiv(ctx, H, blockSizeTensor);\n    auto const W_2 = floorDiv(ctx, W, blockSizeTensor);\n\n    // First reshape to {N, C, H / blockSize, blockSize, W / blockSize, blockSize}\n\n    auto const firstShapeDims = concat(\n        ctx, N, concat(ctx, C, concat(ctx, H_2, concat(ctx, blockSizeTensor, concat(ctx, W_2, blockSizeTensor)))));\n\n    auto* firstShuffle = addShuffle(ctx, *tensorPtr, firstShapeDims);\n    firstShuffle->setSecondTranspose(perm);\n    ctx->registerLayer(firstShuffle, node);\n    tensorPtr = N_CHECK(firstShuffle->getOutput(0));\n\n    // Reshape to {N, C * blockSize * blockSize, H / blockSize, W / blockSize}\n    auto secondShapeDims = concat(ctx, N, concat(ctx, C_2, concat(ctx, H_2, W_2)));\n    auto* secondShuffle = addShuffle(ctx, *tensorPtr, secondShapeDims);\n    tensorPtr = N_CHECK(secondShuffle->getOutput(0));\n\n    return {{tensorPtr}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Split)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    size_t const numOutputs = node.output().size();\n\n    // \"input : T\n    // The tensor to split\"\n    nvinfer1::ITensor& inputTensor = convertToTensor(inputs.at(0), ctx);\n    auto const inputDims = shapeOf(inputTensor);\n\n    // \"axis : int (default is 0)\n    // Which axis to split on.\"\n    OnnxAttrs attrs(node, ctx);\n    int32_t axis = attrs.get<int32_t>(\"axis\", 0);\n\n    // \"A negative value means counting dimensions from the back.\n    // Accepted range is [-rank, rank-1] where r = rank(input).\"\n    convertAxis(axis, inputDims.size(), node, nodeIdx);\n\n    std::vector<int64_t> tmp(inputDims.size());\n    std::iota(tmp.begin(), tmp.end(), 0);\n    tmp[axis] = inputDims.size();\n    ShapeTensor const subscripts = ShapeTensor(1, std::move(tmp));\n\n    // \"split : list of ints\"\n    // \"length of each output\"\n    std::vector<int32_t> splitList;\n    ShapeTensor sizes;\n    ShapeTensor sizesLastOne;\n    ShapeTensor sizeSliceAxis;\n    ShapeTensor sizeSliceAxisLastOne;\n    ShapeTensor splitSizesTensor;\n    bool const hasSplitList = (ctx->getOpsetVersion() >= 13) ? (inputs.size() == 2) : attrs.count(\"split\");\n    if (hasSplitList)\n    {\n        // \"Lengths of the parts can be specified using argument split.\"\n        // In opset >= 13, split lengths are an optional input.\n        if (ctx->getOpsetVersion() >= 13)\n        {\n            if (inputs.at(1).is_weights())\n            {\n                auto const splitWeights = inputs.at(1).weights();\n                int64_t const* splitValues = static_cast<int64_t const*>(splitWeights.values);\n                for (size_t i = 0; i < splitWeights.count(); i++)\n                {\n                    splitList.push_back(static_cast<int32_t>(splitValues[i]));\n                }\n            }\n            else\n            {\n                splitSizesTensor = {ctx, inputs.at(1)};\n            }\n            // In opset >= 18, a new attribute num_outputs has been added.\n            // \"Either input 'split' or the attribute 'num_outputs' should be specified, but not both.\"\n            if (ctx->getOpsetVersion() >= 18)\n            {\n                ONNXTRT_CHECK_NODE(!attrs.count(\"num_outputs\"),\n                    \"Either 'split' should be provided as an input or 'num_outputs' should be provided as an \"\n                    \"attribute. But not both.\",\n                    node, nodeIdx, ErrorCode::kINVALID_NODE);\n            }\n        }\n        // Pre-opset 13 split lengths are provided as an attribute\n        else\n        {\n            splitList = attrs.get<std::vector<int32_t>>(\"split\");\n        }\n        ONNXTRT_CHECK_NODE((splitList.empty() || (splitList.size() == numOutputs)),\n            \"The number of the split attribute misaligns with the number of outputs. Number of split attributes = \"\n                << splitList.size() << \", number of outputs = \" << numOutputs << \".\",\n            node, nodeIdx, ErrorCode::kINVALID_NODE);\n    }\n    else\n    {\n        // In opset >= 18, a new attribute 'num_outputs' has been added.\n        if (ctx->getOpsetVersion() >= 18 && attrs.count(\"num_outputs\"))\n        {\n            ONNXTRT_CHECK_NODE(attrs.get<int32_t>(\"num_outputs\") == static_cast<int32_t>(numOutputs),\n                \"The number of node outputs is not the same as the value of 'num_outputs' attribute. num_outputs \"\n                \"attribute value = \"\n                    << attrs.get<int32_t>(\"num_outputs\") << \", number of node outputs = \" << numOutputs << \".\",\n                node, nodeIdx, ErrorCode::kINVALID_NODE);\n        }\n        // \"Otherwise, the tensor is split to equal sized parts.\"\n        ShapeTensor const dimAxis = gather(ctx, inputDims, shapeVector(axis));\n        sizeSliceAxis = floorDiv(ctx, add(ctx, dimAxis, shapeVector(numOutputs - 1)), shapeVector(numOutputs));\n        sizeSliceAxisLastOne = sub(ctx, dimAxis, mul(ctx, sizeSliceAxis, shapeVector(numOutputs - 1)));\n        // Check for invalid size.\n        if (sizeSliceAxisLastOne.allValuesKnown())\n        {\n            ONNXTRT_CHECK_NODE(sizeSliceAxisLastOne[0] >= 0,\n                \"The last chunk size is negative, see details in https://github.com/onnx/onnx/issues/5766\", node,\n                nodeIdx, ErrorCode::kINVALID_NODE);\n        }\n        sizes = interlace(ctx, inputDims, sizeSliceAxis, subscripts);\n        sizesLastOne = interlace(ctx, inputDims, sizeSliceAxisLastOne, subscripts);\n    }\n\n    std::vector<TensorOrWeights> outputs;\n    outputs.reserve(numOutputs);\n\n    ShapeTensor const zeros = similar(ctx, inputDims, 0);\n    ShapeTensor const ones = similar(ctx, inputDims, 1);\n    ShapeTensor starts = zeros;\n    ShapeTensor startSliceAxis = shapeVector(0);\n    for (int32_t i = 0; i < static_cast<int32_t>(numOutputs); ++i)\n    {\n        if (i)\n        {\n            // Advance from previous start.\n            startSliceAxis = add(ctx, startSliceAxis, sizeSliceAxis);\n            starts = interlace(ctx, zeros, startSliceAxis, subscripts);\n        }\n        if (hasSplitList)\n        {\n            if (splitList.empty())\n            {\n                sizeSliceAxis = gather(ctx, splitSizesTensor, ShapeTensor(1, std::vector<int64_t>{i}));\n            }\n            else\n            {\n                sizeSliceAxis = shapeVector(splitList[i]);\n            }\n            sizes = interlace(ctx, inputDims, sizeSliceAxis, subscripts);\n        }\n        else if (i == static_cast<int32_t>(numOutputs) - 1)\n        {\n            sizes = sizesLastOne;\n        }\n\n        nvinfer1::ISliceLayer* slice = addSlice(ctx, inputTensor, starts, sizes, ones);\n        ctx->registerLayer(slice, node);\n        outputs.emplace_back(N_CHECK(slice->getOutput(0)));\n    }\n\n    return outputs;\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Sqrt)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kSQRT);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Squeeze)\n{\n    // \"data : T\n    // Tensor with at least max(dims) dimensions.\"\n    nvinfer1::ITensor& data = convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor* axesTensor{nullptr};\n    // In opset >= 13, axes are an optional input\n    if (ctx->getOpsetVersion() >= 13)\n    {\n        if (inputs.size() == 2)\n        {\n            axesTensor = &convertToTensor(inputs.at(1), ctx);\n        }\n    }\n    // Pre-opset 13 axes are provided as an attribute\n    else\n    {\n        OnnxAttrs attrs(node, ctx);\n        if (attrs.count(\"axes\"))\n        {\n            std::vector<int64_t> axes = attrs.get<std::vector<int64_t>>(\"axes\");\n            axesTensor = N_CHECK(\n                addConstant(ctx, axes, ::ONNX_NAMESPACE::TensorProto::INT64, {1, {static_cast<int64_t>(axes.size())}})\n                    ->getOutput(0));\n        }\n    }\n\n    // If axes are ommitted, squeeze all dimensions with values 1\n    if (!axesTensor)\n    {\n        auto const shape = data.getDimensions();\n        ONNXTRT_CHECK_NODE(!isDynamic(shape),\n            \"Cannot infer squeeze dimensions from a dynamic shape! Please re-export your model with the Squeeze axes \"\n            \"input set.\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE_DYNAMIC);\n        std::vector<int64_t> axes{};\n        for (int32_t i = 0; i < shape.nbDims; i++)\n        {\n            if (shape.d[i] == 1)\n            {\n                axes.push_back(i);\n            }\n        }\n        axesTensor = N_CHECK(\n            addConstant(ctx, axes, ::ONNX_NAMESPACE::TensorProto::INT64, {1, {static_cast<int64_t>(axes.size())}})\n                ->getOutput(0));\n    }\n\n    ONNXTRT_CHECK_NODE(\n        axesTensor != nullptr, \"Failed to create squeeze axes!\", node, nodeIdx, ErrorCode::kINTERNAL_ERROR);\n\n    // Unsqueeze axes may be a scalar. Convert to a vector if neccessary.\n    axesTensor = convertScalarToVector(ctx, axesTensor);\n\n    // \"squeezed : T\n    // Reshaped tensor with same data as input.\"\n    auto* squeezeLayer = N_CHECK(ctx->network()->addSqueeze(data, *axesTensor));\n    ctx->registerLayer(squeezeLayer, node);\n    RETURN_FIRST_OUTPUT(squeezeLayer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(STFT)\n{\n    /*\n    STFT is implemented via decomposion into 1D convs.\n\n    This assumes that the input signal contains non-complex values, and that all non-signal inputs are initializers.\n\n                                STFT Input (Batch, SignalLength, 1) or (Batch, SignalLength)\n                                    |\n                                    |\n                                Reshape to (Batch, 1, 1, SignalLength)\n                                    |\n                        ____________|____________\n                        |                       |\n                        |                       |\n                        |                       |\n                    Conv 1D (Real)       Conv 1D (Imaginary) with weights (dftUniqueOutputs, 1, 1, FrameLength)\n                        |                       |    output shape is (Batch, dftUniqueOutputs, 1, Frames)\n                        |                       |\n                        |_______________________|\n                                    |\n                                    |\n                                Concat on axis 2 - (Batch, dftUniqueOutputs, 2, Frames)\n                                    |\n                                    |\n                                Transpose to ONNX output shape (Batch, Frames, dftUniqueOutputs, 2)\n                                    |\n                                    |\n                                  Output\n    */\n\n    OnnxAttrs attrs(node, ctx);\n    int64_t onesided = attrs.get<int64_t>(\"onesided\", 1);\n    auto* input = &convertToTensor(inputs.at(0), ctx);\n    auto dims = input->getDimensions();\n    // Signal must composed of real-valued inputs only - if rank == 2 or rank == 3 && dims.d[2] == 1\n    ONNXTRT_CHECK_NODE(dims.nbDims == 2 || (dims.nbDims == 3 && dims.d[2] == 1),\n        \"TensorRT only supports STFT on real-valued signals!\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    // Squeeze 3D shape of (Batch, SignalLength, 1) down to 2D shape of (Batch, SignalLength) to unify future unsqueeze\n    // logic.\n    if (dims.nbDims == 3)\n    {\n        std::vector<int32_t> const axes{2};\n        input = squeezeTensor(ctx, *input, axes);\n    }\n\n    // Float only support.\n    ONNXTRT_CHECK_NODE(input->getType() == nvinfer1::DataType::kFLOAT,\n        \"Input to STFT must be Float32. Received type: \" << input->getType(), node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n\n    int64_t frameStep{0};\n    ShapedWeights windowWeights = ShapedWeights::empty(::ONNX_NAMESPACE::TensorProto_DataType_FLOAT);\n    int64_t frameLength{0};\n\n    // Frame step - must be constant as this corresponds to the strides of the convolution.\n    ONNXTRT_CHECK_NODE(\n        inputs.at(1).is_weights(), \"FrameStep must be an initializer!\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    frameStep = static_cast<int64_t*>(inputs.at(1).weights().values)[0];\n\n    // Window - optional.\n    if (inputs.size() >= 3 && !inputs.at(2).isNullTensor())\n    {\n        ONNXTRT_CHECK_NODE(inputs.at(2).is_weights(), \"windowWeights must be an initializer!\", node, nodeIdx,\n            ErrorCode::kUNSUPPORTED_NODE);\n        windowWeights = inputs.at(2).weights();\n    }\n    // Frame length - scalar value (optional)\n    if (inputs.size() >= 4 && !inputs.at(3).isNullTensor())\n    {\n        ONNXTRT_CHECK_NODE(inputs.at(3).is_weights(), \"Frame length must be an initializer!\", node, nodeIdx,\n            ErrorCode::kUNSUPPORTED_NODE);\n        frameLength = static_cast<int64_t*>(inputs.at(3).weights().values)[0];\n    }\n    // If both windowWeights and frameLength are not provided, we cannot infer the size for the Window\n    if (frameLength == 0 && windowWeights.values == nullptr)\n    {\n        ONNXTRT_CHECK_NODE(false, \"Both frame_length and window inputs are missing for STFT!\", node, nodeIdx,\n            ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    // Generate missing values if necessary.\n    if (frameLength == 0 && windowWeights.values != nullptr)\n    {\n        frameLength = windowWeights.shape.d[0];\n    }\n    if (frameLength != 0 && windowWeights.values == nullptr)\n    {\n        windowWeights = ctx->createNamedTempWeights(::ONNX_NAMESPACE::TensorProto_DataType_FLOAT, {1, {frameLength}});\n        for (int64_t c = 0; c < frameLength; c++)\n        {\n            static_cast<float*>(windowWeights.values)[c] = 1.F;\n        }\n    }\n\n    // Calculate dftUniqueBins depending on the onesided attribute.\n    int64_t dftUniqueBins = onesided == 1 ? ((frameLength >> 1) + 1) : frameLength;\n\n    /*\n        Generate the weights for the convolutions, of shape (dftUniqueBins, 1, 1, frameLength).\n\n        We need to generate weights of values window[k] * e^(-2 * pi * j * w * k / n), where:\n        j is the imaginary number sqrt(-1)\n        w is the frequency for 0 <= w < dftUniqueBins\n        k is the index in the window, for 0 <= k < frameLength\n        n is equal to frameLength.\n\n        The real and imaginary components are generated separately using Euler's formula.\n\n        for each w in dftUniqueBins:\n            for each k in frameLength:\n                realWeights[w, 1, 1, k] = cos(-2 * pi * k * w / n) * window[k]\n                imagWeights[w, 1, 1, k] = sin(-2 * pi * k * w / n) * window[k]\n    */\n\n    auto realWeights = ctx->createNamedTempWeights(\n        ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT, {4, {dftUniqueBins, 1, 1, frameLength}});\n    auto imaginaryWeights = ctx->createNamedTempWeights(\n        ::ONNX_NAMESPACE::TensorProto_DataType_FLOAT, {4, {dftUniqueBins, 1, 1, frameLength}});\n\n    for (int64_t w = 0; w < static_cast<int64_t>(dftUniqueBins); w++)\n    {\n        for (int64_t k = 0; k < static_cast<int64_t>(frameLength); k++)\n        {\n            int64_t weightIndex = w * frameLength + k;\n            auto angle = -2.F * M_PI * w * k / frameLength;\n            static_cast<float*>(realWeights.values)[weightIndex]\n                = static_cast<float>(cos(angle)) * static_cast<float*>(windowWeights.values)[k];\n            static_cast<float*>(imaginaryWeights.values)[weightIndex]\n                = static_cast<float>(sin(angle)) * static_cast<float*>(windowWeights.values)[k];\n        }\n    }\n\n    // Unsqueeze input to [batch, 1, 1, numFrames]\n    auto signalReshaped = unsqueezeTensor(ctx, *input, {1, 2});\n\n    // 1D Convolution to calculate the real part of the signal.\n    auto convReal = N_CHECK(\n        ctx->network()->addConvolutionNd(*signalReshaped, dftUniqueBins, {2, {1, frameLength}}, realWeights, {}));\n    convReal->setStrideNd(nvinfer1::Dims{2, {1, frameStep}});\n    auto* convRealOutput = N_CHECK(convReal->getOutput(0));\n\n    // 1D Convolution to caclulate the imaginary part of the signal.\n    auto convImag\n        = ctx->network()->addConvolutionNd(*signalReshaped, dftUniqueBins, {2, {1, frameLength}}, imaginaryWeights, {});\n    convImag->setStrideNd(nvinfer1::Dims{2, {1, frameStep}});\n    auto* convImagOutput = N_CHECK(convImag->getOutput(0));\n\n    // Concat outputs together on axis 2, convolution outputs have shape: (Batch, dftUniqueOutputs, 1, Frames)\n    std::vector<nvinfer1::ITensor*> concatInputs{convRealOutput, convImagOutput};\n    auto concatLayer = N_CHECK(ctx->network()->addConcatenation(concatInputs.data(), concatInputs.size()));\n    concatLayer->setAxis(2);\n    auto* concatOutput = N_CHECK(concatLayer->getOutput(0));\n\n    // Transpose to ONNX expected output shape - (Batch, dftUniqueOutputs, 2, Frames) -> (Batch, Frames,\n    // dftUniqueOutputs, 2)\n    auto transpose = N_CHECK(ctx->network()->addShuffle(*concatOutput));\n    transpose->setFirstTranspose(nvinfer1::Permutation{{0, 3, 1, 2}});\n    RETURN_FIRST_OUTPUT(transpose, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Sub)\n{\n    return elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kSUB);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Sum)\n{\n    return elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kSUM);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Tan)\n{\n    return unaryHelper(ctx, node, nodeIdx, inputs.at(0), nvinfer1::UnaryOperation::kTAN);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Tanh)\n{\n    return activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kTANH);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(ThresholdedRelu)\n{\n    OnnxAttrs attrs(node, ctx);\n    float alpha = attrs.get<float>(\"alpha\", 1.f);\n    return activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kTHRESHOLDED_RELU, &alpha);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Tile)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    // \"input : T\n    // Input tensor of any shape.\"\n    nvinfer1::ITensor& input = convertToTensor(inputs.at(0), ctx);\n    auto const inputDims = shapeOf(input);\n\n    // \"repeats : T1\n    // 1D int64 tensor of the same length as input's dimension number,\n    // includes numbers of repeated copies along input's dimensions.\n    ShapeTensor const repeats{ctx, inputs.at(1)};\n\n    ShapeTensor outputShape = mul(ctx, inputDims, repeats);\n    nvinfer1::ISliceLayer* tile\n        = addSlice(ctx, input, similar(ctx, inputDims, 0), outputShape, similar(ctx, inputDims, 1));\n    ctx->registerLayer(tile, node);\n    tile->setMode(nvinfer1::SampleMode::kWRAP);\n\n    RETURN_FIRST_OUTPUT(tile, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TopK)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    nvinfer1::ITensor* tensorPtr = &convertToTensor(inputs.at(0), ctx);\n    OnnxAttrs attrs(node, ctx);\n    int32_t axis = attrs.get(\"axis\", -1);\n    int32_t k{1};\n    if (ctx->getOpsetVersion() < 10)\n    {\n        k = attrs.get<int>(\"k\");\n    }\n    int32_t nbDims = tensorPtr->getDimensions().nbDims;\n    convertAxis(axis, nbDims, node, nodeIdx);\n    uint32_t axisMask = 1 << axis;\n\n    bool needToExpandDims = (nbDims == 1);\n    if (needToExpandDims)\n    {\n        // Expand spatial dims from 1D to 2D\n        std::vector<int> axes{1};\n        tensorPtr = unsqueezeTensor(ctx, *tensorPtr, axes);\n        ONNXTRT_CHECK_NODE(tensorPtr, \"Failed to unsqueeze input x.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    // Default is top max k.\n    auto operation = nvinfer1::TopKOperation::kMAX;\n    if (ctx->getOpsetVersion() >= 11)\n    {\n        int32_t const largest = attrs.get<int32_t>(\"largest\", 1);\n        if (largest == 0)\n        {\n            operation = nvinfer1::TopKOperation::kMIN;\n        }\n    }\n    nvinfer1::ITopKLayer* layer = N_CHECK(ctx->network()->addTopK(*tensorPtr, operation, k, axisMask));\n    ONNXTRT_CHECK_NODE(layer, \"Failed to create layer\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    if (ctx->getOpsetVersion() >= 10)\n    {\n        ONNXTRT_CHECK_NODE((inputs.size() == 2),\n            \"Expects two input tensors for opset >= 10: X and K. Current input size = \" << inputs.size() << \".\", node,\n            nodeIdx, ErrorCode::kINVALID_NODE);\n        nvinfer1::ITensor* kPtr = &convertToTensor(inputs.at(1), ctx);\n        kPtr = convertToScalar(ctx, kPtr);\n        layer->setInput(1, *kPtr);\n    }\n    ctx->registerLayer(layer, node);\n    ONNXTRT_CHECK_NODE(layer, \"Failed to add TopK layer.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    nvinfer1::ITensor* values = N_CHECK(layer->getOutput(0));\n    nvinfer1::ITensor* indices = N_CHECK(layer->getOutput(1));\n\n    if (needToExpandDims)\n    {\n        // Un-expand spatial dims back to 1D\n        std::vector<int32_t> axes{1};\n        values = squeezeTensor(ctx, *values, axes);\n        ONNXTRT_CHECK_NODE(values, \"Failed to squeeze the input values.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        indices = squeezeTensor(ctx, *indices, axes);\n        ONNXTRT_CHECK_NODE(\n            indices, \"Failed to squeeze the input indices.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    }\n\n    // TensorRT doesn't support int64 for TopK indices\n    indices = castHelper(ctx, indices, DataType::kINT64);\n    return {{values, indices}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Transpose)\n{\n    TensorOrWeights input = inputs.at(0);\n    OnnxAttrs attrs(node, ctx);\n    int32_t ndim = input.shape().nbDims;\n    ONNXTRT_CHECK_NODE((ndim <= nvinfer1::Dims::MAX_DIMS),\n        \"The rank of the input tensor exceeds the maximum supported by this version of TensorRT. Current rank of \"\n        \"inputs = \"\n            << ndim << \", max supported rank = \" << nvinfer1::Dims::MAX_DIMS << \".\",\n        node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    nvinfer1::Permutation default_perm; // Default is to reverse dims\n    for (int32_t i = 0; i < ndim; ++i)\n    {\n        default_perm.order[i] = ndim - 1 - i;\n    }\n    nvinfer1::Permutation perm = attrs.get(\"perm\", default_perm);\n    for (int32_t i = 0; i < ndim; ++i)\n    {\n        convertAxis(perm.order[i], ndim, node, nodeIdx);\n    }\n    nvinfer1::ITensor& itensor = input.is_tensor() ? input.tensor() : convertToTensor(input, ctx);\n    nvinfer1::ITensor* output_tensor = transposeTensor(ctx, node, itensor, perm);\n    ONNXTRT_CHECK_NODE(output_tensor, \"Failed to transpose the input.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    return {{output_tensor}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Trilu)\n{\n    checkNotInvalidType(inputs.at(0), {\"UINT8\"}, node, nodeIdx);\n    // Data Tensor\n    using eOp = nvinfer1::ElementWiseOperation;\n    auto* data = &convertToTensor(inputs.at(0), ctx);\n    auto const nbDims = data->getDimensions().nbDims;\n    ONNXTRT_CHECK_NODE((nbDims >= 2),\n        \"Trilu input must have at least 2 dimensions! Current number of dimensions = \" << nbDims << \".\", node, nodeIdx,\n        ErrorCode::kINVALID_NODE);\n    OnnxAttrs attrs(node, ctx);\n    int32_t const upper = attrs.get(\"upper\", 0);\n\n    // Input may be in a batch so we need to get NxM dimensions\n    int64_t const N = nbDims - 2;\n    int64_t const M = nbDims - 1;\n\n    // Create iota dims of NxM\n    const ShapeTensor iotadims\n        = concat(ctx, gather(ctx, shapeOf(*data), shapeVector(N)), gather(ctx, shapeOf(*data), shapeVector(M)));\n\n    // Trilu can be represented via trl(A) = select(R >= C, A, 0) for keeping the lower diagonals\n    // Simiarly, trl(A) = select(R <= C, A, 0) represents keeping the upper diagonals\n\n    auto* rows = iota(ctx, iotadims, 0);\n    auto* cols = iota(ctx, iotadims, 1);\n    auto* zero = createZeroTensor(ctx, data);\n\n    // k tensor shifts the number of diagonals we accept. Positive means to include k number of diagonals\n    // above the main, while a negative k means to exclude the main and k number of diagonals below the main.\n    // Adjust column tensor accordingly.\n    if (inputs.size() == 2)\n    {\n        auto* k = &convertToTensor(inputs.at(1), ctx);\n        std::vector<TensorOrWeights> shiftResult = elementwiseHelper(ctx, node, nodeIdx, {cols, k}, eOp::kSUB);\n        cols = &convertToTensor(shiftResult.at(0), ctx);\n    }\n\n    // Unsqueeze to broadcast rows/cols if necessary during next elementwise operation.\n    if (nbDims > 2)\n    {\n        std::vector<int32_t> batchDims(nbDims - 2);\n        std::iota(batchDims.begin(), batchDims.end(), 0);\n        rows = unsqueezeTensor(ctx, *rows, batchDims);\n        cols = unsqueezeTensor(ctx, *cols, batchDims);\n    }\n\n    // For lower Trilus, use greaterOrEquals. For upper Trilus, use lessOrEquals\n    bool const greater = upper == 0 ? true : false;\n    std::vector<TensorOrWeights> greaterOrEqualResult = greaterLessOrEqual(ctx, node, nodeIdx, rows, cols, greater);\n    auto* condition = &convertToTensor(greaterOrEqualResult.at(0), ctx);\n    auto* result = N_CHECK(ctx->network()->addSelect(*condition, *data, *zero));\n\n    RETURN_FIRST_OUTPUT(result, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Unsqueeze)\n{\n    // \"data : T\n    // Original tensor\"\n    nvinfer1::ITensor& data = convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor* axesTensor{nullptr};\n\n    if (ctx->getOpsetVersion() >= 13)\n    {\n        // Per ONNX the 2nd input is mandatory starting at opset 13, but PyTorch\n        // does not comply ATM so allow for single input.\n        // https://github.com/onnx/onnx/blob/master/docs/Changelog.md#unsqueeze-13\n        if (inputs.size() == 2)\n        {\n            axesTensor = &convertToTensor(inputs.at(1), ctx);\n        }\n    }\n\n    if (!axesTensor)\n    {\n        OnnxAttrs attrs(node, ctx);\n        // \"axes : list of ints (required)\n        // List of integers indicating the dimensions to be inserted.\"\n        std::vector<int64_t> axes = attrs.get<std::vector<int64_t>>(\"axes\");\n        axesTensor = N_CHECK(\n            addConstant(ctx, axes, ::ONNX_NAMESPACE::TensorProto::INT64, {1, {static_cast<int64_t>(axes.size())}})\n                ->getOutput(0));\n    }\n\n    // Unsqueeze axes may be a scalar. Convert to a vector if neccessary.\n    axesTensor = convertScalarToVector(ctx, axesTensor);\n\n    ONNXTRT_CHECK_NODE(\n        axesTensor != nullptr, \"Failed to create unsqueeze axes!\", node, nodeIdx, ErrorCode::kINTERNAL_ERROR);\n    // \"expanded : T\n    // Reshaped tensor with same data as input.\"\n    auto* unsqueezeLayer = N_CHECK(ctx->network()->addUnsqueeze(data, *axesTensor));\n    ctx->registerLayer(unsqueezeLayer, node);\n    RETURN_FIRST_OUTPUT(unsqueezeLayer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Upsample)\n{\n    checkNotInvalidType(inputs.at(0), {\"BOOL\", \"UINT8\"}, node, nodeIdx);\n    nvinfer1::ITensor& tensor = convertToTensor(inputs.at(0), ctx);\n    int32_t const nbDims = tensor.getDimensions().nbDims;\n    ONNXTRT_CHECK_NODE((nbDims > 0), \"The input tensor cannot be a scalar.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    OnnxAttrs attrs(node, ctx);\n\n    nvinfer1::IResizeLayer* const layer = N_CHECK(ctx->network()->addResize(tensor));\n    ONNXTRT_CHECK_NODE(layer, \"Failed to create layer\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    auto mode = attrs.get<std::string>(\"mode\", \"nearest\");\n\n    // Set default resize mode. Nearest resize support N-D (where 0 < N <= 8) resize.\n    nvinfer1::InterpolationMode interpolationMode = (mode == \"linear\" || mode == \"bilinear\")\n        ? nvinfer1::InterpolationMode::kLINEAR\n        : nvinfer1::InterpolationMode::kNEAREST;\n\n    if (ctx->getOpsetVersion() >= 9)\n    {\n        // Get scale factors from inputs[1]\n        ONNXTRT_CHECK_NODE((inputs.size() == 2),\n            \"Operator Upsample requires exactly 2 inputs. Current input size = \" << inputs.size() << \".\", node, nodeIdx,\n            ErrorCode::kINVALID_NODE);\n        auto scales_input = inputs.at(1);\n        if (scales_input.is_weights())\n        {\n            // TRT-15340: Remove this and use else path when safety support nbDims == 1.\n            ShapedWeights scales_weights = scales_input.weights();\n            ONNXTRT_CHECK_NODE((scales_weights.shape.nbDims == 1),\n                \"The scales input must be 1D. Current rank of scales input = \" << scales_weights.shape.nbDims << \".\",\n                node, nodeIdx, ErrorCode::kINVALID_NODE);\n            // Scale factors has batch dimension.\n            ONNXTRT_CHECK_NODE((scales_weights.count() == static_cast<size_t>(nbDims)),\n                \"The shape of the scales input must align with the dimensions of the input. Shape of scales input = \"\n                    << scales_weights.count() << \", dimension of input = \" << nbDims << \".\",\n                node, nodeIdx, ErrorCode::kINVALID_NODE);\n            ONNXTRT_CHECK_NODE((scales_weights.type == ::ONNX_NAMESPACE::TensorProto::FLOAT),\n                \"This version of TensorRT only supports FLOAT scales input. Current scales weight type = \"\n                    << scales_weights.type << \".\",\n                node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n            float const* scales_ptr = static_cast<float const*>(scales_weights.values);\n            std::vector<float> scale_factors(nbDims, 1.0F);\n            for (int32_t i = 0; i < nbDims; i++)\n            {\n                scale_factors[i] = scales_ptr[i];\n            }\n            if (mode == \"linear\" || mode == \"bilinear\")\n            {\n                ONNXTRT_CHECK_NODE(canUseNDResize(scale_factors.size(), &scale_factors.front(), 3),\n                    \"This version of TensorRT only supports linear resizing on the outermost 3 dimensions\", node,\n                    nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n            }\n            layer->setScales(scale_factors.data(), nbDims);\n        }\n        else\n        {\n            nvinfer1::ITensor* resizeShape = resizeShapeTensor(ctx, tensor, scales_input);\n            nvinfer1::Dims const outDims = resizeShape->getDimensions();\n            ONNXTRT_CHECK_NODE((outDims.nbDims == 1),\n                \"The scales input must be 1D. Current rank of the scales input = \" << outDims.nbDims << \".\", node,\n                nodeIdx, ErrorCode::kINVALID_NODE);\n            // Scale factors has batch dimension.\n            ONNXTRT_CHECK_NODE((outDims.d[0] == nbDims),\n                \"The shape of the scales input must align with the dimensions of the input. Current shape of the \"\n                \"scales input = \"\n                    << outDims.nbDims << \", dimension of the input = \" << nbDims << \".\",\n                node, nodeIdx, ErrorCode::kINVALID_NODE);\n            ONNXTRT_CHECK_NODE((resizeShape->getType() == DataType::kINT32),\n                \"Resize output shape type must be integral. The actual type is \"\n                    + getTrtDtypeName(resizeShape->getType()) + \".\",\n                node, nodeIdx, ErrorCode::kINVALID_NODE);\n            layer->setInput(1, *resizeShape);\n        }\n    }\n    else\n    {\n        // TRT-15340: Adapt to use resizeShapeTensor instead when safety support nbDims == 1.\n        ONNXTRT_CHECK_NODE(\n            attrs.count(\"scales\"), \"Attribute scales is missing.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE_ATTR);\n        // Get scale factors from OnnxAttrs.\n        auto scales = attrs.get<std::vector<float>>(\"scales\");\n        // Scale factors has batch dimension.\n        ONNXTRT_CHECK_NODE((static_cast<int32_t>(scales.size()) == nbDims),\n            \"The shape of the scales input must align with the dimensions of the input. Current shape of the scales \"\n            \"input = \"\n                << scales.size() << \", dimension of the input = \" << nbDims << \".\",\n            node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n        std::vector<float> scale_factors(nbDims, 1.0F);\n        for (int32_t i = 0; i < nbDims; i++)\n        {\n            scale_factors[i] = scales[i];\n        }\n        if (mode == \"linear\" || mode == \"bilinear\")\n        {\n            ONNXTRT_CHECK_NODE(canUseNDResize(scale_factors.size(), &scale_factors.front(), 3),\n                \"This version of TensorRT only supports linear resizing on the outermost 3 dimensions\", node, nodeIdx,\n                ErrorCode::kUNSUPPORTED_NODE);\n        }\n        layer->setScales(scale_factors.data(), nbDims);\n    }\n    ctx->registerLayer(layer, node);\n    layer->setResizeMode(interpolationMode);\n    layer->setSelectorForSinglePixel(nvinfer1::ResizeSelector::kFORMULA);\n    layer->setNearestRounding(nvinfer1::ResizeRoundMode::kFLOOR);\n    layer->setCoordinateTransformation(nvinfer1::ResizeCoordinateTransformation::kASYMMETRIC);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Where)\n{\n    checkNotInvalidType(inputs.at(1), {\"UINT8\"}, node, nodeIdx);\n    checkNotInvalidType(inputs.at(2), {\"UINT8\"}, node, nodeIdx);\n    nvinfer1::ITensor* condition = &convertToTensor(inputs.at(0), ctx);\n    nvinfer1::ITensor* x = &convertToTensor(inputs.at(1), ctx);\n    nvinfer1::ITensor* y = &convertToTensor(inputs.at(2), ctx);\n\n    broadcastTensors(ctx, x, y, condition);\n\n    nvinfer1::Dims cDims = condition->getDimensions();\n    nvinfer1::Dims xDims = x->getDimensions();\n    nvinfer1::Dims yDims = y->getDimensions();\n\n    ONNXTRT_CHECK_NODE((cDims.nbDims == xDims.nbDims),\n        \"The rank of the condition input tensor must be the same of the input x tensor. Rank of the condition input \"\n        \"tensor = \"\n            << cDims.nbDims << \", rank of the input x tensor = \" << xDims.nbDims << \".\",\n        node, nodeIdx, ErrorCode::kINVALID_NODE);\n    ONNXTRT_CHECK_NODE((cDims.nbDims == yDims.nbDims),\n        \"The rank of the condition input tensor must be the same of the input y tensor. Rank of the condition input \"\n        \"tensor = \"\n            << cDims.nbDims << \", rank of the input y tensor = \" << yDims.nbDims << \".\",\n        node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    auto* layer = N_CHECK(ctx->network()->addSelect(*condition, *x, *y));\n    ctx->registerLayer(layer, node);\n\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\n// Copies the given field into the fieldData map, returns data and number of T elements in the vector in which the data\n// was copied into.\ntemplate <typename T>\nstd::tuple<void const*, size_t> copyField(\n    T const& field, std::string const& fieldName, StringMap<std::vector<uint8_t>>& fieldData)\n{\n    constexpr size_t nbBytes{sizeof(T)};\n    fieldData[fieldName].resize(nbBytes);\n    std::memcpy(fieldData[fieldName].data(), &field, nbBytes);\n    return std::make_tuple(fieldData[fieldName].data(), fieldData[fieldName].size() / nbBytes);\n}\n\ntemplate <typename T>\nstd::tuple<void const*, size_t> copyField(\n    std::vector<T> const& repeatedField, std::string const& fieldName, StringMap<std::vector<uint8_t>>& fieldData)\n{\n    size_t const nbBytes{sizeof(T) * repeatedField.size()};\n    fieldData[fieldName].resize(nbBytes);\n    std::memcpy(fieldData[fieldName].data(), repeatedField.data(), nbBytes);\n    return std::make_tuple(fieldData[fieldName].data(), fieldData[fieldName].size() / sizeof(T));\n}\n\nstd::tuple<void const*, size_t> copyField(\n    std::string const& field, std::string const& fieldName, StringMap<std::vector<uint8_t>>& fieldData)\n{\n    static_assert(sizeof(std::string::value_type) == sizeof(uint8_t), \"String type does not have 1 byte elements\");\n    std::copy(field.begin(), field.end(), std::back_inserter(fieldData[fieldName]));\n    // Append \\0 as end of C style string.\n    fieldData[fieldName].push_back('\\0');\n    return std::make_tuple(fieldData[fieldName].data(), fieldData[fieldName].size());\n}\n\nstd::tuple<void const*, size_t> copyField(std::vector<std::string> const& repeatedField, std::string const& fieldName,\n    StringMap<std::vector<uint8_t>>& fieldData)\n{\n    static_assert(sizeof(std::string::value_type) == sizeof(uint8_t), \"String type does not have 1 byte elements\");\n    for (auto const& field : repeatedField)\n    {\n        std::copy(field.begin(), field.end(), std::back_inserter(fieldData[fieldName]));\n        // Append \\0 as end of C style string.\n        fieldData[fieldName].push_back('\\0');\n    }\n    return std::make_tuple(fieldData[fieldName].data(), fieldData[fieldName].size());\n}\n\nstd::tuple<void const*, size_t> copyField(\n    ShapedWeights const& field, std::string const& fieldName, StringMap<std::vector<uint8_t>>& fieldData)\n{\n    // Weights do not require a copy\n    return std::make_tuple(field.values, field.count());\n}\n\n// Load plugin fields from an ONNX node, using fieldData for temporary allocations.\nstd::vector<nvinfer1::PluginField> loadFields(StringMap<std::vector<uint8_t>>& fieldData, OnnxAttrs const& attrs,\n    nvinfer1::PluginFieldCollection const* fieldNames, ImporterContext* ctx)\n{\n    std::vector<nvinfer1::PluginField> fields{};\n    for (int32_t i = 0; i < fieldNames->nbFields; ++i)\n    {\n        // Some plugins may have default values for fields that map to optional attributes in an ONNX graph.\n        if (!attrs.count(fieldNames->fields[i].name))\n        {\n            LOG_WARNING(\"Attribute \" << fieldNames->fields[i].name\n                                     << \" not found in plugin node! Ensure that the plugin creator has a default value \"\n                                        \"defined or the engine may fail to build.\");\n            continue;\n        }\n\n        // Name must be retrieved from the map so that it is alive for long enough.\n        std::string const& fieldName\n            = fieldData.emplace(fieldNames->fields[i].name, std::vector<uint8_t>{}).first->first;\n        void const* data{nullptr};\n        int32_t length{0};\n        nvinfer1::PluginFieldType type{};\n        switch (attrs.type(fieldName))\n        {\n        case ::ONNX_NAMESPACE::AttributeProto::FLOAT:\n            std::tie(data, length) = copyField(attrs.get<float>(fieldName), fieldName, fieldData);\n            type = nvinfer1::PluginFieldType::kFLOAT32;\n            break;\n        case ::ONNX_NAMESPACE::AttributeProto::INT:\n            std::tie(data, length) = copyField(attrs.get<int32_t>(fieldName), fieldName, fieldData);\n            type = nvinfer1::PluginFieldType::kINT32;\n            break;\n        case ::ONNX_NAMESPACE::AttributeProto::STRING:\n            std::tie(data, length) = copyField(attrs.get<std::string>(fieldName), fieldName, fieldData);\n            type = nvinfer1::PluginFieldType::kCHAR;\n            break;\n        case ::ONNX_NAMESPACE::AttributeProto::FLOATS:\n            std::tie(data, length) = copyField(attrs.get<std::vector<float>>(fieldName), fieldName, fieldData);\n            type = nvinfer1::PluginFieldType::kFLOAT32;\n            break;\n        case ::ONNX_NAMESPACE::AttributeProto::INTS:\n            std::tie(data, length) = copyField(attrs.get<std::vector<int>>(fieldName), fieldName, fieldData);\n            type = nvinfer1::PluginFieldType::kINT32;\n            break;\n        case ::ONNX_NAMESPACE::AttributeProto::STRINGS:\n            std::tie(data, length) = copyField(attrs.get<std::vector<std::string>>(fieldName), fieldName, fieldData);\n            type = nvinfer1::PluginFieldType::kCHAR;\n            break;\n        case ::ONNX_NAMESPACE::AttributeProto::TENSOR:\n        {\n            ShapedWeights tensor{attrs.get<ShapedWeights>(fieldName)};\n            std::tie(data, length) = copyField(tensor, fieldName, fieldData);\n            switch (tensor.type)\n            {\n            case ::ONNX_NAMESPACE::TensorProto::FLOAT: type = nvinfer1::PluginFieldType::kFLOAT32; break;\n            case ::ONNX_NAMESPACE::TensorProto::INT8: type = nvinfer1::PluginFieldType::kINT8; break;\n            case ::ONNX_NAMESPACE::TensorProto::INT16: type = nvinfer1::PluginFieldType::kINT16; break;\n            case ::ONNX_NAMESPACE::TensorProto::INT32: type = nvinfer1::PluginFieldType::kINT32; break;\n            case ::ONNX_NAMESPACE::TensorProto::STRING: type = nvinfer1::PluginFieldType::kCHAR; break;\n            case ::ONNX_NAMESPACE::TensorProto::FLOAT16: type = nvinfer1::PluginFieldType::kFLOAT16; break;\n            case ::ONNX_NAMESPACE::TensorProto::BFLOAT16: type = nvinfer1::PluginFieldType::kBF16; break;\n            case ::ONNX_NAMESPACE::TensorProto::DOUBLE: type = nvinfer1::PluginFieldType::kFLOAT64; break;\n            case ::ONNX_NAMESPACE::TensorProto::UNDEFINED:\n            case ::ONNX_NAMESPACE::TensorProto::UINT8:\n            case ::ONNX_NAMESPACE::TensorProto::UINT16:\n            case ::ONNX_NAMESPACE::TensorProto::INT64:\n            case ::ONNX_NAMESPACE::TensorProto::BOOL:\n            case ::ONNX_NAMESPACE::TensorProto::UINT32:\n            case ::ONNX_NAMESPACE::TensorProto::UINT64:\n            case ::ONNX_NAMESPACE::TensorProto::COMPLEX64:\n            case ::ONNX_NAMESPACE::TensorProto::COMPLEX128:\n                // ::ONNX_NAMESPACE::TensorProto::DataType_Name function not available in protobuf-lite.\n                std::stringstream ss{};\n                ss << \"Tensor type: \";\n#if USE_LITE_PROTOBUF\n                ss << static_cast<::ONNX_NAMESPACE::TensorProto::DataType>(tensor.type);\n#else\n                ss << ::ONNX_NAMESPACE::TensorProto::DataType_Name(\n                    static_cast<::ONNX_NAMESPACE::TensorProto::DataType>(tensor.type));\n#endif // USE_LITE_PROTOBUF\n                ss << \" is unsupported in plugin fields.\" << std::endl;\n                MAKE_ERROR(ss.str(), ErrorCode::kUNSUPPORTED_NODE);\n            }\n            break;\n        }\n        case ::ONNX_NAMESPACE::AttributeProto::UNDEFINED:\n        case ::ONNX_NAMESPACE::AttributeProto::SPARSE_TENSOR:\n        case ::ONNX_NAMESPACE::AttributeProto::GRAPH:\n        case ::ONNX_NAMESPACE::AttributeProto::TENSORS:\n        case ::ONNX_NAMESPACE::AttributeProto::SPARSE_TENSORS:\n        case ::ONNX_NAMESPACE::AttributeProto::GRAPHS:\n        case ::ONNX_NAMESPACE::AttributeProto::TYPE_PROTO:\n        case ::ONNX_NAMESPACE::AttributeProto::TYPE_PROTOS:\n            // ::ONNX_NAMESPACE::AttributeProto::AttributeType_Name function not available in protobuf-lite.\n            std::stringstream ss{};\n            ss << \"Attributes of type: \";\n#if USE_LITE_PROTOBUF\n            ss << attrs.type(fieldName);\n#else\n            ss << ::ONNX_NAMESPACE::AttributeProto::AttributeType_Name(attrs.type(fieldName));\n#endif // USE_LITE_PROTOBUF\n            ss << \" are unsupported as plugin fields.\" << std::endl;\n            MAKE_ERROR(ss.str(), ErrorCode::kUNSUPPORTED_NODE);\n        }\n        fields.emplace_back(fieldName.c_str(), data, type, length);\n    }\n    return fields;\n}\n\nnvinfer1::IPluginV2Layer* addPluginLayer(ImporterContext* ctx, std::vector<nvinfer1::ITensor*> const& pluginInputs,\n    std::vector<nvinfer1::ITensor*> const& /* pluginShapeInputs */, nvinfer1::IPluginV2& plugin)\n{\n    return N_CHECK(ctx->network()->addPluginV2(pluginInputs.data(), pluginInputs.size(), plugin));\n}\n\nnvinfer1::IPluginV3Layer* addPluginLayer(ImporterContext* ctx, std::vector<nvinfer1::ITensor*> const& pluginInputs,\n    std::vector<nvinfer1::ITensor*> const& pluginShapeInputs, nvinfer1::IPluginV3& plugin)\n{\n    return N_CHECK(ctx->network()->addPluginV3(\n        pluginInputs.data(), pluginInputs.size(), pluginShapeInputs.data(), pluginShapeInputs.size(), plugin));\n}\n\ntemplate <typename TPluginCreator>\nNodeOutputs addPluginWithCreator(ImporterContext* ctx, ::ONNX_NAMESPACE::NodeProto const& node, size_t const& nodeIdx,\n    std::string const& pluginNamespace, std::vector<TensorOrWeights>& inputs, OnnxAttrs const& attrs,\n    nvinfer1::IPluginCreatorInterface* creator)\n{\n    ONNXTRT_CHECK_NODE(creator, \"Invalid plugin creator.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    nvinfer1::PluginFieldCollection const* fieldNames = static_cast<TPluginCreator*>(creator)->getFieldNames();\n\n    StringMap<std::vector<uint8_t>> fieldData{};\n    std::vector<nvinfer1::PluginField> fields = loadFields(fieldData, attrs, fieldNames, ctx);\n\n    std::unordered_set<int32_t> shapeInputIdxsSet{};\n    auto const nbInputs{static_cast<int32_t>(inputs.size())};\n\n    if (attrs.count(\"tensorrt_plugin_shape_input_indices\"))\n    {\n        if (std::strcmp(creator->getInterfaceInfo().kind, \"PLUGIN CREATOR_V1\") != 0)\n        {\n            ONNXTRT_CHECK_NODE(\n                attrs.type(\"tensorrt_plugin_shape_input_indices\") == ::ONNX_NAMESPACE::AttributeProto::INTS,\n                \"Shape input indices defined, but attribute tensorrt_plugin_shape_input_indices has unsupported type. \"\n                \"Only AttributeProto::INTS is supported.\",\n                node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n            auto shapeInputIdxsVec = attrs.get<std::vector<int32_t>>(\"tensorrt_plugin_shape_input_indices\");\n            std::vector<bool> issuedWarningIdxs(nbInputs, false);\n            for (auto const& curr : shapeInputIdxsVec)\n            {\n                // check for out-of-range indices: index must be in [-n, n-1] where n = number of inputs.\n                ONNXTRT_CHECK_NODE((curr < nbInputs) && (curr >= (-nbInputs)),\n                    \"Out-of-range shape input index: \" << curr, node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n                auto const& e = (curr >= 0) ? curr : nbInputs + curr;\n                auto res = shapeInputIdxsSet.insert(e);\n                if (!res.second && !issuedWarningIdxs[e])\n                {\n                    LOG_WARNING(\"Node \" << getNodeName(node) << \" has duplicate shape input index: \" << curr);\n                    issuedWarningIdxs[e] = true;\n                }\n            }\n        }\n        else\n        {\n            LOG_WARNING(\"Node \" << getNodeName(node)\n                                << \" has shape input indices defined, but plugin creator is for an \"\n                                   \"IPluginV2-derivative plugin. Ignoring shape input indices.\");\n        }\n    }\n\n    std::string const pluginName{node.op_type()};\n\n    auto const plugin = createPlugin(getNodeName(node), pluginNamespace, static_cast<TPluginCreator*>(creator), fields);\n\n    ONNXTRT_CHECK_NODE(plugin, \"Could not create the plugin.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    std::vector<nvinfer1::ITensor*> pluginInputs{};\n    std::vector<nvinfer1::ITensor*> pluginShapeInputs{};\n\n    for (int32_t idx{}; idx < nbInputs; ++idx)\n    {\n        auto input = inputs[idx];\n        auto pluginInputVec\n            = shapeInputIdxsSet.find(idx) != shapeInputIdxsSet.end() ? &pluginShapeInputs : &pluginInputs;\n        if (input.isNullTensor())\n        {\n            LOG_VERBOSE(\"Found unset input for \" << pluginName << \".\");\n            pluginInputVec->push_back(nullptr);\n            continue;\n        }\n        nvinfer1::ITensor* inputTensor = &convertToTensor(input, ctx);\n        if (onlySupportInt32TRTPlugin(pluginName) && inputTensor->getType() == nvinfer1::DataType::kINT64)\n        {\n            LOG_VERBOSE(\"The TRT plugin (\" << pluginName << \") doesn't support INT64 for inputs, will cast to INT32.\");\n            pluginInputVec->emplace_back(castHelper(ctx, inputTensor, nvinfer1::DataType::kINT32));\n        }\n        else\n        {\n            pluginInputVec->emplace_back(inputTensor);\n        }\n    }\n\n    LOG_INFO(\"Successfully created plugin: \" << pluginName);\n\n    auto* layer = addPluginLayer(ctx, pluginInputs, pluginShapeInputs, *plugin);\n    ONNXTRT_CHECK_NODE(layer, \"Could not add the plugin layer.\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    ctx->registerLayer(layer, node);\n    RETURN_ALL_OUTPUTS(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Xor)\n{\n    return elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kXOR);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Shrink)\n{\n    nvinfer1::ITensor* x = &convertToTensor(inputs.at(0), ctx);\n\n    auto originalType = x->getType();\n    ONNXTRT_CHECK_NODE(\n        (originalType == DataType::kFLOAT || originalType == DataType::kHALF || originalType == DataType::kINT8\n            || originalType == DataType::kINT32 || originalType == DataType::kINT64),\n        \"Only FLOAT, HALF, INT8, INT32, and INT64 are supported in Shrink. The current type = \"\n            + getTrtDtypeName(originalType) + \".\",\n        node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    x = castHelper(ctx, x, DataType::kFLOAT);\n\n    // get attrs\n    OnnxAttrs attrs(node, ctx);\n    float const lambd = attrs.get<float>(\"lambd\", 0.5F);\n    float const bias = attrs.get<float>(\"bias\", 0.0F);\n\n    // prepare Constant Tensors\n    nvinfer1::ITensor* lambdTensor\n        = addConstant(ctx, std::vector<float>{lambd}, ::ONNX_NAMESPACE::TensorProto::FLOAT, {0, {1}})->getOutput(0);\n    broadcastTensors(ctx, lambdTensor, x); // align rank\n\n    nvinfer1::ITensor* negLambdTensor\n        = addConstant(ctx, std::vector<float>{-lambd}, ::ONNX_NAMESPACE::TensorProto::FLOAT, {0, {1}})->getOutput(0);\n    broadcastTensors(ctx, negLambdTensor, x);\n\n    nvinfer1::ITensor* biasTensor\n        = addConstant(ctx, std::vector<float>{bias}, ::ONNX_NAMESPACE::TensorProto::FLOAT, {0, {1}})->getOutput(0);\n    broadcastTensors(ctx, biasTensor, x);\n\n    nvinfer1::ITensor* zeroTensor\n        = addConstant(ctx, std::vector<float>{0.}, ::ONNX_NAMESPACE::TensorProto::FLOAT, {0, {1}})->getOutput(0);\n    broadcastTensors(ctx, zeroTensor, x);\n\n    // If x > lambd, y = x - bias; Otherwise, y = 0\n    std::vector<TensorOrWeights> xGreaterThanLambd\n        = elementwiseHelper(ctx, node, nodeIdx, {x, lambdTensor}, nvinfer1::ElementWiseOperation::kGREATER);\n\n    std::vector<TensorOrWeights> xMinusBias\n        = elementwiseHelper(ctx, node, nodeIdx, {x, biasTensor}, nvinfer1::ElementWiseOperation::kSUB);\n\n    auto firstSelectLayer = N_CHECK(ctx->network()->addSelect(\n        convertToTensor(xGreaterThanLambd.at(0), ctx), convertToTensor(xMinusBias.at(0), ctx), *zeroTensor));\n    nvinfer1::ITensor* output = N_CHECK(firstSelectLayer->getOutput(0));\n\n    // If x < -lambd, y = x + bias;\n    std::vector<TensorOrWeights> xLessThanMinusLambd\n        = elementwiseHelper(ctx, node, nodeIdx, {x, negLambdTensor}, nvinfer1::ElementWiseOperation::kLESS);\n\n    std::vector<TensorOrWeights> xAddBias\n        = elementwiseHelper(ctx, node, nodeIdx, {x, biasTensor}, nvinfer1::ElementWiseOperation::kSUM);\n\n    auto* layer = N_CHECK(ctx->network()->addSelect(\n        convertToTensor(xLessThanMinusLambd.at(0), ctx), convertToTensor(xAddBias.at(0), ctx), *output));\n    ONNXTRT_CHECK_NODE(layer, \"Failed to create layer\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    ctx->registerLayer(layer, node);\n\n    // cast back to originalType\n    return {{castHelper(ctx, N_CHECK(layer->getOutput(0)), originalType)}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(HardSwish)\n{\n    nvinfer1::ITensor* x = &convertToTensor(inputs.at(0), ctx);\n    ONNXTRT_CHECK_NODE(\n        (x->getType() == DataType::kFLOAT || x->getType() == DataType::kHALF || x->getType() == DataType::kINT8),\n        \"Only FLOAT, HALF or INT8 input is supported for the HardSwish operator in this version of TensorRT. \"\n        \"The current type = \"\n            + getTrtDtypeName(x->getType()) + \".\",\n        node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    // activationHelper does not support const and constexpr (compile failed)\n    float kALPHA{1.F / 6};\n    float kBETA{0.5F};\n    std::vector<TensorOrWeights> activationResult\n        = activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kHARD_SIGMOID, &kALPHA, &kBETA);\n\n    return elementwiseHelper(ctx, node, nodeIdx, {x, activationResult.at(0)}, nvinfer1::ElementWiseOperation::kPROD);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(NonZero)\n{\n    nvinfer1::ITensor* x = &convertToTensor(inputs.at(0), ctx);\n    auto const t = x->getType();\n    ONNXTRT_CHECK_NODE((t == DataType::kFLOAT || t == DataType::kHALF || t == DataType::kBF16 || t == DataType::kINT32\n                           || t == DataType::kINT64 || t == DataType::kINT8 || t == DataType::kBOOL),\n        \"Only FLOAT32, FLOAT16, BFLOAT16, INT32, INT64, INT8 or BOOL input is supported for the NonZero operator in \"\n        \"this version of TensorRT. The current type is \"\n            + getTrtDtypeName(t) + \".\",\n        node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    auto* layer = N_CHECK(ctx->network()->addNonZero(*x));\n    ctx->registerLayer(layer, node);\n    return {{castHelper(ctx, N_CHECK(layer->getOutput(0)), DataType::kINT64)}};\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(Mish)\n{\n    nvinfer1::ITensor* x = &convertToTensor(inputs.at(0), ctx);\n    ONNXTRT_CHECK_NODE((x->getType() == DataType::kFLOAT || x->getType() == DataType::kHALF),\n        \"Only FLOAT32 or FLOAT16 input is supported for the Mish operator in this version of TensorRT. \"\n        \"The current type = \"\n            + getTrtDtypeName(x->getType()) + \".\",\n        node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n\n    std::vector<TensorOrWeights> softPlusOutput\n        = activationHelper(ctx, node, nodeIdx, inputs, nvinfer1::ActivationType::kSOFTPLUS);\n\n    std::vector<TensorOrWeights> tanhOutput\n        = activationHelper(ctx, node, nodeIdx, softPlusOutput, nvinfer1::ActivationType::kTANH);\n\n    return elementwiseHelper(ctx, node, nodeIdx, {x, tanhOutput.at(0)}, nvinfer1::ElementWiseOperation::kPROD);\n}\n\n// Any ops that are not supported will attempt to import as plugins.\nDEFINE_BUILTIN_OP_IMPORTER(FallbackPluginImporter)\n{\n    OnnxAttrs attrs(node, ctx);\n    std::string const pluginName{node.op_type()};\n    std::string const pluginVersion{attrs.get<std::string>(\"plugin_version\", \"1\")};\n    std::string const pluginNamespace{attrs.get<std::string>(\"plugin_namespace\", \"\")};\n\n    LOG_INFO(\"Searching for plugin: \" << pluginName << \", plugin_version: \" << pluginVersion\n                                      << \", plugin_namespace: \" << pluginNamespace);\n    nvinfer1::IPluginCreatorInterface* creator = importPluginCreator(ctx, pluginName, pluginVersion, pluginNamespace);\n    ONNXTRT_CHECK_NODE(creator, \"Plugin not found, are the plugin name, version, and namespace correct?\", node, nodeIdx,\n        ErrorCode::kUNSUPPORTED_NODE);\n\n    auto const creatorVersion = getPluginCreatorVersion(creator);\n\n    switch (creatorVersion)\n    {\n    case CreatorVersion::kV1:\n    {\n        return addPluginWithCreator<nvinfer1::IPluginCreator>(\n            ctx, node, nodeIdx, pluginNamespace, inputs, attrs, creator);\n    }\n    case CreatorVersion::kV3ONE:\n    {\n        return addPluginWithCreator<nvinfer1::IPluginCreatorV3One>(\n            ctx, node, nodeIdx, pluginNamespace, inputs, attrs, creator);\n    }\n    case CreatorVersion::kV3QUICK:\n    {\n        return addPluginWithCreator<nvinfer1::IPluginCreatorV3Quick>(\n            ctx, node, nodeIdx, pluginNamespace, inputs, attrs, creator);\n    }\n    default: ONNXTRT_CHECK(false && \"Unsupported plugin creator version.\", ErrorCode::kUNSUPPORTED_NODE);\n    }\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(LocalFunctionImporter)\n{\n    auto function = ctx->localFunctions().at(node.op_type());\n    ONNXTRT_CHECK_NODE(node.input().size() == function.input().size(),\n        \"LocalFunction has an unexpected number of inputs! Number of node inputs = \"\n            << node.input().size() << \", number of function inputs = \" << function.input().size() << \".\",\n        node, nodeIdx, ErrorCode::kINVALID_NODE);\n\n    // Create a namescope local to the subgraph in the local function\n    NameScope nameScope(*ctx);\n\n    // We need to map local input names to the tensors from the outside scope. Keep track of\n    // local input names in order to remove them later\n    std::vector<std::string> localInputs;\n    for (int32_t i = 0; i < function.input().size(); i++)\n    {\n        auto outsideScopeName = node.input(i);\n        auto insideScopeName = function.input(i);\n        if (outsideScopeName != insideScopeName)\n        {\n            if (ctx->tensors().count(insideScopeName))\n            {\n                LOG_WARNING(\"Found input: \"\n                    << insideScopeName\n                    << \" that does not correspond to an outside scope name. Behavior may be incorrect.\");\n                continue;\n            }\n            ctx->tensors().insert({insideScopeName, ctx->tensors().at(outsideScopeName)});\n            localInputs.push_back(insideScopeName);\n        }\n        ONNXTRT_CHECK_NODE(ctx->tensors().count(insideScopeName), \"Could not find mapping of local function input!\",\n            node, nodeIdx, ErrorCode::kINVALID_NODE);\n    }\n\n    // Create attribute map for the local function instance. Attributes can have default values (from the parent\n    // FunctionProto) or local values (from the NodeProto instance of the Function).\n\n    StringMap<::ONNX_NAMESPACE::AttributeProto const*> attrMap;\n    // Add local values first as they override any default values.\n    for (auto const& attr : node.attribute())\n    {\n        attrMap.insert({attr.name(), &attr});\n    }\n    // Add default values\n    for (auto const& attr : function.attribute_proto())\n    {\n        attrMap.insert({attr.name(), &attr});\n    }\n\n    // Push current function name to top of stack in order to properly set layer metadata and track attributes\n    ctx->localFunctionStack().push_back({node.op_type(), getNodeName(node), attrMap});\n\n    // Log current stack of functions for debugging nested functions.\n    auto prettyPrintFunctionStack = [ctx]() {\n        std::stringstream stackStream;\n        stackStream << \"Function stack: [\";\n        size_t stackSize = ctx->localFunctionStack().size();\n        for (size_t i = 0; i < stackSize; i++)\n        {\n            auto const& func = ctx->localFunctionStack()[i];\n            stackStream << func.nodeName << \" (\" << func.functionName << \")\";\n            if (i != stackSize - 1)\n            {\n                stackStream << \", \";\n            }\n        }\n        stackStream << \"]\";\n        return stackStream.str();\n    };\n\n    LOG_VERBOSE(prettyPrintFunctionStack());\n\n    for (auto const& node : function.node())\n    {\n        try\n        {\n            onnx2trt::parseNode(ctx, node, nodeIdx);\n        }\n        catch (OnnxTrtException& e)\n        {\n            if (ctx->localFunctions().count(node.op_type()))\n            {\n                ctx->localFunctionStack().pop_back();\n                ONNXTRT_THROW(e.getStatus());\n            }\n            else\n            {\n                // This is a leaf node. Add local function stack to error log.\n                size_t stackSize = ctx->localFunctionStack().size();\n                std::vector<std::string> localFunctionStackString{};\n                std::vector<char const*> localFunctionStackChar{};\n                for (size_t i = 0; i < stackSize; i++)\n                {\n                    auto const& func = ctx->localFunctionStack()[i];\n                    localFunctionStackString.push_back(func.nodeName + \" (\" + func.functionName + \")\");\n                }\n                ctx->localFunctionErrors().push_back(localFunctionStackString);\n                for (size_t i = 0; i < stackSize; i++)\n                {\n                    localFunctionStackChar.push_back(ctx->localFunctionErrors().back()[i].c_str());\n                }\n                ctx->localFunctionStack().pop_back();\n\n                Status status = e.getStatus();\n                ONNXTRT_THROW(Status(status.code(), std::string(status.desc()), std::string(status.file()),\n                    status.line(), std::string(status.func()), status.node(), std::string(status.nodeName()),\n                    std::string(status.nodeOperator()), localFunctionStackChar));\n            }\n        }\n    }\n\n    // Create output vector\n    std::vector<TensorOrWeights> outputs;\n    for (auto const& output : function.output())\n    {\n        ONNXTRT_CHECK_NODE(\n            ctx->tensors().count(output), \"Could not find output tensor!\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n        outputs.push_back(TensorOrWeights(ctx->tensors().at(output)));\n    }\n\n    // Remove all localInputs as we exit the local function scope, and pop the current function name from the stack\n    for (auto& name : localInputs)\n    {\n        ctx->tensors().erase(name);\n    }\n    ctx->localFunctionStack().pop_back();\n\n    return outputs;\n}\n\n// INetwork Serialization importer functions - TODO: Move to it's own file?\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_Scale)\n{\n    ONNXTRT_CHECK_NODE((inputs.size() >= 1), \"Input is required.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    ONNXTRT_CHECK_NODE(\n        (inputs.at(0).is_tensor()), \"The first input must be a tensor.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    if (inputs.size() >= 2)\n    {\n        ONNXTRT_CHECK_NODE((inputs.at(1).is_weights()), \"The second input must be an initializer.\", node, nodeIdx,\n            nvonnxparser::ErrorCode::kINVALID_NODE);\n    }\n    auto& input = inputs.at(0).tensor();\n\n    OnnxAttrs attrs(node, ctx);\n    int32_t counter = 1;\n\n    nvinfer1::ScaleMode mode = attrs.get<nvinfer1::ScaleMode>(\"mode\");\n\n    // check if there's no weights at all\n    // if no weights, just choose datatype of the input tensor\n    // This is based on the assumption that weights should be\n    // the same datatype as inputs\n    auto type = inputs.size() > 1 ? inputs.at(1).weights().type : trtDataTypeToONNX(inputs.at(0).tensor().getType());\n\n    auto scale = ShapedWeights::empty(type);\n    auto shift = ShapedWeights::empty(type);\n    auto power = ShapedWeights::empty(type);\n\n    if (attrs.get<bool>(\"scale\"))\n    {\n        ONNXTRT_CHECK_NODE((inputs.at(counter).is_weights()), \"The scale input must be an initializer.\", node, nodeIdx,\n            nvonnxparser::ErrorCode::kINVALID_NODE);\n        scale = inputs.at(counter++).weights();\n    }\n    if (attrs.get<bool>(\"shift\"))\n    {\n        ONNXTRT_CHECK_NODE((inputs.at(counter).is_weights()), \"The shift input must be an initializer.\", node, nodeIdx,\n            nvonnxparser::ErrorCode::kINVALID_NODE);\n        shift = inputs.at(counter++).weights();\n    }\n    if (attrs.get<bool>(\"power\"))\n    {\n        ONNXTRT_CHECK_NODE((inputs.at(counter).is_weights()), \"The power input must be an initializer.\", node, nodeIdx,\n            nvonnxparser::ErrorCode::kINVALID_NODE);\n        power = inputs.at(counter++).weights();\n    }\n\n    nvinfer1::IScaleLayer* layer = N_CHECK(ctx->network()->addScale(input, mode, shift, scale, power));\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_Shuffle)\n{\n    ONNXTRT_CHECK_NODE(\n        inputs.at(0).is_tensor(), \"The first input must be a tensor.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    auto& input = inputs.at(0).tensor();\n\n    OnnxAttrs attrs(node, ctx);\n    nvinfer1::Permutation perm1 = attrs.get<nvinfer1::Permutation>(\"first_perm\");\n    nvinfer1::Permutation perm2 = attrs.get<nvinfer1::Permutation>(\"second_perm\");\n    bool zeroIsPlaceholder = attrs.get<bool>(\"zero_is_placeholder\");\n\n    nvinfer1::IShuffleLayer* layer = N_CHECK(ctx->network()->addShuffle(input));\n    ONNXTRT_CHECK_NODE(layer, \"Failed to create layer\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    ctx->registerLayer(layer, node);\n    layer->setFirstTranspose(perm1);\n    layer->setSecondTranspose(perm2);\n    layer->setZeroIsPlaceholder(zeroIsPlaceholder);\n\n    if (inputs.size() == 1)\n    {\n        if (attrs.count(\"reshape_dims\"))\n        {\n            nvinfer1::Dims reshapeDims = attrs.get<nvinfer1::Dims>(\"reshape_dims\");\n            layer->setReshapeDimensions(reshapeDims);\n        }\n    }\n    else\n    {\n        ONNXTRT_CHECK_NODE(inputs.at(1).is_tensor(), \"The second input must be a tensor.\", node, nodeIdx,\n            nvonnxparser::ErrorCode::kINVALID_NODE);\n        layer->setInput(1, inputs.at(1).tensor());\n    }\n\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_TopK_Min)\n{\n    ONNXTRT_CHECK_NODE(\n        inputs.at(0).is_tensor(), \"The first input must be a tensor.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    auto& input = inputs.at(0).tensor();\n\n    OnnxAttrs attrs(node, ctx);\n    ONNXTRT_CHECK_NODE(inputs.at(1).is_weights(), \"The second input must be an initializer.\", node, nodeIdx,\n        nvonnxparser::ErrorCode::kINVALID_NODE);\n    auto& kWeights = inputs.at(1).weights();\n    int k = *static_cast<int*>(kWeights.values);\n\n    int32_t axes = 1 << (attrs.get<int32_t>(\"axis\"));\n\n    nvinfer1::ITopKLayer* layer = N_CHECK(ctx->network()->addTopK(input, nvinfer1::TopKOperation::kMIN, k, axes));\n    ctx->registerLayer(layer, node);\n\n    RETURN_ALL_OUTPUTS(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_MatMul)\n{\n    ONNXTRT_CHECK_NODE(\n        inputs.at(0).is_tensor(), \"The first input must be a tensor.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    ONNXTRT_CHECK_NODE(\n        inputs.at(1).is_tensor(), \"The second input must be a tensor.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    auto& input0 = inputs.at(0).tensor();\n    auto& input1 = inputs.at(1).tensor();\n\n    OnnxAttrs attrs(node, ctx);\n    nvinfer1::MatrixOperation op0 = attrs.get<nvinfer1::MatrixOperation>(\"op_0\");\n    nvinfer1::MatrixOperation op1 = attrs.get<nvinfer1::MatrixOperation>(\"op_1\");\n\n    nvinfer1::IMatrixMultiplyLayer* layer = N_CHECK(ctx->network()->addMatrixMultiply(input0, op0, input1, op1));\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_RaggedSoftmax)\n{\n    ONNXTRT_CHECK_NODE(\n        inputs.at(0).is_tensor(), \"The first input must be a tensor.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    ONNXTRT_CHECK_NODE(\n        inputs.at(1).is_tensor(), \"The second input must be a tensor.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    auto& input = inputs.at(0).tensor();\n    auto& bounds = inputs.at(1).tensor();\n\n    nvinfer1::IRaggedSoftMaxLayer* layer = N_CHECK(ctx->network()->addRaggedSoftMax(input, bounds));\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_MaxAverageBlendPool)\n{\n    ONNXTRT_CHECK_NODE(\n        inputs.at(0).is_tensor(), \"The first input must be a tensor.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    auto& input = inputs.at(0).tensor();\n\n    OnnxAttrs attrs(node, ctx);\n    int32_t nbSpatialDims = attrs.get<nvinfer1::Dims>(\"kernel_shape\").nbDims;\n    nvinfer1::Dims kernelSize = makeDims(nbSpatialDims, 1);\n    nvinfer1::Dims strides = makeDims(nbSpatialDims, 1);\n    nvinfer1::Dims begPadding = makeDims(nbSpatialDims, 0);\n    nvinfer1::Dims endPadding = makeDims(nbSpatialDims, 0);\n    nvinfer1::PaddingMode paddingMode;\n    bool excludePadding(true);\n    getKernelParams(ctx, node, &kernelSize, &strides, &begPadding, &endPadding, paddingMode, excludePadding);\n    float blend = attrs.get<float>(\"blend\");\n\n    nvinfer1::IPoolingLayer* layer\n        = N_CHECK(ctx->network()->addPoolingNd(input, nvinfer1::PoolingType::kMAX_AVERAGE_BLEND, kernelSize));\n    ctx->registerLayer(layer, node);\n    layer->setStrideNd(strides);\n    layer->setAverageCountExcludesPadding(excludePadding);\n    layer->setPaddingMode(paddingMode);\n\n    layer->setPrePadding(begPadding);\n    layer->setPostPadding(endPadding);\n\n    layer->setBlendFactor(blend);\n\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\n#if ENABLE_STD_PLUGIN\nDEFINE_BUILTIN_OP_IMPORTER(TRT_PluginV2)\n{\n    OnnxAttrs attrs(node, ctx);\n\n    nvinfer1::IPluginRegistry& registry = ctx->network()->getBuilder().getPluginRegistry();\n\n    std::string name = attrs.get<std::string>(\"name\");\n    std::string version = attrs.get<std::string>(\"version\");\n    std::string nspace = attrs.get<std::string>(\"namespace\");\n    std::string buffer = attrs.get<std::string>(\"data\");\n\n    nvinfer1::IPluginCreator* creator = registry.getPluginCreator(name.c_str(), version.c_str(), nspace.c_str());\n    ONNXTRT_CHECK_NODE(creator, \"Plugin not found, are the plugin name, version, and namespace correct?\", node, nodeIdx,\n        nvonnxparser::ErrorCode::kINVALID_NODE);\n\n    auto const plugin = creator->deserializePlugin(\"\", buffer.data(), buffer.size());\n\n    std::vector<nvinfer1::ITensor*> tensors;\n    for (auto& input : inputs)\n    {\n        ONNXTRT_CHECK_NODE(input.is_tensor(), \"The input must be a tensor.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n        nvinfer1::ITensor* inputTensor = &input.tensor();\n        if (onlySupportInt32TRTPlugin(name) && inputTensor->getType() == DataType::kINT64)\n        {\n            LOG_VERBOSE(\"The TRT plugin (\" << name << \") doesn't support INT64 for inputs, will cast to INT32.\");\n            tensors.emplace_back(castHelper(ctx, inputTensor, DataType::kINT32));\n        }\n        else\n        {\n            tensors.emplace_back(inputTensor);\n        }\n    }\n\n    nvinfer1::IPluginV2Layer* layer = N_CHECK(ctx->network()->addPluginV2(tensors.data(), tensors.size(), *plugin));\n    ctx->registerLayer(layer, node);\n    RETURN_ALL_OUTPUTS(layer, node, nodeIdx);\n}\n#endif // ENABLE_STD_PLUGIN\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_Gather)\n{\n    ONNXTRT_CHECK_NODE(\n        inputs.at(0).is_tensor(), \"The first input must be a tensor.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    ONNXTRT_CHECK_NODE(\n        inputs.at(1).is_tensor(), \"The second input must be a tensor.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    auto& data = inputs.at(0).tensor();\n    auto& indices = inputs.at(1).tensor();\n    OnnxAttrs attrs(node, ctx);\n    int32_t axis = attrs.get<int32_t>(\"axis\", 0);\n    int32_t nbElementWiseDims = attrs.get<int32_t>(\"nbElementWiseDims\", 0);\n    int32_t r = data.getDimensions().nbDims;\n\n    ONNXTRT_CHECK_NODE((indices.getType() == DataType::kINT32),\n        \"This version of TensorRT only supports INT32 input indices. The current indices type = \"\n            + getTrtDtypeName(indices.getType()) + \".\",\n        node, nodeIdx, nvonnxparser::ErrorCode::kUNSUPPORTED_NODE);\n    ONNXTRT_CHECK_NODE((r >= 1), \"0D input data is not allowed.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    ONNXTRT_CHECK_NODE((-r <= axis && axis < r),\n        \"The attribute axis should be in range [-r, r-1], where r is the rank of the input. Provided r = \"\n            << r << \", axis = \" << axis << \".\",\n        node, nodeIdx, nvonnxparser::ErrorCode::kINVALID_NODE);\n\n    if (axis < 0)\n    {\n        axis += r;\n    }\n\n    nvinfer1::IGatherLayer* layer = N_CHECK(ctx->network()->addGather(data, indices, axis));\n    ONNXTRT_CHECK_NODE(layer, \"Failed to create layer\", node, nodeIdx, ErrorCode::kUNSUPPORTED_NODE);\n    ctx->registerLayer(layer, node);\n    layer->setNbElementWiseDims(nbElementWiseDims);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_Slice)\n{\n    ONNXTRT_CHECK_NODE(\n        inputs.at(0).is_tensor(), \"The first input must be a tensor.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    auto& input = inputs.at(0).tensor();\n\n    nvinfer1::ISliceLayer* layer;\n    // If only one input, then, start, size, stride are all attributes.\n    if (inputs.size() == 1)\n    {\n        OnnxAttrs attrs(node, ctx);\n        auto start = attrs.get<nvinfer1::Dims>(\"start\");\n        auto size = attrs.get<nvinfer1::Dims>(\"size\");\n        auto stride = attrs.get<nvinfer1::Dims>(\"stride\");\n        layer = N_CHECK(ctx->network()->addSlice(input, start, size, stride));\n    }\n    else\n    {\n        // start, size, stride are all inputs\n        ONNXTRT_CHECK_NODE((inputs.size() == 4),\n            \"Exactly 4 inputs are required by TRT_Slice. Current input size = \" << inputs.size() << \".\", node, nodeIdx,\n            ErrorCode::kINVALID_NODE);\n        ShapeTensor const start{ctx, inputs.at(1)};\n        ShapeTensor const size{ctx, inputs.at(2)};\n        ShapeTensor const stride{ctx, inputs.at(3)};\n        layer = addSlice(ctx, input, start, size, stride);\n    }\n    ctx->registerLayer(layer, node);\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_Resize)\n{\n    ONNXTRT_CHECK_NODE(\n        inputs.at(0).is_tensor(), \"The first input must be a tensor.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n    auto& input = inputs.at(0).tensor();\n\n    nvinfer1::IResizeLayer* layer;\n    layer = N_CHECK(ctx->network()->addResize(input));\n    ctx->registerLayer(layer, node);\n\n    OnnxAttrs attrs(node, ctx);\n    auto const mode = attrs.get<nvinfer1::InterpolationMode>(\"mode\");\n    auto const transformation = attrs.get<nvinfer1::ResizeCoordinateTransformation>(\"coordTransform\");\n    auto const selector = attrs.get<nvinfer1::ResizeSelector>(\"resizeSelector\");\n    auto const roundMode = attrs.get<nvinfer1::ResizeRoundMode>(\"round_mode\");\n\n    layer->setResizeMode(mode);\n    layer->setSelectorForSinglePixel(selector);\n    layer->setCoordinateTransformation(transformation);\n    layer->setNearestRounding(roundMode);\n\n    if (inputs.size() == 1)\n    {\n        auto outputDims = attrs.get<nvinfer1::Dims>(\"output_dims\", nvinfer1::Dims{-1, {}});\n        if (outputDims.nbDims > 0)\n        {\n            layer->setOutputDimensions(outputDims);\n        }\n        else\n        {\n            // TRT-15340: Adapt to use resizeShapeTensor instead when safety support nbDims == 1.\n            auto scales = attrs.get<std::vector<float>>(\"scales\");\n            ONNXTRT_CHECK_NODE(\n                (scales.size() > 0), \"Attribute scales is missing.\", node, nodeIdx, ErrorCode::kINVALID_NODE);\n            layer->setScales(&scales[0], scales.size());\n        }\n    }\n    else\n    {\n        ONNXTRT_CHECK_NODE((inputs.at(1).is_tensor()), \"The output dimension input must be a tensor.\", node, nodeIdx,\n            ErrorCode::kINVALID_NODE);\n        layer->setInput(1, inputs.at(1).tensor());\n    }\n    RETURN_FIRST_OUTPUT(layer, node, nodeIdx);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_FloorDiv)\n{\n    return elementwiseHelper(ctx, node, nodeIdx, inputs, nvinfer1::ElementWiseOperation::kFLOOR_DIV);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_Conv)\n{\n    return importConv(ctx, node, nodeIdx, inputs);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_Deconv)\n{\n    return importConvTranspose(ctx, node, nodeIdx, inputs);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_MaxPool)\n{\n    return importMaxPool(ctx, node, nodeIdx, inputs);\n}\n\nDEFINE_BUILTIN_OP_IMPORTER(TRT_AveragePool)\n{\n    return importAveragePool(ctx, node, nodeIdx, inputs);\n}\n\n} // namespace\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "onnxOpImporters.hpp",
          "type": "blob",
          "size": 0.1875,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include \"ImporterContext.hpp\"\n\nnamespace onnx2trt\n{\n\nStringMap<NodeImporter>& getBuiltinOpImporterMap();\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "onnxProtoUtils.cpp",
          "type": "blob",
          "size": 1.884765625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"onnxProtoUtils.hpp\"\n\n#include <fstream>\n#include <google/protobuf/io/coded_stream.h>\n#include <google/protobuf/io/zero_copy_stream_impl.h>\n#include <google/protobuf/text_format.h>\n#include <iostream>\n#include <onnx/onnx_pb.h>\n#include <sstream>\n\nnamespace onnx2trt\n{\nvoid removeRawDataStrings(std::string& s)\n{\n    std::string::size_type beg = 0;\n    const std::string key = \"raw_data: \\\"\";\n    const std::string sub = \"...\";\n    while ((beg = s.find(key, beg)) != std::string::npos)\n    {\n        beg += key.length();\n        std::string::size_type end = beg - 1;\n        // Note: Must skip over escaped end-quotes\n        while (s[(end = s.find(\"\\\"\", ++end)) - 1] == '\\\\')\n        {\n        }\n        if (end - beg > 128)\n        { // Only remove large data strings\n            s.replace(beg, end - beg, \"...\");\n        }\n        beg += sub.length();\n    }\n}\n\nstd::string removeRepeatedDataStrings(std::string const& s)\n{\n    std::istringstream iss(s);\n    std::ostringstream oss;\n    bool is_repeat = false;\n    for (std::string line; std::getline(iss, line);)\n    {\n        if (line.find(\"float_data:\") != std::string::npos || line.find(\"int32_data:\") != std::string::npos\n            || line.find(\"int64_data:\") != std::string::npos)\n        {\n            if (!is_repeat)\n            {\n                is_repeat = true;\n                oss << line.substr(0, line.find(\":\") + 1) << \" ...\\n\";\n            }\n        }\n        else\n        {\n            is_repeat = false;\n            oss << line << \"\\n\";\n        }\n    }\n    return oss.str();\n}\n\nstd::string onnxIRVersionAsString(int64_t irVersion)\n{\n    int64_t verMajor = irVersion / 1000000;\n    int64_t verMinor = irVersion % 1000000 / 10000;\n    int64_t verPatch = irVersion % 10000;\n    return (std::to_string(verMajor) + \".\" + std::to_string(verMinor) + \".\" + std::to_string(verPatch));\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "onnxProtoUtils.hpp",
          "type": "blob",
          "size": 3.490234375,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include \"Status.hpp\"\n#include \"errorHelpers.hpp\"\n#include <iostream>\n#include <onnx/onnx_pb.h>\n#include <sstream>\n\n#include <fstream>\n#include <string>\n\n#include <google/protobuf/io/coded_stream.h>\n#include <google/protobuf/io/zero_copy_stream_impl_lite.h>\n\n#if USE_LITE_PROTOBUF\n#include <google/protobuf/message_lite.h>\n#else // !USE_LITE_PROTOBUF\n#include <google/protobuf/message.h>\n#include <google/protobuf/text_format.h>\n#endif // USE_LITE_PROTOBUF\n\n// This file contains the declaration of helper functions used for converting and working with Protobuf files.\n\nnamespace onnx2trt\n{\n\n// Removes raw data from the text representation of an ONNX model.\nvoid removeRawDataStrings(std::string& s);\n\n// Removes float_data, int32_data etc. from the text representation of an ONNX model.\nstd::string removeRepeatedDataStrings(std::string const& s);\n\n// Returns the ONNX IR version as a string.\nstd::string onnxIRVersionAsString(int64_t ir_version = ::ONNX_NAMESPACE::IR_VERSION);\n\n// Converts a raw protobuf::Message or protobuf::MessageLite into a string representation.\ntemplate <typename ProtoMessage>\nstd::string convertProtoToString(ProtoMessage const& message)\n{\n    std::string s{};\n// Textformat available in full proto only. Return only the name when using protobuf-lite.\n#if USE_LITE_PROTOBUF\n    s = \"Node name: \" + message.name();\n    return s;\n#else\n    ::google::protobuf::TextFormat::PrintToString(message, &s);\n    removeRawDataStrings(s);\n    s = removeRepeatedDataStrings(s);\n    return s;\n#endif // USE_LITE_PROTOBUF\n}\n\n// Deserializes an ONNX ModelProto passed in as a protobuf::Message or a protobuf::MessageLite.\ntemplate <typename ProtoMessage>\nvoid deserializeOnnxModel(void const* serializedModel, size_t serializedModelSize, ProtoMessage* model)\n{\n    google::protobuf::io::ArrayInputStream rawInput(serializedModel, serializedModelSize);\n    google::protobuf::io::CodedInputStream codedInput(&rawInput);\n#if GOOGLE_PROTOBUF_VERSION >= 3011000\n    // Starting Protobuf 3.11 accepts only single parameter.\n    codedInput.SetTotalBytesLimit(std::numeric_limits<int>::max());\n#else\n    // Note: This WARs the very low default size limit (64MB)\n    codedInput.SetTotalBytesLimit(std::numeric_limits<int>::max(), std::numeric_limits<int>::max() / 4);\n#endif\n    ONNXTRT_CHECK(model->ParseFromCodedStream(&codedInput) && \"Failed to parse the ONNX model.\",\n        ErrorCode::kMODEL_DESERIALIZE_FAILED);\n}\n\n// Helper function to dispatch to deserializeOnnxModel when user provides a path to the model.\ntemplate <typename ProtoMessage>\nbool ParseFromFileAsBinary(ProtoMessage* msg, char const* filename)\n{\n    std::ifstream onnxFile(filename, std::ios::ate | std::ios::binary);\n    if (!onnxFile)\n    {\n        std::cerr << \"Could not open file \" << std::string(filename) << std::endl;\n        return false;\n    }\n    // Determine the file size\n    auto fileSize = onnxFile.tellg();\n    onnxFile.seekg(0, std::ios::beg);\n\n    // Create buffer and read tne entire file to the buffer.\n    std::vector<char> buffer(fileSize);\n    if (!onnxFile.read(buffer.data(), fileSize))\n    {\n        std::cerr << \"Error reading file: \" << filename << std::endl;\n        return false;\n    }\n\n    deserializeOnnxModel(buffer.data(), buffer.size(), msg);\n    return true;\n}\n\n// ostream overload for printing NodeProtos.\ninline std::ostream& operator<<(std::ostream& stream, ::ONNX_NAMESPACE::NodeProto const& message)\n{\n    stream << convertProtoToString(message);\n    return stream;\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "onnx_backend_test.py",
          "type": "blob",
          "size": 7.33203125,
          "content": "# SPDX-License-Identifier: Apache-2.0\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\n\nimport unittest\nimport onnx.backend.test\n\nimport onnx_tensorrt.backend as trt\n\n# This is a pytest magic variable to load extra plugins\npytest_plugins = 'onnx.backend.test.report',\n\nbackend_test = onnx.backend.test.BackendTest(trt, __name__)\n\n# Include all of the nodes that we support.\n# Onnx native node tests\nbackend_test.include(r'.*test_abs.*')\nbackend_test.include(r'.*test_acos.*')\nbackend_test.include(r'.*test_acosh.*')\nbackend_test.include(r'.*test_add.*')\nbackend_test.include(r'.*test_argmax.*')\nbackend_test.include(r'.*test_argmin.*')\nbackend_test.include(r'.*test_asin.*')\nbackend_test.include(r'.*test_asinh.*')\nbackend_test.include(r'.*test_atan.*')\nbackend_test.include(r'.*test_atanh.*')\nbackend_test.include(r'.*test_averagepool.*')\nbackend_test.include(r'.*test_AvgPool.*')\nbackend_test.include(r'.*test_BatchNorm.*eval.*')\nbackend_test.include(r'.*test_ceil.*')\nbackend_test.include(r'.*test_celu.*')\nbackend_test.include(r'.*test_clip.*')\nbackend_test.include(r'.*test_concat.*')\nbackend_test.include(r'.*test_constant.*')\nbackend_test.include(r'.*test_Conv[1-3]d*')\nbackend_test.include(r'.*test_cos.*')\nbackend_test.include(r'.*test_cosh.*')\nbackend_test.include(r'.*test_depthtospace.*')\nbackend_test.include(r'.*test_div.*')\nbackend_test.include(r'.*test_dropout.*')\nbackend_test.include(r'.*test_ELU*')\nbackend_test.include(r'.*test_elu.*')\nbackend_test.include(r'.*test_equal.*')\nbackend_test.include(r'.*test_Embedding*')\nbackend_test.include(r'.*test_exp.*')\nbackend_test.include(r'.*test_eyelike.*')\nbackend_test.include(r'.*test_flatten.*')\nbackend_test.include(r'.*test_floor.*')\nbackend_test.include(r'.*test_gather.*')\nbackend_test.include(r'.*test_gemm.*')\nbackend_test.include(r'.*test_globalaveragepool.*')\nbackend_test.include(r'.*test_globalmaxpool.*')\nbackend_test.include(r'.*test_greater.*')\nbackend_test.include(r'.*test_hardsigmoid.*')\nbackend_test.include(r'.*test_identity.*')\nbackend_test.include(r'.*test_LeakyReLU*')\nbackend_test.include(r'.*test_leakyrelu.*')\nbackend_test.include(r'.*test_less.*')\nbackend_test.include(r'.*test_Linear.*')\nbackend_test.include(r'.*test_log.*')\nbackend_test.include(r'.*test_logsoftmax.*')\nbackend_test.include(r'.*test_LogSoftmax.*')\nbackend_test.include(r'.*test_log_softmax.*')\nbackend_test.include(r'.*test_lrn.*')\nbackend_test.include(r'.*test_matmul.*')\nbackend_test.include(r'.*test_max.*')\nbackend_test.include(r'.*test_MaxPool[1-9]d.*')\nbackend_test.include(r'.*test_mean.*')\nbackend_test.include(r'.*test_min.*')\nbackend_test.include(r'.*test_mul.*')\nbackend_test.include(r'.*test_neg.*')\nbackend_test.include(r'.*test_not.*')\nbackend_test.include(r'.*test_operator_addmm.*')\nbackend_test.include(r'.*test_operator_basic.*')\nbackend_test.include(r'.*test_operator_chunk.*')\nbackend_test.include(r'.*test_operator_clip.*')\nbackend_test.include(r'.*test_operator_concat2.*')\nbackend_test.include(r'.*test_operator_conv_.*')\nbackend_test.include(r'.*test_operator_exp.*')\nbackend_test.include(r'.*test_operator_flatten.*')\nbackend_test.include(r'.*test_operator_index.*')\nbackend_test.include(r'.*test_operator_max_.*')\nbackend_test.include(r'.*test_operator_maxpool.*')\nbackend_test.include(r'.*test_operator_min.*')\nbackend_test.include(r'.*test_operator_mm.*')\nbackend_test.include(r'.*test_operator_non_float_params.*')\nbackend_test.include(r'.*test_operator_params.*')\nbackend_test.include(r'.*test_operator_permute2.*')\nbackend_test.include(r'.*test_operator_pow.*')\nbackend_test.include(r'.*test_operator_reduced_mean_.*')\nbackend_test.include(r'.*test_operator_reduced_mean_keepdim.*')\nbackend_test.include(r'.*test_operator_reduced_sum_.*')\nbackend_test.include(r'.*test_operator_reduced_sum_keepdim.*')\nbackend_test.include(r'.*test_operator_selu.*')\nbackend_test.include(r'.*test_operator_sqrt.*')\nbackend_test.include(r'.*test_operator_symbolic_override.*')\nbackend_test.include(r'.*test_operator_symbolic_override_nested.*')\nbackend_test.include(r'.*test_operator_view.*')\nbackend_test.include(r'.*test_pow.*')\nbackend_test.include(r'.*test_PoissonNLLLLoss_no_reduce*')\nbackend_test.include(r'.*test_reciprocal.*')\nbackend_test.include(r'.*test_reduce.*')\nbackend_test.include(r'.*test_ReLU*')\nbackend_test.include(r'.*test_relu.*')\nbackend_test.include(r'.*test_selu.*')\nbackend_test.include(r'.*test_shape.*')\nbackend_test.include(r'.*test_Sigmoid*')\nbackend_test.include(r'.*test_sigmoid.*')\nbackend_test.include(r'.*test_sin.*')\nbackend_test.include(r'.*test_sinh.*')\nbackend_test.include(r'.*test_size.*')\nbackend_test.include(r'.*test_Softmax*')\nbackend_test.include(r'.*test_softmax.*')\nbackend_test.include(r'.*test_Softmin*')\nbackend_test.include(r'.*test_Softplus*')\nbackend_test.include(r'.*test_softplus.*')\nbackend_test.include(r'.*test_softsign.*')\nbackend_test.include(r'.*test_sqrt.*')\nbackend_test.include(r'.*test_squeeze_cuda')\nbackend_test.include(r'.*test_sub.*')\nbackend_test.include(r'.*test_sum.*')\nbackend_test.include(r'.*test_tan.*')\nbackend_test.include(r'.*test_Tanh*')\nbackend_test.include(r'.*test_tanh.*')\nbackend_test.include(r'.*test_thresholdedrelu.*')\nbackend_test.include(r'.*test_transpose.*')\nbackend_test.include(r'.*test_unsqueeze.*')\nbackend_test.include(r'.*test_ZeroPad2d*')\n\n# # Onnx native model tests\nbackend_test.include(r'.*test_bvlc_alexnet.*')\nbackend_test.include(r'.*test_densenet121.*')\nbackend_test.include(r'.*test_inception_v1.*')\nbackend_test.include(r'.*test_inception_v2.*')\nbackend_test.include(r'.*test_resnet50.*')\nbackend_test.include(r'.*test_shufflenet.*')\nbackend_test.include(r'.*test_squeezenet.*')\nbackend_test.include(r'.*test_vgg19.*')\nbackend_test.include(r'.*test_zfnet512.*')\n\n\n#TRT custom tests\nbackend_test.include(r'.*test_basic_conv_.*custom.*')\nbackend_test.include(r'.*test_conv_.*custom.*')\nbackend_test.include(r'.*test_convtranspose.*custom.*')\nbackend_test.include(r'.*test_batchnorm.*custom.*')\nbackend_test.include(r'.*test_reshape.*custom.*')\nbackend_test.include(r'.*test_prelu.*custom.*')\nbackend_test.include(r'.*test_topk.*custom.*')\nbackend_test.include(r'.*test_upsample.*custom.*')\nbackend_test.include(r'.*test_constant_pad_custom.*')\nbackend_test.include(r'.*test_resize.*custom.*')\nbackend_test.include(r'.*test_split.*custom.*')\nbackend_test.include(r'.*test_instancenorm_.*_custom.*')\nbackend_test.include(r'.*test_slice.*custom.*')\n\n\n# exclude unenabled ops get pulled in with wildcards\n# test_constant_pad gets pulled in with the test_constant* wildcard. Explicitly disable padding tests for now.\nbackend_test.exclude(r'.*test_constant_pad.*')\nbackend_test.exclude(r'.*test_constantofshape.*')\nbackend_test.exclude(r'.*test_expand.*')\n# Operator MATMULINTEGER is not supported by TRT\nbackend_test.exclude(r'.*test_matmulinteger.*')\nbackend_test.exclude(r'.*test_maxpool.*')\nbackend_test.exclude(r'.*test_maxunpool.*')\n# Mismatch: 0.476%, relative diff is good.\n# Absolute diff failed because\n# numpy compares the difference between actual and desired to atol + rtol * abs(desired)\nbackend_test.exclude(r'.*test_convtranspose_3d_custom_cuda')\n# dilations not supported in ConvTRanspose layer\nbackend_test.exclude(r'.*test_convtranspose_dilations_custom_cuda')\n\nglobals().update(backend_test\n                 .enable_report()\n                 .test_cases)\n\nif __name__ == '__main__':\n    unittest.main()\n"
        },
        {
          "name": "onnx_tensorrt",
          "type": "tree",
          "content": null
        },
        {
          "name": "onnx_trt_backend.cpp",
          "type": "blob",
          "size": 37.146484375,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"NvOnnxParser.h\"\n#include \"onnx/onnxifi.h\"\n#include <NvInfer.h>\n#include <atomic>\n#include <ctime>\n#include <cuda_runtime.h>\n#include <mutex>\n#include <thrust/device_vector.h>\n#include <unordered_map>\n\n#define BACKEND_NAME \"TensorRT\"\n#define BACKEND_VENDOR \"Nvidia\"\n#define BACKEND_VERSION \"1.0.0\"\n#define BACKEND_EXTENSIONS \"\"\n#define BACKEND_IR_VERSION \"3\"\n#define BACKEND_OPSET_VERSION \"ai.onnx:7\"\n\nnamespace\n{\n\nstruct InferDeleter\n{\n    template <typename T>\n    void operator()(T* obj) const\n    {\n        if (obj)\n        {\n            obj->destroy();\n        }\n    }\n};\ntemplate <typename T>\ninline std::shared_ptr<T> infer_object(T* obj)\n{\n    if (!obj)\n    {\n        throw std::runtime_error(\"Failed to create object\");\n    }\n    return std::shared_ptr<T>(obj, InferDeleter());\n}\n\n// Logger for TRT info/warning/errors\nclass TRT_Logger : public nvinfer1::ILogger\n{\n    nvinfer1::ILogger::Severity _verbosity;\n    std::ostream* _ostream;\n\npublic:\n    TRT_Logger(Severity verbosity = Severity::kWARNING, std::ostream& ostream = std::cout)\n        : _verbosity(verbosity)\n        , _ostream(&ostream)\n    {\n    }\n    void log(Severity severity, const char* msg) override\n    {\n        if (severity <= _verbosity)\n        {\n            time_t rawtime = std::time(0);\n            char buf[256];\n            strftime(&buf[0], 256, \"%Y-%m-%d %H:%M:%S\", std::gmtime(&rawtime));\n            const char* sevstr = (severity == Severity::kINTERNAL_ERROR ? \"    BUG\" : severity == Severity::kERROR\n                        ? \"  ERROR\"\n                        : severity == Severity::kWARNING ? \"WARNING\" : severity == Severity::kINFO ? \"   INFO\"\n                                                                                                   : \"UNKNOWN\");\n            (*_ostream) << \"[\" << buf << \" \" << sevstr << \"] \" << msg << std::endl;\n        }\n    }\n};\n\nonnxStatus CheckShape(const nvinfer1::Dims& dims, const onnxTensorDescriptorV1& desc, bool allow_same_size)\n{\n    bool matched = false;\n    if (desc.dimensions == static_cast<uint32_t>(dims.nbDims) + 1)\n    {\n        matched = true;\n        for (int i = 0; i < dims.nbDims; ++i)\n        {\n            if (desc.shape[i + 1] != static_cast<uint64_t>(dims.d[i]))\n            {\n                return ONNXIFI_STATUS_MISMATCHING_SHAPE;\n            }\n        }\n    }\n    else if (allow_same_size && desc.dimensions > 1)\n    {\n        size_t dim_size = 1;\n        for (int i = 0; i < dims.nbDims; ++i)\n        {\n            dim_size *= dims.d[i];\n        }\n        size_t desc_size = 1;\n        // Skip the first dim which is batch size\n        for (uint32_t i = 1; i < desc.dimensions; ++i)\n        {\n            desc_size *= desc.shape[i];\n        }\n        matched = (dim_size == desc_size) ? true : false;\n        if (!matched)\n        {\n            std::cerr << \"mismatched output \" << desc.name << \": \" << desc_size << \" vs \" << dim_size << std::endl;\n        }\n    }\n\n    return matched ? ONNXIFI_STATUS_SUCCESS : ONNXIFI_STATUS_MISMATCHING_SHAPE;\n}\n\nsize_t GetTensorFootprint(const onnxTensorDescriptorV1& input)\n{\n    size_t acc = 1;\n    for (unsigned i = 0; i < input.dimensions; ++i)\n    {\n        acc *= input.shape[i];\n    }\n    size_t multiplier = 1;\n    switch (input.dataType)\n    {\n    case ONNXIFI_DATATYPE_FLOAT16: multiplier = sizeof(float) / 2; break;\n    case ONNXIFI_DATATYPE_FLOAT32: multiplier = sizeof(float); break;\n    case ONNXIFI_DATATYPE_INT8: multiplier = sizeof(int8_t); break;\n    case ONNXIFI_DATATYPE_INT16: multiplier = sizeof(int16_t); break;\n    case ONNXIFI_DATATYPE_INT32: multiplier = sizeof(int32_t); break;\n    case ONNXIFI_DATATYPE_UINT8: multiplier = sizeof(uint8_t); break;\n    case ONNXIFI_DATATYPE_UINT16: multiplier = sizeof(uint16_t); break;\n    case ONNXIFI_DATATYPE_UINT32: multiplier = sizeof(uint32_t); break;\n    default: multiplier = 0;\n    }\n    return acc * multiplier;\n}\n\nstruct OnnxTensorRTBackendID\n{\n    OnnxTensorRTBackendID(int i)\n        : device_id(i)\n    {\n    }\n    int device_id{0};\n};\n\nclass OnnxTensorRTEvent\n{\npublic:\n    OnnxTensorRTEvent(cudaStream_t s)\n        : stream_(s)\n    {\n        if (cudaEventCreateWithFlags(&event_, cudaEventDisableTiming) != cudaSuccess)\n        {\n            throw std::runtime_error(\"Cannot create cudaEvent\");\n        }\n    }\n\n    ~OnnxTensorRTEvent()\n    {\n        cudaEventDestroy(event_);\n    }\n\n    onnxStatus Signal()\n    {\n        std::lock_guard<std::mutex> guard(mutex_);\n        if (fired_)\n        {\n            return ONNXIFI_STATUS_INVALID_STATE;\n        }\n\n        if (cudaEventRecord(event_, stream_) == cudaSuccess)\n        {\n            fired_ = true;\n            return ONNXIFI_STATUS_SUCCESS;\n        }\n        else\n        {\n            return ONNXIFI_STATUS_INTERNAL_ERROR;\n        }\n    }\n\n    onnxStatus Wait()\n    {\n        std::lock_guard<std::mutex> guard(mutex_);\n        return (cudaEventSynchronize(event_) == cudaSuccess) ? ONNXIFI_STATUS_SUCCESS : ONNXIFI_STATUS_INTERNAL_ERROR;\n    }\n\n    onnxStatus CheckState(onnxEventState* state)\n    {\n        std::lock_guard<std::mutex> guard(mutex_);\n        if (!fired_)\n        {\n            *state = ONNXIFI_EVENT_STATE_NONSIGNALLED;\n            return ONNXIFI_STATUS_SUCCESS;\n        }\n\n        auto rt = cudaEventQuery(event_);\n        if (rt == cudaErrorNotReady)\n        {\n            *state = ONNXIFI_EVENT_STATE_NONSIGNALLED;\n            return ONNXIFI_STATUS_SUCCESS;\n        }\n        else if (rt == cudaSuccess)\n        {\n            *state = ONNXIFI_EVENT_STATE_SIGNALLED;\n            return ONNXIFI_STATUS_SUCCESS;\n        }\n        else\n        {\n            *state = ONNXIFI_EVENT_STATE_INVALID;\n            return ONNXIFI_STATUS_INVALID_STATE;\n        }\n    }\n\nprivate:\n    std::mutex mutex_;\n    std::atomic<bool> fired_{false};\n    cudaStream_t stream_{0};\n    cudaEvent_t event_;\n};\n\nclass CudaDeviceGuard\n{\npublic:\n    CudaDeviceGuard(int backend_id)\n    {\n        if (cudaGetDevice(&saved_device_) != cudaSuccess)\n        {\n            throw std::runtime_error(\"Cannot run cudaGetDevice\");\n        }\n        if (saved_device_ != backend_id)\n        {\n            if (cudaSetDevice(backend_id) != cudaSuccess)\n            {\n                throw std::runtime_error(\"Cannot run cudaSetDevice\");\n            }\n            need_restore_ = true;\n        }\n    }\n\n    ~CudaDeviceGuard()\n    {\n        if (need_restore_)\n        {\n            cudaSetDevice(saved_device_);\n        }\n    }\n\nprivate:\n    int saved_device_{-1};\n    bool need_restore_{false};\n};\nclass OnnxTensorRTBackendRep\n{\npublic:\n    OnnxTensorRTBackendRep(const OnnxTensorRTBackendID& backend_id)\n        : device_id_(backend_id.device_id)\n    {\n        trt_builder_ = infer_object(nvinfer1::createInferBuilder(trt_logger_));\n        trt_builder_->setMaxBatchSize(max_batch_size_);\n        trt_builder_->setMaxWorkspaceSize(max_workspace_size_);\n        trt_network_ = infer_object(trt_builder_->createNetwork());\n        parser_ = infer_object(nvonnxparser::createParser(*trt_network_, trt_logger_));\n        CudaDeviceGuard guard(device_id_);\n        if (cudaStreamCreate(&stream_) != cudaSuccess)\n        {\n            throw std::runtime_error(\"Cannot create cudaStream\");\n        }\n    }\n\n    ~OnnxTensorRTBackendRep()\n    {\n        cudaStreamDestroy(stream_);\n    }\n\n    int device_id() const\n    {\n        return device_id_;\n    }\n    cudaStream_t stream() const\n    {\n        return stream_;\n    }\n\n    onnxStatus ImportModel(void const* serialized_onnx_model, size_t serialized_onnx_model_size, uint32_t weight_count,\n        onnxTensorDescriptorV1 const* weight_descriptors)\n    {\n        auto succeeded = parser_->parseWithWeightDescriptors(\n            serialized_onnx_model, serialized_onnx_model_size, weight_count, weight_descriptors);\n        if (!succeeded)\n        {\n            const auto num_errors = parser_->getNbErrors();\n            if (num_errors > 0)\n            {\n                const auto* error = parser_->getError(num_errors - 1);\n                std::cerr << \"Parsing error: \" << error->desc() << \" at \" << error->file() << \":\" << error->line()\n                          << \" (\" << error->func() << \").\" << std::endl;\n                switch (error->code())\n                {\n                case nvonnxparser::ErrorCode::kMEM_ALLOC_FAILED: return ONNXIFI_STATUS_NO_SYSTEM_MEMORY;\n                case nvonnxparser::ErrorCode::kMODEL_DESERIALIZE_FAILED: return ONNXIFI_STATUS_INVALID_PROTOBUF;\n                case nvonnxparser::ErrorCode::kINVALID_VALUE: return ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE;\n                case nvonnxparser::ErrorCode::kINVALID_GRAPH:\n                case nvonnxparser::ErrorCode::kINVALID_NODE: return ONNXIFI_STATUS_INVALID_MODEL;\n                case nvonnxparser::ErrorCode::kUNSUPPORTED_NODE:\n                case nvonnxparser::ErrorCode::kUNSUPPORTED_GRAPH: return ONNXIFI_STATUS_UNSUPPORTED_OPERATOR;\n                default: return ONNXIFI_STATUS_INTERNAL_ERROR;\n                }\n            }\n        }\n\n        return ONNXIFI_STATUS_SUCCESS;\n    }\n\n    nvinfer1::ICudaEngine* buildCudaEngine()\n    {\n        return trt_builder_->buildCudaEngine(*trt_network_);\n    }\n\n    size_t max_batch_size() const\n    {\n        return max_batch_size_;\n    }\n\nprivate:\n    TRT_Logger trt_logger_;\n    cudaStream_t stream_;\n    std::shared_ptr<nvinfer1::IBuilder> trt_builder_{nullptr};\n    std::shared_ptr<nvinfer1::INetworkDefinition> trt_network_{nullptr};\n    std::shared_ptr<nvonnxparser::IParser> parser_{nullptr};\n    // TODO: configerable max batch size\n    int device_id_{0};\n    size_t max_batch_size_{128};\n    size_t max_workspace_size_{1024UL * 1024UL * 1024UL * 2UL};\n};\n\nclass GraphRep\n{\npublic:\n    GraphRep(OnnxTensorRTBackendRep* backendrep)\n        : device_id_(backendrep->device_id())\n        , max_batch_size_(backendrep->max_batch_size())\n        , stream_(backendrep->stream())\n    {\n        if (cudaSetDevice(device_id_) != cudaSuccess)\n        {\n            throw std::runtime_error(\"Cannot set CUDA device\");\n        }\n        trt_engine_ = infer_object(backendrep->buildCudaEngine());\n        max_batch_size_ = backendrep->max_batch_size();\n    }\n\n    ~GraphRep()\n    {\n        ClearDeviceBuffers();\n    }\n\n    onnxStatus InitIO(uint32_t inputsCount, const onnxTensorDescriptorV1* inputDescriptors, uint32_t outputsCount,\n        const onnxTensorDescriptorV1* outputDescriptors);\n\n    onnxStatus Run();\n\n    cudaStream_t stream() const\n    {\n        return stream_;\n    }\n\nprivate:\n    void ClearDeviceBuffers();\n\n    onnxStatus CheckAndBindTensor(const nvinfer1::Dims& dims, const onnxTensorDescriptorV1& tensor, bool is_output);\n\n    std::shared_ptr<nvinfer1::ICudaEngine> trt_engine_{nullptr};\n    std::shared_ptr<nvinfer1::IExecutionContext> trt_executor_{nullptr};\n    std::vector<void*> bindings_;\n    std::unordered_map<std::string, const onnxTensorDescriptorV1*> input_map_;\n    std::unordered_map<std::string, const onnxTensorDescriptorV1*> output_map_;\n    std::unordered_map<std::string, void*> device_buffers_;\n    int device_id_{0};\n    size_t max_batch_size_{0};\n    size_t batch_size_{0};\n    cudaStream_t stream_;\n};\n\nvoid GraphRep::ClearDeviceBuffers()\n{\n    for (auto kv : device_buffers_)\n    {\n        cudaFree(kv.second);\n    }\n    device_buffers_.clear();\n}\n\nonnxStatus GraphRep::CheckAndBindTensor(\n    const nvinfer1::Dims& dims, const onnxTensorDescriptorV1& tensor, bool is_output)\n{\n    // Check memory type\n    if (tensor.memoryType != ONNXIFI_MEMORY_TYPE_CPU && tensor.memoryType != ONNXIFI_MEMORY_TYPE_CUDA_BUFFER)\n    {\n        return ONNXIFI_STATUS_INVALID_DATATYPE;\n    }\n    // Check tensor shape\n    auto ret = CheckShape(dims, tensor, is_output);\n    if (ret != ONNXIFI_STATUS_SUCCESS)\n    {\n        return ret;\n    }\n\n    // For CPU tensor, we need to create a device memory and the bind. For CUDA\n    // tensor, we can bind directly\n    if (tensor.memoryType == ONNXIFI_MEMORY_TYPE_CPU)\n    {\n        void* cuda_buffer;\n        size_t footprint = GetTensorFootprint(tensor);\n        if (!footprint)\n        {\n            return ONNXIFI_STATUS_INVALID_SHAPE;\n        }\n        if (cudaMalloc(&cuda_buffer, footprint) != cudaSuccess)\n        {\n            return ONNXIFI_STATUS_NO_DEVICE_MEMORY;\n        }\n        device_buffers_.emplace(tensor.name, cuda_buffer);\n        bindings_.push_back(cuda_buffer);\n    }\n    else\n    {\n        bindings_.push_back((void*) (tensor.buffer));\n    }\n\n    return ONNXIFI_STATUS_SUCCESS;\n}\n\nonnxStatus GraphRep::InitIO(uint32_t inputsCount, const onnxTensorDescriptorV1* inputDescriptors, uint32_t outputsCount,\n    const onnxTensorDescriptorV1* outputDescriptors)\n{\n    CudaDeviceGuard guard(device_id_);\n    ClearDeviceBuffers();\n    // Setup the input/output bindings and decide batch size\n    for (unsigned i = 0; i < inputsCount; ++i)\n    {\n        if (inputDescriptors[i].tag != ONNXIFI_TAG_TENSOR_DESCRIPTOR_V1)\n        {\n            return ONNXIFI_STATUS_UNSUPPORTED_TAG;\n        }\n        if (!inputDescriptors[i].name)\n        {\n            return ONNXIFI_STATUS_INVALID_NAME;\n        }\n        // We only support NCHW\n        if (inputDescriptors[i].dimensions != 4)\n        {\n            return ONNXIFI_STATUS_INVALID_SHAPE;\n        }\n        if (i == 0)\n        {\n            batch_size_ = inputDescriptors[i].shape[0];\n        }\n        else\n        {\n            if (batch_size_ != inputDescriptors[i].shape[0])\n            {\n                return ONNXIFI_STATUS_INVALID_SHAPE;\n            }\n        }\n        std::cerr << \"Adding input \" << i << \": \" << inputDescriptors[i].name\n                  << \", type: \" << inputDescriptors[i].memoryType << std::endl;\n        input_map_.emplace(std::string(inputDescriptors[i].name), inputDescriptors + i);\n    }\n\n    // We don't support the case when batch size is larger than max batch size\n    // yet, but this is not a hard constraint.\n    if (batch_size_ > max_batch_size_)\n    {\n        return ONNXIFI_STATUS_NO_DEVICE_RESOURCES;\n    }\n\n    for (unsigned i = 0; i < outputsCount; ++i)\n    {\n        if (outputDescriptors[i].tag != ONNXIFI_TAG_TENSOR_DESCRIPTOR_V1)\n        {\n            return ONNXIFI_STATUS_UNSUPPORTED_TAG;\n        }\n        if (!outputDescriptors[i].name)\n        {\n            return ONNXIFI_STATUS_INVALID_NAME;\n        }\n        output_map_.emplace(std::string(outputDescriptors[i].name), outputDescriptors + i);\n    }\n\n    int nbindings = trt_engine_->getNbBindings();\n    for (int b = 0; b < nbindings; ++b)\n    {\n        nvinfer1::Dims dims = trt_engine_->getBindingDimensions(b);\n        // Check data type consistency\n        auto binding_datatype = trt_engine_->getBindingDataType(b);\n        if (binding_datatype != nvinfer1::DataType::kFLOAT)\n        {\n            return ONNXIFI_STATUS_MISMATCHING_DATATYPE;\n        }\n\n        if (trt_engine_->bindingIsInput(b))\n        {\n            std::cerr << \"Input: \" << trt_engine_->getBindingName(b) << \", Dim: \" << dims.d[0] << \", \" << dims.d[1]\n                      << \", \" << dims.d[2] << std::endl;\n            const auto it = input_map_.find(trt_engine_->getBindingName(b));\n            if (it == input_map_.end())\n            {\n                return ONNXIFI_STATUS_UNIDENTIFIED_NAME;\n            }\n            if (auto ret = CheckAndBindTensor(dims, *it->second, false) != ONNXIFI_STATUS_SUCCESS)\n            {\n                return ret;\n            }\n        }\n        else\n        {\n            // output: for output, we enforce 4D dim although it can be in 2D, we do\n            // an implicit reshape in `CheckAndBindTensor`\n            const auto it = output_map_.find(trt_engine_->getBindingName(b));\n            if (it == output_map_.end())\n            {\n                return ONNXIFI_STATUS_UNIDENTIFIED_NAME;\n            }\n            if (auto ret = CheckAndBindTensor(dims, *it->second, true) != ONNXIFI_STATUS_SUCCESS)\n            {\n                return ret;\n            }\n        }\n    }\n\n    trt_executor_ = infer_object(trt_engine_->createExecutionContext());\n    return ONNXIFI_STATUS_SUCCESS;\n}\n\nonnxStatus GraphRep::Run()\n{\n    CudaDeviceGuard guard(device_id_);\n    // Copy input if necessary\n    // TODO: cache tensor footprint\n    for (auto kv : device_buffers_)\n    {\n        auto it = input_map_.find(kv.first);\n        if (it != input_map_.end())\n        {\n            cudaMemcpyAsync(kv.second, (void*) (it->second->buffer), GetTensorFootprint(*it->second),\n                cudaMemcpyHostToDevice, stream_);\n        }\n        else if (output_map_.find(kv.first) == output_map_.end())\n        {\n            return ONNXIFI_STATUS_UNIDENTIFIED_NAME;\n        }\n    }\n\n    // Run TensorRT\n    trt_executor_->enqueue(batch_size_, bindings_.data(), stream_, nullptr);\n\n    // Copy output if necessary\n    for (auto kv : device_buffers_)\n    {\n        auto it = output_map_.find(kv.first);\n        if (it != output_map_.end())\n        {\n            cudaMemcpyAsync((void*) (it->second->buffer), kv.second, GetTensorFootprint(*it->second),\n                cudaMemcpyDeviceToHost, stream_);\n        }\n        else if (input_map_.find(kv.first) == input_map_.end())\n        {\n            return ONNXIFI_STATUS_UNIDENTIFIED_NAME;\n        }\n    }\n    return ONNXIFI_STATUS_SUCCESS;\n}\n\ntemplate <class F>\nonnxStatus OnnxifiTryCatch(F&& tryBlock)\n{\n    try\n    {\n        return tryBlock();\n    }\n    catch (const std::bad_alloc& e)\n    {\n        std::cerr << \"Allocation failed: \" << e.what() << std::endl;\n        return ONNXIFI_STATUS_NO_SYSTEM_MEMORY;\n    }\n    catch (const std::exception& e)\n    {\n        std::cerr << \"Internal Error: \" << e.what() << std::endl;\n        return ONNXIFI_STATUS_INTERNAL_ERROR;\n    }\n    catch (...)\n    {\n        return ONNXIFI_STATUS_INTERNAL_ERROR;\n    }\n}\n} // namespace\n\nONNXIFI_PUBLIC ONNXIFI_CHECK_RESULT onnxStatus ONNXIFI_ABI onnxGetBackendIDs(\n    onnxBackendID* backendIDs, size_t* numBackends)\n{\n    return OnnxifiTryCatch([&] {\n        if (!numBackends)\n        {\n            return ONNXIFI_STATUS_INVALID_POINTER;\n        }\n\n        int nDevices_int{0};\n        cudaGetDeviceCount(&nDevices_int);\n        size_t nDevices{static_cast<size_t>(nDevices_int)};\n        if (!backendIDs)\n        {\n            *numBackends = nDevices;\n            return ONNXIFI_STATUS_FALLBACK;\n        }\n        else\n        {\n            size_t len = (*numBackends < nDevices) ? (*numBackends) : nDevices;\n            std::vector<std::unique_ptr<OnnxTensorRTBackendID>> vtmp;\n            for (size_t i = 0; i < len; ++i)\n            {\n                vtmp.emplace_back(new OnnxTensorRTBackendID(i));\n            }\n            for (size_t i = 0; i < len; ++i)\n            {\n                backendIDs[i] = (onnxBackendID)(vtmp[i].release());\n            }\n            return (*numBackends < nDevices) ? ONNXIFI_STATUS_FALLBACK : ONNXIFI_STATUS_SUCCESS;\n        }\n    });\n}\n\nONNXIFI_PUBLIC ONNXIFI_CHECK_RESULT onnxStatus ONNXIFI_ABI onnxReleaseBackendID(onnxBackendID backendID)\n{\n    return OnnxifiTryCatch([&] {\n        auto* backend_id = reinterpret_cast<OnnxTensorRTBackendID*>(backendID);\n        if (!backend_id)\n        {\n            return ONNXIFI_STATUS_INVALID_ID;\n        }\n        delete backend_id;\n        return ONNXIFI_STATUS_SUCCESS;\n    });\n}\n\nstatic onnxStatus setUIntInfo(void* valuePtr, size_t* valueSizePtr, uint64_t value)\n{\n    onnxStatus status = ONNXIFI_STATUS_FALLBACK;\n    if (valuePtr != nullptr && *valueSizePtr >= sizeof(uint64_t))\n    {\n        *static_cast<uint64_t*>(valuePtr) = value;\n        status = ONNXIFI_STATUS_SUCCESS;\n    }\n    *valueSizePtr = sizeof(uint64_t);\n    return status;\n}\n\nstatic onnxStatus setStringInfo(void* valuePtr, size_t* valueSizePtr, const char* value, size_t valueSize)\n{\n    onnxStatus status = ONNXIFI_STATUS_FALLBACK;\n    if (valuePtr != nullptr && *valueSizePtr >= valueSize)\n    {\n        memcpy(valuePtr, value, valueSize);\n        status = ONNXIFI_STATUS_SUCCESS;\n    }\n    *valueSizePtr = valueSize;\n    return status;\n}\n\nONNXIFI_PUBLIC ONNXIFI_CHECK_RESULT onnxStatus ONNXIFI_ABI onnxGetBackendInfo(\n    onnxBackendID backendID, onnxBackendInfo infoType, void* infoValue, size_t* infoValueSize)\n{\n    return OnnxifiTryCatch([&] {\n        if (infoValueSize == nullptr)\n        {\n            return ONNXIFI_STATUS_INVALID_POINTER;\n        }\n\n        if (backendID == nullptr)\n        {\n            return ONNXIFI_STATUS_INVALID_ID;\n        }\n\n        const int cudaDeviceId = static_cast<OnnxTensorRTBackendID*>(backendID)->device_id;\n\n        switch (infoType)\n        {\n        case ONNXIFI_BACKEND_ONNXIFI_VERSION:\n            return setUIntInfo(infoValue, infoValueSize, UINT64_C(0x0000000100000000));\n        case ONNXIFI_BACKEND_NAME: return setStringInfo(infoValue, infoValueSize, BACKEND_NAME, strlen(BACKEND_NAME));\n        case ONNXIFI_BACKEND_VENDOR:\n            return setStringInfo(infoValue, infoValueSize, BACKEND_VENDOR, strlen(BACKEND_VENDOR));\n        case ONNXIFI_BACKEND_VERSION:\n            return setStringInfo(infoValue, infoValueSize, BACKEND_VERSION, strlen(BACKEND_VERSION));\n        case ONNXIFI_BACKEND_EXTENSIONS:\n            return setStringInfo(infoValue, infoValueSize, BACKEND_EXTENSIONS, strlen(BACKEND_EXTENSIONS));\n        case ONNXIFI_BACKEND_DEVICE:\n        {\n            cudaDeviceProp deviceProperties = {0};\n            cudaError_t cudaError = cudaGetDeviceProperties(&deviceProperties, cudaDeviceId);\n            switch (cudaError)\n            {\n            case cudaSuccess: break;\n            case cudaErrorInvalidDevice: return ONNXIFI_STATUS_INVALID_ID;\n            default: return ONNXIFI_STATUS_INTERNAL_ERROR;\n            }\n            return setStringInfo(infoValue, infoValueSize, deviceProperties.name,\n                strnlen(deviceProperties.name, sizeof(deviceProperties.name)));\n        }\n        case ONNXIFI_BACKEND_DEVICE_TYPE: return setUIntInfo(infoValue, infoValueSize, ONNXIFI_DEVICE_TYPE_GPU);\n        case ONNXIFI_BACKEND_ONNX_IR_VERSION:\n            return setStringInfo(infoValue, infoValueSize, BACKEND_IR_VERSION, strlen(BACKEND_IR_VERSION));\n        case ONNXIFI_BACKEND_OPSET_VERSION:\n            return setStringInfo(infoValue, infoValueSize, BACKEND_OPSET_VERSION, strlen(BACKEND_OPSET_VERSION));\n        case ONNXIFI_BACKEND_CAPABILITIES: return setUIntInfo(infoValue, infoValueSize, 0);\n        case ONNXIFI_BACKEND_INIT_PROPERTIES: return setUIntInfo(infoValue, infoValueSize, 0);\n        case ONNXIFI_BACKEND_MEMORY_TYPES:\n            return setUIntInfo(infoValue, infoValueSize, ONNXIFI_MEMORY_TYPE_CPU | ONNXIFI_MEMORY_TYPE_CUDA_BUFFER);\n        case ONNXIFI_BACKEND_GRAPH_INIT_PROPERTIES: return setUIntInfo(infoValue, infoValueSize, 0);\n        case ONNXIFI_BACKEND_SYNCHRONIZATION_TYPES:\n            return setUIntInfo(infoValue, infoValueSize, ONNXIFI_SYNCHRONIZATION_EVENT);\n        case ONNXIFI_BACKEND_CPU_MEMORY_READ_BANDWIDTH:\n        case ONNXIFI_BACKEND_CPU_MEMORY_WRITE_BANDWIDTH:\n            /* Assume PCI Express 3.0 x16 */\n            return setUIntInfo(infoValue, infoValueSize, UINT64_C(16519104985));\n        case ONNXIFI_BACKEND_MAX_GRAPH_COUNT: return setUIntInfo(infoValue, infoValueSize, UINT64_MAX);\n        case ONNXIFI_BACKEND_MEMORY_SIZE:\n        case ONNXIFI_BACKEND_MAX_GRAPH_SIZE:\n        case ONNXIFI_BACKEND_PCI_BUS_ID:\n        case ONNXIFI_BACKEND_PCI_DEVICE_ID:\n        case ONNXIFI_BACKEND_PCI_DOMAIN_ID:\n        case ONNXIFI_BACKEND_MACS_FP32:\n        case ONNXIFI_BACKEND_MACS_FP16:\n        case ONNXIFI_BACKEND_MEMORY_BANDWIDTH:\n        {\n            cudaDeviceProp deviceProperties = {0};\n            cudaError_t cudaError = cudaGetDeviceProperties(&deviceProperties, cudaDeviceId);\n            switch (cudaError)\n            {\n            case cudaSuccess: break;\n            case cudaErrorInvalidDevice: return ONNXIFI_STATUS_INVALID_ID;\n            default: return ONNXIFI_STATUS_INTERNAL_ERROR;\n            }\n            switch (infoType)\n            {\n            case ONNXIFI_BACKEND_MEMORY_SIZE:\n            case ONNXIFI_BACKEND_MAX_GRAPH_SIZE:\n                return setUIntInfo(infoValue, infoValueSize, static_cast<uint64_t>(deviceProperties.totalGlobalMem));\n            case ONNXIFI_BACKEND_MEMORY_BANDWIDTH:\n                return setUIntInfo(infoValue, infoValueSize, static_cast<uint64_t>(deviceProperties.memoryClockRate)\n                        * static_cast<uint64_t>(deviceProperties.memoryBusWidth) *\n                        /*\n                         * clock rate: kHZ -> HZ (multiply by 1000)\n                         * bus width: bits -> bytes (divide by 8)\n                         * 2x DDR factor (multiply by 2)\n                         */\n                        UINT64_C(250));\n            case ONNXIFI_BACKEND_PCI_BUS_ID:\n                return setUIntInfo(infoValue, infoValueSize, static_cast<uint64_t>(deviceProperties.pciBusID));\n            case ONNXIFI_BACKEND_PCI_DEVICE_ID:\n                return setUIntInfo(infoValue, infoValueSize, static_cast<uint64_t>(deviceProperties.pciDeviceID));\n            case ONNXIFI_BACKEND_PCI_DOMAIN_ID:\n                return setUIntInfo(infoValue, infoValueSize, static_cast<uint64_t>(deviceProperties.pciDomainID));\n            case ONNXIFI_BACKEND_MACS_FP32:\n            {\n                /*\n                 * See \"32-bit floating-point add, multiply, multiply-add\" in\n                 * \"Throughput of Native Arithmetic Instructions\" table in\n                 * CUDA Programming Guide. Multiply by 2 because we could FMA\n                 * as two FLOPs.\n                 */\n                uint64_t flopsPerCycle = 0;\n                switch (deviceProperties.major)\n                {\n                case 3:\n                    /* Kepler */\n                    flopsPerCycle = 192 * 2;\n                    break;\n                case 5:\n                    /* Maxwell */\n                    flopsPerCycle = 128 * 2;\n                    break;\n                case 6:\n                    /* Pascal */\n                    switch (deviceProperties.minor)\n                    {\n                    case 0: flopsPerCycle = 64 * 2; break;\n                    case 1: flopsPerCycle = 128 * 2; break;\n                    case 2: flopsPerCycle = 128 * 2; break;\n                    }\n                    break;\n                case 7:\n                    /* Volta */\n                    if (deviceProperties.minor == 0)\n                    {\n                        flopsPerCycle = 64 * 2;\n                    }\n                    break;\n                }\n                if (flopsPerCycle == 0)\n                {\n                    return ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE;\n                }\n                return setUIntInfo(infoValue, infoValueSize, UINT64_C(1000) /* KHz -> Hz */\n                        * static_cast<uint64_t>(deviceProperties.clockRate)\n                        * static_cast<uint64_t>(deviceProperties.multiProcessorCount) * flopsPerCycle);\n            }\n            case ONNXIFI_BACKEND_MACS_FP16:\n            {\n                /*\n                 * See \"16-bit floating-point add, multiply, multiply-add\" and\n                 * \"32-bit floating-point add, multiply, multiply-add\" in\n                 * \"Throughput of Native Arithmetic Instructions\" table in\n                 * CUDA Programming Guide. Use the maximum among 16-bit and 32-bit\n                 * throughput. Multiply by 2 because we could FMA as two FLOPs.\n                 */\n                uint64_t flopsPerCycle = 0;\n                switch (deviceProperties.major)\n                {\n                case 3:\n                    /* Kepler */\n                    flopsPerCycle = 192 * 2;\n                    break;\n                case 5:\n                    /* Maxwell */\n                    if (deviceProperties.minor == 3)\n                    {\n                        /* Maxwell-based Tegra supports FP16 at 2x rate */\n                        flopsPerCycle = 256 * 2;\n                    }\n                    else\n                    {\n                        flopsPerCycle = 128 * 2;\n                    }\n                    break;\n                case 6:\n                    /* Pascal */\n                    switch (deviceProperties.minor)\n                    {\n                    case 0:\n                        /* Use FP16 */\n                        flopsPerCycle = 128 * 2;\n                        break;\n                    case 1:\n                        /* Use FP32 */\n                        flopsPerCycle = 128 * 2;\n                        break;\n                    case 2:\n                        /* Use FP16 */\n                        flopsPerCycle = 256 * 2;\n                        break;\n                    }\n                    break;\n                case 7:\n                    /* Volta */\n                    if (deviceProperties.minor == 0)\n                    {\n                        /*\n                         * Tensor Core:\n                         * - 8 Tensor Cores per multiprocessor\n                         * - 64 FMA/cycle on each Tensor Core\n                         * - 2 FLOPs / FMA\n                         */\n                        flopsPerCycle = 8 * 64 * 2;\n                    }\n                    break;\n                }\n                if (flopsPerCycle == 0)\n                {\n                    return ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE;\n                }\n                return setUIntInfo(infoValue, infoValueSize, UINT64_C(1000) /* KHz -> Hz */\n                        * static_cast<uint64_t>(deviceProperties.clockRate)\n                        * static_cast<uint64_t>(deviceProperties.multiProcessorCount) * flopsPerCycle);\n            }\n            default: return ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE;\n            }\n        }\n        case ONNXIFI_BACKEND_CUDA_INDEX:\n            return setUIntInfo(infoValue, infoValueSize, static_cast<uint64_t>(cudaDeviceId));\n        default: return ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE;\n        }\n    });\n}\n\nONNXIFI_PUBLIC ONNXIFI_CHECK_RESULT onnxStatus ONNXIFI_ABI onnxGetBackendCompatibility(\n    onnxBackendID backendID, size_t onnxModelSize, const void* onnxModel)\n{\n    return OnnxifiTryCatch([&] {\n        if (!onnxModel)\n        {\n            return ONNXIFI_STATUS_INVALID_POINTER;\n        }\n        if (onnxModelSize == 0)\n        {\n            return ONNXIFI_STATUS_INVALID_SIZE;\n        }\n\n        TRT_Logger trt_logger;\n        std::shared_ptr<nvinfer1::IBuilder> trt_builder = infer_object(nvinfer1::createInferBuilder(trt_logger));\n        std::shared_ptr<nvinfer1::INetworkDefinition> trt_network = infer_object(trt_builder->createNetwork());\n        auto parser = infer_object(nvonnxparser::createParser(*trt_network, trt_logger));\n        if (parser->supportsModel(onnxModel, onnxModelSize))\n        {\n            return ONNXIFI_STATUS_SUCCESS;\n        }\n        else\n        {\n            return ONNXIFI_STATUS_UNSUPPORTED_OPERATOR;\n        }\n    });\n}\n\n// NB: Passing arguments to backend is tricky. And we need more documentation\n// for it I didn't put any arguments here for now.\n// TODO: submit arguments for\n// - setMaxBatchSize (size_t)\n// - setMaxWorkspaceSize (size_t)\n// - setHalf2Mode (bool)\n// - setInt8Mode (bool)\n// - setDebugSync (bool)\nONNXIFI_PUBLIC ONNXIFI_CHECK_RESULT onnxStatus ONNXIFI_ABI onnxInitBackend(\n    onnxBackendID backendID, const uint64_t* auxPropertiesList, onnxBackend* backend)\n{\n    auto ret = OnnxifiTryCatch([&] {\n        auto* backend_id = reinterpret_cast<OnnxTensorRTBackendID*>(backendID);\n        if (!backend_id)\n        {\n            return ONNXIFI_STATUS_INVALID_ID;\n        }\n        *backend = (onnxBackend)(new OnnxTensorRTBackendRep(*backend_id));\n        return ONNXIFI_STATUS_SUCCESS;\n    });\n    if (ret != ONNXIFI_STATUS_SUCCESS)\n    {\n        *backend = NULL;\n    }\n    return ret;\n}\n\nONNXIFI_PUBLIC ONNXIFI_CHECK_RESULT onnxStatus ONNXIFI_ABI onnxReleaseBackend(onnxBackend backend)\n{\n    return OnnxifiTryCatch([&] {\n        auto* backendrep = reinterpret_cast<OnnxTensorRTBackendRep*>(backend);\n        if (!backendrep)\n        {\n            return ONNXIFI_STATUS_INVALID_BACKEND;\n        }\n        delete backendrep;\n        return ONNXIFI_STATUS_SUCCESS;\n    });\n}\n\nONNXIFI_PUBLIC ONNXIFI_CHECK_RESULT onnxStatus ONNXIFI_ABI onnxInitEvent(onnxBackend backend, onnxEvent* event)\n{\n    auto ret = OnnxifiTryCatch([&] {\n        if (!event)\n        {\n            return ONNXIFI_STATUS_INVALID_POINTER;\n        }\n        auto* backendrep = reinterpret_cast<OnnxTensorRTBackendRep*>(backend);\n        if (!backendrep)\n        {\n            return ONNXIFI_STATUS_INVALID_BACKEND;\n        }\n        *event = reinterpret_cast<onnxEvent>(new OnnxTensorRTEvent(backendrep->stream()));\n        return ONNXIFI_STATUS_SUCCESS;\n    });\n    if (ret != ONNXIFI_STATUS_SUCCESS)\n    {\n        *event = NULL;\n    }\n    return ret;\n}\n\nONNXIFI_PUBLIC ONNXIFI_CHECK_RESULT onnxStatus ONNXIFI_ABI onnxSignalEvent(onnxEvent event)\n{\n    return OnnxifiTryCatch([&] {\n        auto trt_event = reinterpret_cast<OnnxTensorRTEvent*>(event);\n        if (!trt_event)\n        {\n            return ONNXIFI_STATUS_INVALID_EVENT;\n        }\n        return trt_event->Signal();\n    });\n}\n\nONNXIFI_PUBLIC ONNXIFI_CHECK_RESULT onnxStatus ONNXIFI_ABI onnxWaitEvent(onnxEvent event)\n{\n    return OnnxifiTryCatch([&] {\n        auto trt_event = reinterpret_cast<OnnxTensorRTEvent*>(event);\n        if (!trt_event)\n        {\n            return ONNXIFI_STATUS_INVALID_EVENT;\n        }\n        return trt_event->Wait();\n    });\n}\n\nONNXIFI_PUBLIC ONNXIFI_CHECK_RESULT onnxStatus ONNXIFI_ABI onnxGetEventState(onnxEvent event, onnxEventState* state)\n{\n    return OnnxifiTryCatch([&] {\n        if (!state)\n        {\n            return ONNXIFI_STATUS_INVALID_POINTER;\n        }\n        *state = ONNXIFI_EVENT_STATE_INVALID;\n        auto trt_event = reinterpret_cast<OnnxTensorRTEvent*>(event);\n        if (!trt_event)\n        {\n            return ONNXIFI_STATUS_INVALID_EVENT;\n        }\n        return trt_event->CheckState(state);\n    });\n}\n\nONNXIFI_PUBLIC ONNXIFI_CHECK_RESULT onnxStatus ONNXIFI_ABI onnxReleaseEvent(onnxEvent event)\n{\n    return OnnxifiTryCatch([&] {\n        auto* trt_event = reinterpret_cast<OnnxTensorRTEvent*>(event);\n        if (!trt_event)\n        {\n            return ONNXIFI_STATUS_INVALID_EVENT;\n        }\n        delete trt_event;\n        return ONNXIFI_STATUS_SUCCESS;\n    });\n}\n\nONNXIFI_PUBLIC ONNXIFI_CHECK_RESULT onnxStatus ONNXIFI_ABI onnxInitGraph(onnxBackend backend,\n    const uint64_t* auxPropertiesList, size_t onnxModelSize, const void* onnxModel, uint32_t weightsCount,\n    const onnxTensorDescriptorV1* weightDescriptors, onnxGraph* graph)\n{\n    auto ret = OnnxifiTryCatch([&] {\n        auto* backendrep = reinterpret_cast<OnnxTensorRTBackendRep*>(backend);\n        if (!backendrep)\n        {\n            return ONNXIFI_STATUS_INVALID_BACKEND;\n        }\n        if (!onnxModel)\n        {\n            return ONNXIFI_STATUS_INVALID_POINTER;\n        }\n        if (onnxModelSize == 0)\n        {\n            return ONNXIFI_STATUS_INVALID_SIZE;\n        }\n\n        for (auto i = 0U; i < weightsCount; ++i)\n        {\n            if (weightDescriptors[i].tag != ONNXIFI_TAG_TENSOR_DESCRIPTOR_V1)\n            {\n                return ONNXIFI_STATUS_UNSUPPORTED_TAG;\n            }\n        }\n\n        // Parse the model\n        auto ret = backendrep->ImportModel(onnxModel, onnxModelSize, weightsCount, weightDescriptors);\n        if (ret != ONNXIFI_STATUS_SUCCESS)\n        {\n            return ret;\n        }\n\n        // Create the TRT engine\n        *graph = (onnxGraph)(new GraphRep(backendrep));\n        return ONNXIFI_STATUS_SUCCESS;\n    });\n    if (ret != ONNXIFI_STATUS_SUCCESS)\n    {\n        *graph = NULL;\n    }\n    return ret;\n}\n\n// NB: in the context of TRT, this step will setup the input/output bindings for\n// ICudaEngine\nONNXIFI_PUBLIC ONNXIFI_CHECK_RESULT onnxStatus ONNXIFI_ABI onnxSetGraphIO(onnxGraph graph, uint32_t inputsCount,\n    const onnxTensorDescriptorV1* inputDescriptors, uint32_t outputsCount,\n    const onnxTensorDescriptorV1* outputDescriptors)\n{\n    return OnnxifiTryCatch([&] {\n        auto* graph_rep = reinterpret_cast<GraphRep*>(graph);\n        if (!graph_rep)\n        {\n            return ONNXIFI_STATUS_INVALID_GRAPH;\n        }\n        if (!inputDescriptors || !outputDescriptors)\n        {\n            return ONNXIFI_STATUS_INVALID_POINTER;\n        }\n\n        return graph_rep->InitIO(inputsCount, inputDescriptors, outputsCount, outputDescriptors);\n    });\n}\n\nONNXIFI_PUBLIC ONNXIFI_CHECK_RESULT onnxStatus ONNXIFI_ABI onnxRunGraph(\n    onnxGraph graph, const onnxMemoryFenceV1* inputFence, onnxMemoryFenceV1* outputFence)\n{\n    return OnnxifiTryCatch([&] {\n        if (!inputFence || !outputFence)\n        {\n            return ONNXIFI_STATUS_INVALID_POINTER;\n        }\n        if (inputFence->tag != ONNXIFI_TAG_MEMORY_FENCE_V1 || outputFence->tag != ONNXIFI_TAG_MEMORY_FENCE_V1)\n        {\n            return ONNXIFI_STATUS_UNSUPPORTED_TAG;\n        }\n        auto* trt_event = reinterpret_cast<OnnxTensorRTEvent*>(inputFence->event);\n        auto ret = trt_event->Wait();\n        if (ret != ONNXIFI_STATUS_SUCCESS)\n        {\n            return ret;\n        }\n        auto* graph_rep = reinterpret_cast<GraphRep*>(graph);\n        if (!graph_rep)\n        {\n            return ONNXIFI_STATUS_INVALID_GRAPH;\n        }\n\n        ret = graph_rep->Run();\n        auto output_event = new OnnxTensorRTEvent(graph_rep->stream());\n        outputFence->event = reinterpret_cast<onnxEvent>(output_event);\n        outputFence->type = ONNXIFI_SYNCHRONIZATION_EVENT;\n        output_event->Signal();\n        return ret;\n    });\n}\n\nONNXIFI_PUBLIC ONNXIFI_CHECK_RESULT onnxStatus ONNXIFI_ABI onnxReleaseGraph(onnxGraph graph)\n{\n    return OnnxifiTryCatch([&] {\n        auto* graph_rep = reinterpret_cast<GraphRep*>(graph);\n        if (!graph_rep)\n        {\n            return ONNXIFI_STATUS_INVALID_GRAPH;\n        }\n        delete graph_rep;\n        return ONNXIFI_STATUS_SUCCESS;\n    });\n}\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.0107421875,
          "content": "# SPDX-License-Identifier: Apache-2.0\n\nimport os\nimport sys\nimport onnx_tensorrt\nfrom setuptools import setup, find_packages\n\ndef no_publish():\n    blacklist = ['register']\n    for cmd in blacklist:\n        if cmd in sys.argv:\n            raise RuntimeError(\"Command \\\"{}\\\" blacklisted\".format(cmd))\n\n\nREQUIRED_PACKAGES = [\n    \"pycuda\",\n    \"numpy\",\n    \"onnx\"\n]\n\ndef main():\n    no_publish()\n    setup(\n        name=\"onnx_tensorrt\",\n        version=onnx_tensorrt.__version__,\n        description=\"ONNX-TensorRT - TensorRT backend for running ONNX models\",\n        long_description=open(\"README.md\", \"r\", encoding=\"utf-8\").read(),\n        url=\"https://github.com/onnx/onnx-tensorrt\",\n        author=\"NVIDIA\",\n        author_email=\"svc_tensorrt@nvidia.com\",\n        classifiers=[\n            'Intended Audience :: Developers',\n            'Programming Language :: Python :: 3',\n        ],\n        install_requires=REQUIRED_PACKAGES,\n        packages=find_packages(),\n        zip_safe=True,\n    )\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        },
        {
          "name": "toposort.hpp",
          "type": "blob",
          "size": 2.71484375,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n\n#include <unordered_map>\n#include <vector>\n\n#include <iostream>\nusing std::cout;\nusing std::cerr;\nusing std::endl;\n\nnamespace\n{\n\nenum NodeState\n{\n    NODE_UNVISITED,\n    NODE_ACTIVE,\n    NODE_VISITED\n};\n\ntemplate <class Container>\nbool get_post_order(size_t node_idx, Container const& nodes, std::unordered_map<std::string, size_t> const& node_map,\n    std::vector<NodeState>* node_states, std::vector<size_t>* order)\n{\n    NodeState& node_state = node_states->at(node_idx);\n    if (node_state == NODE_ACTIVE)\n    {\n        // Cycle detected!\n        cerr << \"ERROR: Graph contains a cycle\" << endl;\n        return false;\n    }\n    else if (node_state == NODE_VISITED)\n    {\n        return true;\n    }\n    else\n    {\n        node_state = NODE_ACTIVE;\n        // TODO: This .Get().input() is highly specific to protobuf, should\n        //       generalise it somehow.\n        for (auto const& input : nodes.Get(node_idx).input())\n        {\n            if (!node_map.count(input))\n            {\n                // Input node not found in graph!\n                // cerr << \"ERROR: Input node not found in graph: \"\n                //     << input << endl;\n                // return false;\n                continue; // Skip missing input edges\n            }\n            size_t input_node_idx = node_map.at(input);\n            if (!get_post_order(input_node_idx, nodes, node_map, node_states, order))\n            {\n                return false;\n            }\n        }\n        node_state = NODE_VISITED;\n        order->push_back(node_idx);\n    }\n    return true;\n}\n\n} // anonymous namespace\n\ntemplate <class Container>\nbool toposort(Container const& nodes, std::vector<size_t>* order)\n{\n    std::unordered_map<std::string, size_t> node_map;\n    for (size_t i = 0; i < (size_t) nodes.size(); ++i)\n    {\n        // TODO: This .Get().input() is highly specific to protobuf, should\n        //       generalise it somehow.\n        for (auto const& output : nodes.Get(i).output())\n        {\n            // Empty output strings mean null outputs, do not register them.\n            if (output.empty())\n            {\n                continue;\n            }\n            if (!node_map.emplace(output, i).second)\n            {\n                // Output name appears more than once in graph!\n                cerr << \"ERROR: Output name is not unique: \" << output << endl;\n                return false;\n            }\n        }\n    }\n    order->reserve(nodes.size());\n    std::vector<NodeState> node_states(nodes.size(), NODE_UNVISITED);\n    for (size_t i = 0; i < (size_t) nodes.size(); ++i)\n    {\n        if (!get_post_order(i, nodes, node_map, &node_states, order))\n        {\n            return false;\n        }\n    }\n    return true;\n}\n"
        },
        {
          "name": "weightUtils.cpp",
          "type": "blob",
          "size": 5.505859375,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include \"weightUtils.hpp\"\n#include \"bfloat16.hpp\"\n#include \"half.h\"\n#include <cstring> // For std::memcpy\n#include <iostream>\n#include <limits>\n#include <numeric>\n#include <sstream>\n#include <typeindex>\n#include <unordered_map>\n#include <iterator>\n\nnamespace onnx2trt\n{\n\nchar const* getDtypeName(int32_t onnxDtype)\n{\n    switch (onnxDtype)\n    {\n    case ::ONNX_NAMESPACE::TensorProto::FLOAT: return \"FLOAT\";\n    case ::ONNX_NAMESPACE::TensorProto::UINT8: return \"UINT8\";\n    case ::ONNX_NAMESPACE::TensorProto::INT8: return \"INT8\";\n    case ::ONNX_NAMESPACE::TensorProto::UINT16: return \"UINT16\";\n    case ::ONNX_NAMESPACE::TensorProto::INT16: return \"INT16\";\n    case ::ONNX_NAMESPACE::TensorProto::INT32: return \"INT32\";\n    case ::ONNX_NAMESPACE::TensorProto::INT64: return \"INT64\";\n    case ::ONNX_NAMESPACE::TensorProto::STRING: return \"STRING\";\n    case ::ONNX_NAMESPACE::TensorProto::BOOL: return \"BOOL\";\n    case ::ONNX_NAMESPACE::TensorProto::FLOAT16: return \"FLOAT16\";\n    case ::ONNX_NAMESPACE::TensorProto::BFLOAT16: return \"BFLOAT16\";\n    case ::ONNX_NAMESPACE::TensorProto::DOUBLE: return \"DOUBLE\";\n    case ::ONNX_NAMESPACE::TensorProto::UINT32: return \"UINT32\";\n    case ::ONNX_NAMESPACE::TensorProto::UINT64: return \"UINT64\";\n    case ::ONNX_NAMESPACE::TensorProto::COMPLEX64: return \"COMPLEX64\";\n    case ::ONNX_NAMESPACE::TensorProto::COMPLEX128: return \"COMPLEX128\";\n    default: return \"<UNKNOWN>\";\n    }\n}\n\nint32_t getDtypeSizeBits(int32_t onnxDtype)\n{\n    switch (onnxDtype)\n    {\n    case ::ONNX_NAMESPACE::TensorProto::FLOAT16: return 16;\n    case ::ONNX_NAMESPACE::TensorProto::BFLOAT16: return 16;\n    case ::ONNX_NAMESPACE::TensorProto::FLOAT: return 32;\n    case ::ONNX_NAMESPACE::TensorProto::DOUBLE: return 64;\n    case ::ONNX_NAMESPACE::TensorProto::COMPLEX64: return 64;\n    case ::ONNX_NAMESPACE::TensorProto::COMPLEX128: return 128;\n    case ::ONNX_NAMESPACE::TensorProto::UINT8: return 8;\n    case ::ONNX_NAMESPACE::TensorProto::INT8: return 8;\n    case ::ONNX_NAMESPACE::TensorProto::UINT16: return 16;\n    case ::ONNX_NAMESPACE::TensorProto::INT16: return 16;\n    case ::ONNX_NAMESPACE::TensorProto::UINT32: return 32;\n    // Booleans are stored in int32 tensors in ONNX\n    case ::ONNX_NAMESPACE::TensorProto::BOOL: return 8;\n    case ::ONNX_NAMESPACE::TensorProto::INT32: return 32;\n    case ::ONNX_NAMESPACE::TensorProto::UINT64: return 64;\n    case ::ONNX_NAMESPACE::TensorProto::INT64: return 64;\n    case ::ONNX_NAMESPACE::TensorProto::FLOAT8E4M3FN: return 8;\n    case ::ONNX_NAMESPACE::TensorProto::INT4: return 4;\n    default: return -1;\n    }\n}\n\nsize_t getTensorOrWeightsSizeBytes(int64_t count, int32_t onnxDtype)\n{\n\n    int32_t dTypeSize = getDtypeSizeBits(onnxDtype);\n    \n    if (dTypeSize == -1 || static_cast<size_t>(count) > std::numeric_limits<size_t>::max() / static_cast<size_t>(dTypeSize))\n    {\n        throw std::runtime_error(\"Size of weights exceeds maximum!\");\n    }\n\n    int64_t sizeInBits = count * dTypeSize;\n    if (sizeInBits % 8 != 0)\n    {\n        // This is a specific implementation to INT4, since this is currently the only sub-byte data type\n        // we're supporting. Different data-types may have different padding.\n        assert(onnxDtype == ::ONNX_NAMESPACE::TensorProto::INT4);\n        sizeInBits += 4;\n    }\n    assert(sizeInBits % 8 == 0);\n    return static_cast<size_t>(sizeInBits / 8);\n}\n\nint64_t volume(nvinfer1::Dims const& dims)\n{\n    std::for_each(\n        dims.d, dims.d + dims.nbDims, [](int32_t d) { assert(d >= 0 && \"volume makes no sense for dynamic shapes\"); });\n    return std::accumulate(dims.d, dims.d + dims.nbDims, int64_t{1}, std::multiplies<int64_t>{});\n}\n\nstd::string normalizePath(std::string const& path)\n{\n    std::vector<std::string> normPath;\n    auto addToPath = [&normPath](std::string s) {\n        // Ignore all extra slashes, and current directory paths\n        if (s == \"/\" || s == \"./\")\n        {\n            return;\n        }\n        // Push back to normPath under the following circumstances\n        // 1. Current string is not \"../\" or\n        // 2. \"../\" if it's the first string or\n        // 3. \"../\" is the previous string in normPath\n        if (s != \"../\" || normPath.empty() || (!normPath.empty() && normPath.back() == \"../\"))\n        {\n            normPath.push_back(s);\n        }\n        // Remove previous entry since \"../\" was encountered.\n        else\n        {\n            normPath.pop_back();\n        }\n    };\n\n    size_t i = 0;\n    size_t n = path.size();\n    std::string sep = \"/\";\n\n    // Loop through path, split on all path seperator tokens, and append to normPath if applicable.\n    while (i < n)\n    {\n        auto slashPos = path.find(sep, i);\n        if (slashPos == std::string::npos)\n        {\n            addToPath(path.substr(i, n - i));\n            break;\n        }\n        else\n        {\n            addToPath(path.substr(i, slashPos - i + 1));\n            i = slashPos + 1;\n        }\n    }\n\n    // Build final output string\n    std::string out;\n    for (auto s : normPath)\n    {\n        out += s;\n    }\n    return out;\n}\n\nstd::string const& generateUniqueName(\n    std::set<std::string>& namesSet, int64_t& suffixCounter, std::string const& basename)\n{\n    std::string candidate = basename;\n\n    while (namesSet.find(candidate) != namesSet.end())\n    {\n        candidate = basename + \"_\" + std::to_string(suffixCounter);\n        ++suffixCounter;\n    }\n\n    namesSet.insert(candidate);\n    // Return reference to newly inserted string to avoid any c_str()'s going out of scope\n    return *namesSet.find(candidate);\n}\n\n} // namespace onnx2trt\n"
        },
        {
          "name": "weightUtils.hpp",
          "type": "blob",
          "size": 2.7021484375,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#pragma once\n#include \"ShapedWeights.hpp\"\n#include \"bfloat16.hpp\"\n#include \"half.h\"\n#include <NvInfer.h>\n#include <typeindex>\n#include <unordered_map>\n\n// Subset of helper functions that deal exclusively with weights to be shared across IParser and IParserRefitter classes.\n// Define weightLog Macros here to ensure that an ImporterCtx class is not needed to log.\n\nnamespace onnx2trt\n{\n\n// Return the name of an ONNX data enum.\nchar const* getDtypeName(int32_t onnxDtype);\n\n// Return the size in bits of an ONNX data type.\nint32_t getDtypeSizeBits(int32_t onnxDtype);\n\n// Return the size in bytes of an tensor/weights object, handle sub-byte padding.\nsize_t getTensorOrWeightsSizeBytes(int64_t count, int32_t onnxDtype);\n\n// Find the corresponding ONNX data type of a built-in data type.\ntemplate <typename T>\nShapedWeights::DataType getShapedWeightsDataType()\n{\n    static std::unordered_map<std::type_index, ::ONNX_NAMESPACE::TensorProto::DataType> const tMap({\n        {std::type_index(typeid(bool)), ::ONNX_NAMESPACE::TensorProto::BOOL},\n        {std::type_index(typeid(int8_t)), ::ONNX_NAMESPACE::TensorProto::INT8},\n        {std::type_index(typeid(uint8_t)), ::ONNX_NAMESPACE::TensorProto::UINT8},\n        {std::type_index(typeid(int16_t)), ::ONNX_NAMESPACE::TensorProto::INT16},\n        {std::type_index(typeid(uint16_t)), ::ONNX_NAMESPACE::TensorProto::UINT16},\n        {std::type_index(typeid(int32_t)), ::ONNX_NAMESPACE::TensorProto::INT32},\n        {std::type_index(typeid(uint32_t)), ::ONNX_NAMESPACE::TensorProto::UINT32},\n        {std::type_index(typeid(int64_t)), ::ONNX_NAMESPACE::TensorProto::INT64},\n        {std::type_index(typeid(uint64_t)), ::ONNX_NAMESPACE::TensorProto::UINT64},\n        {std::type_index(typeid(float)), ::ONNX_NAMESPACE::TensorProto::FLOAT},\n        {std::type_index(typeid(double)), ::ONNX_NAMESPACE::TensorProto::DOUBLE},\n        {std::type_index(typeid(half_float::half)), ::ONNX_NAMESPACE::TensorProto::FLOAT16},\n        {std::type_index(typeid(BFloat16)), ::ONNX_NAMESPACE::TensorProto::BFLOAT16},\n        // TRT-22989: Add fp8 and int4 support\n    });\n\n    if (tMap.find(std::type_index(typeid(T))) != tMap.end())\n    {\n        return tMap.at(std::type_index(typeid(T)));\n    }\n    return ::ONNX_NAMESPACE::TensorProto::UNDEFINED;\n}\n\n// Return the volume of a Dims object\nint64_t volume(nvinfer1::Dims const& dims);\n\n// Normalize the slashes in a string representing a filepath.\nstd::string normalizePath(std::string const& path);\n\n// Generate a unique name for a given weight or tensor name (passed as the |basename|)\nstd::string const& generateUniqueName(\n    std::set<std::string>& namesSet, int64_t& suffixCounter, std::string const& basename);\n\n} // namespace onnx2trt\n"
        }
      ]
    }
  ]
}