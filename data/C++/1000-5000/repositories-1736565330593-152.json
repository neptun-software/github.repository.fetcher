{
  "metadata": {
    "timestamp": 1736565330593,
    "page": 152,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "cameron314/readerwriterqueue",
      "stars": 3896,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.4921875,
          "content": "*.ipch\r\n*.suo\r\n*.user\r\n*.sdf\r\n*.opensdf\r\n*.exe\r\n*.VC.db\r\n.vs/\r\ntests/stabtest/msvc*/Debug/\r\ntests/stabtest/msvc*/Release/\r\ntests/stabtest/msvc*/obj/\r\ntests/stabtest/msvc*/log.txt\r\ntests/stabtest/log.txt\r\ntests/unittests/msvc*/Debug/\r\ntests/unittests/msvc*/Release/\r\ntests/unittests/msvc*/obj/\r\ntests/CDSChecker/model-checker/\r\nbenchmarks/msvc*/Debug/\r\nbenchmarks/msvc*/Release/\r\nbenchmarks/msvc*/obj/\r\ntest/\r\n# Linux binaries\r\nbenchmarks/benchmarks\r\ntests/stabtest/stabtest\r\ntests/unittests/unittests\r\n\r\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 2.1552734375,
          "content": "cmake_minimum_required(VERSION 3.9)\nproject(readerwriterqueue VERSION 1.0.0)\n\ninclude(GNUInstallDirs)\ninclude(CMakePackageConfigHelpers)\n\nadd_library(${PROJECT_NAME} INTERFACE)\n\ntarget_include_directories(readerwriterqueue INTERFACE\n                                             $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}>\n                                             $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}/${PROJECT_NAME}/>\n)\n\ninstall(FILES atomicops.h readerwriterqueue.h readerwritercircularbuffer.h LICENSE.md\n        DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/${PROJECT_NAME})\n\ninstall(TARGETS ${PROJECT_NAME}\n    EXPORT ${PROJECT_NAME}Targets\n)\n\nwrite_basic_package_version_file(\n        ${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}ConfigVersion.cmake\n    VERSION\n        ${PROJECT_VERSION}\n    COMPATIBILITY AnyNewerVersion\n)\n\nconfigure_package_config_file(${PROJECT_NAME}Config.cmake.in\n                ${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}Config.cmake\n        INSTALL_DESTINATION\n                ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}/\n)\n\ninstall(EXPORT\n                ${PROJECT_NAME}Targets\n        FILE\n                ${PROJECT_NAME}Targets.cmake\n        NAMESPACE\n                \"${PROJECT_NAME}::\"\n        DESTINATION\n                ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\n        COMPONENT\n                Devel\n)\n\ninstall(\n        FILES\n                ${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}Config.cmake\n                ${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}ConfigVersion.cmake\n        DESTINATION\n                ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\n        COMPONENT\n                Devel\n)\n\nset(CPACK_PACKAGE_NAME ${PROJECT_NAME})\nset(CPACK_PACKAGE_VENDOR \"Cameron Desrochers <cameron@moodycamel.com>\")\nset(CPACK_PACKAGE_DESCRIPTION_SUMMARY \"A single-producer, single-consumer lock-free queue for C++.\")\nset(CPACK_PACKAGE_VERSION \"${PROJECT_VERSION}\")\nset(CPACK_PACKAGE_VERSION_MAJOR \"${PROJECT_VERSION_MAJOR}\")\nset(CPACK_PACKAGE_VERSION_MINOR \"${PROJECT_VERSION_MINOR}\")\nset(CPACK_PACKAGE_VERSION_PATCH \"${PROJECT_VERSION_PATCH}\")\nset(CPACK_DEBIAN_PACKAGE_MAINTAINER ${CPACK_PACKAGE_VENDOR})\nset(CPACK_GENERATOR \"RPM;DEB\")\n\ninclude(CPack)\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.5771484375,
          "content": "This license applies to all the code in this repository except that written by third\nparties, namely the files in benchmarks/ext, which have their own licenses, and Jeff\nPreshing's semaphore implementation (used in the blocking queues) which has a zlib\nlicense (embedded in atomicops.h).\n\nSimplified BSD License:\n\nCopyright (c) 2013-2021, Cameron Desrochers  \nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n- Redistributions of source code must retain the above copyright notice, this list of\nconditions and the following disclaimer.\n- Redistributions in binary form must reproduce the above copyright notice, this list of\nconditions and the following disclaimer in the documentation and/or other materials\nprovided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY\nEXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL\nTHE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT\nOF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR\nTORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\nEVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.0537109375,
          "content": "\n# A single-producer, single-consumer lock-free queue for C++\n\nThis mini-repository has my very own implementation of a lock-free queue (that I designed from scratch) for C++.\n\nIt only supports a two-thread use case (one consuming, and one producing). The threads can't switch roles, though\nyou could use this queue completely from a single thread if you wish (but that would sort of defeat the purpose!).\n\nNote: If you need a general-purpose multi-producer, multi-consumer lock free queue, I have [one of those too][mpmc].\n\nThis repository also includes a [circular-buffer SPSC queue][circular] which supports blocking on enqueue as well as dequeue.\n\n\n## Features\n\n- [Blazing fast][benchmarks]\n- Compatible with C++11 (supports moving objects instead of making copies)\n- Fully generic (templated container of any type) -- just like `std::queue`, you never need to allocate memory for elements yourself\n  (which saves you the hassle of writing a lock-free memory manager to hold the elements you're queueing)\n- Allocates memory up front, in contiguous blocks\n- Provides a `try_enqueue` method which is guaranteed never to allocate memory (the queue starts with an initial capacity)\n- Also provides an `enqueue` method which can dynamically grow the size of the queue as needed\n- Also provides `try_emplace`/`emplace` convenience methods\n- Has a blocking version with `wait_dequeue`\n- Completely \"wait-free\" (no compare-and-swap loop). Enqueue and dequeue are always O(1) (not counting memory allocation)\n- On x86, the memory barriers compile down to no-ops, meaning enqueue and dequeue are just a simple series of loads and stores (and branches)\n\n\n## Use\n\nSimply drop the readerwriterqueue.h (or readerwritercircularbuffer.h) and atomicops.h files into your source code and include them :-)\nA modern compiler is required (MSVC2010+, GCC 4.7+, ICC 13+, or any C++11 compliant compiler should work).\n\nNote: If you're using GCC, you really do need GCC 4.7 or above -- [4.6 has a bug][gcc46bug] that prevents the atomic fence primitives\nfrom working correctly.\n\nExample:\n\n```cpp\nusing namespace moodycamel;\n\nReaderWriterQueue<int> q(100);       // Reserve space for at least 100 elements up front\n\nq.enqueue(17);                       // Will allocate memory if the queue is full\nbool succeeded = q.try_enqueue(18);  // Will only succeed if the queue has an empty slot (never allocates)\nassert(succeeded);\n\nint number;\nsucceeded = q.try_dequeue(number);  // Returns false if the queue was empty\n\nassert(succeeded && number == 17);\n\n// You can also peek at the front item of the queue (consumer only)\nint* front = q.peek();\nassert(*front == 18);\nsucceeded = q.try_dequeue(number);\nassert(succeeded && number == 18);\nfront = q.peek(); \nassert(front == nullptr);           // Returns nullptr if the queue was empty\n```\n\nThe blocking version has the exact same API, with the addition of `wait_dequeue` and\n`wait_dequeue_timed` methods:\n\n```cpp\nBlockingReaderWriterQueue<int> q;\n\nstd::thread reader([&]() {\n    int item;\n#if 1\n    for (int i = 0; i != 100; ++i) {\n        // Fully-blocking:\n        q.wait_dequeue(item);\n    }\n#else\n    for (int i = 0; i != 100; ) {\n        // Blocking with timeout\n        if (q.wait_dequeue_timed(item, std::chrono::milliseconds(5)))\n            ++i;\n    }\n#endif\n});\nstd::thread writer([&]() {\n    for (int i = 0; i != 100; ++i) {\n        q.enqueue(i);\n        std::this_thread::sleep_for(std::chrono::milliseconds(10));\n    }\n});\nwriter.join();\nreader.join();\n\nassert(q.size_approx() == 0);\n```\n    \nNote that `wait_dequeue` will block indefinitely while the queue is empty; this\nmeans care must be taken to only call `wait_dequeue` if you're sure another element\nwill come along eventually, or if the queue has a static lifetime. This is because\ndestroying the queue while a thread is waiting on it will invoke undefined behaviour.\n\nThe blocking circular buffer has a fixed number of slots, but is otherwise quite similar to\nuse:\n\n```cpp\nBlockingReaderWriterCircularBuffer<int> q(1024);  // pass initial capacity\n\nq.try_enqueue(1);\nint number;\nq.try_dequeue(number);\nassert(number == 1);\n\nq.wait_enqueue(123);\nq.wait_dequeue(number);\nassert(number == 123);\n\nq.wait_dequeue_timed(number, std::chrono::milliseconds(10));\n```\n\n\n## CMake\n### Using targets in your project\nUsing this project as a part of an existing CMake project is easy.\n\nIn your CMakeLists.txt:\n```\ninclude(FetchContent)\n\nFetchContent_Declare(\n  readerwriterqueue\n  GIT_REPOSITORY    https://github.com/cameron314/readerwriterqueue\n  GIT_TAG           master\n)\n\nFetchContent_MakeAvailable(readerwriterqueue)\n\nadd_library(my_target main.cpp)\ntarget_link_libraries(my_target PUBLIC readerwriterqueue)\n```\n\nIn main.cpp:\n```cpp\n#include <readerwriterqueue.h>\n\nint main()\n{\n    moodycamel::ReaderWriterQueue<int> q(100);\n}\n```\n\n### Installing into system directories\nAs an alternative to including the source files in your project directly,\nyou can use CMake to install the library in your system's include directory:\n\n```\nmkdir build\ncd build\ncmake ..\nmake install\n```\n\nThen, you can include it from your source code:\n```\n#include <readerwriterqueue/readerwriterqueue.h>\n```\n\n## Disclaimers\n\nThe queue should only be used on platforms where aligned integer and pointer access is atomic; fortunately, that\nincludes all modern processors (e.g. x86/x86-64, ARM, and PowerPC). *Not* for use with a DEC Alpha processor (which has very weak memory ordering) :-)\n\nNote that it's only been tested on x86(-64); if someone has access to other processors I'd love to run some tests on\nanything that's not x86-based.\n\n## More info\n\nSee the [LICENSE.md][license] file for the license (simplified BSD).\n\nMy [blog post][blog] introduces the context that led to this code, and may be of interest if you're curious\nabout lock-free programming.\n\n\n[blog]: http://moodycamel.com/blog/2013/a-fast-lock-free-queue-for-c++\n[license]: LICENSE.md\n[benchmarks]: http://moodycamel.com/blog/2013/a-fast-lock-free-queue-for-c++#benchmarks\n[gcc46bug]: http://stackoverflow.com/questions/16429669/stdatomic-thread-fence-has-undefined-reference\n[mpmc]: https://github.com/cameron314/concurrentqueue\n[circular]: readerwritercircularbuffer.h\n"
        },
        {
          "name": "atomicops.h",
          "type": "blob",
          "size": 22.5107421875,
          "content": "﻿// ©2013-2016 Cameron Desrochers.\n// Distributed under the simplified BSD license (see the license file that\n// should have come with this header).\n// Uses Jeff Preshing's semaphore implementation (under the terms of its\n// separate zlib license, embedded below).\n\n#pragma once\n\n// Provides portable (VC++2010+, Intel ICC 13, GCC 4.7+, and anything C++11 compliant) implementation\n// of low-level memory barriers, plus a few semi-portable utility macros (for inlining and alignment).\n// Also has a basic atomic type (limited to hardware-supported atomics with no memory ordering guarantees).\n// Uses the AE_* prefix for macros (historical reasons), and the \"moodycamel\" namespace for symbols.\n\n#include <cerrno>\n#include <cassert>\n#include <type_traits>\n#include <cerrno>\n#include <cstdint>\n#include <ctime>\n\n// Platform detection\n#if defined(__INTEL_COMPILER)\n#define AE_ICC\n#elif defined(_MSC_VER)\n#define AE_VCPP\n#elif defined(__GNUC__)\n#define AE_GCC\n#endif\n\n#if defined(_M_IA64) || defined(__ia64__)\n#define AE_ARCH_IA64\n#elif defined(_WIN64) || defined(__amd64__) || defined(_M_X64) || defined(__x86_64__)\n#define AE_ARCH_X64\n#elif defined(_M_IX86) || defined(__i386__)\n#define AE_ARCH_X86\n#elif defined(_M_PPC) || defined(__powerpc__)\n#define AE_ARCH_PPC\n#else\n#define AE_ARCH_UNKNOWN\n#endif\n\n\n// AE_UNUSED\n#define AE_UNUSED(x) ((void)x)\n\n// AE_NO_TSAN/AE_TSAN_ANNOTATE_*\n// For GCC\n#if defined(__SANITIZE_THREAD__)\n#define AE_TSAN_IS_ENABLED\n#endif\n// For clang\n#if defined(__has_feature)\n#if __has_feature(thread_sanitizer) && !defined(AE_TSAN_IS_ENABLED)\n#define AE_TSAN_IS_ENABLED\n#endif\n#endif\n\n#ifdef AE_TSAN_IS_ENABLED\n#if __cplusplus >= 201703L  // inline variables require C++17\nnamespace moodycamel { inline int ae_tsan_global; }\n#define AE_TSAN_ANNOTATE_RELEASE() AnnotateHappensBefore(__FILE__, __LINE__, (void *)(&::moodycamel::ae_tsan_global))\n#define AE_TSAN_ANNOTATE_ACQUIRE() AnnotateHappensAfter(__FILE__, __LINE__, (void *)(&::moodycamel::ae_tsan_global))\nextern \"C\" void AnnotateHappensBefore(const char*, int, void*);\nextern \"C\" void AnnotateHappensAfter(const char*, int, void*);\n#else  // when we can't work with tsan, attempt to disable its warnings\n#define AE_NO_TSAN __attribute__((no_sanitize(\"thread\")))\n#endif\n#endif\n\n#ifndef AE_NO_TSAN\n#define AE_NO_TSAN\n#endif\n\n#ifndef AE_TSAN_ANNOTATE_RELEASE\n#define AE_TSAN_ANNOTATE_RELEASE()\n#define AE_TSAN_ANNOTATE_ACQUIRE()\n#endif\n\n\n// AE_FORCEINLINE\n#if defined(AE_VCPP) || defined(AE_ICC)\n#define AE_FORCEINLINE __forceinline\n#elif defined(AE_GCC)\n//#define AE_FORCEINLINE __attribute__((always_inline)) \n#define AE_FORCEINLINE inline\n#else\n#define AE_FORCEINLINE inline\n#endif\n\n\n// AE_ALIGN\n#if defined(AE_VCPP) || defined(AE_ICC)\n#define AE_ALIGN(x) __declspec(align(x))\n#elif defined(AE_GCC)\n#define AE_ALIGN(x) __attribute__((aligned(x)))\n#else\n// Assume GCC compliant syntax...\n#define AE_ALIGN(x) __attribute__((aligned(x)))\n#endif\n\n\n// Portable atomic fences implemented below:\n\nnamespace moodycamel {\n\nenum memory_order {\n\tmemory_order_relaxed,\n\tmemory_order_acquire,\n\tmemory_order_release,\n\tmemory_order_acq_rel,\n\tmemory_order_seq_cst,\n\n\t// memory_order_sync: Forces a full sync:\n\t// #LoadLoad, #LoadStore, #StoreStore, and most significantly, #StoreLoad\n\tmemory_order_sync = memory_order_seq_cst\n};\n\n}    // end namespace moodycamel\n\n#if (defined(AE_VCPP) && (_MSC_VER < 1700 || defined(__cplusplus_cli))) || (defined(AE_ICC) && __INTEL_COMPILER < 1600)\n// VS2010 and ICC13 don't support std::atomic_*_fence, implement our own fences\n\n#include <intrin.h>\n\n#if defined(AE_ARCH_X64) || defined(AE_ARCH_X86)\n#define AeFullSync _mm_mfence\n#define AeLiteSync _mm_mfence\n#elif defined(AE_ARCH_IA64)\n#define AeFullSync __mf\n#define AeLiteSync __mf\n#elif defined(AE_ARCH_PPC)\n#include <ppcintrinsics.h>\n#define AeFullSync __sync\n#define AeLiteSync __lwsync\n#endif\n\n\n#ifdef AE_VCPP\n#pragma warning(push)\n#pragma warning(disable: 4365)\t\t// Disable erroneous 'conversion from long to unsigned int, signed/unsigned mismatch' error when using `assert`\n#ifdef __cplusplus_cli\n#pragma managed(push, off)\n#endif\n#endif\n\nnamespace moodycamel {\n\nAE_FORCEINLINE void compiler_fence(memory_order order) AE_NO_TSAN\n{\n\tswitch (order) {\n\t\tcase memory_order_relaxed: break;\n\t\tcase memory_order_acquire: _ReadBarrier(); break;\n\t\tcase memory_order_release: _WriteBarrier(); break;\n\t\tcase memory_order_acq_rel: _ReadWriteBarrier(); break;\n\t\tcase memory_order_seq_cst: _ReadWriteBarrier(); break;\n\t\tdefault: assert(false);\n\t}\n}\n\n// x86/x64 have a strong memory model -- all loads and stores have\n// acquire and release semantics automatically (so only need compiler\n// barriers for those).\n#if defined(AE_ARCH_X86) || defined(AE_ARCH_X64)\nAE_FORCEINLINE void fence(memory_order order) AE_NO_TSAN\n{\n\tswitch (order) {\n\t\tcase memory_order_relaxed: break;\n\t\tcase memory_order_acquire: _ReadBarrier(); break;\n\t\tcase memory_order_release: _WriteBarrier(); break;\n\t\tcase memory_order_acq_rel: _ReadWriteBarrier(); break;\n\t\tcase memory_order_seq_cst:\n\t\t\t_ReadWriteBarrier();\n\t\t\tAeFullSync();\n\t\t\t_ReadWriteBarrier();\n\t\t\tbreak;\n\t\tdefault: assert(false);\n\t}\n}\n#else\nAE_FORCEINLINE void fence(memory_order order) AE_NO_TSAN\n{\n\t// Non-specialized arch, use heavier memory barriers everywhere just in case :-(\n\tswitch (order) {\n\t\tcase memory_order_relaxed:\n\t\t\tbreak;\n\t\tcase memory_order_acquire:\n\t\t\t_ReadBarrier();\n\t\t\tAeLiteSync();\n\t\t\t_ReadBarrier();\n\t\t\tbreak;\n\t\tcase memory_order_release:\n\t\t\t_WriteBarrier();\n\t\t\tAeLiteSync();\n\t\t\t_WriteBarrier();\n\t\t\tbreak;\n\t\tcase memory_order_acq_rel:\n\t\t\t_ReadWriteBarrier();\n\t\t\tAeLiteSync();\n\t\t\t_ReadWriteBarrier();\n\t\t\tbreak;\n\t\tcase memory_order_seq_cst:\n\t\t\t_ReadWriteBarrier();\n\t\t\tAeFullSync();\n\t\t\t_ReadWriteBarrier();\n\t\t\tbreak;\n\t\tdefault: assert(false);\n\t}\n}\n#endif\n}    // end namespace moodycamel\n#else\n// Use standard library of atomics\n#include <atomic>\n\nnamespace moodycamel {\n\nAE_FORCEINLINE void compiler_fence(memory_order order) AE_NO_TSAN\n{\n\tswitch (order) {\n\t\tcase memory_order_relaxed: break;\n\t\tcase memory_order_acquire: std::atomic_signal_fence(std::memory_order_acquire); break;\n\t\tcase memory_order_release: std::atomic_signal_fence(std::memory_order_release); break;\n\t\tcase memory_order_acq_rel: std::atomic_signal_fence(std::memory_order_acq_rel); break;\n\t\tcase memory_order_seq_cst: std::atomic_signal_fence(std::memory_order_seq_cst); break;\n\t\tdefault: assert(false);\n\t}\n}\n\nAE_FORCEINLINE void fence(memory_order order) AE_NO_TSAN\n{\n\tswitch (order) {\n\t\tcase memory_order_relaxed: break;\n\t\tcase memory_order_acquire: AE_TSAN_ANNOTATE_ACQUIRE(); std::atomic_thread_fence(std::memory_order_acquire); break;\n\t\tcase memory_order_release: AE_TSAN_ANNOTATE_RELEASE(); std::atomic_thread_fence(std::memory_order_release); break;\n\t\tcase memory_order_acq_rel: AE_TSAN_ANNOTATE_ACQUIRE(); AE_TSAN_ANNOTATE_RELEASE(); std::atomic_thread_fence(std::memory_order_acq_rel); break;\n\t\tcase memory_order_seq_cst: AE_TSAN_ANNOTATE_ACQUIRE(); AE_TSAN_ANNOTATE_RELEASE(); std::atomic_thread_fence(std::memory_order_seq_cst); break;\n\t\tdefault: assert(false);\n\t}\n}\n\n}    // end namespace moodycamel\n\n#endif\n\n\n#if !defined(AE_VCPP) || (_MSC_VER >= 1700 && !defined(__cplusplus_cli))\n#define AE_USE_STD_ATOMIC_FOR_WEAK_ATOMIC\n#endif\n\n#ifdef AE_USE_STD_ATOMIC_FOR_WEAK_ATOMIC\n#include <atomic>\n#endif\n#include <utility>\n\n// WARNING: *NOT* A REPLACEMENT FOR std::atomic. READ CAREFULLY:\n// Provides basic support for atomic variables -- no memory ordering guarantees are provided.\n// The guarantee of atomicity is only made for types that already have atomic load and store guarantees\n// at the hardware level -- on most platforms this generally means aligned pointers and integers (only).\nnamespace moodycamel {\ntemplate<typename T>\nclass weak_atomic\n{\npublic:\n\tAE_NO_TSAN weak_atomic() : value() { }\n#ifdef AE_VCPP\n#pragma warning(push)\n#pragma warning(disable: 4100)\t\t// Get rid of (erroneous) 'unreferenced formal parameter' warning\n#endif\n\ttemplate<typename U> AE_NO_TSAN weak_atomic(U&& x) : value(std::forward<U>(x)) {  }\n#ifdef __cplusplus_cli\n\t// Work around bug with universal reference/nullptr combination that only appears when /clr is on\n\tAE_NO_TSAN weak_atomic(nullptr_t) : value(nullptr) {  }\n#endif\n\tAE_NO_TSAN weak_atomic(weak_atomic const& other) : value(other.load()) {  }\n\tAE_NO_TSAN weak_atomic(weak_atomic&& other) : value(std::move(other.load())) {  }\n#ifdef AE_VCPP\n#pragma warning(pop)\n#endif\n\n\tAE_FORCEINLINE operator T() const AE_NO_TSAN { return load(); }\n\n\t\n#ifndef AE_USE_STD_ATOMIC_FOR_WEAK_ATOMIC\n\ttemplate<typename U> AE_FORCEINLINE weak_atomic const& operator=(U&& x) AE_NO_TSAN { value = std::forward<U>(x); return *this; }\n\tAE_FORCEINLINE weak_atomic const& operator=(weak_atomic const& other) AE_NO_TSAN { value = other.value; return *this; }\n\t\n\tAE_FORCEINLINE T load() const AE_NO_TSAN { return value; }\n\t\n\tAE_FORCEINLINE T fetch_add_acquire(T increment) AE_NO_TSAN\n\t{\n#if defined(AE_ARCH_X64) || defined(AE_ARCH_X86)\n\t\tif (sizeof(T) == 4) return _InterlockedExchangeAdd((long volatile*)&value, (long)increment);\n#if defined(_M_AMD64)\n\t\telse if (sizeof(T) == 8) return _InterlockedExchangeAdd64((long long volatile*)&value, (long long)increment);\n#endif\n#else\n#error Unsupported platform\n#endif\n\t\tassert(false && \"T must be either a 32 or 64 bit type\");\n\t\treturn value;\n\t}\n\t\n\tAE_FORCEINLINE T fetch_add_release(T increment) AE_NO_TSAN\n\t{\n#if defined(AE_ARCH_X64) || defined(AE_ARCH_X86)\n\t\tif (sizeof(T) == 4) return _InterlockedExchangeAdd((long volatile*)&value, (long)increment);\n#if defined(_M_AMD64)\n\t\telse if (sizeof(T) == 8) return _InterlockedExchangeAdd64((long long volatile*)&value, (long long)increment);\n#endif\n#else\n#error Unsupported platform\n#endif\n\t\tassert(false && \"T must be either a 32 or 64 bit type\");\n\t\treturn value;\n\t}\n#else\n\ttemplate<typename U>\n\tAE_FORCEINLINE weak_atomic const& operator=(U&& x) AE_NO_TSAN\n\t{\n\t\tvalue.store(std::forward<U>(x), std::memory_order_relaxed);\n\t\treturn *this;\n\t}\n\t\n\tAE_FORCEINLINE weak_atomic const& operator=(weak_atomic const& other) AE_NO_TSAN\n\t{\n\t\tvalue.store(other.value.load(std::memory_order_relaxed), std::memory_order_relaxed);\n\t\treturn *this;\n\t}\n\n\tAE_FORCEINLINE T load() const AE_NO_TSAN { return value.load(std::memory_order_relaxed); }\n\t\n\tAE_FORCEINLINE T fetch_add_acquire(T increment) AE_NO_TSAN\n\t{\n\t\treturn value.fetch_add(increment, std::memory_order_acquire);\n\t}\n\t\n\tAE_FORCEINLINE T fetch_add_release(T increment) AE_NO_TSAN\n\t{\n\t\treturn value.fetch_add(increment, std::memory_order_release);\n\t}\n#endif\n\t\n\nprivate:\n#ifndef AE_USE_STD_ATOMIC_FOR_WEAK_ATOMIC\n\t// No std::atomic support, but still need to circumvent compiler optimizations.\n\t// `volatile` will make memory access slow, but is guaranteed to be reliable.\n\tvolatile T value;\n#else\n\tstd::atomic<T> value;\n#endif\n};\n\n}\t// end namespace moodycamel\n\n\n\n// Portable single-producer, single-consumer semaphore below:\n\n#if defined(_WIN32)\n// Avoid including windows.h in a header; we only need a handful of\n// items, so we'll redeclare them here (this is relatively safe since\n// the API generally has to remain stable between Windows versions).\n// I know this is an ugly hack but it still beats polluting the global\n// namespace with thousands of generic names or adding a .cpp for nothing.\nextern \"C\" {\n\tstruct _SECURITY_ATTRIBUTES;\n\t__declspec(dllimport) void* __stdcall CreateSemaphoreW(_SECURITY_ATTRIBUTES* lpSemaphoreAttributes, long lInitialCount, long lMaximumCount, const wchar_t* lpName);\n\t__declspec(dllimport) int __stdcall CloseHandle(void* hObject);\n\t__declspec(dllimport) unsigned long __stdcall WaitForSingleObject(void* hHandle, unsigned long dwMilliseconds);\n\t__declspec(dllimport) int __stdcall ReleaseSemaphore(void* hSemaphore, long lReleaseCount, long* lpPreviousCount);\n}\n#elif defined(__MACH__)\n#include <mach/mach.h>\n#elif defined(__unix__)\n#include <semaphore.h>\n#elif defined(FREERTOS)\n#include <FreeRTOS.h>\n#include <semphr.h>\n#include <task.h>\n#endif\n\nnamespace moodycamel\n{\n\t// Code in the spsc_sema namespace below is an adaptation of Jeff Preshing's\n\t// portable + lightweight semaphore implementations, originally from\n\t// https://github.com/preshing/cpp11-on-multicore/blob/master/common/sema.h\n\t// LICENSE:\n\t// Copyright (c) 2015 Jeff Preshing\n\t//\n\t// This software is provided 'as-is', without any express or implied\n\t// warranty. In no event will the authors be held liable for any damages\n\t// arising from the use of this software.\n\t//\n\t// Permission is granted to anyone to use this software for any purpose,\n\t// including commercial applications, and to alter it and redistribute it\n\t// freely, subject to the following restrictions:\n\t//\n\t// 1. The origin of this software must not be misrepresented; you must not\n\t//    claim that you wrote the original software. If you use this software\n\t//    in a product, an acknowledgement in the product documentation would be\n\t//    appreciated but is not required.\n\t// 2. Altered source versions must be plainly marked as such, and must not be\n\t//    misrepresented as being the original software.\n\t// 3. This notice may not be removed or altered from any source distribution.\n\tnamespace spsc_sema\n\t{\n#if defined(_WIN32)\n\t\tclass Semaphore\n\t\t{\n\t\tprivate:\n\t\t    void* m_hSema;\n\t\t    \n\t\t    Semaphore(const Semaphore& other);\n\t\t    Semaphore& operator=(const Semaphore& other);\n\n\t\tpublic:\n\t\t    AE_NO_TSAN Semaphore(int initialCount = 0) : m_hSema()\n\t\t    {\n\t\t        assert(initialCount >= 0);\n\t\t        const long maxLong = 0x7fffffff;\n\t\t        m_hSema = CreateSemaphoreW(nullptr, initialCount, maxLong, nullptr);\n\t\t        assert(m_hSema);\n\t\t    }\n\n\t\t    AE_NO_TSAN ~Semaphore()\n\t\t    {\n\t\t        CloseHandle(m_hSema);\n\t\t    }\n\n\t\t    bool wait() AE_NO_TSAN\n\t\t    {\n\t\t    \tconst unsigned long infinite = 0xffffffff;\n\t\t        return WaitForSingleObject(m_hSema, infinite) == 0;\n\t\t    }\n\n\t\t\tbool try_wait() AE_NO_TSAN\n\t\t\t{\n\t\t\t\treturn WaitForSingleObject(m_hSema, 0) == 0;\n\t\t\t}\n\n\t\t\tbool timed_wait(std::uint64_t usecs) AE_NO_TSAN\n\t\t\t{\n\t\t\t\treturn WaitForSingleObject(m_hSema, (unsigned long)(usecs / 1000)) == 0;\n\t\t\t}\n\n\t\t    void signal(int count = 1) AE_NO_TSAN\n\t\t    {\n\t\t        while (!ReleaseSemaphore(m_hSema, count, nullptr));\n\t\t    }\n\t\t};\n#elif defined(__MACH__)\n\t\t//---------------------------------------------------------\n\t\t// Semaphore (Apple iOS and OSX)\n\t\t// Can't use POSIX semaphores due to http://lists.apple.com/archives/darwin-kernel/2009/Apr/msg00010.html\n\t\t//---------------------------------------------------------\n\t\tclass Semaphore\n\t\t{\n\t\tprivate:\n\t\t    semaphore_t m_sema;\n\n\t\t    Semaphore(const Semaphore& other);\n\t\t    Semaphore& operator=(const Semaphore& other);\n\n\t\tpublic:\n\t\t    AE_NO_TSAN Semaphore(int initialCount = 0) : m_sema()\n\t\t    {\n\t\t        assert(initialCount >= 0);\n\t\t        kern_return_t rc = semaphore_create(mach_task_self(), &m_sema, SYNC_POLICY_FIFO, initialCount);\n\t\t        assert(rc == KERN_SUCCESS);\n\t\t        AE_UNUSED(rc);\n\t\t    }\n\n\t\t    AE_NO_TSAN ~Semaphore()\n\t\t    {\n\t\t        semaphore_destroy(mach_task_self(), m_sema);\n\t\t    }\n\n\t\t    bool wait() AE_NO_TSAN\n\t\t    {\n\t\t        return semaphore_wait(m_sema) == KERN_SUCCESS;\n\t\t    }\n\n\t\t\tbool try_wait() AE_NO_TSAN\n\t\t\t{\n\t\t\t\treturn timed_wait(0);\n\t\t\t}\n\n\t\t\tbool timed_wait(std::uint64_t timeout_usecs) AE_NO_TSAN\n\t\t\t{\n\t\t\t\tmach_timespec_t ts;\n\t\t\t\tts.tv_sec = static_cast<unsigned int>(timeout_usecs / 1000000);\n\t\t\t\tts.tv_nsec = static_cast<int>((timeout_usecs % 1000000) * 1000);\n\n\t\t\t\t// added in OSX 10.10: https://developer.apple.com/library/prerelease/mac/documentation/General/Reference/APIDiffsMacOSX10_10SeedDiff/modules/Darwin.html\n\t\t\t\tkern_return_t rc = semaphore_timedwait(m_sema, ts);\n\t\t\t\treturn rc == KERN_SUCCESS;\n\t\t\t}\n\n\t\t    void signal() AE_NO_TSAN\n\t\t    {\n\t\t        while (semaphore_signal(m_sema) != KERN_SUCCESS);\n\t\t    }\n\n\t\t    void signal(int count) AE_NO_TSAN\n\t\t    {\n\t\t        while (count-- > 0)\n\t\t        {\n\t\t            while (semaphore_signal(m_sema) != KERN_SUCCESS);\n\t\t        }\n\t\t    }\n\t\t};\n#elif defined(__unix__)\n\t\t//---------------------------------------------------------\n\t\t// Semaphore (POSIX, Linux)\n\t\t//---------------------------------------------------------\n\t\tclass Semaphore\n\t\t{\n\t\tprivate:\n\t\t    sem_t m_sema;\n\n\t\t    Semaphore(const Semaphore& other);\n\t\t    Semaphore& operator=(const Semaphore& other);\n\n\t\tpublic:\n\t\t    AE_NO_TSAN Semaphore(int initialCount = 0) : m_sema()\n\t\t    {\n\t\t        assert(initialCount >= 0);\n\t\t        int rc = sem_init(&m_sema, 0, static_cast<unsigned int>(initialCount));\n\t\t        assert(rc == 0);\n\t\t        AE_UNUSED(rc);\n\t\t    }\n\n\t\t    AE_NO_TSAN ~Semaphore()\n\t\t    {\n\t\t        sem_destroy(&m_sema);\n\t\t    }\n\n\t\t    bool wait() AE_NO_TSAN\n\t\t    {\n\t\t        // http://stackoverflow.com/questions/2013181/gdb-causes-sem-wait-to-fail-with-eintr-error\n\t\t        int rc;\n\t\t        do\n\t\t        {\n\t\t            rc = sem_wait(&m_sema);\n\t\t        }\n\t\t        while (rc == -1 && errno == EINTR);\n\t\t        return rc == 0;\n\t\t    }\n\n\t\t\tbool try_wait() AE_NO_TSAN\n\t\t\t{\n\t\t\t\tint rc;\n\t\t\t\tdo {\n\t\t\t\t\trc = sem_trywait(&m_sema);\n\t\t\t\t} while (rc == -1 && errno == EINTR);\n\t\t\t\treturn rc == 0;\n\t\t\t}\n\n\t\t\tbool timed_wait(std::uint64_t usecs) AE_NO_TSAN\n\t\t\t{\n\t\t\t\tstruct timespec ts;\n\t\t\t\tconst int usecs_in_1_sec = 1000000;\n\t\t\t\tconst int nsecs_in_1_sec = 1000000000;\n\t\t\t\tclock_gettime(CLOCK_REALTIME, &ts);\n\t\t\t\tts.tv_sec += static_cast<time_t>(usecs / usecs_in_1_sec);\n\t\t\t\tts.tv_nsec += static_cast<long>(usecs % usecs_in_1_sec) * 1000;\n\t\t\t\t// sem_timedwait bombs if you have more than 1e9 in tv_nsec\n\t\t\t\t// so we have to clean things up before passing it in\n\t\t\t\tif (ts.tv_nsec >= nsecs_in_1_sec) {\n\t\t\t\t\tts.tv_nsec -= nsecs_in_1_sec;\n\t\t\t\t\t++ts.tv_sec;\n\t\t\t\t}\n\n\t\t\t\tint rc;\n\t\t\t\tdo {\n\t\t\t\t\trc = sem_timedwait(&m_sema, &ts);\n\t\t\t\t} while (rc == -1 && errno == EINTR);\n\t\t\t\treturn rc == 0;\n\t\t\t}\n\n\t\t    void signal() AE_NO_TSAN\n\t\t    {\n\t\t        while (sem_post(&m_sema) == -1);\n\t\t    }\n\n\t\t    void signal(int count) AE_NO_TSAN\n\t\t    {\n\t\t        while (count-- > 0)\n\t\t        {\n\t\t            while (sem_post(&m_sema) == -1);\n\t\t        }\n\t\t    }\n\t\t};\n#elif defined(FREERTOS)\n\t\t//---------------------------------------------------------\n\t\t// Semaphore (FreeRTOS)\n\t\t//---------------------------------------------------------\n\t\tclass Semaphore\n\t\t{\n\t\tprivate:\n\t\t\tSemaphoreHandle_t m_sema;\n\n\t\t\tSemaphore(const Semaphore& other);\n\t\t\tSemaphore& operator=(const Semaphore& other);\n\n\t\tpublic:\n\t\t\tAE_NO_TSAN Semaphore(int initialCount = 0) : m_sema()\n\t\t\t{\n\t\t\t\tassert(initialCount >= 0);\n\t\t\t\tm_sema = xSemaphoreCreateCounting(static_cast<UBaseType_t>(~0ull), static_cast<UBaseType_t>(initialCount));\n\t\t\t\tassert(m_sema);\n\t\t\t}\n\n\t\t\tAE_NO_TSAN ~Semaphore()\n\t\t\t{\n\t\t\t\tvSemaphoreDelete(m_sema);\n\t\t\t}\n\n\t\t\tbool wait() AE_NO_TSAN\n\t\t\t{\n\t\t\t\treturn xSemaphoreTake(m_sema, portMAX_DELAY) == pdTRUE;\n\t\t\t}\n\n\t\t\tbool try_wait() AE_NO_TSAN\n\t\t\t{\n\t\t\t\t// Note: In an ISR context, if this causes a task to unblock,\n\t\t\t\t// the caller won't know about it\n\t\t\t\tif (xPortIsInsideInterrupt())\n\t\t\t\t\treturn xSemaphoreTakeFromISR(m_sema, NULL) == pdTRUE;\n\t\t\t\treturn xSemaphoreTake(m_sema, 0) == pdTRUE;\n\t\t\t}\n\n\t\t\tbool timed_wait(std::uint64_t usecs) AE_NO_TSAN\n\t\t\t{\n\t\t\t\tstd::uint64_t msecs = usecs / 1000;\n\t\t\t\tTickType_t ticks = static_cast<TickType_t>(msecs / portTICK_PERIOD_MS);\n\t\t\t\tif (ticks == 0)\n\t\t\t\t\treturn try_wait();\n\t\t\t\treturn xSemaphoreTake(m_sema, ticks) == pdTRUE;\n\t\t\t}\n\n\t\t\tvoid signal() AE_NO_TSAN\n\t\t\t{\n\t\t\t\t// Note: In an ISR context, if this causes a task to unblock,\n\t\t\t\t// the caller won't know about it\n\t\t\t\tBaseType_t rc;\n\t\t\t\tif (xPortIsInsideInterrupt())\n\t\t\t\t\trc = xSemaphoreGiveFromISR(m_sema, NULL);\n\t\t\t\telse\n\t\t\t\t\trc = xSemaphoreGive(m_sema);\n\t\t\t\tassert(rc == pdTRUE);\n\t\t\t\tAE_UNUSED(rc);\n\t\t\t}\n\n\t\t\tvoid signal(int count) AE_NO_TSAN\n\t\t\t{\n\t\t\t\twhile (count-- > 0)\n\t\t\t\t\tsignal();\n\t\t\t}\n\t\t};\n#else\n#error Unsupported platform! (No semaphore wrapper available)\n#endif\n\n\t\t//---------------------------------------------------------\n\t\t// LightweightSemaphore\n\t\t//---------------------------------------------------------\n\t\tclass LightweightSemaphore\n\t\t{\n\t\tpublic:\n\t\t\ttypedef std::make_signed<std::size_t>::type ssize_t;\n\t\t\t\n\t\tprivate:\n\t\t    weak_atomic<ssize_t> m_count;\n\t\t    Semaphore m_sema;\n\n\t\t    bool waitWithPartialSpinning(std::int64_t timeout_usecs = -1) AE_NO_TSAN\n\t\t    {\n\t\t        ssize_t oldCount;\n\t\t        // Is there a better way to set the initial spin count?\n\t\t        // If we lower it to 1000, testBenaphore becomes 15x slower on my Core i7-5930K Windows PC,\n\t\t        // as threads start hitting the kernel semaphore.\n\t\t        int spin = 1024;\n\t\t        while (--spin >= 0)\n\t\t        {\n\t\t            if (m_count.load() > 0)\n\t\t            {\n\t\t                m_count.fetch_add_acquire(-1);\n\t\t                return true;\n\t\t            }\n\t\t            compiler_fence(memory_order_acquire);     // Prevent the compiler from collapsing the loop.\n\t\t        }\n\t\t        oldCount = m_count.fetch_add_acquire(-1);\n\t\t\t\tif (oldCount > 0)\n\t\t\t\t\treturn true;\n\t\t        if (timeout_usecs < 0)\n\t\t\t\t{\n\t\t\t\t\tif (m_sema.wait())\n\t\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tif (timeout_usecs > 0 && m_sema.timed_wait(static_cast<uint64_t>(timeout_usecs)))\n\t\t\t\t\treturn true;\n\t\t\t\t// At this point, we've timed out waiting for the semaphore, but the\n\t\t\t\t// count is still decremented indicating we may still be waiting on\n\t\t\t\t// it. So we have to re-adjust the count, but only if the semaphore\n\t\t\t\t// wasn't signaled enough times for us too since then. If it was, we\n\t\t\t\t// need to release the semaphore too.\n\t\t\t\twhile (true)\n\t\t\t\t{\n\t\t\t\t\toldCount = m_count.fetch_add_release(1);\n\t\t\t\t\tif (oldCount < 0)\n\t\t\t\t\t\treturn false;    // successfully restored things to the way they were\n\t\t\t\t\t// Oh, the producer thread just signaled the semaphore after all. Try again:\n\t\t\t\t\toldCount = m_count.fetch_add_acquire(-1);\n\t\t\t\t\tif (oldCount > 0 && m_sema.try_wait())\n\t\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t    }\n\n\t\tpublic:\n\t\t    AE_NO_TSAN LightweightSemaphore(ssize_t initialCount = 0) : m_count(initialCount), m_sema()\n\t\t    {\n\t\t        assert(initialCount >= 0);\n\t\t    }\n\n\t\t    bool tryWait() AE_NO_TSAN\n\t\t    {\n\t\t        if (m_count.load() > 0)\n\t\t        {\n\t\t        \tm_count.fetch_add_acquire(-1);\n\t\t        \treturn true;\n\t\t        }\n\t\t        return false;\n\t\t    }\n\n\t\t    bool wait() AE_NO_TSAN\n\t\t    {\n\t\t        return tryWait() || waitWithPartialSpinning();\n\t\t    }\n\n\t\t\tbool wait(std::int64_t timeout_usecs) AE_NO_TSAN\n\t\t\t{\n\t\t\t\treturn tryWait() || waitWithPartialSpinning(timeout_usecs);\n\t\t\t}\n\n\t\t    void signal(ssize_t count = 1) AE_NO_TSAN\n\t\t    {\n\t\t    \tassert(count >= 0);\n\t\t        ssize_t oldCount = m_count.fetch_add_release(count);\n\t\t        assert(oldCount >= -1);\n\t\t        if (oldCount < 0)\n\t\t        {\n\t\t            m_sema.signal(1);\n\t\t        }\n\t\t    }\n\t\t    \n\t\t    std::size_t availableApprox() const AE_NO_TSAN\n\t\t    {\n\t\t    \tssize_t count = m_count.load();\n\t\t    \treturn count > 0 ? static_cast<std::size_t>(count) : 0;\n\t\t    }\n\t\t};\n\t}\t// end namespace spsc_sema\n}\t// end namespace moodycamel\n\n#if defined(AE_VCPP) && (_MSC_VER < 1700 || defined(__cplusplus_cli))\n#pragma warning(pop)\n#ifdef __cplusplus_cli\n#pragma managed(pop)\n#endif\n#endif\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "readerwritercircularbuffer.h",
          "type": "blob",
          "size": 11.412109375,
          "content": "// ©2020 Cameron Desrochers.\r\n// Distributed under the simplified BSD license (see the license file that\r\n// should have come with this header).\r\n\r\n// Provides a C++11 implementation of a single-producer, single-consumer wait-free concurrent\r\n// circular buffer (fixed-size queue).\r\n\r\n#pragma once\r\n\r\n#include <utility>\r\n#include <chrono>\r\n#include <memory>\r\n#include <cstdlib>\r\n#include <cstdint>\r\n#include <cassert>\r\n\r\n// Note that this implementation is fully modern C++11 (not compatible with old MSVC versions)\r\n// but we still include atomicops.h for its LightweightSemaphore implementation.\r\n#include \"atomicops.h\"\r\n\r\n#ifndef MOODYCAMEL_CACHE_LINE_SIZE\r\n#define MOODYCAMEL_CACHE_LINE_SIZE 64\r\n#endif\r\n\r\nnamespace moodycamel {\r\n\r\ntemplate<typename T>\r\nclass BlockingReaderWriterCircularBuffer\r\n{\r\npublic:\r\n\ttypedef T value_type;\r\n\r\npublic:\r\n\texplicit BlockingReaderWriterCircularBuffer(std::size_t capacity)\r\n\t\t: maxcap(capacity), mask(), rawData(), data(),\r\n\t\tslots_(new spsc_sema::LightweightSemaphore(static_cast<spsc_sema::LightweightSemaphore::ssize_t>(capacity))),\r\n\t\titems(new spsc_sema::LightweightSemaphore(0)),\r\n\t\tnextSlot(0), nextItem(0)\r\n\t{\r\n\t\t// Round capacity up to power of two to compute modulo mask.\r\n\t\t// Adapted from http://graphics.stanford.edu/~seander/bithacks.html#RoundUpPowerOf2\r\n\t\t--capacity;\r\n\t\tcapacity |= capacity >> 1;\r\n\t\tcapacity |= capacity >> 2;\r\n\t\tcapacity |= capacity >> 4;\r\n\t\tfor (std::size_t i = 1; i < sizeof(std::size_t); i <<= 1)\r\n\t\t\tcapacity |= capacity >> (i << 3);\r\n\t\tmask = capacity++;\r\n\t\trawData = static_cast<char*>(std::malloc(capacity * sizeof(T) + std::alignment_of<T>::value - 1));\r\n\t\tdata = align_for<T>(rawData);\r\n\t}\r\n\r\n\tBlockingReaderWriterCircularBuffer(BlockingReaderWriterCircularBuffer&& other)\r\n\t\t: maxcap(0), mask(0), rawData(nullptr), data(nullptr),\r\n\t\tslots_(new spsc_sema::LightweightSemaphore(0)),\r\n\t\titems(new spsc_sema::LightweightSemaphore(0)),\r\n\t\tnextSlot(), nextItem()\r\n\t{\r\n\t\tswap(other);\r\n\t}\r\n\r\n\tBlockingReaderWriterCircularBuffer(BlockingReaderWriterCircularBuffer const&) = delete;\r\n\r\n\t// Note: The queue should not be accessed concurrently while it's\r\n\t// being deleted. It's up to the user to synchronize this.\r\n\t~BlockingReaderWriterCircularBuffer()\r\n\t{\r\n\t\tfor (std::size_t i = 0, n = items->availableApprox(); i != n; ++i)\r\n\t\t\treinterpret_cast<T*>(data)[(nextItem + i) & mask].~T();\r\n\t\tstd::free(rawData);\r\n\t}\r\n\r\n\tBlockingReaderWriterCircularBuffer& operator=(BlockingReaderWriterCircularBuffer&& other) noexcept\r\n\t{\r\n\t\tswap(other);\r\n\t\treturn *this;\r\n\t}\r\n\r\n\tBlockingReaderWriterCircularBuffer& operator=(BlockingReaderWriterCircularBuffer const&) = delete;\r\n\r\n\t// Swaps the contents of this buffer with the contents of another.\r\n\t// Not thread-safe.\r\n\tvoid swap(BlockingReaderWriterCircularBuffer& other) noexcept\r\n\t{\r\n\t\tstd::swap(maxcap, other.maxcap);\r\n\t\tstd::swap(mask, other.mask);\r\n\t\tstd::swap(rawData, other.rawData);\r\n\t\tstd::swap(data, other.data);\r\n\t\tstd::swap(slots_, other.slots_);\r\n\t\tstd::swap(items, other.items);\r\n\t\tstd::swap(nextSlot, other.nextSlot);\r\n\t\tstd::swap(nextItem, other.nextItem);\r\n\t}\r\n\r\n\t// Enqueues a single item (by copying it).\r\n\t// Fails if not enough room to enqueue.\r\n\t// Thread-safe when called by producer thread.\r\n\t// No exception guarantee (state will be corrupted) if constructor of T throws.\r\n\tbool try_enqueue(T const& item)\r\n\t{\r\n\t\tif (!slots_->tryWait())\r\n\t\t\treturn false;\r\n\t\tinner_enqueue(item);\r\n\t\treturn true;\r\n\t}\r\n\r\n\t// Enqueues a single item (by moving it, if possible).\r\n\t// Fails if not enough room to enqueue.\r\n\t// Thread-safe when called by producer thread.\r\n\t// No exception guarantee (state will be corrupted) if constructor of T throws.\r\n\tbool try_enqueue(T&& item)\r\n\t{\r\n\t\tif (!slots_->tryWait())\r\n\t\t\treturn false;\r\n\t\tinner_enqueue(std::move(item));\r\n\t\treturn true;\r\n\t}\r\n\r\n\t// Blocks the current thread until there's enough space to enqueue the given item,\r\n\t// then enqueues it (via copy).\r\n\t// Thread-safe when called by producer thread.\r\n\t// No exception guarantee (state will be corrupted) if constructor of T throws.\r\n\tvoid wait_enqueue(T const& item)\r\n\t{\r\n\t\twhile (!slots_->wait());\r\n\t\tinner_enqueue(item);\r\n\t}\r\n\r\n\t// Blocks the current thread until there's enough space to enqueue the given item,\r\n\t// then enqueues it (via move, if possible).\r\n\t// Thread-safe when called by producer thread.\r\n\t// No exception guarantee (state will be corrupted) if constructor of T throws.\r\n\tvoid wait_enqueue(T&& item)\r\n\t{\r\n\t\twhile (!slots_->wait());\r\n\t\tinner_enqueue(std::move(item));\r\n\t}\r\n\r\n\t// Blocks the current thread until there's enough space to enqueue the given item,\r\n\t// or the timeout expires. Returns false without enqueueing the item if the timeout\r\n\t// expires, otherwise enqueues the item (via copy) and returns true.\r\n\t// Thread-safe when called by producer thread.\r\n\t// No exception guarantee (state will be corrupted) if constructor of T throws.\r\n\tbool wait_enqueue_timed(T const& item, std::int64_t timeout_usecs)\r\n\t{\r\n\t\tif (!slots_->wait(timeout_usecs))\r\n\t\t\treturn false;\r\n\t\tinner_enqueue(item);\r\n\t\treturn true;\r\n\t}\r\n\r\n\t// Blocks the current thread until there's enough space to enqueue the given item,\r\n\t// or the timeout expires. Returns false without enqueueing the item if the timeout\r\n\t// expires, otherwise enqueues the item (via move, if possible) and returns true.\r\n\t// Thread-safe when called by producer thread.\r\n\t// No exception guarantee (state will be corrupted) if constructor of T throws.\r\n\tbool wait_enqueue_timed(T&& item, std::int64_t timeout_usecs)\r\n\t{\r\n\t\tif (!slots_->wait(timeout_usecs))\r\n\t\t\treturn false;\r\n\t\tinner_enqueue(std::move(item));\r\n\t\treturn true;\r\n\t}\r\n\r\n\t// Blocks the current thread until there's enough space to enqueue the given item,\r\n\t// or the timeout expires. Returns false without enqueueing the item if the timeout\r\n\t// expires, otherwise enqueues the item (via copy) and returns true.\r\n\t// Thread-safe when called by producer thread.\r\n\t// No exception guarantee (state will be corrupted) if constructor of T throws.\r\n\ttemplate<typename Rep, typename Period>\r\n\tinline bool wait_enqueue_timed(T const& item, std::chrono::duration<Rep, Period> const& timeout)\r\n\t{\r\n\t\treturn wait_enqueue_timed(item, std::chrono::duration_cast<std::chrono::microseconds>(timeout).count());\r\n\t}\r\n\r\n\t// Blocks the current thread until there's enough space to enqueue the given item,\r\n\t// or the timeout expires. Returns false without enqueueing the item if the timeout\r\n\t// expires, otherwise enqueues the item (via move, if possible) and returns true.\r\n\t// Thread-safe when called by producer thread.\r\n\t// No exception guarantee (state will be corrupted) if constructor of T throws.\r\n\ttemplate<typename Rep, typename Period>\r\n\tinline bool wait_enqueue_timed(T&& item, std::chrono::duration<Rep, Period> const& timeout)\r\n\t{\r\n\t\treturn wait_enqueue_timed(std::move(item), std::chrono::duration_cast<std::chrono::microseconds>(timeout).count());\r\n\t}\r\n\r\n\t// Attempts to dequeue a single item.\r\n\t// Returns false if the buffer is empty.\r\n\t// Thread-safe when called by consumer thread.\r\n\t// No exception guarantee (state will be corrupted) if assignment operator of U throws.\r\n\ttemplate<typename U>\r\n\tbool try_dequeue(U& item)\r\n\t{\r\n\t\tif (!items->tryWait())\r\n\t\t\treturn false;\r\n\t\tinner_dequeue(item);\r\n\t\treturn true;\r\n\t}\r\n\r\n\t// Blocks the current thread until there's something to dequeue, then dequeues it.\r\n\t// Thread-safe when called by consumer thread.\r\n\t// No exception guarantee (state will be corrupted) if assignment operator of U throws.\r\n\ttemplate<typename U>\r\n\tvoid wait_dequeue(U& item)\r\n\t{\r\n\t\twhile (!items->wait());\r\n\t\tinner_dequeue(item);\r\n\t}\r\n\r\n\t// Blocks the current thread until either there's something to dequeue\r\n\t// or the timeout expires. Returns false without setting `item` if the\r\n\t// timeout expires, otherwise assigns to `item` and returns true.\r\n\t// Thread-safe when called by consumer thread.\r\n\t// No exception guarantee (state will be corrupted) if assignment operator of U throws.\r\n\ttemplate<typename U>\r\n\tbool wait_dequeue_timed(U& item, std::int64_t timeout_usecs)\r\n\t{\r\n\t\tif (!items->wait(timeout_usecs))\r\n\t\t\treturn false;\r\n\t\tinner_dequeue(item);\r\n\t\treturn true;\r\n\t}\r\n\r\n\t// Blocks the current thread until either there's something to dequeue\r\n\t// or the timeout expires. Returns false without setting `item` if the\r\n\t// timeout expires, otherwise assigns to `item` and returns true.\r\n\t// Thread-safe when called by consumer thread.\r\n\t// No exception guarantee (state will be corrupted) if assignment operator of U throws.\r\n\ttemplate<typename U, typename Rep, typename Period>\r\n\tinline bool wait_dequeue_timed(U& item, std::chrono::duration<Rep, Period> const& timeout)\r\n\t{\r\n\t\treturn wait_dequeue_timed(item, std::chrono::duration_cast<std::chrono::microseconds>(timeout).count());\r\n\t}\r\n\r\n\t// Returns a pointer to the next element in the queue (the one that would\r\n\t// be removed next by a call to `try_dequeue` or `try_pop`). If the queue\r\n\t// appears empty at the time the method is called, returns nullptr instead.\r\n\t// Thread-safe when called by consumer thread.\r\n\tinline T* peek()\r\n\t{\r\n\t\tif (!items->availableApprox())\r\n\t\t\treturn nullptr;\r\n\t\treturn inner_peek();\r\n\t}\r\n\r\n\t// Pops the next element from the queue, if there is one.\r\n\t// Thread-safe when called by consumer thread.\r\n\tinline bool try_pop()\r\n\t{\r\n\t\tif (!items->tryWait())\r\n\t\t\treturn false;\r\n\t\tinner_pop();\r\n\t\treturn true;\r\n\t}\r\n\r\n\t// Returns a (possibly outdated) snapshot of the total number of elements currently in the buffer.\r\n\t// Thread-safe.\r\n\tinline std::size_t size_approx() const\r\n\t{\r\n\t\treturn items->availableApprox();\r\n\t}\r\n\r\n\t// Returns the maximum number of elements that this circular buffer can hold at once.\r\n\t// Thread-safe.\r\n\tinline std::size_t max_capacity() const\r\n\t{\r\n\t\treturn maxcap;\r\n\t}\r\n\r\nprivate:\r\n\ttemplate<typename U>\r\n\tvoid inner_enqueue(U&& item)\r\n\t{\r\n\t\tstd::size_t i = nextSlot++;\r\n\t\tnew (reinterpret_cast<T*>(data) + (i & mask)) T(std::forward<U>(item));\r\n\t\titems->signal();\r\n\t}\r\n\r\n\ttemplate<typename U>\r\n\tvoid inner_dequeue(U& item)\r\n\t{\r\n\t\tstd::size_t i = nextItem++;\r\n\t\tT& element = reinterpret_cast<T*>(data)[i & mask];\r\n\t\titem = std::move(element);\r\n\t\telement.~T();\r\n\t\tslots_->signal();\r\n\t}\r\n\r\n\tT* inner_peek()\r\n\t{\r\n\t\treturn reinterpret_cast<T*>(data) + (nextItem & mask);\r\n\t}\r\n\r\n\tvoid inner_pop()\r\n\t{\r\n\t\tstd::size_t i = nextItem++;\r\n\t\treinterpret_cast<T*>(data)[i & mask].~T();\r\n\t\tslots_->signal();\r\n\t}\r\n\r\n\ttemplate<typename U>\r\n\tstatic inline char* align_for(char* ptr)\r\n\t{\r\n\t\tconst std::size_t alignment = std::alignment_of<U>::value;\r\n\t\treturn ptr + (alignment - (reinterpret_cast<std::uintptr_t>(ptr) % alignment)) % alignment;\r\n\t}\r\n\r\nprivate:\r\n\tstd::size_t maxcap;                           // actual (non-power-of-two) capacity\r\n\tstd::size_t mask;                             // circular buffer capacity mask (for cheap modulo)\r\n\tchar* rawData;                                // raw circular buffer memory\r\n\tchar* data;                                   // circular buffer memory aligned to element alignment\r\n\tstd::unique_ptr<spsc_sema::LightweightSemaphore> slots_;  // number of slots currently free (named with underscore to accommodate Qt's 'slots' macro)\r\n\tstd::unique_ptr<spsc_sema::LightweightSemaphore> items;   // number of elements currently enqueued\r\n\tchar cachelineFiller0[MOODYCAMEL_CACHE_LINE_SIZE - sizeof(char*) * 2 - sizeof(std::size_t) * 2 - sizeof(std::unique_ptr<spsc_sema::LightweightSemaphore>) * 2];\r\n\tstd::size_t nextSlot;                         // index of next free slot to enqueue into\r\n\tchar cachelineFiller1[MOODYCAMEL_CACHE_LINE_SIZE - sizeof(std::size_t)];\r\n\tstd::size_t nextItem;                         // index of next element to dequeue from\r\n};\r\n\r\n}\r\n"
        },
        {
          "name": "readerwriterqueue.h",
          "type": "blob",
          "size": 31.9033203125,
          "content": "// ©2013-2020 Cameron Desrochers.\n// Distributed under the simplified BSD license (see the license file that\n// should have come with this header).\n\n#pragma once\n\n#include \"atomicops.h\"\n#include <new>\n#include <type_traits>\n#include <utility>\n#include <cassert>\n#include <stdexcept>\n#include <new>\n#include <cstdint>\n#include <cstdlib>\t\t// For malloc/free/abort & size_t\n#include <memory>\n#if __cplusplus > 199711L || _MSC_VER >= 1700 // C++11 or VS2012\n#include <chrono>\n#endif\n\n\n// A lock-free queue for a single-consumer, single-producer architecture.\n// The queue is also wait-free in the common path (except if more memory\n// needs to be allocated, in which case malloc is called).\n// Allocates memory sparingly, and only once if the original maximum size\n// estimate is never exceeded.\n// Tested on x86/x64 processors, but semantics should be correct for all\n// architectures (given the right implementations in atomicops.h), provided\n// that aligned integer and pointer accesses are naturally atomic.\n// Note that there should only be one consumer thread and producer thread;\n// Switching roles of the threads, or using multiple consecutive threads for\n// one role, is not safe unless properly synchronized.\n// Using the queue exclusively from one thread is fine, though a bit silly.\n\n#ifndef MOODYCAMEL_CACHE_LINE_SIZE\n#define MOODYCAMEL_CACHE_LINE_SIZE 64\n#endif\n\n#ifndef MOODYCAMEL_EXCEPTIONS_ENABLED\n#if (defined(_MSC_VER) && defined(_CPPUNWIND)) || (defined(__GNUC__) && defined(__EXCEPTIONS)) || (!defined(_MSC_VER) && !defined(__GNUC__))\n#define MOODYCAMEL_EXCEPTIONS_ENABLED\n#endif\n#endif\n\n#ifndef MOODYCAMEL_HAS_EMPLACE\n#if !defined(_MSC_VER) || _MSC_VER >= 1800 // variadic templates: either a non-MS compiler or VS >= 2013\n#define MOODYCAMEL_HAS_EMPLACE    1\n#endif\n#endif\n\n#ifndef MOODYCAMEL_MAYBE_ALIGN_TO_CACHELINE\n#if defined (__APPLE__) && defined (__MACH__) && __cplusplus >= 201703L\n// This is required to find out what deployment target we are using\n#include <AvailabilityMacros.h>\n#if !defined(MAC_OS_X_VERSION_MIN_REQUIRED) || !defined(MAC_OS_X_VERSION_10_14) || MAC_OS_X_VERSION_MIN_REQUIRED < MAC_OS_X_VERSION_10_14\n// C++17 new(size_t, align_val_t) is not backwards-compatible with older versions of macOS, so we can't support over-alignment in this case\n#define MOODYCAMEL_MAYBE_ALIGN_TO_CACHELINE\n#endif\n#endif\n#endif\n\n#ifndef MOODYCAMEL_MAYBE_ALIGN_TO_CACHELINE\n#define MOODYCAMEL_MAYBE_ALIGN_TO_CACHELINE AE_ALIGN(MOODYCAMEL_CACHE_LINE_SIZE)\n#endif\n\n#ifdef AE_VCPP\n#pragma warning(push)\n#pragma warning(disable: 4324)\t// structure was padded due to __declspec(align())\n#pragma warning(disable: 4820)\t// padding was added\n#pragma warning(disable: 4127)\t// conditional expression is constant\n#endif\n\nnamespace moodycamel {\n\ntemplate<typename T, size_t MAX_BLOCK_SIZE = 512>\nclass MOODYCAMEL_MAYBE_ALIGN_TO_CACHELINE ReaderWriterQueue\n{\n\t// Design: Based on a queue-of-queues. The low-level queues are just\n\t// circular buffers with front and tail indices indicating where the\n\t// next element to dequeue is and where the next element can be enqueued,\n\t// respectively. Each low-level queue is called a \"block\". Each block\n\t// wastes exactly one element's worth of space to keep the design simple\n\t// (if front == tail then the queue is empty, and can't be full).\n\t// The high-level queue is a circular linked list of blocks; again there\n\t// is a front and tail, but this time they are pointers to the blocks.\n\t// The front block is where the next element to be dequeued is, provided\n\t// the block is not empty. The back block is where elements are to be\n\t// enqueued, provided the block is not full.\n\t// The producer thread owns all the tail indices/pointers. The consumer\n\t// thread owns all the front indices/pointers. Both threads read each\n\t// other's variables, but only the owning thread updates them. E.g. After\n\t// the consumer reads the producer's tail, the tail may change before the\n\t// consumer is done dequeuing an object, but the consumer knows the tail\n\t// will never go backwards, only forwards.\n\t// If there is no room to enqueue an object, an additional block (of\n\t// equal size to the last block) is added. Blocks are never removed.\n\npublic:\n\ttypedef T value_type;\n\n\t// Constructs a queue that can hold at least `size` elements without further\n\t// allocations. If more than MAX_BLOCK_SIZE elements are requested,\n\t// then several blocks of MAX_BLOCK_SIZE each are reserved (including\n\t// at least one extra buffer block).\n\tAE_NO_TSAN explicit ReaderWriterQueue(size_t size = 15)\n#ifndef NDEBUG\n\t\t: enqueuing(false)\n\t\t,dequeuing(false)\n#endif\n\t{\n\t\tassert(MAX_BLOCK_SIZE == ceilToPow2(MAX_BLOCK_SIZE) && \"MAX_BLOCK_SIZE must be a power of 2\");\n\t\tassert(MAX_BLOCK_SIZE >= 2 && \"MAX_BLOCK_SIZE must be at least 2\");\n\t\t\n\t\tBlock* firstBlock = nullptr;\n\t\t\n\t\tlargestBlockSize = ceilToPow2(size + 1);\t\t// We need a spare slot to fit size elements in the block\n\t\tif (largestBlockSize > MAX_BLOCK_SIZE * 2) {\n\t\t\t// We need a spare block in case the producer is writing to a different block the consumer is reading from, and\n\t\t\t// wants to enqueue the maximum number of elements. We also need a spare element in each block to avoid the ambiguity\n\t\t\t// between front == tail meaning \"empty\" and \"full\".\n\t\t\t// So the effective number of slots that are guaranteed to be usable at any time is the block size - 1 times the\n\t\t\t// number of blocks - 1. Solving for size and applying a ceiling to the division gives us (after simplifying):\n\t\t\tsize_t initialBlockCount = (size + MAX_BLOCK_SIZE * 2 - 3) / (MAX_BLOCK_SIZE - 1);\n\t\t\tlargestBlockSize = MAX_BLOCK_SIZE;\n\t\t\tBlock* lastBlock = nullptr;\n\t\t\tfor (size_t i = 0; i != initialBlockCount; ++i) {\n\t\t\t\tauto block = make_block(largestBlockSize);\n\t\t\t\tif (block == nullptr) {\n#ifdef MOODYCAMEL_EXCEPTIONS_ENABLED\n\t\t\t\t\tthrow std::bad_alloc();\n#else\n\t\t\t\t\tabort();\n#endif\n\t\t\t\t}\n\t\t\t\tif (firstBlock == nullptr) {\n\t\t\t\t\tfirstBlock = block;\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tlastBlock->next = block;\n\t\t\t\t}\n\t\t\t\tlastBlock = block;\n\t\t\t\tblock->next = firstBlock;\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tfirstBlock = make_block(largestBlockSize);\n\t\t\tif (firstBlock == nullptr) {\n#ifdef MOODYCAMEL_EXCEPTIONS_ENABLED\n\t\t\t\tthrow std::bad_alloc();\n#else\n\t\t\t\tabort();\n#endif\n\t\t\t}\n\t\t\tfirstBlock->next = firstBlock;\n\t\t}\n\t\tfrontBlock = firstBlock;\n\t\ttailBlock = firstBlock;\n\t\t\n\t\t// Make sure the reader/writer threads will have the initialized memory setup above:\n\t\tfence(memory_order_sync);\n\t}\n\n\t// Note: The queue should not be accessed concurrently while it's\n\t// being moved. It's up to the user to synchronize this.\n\tAE_NO_TSAN ReaderWriterQueue(ReaderWriterQueue&& other)\n\t\t: frontBlock(other.frontBlock.load()),\n\t\ttailBlock(other.tailBlock.load()),\n\t\tlargestBlockSize(other.largestBlockSize)\n#ifndef NDEBUG\n\t\t,enqueuing(false)\n\t\t,dequeuing(false)\n#endif\n\t{\n\t\tother.largestBlockSize = 32;\n\t\tBlock* b = other.make_block(other.largestBlockSize);\n\t\tif (b == nullptr) {\n#ifdef MOODYCAMEL_EXCEPTIONS_ENABLED\n\t\t\tthrow std::bad_alloc();\n#else\n\t\t\tabort();\n#endif\n\t\t}\n\t\tb->next = b;\n\t\tother.frontBlock = b;\n\t\tother.tailBlock = b;\n\t}\n\n\t// Note: The queue should not be accessed concurrently while it's\n\t// being moved. It's up to the user to synchronize this.\n\tReaderWriterQueue& operator=(ReaderWriterQueue&& other) AE_NO_TSAN\n\t{\n\t\tBlock* b = frontBlock.load();\n\t\tfrontBlock = other.frontBlock.load();\n\t\tother.frontBlock = b;\n\t\tb = tailBlock.load();\n\t\ttailBlock = other.tailBlock.load();\n\t\tother.tailBlock = b;\n\t\tstd::swap(largestBlockSize, other.largestBlockSize);\n\t\treturn *this;\n\t}\n\n\t// Note: The queue should not be accessed concurrently while it's\n\t// being deleted. It's up to the user to synchronize this.\n\tAE_NO_TSAN ~ReaderWriterQueue()\n\t{\n\t\t// Make sure we get the latest version of all variables from other CPUs:\n\t\tfence(memory_order_sync);\n\n\t\t// Destroy any remaining objects in queue and free memory\n\t\tBlock* frontBlock_ = frontBlock;\n\t\tBlock* block = frontBlock_;\n\t\tdo {\n\t\t\tBlock* nextBlock = block->next;\n\t\t\tsize_t blockFront = block->front;\n\t\t\tsize_t blockTail = block->tail;\n\n\t\t\tfor (size_t i = blockFront; i != blockTail; i = (i + 1) & block->sizeMask) {\n\t\t\t\tauto element = reinterpret_cast<T*>(block->data + i * sizeof(T));\n\t\t\t\telement->~T();\n\t\t\t\t(void)element;\n\t\t\t}\n\t\t\t\n\t\t\tauto rawBlock = block->rawThis;\n\t\t\tblock->~Block();\n\t\t\tstd::free(rawBlock);\n\t\t\tblock = nextBlock;\n\t\t} while (block != frontBlock_);\n\t}\n\n\n\t// Enqueues a copy of element if there is room in the queue.\n\t// Returns true if the element was enqueued, false otherwise.\n\t// Does not allocate memory.\n\tAE_FORCEINLINE bool try_enqueue(T const& element) AE_NO_TSAN\n\t{\n\t\treturn inner_enqueue<CannotAlloc>(element);\n\t}\n\n\t// Enqueues a moved copy of element if there is room in the queue.\n\t// Returns true if the element was enqueued, false otherwise.\n\t// Does not allocate memory.\n\tAE_FORCEINLINE bool try_enqueue(T&& element) AE_NO_TSAN\n\t{\n\t\treturn inner_enqueue<CannotAlloc>(std::forward<T>(element));\n\t}\n\n#if MOODYCAMEL_HAS_EMPLACE\n\t// Like try_enqueue() but with emplace semantics (i.e. construct-in-place).\n\ttemplate<typename... Args>\n\tAE_FORCEINLINE bool try_emplace(Args&&... args) AE_NO_TSAN\n\t{\n\t\treturn inner_enqueue<CannotAlloc>(std::forward<Args>(args)...);\n\t}\n#endif\n\n\t// Enqueues a copy of element on the queue.\n\t// Allocates an additional block of memory if needed.\n\t// Only fails (returns false) if memory allocation fails.\n\tAE_FORCEINLINE bool enqueue(T const& element) AE_NO_TSAN\n\t{\n\t\treturn inner_enqueue<CanAlloc>(element);\n\t}\n\n\t// Enqueues a moved copy of element on the queue.\n\t// Allocates an additional block of memory if needed.\n\t// Only fails (returns false) if memory allocation fails.\n\tAE_FORCEINLINE bool enqueue(T&& element) AE_NO_TSAN\n\t{\n\t\treturn inner_enqueue<CanAlloc>(std::forward<T>(element));\n\t}\n\n#if MOODYCAMEL_HAS_EMPLACE\n\t// Like enqueue() but with emplace semantics (i.e. construct-in-place).\n\ttemplate<typename... Args>\n\tAE_FORCEINLINE bool emplace(Args&&... args) AE_NO_TSAN\n\t{\n\t\treturn inner_enqueue<CanAlloc>(std::forward<Args>(args)...);\n\t}\n#endif\n\n\t// Attempts to dequeue an element; if the queue is empty,\n\t// returns false instead. If the queue has at least one element,\n\t// moves front to result using operator=, then returns true.\n\ttemplate<typename U>\n\tbool try_dequeue(U& result) AE_NO_TSAN\n\t{\n#ifndef NDEBUG\n\t\tReentrantGuard guard(this->dequeuing);\n#endif\n\n\t\t// High-level pseudocode:\n\t\t// Remember where the tail block is\n\t\t// If the front block has an element in it, dequeue it\n\t\t// Else\n\t\t//     If front block was the tail block when we entered the function, return false\n\t\t//     Else advance to next block and dequeue the item there\n\n\t\t// Note that we have to use the value of the tail block from before we check if the front\n\t\t// block is full or not, in case the front block is empty and then, before we check if the\n\t\t// tail block is at the front block or not, the producer fills up the front block *and\n\t\t// moves on*, which would make us skip a filled block. Seems unlikely, but was consistently\n\t\t// reproducible in practice.\n\t\t// In order to avoid overhead in the common case, though, we do a double-checked pattern\n\t\t// where we have the fast path if the front block is not empty, then read the tail block,\n\t\t// then re-read the front block and check if it's not empty again, then check if the tail\n\t\t// block has advanced.\n\t\t\n\t\tBlock* frontBlock_ = frontBlock.load();\n\t\tsize_t blockTail = frontBlock_->localTail;\n\t\tsize_t blockFront = frontBlock_->front.load();\n\t\t\n\t\tif (blockFront != blockTail || blockFront != (frontBlock_->localTail = frontBlock_->tail.load())) {\n\t\t\tfence(memory_order_acquire);\n\t\t\t\n\t\tnon_empty_front_block:\n\t\t\t// Front block not empty, dequeue from here\n\t\t\tauto element = reinterpret_cast<T*>(frontBlock_->data + blockFront * sizeof(T));\n\t\t\tresult = std::move(*element);\n\t\t\telement->~T();\n\n\t\t\tblockFront = (blockFront + 1) & frontBlock_->sizeMask;\n\n\t\t\tfence(memory_order_release);\n\t\t\tfrontBlock_->front = blockFront;\n\t\t}\n\t\telse if (frontBlock_ != tailBlock.load()) {\n\t\t\tfence(memory_order_acquire);\n\n\t\t\tfrontBlock_ = frontBlock.load();\n\t\t\tblockTail = frontBlock_->localTail = frontBlock_->tail.load();\n\t\t\tblockFront = frontBlock_->front.load();\n\t\t\tfence(memory_order_acquire);\n\t\t\t\n\t\t\tif (blockFront != blockTail) {\n\t\t\t\t// Oh look, the front block isn't empty after all\n\t\t\t\tgoto non_empty_front_block;\n\t\t\t}\n\t\t\t\n\t\t\t// Front block is empty but there's another block ahead, advance to it\n\t\t\tBlock* nextBlock = frontBlock_->next;\n\t\t\t// Don't need an acquire fence here since next can only ever be set on the tailBlock,\n\t\t\t// and we're not the tailBlock, and we did an acquire earlier after reading tailBlock which\n\t\t\t// ensures next is up-to-date on this CPU in case we recently were at tailBlock.\n\n\t\t\tsize_t nextBlockFront = nextBlock->front.load();\n\t\t\tsize_t nextBlockTail = nextBlock->localTail = nextBlock->tail.load();\n\t\t\tfence(memory_order_acquire);\n\n\t\t\t// Since the tailBlock is only ever advanced after being written to,\n\t\t\t// we know there's for sure an element to dequeue on it\n\t\t\tassert(nextBlockFront != nextBlockTail);\n\t\t\tAE_UNUSED(nextBlockTail);\n\n\t\t\t// We're done with this block, let the producer use it if it needs\n\t\t\tfence(memory_order_release);\t\t// Expose possibly pending changes to frontBlock->front from last dequeue\n\t\t\tfrontBlock = frontBlock_ = nextBlock;\n\n\t\t\tcompiler_fence(memory_order_release);\t// Not strictly needed\n\n\t\t\tauto element = reinterpret_cast<T*>(frontBlock_->data + nextBlockFront * sizeof(T));\n\t\t\t\n\t\t\tresult = std::move(*element);\n\t\t\telement->~T();\n\n\t\t\tnextBlockFront = (nextBlockFront + 1) & frontBlock_->sizeMask;\n\t\t\t\n\t\t\tfence(memory_order_release);\n\t\t\tfrontBlock_->front = nextBlockFront;\n\t\t}\n\t\telse {\n\t\t\t// No elements in current block and no other block to advance to\n\t\t\treturn false;\n\t\t}\n\n\t\treturn true;\n\t}\n\n\n\t// Returns a pointer to the front element in the queue (the one that\n\t// would be removed next by a call to `try_dequeue` or `pop`). If the\n\t// queue appears empty at the time the method is called, nullptr is\n\t// returned instead.\n\t// Must be called only from the consumer thread.\n\tT* peek() const AE_NO_TSAN\n\t{\n#ifndef NDEBUG\n\t\tReentrantGuard guard(this->dequeuing);\n#endif\n\t\t// See try_dequeue() for reasoning\n\n\t\tBlock* frontBlock_ = frontBlock.load();\n\t\tsize_t blockTail = frontBlock_->localTail;\n\t\tsize_t blockFront = frontBlock_->front.load();\n\t\t\n\t\tif (blockFront != blockTail || blockFront != (frontBlock_->localTail = frontBlock_->tail.load())) {\n\t\t\tfence(memory_order_acquire);\n\t\tnon_empty_front_block:\n\t\t\treturn reinterpret_cast<T*>(frontBlock_->data + blockFront * sizeof(T));\n\t\t}\n\t\telse if (frontBlock_ != tailBlock.load()) {\n\t\t\tfence(memory_order_acquire);\n\t\t\tfrontBlock_ = frontBlock.load();\n\t\t\tblockTail = frontBlock_->localTail = frontBlock_->tail.load();\n\t\t\tblockFront = frontBlock_->front.load();\n\t\t\tfence(memory_order_acquire);\n\t\t\t\n\t\t\tif (blockFront != blockTail) {\n\t\t\t\tgoto non_empty_front_block;\n\t\t\t}\n\t\t\t\n\t\t\tBlock* nextBlock = frontBlock_->next;\n\t\t\t\n\t\t\tsize_t nextBlockFront = nextBlock->front.load();\n\t\t\tfence(memory_order_acquire);\n\n\t\t\tassert(nextBlockFront != nextBlock->tail.load());\n\t\t\treturn reinterpret_cast<T*>(nextBlock->data + nextBlockFront * sizeof(T));\n\t\t}\n\t\t\n\t\treturn nullptr;\n\t}\n\t\n\t// Removes the front element from the queue, if any, without returning it.\n\t// Returns true on success, or false if the queue appeared empty at the time\n\t// `pop` was called.\n\tbool pop() AE_NO_TSAN\n\t{\n#ifndef NDEBUG\n\t\tReentrantGuard guard(this->dequeuing);\n#endif\n\t\t// See try_dequeue() for reasoning\n\t\t\n\t\tBlock* frontBlock_ = frontBlock.load();\n\t\tsize_t blockTail = frontBlock_->localTail;\n\t\tsize_t blockFront = frontBlock_->front.load();\n\t\t\n\t\tif (blockFront != blockTail || blockFront != (frontBlock_->localTail = frontBlock_->tail.load())) {\n\t\t\tfence(memory_order_acquire);\n\t\t\t\n\t\tnon_empty_front_block:\n\t\t\tauto element = reinterpret_cast<T*>(frontBlock_->data + blockFront * sizeof(T));\n\t\t\telement->~T();\n\n\t\t\tblockFront = (blockFront + 1) & frontBlock_->sizeMask;\n\n\t\t\tfence(memory_order_release);\n\t\t\tfrontBlock_->front = blockFront;\n\t\t}\n\t\telse if (frontBlock_ != tailBlock.load()) {\n\t\t\tfence(memory_order_acquire);\n\t\t\tfrontBlock_ = frontBlock.load();\n\t\t\tblockTail = frontBlock_->localTail = frontBlock_->tail.load();\n\t\t\tblockFront = frontBlock_->front.load();\n\t\t\tfence(memory_order_acquire);\n\t\t\t\n\t\t\tif (blockFront != blockTail) {\n\t\t\t\tgoto non_empty_front_block;\n\t\t\t}\n\t\t\t\n\t\t\t// Front block is empty but there's another block ahead, advance to it\n\t\t\tBlock* nextBlock = frontBlock_->next;\n\t\t\t\n\t\t\tsize_t nextBlockFront = nextBlock->front.load();\n\t\t\tsize_t nextBlockTail = nextBlock->localTail = nextBlock->tail.load();\n\t\t\tfence(memory_order_acquire);\n\n\t\t\tassert(nextBlockFront != nextBlockTail);\n\t\t\tAE_UNUSED(nextBlockTail);\n\n\t\t\tfence(memory_order_release);\n\t\t\tfrontBlock = frontBlock_ = nextBlock;\n\n\t\t\tcompiler_fence(memory_order_release);\n\n\t\t\tauto element = reinterpret_cast<T*>(frontBlock_->data + nextBlockFront * sizeof(T));\n\t\t\telement->~T();\n\n\t\t\tnextBlockFront = (nextBlockFront + 1) & frontBlock_->sizeMask;\n\t\t\t\n\t\t\tfence(memory_order_release);\n\t\t\tfrontBlock_->front = nextBlockFront;\n\t\t}\n\t\telse {\n\t\t\t// No elements in current block and no other block to advance to\n\t\t\treturn false;\n\t\t}\n\n\t\treturn true;\n\t}\n\t\n\t// Returns the approximate number of items currently in the queue.\n\t// Safe to call from both the producer and consumer threads.\n\tinline size_t size_approx() const AE_NO_TSAN\n\t{\n\t\tsize_t result = 0;\n\t\tBlock* frontBlock_ = frontBlock.load();\n\t\tBlock* block = frontBlock_;\n\t\tdo {\n\t\t\tfence(memory_order_acquire);\n\t\t\tsize_t blockFront = block->front.load();\n\t\t\tsize_t blockTail = block->tail.load();\n\t\t\tresult += (blockTail - blockFront) & block->sizeMask;\n\t\t\tblock = block->next.load();\n\t\t} while (block != frontBlock_);\n\t\treturn result;\n\t}\n\n\t// Returns the total number of items that could be enqueued without incurring\n\t// an allocation when this queue is empty.\n\t// Safe to call from both the producer and consumer threads.\n\t//\n\t// NOTE: The actual capacity during usage may be different depending on the consumer.\n\t//       If the consumer is removing elements concurrently, the producer cannot add to\n\t//       the block the consumer is removing from until it's completely empty, except in\n\t//       the case where the producer was writing to the same block the consumer was\n\t//       reading from the whole time.\n\tinline size_t max_capacity() const {\n\t\tsize_t result = 0;\n\t\tBlock* frontBlock_ = frontBlock.load();\n\t\tBlock* block = frontBlock_;\n\t\tdo {\n\t\t\tfence(memory_order_acquire);\n\t\t\tresult += block->sizeMask;\n\t\t\tblock = block->next.load();\n\t\t} while (block != frontBlock_);\n\t\treturn result;\n\t}\n\n\nprivate:\n\tenum AllocationMode { CanAlloc, CannotAlloc };\n\n#if MOODYCAMEL_HAS_EMPLACE\n\ttemplate<AllocationMode canAlloc, typename... Args>\n\tbool inner_enqueue(Args&&... args) AE_NO_TSAN\n#else\n\ttemplate<AllocationMode canAlloc, typename U>\n\tbool inner_enqueue(U&& element) AE_NO_TSAN\n#endif\n\t{\n#ifndef NDEBUG\n\t\tReentrantGuard guard(this->enqueuing);\n#endif\n\n\t\t// High-level pseudocode (assuming we're allowed to alloc a new block):\n\t\t// If room in tail block, add to tail\n\t\t// Else check next block\n\t\t//     If next block is not the head block, enqueue on next block\n\t\t//     Else create a new block and enqueue there\n\t\t//     Advance tail to the block we just enqueued to\n\n\t\tBlock* tailBlock_ = tailBlock.load();\n\t\tsize_t blockFront = tailBlock_->localFront;\n\t\tsize_t blockTail = tailBlock_->tail.load();\n\n\t\tsize_t nextBlockTail = (blockTail + 1) & tailBlock_->sizeMask;\n\t\tif (nextBlockTail != blockFront || nextBlockTail != (tailBlock_->localFront = tailBlock_->front.load())) {\n\t\t\tfence(memory_order_acquire);\n\t\t\t// This block has room for at least one more element\n\t\t\tchar* location = tailBlock_->data + blockTail * sizeof(T);\n#if MOODYCAMEL_HAS_EMPLACE\n\t\t\tnew (location) T(std::forward<Args>(args)...);\n#else\n\t\t\tnew (location) T(std::forward<U>(element));\n#endif\n\n\t\t\tfence(memory_order_release);\n\t\t\ttailBlock_->tail = nextBlockTail;\n\t\t}\n\t\telse {\n\t\t\tfence(memory_order_acquire);\n\t\t\tif (tailBlock_->next.load() != frontBlock) {\n\t\t\t\t// Note that the reason we can't advance to the frontBlock and start adding new entries there\n\t\t\t\t// is because if we did, then dequeue would stay in that block, eventually reading the new values,\n\t\t\t\t// instead of advancing to the next full block (whose values were enqueued first and so should be\n\t\t\t\t// consumed first).\n\n\t\t\t\tfence(memory_order_acquire);\t\t// Ensure we get latest writes if we got the latest frontBlock\n\n\t\t\t\t// tailBlock is full, but there's a free block ahead, use it\n\t\t\t\tBlock* tailBlockNext = tailBlock_->next.load();\n\t\t\t\tsize_t nextBlockFront = tailBlockNext->localFront = tailBlockNext->front.load();\n\t\t\t\tnextBlockTail = tailBlockNext->tail.load();\n\t\t\t\tfence(memory_order_acquire);\n\n\t\t\t\t// This block must be empty since it's not the head block and we\n\t\t\t\t// go through the blocks in a circle\n\t\t\t\tassert(nextBlockFront == nextBlockTail);\n\t\t\t\ttailBlockNext->localFront = nextBlockFront;\n\n\t\t\t\tchar* location = tailBlockNext->data + nextBlockTail * sizeof(T);\n#if MOODYCAMEL_HAS_EMPLACE\n\t\t\t\tnew (location) T(std::forward<Args>(args)...);\n#else\n\t\t\t\tnew (location) T(std::forward<U>(element));\n#endif\n\n\t\t\t\ttailBlockNext->tail = (nextBlockTail + 1) & tailBlockNext->sizeMask;\n\n\t\t\t\tfence(memory_order_release);\n\t\t\t\ttailBlock = tailBlockNext;\n\t\t\t}\n\t\t\telse if (canAlloc == CanAlloc) {\n\t\t\t\t// tailBlock is full and there's no free block ahead; create a new block\n\t\t\t\tauto newBlockSize = largestBlockSize >= MAX_BLOCK_SIZE ? largestBlockSize : largestBlockSize * 2;\n\t\t\t\tauto newBlock = make_block(newBlockSize);\n\t\t\t\tif (newBlock == nullptr) {\n\t\t\t\t\t// Could not allocate a block!\n\t\t\t\t\treturn false;\n\t\t\t\t}\n\t\t\t\tlargestBlockSize = newBlockSize;\n\n#if MOODYCAMEL_HAS_EMPLACE\n\t\t\t\tnew (newBlock->data) T(std::forward<Args>(args)...);\n#else\n\t\t\t\tnew (newBlock->data) T(std::forward<U>(element));\n#endif\n\t\t\t\tassert(newBlock->front == 0);\n\t\t\t\tnewBlock->tail = newBlock->localTail = 1;\n\n\t\t\t\tnewBlock->next = tailBlock_->next.load();\n\t\t\t\ttailBlock_->next = newBlock;\n\n\t\t\t\t// Might be possible for the dequeue thread to see the new tailBlock->next\n\t\t\t\t// *without* seeing the new tailBlock value, but this is OK since it can't\n\t\t\t\t// advance to the next block until tailBlock is set anyway (because the only\n\t\t\t\t// case where it could try to read the next is if it's already at the tailBlock,\n\t\t\t\t// and it won't advance past tailBlock in any circumstance).\n\n\t\t\t\tfence(memory_order_release);\n\t\t\t\ttailBlock = newBlock;\n\t\t\t}\n\t\t\telse if (canAlloc == CannotAlloc) {\n\t\t\t\t// Would have had to allocate a new block to enqueue, but not allowed\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tassert(false && \"Should be unreachable code\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\n\t\treturn true;\n\t}\n\n\n\t// Disable copying\n\tReaderWriterQueue(ReaderWriterQueue const&) {  }\n\n\t// Disable assignment\n\tReaderWriterQueue& operator=(ReaderWriterQueue const&) {  }\n\n\n\tAE_FORCEINLINE static size_t ceilToPow2(size_t x)\n\t{\n\t\t// From http://graphics.stanford.edu/~seander/bithacks.html#RoundUpPowerOf2\n\t\t--x;\n\t\tx |= x >> 1;\n\t\tx |= x >> 2;\n\t\tx |= x >> 4;\n\t\tfor (size_t i = 1; i < sizeof(size_t); i <<= 1) {\n\t\t\tx |= x >> (i << 3);\n\t\t}\n\t\t++x;\n\t\treturn x;\n\t}\n\t\n\ttemplate<typename U>\n\tstatic AE_FORCEINLINE char* align_for(char* ptr) AE_NO_TSAN\n\t{\n\t\tconst std::size_t alignment = std::alignment_of<U>::value;\n\t\treturn ptr + (alignment - (reinterpret_cast<std::uintptr_t>(ptr) % alignment)) % alignment;\n\t}\nprivate:\n#ifndef NDEBUG\n\tstruct ReentrantGuard\n\t{\n\t\tAE_NO_TSAN ReentrantGuard(weak_atomic<bool>& _inSection)\n\t\t\t: inSection(_inSection)\n\t\t{\n\t\t\tassert(!inSection && \"Concurrent (or re-entrant) enqueue or dequeue operation detected (only one thread at a time may hold the producer or consumer role)\");\n\t\t\tinSection = true;\n\t\t}\n\n\t\tAE_NO_TSAN ~ReentrantGuard() { inSection = false; }\n\n\tprivate:\n\t\tReentrantGuard& operator=(ReentrantGuard const&);\n\n\tprivate:\n\t\tweak_atomic<bool>& inSection;\n\t};\n#endif\n\n\tstruct Block\n\t{\n\t\t// Avoid false-sharing by putting highly contended variables on their own cache lines\n\t\tweak_atomic<size_t> front;\t// (Atomic) Elements are read from here\n\t\tsize_t localTail;\t\t\t// An uncontended shadow copy of tail, owned by the consumer\n\t\t\n\t\tchar cachelineFiller0[MOODYCAMEL_CACHE_LINE_SIZE - sizeof(weak_atomic<size_t>) - sizeof(size_t)];\n\t\tweak_atomic<size_t> tail;\t// (Atomic) Elements are enqueued here\n\t\tsize_t localFront;\n\t\t\n\t\tchar cachelineFiller1[MOODYCAMEL_CACHE_LINE_SIZE - sizeof(weak_atomic<size_t>) - sizeof(size_t)];\t// next isn't very contended, but we don't want it on the same cache line as tail (which is)\n\t\tweak_atomic<Block*> next;\t// (Atomic)\n\t\t\n\t\tchar* data;\t\t// Contents (on heap) are aligned to T's alignment\n\n\t\tconst size_t sizeMask;\n\n\n\t\t// size must be a power of two (and greater than 0)\n\t\tAE_NO_TSAN Block(size_t const& _size, char* _rawThis, char* _data)\n\t\t\t: front(0UL), localTail(0), tail(0UL), localFront(0), next(nullptr), data(_data), sizeMask(_size - 1), rawThis(_rawThis)\n\t\t{\n\t\t}\n\n\tprivate:\n\t\t// C4512 - Assignment operator could not be generated\n\t\tBlock& operator=(Block const&);\n\n\tpublic:\n\t\tchar* rawThis;\n\t};\n\t\n\t\n\tstatic Block* make_block(size_t capacity) AE_NO_TSAN\n\t{\n\t\t// Allocate enough memory for the block itself, as well as all the elements it will contain\n\t\tauto size = sizeof(Block) + std::alignment_of<Block>::value - 1;\n\t\tsize += sizeof(T) * capacity + std::alignment_of<T>::value - 1;\n\t\tauto newBlockRaw = static_cast<char*>(std::malloc(size));\n\t\tif (newBlockRaw == nullptr) {\n\t\t\treturn nullptr;\n\t\t}\n\t\t\n\t\tauto newBlockAligned = align_for<Block>(newBlockRaw);\n\t\tauto newBlockData = align_for<T>(newBlockAligned + sizeof(Block));\n\t\treturn new (newBlockAligned) Block(capacity, newBlockRaw, newBlockData);\n\t}\n\nprivate:\n\tweak_atomic<Block*> frontBlock;\t\t// (Atomic) Elements are dequeued from this block\n\t\n\tchar cachelineFiller[MOODYCAMEL_CACHE_LINE_SIZE - sizeof(weak_atomic<Block*>)];\n\tweak_atomic<Block*> tailBlock;\t\t// (Atomic) Elements are enqueued to this block\n\n\tsize_t largestBlockSize;\n\n#ifndef NDEBUG\n\tweak_atomic<bool> enqueuing;\n\tmutable weak_atomic<bool> dequeuing;\n#endif\n};\n\n// Like ReaderWriterQueue, but also providees blocking operations\ntemplate<typename T, size_t MAX_BLOCK_SIZE = 512>\nclass BlockingReaderWriterQueue\n{\nprivate:\n\ttypedef ::moodycamel::ReaderWriterQueue<T, MAX_BLOCK_SIZE> ReaderWriterQueue;\n\t\npublic:\n\texplicit BlockingReaderWriterQueue(size_t size = 15) AE_NO_TSAN\n\t\t: inner(size), sema(new spsc_sema::LightweightSemaphore())\n\t{ }\n\n\tBlockingReaderWriterQueue(BlockingReaderWriterQueue&& other) AE_NO_TSAN\n\t\t: inner(std::move(other.inner)), sema(std::move(other.sema))\n\t{ }\n\n\tBlockingReaderWriterQueue& operator=(BlockingReaderWriterQueue&& other) AE_NO_TSAN\n\t{\n\t\tstd::swap(sema, other.sema);\n\t\tstd::swap(inner, other.inner);\n\t\treturn *this;\n\t}\n\n\n\t// Enqueues a copy of element if there is room in the queue.\n\t// Returns true if the element was enqueued, false otherwise.\n\t// Does not allocate memory.\n\tAE_FORCEINLINE bool try_enqueue(T const& element) AE_NO_TSAN\n\t{\n\t\tif (inner.try_enqueue(element)) {\n\t\t\tsema->signal();\n\t\t\treturn true;\n\t\t}\n\t\treturn false;\n\t}\n\n\t// Enqueues a moved copy of element if there is room in the queue.\n\t// Returns true if the element was enqueued, false otherwise.\n\t// Does not allocate memory.\n\tAE_FORCEINLINE bool try_enqueue(T&& element) AE_NO_TSAN\n\t{\n\t\tif (inner.try_enqueue(std::forward<T>(element))) {\n\t\t\tsema->signal();\n\t\t\treturn true;\n\t\t}\n\t\treturn false;\n\t}\n\n#if MOODYCAMEL_HAS_EMPLACE\n\t// Like try_enqueue() but with emplace semantics (i.e. construct-in-place).\n\ttemplate<typename... Args>\n\tAE_FORCEINLINE bool try_emplace(Args&&... args) AE_NO_TSAN\n\t{\n\t\tif (inner.try_emplace(std::forward<Args>(args)...)) {\n\t\t\tsema->signal();\n\t\t\treturn true;\n\t\t}\n\t\treturn false;\n\t}\n#endif\n\n\n\t// Enqueues a copy of element on the queue.\n\t// Allocates an additional block of memory if needed.\n\t// Only fails (returns false) if memory allocation fails.\n\tAE_FORCEINLINE bool enqueue(T const& element) AE_NO_TSAN\n\t{\n\t\tif (inner.enqueue(element)) {\n\t\t\tsema->signal();\n\t\t\treturn true;\n\t\t}\n\t\treturn false;\n\t}\n\n\t// Enqueues a moved copy of element on the queue.\n\t// Allocates an additional block of memory if needed.\n\t// Only fails (returns false) if memory allocation fails.\n\tAE_FORCEINLINE bool enqueue(T&& element) AE_NO_TSAN\n\t{\n\t\tif (inner.enqueue(std::forward<T>(element))) {\n\t\t\tsema->signal();\n\t\t\treturn true;\n\t\t}\n\t\treturn false;\n\t}\n\n#if MOODYCAMEL_HAS_EMPLACE\n\t// Like enqueue() but with emplace semantics (i.e. construct-in-place).\n\ttemplate<typename... Args>\n\tAE_FORCEINLINE bool emplace(Args&&... args) AE_NO_TSAN\n\t{\n\t\tif (inner.emplace(std::forward<Args>(args)...)) {\n\t\t\tsema->signal();\n\t\t\treturn true;\n\t\t}\n\t\treturn false;\n\t}\n#endif\n\n\n\t// Attempts to dequeue an element; if the queue is empty,\n\t// returns false instead. If the queue has at least one element,\n\t// moves front to result using operator=, then returns true.\n\ttemplate<typename U>\n\tbool try_dequeue(U& result) AE_NO_TSAN\n\t{\n\t\tif (sema->tryWait()) {\n\t\t\tbool success = inner.try_dequeue(result);\n\t\t\tassert(success);\n\t\t\tAE_UNUSED(success);\n\t\t\treturn true;\n\t\t}\n\t\treturn false;\n\t}\n\t\n\t\n\t// Attempts to dequeue an element; if the queue is empty,\n\t// waits until an element is available, then dequeues it.\n\ttemplate<typename U>\n\tvoid wait_dequeue(U& result) AE_NO_TSAN\n\t{\n\t\twhile (!sema->wait());\n\t\tbool success = inner.try_dequeue(result);\n\t\tAE_UNUSED(result);\n\t\tassert(success);\n\t\tAE_UNUSED(success);\n\t}\n\n\n\t// Attempts to dequeue an element; if the queue is empty,\n\t// waits until an element is available up to the specified timeout,\n\t// then dequeues it and returns true, or returns false if the timeout\n\t// expires before an element can be dequeued.\n\t// Using a negative timeout indicates an indefinite timeout,\n\t// and is thus functionally equivalent to calling wait_dequeue.\n\ttemplate<typename U>\n\tbool wait_dequeue_timed(U& result, std::int64_t timeout_usecs) AE_NO_TSAN\n\t{\n\t\tif (!sema->wait(timeout_usecs)) {\n\t\t\treturn false;\n\t\t}\n\t\tbool success = inner.try_dequeue(result);\n\t\tAE_UNUSED(result);\n\t\tassert(success);\n\t\tAE_UNUSED(success);\n\t\treturn true;\n\t}\n\n\n#if __cplusplus > 199711L || _MSC_VER >= 1700\n\t// Attempts to dequeue an element; if the queue is empty,\n\t// waits until an element is available up to the specified timeout,\n\t// then dequeues it and returns true, or returns false if the timeout\n\t// expires before an element can be dequeued.\n\t// Using a negative timeout indicates an indefinite timeout,\n\t// and is thus functionally equivalent to calling wait_dequeue.\n\ttemplate<typename U, typename Rep, typename Period>\n\tinline bool wait_dequeue_timed(U& result, std::chrono::duration<Rep, Period> const& timeout) AE_NO_TSAN\n\t{\n        return wait_dequeue_timed(result, std::chrono::duration_cast<std::chrono::microseconds>(timeout).count());\n\t}\n#endif\n\n\n\t// Returns a pointer to the front element in the queue (the one that\n\t// would be removed next by a call to `try_dequeue` or `pop`). If the\n\t// queue appears empty at the time the method is called, nullptr is\n\t// returned instead.\n\t// Must be called only from the consumer thread.\n\tAE_FORCEINLINE T* peek() const AE_NO_TSAN\n\t{\n\t\treturn inner.peek();\n\t}\n\t\n\t// Removes the front element from the queue, if any, without returning it.\n\t// Returns true on success, or false if the queue appeared empty at the time\n\t// `pop` was called.\n\tAE_FORCEINLINE bool pop() AE_NO_TSAN\n\t{\n\t\tif (sema->tryWait()) {\n\t\t\tbool result = inner.pop();\n\t\t\tassert(result);\n\t\t\tAE_UNUSED(result);\n\t\t\treturn true;\n\t\t}\n\t\treturn false;\n\t}\n\t\n\t// Returns the approximate number of items currently in the queue.\n\t// Safe to call from both the producer and consumer threads.\n\tAE_FORCEINLINE size_t size_approx() const AE_NO_TSAN\n\t{\n\t\treturn sema->availableApprox();\n\t}\n\n\t// Returns the total number of items that could be enqueued without incurring\n\t// an allocation when this queue is empty.\n\t// Safe to call from both the producer and consumer threads.\n\t//\n\t// NOTE: The actual capacity during usage may be different depending on the consumer.\n\t//       If the consumer is removing elements concurrently, the producer cannot add to\n\t//       the block the consumer is removing from until it's completely empty, except in\n\t//       the case where the producer was writing to the same block the consumer was\n\t//       reading from the whole time.\n\tAE_FORCEINLINE size_t max_capacity() const {\n\t\treturn inner.max_capacity();\n\t}\n\nprivate:\n\t// Disable copying & assignment\n\tBlockingReaderWriterQueue(BlockingReaderWriterQueue const&) {  }\n\tBlockingReaderWriterQueue& operator=(BlockingReaderWriterQueue const&) {  }\n\t\nprivate:\n\tReaderWriterQueue inner;\n\tstd::unique_ptr<spsc_sema::LightweightSemaphore> sema;\n};\n\n}    // end namespace moodycamel\n\n#ifdef AE_VCPP\n#pragma warning(pop)\n#endif\n"
        },
        {
          "name": "readerwriterqueueConfig.cmake.in",
          "type": "blob",
          "size": 0.0771484375,
          "content": "@PACKAGE_INIT@\n\ninclude(${CMAKE_CURRENT_LIST_DIR}/@PROJECT_NAME@Targets.cmake)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}