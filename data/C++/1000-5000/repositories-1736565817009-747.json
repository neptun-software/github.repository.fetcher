{
  "metadata": {
    "timestamp": 1736565817009,
    "page": 747,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Smorodov/Multitarget-tracker",
      "stars": 2218,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0517578125,
          "content": "build\nCMakeLists.txt.user*\nCMakeFiles\nCMakeCache.txt\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 5.6650390625,
          "content": "cmake_minimum_required(VERSION 3.9)\r\n\r\nproject(MTTracking VERSION 1.1.0)\r\n\r\nset_property(GLOBAL PROPERTY USE_FOLDERS ON)\r\n\r\nunset(CMAKE_C_FLAGS CACHE)\r\nunset(CMAKE_CXX_FLAGS CACHE)\r\nunset(CMAKE_CXX_FLAGS_RELEASE CACHE)\r\n# unset(CMAKE_CXX_FLAGS_DEBUG CACHE)\r\n\r\nfind_package(OpenMP)\r\nif (OPENMP_FOUND)\r\n    list(APPEND CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} ${OpenMP_C_FLAGS}\")\r\n    list(APPEND CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}\")\r\nendif()\r\n\r\nset(CMAKE_CXX_STANDARD 17)\r\n\r\nif (CMAKE_COMPILER_IS_GNUCXX)\r\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC\" CACHE STRING COMPILE_FLAGS FORCE)\r\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall -Wextra -pedantic-errors -fPIC\" CACHE STRING COMPILE_FLAGS FORCE)\r\n    set(CMAKE_CXX_FLAGS_RELEASE \"-O3 -g -march=native -mtune=native -funroll-loops -DNDEBUG -DBOOST_DISABLE_ASSERTS\" CACHE STRING COMPILE_FLAGS FORCE)\r\n    set(CMAKE_CXX_FLAGS_DEBUG \"-O0 -g -march=native -mtune=native -DDEBUG\" CACHE STRING COMPILE_FLAGS FORCE)\r\nelseif (MSVC)\r\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /EHsc /W4 -DGTL_STATIC\" CACHE STRING COMPILE_FLAGS FORCE)\r\n    set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} /MD /Ox /Ob2 /Oi /Ot /arch:AVX2 /fp:fast /DNDEBUG\" CACHE STRING COMPILE_FLAGS FORCE)\r\n    # set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} /MDd /Od /Ob0 /DDEBUG\" CACHE STRING COMPILE_FLAGS FORCE)\r\n\r\n    add_definitions(-D_USE_MATH_DEFINES -DNOMINMAX)\r\nendif()\r\n\r\nset(CMAKE_BINARY_DIR ${CMAKE_SOURCE_DIR}/build)\r\nset(EXECUTABLE_OUTPUT_PATH ${CMAKE_BINARY_DIR})\r\nset(LIBRARY_OUTPUT_PATH ${CMAKE_BINARY_DIR})\r\n\r\nFIND_PACKAGE(OpenCV REQUIRED)\r\n\r\noption(SILENT_WORK \"Remove all imshow and waitKey functions?\" OFF)\r\nif (SILENT_WORK)\r\n    add_definitions(-DSILENT_WORK)\r\nendif(SILENT_WORK)\r\n\r\ninclude(CheckIncludeFileCXX)\r\ncheck_include_file_cxx(filesystem HAVE_FILESYSTEM)\r\nif(HAVE_FILESYSTEM)\r\n    add_definitions(-DHAVE_FILESYSTEM)\r\n    message(\"Founded filesystem header\")\r\nelse(HAVE_FILESYSTEM)\r\n    message(\"Do not found filesystem header\")\r\nendif(HAVE_FILESYSTEM)\r\n\r\noption(BUILD_ABANDONED_DETECTOR \"Should compiled abandoned detector example?\" OFF)\r\nif (BUILD_ABANDONED_DETECTOR)\r\n    add_definitions(-DBUILD_ABANDONED_DETECTOR)\r\n    add_subdirectory(combined)\r\nendif(BUILD_ABANDONED_DETECTOR)\r\n\r\noption(BUILD_EXAMPLES \"Should compiled examples (motion detection, pedestrians, faces, DNNs etc)?\" ON)\r\nif (BUILD_EXAMPLES)\r\n    add_subdirectory(example)\r\nendif(BUILD_EXAMPLES)\r\n\r\n\r\noption(USE_CLIP \"Should be used RuCLIP|CLIP for objects classification?\" OFF)\r\nif (USE_CLIP)\r\n    add_definitions(-DUSE_CLIP)\r\nendif(USE_CLIP)\r\n\r\noption(BUILD_CARS_COUNTING \"Should compiled Cars counting example?\" OFF)\r\nif (BUILD_CARS_COUNTING)\r\n    add_definitions(-DBUILD_CARS_COUNTING)\r\nendif(BUILD_CARS_COUNTING)\r\n\r\noption(BUILD_ASYNC_DETECTOR \"Should compiled async example with low fps Detector?\" OFF)\r\nif (BUILD_ASYNC_DETECTOR)\r\n    add_subdirectory(async_detector)\r\nendif(BUILD_ASYNC_DETECTOR)\r\n\r\noption(BUILD_YOLO_LIB \"Should compiled standalone yolo_lib with original darknet?\" OFF)\r\nif (BUILD_YOLO_LIB)\r\n    add_subdirectory(src/Detector/darknet)\r\n    add_definitions(-DBUILD_YOLO_LIB)\r\n\r\nif (MSVC)\r\n    if(\"${CMAKE_SIZEOF_VOID_P}\" STREQUAL \"4\")\r\n        set(BIT_SYSTEM x32)\r\n    else()\r\n        set(BIT_SYSTEM x64)\r\n    endif()\r\n\r\n    set(LIB_PTHREAD pthreadVC2)\r\nelse()\r\n    set(LIB_PTHREAD pthread)\r\nendif()\r\n\r\nif (MSVC)\r\n    file(COPY ${CMAKE_CURRENT_SOURCE_DIR}/src/Detector/darknet/3rdparty/dll/${BIT_SYSTEM}/pthreadVC2.dll DESTINATION ${CMAKE_BINARY_DIR}/Debug)\r\n    file(COPY ${CMAKE_CURRENT_SOURCE_DIR}/src/Detector/darknet/3rdparty/dll/${BIT_SYSTEM}/pthreadVC2.dll DESTINATION ${CMAKE_BINARY_DIR}/Release)\r\n\r\n    file(GLOB CUDNN_DLL ${CUDNN_DIR}/bin/*.dll)\r\n    file(COPY ${CUDNN_DLL} DESTINATION ${CMAKE_BINARY_DIR}/Release)\r\n    file(COPY ${CUDNN_DLL} DESTINATION ${CMAKE_BINARY_DIR}/Debug)\r\nendif()\r\n\r\nendif(BUILD_YOLO_LIB)\r\n\r\noption(BUILD_YOLO_TENSORRT \"Should compiled TensorRT binding for YOLO?\" OFF)\r\nif (BUILD_YOLO_TENSORRT)\r\n    add_subdirectory(src/Detector/tensorrt_yolo)\r\n    add_definitions(-DBUILD_YOLO_TENSORRT)\r\nendif(BUILD_YOLO_TENSORRT)\r\n\r\noption(MTRACKER_PYTHON \"Build mtracking Python bindings?\" OFF)\r\nif(MTRACKER_PYTHON)\r\n    set(NUMPY_INCLUDE_DIR \"\" CACHE FILEPATH \"Path to numpy header if cmake can't find them.\")\r\n    if (NOT ${NUMPY_INCLUDE_DIR} STREQUAL \"\")\r\n      message( \" *** NUMPY_INCLUDE_DIR : ${NUMPY_INCLUDE_DIR}\" )\r\n      if(NOT EXISTS ${NUMPY_INCLUDE_DIR}/numpy/ndarrayobject.h)\r\n          message(SEND_ERROR \"Can't find numpy/ndarrayobject.h in ${NUMPY_INCLUDE_DIR}\")\r\n      endif()\r\n    include_directories(${NUMPY_INCLUDE_DIR})\r\nendif()\r\n\r\n    set(PYBIND11_LTO_CXX_FLAGS \"\")\r\n    set(PYBIND11_PYTHON_VERSION 3)\r\n    add_subdirectory(thirdparty/pybind11)\r\nendif(MTRACKER_PYTHON)\r\n\r\n\r\nadd_subdirectory(thirdparty)\r\nadd_subdirectory(src)\r\n\r\n\r\n# Create CMake config files for distribution\r\nset(INCLUDE_INSTALL_DIR include/ )\r\nset(LIB_INSTALL_DIR lib/ )\r\n\r\ninstall(EXPORT MTTrackingExports\r\n    FILE ${PROJECT_NAME}Targets.cmake\r\n    NAMESPACE ${PROJECT_NAME}::\r\n    DESTINATION ${LIB_INSTALL_DIR}/${PROJECT_NAME}/cmake\r\n)\r\n\r\ninclude(CMakePackageConfigHelpers)\r\n\r\nset(CONFIG_FILENAME ${PROJECT_NAME}Config.cmake)\r\n\r\nconfigure_package_config_file(${CONFIG_FILENAME}.in\r\n    ${CMAKE_CURRENT_BINARY_DIR}/${CONFIG_FILENAME}\r\n    INSTALL_DESTINATION ${LIB_INSTALL_DIR}/${PROJECT_NAME}/cmake\r\n    PATH_VARS INCLUDE_INSTALL_DIR)\r\n\r\nwrite_basic_package_version_file(\r\n    ${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}ConfigVersion.cmake\r\n    VERSION ${PROJECT_VERSION}\r\n    COMPATIBILITY SameMajorVersion )\r\n\r\ninstall(FILES ${CMAKE_CURRENT_BINARY_DIR}/${CONFIG_FILENAME}\r\n              ${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}ConfigVersion.cmake\r\n        DESTINATION ${LIB_INSTALL_DIR}/${PROJECT_NAME}/cmake )\r\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MTTrackingConfig.cmake.in",
          "type": "blob",
          "size": 0.314453125,
          "content": "set(@PROJECT_NAME@_VERSION @PROJECT_VERSION@)\n\n@PACKAGE_INIT@\n\ninclude(\"${CMAKE_CURRENT_LIST_DIR}/@PROJECT_NAME@Targets.cmake\")\n\nset_and_check(@PROJECT_NAME@_INCLUDE_DIR \"@PACKAGE_INCLUDE_INSTALL_DIR@\")\nset_and_check(@PROJECT_NAME@_LIB_DIR \"@PACKAGE_INCLUDE_INSTALL_DIR@../lib\")\n\ncheck_required_components(@PROJECT_NAME@)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.7392578125,
          "content": "[![Status](https://github.com/Nuzhny007/Multitarget-tracker/actions/workflows/cmake.yml/badge.svg?branch=master)](https://github.com/Nuzhny007/Multitarget-tracker/actions?query=workflow%3Abuild-Ubuntu)\n[![CodeQL](https://github.com/Smorodov/Multitarget-tracker/workflows/CodeQL/badge.svg?branch=master)](https://github.com/Smorodov/Multitarget-tracker/actions?query=workflow%3ACodeQL)\n\n# Last changes\n\n* TensorRT 10 is supported\n\n* YOLOv11, YOLOv11-obb and YOLOv11-seg detector worked with TensorRT! Export pretrained Pytorch models [here (ultralytics/ultralytics)](https://github.com/ultralytics/ultralytics) to onnx format and run Multitarget-tracker with -e=6 example\n\n* YOLOv8-obb detector worked with TensorRT! Export pretrained Pytorch models [here (ultralytics/ultralytics)](https://github.com/ultralytics/ultralytics) to onnx format and run Multitarget-tracker with -e=6 example\n\n* YOLOv10 detector worked with TensorRT! Export pretrained Pytorch models [here (THU-MIG/yolov10)](https://github.com/THU-MIG/yolov10) to onnx format and run Multitarget-tracker with -e=6 example\n\n* YOLOv9 detector worked with TensorRT! Export pretrained Pytorch models [here (WongKinYiu/yolov9)](https://github.com/WongKinYiu/yolov9) to onnx format and run Multitarget-tracker with -e=6 example\n\n* YOLOv8 instance segmentation models worked with TensorRT! Export pretrained Pytorch models [here (ultralytics/ultralytics)](https://github.com/ultralytics/ultralytics) to onnx format and run Multitarget-tracker with -e=6 example\n\n* Re-identification model osnet_x0_25_msmt17 from [mikel-brostrom/yolo_tracking](https://github.com/mikel-brostrom/yolo_tracking)\n\n# New videos!\n\n* YOLOv8-obb detection with rotated boxes (DOTA v1.0 trained)\n\n[![YOLOv8-obb detection:](https://img.youtube.com/vi/1e6ur57Fhzs/0.jpg)](https://youtu.be/1e6ur57Fhzs)\n\n\n* YOLOv7 instance segmentation\n\n[![YOLOv7 instance segmentation:](https://img.youtube.com/vi/gZxuYyFz1dU/0.jpg)](https://youtu.be/gZxuYyFz1dU)\n\n\n* Very fast and small objects tracking [Thnx Scianand](https://github.com/Smorodov/Multitarget-tracker/issues/367)\n\n[![Fast and small motion:](https://img.youtube.com/vi/PalIIAfgX88/0.jpg)](https://youtu.be/PalIIAfgX88)\n\n* Vehicles speed calculation with YOLO v4\n\n[![Vehicles speed:](https://img.youtube.com/vi/qOHYvDwpsO0/0.jpg)](https://youtu.be/qOHYvDwpsO0)\n\n\n* First step to ADAS with YOLO v4\n\n[![Simple ADAS:](https://img.youtube.com/vi/5cgg5fy90Xg/0.jpg)](https://youtu.be/5cgg5fy90Xg)\n\n# Multitarget (multiple objects) tracker\n\n#### 1. Objects detector can be created with function [CreateDetector](https://github.com/Smorodov/Multitarget-tracker/blob/master/src/Detector/BaseDetector.cpp) with different values of the detectorType:\n\n1.1. Based on background substraction: built-in Vibe (tracking::Motion_VIBE), SuBSENSE (tracking::Motion_SuBSENSE) and LOBSTER (tracking::Motion_LOBSTER); MOG2 (tracking::Motion_MOG2) from [opencv](https://github.com/opencv/opencv/blob/master/modules/video/include/opencv2/video/background_segm.hpp); MOG (tracking::Motion_MOG), GMG (tracking::Motion_GMG) and CNT (tracking::Motion_CNT) from [opencv_contrib](https://github.com/opencv/opencv_contrib/tree/master/modules/bgsegm). For foreground segmentation used contours from OpenCV with result as cv::RotatedRect\n\n1.2. Haar face detector from OpenCV (tracking::Face_HAAR)\n\n1.3. HOG pedestrian detector from OpenCV (tracking::Pedestrian_HOG) and C4 pedestrian detector from [sturkmen72](https://github.com/sturkmen72/C4-Real-time-pedestrian-detection)  (tracking::Pedestrian_C4)\n\n1.4. Detector based on opencv_dnn (tracking::DNN_OCV) and pretrained models from [chuanqi305](https://github.com/chuanqi305/MobileNet-SSD) and [pjreddie](https://pjreddie.com/darknet/yolo/)\n\n1.5. YOLO detector (tracking::Yolo_Darknet) with darknet inference from [AlexeyAB](https://github.com/AlexeyAB/darknet) and pretrained models from [pjreddie](https://pjreddie.com/darknet/yolo/)\n\n1.6. YOLO detector (tracking::Yolo_TensorRT) with NVidia TensorRT inference from [enazoe](https://github.com/enazoe/yolo-tensorrt) and pretrained models from [pjreddie](https://pjreddie.com/darknet/yolo/)\n\n1.7. You can to use custom detector with bounding or rotated rectangle as output.\n\n#### 2. Matching or solve an [assignment problem](https://github.com/Smorodov/Multitarget-tracker/blob/master/src/Tracker/Ctracker.h):\n\n2.1. Hungrian algorithm (tracking::MatchHungrian) with cubic time O(N^3) where N is objects count\n\n2.2. Algorithm based on weighted bipartite graphs (tracking::MatchBipart) from [rdmpage](https://github.com/rdmpage/maximum-weighted-bipartite-matching) with time O(M * N^2) where N is objects count and M is connections count between detections on frame and tracking objects. It can be faster than Hungrian algorithm\n\n2.3. [Distance](https://github.com/Smorodov/Multitarget-tracker/blob/master/src/Tracker/Ctracker.h) from detections and objects: euclidean distance in pixels between centers (tracking::DistCenters), euclidean distance in pixels between rectangles (tracking::DistRects), Jaccard or IoU distance from 0 to 1 (tracking::DistJaccard)\n\n#### 3. [Smoothing trajectories and predict missed objects](https://github.com/Smorodov/Multitarget-tracker/blob/master/src/Tracker/Ctracker.h):\n\n3.1. Linear Kalman filter from OpenCV (tracking::KalmanLinear)\n\n3.2. Unscented Kalman filter from OpenCV (tracking::KalmanUnscented) with constant velocity or constant acceleration models\n\n3.3. [Kalman goal](https://github.com/Smorodov/Multitarget-tracker/blob/master/src/Tracker/Ctracker.h) is only coordinates (tracking::FilterCenter) or coordinates and size (tracking::FilterRect)\n\n3.4. Simple [Abandoned detector](https://github.com/Smorodov/Multitarget-tracker/blob/master/src/Tracker/Ctracker.h)\n\n3.5. [Line intersection](https://github.com/Smorodov/Multitarget-tracker/blob/master/src/CarsCounting.cpp) counting\n\n#### 4. [Advanced visual search](https://github.com/Smorodov/Multitarget-tracker/blob/master/src/Tracker/Ctracker.h) for objects if they have not been detected:\n\n4.1. No search (tracking::TrackNone)\n\n4.2. Built-in DAT (tracking::TrackDAT) from [foolwood](https://github.com/foolwood/DAT), STAPLE (tracking::TrackSTAPLE) from [xuduo35](https://github.com/xuduo35/STAPLE) or LDES (tracking::TrackLDES) from [yfji](https://github.com/yfji/LDESCpp); KCF (tracking::TrackKCF), MIL (tracking::TrackMIL), MedianFlow (tracking::TrackMedianFlow), GOTURN (tracking::TrackGOTURN), MOSSE (tracking::TrackMOSSE) or CSRT (tracking::TrackCSRT) from [opencv_contrib](https://github.com/opencv/opencv_contrib/tree/master/modules/tracking)\n\nWith this option the tracking can work match slower but more accuracy.\n\n#### 5. Pipeline\n\n5.1. Syncronous [pipeline - SyncProcess](https://github.com/Smorodov/Multitarget-tracker/blob/master/example/VideoExample.h):\n- get frame from capture device;\n- decoding;\n- objects detection (1);\n- tracking (2-4);\n- show result.\n\nThis pipeline is good if all algorithms are fast and works faster than time between two frames (40 ms for device with 25 fps). Or it can be used if we have only 1 core for all (no parallelization).\n\n5.2. Pipeline with [2 threads - AsyncProcess](https://github.com/Smorodov/Multitarget-tracker/blob/master/example/VideoExample.h):\n- 1th thread takes frame t and makes capture, decoding and objects detection;\n- 2th thread takes frame t-1, results from first thread and makes tracking and results presentation (this is the Main read).\n\nSo we have a latency on 1 frame but on two free CPU cores we can increase performance on 2 times.\n\n5.3. Fully [acynchronous pipeline](https://github.com/Smorodov/Multitarget-tracker/tree/master/async_detector) can be used if the objects detector works with low fps and we have a free 2 CPU cores. In this case we use 4 threads:\n- 1th main thread is not busy and used for GUI and result presentation;\n- 2th thread makes capture and decoding, puts frames in threadsafe queue;\n- 3th thread is used for objects detection on the newest frame from the queue;\n- 4th thread is used for objects tracking: waits the frame with detection from 3th tread and used advanced visual search (4) in intermediate frames from queue until it ges a frame with detections.\n\nThis pipeline can used with slow but accuracy DNN and track objects in intermediate frame in realtime without latency.\n\nAlso you can read [Wiki in Russian](https://github.com/Smorodov/Multitarget-tracker/wiki).\n\n#### Demo Videos\n\n* Mouse tracking:\n\n[![Tracking:](https://img.youtube.com/vi/2fW5TmAtAXM/0.jpg)](https://www.youtube.com/watch?v=2fW5TmAtAXM)\n\n* Motion Detection and tracking:\n\n[![Motion Detection and tracking:](https://img.youtube.com/vi/GjN8jOy4kVw/0.jpg)](https://www.youtube.com/watch?v=GjN8jOy4kVw)\n\n* Multiple Faces tracking:\n\n[![Multiple Faces tracking:](https://img.youtube.com/vi/j67CFwFtciU/0.jpg)](https://www.youtube.com/watch?v=j67CFwFtciU)\n\n* Simple Abandoned detector:\n\n[![Simple Abandoned detector:](https://img.youtube.com/vi/fpkHRsFzspA/0.jpg)](https://www.youtube.com/watch?v=fpkHRsFzspA)\n\n#### Tested Platforms\n1. Ubuntu Linux 18.04 with x86 processors\n2. Ubuntu Linux 18.04 with Nvidia Jetson Nano (YOLO + darknet on GPU works!)\n3. Windows 10 (x64 and x32 builds)\n\n#### Build\n1. Download project sources\n2. Install CMake\n3. Install OpenCV (https://github.com/opencv/opencv) and OpenCV contrib (https://github.com/opencv/opencv_contrib) repositories\n4. Configure project CmakeLists.txt, set OpenCV_DIR (-DOpenCV_DIR=/path/to/opencv/build).\n5. If opencv_contrib don't installed then disable options USE_OCV_BGFG=OFF, USE_OCV_KCF=OFF and USE_OCV_UKF=OFF\n6. If you want to use native darknet YOLO detector with CUDA + cuDNN then set BUILD_YOLO_LIB=ON  (Install first CUDA and cuDNN libraries from Nvidia)\n7. If you want to use YOLO detector with TensorRT then set BUILD_YOLO_TENSORRT=ON (Install first TensorRT library from Nvidia)\n8. For building example with low fps detector (now native darknet YOLO detector) and Tracker worked on each frame: BUILD_ASYNC_DETECTOR=ON\n9. For building example with line crossing detection (cars counting): BUILD_CARS_COUNTING=ON\n10. Go to the build directory and run make\n\n**Full build:**\n\n           git clone https://github.com/Smorodov/Multitarget-tracker.git\n           cd Multitarget-tracker\n           mkdir build\n           cd build\n           cmake . .. -DUSE_OCV_BGFG=ON -DUSE_OCV_KCF=ON -DUSE_OCV_UKF=ON -DBUILD_YOLO_LIB=ON -DBUILD_YOLO_TENSORRT=ON -DBUILD_ASYNC_DETECTOR=ON -DBUILD_CARS_COUNTING=ON\n           make -j\n\nHow to run cmake on Windows for Visual Studio 15 2017 Win64: [example](https://github.com/Smorodov/Multitarget-tracker/blob/master/data/cmake_vs2017.bat). You need to add directory with cmake.exe to PATH and change build params in cmake.bat\n\n\n**Usage:**\n\n           Usage:\n             ./MultitargetTracker <path to movie file> [--example]=<number of example 0..7> [--start_frame]=<start a video from this position> [--end_frame]=<play a video to this position> [--end_delay]=<delay in milliseconds after video ending> [--out]=<name of result video file> [--show_logs]=<show logs> [--gpu]=<use OpenCL> [--async]=<async pipeline> [--res]=<csv log file> [--settings]=<ini file> [--batch_size=<number of frames>]\n             ./MultitargetTracker ../data/atrium.avi -e=1 -o=../data/atrium_motion.avi\n           Press:\n           * 'm' key for change mode: play|pause. When video is paused you can press any key for get next frame.\n           * Press Esc to exit from video\n\n           Params:\n           1. Movie file, for example ../data/atrium.avi\n           2. [Optional] Number of example: 0 - MouseTracking, 1 - MotionDetector, 2 - FaceDetector, 3 - PedestrianDetector, 4 - OpenCV dnn objects detector, 5 - Yolo Darknet detector, 6 - YOLO TensorRT Detector, Cars counting\n              -e=0 or --example=1\n           3. [Optional] Frame number to start a video from this position\n              -sf=0 or --start_frame==1500\n           4. [Optional] Play a video to this position (if 0 then played to the end of file)\n              -ef=0 or --end_frame==200\n           5. [Optional] Delay in milliseconds after video ending\n              -ed=0 or --end_delay=1000\n           6. [Optional] Name of result video file\n              -o=out.avi or --out=result.mp4\n           7. [Optional] Show Trackers logs in terminal\n              -sl=1 or --show_logs=0\n           8. [Optional] Use built-in OpenCL\n              -g=1 or --gpu=0\n           9. [Optional] Use 2 threads for processing pipeline\n              -a=1 or --async=0\n           10. [Optional] Path to the csv file with tracking result\n              -r=res.csv or --res=res.csv\n           11. [Optional] Path to the ini file with tracker settings\n              -s=settings.ini or --settings=settings.ini\n           12. [Optional] Batch size - simultaneous detection on several consecutive frames\n              -bs=2 or --batch_size=1\n\nMore details here: [How to run examples](https://github.com/Smorodov/Multitarget-tracker/wiki/Run-examples).\n\n#### Using MT Tracking as a library in your CMake project\n\nBuild MTTracking in the usual way, and choose an installation prefix where the library will be installed\n(see [CMake Documentation](https://cmake.org/cmake/help/latest/variable/CMAKE_INSTALL_PREFIX.html) for the defaults).\n\nIn the `build` directory run\n```\n$ cmake --install .\n```\nThis will generate the CMake files needed to find the MTTracking package with libraries and include files for\nyour project. E.g.\n```\nMTTrackingConfig.cmake\nMTTrackingConfigVersion.cmake\nMTTrackingTargets.cmake\n```\n\nIn your CMake project, do the following:\n```\n    find_package(MTTracking REQUIRED)\n    target_include_directories(MyProjectTarget PUBLIC ${MTTracking_INCLUDE_DIR})\n    target_link_libraries(MyProjectTarget PUBLIC MTTracking::mtracking MTTracking::mdetection)\n```\n\nYou may need to provide CMake with the location to find the above `.cmake` files, e.g.\n```\n$ cmake -DMTTracking_DIR=<location_of_cmake_files> ..\n```\n\nIf CMake succeeds at finding the package, you can use MTTracking in your project e.g.\n```\n#include <mtracking/Ctracker.h>\n//...\n    std::unique_ptr<BaseTracker> m_tracker;\n\n\tTrackerSettings settings;\n\tsettings.SetDistance(tracking::DistJaccard);\n    m_tracker = BaseTracker::CreateTracker(settings);\n//...\n```\nAnd so on.\n\n#### Thirdparty libraries\n* OpenCV (and contrib): https://github.com/opencv/opencv and https://github.com/opencv/opencv_contrib\n* Vibe: https://github.com/BelBES/VIBE\n* SuBSENSE and LOBSTER: https://github.com/ethereon/subsense\n* GTL: https://github.com/rdmpage/graph-template-library\n* MWBM: https://github.com/rdmpage/maximum-weighted-bipartite-matching\n* Pedestrians detector: https://github.com/sturkmen72/C4-Real-time-pedestrian-detection\n* Non Maximum Suppression: https://github.com/Nuzhny007/Non-Maximum-Suppression\n* MobileNet SSD models: https://github.com/chuanqi305/MobileNet-SSD\n* YOLO v3 models: https://pjreddie.com/darknet/yolo/\n* Darknet inference and YOLO v4 models: https://github.com/AlexeyAB/darknet\n* NVidia TensorRT inference and YOLO v5 models: https://github.com/enazoe/yolo-tensorrt\n* YOLOv6 models: https://github.com/meituan/YOLOv6/releases\n* YOLOv7 models: https://github.com/WongKinYiu/yolov7\n* GOTURN models: https://github.com/opencv/opencv_extra/tree/c4219d5eb3105ed8e634278fad312a1a8d2c182d/testdata/tracking\n* DAT tracker: https://github.com/foolwood/DAT\n* STAPLE tracker: https://github.com/xuduo35/STAPLE\n* LDES tracker: https://github.com/yfji/LDESCpp\n* Ini file parser: https://github.com/benhoyt/inih\n* Circular Code from Lior Kogan\n\n#### License\nApache 2.0: [LICENSE text](https://github.com/Smorodov/Multitarget-tracker/blob/master/LICENSE)\n\n#### Project cititations\n1. Jeroen PROVOOST \"Camera gebaseerde analysevan de verkeersstromen aaneen kruispunt\", 2014 ( https://iiw.kuleuven.be/onderzoek/eavise/mastertheses/provoost.pdf )\n2. Roberto Ciano, Dimitrij Klesev \"Autonome Roboterschwarme in geschlossenen Raumen\", 2015 ( https://www.hs-furtwangen.de/fileadmin/user_upload/fak_IN/Dokumente/Forschung_InformatikJournal/informatikJournal_2016.pdf#page=18 )\n3. Wenda Qin, Tian Zhang, Junhe Chen \"Traffic Monitoring By Video: Vehicles Tracking and Vehicle Data Analysing\", 2016 ( http://cs-people.bu.edu/wdqin/FinalProject/CS585%20FinalProjectReport.html )\n4. Ipek BARIS \"CLASSIFICATION AND TRACKING OF VEHICLES WITH HYBRID CAMERA SYSTEMS\", 2016 ( http://cvrg.iyte.edu.tr/publications/IpekBaris_MScThesis.pdf )\n5. Cheng-Ta Lee, Albert Y. Chen, Cheng-Yi Chang \"In-building Coverage of Automated External Defibrillators Considering Pedestrian Flow\", 2016 ( http://www.see.eng.osaka-u.ac.jp/seeit/icccbe2016/Proceedings/Full_Papers/092-132.pdf )\n6. Roberto Ciano, Dimitrij Klesev \"Autonome Roboterschwarme in geschlossenen Raumen\" in \"informatikJournal 2016/17\", 2017 ( https://docplayer.org/124538994-2016-17-informatikjournal-2016-17-aktuelle-berichte-aus-forschung-und-lehre-der-fakultaet-informatik.html )\n7. Omid Noorshams \"Automated systems to assess weights and activity in grouphoused mice\", 2017 ( https://pdfs.semanticscholar.org/e5ff/f04b4200c149fb39d56f171ba7056ab798d3.pdf )\n8. RADEK VOPÁLENSKÝ \"DETECTION,TRACKING AND CLASSIFICATION OF VEHICLES\", 2018 ( https://www.vutbr.cz/www_base/zav_prace_soubor_verejne.php?file_id=181063 )\n9. Márk Rátosi, Gyula Simon \"Real-Time Localization and Tracking  using Visible Light Communication\", 2018 ( https://ieeexplore.ieee.org/abstract/document/8533800 )\n10. Thi Nha Ngo, Kung-Chin Wu, En-Cheng Yang, Ta-Te Lin \"A real-time imaging system for multiple honey bee tracking and activity monitoring\", 2019 ( https://www.sciencedirect.com/science/article/pii/S0168169919301498 )\n11. Tiago Miguel, Rodrigues de Almeida \"Multi-Camera and Multi-Algorithm Architecture for VisualPerception onboard the ATLASCAR2\", 2019 ( http://lars.mec.ua.pt/public/LAR%20Projects/Vision/2019_TiagoAlmeida/Thesis_Tiago_AlmeidaVF_26Jul2019.pdf )\n12. ROS, http://docs.ros.org/lunar/api/costmap_converter/html/Ctracker_8cpp_source.html\n13. Sangeeth Kochanthara, Yanja Dajsuren, Loek Cleophas, Mark van den Brand \"Painting the Landscape of Automotive Software in GitHub\", 2022 ( https://arxiv.org/abs/2203.08936 )\n"
        },
        {
          "name": "TODO",
          "type": "blob",
          "size": 1.0048828125,
          "content": "Global data association for multi-object tracking using network flows:\r\n1. https://github.com/nwojke/mcf\r\n2. https://github.com/jutanke/cabbage\r\n3. Tracking The Untrackable: Learning To Track Multiple Cues with Long-Term Dependencies\r\n4. http://openaccess.thecvf.com/content_cvpr_2017/papers/Tang_Multiple_People_Tracking_CVPR_2017_paper.pdf\r\n5. https://github.com/abhineet123/Deep-Learning-for-Tracking-and-Detection\r\n6. https://arxiv.org/abs/1903.05625\r\n7. https://arxiv.org/abs/1907.03961\r\n8. http://www.cvlibs.net/projects/online_tracking/\r\n9. https://github.com/jwchoi384/Gaussian_YOLOv3\r\n10. muSSP: https://github.com/yu-lab-vt/muSSP\r\n11. https://github.com/ifzhang/FairMOT\r\n12. https://github.com/AndreaHor/LifT_Solver\r\n\r\nDeep SORT:\r\n1. https://github.com/humoncy/YOLOv3-SORT-ReID\r\n2. https://github.com/nwojke/deep_sort\r\n3. https://github.com/bitzy/DeepSort\r\n3. https://github.com/oylz/DS\r\n\r\nNew:\r\n1. https://github.com/ceccocats/tkDNN\r\n2. dasiamrpn_tracker.py -> C++\r\n\r\nTests:\r\n1. Quality tests\r\n2. Performance tests\r\n\r\n"
        },
        {
          "name": "async_detector",
          "type": "tree",
          "content": null
        },
        {
          "name": "combined",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo.py",
          "type": "blob",
          "size": 3.1943359375,
          "content": "import sys\nimport glob\nimport getopt\nimport numpy as np\nimport cv2 as cv\nimport pymtracking as mt\n\nprint(\"OpenCV Version: {}\".format(cv.__version__))\n\n\ndef draw_regions(img, regions, color):\n    for reg in regions:\n        brect = reg.brect\n        cv.rectangle(img, (brect.x, brect.y, brect.width, brect.height), color, 2)\n\n\ndef draw_tracks(img, tracks, fps):\n    for track in tracks:\n        brect = track.GetBoundingRect()\n        if track.isStatic:\n            cv.rectangle(img, (brect.x, brect.y, brect.width, brect.height), (255, 0, 255), 2)\n        elif track.IsRobust(int(fps / 4), 0.7, (0.1, 10.), 3):\n            cv.rectangle(img, (brect.x, brect.y, brect.width, brect.height), (0, 255, 0), 2)\n            trajectory = track.GetTrajectory()\n            for i in range(0, len(trajectory) - 1):\n                cv.line(img, trajectory[i], trajectory[i+1], (0, 255, 0), 1)\n\n\ndef main():\n    args, video_src = getopt.getopt(sys.argv[1:], '', ['cascade=', 'nested-cascade='])\n    try:\n        video_src = video_src[0]\n    except:\n        video_src = 0\n    args = dict(args)\n\n    cam = cv.VideoCapture(video_src)\n\n    _ret, img = cam.read()\n    print(\"cam.read res = \", _ret, \", im size = \", img.shape)\n\n    fps = cam.get(cv.CAP_PROP_FPS)\n    print(video_src, \" fps = \", fps)\n\n    configBGFG = mt.KeyVal()\n    configBGFG.Add('useRotatedRect', '20')\n    configBGFG.Add('history', '1000')\n    configBGFG.Add(\"nmixtures\", \"3\")\n    configBGFG.Add(\"backgroundRatio\", \"0.7\")\n    configBGFG.Add(\"noiseSigma\", \"0\")\n    print(\"configBGFG = \", configBGFG)\n    mdetector = mt.BaseDetector(mt.BaseDetector.Detectors.MOG, configBGFG, img)\n    print(\"CanGrayProcessing: \", mdetector.CanGrayProcessing())\n    mdetector.SetMinObjectSize((1, 1))\n\n    tracker_settings = mt.TrackerSettings()\n\n    tracker_settings.SetDistance(mt.MTracker.DistRects)\n    tracker_settings.kalmanType = mt.MTracker.KalmanLinear\n    tracker_settings.filterGoal = mt.MTracker.FilterCenter\n    tracker_settings.lostTrackType = mt.MTracker.TrackNone\n    tracker_settings.matchType = mt.MTracker.MatchHungrian\n    tracker_settings.useAcceleration = False\n    tracker_settings.dt = 0.5\n    tracker_settings.accelNoiseMag = 0.1\n    tracker_settings.distThres = 0.95\n    tracker_settings.minAreaRadiusPix = img.shape[0] / 5.\n    tracker_settings.minAreaRadiusK = 0.8\n    tracker_settings.useAbandonedDetection = False\n    tracker_settings.maximumAllowedSkippedFrames = int(2 * fps)\n    tracker_settings.maxTraceLength = int(2 * fps)\n\n    mtracker = mt.MTracker(tracker_settings)\n\n    while True:\n        _ret, img = cam.read()\n        if _ret:\n            print(\"cam.read res = \", _ret, \", im size = \", img.shape, \", fps = \", fps)\n        else:\n            break\n\n        mdetector.Detect(img)\n        regions = mdetector.GetDetects()\n        print(\"mdetector.Detect:\", len(regions))\n\n        mtracker.Update(regions, img, fps)\n        tracks = mtracker.GetTracks()\n        print(\"mtracker.Update:\", len(tracks))\n\n        vis = img.copy()\n        # draw_regions(vis, regions, (255, 0, 255))\n        draw_tracks(vis, tracks, fps)\n        cv.imshow('detect', vis)\n\n        if cv.waitKey(int(1000 / fps)) == 27:\n            break\n\n    print('Done')\n\n\nif __name__ == '__main__':\n    main()\n    cv.destroyAllWindows()\n"
        },
        {
          "name": "example",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 9.0966796875,
          "content": "import os, re, sys, shutil, platform, subprocess\n\nfrom setuptools import setup, find_packages, Extension\nfrom setuptools.command.build_ext import build_ext\nfrom setuptools.command.install_lib import install_lib\nfrom setuptools.command.install_scripts import install_scripts\nfrom distutils.command.install_data import install_data\nfrom distutils.version import LooseVersion\n\nPACKAGE_NAME = \"pymtracking\"\n\nclass CMakeExtension(Extension):\n    def __init__(self, name, sourcedir=''):\n        Extension.__init__(self, name, sources=[])\n        self.sourcedir = os.path.abspath(sourcedir)\n\n\nclass InstallCMakeLibsData(install_data):\n    \"\"\"\n    Just a wrapper to get the install data into the egg-info\n    Listing the installed files in the egg-info guarantees that\n    all of the package files will be uninstalled when the user\n    uninstalls your package through pip\n    \"\"\"\n    def run(self):\n        \"\"\"\n        Outfiles are the libraries that were built using cmake\n        \"\"\"\n        # There seems to be no other way to do this; I tried listing the\n        # libraries during the execution of the InstallCMakeLibs.run() but\n        # setuptools never tracked them, seems like setuptools wants to\n        # track the libraries through package data more than anything...\n        # help would be appriciated\n        self.outfiles = self.distribution.data_files\n\n__metaclass__ = type\nclass InstallCMakeLibs(install_lib, object):\n    \"\"\"\n    Get the libraries from the parent distribution, use those as the outfiles\n    Skip building anything; everything is already built, forward libraries to\n    the installation step\n    \"\"\"\n    def run(self):\n        \"\"\"\n        Copy libraries from the bin directory and place them as appropriate\n        \"\"\"\n        self.announce(\"Moving library files\", level=3)\n        # We have already built the libraries in the previous build_ext step\n        self.skip_build = True\n        if hasattr(self.distribution, 'bin_dir'):\n            bin_dir = self.distribution.bin_dir\n        else:\n            bin_dir = os.path.join(self.build_dir, \"Release\")\n            if not os.path.exists(bin_dir):\n                bin_dir = \"build/Release\"\n                self.build_dir = \"build/Release\"\n        print(\"bin_dir:\", bin_dir, \"build_dir:\", self.build_dir)\n        # Depending on the files that are generated from your cmake\n        # build chain, you may need to change the below code, such that\n        # your files are moved to the appropriate location when the installation\n        # is run\n        libs = [os.path.join(bin_dir, _lib) for _lib in \n                os.listdir(bin_dir) if \n                os.path.isfile(os.path.join(bin_dir, _lib)) and \n                os.path.splitext(_lib)[1] in [\".dll\", \".so\"]\n                and not (_lib.startswith(\"python\") or _lib.startswith(PACKAGE_NAME))]\n        for lib in libs:\n            shutil.move(lib, os.path.join(self.build_dir,\n                                          os.path.basename(lib)))\n        # Mark the libs for installation, adding them to \n        # distribution.data_files seems to ensure that setuptools' record \n        # writer appends them to installed-files.txt in the package's egg-info\n        #\n        # Also tried adding the libraries to the distribution.libraries list, \n        # but that never seemed to add them to the installed-files.txt in the \n        # egg-info, and the online recommendation seems to be adding libraries \n        # into eager_resources in the call to setup(), which I think puts them \n        # in data_files anyways. \n        # \n        # What is the best way?\n        # These are the additional installation files that should be\n        # included in the package, but are resultant of the cmake build\n        # step; depending on the files that are generated from your cmake\n        # build chain, you may need to modify the below code\n        self.distribution.data_files = [os.path.join(self.install_dir, \n                                                     os.path.basename(lib))\n                                        for lib in libs]\n        # Must be forced to run after adding the libs to data_files\n        self.distribution.run_command(\"install_data\")\n        super(InstallCMakeLibs, self).run()\n\n__metaclass__ = type\nclass InstallCMakeScripts(install_scripts, object):\n    \"\"\"\n    Install the scripts in the build dir\n    \"\"\"\n    def run(self):\n        \"\"\"\n        Copy the required directory to the build directory and super().run()\n        \"\"\"\n        self.announce(\"Moving scripts files\", level=3)\n        # Scripts were already built in a previous step\n        self.skip_build = True\n        bin_dir = self.distribution.bin_dir\n        scripts_dirs = [os.path.join(bin_dir, _dir) for _dir in\n                        os.listdir(bin_dir) if\n                        os.path.isdir(os.path.join(bin_dir, _dir))]\n        for scripts_dir in scripts_dirs:\n            shutil.move(scripts_dir,\n                        os.path.join(self.build_dir,\n                                     os.path.basename(scripts_dir)))\n        # Mark the scripts for installation, adding them to \n        # distribution.scripts seems to ensure that the setuptools' record \n        # writer appends them to installed-files.txt in the package's egg-info\n        self.distribution.scripts = scripts_dirs\n        super(InstallCMakeScripts, self).run()\n\n__metaclass__ = type\nclass BuildCMakeExt(build_ext, object):\n    \"\"\"\n    Builds using cmake instead of the python setuptools implicit build\n    \"\"\"\n    def run(self):\n        \"\"\"\n        Perform build_cmake before doing the 'normal' stuff\n        \"\"\"\n        for extension in self.extensions:\n            self.build_cmake(extension)\n        super(BuildCMakeExt, self).run()\n\n    def build_cmake(self, extension):\n        \"\"\"\n        The steps required to build the extension\n        \"\"\"\n        self.announce(\"Preparing the build environment\", level=3)\n        build_dir = os.path.join(self.build_temp)\n        extension_path = os.path.abspath(os.path.dirname(self.get_ext_fullpath(extension.name)))\n        os.makedirs(build_dir)\n        os.makedirs(extension_path)\n        python_version = str(sys.version_info[0]) + \".\" + str(sys.version_info[1])\n\n        # Now that the necessary directories are created, build\n        self.announce(\"Configuring cmake project\", level=3)\n        cmake_args = ['-DPYTHON_EXECUTABLE=' + sys.executable,\n                      '-DUSE_OCV_BGFG=ON',\n                      '-DUSE_OCV_KCF=ON',\n                      '-DSILENT_WORK=ON',\n                      '-DBUILD_EXAMPLES=OFF',\n                      '-DBUILD_ASYNC_DETECTOR=OFF',\n                      '-DBUILD_CARS_COUNTING=OFF',\n                      '-DBUILD_YOLO_LIB=OFF',\n                      '-DBUILD_YOLO_TENSORRT=OFF',\n                      '-DMTRACKER_PYTHON=ON']\n        if not os.path.exists(self.build_temp):\n            os.makedirs(self.build_temp)\n        self.spawn(['cmake', '-H'+extension.sourcedir, '-B'+self.build_temp]+ cmake_args)\n\n        self.announce(\"Building binaries\", level=3)\n        self.spawn([\"cmake\", \"--build\", self.build_temp, \n                    \"--config\", \"Release\", '--', '-j8'])\n\n        # Build finished, now copy the files into the copy directory\n        # The copy directory is the parent directory of the extension (.pyd)\n        self.announce(\"Moving built python module\", level=3)\n        \n        bin_dir = \"build\" # self.build_temp\n        self.distribution.bin_dir = bin_dir\n        list_bin = os.listdir(bin_dir)\n        print(\"bin_dir:\", bin_dir, \", extension_path:\", extension_path, \", list_bin:\", list_bin)\n        pyd_path = []\n        for _pyd in list_bin:\n            print(\"_pyd:\", _pyd)\n            if os.path.isfile(os.path.join(bin_dir, _pyd)) and os.path.splitext(_pyd)[0].startswith(PACKAGE_NAME) and os.path.splitext(_pyd)[1] in [\".pyd\", \".so\"]:\n                   pyd_path.append(os.path.join(bin_dir, _pyd))\n        print(\"pyd_path:\", pyd_path)\n        pyd_path = pyd_path[0]\n        shutil.move(pyd_path, extension_path)\n\n        # After build_ext is run, the following commands will run:\n        # \n        # install_lib\n        # install_scripts\n        # \n        # These commands are subclassed above to avoid pitfalls that\n        # setuptools tries to impose when installing these, as it usually\n        # wants to build those libs and scripts as well or move them to a\n        # different place. See comments above for additional information\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=PACKAGE_NAME,\n    version='1.0.1',\n    author='Nuzhny007',\n    author_email='nuzhny@mail.ru',\n    url='https://github.com/Smorodov/Multitarget-tracker',\n    license='Apache 2.0',\n    description='Official Python wrapper for Multitarget-tracker',\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    ext_modules=[CMakeExtension(name=PACKAGE_NAME, sourcedir='.')],\n    cmdclass={\n        'build_ext': BuildCMakeExt,\n        'install_data': InstallCMakeLibsData,\n        'install_lib': InstallCMakeLibs,\n        #'install_scripts': InstallCMakeScripts\n        },\n    zip_safe=False,\n    packages=find_packages(),\n    keywords=['Multitarget-tracker', 'Multiple Object Tracking', 'Computer Vision', 'Machine Learning'],\n)\n\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "thirdparty",
          "type": "tree",
          "content": null
        },
        {
          "name": "vs_garbage_del.bat",
          "type": "blob",
          "size": 0.0244140625,
          "content": "Del *.obj;*.sdf;*.ilk /s\n"
        }
      ]
    }
  ]
}