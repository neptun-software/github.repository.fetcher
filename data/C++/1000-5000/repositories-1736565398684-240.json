{
  "metadata": {
    "timestamp": 1736565398684,
    "page": 240,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "OpenNMT/CTranslate2",
      "stars": 3515,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0810546875,
          "content": "*\n!CMakeLists.txt\n!README.md\n!cli\n!cmake\n!include\n!python\n!src\n!tests\n!third_party\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1220703125,
          "content": "*.pyc\n/.vs\n/build\n\nCMake*.json\n.idea\npython/build/\npython/ctranslate2.egg-info/\npython/dist/\n.cache\ndocs/build/\ndocs/python/\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.740234375,
          "content": "[submodule \"third_party/cxxopts\"]\n\tpath = third_party/cxxopts\n\turl = https://github.com/jarro2783/cxxopts.git\n[submodule \"third_party/thrust\"]\n\tpath = third_party/thrust\n\turl = https://github.com/NVIDIA/thrust.git\n[submodule \"third_party/googletest\"]\n\tpath = third_party/googletest\n\turl = https://github.com/google/googletest.git\n[submodule \"third_party/cpu_features\"]\n\tpath = third_party/cpu_features\n\turl = https://github.com/google/cpu_features.git\n[submodule \"third_party/spdlog\"]\n\tpath = third_party/spdlog\n\turl = https://github.com/gabime/spdlog.git\n[submodule \"third_party/ruy\"]\n\tpath = third_party/ruy\n\turl = https://github.com/google/ruy.git\n[submodule \"third_party/cutlass\"]\n\tpath = third_party/cutlass\n\turl = https://github.com/NVIDIA/cutlass.git\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 75.20703125,
          "content": "## [Unreleased]\n\n### New features\n\n### Fixes and improvements\n\n## [v4.5.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v4.5.0) (2024-10-22)\nNote: The Ctranslate2 Python package now supports CUDNN 9 and is no longer compatible with CUDNN 8.  \n\n### New features\n* Support Phi3 (#1800)\n* Support Mistral Nemo (#1785)\n* Support Wav2Vec2Bert ASR (#1778)\n\n### Fixes and improvements\n* Upgrade to CUDNN9 (#1803)\n* Fix logits vocab (#1786 + #1791)\n* Update doc AWQ (#1795)\n\n## [v4.4.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v4.4.0) (2024-09-09)\n**Removed**: Flash Attention support in the Python package due to significant package size increase with minimal performance gain.  \nNote: Flash Attention remains supported in the C++ package with the `WITH_FLASH_ATTN` option.  \nFlash Attention may be re-added in the future if substantial improvements are made.\n\n### New features\n* Support Llama3 (#1751)\n* Support Gemma2 (1772)\n* Add log probs for all tokens in vocab (#1755)\n* Grouped conv1d (#1749 + #1758)\n\n### Fixes and improvements\n* Fix pipeline (#1723 + #1747)\n* Some improvements in flash attention (#1732)\n* Fix crash when using return_alternative on CUDA (#1733)\n* Quantization AWQ GEMM + GEMV (#1727)\n\n## [v4.3.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v4.3.1) (2024-06-10)\nNote: Because of exceeding project's size on Pypi (> 20 GB), the release v4.3.0 was pushed unsuccessfully.\n\n### Fixes and improvements\n* Improve the compilation (#1706 and #1705)\n* Fix position bias in tensor parallel mode (#1714)\n\n## [v4.3.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v4.3.0) (2024-05-17)\n\n### New features\n* Support phi-3 (8k and 128k) (#1700 and #1680)\n\n### Fixes and improvements\n* Fix regression Flash Attention (#1695)\n\n## [v4.2.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v4.2.1) (2024-04-24)\n\nNote: Because of the increasing of package's size (> 100 MB), the release v4.2.0 was pushed unsuccessfully.\n\n### New features\n* Support load/unload for generator/Whisper Attention (#1670)\n\n### Fixes and improvements\n* Fix Llama 3 (#1671)\n\n## [v4.2.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v4.2.0) (2024-04-10)\n\n### New features\n* Support Flash Attention (#1651)\n* Implementation of gemm for FLOAT32 compute type with RUY backend (#1598)\n* Conv1D quantization for only CPU (DNNL and CUDA backend is not supported) (#1601)\n\n### Fixes and improvements\n* Fix bug tensor parallel (#1643)\n* Use BestSampler when temperature is 0 (#1659)\n* Fix bug gemma (#1660)\n* Optimize loading/unloading time for Translator with cache (#1645)\n\n## [v4.1.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v4.1.0) (2024-03-11)\n\n### New features\n* Support Gemma Model (#1631)\n* Support Tensor Parallelism (#1599)\n\n### Fixes and improvements\n* Avoid initializing unused GPU (#1633)\n* Read very large tensor by chunk if the size > max value of int (#1636)\n* Update Readme\n\n## [v4.0.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v4.0.0) (2024-02-15)\n\nThis major version introduces the breaking change while updating to cuda 12.\n\n### Breaking changes\n\n### Python\n\n* Support cuda 12\n\n### New features\n\n* Add feature to_device() in class StorageView in Python to move data between host <-> device\n\n### Fixes and improvements\n\n* Implement Conv1D with im2col and GEMM to improvement in performance\n* Get tokens in the range of the vocab size for LlaMa models\n* Fix loss of performance\n* Update cibuildwheel to 2.16.5\n\n## [v3.24.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.24.0) (2024-01-08)\n\n### New features\n* Support of new option offset to ignore token score of special tokens\n\n## [v3.23.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.23.0) (2023-12-05)\n\n### New features\n* Support Phi model\n\n### Fixes and improvements\n* Fix the conversion for whisper without the \"alignment_heads\" in the \"generation_config.json\"\n* Fix forward batch\n\n## [v3.22.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.22.0) (2023-11-22)\n\n### New features\n* Support \"sliding window\" and \"chunking input\" for Mistral\n\n### Fixes and improvements\n* Take into account the \"generation_config.json\" and fix \"lang_ids\" getter for Whisper converter\n* Accept callback even on \"generate_tokens\" method\n* Fix iomp5 linking with latest Intel OpenAPI on Ubuntu\n* Fixed \"decoder_start_token_id\" for T5\n\n## [v3.21.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.21.0) (2023-11-09)\n\n### New features\n\n* Minimal Support for Mistral (Loader and Rotary extension for long sequence). No sliding yet\n* Support Distil-Whisper\n* Support Whisper-large-v3\n\n## [v3.20.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.20.0) (2023-09-18)\n\n### New features\n\n* Update the Transformers converter to support more model architectures:\n  * MixFormerSequential (used by microsoft/phi-1_5)\n* Accept batch inputs in method `generate_tokens`\n* Add method `async_generate_tokens` to return an asynchronous generator compatible with `asyncio`\n\n### Fixes and improvements\n\n* Remove the epsilon value in the softmax CPU kernel for consistency with other implementations\n* Optimize implementation of the Dynamic Time Wrapping (DTW) function (used for Whisper alignment)\n* Avoid an unnecessary copy of the input arguments in method `Whisper::align`\n\n## [v3.19.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.19.0) (2023-08-31)\n\n### Changes\n\n* Binary wheels for Python 3.7 are no longer built\n\n### New features\n\n* Build wheels for Python 3.12\n* Update the Transformers converter to support more model architectures:\n  * Falcon-RW\n  * DistilBERT\n  * Llama with linear RoPE scaling (e.g. Vicuna v1.5)\n  * Llama with a non default RoPE base period (e.g. CodeLlama)\n* Accept the token type IDs as inputs for encoder models\n* Add property `GenerationStepResult.hypothesis_id` to identify the different hypotheses when running random sampling with `num_hypotheses` > 1\n\n### Fixes and improvements\n\n* Improve performance of 8-bit models on CPU:\n  * Vectorize the GEMM output dequantization\n  * Fuse the GEMM output dequantization with bias and activation\n* Allow inputs shorter than 30 seconds in Whisper methods\n* Fix incorrect `batch_id` values passed to the callback function\n* Fix a shape error in models using both MQA and relative positions\n* Fix compilation error related to AVX512 when using GCC 7\n* Call `.detach()` on PyTorch tensors before getting the Numpy array in converters\n\n## [v3.18.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.18.0) (2023-08-03)\n\n### Changes\n\nConverted models now uses the same floating point precision as the original models. For example, a model saved in float16 will be converted to a float16 model. Before this change, the weights were casted to float32 by default.\n\nSimilarly, selecting int8 keeps non quantized weights in their original precision unless a more specific quantization type is selected:\n\n* int8_float32\n* int8_float16\n* int8_bfloat16\n\n### New features\n\n* Add property `compute_type` to model instances\n* Extend the Python class `StorageView` with additional methods and properties:\n  * `to(dtype)`\n  * `device_index`\n  * `device`\n  * `dtype`\n  * `shape`\n\n### Fixes and improvements\n\n* Update the function `get_supported_compute_types` to correctly return bfloat16 when supported\n* Update the HF Llama converter to accept extra tokens in the vocabulary\n* Fix a shape error when enabling `return_alternatives` with a model using relative positions\n* Fix a conversion error when using `torch<1.13`\n* Fix a type error when running Whisper models with the bfloat16 type\n* Update pybind11 to 2.11.1\n\n## [v3.17.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.17.1) (2023-07-20)\n\n### Fixes and improvements\n\n* Fix an error when running models with the new `int8_bfloat16` computation type\n* Fix a vocabulary error when converting Llama 2 models with the Transformers converter\n* Update the Transformers converter to correctly convert Llama models using GQA\n* Stop the decoding when the generator returned by the method `generate_tokens` is closed\n\n## [v3.17.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.17.0) (2023-07-18)\n\n### New features\n\n* Add new computation types: `bfloat16` and `int8_bfloat16` (require a GPU with Compute Capability 8.0 or above)\n* Support multi-query attention for encoder-decoder models\n* Allow converters to register weights as PyTorch tensors instead of Numpy arrays\n\n### Fixes and improvements\n\n* Pass the flag `trust_remote_code` when loading the tokenizer in the Transformers converter\n* Improve performance of T5 models by reusing the same relative position bias in every layers\n* Whisper: disable the first timestamp decoding rule when a prefix is used\n* Install the CMake configuration in the correct library directory (e.g. some platforms use `lib64` instead of `lib`)\n\n## [v3.16.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.16.1) (2023-07-03)\n\n### Fixes and improvements\n\n* Fix repeated outputs in version 3.16.0 when using `include_prompt_in_result=False` and a batch input with variable lengths: a typo in the code led to `min_length` being incorrectly applied\n* Update the Transformers converter to accept extra tokens for Falcon models\n* Release the Python GIL when loading the model\n* Initialize the rotary embeddings on the GPU instead of the CPU\n* Avoid a copy for the input features passed to the Whisper methods\n* Vectorize copy in the Tile CUDA operator\n\n## [v3.16.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.16.0) (2023-06-15)\n\n### New features\n\n* Update the Transformers converter to support more architectures:\n  * Falcon-40B\n  * XLM-RoBERTa\n* Add the generation option `sampling_topp` to enable top-p (nucleus) sampling\n* Save vocabulary files in the JSON format to better support tokens containing newlines or carriage returns\n\n### Fixes and improvements\n\n* Fix the application of `min_length` and `max_length` when using `include_prompt_in_result=False` and a batch input with variable lengths: the length constraint should only apply to the sequence after the prompt\n* Update oneDNN to 3.1.1\n\n## [v3.15.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.15.1) (2023-06-09)\n\n### Fixes and improvements\n\n* Fix an error when using the new `static_prompt` argument in the methods `generate_tokens` and `generate_batch`\n* Improve the performance of models using ALiBi\n\n## [v3.15.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.15.0) (2023-06-06)\n\n### New features\n\n* Initial support of encoder-only Transformer model via a new class `ctranslate2.Encoder`\n* Update the Transformers converter to support the Falcon models\n* Add a generation argument `static_prompt` to optimize the execution for models using system prompts: the model state for this prompt is cached and reused in future calls\n* Support early stopping in greedy search when the callback function returns `True`\n* Make the layer norm epsilon value configurable in the model configuration file `config.json`\n* Add Tanh as a possible activation function\n\n### Fixes and improvements\n\n* Fix a performance issue when running models using ALiBi on the GPU\n* Fix application of the rotary embeddings when the multi-query attention is used\n* Fix conversion of Marian models using `tied-embeddings-all: false`\n* Remove `use_fast` argument when loading Hugging Face tokenizers to use the default tokenizer for the model\n\n## [v3.14.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.14.0) (2023-05-26)\n\n### New features\n\n* Update the Transformers converter with new architectures:\n  * CodeGen\n  * GPTBigCode\n  * LLaMa\n  * MPT\n* Update the OpenNMT-py converter to support some recent options:\n  * `layer_norm=\"rms\"`\n  * `max_relative_positions=-1` (rotary embeddings)\n  * `max_relative_positions=-2` (ALiBi)\n  * `pos_ffn_activation_fn=\"silu\"`\n* Update the OpenNMT-tf converter to support models using different configurations for the encoder and decoder (e.g. post-norm in the encoder and pre-norm in the decoder)\n* Implement the multi-query attention (used by GPTBigCode)\n\n### Fixes and improvements\n\n* Support paths containing Unicode characters on Windows\n* Fix the `generate_tokens` method to properly raise the underlying exception instead of hanging indefinitely\n* Fix compilation error when using `-DBUILD_SHARED_LIBS=OFF`\n* Fix runtime errors when linking against `libctranslate2.a` without using the \"whole archive\" flags\n\n## [v3.13.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.13.0) (2023-04-25)\n\n### New features\n\n* Support conversion of GPT-NeoX models with the Transformers converter\n* Extend the `end_token` argument to also accept a list of tokens\n* Add option `return_end_token` to include the end token in the results of the methods `generate_batch` and `translate_batch` (by default the end token is removed)\n* Expose the `callback` argument for the methods `generate_batch` and `translate_batch` to get early results from the decoding loop\n* Fallback to a custom threading implementation when OpenMP is not used (which is currently the case for the macOS ARM64 Python wheels)\n* Define the CMake package `CTranslate2::ctranslate2` to facilitate the library integration in other CMake projects\n\n### Fixes and improvements\n\n* Fix the vocabulary loading when some tokens end with the carriage return\n* Implement a fused kernel to apply the rotary embeddings\n* Update the Ruy library to commit 363f2522\n\n## [v3.12.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.12.0) (2023-04-17)\n\n### New features\n\n* Add methods `Generator.generate_tokens` and `Translator.generate_tokens` returning a generator that yields tokens as soon as they are generated by the model (not compatible with beam search)\n* Improve performance of rotary embeddings on CPU with an alternative implementation that is enabled when setting `rotary_interleave=False` in the model specification (may require to permute QK weights)\n* Support a variable number of input frames in method `Whisper.align` to improve batch support\n* Expose flag `low_cpu_mem_usage` in the Transformers converter to reduce the memory usage when loading large models (requires the package `accelerate`)\n\n### Fixes and improvements\n\n* Fix crash in `Whisper.align` when `num_frames // 2 <= median_filter_width`\n* Raise an error if arguments `end_token` or `suppress_sequences` contain tokens that are not in the vocabulary\n* Optimize the quantization of FP16 weights during the model conversion\n* In the Transformers converter, also load the model weights in FP16 when the selected quantization is `int8_float16`\n* Update the Whisper timestamp decoding rules to prevent the generation of segments with zero duration\n\n## [v3.11.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.11.0) (2023-04-06)\n\n### Changes\n\n* The Python wheels for macOS ARM are now built with the Ruy backend to support INT8 computation. This will change the performance and results when loading an INT8 model and/or using the `auto` compute type. To keep the previous behavior, set `compute_type=\"float32\"`.\n\n### New features\n\n* Support conversion of the GPT-J architecture\n* Support conversion of models using rotary position embeddings\n* Apply the new OpenNMT-py option `decoder_start_token`\n* Add option `revision` in the Transformers converter to download a specific revision of the model from the Hugging Face Hub\n\n## [v3.10.3](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.10.3) (2023-03-28)\n\n### Fixes and improvements\n\n* Fix a synchronization issue when the model input is a CUDA storage\n\n## [v3.10.2](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.10.2) (2023-03-27)\n\n### Fixes and improvements\n\n* Select the correct device when copying a `StorageView` instance\n\n## [v3.10.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.10.1) (2023-03-25)\n\n### Fixes and improvements\n\n* Add missing device setter in `Whisper.encode`\n\n## [v3.10.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.10.0) (2023-03-24)\n\n### New features\n\n* Add `Generator` option `include_prompt_in_result` (`True` by default)\n* Add method `Whisper.encode` to only run the Whisper encoder\n* Add model properties `Whisper.device` and `Whisper.device_index`\n\n### Fixes and improvements\n\n* Update the methods `Whisper.detect_language`, `Whisper.generate`, and `Whisper.align` to accept the encoder output\n* Fix a crash when running `Generator.forward` on GPU and the generator object is destroyed before the forward output\n* Fix parsing of Marian YAML vocabulary files containing \"complex key mappings\" and escaped sequences such as \"\\x84\"\n\n## [v3.9.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.9.1) (2023-03-17)\n\n### Fixes and improvements\n\n* Fix missing alignments in the `Whisper.align` result due to a bug in the DTW implementation\n* Fix error when converting a Whisper model from a path\n\n## [v3.9.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.9.0) (2023-03-15)\n\n### New features\n\n* Support BLOOM language models\n* Add method `Whisper.align` to return the text/audio alignment and implement word-level timestamps\n\n### Fixes and improvements\n\n* Do not force `intra_threads` to 1 when loading a model on the GPU as some ops may still run on the CPU\n* Disable multithreading when copying a batch of small arrays\n\n## [v3.8.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.8.0) (2023-03-06)\n\n### New features\n\n* Experimental support of AVX512 in manually vectorized functions: this code path is not enabled by default but can be enabled by setting the environment variable `CT2_FORCE_CPU_ISA=AVX512`\n* Add Transformers converter option `copy_files` to copy any files from the Hugging Face model to the converted model directory\n* Expose some Whisper parameters:\n  * `max_initial_timestamp_index`\n  * `suppress_blank`\n  * `suppress_tokens`\n\n### Fixes and improvements\n\n* Reduce conversion time for large models by skipping some weights comparisons\n* Reduce maximum memory usage when converting Transformers models with `--quantization float16`\n* Set FP32 compute type for FP16 convolutions to match the PyTorch behavior and accuracy\n* Update oneDNN to 3.0.1\n\n## [v3.7.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.7.0) (2023-02-23)\n\n### Changes\n\n* Rename the \"float\" compute type to \"float32\" for clarity. \"float\" is still accepted for backward compatibility.\n\n### New features\n\n* Add the environment variable `CT2_CUDA_TRUE_FP16_GEMM`. This flag is enabled by default so that FP16 GEMMs are running in full FP16. When disabled, the compute type of FP16 GEMMs is set to FP32, which is what PyTorch and TensorFlow do by default.\n\n### Fixes and improvements\n\n* Improve the numerical precision of Whisper models running in FP16 by setting the FP32 compute type for GEMMs (same behavior as PyTorch)\n* Improve support for running the Whisper models with INT16 quantization\n* Ensure the Whisper decoding does not continue past `max_length`, which could previously happen when the prompt was longer than `max_length/2`\n* Include the EOS score in the score returned by Whisper during greedy search\n\n## [v3.6.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.6.0) (2023-02-16)\n\n### New features\n\n* Build the Windows Python wheels with cuDNN to enable GPU execution of Whisper models\n* Add the model attribute `Whisper.is_multilingual`\n\n### Fixes and improvements\n\n* Reduce the beam search memory usage by not duplicating the decoder states that are the same in each beam (e.g. the projected memory keys and values)\n* Optimize the dot product attention during beam search by moving the query beam dimension to the time dimension\n* Fix support of English-only Whisper models\n* Include the prefix tokens (if they exist) in the output of `Whisper.generate`\n* Log a warning when the model weights are implicitly converted to another type\n\n## [v3.5.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.5.1) (2023-02-13)\n\n### Fixes and improvements\n\n* Whisper: fix an incorrect timestamp rule that prevented timestamps to be generated in pairs\n* Whisper: ignore the EOS token when applying the length penalty to match the original implementation\n\n## [v3.5.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.5.0) (2023-02-10)\n\n### New features\n\n* Add a patience factor for beam search to continue decoding until `beam_size * patience` hypotheses are finished, as described in [Kasai et al. 2022](https://arxiv.org/abs/2204.05424)\n* Implement all GELU variants and select them accordingly when converting models:\n  * Tanh approximation (already implemented)\n  * Sigmoid approximation\n  * Reference implementation based on the CDF\n\n### Fixes and improvements\n\n* Fix incorrect outputs of T5 models due to a bug in the CUDA kernel of the RMS normalization\n* Raise an error if the Whisper input shape is incorrect\n* Optimize the transposition operator used in the multi-head attention when running on GPU\n* Remove the upper limit in `python_requires` to facilitate the package installation with tools like Poetry and PDM\n\n## [v3.4.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.4.0) (2023-02-03)\n\n### Fixes and improvements\n\n* Fix incorrect vocabulary in M2M100 models after conversion with `transformers>=4.24`\n* Fix incorrect model outputs when executing with very large batch sizes on GPU\n* Fix memory error in biased decoding: the vector of divergence was read and updated past its length\n* Allow setting `prefix_bias_beta` > 0 with `beam_size` == 1\n* Prevent timestamps from decreasing during Whisper generation\n* Make some error messages more helpful when implementing a custom converter\n\n## [v3.3.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.3.0) (2023-01-02)\n\n### New features\n\n* Support T5 models, including the variants T5v1.1 and mT5\n* Support loading the model files from memory:\n  * Python: see the `files` argument in the constructor of classes loading models\n  * C++: see the `models::ModelMemoryReader` class\n\n### Fixes and improvements\n\n* Improve the quantization accuracy of OPT models by applying the [SmoothQuant](https://github.com/mit-han-lab/smoothquant) technique during conversion (pre-computed activation scales should be passed to the converter option `--activation_scales`)\n* Fix conversion of BART-like models from HuggingFace that are using a different number of encoder and decoder layers\n* Fix compilation when no BLAS CPU backend is selected\n* Remove no longer relevant CMake warning when the project is compiled without oneDNN\n* Update oneDNN to 3.0\n* Update oneMKL to 2023.0\n\n## [v3.2.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.2.0) (2022-12-12)\n\n### New features\n\n* Add decoding option `suppress_sequences` to prevent specific sequences of tokens from being generated\n* Add decoding option `end_token` to stop the decoding on a different token than the model EOS token\n* Allow returning multiple random hypotheses from greedy search + random sampling when setting `num_hypotheses` > 1\n\n### Fixes and improvements\n\n* Improve support for batch generation with the Whisper model:\n  * Improve performance of batch generation with a context (we only require the prompts to have the same length, which is easily done by adapting the number of previous text tokens)\n  * Support batch mode for option `return_no_speech_prob`\n  * Support cases where some prompts in the batch have the token `<|notimestamps|>` but not others\n* Enable the Conv1D layer in more Python wheels:\n  * macOS x64 (using oneDNN)\n  * macOS ARM64 (using a custom implementation)\n  * Linux AArch64 (using a custom implementation)\n* Update the OpenNMT-py converter to support the latest checkpoint structure\n* Generalize the `TransformerSpec` constructor to accept arbitrary encoder and decoder specifications\n* Remove the global compilation flag `-ffast-math` which introduces unwanted side effects and enable it only for the layer norm CPU kernel where it is actually useful\n* Fix CMake error on Windows when setting `-DOPENMP_RUNTIME=COMP`\n\n## [v3.1.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.1.0) (2022-11-29)\n\n### Changes\n\n* The input prompt is no longer included in the result of `Whisper.generate` as it is usually not useful in a transcription loop\n* The default beam size in `Whisper.generate` is updated from 1 to 5 to match the default value in [openai/whisper](https://github.com/openai/whisper)\n* Generation options `min_length` and `no_repeat_ngram_size` now penalize the logits instead of the log probs which may change some scores\n* Raise a deprecation warning when reading the `TranslationResult` object as a list of dictionaries\n\n### New features\n\n* Allow configuring the C++ logs from Python with the function `ctranslate2.set_log_level`\n* Implement the timestamp decoding rules when the Whisper prompt does not include the token `<|notimestamps|>`\n* Add option `return_no_speech_prob` to the method `Whisper.generate` for the result to include the probability of the no speech token\n\n### Fixes and improvements\n\n* Improve performance of the Whisper model when generating with a context\n* Fix timestamp tokens in the Whisper vocabulary to use the correct format (`<|X.XX|>`)\n* Fix AVX and NEON log functions to return -inf on log(0) instead of NaN\n* When info logs are enabled, log the system configuration only when the first model is loaded and not immediately when the library is loaded\n* Define a `LogitsProcessor` abstract class to apply arbitrary updates to the logits during decoding\n* Update oneDNN to 2.7.2\n\n## [v3.0.2](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.0.2) (2022-11-14)\n\n### Fixes and improvements\n\n* Whisper: fix `generate` arguments that were not correctly passed to the model\n\n## [v3.0.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.0.1) (2022-11-10)\n\n### Fixes and improvements\n\n* Whisper: do not implicitly add `<|startoftranscript|>` in `generate` since it is not always the first token\n\n## [v3.0.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v3.0.0) (2022-11-07)\n\nThis major version integrates the Whisper speech recognition model published by OpenAI. It also introduces some breaking changes to remove deprecated usages and simplify some modules.\n\n### Breaking changes\n\n#### General\n\n* Remove option `normalize_scores`: the scores are now always divided by `pow(length, length_penalty)` with `length_penalty` defaulting to 1\n* Remove option `allow_early_exit`: the beam search now exits early only when no penalties are used\n\n#### Python\n\n* Rename some classes:\n  * `OpenNMTTFConverterV2` -> `OpenNMTTFConverter`\n  * `TranslationStats` -> `ExecutionStats`\n* Remove compatibility for reading `ScoringResult` as a list of scores: the scores can be accessed with the attribute `log_probs`\n* Remove compatibility for reading `ExecutionStats` as a tuple\n* Remove support for deprecated Python version 3.6\n\n#### CLI\n\n* Rename the client executable `translate` to a more specific name `ct2-translator`\n\n#### C++\n\n* Rename or remove some classes and methods:\n  * `TranslationStats` -> `ExecutionStats`\n  * `GeneratorPool` -> `Generator`\n  * `TranslatorPool` -> `Translator`\n  * `TranslatorPool::consume_*` -> `Translator::translate_*`\n  * `TranslatorPool::consume_stream` -> removed\n  * `TranslatorPool::score_stream` -> removed\n* Remove support for building with CUDA 10\n\n### New features\n\n* Integrate the Whisper speech recognition model published by OpenAI\n* Support conversion of models trained with OpenNMT-py V3\n* Add method `Generator.forward_batch` to get the full model output for a batch of sequences\n* Add Python class `StorageView` to expose C++ methods taking or returning N-dimensional arrays: the class implements the array interface for interoperability with Numpy and PyTorch\n* Add a new configuration file `config.json` in the model directory that contains non structual model parameters (e.g. related to the input, the vocabulary, etc.)\n* Implement the Conv1D layer and operator on CPU and GPU (using oneDNN and cuDNN respectively)\n* [C++] Allow registration of external models with `models::ModelFactory`\n\n### Fixes and improvements\n\n* Fix conversion of models that use biases only for some QKV projections but not for all\n* Fuse masking of the output log probs by aggregating disabled tokens from all related options: `disable_unk`, `min_length`, `no_repeat_ngram_size`, etc.\n* Reduce the layer norm epsilon value on GPU to 1e-5 to match the default value in PyTorch\n* Move some Transformer model attributes under the encoder/decoder scopes to simplify loading\n* Redesign the `ReplicaPool` base class to simplify adding new classes with multiple model workers\n* Compile the library with C++17\n* Update oneDNN to 2.7.1\n* Update oneMKL to 2022.2\n* Update pybind11 to 2.10.1\n* Update cibuildwheel to 2.11.2\n\n## [v2.24.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.24.0) (2022-10-03)\n\n### Changes\n\n* The Linux binaries now use the GNU OpenMP runtime instead of Intel OpenMP to workaround an initialization error on systems without `/dev/shm`\n\n### Fixes and improvements\n\n* Fix a memory error when running random sampling on GPU\n* Optimize the model loading on multiple GPUs by copying the finalized model weights instead of reading the model from disk multiple times\n* In the methods `Translator.translate_iterable` and `Translator.score_iterable`, raise an error if the input iterables don't have the same length\n* Fix some compilation warnings\n\n## [v2.23.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.23.0) (2022-09-16)\n\n### New features\n\n* Build wheels for Python 3.11\n\n### Fixes and improvements\n\n* In beam search, get more candidates from the model output and replace finished hypotheses by these additional candidates\n* Fix possibly incorrect attention vectors returned from the beam search\n* Fix coverage penalty that was actually not applied\n* Fix crash when the beam size is larger than the vocabulary size\n* Add missing compilation flag `-fvisibility=hidden` when building the Python module\n* Update oneDNN to 2.6.2\n* Update OpenBLAS to 0.3.21\n\n## [v2.22.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.22.0) (2022-09-02)\n\n### Changes\n\n* `score_batch` methods now return a list of `ScoringResult` instances instead of plain lists of probabilities. In most cases you should not need to update your code: the result object implements the methods `__len__`, `__iter__`, and `__getitem__` so that it can still be used as a list.\n\n### New features\n\n* Add methods to efficiently process long iterables:\n  * `Translator.translate_iterable`\n  * `Translator.score_iterable`\n  * `Generator.generate_iterable`\n  * `Generator.score_iterable`\n* Add decoding option `min_alternative_expansion_prob` to filter out unlikely alternatives in `return_alternatives` mode\n* Return `ScoringResult` instances from `score_batch` to include additional outputs. The current attributes are:\n  * `tokens`: the list of tokens that were actually scored (including special tokens)\n  * `log_probs`: the log probability of each scored token\n* Support running `score_batch` asynchronously by setting the `asynchronous` flag\n\n### Fixes and improvements\n\n* Fix possibly incorrect results when using `disable_unk` or `use_vmap` with one of the following options:\n  * `min_decoding_length`\n  * `no_repeat_ngram_size`\n  * `prefix_bias_beta`\n  * `repetition_penalty`\n* Also pad the output layer during scoring to enable Tensor Cores\n* Improve the correctness of the model output probabilities when the output layer is padded\n* Skip translation when the NLLB input is empty (i.e. when the input only contains EOS and the language token)\n\n## [v2.21.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.21.1) (2022-07-29)\n\n### Fixes and improvements\n\n* Fix conversion of NLLB models when `tokenizer_class` is missing from the configuration\n\n## [v2.21.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.21.0) (2022-07-27)\n\n### New features\n\n* Support NLLB multilingual models via the Transformers converter\n* Support Pegasus summarization models via the Transformers converter\n\n### Fixes and improvements\n\n* Do not stop decoding when the EOS token is coming from the user input: this is required by some text generation models like `microsoft/DialoGPT` where EOS is used as a separator\n* Fix conversion error for language models trained with OpenNMT-py\n* Fix conversion of models that are not using bias terms in the multi-head attention\n* Fix data type error when enabling the translation options `return_alternatives` and `return_attention` with a `float16` model\n* Improve CPU performance of language models quantized to `int8`\n* Implement a new vectorized GELU operator on CPU\n* Raise a more explicit error when trying to convert a unsupported Fairseq model\n* Update pybind11 to 2.10.0\n\n## [v2.20.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.20.0) (2022-07-06)\n\n### New features\n\n* Generation option `no_repeat_ngram_size` to prevent the repetitions of N-grams with a minimum size\n\n### Fixes and improvements\n\n* Fix conversion of OpenNMT-tf models that use static position embeddings\n* Fix a segmentation fault in `return_alternatives` mode when the target prefix is longer than `max_decoding_length`\n* Fix inconsistent state of asynchronous results in Python when a runtime exception is raised\n* Remove `<pad>` token when converting MarianMT models from Transformers: this token is only used to start the decoder from a zero embedding, but it is not included in the original Marian model\n* Optimize CPU kernels with vectorized reduction of accumulated values\n* Do not modify the configuration passed to `OpenNMTTFConverterV2.from_config`\n* Improve Python classes documentation by listing members at the top\n\n## [v2.19.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.19.1) (2022-06-23)\n\n### Fixes and improvements\n\n* Fix missing final bias in some MarianMT models converted from Transformers\n* Fix missing final layer normalization in OPT models converted from Transformers\n* Fix error when converting OpenNMT-tf V1 checkpoints with the new OpenNMT-tf converter\n* Reduce model conversion memory usage when the loaded weights are in FP16 and the model is converted with quantization\n* Add missing C++ type `ctranslate2::float16_t` in the public headers that is required to use some functions\n* Fix some Python typing annotations\n\n## [v2.19.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.19.0) (2022-06-08)\n\n### New features\n\n* Support conversion of decoder-only Transformer models trained with OpenNMT-tf\n\n### Fixes and improvements\n\n* Fix conversion error for Transformers' model `facebook/bart-large-cnn`\n* Fix crash when scoring empty sequences\n* Apply `max_input_length` after all special tokens have been added to the input\n* Clear the GPU memory cache when no new batches are immediately available for execution\n* Improve functions signature in the generated Python API documentation\n* Update oneDNN to 2.6\n* Update spdlog to 1.10.0\n* Update OpenBLAS to 0.3.20\n\n## [v2.18.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.18.0) (2022-05-23)\n\n### New features\n\n* Support Meta's OPT models via the Transformers converter\n* Extend the Fairseq converter to support `transformer_lm` models\n\n### Fixes and improvements\n\n* Fix conversion error for Marian's pre-norm Transformer models\n* Fix conversion error for Transformers' MarianMT models that are missing some configuration fields\n* Improve conversion speed of Marian models (optimize the generation of the sinusoidal position encodings)\n\n## [v2.17.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.17.0) (2022-05-09)\n\n### New features\n\n* Add a converter for Hugging Face's [Transformers](https://github.com/huggingface/transformers). The following models are currently supported:\n  * BART\n  * M2M100\n  * MarianMT\n  * MBART\n  * OpenAI GPT2\n* Revisit the OpenNMT-tf converter to better support custom models and configurations:\n  * Extend the conversion script to accept the training configuration\n  * Add a new converter class `ctranslate2.converters.OpenNMTTFConverterV2`\n* Move all documentation and guides to the [website](https://opennmt.net/CTranslate2) to improve navigation and clarity\n\n### Fixes and improvements\n\n* In text generation, include the start token in the output if it is not the BOS token\n\n## [v2.16.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.16.0) (2022-04-28)\n\n### New features\n\n* Initial support of language models:\n  * Add a high-level class `ctranslate2.Generator` to generate text with language models\n  * Add a converter for OpenAI GPT-2 models\n  * Update the OpenNMT-py converter to support `transformer_lm` decoders\n* Build ARM64 wheels for macOS\n* Allow loading custom Fairseq extensions and architectures during conversion with the option `--user_dir`\n* Enable conversion of the Fairseq architectures `multilingual_transformer` and `multilingual_transformer_iwslt_de_en`\n* Implement random sampling in beam search using the Gumbel-max trick\n* Generate and publish the Python API reference to https://opennmt.net/CTranslate2\n\n### Fixes and improvements\n\n* Fix model loading on a GPU with index > 0\n* Fix memory error when running random sampling on GPU with certain batch sizes\n* Fix incorrect tokens order in some converted Marian vocabularies\n* Properly count the number of layers before building the encoder/decoder instead of relying on runtime exceptions\n\n## [v2.15.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.15.1) (2022-04-04)\n\n### Fixes and improvements\n\n* Fix missing deactivation of OpenMP threading in GPU execution (regression introduced in version 2.15.0)\n\n## [v2.15.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.15.0) (2022-04-04)\n\n### New features\n\n* Expose translator option `max_queued_batches` to configure the maximum number of queued batches (when the queue is full, future requests will block until a free slot is available)\n* Allow converters to customize the vocabulary special tokens `<unk>`, `<s>`, and `</s>`\n\n### Fixes and improvements\n\n* Fix compatibility of models converted on Windows with other platforms by saving the vocabulary files with the newline character \"\\n\" instead of \"\\r\\n\"\n* Clarify conversion error when no TensorFlow checkpoints are found in the configured model directory\n* Enable fused QKV transposition by switching the heads and time dimensions before the QKV split\n* Cache the prepared source lengths mask in the Transformer decoder state and reuse it in the next decoding steps\n* Pad the output layer to enable Tensor Cores only once instead of updating the layer on each batch\n* Vectorize copy in Concat and Split ops on GPU\n* Factorize all OpenMP parallel for loops to call the `parallel_for` function\n* Compile CUDA kernels for deprecated Compute Capabilities that are not yet dropped by CUDA:\n  * CUDA 11: 3.5 and 5.0\n  * CUDA 10: 3.0\n\n## [v2.14.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.14.0) (2022-03-16)\n\n### New features\n\n* Include BART and MBART in the list of supported Fairseq architectures\n* Add Fairseq converter option `--no_default_special_tokens` to require all special tokens to be set by the user during inference, including the decoder start tokens (for example, this is required by MBART-25 to properly set the language tokens)\n\n### Fixes and improvements\n\n* Fix conversion of Post-Norm Transformers trained with OpenNMT-tf\n* Fix scoring with Fairseq models that used an incorrect decoder start token (Fairseq uses `</s>` as the decoder start token, not `<s>`)\n* Fix scoring result to include the end of sentence token\n* Ignore OpenNMT-py options `--alignment_layer` and `--alignment_heads` for models that are not trained with alignments\n* Enable batch encoding in `return_alternatives` translation mode (the decoding still runs sequentially)\n* Make enumerations `ctranslate2.specs.Activation` and `ctranslate2.specs.EmbeddingsMerge` public since they could be used to configure the Transformer specification\n* Update oneDNN to 2.5.3\n* Update cpu_features to 0.7.0\n* Update cxxopts to 3.0.0\n* Update spdlog to 1.9.2\n\n## [v2.13.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.13.1) (2022-03-02)\n\n### Fixes and improvements\n\n* Fix conversion error for old OpenNMT-py models that do not have the option `self_attn_type`\n\n## [v2.13.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.13.0) (2022-02-28)\n\n### New features\n\n* Add converter for [Marian](https://github.com/marian-nmt/marian) and support the collection of [OPUS-MT pretrained models](https://github.com/Helsinki-NLP/Opus-MT-train/tree/master/models)\n* Support models applying a layer normalization after the embedding layer (cf. option `--layernorm-embedding` in Fairseq)\n* Support models using the Swish (a.k.a SiLU) activation function\n* Support models using custom decoder start tokens, which can be passed in the target prefix\n\n### Fixes and improvements\n\n* Remove unexcepted call to a CUDA function in CPU execution when unloading models\n* Add option groups in the translation client help output\n* Use new `thrust::cuda::par_nosync` execution policy when calling Thrust functions\n* Update Thrust to 1.16.0\n* Update pybind11 to 2.9.1\n\n## [v2.12.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.12.0) (2022-02-01)\n\n### New features\n\n* Support models using additional source features (a.k.a. factors)\n\n### Fixes and improvements\n\n* Fix compilation with CUDA < 11.2\n* Fix incorrect revision number reported in the error message for unsupported model revisions\n* Improve quantization correctness by rounding the value instead of truncating (this change will only apply to newly converted models)\n* Improve default value of `intra_threads` when the system has less than 4 logical cores\n* Update oneDNN to 2.5.2\n\n## [v2.11.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.11.0) (2022-01-11)\n\n### Changes\n\n* With CUDA >= 11.2, the environment variable `CT2_CUDA_ALLOCATOR` now defaults to `cuda_malloc_async` which should improve performance on GPU.\n\n### New features\n\n* Build Python wheels for AArch64 Linux\n\n### Fixes and improvements\n\n* Improve performance of Gather CUDA kernel by using vectorized copy\n* Update Intel oneAPI to 2022.1\n* Update oneDNN to 2.5.1\n* Log some additional information with `CT2_VERBOSE` >= 1:\n  * Location and compute type of loaded models\n  * Version of the dynamically loaded cuBLAS library\n  * Selected CUDA memory allocator\n\n## [v2.10.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.10.1) (2021-12-15)\n\n### Fixes and improvements\n\n* Fix stuck execution when loading a model on a second GPU\n* Fix numerical error in INT8 quantization on macOS\n\n## [v2.10.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.10.0) (2021-12-13)\n\n### Changes\n\n* `inter_threads` now also applies to GPU translation, where each translation thread is using a different CUDA stream to allow some parts of the GPU execution to overlap\n\n### New features\n\n* Add option `disable_unk` to disable the generation of unknown tokens\n* Add function `set_random_seed` to fix the seed in random sampling\n* [C++] Add constructors in `Translator` and `TranslatorPool` classes with `ModelReader` parameter\n\n### Fixes and improvements\n\n* Fix incorrect output from the Multinomial op when running on GPU with a small batch size\n* Fix Thrust and CUB headers that were included from the CUDA installation instead of the submodule\n* Fix static library compilation with the default build options (`cmake -DBUILD_SHARED_LIBS=OFF`)\n* Compile the Docker image and the Linux Python wheels with SSE 4.1 (vectorized kernels are still compiled for AVX and AVX2 with automatic dispatch, but other source files are now compiled with SSE 4.1)\n* Enable `/fp:fast` for MSVC to mirror `-ffast-math` that is enabled for GCC and Clang\n* Statically link against oneDNN to reduce the size of published binaries:\n  * Linux Python wheels: 43MB -> 17MB\n  * Windows Python wheels: 41MB -> 11MB\n  * Docker image: 733MB -> 600MB\n\n## [v2.9.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.9.0) (2021-12-01)\n\n### New features\n\n* Add GPU support to the Windows Python wheels\n* Support OpenNMT-py and Fairseq options `--alignment_layer` and `--alignment_heads` which specify how the multi-head attention is reduced and returned by the Transformer decoder\n* Support dynamic loading of CUDA libraries on Windows\n\n### Fixes and improvements\n\n* Fix division by zero when normalizing the score of an empty target\n* Fix error that was not raised when the input length is greater than the number of position encodings\n* Improve performance of random sampling on GPU for large values of `sampling_topk` or when sampling over the full vocabulary\n* Include `transformer_align` and `transformer_wmt_en_de_big_align` in the list of supported Fairseq architectures\n* Add a CUDA kernel to prepare the length mask to avoid moving back to the CPU\n\n## [v2.8.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.8.1) (2021-11-17)\n\n### Fixes and improvements\n\n* Fix dtype error when reading float16 scores in greedy search\n* Fix usage of MSVC linker option `/nodefaultlib` that was not correctly passed to the linker\n\n## [v2.8.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.8.0) (2021-11-15)\n\n### Changes\n\n* The Linux Python wheels now use Intel OpenMP instead of GNU OpenMP for consistency with other published binaries\n\n### New features\n\n* Build Python wheels for Windows\n\n### Fixes and improvements\n\n* Fix segmentation fault when calling `Translator.unload_model` while an asynchronous translation is running\n* Fix implementation of repetition penalty that should be applied to all previously generated tokens and not just the tokens of the last step\n* Fix missing application of repetition penalty in greedy search\n* Fix incorrect token index when using a target prefix and a vocabulary mapping file\n* Set the OpenMP flag when compiling on Windows with `-DOPENMP_RUNTIME=INTEL` or `-DOPENMP_RUNTIME=COMP`\n\n## [v2.7.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.7.0) (2021-11-03)\n\n### Changes\n\n* Inputs are now truncated after 1024 tokens by default (see translation option `max_input_length`)\n\n### New features\n\n* Add translation option `max_input_length` to limit the model input length\n* Add translation option `repetition_penalty` to apply an exponential penalty on repeated sequences\n* Add scoring option `with_tokens_score` to also output token-level scores when scoring a file\n\n### Fixes and improvements\n\n* Adapt the length penalty formula when using `normalize_scores` to match other implementations: the scores are divided by `pow(length, length_penalty)`\n* Implement `LayerNorm` with a single CUDA kernel instead of 2\n* Simplify the beam search implementation\n\n## [v2.6.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.6.0) (2021-10-15)\n\n### New features\n\n* Build wheels for Python 3.10\n* Accept passing the vocabulary as a `opennmt.data.Vocab` object or a list of tokens in the OpenNMT-tf converter\n\n### Fixes and improvements\n\n* Fix segmentation fault in greedy search when `normalize_scores` is enabled but not `return_scores`\n* Fix segmentation fault when `min_decoding_length` and `max_decoding_length` are both set to 0\n* Fix segmentation fault when option `sampling_topk` is larger than the vocabulary size\n* Fix incorrect score normalization in greedy search when `max_decoding_length` is reached\n* Fix incorrect score normalization in the `return_alternatives` translation mode\n* Improve error checking when reading the binary model file\n* Apply `LogSoftMax` in-place during decoding and scoring\n\n## [v2.5.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.5.1) (2021-10-04)\n\n### Fixes and improvements\n\n* Fix logic error in the in-place implementation of the `Gather` op that could lead to incorrect beam search outputs\n\n## [v2.5.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.5.0) (2021-10-01)\n\n### New features\n\n* Add an 8-bit GEMM backend on AArch64 using [Ruy](https://github.com/google/ruy)\n\n### Fixes and improvements\n\n* Skip unnecessary transpositions of the projected decoder queries in the multi-head attention\n* Use 32-bit indexing in all CUDA kernels to slightly improve performance\n* Let the compiler auto-vectorize the `LayerNorm` CPU kernel\n* Update Intel oneAPI to 2021.4\n\n## [v2.4.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.4.0) (2021-09-10)\n\n### New features\n\n* [Python] Support asynchronous translation: `translate_batch` can return future-like objects with argument `asynchronous=True`\n* [Python] `translate_batch` now returns a list of `TranslationResult` objects instead of a list of dictionaries (this object can also be indexed as a list of dictionaries for backward compatibility)\n* Add options `--source_lang` and `--target_lang` to the Fairseq converter for models that do not include these information\n\n### Fixes and improvements\n\n* Fix Fairseq model conversion when the model options are stored in `model[\"cfg\"][\"model\"]`\n* Compile the CPU INT8 quantization kernel with FMA instructions\n* Enable packing of the last linear weight when not using dynamic vocabulary reduction\n* Replace the generic `Tile` implementation by dedicated CPU and CUDA kernels\n* [Python] Implement `__repr__` method for `TranslationStats` objects\n* [Python] Update pybind11 to 2.7.1\n\n## [v2.3.2](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.3.2) (2021-08-05)\n\n### Fixes and improvements\n\n* Fix GPU execution that gets stuck when applying the GELU activation\n\n## [v2.3.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.3.1) (2021-07-28)\n\n### Fixes and improvements\n\n* Fix compilation with CUDA < 10.2\n\n## [v2.3.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.3.0) (2021-07-26)\n\n### New features\n\n* Add compute type `int8_float16` for mixed INT8 and FP16 computation on GPU (requires Compute Capability >= 7.0)\n* Add methods `Translator.score_batch` and `Translator.score_file` to score existing translations\n\n### Fixes and improvements\n\n* Relax the GPU driver requirement for running the Docker image to >= 450.80.02 (same as the published Python package)\n\n## [v2.2.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.2.0) (2021-07-06)\n\n### New features\n\n* Add Python utility functions to query the system capabilities:\n  * `ctranslate2.get_cuda_device_count`\n  * `ctranslate2.get_supported_compute_types`\n* Add option `fixed_dictionary` in the Fairseq converter to support multilingual models\n* Extend environment variable `CT2_VERBOSE` to configure more log levels (see README)\n\n### Fixes and improvements\n\n* Fuse activation with bias addition on GPU for a small performance increase\n* Make the GELU activation compatible with FP16 execution\n* Improve the log format using the [spdlog](https://github.com/gabime/spdlog) library\n* Improve the accuracy of the profiling results on GPU\n* Update Intel oneAPI to 2021.3\n\n## [v2.1.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.1.0) (2021-06-14)\n\n### New features\n\n* Support conversion of Transformer models trained with [Fairseq](https://github.com/pytorch/fairseq/) (see script `ct2-fairseq-converter`)\n* Support conversion of models using GELU activations\n* Add translation option `normalize_scores` to return scores normalized by the hypotheses length: enabling this option can improve the beam search output for some models\n* Add translation option `allow_early_exit` to toggle the beam search early exit optimization: disabling this option has a small negative impact on performance, but it can improve the beam search output when using penalties or normalized scores\n* [C++] Add class `BufferedTranslationWrapper` to buffer and batch independent inputs to the same model\n\n### Fixes and improvements\n\n* Read value of environment variable `OMP_NUM_THREADS` when `intra_threads` is not set\n* Improve file translation performance by enabling local sorting by default\n* [Python] Improve error message when converting unsupported models and list all options that are unuspported\n* [Python] Return statistics of `Translator.translate_file` as an object with named properties\n* [C++] Fix compilation of method `TranlatorPool::consume_raw_text_file` that takes streams as inputs\n\n## [v2.0.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v2.0.0) (2021-06-03)\n\nThis major version introduces some breaking changes to simplify model conversion, improve the consistency of user options, and update the Python package to CUDA 11.x. It also comes with internal improvements to facilitate future changes.\n\n### Breaking changes\n\n#### General\n\n* Disable `return_scores` by default as most applications do not use translation scores\n* Replace all Docker images by a single one: `<version>-ubuntu20.04-cuda11.2`\n* Replace CMake option `LIB_ONLY` by `BUILD_CLI`\n* Require CMake version >= 3.15 for GPU compilation\n\n#### Python\n\n* For GPU execution, the Linux Python wheels published on PyPI now require CUDA 11.x to be installed on the system. The CUDA dependencies (e.g. cuBLAS) are no longer included in the package and are loaded dynamically.\n* Remove support for converting the TensorFlow SavedModel format (checkpoints should be converted instead)\n* Remove the `model_spec` option for converters that can automatically detect it from the checkpoints\n* Force translation options to be set with keyword arguments only (see the API reference)\n* Rename tokenization callables arguments in `translate_file` for clarity:\n  * `tokenize_fn` to `source_tokenize_fn`\n  * `detokenize_fn` to `target_detokenize_fn`\n\n#### CLI\n\n* Rename length contraints options for consistency with other APIs:\n  * `max_sent_length` to `max_decoding_length`\n  * `min_sent_length` to `min_decoding_length`\n\n#### C++\n\n* Move the `max_batch_size` and `batch_type` options from the `TranslationOptions` structure to the translation methods of `TranslatorPool`\n* Simplify the `TranslationResult` structure with public attributes instead of methods\n* Asynchronous translation API now returns one future per example instead of a single future for the batch\n\n### New features\n\n* Add translation option `prefix_bias_beta` to bias the decoding towards the target prefix (see [Arivazhagan et al. 2020](https://arxiv.org/abs/1912.03393))\n* Automatically detect the model specification when converting OpenNMT-py models\n* Support conversion and execution of Post-Norm Transformers\n* Add an experimental asynchronous memory allocator for CUDA 11.2 and above (can be enabled with the environment variable `CT2_CUDA_ALLOCATOR=cuda_malloc_async`)\n* Expose the Python package version in `ctranslate2.__version__`\n\n### Fixes and improvements\n\n* Fix silent activation of `replace_unknowns` when enabling `return_attention`\n* Improve support for the NVIDIA Ampere architecture in prebuilt binaries\n* Reduce the size of the Python wheels published on PyPI\n* Define a custom CUDA kernel for the GEMM output dequantization instead of a Thrust-based implementation\n* Update Thrust to 1.12.0\n\n## [v1.20.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.20.1) (2021-04-29)\n\n### Fixes and improvements\n\n* Do not return scores for empty outputs when `return_scores` is disabled\n* Do not include google/cpu\\_features library in CTranslate2 installation\n\n## [v1.20.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.20.0) (2021-04-20)\n\n### Changes\n\n* Drop Python 3.5 support\n* Docker image tags suffixed with `-gpu` are no longer updated to prefer tags with an explicit CUDA version\n\n### Fixes and improvements\n\n* Fix int8 quantization for rows that only contains zeros\n* Fix type error when running the CUDA code path of the Multinomial operator\n* Add EOS score to the greedy search final score for consistency with the beam search output\n* Use third party library [google/cpu\\_features](https://github.com/google/cpu_features) to resolve CPU features at runtime\n* Small optimizations when manipulating tensor shapes and indices\n* Internal refactoring of Transformer layers\n\n## [v1.19.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.19.0) (2021-03-31)\n\n### Changes\n\n* Rename CMake option `WITH_TESTS` to `BUILD_TESTS`\n\n### New features\n\n* Add \"auto\" compute type to automatically select the fastest compute type on the current system\n\n### Fixes and improvements\n\n* [Python] Clear memory allocator cache when calling `unload_model`\n* [Python] Make methods `unload_model` and `load_model` thread safe\n* Fix conversion of TensorFlow SavedModel with shared embeddings\n* Update Intel oneAPI to 2021.2\n* Compile core library with C++14 standard\n\n## [v1.18.3](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.18.3) (2021-03-02)\n\n### Fixes and improvements\n\n* Use Intel OpenMP instead of GNU OpenMP in the Docker images as a workaround for issue #409.\n\n## [v1.18.2](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.18.2) (2021-02-23)\n\n### Fixes and improvements\n\n* Fix crash when enabling coverage penalty in GPU translation\n* Fix incorrect value of AVX2 flag in `CT2_VERBOSE` output\n\n## [v1.18.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.18.1) (2021-02-01)\n\n### Fixes and improvements\n\n* Fix conversion of models setting the attributes `with_source_bos` or `with_source_eos`\n\n## [v1.18.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.18.0) (2021-01-28)\n\n### Changes\n\n* Some options default value in the `translate` client have been changed to match the Python API:\n  * `batch_size` = 32 (instead of 30)\n  * `beam_size` = 2 (instead of 5)\n  * `intra_threads` = 4 (instead of 0)\n\n### New features\n\n* Support multi-GPU translation: `device_index` argument can now be set to a list of GPU IDs (see [example](https://github.com/OpenNMT/CTranslate2/blob/master/docs/python.md#note-on-parallel-translations))\n\n### Fixes and improvements\n\n* Improve performance when using multiple GPU translators concurrently in the same process\n* [Python] Do nothing when calling `unload_model(to_cpu=True)` on CPU translators\n* [Python] Set a default value for `max_batch_size` argument in method `Translator.translate_file`\n* Disable `CT2_TRANSLATORS_CORE_OFFSET` in OpenMP builds as setting thread affinity does not work when OpenMP is enabled\n\n## [v1.17.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.17.1) (2021-01-15)\n\n### Fixes and improvements\n\n* Fix Python wheel loading error on macOS\n\n## [v1.17.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.17.0) (2021-01-11)\n\n### Changes\n\n* Linux Python wheels are now compiled under `manylinux2014` and require `pip` version >= 19.3\n\n### New features\n\n* Publish Python wheels for macOS (CPU only)\n* Support compilation for ARM 64-bit architecture and add NEON vectorization\n* Add new optional GEMM backends: [Apple Accelerate](https://developer.apple.com/documentation/accelerate) and [OpenBLAS](https://www.openblas.net/)\n* Add `replace_unknowns` translation option to replace unknown target tokens by source tokens with the highest attention\n* Add flags in the model specification to declare that BOS and/or EOS tokens should be added to the source sequences\n\n### Fixes and improvements\n\n* Fix segmentation fault when the model is converted with a wrong vocabulary and predicts an out-of-vocabulary index\n* Fix result of vectorized array reduction when the array length is not a multiple of the SIMD registers width\n* Fix exit code when running `cli/translate -h`\n* Improve performance of vectorized vector math by inlining calls to intrinsics functions\n* Improve accuracy of LogSoftMax CUDA implementation\n* Improve error message when `--model` option is not set in `cli/translate`\n* Update oneMKL to 2020.1 in published binaries\n* Update oneDNN to 2.0 in published binaries\n* Update default search paths to support compilation with oneMKL and oneDNN installed from the oneAPI toolkit\n\n## [v1.16.2](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.16.2) (2020-11-27)\n\n### Fixes and improvements\n\n* Fix cuBLAS version included in the Python wheels published to PyPI. The included library was targetting CUDA 10.2 instead of CUDA 10.1.\n* Re-add Python 3.5 wheels on PyPI to give users more time to transition\n\n## [v1.16.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.16.1) (2020-11-23)\n\n### Fixes and improvements\n\n* Fuse dequantization and bias addition on GPU for improved INT8 performance\n* Improve performance of masked softmax on GPU\n* Fix error when building the CentOS 7 GPU Docker image\n* The previous version listed \"Pad size of INT8 matrices to a multiple of 16 when the GPU has INT8 Tensor Cores\". However, the padding was not applied due to a bug and fixing it degraded the performance, so this behavior is not implemented for now.\n\n## [v1.16.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.16.0) (2020-11-18)\n\n### Changes\n\n* Drop support for Python 2.7 and 3.5\n\n### New features\n\n* Add Docker images using CUDA 11.0\n\n### Fixes and improvements\n\n* Enable parallel CPU translations from `translate_batch` in Python when setting `inter_threads` > 1 and `max_batch_size` > 0\n* Improve GPU performance on Turing architecture when using a Docker image or the Python package\n* Pad size of INT8 matrices to a multiple of 16 when the GPU has INT8 Tensor Cores\n* Add information about detected GPU devices in `CT2_VERBOSE` output\n* Update oneDNN to 1.7\n* [Python] Improve type checking for some arguments\n\n## [v1.15.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.15.0) (2020-11-06)\n\n### New features\n\n* [Experimental] The Python package published on PyPI now includes GPU support. The binary is compiled with CUDA 10.1, but all CUDA dependencies are integrated in the package and do not need to be installed on the system. The only requirement should be a working GPU with driver version >= 418.39.\n\n### Fixes and improvements\n\n* Remove the TensorRT dependency to simplify installation and reduce memory usage:\n  * Reduce GPU Docker images size by 600MB\n  * Reduce memory usage on the GPU and the system by up 1GB\n  * Reduce initialization time during the first GPU translation\n* Improve TopK performance on GPU for K < 5\n* Improve INT8 performance on GPU\n* Accept linear layers without bias when converting models\n* Update Intel MKL to 2020.4\n* [Python] Improve compatibility with Python 3.9\n\n## [v1.14.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.14.0) (2020-10-13)\n\n### New features\n\n* Accept target prefix in file translation APIs\n\n### Fixes and improvements\n\n* Fix CUDA illegal memory access when changing the beam size in the same process\n* Fix decoding with target prefix that sometimes did not go beyond the prefix\n* Fix Intel MKl search paths on macOS\n* Update Intel MKL to 2020.3\n* Clarify error message when selecting a CUDA device in CPU-only builds\n\n## [v1.13.2](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.13.2) (2020-08-31)\n\n### Fixes and improvements\n\n* Fix model conversion to `float16` when using the Python converters: weights were duplicated and not correctly converted\n* Fix incorrect code logic that could lead to incorrect translation results\n\n## [v1.13.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.13.1) (2020-08-06)\n\n### Fixes and improvements\n\n* Fix performance regression when decoding with a large beam size on GPU\n\n## [v1.13.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.13.0) (2020-07-30)\n\n### New features\n\n* Environment variable `CT2_TRANSLATORS_CORE_OFFSET` to pin parallel translators to a range of CPU cores (only for `intra_threads` = 1)\n* [Python] Add some properties to the `Translator` object:\n  * `device`\n  * `device_index`\n  * `num_translators`\n  * `num_queued_batches`\n  * `model_is_loaded`\n\n### Fixes and improvements\n\n* Improve batch performance of target prefix\n* Improve performance when the input batch contains sentences with very different lengths\n* Improve beam search performance by expanding the batch size only after the first decoding step\n* Optimize Transpose op on GPU for the permutation used in multi-head attention\n* Remove padding in returned attention vectors\n* Update Intel MKL to 2020.2\n\n## [v1.12.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.12.1) (2020-07-20)\n\n### Fixes and improvements\n\n* Fix implicit int16 to float16 model conversion on compatible GPUs\n\n## [v1.12.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.12.0) (2020-07-16)\n\n### Changes\n\n* Docker images based on Ubuntu 16.04 are no longer updated\n\n### New features\n\n* Support `float16` data type for model conversion (with `--quantization float16`) and computation (with `--compute_type float16`). FP16 execution can improve performance by up to 50% on NVIDIA GPUs with Compute Capability >= 7.0.\n* Add Docker images with newer CUDA versions, which can improve performance in some cases:\n  * `latest-ubuntu18-cuda10.0` (same as `latest-ubuntu18-gpu`)\n  * `latest-ubuntu18-cuda10.1`\n  * `latest-ubuntu18-cuda10.2`\n  * `latest-centos7-cuda10.0` (same as `latest-centos7-gpu`)\n  * `latest-centos7-cuda10.1`\n  * `latest-centos7-cuda10.2`\n* Allow setting a computation type per device (e.g. `Translator(..., compute_type={\"cuda\": \"float16\", \"cpu\": \"int8\"})` with the Python API)\n* [C++] Add `ModelReader` interface to customize model loading\n\n### Fixes and improvements\n\n* Optimize Transpose op on CPU for the permutation used in multi-head attention\n* Optimize GELU op CPU with Intel MKL\n* Fix compilation when targeting an architecture and disabling ISA dispatch (e.g.: `-DCMAKE_CXX_FLAGS=\"-march=skylake\" -DENABLE_CPU_DISPATCH=OFF`)\n* Inline some frequently called methods\n\n## [v1.11.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.11.0) (2020-06-29)\n\n### New features\n\n* Add tokenization and detokenization hooks for file translation APIs\n* Add alternatives to Intel MKL:\n  * Integrate [oneDNN](https://github.com/oneapi-src/oneDNN) for GEMM functions\n  * Implement vectorized operators that automatically select the instruction set architecture (ISA) (can be manually controlled with the `CT2_FORCE_CPU_ISA` environment variable)\n* When alternatives are available, avoid using Intel MKL on non Intel processors (can be manually controlled with the `CT2_USE_MKL` environment variable)\n* Enable a verbose mode with the environment variable `CT2_VERBOSE=1` to help debugging the run configuration (e.g. the detected CPU, whether Intel MKL is being used, etc.)\n\n### Fixes and improvements\n\n* Improve numerical precision of SoftMax and LogSoftMax layers on CPU\n* Parallelize INT16 quantization/dequantization and ReLU on CPU\n* Add back the translation client in CentOS 7 Docker images\n\n## [v1.10.2](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.10.2) (2020-06-23)\n\n### Fixes and improvements\n\n* [Python] Fix error when calling `unload_model(to_cpu=True)` for models with shared weights\n* [Python] Do not ignore errors when importing the compiled translator extension\n\n## [v1.10.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.10.1) (2020-05-25)\n\n### Fixes and improvements\n\n* Force `intra_threads` to 1 when running a model on GPU to prevent high CPU load\n* Improve handling of decoding length constraints when using a target prefix\n* Do not raise an error when setting `use_vmap` but no vocabulary map exists\n\n## [v1.10.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.10.0) (2020-04-17)\n\n### New features\n\n* Coverage penalty as in [Wu et al. 2016](https://arxiv.org/abs/1609.08144) with the option `coverage_penalty`\n* Batch size can be expressed in number of tokens with the option `batch_type`\n* Translation scores can be disabled with the option `return_scores` (if disabled, the final SoftMax is skipped during greedy decoding)\n* Support compilation without TensorRT by setting `-DWITH_TENSORRT=OFF` during CMake configuration (in this case, beam search is no longer supported)\n* Experimental integration of [Intel MKL's packed GEMM](https://software.intel.com/en-us/articles/introducing-the-new-packed-apis-for-gemm) which can be enabled by setting the environment variable `CT2_USE_EXPERIMENTAL_PACKED_GEMM=1`\n\n### Fixes and improvements\n\n* Remove direct dependency to cuDNN (still an indirect dependency via TensorRT)\n* Static AVX optimization for the ReLU operator\n* Remove unnecessary memory initialization when creating temporary buffers\n* Dissociate SoftMax and LogSoftMax in profiling report\n\n## [v1.9.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.9.1) (2020-04-08)\n\n### Fixes and improvements\n\n* Fix parallel translations when calling `Translator.translate_batch` from multiple Python threads\n* Fix crash on invalid `num_hypotheses` value\n\n## [v1.9.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.9.0) (2020-03-24)\n\n### New features\n\n* Return 2 additional statistics from file translation APIs:\n  * the number of translated examples\n  * the total translation time in milliseconds\n\n### Fixes and improvements\n\n* Fix exceptions that were not catched by the Python wrapper\n* Fix an invalid insertion in the variables collection while iterating over it\n* Optimize filling operation of float storages\n* Internal refactoring of decoding functions to make them reusable for other tasks (e.g. generative language models)\n\n## [v1.8.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.8.0) (2020-03-10)\n\n### New features\n\n* [Python] Add methods `Translator.unload_model` and `Translator.load_model` to manually manage memory\n* [Docker] Move all images to Python 3 only\n* Expose options that enable an internal sorting by length to increase the translation efficiency:\n  * for file translation: `read_batch_size` contiguous examples will be loaded, sorted by length, and batched with size `max_batch_size`\n  * for batch translation: if the batch is larger than `max_batch_size`, examples will be sorted by length and batched with size `max_batch_size`\n\n### Fixes and improvements\n\n* Fix another error when releasing a translator that is placed on a GPU that is not GPU 0\n* Fix possible memory corruption when creating GPU translators in parallel\n* Fix memory that is briefly allocated on GPU 0 when destroying a translator that is placed on another GPU\n* Reduce latency of model loading, especially on GPU\n\n## [v1.7.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.7.1) (2020-03-03)\n\n### Fixes and improvements\n\n* Revert \"Parallelize some low level transformations on CPU\" which caused incorrect computation\n* Avoid unnecessary TensorFlow runtime initialization when converting checkpoints\n* Fix compilation without MKL\n\n## [v1.7.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.7.0) (2020-02-28)\n\n### New features\n\n* Translation option `return_alternatives` to return multiple choices at the first unconstrained decoding position: combined with a target prefix, this could be used to provide alternative words and translations at a specific location in the target\n* Support Transformers with different number of encoder/decoder layers\n* Allow compilation without OpenMP with `-DOPENMP_RUNTIME=NONE`\n\n### Fixes and improvements\n\n* Fix SavedModel conversion when TensorFlow Addons 0.8 is installed\n* Fix error when releasing a translator/model that is placed on a GPU that is not GPU 0\n* Fix memory that was allocated on GPU 0 even when the translator/model was placed on another GPU\n* Query GPU int8 support on the first model load, and then cache the result for future loads\n* Avoid creating an empty model directory on conversion errors\n* Parallelize some low level transformations on CPU\n* Reduce memory usage when translating large files by limiting the work queue size\n\n## [v1.6.3](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.6.3) (2020-02-24)\n\n### Fixes and improvements\n\n* Fix incorrectness in relative representation computation\n\n## [v1.6.2](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.6.2) (2020-02-21)\n\n### Fixes and improvements\n\n* Fix conversion of models with shared embeddings\n\n## [v1.6.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.6.1) (2020-02-11)\n\n### Fixes and improvements\n\n* [Docker] Remove translation client in CentOS 7 images as it can cause compatibility issues with downstream images\n\n## [v1.6.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.6.0) (2020-02-14)\n\n### New features\n\n* Support Transformers with relative position representations (as in [Shaw et al. 2018](https://arxiv.org/abs/1803.02155))\n* Accept target prefix in batch request\n* Support `return_attention` with prefixed translation\n\n## [v1.5.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.5.1) (2020-02-06)\n\n### Fixes and improvements\n\n* Fix INT8 translation on CPU with vocabulary map\n\n## [v1.5.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.5.0) (2020-02-06)\n\n### New features\n\n* [C++] Add `max_batch_size` translation options for single translators\n\n### Fixes and improvements\n\n* Improve INT8 performance on CPU\n* Enable INT8 support on default Intel MKL build\n* Simplify project dependencies:\n  * Replace `boost::program_options` with `cxxopts` for client options\n  * Include header-only dependencies as Git submodules (`cxxopts`, `cub`, and `thrust`)\n  * Remove MKL-DNN\n* Harmonize Python/C++ default values:\n  * [Python] Change default beam size from 4 to 2\n  * [C++] Load models on the CPU by default\n\n## [v1.4.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.4.0) (2020-01-20)\n\n### New features\n\n* Publish a package on [PyPI](https://pypi.org/project/ctranslate2/) (without GPU support)\n* Add method to convert OpenNMT-tf models directly from a dictionary of variables\n* Return statistics from Python method `Translator.translate_file`\n* Add `set_model` methods to support changing models without creating a new `Translator`\n* Add a `contains_model` function to check whether a directory could contain a CTranslate2 model\n\n## [v1.3.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.3.0) (2020-01-14)\n\n### New features\n\n* Support random sampling (see the `sampling_topk` and `sampling_temperature` translation options)\n* `CT2_CUDA_CACHING_ALLOCATOR_CONFIG` environment variable to configure the CUDA caching allocator\n\n### Fixes and improvements\n\n* Fix incorrect translations on Windows due to incompatibility between the compiler OpenMP and Intel OpenMP\n* Release cuDNN/cuBLAS/TensorRT handles on thread exit when destroying a `TranslatorPool`\n* Remove use of `--{start,end}-group` compiler options when compiling on Mac OS\n* Update Intel MKL to 2020.0 in Docker images\n* Load vocabulary assets for SavedModel exported with OpenNMT-tf 2.5 and above\n\n## [v1.2.3](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.2.3) (2019-12-11)\n\n### Fixes and improvements\n\n* Improve translator robustness on empty batch and inputs\n* Speed optimization for `LayerNorm`\n* Check vocabulary size when converting OpenNMT-tf models\n* Add more samples in the execution profiling output which now supports nested functions\n\n## [v1.2.2](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.2.2) (2019-11-25)\n\n### Fixes and improvements\n\n* Fix `PositionEncoder` internal state that was shared with other instances on the same thread\n* Replace Boost.Python by pybind11\n* Include a Python source distribution in the Docker images\n\n## [v1.2.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.2.1) (2019-11-06)\n\n### Fixes and improvements\n\n* Avoid copying decoder states when possible to improve decoding performance (10% to 20% faster)\n* Fix execution profiling on GPU (device was not synchronized before measuring the time)\n* Include `Mul` operation in profiling report\n* Add a Python 3 wheel in Ubuntu Docker images\n\n## [v1.2.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.2.0) (2019-10-28)\n\n### New features\n\n* Accept Transformer models with custom number of layers and heads\n* `--log-profiling` client option to profile ops execution\n\n### Fixes and improvements\n\n* Fix conversion error for models having 2 different weights with the same values\n* Fix invalid MKL function override after a refactoring\n* Add more information and context to several error messages\n\n## [v1.1.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.1.0) (2019-10-18)\n\n### New features\n\n* New Docker images: `latest-ubuntu16-gpu`, `latest-ubuntu18`, `latest-ubuntu18-gpu`\n* Support OpenNMT-tf Transformer models with shared embeddings\n* Update to TensorRT 6\n* Make OpenMP runtime configurable\n\n### Fixes and improvements\n\n* Reduce the size of models with shared weights on disk and in memory\n* Shared words vocabulary is no longer duplicated on disk and in memory\n* Improve performance of translation with a vocabulary map on GPU\n* Statically link against Intel MKL\n* Remove some implementation details from public headers\n\n## [v1.0.1](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.0.1) (2019-10-08)\n\n### Fixes and improvements\n\n* Fix loading of newer OpenNMT-py models\n* Promote FP16 to FP32 in model converter scripts\n* Improve INT8 performance on CPU and GPU\n* Improve performance on GPU by fusing the layer normalization operation `x * gamma + beta`\n* Enable INT8 and INT16 computation on all platforms with Intel MKL 2019.5 and above\n\n## [v1.0.0](https://github.com/OpenNMT/CTranslate2/releases/tag/v1.0.0) (2019-09-23)\n\nFirst stable release.\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 24.353515625,
          "content": "cmake_minimum_required(VERSION 3.7)\n\n# Set policy for setting the MSVC runtime library for static MSVC builds\nif(POLICY CMP0091)\n  cmake_policy(SET CMP0091 NEW)\nendif()\n\nproject(ctranslate2)\n\noption(WITH_MKL \"Compile with Intel MKL backend\" ON)\noption(WITH_DNNL \"Compile with DNNL backend\" OFF)\noption(WITH_ACCELERATE \"Compile with Accelerate backend\" OFF)\noption(WITH_OPENBLAS \"Compile with OpenBLAS backend\" OFF)\noption(WITH_RUY \"Compile with Ruy backend\" OFF)\noption(WITH_CUDA \"Compile with CUDA backend\" OFF)\noption(WITH_CUDNN \"Compile with cuDNN backend\" OFF)\noption(CUDA_DYNAMIC_LOADING \"Dynamically load CUDA libraries at runtime\" OFF)\noption(ENABLE_CPU_DISPATCH \"Compile CPU kernels for multiple ISA and dispatch at runtime\" ON)\noption(ENABLE_PROFILING \"Compile with profiling support\" OFF)\noption(BUILD_CLI \"Compile the clients\" ON)\noption(BUILD_TESTS \"Compile the tests\" OFF)\noption(BUILD_SHARED_LIBS \"Build shared libraries\" ON)\noption(WITH_TENSOR_PARALLEL \"Compile with NCCL and MPI backend\" OFF)\noption(WITH_FLASH_ATTN \"Compile with Flash Attention 2\" OFF)\n\nif(ENABLE_PROFILING)\n  message(STATUS \"Enable profiling support\")\n  add_definitions(-DCT2_ENABLE_PROFILING)\nendif()\n\nif(DEFINED ENV{INTELROOT})\n  set(INTEL_ROOT_DEFAULT $ENV{INTELROOT})\nelseif(DEFINED ENV{ONEAPI_ROOT})\n  set(INTEL_ROOT_DEFAULT $ENV{ONEAPI_ROOT}/..)\nelseif(DEFINED ENV{MKLROOT})\n  if(WIN32)\n    set(INTEL_ROOT_DEFAULT $ENV{MKLROOT}/..)\n  else()\n    # Other system like arch set env MKLROOT by default\n    set(INTEL_ROOT_DEFAULT $ENV{MKLROOT}/../../..)\n  endif()\nelseif(WIN32)\n  set(ProgramFilesx86 \"ProgramFiles(x86)\")\n  set(INTEL_ROOT_DEFAULT PATHS\n      $ENV{${ProgramFilesx86}}/IntelSWTools/compilers_and_libraries/windows\n      $ENV{${ProgramFilesx86}}/Intel)\nelse()\n  set(INTEL_ROOT_DEFAULT \"/opt/intel\")\nendif()\nset(INTEL_ROOT ${INTEL_ROOT_DEFAULT} CACHE FILEPATH \"Path to Intel root directory\")\nset(OPENMP_RUNTIME \"INTEL\" CACHE STRING \"OpenMP runtime (INTEL, COMP, NONE)\")\n\n# Set Release build type by default to get sane performance.\nif(NOT CMAKE_BUILD_TYPE)\n  set(CMAKE_BUILD_TYPE Release)\nendif(NOT CMAKE_BUILD_TYPE)\n\n# Set CXX flags.\nset(CMAKE_CXX_STANDARD 17)\n\nif(APPLE)\n  set(CMAKE_OSX_DEPLOYMENT_TARGET 10.13)\nendif()\n\n\n# Read version from version.py\nfile(STRINGS ${CMAKE_CURRENT_SOURCE_DIR}/python/ctranslate2/version.py VERSION_FILE)\nforeach(line IN LISTS VERSION_FILE)\n  if (line MATCHES \"__version__\")\n    string(REGEX MATCH \"[0-9.]+\" CTRANSLATE2_VERSION ${line})\n    break()\n  endif()\nendforeach()\n\nif(NOT CTRANSLATE2_VERSION)\n  message(FATAL_ERROR \"Version can't be read from version.py\")\nendif()\n\nstring(REPLACE \".\" \";\" CTRANSLATE2_VERSION_LIST ${CTRANSLATE2_VERSION})\nlist(GET CTRANSLATE2_VERSION_LIST 0 CTRANSLATE2_MAJOR_VERSION)\n\nif(MSVC)\n  add_compile_definitions(_USE_MATH_DEFINES) # required for M_PI\n  if(BUILD_SHARED_LIBS)\n    set(CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS ON)\n  else()\n    if(CMAKE_VERSION VERSION_LESS \"3.15.0\")\n      message(FATAL_ERROR \"Use CMake 3.15 or later when setting BUILD_SHARED_LIBS to OFF\")\n    endif()\n    set(CMAKE_MSVC_RUNTIME_LIBRARY \"MultiThreaded$<$<CONFIG:Debug>:Debug>\")\n  endif()\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /W4 /d2FH4-\")\nelse()\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall -Wextra\")\nendif()\n\nfind_package(Threads)\nadd_subdirectory(third_party/spdlog EXCLUDE_FROM_ALL)\n\nset(PRIVATE_INCLUDE_DIRECTORIES\n  ${CMAKE_CURRENT_SOURCE_DIR}/src\n  ${CMAKE_CURRENT_SOURCE_DIR}/third_party\n  )\nset(SOURCES\n  src/allocator.cc\n  src/batch_reader.cc\n  src/buffered_translation_wrapper.cc\n  src/cpu/allocator.cc\n  src/cpu/backend.cc\n  src/cpu/cpu_info.cc\n  src/cpu/cpu_isa.cc\n  src/cpu/kernels.cc\n  src/cpu/parallel.cc\n  src/cpu/primitives.cc\n  src/decoding.cc\n  src/decoding_utils.cc\n  src/devices.cc\n  src/dtw.cc\n  src/encoder.cc\n  src/env.cc\n  src/filesystem.cc\n  src/generator.cc\n  src/layers/attention_layer.cc\n  src/layers/attention.cc\n  src/layers/flash_attention.cc\n  src/layers/common.cc\n  src/layers/decoder.cc\n  src/layers/transformer.cc\n  src/layers/wav2vec2.cc\n  src/layers/wav2vec2bert.cc\n  src/layers/whisper.cc\n  src/logging.cc\n  src/models/language_model.cc\n  src/models/model.cc\n  src/models/model_factory.cc\n  src/models/model_reader.cc\n  src/models/sequence_to_sequence.cc\n  src/models/transformer.cc\n  src/models/wav2vec2.cc\n  src/models/wav2vec2bert.cc\n  src/models/whisper.cc\n  src/ops/activation.cc\n  src/ops/add.cc\n  src/ops/alibi_add.cc\n  src/ops/alibi_add_cpu.cc\n  src/ops/bias_add.cc\n  src/ops/bias_add_cpu.cc\n  src/ops/concat.cc\n  src/ops/concat_split_slide_cpu.cc\n  src/ops/conv1d.cc\n  src/ops/conv1d_cpu.cc\n  src/ops/cos.cc\n  src/ops/dequantize.cc\n  src/ops/dequantize_cpu.cc\n  src/ops/flash_attention.cc\n  src/ops/flash_attention_cpu.cc\n  src/ops/gather.cc\n  src/ops/gather_cpu.cc\n  src/ops/gelu.cc\n  src/ops/gemm.cc\n  src/ops/gumbel_max.cc\n  src/ops/gumbel_max_cpu.cc\n  src/ops/layer_norm.cc\n  src/ops/layer_norm_cpu.cc\n  src/ops/log.cc\n  src/ops/matmul.cc\n  src/ops/mean.cc\n  src/ops/mean_cpu.cc\n  src/ops/median_filter.cc\n  src/ops/min_max.cc\n  src/ops/mul.cc\n  src/ops/multinomial.cc\n  src/ops/multinomial_cpu.cc\n  src/ops/quantize.cc\n  src/ops/quantize_cpu.cc\n  src/ops/relu.cc\n  src/ops/rms_norm.cc\n  src/ops/rms_norm_cpu.cc\n  src/ops/rotary.cc\n  src/ops/rotary_cpu.cc\n  src/ops/sin.cc\n  src/ops/softmax.cc\n  src/ops/softmax_cpu.cc\n  src/ops/split.cc\n  src/ops/slide.cc\n  src/ops/sub.cc\n  src/ops/sigmoid.cc\n  src/ops/swish.cc\n  src/ops/tanh.cc\n  src/ops/tile.cc\n  src/ops/tile_cpu.cc\n  src/ops/topk.cc\n  src/ops/topk_cpu.cc\n  src/ops/topp_mask.cc\n  src/ops/topp_mask_cpu.cc\n  src/ops/transpose.cc\n  src/ops/nccl_ops.cc\n  src/ops/nccl_ops_cpu.cc\n  src/ops/awq/dequantize.cc\n  src/ops/awq/dequantize_cpu.cc\n  src/ops/awq/gemm.cc\n  src/ops/awq/gemm_cpu.cc\n  src/ops/awq/gemv.cc\n  src/ops/awq/gemv_cpu.cc\n  src/ops/sum.cc\n  src/padder.cc\n  src/profiler.cc\n  src/random.cc\n  src/sampling.cc\n  src/scoring.cc\n  src/storage_view.cc\n  src/thread_pool.cc\n  src/translator.cc\n  src/types.cc\n  src/utils.cc\n  src/vocabulary.cc\n  src/vocabulary_map.cc\n)\nset(LIBRARIES\n  ${CMAKE_THREAD_LIBS_INIT}\n  spdlog::spdlog_header_only\n  )\n\nmacro(ct2_compile_kernels_for_isa isa flag)\n  configure_file(\n    src/cpu/kernels.cc\n    ${CMAKE_CURRENT_BINARY_DIR}/kernels_${isa}.cc\n    COPYONLY)\n  set_source_files_properties(\n    ${CMAKE_CURRENT_BINARY_DIR}/kernels_${isa}.cc\n    PROPERTIES COMPILE_FLAGS ${flag})\n  list(APPEND SOURCES ${CMAKE_CURRENT_BINARY_DIR}/kernels_${isa}.cc)\nendmacro()\n\nif(CMAKE_SYSTEM_PROCESSOR MATCHES \"(arm64)|(aarch64)\"\n    OR (APPLE AND CMAKE_OSX_ARCHITECTURES STREQUAL \"arm64\"))\n  add_definitions(-DCT2_ARM64_BUILD)\n  set(CT2_BUILD_ARCH \"arm64\")\nelseif(CMAKE_SYSTEM_PROCESSOR MATCHES \"(x86_64)|(amd64)|(AMD64)\")\n  add_definitions(-DCT2_X86_BUILD)\n  set(CT2_BUILD_ARCH \"x86_64\")\n\n  if(BUILD_SHARED_LIBS)\n    set(CMAKE_POSITION_INDEPENDENT_CODE ON)\n  endif()\n  set(BUILD_SHARED_LIBS_SAVED \"${BUILD_SHARED_LIBS}\")\n  set(BUILD_SHARED_LIBS OFF)\n  set(BUILD_TESTING OFF)\n  add_subdirectory(third_party/cpu_features EXCLUDE_FROM_ALL)\n  set(BUILD_SHARED_LIBS \"${BUILD_SHARED_LIBS_SAVED}\")\n  list(APPEND LIBRARIES cpu_features)\nendif()\n\nif(ENABLE_CPU_DISPATCH)\n  message(STATUS \"Compiling for multiple CPU ISA and enabling runtime dispatch\")\n  add_definitions(-DCT2_WITH_CPU_DISPATCH)\n  if(CT2_BUILD_ARCH STREQUAL \"x86_64\")\n    if(WIN32)\n      ct2_compile_kernels_for_isa(avx \"/arch:AVX\")\n      ct2_compile_kernels_for_isa(avx2 \"/arch:AVX2\")\n      ct2_compile_kernels_for_isa(avx512 \"/arch:AVX512\")\n    else()\n      ct2_compile_kernels_for_isa(avx \"-mavx\")\n      ct2_compile_kernels_for_isa(avx2 \"-mavx2 -mfma\")\n      ct2_compile_kernels_for_isa(avx512 \"-mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq\")\n    endif()\n  elseif(CT2_BUILD_ARCH STREQUAL \"arm64\")\n    ct2_compile_kernels_for_isa(neon \"-DUSE_NEON\")\n  endif()\nendif()\n\nif(NOT OPENMP_RUNTIME STREQUAL \"NONE\")\n  if(WIN32)\n    add_compile_options(\"/openmp\")\n  else()\n    find_package(OpenMP)\n    if(OpenMP_CXX_FOUND)\n      add_compile_options(${OpenMP_CXX_FLAGS})\n    endif()\n  endif()\n\n  if(OPENMP_RUNTIME STREQUAL \"INTEL\")\n    # Find Intel libraries.\n    find_library(IOMP5_LIBRARY iomp5 libiomp5md PATHS\n      ${INTEL_ROOT}/lib\n      ${INTEL_ROOT}/lib/intel64\n      ${INTEL_ROOT}/compiler/lib/intel64\n      ${INTEL_ROOT}/oneAPI/compiler/latest/windows/compiler/lib/intel64_win\n      ${INTEL_ROOT}/oneapi/compiler/latest/linux/compiler/lib/intel64_lin\n      ${INTEL_ROOT}/oneapi/compiler/latest/mac/compiler/lib\n      ${INTEL_ROOT}/oneapi/compiler/latest/lib\n      )\n    if(IOMP5_LIBRARY)\n      list(APPEND LIBRARIES ${IOMP5_LIBRARY})\n      message(STATUS \"Using OpenMP: ${IOMP5_LIBRARY}\")\n    else()\n      message(FATAL_ERROR \"Intel OpenMP runtime libiomp5 not found\")\n    endif()\n    if(WIN32)\n      set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} /nodefaultlib:vcomp\")\n      set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} /nodefaultlib:vcomp\")\n    endif()\n  elseif(OPENMP_RUNTIME STREQUAL \"COMP\")\n    if(OpenMP_CXX_FOUND)\n      list(APPEND LIBRARIES ${OpenMP_CXX_LIBRARIES})\n      message(STATUS \"Using OpenMP: ${OpenMP_CXX_LIBRARIES}\")\n    elseif(NOT WIN32)\n      message(FATAL_ERROR \"OpenMP not found\")\n    endif()\n  else()\n    message(FATAL_ERROR \"Invalid OpenMP runtime ${OPENMP_RUNTIME}\")\n  endif()\nendif()\n\nif(WITH_MKL)\n  find_path(MKL_ROOT include/mkl.h DOC \"Path to MKL root directory\" PATHS\n    $ENV{MKLROOT}\n    ${INTEL_ROOT}/mkl\n    ${INTEL_ROOT}/oneAPI/mkl/latest\n    ${INTEL_ROOT}/oneapi/mkl/latest\n    )\n\n  # Find MKL includes.\n  find_path(MKL_INCLUDE_DIR NAMES mkl.h HINTS ${MKL_ROOT}/include/)\n  if(MKL_INCLUDE_DIR)\n    message(STATUS \"Found MKL include directory: ${MKL_INCLUDE_DIR}\")\n  else()\n    message(FATAL_ERROR \"MKL include directory not found\")\n  endif()\n\n  # Find MKL libraries.\n  find_library(MKL_CORE_LIBRARY NAMES mkl_core HINTS ${MKL_ROOT}/lib ${MKL_ROOT}/lib/intel64)\n  if(MKL_CORE_LIBRARY)\n    get_filename_component(MKL_LIBRARY_DIR ${MKL_CORE_LIBRARY} DIRECTORY)\n    message(STATUS \"Found MKL library directory: ${MKL_LIBRARY_DIR}\")\n  else()\n    message(FATAL_ERROR \"MKL library directory not found\")\n  endif()\n\n  add_definitions(-DCT2_WITH_MKL -DMKL_ILP64)\n  if(WIN32)\n    set(MKL_LIBRARIES\n      ${MKL_LIBRARY_DIR}/mkl_core.lib\n      ${MKL_LIBRARY_DIR}/mkl_intel_ilp64.lib\n      )\n  else()\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -m64\")\n    set(MKL_LIBRARIES\n      ${MKL_LIBRARY_DIR}/libmkl_core.a\n      ${MKL_LIBRARY_DIR}/libmkl_intel_ilp64.a\n      )\n  endif()\n\n  if(OPENMP_RUNTIME STREQUAL \"INTEL\")\n    if(WIN32)\n      list(APPEND MKL_LIBRARIES ${MKL_LIBRARY_DIR}/mkl_intel_thread.lib)\n    else()\n      list(APPEND MKL_LIBRARIES ${MKL_LIBRARY_DIR}/libmkl_intel_thread.a)\n    endif()\n  elseif(OPENMP_RUNTIME STREQUAL \"COMP\")\n    if(WIN32)\n      message(FATAL_ERROR \"Building with MKL requires Intel OpenMP\")\n    else()\n      list(APPEND MKL_LIBRARIES ${MKL_LIBRARY_DIR}/libmkl_gnu_thread.a)\n    endif()\n  elseif(OPENMP_RUNTIME STREQUAL \"NONE\")\n    if(WIN32)\n      list(APPEND MKL_LIBRARIES ${MKL_LIBRARY_DIR}/mkl_sequential.lib)\n    else()\n      list(APPEND MKL_LIBRARIES ${MKL_LIBRARY_DIR}/libmkl_sequential.a)\n    endif()\n  endif()\n  list(APPEND PRIVATE_INCLUDE_DIRECTORIES ${MKL_INCLUDE_DIR})\n  if(WIN32 OR APPLE)\n    list(APPEND LIBRARIES ${MKL_LIBRARIES})\n  else()\n    list(APPEND LIBRARIES -Wl,--start-group ${MKL_LIBRARIES} -Wl,--end-group)\n  endif()\nendif()\n\nif(WITH_DNNL)\n  set(ONEAPI_DNNL_PATH ${INTEL_ROOT}/oneapi/dnnl/latest)\n  if(OPENMP_RUNTIME STREQUAL \"INTEL\")\n    set(ONEAPI_DNNL_PATH ${ONEAPI_DNNL_PATH}/cpu_iomp)\n  else()\n    set(ONEAPI_DNNL_PATH ${ONEAPI_DNNL_PATH}/cpu_gomp)\n  endif()\n\n  find_path(DNNL_INCLUDE_DIR NAMES dnnl.h PATHS ${ONEAPI_DNNL_PATH}/include)\n  if(DNNL_INCLUDE_DIR)\n    message(STATUS \"Found DNNL include directory: ${DNNL_INCLUDE_DIR}\")\n  else()\n    message(FATAL_ERROR \"DNNL include directory not found\")\n  endif()\n\n  find_library(DNNL_LIBRARY NAMES dnnl PATHS ${ONEAPI_DNNL_PATH}/lib)\n  if(DNNL_LIBRARY)\n    message(STATUS \"Found DNNL library: ${DNNL_LIBRARY}\")\n  else()\n    message(FATAL_ERROR \"DNNL library not found\")\n  endif()\n\n  add_definitions(-DCT2_WITH_DNNL)\n  list(APPEND PRIVATE_INCLUDE_DIRECTORIES ${DNNL_INCLUDE_DIR})\n  list(APPEND LIBRARIES ${DNNL_LIBRARY})\nendif()\n\nif (WITH_ACCELERATE)\n  set(BLA_VENDOR Apple)\n  find_package(BLAS REQUIRED)\n  add_definitions(-DCT2_WITH_ACCELERATE)\n  list(APPEND LIBRARIES ${BLAS_LIBRARIES})\nendif()\n\nif (WITH_OPENBLAS)\n  find_path(OPENBLAS_INCLUDE_DIR NAMES cblas.h)\n  if(OPENBLAS_INCLUDE_DIR)\n    message(STATUS \"Found OpenBLAS include directory: ${OPENBLAS_INCLUDE_DIR}\")\n  else()\n    message(FATAL_ERROR \"OpenBLAS include directory not found\")\n  endif()\n\n  find_library(OPENBLAS_LIBRARY NAMES openblas)\n  if(OPENBLAS_LIBRARY)\n    message(STATUS \"Found OpenBLAS library: ${OPENBLAS_LIBRARY}\")\n  else()\n    message(FATAL_ERROR \"OpenBLAS library not found\")\n  endif()\n\n  add_definitions(-DCT2_WITH_OPENBLAS)\n  list(APPEND PRIVATE_INCLUDE_DIRECTORIES ${OPENBLAS_INCLUDE_DIR})\n  list(APPEND LIBRARIES ${OPENBLAS_LIBRARY})\nendif()\n\nif (WITH_RUY)\n  add_definitions(-DCT2_WITH_RUY)\n  set(CMAKE_POSITION_INDEPENDENT_CODE ON)\n  set(CPUINFO_LIBRARY_TYPE static CACHE STRING \"cpuinfo library type\")\n  add_subdirectory(third_party/ruy EXCLUDE_FROM_ALL)\n  unset(CMAKE_POSITION_INDEPENDENT_CODE)\n  list(APPEND LIBRARIES ruy)\nendif()\n\nif (WITH_CUDA)\n  find_package(CUDA 11.0 REQUIRED)\n  list(APPEND CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)\n  if (WITH_TENSOR_PARALLEL)\n    find_package(MPI REQUIRED)\n    find_package(NCCL REQUIRED)\n    include_directories(${NCCL_INCLUDE_DIR})\n    include_directories(${MPI_INCLUDE_PATH})\n    if(CUDA_DYNAMIC_LOADING)\n      list(APPEND SOURCES src/cuda/mpi_stub.cc)\n      list(APPEND SOURCES src/cuda/nccl_stub.cc)\n      add_definitions(-DCT2_WITH_CUDA_DYNAMIC_LOADING)\n    else ()\n      list(APPEND LIBRARIES ${NCCL_LIBRARY})\n      list(APPEND LIBRARIES ${MPI_LIBRARIES})\n    endif ()\n    add_definitions(-DCT2_WITH_TENSOR_PARALLEL)\n  endif ()\n  include_directories(${CUDA_TOOLKIT_ROOT_DIR}/include)\n\n  add_definitions(-DCT2_WITH_CUDA)\n  if(MSVC)\n    if(BUILD_SHARED_LIBS)\n      list(APPEND CUDA_NVCC_FLAGS \"-Xcompiler=/MD$<$<CONFIG:Debug>:d>\")\n    else()\n      list(APPEND CUDA_NVCC_FLAGS \"-Xcompiler=/MT$<$<CONFIG:Debug>:d>\")\n    endif()\n  endif()\n  list(APPEND CUDA_NVCC_FLAGS \"-std=c++17\")\n  if(OpenMP_CXX_FOUND)\n    list(APPEND CUDA_NVCC_FLAGS \"-Xcompiler=${OpenMP_CXX_FLAGS}\")\n  endif()\n\n  if(NOT CUDA_ARCH_LIST)\n    set(CUDA_ARCH_LIST \"Auto\")\n  elseif(CUDA_ARCH_LIST STREQUAL \"Common\")\n    set(CUDA_ARCH_LIST ${CUDA_COMMON_GPU_ARCHITECTURES})\n    # Keep deprecated but not yet dropped Compute Capabilities.\n    if(CUDA_VERSION_MAJOR EQUAL 11)\n      list(INSERT CUDA_ARCH_LIST 0 \"3.5\" \"5.0\")\n    endif()\n    list(REMOVE_DUPLICATES CUDA_ARCH_LIST)\n  endif()\n\n  cuda_select_nvcc_arch_flags(ARCH_FLAGS ${CUDA_ARCH_LIST})\n  list(APPEND CUDA_NVCC_FLAGS ${ARCH_FLAGS})\n  set(CUDA_HOST_COMPILER ${CMAKE_CXX_COMPILER})\n\n  # flags for flash attention\n  if (WITH_FLASH_ATTN)\n    list(APPEND CUDA_NVCC_FLAGS \"--expt-relaxed-constexpr\")\n    list(APPEND CUDA_NVCC_FLAGS \"--expt-extended-lambda\")\n  endif()\n\n  message(STATUS \"NVCC host compiler: ${CUDA_HOST_COMPILER}\")\n  message(STATUS \"NVCC compilation flags: ${CUDA_NVCC_FLAGS}\")\n\n  # We should ensure that the Thrust include directories appear before\n  # -I/usr/local/cuda/include for both GCC and NVCC, so that the headers\n  # are coming from the submodule and not the system.\n  set(THRUST_INCLUDE_DIRS\n    ${CMAKE_CURRENT_SOURCE_DIR}/third_party/thrust/dependencies/cub\n    ${CMAKE_CURRENT_SOURCE_DIR}/third_party/thrust\n    )\n  cuda_include_directories(${THRUST_INCLUDE_DIRS})\n  list(APPEND PRIVATE_INCLUDE_DIRECTORIES ${THRUST_INCLUDE_DIRS})\n\n  set(CUTLASS_INCLUDE_DIRS\n    ${CMAKE_CURRENT_SOURCE_DIR}/third_party/cutlass/include\n  )\n  cuda_include_directories(${CUTLASS_INCLUDE_DIRS})\n  list(APPEND PRIVATE_INCLUDE_DIRECTORIES ${CUTLASS_INCLUDE_DIRS})\n\n  if(WITH_CUDNN)\n    # Find cuDNN includes.\n    find_path(CUDNN_INCLUDE_DIR NAMES cudnn.h HINTS ${CUDA_TOOLKIT_ROOT_DIR}/include)\n    if(CUDNN_INCLUDE_DIR)\n      message(STATUS \"Found cuDNN include directory: ${CUDNN_INCLUDE_DIR}\")\n    else()\n      message(FATAL_ERROR \"cuDNN include directory not found\")\n    endif()\n\n    # Find cuDNN libraries.\n    find_library(CUDNN_LIBRARIES\n      NAMES cudnn\n      HINTS\n      ${CUDA_TOOLKIT_ROOT_DIR}/lib\n      ${CUDA_TOOLKIT_ROOT_DIR}/lib64\n      ${CUDA_TOOLKIT_ROOT_DIR}/lib/x64\n    )\n    if(CUDNN_LIBRARIES)\n      message(STATUS \"Found cuDNN libraries: ${CUDNN_LIBRARIES}\")\n    else()\n      message(FATAL_ERROR \"cuDNN libraries not found\")\n    endif()\n\n    # libcudnn.so is a shim layer that dynamically loads the correct library at runtime,\n    # so we explictly link against it even with CUDA_DYNAMIC_LOADING.\n    list(APPEND PRIVATE_INCLUDE_DIRECTORIES ${CUDNN_INCLUDE_DIR})\n    list(APPEND LIBRARIES ${CUDNN_LIBRARIES})\n    add_definitions(-DCT2_WITH_CUDNN)\n  else()\n    message(WARNING \"cuDNN library is not enabled: convolution layers will not be supported on GPU\")\n  endif()\n\n  if(CUDA_DYNAMIC_LOADING)\n    list(APPEND SOURCES src/cuda/cublas_stub.cc)\n  else()\n    list(APPEND LIBRARIES ${CUDA_CUBLAS_LIBRARIES})\n  endif()\n  if (WITH_FLASH_ATTN)\n    add_definitions(-DCT2_WITH_FLASH_ATTN)\n    list(APPEND SOURCES\n      src/ops/flash-attention/flash_fwd_hdim32_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim32_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim64_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim64_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim96_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim96_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim128_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim128_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim160_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim160_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim192_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim192_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim224_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim224_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim256_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim256_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim32_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim32_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim64_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim64_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim96_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim96_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim128_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim128_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim160_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim160_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim192_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim192_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim224_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim224_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim256_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim256_fp16_sm80.cu\n    )\n\n    set_source_files_properties(\n      src/ops/flash-attention/flash_fwd_hdim32_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim32_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim64_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim64_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim96_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim96_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim128_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim128_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim160_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim160_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim192_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim192_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim224_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim224_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim256_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_hdim256_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim32_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim32_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim64_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim64_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim96_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim96_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim128_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim128_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim160_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim160_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim192_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim192_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim224_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim224_fp16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim256_bf16_sm80.cu\n      src/ops/flash-attention/flash_fwd_split_hdim256_fp16_sm80.cu\n      PROPERTIES COMPILE_FLAGS \"--use_fast_math\")\n  endif()\n  set(CUDA_LINK_LIBRARIES_KEYWORD PRIVATE)\n  cuda_add_library(${PROJECT_NAME}\n    ${SOURCES}\n    src/cuda/allocator.cc\n    src/cuda/primitives.cu\n    src/cuda/random.cu\n    src/cuda/utils.cc\n    src/ops/alibi_add_gpu.cu\n    src/ops/bias_add_gpu.cu\n    src/ops/concat_split_slide_gpu.cu\n    src/ops/conv1d_gpu.cu\n    src/ops/dequantize_gpu.cu\n    src/ops/flash_attention_gpu.cu\n    src/ops/gather_gpu.cu\n    src/ops/gumbel_max_gpu.cu\n    src/ops/layer_norm_gpu.cu\n    src/ops/mean_gpu.cu\n    src/ops/multinomial_gpu.cu\n    src/ops/rms_norm_gpu.cu\n    src/ops/rotary_gpu.cu\n    src/ops/softmax_gpu.cu\n    src/ops/tile_gpu.cu\n    src/ops/topk_gpu.cu\n    src/ops/topp_mask_gpu.cu\n    src/ops/quantize_gpu.cu\n    src/ops/nccl_ops_gpu.cu\n    src/ops/awq/gemm_gpu.cu\n    src/ops/awq/gemv_gpu.cu\n    src/ops/awq/dequantize_gpu.cu\n  )\n\n\nelseif(WITH_CUDNN)\n  message(FATAL_ERROR \"WITH_CUDNN=ON requires WITH_CUDA=ON\")\nelse()\n  add_library(${PROJECT_NAME} ${SOURCES})\nendif()\n\ninclude(GenerateExportHeader)\ngenerate_export_header(${PROJECT_NAME})\nset_property(TARGET ${PROJECT_NAME} PROPERTY VERSION ${CTRANSLATE2_VERSION})\nset_property(TARGET ${PROJECT_NAME} PROPERTY SOVERSION ${CTRANSLATE2_MAJOR_VERSION})\nset_property(TARGET ${PROJECT_NAME} PROPERTY\n  INTERFACE_${PROJECT_NAME}_MAJOR_VERSION ${CTRANSLATE2_MAJOR_VERSION})\nset_property(TARGET ${PROJECT_NAME} APPEND PROPERTY\n  COMPATIBLE_INTERFACE_STRING ${PROJECT_NAME}_MAJOR_VERSION\n)\n\nlist(APPEND LIBRARIES ${CMAKE_DL_LIBS})\ntarget_link_libraries(${PROJECT_NAME} PRIVATE ${LIBRARIES})\ntarget_include_directories(${PROJECT_NAME} BEFORE\n  PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include> $<INSTALL_INTERFACE:include>\n  PRIVATE ${PRIVATE_INCLUDE_DIRECTORIES}\n)\n\nif (WITH_TENSOR_PARALLEL AND CUDA_DYNAMIC_LOADING)\n  target_compile_options(${PROJECT_NAME} PRIVATE -DOMPI_SKIP_MPICXX)\nendif()\n\nif(BUILD_TESTS)\n  add_subdirectory(tests)\nendif()\n\ninclude(GNUInstallDirs)\n\nif (BUILD_CLI)\n  add_subdirectory(cli)\nendif()\n\ninstall(\n  TARGETS ${PROJECT_NAME} EXPORT ${PROJECT_NAME}Targets\n  RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n  ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n  LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n)\ninstall(\n  DIRECTORY \"${CMAKE_CURRENT_SOURCE_DIR}/include/\"\n  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}\n  FILES_MATCHING PATTERN \"*.h*\"\n)\n\ninclude(CMakePackageConfigHelpers)\nwrite_basic_package_version_file(\n  \"${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}/${PROJECT_NAME}ConfigVersion.cmake\"\n  VERSION ${CTRANSLATE2_VERSION}\n  COMPATIBILITY AnyNewerVersion\n)\n\nif(BUILD_SHARED_LIBS)\n  export(EXPORT ${PROJECT_NAME}Targets\n    FILE \"${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}/${PROJECT_NAME}Targets.cmake\"\n    NAMESPACE CTranslate2::\n  )\nendif()\n\nconfigure_file(cmake/${PROJECT_NAME}Config.cmake\n  \"${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}/${PROJECT_NAME}Config.cmake\"\n  COPYONLY\n)\n\nconfigure_file(cmake/FindNCCL.cmake\n  \"${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}/FindNCCL.cmake\"\n  COPYONLY\n)\n\nset(ConfigPackageLocation ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME})\n\nif(BUILD_SHARED_LIBS)\n  install(EXPORT ${PROJECT_NAME}Targets\n    FILE\n      ${PROJECT_NAME}Targets.cmake\n    NAMESPACE\n      CTranslate2::\n    DESTINATION\n      ${ConfigPackageLocation}\n  )\nendif()\n\ninstall(\n  FILES\n    cmake/${PROJECT_NAME}Config.cmake\n    cmake/FindNCCL.cmake\n    \"${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}/${PROJECT_NAME}ConfigVersion.cmake\"\n  DESTINATION\n    ${ConfigPackageLocation}\n  COMPONENT\n    Devel\n)\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 8.216796875,
          "content": "# Contributing\n\nThis document provides some information to help you contribute to the CTranslate2.\n\n## Reporting issues\n\nWe use GitHub issues for bugs in the code that are **reproducible**. A good bug report should contain every information needed to reproduce it. Before opening a new issue, make sure to:\n\n* **use the GitHub issue search** for existing and fixed bugs;\n* **check if the issue has been fixed** in a more recent version;\n* **isolate the problem** to give as much context as possible.\n\nIf you have questions on how to use the project or have trouble getting started with it, consider using [our forum](https://forum.opennmt.net/) instead and tagging your topic with the *ctranslate2* tag.\n\n## Requesting features\n\nDo you think a feature is missing or would be a great addition to the project? Please open a GitHub issue to describe it.\n\n## Developing code\n\n* If you want to contribute with code but are unsure what to do,\n  * search for *TODO* comments in the code: these are small dev tasks that should be addressed at some point.\n  * look for GitHub issues marked with the *help wanted* label: these are developments that we find particularly suited for community contributions.\n* If you are planning to make a large change to the existing code, consider asking first on [the forum](https://forum.opennmt.net/) to confirm that it is welcome.\n\n### Building the sources\n\nSee [Install from sources](https://opennmt.net/CTranslate2/installation.html#install-from-sources).\n\n### Running the tests\n\n#### C++\n\nTo enable the C++ tests, you should configure the project with `cmake -DBUILD_TESTS=ON`. The binary `tests/ctranslate2_test` runs all tests using [Google Test](https://github.com/google/googletest). It expects the path to the test data as argument:\n\n```bash\n./tests/ctranslate2_test ../tests/data\n```\n\n#### Python\n\nThe Python tests can be run with `pytest`:\n\n```bash\ncd python\npip install -r tests/requirements.txt\npytest tests/\n```\n\nThe code should also be checked with `black` (automatic formatting), `isort` (imports ordering), and `flake8` (code checking):\n\n```bash\nblack .\nisort .\nflake8 .\n```\n\n### Measuring performance\n\nYou should make sure that new changes do not negatively impact the general performance. The translation client has some options to measure the performance.\n\n#### Translation throughput\n\nThe command line option `--log_throughput` reports the *tokens generated per second* on the standard error output. This is the recommended metric to compare different runs (higher is better).\n\n#### Execution profile\n\nThe command line option `--log_profiling` reports an execution profile on the standard error output. It prints a list of selected functions in the format:\n\n```text\n  2.51%  80.38%  87.27% beam_search                 557.00ms\n```\n\nwhere the columns mean:\n\n1. Percent of time spent in the function\n2. Percent of time spent in the function and its callees\n3. Percent of time printed so far\n4. Name of the function\n5. Time spent in the function (in milliseconds)\n\nThe list is ordered on 5. from the largest to smallest time.\n\n### Implementation details\n\n#### `StorageView` class\n\nCTranslate2 uses [row-major](https://en.wikipedia.org/wiki/Row-_and_column-major_order) storages, usually encapsulated in the `StorageView` class. This class acts like a tensor representation but without the mathematical semantics. It is convenience wrapper to view a buffer of data in a particular shape, and provides methods to resize, reshape, and copy data. The underlying storage has a type (e.g. `float`) and a location (e.g. GPU #1) which are both resolved at runtime.\n\nTo maximize performance, the implementation avoid new allocations when possible:\n\n* no reallocation occurs when resizing the storage to a smaller size\n* caching allocators are used to reuse previously allocated buffers\n\n#### Abstraction levels\n\nFrom lowest to highest level:\n\n* *kernels*: low-level compute functions (e.g. CUDA implementation of Softmax)\n* *primitives*: basic vector and matrix processing functions (e.g. addition of two C arrays)\n* *ops*: neural network operations (e.g. Softmax, Gemm, etc.)\n* *layers*: stateful neural network layers (e.g. `Dense`, `LayerNorm`, etc.)\n* *models*: collection of neural network layers and weights (e.g. `Transformer`)\n* *replicas*: runnable instances of a model\n* *replicas pool*: thread pool of model replicas\n\n#### Ops\n\nOps define the basic neural network operations. The op interface is sometimes inspired by the [ONNX specification](https://github.com/onnx/onnx/blob/master/docs/Operators.md).\n\nTheir implementation typically require multiple source files:\n\n```text\ninclude/ctranslate2/ops/my_op.h  # Op interface\nsrc/ops/my_op.cc                 # Input checks and dispatch based on device and type.\nsrc/ops/my_op_cpu.cc             # CPU-specific implementation\nsrc/ops/my_op_gpu.cu             # CUDA-specific implementation\n```\n\nIn particular, no compilation flags should be used in the header file to make it easy to use the project as a library.\n\n## Maintenance\n\n### Updating the Python build matrix\n\nBinary Python wheels are built for multiple Python versions in the `build-python-wheels` GitHub Actions job. The list of Python versions is defined by the intersection of:\n\n* `python_requires` in file `python/setup.py`\n* the default build list in [`cibuildwheel`](https://github.com/pypa/cibuildwheel)\n\nBuilding wheels for a new Python version usually means updating the `cibuildwheel` version in `.github/workflows/ci.yml`. See for example commit [8f4c7ade1](https://github.com/OpenNMT/CTranslate2/commit/8f4c7ade14cba114c8acad2cc700edc1704c8396).\n\n### CUDA support in Python wheels\n\nPython wheels for Linux and Windows are compiled against NVIDIA libraries to support GPU execution.\n\nTo limit the size of the packages pushed to PyPI, some libraries are not included in the package and are dynamically loaded at runtime with `dlopen` (or `LoadLibraryA` on Windows).\n\n* `libcudart_static.a` (statically linked)\n* `libcublas.so.12` (dlopened at runtime in [`cublas_stub.cc`](https://github.com/OpenNMT/CTranslate2/blob/master/src/cuda/cublas_stub.cc))\n* `libcudnn.so.8` (dynamically linked)\n  * `libcudnn_ops_infer.so.8` (dlopened at runtime by `libcudnn.so.8`)\n  * `libcudnn_cnn_infer.so.8` (dlopened at runtime by `libcudnn.so.8`)\n\nOne of the benefits of this dynamic loading is that multiple versions of cuBLAS and cuDNN are supported by the same binary. In particular, users can install any CUDA 12.x version as long as it provides `libcublas.so.12`.\n\nThe Python library only support CUDA 12.x. C++ source code is always compatible with CUDA 11, possible to use CUDA 11 libraries during compilation to create CUDA 11.x support wheel.\n\n### Updating other dependencies\n\nUpdating dependencies such as oneMKL or oneDNN can fix security issues, improve performance, or enable new features. Most dependencies were already updated at least once. Search the commit history to see how it was done before.\n\nIf a dependency needs an update, it is particularly important that it is updated consistently for all binaries and platforms where it is used.\n\n## Release procedure\n\n1. Add the release note for the new version in `CHANGELOG.md`\n1. Update the version number in `python/ctranslate2/version.py`\n1. Tag the latest commit in the format `vX.Y.Z`\n1. When the release pipeline is finished, create a new release on GitHub and copy the note from `CHANGELOG.md`\n\n### Managing PyPI project size limit\n\nProjects on PyPI have a size limit. The default limit is 10GB and [we already requested](https://github.com/pypi/support/issues/1480) an increase to 20GB in the past. Because increase requests can take several months to be accepted, we now try to work with this 20GB limit.\n\nSo older releases need to be regularly deleted on PyPI to make room for new releases. **However, make sure to keep the latest release of each major version.**\n\nHere's the process to delete a release:\n\n1. Download the releases to be deleted (see an example script below)\n1. Upload the wheels in the `ctranslate2-wheels` bucket of the OpenNMT AWS S3 account\n1. Delete the release on PyPI\n\n**Example script to download the wheels of a release:**\n\n```bash\n#! /bin/bash\n\nVERSION=\"1.20.0\"\n\nwget https://pypi.org/simple/ctranslate2/ -O /tmp/ctranslate2.html\nmkdir -p /tmp/wheels\ngrep -F \"$VERSION\" /tmp/ctranslate2.html | sed -E 's/.*<a href=\"([^\"]+)\".*/\\1/i' | xargs wget -P /tmp/wheels\n```\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0888671875,
          "content": "MIT License\n\nCopyright (c) 2018-     SYSTRAN.\nCopyright (c) 2019-     The OpenNMT Authors.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.4267578125,
          "content": "[![CI](https://github.com/OpenNMT/CTranslate2/workflows/CI/badge.svg)](https://github.com/OpenNMT/CTranslate2/actions?query=workflow%3ACI) [![PyPI version](https://badge.fury.io/py/ctranslate2.svg)](https://badge.fury.io/py/ctranslate2) [![Documentation](https://img.shields.io/badge/docs-latest-blue.svg)](https://opennmt.net/CTranslate2/) [![Gitter](https://badges.gitter.im/OpenNMT/CTranslate2.svg)](https://gitter.im/OpenNMT/CTranslate2?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge) [![Forum](https://img.shields.io/discourse/status?server=https%3A%2F%2Fforum.opennmt.net%2F)](https://forum.opennmt.net/)\n\n# CTranslate2\n\nCTranslate2 is a C++ and Python library for efficient inference with Transformer models.\n\nThe project implements a custom runtime that applies many performance optimization techniques such as weights quantization, layers fusion, batch reordering, etc., to [accelerate and reduce the memory usage](#benchmarks) of Transformer models on CPU and GPU.\n\nThe following model types are currently supported:\n\n* Encoder-decoder models: Transformer base/big, M2M-100, NLLB, BART, mBART, Pegasus, T5, Whisper\n* Decoder-only models: GPT-2, GPT-J, GPT-NeoX, OPT, BLOOM, MPT, Llama, Mistral, Gemma, CodeGen, GPTBigCode, Falcon, Qwen2\n* Encoder-only models: BERT, DistilBERT, XLM-RoBERTa\n\nCompatible models should be first converted into an optimized model format. The library includes converters for multiple frameworks:\n\n* [OpenNMT-py](https://opennmt.net/CTranslate2/guides/opennmt_py.html)\n* [OpenNMT-tf](https://opennmt.net/CTranslate2/guides/opennmt_tf.html)\n* [Fairseq](https://opennmt.net/CTranslate2/guides/fairseq.html)\n* [Marian](https://opennmt.net/CTranslate2/guides/marian.html)\n* [OPUS-MT](https://opennmt.net/CTranslate2/guides/opus_mt.html)\n* [Transformers](https://opennmt.net/CTranslate2/guides/transformers.html)\n\nThe project is production-oriented and comes with [backward compatibility guarantees](https://opennmt.net/CTranslate2/versioning.html), but it also includes experimental features related to model compression and inference acceleration.\n\n## Key features\n\n* **Fast and efficient execution on CPU and GPU**<br/>The execution [is significantly faster and requires less resources](#benchmarks) than general-purpose deep learning frameworks on supported models and tasks thanks to many advanced optimizations: layer fusion, padding removal, batch reordering, in-place operations, caching mechanism, etc.\n* **Quantization and reduced precision**<br/>The model serialization and computation support weights with [reduced precision](https://opennmt.net/CTranslate2/quantization.html): 16-bit floating points (FP16), 16-bit brain floating points (BF16), 16-bit integers (INT16), 8-bit integers (INT8) and AWQ quantization (INT4).\n* **Multiple CPU architectures support**<br/>The project supports x86-64 and AArch64/ARM64 processors and integrates multiple backends that are optimized for these platforms: [Intel MKL](https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html), [oneDNN](https://github.com/oneapi-src/oneDNN), [OpenBLAS](https://www.openblas.net/), [Ruy](https://github.com/google/ruy), and [Apple Accelerate](https://developer.apple.com/documentation/accelerate).\n* **Automatic CPU detection and code dispatch**<br/>One binary can include multiple backends (e.g. Intel MKL and oneDNN) and instruction set architectures (e.g. AVX, AVX2) that are automatically selected at runtime based on the CPU information.\n* **Parallel and asynchronous execution**<br/>Multiple batches can be processed in parallel and asynchronously using multiple GPUs or CPU cores.\n* **Dynamic memory usage**<br/>The memory usage changes dynamically depending on the request size while still meeting performance requirements thanks to caching allocators on both CPU and GPU.\n* **Lightweight on disk**<br/>Quantization can make the models 4 times smaller on disk with minimal accuracy loss.\n* **Simple integration**<br/>The project has few dependencies and exposes simple APIs in [Python](https://opennmt.net/CTranslate2/python/overview.html) and C++ to cover most integration needs.\n* **Configurable and interactive decoding**<br/>[Advanced decoding features](https://opennmt.net/CTranslate2/decoding.html) allow autocompleting a partial sequence and returning alternatives at a specific location in the sequence.\n* **Support tensor parallelism for distributed inference**<br/>Very large model can be split into multiple GPUs. Following this [documentation](docs/parallel.md#model-and-tensor-parallelism) to set up the required environment.\n\nSome of these features are difficult to achieve with standard deep learning frameworks and are the motivation for this project.\n\n## Installation and usage\n\nCTranslate2 can be installed with pip:\n\n```bash\npip install ctranslate2\n```\n\nThe Python module is used to convert models and can translate or generate text with few lines of code:\n\n```python\ntranslator = ctranslate2.Translator(translation_model_path)\ntranslator.translate_batch(tokens)\n\ngenerator = ctranslate2.Generator(generation_model_path)\ngenerator.generate_batch(start_tokens)\n```\n\nSee the [documentation](https://opennmt.net/CTranslate2) for more information and examples.\n\n## Benchmarks\n\nWe translate the En->De test set *newstest2014* with multiple models:\n\n* [OpenNMT-tf WMT14](https://opennmt.net/Models-tf/#translation): a base Transformer trained with OpenNMT-tf on the WMT14 dataset (4.5M lines)\n* [OpenNMT-py WMT14](https://opennmt.net/Models-py/#translation): a base Transformer trained with OpenNMT-py on the WMT14 dataset (4.5M lines)\n* [OPUS-MT](https://github.com/Helsinki-NLP/OPUS-MT-train/tree/master/models/en-de#opus-2020-02-26zip): a base Transformer trained with Marian on all OPUS data available on 2020-02-26 (81.9M lines)\n\nThe benchmark reports the number of target tokens generated per second (higher is better). The results are aggregated over multiple runs. See the [benchmark scripts](tools/benchmark) for more details and reproduce these numbers.\n\n**Please note that the results presented below are only valid for the configuration used during this benchmark: absolute and relative performance may change with different settings.**\n\n#### CPU\n\n| | Tokens per second | Max. memory | BLEU |\n| --- | --- | --- | --- |\n| **OpenNMT-tf WMT14 model** | | | |\n| OpenNMT-tf 2.31.0 (with TensorFlow 2.11.0) | 209.2 | 2653MB | 26.93 |\n| **OpenNMT-py WMT14 model** | | | |\n| OpenNMT-py 3.0.4 (with PyTorch 1.13.1) | 275.8 | 2012MB | 26.77 |\n| - int8 | 323.3 | 1359MB | 26.72 |\n| CTranslate2 3.6.0 | 658.8 | 849MB | 26.77 |\n| - int16 | 733.0 | 672MB | 26.82 |\n| - int8 | 860.2 | 529MB | 26.78 |\n| - int8 + vmap | 1126.2 | 598MB | 26.64 |\n| **OPUS-MT model** | | | |\n| Transformers 4.26.1 (with PyTorch 1.13.1) | 147.3 | 2332MB | 27.90 |\n| Marian 1.11.0 | 344.5 | 7605MB | 27.93 |\n| - int16 | 330.2 | 5901MB | 27.65 |\n| - int8 | 355.8 | 4763MB | 27.27 |\n| CTranslate2 3.6.0 | 525.0 | 721MB | 27.92 |\n| - int16 | 596.1 | 660MB | 27.53 |\n| - int8 | 696.1 | 516MB | 27.65 |\n\nExecuted with 4 threads on a [*c5.2xlarge*](https://aws.amazon.com/ec2/instance-types/c5/) Amazon EC2 instance equipped with an Intel(R) Xeon(R) Platinum 8275CL CPU.\n\n#### GPU\n\n| | Tokens per second | Max. GPU memory | Max. CPU memory | BLEU |\n| --- | --- | --- | --- | --- |\n| **OpenNMT-tf WMT14 model** | | | | |\n| OpenNMT-tf 2.31.0 (with TensorFlow 2.11.0) | 1483.5 | 3031MB | 3122MB | 26.94 |\n| **OpenNMT-py WMT14 model** | | | | |\n| OpenNMT-py 3.0.4 (with PyTorch 1.13.1) | 1795.2 | 2973MB | 3099MB | 26.77 |\n| FasterTransformer 5.3 | 6979.0 | 2402MB | 1131MB | 26.77 |\n| - float16 | 8592.5 | 1360MB | 1135MB | 26.80 |\n| CTranslate2 3.6.0 | 6634.7 | 1261MB | 953MB | 26.77 |\n| - int8 | 8567.2 | 1005MB | 807MB | 26.85 |\n| - float16 | 10990.7 | 941MB | 807MB | 26.77 |\n| - int8 + float16 | 8725.4 | 813MB | 800MB | 26.83 |\n| **OPUS-MT model** | | | | |\n| Transformers 4.26.1 (with PyTorch 1.13.1) | 1022.9 | 4097MB | 2109MB | 27.90 |\n| Marian 1.11.0 | 3241.0 | 3381MB | 2156MB | 27.92 |\n| - float16 | 3962.4 | 3239MB | 1976MB | 27.94 |\n| CTranslate2 3.6.0 | 5876.4 | 1197MB | 754MB | 27.92 |\n| - int8 | 7521.9 | 1005MB | 792MB | 27.79 |\n| - float16 | 9296.7 | 909MB | 814MB | 27.90 |\n| - int8 + float16 | 8362.7 | 813MB | 766MB | 27.90 |\n\nExecuted with CUDA 11 on a [*g5.xlarge*](https://aws.amazon.com/ec2/instance-types/g5/) Amazon EC2 instance equipped with a NVIDIA A10G GPU (driver version: 510.47.03).\n\n## Additional resources\n\n* [Documentation](https://opennmt.net/CTranslate2)\n* [Forum](https://forum.opennmt.net)\n* [Gitter](https://gitter.im/OpenNMT/CTranslate2)\n"
        },
        {
          "name": "cli",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "include",
          "type": "tree",
          "content": null
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}