{
  "metadata": {
    "timestamp": 1736565345403,
    "page": 169,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NVlabs/tiny-cuda-nn",
      "stars": 3828,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.2138671875,
          "content": "root = true\n\n[*]\nend_of_line = lf\ninsert_final_newline = true\nindent_style = tab\nindent_size = 4\ntrim_trailing_whitespace = true\n\n[*.md]\ntrim_trailing_whitespace = false\n\n[*.clangd]\nindent_style = space\nindent_size = 2\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0712890625,
          "content": ".DS_Store\n*.egg-info\n*.o\n__pycache__\nbuild*\ndist\n/.cache\n/.vscode\n/*.jpg\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.193359375,
          "content": "[submodule \"dependencies/cutlass\"]\n\tpath = dependencies/cutlass\n\turl = https://github.com/NVIDIA/cutlass\n[submodule \"dependencies/fmt\"]\n\tpath = dependencies/fmt\n\turl = https://github.com/fmtlib/fmt\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.837890625,
          "content": "cff-version: 1.2.0\ntitle: tiny-cuda-nn\nmessage: >-\n  If you use this software, please cite it using the\n  metadata from this file.\ntype: software\nauthors:\n  - given-names: Thomas\n    email: thomas94@gmx.net\n    family-names: MÃ¼ller\n    affiliation: NVIDIA\nrepository-code: 'https://github.com/NVlabs/tiny-cuda-nn'\nabstract: >-\n  This is a small, self-contained framework for training and querying neural\n  networks. Most notably, it contains a lightning fast \"fully fused\" multi-\n  layer perceptron (technical paper), a versatile multiresolution hash encoding\n  (technical paper), as well as support for various other input encodings,\n  losses, and optimizers.\nkeywords:\n  - 'neural network, tiny, tensor cores, cuda'\nlicense: BSD-3-Clause\nlicense-url: https://github.com/NVlabs/tiny-cuda-nn/blob/master/LICENSE.txt\nversion: 1.7\ndate-released: '2021-04-21'\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 11.220703125,
          "content": "# Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.\n# \n# Redistribution and use in source and binary forms, with or without modification, are permitted\n# provided that the following conditions are met:\n#     * Redistributions of source code must retain the above copyright notice, this list of\n#       conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above copyright notice, this list of\n#       conditions and the following disclaimer in the documentation and/or other materials\n#       provided with the distribution.\n#     * Neither the name of the NVIDIA CORPORATION nor the names of its contributors may be used\n#       to endorse or promote products derived from this software without specific prior written\n#       permission.\n# \n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR\n# IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND\n# FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,\n# STRICT LIABILITY, OR TOR (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\ncmake_minimum_required(VERSION 3.18)\n\nproject(\n\ttiny-cuda-nn\n\tVERSION 1.7\n\tDESCRIPTION \"Lightning fast & tiny C++/CUDA neural network framework\"\n\tLANGUAGES CXX CUDA\n)\n\noption(TCNN_BUILD_BENCHMARK \"Build tiny-cuda-nn example benchmark?\" ON)\noption(TCNN_BUILD_EXAMPLES \"Build tiny-cuda-nn example applications?\" ON)\noption(TCNN_ALLOW_CUBLAS_CUSOLVER \"Allows tiny-cuda-nn to use cuBLAS and cuSolver. Only required for the Shampoo optimizer.\" OFF)\nset(TCNN_CUDA_ARCHITECTURES \"\" CACHE STRING \"Build tiny-cuda-nn for a specific GPU architecture.\")\n\n###############################################################################\n# Build type and C++ compiler setup\n###############################################################################\n\n# Set a default configuration if none was specified\nif (NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)\n\tmessage(STATUS \"No release type specified. Setting to 'Release'.\")\n\tset(CMAKE_BUILD_TYPE Release CACHE STRING \"Choose the type of build.\" FORCE)\n\tset_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"RelWithDebInfo\")\nendif()\n\nif (NOT EXISTS \"${CMAKE_CURRENT_SOURCE_DIR}/dependencies/cutlass/CMakeLists.txt\")\n\tmessage(FATAL_ERROR\n\t\t\"Some tiny-cuda-nn dependencies are missing. \"\n\t\t\"If you forgot the \\\"--recursive\\\" flag when cloning this project, \"\n\t\t\"this can be fixed by calling \\\"git submodule update --init --recursive\\\".\"\n\t)\nendif()\n\nif (APPLE)\n\tset(CMAKE_MACOSX_RPATH ON)\nendif()\n\nif (MSVC)\n\tset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /D_CRT_SECURE_NO_WARNINGS\")\n\tset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /MP\")\nendif()\n\nset(CMAKE_CXX_STANDARD 14)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_EXTENSIONS OFF)\n\n###############################################################################\n# CUDA compiler setup\n###############################################################################\n\n# Figure out CUDA version\nif(CMAKE_CUDA_COMPILER_LOADED)\n\tif (CMAKE_CUDA_COMPILER_ID STREQUAL \"NVIDIA\" AND CMAKE_CUDA_COMPILER_VERSION MATCHES \"^([0-9]+\\\\.[0-9]+)\")\n\t\tset(CUDA_VERSION \"${CMAKE_MATCH_1}\")\n\tendif()\nendif()\n\n# Adapted from the CMake source code at https://github.com/Kitware/CMake/blob/master/Modules/FindCUDA/select_compute_arch.cmake\n# Simplified to return a semicolon-separated list of the compute capabilities of installed devices\nfunction(TCNN_AUTODETECT_CUDA_ARCHITECTURES OUT_VARIABLE)\n\tif (NOT TCNN_AUTODETECT_CUDA_ARCHITECTURES_OUTPUT)\n\t\tif (CMAKE_CUDA_COMPILER_LOADED) # CUDA as a language\n\t\t\tset(file \"${PROJECT_BINARY_DIR}/detect_tcnn_cuda_architectures.cu\")\n\t\telse()\n\t\t\tset(file \"${PROJECT_BINARY_DIR}/detect_tcnn_cuda_architectures.cpp\")\n\t\tendif()\n\n\t\tfile(WRITE ${file} \"\"\n\t\t\t\"#include <cuda_runtime.h>\\n\"\n\t\t\t\"#include <cstdio>\\n\"\n\t\t\t\"int main() {\\n\"\n\t\t\t\"\tint count = 0;\\n\"\n\t\t\t\"\tif (cudaSuccess != cudaGetDeviceCount(&count)) return -1;\\n\"\n\t\t\t\"\tif (count == 0) return -1;\\n\"\n\t\t\t\"\tfor (int device = 0; device < count; ++device) {\\n\"\n\t\t\t\"\t\tcudaDeviceProp prop;\\n\"\n\t\t\t\"\t\tif (cudaSuccess == cudaGetDeviceProperties(&prop, device)) {\\n\"\n\t\t\t\"\t\t\tstd::printf(\\\"%d%d\\\", prop.major, prop.minor);\\n\"\n\t\t\t\"\t\t\tif (device < count - 1) std::printf(\\\";\\\");\\n\"\n\t\t\t\"\t\t}\\n\"\n\t\t\t\"\t}\\n\"\n\t\t\t\"\treturn 0;\\n\"\n\t\t\t\"}\\n\"\n\t\t)\n\n\t\ttry_run(run_result compile_result ${PROJECT_BINARY_DIR} ${file} RUN_OUTPUT_VARIABLE compute_capabilities)\n\t\tif (run_result EQUAL 0)\n\t\t\t# If the user has multiple GPUs with the same compute capability installed, list that capability only once.\n\t\t\tlist(REMOVE_DUPLICATES compute_capabilities)\n\t\t\tset(TCNN_AUTODETECT_CUDA_ARCHITECTURES_OUTPUT ${compute_capabilities} CACHE INTERNAL \"Returned GPU architectures from detect_gpus tool\" FORCE)\n\t\tendif()\n\tendif()\n\n\tif (NOT TCNN_AUTODETECT_CUDA_ARCHITECTURES_OUTPUT)\n\t\tmessage(STATUS \"Automatic GPU detection failed. Building for Turing and Ampere as a best guess.\")\n\t\tset(${OUT_VARIABLE} \"75;86\" PARENT_SCOPE)\n\telse()\n\t\tset(${OUT_VARIABLE} ${TCNN_AUTODETECT_CUDA_ARCHITECTURES_OUTPUT} PARENT_SCOPE)\n\tendif()\nendfunction()\n\nset(CMAKE_CUDA_STANDARD 14)\nset(CMAKE_CUDA_STANDARD_REQUIRED ON)\nset(CMAKE_CUDA_EXTENSIONS OFF)\nset(CUDA_LINK_LIBRARIES_KEYWORD PUBLIC)\n\nget_directory_property(TCNN_HAS_PARENT PARENT_DIRECTORY)\nif (DEFINED ENV{TCNN_CUDA_ARCHITECTURES})\n\tmessage(STATUS \"Obtained CUDA architectures from environment variable TCNN_CUDA_ARCHITECTURES=$ENV{TCNN_CUDA_ARCHITECTURES}\")\n\tset(CMAKE_CUDA_ARCHITECTURES $ENV{TCNN_CUDA_ARCHITECTURES})\nelseif (TCNN_CUDA_ARCHITECTURES)\n\tmessage(STATUS \"Obtained CUDA architectures from CMake variable TCNN_CUDA_ARCHITECTURES=${TCNN_CUDA_ARCHITECTURES}\")\n\tset(CMAKE_CUDA_ARCHITECTURES ${TCNN_CUDA_ARCHITECTURES})\nelse()\n\tmessage(STATUS \"Obtained CUDA architectures automatically from installed GPUs\")\n\tTCNN_AUTODETECT_CUDA_ARCHITECTURES(CMAKE_CUDA_ARCHITECTURES)\nendif()\n\n# If the CUDA version does not support the chosen architecture, target\n# the latest supported one instead.\nif (CUDA_VERSION VERSION_LESS 11.0)\n\tset(LATEST_SUPPORTED_CUDA_ARCHITECTURE 75)\nelseif (CUDA_VERSION VERSION_LESS 11.1)\n\tset(LATEST_SUPPORTED_CUDA_ARCHITECTURE 80)\nelseif (CUDA_VERSION VERSION_LESS 11.8)\n\tset(LATEST_SUPPORTED_CUDA_ARCHITECTURE 86)\nelse()\n\tset(LATEST_SUPPORTED_CUDA_ARCHITECTURE 90)\nendif()\n\nif (CUDA_VERSION VERSION_GREATER_EQUAL 12.0)\n\tset(EARLIEST_SUPPORTED_CUDA_ARCHITECTURE 50)\nelse()\n\tset(EARLIEST_SUPPORTED_CUDA_ARCHITECTURE 20)\nendif()\n\nforeach (CUDA_CC IN LISTS CMAKE_CUDA_ARCHITECTURES)\n\tif (CUDA_CC GREATER ${LATEST_SUPPORTED_CUDA_ARCHITECTURE})\n\t\tmessage(WARNING \"CUDA version ${CUDA_VERSION} is too low for detected architecture ${CUDA_CC}. Targeting the highest supported architecture ${LATEST_SUPPORTED_CUDA_ARCHITECTURE} instead.\")\n\t\tlist(REMOVE_ITEM CMAKE_CUDA_ARCHITECTURES ${CUDA_CC})\n\t\tif (NOT CMAKE_CUDA_ARCHITECTURES)\n\t\t\tlist(APPEND CMAKE_CUDA_ARCHITECTURES ${LATEST_SUPPORTED_CUDA_ARCHITECTURE})\n\t\tendif()\n\tendif()\n\n\tif (CUDA_CC LESS ${EARLIEST_SUPPORTED_CUDA_ARCHITECTURE})\n\t\tmessage(ERROR \"CUDA version ${CUDA_VERSION} no longer supports detected architecture ${CUDA_CC}. Targeting the lowest supported architecture ${EARLIEST_SUPPORTED_CUDA_ARCHITECTURE} instead.\")\n\t\tlist(REMOVE_ITEM CMAKE_CUDA_ARCHITECTURES ${CUDA_CC})\n\t\tif (NOT CMAKE_CUDA_ARCHITECTURES)\n\t\t\tlist(APPEND CMAKE_CUDA_ARCHITECTURES ${EARLIEST_SUPPORTED_CUDA_ARCHITECTURE})\n\t\tendif()\n\tendif()\nendforeach(CUDA_CC)\n\nif (NOT CMAKE_CUDA_ARCHITECTURES)\n\tlist(APPEND CMAKE_CUDA_ARCHITECTURES ${LATEST_SUPPORTED_CUDA_ARCHITECTURE})\nendif()\n\n# Sort the list to obtain lowest architecture that must be compiled for.\nlist(SORT CMAKE_CUDA_ARCHITECTURES COMPARE NATURAL ORDER ASCENDING)\nlist(GET CMAKE_CUDA_ARCHITECTURES 0 MIN_GPU_ARCH)\n\nstring(REPLACE \"-virtual\" \"\" MIN_GPU_ARCH \"${MIN_GPU_ARCH}\")\n\nmessage(STATUS \"Targeting CUDA architectures: ${CMAKE_CUDA_ARCHITECTURES}\")\nif (TCNN_HAS_PARENT)\n\tset(TCNN_CUDA_ARCHITECTURES ${CMAKE_CUDA_ARCHITECTURES} PARENT_SCOPE)\n\tset(TCNN_CUDA_VERSION ${CUDA_VERSION} PARENT_SCOPE)\nendif()\n\nif (MIN_GPU_ARCH LESS_EQUAL 70)\n\tmessage(WARNING\n\t\t\"Fully fused MLPs do not support GPU architectures of 70 or less. \"\n\t\t\"Falling back to CUTLASS MLPs. Remove GPU architectures 70 and lower \"\n\t\t\"to allow maximum performance\"\n\t)\nendif()\n\nif (CUDA_VERSION VERSION_LESS 10.2)\n\tmessage(FATAL_ERROR \"CUDA version too low. tiny-cuda-nn require CUDA 10.2 or higher.\")\nendif()\n\nlist(APPEND TCNN_LIBRARIES cuda)\nlist(APPEND TCNN_DEFINITIONS -DTCNN_MIN_GPU_ARCH=${MIN_GPU_ARCH})\nif (TCNN_ALLOW_CUBLAS_CUSOLVER AND CUDA_VERSION VERSION_GREATER_EQUAL 11.0)\n\t# Only compile the shampoo optimizer if\n\t# a new enough cuBLAS version is available.\n\tlist(APPEND TCNN_LIBRARIES cublas)\n\tlist(APPEND TCNN_DEFINITIONS -DTCNN_SHAMPOO)\nendif()\n\nif (TCNN_HAS_PARENT)\n\tset(TCNN_DEFINITIONS ${TCNN_DEFINITIONS} PARENT_SCOPE)\nendif()\n\nif (MSVC)\n\tlist(APPEND CUDA_NVCC_FLAGS \"-Xcompiler=/bigobj\")\nelse()\n\tlist(APPEND CUDA_NVCC_FLAGS \"-Xcompiler=-Wno-float-conversion\")\n\tlist(APPEND CUDA_NVCC_FLAGS \"-Xcompiler=-fno-strict-aliasing\")\n\tlist(APPEND CUDA_NVCC_FLAGS \"-Xcudafe=--diag_suppress=unrecognized_gcc_pragma\")\nendif()\nlist(APPEND CUDA_NVCC_FLAGS \"--extended-lambda\")\nlist(APPEND CUDA_NVCC_FLAGS \"--expt-relaxed-constexpr\")\n\n\n###############################################################################\n# Dependencies\n###############################################################################\n\nif (NOT MSVC)\n\tset(CUDA_TOOLKIT_ROOT_DIR /opt/cuda/targets/x86_64-linux)\nendif()\n\nset(BUILD_SHARED_LIBS OFF)\nadd_subdirectory(\"dependencies/fmt\")\n\n###############################################################################\n# tiny-cuda-nn library, samples, and benchmarks\n###############################################################################\n\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY_RELEASE ${CMAKE_BINARY_DIR})\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY_RELWITHDEBINFO ${CMAKE_BINARY_DIR})\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY_MINSIZEREL ${CMAKE_BINARY_DIR})\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY_DEBUG ${CMAKE_BINARY_DIR})\n\nset(TCNN_SOURCES\n\tsrc/common_host.cu\n\tsrc/cpp_api.cu\n\tsrc/cutlass_mlp.cu\n\tsrc/encoding.cu\n\tsrc/loss.cu\n\tsrc/network.cu\n\tsrc/object.cu\n\tsrc/optimizer.cu\n\tsrc/reduce_sum.cu\n)\n\nif (MIN_GPU_ARCH GREATER 70)\n\tlist(APPEND TCNN_SOURCES src/fully_fused_mlp.cu)\nendif()\n\n###############################################################################\n# Linker / library\n###############################################################################\n\nadd_library(tiny-cuda-nn STATIC ${TCNN_SOURCES})\ntarget_compile_definitions(tiny-cuda-nn PUBLIC ${TCNN_DEFINITIONS})\ntarget_compile_options(tiny-cuda-nn PUBLIC $<$<COMPILE_LANGUAGE:CUDA>:${CUDA_NVCC_FLAGS}>)\ntarget_include_directories(tiny-cuda-nn PUBLIC\n\t\"include\"\n\t\"dependencies\"\n\t\"dependencies/cutlass/include\"\n\t\"dependencies/cutlass/tools/util/include\"\n\t\"dependencies/fmt/include\"\n)\ntarget_link_libraries(tiny-cuda-nn PUBLIC ${CUDA_LIBRARIES} ${TCNN_LIBRARIES} fmt)\n\nif (TCNN_BUILD_EXAMPLES)\n\tadd_subdirectory(\"samples\")\nendif()\nif (TCNN_BUILD_BENCHMARK)\n\tadd_subdirectory(\"benchmarks/image\")\nendif()\n"
        },
        {
          "name": "DOCUMENTATION.md",
          "type": "blob",
          "size": 14.8349609375,
          "content": "# JSON Configuration Documentation\n\nThis document lists the JSON parameters of all components of __tiny-cuda-nn__.\n\nFor each component, we provide a sample configuration that lists each parameter's default value.\n\n## Networks\n\n### Activation Functions\n\nActivation functions are specified by string, e.g. as follows:\n```json5\n{\n\t\"activation\": \"ReLU\",\n}\n```\n\nThe following activation functions are supported:\n- `\"None\"` (identity)\n- `\"ReLU\"`\n- `\"LeakyReLU\"` (defined as `max(0, x) + 0.01 * min(0, x)`)\n- `\"Exponential\"`\n- `\"Sine\"`\n- `\"Sigmoid\"` (the logistic function)\n- `\"Squareplus\"` (defined as `X = 10*x; 0.5 * (X + sqrt(X*X + 4)) / 10`)\n- `\"Softplus\"` (defined as `X = 10*x; log(exp(X) + 1) / 10`)\n- `\"Tanh\"` (defined as `(exp(x) - exp(-x)) / (exp(x) + exp(-x))`)\n\nThe factor and divisor `10` in the `Squareplus` and `Softplus` activations can be thought of as \"zooming out\" such that these smooth activations more closely resembly the ReLU. If this is undesired in your use case, you can change the compile-time constant `K_ACT` in `include/tiny-cuda-nn/common_device.h`.\n\n### Fully Fused MLP\n\nLightning fast implementation of small multi-layer perceptrons (MLPs). Restricted to hidden layers of size 16, 32, 64, or 128.\n\n```json5\n{\n\t\"otype\": \"FullyFusedMLP\",    // Component type.\n\t\"activation\": \"ReLU\",        // Activation of hidden layers.\n\t\"output_activation\": \"None\", // Activation of the output layer.\n\t\"n_neurons\": 128,            // Neurons in each hidden layer.\n\t                             // May only be 16, 32, 64, or 128.\n\t\"n_hidden_layers\": 5,        // Number of hidden layers.\n}\n```\n\n### CUTLASS MLP\n\nMulti-layer perceptron (MLP) based on [CUTLASS](https://github.com/NVIDIA/cutlass)' GEMM routines. Slower than the fully fused MLP, but allows for arbitrary numbers of hidden and output neurons. Like the fully fused MLP, it outperforms TensorFlow for small networks.\n\n```json5\n{\n\t\"otype\": \"CutlassMLP\",       // Component type.\n\t\"activation\": \"ReLU\",        // Activation of hidden layers.\n\t\"output_activation\": \"None\", // Activation of the output layer.\n\t\"n_neurons\": 128,            // Neurons in each hidden layer.\n\t\"n_hidden_layers\": 5         // Number of hidden layers.\n}\n```\n\n## Encodings\n\n\n### Composite\n\nAllows composing multiple encodings. The following example replicates the Neural Radiance Caching [[MÃ¼ller et al. 2021]](https://tom94.net/data/publications/mueller21realtime/mueller21realtime.pdf) encoding by composing the `TriangleWave` encoding for the first 3 (spatial) dimensions, the `OneBlob` encoding for the following 5 non-linear appearance dimensions, and the `Identity` for all remaining dimensions.\n\n```json5\n{\n\t\"otype\": \"Composite\",\n\t\"nested\": [\n\t\t{\n\t\t\t\"n_dims_to_encode\": 3, // Spatial dims\n\t\t\t\"otype\": \"TriangleWave\",\n\t\t\t\"n_frequencies\": 12\n\t\t},\n\t\t{\n\t\t\t\"n_dims_to_encode\": 5, // Non-linear appearance dims.\n\t\t\t\"otype\": \"OneBlob\",\n\t\t\t\"n_bins\": 4\n\t\t},\n\t\t{\n\t\t\t// Number of remaining linear dims is automatically derived\n\t\t\t\"otype\": \"Identity\"\n\t\t}\n\t]\n}\n```\n\n### Frequency\n\nFrom NeRF [[Mildenhall et al. 2020]](https://www.matthewtancik.com/nerf). Works better than OneBlob encoding if the dynamic range of the encoded dimension is high. However, suffers from stripe artifacts.\n\nThe number of encoded dimensions is twice the specified number of frequencies for each input dimension. E.g. with `n_frequencies == 4`, an input dimension `x` becomes `sin(Ïx), cos(Ïx), sin(2Ïx), cos(2Ïx), sin(4Ïx), cos(4Ïx), sin(8Ïx), cos(8Ïx)`.\n\nNote that many NeRF implementations (including the official ones) omit the factor of `Ï` from eq. (4) of the paper. This makes little difference in practice as coordinate normalization usually differs by similar amounts. Due to the logarithmic scaling of this encoding, this means that one or two fewer or additional frequency bands might be required to match results across implementations.\n\n```json5\n{\n\t\"otype\": \"Frequency\", // Component type.\n\t\"n_frequencies\": 12   // Number of frequencies (sin & cos)\n\t                      // per encoded dimension.\n}\n```\n\n### Grid\n\nEncoding based on trainable multiresolution grids.\nUsed for [Instant Neural Graphics Primitives [MÃ¼ller et al. 2022]](https://nvlabs.github.io/instant-ngp/). The grids can be backed by hashtables, dense storage, or tiled storage.\n\nThe number of encoded dimensions is `n_levels * n_features_per_level`.\n\n```json5\n{\n\t\"otype\": \"Grid\",           // Component type.\n\t\"type\": \"Hash\",            // Type of backing storage of the\n\t                           // grids. Can be \"Hash\", \"Tiled\"\n\t                           // or \"Dense\".\n\t\"n_levels\": 16,            // Number of levels (resolutions)\n\t\"n_features_per_level\": 2, // Dimensionality of feature vector\n\t                           // stored in each level's entries.\n\t\"log2_hashmap_size\": 19,   // If type is \"Hash\", is the base-2\n\t                           // logarithm of the number of elements\n\t                           // in each backing hash table.\n\t\"base_resolution\": 16,     // The resolution of the coarsest le-\n\t                           // vel is base_resolution^input_dims.\n\t\"per_level_scale\": 2.0,    // The geometric growth factor, i.e.\n\t                           // the factor by which the resolution\n\t                           // of each grid is larger (per axis)\n\t                           // than that of the preceding level.\n\t\"interpolation\": \"Linear\"  // How to interpolate nearby grid\n\t                           // lookups. Can be \"Nearest\", \"Linear\",\n\t                           // or \"Smoothstep\" (for smooth deri-\n\t                           // vatives).\n}\n```\n\n### Identity\n\nLeaves values untouched. Optionally, multiplies each dimension by a scalar and adds an offset.\n\n```json5\n{\n\t\"otype\": \"Identity\", // Component type.\n\t\"scale\": 1.0,        // Scaling of each encoded dimension.\n\t\"offset\": 0.0        // Added to each encoded dimension.\n}\n```\n\n### OneBlob\n\nFrom Neural Importance Sampling [[MÃ¼ller et al. 2019]](https://tom94.net/data/publications/mueller18neural/mueller18neural-v4.pdf) and Neural Control Variates [[MÃ¼ller et al. 2020]](https://tom94.net/data/publications/mueller20neural/mueller20neural.pdf). When the dynamic range of the encoded dimension is limited, it results in a more accurate fit than the identity encoding while not suffering from stripe artifacts like the Frequency encoding.\n\nFor performance reasons, the encoding uses a quartic kernel rather than a Gaussian kernel to compute blob integrals. We measured no loss of reconstruction quality.\n\n```json5\n{\n\t\"otype\": \"OneBlob\", // Component type.\n\t\"n_bins\": 16        // Number of bins per encoded dimension.\n}\n```\n\n### Spherical Harmonics\n\nA frequency-space encoding that is more suitable to direction vectors than component-wise `Frequency` or `TriangleWave` encodings.\nExpects 3D inputs that represent normalized vectors `v` transformed into the unit cube as `(v+1)/2`.\n\nThe number of encoded dimensions is the degree squared.\n\n```json5\n{\n\t\"otype\": \"SphericalHarmonics\", // Component type.\n\t\"degree\": 4                    // The SH degree up to which\n\t                               // to evaluate the encoding.\n\t                               // Produces degree^2 encoded\n\t                               // dimensions.\n}\n```\n\n### TriangleWave\n\nSimilar to the `Frequency` encoding, but replaces the sine function with a cheaper-to-compute triangle wave. Also omits the cosine function. Proposed in [[MÃ¼ller et al. 2021]](https://tom94.net/data/publications/mueller21realtime/mueller21realtime.pdf). Works better than OneBlob encoding if the dynamic range of the encoded dimension is high. However, suffers from stripe artifacts.\n\nThe number of encoded dimensions is the specified number of frequencies for each input dimension.\n\n```json5\n{\n\t\"otype\": \"TriangleWave\", // Component type.\n\t\"n_frequencies\": 12      // Number of frequencies (triwave)\n\t                         // per encoded dimension.\n}\n```\n\n\n## Losses\n\n### L1\n\nStandard L1 loss.\n\n```json5\n{\n\t\"otype\": \"L1\" // Component type.\n}\n```\n\n### Relative L1\n\nRelative L1 loss normalized by the network prediction.\n\n```json5\n{\n\t\"otype\": \"RelativeL1\" // Component type.\n}\n```\n\n### MAPE\n\nMean absolute percentage error (MAPE). The same as Relative L1, but normalized by the target.\n\n```json5\n{\n\t\"otype\": \"MAPE\" // Component type.\n}\n```\n\n### SMAPE\n\nSymmetric mean absolute percentage error (SMAPE). The same as Relative L1, but normalized by the mean of the prediction and the target.\n\n```json5\n{\n\t\"otype\": \"SMAPE\" // Component type.\n}\n```\n\n### L2\n\nStandard L2 loss.\n\n```json5\n{\n\t\"otype\": \"L2\" // Component type.\n}\n```\n\n### Relative L2\n\nRelative L2 loss normalized by the network prediction [[Lehtinen et al. 2018]](https://github.com/NVlabs/noise2noise).\n\n```json5\n{\n\t\"otype\": \"RelativeL2\" // Component type.\n}\n```\n\n### Relative L2 Luminance\n\nSame as above, but normalized by the luminance of the network prediction. Only applicable when network prediction is RGB. Used in Neural Radiance Caching [[MÃ¼ller et al. 2021]](https://tom94.net/data/publications/mueller21realtime/mueller21realtime.pdf).\n\n```json5\n{\n\t\"otype\": \"RelativeL2Luminance\" // Component type.\n}\n```\n\n### Cross Entropy\n\nStandard cross entropy loss. Only applicable when the network prediction is a probability density function.\n\n```json5\n{\n\t\"otype\": \"CrossEntropy\" // Component type.\n}\n```\n\n### Variance\n\nStandard variance loss. Only applicable when the network prediction is a probability density function.\n\n```json5\n{\n\t\"otype\": \"Variance\" // Component type.\n}\n```\n\n\n## Optimizers\n\n### Adam\n\nImplementation of Adam [[Kingma and Ba 2014]](https://arxiv.org/abs/1412.6980), generalized to AdaBound [[Luo et al. 2019]](https://github.com/Luolc/AdaBound).\n\n```json5\n{\n\t\"otype\": \"Adam\",       // Component type.\n\t\"learning_rate\": 1e-3, // Learning rate.\n\t\"beta1\": 0.9,          // Beta1 parameter of Adam.\n\t\"beta2\": 0.999,        // Beta2 parameter of Adam.\n\t\"epsilon\": 1e-8,       // Epsilon parameter of Adam.\n\t\"l2_reg\": 1e-8,        // Strength of L2 regularization\n\t                       // applied to the to-be-optimized params.\n\t\"relative_decay\": 0,   // Percentage of weights lost per step.\n\t\"absolute_decay\": 0,   // Amount of weights lost per step.\n\t\"adabound\": false      // Whether to enable AdaBound.\n}\n```\n\n### Novograd\n\nImplementation of Novograd [[Ginsburg et al. 2019]](https://arxiv.org/abs/1905.11286).\n\n```json5\n{\n\t\"otype\": \"Novograd\",   // Component type.\n\t\"learning_rate\": 1e-3, // Learning rate.\n\t\"beta1\": 0.9,          // Beta1 parameter of Novograd.\n\t\"beta2\": 0.999,        // Beta2 parameter of Novograd.\n\t\"epsilon\": 1e-8,       // Epsilon parameter of Novograd.\n\t\"relative_decay\": 0,   // Percentage of weights lost per step.\n\t\"absolute_decay\": 0    // Amount of weights lost per step.\n}\n```\n\n### Stochastic Gradient Descent (SGD)\n\nStandard stochastic gradient descent (SGD).\n\n```json5\n{\n\t\"otype\": \"SGD\",        // Component type.\n\t\"learning_rate\": 1e-3, // Learning rate.\n\t\"l2_reg\": 1e-8         // Strength of L2 regularization.\n}\n```\n\n### Shampoo\n\nImplementation of the 2nd order Shampoo optimizer [[Gupta et al. 2018]](https://arxiv.org/abs/1802.09568) with home-grown optimizations as well as those by [Anil et al. [2020]](https://arxiv.org/abs/2002.09018).\n\n```json5\n{\n\t\"otype\": \"Shampoo\",              // Component type.\n\t\"learning_rate\": 1e-3,           // Learning rate.\n\t\"beta1\": 0.9,                    // Beta1 parameter similar to Adam.\n\t                                 // Used to exponentially average the\n\t                                 // first gradient moment.\n\t\"beta2\": 0.99,                   // Beta2 parameter similar to Adam.\n\t                                 // Used to exponentially average the\n\t                                 // second gradient moment.\n\t\"beta3\": 0.9,                    // Used to exponentially average L and R.\n\t\"beta_shampoo\": 0.9,             // Used to exponentially average\n\t                                 // Shampoo updates.\n\t\"epsilon\": 1e-8,                 // Epsilon parameter similar Adam.\n\t                                 // Used to avoid singularity when computing\n\t                                 // momentum.\n\t\"identity\": 0.01,                // Blends L and R with I*identity for\n\t                                 // numerical stability.\n\t\"cg_on_momentum\": true,          // Whether to estimate L and R from the\n\t                                 // estimated momentum or from the raw\n\t                                 // gradients.\n\t\"l2_reg\": 1e-5,                  // Strength of L2 regularization\n\t                                 // applied to the to-be-optimized params.\n\t\"relative_decay\": 0,             // Percentage of weights lost per step.\n\t\"absolute_decay\": 0,             // Amount of weights lost per step.\n\t\"frobenius_normalization\": true, // Whether to normalize update\n\t                                 // steps by the would-be Adam\n\t                                 // update's Frobenius norm.\n}\n```\n\n### Average\n\nWraps another optimizer and computes a linear average of the optimized parameters over the last N training steps. The average is used for inference only (does not feed back into training).\n\n```json5\n{\n\t\"otype\": \"Average\", // Component type.\n\t\"n_samples\": 128,   // The number of steps to be averaged over.\n\t\"nested\": {         // The nested optimizer.\n\t\t\"otype\": \"Adam\"\n\t}\n}\n```\n\n### Batched\n\nWraps another optimizer, invoking the nested optimizer once every N steps on the averaged gradient. Has the same effect as increasing the batch size but requires only a constant amount of memory.\n\n```json5\n{\n\t\"otype\": \"Batched\",          // Component type.\n\t\"batch_size_multiplier\": 16, // N from the above description\n\t\"nested\": {                  // The nested optimizer.\n\t\t\"otype\": \"Adam\"\n\t}\n}\n```\n\n### Exponential Moving Average (EMA)\n\nWraps another optimizer and computes an exponential moving average of optimized parameters. The average is used for inference only (does not feed back into training).\n\n```json5\n{\n\t\"otype\": \"EMA\", // Component type.\n\t\"decay\": 0.99,  // The EMA's decay per step.\n\t\"nested\": {     // The nested optimizer.\n\t\t\"otype\": \"Adam\"\n\t}\n}\n```\n\n\n### Exponential Decay\n\nWraps another optimizer and performs piecewise-constant exponential learning-rate decay.\n\n```json5\n{\n\t\"otype\": \"ExponentialDecay\", // Component type.\n\t\"decay_base\": 0.1,           // The amount per decay step.\n\t\"decay_start\": 10000,        // The training step at which\n\t                             // to start the decay.\n\t\"decay_end\": 10000000,       // The training step at which\n\t                             // to end the decay.\n\t\"decay_interval\": 10000,     // Training steps inbetween decay.\n\t\"nested\": {                  // The nested optimizer.\n\t\t\"otype\": \"Adam\"\n\t}\n}\n```\n\n\n### Lookahead\n\nWraps another optimizer, implementing the lookahead algorithm [[Zhang et al. 2019]](https://arxiv.org/abs/1907.08610).\n\n```json5\n{\n\t\"otype\": \"Lookahead\", // Component type.\n\t\"alpha\": 0.5,         // Fraction of lookahead distance to\n\t                      // traverse.\n\t\"n_steps\": 16,        // Nested optimizer steps for each\n\t                      // Lookahead step.\n\t\"nested\": {           // The nested optimizer.\n\t\t\"otype\": \"Adam\"\n\t}\n}\n```\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 1.498046875,
          "content": "Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.\r\n\r\nRedistribution and use in source and binary forms, with or without modification, are permitted\r\nprovided that the following conditions are met:\r\n    * Redistributions of source code must retain the above copyright notice, this list of\r\n      conditions and the following disclaimer.\r\n    * Redistributions in binary form must reproduce the above copyright notice, this list of\r\n      conditions and the following disclaimer in the documentation and/or other materials\r\n      provided with the distribution.\r\n    * Neither the name of the NVIDIA CORPORATION nor the names of its contributors may be used\r\n      to endorse or promote products derived from this software without specific prior written\r\n      permission.\r\n\r\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR\r\nIMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND\r\nFITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE\r\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\r\nBUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\r\nOR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,\r\nSTRICT LIABILITY, OR TOR (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\r\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.390625,
          "content": "# Tiny CUDA Neural Networks ![](https://github.com/NVlabs/tiny-cuda-nn/workflows/CI/badge.svg)\n\nThis is a small, self-contained framework for training and querying neural networks. Most notably, it contains a lightning fast [\"fully fused\" multi-layer perceptron](https://raw.githubusercontent.com/NVlabs/tiny-cuda-nn/master/data/readme/fully-fused-mlp-diagram.png) ([technical paper](https://tom94.net/data/publications/mueller21realtime/mueller21realtime.pdf)), a versatile [multiresolution hash encoding](https://raw.githubusercontent.com/NVlabs/tiny-cuda-nn/master/data/readme/multiresolution-hash-encoding-diagram.png) ([technical paper](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf)), as well as support for various other input encodings, losses, and optimizers.\n\n## Performance\n\n![Image](data/readme/fully-fused-vs-tensorflow.png)\n_Fully fused networks vs. TensorFlow v2.5.0 w/ XLA. Measured on 64 (solid line) and 128 (dashed line) neurons wide multi-layer perceptrons on an RTX 3090. Generated by `benchmarks/bench_ours.cu` and `benchmarks/bench_tensorflow.py` using `data/config_oneblob.json`._\n\n\n## Usage\n\nTiny CUDA neural networks have a simple C++/CUDA API:\n\n```cpp\n#include <tiny-cuda-nn/common.h>\n\n// Configure the model\nnlohmann::json config = {\n\t{\"loss\", {\n\t\t{\"otype\", \"L2\"}\n\t}},\n\t{\"optimizer\", {\n\t\t{\"otype\", \"Adam\"},\n\t\t{\"learning_rate\", 1e-3},\n\t}},\n\t{\"encoding\", {\n\t\t{\"otype\", \"HashGrid\"},\n\t\t{\"n_levels\", 16},\n\t\t{\"n_features_per_level\", 2},\n\t\t{\"log2_hashmap_size\", 19},\n\t\t{\"base_resolution\", 16},\n\t\t{\"per_level_scale\", 2.0},\n\t}},\n\t{\"network\", {\n\t\t{\"otype\", \"FullyFusedMLP\"},\n\t\t{\"activation\", \"ReLU\"},\n\t\t{\"output_activation\", \"None\"},\n\t\t{\"n_neurons\", 64},\n\t\t{\"n_hidden_layers\", 2},\n\t}},\n};\n\nusing namespace tcnn;\n\nauto model = create_from_config(n_input_dims, n_output_dims, config);\n\n// Train the model (batch_size must be a multiple of tcnn::BATCH_SIZE_GRANULARITY)\nGPUMatrix<float> training_batch_inputs(n_input_dims, batch_size);\nGPUMatrix<float> training_batch_targets(n_output_dims, batch_size);\n\nfor (int i = 0; i < n_training_steps; ++i) {\n\tgenerate_training_batch(&training_batch_inputs, &training_batch_targets); // <-- your code\n\n\tfloat loss;\n\tmodel.trainer->training_step(training_batch_inputs, training_batch_targets, &loss);\n\tstd::cout << \"iteration=\" << i << \" loss=\" << loss << std::endl;\n}\n\n// Use the model\nGPUMatrix<float> inference_inputs(n_input_dims, batch_size);\ngenerate_inputs(&inference_inputs); // <-- your code\n\nGPUMatrix<float> inference_outputs(n_output_dims, batch_size);\nmodel.network->inference(inference_inputs, inference_outputs);\n```\n\n\n## Example: learning a 2D image\n\nWe provide a sample application where an image function _(x,y) -> (R,G,B)_ is learned. It can be run via\n```sh\ntiny-cuda-nn$ ./build/mlp_learning_an_image data/images/albert.jpg data/config_hash.json\n```\nproducing an image every couple of training steps. Each 1000 steps should take a bit over 1 second with the default configuration on an RTX 4090.\n\n| 10 steps | 100 steps | 1000 steps | Reference image |\n|:---:|:---:|:---:|:---:|\n| ![10steps](data/readme/10.jpg) | ![100steps](data/readme/100.jpg) | ![1000steps](data/readme/1000.jpg) | ![reference](data/images/albert.jpg) |\n\n\n\n## Requirements\n\n- An __NVIDIA GPU__; tensor cores increase performance when available. All shown results come from an RTX 3090.\n- A __C++14__ capable compiler. The following choices are recommended and have been tested:\n  - __Windows:__ Visual Studio 2019 or 2022\n  - __Linux:__ GCC/G++ 8 or higher\n- A recent version of __[CUDA](https://developer.nvidia.com/cuda-toolkit)__. The following choices are recommended and have been tested:\n  - __Windows:__ CUDA 11.5 or higher\n  - __Linux:__ CUDA 10.2 or higher\n- __[CMake](https://cmake.org/) v3.21 or higher__.\n- The fully fused MLP component of this framework requires a __very large__ amount of shared memory in its default configuration. It will likely only work on an RTX 3090, an RTX 2080 Ti, or higher-end GPUs. Lower end cards must reduce the `n_neurons` parameter or use the `CutlassMLP` (better compatibility but slower) instead.\n\nIf you are using Linux, install the following packages\n```sh\nsudo apt-get install build-essential git\n```\n\nWe also recommend installing [CUDA](https://developer.nvidia.com/cuda-toolkit) in `/usr/local/` and adding the CUDA installation to your PATH.\nFor example, if you have CUDA 11.4, add the following to your `~/.bashrc`\n```sh\nexport PATH=\"/usr/local/cuda-11.4/bin:$PATH\"\nexport LD_LIBRARY_PATH=\"/usr/local/cuda-11.4/lib64:$LD_LIBRARY_PATH\"\n```\n\n\n## Compilation (Windows & Linux)\n\nBegin by cloning this repository and all its submodules using the following command:\n```sh\n$ git clone --recursive https://github.com/nvlabs/tiny-cuda-nn\n$ cd tiny-cuda-nn\n```\n\nThen, use CMake to build the project: (on Windows, this must be in a [developer command prompt](https://docs.microsoft.com/en-us/cpp/build/building-on-the-command-line?view=msvc-160#developer_command_prompt))\n```sh\ntiny-cuda-nn$ cmake . -B build -DCMAKE_BUILD_TYPE=RelWithDebInfo\ntiny-cuda-nn$ cmake --build build --config RelWithDebInfo -j\n```\n\nIf compilation fails inexplicably or takes longer than an hour, you might be running out of memory. Try running the above command without `-j` in that case.\n\n\n## PyTorch extension\n\n__tiny-cuda-nn__ comes with a [PyTorch](https://github.com/pytorch/pytorch) extension that allows using the fast MLPs and input encodings from within a [Python](https://www.python.org/) context.\nThese bindings can be significantly faster than full Python implementations; in particular for the [multiresolution hash encoding](https://raw.githubusercontent.com/NVlabs/tiny-cuda-nn/master/data/readme/multiresolution-hash-encoding-diagram.png).\n\n> The overheads of Python/PyTorch can nonetheless be extensive if the batch size is small.\n> For example, with a batch size of 64k, the bundled `mlp_learning_an_image` example is __~2x slower__ through PyTorch than native CUDA.\n> With a batch size of 256k and higher (default), the performance is much closer.\n\nBegin by setting up a Python 3.X environment with a recent, CUDA-enabled version of PyTorch. Then, invoke\n```sh\npip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch\n```\n\nAlternatively, if you would like to install from a local clone of __tiny-cuda-nn__, invoke\n```sh\ntiny-cuda-nn$ cd bindings/torch\ntiny-cuda-nn/bindings/torch$ python setup.py install\n```\n\nUpon success, you can use __tiny-cuda-nn__ models as in the following example:\n```py\nimport commentjson as json\nimport tinycudann as tcnn\nimport torch\n\nwith open(\"data/config_hash.json\") as f:\n\tconfig = json.load(f)\n\n# Option 1: efficient Encoding+Network combo.\nmodel = tcnn.NetworkWithInputEncoding(\n\tn_input_dims, n_output_dims,\n\tconfig[\"encoding\"], config[\"network\"]\n)\n\n# Option 2: separate modules. Slower but more flexible.\nencoding = tcnn.Encoding(n_input_dims, config[\"encoding\"])\nnetwork = tcnn.Network(encoding.n_output_dims, n_output_dims, config[\"network\"])\nmodel = torch.nn.Sequential(encoding, network)\n```\n\nSee `samples/mlp_learning_an_image_pytorch.py` for an example.\n\n\n\n## Components\n\nFollowing is a summary of the components of this framework. [The JSON documentation](DOCUMENTATION.md) lists configuration options.\n\n\n| Networks | &nbsp; | &nbsp;\n| :--- | :---------- | :-----\n| Fully fused MLP | `src/fully_fused_mlp.cu` | Lightning fast implementation of small multi-layer perceptrons (MLPs).\n| CUTLASS MLP     | `src/cutlass_mlp.cu`     | MLP based on [CUTLASS](https://github.com/NVIDIA/cutlass)' GEMM routines. Slower than fully-fused, but handles larger networks and still is reasonably fast.\n\n| Input encodings | &nbsp; | &nbsp;\n| :--- | :---------- | :-----\n| Composite | `include/tiny-cuda-nn/encodings/composite.h` | Allows composing multiple encodings. Can be, for example, used to assemble the Neural Radiance Caching encoding [[MÃ¼ller et al. 2021]](https://tom94.net/).\n| Frequency | `include/tiny-cuda-nn/encodings/frequency.h` | NeRF's [[Mildenhall et al. 2020]](https://www.matthewtancik.com/nerf) positional encoding applied equally to all dimensions.\n| Grid | `include/tiny-cuda-nn/encodings/grid.h` | Encoding based on trainable multiresolution grids. Used for [Instant Neural Graphics Primitives [MÃ¼ller et al. 2022]](https://nvlabs.github.io/instant-ngp/). The grids can be backed by hashtables, dense storage, or tiled storage.\n| Identity | `include/tiny-cuda-nn/encodings/identity.h` | Leaves values untouched.\n| Oneblob | `include/tiny-cuda-nn/encodings/oneblob.h` | From Neural Importance Sampling [[MÃ¼ller et al. 2019]](https://tom94.net/data/publications/mueller18neural/mueller18neural-v4.pdf) and Neural Control Variates [[MÃ¼ller et al. 2020]](https://tom94.net/data/publications/mueller20neural/mueller20neural.pdf).\n| SphericalHarmonics | `include/tiny-cuda-nn/encodings/spherical_harmonics.h` | A frequency-space encoding that is more suitable to direction vectors than component-wise ones.\n| TriangleWave | `include/tiny-cuda-nn/encodings/triangle_wave.h` | Low-cost alternative to the NeRF's encoding. Used in Neural Radiance Caching [[MÃ¼ller et al. 2021]](https://tom94.net/).\n\n| Losses | &nbsp; | &nbsp;\n| :--- | :---------- | :-----\n| L1 | `include/tiny-cuda-nn/losses/l1.h` | Standard L1 loss.\n| Relative L1 | `include/tiny-cuda-nn/losses/l1.h` | Relative L1 loss normalized by the network prediction.\n| MAPE | `include/tiny-cuda-nn/losses/mape.h` | Mean absolute percentage error (MAPE). The same as Relative L1, but normalized by the target.\n| SMAPE | `include/tiny-cuda-nn/losses/smape.h` | Symmetric mean absolute percentage error (SMAPE). The same as Relative L1, but normalized by the mean of the prediction and the target.\n| L2 | `include/tiny-cuda-nn/losses/l2.h` | Standard L2 loss.\n| Relative L2 | `include/tiny-cuda-nn/losses/relative_l2.h` | Relative L2 loss normalized by the network prediction [[Lehtinen et al. 2018]](https://github.com/NVlabs/noise2noise).\n| Relative L2 Luminance | `include/tiny-cuda-nn/losses/relative_l2_luminance.h` | Same as above, but normalized by the luminance of the network prediction. Only applicable when network prediction is RGB. Used in Neural Radiance Caching [[MÃ¼ller et al. 2021]](https://tom94.net/).\n| Cross Entropy | `include/tiny-cuda-nn/losses/cross_entropy.h` | Standard cross entropy loss. Only applicable when the network prediction is a PDF.\n| Variance | `include/tiny-cuda-nn/losses/variance_is.h` | Standard variance loss. Only applicable when the network prediction is a PDF.\n\n| Optimizers | &nbsp; | &nbsp;\n| :--- | :---------- | :-----\n| Adam | `include/tiny-cuda-nn/optimizers/adam.h` | Implementation of Adam [[Kingma and Ba 2014]](https://arxiv.org/abs/1412.6980), generalized to AdaBound [[Luo et al. 2019]](https://github.com/Luolc/AdaBound).\n| Novograd | `include/tiny-cuda-nn/optimizers/lookahead.h` | Implementation of Novograd [[Ginsburg et al. 2019]](https://arxiv.org/abs/1905.11286).\n| SGD | `include/tiny-cuda-nn/optimizers/sgd.h` | Standard stochastic gradient descent (SGD).\n| Shampoo | `include/tiny-cuda-nn/optimizers/shampoo.h` | Implementation of the 2nd order Shampoo optimizer [[Gupta et al. 2018]](https://arxiv.org/abs/1802.09568) with home-grown optimizations as well as those by [Anil et al. [2020]](https://arxiv.org/abs/2002.09018).\n| Average | `include/tiny-cuda-nn/optimizers/average.h` | Wraps another optimizer and computes a linear average of the weights over the last N iterations. The average is used for inference only (does not feed back into training).\n| Batched | `include/tiny-cuda-nn/optimizers/batched.h` | Wraps another optimizer, invoking the nested optimizer once every N steps on the averaged gradient. Has the same effect as increasing the batch size but requires only a constant amount of memory. |\n| Composite | `include/tiny-cuda-nn/optimizers/composite.h` | Allows using several optimizers on different parameters.\n| EMA | `include/tiny-cuda-nn/optimizers/average.h` | Wraps another optimizer and computes an exponential moving average of the weights. The average is used for inference only (does not feed back into training).\n| Exponential Decay | `include/tiny-cuda-nn/optimizers/exponential_decay.h` | Wraps another optimizer and performs piecewise-constant exponential learning-rate decay.\n| Lookahead | `include/tiny-cuda-nn/optimizers/lookahead.h` | Wraps another optimizer, implementing the lookahead algorithm [[Zhang et al. 2019]](https://arxiv.org/abs/1907.08610).\n\n\n## License and Citation\n\nThis framework is licensed under the BSD 3-clause license. Please see `LICENSE.txt` for details.\n\nIf you use it in your research, we would appreciate a citation via\n```bibtex\n@software{tiny-cuda-nn,\n\tauthor = {M\\\"uller, Thomas},\n\tlicense = {BSD-3-Clause},\n\tmonth = {4},\n\ttitle = {{tiny-cuda-nn}},\n\turl = {https://github.com/NVlabs/tiny-cuda-nn},\n\tversion = {1.7},\n\tyear = {2021}\n}\n```\n\nFor business inquiries, please visit our website and submit the form: [NVIDIA Research Licensing](https://www.nvidia.com/en-us/research/inquiries/)\n\n\n## Publications & Software\n\nAmong others, this framework powers the following publications:\n\n> __Instant Neural Graphics Primitives with a Multiresolution Hash Encoding__  \n> [Thomas MÃ¼ller](https://tom94.net), [Alex Evans](https://research.nvidia.com/person/alex-evans), [Christoph Schied](https://research.nvidia.com/person/christoph-schied), [Alexander Keller](https://research.nvidia.com/person/alex-keller)  \n> _ACM Transactions on Graphics (__SIGGRAPH__), July 2022_  \n> __[Website](https://nvlabs.github.io/instant-ngp/)&nbsp;/ [Paper](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf)&nbsp;/ [Code](https://github.com/NVlabs/instant-ngp)&nbsp;/ [Video](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.mp4)&nbsp;/ [BibTeX](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.bib)__\n\n> __Extracting Triangular 3D Models, Materials, and Lighting From Images__  \n> [Jacob Munkberg](https://research.nvidia.com/person/jacob-munkberg), [Jon Hasselgren](https://research.nvidia.com/person/jon-hasselgren), [Tianchang Shen](http://www.cs.toronto.edu/~shenti11/), [Jun Gao](http://www.cs.toronto.edu/~jungao/), [Wenzheng Chen](http://www.cs.toronto.edu/~wenzheng/), [Alex Evans](https://research.nvidia.com/person/alex-evans), [Thomas MÃ¼ller](https://tom94.net), [Sanja Fidler](https://www.cs.toronto.edu/~fidler/)  \n> __CVPR (Oral)__, June 2022  \n> __[Website](https://nvlabs.github.io/nvdiffrec/)&nbsp;/ [Paper](https://nvlabs.github.io/nvdiffrec/assets/paper.pdf)&nbsp;/ [Video](https://nvlabs.github.io/nvdiffrec/assets/video.mp4)&nbsp;/ [BibTeX](https://nvlabs.github.io/nvdiffrec/assets/bib.txt)__\n\n> __Real-time Neural Radiance Caching for Path Tracing__  \n> [Thomas MÃ¼ller](https://tom94.net), [Fabrice Rousselle](https://research.nvidia.com/person/fabrice-rousselle), [Jan NovÃ¡k](http://jannovak.info), [Alexander Keller](https://research.nvidia.com/person/alex-keller)  \n> _ACM Transactions on Graphics (__SIGGRAPH__), August 2021_  \n> __[Paper](https://tom94.net/data/publications/mueller21realtime/mueller21realtime.pdf)&nbsp;/ [GTC talk](https://gtc21.event.nvidia.com/media/Fully%20Fused%20Neural%20Network%20for%20Radiance%20Caching%20in%20Real%20Time%20Rendering%20%5BE31307%5D/1_liqy6k1c)&nbsp;/ [Video](https://tom94.net/data/publications/mueller21realtime/mueller21realtime.mp4)&nbsp;/ [Interactive results viewer](https://tom94.net/data/publications/mueller21realtime/interactive-viewer/)&nbsp;/ [BibTeX](https://tom94.net/data/publications/mueller21realtime/mueller21realtime.bib)__\n\n\nAs well as the following software:\n\n> __NerfAcc: A General NeRF Accleration Toolbox__  \n> [Ruilong Li](https://www.liruilong.cn/), [Matthew Tancik](https://www.matthewtancik.com/about-me), [Angjoo Kanazawa](https://people.eecs.berkeley.edu/~kanazawa/)  \n> __https://github.com/KAIR-BAIR/nerfacc__\n\n> __Nerfstudio: A Framework for Neural Radiance Field Development__  \n> [Matthew Tancik*](https://www.matthewtancik.com/about-me), [Ethan Weber*](https://ethanweber.me/), [Evonne Ng*](http://people.eecs.berkeley.edu/~evonne_ng/), [Ruilong Li](https://www.liruilong.cn/), Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, [Angjoo Kanazawa](https://people.eecs.berkeley.edu/~kanazawa/)  \n> __https://github.com/nerfstudio-project/nerfstudio__\n\nPlease feel free to make a pull request if your publication or software is not listed.\n\n## Acknowledgments\n\nSpecial thanks go to the NRC authors for helpful discussions and to [Nikolaus Binder](https://research.nvidia.com/person/nikolaus-binder) for providing part of the infrastructure of this framework, as well as for help with utilizing TensorCores from within CUDA.\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "bindings",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "dependencies",
          "type": "tree",
          "content": null
        },
        {
          "name": "include",
          "type": "tree",
          "content": null
        },
        {
          "name": "samples",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}