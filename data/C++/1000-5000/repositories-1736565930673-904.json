{
  "metadata": {
    "timestamp": 1736565930673,
    "page": 904,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "discord/lilliput",
      "stars": 1971,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 2.4296875,
          "content": "---\nAccessModifierOffset: -4\nAlignAfterOpenBracket: true\nAlignConsecutiveAssignments: false\nAlignConsecutiveDeclarations: false\nAlignEscapedNewlines: Left\nAlignOperands: false\nAlignTrailingComments: true\nAllowAllParametersOfDeclarationOnNextLine: false\nAllowShortBlocksOnASingleLine: false\nAllowShortCaseLabelsOnASingleLine: false\nAllowShortFunctionsOnASingleLine: InlineOnly\nAllowShortIfStatementsOnASingleLine: false\nAllowShortLoopsOnASingleLine: false\nAlwaysBreakAfterReturnType: None\nAlwaysBreakBeforeMultilineStrings: false\nAlwaysBreakTemplateDeclarations: true\nBinPackArguments: false\nBinPackParameters: false\nBreakBeforeBinaryOperators: None\nBreakBeforeBraces: Stroustrup\nBreakBeforeInheritanceComma: true\nBreakBeforeTernaryOperators: true\nBreakConstructorInitializers: BeforeComma\nBreakStringLiterals: true\nColumnLimit: 100\nCommentPragmas: ''\nCompactNamespaces: false\nConstructorInitializerAllOnOneLineOrOnePerLine: false\nConstructorInitializerIndentWidth: 2\nContinuationIndentWidth: 2\nCpp11BracedListStyle: true\nDerivePointerAlignment: false\nDisableFormat: false\nFixNamespaceComments: true\nForEachMacros: []\nIndentCaseLabels: false\nIncludeBlocks: Preserve\nIncludeCategories:\n  - Regex: '^<(W|w)indows.h>'\n    Priority: 1\n  - Regex: '^<'\n    Priority: 2\n  - Regex: '.*'\n    Priority: 3\nIncludeIsMainRegex: '(_test|_win|_linux|_mac|_ios|_osx|_null)?$'\nIndentCaseLabels: false\nIndentPPDirectives: None\nIndentWidth: 4\nIndentWrappedFunctionNames: false\nKeepEmptyLinesAtTheStartOfBlocks: false\nMacroBlockBegin: ''\nMacroBlockEnd: ''\nMaxEmptyLinesToKeep: 1\nNamespaceIndentation: None\nPenaltyBreakAssignment: 0\nPenaltyBreakBeforeFirstCallParameter: 1\nPenaltyBreakComment: 300\nPenaltyBreakFirstLessLess: 120\nPenaltyBreakString: 1000\nPenaltyExcessCharacter: 1000000\nPenaltyReturnTypeOnItsOwnLine: 9999999\nPointerAlignment: Left\nReflowComments: true\nSortIncludes: false\nSortUsingDeclarations: true\nSpaceAfterCStyleCast: false\nSpaceAfterTemplateKeyword: true\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeParens: ControlStatements\nSpaceInEmptyParentheses: false\nSpacesBeforeTrailingComments: 1\nSpacesInAngles: false\nSpacesInCStyleCastParentheses: false\nSpacesInContainerLiterals: true\nSpacesInParentheses: false\nSpacesInSquareBrackets: false\nStandard: Cpp11\nTabWidth: 4\nUseTab: Never\n---\nLanguage: Cpp\n---\nLanguage: ObjC\nObjCBlockIndentWidth: 4\nObjCSpaceAfterProperty: true\nObjCSpaceBeforeProtocolList: false\n---\nLanguage: Java\nBasedOnStyle: Google\nBreakAfterJavaFieldAnnotations: true\n...\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.138671875,
          "content": ".vscode\ntestdata/out/\n\n# generated\n*.pc\n*.la\ndeps/aom\ndeps/osx/bin\ndeps/osx/man\ndeps/osx/share\ndeps/linux/bin\ndeps/linux/man\ndeps/linux/share\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.2119140625,
          "content": "MIT License\n\nCopyright (c) 2017 Discord\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nAdditionally, lilliput contains the source and compiled binaries from a number\nof other libraries. The full licenses of these libraries are contained in\nthird-party-licenses/.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.455078125,
          "content": "# lilliput\n\n[lilliput](https://en.wiktionary.org/wiki/lilliputian#Adjective) resizes images in Go.\n\nLilliput relies on mature, high-performance C libraries to do most of the work of\ndecompressing, resizing and compressing images. It aims to do as little memory\nallocation as possible and especially not to create garbage in Go. As a result,\nit is suitable for very high throughput image resizing services.\n\nLilliput supports resizing JPEG, PNG, static WEBP, and animated GIFs & WEBPs. It can also convert formats.\nLilliput also has some support for getting the first frame from MOV and WEBM\nvideos.\n\n**Lilliput presently only supports OSX ARM64 and Linux.**\n\n## Example\nLilliput comes with a [fully working example](examples/main.go) that runs on the command line. The\nexample takes a user supplied filename and prints some basic info about the file.\nIt then resizes and transcodes the image (if flags are supplied) and saves the\nresulting file.\n\nTo use the example, `go get github.com/discord/lilliput` and then run\n`go build` from the examples/ directory.\n\n## License\n\nLilliput is released under MIT license (see [LICENSE](LICENSE)). Additionally, lilliput ships with other\nlibraries, each provided under its own license. See [third-party-licenses](third-party-licenses/) for\nmore info.\n\n## Usage\n\nFirst, `import \"github.com/discord/lilliput\"`.\n\n### Decoder\nLilliput is concerned with in-memory images, so the decoder requires image\ndata to be in a []byte buffer.\n\n```go\nfunc lilliput.NewDecoder([]byte buf) (lilliput.Decoder, error)\n```\nCreate a new `Decoder` object from the compressed image contained by `buf`.\nThis will return an error when the magic bytes of the buffer don't match\none of the supported image types.\n\n```go\nfunc (d lilliput.Decoder) Header() (lilliput.ImageHeader, error)\n```\nRead and return the image's header. The header contains the image's metadata.\nReturns error if the image has a malformed header. An image with a malformed\nheader cannot be decoded.\n\n```go\nfunc (d lilliput.Decoder) Description() string\n```\nReturns a string describing the image's type, e.g. `\"JPEG\"` or `\"PNG\"`.\n\n```go\nfunc (h lilliput.Decoder) Duration() time.Duration\n```\nReturns the length of the content. Returns 0 for static images and\nanimated GIFs.\n\n```go\nfunc (d lilliput.Decoder) DecodeTo(f *lilliput.Framebuffer) error\n```\nFully decodes the image and writes its pixel data to `f`. Returns an error\nif the decoding process fails. If the image contains multiple frames,\nthen each call returns a subsequent frame. `io.EOF` is returned when the image\ndoes not contain any more data to be decoded.\n\n**Users of lilliput generally should not call `DecodeTo` and should instead\nuse an ImageOps object.**\n\n```go\nfunc (d lilliput.Decoder) Close()\n```\nCloses the decoder and releases resources. The Decoder object must have\n`.Close()` called when it is no longer in use.\n\n### ImageOps\nLilliput provides a convenience object to handle image resizing and encoding from an\nopen Decoder object. The ImageOps object can be created and then reused, which reduces memory\nallocations. Generally, users should prefer the ImageOps object over manually controlling\nthe resize and encode process.\n\n```go\nfunc lilliput.NewImageOps(dimension int) *lilliput.ImageOps\n```\nCreate an ImageOps object that can operate on images up to `dimension x dimension` pixels in size.\nThis object can be reused for multiple operations.\n\n```go\nfunc (o *lilliput.ImageOps) Transform(decoder lilliput.Decoder, opts *lilliput.ImageOptions, dst []byte) ([]byte, error)\n```\nTransform the compressed image contained in a Decoder object into the desired output type. **The decoder must not\nhave DecodeTo() called on it already.** However, it is ok to call `decoder.Header()` if you would like to check\nimage properties before transforming the image. Returns an error if the resize or encoding process fails.\n\nThe resulting compressed image will be written into `dst`. The returned []byte\nslice will point to the same region as `dst` but with a different length, so that you can tell where the image ends.\n\nFields for `lilliput.ImageOptions` are as follows\n\n* `FileType`: file extension type, e.g. `\".jpeg\"`\n\n* `Width`: number of pixels of width of output image\n\n* `Height`: number of pixels of height of output image\n\n* `ResizeMethod`: one of `lilliput.ImageOpsNoResize` or `lilliput.ImageOpsFit`. `Fit` behavior\nis the same as `Framebuffer.Fit()` -- it performs a cropping resize that does not stretch the image.\n\n* `NormalizeOrientation`: If `true`, `Transform()` will inspect the image orientation and\nnormalize the output so that it is facing in the standard orientation. This will undo\nJPEG EXIF-based orientation.\n\n* `EncodeOptions`: Of type `map[int]int`, same options accepted as [Encoder.Encode()](#encoder). This\ncontrols output encode quality.\n\n```go\nfunc (o *lilliput.ImageOps) Clear()\n```\nClear out all pixel data contained in ImageOps object from any previous operations.\nThis function does not need to be called between Transform() calls. The user may choose\nto do this if they want to remove image data from memory.\n\n```go\nfunc (o *lilliput.ImageOps) Close()\n```\nClose the ImageOps object and release resources. The ImageOps object must have `.Close()` called when it is no longer in use.\n\n### ImageHeader\nThis interface returns basic metadata about an image. It is created by\ncalling `Decoder.Header()`.\n\n```go\nfunc (h lilliput.ImageHeader) Width() int\n```\nReturns the image's width in number of pixels.\n\n```go\nfunc (h lilliput.ImageHeader) Height() int\n```\nReturns the image's height in number of pixels.\n\n```go\nfunc (h lilliput.ImageHeader) PixelType() lilliput.PixelType\n```\nReturns the basic pixel type for the image's pixels.\n\n```go\nfunc (h lilliput.ImageHeader) Orientation() lilliput.ImageOrientation\n```\nReturns the metadata-based orientation of the image. This function can\nbe called on all image types but presently only detects orientation in\nJPEG images. An orientation value of 1 indicates default orientation.\nAll other values indicate some kind of rotation or mirroring.\n\n### PixelType\n\n```go\nfunc (p lilliput.PixelType) Depth() int\n```\nReturns the number of bits per pixel.\n\n```go\nfunc (p lilliput.PixelType) Channels() int\n```\nReturns the number of channels per pixel, e.g. 3 for RGB or 4 for RGBA.\n\n### Framebuffer\nThis type contains a raw array of pixels, decompressed from an image.\nIn general, you will want to use the ImageOps object instead of operating on\nFramebuffers manually.\n\n```go\nfunc lilliput.NewFramebuffer(width, height int) *lilliput.Framebuffer\n```\nCreate a new Framebuffer with given dimensions without any pixel data.\n\n```go\nfunc (f *lilliput.Framebuffer) Clear()\n```\nSet contents of framebuffer to 0, clearing out any previous pixel data.\n\n```go\nfunc (f *lilliput.Framebuffer) Width() int\n```\nReturns the width in number of pixels of the contained pixel data, if any.\nThis does not return the capacity of the buffer.\n\n```go\nfunc (f *lilliput.Framebuffer) Height() int\n```\nReturns the height in number of pixels of the contained pixel data, if any.\nThis does not return the capacity of the buffer.\n\n```go\nfunc (f *lilliput.Framebuffer) PixelType() lilliput.PixelType\n```\nReturns the `PixelType` of the contained pixel data, if any.\n\n```go\nfunc (f *lilliput.Framebuffer) OrientationTransform(orientation lilliput.ImageOrientation)\n```\nRotate and/or mirror framebuffer according to orientation value. If you\npass the `orientation` value given by the image's `ImageHeader`, then the\nresulting image has its orientation normalized to the\ndefault orientation.\n\n```go\nfunc (f *lilliput.Framebuffer) ResizeTo(width, height int, dst *lilliput.Framebuffer) error\n```\nPerform a resize into `dst` of `f` according to given dimensions. This function does not\npreserve the source's aspect ratio if the new dimensions have a different ratio. The\nresize can fail if the destination is not large enough to hold the new image.\n\n```go\nfunc (f *lilliput.Framebuffer) Fit(width, height int, dst *lilliput.Framebuffer) error\n```\nPerform a cropping resize into `dst` of `f` according to given dimensions. This function\ndoes preserve the source's aspect ratio. The image will be cropped along one axis if\nthe new dimensions have a different ratio than the source. The cropping will occur equally\non the edges, e.g. if the source image is too tall for the new ratio, then the destination will\nhave rows of pixels from the top and bottom removed. Returns error if the destination is\nnot large enough to contain the resized image.\n\n```go\nfunc (f *lilliput.Framebuffer) Close()\n```\nCloses the framebuffer and releases resources. The `Framebuffer` object must have `.Close()` called when it is no longer in use.\n\n### Encoder\nThe Encoder takes a Framebuffer and writes the pixels into a compressed format.\n\n```go\nfunc lilliput.NewEncoder(extension string, decodedBy lilliput.Decoder, dst []byte) (lilliput.Encoder, error)\n```\nCreate a new Encoder object that writes to `dst`. `extension` should be a file extension-like string,\ne.g. `\".jpeg\"` or `\".png\"`. `decodedBy` should be the `Decoder` used to decompress the image, if any.\n`decodedBy` may be left as `nil` in most cases but is required when creating a `.gif` encoder. That is,\n`.gif` outputs can only be created from source GIFs.\n\n```go\nfunc (e lilliput.Encoder) Encode(buffer lilliput.Framebuffer, opts map[int]int) ([]byte, error)\n```\nEncodes the Framebuffer supplied into the output `dst` given when the `Encoder` was created. The\nreturned []byte will point to the same buffer as `dst` but can be a shorter slice, so that if `dst`\nhas 50MB of capacity but the image only occupies 30KB, you can tell where the image data ends. This\nfunction returns an error if the encoding process fails.\n\n`opts` is optional and may be left `nil`. It is used to control encoder behavior\ne.g. `map[int]int{lilliput.JpegQuality: 80}` to set JPEG outputquality to `80`.\n\nValid keys/values for `opts` are\n\n* `JpegQuality` (1 - 100)\n* `PngCompression` (0 - 9)\n* `WebpQuality` (0 - 100).\n\n```go\nfunc (e lilliput.Encoder) Close()\n```\nClose the Encoder and release resources. The `Encoder` object must have `.Close()` called when it is no longer in use.\n\n## Building Dependencies\n\nGo does not provide any mechanism for arbitrary building of dependencies, e.g. invoking\n`make` or `cmake`. In order to make lilliput usable as a standard Go package, prebuilt\nstatic libraries have been provided for all of lilliput's dependencies on Linux and\nOSX. In order to automate this process, lilliput ships with build scripts alongside\ncompressed archives of the sources of its dependencies. These build scripts are provided\nfor [OSX](deps/build-deps-osx.sh) and [Linux](deps/build-deps-linux.sh).\n\n"
        },
        {
          "name": "avcodec.cpp",
          "type": "blob",
          "size": 15.4541015625,
          "content": "#include \"avcodec.hpp\"\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n#include <libavcodec/avcodec.h>\n#include <libavformat/avformat.h>\n#include <libswscale/swscale.h>\n#include <libavutil/display.h>\n#include <libavutil/imgutils.h>\n\n#include \"icc_profiles/rec2020_profile.h\"\n#include \"icc_profiles/rec601_ntsc_profile.h\"\n#include \"icc_profiles/rec601_pal_profile.h\"\n#include \"icc_profiles/rec709_profile.h\"\n#include \"icc_profiles/srgb_profile.h\"\n\n#ifdef __cplusplus\n}\n#endif\n\nextern AVInputFormat ff_mov_demuxer;\nextern AVInputFormat ff_matroska_demuxer;\nextern AVInputFormat ff_mp3_demuxer;\nextern AVInputFormat ff_flac_demuxer;\nextern AVInputFormat ff_wav_demuxer;\nextern AVInputFormat ff_aac_demuxer;\nextern AVInputFormat ff_ogg_demuxer;\nextern AVCodec ff_h264_decoder;\nextern AVCodec ff_hevc_decoder;\nextern AVCodec ff_mpeg4_decoder;\nextern AVCodec ff_vp9_decoder;\nextern AVCodec ff_vp8_decoder;\nextern AVCodec ff_mp3_decoder;\nextern AVCodec ff_flac_decoder;\nextern AVCodec ff_aac_decoder;\nextern AVCodec ff_vorbis_decoder;\n\nvoid avcodec_init()\n{\n    av_log_set_level(AV_LOG_ERROR);\n}\n\nstruct avcodec_decoder_struct {\n    const cv::Mat* mat;\n    ptrdiff_t read_index;\n    AVFormatContext* container;\n    AVCodecContext* codec;\n    AVIOContext* avio;\n    int video_stream_index;\n};\n\nstatic int avcodec_decoder_read_callback(void* d_void, uint8_t* buf, int buf_size)\n{\n    avcodec_decoder d = static_cast<avcodec_decoder>(d_void);\n    size_t buf_len = d->mat->total() - d->read_index;\n    size_t read_len = (buf_len > buf_size) ? buf_size : buf_len;\n    if (read_len == 0) {\n        return AVERROR_EOF;\n    }\n    memmove(buf, d->mat->data + d->read_index, read_len);\n    d->read_index += read_len;\n    return read_len;\n}\n\nstatic int64_t avcodec_decoder_seek_callback(void* d_void, int64_t offset, int whence)\n{\n    avcodec_decoder d = static_cast<avcodec_decoder>(d_void);\n    uint8_t* to;\n    switch (whence) {\n    case SEEK_SET:\n        to = d->mat->data + offset;\n        break;\n    case SEEK_CUR:\n        to = d->mat->data + d->read_index + offset;\n        break;\n    case SEEK_END:\n        to = d->mat->data + d->mat->total() + offset;\n        break;\n    case AVSEEK_SIZE:\n        return d->mat->total();\n    default:\n        return -1;\n    }\n    if (to < d->mat->data) {\n        return -1;\n    }\n    if (to >= (d->mat->data + d->mat->total())) {\n        return -1;\n    }\n    d->read_index = (to - d->mat->data);\n    return 0;\n}\n\nstatic bool avcodec_decoder_is_audio(const avcodec_decoder d)\n{\n    if (!d->container) {\n        return false;\n    }\n    if (d->container->iformat == &ff_mp3_demuxer) {\n        return true;\n    }\n    if (d->container->iformat == &ff_flac_demuxer) {\n        return true;\n    }\n    if (d->container->iformat == &ff_wav_demuxer) {\n        return true;\n    }\n    if (d->container->iformat == &ff_aac_demuxer) {\n        return true;\n    }\n    if (d->container->iformat == &ff_ogg_demuxer) {\n        return true;\n    }\n    return false;\n}\n\nbool avcodec_decoder_is_streamable(const opencv_mat mat) {\n    const int64_t probeBytesLimit = 32 * 1024; // Define the probe limit\n    const size_t atomHeaderSize = 8;\n    int64_t bytesRead = 0;\n    const cv::Mat* buf = static_cast<const cv::Mat*>(mat);\n    size_t bufSize = buf->total();\n    size_t peekSize = MIN(bufSize, probeBytesLimit);\n\n    while(bytesRead + atomHeaderSize <= peekSize) {\n        // Read atom size and type\n        uint32_t atomSize = (buf->data[bytesRead] << 24) | (buf->data[bytesRead + 1] << 16) |\n                            (buf->data[bytesRead + 2] << 8) | buf->data[bytesRead + 3];\n\n        // Validate atom size\n        if (atomSize < atomHeaderSize || atomSize + bytesRead > bufSize) {\n            break;\n        }\n\n        // Read atom type\n        char atomType[4];\n        memcpy(atomType, &buf->data[bytesRead + 4], 4);\n\n        // Check for 'moov' and 'mdat' atoms using byte comparison\n        if (memcmp(atomType, \"moov\", 4) == 0) {\n            return true;\n        }\n        if (memcmp(atomType, \"mdat\", 4) == 0) {\n            return false;\n        }\n\n        // Move to the next atom position\n        bytesRead += atomSize; // Atom size includes the header size\n    }\n\n    return false;\n}\n\navcodec_decoder avcodec_decoder_create(const opencv_mat buf, const bool hevc_enabled)\n{\n    avcodec_decoder d = new struct avcodec_decoder_struct();\n    memset(d, 0, sizeof(struct avcodec_decoder_struct));\n    d->mat = static_cast<const cv::Mat*>(buf);\n\n    d->container = avformat_alloc_context();\n    if (!d->container) {\n        avcodec_decoder_release(d);\n        return NULL;\n    }\n\n    d->avio = avio_alloc_context(\n      NULL, 0, 0, d, avcodec_decoder_read_callback, NULL, avcodec_decoder_seek_callback);\n    if (!d->avio) {\n        avcodec_decoder_release(d);\n        return NULL;\n    }\n    d->container->pb = d->avio;\n\n    int res = avformat_open_input(&d->container, NULL, NULL, NULL);\n    if (res < 0) {\n        avformat_free_context(d->container);\n        d->container = NULL;\n        avcodec_decoder_release(d);\n        return NULL;\n    }\n\n    // perform a quick search for the video stream index in the container\n    AVCodecParameters* codec_params = NULL;\n    for (int i = 0; i < d->container->nb_streams; i++) {\n        if (d->container->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_VIDEO) {\n            codec_params = d->container->streams[i]->codecpar;\n            d->video_stream_index = i;\n            break;\n        }\n    }\n\n    // call avformat_find_stream_info only if no header was found (i.e. mpeg-ts),\n    // or if the duration, width, or height are unknown.\n    // this is an expensive operation that could involve frame decoding, perform judiciously.\n    bool isAudioOnly = avcodec_decoder_is_audio(d);\n    if ((!isAudioOnly && (!codec_params || codec_params->width <= 0 || codec_params->height <= 0)) ||\n        d->container->duration <= 0) {\n        res = avformat_find_stream_info(d->container, NULL);\n        if (res < 0) {\n            avcodec_decoder_release(d);\n            return NULL;\n        }\n\n        if (isAudioOnly) {\n            // in this case, quit out fast since we won't be decoding this anyway\n            // (audio is metadata-only)\n            return d;\n        }\n\n        // repeat the search for the video stream index\n        if (!codec_params) {\n            for (int i = 0; i < d->container->nb_streams; i++) {\n                if (d->container->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_VIDEO) {\n                    codec_params = d->container->streams[i]->codecpar;\n                    d->video_stream_index = i;\n                    break;\n                }\n            }\n            if (!codec_params) {\n                avcodec_decoder_release(d);\n                return NULL;\n            }\n        }\n    }\n\n    const AVCodec* codec = avcodec_find_decoder(codec_params->codec_id);\n    if (!codec) {\n        avcodec_decoder_release(d);\n        return NULL;\n    }\n\n    if (codec->id == AV_CODEC_ID_HEVC && !hevc_enabled) {\n        avcodec_decoder_release(d);\n        return NULL;\n    }\n\n    d->codec = avcodec_alloc_context3(codec);\n\n    res = avcodec_parameters_to_context(d->codec, codec_params);\n    if (res < 0) {\n        avcodec_decoder_release(d);\n        return NULL;\n    }\n\n    res = avcodec_open2(d->codec, codec, NULL);\n    if (res < 0) {\n        avcodec_decoder_release(d);\n        return NULL;\n    }\n\n    return d;\n}\n\nconst uint8_t* avcodec_get_icc_profile(int color_primaries, size_t& profile_size) {\n    switch (color_primaries) {\n        case AVCOL_PRI_BT2020:\n            profile_size = sizeof(rec2020_profile);\n            return rec2020_profile;\n        case AVCOL_PRI_BT470BG:  // BT.601 PAL\n            profile_size = sizeof(rec601_pal_profile);\n            return rec601_pal_profile;\n        case AVCOL_PRI_SMPTE170M: // BT.601 NTSC\n            profile_size = sizeof(rec601_ntsc_profile);\n            return rec601_ntsc_profile;\n        default:\n            // Default to sRGB profile\n            profile_size = sizeof(srgb_profile);\n            return srgb_profile;\n    }\n}\n\nint avcodec_decoder_get_icc(const avcodec_decoder d, void* dest, size_t dest_len) {\n    size_t profile_size;\n    const uint8_t* profile_data = avcodec_get_icc_profile(d->codec->color_primaries, profile_size);\n\n    if (profile_size > dest_len) {\n        return -1; // Destination buffer is too small\n    }\n\n    std::memcpy(dest, profile_data, profile_size);\n    return static_cast<int>(profile_size);\n}\n\nint avcodec_decoder_get_width(const avcodec_decoder d)\n{\n    if (d->codec) {\n        AVStream *st = d->container->streams[d->video_stream_index];\n        if (st->sample_aspect_ratio.num > 0 && st->sample_aspect_ratio.den > 0 &&\n            st->sample_aspect_ratio.num > st->sample_aspect_ratio.den) {\n            return (int64_t)d->codec->width * st->sample_aspect_ratio.num / st->sample_aspect_ratio.den;\n        }\n        return d->codec->width;\n    }\n    return 0;\n}\n\nint avcodec_decoder_get_height(const avcodec_decoder d)\n{\n    if (d->codec) {\n        AVStream *st = d->container->streams[d->video_stream_index];\n        if (st->sample_aspect_ratio.num > 0 && st->sample_aspect_ratio.den > 0 &&\n            st->sample_aspect_ratio.den > st->sample_aspect_ratio.num) {\n            return (int64_t)d->codec->height * st->sample_aspect_ratio.den / st->sample_aspect_ratio.num;\n        }\n        return d->codec->height;\n    }\n    return 0;\n}\n\nint avcodec_decoder_get_orientation(const avcodec_decoder d)\n{\n    if (!d->container) {\n        return CV_IMAGE_ORIENTATION_TL;\n    }\n    if (!d->codec) {\n        return CV_IMAGE_ORIENTATION_TL;\n    }\n    CVImageOrientation orientation = CV_IMAGE_ORIENTATION_TL;\n    AVDictionaryEntry* tag =\n      av_dict_get(d->container->streams[d->video_stream_index]->metadata, \"rotate\", NULL, 0);\n    int rotation = 0;\n    if (tag) {\n        rotation = atoi(tag->value);\n    } else {\n        const uint8_t* side_data =\n          av_stream_get_side_data(d->container->streams[d->video_stream_index], AV_PKT_DATA_DISPLAYMATRIX, NULL);\n        if (side_data) {\n            rotation = (360 - (int)(av_display_rotation_get((const int32_t*)side_data))) % 360;\n        }\n    }\n    switch (rotation) {\n    case 90:\n        orientation = CV_IMAGE_ORIENTATION_RT;\n        break;\n    case 180:\n        orientation = CV_IMAGE_ORIENTATION_BR;\n        break;\n    case 270:\n        orientation = CV_IMAGE_ORIENTATION_LB;\n        break;\n    }\n    return orientation;\n}\n\nfloat avcodec_decoder_get_duration(const avcodec_decoder d)\n{\n    if (d->container) {\n        return d->container->duration / (float)(AV_TIME_BASE);\n    }\n    return 0;\n}\n\nconst char* avcodec_decoder_get_description(const avcodec_decoder d)\n{\n    if (d->container) {\n        if (d->container->iformat == &ff_mov_demuxer) {\n            return \"MOV\";\n        }\n        if (d->container->iformat == &ff_matroska_demuxer) {\n            return \"WEBM\";\n        }\n        if (d->container->iformat == &ff_mp3_demuxer) {\n            return \"MP3\";\n        }\n        if (d->container->iformat == &ff_flac_demuxer) {\n            return \"FLAC\";\n        }\n        if (d->container->iformat == &ff_wav_demuxer) {\n            return \"WAV\";\n        }\n        if (d->container->iformat == &ff_aac_demuxer) {\n            return \"AAC\";\n        }\n        if (d->container->iformat == &ff_ogg_demuxer) {\n            return \"OGG\";\n        }\n    }\n    return \"\";\n}\n\nbool avcodec_decoder_has_subtitles(const avcodec_decoder d) {\n    for (unsigned int i = 0; i < d->container->nb_streams; i++) {\n        AVStream* stream = d->container->streams[i];\n        if (stream->codecpar->codec_type == AVMEDIA_TYPE_SUBTITLE) {\n            return true;\n        }\n    }\n    return false;\n}\n\nstatic int avcodec_decoder_copy_frame(const avcodec_decoder d, opencv_mat mat, AVFrame* frame) {\n    auto cvMat = static_cast<cv::Mat*>(mat);\n\n    int res = avcodec_receive_frame(d->codec, frame);\n    if (res >= 0) {\n        // Calculate the step size based on the cv::Mat's width\n        int stepSize = 4 * cvMat->cols; // Assuming the cv::Mat is in BGRA format, which has 4 channels\n        if (cvMat->cols % 32 != 0) {\n            int width = cvMat->cols + 32 - (cvMat->cols % 32);\n            stepSize = 4 * width;\n        }\n        if (!opencv_mat_set_row_stride(mat, stepSize)) {\n            return -1;\n        }\n\n        // Create SwsContext for converting the frame format and scaling\n        struct SwsContext* sws = sws_getContext(\n            frame->width, frame->height, (AVPixelFormat)(frame->format), // Source dimensions and format\n            cvMat->cols, cvMat->rows, AV_PIX_FMT_BGRA, // Destination dimensions and format\n            SWS_BILINEAR, // Specify the scaling algorithm; you can choose another according to your needs\n            NULL, NULL, NULL);\n\n        // Configure colorspace\n        int colorspace;\n        switch (frame->colorspace) {\n            case AVCOL_SPC_BT2020_NCL:\n            case AVCOL_SPC_BT2020_CL:\n                colorspace = SWS_CS_BT2020;\n                break;\n            case AVCOL_SPC_BT470BG:\n                colorspace = SWS_CS_ITU601;\n                break;\n            case AVCOL_SPC_SMPTE170M:\n                colorspace = SWS_CS_SMPTE170M;\n                break;\n            case AVCOL_SPC_SMPTE240M:\n                colorspace = SWS_CS_SMPTE240M;\n                break;\n            default:\n                colorspace = SWS_CS_ITU709;\n                break;\n        }\n        const int* inv_table = sws_getCoefficients(colorspace);\n\n        // Configure color range\n        int srcRange = frame->color_range == AVCOL_RANGE_JPEG ? 1 : 0;\n\n        // Configure YUV conversion table\n        const int* table = sws_getCoefficients(SWS_CS_DEFAULT);\n\n        sws_setColorspaceDetails(sws, inv_table, srcRange, table, 1, 0, 1 << 16, 1 << 16);\n\n        // The linesizes and data pointers for the destination\n        int dstLinesizes[4];\n        av_image_fill_linesizes(dstLinesizes, AV_PIX_FMT_BGRA, stepSize / 4);\n        uint8_t* dstData[4] = {cvMat->data, NULL, NULL, NULL};\n\n        // Perform the scaling and format conversion\n        sws_scale(sws, frame->data, frame->linesize, 0, frame->height, dstData, dstLinesizes);\n\n        // Free the SwsContext\n        sws_freeContext(sws);\n    }\n\n    return res;\n}\n\nstatic int avcodec_decoder_decode_packet(const avcodec_decoder d, opencv_mat mat, AVPacket* packet)\n{\n    int res = avcodec_send_packet(d->codec, packet);\n    if (res < 0) {\n        return res;\n    }\n\n    AVFrame* frame = av_frame_alloc();\n    if (!frame) {\n        return -1;\n    }\n\n    res = avcodec_decoder_copy_frame(d, mat, frame);\n    av_frame_free(&frame);\n\n    return res;\n}\n\nbool avcodec_decoder_decode(const avcodec_decoder d, opencv_mat mat)\n{\n    if (!d) {\n        return false;\n    }\n    if (!d->container) {\n        return false;\n    }\n    if (!d->codec) {\n        return false;\n    }\n    AVPacket packet;\n    bool done = false;\n    bool success = false;\n    while (!done) {\n        int res = av_read_frame(d->container, &packet);\n        if (res < 0) {\n            return false;\n        }\n        if (packet.stream_index == d->video_stream_index) {\n            res = avcodec_decoder_decode_packet(d, mat, &packet);\n            if (res >= 0) {\n                success = true;\n                done = true;\n            }\n            else if (res != AVERROR(EAGAIN) && res != AVERROR_INVALIDDATA) {\n                done = true;\n            }\n        }\n        av_packet_unref(&packet);\n    }\n    return success;\n}\n\nvoid avcodec_decoder_release(avcodec_decoder d)\n{\n    if (d->codec) {\n        avcodec_free_context(&d->codec);\n    }\n\n    if (d->container) {\n        avformat_close_input(&d->container);\n    }\n\n    if (d->avio) {\n        avio_flush(d->avio);\n        av_free(d->avio->buffer);\n        av_free(d->avio);\n    }\n\n    delete d;\n}\n"
        },
        {
          "name": "avcodec.go",
          "type": "blob",
          "size": 4.8408203125,
          "content": "package lilliput\n\n// #include \"avcodec.hpp\"\nimport \"C\"\n\nimport (\n\t\"io\"\n\t\"time\"\n\t\"unsafe\"\n)\n\nconst probeBytesLimit = 32 * 1024\nconst atomHeaderSize = 8\n\n// Set HEVC decoder enablement behind a build flag, defaults to off\n// Enable by building/running with \"-ldflags=-X=github.com/discord/lilliput.hevcEnabled=true\"\nvar hevcEnabled string\n\n// avCodecDecoder handles decoding of various video/image formats using FFmpeg's avcodec.\ntype avCodecDecoder struct {\n\tdecoder      C.avcodec_decoder\n\tmat          C.opencv_mat\n\tbuf          []byte\n\thasDecoded   bool\n\tmaybeMP4     bool\n\tisStreamable bool\n\thasSubtitles bool\n}\n\n// newAVCodecDecoder creates a new decoder instance from the provided buffer.\n// Returns an error if the buffer is too small or contains invalid data.\nfunc newAVCodecDecoder(buf []byte) (*avCodecDecoder, error) {\n\tmat := createMatFromBytes(buf)\n\tif mat == nil {\n\t\treturn nil, ErrBufTooSmall\n\t}\n\n\tdecoder := C.avcodec_decoder_create(mat, hevcEnabled == \"true\")\n\tif decoder == nil {\n\t\tC.opencv_mat_release(mat)\n\t\treturn nil, ErrInvalidImage\n\t}\n\n\treturn &avCodecDecoder{\n\t\tdecoder:      decoder,\n\t\tmat:          mat,\n\t\tbuf:          buf,\n\t\tmaybeMP4:     isMP4(buf),\n\t\tisStreamable: isStreamable(mat),\n\t\thasSubtitles: hasSubtitles(decoder),\n\t}, nil\n}\n\n// createMatFromBytes creates an OpenCV matrix from a byte buffer.\n// The matrix is created as a single-channel 8-bit unsigned type.\nfunc createMatFromBytes(buf []byte) C.opencv_mat {\n\treturn C.opencv_mat_create_from_data(C.int(len(buf)), 1, C.CV_8U, unsafe.Pointer(&buf[0]), C.size_t(len(buf)))\n}\n\n// hasSubtitles checks if the decoder has detected any subtitle streams.\nfunc hasSubtitles(d C.avcodec_decoder) bool {\n\treturn bool(C.avcodec_decoder_has_subtitles(d))\n}\n\n// isStreamable determines if the media content can be streamed.\nfunc isStreamable(mat C.opencv_mat) bool {\n\treturn bool(C.avcodec_decoder_is_streamable(mat))\n}\n\n// Description returns the format description of the media.\n// Special handling is included to differentiate between MOV and MP4 formats.\nfunc (d *avCodecDecoder) Description() string {\n\tfmt := C.GoString(C.avcodec_decoder_get_description(d.decoder))\n\n\t// differentiate MOV and MP4 based on magic\n\tif fmt == \"MOV\" && d.maybeMP4 {\n\t\treturn \"MP4\"\n\t}\n\n\treturn fmt\n}\n\n// HasSubtitles returns whether the media contains subtitle streams.\nfunc (d *avCodecDecoder) HasSubtitles() bool {\n\treturn d.hasSubtitles\n}\n\n// IsStreamable returns whether the media content can be streamed.\nfunc (d *avCodecDecoder) IsStreamable() bool {\n\treturn d.isStreamable\n}\n\n// BackgroundColor returns the default background color (white).\nfunc (d *avCodecDecoder) BackgroundColor() uint32 {\n\treturn 0xFFFFFFFF\n}\n\n// LoopCount returns the number of times the media should loop (0 for no looping).\nfunc (d *avCodecDecoder) LoopCount() int {\n\treturn 0\n}\n\n// ICC returns the ICC color profile data if present, or an empty slice if not.\nfunc (d *avCodecDecoder) ICC() []byte {\n\ticcDst := make([]byte, 8192)\n\ticcLength := C.avcodec_decoder_get_icc(d.decoder, unsafe.Pointer(&iccDst[0]), C.size_t(cap(iccDst)))\n\tif iccLength <= 0 {\n\t\treturn []byte{}\n\t}\n\treturn iccDst[:iccLength]\n}\n\n// Duration returns the total duration of the media content.\nfunc (d *avCodecDecoder) Duration() time.Duration {\n\treturn time.Duration(float64(C.avcodec_decoder_get_duration(d.decoder)) * float64(time.Second))\n}\n\n// Header returns the image metadata including dimensions, pixel format, and orientation.\n// Frame count is always 1 since it requires the entire buffer to be decoded.\nfunc (d *avCodecDecoder) Header() (*ImageHeader, error) {\n\treturn &ImageHeader{\n\t\twidth:         int(C.avcodec_decoder_get_width(d.decoder)),\n\t\theight:        int(C.avcodec_decoder_get_height(d.decoder)),\n\t\tpixelType:     PixelType(C.CV_8UC4),\n\t\torientation:   ImageOrientation(C.avcodec_decoder_get_orientation(d.decoder)),\n\t\tnumFrames:     1,\n\t\tcontentLength: len(d.buf),\n\t}, nil\n}\n\n// DecodeTo decodes the next frame into the provided Framebuffer.\n// Returns io.EOF when no more frames are available.\nfunc (d *avCodecDecoder) DecodeTo(f *Framebuffer) error {\n\tif d.hasDecoded {\n\t\treturn io.EOF\n\t}\n\th, err := d.Header()\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = f.resizeMat(h.Width(), h.Height(), h.PixelType())\n\tif err != nil {\n\t\treturn err\n\t}\n\tret := C.avcodec_decoder_decode(d.decoder, f.mat)\n\tif !ret {\n\t\treturn ErrDecodingFailed\n\t}\n\tf.blend = NoBlend\n\tf.dispose = DisposeToBackgroundColor\n\tf.duration = time.Duration(0)\n\tf.xOffset = 0\n\tf.yOffset = 0\n\td.hasDecoded = true\n\treturn nil\n}\n\n// SkipFrame attempts to skip the next frame, but is not supported by this decoder.\nfunc (d *avCodecDecoder) SkipFrame() error {\n\treturn ErrSkipNotSupported\n}\n\n// Close releases all resources associated with the decoder.\nfunc (d *avCodecDecoder) Close() {\n\tC.avcodec_decoder_release(d.decoder)\n\tC.opencv_mat_release(d.mat)\n\td.buf = nil\n}\n\n// init initializes the avcodec library when the package is loaded.\nfunc init() {\n\tC.avcodec_init()\n}\n"
        },
        {
          "name": "avcodec.hpp",
          "type": "blob",
          "size": 0.9296875,
          "content": "#ifndef LILLIPUT_AVCODEC_HPP\n#define LILLIPUT_AVCODEC_HPP\n\n#include \"opencv.hpp\"\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\ntypedef struct avcodec_decoder_struct* avcodec_decoder;\n\nvoid avcodec_init();\n\navcodec_decoder avcodec_decoder_create(const opencv_mat buf, const bool hevc_enabled);\nvoid avcodec_decoder_release(avcodec_decoder d);\nint avcodec_decoder_get_width(const avcodec_decoder d);\nint avcodec_decoder_get_height(const avcodec_decoder d);\nint avcodec_decoder_get_orientation(const avcodec_decoder d);\nfloat avcodec_decoder_get_duration(const avcodec_decoder d);\nbool avcodec_decoder_decode(const avcodec_decoder d, opencv_mat mat);\nbool avcodec_decoder_is_streamable(const opencv_mat buf);\nbool avcodec_decoder_has_subtitles(const avcodec_decoder d);\nconst char* avcodec_decoder_get_description(const avcodec_decoder d);\nint avcodec_decoder_get_icc(const avcodec_decoder d, void* dest, size_t dest_len);\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif\n"
        },
        {
          "name": "avcodec_test.go",
          "type": "blob",
          "size": 2.1220703125,
          "content": "package lilliput\n\nimport (\n\t\"os\"\n\t\"testing\"\n)\n\nfunc TestIsStreamable(t *testing.T) {\n\t// Standard MP4\n\tstdMp4, err := os.ReadFile(\"testdata/big_buck_bunny_480p_10s_std.mp4\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to open test file: %v\", err)\n\t}\n\tif isStreamable(createMatFromBytes(stdMp4)) {\n\t\tt.Fatalf(\"expected file to not be streamable\")\n\t}\n\n\t// Web-optimized streamable MP4\n\twebMp4, err := os.ReadFile(\"testdata/big_buck_bunny_480p_10s_web.mp4\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to open test file: %v\", err)\n\t}\n\tif !isStreamable(createMatFromBytes(webMp4)) {\n\t\tt.Fatalf(\"expected file to be streamable\")\n\t}\n\n\t// MP4 with a big atom\n\tbigAtomMp4, err := os.ReadFile(\"testdata/big_buck_bunny_480p_10s_big_atom.mp4\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to open test file: %v\", err)\n\t}\n\tif isStreamable(createMatFromBytes(bigAtomMp4)) {\n\t\tt.Fatalf(\"expected file to not be streamable\")\n\t}\n\n\t// MP4 with a zero-length atom\n\tzeroLengthAtomMp4, err := os.ReadFile(\"testdata/big_buck_bunny_480p_10s_zero_length_atom.mp4\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to open test file: %v\", err)\n\t}\n\tif isStreamable(createMatFromBytes(zeroLengthAtomMp4)) {\n\t\tt.Fatalf(\"expected file to not be streamable\")\n\t}\n}\n\nfunc TestICCProfile(t *testing.T) {\n\t// Web-optimized streamable MP4 using BT.709 color space\n\twebMp4, err := os.ReadFile(\"testdata/big_buck_bunny_480p_10s_web.mp4\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to open test file: %v\", err)\n\t}\n\twebAvCodecDecoder, err := newAVCodecDecoder(webMp4)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to create decoder: %v\", err)\n\t}\n\tdefer webAvCodecDecoder.Close()\n\tif len(webAvCodecDecoder.ICC()) == 0 {\n\t\tt.Fatalf(\"expected ICC profile\")\n\t}\n}\n\nfunc BenchmarkIsStreamableWebMp4(b *testing.B) {\n\t// Read the web-optimized streamable MP4 file\n\twebMp4, err := os.ReadFile(\"testdata/big_buck_bunny_480p_10s_web.mp4\")\n\tif err != nil {\n\t\tb.Fatalf(\"failed to open test file: %v\", err)\n\t}\n\tb.ResetTimer()\n\n\t// Run the benchmark\n\tfor i := 0; i < b.N; i++ {\n\t\twebAvCodecDecoder, err := newAVCodecDecoder(webMp4)\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"failed to create decoder: %v\", err)\n\t\t}\n\t\tdefer webAvCodecDecoder.Close()\n\t\t_ = webAvCodecDecoder.IsStreamable()\n\t}\n}\n"
        },
        {
          "name": "avif.cpp",
          "type": "blob",
          "size": 13.8212890625,
          "content": "#include \"avif.hpp\"\n#include <opencv2/imgproc.hpp>\n#include <avif/avif.h>\n#include <cstring>\n\n#define DEFAULT_BACKGROUND_COLOR 0xFFFFFFFF\n\nstruct avif_decoder_struct {\n    avifDecoder* decoder;\n    avifRGBImage rgb;\n    const uint8_t* buffer;\n    size_t buffer_size;\n    int frame_count;\n    int current_frame;\n    bool has_alpha;\n    uint32_t bgcolor;\n    int timescale;\n    int total_duration;\n};\n\nstruct avif_encoder_struct {\n    avifEncoder* encoder;\n    uint8_t* dst;\n    size_t dst_len;\n    const uint8_t* icc;\n    size_t icc_len;\n    int frame_count;\n    bool has_alpha;\n};\n\n//----------------------\n// Decoder Management\n//----------------------\navif_decoder avif_decoder_create(const opencv_mat buf) {\n    auto cvMat = static_cast<const cv::Mat*>(buf);\n    if (!cvMat || cvMat->empty()) {\n        return nullptr;\n    }\n\n    auto d = new avif_decoder_struct();\n    memset(d, 0, sizeof(avif_decoder_struct));\n\n    d->buffer = cvMat->data;\n    d->buffer_size = cvMat->total();\n    d->decoder = avifDecoderCreate();\n    \n    if (!d->decoder) {\n        delete d;\n        return nullptr;\n    }\n\n    // Enable strict mode for better compatibility\n    d->decoder->strictFlags = AVIF_STRICT_ENABLED;\n\n    // Parse the AVIF data\n    avifResult result = avifDecoderSetIOMemory(d->decoder, d->buffer, d->buffer_size);\n    if (result != AVIF_RESULT_OK) {\n        avifDecoderDestroy(d->decoder);\n        delete d;\n        return nullptr;\n    }\n\n    result = avifDecoderParse(d->decoder);\n    if (result != AVIF_RESULT_OK) {\n        avifDecoderDestroy(d->decoder);\n        delete d;\n        return nullptr;\n    }\n\n    d->frame_count = d->decoder->imageCount;\n    d->total_duration = (d->frame_count > 1) ? \n        (int)((double)d->decoder->durationInTimescales * 1000.0 / d->decoder->timescale) : 0;\n\n    // Read the first frame\n    result = avifDecoderNextImage(d->decoder);\n    if (result != AVIF_RESULT_OK) {\n        avifDecoderDestroy(d->decoder);\n        delete d;\n        return nullptr;\n    }\n\n    // Initialize RGB image\n    avifRGBImageSetDefaults(&d->rgb, d->decoder->image);\n    d->rgb.format = AVIF_RGB_FORMAT_BGR;\n    d->rgb.depth = 8;\n    \n    d->has_alpha = d->decoder->image->alphaPlane != nullptr;\n    if (d->has_alpha) {\n        d->rgb.format = AVIF_RGB_FORMAT_BGRA;\n    }\n\n    result = avifRGBImageAllocatePixels(&d->rgb);\n    if (result != AVIF_RESULT_OK) {\n        avifDecoderDestroy(d->decoder);\n        delete d;\n        return nullptr;\n    }\n\n    d->current_frame = 0;\n    d->bgcolor = DEFAULT_BACKGROUND_COLOR;\n    d->timescale = 1000;\n\n    return d;\n}\n\nvoid avif_decoder_release(avif_decoder d) {\n    if (d) {\n        if (d->decoder) {\n            avifRGBImageFreePixels(&d->rgb);\n            avifDecoderDestroy(d->decoder);\n        }\n        delete d;\n    }\n}\n\n//----------------------\n// Decoder Properties\n//----------------------\nint avif_decoder_get_width(const avif_decoder d) {\n    if (!d || !d->decoder) {\n        return 0;\n    }\n    return d->decoder->image->width;\n}\n\nint avif_decoder_get_height(const avif_decoder d) {\n    if (!d || !d->decoder) {\n        return 0;\n    }\n    return d->decoder->image->height;\n}\n\nint avif_decoder_get_pixel_type(const avif_decoder d) {\n    if (!d || !d->decoder) {\n        return 0;\n    }\n    return d->has_alpha ? CV_8UC4 : CV_8UC3;\n}\n\nbool avif_decoder_is_animated(const avif_decoder d) {\n    if (!d || !d->decoder) {\n        return false;\n    }\n    return d->frame_count > 1;\n}\n\nint avif_decoder_get_frame_count(const avif_decoder d) {\n    if (!d || !d->decoder) {\n        return 0;\n    }\n    return d->frame_count;\n}\n\nint avif_decoder_get_num_frames(const avif_decoder d) {\n    if (!d || !d->decoder) {\n        return 0;\n    }\n    return d->frame_count;\n}\n\nuint32_t avif_decoder_get_duration(const avif_decoder d) {\n    if (!d || !d->decoder) {\n        return 0;\n    }\n    // Get total duration in milliseconds\n    return (uint32_t)(d->decoder->duration * 1000.0f);\n}\n\nuint32_t avif_decoder_get_loop_count(const avif_decoder d) {\n    switch (d->decoder->repetitionCount) {\n        case AVIF_REPETITION_COUNT_INFINITE:\n            return 0;\n        case AVIF_REPETITION_COUNT_UNKNOWN:\n            return 1;\n        default:\n            return d->decoder->repetitionCount;\n    }\n}\n\nsize_t avif_decoder_get_icc(const avif_decoder d, void* buf, size_t buf_len) {\n    if (d->decoder->image->icc.size > 0 && d->decoder->image->icc.size <= buf_len) {\n        memcpy(buf, d->decoder->image->icc.data, d->decoder->image->icc.size);\n        return d->decoder->image->icc.size;\n    }\n    return 0;\n}\n\nuint32_t avif_decoder_get_bg_color(const avif_decoder d) {\n    if (!d || !d->decoder) {\n        return DEFAULT_BACKGROUND_COLOR;\n    }\n    return d->bgcolor;\n}\n\nint avif_decoder_get_total_duration(const avif_decoder d) {\n    if (!d || !d->decoder) {\n        return 0;\n    }\n    return d->total_duration;\n}\n\n//----------------------\n// Frame Properties\n//----------------------\nint avif_decoder_get_frame_duration(const avif_decoder d) {\n    if (!d || !d->decoder) {\n        return 0;\n    }\n    // Get current frame duration in milliseconds\n    return (int)(d->decoder->imageTiming.duration * 1000.0f);\n}\n\nint avif_decoder_get_frame_dispose(const avif_decoder d) {\n    if (!d || !d->decoder || !d->decoder->image) {\n        return 0;\n    }\n    return d->decoder->image->imageOwnsYUVPlanes ? AVIF_DISPOSE_BACKGROUND : AVIF_DISPOSE_NONE;\n}\n\nint avif_decoder_get_frame_blend(const avif_decoder d) {\n    if (!d || !d->decoder || !d->decoder->image) {\n        return AVIF_BLEND_NONE;\n    }\n    return d->has_alpha ? AVIF_BLEND_ALPHA : AVIF_BLEND_NONE;\n}\n\nint avif_decoder_get_frame_x_offset(const avif_decoder d) {\n    if (!d || !d->decoder || !d->decoder->image) {\n        return 0;\n    }\n    // Get horizontal offset from Clean Aperture Box\n    if (d->decoder->image->transformFlags & AVIF_TRANSFORM_CLAP) {\n        return (int)(d->decoder->image->clap.horizOffN / d->decoder->image->clap.horizOffD);\n    }\n    return 0;\n}\n\nint avif_decoder_get_frame_y_offset(const avif_decoder d) {\n    if (!d || !d->decoder || !d->decoder->image) {\n        return 0;\n    }\n    // Get vertical offset from Clean Aperture Box\n    if (d->decoder->image->transformFlags & AVIF_TRANSFORM_CLAP) {\n        return (int)(d->decoder->image->clap.vertOffN / d->decoder->image->clap.vertOffD);\n    }\n    return 0;\n}\n\n//----------------------\n// Frame Operations\n//----------------------\nbool avif_decoder_decode(avif_decoder d, opencv_mat mat) {\n    if (!d || !d->decoder) {\n        fprintf(stderr, \"Decoder null check failed\\n\");\n        return false;\n    }\n\n    // Check if we've already decoded all frames\n    if (!avif_decoder_has_more_frames(d)) {\n        fprintf(stderr, \"EOF: All frames have been decoded\\n\");\n        return false;\n    }\n\n    // Convert current frame to RGB/RGBA\n    avifResult result = avifImageYUVToRGB(d->decoder->image, &d->rgb);\n    if (result != AVIF_RESULT_OK) {\n        fprintf(stderr, \"YUV to RGB conversion failed for frame %d with error: %s\\n\", \n               d->current_frame, avifResultToString(result));\n        return false;\n    }\n\n    // Create OpenCV matrix from AVIF buffer\n    auto cvMat = static_cast<cv::Mat*>(mat);\n    cv::Mat srcMat(d->rgb.height, d->rgb.width, \n                   d->has_alpha ? CV_8UC4 : CV_8UC3, \n                   d->rgb.pixels, \n                   d->rgb.rowBytes);\n    \n    // Keep BGR/BGRA format\n    if (d->has_alpha) {\n        // Direct copy since it's already in BGRA format\n        srcMat.copyTo(*cvMat);\n    } else {\n        // For non-alpha images, just add alpha channel to BGR\n        cv::Mat alpha(srcMat.rows, srcMat.cols, CV_8UC1, cv::Scalar(255));\n        std::vector<cv::Mat> channels;\n        cv::split(srcMat, channels);\n        channels.push_back(alpha);\n        cv::merge(channels, *cvMat);\n    }\n\n    // Advance to next frame if there are more frames\n    if (d->current_frame < d->frame_count - 1) {\n        // Free current RGB pixels before moving to next frame\n        avifRGBImageFreePixels(&d->rgb);\n        \n        result = avifDecoderNextImage(d->decoder);\n        if (result != AVIF_RESULT_OK) {\n            fprintf(stderr, \"Failed to advance to next frame: %s\\n\", avifResultToString(result));\n            return false;\n        }\n        \n        // Reinitialize RGB image for the new frame\n        avifRGBImageSetDefaults(&d->rgb, d->decoder->image);\n        d->rgb.format = d->has_alpha ? AVIF_RGB_FORMAT_BGRA : AVIF_RGB_FORMAT_BGR;\n        d->rgb.depth = 8;\n        \n        // Reallocate pixels for the new frame\n        result = avifRGBImageAllocatePixels(&d->rgb);\n        if (result != AVIF_RESULT_OK) {\n            fprintf(stderr, \"Failed to allocate RGB pixels for frame %d: %s\\n\", \n                   d->current_frame, avifResultToString(result));\n            return false;\n        }\n    }\n    d->current_frame++;\n    return true;\n}\n\nint avif_decoder_has_more_frames(avif_decoder d) {\n    if (!d || !d->decoder) {\n        return 0;\n    }\n    return (d->current_frame < d->frame_count);\n}\n\n//----------------------\n// Encoder Management\n//----------------------\navif_encoder avif_encoder_create(void* buf, size_t buf_len, const void* icc, size_t icc_len, int loop_count) {\n    auto e = new avif_encoder_struct();\n    memset(e, 0, sizeof(avif_encoder_struct));\n\n    e->encoder = avifEncoderCreate();\n    if (!e->encoder) {\n        delete e;\n        return nullptr;\n    }\n\n    e->dst = static_cast<uint8_t*>(buf);\n    e->dst_len = buf_len;\n    \n    if (icc && icc_len > 0) {\n        e->icc = static_cast<const uint8_t*>(icc);\n        e->icc_len = icc_len;\n    }\n\n    // Configure encoder for animation support\n    e->encoder->maxThreads = 1;  // Ensure thread-safe encoding\n    e->encoder->repetitionCount = loop_count == 0 ? AVIF_REPETITION_COUNT_INFINITE : loop_count;\n    e->encoder->quality = 60;    // Default quality\n    e->encoder->timescale = 1000;  // Use milliseconds as timescale (1000 ticks per second)\n    e->encoder->speed = AVIF_SPEED_DEFAULT;\n    e->encoder->keyframeInterval = 0; // Let encoder decide keyframes\n    e->encoder->minQuantizer = AVIF_QUANTIZER_BEST_QUALITY;\n    e->encoder->maxQuantizer = AVIF_QUANTIZER_WORST_QUALITY;\n\n    return e;\n}\n\nvoid avif_encoder_release(avif_encoder e) {\n    if (e) {\n        if (e->encoder) {\n            avifEncoderDestroy(e->encoder);\n        }\n        delete e;\n    }\n}\n\n//----------------------\n// Encoder Operations\n//----------------------\nsize_t avif_encoder_write(avif_encoder e, const opencv_mat src, const int* opt, size_t opt_len, int delay_ms, int blend, int dispose) {\n    if (!e || !e->encoder) {\n        fprintf(stderr, \"AVIF Encoder: null check failed\\n\");\n        return 0;\n    }\n\n    // Handle flush case\n    if (!src) {\n        avifRWData output = AVIF_DATA_EMPTY;\n        avifResult result = avifEncoderFinish(e->encoder, &output);\n        \n        if (result != AVIF_RESULT_OK || output.size == 0) {\n            fprintf(stderr, \"AVIF Encoder: flush failed with error: %s\\n\", avifResultToString(result));\n            avifRWDataFree(&output);\n            return 0;\n        }\n\n        if (output.size <= e->dst_len) {\n            memcpy(e->dst, output.data, output.size);\n            size_t size = output.size;\n            avifRWDataFree(&output);\n            return size;\n        }\n\n        avifRWDataFree(&output);\n        return 0;\n    }\n\n    auto cvMat = static_cast<const cv::Mat*>(src);\n    if (!cvMat || cvMat->empty()) {\n        fprintf(stderr, \"AVIF Encoder: invalid source matrix\\n\");\n        return 0;\n    }\n\n    // Create AVIF image\n    avifImage* avifImage = avifImageCreate(cvMat->cols, cvMat->rows, 8, AVIF_PIXEL_FORMAT_YUV444);\n    if (!avifImage) {\n        fprintf(stderr, \"AVIF Encoder: failed to create image\\n\");\n        return 0;\n    }\n\n    // Set ICC profile if available (only on first frame)\n    if (e->icc && e->icc_len > 0 && e->frame_count == 0) {\n        avifResult result = avifImageSetProfileICC(avifImage, e->icc, e->icc_len);\n        if (result != AVIF_RESULT_OK) {\n            fprintf(stderr, \"AVIF Encoder: failed to set ICC profile: %s\\n\", avifResultToString(result));\n            return 0;\n        }\n    }\n\n    // Set encoding options\n    for (size_t i = 0; i + 1 < opt_len; i += 2) {\n        if (opt[i] == AVIF_QUALITY) {\n            e->encoder->quality = std::min(100, std::max(0, opt[i + 1]));\n        } else if (opt[i] == AVIF_SPEED) {\n            e->encoder->speed = std::min(10, std::max(0, opt[i + 1]));\n        }\n    }\n\n    // Convert from BGR/BGRA to YUV\n    avifRGBImage rgb;\n    avifRGBImageSetDefaults(&rgb, avifImage);\n    \n    // Set the correct pixel format based on input channels\n    rgb.format = cvMat->channels() == 4 ? AVIF_RGB_FORMAT_BGRA : AVIF_RGB_FORMAT_BGR;\n    rgb.depth = 8;\n    rgb.pixels = cvMat->data;\n    rgb.rowBytes = cvMat->step;\n    rgb.width = cvMat->cols;\n    rgb.height = cvMat->rows;\n\n    avifResult result = avifImageRGBToYUV(avifImage, &rgb);\n    if (result != AVIF_RESULT_OK) {\n        fprintf(stderr, \"AVIF Encoder: RGB to YUV conversion failed: %s\\n\", avifResultToString(result));\n        avifImageDestroy(avifImage);\n        return 0;\n    }\n\n    // Set up frame timing and flags\n    float durationInTimescale = (float)delay_ms * e->encoder->timescale;\n    if (durationInTimescale < e->encoder->timescale) {\n        durationInTimescale = e->encoder->timescale;  // Ensure minimum duration\n    }\n    float durationInSeconds = durationInTimescale / e->encoder->timescale;\n\n    // Handle blending mode\n    avifAddImageFlags flags = AVIF_ADD_IMAGE_FLAG_NONE;\n    if (blend == 1) { // BLEND_OVER\n        flags |= AVIF_ADD_IMAGE_FLAG_FORCE_KEYFRAME;\n    }\n\n    // Add frame to encoder\n    result = avifEncoderAddImage(e->encoder, avifImage, durationInSeconds, flags);\n    avifImageDestroy(avifImage);\n\n    if (result != AVIF_RESULT_OK) {\n        fprintf(stderr, \"AVIF Encoder: failed to add frame: %s\\n\", avifResultToString(result));\n        return 0;\n    }\n\n    e->frame_count++;\n    return 1; // Return success without actual data (data comes in flush)\n}\n\nsize_t avif_encoder_flush(avif_encoder e) {\n    return avif_encoder_write(e, nullptr, nullptr, 0, 0, 0, 0);\n}\n"
        },
        {
          "name": "avif.go",
          "type": "blob",
          "size": 4.716796875,
          "content": "package lilliput\n\n// #include \"avif.hpp\"\nimport \"C\"\n\nimport (\n\t\"io\"\n\t\"time\"\n\t\"unsafe\"\n)\n\n// Types\n// ----------------------------------------\n\ntype avifDecoder struct {\n\tdecoder C.avif_decoder\n\tmat     C.opencv_mat\n\tbuf     []byte\n}\n\ntype avifEncoder struct {\n\tencoder    C.avif_encoder\n\tdstBuf     []byte\n\ticc        []byte\n\tbgColor    uint32\n\tframeIndex int\n\thasFlushed bool\n}\n\n// Decoder Implementation\n// ----------------------------------------\n\nfunc newAvifDecoder(buf []byte) (*avifDecoder, error) {\n\tmat := C.opencv_mat_create_from_data(C.int(len(buf)), 1, C.CV_8U, unsafe.Pointer(&buf[0]), C.size_t(len(buf)))\n\tif mat == nil {\n\t\treturn nil, ErrBufTooSmall\n\t}\n\n\tdecoder := C.avif_decoder_create(mat)\n\tif decoder == nil {\n\t\treturn nil, ErrInvalidImage\n\t}\n\n\treturn &avifDecoder{\n\t\tdecoder: decoder,\n\t\tmat:     mat,\n\t\tbuf:     buf,\n\t}, nil\n}\n\nfunc (d *avifDecoder) Header() (*ImageHeader, error) {\n\treturn &ImageHeader{\n\t\twidth:         int(C.avif_decoder_get_width(d.decoder)),\n\t\theight:        int(C.avif_decoder_get_height(d.decoder)),\n\t\tpixelType:     PixelType(C.avif_decoder_get_pixel_type(d.decoder)),\n\t\torientation:   OrientationTopLeft,\n\t\tnumFrames:     int(C.avif_decoder_get_num_frames(d.decoder)),\n\t\tcontentLength: len(d.buf),\n\t}, nil\n}\n\nfunc (d *avifDecoder) DecodeTo(f *Framebuffer) error {\n\tif f == nil {\n\t\treturn io.EOF\n\t}\n\n\tif d.hasReachedEndOfFrames() {\n\t\treturn io.EOF\n\t}\n\n\th, err := d.Header()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = f.resizeMat(h.Width(), h.Height(), h.PixelType())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tret := C.avif_decoder_decode(d.decoder, f.mat)\n\tif !ret {\n\t\treturn ErrDecodingFailed\n\t}\n\n\t// Set frame properties\n\tf.duration = time.Duration(C.avif_decoder_get_frame_duration(d.decoder)) * time.Millisecond\n\tf.dispose = DisposeMethod(C.avif_decoder_get_frame_dispose(d.decoder))\n\tf.blend = BlendMethod(C.avif_decoder_get_frame_blend(d.decoder))\n\tf.xOffset = int(C.avif_decoder_get_frame_x_offset(d.decoder))\n\tf.yOffset = int(C.avif_decoder_get_frame_y_offset(d.decoder))\n\n\treturn nil\n}\n\n// Decoder Helper Methods\n// ----------------------------------------\n\nfunc (d *avifDecoder) hasReachedEndOfFrames() bool {\n\treturn C.avif_decoder_has_more_frames(d.decoder) == 0\n}\n\nfunc (d *avifDecoder) ICC() []byte {\n\ticcDst := make([]byte, 8192)\n\ticcLength := C.avif_decoder_get_icc(d.decoder, unsafe.Pointer(&iccDst[0]), C.size_t(cap(iccDst)))\n\treturn iccDst[:iccLength]\n}\n\nfunc (d *avifDecoder) Description() string {\n\treturn \"AVIF\"\n}\n\nfunc (d *avifDecoder) BackgroundColor() uint32 {\n\treturn uint32(C.avif_decoder_get_bg_color(d.decoder))\n}\n\nfunc (d *avifDecoder) Duration() time.Duration {\n\treturn time.Duration(C.avif_decoder_get_total_duration(d.decoder)) * time.Millisecond\n}\n\nfunc (d *avifDecoder) LoopCount() int {\n\treturn int(C.avif_decoder_get_loop_count(d.decoder))\n}\n\nfunc (d *avifDecoder) IsAnimated() bool {\n\treturn int(C.avif_decoder_get_num_frames(d.decoder)) > 1\n}\n\nfunc (d *avifDecoder) HasSubtitles() bool {\n\treturn false\n}\n\nfunc (d *avifDecoder) IsStreamable() bool {\n\treturn false\n}\n\nfunc (d *avifDecoder) SkipFrame() error {\n\treturn ErrSkipNotSupported\n}\n\nfunc (d *avifDecoder) Close() {\n\tC.avif_decoder_release(d.decoder)\n\tC.opencv_mat_release(d.mat)\n\td.buf = nil\n}\n\n// Encoder Implementation\n// ----------------------------------------\n\nfunc newAvifEncoder(decodedBy Decoder, dstBuf []byte) (*avifEncoder, error) {\n\tdstBuf = dstBuf[:1]\n\ticc := decodedBy.ICC()\n\tloopCount := decodedBy.LoopCount()\n\tbgColor := decodedBy.BackgroundColor()\n\n\tvar enc C.avif_encoder\n\tif len(icc) > 0 {\n\t\tenc = C.avif_encoder_create(unsafe.Pointer(&dstBuf[0]), C.size_t(cap(dstBuf)),\n\t\t\tunsafe.Pointer(&icc[0]), C.size_t(len(icc)), C.int(loopCount))\n\t} else {\n\t\tenc = C.avif_encoder_create(unsafe.Pointer(&dstBuf[0]), C.size_t(cap(dstBuf)),\n\t\t\tnil, 0, C.int(loopCount))\n\t}\n\tif enc == nil {\n\t\treturn nil, ErrBufTooSmall\n\t}\n\n\treturn &avifEncoder{\n\t\tencoder: enc,\n\t\tdstBuf:  dstBuf,\n\t\ticc:     icc,\n\t\tbgColor: bgColor,\n\t}, nil\n}\n\nfunc (e *avifEncoder) Encode(f *Framebuffer, opt map[int]int) ([]byte, error) {\n\tif e.hasFlushed {\n\t\treturn nil, io.EOF\n\t}\n\n\tif f == nil {\n\t\tlength := C.avif_encoder_flush(e.encoder)\n\t\tif length == 0 {\n\t\t\treturn nil, ErrInvalidImage\n\t\t}\n\n\t\te.hasFlushed = true\n\t\treturn e.dstBuf[:length], nil\n\t}\n\n\tvar optList []C.int\n\tvar firstOpt *C.int\n\tfor k, v := range opt {\n\t\toptList = append(optList, C.int(k))\n\t\toptList = append(optList, C.int(v))\n\t}\n\tif len(optList) > 0 {\n\t\tfirstOpt = (*C.int)(unsafe.Pointer(&optList[0]))\n\t}\n\n\tframeDelayMs := int(f.duration.Milliseconds())\n\tlength := C.avif_encoder_write(e.encoder, f.mat, firstOpt, C.size_t(len(optList)),\n\t\tC.int(frameDelayMs), C.int(f.blend), C.int(f.dispose))\n\tif length == 0 {\n\t\treturn nil, ErrInvalidImage\n\t}\n\n\te.frameIndex++\n\treturn nil, nil\n}\n\nfunc (e *avifEncoder) Close() {\n\tC.avif_encoder_release(e.encoder)\n}\n"
        },
        {
          "name": "avif.hpp",
          "type": "blob",
          "size": 2.3564453125,
          "content": "#ifndef LILLIPUT_AVIF_HPP\n#define LILLIPUT_AVIF_HPP\n\n#include \"opencv.hpp\"\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n//----------------------\n// Enums & Constants\n//----------------------\nenum AvifBlendMode {\n    AVIF_BLEND_ALPHA = 0,\n    AVIF_BLEND_NONE = 1\n};\n\nenum AvifDisposeMode {\n    AVIF_DISPOSE_NONE = 0,\n    AVIF_DISPOSE_BACKGROUND = 1\n};\n\nenum AvifEncoderOptions {\n    AVIF_QUALITY = 1,\n    AVIF_SPEED = 2\n};\n\n//----------------------\n// Type Definitions\n//----------------------\ntypedef struct avif_decoder_struct* avif_decoder;\ntypedef struct avif_encoder_struct* avif_encoder;\n\n//----------------------\n// Decoder Management\n//----------------------\navif_decoder avif_decoder_create(const opencv_mat buf);\nvoid avif_decoder_release(avif_decoder d);\n\n//----------------------\n// Decoder Properties\n//----------------------\nint avif_decoder_get_width(const avif_decoder d);\nint avif_decoder_get_height(const avif_decoder d);\nint avif_decoder_get_pixel_type(const avif_decoder d);\nint avif_decoder_get_num_frames(const avif_decoder d);\nuint32_t avif_decoder_get_duration(const avif_decoder d);\nuint32_t avif_decoder_get_loop_count(const avif_decoder d);\nsize_t avif_decoder_get_icc(const avif_decoder d, void* buf, size_t buf_len);\nuint32_t avif_decoder_get_bg_color(const avif_decoder d);\nint avif_decoder_get_total_duration(const avif_decoder d);\n\n//----------------------\n// Frame Properties\n//----------------------\nint avif_decoder_get_frame_duration(const avif_decoder d);\nint avif_decoder_get_frame_dispose(const avif_decoder d);\nint avif_decoder_get_frame_blend(const avif_decoder d);\nint avif_decoder_get_frame_x_offset(const avif_decoder d);\nint avif_decoder_get_frame_y_offset(const avif_decoder d);\n\n//----------------------\n// Frame Operations\n//----------------------\nbool avif_decoder_decode(avif_decoder d, opencv_mat mat);\nint avif_decoder_has_more_frames(avif_decoder d);\n\n//----------------------\n// Encoder Management\n//----------------------\navif_encoder avif_encoder_create(void* buf, size_t buf_len, const void* icc, size_t icc_len, int loop_count);\nvoid avif_encoder_release(avif_encoder e);\n\n//----------------------\n// Encoder Operations\n//----------------------\nsize_t avif_encoder_write(avif_encoder e, const opencv_mat src, const int* opt, size_t opt_len, int delay_ms, int blend, int dispose);\nsize_t avif_encoder_flush(avif_encoder e);\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif "
        },
        {
          "name": "avif_test.go",
          "type": "blob",
          "size": 15.8720703125,
          "content": "package lilliput\n\nimport (\n\t\"math\"\n\t\"os\"\n\t\"reflect\"\n\t\"testing\"\n\t\"time\"\n)\n\n// Test Suite Setup\n// ----------------------------------------\n\nfunc TestAvifOperations(t *testing.T) {\n\tt.Run(\"Basic Operations\", func(t *testing.T) {\n\t\tt.Run(\"NewAvifDecoder\", testNewAvifDecoder)\n\t\tt.Run(\"AvifDecoder_Header\", testAvifDecoderHeader)\n\t\tt.Run(\"AvifDecoder_Duration\", testAvifDecoderDuration)\n\t\tt.Run(\"NewAvifEncoder\", testNewAvifEncoder)\n\t\tt.Run(\"AvifDecoder_DecodeTo\", testAvifDecoderDecodeTo)\n\t\tt.Run(\"AvifEncoder_Encode\", testAvifEncoderEncode)\n\t})\n\n\tt.Run(\"Conversion Operations\", func(t *testing.T) {\n\t\tt.Run(\"AvifToWebP_Conversion\", testAvifToWebPConversion)\n\t\tt.Run(\"NewAvifDecoderWithAnimatedSource\", testNewAvifDecoderWithAnimatedSource)\n\t\tt.Run(\"NewAvifEncoderWithWebPAnimatedSource\", testNewAvifEncoderWithWebPAnimatedSource)\n\t\tt.Run(\"NewAvifEncoderWithVideoSource\", testNewAvifEncoderWithVideoSource)\n\t})\n}\n\n// Basic Decoder Tests\n// ----------------------------------------\n\nfunc testNewAvifDecoder(t *testing.T) {\n\ttestAvifImage, err := os.ReadFile(\"testdata/colors_sdr_srgb.avif\")\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error while reading AVIF image: %v\", err)\n\t}\n\tdecoder, err := newAvifDecoder(testAvifImage)\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error while decoding AVIF image data: %v\", err)\n\t}\n\tdefer decoder.Close()\n}\n\nfunc testAvifDecoderHeader(t *testing.T) {\n\ttestAvifImage, err := os.ReadFile(\"testdata/colors_sdr_srgb.avif\")\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error while reading AVIF image: %v\", err)\n\t}\n\tdecoder, err := newAvifDecoder(testAvifImage)\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error while decoding AVIF image data: %v\", err)\n\t}\n\tdefer decoder.Close()\n\n\theader, err := decoder.Header()\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error: %v\", err)\n\t}\n\tif reflect.TypeOf(header).String() != \"*lilliput.ImageHeader\" {\n\t\tt.Fatalf(\"Expected type *lilliput.ImageHeader, got %v\", reflect.TypeOf(header))\n\t}\n}\n\nfunc testAvifDecoderDuration(t *testing.T) {\n\ttestCases := []struct {\n\t\tname           string\n\t\tfilename       string\n\t\texpectAnimated bool\n\t\twantDuration   float64\n\t}{\n\t\t{\n\t\t\tname:           \"Static AVIF\",\n\t\t\tfilename:       \"testdata/colors_sdr_srgb.avif\",\n\t\t\texpectAnimated: false,\n\t\t\twantDuration:   0,\n\t\t},\n\t\t{\n\t\t\tname:           \"Animated AVIF\",\n\t\t\tfilename:       \"testdata/colors-animated-8bpc-alpha-exif-xmp.avif\",\n\t\t\texpectAnimated: true,\n\t\t\twantDuration:   0.833,\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\ttestAvifImage, err := os.ReadFile(tc.filename)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to read AVIF image: %v\", err)\n\t\t\t}\n\n\t\t\tdecoder, err := newAvifDecoder(testAvifImage)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to create decoder: %v\", err)\n\t\t\t}\n\t\t\tdefer decoder.Close()\n\n\t\t\tif tc.expectAnimated {\n\t\t\t\tduration := decoder.Duration().Seconds()\n\t\t\t\tif duration <= 0 {\n\t\t\t\t\tt.Errorf(\"Expected animated AVIF with duration > 0, got %v\", duration)\n\t\t\t\t}\n\t\t\t\tif !almostEqual(duration, tc.wantDuration, 0.01) {\n\t\t\t\t\tt.Errorf(\"Expected duration %v, got %v\", tc.wantDuration, duration)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif decoder.Duration().Seconds() != 0 {\n\t\t\t\t\tt.Errorf(\"Expected static AVIF with duration 0, got %v\", decoder.Duration().Seconds())\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\n// Basic Encoder Tests\n// ----------------------------------------\n\nfunc testNewAvifEncoder(t *testing.T) {\n\ttestCases := []struct {\n\t\tname     string\n\t\tfilename string\n\t}{\n\t\t{\"No ICC Profile\", \"testdata/colors_sdr_srgb.avif\"},\n\t\t{\"With ICC Profile\", \"testdata/paris_icc_exif_xmp.avif\"},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\ttestAvifImage, err := os.ReadFile(tc.filename)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Unexpected error while reading AVIF image: %v\", err)\n\t\t\t}\n\t\t\tdecoder, err := newAvifDecoder(testAvifImage)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Unexpected error while decoding AVIF image data: %v\", err)\n\t\t\t}\n\t\t\tdefer decoder.Close()\n\n\t\t\tdstBuf := make([]byte, destinationBufferSize)\n\t\t\tencoder, err := newAvifEncoder(decoder, dstBuf)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Unexpected error: %v\", err)\n\t\t\t}\n\t\t\tdefer encoder.Close()\n\t\t})\n\t}\n}\n\n// Decode/Encode Operation Tests\n// ----------------------------------------\n\nfunc testAvifDecoderDecodeTo(t *testing.T) {\n\ttestCases := []struct {\n\t\tname     string\n\t\tfilename string\n\t}{\n\t\t{\"No ICC Profile\", \"testdata/colors_sdr_srgb.avif\"},\n\t\t{\"With ICC Profile\", \"testdata/paris_icc_exif_xmp.avif\"},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\ttestAvifImage, err := os.ReadFile(tc.filename)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to read AVIF image: %v\", err)\n\t\t\t}\n\t\t\tdecoder, err := newAvifDecoder(testAvifImage)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to create a new AVIF decoder: %v\", err)\n\t\t\t}\n\t\t\tdefer decoder.Close()\n\n\t\t\theader, err := decoder.Header()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to get the header: %v\", err)\n\t\t\t}\n\t\t\tframebuffer := NewFramebuffer(header.width, header.height)\n\t\t\tif err = decoder.DecodeTo(framebuffer); err != nil {\n\t\t\t\tt.Errorf(\"DecodeTo failed unexpectedly: %v\", err)\n\t\t\t}\n\t\t})\n\t}\n\n\tt.Run(\"Invalid Framebuffer\", func(t *testing.T) {\n\t\ttestAvifImage, _ := os.ReadFile(\"testdata/colors_sdr_srgb.avif\")\n\t\tdecoder, _ := newAvifDecoder(testAvifImage)\n\t\tdefer decoder.Close()\n\n\t\tif err := decoder.DecodeTo(nil); err == nil {\n\t\t\tt.Error(\"DecodeTo with nil framebuffer should fail, but it did not\")\n\t\t}\n\t})\n}\n\nfunc testAvifEncoderEncode(t *testing.T) {\n\ttestCases := []struct {\n\t\tname     string\n\t\tfilename string\n\t\tquality  int\n\t}{\n\t\t{\"No ICC Profile\", \"testdata/colors_sdr_srgb.avif\", 60},\n\t\t{\"With ICC Profile\", \"testdata/paris_icc_exif_xmp.avif\", 80},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\ttestAvifImage, err := os.ReadFile(tc.filename)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to read AVIF image: %v\", err)\n\t\t\t}\n\n\t\t\tdecoder, err := newAvifDecoder(testAvifImage)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to create a new AVIF decoder: %v\", err)\n\t\t\t}\n\t\t\tdefer decoder.Close()\n\n\t\t\theader, err := decoder.Header()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to get the header: %v\", err)\n\t\t\t}\n\t\t\tframebuffer := NewFramebuffer(header.width, header.height)\n\t\t\tif err = framebuffer.resizeMat(header.width, header.height, header.pixelType); err != nil {\n\t\t\t\tt.Fatalf(\"Failed to resize the framebuffer: %v\", err)\n\t\t\t}\n\n\t\t\tdstBuf := make([]byte, destinationBufferSize)\n\t\t\tencoder, err := newAvifEncoder(decoder, dstBuf)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to create a new AVIF encoder: %v\", err)\n\t\t\t}\n\t\t\tdefer encoder.Close()\n\n\t\t\toptions := map[int]int{AvifQuality: tc.quality, AvifSpeed: 10}\n\t\t\tencodedData, err := encoder.Encode(framebuffer, options)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Encode failed unexpectedly: %v\", err)\n\t\t\t}\n\t\t\tif encodedData, err = encoder.Encode(nil, options); err != nil {\n\t\t\t\tt.Fatalf(\"Encode of empty frame failed unexpectedly: %v\", err)\n\t\t\t}\n\t\t\tif len(encodedData) == 0 {\n\t\t\t\tt.Fatalf(\"Encoded data is empty, but it should not be\")\n\t\t\t}\n\t\t})\n\t}\n}\n\n// Conversion Tests\n// ----------------------------------------\n\nfunc testAvifToWebPConversion(t *testing.T) {\n\ttestCases := []struct {\n\t\tname         string\n\t\tinputPath    string\n\t\toutputPath   string\n\t\twidth        int\n\t\theight       int\n\t\tquality      int\n\t\tresizeMethod ImageOpsSizeMethod\n\t}{\n\t\t{\n\t\t\tname:         \"AVIF to WebP conversion with no ICC Profile\",\n\t\t\tinputPath:    \"testdata/colors_sdr_srgb.avif\",\n\t\t\toutputPath:   \"testdata/out/colors_sdr_srgb_converted.webp\",\n\t\t\twidth:        100,\n\t\t\theight:       100,\n\t\t\tquality:      80,\n\t\t\tresizeMethod: ImageOpsFit,\n\t\t},\n\t\t{\n\t\t\tname:         \"AVIF to WebP conversion with ICC Profile\",\n\t\t\tinputPath:    \"testdata/paris_icc_exif_xmp.avif\",\n\t\t\toutputPath:   \"testdata/out/paris_icc_exif_xmp_converted.webp\",\n\t\t\twidth:        200,\n\t\t\theight:       150,\n\t\t\tquality:      60,\n\t\t\tresizeMethod: ImageOpsFit,\n\t\t},\n\t}\n\n\trunConversionTest(t, testCases, func(tc struct {\n\t\tname         string\n\t\tinputPath    string\n\t\toutputPath   string\n\t\twidth        int\n\t\theight       int\n\t\tquality      int\n\t\tresizeMethod ImageOpsSizeMethod\n\t}) (Decoder, error) {\n\t\ttestAvifImage, err := os.ReadFile(tc.inputPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn newAvifDecoder(testAvifImage)\n\t}, \".webp\", WebpQuality)\n}\n\n// Animation Tests\n// ----------------------------------------\n\nfunc testNewAvifDecoderWithAnimatedSource(t *testing.T) {\n\ttestCases := []struct {\n\t\tname                  string\n\t\tinputPath             string\n\t\toutputPath            string\n\t\twidth                 int\n\t\theight                int\n\t\tquality               int\n\t\tresizeMethod          ImageOpsSizeMethod\n\t\toutputType            string\n\t\tdisableAnimatedOutput bool\n\t}{\n\t\t{\n\t\t\tname:                  \"Animated AVIF to WebP encoding\",\n\t\t\tinputPath:             \"testdata/colors-animated-8bpc-alpha-exif-xmp.avif\",\n\t\t\toutputPath:            \"testdata/out/animated_sample_out.webp\",\n\t\t\twidth:                 100,\n\t\t\theight:                100,\n\t\t\tquality:               60,\n\t\t\tresizeMethod:          ImageOpsFit,\n\t\t\toutputType:            \".webp\",\n\t\t\tdisableAnimatedOutput: false,\n\t\t},\n\t\t{\n\t\t\tname:                  \"Animated AVIF to AVIF encoding\",\n\t\t\tinputPath:             \"testdata/colors-animated-8bpc-alpha-exif-xmp.avif\",\n\t\t\toutputPath:            \"testdata/out/animated_sample_out.avif\",\n\t\t\twidth:                 100,\n\t\t\theight:                100,\n\t\t\tquality:               60,\n\t\t\tresizeMethod:          ImageOpsFit,\n\t\t\toutputType:            \".avif\",\n\t\t\tdisableAnimatedOutput: false,\n\t\t},\n\t}\n\n\trunAnimationTest(t, testCases, func(tc struct {\n\t\tname                  string\n\t\tinputPath             string\n\t\toutputPath            string\n\t\twidth                 int\n\t\theight                int\n\t\tquality               int\n\t\tresizeMethod          ImageOpsSizeMethod\n\t\toutputType            string\n\t\tdisableAnimatedOutput bool\n\t}) (Decoder, error) {\n\t\ttestAvifImage, err := os.ReadFile(tc.inputPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn newAvifDecoder(testAvifImage)\n\t})\n}\n\nfunc testNewAvifEncoderWithWebPAnimatedSource(t *testing.T) {\n\ttestCases := []struct {\n\t\tname                  string\n\t\tinputPath             string\n\t\toutputPath            string\n\t\twidth                 int\n\t\theight                int\n\t\tquality               int\n\t\tresizeMethod          ImageOpsSizeMethod\n\t\toutputType            string\n\t\tdisableAnimatedOutput bool\n\t}{\n\t\t{\n\t\t\tname:                  \"Animated WebP to Animated AVIF encoding\",\n\t\t\tinputPath:             \"testdata/animated-webp-supported.webp\",\n\t\t\toutputPath:            \"testdata/out/animated-webp-supported_out_fit.avif\",\n\t\t\twidth:                 400,\n\t\t\theight:                400,\n\t\t\tquality:               95,\n\t\t\tresizeMethod:          ImageOpsFit,\n\t\t\toutputType:            \".avif\",\n\t\t\tdisableAnimatedOutput: false,\n\t\t},\n\t}\n\n\trunAnimationTest(t, testCases, func(tc struct {\n\t\tname                  string\n\t\tinputPath             string\n\t\toutputPath            string\n\t\twidth                 int\n\t\theight                int\n\t\tquality               int\n\t\tresizeMethod          ImageOpsSizeMethod\n\t\toutputType            string\n\t\tdisableAnimatedOutput bool\n\t}) (Decoder, error) {\n\t\ttestWebpImage, err := os.ReadFile(tc.inputPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn newWebpDecoder(testWebpImage)\n\t})\n}\n\nfunc testNewAvifEncoderWithVideoSource(t *testing.T) {\n\ttestCases := []struct {\n\t\tname                  string\n\t\tinputPath             string\n\t\toutputPath            string\n\t\twidth                 int\n\t\theight                int\n\t\tquality               int\n\t\tresizeMethod          ImageOpsSizeMethod\n\t\toutputType            string\n\t\tdisableAnimatedOutput bool\n\t}{\n\t\t{\n\t\t\tname:                  \"MP4 Video to Resized AVIF\",\n\t\t\tinputPath:             \"testdata/big_buck_bunny_480p_10s_std.mp4\",\n\t\t\toutputPath:            \"testdata/out/video_resized.avif\",\n\t\t\twidth:                 320,\n\t\t\theight:                240,\n\t\t\tquality:               60,\n\t\t\tresizeMethod:          ImageOpsFit,\n\t\t\toutputType:            \".avif\",\n\t\t\tdisableAnimatedOutput: true,\n\t\t},\n\t}\n\n\trunAnimationTest(t, testCases, func(tc struct {\n\t\tname                  string\n\t\tinputPath             string\n\t\toutputPath            string\n\t\twidth                 int\n\t\theight                int\n\t\tquality               int\n\t\tresizeMethod          ImageOpsSizeMethod\n\t\toutputType            string\n\t\tdisableAnimatedOutput bool\n\t}) (Decoder, error) {\n\t\ttestVideoData, err := os.ReadFile(tc.inputPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn newAVCodecDecoder(testVideoData)\n\t})\n}\n\n// Helper Functions\n// ----------------------------------------\n\nfunc runConversionTest(t *testing.T, testCases interface{}, decoderFactory interface{}, outputType string, qualityType int) {\n\tcases := testCases.([]struct {\n\t\tname         string\n\t\tinputPath    string\n\t\toutputPath   string\n\t\twidth        int\n\t\theight       int\n\t\tquality      int\n\t\tresizeMethod ImageOpsSizeMethod\n\t})\n\n\tfor _, tc := range cases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tdecoder, err := decoderFactory.(func(struct {\n\t\t\t\tname         string\n\t\t\t\tinputPath    string\n\t\t\t\toutputPath   string\n\t\t\t\twidth        int\n\t\t\t\theight       int\n\t\t\t\tquality      int\n\t\t\t\tresizeMethod ImageOpsSizeMethod\n\t\t\t}) (Decoder, error))(tc)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to create decoder: %v\", err)\n\t\t\t}\n\t\t\tdefer decoder.Close()\n\n\t\t\tdstBuf := make([]byte, destinationBufferSize)\n\t\t\toptions := &ImageOptions{\n\t\t\t\tFileType:             outputType,\n\t\t\t\tNormalizeOrientation: true,\n\t\t\t\tEncodeOptions:        map[int]int{qualityType: tc.quality},\n\t\t\t\tResizeMethod:         tc.resizeMethod,\n\t\t\t\tWidth:                tc.width,\n\t\t\t\tHeight:               tc.height,\n\t\t\t\tEncodeTimeout:        time.Second * 30,\n\t\t\t}\n\n\t\t\tops := NewImageOps(2000)\n\t\t\tdefer ops.Close()\n\n\t\t\tnewDst, err := ops.Transform(decoder, options, dstBuf)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Transform failed: %v\", err)\n\t\t\t}\n\n\t\t\tif len(newDst) == 0 {\n\t\t\t\tt.Fatal(\"Transform returned empty data\")\n\t\t\t}\n\n\t\t\tensureOutputDir(t)\n\t\t\tif err = os.WriteFile(tc.outputPath, newDst, 0644); err != nil {\n\t\t\t\tt.Fatalf(\"Failed to write output file: %v\", err)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc runAnimationTest(t *testing.T, testCases interface{}, decoderFactory interface{}) {\n\tcases := testCases.([]struct {\n\t\tname                  string\n\t\tinputPath             string\n\t\toutputPath            string\n\t\twidth                 int\n\t\theight                int\n\t\tquality               int\n\t\tresizeMethod          ImageOpsSizeMethod\n\t\toutputType            string\n\t\tdisableAnimatedOutput bool\n\t})\n\n\tfor _, tc := range cases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tdecoder, err := decoderFactory.(func(struct {\n\t\t\t\tname                  string\n\t\t\t\tinputPath             string\n\t\t\t\toutputPath            string\n\t\t\t\twidth                 int\n\t\t\t\theight                int\n\t\t\t\tquality               int\n\t\t\t\tresizeMethod          ImageOpsSizeMethod\n\t\t\t\toutputType            string\n\t\t\t\tdisableAnimatedOutput bool\n\t\t\t}) (Decoder, error))(tc)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to create decoder: %v\", err)\n\t\t\t}\n\t\t\tdefer decoder.Close()\n\n\t\t\tdstBuf := make([]byte, destinationBufferSize)\n\t\t\toptions := &ImageOptions{\n\t\t\t\tFileType:              tc.outputType,\n\t\t\t\tNormalizeOrientation:  true,\n\t\t\t\tEncodeOptions:         map[int]int{AvifQuality: tc.quality, AvifSpeed: 10},\n\t\t\t\tResizeMethod:          tc.resizeMethod,\n\t\t\t\tWidth:                 tc.width,\n\t\t\t\tHeight:                tc.height,\n\t\t\t\tEncodeTimeout:         time.Second * 30,\n\t\t\t\tDisableAnimatedOutput: tc.disableAnimatedOutput,\n\t\t\t}\n\n\t\t\tops := NewImageOps(2000)\n\t\t\tdefer ops.Close()\n\n\t\t\tnewDst, err := ops.Transform(decoder, options, dstBuf)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Transform failed: %v\", err)\n\t\t\t}\n\n\t\t\tif len(newDst) == 0 {\n\t\t\t\tt.Fatal(\"Transform returned empty data\")\n\t\t\t}\n\n\t\t\tensureOutputDir(t)\n\t\t\tif err = os.WriteFile(tc.outputPath, newDst, 0644); err != nil {\n\t\t\t\tt.Fatalf(\"Failed to write output file: %v\", err)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc ensureOutputDir(t *testing.T) {\n\tif _, err := os.Stat(\"testdata/out\"); os.IsNotExist(err) {\n\t\tif err = os.Mkdir(\"testdata/out\", 0755); err != nil {\n\t\t\tt.Fatalf(\"Failed to create output directory: %v\", err)\n\t\t}\n\t}\n}\n\nfunc almostEqual(a, b, tolerance float64) bool {\n\treturn math.Abs(a-b) <= tolerance\n}\n"
        },
        {
          "name": "cgo.go",
          "type": "blob",
          "size": 1.111328125,
          "content": "package lilliput\n\n/*\n#cgo darwin CFLAGS: -I${SRCDIR}/deps/osx/include\n#cgo linux CFLAGS: -msse -msse2 -msse3 -msse4.1 -msse4.2 -mavx -I${SRCDIR}/deps/linux/include\n#cgo CXXFLAGS: -std=c++11\n#cgo darwin CXXFLAGS: -I${SRCDIR}/deps/osx/include\n#cgo linux CXXFLAGS: -I${SRCDIR}/deps/linux/include\n#cgo darwin LDFLAGS: -L${SRCDIR}/deps/osx/lib -lavif -lyuv -laom -lavcodec -lavfilter -lavformat -lavutil -lbz2 -lgif -ljpeg -lopencv_core -lopencv_imgcodecs -lopencv_imgproc -lpng -lsharpyuv -lswscale -lwebp -lwebpmux -lz -framework CoreFoundation -framework CoreMedia -framework CoreVideo -framework VideoToolbox\n#cgo linux LDFLAGS: -L${SRCDIR}/deps/linux/lib -L${SRCDIR}/deps/linux/share/OpenCV/3rdparty/lib -lswscale -lavformat -lavcodec -lavfilter -lavutil -lbz2 -lz -lopencv_core -lopencv_imgcodecs -lopencv_imgproc -ljpeg -lpng -lwebp -lz -lgif -lopencv_core -lopencv_imgcodecs -lopencv_imgproc -ljpeg -lpng -lwebp -lsharpyuv -lz -lopencv_core -lopencv_imgcodecs -lopencv_imgproc -ljpeg -lpng -lwebp -lz -lopencv_core -lopencv_imgproc -lwebp -lwebpmux -lippicv -lavif -lyuv -laom\nvoid dummy() {}\n*/\nimport \"C\"\n\nfunc init() {\n\tC.dummy()\n}\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "deps",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "giflib.cpp",
          "type": "blob",
          "size": 43.7529296875,
          "content": "#include \"giflib.hpp\"\n#include \"gif_lib.h\"\n#include <stdbool.h>\n\nstruct giflib_decoder_struct {\n    GifFileType* gif;\n    const cv::Mat* mat;\n    ptrdiff_t read_index;\n    GifByteType* pixels;\n    size_t pixel_len;\n    int prev_frame_disposal;\n    int prev_frame_delay_time;\n    int prev_frame_left;\n    int prev_frame_top;\n    int prev_frame_width;\n    int prev_frame_height;\n    uint8_t bg_green;\n    uint8_t bg_red;\n    uint8_t bg_blue;\n    uint8_t bg_alpha;\n    bool have_read_first_frame;\n    bool seek_clear_extensions;\n};\n\n// this structure will help save us work of \"reversing\" a palette\n// we will bit-crush a RGB value and use it to look up one of these\n// entries, which if present, prevents us from searching for the\n// nearest palette entry for that color\ntypedef struct {\n    uint8_t index;\n    uint8_t present;\n} encoder_palette_lookup;\n\nstruct giflib_encoder_struct {\n    GifFileType* gif;\n    uint8_t* dst;\n    size_t dst_len;\n    ptrdiff_t dst_offset;\n\n    // palette lookup is a computational-saving structure to convert\n    // (reduced-depth) RGB values into the frame's 256-entry palette\n    encoder_palette_lookup* palette_lookup;\n\n    GifByteType* pixels;\n    size_t pixel_len;\n\n    ColorMapObject* frame_color_map;\n    ColorMapObject* prev_frame_color_map;\n\n    int prev_frame_disposal;\n\n    uint8_t* prev_frame_bgra;\n\n    bool have_written_first_frame;\n\n    // keep track of all of the things we've allocated\n    // we could technically just stuff all of these into a vector\n    // of void*s but it might be interesting to build a pool\n    // for these later, so it makes sense to keep them separated\n    // n.b. that even if we do that, giflib still uses hella mallocs\n    // when building the decoder, so it would only save us on the encoder\n    std::vector<ExtensionBlock*> extension_blocks;\n    std::vector<GifByteType*> gif_bytes;\n    std::vector<ColorMapObject*> color_maps;\n    std::vector<GifColorType*> colors;\n    std::vector<SavedImage*> saved_images;\n};\n\nint decode_func(GifFileType* gif, GifByteType* buf, int len)\n{\n    auto d = static_cast<giflib_decoder>(gif->UserData);\n    size_t buf_len = d->mat->total() - d->read_index;\n    size_t read_len = (buf_len > len) ? len : buf_len;\n    memmove(buf, d->mat->data + d->read_index, read_len);\n    d->read_index += read_len;\n    return read_len;\n}\n\ngiflib_decoder giflib_decoder_create(const opencv_mat buf)\n{\n    giflib_decoder d = new struct giflib_decoder_struct();\n    memset(d, 0, sizeof(struct giflib_decoder_struct));\n    d->mat = static_cast<const cv::Mat*>(buf);\n\n    int error = 0;\n    GifFileType* gif = DGifOpen(d, decode_func, &error);\n    if (error) {\n        delete d;\n        return NULL;\n    }\n    d->gif = gif;\n\n    return d;\n}\n\nint giflib_decoder_get_width(const giflib_decoder d)\n{\n    return d->gif->SWidth;\n}\n\nint giflib_decoder_get_height(const giflib_decoder d)\n{\n    return d->gif->SHeight;\n}\n\nint giflib_decoder_get_num_frames(const giflib_decoder d)\n{\n    return d->gif->ImageCount;\n}\n\nint giflib_decoder_get_frame_width(const giflib_decoder d)\n{\n    return d->gif->Image.Width;\n}\n\nint giflib_decoder_get_frame_height(const giflib_decoder d)\n{\n    return d->gif->Image.Height;\n}\n\nint giflib_decoder_get_prev_frame_delay(const giflib_decoder d)\n{\n    return d->prev_frame_delay_time;\n}\n\nint giflib_decoder_get_prev_frame_disposal(const giflib_decoder d)\n{\n    switch (d->prev_frame_disposal) {\n        case DISPOSE_DO_NOT:\n            return GIF_DISPOSE_NONE;\n        default:\n            return GIF_DISPOSE_BACKGROUND;\n    }\n}\n\nvoid giflib_decoder_release(giflib_decoder d)\n{\n    if (d->pixels) {\n        free(d->pixels);\n    }\n    int error = 0;\n    DGifCloseFile(d->gif, &error);\n    delete d;\n}\n\nstatic bool giflib_decoder_read_extensions(giflib_decoder d)\n{\n    GifByteType* ExtData;\n    int ExtFunction;\n\n    if (DGifGetExtension(d->gif, &ExtFunction, &ExtData) == GIF_ERROR) {\n        return false;\n    }\n\n    // XXX filter out everything but GRAPHICS_EXT_FUNC_CODE\n    if (ExtData != NULL) {\n        int res = GifAddExtensionBlock(&d->gif->ExtensionBlockCount,\n                                       &d->gif->ExtensionBlocks,\n                                       ExtFunction,\n                                       ExtData[0],\n                                       &ExtData[1]);\n        if (res == GIF_ERROR) {\n            return false;\n        }\n    }\n\n    while (ExtData != NULL) {\n        if (DGifGetExtensionNext(d->gif, &ExtData) == GIF_ERROR) {\n            return false;\n        }\n\n        if (ExtData != NULL) {\n            int res = GifAddExtensionBlock(&d->gif->ExtensionBlockCount,\n                                           &d->gif->ExtensionBlocks,\n                                           CONTINUE_EXT_FUNC_CODE,\n                                           ExtData[0],\n                                           &ExtData[1]);\n            if (res == GIF_ERROR) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n\nstatic bool giflib_get_frame_gcb(GifFileType* gif, GraphicsControlBlock* gcb)\n{\n    gcb->DisposalMode = DISPOSAL_UNSPECIFIED;\n    gcb->UserInputFlag = false;\n    gcb->DelayTime = 0;\n    gcb->TransparentColor = NO_TRANSPARENT_COLOR;\n\n    bool success = true;\n    for (int i = 0; i < gif->ExtensionBlockCount; i++) {\n        ExtensionBlock* b = &gif->ExtensionBlocks[i];\n        if (b->Function == GRAPHICS_EXT_FUNC_CODE) {\n            int res = DGifExtensionToGCB(b->ByteCount, b->Bytes, gcb);\n            success = res == GIF_OK;\n        }\n    }\n\n    return success;\n}\n\nstatic bool giflib_set_frame_gcb(GifFileType* gif, const GraphicsControlBlock* gcb)\n{\n    bool success = true;\n    for (int i = 0; i < gif->ExtensionBlockCount; i++) {\n        ExtensionBlock* b = &gif->ExtensionBlocks[i];\n        if (b->Function == GRAPHICS_EXT_FUNC_CODE) {\n            int res = EGifGCBToExtension(gcb, b->Bytes);\n            success = res == GIF_OK;\n        }\n    }\n\n    return success;\n}\n\nstatic giflib_decoder_frame_state giflib_decoder_seek_next_frame(giflib_decoder d)\n{\n    GifRecordType RecordType;\n\n    if (d->seek_clear_extensions) {\n        GifFreeExtensions(&d->gif->ExtensionBlockCount, &d->gif->ExtensionBlocks);\n        d->seek_clear_extensions = false;\n    }\n\n    do {\n        if (DGifGetRecordType(d->gif, &RecordType) == GIF_ERROR) {\n            return giflib_decoder_error;\n        }\n\n        switch (RecordType) {\n        case IMAGE_DESC_RECORD_TYPE:\n            // we are now at the next frame, so quit\n            return giflib_decoder_have_next_frame;\n\n        case EXTENSION_RECORD_TYPE:\n            if (!giflib_decoder_read_extensions(d)) {\n                return giflib_decoder_error;\n            }\n            break;\n\n        case TERMINATE_RECORD_TYPE:\n            break;\n\n        default:\n            break;\n        }\n    } while (RecordType != TERMINATE_RECORD_TYPE);\n\n    return giflib_decoder_eof;\n}\n\n// get just the header without attempting to read its pixel data\n// this sets the image properties on d->gif->Image\n// includes dimensions and frame origin coordinates, color map\ngiflib_decoder_frame_state giflib_decoder_decode_frame_header(giflib_decoder d)\n{\n    giflib_decoder_frame_state seek = giflib_decoder_seek_next_frame(d);\n\n    if (seek == giflib_decoder_eof || seek == giflib_decoder_error) {\n        return seek;\n    }\n\n    if (DGifGetImageHeader(d->gif) == GIF_ERROR) {\n        return giflib_decoder_error;\n    }\n\n    return giflib_decoder_have_next_frame;\n}\n\nstatic bool giflib_decoder_render_frame(giflib_decoder d, GraphicsControlBlock* gcb, opencv_mat mat)\n{\n    auto cvMat = static_cast<cv::Mat*>(mat);\n    GifImageDesc desc = d->gif->Image;\n    int transparency_index = gcb->TransparentColor;\n\n    if (desc.Width < 0) {\n        fprintf(stderr, \"encountered error, gif frame width less than 0\\n\");\n        return false;\n    }\n\n    if (desc.Height < 0) {\n        fprintf(stderr, \"encountered error, gif frame height less than 0\\n\");\n        return false;\n    }\n\n    int frame_left = desc.Left;\n    int frame_top = desc.Top;\n    int frame_width = desc.Width;\n    int frame_height = desc.Height;\n\n    int buf_width = cvMat->cols;\n    int buf_height = cvMat->rows;\n\n    // calculate the out-of-bounds skip lengths\n    // for whatever reason, gifs allow frames to draw outside of the viewport\n    // we can't just cap these values because we have to skip the (unrenderable!) raster bits\n    int skip_left = (frame_left < 0) ? -frame_left : 0;\n    int skip_top = (frame_top < 0) ? -frame_top : 0;\n    int skip_right =\n      (frame_left + frame_width > buf_width) ? (frame_left + frame_width - buf_width) : 0;\n    int skip_bottom =\n      (frame_top + frame_height > buf_height) ? (frame_top + frame_height - buf_height) : 0;\n\n    ColorMapObject* globalColorMap = d->gif->SColorMap;\n    ColorMapObject* frameColorMap = desc.ColorMap;\n    ColorMapObject* colorMap = frameColorMap ? frameColorMap : globalColorMap;\n\n    if (!colorMap) {\n        fprintf(stderr, \"encountered error, gif frame has no color map\\n\");\n        return false;\n    }\n\n    if (!d->have_read_first_frame) {\n        // first frame -- draw the background\n        for (size_t y = 0; y < buf_height; y++) {\n            uint8_t* dst = cvMat->data + y * cvMat->step;\n            for (size_t x = 0; x < buf_width; x++) {\n                *dst++ = d->bg_blue;\n                *dst++ = d->bg_green;\n                *dst++ = d->bg_red;\n                *dst++ = d->bg_alpha;\n            }\n        }\n    }\n\n    if (d->have_read_first_frame) {\n        if (d->prev_frame_disposal == DISPOSE_BACKGROUND) {\n            // draw over the previous frame with the BG color\n            int prev_frame_left = d->prev_frame_left;\n            int prev_frame_top = d->prev_frame_top;\n            int prev_frame_width = d->prev_frame_width;\n            int prev_frame_height = d->prev_frame_height;\n\n            if (prev_frame_left < 0) {\n                // \"subtract\" the width that hangs off the left edge\n                prev_frame_width += prev_frame_left;\n                prev_frame_left = 0;\n            }\n\n            if (prev_frame_top < 0) {\n                // do same subtracting for height off top edge\n                prev_frame_height += prev_frame_top;\n                prev_frame_top = 0;\n            }\n\n            if (prev_frame_left + prev_frame_width > buf_width) {\n                // cap width to keep frame within right edge\n                prev_frame_width = buf_width - prev_frame_left;\n            }\n\n            if (prev_frame_top + prev_frame_height > buf_height) {\n                // do same cap to keep frame within bottom edge\n                prev_frame_height = buf_height - prev_frame_top;\n            }\n\n            // if either of these is true, we'll just do nothing in the loop\n            // we could bail out of here somehow, seems easiest to do it this way\n            prev_frame_height = (prev_frame_height < 0) ? 0 : prev_frame_height;\n            prev_frame_width = (prev_frame_width < 0) ? 0 : prev_frame_width;\n\n            for (int y = prev_frame_top; y < prev_frame_top + prev_frame_height; y++) {\n                uint8_t* dst = cvMat->data + y * cvMat->step + (prev_frame_left * 4);\n                for (int x = prev_frame_left; x < prev_frame_left + prev_frame_width; x++) {\n                    *dst++ = d->bg_blue;\n                    *dst++ = d->bg_green;\n                    *dst++ = d->bg_red;\n                    *dst++ = d->bg_alpha;\n                }\n            }\n        }\n        else if (d->prev_frame_disposal == DISPOSE_PREVIOUS) {\n            // TODO or maybe not to do\n            // should we at least log this happened so that we know this exists?\n            // tldr this crazy method requires you to walk back across all previous\n            //    frames until you reach one with DISPOSAL_DO_NOT\n            //    and \"undraw them\", most likely would be done by building a temp\n            //    buffer when first one is encountered\n        }\n    }\n\n    // TODO handle interlaced gifs?\n\n    // TODO if top > 0 or left > 0, we could actually just return an ROI\n    // of the pixel buffer and then resize just the ROI frame\n    // we would then have to rescale the origin coordinates of that frame\n    // when encoding back to gif, so that the resized frame is drawn to the\n    // correct location\n    int pixel_index = 0;\n\n    // skip entire rows at the top if frame_top < 0\n    // start by skipping the raster bits -- we're skipping full rows here\n    pixel_index += (skip_top * desc.Width);\n    // now reduce how far we iterate by subtracting how many rows we skipped\n    // if we were supposed to start at y = -2 and go for 5 rows, then instead\n    // start at y = 0 and go for 3 rows\n    frame_height -= skip_top;\n    // move the top of the frame over by how far we skipped\n    frame_top += skip_top;\n\n    // do similar thing for left-side skip as with top-side\n    // here we only skip by some columns, not an entire row\n    frame_width -= skip_left;\n    frame_left += skip_left;\n\n    // right-side skip requires shortening the loop iter and moving raster pointer\n    // here we just shorten loop, move raster at bottom of row loop\n    frame_width -= skip_right;\n\n    // bottom skip is simple, we just reduce # of rows we do\n    frame_height -= skip_bottom;\n\n    for (int y = frame_top; y < frame_top + frame_height; y++) {\n        // draw a single row of pixels in this iteration\n\n        // do actual column skipping here\n        pixel_index += skip_left;\n\n        uint8_t* dst = cvMat->data + y * cvMat->step + (frame_left * 4);\n        for (int x = frame_left; x < frame_left + frame_width; x++) {\n            // draw a single pixel in this iteration\n            GifByteType palette_index = d->pixels[pixel_index++];\n            if (palette_index == transparency_index) {\n                // TODO: don't hardcode 4 channels (8UC4) here\n                dst += 4;\n                continue;\n            }\n            *dst++ = colorMap->Colors[palette_index].Blue;\n            *dst++ = colorMap->Colors[palette_index].Green;\n            *dst++ = colorMap->Colors[palette_index].Red;\n            *dst++ = 255;\n        }\n\n        pixel_index += skip_right;\n    }\n\n    // because we turn partial frames into full frames, we need to ensure that a transparency color\n    // is defined, so that the encoder can use it (we convert partial frames to full frames with\n    // a lot of transparency)\n\n    // let's check if we have a partial frame and whether no transparency is defined\n    bool have_partial_frame = false;\n    have_partial_frame |= frame_height < buf_height;\n    have_partial_frame |= frame_width < buf_width;\n    have_partial_frame |= frame_left != 0;\n    have_partial_frame |= frame_top != 0;\n\n    if (have_partial_frame && transparency_index == -1) {\n        // make sure our pseudo partial frame impl can use transparency\n        // if no color is set, force the palette to have one\n        // evict the last color and make it transparency instead\n        gcb->TransparentColor = colorMap->ColorCount - 1;\n        giflib_set_frame_gcb(d->gif, gcb);\n    }\n\n    return true;\n}\n\ngiflib_decoder_frame_state giflib_decoder_skip_frame(giflib_decoder d)\n{\n    giflib_decoder_frame_state seek = giflib_decoder_decode_frame_header(d);\n\n    if (seek != giflib_decoder_have_next_frame) {\n        return seek;\n    }\n\n    GifByteType* block;\n    while (true) {\n        if (DGifGetCodeNext(d->gif, &block) == GIF_ERROR) {\n            return giflib_decoder_error;\n        }\n\n        if (block == NULL) {\n            break;\n        }\n    }\n\n    return giflib_decoder_have_next_frame;\n}\n\nstatic int interlace_offset[] = {0, 4, 2, 1};\nstatic int interlace_jumps[] = {8, 8, 4, 2};\n\nstatic void extract_background_color(GifFileType* gif, GraphicsControlBlock* gcb, \n                                   uint8_t* r, uint8_t* g, uint8_t* b, uint8_t* a) {\n    bool have_transparency = (gcb->TransparentColor != NO_TRANSPARENT_COLOR);\n    if (have_transparency) {\n        *r = *g = *b = *a = 0;\n    }\n    else if (gif->SColorMap && gif->SColorMap->Colors) {\n        *r = gif->SColorMap->Colors[gif->SBackGroundColor].Red;\n        *g = gif->SColorMap->Colors[gif->SBackGroundColor].Green;\n        *b = gif->SColorMap->Colors[gif->SBackGroundColor].Blue;\n        *a = 255;\n    }\n    else {\n        *r = *g = *b = *a = 255;\n    }\n}\n\n// decode the full frame and write it into mat\n// decode_frame_header *must* be called before this function\nbool giflib_decoder_decode_frame(giflib_decoder d, opencv_mat mat)\n{\n    GifImageDesc desc = d->gif->Image;\n\n    if (desc.Width <= 0) {\n        fprintf(stderr, \"encountered error, gif frame has negative or zero width\\n\");\n        return false;\n    }\n\n    if (desc.Height <= 0) {\n        fprintf(stderr, \"encountered error, gif frame has negative or zero height\\n\");\n        return false;\n    }\n\n    if (desc.Width > (INT_MAX / desc.Height)) {\n        fprintf(stderr, \"encountered error, gif frame is too wide\\n\");\n        return false;\n    }\n\n    // since we aren't actually writing into mat, we don't check for large\n    // dimensions here. it is up to the caller to do that after reading the\n    // header\n\n    size_t image_size = desc.Width * desc.Height;\n\n    if (image_size > (SIZE_MAX / sizeof(GifPixelType))) {\n        fprintf(stderr, \"encountered error, gif frame is too large\\n\");\n        return false;\n    }\n\n    if (image_size > d->pixel_len) {\n        // only realloc if we need to size up\n        // no point in shrinking, we'll free when decode has finished\n        d->pixel_len = image_size;\n        d->pixels = (GifByteType*)(realloc(d->pixels, d->pixel_len * sizeof(GifPixelType)));\n    }\n\n    if (d->pixels == NULL) {\n        fprintf(stderr, \"encountered error, gif pixel buffer failed to allocate\\n\");\n        return false;\n    }\n\n    if (desc.Interlace) {\n        for (int i = 0; i < sizeof(interlace_offset) / sizeof(int); i++) {\n            for (int j = interlace_offset[i]; j < desc.Height; j += interlace_jumps[i]) {\n                int res = DGifGetLine(d->gif, d->pixels + j * desc.Width, desc.Width);\n                if (res == GIF_ERROR) {\n                    fprintf(stderr, \"encountered error, could not rasterize gif line\\n\");\n                    return false;\n                }\n            }\n        }\n    }\n    else {\n        int res = DGifGetLine(d->gif, d->pixels, image_size);\n        if (res == GIF_ERROR) {\n            fprintf(stderr, \"encountered error, could not rasterize gif\\n\");\n            return false;\n        }\n    }\n\n    GraphicsControlBlock gcb;\n    giflib_get_frame_gcb(d->gif, &gcb);\n\n    if (!d->have_read_first_frame) {\n        GraphicsControlBlock gcb;\n        giflib_get_frame_gcb(d->gif, &gcb);\n        extract_background_color(d->gif, &gcb, &d->bg_red, &d->bg_green, &d->bg_blue, &d->bg_alpha);\n    }\n\n    if (!giflib_decoder_render_frame(d, &gcb, mat)) {\n        return false;\n    }\n\n    d->prev_frame_disposal = gcb.DisposalMode;\n    d->prev_frame_delay_time = gcb.DelayTime;\n    d->prev_frame_left = d->gif->Image.Left;\n    d->prev_frame_top = d->gif->Image.Top;\n    d->prev_frame_width = d->gif->Image.Width;\n    d->prev_frame_height = d->gif->Image.Height;\n    d->have_read_first_frame = true;\n    d->seek_clear_extensions = true;\n\n    return true;\n}\n\nExtensionBlock* giflib_encoder_allocate_extension_blocks(giflib_encoder e, size_t count)\n{\n    ExtensionBlock* blocks = (ExtensionBlock*)(malloc(count * sizeof(ExtensionBlock)));\n    e->extension_blocks.push_back(blocks);\n    return blocks;\n}\n\nGifByteType* giflib_encoder_allocate_gif_bytes(giflib_encoder e, size_t count)\n{\n    GifByteType* bytes = (GifByteType*)(malloc(count * sizeof(GifByteType)));\n    e->gif_bytes.push_back(bytes);\n    return bytes;\n}\n\nColorMapObject* giflib_encoder_allocate_color_maps(giflib_encoder e, size_t count)\n{\n    ColorMapObject* color_maps = (ColorMapObject*)(malloc(count * sizeof(ColorMapObject)));\n    e->color_maps.push_back(color_maps);\n    return color_maps;\n}\n\nGifColorType* giflib_encoder_allocate_colors(giflib_encoder e, size_t count)\n{\n    GifColorType* colors = (GifColorType*)(malloc(count * sizeof(GifColorType)));\n    e->colors.push_back(colors);\n    return colors;\n}\n\nSavedImage* giflib_encoder_allocate_saved_images(giflib_encoder e, size_t count)\n{\n    SavedImage* saved_images = (SavedImage*)(malloc(count * sizeof(SavedImage)));\n    e->saved_images.push_back(saved_images);\n    return saved_images;\n}\n\nint encode_func(GifFileType* gif, const GifByteType* buf, int len)\n{\n    giflib_encoder e = static_cast<giflib_encoder>(gif->UserData);\n    if (e->dst_offset + len > e->dst_len) {\n        return 0;\n    }\n    memcpy(e->dst + e->dst_offset, &buf[0], len);\n    e->dst_offset += len;\n    return len;\n}\n\ngiflib_encoder giflib_encoder_create(void* buf, size_t buf_len)\n{\n    giflib_encoder e = new struct giflib_encoder_struct();\n    memset(e, 0, sizeof(struct giflib_encoder_struct));\n    e->dst = (uint8_t*)(buf);\n    e->dst_len = buf_len;\n\n    int error = 0;\n    GifFileType* gif_out = EGifOpen(e, encode_func, &error);\n    if (error) {\n        fprintf(stderr, \"encountered error opening gif, %d\\n\", error);\n        delete e;\n        return NULL;\n    }\n    e->gif = gif_out;\n\n    // set up palette lookup table. we need 2^15 entries because we will be\n    // using bit-crushed RGB values, 5 bits each. this is a reasonable compromise\n    // between fidelity and computation/storage\n    e->palette_lookup =\n      (encoder_palette_lookup*)(malloc((1 << 15) * sizeof(encoder_palette_lookup)));\n\n    return e;\n}\n\n// this function should be called just once when we know the global dimensions\nbool giflib_encoder_init(giflib_encoder e, const giflib_decoder d, int width, int height)\n{\n    // all gifs will output as gif89\n    EGifSetGifVersion(e->gif, true);\n    e->gif->SWidth = width;\n    e->gif->SHeight = height;\n\n    e->prev_frame_bgra = (uint8_t*)(malloc(width * height * 4));\n\n    // preserve # of palette entries and aspect ratio of original gif\n    e->gif->SColorResolution = d->gif->SColorResolution;\n    e->gif->AspectByte = d->gif->AspectByte;\n\n    // copy global color palette, if any\n    if (d->gif->SColorMap) {\n        e->gif->SColorMap = giflib_encoder_allocate_color_maps(e, 1);\n        memmove(e->gif->SColorMap, d->gif->SColorMap, sizeof(ColorMapObject));\n        e->gif->SColorMap->Colors =\n          giflib_encoder_allocate_colors(e, e->gif->SColorMap->ColorCount);\n        memmove(e->gif->SColorMap->Colors,\n                d->gif->SColorMap->Colors,\n                e->gif->SColorMap->ColorCount * sizeof(GifColorType));\n    }\n\n    int res = EGifPutScreenDesc(e->gif,\n                                e->gif->SWidth,\n                                e->gif->SHeight,\n                                e->gif->SColorResolution,\n                                e->gif->SBackGroundColor,\n                                e->gif->SColorMap);\n    if (res == GIF_ERROR) {\n        return false;\n    }\n\n    return true;\n}\n\nstatic bool giflib_encoder_setup_frame(giflib_encoder e, const giflib_decoder d)\n{\n    // initialize frame with input gif's frame metadata\n    // this includes, amongst other things, inter-frame delays\n    GifImageDesc* im_in = &d->gif->Image;\n    GifImageDesc* im_out = &e->gif->Image;\n\n    // XXX we're just going to copy here, but this probably isn't right since\n    // the decoder doesn't handle interlacing correctly. might be worthwhile to\n    // just set this to false always (or enhance the decoder)\n    im_out->Interlace = im_in->Interlace;\n\n    // prepare frame local palette, if any\n    e->frame_color_map = NULL;\n    if (im_in->ColorMap) {\n        e->frame_color_map = giflib_encoder_allocate_color_maps(e, 1);\n        memmove(e->frame_color_map, im_in->ColorMap, sizeof(ColorMapObject));\n        // copy all of the RGB color values from input frame palette to output frame palette\n        e->frame_color_map->Colors =\n          giflib_encoder_allocate_colors(e, e->frame_color_map->ColorCount);\n        memmove(e->frame_color_map->Colors,\n                im_in->ColorMap->Colors,\n                e->frame_color_map->ColorCount * sizeof(GifColorType));\n    }\n\n    // copy extension blocks specific to this frame\n    // this sets up the frame delay as well as which palette entry is transparent, if any\n    e->gif->ExtensionBlockCount = d->gif->ExtensionBlockCount;\n    e->gif->ExtensionBlocks = NULL;\n    if (e->gif->ExtensionBlockCount > 0) {\n        // TODO here and in global extension blocks, we should filter out worthless blocks\n        // we're only really interested in ExtensionBlock.Function = GRAPHICS_EXT_FUNC_CODE\n        // other values like COMMENT_ and PLAINTEXT_ are not essential to viewing the image\n        e->gif->ExtensionBlocks =\n          giflib_encoder_allocate_extension_blocks(e, e->gif->ExtensionBlockCount);\n        for (int i = 0; i < e->gif->ExtensionBlockCount; i++) {\n            ExtensionBlock* eb_in = &(d->gif->ExtensionBlocks[i]);\n            ExtensionBlock* eb_out = &(e->gif->ExtensionBlocks[i]);\n            eb_out->ByteCount = eb_in->ByteCount;\n            eb_out->Function = eb_in->Function;\n            eb_out->Bytes = giflib_encoder_allocate_gif_bytes(e, eb_out->ByteCount);\n            memmove(eb_out->Bytes, eb_in->Bytes, eb_out->ByteCount);\n        }\n    }\n\n    return true;\n}\n\n// TODO this probably should be the euclidean distance\n// the manhattan distance will still be \"good enough\"\n// euclidean requires calculating pow(2) and sqrt()?\nstatic inline int rgb_distance(int r0, int g0, int b0, int r1, int g1, int b1)\n{\n    int dist = 0;\n    dist += (r0 > r1) ? r0 - r1 : r1 - r0;\n    dist += (g0 > g1) ? g0 - g1 : g1 - g0;\n    dist += (b0 > b1) ? b0 - b1 : b1 - b0;\n    return dist;\n}\n\nstatic bool giflib_encoder_render_frame(giflib_encoder e,\n                                        const giflib_decoder d,\n                                        const opencv_mat opaque_frame)\n{\n    GifFileType* gif_out = e->gif;\n    auto frame = static_cast<const cv::Mat*>(opaque_frame);\n\n    // basic bounds checking - would this frame be wider than the global gif width?\n    // if we do partial frames, we'll need to change this to account for top/left\n    if (frame->cols > gif_out->SWidth) {\n        fprintf(stderr, \"encountered error, gif frame wider than gif global width\\n\");\n        return false;\n    }\n\n    if (frame->rows > gif_out->SHeight) {\n        fprintf(stderr, \"encountered error, gif frame taller than gif global height\\n\");\n        return false;\n    }\n\n    GifImageDesc* im_out = &gif_out->Image;\n    // TODO some day consider making partial frames/make these not 0\n    im_out->Left = 0;\n    im_out->Top = 0;\n    im_out->Width = frame->cols;\n    im_out->Height = frame->rows;\n\n    int image_size = im_out->Width * im_out->Height;\n\n    if (image_size > e->pixel_len) {\n        // only realloc if we need to size up\n        e->pixel_len = image_size;\n        e->pixels = (GifByteType*)(realloc(e->pixels, e->pixel_len * sizeof(GifPixelType)));\n    }\n\n    ColorMapObject* global_color_map = e->gif->SColorMap;\n    ColorMapObject* frame_color_map = e->frame_color_map;\n    ColorMapObject* color_map = frame_color_map ? frame_color_map : global_color_map;\n\n    if (!color_map) {\n        fprintf(stderr, \"encountered error, gif frame has no color map\\n\");\n        return false;\n    }\n\n    // prepare our palette lookup table. if we used the same (byte-equal) palette table last\n    // frame, we can just reuse it this frame. otherwise we need to clear the lookup out\n    bool clear_palette_lookup = true;\n    // on the first frame, we will always clear\n    if (e->have_written_first_frame) {\n        ColorMapObject* last_color_map = e->prev_frame_color_map;\n        if (last_color_map && last_color_map->ColorCount == color_map->ColorCount) {\n            int cmp = memcmp(last_color_map->Colors,\n                             color_map->Colors,\n                             color_map->ColorCount * sizeof(GifColorType));\n            clear_palette_lookup = (cmp != 0);\n        }\n    }\n\n    if (clear_palette_lookup) {\n        memset(e->palette_lookup, 0, (1 << 15) * sizeof(encoder_palette_lookup));\n    }\n\n    GraphicsControlBlock gcb;\n    giflib_get_frame_gcb(e->gif, &gcb);\n    int transparency_index = gcb.TransparentColor;\n    bool have_transparency = (transparency_index != NO_TRANSPARENT_COLOR);\n\n    // decide whether we can use transparency against the previous frame\n    bool prev_frame_valid = e->have_written_first_frame &&\n      (e->prev_frame_disposal == DISPOSAL_UNSPECIFIED || e->prev_frame_disposal == DISPOSE_DO_NOT);\n\n    // convenience names for these dimensions\n    int frame_left = im_out->Left;\n    int frame_top = im_out->Top;\n    int frame_width = im_out->Width;\n    int frame_height = im_out->Height;\n\n    GifByteType* raster_out = e->pixels;\n\n    int raster_index = 0;\n    for (int y = frame_top; y < frame_top + frame_height; y++) {\n        uint8_t* src = frame->data + y * frame->step + (frame_left * 4);\n        for (int x = frame_left; x < frame_left + frame_width; x++) {\n            uint32_t B = *src++;\n            uint32_t G = *src++;\n            uint32_t R = *src++;\n            uint32_t A = *src++;\n\n            // TODO come up with what this threshold value should be\n            // probably ought to be a lot smaller, but greater than 0\n            // for now we just pick halfway\n            if (A < 128 && have_transparency) {\n                // this composite frame pixel is actually transparent\n                // what this means is that the background color must be transparent\n                // AND this frame pixel must be transparent\n                // for now we'll just assume bg is transparent since otherwise decoder\n                // could not have generated this frame pixel with a low opacity\n                *raster_out++ = transparency_index;\n                continue;\n            }\n\n            uint32_t crushed = ((R >> 3) << 10) | ((G >> 3) << 5) | ((B >> 3));\n            int least_dist = INT_MAX;\n            int best_color = 0;\n            if (!(e->palette_lookup[crushed].present)) {\n\n                    bool is_extreme_color = (R > 240 && G > 240 && B > 240) || (R < 15 && G < 15 && B < 15);\n\n                    // calculate the best palette entry based on the midpoint of the crushed colors.\n                    // what this means is that we drop the crushed bits (& 0xf8)\n                    // and then OR the highest-order crushed bit back in, which is approx midpoint.\n                    // for extreme colors, use actual values\n                    uint32_t R_compare = is_extreme_color ? R : (R & 0xf8) | 4;\n                    uint32_t G_compare = is_extreme_color ? G : (G & 0xf8) | 4;\n                    uint32_t B_compare = is_extreme_color ? B : (B & 0xf8) | 4;\n\n                    // we're calculating the best, so keep track of which\n                    // palette entry has least distance\n                    int count = color_map->ColorCount;\n                    for (int i = 0; i < count; i++) {\n                      if (i == transparency_index) {\n                        // this index doesn't point to an actual color\n                        continue;\n                    }\n                    int dist = rgb_distance(R_compare,\n                                            G_compare,\n                                            B_compare,\n                                            color_map->Colors[i].Red,\n                                            color_map->Colors[i].Green,\n                                            color_map->Colors[i].Blue);\n                    if (dist < least_dist) {\n                        least_dist = dist;\n                        best_color = i;\n                    }\n                }\n                e->palette_lookup[crushed].present = 1;\n                e->palette_lookup[crushed].index = best_color;\n            }\n            else {\n                best_color = e->palette_lookup[crushed].index;\n                least_dist = rgb_distance(R,\n                                          G,\n                                          B,\n                                          color_map->Colors[best_color].Red,\n                                          color_map->Colors[best_color].Green,\n                                          color_map->Colors[best_color].Blue);\n            }\n\n            // now that we for sure know which palette entry to pick, we have one more test\n            // to perform. it's possible that the best color for this pixel is actually\n            // the color of this pixel in the previous frame. if that's true, we'll just\n            // choose the transparency color, which will compress better on average\n            // (plus it improves color range of image)\n            if (prev_frame_valid && have_transparency) {\n                ptrdiff_t frame_index = 4 * ((y * e->gif->SWidth) + x);\n                uint32_t last_B = e->prev_frame_bgra[frame_index];\n                uint32_t last_G = e->prev_frame_bgra[frame_index + 1];\n                uint32_t last_R = e->prev_frame_bgra[frame_index + 2];\n                int dist = rgb_distance(R, G, B, last_R, last_G, last_B);\n                if (dist < least_dist) {\n                    least_dist = dist;\n                    best_color = transparency_index;\n                }\n            }\n\n            *raster_out++ = best_color;\n        }\n    }\n\n    // XXX change this if we do partial frames (only copy over some)\n    memcpy(e->prev_frame_bgra, frame->data, 4 * e->gif->SWidth * e->gif->SHeight);\n\n    e->prev_frame_color_map = color_map;\n    e->prev_frame_disposal = gcb.DisposalMode;\n\n    return true;\n}\n\nstatic int giflib_encoder_write_extensions(giflib_encoder e)\n{\n    if (e->gif->ExtensionBlocks) {\n        ExtensionBlock* ep;\n\n        for (int i = 0; i < e->gif->ExtensionBlockCount; i++) {\n            ep = &e->gif->ExtensionBlocks[i];\n            if (ep->Function != CONTINUE_EXT_FUNC_CODE) {\n                if (EGifPutExtensionLeader(e->gif, ep->Function) == GIF_ERROR) {\n                    return false;\n                }\n            }\n            if (EGifPutExtensionBlock(e->gif, ep->ByteCount, ep->Bytes) == GIF_ERROR) {\n                return false;\n            }\n            if (i == e->gif->ExtensionBlockCount - 1 ||\n                (ep + 1)->Function != CONTINUE_EXT_FUNC_CODE) {\n                if (EGifPutExtensionTrailer(e->gif) == GIF_ERROR) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n\nbool giflib_encoder_encode_frame(giflib_encoder e,\n                                 const giflib_decoder d,\n                                 const opencv_mat opaque_frame)\n{\n    giflib_encoder_setup_frame(e, d);\n    giflib_encoder_render_frame(e, d, opaque_frame);\n\n    GifImageDesc* im_out = &e->gif->Image;\n    int frame_height = im_out->Height;\n    int frame_width = im_out->Width;\n\n    int res = giflib_encoder_write_extensions(e);\n    if (res == GIF_ERROR) {\n        return false;\n    }\n\n    res = EGifPutImageDesc(e->gif,\n                           im_out->Left,\n                           im_out->Top,\n                           im_out->Width,\n                           im_out->Height,\n                           im_out->Interlace,\n                           e->frame_color_map);\n    if (res == GIF_ERROR) {\n        return false;\n    }\n\n    if (im_out->Interlace) {\n        /* Need to perform 4 passes on the images: */\n        for (int i = 0; i < 4; i++) {\n            for (int j = interlace_offset[i]; j < frame_height; j += interlace_jumps[i]) {\n                res = EGifPutLine(e->gif, e->pixels + j * frame_width, frame_width);\n                if (res == GIF_ERROR) {\n                    fprintf(stderr, \"encountered error, could not serialize gif line\\n\");\n                    return false;\n                }\n            }\n        }\n    }\n    else {\n        for (int i = 0; i < frame_height; i++) {\n            res = EGifPutLine(e->gif, e->pixels + i * frame_width, frame_width);\n            if (res == GIF_ERROR) {\n                return false;\n            }\n        }\n    }\n\n    e->have_written_first_frame = true;\n\n    return true;\n}\n\nbool giflib_encoder_flush(giflib_encoder e, const giflib_decoder d)\n{\n    // XXX we need to pull these trailing blocks on d\n    // does decoder's state machine allow that?\n\n    // set up \"trailing\" extension blocks, which appear after all the frames\n    // brian note: what do these do? do we actually need them?\n    e->gif->ExtensionBlockCount = d->gif->ExtensionBlockCount;\n    e->gif->ExtensionBlocks = NULL;\n    if (e->gif->ExtensionBlockCount > 0) {\n        e->gif->ExtensionBlocks =\n          giflib_encoder_allocate_extension_blocks(e, e->gif->ExtensionBlockCount);\n        for (int i = 0; i < e->gif->ExtensionBlockCount; i++) {\n            ExtensionBlock* eb = &(e->gif->ExtensionBlocks[i]);\n            eb->ByteCount = d->gif->ExtensionBlocks[i].ByteCount;\n            eb->Function = d->gif->ExtensionBlocks[i].Function;\n            eb->Bytes = giflib_encoder_allocate_gif_bytes(e, eb->ByteCount);\n            memmove(eb->Bytes, d->gif->ExtensionBlocks[i].Bytes, eb->ByteCount);\n        }\n    }\n\n    int res = giflib_encoder_write_extensions(e);\n    if (res == GIF_ERROR) {\n        return false;\n    }\n\n    if (EGifCloseFile(e->gif, NULL) == GIF_ERROR) {\n        return false;\n    }\n\n    e->gif = NULL;\n\n    return true;\n}\n\nvoid giflib_encoder_release(giflib_encoder e)\n{\n    // don't free dst -- we're borrowing it\n\n    if (e->prev_frame_bgra) {\n        free(e->prev_frame_bgra);\n    }\n\n    if (e->palette_lookup) {\n        free(e->palette_lookup);\n    }\n\n    if (e->pixels) {\n        free(e->pixels);\n    }\n\n    for (std::vector<ExtensionBlock*>::iterator it = e->extension_blocks.begin();\n         it != e->extension_blocks.end();\n         ++it) {\n        free(*it);\n    }\n    e->extension_blocks.clear();\n\n    for (std::vector<GifByteType*>::iterator it = e->gif_bytes.begin(); it != e->gif_bytes.end();\n         ++it) {\n        free(*it);\n    }\n    e->gif_bytes.clear();\n\n    ColorMapObject* gif_scolor = NULL;\n    ColorMapObject* gif_last_color = NULL;\n    if (e->gif) {\n        gif_scolor = e->gif->SColorMap;\n        gif_last_color = e->gif->Image.ColorMap;\n    }\n    for (std::vector<ColorMapObject*>::iterator it = e->color_maps.begin();\n         it != e->color_maps.end();\n         ++it) {\n        if (gif_scolor && gif_scolor == *it) {\n            // this is extremely unlikely to happen, but giflib transitions from\n            // borrowing this ptr to owning it, and it's possible that it will try\n            // to free ours in certain circumstances\n            // so swap its ptr if it matches one we own\n            e->gif->SColorMap = NULL;\n        }\n        if (gif_last_color && gif_last_color == *it) {\n            // this seemingly will never happen, but given how strange last case is,\n            // check for it anyway\n            e->gif->Image.ColorMap = NULL;\n        }\n        free(*it);\n    }\n    e->color_maps.clear();\n\n    for (std::vector<GifColorType*>::iterator it = e->colors.begin(); it != e->colors.end(); ++it) {\n        free(*it);\n    }\n    e->colors.clear();\n\n    for (std::vector<SavedImage*>::iterator it = e->saved_images.begin();\n         it != e->saved_images.end();\n         ++it) {\n        free(*it);\n    }\n    e->saved_images.clear();\n\n    if (e->gif) {\n        // we most likely won't actually call this since Spew() does it\n        // but in exceptional cases we'll need it for cleanup\n        int error_code = 0;\n        EGifCloseFile(e->gif, &error_code);\n        if (error_code) {\n            fprintf(stderr, \"encountered error closing gif, %d\\n\", error_code);\n        }\n    }\n\n    delete e;\n}\n\nint giflib_encoder_get_output_length(giflib_encoder e)\n{\n    return e->dst_offset;\n}\n\nstruct GifAnimationInfo giflib_decoder_get_animation_info(const giflib_decoder d) {\n    // Default to 1 loop (play once) if no NETSCAPE2.0 extension is found\n    GifAnimationInfo info = {1, 0, 255, 255, 255, 0, 0};  // loop_count, frame_count, bg_r, bg_g, bg_b, bg_a, duration_ms\n\n    // Create a temporary decoder to read extension blocks\n    giflib_decoder loopReader = new struct giflib_decoder_struct();\n    if (!loopReader) {\n        return info; // Return default on allocation failure\n    }\n\n    memset(loopReader, 0, sizeof(struct giflib_decoder_struct));\n    loopReader->mat = d->mat;  // Share the source data\n\n    int error = 0;\n    GifFileType* gif = DGifOpen(loopReader, decode_func, &error);\n    if (error) {\n        delete loopReader;\n        return info;\n    }\n\n    bool found_loop_count = false;\n    bool found_gcb = false;\n    GraphicsControlBlock gcb = {};\n    GifRecordType recordType;\n    \n    // Read all blocks until we hit end\n    while (DGifGetRecordType(gif, &recordType) == GIF_OK) {\n        switch (recordType) {\n            case EXTENSION_RECORD_TYPE: {\n                GifByteType* ExtData;\n                int ExtFunction;\n                \n                if (DGifGetExtension(gif, &ExtFunction, &ExtData) == GIF_OK && ExtData != NULL) {\n                    // Check for GraphicsControlBlock to get frame delay\n                    if (ExtFunction == GRAPHICS_EXT_FUNC_CODE) {\n                        GraphicsControlBlock frame_gcb;\n                        DGifExtensionToGCB(ExtData[0], &ExtData[1], &frame_gcb);\n\n                        // Add frame delay with 20ms minimum for multi-frame GIFs\n                        int frame_delay_ms = (info.frame_count > 0 && frame_gcb.DelayTime < 2) ?\n                            20 : frame_gcb.DelayTime * 10;\n                        info.duration_ms += frame_delay_ms;\n\n                        // If this is first GCB, handle background color\n                        if (!found_gcb) {\n                            found_gcb = true;\n                            gcb = frame_gcb;\n                            uint8_t bg_red, bg_green, bg_blue, bg_alpha;\n                            extract_background_color(gif, &gcb, &bg_red, &bg_green,\n                                                  &bg_blue, &bg_alpha);\n                            info.bg_red = bg_red;\n                            info.bg_green = bg_green;\n                            info.bg_blue = bg_blue;\n                            info.bg_alpha = bg_alpha;\n                        }\n                    }\n                    // Look for NETSCAPE2.0 extension\n                    else if (!found_loop_count && \n                        ExtFunction == APPLICATION_EXT_FUNC_CODE && \n                        ExtData[0] >= 11 &&\n                        memcmp(ExtData + 1, \"NETSCAPE2.0\", 11) == 0) {\n                        if (DGifGetExtensionNext(gif, &ExtData) == GIF_OK && \n                            ExtData != NULL && \n                            ExtData[0] >= 3 && \n                            ExtData[1] == 1) {\n                            info.loop_count = ExtData[2] | (ExtData[3] << 8);\n                            found_loop_count = true;\n                        }\n                    }\n                    \n                    // Skip any remaining extension blocks\n                    while (ExtData != NULL) {\n                        if (DGifGetExtensionNext(gif, &ExtData) != GIF_OK) {\n                            goto cleanup;\n                        }\n                    }\n                }\n                break;\n            }\n            \n            case IMAGE_DESC_RECORD_TYPE:\n                // Count frame and skip image data\n                info.frame_count++;\n                if (DGifGetImageDesc(gif) != GIF_OK) {\n                    goto cleanup;\n                }\n                // Skip the image data\n                {\n                    GifByteType* CodeBlock;\n                    if (DGifGetCode(gif, &error, &CodeBlock) == GIF_ERROR) {\n                        goto cleanup;\n                    }\n                    while (CodeBlock != NULL) {\n                        if (DGifGetCodeNext(gif, &CodeBlock) == GIF_ERROR) {\n                            goto cleanup;\n                        }\n                    }\n                }\n                break;\n                \n            case TERMINATE_RECORD_TYPE:\n                goto cleanup;\n                \n            default:\n                break;\n        }\n    }\n\n    // If we never found a GCB, still need to set background color\n    if (!found_gcb) {\n        uint8_t bg_red, bg_green, bg_blue, bg_alpha;\n        extract_background_color(gif, &gcb, &bg_red, &bg_green,\n                               &bg_blue, &bg_alpha);\n\n        // convert to int to handle uint limitations in rust FFI\n        info.bg_red = bg_red;\n        info.bg_green = bg_green;\n        info.bg_blue = bg_blue;\n        info.bg_alpha = bg_alpha;\n    }\n\ncleanup:\n    DGifCloseFile(gif, &error);\n    delete loopReader;\n    return info;\n}\n"
        },
        {
          "name": "giflib.go",
          "type": "blob",
          "size": 7.802734375,
          "content": "package lilliput\n\n// #include \"giflib.hpp\"\nimport \"C\"\n\nimport (\n\t\"errors\"\n\t\"io\"\n\t\"sync/atomic\"\n\t\"time\"\n\t\"unsafe\"\n)\n\n// gifDecoder implements image decoding for GIF format\ntype gifDecoder struct {\n\tdecoder           C.giflib_decoder\n\tmat               C.opencv_mat\n\tbuf               []byte\n\tframeIndex        int\n\tloopCount         int\n\tframeCount        int\n\tanimationInfoRead bool\n\tbgRed             uint8\n\tbgGreen           uint8\n\tbgBlue            uint8\n\tbgAlpha           uint8\n\tdurationMs        int\n}\n\n// gifEncoder implements image encoding for GIF format\ntype gifEncoder struct {\n\tencoder    C.giflib_encoder\n\tdecoder    C.giflib_decoder\n\tbuf        []byte\n\tframeIndex int\n\thasFlushed bool\n}\n\nconst defaultMaxFrameDimension = 10000\n\nvar (\n\tgifMaxFrameDimension uint64\n\n\tErrGifEncoderNeedsDecoder = errors.New(\"GIF encoder needs decoder used to create image\")\n)\n\n// SetGIFMaxFrameDimension sets the largest GIF width/height that can be decoded.\n// This helps prevent loading extremely large GIF images that could exhaust memory.\nfunc SetGIFMaxFrameDimension(dim uint64) {\n\t// TODO we should investigate if this can be removed/become a mat check in decoder\n\tatomic.StoreUint64(&gifMaxFrameDimension, dim)\n}\n\n// newGifDecoder creates a new GIF decoder from the provided byte buffer.\n// Returns an error if the buffer is too small or contains invalid GIF data.\nfunc newGifDecoder(buf []byte) (*gifDecoder, error) {\n\tmat := C.opencv_mat_create_from_data(C.int(len(buf)), 1, C.CV_8U, unsafe.Pointer(&buf[0]), C.size_t(len(buf)))\n\n\tif mat == nil {\n\t\treturn nil, ErrBufTooSmall\n\t}\n\n\tdecoder := C.giflib_decoder_create(mat)\n\tif decoder == nil {\n\t\treturn nil, ErrInvalidImage\n\t}\n\n\treturn &gifDecoder{\n\t\tdecoder:    decoder,\n\t\tmat:        mat,\n\t\tbuf:        buf,\n\t\tframeIndex: 0,\n\t}, nil\n}\n\n// Header returns the image header information including dimensions and pixel format.\nfunc (d *gifDecoder) Header() (*ImageHeader, error) {\n\treturn &ImageHeader{\n\t\twidth:         int(C.giflib_decoder_get_width(d.decoder)),\n\t\theight:        int(C.giflib_decoder_get_height(d.decoder)),\n\t\tpixelType:     PixelType(C.CV_8UC4),\n\t\torientation:   OrientationTopLeft,\n\t\tnumFrames:     d.FrameCount(),\n\t\tcontentLength: len(d.buf),\n\t}, nil\n}\n\n// FrameHeader returns the current frame's header information.\nfunc (d *gifDecoder) FrameHeader() (*ImageHeader, error) {\n\treturn &ImageHeader{\n\t\twidth:         int(C.giflib_decoder_get_frame_width(d.decoder)),\n\t\theight:        int(C.giflib_decoder_get_frame_height(d.decoder)),\n\t\tpixelType:     PixelType(C.CV_8UC4),\n\t\torientation:   OrientationTopLeft,\n\t\tnumFrames:     1,\n\t\tcontentLength: len(d.buf),\n\t}, nil\n}\n\n// Close releases resources associated with the decoder.\nfunc (d *gifDecoder) Close() {\n\tC.giflib_decoder_release(d.decoder)\n\tC.opencv_mat_release(d.mat)\n\td.buf = nil\n}\n\n// Description returns the image format description (\"GIF\").\nfunc (d *gifDecoder) Description() string {\n\treturn \"GIF\"\n}\n\n// IsStreamable returns whether the format supports streaming decoding.\nfunc (d *gifDecoder) IsStreamable() bool {\n\treturn true\n}\n\n// HasSubtitles returns whether the format supports subtitles.\nfunc (d *gifDecoder) HasSubtitles() bool {\n\treturn false\n}\n\n// ICC returns the ICC color profile data, if any.\nfunc (d *gifDecoder) ICC() []byte {\n\treturn []byte{}\n}\n\n// Duration returns the total duration of the GIF animation.\nfunc (d *gifDecoder) Duration() time.Duration {\n\td.readAnimationInfo()\n\treturn time.Duration(d.durationMs) * time.Millisecond\n}\n\n// BackgroundColor returns the GIF background color as a 32-bit RGBA value.\nfunc (d *gifDecoder) BackgroundColor() uint32 {\n\td.readAnimationInfo()\n\treturn uint32(d.bgRed)<<16 | uint32(d.bgGreen)<<8 | uint32(d.bgBlue) | uint32(d.bgAlpha)<<24\n}\n\n// readAnimationInfo reads and caches GIF animation metadata like loop count and frame count.\n// This is done lazily since reading extension blocks is relatively expensive.\nfunc (d *gifDecoder) readAnimationInfo() {\n\tif !d.animationInfoRead {\n\t\tinfo := C.giflib_decoder_get_animation_info(d.decoder)\n\t\td.loopCount = int(info.loop_count)\n\t\td.frameCount = int(info.frame_count)\n\t\td.animationInfoRead = true\n\t\td.bgRed = uint8(info.bg_red)\n\t\td.bgGreen = uint8(info.bg_green)\n\t\td.bgBlue = uint8(info.bg_blue)\n\t\td.bgAlpha = uint8(info.bg_alpha)\n\t\td.durationMs = int(info.duration_ms)\n\t}\n}\n\n// LoopCount returns the number of times the GIF animation should loop.\n// A value of 0 means loop forever.\nfunc (d *gifDecoder) LoopCount() int {\n\td.readAnimationInfo()\n\treturn d.loopCount\n}\n\n// FrameCount returns the total number of frames in the GIF.\nfunc (d *gifDecoder) FrameCount() int {\n\td.readAnimationInfo()\n\treturn d.frameCount\n}\n\n// DecodeTo decodes the next GIF frame into the provided Framebuffer.\n// Returns io.EOF when all frames have been decoded.\nfunc (d *gifDecoder) DecodeTo(f *Framebuffer) error {\n\th, err := d.Header()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = f.resizeMat(h.Width(), h.Height(), h.PixelType())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tnextFrameResult := int(C.giflib_decoder_decode_frame_header(d.decoder))\n\tif nextFrameResult == C.giflib_decoder_eof {\n\t\treturn io.EOF\n\t}\n\tif nextFrameResult == C.giflib_decoder_error {\n\t\treturn ErrInvalidImage\n\t}\n\n\tframeHeader, err := d.FrameHeader()\n\tif err != nil {\n\t\treturn ErrInvalidImage\n\t}\n\tmaxDim := int(atomic.LoadUint64(&gifMaxFrameDimension))\n\tif frameHeader.Width() > maxDim || frameHeader.Height() > maxDim {\n\t\treturn ErrInvalidImage\n\t}\n\n\tret := C.giflib_decoder_decode_frame(d.decoder, f.mat)\n\tif !ret {\n\t\treturn ErrDecodingFailed\n\t}\n\tf.duration = time.Duration(C.giflib_decoder_get_prev_frame_delay(d.decoder)) * 10 * time.Millisecond\n\tf.blend = NoBlend\n\tf.dispose = DisposeMethod(C.giflib_decoder_get_prev_frame_disposal(d.decoder))\n\tf.xOffset = 0\n\tf.yOffset = 0\n\td.frameIndex++\n\treturn nil\n}\n\n// SkipFrame skips decoding of the next frame.\n// Returns io.EOF when all frames have been skipped.\nfunc (d *gifDecoder) SkipFrame() error {\n\tnextFrameResult := int(C.giflib_decoder_skip_frame(d.decoder))\n\n\tif nextFrameResult == C.giflib_decoder_eof {\n\t\treturn io.EOF\n\t}\n\tif nextFrameResult == C.giflib_decoder_error {\n\t\treturn ErrInvalidImage\n\t}\n\n\treturn nil\n}\n\n// newGifEncoder creates a new GIF encoder that will write to the provided buffer.\n// Requires the original decoder that was used to decode the source GIF.\nfunc newGifEncoder(decodedBy Decoder, buf []byte) (*gifEncoder, error) {\n\t// we must have a decoder since we can't build our own palettes\n\t// so if we don't get a gif decoder, bail out\n\tif decodedBy == nil {\n\t\treturn nil, ErrGifEncoderNeedsDecoder\n\t}\n\n\tgifDecoder, ok := decodedBy.(*gifDecoder)\n\tif !ok {\n\t\treturn nil, ErrGifEncoderNeedsDecoder\n\t}\n\n\tbuf = buf[:1]\n\tenc := C.giflib_encoder_create(unsafe.Pointer(&buf[0]), C.size_t(cap(buf)))\n\tif enc == nil {\n\t\treturn nil, ErrBufTooSmall\n\t}\n\n\treturn &gifEncoder{\n\t\tencoder:    enc,\n\t\tdecoder:    gifDecoder.decoder,\n\t\tbuf:        buf,\n\t\tframeIndex: 0,\n\t}, nil\n}\n\n// Encode encodes a frame into the GIF. If f is nil, flushes any remaining data\n// and returns the complete encoded GIF. Returns io.EOF after flushing.\nfunc (e *gifEncoder) Encode(f *Framebuffer, opt map[int]int) ([]byte, error) {\n\tif e.hasFlushed {\n\t\treturn nil, io.EOF\n\t}\n\n\tif f == nil {\n\t\tret := C.giflib_encoder_flush(e.encoder, e.decoder)\n\t\tif !ret {\n\t\t\treturn nil, ErrInvalidImage\n\t\t}\n\t\te.hasFlushed = true\n\n\t\tlen := C.int(C.giflib_encoder_get_output_length(e.encoder))\n\n\t\treturn e.buf[:len], nil\n\t}\n\n\tif e.frameIndex == 0 {\n\t\t// first run setup\n\t\t// TODO figure out actual gif width/height?\n\t\tC.giflib_encoder_init(e.encoder, e.decoder, C.int(f.Width()), C.int(f.Height()))\n\t}\n\n\tif !C.giflib_encoder_encode_frame(e.encoder, e.decoder, f.mat) {\n\t\treturn nil, ErrInvalidImage\n\t}\n\n\te.frameIndex++\n\n\treturn nil, nil\n}\n\n// Close releases resources associated with the encoder.\nfunc (e *gifEncoder) Close() {\n\tC.giflib_encoder_release(e.encoder)\n}\n\n// init initializes the GIF decoder with default settings\nfunc init() {\n\tSetGIFMaxFrameDimension(defaultMaxFrameDimension)\n}\n"
        },
        {
          "name": "giflib.hpp",
          "type": "blob",
          "size": 1.8369140625,
          "content": "#ifndef LILLIPUT_GIFLIB_HPP\n#define LILLIPUT_GIFLIB_HPP\n\n#include \"opencv.hpp\"\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\nstruct GifAnimationInfo {\n    int loop_count;\n    int frame_count;\n    int bg_red;\n    int bg_green;\n    int bg_blue;\n    int bg_alpha;\n    int duration_ms;\n};\n\n#define GIF_DISPOSE_NONE 0\n#define GIF_DISPOSE_BACKGROUND 1\n\ntypedef struct giflib_decoder_struct* giflib_decoder;\ntypedef struct giflib_encoder_struct* giflib_encoder;\n\ntypedef enum {\n    giflib_decoder_have_next_frame,\n    giflib_decoder_eof,\n    giflib_decoder_error,\n} giflib_decoder_frame_state;\n\ngiflib_decoder giflib_decoder_create(const opencv_mat buf);\nint giflib_decoder_get_width(const giflib_decoder d);\nint giflib_decoder_get_height(const giflib_decoder d);\nint giflib_decoder_get_num_frames(const giflib_decoder d);\nint giflib_decoder_get_frame_width(const giflib_decoder d);\nint giflib_decoder_get_frame_height(const giflib_decoder d);\nint giflib_decoder_get_prev_frame_delay(const giflib_decoder d);\nvoid giflib_decoder_release(giflib_decoder d);\ngiflib_decoder_frame_state giflib_decoder_decode_frame_header(giflib_decoder d);\nbool giflib_decoder_decode_frame(giflib_decoder d, opencv_mat mat);\ngiflib_decoder_frame_state giflib_decoder_skip_frame(giflib_decoder d);\n\ngiflib_encoder giflib_encoder_create(void* buf, size_t buf_len);\nbool giflib_encoder_init(giflib_encoder e, const giflib_decoder d, int width, int height);\nbool giflib_encoder_encode_frame(giflib_encoder e, const giflib_decoder d, const opencv_mat frame);\nbool giflib_encoder_flush(giflib_encoder e, const giflib_decoder d);\nvoid giflib_encoder_release(giflib_encoder e);\nint giflib_encoder_get_output_length(giflib_encoder e);\nstruct GifAnimationInfo giflib_decoder_get_animation_info(const giflib_decoder d);\nint giflib_decoder_get_prev_frame_disposal(const giflib_decoder d);\n#ifdef __cplusplus\n}\n#endif\n\n#endif\n"
        },
        {
          "name": "giflib_test.go",
          "type": "blob",
          "size": 3.103515625,
          "content": "package lilliput\n\nimport (\n\t\"io\"\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n)\n\nfunc TestGIFOperations(t *testing.T) {\n\tt.Run(\"GIFDuration\", testGIFDuration)\n}\n\nfunc testGIFDuration(t *testing.T) {\n\ttestCases := []struct {\n\t\tname          string\n\t\tfilename      string\n\t\twantLoopCount int\n\t\twantFrames    int\n\t\twantDuration  time.Duration\n\t\tdescription   string\n\t}{\n\t\t{\n\t\t\tname:          \"Standard animated GIF\",\n\t\t\tfilename:      \"testdata/party-discord.gif\",\n\t\t\twantLoopCount: 0, // infinite loop\n\t\t\twantFrames:    16,\n\t\t\twantDuration:  time.Millisecond * 480,\n\t\t\tdescription:   \"Basic animation with custom delays\",\n\t\t},\n\t\t{\n\t\t\tname:          \"Static GIF image\",\n\t\t\tfilename:      \"testdata/ferry_sunset.gif\",\n\t\t\twantLoopCount: 1, // play once\n\t\t\twantFrames:    1,\n\t\t\twantDuration:  0,\n\t\t\tdescription:   \"Static image, no animation\",\n\t\t},\n\t\t{\n\t\t\tname:          \"Single loop GIF\",\n\t\t\tfilename:      \"testdata/no-loop.gif\",\n\t\t\twantLoopCount: 1, // play once\n\t\t\twantFrames:    44,\n\t\t\twantDuration:  time.Millisecond * 4400,\n\t\t\tdescription:   \"Animation that plays only once\",\n\t\t},\n\t\t{\n\t\t\tname:          \"Duplicate loop count GIF\",\n\t\t\tfilename:      \"testdata/duplicate_number_of_loops.gif\",\n\t\t\twantLoopCount: 2, // play twice\n\t\t\twantFrames:    2,\n\t\t\twantDuration:  0, // unable to determine duration\n\t\t\tdescription:   \"Animation with duplicate NETSCAPE2.0 extension blocks\",\n\t\t},\n\t\t{\n\t\t\tname:          \"Background dispose GIF\",\n\t\t\tfilename:      \"testdata/dispose_bgnd.gif\",\n\t\t\twantLoopCount: 0, // infinite loop\n\t\t\twantFrames:    5,\n\t\t\twantDuration:  time.Second * 5,\n\t\t\tdescription:   \"Animation with background disposal method\",\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\ttestGIFImage, err := os.ReadFile(tc.filename)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to read gif image: %v\", err)\n\t\t\t}\n\n\t\t\tdecoder, err := newGifDecoder(testGIFImage)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to create decoder: %v\", err)\n\t\t\t}\n\t\t\tdefer decoder.Close()\n\n\t\t\t// Test loop count\n\t\t\tif got := decoder.LoopCount(); got != tc.wantLoopCount {\n\t\t\t\tt.Errorf(\"LoopCount() = %v, want %v\", got, tc.wantLoopCount)\n\t\t\t}\n\n\t\t\t// Test frame count\n\t\t\tif got := decoder.FrameCount(); got != tc.wantFrames {\n\t\t\t\tt.Errorf(\"FrameCount() = %v, want %v\", got, tc.wantFrames)\n\t\t\t}\n\n\t\t\t// Test total duration\n\t\t\tif got := decoder.Duration(); got != tc.wantDuration {\n\t\t\t\tt.Errorf(\"Duration() = %v, want %v (%s)\", got, tc.wantDuration, tc.description)\n\t\t\t}\n\n\t\t\t// Test per-frame durations\n\t\t\theader, err := decoder.Header()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to get header: %v\", err)\n\t\t\t}\n\n\t\t\tframebuffer := NewFramebuffer(header.width, header.height)\n\t\t\tdefer framebuffer.Close()\n\n\t\t\tvar totalDuration time.Duration\n\t\t\tframeCount := 0\n\t\t\tfor {\n\t\t\t\terr = decoder.DecodeTo(framebuffer)\n\t\t\t\tif err == io.EOF {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"DecodeTo failed: %v\", err)\n\t\t\t\t}\n\n\t\t\t\ttotalDuration += framebuffer.Duration()\n\t\t\t\tframeCount++\n\t\t\t}\n\n\t\t\t// Verify total duration matches sum of frame durations\n\t\t\tif totalDuration != tc.wantDuration {\n\t\t\t\tt.Errorf(\"Sum of frame durations (%v) doesn't match total duration (%v)\",\n\t\t\t\t\ttotalDuration, tc.wantDuration)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.04296875,
          "content": "module github.com/discord/lilliput\n\ngo 1.15\n"
        },
        {
          "name": "icc_profiles",
          "type": "tree",
          "content": null
        },
        {
          "name": "lilliput.go",
          "type": "blob",
          "size": 4.76953125,
          "content": "// Package lilliput resizes and encodes images from\n// compressed images\npackage lilliput\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"strings\"\n\t\"time\"\n)\n\nvar (\n\tErrInvalidImage     = errors.New(\"unrecognized image format\")\n\tErrDecodingFailed   = errors.New(\"failed to decode image\")\n\tErrBufTooSmall      = errors.New(\"buffer too small to hold image\")\n\tErrFrameBufNoPixels = errors.New(\"Framebuffer contains no pixels\")\n\tErrSkipNotSupported = errors.New(\"skip operation not supported by this decoder\")\n\tErrEncodeTimeout    = errors.New(\"encode timed out\")\n\n\tgif87Magic   = []byte(\"GIF87a\")\n\tgif89Magic   = []byte(\"GIF89a\")\n\twebpMagic    = []byte(\"RIFF\")\n\twebpFormat   = []byte(\"WEBP\")\n\tmp42Magic    = []byte(\"ftypmp42\")\n\tmp4IsomMagic = []byte(\"ftypisom\")\n\tpngMagic     = []byte{0x89, 0x50, 0x4e, 0x47, 0x0d, 0x0a, 0x1a, 0x0a}\n)\n\n// A Decoder decompresses compressed image data.\ntype Decoder interface {\n\t// Header returns basic image metadata from the image.\n\t// This is done lazily, reading only the first part of the image and not\n\t// a full decode.\n\tHeader() (*ImageHeader, error)\n\n\t// Close releases any resources associated with the Decoder\n\tClose()\n\n\t// Description returns a string description of the image type, such as\n\t// \"PNG\"\n\tDescription() string\n\n\t// Duration returns the duration of the content. This property is 0 for\n\t// static images and animated GIFs.\n\tDuration() time.Duration\n\n\t// DecodeTo fully decodes the image pixel data into f. Generally users should\n\t// prefer instead using the ImageOps object to decode images.\n\tDecodeTo(f *Framebuffer) error\n\n\t// SkipFrame skips a frame if the decoder supports multiple frames\n\t// and returns io.EOF if the last frame has been reached\n\tSkipFrame() error\n\n\t// IsStreamable indicates whether the content is optimized for streaming. This is true\n\t// for static images and animated GIFs.\n\tIsStreamable() bool\n\n\t// HasSubtitles indicates whether the content has one or more subtitle tracks.\n\tHasSubtitles() bool\n\n\t// BackgroundColor as BGRA\n\tBackgroundColor() uint32\n\n\t// ICC returns the ICC color profile, if any\n\tICC() []byte\n\n\t// LoopCount() returns the number of loops in the image\n\tLoopCount() int\n}\n\n// An Encoder compresses raw pixel data into a well-known image type.\ntype Encoder interface {\n\t// Encode encodes the pixel data in f into the dst provided to NewEncoder. Encode quality\n\t// options can be passed into opt, such as map[int]int{lilliput.JpegQuality: 80}\n\tEncode(f *Framebuffer, opt map[int]int) ([]byte, error)\n\n\t// Close releases any resources associated with the Encoder\n\tClose()\n}\n\nfunc isGIF(maybeGIF []byte) bool {\n\treturn bytes.HasPrefix(maybeGIF, gif87Magic) || bytes.HasPrefix(maybeGIF, gif89Magic)\n}\n\nfunc isWebp(maybeWebp []byte) bool {\n\tif len(maybeWebp) < 12 {\n\t\treturn false\n\t}\n\treturn bytes.HasPrefix(maybeWebp, webpMagic) && bytes.Equal(maybeWebp[8:12], webpFormat)\n}\n\nfunc isAvif(maybeAvif []byte) bool {\n\tif len(maybeAvif) < 12 {\n\t\treturn false\n\t}\n\treturn bytes.Equal(maybeAvif[4:8], []byte(\"ftyp\")) && (bytes.Equal(maybeAvif[8:12], []byte(\"avif\")) || bytes.Equal(maybeAvif[8:12], []byte(\"avis\")))\n}\n\nfunc isMP4(maybeMP4 []byte) bool {\n\tif len(maybeMP4) < 12 {\n\t\treturn false\n\t}\n\n\tmagic := maybeMP4[4:]\n\treturn bytes.HasPrefix(magic, mp42Magic) || bytes.HasPrefix(magic, mp4IsomMagic)\n}\n\n// NewDecoder returns a Decoder which can be used to decode\n// image data provided in buf. If the first few bytes of buf do not\n// point to a valid magic string, an error will be returned.\nfunc NewDecoder(buf []byte) (Decoder, error) {\n\t// Check buffer length before accessing it\n\tif len(buf) == 0 {\n\t\treturn nil, ErrInvalidImage\n\t}\n\n\tisBufGIF := isGIF(buf)\n\tif isBufGIF {\n\t\treturn newGifDecoder(buf)\n\t}\n\n\tisBufWebp := isWebp(buf)\n\tif isBufWebp {\n\t\treturn newWebpDecoder(buf)\n\t}\n\n\tisBufAvif := isAvif(buf)\n\tif isBufAvif {\n\t\treturn newAvifDecoder(buf)\n\t}\n\n\tmaybeOpenCVDecoder, err := newOpenCVDecoder(buf)\n\tif err == nil {\n\t\treturn maybeOpenCVDecoder, nil\n\t}\n\n\t// Try AVCodec decoder as a fallback\n\treturn newAVCodecDecoder(buf)\n}\n\n// NewEncoder returns an Encode which can be used to encode Framebuffer\n// into compressed image data. ext should be a string like \".jpeg\" or\n// \".png\". decodedBy is optional and can be the Decoder used to make\n// the Framebuffer. dst is where an encoded image will be written.\nfunc NewEncoder(ext string, decodedBy Decoder, dst []byte) (Encoder, error) {\n\tif strings.ToLower(ext) == \".gif\" {\n\t\treturn newGifEncoder(decodedBy, dst)\n\t}\n\n\tif strings.ToLower(ext) == \".webp\" {\n\t\treturn newWebpEncoder(decodedBy, dst)\n\t}\n\n\tif strings.ToLower(ext) == \".avif\" {\n\t\treturn newAvifEncoder(decodedBy, dst)\n\t}\n\n\tif strings.ToLower(ext) == \".mp4\" || strings.ToLower(ext) == \".webm\" {\n\t\treturn nil, errors.New(\"Encoder cannot encode into video types\")\n\t}\n\n\tif strings.ToLower(ext) == \".thumbhash\" {\n\t\treturn newThumbhashEncoder(decodedBy, dst)\n\t}\n\n\treturn newOpenCVEncoder(ext, decodedBy, dst)\n}\n"
        },
        {
          "name": "lilliput_test.go",
          "type": "blob",
          "size": 3.755859375,
          "content": "// Package lilliput resizes and encodes images from\n// compressed images\npackage lilliput\n\nimport (\n\t\"io/ioutil\"\n\t\"testing\"\n)\n\nfunc TestNewDecoder(t *testing.T) {\n\ttests := []struct {\n\t\tname                 string\n\t\tsourceFilePath       string\n\t\twantHeight           int\n\t\twantWidth            int\n\t\twantErr              bool\n\t\twantNegativeDuration bool\n\t\twantAnimated         bool\n\t}{\n\t\t{\n\t\t\tname:           \"Standard MP4\",\n\t\t\tsourceFilePath: \"testdata/big_buck_bunny_480p_10s_std.mp4\",\n\t\t\twantHeight:     480,\n\t\t\twantWidth:      853,\n\t\t\twantErr:        false,\n\t\t},\n\t\t{\n\t\t\tname:           \"Progressive Download MP4\",\n\t\t\tsourceFilePath: \"testdata/big_buck_bunny_480p_10s_web.mp4\",\n\t\t\twantHeight:     480,\n\t\t\twantWidth:      853,\n\t\t\twantErr:        false,\n\t\t},\n\t\t{\n\t\t\tname:           \"Audio-only MP3\",\n\t\t\tsourceFilePath: \"testdata/tos-intro-3s.mp3\",\n\t\t\twantErr:        false,\n\t\t},\n\t\t{\n\t\t\tname:           \"Audio-only OGG\",\n\t\t\tsourceFilePath: \"testdata/tos-intro-3s.ogg\",\n\t\t\twantErr:        false,\n\t\t},\n\t\t{\n\t\t\tname:                 \"Audio-only AAC\",\n\t\t\tsourceFilePath:       \"testdata/tos-intro-3s.aac\",\n\t\t\twantErr:              false,\n\t\t\twantNegativeDuration: true,\n\t\t},\n\t\t{\n\t\t\tname:           \"Audio-only FLAC\",\n\t\t\tsourceFilePath: \"testdata/tos-intro-3s.flac\",\n\t\t\twantErr:        false,\n\t\t},\n\t\t{\n\t\t\tname:           \"Audio-only WAV\",\n\t\t\tsourceFilePath: \"testdata/tos-intro-3s.wav\",\n\t\t\twantErr:        false,\n\t\t},\n\t\t{\n\t\t\tname:                 \"WebP Image\",\n\t\t\tsourceFilePath:       \"testdata/tears_of_steel_icc.webp\",\n\t\t\twantWidth:            1920,\n\t\t\twantHeight:           800,\n\t\t\twantNegativeDuration: true,\n\t\t},\n\t\t{\n\t\t\tname:                 \"Animated WebP\",\n\t\t\tsourceFilePath:       \"testdata/big_buck_bunny_720_5s.webp\",\n\t\t\twantWidth:            480,\n\t\t\twantHeight:           270,\n\t\t\twantNegativeDuration: false,\n\t\t\twantAnimated:         true,\n\t\t},\n\t\t{\n\t\t\tname:                 \"Ordinary WebP\",\n\t\t\tsourceFilePath:       \"testdata/tears_of_steel_icc.webp\",\n\t\t\twantWidth:            1920,\n\t\t\twantHeight:           800,\n\t\t\twantNegativeDuration: true,\n\t\t\twantAnimated:         false,\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tsourceFileData, err := ioutil.ReadFile(tt.sourceFilePath)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to read source file: %v\", err)\n\t\t\t}\n\t\t\tdec, err := NewDecoder(sourceFileData)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\tt.Errorf(\"NewDecoder() error = %v, wantErr %v\", err, tt.wantErr)\n\t\t\t\treturn\n\t\t\t}\n\t\t\theader, err := dec.Header()\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"Failed to get header: %v\", err)\n\t\t\t}\n\t\t\tif header.Width() != tt.wantWidth {\n\t\t\t\tt.Errorf(\"Expected width to be %v, got %v\", tt.wantWidth, header.Width())\n\t\t\t}\n\t\t\tif header.Height() != tt.wantHeight {\n\t\t\t\tt.Errorf(\"Expected height to be %v, got %v\", tt.wantHeight, header.Height())\n\t\t\t}\n\t\t\tif tt.wantNegativeDuration && dec.Duration() > 0 {\n\t\t\t\tt.Errorf(\"Expected duration to be less than 0, got %v\", dec.Duration())\n\t\t\t}\n\t\t\tif !tt.wantNegativeDuration && dec.Duration() <= 0 {\n\t\t\t\tt.Errorf(\"Expected duration to be greater than 0, got %v\", dec.Duration())\n\t\t\t}\n\t\t\tif !tt.wantAnimated && header.IsAnimated() {\n\t\t\t\tt.Errorf(\"Expected image to not be animated\")\n\t\t\t}\n\t\t\tif tt.wantAnimated && !header.IsAnimated() {\n\t\t\t\tt.Errorf(\"Expected image to be animated\")\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc BenchmarkNewDecoder(b *testing.B) {\n\tsourceFilePath := \"testdata/big_buck_bunny_480p_10s_web.mp4\"\n\tsourceFileData, err := ioutil.ReadFile(sourceFilePath)\n\tif err != nil {\n\t\tb.Fatalf(\"Failed to read source file: %v\", err)\n\t}\n\n\tb.ResetTimer() // Start timing after setup\n\n\tfor i := 0; i < b.N; i++ {\n\t\tdec, err := NewDecoder(sourceFileData)\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"Failed to create decoder: %v\", err)\n\t\t}\n\t\tif dec != nil {\n\t\t\tdefer dec.Close()\n\t\t}\n\t\tif _, err := dec.Header(); err != nil {\n\t\t\tb.Fatalf(\"Failed to get header: %v\", err)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "opencv.cpp",
          "type": "blob",
          "size": 17.7373046875,
          "content": "#include \"opencv.hpp\"\n#include <stdbool.h>\n#include <opencv2/highgui.hpp>\n#include <opencv2/imgproc.hpp>\n#include <jpeglib.h>\n#include <png.h>\n#include <setjmp.h>\n#include <iostream>\n\nopencv_mat opencv_mat_create(int width, int height, int type)\n{\n    return new cv::Mat(height, width, type);\n}\n\nopencv_mat opencv_mat_create_from_data(int width, int height, int type, void* data, size_t data_len)\n{\n    size_t total_size = width * height * CV_ELEM_SIZE(type);\n    if (total_size > data_len) {\n        return NULL;\n    }\n    auto mat = new cv::Mat(height, width, type, data);\n    mat->datalimit = (uint8_t*)data + data_len;\n    return mat;\n}\n\nopencv_mat opencv_mat_create_empty_from_data(int length, void* data)\n{\n    // this is slightly sketchy - what we're going to do is build a 1x0 matrix\n    // and then set its data* properties to reflect the capacity (given by length arg here)\n    // this tells opencv internally that the Mat can store more but has nothing in it\n    // this is directly analogous to Go's len and cap\n    auto mat = new cv::Mat(0, 1, CV_8U, data);\n\n    mat->datalimit = mat->data + length;\n\n    return mat;\n}\n\nbool opencv_mat_set_row_stride(opencv_mat mat, size_t stride)\n{\n    auto m = static_cast<cv::Mat*>(mat);\n    if (m->step == stride) {\n        return true;\n    }\n    size_t width = m->cols;\n    size_t height = m->rows;\n    auto type = m->type();\n    auto width_stride = width * CV_ELEM_SIZE(type);\n    if (stride < width_stride) {\n        return false;\n    }\n    if (m->step != width_stride) {\n        // refuse to set the stride if it's already set\n        // the math for that is confusing and probably unnecessary to figure out\n        return false;\n    }\n    size_t total_size = stride * height;\n    if ((m->datastart + total_size) > m->datalimit) {\n        // don't exceed end of data array\n        return false;\n    }\n    m->step = stride;\n    return true;\n}\n\nvoid opencv_mat_release(opencv_mat mat)\n{\n    auto m = static_cast<cv::Mat*>(mat);\n    delete m;\n}\n\nint opencv_type_depth(int type)\n{\n    return CV_ELEM_SIZE1(type) * 8;\n}\n\nint opencv_type_channels(int type)\n{\n    return CV_MAT_CN(type);\n}\n\nint opencv_type_convert_depth(int t, int depth)\n{\n    return CV_MAKETYPE(depth, CV_MAT_CN(t));\n}\n\nopencv_decoder opencv_decoder_create(const opencv_mat buf)\n{\n    auto mat = static_cast<const cv::Mat*>(buf);\n    cv::ImageDecoder* d = new cv::ImageDecoder(*mat);\n    if (d->empty()) {\n        delete d;\n        d = NULL;\n    }\n    return d;\n}\n\nconst char* opencv_decoder_get_description(const opencv_decoder d)\n{\n    auto d_ptr = static_cast<cv::ImageDecoder*>(d);\n    return d_ptr->getDescription().c_str();\n}\n\nvoid opencv_decoder_release(opencv_decoder d)\n{\n    auto d_ptr = static_cast<cv::ImageDecoder*>(d);\n    delete d_ptr;\n}\n\nbool opencv_decoder_read_header(opencv_decoder d)\n{\n    auto d_ptr = static_cast<cv::ImageDecoder*>(d);\n    return d_ptr->readHeader();\n}\n\nint opencv_decoder_get_width(const opencv_decoder d)\n{\n    auto d_ptr = static_cast<cv::ImageDecoder*>(d);\n    return d_ptr->width();\n}\n\nint opencv_decoder_get_height(const opencv_decoder d)\n{\n    auto d_ptr = static_cast<cv::ImageDecoder*>(d);\n    return d_ptr->height();\n}\n\nint opencv_decoder_get_pixel_type(const opencv_decoder d)\n{\n    auto d_ptr = static_cast<cv::ImageDecoder*>(d);\n    return d_ptr->type();\n}\n\nint opencv_decoder_get_orientation(const opencv_decoder d)\n{\n    auto d_ptr = static_cast<cv::ImageDecoder*>(d);\n    return d_ptr->orientation();\n}\n\nbool opencv_decoder_read_data(opencv_decoder d, opencv_mat dst)\n{\n    auto d_ptr = static_cast<cv::ImageDecoder*>(d);\n    auto* mat = static_cast<cv::Mat*>(dst);\n    return d_ptr->readData(*mat);\n}\n\nopencv_encoder opencv_encoder_create(const char* ext, opencv_mat dst)\n{\n    auto* mat = static_cast<cv::Mat*>(dst);\n    return new cv::ImageEncoder(ext, *mat);\n}\n\nvoid opencv_encoder_release(opencv_encoder e)\n{\n    auto e_ptr = static_cast<cv::ImageEncoder*>(e);\n    delete e_ptr;\n}\n\nbool opencv_encoder_write(opencv_encoder e, const opencv_mat src, const int* opt, size_t opt_len)\n{\n    auto e_ptr = static_cast<cv::ImageEncoder*>(e);\n    auto mat = static_cast<const cv::Mat*>(src);\n    std::vector<int> params;\n    for (size_t i = 0; i < opt_len; i++) {\n        params.push_back(opt[i]);\n    }\n    return e_ptr->write(*mat, params);\n};\n\nvoid opencv_mat_resize(const opencv_mat src,\n                       opencv_mat dst,\n                       int width,\n                       int height,\n                       int interpolation)\n{\n    cv::resize(*static_cast<const cv::Mat*>(src),\n               *static_cast<cv::Mat*>(dst),\n               cv::Size(width, height),\n               0,\n               0,\n               interpolation);\n}\n\nopencv_mat opencv_mat_crop(const opencv_mat src, int x, int y, int width, int height)\n{\n    auto ret = new cv::Mat;\n    *ret = (*static_cast<const cv::Mat*>(src))(cv::Rect(x, y, width, height));\n    return ret;\n}\n\nvoid opencv_mat_orientation_transform(CVImageOrientation orientation, opencv_mat mat)\n{\n    auto cvMat = static_cast<cv::Mat*>(mat);\n    cv::OrientationTransform(int(orientation), *cvMat);\n}\n\nint opencv_mat_get_width(const opencv_mat mat)\n{\n    auto cvMat = static_cast<const cv::Mat*>(mat);\n    return cvMat->cols;\n}\n\nint opencv_mat_get_height(const opencv_mat mat)\n{\n    auto cvMat = static_cast<const cv::Mat*>(mat);\n    return cvMat->rows;\n}\n\nvoid* opencv_mat_get_data(const opencv_mat mat)\n{\n    auto cvMat = static_cast<const cv::Mat*>(mat);\n    return cvMat->data;\n}\n\nstruct opencv_jpeg_error_mgr {\n    struct jpeg_error_mgr pub;\n    jmp_buf setjmp_buffer;\n};\n\nvoid opencv_jpeg_error_exit(j_common_ptr cinfo) {\n    opencv_jpeg_error_mgr* myerr = (opencv_jpeg_error_mgr*) cinfo->err;\n    (*cinfo->err->output_message)(cinfo);\n    longjmp(myerr->setjmp_buffer, 1);\n}\n\nint opencv_decoder_get_jpeg_icc(void* src, size_t src_len, void* dest, size_t dest_len) {\n    struct jpeg_decompress_struct cinfo;\n    struct opencv_jpeg_error_mgr jerr;\n\n    cinfo.err = jpeg_std_error(&jerr.pub);\n    jerr.pub.error_exit = opencv_jpeg_error_exit;\n\n    if (setjmp(jerr.setjmp_buffer)) {\n        // JPEG processing error\n        jpeg_destroy_decompress(&cinfo);\n        return 0;\n    }\n\n    jpeg_create_decompress(&cinfo);\n    jpeg_mem_src(&cinfo, static_cast<unsigned char*>(src), src_len);\n\n    // Ask libjpeg to save markers that might be ICC profiles\n    jpeg_save_markers(&cinfo, JPEG_APP0 + 2, 0xFFFF);\n\n    // Read JPEG header\n    if (jpeg_read_header(&cinfo, TRUE) != JPEG_HEADER_OK) {\n        jpeg_destroy_decompress(&cinfo);\n        return 0;\n    }\n\n    // Check if ICC profile is available\n    JOCTET *icc_profile = nullptr;\n    unsigned int icc_length = 0;\n    if (jpeg_read_icc_profile(&cinfo, &icc_profile, &icc_length)) {\n        if (icc_length > 0 && icc_length <= dest_len) {\n            memcpy(dest, icc_profile, icc_length);\n            free(icc_profile);\n            jpeg_destroy_decompress(&cinfo);\n            return icc_length;\n        }\n    }\n\n    if (icc_profile) {\n        // Free the ICC profile if it was allocated but not copied\n        free(icc_profile);\n    }\n    jpeg_destroy_decompress(&cinfo);\n    return 0;\n}\n\nvoid opencv_decoder_png_read(png_structp png_ptr, png_bytep data, png_size_t length) {\n    auto buffer_info = reinterpret_cast<std::pair<const char**, size_t*>*>(png_get_io_ptr(png_ptr));\n    const char* &buffer = *buffer_info->first;\n    size_t &buffer_size = *buffer_info->second;\n\n    if (buffer_size < length) {\n        png_error(png_ptr, \"Read error: attempting to read beyond buffer size\");\n        return;\n    }\n\n    memcpy(data, buffer, length);\n    buffer += length;\n    buffer_size -= length;\n}\n\nint opencv_decoder_get_png_icc(void* src, size_t src_len, void* dest, size_t dest_len) {\n    // Set up libpng to read from memory\n    const char* buffer = reinterpret_cast<const char*>(src);\n    size_t buffer_size = src_len;\n    std::pair<const char**, size_t*> buffer_info(&buffer, &buffer_size);\n\n    png_structp png_ptr = png_create_read_struct(PNG_LIBPNG_VER_STRING, nullptr, nullptr, nullptr);\n    png_infop info_ptr = png_create_info_struct(png_ptr);\n    if (setjmp(png_jmpbuf(png_ptr))) {\n        png_destroy_read_struct(&png_ptr, &info_ptr, nullptr);\n        return 0;\n    }\n    png_set_read_fn(png_ptr, &buffer_info, opencv_decoder_png_read);\n    png_read_info(png_ptr, info_ptr);\n\n    // Check for ICC profile\n    png_charp icc_name;\n    int compression_type;\n    png_bytep icc_profile;\n    png_uint_32 icc_length;\n    if (png_get_iCCP(png_ptr, info_ptr, &icc_name, &compression_type, &icc_profile, &icc_length)) {\n        if (icc_length > 0 && icc_length <= dest_len) {\n            memcpy(dest, icc_profile, icc_length);\n            png_destroy_read_struct(&png_ptr, &info_ptr, nullptr); // handles freeing icc_profile\n            return icc_length;\n        }\n    }\n\n    png_destroy_read_struct(&png_ptr, &info_ptr, nullptr);\n    return 0;\n}\n\n/**\n * @brief Reset all pixels in the matrix to zero.\n * \n * @param mat Pointer to the OpenCV matrix to be reset.\n */\nvoid opencv_mat_reset(opencv_mat mat) {\n    if (mat) {\n        cv::Mat* m = static_cast<cv::Mat*>(mat);\n        m->setTo(cv::Scalar(0));\n    }\n}\n\n/**\n * @brief Set the entire matrix to a specific color.\n * \n * @param mat Pointer to the OpenCV matrix to be colored.\n * @param red Red component of the color (0-255).\n * @param green Green component of the color (0-255).\n * @param blue Blue component of the color (0-255).\n * @param alpha Alpha component of the color (0-255). If negative, treated as a 3-channel image.\n */\nvoid opencv_mat_set_color(opencv_mat mat, int red, int green, int blue, int alpha) {\n    auto cvMat = static_cast<cv::Mat*>(mat);\n    if (cvMat) {\n        cv::Scalar color = (alpha >= 0) ? cv::Scalar(blue, green, red, alpha) : cv::Scalar(blue, green, red);\n        cvMat->setTo(color);\n    }\n}\n\n/**\n * @brief Clear a rectangular region of the matrix to transparent.\n * \n * @param mat Pointer to the OpenCV matrix to be modified.\n * @param xOffset X-coordinate of the top-left corner of the rectangle.\n * @param yOffset Y-coordinate of the top-left corner of the rectangle.\n * @param width Width of the rectangle.\n * @param height Height of the rectangle.\n * @return int Error code.\n */\nint opencv_mat_clear_to_transparent(opencv_mat mat, int xOffset, int yOffset, int width, int height) {\n    auto cvMat = static_cast<cv::Mat*>(mat);\n    if (!cvMat) {\n        return OPENCV_ERROR_NULL_MATRIX;\n    }\n\n    if (xOffset < 0 || yOffset < 0 || xOffset + width > cvMat->cols || yOffset + height > cvMat->rows) {\n        return OPENCV_ERROR_OUT_OF_BOUNDS;\n    }\n\n    if (width <= 0 || height <= 0) {\n        return OPENCV_ERROR_INVALID_DIMENSIONS;\n    }\n\n    try {\n        cv::Rect roi(xOffset, yOffset, width, height);\n        if (cvMat->channels() == 4) {\n            cvMat->operator()(roi).setTo(cv::Scalar(0, 0, 0, 0));\n        } else if (cvMat->channels() == 3) {\n            // For 3-channel images, we'll use black as \"transparent\"\n            cvMat->operator()(roi).setTo(cv::Scalar(0, 0, 0));\n        } else {\n            return OPENCV_ERROR_INVALID_CHANNEL_COUNT;\n        }\n        return OPENCV_SUCCESS;\n    } catch (const cv::Exception& e) {\n        std::cerr << \"OpenCV exception in opencv_mat_clear_to_transparent: \" << e.what() << std::endl;\n        return OPENCV_ERROR_UNKNOWN;\n    }\n}\n\n/**\n * @brief Blend source image with destination image using alpha blending.\n * \n * @param src Pointer to the source OpenCV matrix.\n * @param dst Pointer to the destination OpenCV matrix.\n * @param xOffset X-coordinate offset in the destination image.\n * @param yOffset Y-coordinate offset in the destination image.\n * @param width Width of the region to copy.\n * @param height Height of the region to copy.\n * @return int Error code.\n */\nint opencv_copy_to_region_with_alpha(opencv_mat src, opencv_mat dst, int xOffset, int yOffset, int width, int height) {\n    try {\n        auto srcMat = static_cast<cv::Mat*>(src);\n        auto dstMat = static_cast<cv::Mat*>(dst);\n\n        if (!srcMat || !dstMat || srcMat->empty() || dstMat->empty()) {\n            return OPENCV_ERROR_NULL_MATRIX;\n        }\n\n        if (xOffset < 0 || yOffset < 0 || xOffset + width > dstMat->cols || yOffset + height > dstMat->rows) {\n            return OPENCV_ERROR_OUT_OF_BOUNDS;\n        }\n\n        if (width <= 0 || height <= 0) {\n            return OPENCV_ERROR_INVALID_DIMENSIONS;\n        }\n\n        cv::Rect roi(xOffset, yOffset, width, height);\n        cv::Mat dstROI = dstMat->operator()(roi);\n\n        cv::Mat srcResized;\n        if (srcMat->size() != dstROI.size()) {\n            cv::resize(*srcMat, srcResized, dstROI.size(), 0, 0, cv::INTER_LINEAR);\n        } else {\n            srcResized = *srcMat;\n        }\n\n        // Handle grayscale source\n        if (srcResized.channels() == 1) {\n            cv::cvtColor(srcResized, srcResized, cv::COLOR_GRAY2BGR);\n        }\n\n        // Ensure both matrices are 4-channel\n        cv::Mat src4, dst4;\n        if (srcResized.channels() == 3) {\n            cv::cvtColor(srcResized, src4, cv::COLOR_BGR2BGRA);\n        } else if (srcResized.channels() == 4) {\n            src4 = srcResized;\n        } else {\n            return OPENCV_ERROR_INVALID_CHANNEL_COUNT;\n        }\n\n        if (dstROI.channels() == 3) {\n            cv::cvtColor(dstROI, dst4, cv::COLOR_BGR2BGRA);\n        } else if (dstROI.channels() == 4) {\n            dst4 = dstROI;\n        } else {\n            return OPENCV_ERROR_INVALID_CHANNEL_COUNT;\n        }\n\n        // Perform alpha blending\n        std::vector<cv::Mat> srcChannels, dstChannels;\n        cv::split(src4, srcChannels);\n        cv::split(dst4, dstChannels);\n\n        cv::Mat srcAlpha = srcChannels[3];\n        cv::Mat dstAlpha = dstChannels[3];\n        cv::Mat srcAlphaF, dstAlphaF, outAlphaF;\n        srcAlpha.convertTo(srcAlphaF, CV_32F, 1.0 / 255.0);\n        dstAlpha.convertTo(dstAlphaF, CV_32F, 1.0 / 255.0);\n        outAlphaF = srcAlphaF + dstAlphaF.mul(1.0f - srcAlphaF);\n\n        for (int i = 0; i < 3; ++i) {\n            cv::Mat srcChannelF, dstChannelF;\n            srcChannels[i].convertTo(srcChannelF, CV_32F, 1.0 / 255.0);\n            dstChannels[i].convertTo(dstChannelF, CV_32F, 1.0 / 255.0);\n            cv::Mat blended = (srcChannelF.mul(srcAlphaF) + dstChannelF.mul(dstAlphaF).mul(1.0f - srcAlphaF)) / outAlphaF;\n            blended.convertTo(dstChannels[i], CV_8U, 255.0);\n        }\n        outAlphaF.convertTo(dstChannels[3], CV_8U, 255.0);\n\n        cv::merge(dstChannels, dst4);\n\n        // Convert back to original channel count if necessary\n        if (dstROI.channels() == 3) {\n            cv::cvtColor(dst4, dstROI, cv::COLOR_BGRA2BGR);\n        } else {\n            dst4.copyTo(dstROI);\n        }\n\n        return OPENCV_SUCCESS;\n    } catch (const cv::Exception& e) {\n        std::cerr << \"OpenCV exception in opencv_copy_to_region_with_alpha: \" << e.what() << std::endl;\n        return OPENCV_ERROR_ALPHA_BLENDING_FAILED;\n    } catch (const std::exception& e) {\n        std::cerr << \"Standard exception in opencv_copy_to_region_with_alpha: \" << e.what() << std::endl;\n        return OPENCV_ERROR_UNKNOWN;\n    } catch (...) {\n        std::cerr << \"Unknown exception in opencv_copy_to_region_with_alpha\" << std::endl;\n        return OPENCV_ERROR_UNKNOWN;\n    }\n}\n\n/**\n * @brief Copy source image to a rectangular region of the destination image.\n * \n * @param src Pointer to the source OpenCV matrix.\n * @param dst Pointer to the destination OpenCV matrix.\n * @param xOffset X-coordinate of the top-left corner in the destination image.\n * @param yOffset Y-coordinate of the top-left corner in the destination image.\n * @param width Width of the region to copy.\n * @param height Height of the region to copy.\n * @return int Error code.\n */\nint opencv_copy_to_region(opencv_mat src, opencv_mat dst, int xOffset, int yOffset, int width, int height) {\n    try {\n        auto srcMat = static_cast<cv::Mat*>(src);\n        auto dstMat = static_cast<cv::Mat*>(dst);\n\n        if (!srcMat || !dstMat || srcMat->empty() || dstMat->empty()) {\n            return OPENCV_ERROR_NULL_MATRIX;\n        }\n\n        if (xOffset < 0 || yOffset < 0 || xOffset + width > dstMat->cols || yOffset + height > dstMat->rows) {\n            return OPENCV_ERROR_OUT_OF_BOUNDS;\n        }\n\n        if (width <= 0 || height <= 0) {\n            return OPENCV_ERROR_INVALID_DIMENSIONS;\n        }\n\n        cv::Rect roi(xOffset, yOffset, width, height);\n        cv::Mat dstROI = dstMat->operator()(roi);\n\n        // Resize source if necessary\n        cv::Mat srcResized;\n        if (srcMat->size() != dstROI.size()) {\n            cv::resize(*srcMat, srcResized, dstROI.size(), 0, 0, cv::INTER_LINEAR);\n        } else {\n            srcResized = *srcMat;\n        }\n\n        // Handle channel count mismatch\n        if (srcResized.channels() != dstROI.channels()) {\n            if (srcResized.channels() == 3 && dstROI.channels() == 4) {\n                cv::cvtColor(srcResized, srcResized, cv::COLOR_BGR2BGRA);\n            } else if (srcResized.channels() == 4 && dstROI.channels() == 3) {\n                cv::cvtColor(srcResized, srcResized, cv::COLOR_BGRA2BGR);\n            } else if (srcResized.channels() == 1 && dstROI.channels() == 3) {\n                cv::cvtColor(srcResized, srcResized, cv::COLOR_GRAY2BGR);\n            } else if (srcResized.channels() == 1 && dstROI.channels() == 4) {\n                cv::cvtColor(srcResized, srcResized, cv::COLOR_GRAY2BGRA);\n            } else {\n                return OPENCV_ERROR_INVALID_CHANNEL_COUNT;\n            }\n        }\n\n        // Perform the copy\n        srcResized.copyTo(dstROI);\n\n        return OPENCV_SUCCESS;\n    } catch (const cv::Exception& e) {\n        std::cerr << \"OpenCV exception in opencv_copy_to_region: \" << e.what() << std::endl;\n        return OPENCV_ERROR_COPY_FAILED;\n    } catch (const std::exception& e) {\n        std::cerr << \"Standard exception in opencv_copy_to_region: \" << e.what() << std::endl;\n        return OPENCV_ERROR_UNKNOWN;\n    } catch (...) {\n        std::cerr << \"Unknown exception in opencv_copy_to_region\" << std::endl;\n        return OPENCV_ERROR_UNKNOWN;\n    }\n}\n"
        },
        {
          "name": "opencv.go",
          "type": "blob",
          "size": 23.025390625,
          "content": "package lilliput\n\n// #include \"opencv.hpp\"\n// #include \"avif.hpp\"\nimport \"C\"\n\nimport (\n\t\"bytes\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"image\"\n\t\"io\"\n\t\"time\"\n\t\"unsafe\"\n)\n\n// DisposeMethod describes how the previous frame should be disposed before rendering the next frame.\ntype DisposeMethod int\n\nconst (\n\t// NoDispose indicates the previous frame should remain as-is\n\tNoDispose DisposeMethod = iota\n\t// DisposeToBackgroundColor indicates the previous frame area should be cleared to background color\n\tDisposeToBackgroundColor\n)\n\n// BlendMethod describes how the previous frame should be blended with the next frame.\ntype BlendMethod int\n\nconst (\n\t// UseAlphaBlending indicates alpha blending should be used when compositing frames\n\tUseAlphaBlending BlendMethod = iota\n\t// NoBlend indicates frames should be copied directly without blending\n\tNoBlend\n)\n\n// ImageOrientation describes how the decoded image is oriented according to its metadata.\ntype ImageOrientation int\n\nconst (\n\t// Standard image encoding constants\n\tJpegQuality     = int(C.CV_IMWRITE_JPEG_QUALITY)     // Quality parameter for JPEG encoding (0-100)\n\tPngCompression  = int(C.CV_IMWRITE_PNG_COMPRESSION)  // Compression level for PNG encoding (0-9)\n\tWebpQuality     = int(C.CV_IMWRITE_WEBP_QUALITY)     // Quality parameter for WebP encoding (0-100)\n\tJpegProgressive = int(C.CV_IMWRITE_JPEG_PROGRESSIVE) // Enable progressive JPEG encoding\n\tAvifQuality     = int(C.AVIF_QUALITY)                // Quality parameter for AVIF encoding (0-100)\n\tAvifSpeed       = int(C.AVIF_SPEED)                  // Speed parameter for AVIF encoding (0-10)\n\n\t// Image orientation constants\n\tOrientationTopLeft     = ImageOrientation(C.CV_IMAGE_ORIENTATION_TL)\n\tOrientationTopRight    = ImageOrientation(C.CV_IMAGE_ORIENTATION_TR)\n\tOrientationBottomRight = ImageOrientation(C.CV_IMAGE_ORIENTATION_BR)\n\tOrientationBottomLeft  = ImageOrientation(C.CV_IMAGE_ORIENTATION_BL)\n\tOrientationLeftTop     = ImageOrientation(C.CV_IMAGE_ORIENTATION_LT)\n\tOrientationRightTop    = ImageOrientation(C.CV_IMAGE_ORIENTATION_RT)\n\tOrientationRightBottom = ImageOrientation(C.CV_IMAGE_ORIENTATION_RB)\n\tOrientationLeftBottom  = ImageOrientation(C.CV_IMAGE_ORIENTATION_LB)\n\n\t// PNG chunk field lengths\n\tpngChunkSizeFieldLen = 4\n\tpngChunkTypeFieldLen = 4\n\tpngChunkAllFieldsLen = 12\n\n\t// JPEG segment type markers\n\tjpegEOISegmentType byte = 0xD9 // End of Image marker\n\tjpegSOSSegmentType byte = 0xDA // Start of Scan marker\n)\n\n// PNG chunk type identifiers\nvar (\n\tpngActlChunkType = []byte{byte('a'), byte('c'), byte('T'), byte('L')} // Animation Control Chunk\n\tpngFctlChunkType = []byte{byte('f'), byte('c'), byte('T'), byte('L')} // Frame Control Chunk\n\tpngFdatChunkType = []byte{byte('f'), byte('d'), byte('A'), byte('T')} // Frame Data Chunk\n\tpngIendChunkType = []byte{byte('I'), byte('E'), byte('N'), byte('D')} // Image End Chunk\n\n\t// Map of JPEG segment types that don't have a size field\n\tjpegUnsizedSegmentTypes = map[byte]bool{\n\t\t0xD0:               true, // RST0 marker\n\t\t0xD1:               true, // RST1 marker\n\t\t0xD2:               true, // RST2 marker\n\t\t0xD3:               true, // RST3 marker\n\t\t0xD4:               true, // RST4 marker\n\t\t0xD5:               true, // RST5 marker\n\t\t0xD6:               true, // RST6 marker\n\t\t0xD7:               true, // RST7 marker\n\t\t0xD8:               true, // SOI marker\n\t\tjpegEOISegmentType: true, // EOI marker\n\t}\n)\n\n// PixelType describes the base pixel type of the image.\ntype PixelType int\n\n// ImageHeader contains basic decoded image metadata.\ntype ImageHeader struct {\n\twidth         int              // Width of the image in pixels\n\theight        int              // Height of the image in pixels\n\tpixelType     PixelType        // Type of pixels in the image\n\torientation   ImageOrientation // Orientation from image metadata\n\tnumFrames     int              // Number of frames (1 for static images)\n\tcontentLength int              // Length of actual image content\n}\n\n// Framebuffer contains an array of raw, decoded pixel data.\ntype Framebuffer struct {\n\tbuf       []byte        // Raw pixel data\n\tmat       C.opencv_mat  // OpenCV matrix containing the pixel data\n\twidth     int           // Width of the frame in pixels\n\theight    int           // Height of the frame in pixels\n\tpixelType PixelType     // Type of pixels in the frame\n\tduration  time.Duration // Duration to display this frame\n\txOffset   int           // X offset for drawing this frame\n\tyOffset   int           // Y offset for drawing this frame\n\tdispose   DisposeMethod // How to dispose previous frame\n\tblend     BlendMethod   // How to blend with previous frame\n}\n\n// openCVDecoder implements the Decoder interface for images supported by OpenCV.\ntype openCVDecoder struct {\n\tdecoder       C.opencv_decoder // Native OpenCV decoder\n\tmat           C.opencv_mat     // OpenCV matrix containing the image data\n\tbuf           []byte           // Original encoded image data\n\thasReadHeader bool             // Whether header has been read\n\thasDecoded    bool             // Whether image has been decoded\n}\n\n// openCVEncoder implements the Encoder interface for images supported by OpenCV.\ntype openCVEncoder struct {\n\tencoder C.opencv_encoder // Native OpenCV encoder\n\tdst     C.opencv_mat     // Destination OpenCV matrix\n\tdstBuf  []byte           // Destination buffer for encoded data\n}\n\n// Depth returns the number of bits in the PixelType.\nfunc (p PixelType) Depth() int {\n\treturn int(C.opencv_type_depth(C.int(p)))\n}\n\n// Channels returns the number of channels in the PixelType.\nfunc (p PixelType) Channels() int {\n\treturn int(C.opencv_type_channels(C.int(p)))\n}\n\n// Width returns the width of the image in number of pixels.\nfunc (h *ImageHeader) Width() int {\n\treturn h.width\n}\n\n// Height returns the height of the image in number of pixels.\nfunc (h *ImageHeader) Height() int {\n\treturn h.height\n}\n\n// PixelType returns a PixelType describing the image's pixels.\nfunc (h *ImageHeader) PixelType() PixelType {\n\treturn h.pixelType\n}\n\n// Orientation returns the metadata-based image orientation.\nfunc (h *ImageHeader) Orientation() ImageOrientation {\n\treturn h.orientation\n}\n\n// IsAnimated returns true if the image contains multiple frames.\nfunc (h *ImageHeader) IsAnimated() bool {\n\treturn h.numFrames > 1\n}\n\n// HasAlpha returns true if the image has an alpha channel.\nfunc (h *ImageHeader) HasAlpha() bool {\n\treturn h.pixelType.Channels() == 4\n}\n\n// ContentLength returns the length of the necessary image data.\n// Data past this point can be safely truncated using data[:h.ContentLength()].\n// This helps handle padding bytes and potential unwanted trailing data.\n// This could be applicable to images with unwanted data at the end (e.g. \"acropalypse\" bug).\nfunc (h *ImageHeader) ContentLength() int {\n\treturn h.contentLength\n}\n\n// NewFramebuffer creates a backing store for a pixel frame buffer with the specified dimensions.\nfunc NewFramebuffer(width, height int) *Framebuffer {\n\treturn &Framebuffer{\n\t\tbuf: make([]byte, width*height*4),\n\t\tmat: nil,\n\t}\n}\n\n// Close releases the resources associated with Framebuffer.\nfunc (f *Framebuffer) Close() {\n\tif f.mat != nil {\n\t\tC.opencv_mat_release(f.mat)\n\t\tf.mat = nil\n\t}\n}\n\n// Clear resets all pixel data in Framebuffer for the active frame and resets the mat if it exists.\nfunc (f *Framebuffer) Clear() {\n\tC.memset(unsafe.Pointer(&f.buf[0]), 0, C.size_t(len(f.buf)))\n\tif f.mat != nil {\n\t\tC.opencv_mat_reset(f.mat)\n\t}\n}\n\n// Create3Channel initializes the framebuffer for 3-channel (RGB) image data.\nfunc (f *Framebuffer) Create3Channel(width, height int) error {\n\tif err := f.resizeMat(width, height, C.CV_8UC3); err != nil {\n\t\treturn err\n\t}\n\tf.Clear()\n\treturn nil\n}\n\n// Create4Channel initializes the framebuffer for 4-channel (RGBA) image data.\nfunc (f *Framebuffer) Create4Channel(width, height int) error {\n\tif err := f.resizeMat(width, height, C.CV_8UC4); err != nil {\n\t\treturn err\n\t}\n\tf.Clear()\n\treturn nil\n}\n\n// resizeMat resizes the OpenCV matrix to the specified dimensions and pixel type.\n// Returns ErrBufTooSmall if the matrix cannot be created at the specified size.\nfunc (f *Framebuffer) resizeMat(width, height int, pixelType PixelType) error {\n\tif f.mat != nil {\n\t\tC.opencv_mat_release(f.mat)\n\t\tf.mat = nil\n\t}\n\tif pixelType.Depth() > 8 {\n\t\tpixelType = PixelType(C.opencv_type_convert_depth(C.int(pixelType), C.CV_8U))\n\t}\n\tnewMat := C.opencv_mat_create_from_data(C.int(width), C.int(height), C.int(pixelType), unsafe.Pointer(&f.buf[0]), C.size_t(len(f.buf)))\n\tif newMat == nil {\n\t\treturn ErrBufTooSmall\n\t}\n\tf.mat = newMat\n\tf.width = width\n\tf.height = height\n\tf.pixelType = pixelType\n\treturn nil\n}\n\n// OrientationTransform rotates and/or mirrors the Framebuffer according to the given orientation.\n// Passing the orientation from ImageHeader will normalize the orientation.\nfunc (f *Framebuffer) OrientationTransform(orientation ImageOrientation) {\n\tif f.mat == nil {\n\t\treturn\n\t}\n\n\tC.opencv_mat_orientation_transform(C.CVImageOrientation(orientation), f.mat)\n\tf.width = int(C.opencv_mat_get_width(f.mat))\n\tf.height = int(C.opencv_mat_get_height(f.mat))\n}\n\n// ResizeTo performs a resizing transform on the Framebuffer and puts the result\n// in the provided destination Framebuffer. This function does not preserve aspect\n// ratio if the given dimensions differ in ratio from the source. Returns an error\n// if the destination is not large enough to hold the given dimensions.\nfunc (f *Framebuffer) ResizeTo(width, height int, dst *Framebuffer) error {\n\tif width < 1 {\n\t\twidth = 1\n\t}\n\n\tif height < 1 {\n\t\theight = 1\n\t}\n\n\terr := dst.resizeMat(width, height, f.pixelType)\n\tif err != nil {\n\t\treturn err\n\t}\n\tC.opencv_mat_resize(f.mat, dst.mat, C.int(width), C.int(height), C.CV_INTER_AREA)\n\treturn nil\n}\n\n// ClearToTransparent clears a rectangular region of the framebuffer to transparent.\nfunc (f *Framebuffer) ClearToTransparent(rect image.Rectangle) error {\n\tif f.mat == nil {\n\t\treturn errors.New(\"framebuffer matrix is nil\")\n\t}\n\n\tresult := C.opencv_mat_clear_to_transparent(f.mat, C.int(rect.Min.X), C.int(rect.Min.Y), C.int(rect.Dx()), C.int(rect.Dy()))\n\treturn handleOpenCVError(result)\n}\n\n// Fit performs a resizing and cropping transform on the Framebuffer and puts the result\n// in the provided destination Framebuffer. This function does preserve aspect ratio\n// but will crop columns or rows from the edges of the image as necessary in order to\n// keep from stretching the image content. Returns an error if the destination is\n// not large enough to hold the given dimensions.\nfunc (f *Framebuffer) Fit(width, height int, dst *Framebuffer) error {\n\tif f.mat == nil {\n\t\treturn ErrFrameBufNoPixels\n\t}\n\n\taspectIn := float64(f.width) / float64(f.height)\n\taspectOut := float64(width) / float64(height)\n\n\tvar widthPostCrop, heightPostCrop int\n\tif aspectIn > aspectOut {\n\t\t// input is wider than output, so we'll need to narrow\n\t\t// we preserve input height and reduce width\n\t\twidthPostCrop = int((aspectOut * float64(f.height)) + 0.5)\n\t\theightPostCrop = f.height\n\t} else {\n\t\t// input is taller than output, so we'll need to shrink\n\t\theightPostCrop = int((float64(f.width) / aspectOut) + 0.5)\n\t\twidthPostCrop = f.width\n\t}\n\n\tif widthPostCrop < 1 {\n\t\twidthPostCrop = 1\n\t}\n\n\tif heightPostCrop < 1 {\n\t\theightPostCrop = 1\n\t}\n\n\tvar left, top int\n\tleft = int(float64(f.width-widthPostCrop) * 0.5)\n\tif left < 0 {\n\t\tleft = 0\n\t}\n\n\ttop = int(float64(f.height-heightPostCrop) * 0.5)\n\tif top < 0 {\n\t\ttop = 0\n\t}\n\n\tnewMat := C.opencv_mat_crop(f.mat, C.int(left), C.int(top), C.int(widthPostCrop), C.int(heightPostCrop))\n\tdefer C.opencv_mat_release(newMat)\n\n\terr := dst.resizeMat(width, height, f.pixelType)\n\tif err != nil {\n\t\treturn err\n\t}\n\tC.opencv_mat_resize(newMat, dst.mat, C.int(width), C.int(height), C.CV_INTER_AREA)\n\treturn nil\n}\n\n// Width returns the width of the contained pixel data in number of pixels. This may\n// differ from the capacity of the framebuffer.\nfunc (f *Framebuffer) Width() int {\n\treturn f.width\n}\n\n// Height returns the height of the contained pixel data in number of pixels. This may\n// differ from the capacity of the framebuffer.\nfunc (f *Framebuffer) Height() int {\n\treturn f.height\n}\n\n// PixelType returns the PixelType information of the contained pixel data, if any.\nfunc (f *Framebuffer) PixelType() PixelType {\n\treturn f.pixelType\n}\n\n// Duration returns the length of time this frame plays out in an animated image\nfunc (f *Framebuffer) Duration() time.Duration {\n\treturn f.duration\n}\n\n// handleOpenCVError converts an OpenCV error code to an error\nfunc handleOpenCVError(result C.int) error {\n\tswitch result {\n\tcase C.OPENCV_SUCCESS:\n\t\treturn nil\n\tcase C.OPENCV_ERROR_INVALID_CHANNEL_COUNT:\n\t\treturn errors.New(\"error copying opencv data: source image must have 3 or 4 channels\")\n\tcase C.OPENCV_ERROR_OUT_OF_BOUNDS:\n\t\treturn errors.New(\"error copying opencv data: source image with offsets exceeds the bounds of the destination framebuffer\")\n\tcase C.OPENCV_ERROR_NULL_MATRIX:\n\t\treturn errors.New(\"error copying opencv data: source or destination matrix is null\")\n\tcase C.OPENCV_ERROR_ALPHA_BLENDING_FAILED:\n\t\treturn errors.New(\"error copying opencv data: alpha blending failed\")\n\tcase C.OPENCV_ERROR_FINAL_CONVERSION_FAILED:\n\t\treturn errors.New(\"error copying opencv data: final conversion failed\")\n\tcase C.OPENCV_ERROR_CONVERSION_FAILED:\n\t\treturn errors.New(\"error copying opencv data: conversion failed\")\n\tcase C.OPENCV_ERROR_RESIZE_FAILED:\n\t\treturn errors.New(\"error copying opencv data: resize failed\")\n\tcase C.OPENCV_ERROR_COPY_FAILED:\n\t\treturn errors.New(\"error copying opencv data: copy failed\")\n\tcase C.OPENCV_ERROR_INVALID_DIMENSIONS:\n\t\treturn errors.New(\"error copying opencv data: invalid dimensions\")\n\tcase C.OPENCV_ERROR_UNKNOWN:\n\t\treturn errors.New(\"unknown error copying opencv data\")\n\tdefault:\n\t\treturn errors.New(\"unknown error occurred during alpha blending\")\n\t}\n}\n\n// CopyToOffsetWithAlphaBlending copies the source framebuffer to a specified rectangle within the destination framebuffer.\n// This function performs alpha blending.\nfunc (f *Framebuffer) CopyToOffsetWithAlphaBlending(src *Framebuffer, rect image.Rectangle) error {\n\tresult := C.opencv_copy_to_region_with_alpha(src.mat, f.mat, C.int(rect.Min.X), C.int(rect.Min.Y), C.int(rect.Dx()), C.int(rect.Dy()))\n\treturn handleOpenCVError(result)\n}\n\n// CopyToOffsetNoBlend copies the source framebuffer to a specified rectangle within the destination framebuffer.\n// This function does not perform any blending.\nfunc (f *Framebuffer) CopyToOffsetNoBlend(src *Framebuffer, rect image.Rectangle) error {\n\tresult := C.opencv_copy_to_region(src.mat, f.mat, C.int(rect.Min.X), C.int(rect.Min.Y), C.int(rect.Dx()), C.int(rect.Dy()))\n\treturn handleOpenCVError(result)\n}\n\nfunc newOpenCVDecoder(buf []byte) (*openCVDecoder, error) {\n\tmat := C.opencv_mat_create_from_data(C.int(len(buf)), 1, C.CV_8U, unsafe.Pointer(&buf[0]), C.size_t(len(buf)))\n\n\t// this next check is sort of silly since this array is 1-dimensional\n\t// but if the create ever changes and we goof up, could catch a\n\t// buffer overwrite\n\tif mat == nil {\n\t\treturn nil, ErrBufTooSmall\n\t}\n\n\tdecoder := C.opencv_decoder_create(mat)\n\tif decoder == nil {\n\t\tC.opencv_mat_release(mat)\n\t\treturn nil, ErrInvalidImage\n\t}\n\n\treturn &openCVDecoder{\n\t\tmat:     mat,\n\t\tdecoder: decoder,\n\t\tbuf:     buf,\n\t}, nil\n}\n\n// chunk format https://www.w3.org/TR/PNG-Structure.html\n// TLDR: 4 bytes length, 4 bytes type, variable data, 4 bytes CRC\n// length is only the \"data\" field; does not include itself, the type or the CRC\ntype pngChunkIter struct {\n\tpng        []byte\n\titerOffset int\n}\n\nfunc makePngChunkIter(png []byte) (*pngChunkIter, error) {\n\tif !bytes.HasPrefix(png, pngMagic) {\n\t\treturn nil, errors.New(\"Image is not PNG\")\n\t}\n\n\treturn &pngChunkIter{\n\t\tpng: png, iterOffset: 0,\n\t}, nil\n}\n\nfunc (it *pngChunkIter) hasSpaceForChunk() bool {\n\treturn it.iterOffset+pngChunkAllFieldsLen <= len(it.png)\n}\n\n// byte offset of the next chunk. might be past the end of the data\n// for the last chunk, or if the chunk is malformed\nfunc (it *pngChunkIter) nextChunkOffset() int {\n\tchunkDataSize := (int)(binary.BigEndian.Uint32(it.png[it.iterOffset:]))\n\treturn it.iterOffset + chunkDataSize + pngChunkAllFieldsLen\n}\n\nfunc (it *pngChunkIter) next() bool {\n\tif it.iterOffset < len(pngMagic) {\n\t\t// move to the first chunk by skipping png magic prefix\n\t\tit.iterOffset = len(pngMagic)\n\t\treturn it.hasSpaceForChunk()\n\t}\n\tif !it.hasSpaceForChunk() {\n\t\treturn false\n\t}\n\n\tit.iterOffset = it.nextChunkOffset()\n\treturn it.hasSpaceForChunk()\n}\n\nfunc (it *pngChunkIter) chunkType() []byte {\n\treturn it.png[it.iterOffset+4 : it.iterOffset+8]\n}\n\nfunc detectContentLengthPNG(png []byte) int {\n\tchunkIter, err := makePngChunkIter(png)\n\tif err != nil {\n\t\t// This is not a png, take all the data\n\t\treturn len(png)\n\t}\n\n\tfor chunkIter.next() {\n\t\tchunkType := chunkIter.chunkType()\n\t\tif bytes.Equal(chunkType, pngIendChunkType) {\n\t\t\teofOffset := chunkIter.nextChunkOffset()\n\t\t\tif eofOffset > len(png) {\n\t\t\t\teofOffset = len(png)\n\t\t\t}\n\t\t\treturn eofOffset\n\t\t}\n\t}\n\t// Didn't find IEND. File is malformed but let's continue anyway\n\treturn len(png)\n}\n\nfunc detectContentLengthJPEG(jpeg []byte) int {\n\t// check if this is maybe jpeg\n\tjpegPrefix := []byte{0xFF, 0xD8, 0xFF}\n\tif !bytes.HasPrefix(jpeg, jpegPrefix) {\n\t\t// Not jpeg if it doesn't begin with SOI\n\t\treturn len(jpeg)\n\t}\n\n\t// Iterate through jpeg segments\n\tidx := 0\n\tfor {\n\t\tif idx+1 >= len(jpeg) {\n\t\t\tbreak\n\t\t}\n\t\tif jpeg[idx] != 0xFF {\n\t\t\t// not valid jpeg\n\t\t\tbreak\n\t\t}\n\n\t\t// Segments are at least 2 bytes big\n\t\tnextSegmentStart := idx + 2\n\n\t\t// find current segment type\n\t\tsegmentType := jpeg[idx+1]\n\t\tif segmentType == jpegEOISegmentType {\n\t\t\t// EOI means the end of image content\n\t\t\treturn nextSegmentStart\n\t\t} else if segmentType == 0xFF {\n\t\t\t// Some handling for padding\n\t\t\tidx++\n\t\t\tcontinue\n\t\t}\n\n\t\tif _, isUnsized := jpegUnsizedSegmentTypes[segmentType]; isUnsized {\n\t\t\tidx = nextSegmentStart\n\t\t\tcontinue\n\t\t}\n\n\t\tif idx+3 >= len(jpeg) {\n\t\t\t// not enough data to continue\n\t\t\tbreak\n\t\t}\n\t\t// 2 bytes size includes itself\n\t\tnextSegmentStart += (int)(binary.BigEndian.Uint16(jpeg[idx+2:]))\n\n\t\tif segmentType == jpegSOSSegmentType {\n\t\t\t// start of scan means that ECS data follows\n\t\t\t// ECS data does not start with 0xFF marker\n\t\t\t// scan through ECS to find next segment which starts with 0xFF\n\t\t\tfor ; nextSegmentStart < len(jpeg); nextSegmentStart++ {\n\t\t\t\tif jpeg[nextSegmentStart] != 0xFF {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tif nextSegmentStart+1 >= len(jpeg) {\n\t\t\t\t\tnextSegmentStart = len(jpeg)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tpeek := jpeg[nextSegmentStart+1]\n\t\t\t\tif peek == 0xFF {\n\t\t\t\t\t// there can be padding bytes which are repeated 0xFF\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// 0 means this is a raw 0xFF in the ECS data\n\t\t\t\t// RST segment types are also a continuation of ECS data\n\t\t\t\tif peek != 0 && (peek < 0xD0 || peek > 0xD7) {\n\t\t\t\t\t// Reached the end of ECS!\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tidx = nextSegmentStart\n\t}\n\n\t// if we didn't find EOI, fallback to the full length\n\treturn len(jpeg)\n}\n\nfunc detectContentLength(img []byte) int {\n\t// both of these short circuit if the correct prefix isn't detected\n\t// so we can just call both with little cost for simpler code\n\tjpegLength := detectContentLengthJPEG(img)\n\tpngLength := detectContentLengthPNG(img)\n\tif jpegLength < pngLength {\n\t\treturn jpegLength\n\t}\n\treturn pngLength\n}\n\n// detectAPNG detects if a blob contains a PNG with animated segments\nfunc detectAPNG(maybeAPNG []byte) bool {\n\tchunkIter, err := makePngChunkIter(maybeAPNG)\n\tif err != nil {\n\t\t// This is not a png at all :)\n\t\treturn false\n\t}\n\n\tfor chunkIter.next() {\n\t\tchunkType := chunkIter.chunkType()\n\t\tif bytes.Equal(chunkType, pngActlChunkType) || bytes.Equal(chunkType, pngFctlChunkType) || bytes.Equal(chunkType, pngFdatChunkType) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (d *openCVDecoder) Header() (*ImageHeader, error) {\n\tif !d.hasReadHeader {\n\t\tif !C.opencv_decoder_read_header(d.decoder) {\n\t\t\treturn nil, ErrInvalidImage\n\t\t}\n\t}\n\n\td.hasReadHeader = true\n\n\tnumFrames := 1\n\tif detectAPNG(d.buf) {\n\t\tnumFrames = 2\n\t}\n\n\treturn &ImageHeader{\n\t\twidth:         int(C.opencv_decoder_get_width(d.decoder)),\n\t\theight:        int(C.opencv_decoder_get_height(d.decoder)),\n\t\tpixelType:     PixelType(C.opencv_decoder_get_pixel_type(d.decoder)),\n\t\torientation:   ImageOrientation(C.opencv_decoder_get_orientation(d.decoder)),\n\t\tnumFrames:     numFrames,\n\t\tcontentLength: detectContentLength(d.buf),\n\t}, nil\n}\n\nfunc (d *openCVDecoder) Close() {\n\tC.opencv_decoder_release(d.decoder)\n\tC.opencv_mat_release(d.mat)\n\td.buf = nil\n}\n\nfunc (d *openCVDecoder) Description() string {\n\treturn C.GoString(C.opencv_decoder_get_description(d.decoder))\n}\n\nfunc (d *openCVDecoder) IsStreamable() bool {\n\treturn true\n}\n\nfunc (d *openCVDecoder) BackgroundColor() uint32 {\n\treturn 0xFFFFFFFF\n}\n\nfunc (d *openCVDecoder) LoopCount() int {\n\treturn 0 // loop indefinitely\n}\n\nfunc (d *openCVDecoder) HasSubtitles() bool {\n\treturn false\n}\n\nfunc (d *openCVDecoder) ICC() []byte {\n\tswitch d.Description() {\n\tcase \"JPEG\":\n\t\treturn d.iccJPEG()\n\tcase \"PNG\":\n\t\treturn d.iccPNG()\n\t}\n\treturn []byte{}\n}\n\nfunc (d *openCVDecoder) iccJPEG() []byte {\n\ticcDst := make([]byte, 8192)\n\ticcLength := C.opencv_decoder_get_jpeg_icc(unsafe.Pointer(&d.buf[0]), C.size_t(len(d.buf)), unsafe.Pointer(&iccDst[0]), C.size_t(cap(iccDst)))\n\treturn iccDst[:iccLength]\n}\n\nfunc (d *openCVDecoder) iccPNG() []byte {\n\ticcDst := make([]byte, 8192)\n\ticcLength := C.opencv_decoder_get_png_icc(unsafe.Pointer(&d.buf[0]), C.size_t(len(d.buf)), unsafe.Pointer(&iccDst[0]), C.size_t(cap(iccDst)))\n\treturn iccDst[:iccLength]\n}\n\nfunc (d *openCVDecoder) Duration() time.Duration {\n\treturn time.Duration(0)\n}\n\nfunc (d *openCVDecoder) DecodeTo(f *Framebuffer) error {\n\tif d.hasDecoded {\n\t\treturn io.EOF\n\t}\n\th, err := d.Header()\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = f.resizeMat(h.Width(), h.Height(), h.PixelType())\n\tif err != nil {\n\t\treturn err\n\t}\n\tret := C.opencv_decoder_read_data(d.decoder, f.mat)\n\tif !ret {\n\t\treturn ErrDecodingFailed\n\t}\n\td.hasDecoded = true\n\tf.blend = NoBlend\n\tf.dispose = DisposeToBackgroundColor\n\tf.xOffset = 0\n\tf.yOffset = 0\n\tf.duration = time.Duration(0)\n\treturn nil\n}\n\nfunc (d *openCVDecoder) SkipFrame() error {\n\treturn ErrSkipNotSupported\n}\n\nfunc newOpenCVEncoder(ext string, decodedBy Decoder, dstBuf []byte) (*openCVEncoder, error) {\n\tdstBuf = dstBuf[:1]\n\tdst := C.opencv_mat_create_empty_from_data(C.int(cap(dstBuf)), unsafe.Pointer(&dstBuf[0]))\n\n\tif dst == nil {\n\t\treturn nil, ErrBufTooSmall\n\t}\n\n\tc_ext := C.CString(ext)\n\tdefer C.free(unsafe.Pointer(c_ext))\n\tenc := C.opencv_encoder_create(c_ext, dst)\n\tif enc == nil {\n\t\treturn nil, ErrInvalidImage\n\t}\n\n\treturn &openCVEncoder{\n\t\tencoder: enc,\n\t\tdst:     dst,\n\t\tdstBuf:  dstBuf,\n\t}, nil\n}\n\nfunc (e *openCVEncoder) Encode(f *Framebuffer, opt map[int]int) ([]byte, error) {\n\tif f == nil {\n\t\treturn nil, io.EOF\n\t}\n\tvar optList []C.int\n\tvar firstOpt *C.int\n\tfor k, v := range opt {\n\t\toptList = append(optList, C.int(k))\n\t\toptList = append(optList, C.int(v))\n\t}\n\tif len(optList) > 0 {\n\t\tfirstOpt = (*C.int)(unsafe.Pointer(&optList[0]))\n\t}\n\tif !C.opencv_encoder_write(e.encoder, f.mat, firstOpt, C.size_t(len(optList))) {\n\t\treturn nil, ErrInvalidImage\n\t}\n\n\tptrCheck := C.opencv_mat_get_data(e.dst)\n\tif ptrCheck != unsafe.Pointer(&e.dstBuf[0]) {\n\t\t// mat pointer got reallocated - the passed buf was too small to hold the image\n\t\t// XXX we should free? the mat here, probably want to recreate\n\t\treturn nil, ErrBufTooSmall\n\t}\n\n\tlength := int(C.opencv_mat_get_height(e.dst))\n\n\treturn e.dstBuf[:length], nil\n}\n\nfunc (e *openCVEncoder) Close() {\n\tC.opencv_encoder_release(e.encoder)\n\tC.opencv_mat_release(e.dst)\n}\n"
        },
        {
          "name": "opencv.hpp",
          "type": "blob",
          "size": 3.904296875,
          "content": "#ifndef LILLIPUT_OPENCV_HPP\n#define LILLIPUT_OPENCV_HPP\n\n#include <stdbool.h>\n#include <stddef.h>\n\n#include <opencv2/core/fast_math.hpp>\n#include <opencv2/core/core_c.h>\n#include <opencv2/imgproc/types_c.h>\n#include <opencv2/imgcodecs/imgcodecs_c.h>\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n// duplicated from opencv but without a namespace\ntypedef enum CVImageOrientation {\n    CV_IMAGE_ORIENTATION_TL = 1, ///< Horizontal (normal)\n    CV_IMAGE_ORIENTATION_TR = 2, ///< Mirrored horizontal\n    CV_IMAGE_ORIENTATION_BR = 3, ///< Rotate 180\n    CV_IMAGE_ORIENTATION_BL = 4, ///< Mirrored vertical\n    CV_IMAGE_ORIENTATION_LT = 5, ///< Mirrored horizontal & rotate 270 CW\n    CV_IMAGE_ORIENTATION_RT = 6, ///< Rotate 90 CW\n    CV_IMAGE_ORIENTATION_RB = 7, ///< Mirrored horizontal & rotate 90 CW\n    CV_IMAGE_ORIENTATION_LB = 8  ///< Rotate 270 CW\n} CVImageOrientation;\n\ntypedef void* opencv_mat;\ntypedef void* opencv_decoder;\ntypedef void* opencv_encoder;\n\nint opencv_type_depth(int type);\nint opencv_type_channels(int type);\nint opencv_type_convert_depth(int type, int depth);\n\nopencv_decoder opencv_decoder_create(const opencv_mat buf);\nconst char* opencv_decoder_get_description(const opencv_decoder d);\nvoid opencv_decoder_release(opencv_decoder d);\nbool opencv_decoder_set_source(opencv_decoder d, const opencv_mat buf);\nbool opencv_decoder_read_header(opencv_decoder d);\nint opencv_decoder_get_width(const opencv_decoder d);\nint opencv_decoder_get_height(const opencv_decoder d);\nint opencv_decoder_get_pixel_type(const opencv_decoder d);\nint opencv_decoder_get_orientation(const opencv_decoder d);\nbool opencv_decoder_read_data(opencv_decoder d, opencv_mat dst);\nint opencv_copy_to_region_with_alpha(opencv_mat src, opencv_mat dst, int xOffset, int yOffset, int width, int height);\nint opencv_copy_to_region(opencv_mat src, opencv_mat dst, int xOffset, int yOffset, int width, int height);\nvoid opencv_mat_set_color(opencv_mat, int red, int green, int blue, int alpha);\nvoid opencv_mat_reset(opencv_mat mat);\nint opencv_mat_clear_to_transparent(opencv_mat mat, int xOffset, int yOffset, int width, int height);\n\nopencv_mat opencv_mat_create(int width, int height, int type);\nopencv_mat opencv_mat_create_from_data(int width,\n                                       int height,\n                                       int type,\n                                       void* data,\n                                       size_t data_len);\nopencv_mat opencv_mat_create_empty_from_data(int length, void* data);\nbool opencv_mat_set_row_stride(opencv_mat mat, size_t stride);\nvoid opencv_mat_release(opencv_mat mat);\nvoid opencv_mat_resize(const opencv_mat src,\n                       opencv_mat dst,\n                       int width,\n                       int height,\n                       int interpolation);\nopencv_mat opencv_mat_crop(const opencv_mat src, int x, int y, int width, int height);\nvoid opencv_mat_orientation_transform(CVImageOrientation orientation, opencv_mat mat);\nint opencv_mat_get_width(const opencv_mat mat);\nint opencv_mat_get_height(const opencv_mat mat);\nvoid* opencv_mat_get_data(const opencv_mat mat);\n\nopencv_encoder opencv_encoder_create(const char* ext, opencv_mat dst);\nvoid opencv_encoder_release(opencv_encoder e);\nbool opencv_encoder_write(opencv_encoder e, const opencv_mat src, const int* opt, size_t opt_len);\nint opencv_decoder_get_jpeg_icc(void* src, size_t src_len, void* dest, size_t dest_len);\nint opencv_decoder_get_png_icc(void* src, size_t src_len, void* dest, size_t dest_len);\n\n// Error codes\n#define OPENCV_SUCCESS 0\n#define OPENCV_ERROR_INVALID_CHANNEL_COUNT 1\n#define OPENCV_ERROR_OUT_OF_BOUNDS 2\n#define OPENCV_ERROR_NULL_MATRIX 3\n#define OPENCV_ERROR_RESIZE_FAILED 4\n#define OPENCV_ERROR_COPY_FAILED 5\n#define OPENCV_ERROR_CONVERSION_FAILED 6\n#define OPENCV_ERROR_ALPHA_BLENDING_FAILED 7\n#define OPENCV_ERROR_FINAL_CONVERSION_FAILED 8\n#define OPENCV_ERROR_INVALID_DIMENSIONS 9\n#define OPENCV_ERROR_UNKNOWN 10\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif\n"
        },
        {
          "name": "opencv_test.go",
          "type": "blob",
          "size": 7.42578125,
          "content": "package lilliput\n\nimport (\n\t\"bytes\"\n\t\"io/ioutil\"\n\t\"testing\"\n)\n\nfunc TestAPNG(t *testing.T) {\n\tpngNoMagic := []byte{\n\t\t0, 0, 0, 0, // size\n\t\tbyte('I'), byte('H'), byte('D'), byte('R'), // type\n\t\t0, 0, 0, 0, // crc\n\t}\n\tpng := append(pngMagic[:], pngNoMagic...)\n\n\tif detectAPNG(png) {\n\t\tt.Fatalf(`Incorrectly detected APNG in %v`, png)\n\t}\n\n\tapngChunks := [][]byte{\n\t\t{0x61, 0x63, 0x54, 0x4c}, // acTL\n\t\t{0x66, 0x63, 0x54, 0x4c}, // fcTL\n\t\t{0x66, 0x64, 0x41, 0x54}, // fdAT\n\t}\n\tfor i, chunk := range apngChunks {\n\t\tapng := append(png, 0, 0, 0, 0) // size\n\t\tapng = append(apng, chunk...)   // type\n\t\tapng = append(apng, 0, 0, 0, 0) // crc\n\t\tif !detectAPNG(apng) {\n\t\t\tt.Fatalf(`Failed to detect APNG at idx %d in %v`, i, apng)\n\t\t}\n\t}\n}\n\nfunc TestContentLength_PNG_ExtraData(t *testing.T) {\n\tpngNoMagic := []byte{\n\t\t0, 0, 0, 0, // size\n\t\tbyte('I'), byte('H'), byte('D'), byte('R'), // type\n\t\t0, 0, 0, 0, // crc\n\t\t0, 0, 0, 4, // size\n\t\t1, 2, 3, 4, // type (not real)\n\t\t8, 9, 8, 9, // data\n\t\t0, 0, 0, 0, // crc\n\t\t0, 0, 0, 0, // size\n\t\t7, 7, 7, 7, // type\n\t\t0, 0, 0, 0, // crc\n\t}\n\tpng := append(pngMagic[:], pngNoMagic...)\n\n\tend := detectContentLength(png)\n\tif end != len(png) {\n\t\tt.Fatalf(`end = \"%d\", expected \"%d\"`, end, len(png))\n\t}\n\n\tpng = append(png, 56, 56)\n\tend = detectContentLength(png)\n\tif end != len(png) {\n\t\tt.Fatalf(`end = \"%d\", expected \"%d\"`, end, len(png))\n\t}\n}\n\nfunc TestContentLength_PNG_IEND(t *testing.T) {\n\tpngNoMagic := []byte{\n\t\t0, 0, 0, 0, // size\n\t\tbyte('I'), byte('H'), byte('D'), byte('R'), // type\n\t\t0, 0, 0, 0, // crc\n\t\t0, 0, 0, 0, // size\n\t\tbyte('I'), byte('E'), byte('N'), byte('D'), // type\n\t\t0, 0, 0, 0, // crc\n\t}\n\tpngExtraData := []byte{\n\t\t0, 0, 0, 4, // size\n\t\t1, 2, 3, 4, // type\n\t\t8, 9, 8, 9, // data\n\t\t0, 0, 0, 0, // crc\n\t}\n\tpng := append(pngMagic[:], pngNoMagic...)\n\texpectedLength := len(png)\n\tpng = append(png, pngExtraData...)\n\n\tend := detectContentLength(png)\n\tif end != expectedLength {\n\t\tt.Fatalf(`end = \"%d\", expected \"%d\"`, end, expectedLength)\n\t}\n}\n\nfunc TestContentLength_JPEG_ExtraData(t *testing.T) {\n\tjpeg := []byte{\n\t\t0xFF, 0xD8, // SOI\n\t\t0xFF, 0xE7, 0x00, 0x04, 0xFF, 0xD9, // made up segment\n\t\t0xFF, 0xDA, 0x00, 0x04, 0x00, 0x00, // SOS\n\t\t0x00, 0x01, 0xD9, 0xFF, 0xD5, 0xD5, // ECS data\n\t\t0xFF, 0xD9, // EOI\n\t}\n\tresult := detectContentLength(jpeg)\n\tif result != len(jpeg) {\n\t\tt.Fatalf(`Expected jpeg content length %d, got %d`, len(jpeg), result)\n\t}\n\n\textraStuff := []byte{0xFF, 0xC2, 0x00, 0x02}\n\tjpeg = append(jpeg, extraStuff...)\n\n\tresult = detectContentLength(jpeg)\n\tif result != len(jpeg)-len(extraStuff) {\n\t\tt.Fatalf(`Expected jpeg content length %d, got %d`, len(jpeg)-len(extraStuff), result)\n\t}\n}\n\nfunc TestContentLength_JPEG_EntropyCoding(t *testing.T) {\n\tjpeg := []byte{\n\t\t0xFF, 0xD8, // SOI\n\t\t0xFF, 0xE7, 0x00, 0x04, 0xFF, 0xD9, // made up data\n\t\t0xFF, 0xDA, 0x00, 0x02, // SOS\n\t\t0x02, 0x01, 0xFF, 0x00, 0xD9, // ECS data\n\t\t0xFF, 0xFF, // padding\n\t\t0xFF, 0xD9, // EOI\n\t\t0x01, // extra\n\t}\n\tresult := detectContentLength(jpeg)\n\tif result != len(jpeg)-1 {\n\t\tt.Fatalf(`Expected jpeg content length %d, got %d`, len(jpeg)-1, result)\n\t}\n}\n\nfunc TestContentLength_Unrecognized(t *testing.T) {\n\tdata := make([]byte, 128)\n\tresult := detectContentLength(data)\n\tif result != len(data) {\n\t\tt.Fatalf(`Expected data content length %d, got %d`, len(data), result)\n\t}\n}\n\nfunc expectChunks(t *testing.T, png []byte, chunks [][]byte) {\n\tchunkIter, err := makePngChunkIter(png)\n\tif err != nil {\n\t\tt.Fatalf(`makePngChunkIter failed with error %v`, err)\n\t}\n\n\tchunkIdx := 0\n\tfor chunkIter.next() {\n\t\tif chunkIdx >= len(chunks) {\n\t\t\tt.Fatalf(`Found %d chunks, expected only %d`, chunkIdx+1, len(chunks))\n\t\t}\n\t\tif !bytes.Equal(chunkIter.chunkType(), chunks[chunkIdx]) {\n\t\t\tt.Fatalf(`chunkType = \"%v\", expected \"%v\"`, chunkIter.chunkType(), chunks[chunkIdx])\n\t\t}\n\t\tchunkIdx++\n\t}\n\tif chunkIdx < len(chunks) {\n\t\tt.Fatalf(`Found %d chunks, expected %d`, chunkIdx, len(chunks))\n\t}\n}\n\nfunc TestPNGWalk_ExtraData(t *testing.T) {\n\tpngNoMagic := []byte{\n\t\t0, 0, 0, 0, // size\n\t\tbyte('I'), byte('H'), byte('D'), byte('R'), // type\n\t\t0, 0, 0, 0, // crc\n\t\t0, 0, 0, 4, // size\n\t\t1, 2, 3, 4, // type (not real)\n\t\t8, 9, 8, 9, // data\n\t\t0, 0, 0, 0, // crc\n\t}\n\tpng := append(pngMagic[:], pngNoMagic...)\n\n\tchunkTypes := [][]byte{\n\t\t{byte('I'), byte('H'), byte('D'), byte('R')},\n\t\t{1, 2, 3, 4},\n\t}\n\n\t// min chunk size is 12, try extra data up to that amount\n\tfor i := 0; i < 11; i++ {\n\t\tpng = append(png, 0)\n\t\texpectChunks(t, png, chunkTypes)\n\t}\n}\n\nfunc TestPNGWalk_BadSize(t *testing.T) {\n\tpngNoMagic := []byte{\n\t\t0, 0, 0, 0, // size\n\t\tbyte('I'), byte('H'), byte('D'), byte('R'), // type\n\t\t0, 0, 0, 0, // crc\n\t\t0, 128, 0, 4, // size (massive)\n\t\t1, 2, 3, 4, // type (not real)\n\t\t8, 9, 8, 9, // data\n\t\t0, 0, 0, 0, // crc\n\t}\n\tpng := append(pngMagic[:], pngNoMagic...)\n\n\tchunkTypes := [][]byte{\n\t\t{byte('I'), byte('H'), byte('D'), byte('R')},\n\t\t{1, 2, 3, 4},\n\t}\n\texpectChunks(t, png, chunkTypes)\n}\n\nfunc TestPNGWalk_NotPNG(t *testing.T) {\n\tpngNoMagic := []byte{\n\t\t0, 0, 0, 0, // size\n\t\tbyte('I'), byte('H'), byte('D'), byte('R'), // type\n\t\t0, 0, 0, 0, // crc\n\t}\n\n\t_, err := makePngChunkIter(pngNoMagic)\n\tif err == nil {\n\t\tt.Fatalf(`Expected makePngChunkIter to fail, but it did not`)\n\t}\n}\n\nfunc TestPNGWalk_NoChunks(t *testing.T) {\n\tpngNoMagic := []byte{}\n\tpng := append(pngMagic[:], pngNoMagic...)\n\n\t// min chunk size is 12, try extra data up to that amount\n\tfor i := 0; i < 12; i++ {\n\t\texpectChunks(t, png, [][]byte{})\n\t\tpng = append(png, 0)\n\t}\n}\n\nfunc TestICC(t *testing.T) {\n\ttests := []struct {\n\t\tname     string\n\t\tfilePath string\n\t\twantICC  bool\n\t}{\n\t\t{name: \"JPEG with ICC Profile\", filePath: \"testdata/ferry_sunset.jpg\", wantICC: true},\n\t\t{name: \"JPEG without ICC Profile\", filePath: \"testdata/ferry_sunset_no_icc.jpg\", wantICC: false},\n\t\t{name: \"PNG with ICC Profile\", filePath: \"testdata/ferry_sunset.png\", wantICC: true},\n\t\t{name: \"PNG without ICC Profile\", filePath: \"testdata/ferry_sunset_no_icc.png\", wantICC: false},\n\t}\n\n\tfor _, tc := range tests {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\timgData, err := ioutil.ReadFile(tc.filePath)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to read image file: %v\", err)\n\t\t\t}\n\t\t\tif len(imgData) == 0 {\n\t\t\t\tt.Fatalf(\"Failed to read image file\")\n\t\t\t}\n\n\t\t\tdecoder, err := newOpenCVDecoder(imgData)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to create decoder: %v\", err)\n\t\t\t}\n\n\t\t\theader, err := decoder.Header()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to get the header: %v\", err)\n\t\t\t}\n\n\t\t\tframebuffer := NewFramebuffer(header.width, header.height)\n\t\t\tif err = decoder.DecodeTo(framebuffer); err != nil {\n\t\t\t\tt.Errorf(\"DecodeTo failed unexpectedly: %v\", err)\n\t\t\t}\n\n\t\t\ticcData := decoder.ICC()\n\t\t\tif tc.wantICC {\n\t\t\t\tif len(iccData) == 0 {\n\t\t\t\t\tt.Fatalf(\"Failed to extract ICC profile from the image\")\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif len(iccData) > 0 {\n\t\t\t\t\tt.Fatalf(\"Extracted ICC profile from the image, but it should not be present\")\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// try encoding a WebP image, including ICC profile data when available\n\t\t\tdstBuf := make([]byte, destinationBufferSize)\n\t\t\tencoder, err := newWebpEncoder(decoder, dstBuf)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to create a new webp encoder: %v\", err)\n\t\t\t}\n\n\t\t\toptions := map[int]int{}\n\t\t\tencodedData, err := encoder.Encode(framebuffer, options)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Encode failed unexpectedly: %v\", err)\n\t\t\t}\n\t\t\tif encodedData, err = encoder.Encode(nil, options); err != nil {\n\t\t\t\tt.Fatalf(\"Encode of empty frame failed unexpectedly: %v\", err)\n\t\t\t}\n\t\t\tif len(encodedData) == 0 {\n\t\t\t\tt.Fatalf(\"Encoded data is empty, but it should not be\")\n\t\t\t}\n\n\t\t\tdecoder.Close()\n\t\t\tencoder.Close()\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "ops.go",
          "type": "blob",
          "size": 14.2802734375,
          "content": "package lilliput\n\nimport (\n\t\"fmt\"\n\t\"image\"\n\t\"io\"\n\t\"time\"\n)\n\ntype ImageOpsSizeMethod int\n\nconst (\n\tImageOpsNoResize ImageOpsSizeMethod = iota\n\tImageOpsFit\n\tImageOpsResize\n)\n\n// ImageOptions controls how ImageOps resizes and encodes the\n// pixel data decoded from a Decoder\ntype ImageOptions struct {\n\t// FileType should be a string starting with '.', e.g.\n\t// \".jpeg\"\n\tFileType string\n\n\t// Width controls the width of the output image\n\tWidth int\n\n\t// Height controls the height of the output image\n\tHeight int\n\n\t// ResizeMethod controls how the image will be transformed to\n\t// its output size. Notably, ImageOpsFit will do a cropping\n\t// resize, while ImageOpsResize will stretch the image.\n\tResizeMethod ImageOpsSizeMethod\n\n\t// NormalizeOrientation will flip and rotate the image as necessary\n\t// in order to undo EXIF-based orientation\n\tNormalizeOrientation bool\n\n\t// EncodeOptions controls the encode quality options\n\tEncodeOptions map[int]int\n\n\t// MaxEncodeFrames controls the maximum number of frames that will be resized\n\tMaxEncodeFrames int\n\n\t// MaxEncodeDuration controls the maximum duration of animated image that will be resized\n\tMaxEncodeDuration time.Duration\n\n\t// This is a best effort timeout when encoding multiple frames\n\tEncodeTimeout time.Duration\n\n\t// DisableAnimatedOutput controls the encoder behavior when given a multi-frame input\n\tDisableAnimatedOutput bool\n}\n\n// ImageOps is a reusable object that can resize and encode images.\ntype ImageOps struct {\n\tframes                  []*Framebuffer\n\tframeIndex              int\n\tanimatedCompositeBuffer *Framebuffer\n}\n\n// NewImageOps creates a new ImageOps object that will operate\n// on images up to maxSize on each axis. It initializes two framebuffers\n// for double-buffering operations.\nfunc NewImageOps(maxSize int) *ImageOps {\n\tframes := make([]*Framebuffer, 2)\n\tframes[0] = NewFramebuffer(maxSize, maxSize)\n\tframes[1] = NewFramebuffer(maxSize, maxSize)\n\treturn &ImageOps{\n\t\tframes:     frames,\n\t\tframeIndex: 0,\n\t}\n}\n\n// active returns the currently active framebuffer used for operations\nfunc (o *ImageOps) active() *Framebuffer {\n\treturn o.frames[o.frameIndex]\n}\n\n// secondary returns the secondary framebuffer used for double-buffering operations\nfunc (o *ImageOps) secondary() *Framebuffer {\n\treturn o.frames[1-o.frameIndex]\n}\n\n// swap toggles between the active and secondary framebuffers\nfunc (o *ImageOps) swap() {\n\to.frameIndex = 1 - o.frameIndex\n}\n\n// Clear frees the pixel data held in all framebuffers. While not required between\n// Transform operations, you can call this to reduce memory usage when the ImageOps\n// object will be idle for a while.\nfunc (o *ImageOps) Clear() {\n\to.frames[0].Clear()\n\to.frames[1].Clear()\n\tif o.animatedCompositeBuffer != nil {\n\t\to.animatedCompositeBuffer.Clear()\n\t}\n}\n\n// Close releases resources associated with ImageOps\nfunc (o *ImageOps) Close() {\n\to.frames[0].Close()\n\to.frames[1].Close()\n\tif o.animatedCompositeBuffer != nil {\n\t\to.animatedCompositeBuffer.Close()\n\t\to.animatedCompositeBuffer = nil\n\t}\n}\n\n// setupAnimatedFrameBuffers initializes the composite buffer needed for animated image processing.\n// It creates a buffer with the appropriate number of channels based on whether the image has alpha.\n// Returns an error if buffer creation fails.\nfunc (o *ImageOps) setupAnimatedFrameBuffers(d Decoder, inputCanvasWidth, inputCanvasHeight int, hasAlpha bool) error {\n\t// Create a buffer to hold the composite of the current frame and the previous frame\n\tif o.animatedCompositeBuffer == nil {\n\t\to.animatedCompositeBuffer = NewFramebuffer(inputCanvasWidth, inputCanvasHeight)\n\t\tif !hasAlpha {\n\t\t\tif err := o.animatedCompositeBuffer.Create3Channel(inputCanvasWidth, inputCanvasHeight); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tif err := o.animatedCompositeBuffer.Create4Channel(inputCanvasWidth, inputCanvasHeight); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\trect := image.Rect(0, 0, inputCanvasWidth, inputCanvasHeight)\n\t\treturn o.animatedCompositeBuffer.ClearToTransparent(rect)\n\t}\n\n\treturn nil\n}\n\n// decode reads the current frame from the decoder into the active framebuffer.\n// Returns an error if decoding fails.\nfunc (o *ImageOps) decode(d Decoder) error {\n\tactive := o.active()\n\treturn d.DecodeTo(active)\n}\n\n// fit resizes the active frame to fit within the specified dimensions while maintaining aspect ratio.\n// For animated images, it handles frame compositing and disposal.\n// Returns (true, nil) if resizing was performed successfully, (false, error) if an error occurred.\nfunc (o *ImageOps) fit(d Decoder, inputCanvasWidth, inputCanvasHeight, outputCanvasWidth, outputCanvasHeight int, isAnimated, hasAlpha bool) (bool, error) {\n\tnewWidth, newHeight := calculateExpectedSize(inputCanvasWidth, inputCanvasHeight, outputCanvasWidth, outputCanvasHeight)\n\n\tif isAnimated {\n\t\tif err := o.setupAnimatedFrameBuffers(d, inputCanvasWidth, inputCanvasHeight, hasAlpha); err != nil {\n\t\t\treturn false, err\n\t\t}\n\n\t\t// blend transparent pixels of the active frame with corresponding pixels of the previous canvas, creating a composite\n\t\tif err := o.applyBlendMethod(d); err != nil {\n\t\t\treturn false, err\n\t\t}\n\n\t\t// resize the composite to the output canvas size\n\t\tif err := o.animatedCompositeBuffer.Fit(newWidth, newHeight, o.secondary()); err != nil {\n\t\t\treturn false, err\n\t\t}\n\n\t\t// apply dispose method of the active frame to the composite\n\t\tif err := o.applyDisposeMethod(d); err != nil {\n\t\t\treturn false, err\n\t\t}\n\n\t\to.copyFramePropertiesAndSwap()\n\t\treturn true, nil\n\t}\n\n\t// If the image is not animated, we can fit it directly.\n\tif err := o.active().Fit(newWidth, newHeight, o.secondary()); err != nil {\n\t\treturn false, err\n\t}\n\to.copyFramePropertiesAndSwap()\n\treturn true, nil\n}\n\n// resize scales the active frame to exactly match the specified dimensions.\n// For animated images, it handles frame compositing and disposal.\n// Returns (true, nil) if resizing was performed successfully, (false, error) if an error occurred.\nfunc (o *ImageOps) resize(d Decoder, inputCanvasWidth, inputCanvasHeight, outputCanvasWidth, outputCanvasHeight, frameCount int, isAnimated, hasAlpha bool) (bool, error) {\n\t// If the image is animated, we need to resize the frame to the input canvas size\n\t// and then copy the previous frame's data to the working buffer.\n\tif isAnimated {\n\t\tif err := o.setupAnimatedFrameBuffers(d, inputCanvasWidth, inputCanvasHeight, hasAlpha); err != nil {\n\t\t\treturn false, err\n\t\t}\n\n\t\tif err := o.applyBlendMethod(d); err != nil {\n\t\t\treturn false, err\n\t\t}\n\n\t\tif err := o.animatedCompositeBuffer.ResizeTo(outputCanvasWidth, outputCanvasHeight, o.secondary()); err != nil {\n\t\t\treturn false, err\n\t\t}\n\n\t\tif err := o.applyDisposeMethod(d); err != nil {\n\t\t\treturn false, err\n\t\t}\n\n\t\to.copyFramePropertiesAndSwap()\n\t\treturn true, nil\n\t}\n\n\tif err := o.active().ResizeTo(outputCanvasWidth, outputCanvasHeight, o.secondary()); err != nil {\n\t\treturn false, err\n\t}\n\to.copyFramePropertiesAndSwap()\n\n\treturn true, nil\n}\n\n// calculateExpectedSize determines the final dimensions for an image based on\n// original and requested sizes, handling special cases for square resizing\n// and oversized requests.\nfunc calculateExpectedSize(origWidth, origHeight, reqWidth, reqHeight int) (int, int) {\n\tif reqWidth == reqHeight && reqWidth > min(origWidth, origHeight) {\n\t\t// Square resize request larger than smaller original dimension\n\t\tminDim := min(origWidth, origHeight)\n\t\treturn minDim, minDim\n\t} else if reqWidth > origWidth && reqHeight > origHeight && reqWidth != reqHeight {\n\t\t// Both dimensions larger than original and not square\n\t\treturn origWidth, origHeight\n\t} else {\n\t\t// All other cases\n\t\treturn reqWidth, reqHeight\n\t}\n}\n\n// min returns the smaller of two integers\nfunc min(a, b int) int {\n\tif a < b {\n\t\treturn a\n\t}\n\treturn b\n}\n\n// normalizeOrientation applies EXIF orientation corrections to the active frame\n// by performing the necessary flips and rotations.\nfunc (o *ImageOps) normalizeOrientation(orientation ImageOrientation) {\n\tactive := o.active()\n\tactive.OrientationTransform(orientation)\n}\n\n// encode writes the active frame to an encoded format using the provided encoder\n// and encoding options. Returns the encoded bytes or an error.\nfunc (o *ImageOps) encode(e Encoder, opt map[int]int) ([]byte, error) {\n\tactive := o.active()\n\treturn e.Encode(active, opt)\n}\n\n// encodeEmpty signals the encoder to finalize the encoding process without\n// additional frame data. Used for handling animation termination.\nfunc (o *ImageOps) encodeEmpty(e Encoder, opt map[int]int) ([]byte, error) {\n\treturn e.Encode(nil, opt)\n}\n\n// skipToEnd advances the decoder to the final frame of an animation.\n// Returns io.EOF when the end is reached or an error if seeking fails.\nfunc (o *ImageOps) skipToEnd(d Decoder) error {\n\tvar err error\n\tfor {\n\t\terr = d.SkipFrame()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n}\n\n// Transform performs the requested transform operations on the Decoder specified by d.\n// The result is written into the output buffer dst. A new slice pointing to dst is returned\n// with its length set to the length of the resulting image. Errors may occur if the decoded\n// image is too large for ImageOps or if Encoding fails.\n//\n// It is important that .Decode() not have been called already on d.\nfunc (o *ImageOps) Transform(d Decoder, opt *ImageOptions, dst []byte) ([]byte, error) {\n\tdefer func() {\n\t\tif o.animatedCompositeBuffer != nil {\n\t\t\to.animatedCompositeBuffer.Close()\n\t\t\to.animatedCompositeBuffer = nil\n\t\t}\n\t}()\n\n\tinputHeader, enc, err := o.initializeTransform(d, opt, dst)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer enc.Close()\n\n\tframeCount := 0\n\tduration := time.Duration(0)\n\tencodeTimeoutTime := time.Now().Add(opt.EncodeTimeout)\n\n\t// transform the frames and encode them until we run out of frames or the timeout is reached\n\tfor {\n\t\terr = o.decode(d)\n\t\temptyFrame := false\n\t\tif err != nil {\n\t\t\tif err != io.EOF {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\t// io.EOF means we are out of frames, so we should signal to encoder to wrap up\n\t\t\temptyFrame = true\n\t\t}\n\n\t\tduration += o.active().Duration()\n\n\t\tif opt.MaxEncodeDuration != 0 && duration > opt.MaxEncodeDuration {\n\t\t\terr = o.skipToEnd(d)\n\t\t\tif err != io.EOF {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn o.encodeEmpty(enc, opt.EncodeOptions)\n\t\t}\n\n\t\to.normalizeOrientation(inputHeader.Orientation())\n\n\t\t// transform the frame, resizing if necessary\n\t\tvar swapped bool\n\t\tif !emptyFrame {\n\t\t\tswapped, err = o.transformCurrentFrame(d, opt, inputHeader, frameCount)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\n\t\t// encode the frame to the output buffer\n\t\tvar content []byte\n\t\tif emptyFrame {\n\t\t\tcontent, err = o.encodeEmpty(enc, opt.EncodeOptions)\n\t\t} else {\n\t\t\tcontent, err = o.encode(enc, opt.EncodeOptions)\n\t\t}\n\n\t\t// content == nil and err == nil -- this is encoder telling us to do another frame\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif content != nil {\n\t\t\treturn content, nil\n\t\t}\n\n\t\tframeCount++\n\n\t\t// break out if we're creating a single frame and we've already done one\n\t\tif opt.DisableAnimatedOutput {\n\t\t\treturn o.encodeEmpty(enc, opt.EncodeOptions)\n\t\t}\n\n\t\tif opt.MaxEncodeFrames != 0 && frameCount == opt.MaxEncodeFrames {\n\t\t\terr = o.skipToEnd(d)\n\t\t\tif err != io.EOF {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn o.encodeEmpty(enc, opt.EncodeOptions)\n\t\t}\n\n\t\tif time.Now().After(encodeTimeoutTime) {\n\t\t\treturn nil, ErrEncodeTimeout\n\t\t}\n\n\t\t// for mulitple frames/gifs we need the decoded frame to be active again\n\t\tif swapped {\n\t\t\to.swap()\n\t\t}\n\t}\n}\n\n// transformCurrentFrame applies the requested resize operation to the current frame.\n// Handles both static and animated images, managing frame compositing when needed.\n// Returns (true, nil) if transformation was performed, (false, error) if an error occurred.\nfunc (o *ImageOps) transformCurrentFrame(d Decoder, opt *ImageOptions, inputHeader *ImageHeader, frameCount int) (bool, error) {\n\tif opt.ResizeMethod == ImageOpsNoResize && !inputHeader.IsAnimated() {\n\t\treturn false, nil\n\t}\n\n\toutputWidth, outputHeight := opt.Width, opt.Height\n\tif opt.ResizeMethod == ImageOpsNoResize {\n\t\toutputWidth, outputHeight = inputHeader.Width(), inputHeader.Height()\n\t}\n\n\tswitch opt.ResizeMethod {\n\tcase ImageOpsFit, ImageOpsNoResize:\n\t\treturn o.fit(d, inputHeader.Width(), inputHeader.Height(), outputWidth, outputHeight, inputHeader.IsAnimated(), inputHeader.HasAlpha())\n\tcase ImageOpsResize:\n\t\treturn o.resize(d, inputHeader.Width(), inputHeader.Height(), outputWidth, outputHeight, frameCount, inputHeader.IsAnimated(), inputHeader.HasAlpha())\n\tdefault:\n\t\treturn false, fmt.Errorf(\"unknown resize method: %v\", opt.ResizeMethod)\n\t}\n}\n\n// initializeTransform prepares for image transformation by reading the input header\n// and creating an appropriate encoder. Returns the header, encoder, and any error.\nfunc (o *ImageOps) initializeTransform(d Decoder, opt *ImageOptions, dst []byte) (*ImageHeader, Encoder, error) {\n\tinputHeader, err := d.Header()\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tenc, err := NewEncoder(opt.FileType, d, dst)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn inputHeader, enc, nil\n}\n\n// applyDisposeMethod handles frame disposal according to the active frame's\n// dispose method in animated images. For frames marked with DisposeToBackgroundColor,\n// it clears the affected region to transparent. For NoDispose, the previous frame's\n// content is preserved.\nfunc (o *ImageOps) applyDisposeMethod(d Decoder) error {\n\tactive := o.active()\n\tswitch active.dispose {\n\tcase DisposeToBackgroundColor:\n\t\trect := image.Rect(active.xOffset, active.yOffset, active.xOffset+active.Width(), active.yOffset+active.Height())\n\t\treturn o.animatedCompositeBuffer.ClearToTransparent(rect)\n\tcase NoDispose:\n\t\t// Do nothing\n\t}\n\treturn nil\n}\n\n// applyBlendMethod composites the active frame onto the animation buffer using\n// the specified blending mode (alpha blending or direct copy).\nfunc (o *ImageOps) applyBlendMethod(d Decoder) error {\n\tactive := o.active()\n\trect := image.Rect(\n\t\tactive.xOffset,\n\t\tactive.yOffset,\n\t\tactive.xOffset+active.Width(),\n\t\tactive.yOffset+active.Height(),\n\t)\n\n\tswitch active.blend {\n\tcase UseAlphaBlending:\n\t\treturn o.animatedCompositeBuffer.CopyToOffsetWithAlphaBlending(active, rect)\n\tcase NoBlend:\n\t\treturn o.animatedCompositeBuffer.CopyToOffsetNoBlend(active, rect)\n\t}\n\treturn nil\n}\n\n// copyFramePropertiesAndSwap transfers animation metadata (duration, disposal method,\n// and blend mode) from the active frame to the secondary frame, then swaps buffers.\nfunc (o *ImageOps) copyFramePropertiesAndSwap() {\n\to.secondary().duration = o.active().duration\n\to.secondary().dispose = o.active().dispose\n\to.secondary().blend = o.active().blend\n\to.swap()\n}\n"
        },
        {
          "name": "package.go",
          "type": "blob",
          "size": 0.0556640625,
          "content": "package lilliput // import \"github.com/discord/lilliput\"\n"
        },
        {
          "name": "testdata",
          "type": "tree",
          "content": null
        },
        {
          "name": "third-party-licenses",
          "type": "tree",
          "content": null
        },
        {
          "name": "thumbhash.cpp",
          "type": "blob",
          "size": 9.6474609375,
          "content": "#include \"thumbhash.hpp\"\n#include <stdbool.h>\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <tuple>\n\nstatic constexpr size_t MAX_DIMENSION = 100;\nstatic constexpr float PI = 3.14159265f;\n\nstruct thumbhash_encoder_struct {\n    uint8_t* dst;\n    size_t dst_len;\n};\n\nthumbhash_encoder thumbhash_encoder_create(void* buf, size_t buf_len)\n{\n    thumbhash_encoder e = new struct thumbhash_encoder_struct();\n    if (!e) {\n        return NULL;\n    }\n    memset(e, 0, sizeof(struct thumbhash_encoder_struct));\n    e->dst = (uint8_t*)(buf);\n    e->dst_len = buf_len;\n\n    return e;\n}\n\nstatic std::tuple<float, std::vector<float>, float> encode_channel(\n  const std::vector<float>& channel,\n  size_t nx,\n  size_t ny,\n  size_t w,\n  size_t h)\n{\n    float dc = 0.0f;\n    std::vector<float> ac;\n    ac.reserve(nx * ny / 2);\n    float scale = 0.0f;\n    std::vector<float> fx(w, 0.0f);\n    for (size_t cy = 0; cy < ny; ++cy) {\n        size_t cx = 0;\n        while (cx * ny < nx * (ny - cy)) {\n            float f = 0.0f;\n            for (size_t x = 0; x < w; ++x) {\n                fx[x] = cos(PI / static_cast<float>(w) * static_cast<float>(cx) *\n                            (static_cast<float>(x) + 0.5f));\n            }\n            for (size_t y = 0; y < h; ++y) {\n                float fy = cos(PI / static_cast<float>(h) * static_cast<float>(cy) *\n                               (static_cast<float>(y) + 0.5f));\n                for (size_t x = 0; x < w; ++x) {\n                    f += channel[x + y * w] * fx[x] * fy;\n                }\n            }\n            f /= static_cast<float>(w * h);\n            if (cx > 0 || cy > 0) {\n                ac.push_back(f);\n                scale = std::max(std::abs(f), scale);\n            }\n            else {\n                dc = f;\n            }\n            cx += 1;\n        }\n    }\n    if (scale > 0.0) {\n        for (auto& ac_val : ac) {\n            ac_val = 0.5f + 0.5f / scale * ac_val;\n        }\n    }\n    return std::make_tuple(dc, ac, scale);\n}\n\n// This C++ thumbhash encode function is based on the rust reference\n// implementation found here:\n//\n// https://github.com/evanw/thumbhash/blob/main/rust/src/lib.rs\n//\n// We modified the logic in the following ways:\n//\n// - Make it work with OpenCV mat as input frame\n// - Handle images with or without an alpha channel\n// - Handle grayscale images\n// - Perform simple downscaling of large images. We don't need very many pixels\n//   to get a good hash.\nint thumbhash_encoder_encode(thumbhash_encoder e, const opencv_mat opaque_frame)\n{\n    auto frame = static_cast<const cv::Mat*>(opaque_frame);\n\n    size_t orig_w = frame->cols;\n    size_t orig_h = frame->rows;\n    size_t w = orig_w, h = orig_h;\n\n    // We don't need very many pixels to get a good hash. Downsample the image\n    // when its dimensions exceed the limit.\n    if (orig_w > MAX_DIMENSION || orig_h > MAX_DIMENSION) {\n        float aspect_ratio = static_cast<float>(orig_w) / orig_h;\n        if (orig_w > orig_h) {\n            w = MAX_DIMENSION;\n            h = static_cast<size_t>(w / aspect_ratio);\n        }\n        else {\n            h = MAX_DIMENSION;\n            w = static_cast<size_t>(h * aspect_ratio);\n        }\n    }\n\n    float row_ratio = static_cast<float>(orig_h) / h;\n    float col_ratio = static_cast<float>(orig_w) / w;\n\n    bool has_alpha = false;\n    std::vector<float> l, p, q, a;\n    l.reserve(w * h);\n    p.reserve(w * h);\n    q.reserve(w * h);\n    a.reserve(w * h);\n\n    if (frame->type() == CV_8UC4) {\n        float avg_r = 0.0;\n        float avg_g = 0.0;\n        float avg_b = 0.0;\n        float avg_a = 0.0;\n\n        // 4 channels (BGRA)\n        for (int i = 0; i < h; ++i) {\n            for (int j = 0; j < w; ++j) {\n                size_t orig_i = static_cast<size_t>(i * row_ratio);\n                size_t orig_j = static_cast<size_t>(j * col_ratio);\n                const cv::Vec4b& pixel = frame->at<cv::Vec4b>(orig_i, orig_j);\n                float alpha = static_cast<float>(pixel[3]) / 255.0f;      // A\n                avg_b += (alpha / 255.0f) * static_cast<float>(pixel[0]); // B\n                avg_g += (alpha / 255.0f) * static_cast<float>(pixel[1]); // G\n                avg_r += (alpha / 255.0f) * static_cast<float>(pixel[2]); // R\n                avg_a += alpha;\n            }\n        }\n        if (avg_a > 0.0f) {\n            avg_r /= avg_a;\n            avg_g /= avg_a;\n            avg_b /= avg_a;\n        }\n        has_alpha = avg_a < static_cast<float>(w * h);\n\n        for (int i = 0; i < h; ++i) {\n            for (int j = 0; j < w; ++j) {\n                size_t orig_i = static_cast<size_t>(i * row_ratio);\n                size_t orig_j = static_cast<size_t>(j * col_ratio);\n                const cv::Vec4b& pixel = frame->at<cv::Vec4b>(orig_i, orig_j);\n                float alpha = static_cast<float>(pixel[3]) / 255.0f;                        // A\n                float b =\n                  avg_b * (1.0f - alpha) + (alpha / 255.0f) * static_cast<float>(pixel[0]); // B\n                float g =\n                  avg_g * (1.0f - alpha) + (alpha / 255.0f) * static_cast<float>(pixel[1]); // G\n                float r =\n                  avg_r * (1.0f - alpha) + (alpha / 255.0f) * static_cast<float>(pixel[2]); // R\n                l.push_back((r + g + b) / 3.0f);\n                p.push_back((r + g) / 2.0f - b);\n                q.push_back(r - g);\n                a.push_back(alpha);\n            }\n        }\n    }\n    else if (frame->type() == CV_8UC3) {\n        // 3 channels (BGR)\n        for (int i = 0; i < h; ++i) {\n            for (int j = 0; j < w; ++j) {\n                size_t orig_i = static_cast<size_t>(i * row_ratio);\n                size_t orig_j = static_cast<size_t>(j * col_ratio);\n                const cv::Vec3b& pixel = frame->at<cv::Vec3b>(orig_i, orig_j);\n                float b = (1.0f / 255.0f) * static_cast<float>(pixel[0]); // B\n                float g = (1.0f / 255.0f) * static_cast<float>(pixel[1]); // G\n                float r = (1.0f / 255.0f) * static_cast<float>(pixel[2]); // R\n                l.push_back((r + g + b) / 3.0f);\n                p.push_back((r + g) / 2.0f - b);\n                q.push_back(r - g);\n                a.push_back(1.0f);\n            }\n        }\n    }\n    else if (frame->type() == CV_8U) {\n        for (int i = 0; i < h; ++i) {\n            for (int j = 0; j < w; ++j) {\n                size_t orig_i = static_cast<size_t>(i * row_ratio);\n                size_t orig_j = static_cast<size_t>(j * col_ratio);\n                uchar pixel = frame->at<uchar>(orig_i, orig_j);\n                float l_val = static_cast<float>(pixel) / 255.0f;\n                l.push_back(l_val);\n                p.push_back(0.0f);\n                q.push_back(0.0f);\n                a.push_back(1.0f);\n            }\n        }\n    }\n    else {\n        // Unsupported format\n        return -1;\n    }\n\n    size_t l_limit = has_alpha ? 5 : 7; // Use fewer luminance bits if there's alpha\n\n    size_t lx = std::max(static_cast<size_t>(std::round(static_cast<float>(l_limit * w) /\n                                                        static_cast<float>(std::max(w, h)))),\n                         static_cast<size_t>(1));\n    size_t ly = std::max(static_cast<size_t>(std::round(static_cast<float>(l_limit * h) /\n                                                        static_cast<float>(std::max(w, h)))),\n                         static_cast<size_t>(1));\n\n    float l_dc, l_scale, p_dc, p_scale, q_dc, q_scale, a_dc, a_scale;\n    std::vector<float> l_ac, p_ac, q_ac, a_ac;\n    std::tie(l_dc, l_ac, l_scale) = encode_channel(\n      l, std::max(lx, static_cast<size_t>(3)), std::max(ly, static_cast<size_t>(3)), w, h);\n    std::tie(p_dc, p_ac, p_scale) = encode_channel(p, 3, 3, w, h);\n    std::tie(q_dc, q_ac, q_scale) = encode_channel(q, 3, 3, w, h);\n    if (has_alpha) {\n        std::tie(a_dc, a_ac, a_scale) = encode_channel(a, 5, 5, w, h);\n    }\n    else {\n        a_dc = 1.0f;\n        a_scale = 1.0f;\n    }\n\n    bool is_landscape = w > h;\n    uint32_t header24 = static_cast<uint32_t>(std::round(63.0f * l_dc)) |\n      (static_cast<uint32_t>(std::round(31.5f + 31.5f * p_dc)) << 6) |\n      (static_cast<uint32_t>(std::round(31.5f + 31.5f * q_dc)) << 12) |\n      (static_cast<uint32_t>(std::round(31.0f * l_scale)) << 18) | (has_alpha ? 1 << 23 : 0);\n    uint16_t header16 = static_cast<uint16_t>(is_landscape ? ly : lx) |\n      (static_cast<uint16_t>(std::round(63.0f * p_scale)) << 3) |\n      (static_cast<uint16_t>(std::round(63.0f * q_scale)) << 9) | (is_landscape ? 1 << 15 : 0);\n\n    std::vector<uint8_t> hash;\n    hash.reserve(25);\n\n    hash.push_back(header24 & 255);\n    hash.push_back((header24 >> 8) & 255);\n    hash.push_back(header24 >> 16);\n    hash.push_back(header16 & 255);\n    hash.push_back(header16 >> 8);\n    bool is_odd = false;\n    if (has_alpha) {\n        hash.push_back(static_cast<uint8_t>(std::round(15.0f * a_dc)) |\n                       (static_cast<uint8_t>(std::round(15.0f * a_scale)) << 4));\n    }\n    for (auto ac : {l_ac, p_ac, q_ac}) {\n        for (float f : ac) {\n            uint8_t u = static_cast<uint8_t>(std::round(15.0f * f));\n            if (is_odd) {\n                *hash.rbegin() |= u << 4;\n            }\n            else {\n                hash.push_back(u);\n            }\n            is_odd = !is_odd;\n        }\n    }\n    if (has_alpha) {\n        for (float f : a_ac) {\n            uint8_t u = static_cast<uint8_t>(std::round(15.0f * f));\n            if (is_odd) {\n                *hash.rbegin() |= u << 4;\n            }\n            else {\n                hash.push_back(u);\n            }\n            is_odd = !is_odd;\n        }\n    }\n\n    if (hash.size() <= e->dst_len) {\n        std::copy(hash.begin(), hash.end(), e->dst);\n    }\n    else {\n        return -1;\n    }\n    return hash.size();\n}\n\nvoid thumbhash_encoder_release(thumbhash_encoder e)\n{\n    delete e;\n}\n"
        },
        {
          "name": "thumbhash.go",
          "type": "blob",
          "size": 1.4755859375,
          "content": "package lilliput\n\n// #include \"thumbhash.hpp\"\nimport \"C\"\n\nimport (\n\t\"io\"\n\t\"unsafe\"\n)\n\n// thumbhashEncoder handles the encoding of images into ThumbHash format.\n// ThumbHash is a very compact representation of a placeholder for an image.\ntype thumbhashEncoder struct {\n\tencoder C.thumbhash_encoder\n\tbuf     []byte\n}\n\n// newThumbhashEncoder creates a new ThumbHash encoder instance.\n// It takes a decoder and a buffer as input, initializing the C-based encoder.\n// Returns an error if the provided buffer is too small.\nfunc newThumbhashEncoder(decodedBy Decoder, buf []byte) (*thumbhashEncoder, error) {\n\tbuf = buf[:1]\n\tenc := C.thumbhash_encoder_create(unsafe.Pointer(&buf[0]), C.size_t(cap(buf)))\n\tif enc == nil {\n\t\treturn nil, ErrBufTooSmall\n\t}\n\treturn &thumbhashEncoder{\n\t\tencoder: enc,\n\t\tbuf:     buf,\n\t}, nil\n}\n\n// Encode converts the given Framebuffer into a ThumbHash byte representation.\n// The opt parameter allows passing encoding options as key-value pairs.\n// Returns the encoded bytes or an error if the input is invalid.\nfunc (e *thumbhashEncoder) Encode(f *Framebuffer, opt map[int]int) ([]byte, error) {\n\tif f == nil {\n\t\treturn nil, io.EOF\n\t}\n\n\tlength := C.thumbhash_encoder_encode(e.encoder, f.mat)\n\tif length <= 0 {\n\t\treturn nil, ErrInvalidImage\n\t}\n\n\treturn e.buf[:length], nil\n}\n\n// Close releases the resources associated with the ThumbHash encoder.\n// This should be called when the encoder is no longer needed.\nfunc (e *thumbhashEncoder) Close() {\n\tC.thumbhash_encoder_release(e.encoder)\n}\n"
        },
        {
          "name": "thumbhash.hpp",
          "type": "blob",
          "size": 0.419921875,
          "content": "#ifndef LILLIPUT_THUMBHASH_HPP\n#define LILLIPUT_THUMBHASH_HPP\n\n#include \"opencv.hpp\"\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\ntypedef struct thumbhash_encoder_struct* thumbhash_encoder;\n\nthumbhash_encoder thumbhash_encoder_create(void* buf, size_t buf_len);\nint thumbhash_encoder_encode(thumbhash_encoder e, const opencv_mat opqaue_frame);\nvoid thumbhash_encoder_release(thumbhash_encoder e);\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif\n"
        },
        {
          "name": "thumbhash_test.go",
          "type": "blob",
          "size": 3.0595703125,
          "content": "package lilliput\n\nimport (\n\t\"encoding/base64\"\n\t\"io/ioutil\"\n\t\"testing\"\n)\n\nfunc TestThumbhash(t *testing.T) {\n\tcheckImage := func(expectedB64Hash, filePath string, ops *ImageOps, dst []byte) {\n\t\tinputBuf, err := ioutil.ReadFile(filePath)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to read input file %q: %v\", filePath, err)\n\t\t}\n\n\t\tdecoder, err := NewDecoder(inputBuf)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"error decoding image %q: %v\", filePath, err)\n\t\t}\n\t\tdefer decoder.Close()\n\n\t\theader, err := decoder.Header()\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"error reading image header of %q: %v\", filePath, err)\n\t\t}\n\n\t\topts := &ImageOptions{\n\t\t\tFileType:             \".thumbhash\",\n\t\t\tWidth:                header.width,\n\t\t\tHeight:               header.height,\n\t\t\tResizeMethod:         ImageOpsNoResize,\n\t\t\tNormalizeOrientation: true,\n\t\t}\n\t\thash, err := ops.Transform(decoder, opts, dst)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"error transforming image %q: %v\", filePath, err)\n\t\t}\n\t\tb64Hash := base64.StdEncoding.EncodeToString(hash)\n\n\t\tif b64Hash != expectedB64Hash {\n\t\t\tt.Errorf(\"hash of %q is %q but should be %q\",\n\t\t\t\tfilePath, b64Hash, expectedB64Hash)\n\t\t}\n\t}\n\n\tops := NewImageOps(8192)\n\tdefer ops.Close()\n\tdst := make([]byte, 0, 1024*1024)\n\n\t// These test images came from the demo page at:\n\t// https://evanw.github.io/thumbhash/\n\t//\n\t// The expected thumbhashes in the tests were generated using the reference\n\t// rust implementation there.\n\t//\n\t// Note the thumbhashes for 'field.jpg' and 'opera.png' generated by the\n\t// rust reference code were slightly different than the respective hashes in\n\t// the demo page (presumably generated by the JS reference implementation).\n\t//\n\t// This is not very surprising given the heavy reliance on floating point\n\t// math. The differences were likely rounding errors. The decoded images\n\t// from those hashes were visually identical.\n\tcheckImage(\"1QcSHQRnh493V4dIh4eXh1h4kJUI\", \"data/sunrise.jpg\", ops, dst)\n\tcheckImage(\"3PcNNYSFeXh/d3eld0iHZoZgVwh2\", \"data/sunset.jpg\", ops, dst)\n\tcheckImage(\"3OcRJYB4d3h/iIeHeEh3eIhw+j3A\", \"data/field.jpg\", ops, dst)\n\tcheckImage(\"HBkSHYSIeHiPiHh8eJd4eTN0EEQG\", \"data/fall.jpg\", ops, dst)\n\tcheckImage(\"VggKDYAW6lZvdYd6d2iZh/p4GE/k\", \"data/street.jpg\", ops, dst)\n\tcheckImage(\"2fcZFIB3iId/h3iJh4aIYJ2V8g==\", \"data/mountain.jpg\", ops, dst)\n\tcheckImage(\"IQgSLYZ6iHePh4h1eFeHh4dwgwg3\", \"data/coast.jpg\", ops, dst)\n\tcheckImage(\"YJqGPQw7sFlslqhFafSE+Q6oJ1h2iHB2Rw==\", \"data/firefox.png\", ops, dst)\n\tcheckImage(\"mYqDBQQnxnj0JoLYdN7f8JhpuDeHiHdwZw==\", \"data/opera.png\", ops, dst)\n\n\t// Test other image formats, bit depths, and color spaces.\n\tcheckImage(\"YJqGPQw7oFlslqhGafOE+Q6oJ1h2iHBlVw==\", \"data/firefox-16bit.png\", ops, dst)\n\tcheckImage(\"YJqGPQw7sFlslqhFafSE+Q6oJ1h2iHB2Rw==\", \"data/firefox-16bit-alpha.png\", ops, dst)\n\tcheckImage(\"FwgOBwAxOWl4l3aQpFiIN5iHBgAAAAAA\", \"data/firefox-gray.jpg\", ops, dst)\n\tcheckImage(\"4AeKBQA7oFl7lqhmaDBp92yJJ1h2iHB2Rw==\", \"data/firefox-gray-alpha.webp\", ops, dst)\n\tcheckImage(\"EwiCBQAnwnjzJpHIZAAAAAAAuDeHiHdwZw==\", \"data/opera-gray-alpha.png\", ops, dst)\n\n\t// Test downsampling.\n\tcheckImage(\"VvYRNQRod3x3B4iHeHhYiHeAeQUo\", \"data/large-sunrise.jpg\", ops, dst)\n}\n"
        },
        {
          "name": "webp.cpp",
          "type": "blob",
          "size": 18.6962890625,
          "content": "#include \"webp.hpp\"\n#include <opencv2/imgproc.hpp>\n#include <webp/decode.h>\n#include <webp/encode.h>\n#include <webp/mux.h>\n#include <stdbool.h>\n\nstruct webp_decoder_struct {\n    WebPMux* mux;\n    int total_frame_count;\n    uint32_t bgcolor;\n    uint32_t loop_count;\n    bool has_alpha;\n    bool has_animation;\n    int width;\n    int height;\n\n    int current_frame_index;\n    int prev_frame_delay_time;\n    int prev_frame_x_offset;\n    int prev_frame_y_offset;\n    WebPMuxAnimDispose prev_frame_dispose;\n    WebPMuxAnimBlend prev_frame_blend;\n    uint8_t* decode_buffer;\n    size_t decode_buffer_size;\n    int total_duration;\n};\n\nstruct webp_encoder_struct {\n    // input fields\n    const uint8_t* icc;\n    size_t icc_len;\n    uint32_t bgcolor;\n    uint32_t loop_count;\n\n    // output fields\n    WebPMux* mux;\n    int frame_count;\n    int first_frame_delay;\n    int first_frame_blend;\n    int first_frame_dispose;\n    int first_frame_x_offset;\n    int first_frame_y_offset;\n    uint8_t* dst;\n    size_t dst_len;\n};\n\n/**\n * Creates a WebP decoder from the given OpenCV matrix.\n * @param buf The input OpenCV matrix containing the WebP image data.\n * @return A pointer to the created webp_decoder_struct, or nullptr if creation failed.\n */\nwebp_decoder webp_decoder_create(const opencv_mat buf)\n{\n    auto cvMat = static_cast<const cv::Mat*>(buf);\n    WebPData src = { cvMat->data, cvMat->total() };\n    WebPMux* mux = WebPMuxCreate(&src, 0);\n\n    if (!mux) {\n        return nullptr;\n    }\n\n    // Get features at the container level\n    uint32_t flags;\n    if (WebPMuxGetFeatures(mux, &flags) != WEBP_MUX_OK) {\n        WebPMuxDelete(mux);\n        return nullptr;\n    }\n\n    // Get the first frame to retrieve the image dimensions\n    WebPMuxFrameInfo frame;\n    if (WebPMuxGetFrame(mux, 1, &frame) != WEBP_MUX_OK) {\n        WebPMuxDelete(mux);\n        return nullptr;\n    }\n\n    WebPBitstreamFeatures features;\n    if (WebPGetFeatures(frame.bitstream.bytes, frame.bitstream.size, &features) != VP8_STATUS_OK) {\n        WebPDataClear(&frame.bitstream);\n        WebPMuxDelete(mux);\n        return nullptr;\n    }\n    WebPDataClear(&frame.bitstream);\n\n    webp_decoder d = new webp_decoder_struct();\n    memset(d, 0, sizeof(webp_decoder_struct));\n    d->mux = mux;\n    d->current_frame_index = 1;\n    d->has_alpha = (flags & ALPHA_FLAG);\n\n    // Get the canvas size\n    if (WebPMuxGetCanvasSize(mux, &d->width, &d->height) != WEBP_MUX_OK) {\n        WebPMuxDelete(mux);\n        return nullptr;\n    }\n\n    // Calculate total frame count and duration\n    d->total_frame_count = 0;\n    d->total_duration = 0;\n    do {\n        d->total_frame_count++;\n        d->total_duration += frame.duration;\n        WebPDataClear(&frame.bitstream);\n    } while (WebPMuxGetFrame(mux, d->total_frame_count + 1, &frame) == WEBP_MUX_OK);\n\n    // Get animation parameters\n    d->bgcolor = 0xFFFFFFFF; // Default to white background\n    if (flags & ANIMATION_FLAG) {\n        WebPMuxAnimParams anim_params;\n        if (WebPMuxGetAnimationParams(mux, &anim_params) == WEBP_MUX_OK) {\n            d->bgcolor = anim_params.bgcolor;\n            d->loop_count = anim_params.loop_count;\n        }\n        d->has_animation = true;\n    } else {\n        // For static images, ensure duration is 0\n        d->total_duration = 0;\n    }\n\n    // Pre-allocate decode buffer\n    d->decode_buffer_size = d->width * d->height * 4; // 4 channels for RGBA\n    d->decode_buffer = new uint8_t[d->decode_buffer_size];\n\n    return d;\n}\n\n/**\n * Gets the width of the WebP image.\n * @param d The webp_decoder_struct pointer.\n * @return The width of the WebP image.\n */\nint webp_decoder_get_width(const webp_decoder d)\n{\n    return d->width;\n}\n\n/**\n * Gets the height of the WebP image.\n * @param d The webp_decoder_struct pointer.\n * @return The height of the WebP image.\n */\nint webp_decoder_get_height(const webp_decoder d)\n{\n    return d->height;\n}\n\n/**\n * Gets the pixel type of the WebP image.\n * @param d The webp_decoder_struct pointer.\n * @return The pixel type of the WebP image.\n */\nint webp_decoder_get_pixel_type(const webp_decoder d)\n{\n    return d->has_alpha ? CV_8UC4 : CV_8UC3;\n}\n\n/**\n * Gets the delay time of the previous frame.\n * @param d The webp_decoder_struct pointer.\n * @return The delay time of the previous frame.\n */\nint webp_decoder_get_prev_frame_delay(const webp_decoder d)\n{\n    return d->prev_frame_delay_time;\n}\n\n/**\n * Gets the x-offset of the previous frame.\n * @param d The webp_decoder_struct pointer.\n * @return The x-offset of the previous frame.\n */\nint webp_decoder_get_prev_frame_x_offset(const webp_decoder d)\n{\n    return d->prev_frame_x_offset;\n}\n\n/**\n * Gets the y-offset of the previous frame.\n * @param d The webp_decoder_struct pointer.\n * @return The y-offset of the previous frame.\n */\nint webp_decoder_get_prev_frame_y_offset(const webp_decoder d)\n{\n    return d->prev_frame_y_offset;\n}\n\n/**\n * Gets the dispose method of the previous frame.\n * @param d The webp_decoder_struct pointer.\n * @return The dispose method of the previous frame.\n */\nint webp_decoder_get_prev_frame_dispose(const webp_decoder d)\n{\n    return d->prev_frame_dispose;\n}\n\n/**\n * Gets the blend method of the previous frame.\n * @param d The webp_decoder_struct pointer.\n * @return The blend method of the previous frame.\n */\nint webp_decoder_get_prev_frame_blend(const webp_decoder d)\n{\n    return d->prev_frame_blend;\n}\n\n/**\n * Gets the background color of the WebP image.\n * @param d The webp_decoder_struct pointer.\n * @return The background color of the WebP image.\n */\nuint32_t webp_decoder_get_bg_color(const webp_decoder d)\n{\n    return d->bgcolor;\n}\n\n/**\n * Gets the loop count of the WebP image.\n * @param d The webp_decoder_struct pointer.\n * @return The loop count of the WebP image.\n */\nuint32_t webp_decoder_get_loop_count(const webp_decoder d)\n{\n    return d->loop_count;\n}\n\n/**\n * Gets the total number of frames in the WebP image.\n * @param d The webp_decoder_struct pointer.\n * @return The total number of frames in the WebP image.\n */\nint webp_decoder_get_num_frames(const webp_decoder d)\n{\n    return d ? d->total_frame_count : 0;\n}\n\n/**\n * Gets the total duration of the WebP animation in milliseconds.\n * @param d The webp_decoder_struct pointer.\n * @return The total duration in milliseconds, 0 for static images.\n */\nint webp_decoder_get_total_duration(const webp_decoder d)\n{\n    return d ? d->total_duration : 0;\n}\n\n/**\n * Gets the ICC profile data from the WebP image.\n * @param d The webp_decoder_struct pointer.\n * @param dst The destination buffer to store the ICC profile data.\n * @param dst_len The size of the destination buffer.\n * @return The size of the ICC profile data copied to the destination buffer.\n */\nsize_t webp_decoder_get_icc(const webp_decoder d, void* dst, size_t dst_len)\n{\n    WebPData icc = { nullptr, 0 };\n    auto res = WebPMuxGetChunk(d->mux, \"ICCP\", &icc);\n    if (icc.size > 0 && res == WEBP_MUX_OK) {\n        if (icc.size <= dst_len) {\n            memcpy(dst, icc.bytes, icc.size);\n            return icc.size;\n        }\n    }\n    return 0;\n}\n\n/**\n * Checks if there are more frames to decode in the WebP image.\n * @param d The webp_decoder_struct pointer.\n * @return True if there are more frames to decode, false otherwise.\n */\nint webp_decoder_has_more_frames(webp_decoder d)\n{\n     return d->current_frame_index < d->total_frame_count;\n}\n\n/**\n * Advances to the next frame in the WebP image.\n * @param d The webp_decoder_struct pointer.\n */\nvoid webp_decoder_advance_frame(webp_decoder d)\n{\n    d->current_frame_index++;\n}\n\n/**\n * Decodes the current frame of the WebP image and stores the decoded image in the provided OpenCV matrix.\n * @param d The webp_decoder_struct pointer.\n * @param mat The OpenCV matrix to store the decoded image.\n * @return True if the frame was successfully decoded, false otherwise.\n */\nbool webp_decoder_decode(const webp_decoder d, opencv_mat mat)\n{\n    if (!d) {\n        return false;\n    }\n\n    WebPMuxFrameInfo frame;\n    WebPMuxError mux_error = WebPMuxGetFrame(d->mux, d->current_frame_index, &frame);\n    \n    if (mux_error != WEBP_MUX_OK) {\n        return false;\n    }\n\n    WebPBitstreamFeatures features;\n    if (WebPGetFeatures(frame.bitstream.bytes, frame.bitstream.size, &features) != VP8_STATUS_OK) {\n        WebPDataClear(&frame.bitstream);\n        return false;\n    }\n\n    // Set the cv::Mat dimensions to the frame's width and height\n    auto cvMat = static_cast<cv::Mat*>(mat);\n    cvMat->create(features.height, features.width, webp_decoder_get_pixel_type(d));\n\n    // Recalculate row size based on the new dimensions\n    int row_size = cvMat->cols * cvMat->elemSize();\n\n    // Store frame properties for future use\n    d->prev_frame_delay_time = frame.duration;\n    d->prev_frame_x_offset = frame.x_offset;\n    d->prev_frame_y_offset = frame.y_offset;\n    d->prev_frame_dispose = frame.dispose_method;\n    d->prev_frame_blend = frame.blend_method;\n\n    // Decode the frame\n    uint8_t* res = nullptr;\n    switch (webp_decoder_get_pixel_type(d)) {\n        case CV_8UC4:\n            res = WebPDecodeBGRAInto(frame.bitstream.bytes, frame.bitstream.size,\n                                     d->decode_buffer, d->decode_buffer_size, row_size);\n            break;\n        case CV_8UC3:\n            res = WebPDecodeBGRInto(frame.bitstream.bytes, frame.bitstream.size,\n                                d->decode_buffer, d->decode_buffer_size, row_size);\n            break;\n        default:\n            return false;\n    }\n\n    if (res) {\n        memcpy(cvMat->data, d->decode_buffer, cvMat->total() * cvMat->elemSize());\n    }\n\n    WebPDataClear(&frame.bitstream);\n    return res != nullptr;\n}\n\n/**\n * Releases the resources allocated for the webp_decoder_struct.\n * @param d The webp_decoder_struct pointer.\n */\nvoid webp_decoder_release(webp_decoder d)\n{\n    if (d) {\n        if (d->mux) WebPMuxDelete(d->mux);\n        delete[] d->decode_buffer;\n        delete d;\n    }\n}\n\n/**\n * Creates a WebP encoder with the given parameters.\n * @param buf The output buffer to store the encoded WebP data.\n * @param buf_len The size of the output buffer.\n * @param icc The ICC profile data.\n * @param icc_len The size of the ICC profile data.\n * @param bgcolor The background color for the WebP image.\n * @return A pointer to the created webp_encoder_struct, or nullptr if creation failed.\n */\nwebp_encoder webp_encoder_create(void* buf, size_t buf_len, const void* icc, size_t icc_len, uint32_t bgcolor, int loop_count)\n{\n    webp_encoder e = new struct webp_encoder_struct();\n    memset(e, 0, sizeof(struct webp_encoder_struct));\n    e->dst = (uint8_t*)(buf);\n    e->dst_len = buf_len;\n    e->mux = WebPMuxNew();\n    e->frame_count = 1;\n    e->first_frame_delay = 0;\n    e->bgcolor = bgcolor;\n    e->loop_count = loop_count;\n    if (icc_len) {\n        e->icc = (const uint8_t*)(icc);\n        e->icc_len = icc_len;\n    }\n    return e;\n}\n\n/**\n * Encodes the given OpenCV matrix as a WebP image and writes the encoded data to the output buffer.\n * @param e The webp_encoder_struct pointer.\n * @param src The OpenCV matrix containing the image to encode.\n * @param opt The encoding options.\n * @param opt_len The number of encoding options.\n * @param delay The delay time for the current frame.\n * @param blend The blend method for the current frame.\n * @param dispose The dispose method for the current frame.\n * @param x_offset The x-offset for the current frame.\n * @param y_offset The y-offset for the current frame.\n * @return The size of the encoded WebP data, or 0 if encoding failed.\n */\nsize_t webp_encoder_write(webp_encoder e, const opencv_mat src, const int* opt, size_t opt_len, int delay, int blend, int dispose, int x_offset, int y_offset)\n{\n    if (!e || !e->mux) {\n        return 0;\n    }\n\n    // if the source is null, finalize the animation/image and return the size of the output buffer\n    if (!src) {\n        if (e->frame_count == 1) {\n            // No frames were added\n            WebPMuxDelete(e->mux);\n            e->mux = nullptr;\n            return 0;\n        }\n\n        // Finalize the animation/image and return the size of the output buffer\n        WebPData out_mux = { nullptr, 0 };\n        WebPMuxError mux_error = WebPMuxAssemble(e->mux, &out_mux);\n\n        if (mux_error != WEBP_MUX_OK) {\n            return 0;\n        }\n\n        if (out_mux.size == 0) {\n            return 0;\n        }\n\n        size_t copied = 0;\n        if (out_mux.size < e->dst_len) {\n            memcpy(e->dst, out_mux.bytes, out_mux.size);\n            copied = out_mux.size;\n        }\n\n        WebPDataClear(&out_mux);\n        WebPMuxDelete(e->mux);\n        e->mux = nullptr; // Ensure the mux is no longer used\n\n        return copied;\n    }\n\n    // Encode the source image\n    auto mat = static_cast<const cv::Mat*>(src);\n    if (!mat || mat->empty()) {\n        // Invalid or empty OpenCV matrix\n        return 0;\n    }\n\n    float quality = 100.0f;\n    for (size_t i = 0; i + 1 < opt_len; i += 2) {\n        if (opt[i] == CV_IMWRITE_WEBP_QUALITY) {\n            quality = std::max(1.0f, (float)opt[i + 1]);\n        }\n    }\n\n    if (mat->depth() != CV_8U) {\n        // Image depth is not 8-bit unsigned\n        return 0;\n    }\n\n    cv::Mat grayscaleConversionMat;\n    if (mat->channels() == 1) {\n        // for grayscale images, construct a temporary source\n        cv::cvtColor(*mat, grayscaleConversionMat, CV_GRAY2BGR);\n        if (grayscaleConversionMat.empty()) {\n            // failed to convert grayscale image to BGR\n            return 0;\n        }\n        mat = &grayscaleConversionMat;\n    }\n\n    if (mat->channels() != 3 && mat->channels() != 4) {\n        // Image must have 3 or 4 channels\n        return 0;\n    }\n\n    // webp will always allocate a region for the compressed image\n    // we will have to copy from it, then deallocate this region\n    size_t size = 0;\n    uint8_t* out_picture = nullptr;\n\n    if (quality > 100.0f) {\n        if (mat->channels() == 3) {\n            size = WebPEncodeLosslessBGR(mat->data, mat->cols, mat->rows, mat->step, &out_picture);\n        } else {\n            size = WebPEncodeLosslessBGRA(mat->data, mat->cols, mat->rows, mat->step, &out_picture);\n        }\n    } else {\n        if (mat->channels() == 3) {\n            size = WebPEncodeBGR(mat->data, mat->cols, mat->rows, mat->step, quality, &out_picture);\n        } else {\n            size = WebPEncodeBGRA(mat->data, mat->cols, mat->rows, mat->step, quality, &out_picture);\n        }\n    }\n\n    if (size == 0) {\n        // Failed to encode image\n        return 0;\n    }\n\n    WebPData picture = { out_picture, size };\n    if (e->frame_count == 1) {\n        // First frame handling\n        e->first_frame_delay = delay;\n        e->first_frame_blend = blend;\n        e->first_frame_dispose = dispose;\n        e->first_frame_x_offset = x_offset;\n        e->first_frame_y_offset = y_offset;\n\n        // Add ICC profile to the mux object\n        if (e->icc) {\n            WebPData icc_data = { e->icc, e->icc_len };\n            WebPMuxError mux_error = WebPMuxSetChunk(e->mux, \"ICCP\", &icc_data, 0);\n        }\n\n        WebPMuxError mux_error = WebPMuxSetImage(e->mux, &picture, 1);\n        if (mux_error != WEBP_MUX_OK) {\n            if (out_picture) {\n                WebPFree(out_picture);\n            }\n            return 0;\n        }\n    } else {\n        if (e->frame_count == 2) {\n            // A second frame is provided: we need to recreate the mux for animation\n\n            // Store the first frame\n            WebPMuxFrameInfo first_frame;\n            memset(&first_frame, 0, sizeof(WebPMuxFrameInfo));\n            WebPMuxError get_frame_error = WebPMuxGetFrame(e->mux, 1, &first_frame);\n            if (get_frame_error != WEBP_MUX_OK) {\n                if (out_picture) {\n                    WebPFree(out_picture);\n                }\n                return 0;\n            }\n\n            // Delete the old single-image mux and create a new one for animation\n            WebPMuxDelete(e->mux);\n            e->mux = WebPMuxNew();\n\n            // Set the ICC profile if it exists\n            if (e->icc && e->icc_len > 0) {\n                WebPData icc_data = { e->icc, e->icc_len };\n                WebPMuxError mux_error = WebPMuxSetChunk(e->mux, \"ICCP\", &icc_data, 1);\n                if (mux_error != WEBP_MUX_OK) {\n                    if (out_picture) {\n                        WebPFree(out_picture);\n                    }\n                    return 0;\n                }\n            }\n\n            // Set animation parameters\n            WebPMuxAnimParams anim_params;\n            anim_params.loop_count = e->loop_count;\n            anim_params.bgcolor = e->bgcolor;\n\n            WebPMuxError anim_params_error = WebPMuxSetAnimationParams(e->mux, &anim_params);\n            if (anim_params_error != WEBP_MUX_OK) {\n                if (out_picture) {\n                    WebPFree(out_picture);\n                }\n                return 0;\n            }\n\n            // Convert the first frame into an ANMF chunk and add it to the new mux\n            first_frame.id = WEBP_CHUNK_ANMF;\n            first_frame.duration = e->first_frame_delay;\n            first_frame.x_offset = e->first_frame_x_offset;\n            first_frame.y_offset = e->first_frame_y_offset;\n            first_frame.dispose_method = (WebPMuxAnimDispose)e->first_frame_dispose;\n            first_frame.blend_method = (WebPMuxAnimBlend)e->first_frame_blend;\n            WebPMuxError push_frame_error = WebPMuxPushFrame(e->mux, &first_frame, 1);\n            if (push_frame_error != WEBP_MUX_OK) {\n                if (out_picture) {\n                    WebPFree(out_picture);\n                }\n                return 0;\n            }\n            WebPDataClear(&first_frame.bitstream);\n        }\n\n        // Add the current frame as an ANMF chunk\n        WebPMuxFrameInfo frame;\n        memset(&frame, 0, sizeof(WebPMuxFrameInfo));\n        frame.bitstream = picture;\n        frame.id = WEBP_CHUNK_ANMF;\n        frame.x_offset = x_offset;\n        frame.y_offset = y_offset;\n        frame.duration = delay;\n        frame.dispose_method = (WebPMuxAnimDispose)dispose;\n        frame.blend_method = (WebPMuxAnimBlend)blend;\n\n        // Add the frame to the mux object\n        WebPMuxError push_frame_error = WebPMuxPushFrame(e->mux, &frame, 1);\n        if (push_frame_error != WEBP_MUX_OK) {\n            if (out_picture) {\n                WebPFree(out_picture);\n            }\n            return 0;\n        }\n    }\n\n    e->frame_count++;\n\n    if (out_picture) {\n        WebPFree(out_picture);\n    }\n    return size;\n}\n\n/**\n * Releases the resources allocated for the webp_encoder_struct.\n * @param e The webp_encoder_struct pointer.\n */\nvoid webp_encoder_release(webp_encoder e)\n{\n    if (e) {\n        if (e->mux) {\n            WebPMuxDelete(e->mux);\n        }\n        delete e;\n    }\n}\n\n/**\n * Flushes the remaining data in the webp_encoder_struct and finalizes the WebP image.\n * @param e The webp_encoder_struct pointer.\n * @return The size of the encoded WebP data, or 0 if encoding failed.\n */\nsize_t webp_encoder_flush(webp_encoder e)\n{\n    return webp_encoder_write(e, nullptr, nullptr, 0, 0, 0, 0, 0, 0);\n}"
        },
        {
          "name": "webp.go",
          "type": "blob",
          "size": 6.6162109375,
          "content": "package lilliput\n\n// #include \"webp.hpp\"\nimport \"C\"\n\nimport (\n\t\"io\"\n\t\"time\"\n\t\"unsafe\"\n)\n\n// webpDecoder implements the Decoder interface for WebP images.\ntype webpDecoder struct {\n\tdecoder C.webp_decoder\n\tmat     C.opencv_mat\n\tbuf     []byte\n}\n\n// webpEncoder implements the Encoder interface for WebP images.\ntype webpEncoder struct {\n\tencoder    C.webp_encoder\n\tdstBuf     []byte\n\ticc        []byte\n\tframeIndex int\n\thasFlushed bool\n}\n\n// newWebpDecoder creates a new WebP decoder from the provided byte buffer.\n// Returns an error if the buffer is too small or contains invalid WebP data.\nfunc newWebpDecoder(buf []byte) (*webpDecoder, error) {\n\tmat := C.opencv_mat_create_from_data(C.int(len(buf)), 1, C.CV_8U, unsafe.Pointer(&buf[0]), C.size_t(len(buf)))\n\n\tif mat == nil {\n\t\treturn nil, ErrBufTooSmall\n\t}\n\n\tdecoder := C.webp_decoder_create(mat)\n\tif decoder == nil {\n\t\treturn nil, ErrInvalidImage\n\t}\n\n\treturn &webpDecoder{\n\t\tdecoder: decoder,\n\t\tmat:     mat,\n\t\tbuf:     buf,\n\t}, nil\n}\n\n// Header returns the image metadata including dimensions, pixel type, and frame count.\nfunc (d *webpDecoder) Header() (*ImageHeader, error) {\n\treturn &ImageHeader{\n\t\twidth:         int(C.webp_decoder_get_width(d.decoder)),\n\t\theight:        int(C.webp_decoder_get_height(d.decoder)),\n\t\tpixelType:     PixelType(C.webp_decoder_get_pixel_type(d.decoder)),\n\t\torientation:   OrientationTopLeft,\n\t\tnumFrames:     int(C.webp_decoder_get_num_frames(d.decoder)),\n\t\tcontentLength: len(d.buf),\n\t}, nil\n}\n\n// Close releases all resources associated with the decoder.\nfunc (d *webpDecoder) Close() {\n\tC.webp_decoder_release(d.decoder)\n\tC.opencv_mat_release(d.mat)\n\td.buf = nil\n}\n\n// Description returns the image format description (\"WEBP\").\nfunc (d *webpDecoder) Description() string {\n\treturn \"WEBP\"\n}\n\n// Duration returns the total duration of the WebP animation.\n// Returns 0 for static images.\nfunc (d *webpDecoder) Duration() time.Duration {\n\treturn time.Duration(C.webp_decoder_get_total_duration(d.decoder)) * time.Millisecond\n}\n\n// HasSubtitles returns whether the image contains subtitle data (always false for WebP).\nfunc (d *webpDecoder) HasSubtitles() bool {\n\treturn false\n}\n\n// IsStreamable returns whether the image format supports streaming (always false for WebP).\nfunc (d *webpDecoder) IsStreamable() bool {\n\treturn false\n}\n\n// hasReachedEndOfFrames checks if the decoder has reached the end of all frames.\nfunc (d *webpDecoder) hasReachedEndOfFrames() bool {\n\treturn C.webp_decoder_has_more_frames(d.decoder) == 0\n}\n\n// advanceFrameIndex advances the internal frame index for the next decoding call.\nfunc (d *webpDecoder) advanceFrameIndex() {\n\tC.webp_decoder_advance_frame(d.decoder)\n}\n\n// ICC returns the ICC color profile data embedded in the WebP image.\nfunc (d *webpDecoder) ICC() []byte {\n\ticcDst := make([]byte, 8192)\n\ticcLength := C.webp_decoder_get_icc(d.decoder, unsafe.Pointer(&iccDst[0]), C.size_t(cap(iccDst)))\n\treturn iccDst[:iccLength]\n}\n\n// BackgroundColor returns the background color of the WebP image.\nfunc (d *webpDecoder) BackgroundColor() uint32 {\n\treturn uint32(C.webp_decoder_get_bg_color(d.decoder))\n}\n\n// LoopCount returns the number of times the animation should loop.\nfunc (d *webpDecoder) LoopCount() int {\n\treturn int(C.webp_decoder_get_loop_count(d.decoder))\n}\n\n// DecodeTo decodes the current frame into the provided Framebuffer.\n// Returns io.EOF when all frames have been decoded.\n// Returns ErrDecodingFailed if the frame cannot be decoded.\nfunc (d *webpDecoder) DecodeTo(f *Framebuffer) error {\n\tif f == nil {\n\t\treturn io.EOF\n\t}\n\n\t// Get image header information\n\th, err := d.Header()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Resize the framebuffer matrix to fit the image dimensions and pixel type\n\terr = f.resizeMat(h.Width(), h.Height(), h.PixelType())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Decode the current frame into the framebuffer\n\tret := C.webp_decoder_decode(d.decoder, f.mat)\n\tif !ret {\n\t\t// Check if the decoder has reached the end of the frames\n\t\tif d.hasReachedEndOfFrames() {\n\t\t\treturn io.EOF\n\t\t}\n\t\treturn ErrDecodingFailed\n\t}\n\n\t// Set the frame properties\n\tf.duration = time.Duration(C.webp_decoder_get_prev_frame_delay(d.decoder)) * time.Millisecond\n\tf.xOffset = int(C.webp_decoder_get_prev_frame_x_offset(d.decoder))\n\tf.yOffset = int(C.webp_decoder_get_prev_frame_y_offset(d.decoder))\n\tf.dispose = DisposeMethod(C.webp_decoder_get_prev_frame_dispose(d.decoder))\n\tf.blend = BlendMethod(C.webp_decoder_get_prev_frame_blend(d.decoder))\n\n\t// Advance to the next frame\n\td.advanceFrameIndex()\n\n\treturn nil\n}\n\n// SkipFrame is not supported for WebP images and always returns ErrSkipNotSupported.\nfunc (d *webpDecoder) SkipFrame() error {\n\treturn ErrSkipNotSupported\n}\n\n// newWebpEncoder creates a new WebP encoder using the provided decoder for metadata\n// and destination buffer for the encoded output.\nfunc newWebpEncoder(decodedBy Decoder, dstBuf []byte) (*webpEncoder, error) {\n\tdstBuf = dstBuf[:1]\n\ticc := decodedBy.ICC()\n\tbgColor := decodedBy.BackgroundColor()\n\tloopCount := decodedBy.LoopCount()\n\n\tvar enc C.webp_encoder\n\tif len(icc) > 0 {\n\t\tenc = C.webp_encoder_create(unsafe.Pointer(&dstBuf[0]), C.size_t(cap(dstBuf)), unsafe.Pointer(&icc[0]), C.size_t(len(icc)), C.uint32_t(bgColor), C.int(loopCount))\n\t} else {\n\t\tenc = C.webp_encoder_create(unsafe.Pointer(&dstBuf[0]), C.size_t(cap(dstBuf)), nil, 0, C.uint32_t(bgColor), C.int(loopCount))\n\t}\n\tif enc == nil {\n\t\treturn nil, ErrBufTooSmall\n\t}\n\n\treturn &webpEncoder{\n\t\tencoder: enc,\n\t\tdstBuf:  dstBuf,\n\t\ticc:     icc,\n\t}, nil\n}\n\n// Encode encodes a frame into the WebP format.\n// If f is nil, finalizes the WebP animation and returns the encoded data.\n// Returns io.EOF after the animation has been finalized.\n// The opt parameter allows specifying encoding options as key-value pairs.\nfunc (e *webpEncoder) Encode(f *Framebuffer, opt map[int]int) ([]byte, error) {\n\tif e.hasFlushed {\n\t\treturn nil, io.EOF\n\t}\n\n\tif f == nil {\n\t\t// Finalize the WebP animation\n\t\tlength := C.webp_encoder_flush(e.encoder)\n\t\tif length == 0 {\n\t\t\treturn nil, ErrInvalidImage\n\t\t}\n\n\t\te.hasFlushed = true\n\t\treturn e.dstBuf[:length], nil\n\t}\n\n\tvar optList []C.int\n\tvar firstOpt *C.int\n\tfor k, v := range opt {\n\t\toptList = append(optList, C.int(k))\n\t\toptList = append(optList, C.int(v))\n\t}\n\tif len(optList) > 0 {\n\t\tfirstOpt = (*C.int)(unsafe.Pointer(&optList[0]))\n\t}\n\n\t// Encode the current frame\n\tframeDelay := int(f.duration.Milliseconds())\n\tlength := C.webp_encoder_write(e.encoder, f.mat, firstOpt, C.size_t(len(optList)), C.int(frameDelay), C.int(f.blend), C.int(f.dispose), 0, 0)\n\tif length == 0 {\n\t\treturn nil, ErrInvalidImage\n\t}\n\n\te.frameIndex++\n\n\treturn nil, nil\n}\n\n// Close releases all resources associated with the encoder.\nfunc (e *webpEncoder) Close() {\n\tC.webp_encoder_release(e.encoder)\n}\n"
        },
        {
          "name": "webp.hpp",
          "type": "blob",
          "size": 1.689453125,
          "content": "#ifndef LILLIPUT_WEBP_HPP\n#define LILLIPUT_WEBP_HPP\n\n#include \"opencv.hpp\"\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\ntypedef struct webp_decoder_struct* webp_decoder;\ntypedef struct webp_encoder_struct* webp_encoder;\n\nwebp_decoder webp_decoder_create(const opencv_mat buf);\nint webp_decoder_get_width(const webp_decoder d);\nint webp_decoder_get_height(const webp_decoder d);\nint webp_decoder_get_pixel_type(const webp_decoder d);\nint webp_decoder_get_num_frames(const webp_decoder d);\nint webp_decoder_get_total_duration(const webp_decoder d);\nint webp_decoder_get_prev_frame_delay(const webp_decoder d);\nint webp_decoder_get_prev_frame_dispose(const webp_decoder d);\nint webp_decoder_get_prev_frame_blend(const webp_decoder d);\nint webp_decoder_get_prev_frame_x_offset(const webp_decoder d);\nint webp_decoder_get_prev_frame_y_offset(const webp_decoder d);\nbool webp_decoder_get_prev_frame_has_alpha(const webp_decoder d);\nuint32_t webp_decoder_get_bg_color(const webp_decoder d);\nuint32_t webp_decoder_get_loop_count(const webp_decoder d);\nsize_t webp_decoder_get_icc(const webp_decoder d, void* buf, size_t buf_len);\nvoid webp_decoder_release(webp_decoder d);\nbool webp_decoder_decode(webp_decoder d, opencv_mat mat);\n\nwebp_encoder webp_encoder_create(void* buf, size_t buf_len, const void* icc, size_t icc_len, uint32_t bgcolor, int loop_count);\nsize_t webp_encoder_write(webp_encoder e, const opencv_mat src, const int* opt, size_t opt_len, int delay, int blend, int dispose, int x_offset, int y_offset);\nvoid webp_encoder_release(webp_encoder e);\nsize_t webp_encoder_flush(webp_encoder e);\nvoid webp_decoder_advance_frame(webp_decoder d);\nint webp_decoder_has_more_frames(webp_decoder d);\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif\n"
        },
        {
          "name": "webp_test.go",
          "type": "blob",
          "size": 15.068359375,
          "content": "package lilliput\n\nimport (\n\t\"os\"\n\t\"reflect\"\n\t\"testing\"\n\t\"time\"\n)\n\nconst (\n\tdestinationBufferSize = 10 * 1024 * 1024\n)\n\nfunc TestWebPOperations(t *testing.T) {\n\tt.Run(\"NewWebpDecoder\", testNewWebpDecoder)\n\tt.Run(\"WebpDecoder_Header\", testWebpDecoderHeader)\n\tt.Run(\"NewWebpEncoder\", testNewWebpEncoder)\n\tt.Run(\"WebpDecoder_DecodeTo\", testWebpDecoderDecodeTo)\n\tt.Run(\"WebpEncoder_Encode\", testWebpEncoderEncode)\n\tt.Run(\"NewWebpEncoderWithAnimatedWebPSource\", testNewWebpEncoderWithAnimatedWebPSource)\n\tt.Run(\"NewWebpEncoderWithAnimatedGIFSource\", testNewWebpEncoderWithAnimatedGIFSource)\n}\n\nfunc testNewWebpDecoder(t *testing.T) {\n\ttestWebPImage, err := os.ReadFile(\"testdata/tears_of_steel_no_icc.webp\")\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error while reading webp image: %v\", err)\n\t}\n\tdecoder, err := newWebpDecoder(testWebPImage)\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error while decoding webp image data: %v\", err)\n\t}\n\tdefer decoder.Close()\n}\n\nfunc testWebpDecoderHeader(t *testing.T) {\n\ttestWebPImage, err := os.ReadFile(\"testdata/tears_of_steel_no_icc.webp\")\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error while reading webp image: %v\", err)\n\t}\n\tdecoder, err := newWebpDecoder(testWebPImage)\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error while decoding webp image data: %v\", err)\n\t}\n\tdefer decoder.Close()\n\n\theader, err := decoder.Header()\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error: %v\", err)\n\t}\n\tif reflect.TypeOf(header).String() != \"*lilliput.ImageHeader\" {\n\t\tt.Fatalf(\"Expected type *lilliput.ImageHeader, got %v\", reflect.TypeOf(header))\n\t}\n}\n\nfunc testNewWebpEncoder(t *testing.T) {\n\ttestCases := []struct {\n\t\tname     string\n\t\tfilename string\n\t}{\n\t\t{\"No ICC Profile\", \"testdata/tears_of_steel_no_icc.webp\"},\n\t\t{\"With ICC Profile\", \"testdata/tears_of_steel_icc.webp\"},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\ttestWebPImage, err := os.ReadFile(tc.filename)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Unexpected error while reading webp image: %v\", err)\n\t\t\t}\n\t\t\tdecoder, err := newWebpDecoder(testWebPImage)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Unexpected error while decoding webp image data: %v\", err)\n\t\t\t}\n\t\t\tdefer decoder.Close()\n\n\t\t\tdstBuf := make([]byte, destinationBufferSize)\n\t\t\tencoder, err := newWebpEncoder(decoder, dstBuf)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Unexpected error: %v\", err)\n\t\t\t}\n\t\t\tdefer encoder.Close()\n\t\t})\n\t}\n}\n\nfunc testWebpDecoderDecodeTo(t *testing.T) {\n\ttestCases := []struct {\n\t\tname     string\n\t\tfilename string\n\t}{\n\t\t{\"No ICC Profile\", \"testdata/tears_of_steel_no_icc.webp\"},\n\t\t{\"With ICC Profile\", \"testdata/tears_of_steel_icc.webp\"},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\ttestWebPImage, err := os.ReadFile(tc.filename)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to read webp image: %v\", err)\n\t\t\t}\n\t\t\tdecoder, err := newWebpDecoder(testWebPImage)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to create a new webp decoder: %v\", err)\n\t\t\t}\n\t\t\tdefer decoder.Close()\n\n\t\t\theader, err := decoder.Header()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to get the header: %v\", err)\n\t\t\t}\n\t\t\tframebuffer := NewFramebuffer(header.width, header.height)\n\t\t\tif err = decoder.DecodeTo(framebuffer); err != nil {\n\t\t\t\tt.Errorf(\"DecodeTo failed unexpectedly: %v\", err)\n\t\t\t}\n\t\t})\n\t}\n\n\t// Test invalid framebuffer\n\tt.Run(\"Invalid Framebuffer\", func(t *testing.T) {\n\t\ttestWebPImage, _ := os.ReadFile(\"testdata/tears_of_steel_no_icc.webp\")\n\t\tdecoder, _ := newWebpDecoder(testWebPImage)\n\t\tdefer decoder.Close()\n\n\t\tif err := decoder.DecodeTo(nil); err == nil {\n\t\t\tt.Error(\"DecodeTo with nil framebuffer should fail, but it did not\")\n\t\t}\n\t})\n}\n\nfunc testWebpEncoderEncode(t *testing.T) {\n\ttestCases := []struct {\n\t\tname     string\n\t\tfilename string\n\t\tquality  int\n\t}{\n\t\t{\"No ICC Profile\", \"testdata/tears_of_steel_no_icc.webp\", 60},\n\t\t{\"With ICC Profile\", \"testdata/tears_of_steel_icc.webp\", 80},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\ttestWebPImage, err := os.ReadFile(tc.filename)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to read webp image: %v\", err)\n\t\t\t}\n\n\t\t\tdecoder, err := newWebpDecoder(testWebPImage)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to create a new webp decoder: %v\", err)\n\t\t\t}\n\t\t\tdefer decoder.Close()\n\n\t\t\theader, err := decoder.Header()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to get the header: %v\", err)\n\t\t\t}\n\t\t\tframebuffer := NewFramebuffer(header.width, header.height)\n\t\t\tif err = framebuffer.resizeMat(header.width, header.height, header.pixelType); err != nil {\n\t\t\t\tt.Fatalf(\"Failed to resize the framebuffer: %v\", err)\n\t\t\t}\n\n\t\t\tdstBuf := make([]byte, destinationBufferSize)\n\t\t\tencoder, err := newWebpEncoder(decoder, dstBuf)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to create a new webp encoder: %v\", err)\n\t\t\t}\n\t\t\tdefer encoder.Close()\n\n\t\t\toptions := map[int]int{WebpQuality: tc.quality}\n\t\t\tencodedData, err := encoder.Encode(framebuffer, options)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Encode failed unexpectedly: %v\", err)\n\t\t\t}\n\t\t\tif encodedData, err = encoder.Encode(nil, options); err != nil {\n\t\t\t\tt.Fatalf(\"Encode of empty frame failed unexpectedly: %v\", err)\n\t\t\t}\n\t\t\tif len(encodedData) == 0 {\n\t\t\t\tt.Fatalf(\"Encoded data is empty, but it should not be\")\n\t\t\t}\n\t\t})\n\t}\n\n\t// Test nil framebuffer\n\tt.Run(\"Nil Framebuffer\", func(t *testing.T) {\n\t\ttestWebPImage, err := os.ReadFile(\"testdata/tears_of_steel_icc.webp\")\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"Failed to read webp image: %v\", err)\n\t\t}\n\n\t\tdecoder, err := newWebpDecoder(testWebPImage)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"Failed to create a new webp decoder: %v\", err)\n\t\t}\n\t\tdefer decoder.Close()\n\n\t\tdstBuf := make([]byte, destinationBufferSize)\n\t\tencoder, err := newWebpEncoder(decoder, dstBuf)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"Failed to create a new webp encoder: %v\", err)\n\t\t}\n\t\tdefer encoder.Close()\n\n\t\theader, _ := decoder.Header()\n\t\tframebuffer := NewFramebuffer(header.width, header.height)\n\t\toptions := map[int]int{}\n\t\tif err = framebuffer.resizeMat(header.width, header.height, header.pixelType); err != nil {\n\t\t\tt.Fatalf(\"Failed to resize the framebuffer: %v\", err)\n\t\t}\n\n\t\tif _, err = encoder.Encode(nil, options); err == nil {\n\t\t\tt.Error(\"Encoding a nil framebuffer should fail, but it did not\")\n\t\t}\n\t})\n}\n\nfunc testNewWebpEncoderWithAnimatedWebPSource(t *testing.T) {\n\ttestCases := []struct {\n\t\tname                  string\n\t\tinputPath             string\n\t\toutputPath            string\n\t\twidth                 int\n\t\theight                int\n\t\tquality               int\n\t\tresizeMethod          ImageOpsSizeMethod\n\t\tdisableAnimatedOutput bool\n\t\tencodeTimeout         time.Duration\n\t}{\n\t\t{\n\t\t\tname:          \"Animated WebP - Party Discord\",\n\t\t\tinputPath:     \"testdata/party-discord.webp\",\n\t\t\toutputPath:    \"testdata/out/party-discord_out_webpsource_resize.webp\",\n\t\t\twidth:         26,\n\t\t\theight:        17,\n\t\t\tquality:       60,\n\t\t\tresizeMethod:  ImageOpsResize,\n\t\t\tencodeTimeout: 300,\n\t\t},\n\t\t{\n\t\t\tname:          \"Animated WebP - Resize #1\",\n\t\t\tinputPath:     \"testdata/ferry_sunset.webp\",\n\t\t\toutputPath:    \"testdata/out/ferry_sunset_out_resize.webp\",\n\t\t\twidth:         266,\n\t\t\theight:        99,\n\t\t\tquality:       60,\n\t\t\tresizeMethod:  ImageOpsResize,\n\t\t\tencodeTimeout: 300,\n\t\t},\n\t\t{\n\t\t\tname:          \"Animated WebP - Resize #2\",\n\t\t\tinputPath:     \"testdata/animated-webp-supported.webp\",\n\t\t\toutputPath:    \"testdata/out/animated-webp-supported_out_resize.webp\",\n\t\t\twidth:         400,\n\t\t\theight:        400,\n\t\t\tquality:       60,\n\t\t\tresizeMethod:  ImageOpsResize,\n\t\t\tencodeTimeout: 300,\n\t\t},\n\t\t{\n\t\t\tname:          \"Animated WebP - Fit #1\",\n\t\t\tinputPath:     \"testdata/animated-webp-supported.webp\",\n\t\t\toutputPath:    \"testdata/out/animated-webp-supported_out_fit.webp\",\n\t\t\twidth:         400,\n\t\t\theight:        400,\n\t\t\tquality:       80,\n\t\t\tresizeMethod:  ImageOpsFit,\n\t\t\tencodeTimeout: 300,\n\t\t},\n\t\t{\n\t\t\tname:          \"Animated WebP - No resize\",\n\t\t\tinputPath:     \"testdata/animated-webp-supported.webp\",\n\t\t\toutputPath:    \"testdata/out/animated-webp-supported_out_no_resize.webp\",\n\t\t\twidth:         0,\n\t\t\theight:        0,\n\t\t\tquality:       100,\n\t\t\tresizeMethod:  ImageOpsNoResize,\n\t\t\tencodeTimeout: 300,\n\t\t},\n\t\t{\n\t\t\tname:          \"Animated WebP - Fit #2\",\n\t\t\tinputPath:     \"testdata/animated-webp-supported.webp\",\n\t\t\toutputPath:    \"testdata/out/animated-webp-supported_out.webp\",\n\t\t\twidth:         200,\n\t\t\theight:        200,\n\t\t\tquality:       60,\n\t\t\tresizeMethod:  ImageOpsFit,\n\t\t\tencodeTimeout: 300,\n\t\t},\n\t\t{\n\t\t\tname:                  \"Animated WebP - create single frame\",\n\t\t\tinputPath:             \"testdata/big_buck_bunny_720_5s.webp\",\n\t\t\toutputPath:            \"testdata/out/big_buck_bunny_720_5s_single_frame_out.webp\",\n\t\t\twidth:                 200,\n\t\t\theight:                200,\n\t\t\tquality:               60,\n\t\t\tresizeMethod:          ImageOpsFit,\n\t\t\tdisableAnimatedOutput: true,\n\t\t\tencodeTimeout:         300,\n\t\t},\n\t\t{\n\t\t\tname:                  \"Animated WebP - create single frame without encode timeout set\",\n\t\t\tinputPath:             \"testdata/big_buck_bunny_720_5s.webp\",\n\t\t\toutputPath:            \"testdata/out/big_buck_bunny_720_5s_single_frame_out_without_encode_timeout_set.webp\",\n\t\t\twidth:                 200,\n\t\t\theight:                200,\n\t\t\tquality:               60,\n\t\t\tresizeMethod:          ImageOpsFit,\n\t\t\tdisableAnimatedOutput: true,\n\t\t},\n\t\t{\n\t\t\tname:          \"Animated WebP - Crashing input\",\n\t\t\tinputPath:     \"testdata/8202024-BGS-Headless-Horseman-OO-1200x1200-optimize.webp\",\n\t\t\toutputPath:    \"testdata/out/8202024-BGS-Headless-Horseman-OO-1200x1200-optimize_out.webp\",\n\t\t\twidth:         200,\n\t\t\theight:        200,\n\t\t\tquality:       60,\n\t\t\tresizeMethod:  ImageOpsFit,\n\t\t\tencodeTimeout: 300,\n\t\t},\n\t\t{\n\t\t\tname:          \"Animated WebP - complex dispose and blend\",\n\t\t\tinputPath:     \"testdata/complex_dispose_and_blend.webp\",\n\t\t\toutputPath:    \"testdata/out/complex_dispose_and_blend_out.webp\",\n\t\t\twidth:         960,\n\t\t\theight:        540,\n\t\t\tquality:       60,\n\t\t\tresizeMethod:  ImageOpsFit,\n\t\t\tencodeTimeout: 300,\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tvar err error\n\t\t\tvar testWebPImage []byte\n\t\t\tvar decoder *webpDecoder\n\n\t\t\t// Read the input WebP file\n\t\t\ttestWebPImage, err = os.ReadFile(tc.inputPath)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"Unexpected error while reading %s: %v\", tc.inputPath, err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Decode the WebP image\n\t\t\tif decoder, err = newWebpDecoder(testWebPImage); err != nil {\n\t\t\t\tt.Errorf(\"Unexpected error while decoding %s: %v\", tc.inputPath, err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tdstBuf := make([]byte, destinationBufferSize)\n\n\t\t\toptions := &ImageOptions{\n\t\t\t\tFileType:              \".webp\",\n\t\t\t\tNormalizeOrientation:  true,\n\t\t\t\tEncodeOptions:         map[int]int{WebpQuality: tc.quality},\n\t\t\t\tResizeMethod:          tc.resizeMethod,\n\t\t\t\tWidth:                 tc.width,\n\t\t\t\tHeight:                tc.height,\n\t\t\t\tEncodeTimeout:         time.Second * tc.encodeTimeout,\n\t\t\t\tDisableAnimatedOutput: tc.disableAnimatedOutput,\n\t\t\t}\n\n\t\t\tops := NewImageOps(2000)\n\t\t\tvar newDst []byte\n\t\t\tnewDst, err = ops.Transform(decoder, options, dstBuf)\n\t\t\tif err != nil {\n\t\t\t\tdecoder.Close()\n\t\t\t\tops.Close()\n\t\t\t\tt.Errorf(\"Transform() error for %s: %v\", tc.inputPath, err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// verify length of newDst\n\t\t\tif len(newDst) == 0 {\n\t\t\t\tdecoder.Close()\n\t\t\t\tops.Close()\n\t\t\t\tt.Errorf(\"Transform() returned empty data for %s\", tc.inputPath)\n\t\t\t}\n\n\t\t\t// write the new WebP image to disk\n\t\t\tif tc.outputPath != \"\" {\n\t\t\t\t// create output directory if it does not exist\n\t\t\t\tif _, err := os.Stat(\"testdata/out\"); os.IsNotExist(err) {\n\t\t\t\t\tif err = os.Mkdir(\"testdata/out\", 0755); err != nil {\n\t\t\t\t\t\tdecoder.Close()\n\t\t\t\t\t\tops.Close()\n\t\t\t\t\t\tt.Errorf(\"Failed to create output directory: %v\", err)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif err = os.WriteFile(tc.outputPath, newDst, 0644); err != nil {\n\t\t\t\t\tdecoder.Close()\n\t\t\t\t\tops.Close()\n\t\t\t\t\tt.Errorf(\"Failed to write %s: %v\", tc.outputPath, err)\n\t\t\t\t}\n\t\t\t}\n\t\t\tdecoder.Close()\n\t\t\tops.Close()\n\t\t})\n\t}\n}\n\nfunc testNewWebpEncoderWithAnimatedGIFSource(t *testing.T) {\n\ttestCases := []struct {\n\t\tname         string\n\t\tinputPath    string\n\t\toutputPath   string\n\t\twidth        int\n\t\theight       int\n\t\tquality      int\n\t\tresizeMethod ImageOpsSizeMethod\n\t\twantLoops    int\n\t}{\n\t\t{\n\t\t\tname:         \"Animated GIF with alpha channel\",\n\t\t\tinputPath:    \"testdata/party-discord.gif\",\n\t\t\toutputPath:   \"testdata/out/party-discord_out_resize.webp\",\n\t\t\twidth:        27,\n\t\t\theight:       17,\n\t\t\tquality:      80,\n\t\t\tresizeMethod: ImageOpsResize,\n\t\t\twantLoops:    0,\n\t\t},\n\t\t{\n\t\t\tname:         \"Animated GIF with specific loop count\",\n\t\t\tinputPath:    \"testdata/no-loop.gif\",\n\t\t\toutputPath:   \"testdata/out/no-loop_out.webp\",\n\t\t\twidth:        200,\n\t\t\theight:       200,\n\t\t\tquality:      80,\n\t\t\tresizeMethod: ImageOpsResize,\n\t\t\twantLoops:    1,\n\t\t},\n\t\t{\n\t\t\tname:         \"Animated GIF with duplicate number of loop count, use the first loop count\",\n\t\t\tinputPath:    \"testdata/duplicate_number_of_loops.gif\",\n\t\t\toutputPath:   \"testdata/out/duplicate_number_of_loops.webp\",\n\t\t\twidth:        200,\n\t\t\theight:       200,\n\t\t\tquality:      80,\n\t\t\tresizeMethod: ImageOpsResize,\n\t\t\twantLoops:    2,\n\t\t},\n\t\t{\n\t\t\tname:         \"Animated GIF with multiple extension blocks\",\n\t\t\tinputPath:    \"testdata/dispose_bgnd.gif\",\n\t\t\toutputPath:   \"testdata/out/dispose_bgnd.webp\",\n\t\t\twidth:        200,\n\t\t\theight:       200,\n\t\t\tquality:      80,\n\t\t\tresizeMethod: ImageOpsResize,\n\t\t\twantLoops:    0,\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tvar err error\n\t\t\tvar testWebPImage []byte\n\t\t\tvar decoder *gifDecoder\n\n\t\t\t// Read the input GIF file\n\t\t\ttestWebPImage, err = os.ReadFile(tc.inputPath)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"Unexpected error while reading %s: %v\", tc.inputPath, err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Decode the GIF image\n\t\t\tif decoder, err = newGifDecoder(testWebPImage); err != nil {\n\t\t\t\tt.Errorf(\"Unexpected error while decoding %s: %v\", tc.inputPath, err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Verify loop count\n\t\t\tif decoder.LoopCount() != tc.wantLoops {\n\t\t\t\tt.Errorf(\"Loop count = %d, want %d\", decoder.LoopCount(), tc.wantLoops)\n\t\t\t}\n\n\t\t\tdstBuf := make([]byte, destinationBufferSize)\n\n\t\t\toptions := &ImageOptions{\n\t\t\t\tFileType:             \".webp\",\n\t\t\t\tNormalizeOrientation: true,\n\t\t\t\tEncodeOptions:        map[int]int{WebpQuality: tc.quality},\n\t\t\t\tResizeMethod:         tc.resizeMethod,\n\t\t\t\tWidth:                tc.width,\n\t\t\t\tHeight:               tc.height,\n\t\t\t\tEncodeTimeout:        time.Second * 300,\n\t\t\t}\n\n\t\t\tops := NewImageOps(2000)\n\t\t\tvar newDst []byte\n\t\t\tnewDst, err = ops.Transform(decoder, options, dstBuf)\n\t\t\tif err != nil {\n\t\t\t\tdecoder.Close()\n\t\t\t\tops.Close()\n\t\t\t\tt.Errorf(\"Transform() error for %s: %v\", tc.inputPath, err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// verify length of newDst\n\t\t\tif len(newDst) == 0 {\n\t\t\t\tdecoder.Close()\n\t\t\t\tops.Close()\n\t\t\t\tt.Errorf(\"Transform() returned empty data for %s\", tc.inputPath)\n\t\t\t}\n\n\t\t\t// write the new WebP image to disk\n\t\t\tif tc.outputPath != \"\" {\n\t\t\t\t// create output directory if it does not exist\n\t\t\t\tif _, err := os.Stat(\"testdata/out\"); os.IsNotExist(err) {\n\t\t\t\t\tif err = os.Mkdir(\"testdata/out\", 0755); err != nil {\n\t\t\t\t\t\tdecoder.Close()\n\t\t\t\t\t\tops.Close()\n\t\t\t\t\t\tt.Errorf(\"Failed to create output directory: %v\", err)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif err = os.WriteFile(tc.outputPath, newDst, 0644); err != nil {\n\t\t\t\t\tdecoder.Close()\n\t\t\t\t\tops.Close()\n\t\t\t\t\tt.Errorf(\"Failed to write %s: %v\", tc.outputPath, err)\n\t\t\t\t}\n\t\t\t}\n\t\t\tdecoder.Close()\n\t\t\tops.Close()\n\t\t})\n\t}\n}\n"
        }
      ]
    }
  ]
}