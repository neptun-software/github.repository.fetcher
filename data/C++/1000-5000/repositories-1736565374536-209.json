{
  "metadata": {
    "timestamp": 1736565374536,
    "page": 209,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "apache/kvrocks",
      "stars": 3639,
      "defaultBranch": "unstable",
      "files": [
        {
          "name": ".asf.yaml",
          "type": "blob",
          "size": 1.8720703125,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# For more information, see https://cwiki.apache.org/confluence/display/INFRA/Git+-+.asf.yaml+features.\n\ngithub:\n  description: >-\n    Apache Kvrocks is a distributed key value NoSQL database that uses RocksDB as storage engine\n    and is compatible with Redis protocol.\n  homepage: https://kvrocks.apache.org/\n  labels:\n    - database\n    - distributed\n    - kv\n    - namespace\n    - redis\n    - redis-cluster\n  enabled_merge_buttons:\n    squash:  true\n    merge:   false\n    rebase:  true\n  protected_branches:\n    unstable:\n      required_pull_request_reviews:\n        dismiss_stale_reviews: true\n        required_approving_review_count: 1\n      required_status_checks:\n        strict: true\n        contexts:\n          - Required\n    '1.3': {}\n    '2.0': {}\n    '2.1': {}\n    '2.2': {}\n    '2.3': {}\n    '2.4': {}\n    '2.5': {}\n    '2.6': {}\n    '2.7': {}\n    '2.8': {}\n    '2.9': {}\n    '2.10': {}\n\nnotifications:\n  commits:      commits@kvrocks.apache.org\n  issues:       issues@kvrocks.apache.org\n  pullrequests: issues@kvrocks.apache.org\n  jobs:         builds@kvrocks.apache.org\n  discussions:  issues@kvrocks.apache.org\n"
        },
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 5.2470703125,
          "content": "---\nLanguage:        Cpp\n# BasedOnStyle:  Google\nAccessModifierOffset: -1\nAlignAfterOpenBracket: Align\nAlignConsecutiveMacros: None\nAlignConsecutiveAssignments: None\nAlignConsecutiveBitFields: None\nAlignConsecutiveDeclarations: None\nAlignEscapedNewlines: Left\nAlignOperands:   Align\nAlignTrailingComments: true\nAllowAllArgumentsOnNextLine: true\nAllowAllConstructorInitializersOnNextLine: true\nAllowAllParametersOfDeclarationOnNextLine: true\nAllowShortEnumsOnASingleLine: true\nAllowShortBlocksOnASingleLine: Never\nAllowShortCaseLabelsOnASingleLine: false\nAllowShortFunctionsOnASingleLine: All\nAllowShortLambdasOnASingleLine: All\nAllowShortIfStatementsOnASingleLine: WithoutElse\nAllowShortLoopsOnASingleLine: true\nAlwaysBreakAfterDefinitionReturnType: None\nAlwaysBreakAfterReturnType: None\nAlwaysBreakBeforeMultilineStrings: true\nAlwaysBreakTemplateDeclarations: Yes\nAttributeMacros:\n  - __capability\nBinPackArguments: true\nBinPackParameters: true\nBraceWrapping:\n  AfterCaseLabel:  false\n  AfterClass:      false\n  AfterControlStatement: Never\n  AfterEnum:       false\n  AfterFunction:   false\n  AfterNamespace:  false\n  AfterObjCDeclaration: false\n  AfterStruct:     false\n  AfterUnion:      false\n  AfterExternBlock: false\n  BeforeCatch:     false\n  BeforeElse:      false\n  BeforeLambdaBody: false\n  BeforeWhile:     false\n  IndentBraces:    false\n  SplitEmptyFunction: true\n  SplitEmptyRecord: true\n  SplitEmptyNamespace: true\nBreakBeforeBinaryOperators: None\nBreakBeforeConceptDeclarations: true\nBreakBeforeBraces: Attach\nBreakBeforeInheritanceComma: false\nBreakInheritanceList: BeforeColon\nBreakBeforeTernaryOperators: true\nBreakConstructorInitializersBeforeComma: false\nBreakConstructorInitializers: BeforeColon\nBreakAfterJavaFieldAnnotations: false\nBreakStringLiterals: true\nColumnLimit:     120\nCommentPragmas:  '^ IWYU pragma:'\nCompactNamespaces: false\nConstructorInitializerAllOnOneLineOrOnePerLine: true\nConstructorInitializerIndentWidth: 4\nContinuationIndentWidth: 4\nCpp11BracedListStyle: true\nDeriveLineEnding: true\nDerivePointerAlignment: true\nDisableFormat:   false\nEmptyLineBeforeAccessModifier: LogicalBlock\nExperimentalAutoDetectBinPacking: false\nFixNamespaceComments: true\nForEachMacros:\n  - foreach\n  - Q_FOREACH\n  - BOOST_FOREACH\nStatementAttributeLikeMacros:\n  - Q_EMIT\nIncludeBlocks:   Regroup\nIncludeCategories:\n  - Regex:           '^<ext/.*\\.h>'\n    Priority:        2\n    SortPriority:    0\n    CaseSensitive:   false\n  - Regex:           '^<.*\\.h>'\n    Priority:        1\n    SortPriority:    0\n    CaseSensitive:   false\n  - Regex:           '^<.*'\n    Priority:        2\n    SortPriority:    0\n    CaseSensitive:   false\n  - Regex:           '.*'\n    Priority:        3\n    SortPriority:    0\n    CaseSensitive:   false\nIncludeIsMainRegex: '([-_](test|unittest))?$'\nIncludeIsMainSourceRegex: ''\nIndentCaseLabels: true\nIndentCaseBlocks: false\nIndentGotoLabels: true\nIndentPPDirectives: None\nIndentExternBlock: AfterExternBlock\nIndentRequires:  false\nIndentWidth:     2\nIndentWrappedFunctionNames: false\nInsertTrailingCommas: None\nJavaScriptQuotes: Leave\nJavaScriptWrapImports: true\nKeepEmptyLinesAtTheStartOfBlocks: false\nMacroBlockBegin: ''\nMacroBlockEnd:   ''\nMaxEmptyLinesToKeep: 1\nNamespaceIndentation: None\nObjCBinPackProtocolList: Never\nObjCBlockIndentWidth: 2\nObjCBreakBeforeNestedBlockParam: true\nObjCSpaceAfterProperty: false\nObjCSpaceBeforeProtocolList: true\nPenaltyBreakAssignment: 2\nPenaltyBreakBeforeFirstCallParameter: 1\nPenaltyBreakComment: 300\nPenaltyBreakFirstLessLess: 120\nPenaltyBreakString: 1000\nPenaltyBreakTemplateDeclaration: 10\nPenaltyExcessCharacter: 1000000\nPenaltyReturnTypeOnItsOwnLine: 200\nPenaltyIndentedWhitespace: 0\nPointerAlignment: Left\nRawStringFormats:\n  - Language:        Cpp\n    Delimiters:\n      - cc\n      - CC\n      - cpp\n      - Cpp\n      - CPP\n      - 'c++'\n      - 'C++'\n    CanonicalDelimiter: ''\n    BasedOnStyle:    google\n  - Language:        TextProto\n    Delimiters:\n      - pb\n      - PB\n      - proto\n      - PROTO\n    EnclosingFunctions:\n      - EqualsProto\n      - EquivToProto\n      - PARSE_PARTIAL_TEXT_PROTO\n      - PARSE_TEST_PROTO\n      - PARSE_TEXT_PROTO\n      - ParseTextOrDie\n      - ParseTextProtoOrDie\n      - ParseTestProto\n      - ParsePartialTestProto\n    CanonicalDelimiter: ''\n    BasedOnStyle:    google\nReflowComments:  true\nSortIncludes:    true\nSortJavaStaticImport: Before\nSortUsingDeclarations: true\nSpaceAfterCStyleCast: false\nSpaceAfterLogicalNot: false\nSpaceAfterTemplateKeyword: true\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeCaseColon: false\nSpaceBeforeCpp11BracedList: false\nSpaceBeforeCtorInitializerColon: true\nSpaceBeforeInheritanceColon: true\nSpaceBeforeParens: ControlStatements\nSpaceAroundPointerQualifiers: Default\nSpaceBeforeRangeBasedForLoopColon: true\nSpaceInEmptyBlock: false\nSpaceInEmptyParentheses: false\nSpacesBeforeTrailingComments: 2\nSpacesInAngles:  false\nSpacesInConditionalStatement: false\nSpacesInContainerLiterals: true\nSpacesInCStyleCastParentheses: false\nSpacesInParentheses: false\nSpacesInSquareBrackets: false\nSpaceBeforeSquareBrackets: false\nBitFieldColonSpacing: Both\nStandard:        Auto\nStatementMacros:\n  - Q_UNUSED\n  - QT_REQUIRE_VERSION\nTabWidth:        8\nUseCRLF:         false\nUseTab:          Never\nWhitespaceSensitiveMacros:\n  - STRINGIZE\n  - PP_STRINGIZE\n  - BOOST_PP_STRINGIZE\n  - NS_SWIFT_NAME\n  - CF_SWIFT_NAME\n...\n\n"
        },
        {
          "name": ".clang-tidy",
          "type": "blob",
          "size": 4.3994140625,
          "content": "# refer to https://clang.llvm.org/extra/clang-tidy/checks/list.html\nChecks: -*, clang-analyzer-core.*, clang-analyzer-cplusplus.*, -clang-analyzer-cplusplus.InnerPointer, clang-analyzer-deadcode.*, clang-analyzer-nullability.*, clang-analyzer-security.*, clang-analyzer-unix.*, clang-analyzer-valist.*, cppcoreguidelines-init-variables, cppcoreguidelines-macro-usage, cppcoreguidelines-interfaces-global-init, cppcoreguidelines-narrowing-conversions, cppcoreguidelines-no-malloc, cppcoreguidelines-prefer-member-initializer, cppcoreguidelines-special-member-functions, cppcoreguidelines-slicing, google-build-explicit-make-pair, google-default-arguments, google-explicit-constructor, modernize-avoid-bind, modernize-loop-convert, modernize-macro-to-enum, modernize-make-shared, modernize-make-unique, modernize-pass-by-value, modernize-redundant-void-arg, modernize-return-braced-init-list, modernize-use-auto, modernize-use-bool-literals, modernize-use-emplace, modernize-use-equals-default, modernize-use-equals-delete, modernize-use-nullptr, modernize-use-override, modernize-use-using, performance-faster-string-find, performance-for-range-copy, performance-implicit-conversion-in-loop, performance-inefficient-algorithm, performance-inefficient-vector-operation, performance-move-const-arg, performance-move-constructor-init, performance-no-automatic-move, performance-trivially-destructible, performance-type-promotion-in-math-fn, performance-unnecessary-copy-initialization, performance-unnecessary-value-param, readability-avoid-const-params-in-decls, readability-const-return-type, readability-convert-member-functions-to-static, readability-make-member-function-const, readability-redundant-access-specifiers, readability-redundant-control-flow, readability-redundant-declaration, readability-redundant-member-init, readability-redundant-string-cstr, readability-redundant-string-init, readability-simplify-boolean-expr, readability-simplify-subscript-expr, readability-string-compare, readability-identifier-naming, cppcoreguidelines-avoid-goto, bugprone-use-after-move\n\nWarningsAsErrors: clang-analyzer-*, -clang-analyzer-security.insecureAPI.rand, google-*, performance-*, cppcoreguidelines-*, modernize-*, readability-*, bugprone-*\n\nCheckOptions:\n  - key:           cppcoreguidelines-special-member-functions.AllowSoleDefaultDtor\n    value:         True\n  - key:           cppcoreguidelines-special-member-functions.AllowMissingMoveFunctionsWhenCopyIsDeleted\n    value:         True\n  - key:           performance-move-const-arg.CheckTriviallyCopyableMove\n    value:         False\n  - key:           readability-identifier-naming.LocalVariableCase\n    value:         lower_case\n  - key:           readability-identifier-naming.LocalConstantCase\n    value:         aNy_CasE  # FIXME: use a more strictly case\n  - key:           readability-identifier-naming.StructCase\n    value:         CamelCase\n  - key:           readability-identifier-naming.ClassCase\n    value:         CamelCase\n  - key:           readability-identifier-naming.UnionCase\n    value:         CamelCase\n  - key:           readability-identifier-naming.EnumCase\n    value:         CamelCase\n  - key:           readability-identifier-naming.ParameterCase\n    value:         lower_case\n  - key:           readability-identifier-naming.PublicMethodCase\n    value:         CamelCase\n  - key:           readability-identifier-naming.PrivateMethodCase\n    value:         camelBack\n  - key:           readability-identifier-naming.ProtectedMethodCase\n    value:         camelBack\n  - key:           readability-identifier-naming.MemberCase\n    value:         lower_case\n  - key:           readability-identifier-naming.MemberSuffix\n    value:         _\n  - key:           readability-identifier-naming.PublicMemberCase\n    value:         lower_case\n  - key:           readability-identifier-naming.ClassMemberCase\n    value:         lower_case\n  - key:           readability-identifier-naming.ClassConstantCase\n    value:         aNy_CasE  # FIXME: use a more strictly case\n  - key:           readability-identifier-naming.TypeAliasCase\n    value:         CamelCase\n  - key:           readability-identifier-naming.TypedefCase\n    value:         CamelCase\n  - key:           readability-identifier-naming.MacroCase\n    value:         UPPER_CASE\n  - key:           readability-identifier-naming.NamespaceCase\n    value:         lower_case\n  - key:           readability-identifier-naming.FunctionCase\n    value:         CamelCase\n"
        },
        {
          "name": ".devcontainer",
          "type": "tree",
          "content": null
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.3896484375,
          "content": "/.github\n/tests\n/Dockerfile\n\n# Prerequisites\n*.d\n\n# Compiled Object files\n*.slo\n*.lo\n*.o\n*.obj\n\n# Precompiled Headers\n*.gch\n*.pch\n\n# Compiled Dynamic libraries\n*.so\n*.dylib\n*.dll\n\n# Fortran module files\n*.mod\n*.smod\n\n# Compiled Static libraries\n*.lai\n*.la\n*.a\n*.lib\n\n# Executables\n*.exe\n*.out\n*.app\n\n*.pyc\n*.swp\n*.swo\n.DS_Store\nversion.h\n\n.idea\n.vscode\n.cache\n\ncompactdb\ntestdb\n\nbuild\ncmake-build-*\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.369140625,
          "content": "# Prerequisites\n*.d\n\n# Compiled Object files\n*.slo\n*.lo\n*.o\n*.obj\n\n# Precompiled Headers\n*.gch\n*.pch\n\n# Compiled Dynamic libraries\n*.so\n*.dylib\n*.dll\n\n# Fortran module files\n*.mod\n*.smod\n\n# Compiled Static libraries\n*.lai\n*.la\n*.a\n*.lib\n\n# Executables\n*.exe\n*.out\n*.app\n\n*.pyc\n*.swp\n*.swo\n.DS_Store\nversion.h\n\n.idea\n.vscode\n.cache\n\ncompactdb\ntestdb\n\nbuild\ncmake-build-*\nbuild-*\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 12.2392578125,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\ncmake_minimum_required(VERSION 3.16)\nproject(kvrocks\n        DESCRIPTION \"NoSQL which is based on RocksDB and compatible with the Redis protocol\"\n        LANGUAGES CXX)\n\noption(DISABLE_JEMALLOC \"disable use of the jemalloc library\" OFF)\noption(ENABLE_ASAN \"enable address sanitizer\" OFF)\noption(ENABLE_TSAN \"enable thread sanitizer\" OFF)\noption(ENABLE_UBSAN \"enable undefined behavior sanitizer\" OFF)\noption(ASAN_WITH_LSAN \"enable leak sanitizer while address sanitizer is enabled\" ON)\noption(ENABLE_STATIC_LIBSTDCXX \"link kvrocks with static library of libstd++ instead of shared library\" ON)\noption(ENABLE_LUAJIT \"enable use of luaJIT instead of lua\" ON)\noption(ENABLE_OPENSSL \"enable openssl to support tls connection\" OFF)\noption(ENABLE_IPO \"enable interprocedural optimization\" ON)\nset(SYMBOLIZE_BACKEND \"\" CACHE STRING \"symbolization backend library for cpptrace (libbacktrace, libdwarf, or empty)\")\nset(PORTABLE 0 CACHE STRING \"build a portable binary (disable arch-specific optimizations)\")\n# TODO: set ENABLE_NEW_ENCODING to ON when we are ready\noption(ENABLE_NEW_ENCODING \"enable new encoding (#1033) for storing 64bit size and expire time in milliseconds\" ON)\n\nif (CMAKE_VERSION VERSION_GREATER_EQUAL \"3.24.0\")\n    cmake_policy(SET CMP0135 NEW)\nendif()\n\nfind_package(Backtrace REQUIRED)\n\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n    if(CMAKE_CXX_COMPILER_VERSION VERSION_LESS 8)\n        message(FATAL_ERROR \"It is expected to build kvrocks with GCC 8 or above\")\n    endif()\nelseif(CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n    if(CMAKE_CXX_COMPILER_VERSION VERSION_LESS 8)\n        message(FATAL_ERROR \"It is expected to build kvrocks with Clang 8 or above\")\n    endif()\nelseif(CMAKE_CXX_COMPILER_ID STREQUAL \"AppleClang\")\n    if(CMAKE_CXX_COMPILER_VERSION VERSION_LESS 11)\n        message(FATAL_ERROR \"It is expected to build kvrocks with Xcode toolchains 11 or above\")\n    endif()\nelse()\n    message(WARNING \"The compiler you are currently using is not officially supported,\n        so you can try switching to GCC>=8 or Clang>=8 if you encounter problems\")\nendif()\n\nif(CMAKE_GENERATOR STREQUAL \"Ninja\")\n    set(MAKE_COMMAND make)\n    set(NINJA_MAKE_JOBS 4 CACHE STRING \"specify concurrent level while ninja calling make\")\n    set(NINJA_MAKE_JOBS_FLAG -j${NINJA_MAKE_JOBS})\nelse()\n    set(MAKE_COMMAND $(MAKE))\nendif()\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\nset(DEPS_FETCH_PROXY \"\" CACHE STRING\n    \"a template URL to proxy the traffic for fetching dependencies, e.g. with DEPS_FETCH_PROXY = https://some-proxy/,\n     https://example/some-dep.zip -> https://some-proxy/https://example/some-dep.zip\")\n\nif(ENABLE_ASAN AND ENABLE_TSAN)\n    message(FATAL_ERROR \"ASan and TSan cannot be used at the same time\")\nendif()\n\nif((ENABLE_ASAN OR ENABLE_TSAN) AND (NOT DISABLE_JEMALLOC))\n    message(FATAL_ERROR \"ASan/TSan does not work well with JeMalloc\")\nendif()\n\nif(ENABLE_ASAN)\n    if(ASAN_WITH_LSAN)\n        if((CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\") AND (CMAKE_CXX_COMPILER_VERSION VERSION_LESS \"5\"))\n            message(FATAL_ERROR \"leak sanitizer is not supported until gcc 5\")\n        endif()\n        set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fsanitize=leak\")\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fsanitize=leak\")\n        set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -fsanitize=leak\")\n    endif()\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fsanitize=address\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fsanitize=address\")\n    set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -fsanitize=address\")\nendif()\n\n# Copied from https://github.com/apache/arrow/blob/main/cpp/cmake_modules/san-config.cmake\n#\n# Flag to enable clang undefined behavior sanitizer\n# We explicitly don't enable all of the sanitizer flags:\n# - disable 'vptr' because of RTTI issues across shared libraries (?)\n# - disable 'alignment' because unaligned access is really OK on Nehalem and we do it\n#   all over the place.\n# - disable 'function' because it appears to give a false positive\n#   (https://github.com/google/sanitizers/issues/911)\n# - disable 'float-divide-by-zero' on clang, which considers it UB\n#   (https://bugs.llvm.org/show_bug.cgi?id=17000#c1)\n#   Note: GCC does not support the 'function' flag.\nif(ENABLE_UBSAN)\n    if(CMAKE_CXX_COMPILER_ID STREQUAL \"AppleClang\" OR CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fsanitize=undefined -fno-sanitize=alignment,vptr,function,float-divide-by-zero\")\n        set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fsanitize=undefined -fno-sanitize=alignment,vptr,function,float-divide-by-zero\")\n    elseif(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL \"5.1\")\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fsanitize=undefined -fno-sanitize=alignment,vptr\")\n        set(CMAKE_C_FLAGS \"${CMAKE_CXX_FLAGS} -fsanitize=undefined -fno-sanitize=alignment,vptr\")\n    else()\n        message(FATAL_ERROR \"Cannot use UBSAN without clang or gcc >= 5.1\")\n    endif()\n    set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -fsanitize=undefined\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fno-sanitize-recover=all\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fno-sanitize-recover=all\")\nendif()\nif(ENABLE_TSAN)\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fsanitize=thread\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fsanitize=thread\")\n    set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -fsanitize=thread\")\nendif()\n\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n\n# GLIBC < 2.17 should explicitly specify the real-time library when using clock_*\nfind_library(REALTIME_LIB rt)\nif (REALTIME_LIB)\n    list(APPEND EXTERNAL_LIBS PRIVATE rt)\nendif()\n\nif (CMAKE_HOST_APPLE)\n    set(DISABLE_JEMALLOC ON)\n    set(ENABLE_IPO OFF)\nendif ()\n\nif(NOT DISABLE_JEMALLOC)\n    include(cmake/jemalloc.cmake)\n    list(APPEND EXTERNAL_LIBS PRIVATE jemalloc)\nendif()\n\nset(BUILD_SHARED_LIBS OFF CACHE BOOL \"do not build shared libs by default\")\n\nif(ENABLE_OPENSSL)\n    find_package(OpenSSL REQUIRED)\nendif()\n\ninclude(cmake/gtest.cmake)\ninclude(cmake/glog.cmake)\ninclude(cmake/snappy.cmake)\ninclude(cmake/lz4.cmake)\ninclude(cmake/zlib.cmake)\ninclude(cmake/zstd.cmake)\ninclude(cmake/tbb.cmake)\ninclude(cmake/rocksdb.cmake)\ninclude(cmake/libevent.cmake)\ninclude(cmake/fmt.cmake)\ninclude(cmake/jsoncons.cmake)\ninclude(cmake/xxhash.cmake)\ninclude(cmake/span.cmake)\ninclude(cmake/trie.cmake)\ninclude(cmake/pegtl.cmake)\ninclude(cmake/rangev3.cmake)\ninclude(cmake/cpptrace.cmake)\n\nif (ENABLE_LUAJIT)\n    include(cmake/luajit.cmake)\nelse()\n    include(cmake/lua.cmake)\nendif()\n\nfind_package(Threads REQUIRED)\n\nlist(APPEND EXTERNAL_LIBS glog)\nlist(APPEND EXTERNAL_LIBS snappy)\nlist(APPEND EXTERNAL_LIBS rocksdb_with_headers)\nlist(APPEND EXTERNAL_LIBS event_with_headers)\nlist(APPEND EXTERNAL_LIBS lz4)\nlist(APPEND EXTERNAL_LIBS zstd)\nlist(APPEND EXTERNAL_LIBS zlib_with_headers)\nlist(APPEND EXTERNAL_LIBS fmt)\nif (ENABLE_LUAJIT)\n    list(APPEND EXTERNAL_LIBS luajit)\nelse()\n    list(APPEND EXTERNAL_LIBS lua)\nendif()\nif (ENABLE_OPENSSL)\n    list(APPEND EXTERNAL_LIBS OpenSSL::SSL)\nendif()\nlist(APPEND EXTERNAL_LIBS tbb)\nlist(APPEND EXTERNAL_LIBS jsoncons)\nlist(APPEND EXTERNAL_LIBS Threads::Threads)\nlist(APPEND EXTERNAL_LIBS ${Backtrace_LIBRARY})\nlist(APPEND EXTERNAL_LIBS xxhash)\nlist(APPEND EXTERNAL_LIBS span-lite)\nlist(APPEND EXTERNAL_LIBS tsl_hat_trie)\nlist(APPEND EXTERNAL_LIBS pegtl)\nlist(APPEND EXTERNAL_LIBS range-v3)\nlist(APPEND EXTERNAL_LIBS cpptrace::cpptrace)\n\n# Add git sha to version.h\nfind_package(Git REQUIRED)\nexecute_process(COMMAND sh -c \"cat src/VERSION.txt\"\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR} OUTPUT_VARIABLE PROJECT_VERSION)\nexecute_process(COMMAND git rev-parse --short HEAD\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR} OUTPUT_VARIABLE GIT_SHA)\nstring(STRIP \"${GIT_SHA}\" GIT_SHA)\nif ((PROJECT_VERSION STREQUAL \"unstable\") AND (GIT_SHA STREQUAL \"\"))\n\tmessage(WARNING \"It is highly recommended to build the unstable branch in a Git repo\")\nendif ()\nconfigure_file(src/version.h.in ${PROJECT_BINARY_DIR}/version.h)\n\nif ((CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\") OR (CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\"))\n    if (NOT APPLE)\n        set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -static-libgcc\")\n    endif()\n\n    if(ENABLE_STATIC_LIBSTDCXX)\n        try_compile(FOUND_STATIC_LIBSTDCXX ${PROJECT_BINARY_DIR} ${PROJECT_SOURCE_DIR}/cmake/checks/static_libstdcxx.cc\n            LINK_OPTIONS -static-libstdc++ CXX_STANDARD 11)\n\n        if(NOT FOUND_STATIC_LIBSTDCXX)\n            message(FATAL_ERROR \"cannot find static library of libstdc++, please add ENABLE_STATIC_LIBSTDCXX=OFF to disable\")\n        endif()\n\n        set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -static-libstdc++\")\n    endif()\nendif()\n\n# kvrocks objects target\nfile(GLOB_RECURSE KVROCKS_SRCS src/*.cc)\nlist(FILTER KVROCKS_SRCS EXCLUDE REGEX src/cli/main.cc)\n\nadd_library(kvrocks_objs OBJECT ${KVROCKS_SRCS})\n\ntarget_include_directories(kvrocks_objs PUBLIC src src/common src/vendor ${PROJECT_BINARY_DIR} ${Backtrace_INCLUDE_DIR})\ntarget_compile_features(kvrocks_objs PUBLIC cxx_std_17)\ntarget_compile_options(kvrocks_objs PUBLIC -Wall -Wpedantic -Wsign-compare -Wreturn-type -fno-omit-frame-pointer)\ntarget_compile_options(kvrocks_objs PUBLIC -Werror=unused-parameter)\ntarget_compile_options(kvrocks_objs PUBLIC -Werror=unused-result)\n\n# disable unused-variable check on GCC < 8 due to the structure bindings\n# https://gcc.gnu.org/bugzilla/show_bug.cgi?format=multiple&id=81767\nif (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 8)\n    target_compile_options(kvrocks_objs PUBLIC -Wno-error=unused-variable)\nelse()\n    target_compile_options(kvrocks_objs PUBLIC -Werror=unused-variable)\nendif()\n\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n    target_compile_options(kvrocks_objs PUBLIC -Wno-pedantic)\nelseif((CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\") OR (CMAKE_CXX_COMPILER_ID STREQUAL \"AppleClang\"))\n    target_compile_options(kvrocks_objs PUBLIC -Wno-gnu-statement-expression)\nendif()\ntarget_link_libraries(kvrocks_objs PUBLIC -fno-omit-frame-pointer)\ntarget_link_libraries(kvrocks_objs PUBLIC ${EXTERNAL_LIBS})\ntarget_compile_definitions(kvrocks_objs PUBLIC KVROCKS_STORAGE_ENGINE=RocksDB)\nif(ENABLE_OPENSSL)\n    target_compile_definitions(kvrocks_objs PUBLIC ENABLE_OPENSSL)\nendif()\nif(ENABLE_NEW_ENCODING)\n    target_compile_definitions(kvrocks_objs PUBLIC METADATA_ENCODING_VERSION=1)\nelse()\n    target_compile_definitions(kvrocks_objs PUBLIC METADATA_ENCODING_VERSION=0)\nendif()\n\n# disable LTO on GCC <= 9 due to an ICE\nif((CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\") AND (CMAKE_CXX_COMPILER_VERSION VERSION_LESS 10))\n    set(ENABLE_IPO OFF)\nendif()\n\nif(ENABLE_IPO)\n    include(CheckIPOSupported)\n    check_ipo_supported(RESULT ipo_result OUTPUT ipo_output LANGUAGES CXX)\n\n    if(ipo_result)\n        set_property(TARGET kvrocks_objs PROPERTY INTERPROCEDURAL_OPTIMIZATION TRUE)\n        if(CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n            target_link_libraries(kvrocks_objs PUBLIC \"-fuse-ld=lld\")\n        endif()\n    else()\n        message(WARNING \"IPO is not supported: ${ipo_output}\")\n    endif()\nendif()\n\n# kvrocks main target\nadd_executable(kvrocks src/cli/main.cc)\ntarget_link_libraries(kvrocks PRIVATE kvrocks_objs ${EXTERNAL_LIBS})\n\n# kvrocks2redis sync tool\nfile(GLOB KVROCKS2REDIS_SRCS utils/kvrocks2redis/*.cc)\nadd_executable(kvrocks2redis ${KVROCKS2REDIS_SRCS})\n\ntarget_link_libraries(kvrocks2redis PRIVATE kvrocks_objs ${EXTERNAL_LIBS})\n\n# kvrocks unit tests\nfile(GLOB_RECURSE TESTS_SRCS tests/cppunit/*.cc)\nadd_executable(unittest ${TESTS_SRCS})\ntarget_include_directories(unittest PRIVATE tests/cppunit)\n\ntarget_link_libraries(unittest PRIVATE kvrocks_objs gtest_main gmock ${EXTERNAL_LIBS})\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 2.0703125,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nFROM debian:bookworm-slim AS build\n\nARG MORE_BUILD_ARGS\n\nRUN DEBIAN_FRONTEND=noninteractive && apt-get update && apt-get upgrade -y && apt-get -y --no-install-recommends install git build-essential autoconf cmake libtool python3 libssl-dev && apt-get autoremove && apt-get clean\n\nWORKDIR /kvrocks\n\nCOPY . .\nRUN ./x.py build -DENABLE_OPENSSL=ON -DPORTABLE=1 -DCMAKE_BUILD_TYPE=Release -j $(nproc) $MORE_BUILD_ARGS\n\nFROM debian:bookworm-slim\n\nRUN DEBIAN_FRONTEND=noninteractive && apt-get update && apt-get upgrade -y && apt-get -y install openssl ca-certificates redis-tools binutils && apt-get clean\n\n# Create a dedicated non-root user and group\nRUN groupadd -r kvrocks && useradd -r -g kvrocks kvrocks\n\nRUN mkdir /var/run/kvrocks /var/lib/kvrocks && \\\n    chown -R kvrocks:kvrocks /var/run/kvrocks /var/lib/kvrocks\n\n# Switch to the non-root user\nUSER kvrocks\n\nVOLUME /var/lib/kvrocks\n\nCOPY --from=build /kvrocks/build/kvrocks /bin/\n\nHEALTHCHECK --interval=10s --timeout=1s --start-period=30s --retries=3 \\\n    CMD redis-cli -p 6666 PING | grep -E '(PONG|NOAUTH)' || exit 1\n\nCOPY ./LICENSE ./NOTICE ./licenses /kvrocks/\nCOPY ./kvrocks.conf /var/lib/kvrocks/\n\nEXPOSE 6666:6666\n\nENTRYPOINT [\"kvrocks\", \"-c\", \"/var/lib/kvrocks/kvrocks.conf\", \"--dir\", \"/var/lib/kvrocks\", \"--pidfile\", \"/var/run/kvrocks/kvrocks.pid\", \"--bind\", \"0.0.0.0\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 4.302734375,
          "content": "Apache Kvrocks\nCopyright 2022-2024 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n\n================================================================\n\nThanks to designers Lingyu Tian and Shili Fan for contributing the logo of Kvrocks.\n\n================================================================\n\nThis product includes a number of Dependencies with separate copyright notices\nand license terms. Your use of these submodules is subject to the terms and\nconditions of the following licenses.\n\n================================================================\n\n================================================================\nApache-2.0 licenses\n================================================================\nThe following components are provided under the Apache-2.0 License.See project link for details.\nThe text of each license is the standard Apache 2.0 license.\n\n* rocksdb(https://github.com/facebook/rocksdb)\n\nNB: RocksDB is dual-licensed under both the GPLv2 and Apache 2.0 License.\nThis product uses it under the Apache 2.0 License.\n\n* oneTBB(https://github.com/oneapi-src/oneTBB)\n\nFiles src/common/rocksdb_crc32c.h and src/storage/batch_debugger.h are modified from RocksDB.\nFiles src/types/bloom_filter.* are modified from Apache Arrow.\nThe text of the license is the standard Apache 2.0 license.\n\n================================================================\nBSD-2-Clause licenses\n================================================================\nThe following components are provided under the BSD-2-Clause License. See project link for details.\nThe text of each license is also included in licenses/LICENSE-[project].txt.\n\n* lz4(https://github.com/lz4/lz4)\n* xxHash(https://github.com/Cyan4973/xxHash)\n\nNB: This product only uses the source code in `lib` directory which is under the BSD 2-Clause.\n\n================================================================\nBSD-3-Clause licenses\n================================================================\nThe following components are provided under the BSD-3-Clause License. See project link for details.\nThe text of each license is also included in licenses/LICENSE-[project].txt.\n\n* glog(https://github.com/google/glog)\n* googletest(https://github.com/google/googletest)\n* libevent(https://github.com/libevent/libevent)\n* snappy(https://github.com/google/snappy)\n\nFiles src/types/geohash.*, src/vendor/rand.*, src/storage/scripting.*\nand some utility functions in src/common are modified from Redis.\nThe text of the license is included in licenses/LICENSE-redis.txt.\n\n================================================================\nMIT licenses\n================================================================\nThe following components are provided under the MIT License. See project link for details.\nThe text of each license is also included in licenses/LICENSE-[project].txt\n\n* fmt(https://github.com/fmtlib/fmt)\n* LuaJIT(https://github.com/KvrocksLabs/LuaJIT)\n* lua(https://github.com/KvrocksLabs/lua, alternative to LuaJIT)\n* hat-trie(https://github.com/Tessil/hat-trie)\n* pegtl(https://github.com/taocpp/PEGTL, NOTE: changed to Boost Software License Version 1.0 in main branch)\n* cpptrace(https://github.com/jeremy-rifkin/cpptrace)\n\n================================================================\nBoost Software License Version 1.0\n================================================================\nThe following components are provided under the Boost Software License Version 1.0. See project link for details.\nThe text of each license is also included in licenses/LICENSE-[project].txt\n\n* jsoncons(https://github.com/danielaparker/jsoncons)\n* span-lite(https://github.com/martinmoene/span-lite)\n* range-v3(https://github.com/ericniebler/range-v3)\n\n================================================================\nzlib/libpng licenses\n================================================================\nThe following components are provided under the zlib/libpng License. See project link for details.\nThe text of each license is also included in licenses/LICENSE-[project].txt\n\n* zlib(https://github.com/madler/zlib)\n\n================================================================\nPublic Domain\n================================================================\nFiles src/vendor/sha1.* are provided in Public Domain by Steve Reid <steve@edmweb.com>.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.5791015625,
          "content": "<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n-->\n\n<img src=\"https://kvrocks.apache.org/img/kvrocks-featured.png\" alt=\"kvrocks_logo\" width=\"350\"/>\n\n[![CI](https://github.com/apache/kvrocks/actions/workflows/kvrocks.yaml/badge.svg?branch=unstable)](https://github.com/apache/kvrocks/actions/workflows/kvrocks.yaml)\n[![License](https://img.shields.io/github/license/apache/kvrocks)](https://github.com/apache/kvrocks/blob/unstable/LICENSE)\n[![GitHub stars](https://img.shields.io/github/stars/apache/kvrocks)](https://github.com/apache/kvrocks/stargazers)\n\n---\n* [Chat on Zulip](https://kvrocks.zulipchat.com/)\n* [Mailing List](https://lists.apache.org/list.html?dev@kvrocks.apache.org) ([how to subscribe](https://www.apache.org/foundation/mailinglists.html#subscribing))\n\n**Apache Kvrocks** is a distributed key value NoSQL database that uses RocksDB as storage engine and is compatible with Redis protocol. Kvrocks intends to decrease the cost of memory and increase the capacity while compared to Redis. The design of replication and storage was inspired by [rocksplicator](https://github.com/pinterest/rocksplicator) and [blackwidow](https://github.com/Qihoo360/blackwidow).\n\nKvrocks has the following key features:\n\n* Redis Compatible: Users can access Apache Kvrocks via any Redis client.\n* Namespace: Similar to Redis SELECT but equipped with token per namespace.\n* Replication: Async replication using binlog like MySQL.\n* High Availability: Support Redis sentinel to failover when master or slave was failed.\n* Cluster: Centralized management but accessible via any Redis cluster client.\n\n## Who uses Kvrocks\n\nYou can find Kvrocks users at [the Users page](https://kvrocks.apache.org/users/).\n\nUsers are encouraged to add themselves to the Users page. Either leave a comment on the [\"Who is using Kvrocks\"](https://github.com/apache/kvrocks/issues/414) issue, or directly send a pull request to add company or organization [information](https://github.com/apache/kvrocks-website/blob/main/src/components/UserLogos/index.tsx) and [logo](https://github.com/apache/kvrocks-website/tree/main/static/media/users).\n\n## Build and run Kvrocks\n\n### Prerequisite\n\n```shell\n# Ubuntu / Debian\nsudo apt update\nsudo apt install -y git build-essential cmake libtool python3 libssl-dev\n\n# CentOS / RedHat\nsudo yum install -y centos-release-scl-rh\nsudo yum install -y git devtoolset-11 autoconf automake libtool libstdc++-static python3 openssl-devel\n# download and install cmake via https://cmake.org/download\nwget https://github.com/Kitware/CMake/releases/download/v3.26.4/cmake-3.26.4-linux-x86_64.sh -O cmake.sh\nsudo bash cmake.sh --skip-license --prefix=/usr\n# enable gcc and make in devtoolset-11\nsource /opt/rh/devtoolset-11/enable\n\n# openSUSE / SUSE Linux Enterprise\nsudo zypper install -y gcc11 gcc11-c++ make wget git autoconf automake python3 curl cmake\n\n# Arch Linux\nsudo pacman -Sy --noconfirm autoconf automake python3 git wget which cmake make gcc\n\n# macOS\nbrew install git cmake autoconf automake libtool openssl\n# please link openssl by force if it still cannot be found after installing\nbrew link --force openssl\n```\n\n### Build\n\nIt is as simple as:\n\n```shell\n$ git clone https://github.com/apache/kvrocks.git\n$ cd kvrocks\n$ ./x.py build # `./x.py build -h` to check more options;\n               # especially, `./x.py build --ghproxy` will fetch dependencies via ghproxy.com.\n```\n\nTo build with TLS support, you'll need OpenSSL development libraries (e.g. libssl-dev on Debian/Ubuntu) and run:\n\n```shell\n$ ./x.py build -DENABLE_OPENSSL=ON\n```\n\nTo build with lua instead of luaJIT, run:\n\n```shell\n$ ./x.py build -DENABLE_LUAJIT=OFF\n```\n\nBuild with debug mode, run:\n\n```shell\n# The default build type is RelWithDebInfo and its optimization level is typically -O2.\n# You can change it to -O0 in debug mode.\n\n$ ./x.py build -DCMAKE_BUILD_TYPE=Debug\n```\n\n### Running Kvrocks\n\n```shell\n$ ./build/kvrocks -c kvrocks.conf\n```\n\n### Running Kvrocks using Docker\n\n```shell\n$ docker run -it -p 6666:6666 apache/kvrocks --bind 0.0.0.0\n# or get the nightly image:\n$ docker run -it -p 6666:6666 apache/kvrocks:nightly\n```\n\nPlease visit [Apache Kvrocks on DockerHub](https://hub.docker.com/r/apache/kvrocks) for additional details about images.\n\n### Connect Kvrocks service\n\n```sh\n$ redis-cli -p 6666\n\n127.0.0.1:6666> get a\n(nil)\n```\n\n### Running test cases\n\n```shell\n$ ./x.py build --unittest\n$ ./x.py test cpp # run C++ unit tests\n$ ./x.py test go # run Golang (unit and integration) test cases\n```\n\n### Supported platforms\n\n* Linux\n* macOS\n\n## Namespace\n\nNamespace is used to isolate data between users. Unlike all the Redis databases can be visited by `requirepass`, we use one token per namespace. `requirepass` is regraded as admin token, and only admin token allows to access the namespace command, as well as some commands like `config`, `slaveof`, `bgsave`, etc. See the [Namespace](https://kvrocks.apache.org/docs/namespace) page for more details.\n\n```sh\n# add token\n127.0.0.1:6666> namespace add ns1 my_token\nOK\n\n# update token\n127.0.0.1:6666> namespace set ns1 new_token\nOK\n\n# list namespace\n127.0.0.1:6666> namespace get *\n1) \"ns1\"\n2) \"new_token\"\n3) \"__namespace\"\n4) \"foobared\"\n\n# delete namespace\n127.0.0.1:6666> namespace del ns1\nOK\n```\n\n## Cluster\n\nKvrocks implements a proxyless centralized cluster solution but its accessing method is completely compatible with the Redis cluster client. You can use Redis cluster SDKs to access the kvrocks cluster. More details, please see: [Kvrocks Cluster Introduction](https://kvrocks.apache.org/docs/cluster/)\n\n## Documents\n\nDocuments are hosted at the [official website](https://kvrocks.apache.org/docs/getting-started/).\n\n* [Supported Commands](https://kvrocks.apache.org/docs/supported-commands/)\n* [Design Complex Structure on RocksDB](https://kvrocks.apache.org/community/data-structure-on-rocksdb/)\n* [Replication Design](https://kvrocks.apache.org/docs/replication)\n\n## Tools\n\n* To manage Kvrocks clusters for failover, scaling up/down and more, use [kvrocks-controller](https://github.com/apache/kvrocks-controller)\n* To export the Kvrocks monitor metrics, use [kvrocks_exporter](https://github.com/RocksLabs/kvrocks_exporter)\n* To migrate from Redis to Kvrocks, use [RedisShake](https://github.com/tair-opensource/RedisShake)\n* To migrate from Kvrocks to Redis, use `kvrocks2redis` built via `./x.py build`\n\n## Contributing\n\nKvrocks community welcomes all forms of contribution and you can find out how to get involved on the [Community](https://kvrocks.apache.org/community/) and [How to Contribute](https://kvrocks.apache.org/community/contributing) pages.\n\n## License\n\nApache Kvrocks is licensed under the Apache License Version 2.0. See the [LICENSE](LICENSE) file for details.\n\n## Social Media\n\n- [Medium](https://kvrocks.medium.com/)\n- [X (Twitter)](https://twitter.com/apache_kvrocks)\n- [Zhihu](https://www.zhihu.com/people/kvrocks) (in Chinese)\n- WeChat Official Account (in Chinese, scan the QR code to follow)\n\n![WeChat official account](assets/wechat_account.jpg)\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "dev",
          "type": "tree",
          "content": null
        },
        {
          "name": "kvrocks.conf",
          "type": "blob",
          "size": 42.0029296875,
          "content": "################################ GENERAL #####################################\n\n# By default kvrocks listens for connections from localhost interface.\n# It is possible to listen to just one or multiple interfaces using\n# the \"bind\" configuration directive, followed by one or more IP addresses.\n#\n# Examples:\n#\n# bind 192.168.1.100 10.0.0.1\n# bind 127.0.0.1 ::1\n# bind 0.0.0.0\nbind 127.0.0.1\n\n# Unix socket.\n#\n# Specify the path for the unix socket that will be used to listen for\n# incoming connections. There is no default, so kvrocks will not listen\n# on a unix socket when not specified.\n#\n# unixsocket /tmp/kvrocks.sock\n# unixsocketperm 777\n\n# Allows a parent process to open a socket and pass its FD down to kvrocks as a child \n# process. Useful to reserve a port and prevent race conditions.\n# \n# PLEASE NOTE: \n# If this is overridden to a value other than -1, the bind and tls* directives will be \n# ignored.\n# \n# Default: -1 (not overridden, defer to creating a connection to the specified port)\nsocket-fd -1\n\n# Accept connections on the specified port, default is 6666.\nport 6666\n\n# Close the connection after a client is idle for N seconds (0 to disable)\ntimeout 0\n\n# The number of worker's threads, increase or decrease would affect the performance.\nworkers 8\n\n# By default, kvrocks does not run as a daemon. Use 'yes' if you need it.\n# It will create a PID file when daemonize is enabled, and its path is specified by pidfile.\ndaemonize no\n\n# Kvrocks implements the cluster solution that is similar to the Redis cluster solution.\n# You can get cluster information by CLUSTER NODES|SLOTS|INFO command, it also is\n# adapted to redis-cli, redis-benchmark, Redis cluster SDK, and Redis cluster proxy.\n# But kvrocks doesn't support communicating with each other, so you must set\n# cluster topology by CLUSTER SETNODES|SETNODEID commands, more details: #219.\n#\n# PLEASE NOTE:\n# If you enable cluster, kvrocks will encode key with its slot id calculated by\n# CRC16 and modulo 16384, encoding key with its slot id makes it efficient to\n# migrate keys based on the slot. So if you enabled at first time, cluster mode must\n# not be disabled after restarting, and vice versa. That is to say, data is not\n# compatible between standalone mode with cluster mode, you must migrate data\n# if you want to change mode, otherwise, kvrocks will make data corrupt.\n#\n# Default: no\n\ncluster-enabled no\n\n# By default, namespaces are stored in the configuration file and won't be replicated\n# to replicas. This option allows to change this behavior, so that namespaces are also\n# propagated to slaves. Note that:\n# 1) it won't replicate the 'masterauth' to prevent breaking master/replica replication\n# 2) it will overwrite replica's namespace with master's namespace, so be careful of in-using namespaces\n# 3) cannot switch off the namespace replication once it's enabled\n#\n# Default: no\nrepl-namespace-enabled no\n\n# By default, the max length of bulk string is limited to 512MB. If you want to\n# change this limit to a different value(must >= 1MiB), you can use the following configuration.\n# It can be just an integer (e.g. 10000000), or an integer followed by a unit (e.g. 12M, 7G, 2T).\n#\n# proto-max-bulk-len 536870912\n\n# Persist the cluster nodes topology in local file($dir/nodes.conf). This configuration\n# takes effect only if the cluster mode was enabled.\n#\n# If yes, it will try to load the cluster topology from the local file when starting,\n# and dump the cluster nodes into the file if it was changed.\n#\n# Default: yes\npersist-cluster-nodes-enabled yes\n\n# Set the max number of connected clients at the same time. By default\n# this limit is set to 10000 clients. However, if the server is not\n# able to configure the process file limit to allow for the specified limit\n# the max number of allowed clients is set to the current file limit\n#\n# Once the limit is reached the server will close all the new connections sending\n# an error 'max number of clients reached'.\n#\nmaxclients 10000\n\n# Require clients to issue AUTH <PASSWORD> before processing any other\n# commands.  This might be useful in environments in which you do not trust\n# others with access to the host running kvrocks.\n#\n# This should stay commented out for backward compatibility and because most\n# people do not need auth (e.g. they run their own servers).\n#\n# Warning: since kvrocks is pretty fast an outside user can try up to\n# 150k passwords per second against a good box. This means that you should\n# use a very strong password otherwise it will be very easy to break.\n#\n# requirepass foobared\n\n# If the master is password protected (using the \"masterauth\" configuration\n# directive below) it is possible to tell the slave to authenticate before\n# starting the replication synchronization process. Otherwise, the master will\n# refuse the slave request.\n#\n# masterauth foobared\n\n# Master-Salve replication would check db name is matched. if not, the slave should\n# refuse to sync the db from master. Don't use the default value, set the db-name to identify\n# the cluster.\ndb-name change.me.db\n\n# The working directory\n#\n# The DB will be written inside this directory\n# Note that you must specify a directory here, not a file name.\ndir /tmp/kvrocks\n\n# You can configure where to store your server logs by the log-dir.\n# If you don't specify one, we will use the above `dir` as our default log directory.\n# We also can send logs to stdout/stderr is as simple as:\n#\nlog-dir stdout\n\n# Log level\n# Possible values: info, warning, error, fatal\n# Default: info\nlog-level info\n\n# You can configure log-retention-days to control whether to enable the log cleaner\n# and the maximum retention days that the INFO level logs will be kept.\n#\n# if set to -1, that means to disable the log cleaner.\n# if set to 0, all previous INFO level logs will be immediately removed.\n# if set to between 0 to INT_MAX, that means it will retent latest N(log-retention-days) day logs.\n\n# By default the log-retention-days is -1.\nlog-retention-days -1\n\n# When running in daemonize mode, kvrocks writes a PID file in ${CONFIG_DIR}/kvrocks.pid by\n# default. You can specify a custom pid file location here.\n# pidfile /var/run/kvrocks.pid\n\n# You can configure a slave instance to accept writes or not. Writing against\n# a slave instance may be useful to store some ephemeral data (because data\n# written on a slave will be easily deleted after resync with the master) but\n# may also cause problems if clients are writing to it because of a\n# misconfiguration.\nslave-read-only yes\n\n# The slave priority is an integer number published by Kvrocks in the INFO output.\n# It is used by Redis Sentinel in order to select a slave to promote into a\n# master if the master is no longer working correctly.\n#\n# A slave with a low priority number is considered better for promotion, so\n# for instance if there are three slave with priority 10, 100, 25 Sentinel will\n# pick the one with priority 10, that is the lowest.\n#\n# However a special priority of 0 marks the replica as not able to perform the\n# role of master, so a slave with priority of 0 will never be selected by\n# Redis Sentinel for promotion.\n#\n# By default the priority is 100.\nslave-priority 100\n\n# Change the default timeout in milliseconds for socket connect during replication.\n# The default value is 3100, and 0 means no timeout.\n#\n# If the master is unreachable before connecting, not having a timeout may block future\n# 'clusterx setnodes' commands because the replication thread is blocked on connect.\nreplication-connect-timeout-ms 3100\n\n# Change the default timeout in milliseconds for socket recv during fullsync.\n# The default value is 3200, and 0 means no timeout.\n#\n# If the master is unreachable when fetching SST files, not having a timeout may block\n# future 'clusterx setnodes' commands because the replication thread is blocked on recv.\nreplication-recv-timeout-ms 3200\n\n# TCP listen() backlog.\n#\n# In high requests-per-second environments you need an high backlog in order\n# to avoid slow clients connections issues. Note that the Linux kernel\n# will silently truncate it to the value of /proc/sys/net/core/somaxconn so\n# make sure to raise both the value of somaxconn and tcp_max_syn_backlog\n# in order to Get the desired effect.\ntcp-backlog 511\n\n# If the master is an old version, it may have specified replication threads\n# that use 'port + 1' as listening port, but in new versions, we don't use\n# extra port to implement replication. In order to allow the new replicas to\n# copy old masters, you should indicate that the master uses replication port\n# or not.\n# If yes, that indicates master uses replication port and replicas will connect\n# to 'master's listening port + 1' when synchronization.\n# If no, that indicates master doesn't use replication port and replicas will\n# connect 'master's listening port' when synchronization.\nmaster-use-repl-port no\n\n# Currently, master only checks sequence number when replica asks for PSYNC,\n# that is not enough since they may have different replication histories even\n# the replica asking sequence is in the range of the master current WAL.\n#\n# We design 'Replication Sequence ID' PSYNC, we add unique replication id for\n# every write batch (the operation of each command on the storage engine), so\n# the combination of replication id and sequence is unique for write batch.\n# The master can identify whether the replica has the same replication history\n# by checking replication id and sequence.\n#\n# By default, it is not enabled since this stricter check may easily lead to\n# full synchronization.\nuse-rsid-psync no\n\n# Master-Slave replication. Use slaveof to make a kvrocks instance a copy of\n# another kvrocks server. A few things to understand ASAP about kvrocks replication.\n#\n# 1) Kvrocks replication is asynchronous, but you can configure a master to\n#    stop accepting writes if it appears to be not connected with at least\n#    a given number of slaves.\n# 2) Kvrocks slaves are able to perform a partial resynchronization with the\n#    master if the replication link is lost for a relatively small amount of\n#    time. You may want to configure the replication backlog size (see the next\n#    sections of this file) with a sensible value depending on your needs.\n# 3) Replication is automatic and does not need user intervention. After a\n#    network partition slaves automatically try to reconnect to masters\n#    and resynchronize with them.\n#\n# slaveof <masterip> <masterport>\n# slaveof 127.0.0.1 6379\n\n# When a slave loses its connection with the master, or when the replication\n# is still in progress, the slave can act in two different ways:\n#\n# 1) if slave-serve-stale-data is set to 'yes' (the default) the slave will\n#    still reply to client requests, possibly with out-of-date data, or the\n#    data set may just be empty if this is the first synchronization.\n#\n# 2) if slave-serve-stale-data is set to 'no' the slave will reply with\n#    an error \"SYNC with master in progress\" to all kinds of commands\n#    but to INFO and SLAVEOF.\n#\nslave-serve-stale-data yes\n\n# To guarantee slave's data safe and serve when it is in full synchronization\n# state, slave still keep itself data. But this way needs to occupy much disk\n# space, so we provide a way to reduce disk occupation, slave will delete itself\n# entire database before fetching files from master during full synchronization.\n# If you want to enable this way, you can set 'slave-delete-db-before-fullsync'\n# to yes, but you must know that database will be lost if master is down during\n# full synchronization, unless you have a backup of database.\n#\n# This option is similar redis replicas RDB diskless load option:\n#       repl-diskless-load on-empty-db\n#\n# Default: no\nslave-empty-db-before-fullsync no\n\n# A Kvrocks master is able to list the address and port of the attached\n# replicas in different ways. For example the \"INFO replication\" section\n# offers this information, which is used, among other tools, by\n# Redis Sentinel in order to discover replica instances.\n# Another place where this info is available is in the output of the\n# \"ROLE\" command of a master.\n#\n# The listed IP address and port normally reported by a replica is\n# obtained in the following way:\n#\n#   IP: The address is auto detected by checking the peer address\n#   of the socket used by the replica to connect with the master.\n#\n#   Port: The port is communicated by the replica during the replication\n#   handshake, and is normally the port that the replica is using to\n#   listen for connections.\n#\n# However when port forwarding or Network Address Translation (NAT) is\n# used, the replica may actually be reachable via different IP and port\n# pairs. The following two options can be used by a replica in order to\n# report to its master a specific set of IP and port, so that both INFO\n# and ROLE will report those values.\n#\n# There is no need to use both the options if you need to override just\n# the port or the IP address.\n#\n# replica-announce-ip 5.5.5.5\n# replica-announce-port 1234\n\n# If replicas need full synchronization with master, master need to create\n# checkpoint for feeding replicas, and replicas also stage a checkpoint of\n# the master. If we also keep the backup, it maybe occupy extra disk space.\n# You can enable 'purge-backup-on-fullsync' if disk is not sufficient, but\n# that may cause remote backup copy failing.\n#\n# Default: no\npurge-backup-on-fullsync no\n\n# The maximum allowed rate (in MB/s) that should be used by replication.\n# If the rate exceeds max-replication-mb, replication will slow down.\n# Default: 0 (i.e. no limit)\nmax-replication-mb 0\n\n# The maximum allowed aggregated write rate of flush and compaction (in MB/s).\n# If the rate exceeds max-io-mb, io will slow down.\n# 0 is no limit\n# Default: 0\nmax-io-mb 0\n\n# The maximum allowed space (in GB) that should be used by RocksDB.\n# If the total size of the SST files exceeds max_allowed_space, writes to RocksDB will fail.\n# Please see: https://github.com/facebook/rocksdb/wiki/Managing-Disk-Space-Utilization\n# Default: 0 (i.e. no limit)\nmax-db-size 0\n\n# The maximum backup to keep, server cron would run every minutes to check the num of current\n# backup, and purge the old backup if exceed the max backup num to keep. If max-backup-to-keep\n# is 0, no backup would be kept. But now, we only support 0 or 1.\nmax-backup-to-keep 1\n\n# The maximum hours to keep the backup. If max-backup-keep-hours is 0, wouldn't purge any backup.\n# default: 1 day\nmax-backup-keep-hours 24\n\n# max-bitmap-to-string-mb use to limit the max size of bitmap to string transformation(MB).\n#\n# Default: 16\nmax-bitmap-to-string-mb 16\n\n# Whether to enable SCAN-like cursor compatible with Redis.\n# If enabled, the cursor will be unsigned 64-bit integers.\n# If disabled, the cursor will be a string.\n# Default: yes\nredis-cursor-compatible yes\n\n# Whether to enable the RESP3 protocol.\n# NOTICE: RESP3 is still under development, don't enable it in production environment.\n#\n# Default: no\n# resp3-enabled no\n\n# Maximum nesting depth allowed when parsing and serializing\n# JSON documents while using JSON commands like JSON.SET.\n# Default: 1024\njson-max-nesting-depth 1024\n\n# The underlying storage format of JSON data type\n# NOTE: This option only affects newly written/updated key-values\n# The CBOR format may reduce the storage size and speed up JSON commands\n# Available values: json, cbor\n# Default: json\njson-storage-format json\n\n# Whether to enable transactional mode engine::Context.\n#\n# If enabled, is_txn_mode in engine::Context will be set properly,\n# which is expected to improve the consistency of commands.\n# If disabled, is_txn_mode in engine::Context will be set to false,\n# making engine::Context equivalent to engine::Storage.\n#\n# NOTE: This is an experimental feature. If you find errors, performance degradation,\n# excessive memory usage, excessive disk I/O, etc. after enabling it, please try disabling it.\n# At the same time, we welcome feedback on related issues to help iterative improvements.\n# \n# Default: no\ntxn-context-enabled no\n\n################################## TLS ###################################\n\n# By default, TLS/SSL is disabled, i.e. `tls-port` is set to 0.\n# To enable it, `tls-port` can be used to define TLS-listening ports.\n# tls-port 0\n\n# Configure a X.509 certificate and private key to use for authenticating the\n# server to connected clients, masters or cluster peers.\n# These files should be PEM formatted.\n#\n# tls-cert-file kvrocks.crt\n# tls-key-file kvrocks.key\n\n# If the key file is encrypted using a passphrase, it can be included here\n# as well.\n#\n# tls-key-file-pass secret\n\n# Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL\n# clients and peers.  Kvrocks requires an explicit configuration of at least one\n# of these, and will not implicitly use the system wide configuration.\n#\n# tls-ca-cert-file ca.crt\n# tls-ca-cert-dir /etc/ssl/certs\n\n# By default, clients on a TLS port are required\n# to authenticate using valid client side certificates.\n#\n# If \"no\" is specified, client certificates are not required and not accepted.\n# If \"optional\" is specified, client certificates are accepted and must be\n# valid if provided, but are not required.\n#\n# tls-auth-clients no\n# tls-auth-clients optional\n\n# By default, only TLSv1.2 and TLSv1.3 are enabled and it is highly recommended\n# that older formally deprecated versions are kept disabled to reduce the attack surface.\n# You can explicitly specify TLS versions to support.\n# Allowed values are case insensitive and include \"TLSv1\", \"TLSv1.1\", \"TLSv1.2\",\n# \"TLSv1.3\" (OpenSSL >= 1.1.1) or any combination.\n# To enable only TLSv1.2 and TLSv1.3, use:\n#\n# tls-protocols \"TLSv1.2 TLSv1.3\"\n\n# Configure allowed ciphers.  See the ciphers(1ssl) manpage for more information\n# about the syntax of this string.\n#\n# Note: this configuration applies only to <= TLSv1.2.\n#\n# tls-ciphers DEFAULT:!MEDIUM\n\n# Configure allowed TLSv1.3 ciphersuites.  See the ciphers(1ssl) manpage for more\n# information about the syntax of this string, and specifically for TLSv1.3\n# ciphersuites.\n#\n# tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256\n\n# When choosing a cipher, use the server's preference instead of the client\n# preference. By default, the server follows the client's preference.\n#\n# tls-prefer-server-ciphers yes\n\n# By default, TLS session caching is enabled to allow faster and less expensive\n# reconnections by clients that support it. Use the following directive to disable\n# caching.\n#\n# tls-session-caching no\n\n# Change the default number of TLS sessions cached. A zero value sets the cache\n# to unlimited size. The default size is 20480.\n#\n# tls-session-cache-size 5000\n\n# Change the default timeout of cached TLS sessions. The default timeout is 300\n# seconds.\n#\n# tls-session-cache-timeout 60\n\n# By default, a replica does not attempt to establish a TLS connection\n# with its master.\n#\n# Use the following directive to enable TLS on replication links.\n#\n# tls-replication yes\n\n################################## SLOW LOG ###################################\n\n# The Kvrocks Slow Log is a mechanism to log queries that exceeded a specified\n# execution time. The execution time does not include the I/O operations\n# like talking with the client, sending the reply and so forth,\n# but just the time needed to actually execute the command (this is the only\n# stage of command execution where the thread is blocked and can not serve\n# other requests in the meantime).\n#\n# You can configure the slow log with two parameters: one tells Kvrocks\n# what is the execution time, in microseconds, to exceed in order for the\n# command to get logged, and the other parameter is the length of the\n# slow log. When a new command is logged the oldest one is removed from the\n# queue of logged commands.\n\n# The following time is expressed in microseconds, so 1000000 is equivalent\n# to one second. Note that -1 value disables the slow log, while\n# a value of zero forces the logging of every command.\nslowlog-log-slower-than 100000\n\n# There is no limit to this length. Just be aware that it will consume memory.\n# You can reclaim memory used by the slow log with SLOWLOG RESET.\nslowlog-max-len 128\n\n# If you run kvrocks from upstart or systemd, kvrocks can interact with your\n# supervision tree. Options:\n#   supervised no      - no supervision interaction\n#   supervised upstart - signal upstart by putting kvrocks into SIGSTOP mode\n#   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET\n#   supervised auto    - detect upstart or systemd method based on\n#                        UPSTART_JOB or NOTIFY_SOCKET environment variables\n# Note: these supervision methods only signal \"process is ready.\"\n#       They do not enable continuous liveness pings back to your supervisor.\nsupervised no\n\n################################## PERF LOG ###################################\n\n# The Kvrocks Perf Log is a mechanism to log queries' performance context that\n# exceeded a specified execution time. This mechanism uses rocksdb's\n# Perf Context and IO Stats Context, Please see:\n# https://github.com/facebook/rocksdb/wiki/Perf-Context-and-IO-Stats-Context\n#\n# This mechanism is enabled when profiling-sample-commands is not empty and\n# profiling-sample-ratio greater than 0.\n# It is important to note that this mechanism affects performance, but it is\n# useful for troubleshooting performance bottlenecks, so it should only be\n# enabled when performance problems occur.\n\n# The name of the commands you want to record. Must be original name of\n# commands supported by Kvrocks. Use ',' to separate multiple commands and\n# use '*' to record all commands supported by Kvrocks.\n# Example:\n#   - Single command: profiling-sample-commands get\n#   - Multiple commands: profiling-sample-commands get,mget,hget\n#\n# Default: empty\n# profiling-sample-commands \"\"\n\n# Ratio of the samples would be recorded. It is a number between 0 and 100.\n# We simply use the rand to determine whether to record the sample or not.\n#\n# Default: 0\nprofiling-sample-ratio 0\n\n# There is no limit to this length. Just be aware that it will consume memory.\n# You can reclaim memory used by the perf log with PERFLOG RESET.\n#\n# Default: 256\nprofiling-sample-record-max-len 256\n\n# profiling-sample-record-threshold-ms use to tell the kvrocks when to record.\n#\n# Default: 100 millisecond\nprofiling-sample-record-threshold-ms 100\n\n################################## CRON ###################################\n\n# Compact Scheduler, auto compact at schedule time\n# Time expression format is the same as crontab (supported cron syntax: *, n, */n, `1,3-6,9,11`)\n# e.g. compact-cron 0 3,4 * * *\n# would compact the db at 3am and 4am everyday\n# compact-cron 0 3 * * *\n\n# The hour range that compaction checker would be active\n# e.g. compaction-checker-range 0-7 means compaction checker would be worker between\n# 0-7am every day.\n# WARNING: this config option is deprecated and will be removed,\n# please use compaction-checker-cron instead\n# compaction-checker-range 0-7\n\n# The time pattern that compaction checker would be active\n# Time expression format is the same as crontab (supported cron syntax: *, n, */n, `1,3-6,9,11`)\n# e.g. compaction-checker-cron * 0-7 * * * means compaction checker would be worker between\n# 0-7am every day.\ncompaction-checker-cron * 0-7 * * *\n\n# When the compaction checker is triggered, the db will periodically pick the SST file\n# with the highest \"deleted percentage\" (i.e. the percentage of deleted keys in the SST\n# file) to compact, in order to free disk space.\n# However, if a specific SST file was created more than \"force-compact-file-age\" seconds\n# ago, and its percentage of deleted keys is higher than\n# \"force-compact-file-min-deleted-percentage\", it will be forcibly compacted as well.\n\n# Default: 172800 seconds; Range: [60, INT64_MAX];\n# force-compact-file-age 172800\n# Default: 10 %; Range: [1, 100];\n# force-compact-file-min-deleted-percentage 10\n\n# Bgsave scheduler, auto bgsave at scheduled time\n# Time expression format is the same as crontab (supported cron syntax: *, n, */n, `1,3-6,9,11`)\n# e.g. bgsave-cron 0 3,4 * * *\n# would bgsave the db at 3am and 4am every day\n\n# Kvrocks doesn't store the key number directly. It needs to scan the DB and\n# then retrieve the key number by using the dbsize scan command.\n# The Dbsize scan scheduler auto-recalculates the estimated keys at scheduled time.\n# Time expression format is the same as crontab (supported cron syntax: *, n, */n, `1,3-6,9,11`)\n# e.g. dbsize-scan-cron 0 * * * *\n# would recalculate the keyspace infos of the db every hour.\n\n# Command renaming.\n#\n# It is possible to change the name of dangerous commands in a shared\n# environment. For instance, the KEYS command may be renamed into something\n# hard to guess so that it will still be available for internal-use tools\n# but not available for general clients.\n#\n# Example:\n#\n# rename-command KEYS b840fc02d524045429941cc15f59e41cb7be6c52\n#\n# It is also possible to completely kill a command by renaming it into\n# an empty string:\n#\n# rename-command KEYS \"\"\n\n################################ MIGRATE #####################################\n# Slot migration supports two ways:\n# - redis-command: Migrate data by redis serialization protocol(RESP).\n# - raw-key-value: Migrate the raw key value data of the storage engine directly.\n#                  This way eliminates the overhead of converting to the redis\n#                  command, reduces resource consumption, improves migration\n#                  efficiency, and can implement a finer rate limit.\n#\n# Default: redis-command\nmigrate-type redis-command\n\n# If the network bandwidth is completely consumed by the migration task,\n# it will affect the availability of kvrocks. To avoid this situation,\n# migrate-speed is adopted to limit the migrating speed.\n# Migrating speed is limited by controlling the duration between sending data,\n# the duration is calculated by: 1000000 * migrate-pipeline-size / migrate-speed (us).\n# Value: [0,INT_MAX], 0 means no limit\n#\n# Default: 4096\nmigrate-speed 4096\n\n# In order to reduce data transmission times and improve the efficiency of data migration,\n# pipeline is adopted to send multiple data at once. Pipeline size can be set by this option.\n# Value: [1, INT_MAX], it can't be 0\n#\n# Default: 16\nmigrate-pipeline-size 16\n\n# In order to reduce the write forbidden time during migrating slot, we will migrate the incremental\n# data several times to reduce the amount of incremental data. Until the quantity of incremental\n# data is reduced to a certain threshold, slot will be forbidden write. The threshold is set by\n# this option.\n# Value: [1, INT_MAX], it can't be 0\n#\n# Default: 10000\nmigrate-sequence-gap 10000\n\n# The raw-key-value migration way uses batch for migration. This option sets the batch size\n# for each migration.\n#\n# Default: 16kb\nmigrate-batch-size-kb 16\n\n# Rate limit for migration based on raw-key-value, representing the maximum number of data\n# that can be migrated per second. 0 means no limit.\n#\n# Default: 16M\nmigrate-batch-rate-limit-mb 16\n\n\n# If it is set to yes, kvrocks will skip the deallocation of block cache\n# while closing the database to speed up the shutdown\n#\n# Default: no\n# skip-block-cache-deallocation-on-close no\n\n################################ ROCKSDB #####################################\n\n# Specify the capacity of column family block cache. A larger block cache\n# may make requests faster while more keys would be cached. Max Size is 400*1024.\n# Default: 4096MB\nrocksdb.block_cache_size 4096\n\n# Specify the type of cache used in the block cache.\n# Accept value: \"lru\", \"hcc\"\n# \"lru\" stands for the cache with the LRU(Least Recently Used) replacement policy.\n#\n# \"hcc\" stands for the Hyper Clock Cache, a lock-free cache alternative\n# that offers much improved CPU efficiency vs. LRU cache under high parallel\n# load or high contention.\n#\n# default lru\nrocksdb.block_cache_type lru\n\n# A global cache for table-level rows in RocksDB. If almost always point\n# lookups, enlarging row cache may improve read performance. Otherwise,\n# if we enlarge this value, we can lessen metadata/subkey block cache size.\n#\n# Default: 0 (disabled)\nrocksdb.row_cache_size 0\n\n# Number of open files that can be used by the DB.  You may need to\n# increase this if your database has a large working set. Value -1 means\n# files opened are always kept open. You can estimate number of files based\n# on target_file_size_base and target_file_size_multiplier for level-based\n# compaction. For universal-style compaction, you can usually set it to -1.\n# Default: 8096\nrocksdb.max_open_files 8096\n\n# Amount of data to build up in memory (backed by an unsorted log\n# on disk) before converting to a sorted on-disk file.\n#\n# Larger values increase performance, especially during bulk loads.\n# Up to max_write_buffer_number write buffers may be held in memory\n# at the same time,\n# so you may wish to adjust this parameter to control memory usage.\n# Also, a larger write buffer will result in a longer recovery time\n# the next time the database is opened.\n#\n# Note that write_buffer_size is enforced per column family.\n# See db_write_buffer_size for sharing memory across column families.\n\n# default is 64MB\nrocksdb.write_buffer_size 64\n\n# Target file size for compaction, target file size for Level N can be calculated\n# by target_file_size_base * (target_file_size_multiplier ^ (L-1))\n#\n# Default: 128MB\nrocksdb.target_file_size_base 128\n\n# The maximum number of write buffers that are built up in memory.\n# The default and the minimum number is 2, so that when 1 write buffer\n# is being flushed to storage, new writes can continue to the other\n# write buffer.\n# If max_write_buffer_number > 3, writing will be slowed down to\n# options.delayed_write_rate if we are writing to the last write buffer\n# allowed.\nrocksdb.max_write_buffer_number 4\n\n# Maximum number of concurrent background jobs (compactions and flushes).\n# For backwards compatibility we will set `max_background_jobs =\n# max_background_compactions + max_background_flushes` in the case where user\n# sets at least one of `max_background_compactions` or `max_background_flushes`\n# (we replace -1 by 1 in case one option is unset).\nrocksdb.max_background_jobs 4\n\n# DEPRECATED: it is automatically decided based on the value of rocksdb.max_background_jobs\n# Maximum number of concurrent background compaction jobs, submitted to\n# the default LOW priority thread pool.\nrocksdb.max_background_compactions -1\n\n# DEPRECATED: it is automatically decided based on the value of rocksdb.max_background_jobs\n# Maximum number of concurrent background memtable flush jobs, submitted by\n# default to the HIGH priority thread pool. If the HIGH priority thread pool\n# is configured to have zero threads, flush jobs will share the LOW priority\n# thread pool with compaction jobs.\nrocksdb.max_background_flushes -1\n\n# This value represents the maximum number of threads that will\n# concurrently perform a compaction job by breaking it into multiple,\n# smaller ones that are run simultaneously.\n# Default: 2\nrocksdb.max_subcompactions 2\n\n# If enabled WAL records will be compressed before they are written. Only\n# ZSTD (= kZSTD) is supported (until streaming support is adapted for other\n# compression types). Compressed WAL records will be read in supported\n# versions (>= RocksDB 7.4.0 for ZSTD) regardless of this setting when\n# the WAL is read.\n#\n# Accept value: \"no\", \"zstd\"\n# Default is no\nrocksdb.wal_compression no\n\n# In order to limit the size of WALs, RocksDB uses DBOptions::max_total_wal_size\n# as the trigger of column family flush. Once WALs exceed this size, RocksDB\n# will start forcing the flush of column families to allow deletion of some\n# oldest WALs. This config can be useful when column families are updated at\n# non-uniform frequencies. If there's no size limit, users may need to keep\n# really old WALs when the infrequently-updated column families hasn't flushed\n# for a while.\n#\n# In kvrocks, we use multiple column families to store metadata, subkeys, etc.\n# If users always use string type, but use list, hash and other complex data types\n# infrequently, there will be a lot of old WALs if we don't set size limit\n# (0 by default in rocksdb), because rocksdb will dynamically choose the WAL size\n# limit to be [sum of all write_buffer_size * max_write_buffer_number] * 4 if set to 0.\n#\n# Moreover, you should increase this value if you already set rocksdb.write_buffer_size\n# to a big value, to avoid influencing the effect of rocksdb.write_buffer_size and\n# rocksdb.max_write_buffer_number.\n#\n# default is 512MB\nrocksdb.max_total_wal_size 512\n\n# Whether to print malloc stats together with rocksdb.stats when printing to LOG.\n#\n# Accepted values: \"yes\", \"no\"\n# Default: yes\nrocksdb.dump_malloc_stats yes\n\n# We implement the replication with rocksdb WAL, it would trigger full sync when the seq was out of range.\n# wal_ttl_seconds and wal_size_limit_mb would affect how archived logs will be deleted.\n# If WAL_ttl_seconds is not 0, then WAL files will be checked every WAL_ttl_seconds / 2 and those that\n# are older than WAL_ttl_seconds will be deleted#\n#\n# Default: 3 Hours\nrocksdb.wal_ttl_seconds 10800\n\n# If WAL_ttl_seconds is 0 and WAL_size_limit_MB is not 0,\n# WAL files will be checked every 10 min and if total size is greater\n# then WAL_size_limit_MB, they will be deleted starting with the\n# earliest until size_limit is met. All empty files will be deleted\n# Default: 16GB\nrocksdb.wal_size_limit_mb 16384\n\n# Approximate size of user data packed per block.  Note that the\n# block size specified here corresponds to uncompressed data. The\n# actual size of the unit read from disk may be smaller if\n# compression is enabled.\n#\n# Default: 16KB\nrocksdb.block_size 16384\n\n# Indicating if we'd put index/filter blocks to the block cache\n#\n# Default: yes\nrocksdb.cache_index_and_filter_blocks yes\n\n# Specify the compression to use.\n# Accept value: \"no\", \"snappy\", \"lz4\", \"zstd\", \"zlib\"\n# default snappy\nrocksdb.compression snappy\n\n# Specify the compression level to use. It trades compression speed\n#   and ratio, might be useful when tuning for disk space.\n#   See details: https://github.com/facebook/rocksdb/wiki/Space-Tuning\n# For zstd: valid range is from 1 (fastest) to 19 (best ratio),\n# For zlib: valid range is from 1 (fastest) to 9 (best ratio),\n# For lz4: adjusting the level influences the 'acceleration'.\n#   RocksDB sets a negative level to indicate acceleration directly,\n#   with more negative values indicating higher speed and less compression.\n# Note: This setting is ignored for compression algorithms like Snappy that\n#   do not support variable compression levels.\n#\n# RocksDB Default:\n#   - zstd: 3\n#   - zlib: Z_DEFAULT_COMPRESSION (currently -1)\n#   - kLZ4: -1 (i.e., `acceleration=1`; see `CompressionOptions::level` doc)\n# For all others, RocksDB does not specify a compression level.\n# If the compression type doesn't support the setting, it will be a no-op.\n#\n# Default: 32767 (RocksDB's generic default compression level. Internally\n#   it'll be translated to the default compression level specific to the\n#   compression library as mentioned above)\nrocksdb.compression_level 32767\n\n# If non-zero, we perform bigger reads when doing compaction. If you're\n# running RocksDB on spinning disks, you should set this to at least 2MB.\n# That way RocksDB's compaction is doing sequential instead of random reads.\n# When non-zero, we also force new_table_reader_for_compaction_inputs to\n# true.\n#\n# Default: 2 MB\nrocksdb.compaction_readahead_size 2097152\n\n# Enable compression from n levels of LSM-tree.\n# By default compression is disabled for the first two levels (L0 and L1),\n# because it may contain the frequently accessed data, so it'd be better\n# to use uncompressed data to save the CPU.\n# Value: [0, 7) (upper boundary is kvrocks maximum levels number)\n#\n# Default: 2\nrocksdb.compression_start_level 2\n\n# he limited write rate to DB if soft_pending_compaction_bytes_limit or\n# level0_slowdown_writes_trigger is triggered.\n\n# If the value is 0, we will infer a value from `rater_limiter` value\n# if it is not empty, or 16MB if `rater_limiter` is empty. Note that\n# if users change the rate in `rate_limiter` after DB is opened,\n# `delayed_write_rate` won't be adjusted.\n#\nrocksdb.delayed_write_rate 0\n# If enable_pipelined_write is true, separate write thread queue is\n#  maintained for WAL write and memtable write.\n#\n#  Default: no\nrocksdb.enable_pipelined_write no\n\n# Soft limit on number of level-0 files. We start slowing down writes at this\n#  point. A value <0 means that no writing slow down will be triggered by\n# number of files in level-0.\n#\n# Default: 20\nrocksdb.level0_slowdown_writes_trigger 20\n\n# Maximum number of level-0 files.  We stop writes at this point.\n#\n# Default: 40\nrocksdb.level0_stop_writes_trigger 40\n\n# Number of files to trigger level-0 compaction.\n#\n# Default: 4\nrocksdb.level0_file_num_compaction_trigger 4\n\n# if not zero, dump rocksdb.stats to LOG every stats_dump_period_sec\n#\n# Default: 0\nrocksdb.stats_dump_period_sec 0\n\n# if yes, the auto compaction would be disabled, but the manual compaction remain works\n#\n# Default: no\nrocksdb.disable_auto_compactions no\n\n# BlobDB(key-value separation) is essentially RocksDB for large-value use cases.\n# Since 6.18.0, The new implementation is integrated into the RocksDB core.\n# When set, large values (blobs) are written to separate blob files, and only\n# pointers to them are stored in SST files. This can reduce write amplification\n# for large-value use cases at the cost of introducing a level of indirection\n# for reads. Please see: https://github.com/facebook/rocksdb/wiki/BlobDB.\n#\n# Note that when enable_blob_files is set to yes, BlobDB-related configuration\n# items will take effect.\n#\n# Default: no\nrocksdb.enable_blob_files no\n\n# The size of the smallest value to be stored separately in a blob file. Values\n# which have an uncompressed size smaller than this threshold are stored alongside\n# the keys in SST files in the usual fashion.\n#\n# Default: 4096 byte, 0 means that all values are stored in blob files\nrocksdb.min_blob_size 4096\n\n# The size limit for blob files. When writing blob files, a new file is\n# opened once this limit is reached.\n#\n# Default: 268435456 bytes\nrocksdb.blob_file_size 268435456\n\n# Enables garbage collection of blobs. Valid blobs residing in blob files\n# older than a cutoff get relocated to new files as they are encountered\n# during compaction, which makes it possible to clean up blob files once\n# they contain nothing but obsolete/garbage blobs.\n# See also rocksdb.blob_garbage_collection_age_cutoff below.\n#\n# Default: yes\nrocksdb.enable_blob_garbage_collection yes\n\n# The percentage cutoff in terms of blob file age for garbage collection.\n# Blobs in the oldest N blob files will be relocated when encountered during\n# compaction, where N = (garbage_collection_cutoff/100) * number_of_blob_files.\n# Note that this value must belong to [0, 100].\n#\n# Default: 25\nrocksdb.blob_garbage_collection_age_cutoff 25\n\n\n# The purpose of the following three options are to dynamically adjust the upper limit of\n# the data that each layer can store according to the size of the different\n# layers of the LSM. Enabling this option will bring some improvements in\n# deletion efficiency and space amplification, but it will lose a certain\n# amount of read performance.\n# If you want to know more details about Levels' Target Size, you can read RocksDB wiki:\n# https://github.com/facebook/rocksdb/wiki/Leveled-Compaction#levels-target-size\n#\n# Default: yes\nrocksdb.level_compaction_dynamic_level_bytes yes\n\n# The total file size of level-1 sst.\n#\n# Default: 268435456 bytes\nrocksdb.max_bytes_for_level_base 268435456\n\n# Multiplication factor for the total file size of L(n+1) layers.\n# This option is a double type number in RocksDB, but kvrocks is\n# not support the double data type number yet, so we use integer\n# number instead of double currently.\n#\n# Default: 10\nrocksdb.max_bytes_for_level_multiplier 10\n\n# This feature only takes effect in Iterators and MultiGet.\n# If yes, RocksDB will try to read asynchronously and in parallel as much as possible to hide IO latency.\n# In iterators, it will prefetch data asynchronously in the background for each file being iterated on.\n# In MultiGet, it will read the necessary data blocks from those files in parallel as much as possible.\n\n# Default yes\nrocksdb.read_options.async_io yes\n\n# If yes, the write will be flushed from the operating system\n# buffer cache before the write is considered complete.\n# If this flag is enabled, writes will be slower.\n# If this flag is disabled, and the machine crashes, some recent\n# writes may be lost.  Note that if it is just the process that\n# crashes (i.e., the machine does not reboot), no writes will be\n# lost even if sync==false.\n#\n# Default: no\nrocksdb.write_options.sync no\n\n# If yes, writes will not first go to the write ahead log,\n# and the write may get lost after a crash.\n# You must keep wal enabled if you use replication.\n#\n# Default: no\nrocksdb.write_options.disable_wal no\n\n# If enabled and we need to wait or sleep for the write request, fails\n# immediately.\n#\n# Default: no\nrocksdb.write_options.no_slowdown no\n\n# If enabled, write requests are of lower priority if compaction is\n# behind. In this case, no_slowdown = true, the request will be canceled\n# immediately. Otherwise, it will be slowed down.\n# The slowdown value is determined by RocksDB to guarantee\n# it introduces minimum impacts to high priority writes.\n#\n# Default: no\nrocksdb.write_options.low_pri no\n\n# If enabled, this writebatch will maintain the last insert positions of each\n# memtable as hints in concurrent write. It can improve write performance\n# in concurrent writes if keys in one writebatch are sequential.\n#\n# Default: no\nrocksdb.write_options.memtable_insert_hint_per_batch no\n\n\n# Support RocksDB auto-tune rate limiter for the background IO\n# if enabled, Rate limiter will limit the compaction write if flush write is high\n# Please see https://rocksdb.org/blog/2017/12/18/17-auto-tuned-rate-limiter.html\n#\n# Default: yes\nrocksdb.rate_limiter_auto_tuned yes\n\n# If enabled, rocksdb will use partitioned full filters for each SST file.\n#\n# Default: yes\nrocksdb.partition_filters yes\n\n# Enable this option will schedule the deletion of obsolete files in a background thread\n# on iterator destruction. It can reduce the latency if there are many files to be removed.\n# see https://github.com/facebook/rocksdb/wiki/IO#avoid-blocking-io\n#\n# Default: yes\n# rocksdb.avoid_unnecessary_blocking_io yes\n\n# Specifies the maximum size in bytes for a write batch in RocksDB.\n# If set to 0, there is no size limit for write batches.\n# This option can help control memory usage and manage large WriteBatch operations more effectively.\n# \n# Default: 0\n# rocksdb.write_options.write_batch_max_bytes 0\n\n################################ NAMESPACE #####################################\n# namespace.test change.me\n"
        },
        {
          "name": "licenses",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "x.py",
          "type": "blob",
          "size": 18.1572265625,
          "content": "#!/usr/bin/env python3\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter, REMAINDER\nfrom glob import glob\nimport os\nfrom pathlib import Path\nimport re\nimport filecmp\nfrom subprocess import Popen, PIPE\nimport sys\nfrom typing import List, Any, Optional, TextIO, Tuple\nfrom shutil import which\n\nCMAKE_REQUIRE_VERSION = (3, 16, 0)\nCLANG_FORMAT_REQUIRED_VERSION = (12, 0, 0)\nCLANG_TIDY_REQUIRED_VERSION = (12, 0, 0)\nGOLANGCI_LINT_REQUIRED_VERSION = (1, 63, 3)\n\nSEMVER_REGEX = re.compile(\n    r\"\"\"\n        ^\n        (?P<major>0|[1-9]\\d*)\n        \\.\n        (?P<minor>0|[1-9]\\d*)\n        \\.\n        (?P<patch>0|[1-9]\\d*)\n        (?:-(?P<prerelease>\n            (?:0|[1-9]\\d*|\\d*[a-zA-Z-][0-9a-zA-Z-]*)\n            (?:\\.(?:0|[1-9]\\d*|\\d*[a-zA-Z-][0-9a-zA-Z-]*))*\n        ))?\n        (?:\\+(?P<build>\n            [0-9a-zA-Z-]+\n            (?:\\.[0-9a-zA-Z-]+)*\n        ))?\n        $\n    \"\"\",\n    re.VERBOSE,\n)\n\n\ndef run(*args: str, msg: Optional[str] = None, verbose: bool = False, **kwargs: Any) -> Popen:\n    sys.stdout.flush()\n    if verbose:\n        print(f\"$ {' '.join(args)}\")\n\n    p = Popen(args, **kwargs)\n    code = p.wait()\n    if code != 0:\n        err = f\"\\nfailed to run: {args}\\nexit with code: {code}\\n\"\n        if msg:\n            err += f\"error message: {msg}\\n\"\n        raise RuntimeError(err)\n\n    return p\n\n\ndef run_pipe(*args: str, msg: Optional[str] = None, verbose: bool = False, **kwargs: Any) -> TextIO:\n    p = run(*args, msg=msg, verbose=verbose, stdout=PIPE, universal_newlines=True, **kwargs)\n    return p.stdout  # type: ignore\n\n\ndef find_command(command: str, msg: Optional[str] = None) -> str:\n    return run_pipe(\"which\", command, msg=msg).read().strip()\n\n\ndef check_version(current: str, required: Tuple[int, int, int], prog_name: Optional[str] = None) -> Tuple[\n    int, int, int]:\n    require_version = '.'.join(map(str, required))\n    semver_match = SEMVER_REGEX.match(current)\n    if semver_match is None:\n        raise RuntimeError(f\"{prog_name} {require_version} or higher is required, got: {current}\")\n    semver_dict = semver_match.groupdict()\n    semver = (int(semver_dict[\"major\"]), int(semver_dict[\"minor\"]), int(semver_dict[\"patch\"]))\n    if semver < required:\n        raise RuntimeError(f\"{prog_name} {require_version} or higher is required, got: {current}\")\n\n    return semver\n\ndef prepare() -> None:\n    basedir = Path(__file__).parent.absolute()\n    \n    # Install Git hooks\n    hooks = basedir / \"dev\" / \"hooks\"\n    git_hooks = basedir / \".git\" / \"hooks\"\n\n    git_hooks.mkdir(exist_ok=True)\n    for hook in hooks.iterdir():\n        dst = git_hooks / hook.name\n        if dst.exists():\n            if filecmp.cmp(hook, dst, shallow=False):\n                print(f\"{hook.name} already installed.\")\n                continue\n            raise RuntimeError(f\"{dst} already exists; please remove it first\")\n        else:\n            dst.symlink_to(hook)\n            print(f\"{hook.name} installed at {dst}.\")\n\ndef build(dir: str, jobs: Optional[int], ghproxy: bool, ninja: bool, unittest: bool, compiler: str, cmake_path: str, D: List[str],\n          skip_build: bool) -> None:\n    basedir = Path(__file__).parent.absolute()\n\n    find_command(\"autoconf\", msg=\"autoconf is required to build jemalloc\")\n    cmake = find_command(cmake_path, msg=\"CMake is required\")\n\n    output = run_pipe(cmake, \"-version\")\n    output = run_pipe(\"head\", \"-n\", \"1\", stdin=output)\n    output = run_pipe(\"awk\", \"{print $(NF)}\", stdin=output)\n    cmake_version = output.read().strip()\n    check_version(cmake_version, CMAKE_REQUIRE_VERSION, \"CMake\")\n\n    os.makedirs(dir, exist_ok=True)\n\n    cmake_options = [\"-DCMAKE_BUILD_TYPE=RelWithDebInfo\"]\n    if ghproxy:\n        cmake_options.append(\"-DDEPS_FETCH_PROXY=https://mirror.ghproxy.com/\")\n    if ninja:\n        cmake_options.append(\"-G Ninja\")\n    if compiler == 'gcc':\n        cmake_options += [\"-DCMAKE_C_COMPILER=gcc\", \"-DCMAKE_CXX_COMPILER=g++\"]\n    elif compiler == 'clang':\n        cmake_options += [\"-DCMAKE_C_COMPILER=clang\", \"-DCMAKE_CXX_COMPILER=clang++\"]\n    if D:\n        cmake_options += [f\"-D{o}\" for o in D]\n\n    run(cmake, str(basedir), *cmake_options, verbose=True, cwd=dir)\n\n    if skip_build:\n        return\n\n    target = [\"kvrocks\", \"kvrocks2redis\"]\n    if unittest:\n        target.append(\"unittest\")\n\n    options = [\"--build\", \".\"]\n    if jobs is not None:\n        options.append(f\"-j{jobs}\")\n    options += [\"-t\", *target]\n\n    run(cmake, *options, verbose=True, cwd=dir)\n\n\ndef get_source_files(dir: Path) -> List[str]:\n    return [\n        *glob(str(dir / \"src/**/*.h\"), recursive=True),\n        *glob(str(dir / \"src/**/*.cc\"), recursive=True),\n        *glob(str(dir / \"tests/cppunit/**/*.h\"), recursive=True),\n        *glob(str(dir / \"tests/cppunit/**/*.cc\"), recursive=True),\n        *glob(str(dir / \"utils/kvrocks2redis/**/*.h\"), recursive=True),\n        *glob(str(dir / \"utils/kvrocks2redis/**/*.cc\"), recursive=True),\n    ]\n\n\ndef clang_format(clang_format_path: str, fix: bool = False) -> None:\n    command = find_command(clang_format_path, msg=\"clang-format is required\")\n\n    version_res = run_pipe(command, '--version').read().strip()\n    version_str = re.search(r'version\\s+((?:\\w|\\.)+)', version_res).group(1)\n\n    check_version(version_str, CLANG_FORMAT_REQUIRED_VERSION, \"clang-format\")\n\n    basedir = Path(__file__).parent.absolute()\n    sources = get_source_files(basedir)\n\n    if fix:\n        options = ['-i']\n    else:\n        options = ['--dry-run', '--Werror']\n\n    run(command, *options, *sources, verbose=True, cwd=basedir)\n\n\ndef clang_tidy(dir: str, jobs: Optional[int], clang_tidy_path: str, run_clang_tidy_path: str, fix: bool) -> None:\n    # use the run-clang-tidy Python script provided by LLVM Clang\n    run_command = find_command(run_clang_tidy_path, msg=\"run-clang-tidy is required\")\n    tidy_command = find_command(clang_tidy_path, msg=\"clang-tidy is required\")\n\n    version_res = run_pipe(tidy_command, '--version').read().strip()\n    version_str = re.search(r'version\\s+((?:\\w|\\.)+)', version_res).group(1)\n\n    check_version(version_str, CLANG_TIDY_REQUIRED_VERSION, \"clang-tidy\")\n\n    if not (Path(dir) / 'compile_commands.json').exists():\n        raise RuntimeError(f\"expect compile_commands.json in build directory {dir}\")\n\n    basedir = Path(__file__).parent.absolute()\n\n    options = ['-p', dir, '-clang-tidy-binary', tidy_command]\n    if jobs is not None:\n        options.append(f'-j{jobs}')\n\n    options.extend(['-fix'] if fix else [])\n\n    regexes = ['kvrocks/src/', 'utils/kvrocks2redis/', 'tests/cppunit/']\n\n    options.append(f'-header-filter={\"|\".join(regexes)}')\n\n    run(run_command, *options, *regexes, verbose=True, cwd=basedir)\n\n\ndef golangci_lint(golangci_lint_path: str) -> None:\n    def get_gopath() -> Tuple[Path, Path]:\n        go = find_command('go', msg='go is required for testing')\n        gopath = run_pipe(go, 'env', 'GOPATH').read().strip()\n        bindir = Path(gopath).absolute() / 'bin'\n        binpath = bindir / 'golangci-lint'\n        return bindir, binpath\n\n    def get_syspath(sys_path: str) -> Tuple[str, str]:\n        golangci_command = find_command(sys_path, msg=\"golangci-lint is required\")\n        version_res = run_pipe(golangci_command, '--version').read().strip()\n        version_str = re.search(r'version\\s+((?:\\w|\\.)+)', version_res).group(1)\n        return golangci_command, version_str\n\n    def download_package(bindir: str) -> None:\n        output = run_pipe('curl', '-sfL', 'https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh',\n                            verbose=True)\n        version_str = 'v' + '.'.join(map(str, GOLANGCI_LINT_REQUIRED_VERSION))\n        run('sh', '-s', '--', '-b', bindir, version_str, verbose=True, stdin=output)\n\n    if which(golangci_lint_path) is None:\n        bindir, binpath = get_gopath()\n        binpath_str = str(binpath)\n        if not binpath.exists():\n            download_package(str(bindir))\n\n    else:\n        binpath_str, version_str = get_syspath(golangci_lint_path)\n        check_version(version_str, GOLANGCI_LINT_REQUIRED_VERSION, \"golangci-lint\")\n\n    basedir = Path(__file__).parent.absolute() / 'tests' / 'gocase'\n    run(binpath_str, 'run', '-v', './...', cwd=str(basedir), verbose=True)\n\n\ndef write_version(release_version: str) -> str:\n    version = release_version.strip()\n    if SEMVER_REGEX.match(version) is None:\n        raise RuntimeError(f\"Kvrocks version should follow semver spec, got: {version}\")\n\n    with open('src/VERSION.txt', 'w+') as f:\n        f.write(version)\n\n    return version\n\n\ndef package_source(release_version: str, release_candidate_number: Optional[int]) -> None:\n    # 0. Write input version to VERSION file\n    version = write_version(release_version)\n\n    # 1. Git commit and tag\n    git = find_command('git', msg='git is required for source packaging')\n    run(git, 'commit', '-a', '-m', f'release: prepare source release apache-kvrocks-{version}')\n    if release_candidate_number is None:\n        run(git, 'tag', '-a', f'v{version}', '-m', f'release: copy for tag v{version}')\n    else:\n        run(git, 'tag', '-a', f'v{version}-rc{release_candidate_number}', '-m', f'release: copy for tag v{version}-rc{release_candidate_number}')\n\n    # 2. Create the source tarball\n    folder = f'apache-kvrocks-{version}-src'\n    tarball = f'apache-kvrocks-{version}-src.tar.gz'\n    run(git, 'archive', '--format=tar.gz', f'--output={tarball}', f'--prefix={folder}/', 'HEAD')\n\n    # 3. GPG Sign\n    gpg = find_command('gpg', msg='gpg is required for source packaging')\n    run(gpg, '--detach-sign', '--armor', tarball)\n\n    # 4. Generate sha512 checksum\n    shasum = find_command('shasum', msg='shasum is required for source packaging')\n    with open(f'{tarball}.sha512', 'w+') as f:\n        run(shasum, '-a', '512', tarball, stdout=f)\n\n\ndef test_cpp(dir: str, rest: List[str]) -> None:\n    basedir = Path(dir).absolute()\n    unittest = basedir / 'unittest'\n\n    run(str(unittest), *rest, cwd=str(basedir), verbose=True)\n\n\ndef test_go(dir: str, cli_path: str, rest: List[str]) -> None:\n    go = find_command('go', msg='go is required for testing')\n    find_command(cli_path, msg='redis-cli is required for testing')\n\n    binpath = Path(dir).absolute() / 'kvrocks'\n    basedir = Path(__file__).parent.absolute() / 'tests' / 'gocase'\n    workspace = basedir / 'workspace'\n\n    args = [\n        'test', '-timeout=1800s', '-bench=.', './...',\n        f'-binPath={binpath}',\n        f'-cliPath={cli_path}',\n        f'-workspace={workspace}',\n        *rest\n    ]\n\n    run(go, *args, cwd=str(basedir), verbose=True)\n\n\nif __name__ == '__main__':\n    parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n    parser.set_defaults(func=parser.print_help)\n\n    subparsers = parser.add_subparsers()\n\n    parser_format = subparsers.add_parser(\n        'format',\n        description=\"Format source code\",\n        help=\"Format source code\")\n    parser_format.set_defaults(func=lambda **args: clang_format(**args, fix=True))\n    parser_format.add_argument('--clang-format-path', default='clang-format',\n                               help=\"path of clang-format used to check source\")\n\n    parser_check = subparsers.add_parser(\n        'check',\n        description=\"Check or lint source code\",\n        help=\"Check or lint source code\")\n    parser_check.set_defaults(func=parser_check.print_help)\n    parser_check_subparsers = parser_check.add_subparsers()\n    parser_check_format = parser_check_subparsers.add_parser(\n        'format',\n        description=\"Check source format by clang-format\",\n        help=\"Check source format by clang-format\")\n    parser_check_format.set_defaults(func=lambda **args: clang_format(**args, fix=False))\n    parser_check_format.add_argument('--clang-format-path', default='clang-format',\n                                     help=\"path of clang-format used to check source\")\n    parser_check_tidy = parser_check_subparsers.add_parser(\n        'tidy',\n        description=\"Check code with clang-tidy\",\n        help=\"Check code with clang-tidy\",\n        formatter_class=ArgumentDefaultsHelpFormatter,\n    )\n    parser_check_tidy.set_defaults(func=clang_tidy)\n    parser_check_tidy.add_argument('dir', metavar='BUILD_DIR', nargs='?', default='build',\n                                   help=\"directory to store cmake-generated and build files\")\n    parser_check_tidy.add_argument('-j', '--jobs', metavar='N', help='execute N build jobs concurrently')\n    parser_check_tidy.add_argument('--clang-tidy-path', default='clang-tidy',\n                                   help=\"path of clang-tidy used to check source\")\n    parser_check_tidy.add_argument('--run-clang-tidy-path', default='run-clang-tidy',\n                                   help=\"path of run-clang-tidy used to check source\")\n    parser_check_tidy.add_argument('--fix', default=False, action='store_true',\n                              help='automatically fix codebase via clang-tidy suggested changes')\n    parser_check_golangci_lint = parser_check_subparsers.add_parser(\n        'golangci-lint',\n        description=\"Check code with golangci-lint (https://golangci-lint.run/)\",\n        help=\"Check code with golangci-lint (https://golangci-lint.run/)\",\n        formatter_class=ArgumentDefaultsHelpFormatter,\n    )\n    parser_check_golangci_lint.set_defaults(func=golangci_lint)\n    parser_check_golangci_lint.add_argument('--golangci-lint-path', default='golangci-lint',\n                                   help=\"path of golangci-lint used to check source\")\n    \n    parser_build = subparsers.add_parser(\n        'build',\n        description=\"Build executables to BUILD_DIR [default: build]\",\n        help=\"Build executables to BUILD_DIR [default: build]\",\n        formatter_class=ArgumentDefaultsHelpFormatter,\n    )\n    parser_build.add_argument('dir', metavar='BUILD_DIR', nargs='?', default='build',\n                              help=\"directory to store cmake-generated and build files\")\n    parser_build.add_argument('-j', '--jobs', metavar='N', help='execute N build jobs concurrently')\n    parser_build.add_argument('--ghproxy', default=False, action='store_true',\n                              help='use https://mirror.ghproxy.com to fetch dependencies')\n    parser_build.add_argument('--ninja', default=False, action='store_true', help='use Ninja to build kvrocks')\n    parser_build.add_argument('--unittest', default=False, action='store_true', help='build unittest target')\n    parser_build.add_argument('--compiler', default='auto', choices=('auto', 'gcc', 'clang'),\n                              help=\"compiler used to build kvrocks\")\n    parser_build.add_argument('--cmake-path', default='cmake', help=\"path of cmake binary used to build kvrocks\")\n    parser_build.add_argument('-D', action='append', metavar='key=value', help='extra CMake definitions')\n    parser_build.add_argument('--skip-build', default=False, action='store_true',\n                              help='runs only the configure stage, skip the build stage')\n    parser_build.set_defaults(func=build)\n\n    parser_package = subparsers.add_parser(\n        'package',\n        description=\"Package the source tarball or binary installer\",\n        help=\"Package the source tarball or binary installer\",\n        formatter_class=ArgumentDefaultsHelpFormatter,\n    )\n    parser_package.set_defaults(func=parser_package.print_help)\n    parser_package_subparsers = parser_package.add_subparsers()\n    parser_package_source = parser_package_subparsers.add_parser(\n        'source',\n        description=\"Package the source tarball\",\n        help=\"Package the source tarball\",\n    )\n    parser_package_source.add_argument('-v', '--release-version', required=True, metavar='VERSION',\n                                       help='current releasing version')\n    parser_package_source.add_argument('-rc', '--release-candidate-number',required=False, type=int, help='current releasing candidate number')\n    parser_package_source.set_defaults(func=package_source)\n\n    parser_test = subparsers.add_parser(\n        'test',\n        description=\"Test against a specific kvrocks build\",\n        help=\"Test against a specific kvrocks build\",\n        formatter_class=ArgumentDefaultsHelpFormatter,\n    )\n    parser_test.set_defaults(func=parser_test.print_help)\n    parser_test_subparsers = parser_test.add_subparsers()\n\n    parser_test_cpp = parser_test_subparsers.add_parser(\n        'cpp',\n        description=\"Test kvrocks via cpp unit tests\",\n        help=\"Test kvrocks via cpp unit tests\",\n    )\n    parser_test_cpp.add_argument('dir', metavar='BUILD_DIR', nargs='?', default='build',\n                                 help=\"directory including kvrocks build files\")\n    parser_test_cpp.add_argument('rest', nargs=REMAINDER, help=\"the rest of arguments to forward to cpp unittest\")\n    parser_test_cpp.set_defaults(func=test_cpp)\n\n    parser_test_go = parser_test_subparsers.add_parser(\n        'go',\n        description=\"Test kvrocks via go test cases\",\n        help=\"Test kvrocks via go test cases\",\n    )\n    parser_test_go.add_argument('dir', metavar='BUILD_DIR', nargs='?', default='build',\n                                help=\"directory including kvrocks build files\")\n    parser_test_go.add_argument('--cli-path', default='redis-cli', help=\"path of redis-cli to test kvrocks\")\n    parser_test_go.add_argument('rest', nargs=REMAINDER, help=\"the rest of arguments to forward to go test\")\n    parser_test_go.set_defaults(func=test_go)\n\n    parser_prepare = subparsers.add_parser(\n        'prepare',\n        description=\"Prepare scripts such as git hooks\",\n        help=\"Prepare scripts such as git hooks\"\n    )\n    parser_prepare.set_defaults(func=prepare)\n\n    args = parser.parse_args()\n\n    arg_dict = dict(vars(args))\n    del arg_dict['func']\n    args.func(**arg_dict)\n"
        }
      ]
    }
  ]
}