{
  "metadata": {
    "timestamp": 1736565873385,
    "page": 829,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/SparseConvNet",
      "stars": 2080,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1767578125,
          "content": "SparseConvNetTorch/build/\n*.pth\n*.o\n*.a\n*.so\nbuild\n__pycache__\npickle\n*.pyc\nPyTorch/sparseconvnet.egg-info/\nPyTorch/sparseconvnet/SCN/__init__.py\nsparseconvnet.egg-info\n*.zip\n*.rar\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 0.23828125,
          "content": "# Code of Conduct\n\nFacebook has adopted a Code of Conduct that we expect project participants to adhere to.\nPlease read the [full text](https://code.fb.com/codeofconduct/)\nso that you can understand what actions will and will not be tolerated.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.185546875,
          "content": "# Contributing to SparseConvNet\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `master`.\n2. Ensure the examples still run.\n3. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nFacebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n## Coding Style  \nWe try to follow the PEP style guidelines and encourage you to as well.\n\n## License\nBy contributing to SparseConvNet, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.50390625,
          "content": "BSD License\n\nFor SparseConvNet software\n\nCopyright (c) Facebook, Inc. and its affiliates. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n * Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n\n * Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\n * Neither the name Facebook nor the names of its contributors may be used to\n   endorse or promote products derived from this software without specific\n   prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.310546875,
          "content": "# Submanifold Sparse Convolutional Networks\n\n[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB)](https://opensource.fb.com/support-ukraine)\n\nThis is the PyTorch library for training Submanifold Sparse Convolutional Networks.\n\n## Spatial sparsity\n\nThis library brings [Spatially-sparse convolutional networks](https://github.com/btgraham/SparseConvNet) to PyTorch. Moreover, it introduces **Submanifold Sparse Convolutions**, that can be used to build computationally efficient sparse VGG/ResNet/DenseNet-style networks.\n\nWith regular 3x3 convolutions, the set of active (non-zero) sites grows rapidly:<br />\n![submanifold](img/i.gif) <br />\nWith **Submanifold Sparse Convolutions**, the set of active sites is unchanged. Active sites look at their active neighbors (green); non-active sites (red) have no computational overhead: <br />\n![submanifold](img/img.gif) <br />\nStacking Submanifold Sparse Convolutions to build VGG and ResNet type ConvNets, information can flow along lines or surfaces of active points.<br />\n\nDisconnected components don't communicate at first, although they will merge due to the effect of strided operations, either pooling or convolutions. Additionally, adding ConvolutionWithStride2-SubmanifoldConvolution-DeconvolutionWithStride2 paths to the network allows disjoint active sites to communicate; see the 'VGG+' networks in the paper.<br />\n![Strided Convolution, convolution, deconvolution](img/img_stridedConv_conv_deconv.gif) <br />\n![Strided Convolution, convolution, deconvolution](img/img_stridedConv_conv_deconv.png) <br />\nFrom left: **(i)** an active point is highlighted; a convolution with stride 2 sees the green active sites **(ii)** and produces output **(iii)**, 'children' of hightlighted active point from (i) are highlighted; a submanifold sparse convolution sees the green active sites **(iv)** and produces output **(v)**; a deconvolution operation sees the green active sites **(vi)**  and produces output **(vii)**.\n\n## Dimensionality and 'submanifolds'\n\nSparseConvNet supports input with different numbers of spatial/temporal dimensions.\nHigher dimensional input is more likely to be sparse because of the 'curse of dimensionality'. <br />\n\n  Dimension|Name in 'torch.nn'|Use cases\n  :--:|:--:|:--:\n  1|Conv1d| Text, audio\n  2|Conv2d|Lines in 2D space, e.g. handwriting\n  3|Conv3d|Lines and surfaces in 3D space or (2+1)D space-time\n  4| - |Lines, etc,  in (3+1)D space-time\n\nWe use the term 'submanifold' to refer to input data that is sparse because it has a lower effective dimension than the space in which it lives, for example a one-dimensional curve in 2+ dimensional space, or a two-dimensional surface in 3+ dimensional space.\n\nIn theory, the library supports up to 10 dimensions. In practice, ConvNets with size-3 SVC convolutions in dimension 5+ may be impractical as the number of parameters per convolution is growing exponentially. Possible solutions include factorizing the convolutions (e.g. 3x1x1x..., 1x3x1x..., etc), or switching to a hyper-tetrahedral lattice (see [Sparse 3D convolutional neural networks](http://arxiv.org/abs/1505.02890)).\n\n\n\n\n\n## Hello World\nSparseConvNets can be built either by [defining a function that inherits from torch.nn.Module](examples/Assamese_handwriting/VGGplus.py) or by stacking modules in a [sparseconvnet.Sequential](PyTorch/sparseconvnet/sequential.py):\n```\nimport torch\nimport sparseconvnet as scn\n\n# Use the GPU if there is one, otherwise CPU\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\nmodel = scn.Sequential().add(\n    scn.SparseVggNet(2, 1,\n                     [['C', 8], ['C', 8], ['MP', 3, 2],\n                      ['C', 16], ['C', 16], ['MP', 3, 2],\n                      ['C', 24], ['C', 24], ['MP', 3, 2]])\n).add(\n    scn.SubmanifoldConvolution(2, 24, 32, 3, False)\n).add(\n    scn.BatchNormReLU(32)\n).add(\n    scn.SparseToDense(2, 32)\n).to(device)\n\n# output will be 10x10\ninputSpatialSize = model.input_spatial_size(torch.LongTensor([10, 10]))\ninput_layer = scn.InputLayer(2, inputSpatialSize)\n\nmsgs = [[\" X   X  XXX  X    X    XX     X       X   XX   XXX   X    XXX   \",\n         \" X   X  X    X    X   X  X    X       X  X  X  X  X  X    X  X  \",\n         \" XXXXX  XX   X    X   X  X    X   X   X  X  X  XXX   X    X   X \",\n         \" X   X  X    X    X   X  X     X X X X   X  X  X  X  X    X  X  \",\n         \" X   X  XXX  XXX  XXX  XX       X   X     XX   X  X  XXX  XXX   \"],\n\n        [\" XXX              XXXXX      x   x     x  xxxxx  xxx \",\n         \" X  X  X   XXX       X       x   x x   x  x     x  x \",\n         \" XXX                X        x   xxxx  x  xxxx   xxx \",\n         \" X     X   XXX       X       x     x   x      x    x \",\n         \" X     X          XXXX   x   x     x   x  xxxx     x \",]]\n\n\n# Create Nx3 and Nx1 vectors to encode the messages above:\nlocations = []\nfeatures = []\nfor batchIdx, msg in enumerate(msgs):\n    for y, line in enumerate(msg):\n        for x, c in enumerate(line):\n            if c == 'X':\n                locations.append([y, x, batchIdx])\n                features.append([1])\nlocations = torch.LongTensor(locations)\nfeatures = torch.FloatTensor(features).to(device)\n\ninput = input_layer([locations,features])\nprint('Input SparseConvNetTensor:', input)\noutput = model(input)\n\n# Output is 2x32x10x10: our minibatch has 2 samples, the network has 32 output\n# feature planes, and 10x10 is the spatial size of the output.\nprint('Output SparseConvNetTensor:', output)\n```\n\n\n## Examples\n\nExamples in the examples folder include\n* [Assamese handwriting recognition](https://archive.ics.uci.edu/ml/datasets/Online+Handwritten+Assamese+Characters+Dataset#)\n* [Chinese handwriting for recognition](http://www.nlpr.ia.ac.cn/databases/handwriting/Online_database.html)\n* [3D Segmentation](https://shapenet.cs.stanford.edu/iccv17/) using ShapeNet Core-55\n* [ScanNet](http://www.scan-net.org/) 3D Semantic label benchmark\n\nFor example:\n```\ncd examples/Assamese_handwriting\npython VGGplus.py\n```\n\n## Setup\n\nTested with PyTorch 1.3, CUDA 10.0, and Python 3.3 with [Conda](https://www.anaconda.com/).\n\n```\nconda install pytorch torchvision cudatoolkit=10.0 -c pytorch # See https://pytorch.org/get-started/locally/\ngit clone git@github.com:facebookresearch/SparseConvNet.git\ncd SparseConvNet/\nbash develop.sh\n```\nTo run the examples you may also need to install unrar:\n```\napt-get install unrar\n```\n\n## License\nSparseConvNet is BSD licensed, as found in the LICENSE file. [Terms of use](https://opensource.facebook.com/legal/terms). [Privacy](https://opensource.facebook.com/legal/privacy)\n\nCopyright © Meta Platforms, Inc\n\n## Links\n1. [ICDAR 2013 Chinese Handwriting Recognition Competition 2013](https://web.archive.org/web/20160418143451/http://www.nlpr.ia.ac.cn/events/CHRcompetition2013/competition/Home.html) First place in task 3, with test error of 2.61%. Human performance on the test set was 4.81%. [Report](https://web.archive.org/web/20160910012723/http://www.nlpr.ia.ac.cn/events/CHRcompetition2013/competition/ICDAR%202013%20CHR%20competition.pdf)\n2. [Spatially-sparse convolutional neural networks, 2014](http://arxiv.org/abs/1409.6070) SparseConvNets for Chinese handwriting recognition\n3. [Fractional max-pooling, 2014](http://arxiv.org/abs/1412.6071) A SparseConvNet with fractional max-pooling achieves an error rate of 3.47% for CIFAR-10.\n4. [Sparse 3D convolutional neural networks, BMVC 2015](http://arxiv.org/abs/1505.02890) SparseConvNets for 3D object recognition and (2+1)D video action recognition.\n5. [Kaggle plankton recognition competition, 2015](https://www.kaggle.com/c/datasciencebowl) Third place. The competition solution is being adapted for research purposes in [EcoTaxa](http://ecotaxa.obs-vlfr.fr/).\n6. [Kaggle Diabetic Retinopathy Detection, 2015](https://www.kaggle.com/c/diabetic-retinopathy-detection/) First place in the Kaggle Diabetic Retinopathy Detection competition.\n7. [SparseConvNet 'classic'](https://github.com/btgraham/SparseConvNet-archived) version\n8. [Submanifold Sparse Convolutional Networks, 2017](https://arxiv.org/abs/1706.01307) Introduces deep 'submanifold' SparseConvNets.\n9. [Workshop on Learning to See from 3D Data, 2017](https://shapenet.cs.stanford.edu/iccv17workshop/) First place in the [semantic segmentation](https://shapenet.cs.stanford.edu/iccv17/) competition. [Report](https://arxiv.org/pdf/1710.06104)\n10. [3D Semantic Segmentation with Submanifold Sparse Convolutional Networks, 2017](https://arxiv.org/abs/1711.10275) Semantic segmentation for the ShapeNet Core55 and NYU-DepthV2 datasets, CVPR 2018\n11. [Unsupervised learning with sparse space-and-time autoencoders](https://arxiv.org/abs/1811.10355) (3+1)D space-time autoencoders\n12. [ScanNet 3D semantic label benchmark 2018](http://kaldir.vc.in.tum.de/scannet_benchmark/semantic_label_3d) 0.726 average IOU for 3D semantic segmentation.\n13. [MinkowskiEngine](https://github.com/StanfordVL/MinkowskiEngine) is an alternative implementation of SparseConvNet; [0.736 average IOU for ScanNet]( https://github.com/chrischoy/SpatioTemporalSegmentation).\n14. [SpConv: PyTorch Spatially Sparse Convolution Library](https://github.com/traveller59/spconv) is an alternative implementation of SparseConvNet.\n15. [Live Semantic 3D Perception for Immersive Augmented Reality](https://ieeexplore.ieee.org/document/8998140) describes a way to optimize memory access for SparseConvNet.\n16. [OccuSeg](https://arxiv.org/abs/2003.06537) real-time object detection using SparseConvNets.\n17. [TorchSparse](https://github.com/mit-han-lab/torchsparse) implements 3D submanifold convolutions.\n18. [TensorFlow 3D](https://github.com/google-research/google-research/tree/master/tf3d) implements submanifold convolutions.\n19. [VoTr](https://github.com/PointsCoder/VOTR) implements submanifold [voxel transformers](https://openaccess.thecvf.com/content/ICCV2021/papers/Mao_Voxel_Transformer_for_3D_Object_Detection_ICCV_2021_paper.pdf) using [SpConv](https://github.com/traveller59/spconv).\n20. [Mix3D](https://github.com/kumuji/mix3d) brings [MixUp](https://openreview.net/forum?id=r1Ddp1-Rb) to the sparse setting&mdash; 0.781 average IOU for ScanNet 3D semantic segmentation.\n21. [Point Transformer V3](https://arxiv.org/abs/2312.10035) uses sparse convolutions as an enhanced conditional positional encoding (xCPE); 0.794 average IOU for ScanNet 3D semantic segmentation.\n\n## Citations\n\nIf you find this code useful in your research then please cite:\n\n**[3D Semantic Segmentation with Submanifold Sparse Convolutional Networks, CVPR 2018](https://arxiv.org/abs/1711.10275)** <br />\n[Benjamin Graham](https://research.fb.com/people/graham-benjamin/), <br />\n[Martin Engelcke](http://ori.ox.ac.uk/mrg_people/martin-engelcke/), <br />\n[Laurens van der Maaten](https://lvdmaaten.github.io/), <br />\n\n```\n@article{3DSemanticSegmentationWithSubmanifoldSparseConvNet,\n  title={3D Semantic Segmentation with Submanifold Sparse Convolutional Networks},\n  author={Graham, Benjamin and Engelcke, Martin and van der Maaten, Laurens},\n  journal={CVPR},\n  year={2018}\n}\n```\n\nand/or\n\n**[Submanifold Sparse Convolutional Networks, https://arxiv.org/abs/1706.01307](https://arxiv.org/abs/1706.01307)** <br />\n[Benjamin Graham](https://research.fb.com/people/graham-benjamin/), <br />\n[Laurens van der Maaten](https://lvdmaaten.github.io/), <br />\n\n```\n@article{SubmanifoldSparseConvNet,\n  title={Submanifold Sparse Convolutional Networks},\n  author={Graham, Benjamin and van der Maaten, Laurens},\n  journal={arXiv preprint arXiv:1706.01307},\n  year={2017}\n}\n```\n"
        },
        {
          "name": "build.sh",
          "type": "blob",
          "size": 0.3525390625,
          "content": "#!/bin/bash\n# Copyright 2016-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree.\n\n#export TORCH_CUDA_ARCH_LIST=\"6.0;6.1;6.2;7.0;7.5\"\nrm -rf build/ dist/ sparseconvnet.egg-info\npython setup.py install && python examples/hello-world.py\n"
        },
        {
          "name": "develop.sh",
          "type": "blob",
          "size": 0.3740234375,
          "content": "#!/bin/bash\n# Copyright 2016-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree.\n\n#export TORCH_CUDA_ARCH_LIST=\"6.0;6.1;6.2;7.0;7.5\"\nrm -rf build/ dist/ sparseconvnet.egg-info sparseconvnet_SCN*.so\npython setup.py develop && python examples/hello-world.py\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "img",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.697265625,
          "content": "# Copyright 2016-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch, os\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension, CUDAExtension\nfrom setuptools import setup, find_packages\n\nif torch.cuda.is_available():\n    assert torch.matmul(torch.ones(2097153,2).cuda(),torch.ones(2,2).cuda()).min().item()==2, 'Please upgrade from CUDA 9.0 to CUDA 10.0+'\n\nthis_dir = os.path.dirname(os.path.realpath(__file__))\ntorch_dir = os.path.dirname(torch.__file__)\nconda_include_dir = '/'.join(torch_dir.split('/')[:-4]) + '/include'\nextra = {'cxx': ['-std=c++17', '-fopenmp','-O3'], 'nvcc': ['-std=c++17', '-Xcompiler', '-fopenmp', '-O3']}\n\nsetup(\n    name='sparseconvnet',\n    version='0.2',\n    description='Submanifold (Spatially) Sparse Convolutional Networks https://arxiv.org/abs/1706.01307',\n    author='Facebook AI Research',\n    author_email='benjamingraham@fb.com',\n    url='https://github.com/facebookresearch/SparseConvNet',\n    packages=['sparseconvnet','sparseconvnet.SCN'],\n    ext_modules=[\n      CUDAExtension('sparseconvnet.SCN',\n        [\n         'sparseconvnet/SCN/cuda.cu', 'sparseconvnet/SCN/sparseconvnet_cuda.cpp', 'sparseconvnet/SCN/pybind.cpp'],\n        include_dirs=[this_dir+'/sparseconvnet/SCN/'],\n        extra_compile_args=extra)\n      if torch.cuda.is_available()  else\n      CppExtension('sparseconvnet.SCN',\n        ['sparseconvnet/SCN/pybind.cpp', 'sparseconvnet/SCN/sparseconvnet_cpu.cpp'],\n        include_dirs=[this_dir+'/sparseconvnet/SCN/'],\n        extra_compile_args=extra['cxx'])],\n    cmdclass={'build_ext': BuildExtension},\n    zip_safe=False,\n)\n"
        },
        {
          "name": "sparseconvnet",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}