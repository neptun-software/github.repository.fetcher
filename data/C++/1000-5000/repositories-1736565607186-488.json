{
  "metadata": {
    "timestamp": 1736565607186,
    "page": 488,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "dendibakh/perf-ninja",
      "stars": 2727,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.1455078125,
          "content": "﻿root = true\n\n[*]\ncharset = utf-8\ntab_width = 2\nindent_size = 2\nindent_style = space\ninsert_final_newline = true\n\n[.*]\ninsert_final_newline = false"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3837890625,
          "content": "CMakeLists.txt.user\nCMakeCache.txt\nCMakeFiles\nCMakeScripts\nTesting\nMakefile\ncmake_install.cmake\ninstall_manifest.txt\ncompile_commands.json\nCTestTestfile.cmake\n_deps\n__pycache__\n*.pyc\n.idea/\ncmake-build-debug/\n.DS_Store\nlabs/misc/io_opt1/DataPaths.h\nlabs/misc/io_opt1/data/\nlabs/memory_bound/mem_order_violation_1/DataPaths.h\n\nbuild_solution\nbuild_baseline\nbuild\nresults.json\n/tools/benchmark/\n"
        },
        {
          "name": "Contributing.md",
          "type": "blob",
          "size": 2.0546875,
          "content": "# Contributing\r\n\r\nWe are looking for proposals and implementations of the new lab assignments similar to the ones we already have.\r\n\r\nSome criterias:\r\n- We prefer a small localized example over a big application (ideally within 1KLOC). But if something is hard to write on our own, we could reuse existing benchmark.\r\n- We prefer real-world problems over synthesized ones. But synthesized benchmarks are welcome too.\r\n- Performance issue in the lab assignment should be the top hotspot. I.e. the benchmark should be stressing the major performance bottleneck, not the secondary one.\r\n- Deoptimize existing benchmarks/workloads and making a lab out of it is also fine. I.e. if you know of a case when a certain optimization *was* made in the codebase, we can use the version *before* that change was introduced.\r\n\r\nPlease *do not* submit new lab assignments as PR against this repo. Otherwise we will spoil the solution :) . I have internal repo, which we use for staging the code for new lab assignments. Write to me and I will give you access there.\r\n\r\nWe have created a template for streamlining the process of creating new lab assignments. You don't have to spend time on setting up an infrastructure and you can focus directly on the code of the assignment.\r\n\r\nWe are also looking to increase diversity of the platforms on which we test your submissions. For now, the CI runs on a dedicated Intel x86 Linux box (at Denis' home). We would gladly extend this list with ARM- and AMD-based machines. If you have one sitting idle in at your desk, consider using it to run CI jobs for this project. All you need is to install Github [runner client](https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners), which you can kill at any time.\r\n\r\nPlease write to dendibakh@gmail.com with any comments & suggestions.\r\n\r\n## License\r\n\r\nThis project is licensed under the terms of the Creative Commons license ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)). By contributing to this project, you agree to the copyright terms and release your contribution under these terms."
        },
        {
          "name": "GetStarted.md",
          "type": "blob",
          "size": 6.0361328125,
          "content": "# Get started\n\n## How to set up the environment\n\nHere is the list of tools you *absolutely* have to install to build labs in this video course:\n* CMake 3.13\n* [Google benchmark](https://github.com/google/benchmark), you can also use the scripts in the [tools](tools) directory.\n\nOthers are optional depending on your platform of choice. So far we support native builds on Windows and Linux. Check out the instructions specific to each platform ([Windows](QuickstartWindows.md)) ([Linux](QuickstartLinux.md)) ([MacOS](QuickstartMacOS.md)).\n\n## How to build lab assignments\n\nWatch the warmup video:\n\n[<img src=\"img/WarmupLabAssignment.png\">](https://www.youtube.com/watch?v=jFRwAcIoLgQ&list=PLRWO2AL1QAV6bJAU2kgB4xfodGID43Y5d)\n\nEvery lab assignment has the following:\n* Video that introduces a particular transformation.\n* Baseline version of a workload that has a particular performance bottleneck in it. You need to find it and fix the source code accordingly.\n* Summary video that explains the solution for the lab.\n\nWe encourage you to work on the lab assignment first, without watching the summary video.\n\nEvery lab can be built and run using the following commands:\n```\ncmake -E make_directory build\ncd build\ncmake -DCMAKE_BUILD_TYPE=Release ..\ncmake --build . --config Release --parallel 8\ncmake --build . --target validateLab\ncmake --build . --target benchmarkLab\n```\nWhen you push changes to your private branch, it will automatically trigger a CI benchmarking job. More details about it are at the bottom of the page.\n\n## Profiling\n\nTo match assembly code back to the source in the profile, build your binaries with the debug information:\n```\ncmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_C_FLAGS=\"-g\" -DCMAKE_CXX_FLAGS=\"-g\" ..\n```\n\nLab assignments are built on top of the Google Benchmark library, which by default performs a variable number of benchmark iterations. That makes it hard to compare the performance profiles of two runs since they will not do the same amount of work. You can see the same wall time even though the number of iterations is different. To fix the number of iterations, you can make the following changes:\n\n```\n  BENCHMARK(bench1)->Iterations(10);\n```\n\nThis will instruct the Google Benchmark framework to execute exactly 10 iterations of the benchmark. Now when you improve your code you can also compare performance profiles since the wall time will be different.\n\n## Target platforms\n\nYou are free to work on whatever platform you have at your disposal. However, we use the following CI machines to run your submissions:\n\n**Machine 1 - (Linux + Alderlake)**\n\n* 12th Gen Intel(R) Core(TM) i7-1260P CPU @ 2.10GHz (4.70GHz Turbo), 4P+8E cores, 18MB L3-cache\n* 16 GB RAM, DDR4 @ 3200 MT/s\n* 256GB NVMe PCIe M.2 SSD\n* Ubuntu 24.04 LTS, kernel version 6.8\n* Clang C++ compiler, version 17.0\n\n**Machine 2 - (Mac OS + M1)**\n\n* Mac mini (M1, 2020) @ 3.20GHz max frequency, 4P+4E cores, 8 MB LLC\n* 16 GB RAM, LPDDR4\n* 256GB NVME APPLE SSD AP0256Q\n* macOS 13.5.1 Ventura (22G90)\n* Clang C++ compiler, version 17.0\n\n**Machine 3 - (Windows + Zen3)**\n\n* AMD Ryzen 7 3700X 8-Core Processor @ 3.6GHz (4.40GHz Turbo), 32MB L3-cache\n* 64 GB RAM\n* ADATA XPG SX8200 Pro 1TB 3D NAND NVMe SSD\n* Windows 11 Version 21H2, build 22000.282\n* Clang C++ compiler, version 17.0\n\n**Machine 4 - (Linux + CoffeeLake)**\n\n* Intel(R) Core(TM) i5-8259U CPU @ 2.30GHz (3.80GHz Turbo), 6MB L3-cache\n* 16 GB RAM, DDR4 @ 2400 MT/s\n* 256GB NVME INTEL SSDPEKKW256G8\n* Ubuntu 20.04, kernel version 5.13\n* Clang C++ compiler, version 17.0\n\nKeep in mind that sometimes you may see different speedups on different platforms.\n\n## Local experiments:\n\nHere are a few tips that will help you compare the results of your experiments against the baseline. You can run the baseline version and write down the results, which you will later use to compare with your experiments. But there is a better way to automate this process. You can choose between two options:\n\n1) Use the `compare.py` script, which is a part of the Google benchmark library:\n\n    ```\n    # 1. Benchmark the baseline and save the score into a JSON file\n    ./lab --benchmark_min_time=1 --benchmark_out_format=json --benchmark_out=baseline.json\n    # 2. Change the code\n    # 3. Benchmark your solution and save the score into a JSON file\n    ./lab --benchmark_min_time=1 --benchmark_out_format=json --benchmark_out=solution.json\n    # 4. Compare solution.json against baseline.json\n    /path/to/benchmark/tools/compare.py benchmarks baseline.json solution.json\n    ```\n\n2) Use the `check_speedup.py` script, which is inside the Performance Ninja repo (uses the `compare.py` script under the hood):\n\n    ```\n    # 1. Put your solution under `#ifdef SOLUTION`:\n      #ifdef SOLUTION\n        // your solution\n      #else\n        // baseline version\n      #endif\n    # 2. Run the script, which will build and run your solution against the baseline N times\n    cd build\n    python3 ~/workspace/perf-ninja/tools/check_speedup.py -lab_path ../ -num_runs 3\n    ```\n\n## Submission guidelines:\n\n**IMPORTANT:** Send a request to be added as a collaborator to this Github repo. Otherwise, you won't be able to push your private branch[es]. Send your github handle to dendibakh@gmail.com with the topic \"[PerfNinjaAccessRequest]\". Do not fork the repo and submit a pull request with your solution, the CI job won't be triggered.\n\nPush your submissions into your own branch[es]. CI job will be triggered every time you push changes to your remote Github branch. For now, we use a self-hosted runner, which is configured specifically for benchmarking purposes.\n\nBy default, CI will detect which lab was modified in the last commit and will only benchmark affected assignment. If you make changes to more than one lab, the CI job will benchmark all the labs. You can also force benchmarking all the labs if you add `[CheckAll]` in the commit message.\n\nIn case all the labs were benchmarked, a summary will be provided at the end, e.g.:\n\n```\nLab Assignments Summary:\n  memory_bound:\n    data_packing: Passed\n    sequential_accesses: Failed: not fast enough\n  core_bound:\n    function_inlining: Failed: build error\n  misc:\n    warmup: Skipped\n```\n"
        },
        {
          "name": "QuickstartLinux.md",
          "type": "blob",
          "size": 1.4248046875,
          "content": "## Set up environment on Linux\r\n\r\n1. Run terminal.\r\n\r\n2. Install clang-17 compiler using instructions from [here](https://apt.llvm.org/):\r\n\r\n    ```\r\n    wget https://apt.llvm.org/llvm.sh\r\n    chmod +x llvm.sh\r\n    sudo ./llvm.sh 17 all\r\n    ```\r\n\r\n3. Build release version of google [benchmark library](https://github.com/google/benchmark#installation). It doesn't matter which compiler you use to build it. Install google benchmark library with:\r\n    ```\r\n    cmake --build \"build\" --config Release --target install\r\n    ```\r\n\r\n4. Enable clang-17 compiler for building labs. If you want to make clang-17 to be the default on a system do the following:\r\n    ```\r\n    sudo update-alternatives --install /usr/bin/cc cc /usr/bin/clang-17 30\r\n    sudo update-alternatives --install /usr/bin/c++ c++ /usr/bin/clang++-17 30\r\n    ```\r\n\r\n    If you don't want to make it a default, you can pass `-DCMAKE_C_COMPILER=clang-17 -DCMAKE_CXX_COMPILER=clang++-17` to the CMake.\r\n\r\n5. Go to any lab and check if local lab builds are working. You can find the CMake commands [here](GetStarted.md#how-to-build-lab-assignments). \r\n\r\n6. Set the frequency scaling governor to `performance`.\r\n    ```\r\n    sudo cpupower frequency-set --governor performance\r\n    ```\r\n\r\n7. (Optional) Install [ninja](https://github.com/ninja-build).\r\n    \r\n    ```\r\n    $ sudo apt install ninja-build\r\n    ```\r\n    \r\n    You can use it to build labs by passing `-G Ninja` to the CMake invocation.\r\n"
        },
        {
          "name": "QuickstartMacOS.md",
          "type": "blob",
          "size": 1.69921875,
          "content": "## Set up environment on Mac OS\r\n\r\n**Mac support is still experimental. Submit bugs if you experience issues.**\r\n\r\n1. Run terminal.\r\n\r\n2. Install homebrew (if haven't already) following instructions from [here](https://brew.sh):\r\n\r\n    ```\r\n    /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\r\n    echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' >> ~/.zprofile\\n\r\n    ```\r\n\r\n2. Install clang-17 compiler (make sure that the version is not older than 15, otherwise it will not compile):\r\n\r\n    ```\r\n    brew install llvm@17\r\n    export PATH=\"/opt/homebrew/opt/llvm/bin:$PATH\"\r\n    export CC=clang\r\n    export CXX=clang++\r\n    ```\r\n\r\n    Consider making environment changes permanent:\r\n\r\n    ```\r\n    echo 'export PATH=\"/opt/homebrew/opt/llvm/bin:$PATH\"' >> ~/.zshrc\r\n    echo 'export CC=clang' >> ~/.zshrc\r\n    echo 'export CXX=clang++' >> ~/.zshrc\r\n    ```\r\n\r\n5. Build release version of google [benchmark library](https://github.com/google/benchmark#installation). It doesn't matter which compiler you use to build it. Install google benchmark library with:\r\n    \r\n    ```\r\n    git clone https://github.com/google/benchmark.git\r\n    cd benchmark\r\n    git clone https://github.com/google/googletest.git\r\n    mkdir build\r\n    cd build\r\n    cmake -DCMAKE_BUILD_TYPE=Release ..\r\n    cmake --build . --config Release --target install\r\n    ```\r\n\r\n6. Go to any lab and check if local lab builds are working. You can find the CMake commands [here](GetStarted.md#how-to-build-lab-assignments). \r\n\r\n7. (Optional) Install [ninja](https://github.com/ninja-build).\r\n    \r\n    ```\r\n    $ brew install ninja\r\n    ```\r\n    \r\n    You can use it to build labs by passing `-G Ninja` to the CMake invocation.\r\n"
        },
        {
          "name": "QuickstartWindows.md",
          "type": "blob",
          "size": 1.3095703125,
          "content": "## Set up environment on Windows\r\n\r\n1. Run powershell.\r\n\r\n2. Install [ninja](https://github.com/ninja-build/ninja/releases). \r\n    \r\n    Add it to the PATH. For example:\r\n    ```\r\n    $ENV:PATH=\"$ENV:PATH;C:\\Program Files\\ninja\"\r\n    ```\r\n3. Download clang-17 compiler from [here](https://github.com/llvm/llvm-project/releases/tag/llvmorg-17.0.1) (LLVM-17.0.1-win64.exe) and install it. Select \"add LLVM to the PATH\" while installing.\r\n\r\n4. Build release version of google [benchmark library](https://github.com/google/benchmark#installation). It doesn't matter which compiler you use to build it. Install google benchmark library with:\r\n    ```\r\n    cmake --build \"build\" --config Release --target install\r\n    ```\r\n    Add google benchmark library to PATH\r\n    ```\r\n    $ENV:PATH=\"$ENV:PATH;C:\\Program Files (x86)\\benchmark\\lib\"\r\n    ```\r\n5. Go to any lab and check if local lab builds are working. You can find the CMake commands [here](GetStarted.md#how-to-build-lab-assignments), but note that you need to add `-G Ninja` to the CMake invocation.\r\n\r\n6. If everything works as expected, you can set environment variables permanently (run as Administrator):\r\n    ```\r\n    # be carefull, back up your PATH\r\n    setx /M PATH \"$($env:path);C:\\Program Files\\ninja\"\r\n    setx /M PATH \"$($env:path);C:\\Program Files (x86)\\benchmark\\lib\"\r\n    ```"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.173828125,
          "content": "<p align=\"center\"> <img src=\"/logo.jpg\" width=200> </p>\n\n![Linux](https://github.com/dendibakh/perf-ninja/actions/workflows/CI_Linux_Alderlake.yml/badge.svg) ![Windows](https://github.com/dendibakh/perf-ninja/actions/workflows/CI_Macos_M1.yml/badge.svg) ![Windows](https://github.com/dendibakh/perf-ninja/actions/workflows/CI_Win_Zen3.yml/badge.svg) ![Linux](https://github.com/dendibakh/perf-ninja/actions/workflows/CI_Linux_Coffeelake.yml/badge.svg) \n\n# Performance Ninja Class\n\n[![YouTube](https://img.shields.io/youtube/channel/subscribers/UCGmEJdQ993cdCGdnLZDuOOQ)](https://youtube.com/@easyperf3992)\n[![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/dendibakh)](https://twitter.com/dendibakh)\n![GitHub Repo stars](https://img.shields.io/github/stars/dendibakh/perf-ninja)\n\nThis is an online course where you can learn to find and fix low-level performance issues, for example CPU cache misses and branch mispredictions. It's all about practice. So we offer you this course in a form of lab assignments and youtube videos. You will spend at least 90% of the time analyzing performance of the code and trying to improve it.\n\n[<img src=\"img/WelcomeVideo.png\">](https://www.youtube.com/watch?v=2tzdkC6IDbo&list=PLRWO2AL1QAV6bJAU2kgB4xfodGID43Y5d)\n\nEach lab assignment focuses on a specific performance problem and can take anywhere from 30 mins up to 4 hours depending on your background and the complexity of the lab assignment itself. Once you're done improving the code, you can submit your solution to Github for automated benchmarking and verification.\n\nPerformance Ninja is supported on Linux, Windows, and Mac, and is run on all the recent HW including Intel's 12th-gen Alderlake, AMD's Zen3, and Apple's M1 CPUs. You can observe the effect of your optimizations on a variety of modern platforms.\n\nPrerequisites: basic C++ skills are an absolute must-have for the course. Denis' [book](https://book.easyperf.net/perf_book) \"Performance Analysis and Tuning on Modern CPUs\" is recommended as an introduction to performance analysis basics. Knowledge of compilers, computer architecture, and the ability to read assembly code is a plus.\n\nBefore you start working on lab assignments, make sure you read [Get Started page](GetStarted.md) and watch the [warmup video](https://youtu.be/jFRwAcIoLgQ).\n\nLab assignments in this project are implemented in C++. Also, Performance Ninja was ported to:\n* Rust ([perf-ninja-rs](https://github.com/grahamking/perf-ninja-rs)), thanks to @grahamking.\n* Zig ([perf-ninja-zig](https://github.com/JonathanHallstrom/perf-ninja-zig)), thanks to @JonathanHallstrom.\n\n## Lab assignments\n\n* Core Bound:\n  * [Vectorization 1](labs/core_bound/vectorization_1)\n  * [Vectorization 2](labs/core_bound/vectorization_2)\n  * [Function Inlining](labs/core_bound/function_inlining_1)\n  * [Dependency Chains 1](labs/core_bound/dep_chains_1)\n  * [Dependency Chains 2](labs/core_bound/dep_chains_2)\n  * [Compiler Intrinsics 1](labs/core_bound/compiler_intrinsics_1)\n  * [Compiler Intrinsics 2](labs/core_bound/compiler_intrinsics_2)\n* Memory Bound:\n  * [Data Packing](labs/memory_bound/data_packing) (currently broken)\n  * [Loop Interchange 1](labs/memory_bound/loop_interchange_1)\n  * [Loop Interchange 2](labs/memory_bound/loop_interchange_2)\n  * [Loop Tiling](labs/memory_bound/loop_tiling_1)\n  * [SW memory prefetching](labs/memory_bound/swmem_prefetch_1)\n  * [False Sharing](labs/memory_bound/false_sharing_1)\n  * [Huge Pages](labs/memory_bound/huge_pages_1)\n  * [Memory Order Violation](labs/memory_bound/mem_order_violation_1)\n  * [Memory Alignment](labs/memory_bound/mem_alignment_1)\n* Bad Speculation:\n  * [Branches To CMOVs](labs/bad_speculation/branches_to_cmov_1)\n  * [Conditional Store](labs/bad_speculation/conditional_store_1)\n  * [Replacing Branches With Lookup Tables](labs/bad_speculation/lookup_tables_1)\n  * [C++ Virtual Calls](labs/bad_speculation/virtual_call_mispredict)\n* CPU Frontend Bound:\n* Data-Driven optimizations:\n* Misc:\n  * [Warmup](labs/misc/warmup)\n  * [LTO](labs/misc/lto)\n  * [PGO](labs/misc/pgo)\n  * [Optimize IO](labs/misc/io_opt1)\n\n## Support the project\n\nPerformance Ninja is in a very much work-in-progress state. We will be adding new lab assignments and videos! The course is free by default, but we ask you to support us on [Github Sponsors](https://github.com/sponsors/dendibakh), [Patreon](https://www.patreon.com/dendibakh) or [PayPal](https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=TBM3NW8TKTT34&currency_code=USD&source=url). Your sponsorship will speed up adding new lab assignments.\n\nCurrent sponsors:\n* Pavel Davydov (@pdavydov108)\n* Matias Christensen\n* Maya Lekova (@MayaLekova)\n* Aaron St. George (@AaronStGeorge)\n\nThanks to Mansur Mavliutov (@Mansur) for providing an AMD-based machine for running CI jobs.\n\nLab authors:\n* Andrew Evstyukhin (@andrewevstyukhin)\n* Ivica Bogosavljevic (@ibogosavljevic)\n* René Rahn (@rrahn)\n* Adam Folwarczny (@adamf88)\n* Jakub Beránek (@Kobzol)\n* Jakub Gałecki (@kubagalecki)\n* Denis Bakhvalov (@dendibakh)\n\n## Contributing\n\nWe warmly welcome contributions! See [Contributing.md](Contributing.md) for the details.\n\nPlease write to dendibakh@gmail.com with suggestions.\n\nCopyright © 2022 by Denis Bakhvalov under Creative Commons license (CC BY 4.0).\n"
        },
        {
          "name": "buildbot",
          "type": "tree",
          "content": null
        },
        {
          "name": "img",
          "type": "tree",
          "content": null
        },
        {
          "name": "labs",
          "type": "tree",
          "content": null
        },
        {
          "name": "logo.jpg",
          "type": "blob",
          "size": 762.8857421875,
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}