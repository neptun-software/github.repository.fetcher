{
  "metadata": {
    "timestamp": 1736565692798,
    "page": 593,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "pytorch/xla",
      "stars": 2508,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".bazelrc",
          "type": "blob",
          "size": 8.8212890625,
          "content": "############################################################################\n# All default build options below.\n\n# Make Bazel print out all options from rc files.\nbuild --announce_rc\n\n# TODO(goranpetrovic): figure out visibility of tensorflow libraries.\nbuild --nocheck_visibility\n\nbuild --enable_platform_specific_config\n\nbuild --experimental_cc_shared_library\n\n# Disable enabled-by-default TensorFlow features that we don't care about.\nbuild --define=no_aws_support=true\nbuild --define=no_hdfs_support=true\nbuild --define=no_hdfs_support=true\nbuild --define=no_kafka_support=true\nbuild --define=no_ignite_support=true\n\nbuild --define=grpc_no_ares=true\n\nbuild -c opt\n\nbuild --config=short_logs\n\n# Force GCC because clang/bazel has issues.\nbuild --action_env=CC=gcc\nbuild --action_env=CXX=g++\nbuild --spawn_strategy=standalone\n\n###########################################################################\n\nbuild:posix --copt=-Wno-sign-compare\nbuild:posix --cxxopt=-std=c++17\nbuild:posix --host_cxxopt=-std=c++17\n\nbuild:avx_posix --copt=-mavx\nbuild:avx_posix --host_copt=-mavx\n\nbuild:avx_linux --copt=-mavx\nbuild:avx_linux --host_copt=-mavx\n\nbuild:native_arch_posix --copt=-march=native\nbuild:native_arch_posix --host_copt=-march=native\n\nbuild:mkl_open_source_only --define=tensorflow_mkldnn_contraction_kernel=1\n\nbuild:cuda --repo_env TF_NEED_CUDA=1\n# \"sm\" means we emit only cubin, which is forward compatible within a GPU generation.\n# \"compute\" means we emit both cubin and PTX, which is larger but also forward compatible to future GPU generations.\nbuild:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain\nbuild:cuda --@local_config_cuda//:enable_cuda\nbuild:cuda --define=xla_python_enable_gpu=true\nbuild:cuda --cxxopt=-DXLA_CUDA=1\n\n# Coverage with cuda/gcc/nvcc requires manually setting coverage flags.\ncoverage:cuda --per_file_copt=third_party/.*,torch_xla/.*@--coverage\ncoverage:cuda --linkopt=-lgcov\n\nbuild:acl --define==build_with_acl=true\n\nbuild:nonccl --define=no_nccl_support=true\n\nbuild:linux --config=posix\n\n# Suppress all warning messages.\nbuild:short_logs --output_filter=DONT_MATCH_ANYTHING\n\n#build:tpu --@xla//xla/python:enable_tpu=true\nbuild:tpu --define=with_tpu_support=true\n\n# Run tests serially with TPU and GPU (only 1 device is available).\ntest:tpu --local_test_jobs=1\ntest:cuda --local_test_jobs=1\n\n#########################################################################\n# RBE config options below.\n# Flag to enable remote config\ncommon --experimental_repo_remote_exec\n\n# Inherit environmental variables that are used in testing.\ntest --test_env=TPU_NUM_DEVICES --test_env=GPU_NUM_DEVICES --test_env=CPU_NUM_DEVICES --test_env=XRT_LOCAL_WORKER\ntest --test_env=XRT_TPU_CONFIG --test_env=XRT_DEVICE_MAP --test_env=XRT_WORKERS --test_env=XRT_MESH_SERVICE_ADDRESS\ntest --test_env=XRT_SHARD_WORLD_SIZE --test_env=XRT_MULTI_PROCESSING_DEVICE --test_env=XRT_HOST_ORDINAL --test_env=XRT_SHARD_ORDINAL\ntest --test_env=XRT_START_LOCAL_SERVER --test_env=TPUVM_MODE --test_env=PJRT_DEVICE --test_env=PJRT_TPU_MAX_INFLIGHT_COMPUTATIONS\ntest --test_env=PJRT_CPU_ASYNC_CLIENT --test_env=PJRT_GPU_ASYNC_CLIENT --test_env=TPU_LIBRARY_PATH --test_env=PJRT_DIST_SERVICE_ADDR\ntest --test_env=PJRT_LOCAL_PROCESS_RANK\n\n# This environmental variable is important for properly integrating with XLA.\ntest --test_env=XLA_EXPERIMENTAL\n\n# To find `libpython` that is required to run tests (they run using installed wheels).\ntest --test_env=LD_LIBRARY_PATH\n\n# This fixes an issue where targets are configured differently because of `test_filter`.\n# https://github.com/bazelbuild/bazel/issues/6842\ntest --notrim_test_configuration\n\n# Stabilize the environmental variables used to minimize cache misses (src and env affects cache keys).\nbuild --incompatible_strict_action_env\n\n# By default in local builds, do not upload local results to cache.\nbuild --noremote_upload_local_results\n\n# Remote caching with local builds.\nbuild:remote_cache --remote_cache=grpcs://remotebuildexecution.googleapis.com\nbuild:remote_cache --remote_instance_name=projects/tpu-pytorch/instances/default_instance\nbuild:remote_cache --google_default_credentials\nbuild:remote_cache --remote_upload_local_results\nbuild:remote_cache --bes_backend=buildeventservice.googleapis.com\nbuild:remote_cache --bes_upload_mode=fully_async\nbuild:remote_cache --bes_results_url=\"https://source.cloud.google.com/results/invocations\"\nbuild:remote_cache --bes_instance_name=\"tpu-pytorch\"\nbuild:remote_cache --bes_timeout=600s  # On longer builds, BES can cause a non-zero exit from bazel.\n\n# Attempt to minimize the amount of data transfer between bazel and the remote\n# workers:\nbuild:remote_cache --remote_download_toplevel\n#########################################################################\n\n# Load rc file with user-specific options.\ntry-import %workspace%/.bazelrc.user\n\n# Compile database generation config.\nbuild:compdb --features=-layering_check\n\n# Compiling tests requires Java.\nbuild --java_runtime_version=remotejdk_11\n\n# Coverage setup.\ncoverage --build_tests_only\ncoverage --config=coverage\ncoverage --instrumentation_filter=\"//torch_xla[/:],//third_party[/:],-//test[/:]\"\ncoverage --combined_report=lcov\ncoverage --nocache_test_results\nbuild:coverage --strategy=CoverageReport=sandboxed,local\n\nbuild:coverage --test_tag_filters=-nocoverage\n\n############################################################################\n############## TensorFlow .bazelrc greatest hits ###########################\n############################################################################\n\n# Modular TF build options\nbuild:dynamic_kernels --define=dynamic_loaded_kernels=true\nbuild:dynamic_kernels --copt=-DAUTOLOAD_DYNAMIC_KERNELS\nbuild --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\n\n# Default paths for TF_SYSTEM_LIBS\nbuild:linux --define=PREFIX=/usr\nbuild:linux --define=LIBDIR=$(PREFIX)/lib\nbuild:linux --define=INCLUDEDIR=$(PREFIX)/include\nbuild:linux --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include\n\nbuild:linux --define=build_with_onednn_v2=true\n\n# On linux, we dynamically link small amount of kernels\nbuild:linux --config=dynamic_kernels\n\n# For projects which use TensorFlow as part of a Bazel build process, putting\n# nothing in a bazelrc will default to a monolithic build. Here we force\n# the monolitih build because otherwise there are missing dependencies and\n# linking fails.\nbuild --define framework_shared_object=false\nbuild --define tsl_protobuf_header_only=false\n\nbuild --define=use_fast_cpp_protos=true\nbuild --define=allow_oversize_protos=true\n\n# Enable XLA support by default.\nbuild --define=with_xla_support=true\n\n# Disable some xnnpack compilation flags which are not supported before gcc-13.\nbuild:linux --define=xnn_enable_avxvnni=false\nbuild:linux --define=xnn_enable_avx256vnni=false\nbuild:linux --define=xnn_enable_avxvnniint8=false\nbuild:linux --define=xnn_enable_avx512amx=false\nbuild:linux --define=xnn_enable_avx512fp16=false\n\n# See https://github.com/bazelbuild/bazel/issues/7362 for information on what\n# --incompatible_remove_legacy_whole_archive flag does.\n# This flag is set to true in Bazel 1.0 and newer versions. We tried to migrate\n# Tensorflow to the default, however test coverage wasn't enough to catch the\n# errors.\n# There is ongoing work on Bazel team's side to provide support for transitive\n# shared libraries. As part of migrating to transitive shared libraries, we\n# hope to provide a better mechanism for control over symbol exporting, and\n# then tackle this issue again.\n#\n# TODO: Remove this line once TF doesn't depend on Bazel wrapping all library\n# archives in -whole_archive -no_whole_archive.\nbuild --noincompatible_remove_legacy_whole_archive\n\n# cc_shared_library ensures no library is linked statically more than once.\nbuild --experimental_link_static_libraries_once=false\n\n# On linux, don't cross compile by default\nbuild:linux --distinct_host_configuration=false\n\n# Do not risk cache corruption. See:\n# https://github.com/bazelbuild/bazel/issues/3360\nbuild:linux --experimental_guard_against_concurrent_changes\n\n# Prevent regressions on those two incompatible changes\n# TODO: remove those flags when they are flipped in the default Bazel version TF uses.\nbuild --incompatible_enforce_config_setting_visibility\n\n# Suppress most C++ complier warnings to reduce log size but allow\n# for specific warnings to still be present.\nbuild:linux --copt=\"-Wno-all\"\nbuild:linux --copt=\"-Wno-extra\"\nbuild:linux --copt=\"-Wno-deprecated\"\nbuild:linux --copt=\"-Wno-deprecated-declarations\"\nbuild:linux --copt=\"-Wno-ignored-attributes\"\nbuild:linux --copt=\"-Wno-array-bounds\"\n# Add unused-result as an error on Linux.\nbuild:linux --copt=\"-Wunused-result\"\nbuild:linux --copt=\"-Werror=unused-result\"\n# Add switch as an error on Linux.\nbuild:linux --copt=\"-Wswitch\"\nbuild:linux --copt=\"-Werror=switch\"\n# Required for building with clang\nbuild:linux --copt=\"-Wno-error=unused-but-set-variable\"\n\n# Only include debug info for files not under XLA.\nbuild:dbg -c dbg\nbuild:dbg --per_file_copt=external/xla/.*@-g0,-DNDEBUG\n"
        },
        {
          "name": ".bazelversion",
          "type": "blob",
          "size": 0.005859375,
          "content": "6.5.0\n"
        },
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 4.1552734375,
          "content": "---\nLanguage:        Cpp\n# BasedOnStyle:  Google\nAccessModifierOffset: -1\nAlignAfterOpenBracket: Align\nAlignConsecutiveAssignments: false\nAlignConsecutiveDeclarations: false\nAlignEscapedNewlines: Left\nAlignOperands:   true\nAlignTrailingComments: true\n#AllowAllArgumentsOnNextLine: true\n#AllowAllConstructorInitializersOnNextLine: true\nAllowAllParametersOfDeclarationOnNextLine: true\nAllowShortBlocksOnASingleLine: false\nAllowShortCaseLabelsOnASingleLine: false\nAllowShortFunctionsOnASingleLine: All\n#AllowShortLambdasOnASingleLine: All\nAllowShortIfStatementsOnASingleLine: true\n#AllowShortIfStatementsOnASingleLine: WithoutElse\nAllowShortLoopsOnASingleLine: true\nAlwaysBreakAfterDefinitionReturnType: None\nAlwaysBreakAfterReturnType: None\nAlwaysBreakBeforeMultilineStrings: true\nAlwaysBreakTemplateDeclarations: Yes\nBinPackArguments: true\nBinPackParameters: true\nBraceWrapping:\n  #AfterCaseLabel:  false\n  AfterClass:      false\n  AfterControlStatement: false\n  AfterEnum:       false\n  AfterFunction:   false\n  AfterNamespace:  false\n  AfterObjCDeclaration: false\n  AfterStruct:     false\n  AfterUnion:      false\n  AfterExternBlock: false\n  BeforeCatch:     false\n  BeforeElse:      false\n  IndentBraces:    false\n  SplitEmptyFunction: true\n  SplitEmptyRecord: true\n  SplitEmptyNamespace: true\nBreakBeforeBinaryOperators: None\nBreakBeforeBraces: Attach\nBreakBeforeInheritanceComma: false\nBreakInheritanceList: BeforeColon\nBreakBeforeTernaryOperators: true\nBreakConstructorInitializersBeforeComma: false\nBreakConstructorInitializers: BeforeColon\nBreakAfterJavaFieldAnnotations: false\nBreakStringLiterals: true\nColumnLimit:     80\nCommentPragmas:  '^ IWYU pragma:'\nCompactNamespaces: false\nConstructorInitializerAllOnOneLineOrOnePerLine: true\nConstructorInitializerIndentWidth: 4\nContinuationIndentWidth: 4\nCpp11BracedListStyle: true\nDerivePointerAlignment: true\nDisableFormat:   false\nExperimentalAutoDetectBinPacking: false\nFixNamespaceComments: true\nForEachMacros:\n  - foreach\n  - Q_FOREACH\n  - BOOST_FOREACH\nIncludeBlocks:   Regroup\nIncludeCategories:\n  - Regex:           '^<ext/.*\\.h>'\n    Priority:        2\n  - Regex:           '^<.*\\.h>'\n    Priority:        1\n  - Regex:           '^<.*'\n    Priority:        2\n  - Regex:           '.*'\n    Priority:        3\nIncludeIsMainRegex: '([-_](test|unittest))?$'\nIndentCaseLabels: true\nIndentPPDirectives: None\nIndentWidth:     2\nIndentWrappedFunctionNames: false\nJavaScriptQuotes: Leave\nJavaScriptWrapImports: true\nKeepEmptyLinesAtTheStartOfBlocks: false\nMacroBlockBegin: ''\nMacroBlockEnd:   ''\nMaxEmptyLinesToKeep: 1\nNamespaceIndentation: None\nObjCBinPackProtocolList: Never\nObjCBlockIndentWidth: 2\nObjCSpaceAfterProperty: false\nObjCSpaceBeforeProtocolList: true\nPenaltyBreakAssignment: 2\nPenaltyBreakBeforeFirstCallParameter: 1\nPenaltyBreakComment: 300\nPenaltyBreakFirstLessLess: 120\nPenaltyBreakString: 1000\nPenaltyBreakTemplateDeclaration: 10\nPenaltyExcessCharacter: 1000000\nPenaltyReturnTypeOnItsOwnLine: 200\nPointerAlignment: Left\nRawStringFormats:\n  - Language:        Cpp\n    Delimiters:\n      - cc\n      - CC\n      - cpp\n      - Cpp\n      - CPP\n      - 'c++'\n      - 'C++'\n    CanonicalDelimiter: ''\n    BasedOnStyle:    google\n  - Language:        TextProto\n    Delimiters:\n      - pb\n      - PB\n      - proto\n      - PROTO\n    EnclosingFunctions:\n      - EqualsProto\n      - EquivToProto\n      - PARSE_PARTIAL_TEXT_PROTO\n      - PARSE_TEST_PROTO\n      - PARSE_TEXT_PROTO\n      - ParseTextOrDie\n      - ParseTextProtoOrDie\n    CanonicalDelimiter: ''\n    BasedOnStyle:    google\nReflowComments:  true\nSortIncludes:    true\nSortUsingDeclarations: true\nSpaceAfterCStyleCast: false\n#SpaceAfterLogicalNot: false\nSpaceAfterTemplateKeyword: true\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeCpp11BracedList: false\nSpaceBeforeCtorInitializerColon: true\nSpaceBeforeInheritanceColon: true\nSpaceBeforeParens: ControlStatements\nSpaceBeforeRangeBasedForLoopColon: true\nSpaceInEmptyParentheses: false\nSpacesBeforeTrailingComments: 2\nSpacesInAngles:  false\nSpacesInContainerLiterals: true\nSpacesInCStyleCastParentheses: false\nSpacesInParentheses: false\nSpacesInSquareBrackets: false\nStandard:        Auto\n#StatementMacros:\n#  - Q_UNUSED\n#  - QT_REQUIRE_VERSION\nTabWidth:        8\nUseTab:          Never\n"
        },
        {
          "name": ".devcontainer",
          "type": "tree",
          "content": null
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.470703125,
          "content": "build/\ndist/\n*.egg-info/\ntorch_xla/lib/\ntorch_xla/pb/cpp/*\ntorch_xla/version.py\ntorch_xla/csrc/version.cpp\n*/**/__pycache__\n*.swp\n*.pyc\n*.so\n\n# BEGIN NOT-CLEAN-FILES (setup.py handles this marker. Do not change.)\n#\n# Below files are not deleted by \"setup.py clean\".\n\n# Visual Studio Code files\n.vs\n.vscode/\n\n# Files autogenerated by docs/docs_build.sh\n/core\n/docs/src/*\n\n# Local terraform state\n.terraform\n\n\n# Build system temporary files\nbazel-*\n\n# Clangd cache directory\n.cache/*\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": ".style.yapf",
          "type": "blob",
          "size": 11.6357421875,
          "content": "[style]\n# Align closing bracket with visual indentation.\nalign_closing_bracket_with_visual_indent=False\n\n# Allow dictionary keys to exist on multiple lines. For example:\n#\n#   x = {\n#       ('this is the first element of a tuple',\n#        'this is the second element of a tuple'):\n#            value,\n#   }\nallow_multiline_dictionary_keys=False\n\n# Allow lambdas to be formatted on more than one line.\nallow_multiline_lambdas=False\n\n# Allow splitting before a default / named assignment in an argument list.\nallow_split_before_default_or_named_assigns=False\n\n# Allow splits before the dictionary value.\nallow_split_before_dict_value=True\n\n#   Let spacing indicate operator precedence. For example:\n#\n#     a = 1 * 2 + 3 / 4\n#     b = 1 / 2 - 3 * 4\n#     c = (1 + 2) * (3 - 4)\n#     d = (1 - 2) / (3 + 4)\n#     e = 1 * 2 - 3\n#     f = 1 + 2 + 3 + 4\n#\n# will be formatted as follows to indicate precedence:\n#\n#     a = 1*2 + 3/4\n#     b = 1/2 - 3*4\n#     c = (1+2) * (3-4)\n#     d = (1-2) / (3+4)\n#     e = 1*2 - 3\n#     f = 1 + 2 + 3 + 4\n#\narithmetic_precedence_indication=False\n\n# Number of blank lines surrounding top-level function and class\n# definitions.\nblank_lines_around_top_level_definition=2\n\n# Insert a blank line before a class-level docstring.\nblank_line_before_class_docstring=False\n\n# Insert a blank line before a module docstring.\nblank_line_before_module_docstring=False\n\n# Insert a blank line before a 'def' or 'class' immediately nested\n# within another 'def' or 'class'. For example:\n#\n#   class Foo:\n#                      # <------ this blank line\n#     def method():\n#       ...\nblank_line_before_nested_class_or_def=True\n\n# Do not split consecutive brackets. Only relevant when\n# dedent_closing_brackets is set. For example:\n#\n#    call_func_that_takes_a_dict(\n#        {\n#            'key1': 'value1',\n#            'key2': 'value2',\n#        }\n#    )\n#\n# would reformat to:\n#\n#    call_func_that_takes_a_dict({\n#        'key1': 'value1',\n#        'key2': 'value2',\n#    })\ncoalesce_brackets=False\n\n# The column limit.\ncolumn_limit=80\n\n# The style for continuation alignment. Possible values are:\n#\n# - SPACE: Use spaces for continuation alignment. This is default behavior.\n# - FIXED: Use fixed number (CONTINUATION_INDENT_WIDTH) of columns\n#   (ie: CONTINUATION_INDENT_WIDTH/INDENT_WIDTH tabs or\n#   CONTINUATION_INDENT_WIDTH spaces) for continuation alignment.\n# - VALIGN-RIGHT: Vertically align continuation lines to multiple of\n#   INDENT_WIDTH columns. Slightly right (one tab or a few spaces) if\n#   cannot vertically align continuation lines with indent characters.\ncontinuation_align_style=SPACE\n\n# Indent width used for line continuations.\ncontinuation_indent_width=4\n\n# Put closing brackets on a separate line, dedented, if the bracketed\n# expression can't fit in a single line. Applies to all kinds of brackets,\n# including function definitions and calls. For example:\n#\n#   config = {\n#       'key1': 'value1',\n#       'key2': 'value2',\n#   }        # <--- this bracket is dedented and on a separate line\n#\n#   time_series = self.remote_client.query_entity_counters(\n#       entity='dev3246.region1',\n#       key='dns.query_latency_tcp',\n#       transform=Transformation.AVERAGE(window=timedelta(seconds=60)),\n#       start_ts=now()-timedelta(days=3),\n#       end_ts=now(),\n#   )        # <--- this bracket is dedented and on a separate line\ndedent_closing_brackets=False\n\n# Disable the heuristic which places each list element on a separate line\n# if the list is comma-terminated.\ndisable_ending_comma_heuristic=False\n\n# Place each dictionary entry onto its own line.\neach_dict_entry_on_separate_line=True\n\n# Require multiline dictionary even if it would normally fit on one line.\n# For example:\n#\n#   config = {\n#       'key1': 'value1'\n#   }\nforce_multiline_dict=False\n\n# The regex for an i18n comment. The presence of this comment stops\n# reformatting of that line, because the comments are required to be\n# next to the string they translate.\ni18n_comment=#\\..*\n\n# The i18n function call names. The presence of this function stops\n# reformatting on that line, because the string it has cannot be moved\n# away from the i18n comment.\ni18n_function_call=N_, _\n\n# Indent blank lines.\nindent_blank_lines=False\n\n# Put closing brackets on a separate line, indented, if the bracketed\n# expression can't fit in a single line. Applies to all kinds of brackets,\n# including function definitions and calls. For example:\n#\n#   config = {\n#       'key1': 'value1',\n#       'key2': 'value2',\n#       }        # <--- this bracket is indented and on a separate line\n#\n#   time_series = self.remote_client.query_entity_counters(\n#       entity='dev3246.region1',\n#       key='dns.query_latency_tcp',\n#       transform=Transformation.AVERAGE(window=timedelta(seconds=60)),\n#       start_ts=now()-timedelta(days=3),\n#       end_ts=now(),\n#       )        # <--- this bracket is indented and on a separate line\nindent_closing_brackets=False\n\n# Indent the dictionary value if it cannot fit on the same line as the\n# dictionary key. For example:\n#\n#   config = {\n#       'key1':\n#           'value1',\n#       'key2': value1 +\n#               value2,\n#   }\nindent_dictionary_value=True\n\n# The number of columns to use for indentation.\nindent_width=2\n\n# Join short lines into one line. E.g., single line 'if' statements.\njoin_multiple_lines=False\n\n# Do not include spaces around selected binary operators. For example:\n#\n#   1 + 2 * 3 - 4 / 5\n#\n# will be formatted as follows when configured with \"*,/\":\n#\n#   1 + 2*3 - 4/5\nno_spaces_around_selected_binary_operators=\n\n# Use spaces around default or named assigns.\nspaces_around_default_or_named_assign=False\n\n# Adds a space after the opening '{' and before the ending '}' dict delimiters.\n#\n#   {1: 2}\n#\n# will be formatted as:\n#\n#   { 1: 2 }\nspaces_around_dict_delimiters=False\n\n# Adds a space after the opening '[' and before the ending ']' list delimiters.\n#\n#   [1, 2]\n#\n# will be formatted as:\n#\n#   [ 1, 2 ]\nspaces_around_list_delimiters=False\n\n# Use spaces around the power operator.\nspaces_around_power_operator=False\n\n# Use spaces around the subscript / slice operator.  For example:\n#\n#   my_list[1 : 10 : 2]\nspaces_around_subscript_colon=False\n\n# Adds a space after the opening '(' and before the ending ')' tuple delimiters.\n#\n#   (1, 2, 3)\n#\n# will be formatted as:\n#\n#   ( 1, 2, 3 )\nspaces_around_tuple_delimiters=False\n\n# The number of spaces required before a trailing comment.\n# This can be a single value (representing the number of spaces\n# before each trailing comment) or list of values (representing\n# alignment column values; trailing comments within a block will\n# be aligned to the first column value that is greater than the maximum\n# line length within the block). For example:\n#\n# With spaces_before_comment=5:\n#\n#   1 + 1 # Adding values\n#\n# will be formatted as:\n#\n#   1 + 1     # Adding values <-- 5 spaces between the end of the statement and comment\n#\n# With spaces_before_comment=15, 20:\n#\n#   1 + 1 # Adding values\n#   two + two # More adding\n#\n#   longer_statement # This is a longer statement\n#   short # This is a shorter statement\n#\n#   a_very_long_statement_that_extends_beyond_the_final_column # Comment\n#   short # This is a shorter statement\n#\n# will be formatted as:\n#\n#   1 + 1          # Adding values <-- end of line comments in block aligned to col 15\n#   two + two      # More adding\n#\n#   longer_statement    # This is a longer statement <-- end of line comments in block aligned to col 20\n#   short               # This is a shorter statement\n#\n#   a_very_long_statement_that_extends_beyond_the_final_column  # Comment <-- the end of line comments are aligned based on the line length\n#   short                                                       # This is a shorter statement\n#\nspaces_before_comment=2\n\n# Insert a space between the ending comma and closing bracket of a list,\n# etc.\nspace_between_ending_comma_and_closing_bracket=False\n\n# Use spaces inside brackets, braces, and parentheses.  For example:\n#\n#   method_call( 1 )\n#   my_dict[ 3 ][ 1 ][ get_index( *args, **kwargs ) ]\n#   my_set = { 1, 2, 3 }\nspace_inside_brackets=False\n\n# Split before arguments\nsplit_all_comma_separated_values=False\n\n# Split before arguments, but do not split all subexpressions recursively\n# (unless needed).\nsplit_all_top_level_comma_separated_values=False\n\n# Split before arguments if the argument list is terminated by a\n# comma.\nsplit_arguments_when_comma_terminated=False\n\n# Set to True to prefer splitting before '+', '-', '*', '/', '//', or '@'\n# rather than after.\nsplit_before_arithmetic_operator=False\n\n# Set to True to prefer splitting before '&', '|' or '^' rather than\n# after.\nsplit_before_bitwise_operator=False\n\n# Split before the closing bracket if a list or dict literal doesn't fit on\n# a single line.\nsplit_before_closing_bracket=True\n\n# Split before a dictionary or set generator (comp_for). For example, note\n# the split before the 'for':\n#\n#   foo = {\n#       variable: 'Hello world, have a nice day!'\n#       for variable in bar if variable != 42\n#   }\nsplit_before_dict_set_generator=False\n\n# Split before the '.' if we need to split a longer expression:\n#\n#   foo = ('This is a really long string: {}, {}, {}, {}'.format(a, b, c, d))\n#\n# would reformat to something like:\n#\n#   foo = ('This is a really long string: {}, {}, {}, {}'\n#          .format(a, b, c, d))\nsplit_before_dot=False\n\n# Split after the opening paren which surrounds an expression if it doesn't\n# fit on a single line.\nsplit_before_expression_after_opening_paren=True\n\n# If an argument / parameter list is going to be split, then split before\n# the first argument.\nsplit_before_first_argument=False\n\n# Set to True to prefer splitting before 'and' or 'or' rather than\n# after.\nsplit_before_logical_operator=False\n\n# Split named assignments onto individual lines.\nsplit_before_named_assigns=True\n\n# Set to True to split list comprehensions and generators that have\n# non-trivial expressions and multiple clauses before each of these\n# clauses. For example:\n#\n#   result = [\n#       a_long_var + 100 for a_long_var in xrange(1000)\n#       if a_long_var % 10]\n#\n# would reformat to something like:\n#\n#   result = [\n#       a_long_var + 100\n#       for a_long_var in xrange(1000)\n#       if a_long_var % 10]\nsplit_complex_comprehension=True\n\n# The penalty for splitting right after the opening bracket.\nsplit_penalty_after_opening_bracket=300\n\n# The penalty for splitting the line after a unary operator.\nsplit_penalty_after_unary_operator=10000\n\n# The penalty of splitting the line around the '+', '-', '*', '/', '//',\n# ``%``, and '@' operators.\nsplit_penalty_arithmetic_operator=300\n\n# The penalty for splitting right before an if expression.\nsplit_penalty_before_if_expr=0\n\n# The penalty of splitting the line around the '&', '|', and '^'\n# operators.\nsplit_penalty_bitwise_operator=300\n\n# The penalty for splitting a list comprehension or generator\n# expression.\nsplit_penalty_comprehension=2100\n\n# The penalty for characters over the column limit.\nsplit_penalty_excess_character=7000\n\n# The penalty incurred by adding a line split to the unwrapped line. The\n# more line splits added the higher the penalty.\nsplit_penalty_for_added_line_split=30\n\n# The penalty of splitting a list of \"import as\" names. For example:\n#\n#   from a_very_long_or_indented_module_name_yada_yad import (long_argument_1,\n#                                                             long_argument_2,\n#                                                             long_argument_3)\n#\n# would reformat to something like:\n#\n#   from a_very_long_or_indented_module_name_yada_yad import (\n#       long_argument_1, long_argument_2, long_argument_3)\nsplit_penalty_import_names=0\n\n# The penalty of splitting the line around the 'and' and 'or'\n# operators.\nsplit_penalty_logical_operator=300\n\n# Use the Tab character for indentation.\nuse_tabs=False\n"
        },
        {
          "name": "API_GUIDE.md",
          "type": "blob",
          "size": 13.3955078125,
          "content": "# PyTorch on XLA Devices\n\nPyTorch runs on XLA devices, like TPUs, with the\n[torch_xla package](https://github.com/pytorch/xla/). This document describes\nhow to run your models on these devices.\n\n## Creating an XLA Tensor\n\nPyTorch/XLA adds a new `xla` device type to PyTorch. This device type works just\nlike other PyTorch device types. For example, here's how to create and\nprint an XLA tensor:\n\n```python\nimport torch\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n\nt = torch.randn(2, 2, device=xm.xla_device())\nprint(t.device)\nprint(t)\n```\n\nThis code should look familiar. PyTorch/XLA uses the same interface as regular\nPyTorch with a few additions. Importing `torch_xla` initializes PyTorch/XLA, and\n`xm.xla_device()` returns the current XLA device. This may be a CPU or TPU\ndepending on your environment.\n\n## XLA Tensors are PyTorch Tensors\n\nPyTorch operations can be performed on XLA tensors just like CPU or CUDA tensors.\n\nFor example, XLA tensors can be added together:\n\n```python\nt0 = torch.randn(2, 2, device=xm.xla_device())\nt1 = torch.randn(2, 2, device=xm.xla_device())\nprint(t0 + t1)\n```\n\nOr matrix multiplied:\n\n```python\nprint(t0.mm(t1))\n```\n\nOr used with neural network modules:\n\n```python\nl_in = torch.randn(10, device=xm.xla_device())\nlinear = torch.nn.Linear(10, 20).to(xm.xla_device())\nl_out = linear(l_in)\nprint(l_out)\n```\n\nLike other device types, XLA tensors only work with other XLA tensors on the\nsame device. So code like\n\n```python\nl_in = torch.randn(10, device=xm.xla_device())\nlinear = torch.nn.Linear(10, 20)\nl_out = linear(l_in)\nprint(l_out)\n# Input tensor is not an XLA tensor: torch.FloatTensor\n```\n\nwill throw an error since the `torch.nn.Linear` module is on the CPU.\n\n## Running Models on XLA Devices\n\nBuilding a new PyTorch network or converting an existing one to run on XLA\ndevices requires only a few lines of XLA-specific code. The following snippets\nhighlight these lines when running on a single device and multiple devices with XLA\nmulti-processing.\n\n### Running on a Single XLA Device\n\nThe following snippet shows a network training on a single XLA device:\n\n```python\nimport torch_xla.core.xla_model as xm\n\ndevice = xm.xla_device()\nmodel = MNIST().train().to(device)\nloss_fn = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n\nfor data, target in train_loader:\n  optimizer.zero_grad()\n  data = data.to(device)\n  target = target.to(device)\n  output = model(data)\n  loss = loss_fn(output, target)\n  loss.backward()\n\n  optimizer.step()\n  xm.mark_step()\n```\n\nThis snippet highlights how easy it is to switch your model to run on XLA. The\nmodel definition, dataloader, optimizer and training loop can work on any device.\nThe only XLA-specific code is a couple lines that acquire the XLA device and\nmark the step. Calling\n`xm.mark_step()` at the end of each training\niteration causes XLA to execute its current graph and update the model's\nparameters. See [XLA Tensor Deep Dive](#xla-tensor-deep-dive) for more on\nhow XLA creates graphs and runs operations.\n\n### Running on Multiple XLA Devices with Multi-processing\n\nPyTorch/XLA makes it easy to accelerate training by running on multiple XLA\ndevices. The following snippet shows how:\n\n```python\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\n\ndef _mp_fn(index):\n  device = xm.xla_device()\n  mp_device_loader = pl.MpDeviceLoader(train_loader, device)\n\n  model = MNIST().train().to(device)\n  loss_fn = nn.NLLLoss()\n  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n\n  for data, target in mp_device_loader:\n    optimizer.zero_grad()\n    output = model(data)\n    loss = loss_fn(output, target)\n    loss.backward()\n    xm.optimizer_step(optimizer)\n\nif __name__ == '__main__':\n  torch_xla.launch(_mp_fn, args=())\n```\n\nThere are three differences between this multi-device snippet and the previous\nsingle device snippet. Let's go over then one by one.\n\n- `torch_xla.launch()`\n  - Creates the processes that each run an XLA device.\n  - This function is a wrapper of multithreading spawn to allow user run the script with torchrun command line also. Each process will only be able to access the device assigned to the current process. For example on a TPU v4-8, there will be 4 processes being spawn up and each process will own a TPU device.\n  - Note that if you print the `xm.xla_device()` on each process you will see `xla:0` on all devices. This is because each process can only see one device. This does not mean multi-process is not functioning. The only execution is with PJRT runtime on TPU v2 and TPU v3 since there will be `#devices/2` processes and each process will have 2 threads(check this [doc](https://github.com/pytorch/xla/blob/master/docs/pjrt.md#tpus-v2v3-vs-v4) for more details).\n- `MpDeviceLoader`\n  - Loads the training data onto each device.\n  - `MpDeviceLoader` can wrap on a torch dataloader. It can preload the data to the device and overlap the dataloading with device execution to improve the performance.\n  - `MpDeviceLoader` also call `xm.mark_step` for you every `batches_per_execution`(default to 1) batch being yield.\n- `xm.optimizer_step(optimizer)`\n  - Consolidates the gradients between devices and issues the XLA device step computation.\n  - It is pretty much a `all_reduce_gradients` + `optimizer.step()` + `mark_step` and returns the loss being reduced.\n\nThe model definition, optimizer definition and training loop remain the same.\n\n> **NOTE:** It is important to note that, when using multi-processing, the user can start\nretrieving and accessing XLA devices only from within the target function of\n`torch_xla.launch()` (or any function which has `torch_xla.launch()` as parent in the call\nstack).\n\nSee the\n[full multiprocessing example](https://github.com/pytorch/xla/blob/master/test/test_train_mp_mnist.py)\nfor more on training a network on multiple XLA devices with multi-processing.\n\n### Running on TPU Pods\nMulti-host setup for different accelerators can be very different. This doc will talk about the device independent bits of multi-host training and will use the TPU + PJRT runtime(currently available on 1.13 and 2.x releases) as an example.\n\nBefore you being, please take a look at our user guide at [here](https://cloud.google.com/tpu/docs/run-calculation-pytorch) which will explain some Google Cloud basis like how to use `gcloud` command and how to setup your project. You can also check [here](https://cloud.google.com/tpu/docs/how-to) for all Cloud TPU Howto. This doc will focus on the PyTorch/XLA perspective of the Setup.\n\nLet's assume you have the above mnist example from above section in a `train_mnist_xla.py`. If it is a single host multi device training, you would ssh to the TPUVM and run command like\n\n```\nPJRT_DEVICE=TPU python3 train_mnist_xla.py\n```\n\nNow in order to run the same models on a TPU v4-16 (which has 2 host, each with 4 TPU devices), you will need to\n  - Make sure each host can access the training script and training data. This is usually done by using the `gcloud scp` command or `gcloud ssh` command to copy the training scripts to all hosts.\n  - Run the same training command on all hosts at the same time.\n\n```\ngcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=$ZONE --project=$PROJECT --worker=all --command=\"PJRT_DEVICE=TPU python3 train_mnist_xla.py\"\n```\n\nAbove `gcloud ssh` command will ssh to all hosts in TPUVM Pod and run the same command at the same time..\n\n> **NOTE:** You need to run run above `gcloud` command outside of the TPUVM vm.\n\nThe model code and training script is the same for the multi-process training and the multi-host training. PyTorch/XLA and the underlying infrastructure will make sure each device is aware of the global topology and each device's local and global ordinal. Cross-device communication will happen across all devices instead of local devices.\n\nFor more details regarding PJRT runtime and how to run it on pod, please refer to this [doc](https://github.com/pytorch/xla/blob/master/docs/pjrt.md#tpu). For more information about PyTorch/XLA and TPU pod and a complete guide to run a resnet50 with fakedata on TPU pod, please refer to this [guide](https://cloud.google.com/tpu/docs/pytorch-pods).\n\n## XLA Tensor Deep Dive\n\nUsing XLA tensors and devices requires changing only a few lines of code. But\neven though XLA tensors act a lot like CPU and CUDA tensors, their internals are\ndifferent. This section describes what makes XLA tensors unique.\n\n### XLA Tensors are Lazy\n\nCPU and CUDA tensors launch operations immediately or <b>eagerly</b>. XLA tensors,\non the other hand, are <b>lazy</b>. They record operations in a graph until the\nresults are needed. Deferring execution like this lets XLA optimize it. A graph\nof multiple separate operations might be fused into a single optimized\noperation, for example.\n\nLazy execution is generally invisible to the caller. PyTorch/XLA automatically\nconstructs the graphs, sends them to XLA devices, and synchronizes when\ncopying data between an XLA device and the CPU. Inserting a barrier when\ntaking an optimizer step explicitly synchronizes the CPU and the XLA device. For\nmore information about our lazy tensor design, you can read [this paper](https://arxiv.org/pdf/2102.13267.pdf).\n\n### Memory Layout\n\nThe internal data representation of XLA tensors is opaque to the user. They\ndo not expose their storage and they always appear to be contiguous, unlike\nCPU and CUDA tensors. This allows XLA to adjust a tensor's memory layout for\nbetter performance.\n\n### Moving XLA Tensors to and from the CPU\n\nXLA tensors can be moved from the CPU to an XLA device and from an XLA device\nto the CPU. If a view is moved then the data its viewing is also copied to the\nother device and the view relationship is not preserved. Put another way,\nonce data is copied to another device it has no relationship with its\nprevious device or any tensors on it. Again, depending on how your code operates,\nappreciating and accommodating this transition can be important.\n\n### Saving and Loading XLA Tensors\n\nXLA tensors should be moved to the CPU before saving, as in the following\nsnippet:\n\n```python\nimport torch\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n\ndevice = xm.xla_device()\n\nt0 = torch.randn(2, 2, device=device)\nt1 = torch.randn(2, 2, device=device)\n\ntensors = (t0.cpu(), t1.cpu())\n\ntorch.save(tensors, 'tensors.pt')\n\ntensors = torch.load('tensors.pt')\n\nt0 = tensors[0].to(device)\nt1 = tensors[1].to(device)\n```\n\nThis lets you put the loaded tensors on any available device, not just the one on which they were initialized.\n\nPer the above note on moving XLA tensors to the CPU, care must be taken when\nworking with views. Instead of saving views it is recommended that you recreate\nthem after the tensors have been loaded and moved to their destination device(s).\n\nA utility API is provided to save data by taking care of previously moving it\nto CPU:\n\n```python\nimport torch\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n\nxm.save(model.state_dict(), path)\n```\n\nIn case of multiple devices, the above API will only save the data for the master\ndevice ordinal (0).\n\nIn case where memory is limited compared to the size of the model parameters, an\nAPI is provided that reduces the memory footprint on the host:\n\n```python\nimport torch_xla.utils.serialization as xser\n\nxser.save(model.state_dict(), path)\n```\n\nThis API streams XLA tensors to CPU one at a time, reducing the amount of host\nmemory used, but it requires a matching load API to restore:\n\n```python\nimport torch_xla.utils.serialization as xser\n\nstate_dict = xser.load(path)\nmodel.load_state_dict(state_dict)\n```\n\nDirectly saving XLA tensors is possible but not recommended. XLA\ntensors are always loaded back to the device they were saved from, and if\nthat device is unavailable the load will fail. PyTorch/XLA, like all of PyTorch,\nis under active development and this behavior may change in the future.\n\n## Compilation Caching\n\nThe XLA compiler converts the traced HLO into an executable which runs on\nthe devices. Compilation can be time consuming, and in cases where the HLO\ndoesn't change across executions, the compilation result can be persisted to\ndisk for reuse, significantly reducing development iteration time.\n\nNote that if the HLO changes between executions, a recompilation will still\noccur.\n\nThis is currently an experimental opt-in API, which must be activated before\nany computations are executed. Initialization is done through the\n`initialize_cache` API:\n\n```python\nimport torch_xla.runtime as xr\nxr.initialize_cache('YOUR_CACHE_PATH', readonly=False)\n```\n\nThis will initialize a persistent compilation cache at the specified path. The\n`readonly` parameter can be used to control whether the worker will be able to\nwrite to the cache, which can be useful when a shared cache mount is used for\nan SPMD workload.\n\nIf you want to use  persistent compilation cache in the multi process training(with `torch_xla.launch` or `xmp.spawn`), you should use the different path for different process.\n\n```python\ndef _mp_fn(index):\n  # cache init needs to happens inside the mp_fn.\n  xr.initialize_cache(f'/tmp/xla_cache_{index}', readonly=False)\n  ....\n\nif __name__ == '__main__':\n  torch_xla.launch(_mp_fn, args=())\n```\nIf you don't have the access to the `index`, you can use `xr.global_ordinal()`. Check out the runnable example in [here](https://github.com/pytorch/xla/blob/master/examples/data_parallel/train_resnet_xla_ddp.py).\n\n\n## Further Reading\n\nAdditional documentation is available at the\n[PyTorch/XLA repo](https://github.com/pytorch/xla/). More examples of running\nnetworks on TPUs are available\n[here](https://github.com/pytorch-tpu/examples).\n"
        },
        {
          "name": "BUILD",
          "type": "blob",
          "size": 2.1220703125,
          "content": "load(\n    \"@xla//xla/tsl/platform/default:cuda_build_defs.bzl\",\n    \"if_cuda_is_configured\",\n)\n\nload(\"@python//:defs.bzl\", \"compile_pip_requirements\")\nload(\"@python_version_repo//:py_version.bzl\", \"REQUIREMENTS\")\n\ncompile_pip_requirements(\n    name = \"requirements\",\n    extra_args = [\n        \"--allow-unsafe\",\n        \"--build-isolation\",\n        \"--rebuild\",\n    ],\n    requirements_in = \"requirements.in\",\n    requirements_txt = REQUIREMENTS,\n    generate_hashes = True,\n)\n\ncc_binary(\n    name = \"_XLAC.so\",\n    copts = [\n        \"-DTORCH_API_INCLUDE_EXTENSION_H\",\n        \"-DTORCH_EXTENSION_NAME=_XLAC\",\n        \"-fopenmp\",\n        \"-fPIC\",\n        \"-fwrapv\",\n    ],\n    linkopts = [\n        \"-Wl,-rpath,$$ORIGIN/torch_xla/lib\",  # for libtpu\n        \"-Wl,-soname,_XLAC.so\",\n        \"-lstdc++fs\",  # For std::filesystem\n    ],\n    linkshared = 1,\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \"//torch_xla/csrc:init_python_bindings\",\n        \"@torch//:headers\",\n        \"@torch//:libc10\",\n        \"@torch//:libtorch\",\n        \"@torch//:libtorch_cpu\",\n        \"@torch//:libtorch_python\",\n    ] + if_cuda_is_configured([\n        \"@xla//xla/stream_executor:cuda_platform\",\n    ]),\n)\n\ncc_binary(\n    name = \"_XLAC_cuda_functions.so\",\n    copts = [\n        \"-fopenmp\",\n        \"-fPIC\",\n    ],\n    linkopts = [\n        \"-Wl,-soname,_XLAC_cuda_functions.so\",\n    ],\n    linkshared = 1,\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \"//torch_xla/csrc:aten_cuda_functions\",\n    ],\n)\n\ntest_suite(\n    name = \"cpp_tests\",\n    # testonly = True,\n    tests = [\n        \"//test/cpp:test_aten_xla_tensor_1\",\n        \"//test/cpp:test_aten_xla_tensor_2\",\n        \"//test/cpp:test_aten_xla_tensor_3\",\n        \"//test/cpp:test_aten_xla_tensor_4\",\n        \"//test/cpp:test_aten_xla_tensor_5\",\n        \"//test/cpp:test_aten_xla_tensor_6\",\n        \"//test/cpp:test_ir\",\n        \"//test/cpp:test_lazy\",\n        \"//test/cpp:test_replication\",\n        \"//test/cpp:test_tensor\",\n        \"//test/cpp:test_xla_sharding\",\n        \"//torch_xla/csrc/runtime:pjrt_computation_client_test\",\n        # \"//torch_xla/csrc/runtime:ifrt_computation_client_test\",\n    ],\n)\n"
        },
        {
          "name": "CODEGEN_MIGRATION_GUIDE.md",
          "type": "blob",
          "size": 11.3515625,
          "content": "# Codegen migration Guide\n\n## Background\nAs PyTorch/XLA migrates to the LTC (Lazy Tensor Core), we need to clean up the existing stub code (which spans over 6+ files) that were used to do the op lowering. The complete process and file structure for the old op lowering can be found in [the op lowering guide](https://github.com/pytorch/xla/blob/master/OP_LOWERING_GUIDE.md). Replacing the supported op with the codegen SHOULD NOT introduce any new behavior, it is purely for the clean up purpose.\n\n## Before you start\nYou should follow the instructions in [here](https://github.com/pytorch/xla/blob/master/CONTRIBUTING.md) to install required dependencies and build pytorch and pytorch/XLA from the source. You do not need access to TPU to implement the lowering. It is recommended to experiment on a workstation and configure it to use XLA:CPU. You can configure Pytorch/XLA to use XLA:CPU by running\n\n```\nexport PJRT_DEVICE=CPU\n```\n\nIt is also recommended that you're familiar with our [op lowering process](https://github.com/pytorch/xla/blob/master/OP_LOWERING_GUIDE.md) before you work on the codegen.\n\nPyTorch/XLA uses https://github.com/pytorch/xla/issues/3560 to track the status of codegen migration. When working on a codegen, please put your GitHub alias with the PR link on the issue to avoid duplicate work.\n\n## File structure\nAll file mentioned below lives under the `xla/torch_xla/csrc` folder, with the exception of `xla_native_functions.yaml`\n\n### PyTorch Codegen files\n- torch/csrc/lazy/core/shape_inference.h\n  - Shape inference functions defined for each op that will take for input torch::lazy::shapes and return output torch::lazy::shape. Only the ops that is not structural will require a manual shape inference function\n- torchgen/gen_lazy_tensor.py\n  - Builds on existing data models and helpers used by all ATen backends, and adds new functionality specific to lazy  tensor backends.  run_gen_lazy_tensor is defined in this file\n- torchgen/dest/lazy_ir.py\n  - Contains data class GenLazyIR that can be overridden by the back and defined the generated IR class\n\n### PyTorch/XLA Codegen files\n- xla/xla_native_functions.yaml\n  - Contains all the op XLA supported today. Most of the ops are under the supported category, the goal of this document is to move most of the ops to the full_codegen category.\n- xla/scripts/gen_lazy_tensor.py\n  - Provides necessary XLA versions of the codegen Codegen class and calls the upstream codegen API.\n- xla/torch_xla/csrc/XLANativeFunctions.cpp\n  - Result of the full_codegen column of the xla/codegen/xla_native_functions.yaml. The op function defined here will implement the op declared in the XLANativeFunctions.h. Each op will take at::tensor and return another at::tensor wrapped around a XLATensor.\n- xla/torch_xla/csrc/LazyIr.h\n  - Result of the full_codegen column of the xla/codegen/xla_native_functions.yaml.  Defines the IR that is used to construct the full_codegen ops.\n\n### PyTorch/XLA Old Op Lowering files\n- xla/torch_xla/csrc/generated/aten_xla_type.cpp\n  - Manually implements ops defined in xla/codegen/xla_native_functions.yaml. Will be replaced by XLANativeFunctions.cpp\n- xla/torch_xla/csrc/generated/tensor.h\n  - Defines XLATensor class and XLATensor method declarations. These declarations are usually a one to one mapping of the at::Tensor nodes we declared in XLANativeFunctions.h. XLATensor method will be removed for full_codegen ops\n- xla/torch_xla/csrc/generated/tensor_method.cpp\n  - Implements tensor methods defined in tensor.h. This file will be removed for full_codegen ops\n- xla/torch_xla/csrc/generated/ops/…\n  - Defines IR class for “most” ops. It is possible that multiple ops share the same IR.\n\n## Codegen step by step\n### 1. Identify the op\nWhen you work on your first few codegens, we generally recommend you to start with the simpler ops. This guide will go over one unary one one binary op as examples, but it is recommend that you avoid ops with the following characteristics:\n1. Contains custom fallback code. For example in _adaptive_avg_pool3d, there is a conditional fallback:\n```\n  if (!IsSupportedAdaptivePool(XlaHelpers::I64List(self.sizes()),\n                               output_size_list, /*pool_dim=*/3)) {\n    return at::native::call_fallback_fn<&xla_fallback, ATEN_OP(_adaptive_avg_pool3d)>::call(self, output_size);\n  }\n```\n2. Results in dynamic shape as these ops are WIP and may evolve over time. At some future point, we may bring the ops into codegen.\n3. Does not invoke a tensor_method directly. For example _copy_from:\n```\n if (!self_tensor) {\n   static bool sync_update =\n       torch_xla::runtime::sys_util::GetEnvBool(\"XLA_TENSOR_UPDATE_SYNC\", true);\n   XLA_CHECK(dst_tensor);\n   dst_tensor->UpdateFromTensor(self, /*sync=*/sync_update);\n }\n```\n4. Has a complicated tensor_method, ideally it should be a directly mapping from op to IR.\n\nAn good example of a \"simple\" op would be something like `abs`:\n```\nat::Tensor XLANativeFunctions::abs(const at::Tensor& self) {\n  TORCH_LAZY_FN_COUNTER(\"xla::\");\n  return bridge::AtenFromXlaTensor(XLATensor::abs(bridge::GetXlaTensor(self)));\n}\n```\n\n### 2. Codegen the op and inspect the generated file\nFind the op in  `xla/codegen/xla_native_functions.yaml` and move it to the full_codegen column and run `python setup.py install` under xla directory again. The build will fail (reason explained later in this guide) but you can still see the generated file. The code snippets below uses `abs` as an example.\n#### XLANativeFunctions.cpp\n```\nat::Tensor XLANativeFunctions::abs(const at::Tensor & self) {\n  TORCH_LAZY_FN_COUNTER(\"xla::\");\n  auto common_device = torch_xla::bridge::GetXlaDevice(self);\n  TORCH_INTERNAL_ASSERT(common_device);\n\n  torch_xla::XLATensorPtr lazy_self = torch_xla::bridge::GetXlaTensorOrCreateForWrappedNumber(self, *common_device);\n\n  torch::lazy::NodePtr node = torch::lazy::ReuseNode<Abs>(lazy_self->GetIrValue());\n  if (!node) {\n    node = torch_xla::MakeNode<Abs>(lazy_self->GetIrValue());\n    CacheNode(node);\n  }\n\n  auto result = torch_xla::bridge::AtenFromXlaTensor(\n        torch_xla::XLATensor::Create(std::move(node), *common_device));\n  return result;\n};\n```\nDescribing the generated code line by line:\n- Get and verify device from input tensor\n```\n  auto common_device = torch_xla::bridge::GetXlaDevice(self);\n  TORCH_INTERNAL_ASSERT(common_device);\n```\n- Check if we can reuse the node from previous creation. If not, create corresponding IR node and cache it.\n```\n  torch::lazy::NodePtr node = torch::lazy::ReuseNode<Abs>(lazy_self->GetIrValue());\n  if (!node) {\n    node = torch_xla::MakeNode<Abs>(lazy_self->GetIrValue());\n    CacheNode(node);\n  }\n```\n- Wrap the newly created IR node in a XLATensor. And wrap the XLATensor within the at::Tensor and return it as a result. Note that this part used to be manually done in tensor_method.cpp.\n```\n  auto result = torch_xla::bridge::AtenFromXlaTensor(\n        torch_xla::XLATensor::Create(std::move(node), *common_device));\n  return result;\n```\n\n#### LazyIr.h\n```\nclass Abs : public XlaNode {\n public:\n  Abs(const torch_xla::XlaValue& self)\n      : XlaNode(torch::lazy::OpKind(at::aten::abs), {self},\n                [&]() { return AbsOutputShape(self); },\n                /* num_outputs */ 1, torch::lazy::MHash())\n  {}\n\n  std::string ToString() const override {\n    std::stringstream ss;\n    ss << XlaNode::ToString();\n    return ss.str();\n  }\n  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;\n};\n```\n\nA couple of things to keep in mind:\n- Codegen does not generate the `Clone` method which is expected. There is no use of the `Clone` method even in PyTorch/XLA today, we will remove them as part of the migration.\n- For every op, it will generate a {OP}OutputShape method. We need to manually declare and implement this method in a separate file.\n- For every op, it will generate a Lower declaration. We need to manually implement this lowering function in a separate file.\n\n### 3. Implement the missing IR function\n#### torch_xla/csrc/ops/ops_xla_shape_fn.h\nDeclare the {OP}OutputShape:\n```\nxla::Shape AbsOutputShape(const XlaValue& input);\n```\n#### torch_xla/csrc/ops/ops_xla_shape_fn.cpp\nImplement the {OP}OutputShape:\n```\nxla::Shape AbsOutputShape(const XlaValue& input) { return input.xla_shape(); }\n```\n\n`Abs` is an overly simplified example, in a normal case you need to call the BuildXXXOp function again to get the output shape. A slightly better example would be:\n```\nxla::Shape MaximumOutputShape(const XlaValue& input, const XlaValue& other) {\n  auto lower_for_shape_fn =\n      [&](absl::Span<const xla::XlaOp> operands) -> xla::XlaOp {\n    auto promoted = XlaHelpers::Promote(operands[0], operands[1]);\n    return xla::Max(promoted.first, promoted.second);\n  };\n  return InferOutputShape({input.xla_shape(), other.xla_shape()},\n                          lower_for_shape_fn);\n}\n```\n\nNote that you should not start from scratch. Find the Xla::Shape computation logic from the existing op and move it this these two files.\n\n### 4. Implement the lowering function\n#### torch_xla/csrc/ops/ops_lower_fn.cpp\n```\ntorch_xla::XlaOpVector Abs::Lower(LoweringContext* loctx) const {\n  xla::XlaOp xla_input = loctx->GetOutputOp(operand(0));\n  return ReturnOp(BuildAbs(xla_input), loctx);\n}\n```\nNote that this function should be directly moved from the existing lowering. Some Ops that were originally implemented in `torch_xla/csrc/ops/ops.cpp` use `GenericOp`. You will need to slightly modify their lowering implementation to fit the implementation provided above.\n\n### 5. Cleanup\nDelete the existing op from aten_xla_type.cpp, tensor_methods.h, tensor_methods.cpp, and ops/…. Note that sometimes you have to keep the tensor_method, because it is being used in tensor_ops like. So, before removing the op, cross reference it with `tensor_ops.cpp`.\n```\n  XLATensor s1 = XLATensor::sub(XLATensor::mul(u2, v3), XLATensor::mul(u3, v2), one);\n```\nSometimes other IRNode uses the 'IRNode' you migrated. In this case you need to update those IRNode lowering logic as well. In the long term we need to get rid  of these composite IR from our end and provide a lowering function for each op.\n```\n  torch::lazy::NodePtr exp = Pow(Abs(input), norm_exp);\n```\nto\n```\n  torch::lazy::NodePtr exp =\n      Pow(torch_xla::MakeNode<Abs>(input, std::vector<torch::lazy::Shape>()),\n          norm_exp);\n```\n\n## Run the test and verify the result\nRun the C++ op test or a simple test that only involves the generated ops. To run the C++ test:\n1. Build the xla through `python setup.py install` (note: don't use the `BUILD_CPP_TESTS=0` flag since this will skip building the C++ tests)\n2. Go into the `test/cpp/build` directory in your `pytorch/xla`\n3. Run the command to run the desired C++ test (for example, to run `Abs` C++ test):\n```\n./test_ptxla --gtest_filter=AtenXlaTensorTest.TestAbs\n```\nAs usual, two things to verify are the correctness and the xla counter being incremented correctly.\n\n## Sample PRs\n- Unary/Binary OP -> Codegen erf, erfc, erfinv, and exp (https://github.com/pytorch/xla/pull/3659)\n- OP with optional -> Codegen binary_cross_entropy/backward (https://github.com/pytorch/xla/pull/3809)\n- OP with `at::Scalar` -> Codegen addcdiv and addcmul (https://github.com/pytorch/xla/pull/3768)\n- OP with vector that support negative index -> Codegen amin amax (https://github.com/pytorch/xla/pull/3771)\n- OP with special fallback logic -> partially codegen adaptive_avgpool3d and backward (https://github.com/pytorch/xla/pull/3790)\nTo see more examples, please take a look at the tracking issue (https://github.com/pytorch/xla/issues/3560).\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.2646484375,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\naddress, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <conduct@pytorch.org>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 8.1025390625,
          "content": "# Contribute To PyTorch/XLA\n\nWe appreciate all contributions. If you are planning to contribute a bug fix for \nan open issue, please comment on the thread and we're happy to provide guidance.\nYou are welcome to pick issues with [good first issue](https://github.com/pytorch/xla/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) \nand [help wanted](https://github.com/pytorch/xla/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22) \nlabels to get started.\n\nIf you plan to contribute new features or extensions to this repository, first \nopen an issue and discuss the feature with us. Sending a PR without discussion \nmight result in a rejected PR, because we might be taking the repository in a \ndifferent direction.\n\n## Building from source\n\nWe recommend you use our prebuilt Docker image to start your development work \nusing either VS Code or a local container:\n\n### Visual Studio Code Dev Container\n\n* Create an empty directory for your workspace on your development host. These \n  instructions assume you are using a remote host and are connecting to it over \n  SSH.\n  \n* Clone PyTorch, TorchVision, and PyTorch/XLA into your workspace directory:\n\n```bash\n  git clone --recursive --depth=1 https://github.com/pytorch/pytorch.git\n\n  # Install TorchVision if you need to run tests that involve vision modules\n  git clone --recursive --depth=1 https://github.com/pytorch/vision.git\n\n  # Clone with HTTPS if you use a GitHub a personal access token\n  git clone https://github.com/pytorch/xla.git pytorch/xla\n\n  # Or clone with SSH if you prefer:\n  git clone git@github.com:pytorch/xla.git pytorch/xla\n```\n\n* Create links to VS Code configuration files in your workspace directory:\n\n```bash\n  ln -s pytorch/xla/.devcontainer/ .devcontainer\n  ln -s pytorch/xla/contrib/vscode/ .vscode\n  ln -s pytorch/xla/.style.yapf .style.yapf\n  ln -s pytorch/xla/.clang-format .clang-format\n```\n\n* Start VS Code and ensure you have the [`Remote Development` Extension Pack](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack)\n  installed. It includes the [`Remote - SSH`](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh) and\n  [`Dev Containers`](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)\n  extensions.\n\n* From VS Code, connect to your remote host and open your workspace directory. \n  You will be prompted to reopen your workspace in container. Choose the \n  appropriate container. Use `tpu-contributor` if you are unsure of which to use. \n  If you are not prompted to reopen in a container, in the VS Code command \n  pallete, type `Dev Containers: Reopen in Container` to open your workspace in \n  one of our pre-built Docker containers. Select the correct container based on \n  your local accelerator. If you are unsure, use `tpu-contributor`.\n\n* Open a new terminal window in VS Code. Since you are running as root in this \n  container, mark the repository directories as safe. The commands below assume\n  your workspace directory is `torch`, update the commands to use your workspace\n  directory.\n\n```bash\n  git config --global --add safe.directory /workspaces/torch/pytorch\n  git config --global --add safe.directory /workspaces/torch/pytorch/xla\n  git config --global --add safe.directory /workspaces/torch/vision\n```\n* In the terminal window, run the following commands to build PyTorch, \n  TorchVision, and  PyTorch/XLA:\n\n```bash\n  cd pytorch\n  # pytorch/xla requires pytorch wheel to be presented under pytorch/dist\n  python setup.py bdist_wheel\n  python setup.py install\n  cd ../vision\n  python setup.py develop\n  cd ../pytorch/xla\n  python setup.py develop\n  # Optional: if you're using TPU, install libtpu\n  pip install torch_xla[tpu] \\\n    -f https://storage.googleapis.com/libtpu-wheels/index.html \\\n    -f https://storage.googleapis.com/libtpu-releases/index.html\n  ```\n\n* If you are running on a TPU VM, ensure `torch` and `torch_xla` were built and \n  installed correctly:\n\n```bash\n  python -c 'import torch_xla as xla; print(xla.device())'\n  # Output: xla:0\n```\n\n**Subsequent builds**: after building the packages from source code for the \nfirst time, you may need to build everything again, for example, after a\n`git pull`. You can run `scripts/build_developer.sh` which will rebuild PyTorch,\nTorchVision, and PyTorch/XLA.\n\n### Manually build in Docker container\n\n* Setup Development Docker Image\n\n  ```shell\n  docker pull us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/development:tpu\n  docker run --privileged --name ptxla -it -d -e \"TERM=xterm-256color\" us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/development:tpu\n  docker exec --privileged -it ptxla /bin/bash\n  ```\n  All of the code below will be assumed to be run within the docker.\n\n* Clone the _PyTorch_ repo as per [instructions](https://github.com/pytorch/pytorch#from-source).\n\n  ```Shell\n  git clone --recursive https://github.com/pytorch/pytorch\n  cd pytorch/\n  ```\n\n* Clone the _PyTorch/XLA_ repo:\n\n  ```Shell\n  git clone --recursive https://github.com/pytorch/xla.git\n  ```\n\n* Build PyTorch\n  ```Shell\n  # pytorch/xla requires pytorch wheel to be presented under pytorch/dist\n  python setup.py bdist_wheel\n  python setup.py develop\n  ```\n* Build PyTorch/XLA\n  ```Shell\n  cd xla/\n  python setup.py develop\n  ```\n\n### Additional steps for GPU\n\nPlease refer to this [guide](https://github.com/pytorch/xla/blob/master/docs/gpu.md#develop-pytorchxla-on-a-gpu-instance-build-pytorchxla-from-source-with-gpu-support).\n\n## Before Submitting A Pull Request:\n\nIn `pytorch/xla` repo we enforce coding style for both C++ and Python files. Please try to format\nyour code before submitting a pull request.\n\n### C++ Style Guide\n\n`pytorch/xla` uses `clang-format-11` with a customized style config.\nIf your PR touches the C++ source files, please run the following command before submitting a PR.\n\n```Shell\n# How to install: sudo apt install clang-format-11\n# If your PR only changes foo.cpp, run the following in xla/ folder\nclang-format-11 -i -style=file /PATH/TO/foo.cpp\n# To format all cpp files, run the following in xla/ folder\nfind -name '*.cpp' -o -name '*.h' -o -name '*.cc' | xargs clang-format-11 -i -style=file\n```\n\n### Python Style Guide\n\n`pytorch/xla` uses `yapf`(specially version 0.30.0 in case it's not backward compatible) with a customized style config.\nIf your PR touches the Python source files, please run the following command before submitting a PR.\n\n```Shell\n# How to install: pip install yapf==0.30.0\nyapf --recursive -i *.py test/ scripts/ torch_xla/ benchmarks/\n```\n\n### Running the Tests\n\nTo run the tests, follow __one__ of the options below:\n\n* Run on local CPU:\n\n  ```Shell\n  export PJRT_DEVICE=CPU\n  ```\n\n* Run on Cloud TPU:\n\n  ```Shell\n  export PJRT_DEVICE=TPU\n  ```\n\n* Run on GPU:\n\n  ```Shell\n  export PJRT_DEVICE=CUDA GPU_NUM_DEVICES=${NUM_GPU}\n  ```\n\nFor more detail on configuring the runtime, please refer to [this doc](https://github.com/pytorch/xla/blob/master/docs/pjrt.md#quickstart)\n\nIf you are planning to be building from source and hence using the latest _PyTorch/TPU_ code base,\nit is suggested for you to select the _Nightly_ builds when you create a Cloud TPU instance.\n\nThen run `test/run_tests.sh` and `test/cpp/run_tests.sh` to verify the setup is working.\n\n### Useful materials\n1. [OP Lowering Guide](https://github.com/pytorch/xla/blob/master/OP_LOWERING_GUIDE.md)\n2. [CODEGEN MIGRATION GUIDE](https://github.com/pytorch/xla/blob/master/CODEGEN_MIGRATION_GUIDE.md)\n3. [Dynamo Integration Guide](https://github.com/pytorch/xla/blob/master/docs/dynamo.md)\n\n### Sharp Edges\n\n* If local changes aren't visible, uninstall existing pytorch/xla with `pip uninstall torch_xla` and `pip uninstall torch`, then rebuild PyTorch and PyTorch/XLA with `python setup.py develop` or `python setup.py install`.\n* PJRT errors when running on TPU such as `The PJRT plugin has PJRT API version 0.34. The framework PJRT API version is 0.40`. You need to update your `libtpu.so` and ensure it's in your `LD_LIBRARY_PATH` environmental directory. You can download a new `libtpu.so` at [Google Cloud](https://storage.googleapis.com/libtpu-wheels/index.html), which are sorted by date. Download the newest one and install it at `pip install libtpu...whl`.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.533203125,
          "content": "Copyright (c) 2018 Google Inc.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\n3. Neither the names of Facebook, Deepmind Technologies, NYU, NEC Laboratories America\n   and IDIAP Research Institute nor the names of its contributors may be\n   used to endorse or promote products derived from this software without\n   specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "OP_LOWERING_GUIDE.md",
          "type": "blob",
          "size": 8.7421875,
          "content": "# OP Lowering Guide\n\n## Background\nPyTorch wraps the C++ ATen tensor library that offers a wide range of operations implemented on GPU and CPU. Pytorch/XLA is a PyTorch extension; one of its purposes is to convert PyTorch operations to XLA operations. Lowering defines a process of converting a higher-level representation to a lower-level representation. In this document, I will refer to the process of converting PyTorch operation to XLA operation as the lowering. XLA Compiler will also lower XlaOp to HLO, but that’s beyond the scope of this documentation. We will forward operations that we haven’t provided an XLA lowering yet to CPU and call ATen implementations. Operations that are forwarded to the CPU will cause a significant slowdown. We must lower all operations used in the model to achieve the best performance.\n\nHere's an example of what you might see from the PyTorch/XLA debugging tool for an operation that has not been lowered:\n```\npt-xla-profiler: Op(s) not lowered: aten::_ctc_loss, aten::_ctc_loss_backward,  Please open a GitHub issue with the above op lowering requests.\n```\n\n## Before you start\nYou should follow the instructions in [here](https://github.com/pytorch/xla/blob/master/CONTRIBUTING.md) to install required dependencies and build pytorch and pytorch/XLA from the source. You do not need access to TPU to implement the lowering. It is recommended to experiment on a workstation and configure it to use XLA:CPU. You can configure Pytorch/XLA to use XLA:CPU by running\n\n```\nexport PJRT_DEVICE=CPU\n```\n\n## Understanding the operation\nYou can find the definition of the C++ ATen operations in [native_functions.yaml](https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml). After you build Pytorch/XLA from source, you will also find our default implementation (a boxed kernel which forwards calls to either PyTorch native kernels) in `xla/torch_xla/csrc/aten_fallback.h/cpp`. Pytorch operations can usually be mapped to [PyTorch tensor api](https://pytorch.org/docs/stable/index.html) easily. If that is not the case searching the PyTorch native implementation under [PyTorch repo](https://github.com/pytorch/pytorch) is recommended. The goal is to lower the PyTorch operations into a sequence of XLA operations defined in [here](https://www.tensorflow.org/xla/operation_semantics).\n\n## File structure\nAll file mentioned below lives under the `xla/torch_xla/csrc` folder, with the exception of `codegen/xla_native_functions.yaml`\n\n1. `xla_native_functions.yaml` contains the list of all operators (from the [Core Aten list](https://pytorch.org/docs/stable/torch.compiler_ir.html)) that are explicitly lowered. Composed operators are not listed here. Each operator name here must directly match a pytorch operator listed in [native_functions.yaml](https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml). This file serves as the interface to adding new xla operators, and is an input to PyTorch's [codegen machinery](https://github.com/pytorch/pytorch/blob/main/torchgen/gen_backend_stubs.py). It generates the below 3 files: `XLANativeFunctions.h`, `RegisterXLA.cpp`, and `RegisterAutogradXLA.cpp`\n2. `XLANativeFunctions.h` and `aten_xla_type.cpp` are entry points of PyTorch to the pytorch_xla world, and contain the manually written lowerings to XLA for each operator. `XLANativeFunctions.h` is auto-generated through a combination of `xla_native_functions.yaml` and the PyTorch core `native_functions.yaml` file, and contains declarations for kernels that need to be defined in `aten_xla_type.cpp`. The kernels written here need to construct 'XLATensor' using the input `at::Tensor` and other parameters. The resulting `XLATensor` needs to be converted back to the `at::Tensor` before returning to the PyTorch world.\n3. `RegisterXLA.cpp` and `RegisterAutogradXLA.cpp` are auto-generated files that register all lowerings to the PyTorch Dispatcher. They also include auto-generated wrapper implementations of `out=` and `inplace` operators.\n4. `aten_fallback.h/.cpp` contain our boxed fallback implementation. The boxed fallback kernel will be used if a lowering is not explicitly defined in `xla_native_functions.yaml` + `aten_xla_type.cpp`, and the operator is not composite.\n5. `tensor_methods.h` contains the `XLATensor` declarations. These declarations are usually a one to one mapping of the `at::Tensor` nodes we declared in `XLANativeFunctions.h`\n6. `tensor_methods.cpp` contains the implementation of `XLATensor node` defined in `tensor_methods.h`. We constructed the corresponding `ir::op` from the parameter’s `ir::Value` and wrapped it inside a `XLATensor`. Ir stands for intermediate representation.\n7. `ops/` directory contains all `ir::ops` declaration and definition. Smaller nodes can be put in `ops/ops.h/.cpp`. More complicated nodes can be put into a separate file. All ops inherit from `ir::ops::Node` and provide a way to lower input `ir::Value` to a sequence of `XlaOp`.\n\n## Unit Test\nOur CI runs PyTorch native python tests for every change and every day. Those tests will use XLA implementation if we provide a lowering. We usually don’t need to add additional python tests for PyTorch/XLA unless we want to verify some xla behaviors(like dynamic shape) or we skipped the pytorch native test for some reason. The python test should be added to `xla/test/test_operations.py` if it is required. We also need to add CPP tests in `xla/test/cpp/test_aten_xla_tensor.cpp`. This test should call PyTorch c++ API and verify our implementation yields the same result as PyTorch native implementation. We also need to verify if the xla implementation is called when the tensor is a XLA tensor by checking the `aten::op` and `xla::op` counters.\n\n## Tips\nThe process of lowering is breaking down the PyTorch operations into a sequence of XlaOp. To provide a good lowering of the PyTorch operation, one needs to have a good grasp of what XLA is capable of. Reading the XlaOp document and looking into how similar ops is lowered is the best way to achieve that. You can find a minimal Op lowering example in [this pr](https://github.com/pytorch/xla/pull/2969). You can also find a slightly more complicated example with backward lowering in [this pr](https://github.com/pytorch/xla/pull/2972).\n\nWe have auto-generated wrapper implementations of `out=` and `inplace` operators for some operators in `RegisterXLA.cpp`. We only need to lower the vanilla op in this case. An example would be `lerp` operator which has 6 variants in `native_functions.yaml`, they are\n\n```\n  - lerp_.Scalar\n  - lerp_.Tensor\n  - lerp.Scalar_out\n  - lerp.Tensor_out\n  - lerp.Scalar\n  - lerp.Tensor\n```\n\nand will generate function prototypes\n\n```\nat::Tensor lerp(const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight);\nat::Tensor & lerp_(at::Tensor & self, const at::Tensor & end, const at::Scalar & weight);\nat::Tensor lerp(const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight);\nat::Tensor & lerp_out(const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight, at::Tensor & out);\nat::Tensor & lerp_(at::Tensor & self, const at::Tensor & end, const at::Tensor & weight);\nat::Tensor & lerp_out(const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight, at::Tensor & out);\n```\n\nin `XLANativeFunctions.h` if we add all of them to the `xla_native_functions.yaml`. However if we only lower `lerp.Scalar` and `lerp.Tensor` and check `RegisterXLA.cpp`, we will see\n\n```\nnamespace {\n\nat::Tensor wrapper_Scalar_lerp(const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {\n    // No device check\n\n\n  // DeviceGuard omitted\n  return torch_xla::lerp(self, end, weight);\n}\n\n} // anonymous namespace\n\nat::Tensor & wrapper_Scalar_lerp_(at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {\n  auto wrapper_Scalar_lerp__tmp = wrapper_Scalar_lerp(self, end, weight);\n  at::_copy_from(wrapper_Scalar_lerp__tmp, self);\n  return self;\n}\n\n...\n  m.impl(\"lerp_.Scalar\",\n  TORCH_FN(wrapper_Scalar_lerp_));\n\n```\n\nThe codegen will automatically generate lowerings for `lerp_.Scalar` and `lerp.Scalar_out` that use our `lerp.Scalar` implementation, without us having to provide an explicit lowering.\n\nIn general, if there is an operator in pytorch core that has both an out-of-place and an out= variant, it's better to write a lowering for the out-of-place variant, since you'll get a code-generated out= lowering for free.\n\nFor each node we need to pass an `ir::OpKind`. Here is an ([example](https://github.com/pytorch/xla/blob/5ce99bff336325feb41a982dc80299fb53166b29/torch_xla/csrc/ops/var_mean.cpp#L36)). You can find the `OpKind` definition in [interned_strings.h](https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/interned_strings.h). If the aten symbol is missing, you can submit a PR like [this](https://github.com/pytorch/pytorch/pull/36851).\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.6728515625,
          "content": "# PyTorch/XLA\n\n<b>Current CI status:</b>  ![GitHub Actions\nstatus](https://github.com/pytorch/xla/actions/workflows/build_and_test.yml/badge.svg)\n\nPyTorch/XLA is a Python package that uses the [XLA deep learning\ncompiler](https://www.tensorflow.org/xla) to connect the [PyTorch deep learning\nframework](https://pytorch.org/) and [Cloud\nTPUs](https://cloud.google.com/tpu/). You can try it right now, for free, on a\nsingle Cloud TPU VM with\n[Kaggle](https://www.kaggle.com/discussions/product-feedback/369338)!\n\nTake a look at one of our [Kaggle\nnotebooks](https://github.com/pytorch/xla/tree/master/contrib/kaggle) to get\nstarted:\n\n* [Stable Diffusion with PyTorch/XLA\n  2.0](https://github.com/pytorch/xla/blob/master/contrib/kaggle/pytorch-xla-2-0-on-kaggle.ipynb)\n* [Distributed PyTorch/XLA\n  Basics](https://github.com/pytorch/xla/blob/master/contrib/kaggle/distributed-pytorch-xla-basics-with-pjrt.ipynb)\n\n## Installation\n\n### TPU\n\nTo install PyTorch/XLA stable build in a new TPU VM:\n\n```\npip install torch~=2.5.0 torch_xla[tpu]~=2.5.0 -f https://storage.googleapis.com/libtpu-releases/index.html -f https://storage.googleapis.com/libtpu-wheels/index.html\n```\n\nTo install PyTorch/XLA nightly build in a new TPU VM:\n\n```\npip3 install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cpu\npip install 'torch_xla[tpu] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.7.0.dev-cp310-cp310-linux_x86_64.whl' -f https://storage.googleapis.com/libtpu-releases/index.html -f https://storage.googleapis.com/libtpu-wheels/index.html\n```\n\n### GPU Plugin\n\nPyTorch/XLA now provides GPU support through a plugin package similar to `libtpu`:\n\n```\npip install torch~=2.5.0 torch_xla~=2.5.0 https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla_cuda_plugin-2.5.0-py3-none-any.whl\n```\n\n## Getting Started\n\nTo update your existing training loop, make the following changes:\n\n```diff\n-import torch.multiprocessing as mp\n+import torch_xla as xla\n+import torch_xla.core.xla_model as xm\n\n def _mp_fn(index):\n   ...\n\n+  # Move the model paramters to your XLA device\n+  model.to(xla.device())\n\n   for inputs, labels in train_loader:\n+    with xla.step():\n+      # Transfer data to the XLA device. This happens asynchronously.\n+      inputs, labels = inputs.to(xla.device()), labels.to(xla.device())\n       optimizer.zero_grad()\n       outputs = model(inputs)\n       loss = loss_fn(outputs, labels)\n       loss.backward()\n-      optimizer.step()\n+      # `xm.optimizer_step` combines gradients across replicas\n+      xm.optimizer_step(optimizer)\n\n if __name__ == '__main__':\n-  mp.spawn(_mp_fn, args=(), nprocs=world_size)\n+  # xla.launch automatically selects the correct world size\n+  xla.launch(_mp_fn, args=())\n```\n\nIf you're using `DistributedDataParallel`, make the following changes:\n\n\n```diff\n import torch.distributed as dist\n-import torch.multiprocessing as mp\n+import torch_xla as xla\n+import torch_xla.distributed.xla_backend\n\n def _mp_fn(rank):\n   ...\n\n-  os.environ['MASTER_ADDR'] = 'localhost'\n-  os.environ['MASTER_PORT'] = '12355'\n-  dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n+  # Rank and world size are inferred from the XLA device runtime\n+  dist.init_process_group(\"xla\", init_method='xla://')\n+\n+  model.to(xm.xla_device())\n+  ddp_model = DDP(model)\n\n-  model = model.to(rank)\n-  ddp_model = DDP(model, device_ids=[rank])\n\n   for inputs, labels in train_loader:\n+    with xla.step():\n+      inputs, labels = inputs.to(xla.device()), labels.to(xla.device())\n       optimizer.zero_grad()\n       outputs = ddp_model(inputs)\n       loss = loss_fn(outputs, labels)\n       loss.backward()\n       optimizer.step()\n\n if __name__ == '__main__':\n-  mp.spawn(_mp_fn, args=(), nprocs=world_size)\n+  xla.launch(_mp_fn, args=())\n```\n\nAdditional information on PyTorch/XLA, including a description of its semantics\nand functions, is available at [PyTorch.org](http://pytorch.org/xla/). See the\n[API Guide](API_GUIDE.md) for best practices when writing networks that run on\nXLA devices (TPU, CUDA, CPU and...).\n\nOur comprehensive user guides are available at:\n\n[Documentation for the latest release](https://pytorch.org/xla)\n\n[Documentation for master branch](https://pytorch.org/xla/master)\n\n\n## PyTorch/XLA tutorials\n\n* [Cloud TPU VM\n  quickstart](https://cloud.google.com/tpu/docs/run-calculation-pytorch)\n* [Cloud TPU Pod slice\n  quickstart](https://cloud.google.com/tpu/docs/pytorch-pods)\n* [Profiling on TPU\n  VM](https://cloud.google.com/tpu/docs/pytorch-xla-performance-profiling-tpu-vm)\n* [GPU guide](docs/gpu.md)\n\n## Reference implementations\n\nThe [AI-Hypercomputer/tpu-recipes](https://github.com/AI-Hypercomputer/tpu-recipes)\nrepo. contains examples for training and serving many LLM and diffusion models.\n\n## Available docker images and wheels\n\n### Python packages\n\nPyTorch/XLA releases starting with version r2.1 will be available on PyPI. You\ncan now install the main build with `pip install torch_xla`. To also install the\nCloud TPU plugin corresponding to your installed `torch_xla`, install the optional `tpu` dependencies after installing the main build with\n\n```\npip install torch_xla[tpu] \\\n  -f https://storage.googleapis.com/libtpu-wheels/index.html \\\n  -f https://storage.googleapis.com/libtpu-releases/index.html\n```\n\nGPU and nightly builds are available in our public GCS bucket.\n\n| Version | Cloud GPU VM Wheels |\n| --- | ----------- |\n| 2.5 (CUDA 12.1 + Python 3.9) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla-2.5.0-cp39-cp39-manylinux_2_28_x86_64.whl` |\n| 2.5 (CUDA 12.1 + Python 3.10) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla-2.5.0-cp310-cp310-manylinux_2_28_x86_64.whl` |\n| 2.5 (CUDA 12.1 + Python 3.11) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla-2.5.0-cp311-cp311-manylinux_2_28_x86_64.whl` |\n| 2.5 (CUDA 12.4 + Python 3.9) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.4/torch_xla-2.5.0-cp39-cp39-manylinux_2_28_x86_64.whl` |\n| 2.5 (CUDA 12.4 + Python 3.10) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.4/torch_xla-2.5.0-cp310-cp310-manylinux_2_28_x86_64.whl` |\n| 2.5 (CUDA 12.4 + Python 3.11) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.4/torch_xla-2.5.0-cp311-cp311-manylinux_2_28_x86_64.whl` |\n| nightly (Python 3.9) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.7.0.dev-cp39-cp39-linux_x86_64.whl` |\n| nightly (Python 3.10) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.7.0.dev-cp310-cp310-linux_x86_64.whl` |\n| nightly (Python 3.11) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.7.0.dev-cp311-cp311-linux_x86_64.whl` |\n| nightly (CUDA 12.1 + Python 3.8) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla-2.6.0.dev-cp38-cp38-linux_x86_64.whl` |\n\n<details>\n\n<summary> Use nightly build before 08/13/2024</summary>\nYou can also add `+yyyymmdd` after `torch_xla-nightly` to get the nightly wheel of a specified date. Here is an example:\n\n```\npip3 install torch==2.6.0.dev20240925+cpu --index-url https://download.pytorch.org/whl/nightly/cpu\npip3 install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-nightly%2B20240925-cp310-cp310-linux_x86_64.whl\n```\n\nThe torch wheel version `2.6.0.dev20240925+cpu` can be found at https://download.pytorch.org/whl/nightly/torch/.\n</details>\n\n#### Use nightly build after 08/20/2024\n\nYou can also add `yyyymmdd` after `torch_xla-2.6.0.dev` (or the latest dev version)\nto get the nightly wheel of a specified date. Here is an example:\n\n```\npip3 install torch==2.5.0.dev20240820+cpu --index-url https://download.pytorch.org/whl/nightly/cpu\npip3 install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.5.0.dev20240820-cp310-cp310-linux_x86_64.whl\n```\n\nThe torch wheel version `2.6.0.dev20240925+cpu` can be found at https://download.pytorch.org/whl/nightly/torch/.\n\n#### Use nightly build with C++11 ABI after 10/28/2024\n\nBy default, `torch` is built with pre-C++11 version of ABI (see https://github.com/pytorch/pytorch/issues/51039).\n`torch_xla` follows that and ships pre-C++11 builds by default. However, the lazy\ntensor tracing performance can be improved by building the code with C++11 ABI.\nAs a result, we provide C++11 ABI builds for interested users to try, especially\nif you find your model performance bottlenecked in Python lazy tensor tracing.\n\nYou can add `.cxx11` after `yyyymmdd` to get the C++11 ABI variant of a\nspecific nightly wheel.  Here is an example to install nightly builds from\n10/28/2024:\n\n```\npip3 install torch==2.6.0.dev20241028+cpu.cxx11.abi --index-url https://download.pytorch.org/whl/nightly\npip3 install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.6.0.dev20241028.cxx11-cp310-cp310-linux_x86_64.whl\n```\n\n**As of 12/11/2024, the torch_xla C++11 ABI wheel is named differently and can be installed as follows:**\n```\npip3 install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.6.0.dev20241211+cxx11-cp310-cp310-linux_x86_64.whl\n```\n\nThe torch wheel version `2.6.0.dev20241028+cpu.cxx11.abi` can be found at https://download.pytorch.org/whl/nightly/torch/.\n\n<details>\n\n<summary>older versions</summary>\n\n| Version | Cloud TPU VMs Wheel |\n|---------|-------------------|\n| 2.4 (Python 3.10) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.4.0-cp310-cp310-manylinux_2_28_x86_64.whl` |\n| 2.3 (Python 3.10) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.3.0-cp310-cp310-manylinux_2_28_x86_64.whl` |\n| 2.2 (Python 3.10) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.2.0-cp310-cp310-manylinux_2_28_x86_64.whl` |\n| 2.1 (XRT + Python 3.10) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/xrt/tpuvm/torch_xla-2.1.0%2Bxrt-cp310-cp310-manylinux_2_28_x86_64.whl` |\n| 2.1 (Python 3.8) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.1.0-cp38-cp38-linux_x86_64.whl` |\n\n<br/>\n\n| Version | GPU Wheel |\n| --- | ----------- |\n| 2.5 (CUDA 12.1 + Python 3.9) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla-2.5.0-cp39-cp39-manylinux_2_28_x86_64.whl` |\n| 2.5 (CUDA 12.1 + Python 3.10) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla-2.5.0-cp310-cp310-manylinux_2_28_x86_64.whl` |\n| 2.5 (CUDA 12.1 + Python 3.11) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla-2.5.0-cp311-cp311-manylinux_2_28_x86_64.whl` |\n| 2.5 (CUDA 12.4 + Python 3.9) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.4/torch_xla-2.5.0-cp39-cp39-manylinux_2_28_x86_64.whl` |\n| 2.5 (CUDA 12.4 + Python 3.10) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.4/torch_xla-2.5.0-cp310-cp310-manylinux_2_28_x86_64.whl` |\n| 2.5 (CUDA 12.4 + Python 3.11) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.4/torch_xla-2.5.0-cp311-cp311-manylinux_2_28_x86_64.whl` |\n| 2.4 (CUDA 12.1 + Python 3.9) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla-2.4.0-cp39-cp39-manylinux_2_28_x86_64.whl` |\n| 2.4 (CUDA 12.1 + Python 3.10) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla-2.4.0-cp310-cp310-manylinux_2_28_x86_64.whl` |\n| 2.4 (CUDA 12.1 + Python 3.11) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla-2.4.0-cp311-cp311-manylinux_2_28_x86_64.whl` |\n| 2.3 (CUDA 12.1 + Python 3.8) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla-2.3.0-cp38-cp38-manylinux_2_28_x86_64.whl` |\n| 2.3 (CUDA 12.1 + Python 3.10) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla-2.3.0-cp310-cp310-manylinux_2_28_x86_64.whl` |\n| 2.3 (CUDA 12.1 + Python 3.11) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl` |\n| 2.2 (CUDA 12.1 + Python 3.8) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla-2.2.0-cp38-cp38-manylinux_2_28_x86_64.whl` |\n| 2.2 (CUDA 12.1 + Python 3.10) | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.1/torch_xla-2.2.0-cp310-cp310-manylinux_2_28_x86_64.whl` |\n| 2.1 + CUDA 11.8 | `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/11.8/torch_xla-2.1.0-cp38-cp38-manylinux_2_28_x86_64.whl` |\n| nightly + CUDA 12.0 >= 2023/06/27| `https://storage.googleapis.com/pytorch-xla-releases/wheels/cuda/12.0/torch_xla-nightly-cp38-cp38-linux_x86_64.whl` |\n\n</details>\n\n### Docker\n\n| Version | Cloud TPU VMs Docker |\n| --- | ----------- |\n| 2.5 | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:r2.5.0_3.10_tpuvm` |\n| 2.4 | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:r2.4.0_3.10_tpuvm` |\n| 2.3 | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:r2.3.0_3.10_tpuvm` |\n| 2.2 | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:r2.2.0_3.10_tpuvm` |\n| 2.1 | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:r2.1.0_3.10_tpuvm` |\n| nightly python | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.10_tpuvm` |\n| nightly python (C++11 ABI) | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.10_tpuvm_cxx11` |\n\nTo use the above dockers, please pass `--privileged --net host --shm-size=16G` along. Here is an example:\n```bash\ndocker run --privileged --net host --shm-size=16G -it us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.10_tpuvm /bin/bash\n```\n\n<br/>\n\n\n| Version | GPU CUDA 12.4 Docker |\n| --- | ----------- |\n| 2.5 | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:r2.5.0_3.10_cuda_12.4` |\n| 2.4 | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:r2.4.0_3.10_cuda_12.4` |\n\n<br/>\n\n\n| Version | GPU CUDA 12.1 Docker |\n| --- | ----------- |\n| 2.5 | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:r2.5.0_3.10_cuda_12.1` |\n| 2.4 | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:r2.4.0_3.10_cuda_12.1` |\n| 2.3 | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:r2.3.0_3.10_cuda_12.1` |\n| 2.2 | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:r2.2.0_3.10_cuda_12.1` |\n| 2.1 | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:r2.1.0_3.10_cuda_12.1` |\n| nightly | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.8_cuda_12.1` |\n| nightly at date | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.8_cuda_12.1_YYYYMMDD` |\n\n<br/>\n\n| Version | GPU CUDA 11.8 + Docker |\n| --- | ----------- |\n| 2.1 | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:r2.1.0_3.10_cuda_11.8` |\n| 2.0 | `us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:r2.0_3.8_cuda_11.8` |\n\n<br/>\n\n\nTo run on [compute instances with\nGPUs](https://cloud.google.com/compute/docs/gpus/create-vm-with-gpus).\n\n## Troubleshooting\n\nIf PyTorch/XLA isn't performing as expected, see the [troubleshooting\nguide](docs/source/learn/troubleshoot.md), which has suggestions for debugging and optimizing\nyour network(s).\n\n## Providing Feedback\n\nThe PyTorch/XLA team is always happy to hear from users and OSS contributors!\nThe best way to reach out is by filing an issue on this Github. Questions, bug\nreports, feature requests, build issues, etc. are all welcome!\n\n## Contributing\n\nSee the [contribution guide](CONTRIBUTING.md).\n\n## Disclaimer\n\nThis repository is jointly operated and maintained by Google, Meta and a\nnumber of individual contributors listed in the\n[CONTRIBUTORS](https://github.com/pytorch/xla/graphs/contributors) file. For\nquestions directed at Meta, please send an email to opensource@fb.com. For\nquestions directed at Google, please send an email to\npytorch-xla@googlegroups.com. For all other questions, please open up an issue\nin this repository [here](https://github.com/pytorch/xla/issues).\n\n## Additional Reads\n\nYou can find additional useful reading materials in\n* [Performance debugging on Cloud TPU\n  VM](https://cloud.google.com/blog/topics/developers-practitioners/pytorchxla-performance-debugging-tpu-vm-part-1)\n* [Lazy tensor\n  intro](https://pytorch.org/blog/understanding-lazytensor-system-performance-with-pytorch-xla-on-cloud-tpu/)\n* [Scaling deep learning workloads with PyTorch / XLA and Cloud TPU\n  VM](https://cloud.google.com/blog/topics/developers-practitioners/scaling-deep-learning-workloads-pytorch-xla-and-cloud-tpu-vm)\n* [Scaling PyTorch models on Cloud TPUs with\n  FSDP](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/)\n\n## Related Projects\n\n* [OpenXLA](https://github.com/openxla)\n* [HuggingFace](https://huggingface.co/docs/accelerate/en/basic_tutorials/tpu)\n* [JetStream](https://github.com/google/JetStream-pytorch)\n"
        },
        {
          "name": "WORKSPACE",
          "type": "blob",
          "size": 4.0625,
          "content": "load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\n\n################################ Python Setup ################################\n\n# For embedded python interpreter (libpython.so.)\nhttp_archive(\n    name = \"pybind11_bazel\",\n    strip_prefix = \"pybind11_bazel-fc56ce8a8b51e3dd941139d329b63ccfea1d304b\",\n    urls = [\"https://github.com/pybind/pybind11_bazel/archive/fc56ce8a8b51e3dd941139d329b63ccfea1d304b.zip\"],\n)\n\nhttp_archive(\n    name = \"pybind11\",\n    build_file = \"@pybind11_bazel//:pybind11.BUILD\",\n    strip_prefix = \"pybind11-442261da585536521ff459b1457b2904895f23b4\",\n    urls = [\"https://github.com/pybind/pybind11/archive/442261da585536521ff459b1457b2904895f23b4.tar.gz\"],\n)\n\nhttp_archive(\n    name = \"com_nlohmann_json\",\n    build_file = \"//bazel:nlohmann_json.BUILD\",\n    sha256 = \"d69f9deb6a75e2580465c6c4c5111b89c4dc2fa94e3a85fcd2ffcd9a143d9273\",\n    strip_prefix = \"json-3.11.2\",\n    url = \"https://github.com/nlohmann/json/archive/refs/tags/v3.11.2.tar.gz\",\n)\n\nload(\"@pybind11_bazel//:python_configure.bzl\", \"python_configure\")\n\n# This is required for setting up the linkopts for -lpython.q\npython_configure(\n    name = \"local_config_python\",\n    python_version = \"3\",  # required to use `python3-config`\n)\n\n################################ PyTorch Setup ################################\n\nload(\"//bazel:dependencies.bzl\", \"PYTORCH_LOCAL_DIR\")\n\nnew_local_repository(\n    name = \"torch\",\n    build_file = \"//bazel:torch.BUILD\",\n    path = PYTORCH_LOCAL_DIR,\n)\n\n############################# OpenXLA Setup ###############################\n\n# To update OpenXLA to a new revision,\n# a) update URL and strip_prefix to the new git commit hash\n# b) get the sha256 hash of the commit by running:\n#    curl -L https://github.com/openxla/xla/archive/<git hash>.tar.gz | sha256sum\n#    and update the sha256 with the result.\n\nxla_hash = 'd28bfbdc366627c9ac9f57fcaa512ff04de19d6f'\n\nhttp_archive(\n    name = \"xla\",\n    patch_args = [\n        \"-l\",\n        \"-p1\",\n    ],\n    patch_tool = \"patch\",\n    patches = [\n        \"//openxla_patches:gpu_race_condition.diff\",\n    ],\n    strip_prefix = \"xla-\" + xla_hash,\n    urls = [\n        \"https://github.com/openxla/xla/archive/\" + xla_hash + \".tar.gz\",\n    ],\n)\n\n\n\n# For development, one often wants to make changes to the OpenXLA repository as well\n# as the PyTorch/XLA repository. You can override the pinned repository above with a\n# local checkout by either:\n# a) overriding the OpenXLA repository on the build.py command line by passing a flag\n#    like:\n#    bazel --override_repository=xla=/path/to/openxla\n#    or\n# b) by commenting out the http_archive above and uncommenting the following:\n# local_repository(\n#    name = \"xla\",\n#    path = \"/path/to/openxla\",\n# )\n\n# Initialize hermetic Python\nload(\"@xla//third_party/py:python_init_rules.bzl\", \"python_init_rules\")\n\npython_init_rules()\n\nload(\"@xla//third_party/py:python_init_repositories.bzl\", \"python_init_repositories\")\n\npython_init_repositories(\n    requirements = {\n        \"3.8\": \"//:requirements_lock_3_8.txt\",\n        \"3.9\": \"//:requirements_lock_3_9.txt\",\n        \"3.10\": \"//:requirements_lock_3_10.txt\",\n        \"3.11\": \"//:requirements_lock_3_11.txt\",\n    },\n    local_wheel_workspaces = [\"@torch//:WORKSPACE\"],\n    default_python_version = \"system\",\n)\n\nload(\"@xla//third_party/py:python_init_toolchains.bzl\", \"python_init_toolchains\")\n\npython_init_toolchains()\n\nload(\"@xla//third_party/py:python_init_pip.bzl\", \"python_init_pip\")\n\npython_init_pip()\n\nload(\"@pypi//:requirements.bzl\", \"install_deps\")\n\ninstall_deps()\n\n\n\n# Initialize OpenXLA's external dependencies.\nload(\"@xla//:workspace4.bzl\", \"xla_workspace4\")\n\nxla_workspace4()\n\nload(\"@xla//:workspace3.bzl\", \"xla_workspace3\")\n\nxla_workspace3()\n\nload(\"@xla//:workspace2.bzl\", \"xla_workspace2\")\n\nxla_workspace2()\n\nload(\"@xla//:workspace1.bzl\", \"xla_workspace1\")\n\nxla_workspace1()\n\nload(\"@xla//:workspace0.bzl\", \"xla_workspace0\")\n\nxla_workspace0()\n\nload(\"@tsl//third_party/gpus:cuda_configure.bzl\", \"cuda_configure\")\ncuda_configure(name = \"local_config_cuda\")\nload(\"@tsl//third_party/nccl:nccl_configure.bzl\", \"nccl_configure\")\nnccl_configure(name = \"local_config_nccl\")\n"
        },
        {
          "name": "bazel",
          "type": "tree",
          "content": null
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "build_util.py",
          "type": "blob",
          "size": 2.439453125,
          "content": "import os\nfrom typing import Iterable\nimport subprocess\nimport sys\nimport shutil\n\n\ndef check_env_flag(name: str, default: str = '') -> bool:\n  return os.getenv(name, default).upper() in ['ON', '1', 'YES', 'TRUE', 'Y']\n\n\ndef bazel_options_from_env() -> Iterable[str]:\n  bazel_flags = []\n\n  if check_env_flag('DEBUG'):\n    bazel_flags.append('--config=dbg')\n\n  if check_env_flag('TPUVM_MODE'):\n    bazel_flags.append('--config=tpu')\n\n  gcloud_key_file = os.getenv('GCLOUD_SERVICE_KEY_FILE', default='')\n  # Remote cache authentication.\n  if gcloud_key_file:\n    # Temporary workaround to allow PRs from forked repo to run CI. See details at (#5259).\n    # TODO: Remove the check once self-hosted GHA workers are available to CPU/GPU CI.\n    gcloud_key_file_size = os.path.getsize(gcloud_key_file)\n    if gcloud_key_file_size > 1:\n      bazel_flags.append('--google_credentials=%s' % gcloud_key_file)\n      bazel_flags.append('--config=remote_cache')\n  else:\n    if check_env_flag('BAZEL_REMOTE_CACHE'):\n      bazel_flags.append('--config=remote_cache')\n\n  cache_silo_name = os.getenv('SILO_NAME', default='dev')\n  if cache_silo_name:\n    bazel_flags.append('--remote_default_exec_properties=cache-silo-key=%s' %\n                       cache_silo_name)\n\n  bazel_jobs = os.getenv('BAZEL_JOBS', default='')\n  if bazel_jobs:\n    bazel_flags.append('--jobs=%s' % bazel_jobs)\n\n  # Build configuration.\n  if check_env_flag('BAZEL_VERBOSE'):\n    bazel_flags.append('-s')\n  if check_env_flag('XLA_CUDA'):\n    bazel_flags.append('--config=cuda')\n  if check_env_flag('XLA_CPU_USE_ACL'):\n    bazel_flags.append('--config=acl')\n\n  return bazel_flags\n\n\ndef bazel_build(bazel_target: str,\n                destination_dir: str,\n                options: Iterable[str] = []):\n  bazel_argv = [\n      'bazel', 'build', bazel_target,\n      f\"--symlink_prefix={os.path.join(os.getcwd(), 'bazel-')}\"\n  ]\n\n  # Remove duplicated flags because they confuse bazel\n  flags = set(bazel_options_from_env() + options)\n  bazel_argv.extend(flags)\n\n  print(' '.join(bazel_argv), flush=True)\n  subprocess.check_call(bazel_argv, stdout=sys.stdout, stderr=sys.stderr)\n\n  target_path = bazel_target.replace('@xla//', 'external/xla/').replace(\n      '//', '').replace(':', '/')\n  output_path = os.path.join('bazel-bin', target_path)\n  output_filename = os.path.basename(output_path)\n\n  if not os.path.exists(destination_dir):\n    os.makedirs(destination_dir)\n\n  shutil.copyfile(output_path, os.path.join(destination_dir, output_filename))\n"
        },
        {
          "name": "codegen",
          "type": "tree",
          "content": null
        },
        {
          "name": "configuration.yaml",
          "type": "blob",
          "size": 14.296875,
          "content": "---\nvariables:\n  pjrt_variables:\n    PJRT_DEVICE:\n      description:\n        - Indicates which device is being used with PJRT. It can be either CPU,\n          TPU, or CUDA\n      type: string\n    PJRT_SELECT_DEFAULT_DEVICE:\n      description:\n       - Whether or not to select a default PJRT device based on the environment\n         if the runtime is not already configured.\n    PJRT_CPU_ASYNC_CLIENT:\n      description:\n        - Whether or not to create an async PJRT client for the CPU device(s).\n      type: bool\n      default_value: false\n    PJRT_GPU_ASYNC_CLIENT:\n      description:\n        - Whether or not to create an async PJRT client for the GPU device(s).\n      type: bool\n      default_value: false\n    PJRT_TPU_MAX_INFLIGHT_COMPUTATIONS:\n      description:\n        - Max inflight computations that the PJRT client can handle for TPU.\n      type: int\n      default_value: 32\n  build_variables:\n    DEBUG:\n      description:\n        - Whether or not to build pytorch/xla in the debug mode.\n      type: bool\n      default_value: false\n    GRPC_VERBOSITY:\n      description:\n        - Verbosity level for GRPC, e.g. INFO, ERROR, etc.\n      type: string\n      default_value: \"ERROR\"\n    XLA_CUDA:\n      description:\n        - Build the xla client with CUDA enabled.\n      type: bool\n      default_value: false\n    GIT_VERSIONED_XLA_BUILD:\n      description:\n        - Creates a versioned build. In particular, appends a git sha to the\n          version number string\n      type: bool\n      default_value: false\n    COMPILE_PARALLEL:\n      description:\n        - Enabled parallel compile for PyTorch/XLA build.\n      type: bool\n      default_value: true\n    BUILD_CPP_TESTS:\n      description:\n        - Whether or not to build the cpp tests.\n      type: bool\n      default_value: true\n    BUNDLE_LIBTPU:\n      description:\n        - Whether or not to include libtpu in the final wheel.\n      type: bool\n      default_value: false\n    ALLOW_MULTIPLE_LIBTPU_LOAD:\n      description:\n        - Allow for multiple processes to load libtpu at the same time.\n      type: bool\n      default_value: true\n    PT_XLA_DEBUG:\n      description:\n        - Used to automatically analyze the metrics report and provide a\n          summary.\n      type: bool\n      default_value: false\n    TPUVM_MODE:\n      description:\n        - Include some additional TPUVM features and code when building the\n          third party tensorflow.\n      type: bool\n      default_value: false\n    TORCH_XLA_VERSION:\n      description:\n        - Specifies the version of PyTorch/XLA, rather than the hard-coded\n          version in setup.py; used when we're building binaries for\n          distribution. Should be parseable as a version number, e.g. 1.14\n      type: string\n      default_value: \"1.14\"\n    TORCH_XLA_PACKAGE_NAME:\n      description:\n        - Allows the developer to change the package name to something other\n          than torch_xla.\n      type: string\n      default_value: \"torch_xla\"\n    TPU_ML_PLATFORM:\n      description:\n        - Name of the ML platform being used on TPU, e.g. PyTorch/XLA,\n          Tensorflow, or JAX.\n      type: string\n      default_value: \"PyTorch/XLA\"\n    XLA_BAZEL_VERBOSE:\n      description:\n        - Turn on verbose messages during the bazel build of the xla/xrt client.\n      type: bool\n      default_value: false\n  feature_variables:\n    XLA_SYNC_WAIT:\n      description:\n        - Forces the XLA tensor sync operation to wait for its completion,\n          before moving to the next step.\n      type: bool\n      default_value: false\n    XLA_NO_SPECIAL_SCALARS:\n      description:\n        - When set to false, this will route some tensor values to constant\n          scalars.\n      type: bool\n      default_value: false\n    XLA_TENSOR_UPDATE_SYNC:\n      description:\n        - Used to decide whether or not to sync update in\n          XLANativeFunctions::_copy_from.\n      type: bool\n      default_value: true\n    XLA_IO_THREAD_POOL_SIZE:\n      description:\n        - Number of threads for the IO thread pool in the XLA client. Defaults\n          to std::thread::hardware_concurrency().\n      type: int\n    XLA_TENSOR_ALLOCATOR_MAXSIZE:\n      description:\n        - Max cache size to be used by TensorAllocator in XRT. We only cache\n          blocks smaller than this number, measured in bytes.\n      type: int\n      default_value: 1000000000\n    XLA_IR_SHAPE_CACHE_SIZE:\n      description:\n        - Size for the shape cache used by XLA.\n      type: int\n      default_value: 12288\n    XLA_DEVDATA_CACHE_SIZE:\n      description:\n        - Max cache size for XLA Data cache.\n      type: int\n      default_value: 128\n    XLA_TRIM_GRAPH_CHECK_FREQUENCY:\n      description:\n        - Frequency to check and, if applicable, trim the IR graph.\n      type: int\n      default_value: 5000\n    XLA_DENSE_GATHER_FACTOR:\n      description:\n        - Used as a threshold for when we should use dense gather. We multiply\n          the factor by 10, and compare it to the number of input elements. If\n          there are more input elements, we'll use sparse gather.\n      type: int\n      default_value: 8192\n    XLA_DENSE_SCATTER_FACTOR:\n      description:\n        - Used as a threshold to determine when to use dense scatter. If the\n          dense scatter factor times the number of index elements is greater\n          than or equal to the number of input elements, we use dense scatter\n      type: int\n      default_value: 100\n    XLA_RESIZE_SPLIT_FACTOR:\n      description:\n        - Used as a threshold to determine when the resize is too large to be\n          done all at once, in which case we do one dimension at a time.\n      type: float\n      default_value: 3.0\n    XLA_MAX_PADDING_FACTOR:\n      description:\n        - Used as a threshold to determine whether to use a sorted or\n          descending layout for shape.\n      type: float\n      default_value: 1.25\n    XLA_LAYOUTS:\n      description:\n        - A list of key/value pairs of the format \"k1=v1;k2=v2\". Keys are\n          Shapes and values are layouts.\n      type: string\n      default_value: \"\"\n    XLA_RNG_BIT_GENERATOR:\n      description:\n        - String name of the bit generator type, which can be either default,\n          philox, or three_fry. No default value because in that case there's\n          special behavior.\n      type: string\n    XLA_EXPERIMENTAL:\n      description:\n        - Used to enable experimental features. Representing a list separated\n          by \":\".\n      type: string\n      default_value: \"\"\n    XLA_FLAGS:\n      description:\n        - List of flags used by XLA, separated by \" \". This is only referenced\n          in PyTorch/XLA to add flags to the list.\n      type: string\n      default_value: \"\"\n    DISABLE_NUMERIC_CC_TOKEN:\n      description:\n        - Whether or not to skip modifying the existing token based on the\n          XlaOp when creating a new token. When disabled, the same token is\n          used for every XlaOp.\n      type: bool\n      default_value: false\n    XLA_USE_SPMD:\n      description:\n        - Deprecated. Whether or not to use the SPMD virtual device optimization.\n          Use `torch_xla.runtime.use_spmd()` instead.\n      type: bool\n      default_value: false\n    SPLIT_EXECUTOR_CACHE_SIZE:\n      description:\n        - Compiler cache size for the op by op executor.\n      type: int\n      default_value: 2048\n    XLA_USE_DUMMY_STORE:\n      description:\n        - If set to true, and user skips store based barrier by\n          setting TORCH_DIST_INIT_BARRIER=0, the `pjrt_rendezvous_handler`\n          will create a DummyStore to replace TCPStore to save open file\n          descriptors.\n      type: bool\n      default_value: false\n  device_variables:\n    TPU_NUM_DEVICES:\n      description:\n        - Number of TPU devices being used by this instance of XRT.\n      type: int\n      default_value: 8\n    CPU_NUM_DEVICES:\n      description:\n        - Number of CPU devices being used by this instance of XRT.\n      type: int\n    GPU_NUM_DEVICES:\n      description:\n        - Number of GPU devices being used by this instance of XRT.\n      type: int\n  debug_variables:\n    XLA_FNTRACKER_FILE:\n      description:\n        - If set, the path to a file where output from the function tracker\n          should be written.\n      type: string\n      default_value: \"\"\n    XLA_FNTRACKER_LIST:\n      description:\n        - Tags for the tracker context, which tell the function tracker which\n          functions to track.\n      type: string\n      default_value: \"\"\n    XLA_METRICS_SAMPLES:\n      description:\n        - Max samples to use for any metric.\n      type: int\n      default_value: 1024\n    XLA_COMPILE_TIME_THRESHOLD:\n      description:\n        - Threshold that determines when we log a slow compilation to the hlo\n          folder. Defaults to std::numeric_limits<double>::max().\n      type: int\n    XLA_FNTRACKER_LEVEL:\n      description:\n        - Level for the tracker context. When tracking functions, only\n          functions with a level less than or equal to this level will get\n          tracked. Defaults to std::numeric_limits<int>::max().\n      type: int\n    XLA_SAVE_TENSORS_FILE:\n      description:\n        - The path to a file which will be used to dump the IR graphs during\n          execution. Note that the file can become really big if the option is\n          left enabled and the PyTorch program let run for long time. The\n          graphs are appended to the file, so to have a clean sheet from run to\n          run, the file should be explicitly removed.\n      type: string\n      default_value: \"\"\n    XLA_SAVE_TENSORS_FMT:\n      description:\n        - The format of the graphs stored within the XLA_SAVE_TENSORS_FILE\n          file. Can be text (the default), dot (the Graphviz format) or hlo.\n      type: string\n      default_value: \"text\"\n    XLA_METRICS_FILE:\n      description:\n        - If set, the path to a local file where the internal metrics will be\n          saved at every step. Metrics will be appended to the file, if already\n          existing. Internally defaults to \"None\".\n      type: string\n    XLA_SAVE_HLO_FILE:\n      description:\n        - If set, the path to a local file where, in case of\n          compilation/execution error, the offending HLO graph will be saved.\n      type: string\n      default_value: \"\"\n    XLA_SLOW_COMPILE_HLO_FOLDER:\n      description:\n        - Folder name to save HLO files to, when the compile time is above a\n          certain threshold.\n      type: string\n      default_value: \"\"\n    XLA_TEST_DUMP_GRAPHS:\n      description:\n        - Type of graph to print to std::cerr. It can be empty, text, hlo, or\n          dot.\n      type: string\n      default_value: \"\"\n    PT_XLA_DEBUG_FILE:\n      description:\n        - If set, filepath used for printing out reports.\n      type: string\n      default_value: \"\"\n    TF_CPP_VMODULE:\n      description:\n        - Environment variable used for TF VLOGs and takes the form of\n          TF_CPP_VMODULE=name=value,.... Note that for VLOGs you must set\n          TF_CPP_MIN_LOG_LEVEL=0. For PyTorch/XLA using a configuration like\n          TF_CPP_VMODULE=tensor=5 would enable logging.\n      type: string\n    TF_CPP_MIN_LOG_LEVEL:\n      description:\n        - Level to print messages for. TF_CPP_MIN_LOG_LEVEL=0 will turn on INFO\n          logging, TF_CPP_MIN_LOG_LEVEL=1 WARNING and so on. Our PyTorch/XLA\n          TF_VLOG uses tensorflow::INFO level by default so to see VLOGs set\n          TF_CPP_MIN_LOG_LEVEL=0.\n      type: int\n      default_value: 1\n    TF_CPP_LOG_THREAD_ID:\n      description:\n        - If set to true, the TF logs will show the thread ID helping with\n          debugging multithreaded processes.\n      type: bool\n      default_value: false\n    TORCH_TEST_DEVICES:\n      description:\n        - Provided by the upstream and used to test new device types.\n      type: string\n    XLA_IR_DEBUG:\n      description:\n        - Enables the Python stack trace to be captured where creating IR\n          nodes, hence allowing to understand which PyTorch operation was\n          responsible for generating the IR.\n      type: bool\n      default_value: false\n    XLA_HLO_DEBUG:\n      description:\n        - Enables the Python stack frame captured when XLA_IR_DEBUG is active,\n          to be propagated to the XLA HLO metadata.\n      type: bool\n      default_value: false\n    XLA_TEST_DUMP_METRICS:\n      description:\n        - Controls whether or not metrics are dumped in cpp test tear down.\n      type: bool\n      default_value: false\n    XLA_TEST_DUMP_TENSORS:\n      description:\n        - Whether or not to print tensors to std::cerr in CPP tests.\n      type: bool\n      default_value: false\n    XLA_DUMP_HLO_GRAPH:\n      description:\n        - If set to true in case of a compilation or execution error the\n          offending HLO graph will be dumped as part of the runtime error\n          raised by xla_util.cc.\n      type: bool\n      default_value: false\n    XLA_DUMP_FATAL_STACK:\n      description:\n        - Installs the stack trace handler upon XLA client creation.\n      type: bool\n      default_value: false\n    XLA_METRICS_PERCENTILES:\n      description:\n        - List of metrics percentiles to record.\n      type: string\n      default_value: \"0.01:0.05:0.1:0.2:0.5:0.8:0.9:0.95:0.99\"\n    XLA_RELEASE_GIL_DURING_TRANSFER:\n      description:\n        - Release Python's GIL when transferring data from the runtime.\n      type: bool\n      default_value: true\n    XLA_STABLEHLO_COMPILE:\n      description:\n        - Pass StableHLO to XLA PjRt client for compilation. This compilation\n          flag is experimental. The default_value will be set to true when\n          StableHLO workflow is mature.\n      type: bool\n      default_value: false\n    XLA_DUMP_POST_OPTIMIZATIONS:\n      description:\n        - Dump the HLO graph after optimizations. You need to use it together\n          with XLA_SAVE_TENSORS_FMT='hlo' and XLA_SAVE_TENSORS_FILE='your/location'.\n      type: bool\n      default_value: false\n    XLA_DISABLE_FUNCTIONALIZATION:\n      description:\n        - Setting this to true will disable functionalization, which is a dispatcher\n          pass in PyTorch that remove views and mutations to produce functional graphs.\n          This flag's main purpose is to A/B test the impact of functionalization in\n          your code.\n      type: bool\n      default_value: false\n    XLA_FALLBACK_CPU:\n      description:\n        - Forces CPU OpenXLA fallback. By default, PyTorch/XLA will run any operation\n          that doesn't have a lowering using PyTorch CUDA as fallback. Setting this\n          flag will force PyTorch/XLA to use PyTorch CPU as fallback.\n"
        },
        {
          "name": "contrib",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "experimental",
          "type": "tree",
          "content": null
        },
        {
          "name": "infra",
          "type": "tree",
          "content": null
        },
        {
          "name": "openxla_patches",
          "type": "tree",
          "content": null
        },
        {
          "name": "plugins",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.in",
          "type": "blob",
          "size": 0.0791015625,
          "content": "filelock\nfsspec\njinja2\nmarkupsafe\nmpmath\nnetworkx\npyyaml\nsympy\ntyping-extensions\n"
        },
        {
          "name": "requirements_lock_3_10.txt",
          "type": "blob",
          "size": 10.947265625,
          "content": "#\n# This file is autogenerated by pip-compile with Python 3.10\n# by the following command:\n#\n#    bazel run //:requirements.update\n#\nfilelock==3.14.0 \\\n    --hash=sha256:43339835842f110ca7ae60f1e1c160714c5a6afd15a2873419ab185334975c0f \\\n    --hash=sha256:6ea72da3be9b8c82afd3edcf99f2fffbb5076335a5ae4d03248bb5b6c3eae78a\n    # via -r requirements.in\nfsspec==2024.5.0 \\\n    --hash=sha256:1d021b0b0f933e3b3029ed808eb400c08ba101ca2de4b3483fbc9ca23fcee94a \\\n    --hash=sha256:e0fdbc446d67e182f49a70b82cf7889028a63588fde6b222521f10937b2b670c\n    # via -r requirements.in\njinja2==3.1.4 \\\n    --hash=sha256:4a3aee7acbbe7303aede8e9648d13b8bf88a429282aa6122a993f0ac800cb369 \\\n    --hash=sha256:bc5dd2abb727a5319567b7a813e6a2e7318c39f4f487cfe6c89c6f9c7d25197d\n    # via -r requirements.in\nmarkupsafe==2.1.5 \\\n    --hash=sha256:00e046b6dd71aa03a41079792f8473dc494d564611a8f89bbbd7cb93295ebdcf \\\n    --hash=sha256:075202fa5b72c86ad32dc7d0b56024ebdbcf2048c0ba09f1cde31bfdd57bcfff \\\n    --hash=sha256:0e397ac966fdf721b2c528cf028494e86172b4feba51d65f81ffd65c63798f3f \\\n    --hash=sha256:17b950fccb810b3293638215058e432159d2b71005c74371d784862b7e4683f3 \\\n    --hash=sha256:1f3fbcb7ef1f16e48246f704ab79d79da8a46891e2da03f8783a5b6fa41a9532 \\\n    --hash=sha256:2174c595a0d73a3080ca3257b40096db99799265e1c27cc5a610743acd86d62f \\\n    --hash=sha256:2b7c57a4dfc4f16f7142221afe5ba4e093e09e728ca65c51f5620c9aaeb9a617 \\\n    --hash=sha256:2d2d793e36e230fd32babe143b04cec8a8b3eb8a3122d2aceb4a371e6b09b8df \\\n    --hash=sha256:30b600cf0a7ac9234b2638fbc0fb6158ba5bdcdf46aeb631ead21248b9affbc4 \\\n    --hash=sha256:397081c1a0bfb5124355710fe79478cdbeb39626492b15d399526ae53422b906 \\\n    --hash=sha256:3a57fdd7ce31c7ff06cdfbf31dafa96cc533c21e443d57f5b1ecc6cdc668ec7f \\\n    --hash=sha256:3c6b973f22eb18a789b1460b4b91bf04ae3f0c4234a0a6aa6b0a92f6f7b951d4 \\\n    --hash=sha256:3e53af139f8579a6d5f7b76549125f0d94d7e630761a2111bc431fd820e163b8 \\\n    --hash=sha256:4096e9de5c6fdf43fb4f04c26fb114f61ef0bf2e5604b6ee3019d51b69e8c371 \\\n    --hash=sha256:4275d846e41ecefa46e2015117a9f491e57a71ddd59bbead77e904dc02b1bed2 \\\n    --hash=sha256:4c31f53cdae6ecfa91a77820e8b151dba54ab528ba65dfd235c80b086d68a465 \\\n    --hash=sha256:4f11aa001c540f62c6166c7726f71f7573b52c68c31f014c25cc7901deea0b52 \\\n    --hash=sha256:5049256f536511ee3f7e1b3f87d1d1209d327e818e6ae1365e8653d7e3abb6a6 \\\n    --hash=sha256:58c98fee265677f63a4385256a6d7683ab1832f3ddd1e66fe948d5880c21a169 \\\n    --hash=sha256:598e3276b64aff0e7b3451b72e94fa3c238d452e7ddcd893c3ab324717456bad \\\n    --hash=sha256:5b7b716f97b52c5a14bffdf688f971b2d5ef4029127f1ad7a513973cfd818df2 \\\n    --hash=sha256:5dedb4db619ba5a2787a94d877bc8ffc0566f92a01c0ef214865e54ecc9ee5e0 \\\n    --hash=sha256:619bc166c4f2de5caa5a633b8b7326fbe98e0ccbfacabd87268a2b15ff73a029 \\\n    --hash=sha256:629ddd2ca402ae6dbedfceeba9c46d5f7b2a61d9749597d4307f943ef198fc1f \\\n    --hash=sha256:656f7526c69fac7f600bd1f400991cc282b417d17539a1b228617081106feb4a \\\n    --hash=sha256:6ec585f69cec0aa07d945b20805be741395e28ac1627333b1c5b0105962ffced \\\n    --hash=sha256:72b6be590cc35924b02c78ef34b467da4ba07e4e0f0454a2c5907f473fc50ce5 \\\n    --hash=sha256:7502934a33b54030eaf1194c21c692a534196063db72176b0c4028e140f8f32c \\\n    --hash=sha256:7a68b554d356a91cce1236aa7682dc01df0edba8d043fd1ce607c49dd3c1edcf \\\n    --hash=sha256:7b2e5a267c855eea6b4283940daa6e88a285f5f2a67f2220203786dfa59b37e9 \\\n    --hash=sha256:823b65d8706e32ad2df51ed89496147a42a2a6e01c13cfb6ffb8b1e92bc910bb \\\n    --hash=sha256:8590b4ae07a35970728874632fed7bd57b26b0102df2d2b233b6d9d82f6c62ad \\\n    --hash=sha256:8dd717634f5a044f860435c1d8c16a270ddf0ef8588d4887037c5028b859b0c3 \\\n    --hash=sha256:8dec4936e9c3100156f8a2dc89c4b88d5c435175ff03413b443469c7c8c5f4d1 \\\n    --hash=sha256:97cafb1f3cbcd3fd2b6fbfb99ae11cdb14deea0736fc2b0952ee177f2b813a46 \\\n    --hash=sha256:a17a92de5231666cfbe003f0e4b9b3a7ae3afb1ec2845aadc2bacc93ff85febc \\\n    --hash=sha256:a549b9c31bec33820e885335b451286e2969a2d9e24879f83fe904a5ce59d70a \\\n    --hash=sha256:ac07bad82163452a6884fe8fa0963fb98c2346ba78d779ec06bd7a6262132aee \\\n    --hash=sha256:ae2ad8ae6ebee9d2d94b17fb62763125f3f374c25618198f40cbb8b525411900 \\\n    --hash=sha256:b91c037585eba9095565a3556f611e3cbfaa42ca1e865f7b8015fe5c7336d5a5 \\\n    --hash=sha256:bc1667f8b83f48511b94671e0e441401371dfd0f0a795c7daa4a3cd1dde55bea \\\n    --hash=sha256:bec0a414d016ac1a18862a519e54b2fd0fc8bbfd6890376898a6c0891dd82e9f \\\n    --hash=sha256:bf50cd79a75d181c9181df03572cdce0fbb75cc353bc350712073108cba98de5 \\\n    --hash=sha256:bff1b4290a66b490a2f4719358c0cdcd9bafb6b8f061e45c7a2460866bf50c2e \\\n    --hash=sha256:c061bb86a71b42465156a3ee7bd58c8c2ceacdbeb95d05a99893e08b8467359a \\\n    --hash=sha256:c8b29db45f8fe46ad280a7294f5c3ec36dbac9491f2d1c17345be8e69cc5928f \\\n    --hash=sha256:ce409136744f6521e39fd8e2a24c53fa18ad67aa5bc7c2cf83645cce5b5c4e50 \\\n    --hash=sha256:d050b3361367a06d752db6ead6e7edeb0009be66bc3bae0ee9d97fb326badc2a \\\n    --hash=sha256:d283d37a890ba4c1ae73ffadf8046435c76e7bc2247bbb63c00bd1a709c6544b \\\n    --hash=sha256:d9fad5155d72433c921b782e58892377c44bd6252b5af2f67f16b194987338a4 \\\n    --hash=sha256:daa4ee5a243f0f20d528d939d06670a298dd39b1ad5f8a72a4275124a7819eff \\\n    --hash=sha256:db0b55e0f3cc0be60c1f19efdde9a637c32740486004f20d1cff53c3c0ece4d2 \\\n    --hash=sha256:e61659ba32cf2cf1481e575d0462554625196a1f2fc06a1c777d3f48e8865d46 \\\n    --hash=sha256:ea3d8a3d18833cf4304cd2fc9cbb1efe188ca9b5efef2bdac7adc20594a0e46b \\\n    --hash=sha256:ec6a563cff360b50eed26f13adc43e61bc0c04d94b8be985e6fb24b81f6dcfdf \\\n    --hash=sha256:f5dfb42c4604dddc8e4305050aa6deb084540643ed5804d7455b5df8fe16f5e5 \\\n    --hash=sha256:fa173ec60341d6bb97a89f5ea19c85c5643c1e7dedebc22f5181eb73573142c5 \\\n    --hash=sha256:fa9db3f79de01457b03d4f01b34cf91bc0048eb2c3846ff26f66687c2f6d16ab \\\n    --hash=sha256:fce659a462a1be54d2ffcacea5e3ba2d74daa74f30f5f143fe0c58636e355fdd \\\n    --hash=sha256:ffee1f21e5ef0d712f9033568f8344d5da8cc2869dbd08d87c84656e6a2d2f68\n    # via\n    #   -r requirements.in\n    #   jinja2\nmpmath==1.3.0 \\\n    --hash=sha256:7a28eb2a9774d00c7bc92411c19a89209d5da7c4c9a9e227be8330a23a25b91f \\\n    --hash=sha256:a0b2b9fe80bbcd81a6647ff13108738cfb482d481d826cc0e02f5b35e5c88d2c\n    # via\n    #   -r requirements.in\n    #   sympy\nnetworkx==3.3 \\\n    --hash=sha256:0c127d8b2f4865f59ae9cb8aafcd60b5c70f3241ebd66f7defad7c4ab90126c9 \\\n    --hash=sha256:28575580c6ebdaf4505b22c6256a2b9de86b316dc63ba9e93abde3d78dfdbcf2\n    # via -r requirements.in\npyyaml==6.0.1 \\\n    --hash=sha256:04ac92ad1925b2cff1db0cfebffb6ffc43457495c9b3c39d3fcae417d7125dc5 \\\n    --hash=sha256:062582fca9fabdd2c8b54a3ef1c978d786e0f6b3a1510e0ac93ef59e0ddae2bc \\\n    --hash=sha256:0d3304d8c0adc42be59c5f8a4d9e3d7379e6955ad754aa9d6ab7a398b59dd1df \\\n    --hash=sha256:1635fd110e8d85d55237ab316b5b011de701ea0f29d07611174a1b42f1444741 \\\n    --hash=sha256:184c5108a2aca3c5b3d3bf9395d50893a7ab82a38004c8f61c258d4428e80206 \\\n    --hash=sha256:18aeb1bf9a78867dc38b259769503436b7c72f7a1f1f4c93ff9a17de54319b27 \\\n    --hash=sha256:1d4c7e777c441b20e32f52bd377e0c409713e8bb1386e1099c2415f26e479595 \\\n    --hash=sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62 \\\n    --hash=sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98 \\\n    --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \\\n    --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \\\n    --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \\\n    --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \\\n    --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \\\n    --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \\\n    --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \\\n    --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \\\n    --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \\\n    --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \\\n    --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \\\n    --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \\\n    --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \\\n    --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \\\n    --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \\\n    --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \\\n    --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \\\n    --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \\\n    --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \\\n    --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \\\n    --hash=sha256:a08c6f0fe150303c1c6b71ebcd7213c2858041a7e01975da3a99aed1e7a378ef \\\n    --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \\\n    --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \\\n    --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \\\n    --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \\\n    --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \\\n    --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \\\n    --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \\\n    --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \\\n    --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \\\n    --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \\\n    --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \\\n    --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \\\n    --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \\\n    --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \\\n    --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \\\n    --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \\\n    --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \\\n    --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \\\n    --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \\\n    --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \\\n    --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f\n    # via -r requirements.in\nsympy==1.12 \\\n    --hash=sha256:c3588cd4295d0c0f603d0f2ae780587e64e2efeedb3521e46b9bb1d08d184fa5 \\\n    --hash=sha256:ebf595c8dac3e0fdc4152c51878b498396ec7f30e7a914d6071e674d49420fb8\n    # via -r requirements.in\ntyping-extensions==4.11.0 \\\n    --hash=sha256:83f085bd5ca59c80295fc2a82ab5dac679cbe02b9f33f7d83af68e241bea51b0 \\\n    --hash=sha256:c1f94d72897edaf4ce775bb7558d5b79d8126906a14ea5ed1635921406c0387a\n    # via -r requirements.in\n"
        },
        {
          "name": "requirements_lock_3_11.txt",
          "type": "blob",
          "size": 10.947265625,
          "content": "#\n# This file is autogenerated by pip-compile with Python 3.11\n# by the following command:\n#\n#    bazel run //:requirements.update\n#\nfilelock==3.14.0 \\\n    --hash=sha256:43339835842f110ca7ae60f1e1c160714c5a6afd15a2873419ab185334975c0f \\\n    --hash=sha256:6ea72da3be9b8c82afd3edcf99f2fffbb5076335a5ae4d03248bb5b6c3eae78a\n    # via -r requirements.in\nfsspec==2024.5.0 \\\n    --hash=sha256:1d021b0b0f933e3b3029ed808eb400c08ba101ca2de4b3483fbc9ca23fcee94a \\\n    --hash=sha256:e0fdbc446d67e182f49a70b82cf7889028a63588fde6b222521f10937b2b670c\n    # via -r requirements.in\njinja2==3.1.4 \\\n    --hash=sha256:4a3aee7acbbe7303aede8e9648d13b8bf88a429282aa6122a993f0ac800cb369 \\\n    --hash=sha256:bc5dd2abb727a5319567b7a813e6a2e7318c39f4f487cfe6c89c6f9c7d25197d\n    # via -r requirements.in\nmarkupsafe==2.1.5 \\\n    --hash=sha256:00e046b6dd71aa03a41079792f8473dc494d564611a8f89bbbd7cb93295ebdcf \\\n    --hash=sha256:075202fa5b72c86ad32dc7d0b56024ebdbcf2048c0ba09f1cde31bfdd57bcfff \\\n    --hash=sha256:0e397ac966fdf721b2c528cf028494e86172b4feba51d65f81ffd65c63798f3f \\\n    --hash=sha256:17b950fccb810b3293638215058e432159d2b71005c74371d784862b7e4683f3 \\\n    --hash=sha256:1f3fbcb7ef1f16e48246f704ab79d79da8a46891e2da03f8783a5b6fa41a9532 \\\n    --hash=sha256:2174c595a0d73a3080ca3257b40096db99799265e1c27cc5a610743acd86d62f \\\n    --hash=sha256:2b7c57a4dfc4f16f7142221afe5ba4e093e09e728ca65c51f5620c9aaeb9a617 \\\n    --hash=sha256:2d2d793e36e230fd32babe143b04cec8a8b3eb8a3122d2aceb4a371e6b09b8df \\\n    --hash=sha256:30b600cf0a7ac9234b2638fbc0fb6158ba5bdcdf46aeb631ead21248b9affbc4 \\\n    --hash=sha256:397081c1a0bfb5124355710fe79478cdbeb39626492b15d399526ae53422b906 \\\n    --hash=sha256:3a57fdd7ce31c7ff06cdfbf31dafa96cc533c21e443d57f5b1ecc6cdc668ec7f \\\n    --hash=sha256:3c6b973f22eb18a789b1460b4b91bf04ae3f0c4234a0a6aa6b0a92f6f7b951d4 \\\n    --hash=sha256:3e53af139f8579a6d5f7b76549125f0d94d7e630761a2111bc431fd820e163b8 \\\n    --hash=sha256:4096e9de5c6fdf43fb4f04c26fb114f61ef0bf2e5604b6ee3019d51b69e8c371 \\\n    --hash=sha256:4275d846e41ecefa46e2015117a9f491e57a71ddd59bbead77e904dc02b1bed2 \\\n    --hash=sha256:4c31f53cdae6ecfa91a77820e8b151dba54ab528ba65dfd235c80b086d68a465 \\\n    --hash=sha256:4f11aa001c540f62c6166c7726f71f7573b52c68c31f014c25cc7901deea0b52 \\\n    --hash=sha256:5049256f536511ee3f7e1b3f87d1d1209d327e818e6ae1365e8653d7e3abb6a6 \\\n    --hash=sha256:58c98fee265677f63a4385256a6d7683ab1832f3ddd1e66fe948d5880c21a169 \\\n    --hash=sha256:598e3276b64aff0e7b3451b72e94fa3c238d452e7ddcd893c3ab324717456bad \\\n    --hash=sha256:5b7b716f97b52c5a14bffdf688f971b2d5ef4029127f1ad7a513973cfd818df2 \\\n    --hash=sha256:5dedb4db619ba5a2787a94d877bc8ffc0566f92a01c0ef214865e54ecc9ee5e0 \\\n    --hash=sha256:619bc166c4f2de5caa5a633b8b7326fbe98e0ccbfacabd87268a2b15ff73a029 \\\n    --hash=sha256:629ddd2ca402ae6dbedfceeba9c46d5f7b2a61d9749597d4307f943ef198fc1f \\\n    --hash=sha256:656f7526c69fac7f600bd1f400991cc282b417d17539a1b228617081106feb4a \\\n    --hash=sha256:6ec585f69cec0aa07d945b20805be741395e28ac1627333b1c5b0105962ffced \\\n    --hash=sha256:72b6be590cc35924b02c78ef34b467da4ba07e4e0f0454a2c5907f473fc50ce5 \\\n    --hash=sha256:7502934a33b54030eaf1194c21c692a534196063db72176b0c4028e140f8f32c \\\n    --hash=sha256:7a68b554d356a91cce1236aa7682dc01df0edba8d043fd1ce607c49dd3c1edcf \\\n    --hash=sha256:7b2e5a267c855eea6b4283940daa6e88a285f5f2a67f2220203786dfa59b37e9 \\\n    --hash=sha256:823b65d8706e32ad2df51ed89496147a42a2a6e01c13cfb6ffb8b1e92bc910bb \\\n    --hash=sha256:8590b4ae07a35970728874632fed7bd57b26b0102df2d2b233b6d9d82f6c62ad \\\n    --hash=sha256:8dd717634f5a044f860435c1d8c16a270ddf0ef8588d4887037c5028b859b0c3 \\\n    --hash=sha256:8dec4936e9c3100156f8a2dc89c4b88d5c435175ff03413b443469c7c8c5f4d1 \\\n    --hash=sha256:97cafb1f3cbcd3fd2b6fbfb99ae11cdb14deea0736fc2b0952ee177f2b813a46 \\\n    --hash=sha256:a17a92de5231666cfbe003f0e4b9b3a7ae3afb1ec2845aadc2bacc93ff85febc \\\n    --hash=sha256:a549b9c31bec33820e885335b451286e2969a2d9e24879f83fe904a5ce59d70a \\\n    --hash=sha256:ac07bad82163452a6884fe8fa0963fb98c2346ba78d779ec06bd7a6262132aee \\\n    --hash=sha256:ae2ad8ae6ebee9d2d94b17fb62763125f3f374c25618198f40cbb8b525411900 \\\n    --hash=sha256:b91c037585eba9095565a3556f611e3cbfaa42ca1e865f7b8015fe5c7336d5a5 \\\n    --hash=sha256:bc1667f8b83f48511b94671e0e441401371dfd0f0a795c7daa4a3cd1dde55bea \\\n    --hash=sha256:bec0a414d016ac1a18862a519e54b2fd0fc8bbfd6890376898a6c0891dd82e9f \\\n    --hash=sha256:bf50cd79a75d181c9181df03572cdce0fbb75cc353bc350712073108cba98de5 \\\n    --hash=sha256:bff1b4290a66b490a2f4719358c0cdcd9bafb6b8f061e45c7a2460866bf50c2e \\\n    --hash=sha256:c061bb86a71b42465156a3ee7bd58c8c2ceacdbeb95d05a99893e08b8467359a \\\n    --hash=sha256:c8b29db45f8fe46ad280a7294f5c3ec36dbac9491f2d1c17345be8e69cc5928f \\\n    --hash=sha256:ce409136744f6521e39fd8e2a24c53fa18ad67aa5bc7c2cf83645cce5b5c4e50 \\\n    --hash=sha256:d050b3361367a06d752db6ead6e7edeb0009be66bc3bae0ee9d97fb326badc2a \\\n    --hash=sha256:d283d37a890ba4c1ae73ffadf8046435c76e7bc2247bbb63c00bd1a709c6544b \\\n    --hash=sha256:d9fad5155d72433c921b782e58892377c44bd6252b5af2f67f16b194987338a4 \\\n    --hash=sha256:daa4ee5a243f0f20d528d939d06670a298dd39b1ad5f8a72a4275124a7819eff \\\n    --hash=sha256:db0b55e0f3cc0be60c1f19efdde9a637c32740486004f20d1cff53c3c0ece4d2 \\\n    --hash=sha256:e61659ba32cf2cf1481e575d0462554625196a1f2fc06a1c777d3f48e8865d46 \\\n    --hash=sha256:ea3d8a3d18833cf4304cd2fc9cbb1efe188ca9b5efef2bdac7adc20594a0e46b \\\n    --hash=sha256:ec6a563cff360b50eed26f13adc43e61bc0c04d94b8be985e6fb24b81f6dcfdf \\\n    --hash=sha256:f5dfb42c4604dddc8e4305050aa6deb084540643ed5804d7455b5df8fe16f5e5 \\\n    --hash=sha256:fa173ec60341d6bb97a89f5ea19c85c5643c1e7dedebc22f5181eb73573142c5 \\\n    --hash=sha256:fa9db3f79de01457b03d4f01b34cf91bc0048eb2c3846ff26f66687c2f6d16ab \\\n    --hash=sha256:fce659a462a1be54d2ffcacea5e3ba2d74daa74f30f5f143fe0c58636e355fdd \\\n    --hash=sha256:ffee1f21e5ef0d712f9033568f8344d5da8cc2869dbd08d87c84656e6a2d2f68\n    # via\n    #   -r requirements.in\n    #   jinja2\nmpmath==1.3.0 \\\n    --hash=sha256:7a28eb2a9774d00c7bc92411c19a89209d5da7c4c9a9e227be8330a23a25b91f \\\n    --hash=sha256:a0b2b9fe80bbcd81a6647ff13108738cfb482d481d826cc0e02f5b35e5c88d2c\n    # via\n    #   -r requirements.in\n    #   sympy\nnetworkx==3.3 \\\n    --hash=sha256:0c127d8b2f4865f59ae9cb8aafcd60b5c70f3241ebd66f7defad7c4ab90126c9 \\\n    --hash=sha256:28575580c6ebdaf4505b22c6256a2b9de86b316dc63ba9e93abde3d78dfdbcf2\n    # via -r requirements.in\npyyaml==6.0.1 \\\n    --hash=sha256:04ac92ad1925b2cff1db0cfebffb6ffc43457495c9b3c39d3fcae417d7125dc5 \\\n    --hash=sha256:062582fca9fabdd2c8b54a3ef1c978d786e0f6b3a1510e0ac93ef59e0ddae2bc \\\n    --hash=sha256:0d3304d8c0adc42be59c5f8a4d9e3d7379e6955ad754aa9d6ab7a398b59dd1df \\\n    --hash=sha256:1635fd110e8d85d55237ab316b5b011de701ea0f29d07611174a1b42f1444741 \\\n    --hash=sha256:184c5108a2aca3c5b3d3bf9395d50893a7ab82a38004c8f61c258d4428e80206 \\\n    --hash=sha256:18aeb1bf9a78867dc38b259769503436b7c72f7a1f1f4c93ff9a17de54319b27 \\\n    --hash=sha256:1d4c7e777c441b20e32f52bd377e0c409713e8bb1386e1099c2415f26e479595 \\\n    --hash=sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62 \\\n    --hash=sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98 \\\n    --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \\\n    --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \\\n    --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \\\n    --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \\\n    --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \\\n    --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \\\n    --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \\\n    --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \\\n    --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \\\n    --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \\\n    --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \\\n    --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \\\n    --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \\\n    --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \\\n    --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \\\n    --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \\\n    --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \\\n    --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \\\n    --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \\\n    --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \\\n    --hash=sha256:a08c6f0fe150303c1c6b71ebcd7213c2858041a7e01975da3a99aed1e7a378ef \\\n    --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \\\n    --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \\\n    --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \\\n    --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \\\n    --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \\\n    --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \\\n    --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \\\n    --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \\\n    --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \\\n    --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \\\n    --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \\\n    --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \\\n    --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \\\n    --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \\\n    --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \\\n    --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \\\n    --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \\\n    --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \\\n    --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \\\n    --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \\\n    --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f\n    # via -r requirements.in\nsympy==1.12 \\\n    --hash=sha256:c3588cd4295d0c0f603d0f2ae780587e64e2efeedb3521e46b9bb1d08d184fa5 \\\n    --hash=sha256:ebf595c8dac3e0fdc4152c51878b498396ec7f30e7a914d6071e674d49420fb8\n    # via -r requirements.in\ntyping-extensions==4.11.0 \\\n    --hash=sha256:83f085bd5ca59c80295fc2a82ab5dac679cbe02b9f33f7d83af68e241bea51b0 \\\n    --hash=sha256:c1f94d72897edaf4ce775bb7558d5b79d8126906a14ea5ed1635921406c0387a\n    # via -r requirements.in\n"
        },
        {
          "name": "requirements_lock_3_8.txt",
          "type": "blob",
          "size": 10.9462890625,
          "content": "#\n# This file is autogenerated by pip-compile with Python 3.8\n# by the following command:\n#\n#    bazel run //:requirements.update\n#\nfilelock==3.14.0 \\\n    --hash=sha256:43339835842f110ca7ae60f1e1c160714c5a6afd15a2873419ab185334975c0f \\\n    --hash=sha256:6ea72da3be9b8c82afd3edcf99f2fffbb5076335a5ae4d03248bb5b6c3eae78a\n    # via -r requirements.in\nfsspec==2024.5.0 \\\n    --hash=sha256:1d021b0b0f933e3b3029ed808eb400c08ba101ca2de4b3483fbc9ca23fcee94a \\\n    --hash=sha256:e0fdbc446d67e182f49a70b82cf7889028a63588fde6b222521f10937b2b670c\n    # via -r requirements.in\njinja2==3.1.4 \\\n    --hash=sha256:4a3aee7acbbe7303aede8e9648d13b8bf88a429282aa6122a993f0ac800cb369 \\\n    --hash=sha256:bc5dd2abb727a5319567b7a813e6a2e7318c39f4f487cfe6c89c6f9c7d25197d\n    # via -r requirements.in\nmarkupsafe==2.1.5 \\\n    --hash=sha256:00e046b6dd71aa03a41079792f8473dc494d564611a8f89bbbd7cb93295ebdcf \\\n    --hash=sha256:075202fa5b72c86ad32dc7d0b56024ebdbcf2048c0ba09f1cde31bfdd57bcfff \\\n    --hash=sha256:0e397ac966fdf721b2c528cf028494e86172b4feba51d65f81ffd65c63798f3f \\\n    --hash=sha256:17b950fccb810b3293638215058e432159d2b71005c74371d784862b7e4683f3 \\\n    --hash=sha256:1f3fbcb7ef1f16e48246f704ab79d79da8a46891e2da03f8783a5b6fa41a9532 \\\n    --hash=sha256:2174c595a0d73a3080ca3257b40096db99799265e1c27cc5a610743acd86d62f \\\n    --hash=sha256:2b7c57a4dfc4f16f7142221afe5ba4e093e09e728ca65c51f5620c9aaeb9a617 \\\n    --hash=sha256:2d2d793e36e230fd32babe143b04cec8a8b3eb8a3122d2aceb4a371e6b09b8df \\\n    --hash=sha256:30b600cf0a7ac9234b2638fbc0fb6158ba5bdcdf46aeb631ead21248b9affbc4 \\\n    --hash=sha256:397081c1a0bfb5124355710fe79478cdbeb39626492b15d399526ae53422b906 \\\n    --hash=sha256:3a57fdd7ce31c7ff06cdfbf31dafa96cc533c21e443d57f5b1ecc6cdc668ec7f \\\n    --hash=sha256:3c6b973f22eb18a789b1460b4b91bf04ae3f0c4234a0a6aa6b0a92f6f7b951d4 \\\n    --hash=sha256:3e53af139f8579a6d5f7b76549125f0d94d7e630761a2111bc431fd820e163b8 \\\n    --hash=sha256:4096e9de5c6fdf43fb4f04c26fb114f61ef0bf2e5604b6ee3019d51b69e8c371 \\\n    --hash=sha256:4275d846e41ecefa46e2015117a9f491e57a71ddd59bbead77e904dc02b1bed2 \\\n    --hash=sha256:4c31f53cdae6ecfa91a77820e8b151dba54ab528ba65dfd235c80b086d68a465 \\\n    --hash=sha256:4f11aa001c540f62c6166c7726f71f7573b52c68c31f014c25cc7901deea0b52 \\\n    --hash=sha256:5049256f536511ee3f7e1b3f87d1d1209d327e818e6ae1365e8653d7e3abb6a6 \\\n    --hash=sha256:58c98fee265677f63a4385256a6d7683ab1832f3ddd1e66fe948d5880c21a169 \\\n    --hash=sha256:598e3276b64aff0e7b3451b72e94fa3c238d452e7ddcd893c3ab324717456bad \\\n    --hash=sha256:5b7b716f97b52c5a14bffdf688f971b2d5ef4029127f1ad7a513973cfd818df2 \\\n    --hash=sha256:5dedb4db619ba5a2787a94d877bc8ffc0566f92a01c0ef214865e54ecc9ee5e0 \\\n    --hash=sha256:619bc166c4f2de5caa5a633b8b7326fbe98e0ccbfacabd87268a2b15ff73a029 \\\n    --hash=sha256:629ddd2ca402ae6dbedfceeba9c46d5f7b2a61d9749597d4307f943ef198fc1f \\\n    --hash=sha256:656f7526c69fac7f600bd1f400991cc282b417d17539a1b228617081106feb4a \\\n    --hash=sha256:6ec585f69cec0aa07d945b20805be741395e28ac1627333b1c5b0105962ffced \\\n    --hash=sha256:72b6be590cc35924b02c78ef34b467da4ba07e4e0f0454a2c5907f473fc50ce5 \\\n    --hash=sha256:7502934a33b54030eaf1194c21c692a534196063db72176b0c4028e140f8f32c \\\n    --hash=sha256:7a68b554d356a91cce1236aa7682dc01df0edba8d043fd1ce607c49dd3c1edcf \\\n    --hash=sha256:7b2e5a267c855eea6b4283940daa6e88a285f5f2a67f2220203786dfa59b37e9 \\\n    --hash=sha256:823b65d8706e32ad2df51ed89496147a42a2a6e01c13cfb6ffb8b1e92bc910bb \\\n    --hash=sha256:8590b4ae07a35970728874632fed7bd57b26b0102df2d2b233b6d9d82f6c62ad \\\n    --hash=sha256:8dd717634f5a044f860435c1d8c16a270ddf0ef8588d4887037c5028b859b0c3 \\\n    --hash=sha256:8dec4936e9c3100156f8a2dc89c4b88d5c435175ff03413b443469c7c8c5f4d1 \\\n    --hash=sha256:97cafb1f3cbcd3fd2b6fbfb99ae11cdb14deea0736fc2b0952ee177f2b813a46 \\\n    --hash=sha256:a17a92de5231666cfbe003f0e4b9b3a7ae3afb1ec2845aadc2bacc93ff85febc \\\n    --hash=sha256:a549b9c31bec33820e885335b451286e2969a2d9e24879f83fe904a5ce59d70a \\\n    --hash=sha256:ac07bad82163452a6884fe8fa0963fb98c2346ba78d779ec06bd7a6262132aee \\\n    --hash=sha256:ae2ad8ae6ebee9d2d94b17fb62763125f3f374c25618198f40cbb8b525411900 \\\n    --hash=sha256:b91c037585eba9095565a3556f611e3cbfaa42ca1e865f7b8015fe5c7336d5a5 \\\n    --hash=sha256:bc1667f8b83f48511b94671e0e441401371dfd0f0a795c7daa4a3cd1dde55bea \\\n    --hash=sha256:bec0a414d016ac1a18862a519e54b2fd0fc8bbfd6890376898a6c0891dd82e9f \\\n    --hash=sha256:bf50cd79a75d181c9181df03572cdce0fbb75cc353bc350712073108cba98de5 \\\n    --hash=sha256:bff1b4290a66b490a2f4719358c0cdcd9bafb6b8f061e45c7a2460866bf50c2e \\\n    --hash=sha256:c061bb86a71b42465156a3ee7bd58c8c2ceacdbeb95d05a99893e08b8467359a \\\n    --hash=sha256:c8b29db45f8fe46ad280a7294f5c3ec36dbac9491f2d1c17345be8e69cc5928f \\\n    --hash=sha256:ce409136744f6521e39fd8e2a24c53fa18ad67aa5bc7c2cf83645cce5b5c4e50 \\\n    --hash=sha256:d050b3361367a06d752db6ead6e7edeb0009be66bc3bae0ee9d97fb326badc2a \\\n    --hash=sha256:d283d37a890ba4c1ae73ffadf8046435c76e7bc2247bbb63c00bd1a709c6544b \\\n    --hash=sha256:d9fad5155d72433c921b782e58892377c44bd6252b5af2f67f16b194987338a4 \\\n    --hash=sha256:daa4ee5a243f0f20d528d939d06670a298dd39b1ad5f8a72a4275124a7819eff \\\n    --hash=sha256:db0b55e0f3cc0be60c1f19efdde9a637c32740486004f20d1cff53c3c0ece4d2 \\\n    --hash=sha256:e61659ba32cf2cf1481e575d0462554625196a1f2fc06a1c777d3f48e8865d46 \\\n    --hash=sha256:ea3d8a3d18833cf4304cd2fc9cbb1efe188ca9b5efef2bdac7adc20594a0e46b \\\n    --hash=sha256:ec6a563cff360b50eed26f13adc43e61bc0c04d94b8be985e6fb24b81f6dcfdf \\\n    --hash=sha256:f5dfb42c4604dddc8e4305050aa6deb084540643ed5804d7455b5df8fe16f5e5 \\\n    --hash=sha256:fa173ec60341d6bb97a89f5ea19c85c5643c1e7dedebc22f5181eb73573142c5 \\\n    --hash=sha256:fa9db3f79de01457b03d4f01b34cf91bc0048eb2c3846ff26f66687c2f6d16ab \\\n    --hash=sha256:fce659a462a1be54d2ffcacea5e3ba2d74daa74f30f5f143fe0c58636e355fdd \\\n    --hash=sha256:ffee1f21e5ef0d712f9033568f8344d5da8cc2869dbd08d87c84656e6a2d2f68\n    # via\n    #   -r requirements.in\n    #   jinja2\nmpmath==1.3.0 \\\n    --hash=sha256:7a28eb2a9774d00c7bc92411c19a89209d5da7c4c9a9e227be8330a23a25b91f \\\n    --hash=sha256:a0b2b9fe80bbcd81a6647ff13108738cfb482d481d826cc0e02f5b35e5c88d2c\n    # via\n    #   -r requirements.in\n    #   sympy\nnetworkx==3.1 \\\n    --hash=sha256:4f33f68cb2afcf86f28a45f43efc27a9386b535d567d2127f8f61d51dec58d36 \\\n    --hash=sha256:de346335408f84de0eada6ff9fafafff9bcda11f0a0dfaa931133debb146ab61\n    # via -r requirements.in\npyyaml==6.0.1 \\\n    --hash=sha256:04ac92ad1925b2cff1db0cfebffb6ffc43457495c9b3c39d3fcae417d7125dc5 \\\n    --hash=sha256:062582fca9fabdd2c8b54a3ef1c978d786e0f6b3a1510e0ac93ef59e0ddae2bc \\\n    --hash=sha256:0d3304d8c0adc42be59c5f8a4d9e3d7379e6955ad754aa9d6ab7a398b59dd1df \\\n    --hash=sha256:1635fd110e8d85d55237ab316b5b011de701ea0f29d07611174a1b42f1444741 \\\n    --hash=sha256:184c5108a2aca3c5b3d3bf9395d50893a7ab82a38004c8f61c258d4428e80206 \\\n    --hash=sha256:18aeb1bf9a78867dc38b259769503436b7c72f7a1f1f4c93ff9a17de54319b27 \\\n    --hash=sha256:1d4c7e777c441b20e32f52bd377e0c409713e8bb1386e1099c2415f26e479595 \\\n    --hash=sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62 \\\n    --hash=sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98 \\\n    --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \\\n    --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \\\n    --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \\\n    --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \\\n    --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \\\n    --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \\\n    --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \\\n    --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \\\n    --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \\\n    --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \\\n    --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \\\n    --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \\\n    --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \\\n    --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \\\n    --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \\\n    --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \\\n    --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \\\n    --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \\\n    --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \\\n    --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \\\n    --hash=sha256:a08c6f0fe150303c1c6b71ebcd7213c2858041a7e01975da3a99aed1e7a378ef \\\n    --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \\\n    --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \\\n    --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \\\n    --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \\\n    --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \\\n    --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \\\n    --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \\\n    --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \\\n    --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \\\n    --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \\\n    --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \\\n    --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \\\n    --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \\\n    --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \\\n    --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \\\n    --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \\\n    --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \\\n    --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \\\n    --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \\\n    --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \\\n    --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f\n    # via -r requirements.in\nsympy==1.12 \\\n    --hash=sha256:c3588cd4295d0c0f603d0f2ae780587e64e2efeedb3521e46b9bb1d08d184fa5 \\\n    --hash=sha256:ebf595c8dac3e0fdc4152c51878b498396ec7f30e7a914d6071e674d49420fb8\n    # via -r requirements.in\ntyping-extensions==4.11.0 \\\n    --hash=sha256:83f085bd5ca59c80295fc2a82ab5dac679cbe02b9f33f7d83af68e241bea51b0 \\\n    --hash=sha256:c1f94d72897edaf4ce775bb7558d5b79d8126906a14ea5ed1635921406c0387a\n    # via -r requirements.in\n"
        },
        {
          "name": "requirements_lock_3_9.txt",
          "type": "blob",
          "size": 10.9482421875,
          "content": "#\n# This file is autogenerated by pip-compile with Python 3.9\n# by the following command:\n#\n#    bazel run //:requirements.update\n#\nfilelock==3.14.0 \\\n    --hash=sha256:43339835842f110ca7ae60f1e1c160714c5a6afd15a2873419ab185334975c0f \\\n    --hash=sha256:6ea72da3be9b8c82afd3edcf99f2fffbb5076335a5ae4d03248bb5b6c3eae78a\n    # via -r requirements.in\nfsspec==2024.5.0 \\\n    --hash=sha256:1d021b0b0f933e3b3029ed808eb400c08ba101ca2de4b3483fbc9ca23fcee94a \\\n    --hash=sha256:e0fdbc446d67e182f49a70b82cf7889028a63588fde6b222521f10937b2b670c\n    # via -r requirements.in\njinja2==3.1.4 \\\n    --hash=sha256:4a3aee7acbbe7303aede8e9648d13b8bf88a429282aa6122a993f0ac800cb369 \\\n    --hash=sha256:bc5dd2abb727a5319567b7a813e6a2e7318c39f4f487cfe6c89c6f9c7d25197d\n    # via -r requirements.in\nmarkupsafe==2.1.5 \\\n    --hash=sha256:00e046b6dd71aa03a41079792f8473dc494d564611a8f89bbbd7cb93295ebdcf \\\n    --hash=sha256:075202fa5b72c86ad32dc7d0b56024ebdbcf2048c0ba09f1cde31bfdd57bcfff \\\n    --hash=sha256:0e397ac966fdf721b2c528cf028494e86172b4feba51d65f81ffd65c63798f3f \\\n    --hash=sha256:17b950fccb810b3293638215058e432159d2b71005c74371d784862b7e4683f3 \\\n    --hash=sha256:1f3fbcb7ef1f16e48246f704ab79d79da8a46891e2da03f8783a5b6fa41a9532 \\\n    --hash=sha256:2174c595a0d73a3080ca3257b40096db99799265e1c27cc5a610743acd86d62f \\\n    --hash=sha256:2b7c57a4dfc4f16f7142221afe5ba4e093e09e728ca65c51f5620c9aaeb9a617 \\\n    --hash=sha256:2d2d793e36e230fd32babe143b04cec8a8b3eb8a3122d2aceb4a371e6b09b8df \\\n    --hash=sha256:30b600cf0a7ac9234b2638fbc0fb6158ba5bdcdf46aeb631ead21248b9affbc4 \\\n    --hash=sha256:397081c1a0bfb5124355710fe79478cdbeb39626492b15d399526ae53422b906 \\\n    --hash=sha256:3a57fdd7ce31c7ff06cdfbf31dafa96cc533c21e443d57f5b1ecc6cdc668ec7f \\\n    --hash=sha256:3c6b973f22eb18a789b1460b4b91bf04ae3f0c4234a0a6aa6b0a92f6f7b951d4 \\\n    --hash=sha256:3e53af139f8579a6d5f7b76549125f0d94d7e630761a2111bc431fd820e163b8 \\\n    --hash=sha256:4096e9de5c6fdf43fb4f04c26fb114f61ef0bf2e5604b6ee3019d51b69e8c371 \\\n    --hash=sha256:4275d846e41ecefa46e2015117a9f491e57a71ddd59bbead77e904dc02b1bed2 \\\n    --hash=sha256:4c31f53cdae6ecfa91a77820e8b151dba54ab528ba65dfd235c80b086d68a465 \\\n    --hash=sha256:4f11aa001c540f62c6166c7726f71f7573b52c68c31f014c25cc7901deea0b52 \\\n    --hash=sha256:5049256f536511ee3f7e1b3f87d1d1209d327e818e6ae1365e8653d7e3abb6a6 \\\n    --hash=sha256:58c98fee265677f63a4385256a6d7683ab1832f3ddd1e66fe948d5880c21a169 \\\n    --hash=sha256:598e3276b64aff0e7b3451b72e94fa3c238d452e7ddcd893c3ab324717456bad \\\n    --hash=sha256:5b7b716f97b52c5a14bffdf688f971b2d5ef4029127f1ad7a513973cfd818df2 \\\n    --hash=sha256:5dedb4db619ba5a2787a94d877bc8ffc0566f92a01c0ef214865e54ecc9ee5e0 \\\n    --hash=sha256:619bc166c4f2de5caa5a633b8b7326fbe98e0ccbfacabd87268a2b15ff73a029 \\\n    --hash=sha256:629ddd2ca402ae6dbedfceeba9c46d5f7b2a61d9749597d4307f943ef198fc1f \\\n    --hash=sha256:656f7526c69fac7f600bd1f400991cc282b417d17539a1b228617081106feb4a \\\n    --hash=sha256:6ec585f69cec0aa07d945b20805be741395e28ac1627333b1c5b0105962ffced \\\n    --hash=sha256:72b6be590cc35924b02c78ef34b467da4ba07e4e0f0454a2c5907f473fc50ce5 \\\n    --hash=sha256:7502934a33b54030eaf1194c21c692a534196063db72176b0c4028e140f8f32c \\\n    --hash=sha256:7a68b554d356a91cce1236aa7682dc01df0edba8d043fd1ce607c49dd3c1edcf \\\n    --hash=sha256:7b2e5a267c855eea6b4283940daa6e88a285f5f2a67f2220203786dfa59b37e9 \\\n    --hash=sha256:823b65d8706e32ad2df51ed89496147a42a2a6e01c13cfb6ffb8b1e92bc910bb \\\n    --hash=sha256:8590b4ae07a35970728874632fed7bd57b26b0102df2d2b233b6d9d82f6c62ad \\\n    --hash=sha256:8dd717634f5a044f860435c1d8c16a270ddf0ef8588d4887037c5028b859b0c3 \\\n    --hash=sha256:8dec4936e9c3100156f8a2dc89c4b88d5c435175ff03413b443469c7c8c5f4d1 \\\n    --hash=sha256:97cafb1f3cbcd3fd2b6fbfb99ae11cdb14deea0736fc2b0952ee177f2b813a46 \\\n    --hash=sha256:a17a92de5231666cfbe003f0e4b9b3a7ae3afb1ec2845aadc2bacc93ff85febc \\\n    --hash=sha256:a549b9c31bec33820e885335b451286e2969a2d9e24879f83fe904a5ce59d70a \\\n    --hash=sha256:ac07bad82163452a6884fe8fa0963fb98c2346ba78d779ec06bd7a6262132aee \\\n    --hash=sha256:ae2ad8ae6ebee9d2d94b17fb62763125f3f374c25618198f40cbb8b525411900 \\\n    --hash=sha256:b91c037585eba9095565a3556f611e3cbfaa42ca1e865f7b8015fe5c7336d5a5 \\\n    --hash=sha256:bc1667f8b83f48511b94671e0e441401371dfd0f0a795c7daa4a3cd1dde55bea \\\n    --hash=sha256:bec0a414d016ac1a18862a519e54b2fd0fc8bbfd6890376898a6c0891dd82e9f \\\n    --hash=sha256:bf50cd79a75d181c9181df03572cdce0fbb75cc353bc350712073108cba98de5 \\\n    --hash=sha256:bff1b4290a66b490a2f4719358c0cdcd9bafb6b8f061e45c7a2460866bf50c2e \\\n    --hash=sha256:c061bb86a71b42465156a3ee7bd58c8c2ceacdbeb95d05a99893e08b8467359a \\\n    --hash=sha256:c8b29db45f8fe46ad280a7294f5c3ec36dbac9491f2d1c17345be8e69cc5928f \\\n    --hash=sha256:ce409136744f6521e39fd8e2a24c53fa18ad67aa5bc7c2cf83645cce5b5c4e50 \\\n    --hash=sha256:d050b3361367a06d752db6ead6e7edeb0009be66bc3bae0ee9d97fb326badc2a \\\n    --hash=sha256:d283d37a890ba4c1ae73ffadf8046435c76e7bc2247bbb63c00bd1a709c6544b \\\n    --hash=sha256:d9fad5155d72433c921b782e58892377c44bd6252b5af2f67f16b194987338a4 \\\n    --hash=sha256:daa4ee5a243f0f20d528d939d06670a298dd39b1ad5f8a72a4275124a7819eff \\\n    --hash=sha256:db0b55e0f3cc0be60c1f19efdde9a637c32740486004f20d1cff53c3c0ece4d2 \\\n    --hash=sha256:e61659ba32cf2cf1481e575d0462554625196a1f2fc06a1c777d3f48e8865d46 \\\n    --hash=sha256:ea3d8a3d18833cf4304cd2fc9cbb1efe188ca9b5efef2bdac7adc20594a0e46b \\\n    --hash=sha256:ec6a563cff360b50eed26f13adc43e61bc0c04d94b8be985e6fb24b81f6dcfdf \\\n    --hash=sha256:f5dfb42c4604dddc8e4305050aa6deb084540643ed5804d7455b5df8fe16f5e5 \\\n    --hash=sha256:fa173ec60341d6bb97a89f5ea19c85c5643c1e7dedebc22f5181eb73573142c5 \\\n    --hash=sha256:fa9db3f79de01457b03d4f01b34cf91bc0048eb2c3846ff26f66687c2f6d16ab \\\n    --hash=sha256:fce659a462a1be54d2ffcacea5e3ba2d74daa74f30f5f143fe0c58636e355fdd \\\n    --hash=sha256:ffee1f21e5ef0d712f9033568f8344d5da8cc2869dbd08d87c84656e6a2d2f68\n    # via\n    #   -r requirements.in\n    #   jinja2\nmpmath==1.3.0 \\\n    --hash=sha256:7a28eb2a9774d00c7bc92411c19a89209d5da7c4c9a9e227be8330a23a25b91f \\\n    --hash=sha256:a0b2b9fe80bbcd81a6647ff13108738cfb482d481d826cc0e02f5b35e5c88d2c\n    # via\n    #   -r requirements.in\n    #   sympy\nnetworkx==3.2.1 \\\n    --hash=sha256:9f1bb5cf3409bf324e0a722c20bdb4c20ee39bf1c30ce8ae499c8502b0b5e0c6 \\\n    --hash=sha256:f18c69adc97877c42332c170849c96cefa91881c99a7cb3e95b7c659ebdc1ec2\n    # via -r requirements.in\npyyaml==6.0.1 \\\n    --hash=sha256:04ac92ad1925b2cff1db0cfebffb6ffc43457495c9b3c39d3fcae417d7125dc5 \\\n    --hash=sha256:062582fca9fabdd2c8b54a3ef1c978d786e0f6b3a1510e0ac93ef59e0ddae2bc \\\n    --hash=sha256:0d3304d8c0adc42be59c5f8a4d9e3d7379e6955ad754aa9d6ab7a398b59dd1df \\\n    --hash=sha256:1635fd110e8d85d55237ab316b5b011de701ea0f29d07611174a1b42f1444741 \\\n    --hash=sha256:184c5108a2aca3c5b3d3bf9395d50893a7ab82a38004c8f61c258d4428e80206 \\\n    --hash=sha256:18aeb1bf9a78867dc38b259769503436b7c72f7a1f1f4c93ff9a17de54319b27 \\\n    --hash=sha256:1d4c7e777c441b20e32f52bd377e0c409713e8bb1386e1099c2415f26e479595 \\\n    --hash=sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62 \\\n    --hash=sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98 \\\n    --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \\\n    --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \\\n    --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \\\n    --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \\\n    --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \\\n    --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \\\n    --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \\\n    --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \\\n    --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \\\n    --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \\\n    --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \\\n    --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \\\n    --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \\\n    --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \\\n    --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \\\n    --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \\\n    --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \\\n    --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \\\n    --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \\\n    --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \\\n    --hash=sha256:a08c6f0fe150303c1c6b71ebcd7213c2858041a7e01975da3a99aed1e7a378ef \\\n    --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \\\n    --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \\\n    --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \\\n    --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \\\n    --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \\\n    --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \\\n    --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \\\n    --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \\\n    --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \\\n    --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \\\n    --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \\\n    --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \\\n    --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \\\n    --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \\\n    --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \\\n    --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \\\n    --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \\\n    --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \\\n    --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \\\n    --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \\\n    --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f\n    # via -r requirements.in\nsympy==1.12 \\\n    --hash=sha256:c3588cd4295d0c0f603d0f2ae780587e64e2efeedb3521e46b9bb1d08d184fa5 \\\n    --hash=sha256:ebf595c8dac3e0fdc4152c51878b498396ec7f30e7a914d6071e674d49420fb8\n    # via -r requirements.in\ntyping-extensions==4.11.0 \\\n    --hash=sha256:83f085bd5ca59c80295fc2a82ab5dac679cbe02b9f33f7d83af68e241bea51b0 \\\n    --hash=sha256:c1f94d72897edaf4ce775bb7558d5b79d8126906a14ea5ed1635921406c0387a\n    # via -r requirements.in\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 11.0966796875,
          "content": "#!/usr/bin/env python\n# Welcome to the PyTorch/XLA setup.py.\n#\n# Environment variables you are probably interested in:\n#\n#   DEBUG\n#     build with debug symbols\n#\n#   TORCH_XLA_VERSION\n#     specify the version of PyTorch/XLA, rather than the hard-coded version\n#     in this file; used when we're building binaries for distribution\n#\n#   GIT_VERSIONED_XLA_BUILD\n#     creates a git versioned build\n#\n#   TORCH_XLA_PACKAGE_NAME\n#     change the package name to something other than 'torch_xla'\n#\n#   BAZEL_VERBOSE=0\n#     turn on verbose messages during the bazel build of the xla/xrt client\n#\n#   XLA_CUDA=0\n#     build the xla/xrt client with CUDA enabled\n#\n#   XLA_CPU_USE_ACL=0\n#     whether to use ACL\n#\n#   BUNDLE_LIBTPU=0\n#     include libtpu in final wheel\n\n#   BUILD_CPP_TESTS=0\n#     build the C++ tests\n#\n#   GCLOUD_SERVICE_KEY_FILE=''\n#     file containing the auth tokens for remote cache/build. implies remote cache.\n#\n#   BAZEL_REMOTE_CACHE=\"\"\n#     whether to use remote cache for builds\n#\n#   TPUVM_MODE=0\n#     whether to build for TPU\n#\n#   SILO_NAME=\"\"\n#     name of the remote build cache silo\n#\n#   CXX_ABI=\"\"\n#     value for cxx_abi flag; if empty, it is inferred from `torch._C`.\n#\nfrom setuptools import setup, find_packages, distutils, Extension, command\nfrom setuptools.command import develop, build_ext\nimport posixpath\nimport contextlib\nimport distutils.ccompiler\nimport distutils.command.clean\nimport os\nimport requests\nimport shutil\nimport subprocess\nimport sys\nimport tempfile\nimport zipfile\n\nimport build_util\n\nbase_dir = os.path.dirname(os.path.abspath(__file__))\n\nUSE_NIGHTLY = True  # whether to use nightly or stable libtpu and jax\n_date = '20250106'\n_libtpu_version = f'0.0.8'\n_jax_version = f'0.4.39'\n_jaxlib_version = f'0.4.39'\n_libtpu_wheel_name = f'libtpu-{_libtpu_version}'\n\nif USE_NIGHTLY:\n  _libtpu_version += f\".dev{_date}\"\n  _jax_version += f\".dev{_date}\"\n  _jaxlib_version += f\".dev{_date}\"\n  _libtpu_wheel_name += f\".dev{_date}+nightly\"\n\n_libtpu_storage_path = f'https://storage.googleapis.com/libtpu-nightly-releases/wheels/libtpu/{_libtpu_wheel_name}-py3-none-linux_x86_64.whl'\n\n\ndef _get_build_mode():\n  for i in range(1, len(sys.argv)):\n    if not sys.argv[i].startswith('-'):\n      return sys.argv[i]\n\n\ndef get_git_head_sha(base_dir):\n  xla_git_sha = subprocess.check_output(['git', 'rev-parse', 'HEAD'],\n                                        cwd=base_dir).decode('ascii').strip()\n  if os.path.isdir(os.path.join(base_dir, '..', '.git')):\n    torch_git_sha = subprocess.check_output(['git', 'rev-parse', 'HEAD'],\n                                            cwd=os.path.join(\n                                                base_dir,\n                                                '..')).decode('ascii').strip()\n  else:\n    torch_git_sha = ''\n  return xla_git_sha, torch_git_sha\n\n\ndef get_build_version(xla_git_sha):\n  version = os.getenv('TORCH_XLA_VERSION', '2.7.0')\n  if build_util.check_env_flag('GIT_VERSIONED_XLA_BUILD', default='TRUE'):\n    try:\n      version += '+git' + xla_git_sha[:7]\n    except Exception:\n      pass\n  return version\n\n\ndef create_version_files(base_dir, version, xla_git_sha, torch_git_sha):\n  print('Building torch_xla version: {}'.format(version))\n  print('XLA Commit ID: {}'.format(xla_git_sha))\n  print('PyTorch Commit ID: {}'.format(torch_git_sha))\n  py_version_path = os.path.join(base_dir, 'torch_xla', 'version.py')\n  with open(py_version_path, 'w') as f:\n    f.write('# Autogenerated file, do not edit!\\n')\n    f.write(\"__version__ = '{}'\\n\".format(version))\n    f.write(\"__xla_gitrev__ = '{}'\\n\".format(xla_git_sha))\n    f.write(\"__torch_gitrev__ = '{}'\\n\".format(torch_git_sha))\n\n  cpp_version_path = os.path.join(base_dir, 'torch_xla', 'csrc', 'version.cpp')\n  with open(cpp_version_path, 'w') as f:\n    f.write('// Autogenerated file, do not edit!\\n')\n    f.write('#include \"torch_xla/csrc/version.h\"\\n\\n')\n    f.write('namespace torch_xla {\\n\\n')\n    f.write('const char XLA_GITREV[] = {{\"{}\"}};\\n'.format(xla_git_sha))\n    f.write('const char TORCH_GITREV[] = {{\"{}\"}};\\n\\n'.format(torch_git_sha))\n    f.write('}  // namespace torch_xla\\n')\n\n\ndef maybe_bundle_libtpu(base_dir):\n  libtpu_path = os.path.join(base_dir, 'torch_xla', 'lib', 'libtpu.so')\n  with contextlib.suppress(FileNotFoundError):\n    os.remove(libtpu_path)\n\n  if not build_util.check_env_flag('BUNDLE_LIBTPU', '0'):\n    return\n\n  try:\n    import libtpu\n    module_path = os.path.dirname(libtpu.__file__)\n    print('Found pre-installed libtpu at ', module_path)\n    shutil.copyfile(os.path.join(module_path, 'libtpu.so'), libtpu_path)\n  except ModuleNotFoundError:\n    print('No installed libtpu found. Downloading...')\n\n    with tempfile.NamedTemporaryFile('wb') as whl:\n      resp = requests.get(_libtpu_storage_path)\n      resp.raise_for_status()\n\n      whl.write(resp.content)\n      whl.flush()\n\n      os.makedirs(os.path.join(base_dir, 'torch_xla', 'lib'), exist_ok=True)\n      with open(libtpu_path, 'wb') as libtpu_so:\n        z = zipfile.ZipFile(whl.name)\n        libtpu_so.write(z.read('libtpu/libtpu.so'))\n\n\nclass Clean(distutils.command.clean.clean):\n\n  def bazel_clean_(self):\n    self.spawn(['bazel', 'clean', '--expunge'])\n\n  def run(self):\n    import glob\n    import re\n    with open('.gitignore', 'r') as f:\n      ignores = f.read()\n      pat = re.compile(r'^#( BEGIN NOT-CLEAN-FILES )?')\n      for wildcard in filter(None, ignores.split('\\n')):\n        match = pat.match(wildcard)\n        if match:\n          if match.group(1):\n            # Marker is found and stop reading .gitignore.\n            break\n          # Ignore lines which begin with '#'.\n        else:\n          for filename in glob.glob(wildcard):\n            try:\n              os.remove(filename)\n            except OSError:\n              shutil.rmtree(filename, ignore_errors=True)\n\n    self.execute(self.bazel_clean_, (), msg=\"Cleaning bazel outputs\")\n\n    # It's an old-style class in Python 2.7...\n    distutils.command.clean.clean.run(self)\n\n\nxla_git_sha, torch_git_sha = get_git_head_sha(base_dir)\nversion = get_build_version(xla_git_sha)\n\nbuild_mode = _get_build_mode()\nif build_mode not in ['clean']:\n  # Generate version info (torch_xla.__version__).\n  create_version_files(base_dir, version, xla_git_sha, torch_git_sha)\n\n  # Copy libtpu.so into torch_xla/lib\n  maybe_bundle_libtpu(base_dir)\n\n\nclass BazelExtension(Extension):\n  \"\"\"A C/C++ extension that is defined as a Bazel BUILD target.\"\"\"\n\n  def __init__(self, bazel_target):\n    self.bazel_target = bazel_target\n    self.relpath, self.target_name = (\n        posixpath.relpath(bazel_target, '//').split(':'))\n    ext_name = os.path.join(\n        self.relpath.replace(posixpath.sep, os.path.sep), self.target_name)\n    if ext_name.endswith('.so'):\n      ext_name = ext_name[:-3]\n    Extension.__init__(self, ext_name, sources=[])\n\n\nclass BuildBazelExtension(build_ext.build_ext):\n  \"\"\"A command that runs Bazel to build a C/C++ extension.\"\"\"\n\n  def run(self):\n    for ext in self.extensions:\n      self.bazel_build(ext)\n    command.build_ext.build_ext.run(self)\n\n  def bazel_build(self, ext):\n    if not os.path.exists(self.build_temp):\n      os.makedirs(self.build_temp)\n\n    bazel_argv = [\n        'bazel', 'build', ext.bazel_target,\n        f\"--symlink_prefix={os.path.join(self.build_temp, 'bazel-')}\"\n    ]\n\n    build_cpp_tests = build_util.check_env_flag('BUILD_CPP_TESTS', default='0')\n    if build_cpp_tests:\n      bazel_argv.append('//:cpp_tests')\n\n    import torch\n    cxx_abi = os.getenv('CXX_ABI') or getattr(torch._C,\n                                              '_GLIBCXX_USE_CXX11_ABI', None)\n    if cxx_abi is not None:\n      bazel_argv.append(f'--cxxopt=-D_GLIBCXX_USE_CXX11_ABI={int(cxx_abi)}')\n\n    bazel_argv.extend(build_util.bazel_options_from_env())\n\n    self.spawn(bazel_argv)\n\n    ext_bazel_bin_path = os.path.join(self.build_temp, 'bazel-bin', ext.relpath,\n                                      ext.target_name)\n    ext_dest_path = self.get_ext_fullpath(ext.name)\n    ext_dest_dir = os.path.dirname(ext_dest_path)\n    if not os.path.exists(ext_dest_dir):\n      os.makedirs(ext_dest_dir)\n    shutil.copyfile(ext_bazel_bin_path, ext_dest_path)\n\n\nclass Develop(develop.develop):\n\n  def run(self):\n    self.run_command(\"build_ext\")\n    super().run()\n\n\n# Read in README.md for our long_description\ncwd = os.path.dirname(os.path.abspath(__file__))\nwith open(os.path.join(cwd, \"README.md\"), encoding=\"utf-8\") as f:\n  long_description = f.read()\n\nsetup(\n    name=os.environ.get('TORCH_XLA_PACKAGE_NAME', 'torch_xla'),\n    version=version,\n    description='XLA bridge for PyTorch',\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url='https://github.com/pytorch/xla',\n    author='PyTorch/XLA Dev Team',\n    author_email='pytorch-xla@googlegroups.com',\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Education\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: BSD License\",\n        \"Topic :: Scientific/Engineering\",\n        \"Topic :: Scientific/Engineering :: Mathematics\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Software Development\",\n        \"Topic :: Software Development :: Libraries\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n        \"Programming Language :: C++\",\n        \"Programming Language :: Python :: 3\",\n    ],\n    python_requires=\">=3.8.0\",\n    packages=find_packages(include=['torch_xla*']),\n    ext_modules=[\n        BazelExtension('//:_XLAC.so'),\n        BazelExtension('//:_XLAC_cuda_functions.so'),\n    ],\n    install_requires=[\n        'absl-py>=1.0.0',\n        'numpy',\n        'pyyaml',\n        'requests',\n        # importlib.metadata backport required for PJRT plugin discovery prior\n        # to Python 3.10\n        'importlib_metadata>=4.6;python_version<\"3.10\"',\n    ],\n    package_data={\n        'torch_xla': ['lib/*.so*',],\n    },\n    entry_points={\n        'console_scripts': [\n            'stablehlo-to-saved-model = torch_xla.tf_saved_model_integration:main'\n        ],\n        'torch_xla.plugins': [\n            'tpu = torch_xla._internal.tpu:TpuPlugin',\n            'neuron = torch_xla._internal.neuron:NeuronPlugin',\n            'xpu = torch_xla._internal.xpu:XpuPlugin'\n        ],\n    },\n    extras_require={\n        # On Cloud TPU VM install with:\n        # pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-wheels/index.html -f https://storage.googleapis.com/libtpu-releases/index.html\n        'tpu': [\n            f'libtpu=={_libtpu_version}',\n            'tpu-info',\n            # This special version removes `libtpu.so` from any `libtpu-nightly` installations,\n            # since we have migrated to using the `libtpu.so` from the `libtpu` package.\n            \"libtpu-nightly==0.1.dev20241010+nightly.cleanup\"\n        ],\n        # pip install torch_xla[pallas] -f https://storage.googleapis.com/jax-releases/jax_nightly_releases.html -f https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html\n        'pallas': [f'jaxlib=={_jaxlib_version}', f'jax=={_jax_version}'],\n    },\n    cmdclass={\n        'build_ext': BuildBazelExtension,\n        'clean': Clean,\n        'develop': Develop,\n    })\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "torch_xla",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}