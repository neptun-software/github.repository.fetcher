{
  "metadata": {
    "timestamp": 1736565535029,
    "page": 410,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "heavyai/heavydb",
      "stars": 2963,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 0.17578125,
          "content": "BasedOnStyle:  Chromium\nBinPackArguments: false\nBinPackParameters: false\nColumnLimit:     90\nIndentWidth:     2\nBreakConstructorInitializers: BeforeComma\nIncludeBlocks:   Preserve\n"
        },
        {
          "name": ".clang-tidy",
          "type": "blob",
          "size": 0.138671875,
          "content": "Checks: -*,readability-braces-around-statements,modernize-deprecated-headers,modernize-use-using,modernize-use-emplace,modernize-use-override\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.681640625,
          "content": "ThirdParty/* linguist-vendored\n\n* text=auto\n\n*.c       text\n*.cc      text\n*.cpp     text\n*.cu      text\n*.cuh     text\n*.h       text\n*.hpp     text\n\n*.go      text\n*.java    text\n*.js      text\n*.py      text\n*.rb      text\n*.sh      text\n\n*.html    text\n*.mapd    text\n*.MAPD    text\n*.md      text\n*.txt     text\n*.TXT     text\n*.wkt     text=CRLF\n\n*.cmake   text\n*.conf    text\n*.service text\n*.sql     text\n*.thrift  text\n\n*.csv     text\n*.json    text\n*.toml    text\n*.xml     text\n*.yml     text\n\n*.def     text=CRLF\n*.ini     text=CRLF\n*.props   text=CRLF\n*.rc      text=CRLF\n*.reg     text=CRLF\n*.sln     text=CRLF\n*.vcxproj text=CRLF\n*.vcxproj.user text=CRLF\n*.vcxproj.filters text=CRLF\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.9990234375,
          "content": "# Compiled Object files\n*.slo\n*.lo\n*.o\n*.obj\n\n# Compiled Dynamic libraries\n*.so\n*.dylib\n*.dll\n\n# Compiled Static libraries\n*.lai\n*.la\n*.a\n*.lib\n\n# Executables\n*.exe\n*.out\n*.app\n\n# CMake files\nCMakeCache.txt\nCMakeFiles\nMakefile\ncmake_install.cmake\ninstall_manifest.txt\n_build/\nbuild/\nbuild_debug/\nbuild_release/\nbuild3.6/\n!docker/build/\n\n# history files\nomnisql_history.txt\n.gdb_history\n\n# Eclipse build files\nDebug/\n.settings/\n\n# VS code settings files\n.vscode/\n*.code-workspace\n\n# VS settings files\n.vs\nCMakeSettings.json\n\n# VIM editor files\n.*.swp\n\n# Emacs editor files\n*~\n\n# Thrift-generated files\njava/src/gen/\ngen-js/\ngen-py/\njava/target/\njava/**/log/\n\n# go files\nThirdParty/go/bin/\nThirdParty/go/pkg/\nThirdParty/go/src/*\n!ThirdParty/go/src/mapd/\n\n# sphinx docs\n!docs/**/Makefile\n*.pyc\ndocs/build/\ndocs/generated-docs/\ndocs/sphinx-env\ndocs/source/api/*\n\n# java-generated files\njava/**/target/\njava/thrift/src/\n*.classpath\n*.project\n\nThirdParty/\ndata/\n\n# IntelliJ/CLion settings files\n.idea\n*.iml\n\n# Python\n*.egg-info/\n"
        },
        {
          "name": "Analyzer",
          "type": "tree",
          "content": null
        },
        {
          "name": "Archive",
          "type": "tree",
          "content": null
        },
        {
          "name": "Benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "CLA.txt",
          "type": "blob",
          "size": 4.4638671875,
          "content": "OMNISCI CONTRIBUTOR LICENSE AGREEMENT\n\nThis Contributor License Agreement (\"Agreement\") is between the contributor\nspecified below (\"you\") and OmniSci, Inc. (\"we\" or \"us\").  This\nAgreement applies to all Contributions (as defined below) submitted by you to\nan open source project owned or managed by us (each, a \"Project\").\n\nDefinitions. The terms \"you\" or \"your\" shall mean the copyright owner or the\nindividual or legal entity authorized by the copyright owner that is granting\nthe licenses under this Agreement. For legal entities, the term \"you\" includes\nany entity making a Contribution and all other entities that control, are\ncontrolled by, or are under common control with that entity.  \"Contribution\"\nmeans any work of authorship, including any source code, object code, patch,\ntool, sample, graphic, image, audio or audiovisual work, specification, manual,\nand documentation, and any modifications or additions to an existing work,\nsubmitted by you in connection with any product or other item developed,\nmanaged or maintained by a Project (collectively, such products and other\nitems, \"Work\"). The term \"submitted\" means any form of electronic, verbal, or\nwritten communication sent to a Project or its representatives, including\ncommunication on electronic mailing lists, source code control systems, and\nissue tracking systems that are managed by or on behalf of a Project.\n\nCopyright License. You hereby grant to us and to recipients of products or\nother items distributed by a Project a perpetual, worldwide, non-exclusive,\nno-charge, royalty-free, irrevocable copyright license to reproduce, prepare\nderivative works of, publicly display, publicly perform, transmit, sublicense,\nand distribute your Contributions and derivative works thereof.\n\nPatent License. You hereby grant to us and to recipients of products or other\nitems distributed by the Project a perpetual, worldwide, non-exclusive,\nno-charge, royalty-free, irrevocable patent license to make, have made, use,\noffer to sell, sell, import, and otherwise transfer the Work, where such\nlicense applies only to those patent claims licensable by you that are\nnecessarily infringed by your Contribution(s) alone or by the combination of\nyour Contribution(s) with the Work to which such Contribution(s) was submitted.\n\nAuthority.  You represent that you are legally entitled to grant the licenses\nspecified above. If your employer or other entity that you are associated with\nhas rights to intellectual property that you create which includes your\nContributions, you represent that you have received authorization to make\nContributions and grant the foregoing licenses on behalf of that employer or\nentity in accordance with this Agreement.\n\nOriginality.  You represent that each of your Contributions is your original\ncreation (see section 7 below for submissions on behalf of others). You\nrepresent that your Contributions include complete details of any third-party\nlicense or other restriction (including related patents and trademarks) of\nwhich you are personally aware and which are associated with any part of your\nContributions.\n\nNo Warranty.  You are not expected to provide support for your Contributions.\nYou may provide support for free, for a fee, or not at all. Unless required by\napplicable law or agreed to in writing, you provide your Contributions on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR\nIMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE,\nNON- INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.\n\nSubmissions of Third Party Contributions.  Should you wish to submit work that\nis not your original creation, you may submit it to the Project Leads\nseparately from any Contribution, identifying the complete details of its\nsource and of any license or other restriction (including related patents,\ntrademarks, and license agreements) of which you are personally aware, and\nconspicuously marking the work as \"Submitted on behalf of a third-party: [named\nhere].\"\n\nNotification.  You will notify us of any facts or circumstances of which you\nbecome aware that would make the foregoing representations inaccurate in any\nrespect or would call into question the grants of rights hereunder.\n\nMiscellaneous.  This Agreement is the exclusive agreement between the parties\nwith respect to the subject matter hereof.  We may freely assign this\nAgreement.  This Agreement shall be governed by the laws of the state of\nCalifornia.\n\nContributor\n\nSignature:\n\nPrinted Name:\n\nDate:\n\nAddress:\n\nSV\\821532.2\n\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 55.021484375,
          "content": "cmake_minimum_required(VERSION 3.16)\n\n# Required for LLVM / JIT, alternative to setting CMP0065 to OLD\nset(CMAKE_ENABLE_EXPORTS True)\n\nset(ENABLE_CONDA OFF)\nif(DEFINED ENV{CONDA_PREFIX})\n  set(ENABLE_CONDA ON)\n  list(APPEND CMAKE_PREFIX_PATH \"$ENV{CONDA_PREFIX}\")\n  # add a compile definition for conda build\n  add_definitions(\"-DCONDA_BUILD\")\n  # resolves link issue for zlib\n  link_directories(\"$ENV{CONDA_PREFIX}/lib\" \"$ENV{CONDA_PREFIX}/Library/lib\")\n  # various fixes and workarounds\n  add_definitions(\"-Dsecure_getenv=getenv\")\n  # fixes `undefined reference to `boost::system::detail::system_category_instance'`:\n  add_definitions(\"-DBOOST_ERROR_CODE_HEADER_ONLY\")\n  # Adding formating macros\n  add_definitions(\"-D__STDC_FORMAT_MACROS=1\")\n  # fixes always_inline attribute errors\n  add_compile_options(\"$<$<COMPILE_LANGUAGE:CXX>:-fno-semantic-interposition>\")\n  # Adding `--sysroot=...` resolves `no member named 'signbit' in the global namespace` error:\n  set(CMAKE_SYSROOT \"$ENV{CONDA_BUILD_SYSROOT}\")\nendif(DEFINED ENV{CONDA_PREFIX})\n\n# force `Release` build type if left unspecified\nif(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)\n  set(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"Choose the type of build.\" FORCE)\n  set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"MinSizeRel\" \"RelWithDebInfo\")\nendif()\nstring(TOLOWER \"${CMAKE_BUILD_TYPE}\" CMAKE_BUILD_TYPE_LOWER)\n\nif(\"${CMAKE_VERSION}\" VERSION_GREATER 3.11.999)\n  cmake_policy(SET CMP0074 NEW)\nendif()\n\nfind_program(CCACHE_EXE ccache)\nif(CCACHE_EXE)\n  set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE \"${CCACHE_EXE}\")\nendif()\n\noption(ENABLE_IWYU \"Enable include-what-you-use\" OFF)\nif(ENABLE_IWYU)\n  find_program(IWYU_EXE include-what-you-use)\n  if(IWYU_EXE)\n    set(CMAKE_CXX_INCLUDE_WHAT_YOU_USE \"${IWYU_EXE}\")\n  endif()\nendif()\n\nproject(heavyai)\n\n# ENABLE_IWYU: Generates .iwyu files suitable for input into include-what-you-use's fix_includes.py tool.\noption(ENABLE_IWYU \"Enable include-what-you-use advice (.iwyu files for fix_includes.py)\" OFF)\nif(ENABLE_IWYU)\n if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\")\n   find_program(IWYU_EXE include-what-you-use)\n   if(IWYU_EXE)\n     set(CMAKE_CXX_INCLUDE_WHAT_YOU_USE \"${CMAKE_SOURCE_DIR}/scripts/do-iwyu;${IWYU_EXE};-Xiwyu --cxx17ns\")\n   else()\n     message(FATAL_ERROR \"ENABLE_IWYU failed to find the include-what-you-use binary\")\n   endif()\n else()\n   message(FATAL_ERROR \"ENABLE_IWYU may only be used with the clang compiler (need to set CMAKE_CXX_COMPILER)\")\n  endif()\nendif()\n\n# ENABLE_TIME_TRACE: Generates .json files with build timings viewable in a chrome://tracing/ flame graph.\noption(ENABLE_TIME_TRACE \"Enable build time tracing (.json files for chrome://tracing/)\" OFF)\nif(ENABLE_TIME_TRACE)\n  if((\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\" AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 9.0))\n    add_compile_options(\"$<$<COMPILE_LANGUAGE:CXX>:-ftime-trace>\")\n    # Adding -ftime-trace to clang++ for linking seems to have no effect.\n    # Leaving the commented-out setting here for further investigation.\n    #add_link_options(\"$<$<LINK_LANGUAGE:CXX>:-ftime-trace>\")  # For after we upgrade to: cmake 3.18+\n    #add_link_options(\"-ftime-trace\")\n  else()\n    message(FATAL_ERROR \"ENABLE_TIME_TRACE may only be used with the clang compiler 9.0+ (need to set CMAKE_CXX_COMPILER)\")\n  endif()\nendif()\n \nif(NOT EXISTS \"${CMAKE_SOURCE_DIR}/Rendering\")\n  set(MAPD_EDITION \"OS\")\nelseif(NOT DEFINED MAPD_EDITION)\n  set(MAPD_EDITION \"EE\")\nendif()\nset(MAPD_EDITION \"${MAPD_EDITION}\" CACHE STRING \"MapD edition\" FORCE)\nset_property(CACHE MAPD_EDITION PROPERTY STRINGS \"EE\" \"CE\" \"OS\")\nadd_definitions(\"-DMAPD_EDITION_${MAPD_EDITION}\")\nstring(TOLOWER \"${MAPD_EDITION}\" MAPD_EDITION_LOWER)\n\n# HeavyDB version number\nset(MAPD_VERSION_MAJOR \"8\")\nset(MAPD_VERSION_MINOR \"0\")\nset(MAPD_VERSION_PATCH \"0\")\nset(MAPD_VERSION_EXTRA \"dev\")\nset(MAPD_VERSION_RAW \"${MAPD_VERSION_MAJOR}.${MAPD_VERSION_MINOR}.${MAPD_VERSION_PATCH}${MAPD_VERSION_EXTRA}\")\nset(MAPD_IMMERSE_BUILD_ID \"immerse-v2-latest-master-prod\" CACHE STRING \"Immerse Build ID\")\nset(MAPD_IMMERSE_URL \"http://builds.mapd.com/frontend/${MAPD_IMMERSE_BUILD_ID}.zip\")\nstring(TIMESTAMP MAPD_BUILD_DATE \"%Y%m%d\")\n\nif($ENV{BUILD_NUMBER})\n  set(MAPD_BUILD_NUMBER \"$ENV{BUILD_NUMBER}\")\nelse()\n  set(MAPD_BUILD_NUMBER \"dev\")\nendif()\nset(MAPD_VERSION \"${MAPD_VERSION_RAW}-${MAPD_BUILD_NUMBER}\")\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_EXTENSIONS OFF)\n\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n\nlist(APPEND CMAKE_MODULE_PATH \"${CMAKE_SOURCE_DIR}/cmake/Modules\")\n\nadd_custom_target(clean-all\n  COMMAND ${CMAKE_BUILD_TOOL} clean\n )\n\nmacro(set_alternate_linker linker)\n  find_program(LINKER_EXECUTABLE ld.${USE_ALTERNATE_LINKER} ${USE_ALTERNATE_LINKER})\n  if(LINKER_EXECUTABLE)\n    if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\" AND \"${CMAKE_CXX_COMPILER_VERSION}\" VERSION_LESS 12.0.0)\n      add_link_options(\"-ld-path=${USE_ALTERNATE_LINKER}\")\n    elseif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\" AND \"${CMAKE_CXX_COMPILER_VERSION}\" VERSION_LESS 12.1.0 AND \"${USE_ALTERNATE_LINKER}\" STREQUAL \"mold\")\n      # LINKER_EXECUTABLE will be a full path to ld.mold, so we replace the end of the path, resulting in the relative\n      # libexec/mold dir, and tell GCC to look there first for an override version of executables, in this case, ld\n      string(REPLACE \"bin/ld.mold\" \"libexec/mold\" PATH_TO_LIBEXEC_MOLD ${LINKER_EXECUTABLE})\n      add_link_options(\"-B${PATH_TO_LIBEXEC_MOLD}\")\n    else()\n      add_link_options(\"-fuse-ld=${USE_ALTERNATE_LINKER}\")\n    endif()\n  else()\n    set(USE_ALTERNATE_LINKER \"\" CACHE STRING \"Use alternate linker\" FORCE)\n  endif()\nendmacro()\n\nmacro(InstallVersionFile)\n  # `touch mapd_deps_version.txt` from build dir to silence warning.\n  find_file(MapdDepsVersion_FILE mapd_deps_version.txt PATH ${CMAKE_BINARY_DIR} NO_CACHE)\n\n  if(NOT MapdDepsVersion_FILE)\n    message(WARNING \"Build could NOT find deps version file mapd_deps_version.txt\")\n  else()\n    message(STATUS \"Found deps version file ${MapdDepsVersion_FILE}\")\n    file(MAKE_DIRECTORY ${CMAKE_BINARY_DIR}/version)\n    set(HEAVY_DEPS_CUSTOM ${CMAKE_BINARY_DIR}/version/heavyai_deps_version.txt)\n    # cp mapd_version data to heavyai deps, removing un-tagged lines.\n    add_custom_command(OUTPUT ${HEAVY_DEPS_CUSTOM}\n      DEPENDS ${MapdDepsVersion_FILE}\n      COMMAND ${CMAKE_COMMAND} -E copy ${MapdDepsVersion_FILE} ${HEAVY_DEPS_CUSTOM}\n      # In the copied deps files leave the first line with the deps generated info\n      # and any other line starting with 'Public Release:', though remove the public release tag\n      VERBATIM\n      COMMAND \"sed\" \"-i\" \"-n\" \"/^Public Release:/s/^Public Release://p\" ${HEAVY_DEPS_CUSTOM})\n\n    add_custom_target(HeavyDepsVersionTarget DEPENDS ${HEAVY_DEPS_CUSTOM})\n    add_dependencies(heavydb HeavyDepsVersionTarget)\n    install(FILES ${HEAVY_DEPS_CUSTOM} DESTINATION \".\"  COMPONENT \"doc\")\n  endif()\nendmacro()\n\nset(USE_ALTERNATE_LINKER \"\" CACHE STRING \"Use alternate linker. Leave empty for system default; alternatives are 'gold', 'lld', 'bfd', 'mold'\")\nif(NOT \"${USE_ALTERNATE_LINKER}\" STREQUAL \"\")\n  set_alternate_linker(${USE_ALTERNATE_LINKER})\nendif()\n\n# Ensure that boost's uuid implementation links bcrypt correctly under windows;\n# see issue described here: https://github.com/microsoft/vcpkg/issues/4481\nif(WIN32)\n  message(STATUS \"Defining BOOST_UUID_FORCE_AUTO_LINK\")\n  add_definitions(-DBOOST_UUID_FORCE_AUTO_LINK)\nendif()\n\noption(PREFER_STATIC_LIBS \"Prefer linking against static libraries\" OFF)\nif(PREFER_STATIC_LIBS)\n  set(CMAKE_FIND_LIBRARY_SUFFIXES .lib .a ${CMAKE_FIND_LIBRARY_SUFFIXES})\n  set(Arrow_USE_STATIC_LIBS ON)\n  set(Boost_USE_STATIC_LIBS ON)\n  set(OPENSSL_USE_STATIC_LIBS ON)\n  set(Thrift_USE_STATIC_LIBS ON)\n  if(${CMAKE_CXX_COMPILER_ID} STREQUAL \"GNU\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -static-libgcc -static-libstdc++\")\n  endif()\n\n  set(CUDA_USE_STATIC_CUDA_RUNTIME ON CACHE STRING \"Use static CUDA runtime\")\n\n  # On ppc, build failures occur for targets that depend on locale related functions due to unresolved symbols that are\n  # present in the stdc++ library. Add the library flag to these targets to be used in resolving these symbols.\n  if(CMAKE_SYSTEM_PROCESSOR STREQUAL \"ppc64le\")\n    set(LOCALE_LINK_FLAG \"-lstdc++\")\n  endif()\nelse()\n  add_definitions(\"-DBOOST_LOG_DYN_LINK\")\nendif()\n\n# Required for macOS with Boost 1.71.0+\n# See https://gitlab.kitware.com/cmake/cmake/issues/19714\nset(Boost_NO_BOOST_CMAKE 1)\n\noption(ENABLE_JAVA_REMOTE_DEBUG \"Enable Java Remote Debug\" OFF )\nif(ENABLE_JAVA_REMOTE_DEBUG)\n  add_definitions(\"-DENABLE_JAVA_REMOTE_DEBUG\")\nendif()\n\n# Disable by default until a more satisfactory resolution to QE-215 is found.\noption(ENABLE_UTM_TRANSFORM \"Enable ST_TRANSFORM() with UTM Support\" OFF )\nif(ENABLE_UTM_TRANSFORM )\n  add_definitions(\"-DENABLE_UTM_TRANSFORM\")\nendif()\n\noption(ENABLE_L0 \"Enable level zero support\" OFF)\nif(ENABLE_L0)\n  find_package(LevelZero REQUIRED COMPONENTS ${LevelZero_COMPONENTS})\n  add_definitions(\"-DHAVE_L0\")\nendif()\n\noption(ENABLE_CUDA \"Enable CUDA support\" ON)\nif(ENABLE_CUDA)\n  enable_language(CUDA)\n  include_directories(${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES})\n  list(APPEND CUDA_LIBRARIES cuda)\n  add_definitions(\"-DHAVE_CUDA\")\n\nelse()\n  set(CUDA_LIBRARIES \"\")\n  set(MAPD_PACKAGE_FLAGS \"${MAPD_PACKAGE_FLAGS}-cpu\")\nendif()\n\n# CUDA architecture flags\nif(\"${CMAKE_BUILD_TYPE_LOWER}\" STREQUAL \"debug\")\n  option(ENABLE_ONLY_ONE_ARCH \"Enable quicker building for only one GPU arch\" ON)\nelse()\n  option(ENABLE_ONLY_ONE_ARCH \"Enable quicker building for only one GPU arch\" OFF)\nendif()\nif(ENABLE_CUDA)\n  set(MAPD_CUDA_OPTIONS)\n  # Set Thrust debug mode for CUDA compilation project-wide\n  string(TOUPPER \"${CMAKE_BUILD_TYPE}\" CMAKE_BUILD_TYPE_UPPERCASE)\n  if(CMAKE_BUILD_TYPE_UPPERCASE MATCHES DEBUG)\n    list(APPEND MAPD_CUDA_OPTIONS -DTHRUST_DEBUG --debug)\n  else()\n    list(APPEND MAPD_CUDA_OPTIONS -O3)\n  endif()\n\n  list(APPEND MAPD_CUDA_OPTIONS -Xcompiler -fPIC -D_FORCE_INLINES -std=c++17)\n\n  if(ENABLE_ONLY_ONE_ARCH)\n    execute_process(\n      COMMAND cmake -S ${CMAKE_SOURCE_DIR}/NvidiaComputeCapability -B NvidiaComputeCapability\n      OUTPUT_QUIET\n      ERROR_QUIET\n      WORKING_DIRECTORY ${CMAKE_BINARY_DIR}\n    )\n    execute_process(\n      COMMAND cmake --build NvidiaComputeCapability\n      OUTPUT_FILE ${CMAKE_BINARY_DIR}/NvidiaComputeCapability/build.out.txt\n      ERROR_FILE ${CMAKE_BINARY_DIR}/NvidiaComputeCapability/build.err.txt\n      WORKING_DIRECTORY ${CMAKE_BINARY_DIR}\n    )\n    set(NVIDIA_COMPUTE_CAPABILITY \"\")\n    if (EXISTS ${CMAKE_BINARY_DIR}/NvidiaComputeCapability.txt)\n      file(STRINGS ${CMAKE_BINARY_DIR}/NvidiaComputeCapability.txt NVIDIA_COMPUTE_CAPABILITY)\n    endif()\n  endif()\n  if (ENABLE_ONLY_ONE_ARCH AND NOT \"${NVIDIA_COMPUTE_CAPABILITY}\" STREQUAL \"\")\n    if(${CMAKE_VERSION} VERSION_GREATER_EQUAL \"3.18.0\")\n      set(CMAKE_CUDA_ARCHITECTURES ${NVIDIA_COMPUTE_CAPABILITY}-virtual)\n      list(APPEND MAPD_CUDA_OPTIONS -Wno-deprecated-gpu-targets)\n      message(STATUS \"CUDA_ARCHITECTURES: ${CMAKE_CUDA_ARCHITECTURES}\")\n    else()\n      set (CUDA_COMPILATION_ARCH\n        -gencode=arch=compute_${NVIDIA_COMPUTE_CAPABILITY},code=compute_${NVIDIA_COMPUTE_CAPABILITY}\n        -Wno-deprecated-gpu-targets\n      )\n      message(STATUS \"CUDA_COMPILATION_ARCH: ${CUDA_COMPILATION_ARCH}\")\n      add_compile_options(\"$<$<COMPILE_LANGUAGE:CUDA>:${CUDA_COMPILATION_ARCH}>\")\n    endif()\n    add_custom_target(clean_nvidia_compute_capability\n      COMMAND ${CMAKE_BUILD_TOOL} clean\n      WORKING_DIRECTORY ${CMAKE_BINARY_DIR}/NvidiaComputeCapability\n    )\n    add_dependencies(clean-all clean_nvidia_compute_capability)\n  else()\n    if(${CMAKE_VERSION} VERSION_GREATER_EQUAL \"3.18.0\")\n      message(STATUS \"CMake 3.18+, Setting CUDA_ARCHITECTURES.\")\n      set(CMAKE_CUDA_ARCHITECTURES\n          50-virtual\n          60-virtual\n          70-virtual\n          75-virtual\n          80-virtual)\n      list(APPEND MAPD_CUDA_OPTIONS -Wno-deprecated-gpu-targets)\n      message(STATUS \"CUDA_ARCHITECTURES: ${CMAKE_CUDA_ARCHITECTURES}\")\n    else()\n      message(STATUS \"CMake 3.17 or under, setting CUDA architecture flags manually.\")\n      set(CUDA_COMPILATION_ARCH\n        -gencode=arch=compute_50,code=compute_50;\n        -gencode=arch=compute_60,code=compute_60;\n        -gencode=arch=compute_70,code=compute_70;\n        -gencode=arch=compute_75,code=compute_75;\n        -gencode=arch=compute_80,code=compute_80;\n        -Wno-deprecated-gpu-targets)\n      message(STATUS \"CUDA_COMPILATION_ARCH: ${CUDA_COMPILATION_ARCH}\")\n      list(APPEND MAPD_CUDA_OPTIONS ${CUDA_COMPILATION_ARCH})\n    endif()\n    if(ENABLE_ONLY_ONE_ARCH)\n      message(STATUS \"ENABLE_ONLY_ONE_ARCH ignored because NvidiaComputeCapability.txt not found or not readable\")\n    endif()\n  endif()\n  if(\"${CMAKE_CUDA_COMPILER_ID}\" STREQUAL \"NVIDIA\")\n    include(ProcessorCount)\n    ProcessorCount(N)\n    if(CMAKE_CUDA_COMPILER_VERSION GREATER_EQUAL 11.3 AND NOT N EQUAL 0)\n      message(STATUS \"Enabling NVCC multi-threaded compilation with ${N} threads.\")\n      # Adding to MAPD_CUDA_OPTIONS ensures that code that is compiled via generator expression (eg thrust) uses threads\n      # This breaks the VSCode language server with CMake versions > 16.5, so disable this for now\n      # list(APPEND MAPD_CUDA_OPTIONS --threads ${N})\n      set(NVCC_THREADS --threads ${N}) # used in explicit custom build steps which is most kernel compilation\n    endif()\n  endif()\n\n  add_compile_options(\"$<$<COMPILE_LANGUAGE:CUDA>:${MAPD_CUDA_OPTIONS}>\")\nendif()\n\noption(ENABLE_NVTX \"Enable NVidia Tools Extension library\" OFF)\nif(ENABLE_NVTX)\n  if(NOT ENABLE_CUDA)\n    set(ENABLE_NVTX OFF CACHE BOOL \"Enable NVidia Tools Extension library\" FORCE)\n    message(STATUS \"Cuda must be enabled to use NVTX, disabling NVTX support.\")\n  else()\n    find_package(NVTX)\n    if (NVTX_FOUND)\n      message(STATUS \"Using NVTX profiling markers\")\n      add_definitions(\"-DHAVE_NVTX\")\n    else()\n      set(ENABLE_NVTX OFF CACHE BOOL \"Enable NVidia Tools Extension library\" FORCE)\n      message(STATUS \"NVTX not found, disabling NVTX support.\")\n    endif()\n  endif()\nendif()\n\noption(SUPPRESS_NULL_LOGGER_DEPRECATION_WARNINGS \"Suppress NullLogger deprecated warnings.\")\nif (SUPPRESS_NULL_LOGGER_DEPRECATION_WARNINGS)\n  add_definitions(\"-DSUPPRESS_NULL_LOGGER_DEPRECATION_WARNINGS\")\nendif()\n\noption(ENABLE_CUDA_KERNEL_DEBUG \"Enable debugging symbols for CUDA device Kernels\" OFF)\n\noption(ENABLE_JIT_DEBUG \"Enable debugging symbols for the JIT\" OFF)\nif (ENABLE_JIT_DEBUG)\n  add_definitions(\"-DWITH_JIT_DEBUG\")\nendif()\n\nif(XCODE)\n  if(ENABLE_CUDA)\n    set(CMAKE_EXE_LINKER_FLAGS \"-F/Library/Frameworks -framework CUDA\")\n  endif()\n  add_definitions(\"-DXCODE\")\nendif()\n\noption(ENABLE_GEOS \"Enable GEOS Support\" ON)\nif (ENABLE_GEOS)\n  if(${CMAKE_SYSTEM_NAME} MATCHES \"Darwin\")\n    set(ENABLE_GEOS OFF CACHE BOOL \"Enable GEOS support\" FORCE)\n    message(STATUS \"GEOS functionality is not supported on macOS.\")\n  else()\n    find_package(GEOS)\n    if(GEOS_NOTFOUND)\n      set(ENABLE_GEOS OFF CACHE BOOL \"Enable GEOS support\" FORCE)\n      message(STATUS \"GEOS not found, disabling support.\")\n    else()\n      set(GEOS_LIBRARY_FILENAME '\"${GEOS_LIBRARY}\"')\n      add_definitions(\"-DENABLE_GEOS -DGEOS_LIBRARY_FILENAME=${GEOS_LIBRARY_FILENAME}\")\n      set(GEOS_RT_DEFINITIONS \"-DENABLE_GEOS\")\n    endif()\n  endif()\nendif()\n\n# fixme: hack works for Homebrew, might not work for Conda\nif(ENABLE_CONDA)\n  set(OPENSSL_ROOT_DIR \"$ENV{CONDA_PREFIX}\")\nelseif(${CMAKE_SYSTEM_NAME} MATCHES \"Darwin\" )\n  set(OPENSSL_ROOT_DIR \"/usr/local/opt/openssl/\")\nendif()\n\nfind_package(OpenSSL REQUIRED)\nadd_definitions(\"-DOPENSSL_SUPPRESS_DEPRECATED\")\ninclude_directories(${OPENSSL_INCLUDE_DIR})\n\nif(MSVC)\n  add_definitions(/bigobj)\n  find_package(Thrift CONFIG REQUIRED)\n  set(Thrift_INCLUDE_DIRS ${THRIFT_INCLUDE_DIR})\n  set(Thrift_EXECUTABLE \"${THRIFT_BIN_DIR}/thrift.exe\")\n  set(Thrift_LIBRARIES ${THRIFT_LIBRARIES})\nelse()\n  find_package(Thrift REQUIRED)\nendif()\ninclude_directories(${Thrift_INCLUDE_DIRS})\nif(\"${Thrift_VERSION}\" VERSION_LESS \"0.13.0\")\n  add_definitions(\"-DHAVE_THRIFT_PLATFORMTHREADFACTORY\")\nelse()\n  add_definitions(\"-DHAVE_THRIFT_THREADFACTORY\")\n  if(\"${Thrift_VERSION}\" VERSION_GREATER_EQUAL \"0.14.0\")\n    add_definitions(\"-DHAVE_THRIFT_MESSAGE_LIMIT\")\n  endif()\nendif()\n\nfind_package(Git)\nfind_package(PNG REQUIRED)\nfind_package(ZLIB REQUIRED)\nfind_package(GDAL REQUIRED)\nfind_package(GDALExtra REQUIRED)\nfind_package(BLOSC REQUIRED)\nlist(APPEND GDAL_LIBRARIES ${PNG_LIBRARIES} ${GDALExtra_LIBRARIES})\ninclude_directories(${GDAL_INCLUDE_DIRS})\n\noption(ENABLE_FOLLY \"Use Folly\" ON)\nif(ENABLE_FOLLY)\n  find_package(Folly)\n  if(NOT Folly_FOUND)\n    set(ENABLE_FOLLY OFF CACHE BOOL \"Use Folly\" FORCE)\n  else()\n    include_directories(${Folly_INCLUDE_DIRS})\n    add_definitions(\"-DHAVE_FOLLY\")\n    # TODO: use Folly::folly_deps?\n    if(MSVC)\n      find_package(Libevent COMPONENTS core REQUIRED)\n      list(APPEND Folly_LIBRARIES libevent::core)\n    endif()\n  endif()\nendif()\n\noption(ENABLE_SYSTEM_TFS \"Enable system table functions\" ON)\noption(ENABLE_ML_ONEDAL_TFS \"Enable Intel oneDal ML system table functions\" ON)\noption(ENABLE_ML_MLPACK_TFS \"Enable MLPack ML system table functions\" OFF)\noption(ENABLE_RUNTIME_LIBS \"Enable runtime library support\" OFF)\noption(ENABLE_TORCH_TFS \"Enable Torch system table functions\" OFF)\n\noption(ENABLE_PDAL \"Enable PDAL support\" ON)\noption(ENABLE_POINT_CLOUD_TFS \"Enable point cloud table functions\" ON)\n\nif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\")\n  # Clang will not complile these tests quickly enough to avoid timeout\n  option(ENABLE_RF_PROP_TFS \"Enable RF Propagation module\" OFF)\nelse()\n  option(ENABLE_RF_PROP_TFS \"Enable RF Propagation module\" ON)\nendif()\n\nif (ENABLE_PDAL)\n  find_package(PDAL)\n  if(PDAL_FOUND)\n    include_directories(${PDAL_INCLUDE_DIRS})\n    add_definitions( ${PDAL_DEFINITIONS} )\n    set(ENABLE_PDAL ON)\n    add_definitions(\"-DHAVE_PDAL\")\n  else()\n    set(ENABLE_PDAL OFF CACHE BOOL \"Enable PDAL support\" FORCE)\n    set(ENABLE_POINT_CLOUD_TFS OFF CACHE BOOL \"Enable point cloud table functions\" FORCE)\n  endif()\nelse()\n  set(ENABLE_POINT_CLOUD_TFS OFF CACHE BOOL \"Enable point cloud table functions\" FORCE)\nendif()\n\nif(ENABLE_SYSTEM_TFS)\n  add_definitions (\"-DHAVE_SYSTEM_TFS\")\n  if(\"${MAPD_EDITION_LOWER}\" STREQUAL \"ee\")\n    if(ENABLE_RF_PROP_TFS)\n      add_definitions (\"-DHAVE_RF_PROP_TFS\")\n    endif()\n    if (ENABLE_POINT_CLOUD_TFS)\n      add_definitions(\"-DHAVE_POINT_CLOUD_TFS\")\n    endif()\n  endif()\nelse()\n  set(ENABLE_POINT_CLOUD_TFS OFF CACHE BOOL \"Enable point cloud table functions\" FORCE)\n  set(ENABLE_RF_PROP_TFS OFF CACHE BOOL \"Enable RF Propagation module\" FORCE)\nendif()\n\nif(ENABLE_RF_PROP_TFS)\n  if(NOT \"${MAPD_EDITION_LOWER}\" STREQUAL \"ee\")\n    set(ENABLE_RF_PROP_TFS OFF CACHE BOOL \"Enable RF Propagation module\" FORCE)\n  endif()\nendif()\n\nif(ENABLE_ML_ONEDAL_TFS)\n  if(NOT ENABLE_SYSTEM_TFS)\n    set(ENABLE_ML_ONEDAL_TFS OFF CACHE BOOL \"Enable Intel oneDal ML system table functions\" FORCE)\n    message(STATUS \"System table functions must be enabled to use oneDal ML functions, disabling oneDal support.\")\n  else()\n    set(ONEDAL_USE_DPCPP no)\n    set(ONEDAL_INTERFACE yes)\n    set(ONEDAL_LINK static)\n    find_package(oneDAL)\n    if (oneDAL_FOUND)\n      message(STATUS \"Found the following oneDAL libraries:\")\n      foreach(DAL_LIBRARY IN LISTS DAL_LIBRARIES)\n        string(REPLACE \"oneDAL::\" \"\" DAL_LIBRARY \"${DAL_LIBRARY}\")\n        message(STATUS \"  ${DAL_LIBRARY}\")\n      endforeach()\n      include_directories(${oneDAL_INCLUDE_DIRS})\n      add_definitions(\"-DHAVE_ONEDAL\")\n    else()\n      set(ENABLE_ML_ONEDAL_TFS OFF CACHE BOOL \"Enable Intel oneDal ML system table functions\" FORCE)\n      message(STATUS \"oneDal library not found, disabling oneDal support.\")\n    endif()\n  endif()\nendif()\n\nif(ENABLE_ML_MLPACK_TFS)\n  if(NOT ENABLE_SYSTEM_TFS)\n    set(ENABLE_ML_MLPACK_TFS OFF CACHE BOOL \"Enable MLPACK ML system table functions\" FORCE)\n    message(STATUS \"System table functions must be enabled to use MLPACK ML functions, disabling MLPACK support.\")\n  else()\n    find_package(OpenMP REQUIRED)\n    find_package(Armadillo REQUIRED)\n    find_package(Boost COMPONENTS serialization REQUIRED)\n    find_package(MLPACK REQUIRED)\n    include_directories(${MLPACK_INCLUDE_DIRS})\n    add_definitions(\"-DHAVE_MLPACK\")\n  endif()\nendif()\n\nif(MSVC)\n  include_directories(include_directories(\"${LIBS_PATH}/include/pdcurses\"))\nelse()\n  find_package(Curses)\n  include_directories(${CURSES_INCLUDE_DIRS})\n  if (CURSES_HAVE_NCURSES_CURSES_H AND NOT CURSES_HAVE_CURSES_H)\n    include_directories(${CURSES_INCLUDE_DIRS}/ncurses/)\n  endif()\nendif()\n\nfunction(install_thirdparty_files src_hdr_list lib_base_path_list)\n  foreach(hdr ${src_hdr_list})\n    install(FILES ${hdr} DESTINATION ThirdParty/include)\n  endforeach()\n  include_directories(ThirdParty/include)\n\n  foreach(lib_base ${lib_base_path_list})\n    file(GLOB lib_versions_list ${lib_base}*)\n    foreach(lib ${lib_versions_list})\n      install(FILES ${lib} DESTINATION ThirdParty/lib)\n    endforeach()\n  endforeach()\nendfunction()\n\noption(ENABLE_MEMKIND \"Enable memkind support\" OFF)\nif (ENABLE_MEMKIND)\n  if($ENV{MEMKIND_PREFIX} STREQUAL \"\")\n    message(FATAL_ERROR \"The environment variable \\\"MEMKIND_PREFIX\\\" does not exist, are mapd dependencies sourced?\")\n  endif()\n  find_file( MEMKIND_HDR_PATH NAME memkind.h PATHS $ENV{MEMKIND_PREFIX}/include NO_DEFAULT_PATH)\n  set(_CMAKE_FIND_LIBRARY_SUFFIXES ${CMAKE_FIND_LIBRARY_SUFFIXES})\n  set(CMAKE_FIND_LIBRARY_SUFFIXES .so)\n  find_library(MEMKIND_LIB_PATH NAME memkind PATHS $ENV{MEMKIND_PREFIX}/lib64 $ENV{MEMKIND_PREFIX}/lib NO_DEFAULT_PATH)\n  find_library(NUMA_LIB_PATH NAME numa PATHS $ENV{MEMKIND_PREFIX}/lib NO_DEFAULT_PATH)\n  set(CMAKE_FIND_LIBRARY_SUFFIXES ${_CMAKE_FIND_LIBRARY_SUFFIXES})\n  install_thirdparty_files(\"${MEMKIND_HDR_PATH}\" \"${NUMA_LIB_PATH};${MEMKIND_LIB_PATH}\")\n  add_definitions(\"-DENABLE_MEMKIND\")\nendif()\n\nset(EXECUTABLE_OUTPUT_PATH ${CMAKE_BINARY_DIR}/bin)\n\nif(MSVC)\n  option(ENABLE_NO_WINWARNINGS \"disable most windows warnings\" ON)\n  add_compile_definitions(\"_USE_MATH_DEFINES\") # M_PI https://stackoverflow.com/q/6563810/2700898\n  add_compile_definitions(\"NOMINMAX\")\n  add_compile_definitions(\"WIN32_LEAN_AND_MEAN\")\n  # Fix for thrift_handler.lib(DBHandler.cpp.obj) : error LNK2038: mismatch detected for\n  # 'boost_log_abi': value 'v2_mt_nt6' doesn't match value 'v2_mt_nt62' in initdb.cpp.obj\n  # https://github.com/microsoft/vcpkg/discussions/22762\n  # If a future vcpkg release causes the reverse error message, then this line should be removed.\n  add_compile_definitions(\"BOOST_USE_WINAPI_VERSION=0x0601\") #=BOOST_WINAPI_VERSION_WIN7\n  if(ENABLE_NO_WINWARNINGS)\n    add_compile_definitions(\"_STL_EXTRA_DISABLED_WARNINGS=4146 4242 4244 4267 4355 4365 4458 4624 4820 4996 5204 5219\" \"NOMINMAX\")\n    # disable 4702 unreachable code warning\n    # with /Qspectre set, disable the warning C5045\n    add_compile_options(/W0 /wd4702 /wd5045)\n  else()\n    add_compile_options(/W4 /permisive-)\n  endif()\n  add_compile_options(/EHsc /std:c++17 /Qspectre)\nelse()\n  option(ENABLE_WARNINGS_AS_ERRORS \"Enable treat warnings as errors\" ON)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall -Wno-unused-local-typedefs -fdiagnostics-color=auto -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS\")\n  if (ENABLE_WARNINGS_AS_ERRORS)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror\")\n    if (${CMAKE_CXX_COMPILER_ID} STREQUAL \"GNU\")\n      # Allow maybe-uninitialized warnings with GCC at it is prone to false positives\n      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-error=maybe-uninitialized\")\n    elseif(CMAKE_CXX_COMPILER_ID MATCHES \"Clang\")\n      # Disable warnings as errors for misleading-indentation due to a boost header issue\n      # This can be removed once the boost fix is in our deps https://github.com/boostorg/serialization/pull/262/files\n      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-error=misleading-indentation\")\n    endif()\n  endif()\n  if (CMAKE_CXX_COMPILER_ID MATCHES \"Clang\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-deprecated-register\")\n  endif()\n  if(${CMAKE_SYSTEM_NAME} MATCHES \"Linux\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -pthread\")\n  endif()\nendif()\n\nif (ENABLE_ML_MLPACK_TFS)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}\") \nendif()\n\nif(${CMAKE_SYSTEM_NAME} MATCHES \"Darwin\")\n  set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -Wno-deprecated-declarations\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-deprecated-declarations\")\nendif()\n\n# address and thread sanitizer\noption(ENABLE_STANDALONE_CALCITE \"Require standalone Calcite server\" OFF)\noption(ENABLE_ASAN \"Enable address sanitizer\" OFF)\noption(ENABLE_TSAN \"Enable thread sanitizer\" OFF)\noption(ENABLE_UBSAN \"Enable undefined behavior sanitizer\" OFF)\nif(ENABLE_ASAN)\n  set(SAN_FLAGS \"-fsanitize=address -O1 -fno-omit-frame-pointer\")\n  add_definitions(\"-DHAVE_ASAN\")\n  add_definitions(\"-DWITH_DECODERS_BOUNDS_CHECKING\")\nelseif(ENABLE_TSAN)\n  add_definitions(\"-DHAVE_TSAN\")\n  add_definitions(\"-DTBB_PREVIEW_WAITING_FOR_WORKERS\") # might not be required in later versions\n  # Copy the config directory to the build dir for TSAN suppressions\n  file(COPY config DESTINATION ${CMAKE_BINARY_DIR})\n\n  set(SAN_FLAGS \"-fsanitize=thread -fPIC -O1 -fno-omit-frame-pointer\")\n  # required for older GCC, see https://gcc.gnu.org/bugzilla/show_bug.cgi?id=64354\n  add_definitions(\"-D__SANITIZE_THREAD__\")\nelseif(ENABLE_UBSAN)\n  set(SAN_FLAGS \"-fsanitize=undefined -fPIC -O1 -fno-omit-frame-pointer\")\nendif()\nif(ENABLE_ASAN OR ENABLE_TSAN OR ENABLE_UBSAN)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${SAN_FLAGS}\")\n  set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} ${SAN_FLAGS}\")\n  set(ENABLE_STANDALONE_CALCITE ON)\nendif()\n\n# Embedded database\noption(ENABLE_DBE \"Enable embedded database\" OFF)\nif(ENABLE_DBE)\n  if(NOT MSVC)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fPIC\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC\")\n  endif(NOT MSVC)\n  set(ENABLE_STANDALONE_CALCITE ON)\n  add_definitions(\"-DENABLE_EMBEDDED_DATABASE\")\n  add_definitions(\"-DDBEngine_LIBNAME=\\\"${CMAKE_SHARED_LIBRARY_PREFIX}DBEngine${CMAKE_SHARED_LIBRARY_SUFFIX}\\\"\")\nendif()\n\n# Code coverage\noption(ENABLE_CODE_COVERAGE \"Enable compile time code coverage\" OFF)\nif(ENABLE_CODE_COVERAGE)\n  if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\")\n    set(COVERAGE_FLAGS \"-fprofile-instr-generate -fcoverage-mapping\")\n  else()\n    message(FATAL_ERROR \"Code coverage currently only supported with Clang compiler\")\n  endif()\n  set(CMAKE_CXX_OUTPUT_EXTENSION_REPLACE ON)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${COVERAGE_FLAGS}\")\n  set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} ${COVERAGE_FLAGS}\")\nendif()\n\noption(ENABLE_DECODERS_BOUNDS_CHECKING \"Enable bounds checking for column decoding\" OFF)\n\nif(ENABLE_STANDALONE_CALCITE)\n  add_definitions(\"-DSTANDALONE_CALCITE\")\nendif()\n\ninclude_directories(${CMAKE_SOURCE_DIR}\n                    ${CMAKE_SOURCE_DIR}/Parser\n                    ${CMAKE_CURRENT_BINARY_DIR})\n\n## Dependencies\n\n# LLVM\nif(${CMAKE_SYSTEM_NAME} MATCHES \"Darwin\")\n  list(APPEND CMAKE_PREFIX_PATH \"/usr/local/opt/llvm\")\nendif()\nfind_package(LLVM CONFIG REQUIRED)\nif (TARGET LLVM)\n  get_target_property(LLVM_LIB LLVM IMPORTED_LOCATION_RELEASE)\nendif()\nif (NOT LLVM_LIB)\n  find_library(LLVM_LIB LLVM)\nendif()\nmessage(STATUS \"Found LLVM ${LLVM_PACKAGE_VERSION}\")\nmessage(STATUS \"Using LLVMConfig.cmake in: ${LLVM_DIR}\")\nmessage(STATUS \"Using LLVM_LIB: ${LLVM_LIB}\")\n\ninclude_directories(SYSTEM ${LLVM_INCLUDE_DIRS})\nadd_definitions(${LLVM_DEFINITIONS})\n\nfind_package(Clang CONFIG)\nif (TARGET clang-cpp)\n  get_target_property(CLANG_LIB clang-cpp IMPORTED_LOCATION_RELEASE)\nendif()\nif (NOT CLANG_LIB)\n  find_library(CLANG_LIB clang-cpp)\nendif()\nmessage(STATUS \"Using CLANG_LIB: ${CLANG_LIB}\")\n\n# Deps builds use separate libs for each clang component, while some distros now bundle into a single lib\nif (${CMAKE_SYSTEM_NAME} STREQUAL \"Darwin\" OR NOT LLVM_LIB)\n  set(LLVM_COMPONENTS support mcjit core irreader option linker)\n  if(MSVC) \n    list(APPEND LLVM_COMPONENTS Passes)\n  endif(MSVC)\n  option(ENABLE_INTEL_JIT_LISTENER \"Enable Intel Vtune JIT Listener\" OFF)\n  if(ENABLE_INTEL_JIT_LISTENER)\n    list(APPEND LLVM_COMPONENTS inteljitevents)\n  endif()\n\n  llvm_map_components_to_libnames(llvm_libs ${LLVM_TARGETS_TO_BUILD} ${LLVM_COMPONENTS})\n  set(clang_libs\n      clangFrontend\n      clangSerialization\n      clangDriver\n      clangTooling\n      clangParse\n      clangSema\n      clangAnalysis\n      clangEdit\n      clangAST\n      clangLex\n      clangBasic\n      clangRewrite\n      clangRewriteFrontend)\n\n  # LLVMSupport explicitly lists tinfo in its INTERFACE_LINK_LIBRARIES, even\n  # though we provide it in our build of ncurses. Since LLVMSupport is listed\n  # as a requirement for other llvm libs, we need to walk through the entire\n  # list in order to remove all instances of tinfo.\n  foreach(lib ${llvm_libs})\n    get_target_property(interface_libs ${lib} INTERFACE_LINK_LIBRARIES)\n    list(REMOVE_ITEM interface_libs tinfo z rt pthread -lpthread m dl)\n    set_target_properties(${lib} PROPERTIES INTERFACE_LINK_LIBRARIES \"${interface_libs}\")\n  endforeach()\n\n  list(APPEND llvm_libs ${CURSES_NCURSES_LIBRARY})\nelse()\n  if(NOT CLANG_LIB)\n    message(FATAL_ERROR \"Could not find CLANG library.\")\n  endif()\n\n  set(clang_libs ${CLANG_LIB})\n  set(llvm_libs ${LLVM_LIB})\nendif()\n\n\n# Boost\nfind_package(Boost COMPONENTS log log_setup filesystem program_options regex system thread timer locale iostreams serialization REQUIRED)\ninclude_directories(${Boost_INCLUDE_DIR})\n\n# Allow explicit include statements to access third party headers directly.\n# Ex: raft/canonical/include/raft.h\ninclude_directories(ThirdParty/)\n\n# EGL\ninclude_directories(ThirdParty/egl)\n\n# Google Test and Google Mock\nif(NOT ${CMAKE_SYSTEM_NAME} STREQUAL \"Darwin\")\n  add_definitions(\"-DGTEST_USE_OWN_TR1_TUPLE=0\")\nendif()\ninclude_directories(ThirdParty/googletest)\nadd_subdirectory(ThirdParty/googletest)\n\n# Google Benchmark\nset(BENCHMARK_ENABLE_TESTING OFF CACHE BOOL \"Suppressing benchmark's tests\" FORCE)\nif(WIN32)\n  set(HAVE_POSIX_REGEX 0)\nendif()\nadd_subdirectory(ThirdParty/googlebenchmark)\n\n# aws-sdk\nfind_package(CURL REQUIRED QUIET)\nlist(APPEND CURL_LIBRARIES ${OPENSSL_LIBRARIES})\noption(ENABLE_AWS_S3 \"Enable AWS S3 support\" ON)\nif(ENABLE_AWS_S3)\n  find_package(LibAwsS3)\n  if(NOT LibAwsS3_FOUND)\n    # Note for a build with static arrow libs (centos)\n    # aws-sdk-cpp must be included.\n    set(ENABLE_AWS_S3 OFF CACHE BOOL \"Enable AWS S3 support\" FORCE)\n  else()\n    add_definitions(\"-DHAVE_AWS_S3\")\n    set(AwsS3_CURL_SUPPORT ${CURL_LIBRARIES})\n    list (APPEND LibAwsS3_LIBRARIES ${AwsS3_CURL_SUPPORT})\n  endif()\nendif()\n# Arrow\nfind_package(Arrow REQUIRED)\nadd_definitions(\"-DARROW_NO_DEPRECATED_API\")\ninclude_directories(${Arrow_INCLUDE_DIRS})\n\noption(ENABLE_ARROW_4 \"Enable changes required to support Arrow 4.0+\" \"${HAVE_ARROW_4_IO_CONTEXT}\")\nif(ENABLE_ARROW_4)\n  add_definitions(\"-DENABLE_ARROW_4\")\nendif()\n\noption(ENABLE_IMPORT_PARQUET \"Enable Parquet Importer support\" ON)\nif(ENABLE_IMPORT_PARQUET)\n  find_package(Parquet)\n  if(NOT Parquet_FOUND)\n    set(ENABLE_IMPORT_PARQUET OFF CACHE BOOL \"Enable Parquet Importer support\" FORCE)\n    message(STATUS \"Parquet not found. Disabling Parquet Importer support.\")\n  else()\n    add_definitions(\"-DENABLE_IMPORT_PARQUET\")\n    # when we found libparquet it means we're using arrow 11+\n    # and deps scripts must have built parquet as well as snappy\n    find_package(Snappy REQUIRED)\n  endif()\nendif()\n\nlist(APPEND Arrow_LIBRARIES ${Snappy_LIBRARIES})\nif(ENABLE_AWS_S3)\n  list(INSERT Arrow_LIBRARIES 0 ${LibAwsS3_LIBRARIES})\nendif()\nif (ENABLE_CUDA)\n  list(INSERT Arrow_LIBRARIES 0 ${Arrow_GPU_CUDA_LIBRARIES})\nendif()\n\n# RapidJSON\ninclude_directories(ThirdParty/rapidjson)\nadd_definitions(-DRAPIDJSON_HAS_STDSTRING)\nif(NOT MSVC)\n  # At the present time the current vcpkg version of rapidjson is 2020-09-14:\n  # https://github.com/microsoft/vcpkg/blob/master/versions/r-/rapidjson.json\n  # and the Windows build fails because it does not have this fix:\n  # https://github.com/Tencent/rapidjson/pull/1568\n  # Once vcpkg's rapidjson has this fix then let's try not making this exception for MSVC.\n  # When this changes, remove this exception from all other similar CMakeLists.txt files too.\n  add_definitions(-DRAPIDJSON_NOMEMBERITERATORCLASS)\nendif()\n\n\n# Linenoise\nadd_subdirectory(ThirdParty/linenoise)\n\n# SQLite\ninclude_directories(ThirdParty/sqlite3)\nadd_subdirectory(ThirdParty/sqlite3)\n\n# raft/canonical\noption(ENABLE_CANONICAL_RAFT \"Enable Canonical Raft\" OFF)\nif(ENABLE_CANONICAL_RAFT)\n  add_subdirectory(ThirdParty/raft/canonical)\nendif()\n\n# rdkafka\nfind_package(RdKafka REQUIRED)\ninclude_directories(${RdKafka_INCLUDE_DIRS})\n\n# libarchive\nfind_package(LibArchive REQUIRED)\ninclude_directories(${LibArchive_INCLUDE_DIRS})\n\n#find_package(CURL REQUIRED QUIET)\n#if(CURL_FOUND)\n  #set(CURL_LIBRARIES ${LibAwsS3_SUPPORT_LIBRARIES})\n#endif()\n\n# bcrypt\ninclude_directories(ThirdParty/bcrypt)\nadd_subdirectory(ThirdParty/bcrypt)\n\n# PicoSHA2\ninclude_directories(ThirdParty/PicoSHA2)\n\nif(\"${MAPD_EDITION_LOWER}\" STREQUAL \"ee\")\n# opensaml\n  option(ENABLE_SAML \"Enable SAML support\" ON)\n  if(ENABLE_SAML)\n    find_package(OpenSaml)\n    if(NOT OpenSaml_FOUND)\n      set(ENABLE_SAML OFF CACHE BOOL \"Enable SAML support\" FORCE)\n    else()\n      add_definitions(\"-DHAVE_SAML\")\n    endif()\n  endif()\nendif()\n\n# TBB\n\noption(ENABLE_TBB \"Enable OneTBB for threading (if found)\" ON)\nset(TBB_LIBS \"\")\nfind_package(TBB)\nif(TBB_FOUND)\n  message(STATUS \"TBB library is found with ${TBB_DIR}\")\n  add_definitions(\"-DHAVE_TBB\")\n  add_definitions(\"-DTBB_PREVIEW_TASK_GROUP_EXTENSIONS\")\n  list(APPEND TBB_LIBS ${TBB_LIBRARIES})\n  if(ENABLE_TBB)\n    add_definitions(\"-DENABLE_TBB\")\n  else()\n    message(STATUS \"Using TBB for threading is DISABLED\")\n  endif()\nelse()\n  set(ENABLE_TBB OFF)\nendif()\n\noption(DISABLE_CONCURRENCY \"Disable parallellism at the threading layer\" OFF)\nif(DISABLE_CONCURRENCY)\n  add_definitions(\"-DDISABLE_CONCURRENCY\")\nendif()\n\nset(gen_cpp_files\n    ${CMAKE_BINARY_DIR}/gen-cpp/Heavy.cpp\n    ${CMAKE_BINARY_DIR}/gen-cpp/Heavy.h\n    ${CMAKE_BINARY_DIR}/gen-cpp/heavy_types.cpp\n    ${CMAKE_BINARY_DIR}/gen-cpp/common_types.cpp\n    ${CMAKE_BINARY_DIR}/gen-cpp/completion_hints_types.cpp\n    ${CMAKE_BINARY_DIR}/gen-cpp/serialized_result_set_types.cpp\n    ${CMAKE_BINARY_DIR}/gen-cpp/extension_functions_types.cpp\n    ${CMAKE_BINARY_DIR}/gen-cpp/extension_functions_types.h\n)\nadd_custom_command(\n  DEPENDS\n    ${CMAKE_SOURCE_DIR}/heavy.thrift\n    ${CMAKE_SOURCE_DIR}/common.thrift\n    ${CMAKE_SOURCE_DIR}/completion_hints.thrift\n    ${CMAKE_SOURCE_DIR}/QueryEngine/serialized_result_set.thrift\n    ${CMAKE_SOURCE_DIR}/QueryEngine/extension_functions.thrift\n  OUTPUT ${gen_cpp_files}\n  COMMAND ${Thrift_EXECUTABLE}\n  ARGS -gen cpp -r -o ${CMAKE_BINARY_DIR} ${CMAKE_SOURCE_DIR}/heavy.thrift)\nlist(APPEND ADDITIONAL_MAKE_CLEAN_FILES ${CMAKE_BINARY_DIR}/gen-cpp/)\n\nadd_custom_target(thrift_gen DEPENDS ${gen_cpp_files})\n\nadd_library(mapd_thrift ${gen_cpp_files})\n\nif(NOT MSVC)\n  target_compile_options(mapd_thrift PRIVATE -fPIC)\nendif()\n\ntarget_link_libraries(mapd_thrift ${Thrift_LIBRARIES})\n\nif(\"${MAPD_EDITION_LOWER}\" STREQUAL \"ee\")\n  option(ENABLE_OMNIVERSE_CONNECTOR \"Enable Omniverse Connector\" ON)\n  include_directories(Catalog/ee)\n  include_directories(Distributed/ee)\nelse()\n  include_directories(Catalog/os)\n  include_directories(Distributed/os)\nendif()\n\ninclude_directories(ThirdParty/muparserx)\nadd_subdirectory(ThirdParty/muparserx)\n\nset(MAPD_RENDERING_LIBRARIES \"\")\nif(NOT \"${MAPD_EDITION_LOWER}\" STREQUAL \"os\")\n  option(ENABLE_RENDERING \"Build backend renderer\" OFF)\n  if(ENABLE_RENDERING)\n    option(ENABLE_RENDER_TESTS \"Build backend renderer tests\" ON)\n\n    add_definitions(\"-DHAVE_RENDERING\")\n    add_subdirectory(Rendering)\n    add_subdirectory(QueryRenderer)\n\n    include_directories(${RENDERING_INCLUDE_DIRS})\n    set(MAPD_RENDERING_LIBRARIES QueryRenderer Rendering)\n\n    set(MAPD_PACKAGE_FLAGS \"${MAPD_PACKAGE_FLAGS}-render\")\n    if(${RENDERER_CONTEXT_TYPE} STREQUAL \"GLX\")\n      set(MAPD_PACKAGE_FLAGS \"${MAPD_PACKAGE_FLAGS}-glx\")\n    endif()\n  endif()\nelse()\n  set(ENABLE_RENDERING OFF CACHE BOOL \"Build backend renderer\" FORCE)\nendif()\n\nset(TIME_LIMITED_NUMBER_OF_DAYS \"30\" CACHE STRING \"Number of days this build is valid for if build is time limited\")\n\noption(TIME_LIMITED_BUILD \"Build Time Limited Build\" OFF)\nif(TIME_LIMITED_BUILD)\n  list(APPEND TIME_LIMITED_DEFINITIONS \"TIME_LIMITED_BUILD\")\n  list(APPEND TIME_LIMITED_DEFINITIONS \"TIME_LIMITED_NUMBER_OF_DAYS=${TIME_LIMITED_NUMBER_OF_DAYS}\")\n  set(MAPD_PACKAGE_FLAGS \"${MAPD_PACKAGE_FLAGS}-${TIME_LIMITED_NUMBER_OF_DAYS}d\")\nendif()\n\noption(ENABLE_PROFILER \"Enable google perftools\" OFF)\nif(ENABLE_PROFILER)\n  find_package(Gperftools REQUIRED COMPONENTS TCMALLOC PROFILER)\n  set(PROFILER_LIBS ${Gperftools_TCMALLOC} ${Gperftools_PROFILER})\n  add_definitions(\"-DHAVE_PROFILER\")\nelse()\n  set(PROFILER_LIBS \"\")\nendif()\n\n# Some subdirectories will add optional dependencies to initheavy so we want to declare\n# it before subdirs since it is otherwise independent.\nadd_executable(initheavy initdb.cpp)\n\nadd_subdirectory(SqliteConnector)\n\nadd_subdirectory(StringDictionary)\nadd_subdirectory(Calcite)\nget_target_property(CalciteThrift_BINARY_DIR calciteserver_thrift BINARY_DIR)\ninclude_directories(${CalciteThrift_BINARY_DIR})\n\nif(ENABLE_RUNTIME_LIBS)\n  if(ENABLE_TORCH_TFS)\n    if(NOT ENABLE_SYSTEM_TFS)\n      set(ENABLE_TORCH_TFS OFF CACHE BOOL \"Enable Torch system table functions\" FORCE)\n      message(STATUS \"System table functions must be enabled (-DENABLE_SYSTEM_TFS=ON) to use LibTorch functions, disabling LibTorch support.\")\n    else()\n      find_package(Torch REQUIRED)\n      if (DEFINED TORCH_CUDA_LIBRARIES)\n        # Torch removes the CUDA Driver target from CUDA_LIBRARIES when doing its own CUDA CMake setup, so we have to re-add it\n        list(APPEND CUDA_LIBRARIES cuda)\n        add_compile_definitions(\"HAVE_CUDA_TORCH\")\n      endif()\n      add_compile_definitions(\"HAVE_TORCH_TFS\")\n    endif()\n  endif()\n  add_subdirectory(RuntimeLibManager)\n  add_compile_definitions(\"HAVE_RUNTIME_LIBS\")\nendif()\n\nadd_subdirectory(Catalog)\nadd_subdirectory(StringOps)\nadd_subdirectory(Parser)\nadd_subdirectory(Analyzer)\nadd_subdirectory(ImportExport)\nadd_subdirectory(QueryEngine)\nadd_subdirectory(DataMgr)\nadd_subdirectory(CudaMgr)\nadd_subdirectory(L0Mgr)\nadd_subdirectory(LockMgr)\nadd_subdirectory(Logger)\nadd_subdirectory(MigrationMgr)\nadd_subdirectory(Fragmenter)\nadd_subdirectory(Shared)\nadd_subdirectory(OSDependent)\nadd_subdirectory(Utils)\nadd_subdirectory(QueryRunner)\nadd_subdirectory(SQLFrontend)\nif (NOT \"${MAPD_EDITION_LOWER}\" STREQUAL \"os\")\n  add_subdirectory(Licensing)\nendif()\nadd_subdirectory(TableArchiver)\nadd_subdirectory(Geospatial)\nadd_subdirectory(Distributed)\nadd_subdirectory(UdfCompiler)\nadd_subdirectory(ThriftHandler)\n\nif(ENABLE_DBE)\n  add_subdirectory(Embedded)\nendif()\n\noption(ENABLE_ODBC \"Build ODBC driver\" OFF)\nif(ENABLE_ODBC)\n  add_subdirectory(ODBC)\nendif()\n\nset(MAPD_LIBRARIES OSDependent Shared Catalog SqliteConnector MigrationMgr TableArchiver Parser Analyzer StringOps ImportExport QueryRunner QueryEngine QueryState LockMgr DataMgr Fragmenter Logger Geospatial)\n\nif(\"${MAPD_EDITION_LOWER}\" STREQUAL \"ee\")\n  list(APPEND MAPD_LIBRARIES Distributed)\n  if(ENABLE_DISTRIBUTED_5_0)\n    list(APPEND MAPD_LIBRARIES StringDictionaryThread)\n  endif()\nendif()\n\nlist(APPEND MAPD_LIBRARIES Calcite)\n\nif(ENABLE_RENDERING)\n  add_dependencies(QueryRenderer Parser)\n  list(APPEND MAPD_LIBRARIES ${MAPD_RENDERING_LIBRARIES})\nendif()\n\nlist(APPEND MAPD_LIBRARIES ${Arrow_LIBRARIES})\n\nif(ENABLE_FOLLY)\n  list(APPEND MAPD_LIBRARIES ${Folly_LIBRARIES})\nendif()\n\nif(ENABLE_PDAL)\n  # Correct link issue in test programs\n  list(APPEND GDAL_LIBRARIES ${CURL_LIBRARIES})\n  list(APPEND MAPD_LIBRARIES ${PDAL_LIBRARIES} ${GDAL_LIBRARIES} ${GDALExtra_LIBRARIES})\nendif()\n\nif(ENABLE_LICENSING_AWS)\n  list(APPEND MAPD_LIBRARIES AWSMarketplace)\nendif()\n\nif (ENABLE_ML_ONEDAL_TFS)\n    list(APPEND MAPD_LIBRARIES ${DAL_LIBRARIES})\n    list(APPEND TBB_LIBS ${TBB_MALLOC_LIBRARY})\nendif()\n\nif(ENABLE_ML_MLPACK_TFS)\n  list(APPEND MAPD_LIBRARIES ${MLPACK_LIBRARIES} ${ARMADILLO_LIBRARIES})\nendif()\n\nlist(APPEND MAPD_LIBRARIES ${TBB_LIBS})\n\nif(ENABLE_CANONICAL_RAFT)\n  list(APPEND MAPD_LIBRARIES raft_canonical)\nendif()\n\noption(ENABLE_TESTS \"Build unit tests\" ON)\nif (ENABLE_TESTS)\n  enable_testing()\n  option(ENABLE_INIT_DIR \"Add test dependency on db init\" ON)\n  add_subdirectory(Tests)\n  add_subdirectory(SampleCode)\n  if (ENABLE_RUNTIME_LIBS)\n    add_subdirectory(QueryEngine/TableFunctions/RuntimeLibTestFunctions)\n  endif()\nendif()\n\nif(ENABLE_RENDERING AND ENABLE_RENDER_TESTS)\n  enable_testing()\n  add_subdirectory(Tests/RenderTests)\nendif()\n\nif (ENABLE_MEMKIND OR ENABLE_RENDERING)\n  set(CMAKE_INSTALL_RPATH \"$ORIGIN/../ThirdParty/lib\")\nendif()\n\nadd_executable(heavydb HeavyDB.cpp ${CMAKE_BINARY_DIR}/MapDRelease.h)\nset_target_properties(heavydb PROPERTIES COMPILE_DEFINITIONS \"${TIME_LIMITED_DEFINITIONS}\")\nInstallVersionFile()\n\nadd_custom_command(\n    DEPENDS ${CMAKE_SOURCE_DIR}/heavy.thrift\n    OUTPUT\n        ${CMAKE_SOURCE_DIR}/java/thrift/src/gen/ai/heavy/thrift/server/Heavy.java\n        ${CMAKE_SOURCE_DIR}/java/thrift/src/gen/ai/heavy/thrift/server/TRow.java\n    COMMAND ${CMAKE_COMMAND} -E make_directory ${CMAKE_SOURCE_DIR}/java/thrift/src/gen\n    COMMAND ${Thrift_EXECUTABLE}\n    ARGS -gen java -r -out ${CMAKE_SOURCE_DIR}/java/thrift/src/gen/ ${CMAKE_SOURCE_DIR}/heavy.thrift)\n\nadd_custom_command(\n    DEPENDS ${CMAKE_SOURCE_DIR}/common.thrift\n    OUTPUT\n        ${CMAKE_SOURCE_DIR}/java/thrift/src/gen/ai/heavy/thrift/server/common.java\n    COMMAND ${CMAKE_COMMAND} -E make_directory ${CMAKE_SOURCE_DIR}/java/thrift/src/gen\n    COMMAND ${Thrift_EXECUTABLE}\n    ARGS -gen java -r -out ${CMAKE_SOURCE_DIR}/java/thrift/src/gen/ ${CMAKE_SOURCE_DIR}/common.thrift)\n\nadd_custom_command(\n    DEPENDS ${CMAKE_SOURCE_DIR}/QueryEngine/serialized_result_set.thrift\n    OUTPUT\n        ${CMAKE_SOURCE_DIR}/java/thrift/src/gen/ai/heavy/thrift/server/serialized_result_set.java\n    COMMAND ${CMAKE_COMMAND} -E make_directory ${CMAKE_SOURCE_DIR}/java/thrift/src/gen\n    COMMAND ${Thrift_EXECUTABLE}\n    ARGS -gen java -r -out ${CMAKE_SOURCE_DIR}/java/thrift/src/gen/ ${CMAKE_SOURCE_DIR}/QueryEngine/serialized_result_set.thrift)\n\nadd_custom_command(\n    DEPENDS ${CMAKE_SOURCE_DIR}/java/thrift/calciteserver.thrift ${CMAKE_SOURCE_DIR}/completion_hints.thrift ${CMAKE_SOURCE_DIR}/QueryEngine/extension_functions.thrift\n    OUTPUT ${CMAKE_SOURCE_DIR}/java/thrift/src/gen/ai/heavy/thrift/calciteserver/CalciteServer.java\n    COMMAND ${CMAKE_COMMAND} -E make_directory ${CMAKE_SOURCE_DIR}/java/thrift/src/gen\n    COMMAND ${Thrift_EXECUTABLE}\n    ARGS -gen java -r -I ${CMAKE_SOURCE_DIR} -out ${CMAKE_SOURCE_DIR}/java/thrift/src/gen/ ${CMAKE_SOURCE_DIR}/java/thrift/calciteserver.thrift)\n\nlist(APPEND ADDITIONAL_MAKE_CLEAN_FILES ${CMAKE_SOURCE_DIR}/java/thrift/src/gen/)\n\n# Check if the git hash file exists and set the CPack version if it does\n# If not, get the hash from git and create the file\nexecute_process(\n  WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\n  COMMAND ${GIT_EXECUTABLE} rev-parse --short=10 HEAD\n  OUTPUT_VARIABLE MAPD_GIT_HASH\n)\n\n# Ensure the git hash file exists\nfile(WRITE ${CMAKE_BINARY_DIR}/heavyai_git_hash.txt \"${MAPD_GIT_HASH}\")\n\n# Make the CMake configuration dependent on the git hash file\n# This will trigger a re-run of CMake if it changes, updating the CPack and version files\nset_property(\n  DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${CMAKE_BINARY_DIR}/heavyai_git_hash.txt\n)\n\n# Ensure MAPD_GIT_HASH is set and embed in CPack\nfile(STRINGS ${CMAKE_BINARY_DIR}/heavyai_git_hash.txt MAPD_GIT_HASH)\nset(CPACK_PACKAGE_VERSION \"${MAPD_VERSION_RAW}-${MAPD_BUILD_DATE}-${MAPD_GIT_HASH}\")\n\nconfigure_file(\n  \"${CMAKE_CURRENT_SOURCE_DIR}/Shared/release.h\"\n  \"${CMAKE_BINARY_DIR}/MapDRelease.h\"\n  @ONLY\n  )\n\n# On every build, check if the git hash has changed, and if so update cache file\n# triggering a reconfigure\nadd_custom_target(get_git_hash ALL\n  COMMENT \"Checking git hash\"\n  BYPRODUCTS ${CMAKE_BINARY_DIR}/heavyai_git_hash_new.txt\n  OUTPUT ${CMAKE_BINARY_DIR}/heavyai_git_hash.txt\n  WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\n  COMMAND ${GIT_EXECUTABLE} rev-parse --short=10 HEAD > ${CMAKE_BINARY_DIR}/heavyai_git_hash_new.txt\n  COMMAND bash -c \"${CMAKE_SOURCE_DIR}/scripts/update_git_hash.sh ${CMAKE_BINARY_DIR}\"\n)\n\nlist(APPEND ADDITIONAL_MAKE_CLEAN_FILES ${CMAKE_BINARY_DIR}/heavyai_git_hash.txt)\nlist(APPEND ADDITIONAL_MAKE_CLEAN_FILES ${CMAKE_BINARY_DIR}/MapDRelease.h)\n\ntarget_link_libraries(heavydb mapd_thrift thrift_handler ${MAPD_LIBRARIES} ${Boost_LIBRARIES} ${CMAKE_DL_LIBS} ${CUDA_LIBRARIES} ${PROFILER_LIBS} ${ZLIB_LIBRARIES} ${LOCALE_LINK_FLAG})\n\ntarget_link_libraries(initheavy mapd_thrift thrift_handler ${MAPD_LIBRARIES} ${Boost_LIBRARIES} ${CMAKE_DL_LIBS}\n    ${CUDA_LIBRARIES} ${PROFILER_LIBS} ${ZLIB_LIBRARIES} ${BLOSC_LIBRARIES}\n    ${LOCALE_LINK_FLAG})\n\nmacro(set_dpkg_arch arch_in arch_out)\n  if(\"${arch_in}\" STREQUAL \"x86_64\")\n    set(${arch_out} \"amd64\")\n  elseif(\"${arch_in}\" STREQUAL \"aarch64\")\n    set(${arch_out} \"arm64\")\n  elseif(\"${arch_in}\" STREQUAL \"ppc64le\")\n    set(${arch_out} \"ppc64el\")\n  else()\n    set(${arch_out} \"${arch_in}\")\n  endif()\nendmacro()\n\n# clang-tidy\nfind_program(JQ_EXECUTABLE NAMES jq)\nif (NOT ${JQ_EXECUTABLE} STREQUAL \"JQ_EXECUTABLE-NOTFOUND\")\n  file(WRITE ${CMAKE_BINARY_DIR}/jq.filter \"map(select(.file | test(\\\".*/(build|ThirdParty)/.*\\\") | not))\")\n  add_custom_target(run-clang-tidy\n    COMMAND mkdir -p clang-tidy\n    COMMAND ${JQ_EXECUTABLE} -f jq.filter ${CMAKE_BINARY_DIR}/compile_commands.json > clang-tidy/compile_commands.json\n    COMMAND cd clang-tidy && ${CMAKE_SOURCE_DIR}/ThirdParty/clang/run-clang-tidy.py -quiet -format -fix -header-filter=\"${CMAKE_SOURCE_DIR}/.*\" 2> /dev/null\n    WORKING_DIRECTORY ${CMAKE_BINARY_DIR}\n  )\nelse()\n  message(STATUS \"jq not found, disabling run-clang-tidy target\")\nendif()\n\n# doxygen\nfind_package(Doxygen)\nif(DOXYGEN_FOUND)\n  include(${CMAKE_CURRENT_SOURCE_DIR}/docs/CMakeLists.txt)\nendif(DOXYGEN_FOUND)\n\n# Packaging\n\nif(NOT \"${CMAKE_BUILD_TYPE_LOWER}\" STREQUAL \"debug\" AND NOT \"${CMAKE_BUILD_TYPE_LOWER}\" STREQUAL \"relwithdebinfo\")\n  set(CPACK_STRIP_FILES ON)\nelse()\n  set(MAPD_PACKAGE_FLAGS \"${MAPD_PACKAGE_FLAGS}-debug\")\nendif()\nset(CPACK_PACKAGE_VENDOR \"HEAVY.AI, Inc.\")\nset(CPACK_PACKAGE_CONTACT \"support@heavy.ai\")\nset(CPACK_PACKAGE_DESCRIPTION_SUMMARY \"HeavyDB Database\")\nset(CPACK_PROJECT_CONFIG_FILE ${CMAKE_SOURCE_DIR}/CMakePackaging.txt)\nset(CPACK_DEBIAN_PACKAGE_DEPENDS \"default-jre-headless | openjdk-8-jre-headless | java8-runtime-headless, bsdmainutils, curl | wget\")\nset(CPACK_RPM_PACKAGE_REQUIRES \"java-headless, util-linux, curl\")\nset(CPACK_RPM_PACKAGE_AUTOREQ OFF)\nset(CPACK_RPM_SPEC_MORE_DEFINE \"%define __jar_repack %{nil}\")\nif(\"${MAPD_EDITION_LOWER}\" STREQUAL \"ee\")\n  set(CPACK_DEBIAN_PACKAGE_DEPENDS \"${CPACK_DEBIAN_PACKAGE_DEPENDS}, libldap-2.4-2\")\nendif()\nif(ENABLE_RENDERING)\n  set(CPACK_RPM_PACKAGE_REQUIRES \"${CPACK_RPM_PACKAGE_REQUIRES}, libX11, libXext\")\n  set(CPACK_DEBIAN_PACKAGE_DEPENDS \"${CPACK_DEBIAN_PACKAGE_DEPENDS}, libx11-6, libxext6\")\n  get_filename_component(VULKAN_LIBRARY_PATH ${VULKAN_LIBRARY} DIRECTORY)\n  install(DIRECTORY ${VULKAN_LIBRARY_PATH} DESTINATION \"ThirdParty\" FILES_MATCHING PATTERN \"libvulkan.so*\")\nendif()\n\nset_dpkg_arch(${CMAKE_SYSTEM_PROCESSOR} CPACK_DEBIAN_PACKAGE_ARCHITECTURE)\n\ninstall(DIRECTORY \"${CMAKE_CURRENT_SOURCE_DIR}/ThirdParty/licenses\" DESTINATION \"ThirdParty\" COMPONENT \"doc\")\n\n# heavydbTypes.h local includes (for UDF/UDTF)\ninstall(FILES Shared/funcannotations.h DESTINATION \"Shared/\" COMPONENT \"include\")\ninstall(FILES Shared/InlineNullValues.h DESTINATION \"Shared/\" COMPONENT \"include\")\ninstall(FILES Logger/Logger.h DESTINATION \"Logger/\" COMPONENT \"include\")\n\n# Frontend\noption(MAPD_IMMERSE_DOWNLOAD \"Download OmniSci Immerse for packaging\" OFF)\nset(MAPD_IMMERSE_URL ${MAPD_IMMERSE_URL} CACHE STRING \"URL to bundled frontend\")\nif(MAPD_IMMERSE_DOWNLOAD)\n  include(ExternalProject)\n  externalproject_add(frontend\n    URL ${MAPD_IMMERSE_URL}\n    PREFIX external\n    CONFIGURE_COMMAND \"\"\n    UPDATE_COMMAND \"\"\n    BUILD_COMMAND \"\"\n    INSTALL_COMMAND \"\"\n    LOG_DOWNLOAD on\n    DOWNLOAD_EXTRACT_TIMESTAMP true\n    )\n  externalproject_get_property(frontend source_dir)\n\n  install(DIRECTORY ${source_dir}/ DESTINATION \"frontend/\" PATTERN .git EXCLUDE PATTERN node_modules EXCLUDE)\n  add_custom_command(TARGET frontend COMMAND ${CMAKE_COMMAND} -E copy_directory ${source_dir} frontend)\n  list(APPEND ADDITIONAL_MAKE_CLEAN_FILES ${CMAKE_BINARY_DIR}/frontend)\n\n  ## Go web server\n  if(\"${MAPD_EDITION_LOWER}\" STREQUAL \"ee\" AND EXISTS \"${CMAKE_CURRENT_SOURCE_DIR}/Immerse\")\n    add_subdirectory(Immerse)\n  endif()\nendif()\n\n# HeavyIQ\noption(HEAVYIQ_DOWNLOAD \"Download HeavyIQ for packaging\" ON)\nif(HEAVYIQ_DOWNLOAD)\n  if(\"${MAPD_EDITION_LOWER}\" STREQUAL \"ee\" AND EXISTS \"${CMAKE_CURRENT_SOURCE_DIR}/HeavyIQ\")\n    add_subdirectory(HeavyIQ)\n  endif()\nendif()\n\nadd_subdirectory(ThirdParty/generate_cert)\n\n# systemd\nif(${CMAKE_SYSTEM_NAME} STREQUAL \"Linux\")\n  if(\"${MAPD_EDITION_LOWER}\" STREQUAL \"ee\")\n    install(FILES systemd/heavydb_sd_server.service.in systemd/heavydb_sd_server@.service.in DESTINATION systemd)\n    install(FILES systemd/heavy-sds.conf.in DESTINATION systemd)\n  endif()\n  install(FILES systemd/heavydb.service.in systemd/heavydb@.service.in DESTINATION systemd)\n  install(FILES systemd/heavy.conf.in DESTINATION systemd)\n  install(PROGRAMS systemd/install_heavy_systemd.sh DESTINATION systemd)\nendif()\n\n## mvn process for java code\nfind_program(MVN_EXECUTABLE NAMES mvn)\nif(NOT MVN_EXECUTABLE)\n  message(FATAL_ERROR \"mvn not found. Install Apache Maven.\")\nendif()\nfile(GLOB_RECURSE JAVA_POM RELATIVE ${CMAKE_SOURCE_DIR} java/**/pom.xml)\nfile(GLOB_RECURSE JAVA_FTL RELATIVE ${CMAKE_SOURCE_DIR} java/calcite/src/main/codegen/includes/*.ftl)\nfile(GLOB_RECURSE JAVA_SOURCES RELATIVE ${CMAKE_SOURCE_DIR} java/**/*.java)\nlist(FILTER JAVA_SOURCES EXCLUDE REGEX \".*/gen/.*\")\nlist(FILTER JAVA_SOURCES EXCLUDE REGEX \".*/generated-sources/.*\")\n\nset(OMNISCI_JAR_RELEASE_VERSION \"${MAPD_VERSION_MAJOR}.${MAPD_VERSION_MINOR}.${MAPD_VERSION_PATCH}\")\nif(\"${MAPD_VERSION_EXTRA}\" STREQUAL \"dev\")\n  set (OMNISCI_JAR_RELEASE_VERSION \"${OMNISCI_JAR_RELEASE_VERSION}-SNAPSHOT\")\nendif()\n\nset (JDBC_JAR \"heavyai-jdbc-${OMNISCI_JAR_RELEASE_VERSION}.jar\")\nset (UTILITY_JAR \"heavyai-utility-${OMNISCI_JAR_RELEASE_VERSION}.jar\")\n\nset(MVN_PATH_COMMAND \"\")\nif(NOT MSVC) \n    set(MVN_PATH_COMMAND \"MVNPATH=${CMAKE_SOURCE_DIR}/java\")\nendif()\n\nadd_custom_command(\n  OUTPUT\n    ${CMAKE_BINARY_DIR}/bin/${UTILITY_JAR}\n    ${CMAKE_BINARY_DIR}/bin/${JDBC_JAR}\n    ${CMAKE_BINARY_DIR}/bin/calcite-1.0-SNAPSHOT-jar-with-dependencies.jar\n    COMMAND ${MVN_PATH_COMMAND} ${MVN_EXECUTABLE} -T 4 -l ${CMAKE_BINARY_DIR}/mvn_build.log -e clean install -Dthrift.version=\"${Thrift_VERSION}\" -Dmaven.compiler.showDeprecation=true -Dmaven.compiler.showWarnings=true -Domnisci.release.version=\"${OMNISCI_JAR_RELEASE_VERSION}\" -Djava.net.preferIPv4Stack=true -Dmaven.wagon.http.retryHandler.count=3 -DLOG_DIR=\"${CMAKE_BINARY_DIR}\"\n    COMMAND ${CMAKE_COMMAND} -E copy ${CMAKE_SOURCE_DIR}/java/heavydb/target/${UTILITY_JAR} ${CMAKE_BINARY_DIR}/bin\n    COMMAND ${CMAKE_COMMAND} -E copy ${CMAKE_SOURCE_DIR}/java/heavyaijdbc/target/${JDBC_JAR} ${CMAKE_BINARY_DIR}/bin\n    COMMAND ${CMAKE_COMMAND} -E copy ${CMAKE_SOURCE_DIR}/java/calcite/target/calcite-1.0-SNAPSHOT-jar-with-dependencies.jar ${CMAKE_BINARY_DIR}/bin\n    WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}/java\n  DEPENDS\n    ${CMAKE_SOURCE_DIR}/java/thrift/src/gen/ai/heavy/thrift/server/Heavy.java\n    ${CMAKE_SOURCE_DIR}/java/thrift/src/gen/ai/heavy/thrift/calciteserver/CalciteServer.java\n    ${CMAKE_SOURCE_DIR}/java/calcite/src/main/codegen/config.fmpp\n    ${CMAKE_SOURCE_DIR}/java/pom.xml\n    ${CMAKE_SOURCE_DIR}/heavy.thrift\n    ${JAVA_POM}\n    ${JAVA_SOURCES}\n    ${JAVA_FTL}\n  )\nadd_custom_target(mapd_java_components ALL DEPENDS\n  ${CMAKE_BINARY_DIR}/bin/${UTILITY_JAR}\n  ${CMAKE_BINARY_DIR}/bin/${JDBC_JAR}\n  ${CMAKE_BINARY_DIR}/bin/calcite-1.0-SNAPSHOT-jar-with-dependencies.jar)\nadd_custom_target(mapd_java_clean\n  COMMAND ${MVN_PATH_COMMAND} ${MVN_EXECUTABLE} -q clean -Domnisci.release.version=\"${OMNISCI_JAR_RELEASE_VERSION}\"\n  WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}/java\n  )\nadd_dependencies(clean-all mapd_java_clean)\ninstall(FILES ${CMAKE_SOURCE_DIR}/java/heavydb/target/${UTILITY_JAR} DESTINATION bin COMPONENT \"jar\")\ninstall(FILES ${CMAKE_SOURCE_DIR}/java/heavyaijdbc/target/${JDBC_JAR} DESTINATION bin COMPONENT \"jar\")\ninstall(FILES ${CMAKE_SOURCE_DIR}/java/calcite/target/calcite-1.0-SNAPSHOT-jar-with-dependencies.jar DESTINATION bin COMPONENT \"jar\")\n\nadd_custom_target(maven_populate_cache\n  COMMAND ${MVN_PATH_COMMAND} ${MVN_EXECUTABLE} -q verify -Domnisci.release.version=\"${OMNISCI_JAR_RELEASE_VERSION}\"\n  WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}/java\n  )\n\nset_directory_properties(PROPERTIES ADDITIONAL_MAKE_CLEAN_FILES \"${ADDITIONAL_MAKE_CLEAN_FILES}\")\n\ninstall(TARGETS initheavy heavydb DESTINATION bin COMPONENT \"exe\")\ninstall(FILES ${CMAKE_BINARY_DIR}/heavyai_git_hash.txt DESTINATION \".\" COMPONENT \"doc\")\nif(ENABLE_CUDA)\n  install(FILES ${CMAKE_BINARY_DIR}/QueryEngine/cuda_mapd_rt.fatbin DESTINATION QueryEngine COMPONENT \"exe\")\nendif()\ninstall(FILES completion_hints.thrift DESTINATION \".\" COMPONENT \"thrift\")\ninstall(FILES heavy.thrift DESTINATION \".\" COMPONENT \"thrift\")\ninstall(FILES common.thrift DESTINATION \".\" COMPONENT \"thrift\")\ninstall(FILES QueryEngine/serialized_result_set.thrift DESTINATION \"QueryEngine/\" COMPONENT \"thrift\")\ninstall(FILES QueryEngine/extension_functions.thrift DESTINATION \"QueryEngine/\" COMPONENT \"thrift\")\n\nif(NOT PREFER_STATIC_LIBS AND NOT ENABLE_CONDA)\n  install(FILES ${Boost_LIBRARIES} DESTINATION ThirdParty/lib)\nendif()\n\nif(\"${MAPD_EDITION_LOWER}\" STREQUAL \"ee\")\n  set(EULA_FILE \"${CMAKE_SOURCE_DIR}/EULA-EE.txt\")\nelse()\n  set(EULA_FILE \"${CMAKE_SOURCE_DIR}/LICENSE.md\")\nendif()\n\nif(\"${MAPD_EDITION_LOWER}\" STREQUAL \"os\")\n  install(FILES LICENSE.md DESTINATION \".\" COMPONENT \"doc\")\nendif()\n\nset(CPACK_RESOURCE_FILE_LICENSE \"${EULA_FILE}\")\ninstall(FILES \"${EULA_FILE}\" DESTINATION \".\"  COMPONENT \"doc\")\n\ninstall(PROGRAMS startheavy DESTINATION \".\")\ninstall(PROGRAMS scripts/innerstartheavy DESTINATION \"scripts\")\nif(\"${MAPD_EDITION_LOWER}\" STREQUAL \"ee\")\n  install(PROGRAMS scripts/ee/start_heavyiq.sh DESTINATION \"scripts/ee\")\nendif()\ninstall(PROGRAMS insert_sample_data DESTINATION \".\")\n\nif(\"${MAPD_EDITION_LOWER}\" STREQUAL \"ee\")\n  if(ENABLE_CUDA)\n    install(FILES docker/sds/Dockerfile.cuda RENAME Dockerfile DESTINATION \"docker/sds\")\n  else()\n    install(FILES docker/sds/Dockerfile.cpu RENAME Dockerfile DESTINATION \"docker/sds\")\n  endif()\nendif()\ninstall(FILES docker/README.md DESTINATION \"docker\")\nif(ENABLE_CUDA OR ENABLE_RENDERING)\n  install(FILES docker/Dockerfile.cuda RENAME Dockerfile DESTINATION \"docker\")\nelse()\n  install(FILES docker/Dockerfile.cpu RENAME Dockerfile DESTINATION \"docker\")\nendif()\n\nexec_program(uname ARGS -m OUTPUT_VARIABLE MAPD_HOST_SYSTEM_ARCHITECTURE) # does not account for cross-compiling or Windows\nset(CPACK_PACKAGE_FILE_NAME \"${CMAKE_PROJECT_NAME}-${MAPD_EDITION_LOWER}-${CPACK_PACKAGE_VERSION}-${CMAKE_SYSTEM_NAME}-${MAPD_HOST_SYSTEM_ARCHITECTURE}${MAPD_PACKAGE_FLAGS}\")\n\nset(CPACK_GENERATOR \"STGZ\")\n\ninclude(CPack)\n\nif(DOXYGEN_FOUND)\n  add_custom_target(sphinx\n    COMMAND python3 -m venv sphinx-env\n    COMMAND . sphinx-env/bin/activate && pip install -r requirements.txt\n    COMMAND rm -rf build\n    COMMAND . sphinx-env/bin/activate && make html SPHINXOPTS=\"-D version=${MAPD_VERSION_MAJOR}.${MAPD_VERSION_MINOR}.${MAPD_VERSION_PATCH}\"\n    WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}/docs\n    )\n\n  add_dependencies(sphinx doxygen)\nendif(DOXYGEN_FOUND)\n\nadd_dependencies(heavydb get_git_hash)\n\n# heavy: build our main executables (the entire bin/ directory)\nlist(APPEND HEAVY_TARGETS heavydb initheavy heavysql)\nadd_dependencies(initheavy mapd_java_components)\nadd_dependencies(heavydb mapd_java_components)\nadd_custom_target(heavy ALL DEPENDS ${HEAVY_TARGETS})\n"
        },
        {
          "name": "CMakePackaging.txt",
          "type": "blob",
          "size": 0.171875,
          "content": "if(CPACK_GENERATOR MATCHES \"DEB\" OR CPACK_GENERATOR MATCHES \"RPM\")\n  set(CPACK_PACKAGING_INSTALL_PREFIX \"/opt/heavyai\")\nelse()\n  set(CPACK_PACKAGING_INSTALL_PREFIX \"\")\nendif()\n"
        },
        {
          "name": "Calcite",
          "type": "tree",
          "content": null
        },
        {
          "name": "Catalog",
          "type": "tree",
          "content": null
        },
        {
          "name": "CudaMgr",
          "type": "tree",
          "content": null
        },
        {
          "name": "DataMgr",
          "type": "tree",
          "content": null
        },
        {
          "name": "Distributed",
          "type": "tree",
          "content": null
        },
        {
          "name": "Doxyfile.in",
          "type": "blob",
          "size": 106.3896484375,
          "content": "# Doxyfile 1.8.14\n\n# This file describes the settings to be used by the documentation system\n# doxygen (www.doxygen.org) for a project.\n#\n# All text after a double hash (##) is considered a comment and is placed in\n# front of the TAG it is preceding.\n#\n# All text after a single hash (#) is considered a comment and will be ignored.\n# The format is:\n# TAG = value [value, ...]\n# For lists, items can also be appended using:\n# TAG += value [value, ...]\n# Values that contain spaces should be placed between quotes (\\\" \\\").\n\n#---------------------------------------------------------------------------\n# Project related configuration options\n#---------------------------------------------------------------------------\n\n# This tag specifies the encoding used for all characters in the config file\n# that follow. The default is UTF-8 which is also the encoding used for all text\n# before the first occurrence of this tag. Doxygen uses libiconv (or the iconv\n# built into libc) for the transcoding. See\n# https://www.gnu.org/software/libiconv/ for the list of possible encodings.\n# The default value is: UTF-8.\n\nDOXYFILE_ENCODING      = UTF-8\n\n# The PROJECT_NAME tag is a single word (or a sequence of words surrounded by\n# double-quotes, unless you are using Doxywizard) that should identify the\n# project for which the documentation is generated. This name is used in the\n# title of most generated pages and in a few other places.\n# The default value is: My Project.\n\nPROJECT_NAME           = \"OmniSciDB\"\n\n# The PROJECT_NUMBER tag can be used to enter a project or revision number. This\n# could be handy for archiving the generated documentation or if some version\n# control system is used.\n\nPROJECT_NUMBER         = \"@MAPD_GIT_HASH@\"\n\n# Using the PROJECT_BRIEF tag one can provide an optional one line description\n# for a project that appears at the top of each page and should give viewer a\n# quick idea about the purpose of the project. Keep the description short.\n\nPROJECT_BRIEF          =\n\n# With the PROJECT_LOGO tag one can specify a logo or an icon that is included\n# in the documentation. The maximum height of the logo should not exceed 55\n# pixels and the maximum width should not exceed 200 pixels. Doxygen will copy\n# the logo to the output directory.\n\nPROJECT_LOGO           =\n\n# The OUTPUT_DIRECTORY tag is used to specify the (relative or absolute) path\n# into which the generated documentation will be written. If a relative path is\n# entered, it will be relative to the location where doxygen was started. If\n# left blank the current directory will be used.\n\nOUTPUT_DIRECTORY       = @CMAKE_BINARY_DIR@/doxygen\n\n# If the CREATE_SUBDIRS tag is set to YES then doxygen will create 4096 sub-\n# directories (in 2 levels) under the output directory of each output format and\n# will distribute the generated files over these directories. Enabling this\n# option can be useful when feeding doxygen a huge amount of source files, where\n# putting all generated files in the same directory would otherwise causes\n# performance problems for the file system.\n# The default value is: NO.\n\nCREATE_SUBDIRS         = NO\n\n# If the ALLOW_UNICODE_NAMES tag is set to YES, doxygen will allow non-ASCII\n# characters to appear in the names of generated files. If set to NO, non-ASCII\n# characters will be escaped, for example _xE3_x81_x84 will be used for Unicode\n# U+3044.\n# The default value is: NO.\n\nALLOW_UNICODE_NAMES    = NO\n\n# The OUTPUT_LANGUAGE tag is used to specify the language in which all\n# documentation generated by doxygen is written. Doxygen will use this\n# information to generate all constant output in the proper language.\n# Possible values are: Afrikaans, Arabic, Armenian, Brazilian, Catalan, Chinese,\n# Chinese-Traditional, Croatian, Czech, Danish, Dutch, English (United States),\n# Esperanto, Farsi (Persian), Finnish, French, German, Greek, Hungarian,\n# Indonesian, Italian, Japanese, Japanese-en (Japanese with English messages),\n# Korean, Korean-en (Korean with English messages), Latvian, Lithuanian,\n# Macedonian, Norwegian, Persian (Farsi), Polish, Portuguese, Romanian, Russian,\n# Serbian, Serbian-Cyrillic, Slovak, Slovene, Spanish, Swedish, Turkish,\n# Ukrainian and Vietnamese.\n# The default value is: English.\n\nOUTPUT_LANGUAGE        = English\n\n# If the BRIEF_MEMBER_DESC tag is set to YES, doxygen will include brief member\n# descriptions after the members that are listed in the file and class\n# documentation (similar to Javadoc). Set to NO to disable this.\n# The default value is: YES.\n\nBRIEF_MEMBER_DESC      = YES\n\n# If the REPEAT_BRIEF tag is set to YES, doxygen will prepend the brief\n# description of a member or function before the detailed description\n#\n# Note: If both HIDE_UNDOC_MEMBERS and BRIEF_MEMBER_DESC are set to NO, the\n# brief descriptions will be completely suppressed.\n# The default value is: YES.\n\nREPEAT_BRIEF           = YES\n\n# This tag implements a quasi-intelligent brief description abbreviator that is\n# used to form the text in various listings. Each string in this list, if found\n# as the leading text of the brief description, will be stripped from the text\n# and the result, after processing the whole list, is used as the annotated\n# text. Otherwise, the brief description is used as-is. If left blank, the\n# following values are used ($name is automatically replaced with the name of\n# the entity):The $name class, The $name widget, The $name file, is, provides,\n# specifies, contains, represents, a, an and the.\n\nABBREVIATE_BRIEF       = \"The $name class\" \\\n                         \"The $name widget\" \\\n                         \"The $name file\" \\\n                         is \\\n                         provides \\\n                         specifies \\\n                         contains \\\n                         represents \\\n                         a \\\n                         an \\\n                         the\n\n# If the ALWAYS_DETAILED_SEC and REPEAT_BRIEF tags are both set to YES then\n# doxygen will generate a detailed section even if there is only a brief\n# description.\n# The default value is: NO.\n\nALWAYS_DETAILED_SEC    = NO\n\n# If the INLINE_INHERITED_MEMB tag is set to YES, doxygen will show all\n# inherited members of a class in the documentation of that class as if those\n# members were ordinary class members. Constructors, destructors and assignment\n# operators of the base classes will not be shown.\n# The default value is: NO.\n\nINLINE_INHERITED_MEMB  = NO\n\n# If the FULL_PATH_NAMES tag is set to YES, doxygen will prepend the full path\n# before files name in the file list and in the header files. If set to NO the\n# shortest path that makes the file name unique will be used\n# The default value is: YES.\n\nFULL_PATH_NAMES        = YES\n\n# The STRIP_FROM_PATH tag can be used to strip a user-defined part of the path.\n# Stripping is only done if one of the specified strings matches the left-hand\n# part of the path. The tag can be used to show relative paths in the file list.\n# If left blank the directory from which doxygen is run is used as the path to\n# strip.\n#\n# Note that you can specify absolute paths here, but also relative paths, which\n# will be relative from the directory where doxygen is started.\n# This tag requires that the tag FULL_PATH_NAMES is set to YES.\n\nSTRIP_FROM_PATH        =\n\n# The STRIP_FROM_INC_PATH tag can be used to strip a user-defined part of the\n# path mentioned in the documentation of a class, which tells the reader which\n# header file to include in order to use a class. If left blank only the name of\n# the header file containing the class definition is used. Otherwise one should\n# specify the list of include paths that are normally passed to the compiler\n# using the -I flag.\n\nSTRIP_FROM_INC_PATH    =\n\n# If the SHORT_NAMES tag is set to YES, doxygen will generate much shorter (but\n# less readable) file names. This can be useful is your file systems doesn't\n# support long names like on DOS, Mac, or CD-ROM.\n# The default value is: NO.\n\nSHORT_NAMES            = NO\n\n# If the JAVADOC_AUTOBRIEF tag is set to YES then doxygen will interpret the\n# first line (until the first dot) of a Javadoc-style comment as the brief\n# description. If set to NO, the Javadoc-style will behave just like regular Qt-\n# style comments (thus requiring an explicit @brief command for a brief\n# description.)\n# The default value is: NO.\n\nJAVADOC_AUTOBRIEF      = NO\n\n# If the QT_AUTOBRIEF tag is set to YES then doxygen will interpret the first\n# line (until the first dot) of a Qt-style comment as the brief description. If\n# set to NO, the Qt-style will behave just like regular Qt-style comments (thus\n# requiring an explicit \\brief command for a brief description.)\n# The default value is: NO.\n\nQT_AUTOBRIEF           = NO\n\n# The MULTILINE_CPP_IS_BRIEF tag can be set to YES to make doxygen treat a\n# multi-line C++ special comment block (i.e. a block of //! or /// comments) as\n# a brief description. This used to be the default behavior. The new default is\n# to treat a multi-line C++ comment block as a detailed description. Set this\n# tag to YES if you prefer the old behavior instead.\n#\n# Note that setting this tag to YES also means that rational rose comments are\n# not recognized any more.\n# The default value is: NO.\n\nMULTILINE_CPP_IS_BRIEF = NO\n\n# If the INHERIT_DOCS tag is set to YES then an undocumented member inherits the\n# documentation from any documented member that it re-implements.\n# The default value is: YES.\n\nINHERIT_DOCS           = YES\n\n# If the SEPARATE_MEMBER_PAGES tag is set to YES then doxygen will produce a new\n# page for each member. If set to NO, the documentation of a member will be part\n# of the file/class/namespace that contains it.\n# The default value is: NO.\n\nSEPARATE_MEMBER_PAGES  = NO\n\n# The TAB_SIZE tag can be used to set the number of spaces in a tab. Doxygen\n# uses this value to replace tabs by spaces in code fragments.\n# Minimum value: 1, maximum value: 16, default value: 4.\n\nTAB_SIZE               = 4\n\n# This tag can be used to specify a number of aliases that act as commands in\n# the documentation. An alias has the form:\n# name=value\n# For example adding\n# \"sideeffect=@par Side Effects:\\n\"\n# will allow you to put the command \\sideeffect (or @sideeffect) in the\n# documentation, which will result in a user-defined paragraph with heading\n# \"Side Effects:\". You can put \\n's in the value part of an alias to insert\n# newlines (in the resulting output). You can put ^^ in the value part of an\n# alias to insert a newline as if a physical newline was in the original file.\n\nALIASES                =\n\n# This tag can be used to specify a number of word-keyword mappings (TCL only).\n# A mapping has the form \"name=value\". For example adding \"class=itcl::class\"\n# will allow you to use the command class in the itcl::class meaning.\n\nTCL_SUBST              =\n\n# Set the OPTIMIZE_OUTPUT_FOR_C tag to YES if your project consists of C sources\n# only. Doxygen will then generate output that is more tailored for C. For\n# instance, some of the names that are used will be different. The list of all\n# members will be omitted, etc.\n# The default value is: NO.\n\nOPTIMIZE_OUTPUT_FOR_C  = NO\n\n# Set the OPTIMIZE_OUTPUT_JAVA tag to YES if your project consists of Java or\n# Python sources only. Doxygen will then generate output that is more tailored\n# for that language. For instance, namespaces will be presented as packages,\n# qualified scopes will look different, etc.\n# The default value is: NO.\n\nOPTIMIZE_OUTPUT_JAVA   = NO\n\n# Set the OPTIMIZE_FOR_FORTRAN tag to YES if your project consists of Fortran\n# sources. Doxygen will then generate output that is tailored for Fortran.\n# The default value is: NO.\n\nOPTIMIZE_FOR_FORTRAN   = NO\n\n# Set the OPTIMIZE_OUTPUT_VHDL tag to YES if your project consists of VHDL\n# sources. Doxygen will then generate output that is tailored for VHDL.\n# The default value is: NO.\n\nOPTIMIZE_OUTPUT_VHDL   = NO\n\n# Doxygen selects the parser to use depending on the extension of the files it\n# parses. With this tag you can assign which parser to use for a given\n# extension. Doxygen has a built-in mapping, but you can override or extend it\n# using this tag. The format is ext=language, where ext is a file extension, and\n# language is one of the parsers supported by doxygen: IDL, Java, Javascript,\n# C#, C, C++, D, PHP, Objective-C, Python, Fortran (fixed format Fortran:\n# FortranFixed, free formatted Fortran: FortranFree, unknown formatted Fortran:\n# Fortran. In the later case the parser tries to guess whether the code is fixed\n# or free formatted code, this is the default for Fortran type files), VHDL. For\n# instance to make doxygen treat .inc files as Fortran files (default is PHP),\n# and .f files as C (default is Fortran), use: inc=Fortran f=C.\n#\n# Note: For files without extension you can use no_extension as a placeholder.\n#\n# Note that for custom extensions you also need to set FILE_PATTERNS otherwise\n# the files are not read by doxygen.\n\nEXTENSION_MAPPING      =\n\n# If the MARKDOWN_SUPPORT tag is enabled then doxygen pre-processes all comments\n# according to the Markdown format, which allows for more readable\n# documentation. See http://daringfireball.net/projects/markdown/ for details.\n# The output of markdown processing is further processed by doxygen, so you can\n# mix doxygen, HTML, and XML commands with Markdown formatting. Disable only in\n# case of backward compatibilities issues.\n# The default value is: YES.\n\nMARKDOWN_SUPPORT       = YES\n\n# When the TOC_INCLUDE_HEADINGS tag is set to a non-zero value, all headings up\n# to that level are automatically included in the table of contents, even if\n# they do not have an id attribute.\n# Note: This feature currently applies only to Markdown headings.\n# Minimum value: 0, maximum value: 99, default value: 0.\n# This tag requires that the tag MARKDOWN_SUPPORT is set to YES.\n\nTOC_INCLUDE_HEADINGS   = 0\n\n# When enabled doxygen tries to link words that correspond to documented\n# classes, or namespaces to their corresponding documentation. Such a link can\n# be prevented in individual cases by putting a % sign in front of the word or\n# globally by setting AUTOLINK_SUPPORT to NO.\n# The default value is: YES.\n\nAUTOLINK_SUPPORT       = YES\n\n# If you use STL classes (i.e. std::string, std::vector, etc.) but do not want\n# to include (a tag file for) the STL sources as input, then you should set this\n# tag to YES in order to let doxygen match functions declarations and\n# definitions whose arguments contain STL classes (e.g. func(std::string);\n# versus func(std::string) {}). This also make the inheritance and collaboration\n# diagrams that involve STL classes more complete and accurate.\n# The default value is: NO.\n\nBUILTIN_STL_SUPPORT    = NO\n\n# If you use Microsoft's C++/CLI language, you should set this option to YES to\n# enable parsing support.\n# The default value is: NO.\n\nCPP_CLI_SUPPORT        = YES\n\n# Set the SIP_SUPPORT tag to YES if your project consists of sip (see:\n# https://www.riverbankcomputing.com/software/sip/intro) sources only. Doxygen\n# will parse them like normal C++ but will assume all classes use public instead\n# of private inheritance when no explicit protection keyword is present.\n# The default value is: NO.\n\nSIP_SUPPORT            = NO\n\n# For Microsoft's IDL there are propget and propput attributes to indicate\n# getter and setter methods for a property. Setting this option to YES will make\n# doxygen to replace the get and set methods by a property in the documentation.\n# This will only work if the methods are indeed getting or setting a simple\n# type. If this is not the case, or you want to show the methods anyway, you\n# should set this option to NO.\n# The default value is: YES.\n\nIDL_PROPERTY_SUPPORT   = YES\n\n# If member grouping is used in the documentation and the DISTRIBUTE_GROUP_DOC\n# tag is set to YES then doxygen will reuse the documentation of the first\n# member in the group (if any) for the other members of the group. By default\n# all members of a group must be documented explicitly.\n# The default value is: NO.\n\nDISTRIBUTE_GROUP_DOC   = NO\n\n# If one adds a struct or class to a group and this option is enabled, then also\n# any nested class or struct is added to the same group. By default this option\n# is disabled and one has to add nested compounds explicitly via \\ingroup.\n# The default value is: NO.\n\nGROUP_NESTED_COMPOUNDS = NO\n\n# Set the SUBGROUPING tag to YES to allow class member groups of the same type\n# (for instance a group of public functions) to be put as a subgroup of that\n# type (e.g. under the Public Functions section). Set it to NO to prevent\n# subgrouping. Alternatively, this can be done per class using the\n# \\nosubgrouping command.\n# The default value is: YES.\n\nSUBGROUPING            = YES\n\n# When the INLINE_GROUPED_CLASSES tag is set to YES, classes, structs and unions\n# are shown inside the group in which they are included (e.g. using \\ingroup)\n# instead of on a separate page (for HTML and Man pages) or section (for LaTeX\n# and RTF).\n#\n# Note that this feature does not work in combination with\n# SEPARATE_MEMBER_PAGES.\n# The default value is: NO.\n\nINLINE_GROUPED_CLASSES = NO\n\n# When the INLINE_SIMPLE_STRUCTS tag is set to YES, structs, classes, and unions\n# with only public data fields or simple typedef fields will be shown inline in\n# the documentation of the scope in which they are defined (i.e. file,\n# namespace, or group documentation), provided this scope is documented. If set\n# to NO, structs, classes, and unions are shown on a separate page (for HTML and\n# Man pages) or section (for LaTeX and RTF).\n# The default value is: NO.\n\nINLINE_SIMPLE_STRUCTS  = NO\n\n# When TYPEDEF_HIDES_STRUCT tag is enabled, a typedef of a struct, union, or\n# enum is documented as struct, union, or enum with the name of the typedef. So\n# typedef struct TypeS {} TypeT, will appear in the documentation as a struct\n# with name TypeT. When disabled the typedef will appear as a member of a file,\n# namespace, or class. And the struct will be named TypeS. This can typically be\n# useful for C code in case the coding convention dictates that all compound\n# types are typedef'ed and only the typedef is referenced, never the tag name.\n# The default value is: NO.\n\nTYPEDEF_HIDES_STRUCT   = NO\n\n# The size of the symbol lookup cache can be set using LOOKUP_CACHE_SIZE. This\n# cache is used to resolve symbols given their name and scope. Since this can be\n# an expensive process and often the same symbol appears multiple times in the\n# code, doxygen keeps a cache of pre-resolved symbols. If the cache is too small\n# doxygen will become slower. If the cache is too large, memory is wasted. The\n# cache size is given by this formula: 2^(16+LOOKUP_CACHE_SIZE). The valid range\n# is 0..9, the default is 0, corresponding to a cache size of 2^16=65536\n# symbols. At the end of a run doxygen will report the cache usage and suggest\n# the optimal cache size from a speed point of view.\n# Minimum value: 0, maximum value: 9, default value: 0.\n\nLOOKUP_CACHE_SIZE      = 1\n\n#---------------------------------------------------------------------------\n# Build related configuration options\n#---------------------------------------------------------------------------\n\n# If the EXTRACT_ALL tag is set to YES, doxygen will assume all entities in\n# documentation are documented, even if no documentation was available. Private\n# class members and static file members will be hidden unless the\n# EXTRACT_PRIVATE respectively EXTRACT_STATIC tags are set to YES.\n# Note: This will also disable the warnings about undocumented members that are\n# normally produced when WARNINGS is set to YES.\n# The default value is: NO.\n\nEXTRACT_ALL            = YES\n\n# If the EXTRACT_PRIVATE tag is set to YES, all private members of a class will\n# be included in the documentation.\n# The default value is: NO.\n\nEXTRACT_PRIVATE        = YES\n\n# If the EXTRACT_PACKAGE tag is set to YES, all members with package or internal\n# scope will be included in the documentation.\n# The default value is: NO.\n\nEXTRACT_PACKAGE        = YES\n\n# If the EXTRACT_STATIC tag is set to YES, all static members of a file will be\n# included in the documentation.\n# The default value is: NO.\n\nEXTRACT_STATIC         = YES\n\n# If the EXTRACT_LOCAL_CLASSES tag is set to YES, classes (and structs) defined\n# locally in source files will be included in the documentation. If set to NO,\n# only classes defined in header files are included. Does not have any effect\n# for Java sources.\n# The default value is: YES.\n\nEXTRACT_LOCAL_CLASSES  = YES\n\n# This flag is only useful for Objective-C code. If set to YES, local methods,\n# which are defined in the implementation section but not in the interface are\n# included in the documentation. If set to NO, only methods in the interface are\n# included.\n# The default value is: NO.\n\nEXTRACT_LOCAL_METHODS  = YES\n\n# If this flag is set to YES, the members of anonymous namespaces will be\n# extracted and appear in the documentation as a namespace called\n# 'anonymous_namespace{file}', where file will be replaced with the base name of\n# the file that contains the anonymous namespace. By default anonymous namespace\n# are hidden.\n# The default value is: NO.\n\nEXTRACT_ANON_NSPACES   = YES\n\n# If the HIDE_UNDOC_MEMBERS tag is set to YES, doxygen will hide all\n# undocumented members inside documented classes or files. If set to NO these\n# members will be included in the various overviews, but no documentation\n# section is generated. This option has no effect if EXTRACT_ALL is enabled.\n# The default value is: NO.\n\nHIDE_UNDOC_MEMBERS     = NO\n\n# If the HIDE_UNDOC_CLASSES tag is set to YES, doxygen will hide all\n# undocumented classes that are normally visible in the class hierarchy. If set\n# to NO, these classes will be included in the various overviews. This option\n# has no effect if EXTRACT_ALL is enabled.\n# The default value is: NO.\n\nHIDE_UNDOC_CLASSES     = NO\n\n# If the HIDE_FRIEND_COMPOUNDS tag is set to YES, doxygen will hide all friend\n# (class|struct|union) declarations. If set to NO, these declarations will be\n# included in the documentation.\n# The default value is: NO.\n\nHIDE_FRIEND_COMPOUNDS  = NO\n\n# If the HIDE_IN_BODY_DOCS tag is set to YES, doxygen will hide any\n# documentation blocks found inside the body of a function. If set to NO, these\n# blocks will be appended to the function's detailed documentation block.\n# The default value is: NO.\n\nHIDE_IN_BODY_DOCS      = NO\n\n# The INTERNAL_DOCS tag determines if documentation that is typed after a\n# \\internal command is included. If the tag is set to NO then the documentation\n# will be excluded. Set it to YES to include the internal documentation.\n# The default value is: NO.\n\nINTERNAL_DOCS          = NO\n\n# If the CASE_SENSE_NAMES tag is set to NO then doxygen will only generate file\n# names in lower-case letters. If set to YES, upper-case letters are also\n# allowed. This is useful if you have classes or files whose names only differ\n# in case and if your file system supports case sensitive file names. Windows\n# and Mac users are advised to set this option to NO.\n# The default value is: system dependent.\n\nCASE_SENSE_NAMES       = NO\n\n# If the HIDE_SCOPE_NAMES tag is set to NO then doxygen will show members with\n# their full class and namespace scopes in the documentation. If set to YES, the\n# scope will be hidden.\n# The default value is: NO.\n\nHIDE_SCOPE_NAMES       = NO\n\n# If the HIDE_COMPOUND_REFERENCE tag is set to NO (default) then doxygen will\n# append additional text to a page's title, such as Class Reference. If set to\n# YES the compound reference will be hidden.\n# The default value is: NO.\n\nHIDE_COMPOUND_REFERENCE= NO\n\n# If the SHOW_INCLUDE_FILES tag is set to YES then doxygen will put a list of\n# the files that are included by a file in the documentation of that file.\n# The default value is: YES.\n\nSHOW_INCLUDE_FILES     = YES\n\n# If the SHOW_GROUPED_MEMB_INC tag is set to YES then Doxygen will add for each\n# grouped member an include statement to the documentation, telling the reader\n# which file to include in order to use the member.\n# The default value is: NO.\n\nSHOW_GROUPED_MEMB_INC  = NO\n\n# If the FORCE_LOCAL_INCLUDES tag is set to YES then doxygen will list include\n# files with double quotes in the documentation rather than with sharp brackets.\n# The default value is: NO.\n\nFORCE_LOCAL_INCLUDES   = NO\n\n# If the INLINE_INFO tag is set to YES then a tag [inline] is inserted in the\n# documentation for inline members.\n# The default value is: YES.\n\nINLINE_INFO            = YES\n\n# If the SORT_MEMBER_DOCS tag is set to YES then doxygen will sort the\n# (detailed) documentation of file and class members alphabetically by member\n# name. If set to NO, the members will appear in declaration order.\n# The default value is: YES.\n\nSORT_MEMBER_DOCS       = YES\n\n# If the SORT_BRIEF_DOCS tag is set to YES then doxygen will sort the brief\n# descriptions of file, namespace and class members alphabetically by member\n# name. If set to NO, the members will appear in declaration order. Note that\n# this will also influence the order of the classes in the class list.\n# The default value is: NO.\n\nSORT_BRIEF_DOCS        = NO\n\n# If the SORT_MEMBERS_CTORS_1ST tag is set to YES then doxygen will sort the\n# (brief and detailed) documentation of class members so that constructors and\n# destructors are listed first. If set to NO the constructors will appear in the\n# respective orders defined by SORT_BRIEF_DOCS and SORT_MEMBER_DOCS.\n# Note: If SORT_BRIEF_DOCS is set to NO this option is ignored for sorting brief\n# member documentation.\n# Note: If SORT_MEMBER_DOCS is set to NO this option is ignored for sorting\n# detailed member documentation.\n# The default value is: NO.\n\nSORT_MEMBERS_CTORS_1ST = NO\n\n# If the SORT_GROUP_NAMES tag is set to YES then doxygen will sort the hierarchy\n# of group names into alphabetical order. If set to NO the group names will\n# appear in their defined order.\n# The default value is: NO.\n\nSORT_GROUP_NAMES       = NO\n\n# If the SORT_BY_SCOPE_NAME tag is set to YES, the class list will be sorted by\n# fully-qualified names, including namespaces. If set to NO, the class list will\n# be sorted only by class name, not including the namespace part.\n# Note: This option is not very useful if HIDE_SCOPE_NAMES is set to YES.\n# Note: This option applies only to the class list, not to the alphabetical\n# list.\n# The default value is: NO.\n\nSORT_BY_SCOPE_NAME     = NO\n\n# If the STRICT_PROTO_MATCHING option is enabled and doxygen fails to do proper\n# type resolution of all parameters of a function it will reject a match between\n# the prototype and the implementation of a member function even if there is\n# only one candidate or it is obvious which candidate to choose by doing a\n# simple string match. By disabling STRICT_PROTO_MATCHING doxygen will still\n# accept a match between prototype and implementation in such cases.\n# The default value is: NO.\n\nSTRICT_PROTO_MATCHING  = NO\n\n# The GENERATE_TODOLIST tag can be used to enable (YES) or disable (NO) the todo\n# list. This list is created by putting \\todo commands in the documentation.\n# The default value is: YES.\n\nGENERATE_TODOLIST      = YES\n\n# The GENERATE_TESTLIST tag can be used to enable (YES) or disable (NO) the test\n# list. This list is created by putting \\test commands in the documentation.\n# The default value is: YES.\n\nGENERATE_TESTLIST      = YES\n\n# The GENERATE_BUGLIST tag can be used to enable (YES) or disable (NO) the bug\n# list. This list is created by putting \\bug commands in the documentation.\n# The default value is: YES.\n\nGENERATE_BUGLIST       = YES\n\n# The GENERATE_DEPRECATEDLIST tag can be used to enable (YES) or disable (NO)\n# the deprecated list. This list is created by putting \\deprecated commands in\n# the documentation.\n# The default value is: YES.\n\nGENERATE_DEPRECATEDLIST= YES\n\n# The ENABLED_SECTIONS tag can be used to enable conditional documentation\n# sections, marked by \\if <section_label> ... \\endif and \\cond <section_label>\n# ... \\endcond blocks.\n\nENABLED_SECTIONS       =\n\n# The MAX_INITIALIZER_LINES tag determines the maximum number of lines that the\n# initial value of a variable or macro / define can have for it to appear in the\n# documentation. If the initializer consists of more lines than specified here\n# it will be hidden. Use a value of 0 to hide initializers completely. The\n# appearance of the value of individual variables and macros / defines can be\n# controlled using \\showinitializer or \\hideinitializer command in the\n# documentation regardless of this setting.\n# Minimum value: 0, maximum value: 10000, default value: 30.\n\nMAX_INITIALIZER_LINES  = 30\n\n# Set the SHOW_USED_FILES tag to NO to disable the list of files generated at\n# the bottom of the documentation of classes and structs. If set to YES, the\n# list will mention the files that were used to generate the documentation.\n# The default value is: YES.\n\nSHOW_USED_FILES        = YES\n\n# Set the SHOW_FILES tag to NO to disable the generation of the Files page. This\n# will remove the Files entry from the Quick Index and from the Folder Tree View\n# (if specified).\n# The default value is: YES.\n\nSHOW_FILES             = YES\n\n# Set the SHOW_NAMESPACES tag to NO to disable the generation of the Namespaces\n# page. This will remove the Namespaces entry from the Quick Index and from the\n# Folder Tree View (if specified).\n# The default value is: YES.\n\nSHOW_NAMESPACES        = YES\n\n# The FILE_VERSION_FILTER tag can be used to specify a program or script that\n# doxygen should invoke to get the current version for each file (typically from\n# the version control system). Doxygen will invoke the program by executing (via\n# popen()) the command command input-file, where command is the value of the\n# FILE_VERSION_FILTER tag, and input-file is the name of an input file provided\n# by doxygen. Whatever the program writes to standard output is used as the file\n# version. For an example see the documentation.\n\nFILE_VERSION_FILTER    =\n\n# The LAYOUT_FILE tag can be used to specify a layout file which will be parsed\n# by doxygen. The layout file controls the global structure of the generated\n# output files in an output format independent way. To create the layout file\n# that represents doxygen's defaults, run doxygen with the -l option. You can\n# optionally specify a file name after the option, if omitted DoxygenLayout.xml\n# will be used as the name of the layout file.\n#\n# Note that if you run doxygen from a directory containing a file called\n# DoxygenLayout.xml, doxygen will parse it automatically even if the LAYOUT_FILE\n# tag is left empty.\n\nLAYOUT_FILE            =\n\n# The CITE_BIB_FILES tag can be used to specify one or more bib files containing\n# the reference definitions. This must be a list of .bib files. The .bib\n# extension is automatically appended if omitted. This requires the bibtex tool\n# to be installed. See also https://en.wikipedia.org/wiki/BibTeX for more info.\n# For LaTeX the style of the bibliography can be controlled using\n# LATEX_BIB_STYLE. To use this feature you need bibtex and perl available in the\n# search path. See also \\cite for info how to create references.\n\nCITE_BIB_FILES         =\n\n#---------------------------------------------------------------------------\n# Configuration options related to warning and progress messages\n#---------------------------------------------------------------------------\n\n# The QUIET tag can be used to turn on/off the messages that are generated to\n# standard output by doxygen. If QUIET is set to YES this implies that the\n# messages are off.\n# The default value is: NO.\n\nQUIET                  = NO\n\n# The WARNINGS tag can be used to turn on/off the warning messages that are\n# generated to standard error (stderr) by doxygen. If WARNINGS is set to YES\n# this implies that the warnings are on.\n#\n# Tip: Turn warnings on while writing the documentation.\n# The default value is: YES.\n\nWARNINGS               = YES\n\n# If the WARN_IF_UNDOCUMENTED tag is set to YES then doxygen will generate\n# warnings for undocumented members. If EXTRACT_ALL is set to YES then this flag\n# will automatically be disabled.\n# The default value is: YES.\n\nWARN_IF_UNDOCUMENTED   = YES\n\n# If the WARN_IF_DOC_ERROR tag is set to YES, doxygen will generate warnings for\n# potential errors in the documentation, such as not documenting some parameters\n# in a documented function, or documenting parameters that don't exist or using\n# markup commands wrongly.\n# The default value is: YES.\n\nWARN_IF_DOC_ERROR      = YES\n\n# This WARN_NO_PARAMDOC option can be enabled to get warnings for functions that\n# are documented, but have no documentation for their parameters or return\n# value. If set to NO, doxygen will only warn about wrong or incomplete\n# parameter documentation, but not about the absence of documentation.\n# The default value is: NO.\n\nWARN_NO_PARAMDOC       = NO\n\n# If the WARN_AS_ERROR tag is set to YES then doxygen will immediately stop when\n# a warning is encountered.\n# The default value is: NO.\n\nWARN_AS_ERROR          = NO\n\n# The WARN_FORMAT tag determines the format of the warning messages that doxygen\n# can produce. The string should contain the $file, $line, and $text tags, which\n# will be replaced by the file and line number from which the warning originated\n# and the warning text. Optionally the format may contain $version, which will\n# be replaced by the version of the file (if it could be obtained via\n# FILE_VERSION_FILTER)\n# The default value is: $file:$line: $text.\n\nWARN_FORMAT            = \"$file:$line: $text\"\n\n# The WARN_LOGFILE tag can be used to specify a file to which warning and error\n# messages should be written. If left blank the output is written to standard\n# error (stderr).\n\nWARN_LOGFILE           =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the input files\n#---------------------------------------------------------------------------\n\n# The INPUT tag is used to specify the files and/or directories that contain\n# documented source files. You may enter file names like myfile.cpp or\n# directories like /usr/src/myproject. Separate the files or directories with\n# spaces. See also FILE_PATTERNS and EXTENSION_MAPPING\n# Note: If this tag is empty the current directory is searched.\n\nINPUT                  = @CMAKE_SOURCE_DIR@\n\n# This tag can be used to specify the character encoding of the source files\n# that doxygen parses. Internally doxygen uses the UTF-8 encoding. Doxygen uses\n# libiconv (or the iconv built into libc) for the transcoding. See the libiconv\n# documentation (see: https://www.gnu.org/software/libiconv/) for the list of\n# possible encodings.\n# The default value is: UTF-8.\n\nINPUT_ENCODING         = UTF-8\n\n# If the value of the INPUT tag contains directories, you can use the\n# FILE_PATTERNS tag to specify one or more wildcard patterns (like *.cpp and\n# *.h) to filter out the source-files in the directories.\n#\n# Note that for custom extensions or not directly supported extensions you also\n# need to set EXTENSION_MAPPING for the extension otherwise the files are not\n# read by doxygen.\n#\n# If left blank the following patterns are tested:*.c, *.cc, *.cxx, *.cpp,\n# *.c++, *.java, *.ii, *.ixx, *.ipp, *.i++, *.inl, *.idl, *.ddl, *.odl, *.h,\n# *.hh, *.hxx, *.hpp, *.h++, *.cs, *.d, *.php, *.php4, *.php5, *.phtml, *.inc,\n# *.m, *.markdown, *.md, *.mm, *.dox, *.py, *.pyw, *.f90, *.f95, *.f03, *.f08,\n# *.f, *.for, *.tcl, *.vhd, *.vhdl, *.ucf and *.qsf.\n\nFILE_PATTERNS          = *.c \\\n                         *.cc \\\n                         *.cxx \\\n                         *.cpp \\\n                         *.cu \\\n                         *.c++ \\\n                         *.java \\\n                         *.ii \\\n                         *.ixx \\\n                         *.ipp \\\n                         *.i++ \\\n                         *.inl \\\n                         *.idl \\\n                         *.ddl \\\n                         *.odl \\\n                         *.h \\\n                         *.hh \\\n                         *.hxx \\\n                         *.hpp \\\n                         *.h++ \\\n                         *.cs \\\n                         *.d \\\n                         *.php \\\n                         *.php4 \\\n                         *.php5 \\\n                         *.phtml \\\n                         *.inc \\\n                         *.m \\\n                         *.markdown \\\n                         *.md \\\n                         *.mm \\\n                         *.dox \\\n                         *.py \\\n                         *.pyw \\\n                         *.f90 \\\n                         *.f95 \\\n                         *.f03 \\\n                         *.f08 \\\n                         *.f \\\n                         *.for \\\n                         *.tcl \\\n                         *.vhd \\\n                         *.vhdl \\\n                         *.ucf \\\n                         *.qsf\n\n# The RECURSIVE tag can be used to specify whether or not subdirectories should\n# be searched for input files as well.\n# The default value is: NO.\n\nRECURSIVE              = YES\n\n# The EXCLUDE tag can be used to specify files and/or directories that should be\n# excluded from the INPUT source files. This way you can easily exclude a\n# subdirectory from a directory tree whose root is specified with the INPUT tag.\n#\n# Note that relative paths are relative to the directory from which doxygen is\n# run.\n\nEXCLUDE                = ../blog\nEXCLUDE               += ../Catalog/ee/Utils\nEXCLUDE               += ../docker\nEXCLUDE               += ../docs\nEXCLUDE               += ../Experimental\nEXCLUDE               += ../issue_template.md\nEXCLUDE               += ../README.md\nEXCLUDE               += ../ThirdParty\nEXCLUDE               += ../Tests\nEXCLUDE               += ../ODBC\nEXCLUDE               += ../QueryEngine/GroupByHashTest.cpp\nEXCLUDE               += ../SQLFrontend\nEXCLUDE               += ../SampleCode\nEXCLUDE               += ../QueryRenderer\nEXCLUDE               += @CMAKE_BINARY_DIR@\n\n# The EXCLUDE_SYMLINKS tag can be used to select whether or not files or\n# directories that are symbolic links (a Unix file system feature) are excluded\n# from the input.\n# The default value is: NO.\n\nEXCLUDE_SYMLINKS       = NO\n\n# If the value of the INPUT tag contains directories, you can use the\n# EXCLUDE_PATTERNS tag to specify one or more wildcard patterns to exclude\n# certain files from those directories.\n#\n# Note that the wildcards are matched against the file with absolute path, so to\n# exclude all test directories for example use the pattern */test/*\n\nEXCLUDE_PATTERNS       = */src/gen/*\nEXCLUDE_PATTERNS      += */target/*\nEXCLUDE_PATTERNS      += */ODBC/*/README*\n\n# The EXCLUDE_SYMBOLS tag can be used to specify one or more symbol names\n# (namespaces, classes, functions, etc.) that should be excluded from the\n# output. The symbol name can be a fully qualified name, a word, or if the\n# wildcard * is used, a substring. Examples: ANamespace, AClass,\n# AClass::ANamespace, ANamespace::*Test\n#\n# Note that the wildcards are matched against the file with absolute path, so to\n# exclude all test directories use the pattern */test/*\n\nEXCLUDE_SYMBOLS        =\n\n# The EXAMPLE_PATH tag can be used to specify one or more files or directories\n# that contain example code fragments that are included (see the \\include\n# command).\n\nEXAMPLE_PATH           =\n\n# If the value of the EXAMPLE_PATH tag contains directories, you can use the\n# EXAMPLE_PATTERNS tag to specify one or more wildcard pattern (like *.cpp and\n# *.h) to filter out the source-files in the directories. If left blank all\n# files are included.\n\nEXAMPLE_PATTERNS       = *\n\n# If the EXAMPLE_RECURSIVE tag is set to YES then subdirectories will be\n# searched for input files to be used with the \\include or \\dontinclude commands\n# irrespective of the value of the RECURSIVE tag.\n# The default value is: NO.\n\nEXAMPLE_RECURSIVE      = NO\n\n# The IMAGE_PATH tag can be used to specify one or more files or directories\n# that contain images that are to be included in the documentation (see the\n# \\image command).\n\nIMAGE_PATH             =\n\n# The INPUT_FILTER tag can be used to specify a program that doxygen should\n# invoke to filter for each input file. Doxygen will invoke the filter program\n# by executing (via popen()) the command:\n#\n# <filter> <input-file>\n#\n# where <filter> is the value of the INPUT_FILTER tag, and <input-file> is the\n# name of an input file. Doxygen will then use the output that the filter\n# program writes to standard output. If FILTER_PATTERNS is specified, this tag\n# will be ignored.\n#\n# Note that the filter must not add or remove lines; it is applied before the\n# code is scanned, but not when the output code is generated. If lines are added\n# or removed, the anchors will not be placed correctly.\n#\n# Note that for custom extensions or not directly supported extensions you also\n# need to set EXTENSION_MAPPING for the extension otherwise the files are not\n# properly processed by doxygen.\n\nINPUT_FILTER           =\n\n# The FILTER_PATTERNS tag can be used to specify filters on a per file pattern\n# basis. Doxygen will compare the file name with each pattern and apply the\n# filter if there is a match. The filters are a list of the form: pattern=filter\n# (like *.cpp=my_cpp_filter). See INPUT_FILTER for further information on how\n# filters are used. If the FILTER_PATTERNS tag is empty or if none of the\n# patterns match the file name, INPUT_FILTER is applied.\n#\n# Note that for custom extensions or not directly supported extensions you also\n# need to set EXTENSION_MAPPING for the extension otherwise the files are not\n# properly processed by doxygen.\n\nFILTER_PATTERNS        =\n\n# If the FILTER_SOURCE_FILES tag is set to YES, the input filter (if set using\n# INPUT_FILTER) will also be used to filter the input files that are used for\n# producing the source files to browse (i.e. when SOURCE_BROWSER is set to YES).\n# The default value is: NO.\n\nFILTER_SOURCE_FILES    = NO\n\n# The FILTER_SOURCE_PATTERNS tag can be used to specify source filters per file\n# pattern. A pattern will override the setting for FILTER_PATTERN (if any) and\n# it is also possible to disable source filtering for a specific pattern using\n# *.ext= (so without naming a filter).\n# This tag requires that the tag FILTER_SOURCE_FILES is set to YES.\n\nFILTER_SOURCE_PATTERNS =\n\n# If the USE_MDFILE_AS_MAINPAGE tag refers to the name of a markdown file that\n# is part of the input, its contents will be placed on the main page\n# (index.html). This can be useful if you have a project on for instance GitHub\n# and want to reuse the introduction page also for the doxygen output.\n\nUSE_MDFILE_AS_MAINPAGE = ../README.md\n\n#---------------------------------------------------------------------------\n# Configuration options related to source browsing\n#---------------------------------------------------------------------------\n\n# If the SOURCE_BROWSER tag is set to YES then a list of source files will be\n# generated. Documented entities will be cross-referenced with these sources.\n#\n# Note: To get rid of all source code in the generated output, make sure that\n# also VERBATIM_HEADERS is set to NO.\n# The default value is: NO.\n\nSOURCE_BROWSER         = YES\n\n# Setting the INLINE_SOURCES tag to YES will include the body of functions,\n# classes and enums directly into the documentation.\n# The default value is: NO.\n\nINLINE_SOURCES         = YES\n\n# Setting the STRIP_CODE_COMMENTS tag to YES will instruct doxygen to hide any\n# special comment blocks from generated source code fragments. Normal C, C++ and\n# Fortran comments will always remain visible.\n# The default value is: YES.\n\nSTRIP_CODE_COMMENTS    = YES\n\n# If the REFERENCED_BY_RELATION tag is set to YES then for each documented\n# function all documented functions referencing it will be listed.\n# The default value is: NO.\n\nREFERENCED_BY_RELATION = YES\n\n# If the REFERENCES_RELATION tag is set to YES then for each documented function\n# all documented entities called/used by that function will be listed.\n# The default value is: NO.\n\nREFERENCES_RELATION    = YES\n\n# If the REFERENCES_LINK_SOURCE tag is set to YES and SOURCE_BROWSER tag is set\n# to YES then the hyperlinks from functions in REFERENCES_RELATION and\n# REFERENCED_BY_RELATION lists will link to the source code. Otherwise they will\n# link to the documentation.\n# The default value is: YES.\n\nREFERENCES_LINK_SOURCE = YES\n\n# If SOURCE_TOOLTIPS is enabled (the default) then hovering a hyperlink in the\n# source code will show a tooltip with additional information such as prototype,\n# brief description and links to the definition and documentation. Since this\n# will make the HTML file larger and loading of large files a bit slower, you\n# can opt to disable this feature.\n# The default value is: YES.\n# This tag requires that the tag SOURCE_BROWSER is set to YES.\n\nSOURCE_TOOLTIPS        = YES\n\n# If the USE_HTAGS tag is set to YES then the references to source code will\n# point to the HTML generated by the htags(1) tool instead of doxygen built-in\n# source browser. The htags tool is part of GNU's global source tagging system\n# (see https://www.gnu.org/software/global/global.html). You will need version\n# 4.8.6 or higher.\n#\n# To use it do the following:\n# - Install the latest version of global\n# - Enable SOURCE_BROWSER and USE_HTAGS in the config file\n# - Make sure the INPUT points to the root of the source tree\n# - Run doxygen as normal\n#\n# Doxygen will invoke htags (and that will in turn invoke gtags), so these\n# tools must be available from the command line (i.e. in the search path).\n#\n# The result: instead of the source browser generated by doxygen, the links to\n# source code will now point to the output of htags.\n# The default value is: NO.\n# This tag requires that the tag SOURCE_BROWSER is set to YES.\n\nUSE_HTAGS              = NO\n\n# If the VERBATIM_HEADERS tag is set the YES then doxygen will generate a\n# verbatim copy of the header file for each class for which an include is\n# specified. Set to NO to disable this.\n# See also: Section \\class.\n# The default value is: YES.\n\nVERBATIM_HEADERS       = YES\n\n#---------------------------------------------------------------------------\n# Configuration options related to the alphabetical class index\n#---------------------------------------------------------------------------\n\n# If the ALPHABETICAL_INDEX tag is set to YES, an alphabetical index of all\n# compounds will be generated. Enable this if the project contains a lot of\n# classes, structs, unions or interfaces.\n# The default value is: YES.\n\nALPHABETICAL_INDEX     = YES\n\n# The COLS_IN_ALPHA_INDEX tag can be used to specify the number of columns in\n# which the alphabetical index list will be split.\n# Minimum value: 1, maximum value: 20, default value: 5.\n# This tag requires that the tag ALPHABETICAL_INDEX is set to YES.\n\nCOLS_IN_ALPHA_INDEX    = 5\n\n# In case all classes in a project start with a common prefix, all classes will\n# be put under the same header in the alphabetical index. The IGNORE_PREFIX tag\n# can be used to specify a prefix (or a list of prefixes) that should be ignored\n# while generating the index headers.\n# This tag requires that the tag ALPHABETICAL_INDEX is set to YES.\n\nIGNORE_PREFIX          =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the HTML output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_HTML tag is set to YES, doxygen will generate HTML output\n# The default value is: YES.\n\nGENERATE_HTML          = YES\n\n# The HTML_OUTPUT tag is used to specify where the HTML docs will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: html.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_OUTPUT            = html\n\n# The HTML_FILE_EXTENSION tag can be used to specify the file extension for each\n# generated HTML page (for example: .htm, .php, .asp).\n# The default value is: .html.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_FILE_EXTENSION    = .html\n\n# The HTML_HEADER tag can be used to specify a user-defined HTML header file for\n# each generated HTML page. If the tag is left blank doxygen will generate a\n# standard header.\n#\n# To get valid HTML the header file that includes any scripts and style sheets\n# that doxygen needs, which is dependent on the configuration options used (e.g.\n# the setting GENERATE_TREEVIEW). It is highly recommended to start with a\n# default header using\n# doxygen -w html new_header.html new_footer.html new_stylesheet.css\n# YourConfigFile\n# and then modify the file new_header.html. See also section \"Doxygen usage\"\n# for information on how to generate the default header that doxygen normally\n# uses.\n# Note: The header is subject to change so you typically have to regenerate the\n# default header when upgrading to a newer version of doxygen. For a description\n# of the possible markers and block names see the documentation.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_HEADER            =\n\n# The HTML_FOOTER tag can be used to specify a user-defined HTML footer for each\n# generated HTML page. If the tag is left blank doxygen will generate a standard\n# footer. See HTML_HEADER for more information on how to generate a default\n# footer and what special commands can be used inside the footer. See also\n# section \"Doxygen usage\" for information on how to generate the default footer\n# that doxygen normally uses.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_FOOTER            =\n\n# The HTML_STYLESHEET tag can be used to specify a user-defined cascading style\n# sheet that is used by each HTML page. It can be used to fine-tune the look of\n# the HTML output. If left blank doxygen will generate a default style sheet.\n# See also section \"Doxygen usage\" for information on how to generate the style\n# sheet that doxygen normally uses.\n# Note: It is recommended to use HTML_EXTRA_STYLESHEET instead of this tag, as\n# it is more robust and this tag (HTML_STYLESHEET) will in the future become\n# obsolete.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_STYLESHEET        =\n\n# The HTML_EXTRA_STYLESHEET tag can be used to specify additional user-defined\n# cascading style sheets that are included after the standard style sheets\n# created by doxygen. Using this option one can overrule certain style aspects.\n# This is preferred over using HTML_STYLESHEET since it does not replace the\n# standard style sheet and is therefore more robust against future updates.\n# Doxygen will copy the style sheet files to the output directory.\n# Note: The order of the extra style sheet files is of importance (e.g. the last\n# style sheet in the list overrules the setting of the previous ones in the\n# list). For an example see the documentation.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_EXTRA_STYLESHEET  =\n\n# The HTML_EXTRA_FILES tag can be used to specify one or more extra images or\n# other source files which should be copied to the HTML output directory. Note\n# that these files will be copied to the base HTML output directory. Use the\n# $relpath^ marker in the HTML_HEADER and/or HTML_FOOTER files to load these\n# files. In the HTML_STYLESHEET file, use the file name only. Also note that the\n# files will be copied as-is; there are no commands or markers available.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_EXTRA_FILES       =\n\n# The HTML_COLORSTYLE_HUE tag controls the color of the HTML output. Doxygen\n# will adjust the colors in the style sheet and background images according to\n# this color. Hue is specified as an angle on a colorwheel, see\n# https://en.wikipedia.org/wiki/Hue for more information. For instance the value\n# 0 represents red, 60 is yellow, 120 is green, 180 is cyan, 240 is blue, 300\n# purple, and 360 is red again.\n# Minimum value: 0, maximum value: 359, default value: 220.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_COLORSTYLE_HUE    = 220\n\n# The HTML_COLORSTYLE_SAT tag controls the purity (or saturation) of the colors\n# in the HTML output. For a value of 0 the output will use grayscales only. A\n# value of 255 will produce the most vivid colors.\n# Minimum value: 0, maximum value: 255, default value: 100.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_COLORSTYLE_SAT    = 100\n\n# The HTML_COLORSTYLE_GAMMA tag controls the gamma correction applied to the\n# luminance component of the colors in the HTML output. Values below 100\n# gradually make the output lighter, whereas values above 100 make the output\n# darker. The value divided by 100 is the actual gamma applied, so 80 represents\n# a gamma of 0.8, The value 220 represents a gamma of 2.2, and 100 does not\n# change the gamma.\n# Minimum value: 40, maximum value: 240, default value: 80.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_COLORSTYLE_GAMMA  = 80\n\n# If the HTML_TIMESTAMP tag is set to YES then the footer of each generated HTML\n# page will contain the date and time when the page was generated. Setting this\n# to YES can help to show when doxygen was last run and thus if the\n# documentation is up to date.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_TIMESTAMP         = YES\n\n# If the HTML_DYNAMIC_MENUS tag is set to YES then the generated HTML\n# documentation will contain a main index with vertical navigation menus that\n# are dynamically created via Javascript. If disabled, the navigation index will\n# consists of multiple levels of tabs that are statically embedded in every HTML\n# page. Disable this option to support browsers that do not have Javascript,\n# like the Qt help browser.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_DYNAMIC_MENUS     = YES\n\n# If the HTML_DYNAMIC_SECTIONS tag is set to YES then the generated HTML\n# documentation will contain sections that can be hidden and shown after the\n# page has loaded.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_DYNAMIC_SECTIONS  = YES\n\n# With HTML_INDEX_NUM_ENTRIES one can control the preferred number of entries\n# shown in the various tree structured indices initially; the user can expand\n# and collapse entries dynamically later on. Doxygen will expand the tree to\n# such a level that at most the specified number of entries are visible (unless\n# a fully collapsed tree already exceeds this amount). So setting the number of\n# entries 1 will produce a full collapsed tree by default. 0 is a special value\n# representing an infinite number of entries and will result in a full expanded\n# tree by default.\n# Minimum value: 0, maximum value: 9999, default value: 100.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_INDEX_NUM_ENTRIES = 100\n\n# If the GENERATE_DOCSET tag is set to YES, additional index files will be\n# generated that can be used as input for Apple's Xcode 3 integrated development\n# environment (see: https://developer.apple.com/tools/xcode/), introduced with\n# OSX 10.5 (Leopard). To create a documentation set, doxygen will generate a\n# Makefile in the HTML output directory. Running make will produce the docset in\n# that directory and running make install will install the docset in\n# ~/Library/Developer/Shared/Documentation/DocSets so that Xcode will find it at\n# startup. See https://developer.apple.com/tools/creatingdocsetswithdoxygen.html\n# for more information.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_DOCSET        = NO\n\n# This tag determines the name of the docset feed. A documentation feed provides\n# an umbrella under which multiple documentation sets from a single provider\n# (such as a company or product suite) can be grouped.\n# The default value is: Doxygen generated docs.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_FEEDNAME        = \"Doxygen generated docs\"\n\n# This tag specifies a string that should uniquely identify the documentation\n# set bundle. This should be a reverse domain-name style string, e.g.\n# com.mycompany.MyDocSet. Doxygen will append .docset to the name.\n# The default value is: org.doxygen.Project.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_BUNDLE_ID       = org.doxygen.Project\n\n# The DOCSET_PUBLISHER_ID tag specifies a string that should uniquely identify\n# the documentation publisher. This should be a reverse domain-name style\n# string, e.g. com.mycompany.MyDocSet.documentation.\n# The default value is: org.doxygen.Publisher.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_PUBLISHER_ID    = org.doxygen.Publisher\n\n# The DOCSET_PUBLISHER_NAME tag identifies the documentation publisher.\n# The default value is: Publisher.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_PUBLISHER_NAME  = Publisher\n\n# If the GENERATE_HTMLHELP tag is set to YES then doxygen generates three\n# additional HTML index files: index.hhp, index.hhc, and index.hhk. The\n# index.hhp is a project file that can be read by Microsoft's HTML Help Workshop\n# (see: http://www.microsoft.com/en-us/download/details.aspx?id=21138) on\n# Windows.\n#\n# The HTML Help Workshop contains a compiler that can convert all HTML output\n# generated by doxygen into a single compiled HTML file (.chm). Compiled HTML\n# files are now used as the Windows 98 help format, and will replace the old\n# Windows help format (.hlp) on all Windows platforms in the future. Compressed\n# HTML files also contain an index, a table of contents, and you can search for\n# words in the documentation. The HTML workshop also contains a viewer for\n# compressed HTML files.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_HTMLHELP      = NO\n\n# The CHM_FILE tag can be used to specify the file name of the resulting .chm\n# file. You can add a path in front of the file if the result should not be\n# written to the html output directory.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nCHM_FILE               =\n\n# The HHC_LOCATION tag can be used to specify the location (absolute path\n# including file name) of the HTML help compiler (hhc.exe). If non-empty,\n# doxygen will try to run the HTML help compiler on the generated index.hhp.\n# The file has to be specified with full path.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nHHC_LOCATION           =\n\n# The GENERATE_CHI flag controls if a separate .chi index file is generated\n# (YES) or that it should be included in the master .chm file (NO).\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nGENERATE_CHI           = NO\n\n# The CHM_INDEX_ENCODING is used to encode HtmlHelp index (hhk), content (hhc)\n# and project file content.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nCHM_INDEX_ENCODING     =\n\n# The BINARY_TOC flag controls whether a binary table of contents is generated\n# (YES) or a normal table of contents (NO) in the .chm file. Furthermore it\n# enables the Previous and Next buttons.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nBINARY_TOC             = NO\n\n# The TOC_EXPAND flag can be set to YES to add extra items for group members to\n# the table of contents of the HTML help documentation and to the tree view.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nTOC_EXPAND             = NO\n\n# If the GENERATE_QHP tag is set to YES and both QHP_NAMESPACE and\n# QHP_VIRTUAL_FOLDER are set, an additional index file will be generated that\n# can be used as input for Qt's qhelpgenerator to generate a Qt Compressed Help\n# (.qch) of the generated HTML documentation.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_QHP           = NO\n\n# If the QHG_LOCATION tag is specified, the QCH_FILE tag can be used to specify\n# the file name of the resulting .qch file. The path specified is relative to\n# the HTML output folder.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQCH_FILE               =\n\n# The QHP_NAMESPACE tag specifies the namespace to use when generating Qt Help\n# Project output. For more information please see Qt Help Project / Namespace\n# (see: http://doc.qt.io/qt-4.8/qthelpproject.html#namespace).\n# The default value is: org.doxygen.Project.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_NAMESPACE          = org.doxygen.Project\n\n# The QHP_VIRTUAL_FOLDER tag specifies the namespace to use when generating Qt\n# Help Project output. For more information please see Qt Help Project / Virtual\n# Folders (see: http://doc.qt.io/qt-4.8/qthelpproject.html#virtual-folders).\n# The default value is: doc.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_VIRTUAL_FOLDER     = doc\n\n# If the QHP_CUST_FILTER_NAME tag is set, it specifies the name of a custom\n# filter to add. For more information please see Qt Help Project / Custom\n# Filters (see: http://doc.qt.io/qt-4.8/qthelpproject.html#custom-filters).\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_CUST_FILTER_NAME   =\n\n# The QHP_CUST_FILTER_ATTRS tag specifies the list of the attributes of the\n# custom filter to add. For more information please see Qt Help Project / Custom\n# Filters (see: http://doc.qt.io/qt-4.8/qthelpproject.html#custom-filters).\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_CUST_FILTER_ATTRS  =\n\n# The QHP_SECT_FILTER_ATTRS tag specifies the list of the attributes this\n# project's filter section matches. Qt Help Project / Filter Attributes (see:\n# http://doc.qt.io/qt-4.8/qthelpproject.html#filter-attributes).\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_SECT_FILTER_ATTRS  =\n\n# The QHG_LOCATION tag can be used to specify the location of Qt's\n# qhelpgenerator. If non-empty doxygen will try to run qhelpgenerator on the\n# generated .qhp file.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHG_LOCATION           =\n\n# If the GENERATE_ECLIPSEHELP tag is set to YES, additional index files will be\n# generated, together with the HTML files, they form an Eclipse help plugin. To\n# install this plugin and make it available under the help contents menu in\n# Eclipse, the contents of the directory containing the HTML and XML files needs\n# to be copied into the plugins directory of eclipse. The name of the directory\n# within the plugins directory should be the same as the ECLIPSE_DOC_ID value.\n# After copying Eclipse needs to be restarted before the help appears.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_ECLIPSEHELP   = NO\n\n# A unique identifier for the Eclipse help plugin. When installing the plugin\n# the directory name containing the HTML and XML files should also have this\n# name. Each documentation set should have its own identifier.\n# The default value is: org.doxygen.Project.\n# This tag requires that the tag GENERATE_ECLIPSEHELP is set to YES.\n\nECLIPSE_DOC_ID         = org.doxygen.Project\n\n# If you want full control over the layout of the generated HTML pages it might\n# be necessary to disable the index and replace it with your own. The\n# DISABLE_INDEX tag can be used to turn on/off the condensed index (tabs) at top\n# of each HTML page. A value of NO enables the index and the value YES disables\n# it. Since the tabs in the index contain the same information as the navigation\n# tree, you can set this option to YES if you also set GENERATE_TREEVIEW to YES.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nDISABLE_INDEX          = NO\n\n# The GENERATE_TREEVIEW tag is used to specify whether a tree-like index\n# structure should be generated to display hierarchical information. If the tag\n# value is set to YES, a side panel will be generated containing a tree-like\n# index structure (just like the one that is generated for HTML Help). For this\n# to work a browser that supports JavaScript, DHTML, CSS and frames is required\n# (i.e. any modern browser). Windows users are probably better off using the\n# HTML help feature. Via custom style sheets (see HTML_EXTRA_STYLESHEET) one can\n# further fine-tune the look of the index. As an example, the default style\n# sheet generated by doxygen has an example that shows how to put an image at\n# the root of the tree instead of the PROJECT_NAME. Since the tree basically has\n# the same information as the tab index, you could consider setting\n# DISABLE_INDEX to YES when enabling this option.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_TREEVIEW      = YES\n\n# The ENUM_VALUES_PER_LINE tag can be used to set the number of enum values that\n# doxygen will group on one line in the generated HTML documentation.\n#\n# Note that a value of 0 will completely suppress the enum values from appearing\n# in the overview section.\n# Minimum value: 0, maximum value: 20, default value: 4.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nENUM_VALUES_PER_LINE   = 4\n\n# If the treeview is enabled (see GENERATE_TREEVIEW) then this tag can be used\n# to set the initial width (in pixels) of the frame in which the tree is shown.\n# Minimum value: 0, maximum value: 1500, default value: 250.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nTREEVIEW_WIDTH         = 250\n\n# If the EXT_LINKS_IN_WINDOW option is set to YES, doxygen will open links to\n# external symbols imported via tag files in a separate window.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nEXT_LINKS_IN_WINDOW    = NO\n\n# Use this tag to change the font size of LaTeX formulas included as images in\n# the HTML documentation. When you change the font size after a successful\n# doxygen run you need to manually remove any form_*.png images from the HTML\n# output directory to force them to be regenerated.\n# Minimum value: 8, maximum value: 50, default value: 10.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nFORMULA_FONTSIZE       = 10\n\n# Use the FORMULA_TRANSPARENT tag to determine whether or not the images\n# generated for formulas are transparent PNGs. Transparent PNGs are not\n# supported properly for IE 6.0, but are supported on all modern browsers.\n#\n# Note that when changing this option you need to delete any form_*.png files in\n# the HTML output directory before the changes have effect.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nFORMULA_TRANSPARENT    = YES\n\n# Enable the USE_MATHJAX option to render LaTeX formulas using MathJax (see\n# https://www.mathjax.org) which uses client side Javascript for the rendering\n# instead of using pre-rendered bitmaps. Use this if you do not have LaTeX\n# installed or if you want to formulas look prettier in the HTML output. When\n# enabled you may also need to install MathJax separately and configure the path\n# to it using the MATHJAX_RELPATH option.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nUSE_MATHJAX            = NO\n\n# When MathJax is enabled you can set the default output format to be used for\n# the MathJax output. See the MathJax site (see:\n# http://docs.mathjax.org/en/latest/output.html) for more details.\n# Possible values are: HTML-CSS (which is slower, but has the best\n# compatibility), NativeMML (i.e. MathML) and SVG.\n# The default value is: HTML-CSS.\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_FORMAT         = HTML-CSS\n\n# When MathJax is enabled you need to specify the location relative to the HTML\n# output directory using the MATHJAX_RELPATH option. The destination directory\n# should contain the MathJax.js script. For instance, if the mathjax directory\n# is located at the same level as the HTML output directory, then\n# MATHJAX_RELPATH should be ../mathjax. The default value points to the MathJax\n# Content Delivery Network so you can quickly see the result without installing\n# MathJax. However, it is strongly recommended to install a local copy of\n# MathJax from https://www.mathjax.org before deployment.\n# The default value is: https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/.\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_RELPATH        = https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/\n\n# The MATHJAX_EXTENSIONS tag can be used to specify one or more MathJax\n# extension names that should be enabled during MathJax rendering. For example\n# MATHJAX_EXTENSIONS = TeX/AMSmath TeX/AMSsymbols\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_EXTENSIONS     =\n\n# The MATHJAX_CODEFILE tag can be used to specify a file with javascript pieces\n# of code that will be used on startup of the MathJax code. See the MathJax site\n# (see: http://docs.mathjax.org/en/latest/output.html) for more details. For an\n# example see the documentation.\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_CODEFILE       =\n\n# When the SEARCHENGINE tag is enabled doxygen will generate a search box for\n# the HTML output. The underlying search engine uses javascript and DHTML and\n# should work on any modern browser. Note that when using HTML help\n# (GENERATE_HTMLHELP), Qt help (GENERATE_QHP), or docsets (GENERATE_DOCSET)\n# there is already a search function so this one should typically be disabled.\n# For large projects the javascript based search engine can be slow, then\n# enabling SERVER_BASED_SEARCH may provide a better solution. It is possible to\n# search using the keyboard; to jump to the search box use <access key> + S\n# (what the <access key> is depends on the OS and browser, but it is typically\n# <CTRL>, <ALT>/<option>, or both). Inside the search box use the <cursor down\n# key> to jump into the search results window, the results can be navigated\n# using the <cursor keys>. Press <Enter> to select an item or <escape> to cancel\n# the search. The filter options can be selected when the cursor is inside the\n# search box by pressing <Shift>+<cursor down>. Also here use the <cursor keys>\n# to select a filter and <Enter> or <escape> to activate or cancel the filter\n# option.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nSEARCHENGINE           = YES\n\n# When the SERVER_BASED_SEARCH tag is enabled the search engine will be\n# implemented using a web server instead of a web client using Javascript. There\n# are two flavors of web server based searching depending on the EXTERNAL_SEARCH\n# setting. When disabled, doxygen will generate a PHP script for searching and\n# an index file used by the script. When EXTERNAL_SEARCH is enabled the indexing\n# and searching needs to be provided by external tools. See the section\n# \"External Indexing and Searching\" for details.\n# The default value is: NO.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nSERVER_BASED_SEARCH    = NO\n\n# When EXTERNAL_SEARCH tag is enabled doxygen will no longer generate the PHP\n# script for searching. Instead the search results are written to an XML file\n# which needs to be processed by an external indexer. Doxygen will invoke an\n# external search engine pointed to by the SEARCHENGINE_URL option to obtain the\n# search results.\n#\n# Doxygen ships with an example indexer (doxyindexer) and search engine\n# (doxysearch.cgi) which are based on the open source search engine library\n# Xapian (see: https://xapian.org/).\n#\n# See the section \"External Indexing and Searching\" for details.\n# The default value is: NO.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nEXTERNAL_SEARCH        = NO\n\n# The SEARCHENGINE_URL should point to a search engine hosted by a web server\n# which will return the search results when EXTERNAL_SEARCH is enabled.\n#\n# Doxygen ships with an example indexer (doxyindexer) and search engine\n# (doxysearch.cgi) which are based on the open source search engine library\n# Xapian (see: https://xapian.org/). See the section \"External Indexing and\n# Searching\" for details.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nSEARCHENGINE_URL       =\n\n# When SERVER_BASED_SEARCH and EXTERNAL_SEARCH are both enabled the unindexed\n# search data is written to a file for indexing by an external tool. With the\n# SEARCHDATA_FILE tag the name of this file can be specified.\n# The default file is: searchdata.xml.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nSEARCHDATA_FILE        = searchdata.xml\n\n# When SERVER_BASED_SEARCH and EXTERNAL_SEARCH are both enabled the\n# EXTERNAL_SEARCH_ID tag can be used as an identifier for the project. This is\n# useful in combination with EXTRA_SEARCH_MAPPINGS to search through multiple\n# projects and redirect the results back to the right project.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nEXTERNAL_SEARCH_ID     =\n\n# The EXTRA_SEARCH_MAPPINGS tag can be used to enable searching through doxygen\n# projects other than the one defined by this configuration file, but that are\n# all added to the same external search index. Each project needs to have a\n# unique id set via EXTERNAL_SEARCH_ID. The search mapping then maps the id of\n# to a relative location where the documentation can be found. The format is:\n# EXTRA_SEARCH_MAPPINGS = tagname1=loc1 tagname2=loc2 ...\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nEXTRA_SEARCH_MAPPINGS  =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the LaTeX output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_LATEX tag is set to YES, doxygen will generate LaTeX output.\n# The default value is: YES.\n\nGENERATE_LATEX         = NO\n\n# The LATEX_OUTPUT tag is used to specify where the LaTeX docs will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: latex.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_OUTPUT           = latex\n\n# The LATEX_CMD_NAME tag can be used to specify the LaTeX command name to be\n# invoked.\n#\n# Note that when enabling USE_PDFLATEX this option is only used for generating\n# bitmaps for formulas in the HTML output, but not in the Makefile that is\n# written to the output directory.\n# The default file is: latex.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_CMD_NAME         = latex\n\n# The MAKEINDEX_CMD_NAME tag can be used to specify the command name to generate\n# index for LaTeX.\n# The default file is: makeindex.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nMAKEINDEX_CMD_NAME     = makeindex\n\n# If the COMPACT_LATEX tag is set to YES, doxygen generates more compact LaTeX\n# documents. This may be useful for small projects and may help to save some\n# trees in general.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nCOMPACT_LATEX          = NO\n\n# The PAPER_TYPE tag can be used to set the paper type that is used by the\n# printer.\n# Possible values are: a4 (210 x 297 mm), letter (8.5 x 11 inches), legal (8.5 x\n# 14 inches) and executive (7.25 x 10.5 inches).\n# The default value is: a4.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nPAPER_TYPE             = a4\n\n# The EXTRA_PACKAGES tag can be used to specify one or more LaTeX package names\n# that should be included in the LaTeX output. The package can be specified just\n# by its name or with the correct syntax as to be used with the LaTeX\n# \\usepackage command. To get the times font for instance you can specify :\n# EXTRA_PACKAGES=times or EXTRA_PACKAGES={times}\n# To use the option intlimits with the amsmath package you can specify:\n# EXTRA_PACKAGES=[intlimits]{amsmath}\n# If left blank no extra packages will be included.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nEXTRA_PACKAGES         =\n\n# The LATEX_HEADER tag can be used to specify a personal LaTeX header for the\n# generated LaTeX document. The header should contain everything until the first\n# chapter. If it is left blank doxygen will generate a standard header. See\n# section \"Doxygen usage\" for information on how to let doxygen write the\n# default header to a separate file.\n#\n# Note: Only use a user-defined header if you know what you are doing! The\n# following commands have a special meaning inside the header: $title,\n# $datetime, $date, $doxygenversion, $projectname, $projectnumber,\n# $projectbrief, $projectlogo. Doxygen will replace $title with the empty\n# string, for the replacement values of the other commands the user is referred\n# to HTML_HEADER.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_HEADER           =\n\n# The LATEX_FOOTER tag can be used to specify a personal LaTeX footer for the\n# generated LaTeX document. The footer should contain everything after the last\n# chapter. If it is left blank doxygen will generate a standard footer. See\n# LATEX_HEADER for more information on how to generate a default footer and what\n# special commands can be used inside the footer.\n#\n# Note: Only use a user-defined footer if you know what you are doing!\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_FOOTER           =\n\n# The LATEX_EXTRA_STYLESHEET tag can be used to specify additional user-defined\n# LaTeX style sheets that are included after the standard style sheets created\n# by doxygen. Using this option one can overrule certain style aspects. Doxygen\n# will copy the style sheet files to the output directory.\n# Note: The order of the extra style sheet files is of importance (e.g. the last\n# style sheet in the list overrules the setting of the previous ones in the\n# list).\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_EXTRA_STYLESHEET =\n\n# The LATEX_EXTRA_FILES tag can be used to specify one or more extra images or\n# other source files which should be copied to the LATEX_OUTPUT output\n# directory. Note that the files will be copied as-is; there are no commands or\n# markers available.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_EXTRA_FILES      =\n\n# If the PDF_HYPERLINKS tag is set to YES, the LaTeX that is generated is\n# prepared for conversion to PDF (using ps2pdf or pdflatex). The PDF file will\n# contain links (just like the HTML output) instead of page references. This\n# makes the output suitable for online browsing using a PDF viewer.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nPDF_HYPERLINKS         = YES\n\n# If the USE_PDFLATEX tag is set to YES, doxygen will use pdflatex to generate\n# the PDF file directly from the LaTeX files. Set this option to YES, to get a\n# higher quality PDF documentation.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nUSE_PDFLATEX           = YES\n\n# If the LATEX_BATCHMODE tag is set to YES, doxygen will add the \\batchmode\n# command to the generated LaTeX files. This will instruct LaTeX to keep running\n# if errors occur, instead of asking the user for help. This option is also used\n# when generating formulas in HTML.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_BATCHMODE        = NO\n\n# If the LATEX_HIDE_INDICES tag is set to YES then doxygen will not include the\n# index chapters (such as File Index, Compound Index, etc.) in the output.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_HIDE_INDICES     = NO\n\n# If the LATEX_SOURCE_CODE tag is set to YES then doxygen will include source\n# code with syntax highlighting in the LaTeX output.\n#\n# Note that which sources are shown also depends on other settings such as\n# SOURCE_BROWSER.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_SOURCE_CODE      = NO\n\n# The LATEX_BIB_STYLE tag can be used to specify the style to use for the\n# bibliography, e.g. plainnat, or ieeetr. See\n# https://en.wikipedia.org/wiki/BibTeX and \\cite for more info.\n# The default value is: plain.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_BIB_STYLE        = plain\n\n# If the LATEX_TIMESTAMP tag is set to YES then the footer of each generated\n# page will contain the date and time when the page was generated. Setting this\n# to NO can help when comparing the output of multiple runs.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_TIMESTAMP        = NO\n\n#---------------------------------------------------------------------------\n# Configuration options related to the RTF output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_RTF tag is set to YES, doxygen will generate RTF output. The\n# RTF output is optimized for Word 97 and may not look too pretty with other RTF\n# readers/editors.\n# The default value is: NO.\n\nGENERATE_RTF           = NO\n\n# The RTF_OUTPUT tag is used to specify where the RTF docs will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: rtf.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_OUTPUT             = rtf\n\n# If the COMPACT_RTF tag is set to YES, doxygen generates more compact RTF\n# documents. This may be useful for small projects and may help to save some\n# trees in general.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nCOMPACT_RTF            = NO\n\n# If the RTF_HYPERLINKS tag is set to YES, the RTF that is generated will\n# contain hyperlink fields. The RTF file will contain links (just like the HTML\n# output) instead of page references. This makes the output suitable for online\n# browsing using Word or some other Word compatible readers that support those\n# fields.\n#\n# Note: WordPad (write) and others do not support links.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_HYPERLINKS         = NO\n\n# Load stylesheet definitions from file. Syntax is similar to doxygen's config\n# file, i.e. a series of assignments. You only have to provide replacements,\n# missing definitions are set to their default value.\n#\n# See also section \"Doxygen usage\" for information on how to generate the\n# default style sheet that doxygen normally uses.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_STYLESHEET_FILE    =\n\n# Set optional variables used in the generation of an RTF document. Syntax is\n# similar to doxygen's config file. A template extensions file can be generated\n# using doxygen -e rtf extensionFile.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_EXTENSIONS_FILE    =\n\n# If the RTF_SOURCE_CODE tag is set to YES then doxygen will include source code\n# with syntax highlighting in the RTF output.\n#\n# Note that which sources are shown also depends on other settings such as\n# SOURCE_BROWSER.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_SOURCE_CODE        = NO\n\n#---------------------------------------------------------------------------\n# Configuration options related to the man page output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_MAN tag is set to YES, doxygen will generate man pages for\n# classes and files.\n# The default value is: NO.\n\nGENERATE_MAN           = NO\n\n# The MAN_OUTPUT tag is used to specify where the man pages will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it. A directory man3 will be created inside the directory specified by\n# MAN_OUTPUT.\n# The default directory is: man.\n# This tag requires that the tag GENERATE_MAN is set to YES.\n\nMAN_OUTPUT             = man\n\n# The MAN_EXTENSION tag determines the extension that is added to the generated\n# man pages. In case the manual section does not start with a number, the number\n# 3 is prepended. The dot (.) at the beginning of the MAN_EXTENSION tag is\n# optional.\n# The default value is: .3.\n# This tag requires that the tag GENERATE_MAN is set to YES.\n\nMAN_EXTENSION          = .3\n\n# The MAN_SUBDIR tag determines the name of the directory created within\n# MAN_OUTPUT in which the man pages are placed. If defaults to man followed by\n# MAN_EXTENSION with the initial . removed.\n# This tag requires that the tag GENERATE_MAN is set to YES.\n\nMAN_SUBDIR             =\n\n# If the MAN_LINKS tag is set to YES and doxygen generates man output, then it\n# will generate one additional man file for each entity documented in the real\n# man page(s). These additional files only source the real man page, but without\n# them the man command would be unable to find the correct page.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_MAN is set to YES.\n\nMAN_LINKS              = NO\n\n#---------------------------------------------------------------------------\n# Configuration options related to the XML output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_XML tag is set to YES, doxygen will generate an XML file that\n# captures the structure of the code including all documentation.\n# The default value is: NO.\n\nGENERATE_XML           = YES\n\n# The XML_OUTPUT tag is used to specify where the XML pages will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: xml.\n# This tag requires that the tag GENERATE_XML is set to YES.\n\nXML_OUTPUT             = xml\n\n# If the XML_PROGRAMLISTING tag is set to YES, doxygen will dump the program\n# listings (including syntax highlighting and cross-referencing information) to\n# the XML output. Note that enabling this will significantly increase the size\n# of the XML output.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_XML is set to YES.\n\nXML_PROGRAMLISTING     = YES\n\n#---------------------------------------------------------------------------\n# Configuration options related to the DOCBOOK output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_DOCBOOK tag is set to YES, doxygen will generate Docbook files\n# that can be used to generate PDF.\n# The default value is: NO.\n\nGENERATE_DOCBOOK       = NO\n\n# The DOCBOOK_OUTPUT tag is used to specify where the Docbook pages will be put.\n# If a relative path is entered the value of OUTPUT_DIRECTORY will be put in\n# front of it.\n# The default directory is: docbook.\n# This tag requires that the tag GENERATE_DOCBOOK is set to YES.\n\nDOCBOOK_OUTPUT         = docbook\n\n# If the DOCBOOK_PROGRAMLISTING tag is set to YES, doxygen will include the\n# program listings (including syntax highlighting and cross-referencing\n# information) to the DOCBOOK output. Note that enabling this will significantly\n# increase the size of the DOCBOOK output.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_DOCBOOK is set to YES.\n\nDOCBOOK_PROGRAMLISTING = NO\n\n#---------------------------------------------------------------------------\n# Configuration options for the AutoGen Definitions output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_AUTOGEN_DEF tag is set to YES, doxygen will generate an\n# AutoGen Definitions (see http://autogen.sourceforge.net/) file that captures\n# the structure of the code including all documentation. Note that this feature\n# is still experimental and incomplete at the moment.\n# The default value is: NO.\n\nGENERATE_AUTOGEN_DEF   = NO\n\n#---------------------------------------------------------------------------\n# Configuration options related to the Perl module output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_PERLMOD tag is set to YES, doxygen will generate a Perl module\n# file that captures the structure of the code including all documentation.\n#\n# Note that this feature is still experimental and incomplete at the moment.\n# The default value is: NO.\n\nGENERATE_PERLMOD       = NO\n\n# If the PERLMOD_LATEX tag is set to YES, doxygen will generate the necessary\n# Makefile rules, Perl scripts and LaTeX code to be able to generate PDF and DVI\n# output from the Perl module output.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_PERLMOD is set to YES.\n\nPERLMOD_LATEX          = NO\n\n# If the PERLMOD_PRETTY tag is set to YES, the Perl module output will be nicely\n# formatted so it can be parsed by a human reader. This is useful if you want to\n# understand what is going on. On the other hand, if this tag is set to NO, the\n# size of the Perl module output will be much smaller and Perl will parse it\n# just the same.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_PERLMOD is set to YES.\n\nPERLMOD_PRETTY         = YES\n\n# The names of the make variables in the generated doxyrules.make file are\n# prefixed with the string contained in PERLMOD_MAKEVAR_PREFIX. This is useful\n# so different doxyrules.make files included by the same Makefile don't\n# overwrite each other's variables.\n# This tag requires that the tag GENERATE_PERLMOD is set to YES.\n\nPERLMOD_MAKEVAR_PREFIX =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the preprocessor\n#---------------------------------------------------------------------------\n\n# If the ENABLE_PREPROCESSING tag is set to YES, doxygen will evaluate all\n# C-preprocessor directives found in the sources and include files.\n# The default value is: YES.\n\nENABLE_PREPROCESSING   = YES\n\n# If the MACRO_EXPANSION tag is set to YES, doxygen will expand all macro names\n# in the source code. If set to NO, only conditional compilation will be\n# performed. Macro expansion can be done in a controlled way by setting\n# EXPAND_ONLY_PREDEF to YES.\n# The default value is: NO.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nMACRO_EXPANSION        = NO\n\n# If the EXPAND_ONLY_PREDEF and MACRO_EXPANSION tags are both set to YES then\n# the macro expansion is limited to the macros specified with the PREDEFINED and\n# EXPAND_AS_DEFINED tags.\n# The default value is: NO.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nEXPAND_ONLY_PREDEF     = NO\n\n# If the SEARCH_INCLUDES tag is set to YES, the include files in the\n# INCLUDE_PATH will be searched if a #include is found.\n# The default value is: YES.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nSEARCH_INCLUDES        = YES\n\n# The INCLUDE_PATH tag can be used to specify one or more directories that\n# contain include files that are not input files but should be processed by the\n# preprocessor.\n# This tag requires that the tag SEARCH_INCLUDES is set to YES.\n\nINCLUDE_PATH           =\n\n# You can use the INCLUDE_FILE_PATTERNS tag to specify one or more wildcard\n# patterns (like *.h and *.hpp) to filter out the header-files in the\n# directories. If left blank, the patterns specified with FILE_PATTERNS will be\n# used.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nINCLUDE_FILE_PATTERNS  =\n\n# The PREDEFINED tag can be used to specify one or more macro names that are\n# defined before the preprocessor is started (similar to the -D option of e.g.\n# gcc). The argument of the tag is a list of macros of the form: name or\n# name=definition (no spaces). If the definition and the \"=\" are omitted, \"=1\"\n# is assumed. To prevent a macro definition from being undefined via #undef or\n# recursively expanded use the := operator instead of the = operator.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nPREDEFINED             =    \n\n# If the MACRO_EXPANSION and EXPAND_ONLY_PREDEF tags are set to YES then this\n# tag can be used to specify a list of macro names that should be expanded. The\n# macro definition that is found in the sources will be used. Use the PREDEFINED\n# tag if you want to use a different macro definition that overrules the\n# definition found in the source code.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nEXPAND_AS_DEFINED      =\n\n# If the SKIP_FUNCTION_MACROS tag is set to YES then doxygen's preprocessor will\n# remove all references to function-like macros that are alone on a line, have\n# an all uppercase name, and do not end with a semicolon. Such function macros\n# are typically used for boiler-plate code, and will confuse the parser if not\n# removed.\n# The default value is: YES.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nSKIP_FUNCTION_MACROS   = YES\n\n#---------------------------------------------------------------------------\n# Configuration options related to external references\n#---------------------------------------------------------------------------\n\n# The TAGFILES tag can be used to specify one or more tag files. For each tag\n# file the location of the external documentation should be added. The format of\n# a tag file without this location is as follows:\n# TAGFILES = file1 file2 ...\n# Adding location for the tag files is done as follows:\n# TAGFILES = file1=loc1 \"file2 = loc2\" ...\n# where loc1 and loc2 can be relative or absolute paths or URLs. See the\n# section \"Linking to external documentation\" for more information about the use\n# of tag files.\n# Note: Each tag file must have a unique name (where the name does NOT include\n# the path). If a tag file is not located in the directory in which doxygen is\n# run, you must also specify the path to the tagfile here.\n\nTAGFILES               =\n\n# When a file name is specified after GENERATE_TAGFILE, doxygen will create a\n# tag file that is based on the input files it reads. See section \"Linking to\n# external documentation\" for more information about the usage of tag files.\n\nGENERATE_TAGFILE       =\n\n# If the ALLEXTERNALS tag is set to YES, all external class will be listed in\n# the class index. If set to NO, only the inherited external classes will be\n# listed.\n# The default value is: NO.\n\nALLEXTERNALS           = NO\n\n# If the EXTERNAL_GROUPS tag is set to YES, all external groups will be listed\n# in the modules index. If set to NO, only the current project's groups will be\n# listed.\n# The default value is: YES.\n\nEXTERNAL_GROUPS        = YES\n\n# If the EXTERNAL_PAGES tag is set to YES, all external pages will be listed in\n# the related pages index. If set to NO, only the current project's pages will\n# be listed.\n# The default value is: YES.\n\nEXTERNAL_PAGES         = YES\n\n# The PERL_PATH should be the absolute path and name of the perl script\n# interpreter (i.e. the result of 'which perl').\n# The default file (with absolute path) is: /usr/bin/perl.\n\nPERL_PATH              = /usr/bin/perl\n\n#---------------------------------------------------------------------------\n# Configuration options related to the dot tool\n#---------------------------------------------------------------------------\n\n# If the CLASS_DIAGRAMS tag is set to YES, doxygen will generate a class diagram\n# (in HTML and LaTeX) for classes with base or super classes. Setting the tag to\n# NO turns the diagrams off. Note that this option also works with HAVE_DOT\n# disabled, but it is recommended to install and use dot, since it yields more\n# powerful graphs.\n# The default value is: YES.\n\nCLASS_DIAGRAMS         = YES\n\n# You can define message sequence charts within doxygen comments using the \\msc\n# command. Doxygen will then run the mscgen tool (see:\n# http://www.mcternan.me.uk/mscgen/)) to produce the chart and insert it in the\n# documentation. The MSCGEN_PATH tag allows you to specify the directory where\n# the mscgen tool resides. If left empty the tool is assumed to be found in the\n# default search path.\n\nMSCGEN_PATH            =\n\n# You can include diagrams made with dia in doxygen documentation. Doxygen will\n# then run dia to produce the diagram and insert it in the documentation. The\n# DIA_PATH tag allows you to specify the directory where the dia binary resides.\n# If left empty dia is assumed to be found in the default search path.\n\nDIA_PATH               =\n\n# If set to YES the inheritance and collaboration graphs will hide inheritance\n# and usage relations if the target is undocumented or is not a class.\n# The default value is: YES.\n\nHIDE_UNDOC_RELATIONS   = YES\n\n# If you set the HAVE_DOT tag to YES then doxygen will assume the dot tool is\n# available from the path. This tool is part of Graphviz (see:\n# http://www.graphviz.org/), a graph visualization toolkit from AT&T and Lucent\n# Bell Labs. The other options in this section have no effect if this option is\n# set to NO\n# The default value is: NO.\n\nHAVE_DOT               = YES\n\n# The DOT_NUM_THREADS specifies the number of dot invocations doxygen is allowed\n# to run in parallel. When set to 0 doxygen will base this on the number of\n# processors available in the system. You can set it explicitly to a value\n# larger than 0 to get control over the balance between CPU load and processing\n# speed.\n# Minimum value: 0, maximum value: 32, default value: 0.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_NUM_THREADS        = 0\n\n# When you want a differently looking font in the dot files that doxygen\n# generates you can specify the font name using DOT_FONTNAME. You need to make\n# sure dot is able to find the font, which can be done by putting it in a\n# standard location or by setting the DOTFONTPATH environment variable or by\n# setting DOT_FONTPATH to the directory containing the font.\n# The default value is: Helvetica.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_FONTNAME           = Helvetica\n\n# The DOT_FONTSIZE tag can be used to set the size (in points) of the font of\n# dot graphs.\n# Minimum value: 4, maximum value: 24, default value: 10.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_FONTSIZE           = 10\n\n# By default doxygen will tell dot to use the default font as specified with\n# DOT_FONTNAME. If you specify a different font using DOT_FONTNAME you can set\n# the path where dot can find it using this tag.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_FONTPATH           =\n\n# If the CLASS_GRAPH tag is set to YES then doxygen will generate a graph for\n# each documented class showing the direct and indirect inheritance relations.\n# Setting this tag to YES will force the CLASS_DIAGRAMS tag to NO.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCLASS_GRAPH            = YES\n\n# If the COLLABORATION_GRAPH tag is set to YES then doxygen will generate a\n# graph for each documented class showing the direct and indirect implementation\n# dependencies (inheritance, containment, and class references variables) of the\n# class with other documented classes.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCOLLABORATION_GRAPH    = YES\n\n# If the GROUP_GRAPHS tag is set to YES then doxygen will generate a graph for\n# groups, showing the direct groups dependencies.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nGROUP_GRAPHS           = YES\n\n# If the UML_LOOK tag is set to YES, doxygen will generate inheritance and\n# collaboration diagrams in a style similar to the OMG's Unified Modeling\n# Language.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nUML_LOOK               = NO\n\n# If the UML_LOOK tag is enabled, the fields and methods are shown inside the\n# class node. If there are many fields or methods and many nodes the graph may\n# become too big to be useful. The UML_LIMIT_NUM_FIELDS threshold limits the\n# number of items for each type to make the size more manageable. Set this to 0\n# for no limit. Note that the threshold may be exceeded by 50% before the limit\n# is enforced. So when you set the threshold to 10, up to 15 fields may appear,\n# but if the number exceeds 15, the total amount of fields shown is limited to\n# 10.\n# Minimum value: 0, maximum value: 100, default value: 10.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nUML_LIMIT_NUM_FIELDS   = 10\n\n# If the TEMPLATE_RELATIONS tag is set to YES then the inheritance and\n# collaboration graphs will show the relations between templates and their\n# instances.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nTEMPLATE_RELATIONS     = NO\n\n# If the INCLUDE_GRAPH, ENABLE_PREPROCESSING and SEARCH_INCLUDES tags are set to\n# YES then doxygen will generate a graph for each documented file showing the\n# direct and indirect include dependencies of the file with other documented\n# files.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nINCLUDE_GRAPH          = YES\n\n# If the INCLUDED_BY_GRAPH, ENABLE_PREPROCESSING and SEARCH_INCLUDES tags are\n# set to YES then doxygen will generate a graph for each documented file showing\n# the direct and indirect include dependencies of the file with other documented\n# files.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nINCLUDED_BY_GRAPH      = YES\n\n# If the CALL_GRAPH tag is set to YES then doxygen will generate a call\n# dependency graph for every global function or class method.\n#\n# Note that enabling this option will significantly increase the time of a run.\n# So in most cases it will be better to enable call graphs for selected\n# functions only using the \\callgraph command. Disabling a call graph can be\n# accomplished by means of the command \\hidecallgraph.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCALL_GRAPH             = YES\n\n# If the CALLER_GRAPH tag is set to YES then doxygen will generate a caller\n# dependency graph for every global function or class method.\n#\n# Note that enabling this option will significantly increase the time of a run.\n# So in most cases it will be better to enable caller graphs for selected\n# functions only using the \\callergraph command. Disabling a caller graph can be\n# accomplished by means of the command \\hidecallergraph.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCALLER_GRAPH           = YES\n\n# If the GRAPHICAL_HIERARCHY tag is set to YES then doxygen will graphical\n# hierarchy of all classes instead of a textual one.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nGRAPHICAL_HIERARCHY    = YES\n\n# If the DIRECTORY_GRAPH tag is set to YES then doxygen will show the\n# dependencies a directory has on other directories in a graphical way. The\n# dependency relations are determined by the #include relations between the\n# files in the directories.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDIRECTORY_GRAPH        = YES\n\n# The DOT_IMAGE_FORMAT tag can be used to set the image format of the images\n# generated by dot. For an explanation of the image formats see the section\n# output formats in the documentation of the dot tool (Graphviz (see:\n# http://www.graphviz.org/)).\n# Note: If you choose svg you need to set HTML_FILE_EXTENSION to xhtml in order\n# to make the SVG files visible in IE 9+ (other browsers do not have this\n# requirement).\n# Possible values are: png, jpg, gif, svg, png:gd, png:gd:gd, png:cairo,\n# png:cairo:gd, png:cairo:cairo, png:cairo:gdiplus, png:gdiplus and\n# png:gdiplus:gdiplus.\n# The default value is: png.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_IMAGE_FORMAT       = svg\n\n# If DOT_IMAGE_FORMAT is set to svg, then this option can be set to YES to\n# enable generation of interactive SVG images that allow zooming and panning.\n#\n# Note that this requires a modern browser other than Internet Explorer. Tested\n# and working are Firefox, Chrome, Safari, and Opera.\n# Note: For IE 9+ you need to set HTML_FILE_EXTENSION to xhtml in order to make\n# the SVG files visible. Older versions of IE do not have SVG support.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nINTERACTIVE_SVG        = YES\n\n# The DOT_PATH tag can be used to specify the path where the dot tool can be\n# found. If left blank, it is assumed the dot tool can be found in the path.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_PATH               =\n\n# The DOTFILE_DIRS tag can be used to specify one or more directories that\n# contain dot files that are included in the documentation (see the \\dotfile\n# command).\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOTFILE_DIRS           =\n\n# The MSCFILE_DIRS tag can be used to specify one or more directories that\n# contain msc files that are included in the documentation (see the \\mscfile\n# command).\n\nMSCFILE_DIRS           =\n\n# The DIAFILE_DIRS tag can be used to specify one or more directories that\n# contain dia files that are included in the documentation (see the \\diafile\n# command).\n\nDIAFILE_DIRS           =\n\n# When using plantuml, the PLANTUML_JAR_PATH tag should be used to specify the\n# path where java can find the plantuml.jar file. If left blank, it is assumed\n# PlantUML is not used or called during a preprocessing step. Doxygen will\n# generate a warning when it encounters a \\startuml command in this case and\n# will not generate output for the diagram.\n\nPLANTUML_JAR_PATH      =\n\n# When using plantuml, the PLANTUML_CFG_FILE tag can be used to specify a\n# configuration file for plantuml.\n\nPLANTUML_CFG_FILE      =\n\n# When using plantuml, the specified paths are searched for files specified by\n# the !include statement in a plantuml block.\n\nPLANTUML_INCLUDE_PATH  =\n\n# The DOT_GRAPH_MAX_NODES tag can be used to set the maximum number of nodes\n# that will be shown in the graph. If the number of nodes in a graph becomes\n# larger than this value, doxygen will truncate the graph, which is visualized\n# by representing a node as a red box. Note that doxygen if the number of direct\n# children of the root node in a graph is already larger than\n# DOT_GRAPH_MAX_NODES then the graph will not be shown at all. Also note that\n# the size of a graph can be further restricted by MAX_DOT_GRAPH_DEPTH.\n# Minimum value: 0, maximum value: 10000, default value: 50.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_GRAPH_MAX_NODES    = 50\n\n# The MAX_DOT_GRAPH_DEPTH tag can be used to set the maximum depth of the graphs\n# generated by dot. A depth value of 3 means that only nodes reachable from the\n# root by following a path via at most 3 edges will be shown. Nodes that lay\n# further from the root node will be omitted. Note that setting this option to 1\n# or 2 may greatly reduce the computation time needed for large code bases. Also\n# note that the size of a graph can be further restricted by\n# DOT_GRAPH_MAX_NODES. Using a depth of 0 means no depth restriction.\n# Minimum value: 0, maximum value: 1000, default value: 0.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nMAX_DOT_GRAPH_DEPTH    = 0\n\n# Set the DOT_TRANSPARENT tag to YES to generate images with a transparent\n# background. This is disabled by default, because dot on Windows does not seem\n# to support this out of the box.\n#\n# Warning: Depending on the platform used, enabling this option may lead to\n# badly anti-aliased labels on the edges of a graph (i.e. they become hard to\n# read).\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_TRANSPARENT        = YES\n\n# Set the DOT_MULTI_TARGETS tag to YES to allow dot to generate multiple output\n# files in one run (i.e. multiple -o and -T options on the command line). This\n# makes dot run faster, but since only newer versions of dot (>1.8.10) support\n# this, this feature is disabled by default.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_MULTI_TARGETS      = NO\n\n# If the GENERATE_LEGEND tag is set to YES doxygen will generate a legend page\n# explaining the meaning of the various boxes and arrows in the dot generated\n# graphs.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nGENERATE_LEGEND        = YES\n\n# If the DOT_CLEANUP tag is set to YES, doxygen will remove the intermediate dot\n# files that are used to generate the various graphs.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_CLEANUP            = YES\n"
        },
        {
          "name": "Embedded",
          "type": "tree",
          "content": null
        },
        {
          "name": "Fragmenter",
          "type": "tree",
          "content": null
        },
        {
          "name": "Geospatial",
          "type": "tree",
          "content": null
        },
        {
          "name": "HeavyDB.cpp",
          "type": "blob",
          "size": 24.1513671875,
          "content": "/*\n * Copyright 2022 HEAVY.AI, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include \"DataMgr/ForeignStorage/ForeignStorageInterface.h\"\n#include \"ThriftHandler/DBHandler.h\"\n#ifdef HAVE_THRIFT_MESSAGE_LIMIT\n#include \"Shared/ThriftConfig.h\"\n#endif\n\n#ifdef HAVE_THRIFT_THREADFACTORY\n#include <thrift/concurrency/ThreadFactory.h>\n#else\n#include <thrift/concurrency/PlatformThreadFactory.h>\n#endif\n\n#include <thrift/concurrency/ThreadManager.h>\n#include <thrift/protocol/TBinaryProtocol.h>\n#include <thrift/server/TThreadedServer.h>\n#include <thrift/transport/TBufferTransports.h>\n#include <thrift/transport/THttpServer.h>\n#include <thrift/transport/TSSLServerSocket.h>\n#include <thrift/transport/TSSLSocket.h>\n#include <thrift/transport/TServerSocket.h>\n#include \"Shared/ThriftJSONProtocolInclude.h\"\n\n#include \"Logger/Logger.h\"\n#include \"Shared/SystemParameters.h\"\n#include \"Shared/file_delete.h\"\n#include \"Shared/heavyai_shared_mutex.h\"\n#include \"Shared/misc.h\"\n#include \"Shared/scope.h\"\n\n#include <boost/algorithm/string.hpp>\n#include <boost/algorithm/string/trim.hpp>\n#include <boost/filesystem.hpp>\n#include <boost/locale/generator.hpp>\n#include <boost/make_shared.hpp>\n#include <boost/program_options.hpp>\n\n#ifdef ENABLE_TBB\n#include <tbb/global_control.h>\n#endif\n\n#ifdef __linux__\n#include <unistd.h>\n#endif\n\n#include <csignal>\n#include <cstdlib>\n#include <sstream>\n#include <thread>\n#include <vector>\n\n#ifdef HAVE_AWS_S3\n#include \"DataMgr/HeavyDbAwsSdk.h\"\n#endif\n#include \"Catalog/AlterColumnRecovery.h\"\n#include \"MigrationMgr/MigrationMgr.h\"\n#include \"Shared/Compressor.h\"\n#include \"Shared/SystemParameters.h\"\n#include \"Shared/file_delete.h\"\n#include \"Shared/scope.h\"\n#include \"ThriftHandler/ForeignTableRefreshScheduler.h\"\n\nusing namespace ::apache::thrift;\nusing namespace ::apache::thrift::concurrency;\nusing namespace ::apache::thrift::protocol;\nusing namespace ::apache::thrift::server;\nusing namespace ::apache::thrift::transport;\n\nextern bool g_enable_thrift_logs;\n\n// Set g_running to false to trigger normal server shutdown.\nstd::atomic<bool> g_running{true};\n\nextern bool g_enable_http_binary_server;\n\nnamespace {  // anonymous\n\nstd::atomic<int> g_saw_signal{-1};\n\nstd::shared_ptr<TThreadedServer> g_thrift_http_server;\nstd::shared_ptr<TThreadedServer> g_thrift_http_binary_server;\nstd::shared_ptr<TThreadedServer> g_thrift_tcp_server;\n\nstd::shared_ptr<DBHandler> g_warmup_handler;\n// global \"g_warmup_handler\" needed to avoid circular dependency\n// between \"DBHandler\" & function \"run_warmup_queries\"\n\nstd::shared_ptr<DBHandler> g_db_handler;\n\nvoid register_signal_handler(int signum, void (*handler)(int)) {\n#ifdef _WIN32\n  signal(signum, handler);\n#else\n  struct sigaction act;\n  memset(&act, 0, sizeof(act));\n  if (handler != SIG_DFL && handler != SIG_IGN) {\n    // block all signal deliveries while inside the signal handler\n    sigfillset(&act.sa_mask);\n  }\n  act.sa_handler = handler;\n  sigaction(signum, &act, NULL);\n#endif\n}\n\n// Signal handler to set a global flag telling the server to exit.\n// Do not call other functions inside this (or any) signal handler\n// unless you really know what you are doing. See also:\n//   man 7 signal-safety\n//   man 7 signal\n//   https://en.wikipedia.org/wiki/Reentrancy_(computing)\nvoid heavydb_signal_handler(int signum) {\n  // Record the signal number for logging during shutdown.\n  // Only records the first signal if called more than once.\n  int expected_signal{-1};\n  if (!g_saw_signal.compare_exchange_strong(expected_signal, signum)) {\n    return;  // this wasn't the first signal\n  }\n\n  // This point should never be reached more than once.\n\n  // Tell heartbeat() to shutdown by unsetting the 'g_running' flag.\n  // If 'g_running' is already false, this has no effect and the\n  // shutdown is already in progress.\n  g_running = false;\n\n  // Handle core dumps specially by pausing inside this signal handler\n  // because on some systems, some signals will execute their default\n  // action immediately when and if the signal handler returns.\n  // We would like to do some emergency cleanup before core dump.\n  if (signum == SIGABRT || signum == SIGSEGV || signum == SIGFPE\n#ifndef _WIN32\n      || signum == SIGQUIT\n#endif\n  ) {\n    // Wait briefly to give heartbeat() a chance to flush the logs and\n    // do any other emergency shutdown tasks.\n    std::this_thread::sleep_for(std::chrono::seconds(2));\n\n    // Explicitly trigger whatever default action this signal would\n    // have done, such as terminate the process or dump core.\n    // Signals are currently blocked so this new signal will be queued\n    // until this signal handler returns.\n    register_signal_handler(signum, SIG_DFL);\n#ifdef _WIN32\n    raise(signum);\n#else\n    kill(getpid(), signum);\n#endif\n    std::this_thread::sleep_for(std::chrono::seconds(5));\n\n#ifndef __APPLE__\n    // as a last resort, abort\n    // primary used in Docker environments, where we can end up with PID 1 and fail to\n    // catch unix signals\n    quick_exit(signum);\n#endif\n  }\n}\n\nvoid register_signal_handlers() {\n  register_signal_handler(SIGINT, heavydb_signal_handler);\n#ifndef _WIN32\n  register_signal_handler(SIGQUIT, heavydb_signal_handler);\n  register_signal_handler(SIGHUP, heavydb_signal_handler);\n#endif\n  register_signal_handler(SIGTERM, heavydb_signal_handler);\n  register_signal_handler(SIGSEGV, heavydb_signal_handler);\n  register_signal_handler(SIGABRT, heavydb_signal_handler);\n#ifndef _WIN32\n  // Thrift secure socket can cause problems with SIGPIPE\n  register_signal_handler(SIGPIPE, SIG_IGN);\n#endif\n}\n}  // anonymous namespace\n\nvoid start_server(std::shared_ptr<TThreadedServer> server, const int port) {\n  try {\n    server->serve();\n    if (errno != 0) {\n      throw std::runtime_error(std::string(\"Thrift server exited: \") +\n                               std::strerror(errno));\n    }\n  } catch (std::exception& e) {\n    LOG(ERROR) << \"Exception: \" << e.what() << \": port \" << port << std::endl;\n  }\n}\n\nvoid releaseWarmupSession(TSessionId& sessionId, std::ifstream& query_file) noexcept {\n  query_file.close();\n  if (sessionId != g_warmup_handler->getInvalidSessionId()) {\n    try {\n      g_warmup_handler->disconnect(sessionId);\n    } catch (...) {\n      LOG(ERROR) << \"Failed to disconnect warmup session, possible failure to run warmup \"\n                    \"queries.\";\n    }\n  }\n}\n\nvoid run_warmup_queries(std::shared_ptr<DBHandler> handler,\n                        std::string base_path,\n                        std::string query_file_path) {\n  // run warmup queries to load cache if requested\n  if (query_file_path.empty()) {\n    return;\n  }\n  if (handler->isAggregator()) {\n    LOG(INFO) << \"Skipping warmup query execution on the aggregator, queries should be \"\n                 \"run directly on the leaf nodes.\";\n    return;\n  }\n\n  LOG(INFO) << \"Running DB warmup with queries from \" << query_file_path;\n  try {\n    g_warmup_handler = handler;\n    std::string db_info;\n    std::string user_keyword, user_name, db_name;\n    std::ifstream query_file;\n    Catalog_Namespace::UserMetadata user;\n    Catalog_Namespace::DBMetadata db;\n    TSessionId sessionId = g_warmup_handler->getInvalidSessionId();\n\n    ScopeGuard session_guard = [&] { releaseWarmupSession(sessionId, query_file); };\n    query_file.open(query_file_path);\n    while (std::getline(query_file, db_info)) {\n      if (db_info.length() == 0) {\n        continue;\n      }\n      std::istringstream iss(db_info);\n      iss >> user_keyword >> user_name >> db_name;\n      if (user_keyword.compare(0, 4, \"USER\") == 0) {\n        // connect to DB for given user_name/db_name with super_user_rights (without\n        // password), & start session\n        g_warmup_handler->super_user_rights_ = true;\n        g_warmup_handler->connect(sessionId, user_name, \"\", db_name);\n        g_warmup_handler->super_user_rights_ = false;\n\n        // read and run one query at a time for the DB with the setup connection\n        TQueryResult ret;\n        std::string single_query;\n        while (std::getline(query_file, single_query)) {\n          boost::algorithm::trim(single_query);\n          if (single_query.length() == 0 || single_query[0] == '-') {\n            continue;\n          }\n          if (single_query[0] == '}') {\n            single_query.clear();\n            break;\n          }\n          if (single_query.find(';') == single_query.npos) {\n            std::string multiline_query;\n            std::getline(query_file, multiline_query, ';');\n            single_query += multiline_query;\n          }\n\n          try {\n            g_warmup_handler->sql_execute(ret, sessionId, single_query, true, \"\", -1, -1);\n          } catch (...) {\n            LOG(WARNING) << \"Exception while executing '\" << single_query\n                         << \"', ignoring\";\n          }\n          single_query.clear();\n        }\n\n        // stop session and disconnect from the DB\n        g_warmup_handler->disconnect(sessionId);\n        sessionId = g_warmup_handler->getInvalidSessionId();\n      } else {\n        LOG(WARNING) << \"\\nSyntax error in the file: \" << query_file_path.c_str()\n                     << \" Missing expected keyword USER. Following line will be ignored: \"\n                     << db_info.c_str() << std::endl;\n      }\n      db_info.clear();\n    }\n  } catch (const std::exception& e) {\n    LOG(WARNING)\n        << \"Exception while executing warmup queries. \"\n        << \"Warmup may not be fully completed. Will proceed nevertheless.\\nError was: \"\n        << e.what();\n  }\n}\n\nextern bool g_enable_thrift_logs;\nextern bool g_enable_fsi;\nextern bool g_enable_foreign_table_scheduled_refresh;\n\nvoid thrift_stop() {\n  if (auto thrift_http_server = g_thrift_http_server; thrift_http_server) {\n    thrift_http_server->stop();\n  }\n  g_thrift_http_server.reset();\n\n  if (auto thrift_http_binary_server = g_thrift_http_binary_server;\n      thrift_http_binary_server) {\n    thrift_http_binary_server->stop();\n  }\n  g_thrift_http_binary_server.reset();\n\n  if (auto thrift_tcp_server = g_thrift_tcp_server; thrift_tcp_server) {\n    thrift_tcp_server->stop();\n  }\n  g_thrift_tcp_server.reset();\n}\n\nvoid heartbeat() {\n#ifndef _WIN32\n  // Block all signals for this heartbeat thread, only.\n  sigset_t set;\n  sigfillset(&set);\n  int result = pthread_sigmask(SIG_BLOCK, &set, NULL);\n  if (result != 0) {\n    throw std::runtime_error(\"heartbeat() thread startup failed\");\n  }\n#endif\n\n  // Sleep until heavydb_signal_handler or anything clears the g_running flag.\n  VLOG(1) << \"heartbeat thread starting\";\n  while (::g_running) {\n    using namespace std::chrono;\n    std::this_thread::sleep_for(1s);\n  }\n  VLOG(1) << \"heartbeat thread exiting\";\n\n  // Get the signal number if there was a signal.\n  int signum = g_saw_signal;\n  if (signum >= 1 && signum != SIGTERM) {\n    LOG(INFO) << \"Interrupt signal (\" << signum << \") received.\";\n  }\n\n  // If dumping core, try to do some quick stuff.\n  if (signum == SIGABRT || signum == SIGSEGV || signum == SIGFPE\n#ifndef _WIN32\n      || signum == SIGQUIT\n#endif\n  ) {\n    // Need to shut down calcite.\n    if (auto db_handler = g_db_handler; db_handler) {\n      db_handler->emergency_shutdown();\n    }\n    // Need to flush the logs for debugging.\n    logger::shutdown();\n    return;\n    // Core dump should begin soon after this. See heavydb_signal_handler().\n    // We leave the rest of the server process as is for the core dump image.\n  }\n\n  // Stopping the Thrift thread(s) will allow main() to return.\n  thrift_stop();\n}\n\n#ifdef HAVE_THRIFT_MESSAGE_LIMIT\nnamespace {\nclass UnboundedTBufferedTransportFactory : public TBufferedTransportFactory {\n public:\n  UnboundedTBufferedTransportFactory() : TBufferedTransportFactory() {}\n\n  std::shared_ptr<TTransport> getTransport(\n      std::shared_ptr<TTransport> transport) override {\n    return std::make_shared<TBufferedTransport>(transport, shared::default_tconfig());\n  }\n};\n\nclass UnboundedTHttpServerTransportFactory : public THttpServerTransportFactory {\n public:\n  UnboundedTHttpServerTransportFactory() : THttpServerTransportFactory() {}\n\n  std::shared_ptr<TTransport> getTransport(\n      std::shared_ptr<TTransport> transport) override {\n    return std::make_shared<THttpServer>(transport, shared::default_tconfig());\n  }\n};\n}  // namespace\n#endif\n\nint startHeavyDBServer(CommandLineOptions& prog_config_opts,\n                       bool start_http_server = true) {\n  // Prepare to launch the Thrift server.\n  LOG(INFO) << \"HeavyDB starting up\";\n  register_signal_handlers();\n#ifdef ENABLE_TBB\n  auto num_cpu_threads = cpu_threads();\n  LOG(INFO) << \"Initializing TBB with \" << num_cpu_threads << \" threads.\";\n  tbb::global_control tbb_control(tbb::global_control::max_allowed_parallelism,\n                                  num_cpu_threads);\n  threading_tbb::g_tbb_arena.initialize(num_cpu_threads);\n  const int32_t tbb_max_concurrency{threading_tbb::g_tbb_arena.max_concurrency()};\n  LOG(INFO) << \"TBB max concurrency: \" << tbb_max_concurrency << \" threads.\";\n#endif  // ENABLE_TBB\n#ifdef HAVE_AWS_S3\n  heavydb_aws_sdk::init_sdk();\n#endif  // HAVE_AWS_S3\n  std::set<std::unique_ptr<std::thread>> server_threads;\n  auto wait_for_server_threads = [&] {\n    for (auto& th : server_threads) {\n      try {\n        th->join();\n      } catch (const std::system_error& e) {\n        if (e.code() != std::errc::invalid_argument) {\n          LOG(WARNING) << \"std::thread join failed: \" << e.what();\n        }\n      } catch (const std::exception& e) {\n        LOG(WARNING) << \"std::thread join failed: \" << e.what();\n      } catch (...) {\n        LOG(WARNING) << \"std::thread join failed\";\n      }\n    }\n  };\n  ScopeGuard server_shutdown_guard = [&] {\n    // This function will never be called by exit(), but we shouldn't ever be calling\n    // exit(), we should be setting g_running to false instead.\n    LOG(INFO) << \"HeavyDB shutting down\";\n\n    g_running = false;\n\n    thrift_stop();\n\n    if (g_enable_fsi) {\n      foreign_storage::ForeignTableRefreshScheduler::stop();\n    }\n\n    g_db_handler.reset();\n\n    wait_for_server_threads();\n\n#ifdef HAVE_AWS_S3\n    heavydb_aws_sdk::shutdown_sdk();\n#endif  // HAVE_AWS_S3\n\n    // Flush the logs last to capture maximum debugging information.\n    logger::shutdown();\n  };\n\n  // start background thread to clean up _DELETE_ME files\n  const unsigned int wait_interval =\n      3;  // wait time in secs after looking for deleted file before looking again\n  server_threads.insert(std::make_unique<std::thread>(\n      file_delete,\n      std::ref(g_running),\n      wait_interval,\n      prog_config_opts.base_path + \"/\" + shared::kDataDirectoryName));\n  server_threads.insert(std::make_unique<std::thread>(heartbeat));\n\n  if (!g_enable_thrift_logs) {\n    apache::thrift::GlobalOutput.setOutputFunction([](const char* msg) {});\n  }\n\n  // Thrift event handler for database server setup.\n  try {\n    if (prog_config_opts.system_parameters.master_address.empty()) {\n      // Handler for a single database server. (DBHandler)\n      g_db_handler =\n          std::make_shared<DBHandler>(prog_config_opts.db_leaves,\n                                      prog_config_opts.string_leaves,\n                                      prog_config_opts.base_path,\n                                      prog_config_opts.allow_multifrag,\n                                      prog_config_opts.jit_debug,\n                                      prog_config_opts.intel_jit_profile,\n                                      prog_config_opts.read_only,\n                                      prog_config_opts.allow_loop_joins,\n                                      prog_config_opts.enable_rendering,\n                                      prog_config_opts.renderer_prefer_igpu,\n                                      prog_config_opts.renderer_vulkan_timeout_ms,\n                                      prog_config_opts.renderer_use_parallel_executors,\n                                      prog_config_opts.enable_auto_clear_render_mem,\n                                      prog_config_opts.render_oom_retry_threshold,\n                                      prog_config_opts.render_mem_bytes,\n                                      prog_config_opts.max_concurrent_render_sessions,\n                                      prog_config_opts.reserved_gpu_mem,\n                                      prog_config_opts.render_compositor_use_last_gpu,\n                                      prog_config_opts.renderer_enable_slab_allocation,\n                                      prog_config_opts.num_reader_threads,\n                                      prog_config_opts.authMetadata,\n                                      prog_config_opts.system_parameters,\n                                      prog_config_opts.enable_legacy_syntax,\n                                      prog_config_opts.idle_session_duration,\n                                      prog_config_opts.max_session_duration,\n                                      prog_config_opts.udf_file_name,\n                                      prog_config_opts.udf_compiler_path,\n                                      prog_config_opts.udf_compiler_options,\n#ifdef ENABLE_GEOS\n                                      prog_config_opts.libgeos_so_filename,\n#endif\n#ifdef HAVE_TORCH_TFS\n                                      prog_config_opts.torch_lib_path,\n#endif\n                                      prog_config_opts.disk_cache_config,\n                                      false);\n    } else {  // running ha server\n      LOG(FATAL)\n          << \"No High Availability module available, please contact OmniSci support\";\n    }\n  } catch (const std::exception& e) {\n    LOG(FATAL) << \"Failed to initialize service handler: \" << e.what();\n  }\n\n  // do the drop render group columns migration here too\n  // @TODO make a single entry point in MigrationMgr that will do these two and futures\n  Catalog_Namespace::SysCatalog::instance().checkDropRenderGroupColumnsMigration();\n\n  // Recover from any partially complete alter table alter column commands\n  AlterTableAlterColumnCommandRecoveryMgr::\n      resolveIncompleteAlterColumnCommandsForAllCatalogs();\n\n  if (g_enable_fsi && g_enable_foreign_table_scheduled_refresh) {\n    foreign_storage::ForeignTableRefreshScheduler::start(g_running);\n  }\n\n  // TCP port setup. We use Thrift both for a TCP socket and for an optional HTTP socket.\n  std::shared_ptr<TServerSocket> tcp_socket;\n  std::shared_ptr<TServerSocket> http_socket;\n  std::shared_ptr<TServerSocket> http_binary_socket;\n\n  if (!prog_config_opts.system_parameters.ssl_cert_file.empty() &&\n      !prog_config_opts.system_parameters.ssl_key_file.empty()) {\n    // SSL port setup.\n    auto sslSocketFactory = std::make_shared<TSSLSocketFactory>(SSLProtocol::SSLTLS);\n    sslSocketFactory->loadCertificate(\n        prog_config_opts.system_parameters.ssl_cert_file.c_str());\n    sslSocketFactory->loadPrivateKey(\n        prog_config_opts.system_parameters.ssl_key_file.c_str());\n    if (prog_config_opts.system_parameters.ssl_transport_client_auth) {\n      sslSocketFactory->authenticate(true);\n    } else {\n      sslSocketFactory->authenticate(false);\n    }\n    sslSocketFactory->ciphers(\"ALL:!ADH:!LOW:!EXP:!MD5:@STRENGTH\");\n    tcp_socket = std::make_shared<TSSLServerSocket>(\n        prog_config_opts.system_parameters.omnisci_server_port, sslSocketFactory);\n    if (start_http_server) {\n      http_socket = std::make_shared<TSSLServerSocket>(prog_config_opts.http_port,\n                                                       sslSocketFactory);\n    }\n    if (g_enable_http_binary_server) {\n      http_binary_socket = std::make_shared<TSSLServerSocket>(\n          prog_config_opts.http_binary_port, sslSocketFactory);\n    }\n    LOG(INFO) << \" HeavyDB server using encrypted connection. Cert file [\"\n              << prog_config_opts.system_parameters.ssl_cert_file << \"], key file [\"\n              << prog_config_opts.system_parameters.ssl_key_file << \"]\";\n\n  } else {\n    // Non-SSL port setup.\n    LOG(INFO) << \" HeavyDB server using unencrypted connection\";\n    tcp_socket = std::make_shared<TServerSocket>(\n        prog_config_opts.system_parameters.omnisci_server_port);\n    if (start_http_server) {\n      http_socket = std::make_shared<TServerSocket>(prog_config_opts.http_port);\n    }\n    if (g_enable_http_binary_server) {\n      http_binary_socket =\n          std::make_shared<TServerSocket>(prog_config_opts.http_binary_port);\n    }\n  }\n\n  // Thrift uses the same processor for both the TCP port and the HTTP port.\n  std::shared_ptr<TProcessor> processor{std::make_shared<TrackingProcessor>(\n      g_db_handler, prog_config_opts.log_user_origin)};\n\n  // Thrift TCP server launch.\n  std::shared_ptr<TServerTransport> tcp_st = tcp_socket;\n#ifdef HAVE_THRIFT_MESSAGE_LIMIT\n  std::shared_ptr<TTransportFactory> tcp_tf{\n      std::make_shared<UnboundedTBufferedTransportFactory>()};\n#else\n  std::shared_ptr<TTransportFactory> tcp_tf{\n      std::make_shared<TBufferedTransportFactory>()};\n#endif\n  std::shared_ptr<TProtocolFactory> tcp_pf{std::make_shared<TBinaryProtocolFactory>()};\n  g_thrift_tcp_server.reset(new TThreadedServer(processor, tcp_st, tcp_tf, tcp_pf));\n  server_threads.insert(std::make_unique<std::thread>(\n      start_server,\n      g_thrift_tcp_server,\n      prog_config_opts.system_parameters.omnisci_server_port));\n\n  // Thrift HTTP server launch.\n  if (start_http_server) {\n    std::shared_ptr<TServerTransport> http_st = http_socket;\n#ifdef HAVE_THRIFT_MESSAGE_LIMIT\n    std::shared_ptr<TTransportFactory> http_tf{\n        std::make_shared<UnboundedTHttpServerTransportFactory>()};\n#else\n    std::shared_ptr<TTransportFactory> http_tf{\n        std::make_shared<THttpServerTransportFactory>()};\n#endif\n    std::shared_ptr<TProtocolFactory> http_pf{std::make_shared<TJSONProtocolFactory>()};\n    g_thrift_http_server.reset(new TThreadedServer(processor, http_st, http_tf, http_pf));\n    server_threads.insert(std::make_unique<std::thread>(\n        start_server, g_thrift_http_server, prog_config_opts.http_port));\n  }\n\n  // Thrift HTTP binary protocol server launch.\n  if (g_enable_http_binary_server) {\n    std::shared_ptr<TServerTransport> http_binary_st = http_binary_socket;\n#ifdef HAVE_THRIFT_MESSAGE_LIMIT\n    std::shared_ptr<TTransportFactory> http_binary_tf{\n        std::make_shared<UnboundedTHttpServerTransportFactory>()};\n#else\n    std::shared_ptr<TTransportFactory> http_binary_tf{\n        std::make_shared<THttpServerTransportFactory>()};\n#endif\n    std::shared_ptr<TProtocolFactory> http_binary_pf{\n        std::make_shared<TBinaryProtocolFactory>()};\n    g_thrift_http_binary_server.reset(\n        new TThreadedServer(processor, http_binary_st, http_binary_tf, http_binary_pf));\n    server_threads.insert(std::make_unique<std::thread>(\n        start_server, g_thrift_http_binary_server, prog_config_opts.http_binary_port));\n  }\n\n  // Run warm up queries if any exist.\n  run_warmup_queries(\n      g_db_handler, prog_config_opts.base_path, prog_config_opts.db_query_file);\n  if (prog_config_opts.exit_after_warmup) {\n    g_running = false;\n  }\n\n  // Main thread blocks for as long as the servers are running.\n  wait_for_server_threads();\n\n  // Clean shutdown.\n  int signum = g_saw_signal;\n  if (signum <= 0 || signum == SIGTERM) {\n    return 0;\n  } else {\n    return signum;\n  }\n}\n\nvoid log_startup_info() {\n#ifdef __linux__\n  VLOG(1) << \"sysconf(_SC_PAGE_SIZE): \" << sysconf(_SC_PAGE_SIZE);\n  VLOG(1) << \"/proc/buddyinfo: \" << shared::FileContentsEscaper{\"/proc/buddyinfo\"};\n  VLOG(1) << \"/proc/meminfo: \" << shared::FileContentsEscaper{\"/proc/meminfo\"};\n#endif\n}\n\nint main(int argc, char** argv) {\n  bool has_clust_topo = false;\n\n  CommandLineOptions prog_config_opts(argv[0], has_clust_topo);\n\n  try {\n    if (auto return_code =\n            prog_config_opts.parse_command_line(argc, argv, !has_clust_topo)) {\n      return *return_code;\n    }\n\n    if (!has_clust_topo) {\n      prog_config_opts.validate_base_path();\n      prog_config_opts.validate();\n      log_startup_info();\n      return (startHeavyDBServer(prog_config_opts));\n    }\n  } catch (std::runtime_error& e) {\n    std::cerr << \"Server Error: \" << e.what() << std::endl;\n    return 1;\n  } catch (boost::program_options::error& e) {\n    std::cerr << \"Usage Error: \" << e.what() << std::endl;\n    return 1;\n  }\n}\n"
        },
        {
          "name": "HeavyIQ",
          "type": "tree",
          "content": null
        },
        {
          "name": "ImportExport",
          "type": "tree",
          "content": null
        },
        {
          "name": "L0Mgr",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 11.1162109375,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2017 MapD Technologies, Inc.\n   Copyright 2017 Google LLC\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "LockMgr",
          "type": "tree",
          "content": null
        },
        {
          "name": "Logger",
          "type": "tree",
          "content": null
        },
        {
          "name": "MigrationMgr",
          "type": "tree",
          "content": null
        },
        {
          "name": "NvidiaComputeCapability",
          "type": "tree",
          "content": null
        },
        {
          "name": "OSDependent",
          "type": "tree",
          "content": null
        },
        {
          "name": "Parser",
          "type": "tree",
          "content": null
        },
        {
          "name": "QueryEngine",
          "type": "tree",
          "content": null
        },
        {
          "name": "QueryRunner",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 18.2255859375,
          "content": "HeavyDB (formerly OmniSciDB)\n==============================\n\nHeavyDB is an open source SQL-based, relational, columnar database engine that leverages the full performance and parallelism of modern hardware (both CPUs and GPUs) to enable querying of multi-billion row datasets in milliseconds, without the need for indexing, pre-aggregation, or downsampling.  HeavyDB can be run on hybrid CPU/GPU systems (Nvidia GPUs are currently supported), as well as on CPU-only systems featuring X86, Power, and ARM (experimental support) architectures. To achieve maximum performance, HeavyDB features multi-tiered caching of data between storage, CPU memory, and GPU memory, and an innovative Just-In-Time (JIT) query compilation framework.\n\nFor usage info, see the [product documentation](https://docs.heavy.ai/), and for more details about the system's internal architecture, check out the [developer documentation](https://heavyai.github.io/heavydb/). Further technical discussion can be found on the [HEAVY.AI Community Forum](https://community.heavy.ai).\n\nThe repository includes a number of third party packages provided under separate licenses. Details about these packages and their respective licenses is at [ThirdParty/licenses/index.md](ThirdParty/licenses/index.md).\n\n# Downloads and Installation Instructions\n\nHEAVY.AI provides pre-built binaries for Linux for stable releases of the project:\n\n| Distro | Package type | CPU/GPU | Repository | Docs |\n| --- | --- | --- | --- | --- |\n| CentOS | RPM | CPU | https://releases.heavy.ai/os/yum/stable/cpu |  https://docs.heavy.ai/installation-and-configuration/installation/installing-on-centos/centos-yum-gpu-ee |\n| CentOS | RPM | GPU | https://releases.heavy.ai/os/yum/stable/cuda | https://docs.heavy.ai/installation-and-configuration/installation/installing-on-centos/centos-yum-gpu-ee |\n| Ubuntu | DEB | CPU | https://releases.heavy.ai/os/apt/dists/stable/cpu | https://docs.heavy.ai/installation-and-configuration/installation/installing-on-ubuntu/centos-yum-gpu-ee |\n| Ubuntu | DEB | GPU | https://releases.heavy.ai/os/apt/dists/stable/cuda | https://docs.heavy.ai/installation-and-configuration/installation/installing-on-ubuntu/centos-yum-gpu-ee |\n| * | tarball | CPU | https://releases.heavy.ai/os/tar/heavyai-os-latest-Linux-x86_64-cpu.tar.gz |  |\n| * | tarball | GPU | https://releases.heavy.ai/os/tar/heavyai-os-latest-Linux-x86_64.tar.gz |  |\n***\n\n# Developing HeavyDB: Table of Contents\n\n- [Links](#links)\n- [License](#license)\n- [Contributing](#contributing)\n- [Building](#building)\n- [Testing](#testing)\n- [Using](#using)\n- [Code Style](#code-style)\n- [Dependencies](#dependencies)\n- [Roadmap](ROADMAP.md)\n\n# Links\n\n- [Developer Documentation](https://heavyai.github.io/heavydb/)\n- [Doxygen-generated Documentation](http://doxygen.mapd.com/)\n- [Product Documentation](https://docs.heavy.ai/)\n- [Release Notes](https://docs.heavy.ai/overview/release-notes)\n- [Community Forum](https://community.heavy.ai)\n- [HEAVY.AI Homepage](https://www.heavy.ai)\n- [HEAVY.AI Blog](https://www.heavy.ai/blog/)\n- [HEAVY.AI Downloads](https://www.heavy.ai/platform/downloads/)\n\n# License\n\nThis project is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0).\n\nThe repository includes a number of third party packages provided under separate licenses. Details about these packages and their respective licenses is at [ThirdParty/licenses/index.md](ThirdParty/licenses/index.md).\n\n# Contributing\n\nIn order to clarify the intellectual property license granted with Contributions from any person or entity, HEAVY.AI must have a Contributor License Agreement (\"CLA\") on file that has been signed by each Contributor, indicating agreement to the [Contributor License Agreement](CLA.txt). After making a pull request, a bot will notify you if a signed CLA is required and provide instructions for how to sign it. Please read the agreement carefully before signing and keep a copy for your records.\n\n# Building\n\nIf this is your first time building HeavyDB, install the dependencies mentioned in the [Dependencies](#dependencies) section below.\n\nHeavyDB uses CMake for its build system.\n\n    mkdir build\n    cd build\n    cmake -DCMAKE_BUILD_TYPE=debug ..\n    make -j 4\n\nThe following `cmake`/`ccmake` options can enable/disable different features:\n\n- `-DCMAKE_BUILD_TYPE=release` - Build type and compiler options to use.\n                                 Options are `Debug`, `Release`, `RelWithDebInfo`, `MinSizeRel`, and unset.\n- `-DENABLE_ASAN=off` - Enable address sanitizer. Default is `off`.\n- `-DENABLE_AWS_S3=on` - Enable AWS S3 support, if available. Default is `on`.\n- `-DENABLE_CUDA=off` - Disable CUDA. Default is `on`.\n- `-DENABLE_CUDA_KERNEL_DEBUG=off` - Enable debugging symbols for CUDA kernels. Will dramatically reduce kernel performance. Default is `off`.\n- `-DENABLE_DECODERS_BOUNDS_CHECKING=off` - Enable bounds checking for column decoding. Default is `off`.\n- `-DENABLE_FOLLY=on` - Use Folly. Default is `on`.\n- `-DENABLE_IWYU=off` - Enable include-what-you-use. Default is `off`.\n- `-DENABLE_JIT_DEBUG=off` - Enable debugging symbols for the JIT. Default is `off`.\n- `-DENABLE_ONLY_ONE_ARCH=off` - Compile GPU code only for the host machine's architecture, speeding up compilation. Default is `off`.\n- `-DENABLE_PROFILER=off` - Enable google perftools. Default is `off`.\n- `-DENABLE_STANDALONE_CALCITE=off` - Require standalone Calcite server. Default is `off`.\n- `-DENABLE_TESTS=on` - Build unit tests. Default is `on`.\n- `-DENABLE_TSAN=off` - Enable thread sanitizer. Default is `off`.\n- `-DENABLE_CODE_COVERAGE=off` - Enable code coverage symbols (clang only). Default is `off`.\n- `-DPREFER_STATIC_LIBS=off` - Static link dependencies, if available. Default is `off`. Only works on CentOS.\n\n# Testing\n\nHeavyDB uses [Google Test](https://github.com/google/googletest) as its main testing framework. Tests reside under the [Tests](Tests) directory.\n\nThe `sanity_tests` target runs the most common tests. If using Makefiles to build, the tests may be run using:\n\n    make sanity_tests\n\n## AddressSanitizer\n\n[AddressSanitizer](https://github.com/google/sanitizers/wiki/AddressSanitizer) can be activated by setting the `ENABLE_ASAN` CMake flag in a fresh build directory. At this time CUDA must also be disabled. In an empty build directory run CMake and compile:\n\n    mkdir build && cd build\n    cmake -DENABLE_ASAN=on -DENABLE_CUDA=off ..\n    make -j 4\n\nFinally run the tests:\n\n    export ASAN_OPTIONS=alloc_dealloc_mismatch=0:handle_segv=0\n    make sanity_tests\n\n## ThreadSanitizer\n\n[ThreadSanitizer](https://github.com/google/sanitizers/wiki/ThreadSanitizerCppManual) can be activated by setting the `ENABLE_TSAN` CMake flag in a fresh build directory. At this time CUDA must also be disabled. In an empty build directory run CMake and compile:\n\n    mkdir build && cd build\n    cmake -DENABLE_TSAN=on -DENABLE_CUDA=off ..\n    make -j 4\n\nWe use a TSAN suppressions file to ignore warnings in third party libraries. Source the suppressions file by adding it to your `TSAN_OPTIONS` env:\n\n    export TSAN_OPTIONS=\"suppressions=/path/to/heavydb/config/tsan.suppressions\"\n\nFinally run the tests:\n\n    make sanity_tests\n\n# Generating Packages\n\nHeavyDB uses [CPack](https://cmake.org/cmake/help/latest/manual/cpack.1.html) to generate packages for distribution. Packages generated on CentOS with static linking enabled can be used on most other recent Linux distributions.\n\nTo generate packages on CentOS (assuming starting from top level of the heavydb repository):\n\n    mkdir build-package && cd build-package\n    cmake -DPREFER_STATIC_LIBS=on -DCMAKE_BUILD_TYPE=release ..\n    make -j 4\n    cpack -G TGZ\n\nThe first command creates a fresh build directory, to ensure there is nothing left over from a previous build.\n\nThe second command configures the build to prefer linking to the dependencies' static libraries instead of the (default) shared libraries, and to build using CMake's `release` configuration (enables compiler optimizations). Linking to the static versions of the libraries libraries reduces the number of dependencies that must be installed on target systems.\n\nThe last command generates a `.tar.gz` package. The `TGZ` can be replaced with, for example, `RPM` or `DEB` to generate a `.rpm` or `.deb`, respectively.\n\n# Using\n\nThe [`startheavy`](startheavy) wrapper script may be used to start HeavyDB in a testing environment. This script performs the following tasks:\n\n- initializes the `data` storage directory via `initdb`, if required\n- starts the main HeavyDB server, `heavydb`\n- offers to download and import a sample dataset, using the `insert_sample_data` script\n\nAssuming you are in the `build` directory, and it is a subdirectory of the `heavydb` repository, `startheavy` may be run by:\n\n    ../startheavy\n\n## Starting Manually\n\nIt is assumed that the following commands are run from inside the `build` directory.\n\nInitialize the `data` storage directory. This command only needs to be run once.\n\n    mkdir data && ./bin/initdb data\n\nStart the HeavyDB server:\n\n    ./bin/heavydb\n\nIf desired, insert a sample dataset by running the `insert_sample_data` script in a new terminal:\n\n    ../insert_sample_data\n\nYou can now start using the database. The `heavysql` utility may be used to interact with the database from the command line:\n\n    ./bin/heavysql -p HyperInteractive\n\nwhere `HyperInteractive` is the default password. The default user `admin` is assumed if not provided.\n\n# Code Style\n\nContributed code should compile without generating warnings by recent compilers on most Linux distributions. Changes to the code should follow the [C++ Core Guidelines](https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines).\n\n## clang-format\n\nA [`.clang-format`](https://clang.llvm.org/docs/ClangFormat.html) style configuration, based on the Chromium style guide, is provided at the top level of the repository. Please format your code using a recent version (8.0+ preferred) of ClangFormat before submitting.\n\nTo use:\n\n    clang-format -i File.cpp\n\n## clang-tidy\n\nA [`.clang-tidy`](https://clang.llvm.org/extra/clang-tidy/) configuration is provided at the top level of the repository. Please lint your code using a recent version (6.0+ preferred) of clang-tidy before submitting.\n\n`clang-tidy` requires all generated files to exist before running. The easiest way to accomplish this is to simply run a full build before running `clang-tidy`. A build target which runs `clang-tidy` is provided. To use:\n\n    make run-clang-tidy\n\nNote: `clang-tidy` may make invalid or overly verbose changes to the source code. It is recommended to first commit your changes, then run `clang-tidy` and review its recommended changes before amending them to your commit.\n\nNote: the `clang-tidy` target uses the `run-clang-tidy.py` script provided with LLVM, which may depend on `PyYAML`. The target also depends on `jq`, which is used to filter portions of the `compile_commands.json` file.\n\n# Dependencies\n\nHeavyDB has the following dependencies:\n\n| Package | Min Version | Required |\n| ------- | ----------- | -------- |\n| [CMake](https://cmake.org/) | 3.16 | yes |\n| [LLVM](http://llvm.org/) | 9.0 | yes |\n| [GCC](http://gcc.gnu.org/) | 8.4.0 | no, if building with clang |\n| [Go](https://golang.org/) | 1.12 | yes |\n| [Boost](http://www.boost.org/) | 1.72.0 | yes |\n| [OpenJDK](http://openjdk.java.net/) | 1.7 | yes |\n| [CUDA](http://nvidia.com/cuda) | 11.0 | yes, if compiling with GPU support |\n| [gperftools](https://github.com/gperftools/gperftools) | | yes |\n| [gdal](http://gdal.org/) | 2.4.2 | yes |\n| [Arrow](https://arrow.apache.org/) | 3.0.0 | yes |\n\n## CentOS 7\n\nHeavyDB requires a number of dependencies which are not provided in the common CentOS/RHEL package repositories. A prebuilt package containing all these dependencies is provided for CentOS 7 (x86_64).\n\nUse the [scripts/mapd-deps-prebuilt.sh](scripts/mapd-deps-prebuilt.sh) build script to install prebuilt dependencies.\n\nThese dependencies will be installed to a directory under `/usr/local/mapd-deps`. The `mapd-deps-prebuilt.sh` script also installs [Environment Modules](http://modules.sf.net) in order to simplify managing the required environment variables. Log out and log back in after running the `mapd-deps-prebuilt.sh` script in order to active Environment Modules command, `module`.\n\nThe `mapd-deps` environment module is disabled by default. To activate for your current session, run:\n\n    module load mapd-deps\n\nTo disable the `mapd-deps` module:\n\n    module unload mapd-deps\n\nWARNING: The `mapd-deps` package contains newer versions of packages such as GCC and ncurses which might not be compatible with the rest of your environment. Make sure to disable the `mapd-deps` module before compiling other packages.\n\nInstructions for installing CUDA are below.\n\n### CUDA\n\nIt is preferred, but not necessary, to install CUDA and the NVIDIA drivers using the .rpm using the [instructions provided by NVIDIA](https://developer.nvidia.com/cuda-downloads). The `rpm (network)` method (preferred) will ensure you always have the latest stable drivers, while the `rpm (local)` method allows you to install does not require Internet access.\n\nThe .rpm method requires DKMS to be installed, which is available from the [Extra Packages for Enterprise Linux](https://fedoraproject.org/wiki/EPEL) repository:\n\n    sudo yum install epel-release\n\nBe sure to reboot after installing in order to activate the NVIDIA drivers.\n\n### Environment Variables\n\nThe `mapd-deps-prebuilt.sh` script includes two files with the appropriate environment variables: `mapd-deps-<date>.sh` (for sourcing from your shell config) and `mapd-deps-<date>.modulefile` (for use with [Environment Modules](http://modules.sf.net), yum package `environment-modules`). These files are placed in mapd-deps install directory, usually `/usr/local/mapd-deps/<date>`. Either of these may be used to configure your environment: the `.sh` may be sourced in your shell config; the `.modulefile` needs to be moved to the modulespath.\n\n### Building Dependencies\n\nThe [scripts/mapd-deps-centos.sh](scripts/mapd-deps-centos.sh) script is used to build the dependencies. Modify this script and run if you would like to change dependency versions or to build on alternative CPU architectures.\n\n    cd scripts\n    module unload mapd-deps\n    ./mapd-deps-centos.sh --compress\n\n## macOS\n\n[scripts/mapd-deps-osx.sh](scripts/mapd-deps-osx.sh) is provided that will automatically install and/or update [Homebrew](http://brew.sh/) and use that to install all dependencies. Please make sure macOS is completely up to date and Xcode is installed before running. Xcode can be installed from the App Store.\n\n### CUDA\n\n`mapd-deps-osx.sh` will automatically install CUDA via Homebrew and add the correct environment variables to `~/.bash_profile`.\n\n### Java\n\n`mapd-deps-osx.sh` will automatically install Java and Maven via Homebrew and add the correct environment variables to `~/.bash_profile`.\n\n## Ubuntu\n\nMost build dependencies required by HeavyDB are available via APT. Certain dependencies such as Thrift, Blosc, and Folly must be built as they either do not exist in the default repositories or have outdated versions. A prebuilt package containing all these dependencies is provided for Ubuntu 18.04 (x86_64). The dependencies will be installed to `/usr/local/mapd-deps/` by default; see the Environment Variables section below for how to add these dependencies to your environment.\n\n### Ubuntu 16.04\n\nHeavyDB requires a newer version of Boost than the version which is provided by Ubuntu 16.04. The [scripts/mapd-deps-ubuntu1604.sh](scripts/mapd-deps-ubuntu1604.sh) build script will compile and install a newer version of Boost into the `/usr/local/mapd-deps/` directory.\n\n### Ubuntu 18.04\n\nUse the [scripts/mapd-deps-prebuilt.sh](scripts/mapd-deps-prebuilt.sh) build script to install prebuilt dependencies.\n\nThese dependencies will be installed to a directory under `/usr/local/mapd-deps`. The `mapd-deps-prebuilt.sh` script above will generate a script named `mapd-deps.sh` containing the environment variables which need to be set. Simply source this file in your current session (or symlink it to `/etc/profile.d/mapd-deps.sh`) in order to activate it:\n\n    source /usr/local/mapd-deps/mapd-deps.sh\n\n### Environment Variables\n\nThe CUDA and mapd-deps `lib` directories need to be added to `LD_LIBRARY_PATH`; the CUDA and mapd-deps `bin` directories need to be added to `PATH`. The `mapd-deps-ubuntu.sh` and `mapd-deps-prebuilt.sh` scripts will generate a script named `mapd-deps.sh` containing the environment variables which need to be set. Simply source this file in your current session (or symlink it to `/etc/profile.d/mapd-deps.sh`) in order to activate it:\n\n    source /usr/local/mapd-deps/mapd-deps.sh\n\n### CUDA\n\nRecent versions of Ubuntu provide the NVIDIA CUDA Toolkit and drivers in the standard repositories. To install:\n\n    sudo apt install -y \\\n        nvidia-cuda-toolkit\n\nBe sure to reboot after installing in order to activate the NVIDIA drivers.\n\n### Building Dependencies\n\nThe [scripts/mapd-deps-ubuntu.sh](scripts/mapd-deps-ubuntu.sh) and [scripts/mapd-deps-ubuntu1604.sh](scripts/mapd-deps-ubuntu1604.sh) scripts are used to build the dependencies for Ubuntu 18.04 and 16.04, respectively. The scripts will install all required dependencies (except CUDA) and build the dependencies which require it. Modify this script and run if you would like to change dependency versions or to build on alternative CPU architectures.\n\n    cd scripts\n    ./mapd-deps-ubuntu.sh --compress\n\n## Arch\n\n[scripts/mapd-deps-arch.sh](scripts/mapd-deps-arch.sh) is provided that will use [yay](https://aur.archlinux.org/packages/yay/) to install packages from the [Arch User Repository](https://wiki.archlinux.org/index.php/Arch_User_Repository) and custom PKGBUILD scripts for a few packages listed below. If you don't have `yay` yet, install it first: https://github.com/Jguer/yay#installation\n\n### Package Version Requirements:\n\n### CUDA\n\nCUDA and the NVIDIA drivers may be installed using the following.\n\n    yay -S \\\n        linux-headers \\\n        cuda \\\n        nvidia\n\nBe sure to reboot after installing in order to activate the NVIDIA drivers.\n\n### Environment Variables\n\nThe `cuda` package should set up the environment variables required to use CUDA. If you receive errors saying `nvcc` is not found, then CUDA `bin` directories need to be added to `PATH`: the easiest way to do so is by creating a new file named `/etc/profile.d/mapd-deps.sh` containing the following:\n\n    PATH=/opt/cuda/bin:$PATH\n    export PATH\n"
        },
        {
          "name": "ROADMAP.md",
          "type": "blob",
          "size": 3.5732421875,
          "content": "OmniSciDB is being intensively developed and is evolving quickly. In order to help the MapD developer community understand our near term priorities, this document shares work targeted for the next 3-6 months. Many of these items are the result of community requests.\n\nWe welcome and encourage developer contributions to the project. Please see the [contribution guidelines](https://github.com/mapd/mapd-core#contributing) and [GitHub issues](https://github.com/mapd/mapd-core/issues).\n\n## Database\n\n#### Completed\n- Update via subquery (Completed 5.0)\n- Binary/dump restore of tables (Completed 5.0)\n- In-memory temporary tables (Completed 5.1)\n- Support for SQL VALUES syntax, allowing literals to be used inline in SQL queries (Completed 5.1.2)\n- More performant multi-fragment joins (Completed 5.2)\n- `ALTER TABLE DROP COLUMN` (Completed 5.2)\n- `SQL SHOW` Commands (Completed 5.2)\n- Import of compressed parquet files (Completed 5.2)\n- Initial support for UNION ALL (Completed 5.3)\n- Initial support of multiple executors for improved concurrency (Completed 5.3+)\n- Query hint framework, initially allowing `/*+ cpu_mode */ hint (Completed 5.3.1)\n- Support for implicit casting for `INSERT AS SELECT` queries to match existing table types when possible\n- Concurrent `UPDATE` and `SELECT` on the same table (Completed 5.5)\n- Allow none-encoded inputs into user defined functions (Completed 5.5)\n- Initial foreign Server/Table support for CSV and Parquet (Completed 5.4-5.5) \n- Query interrupt improvements (Completed 5.5 and ongoing)\n\n\n#### Upcoming\n- `APPROX_MEDIAN` and `APPROX_PERCENTILE` operators\n- Additional string function support\n- Queryable system metadata tables\n- Accelerated range joins\n- Query interrupt improvements\n- Query/subquery result set recycling for greater performance\n\n\n## Geospatial/GIS\n\n#### Completed\n- OGC full \"simple features\" constructive geospatial operators (`ST_Buffer`, `ST_Intersects`, `ST_Union`, etc) (Completed 5.2+)\n- Null support for geo types (5.2)\n- Export of conventional and line-oriented geoJSON in geoSQL (5.4)\n- Geopandas dataframe generation through Ibis (https://github.com/ibis-project/ibis)\n- Support for well-known binary (WKB) in columnar loading for increased import performance (5.5)\n- Support for CSV file reprojection on import (5.5)\n- ST_Buffer support for automatic planar projections in meters (5.5)\n- `ST_Centroid` operator (5.5)\n\n#### Upcoming\n- Additional OGC multipart geospatial types: Multi(Point|Line)\n- Additional geometric constructors (`ST_Line`, `ST_Polygon`, etc.)\n- Accelerated geospatial joins (with dynamic spatial hashing)\n- OGC Geopackage import and export\n- GPU-accelerated transforms between supported coordinate systems\n\n## Data Science/[GPU Data Frame (GDF)](http://gpuopenanalytics.com/#/)/[Apache Arrow](https://arrow.apache.org/)\n\n#### Completed\n- [PyMapD DB-API Python client](https://github.com/mapd/pymapd)\n- [Ibis backend for MapD](https://github.com/ibis-project/ibis)\n- Support for Arrow result sets over the wire (in addition to existing in-situ Arrow egress) (Completed 5.5)\n- Basic user-defined row and table functions (Completed 5.0, with ongoing improvements)\n- User-defined table function (Completed UDTF) improvements (multiple column/query inputs, composability, lazy linking, function redefinition) (Completed 5.4-5.5) \n- `CREATE DATAFRAME` temporary table creation from csv via Arrow (Completed 5.4 and ongoing)\n\n#### Upcoming\n- Further increase efficiency of Arrow serialization\n- Additional UDF/UDTF improvements (dictionary-encoded text column support, variadic types, performance on large inputs, semantics)\n- Experimental ML operators built on UDTFs\n"
        },
        {
          "name": "RuntimeLibManager",
          "type": "tree",
          "content": null
        },
        {
          "name": "SQLFrontend",
          "type": "tree",
          "content": null
        },
        {
          "name": "SampleCode",
          "type": "tree",
          "content": null
        },
        {
          "name": "SampleData",
          "type": "tree",
          "content": null
        },
        {
          "name": "Shared",
          "type": "tree",
          "content": null
        },
        {
          "name": "SqliteConnector",
          "type": "tree",
          "content": null
        },
        {
          "name": "StringDictionary",
          "type": "tree",
          "content": null
        },
        {
          "name": "StringOps",
          "type": "tree",
          "content": null
        },
        {
          "name": "TableArchiver",
          "type": "tree",
          "content": null
        },
        {
          "name": "Tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "ThirdParty",
          "type": "tree",
          "content": null
        },
        {
          "name": "ThriftHandler",
          "type": "tree",
          "content": null
        },
        {
          "name": "UdfCompiler",
          "type": "tree",
          "content": null
        },
        {
          "name": "Utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "common.thrift",
          "type": "blob",
          "size": 0.787109375,
          "content": "namespace java ai.heavy.thrift.server\nnamespace py heavydb.common\n\nenum TDeviceType {\n  CPU,\n  GPU\n}\n\nenum TDatumType {\n  SMALLINT,\n  INT,\n  BIGINT,\n  FLOAT,\n  DECIMAL,\n  DOUBLE,\n  STR,\n  TIME,\n  TIMESTAMP,\n  DATE,\n  BOOL,\n  INTERVAL_DAY_TIME,\n  INTERVAL_YEAR_MONTH,\n  POINT,\n  LINESTRING,\n  POLYGON,\n  MULTIPOLYGON,\n  TINYINT,\n  GEOMETRY,\n  GEOGRAPHY,\n  MULTILINESTRING,\n  MULTIPOINT\n}\n\nenum TEncodingType {\n  NONE,\n  FIXED,\n  RL,\n  DIFF,\n  DICT,\n  SPARSE,\n  GEOINT,\n  DATE_IN_DAYS,\n  ARRAY,\n  ARRAY_DICT\n}\n\nstruct TStringDictKey {\n  1: i32 db_id;\n  2: i32 dict_id;\n}\n\nstruct TTypeInfo {\n  1: TDatumType type;\n  4: TEncodingType encoding;\n  2: bool nullable;\n  3: bool is_array;\n  5: i32 precision;\n  6: i32 scale;\n  7: i32 comp_param;\n  8: optional i32 size=-1;\n  9: optional TStringDictKey dict_key;\n}\n\n"
        },
        {
          "name": "completion_hints.thrift",
          "type": "blob",
          "size": 0.2958984375,
          "content": "namespace java ai.heavy.thrift.calciteserver\nnamespace py heavydb.completion_hints\n\nenum TCompletionHintType {\n  COLUMN,\n  TABLE,\n  VIEW,\n  SCHEMA,\n  CATALOG,\n  REPOSITORY,\n  FUNCTION,\n  KEYWORD\n}\n\nstruct TCompletionHint {\n  1: TCompletionHintType type;\n  2: list<string> hints;\n  3: string replaced;\n}\n"
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "heavy.thrift",
          "type": "blob",
          "size": 24.5849609375,
          "content": "namespace java ai.heavy.thrift.server\nnamespace py heavydb.thrift\n\ninclude \"common.thrift\"\ninclude \"completion_hints.thrift\"\ninclude \"QueryEngine/serialized_result_set.thrift\"\ninclude \"QueryEngine/extension_functions.thrift\"\n\nenum TExecuteMode {\n  GPU = 1,\n  CPU\n}\n\nenum TSourceType {\n  DELIMITED_FILE,\n  GEO_FILE,\n  PARQUET_FILE,\n  RASTER_FILE,\n  ODBC\n}\n\nenum TPartitionDetail {\n  DEFAULT,\n  REPLICATED,\n  SHARDED,\n  OTHER\n}\n\nenum TGeoFileLayerContents {\n  EMPTY,\n  GEO,\n  NON_GEO,\n  UNSUPPORTED_GEO\n}\n\nenum TImportHeaderRow {\n  AUTODETECT,\n  NO_HEADER,\n  HAS_HEADER\n}\n\nenum TRole {\n  SERVER, // A single node instance\n  AGGREGATOR,\n  LEAF,\n  STRING_DICTIONARY\n}\n\nenum TTableType {\n  DEFAULT,\n  TEMPORARY,\n  FOREIGN,\n  VIEW\n}\n\nenum TTableRefreshUpdateType {\n  ALL,\n  APPEND\n}\n\nenum TTableRefreshTimingType {\n  MANUAL,\n  SCHEDULED\n}\n\nenum TTableRefreshIntervalType {\n  NONE,\n  HOUR,\n  DAY\n}\n\n/* union */ struct TDatumVal {\n  1: i64 int_val;\n  2: double real_val;\n  3: string str_val;\n  4: list<TDatum> arr_val;\n}\n\nstruct TDatum {\n  1: TDatumVal val;\n  2: bool is_null;\n}\n\nstruct TStringValue {\n  1: string str_val;\n  2: bool is_null;\n}\n\nstruct TColumnType {\n  1: string col_name;\n  2: common.TTypeInfo col_type;\n  3: bool is_reserved_keyword;\n  4: string src_name;\n  5: bool is_system;\n  6: bool is_physical;\n  7: i64 col_id;\n  8: optional string default_value;\n}\n\nstruct TRow {\n  1: list<TDatum> cols;\n}\n\n/* union */ struct TColumnData {\n  1: list<i64> int_col;\n  2: list<double> real_col;\n  3: list<string> str_col;\n  4: list<TColumn> arr_col;\n}\n\nstruct TColumn {\n  1: TColumnData data;\n  2: list<bool> nulls;\n}\n\nstruct TStringRow {\n  1: list<TStringValue> cols;\n}\n\ntypedef list<TColumnType> TRowDescriptor\ntypedef string TSessionId\ntypedef string TKrb5Token\ntypedef i64 TQueryId\ntypedef i64 TSubqueryId\n\nstruct TKrb5Session {\n  1: TSessionId sessionId;\n  2: TKrb5Token krbToken;\n}\n\nenum TMergeType {\n  UNION,\n  REDUCE\n}\n\nenum TRasterPointType {\n  NONE,\n  AUTO,\n  SMALLINT,\n  INT,\n  FLOAT,\n  DOUBLE,\n  POINT\n}\n\nenum TRasterPointTransform {\n  NONE,\n  AUTO,\n  FILE,\n  WORLD\n}\n\nstruct TStepResult {\n  1: serialized_result_set.TSerializedRows serialized_rows;\n  2: bool execution_finished;\n  3: TMergeType merge_type;\n  4: bool sharded;\n  5: TRowDescriptor row_desc;\n  6: i32 node_id;\n}\n\nstruct TRowSet {\n  1: TRowDescriptor row_desc;\n  2: list<TRow> rows;\n  3: list<TColumn> columns;\n  4: bool is_columnar;\n}\n\nenum TQueryType {\n  UNKNOWN,\n  READ,\n  WRITE,\n  SCHEMA_READ,\n  SCHEMA_WRITE\n}\n\nenum TArrowTransport {\n  SHARED_MEMORY,\n  WIRE\n}\n\nstruct TQueryResult {\n  1: TRowSet row_set;\n  2: i64 execution_time_ms;\n  3: i64 total_time_ms;\n  4: string nonce;\n  5: string debug;\n  6: bool success=true;\n  7: TQueryType query_type=TQueryType.UNKNOWN;\n}\n\nstruct TDataFrame {\n  1: binary sm_handle;\n  2: i64 sm_size;\n  3: binary df_handle;\n  4: i64 df_size;\n  5: i64 execution_time_ms;\n  6: i64 arrow_conversion_time_ms;\n  7: binary df_buffer;\n}\n\nstruct TDBInfo {\n  1: string db_name;\n  2: string db_owner;\n}\n\nexception TDBException {\n  1: string error_msg;\n}\n\nstruct TCopyParams {\n  1: string delimiter;\n  2: string null_str;\n  3: TImportHeaderRow has_header=TImportHeaderRow.AUTODETECT;\n  4: bool quoted;\n  5: string quote;\n  6: string escape;\n  7: string line_delim;\n  8: string array_delim;\n  9: string array_begin;\n  10: string array_end;\n  11: i32 threads;\n  12: TSourceType source_type=TSourceType.DELIMITED_FILE;\n  13: string s3_access_key;\n  14: string s3_secret_key;\n  15: string s3_region;\n  16: common.TEncodingType geo_coords_encoding=TEncodingType.GEOINT;\n  17: i32 geo_coords_comp_param=32;\n  18: common.TDatumType geo_coords_type=TDatumType.GEOMETRY;\n  19: i32 geo_coords_srid=4326;\n  20: bool sanitize_column_names=true;\n  21: string geo_layer_name;\n  22: string s3_endpoint;\n  23: bool geo_assign_render_groups=false;\n  24: bool geo_explode_collections=false;\n  25: i32 source_srid=0;\n  26: string s3_session_token;\n  27: TRasterPointType raster_point_type=TRasterPointType.AUTO;\n  28: string raster_import_bands;\n  29: i32 raster_scanlines_per_thread;\n  30: TRasterPointTransform raster_point_transform=TRasterPointTransform.AUTO;\n  31: bool raster_point_compute_angle=false;\n  32: string raster_import_dimensions;\n  33: string odbc_dsn;\n  34: string odbc_connection_string;\n  35: string odbc_sql_select;\n  36: string odbc_sql_order_by;\n  37: string odbc_username;\n  38: string odbc_password;\n  39: string odbc_credential_string;\n  40: string add_metadata_columns;\n  41: bool trim_spaces=true;\n  42: bool geo_validate_geometry=false;\n  43: bool raster_drop_if_all_null=false;\n}\n\nstruct TCreateParams {\n  1: bool is_replicated;\n}\n\nstruct TDetectResult {\n  1: TRowSet row_set;\n  2: TCopyParams copy_params;\n}\n\nstruct TImportStatus {\n  1: i64 elapsed;\n  2: i64 rows_completed;\n  3: i64 rows_estimated;\n  4: i64 rows_rejected;\n}\n\nstruct TFrontendView {\n  1: string view_name;\n  2: string view_state;\n  3: string image_hash;\n  4: string update_time;\n  5: string view_metadata;\n}\n\nstruct TServerStatus {\n  1: bool read_only;\n  2: string version;\n  3: bool rendering_enabled;\n  4: i64 start_time;\n  5: string edition;\n  6: string host_name;\n  7: bool poly_rendering_enabled;\n  8: TRole role;\n  9: string renderer_status_json;\n  10: string host_id;\n}\n\nstruct TPixel {\n  1: i64 x;\n  2: i64 y;\n}\n\nstruct TPixelTableRowResult {\n  1: TPixel pixel;\n  2: string vega_table_name;\n  3: list<i64> table_id;\n  4: list<i64> row_id;\n  5: TRowSet row_set;\n  6: string nonce;\n}\n\nstruct TRenderResult {\n  1: binary image;\n  2: string nonce;\n  3: i64 execution_time_ms;\n  4: i64 render_time_ms;\n  5: i64 total_time_ms;\n  6: string vega_metadata;\n}\n\nstruct TGpuSpecification {\n  1: i32 num_sm;\n  2: i64 clock_frequency_kHz;\n  3: i64 memory;\n  4: i16 compute_capability_major;\n  5: i16 compute_capability_minor;\n}\n\nstruct THardwareInfo {\n  1: i16 num_gpu_hw;\n  2: i16 num_cpu_hw;\n  3: i16 num_gpu_allocated;\n  4: i16 start_gpu;\n  5: string host_name;\n  6: list<TGpuSpecification> gpu_info;\n}\n\nstruct TClusterHardwareInfo {\n  1: list<THardwareInfo> hardware_info;\n}\n\nstruct TMemoryData {\n  1: i64 slab;\n  2: i32 start_page;\n  3: i64 num_pages;\n  4: i32 touch;\n  5: list<i64> chunk_key;\n  6: i32 buffer_epoch;\n  7: bool is_free;\n}\n\nstruct TNodeMemoryInfo {\n  1: string host_name;\n  2: i64 page_size;\n  3: i64 max_num_pages;\n  4: i64 num_pages_allocated;\n  5: bool is_allocation_capped;\n  6: list<TMemoryData> node_memory_data;\n}\n\nstruct TTableMeta {\n  1: string table_name;\n  2: i64 num_cols;\n  4: bool is_view;\n  5: bool is_replicated;\n  6: i64 shard_count;\n  7: i64 max_rows;\n  8: i64 table_id;\n  9: i64 max_table_id;\n  10: list<common.TTypeInfo> col_types;\n  11: list<string> col_names;\n}\n\nstruct TTableRefreshInfo {\n  1: TTableRefreshUpdateType update_type;\n  2: TTableRefreshTimingType timing_type;\n  3: string start_date_time;\n  4: TTableRefreshIntervalType interval_type;\n  5: i64 interval_count;\n  6: string last_refresh_time;\n  7: string next_refresh_time;\n}\n\nstruct TTableDetails {\n  1: TRowDescriptor row_desc;\n  2: i64 fragment_size;\n  3: i64 page_size;\n  4: i64 max_rows;\n  5: string view_sql;\n  6: i64 shard_count;\n  7: string key_metainfo;\n  8: bool is_temporary;\n  9: TPartitionDetail partition_detail;\n  10: TTableType table_type;\n  11: TTableRefreshInfo refresh_info;\n  12: string sharded_column_name;\n}\n\nenum TExpressionRangeType {\n  INVALID,\n  INTEGER,\n  FLOAT,\n  DOUBLE\n}\n\nstruct TColumnRange {\n  1: TExpressionRangeType type;\n  2: i32 col_id;\n  3: i32 table_id;\n  4: bool has_nulls;\n  5: i64 int_min;\n  6: i64 int_max;\n  7: i64 bucket;\n  8: double fp_min;\n  9: double fp_max;\n  10: i32 db_id;\n}\n\nstruct TDictionaryGeneration {\n  1: i32 dict_id;\n  2: i64 entry_count;\n  3: i32 db_id;\n}\n\nstruct TTableGeneration {\n  1: i32 table_id;\n  2: i64 tuple_count;\n  3: i64 start_rowid;\n  4: i32 db_id;\n}\n\nstruct TTableCacheStatus {\n  1: i32 table_id;\n  2: i32 db_id;\n  3: bool is_cached_on_disk;\n}\n\nstruct TPendingQuery {\n  1: TQueryId id;\n  2: list<TColumnRange> column_ranges;\n  3: list<TDictionaryGeneration> dictionary_generations;\n  4: list<TTableGeneration> table_generations;\n  5: TSessionId parent_session_id;\n  6: list<TTableCacheStatus> table_cache_status;\n}\n\nstruct TVarLen {\n  1: binary payload;\n  2: bool is_null;\n}\n\nunion TDataBlockPtr {\n  1: binary fixed_len_data;\n  2: list<TVarLen> var_len_data;\n}\n\nstruct TInsertData {\n  1: i32 db_id;\n  2: i32 table_id;\n  3: list<i32> column_ids;\n  4: list<TDataBlockPtr> data;\n  5: i64 num_rows;\n  6: list<bool> is_default;\n}\n\nunion TChunkData {\n  1: binary data_buffer;\n  2: binary index_buffer;\n}\n\nstruct TInsertChunks {\n  1: i32 db_id;\n  2: i32 table_id;\n  3: list<TChunkData> data;\n  4: list<i64> valid_indices;\n  5: i64 num_rows;\n}\n\nstruct TPendingRenderQuery {\n  1: TQueryId id;\n}\n\nstruct TRenderParseResult {\n  1: TMergeType merge_type;\n  2: i32 node_id;\n  3: i64 execution_time_ms;\n  4: i64 render_time_ms;\n  5: i64 total_time_ms;\n}\n\nstruct TRawRenderPassDataResult {\n  1: i32 num_pixel_channels;\n  2: i32 num_pixel_samples;\n  3: binary pixels;\n  4: binary row_ids_A;\n  5: binary row_ids_B;\n  6: binary table_ids;\n  7: binary accum_data;\n  8: i32 accum_depth;\n}\n\ntypedef map<i32, TRawRenderPassDataResult> TRenderPassMap\n\nstruct TRawPixelData {\n  1: i32 width;\n  2: i32 height;\n  3: TRenderPassMap render_pass_map;\n}\n\nstruct TRenderDatum {\n  1: common.TDatumType type;\n  2: i32 cnt;\n  3: binary value;\n}\n\ntypedef map<string, map<string, map<string, map<string, list<TRenderDatum>>>>> TRenderAggDataMap\n\nstruct TRenderStepResult {\n  1: TRenderAggDataMap merge_data;\n  2: TRawPixelData raw_pixel_data;\n  3: i64 execution_time_ms;\n  4: i64 render_time_ms;\n  5: i64 total_time_ms;\n}\nstruct TDatabasePermissions {\n  1: bool create_;\n  2: bool delete_;\n  3: bool view_sql_editor_;\n  4: bool access_;\n}\n\nstruct TTablePermissions {\n  1: bool create_;\n  2: bool drop_;\n  3: bool select_;\n  4: bool insert_;\n  5: bool update_;\n  6: bool delete_;\n  7: bool truncate_;\n  8: bool alter_;\n}\n\nstruct TDashboardPermissions {\n  1: bool create_;\n  2: bool delete_;\n  3: bool view_;\n  4: bool edit_;\n}\n\nstruct TViewPermissions {\n  1: bool create_;\n  2: bool drop_;\n  3: bool select_;\n  4: bool insert_;\n  5: bool update_;\n  6: bool delete_;\n}\n\nstruct TServerPermissions {\n  1: bool create_;\n  2: bool drop_;\n  3: bool alter_;\n  4: bool usage_;\n}\n\nunion TDBObjectPermissions {\n  1: TDatabasePermissions database_permissions_;\n  2: TTablePermissions table_permissions_;\n  3: TDashboardPermissions dashboard_permissions_;\n  4: TViewPermissions view_permissions_;\n  5: TServerPermissions server_permissions_;\n}\n\nenum TDBObjectType {\n  AbstractDBObjectType = 0,\n  DatabaseDBObjectType,\n  TableDBObjectType,\n  DashboardDBObjectType,\n  ViewDBObjectType,\n  ServerDBObjectType\n}\n\nstruct TDBObject {\n  1: string objectName;\n  2: TDBObjectType objectType;\n  3: list<bool> privs;\n  4: string grantee;\n  5: TDBObjectType privilegeObjectType;\n  6: i32 objectId;\n}\n\nstruct TDashboardGrantees {\n  1: string name;\n  2: bool is_user;\n  3: TDashboardPermissions permissions;\n}\n\nstruct TDashboard {\n  1: string dashboard_name;\n  2: string dashboard_state;\n  3: string image_hash;\n  4: string update_time;\n  5: string dashboard_metadata;\n  6: i32 dashboard_id;\n  7: string dashboard_owner;\n  8: bool is_dash_shared;\n  9: TDashboardPermissions dashboard_permissions;\n}\n\nstruct TLicenseInfo {\n  1: list<string> claims;\n}\n\nstruct TSessionInfo {\n  1: string user;\n  2: string database;\n  3: i64 start_time;\n  4: bool is_super;\n}\n\nstruct TGeoFileLayerInfo {\n  1: string name;\n  2: TGeoFileLayerContents contents;\n}\n\nstruct TTableEpochInfo {\n  1: i32 table_id;\n  2: i32 table_epoch;\n  3: i32 leaf_index;\n}\n\nenum TDataSourceType {\n  TABLE\n}\n\nstruct TCustomExpression {\n  1: i32 id;\n  2: string name;\n  4: string expression_json;\n  5: TDataSourceType data_source_type;\n  6: i32 data_source_id;\n  7: bool is_deleted;\n  8: string data_source_name;\n}\n\nstruct TQueryInfo {\n  1: string query_session_id;\n  2: string query_public_session_id;\n  3: string current_status;\n  4: i32 executor_id;\n  5: string submitted;\n  6: string query_str;\n  7: string login_name;\n  8: string client_address;\n  9: string db_name;\n  10: string exec_device_type;\n}\n\nstruct TLeafInfo {\n  1: i32 leaf_id;\n  2: i32 num_leaves;\n}\n\nservice Heavy {\n  # connection, admin\n  TSessionId connect(1: string user, 2: string passwd, 3: string dbname) throws (1: TDBException e)\n  TKrb5Session krb5_connect(1: string inputToken, 2: string dbname) throws (1: TDBException e)\n  void disconnect(1: TSessionId session) throws (1: TDBException e)\n  void switch_database(1: TSessionId session, 2: string dbname) throws(1: TDBException e)\n  TSessionId clone_session(1: TSessionId session) throws(1: TDBException e)\n  TServerStatus get_server_status(1: TSessionId session) throws (1: TDBException e)\n  list<TServerStatus> get_status(1: TSessionId session) throws (1: TDBException e)\n  TClusterHardwareInfo get_hardware_info(1: TSessionId session) throws (1: TDBException e)\n  list<string> get_tables(1: TSessionId session) throws (1: TDBException e)\n  list<string> get_tables_for_database(1: TSessionId session, 2: string database_name) throws (1: TDBException e)\n  list<string> get_physical_tables(1: TSessionId session) throws (1: TDBException e)\n  list<string> get_views(1: TSessionId session) throws (1: TDBException e)\n  list<TTableMeta> get_tables_meta(1: TSessionId session) throws (1: TDBException e)\n  TTableDetails get_table_details(1: TSessionId session, 2: string table_name) throws (1: TDBException e)\n  TTableDetails get_table_details_for_database(1: TSessionId session, 2: string table_name, 3: string database_name) throws (1: TDBException e)\n  TTableDetails get_internal_table_details(1: TSessionId session, 2: string table_name, 3: bool include_system_columns = true) throws (1: TDBException e)\n  TTableDetails get_internal_table_details_for_database(1: TSessionId session, 2: string table_name, 3: string database_name) throws (1: TDBException e)\n  list<string> get_users(1: TSessionId session) throws (1: TDBException e)\n  list<TDBInfo> get_databases(1: TSessionId session) throws (1: TDBException e)\n  string get_version() throws (1: TDBException e)\n  void start_heap_profile(1: TSessionId session) throws (1: TDBException e)\n  void stop_heap_profile(1: TSessionId session) throws (1: TDBException e)\n  string get_heap_profile(1: TSessionId session) throws (1: TDBException e)\n  list<TNodeMemoryInfo> get_memory(1: TSessionId session, 2: string memory_level) throws (1: TDBException e)\n  void clear_cpu_memory(1: TSessionId session) throws (1: TDBException e)\n  void clear_gpu_memory(1: TSessionId session) throws (1: TDBException e)\n  void set_cur_session(1: TSessionId parent_session, 2: TSessionId leaf_session, 3: string start_time_str, 4: string label, 5: bool for_running_query_kernel) throws (1: TDBException e)\n  void invalidate_cur_session(1: TSessionId parent_session, 2: TSessionId leaf_session, 3: string start_time_str, 4: string label, 5: bool for_running_query_kernel) throws (1: TDBException e)\n  void set_table_epoch (1: TSessionId session, 2: i32 db_id, 3: i32 table_id, 4: i32 new_epoch) throws (1: TDBException e)\n  void set_table_epoch_by_name (1: TSessionId session, 2: string table_name, 3: i32 new_epoch) throws (1: TDBException e)\n  i32 get_table_epoch (1: TSessionId session, 2: i32 db_id, 3: i32 table_id)\n  i32 get_table_epoch_by_name (1: TSessionId session, 2: string table_name)\n  list<TTableEpochInfo> get_table_epochs(1: TSessionId session, 2: i32 db_id, 3: i32 table_id)\n  void set_table_epochs(1: TSessionId session, 2: i32 db_id, 3: list<TTableEpochInfo> table_epochs)\n  TSessionInfo get_session_info(1: TSessionId session) throws (1: TDBException e)\n  list<TQueryInfo> get_queries_info(1: TSessionId session) throws (1: TDBException e)\n  void set_leaf_info(1: TSessionId session, 2: TLeafInfo leaf_info) throws (1: TDBException e)\n\n  # query, render\n  TQueryResult sql_execute(1: TSessionId session, 2: string query, 3: bool column_format, 4: string nonce, 5: i32 first_n = -1, 6: i32 at_most_n = -1) throws (1: TDBException e)\n  TDataFrame sql_execute_df(1: TSessionId session, 2: string query, 3: common.TDeviceType device_type, 4: i32 device_id = 0, 5: i32 first_n = -1, 6: TArrowTransport transport_method) throws (1: TDBException e)\n  TDataFrame sql_execute_gdf(1: TSessionId session, 2: string query, 3: i32 device_id = 0, 4: i32 first_n = -1) throws (1: TDBException e)\n  void deallocate_df(1: TSessionId session, 2: TDataFrame df, 3: common.TDeviceType device_type, 4: i32 device_id = 0) throws (1: TDBException e)\n  void interrupt(1: TSessionId query_session, 2: TSessionId interrupt_session) throws (1: TDBException e)\n  TRowDescriptor sql_validate(1: TSessionId session, 2: string query) throws (1: TDBException e)\n  list<completion_hints.TCompletionHint> get_completion_hints(1: TSessionId session, 2: string sql, 3: i32 cursor) throws (1: TDBException e)\n  void set_execution_mode(1: TSessionId session, 2: TExecuteMode mode) throws (1: TDBException e)\n  TRenderResult render_vega(1: TSessionId session, 2: i64 widget_id, 3: string vega_json, 4: i32 compression_level, 5: string nonce) throws (1: TDBException e)\n  TPixelTableRowResult get_result_row_for_pixel(1: TSessionId session, 2: i64 widget_id, 3: TPixel pixel, 4: map<string, list<string>> table_col_names, 5: bool column_format, 6: i32 pixelRadius, 7: string nonce) throws (1: TDBException e)\n\n  # custom expressions\n  i32 create_custom_expression(1: TSessionId session, 2: TCustomExpression custom_expression) throws (1: TDBException e)\n  list<TCustomExpression> get_custom_expressions(1: TSessionId session) throws (1: TDBException e)\n  void update_custom_expression(1: TSessionId session, 2: i32 id, 3: string expression_json) throws (1: TDBException e)\n  void delete_custom_expressions(1: TSessionId session, 2: list<i32> custom_expression_ids, 3: bool do_soft_delete) throws (1: TDBException e)\n\n  # dashboards\n  TDashboard get_dashboard(1: TSessionId session, 2: i32 dashboard_id) throws (1: TDBException e)\n  list<TDashboard> get_dashboards(1: TSessionId session) throws (1: TDBException e)\n  i32 create_dashboard(1: TSessionId session, 2: string dashboard_name, 3: string dashboard_state, 4: string image_hash, 5: string dashboard_metadata) throws (1: TDBException e)\n  void replace_dashboard(1: TSessionId session, 2: i32 dashboard_id, 3: string dashboard_name, 4: string dashboard_owner, 5: string dashboard_state, 6: string image_hash, 7: string dashboard_metadata) throws (1: TDBException e)\n  void delete_dashboard(1: TSessionId session, 2: i32 dashboard_id) throws (1: TDBException e)\n  void share_dashboards(1: TSessionId session, 2: list<i32> dashboard_ids, 3: list<string> groups, 4: TDashboardPermissions permissions) throws (1: TDBException e)\n  void delete_dashboards(1: TSessionId session, 2: list<i32> dashboard_ids) throws (1: TDBException e)\n  void share_dashboard(1: TSessionId session, 2: i32 dashboard_id, 3: list<string> groups, 4: list<string> objects, 5: TDashboardPermissions permissions, 6: bool grant_role = false) throws (1: TDBException e)\n  void unshare_dashboard(1: TSessionId session, 2: i32 dashboard_id, 3: list<string> groups, 4: list<string> objects, 5: TDashboardPermissions permissions) throws (1: TDBException e)\n  void unshare_dashboards(1: TSessionId session, 2: list<i32> dashboard_ids, 3: list<string> groups, 4: TDashboardPermissions permissions) throws (1: TDBException e)\n  list<TDashboardGrantees> get_dashboard_grantees(1: TSessionId session, 2: i32 dashboard_id) throws (1: TDBException e)\n  #dashboard links\n  TFrontendView get_link_view(1: TSessionId session, 2: string link) throws (1: TDBException e)\n  string create_link(1: TSessionId session, 2: string view_state, 3: string view_metadata) throws (1: TDBException e)\n  # import\n  void load_table_binary(1: TSessionId session, 2: string table_name, 3: list<TRow> rows, 4: list<string> column_names = {}) throws (1: TDBException e)\n  void load_table_binary_columnar(1: TSessionId session, 2: string table_name, 3: list<TColumn> cols, 4: list<string> column_names = {}) throws (1: TDBException e)\n  void load_table_binary_arrow(1: TSessionId session, 2: string table_name, 3: binary arrow_stream, 4: bool use_column_names = false) throws (1: TDBException e)\n  void load_table(1: TSessionId session, 2: string table_name, 3: list<TStringRow> rows, 4: list<string> column_names = {}) throws (1: TDBException e)\n  TDetectResult detect_column_types(1: TSessionId session, 2: string file_name, 3: TCopyParams copy_params) throws (1: TDBException e)\n  void create_table(1: TSessionId session, 2: string table_name, 3: TRowDescriptor row_desc, 4: TCreateParams create_params) throws (1: TDBException e)\n  void import_table(1: TSessionId session, 2: string table_name, 3: string file_name, 4: TCopyParams copy_params) throws (1: TDBException e)\n  void import_geo_table(1: TSessionId session, 2: string table_name, 3: string file_name, 4: TCopyParams copy_params, 5: TRowDescriptor row_desc, 6: TCreateParams create_params) throws (1: TDBException e)\n  TImportStatus import_table_status(1: TSessionId session, 2: string import_id) throws (1: TDBException e)\n  string get_first_geo_file_in_archive(1: TSessionId session, 2: string archive_path, 3: TCopyParams copy_params) throws (1: TDBException e)\n  list<string> get_all_files_in_archive(1: TSessionId session, 2: string archive_path, 3: TCopyParams copy_params) throws (1: TDBException e)\n  list<TGeoFileLayerInfo> get_layers_in_geo_file(1: TSessionId session, 2: string file_name, 3: TCopyParams copy_params) throws (1: TDBException e)\n  # distributed\n  i64 query_get_outer_fragment_count(1: TSessionId session, 2: string query) throws(1: TDBException e)\n  TTableMeta check_table_consistency(1: TSessionId session, 2: i32 table_id) throws (1: TDBException e)\n  TPendingQuery start_query(1: TSessionId leaf_session, 2: TSessionId parent_session, 3: string query_ra, 4: string start_time_str, 5: bool just_explain, 6: list<i64> outer_fragment_indices) throws (1: TDBException e)\n  TStepResult execute_query_step(1: TPendingQuery pending_query, 2: TSubqueryId subquery_id, 3: string start_time_str) throws (1: TDBException e)\n  void broadcast_serialized_rows(1: serialized_result_set.TSerializedRows serialized_rows, 2: TRowDescriptor row_desc, 3: TQueryId query_id, 4: TSubqueryId subquery_id, 5: bool is_final_subquery_result) throws (1: TDBException e)\n  TPendingRenderQuery start_render_query(1: TSessionId session, 2: i64 widget_id, 3: i16 node_idx, 4: string vega_json) throws (1: TDBException e)\n  TRenderStepResult execute_next_render_step(1: TPendingRenderQuery pending_render, 2: TRenderAggDataMap merged_data) throws (1: TDBException e)\n  void insert_data(1: TSessionId session, 2: TInsertData insert_data) throws (1: TDBException e)\n  void insert_chunks(1: TSessionId session, 2: TInsertChunks insert_chunks) throws (1: TDBException e)\n  void checkpoint(1: TSessionId session, 2: i32 table_id) throws (1: TDBException e)\n  # object privileges\n  list<string> get_roles(1: TSessionId session) throws (1: TDBException e)\n  list<TDBObject> get_db_objects_for_grantee(1: TSessionId session, 2: string roleName) throws (1: TDBException e)\n  list<TDBObject> get_db_object_privs(1: TSessionId session, 2: string objectName, 3: TDBObjectType type) throws (1: TDBException e)\n  list<string> get_all_roles_for_user(1: TSessionId session, 2: string userName) throws (1: TDBException e)  # NOTE: only gives direct roles, not all effective roles\n  list<string> get_all_effective_roles_for_user(1: TSessionId session, 2: string userName) throws (1: TDBException e)\n  bool has_role(1: TSessionId session, 2: string granteeName, 3: string roleName) throws (1: TDBException e)\n  bool has_object_privilege(1: TSessionId session, 2: string granteeName, 3: string ObjectName, 4: TDBObjectType objectType, 5: TDBObjectPermissions permissions) throws (1: TDBException e)\n  # licensing\n  TLicenseInfo set_license_key(1: TSessionId session, 2: string key, 3: string nonce = \"\") throws (1: TDBException e)\n  TLicenseInfo get_license_claims(1: TSessionId session, 2: string nonce = \"\") throws (1: TDBException e)\n  # user-defined functions\n  map<string, string> get_device_parameters(1: TSessionId session) throws (1: TDBException e)\n  void register_runtime_extension_functions(1: TSessionId session, 2: list<extension_functions.TUserDefinedFunction> udfs, 3: list<extension_functions.TUserDefinedTableFunction> udtfs, 4: map<string, string> device_ir_map) throws (1: TDBException e)\n  list<string> get_table_function_names(1: TSessionId session) throws (1: TDBException e)\n  list<string> get_runtime_table_function_names(1: TSessionId session) throws (1: TDBException e)\n  list<extension_functions.TUserDefinedTableFunction> get_table_function_details(1: TSessionId session, 2: list<string> udtf_names) throws (1: TDBException e)\n  list<string> get_function_names(1: TSessionId session) throws (1: TDBException e)\n  list<string> get_runtime_function_names(1: TSessionId session) throws (1: TDBException e)\n  list<extension_functions.TUserDefinedFunction> get_function_details(1: TSessionId session, 2: list<string> udf_names) throws (1: TDBException e)\n}\n"
        },
        {
          "name": "heavyai.conf.sample",
          "type": "blob",
          "size": 0.134765625,
          "content": "port = 6274\nhttp-port = 6278\ncalcite-port = 6279\ndata = \"data\"\nread-only = false\nverbose = false\n\n[web]\nport = 6273\nfrontend = \"frontend\"\n"
        },
        {
          "name": "initdb.cpp",
          "type": "blob",
          "size": 13.0673828125,
          "content": "/*\n * Copyright 2022 HEAVY.AI, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <thrift/Thrift.h>\n#include <array>\n#include <boost/filesystem.hpp>\n#include <boost/program_options.hpp>\n#include <exception>\n#include <iostream>\n#include <memory>\n#include <string>\n\n#include \"Catalog/Catalog.h\"\n#include \"Logger/Logger.h\"\n#include \"OSDependent/heavyai_path.h\"\n#include \"Shared/SysDefinitions.h\"\n#include \"ThriftHandler/DBHandler.h\"\n\n#define CALCITEPORT 3279\n\nstatic const std::array<std::string, 3> SampleGeoFileNames{\"us-states.json\",\n                                                           \"us-counties.json\",\n                                                           \"countries.json\"};\nstatic const std::array<std::string, 3> SampleGeoTableNames{\"heavyai_us_states\",\n                                                            \"heavyai_us_counties\",\n                                                            \"heavyai_countries\"};\n\nextern bool g_enable_thrift_logs;\n\nstatic void loadGeo(std::string base_path) {\n  TSessionId session_id{};\n  SystemParameters system_parameters{};\n  AuthMetadata auth_metadata{};\n  std::string udf_filename{};\n  std::string udf_compiler_path{};\n  std::vector<std::string> udf_compiler_options{};\n#ifdef ENABLE_GEOS\n  std::string libgeos_so_filename{};\n#endif\n#ifdef HAVE_TORCH_TFS\n  std::string torch_lib_path{};\n#endif\n  std::vector<LeafHostInfo> db_leaves{};\n  std::vector<LeafHostInfo> string_leaves{};\n\n  // Whitelist root path for tests by default\n  ddl_utils::FilePathWhitelist::clear();\n  ddl_utils::FilePathWhitelist::initialize(base_path, \"[\\\"/\\\"]\", \"[\\\"/\\\"]\");\n\n  // Based on default values observed from starting up an OmniSci DB server.\n  const bool allow_multifrag{true};\n  const bool jit_debug{false};\n  const bool intel_jit_profile{false};\n  const bool read_only{false};\n  const bool allow_loop_joins{false};\n  const bool enable_rendering{false};\n  const bool renderer_prefer_igpu{false};\n  const unsigned renderer_vulkan_timeout_ms{300000};\n  const bool renderer_use_parallel_executors{false};\n  const bool enable_auto_clear_render_mem{false};\n  const int render_oom_retry_threshold{0};\n  const size_t render_mem_bytes{500000000};\n  const size_t max_concurrent_render_sessions{500};\n  const bool render_compositor_use_last_gpu{false};\n  const bool renderer_enable_slab_allocation{true};\n  const size_t reserved_gpu_mem{134217728};\n  const size_t num_reader_threads{0};\n  const bool legacy_syntax{true};\n  const int idle_session_duration{60};\n  const int max_session_duration{43200};\n  system_parameters.runtime_udf_registration_policy =\n      SystemParameters::RuntimeUdfRegistrationPolicy::DISALLOWED;\n  system_parameters.omnisci_server_port = -1;\n  system_parameters.calcite_port = 3280;\n\n  system_parameters.aggregator = false;\n  g_leaf_count = 0;\n  g_cluster = false;\n\n  File_Namespace::DiskCacheLevel cache_level{File_Namespace::DiskCacheLevel::fsi};\n  File_Namespace::DiskCacheConfig disk_cache_config{\n      File_Namespace::DiskCacheConfig::getDefaultPath(std::string(base_path)),\n      cache_level};\n\n  auto db_handler = std::make_unique<DBHandler>(db_leaves,\n                                                string_leaves,\n                                                base_path,\n                                                allow_multifrag,\n                                                jit_debug,\n                                                intel_jit_profile,\n                                                read_only,\n                                                allow_loop_joins,\n                                                enable_rendering,\n                                                renderer_prefer_igpu,\n                                                renderer_vulkan_timeout_ms,\n                                                renderer_use_parallel_executors,\n                                                enable_auto_clear_render_mem,\n                                                render_oom_retry_threshold,\n                                                render_mem_bytes,\n                                                max_concurrent_render_sessions,\n                                                reserved_gpu_mem,\n                                                render_compositor_use_last_gpu,\n                                                renderer_enable_slab_allocation,\n                                                num_reader_threads,\n                                                auth_metadata,\n                                                system_parameters,\n                                                legacy_syntax,\n                                                idle_session_duration,\n                                                max_session_duration,\n                                                udf_filename,\n                                                udf_compiler_path,\n                                                udf_compiler_options,\n#ifdef ENABLE_GEOS\n                                                libgeos_so_filename,\n#endif\n#ifdef HAVE_TORCH_TFS\n                                                torch_lib_path,\n#endif\n                                                disk_cache_config,\n                                                false);\n  db_handler->internal_connect(session_id, shared::kRootUsername, shared::kDefaultDbName);\n\n  // Execute on CPU by default\n  db_handler->set_execution_mode(session_id, TExecuteMode::CPU);\n  TQueryResult res;\n\n  const size_t num_samples = SampleGeoFileNames.size();\n  for (size_t i = 0; i < num_samples; i++) {\n    const std::string table_name = SampleGeoTableNames[i];\n    const std::string file_name = SampleGeoFileNames[i];\n\n    auto file_path = boost::filesystem::path(heavyai::get_root_abs_path()) /\n                     \"ThirdParty\" / \"geo_samples\" / file_name;\n\n    if (!boost::filesystem::exists(file_path)) {\n      throw std::runtime_error(\n          \"Unable to populate geo sample data. File does not exist: \" +\n          file_path.string());\n    }\n#ifdef _WIN32\n    std::string sql_string = \"COPY \" + table_name + \" FROM '\" +\n                             file_path.generic_string() + \"' WITH (GEO='true');\";\n#else\n    std::string sql_string =\n        \"COPY \" + table_name + \" FROM '\" + file_path.string() + \"' WITH (GEO='true');\";\n#endif\n    db_handler->sql_execute(res, session_id, sql_string, true, \"\", -1, -1);\n  }\n}\n\nint main(int argc, char* argv[]) {\n  std::string base_path;\n  bool force = false;\n  bool skip_geo = false;\n  namespace po = boost::program_options;\n\n  po::options_description desc(\"Options\");\n  desc.add_options()(\"help,h\", \"Print help messages \")(\n      \"data\",\n      po::value<std::string>(&base_path)->required(),\n      \"Directory path to HeavyDB catalogs\")(\"force,f\",\n                                            \"Force overwriting of existing HeavyDB \"\n                                            \"instance\")(\"skip-geo\",\n                                                        \"Skip inserting sample geo data\");\n\n  desc.add_options()(\"enable-thrift-logs\",\n                     po::value<bool>(&g_enable_thrift_logs)\n                         ->default_value(g_enable_thrift_logs)\n                         ->implicit_value(true),\n                     \"Enable writing messages directly from thrift to stdout/stderr.\");\n\n  logger::LogOptions log_options(argv[0]);\n  desc.add(log_options.get_options());\n\n  po::positional_options_description positionalOptions;\n  positionalOptions.add(\"data\", 1);\n\n  po::variables_map vm;\n\n  try {\n    po::store(po::command_line_parser(argc, argv)\n                  .options(desc)\n                  .positional(positionalOptions)\n                  .run(),\n              vm);\n    if (vm.count(\"help\")) {\n      std::cout << desc;\n      return 0;\n    }\n    if (vm.count(\"force\")) {\n      force = true;\n    }\n    if (vm.count(\"skip-geo\")) {\n      skip_geo = true;\n    }\n    po::notify(vm);\n  } catch (boost::program_options::error& e) {\n    std::cerr << \"Usage Error: \" << e.what() << std::endl;\n    return 1;\n  }\n\n  if (!g_enable_thrift_logs) {\n    apache::thrift::GlobalOutput.setOutputFunction([](const char* msg) {});\n  }\n\n  if (!boost::filesystem::exists(base_path)) {\n    std::cerr << \"Catalog basepath \" + base_path + \" does not exist.\\n\";\n    return 1;\n  }\n  std::string catalogs_path = base_path + \"/\" + shared::kCatalogDirectoryName;\n  if (boost::filesystem::exists(catalogs_path)) {\n    if (force) {\n      boost::filesystem::remove_all(catalogs_path);\n    } else {\n      std::cerr << \"HeavyDB catalogs directory already exists at \" + catalogs_path +\n                       \". Use -f to force reinitialization.\\n\";\n      return 1;\n    }\n  }\n  std::string data_path = base_path + \"/\" + shared::kDataDirectoryName;\n  if (boost::filesystem::exists(data_path)) {\n    if (force) {\n      boost::filesystem::remove_all(data_path);\n    } else {\n      std::cerr << \"HeavyDB data directory already exists at \" + data_path +\n                       \". Use -f to force reinitialization.\\n\";\n      return 1;\n    }\n  }\n  std::string lockfiles_path = base_path + \"/\" + shared::kLockfilesDirectoryName;\n  if (boost::filesystem::exists(lockfiles_path)) {\n    if (force) {\n      boost::filesystem::remove_all(lockfiles_path);\n    } else {\n      std::cerr << \"HeavyDB lockfiles directory already exists at \" + lockfiles_path +\n                       \". Use -f to force reinitialization.\\n\";\n      return 1;\n    }\n  }\n  std::string lockfiles_path2 = lockfiles_path + \"/\" + shared::kCatalogDirectoryName;\n  if (boost::filesystem::exists(lockfiles_path2)) {\n    if (force) {\n      boost::filesystem::remove_all(lockfiles_path2);\n    } else {\n      std::cerr << \"HeavyDB lockfiles catalogs directory already exists at \" +\n                       lockfiles_path2 + \". Use -f to force reinitialization.\\n\";\n      return 1;\n    }\n  }\n  std::string lockfiles_path3 = lockfiles_path + \"/\" + shared::kDataDirectoryName;\n  if (boost::filesystem::exists(lockfiles_path3)) {\n    if (force) {\n      boost::filesystem::remove_all(lockfiles_path3);\n    } else {\n      std::cerr << \"HeavyDB lockfiles data directory already exists at \" +\n                       lockfiles_path3 + \". Use -f to force reinitialization.\\n\";\n      return 1;\n    }\n  }\n  std::string export_path = base_path + \"/\" + shared::kDefaultExportDirName;\n  if (boost::filesystem::exists(export_path)) {\n    if (force) {\n      boost::filesystem::remove_all(export_path);\n    } else {\n      std::cerr << \"HeavyDB export directory already exists at \" + export_path +\n                       \". Use -f to force reinitialization.\\n\";\n      return 1;\n    }\n  }\n  std::string disk_cache_path = base_path + \"/\" + shared::kDefaultDiskCacheDirName;\n  if (boost::filesystem::exists(disk_cache_path)) {\n    if (force) {\n      boost::filesystem::remove_all(disk_cache_path);\n    } else {\n      std::cerr << \"HeavyDB disk cache already exists at \" + disk_cache_path +\n                       \". Use -f to force reinitialization.\\n\";\n      return 1;\n    }\n  }\n\n  if (!boost::filesystem::create_directory(catalogs_path)) {\n    std::cerr << \"Cannot create \" + shared::kCatalogDirectoryName + \" subdirectory under \"\n              << base_path << std::endl;\n  }\n  if (!boost::filesystem::create_directory(lockfiles_path)) {\n    std::cerr << \"Cannot create \" + shared::kLockfilesDirectoryName +\n                     \" subdirectory under \"\n              << base_path << std::endl;\n  }\n  if (!boost::filesystem::create_directory(lockfiles_path2)) {\n    std::cerr << \"Cannot create \" + shared::kLockfilesDirectoryName + \"/\" +\n                     shared::kCatalogDirectoryName + \" subdirectory under \"\n              << base_path << std::endl;\n  }\n  if (!boost::filesystem::create_directory(lockfiles_path3)) {\n    std::cerr << \"Cannot create \" + shared::kLockfilesDirectoryName + \"/\" +\n                     shared::kDataDirectoryName + \" subdirectory under \"\n              << base_path << std::endl;\n  }\n  if (!boost::filesystem::create_directory(export_path)) {\n    std::cerr << \"Cannot create \" + shared::kDefaultExportDirName + \" subdirectory under \"\n              << base_path << std::endl;\n  }\n\n  log_options.set_base_path(base_path);\n  logger::init(log_options);\n\n  try {\n    SystemParameters sys_parms;\n    auto dummy = std::make_shared<Data_Namespace::DataMgr>(\n        data_path, sys_parms, nullptr, false, 0);\n    auto calcite =\n        std::make_shared<Calcite>(-1, CALCITEPORT, base_path, 1024, 5000, true, \"\");\n    g_base_path = base_path;\n    auto& sys_cat = Catalog_Namespace::SysCatalog::instance();\n    sys_cat.init(base_path, dummy, {}, calcite, true, false, {});\n\n  } catch (std::exception& e) {\n    std::cerr << \"Exception: \" << e.what() << \"\\n\";\n  }\n\n  if (!skip_geo) {\n    loadGeo(base_path);\n  } else {\n    Catalog_Namespace::SysCatalog::destroy();\n  }\n\n  return 0;\n}\n"
        },
        {
          "name": "insert_sample_data",
          "type": "blob",
          "size": 2.6416015625,
          "content": "#!/usr/bin/env bash\n\nset -e\n\nMAPD_TCP_PORT=${MAPD_TCP_PORT:=6274}\n\nDATA_URL=${DATA_URL:=\"https://data.mapd.com\"}\n\nALLOW_DOWNLOADS=${ALLOW_DOWNLOADS:=true}\n\nif hash wget 2>/dev/null; then\n  GETTER=\"wget --continue\"\nelif hash curl 2>/dev/null; then\n  GETTER=\"curl --continue - --remote-name --location\"\nelse\n  GETTER=\"echo Please download: \"\nfi\n\ndownload_and_extract_file() {\n  pushd $SAMPLE_PATH\n  echo \"- downloading and extracting $1\"\n  $GETTER \"$DATA_URL/$1\"\n  tar xvf \"$1\"\n  popd\n}\n\nwhile (( $# )); do\n  case \"$1\" in\n    --port)\n      shift\n      MAPD_TCP_PORT=$1 ;;\n    --url)\n      shift\n      DATA_URL=$1 ;;\n    --path)\n      shift\n      SAMPLE_PATH=$1 ;;\n    --no-download)\n      ALLOW_DOWNLOADS=false\n      ;;\n    --data)\n      shift\n      MAPD_DATA=$1\n      ;;\n    *)\n      break ;;\n  esac\n  shift\ndone\n\nif [ -z \"${MAPD_DATA}\" ]; then\n  MAPD_DATA=${MAPD_DATA:=\"$PWD/storage\"}\n  echo \"Using default storage directory: \\\"$MAPD_DATA\\\" if file path is not whitelisted use '--data /path_to_server_data_directory'\"\nfi\n\nSAMPLE_PATH=${SAMPLE_PATH:=\"$MAPD_DATA/import/sample_datasets\"}\n\nMKRES=$(mkdir -p \"$SAMPLE_PATH\")\nif ! mkdir -p \"$SAMPLE_PATH\" || [ ! -w \"$SAMPLE_PATH\" ] ; then\n  SAMPLE_PATH2=$(mktemp -d)\n  echo \"Cannot write sample data to: $SAMPLE_PATH\"\n  echo \"Saving instead to: $SAMPLE_PATH2\"\n  echo\n  SAMPLE_PATH=$SAMPLE_PATH2\n  mkdir -p \"$SAMPLE_PATH\"\nfi\n\nif [ \"$ALLOW_DOWNLOADS\" = false ] ; then\n  GETTER=\"echo Using: \"\nfi\n\nif [ \"$ALLOW_DOWNLOADS\" = true ] ; then\n  pushd \"$SAMPLE_PATH\"\n  rm -f manifest.tsv\n  $GETTER \"$DATA_URL/manifest.tsv\"\n  popd\nfi\n\ncounter=1\nwhile IFS=$'\\t' read -r name size tablename filename ; do\n  names[$counter]=$name\n  sizes[$counter]=$size\n  tables[$counter]=$tablename\n  files[$counter]=$filename\n  counter=$((counter+1))\ndone < \"$SAMPLE_PATH/manifest.tsv\"\n\necho \"Enter dataset number to download, or 'q' to quit:\"\ntable=\" # | Dataset | Rows | Table Name | File Name\"\nfor key in \"${!files[@]}\"; do\n  table=\"$table\n $key) | ${names[$key]} | ${sizes[$key]} | ${tables[$key]} | ${files[$key]}\"\ndone\n\ncolumn -t -s'|' <(echo \"$table\")\n\nread -r idxs\n\nif [ -z \"$idxs\" ]; then\n  idxs=(${!files[@]})\nfi\n\nfor idx in $idxs; do\n  if [ \"${files[$idx]}\" ]; then\n    filename=\"${files[$idx]}\"\n    download_and_extract_file \"$filename\"\n\n    filebase=\"${filename%%.*}\"\n\n    echo \"- adding schema\"\n    ./bin/heavysql heavyai -u admin -p HyperInteractive --port \"$MAPD_TCP_PORT\" < \"$SAMPLE_PATH/$filebase\"/*.sql\n    table=${tables[$idx]}\n    for csv in $SAMPLE_PATH/$filebase/*csv; do\n      echo \"- inserting file: $csv\"\n      echo \"copy $table from '${csv}' with (quoted='true');\" | ./bin/heavysql heavyai -u admin -p HyperInteractive --port \"$MAPD_TCP_PORT\"\n    done\n  fi\ndone\n"
        },
        {
          "name": "java",
          "type": "tree",
          "content": null
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "startheavy",
          "type": "blob",
          "size": 2.6484375,
          "content": "#!/bin/bash\n\nset -e\n\nfunction get_script_location()    # https://stackoverflow.com/a/246128\n{\n    SOURCE=\"${BASH_SOURCE[0]}\"\n    while [ -h \"$SOURCE\" ]; do # resolve $SOURCE until the file is no longer a symlink\n      DIR=\"$( cd -P \"$( dirname \"$SOURCE\" )\" >/dev/null 2>&1 && pwd )\"\n      # Note symlinks will fail on macOS due to missing readlink command\n      SOURCE=\"$(readlink \"$SOURCE\")\"\n      [[ $SOURCE != /* ]] && SOURCE=\"$DIR/$SOURCE\" # if $SOURCE was a relative symlink, we need to resolve it relative to the path where the symlink file was located\n    done\n    DIR=\"$( cd -P \"$( dirname \"$SOURCE\" )\" >/dev/null 2>&1 && pwd )\"\n    echo $DIR\n}\n\nDIR=$(get_script_location)\n\nif [[ -t 0 || -p /dev/stdin ]]; then    # http://tldp.org/LDP/abs/html/intandnonint.html\n    TTY=$(tty || true)\nelse\n    unset TTY\nfi\nif [[ \"$TTY\" = \"not a tty\" ]]; then    # work around old versions of Docker\n  unset TTY\nfi\n\necho \"${0##*/} $$ running\"\n\nif hash setsid 2>/dev/null; then\n    # creates a new process group (and a new session)\n    # to see process groups (PGID's), try 'pstree -g' or something like:\n    #     ps -u yourname -o pid,ppid,pgid,args | head -1 ; ps -u yourname -o pid,ppid,pgid,args | grep -E 'omnisci|calcite' | grep -v grep\n    if [ \"$TTY\" != \"\" ]; then\n      setsid $DIR/scripts/innerstartheavy \"$@\" <$TTY &\n    else\n      setsid $DIR/scripts/innerstartheavy --non-interactive \"$@\" &\n    fi\n    SUBPID=$!\nelse\n    # setsid missing on macOS\n    # this should still work but not quite as cleanly as with setsid\n    PGID=$$\n    export PGID\n    if [ \"$TTY\" != \"\" ]; then\n      $DIR/scripts/innerstartheavy \"$@\" <$TTY &\n    else\n      $DIR/scripts/innerstartheavy --non-interactive \"$@\" &\n    fi\n    SUBPID=$!\nfi\n\n# Immediately kill the single $SUBPID process if this script gets a signal.\n# This script will also exit due to the wait on the last line unblocking and/or\n# because bash will always stop this script if a CTRL-C is typed.\ntrap 'set +e ; trap - SIGTERM ; echo ; echo \"${0##*/} $$ shutting down\" ; kill $SUBPID 2>/dev/null' SIGHUP SIGINT SIGTERM\n\n# To be safe kill the entire $SUBPID process group when this script exits for any reason.\n# This happens after a short delay to give OmniSciDB C++ a chance to shut itself down cleanly\n# if an earlier signal+trap triggered this exit.\ntrap 'set +e ; trap - SIGTERM ; sleep 1 ; kill -- -$SUBPID 2>/dev/null ; sleep 1 ; echo \"${0##*/} $$ exited\"' EXIT\n\n# Be aware that CTRL-C has special handling by bash. Pressing\n# CTRL-C at the terminal causes Bash to send SIGINT to the entire\n# process group before this trap gets run. Bash also kills this\n# script after CTRL-C after the trap runs even if we wanted this\n# script to keep running.\n\nwait $SUBPID\n"
        },
        {
          "name": "systemd",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}