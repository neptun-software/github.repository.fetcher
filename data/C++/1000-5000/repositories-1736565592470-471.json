{
  "metadata": {
    "timestamp": 1736565592470,
    "page": 471,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "mapbox/tippecanoe",
      "stars": 2763,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0810546875,
          "content": "# Don't copy Dockerfile or git items\n.gitignore\n.git\nDockerfile\nDockerfile.centos7\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.4736328125,
          "content": "# Compiled Object files\n*.slo\n*.lo\n*.o\n*.obj\n\n# Autogenerated dependencies\n*.d\n\n# Precompiled Headers\n*.gch\n*.pch\n\n# Compiled Dynamic libraries\n*.so\n*.dylib\n*.dll\n\n# Fortran module files\n*.mod\n*.smod\n\n# Compiled Static libraries\n*.lai\n*.la\n*.a\n*.lib\n\n# Executables\n*.exe\n*.out\n*.app\ntippecanoe\ntile-join\ntippecanoe-decode\ntippecanoe-enumerate\ntippecanoe-json-tool\nunit\n\n# Tests\ntests/**/*.mbtiles\ntests/**/*.check\ntests/**/*.geobuf\n\n# Vim\n*.swp\n\n# Mac\n.DS_Store\n\n# Nodejs\nnode_modules\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 3.3935546875,
          "content": "language: node_js\nnode_js:\n  - \"6\"\n\nsudo: false\n\nmatrix:\n  include:\n    # test on docker+centos7\n    - os: linux\n      compiler: clang\n      services:\n       - docker\n      sudo: true\n      dist: trusty\n      env: DOCKERFILE=Dockerfile.centos7\n      before_install: []\n      install:\n        - docker build -t tippecanoe-image -f ${DOCKERFILE} .\n      script:\n        - docker run -it tippecanoe-image\n    # test on docker+ubuntu\n    - os: linux\n      compiler: clang\n      services:\n       - docker\n      sudo: true\n      dist: trusty\n      env: DOCKERFILE=Dockerfile\n      before_install: []\n      install:\n        - docker build -t tippecanoe-image -f ${DOCKERFILE} .\n      script:\n        - docker run -it tippecanoe-image\n    # debug+integer-santizer build\n    - os: linux\n      compiler: clang\n      env: CLANG_VERSION='3.8.0' BUILDTYPE=Debug CC=\"clang-3.8\" CXX=\"clang++-3.8\" CXXFLAGS=\"-fsanitize=integer\" CFLAGS=\"-fsanitize=integer\" LDFLAGS=\"-fsanitize=integer\"\n      addons:\n        apt:\n          sources: ['ubuntu-toolchain-r-test' ]\n          packages: [ 'libstdc++6','libstdc++-5-dev' ]\n    # debug+leak+address-sanitizer build\n    - os: linux\n      compiler: clang\n      env: CLANG_VERSION='3.8.0' BUILDTYPE=Debug ASAN_OPTIONS=detect_leaks=1 CC=\"clang-3.8\" CXX=\"clang++-3.8\" CXXFLAGS=\"-fsanitize=address,undefined\" CFLAGS=\"-fsanitize=address,undefined\" LDFLAGS=\"-fsanitize=address,undefined\" FEWER=true\n      addons:\n        apt:\n          sources: ['ubuntu-toolchain-r-test' ]\n          packages: [ 'libstdc++6','libstdc++-5-dev' ]\n    # coverage+debug build\n    - os: linux\n      compiler: clang\n      env: CLANG_VERSION='3.8.0' BUILDTYPE=Debug CC=\"clang-3.8\" CXX=\"clang++-3.8\" CXXFLAGS=\"--coverage\" CFLAGS=\"--coverage\" LDFLAGS=\"--coverage\"\n      after_script:\n        - mason install llvm-cov 3.9.1\n        - mason link llvm-cov 3.9.1\n        - which llvm-cov\n        - curl -S -f https://codecov.io/bash -o codecov\n        - chmod +x codecov\n        - ./codecov -x \"llvm-cov gcov\" -Z\n      addons:\n        apt:\n          sources: ['ubuntu-toolchain-r-test' ]\n          packages: [ 'libstdc++6','libstdc++-5-dev' ]\n    # release+linux+g++\n    - os: linux\n      compiler: gcc\n      env: BUILDTYPE=Release CC=\"gcc-4.9\" CXX=\"g++-4.9\"\n      addons:\n        apt:\n          sources: ['ubuntu-toolchain-r-test']\n          packages: [ 'g++-4.9' ]\n    # release+linux+clang++\n    - os: linux\n      compiler: clang\n      env: CLANG_VERSION='3.8.0' BUILDTYPE=Release CC=\"clang-3.8\" CXX=\"clang++-3.8\"\n      addons:\n        apt:\n          sources: ['ubuntu-toolchain-r-test' ]\n          packages: [ 'libstdc++6','libstdc++-5-dev' ]\n    # release+osx\n    - os: osx\n      compiler: clang\n      env: BUILDTYPE=Release\n    # debug+osx\n    - os: osx\n      compiler: clang\n      env: BUILDTYPE=Debug\n\nbefore_install:\n  - DEPS_DIR=\"${TRAVIS_BUILD_DIR}/deps\"\n  - export PATH=${DEPS_DIR}/bin:${PATH} && mkdir -p ${DEPS_DIR}\n  - |\n    if [[ ${CLANG_VERSION:-false} != false ]]; then\n      export CCOMPILER='clang'\n      export CXXCOMPILER='clang++'\n      CLANG_URL=\"https://mason-binaries.s3.amazonaws.com/${TRAVIS_OS_NAME}-x86_64/clang/${CLANG_VERSION}.tar.gz\"\n      travis_retry wget --quiet -O - ${CLANG_URL} | tar --strip-components=1 -xz -C ${DEPS_DIR}\n    fi\n\ninstall:\n - BUILDTYPE=${BUILDTYPE} make -j\n\nscript:\n - npm install geobuf\n - if [ -n \"${FEWER}\" ]; then\n       BUILDTYPE=${BUILDTYPE} make fewer-tests; else\n       BUILDTYPE=${BUILDTYPE} make test geobuf-test;\n   fi\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 18.9951171875,
          "content": "## 1.36.0\n\n* Update Wagyu to version 0.5.0\n\n## 1.35.0\n\n* Fix calculation of mean when accumulating attributes in clusters\n\n## 1.34.6\n\n* Fix crash when there are null entries in the metadata table\n\n## 1.34.5\n\n* Fix line numbers in GeoJSON feature parsing error messages\n\n## 1.34.4\n\n* Be careful to avoid undefined behavior from shifting negative numbers\n\n## 1.34.3\n\n* Add an option to keep intersection nodes from being simplified away\n\n## 1.34.2\n\n* Be more consistent about when longitudes beyond 180 are allowed.\n  Now if the entire feature is beyond 180, it will still appear.\n\n## 1.34.1\n\n* Don't run shell filters if the current zoom is below the minzoom\n* Fix -Z and -z for tile directories in tile-join and tippecanoe-decode\n* Return a successful error status for --help and --version\n\n## 1.34.0\n\n* Record the command line options in the tileset metadata\n\n## 1.33.0\n\n* MultiLineStrings were previously ignored in Geobuf input\n\n## 1.32.12\n\n* Accept .mvt as well as .pbf in directories of tiles\n* Allow tippecanoe-decode and tile-join of directories with no metadata\n\n## 1.32.11\n* Don't let attribute exclusion apply to the attribute that has been specified\n  to become the feature ID\n\n## 1.32.10\n\n* Fix a bug that disallowed a per-feature minzoom of 0\n\n## 1.32.9\n\n* Limit tile detail to 30 and buffer size to 127 to prevent coordinate\n  delta overflow in vector tiles.\n\n## 1.32.8\n\n* Better error message if the output tileset already exists\n\n## 1.32.7\n\n* Point features may now be coalesced into MultiPoint features with --coalesce.\n* Add --hilbert option to put features in Hilbert Curve sequence\n\n## 1.32.6\n\n* Make it an error, not a warning, to have missing coordinates for a point\n\n## 1.32.5\n\n* Use less memory on lines and polygons that are too small for the tile\n* Fix coordinate rounding problem that was causing --grid-low-zooms grids\n  to be lost at low zooms if the original polygons were not aligned to\n  tile boundaries\n\n## 1.32.4\n\n* Ignore leading zeroes when converting string attributes to feature IDs\n\n## 1.32.3\n\n* Add an option to convert stringified number feature IDs to numbers\n* Add an option to use a specified feature attribute as the feature ID\n\n## 1.32.2\n\n* Warn in tile-join if tilesets being joined have inconsistent maxzooms\n\n## 1.32.1\n\n* Fix null pointer crash when reading filter output that does not\n  tag features with their extent\n* Add `--clip-bounding-box` option to clip input geometry\n\n## 1.32.0\n\n* Fix a bug that allowed coalescing of features with mismatched attributes\n  if they had been passed through a shell prefilter\n\n## 1.31.7\n\n* Create the output tile directory even if there are no valid features\n\n## 1.31.6\n\n* Issue an error message in tile-join if minzoom is greater than maxzoom\n\n## 1.31.5\n\n* Add options to change the tilestats limits\n\n## 1.31.4\n\n* Keep tile-join from generating a tileset name longer than 255 characters\n\n## 1.31.3\n\n* Fix the missing filename in JSON parsing warning messages\n\n## 1.31.2\n\n* Don't accept anything inside another JSON object's properties as a\n  feature or geometry of its own.\n\n## 1.31.1\n\n* Add --exclude-all to tile-join\n\n## 1.31.0\n\n* Upgrade Wagyu to version 0.4.3\n\n## 1.30.6\n\n* Take cluster distance into account when guessing a maxzoom\n\n## 1.30.4\n\n* Features within the z0 tile buffer of the antimeridian (not only\n  those that cross it) are duplicated on both sides.\n\n## 1.30.3\n\n* Add an option to automatically assign ids to features\n\n## 1.30.2\n\n* Don't guess a higher maxzoom than is allowed for manual selection\n\n## 1.30.1\n\n* Ensure that per-feature minzoom and maxzoom are integers\n* Report compression errors in tippecanoe-decode\n* Add the ability to specify the file format with -L{\"format\":\"…\"}\n* Add an option to treat empty CSV columns as nulls, not empty strings\n\n## 1.30.0\n\n* Add a filter extension to allow filtering individual attributes\n\n## 1.29.3\n\n* Include a generator field in tileset metadata with the Tippecanoe version\n\n## 1.29.2\n\n* Be careful to remove null attributes from prefilter/postfilter output\n\n## 1.29.1\n\n* Add --use-source-polygon-winding and --reverse-source-polygon-winding\n\n## 1.29.0\n\n* Add the option to specify layer file, name, and description as JSON\n* Add the option to specify the description for attributes in the\n  tileset metadata\n* In CSV input, a trailing comma now counts as a trailing empty field\n* In tippecanoe-json-tool, an empty CSV field is now an empty string,\n  not null (for consistency with tile-join)\n\n## 1.28.1\n\n* Explicitly check for infinite and not-a-number input coordinates\n\n## 1.28.0\n\n* Directly support gzipped GeoJSON as input files\n\n## 1.27.16\n\n* Fix thread safety issues related to the out-of-disk-space checker\n\n## 1.27.15\n\n* --extend-zooms-if-still-dropping now also extends zooms if features\n  are dropped by --force-feature-limit\n\n## 1.27.14\n\n* Use an exit status of 100 if some zoom levels were successfully\n  written but not all zoom levels could be tiled.\n\n## 1.27.13\n\n* Allow filtering features by zoom level in conditional expressions\n* Lines in CSV input with empty geometry columns will be ignored\n\n## 1.27.12\n\n* Check integrity of sqlite3 file before decoding or tile-joining\n\n## 1.27.11\n\n* Always include tile and layer in tippecanoe-decode, fixing corrupt JSON.\n* Clean up writing of JSON in general.\n\n## 1.27.10\n\n* Add --progress-interval setting to reduce progress indicator frequency\n\n## 1.27.9\n\n* Make clusters look better by averaging locations of clustered points\n\n## 1.27.8\n\n* Add --accumulate-attribute to keep attributes of dropped, coalesced,\n  or clustered features\n* Make sure numeric command line arguments are actually numbers\n* Don't coalesce features whose non-string-pool attributes don't match\n\n## 1.27.7\n\n* Add an option to produce only a single tile\n* Retain non-ASCII characters in layernames generated from filenames\n* Remember to close input files after reading them\n* Add --coalesce-fraction-as-needed and --coalesce-densest-as-needed\n* Report distances in both feet and meters\n\n## 1.27.6\n\n* Fix opportunities for integer overflow and out-of-bounds references\n\n## 1.27.5\n\n* Add --cluster-densest-as-needed to cluster features\n* Add --maximum-tile-features to set the maximum number of features in a tile\n\n## 1.27.4\n\n* Support CSV point input\n* Don't coalesce features that have different IDs but are otherwise identical\n* Remove the 700-point limit on coalesced features, since polygon merging\n  is no longer a performance problem\n\n## 1.27.3\n\n* Clean up duplicated code for reading tiles from a directory\n\n## 1.27.2\n\n* Tippecanoe-decode can decode directories of tiles, not just mbtiles\n* The --allow-existing option works on directories of tiles\n* Trim .geojson, not just .json, when making layer names from filenames\n\n## 1.27.1\n\n* Fix a potential null pointer when parsing GeoJSON with bare geometries\n* Fix a bug that could cause the wrong features to be coalesced when\n  input was parsed in parallel\n\n## 1.27.0\n\n* Add tippecanoe-json-tool for sorting and joining GeoJSON files\n* Fix problem where --detect-shared-borders could simplify polygons away\n* Attach --coalesce-smallest-as-needed leftovers to the last feature, not the first\n* Fix overflow when iterating through 0-length lists backwards\n\n## 1.26.7\n\n* Add an option to quiet the progress indicator but not warnings\n* Enable more compiler warnings and fix related problems\n\n## 1.26.6\n\n* Be more careful about checking for overflow when parsing numbers\n\n## 1.26.5\n\n* Support UTF-16 surrogate pairs in JSON strings\n* Support arbitrarily long lines in CSV files.\n* Treat CSV fields as numbers only if they follow JSON number syntax\n\n## 1.26.4\n\n* Array bounds bug fix in binary to decimal conversion library\n\n## 1.26.3\n\n* Guard against impossible coordinates when decoding tilesets\n\n## 1.26.2\n\n* Make sure to encode tile-joined integers as ints, not doubles\n\n## 1.26.1\n\n* Add tile-join option to rename layers\n\n## 1.26.0\n\nFix error when parsing attributes with empty-string keys\n\n## 1.25.0\n\n* Add --coalesce-smallest-as-needed strategy for reducing tile sizes\n* Add --stats option to tipppecanoe-decode\n\n## 1.24.1\n\n* Limit the size and depth of the string pool for better performance\n\n## 1.24.0\n\n* Add feature filters using the Mapbox GL Style Specification filter syntax\n\n## 1.23.0\n\n* Add input support for Geobuf file format\n\n## 1.22.2\n\n* Add better diagnostics for NaN or Infinity in input JSON\n\n## 1.22.1\n\n* Fix tilestats generation when long string attribute values are elided\n* Add option not to produce tilestats\n* Add tile-join options to select zoom levels to copy\n\n## 1.22.0\n\n* Add options to filter each tile's contents through a shell pipeline\n\n## 1.21.0\n\n* Generate layer, feature, and attribute statistics as part of tileset metadata\n\n## 1.20.1\n\n* Close mbtiles file properly when there are no valid features in the input\n\n## 1.20.0\n\n* Add long options to tippecanoe-decode and tile-join. Add --quiet to tile-join.\n\n## 1.19.3\n\n* Upgrade protozero to version 1.5.2\n\n## 1.19.2\n\n* Ignore UTF-8 byte order mark if present\n\n## 1.19.1\n\n* Add an option to increase maxzoom if features are still being dropped\n\n## 1.19.0\n\n* Tile-join can merge and create directories, not only mbtiles\n* Maxzoom guessing (-zg) takes into account resolution within each feature\n\n## 1.18.2\n\n* Fix crash with very long (>128K) attribute values\n\n## 1.18.1\n\n* Only warn once about invalid polygons in tippecanoe-decode\n\n## 1.18.0\n\n* Fix compression of tiles in tile-join\n* Calculate the tileset bounding box in tile-join from the tile boundaries\n\n## 1.17.7\n\n* Enforce polygon winding and closure rules in tippecanoe-decode\n\n## 1.17.6\n\n* Add tile-join options to set name, attribution, description\n\n## 1.17.5\n\n* Preserve the tileset names from the source mbtiles in tile-join\n\n## 1.17.4\n\n* Fix RFC 8142 support: Don't try to split *all* memory mapped files\n\n## 1.17.3\n\n* Support RFC 8142 GeoJSON text sequences\n\n## 1.17.2\n\n* Organize usage output the same way as in the README\n\n## 1.17.1\n\n* Add -T option to coerce the types of feature attributes\n\n## 1.17.0\n\n* Add -zg option to guess an appropriate maxzoom\n\n## 1.16.17\n\n* Clean up JSON parsing at the end of each FeatureCollection\n  to avoid running out of memory\n\n## 1.16.16\n\n* Add tile-join options to include or exclude specific layers\n\n## 1.16.15\n\n* Add --output-to-directory and --no-tile-compression options\n\n## 1.16.14\n\n* Add --description option for mbtiles metadata\n* Clean up some utility functions\n\n## 1.16.13\n\n* Add --detect-longitude-wraparound option\n\n## 1.16.12\n\n* Stop processing higher zooms when a feature reaches its explicit maxzoom tag\n\n## 1.16.11\n\n* Remove polygon splitting, since polygon cleaning is now fast enough\n\n## 1.16.10\n\n* Add a tippecanoe-decode option to specify layer names\n\n## 1.16.9\n\n* Clean up layer name handling to fix layer merging crash\n\n## 1.16.8\n\n* Fix some code that could sometimes try to divide by zero\n* Add check for $TIPPECANOE_MAX_THREADS environmental variable\n\n## 1.16.7\n\n* Fix area of placeholders for degenerate multipolygons\n\n## 1.16.6\n\n* Upgrade Wagyu to 0.3.0; downgrade C++ requirement to C++ 11\n\n## 1.16.5\n\n* Add -z and -Z options to tippecanoe-decode\n\n## 1.16.4\n\n* Use Wagyu's quick_lr_clip() instead of a separate implementation\n\n## 1.16.3\n\n* Upgrade Wagyu to bfbf2893\n\n## 1.16.2\n\n* Associate attributes with the right layer when explicitly tagged\n\n## 1.16.1\n\n* Choose a deeper starting tile than 0/0/0 if there is one that contains\n  all the features\n\n## 1.16.0\n\n* Switch from Clipper to Wagyu for polygon topology correction\n\n## 1.15.4\n\n* Dot-dropping with -r/-B doesn't apply if there is a per-feature minzoom tag\n\n## 1.15.3\n\n* Round coordinates in low-zoom grid math instead of truncating\n\n## 1.15.2\n\n* Add --grid-low-zooms option to snap low-zoom features to the tile grid\n\n## 1.15.1\n\n* Stop --drop-smallest-as-needed from always dropping all points\n\n## 1.15.0\n\n* New strategies for making tiles smaller, with uniform behavior across\n  the whole zoom level: --increase-gamma-as-needed,\n  --drop-densest-as-needed, --drop-fraction-as-needed,\n  --drop-smallest-as-needed.\n* Option to specify the maximum tile size in bytes\n* Option to turn off tiny polygon reduction\n* Better error checking in JSON parsing\n\n## 1.14.4\n\n* Make -B/-r feature-dropping consistent between tiles and zoom levels\n\n## 1.14.3\n\n* Add --detect-shared-borders option for better polygon simplification\n\n## 1.14.2\n\n* Enforce that string feature attributes must be encoded as UTF-8\n\n## 1.14.1\n\n* Whitespace after commas in tile-join .csv input is no longer significant\n\n## 1.14.0\n\n* Tile-join is multithreaded and can merge multiple vector mbtiles files together\n\n## 1.13.0\n\n* Add the ability to specify layer names within the GeoJSON input\n\n## 1.12.11\n\n* Don't try to revive a placeholder for a degenerate polygon that had negative area\n\n## 1.12.10\n\n* Pass feature IDs through in tile-join\n\n## 1.12.9\n\n* Clean up parsing and serialization. Provide some context with parsing errors.\n\n## 1.12.8\n\n* Fix the spelling of the --preserve-input-order option\n\n## 1.12.7\n\n* Support the \"id\" field of GeoJSON objects and vector tile features\n\n## 1.12.6\n\n* Fix error reports when reading from an empty file with parallel input\n\n## 1.12.5\n\n* Add an option to vary the level of line and polygon simplification\n* Be careful not to produce an empty tile if there was a feature with\n  empty geometry.\n\n## 1.12.4\n\n* Be even more careful not to produce features with empty geometry\n\n## 1.12.3\n\n* Fix double-counted progress in the progress indicator\n\n## 1.12.2\n\n* Add ability to specify a projection to tippecanoe-decode\n\n## 1.12.1\n\n* Fix incorrect tile layer version numbers in tile-join output\n\n## 1.12.0\n\n* Fix a tile-join bug that would retain fields that were supposed to be excluded\n\n## 1.11.9\n\n* Add minimal support for alternate input projections (EPSG:3857).\n\n## 1.11.8\n\n* Add an option to calculate the density of features as a feature attribute\n\n## 1.11.7\n\n* Keep metadata together with geometry for features that don't span many tiles,\n  to avoid extra memory load from indexing into a separate metadata file\n\n## 1.11.6\n\n* Reduce the size of critical data structures to reduce dynamic memory use\n\n## 1.11.5\n\n* Let zoom level 0 have just as much extent and buffer as any other zoom\n* Fix tippecanoe-decode bug that would sometimes show outer rings as inner\n\n## 1.11.4\n\n* Don't let polygons with nonzero area disappear during cleaning\n\n## 1.11.3\n\n* Internal code cleanup\n\n## 1.11.2\n\n* Update Clipper to fix potential crash\n\n## 1.11.1\n\n* Make better use of C++ standard libraries\n\n## 1.11.0\n\n* Convert C source files to C++\n\n## 1.10.0\n\n* Upgrade Clipper to fix potential crashes and improve polygon topology\n\n## 1.9.16\n\n* Switch to protozero as the library for reading and writing protocol buffers\n\n## 1.9.15\n\n* Add option not to clip features\n\n## 1.9.14\n\n* Clean up polygons after coalescing, if necessary\n\n## 1.9.13\n\n* Don't trust the OS so much about how many files can be open\n\n## 1.9.12\n\n* Limit the size of the parallel parsing streaming input buffer\n* Add an option to set the tileset's attribution\n\n## 1.9.11\n\n* Fix a line simplification crash when a segment degenerates to a single point\n\n## 1.9.10\n\n* Warn if temporary disk space starts to run low\n\n## 1.9.9\n\n* Add --drop-polygons to drop a fraction of polygons by zoom level\n* Only complain once about failing to clean polygons\n\n## 1.9.8\n\n* Use an on-disk radix sort for the index to control virtual memory thrashing\n  when the geometry and index are too large to fit in memory\n\n## 1.9.7\n\n* Fix build problem (wrong spelling of long long max/min constants)\n\n## 1.9.6\n\n* Add an option to give specific layer names to specific input files\n\n## 1.9.5\n\n* Remove temporary files that were accidentally left behind\n* Be more careful about checking memory allocations and array bounds\n* Add GNU-style long options\n\n## 1.9.4\n\n* Tippecanoe-decode can decode .pbf files that aren't in an .mbtiles container\n\n## 1.9.3\n\n* Don't get stuck in a loop trying to split up very small, very complicated polygons\n\n## 1.9.2\n\n* Increase maximum tile size for tippecanoe-decode\n\n## 1.9.1\n\n* Incorporate Mapnik's Clipper upgrades for consistent results between Mac and Linux\n\n## 1.9.0\n\n* Claim vector tile version 2 in mbtiles\n* Split too-complex polygons into multiple features\n\n## 1.8.1\n\n* Bug fixes to maxzoom, and more tests\n\n## 1.8.0\n\n* There are tests that can be run with \"make test\".\n\n## 1.7.2\n\n* Feature properties that are arrays or hashes get stringified\n  rather than being left out with a warning.\n\n## 1.7.1\n\n* Make clipping behavior with no buffer consistent with Mapnik.\n  Features that are exactly on a tile boundary appear in both tiles.\n\n## 1.7.0\n\n* Parallel processing of input with -P works with streamed input too\n* Error handling if unsupported options given to -p or -a\n\n## 1.6.4\n\n* Fix crashing bug when layers are being merged with -l\n\n## 1.6.3\n\n* Add an option to do line simplification only at zooms below maxzoom\n\n## 1.6.2\n\n* Make sure line simplification matches on opposite sides of a tile boundary\n\n## 1.6.1\n\n* Use multiple threads for line simplification and polygon cleaning\n\n## 1.6.0\n\n* Add option of parallelized input when reading from a line-delimited file\n\n## 1.5.1\n\n* Fix internal error when number of CPUs is not a power of 2\n* Add missing #include\n\n## 1.5.0\n\n* Base zoom for dot-dropping can be specified independently of\n  maxzoom for tiling.\n* Tippecanoe can calculate a base zoom and drop rate for you.\n\n## 1.4.3\n\n* Encode numeric attributes as integers instead of floating point if possible\n\n## 1.4.2\n\n* Bug fix for problem that would occasionally produce empty point geometries\n* More bug fixes for polygon generation\n\n## 1.4.1\n\n* Features that cross the antimeridian are split into two parts instead\n  of being partially lost off the edge\n\n## 1.4.0\n\n* More polygon correctness\n* Query the system for the number of available CPUs instead of guessing\n* Merge input files into one layer if a layer name is specified\n* Document and install tippecanoe-enumerate and tippecanoe-decode\n\n## 1.3.0\n\n* Tile generation is multithreaded to take advantage of multiple CPUs\n* More compact data representation reduces memory usage and improves speed\n* Polygon clipping uses [Clipper](http://www.angusj.com/delphi/clipper/documentation/Docs/_Body.htm)\n  and makes sure interior and exterior rings are distinguished by winding order\n* Individual GeoJSON features can specify their own minzoom and maxzoom\n* New `tile-join` utility can add new properties from a CSV file to an existing tileset\n* Feature coalescing, line-reversing, and reordering by attribute are now options, not defaults\n* Output of `decode` utility is now in GeoJSON format\n* Tile generation with a minzoom spends less time on unused lower zoom levels\n* Bare geometries without a Feature wrapper are accepted\n* Default tile resolution is 4096 units at all zooms since renderers assume it\n\n## 1.2.0\n\n* Switched to top-down rendering, yielding performance improvements\n* Add a dot-density gamma feature to thin out especially dense clusters\n* Add support for multiple layers, making it possible to include more\n  than one GeoJSON featurecollection in a map. [#29](https://github.com/mapbox/tippecanoe/pull/29)\n* Added flags that let you optionally avoid simplifying lines, restricting\n  maximum tile sizes, and coalescing features [#30](https://github.com/mapbox/tippecanoe/pull/30)\n* Added check that minimum zoom level is less than maximum zoom level\n* Added `-v` flag to check tippecanoe's version\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.39453125,
          "content": "# Start from ubuntu\nFROM ubuntu:16.04\n\n# Update repos and install dependencies\nRUN apt-get update \\\n  && apt-get -y upgrade \\\n  && apt-get -y install build-essential libsqlite3-dev zlib1g-dev\n\n# Create a directory and copy in all files\nRUN mkdir -p /tmp/tippecanoe-src\nWORKDIR /tmp/tippecanoe-src\nCOPY . /tmp/tippecanoe-src\n\n# Build tippecanoe\nRUN make \\\n  && make install\n\n# Run the tests\nCMD make test\n"
        },
        {
          "name": "Dockerfile.centos7",
          "type": "blob",
          "size": 0.28515625,
          "content": "FROM centos:7\n\nRUN yum install -y make sqlite-devel zlib-devel bash git gcc-c++\n\n# Create a directory and copy in all files\nRUN mkdir -p /tmp/tippecanoe-src\nWORKDIR /tmp/tippecanoe-src\nCOPY . /tmp/tippecanoe-src\n\n# Build tippecanoe\nRUN make \\\n  && make install\n\n# Run the tests\nCMD make test\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.2646484375,
          "content": "Copyright (c) 2014, Mapbox Inc.  \nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n\n- Redistributions of source code must retain the above copyright notice,\n  this list of conditions and the following disclaimer.\n- Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nHOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED\nTO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "MADE_WITH.md",
          "type": "blob",
          "size": 0.8564453125,
          "content": "## [Visualizing a Month of Lightning](http://rousseau.io/2015/03/23/visualizing-a-month-of-lightning) by Jordan Rousseau\n\n![](http://rousseau.io/assets/img/ltg-studio-style.png)\n\n## [Making the most detailed tweet map ever](https://www.mapbox.com/blog/twitter-map-every-tweet/) by Eric Fischer\n\n![](https://farm8.staticflickr.com/7505/15869589271_8a02e84c24_b.jpg)\n\n## [Superpowering Runkeeper's 1.5 million walks, runs, and bike rides](https://www.mapbox.com/blog/runkeeper-million-routes/)\n\n![](https://c1.staticflickr.com/9/8605/15852245980_1ecf0894b8_b.jpg)\n\n## [The Geotaggers' World Atlas](https://www.mapbox.com/blog/geotaggers-world-atlas/) by Eric Fischer\n\n![](http://farm8.staticflickr.com/7634/17040546408_0a14752e6d_b.jpg)\n\n## [Atmospheric River](https://www.mapbox.com/blog/atmospheric-river/)\n\n![](http://farm9.staticflickr.com/8630/16253097589_4dfc706b22_b.jpg)\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 31.34375,
          "content": "PREFIX ?= /usr/local\nMANDIR ?= $(PREFIX)/share/man/man1/\nBUILDTYPE ?= Release\nSHELL = /bin/bash\n\n# inherit from env if set\nCC := $(CC)\nCXX := $(CXX)\nCFLAGS := $(CFLAGS)\nCXXFLAGS := $(CXXFLAGS) -std=c++11\nLDFLAGS := $(LDFLAGS)\nWARNING_FLAGS := -Wall -Wshadow -Wsign-compare -Wextra -Wunreachable-code -Wuninitialized -Wshadow\nRELEASE_FLAGS := -O3 -DNDEBUG\nDEBUG_FLAGS := -O0 -DDEBUG -fno-inline-functions -fno-omit-frame-pointer\n\nifeq ($(BUILDTYPE),Release)\n\tFINAL_FLAGS := -g $(WARNING_FLAGS) $(RELEASE_FLAGS)\nelse\n\tFINAL_FLAGS := -g $(WARNING_FLAGS) $(DEBUG_FLAGS)\nendif\n\nall: tippecanoe tippecanoe-enumerate tippecanoe-decode tile-join unit tippecanoe-json-tool\n\ndocs: man/tippecanoe.1\n\ninstall: tippecanoe tippecanoe-enumerate tippecanoe-decode tile-join tippecanoe-json-tool\n\tmkdir -p $(PREFIX)/bin\n\tmkdir -p $(MANDIR)\n\tcp tippecanoe $(PREFIX)/bin/tippecanoe\n\tcp tippecanoe-enumerate $(PREFIX)/bin/tippecanoe-enumerate\n\tcp tippecanoe-decode $(PREFIX)/bin/tippecanoe-decode\n\tcp tippecanoe-json-tool $(PREFIX)/bin/tippecanoe-json-tool\n\tcp tile-join $(PREFIX)/bin/tile-join\n\tcp man/tippecanoe.1 $(MANDIR)/tippecanoe.1\n\nuninstall:\n\trm $(PREFIX)/bin/tippecanoe $(PREFIX)/bin/tippecanoe-enumerate $(PREFIX)/bin/tippecanoe-decode $(PREFIX)/bin/tile-join $(MANDIR)/tippecanoe.1 $(PREFIX)/bin/tippecanoe-json-tool\n\nman/tippecanoe.1: README.md\n\tmd2man-roff README.md > man/tippecanoe.1\n\nPG=\n\nH = $(wildcard *.h) $(wildcard *.hpp)\nC = $(wildcard *.c) $(wildcard *.cpp)\n\nINCLUDES = -I/usr/local/include -I.\nLIBS = -L/usr/local/lib\n\ntippecanoe: geojson.o jsonpull/jsonpull.o tile.o pool.o mbtiles.o geometry.o projection.o memfile.o mvt.o serial.o main.o text.o dirtiles.o plugin.o read_json.o write_json.o geobuf.o evaluator.o geocsv.o csv.o geojson-loop.o\n\t$(CXX) $(PG) $(LIBS) $(FINAL_FLAGS) $(CXXFLAGS) -o $@ $^ $(LDFLAGS) -lm -lz -lsqlite3 -lpthread\n\ntippecanoe-enumerate: enumerate.o\n\t$(CXX) $(PG) $(LIBS) $(FINAL_FLAGS) $(CXXFLAGS) -o $@ $^ $(LDFLAGS) -lsqlite3\n\ntippecanoe-decode: decode.o projection.o mvt.o write_json.o text.o jsonpull/jsonpull.o dirtiles.o\n\t$(CXX) $(PG) $(LIBS) $(FINAL_FLAGS) $(CXXFLAGS) -o $@ $^ $(LDFLAGS) -lm -lz -lsqlite3\n\ntile-join: tile-join.o projection.o pool.o mbtiles.o mvt.o memfile.o dirtiles.o jsonpull/jsonpull.o text.o evaluator.o csv.o write_json.o\n\t$(CXX) $(PG) $(LIBS) $(FINAL_FLAGS) $(CXXFLAGS) -o $@ $^ $(LDFLAGS) -lm -lz -lsqlite3 -lpthread\n\ntippecanoe-json-tool: jsontool.o jsonpull/jsonpull.o csv.o text.o geojson-loop.o\n\t$(CXX) $(PG) $(LIBS) $(FINAL_FLAGS) $(CXXFLAGS) -o $@ $^ $(LDFLAGS) -lm -lz -lsqlite3 -lpthread\n\nunit: unit.o text.o\n\t$(CXX) $(PG) $(LIBS) $(FINAL_FLAGS) $(CXXFLAGS) -o $@ $^ $(LDFLAGS) -lm -lz -lsqlite3 -lpthread\n\n-include $(wildcard *.d)\n\n%.o: %.c\n\t$(CC) -MMD $(PG) $(INCLUDES) $(FINAL_FLAGS) $(CFLAGS) -c -o $@ $<\n\n%.o: %.cpp\n\t$(CXX) -MMD $(PG) $(INCLUDES) $(FINAL_FLAGS) $(CXXFLAGS) -c -o $@ $<\n\nclean:\n\trm -f ./tippecanoe ./tippecanoe-* ./tile-join ./unit *.o *.d */*.o */*.d tests/**/*.mbtiles tests/**/*.check\n\nindent:\n\tclang-format -i -style=\"{BasedOnStyle: Google, IndentWidth: 8, UseTab: Always, AllowShortIfStatementsOnASingleLine: false, ColumnLimit: 0, ContinuationIndentWidth: 8, SpaceAfterCStyleCast: true, IndentCaseLabels: false, AllowShortBlocksOnASingleLine: false, AllowShortFunctionsOnASingleLine: false, SortIncludes: false}\" $(C) $(H)\n\nTESTS = $(wildcard tests/*/out/*.json)\nSPACE = $(NULL) $(NULL)\n\ntest: tippecanoe tippecanoe-decode $(addsuffix .check,$(TESTS)) raw-tiles-test parallel-test pbf-test join-test enumerate-test decode-test join-filter-test unit json-tool-test allow-existing-test csv-test layer-json-test\n\t./unit\n\nsuffixes = json json.gz\n\n# Work around Makefile and filename punctuation limits: _ for space, @ for :, % for /\n%.json.check:\n\t./tippecanoe -q -a@ -f -o $@.mbtiles $(subst @,:,$(subst %,/,$(subst _, ,$(patsubst %.json.check,%,$(word 4,$(subst /, ,$@)))))) $(foreach suffix,$(suffixes),$(sort $(wildcard $(subst $(SPACE),/,$(wordlist 1,2,$(subst /, ,$@)))/*.$(suffix)))) < /dev/null\n\t./tippecanoe-decode -x generator $@.mbtiles > $@.out\n\tcmp $@.out $(patsubst %.check,%,$@)\n\trm $@.out $@.mbtiles\n\n# Don't test overflow with geobuf, because it fails (https://github.com/mapbox/geobuf/issues/87)\n# Don't test stringids with geobuf, because it fails\nnogeobuf = tests/overflow/out/-z0.json $(wildcard tests/stringid/out/*.json)\ngeobuf-test: tippecanoe-json-tool $(addsuffix .checkbuf,$(filter-out $(nogeobuf),$(TESTS)))\n\n# For quicker address sanitizer build, hope that regular JSON parsing is tested enough by parallel and join tests\nfewer-tests: tippecanoe tippecanoe-decode geobuf-test raw-tiles-test parallel-test pbf-test join-test enumerate-test decode-test join-filter-test unit\n\n# XXX Use proper makefile rules instead of a for loop\n%.json.checkbuf:\n\tfor i in $(wildcard $(subst $(SPACE),/,$(wordlist 1,2,$(subst /, ,$@)))/*.json); do ./tippecanoe-json-tool -w $$i | ./node_modules/geobuf/bin/json2geobuf > $$i.geobuf; done\n\tfor i in $(wildcard $(subst $(SPACE),/,$(wordlist 1,2,$(subst /, ,$@)))/*.json.gz); do gzip -dc $$i | ./tippecanoe-json-tool -w | ./node_modules/geobuf/bin/json2geobuf > $$i.geobuf; done\n\t./tippecanoe -q -a@ -f -o $@.mbtiles $(subst @,:,$(subst %,/,$(subst _, ,$(patsubst %.json.checkbuf,%,$(word 4,$(subst /, ,$@)))))) $(foreach suffix,$(suffixes),$(addsuffix .geobuf,$(sort $(wildcard $(subst $(SPACE),/,$(wordlist 1,2,$(subst /, ,$@)))/*.$(suffix))))) < /dev/null\n\t./tippecanoe-decode -x generator $@.mbtiles | sed 's/checkbuf/check/g' | sed 's/\\.geobuf//g' > $@.out\n\tcmp $@.out $(patsubst %.checkbuf,%,$@)\n\trm $@.out $@.mbtiles\n\nparallel-test:\n\tmkdir -p tests/parallel\n\tperl -e 'for ($$i = 0; $$i < 20; $$i++) { $$lon = rand(360) - 180; $$lat = rand(180) - 90; $$k = rand(1); $$v = rand(1); print \"{ \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": { \\\"yes\\\": \\\"no\\\", \\\"who\\\": 1, \\\"$$k\\\": \\\"$$v\\\" }, \\\"geometry\\\": { \\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [ $$lon, $$lat ] } }\\n\"; }' > tests/parallel/in1.json\n\tperl -e 'for ($$i = 0; $$i < 300000; $$i++) { $$lon = rand(360) - 180; $$lat = rand(180) - 90; print \"{ \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": { }, \\\"geometry\\\": { \\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [ $$lon, $$lat ] } }\\n\"; }' > tests/parallel/in2.json\n\tperl -e 'for ($$i = 0; $$i < 20; $$i++) { $$lon = rand(360) - 180; $$lat = rand(180) - 90; print \"{ \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": { }, \\\"geometry\\\": { \\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [ $$lon, $$lat ] } }\\n\"; }' > tests/parallel/in3.json\n\tperl -e 'for ($$i = 0; $$i < 20; $$i++) { $$lon = rand(360) - 180; $$lat = rand(180) - 90; $$v = rand(1); print \"{ \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": { }, \\\"tippecanoe\\\": { \\\"layer\\\": \\\"$$v\\\" }, \\\"geometry\\\": { \\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [ $$lon, $$lat ] } }\\n\"; }' > tests/parallel/in4.json\n\techo -n \"\" > tests/parallel/empty1.json\n\techo \"\" > tests/parallel/empty2.json\n\t./tippecanoe -q -z5 -f -pi -l test -n test -o tests/parallel/linear-file.mbtiles tests/parallel/in[1234].json tests/parallel/empty[12].json\n\t./tippecanoe -q -z5 -f -pi -l test -n test -P -o tests/parallel/parallel-file.mbtiles tests/parallel/in[1234].json tests/parallel/empty[12].json\n\tcat tests/parallel/in[1234].json | ./tippecanoe -q -z5 -f -pi -l test -n test -o tests/parallel/linear-pipe.mbtiles\n\tcat tests/parallel/in[1234].json | ./tippecanoe -q -z5 -f -pi -l test -n test -P -o tests/parallel/parallel-pipe.mbtiles\n\tcat tests/parallel/in[1234].json | sed 's/^/@/' | tr '@' '\\036' | ./tippecanoe -q -z5 -f -pi -l test -n test -o tests/parallel/implicit-pipe.mbtiles\n\t./tippecanoe -q -z5 -f -pi -l test -n test -P -o tests/parallel/parallel-pipes.mbtiles <(cat tests/parallel/in1.json) <(cat tests/parallel/empty1.json) <(cat tests/parallel/empty2.json) <(cat tests/parallel/in2.json) /dev/null <(cat tests/parallel/in3.json) <(cat tests/parallel/in4.json)\n\t./tippecanoe-decode -x generator -x generator_options tests/parallel/linear-file.mbtiles > tests/parallel/linear-file.json\n\t./tippecanoe-decode -x generator -x generator_options tests/parallel/parallel-file.mbtiles > tests/parallel/parallel-file.json\n\t./tippecanoe-decode -x generator -x generator_options tests/parallel/linear-pipe.mbtiles > tests/parallel/linear-pipe.json\n\t./tippecanoe-decode -x generator -x generator_options tests/parallel/parallel-pipe.mbtiles > tests/parallel/parallel-pipe.json\n\t./tippecanoe-decode -x generator -x generator_options tests/parallel/implicit-pipe.mbtiles > tests/parallel/implicit-pipe.json\n\t./tippecanoe-decode -x generator -x generator_options tests/parallel/parallel-pipes.mbtiles > tests/parallel/parallel-pipes.json\n\tcmp tests/parallel/linear-file.json tests/parallel/parallel-file.json\n\tcmp tests/parallel/linear-file.json tests/parallel/linear-pipe.json\n\tcmp tests/parallel/linear-file.json tests/parallel/parallel-pipe.json\n\tcmp tests/parallel/linear-file.json tests/parallel/implicit-pipe.json\n\tcmp tests/parallel/linear-file.json tests/parallel/parallel-pipes.json\n\trm tests/parallel/*.mbtiles tests/parallel/*.json\n\nraw-tiles-test:\n\t./tippecanoe -q -f -e tests/raw-tiles/raw-tiles -r1 -pC tests/raw-tiles/hackspots.geojson\n\t./tippecanoe-decode -x generator tests/raw-tiles/raw-tiles > tests/raw-tiles/raw-tiles.json.check\n\tcmp tests/raw-tiles/raw-tiles.json.check tests/raw-tiles/raw-tiles.json\n\t# Test that -z and -Z work in tippecanoe-decode\n\t./tippecanoe-decode -x generator -Z6 -z7 tests/raw-tiles/raw-tiles > tests/raw-tiles/raw-tiles-z67.json.check\n\tcmp tests/raw-tiles/raw-tiles-z67.json.check tests/raw-tiles/raw-tiles-z67.json\n\t# Test that -z and -Z work in tile-join\n\t./tile-join -q -f -Z6 -z7 -e tests/raw-tiles/raw-tiles-z67 tests/raw-tiles/raw-tiles\n\t./tippecanoe-decode -x generator tests/raw-tiles/raw-tiles-z67 > tests/raw-tiles/raw-tiles-z67-join.json.check\n\tcmp tests/raw-tiles/raw-tiles-z67-join.json.check tests/raw-tiles/raw-tiles-z67-join.json\n\trm -rf tests/raw-tiles/raw-tiles tests/raw-tiles/raw-tiles-z67 tests/raw-tiles/raw-tiles.json.check raw-tiles-z67.json.check tests/raw-tiles/raw-tiles-z67-join.json.check\n\t# Test that metadata.json is created even if all features are clipped away\n\t./tippecanoe -q -f -e tests/raw-tiles/nothing tests/raw-tiles/nothing.geojson\n\t./tippecanoe-decode -x generator tests/raw-tiles/nothing > tests/raw-tiles/nothing.json.check\n\tcmp tests/raw-tiles/nothing.json.check tests/raw-tiles/nothing.json\n\trm -r tests/raw-tiles/nothing tests/raw-tiles/nothing.json.check\n\ndecode-test:\n\tmkdir -p tests/muni/decode\n\t./tippecanoe -q -z11 -Z11 -f -o tests/muni/decode/multi.mbtiles tests/muni/*.json\n\t./tippecanoe-decode -x generator -l subway tests/muni/decode/multi.mbtiles > tests/muni/decode/multi.mbtiles.json.check\n\t./tippecanoe-decode -x generator -c tests/muni/decode/multi.mbtiles > tests/muni/decode/multi.mbtiles.pipeline.json.check\n\t./tippecanoe-decode -x generator tests/muni/decode/multi.mbtiles 11 327 791 > tests/muni/decode/multi.mbtiles.onetile.json.check\n\t./tippecanoe-decode -x generator --stats tests/muni/decode/multi.mbtiles > tests/muni/decode/multi.mbtiles.stats.json.check\n\tcmp tests/muni/decode/multi.mbtiles.json.check tests/muni/decode/multi.mbtiles.json\n\tcmp tests/muni/decode/multi.mbtiles.pipeline.json.check tests/muni/decode/multi.mbtiles.pipeline.json\n\tcmp tests/muni/decode/multi.mbtiles.onetile.json.check tests/muni/decode/multi.mbtiles.onetile.json\n\tcmp tests/muni/decode/multi.mbtiles.stats.json.check tests/muni/decode/multi.mbtiles.stats.json\n\trm -f tests/muni/decode/multi.mbtiles.json.check tests/muni/decode/multi.mbtiles tests/muni/decode/multi.mbtiles.pipeline.json.check tests/muni/decode/multi.mbtiles.stats.json.check tests/muni/decode/multi.mbtiles.onetile.json.check\n\npbf-test:\n\t./tippecanoe-decode -x generator tests/pbf/11-328-791.vector.pbf 11 328 791 > tests/pbf/11-328-791.vector.pbf.out\n\tcmp tests/pbf/11-328-791.json tests/pbf/11-328-791.vector.pbf.out\n\trm tests/pbf/11-328-791.vector.pbf.out\n\t./tippecanoe-decode -x generator -s EPSG:3857 tests/pbf/11-328-791.vector.pbf 11 328 791 > tests/pbf/11-328-791.3857.vector.pbf.out\n\tcmp tests/pbf/11-328-791.3857.json tests/pbf/11-328-791.3857.vector.pbf.out\n\trm tests/pbf/11-328-791.3857.vector.pbf.out\n\nenumerate-test:\n\t./tippecanoe -q -z5 -f -o tests/ne_110m_admin_0_countries/out/enum.mbtiles tests/ne_110m_admin_0_countries/in.json.gz\n\t./tippecanoe-enumerate tests/ne_110m_admin_0_countries/out/enum.mbtiles > tests/ne_110m_admin_0_countries/out/enum.check\n\tcmp tests/ne_110m_admin_0_countries/out/enum tests/ne_110m_admin_0_countries/out/enum.check\n\trm tests/ne_110m_admin_0_countries/out/enum.mbtiles tests/ne_110m_admin_0_countries/out/enum.check\n\njoin-test: tile-join\n\t./tippecanoe -q -f -z12 -o tests/join-population/tabblock_06001420.mbtiles -YALAND10:'Land area' -L'{\"file\": \"tests/join-population/tabblock_06001420.json\", \"description\": \"population\"}'\n\t./tippecanoe -q -f -Z5 -z10 -o tests/join-population/macarthur.mbtiles -l macarthur tests/join-population/macarthur.json\n\t./tile-join -q -f -Z6 -z9 -o tests/join-population/macarthur-6-9.mbtiles tests/join-population/macarthur.mbtiles\n\t./tippecanoe-decode -x generator tests/join-population/macarthur-6-9.mbtiles > tests/join-population/macarthur-6-9.mbtiles.json.check\n\tcmp tests/join-population/macarthur-6-9.mbtiles.json.check tests/join-population/macarthur-6-9.mbtiles.json\n\t./tile-join -q -f -Z6 -z9 -X -o tests/join-population/macarthur-6-9-exclude.mbtiles tests/join-population/macarthur.mbtiles\n\t./tippecanoe-decode -x generator tests/join-population/macarthur-6-9-exclude.mbtiles > tests/join-population/macarthur-6-9-exclude.mbtiles.json.check\n\tcmp tests/join-population/macarthur-6-9-exclude.mbtiles.json.check tests/join-population/macarthur-6-9-exclude.mbtiles.json\n\trm -f tests/join-population/macarthur-6-9.mbtiles.json.check tests/join-population/macarthur-6-9.mbtiles tests/join-population/macarthur-6-9-exclude.mbtiles.json.check tests/join-population/macarthur-6-9-exclude.mbtiles\n\t./tippecanoe -q -f -d10 -D10 -Z9 -z11 -o tests/join-population/macarthur2.mbtiles -l macarthur tests/join-population/macarthur2.json\n\t./tile-join --quiet --force -o tests/join-population/joined.mbtiles -x GEOID10 -c tests/join-population/population.csv tests/join-population/tabblock_06001420.mbtiles\n\t./tile-join --quiet --force -o tests/join-population/joined-null.mbtiles --empty-csv-columns-are-null -x GEOID10 -c tests/join-population/population.csv tests/join-population/tabblock_06001420.mbtiles\n\t./tile-join --quiet --force --no-tile-stats -o tests/join-population/joined-no-tile-stats.mbtiles -x GEOID10 -c tests/join-population/population.csv tests/join-population/tabblock_06001420.mbtiles\n\t./tile-join -q -f -i -o tests/join-population/joined-i.mbtiles -x GEOID10 -c tests/join-population/population.csv tests/join-population/tabblock_06001420.mbtiles\n\t./tile-join -q -f -o tests/join-population/merged.mbtiles tests/join-population/tabblock_06001420.mbtiles tests/join-population/macarthur.mbtiles tests/join-population/macarthur2.mbtiles\n\t./tile-join -q -f -c tests/join-population/windows.csv -o tests/join-population/windows.mbtiles tests/join-population/macarthur.mbtiles\n\t./tippecanoe-decode -x generator --maximum-zoom=11 --minimum-zoom=4 tests/join-population/joined.mbtiles > tests/join-population/joined.mbtiles.json.check\n\t./tippecanoe-decode -x generator --maximum-zoom=11 --minimum-zoom=4 tests/join-population/joined-null.mbtiles > tests/join-population/joined-null.mbtiles.json.check\n\t./tippecanoe-decode -x generator --maximum-zoom=11 --minimum-zoom=4 tests/join-population/joined-no-tile-stats.mbtiles > tests/join-population/joined-no-tile-stats.mbtiles.json.check\n\t./tippecanoe-decode -x generator tests/join-population/joined-i.mbtiles > tests/join-population/joined-i.mbtiles.json.check\n\t./tippecanoe-decode -x generator tests/join-population/merged.mbtiles > tests/join-population/merged.mbtiles.json.check\n\t./tippecanoe-decode -x generator tests/join-population/windows.mbtiles > tests/join-population/windows.mbtiles.json.check\n\tcmp tests/join-population/joined.mbtiles.json.check tests/join-population/joined.mbtiles.json\n\tcmp tests/join-population/joined-null.mbtiles.json.check tests/join-population/joined-null.mbtiles.json\n\tcmp tests/join-population/joined-no-tile-stats.mbtiles.json.check tests/join-population/joined-no-tile-stats.mbtiles.json\n\tcmp tests/join-population/joined-i.mbtiles.json.check tests/join-population/joined-i.mbtiles.json\n\tcmp tests/join-population/merged.mbtiles.json.check tests/join-population/merged.mbtiles.json\n\tcmp tests/join-population/windows.mbtiles.json.check tests/join-population/windows.mbtiles.json\n\trm -f tests/join-population/joined-null.mbtiles tests/join-population/joined-null.mbtiles.json.check\n\t./tile-join -q -f -l macarthur -n \"macarthur name\" -N \"macarthur description\" -A \"macarthur's attribution\" -o tests/join-population/just-macarthur.mbtiles tests/join-population/merged.mbtiles\n\t./tile-join -q -f -L macarthur -o tests/join-population/no-macarthur.mbtiles tests/join-population/merged.mbtiles\n\t./tippecanoe-decode -x generator tests/join-population/just-macarthur.mbtiles > tests/join-population/just-macarthur.mbtiles.json.check\n\t./tippecanoe-decode -x generator tests/join-population/no-macarthur.mbtiles > tests/join-population/no-macarthur.mbtiles.json.check\n\tcmp tests/join-population/just-macarthur.mbtiles.json.check tests/join-population/just-macarthur.mbtiles.json\n\tcmp tests/join-population/no-macarthur.mbtiles.json.check tests/join-population/no-macarthur.mbtiles.json\n\t./tile-join -q --no-tile-compression -f -e tests/join-population/raw-merged-folder tests/join-population/tabblock_06001420.mbtiles tests/join-population/macarthur.mbtiles tests/join-population/macarthur2.mbtiles\n\t./tippecanoe-decode -x generator tests/join-population/raw-merged-folder > tests/join-population/raw-merged-folder.json.check\n\tcmp tests/join-population/raw-merged-folder.json.check tests/join-population/raw-merged-folder.json\n\trm -f tests/join-population/raw-merged-folder.json.check\n\t./tippecanoe -q -z12 -f -e tests/join-population/tabblock_06001420-folder -YALAND10:'Land area' -L'{\"file\": \"tests/join-population/tabblock_06001420.json\", \"description\": \"population\"}'\n\t./tippecanoe -q -Z5 -z10 -f -e tests/join-population/macarthur-folder -l macarthur tests/join-population/macarthur.json\n\t./tippecanoe -q -d10 -D10 -Z9 -z11 -f -e tests/join-population/macarthur2-folder -l macarthur tests/join-population/macarthur2.json\n\t./tile-join -q -f -o tests/join-population/merged-folder.mbtiles tests/join-population/tabblock_06001420-folder tests/join-population/macarthur-folder tests/join-population/macarthur2-folder\n\t./tippecanoe-decode -x generator tests/join-population/merged-folder.mbtiles > tests/join-population/merged-folder.mbtiles.json.check\n\tcmp tests/join-population/merged-folder.mbtiles.json.check tests/join-population/merged-folder.mbtiles.json\n\t./tile-join -q -n \"merged name\" -N \"merged description\" -f -e tests/join-population/merged-mbtiles-to-folder tests/join-population/tabblock_06001420.mbtiles tests/join-population/macarthur.mbtiles tests/join-population/macarthur2.mbtiles\n\t./tile-join -q -n \"merged name\" -N \"merged description\" -f -e tests/join-population/merged-folders-to-folder tests/join-population/tabblock_06001420-folder tests/join-population/macarthur-folder tests/join-population/macarthur2-folder\n\t./tippecanoe-decode -x generator -x generator_options tests/join-population/merged-mbtiles-to-folder > tests/join-population/merged-mbtiles-to-folder.json.check\n\t./tippecanoe-decode -x generator -x generator_options tests/join-population/merged-folders-to-folder > tests/join-population/merged-folders-to-folder.json.check\n\tcmp tests/join-population/merged-mbtiles-to-folder.json.check tests/join-population/merged-folders-to-folder.json.check\n\trm -f tests/join-population/merged-mbtiles-to-folder.json.check tests/join-population/merged-folders-to-folder.json.check\n\t./tile-join -q -f -c tests/join-population/windows.csv -o tests/join-population/windows-merged.mbtiles tests/join-population/macarthur.mbtiles tests/join-population/macarthur2-folder\n\t./tile-join -q -c tests/join-population/windows.csv -f -e tests/join-population/windows-merged-folder tests/join-population/macarthur.mbtiles tests/join-population/macarthur2-folder\n\t./tile-join -q -f -o tests/join-population/windows-merged2.mbtiles tests/join-population/windows-merged-folder\n\t./tippecanoe-decode -x generator -x generator_options tests/join-population/windows-merged.mbtiles > tests/join-population/windows-merged.mbtiles.json.check\n\t./tippecanoe-decode -x generator -x generator_options tests/join-population/windows-merged2.mbtiles > tests/join-population/windows-merged2.mbtiles.json.check\n\tcmp tests/join-population/windows-merged.mbtiles.json.check tests/join-population/windows-merged2.mbtiles.json.check\n\t./tile-join -q -f -o tests/join-population/macarthur-and-macarthur2-merged.mbtiles tests/join-population/macarthur.mbtiles tests/join-population/macarthur2-folder\n\t./tile-join -q -f -e tests/join-population/macarthur-and-macarthur2-folder tests/join-population/macarthur.mbtiles tests/join-population/macarthur2-folder\n\t./tile-join -q -f -o tests/join-population/macarthur-and-macarthur2-merged2.mbtiles tests/join-population/macarthur-and-macarthur2-folder\n\t./tippecanoe-decode -x generator -x generator_options tests/join-population/macarthur-and-macarthur2-merged.mbtiles > tests/join-population/macarthur-and-macarthur2-merged.mbtiles.json.check\n\t./tippecanoe-decode -x generator -x generator_options tests/join-population/macarthur-and-macarthur2-merged2.mbtiles > tests/join-population/macarthur-and-macarthur2-merged2.mbtiles.json.check\n\tcmp tests/join-population/macarthur-and-macarthur2-merged.mbtiles.json.check tests/join-population/macarthur-and-macarthur2-merged2.mbtiles.json.check\n\trm tests/join-population/tabblock_06001420.mbtiles tests/join-population/joined.mbtiles tests/join-population/joined-i.mbtiles tests/join-population/joined.mbtiles.json.check tests/join-population/joined-i.mbtiles.json.check tests/join-population/macarthur.mbtiles tests/join-population/merged.mbtiles tests/join-population/merged.mbtiles.json.check  tests/join-population/merged-folder.mbtiles tests/join-population/macarthur2.mbtiles tests/join-population/windows.mbtiles tests/join-population/windows-merged.mbtiles tests/join-population/windows-merged2.mbtiles tests/join-population/windows.mbtiles.json.check tests/join-population/just-macarthur.mbtiles tests/join-population/no-macarthur.mbtiles tests/join-population/just-macarthur.mbtiles.json.check tests/join-population/no-macarthur.mbtiles.json.check tests/join-population/merged-folder.mbtiles.json.check tests/join-population/windows-merged.mbtiles.json.check tests/join-population/windows-merged2.mbtiles.json.check tests/join-population/macarthur-and-macarthur2-merged.mbtiles tests/join-population/macarthur-and-macarthur2-merged2.mbtiles tests/join-population/macarthur-and-macarthur2-merged.mbtiles.json.check tests/join-population/macarthur-and-macarthur2-merged2.mbtiles.json.check\n\trm -rf tests/join-population/raw-merged-folder tests/join-population/tabblock_06001420-folder tests/join-population/macarthur-folder tests/join-population/macarthur2-folder tests/join-population/merged-mbtiles-to-folder tests/join-population/merged-folders-to-folder tests/join-population/windows-merged-folder tests/join-population/macarthur-and-macarthur2-folder\n\t# Test renaming of layers\n\t./tippecanoe -q -f -Z5 -z10 -o tests/join-population/macarthur.mbtiles -l macarthur1 tests/join-population/macarthur.json\n\t./tippecanoe -q -f -Z5 -z10 -o tests/join-population/macarthur2.mbtiles -l macarthur2 tests/join-population/macarthur2.json\n\t./tile-join -q -R macarthur1:one --rename-layer=macarthur2:two -f -o tests/join-population/renamed.mbtiles tests/join-population/macarthur.mbtiles tests/join-population/macarthur2.mbtiles\n\t./tippecanoe-decode -x generator tests/join-population/renamed.mbtiles > tests/join-population/renamed.mbtiles.json.check\n\tcmp tests/join-population/renamed.mbtiles.json.check tests/join-population/renamed.mbtiles.json\n\trm -f tests/join-population/renamed.mbtiles.json.check tests/join-population/renamed.mbtiles.json.check tests/join-population/macarthur.mbtiles tests/join-population/macarthur2.mbtiles\n\t# Make sure the concatenated name isn't too long\n\t./tippecanoe -q -f -z0 -n 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ' -o tests/join-population/macarthur.mbtiles tests/join-population/macarthur.json\n\t./tile-join -f -o tests/join-population/concat.mbtiles tests/join-population/macarthur.mbtiles tests/join-population/macarthur.mbtiles tests/join-population/macarthur.mbtiles tests/join-population/macarthur.mbtiles tests/join-population/macarthur.mbtiles tests/join-population/macarthur.mbtiles\n\t./tippecanoe-decode -x generator tests/join-population/concat.mbtiles > tests/join-population/concat.mbtiles.json.check\n\tcmp tests/join-population/concat.mbtiles.json.check tests/join-population/concat.mbtiles.json\n\trm tests/join-population/concat.mbtiles.json.check tests/join-population/concat.mbtiles tests/join-population/macarthur.mbtiles\n\njoin-filter-test:\n\t# Comes out different from the direct tippecanoe run because null attributes are lost\n\t./tippecanoe -q -z0 -f -o tests/feature-filter/out/all.mbtiles tests/feature-filter/in.json\n\t./tile-join -q -J tests/feature-filter/filter -f -o tests/feature-filter/out/filtered.mbtiles tests/feature-filter/out/all.mbtiles\n\t./tippecanoe-decode -x generator tests/feature-filter/out/filtered.mbtiles > tests/feature-filter/out/filtered.json.check\n\tcmp tests/feature-filter/out/filtered.json.check tests/feature-filter/out/filtered.json.standard\n\trm -f tests/feature-filter/out/filtered.json.check tests/feature-filter/out/filtered.mbtiles tests/feature-filter/out/all.mbtiles\n\t# Test zoom level filtering\n\t./tippecanoe -q -r1 -z8 -f -o tests/feature-filter/out/places.mbtiles tests/ne_110m_populated_places/in.json\n\t./tile-join -q -J tests/feature-filter/places-filter -f -o tests/feature-filter/out/places-filter.mbtiles tests/feature-filter/out/places.mbtiles\n\t./tippecanoe-decode -x generator tests/feature-filter/out/places-filter.mbtiles > tests/feature-filter/out/places-filter.mbtiles.json.check\n\tcmp tests/feature-filter/out/places-filter.mbtiles.json.check tests/feature-filter/out/places-filter.mbtiles.json.standard\n\trm -f tests/feature-filter/out/places.mbtiles tests/feature-filter/out/places-filter.mbtiles tests/feature-filter/out/places-filter.mbtiles.json.check\n\njson-tool-test: tippecanoe-json-tool\n\t./tippecanoe-json-tool -e GEOID10 tests/join-population/tabblock_06001420.json | sort > tests/join-population/tabblock_06001420.json.sort\n\t./tippecanoe-json-tool -c tests/join-population/population.csv tests/join-population/tabblock_06001420.json.sort > tests/join-population/tabblock_06001420.json.sort.joined\n\t./tippecanoe-json-tool --empty-csv-columns-are-null -c tests/join-population/population.csv tests/join-population/tabblock_06001420.json.sort > tests/join-population/tabblock_06001420-null.json.sort.joined\n\tcmp tests/join-population/tabblock_06001420.json.sort.joined tests/join-population/tabblock_06001420.json.sort.joined.standard\n\tcmp tests/join-population/tabblock_06001420-null.json.sort.joined tests/join-population/tabblock_06001420-null.json.sort.joined.standard\n\trm -f tests/join-population/tabblock_06001420.json.sort tests/join-population/tabblock_06001420.json.sort.joined\n\trm -f tests/join-population/tabblock_06001420-null.json.sort.joined\n\nallow-existing-test:\n\t# Make a tileset\n\t./tippecanoe -q -Z0 -z0 -f -o tests/allow-existing/both.mbtiles tests/coalesce-tract/tl_2010_06001_tract10.json\n\t# Writing to existing should fail\n\tif ./tippecanoe -q -Z1 -z1 -o tests/allow-existing/both.mbtiles tests/coalesce-tract/tl_2010_06001_tract10.json; then exit 1; else exit 0; fi\n\t# Replace existing\n\t./tippecanoe -q -Z8 -z9 -f -o tests/allow-existing/both.mbtiles tests/coalesce-tract/tl_2010_06001_tract10.json\n\t./tippecanoe -q -Z10 -z11 -F -o tests/allow-existing/both.mbtiles tests/coalesce-tract/tl_2010_06001_tract10.json\n\t./tippecanoe-decode -x generator -x generator_options tests/allow-existing/both.mbtiles > tests/allow-existing/both.mbtiles.json.check\n\tcmp tests/allow-existing/both.mbtiles.json.check tests/allow-existing/both.mbtiles.json\n\t# Make a tileset\n\t./tippecanoe -q -Z0 -z0 -f -e tests/allow-existing/both.dir tests/coalesce-tract/tl_2010_06001_tract10.json\n\t# Writing to existing should fail\n\tif ./tippecanoe -q -Z1 -z1 -e tests/allow-existing/both.dir tests/coalesce-tract/tl_2010_06001_tract10.json; then exit 1; else exit 0; fi\n\t# Replace existing\n\t./tippecanoe -q -Z8 -z9 -f -e tests/allow-existing/both.dir tests/coalesce-tract/tl_2010_06001_tract10.json\n\t./tippecanoe -q -Z10 -z11 -F -e tests/allow-existing/both.dir tests/coalesce-tract/tl_2010_06001_tract10.json\n\t./tippecanoe-decode -x generator -x generator_options tests/allow-existing/both.dir | sed 's/both\\.dir/both.mbtiles/g' > tests/allow-existing/both.dir.json.check\n\tcmp tests/allow-existing/both.dir.json.check tests/allow-existing/both.mbtiles.json\n\trm -r tests/allow-existing/both.dir.json.check tests/allow-existing/both.dir tests/allow-existing/both.mbtiles.json.check tests/allow-existing/both.mbtiles\n\ncsv-test:\n\t# Reading from named CSV\n\t./tippecanoe -q -zg -f -o tests/csv/out.mbtiles tests/csv/ne_110m_populated_places_simple.csv\n\t./tippecanoe-decode -x generator -x generator_options tests/csv/out.mbtiles > tests/csv/out.mbtiles.json.check\n\tcmp tests/csv/out.mbtiles.json.check tests/csv/out.mbtiles.json\n\trm -f tests/csv/out.mbtiles.json.check tests/csv/out.mbtiles\n\t# Reading from named CSV, with nulls\n\t./tippecanoe -q --empty-csv-columns-are-null -zg -f -o tests/csv/out-null.mbtiles tests/csv/ne_110m_populated_places_simple.csv\n\t./tippecanoe-decode -x generator tests/csv/out-null.mbtiles > tests/csv/out-null.mbtiles.json.check\n\tcmp tests/csv/out-null.mbtiles.json.check tests/csv/out-null.mbtiles.json\n\trm -f tests/csv/out-null.mbtiles.json.check tests/csv/out-null.mbtiles\n\t# Same, but specifying csv with -L format\n\t./tippecanoe -q -zg -f -o tests/csv/out.mbtiles -L'{\"file\":\"\", \"format\":\"csv\", \"layer\":\"ne_110m_populated_places_simple\"}' < tests/csv/ne_110m_populated_places_simple.csv\n\t./tippecanoe-decode -x generator -x generator_options tests/csv/out.mbtiles > tests/csv/out.mbtiles.json.check\n\tcmp tests/csv/out.mbtiles.json.check tests/csv/out.mbtiles.json\n\trm -f tests/csv/out.mbtiles.json.check tests/csv/out.mbtiles\n\nlayer-json-test:\n\t# GeoJSON with description and named layer\n\t./tippecanoe -q -z0 -r1 -yNAME -f -o tests/layer-json/out.mbtiles -L'{\"file\":\"tests/ne_110m_populated_places/in.json\", \"description\":\"World cities\", \"layer\":\"places\"}'\n\t./tippecanoe-decode -x generator -x generator_options tests/layer-json/out.mbtiles > tests/layer-json/out.mbtiles.json.check\n\tcmp tests/layer-json/out.mbtiles.json.check tests/layer-json/out.mbtiles.json\n\trm -f tests/layer-json/out.mbtiles.json.check tests/layer-json/out.mbtiles\n\t# Same, but reading from the standard input\n\t./tippecanoe -q -z0 -r1 -yNAME -f -o tests/layer-json/out.mbtiles -L'{\"file\":\"\", \"description\":\"World cities\", \"layer\":\"places\"}' < tests/ne_110m_populated_places/in.json\n\t./tippecanoe-decode -x generator -x generator_options tests/layer-json/out.mbtiles > tests/layer-json/out.mbtiles.json.check\n\tcmp tests/layer-json/out.mbtiles.json.check tests/layer-json/out.mbtiles.json\n\trm -f tests/layer-json/out.mbtiles.json.check tests/layer-json/out.mbtiles\n\n# Use this target to regenerate the standards that the tests are compared against\n# after making a change that legitimately changes their output\n\nprep-test: $(TESTS)\n\ntests/%.json: Makefile tippecanoe tippecanoe-decode\n\t./tippecanoe -q -a@ -f -o $@.check.mbtiles $(subst @,:,$(subst %,/,$(subst _, ,$(patsubst %.json,%,$(word 4,$(subst /, ,$@)))))) $(foreach suffix,$(suffixes),$(sort $(wildcard $(subst $(SPACE),/,$(wordlist 1,2,$(subst /, ,$@)))/*.$(suffix))))\n\t./tippecanoe-decode -x generator $@.check.mbtiles > $@\n\tcmp $(patsubst %.check,%,$@) $@\n\trm $@.check.mbtiles\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 56.0849609375,
          "content": "tippecanoe\n==========\n\n**Note**: there is an active fork of this project over at https://github.com/felt/tippecanoe\n\nBuilds [vector tilesets](https://www.mapbox.com/developers/vector-tiles/) from large (or small) collections of [GeoJSON](http://geojson.org/), [Geobuf](https://github.com/mapbox/geobuf), or [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) features,\n[like these](MADE_WITH.md).\n\n![Mapbox Tippecanoe](https://user-images.githubusercontent.com/1951835/36568734-ede27ec0-17df-11e8-8c22-ffaaebb8daf4.JPG)\n\n[![Build Status](https://travis-ci.org/mapbox/tippecanoe.svg)](https://travis-ci.org/mapbox/tippecanoe)\n[![Coverage Status](https://codecov.io/gh/mapbox/tippecanoe/branch/master/graph/badge.svg)](https://codecov.io/gh/mapbox/tippecanoe)\n\n### :zap: Mapbox has a new service for creating vector tilesets! :zap:\n\n[Mapbox Tiling Service (MTS)](https://docs.mapbox.com/mapbox-tiling-service/overview/) is a hosted, data processing service that allows you to integrate custom datasets of any scale into your maps faster, cheaper, and with more flexibility and control than previously possible.\n\nMTS is the same service we use internally to create our global, daily updating basemap product Mapbox Streets, which serves over 650 million monthly active users and customers such as Facebook, Snap, the Weather Channel, Tableau, and Shopify. \n\nMTS creates and updates data using distributed and parallelized processing, meaning data is processed much more quickly than is possible with a standard, single server setup with comparable tools. For example, a global basemap at 30cm precision (max zoom of 16) can be processed in under 2 hours with MTS, whereas a comparable workload would take multiple days to process on a single server.\n\nCustomers like AllTrails, Plume Labs, and Ookla have noted that MTS helps them:\n- build applications faster by focusing more on app development, not infrastructure\n- build more compelling user experiences that drive better user engagement\n- get updated data to their users faster—in some cases up to 90% faster than previous tools\n\nLearn more about [MTS](https://blog.mapbox.com/introducing-mapbox-tiling-service-df1df636c7cf).\n\nIntent\n------\n\nThe goal of Tippecanoe is to enable making a scale-independent view of your data,\nso that at any level from the entire world to a single building, you can see\nthe density and texture of the data rather than a simplification from dropping\nsupposedly unimportant features or clustering or aggregating them.\n\nIf you give it all of OpenStreetMap and zoom out, it should give you back\nsomething that looks like \"[All Streets](http://benfry.com/allstreets/map5.html)\"\nrather than something that looks like an Interstate road atlas.\n\nIf you give it all the building footprints in Los Angeles and zoom out\nfar enough that most individual buildings are no longer discernable, you\nshould still be able to see the extent and variety of development in every neighborhood,\nnot just the largest downtown buildings.\n\nIf you give it a collection of years of tweet locations, you should be able to\nsee the shape and relative popularity of every point of interest and every\nsignificant travel corridor.\n\nInstallation\n------------\n\nThe easiest way to install tippecanoe on OSX is with [Homebrew](http://brew.sh/):\n\n```sh\n$ brew install tippecanoe\n```\n\nOn Ubuntu it will usually be easiest to build from the source repository:\n\n```sh\n$ git clone https://github.com/mapbox/tippecanoe.git\n$ cd tippecanoe\n$ make -j\n$ make install\n```\n\nSee [Development](#development) below for how to upgrade your\nC++ compiler or install prerequisite packages if you get\ncompiler errors.\n\nUsage\n-----\n\n```sh\n$ tippecanoe -o file.mbtiles [options] [file.json file.json.gz file.geobuf ...]\n```\n\nIf no files are specified, it reads GeoJSON from the standard input.\nIf multiple files are specified, each is placed in its own layer.\n\nThe GeoJSON features need not be wrapped in a FeatureCollection.\nYou can concatenate multiple GeoJSON features or files together,\nand it will parse out the features and ignore whatever other objects\nit encounters.\n\nTry this first\n--------------\n\nIf you aren't sure what options to use, try this:\n\n```sh\n$ tippecanoe -zg -o out.mbtiles --drop-densest-as-needed in.geojson\n```\n\nThe `-zg` option will make Tippecanoe choose a maximum zoom level that should be\nhigh enough to reflect the precision of the original data. (If it turns out still\nnot to be as detailed as you want, use `-z` manually with a higher number.)\n\nIf the tiles come out too big, the `--drop-densest-as-needed` option will make\nTippecanoe try dropping what should be the least visible features at each zoom level.\n(If it drops too many features, use `-x` to leave out some feature attributes that\nyou didn't really need.)\n\nExamples\n--------\n\nCreate a tileset of TIGER roads for Alameda County, to zoom level 13, with a custom layer name and description:\n\n```sh\n$ tippecanoe -o alameda.mbtiles -l alameda -n \"Alameda County from TIGER\" -z13 tl_2014_06001_roads.json\n```\n\nCreate a tileset of all TIGER roads, at only zoom level 12, but with higher detail than normal,\nwith a custom layer name and description, and leaving out the `LINEARID` and `RTTYP` attributes:\n\n```\n$ cat tiger/tl_2014_*_roads.json | tippecanoe -o tiger.mbtiles -l roads -n \"All TIGER roads, one zoom\" -z12 -Z12 -d14 -x LINEARID -x RTTYP\n```\n\nCookbook\n--------\n\n### Linear features (world railroads), visible at all zoom levels\n\n```\ncurl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_railroads.zip\nunzip ne_10m_railroads.zip\nogr2ogr -f GeoJSON ne_10m_railroads.geojson ne_10m_railroads.shp\n\ntippecanoe -zg -o ne_10m_railroads.mbtiles --drop-densest-as-needed --extend-zooms-if-still-dropping ne_10m_railroads.geojson\n```\n\n* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature\n* `--drop-densest-as-needed`: If the tiles are too big at low zoom levels, drop the least-visible features to allow tiles to be created with those features that remain\n* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features\n\n### Discontinuous polygon features (buildings of Rhode Island), visible at all zoom levels\n\n```\ncurl -L -O https://usbuildingdata.blob.core.windows.net/usbuildings-v1-1/RhodeIsland.zip\nunzip RhodeIsland.zip\n\ntippecanoe -zg -o RhodeIsland.mbtiles --drop-densest-as-needed --extend-zooms-if-still-dropping RhodeIsland.geojson\n```\n\n* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature\n* `--drop-densest-as-needed`: If the tiles are too big at low or medium zoom levels, drop the least-visible features to allow tiles to be created with those features that remain\n* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features\n\n### Continuous polygon features (states and provinces), visible at all zoom levels\n\n```\ncurl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_1_states_provinces.zip\nunzip -o ne_10m_admin_1_states_provinces.zip\nogr2ogr -f GeoJSON ne_10m_admin_1_states_provinces.geojson ne_10m_admin_1_states_provinces.shp\n\ntippecanoe -zg -o ne_10m_admin_1_states_provinces.mbtiles --coalesce-densest-as-needed --extend-zooms-if-still-dropping ne_10m_admin_1_states_provinces.geojson\n```\n\n* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature\n* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished\n* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features\n\n### Large point dataset (GPS bus locations), for visualization at all zoom levels\n\n```\ncurl -L -O ftp://avl-data.sfmta.com/avl_data/avl_raw/sfmtaAVLRawData01012013.csv\nsed 's/PREDICTABLE.*/PREDICTABLE/' sfmtaAVLRawData01012013.csv > sfmta.csv\ntippecanoe -zg -o sfmta.mbtiles --drop-densest-as-needed --extend-zooms-if-still-dropping sfmta.csv\n```\n\n(The `sed` line is to clean the corrupt CSV header, which contains the wrong number of fields.)\n\n* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature\n* `--drop-densest-as-needed`: If the tiles are too big at low or medium zoom levels, drop the least-visible features to allow tiles to be created with those features that remain\n* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features\n\n### Clustered points (world cities), summing the clustered population, visible at all zoom levels\n\n```\ncurl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_populated_places.zip\nunzip -o ne_10m_populated_places.zip\nogr2ogr -f GeoJSON ne_10m_populated_places.geojson ne_10m_populated_places.shp\n\ntippecanoe -zg -o ne_10m_populated_places.mbtiles -r1 --cluster-distance=10 --accumulate-attribute=POP_MAX:sum ne_10m_populated_places.geojson\n```\n\n* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature\n* `-r1`: Do not automatically drop a fraction of points at low zoom levels, since clustering will be used instead\n* `--cluster-distance=10`: Cluster together features that are closer than about 10 pixels from each other\n* `--accumulate-attribute=POP_MAX:sum`: Sum the `POP_MAX` (population) attribute in features that are clustered together. Other attributes will be arbitrarily taken from the first feature in the cluster.\n\n### Show countries at low zoom levels but states at higher zoom levels\n\n```\ncurl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries.zip\nunzip ne_10m_admin_0_countries.zip\nogr2ogr -f GeoJSON ne_10m_admin_0_countries.geojson ne_10m_admin_0_countries.shp\n\ncurl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_1_states_provinces.zip\nunzip -o ne_10m_admin_1_states_provinces.zip\nogr2ogr -f GeoJSON ne_10m_admin_1_states_provinces.geojson ne_10m_admin_1_states_provinces.shp\n\ntippecanoe -z3 -o countries-z3.mbtiles --coalesce-densest-as-needed ne_10m_admin_0_countries.geojson\ntippecanoe -zg -Z4 -o states-Z4.mbtiles --coalesce-densest-as-needed --extend-zooms-if-still-dropping ne_10m_admin_1_states_provinces.geojson\ntile-join -o states-countries.mbtiles countries-z3.mbtiles states-Z4.mbtiles\n```\n\nCountries:\n\n* `-z3`: Only generate zoom levels 0 through 3\n* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished\n\nStates and Provinces:\n\n* `-Z4`: Only generate zoom levels 4 and beyond\n* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature\n* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished\n* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features\n\n### Represent multiple sources (Illinois and Indiana counties) as separate layers\n\n```\ncurl -L -O https://www2.census.gov/geo/tiger/TIGER2010/COUNTY/2010/tl_2010_17_county10.zip\nunzip tl_2010_17_county10.zip\nogr2ogr -f GeoJSON tl_2010_17_county10.geojson tl_2010_17_county10.shp\n\ncurl -L -O https://www2.census.gov/geo/tiger/TIGER2010/COUNTY/2010/tl_2010_18_county10.zip\nunzip tl_2010_18_county10.zip\nogr2ogr -f GeoJSON tl_2010_18_county10.geojson tl_2010_18_county10.shp\n\ntippecanoe -zg -o counties-separate.mbtiles --coalesce-densest-as-needed --extend-zooms-if-still-dropping tl_2010_17_county10.geojson tl_2010_18_county10.geojson\n```\n\n* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature\n* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished\n* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features\n\n### Merge multiple sources (Illinois and Indiana counties) into the same layer\n\n```\ncurl -L -O https://www2.census.gov/geo/tiger/TIGER2010/COUNTY/2010/tl_2010_17_county10.zip\nunzip tl_2010_17_county10.zip\nogr2ogr -f GeoJSON tl_2010_17_county10.geojson tl_2010_17_county10.shp\n\ncurl -L -O https://www2.census.gov/geo/tiger/TIGER2010/COUNTY/2010/tl_2010_18_county10.zip\nunzip tl_2010_18_county10.zip\nogr2ogr -f GeoJSON tl_2010_18_county10.geojson tl_2010_18_county10.shp\n\ntippecanoe -zg -o counties-merged.mbtiles -l counties --coalesce-densest-as-needed --extend-zooms-if-still-dropping tl_2010_17_county10.geojson tl_2010_18_county10.geojson\n```\n\nAs above, but\n\n* `-l counties`: Specify the layer name instead of letting it be derived from the source file names\n\n### Selectively remove and replace features (Census tracts) to update a tileset\n\n```\n# Retrieve and tile California 2000 Census tracts\ncurl -L -O https://www2.census.gov/geo/tiger/TIGER2010/TRACT/2000/tl_2010_06_tract00.zip\nunzip tl_2010_06_tract00.zip\nogr2ogr -f GeoJSON tl_2010_06_tract00.shp.json tl_2010_06_tract00.shp\ntippecanoe -z11 -o tracts.mbtiles -l tracts tl_2010_06_tract00.shp.json\n\n# Create a copy of the tileset, minus Alameda County (FIPS code 001)\ntile-join -j '{\"*\":[\"none\",[\"==\",\"COUNTYFP00\",\"001\"]]}' -f -o tracts-filtered.mbtiles tracts.mbtiles\n\n# Retrieve and tile Alameda County Census tracts for 2010\ncurl -L -O https://www2.census.gov/geo/tiger/TIGER2010/TRACT/2010/tl_2010_06001_tract10.zip\nunzip tl_2010_06001_tract10.zip\nogr2ogr -f GeoJSON tl_2010_06001_tract10.shp.json tl_2010_06001_tract10.shp\ntippecanoe -z11 -o tracts-added.mbtiles -l tracts tl_2010_06001_tract10.shp.json\n\n# Merge the filtered tileset and the tileset of new tracts into a final tileset\ntile-join -o tracts-final.mbtiles tracts-filtered.mbtiles tracts-added.mbtiles\n```\n\nThe `-z11` option explicitly specifies the maxzoom, to make sure both the old and new tilesets have the same zoom range.\n\nThe `-j` option to `tile-join` specifies a filter, so that only the desired features will be copied to the new tileset.\nThis filter excludes (using `none`) any features whose FIPS code (`COUNTYFP00`) is the code for Alameda County (`001`).\n\nOptions\n-------\n\nThere are a lot of options. A lot of the time you won't want to use any of them\nother than `-o` _output_`.mbtiles` to name the output file, and probably `-f` to\ndelete the file that already exists with that name.\n\nIf you aren't sure what the right maxzoom is for your data, `-zg` will guess one for you\nbased on the density of features.\n\nTippecanoe will normally drop a fraction of point features at zooms below the maxzoom,\nto keep the low-zoom tiles from getting too big. If you have a smaller data set where\nall the points would fit without dropping any of them, use `-r1` to keep them all.\nIf you do want point dropping, but you still want the tiles to be denser than `-zg`\nthinks they should be, use `-B` to set a basezoom lower than the maxzoom.\n\nIf some of your tiles are coming out too big in spite of the settings above, you will\noften want to use `--drop-densest-as-needed` to drop whatever fraction of the features\nis necessary at each zoom level to make that zoom level's tiles work.\n\nIf your features have a lot of attributes, use `-y` to keep only the ones you really need.\n\nIf your input is formatted as newline-delimited GeoJSON, use `-P` to make input parsing a lot faster.\n\n### Output tileset\n\n * `-o` _file_`.mbtiles` or `--output=`_file_`.mbtiles`: Name the output file.\n * `-e` _directory_ or `--output-to-directory`=_directory_: Write tiles to the specified *directory* instead of to an mbtiles file.\n * `-f` or `--force`: Delete the mbtiles file if it already exists instead of giving an error\n * `-F` or `--allow-existing`: Proceed (without deleting existing data) if the metadata or tiles table already exists\n   or if metadata fields can't be set. You probably don't want to use this.\n\n### Tileset description and attribution\n\n * `-n` _name_ or `--name=`_name_: Human-readable name for the tileset (default file.json)\n * `-A` _text_ or `--attribution=`_text_: Attribution (HTML) to be shown with maps that use data from this tileset.\n * `-N` _description_ or `--description=`_description_: Description for the tileset (default file.mbtiles)\n\n### Input files and layer names\n\n * _name_`.json` or _name_`.geojson`: Read the named GeoJSON input file into a layer called _name_.\n * _name_`.json.gz` or _name_`.geojson.gz`: Read the named gzipped GeoJSON input file into a layer called _name_.\n * _name_`.geobuf`: Read the named Geobuf input file into a layer called _name_.\n * _name_`.csv`: Read the named CSV input file into a layer called _name_.\n * `-l` _name_ or `--layer=`_name_: Use the specified layer name instead of deriving a name from the input filename or output tileset. If there are multiple input files\n   specified, the files are all merged into the single named layer, even if they try to specify individual names with `-L`.\n * `-L` _name_`:`_file.json_ or `--named-layer=`_name_`:`_file.json_: Specify layer names for individual files. If your shell supports it, you can use a subshell redirect like `-L` _name_`:<(cat dir/*.json)` to specify a layer name for the output of streamed input.\n * `-L{`_layer-json_`}` or `--named-layer={`_layer-json_`}`: Specify an input file and layer options by a JSON object. The JSON object must contain a `\"file\"` key to specify the filename to read from. (If the `\"file\"` key is an empty string, it means to read from the standard input stream.) It may also contain a `\"layer\"` field to specify the name of the layer, and/or a `\"description\"` field to specify the layer's description in the tileset metadata, and/or a `\"format\"` field to specify `csv` or `geobuf` file format if it is not obvious from the `name`. Example:\n\n```\ntippecanoe -z5 -o world.mbtiles -L'{\"file\":\"ne_10m_admin_0_countries.json\", \"layer\":\"countries\", \"description\":\"Natural Earth countries\"}'\n```\n\nCSV input files currently support only Point geometries, from columns named `latitude`, `longitude`, `lat`, `lon`, `long`, `lng`, `x`, or `y`.\n\n### Parallel processing of input\n\n * `-P` or `--read-parallel`: Use multiple threads to read different parts of each GeoJSON input file at once.\n   This will only work if the input is line-delimited JSON with each Feature on its\n   own line, because it knows nothing of the top-level structure around the Features. Spurious \"EOF\" error\n   messages may result otherwise.\n   Performance will be better if the input is a named file that can be mapped into memory\n   rather than a stream that can only be read sequentially.\n\nIf the input file begins with the [RFC 8142](https://tools.ietf.org/html/rfc8142) record separator,\nparallel processing of input will be invoked automatically, splitting at record separators rather\nthan at all newlines.\n\nParallel processing will also be automatic if the input file is in Geobuf format.\n\n### Projection of input\n\n * `-s` _projection_ or `--projection=`_projection_: Specify the projection of the input data. Currently supported are `EPSG:4326` (WGS84, the default) and `EPSG:3857` (Web Mercator). In general you should use WGS84 for your input files if at all possible.\n\n### Zoom levels\n\n * `-z` _zoom_ or `--maximum-zoom=`_zoom_: Maxzoom: the highest zoom level for which tiles are generated (default 14)\n * `-zg` or `--maximum-zoom=g`: Guess what is probably a reasonable maxzoom based on the spacing of features.\n * `-Z` _zoom_ or `--minimum-zoom=`_zoom_: Minzoom: the lowest zoom level for which tiles are generated (default 0)\n * `-ae` or `--extend-zooms-if-still-dropping`: Increase the maxzoom if features are still being dropped at that zoom level.\n   The detail and simplification options that ordinarily apply only to the maximum zoom level will apply both to the originally\n   specified maximum zoom and to any levels added beyond that.\n * `-R` _zoom_`/`_x_`/`_y_ or `--one-tile=`_zoom_`/`_x_`/`_y_: Set the minzoom and maxzoom to _zoom_ and produce only\n   the single specified tile at that zoom level.\n\nIf you know the precision to which you want your data to be represented,\nor the map scale of a corresponding printed map,\nthis table shows the approximate precision and scale corresponding to various\n`-z` options if you use the default `-d` detail of 12:\n\nzoom level | precision (ft) | precision (m) | map scale\n---------- | -------------- | ------------- | ---------\n`-z0` | 32000 ft | 10000 m | 1:320,000,000\n`-z1` | 16000 ft | 5000 m | 1:160,000,000\n`-z2` | 8000 ft | 2500 m | 1:80,000,000\n`-z3` | 4000 ft | 1250 m | 1:40,000,000\n`-z4` | 2000 ft | 600 m | 1:20,000,000\n`-z5` | 1000 ft | 300 m | 1:10,000,000\n`-z6` | 500 ft | 150 m | 1:5,000,000\n`-z7` | 250 ft | 80 m | 1:2,500,000\n`-z8` | 125 ft | 40 m | 1:1,250,000\n`-z9` | 64 ft | 20 m | 1:640,000\n`-z10` | 32 ft | 10 m | 1:320,000\n`-z11` | 16 ft | 5 m | 1:160,000\n`-z12` | 8 ft | 2 m | 1:80,000\n`-z13` | 4 ft | 1 m | 1:40,000\n`-z14` | 2 ft | 0.5 m | 1:20,000\n`-z15` | 1 ft | 0.25 m | 1:10,000\n`-z16` | 6 in | 15 cm | 1:5000\n`-z17` | 3 in | 8 cm | 1:2500\n`-z18` | 1.5 in | 4 cm | 1:1250\n`-z19` | 0.8 in | 2 cm | 1:600\n`-z20` | 0.4 in | 1 cm | 1:300\n`-z21` | 0.2 in | 0.5 cm | 1:150\n`-z22` | 0.1 in | 0.25 cm | 1:75\n\n### Tile resolution\n\n * `-d` _detail_ or `--full-detail=`_detail_: Detail at max zoom level (default 12, for tile resolution of 2^12=4096)\n * `-D` _detail_ or `--low-detail=`_detail_: Detail at lower zoom levels (default 12, for tile resolution of 2^12=4096)\n * `-m` _detail_ or `--minimum-detail=`_detail_: Minimum detail that it will try if tiles are too big at regular detail (default 7)\n\nAll internal math is done in terms of a 32-bit tile coordinate system, so 1/(2^32) of the size of Earth,\nor about 1cm, is the smallest distinguishable distance. If _maxzoom_ + _detail_ > 32, no additional\nresolution is obtained than by using a smaller _maxzoom_ or _detail_.\n\n### Filtering feature attributes\n\n * `-x` _name_ or `--exclude=`_name_: Exclude the named attributes from all features. You can specify multiple `-x` options to exclude several attributes. (Don't comma-separate names within a single `-x`.)\n * `-y` _name_ or `--include=`_name_: Include the named attributes in all features, excluding all those not explicitly named. You can specify multiple `-y` options to explicitly include several attributes. (Don't comma-separate names within a single `-y`.)\n * `-X` or `--exclude-all`: Exclude all attributes and encode only geometries\n\n### Modifying feature attributes\n\n * `-T`_attribute_`:`_type_ or `--attribute-type=`_attribute_`:`_type_: Coerce the named feature _attribute_ to be of the specified _type_.\n   The _type_ may be `string`, `float`, `int`, or `bool`.\n   If the type is `bool`, then original attributes of `0` (or, if numeric, `0.0`, etc.), `false`, `null`, or the empty string become `false`, and otherwise become `true`.\n   If the type is `float` or `int` and the original attribute was non-numeric, it becomes `0`.\n   If the type is `int` and the original attribute was floating-point, it is rounded to the nearest integer.\n * `-Y`_attribute_`:`_description_ or `--attribute-description=`_attribute_`:`_description_: Set the `description` for the specified attribute in the tileset metadata to _description_ instead of the usual `String`, `Number`, or `Boolean`.\n * `-E`_attribute_`:`_operation_ or `--accumulate-attribute=`_attribute_`:`_operation_: Preserve the named _attribute_ from features\n   that are dropped, coalesced-as-needed, or clustered. The _operation_ may be\n   `sum`, `product`, `mean`, `max`, `min`, `concat`, or `comma`\n   to specify how the named _attribute_ is accumulated onto the attribute of the same name in a feature that does survive.\n * `-pe` or `--empty-csv-columns-are-null`: Treat empty CSV columns as nulls rather than as empty strings.\n * `-aI` or `--convert-stringified-ids-to-numbers`: If a feature ID is the string representation of a number, convert it to a plain number to use as the feature ID.\n * `--use-attribute-for-id=`*name*: Use the attribute with the specified *name* as if it were specified as the feature ID. (If this attribute is a stringified number, you must also use `-aI` to convert it to a number.)\n\n### Filtering features by attributes\n\n * `-j` *filter* or `--feature-filter`=*filter*: Check features against a per-layer filter (as defined in the [Mapbox GL Style Specification](https://docs.mapbox.com/mapbox-gl-js/style-spec/#other-filter)) and only include those that match. Any features in layers that have no filter specified will be passed through. Filters for the layer `\"*\"` apply to all layers. The special variable `$zoom` refers to the current zoom level.\n * `-J` *filter-file* or `--feature-filter-file`=*filter-file*: Like `-j`, but read the filter from a file.\n\nExample: to find the Natural Earth countries with low `scalerank` but high `LABELRANK`:\n\n```\ntippecanoe -z5 -o filtered.mbtiles -j '{ \"ne_10m_admin_0_countries\": [ \"all\", [ \"<\", \"scalerank\", 3 ], [ \">\", \"LABELRANK\", 5 ] ] }' ne_10m_admin_0_countries.geojson\n```\n\nExample: to retain only major TIGER roads at low zoom levels:\n\n```\ntippecanoe -o roads.mbtiles -j '{ \"*\": [ \"any\", [ \">=\", \"$zoom\", 11 ], [ \"in\", \"MTFCC\", \"S1100\", \"S1200\" ] ] }' tl_2015_06001_roads.json\n```\n\nTippecanoe also accepts expressions of the form `[ \"attribute-filter\", name, expression ]`, to filter individual feature attributes\ninstead of entire features. For example, you can exclude the road names at low zoom levels by doing\n\n```\ntippecanoe -o roads.mbtiles -j '{ \"*\": [ \"attribute-filter\", \"FULLNAME\", [ \">=\", \"$zoom\", 9 ] ] }' tl_2015_06001_roads.json\n```\n\nAn `attribute-filter` expression itself is always considered to evaluate to `true` (in other words, to retain the feature instead\nof dropping it). If you want to use multiple `attribute-filter` expressions, or to use other expressions to remove features from\nthe same layer, enclose them in an `all` expression so they will all be evaluated.\n\n### Dropping a fixed fraction of features by zoom level\n\n * `-r` _rate_ or `--drop-rate=`_rate_: Rate at which dots are dropped at zoom levels below basezoom (default 2.5).\n   If you use `-rg`, it will guess a drop rate that will keep at most 50,000 features in the densest tile.\n   You can also specify a marker-width with `-rg`*width* to allow fewer features in the densest tile to\n   compensate for the larger marker, or `-rf`*number* to allow at most *number* features in the densest tile.\n * `-B` _zoom_ or `--base-zoom=`_zoom_: Base zoom, the level at and above which all points are included in the tiles (default maxzoom).\n   If you use `-Bg`, it will guess a zoom level that will keep at most 50,000 features in the densest tile.\n   You can also specify a marker-width with `-Bg`*width* to allow fewer features in the densest tile to\n   compensate for the larger marker, or `-Bf`*number* to allow at most *number* features in the densest tile.\n * `-al` or `--drop-lines`: Let \"dot\" dropping at lower zooms apply to lines too\n * `-ap` or `--drop-polygons`: Let \"dot\" dropping at lower zooms apply to polygons too\n * `-K` _distance_ or `--cluster-distance=`_distance_: Cluster points (as with `--cluster-densest-as-needed`, but without the experimental discovery process) that are approximately within _distance_ of each other. The units are tile coordinates within a nominally 256-pixel tile, so the maximum value of 255 allows only one feature per tile. Values around 10 are probably appropriate for typical marker sizes. See `--cluster-densest-as-needed` below for behavior.\n\n### Dropping a fraction of features to keep under tile size limits\n\n * `-as` or `--drop-densest-as-needed`: If a tile is too large, try to reduce it to under 500K by increasing the minimum spacing between features. The discovered spacing applies to the entire zoom level.\n * `-ad` or `--drop-fraction-as-needed`: Dynamically drop some fraction of features from each zoom level to keep large tiles under the 500K size limit. (This is like `-pd` but applies to the entire zoom level, not to each tile.)\n * `-an` or `--drop-smallest-as-needed`: Dynamically drop the smallest features (physically smallest: the shortest lines or the smallest polygons) from each zoom level to keep large tiles under the 500K size limit. This option will not work for point features.\n * `-aN` or `--coalesce-smallest-as-needed`: Dynamically combine the smallest features (physically smallest: the shortest lines or the smallest polygons) from each zoom level into other nearby features to keep large tiles under the 500K size limit. This option will not work for point features, and will probably not help very much with LineStrings. It is mostly intended for polygons, to maintain the full original area covered by polygons while still reducing the feature count somehow. The attributes of the small polygons are *not* preserved into the combined features (except through `--accumulate-attribute`), only their geometry. Furthermore, the polygons to which nested polygons are coalesced may not necessarily be the immediately enclosing features.\n * `-aD` or `--coalesce-densest-as-needed`: Dynamically combine the densest features from each zoom level into other nearby features to keep large tiles under the 500K size limit. (Again, mostly useful for polygons.)\n * `-aS` or `--coalesce-fraction-as-needed`: Dynamically combine a fraction of features from each zoom level into other nearby features to keep large tiles under the 500K size limit. (Again, mostly useful for polygons.)\n * `-pd` or `--force-feature-limit`: Dynamically drop some fraction of features from large tiles to keep them under the 500K size limit. It will probably look ugly at the tile boundaries. (This is like `-ad` but applies to each tile individually, not to the entire zoom level.) You probably don't want to use this.\n * `-aC` or `--cluster-densest-as-needed`: If a tile is too large, try to reduce its size by increasing the minimum spacing between features, and leaving one placeholder feature from each group.  The remaining feature will be given a `\"clustered\": true` attribute to indicate that it represents a cluster, a `\"point_count\"` attribute to indicate the number of features that were clustered into it, and a `\"sqrt_point_count\"` attribute to indicate the relative width of a feature to represent the cluster. If the features being clustered are points, the representative feature will be located at the average of the original points' locations; otherwise, one of the original features will be left as the representative.\n\n### Dropping tightly overlapping features\n\n * `-g` _gamma_ or `--gamma=_gamma`_: Rate at which especially dense dots are dropped (default 0, for no effect). A gamma of 2 reduces the number of dots less than a pixel apart to the square root of their original number.\n * `-aG` or `--increase-gamma-as-needed`: If a tile is too large, try to reduce it to under 500K by increasing the `-g` gamma. The discovered gamma applies to the entire zoom level. You probably want to use `--drop-densest-as-needed` instead.\n\n### Line and polygon simplification\n\n * `-S` _scale_ or `--simplification=`_scale_: Multiply the tolerance for line and polygon simplification by _scale_. The standard tolerance tries to keep\n   the line or polygon within one tile unit of its proper location. You can probably go up to about 10 without too much visible difference.\n * `-ps` or `--no-line-simplification`: Don't simplify lines and polygons\n * `-pS` or `--simplify-only-low-zooms`: Don't simplify lines and polygons at maxzoom (but do simplify at lower zooms)\n * `-pn` or `--no-simplification-of-shared-nodes`: Don't simplify away nodes that appear in more than one feature or are used multiple times within the same feature, so that the intersection node will not be lost from intersecting roads. (This will not be effective if you also use `--coalesce` or `--detect-shared-borders`.)\n * `-pt` or `--no-tiny-polygon-reduction`: Don't combine the area of very small polygons into small squares that represent their combined area.\n\n### Attempts to improve shared polygon boundaries\n\n * `-ab` or `--detect-shared-borders`: In the manner of [TopoJSON](https://github.com/mbostock/topojson/wiki/Introduction), detect borders that are shared between multiple polygons and simplify them identically in each polygon. This takes more time and memory than considering each polygon individually.\n * `-aL` or `--grid-low-zooms`: At all zoom levels below _maxzoom_, snap all lines and polygons to a stairstep grid instead of allowing diagonals. You will also want to specify a tile resolution, probably `-D8`. This option provides a way to display continuous parcel, gridded, or binned data at low zooms without overwhelming the tiles with tiny polygons, since features will either get stretched out to the grid unit or lost entirely, depending on how they happened to be aligned in the original data. You probably don't want to use this.\n\n### Controlling clipping to tile boundaries\n\n * `-b` _pixels_ or `--buffer=`_pixels_: Buffer size where features are duplicated from adjacent tiles. Units are \"screen pixels\"—1/256th of the tile width or height. (default 5)\n * `-pc` or `--no-clipping`: Don't clip features to the size of the tile. If a feature overlaps the tile's bounds or buffer at all, it is included completely. Be careful: this can produce very large tilesets, especially with large polygons.\n * `-pD` or `--no-duplication`: As with `--no-clipping`, each feature is included intact instead of cut to tile boundaries. In addition, it is included only in a single tile per zoom level rather than potentially in multiple copies. Clients of the tileset must check adjacent tiles (possibly some distance away) to ensure they have all features.\n\n### Reordering features within each tile\n\n * `-pi` or `--preserve-input-order`: Preserve the original input order of features as the drawing order instead of ordering geographically. (This is implemented as a restoration of the original order at the end, so that dot-dropping is still geographic, which means it also undoes `-ao`).\n * `-ac` or `--coalesce`: Coalesce consecutive features that have the same attributes. This can be useful if you have lots of small polygons with identical attributes and you would like to merge them together.\n * `-ao` or `--reorder`: Reorder features to put ones with the same attributes in sequence (instead of ones that are approximately spatially adjacent), to try to get them to coalesce. You probably want to use this if you use `--coalesce`.\n * `-ar` or `--reverse`: Try reversing the directions of lines to make them coalesce and compress better. You probably don't want to use this.\n * `-ah` or `--hilbert`: Put features in Hilbert Curve order instead of the usual Z-Order. This improves the odds that spatially adjacent features will be sequentially adjacent, and should improve density calculations and spatial coalescing. It should be the default eventually.\n\n### Adding calculated attributes\n\n * `-ag` or `--calculate-feature-density`: Add a new attribute, `tippecanoe_feature_density`, to each feature, to record how densely features are spaced in that area of the tile. You can use this attribute in the style to produce a glowing effect where points are densely packed. It can range from 0 in the sparsest areas to 255 in the densest.\n * `-ai` or `--generate-ids`: Add an `id` (a feature ID, not an attribute named `id`) to each feature that does not already have one. There is currently no guarantee that the `id` added will be stable between runs or that it will not conflict with manually-assigned feature IDs. Future versions of Tippecanoe may change the mechanism for allocating IDs.\n\n### Trying to correct bad source geometry\n\n * `-aw` or `--detect-longitude-wraparound`: Detect when consecutive points within a feature jump to the other side of the world, and try to fix the geometry.\n * `-pw` or `--use-source-polygon-winding`: Instead of respecting GeoJSON polygon ring order, use the original polygon winding in the source data to distinguish inner (clockwise) and outer (counterclockwise) polygon rings.\n * `-pW` or `--reverse-source-polygon-winding`: Instead of respecting GeoJSON polygon ring order, use the opposite of the original polygon winding in the source data to distinguish inner (counterclockwise) and outer (clockwise) polygon rings.\n * `--clip-bounding-box=`*minlon*`,`*minlat*`,`*maxlon*`,`*maxlat*: Clip all features to the specified bounding box.\n\n### Setting or disabling tile size limits\n\n * `-M` _bytes_ or `--maximum-tile-bytes=`_bytes_: Use the specified number of _bytes_ as the maximum compressed tile size instead of 500K.\n * `-O` _features_ or `--maximum-tile-features=`_features_: Use the specified number of _features_ as the maximum in a tile instead of 200,000.\n * `-pf` or `--no-feature-limit`: Don't limit tiles to 200,000 features\n * `-pk` or `--no-tile-size-limit`: Don't limit tiles to 500K bytes\n * `-pC` or `--no-tile-compression`: Don't compress the PBF vector tile data. If you are getting \"Unimplemented type 3\" error messages from a renderer, it is probably because it expects uncompressed tiles using this option rather than the normal gzip-compressed tiles.\n * `-pg` or `--no-tile-stats`: Don't generate the `tilestats` row in the tileset metadata. Uploads without [tilestats](https://github.com/mapbox/mapbox-geostats) will take longer to process.\n * `--tile-stats-attributes-limit=`*count*: Include `tilestats` information about at most *count* attributes instead of the default 1000.\n * `--tile-stats-sample-values-limit=`*count*: Calculate `tilestats` attribute statistics based on *count* values instead of the default 1000.\n * `--tile-stats-values-limit=`*count*: Report *count* unique attribute values in `tilestats` instead of the default 100.\n\n### Temporary storage\n\n * `-t` _directory_ or `--temporary-directory=`_directory_: Put the temporary files in _directory_.\n   If you don't specify, it will use `/tmp`.\n\n### Progress indicator\n\n * `-q` or `--quiet`: Work quietly instead of reporting progress or warning messages\n * `-Q` or `--no-progress-indicator`: Don't report progress, but still give warnings\n * `-U` _seconds_ or `--progress-interval=`_seconds_: Don't report progress more often than the specified number of _seconds_.\n * `-v` or `--version`: Report Tippecanoe's version number\n\n### Filters\n\n * `-C` _command_ or `--prefilter=`_command_: Specify a shell filter command to be run at the start of assembling each tile\n * `-c` _command_ or `--postfilter=`_command_: Specify a shell filter command to be run at the end of assembling each tile\n\nThe pre- and post-filter commands allow you to do optional filtering or transformation on the features of each tile\nas it is created. They are shell commands, run with the zoom level, X, and Y as the `$1`, `$2`, and `$3` arguments.\nFuture versions of Tippecanoe may add additional arguments for more context.\n\nThe features are provided to the filter\nas a series of newline-delimited GeoJSON objects on the standard input, and `tippecanoe` expects to read another\nset of GeoJSON features from the filter's standard output.\n\nThe prefilter receives the features at the highest available resolution, before line simplification,\npolygon topology repair, gamma calculation, dynamic feature dropping, or other internal processing.\nThe postfilter receives the features at tile resolution, after simplification, cleaning, and dropping.\n\nThe layer name is provided as part of the `tippecanoe` element of the feature and must be passed through\nto keep the feature in its correct layer. In the case of the prefilter, the `tippecanoe` element may also\ncontain `index`, `sequence`, `extent`, and `dropped`, elements, which must be passed through for internal operations like\n`--drop-densest-as-needed`, `--drop-smallest-as-needed`, and `--preserve-input-order` to work.\n\n#### Examples:\n\n * Make a tileset of the Natural Earth countries to zoom level 5, and also copy the GeoJSON features\n   to files in a `tiles/z/x/y.geojson` directory hierarchy.\n\n```\ntippecanoe -o countries.mbtiles -z5 -C 'mkdir -p tiles/$1/$2; tee tiles/$1/$2/$3.geojson' ne_10m_admin_0_countries.json\n```\n\n * Make a tileset of the Natural Earth countries to zoom level 5, but including only those tiles that\n   intersect the [bounding box of Germany](https://www.flickr.com/places/info/23424829).\n   (The `limit-tiles-to-bbox` script is [in the Tippecanoe source directory](filters/limit-tiles-to-bbox).)\n\n```\ntippecanoe -o countries.mbtiles -z5 -C './filters/limit-tiles-to-bbox 5.8662 47.2702 15.0421 55.0581 $*' ne_10m_admin_0_countries.json\n```\n\n * Make a tileset of TIGER roads in Tippecanoe County, leaving out all but primary and secondary roads (as [classified by TIGER](https://www.census.gov/geo/reference/mtfcc.html)) below zoom level 11.\n\n```\ntippecanoe -o roads.mbtiles -c 'if [ $1 -lt 11 ]; then grep \"\\\"MTFCC\\\": \\\"S1[12]00\\\"\"; else cat; fi' tl_2016_18157_roads.json\n```\n\nEnvironment\n-----------\n\nTippecanoe ordinarily uses as many parallel threads as the operating system claims that CPUs are available.\nYou can override this number by setting the `TIPPECANOE_MAX_THREADS` environmental variable.\n\nGeoJSON extension\n-----------------\n\nTippecanoe defines a GeoJSON extension that you can use to specify the minimum and/or maximum zoom level\nat which an individual feature will be included in the vector tileset being produced.\nIf you have a feature like this:\n\n```\n{\n    \"type\" : \"Feature\",\n    \"tippecanoe\" : { \"maxzoom\" : 9, \"minzoom\" : 4 },\n    \"properties\" : { \"FULLNAME\" : \"N Vasco Rd\" },\n    \"geometry\" : {\n        \"type\" : \"LineString\",\n        \"coordinates\" : [ [ -121.733350, 37.767671 ], [ -121.733600, 37.767483 ], [ -121.733131, 37.766952 ] ]\n    }\n}\n```\n\nwith a `tippecanoe` object specifiying a `maxzoom` of 9 and a `minzoom` of 4, the feature\nwill only appear in the vector tiles for zoom levels 4 through 9. Note that the `tippecanoe`\nobject belongs to the Feature, not to its `properties`. If you specify a `minzoom` for a feature,\nit will be preserved down to that zoom level even if dot-dropping with `-r` would otherwise have\ndropped it.\n\nYou can also specify a layer name in the `tippecanoe` object, which will take precedence over\nthe filename or name specified using `--layer`, like this:\n\n```\n{\n    \"type\" : \"Feature\",\n    \"tippecanoe\" : { \"layer\" : \"streets\" },\n    \"properties\" : { \"FULLNAME\" : \"N Vasco Rd\" },\n    \"geometry\" : {\n        \"type\" : \"LineString\",\n        \"coordinates\" : [ [ -121.733350, 37.767671 ], [ -121.733600, 37.767483 ], [ -121.733131, 37.766952 ] ]\n    }\n}\n```\n\nIf your source GeoJSON only has `minzoom`, `maxzoom` and/or `layer` within `properties` you can use [ndjson-cli](https://github.com/mbostock/ndjson-cli/blob/master/README.md) to move them into the required `tippecanoe` object by piping the GeoJSON like this:\n\n```sh\nndjson-map 'd.tippecanoe = { minzoom: d.properties.minzoom, maxzoom: d.properties.maxzoom, layer: d.properties.layer }, delete d.properties.minzoom, delete d.properties.maxzoom, delete d.properties.layer, d'\n```\n\nGeometric simplifications\n-------------------------\n\nAt every zoom level, line and polygon features are subjected to Douglas-Peucker\nsimplification to the resolution of the tile.\n\nFor point features, it drops 1/2.5 of the dots for each zoom level above the\npoint base zoom (which is normally the same as the `-z` max zoom, but can be\na different zoom specified with `-B` if you have precise but sparse data).\nI don't know why 2.5 is the appropriate number, but the densities of many different\ndata sets fall off at about this same rate. You can use -r to specify a different rate.\n\nYou can use the gamma option to thin out especially dense clusters of points.\nFor any area where dots are closer than one pixel together (at whatever zoom level),\na gamma of 3, for example, will reduce these clusters to the cube root of their original density.\n\nFor line features, it drops any features that are too small to draw at all.\nThis still leaves the lower zooms too dark (and too dense for the 500K tile limit,\nin some places), so I need to figure out an equitable way to throw features away.\n\nUnless you specify `--no-tiny-polygon-reduction`,\nany polygons that are smaller than a minimum area (currently 4 square subpixels) will\nhave their probability diffused, so that some of them will be drawn as a square of\nthis minimum size and others will not be drawn at all, preserving the total area that\nall of them should have had together.\n\nFeatures in the same tile that share the same type and attributes are coalesced\ntogether into a single geometry if you use `--coalesce`. You are strongly encouraged to use -x to exclude\nany unnecessary attributes to reduce wasted file size.\n\nIf a tile is larger than 500K, it will try encoding that tile at progressively\nlower resolutions before failing if it still doesn't fit.\n\nDevelopment\n-----------\n\nRequires sqlite3 and zlib (should already be installed on MacOS). Rebuilding the manpage\nuses md2man (`gem install md2man`).\n\nLinux:\n\n    sudo apt-get install build-essential libsqlite3-dev zlib1g-dev\n\nThen build:\n\n    make\n\nand perhaps\n\n    make install\n\nTippecanoe now requires features from the 2011 C++ standard. If your compiler is older than\nthat, you will need to install a newer one. On MacOS, updating to the lastest XCode should\nget you a new enough version of `clang++`. On Linux, you should be able to upgrade `g++` with\n\n```\nsudo add-apt-repository -y ppa:ubuntu-toolchain-r/test\nsudo apt-get update -y\nsudo apt-get install -y g++-5\nexport CXX=g++-5\n```\n\nDocker Image\n------------\n\nA tippecanoe Docker image can be built from source and executed as a task to\nautomatically install dependencies and allow tippecanoe to run on any system\nsupported by Docker.\n\n```docker\n$ docker build -t tippecanoe:latest .\n$ docker run -it --rm \\\n  -v /tiledata:/data \\\n  tippecanoe:latest \\\n  tippecanoe --output=/data/output.mbtiles /data/example.geojson\n```\n\nThe commands above will build a Docker image from the source and compile the\nlatest version. The image supports all tippecanoe flags and options.\n\nExamples\n------\n\nCheck out [some examples of maps made with tippecanoe](MADE_WITH.md)\n\nName\n----\n\nThe name is [a joking reference](http://en.wikipedia.org/wiki/Tippecanoe_and_Tyler_Too) to a \"tiler\" for making map tiles.\n\ntile-join\n=========\n\nTile-join is a tool for copying and merging vector mbtiles files and for\njoining new attributes from a CSV file to existing features in them.\n\nIt reads the tiles from an\nexisting .mbtiles file or a directory of tiles, matches them against the\nrecords of the CSV (if one is specified), and writes out a new tileset.\n\nIf you specify multiple source mbtiles files or source directories of tiles,\nall the sources are read and their combined contents are written to the new\nmbtiles output. If they define the same layers or the same tiles, the layers\nor tiles are merged.\n\nThe options are:\n\n### Output tileset\n\n * `-o` *out.mbtiles* or `--output=`*out.mbtiles*: Write the new tiles to the specified .mbtiles file.\n * `-e` *directory* or `--output-to-directory=`*directory*: Write the new tiles to the specified directory instead of to an mbtiles file.\n * `-f` or `--force`: Remove *out.mbtiles* if it already exists.\n\n### Tileset description and attribution\n\n * `-A` *attribution* or `--attribution=`*attribution*: Set the attribution string.\n * `-n` *name* or `--name=`*name*: Set the tileset name.\n * `-N` *description* or `--description=`*description*: Set the tileset description.\n\n### Layer filtering and naming\n\n * `-l` *layer* or `--layer=`*layer*: Include the named layer in the output. You can specify multiple `-l` options to keep multiple layers. If you don't specify, they will all be retained.\n * `-L` *layer* or `--exclude-layer=`*layer*: Remove the named layer from the output. You can specify multiple `-L` options to remove multiple layers.\n * `-R`*old*`:`*new* or `--rename-layer=`*old*`:`*new*: Rename the layer named *old* to be named *new* instead. You can specify multiple `-R` options to rename multiple layers. Renaming happens before filtering.\n\n### Zoom levels\n\n * `-z` _zoom_ or `--maximum-zoom=`_zoom_: Don't copy tiles from higher zoom levels than the specified zoom\n * `-Z` _zoom_ or `--minimum-zoom=`_zoom_: Don't copy tiles from lower zoom levels than the specified zoom\n\n### Merging attributes from a CSV file\n\n * `-c` *match*`.csv` or `--csv=`*match*`.csv`: Use *match*`.csv` as the source for new attributes to join to the features. The first line of the file should be the key names; the other lines are values. The first column is the one to match against the existing features; the other columns are the new data to add.\n\n### Filtering features and feature attributes\n\n * `-x` *key* or `--exclude=`*key*: Remove attributes of type *key* from the output. You can use this to remove the field you are matching against if you no longer need it after joining, or to remove any other attributes you don't want.\n * `-X` or `--exclude-all`: Remove all attributes from the output.\n * `-i` or `--if-matched`: Only include features that matched the CSV.\n * `-j` *filter* or `--feature-filter`=*filter*: Check features against a per-layer filter (as defined in the [Mapbox GL Style Specification](https://docs.mapbox.com/mapbox-gl-js/style-spec/#other-filter)) and only include those that match. Any features in layers that have no filter specified will be passed through. Filters for the layer `\"*\"` apply to all layers.\n * `-J` *filter-file* or `--feature-filter-file`=*filter-file*: Like `-j`, but read the filter from a file.\n * `-pe` or `--empty-csv-columns-are-null`: Treat empty CSV columns as nulls rather than as empty strings.\n\n### Setting or disabling tile size limits\n\n * `-pk` or `--no-tile-size-limit`: Don't skip tiles larger than 500K.\n * `-pC` or `--no-tile-compression`: Don't compress the PBF vector tile data.\n * `-pg` or `--no-tile-stats`: Don't generate the `tilestats` row in the tileset metadata. Uploads without [tilestats](https://github.com/mapbox/mapbox-geostats) will take longer to process.\n\nBecause tile-join just copies the geometries to the new .mbtiles without processing them\n(except to rescale the extents if necessary),\nit doesn't have any of tippecanoe's recourses if the new tiles are bigger than the 500K tile limit.\nIf a tile is too big and you haven't specified `-pk`, it is just left out of the new tileset.\n\nExample\n-------\n\nImagine you have a tileset of census blocks:\n\n```sh\ncurl -L -O http://www2.census.gov/geo/tiger/TIGER2010/TABBLOCK/2010/tl_2010_06001_tabblock10.zip\nunzip tl_2010_06001_tabblock10.zip\nogr2ogr -f GeoJSON tl_2010_06001_tabblock10.json tl_2010_06001_tabblock10.shp\n./tippecanoe -o tl_2010_06001_tabblock10.mbtiles tl_2010_06001_tabblock10.json\n```\n\nand a CSV of their populations:\n\n```sh\ncurl -L -O http://www2.census.gov/census_2010/01-Redistricting_File--PL_94-171/California/ca2010.pl.zip\nunzip -p ca2010.pl.zip cageo2010.pl |\nawk 'BEGIN {\n    print \"GEOID10,population\"\n}\n(substr($0, 9, 3) == \"750\") {\n    print \"\\\"\" substr($0, 28, 2) substr($0, 30, 3) substr($0, 55, 6) substr($0, 62, 4) \"\\\",\" (0 + substr($0, 328, 9))\n}' > population.csv\n```\n\nwhich looks like this:\n\n```\nGEOID10,population\n\"060014277003018\",0\n\"060014283014046\",0\n\"060014284001020\",0\n...\n\"060014507501001\",202\n\"060014507501002\",119\n\"060014507501003\",193\n\"060014507501004\",85\n...\n```\n\nThen you can join those populations to the geometries and discard the no-longer-needed ID field:\n\n```sh\n./tile-join -o population.mbtiles -x GEOID10 -c population.csv tl_2010_06001_tabblock10.mbtiles\n```\n\ntippecanoe-enumerate\n====================\n\nThe `tippecanoe-enumerate` utility lists the tiles that an `mbtiles` file defines.\nEach line of the output lists the name of the `mbtiles` file and the zoom, x, and y\ncoordinates of one of the tiles. It does basically the same thing as\n\n    select zoom_level, tile_column, (1 << zoom_level) - 1 - tile_row from tiles;\n\non the file in sqlite3.\n\ntippecanoe-decode\n=================\n\nThe `tippecanoe-decode` utility turns vector mbtiles back to GeoJSON. You can use it either\non an entire file:\n\n    tippecanoe-decode file.mbtiles\n\nor on an individual tile:\n\n    tippecanoe-decode file.mbtiles zoom x y\n    tippecanoe-decode file.vector.pbf zoom x y\n\nUnless you use `-c`, the output is a set of nested FeatureCollections identifying each\ntile and layer separately. Note that the same features generally appear at all zooms,\nso the output for the file will have many copies of the same features at different\nresolutions.\n\n### Options\n\n * `-s` _projection_ or `--projection=`*projection*: Specify the projection of the output data. Currently supported are EPSG:4326 (WGS84, the default) and EPSG:3857 (Web Mercator).\n * `-z` _maxzoom_ or `--maximum-zoom=`*maxzoom*: Specify the highest zoom level to decode from the tileset\n * `-Z` _minzoom_ or `--minimum-zoom=`*minzoom*: Specify the lowest zoom level to decode from the tileset\n * `-l` _layer_ or `--layer=`*layer*: Decode only layers with the specified names. (Multiple `-l` options can be specified.)\n * `-c` or `--tag-layer-and-zoom`: Include each feature's layer and zoom level as part of its `tippecanoe` object rather than as a FeatureCollection wrapper\n * `-S` or `--stats`: Just report statistics about each tile's size and the number of features in it, as a JSON structure.\n * `-f` or `--force`: Decode tiles even if polygon ring order or closure problems are detected\n\ntippecanoe-json-tool\n====================\n\nExtracts GeoJSON features or standalone geometries as line-delimited JSON objects from a larger JSON file,\nfollowing the same extraction rules that Tippecanoe uses when parsing JSON.\n\n    tippecanoe-json-tool file.json [... file.json]\n\nOptionally also wraps them in a FeatureCollection or GeometryCollection as appropriate.\n\nOptionally extracts an attribute from the GeoJSON `properties` for sorting.\n\nOptionally joins a sorted CSV of new attributes to a sorted GeoJSON file.\n\nThe reason for requiring sorting is so that it is possible to work on CSV and GeoJSON files that are larger\nthan can comfortably fit in memory by streaming through them in parallel, in the same way that the Unix\n`join` command does. The Unix `sort` command can be used to sort large files to prepare them for joining.\n\nThe sorting interface is weird, and future version of `tippecanoe-json-tool` will replace it with\nsomething better.\n\n### Options\n\n * `-w` or `--wrap`: Add the FeatureCollection or GeometryCollection wrapper.\n * `-e` *attribute* or `--extract=`*attribute*: Extract the named attribute as a prefix to each feature.\n   The formatting makes excessive use of `\\u` quoting so that it follows JSON string rules but will still\n   be sorted correctly by tools that just do ASCII comparisons.\n * `-c` *file.csv* or `--csv=`*file.csv*: Join attributes from the named sorted CSV file, using its first column as the join key. Geometries will be passed through even if they do not match the CSV; CSV lines that do not match a geometry will be discarded.\n * `-pe` or `--empty-csv-columns-are-null`: Treat empty CSV columns as nulls rather than as empty strings.\n\n### Example\n\nJoin Census LEHD ([Longitudinal Employer-Household Dynamics](https://lehd.ces.census.gov/)) employment data to a file of Census block geography\nfor Tippecanoe County, Indiana.\n\nDownload Census block geometry, and convert to GeoJSON:\n\n```\n$ curl -L -O https://www2.census.gov/geo/tiger/TIGER2010/TABBLOCK/2010/tl_2010_18157_tabblock10.zip\n$ unzip tl_2010_18157_tabblock10.zip\n$ ogr2ogr -f GeoJSON tl_2010_18157_tabblock10.json tl_2010_18157_tabblock10.shp\n```\n\nDownload Indiana employment data, and fix name of join key in header\n\n```\n$ curl -L -O https://lehd.ces.census.gov/data/lodes/LODES7/in/wac/in_wac_S000_JT00_2015.csv.gz\n$ gzip -dc in_wac_S000_JT00_2015.csv.gz | sed '1s/w_geocode/GEOID10/' > in_wac_S000_JT00_2015.csv\n```\n\nSort GeoJSON block geometry so it is ordered by block ID. If you don't do this, you will get a\n\"GeoJSON file is out of sort\" error.\n\n```\n$ tippecanoe-json-tool -e GEOID10 tl_2010_18157_tabblock10.json | LC_ALL=C sort > tl_2010_18157_tabblock10.sort.json\n```\n\nJoin block geometries to employment attributes:\n\n```\n$ tippecanoe-json-tool -c in_wac_S000_JT00_2015.csv tl_2010_18157_tabblock10.sort.json > blocks-wac.json\n```\n"
        },
        {
          "name": "catch",
          "type": "tree",
          "content": null
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.06640625,
          "content": "ignore:\n  - \"test\"\n  - \"mapbox\"\n\ncoverage:\n  status:\n    patch: off\n"
        },
        {
          "name": "csv.cpp",
          "type": "blob",
          "size": 2.958984375,
          "content": "#include \"csv.hpp\"\n#include \"text.hpp\"\n\nstd::vector<std::string> csv_split(const char *s) {\n\tstd::vector<std::string> ret;\n\n\twhile (*s && *s != '\\n' && *s != '\\r') {\n\t\tconst char *start = s;\n\t\tint within = 0;\n\n\t\tfor (; *s && *s != '\\n' && *s != '\\r'; s++) {\n\t\t\tif (*s == '\"') {\n\t\t\t\twithin = !within;\n\t\t\t}\n\n\t\t\tif (*s == ',' && !within) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tstd::string v = std::string(start, s - start);\n\t\tret.push_back(v);\n\n\t\tif (*s == ',') {\n\t\t\ts++;\n\n\t\t\twhile (*s && isspace(*s)) {\n\t\t\t\ts++;\n\t\t\t}\n\n\t\t\tif (*s == '\\0' || *s == '\\r' || *s == '\\n') {\n\t\t\t\tret.push_back(std::string(\"\"));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstd::string csv_dequote(std::string s) {\n\tstd::string out;\n\tfor (size_t i = 0; i < s.size(); i++) {\n\t\tif (s[i] == '\"') {\n\t\t\tif (i + 1 < s.size() && s[i + 1] == '\"') {\n\t\t\t\tout.push_back('\"');\n\t\t\t}\n\t\t} else {\n\t\t\tout.push_back(s[i]);\n\t\t}\n\t}\n\treturn out;\n}\n\nstd::string csv_getline(FILE *f) {\n\tstd::string out;\n\tint c;\n\twhile ((c = getc(f)) != EOF) {\n\t\tout.push_back(c);\n\t\tif (c == '\\n') {\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn out;\n}\n\nvoid readcsv(const char *fn, std::vector<std::string> &header, std::map<std::string, std::vector<std::string>> &mapping) {\n\tFILE *f = fopen(fn, \"r\");\n\tif (f == NULL) {\n\t\tperror(fn);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tstd::string s;\n\tif ((s = csv_getline(f)).size() > 0) {\n\t\tstd::string err = check_utf8(s);\n\t\tif (err != \"\") {\n\t\t\tfprintf(stderr, \"%s: %s\\n\", fn, err.c_str());\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\theader = csv_split(s.c_str());\n\n\t\tfor (size_t i = 0; i < header.size(); i++) {\n\t\t\theader[i] = csv_dequote(header[i]);\n\t\t}\n\t}\n\twhile ((s = csv_getline(f)).size() > 0) {\n\t\tstd::string err = check_utf8(s);\n\t\tif (err != \"\") {\n\t\t\tfprintf(stderr, \"%s: %s\\n\", fn, err.c_str());\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tstd::vector<std::string> line = csv_split(s.c_str());\n\t\tif (line.size() > 0) {\n\t\t\tline[0] = csv_dequote(line[0]);\n\t\t}\n\n\t\tfor (size_t i = 0; i < line.size() && i < header.size(); i++) {\n\t\t\t// printf(\"putting %s\\n\", line[0].c_str());\n\t\t\tmapping.insert(std::pair<std::string, std::vector<std::string>>(line[0], line));\n\t\t}\n\t}\n\n\tif (fclose(f) != 0) {\n\t\tperror(\"fclose\");\n\t\texit(EXIT_FAILURE);\n\t}\n}\n\n// Follow JSON rules for what looks like a number\nbool is_number(std::string const &s) {\n\tconst char *cp = s.c_str();\n\tchar c = *(cp++);\n\n\tif (c == '-' || (c >= '0' && c <= '9')) {\n\t\tif (c == '-') {\n\t\t\tc = *(cp++);\n\t\t}\n\n\t\tif (c == '0') {\n\t\t\t;\n\t\t} else if (c >= '1' && c <= '9') {\n\t\t\tc = *cp;\n\n\t\t\twhile (c >= '0' && c <= '9') {\n\t\t\t\tcp++;\n\t\t\t\tc = *cp;\n\t\t\t}\n\t\t}\n\n\t\tif (*cp == '.') {\n\t\t\tcp++;\n\n\t\t\tc = *cp;\n\t\t\tif (c < '0' || c > '9') {\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\twhile (c >= '0' && c <= '9') {\n\t\t\t\tcp++;\n\t\t\t\tc = *cp;\n\t\t\t}\n\t\t}\n\n\t\tc = *cp;\n\t\tif (c == 'e' || c == 'E') {\n\t\t\tcp++;\n\n\t\t\tc = *cp;\n\t\t\tif (c == '+' || c == '-') {\n\t\t\t\tcp++;\n\t\t\t}\n\n\t\t\tc = *cp;\n\t\t\tif (c < '0' || c > '9') {\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\twhile (c >= '0' && c <= '9') {\n\t\t\t\tcp++;\n\t\t\t\tc = *cp;\n\t\t\t}\n\t\t}\n\n\t\tif (*cp == '\\0') {\n\t\t\treturn true;\n\t\t} else {\n\t\t\t// Something non-numeric at the end\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn false;\n}\n"
        },
        {
          "name": "csv.hpp",
          "type": "blob",
          "size": 0.4072265625,
          "content": "#ifndef CSV_HPP\n#define CSV_HPP\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <vector>\n#include <string>\n#include <map>\n\nstd::vector<std::string> csv_split(const char *s);\nstd::string csv_dequote(std::string s);\nvoid readcsv(const char *fn, std::vector<std::string> &header, std::map<std::string, std::vector<std::string>> &mapping);\nstd::string csv_getline(FILE *f);\nbool is_number(std::string const &s);\n\n#endif\n"
        },
        {
          "name": "decode.cpp",
          "type": "blob",
          "size": 13.3955078125,
          "content": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <unistd.h>\n#include <sqlite3.h>\n#include <getopt.h>\n#include <string>\n#include <vector>\n#include <map>\n#include <set>\n#include <zlib.h>\n#include <math.h>\n#include <fcntl.h>\n#include <dirent.h>\n#include <sys/stat.h>\n#include <sys/mman.h>\n#include <protozero/pbf_reader.hpp>\n#include <sys/stat.h>\n#include \"mvt.hpp\"\n#include \"projection.hpp\"\n#include \"geometry.hpp\"\n#include \"write_json.hpp\"\n#include \"jsonpull/jsonpull.h\"\n#include \"dirtiles.hpp\"\n\nint minzoom = 0;\nint maxzoom = 32;\nbool force = false;\n\nvoid do_stats(mvt_tile &tile, size_t size, bool compressed, int z, unsigned x, unsigned y, json_writer &state) {\n\tstate.json_write_hash();\n\n\tstate.json_write_string(\"zoom\");\n\tstate.json_write_signed(z);\n\n\tstate.json_write_string(\"x\");\n\tstate.json_write_unsigned(x);\n\n\tstate.json_write_string(\"y\");\n\tstate.json_write_unsigned(y);\n\n\tstate.json_write_string(\"bytes\");\n\tstate.json_write_unsigned(size);\n\n\tstate.json_write_string(\"compressed\");\n\tstate.json_write_bool(compressed);\n\n\tstate.json_write_string(\"layers\");\n\tstate.json_write_hash();\n\n\tfor (size_t i = 0; i < tile.layers.size(); i++) {\n\t\tstate.json_write_string(tile.layers[i].name);\n\n\t\tsize_t points = 0, lines = 0, polygons = 0;\n\t\tfor (size_t j = 0; j < tile.layers[i].features.size(); j++) {\n\t\t\tif (tile.layers[i].features[j].type == mvt_point) {\n\t\t\t\tpoints++;\n\t\t\t} else if (tile.layers[i].features[j].type == mvt_linestring) {\n\t\t\t\tlines++;\n\t\t\t} else if (tile.layers[i].features[j].type == mvt_polygon) {\n\t\t\t\tpolygons++;\n\t\t\t}\n\t\t}\n\n\t\tstate.json_write_hash();\n\n\t\tstate.json_write_string(\"points\");\n\t\tstate.json_write_unsigned(points);\n\n\t\tstate.json_write_string(\"lines\");\n\t\tstate.json_write_unsigned(lines);\n\n\t\tstate.json_write_string(\"polygons\");\n\t\tstate.json_write_unsigned(polygons);\n\n\t\tstate.json_write_string(\"extent\");\n\t\tstate.json_write_signed(tile.layers[i].extent);\n\n\t\tstate.json_end_hash();\n\t}\n\n\tstate.json_end_hash();\n\tstate.json_end_hash();\n\n\tstate.json_write_newline();\n}\n\nvoid handle(std::string message, int z, unsigned x, unsigned y, std::set<std::string> const &to_decode, bool pipeline, bool stats, json_writer &state) {\n\tmvt_tile tile;\n\tbool was_compressed;\n\n\ttry {\n\t\tif (!tile.decode(message, was_compressed)) {\n\t\t\tfprintf(stderr, \"Couldn't parse tile %d/%u/%u\\n\", z, x, y);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t} catch (std::exception const &e) {\n\t\tfprintf(stderr, \"PBF decoding error in tile %d/%u/%u\\n\", z, x, y);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (stats) {\n\t\tdo_stats(tile, message.size(), was_compressed, z, x, y, state);\n\t\treturn;\n\t}\n\n\tif (!pipeline) {\n\t\tstate.json_write_hash();\n\n\t\tstate.json_write_string(\"type\");\n\t\tstate.json_write_string(\"FeatureCollection\");\n\n\t\tif (true) {\n\t\t\tstate.json_write_string(\"properties\");\n\t\t\tstate.json_write_hash();\n\n\t\t\tstate.json_write_string(\"zoom\");\n\t\t\tstate.json_write_signed(z);\n\n\t\t\tstate.json_write_string(\"x\");\n\t\t\tstate.json_write_signed(x);\n\n\t\t\tstate.json_write_string(\"y\");\n\t\t\tstate.json_write_signed(y);\n\n\t\t\tif (!was_compressed) {\n\t\t\t\tstate.json_write_string(\"compressed\");\n\t\t\t\tstate.json_write_bool(false);\n\t\t\t}\n\n\t\t\tstate.json_end_hash();\n\n\t\t\tif (projection != projections) {\n\t\t\t\tstate.json_write_string(\"crs\");\n\t\t\t\tstate.json_write_hash();\n\n\t\t\t\tstate.json_write_string(\"type\");\n\t\t\t\tstate.json_write_string(\"name\");\n\n\t\t\t\tstate.json_write_string(\"properties\");\n\t\t\t\tstate.json_write_hash();\n\n\t\t\t\tstate.json_write_string(\"name\");\n\t\t\t\tstate.json_write_string(projection->alias);\n\n\t\t\t\tstate.json_end_hash();\n\t\t\t\tstate.json_end_hash();\n\t\t\t}\n\t\t}\n\n\t\tstate.json_write_string(\"features\");\n\t\tstate.json_write_array();\n\t\tstate.json_write_newline();\n\t}\n\n\tbool first_layer = true;\n\tfor (size_t l = 0; l < tile.layers.size(); l++) {\n\t\tmvt_layer &layer = tile.layers[l];\n\n\t\tif (layer.extent <= 0) {\n\t\t\tfprintf(stderr, \"Impossible layer extent %lld in mbtiles\\n\", layer.extent);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tif (to_decode.size() != 0 && !to_decode.count(layer.name)) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!pipeline) {\n\t\t\tif (true) {\n\t\t\t\tif (!first_layer) {\n\t\t\t\t\tstate.json_comma_newline();\n\t\t\t\t}\n\n\t\t\t\tstate.json_write_hash();\n\n\t\t\t\tstate.json_write_string(\"type\");\n\t\t\t\tstate.json_write_string(\"FeatureCollection\");\n\n\t\t\t\tstate.json_write_string(\"properties\");\n\t\t\t\tstate.json_write_hash();\n\n\t\t\t\tstate.json_write_string(\"layer\");\n\t\t\t\tstate.json_write_string(layer.name);\n\n\t\t\t\tstate.json_write_string(\"version\");\n\t\t\t\tstate.json_write_signed(layer.version);\n\n\t\t\t\tstate.json_write_string(\"extent\");\n\t\t\t\tstate.json_write_signed(layer.extent);\n\n\t\t\t\tstate.json_end_hash();\n\n\t\t\t\tstate.json_write_string(\"features\");\n\t\t\t\tstate.json_write_array();\n\n\t\t\t\tstate.json_write_newline();\n\t\t\t\tfirst_layer = false;\n\t\t\t}\n\t\t}\n\n\t\t// X and Y are unsigned, so no need to check <0\n\t\tif (x > (1ULL << z) || y > (1ULL << z)) {\n\t\t\tfprintf(stderr, \"Impossible tile %d/%u/%u\\n\", z, x, y);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tlayer_to_geojson(layer, z, x, y, !pipeline, pipeline, pipeline, false, 0, 0, 0, !force, state);\n\n\t\tif (!pipeline) {\n\t\t\tif (true) {\n\t\t\t\tstate.json_end_array();\n\t\t\t\tstate.json_end_hash();\n\t\t\t\tstate.json_write_newline();\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!pipeline) {\n\t\tstate.json_end_array();\n\t\tstate.json_end_hash();\n\t\tstate.json_write_newline();\n\t}\n}\n\nvoid decode(char *fname, int z, unsigned x, unsigned y, std::set<std::string> const &to_decode, bool pipeline, bool stats, std::set<std::string> const &exclude_meta) {\n\tsqlite3 *db = NULL;\n\tbool isdir = false;\n\tint oz = z;\n\tunsigned ox = x, oy = y;\n\tjson_writer state(stdout);\n\n\tint fd = open(fname, O_RDONLY | O_CLOEXEC);\n\tif (fd >= 0) {\n\t\tstruct stat st;\n\t\tif (fstat(fd, &st) == 0) {\n\t\t\tif (st.st_size < 50 * 1024 * 1024) {\n\t\t\t\tchar *map = (char *) mmap(NULL, st.st_size, PROT_READ, MAP_PRIVATE, fd, 0);\n\t\t\t\tif (map != NULL && map != MAP_FAILED) {\n\t\t\t\t\tif (strcmp(map, \"SQLite format 3\") != 0) {\n\t\t\t\t\t\tif (z >= 0) {\n\t\t\t\t\t\t\tstd::string s = std::string(map, st.st_size);\n\t\t\t\t\t\t\thandle(s, z, x, y, to_decode, pipeline, stats, state);\n\t\t\t\t\t\t\tmunmap(map, st.st_size);\n\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tfprintf(stderr, \"Must specify zoom/x/y to decode a single pbf file\\n\");\n\t\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tmunmap(map, st.st_size);\n\t\t\t}\n\t\t} else {\n\t\t\tperror(\"fstat\");\n\t\t}\n\t\tif (close(fd) != 0) {\n\t\t\tperror(\"close\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t} else {\n\t\tperror(fname);\n\t}\n\n\tstruct stat st;\n\tstd::vector<zxy> tiles;\n\tif (stat(fname, &st) == 0 && (st.st_mode & S_IFDIR) != 0) {\n\t\tisdir = true;\n\n\t\tdb = dirmeta2tmp(fname);\n\t\ttiles = enumerate_dirtiles(fname, minzoom, maxzoom);\n\t} else {\n\t\tif (sqlite3_open(fname, &db) != SQLITE_OK) {\n\t\t\tfprintf(stderr, \"%s: %s\\n\", fname, sqlite3_errmsg(db));\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tchar *err = NULL;\n\t\tif (sqlite3_exec(db, \"PRAGMA integrity_check;\", NULL, NULL, &err) != SQLITE_OK) {\n\t\t\tfprintf(stderr, \"%s: integrity_check: %s\\n\", fname, err);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\tif (z < 0) {\n\t\tint within = 0;\n\n\t\tif (!pipeline && !stats) {\n\t\t\tstate.json_write_hash();\n\n\t\t\tstate.json_write_string(\"type\");\n\t\t\tstate.json_write_string(\"FeatureCollection\");\n\n\t\t\tstate.json_write_string(\"properties\");\n\t\t\tstate.json_write_hash();\n\t\t\tstate.json_write_newline();\n\n\t\t\tconst char *sql2 = \"SELECT name, value from metadata order by name;\";\n\t\t\tsqlite3_stmt *stmt2;\n\t\t\tif (sqlite3_prepare_v2(db, sql2, -1, &stmt2, NULL) != SQLITE_OK) {\n\t\t\t\tfprintf(stderr, \"%s: select failed: %s\\n\", fname, sqlite3_errmsg(db));\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\twhile (sqlite3_step(stmt2) == SQLITE_ROW) {\n\t\t\t\tconst unsigned char *name = sqlite3_column_text(stmt2, 0);\n\t\t\t\tconst unsigned char *value = sqlite3_column_text(stmt2, 1);\n\n\t\t\t\tif (name == NULL || value == NULL) {\n\t\t\t\t\tfprintf(stderr, \"Corrupt mbtiles file: null metadata\\n\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\n\t\t\t\tif (exclude_meta.count((char *) name) == 0) {\n\t\t\t\t\tif (within) {\n\t\t\t\t\t\tstate.json_comma_newline();\n\t\t\t\t\t}\n\t\t\t\t\twithin = 1;\n\n\t\t\t\t\tstate.json_write_string((char *) name);\n\t\t\t\t\tstate.json_write_string((char *) value);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tstate.json_write_newline();\n\t\t\tstate.wantnl = false;  // XXX\n\n\t\t\tsqlite3_finalize(stmt2);\n\t\t}\n\n\t\tif (stats) {\n\t\t\tstate.json_write_array();\n\t\t\tstate.json_write_newline();\n\t\t}\n\n\t\tif (!pipeline && !stats) {\n\t\t\tstate.json_end_hash();\n\n\t\t\tstate.json_write_string(\"features\");\n\t\t\tstate.json_write_array();\n\t\t\tstate.json_write_newline();\n\t\t}\n\n\t\tif (isdir) {\n\t\t\twithin = 0;\n\t\t\tfor (size_t i = 0; i < tiles.size(); i++) {\n\t\t\t\tif (!pipeline && !stats) {\n\t\t\t\t\tif (within) {\n\t\t\t\t\t\tstate.json_comma_newline();\n\t\t\t\t\t}\n\t\t\t\t\twithin = 1;\n\t\t\t\t}\n\t\t\t\tif (stats) {\n\t\t\t\t\tif (within) {\n\t\t\t\t\t\tstate.json_comma_newline();\n\t\t\t\t\t}\n\t\t\t\t\twithin = 1;\n\t\t\t\t}\n\n\t\t\t\tstd::string fn = std::string(fname) + \"/\" + tiles[i].path();\n\t\t\t\tFILE *f = fopen(fn.c_str(), \"rb\");\n\t\t\t\tif (f == NULL) {\n\t\t\t\t\tperror(fn.c_str());\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\n\t\t\t\tstd::string s;\n\t\t\t\tchar buf[2000];\n\t\t\t\tssize_t n;\n\t\t\t\twhile ((n = fread(buf, 1, 2000, f)) > 0) {\n\t\t\t\t\ts.append(std::string(buf, n));\n\t\t\t\t}\n\t\t\t\tfclose(f);\n\n\t\t\t\thandle(s, tiles[i].z, tiles[i].x, tiles[i].y, to_decode, pipeline, stats, state);\n\t\t\t}\n\t\t} else {\n\t\t\tconst char *sql = \"SELECT tile_data, zoom_level, tile_column, tile_row from tiles where zoom_level between ? and ? order by zoom_level, tile_column, tile_row;\";\n\t\t\tsqlite3_stmt *stmt;\n\t\t\tif (sqlite3_prepare_v2(db, sql, -1, &stmt, NULL) != SQLITE_OK) {\n\t\t\t\tfprintf(stderr, \"%s: select failed: %s\\n\", fname, sqlite3_errmsg(db));\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tsqlite3_bind_int(stmt, 1, minzoom);\n\t\t\tsqlite3_bind_int(stmt, 2, maxzoom);\n\n\t\t\twithin = 0;\n\t\t\twhile (sqlite3_step(stmt) == SQLITE_ROW) {\n\t\t\t\tif (!pipeline && !stats) {\n\t\t\t\t\tif (within) {\n\t\t\t\t\t\tstate.json_comma_newline();\n\t\t\t\t\t}\n\t\t\t\t\twithin = 1;\n\t\t\t\t}\n\t\t\t\tif (stats) {\n\t\t\t\t\tif (within) {\n\t\t\t\t\t\tstate.json_comma_newline();\n\t\t\t\t\t}\n\t\t\t\t\twithin = 1;\n\t\t\t\t}\n\n\t\t\t\tint len = sqlite3_column_bytes(stmt, 0);\n\t\t\t\tint tz = sqlite3_column_int(stmt, 1);\n\t\t\t\tint tx = sqlite3_column_int(stmt, 2);\n\t\t\t\tint ty = sqlite3_column_int(stmt, 3);\n\n\t\t\t\tif (tz < 0 || tz >= 32) {\n\t\t\t\t\tfprintf(stderr, \"Impossible zoom level %d in mbtiles\\n\", tz);\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\n\t\t\t\tty = (1LL << tz) - 1 - ty;\n\t\t\t\tconst char *s = (const char *) sqlite3_column_blob(stmt, 0);\n\n\t\t\t\tif (s == NULL) {\n\t\t\t\t\tfprintf(stderr, \"Corrupt mbtiles file: null entry in tiles table\\n\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\n\t\t\t\thandle(std::string(s, len), tz, tx, ty, to_decode, pipeline, stats, state);\n\t\t\t}\n\n\t\t\tsqlite3_finalize(stmt);\n\t\t}\n\n\t\tif (!pipeline && !stats) {\n\t\t\tstate.json_end_array();\n\t\t\tstate.json_end_hash();\n\t\t\tstate.json_write_newline();\n\t\t}\n\t\tif (stats) {\n\t\t\tstate.json_end_array();\n\t\t\tstate.json_write_newline();\n\t\t}\n\t\tif (pipeline) {\n\t\t\tstate.json_write_newline();\n\t\t}\n\t} else {\n\t\tint handled = 0;\n\t\twhile (z >= 0 && !handled) {\n\t\t\tconst char *sql = \"SELECT tile_data from tiles where zoom_level = ? and tile_column = ? and tile_row = ?;\";\n\t\t\tsqlite3_stmt *stmt;\n\t\t\tif (sqlite3_prepare_v2(db, sql, -1, &stmt, NULL) != SQLITE_OK) {\n\t\t\t\tfprintf(stderr, \"%s: select failed: %s\\n\", fname, sqlite3_errmsg(db));\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tsqlite3_bind_int(stmt, 1, z);\n\t\t\tsqlite3_bind_int(stmt, 2, x);\n\t\t\tsqlite3_bind_int(stmt, 3, (1LL << z) - 1 - y);\n\n\t\t\twhile (sqlite3_step(stmt) == SQLITE_ROW) {\n\t\t\t\tint len = sqlite3_column_bytes(stmt, 0);\n\t\t\t\tconst char *s = (const char *) sqlite3_column_blob(stmt, 0);\n\n\t\t\t\tif (s == NULL) {\n\t\t\t\t\tfprintf(stderr, \"Corrupt mbtiles file: null entry in tiles table\\n\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\n\t\t\t\tif (z != oz) {\n\t\t\t\t\tfprintf(stderr, \"%s: Warning: using tile %d/%u/%u instead of %d/%u/%u\\n\", fname, z, x, y, oz, ox, oy);\n\t\t\t\t}\n\n\t\t\t\thandle(std::string(s, len), z, x, y, to_decode, pipeline, stats, state);\n\t\t\t\thandled = 1;\n\t\t\t}\n\n\t\t\tsqlite3_finalize(stmt);\n\n\t\t\tz--;\n\t\t\tx /= 2;\n\t\t\ty /= 2;\n\t\t}\n\t}\n\n\tif (sqlite3_close(db) != SQLITE_OK) {\n\t\tfprintf(stderr, \"%s: could not close database: %s\\n\", fname, sqlite3_errmsg(db));\n\t\texit(EXIT_FAILURE);\n\t}\n}\n\nvoid usage(char **argv) {\n\tfprintf(stderr, \"Usage: %s [-s projection] [-Z minzoom] [-z maxzoom] [-l layer ...] file.mbtiles [zoom x y]\\n\", argv[0]);\n\texit(EXIT_FAILURE);\n}\n\nint main(int argc, char **argv) {\n\textern int optind;\n\textern char *optarg;\n\tint i;\n\tstd::set<std::string> to_decode;\n\tbool pipeline = false;\n\tbool stats = false;\n\tstd::set<std::string> exclude_meta;\n\n\tstruct option long_options[] = {\n\t\t{\"projection\", required_argument, 0, 's'},\n\t\t{\"maximum-zoom\", required_argument, 0, 'z'},\n\t\t{\"minimum-zoom\", required_argument, 0, 'Z'},\n\t\t{\"layer\", required_argument, 0, 'l'},\n\t\t{\"tag-layer-and-zoom\", no_argument, 0, 'c'},\n\t\t{\"stats\", no_argument, 0, 'S'},\n\t\t{\"force\", no_argument, 0, 'f'},\n\t\t{\"exclude-metadata-row\", required_argument, 0, 'x'},\n\t\t{0, 0, 0, 0},\n\t};\n\n\tstd::string getopt_str;\n\tfor (size_t lo = 0; long_options[lo].name != NULL; lo++) {\n\t\tif (long_options[lo].val > ' ') {\n\t\t\tgetopt_str.push_back(long_options[lo].val);\n\n\t\t\tif (long_options[lo].has_arg == required_argument) {\n\t\t\t\tgetopt_str.push_back(':');\n\t\t\t}\n\t\t}\n\t}\n\n\twhile ((i = getopt_long(argc, argv, getopt_str.c_str(), long_options, NULL)) != -1) {\n\t\tswitch (i) {\n\t\tcase 0:\n\t\t\tbreak;\n\n\t\tcase 's':\n\t\t\tset_projection_or_exit(optarg);\n\t\t\tbreak;\n\n\t\tcase 'z':\n\t\t\tmaxzoom = atoi(optarg);\n\t\t\tbreak;\n\n\t\tcase 'Z':\n\t\t\tminzoom = atoi(optarg);\n\t\t\tbreak;\n\n\t\tcase 'l':\n\t\t\tto_decode.insert(optarg);\n\t\t\tbreak;\n\n\t\tcase 'c':\n\t\t\tpipeline = true;\n\t\t\tbreak;\n\n\t\tcase 'S':\n\t\t\tstats = true;\n\t\t\tbreak;\n\n\t\tcase 'f':\n\t\t\tforce = true;\n\t\t\tbreak;\n\n\t\tcase 'x':\n\t\t\texclude_meta.insert(optarg);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tusage(argv);\n\t\t}\n\t}\n\n\tif (argc == optind + 4) {\n\t\tdecode(argv[optind], atoi(argv[optind + 1]), atoi(argv[optind + 2]), atoi(argv[optind + 3]), to_decode, pipeline, stats, exclude_meta);\n\t} else if (argc == optind + 1) {\n\t\tdecode(argv[optind], -1, -1, -1, to_decode, pipeline, stats, exclude_meta);\n\t} else {\n\t\tusage(argv);\n\t}\n\n\treturn 0;\n}\n"
        },
        {
          "name": "dirtiles.cpp",
          "type": "blob",
          "size": 5.169921875,
          "content": "#include <iostream>\n#include <fstream>\n#include <sstream>\n#include <string>\n#include <algorithm>\n#include <stdio.h>\n#include <string.h>\n#include <unistd.h>\n#include <dirent.h>\n#include <limits.h>\n#include <sys/stat.h>\n#include <sqlite3.h>\n#include \"jsonpull/jsonpull.h\"\n#include \"dirtiles.hpp\"\n\nstd::string dir_read_tile(std::string base, struct zxy tile) {\n\tstd::ifstream pbfFile(base + \"/\" + tile.path(), std::ios::in | std::ios::binary);\n\tstd::ostringstream contents;\n\tcontents << pbfFile.rdbuf();\n\tpbfFile.close();\n\n\treturn (contents.str());\n}\n\nvoid dir_write_tile(const char *outdir, int z, int tx, int ty, std::string const &pbf) {\n\tmkdir(outdir, S_IRWXU | S_IRWXG | S_IRWXO);\n\tstd::string curdir(outdir);\n\tstd::string slash(\"/\");\n\tstd::string newdir = curdir + slash + std::to_string(z);\n\tmkdir(newdir.c_str(), S_IRWXU | S_IRWXG | S_IRWXO);\n\tnewdir = newdir + \"/\" + std::to_string(tx);\n\tmkdir(newdir.c_str(), S_IRWXU | S_IRWXG | S_IRWXO);\n\tnewdir = newdir + \"/\" + std::to_string(ty) + \".pbf\";\n\n\tstruct stat st;\n\tif (stat(newdir.c_str(), &st) == 0) {\n\t\tfprintf(stderr, \"Can't write tile to already existing %s\\n\", newdir.c_str());\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tstd::ofstream pbfFile(newdir, std::ios::out | std::ios::binary);\n\tpbfFile.write(pbf.data(), pbf.size());\n\tpbfFile.close();\n}\n\nstatic bool numeric(const char *s) {\n\tif (*s == '\\0') {\n\t\treturn false;\n\t}\n\tfor (; *s != 0; s++) {\n\t\tif (*s < '0' || *s > '9') {\n\t\t\treturn false;\n\t\t}\n\t}\n\treturn true;\n}\n\nstatic bool pbfname(const char *s) {\n\twhile (*s >= '0' && *s <= '9') {\n\t\ts++;\n\t}\n\n\treturn strcmp(s, \".pbf\") == 0 || strcmp(s, \".mvt\") == 0;\n}\n\nvoid check_dir(const char *dir, char **argv, bool force, bool forcetable) {\n\tstruct stat st;\n\n\tmkdir(dir, S_IRWXU | S_IRWXG | S_IRWXO);\n\tstd::string meta = std::string(dir) + \"/\" + \"metadata.json\";\n\tif (force) {\n\t\tunlink(meta.c_str());  // error OK since it may not exist;\n\t} else {\n\t\tif (stat(meta.c_str(), &st) == 0) {\n\t\t\tfprintf(stderr, \"%s: Tileset \\\"%s\\\" already exists. You can use --force if you want to delete the old tileset.\\n\", argv[0], dir);\n\t\t\tfprintf(stderr, \"%s: %s: file exists\\n\", argv[0], meta.c_str());\n\t\t\tif (!forcetable) {\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (forcetable) {\n\t\t// Don't clear existing tiles\n\t\treturn;\n\t}\n\n\tstd::vector<zxy> tiles = enumerate_dirtiles(dir, INT_MIN, INT_MAX);\n\n\tfor (size_t i = 0; i < tiles.size(); i++) {\n\t\tstd::string fn = std::string(dir) + \"/\" + tiles[i].path();\n\n\t\tif (force) {\n\t\t\tif (unlink(fn.c_str()) != 0) {\n\t\t\t\tperror(fn.c_str());\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t} else {\n\t\t\tfprintf(stderr, \"%s: file exists\\n\", fn.c_str());\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n}\n\nstd::vector<zxy> enumerate_dirtiles(const char *fname, int minzoom, int maxzoom) {\n\tstd::vector<zxy> tiles;\n\n\tDIR *d1 = opendir(fname);\n\tif (d1 != NULL) {\n\t\tstruct dirent *dp;\n\t\twhile ((dp = readdir(d1)) != NULL) {\n\t\t\tif (numeric(dp->d_name) && atoi(dp->d_name) >= minzoom && atoi(dp->d_name) <= maxzoom) {\n\t\t\t\tstd::string z = std::string(fname) + \"/\" + dp->d_name;\n\t\t\t\tint tz = atoi(dp->d_name);\n\n\t\t\t\tDIR *d2 = opendir(z.c_str());\n\t\t\t\tif (d2 == NULL) {\n\t\t\t\t\tperror(z.c_str());\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\n\t\t\t\tstruct dirent *dp2;\n\t\t\t\twhile ((dp2 = readdir(d2)) != NULL) {\n\t\t\t\t\tif (numeric(dp2->d_name)) {\n\t\t\t\t\t\tstd::string x = z + \"/\" + dp2->d_name;\n\t\t\t\t\t\tint tx = atoi(dp2->d_name);\n\n\t\t\t\t\t\tDIR *d3 = opendir(x.c_str());\n\t\t\t\t\t\tif (d3 == NULL) {\n\t\t\t\t\t\t\tperror(x.c_str());\n\t\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tstruct dirent *dp3;\n\t\t\t\t\t\twhile ((dp3 = readdir(d3)) != NULL) {\n\t\t\t\t\t\t\tif (pbfname(dp3->d_name)) {\n\t\t\t\t\t\t\t\tint ty = atoi(dp3->d_name);\n\t\t\t\t\t\t\t\tzxy tile(tz, tx, ty);\n\t\t\t\t\t\t\t\tif (strstr(dp3->d_name, \".mvt\") != NULL) {\n\t\t\t\t\t\t\t\t\ttile.extension = \".mvt\";\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\ttiles.push_back(tile);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tclosedir(d3);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tclosedir(d2);\n\t\t\t}\n\t\t}\n\n\t\tclosedir(d1);\n\t}\n\n\tstd::sort(tiles.begin(), tiles.end());\n\treturn tiles;\n}\n\nsqlite3 *dirmeta2tmp(const char *fname) {\n\tsqlite3 *db;\n\tchar *err = NULL;\n\n\tif (sqlite3_open(\"\", &db) != SQLITE_OK) {\n\t\tfprintf(stderr, \"Temporary db: %s\\n\", sqlite3_errmsg(db));\n\t\texit(EXIT_FAILURE);\n\t}\n\tif (sqlite3_exec(db, \"CREATE TABLE metadata (name text, value text);\", NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"Create metadata table: %s\\n\", err);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tstd::string name = fname;\n\tname += \"/metadata.json\";\n\n\tFILE *f = fopen(name.c_str(), \"r\");\n\tif (f == NULL) {\n\t\tperror(name.c_str());\n\t} else {\n\t\tjson_pull *jp = json_begin_file(f);\n\t\tjson_object *o = json_read_tree(jp);\n\t\tif (o == NULL) {\n\t\t\tfprintf(stderr, \"%s: metadata parsing error: %s\\n\", name.c_str(), jp->error);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tif (o->type != JSON_HASH) {\n\t\t\tfprintf(stderr, \"%s: bad metadata format\\n\", name.c_str());\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tfor (size_t i = 0; i < o->length; i++) {\n\t\t\tif (o->keys[i]->type != JSON_STRING || o->values[i]->type != JSON_STRING) {\n\t\t\t\tfprintf(stderr, \"%s: non-string in metadata\\n\", name.c_str());\n\t\t\t}\n\n\t\t\tchar *sql = sqlite3_mprintf(\"INSERT INTO metadata (name, value) VALUES (%Q, %Q);\", o->keys[i]->string, o->values[i]->string);\n\t\t\tif (sqlite3_exec(db, sql, NULL, NULL, &err) != SQLITE_OK) {\n\t\t\t\tfprintf(stderr, \"set %s in metadata: %s\\n\", o->keys[i]->string, err);\n\t\t\t}\n\t\t\tsqlite3_free(sql);\n\t\t}\n\n\t\tjson_end(jp);\n\t\tfclose(f);\n\t}\n\n\treturn db;\n}\n"
        },
        {
          "name": "dirtiles.hpp",
          "type": "blob",
          "size": 0.9794921875,
          "content": "#include <string>\n#include <vector>\n#include <sys/stat.h>\n\n#ifndef DIRTILES_HPP\n#define DIRTILES_HPP\n\nvoid dir_write_tile(const char *outdir, int z, int tx, int ty, std::string const &pbf);\n\nvoid check_dir(const char *d, char **argv, bool force, bool forcetable);\n\nstruct zxy {\n\tlong long z;\n\tlong long x;\n\tlong long y;\n\tstd::string extension = \".pbf\";\n\n\tzxy(int _z, int _x, int _y)\n\t    : z(_z), x(_x), y(_y) {\n\t}\n\n\tbool operator<(const zxy &other) const {\n\t\tif (z < other.z) {\n\t\t\treturn true;\n\t\t}\n\t\tif (z == other.z) {\n\t\t\tif (x < other.x) {\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\tif (x == other.x) {\n\t\t\t\tif (y > other.y) {\n\t\t\t\t\treturn true;  // reversed for TMS\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn false;\n\t}\n\n\tstd::string path() {\n\t\treturn std::to_string(z) + \"/\" + std::to_string(x) + \"/\" + std::to_string(y) + extension;\n\t}\n};\n\nstd::vector<zxy> enumerate_dirtiles(const char *fname, int minzoom, int maxzoom);\nsqlite3 *dirmeta2tmp(const char *fname);\nstd::string dir_read_tile(std::string pbfPath, struct zxy tile);\n\n#endif\n"
        },
        {
          "name": "enumerate.cpp",
          "type": "blob",
          "size": 1.6708984375,
          "content": "#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <sqlite3.h>\n\nvoid enumerate(char *fname) {\n\tsqlite3 *db;\n\n\tif (sqlite3_open(fname, &db) != SQLITE_OK) {\n\t\tfprintf(stderr, \"%s: %s\\n\", fname, sqlite3_errmsg(db));\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tchar *err = NULL;\n\tif (sqlite3_exec(db, \"PRAGMA integrity_check;\", NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"%s: integrity_check: %s\\n\", fname, err);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tconst char *sql = \"SELECT zoom_level, tile_column, tile_row from tiles order by zoom_level, tile_column, tile_row;\";\n\n\tsqlite3_stmt *stmt;\n\tif (sqlite3_prepare_v2(db, sql, -1, &stmt, NULL) != SQLITE_OK) {\n\t\tfprintf(stderr, \"%s: select failed: %s\\n\", fname, sqlite3_errmsg(db));\n\t\texit(EXIT_FAILURE);\n\t}\n\n\twhile (sqlite3_step(stmt) == SQLITE_ROW) {\n\t\tlong long zoom = sqlite3_column_int(stmt, 0);\n\t\tlong long x = sqlite3_column_int(stmt, 1);\n\t\tlong long y = sqlite3_column_int(stmt, 2);\n\n\t\tif (zoom < 0 || zoom > 31) {\n\t\t\tfprintf(stderr, \"Corrupt mbtiles file: impossible zoom level %lld\\n\", zoom);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\ty = (1LL << zoom) - 1 - y;\n\t\tprintf(\"%s %lld %lld %lld\\n\", fname, zoom, x, y);\n\t}\n\n\tsqlite3_finalize(stmt);\n\n\tif (sqlite3_close(db) != SQLITE_OK) {\n\t\tfprintf(stderr, \"%s: could not close database: %s\\n\", fname, sqlite3_errmsg(db));\n\t\texit(EXIT_FAILURE);\n\t}\n}\n\nvoid usage(char **argv) {\n\tfprintf(stderr, \"Usage: %s file.mbtiles ...\\n\", argv[0]);\n\texit(EXIT_FAILURE);\n}\n\nint main(int argc, char **argv) {\n\textern int optind;\n\t// extern char *optarg;\n\tint i;\n\n\twhile ((i = getopt(argc, argv, \"\")) != -1) {\n\t\tusage(argv);\n\t}\n\n\tif (optind >= argc) {\n\t\tusage(argv);\n\t}\n\n\tfor (i = optind; i < argc; i++) {\n\t\tenumerate(argv[i]);\n\t}\n\n\treturn 0;\n}\n"
        },
        {
          "name": "evaluator.cpp",
          "type": "blob",
          "size": 8.6591796875,
          "content": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <map>\n#include \"mvt.hpp\"\n#include \"evaluator.hpp\"\n\nint compare(mvt_value one, json_object *two, bool &fail) {\n\tif (one.type == mvt_string) {\n\t\tif (two->type != JSON_STRING) {\n\t\t\tfail = true;\n\t\t\treturn false;  // string vs non-string\n\t\t}\n\n\t\treturn strcmp(one.string_value.c_str(), two->string);\n\t}\n\n\tif (one.type == mvt_double || one.type == mvt_float || one.type == mvt_int || one.type == mvt_uint || one.type == mvt_sint) {\n\t\tif (two->type != JSON_NUMBER) {\n\t\t\tfail = true;\n\t\t\treturn false;  // number vs non-number\n\t\t}\n\n\t\tdouble v;\n\t\tif (one.type == mvt_double) {\n\t\t\tv = one.numeric_value.double_value;\n\t\t} else if (one.type == mvt_float) {\n\t\t\tv = one.numeric_value.float_value;\n\t\t} else if (one.type == mvt_int) {\n\t\t\tv = one.numeric_value.int_value;\n\t\t} else if (one.type == mvt_uint) {\n\t\t\tv = one.numeric_value.uint_value;\n\t\t} else if (one.type == mvt_sint) {\n\t\t\tv = one.numeric_value.sint_value;\n\t\t} else {\n\t\t\tfprintf(stderr, \"Internal error: bad mvt type %d\\n\", one.type);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tif (v < two->number) {\n\t\t\treturn -1;\n\t\t} else if (v > two->number) {\n\t\t\treturn 1;\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (one.type == mvt_bool) {\n\t\tif (two->type != JSON_TRUE && two->type != JSON_FALSE) {\n\t\t\tfail = true;\n\t\t\treturn false;  // bool vs non-bool\n\t\t}\n\n\t\tbool b = two->type != JSON_FALSE;\n\t\treturn one.numeric_value.bool_value > b;\n\t}\n\n\tif (one.type == mvt_null) {\n\t\tif (two->type != JSON_NULL) {\n\t\t\tfail = true;\n\t\t\treturn false;  // null vs non-null\n\t\t}\n\n\t\treturn 0;  // null equals null\n\t}\n\n\tfprintf(stderr, \"Internal error: bad mvt type %d\\n\", one.type);\n\texit(EXIT_FAILURE);\n}\n\nbool eval(std::map<std::string, mvt_value> const &feature, json_object *f, std::set<std::string> &exclude_attributes) {\n\tif (f == NULL || f->type != JSON_ARRAY) {\n\t\tfprintf(stderr, \"Filter is not an array: %s\\n\", json_stringify(f));\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (f->length < 1) {\n\t\tfprintf(stderr, \"Array too small in filter: %s\\n\", json_stringify(f));\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (f->array[0]->type != JSON_STRING) {\n\t\tfprintf(stderr, \"Filter operation is not a string: %s\\n\", json_stringify(f));\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (strcmp(f->array[0]->string, \"has\") == 0 ||\n\t    strcmp(f->array[0]->string, \"!has\") == 0) {\n\t\tif (f->length != 2) {\n\t\t\tfprintf(stderr, \"Wrong number of array elements in filter: %s\\n\", json_stringify(f));\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tif (strcmp(f->array[0]->string, \"has\") == 0) {\n\t\t\tif (f->array[1]->type != JSON_STRING) {\n\t\t\t\tfprintf(stderr, \"\\\"has\\\" key is not a string: %s\\n\", json_stringify(f));\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\treturn feature.count(std::string(f->array[1]->string)) != 0;\n\t\t}\n\n\t\tif (strcmp(f->array[0]->string, \"!has\") == 0) {\n\t\t\tif (f->array[1]->type != JSON_STRING) {\n\t\t\t\tfprintf(stderr, \"\\\"!has\\\" key is not a string: %s\\n\", json_stringify(f));\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\treturn feature.count(std::string(f->array[1]->string)) == 0;\n\t\t}\n\t}\n\n\tif (strcmp(f->array[0]->string, \"==\") == 0 ||\n\t    strcmp(f->array[0]->string, \"!=\") == 0 ||\n\t    strcmp(f->array[0]->string, \">\") == 0 ||\n\t    strcmp(f->array[0]->string, \">=\") == 0 ||\n\t    strcmp(f->array[0]->string, \"<\") == 0 ||\n\t    strcmp(f->array[0]->string, \"<=\") == 0) {\n\t\tif (f->length != 3) {\n\t\t\tfprintf(stderr, \"Wrong number of array elements in filter: %s\\n\", json_stringify(f));\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (f->array[1]->type != JSON_STRING) {\n\t\t\tfprintf(stderr, \"\\\"!has\\\" key is not a string: %s\\n\", json_stringify(f));\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tauto ff = feature.find(std::string(f->array[1]->string));\n\t\tif (ff == feature.end()) {\n\t\t\tstatic bool warned = false;\n\t\t\tif (!warned) {\n\t\t\t\tconst char *s = json_stringify(f);\n\t\t\t\tfprintf(stderr, \"Warning: attribute not found for comparison: %s\\n\", s);\n\t\t\t\tfree((void *) s);\n\t\t\t\twarned = true;\n\t\t\t}\n\t\t\tif (strcmp(f->array[0]->string, \"!=\") == 0) {\n\t\t\t\treturn true;  //  attributes that aren't found are not equal\n\t\t\t}\n\t\t\treturn false;  // not found: comparison is false\n\t\t}\n\n\t\tbool fail = false;\n\t\tint cmp = compare(ff->second, f->array[2], fail);\n\n\t\tif (fail) {\n\t\t\tstatic bool warned = false;\n\t\t\tif (!warned) {\n\t\t\t\tconst char *s = json_stringify(f);\n\t\t\t\tfprintf(stderr, \"Warning: mismatched type in comparison: %s\\n\", s);\n\t\t\t\tfree((void *) s);\n\t\t\t\twarned = true;\n\t\t\t}\n\t\t\tif (strcmp(f->array[0]->string, \"!=\") == 0) {\n\t\t\t\treturn true;  // mismatched types are not equal\n\t\t\t}\n\t\t\treturn false;\n\t\t}\n\n\t\tif (strcmp(f->array[0]->string, \"==\") == 0) {\n\t\t\treturn cmp == 0;\n\t\t}\n\t\tif (strcmp(f->array[0]->string, \"!=\") == 0) {\n\t\t\treturn cmp != 0;\n\t\t}\n\t\tif (strcmp(f->array[0]->string, \">\") == 0) {\n\t\t\treturn cmp > 0;\n\t\t}\n\t\tif (strcmp(f->array[0]->string, \">=\") == 0) {\n\t\t\treturn cmp >= 0;\n\t\t}\n\t\tif (strcmp(f->array[0]->string, \"<\") == 0) {\n\t\t\treturn cmp < 0;\n\t\t}\n\t\tif (strcmp(f->array[0]->string, \"<=\") == 0) {\n\t\t\treturn cmp <= 0;\n\t\t}\n\n\t\tfprintf(stderr, \"Internal error: can't happen: %s\\n\", json_stringify(f));\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (strcmp(f->array[0]->string, \"all\") == 0 ||\n\t    strcmp(f->array[0]->string, \"any\") == 0 ||\n\t    strcmp(f->array[0]->string, \"none\") == 0) {\n\t\tbool v;\n\n\t\tif (strcmp(f->array[0]->string, \"all\") == 0) {\n\t\t\tv = true;\n\t\t} else {\n\t\t\tv = false;\n\t\t}\n\n\t\tfor (size_t i = 1; i < f->length; i++) {\n\t\t\tbool out = eval(feature, f->array[i], exclude_attributes);\n\n\t\t\tif (strcmp(f->array[0]->string, \"all\") == 0) {\n\t\t\t\tv = v && out;\n\t\t\t\tif (!v) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tv = v || out;\n\t\t\t\tif (v) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (strcmp(f->array[0]->string, \"none\") == 0) {\n\t\t\treturn !v;\n\t\t} else {\n\t\t\treturn v;\n\t\t}\n\t}\n\n\tif (strcmp(f->array[0]->string, \"in\") == 0 ||\n\t    strcmp(f->array[0]->string, \"!in\") == 0) {\n\t\tif (f->length < 2) {\n\t\t\tfprintf(stderr, \"Array too small in filter: %s\\n\", json_stringify(f));\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tif (f->array[1]->type != JSON_STRING) {\n\t\t\tfprintf(stderr, \"\\\"!has\\\" key is not a string: %s\\n\", json_stringify(f));\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tauto ff = feature.find(std::string(f->array[1]->string));\n\t\tif (ff == feature.end()) {\n\t\t\tstatic bool warned = false;\n\t\t\tif (!warned) {\n\t\t\t\tconst char *s = json_stringify(f);\n\t\t\t\tfprintf(stderr, \"Warning: attribute not found for comparison: %s\\n\", s);\n\t\t\t\tfree((void *) s);\n\t\t\t\twarned = true;\n\t\t\t}\n\t\t\tif (strcmp(f->array[0]->string, \"!in\") == 0) {\n\t\t\t\treturn true;  // attributes that aren't found are not in\n\t\t\t}\n\t\t\treturn false;  // not found: comparison is false\n\t\t}\n\n\t\tbool found = false;\n\t\tfor (size_t i = 2; i < f->length; i++) {\n\t\t\tbool fail = false;\n\t\t\tint cmp = compare(ff->second, f->array[i], fail);\n\n\t\t\tif (fail) {\n\t\t\t\tstatic bool warned = false;\n\t\t\t\tif (!warned) {\n\t\t\t\t\tconst char *s = json_stringify(f);\n\t\t\t\t\tfprintf(stderr, \"Warning: mismatched type in comparison: %s\\n\", s);\n\t\t\t\t\tfree((void *) s);\n\t\t\t\t\twarned = true;\n\t\t\t\t}\n\t\t\t\tcmp = 1;\n\t\t\t}\n\n\t\t\tif (cmp == 0) {\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (strcmp(f->array[0]->string, \"in\") == 0) {\n\t\t\treturn found;\n\t\t} else {\n\t\t\treturn !found;\n\t\t}\n\t}\n\n\tif (strcmp(f->array[0]->string, \"attribute-filter\") == 0) {\n\t\tif (f->length != 3) {\n\t\t\tfprintf(stderr, \"Wrong number of array elements in filter: %s\\n\", json_stringify(f));\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tif (f->array[1]->type != JSON_STRING) {\n\t\t\tfprintf(stderr, \"\\\"attribute-filter\\\" key is not a string: %s\\n\", json_stringify(f));\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tbool ok = eval(feature, f->array[2], exclude_attributes);\n\t\tif (!ok) {\n\t\t\texclude_attributes.insert(f->array[1]->string);\n\t\t}\n\n\t\treturn true;\n\t}\n\n\tfprintf(stderr, \"Unknown filter %s\\n\", json_stringify(f));\n\texit(EXIT_FAILURE);\n}\n\nbool evaluate(std::map<std::string, mvt_value> const &feature, std::string const &layer, json_object *filter, std::set<std::string> &exclude_attributes) {\n\tif (filter == NULL || filter->type != JSON_HASH) {\n\t\tfprintf(stderr, \"Error: filter is not a hash: %s\\n\", json_stringify(filter));\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tbool ok = true;\n\tjson_object *f;\n\n\tf = json_hash_get(filter, layer.c_str());\n\tif (ok && f != NULL) {\n\t\tok = eval(feature, f, exclude_attributes);\n\t}\n\n\tf = json_hash_get(filter, \"*\");\n\tif (ok && f != NULL) {\n\t\tok = eval(feature, f, exclude_attributes);\n\t}\n\n\treturn ok;\n}\n\njson_object *read_filter(const char *fname) {\n\tFILE *fp = fopen(fname, \"r\");\n\tif (fp == NULL) {\n\t\tperror(fname);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tjson_pull *jp = json_begin_file(fp);\n\tjson_object *filter = json_read_tree(jp);\n\tif (filter == NULL) {\n\t\tfprintf(stderr, \"%s: %s\\n\", fname, jp->error);\n\t\texit(EXIT_FAILURE);\n\t}\n\tjson_disconnect(filter);\n\tjson_end(jp);\n\tfclose(fp);\n\treturn filter;\n}\n\njson_object *parse_filter(const char *s) {\n\tjson_pull *jp = json_begin_string(s);\n\tjson_object *filter = json_read_tree(jp);\n\tif (filter == NULL) {\n\t\tfprintf(stderr, \"Could not parse filter %s\\n\", s);\n\t\tfprintf(stderr, \"%s\\n\", jp->error);\n\t\texit(EXIT_FAILURE);\n\t}\n\tjson_disconnect(filter);\n\tjson_end(jp);\n\treturn filter;\n}\n"
        },
        {
          "name": "evaluator.hpp",
          "type": "blob",
          "size": 0.3837890625,
          "content": "#ifndef EVALUATOR_HPP\n#define EVALUATOR HPP\n\n#include <map>\n#include <string>\n#include <set>\n#include \"jsonpull/jsonpull.h\"\n#include \"mvt.hpp\"\n\nbool evaluate(std::map<std::string, mvt_value> const &feature, std::string const &layer, json_object *filter, std::set<std::string> &exclude_attributes);\njson_object *parse_filter(const char *s);\njson_object *read_filter(const char *fname);\n\n#endif\n"
        },
        {
          "name": "filters",
          "type": "tree",
          "content": null
        },
        {
          "name": "geobuf.cpp",
          "type": "blob",
          "size": 13.357421875,
          "content": "#include <stdio.h>\n#include <string>\n#include <limits.h>\n#include <pthread.h>\n#include \"mvt.hpp\"\n#include \"serial.hpp\"\n#include \"geobuf.hpp\"\n#include \"geojson.hpp\"\n#include \"projection.hpp\"\n#include \"main.hpp\"\n#include \"protozero/varint.hpp\"\n#include \"protozero/pbf_reader.hpp\"\n#include \"protozero/pbf_writer.hpp\"\n#include \"milo/dtoa_milo.h\"\n#include \"jsonpull/jsonpull.h\"\n#include \"text.hpp\"\n\n#define POINT 0\n#define MULTIPOINT 1\n#define LINESTRING 2\n#define MULTILINESTRING 3\n#define POLYGON 4\n#define MULTIPOLYGON 5\n\nstruct queued_feature {\n\tprotozero::pbf_reader pbf{};\n\tsize_t dim = 0;\n\tdouble e = 0;\n\tstd::vector<std::string> *keys = NULL;\n\tstd::vector<struct serialization_state> *sst = NULL;\n\tint layer = 0;\n\tstd::string layername = \"\";\n};\n\nstatic std::vector<queued_feature> feature_queue;\n\nvoid ensureDim(size_t dim) {\n\tif (dim < 2) {\n\t\tfprintf(stderr, \"Geometry has fewer than 2 dimensions: %zu\\n\", dim);\n\t\texit(EXIT_FAILURE);\n\t}\n}\n\nserial_val readValue(protozero::pbf_reader &pbf) {\n\tserial_val sv;\n\tsv.type = mvt_null;\n\tsv.s = \"null\";\n\n\twhile (pbf.next()) {\n\t\tswitch (pbf.tag()) {\n\t\tcase 1:\n\t\t\tsv.type = mvt_string;\n\t\t\tsv.s = pbf.get_string();\n\t\t\tbreak;\n\n\t\tcase 2:\n\t\t\tsv.type = mvt_double;\n\t\t\tsv.s = milo::dtoa_milo(pbf.get_double());\n\t\t\tbreak;\n\n\t\tcase 3:\n\t\t\tsv.type = mvt_double;\n\t\t\tsv.s = std::to_string(pbf.get_uint64());\n\t\t\tbreak;\n\n\t\tcase 4:\n\t\t\tsv.type = mvt_double;\n\t\t\tsv.s = std::to_string(-(long long) pbf.get_uint64());\n\t\t\tbreak;\n\n\t\tcase 5:\n\t\t\tsv.type = mvt_bool;\n\t\t\tif (pbf.get_bool()) {\n\t\t\t\tsv.s = \"true\";\n\t\t\t} else {\n\t\t\t\tsv.s = \"false\";\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 6:\n\t\t\tsv.type = mvt_string;  // stringified JSON\n\t\t\tsv.s = pbf.get_string();\n\n\t\t\tif (sv.s == \"null\") {\n\t\t\t\tsv.type = mvt_null;\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tpbf.skip();\n\t\t}\n\t}\n\n\treturn sv;\n}\n\ndrawvec readPoint(std::vector<long long> &coords, size_t dim, double e) {\n\tensureDim(dim);\n\n\tlong long x, y;\n\tprojection->project(coords[0] / e, coords[1] / e, 32, &x, &y);\n\tdrawvec dv;\n\tdv.push_back(draw(VT_MOVETO, x, y));\n\treturn dv;\n}\n\ndrawvec readLinePart(std::vector<long long> &coords, size_t dim, double e, size_t start, size_t end, bool closed) {\n\tensureDim(dim);\n\n\tdrawvec dv;\n\tstd::vector<long long> prev;\n\tstd::vector<double> p;\n\tprev.resize(dim);\n\tp.resize(dim);\n\n\tfor (size_t i = start; i + dim - 1 < end; i += dim) {\n\t\tif (i + dim - 1 >= coords.size()) {\n\t\t\tfprintf(stderr, \"Internal error: line segment %zu vs %zu\\n\", i + dim - 1, coords.size());\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tfor (size_t d = 0; d < dim; d++) {\n\t\t\tprev[d] += coords[i + d];\n\t\t\tp[d] = prev[d] / e;\n\t\t}\n\n\t\tlong long x, y;\n\t\tprojection->project(p[0], p[1], 32, &x, &y);\n\n\t\tif (i == start) {\n\t\t\tdv.push_back(draw(VT_MOVETO, x, y));\n\t\t} else {\n\t\t\tdv.push_back(draw(VT_LINETO, x, y));\n\t\t}\n\t}\n\n\tif (closed && dv.size() > 0) {\n\t\tdv.push_back(draw(VT_LINETO, dv[0].x, dv[0].y));\n\t}\n\n\treturn dv;\n}\n\ndrawvec readLine(std::vector<long long> &coords, size_t dim, double e, bool closed) {\n\treturn readLinePart(coords, dim, e, 0, coords.size(), closed);\n}\n\ndrawvec readMultiLine(std::vector<long long> &coords, std::vector<int> &lengths, size_t dim, double e, bool closed) {\n\tif (lengths.size() == 0) {\n\t\treturn readLinePart(coords, dim, e, 0, coords.size(), closed);\n\t}\n\n\tdrawvec dv;\n\tsize_t here = 0;\n\tfor (size_t i = 0; i < lengths.size(); i++) {\n\t\tdrawvec dv2 = readLinePart(coords, dim, e, here, here + lengths[i] * dim, closed);\n\t\there += lengths[i] * dim;\n\n\t\tfor (size_t j = 0; j < dv2.size(); j++) {\n\t\t\tdv.push_back(dv2[j]);\n\t\t}\n\t}\n\n\treturn dv;\n}\n\ndrawvec readMultiPolygon(std::vector<long long> &coords, std::vector<int> &lengths, size_t dim, double e) {\n\tensureDim(dim);\n\n\tif (lengths.size() == 0) {\n\t\treturn readLinePart(coords, dim, e, 0, coords.size(), true);\n\t}\n\n\tsize_t polys = lengths[0];\n\tsize_t n = 1;\n\tsize_t here = 0;\n\tdrawvec dv;\n\n\tfor (size_t i = 0; i < polys; i++) {\n\t\tsize_t rings = lengths[n++];\n\n\t\tfor (size_t j = 0; j < rings; j++) {\n\t\t\tdrawvec dv2 = readLinePart(coords, dim, e, here, here + lengths[n] * dim, true);\n\t\t\there += lengths[n] * dim;\n\t\t\tn++;\n\n\t\t\tfor (size_t k = 0; k < dv2.size(); k++) {\n\t\t\t\tdv.push_back(dv2[k]);\n\t\t\t}\n\t\t}\n\n\t\tdv.push_back(draw(VT_CLOSEPATH, 0, 0));  // mark that the next ring is outer\n\t}\n\n\treturn dv;\n}\n\nstruct drawvec_type {\n\tdrawvec dv{};\n\tint type = 0;\n};\n\nstd::vector<drawvec_type> readGeometry(protozero::pbf_reader &pbf, size_t dim, double e, std::vector<std::string> &keys) {\n\tstd::vector<drawvec_type> ret;\n\tstd::vector<long long> coords;\n\tstd::vector<int> lengths;\n\tint type = -1;\n\n\twhile (pbf.next()) {\n\t\tswitch (pbf.tag()) {\n\t\tcase 1:\n\t\t\ttype = pbf.get_enum();\n\t\t\tbreak;\n\n\t\tcase 2: {\n\t\t\tauto pi = pbf.get_packed_uint32();\n\t\t\tfor (auto it = pi.first; it != pi.second; ++it) {\n\t\t\t\tlengths.push_back(*it);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tcase 3: {\n\t\t\tauto pi = pbf.get_packed_sint64();\n\t\t\tfor (auto it = pi.first; it != pi.second; ++it) {\n\t\t\t\tcoords.push_back(*it);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tcase 4: {\n\t\t\tprotozero::pbf_reader geometry_reader(pbf.get_message());\n\t\t\tstd::vector<drawvec_type> dv2 = readGeometry(geometry_reader, dim, e, keys);\n\n\t\t\tfor (size_t i = 0; i < dv2.size(); i++) {\n\t\t\t\tret.push_back(dv2[i]);\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\n\t\tdefault:\n\t\t\tpbf.skip();\n\t\t}\n\t}\n\n\tdrawvec_type dv;\n\tif (type == POINT) {\n\t\tdv.dv = readPoint(coords, dim, e);\n\t} else if (type == MULTIPOINT) {\n\t\tdv.dv = readLine(coords, dim, e, false);\n\t} else if (type == LINESTRING) {\n\t\tdv.dv = readLine(coords, dim, e, false);\n\t} else if (type == POLYGON) {\n\t\tdv.dv = readMultiLine(coords, lengths, dim, e, true);\n\t} else if (type == MULTILINESTRING) {\n\t\tdv.dv = readMultiLine(coords, lengths, dim, e, false);\n\t} else if (type == MULTIPOLYGON) {\n\t\tdv.dv = readMultiPolygon(coords, lengths, dim, e);\n\t} else {\n\t\t// GeometryCollection\n\t\treturn ret;\n\t}\n\n\tdv.type = type / 2 + 1;\n\tret.push_back(dv);\n\treturn ret;\n}\n\nvoid readFeature(protozero::pbf_reader &pbf, size_t dim, double e, std::vector<std::string> &keys, struct serialization_state *sst, int layer, std::string layername) {\n\tstd::vector<drawvec_type> dv;\n\tlong long id = 0;\n\tbool has_id = false;\n\tstd::vector<serial_val> values;\n\tstd::map<std::string, serial_val> other;\n\n\tstd::vector<std::string> full_keys;\n\tstd::vector<serial_val> full_values;\n\n\twhile (pbf.next()) {\n\t\tswitch (pbf.tag()) {\n\t\tcase 1: {\n\t\t\tprotozero::pbf_reader geometry_reader(pbf.get_message());\n\t\t\tstd::vector<drawvec_type> dv2 = readGeometry(geometry_reader, dim, e, keys);\n\t\t\tfor (size_t i = 0; i < dv2.size(); i++) {\n\t\t\t\tdv.push_back(dv2[i]);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tcase 11: {\n\t\t\tstatic bool warned = false;\n\t\t\tif (!warned) {\n\t\t\t\tfprintf(stderr, \"Non-numeric feature IDs not supported\\n\");\n\t\t\t\twarned = true;\n\t\t\t}\n\t\t\tpbf.skip();\n\t\t\tbreak;\n\t\t}\n\n\t\tcase 12:\n\t\t\thas_id = true;\n\t\t\tid = pbf.get_sint64();\n\t\t\tif (id < 0) {\n\t\t\t\tstatic bool warned = false;\n\t\t\t\tif (!warned) {\n\t\t\t\t\tfprintf(stderr, \"Out of range feature id %lld\\n\", id);\n\t\t\t\t\twarned = true;\n\t\t\t\t}\n\t\t\t\thas_id = false;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 13: {\n\t\t\tprotozero::pbf_reader value_reader(pbf.get_message());\n\t\t\tvalues.push_back(readValue(value_reader));\n\t\t\tbreak;\n\t\t}\n\n\t\tcase 14: {\n\t\t\tstd::vector<size_t> properties;\n\t\t\tauto pi = pbf.get_packed_uint32();\n\t\t\tfor (auto it = pi.first; it != pi.second; ++it) {\n\t\t\t\tproperties.push_back(*it);\n\t\t\t}\n\n\t\t\tfor (size_t i = 0; i + 1 < properties.size(); i += 2) {\n\t\t\t\tif (properties[i] >= keys.size()) {\n\t\t\t\t\tfprintf(stderr, \"Out of bounds key: %zu in %zu\\n\", properties[i], keys.size());\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\n\t\t\t\tif (properties[i + 1] >= values.size()) {\n\t\t\t\t\tfprintf(stderr, \"Out of bounds value: %zu in %zu\\n\", properties[i + 1], values.size());\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\n\t\t\t\tfull_keys.push_back(keys[properties[i]]);\n\t\t\t\tfull_values.push_back(values[properties[i + 1]]);\n\t\t\t}\n\n\t\t\tvalues.clear();\n\t\t\tbreak;\n\t\t}\n\n\t\tcase 15: {\n\t\t\tstd::vector<size_t> misc;\n\t\t\tauto pi = pbf.get_packed_uint32();\n\t\t\tfor (auto it = pi.first; it != pi.second; ++it) {\n\t\t\t\tmisc.push_back(*it);\n\t\t\t}\n\n\t\t\tfor (size_t i = 0; i + 1 < misc.size(); i += 2) {\n\t\t\t\tif (misc[i] >= keys.size()) {\n\t\t\t\t\tfprintf(stderr, \"Out of bounds key: %zu in %zu\\n\", misc[i], keys.size());\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\n\t\t\t\tif (misc[i + 1] >= values.size()) {\n\t\t\t\t\tfprintf(stderr, \"Out of bounds value: %zu in %zu\\n\", misc[i + 1], values.size());\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\n\t\t\t\tother.insert(std::pair<std::string, serial_val>(keys[misc[i]], values[misc[i + 1]]));\n\t\t\t}\n\n\t\t\tvalues.clear();\n\t\t\tbreak;\n\t\t}\n\n\t\tdefault:\n\t\t\tpbf.skip();\n\t\t}\n\t}\n\n\tfor (size_t i = 0; i < dv.size(); i++) {\n\t\tserial_feature sf;\n\n\t\tsf.layer = layer;\n\t\tsf.layername = layername;\n\t\tsf.segment = sst->segment;\n\t\tsf.has_id = has_id;\n\t\tsf.id = id;\n\t\tsf.has_tippecanoe_minzoom = false;\n\t\tsf.has_tippecanoe_maxzoom = false;\n\t\tsf.feature_minzoom = false;\n\t\tsf.seq = *(sst->layer_seq);\n\t\tsf.geometry = dv[i].dv;\n\t\tsf.t = dv[i].type;\n\t\tsf.full_keys = full_keys;\n\t\tsf.full_values = full_values;\n\n\t\tauto tip = other.find(\"tippecanoe\");\n\t\tif (tip != other.end()) {\n\t\t\tjson_pull *jp = json_begin_string(tip->second.s.c_str());\n\t\t\tjson_object *o = json_read_tree(jp);\n\n\t\t\tif (o != NULL) {\n\t\t\t\tjson_object *min = json_hash_get(o, \"minzoom\");\n\t\t\t\tif (min != NULL && (min->type == JSON_STRING || min->type == JSON_NUMBER)) {\n\t\t\t\t\tsf.has_tippecanoe_minzoom = true;\n\t\t\t\t\tsf.tippecanoe_minzoom = integer_zoom(sst->fname, min->string);\n\t\t\t\t}\n\n\t\t\t\tjson_object *max = json_hash_get(o, \"maxzoom\");\n\t\t\t\tif (max != NULL && (max->type == JSON_STRING || max->type == JSON_NUMBER)) {\n\t\t\t\t\tsf.has_tippecanoe_maxzoom = true;\n\t\t\t\t\tsf.tippecanoe_maxzoom = integer_zoom(sst->fname, max->string);\n\t\t\t\t}\n\n\t\t\t\tjson_object *tlayer = json_hash_get(o, \"layer\");\n\t\t\t\tif (tlayer != NULL && (tlayer->type == JSON_STRING || tlayer->type == JSON_NUMBER)) {\n\t\t\t\t\tsf.layername = tlayer->string;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tjson_free(o);\n\t\t\tjson_end(jp);\n\t\t}\n\n\t\tserialize_feature(sst, sf);\n\t}\n}\n\nstruct queue_run_arg {\n\tsize_t start;\n\tsize_t end;\n\tsize_t segment;\n\n\tqueue_run_arg(size_t start1, size_t end1, size_t segment1)\n\t    : start(start1), end(end1), segment(segment1) {\n\t}\n};\n\nvoid *run_parse_feature(void *v) {\n\tstruct queue_run_arg *qra = (struct queue_run_arg *) v;\n\n\tfor (size_t i = qra->start; i < qra->end; i++) {\n\t\tstruct queued_feature &qf = feature_queue[i];\n\t\treadFeature(qf.pbf, qf.dim, qf.e, *qf.keys, &(*qf.sst)[qra->segment], qf.layer, qf.layername);\n\t}\n\n\treturn NULL;\n}\n\nvoid runQueue() {\n\tif (feature_queue.size() == 0) {\n\t\treturn;\n\t}\n\n\tstd::vector<struct queue_run_arg> qra;\n\n\tstd::vector<pthread_t> pthreads;\n\tpthreads.resize(CPUS);\n\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\t*((*(feature_queue[0].sst))[i].layer_seq) = *((*(feature_queue[0].sst))[0].layer_seq) + feature_queue.size() * i / CPUS;\n\n\t\tqra.push_back(queue_run_arg(\n\t\t\tfeature_queue.size() * i / CPUS,\n\t\t\tfeature_queue.size() * (i + 1) / CPUS,\n\t\t\ti));\n\t}\n\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\tif (pthread_create(&pthreads[i], NULL, run_parse_feature, &qra[i]) != 0) {\n\t\t\tperror(\"pthread_create\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\tvoid *retval;\n\n\t\tif (pthread_join(pthreads[i], &retval) != 0) {\n\t\t\tperror(\"pthread_join\");\n\t\t}\n\t}\n\n\t// Lack of atomicity is OK, since we are single-threaded again here\n\tlong long was = *((*(feature_queue[0].sst))[CPUS - 1].layer_seq);\n\t*((*(feature_queue[0].sst))[0].layer_seq) = was;\n\tfeature_queue.clear();\n}\n\nvoid queueFeature(protozero::pbf_reader &pbf, size_t dim, double e, std::vector<std::string> &keys, std::vector<struct serialization_state> *sst, int layer, std::string layername) {\n\tstruct queued_feature qf;\n\tqf.pbf = pbf;\n\tqf.dim = dim;\n\tqf.e = e;\n\tqf.keys = &keys;\n\tqf.sst = sst;\n\tqf.layer = layer;\n\tqf.layername = layername;\n\n\tfeature_queue.push_back(qf);\n\n\tif (feature_queue.size() > CPUS * 500) {\n\t\trunQueue();\n\t}\n}\n\nvoid outBareGeometry(drawvec const &dv, int type, struct serialization_state *sst, int layer, std::string layername) {\n\tserial_feature sf;\n\n\tsf.layer = layer;\n\tsf.layername = layername;\n\tsf.segment = sst->segment;\n\tsf.has_id = false;\n\tsf.has_tippecanoe_minzoom = false;\n\tsf.has_tippecanoe_maxzoom = false;\n\tsf.feature_minzoom = false;\n\tsf.seq = (*sst->layer_seq);\n\tsf.geometry = dv;\n\tsf.t = type;\n\n\tserialize_feature(sst, sf);\n}\n\nvoid readFeatureCollection(protozero::pbf_reader &pbf, size_t dim, double e, std::vector<std::string> &keys, std::vector<struct serialization_state> *sst, int layer, std::string layername) {\n\twhile (pbf.next()) {\n\t\tswitch (pbf.tag()) {\n\t\tcase 1: {\n\t\t\tprotozero::pbf_reader feature_reader(pbf.get_message());\n\t\t\tqueueFeature(feature_reader, dim, e, keys, sst, layer, layername);\n\t\t\tbreak;\n\t\t}\n\n\t\tdefault:\n\t\t\tpbf.skip();\n\t\t}\n\t}\n}\n\nvoid parse_geobuf(std::vector<struct serialization_state> *sst, const char *src, size_t len, int layer, std::string layername) {\n\tprotozero::pbf_reader pbf(src, len);\n\n\tsize_t dim = 2;\n\tdouble e = 1e6;\n\tstd::vector<std::string> keys;\n\n\twhile (pbf.next()) {\n\t\tswitch (pbf.tag()) {\n\t\tcase 1:\n\t\t\tkeys.push_back(pbf.get_string());\n\t\t\tbreak;\n\n\t\tcase 2:\n\t\t\tdim = pbf.get_int64();\n\t\t\tbreak;\n\n\t\tcase 3:\n\t\t\te = pow(10, pbf.get_int64());\n\t\t\tbreak;\n\n\t\tcase 4: {\n\t\t\tprotozero::pbf_reader feature_collection_reader(pbf.get_message());\n\t\t\treadFeatureCollection(feature_collection_reader, dim, e, keys, sst, layer, layername);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase 5: {\n\t\t\tprotozero::pbf_reader feature_reader(pbf.get_message());\n\t\t\tqueueFeature(feature_reader, dim, e, keys, sst, layer, layername);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase 6: {\n\t\t\tprotozero::pbf_reader geometry_reader(pbf.get_message());\n\t\t\tstd::vector<drawvec_type> dv = readGeometry(geometry_reader, dim, e, keys);\n\t\t\tfor (size_t i = 0; i < dv.size(); i++) {\n\t\t\t\t// Always on thread 0\n\t\t\t\toutBareGeometry(dv[i].dv, dv[i].type, &(*sst)[0], layer, layername);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tdefault:\n\t\t\tpbf.skip();\n\t\t}\n\t}\n\n\trunQueue();\n}\n"
        },
        {
          "name": "geobuf.hpp",
          "type": "blob",
          "size": 0.279296875,
          "content": "#ifndef GEOBUF_HPP\n#define GEOBUF_HPP\n\n#include <stdio.h>\n#include <set>\n#include <map>\n#include <string>\n#include \"mbtiles.hpp\"\n#include \"serial.hpp\"\n\nvoid parse_geobuf(std::vector<struct serialization_state> *sst, const char *s, size_t len, int layer, std::string layername);\n\n#endif\n"
        },
        {
          "name": "geocsv.cpp",
          "type": "blob",
          "size": 3.3310546875,
          "content": "#include <stdlib.h>\n#include <algorithm>\n#include \"geocsv.hpp\"\n#include \"mvt.hpp\"\n#include \"serial.hpp\"\n#include \"projection.hpp\"\n#include \"main.hpp\"\n#include \"text.hpp\"\n#include \"csv.hpp\"\n#include \"milo/dtoa_milo.h\"\n#include \"options.hpp\"\n\nvoid parse_geocsv(std::vector<struct serialization_state> &sst, std::string fname, int layer, std::string layername) {\n\tFILE *f;\n\n\tif (fname.size() == 0) {\n\t\tf = stdin;\n\t} else {\n\t\tf = fopen(fname.c_str(), \"r\");\n\t\tif (f == NULL) {\n\t\t\tperror(fname.c_str());\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\tstd::string s;\n\tstd::vector<std::string> header;\n\tssize_t latcol = -1, loncol = -1;\n\n\tif ((s = csv_getline(f)).size() > 0) {\n\t\tstd::string err = check_utf8(s);\n\t\tif (err != \"\") {\n\t\t\tfprintf(stderr, \"%s: %s\\n\", fname.c_str(), err.c_str());\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\theader = csv_split(s.c_str());\n\n\t\tfor (size_t i = 0; i < header.size(); i++) {\n\t\t\theader[i] = csv_dequote(header[i]);\n\n\t\t\tstd::string lower(header[i]);\n\t\t\tstd::transform(lower.begin(), lower.end(), lower.begin(), ::tolower);\n\n\t\t\tif (lower == \"y\" || lower == \"lat\" || (lower.find(\"latitude\") != std::string::npos)) {\n\t\t\t\tlatcol = i;\n\t\t\t}\n\t\t\tif (lower == \"x\" || lower == \"lon\" || lower == \"lng\" || lower == \"long\" || (lower.find(\"longitude\") != std::string::npos)) {\n\t\t\t\tloncol = i;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (latcol < 0 || loncol < 0) {\n\t\tfprintf(stderr, \"%s: Can't find \\\"lat\\\" and \\\"lon\\\" columns\\n\", fname.c_str());\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tsize_t seq = 0;\n\twhile ((s = csv_getline(f)).size() > 0) {\n\t\tstd::string err = check_utf8(s);\n\t\tif (err != \"\") {\n\t\t\tfprintf(stderr, \"%s: %s\\n\", fname.c_str(), err.c_str());\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tseq++;\n\t\tstd::vector<std::string> line = csv_split(s.c_str());\n\n\t\tif (line.size() != header.size()) {\n\t\t\tfprintf(stderr, \"%s:%zu: Mismatched column count: %zu in line, %zu in header\\n\", fname.c_str(), seq + 1, line.size(), header.size());\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tif (line[loncol].empty() || line[latcol].empty()) {\n\t\t\tstatic int warned = 0;\n\t\t\tif (!warned) {\n\t\t\t\tfprintf(stderr, \"%s:%zu: null geometry (additional not reported)\\n\", fname.c_str(), seq + 1);\n\t\t\t\twarned = 1;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\tdouble lon = atof(line[loncol].c_str());\n\t\tdouble lat = atof(line[latcol].c_str());\n\n\t\tlong long x, y;\n\t\tprojection->project(lon, lat, 32, &x, &y);\n\t\tdrawvec dv;\n\t\tdv.push_back(draw(VT_MOVETO, x, y));\n\n\t\tstd::vector<std::string> full_keys;\n\t\tstd::vector<serial_val> full_values;\n\n\t\tfor (size_t i = 0; i < line.size(); i++) {\n\t\t\tif (i != (size_t) latcol && i != (size_t) loncol) {\n\t\t\t\tline[i] = csv_dequote(line[i]);\n\n\t\t\t\tserial_val sv;\n\t\t\t\tif (is_number(line[i])) {\n\t\t\t\t\tsv.type = mvt_double;\n\t\t\t\t} else if (line[i].size() == 0 && prevent[P_EMPTY_CSV_COLUMNS]) {\n\t\t\t\t\tsv.type = mvt_null;\n\t\t\t\t\tline[i] = \"null\";\n\t\t\t\t} else {\n\t\t\t\t\tsv.type = mvt_string;\n\t\t\t\t}\n\t\t\t\tsv.s = line[i];\n\n\t\t\t\tfull_keys.push_back(header[i]);\n\t\t\t\tfull_values.push_back(sv);\n\t\t\t}\n\t\t}\n\n\t\tserial_feature sf;\n\n\t\tsf.layer = layer;\n\t\tsf.layername = layername;\n\t\tsf.segment = sst[0].segment;\n\t\tsf.has_id = false;\n\t\tsf.id = 0;\n\t\tsf.has_tippecanoe_minzoom = false;\n\t\tsf.has_tippecanoe_maxzoom = false;\n\t\tsf.feature_minzoom = false;\n\t\tsf.seq = *(sst[0].layer_seq);\n\t\tsf.geometry = dv;\n\t\tsf.t = 1;  // POINT\n\t\tsf.full_keys = full_keys;\n\t\tsf.full_values = full_values;\n\n\t\tserialize_feature(&sst[0], sf);\n\t}\n\n\tif (fname.size() != 0) {\n\t\tif (fclose(f) != 0) {\n\t\t\tperror(\"fclose\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "geocsv.hpp",
          "type": "blob",
          "size": 0.271484375,
          "content": "#ifndef GEOCSV_HPP\n#define GEOCSV_HPP\n\n#include <stdio.h>\n#include <set>\n#include <map>\n#include <string>\n#include \"mbtiles.hpp\"\n#include \"serial.hpp\"\n\nvoid parse_geocsv(std::vector<struct serialization_state> &sst, std::string fname, int layer, std::string layername);\n\n#endif\n"
        },
        {
          "name": "geojson-loop.cpp",
          "type": "blob",
          "size": 4.6201171875,
          "content": "#ifdef MTRACE\n#include <mcheck.h>\n#endif\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <string.h>\n#include <unistd.h>\n#include <fcntl.h>\n#include <ctype.h>\n#include <errno.h>\n#include \"geojson-loop.hpp\"\n#include \"jsonpull/jsonpull.h\"\n\n// XXX duplicated\n#define GEOM_TYPES 6\nstatic const char *geometry_names[GEOM_TYPES] = {\n\t\"Point\", \"MultiPoint\", \"LineString\", \"MultiLineString\", \"Polygon\", \"MultiPolygon\",\n};\n\n// XXX duplicated\nstatic void json_context(json_object *j) {\n\tchar *s = json_stringify(j);\n\n\tif (strlen(s) >= 500) {\n\t\tsprintf(s + 497, \"...\");\n\t}\n\n\tfprintf(stderr, \"In JSON object %s\\n\", s);\n\tfree(s);  // stringify\n}\n\nvoid parse_json(json_feature_action *jfa, json_pull *jp) {\n\tlong long found_hashes = 0;\n\tlong long found_features = 0;\n\tlong long found_geometries = 0;\n\n\twhile (1) {\n\t\tjson_object *j = json_read(jp);\n\t\tif (j == NULL) {\n\t\t\tif (jp->error != NULL) {\n\t\t\t\tfprintf(stderr, \"%s:%d: %s\\n\", jfa->fname.c_str(), jp->line, jp->error);\n\t\t\t\tif (jp->root != NULL) {\n\t\t\t\t\tjson_context(jp->root);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tjson_free(jp->root);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (j->type == JSON_HASH) {\n\t\t\tfound_hashes++;\n\n\t\t\tif (found_hashes == 50 && found_features == 0 && found_geometries == 0) {\n\t\t\t\tfprintf(stderr, \"%s:%d: Warning: not finding any GeoJSON features or geometries in input yet after 50 objects.\\n\", jfa->fname.c_str(), jp->line);\n\t\t\t}\n\t\t}\n\n\t\tjson_object *type = json_hash_get(j, \"type\");\n\t\tif (type == NULL || type->type != JSON_STRING) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (found_features == 0) {\n\t\t\tint i;\n\t\t\tint is_geometry = 0;\n\t\t\tfor (i = 0; i < GEOM_TYPES; i++) {\n\t\t\t\tif (strcmp(type->string, geometry_names[i]) == 0) {\n\t\t\t\t\tis_geometry = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (is_geometry) {\n\t\t\t\tif (j->parent != NULL) {\n\t\t\t\t\tif (j->parent->type == JSON_ARRAY && j->parent->parent != NULL) {\n\t\t\t\t\t\tif (j->parent->parent->type == JSON_HASH) {\n\t\t\t\t\t\t\tjson_object *geometries = json_hash_get(j->parent->parent, \"geometries\");\n\t\t\t\t\t\t\tif (geometries != NULL) {\n\t\t\t\t\t\t\t\t// Parent of Parent must be a GeometryCollection\n\t\t\t\t\t\t\t\tis_geometry = 0;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if (j->parent->type == JSON_HASH) {\n\t\t\t\t\t\tjson_object *geometry = json_hash_get(j->parent, \"geometry\");\n\t\t\t\t\t\tif (geometry != NULL) {\n\t\t\t\t\t\t\t// Parent must be a Feature\n\t\t\t\t\t\t\tis_geometry = 0;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (is_geometry) {\n\t\t\t\tjson_object *jo = j;\n\t\t\t\twhile (jo != NULL) {\n\t\t\t\t\tif (jo->parent != NULL && jo->parent->type == JSON_HASH) {\n\t\t\t\t\t\tif (json_hash_get(jo->parent, \"properties\") == jo) {\n\t\t\t\t\t\t\t// Ancestor is the value corresponding to a properties key\n\t\t\t\t\t\t\tis_geometry = 0;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tjo = jo->parent;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (is_geometry) {\n\t\t\t\tif (found_features != 0 && found_geometries == 0) {\n\t\t\t\t\tfprintf(stderr, \"%s:%d: Warning: found a mixture of features and bare geometries\\n\", jfa->fname.c_str(), jp->line);\n\t\t\t\t}\n\t\t\t\tfound_geometries++;\n\n\t\t\t\tjfa->add_feature(j, false, NULL, NULL, NULL, j);\n\t\t\t\tjson_free(j);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (strcmp(type->string, \"Feature\") != 0) {\n\t\t\tif (strcmp(type->string, \"FeatureCollection\") == 0) {\n\t\t\t\tjfa->check_crs(j);\n\t\t\t\tjson_free(j);\n\t\t\t}\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (found_features == 0 && found_geometries != 0) {\n\t\t\tfprintf(stderr, \"%s:%d: Warning: found a mixture of features and bare geometries\\n\", jfa->fname.c_str(), jp->line);\n\t\t}\n\t\tfound_features++;\n\n\t\tjson_object *geometry = json_hash_get(j, \"geometry\");\n\t\tif (geometry == NULL) {\n\t\t\tfprintf(stderr, \"%s:%d: feature with no geometry\\n\", jfa->fname.c_str(), jp->line);\n\t\t\tjson_context(j);\n\t\t\tjson_free(j);\n\t\t\tcontinue;\n\t\t}\n\n\t\tjson_object *properties = json_hash_get(j, \"properties\");\n\t\tif (properties == NULL || (properties->type != JSON_HASH && properties->type != JSON_NULL)) {\n\t\t\tfprintf(stderr, \"%s:%d: feature without properties hash\\n\", jfa->fname.c_str(), jp->line);\n\t\t\tjson_context(j);\n\t\t\tjson_free(j);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbool is_feature = true;\n\t\t{\n\t\t\tjson_object *jo = j;\n\t\t\twhile (jo != NULL) {\n\t\t\t\tif (jo->parent != NULL && jo->parent->type == JSON_HASH) {\n\t\t\t\t\tif (json_hash_get(jo->parent, \"properties\") == jo) {\n\t\t\t\t\t\t// Ancestor is the value corresponding to a properties key\n\t\t\t\t\t\tis_feature = false;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tjo = jo->parent;\n\t\t\t}\n\t\t}\n\t\tif (!is_feature) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tjson_object *tippecanoe = json_hash_get(j, \"tippecanoe\");\n\t\tjson_object *id = json_hash_get(j, \"id\");\n\n\t\tjson_object *geometries = json_hash_get(geometry, \"geometries\");\n\t\tif (geometries != NULL && geometries->type == JSON_ARRAY) {\n\t\t\tjfa->add_feature(geometries, true, properties, id, tippecanoe, j);\n\t\t} else {\n\t\t\tjfa->add_feature(geometry, false, properties, id, tippecanoe, j);\n\t\t}\n\n\t\tjson_free(j);\n\n\t\t/* XXX check for any non-features in the outer object */\n\t}\n}\n"
        },
        {
          "name": "geojson-loop.hpp",
          "type": "blob",
          "size": 0.3681640625,
          "content": "#include <string>\n#include \"jsonpull/jsonpull.h\"\n\nstruct json_feature_action {\n\tstd::string fname;\n\n\tvirtual int add_feature(json_object *geometry, bool geometrycollection, json_object *properties, json_object *id, json_object *tippecanoe, json_object *feature) = 0;\n\tvirtual void check_crs(json_object *j) = 0;\n};\n\nvoid parse_json(json_feature_action *action, json_pull *jp);\n"
        },
        {
          "name": "geojson.cpp",
          "type": "blob",
          "size": 8.6611328125,
          "content": "#ifdef MTRACE\n#include <mcheck.h>\n#endif\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <string.h>\n#include <unistd.h>\n#include <sys/stat.h>\n#include <sys/types.h>\n#include <sys/mman.h>\n#include <string.h>\n#include <fcntl.h>\n#include <ctype.h>\n#include <errno.h>\n#include <limits.h>\n#include <sqlite3.h>\n#include <stdarg.h>\n#include <sys/resource.h>\n#include <pthread.h>\n#include <vector>\n#include <algorithm>\n#include <set>\n#include <map>\n#include <string>\n#include \"jsonpull/jsonpull.h\"\n#include \"pool.hpp\"\n#include \"projection.hpp\"\n#include \"memfile.hpp\"\n#include \"main.hpp\"\n#include \"mbtiles.hpp\"\n#include \"geojson.hpp\"\n#include \"geometry.hpp\"\n#include \"options.hpp\"\n#include \"serial.hpp\"\n#include \"text.hpp\"\n#include \"read_json.hpp\"\n#include \"mvt.hpp\"\n#include \"geojson-loop.hpp\"\n\nint serialize_geojson_feature(struct serialization_state *sst, json_object *geometry, json_object *properties, json_object *id, int layer, json_object *tippecanoe, json_object *feature, std::string layername) {\n\tjson_object *geometry_type = json_hash_get(geometry, \"type\");\n\tif (geometry_type == NULL) {\n\t\tstatic int warned = 0;\n\t\tif (!warned) {\n\t\t\tfprintf(stderr, \"%s:%d: null geometry (additional not reported)\\n\", sst->fname, sst->line);\n\t\t\tjson_context(feature);\n\t\t\twarned = 1;\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\tif (geometry_type->type != JSON_STRING) {\n\t\tfprintf(stderr, \"%s:%d: geometry type is not a string\\n\", sst->fname, sst->line);\n\t\tjson_context(feature);\n\t\treturn 0;\n\t}\n\n\tjson_object *coordinates = json_hash_get(geometry, \"coordinates\");\n\tif (coordinates == NULL || coordinates->type != JSON_ARRAY) {\n\t\tfprintf(stderr, \"%s:%d: feature without coordinates array\\n\", sst->fname, sst->line);\n\t\tjson_context(feature);\n\t\treturn 0;\n\t}\n\n\tint t;\n\tfor (t = 0; t < GEOM_TYPES; t++) {\n\t\tif (strcmp(geometry_type->string, geometry_names[t]) == 0) {\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (t >= GEOM_TYPES) {\n\t\tfprintf(stderr, \"%s:%d: Can't handle geometry type %s\\n\", sst->fname, sst->line, geometry_type->string);\n\t\tjson_context(feature);\n\t\treturn 0;\n\t}\n\n\tint tippecanoe_minzoom = -1;\n\tint tippecanoe_maxzoom = -1;\n\tstd::string tippecanoe_layername;\n\n\tif (tippecanoe != NULL) {\n\t\tjson_object *min = json_hash_get(tippecanoe, \"minzoom\");\n\t\tif (min != NULL && (min->type == JSON_STRING || min->type == JSON_NUMBER)) {\n\t\t\ttippecanoe_minzoom = integer_zoom(sst->fname, min->string);\n\t\t}\n\n\t\tjson_object *max = json_hash_get(tippecanoe, \"maxzoom\");\n\t\tif (max != NULL && (max->type == JSON_STRING || max->type == JSON_NUMBER)) {\n\t\t\ttippecanoe_maxzoom = integer_zoom(sst->fname, max->string);\n\t\t}\n\n\t\tjson_object *ln = json_hash_get(tippecanoe, \"layer\");\n\t\tif (ln != NULL && (ln->type == JSON_STRING || ln->type == JSON_NUMBER)) {\n\t\t\ttippecanoe_layername = std::string(ln->string);\n\t\t}\n\t}\n\n\tbool has_id = false;\n\tunsigned long long id_value = 0;\n\tif (id != NULL) {\n\t\tif (id->type == JSON_NUMBER) {\n\t\t\tif (id->number >= 0) {\n\t\t\t\tchar *err = NULL;\n\t\t\t\tid_value = strtoull(id->string, &err, 10);\n\n\t\t\t\tif (err != NULL && *err != '\\0') {\n\t\t\t\t\tstatic bool warned_frac = false;\n\n\t\t\t\t\tif (!warned_frac) {\n\t\t\t\t\t\tfprintf(stderr, \"Warning: Can't represent non-integer feature ID %s\\n\", id->string);\n\t\t\t\t\t\twarned_frac = true;\n\t\t\t\t\t}\n\t\t\t\t} else if (std::to_string(id_value) != id->string) {\n\t\t\t\t\tstatic bool warned = false;\n\n\t\t\t\t\tif (!warned) {\n\t\t\t\t\t\tfprintf(stderr, \"Warning: Can't represent too-large feature ID %s\\n\", id->string);\n\t\t\t\t\t\twarned = true;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\thas_id = true;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tstatic bool warned_neg = false;\n\n\t\t\t\tif (!warned_neg) {\n\t\t\t\t\tfprintf(stderr, \"Warning: Can't represent negative feature ID %s\\n\", id->string);\n\t\t\t\t\twarned_neg = true;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tbool converted = false;\n\n\t\t\tif (additional[A_CONVERT_NUMERIC_IDS] && id->type == JSON_STRING) {\n\t\t\t\tchar *err = NULL;\n\t\t\t\tid_value = strtoull(id->string, &err, 10);\n\n\t\t\t\tif (err != NULL && *err != '\\0') {\n\t\t\t\t\tstatic bool warned_frac = false;\n\n\t\t\t\t\tif (!warned_frac) {\n\t\t\t\t\t\tfprintf(stderr, \"Warning: Can't represent non-integer feature ID %s\\n\", id->string);\n\t\t\t\t\t\twarned_frac = true;\n\t\t\t\t\t}\n\t\t\t\t} else if (std::to_string(id_value) != id->string) {\n\t\t\t\t\tstatic bool warned = false;\n\n\t\t\t\t\tif (!warned) {\n\t\t\t\t\t\tfprintf(stderr, \"Warning: Can't represent too-large feature ID %s\\n\", id->string);\n\t\t\t\t\t\twarned = true;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\thas_id = true;\n\t\t\t\t\tconverted = true;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (!converted) {\n\t\t\t\tstatic bool warned_nan = false;\n\n\t\t\t\tif (!warned_nan) {\n\t\t\t\t\tchar *s = json_stringify(id);\n\t\t\t\t\tfprintf(stderr, \"Warning: Can't represent non-numeric feature ID %s\\n\", s);\n\t\t\t\t\tfree(s);  // stringify\n\t\t\t\t\twarned_nan = true;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tsize_t nprop = 0;\n\tif (properties != NULL && properties->type == JSON_HASH) {\n\t\tnprop = properties->length;\n\t}\n\n\tstd::vector<char *> metakey;\n\tmetakey.resize(nprop);\n\n\tstd::vector<std::string> metaval;\n\tmetaval.resize(nprop);\n\n\tstd::vector<int> metatype;\n\tmetatype.resize(nprop);\n\n\tsize_t m = 0;\n\n\tfor (size_t i = 0; i < nprop; i++) {\n\t\tif (properties->keys[i]->type == JSON_STRING) {\n\t\t\tstd::string s(properties->keys[i]->string);\n\n\t\t\tint type = -1;\n\t\t\tstd::string val;\n\t\t\tstringify_value(properties->values[i], type, val, sst->fname, sst->line, feature);\n\n\t\t\tif (type >= 0) {\n\t\t\t\tmetakey[m] = properties->keys[i]->string;\n\t\t\t\tmetatype[m] = type;\n\t\t\t\tmetaval[m] = val;\n\t\t\t\tm++;\n\t\t\t} else {\n\t\t\t\tmetakey[m] = properties->keys[i]->string;\n\t\t\t\tmetatype[m] = mvt_null;\n\t\t\t\tmetaval[m] = \"null\";\n\t\t\t\tm++;\n\t\t\t}\n\t\t}\n\t}\n\n\tdrawvec dv;\n\tparse_geometry(t, coordinates, dv, VT_MOVETO, sst->fname, sst->line, feature);\n\n\tserial_feature sf;\n\tsf.layer = layer;\n\tsf.segment = sst->segment;\n\tsf.t = mb_geometry[t];\n\tsf.has_id = has_id;\n\tsf.id = id_value;\n\tsf.has_tippecanoe_minzoom = (tippecanoe_minzoom != -1);\n\tsf.tippecanoe_minzoom = tippecanoe_minzoom;\n\tsf.has_tippecanoe_maxzoom = (tippecanoe_maxzoom != -1);\n\tsf.tippecanoe_maxzoom = tippecanoe_maxzoom;\n\tsf.geometry = dv;\n\tsf.feature_minzoom = 0;  // Will be filled in during index merging\n\tsf.seq = *(sst->layer_seq);\n\n\tif (tippecanoe_layername.size() != 0) {\n\t\tsf.layername = tippecanoe_layername;\n\t} else {\n\t\tsf.layername = layername;\n\t}\n\n\tfor (size_t i = 0; i < m; i++) {\n\t\tsf.full_keys.push_back(metakey[i]);\n\n\t\tserial_val sv;\n\t\tsv.type = metatype[i];\n\t\tsv.s = metaval[i];\n\n\t\tsf.full_values.push_back(sv);\n\t}\n\n\treturn serialize_feature(sst, sf);\n}\n\nvoid check_crs(json_object *j, const char *reading) {\n\tjson_object *crs = json_hash_get(j, \"crs\");\n\tif (crs != NULL) {\n\t\tjson_object *properties = json_hash_get(crs, \"properties\");\n\t\tif (properties != NULL) {\n\t\t\tjson_object *name = json_hash_get(properties, \"name\");\n\t\t\tif (name != NULL && name->type == JSON_STRING) {\n\t\t\t\tif (strcmp(name->string, projection->alias) != 0) {\n\t\t\t\t\tif (!quiet) {\n\t\t\t\t\t\tfprintf(stderr, \"%s: Warning: GeoJSON specified projection \\\"%s\\\", not the expected \\\"%s\\\".\\n\", reading, name->string, projection->alias);\n\t\t\t\t\t\tfprintf(stderr, \"%s: If \\\"%s\\\" is not the expected projection, use -s to specify the right one.\\n\", reading, projection->alias);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nstruct json_serialize_action : json_feature_action {\n\tserialization_state *sst;\n\tint layer;\n\tstd::string layername;\n\n\tint add_feature(json_object *geometry, bool geometrycollection, json_object *properties, json_object *id, json_object *tippecanoe, json_object *feature) {\n\t\tsst->line = geometry->parser->line;\n\t\tif (geometrycollection) {\n\t\t\tint ret = 1;\n\t\t\tfor (size_t g = 0; g < geometry->length; g++) {\n\t\t\t\tret &= serialize_geojson_feature(sst, geometry->array[g], properties, id, layer, tippecanoe, feature, layername);\n\t\t\t}\n\t\t\treturn ret;\n\t\t} else {\n\t\t\treturn serialize_geojson_feature(sst, geometry, properties, id, layer, tippecanoe, feature, layername);\n\t\t}\n\t}\n\n\tvoid check_crs(json_object *j) {\n\t\t::check_crs(j, fname.c_str());\n\t}\n};\n\nvoid parse_json(struct serialization_state *sst, json_pull *jp, int layer, std::string layername) {\n\tjson_serialize_action jsa;\n\tjsa.fname = sst->fname;\n\tjsa.sst = sst;\n\tjsa.layer = layer;\n\tjsa.layername = layername;\n\n\tparse_json(&jsa, jp);\n}\n\nvoid *run_parse_json(void *v) {\n\tstruct parse_json_args *pja = (struct parse_json_args *) v;\n\n\tparse_json(pja->sst, pja->jp, pja->layer, *pja->layername);\n\n\treturn NULL;\n}\n\nstruct jsonmap {\n\tchar *map;\n\tunsigned long long off;\n\tunsigned long long end;\n};\n\nssize_t json_map_read(struct json_pull *jp, char *buffer, size_t n) {\n\tstruct jsonmap *jm = (struct jsonmap *) jp->source;\n\n\tif (jm->off + n >= jm->end) {\n\t\tn = jm->end - jm->off;\n\t}\n\n\tmemcpy(buffer, jm->map + jm->off, n);\n\tjm->off += n;\n\n\treturn n;\n}\n\nstruct json_pull *json_begin_map(char *map, long long len) {\n\tstruct jsonmap *jm = new jsonmap;\n\tif (jm == NULL) {\n\t\tperror(\"Out of memory\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tjm->map = map;\n\tjm->off = 0;\n\tjm->end = len;\n\n\treturn json_begin(json_map_read, jm);\n}\n\nvoid json_end_map(struct json_pull *jp) {\n\tdelete (struct jsonmap *) jp->source;\n\tjson_end(jp);\n}\n"
        },
        {
          "name": "geojson.hpp",
          "type": "blob",
          "size": 0.69921875,
          "content": "#ifndef GEOJSON_HPP\n#define GEOJSON_HPP\n\n#include <stdio.h>\n#include <set>\n#include <map>\n#include <string>\n#include \"mbtiles.hpp\"\n#include \"jsonpull/jsonpull.h\"\n#include \"serial.hpp\"\n\nstruct parse_json_args {\n\tjson_pull *jp;\n\tint layer;\n\tstd::string *layername;\n\n\tstruct serialization_state *sst;\n\n\tparse_json_args(json_pull *jp1, int layer1, std::string *layername1, struct serialization_state *sst1)\n\t    : jp(jp1), layer(layer1), layername(layername1), sst(sst1) {\n\t}\n};\n\nstruct json_pull *json_begin_map(char *map, long long len);\nvoid json_end_map(struct json_pull *jp);\n\nvoid parse_json(struct serialization_state *sst, json_pull *jp, int layer, std::string layername);\nvoid *run_parse_json(void *v);\n\n#endif\n"
        },
        {
          "name": "geometry.cpp",
          "type": "blob",
          "size": 31.1240234375,
          "content": "#include <iostream>\n#include <fstream>\n#include <string>\n#include <stack>\n#include <vector>\n#include <map>\n#include <algorithm>\n#include <cstdio>\n#include <unistd.h>\n#include <cmath>\n#include <limits.h>\n#include <sqlite3.h>\n#include <mapbox/geometry/point.hpp>\n#include <mapbox/geometry/multi_polygon.hpp>\n#include <mapbox/geometry/wagyu/wagyu.hpp>\n#include <mapbox/geometry/wagyu/quick_clip.hpp>\n#include <mapbox/geometry/snap_rounding.hpp>\n#include \"geometry.hpp\"\n#include \"projection.hpp\"\n#include \"serial.hpp\"\n#include \"main.hpp\"\n#include \"options.hpp\"\n\nstatic int pnpoly(drawvec &vert, size_t start, size_t nvert, long long testx, long long testy);\nstatic int clip(double *x0, double *y0, double *x1, double *y1, double xmin, double ymin, double xmax, double ymax);\n\ndrawvec decode_geometry(FILE *meta, std::atomic<long long> *geompos, int z, unsigned tx, unsigned ty, long long *bbox, unsigned initial_x, unsigned initial_y) {\n\tdrawvec out;\n\n\tbbox[0] = LLONG_MAX;\n\tbbox[1] = LLONG_MAX;\n\tbbox[2] = LLONG_MIN;\n\tbbox[3] = LLONG_MIN;\n\n\tlong long wx = initial_x, wy = initial_y;\n\n\twhile (1) {\n\t\tdraw d;\n\n\t\tif (!deserialize_byte_io(meta, &d.op, geompos)) {\n\t\t\tfprintf(stderr, \"Internal error: Unexpected end of file in geometry\\n\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (d.op == VT_END) {\n\t\t\tbreak;\n\t\t}\n\n\t\tif (d.op == VT_MOVETO || d.op == VT_LINETO) {\n\t\t\tlong long dx, dy;\n\n\t\t\tdeserialize_long_long_io(meta, &dx, geompos);\n\t\t\tdeserialize_long_long_io(meta, &dy, geompos);\n\n\t\t\twx += dx * (1 << geometry_scale);\n\t\t\twy += dy * (1 << geometry_scale);\n\n\t\t\tlong long wwx = wx;\n\t\t\tlong long wwy = wy;\n\n\t\t\tif (z != 0) {\n\t\t\t\twwx -= tx << (32 - z);\n\t\t\t\twwy -= ty << (32 - z);\n\t\t\t}\n\n\t\t\tif (wwx < bbox[0]) {\n\t\t\t\tbbox[0] = wwx;\n\t\t\t}\n\t\t\tif (wwy < bbox[1]) {\n\t\t\t\tbbox[1] = wwy;\n\t\t\t}\n\t\t\tif (wwx > bbox[2]) {\n\t\t\t\tbbox[2] = wwx;\n\t\t\t}\n\t\t\tif (wwy > bbox[3]) {\n\t\t\t\tbbox[3] = wwy;\n\t\t\t}\n\n\t\t\td.x = wwx;\n\t\t\td.y = wwy;\n\t\t}\n\n\t\tout.push_back(d);\n\t}\n\n\treturn out;\n}\n\nvoid to_tile_scale(drawvec &geom, int z, int detail) {\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tgeom[i].x >>= (32 - detail - z);\n\t\tgeom[i].y >>= (32 - detail - z);\n\t}\n}\n\ndrawvec remove_noop(drawvec geom, int type, int shift) {\n\t// first pass: remove empty linetos\n\n\tlong long x = 0, y = 0;\n\tdrawvec out;\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_LINETO && (geom[i].x >> shift) == x && (geom[i].y >> shift) == y) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (geom[i].op == VT_CLOSEPATH) {\n\t\t\tout.push_back(geom[i]);\n\t\t} else { /* moveto or lineto */\n\t\t\tout.push_back(geom[i]);\n\t\t\tx = geom[i].x >> shift;\n\t\t\ty = geom[i].y >> shift;\n\t\t}\n\t}\n\n\t// second pass: remove unused movetos\n\n\tif (type != VT_POINT) {\n\t\tgeom = out;\n\t\tout.resize(0);\n\n\t\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\t\tif (geom[i].op == VT_MOVETO) {\n\t\t\t\tif (i + 1 >= geom.size()) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tif (geom[i + 1].op == VT_MOVETO) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tif (geom[i + 1].op == VT_CLOSEPATH) {\n\t\t\t\t\tfprintf(stderr, \"Shouldn't happen\\n\");\n\t\t\t\t\ti++;  // also remove unused closepath\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tout.push_back(geom[i]);\n\t\t}\n\t}\n\n\t// second pass: remove empty movetos\n\n\tif (type == VT_LINE) {\n\t\tgeom = out;\n\t\tout.resize(0);\n\n\t\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\t\tif (geom[i].op == VT_MOVETO) {\n\t\t\t\tif (i > 0 && geom[i - 1].op == VT_LINETO && (geom[i - 1].x >> shift) == (geom[i].x >> shift) && (geom[i - 1].y >> shift) == (geom[i].y >> shift)) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tout.push_back(geom[i]);\n\t\t}\n\t}\n\n\treturn out;\n}\n\ndouble get_area(drawvec &geom, size_t i, size_t j) {\n\tdouble area = 0;\n\tfor (size_t k = i; k < j; k++) {\n\t\tarea += (long double) geom[k].x * (long double) geom[i + ((k - i + 1) % (j - i))].y;\n\t\tarea -= (long double) geom[k].y * (long double) geom[i + ((k - i + 1) % (j - i))].x;\n\t}\n\tarea /= 2;\n\treturn area;\n}\n\ndouble get_mp_area(drawvec &geom) {\n\tdouble ret = 0;\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_MOVETO) {\n\t\t\tsize_t j;\n\n\t\t\tfor (j = i + 1; j < geom.size(); j++) {\n\t\t\t\tif (geom[j].op != VT_LINETO) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tret += get_area(geom, i, j);\n\t\t\ti = j - 1;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic void decode_clipped(mapbox::geometry::multi_polygon<long long> &t, drawvec &out) {\n\tout.clear();\n\n\tfor (size_t i = 0; i < t.size(); i++) {\n\t\tfor (size_t j = 0; j < t[i].size(); j++) {\n\t\t\tdrawvec ring;\n\n\t\t\tfor (size_t k = 0; k < t[i][j].size(); k++) {\n\t\t\t\tring.push_back(draw((k == 0) ? VT_MOVETO : VT_LINETO, t[i][j][k].x, t[i][j][k].y));\n\t\t\t}\n\n\t\t\tif (ring.size() > 0 && ring[ring.size() - 1] != ring[0]) {\n\t\t\t\tfprintf(stderr, \"Had to close ring\\n\");\n\t\t\t\tring.push_back(draw(VT_LINETO, ring[0].x, ring[0].y));\n\t\t\t}\n\n\t\t\tdouble area = get_area(ring, 0, ring.size());\n\n\t\t\tif ((j == 0 && area < 0) || (j != 0 && area > 0)) {\n\t\t\t\tfprintf(stderr, \"Ring area has wrong sign: %f for %zu\\n\", area, j);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tfor (size_t k = 0; k < ring.size(); k++) {\n\t\t\t\tout.push_back(ring[k]);\n\t\t\t}\n\t\t}\n\t}\n}\n\ndrawvec clean_or_clip_poly(drawvec &geom, int z, int buffer, bool clip) {\n\tmapbox::geometry::wagyu::wagyu<long long> wagyu;\n\n\tgeom = remove_noop(geom, VT_POLYGON, 0);\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_MOVETO) {\n\t\t\tsize_t j;\n\t\t\tfor (j = i + 1; j < geom.size(); j++) {\n\t\t\t\tif (geom[j].op != VT_LINETO) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (j >= i + 4) {\n\t\t\t\tmapbox::geometry::linear_ring<long long> lr;\n\n\t\t\t\tfor (size_t k = i; k < j; k++) {\n\t\t\t\t\tlr.push_back(mapbox::geometry::point<long long>(geom[k].x, geom[k].y));\n\t\t\t\t}\n\n\t\t\t\tif (lr.size() >= 3) {\n\t\t\t\t\twagyu.add_ring(lr);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\ti = j - 1;\n\t\t}\n\t}\n\n\tif (clip) {\n\t\tlong long area = 0xFFFFFFFF;\n\t\tif (z != 0) {\n\t\t\tarea = 1LL << (32 - z);\n\t\t}\n\t\tlong long clip_buffer = buffer * area / 256;\n\n\t\tmapbox::geometry::linear_ring<long long> lr;\n\n\t\tlr.push_back(mapbox::geometry::point<long long>(-clip_buffer, -clip_buffer));\n\t\tlr.push_back(mapbox::geometry::point<long long>(-clip_buffer, area + clip_buffer));\n\t\tlr.push_back(mapbox::geometry::point<long long>(area + clip_buffer, area + clip_buffer));\n\t\tlr.push_back(mapbox::geometry::point<long long>(area + clip_buffer, -clip_buffer));\n\t\tlr.push_back(mapbox::geometry::point<long long>(-clip_buffer, -clip_buffer));\n\n\t\twagyu.add_ring(lr, mapbox::geometry::wagyu::polygon_type_clip);\n\t}\n\n\tmapbox::geometry::multi_polygon<long long> result;\n\ttry {\n\t\twagyu.execute(mapbox::geometry::wagyu::clip_type_union, result, mapbox::geometry::wagyu::fill_type_positive, mapbox::geometry::wagyu::fill_type_positive);\n\t} catch (std::runtime_error &e) {\n\t\tFILE *f = fopen(\"/tmp/wagyu.log\", \"w\");\n\t\tfprintf(f, \"%s\\n\", e.what());\n\t\tfprintf(stderr, \"%s\\n\", e.what());\n\t\tfprintf(f, \"[\");\n\n\t\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\t\tif (geom[i].op == VT_MOVETO) {\n\t\t\t\tsize_t j;\n\t\t\t\tfor (j = i + 1; j < geom.size(); j++) {\n\t\t\t\t\tif (geom[j].op != VT_LINETO) {\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (j >= i + 4) {\n\t\t\t\t\tmapbox::geometry::linear_ring<long long> lr;\n\n\t\t\t\t\tif (i != 0) {\n\t\t\t\t\t\tfprintf(f, \",\");\n\t\t\t\t\t}\n\t\t\t\t\tfprintf(f, \"[\");\n\n\t\t\t\t\tfor (size_t k = i; k < j; k++) {\n\t\t\t\t\t\tlr.push_back(mapbox::geometry::point<long long>(geom[k].x, geom[k].y));\n\t\t\t\t\t\tif (k != i) {\n\t\t\t\t\t\t\tfprintf(f, \",\");\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfprintf(f, \"[%lld,%lld]\", geom[k].x, geom[k].y);\n\t\t\t\t\t}\n\n\t\t\t\t\tfprintf(f, \"]\");\n\n\t\t\t\t\tif (lr.size() >= 3) {\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ti = j - 1;\n\t\t\t}\n\t\t}\n\n\t\tfprintf(f, \"]\");\n\t\tfprintf(f, \"\\n\\n\\n\\n\\n\");\n\n\t\tfclose(f);\n\t\tfprintf(stderr, \"Internal error: Polygon cleaning failed. Log in /tmp/wagyu.log\\n\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tdrawvec ret;\n\tdecode_clipped(result, ret);\n\treturn ret;\n}\n\n/* pnpoly:\nCopyright (c) 1970-2003, Wm. Randolph Franklin\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice in the documentation and/or other materials provided with the distribution.\nThe name of W. Randolph Franklin may not be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n*/\n\nstatic int pnpoly(drawvec &vert, size_t start, size_t nvert, long long testx, long long testy) {\n\tsize_t i, j;\n\tbool c = false;\n\tfor (i = 0, j = nvert - 1; i < nvert; j = i++) {\n\t\tif (((vert[i + start].y > testy) != (vert[j + start].y > testy)) &&\n\t\t    (testx < (vert[j + start].x - vert[i + start].x) * (testy - vert[i + start].y) / (double) (vert[j + start].y - vert[i + start].y) + vert[i + start].x))\n\t\t\tc = !c;\n\t}\n\treturn c;\n}\n\nvoid check_polygon(drawvec &geom) {\n\tgeom = remove_noop(geom, VT_POLYGON, 0);\n\n\tmapbox::geometry::multi_polygon<long long> mp;\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_MOVETO) {\n\t\t\tsize_t j;\n\t\t\tfor (j = i + 1; j < geom.size(); j++) {\n\t\t\t\tif (geom[j].op != VT_LINETO) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (j >= i + 4) {\n\t\t\t\tmapbox::geometry::linear_ring<long long> lr;\n\n\t\t\t\tfor (size_t k = i; k < j; k++) {\n\t\t\t\t\tlr.push_back(mapbox::geometry::point<long long>(geom[k].x, geom[k].y));\n\t\t\t\t}\n\n\t\t\t\tif (lr.size() >= 3) {\n\t\t\t\t\tmapbox::geometry::polygon<long long> p;\n\t\t\t\t\tp.push_back(lr);\n\t\t\t\t\tmp.push_back(p);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\ti = j - 1;\n\t\t}\n\t}\n\n\tmapbox::geometry::multi_polygon<long long> mp2 = mapbox::geometry::snap_round(mp, true, true);\n\tif (mp != mp2) {\n\t\tfprintf(stderr, \"Internal error: self-intersecting polygon\\n\");\n\t}\n\n\tsize_t outer_start = -1;\n\tsize_t outer_len = 0;\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_MOVETO) {\n\t\t\tsize_t j;\n\t\t\tfor (j = i + 1; j < geom.size(); j++) {\n\t\t\t\tif (geom[j].op != VT_LINETO) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tdouble area = get_area(geom, i, j);\n\n#if 0\n\t\t\tfprintf(stderr, \"looking at %lld to %lld, area %f\\n\", (long long) i, (long long) j, area);\n#endif\n\n\t\t\tif (area > 0) {\n\t\t\t\touter_start = i;\n\t\t\t\touter_len = j - i;\n\t\t\t} else {\n\t\t\t\tfor (size_t k = i; k < j; k++) {\n\t\t\t\t\tif (!pnpoly(geom, outer_start, outer_len, geom[k].x, geom[k].y)) {\n\t\t\t\t\t\tbool on_edge = false;\n\n\t\t\t\t\t\tfor (size_t l = outer_start; l < outer_start + outer_len; l++) {\n\t\t\t\t\t\t\tif (geom[k].x == geom[l].x || geom[k].y == geom[l].y) {\n\t\t\t\t\t\t\t\ton_edge = true;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (!on_edge) {\n\t\t\t\t\t\t\tprintf(\"%lld,%lld at %lld not in outer ring (%lld to %lld)\\n\", geom[k].x, geom[k].y, (long long) k, (long long) outer_start, (long long) (outer_start + outer_len));\n\n#if 0\n\t\t\t\t\t\t\tfor (size_t l = outer_start; l < outer_start + outer_len; l++) {\n\t\t\t\t\t\t\t\tfprintf(stderr, \" %lld,%lld\", geom[l].x, geom[l].y);\n\t\t\t\t\t\t\t}\n#endif\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\ndrawvec close_poly(drawvec &geom) {\n\tdrawvec out;\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_MOVETO) {\n\t\t\tsize_t j;\n\t\t\tfor (j = i + 1; j < geom.size(); j++) {\n\t\t\t\tif (geom[j].op != VT_LINETO) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (j - 1 > i) {\n\t\t\t\tif (geom[j - 1].x != geom[i].x || geom[j - 1].y != geom[i].y) {\n\t\t\t\t\tfprintf(stderr, \"Internal error: polygon not closed\\n\");\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor (size_t n = i; n < j - 1; n++) {\n\t\t\t\tout.push_back(geom[n]);\n\t\t\t}\n\t\t\tout.push_back(draw(VT_CLOSEPATH, 0, 0));\n\n\t\t\ti = j - 1;\n\t\t}\n\t}\n\n\treturn out;\n}\n\ndrawvec simple_clip_poly(drawvec &geom, long long minx, long long miny, long long maxx, long long maxy) {\n\tdrawvec out;\n\n\tmapbox::geometry::point<long long> min(minx, miny);\n\tmapbox::geometry::point<long long> max(maxx, maxy);\n\tmapbox::geometry::box<long long> bbox(min, max);\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_MOVETO) {\n\t\t\tsize_t j;\n\t\t\tfor (j = i + 1; j < geom.size(); j++) {\n\t\t\t\tif (geom[j].op != VT_LINETO) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tmapbox::geometry::linear_ring<long long> ring;\n\t\t\tfor (size_t k = i; k < j; k++) {\n\t\t\t\tring.push_back(mapbox::geometry::point<long long>(geom[k].x, geom[k].y));\n\t\t\t}\n\n\t\t\tmapbox::geometry::linear_ring<long long> lr = mapbox::geometry::wagyu::quick_clip::quick_lr_clip(ring, bbox);\n\n\t\t\tif (lr.size() > 0) {\n\t\t\t\tfor (size_t k = 0; k < lr.size(); k++) {\n\t\t\t\t\tif (k == 0) {\n\t\t\t\t\t\tout.push_back(draw(VT_MOVETO, lr[k].x, lr[k].y));\n\t\t\t\t\t} else {\n\t\t\t\t\t\tout.push_back(draw(VT_LINETO, lr[k].x, lr[k].y));\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (lr.size() > 0 && lr[0] != lr[lr.size() - 1]) {\n\t\t\t\t\tout.push_back(draw(VT_LINETO, lr[0].x, lr[0].y));\n\t\t\t\t}\n\t\t\t}\n\n\t\t\ti = j - 1;\n\t\t} else {\n\t\t\tfprintf(stderr, \"Unexpected operation in polygon %d\\n\", (int) geom[i].op);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\treturn out;\n}\n\ndrawvec simple_clip_poly(drawvec &geom, int z, int buffer) {\n\tlong long area = 1LL << (32 - z);\n\tlong long clip_buffer = buffer * area / 256;\n\n\treturn simple_clip_poly(geom, -clip_buffer, -clip_buffer, area + clip_buffer, area + clip_buffer);\n}\n\ndrawvec reduce_tiny_poly(drawvec &geom, int z, int detail, bool *reduced, double *accum_area) {\n\tdrawvec out;\n\tlong long pixel = (1 << (32 - detail - z)) * 2;\n\n\t*reduced = true;\n\tbool included_last_outer = false;\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_MOVETO) {\n\t\t\tsize_t j;\n\t\t\tfor (j = i + 1; j < geom.size(); j++) {\n\t\t\t\tif (geom[j].op != VT_LINETO) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tdouble area = get_area(geom, i, j);\n\n\t\t\t// XXX There is an ambiguity here: If the area of a ring is 0 and it is followed by holes,\n\t\t\t// we don't know whether the area-0 ring was a hole too or whether it was the outer ring\n\t\t\t// that these subsequent holes are somehow being subtracted from. I hope that if a polygon\n\t\t\t// was simplified down to nothing, its holes also became nothing.\n\n\t\t\tif (area != 0) {\n\t\t\t\t// These are pixel coordinates, so area > 0 for the outer ring.\n\t\t\t\t// If the outer ring of a polygon was reduced to a pixel, its\n\t\t\t\t// inner rings must just have their area de-accumulated rather\n\t\t\t\t// than being drawn since we don't really know where they are.\n\n\t\t\t\tif (std::fabs(area) <= pixel * pixel || (area < 0 && !included_last_outer)) {\n\t\t\t\t\t// printf(\"area is only %f vs %lld so using square\\n\", area, pixel * pixel);\n\n\t\t\t\t\t*accum_area += area;\n\t\t\t\t\tif (area > 0 && *accum_area > pixel * pixel) {\n\t\t\t\t\t\t// XXX use centroid;\n\n\t\t\t\t\t\tout.push_back(draw(VT_MOVETO, geom[i].x - pixel / 2, geom[i].y - pixel / 2));\n\t\t\t\t\t\tout.push_back(draw(VT_LINETO, geom[i].x + pixel / 2, geom[i].y - pixel / 2));\n\t\t\t\t\t\tout.push_back(draw(VT_LINETO, geom[i].x + pixel / 2, geom[i].y + pixel / 2));\n\t\t\t\t\t\tout.push_back(draw(VT_LINETO, geom[i].x - pixel / 2, geom[i].y + pixel / 2));\n\t\t\t\t\t\tout.push_back(draw(VT_LINETO, geom[i].x - pixel / 2, geom[i].y - pixel / 2));\n\n\t\t\t\t\t\t*accum_area -= pixel * pixel;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (area > 0) {\n\t\t\t\t\t\tincluded_last_outer = false;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// printf(\"area is %f so keeping instead of %lld\\n\", area, pixel * pixel);\n\n\t\t\t\t\tfor (size_t k = i; k <= j && k < geom.size(); k++) {\n\t\t\t\t\t\tout.push_back(geom[k]);\n\t\t\t\t\t}\n\n\t\t\t\t\t*reduced = false;\n\n\t\t\t\t\tif (area > 0) {\n\t\t\t\t\t\tincluded_last_outer = true;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\ti = j - 1;\n\t\t} else {\n\t\t\tfprintf(stderr, \"how did we get here with %d in %d?\\n\", geom[i].op, (int) geom.size());\n\n\t\t\tfor (size_t n = 0; n < geom.size(); n++) {\n\t\t\t\tfprintf(stderr, \"%d/%lld/%lld \", geom[n].op, geom[n].x, geom[n].y);\n\t\t\t}\n\t\t\tfprintf(stderr, \"\\n\");\n\n\t\t\tout.push_back(geom[i]);\n\t\t}\n\t}\n\n\treturn out;\n}\n\ndrawvec clip_point(drawvec &geom, int z, long long buffer) {\n\tlong long min = 0;\n\tlong long area = 1LL << (32 - z);\n\n\tmin -= buffer * area / 256;\n\tarea += buffer * area / 256;\n\n\treturn clip_point(geom, min, min, area, area);\n}\n\ndrawvec clip_point(drawvec &geom, long long minx, long long miny, long long maxx, long long maxy) {\n\tdrawvec out;\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].x >= minx && geom[i].y >= miny && geom[i].x <= maxx && geom[i].y <= maxy) {\n\t\t\tout.push_back(geom[i]);\n\t\t}\n\t}\n\n\treturn out;\n}\n\nint quick_check(long long *bbox, int z, long long buffer) {\n\tlong long min = 0;\n\tlong long area = 1LL << (32 - z);\n\n\tmin -= buffer * area / 256;\n\tarea += buffer * area / 256;\n\n\t// bbox entirely outside the tile\n\tif (bbox[0] > area || bbox[1] > area) {\n\t\treturn 0;\n\t}\n\tif (bbox[2] < min || bbox[3] < min) {\n\t\treturn 0;\n\t}\n\n\t// bbox entirely within the tile\n\tif (bbox[0] > min && bbox[1] > min && bbox[2] < area && bbox[3] < area) {\n\t\treturn 1;\n\t}\n\n\t// some overlap of edge\n\treturn 2;\n}\n\nbool point_within_tile(long long x, long long y, int z) {\n\t// No adjustment for buffer, because the point must be\n\t// strictly within the tile to appear exactly once\n\n\tlong long area = 1LL << (32 - z);\n\n\treturn x >= 0 && y >= 0 && x < area && y < area;\n}\n\ndrawvec clip_lines(drawvec &geom, int z, long long buffer) {\n\tlong long min = 0;\n\tlong long area = 1LL << (32 - z);\n\tmin -= buffer * area / 256;\n\tarea += buffer * area / 256;\n\n\treturn clip_lines(geom, min, min, area, area);\n}\n\ndrawvec clip_lines(drawvec &geom, long long minx, long long miny, long long maxx, long long maxy) {\n\tdrawvec out;\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (i > 0 && (geom[i - 1].op == VT_MOVETO || geom[i - 1].op == VT_LINETO) && geom[i].op == VT_LINETO) {\n\t\t\tdouble x1 = geom[i - 1].x;\n\t\t\tdouble y1 = geom[i - 1].y;\n\n\t\t\tdouble x2 = geom[i - 0].x;\n\t\t\tdouble y2 = geom[i - 0].y;\n\n\t\t\tint c = clip(&x1, &y1, &x2, &y2, minx, miny, maxx, maxy);\n\n\t\t\tif (c > 1) {  // clipped\n\t\t\t\tout.push_back(draw(VT_MOVETO, x1, y1));\n\t\t\t\tout.push_back(draw(VT_LINETO, x2, y2));\n\t\t\t\tout.push_back(draw(VT_MOVETO, geom[i].x, geom[i].y));\n\t\t\t} else if (c == 1) {  // unchanged\n\t\t\t\tout.push_back(geom[i]);\n\t\t\t} else {  // clipped away entirely\n\t\t\t\tout.push_back(draw(VT_MOVETO, geom[i].x, geom[i].y));\n\t\t\t}\n\t\t} else {\n\t\t\tout.push_back(geom[i]);\n\t\t}\n\t}\n\n\treturn out;\n}\n\nstatic double square_distance_from_line(long long point_x, long long point_y, long long segA_x, long long segA_y, long long segB_x, long long segB_y) {\n\tdouble p2x = segB_x - segA_x;\n\tdouble p2y = segB_y - segA_y;\n\tdouble something = p2x * p2x + p2y * p2y;\n\tdouble u = 0 == something ? 0 : ((point_x - segA_x) * p2x + (point_y - segA_y) * p2y) / something;\n\n\tif (u > 1) {\n\t\tu = 1;\n\t} else if (u < 0) {\n\t\tu = 0;\n\t}\n\n\tdouble x = segA_x + u * p2x;\n\tdouble y = segA_y + u * p2y;\n\n\tdouble dx = x - point_x;\n\tdouble dy = y - point_y;\n\n\treturn dx * dx + dy * dy;\n}\n\n// https://github.com/Project-OSRM/osrm-backend/blob/733d1384a40f/Algorithms/DouglasePeucker.cpp\nstatic void douglas_peucker(drawvec &geom, int start, int n, double e, size_t kept, size_t retain) {\n\te = e * e;\n\tstd::stack<int> recursion_stack;\n\n\t{\n\t\tint left_border = 0;\n\t\tint right_border = 1;\n\t\t// Sweep linerarily over array and identify those ranges that need to be checked\n\t\tdo {\n\t\t\tif (geom[start + right_border].necessary) {\n\t\t\t\trecursion_stack.push(left_border);\n\t\t\t\trecursion_stack.push(right_border);\n\t\t\t\tleft_border = right_border;\n\t\t\t}\n\t\t\t++right_border;\n\t\t} while (right_border < n);\n\t}\n\n\twhile (!recursion_stack.empty()) {\n\t\t// pop next element\n\t\tint second = recursion_stack.top();\n\t\trecursion_stack.pop();\n\t\tint first = recursion_stack.top();\n\t\trecursion_stack.pop();\n\n\t\tdouble max_distance = -1;\n\t\tint farthest_element_index = second;\n\n\t\t// find index idx of element with max_distance\n\t\tint i;\n\t\tfor (i = first + 1; i < second; i++) {\n\t\t\tdouble temp_dist = square_distance_from_line(geom[start + i].x, geom[start + i].y, geom[start + first].x, geom[start + first].y, geom[start + second].x, geom[start + second].y);\n\n\t\t\tdouble distance = std::fabs(temp_dist);\n\n\t\t\tif ((distance > e || kept < retain) && distance > max_distance) {\n\t\t\t\tfarthest_element_index = i;\n\t\t\t\tmax_distance = distance;\n\t\t\t}\n\t\t}\n\n\t\tif (max_distance >= 0) {\n\t\t\t// mark idx as necessary\n\t\t\tgeom[start + farthest_element_index].necessary = 1;\n\t\t\tkept++;\n\n\t\t\tif (1 < farthest_element_index - first) {\n\t\t\t\trecursion_stack.push(first);\n\t\t\t\trecursion_stack.push(farthest_element_index);\n\t\t\t}\n\t\t\tif (1 < second - farthest_element_index) {\n\t\t\t\trecursion_stack.push(farthest_element_index);\n\t\t\t\trecursion_stack.push(second);\n\t\t\t}\n\t\t}\n\t}\n}\n\n// If any line segment crosses a tile boundary, add a node there\n// that cannot be simplified away, to prevent the edge of any\n// feature from jumping abruptly at the tile boundary.\ndrawvec impose_tile_boundaries(drawvec &geom, long long extent) {\n\tdrawvec out;\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (i > 0 && geom[i].op == VT_LINETO && (geom[i - 1].op == VT_MOVETO || geom[i - 1].op == VT_LINETO)) {\n\t\t\tdouble x1 = geom[i - 1].x;\n\t\t\tdouble y1 = geom[i - 1].y;\n\n\t\t\tdouble x2 = geom[i - 0].x;\n\t\t\tdouble y2 = geom[i - 0].y;\n\n\t\t\tint c = clip(&x1, &y1, &x2, &y2, 0, 0, extent, extent);\n\n\t\t\tif (c > 1) {  // clipped\n\t\t\t\tif (x1 != geom[i - 1].x || y1 != geom[i - 1].y) {\n\t\t\t\t\tout.push_back(draw(VT_LINETO, x1, y1));\n\t\t\t\t\tout[out.size() - 1].necessary = 1;\n\t\t\t\t}\n\t\t\t\tif (x2 != geom[i - 0].x || y2 != geom[i - 0].y) {\n\t\t\t\t\tout.push_back(draw(VT_LINETO, x2, y2));\n\t\t\t\t\tout[out.size() - 1].necessary = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tout.push_back(geom[i]);\n\t}\n\n\treturn out;\n}\n\ndrawvec simplify_lines(drawvec &geom, int z, int detail, bool mark_tile_bounds, double simplification, size_t retain, drawvec const &shared_nodes) {\n\tint res = 1 << (32 - detail - z);\n\tlong long area = 1LL << (32 - z);\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_MOVETO) {\n\t\t\tgeom[i].necessary = 1;\n\t\t} else if (geom[i].op == VT_LINETO) {\n\t\t\tgeom[i].necessary = 0;\n\t\t} else {\n\t\t\tgeom[i].necessary = 1;\n\t\t}\n\n\t\tif (prevent[P_SIMPLIFY_SHARED_NODES]) {\n\t\t\tauto pt = std::lower_bound(shared_nodes.begin(), shared_nodes.end(), geom[i]);\n\t\t\tif (pt != shared_nodes.end() && *pt == geom[i]) {\n\t\t\t\tgeom[i].necessary = true;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (mark_tile_bounds) {\n\t\tgeom = impose_tile_boundaries(geom, area);\n\t}\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_MOVETO) {\n\t\t\tsize_t j;\n\t\t\tfor (j = i + 1; j < geom.size(); j++) {\n\t\t\t\tif (geom[j].op != VT_LINETO) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tgeom[i].necessary = 1;\n\t\t\tgeom[j - 1].necessary = 1;\n\n\t\t\tif (j - i > 1) {\n\t\t\t\tdouglas_peucker(geom, i, j - i, res * simplification, 2, retain);\n\t\t\t}\n\t\t\ti = j - 1;\n\t\t}\n\t}\n\n\tdrawvec out;\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].necessary) {\n\t\t\tout.push_back(geom[i]);\n\t\t}\n\t}\n\n\treturn out;\n}\n\ndrawvec reorder_lines(drawvec &geom) {\n\t// Only reorder simple linestrings with a single moveto\n\n\tif (geom.size() == 0) {\n\t\treturn geom;\n\t}\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_MOVETO) {\n\t\t\tif (i != 0) {\n\t\t\t\treturn geom;\n\t\t\t}\n\t\t} else if (geom[i].op == VT_LINETO) {\n\t\t\tif (i == 0) {\n\t\t\t\treturn geom;\n\t\t\t}\n\t\t} else {\n\t\t\treturn geom;\n\t\t}\n\t}\n\n\t// Reorder anything that goes up and to the left\n\t// instead of down and to the right\n\t// so that it will coalesce better\n\n\tunsigned long long l1 = encode_index(geom[0].x, geom[0].y);\n\tunsigned long long l2 = encode_index(geom[geom.size() - 1].x, geom[geom.size() - 1].y);\n\n\tif (l1 > l2) {\n\t\tdrawvec out;\n\t\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\t\tout.push_back(geom[geom.size() - 1 - i]);\n\t\t}\n\t\tout[0].op = VT_MOVETO;\n\t\tout[out.size() - 1].op = VT_LINETO;\n\t\treturn out;\n\t}\n\n\treturn geom;\n}\n\ndrawvec fix_polygon(drawvec &geom) {\n\tint outer = 1;\n\tdrawvec out;\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_CLOSEPATH) {\n\t\t\touter = 1;\n\t\t} else if (geom[i].op == VT_MOVETO) {\n\t\t\t// Find the end of the ring\n\n\t\t\tsize_t j;\n\t\t\tfor (j = i + 1; j < geom.size(); j++) {\n\t\t\t\tif (geom[j].op != VT_LINETO) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Make a temporary copy of the ring.\n\t\t\t// Close it if it isn't closed.\n\n\t\t\tdrawvec ring;\n\t\t\tfor (size_t a = i; a < j; a++) {\n\t\t\t\tring.push_back(geom[a]);\n\t\t\t}\n\t\t\tif (j - i != 0 && (ring[0].x != ring[j - i - 1].x || ring[0].y != ring[j - i - 1].y)) {\n\t\t\t\tring.push_back(ring[0]);\n\t\t\t}\n\n\t\t\t// Reverse ring if winding order doesn't match\n\t\t\t// inner/outer expectation\n\n\t\t\tbool reverse_ring = false;\n\t\t\tif (prevent[P_USE_SOURCE_POLYGON_WINDING]) {\n\t\t\t\t// GeoJSON winding is reversed from vector winding\n\t\t\t\treverse_ring = true;\n\t\t\t} else if (prevent[P_REVERSE_SOURCE_POLYGON_WINDING]) {\n\t\t\t\t// GeoJSON winding is reversed from vector winding\n\t\t\t\treverse_ring = false;\n\t\t\t} else {\n\t\t\t\tdouble area = get_area(ring, 0, ring.size());\n\t\t\t\tif ((area > 0) != outer) {\n\t\t\t\t\treverse_ring = true;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (reverse_ring) {\n\t\t\t\tdrawvec tmp;\n\t\t\t\tfor (int a = ring.size() - 1; a >= 0; a--) {\n\t\t\t\t\ttmp.push_back(ring[a]);\n\t\t\t\t}\n\t\t\t\tring = tmp;\n\t\t\t}\n\n\t\t\t// Copy ring into output, fixing the moveto/lineto ops if necessary because of\n\t\t\t// reversal or closing\n\n\t\t\tfor (size_t a = 0; a < ring.size(); a++) {\n\t\t\t\tif (a == 0) {\n\t\t\t\t\tout.push_back(draw(VT_MOVETO, ring[a].x, ring[a].y));\n\t\t\t\t} else {\n\t\t\t\t\tout.push_back(draw(VT_LINETO, ring[a].x, ring[a].y));\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Next ring or polygon begins on the non-lineto that ended this one\n\t\t\t// and is not an outer ring unless there is a terminator first\n\n\t\t\ti = j - 1;\n\t\t\touter = 0;\n\t\t} else {\n\t\t\tfprintf(stderr, \"Internal error: polygon ring begins with %d, not moveto\\n\", geom[i].op);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\treturn out;\n}\n\nstd::vector<drawvec> chop_polygon(std::vector<drawvec> &geoms) {\n\twhile (1) {\n\t\tbool again = false;\n\t\tstd::vector<drawvec> out;\n\n\t\tfor (size_t i = 0; i < geoms.size(); i++) {\n\t\t\tif (geoms[i].size() > 700) {\n\t\t\t\tstatic bool warned = false;\n\t\t\t\tif (!warned) {\n\t\t\t\t\tfprintf(stderr, \"Warning: splitting up polygon with more than 700 sides\\n\");\n\t\t\t\t\twarned = true;\n\t\t\t\t}\n\n\t\t\t\tlong long midx = 0, midy = 0, count = 0;\n\t\t\t\tlong long maxx = LLONG_MIN, maxy = LLONG_MIN, minx = LLONG_MAX, miny = LLONG_MAX;\n\n\t\t\t\tfor (size_t j = 0; j < geoms[i].size(); j++) {\n\t\t\t\t\tif (geoms[i][j].op == VT_MOVETO || geoms[i][j].op == VT_LINETO) {\n\t\t\t\t\t\tmidx += geoms[i][j].x;\n\t\t\t\t\t\tmidy += geoms[i][j].y;\n\t\t\t\t\t\tcount++;\n\n\t\t\t\t\t\tif (geoms[i][j].x > maxx) {\n\t\t\t\t\t\t\tmaxx = geoms[i][j].x;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (geoms[i][j].y > maxy) {\n\t\t\t\t\t\t\tmaxy = geoms[i][j].y;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (geoms[i][j].x < minx) {\n\t\t\t\t\t\t\tminx = geoms[i][j].x;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (geoms[i][j].y < miny) {\n\t\t\t\t\t\t\tminy = geoms[i][j].y;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tmidx /= count;\n\t\t\t\tmidy /= count;\n\n\t\t\t\tdrawvec c1, c2;\n\n\t\t\t\tif (maxy - miny > maxx - minx) {\n\t\t\t\t\t// printf(\"clipping y to %lld %lld %lld %lld\\n\", minx, miny, maxx, midy);\n\t\t\t\t\tc1 = simple_clip_poly(geoms[i], minx, miny, maxx, midy);\n\t\t\t\t\t// printf(\"          and %lld %lld %lld %lld\\n\", minx, midy, maxx, maxy);\n\t\t\t\t\tc2 = simple_clip_poly(geoms[i], minx, midy, maxx, maxy);\n\t\t\t\t} else {\n\t\t\t\t\t// printf(\"clipping x to %lld %lld %lld %lld\\n\", minx, miny, midx, maxy);\n\t\t\t\t\tc1 = simple_clip_poly(geoms[i], minx, miny, midx, maxy);\n\t\t\t\t\t// printf(\"          and %lld %lld %lld %lld\\n\", midx, midy, maxx, maxy);\n\t\t\t\t\tc2 = simple_clip_poly(geoms[i], midx, miny, maxx, maxy);\n\t\t\t\t}\n\n\t\t\t\tif (c1.size() >= geoms[i].size()) {\n\t\t\t\t\tfprintf(stderr, \"Subdividing complex polygon failed\\n\");\n\t\t\t\t} else {\n\t\t\t\t\tout.push_back(c1);\n\t\t\t\t}\n\t\t\t\tif (c2.size() >= geoms[i].size()) {\n\t\t\t\t\tfprintf(stderr, \"Subdividing complex polygon failed\\n\");\n\t\t\t\t} else {\n\t\t\t\t\tout.push_back(c2);\n\t\t\t\t}\n\n\t\t\t\tagain = true;\n\t\t\t} else {\n\t\t\t\tout.push_back(geoms[i]);\n\t\t\t}\n\t\t}\n\n\t\tif (!again) {\n\t\t\treturn out;\n\t\t}\n\n\t\tgeoms = out;\n\t}\n}\n\n#define INSIDE 0\n#define LEFT 1\n#define RIGHT 2\n#define BOTTOM 4\n#define TOP 8\n\nstatic int computeOutCode(double x, double y, double xmin, double ymin, double xmax, double ymax) {\n\tint code = INSIDE;\n\n\tif (x < xmin) {\n\t\tcode |= LEFT;\n\t} else if (x > xmax) {\n\t\tcode |= RIGHT;\n\t}\n\n\tif (y < ymin) {\n\t\tcode |= BOTTOM;\n\t} else if (y > ymax) {\n\t\tcode |= TOP;\n\t}\n\n\treturn code;\n}\n\nstatic int clip(double *x0, double *y0, double *x1, double *y1, double xmin, double ymin, double xmax, double ymax) {\n\tint outcode0 = computeOutCode(*x0, *y0, xmin, ymin, xmax, ymax);\n\tint outcode1 = computeOutCode(*x1, *y1, xmin, ymin, xmax, ymax);\n\tint accept = 0;\n\tint changed = 0;\n\n\twhile (1) {\n\t\tif (!(outcode0 | outcode1)) {  // Bitwise OR is 0. Trivially accept and get out of loop\n\t\t\taccept = 1;\n\t\t\tbreak;\n\t\t} else if (outcode0 & outcode1) {  // Bitwise AND is not 0. Trivially reject and get out of loop\n\t\t\tbreak;\n\t\t} else {\n\t\t\t// failed both tests, so calculate the line segment to clip\n\t\t\t// from an outside point to an intersection with clip edge\n\t\t\tdouble x = *x0, y = *y0;\n\n\t\t\t// At least one endpoint is outside the clip rectangle; pick it.\n\t\t\tint outcodeOut = outcode0 ? outcode0 : outcode1;\n\n\t\t\t// Now find the intersection point;\n\t\t\t// use formulas y = y0 + slope * (x - x0), x = x0 + (1 / slope) * (y - y0)\n\t\t\tif (outcodeOut & TOP) {  // point is above the clip rectangle\n\t\t\t\tx = *x0 + (*x1 - *x0) * (ymax - *y0) / (*y1 - *y0);\n\t\t\t\ty = ymax;\n\t\t\t} else if (outcodeOut & BOTTOM) {  // point is below the clip rectangle\n\t\t\t\tx = *x0 + (*x1 - *x0) * (ymin - *y0) / (*y1 - *y0);\n\t\t\t\ty = ymin;\n\t\t\t} else if (outcodeOut & RIGHT) {  // point is to the right of clip rectangle\n\t\t\t\ty = *y0 + (*y1 - *y0) * (xmax - *x0) / (*x1 - *x0);\n\t\t\t\tx = xmax;\n\t\t\t} else if (outcodeOut & LEFT) {  // point is to the left of clip rectangle\n\t\t\t\ty = *y0 + (*y1 - *y0) * (xmin - *x0) / (*x1 - *x0);\n\t\t\t\tx = xmin;\n\t\t\t}\n\n\t\t\t// Now we move outside point to intersection point to clip\n\t\t\t// and get ready for next pass.\n\t\t\tif (outcodeOut == outcode0) {\n\t\t\t\t*x0 = x;\n\t\t\t\t*y0 = y;\n\t\t\t\toutcode0 = computeOutCode(*x0, *y0, xmin, ymin, xmax, ymax);\n\t\t\t\tchanged = 1;\n\t\t\t} else {\n\t\t\t\t*x1 = x;\n\t\t\t\t*y1 = y;\n\t\t\t\toutcode1 = computeOutCode(*x1, *y1, xmin, ymin, xmax, ymax);\n\t\t\t\tchanged = 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (accept == 0) {\n\t\treturn 0;\n\t} else {\n\t\treturn changed + 1;\n\t}\n}\n\ndrawvec stairstep(drawvec &geom, int z, int detail) {\n\tdrawvec out;\n\tdouble scale = 1 << (32 - detail - z);\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tgeom[i].x = std::floor(geom[i].x / scale);\n\t\tgeom[i].y = std::floor(geom[i].y / scale);\n\t}\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_MOVETO) {\n\t\t\tout.push_back(geom[i]);\n\t\t} else if (out.size() > 0) {\n\t\t\tlong long x0 = out[out.size() - 1].x;\n\t\t\tlong long y0 = out[out.size() - 1].y;\n\t\t\tlong long x1 = geom[i].x;\n\t\t\tlong long y1 = geom[i].y;\n\t\t\tbool swap = false;\n\n\t\t\tif (y0 < y1) {\n\t\t\t\tswap = true;\n\t\t\t\tstd::swap(x0, x1);\n\t\t\t\tstd::swap(y0, y1);\n\t\t\t}\n\n\t\t\tlong long xx = x0, yy = y0;\n\t\t\tlong long dx = std::abs(x1 - x0);\n\t\t\tlong long sx = (x0 < x1) ? 1 : -1;\n\t\t\tlong long dy = std::abs(y1 - y0);\n\t\t\tlong long sy = (y0 < y1) ? 1 : -1;\n\t\t\tlong long err = ((dx > dy) ? dx : -dy) / 2;\n\t\t\tint last = -1;\n\n\t\t\tdrawvec tmp;\n\t\t\ttmp.push_back(draw(VT_LINETO, xx, yy));\n\n\t\t\twhile (xx != x1 || yy != y1) {\n\t\t\t\tlong long e2 = err;\n\n\t\t\t\tif (e2 > -dx) {\n\t\t\t\t\terr -= dy;\n\t\t\t\t\txx += sx;\n\t\t\t\t\tif (last == 1) {\n\t\t\t\t\t\ttmp[tmp.size() - 1] = draw(VT_LINETO, xx, yy);\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttmp.push_back(draw(VT_LINETO, xx, yy));\n\t\t\t\t\t}\n\t\t\t\t\tlast = 1;\n\t\t\t\t}\n\t\t\t\tif (e2 < dy) {\n\t\t\t\t\terr += dx;\n\t\t\t\t\tyy += sy;\n\t\t\t\t\tif (last == 2) {\n\t\t\t\t\t\ttmp[tmp.size() - 1] = draw(VT_LINETO, xx, yy);\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttmp.push_back(draw(VT_LINETO, xx, yy));\n\t\t\t\t\t}\n\t\t\t\t\tlast = 2;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (swap) {\n\t\t\t\tfor (size_t j = tmp.size(); j > 0; j--) {\n\t\t\t\t\tout.push_back(tmp[j - 1]);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor (size_t j = 0; j < tmp.size(); j++) {\n\t\t\t\t\tout.push_back(tmp[j]);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// out.push_back(draw(VT_LINETO, xx, yy));\n\t\t} else {\n\t\t\tfprintf(stderr, \"Can't happen: stairstepping lineto with no moveto\\n\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\tfor (size_t i = 0; i < out.size(); i++) {\n\t\tout[i].x *= 1 << (32 - detail - z);\n\t\tout[i].y *= 1 << (32 - detail - z);\n\t}\n\n\treturn out;\n}\n"
        },
        {
          "name": "geometry.hpp",
          "type": "blob",
          "size": 2.4560546875,
          "content": "#ifndef GEOMETRY_HPP\n#define GEOMETRY_HPP\n\n#include <vector>\n#include <atomic>\n#include <sqlite3.h>\n\n#define VT_POINT 1\n#define VT_LINE 2\n#define VT_POLYGON 3\n\n#define VT_END 0\n#define VT_MOVETO 1\n#define VT_LINETO 2\n#define VT_CLOSEPATH 7\n\n// The bitfield is to make sizeof(draw) be 16 instead of 24\n// at the cost, apparently, of a 0.7% increase in running time\n// for packing and unpacking.\nstruct draw {\n\tlong long x : 40;\n\tsigned char op;\n\tlong long y : 40;\n\tsigned char necessary;\n\n\tdraw(int nop, long long nx, long long ny)\n\t    : x(nx),\n\t      op(nop),\n\t      y(ny),\n\t      necessary(0) {\n\t}\n\n\tdraw()\n\t    : x(0),\n\t      op(0),\n\t      y(0),\n\t      necessary(0) {\n\t}\n\n\tbool operator<(draw const &s) const {\n\t\tif (y < s.y || (y == s.y && x < s.x)) {\n\t\t\treturn true;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tbool operator==(draw const &s) const {\n\t\treturn y == s.y && x == s.x;\n\t}\n\n\tbool operator!=(draw const &s) const {\n\t\treturn y != s.y || x != s.x;\n\t}\n};\n\ntypedef std::vector<draw> drawvec;\n\ndrawvec decode_geometry(FILE *meta, std::atomic<long long> *geompos, int z, unsigned tx, unsigned ty, long long *bbox, unsigned initial_x, unsigned initial_y);\nvoid to_tile_scale(drawvec &geom, int z, int detail);\ndrawvec remove_noop(drawvec geom, int type, int shift);\ndrawvec clip_point(drawvec &geom, int z, long long buffer);\ndrawvec clean_or_clip_poly(drawvec &geom, int z, int buffer, bool clip);\ndrawvec simple_clip_poly(drawvec &geom, int z, int buffer);\ndrawvec close_poly(drawvec &geom);\ndrawvec reduce_tiny_poly(drawvec &geom, int z, int detail, bool *reduced, double *accum_area);\ndrawvec clip_lines(drawvec &geom, int z, long long buffer);\ndrawvec stairstep(drawvec &geom, int z, int detail);\nbool point_within_tile(long long x, long long y, int z);\nint quick_check(long long *bbox, int z, long long buffer);\ndrawvec simplify_lines(drawvec &geom, int z, int detail, bool mark_tile_bounds, double simplification, size_t retain, drawvec const &shared_nodes);\ndrawvec reorder_lines(drawvec &geom);\ndrawvec fix_polygon(drawvec &geom);\nstd::vector<drawvec> chop_polygon(std::vector<drawvec> &geoms);\nvoid check_polygon(drawvec &geom);\ndouble get_area(drawvec &geom, size_t i, size_t j);\ndouble get_mp_area(drawvec &geom);\n\ndrawvec simple_clip_poly(drawvec &geom, long long x1, long long y1, long long x2, long long y2);\ndrawvec clip_lines(drawvec &geom, long long x1, long long y1, long long x2, long long y2);\ndrawvec clip_point(drawvec &geom, long long x1, long long y1, long long x2, long long y2);\n\n#endif\n"
        },
        {
          "name": "jsonpull",
          "type": "tree",
          "content": null
        },
        {
          "name": "jsontool.cpp",
          "type": "blob",
          "size": 10.7822265625,
          "content": "#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <string.h>\n#include <stdarg.h>\n#include <unistd.h>\n#include <string>\n#include <getopt.h>\n#include <vector>\n#include \"jsonpull/jsonpull.h\"\n#include \"csv.hpp\"\n#include \"text.hpp\"\n#include \"geojson-loop.hpp\"\n\nint fail = EXIT_SUCCESS;\nbool wrap = false;\nconst char *extract = NULL;\n\nFILE *csvfile = NULL;\nstd::vector<std::string> header;\nstd::vector<std::string> fields;\nint pe = false;\n\nstd::string buffered;\nint buffered_type = -1;\n// 0: nothing yet\n// 1: buffered a line\n// 2: wrote the line and the wrapper\nint buffer_state = 0;\n\nstd::vector<unsigned long> decode32(const char *s) {\n\tstd::vector<unsigned long> utf32;\n\n\twhile (*s != '\\0') {\n\t\tunsigned long b = *(s++) & 0xFF;\n\n\t\tif (b < 0x80) {\n\t\t\tutf32.push_back(b);\n\t\t} else if ((b & 0xe0) == 0xc0) {\n\t\t\tunsigned long c = (b & 0x1f) << 6;\n\t\t\tunsigned long b1 = *(s++) & 0xFF;\n\n\t\t\tif ((b1 & 0xc0) == 0x80) {\n\t\t\t\tc |= b1 & 0x3f;\n\t\t\t\tutf32.push_back(c);\n\t\t\t} else {\n\t\t\t\ts--;\n\t\t\t\tutf32.push_back(0xfffd);\n\t\t\t}\n\t\t} else if ((b & 0xf0) == 0xe0) {\n\t\t\tunsigned long c = (b & 0x0f) << 12;\n\t\t\tunsigned long b1 = *(s++) & 0xFF;\n\n\t\t\tif ((b1 & 0xc0) == 0x80) {\n\t\t\t\tc |= (b1 & 0x3f) << 6;\n\t\t\t\tunsigned long b2 = *(s++) & 0xFF;\n\n\t\t\t\tif ((b2 & 0xc0) == 0x80) {\n\t\t\t\t\tc |= b2 & 0x3f;\n\t\t\t\t\tutf32.push_back(c);\n\t\t\t\t} else {\n\t\t\t\t\ts -= 2;\n\t\t\t\t\tutf32.push_back(0xfffd);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\ts--;\n\t\t\t\tutf32.push_back(0xfffd);\n\t\t\t}\n\t\t} else if ((b & 0xf8) == 0xf0) {\n\t\t\tunsigned long c = (b & 0x07) << 18;\n\t\t\tunsigned long b1 = *(s++) & 0xFF;\n\n\t\t\tif ((b1 & 0xc0) == 0x80) {\n\t\t\t\tc |= (b1 & 0x3f) << 12;\n\t\t\t\tunsigned long b2 = *(s++) & 0xFF;\n\n\t\t\t\tif ((b2 & 0xc0) == 0x80) {\n\t\t\t\t\tc |= (b2 & 0x3f) << 6;\n\t\t\t\t\tunsigned long b3 = *(s++) & 0xFF;\n\n\t\t\t\t\tif ((b3 & 0xc0) == 0x80) {\n\t\t\t\t\t\tc |= b3 & 0x3f;\n\n\t\t\t\t\t\tutf32.push_back(c);\n\t\t\t\t\t} else {\n\t\t\t\t\t\ts -= 3;\n\t\t\t\t\t\tutf32.push_back(0xfffd);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\ts -= 2;\n\t\t\t\t\tutf32.push_back(0xfffd);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\ts -= 1;\n\t\t\t\tutf32.push_back(0xfffd);\n\t\t\t}\n\t\t} else {\n\t\t\tutf32.push_back(0xfffd);\n\t\t}\n\t}\n\n\treturn utf32;\n}\n\n// This uses a really weird encoding for strings\n// so that they will sort in UTF-32 order in spite of quoting\n\nstd::string sort_quote(const char *s) {\n\tstd::vector<unsigned long> utf32 = decode32(s);\n\tstd::string ret;\n\n\tfor (size_t i = 0; i < utf32.size(); i++) {\n\t\tif (utf32[i] < 0xD800) {\n\t\t\tchar buf[8];\n\t\t\tsprintf(buf, \"\\\\u%04lu\", utf32[i]);\n\t\t\tret.append(std::string(buf));\n\t\t} else {\n\t\t\tunsigned long c = utf32[i];\n\n\t\t\tif (c <= 0x7f) {\n\t\t\t\tret.push_back(c);\n\t\t\t} else if (c <= 0x7ff) {\n\t\t\t\tret.push_back(0xc0 | (c >> 6));\n\t\t\t\tret.push_back(0x80 | (c & 0x3f));\n\t\t\t} else if (c <= 0xffff) {\n\t\t\t\tret.push_back(0xe0 | (c >> 12));\n\t\t\t\tret.push_back(0x80 | ((c >> 6) & 0x3f));\n\t\t\t\tret.push_back(0x80 | (c & 0x3f));\n\t\t\t} else {\n\t\t\t\tret.push_back(0xf0 | (c >> 18));\n\t\t\t\tret.push_back(0x80 | ((c >> 12) & 0x3f));\n\t\t\t\tret.push_back(0x80 | ((c >> 6) & 0x3f));\n\t\t\t\tret.push_back(0x80 | (c & 0x3f));\n\t\t\t}\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nvoid out(std::string const &s, int type, json_object *properties) {\n\tif (extract != NULL) {\n\t\tstd::string extracted = sort_quote(\"null\");\n\t\tbool found = false;\n\n\t\tjson_object *o = json_hash_get(properties, extract);\n\t\tif (o != NULL) {\n\t\t\tfound = true;\n\t\t\tif (o->type == JSON_STRING || o->type == JSON_NUMBER) {\n\t\t\t\textracted = sort_quote(o->string);\n\t\t\t} else {\n\t\t\t\t// Don't really know what to do about sort quoting\n\t\t\t\t// for arbitrary objects\n\n\t\t\t\tconst char *out = json_stringify(o);\n\t\t\t\textracted = sort_quote(out);\n\t\t\t\tfree((void *) out);\n\t\t\t}\n\t\t}\n\n\t\tif (!found) {\n\t\t\tstatic bool warned = false;\n\t\t\tif (!warned) {\n\t\t\t\tfprintf(stderr, \"Warning: extract key \\\"%s\\\" not found in JSON\\n\", extract);\n\t\t\t\twarned = true;\n\t\t\t}\n\t\t}\n\n\t\tprintf(\"{\\\"%s\\\":%s}\\n\", extracted.c_str(), s.c_str());\n\t\treturn;\n\t}\n\n\tif (!wrap) {\n\t\tprintf(\"%s\\n\", s.c_str());\n\t\treturn;\n\t}\n\n\tif (buffer_state == 0) {\n\t\tbuffered = s;\n\t\tbuffered_type = type;\n\t\tbuffer_state = 1;\n\t\treturn;\n\t}\n\n\tif (buffer_state == 1) {\n\t\tif (buffered_type == 1) {\n\t\t\tprintf(\"{\\\"type\\\":\\\"FeatureCollection\\\",\\\"features\\\":[\\n\");\n\t\t} else {\n\t\t\tprintf(\"{\\\"type\\\":\\\"GeometryCollection\\\",\\\"geometries\\\":[\\n\");\n\t\t}\n\n\t\tprintf(\"%s\\n\", buffered.c_str());\n\t\tbuffer_state = 2;\n\t}\n\n\tprintf(\",\\n%s\\n\", s.c_str());\n\n\tif (type != buffered_type) {\n\t\tfprintf(stderr, \"Error: mix of bare geometries and features\\n\");\n\t\texit(EXIT_FAILURE);\n\t}\n}\n\nstd::string prev_joinkey;\n\nvoid join_csv(json_object *j) {\n\tif (header.size() == 0) {\n\t\tstd::string s = csv_getline(csvfile);\n\t\tif (s.size() == 0) {\n\t\t\tfprintf(stderr, \"Couldn't get column header from CSV file\\n\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tstd::string err = check_utf8(s);\n\t\tif (err != \"\") {\n\t\t\tfprintf(stderr, \"%s\\n\", err.c_str());\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\theader = csv_split(s.c_str());\n\n\t\tfor (size_t i = 0; i < header.size(); i++) {\n\t\t\theader[i] = csv_dequote(header[i]);\n\t\t}\n\n\t\tif (header.size() == 0) {\n\t\t\tfprintf(stderr, \"No columns in CSV header \\\"%s\\\"\\n\", s.c_str());\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\tjson_object *properties = json_hash_get(j, \"properties\");\n\tjson_object *key = NULL;\n\n\tif (properties != NULL) {\n\t\tkey = json_hash_get(properties, header[0].c_str());\n\t}\n\n\tif (key == NULL) {\n\t\tstatic bool warned = false;\n\t\tif (!warned) {\n\t\t\tfprintf(stderr, \"Warning: couldn't find CSV key \\\"%s\\\" in JSON\\n\", header[0].c_str());\n\t\t\twarned = true;\n\t\t}\n\t\treturn;\n\t}\n\n\tstd::string joinkey;\n\tif (key->type == JSON_STRING || key->type == JSON_NUMBER) {\n\t\tjoinkey = key->string;\n\t} else {\n\t\tconst char *s = json_stringify(key);\n\t\tjoinkey = s;\n\t\tfree((void *) s);\n\t}\n\n\tif (joinkey < prev_joinkey) {\n\t\tfprintf(stderr, \"GeoJSON file is out of sort: \\\"%s\\\" follows \\\"%s\\\"\\n\", joinkey.c_str(), prev_joinkey.c_str());\n\t\texit(EXIT_FAILURE);\n\t}\n\tprev_joinkey = joinkey;\n\n\tif (fields.size() == 0 || joinkey > fields[0]) {\n\t\tstd::string prevkey;\n\t\tif (fields.size() > 0) {\n\t\t\tprevkey = fields[0];\n\t\t}\n\n\t\twhile (true) {\n\t\t\tstd::string s = csv_getline(csvfile);\n\t\t\tif (s.size() == 0) {\n\t\t\t\tfields.clear();\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tstd::string err = check_utf8(s);\n\t\t\tif (err != \"\") {\n\t\t\t\tfprintf(stderr, \"%s\\n\", err.c_str());\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tfields = csv_split(s.c_str());\n\n\t\t\tfor (size_t i = 0; i < fields.size(); i++) {\n\t\t\t\tfields[i] = csv_dequote(fields[i]);\n\t\t\t}\n\n\t\t\tif (fields.size() > 0 && fields[0] < prevkey) {\n\t\t\t\tfprintf(stderr, \"CSV file is out of sort: \\\"%s\\\" follows \\\"%s\\\"\\n\", fields[0].c_str(), prevkey.c_str());\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tif (fields.size() > 0 && fields[0] >= joinkey) {\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (fields.size() > 0) {\n\t\t\t\tprevkey = fields[0];\n\t\t\t}\n\t\t}\n\t}\n\n\tif (fields.size() > 0 && joinkey == fields[0]) {\n\t\t// This knows more about the structure of JSON objects than it ought to\n\t\tproperties->keys = (json_object **) realloc((void *) properties->keys, (properties->length + 32 + fields.size()) * sizeof(json_object *));\n\t\tproperties->values = (json_object **) realloc((void *) properties->values, (properties->length + 32 + fields.size()) * sizeof(json_object *));\n\t\tif (properties->keys == NULL || properties->values == NULL) {\n\t\t\tperror(\"realloc\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tfor (size_t i = 1; i < fields.size(); i++) {\n\t\t\tstd::string k = header[i];\n\t\t\tstd::string v = fields[i];\n\t\t\tjson_type attr_type = JSON_STRING;\n\n\t\t\tif (v.size() > 0) {\n\t\t\t\tif (v[0] == '\"') {\n\t\t\t\t\tv = csv_dequote(v);\n\t\t\t\t} else if (is_number(v)) {\n\t\t\t\t\tattr_type = JSON_NUMBER;\n\t\t\t\t}\n\t\t\t} else if (pe) {\n\t\t\t\tattr_type = JSON_NULL;\n\t\t\t}\n\n\t\t\tif (attr_type != JSON_NULL) {\n\t\t\t\t// This knows more about the structure of JSON objects than it ought to\n\n\t\t\t\tjson_object *ko = (json_object *) malloc(sizeof(json_object));\n\t\t\t\tjson_object *vo = (json_object *) malloc(sizeof(json_object));\n\t\t\t\tif (ko == NULL || vo == NULL) {\n\t\t\t\t\tperror(\"malloc\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\n\t\t\t\tko->type = JSON_STRING;\n\t\t\t\tvo->type = attr_type;\n\n\t\t\t\tko->parent = vo->parent = properties;\n\t\t\t\tko->array = vo->array = NULL;\n\t\t\t\tko->keys = vo->keys = NULL;\n\t\t\t\tko->values = vo->values = NULL;\n\t\t\t\tko->parser = vo->parser = properties->parser;\n\n\t\t\t\tko->string = strdup(k.c_str());\n\t\t\t\tvo->string = strdup(v.c_str());\n\n\t\t\t\tif (ko->string == NULL || vo->string == NULL) {\n\t\t\t\t\tperror(\"strdup\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\n\t\t\t\tko->length = strlen(ko->string);\n\t\t\t\tvo->length = strlen(vo->string);\n\t\t\t\tvo->number = atof(vo->string);\n\n\t\t\t\tproperties->keys[properties->length] = ko;\n\t\t\t\tproperties->values[properties->length] = vo;\n\t\t\t\tproperties->length++;\n\t\t\t}\n\t\t}\n\t}\n}\n\nstruct json_join_action : json_feature_action {\n\tint add_feature(json_object *geometry, bool, json_object *, json_object *, json_object *, json_object *feature) {\n\t\tif (feature != geometry) {  // a real feature, not a bare geometry\n\t\t\tif (csvfile != NULL) {\n\t\t\t\tjoin_csv(feature);\n\t\t\t}\n\n\t\t\tchar *s = json_stringify(feature);\n\t\t\tout(s, 1, json_hash_get(feature, \"properties\"));\n\t\t\tfree(s);\n\t\t} else {\n\t\t\tchar *s = json_stringify(geometry);\n\t\t\tout(s, 2, NULL);\n\t\t\tfree(s);\n\t\t}\n\n\t\treturn 1;\n\t}\n\n\tvoid check_crs(json_object *) {\n\t}\n};\n\nvoid process(FILE *fp, const char *fname) {\n\tjson_pull *jp = json_begin_file(fp);\n\n\tjson_join_action jja;\n\tjja.fname = fname;\n\tparse_json(&jja, jp);\n\tjson_end(jp);\n}\n\nint main(int argc, char **argv) {\n\tconst char *csv = NULL;\n\n\tstruct option long_options[] = {\n\t\t{\"wrap\", no_argument, 0, 'w'},\n\t\t{\"extract\", required_argument, 0, 'e'},\n\t\t{\"csv\", required_argument, 0, 'c'},\n\t\t{\"empty-csv-columns-are-null\", no_argument, &pe, 1},\n\t\t{\"prevent\", required_argument, 0, 'p'},\n\n\t\t{0, 0, 0, 0},\n\t};\n\n\tstd::string getopt_str;\n\tfor (size_t lo = 0; long_options[lo].name != NULL; lo++) {\n\t\tif (long_options[lo].val > ' ') {\n\t\t\tgetopt_str.push_back(long_options[lo].val);\n\n\t\t\tif (long_options[lo].has_arg == required_argument) {\n\t\t\t\tgetopt_str.push_back(':');\n\t\t\t}\n\t\t}\n\t}\n\n\textern int optind;\n\tint i;\n\n\twhile ((i = getopt_long(argc, argv, getopt_str.c_str(), long_options, NULL)) != -1) {\n\t\tswitch (i) {\n\t\tcase 0:\n\t\t\tbreak;\n\n\t\tcase 'w':\n\t\t\twrap = true;\n\t\t\tbreak;\n\n\t\tcase 'e':\n\t\t\textract = optarg;\n\t\t\tbreak;\n\n\t\tcase 'c':\n\t\t\tcsv = optarg;\n\t\t\tbreak;\n\n\t\tcase 'p':\n\t\t\tif (strcmp(optarg, \"e\") == 0) {\n\t\t\t\tpe = true;\n\t\t\t} else {\n\t\t\t\tfprintf(stderr, \"%s: Unknown option for -p%s\\n\", argv[0], optarg);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tfprintf(stderr, \"Unexpected option -%c\\n\", i);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\tif (extract != NULL && wrap) {\n\t\tfprintf(stderr, \"%s: --wrap and --extract not supported together\\n\", argv[0]);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (csv != NULL) {\n\t\tcsvfile = fopen(csv, \"r\");\n\t\tif (csvfile == NULL) {\n\t\t\tperror(csv);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\tif (optind >= argc) {\n\t\tprocess(stdin, \"standard input\");\n\t} else {\n\t\tfor (i = optind; i < argc; i++) {\n\t\t\tFILE *f = fopen(argv[i], \"r\");\n\t\t\tif (f == NULL) {\n\t\t\t\tperror(argv[i]);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tprocess(f, argv[i]);\n\t\t\tfclose(f);\n\t\t}\n\t}\n\n\tif (buffer_state == 1) {\n\t\tprintf(\"%s\\n\", buffered.c_str());\n\t} else if (buffer_state == 2) {\n\t\tprintf(\"]}\\n\");\n\t}\n\n\tif (csvfile != NULL) {\n\t\tif (fclose(csvfile) != 0) {\n\t\t\tperror(\"close\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\treturn fail;\n}\n"
        },
        {
          "name": "main.cpp",
          "type": "blob",
          "size": 90.5791015625,
          "content": "#ifdef MTRACE\n#include <mcheck.h>\n#endif\n\n#ifdef __APPLE__\n#define _DARWIN_UNLIMITED_STREAMS\n#endif\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <string.h>\n#include <unistd.h>\n#include <sys/stat.h>\n#include <sys/types.h>\n#include <sys/mman.h>\n#include <string.h>\n#include <fcntl.h>\n#include <ctype.h>\n#include <errno.h>\n#include <limits.h>\n#include <sqlite3.h>\n#include <stdarg.h>\n#include <sys/resource.h>\n#include <pthread.h>\n#include <getopt.h>\n#include <signal.h>\n#include <sys/time.h>\n#include <zlib.h>\n#include <algorithm>\n#include <vector>\n#include <string>\n#include <set>\n#include <map>\n#include <cmath>\n\n#ifdef __APPLE__\n#include <sys/types.h>\n#include <sys/sysctl.h>\n#include <sys/param.h>\n#include <sys/mount.h>\n#else\n#include <sys/statfs.h>\n#endif\n\n#include \"jsonpull/jsonpull.h\"\n#include \"mbtiles.hpp\"\n#include \"tile.hpp\"\n#include \"pool.hpp\"\n#include \"projection.hpp\"\n#include \"version.hpp\"\n#include \"memfile.hpp\"\n#include \"main.hpp\"\n#include \"geojson.hpp\"\n#include \"geobuf.hpp\"\n#include \"geocsv.hpp\"\n#include \"geometry.hpp\"\n#include \"serial.hpp\"\n#include \"options.hpp\"\n#include \"mvt.hpp\"\n#include \"dirtiles.hpp\"\n#include \"evaluator.hpp\"\n#include \"text.hpp\"\n\nstatic int low_detail = 12;\nstatic int full_detail = -1;\nstatic int min_detail = 7;\n\nint quiet = 0;\nint quiet_progress = 0;\ndouble progress_interval = 0;\nstd::atomic<double> last_progress(0);\nint geometry_scale = 0;\ndouble simplification = 1;\nsize_t max_tile_size = 500000;\nsize_t max_tile_features = 200000;\nint cluster_distance = 0;\nlong justx = -1, justy = -1;\nstd::string attribute_for_id = \"\";\n\nint prevent[256];\nint additional[256];\n\nstruct source {\n\tstd::string layer = \"\";\n\tstd::string file = \"\";\n\tstd::string description = \"\";\n\tstd::string format = \"\";\n};\n\nsize_t CPUS;\nsize_t TEMP_FILES;\nlong long MAX_FILES;\nstatic long long diskfree;\nchar **av;\n\nstd::vector<clipbbox> clipbboxes;\n\nvoid checkdisk(std::vector<struct reader> *r) {\n\tlong long used = 0;\n\tfor (size_t i = 0; i < r->size(); i++) {\n\t\t// Meta, pool, and tree are used once.\n\t\t// Geometry and index will be duplicated during sorting and tiling.\n\t\tused += (*r)[i].metapos + 2 * (*r)[i].geompos + 2 * (*r)[i].indexpos + (*r)[i].poolfile->len + (*r)[i].treefile->len;\n\t}\n\n\tstatic int warned = 0;\n\tif (used > diskfree * .9 && !warned) {\n\t\tfprintf(stderr, \"You will probably run out of disk space.\\n%lld bytes used or committed, of %lld originally available\\n\", used, diskfree);\n\t\twarned = 1;\n\t}\n};\n\nint atoi_require(const char *s, const char *what) {\n\tchar *err = NULL;\n\tif (*s == '\\0') {\n\t\tfprintf(stderr, \"%s: %s must be a number (got %s)\\n\", *av, what, s);\n\t\texit(EXIT_FAILURE);\n\t}\n\tint ret = strtol(s, &err, 10);\n\tif (*err != '\\0') {\n\t\tfprintf(stderr, \"%s: %s must be a number (got %s)\\n\", *av, what, s);\n\t\texit(EXIT_FAILURE);\n\t}\n\treturn ret;\n}\n\ndouble atof_require(const char *s, const char *what) {\n\tchar *err = NULL;\n\tif (*s == '\\0') {\n\t\tfprintf(stderr, \"%s: %s must be a number (got %s)\\n\", *av, what, s);\n\t\texit(EXIT_FAILURE);\n\t}\n\tdouble ret = strtod(s, &err);\n\tif (*err != '\\0') {\n\t\tfprintf(stderr, \"%s: %s must be a number (got %s)\\n\", *av, what, s);\n\t\texit(EXIT_FAILURE);\n\t}\n\treturn ret;\n}\n\nlong long atoll_require(const char *s, const char *what) {\n\tchar *err = NULL;\n\tif (*s == '\\0') {\n\t\tfprintf(stderr, \"%s: %s must be a number (got %s)\\n\", *av, what, s);\n\t\texit(EXIT_FAILURE);\n\t}\n\tlong long ret = strtoll(s, &err, 10);\n\tif (*err != '\\0') {\n\t\tfprintf(stderr, \"%s: %s must be a number (got %s)\\n\", *av, what, s);\n\t\texit(EXIT_FAILURE);\n\t}\n\treturn ret;\n}\n\nvoid init_cpus() {\n\tconst char *TIPPECANOE_MAX_THREADS = getenv(\"TIPPECANOE_MAX_THREADS\");\n\n\tif (TIPPECANOE_MAX_THREADS != NULL) {\n\t\tCPUS = atoi_require(TIPPECANOE_MAX_THREADS, \"TIPPECANOE_MAX_THREADS\");\n\t} else {\n\t\tCPUS = sysconf(_SC_NPROCESSORS_ONLN);\n\t}\n\n\tif (CPUS < 1) {\n\t\tCPUS = 1;\n\t}\n\n\t// Guard against short struct index.segment\n\tif (CPUS > 32767) {\n\t\tCPUS = 32767;\n\t}\n\n\t// Round down to a power of 2\n\tCPUS = 1 << (int) (log(CPUS) / log(2));\n\n\tstruct rlimit rl;\n\tif (getrlimit(RLIMIT_NOFILE, &rl) != 0) {\n\t\tperror(\"getrlimit\");\n\t\texit(EXIT_FAILURE);\n\t} else {\n\t\tMAX_FILES = rl.rlim_cur;\n\t}\n\n\t// Don't really want too many temporary files, because the file system\n\t// will start to bog down eventually\n\tif (MAX_FILES > 2000) {\n\t\tMAX_FILES = 2000;\n\t}\n\n\t// MacOS can run out of system file descriptors\n\t// even if we stay under the rlimit, so try to\n\t// find out the real limit.\n\tlong long fds[MAX_FILES];\n\tlong long i;\n\tfor (i = 0; i < MAX_FILES; i++) {\n\t\tfds[i] = open(\"/dev/null\", O_RDONLY | O_CLOEXEC);\n\t\tif (fds[i] < 0) {\n\t\t\tbreak;\n\t\t}\n\t}\n\tlong long j;\n\tfor (j = 0; j < i; j++) {\n\t\tif (close(fds[j]) < 0) {\n\t\t\tperror(\"close\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\t// Scale down because we really don't want to run the system out of files\n\tMAX_FILES = i * 3 / 4;\n\tif (MAX_FILES < 32) {\n\t\tfprintf(stderr, \"Can't open a useful number of files: %lld\\n\", MAX_FILES);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tTEMP_FILES = (MAX_FILES - 10) / 2;\n\tif (TEMP_FILES > CPUS * 4) {\n\t\tTEMP_FILES = CPUS * 4;\n\t}\n}\n\nint indexcmp(const void *v1, const void *v2) {\n\tconst struct index *i1 = (const struct index *) v1;\n\tconst struct index *i2 = (const struct index *) v2;\n\n\tif (i1->ix < i2->ix) {\n\t\treturn -1;\n\t} else if (i1->ix > i2->ix) {\n\t\treturn 1;\n\t}\n\n\tif (i1->seq < i2->seq) {\n\t\treturn -1;\n\t} else if (i1->seq > i2->seq) {\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstruct mergelist {\n\tlong long start;\n\tlong long end;\n\n\tstruct mergelist *next;\n};\n\nstatic void insert(struct mergelist *m, struct mergelist **head, unsigned char *map) {\n\twhile (*head != NULL && indexcmp(map + m->start, map + (*head)->start) > 0) {\n\t\thead = &((*head)->next);\n\t}\n\n\tm->next = *head;\n\t*head = m;\n}\n\nstruct drop_state {\n\tdouble gap;\n\tunsigned long long previndex;\n\tdouble interval;\n\tdouble scale;\n\tdouble seq;\n\tlong long included;\n\tunsigned x;\n\tunsigned y;\n};\n\nint calc_feature_minzoom(struct index *ix, struct drop_state *ds, int maxzoom, double gamma) {\n\tint feature_minzoom = 0;\n\tunsigned xx, yy;\n\tdecode_index(ix->ix, &xx, &yy);\n\n\tif (gamma >= 0 && (ix->t == VT_POINT ||\n\t\t\t   (additional[A_LINE_DROP] && ix->t == VT_LINE) ||\n\t\t\t   (additional[A_POLYGON_DROP] && ix->t == VT_POLYGON))) {\n\t\tfor (ssize_t i = maxzoom; i >= 0; i--) {\n\t\t\tds[i].seq++;\n\t\t}\n\t\tfor (ssize_t i = maxzoom; i >= 0; i--) {\n\t\t\tif (ds[i].seq >= 0) {\n\t\t\t\tds[i].seq -= ds[i].interval;\n\t\t\t\tds[i].included++;\n\t\t\t} else {\n\t\t\t\tfeature_minzoom = i + 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t// XXX manage_gap\n\t}\n\n\treturn feature_minzoom;\n}\n\nstatic void merge(struct mergelist *merges, size_t nmerges, unsigned char *map, FILE *indexfile, int bytes, char *geom_map, FILE *geom_out, std::atomic<long long> *geompos, long long *progress, long long *progress_max, long long *progress_reported, int maxzoom, double gamma, struct drop_state *ds) {\n\tstruct mergelist *head = NULL;\n\n\tfor (size_t i = 0; i < nmerges; i++) {\n\t\tif (merges[i].start < merges[i].end) {\n\t\t\tinsert(&(merges[i]), &head, map);\n\t\t}\n\t}\n\n\tlast_progress = 0;\n\n\twhile (head != NULL) {\n\t\tstruct index ix = *((struct index *) (map + head->start));\n\t\tlong long pos = *geompos;\n\t\tfwrite_check(geom_map + ix.start, 1, ix.end - ix.start, geom_out, \"merge geometry\");\n\t\t*geompos += ix.end - ix.start;\n\t\tint feature_minzoom = calc_feature_minzoom(&ix, ds, maxzoom, gamma);\n\t\tserialize_byte(geom_out, feature_minzoom, geompos, \"merge geometry\");\n\n\t\t// Count this as an 75%-accomplishment, since we already 25%-counted it\n\t\t*progress += (ix.end - ix.start) * 3 / 4;\n\t\tif (!quiet && !quiet_progress && progress_time() && 100 * *progress / *progress_max != *progress_reported) {\n\t\t\tfprintf(stderr, \"Reordering geometry: %lld%% \\r\", 100 * *progress / *progress_max);\n\t\t\t*progress_reported = 100 * *progress / *progress_max;\n\t\t}\n\n\t\tix.start = pos;\n\t\tix.end = *geompos;\n\t\tfwrite_check(&ix, bytes, 1, indexfile, \"merge temporary\");\n\t\thead->start += bytes;\n\n\t\tstruct mergelist *m = head;\n\t\thead = m->next;\n\t\tm->next = NULL;\n\n\t\tif (m->start < m->end) {\n\t\t\tinsert(m, &head, map);\n\t\t}\n\t}\n}\n\nstruct sort_arg {\n\tint task;\n\tint cpus;\n\tlong long indexpos;\n\tstruct mergelist *merges;\n\tint indexfd;\n\tsize_t nmerges;\n\tlong long unit;\n\tint bytes;\n\n\tsort_arg(int task1, int cpus1, long long indexpos1, struct mergelist *merges1, int indexfd1, size_t nmerges1, long long unit1, int bytes1)\n\t    : task(task1), cpus(cpus1), indexpos(indexpos1), merges(merges1), indexfd(indexfd1), nmerges(nmerges1), unit(unit1), bytes(bytes1) {\n\t}\n};\n\nvoid *run_sort(void *v) {\n\tstruct sort_arg *a = (struct sort_arg *) v;\n\n\tlong long start;\n\tfor (start = a->task * a->unit; start < a->indexpos; start += a->unit * a->cpus) {\n\t\tlong long end = start + a->unit;\n\t\tif (end > a->indexpos) {\n\t\t\tend = a->indexpos;\n\t\t}\n\n\t\ta->merges[start / a->unit].start = start;\n\t\ta->merges[start / a->unit].end = end;\n\t\ta->merges[start / a->unit].next = NULL;\n\n\t\t// MAP_PRIVATE to avoid disk writes if it fits in memory\n\t\tvoid *map = mmap(NULL, end - start, PROT_READ | PROT_WRITE, MAP_PRIVATE, a->indexfd, start);\n\t\tif (map == MAP_FAILED) {\n\t\t\tperror(\"mmap in run_sort\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tmadvise(map, end - start, MADV_RANDOM);\n\t\tmadvise(map, end - start, MADV_WILLNEED);\n\n\t\tqsort(map, (end - start) / a->bytes, a->bytes, indexcmp);\n\n\t\t// Sorting and then copying avoids disk access to\n\t\t// write out intermediate stages of the sort.\n\n\t\tvoid *map2 = mmap(NULL, end - start, PROT_READ | PROT_WRITE, MAP_SHARED, a->indexfd, start);\n\t\tif (map2 == MAP_FAILED) {\n\t\t\tperror(\"mmap (write)\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tmadvise(map2, end - start, MADV_SEQUENTIAL);\n\n\t\tmemcpy(map2, map, end - start);\n\n\t\t// No madvise, since caller will want the sorted data\n\t\tmunmap(map, end - start);\n\t\tmunmap(map2, end - start);\n\t}\n\n\treturn NULL;\n}\n\nvoid do_read_parallel(char *map, long long len, long long initial_offset, const char *reading, std::vector<struct reader> *readers, std::atomic<long long> *progress_seq, std::set<std::string> *exclude, std::set<std::string> *include, int exclude_all, int basezoom, int source, std::vector<std::map<std::string, layermap_entry> > *layermaps, int *initialized, unsigned *initial_x, unsigned *initial_y, int maxzoom, std::string layername, bool uses_gamma, std::map<std::string, int> const *attribute_types, int separator, double *dist_sum, size_t *dist_count, bool want_dist, bool filters) {\n\tlong long segs[CPUS + 1];\n\tsegs[0] = 0;\n\tsegs[CPUS] = len;\n\n\tfor (size_t i = 1; i < CPUS; i++) {\n\t\tsegs[i] = len * i / CPUS;\n\n\t\twhile (segs[i] < len && map[segs[i]] != separator) {\n\t\t\tsegs[i]++;\n\t\t}\n\t}\n\n\tdouble dist_sums[CPUS];\n\tsize_t dist_counts[CPUS];\n\n\tstd::atomic<long long> layer_seq[CPUS];\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\t// To preserve feature ordering, unique id for each segment\n\t\t// begins with that segment's offset into the input\n\t\tlayer_seq[i] = segs[i] + initial_offset;\n\t\tdist_sums[i] = dist_counts[i] = 0;\n\t}\n\n\tstd::vector<parse_json_args> pja;\n\n\tstd::vector<serialization_state> sst;\n\tsst.resize(CPUS);\n\n\tpthread_t pthreads[CPUS];\n\tstd::vector<std::set<type_and_string> > file_subkeys;\n\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\tfile_subkeys.push_back(std::set<type_and_string>());\n\t}\n\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\tsst[i].fname = reading;\n\t\tsst[i].line = 0;\n\t\tsst[i].layer_seq = &layer_seq[i];\n\t\tsst[i].progress_seq = progress_seq;\n\t\tsst[i].readers = readers;\n\t\tsst[i].segment = i;\n\t\tsst[i].initialized = &initialized[i];\n\t\tsst[i].initial_x = &initial_x[i];\n\t\tsst[i].initial_y = &initial_y[i];\n\t\tsst[i].dist_sum = &(dist_sums[i]);\n\t\tsst[i].dist_count = &(dist_counts[i]);\n\t\tsst[i].want_dist = want_dist;\n\t\tsst[i].maxzoom = maxzoom;\n\t\tsst[i].uses_gamma = uses_gamma;\n\t\tsst[i].filters = filters;\n\t\tsst[i].layermap = &(*layermaps)[i];\n\t\tsst[i].exclude = exclude;\n\t\tsst[i].include = include;\n\t\tsst[i].exclude_all = exclude_all;\n\t\tsst[i].basezoom = basezoom;\n\t\tsst[i].attribute_types = attribute_types;\n\n\t\tpja.push_back(parse_json_args(\n\t\t\tjson_begin_map(map + segs[i], segs[i + 1] - segs[i]),\n\t\t\tsource,\n\t\t\t&layername,\n\t\t\t&sst[i]));\n\t}\n\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\tif (pthread_create(&pthreads[i], NULL, run_parse_json, &pja[i]) != 0) {\n\t\t\tperror(\"pthread_create\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\tvoid *retval;\n\n\t\tif (pthread_join(pthreads[i], &retval) != 0) {\n\t\t\tperror(\"pthread_join 370\");\n\t\t}\n\n\t\t*dist_sum += dist_sums[i];\n\t\t*dist_count += dist_counts[i];\n\n\t\tjson_end_map(pja[i].jp);\n\t}\n}\n\nstatic ssize_t read_stream(json_pull *j, char *buffer, size_t n);\n\nstruct STREAM {\n\tFILE *fp = NULL;\n\tgzFile gz = NULL;\n\n\tint fclose() {\n\t\tint ret;\n\n\t\tif (gz != NULL) {\n\t\t\tret = gzclose(gz);\n\t\t} else {\n\t\t\tret = ::fclose(fp);\n\t\t}\n\n\t\tdelete this;\n\t\treturn ret;\n\t}\n\n\tint peekc() {\n\t\tif (gz != NULL) {\n\t\t\tint c = gzgetc(gz);\n\t\t\tif (c != EOF) {\n\t\t\t\tgzungetc(c, gz);\n\t\t\t}\n\t\t\treturn c;\n\t\t} else {\n\t\t\tint c = getc(fp);\n\t\t\tif (c != EOF) {\n\t\t\t\tungetc(c, fp);\n\t\t\t}\n\t\t\treturn c;\n\t\t}\n\t}\n\n\tsize_t read(char *out, size_t count) {\n\t\tif (gz != NULL) {\n\t\t\tint ret = gzread(gz, out, count);\n\t\t\tif (ret < 0) {\n\t\t\t\tfprintf(stderr, \"%s: Error reading compressed data\\n\", *av);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\treturn ret;\n\t\t} else {\n\t\t\treturn ::fread(out, 1, count, fp);\n\t\t}\n\t}\n\n\tjson_pull *json_begin() {\n\t\treturn ::json_begin(read_stream, this);\n\t}\n};\n\nstatic ssize_t read_stream(json_pull *j, char *buffer, size_t n) {\n\treturn ((STREAM *) j->source)->read(buffer, n);\n}\n\nSTREAM *streamfdopen(int fd, const char *mode, std::string const &fname) {\n\tSTREAM *s = new STREAM;\n\ts->fp = NULL;\n\ts->gz = NULL;\n\n\tif (fname.size() > 3 && fname.substr(fname.size() - 3) == std::string(\".gz\")) {\n\t\ts->gz = gzdopen(fd, mode);\n\t\tif (s->gz == NULL) {\n\t\t\tfprintf(stderr, \"%s: %s: Decompression error\\n\", *av, fname.c_str());\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t} else {\n\t\ts->fp = fdopen(fd, mode);\n\t\tif (s->fp == NULL) {\n\t\t\tperror(fname.c_str());\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\treturn s;\n}\n\nSTREAM *streamfpopen(FILE *fp) {\n\tSTREAM *s = new STREAM;\n\ts->fp = fp;\n\ts->gz = NULL;\n\n\treturn s;\n}\n\nstruct read_parallel_arg {\n\tint fd = 0;\n\tSTREAM *fp = NULL;\n\tlong long offset = 0;\n\tlong long len = 0;\n\tstd::atomic<int> *is_parsing = NULL;\n\tint separator = 0;\n\n\tconst char *reading = NULL;\n\tstd::vector<struct reader> *readers = NULL;\n\tstd::atomic<long long> *progress_seq = NULL;\n\tstd::set<std::string> *exclude = NULL;\n\tstd::set<std::string> *include = NULL;\n\tint exclude_all = 0;\n\tint maxzoom = 0;\n\tint basezoom = 0;\n\tint source = 0;\n\tstd::vector<std::map<std::string, layermap_entry> > *layermaps = NULL;\n\tint *initialized = NULL;\n\tunsigned *initial_x = NULL;\n\tunsigned *initial_y = NULL;\n\tstd::string layername = \"\";\n\tbool uses_gamma = false;\n\tstd::map<std::string, int> const *attribute_types = NULL;\n\tdouble *dist_sum = NULL;\n\tsize_t *dist_count = NULL;\n\tbool want_dist = false;\n\tbool filters = false;\n};\n\nvoid *run_read_parallel(void *v) {\n\tstruct read_parallel_arg *rpa = (struct read_parallel_arg *) v;\n\n\tstruct stat st;\n\tif (fstat(rpa->fd, &st) != 0) {\n\t\tperror(\"stat read temp\");\n\t}\n\tif (rpa->len != st.st_size) {\n\t\tfprintf(stderr, \"wrong number of bytes in temporary: %lld vs %lld\\n\", rpa->len, (long long) st.st_size);\n\t}\n\trpa->len = st.st_size;\n\n\tchar *map = (char *) mmap(NULL, rpa->len, PROT_READ, MAP_PRIVATE, rpa->fd, 0);\n\tif (map == NULL || map == MAP_FAILED) {\n\t\tperror(\"map intermediate input\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tmadvise(map, rpa->len, MADV_RANDOM);  // sequential, but from several pointers at once\n\n\tdo_read_parallel(map, rpa->len, rpa->offset, rpa->reading, rpa->readers, rpa->progress_seq, rpa->exclude, rpa->include, rpa->exclude_all, rpa->basezoom, rpa->source, rpa->layermaps, rpa->initialized, rpa->initial_x, rpa->initial_y, rpa->maxzoom, rpa->layername, rpa->uses_gamma, rpa->attribute_types, rpa->separator, rpa->dist_sum, rpa->dist_count, rpa->want_dist, rpa->filters);\n\n\tmadvise(map, rpa->len, MADV_DONTNEED);\n\tif (munmap(map, rpa->len) != 0) {\n\t\tperror(\"munmap source file\");\n\t}\n\tif (rpa->fp->fclose() != 0) {\n\t\tperror(\"close source file\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\t*(rpa->is_parsing) = 0;\n\tdelete rpa;\n\n\treturn NULL;\n}\n\nvoid start_parsing(int fd, STREAM *fp, long long offset, long long len, std::atomic<int> *is_parsing, pthread_t *parallel_parser, bool &parser_created, const char *reading, std::vector<struct reader> *readers, std::atomic<long long> *progress_seq, std::set<std::string> *exclude, std::set<std::string> *include, int exclude_all, int basezoom, int source, std::vector<std::map<std::string, layermap_entry> > &layermaps, int *initialized, unsigned *initial_x, unsigned *initial_y, int maxzoom, std::string layername, bool uses_gamma, std::map<std::string, int> const *attribute_types, int separator, double *dist_sum, size_t *dist_count, bool want_dist, bool filters) {\n\t// This has to kick off an intermediate thread to start the parser threads,\n\t// so the main thread can get back to reading the next input stage while\n\t// the intermediate thread waits for the completion of the parser threads.\n\n\t*is_parsing = 1;\n\n\tstruct read_parallel_arg *rpa = new struct read_parallel_arg;\n\tif (rpa == NULL) {\n\t\tperror(\"Out of memory\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\trpa->fd = fd;\n\trpa->fp = fp;\n\trpa->offset = offset;\n\trpa->len = len;\n\trpa->is_parsing = is_parsing;\n\trpa->separator = separator;\n\n\trpa->reading = reading;\n\trpa->readers = readers;\n\trpa->progress_seq = progress_seq;\n\trpa->exclude = exclude;\n\trpa->include = include;\n\trpa->exclude_all = exclude_all;\n\trpa->basezoom = basezoom;\n\trpa->source = source;\n\trpa->layermaps = &layermaps;\n\trpa->initialized = initialized;\n\trpa->initial_x = initial_x;\n\trpa->initial_y = initial_y;\n\trpa->maxzoom = maxzoom;\n\trpa->layername = layername;\n\trpa->uses_gamma = uses_gamma;\n\trpa->attribute_types = attribute_types;\n\trpa->dist_sum = dist_sum;\n\trpa->dist_count = dist_count;\n\trpa->want_dist = want_dist;\n\trpa->filters = filters;\n\n\tif (pthread_create(parallel_parser, NULL, run_read_parallel, rpa) != 0) {\n\t\tperror(\"pthread_create\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tparser_created = true;\n}\n\nvoid radix1(int *geomfds_in, int *indexfds_in, int inputs, int prefix, int splits, long long mem, const char *tmpdir, long long *availfiles, FILE *geomfile, FILE *indexfile, std::atomic<long long> *geompos_out, long long *progress, long long *progress_max, long long *progress_reported, int maxzoom, int basezoom, double droprate, double gamma, struct drop_state *ds) {\n\t// Arranged as bits to facilitate subdividing again if a subdivided file is still huge\n\tint splitbits = log(splits) / log(2);\n\tsplits = 1 << splitbits;\n\n\tFILE *geomfiles[splits];\n\tFILE *indexfiles[splits];\n\tint geomfds[splits];\n\tint indexfds[splits];\n\tstd::atomic<long long> sub_geompos[splits];\n\n\tint i;\n\tfor (i = 0; i < splits; i++) {\n\t\tsub_geompos[i] = 0;\n\n\t\tchar geomname[strlen(tmpdir) + strlen(\"/geom.XXXXXXXX\") + 1];\n\t\tsprintf(geomname, \"%s%s\", tmpdir, \"/geom.XXXXXXXX\");\n\t\tchar indexname[strlen(tmpdir) + strlen(\"/index.XXXXXXXX\") + 1];\n\t\tsprintf(indexname, \"%s%s\", tmpdir, \"/index.XXXXXXXX\");\n\n\t\tgeomfds[i] = mkstemp_cloexec(geomname);\n\t\tif (geomfds[i] < 0) {\n\t\t\tperror(geomname);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tindexfds[i] = mkstemp_cloexec(indexname);\n\t\tif (indexfds[i] < 0) {\n\t\t\tperror(indexname);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tgeomfiles[i] = fopen_oflag(geomname, \"wb\", O_WRONLY | O_CLOEXEC);\n\t\tif (geomfiles[i] == NULL) {\n\t\t\tperror(geomname);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tindexfiles[i] = fopen_oflag(indexname, \"wb\", O_WRONLY | O_CLOEXEC);\n\t\tif (indexfiles[i] == NULL) {\n\t\t\tperror(indexname);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\t*availfiles -= 4;\n\n\t\tunlink(geomname);\n\t\tunlink(indexname);\n\t}\n\n\tfor (i = 0; i < inputs; i++) {\n\t\tstruct stat geomst, indexst;\n\t\tif (fstat(geomfds_in[i], &geomst) < 0) {\n\t\t\tperror(\"stat geom\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (fstat(indexfds_in[i], &indexst) < 0) {\n\t\t\tperror(\"stat index\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tif (indexst.st_size != 0) {\n\t\t\tstruct index *indexmap = (struct index *) mmap(NULL, indexst.st_size, PROT_READ, MAP_PRIVATE, indexfds_in[i], 0);\n\t\t\tif (indexmap == MAP_FAILED) {\n\t\t\t\tfprintf(stderr, \"fd %lld, len %lld\\n\", (long long) indexfds_in[i], (long long) indexst.st_size);\n\t\t\t\tperror(\"map index\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tmadvise(indexmap, indexst.st_size, MADV_SEQUENTIAL);\n\t\t\tmadvise(indexmap, indexst.st_size, MADV_WILLNEED);\n\t\t\tchar *geommap = (char *) mmap(NULL, geomst.st_size, PROT_READ, MAP_PRIVATE, geomfds_in[i], 0);\n\t\t\tif (geommap == MAP_FAILED) {\n\t\t\t\tperror(\"map geom\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tmadvise(geommap, geomst.st_size, MADV_SEQUENTIAL);\n\t\t\tmadvise(geommap, geomst.st_size, MADV_WILLNEED);\n\n\t\t\tfor (size_t a = 0; a < indexst.st_size / sizeof(struct index); a++) {\n\t\t\t\tstruct index ix = indexmap[a];\n\t\t\t\tunsigned long long which = (ix.ix << prefix) >> (64 - splitbits);\n\t\t\t\tlong long pos = sub_geompos[which];\n\n\t\t\t\tfwrite_check(geommap + ix.start, ix.end - ix.start, 1, geomfiles[which], \"geom\");\n\t\t\t\tsub_geompos[which] += ix.end - ix.start;\n\n\t\t\t\t// Count this as a 25%-accomplishment, since we will copy again\n\t\t\t\t*progress += (ix.end - ix.start) / 4;\n\t\t\t\tif (!quiet && !quiet_progress && progress_time() && 100 * *progress / *progress_max != *progress_reported) {\n\t\t\t\t\tfprintf(stderr, \"Reordering geometry: %lld%% \\r\", 100 * *progress / *progress_max);\n\t\t\t\t\t*progress_reported = 100 * *progress / *progress_max;\n\t\t\t\t}\n\n\t\t\t\tix.start = pos;\n\t\t\t\tix.end = sub_geompos[which];\n\n\t\t\t\tfwrite_check(&ix, sizeof(struct index), 1, indexfiles[which], \"index\");\n\t\t\t}\n\n\t\t\tmadvise(indexmap, indexst.st_size, MADV_DONTNEED);\n\t\t\tif (munmap(indexmap, indexst.st_size) < 0) {\n\t\t\t\tperror(\"unmap index\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tmadvise(geommap, geomst.st_size, MADV_DONTNEED);\n\t\t\tif (munmap(geommap, geomst.st_size) < 0) {\n\t\t\t\tperror(\"unmap geom\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t}\n\n\t\tif (close(geomfds_in[i]) < 0) {\n\t\t\tperror(\"close geom\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (close(indexfds_in[i]) < 0) {\n\t\t\tperror(\"close index\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\t*availfiles += 2;\n\t}\n\n\tfor (i = 0; i < splits; i++) {\n\t\tif (fclose(geomfiles[i]) != 0) {\n\t\t\tperror(\"fclose geom\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (fclose(indexfiles[i]) != 0) {\n\t\t\tperror(\"fclose index\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\t*availfiles += 2;\n\t}\n\n\tfor (i = 0; i < splits; i++) {\n\t\tint already_closed = 0;\n\n\t\tstruct stat geomst, indexst;\n\t\tif (fstat(geomfds[i], &geomst) < 0) {\n\t\t\tperror(\"stat geom\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (fstat(indexfds[i], &indexst) < 0) {\n\t\t\tperror(\"stat index\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tif (indexst.st_size > 0) {\n\t\t\tif (indexst.st_size + geomst.st_size < mem) {\n\t\t\t\tstd::atomic<long long> indexpos(indexst.st_size);\n\t\t\t\tint bytes = sizeof(struct index);\n\n\t\t\t\tint page = sysconf(_SC_PAGESIZE);\n\t\t\t\t// Don't try to sort more than 2GB at once,\n\t\t\t\t// which used to crash Macs and may still\n\t\t\t\tlong long max_unit = 2LL * 1024 * 1024 * 1024;\n\t\t\t\tlong long unit = ((indexpos / CPUS + bytes - 1) / bytes) * bytes;\n\t\t\t\tif (unit > max_unit) {\n\t\t\t\t\tunit = max_unit;\n\t\t\t\t}\n\t\t\t\tunit = ((unit + page - 1) / page) * page;\n\t\t\t\tif (unit < page) {\n\t\t\t\t\tunit = page;\n\t\t\t\t}\n\n\t\t\t\tsize_t nmerges = (indexpos + unit - 1) / unit;\n\t\t\t\tstruct mergelist merges[nmerges];\n\n\t\t\t\tfor (size_t a = 0; a < nmerges; a++) {\n\t\t\t\t\tmerges[a].start = merges[a].end = 0;\n\t\t\t\t}\n\n\t\t\t\tpthread_t pthreads[CPUS];\n\t\t\t\tstd::vector<sort_arg> args;\n\n\t\t\t\tfor (size_t a = 0; a < CPUS; a++) {\n\t\t\t\t\targs.push_back(sort_arg(\n\t\t\t\t\t\ta,\n\t\t\t\t\t\tCPUS,\n\t\t\t\t\t\tindexpos,\n\t\t\t\t\t\tmerges,\n\t\t\t\t\t\tindexfds[i],\n\t\t\t\t\t\tnmerges,\n\t\t\t\t\t\tunit,\n\t\t\t\t\t\tbytes));\n\t\t\t\t}\n\n\t\t\t\tfor (size_t a = 0; a < CPUS; a++) {\n\t\t\t\t\tif (pthread_create(&pthreads[a], NULL, run_sort, &args[a]) != 0) {\n\t\t\t\t\t\tperror(\"pthread_create\");\n\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tfor (size_t a = 0; a < CPUS; a++) {\n\t\t\t\t\tvoid *retval;\n\n\t\t\t\t\tif (pthread_join(pthreads[a], &retval) != 0) {\n\t\t\t\t\t\tperror(\"pthread_join 679\");\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tstruct indexmap *indexmap = (struct indexmap *) mmap(NULL, indexst.st_size, PROT_READ, MAP_PRIVATE, indexfds[i], 0);\n\t\t\t\tif (indexmap == MAP_FAILED) {\n\t\t\t\t\tfprintf(stderr, \"fd %lld, len %lld\\n\", (long long) indexfds[i], (long long) indexst.st_size);\n\t\t\t\t\tperror(\"map index\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t\tmadvise(indexmap, indexst.st_size, MADV_RANDOM);  // sequential, but from several pointers at once\n\t\t\t\tmadvise(indexmap, indexst.st_size, MADV_WILLNEED);\n\t\t\t\tchar *geommap = (char *) mmap(NULL, geomst.st_size, PROT_READ, MAP_PRIVATE, geomfds[i], 0);\n\t\t\t\tif (geommap == MAP_FAILED) {\n\t\t\t\t\tperror(\"map geom\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t\tmadvise(geommap, geomst.st_size, MADV_RANDOM);\n\t\t\t\tmadvise(geommap, geomst.st_size, MADV_WILLNEED);\n\n\t\t\t\tmerge(merges, nmerges, (unsigned char *) indexmap, indexfile, bytes, geommap, geomfile, geompos_out, progress, progress_max, progress_reported, maxzoom, gamma, ds);\n\n\t\t\t\tmadvise(indexmap, indexst.st_size, MADV_DONTNEED);\n\t\t\t\tif (munmap(indexmap, indexst.st_size) < 0) {\n\t\t\t\t\tperror(\"unmap index\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t\tmadvise(geommap, geomst.st_size, MADV_DONTNEED);\n\t\t\t\tif (munmap(geommap, geomst.st_size) < 0) {\n\t\t\t\t\tperror(\"unmap geom\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t} else if (indexst.st_size == sizeof(struct index) || prefix + splitbits >= 64) {\n\t\t\t\tstruct index *indexmap = (struct index *) mmap(NULL, indexst.st_size, PROT_READ, MAP_PRIVATE, indexfds[i], 0);\n\t\t\t\tif (indexmap == MAP_FAILED) {\n\t\t\t\t\tfprintf(stderr, \"fd %lld, len %lld\\n\", (long long) indexfds[i], (long long) indexst.st_size);\n\t\t\t\t\tperror(\"map index\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t\tmadvise(indexmap, indexst.st_size, MADV_SEQUENTIAL);\n\t\t\t\tmadvise(indexmap, indexst.st_size, MADV_WILLNEED);\n\t\t\t\tchar *geommap = (char *) mmap(NULL, geomst.st_size, PROT_READ, MAP_PRIVATE, geomfds[i], 0);\n\t\t\t\tif (geommap == MAP_FAILED) {\n\t\t\t\t\tperror(\"map geom\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t\tmadvise(geommap, geomst.st_size, MADV_RANDOM);\n\t\t\t\tmadvise(geommap, geomst.st_size, MADV_WILLNEED);\n\n\t\t\t\tfor (size_t a = 0; a < indexst.st_size / sizeof(struct index); a++) {\n\t\t\t\t\tstruct index ix = indexmap[a];\n\t\t\t\t\tlong long pos = *geompos_out;\n\n\t\t\t\t\tfwrite_check(geommap + ix.start, ix.end - ix.start, 1, geomfile, \"geom\");\n\t\t\t\t\t*geompos_out += ix.end - ix.start;\n\t\t\t\t\tint feature_minzoom = calc_feature_minzoom(&ix, ds, maxzoom, gamma);\n\t\t\t\t\tserialize_byte(geomfile, feature_minzoom, geompos_out, \"merge geometry\");\n\n\t\t\t\t\t// Count this as an 75%-accomplishment, since we already 25%-counted it\n\t\t\t\t\t*progress += (ix.end - ix.start) * 3 / 4;\n\t\t\t\t\tif (!quiet && !quiet_progress && progress_time() && 100 * *progress / *progress_max != *progress_reported) {\n\t\t\t\t\t\tfprintf(stderr, \"Reordering geometry: %lld%% \\r\", 100 * *progress / *progress_max);\n\t\t\t\t\t\t*progress_reported = 100 * *progress / *progress_max;\n\t\t\t\t\t}\n\n\t\t\t\t\tix.start = pos;\n\t\t\t\t\tix.end = *geompos_out;\n\t\t\t\t\tfwrite_check(&ix, sizeof(struct index), 1, indexfile, \"index\");\n\t\t\t\t}\n\n\t\t\t\tmadvise(indexmap, indexst.st_size, MADV_DONTNEED);\n\t\t\t\tif (munmap(indexmap, indexst.st_size) < 0) {\n\t\t\t\t\tperror(\"unmap index\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t\tmadvise(geommap, geomst.st_size, MADV_DONTNEED);\n\t\t\t\tif (munmap(geommap, geomst.st_size) < 0) {\n\t\t\t\t\tperror(\"unmap geom\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// We already reported the progress from splitting this radix out\n\t\t\t\t// but we need to split it again, which will be credited with more\n\t\t\t\t// progress. So increase the total amount of progress to report by\n\t\t\t\t// the additional progress that will happpen, which may move the\n\t\t\t\t// counter backward but will be an honest estimate of the work remaining.\n\t\t\t\t*progress_max += geomst.st_size / 4;\n\n\t\t\t\tradix1(&geomfds[i], &indexfds[i], 1, prefix + splitbits, *availfiles / 4, mem, tmpdir, availfiles, geomfile, indexfile, geompos_out, progress, progress_max, progress_reported, maxzoom, basezoom, droprate, gamma, ds);\n\t\t\t\talready_closed = 1;\n\t\t\t}\n\t\t}\n\n\t\tif (!already_closed) {\n\t\t\tif (close(geomfds[i]) < 0) {\n\t\t\t\tperror(\"close geom\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tif (close(indexfds[i]) < 0) {\n\t\t\t\tperror(\"close index\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\t*availfiles += 2;\n\t\t}\n\t}\n}\n\nvoid prep_drop_states(struct drop_state *ds, int maxzoom, int basezoom, double droprate) {\n\t// Needs to be signed for interval calculation\n\tfor (ssize_t i = 0; i <= maxzoom; i++) {\n\t\tds[i].gap = 0;\n\t\tds[i].previndex = 0;\n\t\tds[i].interval = 0;\n\n\t\tif (i < basezoom) {\n\t\t\tds[i].interval = std::exp(std::log(droprate) * (basezoom - i));\n\t\t}\n\n\t\tds[i].scale = (double) (1LL << (64 - 2 * (i + 8)));\n\t\tds[i].seq = 0;\n\t\tds[i].included = 0;\n\t\tds[i].x = 0;\n\t\tds[i].y = 0;\n\t}\n}\n\nvoid radix(std::vector<struct reader> &readers, int nreaders, FILE *geomfile, FILE *indexfile, const char *tmpdir, std::atomic<long long> *geompos, int maxzoom, int basezoom, double droprate, double gamma) {\n\t// Run through the index and geometry for each reader,\n\t// splitting the contents out by index into as many\n\t// sub-files as we can write to simultaneously.\n\n\t// Then sort each of those by index, recursively if it is\n\t// too big to fit in memory.\n\n\t// Then concatenate each of the sub-outputs into a final output.\n\n\tlong long mem;\n\n#ifdef __APPLE__\n\tint64_t hw_memsize;\n\tsize_t len = sizeof(int64_t);\n\tif (sysctlbyname(\"hw.memsize\", &hw_memsize, &len, NULL, 0) < 0) {\n\t\tperror(\"sysctl hw.memsize\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tmem = hw_memsize;\n#else\n\tlong long pagesize = sysconf(_SC_PAGESIZE);\n\tlong long pages = sysconf(_SC_PHYS_PAGES);\n\tif (pages < 0 || pagesize < 0) {\n\t\tperror(\"sysconf _SC_PAGESIZE or _SC_PHYS_PAGES\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tmem = (long long) pages * pagesize;\n#endif\n\n\t// Just for code coverage testing. Deeply recursive sorting is very slow\n\t// compared to sorting in memory.\n\tif (additional[A_PREFER_RADIX_SORT]) {\n\t\tmem = 8192;\n\t}\n\n\tlong long availfiles = MAX_FILES - 2 * nreaders  // each reader has a geom and an index\n\t\t\t       - 4\t\t\t // pool, meta, mbtiles, mbtiles journal\n\t\t\t       - 4\t\t\t // top-level geom and index output, both FILE and fd\n\t\t\t       - 3;\t\t\t // stdin, stdout, stderr\n\n\t// 4 because for each we have output and input FILE and fd for geom and index\n\tint splits = availfiles / 4;\n\n\t// Be somewhat conservative about memory availability because the whole point of this\n\t// is to keep from thrashing by working on chunks that will fit in memory.\n\tmem /= 2;\n\n\tlong long geom_total = 0;\n\tint geomfds[nreaders];\n\tint indexfds[nreaders];\n\tfor (int i = 0; i < nreaders; i++) {\n\t\tgeomfds[i] = readers[i].geomfd;\n\t\tindexfds[i] = readers[i].indexfd;\n\n\t\tstruct stat geomst;\n\t\tif (fstat(readers[i].geomfd, &geomst) < 0) {\n\t\t\tperror(\"stat geom\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tgeom_total += geomst.st_size;\n\t}\n\n\tstruct drop_state ds[maxzoom + 1];\n\tprep_drop_states(ds, maxzoom, basezoom, droprate);\n\n\tlong long progress = 0, progress_max = geom_total, progress_reported = -1;\n\tlong long availfiles_before = availfiles;\n\tradix1(geomfds, indexfds, nreaders, 0, splits, mem, tmpdir, &availfiles, geomfile, indexfile, geompos, &progress, &progress_max, &progress_reported, maxzoom, basezoom, droprate, gamma, ds);\n\n\tif (availfiles - 2 * nreaders != availfiles_before) {\n\t\tfprintf(stderr, \"Internal error: miscounted available file descriptors: %lld vs %lld\\n\", availfiles - 2 * nreaders, availfiles);\n\t\texit(EXIT_FAILURE);\n\t}\n}\n\nvoid choose_first_zoom(long long *file_bbox, std::vector<struct reader> &readers, unsigned *iz, unsigned *ix, unsigned *iy, int minzoom, int buffer) {\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\tif (readers[i].file_bbox[0] < file_bbox[0]) {\n\t\t\tfile_bbox[0] = readers[i].file_bbox[0];\n\t\t}\n\t\tif (readers[i].file_bbox[1] < file_bbox[1]) {\n\t\t\tfile_bbox[1] = readers[i].file_bbox[1];\n\t\t}\n\t\tif (readers[i].file_bbox[2] > file_bbox[2]) {\n\t\t\tfile_bbox[2] = readers[i].file_bbox[2];\n\t\t}\n\t\tif (readers[i].file_bbox[3] > file_bbox[3]) {\n\t\t\tfile_bbox[3] = readers[i].file_bbox[3];\n\t\t}\n\t}\n\n\t// If the bounding box extends off the plane on either side,\n\t// a feature wrapped across the date line, so the width of the\n\t// bounding box is the whole world.\n\tif (file_bbox[0] < 0) {\n\t\tfile_bbox[0] = 0;\n\t\tfile_bbox[2] = (1LL << 32) - 1;\n\t}\n\tif (file_bbox[2] > (1LL << 32) - 1) {\n\t\tfile_bbox[0] = 0;\n\t\tfile_bbox[2] = (1LL << 32) - 1;\n\t}\n\tif (file_bbox[1] < 0) {\n\t\tfile_bbox[1] = 0;\n\t}\n\tif (file_bbox[3] > (1LL << 32) - 1) {\n\t\tfile_bbox[3] = (1LL << 32) - 1;\n\t}\n\n\tfor (ssize_t z = minzoom; z >= 0; z--) {\n\t\tlong long shift = 1LL << (32 - z);\n\n\t\tlong long left = (file_bbox[0] - buffer * shift / 256) / shift;\n\t\tlong long top = (file_bbox[1] - buffer * shift / 256) / shift;\n\t\tlong long right = (file_bbox[2] + buffer * shift / 256) / shift;\n\t\tlong long bottom = (file_bbox[3] + buffer * shift / 256) / shift;\n\n\t\tif (left == right && top == bottom) {\n\t\t\t*iz = z;\n\t\t\t*ix = left;\n\t\t\t*iy = top;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nint read_input(std::vector<source> &sources, char *fname, int maxzoom, int minzoom, int basezoom, double basezoom_marker_width, sqlite3 *outdb, const char *outdir, std::set<std::string> *exclude, std::set<std::string> *include, int exclude_all, json_object *filter, double droprate, int buffer, const char *tmpdir, double gamma, int read_parallel, int forcetable, const char *attribution, bool uses_gamma, long long *file_bbox, const char *prefilter, const char *postfilter, const char *description, bool guess_maxzoom, std::map<std::string, int> const *attribute_types, const char *pgm, std::map<std::string, attribute_op> const *attribute_accum, std::map<std::string, std::string> const &attribute_descriptions, std::string const &commandline) {\n\tint ret = EXIT_SUCCESS;\n\n\tstd::vector<struct reader> readers;\n\treaders.resize(CPUS);\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\tstruct reader *r = &readers[i];\n\n\t\tchar metaname[strlen(tmpdir) + strlen(\"/meta.XXXXXXXX\") + 1];\n\t\tchar poolname[strlen(tmpdir) + strlen(\"/pool.XXXXXXXX\") + 1];\n\t\tchar treename[strlen(tmpdir) + strlen(\"/tree.XXXXXXXX\") + 1];\n\t\tchar geomname[strlen(tmpdir) + strlen(\"/geom.XXXXXXXX\") + 1];\n\t\tchar indexname[strlen(tmpdir) + strlen(\"/index.XXXXXXXX\") + 1];\n\n\t\tsprintf(metaname, \"%s%s\", tmpdir, \"/meta.XXXXXXXX\");\n\t\tsprintf(poolname, \"%s%s\", tmpdir, \"/pool.XXXXXXXX\");\n\t\tsprintf(treename, \"%s%s\", tmpdir, \"/tree.XXXXXXXX\");\n\t\tsprintf(geomname, \"%s%s\", tmpdir, \"/geom.XXXXXXXX\");\n\t\tsprintf(indexname, \"%s%s\", tmpdir, \"/index.XXXXXXXX\");\n\n\t\tr->metafd = mkstemp_cloexec(metaname);\n\t\tif (r->metafd < 0) {\n\t\t\tperror(metaname);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tr->poolfd = mkstemp_cloexec(poolname);\n\t\tif (r->poolfd < 0) {\n\t\t\tperror(poolname);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tr->treefd = mkstemp_cloexec(treename);\n\t\tif (r->treefd < 0) {\n\t\t\tperror(treename);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tr->geomfd = mkstemp_cloexec(geomname);\n\t\tif (r->geomfd < 0) {\n\t\t\tperror(geomname);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tr->indexfd = mkstemp_cloexec(indexname);\n\t\tif (r->indexfd < 0) {\n\t\t\tperror(indexname);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tr->metafile = fopen_oflag(metaname, \"wb\", O_WRONLY | O_CLOEXEC);\n\t\tif (r->metafile == NULL) {\n\t\t\tperror(metaname);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tr->poolfile = memfile_open(r->poolfd);\n\t\tif (r->poolfile == NULL) {\n\t\t\tperror(poolname);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tr->treefile = memfile_open(r->treefd);\n\t\tif (r->treefile == NULL) {\n\t\t\tperror(treename);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tr->geomfile = fopen_oflag(geomname, \"wb\", O_WRONLY | O_CLOEXEC);\n\t\tif (r->geomfile == NULL) {\n\t\t\tperror(geomname);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tr->indexfile = fopen_oflag(indexname, \"wb\", O_WRONLY | O_CLOEXEC);\n\t\tif (r->indexfile == NULL) {\n\t\t\tperror(indexname);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tr->metapos = 0;\n\t\tr->geompos = 0;\n\t\tr->indexpos = 0;\n\n\t\tunlink(metaname);\n\t\tunlink(poolname);\n\t\tunlink(treename);\n\t\tunlink(geomname);\n\t\tunlink(indexname);\n\n\t\t// To distinguish a null value\n\t\t{\n\t\t\tstruct stringpool p;\n\t\t\tmemfile_write(r->treefile, &p, sizeof(struct stringpool));\n\t\t}\n\t\t// Keep metadata file from being completely empty if no attributes\n\t\tserialize_int(r->metafile, 0, &r->metapos, \"meta\");\n\n\t\tr->file_bbox[0] = r->file_bbox[1] = UINT_MAX;\n\t\tr->file_bbox[2] = r->file_bbox[3] = 0;\n\t}\n\n\tstruct statfs fsstat;\n\tif (fstatfs(readers[0].geomfd, &fsstat) != 0) {\n\t\tperror(\"fstatfs\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tdiskfree = (long long) fsstat.f_bsize * fsstat.f_bavail;\n\n\tstd::atomic<long long> progress_seq(0);\n\n\t// 2 * CPUS: One per reader thread, one per tiling thread\n\tint initialized[2 * CPUS];\n\tunsigned initial_x[2 * CPUS], initial_y[2 * CPUS];\n\tfor (size_t i = 0; i < 2 * CPUS; i++) {\n\t\tinitialized[i] = initial_x[i] = initial_y[i] = 0;\n\t}\n\n\tsize_t nlayers = sources.size();\n\tfor (size_t l = 0; l < nlayers; l++) {\n\t\tif (sources[l].layer.size() == 0) {\n\t\t\tconst char *src;\n\t\t\tif (sources[l].file.size() == 0) {\n\t\t\t\tsrc = fname;\n\t\t\t} else {\n\t\t\t\tsrc = sources[l].file.c_str();\n\t\t\t}\n\n\t\t\t// Find the last component of the pathname\n\t\t\tconst char *ocp, *use = src;\n\t\t\tfor (ocp = src; *ocp; ocp++) {\n\t\t\t\tif (*ocp == '/' && ocp[1] != '\\0') {\n\t\t\t\t\tuse = ocp + 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\tstd::string trunc = std::string(use);\n\n\t\t\tstd::vector<std::string> trim = {\n\t\t\t\t\".json\",\n\t\t\t\t\".geojson\",\n\t\t\t\t\".geobuf\",\n\t\t\t\t\".mbtiles\",\n\t\t\t\t\".csv\",\n\t\t\t\t\".gz\",\n\t\t\t};\n\n\t\t\t// Trim .json or .mbtiles from the name\n\t\t\tbool again = true;\n\t\t\twhile (again) {\n\t\t\t\tagain = false;\n\t\t\t\tfor (size_t i = 0; i < trim.size(); i++) {\n\t\t\t\t\tif (trunc.size() > trim[i].size() && trunc.substr(trunc.size() - trim[i].size()) == trim[i]) {\n\t\t\t\t\t\ttrunc = trunc.substr(0, trunc.size() - trim[i].size());\n\t\t\t\t\t\tagain = true;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Trim out characters that can't be part of selector\n\t\t\tstd::string out;\n\t\t\tfor (size_t p = 0; p < trunc.size(); p++) {\n\t\t\t\tif (isalpha(trunc[p]) || isdigit(trunc[p]) || trunc[p] == '_' || (trunc[p] & 0x80) != 0) {\n\t\t\t\t\tout.append(trunc, p, 1);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tsources[l].layer = out;\n\t\t\tif (sources[l].layer.size() == 0 || check_utf8(out).size() != 0) {\n\t\t\t\tsources[l].layer = \"unknown\" + std::to_string(l);\n\t\t\t}\n\n\t\t\tif (!quiet) {\n\t\t\t\tfprintf(stderr, \"For layer %d, using name \\\"%s\\\"\\n\", (int) l, sources[l].layer.c_str());\n\t\t\t}\n\t\t}\n\t}\n\n\tstd::map<std::string, layermap_entry> layermap;\n\tfor (size_t l = 0; l < nlayers; l++) {\n\t\tlayermap_entry e = layermap_entry(l);\n\t\te.description = sources[l].description;\n\t\tlayermap.insert(std::pair<std::string, layermap_entry>(sources[l].layer, e));\n\t}\n\n\tstd::vector<std::map<std::string, layermap_entry> > layermaps;\n\tfor (size_t l = 0; l < CPUS; l++) {\n\t\tlayermaps.push_back(layermap);\n\t}\n\n\tlong overall_offset = 0;\n\tdouble dist_sum = 0;\n\tsize_t dist_count = 0;\n\n\tint files_open_before_reading = open(\"/dev/null\", O_RDONLY | O_CLOEXEC);\n\tif (files_open_before_reading < 0) {\n\t\tperror(\"open /dev/null\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tif (close(files_open_before_reading) != 0) {\n\t\tperror(\"close\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tsize_t nsources = sources.size();\n\tfor (size_t source = 0; source < nsources; source++) {\n\t\tstd::string reading;\n\t\tint fd;\n\n\t\tif (sources[source].file.size() == 0) {\n\t\t\treading = \"standard input\";\n\t\t\tfd = 0;\n\t\t} else {\n\t\t\treading = sources[source].file;\n\t\t\tfd = open(sources[source].file.c_str(), O_RDONLY, O_CLOEXEC);\n\t\t\tif (fd < 0) {\n\t\t\t\tperror(sources[source].file.c_str());\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tauto a = layermap.find(sources[source].layer);\n\t\tif (a == layermap.end()) {\n\t\t\tfprintf(stderr, \"Internal error: couldn't find layer %s\", sources[source].layer.c_str());\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tsize_t layer = a->second.id;\n\n\t\tif (sources[source].format == \"geobuf\" || (sources[source].file.size() > 7 && sources[source].file.substr(sources[source].file.size() - 7) == std::string(\".geobuf\"))) {\n\t\t\tstruct stat st;\n\t\t\tif (fstat(fd, &st) != 0) {\n\t\t\t\tperror(\"fstat\");\n\t\t\t\tperror(sources[source].file.c_str());\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tchar *map = (char *) mmap(NULL, st.st_size, PROT_READ, MAP_PRIVATE, fd, 0);\n\t\t\tif (map == MAP_FAILED) {\n\t\t\t\tfprintf(stderr, \"%s: mmap: %s: %s\\n\", *av, reading.c_str(), strerror(errno));\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tstd::atomic<long long> layer_seq[CPUS];\n\t\t\tdouble dist_sums[CPUS];\n\t\t\tsize_t dist_counts[CPUS];\n\t\t\tstd::vector<struct serialization_state> sst;\n\t\t\tsst.resize(CPUS);\n\n\t\t\tfor (size_t i = 0; i < CPUS; i++) {\n\t\t\t\tlayer_seq[i] = overall_offset;\n\t\t\t\tdist_sums[i] = 0;\n\t\t\t\tdist_counts[i] = 0;\n\n\t\t\t\tsst[i].fname = reading.c_str();\n\t\t\t\tsst[i].line = 0;\n\t\t\t\tsst[i].layer_seq = &layer_seq[i];\n\t\t\t\tsst[i].progress_seq = &progress_seq;\n\t\t\t\tsst[i].readers = &readers;\n\t\t\t\tsst[i].segment = i;\n\t\t\t\tsst[i].initial_x = &initial_x[i];\n\t\t\t\tsst[i].initial_y = &initial_y[i];\n\t\t\t\tsst[i].initialized = &initialized[i];\n\t\t\t\tsst[i].dist_sum = &dist_sums[i];\n\t\t\t\tsst[i].dist_count = &dist_counts[i];\n\t\t\t\tsst[i].want_dist = guess_maxzoom;\n\t\t\t\tsst[i].maxzoom = maxzoom;\n\t\t\t\tsst[i].filters = prefilter != NULL || postfilter != NULL;\n\t\t\t\tsst[i].uses_gamma = uses_gamma;\n\t\t\t\tsst[i].layermap = &layermaps[i];\n\t\t\t\tsst[i].exclude = exclude;\n\t\t\t\tsst[i].include = include;\n\t\t\t\tsst[i].exclude_all = exclude_all;\n\t\t\t\tsst[i].basezoom = basezoom;\n\t\t\t\tsst[i].attribute_types = attribute_types;\n\t\t\t}\n\n\t\t\tparse_geobuf(&sst, map, st.st_size, layer, sources[layer].layer);\n\n\t\t\tfor (size_t i = 0; i < CPUS; i++) {\n\t\t\t\tdist_sum += dist_sums[i];\n\t\t\t\tdist_count += dist_counts[i];\n\t\t\t}\n\n\t\t\tif (munmap(map, st.st_size) != 0) {\n\t\t\t\tperror(\"munmap source file\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tif (close(fd) != 0) {\n\t\t\t\tperror(\"close\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\toverall_offset = layer_seq[0];\n\t\t\tcheckdisk(&readers);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (sources[source].format == \"csv\" || (sources[source].file.size() > 4 && sources[source].file.substr(sources[source].file.size() - 4) == std::string(\".csv\"))) {\n\t\t\tstd::atomic<long long> layer_seq[CPUS];\n\t\t\tdouble dist_sums[CPUS];\n\t\t\tsize_t dist_counts[CPUS];\n\n\t\t\tstd::vector<struct serialization_state> sst;\n\t\t\tsst.resize(CPUS);\n\n\t\t\t// XXX factor out this duplicated setup\n\t\t\tfor (size_t i = 0; i < CPUS; i++) {\n\t\t\t\tlayer_seq[i] = overall_offset;\n\t\t\t\tdist_sums[i] = 0;\n\t\t\t\tdist_counts[i] = 0;\n\n\t\t\t\tsst[i].fname = reading.c_str();\n\t\t\t\tsst[i].line = 0;\n\t\t\t\tsst[i].layer_seq = &layer_seq[i];\n\t\t\t\tsst[i].progress_seq = &progress_seq;\n\t\t\t\tsst[i].readers = &readers;\n\t\t\t\tsst[i].segment = i;\n\t\t\t\tsst[i].initial_x = &initial_x[i];\n\t\t\t\tsst[i].initial_y = &initial_y[i];\n\t\t\t\tsst[i].initialized = &initialized[i];\n\t\t\t\tsst[i].dist_sum = &dist_sums[i];\n\t\t\t\tsst[i].dist_count = &dist_counts[i];\n\t\t\t\tsst[i].want_dist = guess_maxzoom;\n\t\t\t\tsst[i].maxzoom = maxzoom;\n\t\t\t\tsst[i].filters = prefilter != NULL || postfilter != NULL;\n\t\t\t\tsst[i].uses_gamma = uses_gamma;\n\t\t\t\tsst[i].layermap = &layermaps[i];\n\t\t\t\tsst[i].exclude = exclude;\n\t\t\t\tsst[i].include = include;\n\t\t\t\tsst[i].exclude_all = exclude_all;\n\t\t\t\tsst[i].basezoom = basezoom;\n\t\t\t\tsst[i].attribute_types = attribute_types;\n\t\t\t}\n\n\t\t\tparse_geocsv(sst, sources[source].file, layer, sources[layer].layer);\n\n\t\t\tif (close(fd) != 0) {\n\t\t\t\tperror(\"close\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\toverall_offset = layer_seq[0];\n\t\t\tcheckdisk(&readers);\n\t\t\tcontinue;\n\t\t}\n\n\t\tstruct stat st;\n\t\tchar *map = NULL;\n\t\toff_t off = 0;\n\n\t\tint read_parallel_this = read_parallel ? '\\n' : 0;\n\n\t\tif (!(sources[source].file.size() > 3 && sources[source].file.substr(sources[source].file.size() - 3) == std::string(\".gz\"))) {\n\t\t\tif (fstat(fd, &st) == 0) {\n\t\t\t\toff = lseek(fd, 0, SEEK_CUR);\n\t\t\t\tif (off >= 0) {\n\t\t\t\t\tmap = (char *) mmap(NULL, st.st_size - off, PROT_READ, MAP_PRIVATE, fd, off);\n\t\t\t\t\t// No error if MAP_FAILED because check is below\n\t\t\t\t\tif (map != MAP_FAILED) {\n\t\t\t\t\t\tmadvise(map, st.st_size - off, MADV_RANDOM);  // sequential, but from several pointers at once\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (map != NULL && map != MAP_FAILED && st.st_size - off > 0) {\n\t\t\tif (map[0] == 0x1E) {\n\t\t\t\tread_parallel_this = 0x1E;\n\t\t\t}\n\n\t\t\tif (!read_parallel_this) {\n\t\t\t\t// Not a GeoJSON text sequence, so unmap and read serially\n\n\t\t\t\tif (munmap(map, st.st_size - off) != 0) {\n\t\t\t\t\tperror(\"munmap source file\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\n\t\t\t\tmap = NULL;\n\t\t\t}\n\t\t}\n\n\t\tif (map != NULL && map != MAP_FAILED && read_parallel_this) {\n\t\t\tdo_read_parallel(map, st.st_size - off, overall_offset, reading.c_str(), &readers, &progress_seq, exclude, include, exclude_all, basezoom, layer, &layermaps, initialized, initial_x, initial_y, maxzoom, sources[layer].layer, uses_gamma, attribute_types, read_parallel_this, &dist_sum, &dist_count, guess_maxzoom, prefilter != NULL || postfilter != NULL);\n\t\t\toverall_offset += st.st_size - off;\n\t\t\tcheckdisk(&readers);\n\n\t\t\tif (munmap(map, st.st_size - off) != 0) {\n\t\t\t\tperror(\"munmap source file\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tif (close(fd) != 0) {\n\t\t\t\tperror(\"close input file\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t} else {\n\t\t\tSTREAM *fp = streamfdopen(fd, \"r\", sources[layer].file);\n\t\t\tif (fp == NULL) {\n\t\t\t\tperror(sources[layer].file.c_str());\n\t\t\t\tif (close(fd) != 0) {\n\t\t\t\t\tperror(\"close source file\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tint c = fp->peekc();\n\t\t\tif (c == 0x1E) {\n\t\t\t\tread_parallel_this = 0x1E;\n\t\t\t}\n\n\t\t\tif (read_parallel_this) {\n\t\t\t\t// Serial reading of chunks that are then parsed in parallel\n\n\t\t\t\tchar readname[strlen(tmpdir) + strlen(\"/read.XXXXXXXX\") + 1];\n\t\t\t\tsprintf(readname, \"%s%s\", tmpdir, \"/read.XXXXXXXX\");\n\t\t\t\tint readfd = mkstemp_cloexec(readname);\n\t\t\t\tif (readfd < 0) {\n\t\t\t\t\tperror(readname);\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t\tFILE *readfp = fdopen(readfd, \"w\");\n\t\t\t\tif (readfp == NULL) {\n\t\t\t\t\tperror(readname);\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t\tunlink(readname);\n\n\t\t\t\tstd::atomic<int> is_parsing(0);\n\t\t\t\tlong long ahead = 0;\n\t\t\t\tlong long initial_offset = overall_offset;\n\t\t\t\tpthread_t parallel_parser;\n\t\t\t\tbool parser_created = false;\n\n#define READ_BUF 2000\n#define PARSE_MIN 10000000\n#define PARSE_MAX (1LL * 1024 * 1024 * 1024)\n\n\t\t\t\tchar buf[READ_BUF];\n\t\t\t\tint n;\n\n\t\t\t\twhile ((n = fp->read(buf, READ_BUF)) > 0) {\n\t\t\t\t\tfwrite_check(buf, sizeof(char), n, readfp, reading.c_str());\n\t\t\t\t\tahead += n;\n\n\t\t\t\t\tif (buf[n - 1] == read_parallel_this && ahead > PARSE_MIN) {\n\t\t\t\t\t\t// Don't let the streaming reader get too far ahead of the parsers.\n\t\t\t\t\t\t// If the buffered input gets huge, even if the parsers are still running,\n\t\t\t\t\t\t// wait for the parser thread instead of continuing to stream input.\n\n\t\t\t\t\t\tif (is_parsing == 0 || ahead >= PARSE_MAX) {\n\t\t\t\t\t\t\tif (parser_created) {\n\t\t\t\t\t\t\t\tif (pthread_join(parallel_parser, NULL) != 0) {\n\t\t\t\t\t\t\t\t\tperror(\"pthread_join 1088\");\n\t\t\t\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tparser_created = false;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tfflush(readfp);\n\t\t\t\t\t\t\tstart_parsing(readfd, streamfpopen(readfp), initial_offset, ahead, &is_parsing, &parallel_parser, parser_created, reading.c_str(), &readers, &progress_seq, exclude, include, exclude_all, basezoom, layer, layermaps, initialized, initial_x, initial_y, maxzoom, sources[layer].layer, gamma != 0, attribute_types, read_parallel_this, &dist_sum, &dist_count, guess_maxzoom, prefilter != NULL || postfilter != NULL);\n\n\t\t\t\t\t\t\tinitial_offset += ahead;\n\t\t\t\t\t\t\toverall_offset += ahead;\n\t\t\t\t\t\t\tcheckdisk(&readers);\n\t\t\t\t\t\t\tahead = 0;\n\n\t\t\t\t\t\t\tsprintf(readname, \"%s%s\", tmpdir, \"/read.XXXXXXXX\");\n\t\t\t\t\t\t\treadfd = mkstemp_cloexec(readname);\n\t\t\t\t\t\t\tif (readfd < 0) {\n\t\t\t\t\t\t\t\tperror(readname);\n\t\t\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\treadfp = fdopen(readfd, \"w\");\n\t\t\t\t\t\t\tif (readfp == NULL) {\n\t\t\t\t\t\t\t\tperror(readname);\n\t\t\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tunlink(readname);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (n < 0) {\n\t\t\t\t\tperror(reading.c_str());\n\t\t\t\t}\n\n\t\t\t\tif (parser_created) {\n\t\t\t\t\tif (pthread_join(parallel_parser, NULL) != 0) {\n\t\t\t\t\t\tperror(\"pthread_join 1122\");\n\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t}\n\t\t\t\t\tparser_created = false;\n\t\t\t\t}\n\n\t\t\t\tfflush(readfp);\n\n\t\t\t\tif (ahead > 0) {\n\t\t\t\t\tstart_parsing(readfd, streamfpopen(readfp), initial_offset, ahead, &is_parsing, &parallel_parser, parser_created, reading.c_str(), &readers, &progress_seq, exclude, include, exclude_all, basezoom, layer, layermaps, initialized, initial_x, initial_y, maxzoom, sources[layer].layer, gamma != 0, attribute_types, read_parallel_this, &dist_sum, &dist_count, guess_maxzoom, prefilter != NULL || postfilter != NULL);\n\n\t\t\t\t\tif (parser_created) {\n\t\t\t\t\t\tif (pthread_join(parallel_parser, NULL) != 0) {\n\t\t\t\t\t\t\tperror(\"pthread_join 1133\");\n\t\t\t\t\t\t}\n\t\t\t\t\t\tparser_created = false;\n\t\t\t\t\t}\n\n\t\t\t\t\toverall_offset += ahead;\n\t\t\t\t\tcheckdisk(&readers);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// Plain serial reading\n\n\t\t\t\tstd::atomic<long long> layer_seq(overall_offset);\n\t\t\t\tjson_pull *jp = fp->json_begin();\n\t\t\t\tstruct serialization_state sst;\n\n\t\t\t\tsst.fname = reading.c_str();\n\t\t\t\tsst.line = 0;\n\t\t\t\tsst.layer_seq = &layer_seq;\n\t\t\t\tsst.progress_seq = &progress_seq;\n\t\t\t\tsst.readers = &readers;\n\t\t\t\tsst.segment = 0;\n\t\t\t\tsst.initial_x = &initial_x[0];\n\t\t\t\tsst.initial_y = &initial_y[0];\n\t\t\t\tsst.initialized = &initialized[0];\n\t\t\t\tsst.dist_sum = &dist_sum;\n\t\t\t\tsst.dist_count = &dist_count;\n\t\t\t\tsst.want_dist = guess_maxzoom;\n\t\t\t\tsst.maxzoom = maxzoom;\n\t\t\t\tsst.filters = prefilter != NULL || postfilter != NULL;\n\t\t\t\tsst.uses_gamma = uses_gamma;\n\t\t\t\tsst.layermap = &layermaps[0];\n\t\t\t\tsst.exclude = exclude;\n\t\t\t\tsst.include = include;\n\t\t\t\tsst.exclude_all = exclude_all;\n\t\t\t\tsst.basezoom = basezoom;\n\t\t\t\tsst.attribute_types = attribute_types;\n\n\t\t\t\tparse_json(&sst, jp, layer, sources[layer].layer);\n\t\t\t\tjson_end(jp);\n\t\t\t\toverall_offset = layer_seq;\n\t\t\t\tcheckdisk(&readers);\n\t\t\t}\n\n\t\t\tif (fp->fclose() != 0) {\n\t\t\t\tperror(\"fclose input\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t}\n\t}\n\n\tint files_open_after_reading = open(\"/dev/null\", O_RDONLY | O_CLOEXEC);\n\tif (files_open_after_reading < 0) {\n\t\tperror(\"open /dev/null\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tif (close(files_open_after_reading) != 0) {\n\t\tperror(\"close\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (files_open_after_reading > files_open_before_reading) {\n\t\tfprintf(stderr, \"Internal error: Files left open after reading input. (%d vs %d)\\n\",\n\t\t\tfiles_open_before_reading, files_open_after_reading);\n\t\tret = EXIT_FAILURE;\n\t}\n\n\tif (!quiet) {\n\t\tfprintf(stderr, \"                              \\r\");\n\t\t//     (stderr, \"Read 10000.00 million features\\r\", *progress_seq / 1000000.0);\n\t}\n\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\tif (fclose(readers[i].metafile) != 0) {\n\t\t\tperror(\"fclose meta\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (fclose(readers[i].geomfile) != 0) {\n\t\t\tperror(\"fclose geom\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (fclose(readers[i].indexfile) != 0) {\n\t\t\tperror(\"fclose index\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tmemfile_close(readers[i].treefile);\n\n\t\tif (fstat(readers[i].geomfd, &readers[i].geomst) != 0) {\n\t\t\tperror(\"stat geom\\n\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (fstat(readers[i].metafd, &readers[i].metast) != 0) {\n\t\t\tperror(\"stat meta\\n\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\t// Create a combined string pool and a combined metadata file\n\t// but keep track of the offsets into it since we still need\n\t// segment+offset to find the data.\n\n\t// 2 * CPUS: One per input thread, one per tiling thread\n\tlong long pool_off[2 * CPUS];\n\tlong long meta_off[2 * CPUS];\n\tfor (size_t i = 0; i < 2 * CPUS; i++) {\n\t\tpool_off[i] = meta_off[i] = 0;\n\t}\n\n\tchar poolname[strlen(tmpdir) + strlen(\"/pool.XXXXXXXX\") + 1];\n\tsprintf(poolname, \"%s%s\", tmpdir, \"/pool.XXXXXXXX\");\n\n\tint poolfd = mkstemp_cloexec(poolname);\n\tif (poolfd < 0) {\n\t\tperror(poolname);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tFILE *poolfile = fopen_oflag(poolname, \"wb\", O_WRONLY | O_CLOEXEC);\n\tif (poolfile == NULL) {\n\t\tperror(poolname);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tunlink(poolname);\n\n\tchar metaname[strlen(tmpdir) + strlen(\"/meta.XXXXXXXX\") + 1];\n\tsprintf(metaname, \"%s%s\", tmpdir, \"/meta.XXXXXXXX\");\n\n\tint metafd = mkstemp_cloexec(metaname);\n\tif (metafd < 0) {\n\t\tperror(metaname);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tFILE *metafile = fopen_oflag(metaname, \"wb\", O_WRONLY | O_CLOEXEC);\n\tif (metafile == NULL) {\n\t\tperror(metaname);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tunlink(metaname);\n\n\tstd::atomic<long long> metapos(0);\n\tstd::atomic<long long> poolpos(0);\n\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\tif (readers[i].metapos > 0) {\n\t\t\tvoid *map = mmap(NULL, readers[i].metapos, PROT_READ, MAP_PRIVATE, readers[i].metafd, 0);\n\t\t\tif (map == MAP_FAILED) {\n\t\t\t\tperror(\"mmap unmerged meta\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tmadvise(map, readers[i].metapos, MADV_SEQUENTIAL);\n\t\t\tmadvise(map, readers[i].metapos, MADV_WILLNEED);\n\t\t\tif (fwrite(map, readers[i].metapos, 1, metafile) != 1) {\n\t\t\t\tperror(\"Reunify meta\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tmadvise(map, readers[i].metapos, MADV_DONTNEED);\n\t\t\tif (munmap(map, readers[i].metapos) != 0) {\n\t\t\t\tperror(\"unmap unmerged meta\");\n\t\t\t}\n\t\t}\n\n\t\tmeta_off[i] = metapos;\n\t\tmetapos += readers[i].metapos;\n\t\tif (close(readers[i].metafd) != 0) {\n\t\t\tperror(\"close unmerged meta\");\n\t\t}\n\n\t\tif (readers[i].poolfile->off > 0) {\n\t\t\tif (fwrite(readers[i].poolfile->map, readers[i].poolfile->off, 1, poolfile) != 1) {\n\t\t\t\tperror(\"Reunify string pool\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t}\n\n\t\tpool_off[i] = poolpos;\n\t\tpoolpos += readers[i].poolfile->off;\n\t\tmemfile_close(readers[i].poolfile);\n\t}\n\n\tif (fclose(poolfile) != 0) {\n\t\tperror(\"fclose pool\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tif (fclose(metafile) != 0) {\n\t\tperror(\"fclose meta\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tchar *meta = (char *) mmap(NULL, metapos, PROT_READ, MAP_PRIVATE, metafd, 0);\n\tif (meta == MAP_FAILED) {\n\t\tperror(\"mmap meta\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tmadvise(meta, metapos, MADV_RANDOM);\n\n\tchar *stringpool = NULL;\n\tif (poolpos > 0) {  // Will be 0 if -X was specified\n\t\tstringpool = (char *) mmap(NULL, poolpos, PROT_READ, MAP_PRIVATE, poolfd, 0);\n\t\tif (stringpool == MAP_FAILED) {\n\t\t\tperror(\"mmap string pool\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tmadvise(stringpool, poolpos, MADV_RANDOM);\n\t}\n\n\tchar indexname[strlen(tmpdir) + strlen(\"/index.XXXXXXXX\") + 1];\n\tsprintf(indexname, \"%s%s\", tmpdir, \"/index.XXXXXXXX\");\n\n\tint indexfd = mkstemp_cloexec(indexname);\n\tif (indexfd < 0) {\n\t\tperror(indexname);\n\t\texit(EXIT_FAILURE);\n\t}\n\tFILE *indexfile = fopen_oflag(indexname, \"wb\", O_WRONLY | O_CLOEXEC);\n\tif (indexfile == NULL) {\n\t\tperror(indexname);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tunlink(indexname);\n\n\tchar geomname[strlen(tmpdir) + strlen(\"/geom.XXXXXXXX\") + 1];\n\tsprintf(geomname, \"%s%s\", tmpdir, \"/geom.XXXXXXXX\");\n\n\tint geomfd = mkstemp_cloexec(geomname);\n\tif (geomfd < 0) {\n\t\tperror(geomname);\n\t\texit(EXIT_FAILURE);\n\t}\n\tFILE *geomfile = fopen_oflag(geomname, \"wb\", O_WRONLY | O_CLOEXEC);\n\tif (geomfile == NULL) {\n\t\tperror(geomname);\n\t\texit(EXIT_FAILURE);\n\t}\n\tunlink(geomname);\n\n\tunsigned iz = 0, ix = 0, iy = 0;\n\tchoose_first_zoom(file_bbox, readers, &iz, &ix, &iy, minzoom, buffer);\n\n\tif (justx >= 0) {\n\t\tiz = minzoom;\n\t\tix = justx;\n\t\tiy = justy;\n\t}\n\n\tstd::atomic<long long> geompos(0);\n\n\t/* initial tile is 0/0/0 */\n\tserialize_int(geomfile, iz, &geompos, fname);\n\tserialize_uint(geomfile, ix, &geompos, fname);\n\tserialize_uint(geomfile, iy, &geompos, fname);\n\n\tradix(readers, CPUS, geomfile, indexfile, tmpdir, &geompos, maxzoom, basezoom, droprate, gamma);\n\n\t/* end of tile */\n\tserialize_byte(geomfile, -2, &geompos, fname);\n\n\tif (fclose(geomfile) != 0) {\n\t\tperror(\"fclose geom\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tif (fclose(indexfile) != 0) {\n\t\tperror(\"fclose index\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tstruct stat indexst;\n\tif (fstat(indexfd, &indexst) < 0) {\n\t\tperror(\"stat index\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tstd::atomic<long long> indexpos(indexst.st_size);\n\tprogress_seq = indexpos / sizeof(struct index);\n\n\tlast_progress = 0;\n\tif (!quiet) {\n\t\tlong long s = progress_seq;\n\t\tlong long geompos_print = geompos;\n\t\tlong long metapos_print = metapos;\n\t\tlong long poolpos_print = poolpos;\n\t\tfprintf(stderr, \"%lld features, %lld bytes of geometry, %lld bytes of separate metadata, %lld bytes of string pool\\n\", s, geompos_print, metapos_print, poolpos_print);\n\t}\n\n\tif (indexpos == 0) {\n\t\tfprintf(stderr, \"Did not read any valid geometries\\n\");\n\t\tif (outdb != NULL) {\n\t\t\tmbtiles_close(outdb, pgm);\n\t\t}\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tstruct index *map = (struct index *) mmap(NULL, indexpos, PROT_READ, MAP_PRIVATE, indexfd, 0);\n\tif (map == MAP_FAILED) {\n\t\tperror(\"mmap index for basezoom\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tmadvise(map, indexpos, MADV_SEQUENTIAL);\n\tmadvise(map, indexpos, MADV_WILLNEED);\n\tlong long indices = indexpos / sizeof(struct index);\n\tbool fix_dropping = false;\n\n\tif (guess_maxzoom) {\n\t\tdouble sum = 0;\n\t\tsize_t count = 0;\n\n\t\tlong long progress = -1;\n\t\tlong long ip;\n\t\tfor (ip = 1; ip < indices; ip++) {\n\t\t\tif (map[ip].ix != map[ip - 1].ix) {\n\t\t\t\tcount++;\n\t\t\t\tsum += log(map[ip].ix - map[ip - 1].ix);\n\t\t\t}\n\n\t\t\tlong long nprogress = 100 * ip / indices;\n\t\t\tif (nprogress != progress) {\n\t\t\t\tprogress = nprogress;\n\t\t\t\tif (!quiet && !quiet_progress && progress_time()) {\n\t\t\t\t\tfprintf(stderr, \"Maxzoom: %lld%% \\r\", progress);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (count == 0 && dist_count == 0) {\n\t\t\tfprintf(stderr, \"Can't guess maxzoom (-zg) without at least two distinct feature locations\\n\");\n\t\t\tif (outdb != NULL) {\n\t\t\t\tmbtiles_close(outdb, pgm);\n\t\t\t}\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tif (count > 0) {\n\t\t\t// Geometric mean is appropriate because distances between features\n\t\t\t// are typically lognormally distributed\n\t\t\tdouble avg = exp(sum / count);\n\n\t\t\t// Convert approximately from tile units to feet\n\t\t\tdouble dist_ft = sqrt(avg) / 33;\n\t\t\t// Factor of 8 (3 zooms) beyond minimum required to distinguish features\n\t\t\tdouble want = dist_ft / 8;\n\n\t\t\tmaxzoom = ceil(log(360 / (.00000274 * want)) / log(2) - full_detail);\n\t\t\tif (maxzoom < 0) {\n\t\t\t\tmaxzoom = 0;\n\t\t\t}\n\t\t\tif (maxzoom > 32 - full_detail) {\n\t\t\t\tmaxzoom = 32 - full_detail;\n\t\t\t}\n\t\t\tif (maxzoom > 33 - low_detail) {  // that is, maxzoom - 1 > 32 - low_detail\n\t\t\t\tmaxzoom = 33 - low_detail;\n\t\t\t}\n\n\t\t\tif (!quiet) {\n\t\t\t\tfprintf(stderr, \"Choosing a maxzoom of -z%d for features about %d feet (%d meters) apart\\n\", maxzoom, (int) ceil(dist_ft), (int) ceil(dist_ft / 3.28084));\n\t\t\t}\n\n\t\t\tbool changed = false;\n\t\t\twhile (maxzoom < 32 - full_detail && maxzoom < 33 - low_detail && cluster_distance > 0) {\n\t\t\t\tunsigned long long zoom_mingap = ((1LL << (32 - maxzoom)) / 256 * cluster_distance) * ((1LL << (32 - maxzoom)) / 256 * cluster_distance);\n\t\t\t\tif (avg > zoom_mingap) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tmaxzoom++;\n\t\t\t\tchanged = true;\n\t\t\t}\n\t\t\tif (changed) {\n\t\t\t\tprintf(\"Choosing a maxzoom of -z%d to keep most features distinct with cluster distance %d\\n\", maxzoom, cluster_distance);\n\t\t\t}\n\t\t}\n\n\t\tif (dist_count != 0) {\n\t\t\tdouble want2 = exp(dist_sum / dist_count) / 8;\n\t\t\tint mz = ceil(log(360 / (.00000274 * want2)) / log(2) - full_detail);\n\n\t\t\tif (mz < 0) {\n\t\t\t\tmz = 0;\n\t\t\t}\n\t\t\tif (mz > 32 - full_detail) {\n\t\t\t\tmz = 32 - full_detail;\n\t\t\t}\n\t\t\tif (mz > 33 - low_detail) {  // that is, mz - 1 > 32 - low_detail\n\t\t\t\tmz = 33 - low_detail;\n\t\t\t}\n\n\t\t\tif (mz > maxzoom || count <= 0) {\n\t\t\t\tif (!quiet) {\n\t\t\t\t\tfprintf(stderr, \"Choosing a maxzoom of -z%d for resolution of about %d feet (%d meters) within features\\n\", mz, (int) exp(dist_sum / dist_count), (int) (exp(dist_sum / dist_count) / 3.28084));\n\t\t\t\t}\n\t\t\t\tmaxzoom = mz;\n\t\t\t}\n\t\t}\n\n\t\tif (maxzoom < minzoom) {\n\t\t\tfprintf(stderr, \"Can't use %d for maxzoom because minzoom is %d\\n\", maxzoom, minzoom);\n\t\t\tmaxzoom = minzoom;\n\t\t}\n\n\t\tfix_dropping = true;\n\n\t\tif (basezoom == -1) {\n\t\t\tbasezoom = maxzoom;\n\t\t}\n\t}\n\n\tif (basezoom < 0 || droprate < 0) {\n\t\tstruct tile {\n\t\t\tunsigned x;\n\t\t\tunsigned y;\n\t\t\tlong long count;\n\t\t\tlong long fullcount;\n\t\t\tdouble gap;\n\t\t\tunsigned long long previndex;\n\t\t} tile[MAX_ZOOM + 1], max[MAX_ZOOM + 1];\n\n\t\t{\n\t\t\tint z;\n\t\t\tfor (z = 0; z <= MAX_ZOOM; z++) {\n\t\t\t\ttile[z].x = tile[z].y = tile[z].count = tile[z].fullcount = tile[z].gap = tile[z].previndex = 0;\n\t\t\t\tmax[z].x = max[z].y = max[z].count = max[z].fullcount = 0;\n\t\t\t}\n\t\t}\n\n\t\tlong long progress = -1;\n\n\t\tlong long ip;\n\t\tfor (ip = 0; ip < indices; ip++) {\n\t\t\tunsigned xx, yy;\n\t\t\tdecode_index(map[ip].ix, &xx, &yy);\n\n\t\t\tlong long nprogress = 100 * ip / indices;\n\t\t\tif (nprogress != progress) {\n\t\t\t\tprogress = nprogress;\n\t\t\t\tif (!quiet && !quiet_progress && progress_time()) {\n\t\t\t\t\tfprintf(stderr, \"Base zoom/drop rate: %lld%% \\r\", progress);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tint z;\n\t\t\tfor (z = 0; z <= MAX_ZOOM; z++) {\n\t\t\t\tunsigned xxx = 0, yyy = 0;\n\t\t\t\tif (z != 0) {\n\t\t\t\t\txxx = xx >> (32 - z);\n\t\t\t\t\tyyy = yy >> (32 - z);\n\t\t\t\t}\n\n\t\t\t\tdouble scale = (double) (1LL << (64 - 2 * (z + 8)));\n\n\t\t\t\tif (tile[z].x != xxx || tile[z].y != yyy) {\n\t\t\t\t\tif (tile[z].count > max[z].count) {\n\t\t\t\t\t\tmax[z] = tile[z];\n\t\t\t\t\t}\n\n\t\t\t\t\ttile[z].x = xxx;\n\t\t\t\t\ttile[z].y = yyy;\n\t\t\t\t\ttile[z].count = 0;\n\t\t\t\t\ttile[z].fullcount = 0;\n\t\t\t\t\ttile[z].gap = 0;\n\t\t\t\t\ttile[z].previndex = 0;\n\t\t\t\t}\n\n\t\t\t\ttile[z].fullcount++;\n\n\t\t\t\tif (manage_gap(map[ip].ix, &tile[z].previndex, scale, gamma, &tile[z].gap)) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\ttile[z].count++;\n\t\t\t}\n\t\t}\n\n\t\tint z;\n\t\tfor (z = MAX_ZOOM; z >= 0; z--) {\n\t\t\tif (tile[z].count > max[z].count) {\n\t\t\t\tmax[z] = tile[z];\n\t\t\t}\n\t\t}\n\n\t\tint max_features = 50000 / (basezoom_marker_width * basezoom_marker_width);\n\n\t\tint obasezoom = basezoom;\n\t\tif (basezoom < 0) {\n\t\t\tbasezoom = MAX_ZOOM;\n\n\t\t\tfor (z = MAX_ZOOM; z >= 0; z--) {\n\t\t\t\tif (max[z].count < max_features) {\n\t\t\t\t\tbasezoom = z;\n\t\t\t\t}\n\n\t\t\t\t// printf(\"%d/%u/%u %lld\\n\", z, max[z].x, max[z].y, max[z].count);\n\t\t\t}\n\n\t\t\tif (!quiet) {\n\t\t\t\tfprintf(stderr, \"Choosing a base zoom of -B%d to keep %lld features in tile %d/%u/%u.\\n\", basezoom, max[basezoom].count, basezoom, max[basezoom].x, max[basezoom].y);\n\t\t\t}\n\t\t}\n\n\t\tif (obasezoom < 0 && basezoom > maxzoom) {\n\t\t\tfprintf(stderr, \"Couldn't find a suitable base zoom. Working from the other direction.\\n\");\n\t\t\tif (gamma == 0) {\n\t\t\t\tfprintf(stderr, \"You might want to try -g1 to limit near-duplicates.\\n\");\n\t\t\t}\n\n\t\t\tif (droprate < 0) {\n\t\t\t\tif (maxzoom == 0) {\n\t\t\t\t\tdroprate = 2.5;\n\t\t\t\t} else {\n\t\t\t\t\tdroprate = exp(log((long double) max[0].count / max[maxzoom].count) / (maxzoom));\n\t\t\t\t\tif (!quiet) {\n\t\t\t\t\t\tfprintf(stderr, \"Choosing a drop rate of -r%f to get from %lld to %lld in %d zooms\\n\", droprate, max[maxzoom].count, max[0].count, maxzoom);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tbasezoom = 0;\n\t\t\tfor (z = 0; z <= maxzoom; z++) {\n\t\t\t\tdouble zoomdiff = log((long double) max[z].count / max_features) / log(droprate);\n\t\t\t\tif (zoomdiff + z > basezoom) {\n\t\t\t\t\tbasezoom = ceil(zoomdiff + z);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (!quiet) {\n\t\t\t\tfprintf(stderr, \"Choosing a base zoom of -B%d to keep %f features in tile %d/%u/%u.\\n\", basezoom, max[maxzoom].count * exp(log(droprate) * (maxzoom - basezoom)), maxzoom, max[maxzoom].x, max[maxzoom].y);\n\t\t\t}\n\t\t} else if (droprate < 0) {\n\t\t\tdroprate = 1;\n\n\t\t\tfor (z = basezoom - 1; z >= 0; z--) {\n\t\t\t\tdouble interval = exp(log(droprate) * (basezoom - z));\n\n\t\t\t\tif (max[z].count / interval >= max_features) {\n\t\t\t\t\tinterval = (long double) max[z].count / max_features;\n\t\t\t\t\tdroprate = exp(log(interval) / (basezoom - z));\n\t\t\t\t\tinterval = exp(log(droprate) * (basezoom - z));\n\n\t\t\t\t\tif (!quiet) {\n\t\t\t\t\t\tfprintf(stderr, \"Choosing a drop rate of -r%f to keep %f features in tile %d/%u/%u.\\n\", droprate, max[z].count / interval, z, max[z].x, max[z].y);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (gamma > 0) {\n\t\t\tint effective = 0;\n\n\t\t\tfor (z = 0; z < maxzoom; z++) {\n\t\t\t\tif (max[z].count < max[z].fullcount) {\n\t\t\t\t\teffective = z + 1;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (effective == 0) {\n\t\t\t\tif (!quiet) {\n\t\t\t\t\tfprintf(stderr, \"With gamma, effective base zoom is 0, so no effective drop rate\\n\");\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tdouble interval_0 = exp(log(droprate) * (basezoom - 0));\n\t\t\t\tdouble interval_eff = exp(log(droprate) * (basezoom - effective));\n\t\t\t\tif (effective > basezoom) {\n\t\t\t\t\tinterval_eff = 1;\n\t\t\t\t}\n\n\t\t\t\tdouble scaled_0 = max[0].count / interval_0;\n\t\t\t\tdouble scaled_eff = max[effective].count / interval_eff;\n\n\t\t\t\tdouble rate_at_0 = scaled_0 / max[0].fullcount;\n\t\t\t\tdouble rate_at_eff = scaled_eff / max[effective].fullcount;\n\n\t\t\t\tdouble eff_drop = exp(log(rate_at_eff / rate_at_0) / (effective - 0));\n\n\t\t\t\tif (!quiet) {\n\t\t\t\t\tfprintf(stderr, \"With gamma, effective base zoom of %d, effective drop rate of %f\\n\", effective, eff_drop);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tfix_dropping = true;\n\t}\n\n\tif (fix_dropping) {\n\t\t// Fix up the minzooms for features, now that we really know the base zoom\n\t\t// and drop rate.\n\n\t\tstruct stat geomst;\n\t\tif (fstat(geomfd, &geomst) != 0) {\n\t\t\tperror(\"stat sorted geom\\n\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tchar *geom = (char *) mmap(NULL, geomst.st_size, PROT_READ | PROT_WRITE, MAP_SHARED, geomfd, 0);\n\t\tif (geom == MAP_FAILED) {\n\t\t\tperror(\"mmap geom for fixup\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tmadvise(geom, indexpos, MADV_SEQUENTIAL);\n\t\tmadvise(geom, indexpos, MADV_WILLNEED);\n\n\t\tstruct drop_state ds[maxzoom + 1];\n\t\tprep_drop_states(ds, maxzoom, basezoom, droprate);\n\n\t\tfor (long long ip = 0; ip < indices; ip++) {\n\t\t\tif (ip > 0 && map[ip].start != map[ip - 1].end) {\n\t\t\t\tfprintf(stderr, \"Mismatched index at %lld: %lld vs %lld\\n\", ip, map[ip].start, map[ip].end);\n\t\t\t}\n\t\t\tint feature_minzoom = calc_feature_minzoom(&map[ip], ds, maxzoom, gamma);\n\t\t\tgeom[map[ip].end - 1] = feature_minzoom;\n\t\t}\n\n\t\tmunmap(geom, geomst.st_size);\n\t}\n\n\tmadvise(map, indexpos, MADV_DONTNEED);\n\tmunmap(map, indexpos);\n\n\tif (close(indexfd) != 0) {\n\t\tperror(\"close sorted index\");\n\t}\n\n\t/* Traverse and split the geometries for each zoom level */\n\n\tstruct stat geomst;\n\tif (fstat(geomfd, &geomst) != 0) {\n\t\tperror(\"stat sorted geom\\n\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tint fd[TEMP_FILES];\n\toff_t size[TEMP_FILES];\n\n\tfd[0] = geomfd;\n\tsize[0] = geomst.st_size;\n\n\tfor (size_t j = 1; j < TEMP_FILES; j++) {\n\t\tfd[j] = -1;\n\t\tsize[j] = 0;\n\t}\n\n\tstd::atomic<unsigned> midx(0);\n\tstd::atomic<unsigned> midy(0);\n\tint written = traverse_zooms(fd, size, meta, stringpool, &midx, &midy, maxzoom, minzoom, outdb, outdir, buffer, fname, tmpdir, gamma, full_detail, low_detail, min_detail, meta_off, pool_off, initial_x, initial_y, simplification, layermaps, prefilter, postfilter, attribute_accum, filter);\n\n\tif (maxzoom != written) {\n\t\tif (written > minzoom) {\n\t\t\tfprintf(stderr, \"\\n\\n\\n*** NOTE TILES ONLY COMPLETE THROUGH ZOOM %d ***\\n\\n\\n\", written);\n\t\t\tmaxzoom = written;\n\t\t\tret = 100;\n\t\t} else {\n\t\t\tfprintf(stderr, \"%s: No zoom levels were successfully written\\n\", *av);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\tmadvise(meta, metapos, MADV_DONTNEED);\n\tif (munmap(meta, metapos) != 0) {\n\t\tperror(\"munmap meta\");\n\t}\n\tif (close(metafd) < 0) {\n\t\tperror(\"close meta\");\n\t}\n\n\tif (poolpos > 0) {\n\t\tmadvise((void *) stringpool, poolpos, MADV_DONTNEED);\n\t\tif (munmap(stringpool, poolpos) != 0) {\n\t\t\tperror(\"munmap stringpool\");\n\t\t}\n\t}\n\tif (close(poolfd) < 0) {\n\t\tperror(\"close pool\");\n\t}\n\n\tdouble minlat = 0, minlon = 0, maxlat = 0, maxlon = 0, midlat = 0, midlon = 0;\n\n\ttile2lonlat(midx, midy, maxzoom, &minlon, &maxlat);\n\ttile2lonlat(midx + 1, midy + 1, maxzoom, &maxlon, &minlat);\n\n\tmidlat = (maxlat + minlat) / 2;\n\tmidlon = (maxlon + minlon) / 2;\n\n\ttile2lonlat(file_bbox[0], file_bbox[1], 32, &minlon, &maxlat);\n\ttile2lonlat(file_bbox[2], file_bbox[3], 32, &maxlon, &minlat);\n\n\tif (midlat < minlat) {\n\t\tmidlat = minlat;\n\t}\n\tif (midlat > maxlat) {\n\t\tmidlat = maxlat;\n\t}\n\tif (midlon < minlon) {\n\t\tmidlon = minlon;\n\t}\n\tif (midlon > maxlon) {\n\t\tmidlon = maxlon;\n\t}\n\n\tstd::map<std::string, layermap_entry> merged_lm = merge_layermaps(layermaps);\n\n\tfor (auto ai = merged_lm.begin(); ai != merged_lm.end(); ++ai) {\n\t\tai->second.minzoom = minzoom;\n\t\tai->second.maxzoom = maxzoom;\n\t}\n\n\tmbtiles_write_metadata(outdb, outdir, fname, minzoom, maxzoom, minlat, minlon, maxlat, maxlon, midlat, midlon, forcetable, attribution, merged_lm, true, description, !prevent[P_TILE_STATS], attribute_descriptions, \"tippecanoe\", commandline);\n\n\treturn ret;\n}\n\nstatic bool has_name(struct option *long_options, int *pl) {\n\tfor (size_t lo = 0; long_options[lo].name != NULL; lo++) {\n\t\tif (long_options[lo].flag == pl) {\n\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nvoid set_attribute_type(std::map<std::string, int> &attribute_types, const char *arg) {\n\tconst char *s = strchr(arg, ':');\n\tif (s == NULL) {\n\t\tfprintf(stderr, \"-T%s option must be in the form -Tname:type\\n\", arg);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tstd::string name = std::string(arg, s - arg);\n\tstd::string type = std::string(s + 1);\n\tint t = -1;\n\n\tif (type == \"int\") {\n\t\tt = mvt_int;\n\t} else if (type == \"float\") {\n\t\tt = mvt_float;\n\t} else if (type == \"string\") {\n\t\tt = mvt_string;\n\t} else if (type == \"bool\") {\n\t\tt = mvt_bool;\n\t} else {\n\t\tfprintf(stderr, \"Attribute type (%s) must be int, float, string, or bool\\n\", type.c_str());\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tattribute_types.insert(std::pair<std::string, int>(name, t));\n}\n\nvoid set_attribute_accum(std::map<std::string, attribute_op> &attribute_accum, const char *arg) {\n\tconst char *s = strchr(arg, ':');\n\tif (s == NULL) {\n\t\tfprintf(stderr, \"-E%s option must be in the form -Ename:method\\n\", arg);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tstd::string name = std::string(arg, s - arg);\n\tstd::string type = std::string(s + 1);\n\tattribute_op t;\n\n\tif (type == \"sum\") {\n\t\tt = op_sum;\n\t} else if (type == \"product\") {\n\t\tt = op_product;\n\t} else if (type == \"mean\") {\n\t\tt = op_mean;\n\t} else if (type == \"max\") {\n\t\tt = op_max;\n\t} else if (type == \"min\") {\n\t\tt = op_min;\n\t} else if (type == \"concat\") {\n\t\tt = op_concat;\n\t} else if (type == \"comma\") {\n\t\tt = op_comma;\n\t} else {\n\t\tfprintf(stderr, \"Attribute method (%s) must be sum, product, mean, max, min, concat, or comma\\n\", type.c_str());\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tattribute_accum.insert(std::pair<std::string, attribute_op>(name, t));\n}\n\nvoid parse_json_source(const char *arg, struct source &src) {\n\tjson_pull *jp = json_begin_string(arg);\n\tjson_object *o = json_read_tree(jp);\n\n\tif (o == NULL) {\n\t\tfprintf(stderr, \"%s: -L%s: %s\\n\", *av, arg, jp->error);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (o->type != JSON_HASH) {\n\t\tfprintf(stderr, \"%s: -L%s: not a JSON object\\n\", *av, arg);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tjson_object *fname = json_hash_get(o, \"file\");\n\tif (fname == NULL || fname->type != JSON_STRING) {\n\t\tfprintf(stderr, \"%s: -L%s: requires \\\"file\\\": filename\\n\", *av, arg);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tsrc.file = std::string(fname->string);\n\n\tjson_object *layer = json_hash_get(o, \"layer\");\n\tif (layer != NULL && layer->type == JSON_STRING) {\n\t\tsrc.layer = std::string(layer->string);\n\t}\n\n\tjson_object *description = json_hash_get(o, \"description\");\n\tif (description != NULL && description->type == JSON_STRING) {\n\t\tsrc.description = std::string(description->string);\n\t}\n\n\tjson_object *format = json_hash_get(o, \"format\");\n\tif (format != NULL && format->type == JSON_STRING) {\n\t\tsrc.format = std::string(format->string);\n\t}\n\n\tjson_free(o);\n\tjson_end(jp);\n}\n\nint main(int argc, char **argv) {\n#ifdef MTRACE\n\tmtrace();\n#endif\n\n\tav = argv;\n\tinit_cpus();\n\n\textern int optind;\n\textern char *optarg;\n\tint i;\n\n\tchar *name = NULL;\n\tchar *description = NULL;\n\tchar *layername = NULL;\n\tchar *out_mbtiles = NULL;\n\tchar *out_dir = NULL;\n\tsqlite3 *outdb = NULL;\n\tint maxzoom = 14;\n\tint minzoom = 0;\n\tint basezoom = -1;\n\tdouble basezoom_marker_width = 1;\n\tint force = 0;\n\tint forcetable = 0;\n\tdouble droprate = 2.5;\n\tdouble gamma = 0;\n\tint buffer = 5;\n\tconst char *tmpdir = \"/tmp\";\n\tconst char *attribution = NULL;\n\tstd::vector<source> sources;\n\tconst char *prefilter = NULL;\n\tconst char *postfilter = NULL;\n\tbool guess_maxzoom = false;\n\n\tstd::set<std::string> exclude, include;\n\tstd::map<std::string, int> attribute_types;\n\tstd::map<std::string, attribute_op> attribute_accum;\n\tstd::map<std::string, std::string> attribute_descriptions;\n\tint exclude_all = 0;\n\tint read_parallel = 0;\n\tint files_open_at_start;\n\tjson_object *filter = NULL;\n\n\tfor (i = 0; i < 256; i++) {\n\t\tprevent[i] = 0;\n\t\tadditional[i] = 0;\n\t}\n\n\tstatic struct option long_options_orig[] = {\n\t\t{\"Output tileset\", 0, 0, 0},\n\t\t{\"output\", required_argument, 0, 'o'},\n\t\t{\"output-to-directory\", required_argument, 0, 'e'},\n\t\t{\"force\", no_argument, 0, 'f'},\n\t\t{\"allow-existing\", no_argument, 0, 'F'},\n\n\t\t{\"Tileset description and attribution\", 0, 0, 0},\n\t\t{\"name\", required_argument, 0, 'n'},\n\t\t{\"attribution\", required_argument, 0, 'A'},\n\t\t{\"description\", required_argument, 0, 'N'},\n\n\t\t{\"Input files and layer names\", 0, 0, 0},\n\t\t{\"layer\", required_argument, 0, 'l'},\n\t\t{\"named-layer\", required_argument, 0, 'L'},\n\n\t\t{\"Parallel processing of input\", 0, 0, 0},\n\t\t{\"read-parallel\", no_argument, 0, 'P'},\n\n\t\t{\"Projection of input\", 0, 0, 0},\n\t\t{\"projection\", required_argument, 0, 's'},\n\n\t\t{\"Zoom levels\", 0, 0, 0},\n\t\t{\"maximum-zoom\", required_argument, 0, 'z'},\n\t\t{\"minimum-zoom\", required_argument, 0, 'Z'},\n\t\t{\"extend-zooms-if-still-dropping\", no_argument, &additional[A_EXTEND_ZOOMS], 1},\n\t\t{\"one-tile\", required_argument, 0, 'R'},\n\n\t\t{\"Tile resolution\", 0, 0, 0},\n\t\t{\"full-detail\", required_argument, 0, 'd'},\n\t\t{\"low-detail\", required_argument, 0, 'D'},\n\t\t{\"minimum-detail\", required_argument, 0, 'm'},\n\n\t\t{\"Filtering feature attributes\", 0, 0, 0},\n\t\t{\"exclude\", required_argument, 0, 'x'},\n\t\t{\"include\", required_argument, 0, 'y'},\n\t\t{\"exclude-all\", no_argument, 0, 'X'},\n\n\t\t{\"Modifying feature attributes\", 0, 0, 0},\n\t\t{\"attribute-type\", required_argument, 0, 'T'},\n\t\t{\"attribute-description\", required_argument, 0, 'Y'},\n\t\t{\"accumulate-attribute\", required_argument, 0, 'E'},\n\t\t{\"empty-csv-columns-are-null\", no_argument, &prevent[P_EMPTY_CSV_COLUMNS], 1},\n\t\t{\"convert-stringified-ids-to-numbers\", no_argument, &additional[A_CONVERT_NUMERIC_IDS], 1},\n\t\t{\"use-attribute-for-id\", required_argument, 0, '~'},\n\n\t\t{\"Filtering features by attributes\", 0, 0, 0},\n\t\t{\"feature-filter-file\", required_argument, 0, 'J'},\n\t\t{\"feature-filter\", required_argument, 0, 'j'},\n\n\t\t{\"Dropping a fixed fraction of features by zoom level\", 0, 0, 0},\n\t\t{\"drop-rate\", required_argument, 0, 'r'},\n\t\t{\"base-zoom\", required_argument, 0, 'B'},\n\t\t{\"drop-lines\", no_argument, &additional[A_LINE_DROP], 1},\n\t\t{\"drop-polygons\", no_argument, &additional[A_POLYGON_DROP], 1},\n\t\t{\"cluster-distance\", required_argument, 0, 'K'},\n\n\t\t{\"Dropping or merging a fraction of features to keep under tile size limits\", 0, 0, 0},\n\t\t{\"drop-densest-as-needed\", no_argument, &additional[A_DROP_DENSEST_AS_NEEDED], 1},\n\t\t{\"drop-fraction-as-needed\", no_argument, &additional[A_DROP_FRACTION_AS_NEEDED], 1},\n\t\t{\"drop-smallest-as-needed\", no_argument, &additional[A_DROP_SMALLEST_AS_NEEDED], 1},\n\t\t{\"coalesce-densest-as-needed\", no_argument, &additional[A_COALESCE_DENSEST_AS_NEEDED], 1},\n\t\t{\"coalesce-fraction-as-needed\", no_argument, &additional[A_COALESCE_FRACTION_AS_NEEDED], 1},\n\t\t{\"coalesce-smallest-as-needed\", no_argument, &additional[A_COALESCE_SMALLEST_AS_NEEDED], 1},\n\t\t{\"force-feature-limit\", no_argument, &prevent[P_DYNAMIC_DROP], 1},\n\t\t{\"cluster-densest-as-needed\", no_argument, &additional[A_CLUSTER_DENSEST_AS_NEEDED], 1},\n\n\t\t{\"Dropping tightly overlapping features\", 0, 0, 0},\n\t\t{\"gamma\", required_argument, 0, 'g'},\n\t\t{\"increase-gamma-as-needed\", no_argument, &additional[A_INCREASE_GAMMA_AS_NEEDED], 1},\n\n\t\t{\"Line and polygon simplification\", 0, 0, 0},\n\t\t{\"simplification\", required_argument, 0, 'S'},\n\t\t{\"no-line-simplification\", no_argument, &prevent[P_SIMPLIFY], 1},\n\t\t{\"simplify-only-low-zooms\", no_argument, &prevent[P_SIMPLIFY_LOW], 1},\n\t\t{\"no-tiny-polygon-reduction\", no_argument, &prevent[P_TINY_POLYGON_REDUCTION], 1},\n\t\t{\"no-simplification-of-shared-nodes\", no_argument, &prevent[P_SIMPLIFY_SHARED_NODES], 1},\n\n\t\t{\"Attempts to improve shared polygon boundaries\", 0, 0, 0},\n\t\t{\"detect-shared-borders\", no_argument, &additional[A_DETECT_SHARED_BORDERS], 1},\n\t\t{\"grid-low-zooms\", no_argument, &additional[A_GRID_LOW_ZOOMS], 1},\n\n\t\t{\"Controlling clipping to tile boundaries\", 0, 0, 0},\n\t\t{\"buffer\", required_argument, 0, 'b'},\n\t\t{\"no-clipping\", no_argument, &prevent[P_CLIPPING], 1},\n\t\t{\"no-duplication\", no_argument, &prevent[P_DUPLICATION], 1},\n\n\t\t{\"Reordering features within each tile\", 0, 0, 0},\n\t\t{\"preserve-input-order\", no_argument, &prevent[P_INPUT_ORDER], 1},\n\t\t{\"reorder\", no_argument, &additional[A_REORDER], 1},\n\t\t{\"coalesce\", no_argument, &additional[A_COALESCE], 1},\n\t\t{\"reverse\", no_argument, &additional[A_REVERSE], 1},\n\t\t{\"hilbert\", no_argument, &additional[A_HILBERT], 1},\n\n\t\t{\"Adding calculated attributes\", 0, 0, 0},\n\t\t{\"calculate-feature-density\", no_argument, &additional[A_CALCULATE_FEATURE_DENSITY], 1},\n\t\t{\"generate-ids\", no_argument, &additional[A_GENERATE_IDS], 1},\n\n\t\t{\"Trying to correct bad source geometry\", 0, 0, 0},\n\t\t{\"detect-longitude-wraparound\", no_argument, &additional[A_DETECT_WRAPAROUND], 1},\n\t\t{\"use-source-polygon-winding\", no_argument, &prevent[P_USE_SOURCE_POLYGON_WINDING], 1},\n\t\t{\"reverse-source-polygon-winding\", no_argument, &prevent[P_REVERSE_SOURCE_POLYGON_WINDING], 1},\n\t\t{\"clip-bounding-box\", required_argument, 0, '~'},\n\n\t\t{\"Filtering tile contents\", 0, 0, 0},\n\t\t{\"prefilter\", required_argument, 0, 'C'},\n\t\t{\"postfilter\", required_argument, 0, 'c'},\n\n\t\t{\"Setting or disabling tile size limits\", 0, 0, 0},\n\t\t{\"maximum-tile-bytes\", required_argument, 0, 'M'},\n\t\t{\"maximum-tile-features\", required_argument, 0, 'O'},\n\t\t{\"no-feature-limit\", no_argument, &prevent[P_FEATURE_LIMIT], 1},\n\t\t{\"no-tile-size-limit\", no_argument, &prevent[P_KILOBYTE_LIMIT], 1},\n\t\t{\"no-tile-compression\", no_argument, &prevent[P_TILE_COMPRESSION], 1},\n\t\t{\"no-tile-stats\", no_argument, &prevent[P_TILE_STATS], 1},\n\t\t{\"tile-stats-attributes-limit\", required_argument, 0, '~'},\n\t\t{\"tile-stats-sample-values-limit\", required_argument, 0, '~'},\n\t\t{\"tile-stats-values-limit\", required_argument, 0, '~'},\n\n\t\t{\"Temporary storage\", 0, 0, 0},\n\t\t{\"temporary-directory\", required_argument, 0, 't'},\n\n\t\t{\"Progress indicator\", 0, 0, 0},\n\t\t{\"quiet\", no_argument, 0, 'q'},\n\t\t{\"no-progress-indicator\", no_argument, 0, 'Q'},\n\t\t{\"progress-interval\", required_argument, 0, 'U'},\n\t\t{\"version\", no_argument, 0, 'v'},\n\n\t\t{\"\", 0, 0, 0},\n\t\t{\"prevent\", required_argument, 0, 'p'},\n\t\t{\"additional\", required_argument, 0, 'a'},\n\t\t{\"check-polygons\", no_argument, &additional[A_DEBUG_POLYGON], 1},\n\t\t{\"no-polygon-splitting\", no_argument, &prevent[P_POLYGON_SPLIT], 1},\n\t\t{\"prefer-radix-sort\", no_argument, &additional[A_PREFER_RADIX_SORT], 1},\n\t\t{\"help\", no_argument, 0, 'H'},\n\n\t\t{0, 0, 0, 0},\n\t};\n\n\tstatic struct option long_options[sizeof(long_options_orig) / sizeof(long_options_orig[0])];\n\tstatic char getopt_str[sizeof(long_options_orig) / sizeof(long_options_orig[0]) * 2 + 1];\n\n\t{\n\t\tsize_t out = 0;\n\t\tsize_t cout = 0;\n\t\tfor (size_t lo = 0; long_options_orig[lo].name != NULL; lo++) {\n\t\t\tif (long_options_orig[lo].val != 0) {\n\t\t\t\tlong_options[out++] = long_options_orig[lo];\n\n\t\t\t\tif (long_options_orig[lo].val > ' ') {\n\t\t\t\t\tgetopt_str[cout++] = long_options_orig[lo].val;\n\n\t\t\t\t\tif (long_options_orig[lo].has_arg == required_argument) {\n\t\t\t\t\t\tgetopt_str[cout++] = ':';\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tlong_options[out] = {0, 0, 0, 0};\n\t\tgetopt_str[cout] = '\\0';\n\n\t\tfor (size_t lo = 0; long_options[lo].name != NULL; lo++) {\n\t\t\tif (long_options[lo].flag != NULL) {\n\t\t\t\tif (*long_options[lo].flag != 0) {\n\t\t\t\t\tfprintf(stderr, \"Internal error: reused %s\\n\", long_options[lo].name);\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t\t*long_options[lo].flag = 1;\n\t\t\t}\n\t\t}\n\n\t\tfor (size_t lo = 0; long_options[lo].name != NULL; lo++) {\n\t\t\tif (long_options[lo].flag != NULL) {\n\t\t\t\t*long_options[lo].flag = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tstd::string commandline = format_commandline(argc, argv);\n\n\tint option_index = 0;\n\twhile ((i = getopt_long(argc, argv, getopt_str, long_options, &option_index)) != -1) {\n\t\tswitch (i) {\n\t\tcase 0:\n\t\t\tbreak;\n\n\t\tcase '~': {\n\t\t\tconst char *opt = long_options[option_index].name;\n\t\t\tif (strcmp(opt, \"tile-stats-attributes-limit\") == 0) {\n\t\t\t\tmax_tilestats_attributes = atoi(optarg);\n\t\t\t} else if (strcmp(opt, \"tile-stats-sample-values-limit\") == 0) {\n\t\t\t\tmax_tilestats_sample_values = atoi(optarg);\n\t\t\t} else if (strcmp(opt, \"tile-stats-values-limit\") == 0) {\n\t\t\t\tmax_tilestats_values = atoi(optarg);\n\t\t\t} else if (strcmp(opt, \"clip-bounding-box\") == 0) {\n\t\t\t\tclipbbox clip;\n\t\t\t\tif (sscanf(optarg, \"%lf,%lf,%lf,%lf\", &clip.lon1, &clip.lat1, &clip.lon2, &clip.lat2) == 4) {\n\t\t\t\t\tclipbboxes.push_back(clip);\n\t\t\t\t} else {\n\t\t\t\t\tfprintf(stderr, \"%s: Can't parse bounding box --%s=%s\\n\", argv[0], opt, optarg);\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t} else if (strcmp(opt, \"use-attribute-for-id\") == 0) {\n\t\t\t\tattribute_for_id = optarg;\n\t\t\t} else {\n\t\t\t\tfprintf(stderr, \"%s: Unrecognized option --%s\\n\", argv[0], opt);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tcase 'n':\n\t\t\tname = optarg;\n\t\t\tbreak;\n\n\t\tcase 'N':\n\t\t\tdescription = optarg;\n\t\t\tbreak;\n\n\t\tcase 'l':\n\t\t\tlayername = optarg;\n\t\t\tbreak;\n\n\t\tcase 'A':\n\t\t\tattribution = optarg;\n\t\t\tbreak;\n\n\t\tcase 'L': {\n\t\t\tstruct source src;\n\t\t\tif (optarg[0] == '{') {\n\t\t\t\tparse_json_source(optarg, src);\n\t\t\t} else {\n\t\t\t\tchar *cp = strchr(optarg, ':');\n\t\t\t\tif (cp == NULL || cp == optarg) {\n\t\t\t\t\tfprintf(stderr, \"%s: -L requires layername:file\\n\", argv[0]);\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t\tsrc.layer = std::string(optarg).substr(0, cp - optarg);\n\t\t\t\tsrc.file = std::string(cp + 1);\n\t\t\t}\n\t\t\tsources.push_back(src);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase 'z':\n\t\t\tif (strcmp(optarg, \"g\") == 0) {\n\t\t\t\tmaxzoom = MAX_ZOOM;\n\t\t\t\tguess_maxzoom = true;\n\t\t\t} else {\n\t\t\t\tmaxzoom = atoi_require(optarg, \"Maxzoom\");\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 'Z':\n\t\t\tminzoom = atoi_require(optarg, \"Minzoom\");\n\t\t\tbreak;\n\n\t\tcase 'R': {\n\t\t\tunsigned z, x, y;\n\t\t\tif (sscanf(optarg, \"%u/%u/%u\", &z, &x, &y) == 3) {\n\t\t\t\tminzoom = z;\n\t\t\t\tmaxzoom = z;\n\t\t\t\tjustx = x;\n\t\t\t\tjusty = y;\n\t\t\t} else {\n\t\t\t\tfprintf(stderr, \"--one-tile argument must be z/x/y\\n\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tcase 'B':\n\t\t\tif (strcmp(optarg, \"g\") == 0) {\n\t\t\t\tbasezoom = -2;\n\t\t\t} else if (optarg[0] == 'g' || optarg[0] == 'f') {\n\t\t\t\tbasezoom = -2;\n\t\t\t\tif (optarg[0] == 'g') {\n\t\t\t\t\tbasezoom_marker_width = atof_require(optarg + 1, \"Marker width\");\n\t\t\t\t} else {\n\t\t\t\t\tbasezoom_marker_width = sqrt(50000 / atof_require(optarg + 1, \"Marker width\"));\n\t\t\t\t}\n\t\t\t\tif (basezoom_marker_width == 0 || atof_require(optarg + 1, \"Marker width\") == 0) {\n\t\t\t\t\tfprintf(stderr, \"%s: Must specify value >0 with -B%c\\n\", argv[0], optarg[0]);\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tbasezoom = atoi_require(optarg, \"Basezoom\");\n\t\t\t\tif (basezoom == 0 && strcmp(optarg, \"0\") != 0) {\n\t\t\t\t\tfprintf(stderr, \"%s: Couldn't understand -B%s\\n\", argv[0], optarg);\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 'K':\n\t\t\tcluster_distance = atoi_require(optarg, \"Cluster distance\");\n\t\t\tif (cluster_distance > 255) {\n\t\t\t\tfprintf(stderr, \"%s: --cluster-distance %d is too big; limit is 255\\n\", argv[0], cluster_distance);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 'd':\n\t\t\tfull_detail = atoi_require(optarg, \"Full detail\");\n\t\t\tif (full_detail > 30) {\n\t\t\t\t// So the maximum geometry delta of just under 2 tile extents\n\t\t\t\t// is less than 2^31\n\n\t\t\t\tfprintf(stderr, \"%s: --full-detail can be at most 30\\n\", argv[0]);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 'D':\n\t\t\tlow_detail = atoi_require(optarg, \"Low detail\");\n\t\t\tif (low_detail > 30) {\n\t\t\t\tfprintf(stderr, \"%s: --low-detail can be at most 30\\n\", argv[0]);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 'm':\n\t\t\tmin_detail = atoi_require(optarg, \"Min detail\");\n\t\t\tbreak;\n\n\t\tcase 'o':\n\t\t\tif (out_mbtiles != NULL) {\n\t\t\t\tfprintf(stderr, \"%s: Can't specify both %s and %s as output\\n\", argv[0], out_mbtiles, optarg);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tif (out_dir != NULL) {\n\t\t\t\tfprintf(stderr, \"%s: Can't specify both %s and %s as output\\n\", argv[0], out_dir, optarg);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tout_mbtiles = optarg;\n\t\t\tbreak;\n\n\t\tcase 'e':\n\t\t\tif (out_mbtiles != NULL) {\n\t\t\t\tfprintf(stderr, \"%s: Can't specify both %s and %s as output\\n\", argv[0], out_mbtiles, optarg);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tif (out_dir != NULL) {\n\t\t\t\tfprintf(stderr, \"%s: Can't specify both %s and %s as output\\n\", argv[0], out_dir, optarg);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tout_dir = optarg;\n\t\t\tbreak;\n\n\t\tcase 'x':\n\t\t\texclude.insert(std::string(optarg));\n\t\t\tbreak;\n\n\t\tcase 'y':\n\t\t\texclude_all = 1;\n\t\t\tinclude.insert(std::string(optarg));\n\t\t\tbreak;\n\n\t\tcase 'X':\n\t\t\texclude_all = 1;\n\t\t\tbreak;\n\n\t\tcase 'Y': {\n\t\t\tchar *cp = strchr(optarg, ':');\n\t\t\tif (cp == NULL || cp == optarg) {\n\t\t\t\tfprintf(stderr, \"%s: -Y requires attribute:description\\n\", argv[0]);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tstd::string attrib = std::string(optarg).substr(0, cp - optarg);\n\t\t\tstd::string desc = std::string(cp + 1);\n\t\t\tattribute_descriptions.insert(std::pair<std::string, std::string>(attrib, desc));\n\t\t} break;\n\n\t\tcase 'J':\n\t\t\tfilter = read_filter(optarg);\n\t\t\tbreak;\n\n\t\tcase 'j':\n\t\t\tfilter = parse_filter(optarg);\n\t\t\tbreak;\n\n\t\tcase 'r':\n\t\t\tif (strcmp(optarg, \"g\") == 0) {\n\t\t\t\tdroprate = -2;\n\t\t\t} else if (optarg[0] == 'g' || optarg[0] == 'f') {\n\t\t\t\tdroprate = -2;\n\t\t\t\tif (optarg[0] == 'g') {\n\t\t\t\t\tbasezoom_marker_width = atof_require(optarg + 1, \"Marker width\");\n\t\t\t\t} else {\n\t\t\t\t\tbasezoom_marker_width = sqrt(50000 / atof_require(optarg + 1, \"Marker width\"));\n\t\t\t\t}\n\t\t\t\tif (basezoom_marker_width == 0 || atof_require(optarg + 1, \"Marker width\") == 0) {\n\t\t\t\t\tfprintf(stderr, \"%s: Must specify value >0 with -r%c\\n\", argv[0], optarg[0]);\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tdroprate = atof_require(optarg, \"Drop rate\");\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 'b':\n\t\t\tbuffer = atoi_require(optarg, \"Buffer\");\n\t\t\tif (buffer > 127) {\n\t\t\t\t// So the maximum geometry delta is under 2 tile extents,\n\t\t\t\t// from less than half a tile beyond one side to less than\n\t\t\t\t// half a tile beyond the other.\n\n\t\t\t\tfprintf(stderr, \"%s: --buffer can be at most 127\\n\", argv[0]);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 'f':\n\t\t\tforce = 1;\n\t\t\tbreak;\n\n\t\tcase 'F':\n\t\t\tforcetable = 1;\n\t\t\tbreak;\n\n\t\tcase 't':\n\t\t\ttmpdir = optarg;\n\t\t\tif (tmpdir[0] != '/') {\n\t\t\t\tfprintf(stderr, \"Warning: temp directory %s doesn't begin with /\\n\", tmpdir);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 'g':\n\t\t\tgamma = atof_require(optarg, \"Gamma\");\n\t\t\tbreak;\n\n\t\tcase 'q':\n\t\t\tquiet = 1;\n\t\t\tbreak;\n\n\t\tcase 'Q':\n\t\t\tquiet_progress = 1;\n\t\t\tbreak;\n\n\t\tcase 'U':\n\t\t\tprogress_interval = atof_require(optarg, \"Progress interval\");\n\t\t\tbreak;\n\n\t\tcase 'p': {\n\t\t\tchar *cp;\n\t\t\tfor (cp = optarg; *cp != '\\0'; cp++) {\n\t\t\t\tif (has_name(long_options, &prevent[*cp & 0xFF])) {\n\t\t\t\t\tprevent[*cp & 0xFF] = 1;\n\t\t\t\t} else {\n\t\t\t\t\tfprintf(stderr, \"%s: Unknown option -p%c\\n\", argv[0], *cp);\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tcase 'a': {\n\t\t\tchar *cp;\n\t\t\tfor (cp = optarg; *cp != '\\0'; cp++) {\n\t\t\t\tif (has_name(long_options, &additional[*cp & 0xFF])) {\n\t\t\t\t\tadditional[*cp & 0xFF] = 1;\n\t\t\t\t} else {\n\t\t\t\t\tfprintf(stderr, \"%s: Unknown option -a%c\\n\", argv[0], *cp);\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tcase 'v':\n\t\t\tfprintf(stderr, \"tippecanoe %s\\n\", VERSION);\n\t\t\texit(EXIT_SUCCESS);\n\n\t\tcase 'P':\n\t\t\tread_parallel = 1;\n\t\t\tbreak;\n\n\t\tcase 's':\n\t\t\tset_projection_or_exit(optarg);\n\t\t\tbreak;\n\n\t\tcase 'S':\n\t\t\tsimplification = atof_require(optarg, \"Simplification\");\n\t\t\tif (simplification <= 0) {\n\t\t\t\tfprintf(stderr, \"%s: --simplification must be > 0\\n\", argv[0]);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 'M':\n\t\t\tmax_tile_size = atoll_require(optarg, \"Max tile size\");\n\t\t\tbreak;\n\n\t\tcase 'O':\n\t\t\tmax_tile_features = atoll_require(optarg, \"Max tile features\");\n\t\t\tbreak;\n\n\t\tcase 'c':\n\t\t\tpostfilter = optarg;\n\t\t\tbreak;\n\n\t\tcase 'C':\n\t\t\tprefilter = optarg;\n\t\t\tbreak;\n\n\t\tcase 'T':\n\t\t\tset_attribute_type(attribute_types, optarg);\n\t\t\tbreak;\n\n\t\tcase 'E':\n\t\t\tset_attribute_accum(attribute_accum, optarg);\n\t\t\tbreak;\n\n\t\tdefault: {\n\t\t\tif (i != 'H' && i != '?') {\n\t\t\t\tfprintf(stderr, \"Unknown option -%c\\n\", i);\n\t\t\t}\n\t\t\tint width = 7 + strlen(argv[0]);\n\t\t\tfprintf(stderr, \"Usage: %s [options] [file.json ...]\", argv[0]);\n\t\t\tfor (size_t lo = 0; long_options_orig[lo].name != NULL && strlen(long_options_orig[lo].name) > 0; lo++) {\n\t\t\t\tif (long_options_orig[lo].val == 0) {\n\t\t\t\t\tfprintf(stderr, \"\\n  %s\\n        \", long_options_orig[lo].name);\n\t\t\t\t\twidth = 8;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (width + strlen(long_options_orig[lo].name) + 9 >= 80) {\n\t\t\t\t\tfprintf(stderr, \"\\n        \");\n\t\t\t\t\twidth = 8;\n\t\t\t\t}\n\t\t\t\twidth += strlen(long_options_orig[lo].name) + 9;\n\t\t\t\tif (strcmp(long_options_orig[lo].name, \"output\") == 0) {\n\t\t\t\t\tfprintf(stderr, \" --%s=output.mbtiles\", long_options_orig[lo].name);\n\t\t\t\t\twidth += 9;\n\t\t\t\t} else if (long_options_orig[lo].has_arg) {\n\t\t\t\t\tfprintf(stderr, \" [--%s=...]\", long_options_orig[lo].name);\n\t\t\t\t} else {\n\t\t\t\t\tfprintf(stderr, \" [--%s]\", long_options_orig[lo].name);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (width + 16 >= 80) {\n\t\t\t\tfprintf(stderr, \"\\n        \");\n\t\t\t\twidth = 8;\n\t\t\t}\n\t\t\tfprintf(stderr, \"\\n\");\n\t\t\tif (i == 'H') {\n\t\t\t\texit(EXIT_SUCCESS);\n\t\t\t} else {\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t}\n\t\t}\n\t}\n\n\tif (additional[A_HILBERT]) {\n\t\tencode_index = encode_hilbert;\n\t\tdecode_index = decode_hilbert;\n\t} else {\n\t\tencode_index = encode_quadkey;\n\t\tdecode_index = decode_quadkey;\n\t}\n\n\t// Wait until here to project the bounding box, so that the behavior is\n\t// the same no matter what order the projection and bounding box are\n\t// specified in\n\tfor (auto &c : clipbboxes) {\n\t\tprojection->project(c.lon1, c.lat1, 32, &c.minx, &c.maxy);\n\t\tprojection->project(c.lon2, c.lat2, 32, &c.maxx, &c.miny);\n\t}\n\n\tif (max_tilestats_sample_values < max_tilestats_values) {\n\t\tmax_tilestats_sample_values = max_tilestats_values;\n\t}\n\n\tsignal(SIGPIPE, SIG_IGN);\n\n\tfiles_open_at_start = open(\"/dev/null\", O_RDONLY | O_CLOEXEC);\n\tif (files_open_at_start < 0) {\n\t\tperror(\"open /dev/null\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tif (close(files_open_at_start) != 0) {\n\t\tperror(\"close\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (full_detail <= 0) {\n\t\tfull_detail = 12;\n\t}\n\n\tif (full_detail < min_detail) {\n\t\tmin_detail = full_detail;\n\t\tfprintf(stderr, \"%s: Reducing minimum detail to match full detail %d\\n\", argv[0], min_detail);\n\t}\n\n\tif (low_detail < min_detail) {\n\t\tmin_detail = low_detail;\n\t\tfprintf(stderr, \"%s: Reducing minimum detail to match low detail %d\\n\", argv[0], min_detail);\n\t}\n\n\t// Need two checks: one for geometry representation, the other for\n\t// index traversal when guessing base zoom and drop rate\n\tif (!guess_maxzoom) {\n\t\tif (maxzoom > 32 - full_detail) {\n\t\t\tmaxzoom = 32 - full_detail;\n\t\t\tfprintf(stderr, \"Highest supported zoom with detail %d is %d\\n\", full_detail, maxzoom);\n\t\t}\n\t\tif (maxzoom > 33 - low_detail) {  // that is, maxzoom - 1 > 32 - low_detail\n\t\t\tmaxzoom = 33 - low_detail;\n\t\t\tfprintf(stderr, \"Highest supported zoom with low detail %d is %d\\n\", low_detail, maxzoom);\n\t\t}\n\t}\n\tif (maxzoom > MAX_ZOOM) {\n\t\tmaxzoom = MAX_ZOOM;\n\t\tfprintf(stderr, \"Highest supported zoom is %d\\n\", maxzoom);\n\t}\n\n\tif (minzoom > maxzoom) {\n\t\tfprintf(stderr, \"%s: Minimum zoom -Z%d cannot be greater than maxzoom -z%d\\n\", argv[0], minzoom, maxzoom);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (basezoom == -1) {\n\t\tif (!guess_maxzoom) {\n\t\t\tbasezoom = maxzoom;\n\t\t}\n\t}\n\n\tgeometry_scale = 32 - (full_detail + maxzoom);\n\tif (geometry_scale < 0) {\n\t\tgeometry_scale = 0;\n\t\tif (!guess_maxzoom) {\n\t\t\tfprintf(stderr, \"Full detail + maxzoom > 32, so you are asking for more detail than is available.\\n\");\n\t\t}\n\t}\n\n\tif ((basezoom < 0 || droprate < 0) && (gamma < 0)) {\n\t\t// Can't use randomized (as opposed to evenly distributed) dot dropping\n\t\t// if rate and base aren't known during feature reading.\n\t\tgamma = 0;\n\t\tfprintf(stderr, \"Forcing -g0 since -B or -r is not known\\n\");\n\t}\n\n\tif (out_mbtiles == NULL && out_dir == NULL) {\n\t\tfprintf(stderr, \"%s: must specify -o out.mbtiles or -e directory\\n\", argv[0]);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (out_mbtiles != NULL && out_dir != NULL) {\n\t\tfprintf(stderr, \"%s: Options -o and -e cannot be used together\\n\", argv[0]);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (out_mbtiles != NULL) {\n\t\tif (force) {\n\t\t\tunlink(out_mbtiles);\n\t\t}\n\n\t\toutdb = mbtiles_open(out_mbtiles, argv, forcetable);\n\t}\n\tif (out_dir != NULL) {\n\t\tcheck_dir(out_dir, argv, force, forcetable);\n\t}\n\n\tint ret = EXIT_SUCCESS;\n\n\tfor (i = optind; i < argc; i++) {\n\t\tstruct source src;\n\t\tsrc.layer = \"\";\n\t\tsrc.file = std::string(argv[i]);\n\t\tsources.push_back(src);\n\t}\n\n\tif (sources.size() == 0) {\n\t\tstruct source src;\n\t\tsrc.layer = \"\";\n\t\tsrc.file = \"\";  // standard input\n\t\tsources.push_back(src);\n\t}\n\n\tif (layername != NULL) {\n\t\tfor (size_t a = 0; a < sources.size(); a++) {\n\t\t\tsources[a].layer = layername;\n\t\t}\n\t}\n\n\tlong long file_bbox[4] = {UINT_MAX, UINT_MAX, 0, 0};\n\n\tret = read_input(sources, name ? name : out_mbtiles ? out_mbtiles : out_dir, maxzoom, minzoom, basezoom, basezoom_marker_width, outdb, out_dir, &exclude, &include, exclude_all, filter, droprate, buffer, tmpdir, gamma, read_parallel, forcetable, attribution, gamma != 0, file_bbox, prefilter, postfilter, description, guess_maxzoom, &attribute_types, argv[0], &attribute_accum, attribute_descriptions, commandline);\n\n\tif (outdb != NULL) {\n\t\tmbtiles_close(outdb, argv[0]);\n\t}\n\n#ifdef MTRACE\n\tmuntrace();\n#endif\n\n\ti = open(\"/dev/null\", O_RDONLY | O_CLOEXEC);\n\t// i < files_open_at_start is not an error, because reading from a pipe closes stdin\n\tif (i > files_open_at_start) {\n\t\tfprintf(stderr, \"Internal error: did not close all files: %d\\n\", i);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (filter != NULL) {\n\t\tjson_free(filter);\n\t}\n\n\treturn ret;\n}\n\nint mkstemp_cloexec(char *name) {\n\tint fd = mkstemp(name);\n\tif (fd >= 0) {\n\t\tif (fcntl(fd, F_SETFD, FD_CLOEXEC) < 0) {\n\t\t\tperror(\"cloexec for temporary file\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\treturn fd;\n}\n\nFILE *fopen_oflag(const char *name, const char *mode, int oflag) {\n\tint fd = open(name, oflag);\n\tif (fd < 0) {\n\t\treturn NULL;\n\t}\n\treturn fdopen(fd, mode);\n}\n\nbool progress_time() {\n\tif (progress_interval == 0.0) {\n\t\treturn true;\n\t}\n\n\tstruct timeval tv;\n\tdouble now;\n\tif (gettimeofday(&tv, NULL) != 0) {\n\t\tfprintf(stderr, \"%s: Can't get the time of day: %s\\n\", *av, strerror(errno));\n\t\tnow = 0;\n\t} else {\n\t\tnow = tv.tv_sec + tv.tv_usec / 1000000.0;\n\t}\n\n\tif (now - last_progress >= progress_interval) {\n\t\tlast_progress = now;\n\t\treturn true;\n\t} else {\n\t\treturn false;\n\t}\n}\n"
        },
        {
          "name": "main.hpp",
          "type": "blob",
          "size": 1.037109375,
          "content": "#ifndef MAIN_HPP\n#define MAIN_HPP\n\n#include <stddef.h>\n#include <atomic>\n#include <string>\n\nstruct index {\n\tlong long start = 0;\n\tlong long end = 0;\n\tunsigned long long ix = 0;\n\tshort segment = 0;\n\tunsigned short t : 2;\n\tunsigned long long seq : (64 - 18);  // pack with segment and t to stay in 32 bytes\n\n\tindex()\n\t    : t(0),\n\t      seq(0) {\n\t}\n};\n\nstruct clipbbox {\n\tdouble lon1;\n\tdouble lat1;\n\tdouble lon2;\n\tdouble lat2;\n\n\tlong long minx;\n\tlong long miny;\n\tlong long maxx;\n\tlong long maxy;\n};\n\nextern std::vector<clipbbox> clipbboxes;\n\nvoid checkdisk(std::vector<struct reader> *r);\n\nextern int geometry_scale;\nextern int quiet;\nextern int quiet_progress;\nextern double progress_interval;\nextern std::atomic<double> last_progress;\n\nextern size_t CPUS;\nextern size_t TEMP_FILES;\n\nextern size_t max_tile_size;\nextern size_t max_tile_features;\nextern int cluster_distance;\nextern std::string attribute_for_id;\n\nint mkstemp_cloexec(char *name);\nFILE *fopen_oflag(const char *name, const char *mode, int oflag);\nbool progress_time();\n\n#define MAX_ZOOM 24\n\n#endif\n"
        },
        {
          "name": "man",
          "type": "tree",
          "content": null
        },
        {
          "name": "mapbox",
          "type": "tree",
          "content": null
        },
        {
          "name": "mbtiles.cpp",
          "type": "blob",
          "size": 19.126953125,
          "content": "// for vasprintf() on Linux\n#ifndef _GNU_SOURCE\n#define _GNU_SOURCE\n#endif\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sqlite3.h>\n#include <vector>\n#include <string>\n#include <set>\n#include <map>\n#include <sys/stat.h>\n#include \"mvt.hpp\"\n#include \"mbtiles.hpp\"\n#include \"text.hpp\"\n#include \"milo/dtoa_milo.h\"\n#include \"write_json.hpp\"\n#include \"version.hpp\"\n\nsize_t max_tilestats_attributes = 1000;\nsize_t max_tilestats_sample_values = 1000;\nsize_t max_tilestats_values = 100;\n\nsqlite3 *mbtiles_open(char *dbname, char **argv, int forcetable) {\n\tsqlite3 *outdb;\n\n\tif (sqlite3_open(dbname, &outdb) != SQLITE_OK) {\n\t\tfprintf(stderr, \"%s: %s: %s\\n\", argv[0], dbname, sqlite3_errmsg(outdb));\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tchar *err = NULL;\n\tif (sqlite3_exec(outdb, \"PRAGMA synchronous=0\", NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"%s: async: %s\\n\", argv[0], err);\n\t\texit(EXIT_FAILURE);\n\t}\n\tif (sqlite3_exec(outdb, \"PRAGMA locking_mode=EXCLUSIVE\", NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"%s: async: %s\\n\", argv[0], err);\n\t\texit(EXIT_FAILURE);\n\t}\n\tif (sqlite3_exec(outdb, \"PRAGMA journal_mode=DELETE\", NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"%s: async: %s\\n\", argv[0], err);\n\t\texit(EXIT_FAILURE);\n\t}\n\tif (sqlite3_exec(outdb, \"CREATE TABLE metadata (name text, value text);\", NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"%s: Tileset \\\"%s\\\" already exists. You can use --force if you want to delete the old tileset.\\n\", argv[0], dbname);\n\t\tfprintf(stderr, \"%s: %s\\n\", argv[0], err);\n\t\tif (!forcetable) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\tif (sqlite3_exec(outdb, \"CREATE TABLE tiles (zoom_level integer, tile_column integer, tile_row integer, tile_data blob);\", NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"%s: create tiles table: %s\\n\", argv[0], err);\n\t\tif (!forcetable) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\tif (sqlite3_exec(outdb, \"create unique index name on metadata (name);\", NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"%s: index metadata: %s\\n\", argv[0], err);\n\t\tif (!forcetable) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\tif (sqlite3_exec(outdb, \"create unique index tile_index on tiles (zoom_level, tile_column, tile_row);\", NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"%s: index tiles: %s\\n\", argv[0], err);\n\t\tif (!forcetable) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\treturn outdb;\n}\n\nvoid mbtiles_write_tile(sqlite3 *outdb, int z, int tx, int ty, const char *data, int size) {\n\tsqlite3_stmt *stmt;\n\tconst char *query = \"insert into tiles (zoom_level, tile_column, tile_row, tile_data) values (?, ?, ?, ?)\";\n\tif (sqlite3_prepare_v2(outdb, query, -1, &stmt, NULL) != SQLITE_OK) {\n\t\tfprintf(stderr, \"sqlite3 insert prep failed\\n\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tsqlite3_bind_int(stmt, 1, z);\n\tsqlite3_bind_int(stmt, 2, tx);\n\tsqlite3_bind_int(stmt, 3, (1 << z) - 1 - ty);\n\tsqlite3_bind_blob(stmt, 4, data, size, NULL);\n\n\tif (sqlite3_step(stmt) != SQLITE_DONE) {\n\t\tfprintf(stderr, \"sqlite3 insert failed: %s\\n\", sqlite3_errmsg(outdb));\n\t}\n\tif (sqlite3_finalize(stmt) != SQLITE_OK) {\n\t\tfprintf(stderr, \"sqlite3 finalize failed: %s\\n\", sqlite3_errmsg(outdb));\n\t}\n}\n\nbool type_and_string::operator<(const type_and_string &o) const {\n\tif (string < o.string) {\n\t\treturn true;\n\t}\n\tif (string == o.string && type < o.type) {\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nbool type_and_string::operator!=(const type_and_string &o) const {\n\tif (type != o.type) {\n\t\treturn true;\n\t}\n\tif (string != o.string) {\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nvoid tilestats(std::map<std::string, layermap_entry> const &layermap1, size_t elements, json_writer &state) {\n\t// Consolidate layers/attributes whose names are truncated\n\tstd::vector<std::map<std::string, layermap_entry>> lmv;\n\tlmv.push_back(layermap1);\n\tstd::map<std::string, layermap_entry> layermap = merge_layermaps(lmv, true);\n\n\tstate.json_write_hash();\n\n\tstate.nospace = true;\n\tstate.json_write_string(\"layerCount\");\n\tstate.json_write_unsigned(layermap.size());\n\n\tstate.nospace = true;\n\tstate.json_write_string(\"layers\");\n\tstate.json_write_array();\n\n\tfor (auto layer : layermap) {\n\t\tstate.nospace = true;\n\t\tstate.json_write_hash();\n\n\t\tstate.nospace = true;\n\t\tstate.json_write_string(\"layer\");\n\t\tstate.json_write_string(layer.first);\n\n\t\tstate.nospace = true;\n\t\tstate.json_write_string(\"count\");\n\t\tstate.json_write_unsigned(layer.second.points + layer.second.lines + layer.second.polygons);\n\n\t\tstd::string geomtype = \"Polygon\";\n\t\tif (layer.second.points >= layer.second.lines && layer.second.points >= layer.second.polygons) {\n\t\t\tgeomtype = \"Point\";\n\t\t} else if (layer.second.lines >= layer.second.polygons && layer.second.lines >= layer.second.points) {\n\t\t\tgeomtype = \"LineString\";\n\t\t}\n\n\t\tstate.nospace = true;\n\t\tstate.json_write_string(\"geometry\");\n\t\tstate.json_write_string(geomtype);\n\n\t\tsize_t attrib_count = layer.second.file_keys.size();\n\t\tif (attrib_count > max_tilestats_attributes) {\n\t\t\tattrib_count = max_tilestats_attributes;\n\t\t}\n\n\t\tstate.nospace = true;\n\t\tstate.json_write_string(\"attributeCount\");\n\t\tstate.json_write_unsigned(attrib_count);\n\n\t\tstate.nospace = true;\n\t\tstate.json_write_string(\"attributes\");\n\t\tstate.nospace = true;\n\t\tstate.json_write_array();\n\n\t\tsize_t attrs = 0;\n\t\tfor (auto attribute : layer.second.file_keys) {\n\t\t\tif (attrs == elements) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tattrs++;\n\n\t\t\tstate.nospace = true;\n\t\t\tstate.json_write_hash();\n\n\t\t\tstate.nospace = true;\n\t\t\tstate.json_write_string(\"attribute\");\n\t\t\tstate.json_write_string(attribute.first);\n\n\t\t\tsize_t val_count = attribute.second.sample_values.size();\n\t\t\tif (val_count > max_tilestats_sample_values) {\n\t\t\t\tval_count = max_tilestats_sample_values;\n\t\t\t}\n\n\t\t\tstate.nospace = true;\n\t\t\tstate.json_write_string(\"count\");\n\t\t\tstate.json_write_unsigned(val_count);\n\n\t\t\tint type = 0;\n\t\t\tfor (auto s : attribute.second.sample_values) {\n\t\t\t\ttype |= (1 << s.type);\n\t\t\t}\n\n\t\t\tstd::string type_str;\n\t\t\t// No \"null\" because null attributes are dropped\n\t\t\tif (type == (1 << mvt_double)) {\n\t\t\t\ttype_str = \"number\";\n\t\t\t} else if (type == (1 << mvt_bool)) {\n\t\t\t\ttype_str = \"boolean\";\n\t\t\t} else if (type == (1 << mvt_string)) {\n\t\t\t\ttype_str = \"string\";\n\t\t\t} else {\n\t\t\t\ttype_str = \"mixed\";\n\t\t\t}\n\n\t\t\tstate.nospace = true;\n\t\t\tstate.json_write_string(\"type\");\n\t\t\tstate.json_write_string(type_str);\n\n\t\t\tstate.nospace = true;\n\t\t\tstate.json_write_string(\"values\");\n\t\t\tstate.json_write_array();\n\n\t\t\tsize_t vals = 0;\n\t\t\tfor (auto value : attribute.second.sample_values) {\n\t\t\t\tif (vals == elements) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tstate.nospace = true;\n\n\t\t\t\tif (value.type == mvt_double || value.type == mvt_bool) {\n\t\t\t\t\tvals++;\n\n\t\t\t\t\tstate.json_write_stringified(value.string);\n\t\t\t\t} else {\n\t\t\t\t\tstd::string trunc = truncate16(value.string, 256);\n\n\t\t\t\t\tif (trunc.size() == value.string.size()) {\n\t\t\t\t\t\tvals++;\n\n\t\t\t\t\t\tstate.json_write_string(value.string);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tstate.nospace = true;\n\t\t\tstate.json_end_array();\n\n\t\t\tif ((type & (1 << mvt_double)) != 0) {\n\t\t\t\tstate.nospace = true;\n\t\t\t\tstate.json_write_string(\"min\");\n\t\t\t\tstate.json_write_number(attribute.second.min);\n\n\t\t\t\tstate.nospace = true;\n\t\t\t\tstate.json_write_string(\"max\");\n\t\t\t\tstate.json_write_number(attribute.second.max);\n\t\t\t}\n\n\t\t\tstate.nospace = true;\n\t\t\tstate.json_end_hash();\n\t\t}\n\n\t\tstate.nospace = true;\n\t\tstate.json_end_array();\n\t\tstate.nospace = true;\n\t\tstate.json_end_hash();\n\t}\n\n\tstate.nospace = true;\n\tstate.json_end_array();\n\tstate.nospace = true;\n\tstate.json_end_hash();\n}\n\nvoid mbtiles_write_metadata(sqlite3 *outdb, const char *outdir, const char *fname, int minzoom, int maxzoom, double minlat, double minlon, double maxlat, double maxlon, double midlat, double midlon, int forcetable, const char *attribution, std::map<std::string, layermap_entry> const &layermap, bool vector, const char *description, bool do_tilestats, std::map<std::string, std::string> const &attribute_descriptions, std::string const &program, std::string const &commandline) {\n\tchar *sql, *err;\n\n\tsqlite3 *db = outdb;\n\tif (outdb == NULL) {\n\t\tif (sqlite3_open(\"\", &db) != SQLITE_OK) {\n\t\t\tfprintf(stderr, \"Temporary db: %s\\n\", sqlite3_errmsg(db));\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (sqlite3_exec(db, \"CREATE TABLE metadata (name text, value text);\", NULL, NULL, &err) != SQLITE_OK) {\n\t\t\tfprintf(stderr, \"Create metadata table: %s\\n\", err);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\tsql = sqlite3_mprintf(\"INSERT INTO metadata (name, value) VALUES ('name', %Q);\", fname);\n\tif (sqlite3_exec(db, sql, NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"set name in metadata: %s\\n\", err);\n\t\tif (!forcetable) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\tsqlite3_free(sql);\n\n\tsql = sqlite3_mprintf(\"INSERT INTO metadata (name, value) VALUES ('description', %Q);\", description != NULL ? description : fname);\n\tif (sqlite3_exec(db, sql, NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"set description in metadata: %s\\n\", err);\n\t\tif (!forcetable) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\tsqlite3_free(sql);\n\n\tsql = sqlite3_mprintf(\"INSERT INTO metadata (name, value) VALUES ('version', %d);\", 2);\n\tif (sqlite3_exec(db, sql, NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"set version : %s\\n\", err);\n\t\tif (!forcetable) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\tsqlite3_free(sql);\n\n\tsql = sqlite3_mprintf(\"INSERT INTO metadata (name, value) VALUES ('minzoom', %d);\", minzoom);\n\tif (sqlite3_exec(db, sql, NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"set minzoom: %s\\n\", err);\n\t\tif (!forcetable) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\tsqlite3_free(sql);\n\n\tsql = sqlite3_mprintf(\"INSERT INTO metadata (name, value) VALUES ('maxzoom', %d);\", maxzoom);\n\tif (sqlite3_exec(db, sql, NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"set maxzoom: %s\\n\", err);\n\t\tif (!forcetable) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\tsqlite3_free(sql);\n\n\tsql = sqlite3_mprintf(\"INSERT INTO metadata (name, value) VALUES ('center', '%f,%f,%d');\", midlon, midlat, maxzoom);\n\tif (sqlite3_exec(db, sql, NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"set center: %s\\n\", err);\n\t\tif (!forcetable) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\tsqlite3_free(sql);\n\n\tsql = sqlite3_mprintf(\"INSERT INTO metadata (name, value) VALUES ('bounds', '%f,%f,%f,%f');\", minlon, minlat, maxlon, maxlat);\n\tif (sqlite3_exec(db, sql, NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"set bounds: %s\\n\", err);\n\t\tif (!forcetable) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\tsqlite3_free(sql);\n\n\tsql = sqlite3_mprintf(\"INSERT INTO metadata (name, value) VALUES ('type', %Q);\", \"overlay\");\n\tif (sqlite3_exec(db, sql, NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"set type: %s\\n\", err);\n\t\tif (!forcetable) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\tsqlite3_free(sql);\n\n\tif (attribution != NULL) {\n\t\tsql = sqlite3_mprintf(\"INSERT INTO metadata (name, value) VALUES ('attribution', %Q);\", attribution);\n\t\tif (sqlite3_exec(db, sql, NULL, NULL, &err) != SQLITE_OK) {\n\t\t\tfprintf(stderr, \"set type: %s\\n\", err);\n\t\t\tif (!forcetable) {\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t}\n\t\tsqlite3_free(sql);\n\t}\n\n\tsql = sqlite3_mprintf(\"INSERT INTO metadata (name, value) VALUES ('format', %Q);\", vector ? \"pbf\" : \"png\");\n\tif (sqlite3_exec(db, sql, NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"set format: %s\\n\", err);\n\t\tif (!forcetable) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\tsqlite3_free(sql);\n\n\tstd::string version = program + \" \" + VERSION;\n\tsql = sqlite3_mprintf(\"INSERT INTO metadata (name, value) VALUES ('generator', %Q);\", version.c_str());\n\tif (sqlite3_exec(db, sql, NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"set version: %s\\n\", err);\n\t\tif (!forcetable) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\tsqlite3_free(sql);\n\n\tsql = sqlite3_mprintf(\"INSERT INTO metadata (name, value) VALUES ('generator_options', %Q);\", commandline.c_str());\n\tif (sqlite3_exec(db, sql, NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"set commandline: %s\\n\", err);\n\t\tif (!forcetable) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\tsqlite3_free(sql);\n\n\tif (vector) {\n\t\tsize_t elements = max_tilestats_values;\n\t\tstd::string buf;\n\n\t\t{\n\t\t\tjson_writer state(&buf);\n\n\t\t\tstate.json_write_hash();\n\t\t\tstate.nospace = true;\n\n\t\t\tstate.json_write_string(\"vector_layers\");\n\t\t\tstate.json_write_array();\n\n\t\t\tstd::vector<std::string> lnames;\n\t\t\tfor (auto ai = layermap.begin(); ai != layermap.end(); ++ai) {\n\t\t\t\tlnames.push_back(ai->first);\n\t\t\t}\n\n\t\t\tfor (size_t i = 0; i < lnames.size(); i++) {\n\t\t\t\tauto fk = layermap.find(lnames[i]);\n\t\t\t\tstate.json_write_hash();\n\n\t\t\t\tstate.json_write_string(\"id\");\n\t\t\t\tstate.json_write_string(lnames[i]);\n\n\t\t\t\tstate.json_write_string(\"description\");\n\t\t\t\tstate.json_write_string(fk->second.description);\n\n\t\t\t\tstate.json_write_string(\"minzoom\");\n\t\t\t\tstate.json_write_signed(fk->second.minzoom);\n\n\t\t\t\tstate.json_write_string(\"maxzoom\");\n\t\t\t\tstate.json_write_signed(fk->second.maxzoom);\n\n\t\t\t\tstate.json_write_string(\"fields\");\n\t\t\t\tstate.json_write_hash();\n\t\t\t\tstate.nospace = true;\n\n\t\t\t\tbool first = true;\n\t\t\t\tfor (auto j = fk->second.file_keys.begin(); j != fk->second.file_keys.end(); ++j) {\n\t\t\t\t\tif (first) {\n\t\t\t\t\t\tfirst = false;\n\t\t\t\t\t}\n\n\t\t\t\t\tstate.json_write_string(j->first);\n\n\t\t\t\t\tauto f = attribute_descriptions.find(j->first);\n\t\t\t\t\tif (f == attribute_descriptions.end()) {\n\t\t\t\t\t\tint type = 0;\n\t\t\t\t\t\tfor (auto s : j->second.sample_values) {\n\t\t\t\t\t\t\ttype |= (1 << s.type);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (type == (1 << mvt_double)) {\n\t\t\t\t\t\t\tstate.json_write_string(\"Number\");\n\t\t\t\t\t\t} else if (type == (1 << mvt_bool)) {\n\t\t\t\t\t\t\tstate.json_write_string(\"Boolean\");\n\t\t\t\t\t\t} else if (type == (1 << mvt_string)) {\n\t\t\t\t\t\t\tstate.json_write_string(\"String\");\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tstate.json_write_string(\"Mixed\");\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstate.json_write_string(f->second);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tstate.nospace = true;\n\t\t\t\tstate.json_end_hash();\n\t\t\t\tstate.json_end_hash();\n\t\t\t}\n\n\t\t\tstate.json_end_array();\n\n\t\t\tif (do_tilestats && elements > 0) {\n\t\t\t\tstate.nospace = true;\n\t\t\t\tstate.json_write_string(\"tilestats\");\n\t\t\t\ttilestats(layermap, elements, state);\n\t\t\t}\n\n\t\t\tstate.nospace = true;\n\t\t\tstate.json_end_hash();\n\t\t}\n\n\t\tsql = sqlite3_mprintf(\"INSERT INTO metadata (name, value) VALUES ('json', %Q);\", buf.c_str());\n\t\tif (sqlite3_exec(db, sql, NULL, NULL, &err) != SQLITE_OK) {\n\t\t\tfprintf(stderr, \"set json: %s\\n\", err);\n\t\t\tif (!forcetable) {\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t}\n\t\tsqlite3_free(sql);\n\t}\n\n\tif (outdir != NULL) {\n\t\tstd::string metadata = std::string(outdir) + \"/metadata.json\";\n\n\t\tstruct stat st;\n\t\tif (stat(metadata.c_str(), &st) == 0) {\n\t\t\t// Leave existing metadata in place with --allow-existing\n\t\t} else {\n\t\t\tFILE *fp = fopen(metadata.c_str(), \"w\");\n\t\t\tif (fp == NULL) {\n\t\t\t\tperror(metadata.c_str());\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tjson_writer state(fp);\n\n\t\t\tstate.json_write_hash();\n\t\t\tstate.json_write_newline();\n\n\t\t\tsqlite3_stmt *stmt;\n\t\t\tif (sqlite3_prepare_v2(db, \"SELECT name, value from metadata;\", -1, &stmt, NULL) == SQLITE_OK) {\n\t\t\t\twhile (sqlite3_step(stmt) == SQLITE_ROW) {\n\t\t\t\t\tstd::string key, value;\n\n\t\t\t\t\tconst char *k = (const char *) sqlite3_column_text(stmt, 0);\n\t\t\t\t\tconst char *v = (const char *) sqlite3_column_text(stmt, 1);\n\t\t\t\t\tif (k == NULL || v == NULL) {\n\t\t\t\t\t\tfprintf(stderr, \"Corrupt mbtiles file: null metadata\\n\");\n\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t}\n\n\t\t\t\t\tstate.json_comma_newline();\n\t\t\t\t\tstate.json_write_string(k);\n\t\t\t\t\tstate.json_write_string(v);\n\t\t\t\t}\n\t\t\t\tsqlite3_finalize(stmt);\n\t\t\t}\n\n\t\t\tstate.json_write_newline();\n\t\t\tstate.json_end_hash();\n\t\t\tstate.json_write_newline();\n\t\t\tfclose(fp);\n\t\t}\n\t}\n\n\tif (outdb == NULL) {\n\t\tif (sqlite3_close(db) != SQLITE_OK) {\n\t\t\tfprintf(stderr, \"Could not close temp database: %s\\n\", sqlite3_errmsg(db));\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n}\n\nvoid mbtiles_close(sqlite3 *outdb, const char *pgm) {\n\tchar *err;\n\n\tif (sqlite3_exec(outdb, \"ANALYZE;\", NULL, NULL, &err) != SQLITE_OK) {\n\t\tfprintf(stderr, \"%s: ANALYZE failed: %s\\n\", pgm, err);\n\t\texit(EXIT_FAILURE);\n\t}\n\tif (sqlite3_close(outdb) != SQLITE_OK) {\n\t\tfprintf(stderr, \"%s: could not close database: %s\\n\", pgm, sqlite3_errmsg(outdb));\n\t\texit(EXIT_FAILURE);\n\t}\n}\n\nstd::map<std::string, layermap_entry> merge_layermaps(std::vector<std::map<std::string, layermap_entry>> const &maps) {\n\treturn merge_layermaps(maps, false);\n}\n\nstd::map<std::string, layermap_entry> merge_layermaps(std::vector<std::map<std::string, layermap_entry>> const &maps, bool trunc) {\n\tstd::map<std::string, layermap_entry> out;\n\n\tfor (size_t i = 0; i < maps.size(); i++) {\n\t\tfor (auto map = maps[i].begin(); map != maps[i].end(); ++map) {\n\t\t\tif (map->second.points + map->second.lines + map->second.polygons + map->second.retain == 0) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tstd::string layername = map->first;\n\t\t\tif (trunc) {\n\t\t\t\tlayername = truncate16(layername, 256);\n\t\t\t}\n\n\t\t\tif (out.count(layername) == 0) {\n\t\t\t\tout.insert(std::pair<std::string, layermap_entry>(layername, layermap_entry(out.size())));\n\t\t\t\tauto out_entry = out.find(layername);\n\t\t\t\tout_entry->second.minzoom = map->second.minzoom;\n\t\t\t\tout_entry->second.maxzoom = map->second.maxzoom;\n\t\t\t\tout_entry->second.description = map->second.description;\n\t\t\t}\n\n\t\t\tauto out_entry = out.find(layername);\n\t\t\tif (out_entry == out.end()) {\n\t\t\t\tfprintf(stderr, \"Internal error merging layers\\n\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tfor (auto fk = map->second.file_keys.begin(); fk != map->second.file_keys.end(); ++fk) {\n\t\t\t\tstd::string attribname = fk->first;\n\t\t\t\tif (trunc) {\n\t\t\t\t\tattribname = truncate16(attribname, 256);\n\t\t\t\t}\n\n\t\t\t\tauto fk2 = out_entry->second.file_keys.find(attribname);\n\n\t\t\t\tif (fk2 == out_entry->second.file_keys.end()) {\n\t\t\t\t\tout_entry->second.file_keys.insert(std::pair<std::string, type_and_string_stats>(attribname, fk->second));\n\t\t\t\t} else {\n\t\t\t\t\tfor (auto val : fk->second.sample_values) {\n\t\t\t\t\t\tauto pt = std::lower_bound(fk2->second.sample_values.begin(), fk2->second.sample_values.end(), val);\n\t\t\t\t\t\tif (pt == fk2->second.sample_values.end() || *pt != val) {  // not found\n\t\t\t\t\t\t\tfk2->second.sample_values.insert(pt, val);\n\n\t\t\t\t\t\t\tif (fk2->second.sample_values.size() > max_tilestats_sample_values) {\n\t\t\t\t\t\t\t\tfk2->second.sample_values.pop_back();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tfk2->second.type |= fk->second.type;\n\n\t\t\t\t\tif (fk->second.min < fk2->second.min) {\n\t\t\t\t\t\tfk2->second.min = fk->second.min;\n\t\t\t\t\t}\n\t\t\t\t\tif (fk->second.max > fk2->second.max) {\n\t\t\t\t\t\tfk2->second.max = fk->second.max;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (map->second.minzoom < out_entry->second.minzoom) {\n\t\t\t\tout_entry->second.minzoom = map->second.minzoom;\n\t\t\t}\n\t\t\tif (map->second.maxzoom > out_entry->second.maxzoom) {\n\t\t\t\tout_entry->second.maxzoom = map->second.maxzoom;\n\t\t\t}\n\n\t\t\tout_entry->second.points += map->second.points;\n\t\t\tout_entry->second.lines += map->second.lines;\n\t\t\tout_entry->second.polygons += map->second.polygons;\n\t\t}\n\t}\n\n\treturn out;\n}\n\nvoid add_to_file_keys(std::map<std::string, type_and_string_stats> &file_keys, std::string const &attrib, type_and_string const &val) {\n\tif (val.type == mvt_null) {\n\t\treturn;\n\t}\n\n\tauto fka = file_keys.find(attrib);\n\tif (fka == file_keys.end()) {\n\t\tfile_keys.insert(std::pair<std::string, type_and_string_stats>(attrib, type_and_string_stats()));\n\t\tfka = file_keys.find(attrib);\n\t}\n\n\tif (fka == file_keys.end()) {\n\t\tfprintf(stderr, \"Can't happen (tilestats)\\n\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (val.type == mvt_double) {\n\t\tdouble d = atof(val.string.c_str());\n\n\t\tif (d < fka->second.min) {\n\t\t\tfka->second.min = d;\n\t\t}\n\t\tif (d > fka->second.max) {\n\t\t\tfka->second.max = d;\n\t\t}\n\t}\n\n\tauto pt = std::lower_bound(fka->second.sample_values.begin(), fka->second.sample_values.end(), val);\n\tif (pt == fka->second.sample_values.end() || *pt != val) {  // not found\n\t\tfka->second.sample_values.insert(pt, val);\n\n\t\tif (fka->second.sample_values.size() > max_tilestats_sample_values) {\n\t\t\tfka->second.sample_values.pop_back();\n\t\t}\n\t}\n\n\tfka->second.type |= (1 << val.type);\n}\n"
        },
        {
          "name": "mbtiles.hpp",
          "type": "blob",
          "size": 1.9736328125,
          "content": "#ifndef MBTILES_HPP\n#define MBTILES_HPP\n\n#include <math.h>\n#include <map>\n#include \"mvt.hpp\"\n\nextern size_t max_tilestats_attributes;\nextern size_t max_tilestats_sample_values;\nextern size_t max_tilestats_values;\n\nstruct type_and_string {\n\tint type = 0;\n\tstd::string string = \"\";\n\n\tbool operator<(const type_and_string &o) const;\n\tbool operator!=(const type_and_string &o) const;\n};\n\nstruct type_and_string_stats {\n\tstd::vector<type_and_string> sample_values = std::vector<type_and_string>();  // sorted\n\tdouble min = INFINITY;\n\tdouble max = -INFINITY;\n\tint type = 0;\n};\n\nstruct layermap_entry {\n\tsize_t id = 0;\n\tstd::map<std::string, type_and_string_stats> file_keys{};\n\tint minzoom = 0;\n\tint maxzoom = 0;\n\tstd::string description = \"\";\n\n\tsize_t points = 0;\n\tsize_t lines = 0;\n\tsize_t polygons = 0;\n\tsize_t retain = 0;  // keep for tilestats, even if no features directly here\n\n\tlayermap_entry(size_t _id) {\n\t\tid = _id;\n\t}\n};\n\nsqlite3 *mbtiles_open(char *dbname, char **argv, int forcetable);\n\nvoid mbtiles_write_tile(sqlite3 *outdb, int z, int tx, int ty, const char *data, int size);\n\nvoid mbtiles_write_metadata(sqlite3 *outdb, const char *outdir, const char *fname, int minzoom, int maxzoom, double minlat, double minlon, double maxlat, double maxlon, double midlat, double midlon, int forcetable, const char *attribution, std::map<std::string, layermap_entry> const &layermap, bool vector, const char *description, bool do_tilestats, std::map<std::string, std::string> const &attribute_descriptions, std::string const &program, std::string const &commandline);\n\nvoid mbtiles_close(sqlite3 *outdb, const char *pgm);\n\nstd::map<std::string, layermap_entry> merge_layermaps(std::vector<std::map<std::string, layermap_entry> > const &maps);\nstd::map<std::string, layermap_entry> merge_layermaps(std::vector<std::map<std::string, layermap_entry> > const &maps, bool trunc);\n\nvoid add_to_file_keys(std::map<std::string, type_and_string_stats> &file_keys, std::string const &layername, type_and_string const &val);\n\n#endif\n"
        },
        {
          "name": "memfile.cpp",
          "type": "blob",
          "size": 1.2568359375,
          "content": "#include <stdlib.h>\n#include <string.h>\n#include <unistd.h>\n#include <sys/mman.h>\n#include \"memfile.hpp\"\n\n#define INCREMENT 131072\n#define INITIAL 256\n\nstruct memfile *memfile_open(int fd) {\n\tif (ftruncate(fd, INITIAL) != 0) {\n\t\treturn NULL;\n\t}\n\n\tchar *map = (char *) mmap(NULL, INITIAL, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);\n\tif (map == MAP_FAILED) {\n\t\treturn NULL;\n\t}\n\n\tstruct memfile *mf = new memfile;\n\tif (mf == NULL) {\n\t\tmunmap(map, INITIAL);\n\t\treturn NULL;\n\t}\n\n\tmf->fd = fd;\n\tmf->map = map;\n\tmf->len = INITIAL;\n\tmf->off = 0;\n\tmf->tree = 0;\n\n\treturn mf;\n}\n\nint memfile_close(struct memfile *file) {\n\tif (munmap(file->map, file->len) != 0) {\n\t\treturn -1;\n\t}\n\n\tif (file->fd >= 0) {\n\t\tif (close(file->fd) != 0) {\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tdelete file;\n\treturn 0;\n}\n\nint memfile_write(struct memfile *file, void *s, long long len) {\n\tif (file->off + len > file->len) {\n\t\tif (munmap(file->map, file->len) != 0) {\n\t\t\treturn -1;\n\t\t}\n\n\t\tfile->len += (len + INCREMENT + 1) / INCREMENT * INCREMENT;\n\n\t\tif (ftruncate(file->fd, file->len) != 0) {\n\t\t\treturn -1;\n\t\t}\n\n\t\tfile->map = (char *) mmap(NULL, file->len, PROT_READ | PROT_WRITE, MAP_SHARED, file->fd, 0);\n\t\tif (file->map == MAP_FAILED) {\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tmemcpy(file->map + file->off, s, len);\n\tfile->off += len;\n\treturn len;\n}\n"
        },
        {
          "name": "memfile.hpp",
          "type": "blob",
          "size": 0.361328125,
          "content": "#ifndef MEMFILE_HPP\n#define MEMFILE_HPP\n\n#include <atomic>\n\nstruct memfile {\n\tint fd = 0;\n\tchar *map = NULL;\n\tstd::atomic<long long> len;\n\tlong long off = 0;\n\tunsigned long tree = 0;\n\n\tmemfile()\n\t    : len(0) {\n\t}\n};\n\nstruct memfile *memfile_open(int fd);\nint memfile_close(struct memfile *file);\nint memfile_write(struct memfile *file, void *s, long long len);\n\n#endif\n"
        },
        {
          "name": "milo",
          "type": "tree",
          "content": null
        },
        {
          "name": "mvt.cpp",
          "type": "blob",
          "size": 16.169921875,
          "content": "#include <stdio.h>\n#include <string.h>\n#include <string>\n#include <vector>\n#include <map>\n#include <zlib.h>\n#include <errno.h>\n#include <limits.h>\n#include <ctype.h>\n#include \"mvt.hpp\"\n#include \"geometry.hpp\"\n#include \"protozero/varint.hpp\"\n#include \"protozero/pbf_reader.hpp\"\n#include \"protozero/pbf_writer.hpp\"\n#include \"milo/dtoa_milo.h\"\n\nmvt_geometry::mvt_geometry(int nop, long long nx, long long ny) {\n\tthis->op = nop;\n\tthis->x = nx;\n\tthis->y = ny;\n}\n\n// https://github.com/mapbox/mapnik-vector-tile/blob/master/src/vector_tile_compression.hpp\nbool is_compressed(std::string const &data) {\n\treturn data.size() > 2 && (((uint8_t) data[0] == 0x78 && (uint8_t) data[1] == 0x9C) || ((uint8_t) data[0] == 0x1F && (uint8_t) data[1] == 0x8B));\n}\n\n// https://github.com/mapbox/mapnik-vector-tile/blob/master/src/vector_tile_compression.hpp\nint decompress(std::string const &input, std::string &output) {\n\tz_stream inflate_s;\n\tinflate_s.zalloc = Z_NULL;\n\tinflate_s.zfree = Z_NULL;\n\tinflate_s.opaque = Z_NULL;\n\tinflate_s.avail_in = 0;\n\tinflate_s.next_in = Z_NULL;\n\tif (inflateInit2(&inflate_s, 32 + 15) != Z_OK) {\n\t\tfprintf(stderr, \"Decompression error: %s\\n\", inflate_s.msg);\n\t}\n\tinflate_s.next_in = (Bytef *) input.data();\n\tinflate_s.avail_in = input.size();\n\tinflate_s.next_out = (Bytef *) output.data();\n\tinflate_s.avail_out = output.size();\n\n\twhile (true) {\n\t\tsize_t existing_output = inflate_s.next_out - (Bytef *) output.data();\n\n\t\toutput.resize(existing_output + 2 * inflate_s.avail_in + 100);\n\t\tinflate_s.next_out = (Bytef *) output.data() + existing_output;\n\t\tinflate_s.avail_out = output.size() - existing_output;\n\n\t\tint ret = inflate(&inflate_s, 0);\n\t\tif (ret < 0) {\n\t\t\tfprintf(stderr, \"Decompression error: \");\n\t\t\tif (ret == Z_DATA_ERROR) {\n\t\t\t\tfprintf(stderr, \"data error\");\n\t\t\t}\n\t\t\tif (ret == Z_STREAM_ERROR) {\n\t\t\t\tfprintf(stderr, \"stream error\");\n\t\t\t}\n\t\t\tif (ret == Z_MEM_ERROR) {\n\t\t\t\tfprintf(stderr, \"out of memory\");\n\t\t\t}\n\t\t\tif (ret == Z_BUF_ERROR) {\n\t\t\t\tfprintf(stderr, \"no data in buffer\");\n\t\t\t}\n\t\t\tfprintf(stderr, \"\\n\");\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (ret == Z_STREAM_END) {\n\t\t\tbreak;\n\t\t}\n\n\t\t// ret must be Z_OK or Z_NEED_DICT;\n\t\t// continue decompresing\n\t}\n\n\toutput.resize(inflate_s.next_out - (Bytef *) output.data());\n\tinflateEnd(&inflate_s);\n\treturn 1;\n}\n\n// https://github.com/mapbox/mapnik-vector-tile/blob/master/src/vector_tile_compression.hpp\nint compress(std::string const &input, std::string &output) {\n\tz_stream deflate_s;\n\tdeflate_s.zalloc = Z_NULL;\n\tdeflate_s.zfree = Z_NULL;\n\tdeflate_s.opaque = Z_NULL;\n\tdeflate_s.avail_in = 0;\n\tdeflate_s.next_in = Z_NULL;\n\tdeflateInit2(&deflate_s, Z_BEST_COMPRESSION, Z_DEFLATED, 31, 8, Z_DEFAULT_STRATEGY);\n\tdeflate_s.next_in = (Bytef *) input.data();\n\tdeflate_s.avail_in = input.size();\n\tsize_t length = 0;\n\tdo {\n\t\tsize_t increase = input.size() / 2 + 1024;\n\t\toutput.resize(length + increase);\n\t\tdeflate_s.avail_out = increase;\n\t\tdeflate_s.next_out = (Bytef *) (output.data() + length);\n\t\tint ret = deflate(&deflate_s, Z_FINISH);\n\t\tif (ret != Z_STREAM_END && ret != Z_OK && ret != Z_BUF_ERROR) {\n\t\t\treturn -1;\n\t\t}\n\t\tlength += (increase - deflate_s.avail_out);\n\t} while (deflate_s.avail_out == 0);\n\tdeflateEnd(&deflate_s);\n\toutput.resize(length);\n\treturn 0;\n}\n\nbool mvt_tile::decode(std::string &message, bool &was_compressed) {\n\tlayers.clear();\n\tstd::string src;\n\n\tif (is_compressed(message)) {\n\t\tstd::string uncompressed;\n\t\tif (decompress(message, uncompressed) == 0) {\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tsrc = uncompressed;\n\t\twas_compressed = true;\n\t} else {\n\t\tsrc = message;\n\t\twas_compressed = false;\n\t}\n\n\tprotozero::pbf_reader reader(src);\n\n\twhile (reader.next()) {\n\t\tswitch (reader.tag()) {\n\t\tcase 3: /* layer */\n\t\t{\n\t\t\tprotozero::pbf_reader layer_reader(reader.get_message());\n\t\t\tmvt_layer layer;\n\n\t\t\twhile (layer_reader.next()) {\n\t\t\t\tswitch (layer_reader.tag()) {\n\t\t\t\tcase 1: /* name */\n\t\t\t\t\tlayer.name = layer_reader.get_string();\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 3: /* key */\n\t\t\t\t\tlayer.keys.push_back(layer_reader.get_string());\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 4: /* value */\n\t\t\t\t{\n\t\t\t\t\tprotozero::pbf_reader value_reader(layer_reader.get_message());\n\t\t\t\t\tmvt_value value;\n\n\t\t\t\t\tvalue.type = mvt_null;\n\t\t\t\t\tvalue.numeric_value.null_value = 0;\n\n\t\t\t\t\twhile (value_reader.next()) {\n\t\t\t\t\t\tswitch (value_reader.tag()) {\n\t\t\t\t\t\tcase 1: /* string */\n\t\t\t\t\t\t\tvalue.type = mvt_string;\n\t\t\t\t\t\t\tvalue.string_value = value_reader.get_string();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 2: /* float */\n\t\t\t\t\t\t\tvalue.type = mvt_float;\n\t\t\t\t\t\t\tvalue.numeric_value.float_value = value_reader.get_float();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 3: /* double */\n\t\t\t\t\t\t\tvalue.type = mvt_double;\n\t\t\t\t\t\t\tvalue.numeric_value.double_value = value_reader.get_double();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 4: /* int */\n\t\t\t\t\t\t\tvalue.type = mvt_int;\n\t\t\t\t\t\t\tvalue.numeric_value.int_value = value_reader.get_int64();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 5: /* uint */\n\t\t\t\t\t\t\tvalue.type = mvt_uint;\n\t\t\t\t\t\t\tvalue.numeric_value.uint_value = value_reader.get_uint64();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 6: /* sint */\n\t\t\t\t\t\t\tvalue.type = mvt_sint;\n\t\t\t\t\t\t\tvalue.numeric_value.sint_value = value_reader.get_sint64();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 7: /* bool */\n\t\t\t\t\t\t\tvalue.type = mvt_bool;\n\t\t\t\t\t\t\tvalue.numeric_value.bool_value = value_reader.get_bool();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\tvalue_reader.skip();\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tlayer.values.push_back(value);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tcase 5: /* extent */\n\t\t\t\t\tlayer.extent = layer_reader.get_uint32();\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 15: /* version */\n\t\t\t\t\tlayer.version = layer_reader.get_uint32();\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 2: /* feature */\n\t\t\t\t{\n\t\t\t\t\tprotozero::pbf_reader feature_reader(layer_reader.get_message());\n\t\t\t\t\tmvt_feature feature;\n\t\t\t\t\tstd::vector<uint32_t> geoms;\n\n\t\t\t\t\twhile (feature_reader.next()) {\n\t\t\t\t\t\tswitch (feature_reader.tag()) {\n\t\t\t\t\t\tcase 1: /* id */\n\t\t\t\t\t\t\tfeature.id = feature_reader.get_uint64();\n\t\t\t\t\t\t\tfeature.has_id = true;\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 2: /* tag */\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tauto pi = feature_reader.get_packed_uint32();\n\t\t\t\t\t\t\tfor (auto it = pi.first; it != pi.second; ++it) {\n\t\t\t\t\t\t\t\tfeature.tags.push_back(*it);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tcase 3: /* feature type */\n\t\t\t\t\t\t\tfeature.type = feature_reader.get_enum();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 4: /* geometry */\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tauto pi = feature_reader.get_packed_uint32();\n\t\t\t\t\t\t\tfor (auto it = pi.first; it != pi.second; ++it) {\n\t\t\t\t\t\t\t\tgeoms.push_back(*it);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\tfeature_reader.skip();\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tlong long px = 0, py = 0;\n\t\t\t\t\tfor (size_t g = 0; g < geoms.size(); g++) {\n\t\t\t\t\t\tuint32_t geom = geoms[g];\n\t\t\t\t\t\tuint32_t op = geom & 7;\n\t\t\t\t\t\tuint32_t count = geom >> 3;\n\n\t\t\t\t\t\tif (op == mvt_moveto || op == mvt_lineto) {\n\t\t\t\t\t\t\tfor (size_t k = 0; k < count && g + 2 < geoms.size(); k++) {\n\t\t\t\t\t\t\t\tpx += protozero::decode_zigzag32(geoms[g + 1]);\n\t\t\t\t\t\t\t\tpy += protozero::decode_zigzag32(geoms[g + 2]);\n\t\t\t\t\t\t\t\tg += 2;\n\n\t\t\t\t\t\t\t\tfeature.geometry.push_back(mvt_geometry(op, px, py));\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tfeature.geometry.push_back(mvt_geometry(op, 0, 0));\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tlayer.features.push_back(feature);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tdefault:\n\t\t\t\t\tlayer_reader.skip();\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor (size_t i = 0; i < layer.keys.size(); i++) {\n\t\t\t\tlayer.key_map.insert(std::pair<std::string, size_t>(layer.keys[i], i));\n\t\t\t}\n\t\t\tfor (size_t i = 0; i < layer.values.size(); i++) {\n\t\t\t\tlayer.value_map.insert(std::pair<mvt_value, size_t>(layer.values[i], i));\n\t\t\t}\n\n\t\t\tlayers.push_back(layer);\n\t\t\tbreak;\n\t\t}\n\n\t\tdefault:\n\t\t\treader.skip();\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstd::string mvt_tile::encode() {\n\tstd::string data;\n\n\tprotozero::pbf_writer writer(data);\n\n\tfor (size_t i = 0; i < layers.size(); i++) {\n\t\tstd::string layer_string;\n\t\tprotozero::pbf_writer layer_writer(layer_string);\n\n\t\tlayer_writer.add_uint32(15, layers[i].version); /* version */\n\t\tlayer_writer.add_string(1, layers[i].name);     /* name */\n\t\tlayer_writer.add_uint32(5, layers[i].extent);   /* extent */\n\n\t\tfor (size_t j = 0; j < layers[i].keys.size(); j++) {\n\t\t\tlayer_writer.add_string(3, layers[i].keys[j]); /* key */\n\t\t}\n\n\t\tfor (size_t v = 0; v < layers[i].values.size(); v++) {\n\t\t\tstd::string value_string;\n\t\t\tprotozero::pbf_writer value_writer(value_string);\n\t\t\tmvt_value &pbv = layers[i].values[v];\n\n\t\t\tif (pbv.type == mvt_string) {\n\t\t\t\tvalue_writer.add_string(1, pbv.string_value);\n\t\t\t} else if (pbv.type == mvt_float) {\n\t\t\t\tvalue_writer.add_float(2, pbv.numeric_value.float_value);\n\t\t\t} else if (pbv.type == mvt_double) {\n\t\t\t\tvalue_writer.add_double(3, pbv.numeric_value.double_value);\n\t\t\t} else if (pbv.type == mvt_int) {\n\t\t\t\tvalue_writer.add_int64(4, pbv.numeric_value.int_value);\n\t\t\t} else if (pbv.type == mvt_uint) {\n\t\t\t\tvalue_writer.add_uint64(5, pbv.numeric_value.uint_value);\n\t\t\t} else if (pbv.type == mvt_sint) {\n\t\t\t\tvalue_writer.add_sint64(6, pbv.numeric_value.sint_value);\n\t\t\t} else if (pbv.type == mvt_bool) {\n\t\t\t\tvalue_writer.add_bool(7, pbv.numeric_value.bool_value);\n\t\t\t} else if (pbv.type == mvt_null) {\n\t\t\t\tfprintf(stderr, \"Internal error: trying to write null attribute to tile\\n\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t} else {\n\t\t\t\tfprintf(stderr, \"Internal error: trying to write undefined attribute type to tile\\n\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tlayer_writer.add_message(4, value_string);\n\t\t}\n\n\t\tfor (size_t f = 0; f < layers[i].features.size(); f++) {\n\t\t\tstd::string feature_string;\n\t\t\tprotozero::pbf_writer feature_writer(feature_string);\n\n\t\t\tfeature_writer.add_enum(3, layers[i].features[f].type);\n\t\t\tfeature_writer.add_packed_uint32(2, std::begin(layers[i].features[f].tags), std::end(layers[i].features[f].tags));\n\n\t\t\tif (layers[i].features[f].has_id) {\n\t\t\t\tfeature_writer.add_uint64(1, layers[i].features[f].id);\n\t\t\t}\n\n\t\t\tstd::vector<uint32_t> geometry;\n\n\t\t\tlong long px = 0, py = 0;\n\t\t\tint cmd_idx = -1;\n\t\t\tint cmd = -1;\n\t\t\tint length = 0;\n\n\t\t\tstd::vector<mvt_geometry> &geom = layers[i].features[f].geometry;\n\n\t\t\tfor (size_t g = 0; g < geom.size(); g++) {\n\t\t\t\tint op = geom[g].op;\n\n\t\t\t\tif (op != cmd) {\n\t\t\t\t\tif (cmd_idx >= 0) {\n\t\t\t\t\t\tgeometry[cmd_idx] = (length << 3) | (cmd & ((1 << 3) - 1));\n\t\t\t\t\t}\n\n\t\t\t\t\tcmd = op;\n\t\t\t\t\tlength = 0;\n\t\t\t\t\tcmd_idx = geometry.size();\n\t\t\t\t\tgeometry.push_back(0);\n\t\t\t\t}\n\n\t\t\t\tif (op == mvt_moveto || op == mvt_lineto) {\n\t\t\t\t\tlong long wwx = geom[g].x;\n\t\t\t\t\tlong long wwy = geom[g].y;\n\n\t\t\t\t\tlong long dx = wwx - px;\n\t\t\t\t\tlong long dy = wwy - py;\n\n\t\t\t\t\tif (dx < INT_MIN || dx > INT_MAX || dy < INT_MIN || dy > INT_MAX) {\n\t\t\t\t\t\tfprintf(stderr, \"Internal error: Geometry delta is too big: %lld,%lld\\n\", dx, dy);\n\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t}\n\n\t\t\t\t\tgeometry.push_back(protozero::encode_zigzag32(dx));\n\t\t\t\t\tgeometry.push_back(protozero::encode_zigzag32(dy));\n\n\t\t\t\t\tpx = wwx;\n\t\t\t\t\tpy = wwy;\n\t\t\t\t\tlength++;\n\t\t\t\t} else if (op == mvt_closepath) {\n\t\t\t\t\tlength++;\n\t\t\t\t} else {\n\t\t\t\t\tfprintf(stderr, \"\\nInternal error: corrupted geometry\\n\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (cmd_idx >= 0) {\n\t\t\t\tgeometry[cmd_idx] = (length << 3) | (cmd & ((1 << 3) - 1));\n\t\t\t}\n\n\t\t\tfeature_writer.add_packed_uint32(4, std::begin(geometry), std::end(geometry));\n\t\t\tlayer_writer.add_message(2, feature_string);\n\t\t}\n\n\t\twriter.add_message(3, layer_string);\n\t}\n\n\treturn data;\n}\n\nbool mvt_value::operator<(const mvt_value &o) const {\n\tif (type < o.type) {\n\t\treturn true;\n\t}\n\tif (type == o.type) {\n\t\tif ((type == mvt_string && string_value < o.string_value) ||\n\t\t    (type == mvt_float && numeric_value.float_value < o.numeric_value.float_value) ||\n\t\t    (type == mvt_double && numeric_value.double_value < o.numeric_value.double_value) ||\n\t\t    (type == mvt_int && numeric_value.int_value < o.numeric_value.int_value) ||\n\t\t    (type == mvt_uint && numeric_value.uint_value < o.numeric_value.uint_value) ||\n\t\t    (type == mvt_sint && numeric_value.sint_value < o.numeric_value.sint_value) ||\n\t\t    (type == mvt_bool && numeric_value.bool_value < o.numeric_value.bool_value) ||\n\t\t    (type == mvt_null && numeric_value.null_value < o.numeric_value.null_value)) {\n\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic std::string quote(std::string const &s) {\n\tstd::string buf;\n\n\tfor (size_t i = 0; i < s.size(); i++) {\n\t\tunsigned char ch = s[i];\n\n\t\tif (ch == '\\\\' || ch == '\\\"') {\n\t\t\tbuf.push_back('\\\\');\n\t\t\tbuf.push_back(ch);\n\t\t} else if (ch < ' ') {\n\t\t\tchar tmp[7];\n\t\t\tsprintf(tmp, \"\\\\u%04x\", ch);\n\t\t\tbuf.append(std::string(tmp));\n\t\t} else {\n\t\t\tbuf.push_back(ch);\n\t\t}\n\t}\n\n\treturn buf;\n}\n\nstd::string mvt_value::toString() {\n\tif (type == mvt_string) {\n\t\treturn quote(string_value);\n\t} else if (type == mvt_int) {\n\t\treturn std::to_string(numeric_value.int_value);\n\t} else if (type == mvt_double) {\n\t\tdouble v = numeric_value.double_value;\n\t\tif (v == (long long) v) {\n\t\t\treturn std::to_string((long long) v);\n\t\t} else {\n\t\t\treturn milo::dtoa_milo(v);\n\t\t}\n\t} else if (type == mvt_float) {\n\t\tdouble v = numeric_value.float_value;\n\t\tif (v == (long long) v) {\n\t\t\treturn std::to_string((long long) v);\n\t\t} else {\n\t\t\treturn milo::dtoa_milo(v);\n\t\t}\n\t} else if (type == mvt_sint) {\n\t\treturn std::to_string(numeric_value.sint_value);\n\t} else if (type == mvt_uint) {\n\t\treturn std::to_string(numeric_value.uint_value);\n\t} else if (type == mvt_bool) {\n\t\treturn numeric_value.bool_value ? \"true\" : \"false\";\n\t} else if (type == mvt_null) {\n\t\treturn \"null\";\n\t} else {\n\t\treturn \"unknown\";\n\t}\n}\n\nvoid mvt_layer::tag(mvt_feature &feature, std::string key, mvt_value value) {\n\tsize_t ko, vo;\n\n\tstd::map<std::string, size_t>::iterator ki = key_map.find(key);\n\tstd::map<mvt_value, size_t>::iterator vi = value_map.find(value);\n\n\tif (ki == key_map.end()) {\n\t\tko = keys.size();\n\t\tkeys.push_back(key);\n\t\tkey_map.insert(std::pair<std::string, size_t>(key, ko));\n\t} else {\n\t\tko = ki->second;\n\t}\n\n\tif (vi == value_map.end()) {\n\t\tvo = values.size();\n\t\tvalues.push_back(value);\n\t\tvalue_map.insert(std::pair<mvt_value, size_t>(value, vo));\n\t} else {\n\t\tvo = vi->second;\n\t}\n\n\tfeature.tags.push_back(ko);\n\tfeature.tags.push_back(vo);\n}\n\nbool is_integer(const char *s, long long *v) {\n\terrno = 0;\n\tchar *endptr;\n\n\t*v = strtoll(s, &endptr, 0);\n\tif (*v == 0 && errno != 0) {\n\t\treturn 0;\n\t}\n\tif ((*v == LLONG_MIN || *v == LLONG_MAX) && (errno == ERANGE || errno == EINVAL)) {\n\t\treturn 0;\n\t}\n\tif (*endptr != '\\0') {\n\t\t// Special case: If it is an integer followed by .0000 or similar,\n\t\t// it is still an integer\n\n\t\tif (*endptr != '.') {\n\t\t\treturn 0;\n\t\t}\n\t\tendptr++;\n\t\tfor (; *endptr != '\\0'; endptr++) {\n\t\t\tif (*endptr != '0') {\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\n\t\treturn 1;\n\t}\n\n\treturn 1;\n}\n\nbool is_unsigned_integer(const char *s, unsigned long long *v) {\n\terrno = 0;\n\tchar *endptr;\n\n\t// Special check because MacOS stroull() returns 1\n\t// for -18446744073709551615\n\twhile (isspace(*s)) {\n\t\ts++;\n\t}\n\tif (*s == '-') {\n\t\treturn 0;\n\t}\n\n\t*v = strtoull(s, &endptr, 0);\n\tif (*v == 0 && errno != 0) {\n\t\treturn 0;\n\t}\n\tif ((*v == ULLONG_MAX) && (errno == ERANGE || errno == EINVAL)) {\n\t\treturn 0;\n\t}\n\tif (*endptr != '\\0') {\n\t\t// Special case: If it is an integer followed by .0000 or similar,\n\t\t// it is still an integer\n\n\t\tif (*endptr != '.') {\n\t\t\treturn 0;\n\t\t}\n\t\tendptr++;\n\t\tfor (; *endptr != '\\0'; endptr++) {\n\t\t\tif (*endptr != '0') {\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\n\t\treturn 1;\n\t}\n\n\treturn 1;\n}\n\nmvt_value stringified_to_mvt_value(int type, const char *s) {\n\tmvt_value tv;\n\n\tif (type == mvt_double) {\n\t\tlong long v;\n\t\tunsigned long long uv;\n\t\tif (is_unsigned_integer(s, &uv)) {\n\t\t\tif (uv <= LLONG_MAX) {\n\t\t\t\ttv.type = mvt_int;\n\t\t\t\ttv.numeric_value.int_value = uv;\n\t\t\t} else {\n\t\t\t\ttv.type = mvt_uint;\n\t\t\t\ttv.numeric_value.uint_value = uv;\n\t\t\t}\n\t\t} else if (is_integer(s, &v)) {\n\t\t\ttv.type = mvt_sint;\n\t\t\ttv.numeric_value.sint_value = v;\n\t\t} else {\n\t\t\terrno = 0;\n\t\t\tchar *endptr;\n\n\t\t\tfloat f = strtof(s, &endptr);\n\n\t\t\tif (endptr == s || ((f == HUGE_VAL || f == HUGE_VALF || f == HUGE_VALL) && errno == ERANGE)) {\n\t\t\t\tdouble d = strtod(s, &endptr);\n\t\t\t\tif (endptr == s || ((d == HUGE_VAL || d == HUGE_VALF || d == HUGE_VALL) && errno == ERANGE)) {\n\t\t\t\t\tfprintf(stderr, \"Warning: numeric value %s could not be represented\\n\", s);\n\t\t\t\t}\n\t\t\t\ttv.type = mvt_double;\n\t\t\t\ttv.numeric_value.double_value = d;\n\t\t\t} else {\n\t\t\t\tdouble d = atof(s);\n\t\t\t\tif (f == d) {\n\t\t\t\t\ttv.type = mvt_float;\n\t\t\t\t\ttv.numeric_value.float_value = f;\n\t\t\t\t} else {\n\t\t\t\t\t// Conversion succeeded, but lost precision, so use double\n\t\t\t\t\ttv.type = mvt_double;\n\t\t\t\t\ttv.numeric_value.double_value = d;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else if (type == mvt_bool) {\n\t\ttv.type = mvt_bool;\n\t\ttv.numeric_value.bool_value = (s[0] == 't');\n\t} else if (type == mvt_null) {\n\t\ttv.type = mvt_null;\n\t\ttv.numeric_value.null_value = 0;\n\t} else {\n\t\ttv.type = mvt_string;\n\t\ttv.string_value = s;\n\t}\n\n\treturn tv;\n}\n"
        },
        {
          "name": "mvt.hpp",
          "type": "blob",
          "size": 2.4423828125,
          "content": "#ifndef MVT_HPP\n#define MVT_HPP\n\n#include <sqlite3.h>\n#include <string>\n#include <map>\n#include <set>\n#include <vector>\n\nstruct mvt_value;\nstruct mvt_layer;\n\nenum mvt_operation {\n\tmvt_moveto = 1,\n\tmvt_lineto = 2,\n\tmvt_closepath = 7\n};\n\nstruct mvt_geometry {\n\tlong long x = 0;\n\tlong long y = 0;\n\tint /* mvt_operation */ op = 0;\n\n\tmvt_geometry(int op, long long x, long long y);\n\n\tbool operator<(mvt_geometry const &s) const {\n\t\tif (y < s.y || (y == s.y && x < s.x)) {\n\t\t\treturn true;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tbool operator==(mvt_geometry const &s) const {\n\t\treturn y == s.y && x == s.x;\n\t}\n};\n\nenum mvt_geometry_type {\n\tmvt_point = 1,\n\tmvt_linestring = 2,\n\tmvt_polygon = 3\n};\n\nstruct mvt_feature {\n\tstd::vector<unsigned> tags{};\n\tstd::vector<mvt_geometry> geometry{};\n\tint /* mvt_geometry_type */ type = 0;\n\tunsigned long long id = 0;\n\tbool has_id = false;\n\tbool dropped = false;\n\n\tmvt_feature() {\n\t\thas_id = false;\n\t\tid = 0;\n\t}\n};\n\nenum mvt_value_type {\n\tmvt_string,\n\tmvt_float,\n\tmvt_double,\n\tmvt_int,\n\tmvt_uint,\n\tmvt_sint,\n\tmvt_bool,\n\tmvt_null,\n};\n\nstruct mvt_value {\n\tmvt_value_type type;\n\tstd::string string_value;\n\tunion {\n\t\tfloat float_value;\n\t\tdouble double_value;\n\t\tlong long int_value;\n\t\tunsigned long long uint_value;\n\t\tlong long sint_value;\n\t\tbool bool_value;\n\t\tint null_value;\n\t} numeric_value;\n\n\tbool operator<(const mvt_value &o) const;\n\tstd::string toString();\n\n\tmvt_value() {\n\t\tthis->type = mvt_double;\n\t\tthis->string_value = \"\";\n\t\tthis->numeric_value.double_value = 0;\n\t}\n};\n\nstruct mvt_layer {\n\tint version = 0;\n\tstd::string name = \"\";\n\tstd::vector<mvt_feature> features{};\n\tstd::vector<std::string> keys{};\n\tstd::vector<mvt_value> values{};\n\tlong long extent = 0;\n\n\t// Add a key-value pair to a feature, using this layer's constant pool\n\tvoid tag(mvt_feature &feature, std::string key, mvt_value value);\n\n\t// For tracking the key-value constants already used in this layer\n\tstd::map<std::string, size_t> key_map{};\n\tstd::map<mvt_value, size_t> value_map{};\n};\n\nstruct mvt_tile {\n\tstd::vector<mvt_layer> layers{};\n\n\tstd::string encode();\n\tbool decode(std::string &message, bool &was_compressed);\n};\n\nbool is_compressed(std::string const &data);\nint decompress(std::string const &input, std::string &output);\nint compress(std::string const &input, std::string &output);\nint dezig(unsigned n);\n\nmvt_value stringified_to_mvt_value(int type, const char *s);\n\nbool is_integer(const char *s, long long *v);\nbool is_unsigned_integer(const char *s, unsigned long long *v);\n#endif\n"
        },
        {
          "name": "options.hpp",
          "type": "blob",
          "size": 1.6494140625,
          "content": "#ifndef OPTIONS_HPP\n#define OPTIONS_HPP\n\n#define A_COALESCE ((int) 'c')\n#define A_REVERSE ((int) 'r')\n#define A_REORDER ((int) 'o')\n#define A_LINE_DROP ((int) 'l')\n#define A_DEBUG_POLYGON ((int) '@')\n#define A_POLYGON_DROP ((int) 'p')\n#define A_DETECT_SHARED_BORDERS ((int) 'b')\n#define A_PREFER_RADIX_SORT ((int) 'R')\n#define A_CALCULATE_FEATURE_DENSITY ((int) 'g')\n#define A_INCREASE_GAMMA_AS_NEEDED ((int) 'G')\n#define A_MERGE_POLYGONS_AS_NEEDED ((int) 'm')\n#define A_DROP_DENSEST_AS_NEEDED ((int) 's')\n#define A_DROP_FRACTION_AS_NEEDED ((int) 'd')\n#define A_DROP_SMALLEST_AS_NEEDED ((int) 'n')\n#define A_COALESCE_DENSEST_AS_NEEDED ((int) 'S')\n#define A_COALESCE_SMALLEST_AS_NEEDED ((int) 'N')\n#define A_COALESCE_FRACTION_AS_NEEDED ((int) 'D')\n#define A_GRID_LOW_ZOOMS ((int) 'L')\n#define A_DETECT_WRAPAROUND ((int) 'w')\n#define A_EXTEND_ZOOMS ((int) 'e')\n#define A_CLUSTER_DENSEST_AS_NEEDED ((int) 'C')\n#define A_GENERATE_IDS ((int) 'i')\n#define A_CONVERT_NUMERIC_IDS ((int) 'I')\n#define A_HILBERT ((int) 'h')\n\n#define P_SIMPLIFY ((int) 's')\n#define P_SIMPLIFY_LOW ((int) 'S')\n#define P_SIMPLIFY_SHARED_NODES ((int) 'n')\n#define P_FEATURE_LIMIT ((int) 'f')\n#define P_KILOBYTE_LIMIT ((int) 'k')\n#define P_DYNAMIC_DROP ((int) 'd')\n#define P_INPUT_ORDER ((int) 'i')\n#define P_POLYGON_SPLIT ((int) 'p')\n#define P_CLIPPING ((int) 'c')\n#define P_DUPLICATION ((int) 'D')\n#define P_TINY_POLYGON_REDUCTION ((int) 't')\n#define P_TILE_COMPRESSION ((int) 'C')\n#define P_TILE_STATS ((int) 'g')\n#define P_USE_SOURCE_POLYGON_WINDING ((int) 'w')\n#define P_REVERSE_SOURCE_POLYGON_WINDING ((int) 'W')\n#define P_EMPTY_CSV_COLUMNS ((int) 'e')\n\nextern int prevent[256];\nextern int additional[256];\n\n#endif\n"
        },
        {
          "name": "plugin.cpp",
          "type": "blob",
          "size": 16.9794921875,
          "content": "#ifdef __APPLE__\n#define _DARWIN_UNLIMITED_STREAMS\n#endif\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <vector>\n#include <string>\n#include <map>\n#include <set>\n#include <pthread.h>\n#include <unistd.h>\n#include <fcntl.h>\n#include <errno.h>\n#include <cmath>\n#include <sys/types.h>\n#include <sys/wait.h>\n#include <sqlite3.h>\n#include <limits.h>\n#include \"main.hpp\"\n#include \"mvt.hpp\"\n#include \"mbtiles.hpp\"\n#include \"projection.hpp\"\n#include \"geometry.hpp\"\n#include \"serial.hpp\"\n\nextern \"C\" {\n#include \"jsonpull/jsonpull.h\"\n}\n\n#include \"plugin.hpp\"\n#include \"write_json.hpp\"\n#include \"read_json.hpp\"\n\nstruct writer_arg {\n\tint write_to;\n\tstd::vector<mvt_layer> *layers;\n\tunsigned z;\n\tunsigned x;\n\tunsigned y;\n\tint extent;\n};\n\nvoid *run_writer(void *a) {\n\twriter_arg *wa = (writer_arg *) a;\n\n\tFILE *fp = fdopen(wa->write_to, \"w\");\n\tif (fp == NULL) {\n\t\tperror(\"fdopen (pipe writer)\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tjson_writer state(fp);\n\tfor (size_t i = 0; i < wa->layers->size(); i++) {\n\t\tlayer_to_geojson((*(wa->layers))[i], wa->z, wa->x, wa->y, false, true, false, true, 0, 0, 0, true, state);\n\t}\n\n\tif (fclose(fp) != 0) {\n\t\tif (errno == EPIPE) {\n\t\t\tstatic bool warned = false;\n\t\t\tif (!warned) {\n\t\t\t\tfprintf(stderr, \"Warning: broken pipe in postfilter\\n\");\n\t\t\t\twarned = true;\n\t\t\t}\n\t\t} else {\n\t\t\tperror(\"fclose output to filter\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n// XXX deduplicate\nstatic std::vector<mvt_geometry> to_feature(drawvec &geom) {\n\tstd::vector<mvt_geometry> out;\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tout.push_back(mvt_geometry(geom[i].op, geom[i].x, geom[i].y));\n\t}\n\n\treturn out;\n}\n\n// Reads from the postfilter\nstd::vector<mvt_layer> parse_layers(int fd, int z, unsigned x, unsigned y, std::vector<std::map<std::string, layermap_entry>> *layermaps, size_t tiling_seg, std::vector<std::vector<std::string>> *layer_unmaps, int extent) {\n\tstd::map<std::string, mvt_layer> ret;\n\n\tFILE *f = fdopen(fd, \"r\");\n\tif (f == NULL) {\n\t\tperror(\"fdopen filter output\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tjson_pull *jp = json_begin_file(f);\n\n\twhile (1) {\n\t\tjson_object *j = json_read(jp);\n\t\tif (j == NULL) {\n\t\t\tif (jp->error != NULL) {\n\t\t\t\tfprintf(stderr, \"Filter output:%d: %s\\n\", jp->line, jp->error);\n\t\t\t\tif (jp->root != NULL) {\n\t\t\t\t\tjson_context(jp->root);\n\t\t\t\t}\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tjson_free(jp->root);\n\t\t\tbreak;\n\t\t}\n\n\t\tjson_object *type = json_hash_get(j, \"type\");\n\t\tif (type == NULL || type->type != JSON_STRING) {\n\t\t\tcontinue;\n\t\t}\n\t\tif (strcmp(type->string, \"Feature\") != 0) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tjson_object *geometry = json_hash_get(j, \"geometry\");\n\t\tif (geometry == NULL) {\n\t\t\tfprintf(stderr, \"Filter output:%d: filtered feature with no geometry\\n\", jp->line);\n\t\t\tjson_context(j);\n\t\t\tjson_free(j);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tjson_object *properties = json_hash_get(j, \"properties\");\n\t\tif (properties == NULL || (properties->type != JSON_HASH && properties->type != JSON_NULL)) {\n\t\t\tfprintf(stderr, \"Filter output:%d: feature without properties hash\\n\", jp->line);\n\t\t\tjson_context(j);\n\t\t\tjson_free(j);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tjson_object *geometry_type = json_hash_get(geometry, \"type\");\n\t\tif (geometry_type == NULL) {\n\t\t\tfprintf(stderr, \"Filter output:%d: null geometry (additional not reported)\\n\", jp->line);\n\t\t\tjson_context(j);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tif (geometry_type->type != JSON_STRING) {\n\t\t\tfprintf(stderr, \"Filter output:%d: geometry type is not a string\\n\", jp->line);\n\t\t\tjson_context(j);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tjson_object *coordinates = json_hash_get(geometry, \"coordinates\");\n\t\tif (coordinates == NULL || coordinates->type != JSON_ARRAY) {\n\t\t\tfprintf(stderr, \"Filter output:%d: feature without coordinates array\\n\", jp->line);\n\t\t\tjson_context(j);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tint t;\n\t\tfor (t = 0; t < GEOM_TYPES; t++) {\n\t\t\tif (strcmp(geometry_type->string, geometry_names[t]) == 0) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (t >= GEOM_TYPES) {\n\t\t\tfprintf(stderr, \"Filter output:%d: Can't handle geometry type %s\\n\", jp->line, geometry_type->string);\n\t\t\tjson_context(j);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tstd::string layername = \"unknown\";\n\t\tjson_object *tippecanoe = json_hash_get(j, \"tippecanoe\");\n\t\tjson_object *layer = NULL;\n\t\tif (tippecanoe != NULL) {\n\t\t\tlayer = json_hash_get(tippecanoe, \"layer\");\n\t\t\tif (layer != NULL && layer->type == JSON_STRING) {\n\t\t\t\tlayername = std::string(layer->string);\n\t\t\t}\n\t\t}\n\n\t\tif (ret.count(layername) == 0) {\n\t\t\tmvt_layer l;\n\t\t\tl.name = layername;\n\t\t\tl.version = 2;\n\t\t\tl.extent = extent;\n\n\t\t\tret.insert(std::pair<std::string, mvt_layer>(layername, l));\n\t\t}\n\t\tauto l = ret.find(layername);\n\n\t\tdrawvec dv;\n\t\tparse_geometry(t, coordinates, dv, VT_MOVETO, \"Filter output\", jp->line, j);\n\t\tif (mb_geometry[t] == VT_POLYGON) {\n\t\t\tdv = fix_polygon(dv);\n\t\t}\n\n\t\t// Scale and offset geometry from global to tile\n\t\tfor (size_t i = 0; i < dv.size(); i++) {\n\t\t\tlong long scale = 1LL << (32 - z);\n\t\t\tdv[i].x = std::round((dv[i].x - scale * x) * extent / (double) scale);\n\t\t\tdv[i].y = std::round((dv[i].y - scale * y) * extent / (double) scale);\n\t\t}\n\n\t\tif (mb_geometry[t] == VT_POLYGON) {\n\t\t\tdv = clean_or_clip_poly(dv, 0, 0, false);\n\t\t\tif (dv.size() < 3) {\n\t\t\t\tdv.clear();\n\t\t\t}\n\t\t}\n\t\tdv = remove_noop(dv, mb_geometry[t], 0);\n\t\tif (mb_geometry[t] == VT_POLYGON) {\n\t\t\tdv = close_poly(dv);\n\t\t}\n\n\t\tif (dv.size() > 0) {\n\t\t\tmvt_feature feature;\n\t\t\tfeature.type = mb_geometry[t];\n\t\t\tfeature.geometry = to_feature(dv);\n\n\t\t\tjson_object *id = json_hash_get(j, \"id\");\n\t\t\tif (id != NULL) {\n\t\t\t\tfeature.id = atoll(id->string);\n\t\t\t\tfeature.has_id = true;\n\t\t\t}\n\n\t\t\tstd::map<std::string, layermap_entry> &layermap = (*layermaps)[tiling_seg];\n\t\t\tif (layermap.count(layername) == 0) {\n\t\t\t\tlayermap_entry lme = layermap_entry(layermap.size());\n\t\t\t\tlme.minzoom = z;\n\t\t\t\tlme.maxzoom = z;\n\n\t\t\t\tlayermap.insert(std::pair<std::string, layermap_entry>(layername, lme));\n\n\t\t\t\tif (lme.id >= (*layer_unmaps)[tiling_seg].size()) {\n\t\t\t\t\t(*layer_unmaps)[tiling_seg].resize(lme.id + 1);\n\t\t\t\t\t(*layer_unmaps)[tiling_seg][lme.id] = layername;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tauto fk = layermap.find(layername);\n\t\t\tif (fk == layermap.end()) {\n\t\t\t\tfprintf(stderr, \"Internal error: layer %s not found\\n\", layername.c_str());\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tif (z < fk->second.minzoom) {\n\t\t\t\tfk->second.minzoom = z;\n\t\t\t}\n\t\t\tif (z > fk->second.maxzoom) {\n\t\t\t\tfk->second.maxzoom = z;\n\t\t\t}\n\n\t\t\tif (feature.type == mvt_point) {\n\t\t\t\tfk->second.points++;\n\t\t\t} else if (feature.type == mvt_linestring) {\n\t\t\t\tfk->second.lines++;\n\t\t\t} else if (feature.type == mvt_polygon) {\n\t\t\t\tfk->second.polygons++;\n\t\t\t}\n\n\t\t\tfor (size_t i = 0; i < properties->length; i++) {\n\t\t\t\tint tp = -1;\n\t\t\t\tstd::string s;\n\n\t\t\t\tstringify_value(properties->values[i], tp, s, \"Filter output\", jp->line, j);\n\n\t\t\t\t// Nulls can be excluded here because this is the postfilter\n\t\t\t\t// and it is nearly time to create the vector representation\n\n\t\t\t\tif (tp >= 0 && tp != mvt_null) {\n\t\t\t\t\tmvt_value v = stringified_to_mvt_value(tp, s.c_str());\n\t\t\t\t\tl->second.tag(feature, std::string(properties->keys[i]->string), v);\n\n\t\t\t\t\ttype_and_string attrib;\n\t\t\t\t\tattrib.type = tp;\n\t\t\t\t\tattrib.string = s;\n\n\t\t\t\t\tadd_to_file_keys(fk->second.file_keys, std::string(properties->keys[i]->string), attrib);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tl->second.features.push_back(feature);\n\t\t}\n\n\t\tjson_free(j);\n\t}\n\n\tjson_end(jp);\n\tif (fclose(f) != 0) {\n\t\tperror(\"fclose postfilter output\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tstd::vector<mvt_layer> final;\n\tfor (auto a : ret) {\n\t\tfinal.push_back(a.second);\n\t}\n\treturn final;\n}\n\n// Reads from the prefilter\nserial_feature parse_feature(json_pull *jp, int z, unsigned x, unsigned y, std::vector<std::map<std::string, layermap_entry>> *layermaps, size_t tiling_seg, std::vector<std::vector<std::string>> *layer_unmaps, bool postfilter) {\n\tserial_feature sf;\n\n\twhile (1) {\n\t\tjson_object *j = json_read(jp);\n\t\tif (j == NULL) {\n\t\t\tif (jp->error != NULL) {\n\t\t\t\tfprintf(stderr, \"Filter output:%d: %s\\n\", jp->line, jp->error);\n\t\t\t\tif (jp->root != NULL) {\n\t\t\t\t\tjson_context(jp->root);\n\t\t\t\t}\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tjson_free(jp->root);\n\t\t\tsf.t = -1;\n\t\t\treturn sf;\n\t\t}\n\n\t\tjson_object *type = json_hash_get(j, \"type\");\n\t\tif (type == NULL || type->type != JSON_STRING) {\n\t\t\tcontinue;\n\t\t}\n\t\tif (strcmp(type->string, \"Feature\") != 0) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tjson_object *geometry = json_hash_get(j, \"geometry\");\n\t\tif (geometry == NULL) {\n\t\t\tfprintf(stderr, \"Filter output:%d: filtered feature with no geometry\\n\", jp->line);\n\t\t\tjson_context(j);\n\t\t\tjson_free(j);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tjson_object *properties = json_hash_get(j, \"properties\");\n\t\tif (properties == NULL || (properties->type != JSON_HASH && properties->type != JSON_NULL)) {\n\t\t\tfprintf(stderr, \"Filter output:%d: feature without properties hash\\n\", jp->line);\n\t\t\tjson_context(j);\n\t\t\tjson_free(j);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tjson_object *geometry_type = json_hash_get(geometry, \"type\");\n\t\tif (geometry_type == NULL) {\n\t\t\tfprintf(stderr, \"Filter output:%d: null geometry (additional not reported)\\n\", jp->line);\n\t\t\tjson_context(j);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tif (geometry_type->type != JSON_STRING) {\n\t\t\tfprintf(stderr, \"Filter output:%d: geometry type is not a string\\n\", jp->line);\n\t\t\tjson_context(j);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tjson_object *coordinates = json_hash_get(geometry, \"coordinates\");\n\t\tif (coordinates == NULL || coordinates->type != JSON_ARRAY) {\n\t\t\tfprintf(stderr, \"Filter output:%d: feature without coordinates array\\n\", jp->line);\n\t\t\tjson_context(j);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tint t;\n\t\tfor (t = 0; t < GEOM_TYPES; t++) {\n\t\t\tif (strcmp(geometry_type->string, geometry_names[t]) == 0) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (t >= GEOM_TYPES) {\n\t\t\tfprintf(stderr, \"Filter output:%d: Can't handle geometry type %s\\n\", jp->line, geometry_type->string);\n\t\t\tjson_context(j);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tdrawvec dv;\n\t\tparse_geometry(t, coordinates, dv, VT_MOVETO, \"Filter output\", jp->line, j);\n\t\tif (mb_geometry[t] == VT_POLYGON) {\n\t\t\tdv = fix_polygon(dv);\n\t\t}\n\n\t\t// Scale and offset geometry from global to tile\n\t\tdouble scale = 1LL << geometry_scale;\n\t\tfor (size_t i = 0; i < dv.size(); i++) {\n\t\t\tunsigned sx = 0, sy = 0;\n\t\t\tif (z != 0) {\n\t\t\t\tsx = x << (32 - z);\n\t\t\t\tsy = y << (32 - z);\n\t\t\t}\n\t\t\tdv[i].x = std::round(dv[i].x / scale) * scale - sx;\n\t\t\tdv[i].y = std::round(dv[i].y / scale) * scale - sy;\n\t\t}\n\n\t\tif (dv.size() > 0) {\n\t\t\tsf.t = mb_geometry[t];\n\t\t\tsf.segment = tiling_seg;\n\t\t\tsf.geometry = dv;\n\t\t\tsf.seq = 0;\n\t\t\tsf.index = 0;\n\t\t\tsf.bbox[0] = sf.bbox[1] = LLONG_MAX;\n\t\t\tsf.bbox[2] = sf.bbox[3] = LLONG_MIN;\n\t\t\tsf.extent = 0;\n\t\t\tsf.metapos = 0;\n\t\t\tsf.has_id = false;\n\n\t\t\tstd::string layername = \"unknown\";\n\t\t\tjson_object *tippecanoe = json_hash_get(j, \"tippecanoe\");\n\t\t\tif (tippecanoe != NULL) {\n\t\t\t\tjson_object *layer = json_hash_get(tippecanoe, \"layer\");\n\t\t\t\tif (layer != NULL && layer->type == JSON_STRING) {\n\t\t\t\t\tlayername = std::string(layer->string);\n\t\t\t\t}\n\n\t\t\t\tjson_object *index = json_hash_get(tippecanoe, \"index\");\n\t\t\t\tif (index != NULL && index->type == JSON_NUMBER) {\n\t\t\t\t\tsf.index = index->number;\n\t\t\t\t}\n\n\t\t\t\tjson_object *sequence = json_hash_get(tippecanoe, \"sequence\");\n\t\t\t\tif (sequence != NULL && sequence->type == JSON_NUMBER) {\n\t\t\t\t\tsf.seq = sequence->number;\n\t\t\t\t}\n\n\t\t\t\tjson_object *extent = json_hash_get(tippecanoe, \"extent\");\n\t\t\t\tif (extent != NULL && extent->type == JSON_NUMBER) {\n\t\t\t\t\tsf.extent = extent->number;\n\t\t\t\t}\n\n\t\t\t\tjson_object *dropped = json_hash_get(tippecanoe, \"dropped\");\n\t\t\t\tif (dropped != NULL && dropped->type == JSON_TRUE) {\n\t\t\t\t\tsf.dropped = true;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor (size_t i = 0; i < dv.size(); i++) {\n\t\t\t\tif (dv[i].op == VT_MOVETO || dv[i].op == VT_LINETO) {\n\t\t\t\t\tif (dv[i].x < sf.bbox[0]) {\n\t\t\t\t\t\tsf.bbox[0] = dv[i].x;\n\t\t\t\t\t}\n\t\t\t\t\tif (dv[i].y < sf.bbox[1]) {\n\t\t\t\t\t\tsf.bbox[1] = dv[i].y;\n\t\t\t\t\t}\n\t\t\t\t\tif (dv[i].x > sf.bbox[2]) {\n\t\t\t\t\t\tsf.bbox[2] = dv[i].x;\n\t\t\t\t\t}\n\t\t\t\t\tif (dv[i].y > sf.bbox[3]) {\n\t\t\t\t\t\tsf.bbox[3] = dv[i].y;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tjson_object *id = json_hash_get(j, \"id\");\n\t\t\tif (id != NULL) {\n\t\t\t\tsf.id = atoll(id->string);\n\t\t\t\tsf.has_id = true;\n\t\t\t}\n\n\t\t\tstd::map<std::string, layermap_entry> &layermap = (*layermaps)[tiling_seg];\n\n\t\t\tif (layermap.count(layername) == 0) {\n\t\t\t\tlayermap_entry lme = layermap_entry(layermap.size());\n\t\t\t\tlme.minzoom = z;\n\t\t\t\tlme.maxzoom = z;\n\n\t\t\t\tlayermap.insert(std::pair<std::string, layermap_entry>(layername, lme));\n\n\t\t\t\tif (lme.id >= (*layer_unmaps)[tiling_seg].size()) {\n\t\t\t\t\t(*layer_unmaps)[tiling_seg].resize(lme.id + 1);\n\t\t\t\t\t(*layer_unmaps)[tiling_seg][lme.id] = layername;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tauto fk = layermap.find(layername);\n\t\t\tif (fk == layermap.end()) {\n\t\t\t\tfprintf(stderr, \"Internal error: layer %s not found\\n\", layername.c_str());\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tsf.layer = fk->second.id;\n\n\t\t\tif (z < fk->second.minzoom) {\n\t\t\t\tfk->second.minzoom = z;\n\t\t\t}\n\t\t\tif (z > fk->second.maxzoom) {\n\t\t\t\tfk->second.maxzoom = z;\n\t\t\t}\n\n\t\t\tif (!postfilter) {\n\t\t\t\tif (sf.t == mvt_point) {\n\t\t\t\t\tfk->second.points++;\n\t\t\t\t} else if (sf.t == mvt_linestring) {\n\t\t\t\t\tfk->second.lines++;\n\t\t\t\t} else if (sf.t == mvt_polygon) {\n\t\t\t\t\tfk->second.polygons++;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor (size_t i = 0; i < properties->length; i++) {\n\t\t\t\tserial_val v;\n\t\t\t\tv.type = -1;\n\n\t\t\t\tstringify_value(properties->values[i], v.type, v.s, \"Filter output\", jp->line, j);\n\n\t\t\t\t// Nulls can be excluded here because the expression evaluation filter\n\t\t\t\t// would have already run before prefiltering\n\n\t\t\t\tif (v.type >= 0 && v.type != mvt_null) {\n\t\t\t\t\tsf.full_keys.push_back(std::string(properties->keys[i]->string));\n\t\t\t\t\tsf.full_values.push_back(v);\n\n\t\t\t\t\ttype_and_string attrib;\n\t\t\t\t\tattrib.string = v.s;\n\t\t\t\t\tattrib.type = v.type;\n\n\t\t\t\t\tif (!postfilter) {\n\t\t\t\t\t\tadd_to_file_keys(fk->second.file_keys, std::string(properties->keys[i]->string), attrib);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tjson_free(j);\n\t\t\treturn sf;\n\t\t}\n\n\t\tjson_free(j);\n\t}\n}\n\nstatic pthread_mutex_t pipe_lock = PTHREAD_MUTEX_INITIALIZER;\n\nvoid setup_filter(const char *filter, int *write_to, int *read_from, pid_t *pid, unsigned z, unsigned x, unsigned y) {\n\t// This will create two pipes, a new thread, and a new process.\n\t//\n\t// The new process will read from one pipe and write to the other, and execute the filter.\n\t// The new thread will write the GeoJSON to the pipe that leads to the filter.\n\t// The original thread will read the GeoJSON from the filter and convert it back into vector tiles.\n\n\tif (pthread_mutex_lock(&pipe_lock) != 0) {\n\t\tperror(\"pthread_mutex_lock (pipe)\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tint pipe_orig[2], pipe_filtered[2];\n\tif (pipe(pipe_orig) < 0) {\n\t\tperror(\"pipe (original features)\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tif (pipe(pipe_filtered) < 0) {\n\t\tperror(\"pipe (filtered features)\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tstd::string z_str = std::to_string(z);\n\tstd::string x_str = std::to_string(x);\n\tstd::string y_str = std::to_string(y);\n\n\t*pid = fork();\n\tif (*pid < 0) {\n\t\tperror(\"fork\");\n\t\texit(EXIT_FAILURE);\n\t} else if (*pid == 0) {\n\t\t// child\n\n\t\tif (dup2(pipe_orig[0], 0) < 0) {\n\t\t\tperror(\"dup child stdin\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (dup2(pipe_filtered[1], 1) < 0) {\n\t\t\tperror(\"dup child stdout\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (close(pipe_orig[1]) != 0) {\n\t\t\tperror(\"close output to filter\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (close(pipe_filtered[0]) != 0) {\n\t\t\tperror(\"close input from filter\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (close(pipe_orig[0]) != 0) {\n\t\t\tperror(\"close dup input of filter\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (close(pipe_filtered[1]) != 0) {\n\t\t\tperror(\"close dup output of filter\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\t// XXX close other fds?\n\n\t\tif (execlp(\"sh\", \"sh\", \"-c\", filter, \"sh\", z_str.c_str(), x_str.c_str(), y_str.c_str(), NULL) != 0) {\n\t\t\tperror(\"exec\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t} else {\n\t\t// parent\n\n\t\tif (close(pipe_orig[0]) != 0) {\n\t\t\tperror(\"close filter-side reader\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (close(pipe_filtered[1]) != 0) {\n\t\t\tperror(\"close filter-side writer\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (fcntl(pipe_orig[1], F_SETFD, FD_CLOEXEC) != 0) {\n\t\t\tperror(\"cloxec output to filter\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (fcntl(pipe_filtered[0], F_SETFD, FD_CLOEXEC) != 0) {\n\t\t\tperror(\"cloxec input from filter\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tif (pthread_mutex_unlock(&pipe_lock) != 0) {\n\t\t\tperror(\"pthread_mutex_unlock (pipe_lock)\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\t*write_to = pipe_orig[1];\n\t\t*read_from = pipe_filtered[0];\n\t}\n}\n\nstd::vector<mvt_layer> filter_layers(const char *filter, std::vector<mvt_layer> &layers, unsigned z, unsigned x, unsigned y, std::vector<std::map<std::string, layermap_entry>> *layermaps, size_t tiling_seg, std::vector<std::vector<std::string>> *layer_unmaps, int extent) {\n\tint write_to, read_from;\n\tpid_t pid;\n\tsetup_filter(filter, &write_to, &read_from, &pid, z, x, y);\n\n\twriter_arg wa;\n\twa.write_to = write_to;\n\twa.layers = &layers;\n\twa.z = z;\n\twa.x = x;\n\twa.y = y;\n\twa.extent = extent;\n\n\tpthread_t writer;\n\tif (pthread_create(&writer, NULL, run_writer, &wa) != 0) {\n\t\tperror(\"pthread_create (filter writer)\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tstd::vector<mvt_layer> nlayers = parse_layers(read_from, z, x, y, layermaps, tiling_seg, layer_unmaps, extent);\n\n\twhile (1) {\n\t\tint stat_loc;\n\t\tif (waitpid(pid, &stat_loc, 0) < 0) {\n\t\t\tperror(\"waitpid for filter\\n\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t\tif (WIFEXITED(stat_loc) || WIFSIGNALED(stat_loc)) {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tvoid *ret;\n\tif (pthread_join(writer, &ret) != 0) {\n\t\tperror(\"pthread_join filter writer\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\treturn nlayers;\n}\n"
        },
        {
          "name": "plugin.hpp",
          "type": "blob",
          "size": 0.6005859375,
          "content": "std::vector<mvt_layer> filter_layers(const char *filter, std::vector<mvt_layer> &layer, unsigned z, unsigned x, unsigned y, std::vector<std::map<std::string, layermap_entry>> *layermaps, size_t tiling_seg, std::vector<std::vector<std::string>> *layer_unmaps, int extent);\nvoid setup_filter(const char *filter, int *write_to, int *read_from, pid_t *pid, unsigned z, unsigned x, unsigned y);\nserial_feature parse_feature(json_pull *jp, int z, unsigned x, unsigned y, std::vector<std::map<std::string, layermap_entry>> *layermaps, size_t tiling_seg, std::vector<std::vector<std::string>> *layer_unmaps, bool filters);\n"
        },
        {
          "name": "pool.cpp",
          "type": "blob",
          "size": 2.779296875,
          "content": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <limits.h>\n#include <math.h>\n#include \"memfile.hpp\"\n#include \"pool.hpp\"\n\nint swizzlecmp(const char *a, const char *b) {\n\tssize_t alen = strlen(a);\n\tssize_t blen = strlen(b);\n\n\tif (strcmp(a, b) == 0) {\n\t\treturn 0;\n\t}\n\n\tlong long hash1 = 0, hash2 = 0;\n\tfor (ssize_t i = alen - 1; i >= 0; i--) {\n\t\thash1 = (hash1 * 37 + a[i]) & INT_MAX;\n\t}\n\tfor (ssize_t i = blen - 1; i >= 0; i--) {\n\t\thash2 = (hash2 * 37 + b[i]) & INT_MAX;\n\t}\n\n\tint h1 = hash1, h2 = hash2;\n\tif (h1 == h2) {\n\t\treturn strcmp(a, b);\n\t}\n\n\treturn h1 - h2;\n}\n\nlong long addpool(struct memfile *poolfile, struct memfile *treefile, const char *s, char type) {\n\tunsigned long *sp = &treefile->tree;\n\tsize_t depth = 0;\n\n\t// In typical data, traversal depth generally stays under 2.5x\n\tsize_t max = 3 * log(treefile->off / sizeof(struct stringpool)) / log(2);\n\tif (max < 30) {\n\t\tmax = 30;\n\t}\n\n\twhile (*sp != 0) {\n\t\tint cmp = swizzlecmp(s, poolfile->map + ((struct stringpool *) (treefile->map + *sp))->off + 1);\n\n\t\tif (cmp == 0) {\n\t\t\tcmp = type - (poolfile->map + ((struct stringpool *) (treefile->map + *sp))->off)[0];\n\t\t}\n\n\t\tif (cmp < 0) {\n\t\t\tsp = &(((struct stringpool *) (treefile->map + *sp))->left);\n\t\t} else if (cmp > 0) {\n\t\t\tsp = &(((struct stringpool *) (treefile->map + *sp))->right);\n\t\t} else {\n\t\t\treturn ((struct stringpool *) (treefile->map + *sp))->off;\n\t\t}\n\n\t\tdepth++;\n\t\tif (depth > max) {\n\t\t\t// Search is very deep, so string is probably unique.\n\t\t\t// Add it to the pool without adding it to the search tree.\n\n\t\t\tlong long off = poolfile->off;\n\t\t\tif (memfile_write(poolfile, &type, 1) < 0) {\n\t\t\t\tperror(\"memfile write\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tif (memfile_write(poolfile, (void *) s, strlen(s) + 1) < 0) {\n\t\t\t\tperror(\"memfile write\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\treturn off;\n\t\t}\n\t}\n\n\t// *sp is probably in the memory-mapped file, and will move if the file grows.\n\tlong long ssp;\n\tif (sp == &treefile->tree) {\n\t\tssp = -1;\n\t} else {\n\t\tssp = ((char *) sp) - treefile->map;\n\t}\n\n\tlong long off = poolfile->off;\n\tif (memfile_write(poolfile, &type, 1) < 0) {\n\t\tperror(\"memfile write\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tif (memfile_write(poolfile, (void *) s, strlen(s) + 1) < 0) {\n\t\tperror(\"memfile write\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (off >= LONG_MAX || treefile->off >= LONG_MAX) {\n\t\t// Tree or pool is bigger than 2GB\n\t\tstatic bool warned = false;\n\t\tif (!warned) {\n\t\t\tfprintf(stderr, \"Warning: string pool is very large.\\n\");\n\t\t\twarned = true;\n\t\t}\n\t\treturn off;\n\t}\n\n\tstruct stringpool tsp;\n\ttsp.left = 0;\n\ttsp.right = 0;\n\ttsp.off = off;\n\n\tlong long p = treefile->off;\n\tif (memfile_write(treefile, &tsp, sizeof(struct stringpool)) < 0) {\n\t\tperror(\"memfile write\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (ssp == -1) {\n\t\ttreefile->tree = p;\n\t} else {\n\t\t*((long long *) (treefile->map + ssp)) = p;\n\t}\n\treturn off;\n}\n"
        },
        {
          "name": "pool.hpp",
          "type": "blob",
          "size": 0.2333984375,
          "content": "#ifndef POOL_HPP\n#define POOL_HPP\n\nstruct stringpool {\n\tunsigned long left = 0;\n\tunsigned long right = 0;\n\tunsigned long off = 0;\n};\n\nlong long addpool(struct memfile *poolfile, struct memfile *treefile, const char *s, char type);\n\n#endif\n"
        },
        {
          "name": "projection.cpp",
          "type": "blob",
          "size": 5.330078125,
          "content": "#include <stdio.h>\n#include <string.h>\n#include <stdlib.h>\n#include <math.h>\n#include <atomic>\n#include \"projection.hpp\"\n\nunsigned long long (*encode_index)(unsigned int wx, unsigned int wy) = NULL;\nvoid (*decode_index)(unsigned long long index, unsigned *wx, unsigned *wy) = NULL;\n\nstruct projection projections[] = {\n\t{\"EPSG:4326\", lonlat2tile, tile2lonlat, \"urn:ogc:def:crs:OGC:1.3:CRS84\"},\n\t{\"EPSG:3857\", epsg3857totile, tiletoepsg3857, \"urn:ogc:def:crs:EPSG::3857\"},\n\t{NULL, NULL, NULL, NULL},\n};\n\nstruct projection *projection = &projections[0];\n\n// http://wiki.openstreetmap.org/wiki/Slippy_map_tilenames\nvoid lonlat2tile(double lon, double lat, int zoom, long long *x, long long *y) {\n\t// Place infinite and NaN coordinates off the edge of the Mercator plane\n\n\tint lat_class = fpclassify(lat);\n\tint lon_class = fpclassify(lon);\n\tbool bad_lon = false;\n\n\tif (lat_class == FP_INFINITE || lat_class == FP_NAN) {\n\t\tlat = 89.9;\n\t}\n\tif (lon_class == FP_INFINITE || lon_class == FP_NAN) {\n\t\t// Keep these far enough from the plane that they don't get\n\t\t// moved back into it by 360-degree offsetting\n\n\t\tlon = 720;\n\t\tbad_lon = true;\n\t}\n\n\t// Must limit latitude somewhere to prevent overflow.\n\t// 89.9 degrees latitude is 0.621 worlds beyond the edge of the flat earth,\n\t// hopefully far enough out that there are few expectations about the shape.\n\tif (lat < -89.9) {\n\t\tlat = -89.9;\n\t}\n\tif (lat > 89.9) {\n\t\tlat = 89.9;\n\t}\n\n\tif (lon < -360 && !bad_lon) {\n\t\tlon = -360;\n\t}\n\tif (lon > 360 && !bad_lon) {\n\t\tlon = 360;\n\t}\n\n\tdouble lat_rad = lat * M_PI / 180;\n\tunsigned long long n = 1LL << zoom;\n\n\tlong long llx = n * ((lon + 180) / 360);\n\tlong long lly = n * (1 - (log(tan(lat_rad) + 1 / cos(lat_rad)) / M_PI)) / 2;\n\n\t*x = llx;\n\t*y = lly;\n}\n\n// http://wiki.openstreetmap.org/wiki/Slippy_map_tilenames\nvoid tile2lonlat(long long x, long long y, int zoom, double *lon, double *lat) {\n\tunsigned long long n = 1LL << zoom;\n\t*lon = 360.0 * x / n - 180.0;\n\t*lat = atan(sinh(M_PI * (1 - 2.0 * y / n))) * 180.0 / M_PI;\n}\n\nvoid epsg3857totile(double ix, double iy, int zoom, long long *x, long long *y) {\n\t// Place infinite and NaN coordinates off the edge of the Mercator plane\n\n\tint iy_class = fpclassify(iy);\n\tint ix_class = fpclassify(ix);\n\n\tif (iy_class == FP_INFINITE || iy_class == FP_NAN) {\n\t\tiy = 40000000.0;\n\t}\n\tif (ix_class == FP_INFINITE || ix_class == FP_NAN) {\n\t\tix = 40000000.0;\n\t}\n\n\t*x = ix * (1LL << 31) / 6378137.0 / M_PI + (1LL << 31);\n\t*y = ((1LL << 32) - 1) - (iy * (1LL << 31) / 6378137.0 / M_PI + (1LL << 31));\n\n\tif (zoom != 0) {\n\t\t*x >>= (32 - zoom);\n\t\t*y >>= (32 - zoom);\n\t}\n}\n\nvoid tiletoepsg3857(long long ix, long long iy, int zoom, double *ox, double *oy) {\n\tif (zoom != 0) {\n\t\tix <<= (32 - zoom);\n\t\tiy <<= (32 - zoom);\n\t}\n\n\t*ox = (ix - (1LL << 31)) * M_PI * 6378137.0 / (1LL << 31);\n\t*oy = ((1LL << 32) - 1 - iy - (1LL << 31)) * M_PI * 6378137.0 / (1LL << 31);\n}\n\n// https://en.wikipedia.org/wiki/Hilbert_curve\n\nvoid hilbert_rot(unsigned long long n, unsigned *x, unsigned *y, unsigned long long rx, unsigned long long ry) {\n\tif (ry == 0) {\n\t\tif (rx == 1) {\n\t\t\t*x = n - 1 - *x;\n\t\t\t*y = n - 1 - *y;\n\t\t}\n\n\t\tunsigned t = *x;\n\t\t*x = *y;\n\t\t*y = t;\n\t}\n}\n\nunsigned long long hilbert_xy2d(unsigned long long n, unsigned x, unsigned y) {\n\tunsigned long long d = 0;\n\tunsigned long long rx, ry;\n\n\tfor (unsigned long long s = n / 2; s > 0; s /= 2) {\n\t\trx = (x & s) != 0;\n\t\try = (y & s) != 0;\n\n\t\td += s * s * ((3 * rx) ^ ry);\n\t\thilbert_rot(s, &x, &y, rx, ry);\n\t}\n\n\treturn d;\n}\n\nvoid hilbert_d2xy(unsigned long long n, unsigned long long d, unsigned *x, unsigned *y) {\n\tunsigned long long rx, ry;\n\tunsigned long long t = d;\n\n\t*x = *y = 0;\n\tfor (unsigned long long s = 1; s < n; s *= 2) {\n\t\trx = 1 & (t / 2);\n\t\try = 1 & (t ^ rx);\n\t\thilbert_rot(s, x, y, rx, ry);\n\t\t*x += s * rx;\n\t\t*y += s * ry;\n\t\tt /= 4;\n\t}\n}\n\nunsigned long long encode_hilbert(unsigned int wx, unsigned int wy) {\n\treturn hilbert_xy2d(1LL << 32, wx, wy);\n}\n\nvoid decode_hilbert(unsigned long long index, unsigned *wx, unsigned *wy) {\n\thilbert_d2xy(1LL << 32, index, wx, wy);\n}\n\nunsigned long long encode_quadkey(unsigned int wx, unsigned int wy) {\n\tunsigned long long out = 0;\n\n\tint i;\n\tfor (i = 0; i < 32; i++) {\n\t\tunsigned long long v = ((wx >> (32 - (i + 1))) & 1) << 1;\n\t\tv |= (wy >> (32 - (i + 1))) & 1;\n\t\tv = v << (64 - 2 * (i + 1));\n\n\t\tout |= v;\n\t}\n\n\treturn out;\n}\n\nstatic std::atomic<unsigned char> decodex[256];\nstatic std::atomic<unsigned char> decodey[256];\n\nvoid decode_quadkey(unsigned long long index, unsigned *wx, unsigned *wy) {\n\tstatic std::atomic<int> initialized(0);\n\tif (!initialized) {\n\t\tfor (size_t ix = 0; ix < 256; ix++) {\n\t\t\tsize_t xx = 0, yy = 0;\n\n\t\t\tfor (size_t i = 0; i < 32; i++) {\n\t\t\t\txx |= ((ix >> (64 - 2 * (i + 1) + 1)) & 1) << (32 - (i + 1));\n\t\t\t\tyy |= ((ix >> (64 - 2 * (i + 1) + 0)) & 1) << (32 - (i + 1));\n\t\t\t}\n\n\t\t\tdecodex[ix] = xx;\n\t\t\tdecodey[ix] = yy;\n\t\t}\n\n\t\tinitialized = 1;\n\t}\n\n\t*wx = *wy = 0;\n\n\tfor (size_t i = 0; i < 8; i++) {\n\t\t*wx |= ((unsigned) decodex[(index >> (8 * i)) & 0xFF]) << (4 * i);\n\t\t*wy |= ((unsigned) decodey[(index >> (8 * i)) & 0xFF]) << (4 * i);\n\t}\n}\n\nvoid set_projection_or_exit(const char *optarg) {\n\tstruct projection *p;\n\tfor (p = projections; p->name != NULL; p++) {\n\t\tif (strcmp(p->name, optarg) == 0) {\n\t\t\tprojection = p;\n\t\t\tbreak;\n\t\t}\n\t\tif (strcmp(p->alias, optarg) == 0) {\n\t\t\tprojection = p;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (p->name == NULL) {\n\t\tfprintf(stderr, \"Unknown projection (-s): %s\\n\", optarg);\n\t\texit(EXIT_FAILURE);\n\t}\n}\n"
        },
        {
          "name": "projection.hpp",
          "type": "blob",
          "size": 1.1533203125,
          "content": "#ifndef PROJECTION_HPP\n#define PROJECTION_HPP\n\nvoid lonlat2tile(double lon, double lat, int zoom, long long *x, long long *y);\nvoid epsg3857totile(double ix, double iy, int zoom, long long *x, long long *y);\nvoid tile2lonlat(long long x, long long y, int zoom, double *lon, double *lat);\nvoid tiletoepsg3857(long long x, long long y, int zoom, double *ox, double *oy);\nvoid set_projection_or_exit(const char *optarg);\n\nstruct projection {\n\tconst char *name;\n\tvoid (*project)(double ix, double iy, int zoom, long long *ox, long long *oy);\n\tvoid (*unproject)(long long ix, long long iy, int zoom, double *ox, double *oy);\n\tconst char *alias;\n};\n\nextern struct projection *projection;\nextern struct projection projections[];\n\nextern unsigned long long (*encode_index)(unsigned int wx, unsigned int wy);\nextern void (*decode_index)(unsigned long long index, unsigned *wx, unsigned *wy);\n\nunsigned long long encode_quadkey(unsigned int wx, unsigned int wy);\nvoid decode_quadkey(unsigned long long index, unsigned *wx, unsigned *wy);\n\nunsigned long long encode_hilbert(unsigned int wx, unsigned int wy);\nvoid decode_hilbert(unsigned long long index, unsigned *wx, unsigned *wy);\n\n#endif\n"
        },
        {
          "name": "protozero",
          "type": "tree",
          "content": null
        },
        {
          "name": "read_json.cpp",
          "type": "blob",
          "size": 4.587890625,
          "content": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <vector>\n#include <string>\n#include <map>\n#include \"jsonpull/jsonpull.h\"\n#include \"geometry.hpp\"\n#include \"projection.hpp\"\n#include \"read_json.hpp\"\n#include \"text.hpp\"\n#include \"mvt.hpp\"\n#include \"milo/dtoa_milo.h\"\n\nconst char *geometry_names[GEOM_TYPES] = {\n\t\"Point\", \"MultiPoint\", \"LineString\", \"MultiLineString\", \"Polygon\", \"MultiPolygon\",\n};\n\nint geometry_within[GEOM_TYPES] = {\n\t-1,\t\t /* point */\n\tGEOM_POINT,      /* multipoint */\n\tGEOM_POINT,      /* linestring */\n\tGEOM_LINESTRING, /* multilinestring */\n\tGEOM_LINESTRING, /* polygon */\n\tGEOM_POLYGON,    /* multipolygon */\n};\n\nint mb_geometry[GEOM_TYPES] = {\n\tVT_POINT, VT_POINT, VT_LINE, VT_LINE, VT_POLYGON, VT_POLYGON,\n};\n\nvoid json_context(json_object *j) {\n\tchar *s = json_stringify(j);\n\n\tif (strlen(s) >= 500) {\n\t\tsprintf(s + 497, \"...\");\n\t}\n\n\tfprintf(stderr, \"In JSON object %s\\n\", s);\n\tfree(s);  // stringify\n}\n\nvoid parse_geometry(int t, json_object *j, drawvec &out, int op, const char *fname, int line, json_object *feature) {\n\tif (j == NULL || j->type != JSON_ARRAY) {\n\t\tfprintf(stderr, \"%s:%d: expected array for type %d\\n\", fname, line, t);\n\t\tjson_context(feature);\n\t\treturn;\n\t}\n\n\tint within = geometry_within[t];\n\tif (within >= 0) {\n\t\tsize_t i;\n\t\tfor (i = 0; i < j->length; i++) {\n\t\t\tif (within == GEOM_POINT) {\n\t\t\t\tif (i == 0 || mb_geometry[t] == VT_POINT) {\n\t\t\t\t\top = VT_MOVETO;\n\t\t\t\t} else {\n\t\t\t\t\top = VT_LINETO;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tparse_geometry(within, j->array[i], out, op, fname, line, feature);\n\t\t}\n\t} else {\n\t\tif (j->length >= 2 && j->array[0]->type == JSON_NUMBER && j->array[1]->type == JSON_NUMBER) {\n\t\t\tlong long x, y;\n\t\t\tdouble lon = j->array[0]->number;\n\t\t\tdouble lat = j->array[1]->number;\n\t\t\tprojection->project(lon, lat, 32, &x, &y);\n\n\t\t\tif (j->length > 2) {\n\t\t\t\tstatic int warned = 0;\n\n\t\t\t\tif (!warned) {\n\t\t\t\t\tfprintf(stderr, \"%s:%d: ignoring dimensions beyond two\\n\", fname, line);\n\t\t\t\t\tjson_context(j);\n\t\t\t\t\tjson_context(feature);\n\t\t\t\t\twarned = 1;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tout.push_back(draw(op, x, y));\n\t\t} else {\n\t\t\tfprintf(stderr, \"%s:%d: malformed point\\n\", fname, line);\n\t\t\tjson_context(j);\n\t\t\tjson_context(feature);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\tif (t == GEOM_POLYGON) {\n\t\t// Note that this is not using the correct meaning of closepath.\n\t\t//\n\t\t// We are using it here to close an entire Polygon, to distinguish\n\t\t// the Polygons within a MultiPolygon from each other.\n\t\t//\n\t\t// This will be undone in fix_polygon(), which needs to know which\n\t\t// rings come from which Polygons so that it can make the winding order\n\t\t// of the outer ring be the opposite of the order of the inner rings.\n\n\t\tout.push_back(draw(VT_CLOSEPATH, 0, 0));\n\t}\n}\n\nvoid canonicalize(json_object *o) {\n\tif (o->type == JSON_NUMBER) {\n\t\tstd::string s;\n\t\tlong long v;\n\t\tunsigned long long uv;\n\n\t\tif (is_integer(o->string, &v)) {\n\t\t\ts = std::to_string(v);\n\t\t} else if (is_unsigned_integer(o->string, &uv)) {\n\t\t\ts = std::to_string(uv);\n\t\t} else {\n\t\t\ts = milo::dtoa_milo(o->number);\n\t\t}\n\t\tfree(o->string);\n\t\to->string = strdup(s.c_str());\n\t} else if (o->type == JSON_HASH) {\n\t\tfor (size_t i = 0; i < o->length; i++) {\n\t\t\tcanonicalize(o->values[i]);\n\t\t}\n\t} else if (o->type == JSON_ARRAY) {\n\t\tfor (size_t i = 0; i < o->length; i++) {\n\t\t\tcanonicalize(o->array[i]);\n\t\t}\n\t}\n}\n\nvoid stringify_value(json_object *value, int &type, std::string &stringified, const char *reading, int line, json_object *feature) {\n\tif (value != NULL) {\n\t\tint vt = value->type;\n\t\tstd::string val;\n\n\t\tif (vt == JSON_STRING || vt == JSON_NUMBER) {\n\t\t\tval = value->string;\n\t\t} else if (vt == JSON_TRUE) {\n\t\t\tval = \"true\";\n\t\t} else if (vt == JSON_FALSE) {\n\t\t\tval = \"false\";\n\t\t} else if (vt == JSON_NULL) {\n\t\t\tval = \"null\";\n\t\t} else {\n\t\t\tcanonicalize(value);\n\t\t\tconst char *v = json_stringify(value);\n\t\t\tval = std::string(v);\n\t\t\tfree((void *) v);  // stringify\n\t\t}\n\n\t\tif (vt == JSON_STRING) {\n\t\t\ttype = mvt_string;\n\t\t\tstringified = val;\n\t\t\tstd::string err = check_utf8(val);\n\t\t\tif (err != \"\") {\n\t\t\t\tfprintf(stderr, \"%s:%d: %s\\n\", reading, line, err.c_str());\n\t\t\t\tjson_context(feature);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t} else if (vt == JSON_NUMBER) {\n\t\t\ttype = mvt_double;\n\n\t\t\tlong long v;\n\t\t\tunsigned long long uv;\n\n\t\t\tif (is_integer(value->string, &v)) {\n\t\t\t\tstringified = std::to_string(v);\n\t\t\t} else if (is_unsigned_integer(value->string, &uv)) {\n\t\t\t\tstringified = std::to_string(uv);\n\t\t\t} else {\n\t\t\t\tstringified = milo::dtoa_milo(value->number);\n\t\t\t}\n\t\t} else if (vt == JSON_TRUE || vt == JSON_FALSE) {\n\t\t\ttype = mvt_bool;\n\t\t\tstringified = val;\n\t\t} else if (vt == JSON_NULL) {\n\t\t\ttype = mvt_null;\n\t\t\tstringified = \"null\";\n\t\t} else {\n\t\t\ttype = mvt_string;\n\t\t\tstringified = val;\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "read_json.hpp",
          "type": "blob",
          "size": 0.8232421875,
          "content": "#define GEOM_POINT 0\t   /* array of positions */\n#define GEOM_MULTIPOINT 1      /* array of arrays of positions */\n#define GEOM_LINESTRING 2      /* array of arrays of positions */\n#define GEOM_MULTILINESTRING 3 /* array of arrays of arrays of positions */\n#define GEOM_POLYGON 4\t /* array of arrays of arrays of positions */\n#define GEOM_MULTIPOLYGON 5    /* array of arrays of arrays of arrays of positions */\n#define GEOM_TYPES 6\n\nextern const char *geometry_names[GEOM_TYPES];\nextern int geometry_within[GEOM_TYPES];\nextern int mb_geometry[GEOM_TYPES];\n\nvoid json_context(json_object *j);\nvoid parse_geometry(int t, json_object *j, drawvec &out, int op, const char *fname, int line, json_object *feature);\n\nvoid stringify_value(json_object *value, int &type, std::string &stringified, const char *reading, int line, json_object *feature);\n"
        },
        {
          "name": "serial.cpp",
          "type": "blob",
          "size": 20.60546875,
          "content": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <errno.h>\n#include <string>\n#include <vector>\n#include <sqlite3.h>\n#include <set>\n#include <map>\n#include <algorithm>\n#include <limits.h>\n#include \"protozero/varint.hpp\"\n#include \"geometry.hpp\"\n#include \"mbtiles.hpp\"\n#include \"tile.hpp\"\n#include \"serial.hpp\"\n#include \"options.hpp\"\n#include \"main.hpp\"\n#include \"pool.hpp\"\n#include \"projection.hpp\"\n#include \"evaluator.hpp\"\n#include \"milo/dtoa_milo.h\"\n\n// Offset coordinates to keep them positive\n#define COORD_OFFSET (4LL << 32)\n#define SHIFT_RIGHT(a) ((((a) + COORD_OFFSET) >> geometry_scale) - (COORD_OFFSET >> geometry_scale))\n#define SHIFT_LEFT(a) ((((a) + (COORD_OFFSET >> geometry_scale)) << geometry_scale) - COORD_OFFSET)\n\nsize_t fwrite_check(const void *ptr, size_t size, size_t nitems, FILE *stream, const char *fname) {\n\tsize_t w = fwrite(ptr, size, nitems, stream);\n\tif (w != nitems) {\n\t\tfprintf(stderr, \"%s: Write to temporary file failed: %s\\n\", fname, strerror(errno));\n\t\texit(EXIT_FAILURE);\n\t}\n\treturn w;\n}\n\nvoid serialize_int(FILE *out, int n, std::atomic<long long> *fpos, const char *fname) {\n\tserialize_long_long(out, n, fpos, fname);\n}\n\nvoid serialize_long_long(FILE *out, long long n, std::atomic<long long> *fpos, const char *fname) {\n\tunsigned long long zigzag = protozero::encode_zigzag64(n);\n\n\tserialize_ulong_long(out, zigzag, fpos, fname);\n}\n\nvoid serialize_ulong_long(FILE *out, unsigned long long zigzag, std::atomic<long long> *fpos, const char *fname) {\n\twhile (1) {\n\t\tunsigned char b = zigzag & 0x7F;\n\t\tif ((zigzag >> 7) != 0) {\n\t\t\tb |= 0x80;\n\t\t\tif (putc(b, out) == EOF) {\n\t\t\t\tfprintf(stderr, \"%s: Write to temporary file failed: %s\\n\", fname, strerror(errno));\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\t*fpos += 1;\n\t\t\tzigzag >>= 7;\n\t\t} else {\n\t\t\tif (putc(b, out) == EOF) {\n\t\t\t\tfprintf(stderr, \"%s: Write to temporary file failed: %s\\n\", fname, strerror(errno));\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\t*fpos += 1;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nvoid serialize_byte(FILE *out, signed char n, std::atomic<long long> *fpos, const char *fname) {\n\tfwrite_check(&n, sizeof(signed char), 1, out, fname);\n\t*fpos += sizeof(signed char);\n}\n\nvoid serialize_uint(FILE *out, unsigned n, std::atomic<long long> *fpos, const char *fname) {\n\tfwrite_check(&n, sizeof(unsigned), 1, out, fname);\n\t*fpos += sizeof(unsigned);\n}\n\nvoid deserialize_int(char **f, int *n) {\n\tlong long ll;\n\tdeserialize_long_long(f, &ll);\n\t*n = ll;\n}\n\nvoid deserialize_long_long(char **f, long long *n) {\n\tunsigned long long zigzag = 0;\n\tdeserialize_ulong_long(f, &zigzag);\n\t*n = protozero::decode_zigzag64(zigzag);\n}\n\nvoid deserialize_ulong_long(char **f, unsigned long long *zigzag) {\n\t*zigzag = 0;\n\tint shift = 0;\n\n\twhile (1) {\n\t\tif ((**f & 0x80) == 0) {\n\t\t\t*zigzag |= ((unsigned long long) **f) << shift;\n\t\t\t*f += 1;\n\t\t\tshift += 7;\n\t\t\tbreak;\n\t\t} else {\n\t\t\t*zigzag |= ((unsigned long long) (**f & 0x7F)) << shift;\n\t\t\t*f += 1;\n\t\t\tshift += 7;\n\t\t}\n\t}\n}\n\nvoid deserialize_uint(char **f, unsigned *n) {\n\tmemcpy(n, *f, sizeof(unsigned));\n\t*f += sizeof(unsigned);\n}\n\nvoid deserialize_byte(char **f, signed char *n) {\n\tmemcpy(n, *f, sizeof(signed char));\n\t*f += sizeof(signed char);\n}\n\nint deserialize_long_long_io(FILE *f, long long *n, std::atomic<long long> *geompos) {\n\tunsigned long long zigzag = 0;\n\tint ret = deserialize_ulong_long_io(f, &zigzag, geompos);\n\t*n = protozero::decode_zigzag64(zigzag);\n\treturn ret;\n}\n\nint deserialize_ulong_long_io(FILE *f, unsigned long long *zigzag, std::atomic<long long> *geompos) {\n\t*zigzag = 0;\n\tint shift = 0;\n\n\twhile (1) {\n\t\tint c = getc(f);\n\t\tif (c == EOF) {\n\t\t\treturn 0;\n\t\t}\n\t\t(*geompos)++;\n\n\t\tif ((c & 0x80) == 0) {\n\t\t\t*zigzag |= ((unsigned long long) c) << shift;\n\t\t\tshift += 7;\n\t\t\tbreak;\n\t\t} else {\n\t\t\t*zigzag |= ((unsigned long long) (c & 0x7F)) << shift;\n\t\t\tshift += 7;\n\t\t}\n\t}\n\n\treturn 1;\n}\n\nint deserialize_int_io(FILE *f, int *n, std::atomic<long long> *geompos) {\n\tlong long ll = 0;\n\tint ret = deserialize_long_long_io(f, &ll, geompos);\n\t*n = ll;\n\treturn ret;\n}\n\nint deserialize_uint_io(FILE *f, unsigned *n, std::atomic<long long> *geompos) {\n\tif (fread(n, sizeof(unsigned), 1, f) != 1) {\n\t\treturn 0;\n\t}\n\t*geompos += sizeof(unsigned);\n\treturn 1;\n}\n\nint deserialize_byte_io(FILE *f, signed char *n, std::atomic<long long> *geompos) {\n\tint c = getc(f);\n\tif (c == EOF) {\n\t\treturn 0;\n\t}\n\t*n = c;\n\t(*geompos)++;\n\treturn 1;\n}\n\nstatic void write_geometry(drawvec const &dv, std::atomic<long long> *fpos, FILE *out, const char *fname, long long wx, long long wy) {\n\tfor (size_t i = 0; i < dv.size(); i++) {\n\t\tif (dv[i].op == VT_MOVETO || dv[i].op == VT_LINETO) {\n\t\t\tserialize_byte(out, dv[i].op, fpos, fname);\n\t\t\tserialize_long_long(out, dv[i].x - wx, fpos, fname);\n\t\t\tserialize_long_long(out, dv[i].y - wy, fpos, fname);\n\t\t\twx = dv[i].x;\n\t\t\twy = dv[i].y;\n\t\t} else {\n\t\t\tserialize_byte(out, dv[i].op, fpos, fname);\n\t\t}\n\t}\n}\n\n// called from generating the next zoom level\nvoid serialize_feature(FILE *geomfile, serial_feature *sf, std::atomic<long long> *geompos, const char *fname, long long wx, long long wy, bool include_minzoom) {\n\tserialize_byte(geomfile, sf->t, geompos, fname);\n\n\tlong long layer = 0;\n\tlayer |= sf->layer << 6;\n\tlayer |= (sf->seq != 0) << 5;\n\tlayer |= (sf->index != 0) << 4;\n\tlayer |= (sf->extent != 0) << 3;\n\tlayer |= sf->has_id << 2;\n\tlayer |= sf->has_tippecanoe_minzoom << 1;\n\tlayer |= sf->has_tippecanoe_maxzoom << 0;\n\n\tserialize_long_long(geomfile, layer, geompos, fname);\n\tif (sf->seq != 0) {\n\t\tserialize_long_long(geomfile, sf->seq, geompos, fname);\n\t}\n\tif (sf->has_tippecanoe_minzoom) {\n\t\tserialize_int(geomfile, sf->tippecanoe_minzoom, geompos, fname);\n\t}\n\tif (sf->has_tippecanoe_maxzoom) {\n\t\tserialize_int(geomfile, sf->tippecanoe_maxzoom, geompos, fname);\n\t}\n\tif (sf->has_id) {\n\t\tserialize_ulong_long(geomfile, sf->id, geompos, fname);\n\t}\n\n\tserialize_int(geomfile, sf->segment, geompos, fname);\n\n\twrite_geometry(sf->geometry, geompos, geomfile, fname, wx, wy);\n\tserialize_byte(geomfile, VT_END, geompos, fname);\n\tif (sf->index != 0) {\n\t\tserialize_ulong_long(geomfile, sf->index, geompos, fname);\n\t}\n\tif (sf->extent != 0) {\n\t\tserialize_long_long(geomfile, sf->extent, geompos, fname);\n\t}\n\n\tserialize_long_long(geomfile, sf->metapos, geompos, fname);\n\n\tif (sf->metapos < 0) {\n\t\tserialize_long_long(geomfile, sf->keys.size(), geompos, fname);\n\n\t\tfor (size_t i = 0; i < sf->keys.size(); i++) {\n\t\t\tserialize_long_long(geomfile, sf->keys[i], geompos, fname);\n\t\t\tserialize_long_long(geomfile, sf->values[i], geompos, fname);\n\t\t}\n\t}\n\n\tif (include_minzoom) {\n\t\tserialize_byte(geomfile, sf->feature_minzoom, geompos, fname);\n\t}\n}\n\nserial_feature deserialize_feature(FILE *geoms, std::atomic<long long> *geompos_in, char *metabase, long long *meta_off, unsigned z, unsigned tx, unsigned ty, unsigned *initial_x, unsigned *initial_y) {\n\tserial_feature sf;\n\n\tdeserialize_byte_io(geoms, &sf.t, geompos_in);\n\tif (sf.t < 0) {\n\t\treturn sf;\n\t}\n\n\tdeserialize_long_long_io(geoms, &sf.layer, geompos_in);\n\n\tsf.seq = 0;\n\tif (sf.layer & (1 << 5)) {\n\t\tdeserialize_long_long_io(geoms, &sf.seq, geompos_in);\n\t}\n\n\tsf.tippecanoe_minzoom = -1;\n\tsf.tippecanoe_maxzoom = -1;\n\tsf.id = 0;\n\tsf.has_id = false;\n\tif (sf.layer & (1 << 1)) {\n\t\tdeserialize_int_io(geoms, &sf.tippecanoe_minzoom, geompos_in);\n\t}\n\tif (sf.layer & (1 << 0)) {\n\t\tdeserialize_int_io(geoms, &sf.tippecanoe_maxzoom, geompos_in);\n\t}\n\tif (sf.layer & (1 << 2)) {\n\t\tsf.has_id = true;\n\t\tdeserialize_ulong_long_io(geoms, &sf.id, geompos_in);\n\t}\n\n\tdeserialize_int_io(geoms, &sf.segment, geompos_in);\n\n\tsf.index = 0;\n\tsf.extent = 0;\n\n\tsf.geometry = decode_geometry(geoms, geompos_in, z, tx, ty, sf.bbox, initial_x[sf.segment], initial_y[sf.segment]);\n\tif (sf.layer & (1 << 4)) {\n\t\tdeserialize_ulong_long_io(geoms, &sf.index, geompos_in);\n\t}\n\tif (sf.layer & (1 << 3)) {\n\t\tdeserialize_long_long_io(geoms, &sf.extent, geompos_in);\n\t}\n\n\tsf.layer >>= 6;\n\n\tsf.metapos = 0;\n\tdeserialize_long_long_io(geoms, &sf.metapos, geompos_in);\n\n\tif (sf.metapos >= 0) {\n\t\tchar *meta = metabase + sf.metapos + meta_off[sf.segment];\n\t\tlong long count;\n\t\tdeserialize_long_long(&meta, &count);\n\n\t\tfor (long long i = 0; i < count; i++) {\n\t\t\tlong long k, v;\n\t\t\tdeserialize_long_long(&meta, &k);\n\t\t\tdeserialize_long_long(&meta, &v);\n\t\t\tsf.keys.push_back(k);\n\t\t\tsf.values.push_back(v);\n\t\t}\n\t} else {\n\t\tlong long count;\n\t\tdeserialize_long_long_io(geoms, &count, geompos_in);\n\n\t\tfor (long long i = 0; i < count; i++) {\n\t\t\tlong long k, v;\n\t\t\tdeserialize_long_long_io(geoms, &k, geompos_in);\n\t\t\tdeserialize_long_long_io(geoms, &v, geompos_in);\n\t\t\tsf.keys.push_back(k);\n\t\t\tsf.values.push_back(v);\n\t\t}\n\t}\n\n\tdeserialize_byte_io(geoms, &sf.feature_minzoom, geompos_in);\n\n\treturn sf;\n}\n\nstatic long long scale_geometry(struct serialization_state *sst, long long *bbox, drawvec &geom) {\n\tlong long offset = 0;\n\tlong long prev = 0;\n\tbool has_prev = false;\n\tdouble scale = 1.0 / (1 << geometry_scale);\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_MOVETO || geom[i].op == VT_LINETO) {\n\t\t\tlong long x = geom[i].x;\n\t\t\tlong long y = geom[i].y;\n\n\t\t\tif (additional[A_DETECT_WRAPAROUND]) {\n\t\t\t\tx += offset;\n\t\t\t\tif (has_prev) {\n\t\t\t\t\tif (x - prev > (1LL << 31)) {\n\t\t\t\t\t\toffset -= 1LL << 32;\n\t\t\t\t\t\tx -= 1LL << 32;\n\t\t\t\t\t} else if (prev - x > (1LL << 31)) {\n\t\t\t\t\t\toffset += 1LL << 32;\n\t\t\t\t\t\tx += 1LL << 32;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\thas_prev = true;\n\t\t\t\tprev = x;\n\t\t\t}\n\n\t\t\tif (x < bbox[0]) {\n\t\t\t\tbbox[0] = x;\n\t\t\t}\n\t\t\tif (y < bbox[1]) {\n\t\t\t\tbbox[1] = y;\n\t\t\t}\n\t\t\tif (x > bbox[2]) {\n\t\t\t\tbbox[2] = x;\n\t\t\t}\n\t\t\tif (y > bbox[3]) {\n\t\t\t\tbbox[3] = y;\n\t\t\t}\n\n\t\t\tif (!*(sst->initialized)) {\n\t\t\t\tif (x < 0 || x >= (1LL << 32) || y < 0 || y >= (1LL << 32)) {\n\t\t\t\t\t*(sst->initial_x) = 1LL << 31;\n\t\t\t\t\t*(sst->initial_y) = 1LL << 31;\n\t\t\t\t} else {\n\t\t\t\t\t*(sst->initial_x) = (((x + COORD_OFFSET) >> geometry_scale) << geometry_scale) - COORD_OFFSET;\n\t\t\t\t\t*(sst->initial_y) = (((y + COORD_OFFSET) >> geometry_scale) << geometry_scale) - COORD_OFFSET;\n\t\t\t\t}\n\n\t\t\t\t*(sst->initialized) = 1;\n\t\t\t}\n\n\t\t\tif (additional[A_GRID_LOW_ZOOMS]) {\n\t\t\t\t// If we are gridding, snap to the maxzoom grid in case the incoming data\n\t\t\t\t// is already supposed to be aligned to tile boundaries (but is not, exactly,\n\t\t\t\t// because of rounding error during projection).\n\n\t\t\t\tgeom[i].x = std::round(x * scale);\n\t\t\t\tgeom[i].y = std::round(y * scale);\n\t\t\t} else {\n\t\t\t\tgeom[i].x = SHIFT_RIGHT(x);\n\t\t\t\tgeom[i].y = SHIFT_RIGHT(y);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn geom.size();\n}\n\nstatic std::string strip_zeroes(std::string s) {\n\t// Doesn't do anything special with '-' followed by leading zeros\n\t// since integer IDs must be positive\n\n\twhile (s.size() > 0 && s[0] == '0') {\n\t\ts.erase(s.begin());\n\t}\n\n\treturn s;\n}\n\n// called from frontends\nint serialize_feature(struct serialization_state *sst, serial_feature &sf) {\n\tstruct reader *r = &(*sst->readers)[sst->segment];\n\n\tsf.bbox[0] = LLONG_MAX;\n\tsf.bbox[1] = LLONG_MAX;\n\tsf.bbox[2] = LLONG_MIN;\n\tsf.bbox[3] = LLONG_MIN;\n\tscale_geometry(sst, sf.bbox, sf.geometry);\n\n\t// This has to happen after scaling so that the wraparound detection has happened first.\n\t// Otherwise the inner/outer calculation will be confused by bad geometries.\n\tif (sf.t == VT_POLYGON) {\n\t\tsf.geometry = fix_polygon(sf.geometry);\n\t}\n\n\tfor (auto &c : clipbboxes) {\n\t\tif (sf.t == VT_POLYGON) {\n\t\t\tsf.geometry = simple_clip_poly(sf.geometry, SHIFT_RIGHT(c.minx), SHIFT_RIGHT(c.miny), SHIFT_RIGHT(c.maxx), SHIFT_RIGHT(c.maxy));\n\t\t} else if (sf.t == VT_LINE) {\n\t\t\tsf.geometry = clip_lines(sf.geometry, SHIFT_RIGHT(c.minx), SHIFT_RIGHT(c.miny), SHIFT_RIGHT(c.maxx), SHIFT_RIGHT(c.maxy));\n\t\t\tsf.geometry = remove_noop(sf.geometry, sf.t, 0);\n\t\t} else if (sf.t == VT_POINT) {\n\t\t\tsf.geometry = clip_point(sf.geometry, SHIFT_RIGHT(c.minx), SHIFT_RIGHT(c.miny), SHIFT_RIGHT(c.maxx), SHIFT_RIGHT(c.maxy));\n\t\t}\n\n\t\tsf.bbox[0] = LLONG_MAX;\n\t\tsf.bbox[1] = LLONG_MAX;\n\t\tsf.bbox[2] = LLONG_MIN;\n\t\tsf.bbox[3] = LLONG_MIN;\n\n\t\tfor (auto &g : sf.geometry) {\n\t\t\tlong long x = SHIFT_LEFT(g.x);\n\t\t\tlong long y = SHIFT_LEFT(g.y);\n\n\t\t\tif (x < sf.bbox[0]) {\n\t\t\t\tsf.bbox[0] = x;\n\t\t\t}\n\t\t\tif (y < sf.bbox[1]) {\n\t\t\t\tsf.bbox[1] = y;\n\t\t\t}\n\t\t\tif (x > sf.bbox[2]) {\n\t\t\t\tsf.bbox[2] = x;\n\t\t\t}\n\t\t\tif (y > sf.bbox[3]) {\n\t\t\t\tsf.bbox[3] = y;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (sf.geometry.size() == 0) {\n\t\t// Feature was clipped away\n\t\treturn 1;\n\t}\n\n\tif (!sf.has_id) {\n\t\tif (additional[A_GENERATE_IDS]) {\n\t\t\tsf.has_id = true;\n\t\t\tsf.id = sf.seq + 1;\n\t\t}\n\t}\n\n\tif (sst->want_dist) {\n\t\tstd::vector<unsigned long long> locs;\n\t\tfor (size_t i = 0; i < sf.geometry.size(); i++) {\n\t\t\tif (sf.geometry[i].op == VT_MOVETO || sf.geometry[i].op == VT_LINETO) {\n\t\t\t\tlocs.push_back(encode_index(SHIFT_LEFT(sf.geometry[i].x), SHIFT_LEFT(sf.geometry[i].y)));\n\t\t\t}\n\t\t}\n\t\tstd::sort(locs.begin(), locs.end());\n\t\tsize_t n = 0;\n\t\tdouble sum = 0;\n\t\tfor (size_t i = 1; i < locs.size(); i++) {\n\t\t\tif (locs[i - 1] != locs[i]) {\n\t\t\t\tsum += log(locs[i] - locs[i - 1]);\n\t\t\t\tn++;\n\t\t\t}\n\t\t}\n\t\tif (n > 0) {\n\t\t\tdouble avg = exp(sum / n);\n\t\t\t// Convert approximately from tile units to feet\n\t\t\tdouble dist_ft = sqrt(avg) / 33;\n\n\t\t\t*(sst->dist_sum) += log(dist_ft) * n;\n\t\t\t*(sst->dist_count) += n;\n\t\t}\n\t\tlocs.clear();\n\t}\n\n\tbool inline_meta = true;\n\t// Don't inline metadata for features that will span several tiles at maxzoom\n\tif (sf.geometry.size() > 0 && (sf.bbox[2] < sf.bbox[0] || sf.bbox[3] < sf.bbox[1])) {\n\t\tfprintf(stderr, \"Internal error: impossible feature bounding box %llx,%llx,%llx,%llx\\n\", sf.bbox[0], sf.bbox[1], sf.bbox[2], sf.bbox[3]);\n\t}\n\tif (sf.bbox[0] == LLONG_MAX) {\n\t\t// No bounding box (empty geometry)\n\t\t// Shouldn't happen, but avoid arithmetic overflow below\n\t} else if (sf.bbox[2] - sf.bbox[0] > (2LL << (32 - sst->maxzoom)) || sf.bbox[3] - sf.bbox[1] > (2LL << (32 - sst->maxzoom))) {\n\t\tinline_meta = false;\n\n\t\tif (prevent[P_CLIPPING]) {\n\t\t\tstatic std::atomic<long long> warned(0);\n\t\t\tlong long extent = ((sf.bbox[2] - sf.bbox[0]) / ((1LL << (32 - sst->maxzoom)) + 1)) * ((sf.bbox[3] - sf.bbox[1]) / ((1LL << (32 - sst->maxzoom)) + 1));\n\t\t\tif (extent > warned) {\n\t\t\t\tfprintf(stderr, \"Warning: %s:%d: Large unclipped (-pc) feature may be duplicated across %lld tiles\\n\", sst->fname, sst->line, extent);\n\t\t\t\twarned = extent;\n\n\t\t\t\tif (extent > 10000) {\n\t\t\t\t\tfprintf(stderr, \"Exiting because this can't be right.\\n\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tdouble extent = 0;\n\tif (additional[A_DROP_SMALLEST_AS_NEEDED] || additional[A_COALESCE_SMALLEST_AS_NEEDED]) {\n\t\tif (sf.t == VT_POLYGON) {\n\t\t\tfor (size_t i = 0; i < sf.geometry.size(); i++) {\n\t\t\t\tif (sf.geometry[i].op == VT_MOVETO) {\n\t\t\t\t\tsize_t j;\n\t\t\t\t\tfor (j = i + 1; j < sf.geometry.size(); j++) {\n\t\t\t\t\t\tif (sf.geometry[j].op != VT_LINETO) {\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\textent += get_area(sf.geometry, i, j);\n\t\t\t\t\ti = j - 1;\n\t\t\t\t}\n\t\t\t}\n\t\t} else if (sf.t == VT_LINE) {\n\t\t\tfor (size_t i = 1; i < sf.geometry.size(); i++) {\n\t\t\t\tif (sf.geometry[i].op == VT_LINETO) {\n\t\t\t\t\tdouble xd = sf.geometry[i].x - sf.geometry[i - 1].x;\n\t\t\t\t\tdouble yd = sf.geometry[i].y - sf.geometry[i - 1].y;\n\t\t\t\t\textent += sqrt(xd * xd + yd * yd);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (extent <= LLONG_MAX) {\n\t\tsf.extent = (long long) extent;\n\t} else {\n\t\tsf.extent = LLONG_MAX;\n\t}\n\n\tif (!prevent[P_INPUT_ORDER]) {\n\t\tsf.seq = 0;\n\t}\n\n\tlong long bbox_index;\n\n\t// Calculate the center even if off the edge of the plane,\n\t// and then mask to bring it back into the addressable area\n\tlong long midx = (sf.bbox[0] / 2 + sf.bbox[2] / 2) & ((1LL << 32) - 1);\n\tlong long midy = (sf.bbox[1] / 2 + sf.bbox[3] / 2) & ((1LL << 32) - 1);\n\tbbox_index = encode_index(midx, midy);\n\n\tif (additional[A_DROP_DENSEST_AS_NEEDED] || additional[A_COALESCE_DENSEST_AS_NEEDED] || additional[A_CLUSTER_DENSEST_AS_NEEDED] || additional[A_CALCULATE_FEATURE_DENSITY] || additional[A_INCREASE_GAMMA_AS_NEEDED] || sst->uses_gamma || cluster_distance != 0) {\n\t\tsf.index = bbox_index;\n\t} else {\n\t\tsf.index = 0;\n\t}\n\n\tif (sst->layermap->count(sf.layername) == 0) {\n\t\tsst->layermap->insert(std::pair<std::string, layermap_entry>(sf.layername, layermap_entry(sst->layermap->size())));\n\t}\n\n\tauto ai = sst->layermap->find(sf.layername);\n\tif (ai != sst->layermap->end()) {\n\t\tsf.layer = ai->second.id;\n\n\t\tif (!sst->filters) {\n\t\t\tif (sf.t == VT_POINT) {\n\t\t\t\tai->second.points++;\n\t\t\t} else if (sf.t == VT_LINE) {\n\t\t\t\tai->second.lines++;\n\t\t\t} else if (sf.t == VT_POLYGON) {\n\t\t\t\tai->second.polygons++;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfprintf(stderr, \"Internal error: can't find layer name %s\\n\", sf.layername.c_str());\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tfor (ssize_t i = (ssize_t) sf.full_keys.size() - 1; i >= 0; i--) {\n\t\tcoerce_value(sf.full_keys[i], sf.full_values[i].type, sf.full_values[i].s, sst->attribute_types);\n\n\t\tif (sf.full_keys[i] == attribute_for_id) {\n\t\t\tif (sf.full_values[i].type != mvt_double && !additional[A_CONVERT_NUMERIC_IDS]) {\n\t\t\t\tstatic bool warned = false;\n\n\t\t\t\tif (!warned) {\n\t\t\t\t\tfprintf(stderr, \"Warning: Attribute \\\"%s\\\"=\\\"%s\\\" as feature ID is not a number\\n\", sf.full_keys[i].c_str(), sf.full_values[i].s.c_str());\n\t\t\t\t\twarned = true;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tchar *err;\n\t\t\t\tlong long id_value = strtoull(sf.full_values[i].s.c_str(), &err, 10);\n\n\t\t\t\tif (err != NULL && *err != '\\0') {\n\t\t\t\t\tstatic bool warned_frac = false;\n\n\t\t\t\t\tif (!warned_frac) {\n\t\t\t\t\t\tfprintf(stderr, \"Warning: Can't represent non-integer feature ID %s\\n\", sf.full_values[i].s.c_str());\n\t\t\t\t\t\twarned_frac = true;\n\t\t\t\t\t}\n\t\t\t\t} else if (std::to_string(id_value) != strip_zeroes(sf.full_values[i].s)) {\n\t\t\t\t\tstatic bool warned = false;\n\n\t\t\t\t\tif (!warned) {\n\t\t\t\t\t\tfprintf(stderr, \"Warning: Can't represent too-large feature ID %s\\n\", sf.full_values[i].s.c_str());\n\t\t\t\t\t\twarned = true;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tsf.id = id_value;\n\t\t\t\t\tsf.has_id = true;\n\n\t\t\t\t\tsf.full_keys.erase(sf.full_keys.begin() + i);\n\t\t\t\t\tsf.full_values.erase(sf.full_values.begin() + i);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (sst->exclude_all) {\n\t\t\tif (sst->include->count(sf.full_keys[i]) == 0) {\n\t\t\t\tsf.full_keys.erase(sf.full_keys.begin() + i);\n\t\t\t\tsf.full_values.erase(sf.full_values.begin() + i);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else if (sst->exclude->count(sf.full_keys[i]) != 0) {\n\t\t\tsf.full_keys.erase(sf.full_keys.begin() + i);\n\t\t\tsf.full_values.erase(sf.full_values.begin() + i);\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\tif (!sst->filters) {\n\t\tfor (size_t i = 0; i < sf.full_keys.size(); i++) {\n\t\t\ttype_and_string attrib;\n\t\t\tattrib.type = sf.full_values[i].type;\n\t\t\tattrib.string = sf.full_values[i].s;\n\n\t\t\tauto fk = sst->layermap->find(sf.layername);\n\t\t\tadd_to_file_keys(fk->second.file_keys, sf.full_keys[i], attrib);\n\t\t}\n\t}\n\n\tif (inline_meta) {\n\t\tsf.metapos = -1;\n\t\tfor (size_t i = 0; i < sf.full_keys.size(); i++) {\n\t\t\tsf.keys.push_back(addpool(r->poolfile, r->treefile, sf.full_keys[i].c_str(), mvt_string));\n\t\t\tsf.values.push_back(addpool(r->poolfile, r->treefile, sf.full_values[i].s.c_str(), sf.full_values[i].type));\n\t\t}\n\t} else {\n\t\tsf.metapos = r->metapos;\n\t\tserialize_long_long(r->metafile, sf.full_keys.size(), &r->metapos, sst->fname);\n\t\tfor (size_t i = 0; i < sf.full_keys.size(); i++) {\n\t\t\tserialize_long_long(r->metafile, addpool(r->poolfile, r->treefile, sf.full_keys[i].c_str(), mvt_string), &r->metapos, sst->fname);\n\t\t\tserialize_long_long(r->metafile, addpool(r->poolfile, r->treefile, sf.full_values[i].s.c_str(), sf.full_values[i].type), &r->metapos, sst->fname);\n\t\t}\n\t}\n\n\tlong long geomstart = r->geompos;\n\tserialize_feature(r->geomfile, &sf, &r->geompos, sst->fname, SHIFT_RIGHT(*(sst->initial_x)), SHIFT_RIGHT(*(sst->initial_y)), false);\n\n\tstruct index index;\n\tindex.start = geomstart;\n\tindex.end = r->geompos;\n\tindex.segment = sst->segment;\n\tindex.seq = *(sst->layer_seq);\n\tindex.t = sf.t;\n\tindex.ix = bbox_index;\n\n\tfwrite_check(&index, sizeof(struct index), 1, r->indexfile, sst->fname);\n\tr->indexpos += sizeof(struct index);\n\n\tfor (size_t i = 0; i < 2; i++) {\n\t\tif (sf.bbox[i] < r->file_bbox[i]) {\n\t\t\tr->file_bbox[i] = sf.bbox[i];\n\t\t}\n\t}\n\tfor (size_t i = 2; i < 4; i++) {\n\t\tif (sf.bbox[i] > r->file_bbox[i]) {\n\t\t\tr->file_bbox[i] = sf.bbox[i];\n\t\t}\n\t}\n\n\tif (*(sst->progress_seq) % 10000 == 0) {\n\t\tcheckdisk(sst->readers);\n\t\tif (!quiet && !quiet_progress && progress_time()) {\n\t\t\tfprintf(stderr, \"Read %.2f million features\\r\", *sst->progress_seq / 1000000.0);\n\t\t}\n\t}\n\t(*(sst->progress_seq))++;\n\t(*(sst->layer_seq))++;\n\n\treturn 1;\n}\n\nvoid coerce_value(std::string const &key, int &vt, std::string &val, std::map<std::string, int> const *attribute_types) {\n\tauto a = (*attribute_types).find(key);\n\tif (a != attribute_types->end()) {\n\t\tif (a->second == mvt_string) {\n\t\t\tvt = mvt_string;\n\t\t} else if (a->second == mvt_float) {\n\t\t\tvt = mvt_double;\n\t\t\tval = milo::dtoa_milo(atof(val.c_str()));\n\t\t} else if (a->second == mvt_int) {\n\t\t\tvt = mvt_double;\n\t\t\tif (val.size() == 0) {\n\t\t\t\tval = \"0\";\n\t\t\t}\n\n\t\t\tfor (size_t ii = 0; ii < val.size(); ii++) {\n\t\t\t\tchar c = val[ii];\n\t\t\t\tif (c < '0' || c > '9') {\n\t\t\t\t\tval = std::to_string(round(atof(val.c_str())));\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t} else if (a->second == mvt_bool) {\n\t\t\tif (val == \"false\" || val == \"0\" || val == \"null\" || val.size() == 0 || (vt == mvt_double && atof(val.c_str()) == 0)) {\n\t\t\t\tvt = mvt_bool;\n\t\t\t\tval = \"false\";\n\t\t\t} else {\n\t\t\t\tvt = mvt_bool;\n\t\t\t\tval = \"true\";\n\t\t\t}\n\t\t} else {\n\t\t\tfprintf(stderr, \"Can't happen: attribute type %d\\n\", a->second);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "serial.hpp",
          "type": "blob",
          "size": 4.9287109375,
          "content": "#ifndef SERIAL_HPP\n#define SERIAL_HPP\n\n#include <stddef.h>\n#include <stdio.h>\n#include <string.h>\n#include <vector>\n#include <atomic>\n#include <sys/stat.h>\n#include \"geometry.hpp\"\n#include \"mbtiles.hpp\"\n#include \"jsonpull/jsonpull.h\"\n\nsize_t fwrite_check(const void *ptr, size_t size, size_t nitems, FILE *stream, const char *fname);\n\nvoid serialize_int(FILE *out, int n, std::atomic<long long> *fpos, const char *fname);\nvoid serialize_long_long(FILE *out, long long n, std::atomic<long long> *fpos, const char *fname);\nvoid serialize_ulong_long(FILE *out, unsigned long long n, std::atomic<long long> *fpos, const char *fname);\nvoid serialize_byte(FILE *out, signed char n, std::atomic<long long> *fpos, const char *fname);\nvoid serialize_uint(FILE *out, unsigned n, std::atomic<long long> *fpos, const char *fname);\nvoid serialize_string(FILE *out, const char *s, std::atomic<long long> *fpos, const char *fname);\n\nvoid deserialize_int(char **f, int *n);\nvoid deserialize_long_long(char **f, long long *n);\nvoid deserialize_ulong_long(char **f, unsigned long long *n);\nvoid deserialize_uint(char **f, unsigned *n);\nvoid deserialize_byte(char **f, signed char *n);\n\nint deserialize_int_io(FILE *f, int *n, std::atomic<long long> *geompos);\nint deserialize_long_long_io(FILE *f, long long *n, std::atomic<long long> *geompos);\nint deserialize_ulong_long_io(FILE *f, unsigned long long *n, std::atomic<long long> *geompos);\nint deserialize_uint_io(FILE *f, unsigned *n, std::atomic<long long> *geompos);\nint deserialize_byte_io(FILE *f, signed char *n, std::atomic<long long> *geompos);\n\nstruct serial_val {\n\tint type = 0;\n\tstd::string s = \"\";\n};\n\nstruct serial_feature {\n\tlong long layer = 0;\n\tint segment = 0;\n\tlong long seq = 0;\n\n\tsigned char t = 0;\n\tsigned char feature_minzoom = 0;\n\n\tbool has_id = false;\n\tunsigned long long id = 0;\n\n\tbool has_tippecanoe_minzoom = false;\n\tint tippecanoe_minzoom = 0;\n\n\tbool has_tippecanoe_maxzoom = false;\n\tint tippecanoe_maxzoom = 0;\n\n\tdrawvec geometry = drawvec();\n\tunsigned long long index = 0;\n\tlong long extent = 0;\n\n\tstd::vector<long long> keys{};\n\tstd::vector<long long> values{};\n\t// If >= 0, metadata is external\n\tlong long metapos = 0;\n\n\t// XXX This isn't serialized. Should it be here?\n\tlong long bbox[4] = {0, 0, 0, 0};\n\tstd::vector<std::string> full_keys{};\n\tstd::vector<serial_val> full_values{};\n\tstd::string layername = \"\";\n\tbool dropped = false;\n};\n\nvoid serialize_feature(FILE *geomfile, serial_feature *sf, std::atomic<long long> *geompos, const char *fname, long long wx, long long wy, bool include_minzoom);\nserial_feature deserialize_feature(FILE *geoms, std::atomic<long long> *geompos_in, char *metabase, long long *meta_off, unsigned z, unsigned tx, unsigned ty, unsigned *initial_x, unsigned *initial_y);\n\nstruct reader {\n\tint metafd = -1;\n\tint poolfd = -1;\n\tint treefd = -1;\n\tint geomfd = -1;\n\tint indexfd = -1;\n\n\tFILE *metafile = NULL;\n\tstruct memfile *poolfile = NULL;\n\tstruct memfile *treefile = NULL;\n\tFILE *geomfile = NULL;\n\tFILE *indexfile = NULL;\n\n\tstd::atomic<long long> metapos;\n\tstd::atomic<long long> geompos;\n\tstd::atomic<long long> indexpos;\n\n\tlong long file_bbox[4] = {0, 0, 0, 0};\n\n\tstruct stat geomst {};\n\tstruct stat metast {};\n\n\tchar *geom_map = NULL;\n\n\treader()\n\t    : metapos(0), geompos(0), indexpos(0) {\n\t}\n\n\treader(reader const &r) {\n\t\tmetafd = r.metafd;\n\t\tpoolfd = r.poolfd;\n\t\ttreefd = r.treefd;\n\t\tgeomfd = r.geomfd;\n\t\tindexfd = r.indexfd;\n\n\t\tmetafile = r.metafile;\n\t\tpoolfile = r.poolfile;\n\t\ttreefile = r.treefile;\n\t\tgeomfile = r.geomfile;\n\t\tindexfile = r.indexfile;\n\n\t\tlong long p = r.metapos;\n\t\tmetapos = p;\n\n\t\tp = r.geompos;\n\t\tgeompos = p;\n\n\t\tp = r.indexpos;\n\t\tindexpos = p;\n\n\t\tmemcpy(file_bbox, r.file_bbox, sizeof(file_bbox));\n\n\t\tgeomst = r.geomst;\n\t\tmetast = r.metast;\n\n\t\tgeom_map = r.geom_map;\n\t}\n};\n\nstruct serialization_state {\n\tconst char *fname = NULL;  // source file name\n\tint line = 0;\t\t   // user-oriented location within source for error reports\n\n\tstd::atomic<long long> *layer_seq = NULL;     // sequence within current layer\n\tstd::atomic<long long> *progress_seq = NULL;  // overall sequence for progress indicator\n\n\tstd::vector<struct reader> *readers = NULL;  // array of data for each input thread\n\tint segment = 0;\t\t\t     // the current input thread\n\n\tunsigned *initial_x = NULL;  // relative offset of all geometries\n\tunsigned *initial_y = NULL;\n\tint *initialized = NULL;\n\n\tdouble *dist_sum = NULL;  // running tally for calculation of resolution within features\n\tsize_t *dist_count = NULL;\n\tbool want_dist = false;\n\n\tint maxzoom = 0;\n\tint basezoom = 0;\n\n\tbool filters = false;\n\tbool uses_gamma = false;\n\n\tstd::map<std::string, layermap_entry> *layermap = NULL;\n\n\tstd::map<std::string, int> const *attribute_types = NULL;\n\tstd::set<std::string> *exclude = NULL;\n\tstd::set<std::string> *include = NULL;\n\tint exclude_all = 0;\n};\n\nint serialize_feature(struct serialization_state *sst, serial_feature &sf);\nvoid coerce_value(std::string const &key, int &vt, std::string &val, std::map<std::string, int> const *attribute_types);\n\n#endif\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "text.cpp",
          "type": "blob",
          "size": 3.5078125,
          "content": "#include \"text.hpp\"\n#include <stdio.h>\n#include <math.h>\n#include <stdlib.h>\n\n/**\n * Returns an empty string if `s` is valid utf8;\n * otherwise returns an error message.\n */\nstd::string check_utf8(std::string s) {\n\tfor (size_t i = 0; i < s.size(); i++) {\n\t\tsize_t fail = 0;\n\n\t\tif ((s[i] & 0x80) == 0x80) {\n\t\t\tif ((s[i] & 0xE0) == 0xC0) {\n\t\t\t\tif (i + 1 >= s.size() || (s[i + 1] & 0xC0) != 0x80) {\n\t\t\t\t\tfail = 2;\n\t\t\t\t} else {\n\t\t\t\t\ti += 1;\n\t\t\t\t}\n\t\t\t} else if ((s[i] & 0xF0) == 0xE0) {\n\t\t\t\tif (i + 2 >= s.size() || (s[i + 1] & 0xC0) != 0x80 || (s[i + 2] & 0xC0) != 0x80) {\n\t\t\t\t\tfail = 3;\n\t\t\t\t} else {\n\t\t\t\t\ti += 2;\n\t\t\t\t}\n\t\t\t} else if ((s[i] & 0xF8) == 0xF0) {\n\t\t\t\tif (i + 3 >= s.size() || (s[i + 1] & 0xC0) != 0x80 || (s[i + 2] & 0xC0) != 0x80 || (s[i + 3] & 0xC0) != 0x80) {\n\t\t\t\t\tfail = 4;\n\t\t\t\t} else {\n\t\t\t\t\ti += 3;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfail = 1;\n\t\t\t}\n\t\t}\n\n\t\tif (fail != 0) {\n\t\t\tstd::string out = \"\\\"\" + s + \"\\\" is not valid UTF-8 (\";\n\t\t\tfor (size_t j = 0; j < fail && i + j < s.size(); j++) {\n\t\t\t\tif (j != 0) {\n\t\t\t\t\tout += \" \";\n\t\t\t\t}\n\t\t\t\tchar tmp[6];\n\t\t\t\tsprintf(tmp, \"0x%02X\", s[i + j] & 0xFF);\n\t\t\t\tout += std::string(tmp);\n\t\t\t}\n\t\t\tout += \")\";\n\t\t\treturn out;\n\t\t}\n\t}\n\n\treturn \"\";\n}\n\nconst char *utf8_next(const char *s, long *c) {\n\tif (s == NULL) {\n\t\t*c = -1;\n\t\treturn NULL;\n\t}\n\n\tif (*s == '\\0') {\n\t\t*c = -1;\n\t\treturn NULL;\n\t}\n\n\tif ((s[0] & 0x80) == 0x80) {\n\t\tif ((s[0] & 0xE0) == 0xC0) {\n\t\t\tif ((s[1] & 0xC0) != 0x80) {\n\t\t\t\t*c = 0xFFFD;\n\t\t\t\ts++;\n\t\t\t} else {\n\t\t\t\t*c = ((long) (s[0] & 0x1F) << 6) | ((long) (s[1] & 0x7F));\n\t\t\t\ts += 2;\n\t\t\t}\n\t\t} else if ((s[0] & 0xF0) == 0xE0) {\n\t\t\tif ((s[1] & 0xC0) != 0x80 || (s[2] & 0xC0) != 0x80) {\n\t\t\t\t*c = 0xFFFD;\n\t\t\t\ts++;\n\t\t\t} else {\n\t\t\t\t*c = ((long) (s[0] & 0x0F) << 12) | ((long) (s[1] & 0x7F) << 6) | ((long) (s[2] & 0x7F));\n\t\t\t\ts += 3;\n\t\t\t}\n\t\t} else if ((s[0] & 0xF8) == 0xF0) {\n\t\t\tif ((s[1] & 0xC0) != 0x80 || (s[2] & 0xC0) != 0x80 || (s[3] & 0xC0) != 0x80) {\n\t\t\t\t*c = 0xFFFD;\n\t\t\t\ts++;\n\t\t\t} else {\n\t\t\t\t*c = ((long) (s[0] & 0x0F) << 18) | ((long) (s[1] & 0x7F) << 12) | ((long) (s[2] & 0x7F) << 6) | ((long) (s[3] & 0x7F));\n\t\t\t\ts += 4;\n\t\t\t}\n\t\t} else {\n\t\t\t*c = 0xFFFD;\n\t\t\ts++;\n\t\t}\n\t} else {\n\t\t*c = s[0];\n\t\ts++;\n\t}\n\n\treturn s;\n}\n\nstd::string truncate16(std::string const &s, size_t runes) {\n\tconst char *cp = s.c_str();\n\tconst char *start = cp;\n\tconst char *lastgood = cp;\n\tsize_t len = 0;\n\tlong c;\n\n\twhile ((cp = utf8_next(cp, &c)) != NULL) {\n\t\tif (c <= 0xFFFF) {\n\t\t\tlen++;\n\t\t} else {\n\t\t\tlen += 2;\n\t\t}\n\n\t\tif (len <= runes) {\n\t\t\tlastgood = cp;\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn std::string(s, 0, lastgood - start);\n}\n\nint integer_zoom(std::string where, std::string text) {\n\tdouble d = atof(text.c_str());\n\tif (!isfinite(d) || d != floor(d) || d < 0 || d > 32) {\n\t\tfprintf(stderr, \"%s: Expected integer zoom level in \\\"tippecanoe\\\" GeoJSON extension, not %s\\n\", where.c_str(), text.c_str());\n\t\texit(EXIT_FAILURE);\n\t}\n\treturn d;\n}\n\nstd::string format_commandline(int argc, char **argv) {\n\tstd::string out;\n\n\tfor (int i = 0; i < argc; i++) {\n\t\tbool need_quote = false;\n\t\tfor (char *cp = argv[i]; *cp != '\\0'; cp++) {\n\t\t\tif (!isalpha(*cp) && !isdigit(*cp) &&\n\t\t\t    *cp != '/' && *cp != '-' && *cp != '_' && *cp != '@' && *cp != ':' &&\n\t\t\t    *cp != '.' && *cp != '%' && *cp != ',') {\n\t\t\t\tneed_quote = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (need_quote) {\n\t\t\tout.push_back('\\'');\n\t\t\tfor (char *cp = argv[i]; *cp != '\\0'; cp++) {\n\t\t\t\tif (*cp == '\\'') {\n\t\t\t\t\tout.append(\"'\\\"'\\\"'\");\n\t\t\t\t} else {\n\t\t\t\t\tout.push_back(*cp);\n\t\t\t\t}\n\t\t\t}\n\t\t\tout.push_back('\\'');\n\t\t} else {\n\t\t\tout.append(argv[i]);\n\t\t}\n\n\t\tif (i + 1 < argc) {\n\t\t\tout.push_back(' ');\n\t\t}\n\t}\n\n\treturn out;\n}\n"
        },
        {
          "name": "text.hpp",
          "type": "blob",
          "size": 0.3134765625,
          "content": "#ifndef TEXT_HPP\n#define TEXT_HPP\n\n#include <string>\n\nstd::string check_utf8(std::string text);\nconst char *utf8_next(const char *s, long *c);\nstd::string truncate16(std::string const &s, size_t runes);\nint integer_zoom(std::string where, std::string text);\nstd::string format_commandline(int argc, char **argv);\n\n#endif\n"
        },
        {
          "name": "tile-join.cpp",
          "type": "blob",
          "size": 30.7373046875,
          "content": "// for vasprintf() on Linux\n#ifndef _GNU_SOURCE\n#define _GNU_SOURCE\n#endif\n\n#define _DEFAULT_SOURCE\n#include <dirent.h>\n#include <sys/stat.h>\n#include <sys/types.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <unistd.h>\n#include <sqlite3.h>\n#include <limits.h>\n#include <getopt.h>\n#include <vector>\n#include <string>\n#include <map>\n#include <set>\n#include <zlib.h>\n#include <math.h>\n#include <pthread.h>\n#include \"mvt.hpp\"\n#include \"projection.hpp\"\n#include \"pool.hpp\"\n#include \"mbtiles.hpp\"\n#include \"geometry.hpp\"\n#include \"dirtiles.hpp\"\n#include \"evaluator.hpp\"\n#include \"csv.hpp\"\n#include \"text.hpp\"\n#include <fstream>\n#include <sstream>\n#include <algorithm>\n#include <functional>\n#include \"jsonpull/jsonpull.h\"\n#include \"milo/dtoa_milo.h\"\n\nint pk = false;\nint pC = false;\nint pg = false;\nint pe = false;\nsize_t CPUS;\nint quiet = false;\nint maxzoom = 32;\nint minzoom = 0;\nstd::map<std::string, std::string> renames;\nbool exclude_all = false;\n\nstruct stats {\n\tint minzoom;\n\tint maxzoom;\n\tdouble midlat, midlon;\n\tdouble minlat, minlon, maxlat, maxlon;\n};\n\nvoid aprintf(std::string *buf, const char *format, ...) {\n\tva_list ap;\n\tchar *tmp;\n\n\tva_start(ap, format);\n\tif (vasprintf(&tmp, format, ap) < 0) {\n\t\tfprintf(stderr, \"memory allocation failure\\n\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tva_end(ap);\n\n\tbuf->append(tmp, strlen(tmp));\n\tfree(tmp);\n}\n\nvoid handle(std::string message, int z, unsigned x, unsigned y, std::map<std::string, layermap_entry> &layermap, std::vector<std::string> &header, std::map<std::string, std::vector<std::string>> &mapping, std::set<std::string> &exclude, std::set<std::string> &keep_layers, std::set<std::string> &remove_layers, int ifmatched, mvt_tile &outtile, json_object *filter) {\n\tmvt_tile tile;\n\tint features_added = 0;\n\tbool was_compressed;\n\n\tif (!tile.decode(message, was_compressed)) {\n\t\tfprintf(stderr, \"Couldn't decompress tile %d/%u/%u\\n\", z, x, y);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tfor (size_t l = 0; l < tile.layers.size(); l++) {\n\t\tmvt_layer &layer = tile.layers[l];\n\n\t\tauto found = renames.find(layer.name);\n\t\tif (found != renames.end()) {\n\t\t\tlayer.name = found->second;\n\t\t}\n\n\t\tif (keep_layers.size() > 0 && keep_layers.count(layer.name) == 0) {\n\t\t\tcontinue;\n\t\t}\n\t\tif (remove_layers.count(layer.name) != 0) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tsize_t ol;\n\t\tfor (ol = 0; ol < outtile.layers.size(); ol++) {\n\t\t\tif (tile.layers[l].name == outtile.layers[ol].name) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (ol == outtile.layers.size()) {\n\t\t\touttile.layers.push_back(mvt_layer());\n\n\t\t\touttile.layers[ol].name = layer.name;\n\t\t\touttile.layers[ol].version = layer.version;\n\t\t\touttile.layers[ol].extent = layer.extent;\n\t\t}\n\n\t\tmvt_layer &outlayer = outtile.layers[ol];\n\n\t\tif (layer.extent != outlayer.extent) {\n\t\t\tif (layer.extent > outlayer.extent) {\n\t\t\t\tfor (size_t i = 0; i < outlayer.features.size(); i++) {\n\t\t\t\t\tfor (size_t j = 0; j < outlayer.features[i].geometry.size(); j++) {\n\t\t\t\t\t\toutlayer.features[i].geometry[j].x = outlayer.features[i].geometry[j].x * layer.extent / outlayer.extent;\n\t\t\t\t\t\toutlayer.features[i].geometry[j].y = outlayer.features[i].geometry[j].y * layer.extent / outlayer.extent;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\toutlayer.extent = layer.extent;\n\t\t\t}\n\t\t}\n\n\t\tauto file_keys = layermap.find(layer.name);\n\n\t\tfor (size_t f = 0; f < layer.features.size(); f++) {\n\t\t\tmvt_feature feat = layer.features[f];\n\t\t\tstd::set<std::string> exclude_attributes;\n\n\t\t\tif (filter != NULL) {\n\t\t\t\tstd::map<std::string, mvt_value> attributes;\n\n\t\t\t\tfor (size_t t = 0; t + 1 < feat.tags.size(); t += 2) {\n\t\t\t\t\tstd::string key = layer.keys[feat.tags[t]];\n\t\t\t\t\tmvt_value &val = layer.values[feat.tags[t + 1]];\n\n\t\t\t\t\tattributes.insert(std::pair<std::string, mvt_value>(key, val));\n\t\t\t\t}\n\n\t\t\t\tif (feat.has_id) {\n\t\t\t\t\tmvt_value v;\n\t\t\t\t\tv.type = mvt_uint;\n\t\t\t\t\tv.numeric_value.uint_value = feat.id;\n\n\t\t\t\t\tattributes.insert(std::pair<std::string, mvt_value>(\"$id\", v));\n\t\t\t\t}\n\n\t\t\t\tmvt_value v;\n\t\t\t\tv.type = mvt_string;\n\n\t\t\t\tif (feat.type == mvt_point) {\n\t\t\t\t\tv.string_value = \"Point\";\n\t\t\t\t} else if (feat.type == mvt_linestring) {\n\t\t\t\t\tv.string_value = \"LineString\";\n\t\t\t\t} else if (feat.type == mvt_polygon) {\n\t\t\t\t\tv.string_value = \"Polygon\";\n\t\t\t\t}\n\n\t\t\t\tattributes.insert(std::pair<std::string, mvt_value>(\"$type\", v));\n\n\t\t\t\tmvt_value v2;\n\t\t\t\tv2.type = mvt_uint;\n\t\t\t\tv2.numeric_value.uint_value = z;\n\n\t\t\t\tattributes.insert(std::pair<std::string, mvt_value>(\"$zoom\", v2));\n\n\t\t\t\tif (!evaluate(attributes, layer.name, filter, exclude_attributes)) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tmvt_feature outfeature;\n\t\t\tint matched = 0;\n\n\t\t\tif (feat.has_id) {\n\t\t\t\toutfeature.has_id = true;\n\t\t\t\toutfeature.id = feat.id;\n\t\t\t}\n\n\t\t\tstd::map<std::string, std::pair<mvt_value, type_and_string>> attributes;\n\t\t\tstd::vector<std::string> key_order;\n\n\t\t\tfor (size_t t = 0; t + 1 < feat.tags.size(); t += 2) {\n\t\t\t\tconst char *key = layer.keys[feat.tags[t]].c_str();\n\t\t\t\tmvt_value &val = layer.values[feat.tags[t + 1]];\n\t\t\t\tstd::string value;\n\t\t\t\tint type = -1;\n\n\t\t\t\tif (val.type == mvt_string) {\n\t\t\t\t\tvalue = val.string_value;\n\t\t\t\t\ttype = mvt_string;\n\t\t\t\t} else if (val.type == mvt_int) {\n\t\t\t\t\taprintf(&value, \"%lld\", (long long) val.numeric_value.int_value);\n\t\t\t\t\ttype = mvt_double;\n\t\t\t\t} else if (val.type == mvt_double) {\n\t\t\t\t\taprintf(&value, \"%s\", milo::dtoa_milo(val.numeric_value.double_value).c_str());\n\t\t\t\t\ttype = mvt_double;\n\t\t\t\t} else if (val.type == mvt_float) {\n\t\t\t\t\taprintf(&value, \"%s\", milo::dtoa_milo(val.numeric_value.float_value).c_str());\n\t\t\t\t\ttype = mvt_double;\n\t\t\t\t} else if (val.type == mvt_bool) {\n\t\t\t\t\taprintf(&value, \"%s\", val.numeric_value.bool_value ? \"true\" : \"false\");\n\t\t\t\t\ttype = mvt_bool;\n\t\t\t\t} else if (val.type == mvt_sint) {\n\t\t\t\t\taprintf(&value, \"%lld\", (long long) val.numeric_value.sint_value);\n\t\t\t\t\ttype = mvt_double;\n\t\t\t\t} else if (val.type == mvt_uint) {\n\t\t\t\t\taprintf(&value, \"%llu\", (long long) val.numeric_value.uint_value);\n\t\t\t\t\ttype = mvt_double;\n\t\t\t\t} else {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tif (type < 0) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tif (!exclude_all && exclude.count(std::string(key)) == 0 && exclude_attributes.count(std::string(key)) == 0) {\n\t\t\t\t\ttype_and_string tas;\n\t\t\t\t\ttas.type = type;\n\t\t\t\t\ttas.string = value;\n\n\t\t\t\t\tattributes.insert(std::pair<std::string, std::pair<mvt_value, type_and_string>>(key, std::pair<mvt_value, type_and_string>(val, tas)));\n\t\t\t\t\tkey_order.push_back(key);\n\t\t\t\t}\n\n\t\t\t\tif (header.size() > 0 && strcmp(key, header[0].c_str()) == 0) {\n\t\t\t\t\tstd::map<std::string, std::vector<std::string>>::iterator ii = mapping.find(value);\n\n\t\t\t\t\tif (ii != mapping.end()) {\n\t\t\t\t\t\tstd::vector<std::string> fields = ii->second;\n\t\t\t\t\t\tmatched = 1;\n\n\t\t\t\t\t\tfor (size_t i = 1; i < fields.size(); i++) {\n\t\t\t\t\t\t\tstd::string joinkey = header[i];\n\t\t\t\t\t\t\tstd::string joinval = fields[i];\n\t\t\t\t\t\t\tint attr_type = mvt_string;\n\n\t\t\t\t\t\t\tif (joinval.size() > 0) {\n\t\t\t\t\t\t\t\tif (joinval[0] == '\"') {\n\t\t\t\t\t\t\t\t\tjoinval = csv_dequote(joinval);\n\t\t\t\t\t\t\t\t} else if (is_number(joinval)) {\n\t\t\t\t\t\t\t\t\tattr_type = mvt_double;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t} else if (pe) {\n\t\t\t\t\t\t\t\tattr_type = mvt_null;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tconst char *sjoinkey = joinkey.c_str();\n\n\t\t\t\t\t\t\tif (!exclude_all && exclude.count(joinkey) == 0 && exclude_attributes.count(joinkey) == 0 && attr_type != mvt_null) {\n\t\t\t\t\t\t\t\tmvt_value outval;\n\t\t\t\t\t\t\t\tif (attr_type == mvt_string) {\n\t\t\t\t\t\t\t\t\toutval.type = mvt_string;\n\t\t\t\t\t\t\t\t\toutval.string_value = joinval;\n\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\toutval.type = mvt_double;\n\t\t\t\t\t\t\t\t\toutval.numeric_value.double_value = atof(joinval.c_str());\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\tauto fa = attributes.find(sjoinkey);\n\t\t\t\t\t\t\t\tif (fa != attributes.end()) {\n\t\t\t\t\t\t\t\t\tattributes.erase(fa);\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\ttype_and_string tas;\n\t\t\t\t\t\t\t\ttas.type = outval.type;\n\t\t\t\t\t\t\t\ttas.string = joinval;\n\n\t\t\t\t\t\t\t\t// Convert from double to int if the joined attribute is an integer\n\t\t\t\t\t\t\t\toutval = stringified_to_mvt_value(outval.type, joinval.c_str());\n\n\t\t\t\t\t\t\t\tattributes.insert(std::pair<std::string, std::pair<mvt_value, type_and_string>>(joinkey, std::pair<mvt_value, type_and_string>(outval, tas)));\n\t\t\t\t\t\t\t\tkey_order.push_back(joinkey);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (matched || !ifmatched) {\n\t\t\t\tif (file_keys == layermap.end()) {\n\t\t\t\t\tlayermap.insert(std::pair<std::string, layermap_entry>(layer.name, layermap_entry(layermap.size())));\n\t\t\t\t\tfile_keys = layermap.find(layer.name);\n\t\t\t\t\tfile_keys->second.minzoom = z;\n\t\t\t\t\tfile_keys->second.maxzoom = z;\n\t\t\t\t}\n\n\t\t\t\t// To keep attributes in their original order instead of alphabetical\n\t\t\t\tfor (auto k : key_order) {\n\t\t\t\t\tauto fa = attributes.find(k);\n\n\t\t\t\t\tif (fa != attributes.end()) {\n\t\t\t\t\t\toutlayer.tag(outfeature, k, fa->second.first);\n\t\t\t\t\t\tadd_to_file_keys(file_keys->second.file_keys, k, fa->second.second);\n\t\t\t\t\t\tattributes.erase(fa);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\toutfeature.type = feat.type;\n\t\t\t\toutfeature.geometry = feat.geometry;\n\n\t\t\t\tif (layer.extent != outlayer.extent) {\n\t\t\t\t\tfor (size_t i = 0; i < outfeature.geometry.size(); i++) {\n\t\t\t\t\t\toutfeature.geometry[i].x = outfeature.geometry[i].x * outlayer.extent / layer.extent;\n\t\t\t\t\t\toutfeature.geometry[i].y = outfeature.geometry[i].y * outlayer.extent / layer.extent;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tfeatures_added++;\n\t\t\t\toutlayer.features.push_back(outfeature);\n\n\t\t\t\tif (z < file_keys->second.minzoom) {\n\t\t\t\t\tfile_keys->second.minzoom = z;\n\t\t\t\t}\n\t\t\t\tif (z > file_keys->second.maxzoom) {\n\t\t\t\t\tfile_keys->second.maxzoom = z;\n\t\t\t\t}\n\n\t\t\t\tif (feat.type == mvt_point) {\n\t\t\t\t\tfile_keys->second.points++;\n\t\t\t\t} else if (feat.type == mvt_linestring) {\n\t\t\t\t\tfile_keys->second.lines++;\n\t\t\t\t} else if (feat.type == mvt_polygon) {\n\t\t\t\t\tfile_keys->second.polygons++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (features_added == 0) {\n\t\treturn;\n\t}\n}\n\ndouble min(double a, double b) {\n\tif (a < b) {\n\t\treturn a;\n\t} else {\n\t\treturn b;\n\t}\n}\n\ndouble max(double a, double b) {\n\tif (a > b) {\n\t\treturn a;\n\t} else {\n\t\treturn b;\n\t}\n}\n\nstruct reader {\n\tlong long zoom = 0;\n\tlong long x = 0;\n\tlong long sorty = 0;\n\tlong long y = 0;\n\tint z_flag = 0;\n\n\tstd::string data = \"\";\n\n\tstd::vector<zxy> dirtiles;\n\tstd::string dirbase;\n\tstd::string name;\n\n\tsqlite3 *db = NULL;\n\tsqlite3_stmt *stmt = NULL;\n\tstruct reader *next = NULL;\n\n\tbool operator<(const struct reader &r) const {\n\t\tif (zoom < r.zoom) {\n\t\t\treturn true;\n\t\t}\n\t\tif (zoom > r.zoom) {\n\t\t\treturn false;\n\t\t}\n\n\t\tif (x < r.x) {\n\t\t\treturn true;\n\t\t}\n\t\tif (x > r.x) {\n\t\t\treturn false;\n\t\t}\n\n\t\tif (sorty < r.sorty) {\n\t\t\treturn true;\n\t\t}\n\t\tif (sorty > r.sorty) {\n\t\t\treturn false;\n\t\t}\n\n\t\tif (data < r.data) {\n\t\t\treturn true;\n\t\t}\n\n\t\treturn false;\n\t}\n};\n\nstruct reader *begin_reading(char *fname) {\n\tstruct reader *r = new reader;\n\tr->name = fname;\n\n\tstruct stat st;\n\tif (stat(fname, &st) == 0 && (st.st_mode & S_IFDIR) != 0) {\n\t\tr->db = NULL;\n\t\tr->stmt = NULL;\n\t\tr->next = NULL;\n\n\t\tr->dirtiles = enumerate_dirtiles(fname, minzoom, maxzoom);\n\t\tr->dirbase = fname;\n\n\t\tif (r->dirtiles.size() == 0) {\n\t\t\tr->zoom = 32;\n\t\t} else {\n\t\t\tr->zoom = r->dirtiles[0].z;\n\t\t\tr->x = r->dirtiles[0].x;\n\t\t\tr->y = r->dirtiles[0].y;\n\t\t\tr->sorty = (1LL << r->zoom) - 1 - r->y;\n\t\t\tr->data = dir_read_tile(r->dirbase, r->dirtiles[0]);\n\n\t\t\tr->dirtiles.erase(r->dirtiles.begin());\n\t\t}\n\t} else {\n\t\tsqlite3 *db;\n\n\t\tif (sqlite3_open(fname, &db) != SQLITE_OK) {\n\t\t\tfprintf(stderr, \"%s: %s\\n\", fname, sqlite3_errmsg(db));\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tchar *err = NULL;\n\t\tif (sqlite3_exec(db, \"PRAGMA integrity_check;\", NULL, NULL, &err) != SQLITE_OK) {\n\t\t\tfprintf(stderr, \"%s: integrity_check: %s\\n\", fname, err);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tconst char *sql = \"SELECT zoom_level, tile_column, tile_row, tile_data from tiles order by zoom_level, tile_column, tile_row;\";\n\t\tsqlite3_stmt *stmt;\n\n\t\tif (sqlite3_prepare_v2(db, sql, -1, &stmt, NULL) != SQLITE_OK) {\n\t\t\tfprintf(stderr, \"%s: select failed: %s\\n\", fname, sqlite3_errmsg(db));\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tr->db = db;\n\t\tr->stmt = stmt;\n\t\tr->next = NULL;\n\n\t\tif (sqlite3_step(stmt) == SQLITE_ROW) {\n\t\t\tr->zoom = sqlite3_column_int(stmt, 0);\n\t\t\tr->x = sqlite3_column_int(stmt, 1);\n\t\t\tr->sorty = sqlite3_column_int(stmt, 2);\n\t\t\tr->y = (1LL << r->zoom) - 1 - r->sorty;\n\n\t\t\tconst char *data = (const char *) sqlite3_column_blob(stmt, 3);\n\t\t\tsize_t len = sqlite3_column_bytes(stmt, 3);\n\n\t\t\tr->data = std::string(data, len);\n\t\t} else {\n\t\t\tr->zoom = 32;\n\t\t}\n\t}\n\n\treturn r;\n}\n\nstruct arg {\n\tstd::map<zxy, std::vector<std::string>> inputs{};\n\tstd::map<zxy, std::string> outputs{};\n\n\tstd::map<std::string, layermap_entry> *layermap = NULL;\n\n\tstd::vector<std::string> *header = NULL;\n\tstd::map<std::string, std::vector<std::string>> *mapping = NULL;\n\tstd::set<std::string> *exclude = NULL;\n\tstd::set<std::string> *keep_layers = NULL;\n\tstd::set<std::string> *remove_layers = NULL;\n\tint ifmatched = 0;\n\tjson_object *filter = NULL;\n};\n\nvoid *join_worker(void *v) {\n\targ *a = (arg *) v;\n\n\tfor (auto ai = a->inputs.begin(); ai != a->inputs.end(); ++ai) {\n\t\tmvt_tile tile;\n\n\t\tfor (size_t i = 0; i < ai->second.size(); i++) {\n\t\t\thandle(ai->second[i], ai->first.z, ai->first.x, ai->first.y, *(a->layermap), *(a->header), *(a->mapping), *(a->exclude), *(a->keep_layers), *(a->remove_layers), a->ifmatched, tile, a->filter);\n\t\t}\n\n\t\tai->second.clear();\n\n\t\tbool anything = false;\n\t\tmvt_tile outtile;\n\t\tfor (size_t i = 0; i < tile.layers.size(); i++) {\n\t\t\tif (tile.layers[i].features.size() > 0) {\n\t\t\t\touttile.layers.push_back(tile.layers[i]);\n\t\t\t\tanything = true;\n\t\t\t}\n\t\t}\n\n\t\tif (anything) {\n\t\t\tstd::string pbf = outtile.encode();\n\t\t\tstd::string compressed;\n\n\t\t\tif (!pC) {\n\t\t\t\tcompress(pbf, compressed);\n\t\t\t} else {\n\t\t\t\tcompressed = pbf;\n\t\t\t}\n\n\t\t\tif (!pk && compressed.size() > 500000) {\n\t\t\t\tfprintf(stderr, \"Tile %lld/%lld/%lld size is %lld, >500000. Skipping this tile\\n.\", ai->first.z, ai->first.x, ai->first.y, (long long) compressed.size());\n\t\t\t} else {\n\t\t\t\ta->outputs.insert(std::pair<zxy, std::string>(ai->first, compressed));\n\t\t\t}\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nvoid handle_tasks(std::map<zxy, std::vector<std::string>> &tasks, std::vector<std::map<std::string, layermap_entry>> &layermaps, sqlite3 *outdb, const char *outdir, std::vector<std::string> &header, std::map<std::string, std::vector<std::string>> &mapping, std::set<std::string> &exclude, int ifmatched, std::set<std::string> &keep_layers, std::set<std::string> &remove_layers, json_object *filter) {\n\tpthread_t pthreads[CPUS];\n\tstd::vector<arg> args;\n\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\targs.push_back(arg());\n\n\t\targs[i].layermap = &layermaps[i];\n\t\targs[i].header = &header;\n\t\targs[i].mapping = &mapping;\n\t\targs[i].exclude = &exclude;\n\t\targs[i].keep_layers = &keep_layers;\n\t\targs[i].remove_layers = &remove_layers;\n\t\targs[i].ifmatched = ifmatched;\n\t\targs[i].filter = filter;\n\t}\n\n\tsize_t count = 0;\n\t// This isn't careful about distributing tasks evenly across CPUs,\n\t// but, from testing, it actually takes a little longer to do\n\t// the proper allocation than is saved by perfectly balanced threads.\n\tfor (auto ai = tasks.begin(); ai != tasks.end(); ++ai) {\n\t\targs[count].inputs.insert(*ai);\n\t\tcount = (count + 1) % CPUS;\n\n\t\tif (ai == tasks.begin()) {\n\t\t\tif (!quiet) {\n\t\t\t\tfprintf(stderr, \"%lld/%lld/%lld  \\r\", ai->first.z, ai->first.x, ai->first.y);\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\tif (pthread_create(&pthreads[i], NULL, join_worker, &args[i]) != 0) {\n\t\t\tperror(\"pthread_create\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\tvoid *retval;\n\n\t\tif (pthread_join(pthreads[i], &retval) != 0) {\n\t\t\tperror(\"pthread_join\");\n\t\t}\n\n\t\tfor (auto ai = args[i].outputs.begin(); ai != args[i].outputs.end(); ++ai) {\n\t\t\tif (outdb != NULL) {\n\t\t\t\tmbtiles_write_tile(outdb, ai->first.z, ai->first.x, ai->first.y, ai->second.data(), ai->second.size());\n\t\t\t} else if (outdir != NULL) {\n\t\t\t\tdir_write_tile(outdir, ai->first.z, ai->first.x, ai->first.y, ai->second);\n\t\t\t}\n\t\t}\n\t}\n}\n\nvoid handle_vector_layers(json_object *vector_layers, std::map<std::string, layermap_entry> &layermap, std::map<std::string, std::string> &attribute_descriptions) {\n\tif (vector_layers != NULL && vector_layers->type == JSON_ARRAY) {\n\t\tfor (size_t i = 0; i < vector_layers->length; i++) {\n\t\t\tif (vector_layers->array[i]->type == JSON_HASH) {\n\t\t\t\tjson_object *id = json_hash_get(vector_layers->array[i], \"id\");\n\t\t\t\tjson_object *desc = json_hash_get(vector_layers->array[i], \"description\");\n\n\t\t\t\tif (id != NULL && desc != NULL && id->type == JSON_STRING && desc->type == JSON_STRING) {\n\t\t\t\t\tstd::string sid = id->string;\n\t\t\t\t\tstd::string sdesc = desc->string;\n\n\t\t\t\t\tif (sdesc.size() != 0) {\n\t\t\t\t\t\tauto f = layermap.find(sid);\n\t\t\t\t\t\tif (f != layermap.end()) {\n\t\t\t\t\t\t\tf->second.description = sdesc;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tjson_object *fields = json_hash_get(vector_layers->array[i], \"fields\");\n\t\t\t\tif (fields != NULL && fields->type == JSON_HASH) {\n\t\t\t\t\tfor (size_t j = 0; j < fields->length; j++) {\n\t\t\t\t\t\tif (fields->keys[j]->type == JSON_STRING && fields->values[j]->type) {\n\t\t\t\t\t\t\tconst char *desc2 = fields->values[j]->string;\n\n\t\t\t\t\t\t\tif (strcmp(desc2, \"Number\") != 0 &&\n\t\t\t\t\t\t\t    strcmp(desc2, \"String\") != 0 &&\n\t\t\t\t\t\t\t    strcmp(desc2, \"Boolean\") != 0 &&\n\t\t\t\t\t\t\t    strcmp(desc2, \"Mixed\") != 0) {\n\t\t\t\t\t\t\t\tattribute_descriptions.insert(std::pair<std::string, std::string>(fields->keys[j]->string, desc2));\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nvoid decode(struct reader *readers, std::map<std::string, layermap_entry> &layermap, sqlite3 *outdb, const char *outdir, struct stats *st, std::vector<std::string> &header, std::map<std::string, std::vector<std::string>> &mapping, std::set<std::string> &exclude, int ifmatched, std::string &attribution, std::string &description, std::set<std::string> &keep_layers, std::set<std::string> &remove_layers, std::string &name, json_object *filter, std::map<std::string, std::string> &attribute_descriptions, std::string &generator_options) {\n\tstd::vector<std::map<std::string, layermap_entry>> layermaps;\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\tlayermaps.push_back(std::map<std::string, layermap_entry>());\n\t}\n\n\tstd::map<zxy, std::vector<std::string>> tasks;\n\tdouble minlat = INT_MAX;\n\tdouble minlon = INT_MAX;\n\tdouble maxlat = INT_MIN;\n\tdouble maxlon = INT_MIN;\n\tint zoom_for_bbox = -1;\n\n\twhile (readers != NULL && readers->zoom < 32) {\n\t\treader *r = readers;\n\t\treaders = readers->next;\n\t\tr->next = NULL;\n\t\tif (r->zoom != zoom_for_bbox) {\n\t\t\t// Only use highest zoom for bbox calculation\n\t\t\t// to avoid z0 always covering the world\n\n\t\t\tminlat = minlon = INT_MAX;\n\t\t\tmaxlat = maxlon = INT_MIN;\n\t\t\tzoom_for_bbox = r->zoom;\n\t\t}\n\n\t\tdouble lat1, lon1, lat2, lon2;\n\t\ttile2lonlat(r->x, r->y, r->zoom, &lon1, &lat1);\n\t\ttile2lonlat(r->x + 1, r->y + 1, r->zoom, &lon2, &lat2);\n\t\tminlat = min(lat2, minlat);\n\t\tminlon = min(lon1, minlon);\n\t\tmaxlat = max(lat1, maxlat);\n\t\tmaxlon = max(lon2, maxlon);\n\n\t\tif (r->zoom >= minzoom && r->zoom <= maxzoom) {\n\t\t\tzxy tile = zxy(r->zoom, r->x, r->y);\n\t\t\tif (tasks.count(tile) == 0) {\n\t\t\t\ttasks.insert(std::pair<zxy, std::vector<std::string>>(tile, std::vector<std::string>()));\n\t\t\t}\n\t\t\tauto f = tasks.find(tile);\n\t\t\tf->second.push_back(r->data);\n\t\t}\n\n\t\tif (readers == NULL || readers->zoom != r->zoom || readers->x != r->x || readers->y != r->y) {\n\t\t\tif (tasks.size() > 100 * CPUS) {\n\t\t\t\thandle_tasks(tasks, layermaps, outdb, outdir, header, mapping, exclude, ifmatched, keep_layers, remove_layers, filter);\n\t\t\t\ttasks.clear();\n\t\t\t}\n\t\t}\n\n\t\tif (r->db != NULL) {\n\t\t\tif (sqlite3_step(r->stmt) == SQLITE_ROW) {\n\t\t\t\tr->zoom = sqlite3_column_int(r->stmt, 0);\n\t\t\t\tr->x = sqlite3_column_int(r->stmt, 1);\n\t\t\t\tr->sorty = sqlite3_column_int(r->stmt, 2);\n\t\t\t\tr->y = (1LL << r->zoom) - 1 - r->sorty;\n\t\t\t\tconst char *data = (const char *) sqlite3_column_blob(r->stmt, 3);\n\t\t\t\tsize_t len = sqlite3_column_bytes(r->stmt, 3);\n\n\t\t\t\tr->data = std::string(data, len);\n\t\t\t} else {\n\t\t\t\tr->zoom = 32;\n\t\t\t}\n\t\t} else {\n\t\t\tif (r->dirtiles.size() == 0) {\n\t\t\t\tr->zoom = 32;\n\t\t\t} else {\n\t\t\t\tr->zoom = r->dirtiles[0].z;\n\t\t\t\tr->x = r->dirtiles[0].x;\n\t\t\t\tr->y = r->dirtiles[0].y;\n\t\t\t\tr->sorty = (1LL << r->zoom) - 1 - r->y;\n\t\t\t\tr->data = dir_read_tile(r->dirbase, r->dirtiles[0]);\n\n\t\t\t\tr->dirtiles.erase(r->dirtiles.begin());\n\t\t\t}\n\t\t}\n\n\t\tstruct reader **rr;\n\n\t\tfor (rr = &readers; *rr != NULL; rr = &((*rr)->next)) {\n\t\t\tif (*r < **rr) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tr->next = *rr;\n\t\t*rr = r;\n\t}\n\n\tst->minlon = min(minlon, st->minlon);\n\tst->maxlon = max(maxlon, st->maxlon);\n\tst->minlat = min(minlat, st->minlat);\n\tst->maxlat = max(maxlat, st->maxlat);\n\n\thandle_tasks(tasks, layermaps, outdb, outdir, header, mapping, exclude, ifmatched, keep_layers, remove_layers, filter);\n\tlayermap = merge_layermaps(layermaps);\n\n\tstruct reader *next;\n\tfor (struct reader *r = readers; r != NULL; r = next) {\n\t\tnext = r->next;\n\n\t\tsqlite3 *db = r->db;\n\t\tif (db == NULL) {\n\t\t\tdb = dirmeta2tmp(r->dirbase.c_str());\n\t\t} else {\n\t\t\tsqlite3_finalize(r->stmt);\n\t\t}\n\n\t\tif (sqlite3_prepare_v2(db, \"SELECT value from metadata where name = 'minzoom'\", -1, &r->stmt, NULL) == SQLITE_OK) {\n\t\t\tif (sqlite3_step(r->stmt) == SQLITE_ROW) {\n\t\t\t\tint minz = max(sqlite3_column_int(r->stmt, 0), minzoom);\n\t\t\t\tst->minzoom = min(st->minzoom, minz);\n\t\t\t}\n\t\t\tsqlite3_finalize(r->stmt);\n\t\t}\n\t\tif (sqlite3_prepare_v2(db, \"SELECT value from metadata where name = 'maxzoom'\", -1, &r->stmt, NULL) == SQLITE_OK) {\n\t\t\tif (sqlite3_step(r->stmt) == SQLITE_ROW) {\n\t\t\t\tint maxz = min(sqlite3_column_int(r->stmt, 0), maxzoom);\n\n\t\t\t\tif (st->maxzoom >= 0 && maxz != st->maxzoom) {\n\t\t\t\t\tfprintf(stderr, \"Warning: mismatched maxzooms: %d in %s vs previous %d\\n\", maxz, r->name.c_str(), st->maxzoom);\n\t\t\t\t}\n\n\t\t\t\tst->maxzoom = max(st->maxzoom, maxz);\n\t\t\t}\n\t\t\tsqlite3_finalize(r->stmt);\n\t\t}\n\t\tif (sqlite3_prepare_v2(db, \"SELECT value from metadata where name = 'center'\", -1, &r->stmt, NULL) == SQLITE_OK) {\n\t\t\tif (sqlite3_step(r->stmt) == SQLITE_ROW) {\n\t\t\t\tconst unsigned char *s = sqlite3_column_text(r->stmt, 0);\n\t\t\t\tif (s != NULL) {\n\t\t\t\t\tsscanf((char *) s, \"%lf,%lf\", &st->midlon, &st->midlat);\n\t\t\t\t}\n\t\t\t}\n\t\t\tsqlite3_finalize(r->stmt);\n\t\t}\n\t\tif (sqlite3_prepare_v2(db, \"SELECT value from metadata where name = 'attribution'\", -1, &r->stmt, NULL) == SQLITE_OK) {\n\t\t\tif (sqlite3_step(r->stmt) == SQLITE_ROW) {\n\t\t\t\tconst unsigned char *s = sqlite3_column_text(r->stmt, 0);\n\t\t\t\tif (s != NULL) {\n\t\t\t\t\tattribution = std::string((char *) s);\n\t\t\t\t}\n\t\t\t}\n\t\t\tsqlite3_finalize(r->stmt);\n\t\t}\n\t\tif (sqlite3_prepare_v2(db, \"SELECT value from metadata where name = 'description'\", -1, &r->stmt, NULL) == SQLITE_OK) {\n\t\t\tif (sqlite3_step(r->stmt) == SQLITE_ROW) {\n\t\t\t\tconst unsigned char *s = sqlite3_column_text(r->stmt, 0);\n\t\t\t\tif (s != NULL) {\n\t\t\t\t\tdescription = std::string((char *) s);\n\t\t\t\t}\n\t\t\t}\n\t\t\tsqlite3_finalize(r->stmt);\n\t\t}\n\t\tif (sqlite3_prepare_v2(db, \"SELECT value from metadata where name = 'name'\", -1, &r->stmt, NULL) == SQLITE_OK) {\n\t\t\tif (sqlite3_step(r->stmt) == SQLITE_ROW) {\n\t\t\t\tconst unsigned char *s = sqlite3_column_text(r->stmt, 0);\n\t\t\t\tif (s != NULL) {\n\t\t\t\t\tif (name.size() == 0) {\n\t\t\t\t\t\tname = std::string((char *) s);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstd::string proposed = name + \" + \" + std::string((char *) s);\n\t\t\t\t\t\tif (proposed.size() < 255) {\n\t\t\t\t\t\t\tname = proposed;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tsqlite3_finalize(r->stmt);\n\t\t}\n\t\tif (sqlite3_prepare_v2(db, \"SELECT value from metadata where name = 'bounds'\", -1, &r->stmt, NULL) == SQLITE_OK) {\n\t\t\tif (sqlite3_step(r->stmt) == SQLITE_ROW) {\n\t\t\t\tconst unsigned char *s = sqlite3_column_text(r->stmt, 0);\n\t\t\t\tif (s != NULL) {\n\t\t\t\t\tif (sscanf((char *) s, \"%lf,%lf,%lf,%lf\", &minlon, &minlat, &maxlon, &maxlat) == 4) {\n\t\t\t\t\t\tst->minlon = min(minlon, st->minlon);\n\t\t\t\t\t\tst->maxlon = max(maxlon, st->maxlon);\n\t\t\t\t\t\tst->minlat = min(minlat, st->minlat);\n\t\t\t\t\t\tst->maxlat = max(maxlat, st->maxlat);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tsqlite3_finalize(r->stmt);\n\t\t}\n\t\tif (sqlite3_prepare_v2(db, \"SELECT value from metadata where name = 'json'\", -1, &r->stmt, NULL) == SQLITE_OK) {\n\t\t\tif (sqlite3_step(r->stmt) == SQLITE_ROW) {\n\t\t\t\tconst unsigned char *s = sqlite3_column_text(r->stmt, 0);\n\n\t\t\t\tif (s != NULL) {\n\t\t\t\t\tjson_pull *jp = json_begin_string((const char *) s);\n\t\t\t\t\tjson_object *o = json_read_tree(jp);\n\n\t\t\t\t\tif (o != NULL && o->type == JSON_HASH) {\n\t\t\t\t\t\tjson_object *vector_layers = json_hash_get(o, \"vector_layers\");\n\n\t\t\t\t\t\thandle_vector_layers(vector_layers, layermap, attribute_descriptions);\n\t\t\t\t\t\tjson_free(o);\n\t\t\t\t\t}\n\n\t\t\t\t\tjson_end(jp);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tsqlite3_finalize(r->stmt);\n\t\t}\n\t\tif (sqlite3_prepare_v2(db, \"SELECT value from metadata where name = 'generator_options'\", -1, &r->stmt, NULL) == SQLITE_OK) {\n\t\t\tif (sqlite3_step(r->stmt) == SQLITE_ROW) {\n\t\t\t\tconst unsigned char *s = sqlite3_column_text(r->stmt, 0);\n\t\t\t\tif (s != NULL) {\n\t\t\t\t\tif (generator_options.size() != 0) {\n\t\t\t\t\t\tgenerator_options.append(\"; \");\n\t\t\t\t\t\tgenerator_options.append((const char *) s);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tgenerator_options = (const char *) s;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tsqlite3_finalize(r->stmt);\n\t\t}\n\n\t\t// Closes either real db or temp mirror of metadata.json\n\t\tif (sqlite3_close(db) != SQLITE_OK) {\n\t\t\tfprintf(stderr, \"Could not close database: %s\\n\", sqlite3_errmsg(db));\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tdelete r;\n\t}\n}\n\nvoid usage(char **argv) {\n\tfprintf(stderr, \"Usage: %s [-f] [-i] [-pk] [-pC] [-c joins.csv] [-X] [-x exclude ...] -o new.mbtiles source.mbtiles ...\\n\", argv[0]);\n\texit(EXIT_FAILURE);\n}\n\nint main(int argc, char **argv) {\n\tchar *out_mbtiles = NULL;\n\tchar *out_dir = NULL;\n\tsqlite3 *outdb = NULL;\n\tchar *csv = NULL;\n\tint force = 0;\n\tint ifmatched = 0;\n\tjson_object *filter = NULL;\n\n\tCPUS = sysconf(_SC_NPROCESSORS_ONLN);\n\n\tconst char *TIPPECANOE_MAX_THREADS = getenv(\"TIPPECANOE_MAX_THREADS\");\n\tif (TIPPECANOE_MAX_THREADS != NULL) {\n\t\tCPUS = atoi(TIPPECANOE_MAX_THREADS);\n\t}\n\tif (CPUS < 1) {\n\t\tCPUS = 1;\n\t}\n\n\tstd::vector<std::string> header;\n\tstd::map<std::string, std::vector<std::string>> mapping;\n\n\tstd::set<std::string> exclude;\n\tstd::set<std::string> keep_layers;\n\tstd::set<std::string> remove_layers;\n\n\tstd::string set_name, set_description, set_attribution;\n\n\tstruct option long_options[] = {\n\t\t{\"output\", required_argument, 0, 'o'},\n\t\t{\"output-to-directory\", required_argument, 0, 'e'},\n\t\t{\"force\", no_argument, 0, 'f'},\n\t\t{\"if-matched\", no_argument, 0, 'i'},\n\t\t{\"attribution\", required_argument, 0, 'A'},\n\t\t{\"name\", required_argument, 0, 'n'},\n\t\t{\"description\", required_argument, 0, 'N'},\n\t\t{\"prevent\", required_argument, 0, 'p'},\n\t\t{\"csv\", required_argument, 0, 'c'},\n\t\t{\"exclude\", required_argument, 0, 'x'},\n\t\t{\"exclude-all\", no_argument, 0, 'X'},\n\t\t{\"layer\", required_argument, 0, 'l'},\n\t\t{\"exclude-layer\", required_argument, 0, 'L'},\n\t\t{\"quiet\", no_argument, 0, 'q'},\n\t\t{\"maximum-zoom\", required_argument, 0, 'z'},\n\t\t{\"minimum-zoom\", required_argument, 0, 'Z'},\n\t\t{\"feature-filter-file\", required_argument, 0, 'J'},\n\t\t{\"feature-filter\", required_argument, 0, 'j'},\n\t\t{\"rename-layer\", required_argument, 0, 'R'},\n\n\t\t{\"no-tile-size-limit\", no_argument, &pk, 1},\n\t\t{\"no-tile-compression\", no_argument, &pC, 1},\n\t\t{\"empty-csv-columns-are-null\", no_argument, &pe, 1},\n\t\t{\"no-tile-stats\", no_argument, &pg, 1},\n\n\t\t{0, 0, 0, 0},\n\t};\n\n\tstd::string getopt_str;\n\tfor (size_t lo = 0; long_options[lo].name != NULL; lo++) {\n\t\tif (long_options[lo].val > ' ') {\n\t\t\tgetopt_str.push_back(long_options[lo].val);\n\n\t\t\tif (long_options[lo].has_arg == required_argument) {\n\t\t\t\tgetopt_str.push_back(':');\n\t\t\t}\n\t\t}\n\t}\n\n\textern int optind;\n\textern char *optarg;\n\tint i;\n\n\tstd::string commandline = format_commandline(argc, argv);\n\n\twhile ((i = getopt_long(argc, argv, getopt_str.c_str(), long_options, NULL)) != -1) {\n\t\tswitch (i) {\n\t\tcase 0:\n\t\t\tbreak;\n\n\t\tcase 'o':\n\t\t\tout_mbtiles = optarg;\n\t\t\tbreak;\n\n\t\tcase 'e':\n\t\t\tout_dir = optarg;\n\t\t\tbreak;\n\n\t\tcase 'f':\n\t\t\tforce = 1;\n\t\t\tbreak;\n\n\t\tcase 'i':\n\t\t\tifmatched = 1;\n\t\t\tbreak;\n\n\t\tcase 'A':\n\t\t\tset_attribution = optarg;\n\t\t\tbreak;\n\n\t\tcase 'n':\n\t\t\tset_name = optarg;\n\t\t\tbreak;\n\n\t\tcase 'N':\n\t\t\tset_description = optarg;\n\t\t\tbreak;\n\n\t\tcase 'z':\n\t\t\tmaxzoom = atoi(optarg);\n\t\t\tbreak;\n\n\t\tcase 'Z':\n\t\t\tminzoom = atoi(optarg);\n\t\t\tbreak;\n\n\t\tcase 'J':\n\t\t\tfilter = read_filter(optarg);\n\t\t\tbreak;\n\n\t\tcase 'j':\n\t\t\tfilter = parse_filter(optarg);\n\t\t\tbreak;\n\n\t\tcase 'p':\n\t\t\tif (strcmp(optarg, \"k\") == 0) {\n\t\t\t\tpk = true;\n\t\t\t} else if (strcmp(optarg, \"C\") == 0) {\n\t\t\t\tpC = true;\n\t\t\t} else if (strcmp(optarg, \"g\") == 0) {\n\t\t\t\tpg = true;\n\t\t\t} else if (strcmp(optarg, \"e\") == 0) {\n\t\t\t\tpe = true;\n\t\t\t} else {\n\t\t\t\tfprintf(stderr, \"%s: Unknown option for -p%s\\n\", argv[0], optarg);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 'c':\n\t\t\tif (csv != NULL) {\n\t\t\t\tfprintf(stderr, \"Only one -c for now\\n\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tcsv = optarg;\n\t\t\treadcsv(csv, header, mapping);\n\t\t\tbreak;\n\n\t\tcase 'x':\n\t\t\texclude.insert(std::string(optarg));\n\t\t\tbreak;\n\n\t\tcase 'X':\n\t\t\texclude_all = true;\n\t\t\tbreak;\n\n\t\tcase 'l':\n\t\t\tkeep_layers.insert(std::string(optarg));\n\t\t\tbreak;\n\n\t\tcase 'L':\n\t\t\tremove_layers.insert(std::string(optarg));\n\t\t\tbreak;\n\n\t\tcase 'R': {\n\t\t\tchar *cp = strchr(optarg, ':');\n\t\t\tif (cp == NULL || cp == optarg) {\n\t\t\t\tfprintf(stderr, \"%s: -R requires old:new\\n\", argv[0]);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tstd::string before = std::string(optarg).substr(0, cp - optarg);\n\t\t\tstd::string after = std::string(cp + 1);\n\t\t\trenames.insert(std::pair<std::string, std::string>(before, after));\n\t\t\tbreak;\n\t\t}\n\n\t\tcase 'q':\n\t\t\tquiet = true;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tusage(argv);\n\t\t}\n\t}\n\n\tif (argc - optind < 1) {\n\t\tusage(argv);\n\t}\n\n\tif (out_mbtiles == NULL && out_dir == NULL) {\n\t\tfprintf(stderr, \"%s: must specify -o out.mbtiles or -e directory\\n\", argv[0]);\n\t\tusage(argv);\n\t}\n\n\tif (out_mbtiles != NULL && out_dir != NULL) {\n\t\tfprintf(stderr, \"%s: Options -o and -e cannot be used together\\n\", argv[0]);\n\t\tusage(argv);\n\t}\n\n\tif (minzoom > maxzoom) {\n\t\tfprintf(stderr, \"%s: Minimum zoom -Z%d cannot be greater than maxzoom -z%d\\n\", argv[0], minzoom, maxzoom);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tif (out_mbtiles != NULL) {\n\t\tif (force) {\n\t\t\tunlink(out_mbtiles);\n\t\t}\n\t\toutdb = mbtiles_open(out_mbtiles, argv, 0);\n\t}\n\tif (out_dir != NULL) {\n\t\tcheck_dir(out_dir, argv, force, false);\n\t}\n\n\tstruct stats st;\n\tmemset(&st, 0, sizeof(st));\n\tst.minzoom = st.minlat = st.minlon = INT_MAX;\n\tst.maxzoom = st.maxlat = st.maxlon = INT_MIN;\n\n\tstd::map<std::string, layermap_entry> layermap;\n\tstd::string attribution;\n\tstd::string description;\n\tstd::string name;\n\n\tstruct reader *readers = NULL;\n\n\tfor (i = optind; i < argc; i++) {\n\t\treader *r = begin_reading(argv[i]);\n\t\tstruct reader **rr;\n\n\t\tfor (rr = &readers; *rr != NULL; rr = &((*rr)->next)) {\n\t\t\tif (*r < **rr) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tr->next = *rr;\n\t\t*rr = r;\n\t}\n\n\tstd::map<std::string, std::string> attribute_descriptions;\n\tstd::string generator_options;\n\n\tdecode(readers, layermap, outdb, out_dir, &st, header, mapping, exclude, ifmatched, attribution, description, keep_layers, remove_layers, name, filter, attribute_descriptions, generator_options);\n\n\tif (set_attribution.size() != 0) {\n\t\tattribution = set_attribution;\n\t}\n\tif (set_description.size() != 0) {\n\t\tdescription = set_description;\n\t}\n\tif (set_name.size() != 0) {\n\t\tname = set_name;\n\t}\n\n\tif (generator_options.size() != 0) {\n\t\tgenerator_options.append(\"; \");\n\t}\n\tgenerator_options.append(commandline);\n\n\tfor (auto &l : layermap) {\n\t\tif (l.second.minzoom < st.minzoom) {\n\t\t\tst.minzoom = l.second.minzoom;\n\t\t}\n\t\tif (l.second.maxzoom > st.maxzoom) {\n\t\t\tst.maxzoom = l.second.maxzoom;\n\t\t}\n\t}\n\n\tmbtiles_write_metadata(outdb, out_dir, name.c_str(), st.minzoom, st.maxzoom, st.minlat, st.minlon, st.maxlat, st.maxlon, st.midlat, st.midlon, 0, attribution.size() != 0 ? attribution.c_str() : NULL, layermap, true, description.c_str(), !pg, attribute_descriptions, \"tile-join\", generator_options);\n\n\tif (outdb != NULL) {\n\t\tmbtiles_close(outdb, argv[0]);\n\t}\n\n\tif (filter != NULL) {\n\t\tjson_free(filter);\n\t}\n\n\treturn 0;\n}\n"
        },
        {
          "name": "tile.cpp",
          "type": "blob",
          "size": 85.921875,
          "content": "#ifdef __APPLE__\n#define _DARWIN_UNLIMITED_STREAMS\n#endif\n\n#include <iostream>\n#include <fstream>\n#include <string>\n#include <stack>\n#include <vector>\n#include <map>\n#include <set>\n#include <algorithm>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <unistd.h>\n#include <limits.h>\n#include <zlib.h>\n#include <sys/stat.h>\n#include <sys/types.h>\n#include <sys/mman.h>\n#include <cmath>\n#include <sqlite3.h>\n#include <pthread.h>\n#include <errno.h>\n#include <time.h>\n#include <fcntl.h>\n#include <sys/wait.h>\n#include \"mvt.hpp\"\n#include \"mbtiles.hpp\"\n#include \"dirtiles.hpp\"\n#include \"geometry.hpp\"\n#include \"tile.hpp\"\n#include \"pool.hpp\"\n#include \"projection.hpp\"\n#include \"serial.hpp\"\n#include \"options.hpp\"\n#include \"main.hpp\"\n#include \"write_json.hpp\"\n#include \"milo/dtoa_milo.h\"\n#include \"evaluator.hpp\"\n\nextern \"C\" {\n#include \"jsonpull/jsonpull.h\"\n}\n\n#include \"plugin.hpp\"\n\n#define CMD_BITS 3\n\n// Offset coordinates to keep them positive\n#define COORD_OFFSET (4LL << 32)\n#define SHIFT_RIGHT(a) ((((a) + COORD_OFFSET) >> geometry_scale) - (COORD_OFFSET >> geometry_scale))\n\n#define XSTRINGIFY(s) STRINGIFY(s)\n#define STRINGIFY(s) #s\n\npthread_mutex_t db_lock = PTHREAD_MUTEX_INITIALIZER;\npthread_mutex_t var_lock = PTHREAD_MUTEX_INITIALIZER;\n\nstd::vector<mvt_geometry> to_feature(drawvec &geom) {\n\tstd::vector<mvt_geometry> out;\n\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tout.push_back(mvt_geometry(geom[i].op, geom[i].x, geom[i].y));\n\t}\n\n\treturn out;\n}\n\nbool draws_something(drawvec &geom) {\n\tfor (size_t i = 1; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_LINETO && (geom[i].x != geom[i - 1].x || geom[i].y != geom[i - 1].y)) {\n\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic int metacmp(const std::vector<long long> &keys1, const std::vector<long long> &values1, char *stringpool1, const std::vector<long long> &keys2, const std::vector<long long> &values2, char *stringpool2);\nint coalindexcmp(const struct coalesce *c1, const struct coalesce *c2);\n\nstruct coalesce {\n\tchar *stringpool = NULL;\n\tstd::vector<long long> keys = std::vector<long long>();\n\tstd::vector<long long> values = std::vector<long long>();\n\tstd::vector<std::string> full_keys = std::vector<std::string>();\n\tstd::vector<serial_val> full_values = std::vector<serial_val>();\n\tdrawvec geom = drawvec();\n\tunsigned long long index = 0;\n\tlong long original_seq = 0;\n\tint type = 0;\n\tbool coalesced = false;\n\tdouble spacing = 0;\n\tbool has_id = false;\n\tunsigned long long id = 0;\n\n\tbool operator<(const coalesce &o) const {\n\t\tint cmp = coalindexcmp(this, &o);\n\t\tif (cmp < 0) {\n\t\t\treturn true;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n};\n\nstruct preservecmp {\n\tbool operator()(const struct coalesce &a, const struct coalesce &b) {\n\t\treturn a.original_seq < b.original_seq;\n\t}\n} preservecmp;\n\nint coalcmp(const void *v1, const void *v2) {\n\tconst struct coalesce *c1 = (const struct coalesce *) v1;\n\tconst struct coalesce *c2 = (const struct coalesce *) v2;\n\n\tint cmp = c1->type - c2->type;\n\tif (cmp != 0) {\n\t\treturn cmp;\n\t}\n\n\tif (c1->has_id != c2->has_id) {\n\t\treturn (int) c1->has_id - (int) c2->has_id;\n\t}\n\n\tif (c1->has_id && c2->has_id) {\n\t\tif (c1->id < c2->id) {\n\t\t\treturn -1;\n\t\t}\n\t\tif (c1->id > c2->id) {\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tcmp = metacmp(c1->keys, c1->values, c1->stringpool, c2->keys, c2->values, c2->stringpool);\n\tif (cmp != 0) {\n\t\treturn cmp;\n\t}\n\n\tif (c1->full_keys.size() < c2->full_keys.size()) {\n\t\treturn -1;\n\t} else if (c1->full_keys.size() > c2->full_keys.size()) {\n\t\treturn 1;\n\t}\n\n\tfor (size_t i = 0; i < c1->full_keys.size(); i++) {\n\t\tif (c1->full_keys[i] < c2->full_keys[i]) {\n\t\t\treturn -1;\n\t\t} else if (c1->full_keys[i] > c2->full_keys[i]) {\n\t\t\treturn 1;\n\t\t}\n\n\t\tif (c1->full_values[i].type < c2->full_values[i].type) {\n\t\t\treturn -1;\n\t\t} else if (c1->full_values[i].type > c2->full_values[i].type) {\n\t\t\treturn 1;\n\t\t}\n\n\t\tif (c1->full_values[i].s < c2->full_values[i].s) {\n\t\t\treturn -1;\n\t\t} else if (c1->full_values[i].s > c2->full_values[i].s) {\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint coalindexcmp(const struct coalesce *c1, const struct coalesce *c2) {\n\tint cmp = coalcmp((const void *) c1, (const void *) c2);\n\n\tif (cmp == 0) {\n\t\tif (c1->index < c2->index) {\n\t\t\treturn -1;\n\t\t} else if (c1->index > c2->index) {\n\t\t\treturn 1;\n\t\t}\n\n\t\tif (c1->geom < c2->geom) {\n\t\t\treturn -1;\n\t\t} else if (c1->geom > c2->geom) {\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn cmp;\n}\n\nmvt_value retrieve_string(long long off, char *stringpool, int *otype) {\n\tint type = stringpool[off];\n\tchar *s = stringpool + off + 1;\n\n\tif (otype != NULL) {\n\t\t*otype = type;\n\t}\n\n\treturn stringified_to_mvt_value(type, s);\n}\n\nvoid decode_meta(std::vector<long long> const &metakeys, std::vector<long long> const &metavals, char *stringpool, mvt_layer &layer, mvt_feature &feature) {\n\tsize_t i;\n\tfor (i = 0; i < metakeys.size(); i++) {\n\t\tint otype;\n\t\tmvt_value key = retrieve_string(metakeys[i], stringpool, NULL);\n\t\tmvt_value value = retrieve_string(metavals[i], stringpool, &otype);\n\n\t\tlayer.tag(feature, key.string_value, value);\n\t}\n}\n\nstatic int metacmp(const std::vector<long long> &keys1, const std::vector<long long> &values1, char *stringpool1, const std::vector<long long> &keys2, const std::vector<long long> &values2, char *stringpool2) {\n\tsize_t i;\n\tfor (i = 0; i < keys1.size() && i < keys2.size(); i++) {\n\t\tmvt_value key1 = retrieve_string(keys1[i], stringpool1, NULL);\n\t\tmvt_value key2 = retrieve_string(keys2[i], stringpool2, NULL);\n\n\t\tif (key1.string_value < key2.string_value) {\n\t\t\treturn -1;\n\t\t} else if (key1.string_value > key2.string_value) {\n\t\t\treturn 1;\n\t\t}\n\n\t\tlong long off1 = values1[i];\n\t\tint type1 = stringpool1[off1];\n\t\tchar *s1 = stringpool1 + off1 + 1;\n\n\t\tlong long off2 = values2[i];\n\t\tint type2 = stringpool2[off2];\n\t\tchar *s2 = stringpool2 + off2 + 1;\n\n\t\tif (type1 != type2) {\n\t\t\treturn type1 - type2;\n\t\t}\n\t\tint cmp = strcmp(s1, s2);\n\t\tif (cmp != 0) {\n\t\t\treturn cmp;\n\t\t}\n\t}\n\n\tif (keys1.size() < keys2.size()) {\n\t\treturn -1;\n\t} else if (keys1.size() > keys2.size()) {\n\t\treturn 1;\n\t} else {\n\t\treturn 0;\n\t}\n}\n\nvoid rewrite(drawvec &geom, int z, int nextzoom, int maxzoom, long long *bbox, unsigned tx, unsigned ty, int buffer, int *within, std::atomic<long long> *geompos, FILE **geomfile, const char *fname, signed char t, int layer, long long metastart, signed char feature_minzoom, int child_shards, int max_zoom_increment, long long seq, int tippecanoe_minzoom, int tippecanoe_maxzoom, int segment, unsigned *initial_x, unsigned *initial_y, std::vector<long long> &metakeys, std::vector<long long> &metavals, bool has_id, unsigned long long id, unsigned long long index, long long extent) {\n\tif (geom.size() > 0 && (nextzoom <= maxzoom || additional[A_EXTEND_ZOOMS])) {\n\t\tint xo, yo;\n\t\tint span = 1 << (nextzoom - z);\n\n\t\t// Get the feature bounding box in pixel (256) coordinates at the child zoom\n\t\t// in order to calculate which sub-tiles it can touch including the buffer.\n\t\tlong long bbox2[4];\n\t\tint k;\n\t\tfor (k = 0; k < 4; k++) {\n\t\t\t// Division instead of right-shift because coordinates can be negative\n\t\t\tbbox2[k] = bbox[k] / (1 << (32 - nextzoom - 8));\n\t\t}\n\t\t// Decrement the top and left edges so that any features that are\n\t\t// touching the edge can potentially be included in the adjacent tiles too.\n\t\tbbox2[0] -= buffer + 1;\n\t\tbbox2[1] -= buffer + 1;\n\t\tbbox2[2] += buffer;\n\t\tbbox2[3] += buffer;\n\n\t\tfor (k = 0; k < 4; k++) {\n\t\t\tif (bbox2[k] < 0) {\n\t\t\t\tbbox2[k] = 0;\n\t\t\t}\n\t\t\tif (bbox2[k] >= 256 * span) {\n\t\t\t\tbbox2[k] = 256 * (span - 1);\n\t\t\t}\n\n\t\t\tbbox2[k] /= 256;\n\t\t}\n\n\t\t// Offset from tile coordinates back to world coordinates\n\t\tunsigned sx = 0, sy = 0;\n\t\tif (z != 0) {\n\t\t\tsx = tx << (32 - z);\n\t\t\tsy = ty << (32 - z);\n\t\t}\n\n\t\tdrawvec geom2;\n\t\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\t\tgeom2.push_back(draw(geom[i].op, SHIFT_RIGHT(geom[i].x + sx), SHIFT_RIGHT(geom[i].y + sy)));\n\t\t}\n\n\t\tfor (xo = bbox2[0]; xo <= bbox2[2]; xo++) {\n\t\t\tfor (yo = bbox2[1]; yo <= bbox2[3]; yo++) {\n\t\t\t\tunsigned jx = tx * span + xo;\n\t\t\t\tunsigned jy = ty * span + yo;\n\n\t\t\t\t// j is the shard that the child tile's data is being written to.\n\t\t\t\t//\n\t\t\t\t// Be careful: We can't jump more zoom levels than max_zoom_increment\n\t\t\t\t// because that could break the constraint that each of the children\n\t\t\t\t// of the current tile must have its own shard, because the data for\n\t\t\t\t// the child tile must be contiguous within the shard.\n\t\t\t\t//\n\t\t\t\t// But it's OK to spread children across all the shards, not just\n\t\t\t\t// the four that would normally result from splitting one tile,\n\t\t\t\t// because it will go through all the shards when it does the\n\t\t\t\t// next zoom.\n\t\t\t\t//\n\t\t\t\t// If child_shards is a power of 2 but not a power of 4, this will\n\t\t\t\t// shard X more widely than Y. XXX Is there a better way to do this\n\t\t\t\t// without causing collisions?\n\n\t\t\t\tint j = ((jx << max_zoom_increment) |\n\t\t\t\t\t ((jy & ((1 << max_zoom_increment) - 1)))) &\n\t\t\t\t\t(child_shards - 1);\n\n\t\t\t\t{\n\t\t\t\t\tif (!within[j]) {\n\t\t\t\t\t\tserialize_int(geomfile[j], nextzoom, &geompos[j], fname);\n\t\t\t\t\t\tserialize_uint(geomfile[j], tx * span + xo, &geompos[j], fname);\n\t\t\t\t\t\tserialize_uint(geomfile[j], ty * span + yo, &geompos[j], fname);\n\t\t\t\t\t\twithin[j] = 1;\n\t\t\t\t\t}\n\n\t\t\t\t\tserial_feature sf;\n\t\t\t\t\tsf.layer = layer;\n\t\t\t\t\tsf.segment = segment;\n\t\t\t\t\tsf.seq = seq;\n\t\t\t\t\tsf.t = t;\n\t\t\t\t\tsf.has_id = has_id;\n\t\t\t\t\tsf.id = id;\n\t\t\t\t\tsf.has_tippecanoe_minzoom = tippecanoe_minzoom != -1;\n\t\t\t\t\tsf.tippecanoe_minzoom = tippecanoe_minzoom;\n\t\t\t\t\tsf.has_tippecanoe_maxzoom = tippecanoe_maxzoom != -1;\n\t\t\t\t\tsf.tippecanoe_maxzoom = tippecanoe_maxzoom;\n\t\t\t\t\tsf.metapos = metastart;\n\t\t\t\t\tsf.geometry = geom2;\n\t\t\t\t\tsf.index = index;\n\t\t\t\t\tsf.extent = extent;\n\t\t\t\t\tsf.feature_minzoom = feature_minzoom;\n\n\t\t\t\t\tif (metastart < 0) {\n\t\t\t\t\t\tfor (size_t i = 0; i < metakeys.size(); i++) {\n\t\t\t\t\t\t\tsf.keys.push_back(metakeys[i]);\n\t\t\t\t\t\t\tsf.values.push_back(metavals[i]);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tserialize_feature(geomfile[j], &sf, &geompos[j], fname, SHIFT_RIGHT(initial_x[segment]), SHIFT_RIGHT(initial_y[segment]), true);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nstruct accum_state {\n\tdouble sum = 0;\n\tdouble count = 0;\n};\n\nstruct partial {\n\tstd::vector<drawvec> geoms = std::vector<drawvec>();\n\tstd::vector<long long> keys = std::vector<long long>();\n\tstd::vector<long long> values = std::vector<long long>();\n\tstd::vector<std::string> full_keys = std::vector<std::string>();\n\tstd::vector<serial_val> full_values = std::vector<serial_val>();\n\tstd::vector<ssize_t> arc_polygon = std::vector<ssize_t>();\n\tlong long layer = 0;\n\tlong long original_seq = 0;\n\tunsigned long long index = 0;\n\tint segment = 0;\n\tbool reduced = 0;\n\tint z = 0;\n\tint line_detail = 0;\n\tint maxzoom = 0;\n\tdouble spacing = 0;\n\tdouble simplification = 0;\n\tsigned char t = 0;\n\tunsigned long long id = 0;\n\tbool has_id = 0;\n\tssize_t renamed = 0;\n\tlong long extent = 0;\n\tlong long clustered = 0;\n\tstd::set<std::string> need_tilestats;\n\tstd::map<std::string, accum_state> attribute_accum_state;\n};\n\nstruct partial_arg {\n\tstd::vector<struct partial> *partials = NULL;\n\tint task = 0;\n\tint tasks = 0;\n\tdrawvec *shared_nodes;\n};\n\ndrawvec revive_polygon(drawvec &geom, double area, int z, int detail) {\n\t// From area in world coordinates to area in tile coordinates\n\tlong long divisor = 1LL << (32 - detail - z);\n\tarea /= divisor * divisor;\n\n\tif (area == 0) {\n\t\treturn drawvec();\n\t}\n\n\tint height = ceil(sqrt(area));\n\tint width = round(area / height);\n\tif (width == 0) {\n\t\twidth = 1;\n\t}\n\n\tlong long sx = 0, sy = 0, n = 0;\n\tfor (size_t i = 0; i < geom.size(); i++) {\n\t\tif (geom[i].op == VT_MOVETO || geom[i].op == VT_LINETO) {\n\t\t\tsx += geom[i].x;\n\t\t\tsy += geom[i].y;\n\t\t\tn++;\n\t\t}\n\t}\n\n\tif (n > 0) {\n\t\tsx /= n;\n\t\tsy /= n;\n\n\t\tdrawvec out;\n\t\tout.push_back(draw(VT_MOVETO, sx - (width / 2), sy - (height / 2)));\n\t\tout.push_back(draw(VT_LINETO, sx - (width / 2) + width, sy - (height / 2)));\n\t\tout.push_back(draw(VT_LINETO, sx - (width / 2) + width, sy - (height / 2) + height));\n\t\tout.push_back(draw(VT_LINETO, sx - (width / 2), sy - (height / 2) + height));\n\t\tout.push_back(draw(VT_LINETO, sx - (width / 2), sy - (height / 2)));\n\n\t\treturn out;\n\t} else {\n\t\treturn drawvec();\n\t}\n}\n\nvoid *partial_feature_worker(void *v) {\n\tstruct partial_arg *a = (struct partial_arg *) v;\n\tstd::vector<struct partial> *partials = a->partials;\n\n\tfor (size_t i = a->task; i < (*partials).size(); i += a->tasks) {\n\t\tdrawvec geom;\n\n\t\tfor (size_t j = 0; j < (*partials)[i].geoms.size(); j++) {\n\t\t\tfor (size_t k = 0; k < (*partials)[i].geoms[j].size(); k++) {\n\t\t\t\tgeom.push_back((*partials)[i].geoms[j][k]);\n\t\t\t}\n\t\t}\n\n\t\t(*partials)[i].geoms.clear();  // avoid keeping two copies in memory\n\t\tsigned char t = (*partials)[i].t;\n\t\tint z = (*partials)[i].z;\n\t\tint line_detail = (*partials)[i].line_detail;\n\t\tint maxzoom = (*partials)[i].maxzoom;\n\n\t\tif (additional[A_GRID_LOW_ZOOMS] && z < maxzoom) {\n\t\t\tgeom = stairstep(geom, z, line_detail);\n\t\t}\n\n\t\tdouble area = 0;\n\t\tif (t == VT_POLYGON) {\n\t\t\tarea = get_mp_area(geom);\n\t\t}\n\n\t\tif ((t == VT_LINE || t == VT_POLYGON) && !(prevent[P_SIMPLIFY] || (z == maxzoom && prevent[P_SIMPLIFY_LOW]) || (z < maxzoom && additional[A_GRID_LOW_ZOOMS]))) {\n\t\t\tif (1 /* !reduced */) {  // XXX why did this not simplify if reduced?\n\t\t\t\tif (t == VT_LINE) {\n\t\t\t\t\tgeom = remove_noop(geom, t, 32 - z - line_detail);\n\t\t\t\t}\n\n\t\t\t\tbool already_marked = false;\n\t\t\t\tif (additional[A_DETECT_SHARED_BORDERS] && t == VT_POLYGON) {\n\t\t\t\t\talready_marked = true;\n\t\t\t\t}\n\n\t\t\t\tif (!already_marked) {\n\t\t\t\t\tdrawvec ngeom = simplify_lines(geom, z, line_detail, !(prevent[P_CLIPPING] || prevent[P_DUPLICATION]), (*partials)[i].simplification, t == VT_POLYGON ? 4 : 0, *(a->shared_nodes));\n\n\t\t\t\t\tif (t != VT_POLYGON || ngeom.size() >= 3) {\n\t\t\t\t\t\tgeom = ngeom;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n#if 0\n\t\tif (t == VT_LINE && z != basezoom) {\n\t\t\tgeom = shrink_lines(geom, z, line_detail, basezoom, &along);\n\t\t}\n#endif\n\n\t\tif (t == VT_LINE && additional[A_REVERSE]) {\n\t\t\tgeom = reorder_lines(geom);\n\t\t}\n\n\t\tto_tile_scale(geom, z, line_detail);\n\n\t\tstd::vector<drawvec> geoms;\n\t\tgeoms.push_back(geom);\n\n\t\tif (t == VT_POLYGON) {\n\t\t\t// Scaling may have made the polygon degenerate.\n\t\t\t// Give Clipper a chance to try to fix it.\n\t\t\tfor (size_t g = 0; g < geoms.size(); g++) {\n\t\t\t\tdrawvec before = geoms[g];\n\t\t\t\tgeoms[g] = clean_or_clip_poly(geoms[g], 0, 0, false);\n\t\t\t\tif (additional[A_DEBUG_POLYGON]) {\n\t\t\t\t\tcheck_polygon(geoms[g]);\n\t\t\t\t}\n\n\t\t\t\tif (geoms[g].size() < 3) {\n\t\t\t\t\tif (area > 0) {\n\t\t\t\t\t\tgeoms[g] = revive_polygon(before, area / geoms.size(), z, line_detail);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tgeoms[g].clear();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t(*partials)[i].index = i;\n\t\t(*partials)[i].geoms = geoms;\n\t}\n\n\treturn NULL;\n}\n\nint manage_gap(unsigned long long index, unsigned long long *previndex, double scale, double gamma, double *gap) {\n\tif (gamma > 0) {\n\t\tif (*gap > 0) {\n\t\t\tif (index == *previndex) {\n\t\t\t\treturn 1;  // Exact duplicate: can't fulfil the gap requirement\n\t\t\t}\n\n\t\t\tif (index < *previndex || std::exp(std::log((index - *previndex) / scale) * gamma) >= *gap) {\n\t\t\t\t// Dot is further from the previous than the nth root of the gap,\n\t\t\t\t// so produce it, and choose a new gap at the next point.\n\t\t\t\t*gap = 0;\n\t\t\t} else {\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t} else if (index >= *previndex) {\n\t\t\t*gap = (index - *previndex) / scale;\n\n\t\t\tif (*gap == 0) {\n\t\t\t\treturn 1;  // Exact duplicate: skip\n\t\t\t} else if (*gap < 1) {\n\t\t\t\treturn 1;  // Narrow dot spacing: need to stretch out\n\t\t\t} else {\n\t\t\t\t*gap = 0;  // Wider spacing than minimum: so pass through unchanged\n\t\t\t}\n\t\t}\n\n\t\t*previndex = index;\n\t}\n\n\treturn 0;\n}\n\n// Does not fix up moveto/lineto\nstatic drawvec reverse_subring(drawvec const &dv) {\n\tdrawvec out;\n\n\tfor (size_t i = dv.size(); i > 0; i--) {\n\t\tout.push_back(dv[i - 1]);\n\t}\n\n\treturn out;\n}\n\nstruct edge {\n\tunsigned x1 = 0;\n\tunsigned y1 = 0;\n\tunsigned x2 = 0;\n\tunsigned y2 = 0;\n\tunsigned ring = 0;\n\n\tedge(unsigned _x1, unsigned _y1, unsigned _x2, unsigned _y2, unsigned _ring) {\n\t\tx1 = _x1;\n\t\ty1 = _y1;\n\t\tx2 = _x2;\n\t\ty2 = _y2;\n\t\tring = _ring;\n\t}\n\n\tbool operator<(const edge &s) const {\n\t\tlong long cmp = (long long) y1 - s.y1;\n\t\tif (cmp == 0) {\n\t\t\tcmp = (long long) x1 - s.x1;\n\t\t}\n\t\tif (cmp == 0) {\n\t\t\tcmp = (long long) y2 - s.y2;\n\t\t}\n\t\tif (cmp == 0) {\n\t\t\tcmp = (long long) x2 - s.x2;\n\t\t}\n\t\treturn cmp < 0;\n\t}\n};\n\nstruct edgecmp_ring {\n\tbool operator()(const edge &a, const edge &b) {\n\t\tlong long cmp = (long long) a.y1 - b.y1;\n\t\tif (cmp == 0) {\n\t\t\tcmp = (long long) a.x1 - b.x1;\n\t\t}\n\t\tif (cmp == 0) {\n\t\t\tcmp = (long long) a.y2 - b.y2;\n\t\t}\n\t\tif (cmp == 0) {\n\t\t\tcmp = (long long) a.x2 - b.x2;\n\t\t}\n\t\tif (cmp == 0) {\n\t\t\tcmp = (long long) a.ring - b.ring;\n\t\t}\n\t\treturn cmp < 0;\n\t}\n} edgecmp_ring;\n\nbool edges_same(std::pair<std::vector<edge>::iterator, std::vector<edge>::iterator> e1, std::pair<std::vector<edge>::iterator, std::vector<edge>::iterator> e2) {\n\tif ((e2.second - e2.first) != (e1.second - e1.first)) {\n\t\treturn false;\n\t}\n\n\twhile (e1.first != e1.second) {\n\t\tif (e1.first->ring != e2.first->ring) {\n\t\t\treturn false;\n\t\t}\n\n\t\t++e1.first;\n\t\t++e2.first;\n\t}\n\n\treturn true;\n}\n\nbool find_common_edges(std::vector<partial> &partials, int z, int line_detail, double simplification, int maxzoom, double merge_fraction) {\n\tsize_t merge_count = ceil((1 - merge_fraction) * partials.size());\n\n\tfor (size_t i = 0; i < partials.size(); i++) {\n\t\tif (partials[i].t == VT_POLYGON) {\n\t\t\tfor (size_t j = 0; j < partials[i].geoms.size(); j++) {\n\t\t\t\tdrawvec &g = partials[i].geoms[j];\n\t\t\t\tdrawvec out;\n\n\t\t\t\tfor (size_t k = 0; k < g.size(); k++) {\n\t\t\t\t\tif (g[k].op == VT_LINETO && k > 0 && g[k - 1] == g[k]) {\n\t\t\t\t\t\t;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tout.push_back(g[k]);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tpartials[i].geoms[j] = out;\n\t\t\t}\n\t\t}\n\t}\n\n\t// Construct a mapping from all polygon edges to the set of rings\n\t// that each edge appears in. (The ring number is across all polygons;\n\t// we don't need to look it back up, just to tell where it changes.)\n\n\tstd::vector<edge> edges;\n\tsize_t ring = 0;\n\tfor (size_t i = 0; i < partials.size(); i++) {\n\t\tif (partials[i].t == VT_POLYGON) {\n\t\t\tfor (size_t j = 0; j < partials[i].geoms.size(); j++) {\n\t\t\t\tfor (size_t k = 0; k + 1 < partials[i].geoms[j].size(); k++) {\n\t\t\t\t\tif (partials[i].geoms[j][k].op == VT_MOVETO) {\n\t\t\t\t\t\tring++;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (partials[i].geoms[j][k + 1].op == VT_LINETO) {\n\t\t\t\t\t\tdrawvec dv;\n\t\t\t\t\t\tif (partials[i].geoms[j][k] < partials[i].geoms[j][k + 1]) {\n\t\t\t\t\t\t\tdv.push_back(partials[i].geoms[j][k]);\n\t\t\t\t\t\t\tdv.push_back(partials[i].geoms[j][k + 1]);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tdv.push_back(partials[i].geoms[j][k + 1]);\n\t\t\t\t\t\t\tdv.push_back(partials[i].geoms[j][k]);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tedges.push_back(edge(dv[0].x, dv[0].y, dv[1].x, dv[1].y, ring));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tstd::sort(edges.begin(), edges.end(), edgecmp_ring);\n\tstd::set<draw> necessaries;\n\n\t// Now mark all the points where the set of rings using the edge on one side\n\t// is not the same as the set of rings using the edge on the other side.\n\n\tfor (size_t i = 0; i < partials.size(); i++) {\n\t\tif (partials[i].t == VT_POLYGON) {\n\t\t\tfor (size_t j = 0; j < partials[i].geoms.size(); j++) {\n\t\t\t\tdrawvec &g = partials[i].geoms[j];\n\n\t\t\t\tfor (size_t k = 0; k < g.size(); k++) {\n\t\t\t\t\tg[k].necessary = 0;\n\t\t\t\t}\n\n\t\t\t\tfor (size_t a = 0; a < g.size(); a++) {\n\t\t\t\t\tif (g[a].op == VT_MOVETO) {\n\t\t\t\t\t\tsize_t b;\n\n\t\t\t\t\t\tfor (b = a + 1; b < g.size(); b++) {\n\t\t\t\t\t\t\tif (g[b].op != VT_LINETO) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// -1 because of duplication at the end\n\t\t\t\t\t\tsize_t s = b - a - 1;\n\n\t\t\t\t\t\tif (s > 0) {\n\t\t\t\t\t\t\tdrawvec left;\n\t\t\t\t\t\t\tif (g[a + (s - 1) % s] < g[a]) {\n\t\t\t\t\t\t\t\tleft.push_back(g[a + (s - 1) % s]);\n\t\t\t\t\t\t\t\tleft.push_back(g[a]);\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tleft.push_back(g[a]);\n\t\t\t\t\t\t\t\tleft.push_back(g[a + (s - 1) % s]);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif (left[1] < left[0]) {\n\t\t\t\t\t\t\t\tfprintf(stderr, \"left misordered\\n\");\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tstd::pair<std::vector<edge>::iterator, std::vector<edge>::iterator> e1 = std::equal_range(edges.begin(), edges.end(), edge(left[0].x, left[0].y, left[1].x, left[1].y, 0));\n\n\t\t\t\t\t\t\tfor (size_t k = 0; k < s; k++) {\n\t\t\t\t\t\t\t\tdrawvec right;\n\n\t\t\t\t\t\t\t\tif (g[a + k] < g[a + k + 1]) {\n\t\t\t\t\t\t\t\t\tright.push_back(g[a + k]);\n\t\t\t\t\t\t\t\t\tright.push_back(g[a + k + 1]);\n\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\tright.push_back(g[a + k + 1]);\n\t\t\t\t\t\t\t\t\tright.push_back(g[a + k]);\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\tstd::pair<std::vector<edge>::iterator, std::vector<edge>::iterator> e2 = std::equal_range(edges.begin(), edges.end(), edge(right[0].x, right[0].y, right[1].x, right[1].y, 0));\n\n\t\t\t\t\t\t\t\tif (right[1] < right[0]) {\n\t\t\t\t\t\t\t\t\tfprintf(stderr, \"left misordered\\n\");\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\tif (e1.first == e1.second || e2.first == e2.second) {\n\t\t\t\t\t\t\t\t\tfprintf(stderr, \"Internal error: polygon edge lookup failed for %lld,%lld to %lld,%lld or %lld,%lld to %lld,%lld\\n\", left[0].x, left[0].y, left[1].x, left[1].y, right[0].x, right[0].y, right[1].x, right[1].y);\n\t\t\t\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\tif (!edges_same(e1, e2)) {\n\t\t\t\t\t\t\t\t\tg[a + k].necessary = 1;\n\t\t\t\t\t\t\t\t\tnecessaries.insert(g[a + k]);\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\te1 = e2;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\ta = b - 1;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tedges.clear();\n\tstd::map<drawvec, size_t> arcs;\n\tstd::multimap<ssize_t, size_t> merge_candidates;  // from arc to partial\n\n\t// Roll rings that include a necessary point around so they start at one\n\n\tfor (size_t i = 0; i < partials.size(); i++) {\n\t\tif (partials[i].t == VT_POLYGON) {\n\t\t\tfor (size_t j = 0; j < partials[i].geoms.size(); j++) {\n\t\t\t\tdrawvec &g = partials[i].geoms[j];\n\n\t\t\t\tfor (size_t k = 0; k < g.size(); k++) {\n\t\t\t\t\tif (necessaries.count(g[k]) != 0) {\n\t\t\t\t\t\tg[k].necessary = 1;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tfor (size_t k = 0; k < g.size(); k++) {\n\t\t\t\t\tif (g[k].op == VT_MOVETO) {\n\t\t\t\t\t\tssize_t necessary = -1;\n\t\t\t\t\t\tssize_t lowest = k;\n\t\t\t\t\t\tsize_t l;\n\t\t\t\t\t\tfor (l = k + 1; l < g.size(); l++) {\n\t\t\t\t\t\t\tif (g[l].op != VT_LINETO) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tif (g[l].necessary) {\n\t\t\t\t\t\t\t\tnecessary = l;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif (g[l] < g[lowest]) {\n\t\t\t\t\t\t\t\tlowest = l;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (necessary < 0) {\n\t\t\t\t\t\t\tnecessary = lowest;\n\t\t\t\t\t\t\t// Add a necessary marker if there was none in the ring,\n\t\t\t\t\t\t\t// so the arc code below can find it.\n\t\t\t\t\t\t\tg[lowest].necessary = 1;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tdrawvec tmp;\n\n\t\t\t\t\t\t\t// l - 1 because the endpoint is duplicated\n\t\t\t\t\t\t\tfor (size_t m = necessary; m < l - 1; m++) {\n\t\t\t\t\t\t\t\ttmp.push_back(g[m]);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tfor (ssize_t m = k; m < necessary; m++) {\n\t\t\t\t\t\t\t\ttmp.push_back(g[m]);\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// replace the endpoint\n\t\t\t\t\t\t\ttmp.push_back(g[necessary]);\n\n\t\t\t\t\t\t\tif (tmp.size() != l - k) {\n\t\t\t\t\t\t\t\tfprintf(stderr, \"internal error shifting ring\\n\");\n\t\t\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tfor (size_t m = 0; m < tmp.size(); m++) {\n\t\t\t\t\t\t\t\tif (m == 0) {\n\t\t\t\t\t\t\t\t\ttmp[m].op = VT_MOVETO;\n\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\ttmp[m].op = VT_LINETO;\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\tg[k + m] = tmp[m];\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// Now peel off each set of segments from one necessary point to the next\n\t\t\t\t\t\t// into an \"arc\" as in TopoJSON\n\n\t\t\t\t\t\tfor (size_t m = k; m < l; m++) {\n\t\t\t\t\t\t\tif (!g[m].necessary) {\n\t\t\t\t\t\t\t\tfprintf(stderr, \"internal error in arc building\\n\");\n\t\t\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tdrawvec arc;\n\t\t\t\t\t\t\tsize_t n;\n\t\t\t\t\t\t\tfor (n = m; n < l; n++) {\n\t\t\t\t\t\t\t\tarc.push_back(g[n]);\n\t\t\t\t\t\t\t\tif (n > m && g[n].necessary) {\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tauto f = arcs.find(arc);\n\t\t\t\t\t\t\tif (f == arcs.end()) {\n\t\t\t\t\t\t\t\tdrawvec arc2 = reverse_subring(arc);\n\n\t\t\t\t\t\t\t\tauto f2 = arcs.find(arc2);\n\t\t\t\t\t\t\t\tif (f2 == arcs.end()) {\n\t\t\t\t\t\t\t\t\t// Add new arc\n\t\t\t\t\t\t\t\t\tsize_t added = arcs.size() + 1;\n\t\t\t\t\t\t\t\t\tarcs.insert(std::pair<drawvec, size_t>(arc, added));\n\t\t\t\t\t\t\t\t\tpartials[i].arc_polygon.push_back(added);\n\t\t\t\t\t\t\t\t\tmerge_candidates.insert(std::pair<ssize_t, size_t>(added, i));\n\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\tpartials[i].arc_polygon.push_back(-(ssize_t) f2->second);\n\t\t\t\t\t\t\t\t\tmerge_candidates.insert(std::pair<ssize_t, size_t>(-(ssize_t) f2->second, i));\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tpartials[i].arc_polygon.push_back(f->second);\n\t\t\t\t\t\t\t\tmerge_candidates.insert(std::pair<ssize_t, size_t>(f->second, i));\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tm = n - 1;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tpartials[i].arc_polygon.push_back(0);\n\n\t\t\t\t\t\tk = l - 1;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Simplify each arc\n\n\tstd::vector<drawvec> simplified_arcs;\n\n\tsize_t count = 0;\n\tfor (auto ai = arcs.begin(); ai != arcs.end(); ++ai) {\n\t\tif (simplified_arcs.size() < ai->second + 1) {\n\t\t\tsimplified_arcs.resize(ai->second + 1);\n\t\t}\n\n\t\tdrawvec dv = ai->first;\n\t\tfor (size_t i = 0; i < dv.size(); i++) {\n\t\t\tif (i == 0) {\n\t\t\t\tdv[i].op = VT_MOVETO;\n\t\t\t} else {\n\t\t\t\tdv[i].op = VT_LINETO;\n\t\t\t}\n\t\t}\n\t\tif (!(prevent[P_SIMPLIFY] || (z == maxzoom && prevent[P_SIMPLIFY_LOW]) || (z < maxzoom && additional[A_GRID_LOW_ZOOMS]))) {\n\t\t\tsimplified_arcs[ai->second] = simplify_lines(dv, z, line_detail, !(prevent[P_CLIPPING] || prevent[P_DUPLICATION]), simplification, 4, drawvec());\n\t\t} else {\n\t\t\tsimplified_arcs[ai->second] = dv;\n\t\t}\n\t\tcount++;\n\t}\n\n\t// If necessary, merge some adjacent polygons into some other polygons\n\n\tstruct merge_order {\n\t\tssize_t edge = 0;\n\t\tunsigned long long gap = 0;\n\t\tsize_t p1 = 0;\n\t\tsize_t p2 = 0;\n\n\t\tbool operator<(const merge_order &m) const {\n\t\t\treturn gap < m.gap;\n\t\t}\n\t};\n\tstd::vector<merge_order> order;\n\n\tfor (ssize_t i = 0; i < (ssize_t) simplified_arcs.size(); i++) {\n\t\tauto r1 = merge_candidates.equal_range(i);\n\t\tfor (auto r1i = r1.first; r1i != r1.second; ++r1i) {\n\t\t\tauto r2 = merge_candidates.equal_range(-i);\n\t\t\tfor (auto r2i = r2.first; r2i != r2.second; ++r2i) {\n\t\t\t\tif (r1i->second != r2i->second) {\n\t\t\t\t\tmerge_order mo;\n\t\t\t\t\tmo.edge = i;\n\t\t\t\t\tif (partials[r1i->second].index > partials[r2i->second].index) {\n\t\t\t\t\t\tmo.gap = partials[r1i->second].index - partials[r2i->second].index;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmo.gap = partials[r2i->second].index - partials[r1i->second].index;\n\t\t\t\t\t}\n\t\t\t\t\tmo.p1 = r1i->second;\n\t\t\t\t\tmo.p2 = r2i->second;\n\t\t\t\t\torder.push_back(mo);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tstd::sort(order.begin(), order.end());\n\n\tsize_t merged = 0;\n\tfor (size_t o = 0; o < order.size(); o++) {\n\t\tif (merged >= merge_count) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsize_t i = order[o].p1;\n\t\twhile (partials[i].renamed >= 0) {\n\t\t\ti = partials[i].renamed;\n\t\t}\n\t\tsize_t i2 = order[o].p2;\n\t\twhile (partials[i2].renamed >= 0) {\n\t\t\ti2 = partials[i2].renamed;\n\t\t}\n\n\t\tfor (size_t j = 0; j < partials[i].arc_polygon.size() && merged < merge_count; j++) {\n\t\t\tif (partials[i].arc_polygon[j] == order[o].edge) {\n\t\t\t\t{\n\t\t\t\t\t// XXX snap links\n\t\t\t\t\tif (partials[order[o].p2].arc_polygon.size() > 0) {\n\t\t\t\t\t\t// This has to merge the ring that contains the anti-arc to this arc\n\t\t\t\t\t\t// into the current ring, and then add whatever other rings were in\n\t\t\t\t\t\t// that feature on to the end.\n\t\t\t\t\t\t//\n\t\t\t\t\t\t// This can't be good for keeping parent-child relationships among\n\t\t\t\t\t\t// the rings in order, but Wagyu should sort that out later\n\n\t\t\t\t\t\tstd::vector<ssize_t> additions;\n\t\t\t\t\t\tstd::vector<ssize_t> &here = partials[i].arc_polygon;\n\t\t\t\t\t\tstd::vector<ssize_t> &other = partials[i2].arc_polygon;\n\n#if 0\n\t\t\t\t\t\tprintf(\"seeking %zd\\n\", partials[i].arc_polygon[j]);\n\t\t\t\t\t\tprintf(\"before: \");\n\t\t\t\t\t\tfor (size_t k = 0; k < here.size(); k++) {\n\t\t\t\t\t\t\tprintf(\"%zd \", here[k]);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tprintf(\"\\n\");\n\t\t\t\t\t\tprintf(\"other: \");\n\t\t\t\t\t\tfor (size_t k = 0; k < other.size(); k++) {\n\t\t\t\t\t\t\tprintf(\"%zd \", other[k]);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tprintf(\"\\n\");\n#endif\n\n\t\t\t\t\t\tfor (size_t k = 0; k < other.size(); k++) {\n\t\t\t\t\t\t\tsize_t l;\n\t\t\t\t\t\t\tfor (l = k; l < other.size(); l++) {\n\t\t\t\t\t\t\t\tif (other[l] == 0) {\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif (l >= other.size()) {\n\t\t\t\t\t\t\t\tl--;\n\t\t\t\t\t\t\t}\n\n#if 0\n\t\t\t\t\t\t\tfor (size_t m = k; m <= l; m++) {\n\t\t\t\t\t\t\t\tprintf(\"%zd \", other[m]);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tprintf(\"\\n\");\n#endif\n\n\t\t\t\t\t\t\tsize_t m;\n\t\t\t\t\t\t\tfor (m = k; m <= l; m++) {\n\t\t\t\t\t\t\t\tif (other[m] == -partials[i].arc_polygon[j]) {\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tif (m <= l) {\n\t\t\t\t\t\t\t\t// Found the shared arc\n\n\t\t\t\t\t\t\t\there.erase(here.begin() + j);\n\n\t\t\t\t\t\t\t\tsize_t off = 0;\n\t\t\t\t\t\t\t\tfor (size_t n = m + 1; n < l; n++) {\n\t\t\t\t\t\t\t\t\there.insert(here.begin() + j + off, other[n]);\n\t\t\t\t\t\t\t\t\toff++;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tfor (size_t n = k; n < m; n++) {\n\t\t\t\t\t\t\t\t\there.insert(here.begin() + j + off, other[n]);\n\t\t\t\t\t\t\t\t\toff++;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t// Looking at some other ring\n\n\t\t\t\t\t\t\t\tfor (size_t n = k; n <= l; n++) {\n\t\t\t\t\t\t\t\t\tadditions.push_back(other[n]);\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tk = l;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tpartials[i2].arc_polygon.clear();\n\t\t\t\t\t\tpartials[i2].renamed = i;\n\t\t\t\t\t\tmerged++;\n\n\t\t\t\t\t\tfor (size_t k = 0; k < additions.size(); k++) {\n\t\t\t\t\t\t\tpartials[i].arc_polygon.push_back(additions[k]);\n\t\t\t\t\t\t}\n\n#if 0\n\t\t\t\t\t\tprintf(\"after: \");\n\t\t\t\t\t\tfor (size_t k = 0; k < here.size(); k++) {\n\t\t\t\t\t\t\tprintf(\"%zd \", here[k]);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tprintf(\"\\n\");\n#endif\n\n#if 0\n\t\t\t\t\t\tfor (size_t k = 0; k + 1 < here.size(); k++) {\n\t\t\t\t\t\t\tif (here[k] != 0 && here[k + 1] != 0) {\n\t\t\t\t\t\t\t\tif (simplified_arcs[here[k + 1]][0] != simplified_arcs[here[k]][simplified_arcs[here[k]].size() - 1]) {\n\t\t\t\t\t\t\t\t\tprintf(\"error from %zd to %zd\\n\", here[k], here[k + 1]);\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n#endif\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Turn the arc representations of the polygons back into standard polygon geometries\n\n\tfor (size_t i = 0; i < partials.size(); i++) {\n\t\tif (partials[i].t == VT_POLYGON) {\n\t\t\tpartials[i].geoms.resize(0);\n\t\t\tpartials[i].geoms.push_back(drawvec());\n\t\t\tbool at_start = true;\n\t\t\tdraw first(-1, 0, 0);\n\n\t\t\tfor (size_t j = 0; j < partials[i].arc_polygon.size(); j++) {\n\t\t\t\tssize_t p = partials[i].arc_polygon[j];\n\n\t\t\t\tif (p == 0) {\n\t\t\t\t\tif (first.op >= 0) {\n\t\t\t\t\t\tpartials[i].geoms[0].push_back(first);\n\t\t\t\t\t\tfirst = draw(-1, 0, 0);\n\t\t\t\t\t}\n\t\t\t\t\tat_start = true;\n\t\t\t\t} else if (p > 0) {\n\t\t\t\t\tfor (size_t k = 0; k + 1 < simplified_arcs[p].size(); k++) {\n\t\t\t\t\t\tif (at_start) {\n\t\t\t\t\t\t\tpartials[i].geoms[0].push_back(draw(VT_MOVETO, simplified_arcs[p][k].x, simplified_arcs[p][k].y));\n\t\t\t\t\t\t\tfirst = draw(VT_LINETO, simplified_arcs[p][k].x, simplified_arcs[p][k].y);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tpartials[i].geoms[0].push_back(draw(VT_LINETO, simplified_arcs[p][k].x, simplified_arcs[p][k].y));\n\t\t\t\t\t\t}\n\t\t\t\t\t\tat_start = 0;\n\t\t\t\t\t}\n\t\t\t\t} else { /* p < 0 */\n\t\t\t\t\tfor (ssize_t k = simplified_arcs[-p].size() - 1; k > 0; k--) {\n\t\t\t\t\t\tif (at_start) {\n\t\t\t\t\t\t\tpartials[i].geoms[0].push_back(draw(VT_MOVETO, simplified_arcs[-p][k].x, simplified_arcs[-p][k].y));\n\t\t\t\t\t\t\tfirst = draw(VT_LINETO, simplified_arcs[-p][k].x, simplified_arcs[-p][k].y);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tpartials[i].geoms[0].push_back(draw(VT_LINETO, simplified_arcs[-p][k].x, simplified_arcs[-p][k].y));\n\t\t\t\t\t\t}\n\t\t\t\t\t\tat_start = 0;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (merged >= merge_count) {\n\t\treturn true;\n\t} else {\n\t\treturn false;\n\t}\n}\n\nunsigned long long choose_mingap(std::vector<unsigned long long> const &indices, double f) {\n\tunsigned long long bot = ULLONG_MAX;\n\tunsigned long long top = 0;\n\n\tfor (size_t i = 0; i < indices.size(); i++) {\n\t\tif (i > 0 && indices[i] >= indices[i - 1]) {\n\t\t\tif (indices[i] - indices[i - 1] > top) {\n\t\t\t\ttop = indices[i] - indices[i - 1];\n\t\t\t}\n\t\t\tif (indices[i] - indices[i - 1] < bot) {\n\t\t\t\tbot = indices[i] - indices[i - 1];\n\t\t\t}\n\t\t}\n\t}\n\n\tsize_t want = indices.size() * f;\n\twhile (top - bot > 2) {\n\t\tunsigned long long guess = bot / 2 + top / 2;\n\t\tsize_t count = 0;\n\t\tunsigned long long prev = 0;\n\n\t\tfor (size_t i = 0; i < indices.size(); i++) {\n\t\t\tif (indices[i] - prev >= guess) {\n\t\t\t\tcount++;\n\t\t\t\tprev = indices[i];\n\t\t\t}\n\t\t}\n\n\t\tif (count > want) {\n\t\t\tbot = guess;\n\t\t} else if (count < want) {\n\t\t\ttop = guess;\n\t\t} else {\n\t\t\treturn guess;\n\t\t}\n\t}\n\n\treturn top;\n}\n\nlong long choose_minextent(std::vector<long long> &extents, double f) {\n\tstd::sort(extents.begin(), extents.end());\n\treturn extents[(extents.size() - 1) * (1 - f)];\n}\n\nstruct write_tile_args {\n\tstruct task *tasks = NULL;\n\tchar *metabase = NULL;\n\tchar *stringpool = NULL;\n\tint min_detail = 0;\n\tsqlite3 *outdb = NULL;\n\tconst char *outdir = NULL;\n\tint buffer = 0;\n\tconst char *fname = NULL;\n\tFILE **geomfile = NULL;\n\tdouble todo = 0;\n\tstd::atomic<long long> *along = NULL;\n\tdouble gamma = 0;\n\tdouble gamma_out = 0;\n\tint child_shards = 0;\n\tint *geomfd = NULL;\n\toff_t *geom_size = NULL;\n\tstd::atomic<unsigned> *midx = NULL;\n\tstd::atomic<unsigned> *midy = NULL;\n\tint maxzoom = 0;\n\tint minzoom = 0;\n\tint full_detail = 0;\n\tint low_detail = 0;\n\tdouble simplification = 0;\n\tstd::atomic<long long> *most = NULL;\n\tlong long *meta_off = NULL;\n\tlong long *pool_off = NULL;\n\tunsigned *initial_x = NULL;\n\tunsigned *initial_y = NULL;\n\tstd::atomic<int> *running = NULL;\n\tint err = 0;\n\tstd::vector<std::map<std::string, layermap_entry>> *layermaps = NULL;\n\tstd::vector<std::vector<std::string>> *layer_unmaps = NULL;\n\tsize_t pass = 0;\n\tsize_t passes = 0;\n\tunsigned long long mingap = 0;\n\tunsigned long long mingap_out = 0;\n\tlong long minextent = 0;\n\tlong long minextent_out = 0;\n\tdouble fraction = 0;\n\tdouble fraction_out = 0;\n\tconst char *prefilter = NULL;\n\tconst char *postfilter = NULL;\n\tstd::map<std::string, attribute_op> const *attribute_accum = NULL;\n\tbool still_dropping = false;\n\tint wrote_zoom = 0;\n\tsize_t tiling_seg = 0;\n\tstruct json_object *filter = NULL;\n};\n\nbool clip_to_tile(serial_feature &sf, int z, long long buffer) {\n\tint quick = quick_check(sf.bbox, z, buffer);\n\n\tif (z == 0) {\n\t\tif (sf.bbox[0] <= (1LL << 32) * buffer / 256 || sf.bbox[2] >= (1LL << 32) - ((1LL << 32) * buffer / 256)) {\n\t\t\t// If the geometry extends off the edge of the world, concatenate on another copy\n\t\t\t// shifted by 360 degrees, and then make sure both copies get clipped down to size.\n\n\t\t\tsize_t n = sf.geometry.size();\n\n\t\t\tif (sf.bbox[0] <= (1LL << 32) * buffer / 256) {\n\t\t\t\tfor (size_t i = 0; i < n; i++) {\n\t\t\t\t\tsf.geometry.push_back(draw(sf.geometry[i].op, sf.geometry[i].x + (1LL << 32), sf.geometry[i].y));\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (sf.bbox[2] >= (1LL << 32) - ((1LL << 32) * buffer / 256)) {\n\t\t\t\tfor (size_t i = 0; i < n; i++) {\n\t\t\t\t\tsf.geometry.push_back(draw(sf.geometry[i].op, sf.geometry[i].x - (1LL << 32), sf.geometry[i].y));\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tsf.bbox[0] = 0;\n\t\t\tsf.bbox[2] = 1LL << 32;\n\n\t\t\tquick = -1;\n\t\t}\n\t}\n\n\tif (quick == 0) {\n\t\treturn true;\n\t}\n\n\t// Can't accept the quick check if guaranteeing no duplication, since the\n\t// overlap might have been in the buffer.\n\tif (quick != 1 || prevent[P_DUPLICATION]) {\n\t\tdrawvec clipped;\n\n\t\t// Do the clipping, even if we are going to include the whole feature,\n\t\t// so that we can know whether the feature itself, or only the feature's\n\t\t// bounding box, touches the tile.\n\n\t\tif (sf.t == VT_LINE) {\n\t\t\tclipped = clip_lines(sf.geometry, z, buffer);\n\t\t}\n\t\tif (sf.t == VT_POLYGON) {\n\t\t\tclipped = simple_clip_poly(sf.geometry, z, buffer);\n\t\t}\n\t\tif (sf.t == VT_POINT) {\n\t\t\tclipped = clip_point(sf.geometry, z, buffer);\n\t\t}\n\n\t\tclipped = remove_noop(clipped, sf.t, 0);\n\n\t\t// Must clip at z0 even if we don't want clipping, to handle features\n\t\t// that are duplicated across the date line\n\n\t\tif (prevent[P_DUPLICATION] && z != 0) {\n\t\t\tif (point_within_tile((sf.bbox[0] + sf.bbox[2]) / 2, (sf.bbox[1] + sf.bbox[3]) / 2, z)) {\n\t\t\t\t// sf.geometry is unchanged\n\t\t\t} else {\n\t\t\t\tsf.geometry.clear();\n\t\t\t}\n\t\t} else if (prevent[P_CLIPPING] && z != 0) {\n\t\t\tif (clipped.size() == 0) {\n\t\t\t\tsf.geometry.clear();\n\t\t\t} else {\n\t\t\t\t// sf.geometry is unchanged\n\t\t\t}\n\t\t} else {\n\t\t\tsf.geometry = clipped;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nvoid remove_attributes(serial_feature &sf, std::set<std::string> const &exclude_attributes, const char *stringpool, long long *pool_off) {\n\tfor (ssize_t i = sf.keys.size() - 1; i >= 0; i--) {\n\t\tstd::string key = stringpool + pool_off[sf.segment] + sf.keys[i] + 1;\n\t\tif (exclude_attributes.count(key) > 0) {\n\t\t\tsf.keys.erase(sf.keys.begin() + i);\n\t\t\tsf.values.erase(sf.values.begin() + i);\n\t\t}\n\t}\n\n\tfor (ssize_t i = sf.full_keys.size() - 1; i >= 0; i--) {\n\t\tstd::string key = sf.full_keys[i];\n\t\tif (exclude_attributes.count(key) > 0) {\n\t\t\tsf.full_keys.erase(sf.full_keys.begin() + i);\n\t\t\tsf.full_values.erase(sf.full_values.begin() + i);\n\t\t}\n\t}\n}\n\nserial_feature next_feature(FILE *geoms, std::atomic<long long> *geompos_in, char *metabase, long long *meta_off, int z, unsigned tx, unsigned ty, unsigned *initial_x, unsigned *initial_y, long long *original_features, long long *unclipped_features, int nextzoom, int maxzoom, int minzoom, int max_zoom_increment, size_t pass, size_t passes, std::atomic<long long> *along, long long alongminus, int buffer, int *within, bool *first_time, FILE **geomfile, std::atomic<long long> *geompos, std::atomic<double> *oprogress, double todo, const char *fname, int child_shards, struct json_object *filter, const char *stringpool, long long *pool_off, std::vector<std::vector<std::string>> *layer_unmaps) {\n\twhile (1) {\n\t\tserial_feature sf = deserialize_feature(geoms, geompos_in, metabase, meta_off, z, tx, ty, initial_x, initial_y);\n\t\tif (sf.t < 0) {\n\t\t\treturn sf;\n\t\t}\n\n\t\tdouble progress = floor(((((*geompos_in + *along - alongminus) / (double) todo) + (pass - (2 - passes))) / passes + z) / (maxzoom + 1) * 1000) / 10;\n\t\tif (progress >= *oprogress + 0.1) {\n\t\t\tif (!quiet && !quiet_progress && progress_time()) {\n\t\t\t\tfprintf(stderr, \"  %3.1f%%  %d/%u/%u  \\r\", progress, z, tx, ty);\n\t\t\t}\n\t\t\t*oprogress = progress;\n\t\t}\n\n\t\t(*original_features)++;\n\n\t\tif (clip_to_tile(sf, z, buffer)) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (sf.geometry.size() > 0) {\n\t\t\t(*unclipped_features)++;\n\t\t}\n\n\t\tif (*first_time && pass == 1) { /* only write out the next zoom once, even if we retry */\n\t\t\tif (sf.tippecanoe_maxzoom == -1 || sf.tippecanoe_maxzoom >= nextzoom) {\n\t\t\t\trewrite(sf.geometry, z, nextzoom, maxzoom, sf.bbox, tx, ty, buffer, within, geompos, geomfile, fname, sf.t, sf.layer, sf.metapos, sf.feature_minzoom, child_shards, max_zoom_increment, sf.seq, sf.tippecanoe_minzoom, sf.tippecanoe_maxzoom, sf.segment, initial_x, initial_y, sf.keys, sf.values, sf.has_id, sf.id, sf.index, sf.extent);\n\t\t\t}\n\t\t}\n\n\t\tif (z < minzoom) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (sf.tippecanoe_minzoom != -1 && z < sf.tippecanoe_minzoom) {\n\t\t\tcontinue;\n\t\t}\n\t\tif (sf.tippecanoe_maxzoom != -1 && z > sf.tippecanoe_maxzoom) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (filter != NULL) {\n\t\t\tstd::map<std::string, mvt_value> attributes;\n\t\t\tstd::string layername = (*layer_unmaps)[sf.segment][sf.layer];\n\t\t\tstd::set<std::string> exclude_attributes;\n\n\t\t\tfor (size_t i = 0; i < sf.keys.size(); i++) {\n\t\t\t\tstd::string key = stringpool + pool_off[sf.segment] + sf.keys[i] + 1;\n\n\t\t\t\tserial_val sv;\n\t\t\t\tsv.type = (stringpool + pool_off[sf.segment])[sf.values[i]];\n\t\t\t\tsv.s = stringpool + pool_off[sf.segment] + sf.values[i] + 1;\n\n\t\t\t\tmvt_value val = stringified_to_mvt_value(sv.type, sv.s.c_str());\n\t\t\t\tattributes.insert(std::pair<std::string, mvt_value>(key, val));\n\t\t\t}\n\n\t\t\tfor (size_t i = 0; i < sf.full_keys.size(); i++) {\n\t\t\t\tstd::string key = sf.full_keys[i];\n\t\t\t\tmvt_value val = stringified_to_mvt_value(sf.full_values[i].type, sf.full_values[i].s.c_str());\n\n\t\t\t\tattributes.insert(std::pair<std::string, mvt_value>(key, val));\n\t\t\t}\n\n\t\t\tif (sf.has_id) {\n\t\t\t\tmvt_value v;\n\t\t\t\tv.type = mvt_uint;\n\t\t\t\tv.numeric_value.uint_value = sf.id;\n\n\t\t\t\tattributes.insert(std::pair<std::string, mvt_value>(\"$id\", v));\n\t\t\t}\n\n\t\t\tmvt_value v;\n\t\t\tv.type = mvt_string;\n\n\t\t\tif (sf.t == mvt_point) {\n\t\t\t\tv.string_value = \"Point\";\n\t\t\t} else if (sf.t == mvt_linestring) {\n\t\t\t\tv.string_value = \"LineString\";\n\t\t\t} else if (sf.t == mvt_polygon) {\n\t\t\t\tv.string_value = \"Polygon\";\n\t\t\t}\n\n\t\t\tattributes.insert(std::pair<std::string, mvt_value>(\"$type\", v));\n\n\t\t\tmvt_value v2;\n\t\t\tv2.type = mvt_uint;\n\t\t\tv2.numeric_value.uint_value = z;\n\n\t\t\tattributes.insert(std::pair<std::string, mvt_value>(\"$zoom\", v2));\n\n\t\t\tif (!evaluate(attributes, layername, filter, exclude_attributes)) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (exclude_attributes.size() > 0) {\n\t\t\t\tremove_attributes(sf, exclude_attributes, stringpool, pool_off);\n\t\t\t}\n\t\t}\n\n\t\tif (sf.tippecanoe_minzoom == -1 && z < sf.feature_minzoom) {\n\t\t\tsf.dropped = true;\n\t\t}\n\n\t\t// Remove nulls, now that the expression evaluation filter has run\n\n\t\tfor (ssize_t i = (ssize_t) sf.keys.size() - 1; i >= 0; i--) {\n\t\t\tint type = (stringpool + pool_off[sf.segment])[sf.values[i]];\n\n\t\t\tif (type == mvt_null) {\n\t\t\t\tsf.keys.erase(sf.keys.begin() + i);\n\t\t\t\tsf.values.erase(sf.values.begin() + i);\n\t\t\t}\n\t\t}\n\n\t\tfor (ssize_t i = (ssize_t) sf.full_keys.size() - 1; i >= 0; i--) {\n\t\t\tif (sf.full_values[i].type == mvt_null) {\n\t\t\t\tsf.full_keys.erase(sf.full_keys.begin() + i);\n\t\t\t\tsf.full_values.erase(sf.full_values.begin() + i);\n\t\t\t}\n\t\t}\n\n\t\treturn sf;\n\t}\n}\n\nstruct run_prefilter_args {\n\tFILE *geoms = NULL;\n\tstd::atomic<long long> *geompos_in = NULL;\n\tchar *metabase = NULL;\n\tlong long *meta_off = NULL;\n\tint z = 0;\n\tunsigned tx = 0;\n\tunsigned ty = 0;\n\tunsigned *initial_x = 0;\n\tunsigned *initial_y = 0;\n\tlong long *original_features = 0;\n\tlong long *unclipped_features = 0;\n\tint nextzoom = 0;\n\tint maxzoom = 0;\n\tint minzoom = 0;\n\tint max_zoom_increment = 0;\n\tsize_t pass = 0;\n\tsize_t passes = 0;\n\tstd::atomic<long long> *along = 0;\n\tlong long alongminus = 0;\n\tint buffer = 0;\n\tint *within = NULL;\n\tbool *first_time = NULL;\n\tFILE **geomfile = NULL;\n\tstd::atomic<long long> *geompos = NULL;\n\tstd::atomic<double> *oprogress = NULL;\n\tdouble todo = 0;\n\tconst char *fname = 0;\n\tint child_shards = 0;\n\tstd::vector<std::vector<std::string>> *layer_unmaps = NULL;\n\tchar *stringpool = NULL;\n\tlong long *pool_off = NULL;\n\tFILE *prefilter_fp = NULL;\n\tstruct json_object *filter = NULL;\n};\n\nvoid *run_prefilter(void *v) {\n\trun_prefilter_args *rpa = (run_prefilter_args *) v;\n\tjson_writer state(rpa->prefilter_fp);\n\n\twhile (1) {\n\t\tserial_feature sf = next_feature(rpa->geoms, rpa->geompos_in, rpa->metabase, rpa->meta_off, rpa->z, rpa->tx, rpa->ty, rpa->initial_x, rpa->initial_y, rpa->original_features, rpa->unclipped_features, rpa->nextzoom, rpa->maxzoom, rpa->minzoom, rpa->max_zoom_increment, rpa->pass, rpa->passes, rpa->along, rpa->alongminus, rpa->buffer, rpa->within, rpa->first_time, rpa->geomfile, rpa->geompos, rpa->oprogress, rpa->todo, rpa->fname, rpa->child_shards, rpa->filter, rpa->stringpool, rpa->pool_off, rpa->layer_unmaps);\n\t\tif (sf.t < 0) {\n\t\t\tbreak;\n\t\t}\n\n\t\tmvt_layer tmp_layer;\n\t\ttmp_layer.extent = 1LL << 32;\n\t\ttmp_layer.name = (*(rpa->layer_unmaps))[sf.segment][sf.layer];\n\n\t\tif (sf.t == VT_POLYGON) {\n\t\t\tsf.geometry = close_poly(sf.geometry);\n\t\t}\n\n\t\tmvt_feature tmp_feature;\n\t\ttmp_feature.type = sf.t;\n\t\ttmp_feature.geometry = to_feature(sf.geometry);\n\t\ttmp_feature.id = sf.id;\n\t\ttmp_feature.has_id = sf.has_id;\n\t\ttmp_feature.dropped = sf.dropped;\n\n\t\t// Offset from tile coordinates back to world coordinates\n\t\tunsigned sx = 0, sy = 0;\n\t\tif (rpa->z != 0) {\n\t\t\tsx = rpa->tx << (32 - rpa->z);\n\t\t\tsy = rpa->ty << (32 - rpa->z);\n\t\t}\n\t\tfor (size_t i = 0; i < tmp_feature.geometry.size(); i++) {\n\t\t\ttmp_feature.geometry[i].x += sx;\n\t\t\ttmp_feature.geometry[i].y += sy;\n\t\t}\n\n\t\tdecode_meta(sf.keys, sf.values, rpa->stringpool + rpa->pool_off[sf.segment], tmp_layer, tmp_feature);\n\t\ttmp_layer.features.push_back(tmp_feature);\n\n\t\tlayer_to_geojson(tmp_layer, 0, 0, 0, false, true, false, true, sf.index, sf.seq, sf.extent, true, state);\n\t}\n\n\tif (fclose(rpa->prefilter_fp) != 0) {\n\t\tif (errno == EPIPE) {\n\t\t\tstatic bool warned = false;\n\t\t\tif (!warned) {\n\t\t\t\tfprintf(stderr, \"Warning: broken pipe in prefilter\\n\");\n\t\t\t\twarned = true;\n\t\t\t}\n\t\t} else {\n\t\t\tperror(\"fclose output to prefilter\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\treturn NULL;\n}\n\nvoid add_tilestats(std::string const &layername, int z, std::vector<std::map<std::string, layermap_entry>> *layermaps, size_t tiling_seg, std::vector<std::vector<std::string>> *layer_unmaps, std::string const &key, serial_val const &val) {\n\tstd::map<std::string, layermap_entry> &layermap = (*layermaps)[tiling_seg];\n\tif (layermap.count(layername) == 0) {\n\t\tlayermap_entry lme = layermap_entry(layermap.size());\n\t\tlme.minzoom = z;\n\t\tlme.maxzoom = z;\n\t\tlme.retain = 1;\n\n\t\tlayermap.insert(std::pair<std::string, layermap_entry>(layername, lme));\n\n\t\tif (lme.id >= (*layer_unmaps)[tiling_seg].size()) {\n\t\t\t(*layer_unmaps)[tiling_seg].resize(lme.id + 1);\n\t\t\t(*layer_unmaps)[tiling_seg][lme.id] = layername;\n\t\t}\n\t}\n\tauto fk = layermap.find(layername);\n\tif (fk == layermap.end()) {\n\t\tfprintf(stderr, \"Internal error: layer %s not found\\n\", layername.c_str());\n\t\texit(EXIT_FAILURE);\n\t}\n\n\ttype_and_string attrib;\n\tattrib.type = val.type;\n\tattrib.string = val.s;\n\n\tadd_to_file_keys(fk->second.file_keys, key, attrib);\n}\n\nvoid preserve_attribute(attribute_op op, serial_feature &, char *stringpool, long long *pool_off, std::string &key, serial_val &val, partial &p) {\n\tif (p.need_tilestats.count(key) == 0) {\n\t\tp.need_tilestats.insert(key);\n\t}\n\n\t// If the feature being merged into has this key as a metadata reference,\n\t// promote it to a full_key so it can be modified\n\n\tfor (size_t i = 0; i < p.keys.size(); i++) {\n\t\tif (strcmp(key.c_str(), stringpool + pool_off[p.segment] + p.keys[i] + 1) == 0) {\n\t\t\tserial_val sv;\n\t\t\tsv.s = stringpool + pool_off[p.segment] + p.values[i] + 1;\n\t\t\tsv.type = (stringpool + pool_off[p.segment])[p.values[i]];\n\n\t\t\tp.full_keys.push_back(key);\n\t\t\tp.full_values.push_back(sv);\n\n\t\t\tp.keys.erase(p.keys.begin() + i);\n\t\t\tp.values.erase(p.values.begin() + i);\n\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfor (size_t i = 0; i < p.full_keys.size(); i++) {\n\t\tif (key == p.full_keys[i]) {\n\t\t\tswitch (op) {\n\t\t\tcase op_sum:\n\t\t\t\tp.full_values[i].s = milo::dtoa_milo(atof(p.full_values[i].s.c_str()) + atof(val.s.c_str()));\n\t\t\t\tp.full_values[i].type = mvt_double;\n\t\t\t\tbreak;\n\n\t\t\tcase op_product:\n\t\t\t\tp.full_values[i].s = milo::dtoa_milo(atof(p.full_values[i].s.c_str()) * atof(val.s.c_str()));\n\t\t\t\tp.full_values[i].type = mvt_double;\n\t\t\t\tbreak;\n\n\t\t\tcase op_max: {\n\t\t\t\tdouble existing = atof(p.full_values[i].s.c_str());\n\t\t\t\tdouble maybe = atof(val.s.c_str());\n\t\t\t\tif (maybe > existing) {\n\t\t\t\t\tp.full_values[i].s = val.s.c_str();\n\t\t\t\t\tp.full_values[i].type = mvt_double;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcase op_min: {\n\t\t\t\tdouble existing = atof(p.full_values[i].s.c_str());\n\t\t\t\tdouble maybe = atof(val.s.c_str());\n\t\t\t\tif (maybe < existing) {\n\t\t\t\t\tp.full_values[i].s = val.s.c_str();\n\t\t\t\t\tp.full_values[i].type = mvt_double;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcase op_mean: {\n\t\t\t\tauto state = p.attribute_accum_state.find(key);\n\t\t\t\tif (state == p.attribute_accum_state.end()) {\n\t\t\t\t\taccum_state s;\n\t\t\t\t\ts.sum = atof(p.full_values[i].s.c_str()) + atof(val.s.c_str());\n\t\t\t\t\ts.count = 2;\n\t\t\t\t\tp.attribute_accum_state.insert(std::pair<std::string, accum_state>(key, s));\n\n\t\t\t\t\tp.full_values[i].s = milo::dtoa_milo(s.sum / s.count);\n\t\t\t\t} else {\n\t\t\t\t\tstate->second.sum += atof(val.s.c_str());\n\t\t\t\t\tstate->second.count += 1;\n\n\t\t\t\t\tp.full_values[i].s = milo::dtoa_milo(state->second.sum / state->second.count);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcase op_concat:\n\t\t\t\tp.full_values[i].s += val.s;\n\t\t\t\tp.full_values[i].type = mvt_string;\n\t\t\t\tbreak;\n\n\t\t\tcase op_comma:\n\t\t\t\tp.full_values[i].s += std::string(\",\") + val.s;\n\t\t\t\tp.full_values[i].type = mvt_string;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n}\n\nvoid preserve_attributes(std::map<std::string, attribute_op> const *attribute_accum, serial_feature &sf, char *stringpool, long long *pool_off, partial &p) {\n\tfor (size_t i = 0; i < sf.keys.size(); i++) {\n\t\tstd::string key = stringpool + pool_off[sf.segment] + sf.keys[i] + 1;\n\n\t\tserial_val sv;\n\t\tsv.type = (stringpool + pool_off[sf.segment])[sf.values[i]];\n\t\tsv.s = stringpool + pool_off[sf.segment] + sf.values[i] + 1;\n\n\t\tauto f = attribute_accum->find(key);\n\t\tif (f != attribute_accum->end()) {\n\t\t\tpreserve_attribute(f->second, sf, stringpool, pool_off, key, sv, p);\n\t\t}\n\t}\n\tfor (size_t i = 0; i < sf.full_keys.size(); i++) {\n\t\tstd::string key = sf.full_keys[i];\n\t\tserial_val sv = sf.full_values[i];\n\n\t\tauto f = attribute_accum->find(key);\n\t\tif (f != attribute_accum->end()) {\n\t\t\tpreserve_attribute(f->second, sf, stringpool, pool_off, key, sv, p);\n\t\t}\n\t}\n}\n\nbool find_partial(std::vector<partial> &partials, serial_feature &sf, ssize_t &out, std::vector<std::vector<std::string>> *layer_unmaps) {\n\tfor (size_t i = partials.size(); i > 0; i--) {\n\t\tif (partials[i - 1].t == sf.t) {\n\t\t\tstd::string &layername1 = (*layer_unmaps)[partials[i - 1].segment][partials[i - 1].layer];\n\t\t\tstd::string &layername2 = (*layer_unmaps)[sf.segment][sf.layer];\n\n\t\t\tif (layername1 == layername2) {\n\t\t\t\tout = i - 1;\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic bool line_is_too_small(drawvec const &geometry, int z, int detail) {\n\tif (geometry.size() == 0) {\n\t\treturn true;\n\t}\n\n\tlong long x = geometry[0].x >> (32 - detail - z);\n\tlong long y = geometry[0].y >> (32 - detail - z);\n\n\tfor (auto &g : geometry) {\n\t\tlong long xx = g.x >> (32 - detail - z);\n\t\tlong long yy = g.y >> (32 - detail - z);\n\n\t\tif (xx != x || yy != y) {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nlong long write_tile(FILE *geoms, std::atomic<long long> *geompos_in, char *metabase, char *stringpool, int z, unsigned tx, unsigned ty, int detail, int min_detail, sqlite3 *outdb, const char *outdir, int buffer, const char *fname, FILE **geomfile, int minzoom, int maxzoom, double todo, std::atomic<long long> *along, long long alongminus, double gamma, int child_shards, long long *meta_off, long long *pool_off, unsigned *initial_x, unsigned *initial_y, std::atomic<int> *running, double simplification, std::vector<std::map<std::string, layermap_entry>> *layermaps, std::vector<std::vector<std::string>> *layer_unmaps, size_t tiling_seg, size_t pass, size_t passes, unsigned long long mingap, long long minextent, double fraction, const char *prefilter, const char *postfilter, struct json_object *filter, write_tile_args *arg) {\n\tint line_detail;\n\tdouble merge_fraction = 1;\n\tdouble mingap_fraction = 1;\n\tdouble minextent_fraction = 1;\n\n\tstatic std::atomic<double> oprogress(0);\n\tlong long og = *geompos_in;\n\n\t// XXX is there a way to do this without floating point?\n\tint max_zoom_increment = std::log(child_shards) / std::log(4);\n\tif (child_shards < 4 || max_zoom_increment < 1) {\n\t\tfprintf(stderr, \"Internal error: %d shards, max zoom increment %d\\n\", child_shards, max_zoom_increment);\n\t\texit(EXIT_FAILURE);\n\t}\n\tif ((((child_shards - 1) << 1) & child_shards) != child_shards) {\n\t\tfprintf(stderr, \"Internal error: %d shards not a power of 2\\n\", child_shards);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tint nextzoom = z + 1;\n\tif (nextzoom < minzoom) {\n\t\tif (z + max_zoom_increment > minzoom) {\n\t\t\tnextzoom = minzoom;\n\t\t} else {\n\t\t\tnextzoom = z + max_zoom_increment;\n\t\t}\n\t}\n\n\tbool has_polygons = false;\n\n\tbool first_time = true;\n\t// This only loops if the tile data didn't fit, in which case the detail\n\t// goes down and the progress indicator goes backward for the next try.\n\tfor (line_detail = detail; line_detail >= min_detail || line_detail == detail; line_detail--, oprogress = 0) {\n\t\tlong long count = 0;\n\t\tdouble accum_area = 0;\n\n\t\tdouble fraction_accum = 0;\n\n\t\tunsigned long long previndex = 0, density_previndex = 0, merge_previndex = 0;\n\t\tdouble scale = (double) (1LL << (64 - 2 * (z + 8)));\n\t\tdouble gap = 0, density_gap = 0;\n\t\tdouble spacing = 0;\n\n\t\tlong long original_features = 0;\n\t\tlong long unclipped_features = 0;\n\n\t\tstd::vector<struct partial> partials;\n\t\tstd::map<std::string, std::vector<coalesce>> layers;\n\t\tstd::vector<unsigned long long> indices;\n\t\tstd::vector<long long> extents;\n\t\tdouble coalesced_area = 0;\n\t\tdrawvec shared_nodes;\n\n\t\tint within[child_shards];\n\t\tstd::atomic<long long> geompos[child_shards];\n\t\tfor (size_t i = 0; i < (size_t) child_shards; i++) {\n\t\t\tgeompos[i] = 0;\n\t\t\twithin[i] = 0;\n\t\t}\n\n\t\tif (*geompos_in != og) {\n\t\t\tif (fseek(geoms, og, SEEK_SET) != 0) {\n\t\t\t\tperror(\"fseek geom\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\t*geompos_in = og;\n\t\t}\n\n\t\tint prefilter_write = -1, prefilter_read = -1;\n\t\tpid_t prefilter_pid = 0;\n\t\tFILE *prefilter_fp = NULL;\n\t\tpthread_t prefilter_writer;\n\t\trun_prefilter_args rpa;  // here so it stays in scope until joined\n\t\tFILE *prefilter_read_fp = NULL;\n\t\tjson_pull *prefilter_jp = NULL;\n\n\t\tif (z < minzoom) {\n\t\t\tprefilter = NULL;\n\t\t\tpostfilter = NULL;\n\t\t}\n\n\t\tif (prefilter != NULL) {\n\t\t\tsetup_filter(prefilter, &prefilter_write, &prefilter_read, &prefilter_pid, z, tx, ty);\n\t\t\tprefilter_fp = fdopen(prefilter_write, \"w\");\n\t\t\tif (prefilter_fp == NULL) {\n\t\t\t\tperror(\"freopen prefilter\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\trpa.geoms = geoms;\n\t\t\trpa.geompos_in = geompos_in;\n\t\t\trpa.metabase = metabase;\n\t\t\trpa.meta_off = meta_off;\n\t\t\trpa.z = z;\n\t\t\trpa.tx = tx;\n\t\t\trpa.ty = ty;\n\t\t\trpa.initial_x = initial_x;\n\t\t\trpa.initial_y = initial_y;\n\t\t\trpa.original_features = &original_features;\n\t\t\trpa.unclipped_features = &unclipped_features;\n\t\t\trpa.nextzoom = nextzoom;\n\t\t\trpa.maxzoom = maxzoom;\n\t\t\trpa.minzoom = minzoom;\n\t\t\trpa.max_zoom_increment = max_zoom_increment;\n\t\t\trpa.pass = pass;\n\t\t\trpa.passes = passes;\n\t\t\trpa.along = along;\n\t\t\trpa.alongminus = alongminus;\n\t\t\trpa.buffer = buffer;\n\t\t\trpa.within = within;\n\t\t\trpa.first_time = &first_time;\n\t\t\trpa.geomfile = geomfile;\n\t\t\trpa.geompos = geompos;\n\t\t\trpa.oprogress = &oprogress;\n\t\t\trpa.todo = todo;\n\t\t\trpa.fname = fname;\n\t\t\trpa.child_shards = child_shards;\n\t\t\trpa.prefilter_fp = prefilter_fp;\n\t\t\trpa.layer_unmaps = layer_unmaps;\n\t\t\trpa.stringpool = stringpool;\n\t\t\trpa.pool_off = pool_off;\n\t\t\trpa.filter = filter;\n\n\t\t\tif (pthread_create(&prefilter_writer, NULL, run_prefilter, &rpa) != 0) {\n\t\t\t\tperror(\"pthread_create (prefilter writer)\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tprefilter_read_fp = fdopen(prefilter_read, \"r\");\n\t\t\tif (prefilter_read_fp == NULL) {\n\t\t\t\tperror(\"fdopen prefilter output\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tprefilter_jp = json_begin_file(prefilter_read_fp);\n\t\t}\n\n\t\twhile (1) {\n\t\t\tserial_feature sf;\n\t\t\tssize_t which_partial = -1;\n\n\t\t\tif (prefilter == NULL) {\n\t\t\t\tsf = next_feature(geoms, geompos_in, metabase, meta_off, z, tx, ty, initial_x, initial_y, &original_features, &unclipped_features, nextzoom, maxzoom, minzoom, max_zoom_increment, pass, passes, along, alongminus, buffer, within, &first_time, geomfile, geompos, &oprogress, todo, fname, child_shards, filter, stringpool, pool_off, layer_unmaps);\n\t\t\t} else {\n\t\t\t\tsf = parse_feature(prefilter_jp, z, tx, ty, layermaps, tiling_seg, layer_unmaps, postfilter != NULL);\n\t\t\t}\n\n\t\t\tif (sf.t < 0) {\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (sf.dropped) {\n\t\t\t\tif (find_partial(partials, sf, which_partial, layer_unmaps)) {\n\t\t\t\t\tpreserve_attributes(arg->attribute_accum, sf, stringpool, pool_off, partials[which_partial]);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (gamma > 0) {\n\t\t\t\tif (manage_gap(sf.index, &previndex, scale, gamma, &gap) && find_partial(partials, sf, which_partial, layer_unmaps)) {\n\t\t\t\t\tpreserve_attributes(arg->attribute_accum, sf, stringpool, pool_off, partials[which_partial]);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (additional[A_CLUSTER_DENSEST_AS_NEEDED] || cluster_distance != 0) {\n\t\t\t\tindices.push_back(sf.index);\n\t\t\t\tif ((sf.index < merge_previndex || sf.index - merge_previndex < mingap) && find_partial(partials, sf, which_partial, layer_unmaps)) {\n\t\t\t\t\tpartials[which_partial].clustered++;\n\n\t\t\t\t\tif (partials[which_partial].t == VT_POINT &&\n\t\t\t\t\t    partials[which_partial].geoms.size() == 1 &&\n\t\t\t\t\t    partials[which_partial].geoms[0].size() == 1 &&\n\t\t\t\t\t    sf.geometry.size() == 1) {\n\t\t\t\t\t\tdouble x = (double) partials[which_partial].geoms[0][0].x * partials[which_partial].clustered;\n\t\t\t\t\t\tdouble y = (double) partials[which_partial].geoms[0][0].y * partials[which_partial].clustered;\n\t\t\t\t\t\tx += sf.geometry[0].x;\n\t\t\t\t\t\ty += sf.geometry[0].y;\n\t\t\t\t\t\tpartials[which_partial].geoms[0][0].x = x / (partials[which_partial].clustered + 1);\n\t\t\t\t\t\tpartials[which_partial].geoms[0][0].y = y / (partials[which_partial].clustered + 1);\n\t\t\t\t\t}\n\n\t\t\t\t\tpreserve_attributes(arg->attribute_accum, sf, stringpool, pool_off, partials[which_partial]);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else if (additional[A_DROP_DENSEST_AS_NEEDED]) {\n\t\t\t\tindices.push_back(sf.index);\n\t\t\t\tif (sf.index - merge_previndex < mingap && find_partial(partials, sf, which_partial, layer_unmaps)) {\n\t\t\t\t\tpreserve_attributes(arg->attribute_accum, sf, stringpool, pool_off, partials[which_partial]);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else if (additional[A_COALESCE_DENSEST_AS_NEEDED]) {\n\t\t\t\tindices.push_back(sf.index);\n\t\t\t\tif (sf.index - merge_previndex < mingap && find_partial(partials, sf, which_partial, layer_unmaps)) {\n\t\t\t\t\tpartials[which_partial].geoms.push_back(sf.geometry);\n\t\t\t\t\tcoalesced_area += sf.extent;\n\t\t\t\t\tpreserve_attributes(arg->attribute_accum, sf, stringpool, pool_off, partials[which_partial]);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else if (additional[A_DROP_SMALLEST_AS_NEEDED]) {\n\t\t\t\textents.push_back(sf.extent);\n\t\t\t\tif (sf.extent + coalesced_area <= minextent && sf.t != VT_POINT && find_partial(partials, sf, which_partial, layer_unmaps)) {\n\t\t\t\t\tpreserve_attributes(arg->attribute_accum, sf, stringpool, pool_off, partials[which_partial]);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else if (additional[A_COALESCE_SMALLEST_AS_NEEDED]) {\n\t\t\t\textents.push_back(sf.extent);\n\t\t\t\tif (sf.extent + coalesced_area <= minextent && find_partial(partials, sf, which_partial, layer_unmaps)) {\n\t\t\t\t\tpartials[which_partial].geoms.push_back(sf.geometry);\n\t\t\t\t\tcoalesced_area += sf.extent;\n\t\t\t\t\tpreserve_attributes(arg->attribute_accum, sf, stringpool, pool_off, partials[which_partial]);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (additional[A_CALCULATE_FEATURE_DENSITY]) {\n\t\t\t\t// Gamma is always 1 for this calculation so there is a reasonable\n\t\t\t\t// interpretation when no features are being dropped.\n\t\t\t\t// The spacing is only calculated if a feature would be retained by\n\t\t\t\t// that standard, so that duplicates aren't reported as infinitely dense.\n\n\t\t\t\tdouble o_density_previndex = density_previndex;\n\t\t\t\tif (!manage_gap(sf.index, &density_previndex, scale, 1, &density_gap)) {\n\t\t\t\t\tspacing = (sf.index - o_density_previndex) / scale;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfraction_accum += fraction;\n\t\t\tif (fraction_accum < 1 && find_partial(partials, sf, which_partial, layer_unmaps)) {\n\t\t\t\tif (additional[A_COALESCE_FRACTION_AS_NEEDED]) {\n\t\t\t\t\tpartials[which_partial].geoms.push_back(sf.geometry);\n\t\t\t\t\tcoalesced_area += sf.extent;\n\t\t\t\t}\n\t\t\t\tpreserve_attributes(arg->attribute_accum, sf, stringpool, pool_off, partials[which_partial]);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tfraction_accum -= 1;\n\n\t\t\tbool reduced = false;\n\t\t\tif (sf.t == VT_POLYGON) {\n\t\t\t\tif (!prevent[P_TINY_POLYGON_REDUCTION] && !additional[A_GRID_LOW_ZOOMS]) {\n\t\t\t\t\tsf.geometry = reduce_tiny_poly(sf.geometry, z, line_detail, &reduced, &accum_area);\n\t\t\t\t}\n\t\t\t\thas_polygons = true;\n\t\t\t}\n\t\t\tif (sf.t == VT_POLYGON || sf.t == VT_LINE) {\n\t\t\t\tif (line_is_too_small(sf.geometry, z, line_detail)) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (sf.geometry.size() > 0) {\n\t\t\t\tif (prevent[P_SIMPLIFY_SHARED_NODES]) {\n\t\t\t\t\tfor (auto &g : sf.geometry) {\n\t\t\t\t\t\tshared_nodes.push_back(g);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tpartial p;\n\t\t\t\tp.geoms.push_back(sf.geometry);\n\t\t\t\tp.layer = sf.layer;\n\t\t\t\tp.t = sf.t;\n\t\t\t\tp.segment = sf.segment;\n\t\t\t\tp.original_seq = sf.seq;\n\t\t\t\tp.reduced = reduced;\n\t\t\t\tp.z = z;\n\t\t\t\tp.line_detail = line_detail;\n\t\t\t\tp.maxzoom = maxzoom;\n\t\t\t\tp.keys = sf.keys;\n\t\t\t\tp.values = sf.values;\n\t\t\t\tp.full_keys = sf.full_keys;\n\t\t\t\tp.full_values = sf.full_values;\n\t\t\t\tp.spacing = spacing;\n\t\t\t\tp.simplification = simplification;\n\t\t\t\tp.id = sf.id;\n\t\t\t\tp.has_id = sf.has_id;\n\t\t\t\tp.index = sf.index;\n\t\t\t\tp.renamed = -1;\n\t\t\t\tp.extent = sf.extent;\n\t\t\t\tp.clustered = 0;\n\t\t\t\tpartials.push_back(p);\n\t\t\t}\n\n\t\t\tmerge_previndex = sf.index;\n\t\t\tcoalesced_area = 0;\n\t\t}\n\n\t\t{\n\t\t\tdrawvec just_shared_nodes;\n\t\t\tstd::sort(shared_nodes.begin(), shared_nodes.end());\n\n\t\t\tfor (size_t i = 0; i + 1 < shared_nodes.size(); i++) {\n\t\t\t\tif (shared_nodes[i] == shared_nodes[i + 1]) {\n\t\t\t\t\tjust_shared_nodes.push_back(shared_nodes[i]);\n\n\t\t\t\t\tdraw d = shared_nodes[i];\n\t\t\t\t\ti++;\n\t\t\t\t\twhile (i + 1 < shared_nodes.size() && shared_nodes[i + 1] == d) {\n\t\t\t\t\t\ti++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tshared_nodes = just_shared_nodes;\n\t\t}\n\n\t\tfor (size_t i = 0; i < partials.size(); i++) {\n\t\t\tpartial &p = partials[i];\n\n\t\t\tif (p.clustered > 0) {\n\t\t\t\tstd::string layername = (*layer_unmaps)[p.segment][p.layer];\n\t\t\t\tserial_val sv, sv2, sv3;\n\n\t\t\t\tp.full_keys.push_back(\"clustered\");\n\t\t\t\tsv.type = mvt_bool;\n\t\t\t\tsv.s = \"true\";\n\t\t\t\tp.full_values.push_back(sv);\n\n\t\t\t\tadd_tilestats(layername, z, layermaps, tiling_seg, layer_unmaps, \"clustered\", sv);\n\n\t\t\t\tp.full_keys.push_back(\"point_count\");\n\t\t\t\tsv2.type = mvt_double;\n\t\t\t\tsv2.s = std::to_string(p.clustered + 1);\n\t\t\t\tp.full_values.push_back(sv2);\n\n\t\t\t\tadd_tilestats(layername, z, layermaps, tiling_seg, layer_unmaps, \"point_count\", sv2);\n\n\t\t\t\tp.full_keys.push_back(\"sqrt_point_count\");\n\t\t\t\tsv3.type = mvt_double;\n\t\t\t\tsv3.s = std::to_string(round(100 * sqrt(p.clustered + 1)) / 100.0);\n\t\t\t\tp.full_values.push_back(sv3);\n\n\t\t\t\tadd_tilestats(layername, z, layermaps, tiling_seg, layer_unmaps, \"sqrt_point_count\", sv3);\n\t\t\t}\n\n\t\t\tif (p.need_tilestats.size() > 0) {\n\t\t\t\tstd::string layername = (*layer_unmaps)[p.segment][p.layer];\n\n\t\t\t\tfor (size_t j = 0; j < p.full_keys.size(); j++) {\n\t\t\t\t\tif (p.need_tilestats.count(p.full_keys[j]) > 0) {\n\t\t\t\t\t\tadd_tilestats(layername, z, layermaps, tiling_seg, layer_unmaps, p.full_keys[j], p.full_values[j]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (prefilter != NULL) {\n\t\t\tjson_end(prefilter_jp);\n\t\t\tif (fclose(prefilter_read_fp) != 0) {\n\t\t\t\tperror(\"close output from prefilter\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\twhile (1) {\n\t\t\t\tint stat_loc;\n\t\t\t\tif (waitpid(prefilter_pid, &stat_loc, 0) < 0) {\n\t\t\t\t\tperror(\"waitpid for prefilter\\n\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t\tif (WIFEXITED(stat_loc) || WIFSIGNALED(stat_loc)) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tvoid *ret;\n\t\t\tif (pthread_join(prefilter_writer, &ret) != 0) {\n\t\t\t\tperror(\"pthread_join prefilter writer\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t}\n\n\t\tfirst_time = false;\n\t\tbool merge_successful = true;\n\n\t\tif (additional[A_DETECT_SHARED_BORDERS] || (additional[A_MERGE_POLYGONS_AS_NEEDED] && merge_fraction < 1)) {\n\t\t\tmerge_successful = find_common_edges(partials, z, line_detail, simplification, maxzoom, merge_fraction);\n\t\t}\n\n\t\tint tasks = ceil((double) CPUS / *running);\n\t\tif (tasks < 1) {\n\t\t\ttasks = 1;\n\t\t}\n\n\t\tpthread_t pthreads[tasks];\n\t\tstd::vector<partial_arg> args;\n\t\targs.resize(tasks);\n\t\tfor (int i = 0; i < tasks; i++) {\n\t\t\targs[i].task = i;\n\t\t\targs[i].tasks = tasks;\n\t\t\targs[i].partials = &partials;\n\t\t\targs[i].shared_nodes = &shared_nodes;\n\n\t\t\tif (tasks > 1) {\n\t\t\t\tif (pthread_create(&pthreads[i], NULL, partial_feature_worker, &args[i]) != 0) {\n\t\t\t\t\tperror(\"pthread_create\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tpartial_feature_worker(&args[i]);\n\t\t\t}\n\t\t}\n\n\t\tif (tasks > 1) {\n\t\t\tfor (int i = 0; i < tasks; i++) {\n\t\t\t\tvoid *retval;\n\n\t\t\t\tif (pthread_join(pthreads[i], &retval) != 0) {\n\t\t\t\t\tperror(\"pthread_join\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tfor (size_t i = 0; i < partials.size(); i++) {\n\t\t\tstd::vector<drawvec> &pgeoms = partials[i].geoms;\n\t\t\tsigned char t = partials[i].t;\n\t\t\tlong long original_seq = partials[i].original_seq;\n\n\t\t\t// A complex polygon may have been split up into multiple geometries.\n\t\t\t// Break them out into multiple features if necessary.\n\t\t\tfor (size_t j = 0; j < pgeoms.size(); j++) {\n\t\t\t\tif (t == VT_POINT || draws_something(pgeoms[j])) {\n\t\t\t\t\tstruct coalesce c;\n\n\t\t\t\t\tc.type = t;\n\t\t\t\t\tc.index = partials[i].index;\n\t\t\t\t\tc.geom = pgeoms[j];\n\t\t\t\t\tpgeoms[j].clear();\n\t\t\t\t\tc.coalesced = false;\n\t\t\t\t\tc.original_seq = original_seq;\n\t\t\t\t\tc.stringpool = stringpool + pool_off[partials[i].segment];\n\t\t\t\t\tc.keys = partials[i].keys;\n\t\t\t\t\tc.values = partials[i].values;\n\t\t\t\t\tc.full_keys = partials[i].full_keys;\n\t\t\t\t\tc.full_values = partials[i].full_values;\n\t\t\t\t\tc.spacing = partials[i].spacing;\n\t\t\t\t\tc.id = partials[i].id;\n\t\t\t\t\tc.has_id = partials[i].has_id;\n\n\t\t\t\t\t// printf(\"segment %d layer %lld is %s\\n\", partials[i].segment, partials[i].layer, (*layer_unmaps)[partials[i].segment][partials[i].layer].c_str());\n\n\t\t\t\t\tstd::string layername = (*layer_unmaps)[partials[i].segment][partials[i].layer];\n\t\t\t\t\tif (layers.count(layername) == 0) {\n\t\t\t\t\t\tlayers.insert(std::pair<std::string, std::vector<coalesce>>(layername, std::vector<coalesce>()));\n\t\t\t\t\t}\n\n\t\t\t\t\tauto l = layers.find(layername);\n\t\t\t\t\tif (l == layers.end()) {\n\t\t\t\t\t\tfprintf(stderr, \"Internal error: couldn't find layer %s\\n\", layername.c_str());\n\t\t\t\t\t\tfprintf(stderr, \"segment %d\\n\", partials[i].segment);\n\t\t\t\t\t\tfprintf(stderr, \"layer %lld\\n\", partials[i].layer);\n\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t}\n\t\t\t\t\tl->second.push_back(c);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tpartials.clear();\n\n\t\tint j;\n\t\tfor (j = 0; j < child_shards; j++) {\n\t\t\tif (within[j]) {\n\t\t\t\tserialize_byte(geomfile[j], -2, &geompos[j], fname);\n\t\t\t\twithin[j] = 0;\n\t\t\t}\n\t\t}\n\n\t\tfor (auto layer_iterator = layers.begin(); layer_iterator != layers.end(); ++layer_iterator) {\n\t\t\tstd::vector<coalesce> &layer_features = layer_iterator->second;\n\n\t\t\tif (additional[A_REORDER]) {\n\t\t\t\tstd::sort(layer_features.begin(), layer_features.end());\n\t\t\t}\n\n\t\t\tstd::vector<coalesce> out;\n\t\t\tif (layer_features.size() > 0) {\n\t\t\t\tout.push_back(layer_features[0]);\n\t\t\t}\n\t\t\tfor (size_t x = 1; x < layer_features.size(); x++) {\n\t\t\t\tsize_t y = out.size() - 1;\n\n#if 0\n\t\t\t\tif (out.size() > 0 && coalcmp(&layer_features[x], &out[y]) < 0) {\n\t\t\t\t\tfprintf(stderr, \"\\nfeature out of order\\n\");\n\t\t\t\t}\n#endif\n\n\t\t\t\tif (additional[A_COALESCE] && out.size() > 0 && coalcmp(&layer_features[x], &out[y]) == 0) {\n\t\t\t\t\tfor (size_t g = 0; g < layer_features[x].geom.size(); g++) {\n\t\t\t\t\t\tout[y].geom.push_back(layer_features[x].geom[g]);\n\t\t\t\t\t}\n\t\t\t\t\tout[y].coalesced = true;\n\t\t\t\t} else {\n\t\t\t\t\tout.push_back(layer_features[x]);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tlayer_features = out;\n\n\t\t\tout.clear();\n\t\t\tfor (size_t x = 0; x < layer_features.size(); x++) {\n\t\t\t\tif (layer_features[x].coalesced && layer_features[x].type == VT_LINE) {\n\t\t\t\t\tlayer_features[x].geom = remove_noop(layer_features[x].geom, layer_features[x].type, 0);\n\t\t\t\t\tlayer_features[x].geom = simplify_lines(layer_features[x].geom, 32, 0,\n\t\t\t\t\t\t\t\t\t\t!(prevent[P_CLIPPING] || prevent[P_DUPLICATION]), simplification, layer_features[x].type == VT_POLYGON ? 4 : 0, shared_nodes);\n\t\t\t\t}\n\n\t\t\t\tif (layer_features[x].type == VT_POLYGON) {\n\t\t\t\t\tif (layer_features[x].coalesced) {\n\t\t\t\t\t\tlayer_features[x].geom = clean_or_clip_poly(layer_features[x].geom, 0, 0, false);\n\t\t\t\t\t}\n\n\t\t\t\t\tlayer_features[x].geom = close_poly(layer_features[x].geom);\n\t\t\t\t}\n\n\t\t\t\tif (layer_features[x].geom.size() > 0) {\n\t\t\t\t\tout.push_back(layer_features[x]);\n\t\t\t\t}\n\t\t\t}\n\t\t\tlayer_features = out;\n\n\t\t\tif (prevent[P_INPUT_ORDER]) {\n\t\t\t\tstd::sort(layer_features.begin(), layer_features.end(), preservecmp);\n\t\t\t}\n\t\t}\n\n\t\tmvt_tile tile;\n\n\t\tfor (auto layer_iterator = layers.begin(); layer_iterator != layers.end(); ++layer_iterator) {\n\t\t\tstd::vector<coalesce> &layer_features = layer_iterator->second;\n\n\t\t\tmvt_layer layer;\n\t\t\tlayer.name = layer_iterator->first;\n\t\t\tlayer.version = 2;\n\t\t\tlayer.extent = 1 << line_detail;\n\n\t\t\tfor (size_t x = 0; x < layer_features.size(); x++) {\n\t\t\t\tmvt_feature feature;\n\n\t\t\t\tif (layer_features[x].type == VT_LINE || layer_features[x].type == VT_POLYGON) {\n\t\t\t\t\tlayer_features[x].geom = remove_noop(layer_features[x].geom, layer_features[x].type, 0);\n\t\t\t\t}\n\n\t\t\t\tif (layer_features[x].geom.size() == 0) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tfeature.type = layer_features[x].type;\n\t\t\t\tfeature.geometry = to_feature(layer_features[x].geom);\n\t\t\t\tcount += layer_features[x].geom.size();\n\t\t\t\tlayer_features[x].geom.clear();\n\n\t\t\t\tfeature.id = layer_features[x].id;\n\t\t\t\tfeature.has_id = layer_features[x].has_id;\n\n\t\t\t\tdecode_meta(layer_features[x].keys, layer_features[x].values, layer_features[x].stringpool, layer, feature);\n\t\t\t\tfor (size_t a = 0; a < layer_features[x].full_keys.size(); a++) {\n\t\t\t\t\tserial_val sv = layer_features[x].full_values[a];\n\t\t\t\t\tmvt_value v = stringified_to_mvt_value(sv.type, sv.s.c_str());\n\t\t\t\t\tlayer.tag(feature, layer_features[x].full_keys[a], v);\n\t\t\t\t}\n\n\t\t\t\tif (additional[A_CALCULATE_FEATURE_DENSITY]) {\n\t\t\t\t\tint glow = 255;\n\t\t\t\t\tif (layer_features[x].spacing > 0) {\n\t\t\t\t\t\tglow = (1 / layer_features[x].spacing);\n\t\t\t\t\t\tif (glow > 255) {\n\t\t\t\t\t\t\tglow = 255;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tmvt_value v;\n\t\t\t\t\tv.type = mvt_sint;\n\t\t\t\t\tv.numeric_value.sint_value = glow;\n\t\t\t\t\tlayer.tag(feature, \"tippecanoe_feature_density\", v);\n\n\t\t\t\t\tserial_val sv;\n\t\t\t\t\tsv.type = mvt_double;\n\t\t\t\t\tsv.s = std::to_string(glow);\n\n\t\t\t\t\tadd_tilestats(layer.name, z, layermaps, tiling_seg, layer_unmaps, \"tippecanoe_feature_density\", sv);\n\t\t\t\t}\n\n\t\t\t\tlayer.features.push_back(feature);\n\t\t\t}\n\n\t\t\tif (layer.features.size() > 0) {\n\t\t\t\ttile.layers.push_back(layer);\n\t\t\t}\n\t\t}\n\n\t\tif (postfilter != NULL) {\n\t\t\ttile.layers = filter_layers(postfilter, tile.layers, z, tx, ty, layermaps, tiling_seg, layer_unmaps, 1 << line_detail);\n\t\t}\n\n\t\tif (z == 0 && unclipped_features < original_features / 2 && clipbboxes.size() == 0) {\n\t\t\tfprintf(stderr, \"\\n\\nMore than half the features were clipped away at zoom level 0.\\n\");\n\t\t\tfprintf(stderr, \"Is your data in the wrong projection? It should be in WGS84/EPSG:4326.\\n\");\n\t\t}\n\n\t\tsize_t totalsize = 0;\n\t\tfor (auto layer_iterator = layers.begin(); layer_iterator != layers.end(); ++layer_iterator) {\n\t\t\tstd::vector<coalesce> &layer_features = layer_iterator->second;\n\t\t\ttotalsize += layer_features.size();\n\t\t}\n\n\t\tdouble progress = floor(((((*geompos_in + *along - alongminus) / (double) todo) + (pass - (2 - passes))) / passes + z) / (maxzoom + 1) * 1000) / 10;\n\t\tif (progress >= oprogress + 0.1) {\n\t\t\tif (!quiet && !quiet_progress && progress_time()) {\n\t\t\t\tfprintf(stderr, \"  %3.1f%%  %d/%u/%u  \\r\", progress, z, tx, ty);\n\t\t\t}\n\t\t\toprogress = progress;\n\t\t}\n\n\t\tif (totalsize > 0 && tile.layers.size() > 0) {\n\t\t\tif (totalsize > max_tile_features && !prevent[P_FEATURE_LIMIT]) {\n\t\t\t\tif (!quiet) {\n\t\t\t\t\tfprintf(stderr, \"tile %d/%u/%u has %zu features, >%zu    \\n\", z, tx, ty, totalsize, max_tile_features);\n\t\t\t\t}\n\n\t\t\t\tif (has_polygons && additional[A_MERGE_POLYGONS_AS_NEEDED] && merge_fraction > .05 && merge_successful) {\n\t\t\t\t\tmerge_fraction = merge_fraction * max_tile_features / tile.layers.size() * 0.95;\n\t\t\t\t\tif (!quiet) {\n\t\t\t\t\t\tfprintf(stderr, \"Going to try merging %0.2f%% of the polygons to make it fit\\n\", 100 - merge_fraction * 100);\n\t\t\t\t\t}\n\t\t\t\t\tline_detail++;  // to keep it the same when the loop decrements it\n\t\t\t\t\tcontinue;\n\t\t\t\t} else if (additional[A_INCREASE_GAMMA_AS_NEEDED] && gamma < 10) {\n\t\t\t\t\tif (gamma < 1) {\n\t\t\t\t\t\tgamma = 1;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tgamma = gamma * 1.25;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (gamma > arg->gamma_out) {\n\t\t\t\t\t\targ->gamma_out = gamma;\n\t\t\t\t\t\targ->still_dropping = true;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (!quiet) {\n\t\t\t\t\t\tfprintf(stderr, \"Going to try gamma of %0.3f to make it fit\\n\", gamma);\n\t\t\t\t\t}\n\t\t\t\t\tline_detail++;  // to keep it the same when the loop decrements it\n\t\t\t\t\tcontinue;\n\t\t\t\t} else if (mingap < ULONG_MAX && (additional[A_DROP_DENSEST_AS_NEEDED] || additional[A_COALESCE_DENSEST_AS_NEEDED] || additional[A_CLUSTER_DENSEST_AS_NEEDED])) {\n\t\t\t\t\tmingap_fraction = mingap_fraction * max_tile_features / totalsize * 0.90;\n\t\t\t\t\tunsigned long long mg = choose_mingap(indices, mingap_fraction);\n\t\t\t\t\tif (mg <= mingap) {\n\t\t\t\t\t\tmg = (mingap + 1) * 1.5;\n\n\t\t\t\t\t\tif (mg <= mingap) {\n\t\t\t\t\t\t\tmg = ULONG_MAX;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tmingap = mg;\n\t\t\t\t\tif (mingap > arg->mingap_out) {\n\t\t\t\t\t\targ->mingap_out = mingap;\n\t\t\t\t\t\targ->still_dropping = true;\n\t\t\t\t\t}\n\t\t\t\t\tif (!quiet) {\n\t\t\t\t\t\tfprintf(stderr, \"Going to try keeping the sparsest %0.2f%% of the features to make it fit\\n\", mingap_fraction * 100.0);\n\t\t\t\t\t}\n\t\t\t\t\tline_detail++;\n\t\t\t\t\tcontinue;\n\t\t\t\t} else if (additional[A_DROP_SMALLEST_AS_NEEDED] || additional[A_COALESCE_SMALLEST_AS_NEEDED]) {\n\t\t\t\t\tminextent_fraction = minextent_fraction * max_tile_features / totalsize * 0.90;\n\t\t\t\t\tlong long m = choose_minextent(extents, minextent_fraction);\n\t\t\t\t\tif (m != minextent) {\n\t\t\t\t\t\tminextent = m;\n\t\t\t\t\t\tif (minextent > arg->minextent_out) {\n\t\t\t\t\t\t\targ->minextent_out = minextent;\n\t\t\t\t\t\t\targ->still_dropping = true;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (!quiet) {\n\t\t\t\t\t\t\tfprintf(stderr, \"Going to try keeping the biggest %0.2f%% of the features to make it fit\\n\", minextent_fraction * 100.0);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tline_detail++;\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t} else if (totalsize > layers.size() && (prevent[P_DYNAMIC_DROP] || additional[A_DROP_FRACTION_AS_NEEDED] || additional[A_COALESCE_FRACTION_AS_NEEDED])) {\n\t\t\t\t\t// The 95% is a guess to avoid too many retries\n\t\t\t\t\t// and probably actually varies based on how much duplicated metadata there is\n\n\t\t\t\t\tfraction = fraction * max_tile_features / totalsize * 0.95;\n\t\t\t\t\tif (!quiet) {\n\t\t\t\t\t\tfprintf(stderr, \"Going to try keeping %0.2f%% of the features to make it fit\\n\", fraction * 100);\n\t\t\t\t\t}\n\t\t\t\t\tif ((additional[A_DROP_FRACTION_AS_NEEDED] || additional[A_COALESCE_FRACTION_AS_NEEDED]) && fraction < arg->fraction_out) {\n\t\t\t\t\t\targ->fraction_out = fraction;\n\t\t\t\t\t\targ->still_dropping = true;\n\t\t\t\t\t} else if (prevent[P_DYNAMIC_DROP]) {\n\t\t\t\t\t\targ->still_dropping = true;\n\t\t\t\t\t}\n\t\t\t\t\tline_detail++;  // to keep it the same when the loop decrements it\n\t\t\t\t\tcontinue;\n\t\t\t\t} else {\n\t\t\t\t\tfprintf(stderr, \"Try using --drop-fraction-as-needed or --drop-densest-as-needed.\\n\");\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tstd::string compressed;\n\t\t\tstd::string pbf = tile.encode();\n\n\t\t\tif (!prevent[P_TILE_COMPRESSION]) {\n\t\t\t\tcompress(pbf, compressed);\n\t\t\t} else {\n\t\t\t\tcompressed = pbf;\n\t\t\t}\n\n\t\t\tif (compressed.size() > max_tile_size && !prevent[P_KILOBYTE_LIMIT]) {\n\t\t\t\tif (!quiet) {\n\t\t\t\t\tfprintf(stderr, \"tile %d/%u/%u size is %lld with detail %d, >%zu    \\n\", z, tx, ty, (long long) compressed.size(), line_detail, max_tile_size);\n\t\t\t\t}\n\n\t\t\t\tif (has_polygons && additional[A_MERGE_POLYGONS_AS_NEEDED] && merge_fraction > .05 && merge_successful) {\n\t\t\t\t\tmerge_fraction = merge_fraction * max_tile_size / compressed.size() * 0.95;\n\t\t\t\t\tif (!quiet) {\n\t\t\t\t\t\tfprintf(stderr, \"Going to try merging %0.2f%% of the polygons to make it fit\\n\", 100 - merge_fraction * 100);\n\t\t\t\t\t}\n\t\t\t\t\tline_detail++;  // to keep it the same when the loop decrements it\n\t\t\t\t} else if (additional[A_INCREASE_GAMMA_AS_NEEDED] && gamma < 10) {\n\t\t\t\t\tif (gamma < 1) {\n\t\t\t\t\t\tgamma = 1;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tgamma = gamma * 1.25;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (gamma > arg->gamma_out) {\n\t\t\t\t\t\targ->gamma_out = gamma;\n\t\t\t\t\t\targ->still_dropping = true;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (!quiet) {\n\t\t\t\t\t\tfprintf(stderr, \"Going to try gamma of %0.3f to make it fit\\n\", gamma);\n\t\t\t\t\t}\n\t\t\t\t\tline_detail++;  // to keep it the same when the loop decrements it\n\t\t\t\t} else if (mingap < ULONG_MAX && (additional[A_DROP_DENSEST_AS_NEEDED] || additional[A_COALESCE_DENSEST_AS_NEEDED] || additional[A_CLUSTER_DENSEST_AS_NEEDED])) {\n\t\t\t\t\tmingap_fraction = mingap_fraction * max_tile_size / compressed.size() * 0.90;\n\t\t\t\t\tunsigned long long mg = choose_mingap(indices, mingap_fraction);\n\t\t\t\t\tif (mg <= mingap) {\n\t\t\t\t\t\tdouble nmg = (mingap + 1) * 1.5;\n\n\t\t\t\t\t\tif (nmg <= mingap || nmg > ULONG_MAX) {\n\t\t\t\t\t\t\tmg = ULONG_MAX;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tmg = nmg;\n\n\t\t\t\t\t\t\tif (mg <= mingap) {\n\t\t\t\t\t\t\t\tmg = ULONG_MAX;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tmingap = mg;\n\t\t\t\t\tif (mingap > arg->mingap_out) {\n\t\t\t\t\t\targ->mingap_out = mingap;\n\t\t\t\t\t\targ->still_dropping = true;\n\t\t\t\t\t}\n\t\t\t\t\tif (!quiet) {\n\t\t\t\t\t\tfprintf(stderr, \"Going to try keeping the sparsest %0.2f%% of the features to make it fit\\n\", mingap_fraction * 100.0);\n\t\t\t\t\t}\n\t\t\t\t\tline_detail++;\n\t\t\t\t} else if (additional[A_DROP_SMALLEST_AS_NEEDED] || additional[A_COALESCE_SMALLEST_AS_NEEDED]) {\n\t\t\t\t\tminextent_fraction = minextent_fraction * max_tile_size / compressed.size() * 0.90;\n\t\t\t\t\tlong long m = choose_minextent(extents, minextent_fraction);\n\t\t\t\t\tif (m != minextent) {\n\t\t\t\t\t\tminextent = m;\n\t\t\t\t\t\tif (minextent > arg->minextent_out) {\n\t\t\t\t\t\t\targ->minextent_out = minextent;\n\t\t\t\t\t\t\targ->still_dropping = true;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (!quiet) {\n\t\t\t\t\t\t\tfprintf(stderr, \"Going to try keeping the biggest %0.2f%% of the features to make it fit\\n\", minextent_fraction * 100.0);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tline_detail++;\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t} else if (totalsize > layers.size() && (prevent[P_DYNAMIC_DROP] || additional[A_DROP_FRACTION_AS_NEEDED] || additional[A_COALESCE_FRACTION_AS_NEEDED])) {\n\t\t\t\t\t// The 95% is a guess to avoid too many retries\n\t\t\t\t\t// and probably actually varies based on how much duplicated metadata there is\n\n\t\t\t\t\tfraction = fraction * max_tile_size / compressed.size() * 0.95;\n\t\t\t\t\tif (!quiet) {\n\t\t\t\t\t\tfprintf(stderr, \"Going to try keeping %0.2f%% of the features to make it fit\\n\", fraction * 100);\n\t\t\t\t\t}\n\t\t\t\t\tif ((additional[A_DROP_FRACTION_AS_NEEDED] || additional[A_COALESCE_FRACTION_AS_NEEDED]) && fraction < arg->fraction_out) {\n\t\t\t\t\t\targ->fraction_out = fraction;\n\t\t\t\t\t\targ->still_dropping = true;\n\t\t\t\t\t} else if (prevent[P_DYNAMIC_DROP]) {\n\t\t\t\t\t\targ->still_dropping = true;\n\t\t\t\t\t}\n\t\t\t\t\tline_detail++;  // to keep it the same when the loop decrements it\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (pass == 1) {\n\t\t\t\t\tif (pthread_mutex_lock(&db_lock) != 0) {\n\t\t\t\t\t\tperror(\"pthread_mutex_lock\");\n\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t}\n\n\t\t\t\t\tif (outdb != NULL) {\n\t\t\t\t\t\tmbtiles_write_tile(outdb, z, tx, ty, compressed.data(), compressed.size());\n\t\t\t\t\t} else if (outdir != NULL) {\n\t\t\t\t\t\tdir_write_tile(outdir, z, tx, ty, compressed);\n\t\t\t\t\t}\n\n\t\t\t\t\tif (pthread_mutex_unlock(&db_lock) != 0) {\n\t\t\t\t\t\tperror(\"pthread_mutex_unlock\");\n\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\treturn count;\n\t\t\t}\n\t\t} else {\n\t\t\treturn count;\n\t\t}\n\t}\n\n\tfprintf(stderr, \"could not make tile %d/%u/%u small enough\\n\", z, tx, ty);\n\treturn -1;\n}\n\nstruct task {\n\tint fileno = 0;\n\tstruct task *next = NULL;\n};\n\nvoid *run_thread(void *vargs) {\n\twrite_tile_args *arg = (write_tile_args *) vargs;\n\tstruct task *task;\n\n\tfor (task = arg->tasks; task != NULL; task = task->next) {\n\t\tint j = task->fileno;\n\n\t\tif (arg->geomfd[j] < 0) {\n\t\t\t// only one source file for zoom level 0\n\t\t\tcontinue;\n\t\t}\n\t\tif (arg->geom_size[j] == 0) {\n\t\t\tcontinue;\n\t\t}\n\n\t\t// printf(\"%lld of geom_size\\n\", (long long) geom_size[j]);\n\n\t\tFILE *geom = fdopen(arg->geomfd[j], \"rb\");\n\t\tif (geom == NULL) {\n\t\t\tperror(\"mmap geom\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tstd::atomic<long long> geompos(0);\n\t\tlong long prevgeom = 0;\n\n\t\twhile (1) {\n\t\t\tint z;\n\t\t\tunsigned x, y;\n\n\t\t\tif (!deserialize_int_io(geom, &z, &geompos)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdeserialize_uint_io(geom, &x, &geompos);\n\t\t\tdeserialize_uint_io(geom, &y, &geompos);\n\n\t\t\targ->wrote_zoom = z;\n\n\t\t\t// fprintf(stderr, \"%d/%u/%u\\n\", z, x, y);\n\n\t\t\tlong long len = write_tile(geom, &geompos, arg->metabase, arg->stringpool, z, x, y, z == arg->maxzoom ? arg->full_detail : arg->low_detail, arg->min_detail, arg->outdb, arg->outdir, arg->buffer, arg->fname, arg->geomfile, arg->minzoom, arg->maxzoom, arg->todo, arg->along, geompos, arg->gamma, arg->child_shards, arg->meta_off, arg->pool_off, arg->initial_x, arg->initial_y, arg->running, arg->simplification, arg->layermaps, arg->layer_unmaps, arg->tiling_seg, arg->pass, arg->passes, arg->mingap, arg->minextent, arg->fraction, arg->prefilter, arg->postfilter, arg->filter, arg);\n\n\t\t\tif (len < 0) {\n\t\t\t\tint *err = &arg->err;\n\t\t\t\t*err = z - 1;\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\tif (pthread_mutex_lock(&var_lock) != 0) {\n\t\t\t\tperror(\"pthread_mutex_lock\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tif (z == arg->maxzoom) {\n\t\t\t\tif (len > *arg->most) {\n\t\t\t\t\t*arg->midx = x;\n\t\t\t\t\t*arg->midy = y;\n\t\t\t\t\t*arg->most = len;\n\t\t\t\t} else if (len == *arg->most) {\n\t\t\t\t\tunsigned long long a = (((unsigned long long) x) << 32) | y;\n\t\t\t\t\tunsigned long long b = (((unsigned long long) *arg->midx) << 32) | *arg->midy;\n\n\t\t\t\t\tif (a < b) {\n\t\t\t\t\t\t*arg->midx = x;\n\t\t\t\t\t\t*arg->midy = y;\n\t\t\t\t\t\t*arg->most = len;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t*arg->along += geompos - prevgeom;\n\t\t\tprevgeom = geompos;\n\n\t\t\tif (pthread_mutex_unlock(&var_lock) != 0) {\n\t\t\t\tperror(\"pthread_mutex_unlock\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t}\n\n\t\tif (arg->pass == 1) {\n\t\t\t// Since the fclose() has closed the underlying file descriptor\n\t\t\targ->geomfd[j] = -1;\n\t\t} else {\n\t\t\tint newfd = dup(arg->geomfd[j]);\n\t\t\tif (newfd < 0) {\n\t\t\t\tperror(\"dup geometry\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tif (lseek(newfd, 0, SEEK_SET) < 0) {\n\t\t\t\tperror(\"lseek geometry\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\targ->geomfd[j] = newfd;\n\t\t}\n\n\t\tif (fclose(geom) != 0) {\n\t\t\tperror(\"close geom\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\t}\n\n\targ->running--;\n\treturn NULL;\n}\n\nint traverse_zooms(int *geomfd, off_t *geom_size, char *metabase, char *stringpool, std::atomic<unsigned> *midx, std::atomic<unsigned> *midy, int &maxzoom, int minzoom, sqlite3 *outdb, const char *outdir, int buffer, const char *fname, const char *tmpdir, double gamma, int full_detail, int low_detail, int min_detail, long long *meta_off, long long *pool_off, unsigned *initial_x, unsigned *initial_y, double simplification, std::vector<std::map<std::string, layermap_entry>> &layermaps, const char *prefilter, const char *postfilter, std::map<std::string, attribute_op> const *attribute_accum, struct json_object *filter) {\n\tlast_progress = 0;\n\n\t// The existing layermaps are one table per input thread.\n\t// We need to add another one per *tiling* thread so that it can be\n\t// safely changed during tiling.\n\tsize_t layermaps_off = layermaps.size();\n\tfor (size_t i = 0; i < CPUS; i++) {\n\t\tlayermaps.push_back(std::map<std::string, layermap_entry>());\n\t}\n\n\t// Table to map segment and layer number back to layer name\n\tstd::vector<std::vector<std::string>> layer_unmaps;\n\tfor (size_t seg = 0; seg < layermaps.size(); seg++) {\n\t\tlayer_unmaps.push_back(std::vector<std::string>());\n\n\t\tfor (auto a = layermaps[seg].begin(); a != layermaps[seg].end(); ++a) {\n\t\t\tif (a->second.id >= layer_unmaps[seg].size()) {\n\t\t\t\tlayer_unmaps[seg].resize(a->second.id + 1);\n\t\t\t}\n\t\t\tlayer_unmaps[seg][a->second.id] = a->first;\n\t\t}\n\t}\n\n\tint i;\n\tfor (i = 0; i <= maxzoom; i++) {\n\t\tstd::atomic<long long> most(0);\n\n\t\tFILE *sub[TEMP_FILES];\n\t\tint subfd[TEMP_FILES];\n\t\tfor (size_t j = 0; j < TEMP_FILES; j++) {\n\t\t\tchar geomname[strlen(tmpdir) + strlen(\"/geom.XXXXXXXX\" XSTRINGIFY(INT_MAX)) + 1];\n\t\t\tsprintf(geomname, \"%s/geom%zu.XXXXXXXX\", tmpdir, j);\n\t\t\tsubfd[j] = mkstemp_cloexec(geomname);\n\t\t\t// printf(\"%s\\n\", geomname);\n\t\t\tif (subfd[j] < 0) {\n\t\t\t\tperror(geomname);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tsub[j] = fopen_oflag(geomname, \"wb\", O_WRONLY | O_CLOEXEC);\n\t\t\tif (sub[j] == NULL) {\n\t\t\t\tperror(geomname);\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tunlink(geomname);\n\t\t}\n\n\t\tsize_t useful_threads = 0;\n\t\tlong long todo = 0;\n\t\tfor (size_t j = 0; j < TEMP_FILES; j++) {\n\t\t\ttodo += geom_size[j];\n\t\t\tif (geom_size[j] > 0) {\n\t\t\t\tuseful_threads++;\n\t\t\t}\n\t\t}\n\n\t\tsize_t threads = CPUS;\n\t\tif (threads > TEMP_FILES / 4) {\n\t\t\tthreads = TEMP_FILES / 4;\n\t\t}\n\t\t// XXX is it useful to divide further if we know we are skipping\n\t\t// some zoom levels? Is it faster to have fewer CPUs working on\n\t\t// sharding, but more deeply, or fewer CPUs, less deeply?\n\t\tif (threads > useful_threads) {\n\t\t\tthreads = useful_threads;\n\t\t}\n\n\t\t// Round down to a power of 2\n\t\tfor (int e = 0; e < 30; e++) {\n\t\t\tif (threads >= (1U << e) && threads < (1U << (e + 1))) {\n\t\t\t\tthreads = 1U << e;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (threads >= (1U << 30)) {\n\t\t\tthreads = 1U << 30;\n\t\t}\n\t\tif (threads < 1) {\n\t\t\tthreads = 1;\n\t\t}\n\n\t\t// Assign temporary files to threads\n\n\t\tstd::vector<struct task> tasks;\n\t\ttasks.resize(TEMP_FILES);\n\n\t\tstruct dispatch {\n\t\t\tstruct task *tasks = NULL;\n\t\t\tlong long todo = 0;\n\t\t\tstruct dispatch *next = NULL;\n\t\t};\n\t\tstd::vector<struct dispatch> dispatches;\n\t\tdispatches.resize(threads);\n\n\t\tstruct dispatch *dispatch_head = &dispatches[0];\n\t\tfor (size_t j = 0; j < threads; j++) {\n\t\t\tdispatches[j].tasks = NULL;\n\t\t\tdispatches[j].todo = 0;\n\t\t\tif (j + 1 < threads) {\n\t\t\t\tdispatches[j].next = &dispatches[j + 1];\n\t\t\t} else {\n\t\t\t\tdispatches[j].next = NULL;\n\t\t\t}\n\t\t}\n\n\t\tfor (size_t j = 0; j < TEMP_FILES; j++) {\n\t\t\tif (geom_size[j] == 0) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\ttasks[j].fileno = j;\n\t\t\ttasks[j].next = dispatch_head->tasks;\n\t\t\tdispatch_head->tasks = &tasks[j];\n\t\t\tdispatch_head->todo += geom_size[j];\n\n\t\t\tstruct dispatch *here = dispatch_head;\n\t\t\tdispatch_head = dispatch_head->next;\n\n\t\t\tdispatch **d;\n\t\t\tfor (d = &dispatch_head; *d != NULL; d = &((*d)->next)) {\n\t\t\t\tif (here->todo < (*d)->todo) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\there->next = *d;\n\t\t\t*d = here;\n\t\t}\n\n\t\tint err = INT_MAX;\n\n\t\tsize_t start = 1;\n\t\tif (additional[A_INCREASE_GAMMA_AS_NEEDED] || additional[A_DROP_DENSEST_AS_NEEDED] || additional[A_COALESCE_DENSEST_AS_NEEDED] || additional[A_CLUSTER_DENSEST_AS_NEEDED] || additional[A_DROP_FRACTION_AS_NEEDED] || additional[A_COALESCE_FRACTION_AS_NEEDED] || additional[A_DROP_SMALLEST_AS_NEEDED] || additional[A_COALESCE_SMALLEST_AS_NEEDED]) {\n\t\t\tstart = 0;\n\t\t}\n\n\t\tdouble zoom_gamma = gamma;\n\t\tunsigned long long zoom_mingap = ((1LL << (32 - i)) / 256 * cluster_distance) * ((1LL << (32 - i)) / 256 * cluster_distance);\n\t\tlong long zoom_minextent = 0;\n\t\tdouble zoom_fraction = 1;\n\n\t\tfor (size_t pass = start; pass < 2; pass++) {\n\t\t\tpthread_t pthreads[threads];\n\t\t\tstd::vector<write_tile_args> args;\n\t\t\targs.resize(threads);\n\t\t\tstd::atomic<int> running(threads);\n\t\t\tstd::atomic<long long> along(0);\n\n\t\t\tfor (size_t thread = 0; thread < threads; thread++) {\n\t\t\t\targs[thread].metabase = metabase;\n\t\t\t\targs[thread].stringpool = stringpool;\n\t\t\t\targs[thread].min_detail = min_detail;\n\t\t\t\targs[thread].outdb = outdb;  // locked with db_lock\n\t\t\t\targs[thread].outdir = outdir;\n\t\t\t\targs[thread].buffer = buffer;\n\t\t\t\targs[thread].fname = fname;\n\t\t\t\targs[thread].geomfile = sub + thread * (TEMP_FILES / threads);\n\t\t\t\targs[thread].todo = todo;\n\t\t\t\targs[thread].along = &along;  // locked with var_lock\n\t\t\t\targs[thread].gamma = zoom_gamma;\n\t\t\t\targs[thread].gamma_out = zoom_gamma;\n\t\t\t\targs[thread].mingap = zoom_mingap;\n\t\t\t\targs[thread].mingap_out = zoom_mingap;\n\t\t\t\targs[thread].minextent = zoom_minextent;\n\t\t\t\targs[thread].minextent_out = zoom_minextent;\n\t\t\t\targs[thread].fraction = zoom_fraction;\n\t\t\t\targs[thread].fraction_out = zoom_fraction;\n\t\t\t\targs[thread].child_shards = TEMP_FILES / threads;\n\t\t\t\targs[thread].simplification = simplification;\n\n\t\t\t\targs[thread].geomfd = geomfd;\n\t\t\t\targs[thread].geom_size = geom_size;\n\t\t\t\targs[thread].midx = midx;  // locked with var_lock\n\t\t\t\targs[thread].midy = midy;  // locked with var_lock\n\t\t\t\targs[thread].maxzoom = maxzoom;\n\t\t\t\targs[thread].minzoom = minzoom;\n\t\t\t\targs[thread].full_detail = full_detail;\n\t\t\t\targs[thread].low_detail = low_detail;\n\t\t\t\targs[thread].most = &most;  // locked with var_lock\n\t\t\t\targs[thread].meta_off = meta_off;\n\t\t\t\targs[thread].pool_off = pool_off;\n\t\t\t\targs[thread].initial_x = initial_x;\n\t\t\t\targs[thread].initial_y = initial_y;\n\t\t\t\targs[thread].layermaps = &layermaps;\n\t\t\t\targs[thread].layer_unmaps = &layer_unmaps;\n\t\t\t\targs[thread].tiling_seg = thread + layermaps_off;\n\t\t\t\targs[thread].prefilter = prefilter;\n\t\t\t\targs[thread].postfilter = postfilter;\n\t\t\t\targs[thread].attribute_accum = attribute_accum;\n\t\t\t\targs[thread].filter = filter;\n\n\t\t\t\targs[thread].tasks = dispatches[thread].tasks;\n\t\t\t\targs[thread].running = &running;\n\t\t\t\targs[thread].pass = pass;\n\t\t\t\targs[thread].passes = 2 - start;\n\t\t\t\targs[thread].wrote_zoom = -1;\n\t\t\t\targs[thread].still_dropping = false;\n\n\t\t\t\tif (pthread_create(&pthreads[thread], NULL, run_thread, &args[thread]) != 0) {\n\t\t\t\t\tperror(\"pthread_create\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor (size_t thread = 0; thread < threads; thread++) {\n\t\t\t\tvoid *retval;\n\n\t\t\t\tif (pthread_join(pthreads[thread], &retval) != 0) {\n\t\t\t\t\tperror(\"pthread_join\");\n\t\t\t\t}\n\n\t\t\t\tif (retval != NULL) {\n\t\t\t\t\terr = *((int *) retval);\n\t\t\t\t}\n\n\t\t\t\tif (args[thread].gamma_out > zoom_gamma) {\n\t\t\t\t\tzoom_gamma = args[thread].gamma_out;\n\t\t\t\t}\n\t\t\t\tif (args[thread].mingap_out > zoom_mingap) {\n\t\t\t\t\tzoom_mingap = args[thread].mingap_out;\n\t\t\t\t}\n\t\t\t\tif (args[thread].minextent_out > zoom_minextent) {\n\t\t\t\t\tzoom_minextent = args[thread].minextent_out;\n\t\t\t\t}\n\t\t\t\tif (args[thread].fraction_out < zoom_fraction) {\n\t\t\t\t\tzoom_fraction = args[thread].fraction_out;\n\t\t\t\t}\n\n\t\t\t\t// Zoom counter might be lower than reality if zooms are being skipped\n\t\t\t\tif (args[thread].wrote_zoom > i) {\n\t\t\t\t\ti = args[thread].wrote_zoom;\n\t\t\t\t}\n\n\t\t\t\tif (additional[A_EXTEND_ZOOMS] && i == maxzoom && args[thread].still_dropping && maxzoom < MAX_ZOOM) {\n\t\t\t\t\tmaxzoom++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tfor (size_t j = 0; j < TEMP_FILES; j++) {\n\t\t\t// Can be < 0 if there is only one source file, at z0\n\t\t\tif (geomfd[j] >= 0) {\n\t\t\t\tif (close(geomfd[j]) != 0) {\n\t\t\t\t\tperror(\"close geom\");\n\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (fclose(sub[j]) != 0) {\n\t\t\t\tperror(\"close subfile\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tstruct stat geomst;\n\t\t\tif (fstat(subfd[j], &geomst) != 0) {\n\t\t\t\tperror(\"stat geom\\n\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tgeomfd[j] = subfd[j];\n\t\t\tgeom_size[j] = geomst.st_size;\n\t\t}\n\n\t\tif (err != INT_MAX) {\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tfor (size_t j = 0; j < TEMP_FILES; j++) {\n\t\t// Can be < 0 if there is only one source file, at z0\n\t\tif (geomfd[j] >= 0) {\n\t\t\tif (close(geomfd[j]) != 0) {\n\t\t\t\tperror(\"close geom\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!quiet) {\n\t\tfprintf(stderr, \"\\n\");\n\t}\n\treturn maxzoom;\n}\n"
        },
        {
          "name": "tile.hpp",
          "type": "blob",
          "size": 1.3681640625,
          "content": "#ifndef TILE_HPP\n#define TILE_HPP\n\n#include <stdio.h>\n#include <sqlite3.h>\n#include <vector>\n#include <atomic>\n#include <map>\n#include \"mbtiles.hpp\"\n#include \"jsonpull/jsonpull.h\"\n\nenum attribute_op {\n\top_sum,\n\top_product,\n\top_mean,\n\top_concat,\n\top_comma,\n\top_max,\n\top_min,\n};\n\nlong long write_tile(char **geom, char *metabase, char *stringpool, unsigned *file_bbox, int z, unsigned x, unsigned y, int detail, int min_detail, int basezoom, sqlite3 *outdb, const char *outdir, double droprate, int buffer, const char *fname, FILE **geomfile, int file_minzoom, int file_maxzoom, double todo, char *geomstart, long long along, double gamma, int nlayers);\n\nint traverse_zooms(int *geomfd, off_t *geom_size, char *metabase, char *stringpool, std::atomic<unsigned> *midx, std::atomic<unsigned> *midy, int &maxzoom, int minzoom, sqlite3 *outdb, const char *outdir, int buffer, const char *fname, const char *tmpdir, double gamma, int full_detail, int low_detail, int min_detail, long long *meta_off, long long *pool_off, unsigned *initial_x, unsigned *initial_y, double simplification, std::vector<std::map<std::string, layermap_entry> > &layermap, const char *prefilter, const char *postfilter, std::map<std::string, attribute_op> const *attribute_accum, struct json_object *filter);\n\nint manage_gap(unsigned long long index, unsigned long long *previndex, double scale, double gamma, double *gap);\n\n#endif\n"
        },
        {
          "name": "unit.cpp",
          "type": "blob",
          "size": 1.0634765625,
          "content": "#define CATCH_CONFIG_MAIN\n#include \"catch/catch.hpp\"\n#include \"text.hpp\"\n\nTEST_CASE(\"UTF-8 enforcement\", \"[utf8]\") {\n\tREQUIRE(check_utf8(\"\") == std::string(\"\"));\n\tREQUIRE(check_utf8(\"hello world\") == std::string(\"\"));\n\tREQUIRE(check_utf8(\"Καλημέρα κόσμε\") == std::string(\"\"));\n\tREQUIRE(check_utf8(\"こんにちは 世界\") == std::string(\"\"));\n\tREQUIRE(check_utf8(\"👋🌏\") == std::string(\"\"));\n\tREQUIRE(check_utf8(\"Hola m\\xF3n\") == std::string(\"\\\"Hola m\\xF3n\\\" is not valid UTF-8 (0xF3 0x6E)\"));\n}\n\nTEST_CASE(\"UTF-8 truncation\", \"[trunc]\") {\n\tREQUIRE(truncate16(\"0123456789abcdefghi\", 16) == std::string(\"0123456789abcdef\"));\n\tREQUIRE(truncate16(\"0123456789éîôüéîôüç\", 16) == std::string(\"0123456789éîôüéî\"));\n\tREQUIRE(truncate16(\"0123456789😀😬😁😂😃😄😅😆\", 16) == std::string(\"0123456789😀😬😁\"));\n\tREQUIRE(truncate16(\"0123456789😀😬😁😂😃😄😅😆\", 17) == std::string(\"0123456789😀😬😁\"));\n\tREQUIRE(truncate16(\"0123456789あいうえおかきくけこさ\", 16) == std::string(\"0123456789あいうえおか\"));\n}\n"
        },
        {
          "name": "vector_tile.proto",
          "type": "blob",
          "size": 3.67578125,
          "content": "// Protocol Version 1\n\npackage mapnik.vector;\n\noption optimize_for = LITE_RUNTIME;\n\nmessage tile {\n        enum GeomType {\n             Unknown = 0;\n             Point = 1;\n             LineString = 2;\n             Polygon = 3;\n        }\n\n        // Variant type encoding\n        message value {\n                // Exactly one of these values may be present in a valid message\n                optional string string_value = 1;\n                optional float float_value = 2;\n                optional double double_value = 3;\n                optional int64 int_value = 4;\n                optional uint64 uint_value = 5;\n                optional sint64 sint_value = 6;\n                optional bool bool_value = 7;\n\n                extensions 8 to max;\n        }\n\n        message feature {\n                optional uint64 id = 1;\n\n                // Tags of this feature. Even numbered values refer to the nth\n                // value in the keys list on the tile message, odd numbered\n                // values refer to the nth value in the values list on the tile\n                // message.\n                repeated uint32 tags = 2 [ packed = true ];\n\n                // The type of geometry stored in this feature.\n                optional GeomType type = 3 [ default = Unknown ];\n\n                // Contains a stream of commands and parameters (vertices). The\n                // repeat count is shifted to the left by 3 bits. This means\n                // that the command has 3 bits (0-7). The repeat count\n                // indicates how often this command is to be repeated. Defined\n                // commands are:\n                // - MoveTo:    1   (2 parameters follow)\n                // - LineTo:    2   (2 parameters follow)\n                // - ClosePath: 7   (no parameters follow)\n                //\n                // Ex.: MoveTo(3, 6), LineTo(8, 12), LineTo(20, 34), ClosePath\n                // Encoded as: [ 9 3 6 18 5 6 12 22 7 ]\n                //                                  == command type 7 (ClosePath)\n                //                             ===== relative LineTo(+12, +22) == LineTo(20, 34)\n                //                         === relative LineTo(+5, +6) == LineTo(8, 12)\n                //                      == [00010 010] = command type 2 (LineTo), length 2\n                //                  === relative MoveTo(+3, +6)\n                //              == [00001 001] = command type 1 (MoveTo), length 1\n                // Commands are encoded as uint32 varints, vertex parameters are\n                // encoded as sint32 varints (zigzag). Vertex parameters are\n                // also encoded as deltas to the previous position. The original\n                // position is (0,0)\n                repeated uint32 geometry = 4 [ packed = true ];\n        }\n\n        message layer {\n                // Any compliant implementation must first read the version\n                // number encoded in this message and choose the correct\n                // implementation for this version number before proceeding to\n                // decode other parts of this message.\n                required uint32 version = 15 [ default = 1 ];\n\n                required string name = 1;\n\n                // The actual features in this tile.\n                repeated feature features = 2;\n\n                // Dictionary encoding for keys\n                repeated string keys = 3;\n\n                // Dictionary encoding for values\n                repeated value values = 4;\n\n                // The bounding box in this tile spans from 0..4095 units\n                optional uint32 extent = 5 [ default = 4096 ];\n\n                extensions 16 to max;\n        }\n\n        repeated layer layers = 3;\n\n        extensions 16 to 8191;\n}\n"
        },
        {
          "name": "version.hpp",
          "type": "blob",
          "size": 0.0732421875,
          "content": "#ifndef VERSION_HPP\n#define VERSION_HPP\n\n#define VERSION \"v1.36.0\"\n\n#endif\n"
        },
        {
          "name": "write_json.cpp",
          "type": "blob",
          "size": 14.025390625,
          "content": "// for vasprintf() on Linux\n#ifndef _GNU_SOURCE\n#define _GNU_SOURCE\n#endif\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <vector>\n#include <map>\n#include <string>\n#include \"projection.hpp\"\n#include \"geometry.hpp\"\n#include \"mvt.hpp\"\n#include \"write_json.hpp\"\n#include \"milo/dtoa_milo.h\"\n\nvoid json_writer::json_adjust() {\n\tif (state.size() == 0) {\n\t\tstate.push_back(JSON_WRITE_TOP);\n\t} else if (state[state.size() - 1] == JSON_WRITE_TOP) {\n\t\taddc('\\n');\n\t\tstate[state.size() - 1] = JSON_WRITE_TOP;\n\t} else if (state[state.size() - 1] == JSON_WRITE_HASH) {\n\t\tif (!nospace) {\n\t\t\taddc(' ');\n\t\t}\n\t\tnospace = false;\n\t\tstate[state.size() - 1] = JSON_WRITE_HASH_KEY;\n\t} else if (state[state.size() - 1] == JSON_WRITE_HASH_KEY) {\n\t\tadds(\": \");\n\t\tstate[state.size() - 1] = JSON_WRITE_HASH_VALUE;\n\t} else if (state[state.size() - 1] == JSON_WRITE_HASH_VALUE) {\n\t\tif (wantnl) {\n\t\t\tadds(\",\\n\");\n\t\t\tnospace = false;\n\t\t} else if (nospace) {\n\t\t\taddc(',');\n\t\t\tnospace = false;\n\t\t} else {\n\t\t\tadds(\", \");\n\t\t}\n\t\twantnl = false;\n\t\tstate[state.size() - 1] = JSON_WRITE_HASH_KEY;\n\t} else if (state[state.size() - 1] == JSON_WRITE_ARRAY) {\n\t\tif (!nospace) {\n\t\t\taddc(' ');\n\t\t}\n\t\tnospace = false;\n\t\tstate[state.size() - 1] = JSON_WRITE_ARRAY_ELEMENT;\n\t} else if (state[state.size() - 1] == JSON_WRITE_ARRAY_ELEMENT) {\n\t\tif (wantnl) {\n\t\t\tadds(\",\\n\");\n\t\t\tnospace = false;\n\t\t} else if (nospace) {\n\t\t\taddc(',');\n\t\t\tnospace = false;\n\t\t} else {\n\t\t\tadds(\", \");\n\t\t}\n\t\twantnl = false;\n\t\tstate[state.size() - 1] = JSON_WRITE_ARRAY_ELEMENT;\n\t} else {\n\t\tfprintf(stderr, \"Impossible JSON state\\n\");\n\t\texit(EXIT_FAILURE);\n\t}\n}\n\nvoid json_writer::json_write_array() {\n\tjson_adjust();\n\taddc('[');\n\n\tstate.push_back(JSON_WRITE_ARRAY);\n}\n\nvoid json_writer::json_end_array() {\n\tif (state.size() == 0) {\n\t\tfprintf(stderr, \"End JSON array at top level\\n\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tjson_write_tok tok = state[state.size() - 1];\n\tstate.pop_back();\n\n\tif (tok == JSON_WRITE_ARRAY || tok == JSON_WRITE_ARRAY_ELEMENT) {\n\t\tif (!nospace) {\n\t\t\taddc(' ');\n\t\t}\n\t\tnospace = false;\n\t\taddc(']');\n\t} else {\n\t\tfprintf(stderr, \"End JSON array with unexpected state\\n\");\n\t\texit(EXIT_FAILURE);\n\t}\n}\n\nvoid json_writer::json_write_hash() {\n\tjson_adjust();\n\taddc('{');\n\n\tstate.push_back(JSON_WRITE_HASH);\n}\n\nvoid json_writer::json_end_hash() {\n\tif (state.size() == 0) {\n\t\tfprintf(stderr, \"End JSON hash at top level\\n\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tjson_write_tok tok = state[state.size() - 1];\n\tstate.pop_back();\n\n\tif (tok == JSON_WRITE_HASH) {\n\t\tif (!nospace) {\n\t\t\tadds(\"  \");  // Preserve accidental extra space from before\n\t\t}\n\t\tnospace = false;\n\t\taddc('}');\n\t} else if (tok == JSON_WRITE_HASH_VALUE) {\n\t\tif (!nospace) {\n\t\t\taddc(' ');\n\t\t}\n\t\tnospace = false;\n\t\taddc('}');\n\t} else {\n\t\tfprintf(stderr, \"End JSON hash with unexpected state\\n\");\n\t\texit(EXIT_FAILURE);\n\t}\n}\n\nvoid json_writer::json_write_string(std::string const &str) {\n\tjson_adjust();\n\n\taddc('\"');\n\tfor (size_t i = 0; i < str.size(); i++) {\n\t\tif (str[i] == '\\\\' || str[i] == '\"') {\n\t\t\taprintf(\"\\\\%c\", str[i]);\n\t\t} else if ((unsigned char) str[i] < ' ') {\n\t\t\taprintf(\"\\\\u%04x\", str[i]);\n\t\t} else {\n\t\t\taddc(str[i]);\n\t\t}\n\t}\n\taddc('\"');\n}\n\nvoid json_writer::json_write_number(double d) {\n\tjson_adjust();\n\n\tadds(milo::dtoa_milo(d).c_str());\n}\n\n// Just to avoid json_writer:: changing expected output format\nvoid json_writer::json_write_float(double d) {\n\tjson_adjust();\n\n\taprintf(\"%f\", d);\n}\n\nvoid json_writer::json_write_unsigned(unsigned long long v) {\n\tjson_adjust();\n\n\taprintf(\"%llu\", v);\n}\n\nvoid json_writer::json_write_signed(long long v) {\n\tjson_adjust();\n\n\taprintf(\"%lld\", v);\n}\n\nvoid json_writer::json_write_stringified(std::string const &str) {\n\tjson_adjust();\n\n\tadds(str);\n}\n\nvoid json_writer::json_write_bool(bool b) {\n\tjson_adjust();\n\n\tif (b) {\n\t\tadds(\"true\");\n\t} else {\n\t\tadds(\"false\");\n\t}\n}\n\nvoid json_writer::json_write_null() {\n\tjson_adjust();\n\n\tadds(\"null\");\n}\n\nvoid json_writer::json_write_newline() {\n\taddc('\\n');\n\tnospace = true;\n}\n\nvoid json_writer::json_comma_newline() {\n\twantnl = true;\n}\n\nvoid json_writer::aprintf(const char *format, ...) {\n\tva_list ap;\n\tchar *tmp;\n\n\tva_start(ap, format);\n\tif (vasprintf(&tmp, format, ap) < 0) {\n\t\tfprintf(stderr, \"memory allocation failure\\n\");\n\t\texit(EXIT_FAILURE);\n\t}\n\tva_end(ap);\n\n\tadds(std::string(tmp, strlen(tmp)));\n\tfree(tmp);\n}\n\nvoid json_writer::addc(char c) {\n\tif (f != NULL) {\n\t\tputc(c, f);\n\t} else if (s != NULL) {\n\t\ts->push_back(c);\n\t}\n}\n\nvoid json_writer::adds(std::string const &str) {\n\tif (f != NULL) {\n\t\tfputs(str.c_str(), f);\n\t} else if (s != NULL) {\n\t\ts->append(str);\n\t}\n}\n\nstruct lonlat {\n\tint op;\n\tdouble lon;\n\tdouble lat;\n\tlong long x;\n\tlong long y;\n\n\tlonlat(int nop, double nlon, double nlat, long long nx, long long ny)\n\t    : op(nop),\n\t      lon(nlon),\n\t      lat(nlat),\n\t      x(nx),\n\t      y(ny) {\n\t}\n};\n\nvoid layer_to_geojson(mvt_layer const &layer, unsigned z, unsigned x, unsigned y, bool comma, bool name, bool zoom, bool dropped, unsigned long long index, long long sequence, long long extent, bool complain, json_writer &state) {\n\tfor (size_t f = 0; f < layer.features.size(); f++) {\n\t\tmvt_feature const &feat = layer.features[f];\n\n\t\tstate.json_write_hash();\n\t\tstate.json_write_string(\"type\");\n\t\tstate.json_write_string(\"Feature\");\n\n\t\tif (feat.has_id) {\n\t\t\tstate.json_write_string(\"id\");\n\t\t\tstate.json_write_unsigned(feat.id);\n\t\t}\n\n\t\tif (name || zoom || index != 0 || sequence != 0 || extent != 0) {\n\t\t\tstate.json_write_string(\"tippecanoe\");\n\t\t\tstate.json_write_hash();\n\n\t\t\tif (name) {\n\t\t\t\tstate.json_write_string(\"layer\");\n\t\t\t\tstate.json_write_string(layer.name);\n\t\t\t}\n\n\t\t\tif (zoom) {\n\t\t\t\tstate.json_write_string(\"minzoom\");\n\t\t\t\tstate.json_write_unsigned(z);\n\n\t\t\t\tstate.json_write_string(\"maxzoom\");\n\t\t\t\tstate.json_write_unsigned(z);\n\t\t\t}\n\n\t\t\tif (dropped) {\n\t\t\t\tstate.json_write_string(\"dropped\");\n\t\t\t\tstate.json_write_bool(feat.dropped);\n\t\t\t}\n\n\t\t\tif (index != 0) {\n\t\t\t\tstate.json_write_string(\"index\");\n\t\t\t\tstate.json_write_unsigned(index);\n\t\t\t}\n\n\t\t\tif (sequence != 0) {\n\t\t\t\tstate.json_write_string(\"sequence\");\n\t\t\t\tstate.json_write_signed(sequence);\n\t\t\t}\n\n\t\t\tif (extent != 0) {\n\t\t\t\tstate.json_write_string(\"extent\");\n\t\t\t\tstate.json_write_signed(extent);\n\t\t\t}\n\n\t\t\tstate.json_end_hash();\n\t\t}\n\n\t\tstate.json_write_string(\"properties\");\n\t\tstate.json_write_hash();\n\n\t\tfor (size_t t = 0; t + 1 < feat.tags.size(); t += 2) {\n\t\t\tif (feat.tags[t] >= layer.keys.size()) {\n\t\t\t\tfprintf(stderr, \"Error: out of bounds feature key (%u in %zu)\\n\", feat.tags[t], layer.keys.size());\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tif (feat.tags[t + 1] >= layer.values.size()) {\n\t\t\t\tfprintf(stderr, \"Error: out of bounds feature value (%u in %zu)\\n\", feat.tags[t + 1], layer.values.size());\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\n\t\t\tconst char *key = layer.keys[feat.tags[t]].c_str();\n\t\t\tmvt_value const &val = layer.values[feat.tags[t + 1]];\n\n\t\t\tif (val.type == mvt_string) {\n\t\t\t\tstate.json_write_string(key);\n\t\t\t\tstate.json_write_string(val.string_value);\n\t\t\t} else if (val.type == mvt_int) {\n\t\t\t\tstate.json_write_string(key);\n\t\t\t\tstate.json_write_signed(val.numeric_value.int_value);\n\t\t\t} else if (val.type == mvt_double) {\n\t\t\t\tstate.json_write_string(key);\n\t\t\t\tstate.json_write_number(val.numeric_value.double_value);\n\t\t\t} else if (val.type == mvt_float) {\n\t\t\t\tstate.json_write_string(key);\n\t\t\t\tstate.json_write_number(val.numeric_value.float_value);\n\t\t\t} else if (val.type == mvt_sint) {\n\t\t\t\tstate.json_write_string(key);\n\t\t\t\tstate.json_write_signed(val.numeric_value.sint_value);\n\t\t\t} else if (val.type == mvt_uint) {\n\t\t\t\tstate.json_write_string(key);\n\t\t\t\tstate.json_write_unsigned(val.numeric_value.uint_value);\n\t\t\t} else if (val.type == mvt_bool) {\n\t\t\t\tstate.json_write_string(key);\n\t\t\t\tstate.json_write_bool(val.numeric_value.bool_value);\n\t\t\t} else if (val.type == mvt_null) {\n\t\t\t\tstate.json_write_string(key);\n\t\t\t\tstate.json_write_null();\n\t\t\t} else {\n\t\t\t\tfprintf(stderr, \"Internal error: property with unknown type\\n\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t}\n\n\t\tstate.json_end_hash();\n\n\t\tstate.json_write_string(\"geometry\");\n\t\tstate.json_write_hash();\n\n\t\tstd::vector<lonlat> ops;\n\n\t\tfor (size_t g = 0; g < feat.geometry.size(); g++) {\n\t\t\tint op = feat.geometry[g].op;\n\t\t\tlong long px = feat.geometry[g].x;\n\t\t\tlong long py = feat.geometry[g].y;\n\n\t\t\tif (op == VT_MOVETO || op == VT_LINETO) {\n\t\t\t\tlong long scale = 1LL << (32 - z);\n\t\t\t\tlong long wx = scale * x + (scale / layer.extent) * px;\n\t\t\t\tlong long wy = scale * y + (scale / layer.extent) * py;\n\n\t\t\t\tdouble lat, lon;\n\t\t\t\tprojection->unproject(wx, wy, 32, &lon, &lat);\n\n\t\t\t\tops.push_back(lonlat(op, lon, lat, px, py));\n\t\t\t} else {\n\t\t\t\tops.push_back(lonlat(op, 0, 0, 0, 0));\n\t\t\t}\n\t\t}\n\n\t\tif (feat.type == VT_POINT) {\n\t\t\tif (ops.size() == 1) {\n\t\t\t\tstate.json_write_string(\"type\");\n\t\t\t\tstate.json_write_string(\"Point\");\n\n\t\t\t\tstate.json_write_string(\"coordinates\");\n\n\t\t\t\tstate.json_write_array();\n\t\t\t\tstate.json_write_float(ops[0].lon);\n\t\t\t\tstate.json_write_float(ops[0].lat);\n\t\t\t\tstate.json_end_array();\n\t\t\t} else {\n\t\t\t\tstate.json_write_string(\"type\");\n\t\t\t\tstate.json_write_string(\"MultiPoint\");\n\n\t\t\t\tstate.json_write_string(\"coordinates\");\n\t\t\t\tstate.json_write_array();\n\n\t\t\t\tfor (size_t i = 0; i < ops.size(); i++) {\n\t\t\t\t\tstate.json_write_array();\n\t\t\t\t\tstate.json_write_float(ops[i].lon);\n\t\t\t\t\tstate.json_write_float(ops[i].lat);\n\t\t\t\t\tstate.json_end_array();\n\t\t\t\t}\n\n\t\t\t\tstate.json_end_array();\n\t\t\t}\n\t\t} else if (feat.type == VT_LINE) {\n\t\t\tint movetos = 0;\n\t\t\tfor (size_t i = 0; i < ops.size(); i++) {\n\t\t\t\tif (ops[i].op == VT_MOVETO) {\n\t\t\t\t\tmovetos++;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (movetos < 2) {\n\t\t\t\tstate.json_write_string(\"type\");\n\t\t\t\tstate.json_write_string(\"LineString\");\n\n\t\t\t\tstate.json_write_string(\"coordinates\");\n\t\t\t\tstate.json_write_array();\n\n\t\t\t\tfor (size_t i = 0; i < ops.size(); i++) {\n\t\t\t\t\tstate.json_write_array();\n\t\t\t\t\tstate.json_write_float(ops[i].lon);\n\t\t\t\t\tstate.json_write_float(ops[i].lat);\n\t\t\t\t\tstate.json_end_array();\n\t\t\t\t}\n\n\t\t\t\tstate.json_end_array();\n\t\t\t} else {\n\t\t\t\tstate.json_write_string(\"type\");\n\t\t\t\tstate.json_write_string(\"MultiLineString\");\n\n\t\t\t\tstate.json_write_string(\"coordinates\");\n\t\t\t\tstate.json_write_array();\n\t\t\t\tstate.json_write_array();\n\n\t\t\t\tint sstate = 0;\n\t\t\t\tfor (size_t i = 0; i < ops.size(); i++) {\n\t\t\t\t\tif (ops[i].op == VT_MOVETO) {\n\t\t\t\t\t\tif (sstate == 0) {\n\t\t\t\t\t\t\tstate.json_write_array();\n\t\t\t\t\t\t\tstate.json_write_float(ops[i].lon);\n\t\t\t\t\t\t\tstate.json_write_float(ops[i].lat);\n\t\t\t\t\t\t\tstate.json_end_array();\n\n\t\t\t\t\t\t\tsstate = 1;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tstate.json_end_array();\n\t\t\t\t\t\t\tstate.json_write_array();\n\n\t\t\t\t\t\t\tstate.json_write_array();\n\t\t\t\t\t\t\tstate.json_write_float(ops[i].lon);\n\t\t\t\t\t\t\tstate.json_write_float(ops[i].lat);\n\t\t\t\t\t\t\tstate.json_end_array();\n\n\t\t\t\t\t\t\tsstate = 1;\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstate.json_write_array();\n\t\t\t\t\t\tstate.json_write_float(ops[i].lon);\n\t\t\t\t\t\tstate.json_write_float(ops[i].lat);\n\t\t\t\t\t\tstate.json_end_array();\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tstate.json_end_array();\n\t\t\t\tstate.json_end_array();\n\t\t\t}\n\t\t} else if (feat.type == VT_POLYGON) {\n\t\t\tstd::vector<std::vector<lonlat> > rings;\n\t\t\tstd::vector<double> areas;\n\n\t\t\tfor (size_t i = 0; i < ops.size(); i++) {\n\t\t\t\tif (ops[i].op == VT_MOVETO) {\n\t\t\t\t\trings.push_back(std::vector<lonlat>());\n\t\t\t\t\tareas.push_back(0);\n\t\t\t\t}\n\n\t\t\t\tint n = rings.size() - 1;\n\t\t\t\tif (n >= 0) {\n\t\t\t\t\tif (ops[i].op == VT_CLOSEPATH) {\n\t\t\t\t\t\trings[n].push_back(rings[n][0]);\n\t\t\t\t\t} else {\n\t\t\t\t\t\trings[n].push_back(ops[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (i + 1 >= ops.size() || ops[i + 1].op == VT_MOVETO) {\n\t\t\t\t\tif (ops[i].op != VT_CLOSEPATH) {\n\t\t\t\t\t\tstatic bool warned = false;\n\n\t\t\t\t\t\tif (!warned) {\n\t\t\t\t\t\t\tfprintf(stderr, \"Ring does not end with closepath (ends with %d)\\n\", ops[i].op);\n\t\t\t\t\t\t\tif (complain) {\n\t\t\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\twarned = true;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tint outer = 0;\n\n\t\t\tfor (size_t i = 0; i < rings.size(); i++) {\n\t\t\t\tlong double area = 0;\n\t\t\t\tfor (size_t k = 0; k < rings[i].size(); k++) {\n\t\t\t\t\tif (rings[i][k].op != VT_CLOSEPATH) {\n\t\t\t\t\t\tarea += (long double) rings[i][k].x * (long double) rings[i][(k + 1) % rings[i].size()].y;\n\t\t\t\t\t\tarea -= (long double) rings[i][k].y * (long double) rings[i][(k + 1) % rings[i].size()].x;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tarea /= 2;\n\n\t\t\t\tareas[i] = area;\n\t\t\t\tif (areas[i] >= 0 || i == 0) {\n\t\t\t\t\touter++;\n\t\t\t\t}\n\n\t\t\t\t// fprintf(\"\\\"area\\\": %Lf,\", area);\n\t\t\t}\n\n\t\t\tif (outer > 1) {\n\t\t\t\tstate.json_write_string(\"type\");\n\t\t\t\tstate.json_write_string(\"MultiPolygon\");\n\n\t\t\t\tstate.json_write_string(\"coordinates\");\n\t\t\t\tstate.json_write_array();\n\t\t\t\tstate.json_write_array();\n\t\t\t\tstate.json_write_array();\n\t\t\t} else {\n\t\t\t\tstate.json_write_string(\"type\");\n\t\t\t\tstate.json_write_string(\"Polygon\");\n\n\t\t\t\tstate.json_write_string(\"coordinates\");\n\t\t\t\tstate.json_write_array();\n\t\t\t\tstate.json_write_array();\n\t\t\t}\n\n\t\t\tint sstate = 0;\n\t\t\tfor (size_t i = 0; i < rings.size(); i++) {\n\t\t\t\tif (i == 0 && areas[i] < 0) {\n\t\t\t\t\tstatic bool warned = false;\n\n\t\t\t\t\tif (!warned) {\n\t\t\t\t\t\tfprintf(stderr, \"Polygon begins with an inner ring\\n\");\n\t\t\t\t\t\tif (complain) {\n\t\t\t\t\t\t\texit(EXIT_FAILURE);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\twarned = true;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (areas[i] >= 0) {\n\t\t\t\t\tif (sstate != 0) {\n\t\t\t\t\t\t// new multipolygon\n\t\t\t\t\t\tstate.json_end_array();\n\t\t\t\t\t\tstate.json_end_array();\n\n\t\t\t\t\t\tstate.json_write_array();\n\t\t\t\t\t\tstate.json_write_array();\n\t\t\t\t\t}\n\t\t\t\t\tsstate = 1;\n\t\t\t\t}\n\n\t\t\t\tif (sstate == 2) {\n\t\t\t\t\t// new ring in the same polygon\n\t\t\t\t\tstate.json_end_array();\n\t\t\t\t\tstate.json_write_array();\n\t\t\t\t}\n\n\t\t\t\tfor (size_t j = 0; j < rings[i].size(); j++) {\n\t\t\t\t\tif (rings[i][j].op != VT_CLOSEPATH) {\n\t\t\t\t\t\tstate.json_write_array();\n\t\t\t\t\t\tstate.json_write_float(rings[i][j].lon);\n\t\t\t\t\t\tstate.json_write_float(rings[i][j].lat);\n\t\t\t\t\t\tstate.json_end_array();\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstate.json_write_array();\n\t\t\t\t\t\tstate.json_write_float(rings[i][0].lon);\n\t\t\t\t\t\tstate.json_write_float(rings[i][0].lat);\n\t\t\t\t\t\tstate.json_end_array();\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tsstate = 2;\n\t\t\t}\n\n\t\t\tif (outer > 1) {\n\t\t\t\tstate.json_end_array();\n\t\t\t\tstate.json_end_array();\n\t\t\t\tstate.json_end_array();\n\t\t\t} else {\n\t\t\t\tstate.json_end_array();\n\t\t\t\tstate.json_end_array();\n\t\t\t}\n\t\t}\n\n\t\tstate.json_end_hash();\n\t\tstate.json_end_hash();\n\n\t\tif (comma) {\n\t\t\tstate.json_write_newline();\n\t\t\tstate.json_comma_newline();\n\t\t}\n\t}\n}\n\nvoid fprintq(FILE *fp, const char *s) {\n\tfputc('\"', fp);\n\tfor (; *s; s++) {\n\t\tif (*s == '\\\\' || *s == '\"') {\n\t\t\tfprintf(fp, \"\\\\%c\", *s);\n\t\t} else if (*s >= 0 && *s < ' ') {\n\t\t\tfprintf(fp, \"\\\\u%04x\", *s);\n\t\t} else {\n\t\t\tfputc(*s, fp);\n\t\t}\n\t}\n\tfputc('\"', fp);\n}\n"
        },
        {
          "name": "write_json.hpp",
          "type": "blob",
          "size": 1.515625,
          "content": "#ifndef WRITE_JSON_HPP\n#define WRITE_JSON_HPP\n\n#include <string>\n#include <vector>\n#include <stdio.h>\n\nenum json_write_tok {\n\tJSON_WRITE_HASH,\n\tJSON_WRITE_HASH_KEY,\n\tJSON_WRITE_HASH_VALUE,\n\tJSON_WRITE_ARRAY,\n\tJSON_WRITE_ARRAY_ELEMENT,\n\tJSON_WRITE_TOP,\n};\n\nstruct json_writer {\n\tstd::vector<json_write_tok> state;\n\tbool nospace = false;\n\tbool wantnl = false;\n\tFILE *f = NULL;\n\tstd::string *s = NULL;\n\n\t~json_writer() {\n\t\tif (state.size() > 0) {\n\t\t\tif (state.size() != 1 || state[0] != JSON_WRITE_TOP) {\n\t\t\t\tfprintf(stderr, \"JSON not closed at end\\n\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t}\n\t}\n\n\tjson_writer(FILE *fp) {\n\t\tf = fp;\n\t}\n\n\tjson_writer(std::string *out) {\n\t\ts = out;\n\t}\n\n\tvoid json_write_array();\n\tvoid json_end_array();\n\tvoid json_write_hash();\n\tvoid json_end_hash();\n\tvoid json_write_string(std::string const &s);\n\tvoid json_write_number(double d);\n\tvoid json_write_float(double d);\n\tvoid json_write_unsigned(unsigned long long v);\n\tvoid json_write_signed(long long v);\n\tvoid json_write_stringified(std::string const &s);\n\tvoid json_write_bool(bool b);\n\tvoid json_write_null();\n\tvoid json_write_newline();\n\tvoid json_comma_newline();\n\n       private:\n\tvoid json_adjust();\n\tvoid aprintf(const char *format, ...);\n\tvoid addc(char c);\n\tvoid adds(std::string const &s);\n};\n\nvoid layer_to_geojson(mvt_layer const &layer, unsigned z, unsigned x, unsigned y, bool comma, bool name, bool zoom, bool dropped, unsigned long long index, long long sequence, long long extent, bool complain, json_writer &state);\nvoid fprintq(FILE *f, const char *s);\n\n#endif\n"
        }
      ]
    }
  ]
}