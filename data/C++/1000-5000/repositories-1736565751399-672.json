{
  "metadata": {
    "timestamp": 1736565751399,
    "page": 672,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ashvardanian/StringZilla",
      "stars": 2348,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 1.763671875,
          "content": "Language: Cpp\nBasedOnStyle: LLVM\nIndentWidth: 4\nTabWidth: 4\nNamespaceIndentation: None\nColumnLimit: 120\nReflowComments: true\nUseTab: Never\n\nAlignConsecutiveAssignments: false\nAlignConsecutiveDeclarations: false\nAlignEscapedNewlines: true\nAlignOperands: true\nAlignTrailingComments: true\n\nAllowAllArgumentsOnNextLine: false\nAllowAllConstructorInitializersOnNextLine: true\nAllowAllParametersOfDeclarationOnNextLine: true\nAllowShortBlocksOnASingleLine: Always\nAllowShortIfStatementsOnASingleLine: Always\nAllowShortCaseLabelsOnASingleLine: true\nAllowShortFunctionsOnASingleLine: true\nAllowShortLambdasOnASingleLine: true\nAllowShortLoopsOnASingleLine: true\nAlwaysBreakTemplateDeclarations: Yes\nAlwaysBreakAfterReturnType: None\nPenaltyReturnTypeOnItsOwnLine: 200\n\nBreakBeforeBraces: Custom\nBraceWrapping:\n  AfterCaseLabel: false\n  AfterClass: false\n  AfterControlStatement: false\n  AfterEnum: false\n  AfterExternBlock: false\n  AfterFunction: false\n  AfterStruct: false\n  AfterNamespace: false\n  AfterUnion: false\n  BeforeCatch: true\n  BeforeElse: true\n  SplitEmptyFunction: false\n  SplitEmptyRecord: false\n  SplitEmptyNamespace: false\n  IndentBraces: false\n\nSortIncludes: true\nSortUsingDeclarations: true\n\nSpaceAfterCStyleCast: false\nSpaceAfterLogicalNot: false\nSpaceAfterTemplateKeyword: true\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeCpp11BracedList: true\nSpaceBeforeCtorInitializerColon: true\nSpaceBeforeInheritanceColon: true\nSpaceBeforeParens: ControlStatements\nSpaceBeforeRangeBasedForLoopColon: true\nSpaceInEmptyParentheses: false\nSpacesBeforeTrailingComments: 1\nSpacesInAngles: false\nSpacesInCStyleCastParentheses: false\nSpacesInContainerLiterals: false\nSpacesInParentheses: false\nSpacesInSquareBrackets: false\n\nBinPackArguments: true\nBinPackParameters: true\nPenaltyBreakBeforeFirstCallParameter: 1\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.6181640625,
          "content": "# Primary build folders\nbuild/\nbuild_debug/\nbuild_release/\nbuild_artifacts*\n\n# Yes, everyone loves keeping this file in the history.\n# But with a very minimalistic binding and just a couple of dependencies \n# it brings 7000 lines of text polluting the entire repo.\npackage-lock.json\n\n# Temporary files\n.DS_Store\n.swiftpm/\n.build/\ntmp/\ntarget/\n__pycache__\n.pytest_cache\n.conda\nMakefile\nCMakeCache.txt\ncmake_install.cmake\nCMakeFiles\n*.so\n*.egg-info\n*.whl\n*.pyd\n.venv/*\nnode_modules/\n.vs/\n*.tar.gz\n\n# Recommended datasets\nleipzig1M.txt\nenwik9.txt\nxlsum.csv\nhuman_protein_1200row_800len.txt\n\n# StringZilla-specific log files\n/failed_sz_*"
        },
        {
          "name": ".npmignore",
          "type": "blob",
          "size": 0.373046875,
          "content": "python/\nscripts/\n.github/\n.vscode/\n.releaserc\nCMakeLists.txt\npyproject.toml\nsetup.py\nstringzilla.jpeg\n.clang-format\n.gitignore\n\n# Copied from .gitignore\n\nbuild/\n.DS_Store\ntmp/\nbuild_debug/\nbuild_release/\n__pycache__\n.pytest_cache\n.conda\nMakefile\nCMakeCache.txt\ncmake_install.cmake\nCMakeFiles\nCMakeLists.txt~\nsubstr_search_cpp\n*.so\n*.egg-info\n*.whl\n*.pyd\nnode_modules/\n\nleipzig1M.txt"
        },
        {
          "name": ".releaserc",
          "type": "blob",
          "size": 2.44140625,
          "content": "{\n    \"branches\": [\n        \"main\"\n    ],\n    \"debug\": true,\n    \"ci\": true,\n    \"dryRun\": false,\n    \"plugins\": [\n        [\n            \"@semantic-release/commit-analyzer\",\n            {\n                \"preset\": \"eslint\",\n                \"releaseRules\": [\n                    {\n                        \"tag\": \"Break\",\n                        \"release\": \"major\"\n                    },\n                    {\n                        \"tag\": \"Add\",\n                        \"release\": \"minor\"\n                    },\n                    {\n                        \"tag\": \"Improve\",\n                        \"release\": \"patch\"\n                    },\n                    {\n                        \"tag\": \"Make\",\n                        \"release\": \"patch\"\n                    },\n                    {\n                        \"tag\": \"Refactor\",\n                        \"release\": false\n                    }\n                ]\n            }\n        ],\n        [\n            \"@semantic-release/release-notes-generator\",\n            {\n                \"preset\": \"eslint\",\n                \"releaseRules\": [\n                    {\n                        \"tag\": \"Break\",\n                        \"release\": \"major\"\n                    },\n                    {\n                        \"tag\": \"Add\",\n                        \"release\": \"minor\"\n                    },\n                    {\n                        \"tag\": \"Improve\",\n                        \"release\": \"patch\"\n                    },\n                    {\n                        \"tag\": \"Make\",\n                        \"release\": \"patch\"\n                    },\n                    {\n                        \"tag\": \"Refactor\",\n                        \"release\": false\n                    }\n                ]\n            }\n        ],\n        \"@semantic-release/github\",\n        [\n            \"@semantic-release/exec\",\n            {\n                \"prepareCmd\": \"bash .github/workflows/update_version.sh '${nextRelease.version}'\"\n            }\n        ],\n        [\n            \"@semantic-release/git\",\n            {\n                \"assets\": [\n                    \"VERSION\",\n                    \"conanfile.py\",\n                    \"package.json\",\n                    \"Cargo.toml\",\n                    \"Cargo.lock\",\n                    \"CMakeLists.txt\",\n                    \"include/stringzilla/stringzilla.h\"\n                ],\n                \"message\": \"Build: Released ${nextRelease.version} [skip ci]\\n\\n${nextRelease.notes}\"\n            }\n        ]\n    ]\n}"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 14.4248046875,
          "content": "cmake_minimum_required(VERSION 3.1)\nproject(\n  stringzilla\n  VERSION 3.11.3\n  LANGUAGES C CXX\n  DESCRIPTION \"SIMD-accelerated string search, sort, hashes, fingerprints, & edit distances\"\n  HOMEPAGE_URL \"https://github.com/ashvardanian/stringzilla\")\n\nset(CMAKE_C_STANDARD 99)\nset(CMAKE_CXX_STANDARD 17) # This gives many issues for msvc and clang-cl, especially if later on you set it to std-c++11 later on in the tests...\n\nset(CMAKE_C_EXTENSIONS OFF)\nset(CMAKE_CXX_EXTENSIONS OFF)\nset(CMAKE_COMPILE_WARNING_AS_ERROR)\nset(DEV_USER_NAME $ENV{USER})\n\nmessage(STATUS \"C Compiler ID: ${CMAKE_C_COMPILER_ID}\")\nmessage(STATUS \"C Compiler Version: ${CMAKE_C_COMPILER_VERSION}\")\nmessage(STATUS \"C Compiler: ${CMAKE_C_COMPILER}\")\nmessage(STATUS \"C++ Compiler ID: ${CMAKE_CXX_COMPILER_ID}\")\nmessage(STATUS \"C++ Compiler Version: ${CMAKE_CXX_COMPILER_VERSION}\")\nmessage(STATUS \"C++ Compiler: ${CMAKE_CXX_COMPILER}\")\nmessage(STATUS \"Build type: ${CMAKE_BUILD_TYPE}\")\n\nif(CMAKE_SIZEOF_VOID_P EQUAL 8)\n  message(STATUS \"Pointer size: 64-bit\")\nelse()\n  message(STATUS \"Pointer size: 32-bit\")\nendif()\n\n# Set a default build type to \"Release\" if none was specified\nif(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)\n  message(STATUS \"Setting build type to 'Release' as none was specified.\")\n  set(CMAKE_BUILD_TYPE\n    Release\n    CACHE STRING \"Choose the type of build.\" FORCE)\n  set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\"\n    \"MinSizeRel\" \"RelWithDebInfo\")\nendif()\n\nif(CMAKE_SYSTEM_PROCESSOR MATCHES \"x86_64|AMD64|amd64\")\n  SET(SZ_PLATFORM_X86 TRUE)\n  message(STATUS \"Platform: x86\")\nelseif(CMAKE_SYSTEM_PROCESSOR MATCHES \"aarch64|AARCH64|arm64|ARM64\")\n  SET(SZ_PLATFORM_ARM TRUE)\n  message(STATUS \"Platform: ARM\")\nendif()\n\n# Determine if StringZilla is built as a subproject (using `add_subdirectory`)\n# or if it is the main project\nset(STRINGZILLA_IS_MAIN_PROJECT OFF)\n\nif(CMAKE_CURRENT_SOURCE_DIR STREQUAL CMAKE_SOURCE_DIR)\n  set(STRINGZILLA_IS_MAIN_PROJECT ON)\nendif()\n\n# Installation options\noption(STRINGZILLA_INSTALL \"Install CMake targets\" OFF)\noption(STRINGZILLA_BUILD_TEST \"Compile a native unit test in C++\"\n  ${STRINGZILLA_IS_MAIN_PROJECT})\noption(STRINGZILLA_BUILD_BENCHMARK \"Compile a native benchmark in C++\"\n  ${STRINGZILLA_IS_MAIN_PROJECT})\noption(STRINGZILLA_BUILD_SHARED \"Compile a dynamic library\" ${STRINGZILLA_IS_MAIN_PROJECT})\nset(STRINGZILLA_TARGET_ARCH\n  \"\"\n  CACHE STRING \"Architecture to tell the compiler to optimize for (-march)\")\n\n# Includes\nset(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake ${CMAKE_MODULE_PATH})\ninclude(ExternalProject)\ninclude(CheckCSourceCompiles)\n\n# Allow CMake 3.13+ to override options when using FetchContent /\n# add_subdirectory\nif(POLICY CMP0077)\n  cmake_policy(SET CMP0077 NEW)\nendif()\n\n# Configuration\ninclude(GNUInstallDirs)\nset(STRINGZILLA_TARGET_NAME ${PROJECT_NAME})\nset(STRINGZILLA_INCLUDE_BUILD_DIR \"${PROJECT_SOURCE_DIR}/include/\")\nset(STRINGZILLA_INCLUDE_INSTALL_DIR \"${CMAKE_INSTALL_INCLUDEDIR}\")\n\n# Define our library\nadd_library(${STRINGZILLA_TARGET_NAME} INTERFACE)\nadd_library(${PROJECT_NAME}::${STRINGZILLA_TARGET_NAME} ALIAS ${STRINGZILLA_TARGET_NAME})\n\ntarget_include_directories(\n  ${STRINGZILLA_TARGET_NAME}\n  INTERFACE $<BUILD_INTERFACE:${STRINGZILLA_INCLUDE_BUILD_DIR}>\n  $<INSTALL_INTERFACE:include>)\n\n\nif(${CMAKE_VERSION} VERSION_EQUAL 3.13 OR ${CMAKE_VERSION} VERSION_GREATER 3.13)\n  include(CTest)\n  enable_testing()\nendif()\n\nif (MSVC)\n  # Remove /RTC* from MSVC debug flags by default (it will be added back in the set_compiler_flags function)\n  # Beacuse /RTC* cannot be used without the crt so it needs to be disabled for that specifc target\n  string(REGEX REPLACE \"/RTC[^ ]*\" \"\" CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG}\")\n  string(REGEX REPLACE \"/RTC[^ ]*\" \"\" CMAKE_C_FLAGS_DEBUG \"${CMAKE_C_FLAGS_DEBUG}\")\nendif()\n\n# Function to set compiler-specific flags\nfunction(set_compiler_flags target cpp_standard target_arch)\n  get_target_property(target_type ${target} TYPE)\n\n  target_include_directories(${target} PRIVATE scripts)\n  target_link_libraries(${target} PRIVATE ${STRINGZILLA_TARGET_NAME})\n\n  # Set output directory for single-configuration generators (like Make)\n  set_target_properties(${target} PROPERTIES\n    RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/$<0:>\n  )\n\n  # Set output directory for multi-configuration generators (like Visual Studio)\n  foreach(config IN LISTS CMAKE_CONFIGURATION_TYPES)\n    string(TOUPPER ${config} config_upper)\n    set_target_properties(${target} PROPERTIES\n      RUNTIME_OUTPUT_DIRECTORY_${config_upper} ${CMAKE_BINARY_DIR}/$<0:>\n    )\n  endforeach()\n\n  # Set the C++ standard\n  if(NOT ${cpp_standard} STREQUAL \"\")\n    set_target_properties(${target} PROPERTIES CXX_STANDARD ${cpp_standard})\n  endif()\n\n  # Use the /Zc:__cplusplus flag to correctly define the __cplusplus macro in MSVC\n  target_compile_options(${target} PRIVATE \"$<$<CXX_COMPILER_ID:MSVC>:/Zc:__cplusplus>\")\n\n  # Maximum warnings level & warnings as error.\n  # MVC uses numeric values:\n  # > 4068 for \"unknown pragmas\".\n  # > 4146 for \"unary minus operator applied to unsigned type, result still unsigned\".\n  # We also specify /utf-8 to properly UTF-8 symbols in tests.\n  target_compile_options(\n    ${target}\n    PRIVATE\n    \"$<$<CXX_COMPILER_ID:MSVC>:/Bt;/wd4068;/wd4146;/utf-8;/WX>\"\n    \"$<$<CXX_COMPILER_ID:GNU>:-Wall;-Wextra;-pedantic;-Werror;-Wfatal-errors;-Wno-unknown-pragmas;-Wno-cast-function-type;-Wno-unused-function>\"\n    \"$<$<CXX_COMPILER_ID:Clang>:-Wall;-Wextra;-pedantic;-Werror;-Wfatal-errors;-Wno-unknown-pragmas>\"\n    \"$<$<CXX_COMPILER_ID:AppleClang>:-Wall;-Wextra;-pedantic;-Werror;-Wfatal-errors;-Wno-unknown-pragmas>\"\n  )\n\n  # Set optimization options for different compilers differently\n  target_compile_options(\n    ${target}\n    PRIVATE\n    \"$<$<AND:$<CXX_COMPILER_ID:GNU>,$<OR:$<CONFIG:Release>,$<CONFIG:RelWithDebInfo>>>:-O3>\"\n    \"$<$<AND:$<CXX_COMPILER_ID:GNU>,$<OR:$<CONFIG:Debug>,$<CONFIG:RelWithDebInfo>>>:-g>\"\n    \"$<$<AND:$<CXX_COMPILER_ID:Clang>,$<OR:$<CONFIG:Release>,$<CONFIG:RelWithDebInfo>>>:-O3>\"\n    \"$<$<AND:$<CXX_COMPILER_ID:Clang>,$<OR:$<CONFIG:Debug>,$<CONFIG:RelWithDebInfo>>>:-g>\"\n    \"$<$<AND:$<CXX_COMPILER_ID:MSVC>,$<CONFIG:Release>>:/O2>\"\n    \"$<$<AND:$<CXX_COMPILER_ID:MSVC>,$<OR:$<CONFIG:Release>,$<CONFIG:RelWithDebInfo>>>:/O2>\"\n    \"$<$<AND:$<CXX_COMPILER_ID:MSVC>,$<OR:$<CONFIG:Debug>,$<CONFIG:RelWithDebInfo>>>:/Zi>\"\n  )\n\n  if(NOT target_type STREQUAL \"SHARED_LIBRARY\")\n    if(MSVC)\n      target_compile_options(${target} PRIVATE \"$<$<CONFIG:Debug>:/RTC1>\")\n    endif()\n  endif()\n\n  # If available, enable Position Independent Code\n  get_target_property(target_pic ${target} POSITION_INDEPENDENT_CODE)\n  if(target_pic)\n    target_compile_options(${target} PRIVATE \"$<$<CXX_COMPILER_ID:GNU,Clang,AppleClang>:-fPIC>\")\n    target_link_options(${target} PRIVATE \"$<$<CXX_COMPILER_ID:GNU,Clang,AppleClang>:-fPIC>\")\n    target_compile_definitions(${target} PRIVATE \"$<$<CXX_COMPILER_ID:GNU,Clang,AppleClang>:SZ_PIC>\")\n  endif()\n\n  # Avoid builtin functions where we know what we are doing.\n  target_compile_options(${target} PRIVATE \"$<$<CXX_COMPILER_ID:GNU,Clang,AppleClang>:-fno-builtin-memcmp>\")\n  target_compile_options(${target} PRIVATE \"$<$<CXX_COMPILER_ID:GNU,Clang,AppleClang>:-fno-builtin-memchr>\")\n  target_compile_options(${target} PRIVATE \"$<$<CXX_COMPILER_ID:GNU,Clang,AppleClang>:-fno-builtin-memcpy>\")\n  target_compile_options(${target} PRIVATE \"$<$<CXX_COMPILER_ID:GNU,Clang,AppleClang>:-fno-builtin-memset>\")\n  target_compile_options(${target} PRIVATE \"$<$<CXX_COMPILER_ID:MSVC>:/Oi->\")\n\n  # Check for ${target_arch} and set it or use the current system if not defined\n  if(\"${target_arch}\" STREQUAL \"\")\n    # Only use the current system if we are not cross compiling\n    if((NOT CMAKE_CROSSCOMPILING) OR (CMAKE_SYSTEM_PROCESSOR MATCHES CMAKE_HOST_SYSTEM_PROCESSOR))\n      if (NOT MSVC)\n        include(CheckCXXCompilerFlag)\n        check_cxx_compiler_flag(\"-march=native\" supports_march_native)\n        if (supports_march_native)\n          target_compile_options(${target} PRIVATE \"-march=native\")\n        endif()\n      else()\n        # MSVC does not have a direct equivalent to -march=native\n        target_compile_options(${target} PRIVATE \"/arch:AVX2\")\n      endif()\n    endif()\n  else()\n    target_compile_options(\n      ${target}\n      PRIVATE\n      \"$<$<CXX_COMPILER_ID:GNU,Clang,AppleClang>:-march=${target_arch}>\"\n      \"$<$<CXX_COMPILER_ID:MSVC>:/arch:${target_arch}>\")\n  endif()\n\n  # Define SZ_DETECT_BIG_ENDIAN macro based on system byte order\n  if(CMAKE_C_BYTE_ORDER STREQUAL \"BIG_ENDIAN\")\n    set(SZ_DETECT_BIG_ENDIAN 1)\n  else()\n    set(SZ_DETECT_BIG_ENDIAN 0)\n  endif()\n\n  target_compile_definitions(\n    ${target}\n    PRIVATE\n    \"SZ_DETECT_BIG_ENDIAN=${SZ_DETECT_BIG_ENDIAN}\"\n  )\n\n  # Sanitizer options for Debug mode\n  if(CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n    if(NOT target_type STREQUAL \"SHARED_LIBRARY\")\n      target_compile_options(\n        ${target}\n        PRIVATE\n        \"$<$<CXX_COMPILER_ID:GNU,Clang>:-fsanitize=address;-fsanitize=leak>\"\n        \"$<$<CXX_COMPILER_ID:MSVC>:/fsanitize=address>\")\n\n      target_link_options(\n        ${target}\n        PRIVATE\n        \"$<$<CXX_COMPILER_ID:GNU,Clang>:-fsanitize=address;-fsanitize=leak>\"\n        \"$<$<CXX_COMPILER_ID:MSVC>:/fsanitize=address>\")\n    endif()\n\n    # Define SZ_DEBUG macro based on build configuration\n    target_compile_definitions(\n      ${target}\n      PRIVATE\n      \"$<$<CONFIG:Debug>:SZ_DEBUG=1>\"\n      \"$<$<NOT:$<CONFIG:Debug>>:SZ_DEBUG=0>\"\n    )\n  endif()\nendfunction()\n\nfunction(define_launcher exec_name source cpp_standard target_arch)\n  add_executable(${exec_name} ${source})\n  set_compiler_flags(${exec_name} ${cpp_standard} \"${target_arch}\")\n  add_test(NAME ${exec_name} COMMAND ${exec_name})\nendfunction()\n\nif(${STRINGZILLA_BUILD_BENCHMARK})\n  define_launcher(stringzilla_bench_search scripts/bench_search.cpp 17 \"${STRINGZILLA_TARGET_ARCH}\")\n  define_launcher(stringzilla_bench_similarity scripts/bench_similarity.cpp 17 \"${STRINGZILLA_TARGET_ARCH}\")\n  define_launcher(stringzilla_bench_sort scripts/bench_sort.cpp 17 \"${STRINGZILLA_TARGET_ARCH}\")\n  define_launcher(stringzilla_bench_token scripts/bench_token.cpp 17 \"${STRINGZILLA_TARGET_ARCH}\")\n  define_launcher(stringzilla_bench_container scripts/bench_container.cpp 17 \"${STRINGZILLA_TARGET_ARCH}\")\n  define_launcher(stringzilla_bench_memory scripts/bench_memory.cpp 17 \"${STRINGZILLA_TARGET_ARCH}\")\nendif()\n\nif(${STRINGZILLA_BUILD_TEST})\n  # Make sure that the compilation passes for different C++ standards\n  # ! Keep in mind, MSVC only supports C++11 and newer.\n  define_launcher(stringzilla_test_cpp11 scripts/test.cpp 11 \"${STRINGZILLA_TARGET_ARCH}\")\n  define_launcher(stringzilla_test_cpp14 scripts/test.cpp 14 \"${STRINGZILLA_TARGET_ARCH}\")\n  define_launcher(stringzilla_test_cpp17 scripts/test.cpp 17 \"${STRINGZILLA_TARGET_ARCH}\")\n  define_launcher(stringzilla_test_cpp20 scripts/test.cpp 20 \"${STRINGZILLA_TARGET_ARCH}\")\n\n  # Check system architecture to avoid complex cross-compilation workflows, but\n  # compile multiple backends: disabling all SIMD, enabling only AVX2, only AVX-512, only Arm Neon.\n  if(SZ_PLATFORM_X86)\n    # x86 specific backends\n    if (MSVC)\n      define_launcher(stringzilla_test_cpp20_x86_serial scripts/test.cpp 20 \"AVX\")\n      define_launcher(stringzilla_test_cpp20_x86_avx2 scripts/test.cpp 20 \"AVX2\")\n      define_launcher(stringzilla_test_cpp20_x86_avx512 scripts/test.cpp 20 \"AVX512\")\n    else()\n      define_launcher(stringzilla_test_cpp20_x86_serial scripts/test.cpp 20 \"ivybridge\")\n      define_launcher(stringzilla_test_cpp20_x86_avx2 scripts/test.cpp 20 \"haswell\")\n      define_launcher(stringzilla_test_cpp20_x86_avx512 scripts/test.cpp 20 \"sapphirerapids\")\n    endif()\n  elseif(SZ_PLATFORM_ARM)\n    # ARM specific backends\n    define_launcher(stringzilla_test_cpp20_arm_serial scripts/test.cpp 20 \"armv8-a\")\n    define_launcher(stringzilla_test_cpp20_arm_neon scripts/test.cpp 20 \"armv8-a+simd\")\n  endif()\nendif()\n\nif(${STRINGZILLA_BUILD_SHARED})\n\n  function(define_shared target)\n    add_library(${target} SHARED c/lib.c)\n\n    set_target_properties(${target} PROPERTIES\n      VERSION ${PROJECT_VERSION}\n      SOVERSION 1\n      POSITION_INDEPENDENT_CODE ON)\n\n    if (SZ_PLATFORM_X86)\n      if (MSVC)\n        set_compiler_flags(${target} \"\" \"SSE2\")\n      else()\n        set_compiler_flags(${target} \"\" \"ivybridge\")\n      endif()\n\n      target_compile_definitions(${target} PRIVATE\n        \"SZ_USE_X86_AVX512=1\"\n        \"SZ_USE_X86_AVX2=1\"\n        \"SZ_USE_ARM_NEON=0\"\n        \"SZ_USE_ARM_SVE=0\")\n    elseif(SZ_PLATFORM_ARM)\n      set_compiler_flags(${target} \"\" \"armv8-a\")\n\n      target_compile_definitions(${target} PRIVATE\n        \"SZ_USE_X86_AVX512=0\"\n        \"SZ_USE_X86_AVX2=0\"\n        \"SZ_USE_ARM_NEON=1\"\n        \"SZ_USE_ARM_SVE=1\")\n    endif()\n\n    if (MSVC)\n      # Add dependencies for necessary runtime libraries in case of static linking\n      # This ensures that basic runtime functions are available:\n      # msvcrt.lib: Microsoft Visual C Runtime, required for basic C runtime functions on Windows.\n      # vcruntime.lib: Microsoft Visual C++ Runtime library for basic runtime functions.\n      # ucrt.lib: Universal C Runtime, necessary for linking basic C functions like I/O.\n      target_link_libraries(${target} PRIVATE msvcrt.lib vcruntime.lib ucrt.lib)\n    endif()\n\n  endfunction()\n\n  define_shared(stringzilla_shared)\n  target_compile_definitions(stringzilla_shared PRIVATE \"SZ_AVOID_LIBC=0\")\n  target_compile_definitions(stringzilla_shared PRIVATE \"SZ_OVERRIDE_LIBC=1\")\n\n  # Try compiling a version without linking the LibC\n  define_shared(stringzillite)\n  target_compile_definitions(stringzillite PRIVATE \"SZ_AVOID_LIBC=1\")\n  target_compile_definitions(stringzillite PRIVATE \"SZ_OVERRIDE_LIBC=1\")\n\n  # Avoid built-ins on MSVC and other compilers, as that will cause compileration errors\n  target_compile_options(stringzillite PRIVATE\n    \"$<$<CXX_COMPILER_ID:GNU,Clang>:-fno-builtin;-nostdlib>\"\n    \"$<$<CXX_COMPILER_ID:MSVC>:/Oi-;/GS->\")\n  target_link_options(stringzillite PRIVATE \"$<$<CXX_COMPILER_ID:GNU,Clang>:-nostdlib>\")\n  target_link_options(stringzillite PRIVATE \"$<$<CXX_COMPILER_ID:MSVC>:/NODEFAULTLIB>\")\n\n\nendif()\n\nif(STRINGZILLA_INSTALL)\n  install(\n    TARGETS stringzilla_shared\n    ARCHIVE\n    BUNDLE\n    FRAMEWORK\n    LIBRARY\n    OBJECTS\n    PRIVATE_HEADER\n    PUBLIC_HEADER\n    RESOURCE\n    RUNTIME)\n  install(\n    TARGETS stringzillite\n    ARCHIVE\n    BUNDLE\n    FRAMEWORK\n    LIBRARY\n    OBJECTS\n    PRIVATE_HEADER\n    PUBLIC_HEADER\n    RESOURCE\n    RUNTIME)\n  install(DIRECTORY ${STRINGZILLA_INCLUDE_BUILD_DIR} DESTINATION ${STRINGZILLA_INCLUDE_INSTALL_DIR})\n  install(DIRECTORY ./c/ DESTINATION /usr/src/${PROJECT_NAME}/)\nendif()\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 22.275390625,
          "content": "# Contributing to StringZilla\n\nThank you for coming here! It's always nice to have third-party contributors 🤗\nDepending on the type of contribution, you may need to follow different steps.\n\n---\n\n## Project Structure\n\nThe project is split into the following parts:\n\n- `include/stringzilla/stringzilla.h` - single-header C implementation.\n- `include/stringzilla/stringzilla.hpp` - single-header C++ wrapper.\n- `python/*` - Python bindings.\n- `javascript/*` - JavaScript bindings.\n- `scripts/*` - Scripts for benchmarking and testing.\n\nFor minimal test coverage, check the following scripts:\n\n- `test.cpp` - tests C++ API (not underlying C) against STL.\n- `test.py` - tests Python API against native strings.\n- `test.js`.\n\nAt the C++ level all benchmarks also validate the results against the STL baseline, serving as tests on real-world data.\nThey have the broadest coverage of the library, and are the most important to keep up-to-date:\n\n- `bench_token.cpp` - token-level ops, like hashing, ordering, equality checks.\n- `bench_search.cpp` - bidirectional substring search, both exact and fuzzy.\n- `bench_similarity.cpp` - benchmark all edit distance backends.\n- `bench_sort.cpp` - sorting, partitioning, merging.\n- `bench_container.cpp` - STL containers with different string keys.\n\nThe role of Python benchmarks is less to provide absolute number, but to compare against popular tools in the Python ecosystem.\n\n- `bench_search.(py|ipynb)` - compares against native Python `str`.\n- `bench_sort.(py|ipynb)` - compares against `pandas`.\n- `bench_similarity.(ipynb)` - compares against `jellyfish`, `editdistance`, etc.\n\n## Benchmarking Datasets\n\nIt's not always easy to find good datasets for benchmarking strings workloads.\nI use several ASCII and UTF8 international datasets.\nYou can download them using the following commands:\n\n```sh\n# English Leipzig Corpora Collection\n# 124 MB, 1'000'000 lines of ASCII, 8'388'608 tokens of mean length 5\nwget --no-clobber -O leipzig1M.txt https://introcs.cs.princeton.edu/python/42sort/leipzig1m.txt\n\n# Hutter Prize \"enwik9\" dataset for compression\n# 1 GB (0.3 GB compressed), 13'147'025 lines of ASCII, 67'108'864 tokens of mean length 6\nwget --no-clobber -O enwik9.zip http://mattmahoney.net/dc/enwik9.zip\nunzip enwik9.zip && rm enwik9.zip && mv enwik9 enwik9.txt\n\n# XL Sum dataset for multilingual extractive summarization\n# 4.7 GB (1.7 GB compressed), 1'004'598 lines of UTF8, 268'435'456 tokens of mean length 8\nwget --no-clobber -O xlsum.csv.gz https://github.com/ashvardanian/xl-sum/releases/download/v1.0.0/xlsum.csv.gz\ngzip -d  xlsum.csv.gz\n\n# Human chromosome generator dataset generated by:\n# 1200 rows, each 800 characters long (939K)\n# https://github.com/rghilduta/human-chromosome-data-generator/blob/main/generate_chromosome_data.sh\nwget --no-clobber -O human_protein_1200row_800len.txt https://media.githubusercontent.com/media/rghilduta/human-chromosome-data-generator/main/examples/human_protein_1200row_800len.txt\n```\n\n## IDE Integrations\n\nThe project was originally developed in VS Code, and contains a set of configuration files for that IDE under `.vscode/`.\n\n- `tasks.json` - build tasks for CMake.\n- `launch.json` - debugger launchers for CMake.\n- `extensions.json` - recommended extensions for VS Code, including:\n    - `ms-vscode.cpptools-themes` - C++ language support.\n    - `ms-vscode.cmake-tools`, `cheshirekow.cmake-format` - CMake integration.\n    - `ms-python.python`, `ms-python.black-formatter` - Python language support.\n    - `yzhang.markdown-all-in-one` - formatting Markdown.\n    - `aaron-bond.better-comments` - color-coded comments.\n\n## Code Styling\n\nThe project uses `.clang-format` to enforce a consistent code style.\nModern IDEs, like VS Code, can be configured to automatically format the code on save.\n\n- East const over const West. Write `char const*` instead of `const char*`.\n- For color-coded comments start the line with `!` for warnings or `?` for questions.\n- Sort the includes: standard libraries, third-party libraries, and only then internal project headers.\n\nFor C++ code:\n\n- Explicitly use `std::` or `sz::` namespaces over global `memcpy`, `uint64_t`, etc.\n- Explicitly mark `noexcept` or `noexcept(false)` for all library interfaces.\n- Document all possible exceptions of an interface using `@throw` in Doxygen.\n- Avoid C-style variadic arguments in favor of templates.\n- Avoid C-style casts in favor of `static_cast`, `reinterpret_cast`, and `const_cast`, except for places where a C function is called.\n- Use lower-case names for everything, except settings/conditions macros. Function-like macros, that take arguments, should be lowercase as well.\n- In templates prefer `typename` over `class`.\n- Prepend \"private\" symbols with `_` underscore.\n\nFor Python code:\n\n- Use lower-case names for functions and variables.\n\n## Contributing in C++ and C\n\nThe primary C implementation and the C++ wrapper are built with CMake.\nAssuming the extensive use of new SIMD intrinsics and recent C++ language features, using a recent compiler is recommended.\nWe prefer GCC 12, which is available from default Ubuntu repositories with Ubuntu 22.04 LTS onwards.\nIf this is your first experience with CMake, use the following commands to get started on Ubuntu:\n\n```bash\nsudo apt-get update && sudo apt-get install cmake build-essential libjemalloc-dev g++-12 gcc-12\n```\n\nOn MacOS it's recommended to use Homebrew and install Clang, as opposed to \"Apple Clang\".\nReplacing the default compiler is not recommended, as it may break the system, but you can pass it as an environment variable:\n\n```bash\nbrew install llvm\ncmake -D CMAKE_BUILD_TYPE=Release -D STRINGZILLA_BUILD_TEST=1 \\\n    -D CMAKE_C_COMPILER=\"$(brew --prefix llvm)/bin/clang\" \\\n    -D CMAKE_CXX_COMPILER=\"$(brew --prefix llvm)/bin/clang++\" \\\n    -B build_release\ncmake --build build_release --config Release\n```\n\n### Testing\n\nUsing modern syntax, this is how you build and run the test suite:\n\n```bash\ncmake -D STRINGZILLA_BUILD_TEST=1 -D CMAKE_BUILD_TYPE=Debug -B build_debug\ncmake --build build_debug --config Debug          # Which will produce the following targets:\nbuild_debug/stringzilla_test_cpp20                # Unit test for the entire library compiled for current hardware\nbuild_debug/stringzilla_test_cpp20_x86_serial     # x86 variant compiled for IvyBridge - last arch. before AVX2\nbuild_debug/stringzilla_test_cpp20_arm_serial     # Arm variant compiled without Neon\n```\n\nTo use CppCheck for static analysis make sure to export the compilation commands.\nOverall, CppCheck and Clang-Tidy are extremely noisy and not suitable for CI, but may be useful for local development.\n\n```bash\nsudo apt install cppcheck clang-tidy-11\n\ncmake -B build_artifacts \\\n  -D CMAKE_BUILD_TYPE=RelWithDebInfo \\\n  -D CMAKE_EXPORT_COMPILE_COMMANDS=1 \\\n  -D STRINGZILLA_BUILD_BENCHMARK=1 \\\n  -D STRINGZILLA_BUILD_TEST=1\n\ncppcheck --project=build_artifacts/compile_commands.json --enable=all\n\nclang-tidy-11 -p build_artifacts\n```\n\nI'd recommend putting the following breakpoints:\n\n- `__asan::ReportGenericError` - to detect illegal memory accesses.\n- `__GI_exit` - to stop at exit points - the end of running any executable.\n- `__builtin_unreachable` - to catch unexpected code paths.\n- `_sz_assert_failure` - to catch StringZilla logic assertions.\n\n### Benchmarking\n\nFor benchmarks, you can use the following commands:\n\n```bash\ncmake -D STRINGZILLA_BUILD_BENCHMARK=1 -B build_release\ncmake --build build_release --config Release      # Produces the following targets:\nbuild_release/stringzilla_bench_memory <path>     # - for string copies and fills\nbuild_release/stringzilla_bench_search <path>     # - for substring search\nbuild_release/stringzilla_bench_token <path>      # - for hashing, equality comparisons, etc.\nbuild_release/stringzilla_bench_similarity <path> # - for edit distances and alignment scores\nbuild_release/stringzilla_bench_sort <path>       # - for sorting arrays of strings\nbuild_release/stringzilla_bench_container <path>  # - for STL containers with string keys\n```\n\n### Benchmarking Hardware-Specific Optimizations\n\nRunning on modern hardware, you may want to compile the code for older generations to compare the relative performance.\nThe assumption would be that newer ISA extensions would provide better performance.\nOn x86_64, you can use the following commands to compile for Sandy Bridge, Haswell, and Sapphire Rapids:\n\n```bash\ncmake -D CMAKE_BUILD_TYPE=Release -D STRINGZILLA_BUILD_BENCHMARK=1 \\\n    -D STRINGZILLA_TARGET_ARCH=\"ivybridge\" -B build_release/ivybridge && \\\n    cmake --build build_release/ivybridge --config Release\ncmake -D CMAKE_BUILD_TYPE=Release -D STRINGZILLA_BUILD_BENCHMARK=1 \\\n    -D STRINGZILLA_TARGET_ARCH=\"haswell\" -B build_release/haswell && \\\n    cmake --build build_release/haswell --config Release\ncmake -D CMAKE_BUILD_TYPE=Release -D STRINGZILLA_BUILD_BENCHMARK=1 \\\n    -D STRINGZILLA_TARGET_ARCH=\"sapphirerapids\" -B build_release/sapphirerapids && \\\n    cmake --build build_release/sapphirerapids --config Release\n```\n\n### Benchmarking Compiler-Specific Optimizations\n\nAlternatively, you may want to compare the performance of the code compiled with different compilers.\nOn x86_64, you may want to compare GCC, Clang, and ICX.\n\n```bash\ncmake -D CMAKE_BUILD_TYPE=Release -D STRINGZILLA_BUILD_BENCHMARK=1 -D STRINGZILLA_BUILD_SHARED=1 \\\n    -D CMAKE_CXX_COMPILER=g++-12 -D CMAKE_C_COMPILER=gcc-12 \\\n    -B build_release/gcc && cmake --build build_release/gcc --config Release\ncmake -D CMAKE_BUILD_TYPE=Release -D STRINGZILLA_BUILD_BENCHMARK=1 -D STRINGZILLA_BUILD_SHARED=1 \\\n    -D CMAKE_CXX_COMPILER=clang++-14 -D CMAKE_C_COMPILER=clang-14 \\\n    -B build_release/clang && cmake --build build_release/clang --config Release\n```\n\n### Profiling\n\nTo simplify tracing and profiling, build with symbols using the `RelWithDebInfo` configuration.\nHere is an example for profiling one target - `stringzilla_bench_token`.\n\n```bash\ncmake -D STRINGZILLA_BUILD_BENCHMARK=1 \\\n    -D STRINGZILLA_BUILD_TEST=1 \\\n    -D STRINGZILLA_BUILD_SHARED=1 \\\n    -D CMAKE_BUILD_TYPE=RelWithDebInfo \\\n    -B build_profile\ncmake --build build_profile --config Release --target stringzilla_bench_token\n\n# Check that the debugging symbols are there with your favorite tool\nreadelf --sections build_profile/stringzilla_bench_token | grep debug\nobjdump -h build_profile/stringzilla_bench_token | grep debug\n\n# Profile\nsudo perf record -g build_profile/stringzilla_bench_token ./leipzig1M.txt\nsudo perf report\n```\n\n### Testing in Docker\n\nIt might be a good idea to check the compatibility against the most popular Linux distributions.\nDocker is the goto-choice for that.\n\n#### Alpine\n\nAlpine is one of the most popular Linux distributions for containers, due to it's size.\nThe base image is only ~3 MB, and it's based on musl libc, which is different from glibc.\n\n```bash\nsudo docker run -it --rm -v \"$(pwd)\":/workspace/StringZilla alpine:latest /bin/ash\ncd /workspace/StringZilla\napk add --update make cmake g++ gcc\ncmake -D STRINGZILLA_BUILD_TEST=1 -D CMAKE_BUILD_TYPE=Debug -B build_debug\ncmake --build build_debug --config Debug\nbuild_debug/stringzilla_test_cpp20\n```\n\n#### Intel Clear Linux\n\nClear Linux is a distribution optimized for Intel hardware, and is known for its performance.\nIt has rolling releases, and is based on `glibc`.\nIt might be a good choice for compiling with Intel oneAPI compilers.\n\n```bash\nsudo docker run -it --rm -v \"$(pwd)\":/workspace/StringZilla clearlinux:latest /bin/bash\ncd /workspace/StringZilla\nswupd update\nswupd bundle-add c-basic dev-utils\ncmake -D STRINGZILLA_BUILD_TEST=1 -D CMAKE_BUILD_TYPE=Debug -B build_debug\ncmake --build build_debug --config Debug\nbuild_debug/stringzilla_test_cpp20\n```\n\nFor benchmarks:\n\n```bash\ncmake -D STRINGZILLA_BUILD_TEST=1 -D STRINGZILLA_BUILD_BENCHMARK=1 -B build_release\ncmake --build build_release --config Release\n```\n\n#### Amazon Linux\n\nFor CentOS-based __Amazon Linux 2023__:\n\n```bash\nsudo docker run -it --rm -v \"$(pwd)\":/workspace/StringZilla amazonlinux:2023 bash\ncd /workspace/StringZilla\nyum install -y make cmake3 gcc g++\ncmake3 -D STRINGZILLA_BUILD_TEST=1 -D CMAKE_BUILD_TYPE=Debug \\\n    -D CMAKE_CXX_COMPILER=g++ -D CMAKE_C_COMPILER=gcc -D STRINGZILLA_TARGET_ARCH=\"ivybridge\" \\\n    -B build_debug\ncmake3 --build build_debug --config Debug --target stringzilla_test_cpp11\nbuild_debug/stringzilla_test_cpp11\n```\n\nThe CentOS-based __Amazon Linux 2__ is still used in older AWS Lambda functions.\nSadly, the newest GCC version it supports is 10, and it can't handle AVX-512 instructions.\n\n```bash\nsudo docker run -it --rm -v \"$(pwd)\":/workspace/StringZilla amazonlinux:2 bash\ncd /workspace/StringZilla\nyum install -y make cmake3 gcc10 gcc10-c++\ncmake3 -D STRINGZILLA_BUILD_TEST=1 -D CMAKE_BUILD_TYPE=Debug \\\n    -D CMAKE_CXX_COMPILER=g++ -D CMAKE_C_COMPILER=gcc -D STRINGZILLA_TARGET_ARCH=\"ivybridge\" \\\n    -B build_debug\ncmake3 --build build_debug --config Debug --target stringzilla_test_cpp11\nbuild_debug/stringzilla_test_cpp11\n```\n\n> [!CAUTION]\n> \n> Even with GCC 10 the tests compilation will fail, as the STL implementation of the `insert` function doesn't conform to standard.\n> The `s.insert(s.begin() + 1, {'a', 'b', 'c'}) == (s.begin() + 1)` expression is illformed, as the `std::string::insert` return `void`.\n\n---\n\nDon't forget to clean up Docker afterwards.\n\n```bash\ndocker system prune -a --volumes\n```\n\n### Cross Compilation\n\nUnlike GCC, LLVM handles cross compilation very easily.\nYou just need to pass the right `TARGET_ARCH` and `BUILD_ARCH` to CMake.\nThe [list includes](https://packages.ubuntu.com/search?keywords=crossbuild-essential&searchon=names):\n\n- `crossbuild-essential-amd64` for 64-bit x86\n- `crossbuild-essential-arm64` for 64-bit Arm\n- `crossbuild-essential-armhf` for 32-bit ARM hard-float\n- `crossbuild-essential-armel` for 32-bit ARM soft-float (emulates `float`)\n- `crossbuild-essential-riscv64` for RISC-V\n- `crossbuild-essential-powerpc` for PowerPC\n- `crossbuild-essential-s390x` for IBM Z\n- `crossbuild-essential-mips` for MIPS\n- `crossbuild-essential-ppc64el` for PowerPC 64-bit little-endian\n\nHere is an example for cross-compiling for Arm64 on an x86_64 machine:\n\n```sh\nsudo apt-get update\nsudo apt-get install -y clang lld make crossbuild-essential-arm64 crossbuild-essential-armhf\nexport CC=\"clang\"\nexport CXX=\"clang++\"\nexport AR=\"llvm-ar\"\nexport NM=\"llvm-nm\"\nexport RANLIB=\"llvm-ranlib\"\nexport TARGET_ARCH=\"aarch64-linux-gnu\" # Or \"x86_64-linux-gnu\"\nexport BUILD_ARCH=\"arm64\" # Or \"amd64\"\n\ncmake -D CMAKE_BUILD_TYPE=Release \\\n    -D CMAKE_C_COMPILER_TARGET=${TARGET_ARCH} \\\n    -D CMAKE_CXX_COMPILER_TARGET=${TARGET_ARCH} \\\n    -D CMAKE_SYSTEM_NAME=Linux \\\n    -D CMAKE_SYSTEM_PROCESSOR=${BUILD_ARCH} \\\n    -B build_artifacts\ncmake --build build_artifacts --config Release\n```\n\n## Contributing in Python\n\nPython bindings are implemented using pure CPython, so you wouldn't need to install SWIG, PyBind11, or any other third-party library.\n\n```bash\npip install -e . # To build locally from source\n```\n\n### Testing\n\nFor testing we use PyTest, which may not be installed on your system.\n\n```bash\npip install pytest              # To install PyTest\npytest scripts/test.py -s -x    # Runs tests printing logs and stops on the first failure\n```\n\nOn a related note, StringZilla for Python seems to cover more OS and hardware combinations, than NumPy.\nThat's why NumPy isn't a required dependency.\nStill, many tests may use NumPy, so consider installing it on mainstream platforms.\n\n```bash\npip install numpy\n```\n\nBefore you ship, please make sure the `cibuilwheel` packaging works and tests pass on other platforms.\nDon't forget to use the right [CLI arguments][cibuildwheel-cli] to avoid overloading your Docker runtime.\n\n```bash\ncibuildwheel\ncibuildwheel --platform linux                   # works on any OS and builds all Linux backends\ncibuildwheel --platform linux --archs x86_64    # 64-bit x86, the most common on desktop and servers\ncibuildwheel --platform linux --archs aarch64   # 64-bit Arm for mobile devices, Apple M-series, and AWS Graviton\ncibuildwheel --platform linux --archs i686      # 32-bit Linux\ncibuildwheel --platform linux --archs s390x     # emulating big-endian IBM Z\ncibuildwheel --platform macos                   # works only on MacOS\ncibuildwheel --platform windows                 # works only on Windows\n```\n\nYou may need root privileges for multi-architecture builds:\n\n```bash\nsudo $(which cibuildwheel) --platform linux\n```\n\nOn Windows and MacOS, to avoid frequent path resolution issues, you may want to use:\n\n```bash\npython -m cibuildwheel --platform windows\n```\n\n[cibuildwheel-cli]: https://cibuildwheel.readthedocs.io/en/stable/options/#command-line\n\n### Benchmarking\n\nFor high-performance low-latency benchmarking, stick to C/C++ native benchmarks, as the CPython is likely to cause bottlenecks.\nFor benchmarking, the following scripts are provided.\n\n```sh\npython scripts/bench_search.py --haystack_path \"your file\" --needle \"your pattern\" # real data\npython scripts/bench_search.py --haystack_pattern \"abcd\" --haystack_length 1e9 --needle \"abce\" # synthetic data\npython scripts/similarity_bench.py --text_path \"your file\" # edit distance computations\n```\n\nAlternatively, you can explore the Jupyter notebooks in `scripts/` directory.\n\n## Contributing in JavaScript\n\n```bash\nnpm ci && npm test\n```\n\n## Contributing in Swift\n\n```bash\nswift build && swift test\n```\n\nRunning Swift on Linux requires a couple of extra steps, as the Swift compiler is not available in the default repositories.\nPlease get the most recent Swift tarball from the [official website](https://www.swift.org/install/).\nAt the time of writing, for 64-bit Arm CPU running Ubuntu 22.04, the following commands would work:\n\n```bash\nwget https://download.swift.org/swift-5.9.2-release/ubuntu2204-aarch64/swift-5.9.2-RELEASE/swift-5.9.2-RELEASE-ubuntu22.04-aarch64.tar.gz\ntar xzf swift-5.9.2-RELEASE-ubuntu22.04-aarch64.tar.gz\nsudo mv swift-5.9.2-RELEASE-ubuntu22.04-aarch64 /usr/share/swift\necho \"export PATH=/usr/share/swift/usr/bin:$PATH\" >> ~/.bashrc\nsource ~/.bashrc\n```\n\nYou can check the available images on [`swift.org/download` page](https://www.swift.org/download/#releases).\nFor x86 CPUs, the following commands would work:\n\n```bash\nwget https://download.swift.org/swift-5.9.2-release/ubuntu2204/swift-5.9.2-RELEASE/swift-5.9.2-RELEASE-ubuntu22.04.tar.gz\ntar xzf swift-5.9.2-RELEASE-ubuntu22.04.tar.gz\nsudo mv swift-5.9.2-RELEASE-ubuntu22.04 /usr/share/swift\necho \"export PATH=/usr/share/swift/usr/bin:$PATH\" >> ~/.bashrc\nsource ~/.bashrc\n```\n\nAlternatively, on Linux, the official Swift Docker image can be used for builds and tests:\n\n```bash\nsudo docker run --rm -v \"$PWD:/workspace\" -w /workspace swift:5.9 /bin/bash -cl \"swift build -c release --static-swift-stdlib && swift test -c release --enable-test-discovery\"\n```\n\n## Contributing in Rust\n\n```bash\ncargo test\n```\n\nIf you are updating the package contents, you can validate the list of included files using the following command:\n\n```bash\ncargo package --list --allow-dirty\n```\n\nIf you want to run benchmarks against third-party implementations, check out the [`ashvardanian/memchr_vs_stringzilla`](https://github.com/ashvardanian/memchr_vs_stringzilla/) repository.\n\n## General Recommendations\n\n### Operations Not Worth Optimizing\n\nOne of the hardest things to learn in HPC is when to stop optimizing, and where not to start.\n\nIt doesn't make sense to optimize `sz_order`, because almost always, the relative order of two strings depends on the first bytes.\nFetching more bytes is not worth it.\nIn `sz_equal`, however, in rare cases, SIMD can help, if the user is comparing two mostly similar strings with identical hashes or checksums.\n\n### Unaligned Loads\n\nOne common surface of attack for performance optimizations is minimizing unaligned loads.\nSuch solutions are beautiful from the algorithmic perspective, but often lead to worse performance.\nIt's often cheaper to issue two interleaving wide-register loads, than try minimizing those loads at the cost of juggling registers.\nUnaligned stores are a different story, especially on x86, where multiple reads can be issued in parallel, but only one write can be issued at a time.\n\n### Register Pressure\n\nByte-level comparisons are simpler and often faster, than n-gram comparisons with subsequent interleaving.\nIn the following example we search for 4-byte needles in a haystack, loading at different offsets, and comparing then as arrays of 32-bit integers.\n\n```c\nh0_vec.zmm = _mm512_loadu_epi8(h);\nh1_vec.zmm = _mm512_loadu_epi8(h + 1);\nh2_vec.zmm = _mm512_loadu_epi8(h + 2);\nh3_vec.zmm = _mm512_loadu_epi8(h + 3);\nmatches0 = _mm512_cmpeq_epi32_mask(h0_vec.zmm, n_vec.zmm);\nmatches1 = _mm512_cmpeq_epi32_mask(h1_vec.zmm, n_vec.zmm);\nmatches2 = _mm512_cmpeq_epi32_mask(h2_vec.zmm, n_vec.zmm);\nmatches3 = _mm512_cmpeq_epi32_mask(h3_vec.zmm, n_vec.zmm);\nif (matches0 | matches1 | matches2 | matches3)\n    return h + sz_u64_ctz(_pdep_u64(matches0, 0x1111111111111111) | //\n                          _pdep_u64(matches1, 0x2222222222222222) | //\n                          _pdep_u64(matches2, 0x4444444444444444) | //\n                          _pdep_u64(matches3, 0x8888888888888888));\n```\n\nA simpler solution would be to compare byte-by-byte, but in that case we would need to populate multiple registers, broadcasting different letters of the needle into them.\nThat may not be noticeable on a micro-benchmark, but it would be noticeable on real-world workloads, where the CPU will speculatively interleave those search operations with something else happening in that context.\n\n### Working on Alternative Hardware Backends\n\nIt's important to keep compiler support in mind when extending to new instruction sets.\nCheck the most recent CI pipeline configurations in `prerelease.yml` and `release.yml` to see which compilers are used.\nWhen implementing dynamic dispatch, avoid compiler intrinsics and OS-specific APIs, as they may not be available on all platforms.\nInstead, use inline assembly to check feature flags and dispatch them to the proper implementation.\n\n### Working on Faster Edit Distances\n\nWhen dealing with non-trivial algorithms, like edit distances, it's advisory to provide pseudo-code or a reference implementation in addition to the optimized one.\nIdeally, include it in `scripts/` as a Python Jupyter Notebook with explanations and visualizations.\n\n### Working on Sequence Processing and Sorting\n\nSorting algorithms for strings are a deeply studied area.\nIn general, string sorting algorithms discourage the use of comparisons, as they are expensive for variable-length data and also require pointer-chasing for most array layouts.\nThey are also harder to accelerate with SIMD, as most layouts imply 16-byte entries, which are often too big to benefit from simple SIMD techniques."
        },
        {
          "name": "Cargo.lock",
          "type": "blob",
          "size": 0.3603515625,
          "content": "# This file is automatically @generated by Cargo.\n# It is not intended for manual editing.\nversion = 3\n\n[[package]]\nname = \"cc\"\nversion = \"1.0.95\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\nchecksum = \"d32a725bc159af97c3e629873bb9f88fb8cf8a4867175f76dc987815ea07c83b\"\n\n[[package]]\nname = \"stringzilla\"\nversion = \"3.11.3\"\ndependencies = [\n \"cc\",\n]\n"
        },
        {
          "name": "Cargo.toml",
          "type": "blob",
          "size": 0.7421875,
          "content": "[package]\nname = \"stringzilla\"\nversion = \"3.11.3\"\nauthors = [\"Ash Vardanian <1983160+ashvardanian@users.noreply.github.com>\"]\ndescription = \"Faster SIMD-accelerated string search, sorting, fingerprints, and edit distances\"\nedition = \"2021\"\nlicense = \"Apache-2.0\"\npublish = true\nrepository = \"https://github.com/ashvardanian/stringzilla\"\ndocumentation = \"https://docs.rs/stringzilla\"\nhomepage = \"https://ashvardanian.com/posts/stringzilla/\"\nkeywords = [\"simd\", \"search\", \"retrieval\", \"hash\", \"sort\"]\ncategories = [\n    \"text-processing\",\n    \"hardware-support\",\n    \"no-std\",\n    \"wasm\",\n    \"external-ffi-bindings\",\n]\ninclude = [\"/rust/**\", \"/c/**\", \"/include/**\", \"/build.rs\"]\n\n[lib]\nname = \"stringzilla\"\npath = \"rust/lib.rs\"\n\n[build-dependencies]\ncc = \"1.0\"\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.056640625,
          "content": "include include/stringzilla/stringzilla.h\ninclude VERSION\n"
        },
        {
          "name": "Package.swift",
          "type": "blob",
          "size": 1.67578125,
          "content": "// swift-tools-version:5.9\nimport PackageDescription\n\nlet package = Package(\n    name: \"StringZilla\",\n    platforms: [\n        // Linux doesn't have to be explicitly listed\n        .iOS(.v13),      // For iOS, version 13 and later\n        .tvOS(.v13),     // For tvOS, version 13 and later\n        .macOS(.v10_15), // For macOS, version 10.15 (Catalina) and later\n        .watchOS(.v6)    // For watchOS, version 6 and later\n    ],\n    products: [\n        .library(\n            name: \"StringZilla\",\n            targets: [\"StringZillaC\", \"StringZilla\"]\n        )\n    ],\n    targets: [\n        .target(\n            name: \"StringZillaC\",\n            path: \"include/stringzilla\", // Adjust the path to include your C source files\n            sources: [\"../../c/lib.c\"], // Include the source file here\n            publicHeadersPath: \".\",\n            cSettings: [\n                .define(\"SZ_DYNAMIC_DISPATCH\", to: \"1\"), // Define a macro\n                .define(\"SZ_AVOID_LIBC\", to: \"0\"), // We need `malloc` from LibC\n                .define(\"SZ_DEBUG\", to: \"0\"), // We don't need any extra assertions in the C layer\n                .headerSearchPath(\"include/stringzilla\"), // Specify header search paths\n                .unsafeFlags([\"-Wall\"]) // Use with caution: specify custom compiler flags\n            ]\n        ),\n        .target(\n            name: \"StringZilla\",\n            dependencies: [\"StringZillaC\"],\n            path: \"swift\",\n            exclude: [\"Test.swift\"]\n        ),\n        .testTarget(\n            name: \"StringZillaTests\",\n            dependencies: [\"StringZilla\"],\n            path: \"swift\",\n            sources: [\"Test.swift\"]\n        )\n    ],\n    cLanguageStandard: CLanguageStandard.c99\n)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 76.7060546875,
          "content": "# StringZilla 🦖\n\n![StringZilla banner](https://github.com/ashvardanian/ashvardanian/blob/master/repositories/StringZilla.png?raw=true)\n\nThe world wastes a minimum of $100M annually due to inefficient string operations.\nA typical codebase processes strings character by character, resulting in too many branches and data-dependencies, neglecting 90% of modern CPU's potential.\nLibC is different.\nIt attempts to leverage SIMD instructions to boost some operations, and is often used by higher-level languages, runtimes, and databases.\nBut it isn't perfect.\n1️⃣ First, even on common hardware, including over a billion 64-bit ARM CPUs, common functions like `strstr` and `memmem` only achieve 1/3 of the CPU's throughput.\n2️⃣ Second, SIMD coverage is inconsistent: acceleration in forward scans does not guarantee speed in the reverse-order search.\n3️⃣ At last, most high-level languages can't always use LibC, as the strings are often not NULL-terminated or may contain the Unicode \"Zero\" character in the middle of the string.\nThat's why StringZilla was created.\nTo provide predictably high performance, portable to any modern platform, operating system, and programming language.\n\n[![StringZilla Python installs](https://static.pepy.tech/personalized-badge/stringzilla?period=total&units=abbreviation&left_color=black&right_color=blue&left_text=StringZilla%20Python%20installs)](https://github.com/ashvardanian/stringzilla)\n[![StringZilla Rust installs](https://img.shields.io/crates/d/stringzilla?logo=rust&label=Rust%20installs)](https://crates.io/crates/stringzilla)\n[![GitHub Actions Workflow Status](https://img.shields.io/github/actions/workflow/status/ashvardanian/StringZilla/release.yml?branch=main&label=Ubuntu)](https://github.com/ashvardanian/StringZilla/actions/workflows/release.yml)\n[![GitHub Actions Workflow Status](https://img.shields.io/github/actions/workflow/status/ashvardanian/StringZilla/release.yml?branch=main&label=Windows)](https://github.com/ashvardanian/StringZilla/actions/workflows/release.yml)\n[![GitHub Actions Workflow Status](https://img.shields.io/github/actions/workflow/status/ashvardanian/StringZilla/release.yml?branch=main&label=MacOS)](https://github.com/ashvardanian/StringZilla/actions/workflows/release.yml)\n[![GitHub Actions Workflow Status](https://img.shields.io/github/actions/workflow/status/ashvardanian/StringZilla/release.yml?branch=main&label=Alpine%20Linux)](https://github.com/ashvardanian/StringZilla/actions/workflows/release.yml)\n![StringZilla code size](https://img.shields.io/github/languages/code-size/ashvardanian/stringzilla)\n\nStringZilla is the GodZilla of string libraries, using [SIMD][faq-simd] and [SWAR][faq-swar] to accelerate string operations on modern CPUs.\nIt is up to __10x faster than the default and even other SIMD-accelerated string libraries__ in C, C++, Python, and other languages, while covering broad functionality.\nIt __accelerates exact and fuzzy string matching, edit distance computations, sorting, lazily-evaluated ranges to avoid memory allocations, and even random-string generators__.\n\n[faq-simd]: https://en.wikipedia.org/wiki/Single_instruction,_multiple_data\n[faq-swar]: https://en.wikipedia.org/wiki/SWAR\n\n- 🐂 __[C](#Basic-Usage-with-C-99-and-Newer) :__ Upgrade LibC's `<string.h>` to `<stringzilla.h>`  in C 99\n- 🐉 __[C++](#basic-usage-with-c-11-and-newer):__ Upgrade STL's `<string>` to `<stringzilla.hpp>` in C++ 11\n- 🐍 __[Python](#quick-start-python-🐍):__ Upgrade your `str` to faster `Str`\n- 🍎 __[Swift](#quick-start-swift-🍏):__ Use the `String+StringZilla` extension\n- 🦀 __[Rust](#quick-start-rust-🦀):__ Use the `StringZilla` traits crate\n- 🐚 __[Shell][faq-shell]__: Accelerate common CLI tools with `sz_` prefix\n- 📚 Researcher? Jump to [Algorithms & Design Decisions](#algorithms--design-decisions-📚)\n- 💡 Thinking to contribute? Look for [\"good first issues\"][first-issues]\n- 🤝 And check the [guide](https://github.com/ashvardanian/StringZilla/blob/main/CONTRIBUTING.md) to setup the environment\n- Want more bindings or features? Let [me](https://github.com/ashvardanian) know!\n\n[faq-shell]: https://github.com/ashvardanian/StringZilla/blob/main/cli/README.md\n[first-issues]: https://github.com/ashvardanian/StringZilla/issues\n\n__Who is this for?__\n\n- For data-engineers parsing large datasets, like the [CommonCrawl](https://commoncrawl.org/), [RedPajama](https://github.com/togethercomputer/RedPajama-Data), or [LAION](https://laion.ai/blog/laion-5b/).\n- For software engineers optimizing strings in their apps and services.\n- For bioinformaticians and search engineers looking for edit-distances for [USearch](https://github.com/unum-cloud/usearch).\n- For [DBMS][faq-dbms] devs, optimizing `LIKE`, `ORDER BY`, and `GROUP BY` operations.\n- For hardware designers, needing a SWAR baseline for strings-processing functionality.\n- For students studying SIMD/SWAR applications to non-data-parallel operations.\n\n[faq-dbms]: https://en.wikipedia.org/wiki/Database\n\n## Performance\n\n<table style=\"width: 100%; text-align: center; table-layout: fixed;\">\n  <colgroup>\n    <col style=\"width: 25%;\">\n    <col style=\"width: 25%;\">\n    <col style=\"width: 25%;\">\n    <col style=\"width: 25%;\">\n  </colgroup>\n  <tr>\n    <th align=\"center\">C</th>\n    <th align=\"center\">C++</th>\n    <th align=\"center\">Python</th>\n    <th align=\"center\">StringZilla</th>\n  </tr>\n  <!-- Substrings, normal order -->\n  <tr>\n    <td colspan=\"4\" align=\"center\">find the first occurrence of a random word from text, ≅ 5 bytes long</td>\n  </tr>\n  <tr>\n    <td align=\"center\">\n      <code>strstr</code> <sup>1</sup><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>7.4</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>2.0</b> GB/s\n    </td>\n    <td align=\"center\">\n      <code>.find</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>2.9</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>1.6</b> GB/s\n    </td>\n    <td align=\"center\">\n      <code>.find</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>1.1</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>0.6</b> GB/s\n    </td>\n    <td align=\"center\">\n      <code>sz_find</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>10.6</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>7.1</b> GB/s\n    </td>\n  </tr>\n  <!-- Substrings, reverse order -->\n  <tr>\n    <td colspan=\"4\" align=\"center\">find the last occurrence of a random word from text, ≅ 5 bytes long</td>\n  </tr>\n  <tr>\n    <td align=\"center\">⚪</td>\n    <td align=\"center\">\n      <code>.rfind</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>0.5</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>0.4</b> GB/s\n    </td>\n    <td align=\"center\">\n      <code>.rfind</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>0.9</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>0.5</b> GB/s\n    </td>\n    <td align=\"center\">\n      <code>sz_rfind</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>10.8</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>6.7</b> GB/s\n    </td>\n  </tr>\n  <!-- Characters, normal order -->\n  <tr>\n    <td colspan=\"4\" align=\"center\">split lines separated by <code>\\n</code> or <code>\\r</code> <sup>2</sup></td>\n  </tr>\n  <tr>\n    <td align=\"center\">\n      <code>strcspn</code> <sup>1</sup><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>5.42</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>2.19</b> GB/s\n    </td>\n    <td align=\"center\">\n      <code>.find_first_of</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>0.59</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>0.46</b> GB/s\n    </td>\n    <td align=\"center\">\n      <code>re.finditer</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>0.06</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>0.02</b> GB/s\n    </td>\n    <td align=\"center\">\n      <code>sz_find_charset</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>4.08</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>3.22</b> GB/s\n    </td>\n  </tr>\n  <!-- Characters, reverse order -->\n  <tr>\n    <td colspan=\"4\" align=\"center\">find the last occurrence of any of 6 whitespaces <sup>2</sup></td>\n  </tr>\n  <tr>\n    <td align=\"center\">⚪</td>\n    <td align=\"center\">\n      <code>.find_last_of</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>0.25</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>0.25</b> GB/s\n    </td>\n    <td align=\"center\">⚪</td>\n    <td align=\"center\">\n      <code>sz_rfind_charset</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>0.43</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>0.23</b> GB/s\n    </td>\n  </tr>\n  <!-- Random Generation -->\n  <tr>\n    <td colspan=\"4\" align=\"center\">Random string from a given alphabet, 20 bytes long <sup>5</sup></td>\n  </tr>\n  <tr>\n    <td align=\"center\">\n      <code>rand() % n</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>18.0</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>9.4</b> MB/s\n    </td>\n    <td align=\"center\">\n      <code>std::uniform_int_distribution</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>47.2</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>20.4</b> MB/s\n    </td>\n    <td align=\"center\">\n      <code>join(random.choices(...))</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>13.3</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>5.9</b> MB/s\n    </td>\n    <td align=\"center\">\n      <code>sz_generate</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>56.2</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>25.8</b> MB/s\n    </td>\n  </tr>\n  <!-- Mapping Characters with Look-Up Table Transforms -->\n  <tr>\n    <td colspan=\"4\" align=\"center\">Mapping Characters with Look-Up Table Transforms</td>\n  </tr>\n  <tr>\n    <td align=\"center\">⚪</td>\n    <td align=\"center\">\n      <code>std::transform</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>3.81</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>2.65</b> GB/s\n    </td>\n    <td align=\"center\">\n      <code>str.translate</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>260.0</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>140.0</b> MB/s\n    </td>\n    <td align=\"center\">\n      <code>sz_look_up_transform</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>21.2</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>8.5</b> GB/s\n    </td>\n  </tr>\n  <!-- Sorting -->\n  <tr>\n    <td colspan=\"4\" align=\"center\">Get sorted order, ≅ 8 million English words <sup>6</sup></td>\n  </tr>\n  <tr>\n    <td align=\"center\">\n      <code>qsort_r</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>3.55</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>5.77</b> s\n    </td>\n    <td align=\"center\">\n      <code>std::sort</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>2.79</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>4.02</b> s\n    </td>\n    <td align=\"center\">\n      <code>numpy.argsort</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>7.58</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>13.00</b> s\n    </td>\n    <td align=\"center\">\n      <code>sz_sort</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>1.91</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>2.37</b> s\n    </td>\n  </tr>\n  <!-- Edit Distance -->\n  <tr>\n    <td colspan=\"4\" align=\"center\">Levenshtein edit distance, ≅ 5 bytes long</td>\n  </tr>\n  <tr>\n    <td align=\"center\">⚪</td>\n    <td align=\"center\">⚪</td>\n    <td align=\"center\">\n      via <code>jellyfish</code> <sup>3</sup><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>1,550</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>2,220</b> ns\n    </td>\n    <td align=\"center\">\n      <code>sz_edit_distance</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>99</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>180</b> ns\n    </td>\n  </tr>\n  <!-- Alignment Score -->\n  <tr>\n    <td colspan=\"4\" align=\"center\">Needleman-Wunsch alignment scores, ≅ 10 K aminoacids long</td>\n  </tr>\n  <tr>\n    <td align=\"center\">⚪</td>\n    <td align=\"center\">⚪</td>\n    <td align=\"center\">\n      via <code>biopython</code> <sup>4</sup><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>257</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>367</b> ms\n    </td>\n    <td align=\"center\">\n      <code>sz_alignment_score</code><br/>\n      <span style=\"color:#ABABAB;\">x86:</span> <b>73</b> &centerdot;\n      <span style=\"color:#ABABAB;\">arm:</span> <b>177</b> ms\n    </td>\n  </tr>\n</table>\n\nStringZilla has a lot of functionality, most of which is covered by benchmarks across C, C++, Python and other languages.\nYou can find those in the `./scripts` directory, with usage notes listed in the [`CONTRIBUTING.md`](CONTRIBUTING.md) file.\nNotably, if the CPU supports misaligned loads, even the 64-bit SWAR backends are faster than either standard library.\n\n> Most benchmarks were conducted on a 1 GB English text corpus, with an average word length of 6 characters.\n> The code was compiled with GCC 12, using `glibc` v2.35.\n> The benchmarks performed on Arm-based Graviton3 AWS `c7g` instances and `r7iz` Intel Sapphire Rapids.\n> Most modern Arm-based 64-bit CPUs will have similar relative speedups.\n> Variance withing x86 CPUs will be larger.\n> <sup>1</sup> Unlike other libraries, LibC requires strings to be NULL-terminated.\n> <sup>2</sup> Six whitespaces in the ASCII set are: ` \\t\\n\\v\\f\\r`. Python's and other standard libraries have specialized functions for those.\n> <sup>3</sup> Most Python libraries for strings are also implemented in C.\n> <sup>4</sup> Unlike the rest of BioPython, the alignment score computation is [implemented in C](https://github.com/biopython/biopython/blob/master/Bio/Align/_pairwisealigner.c).\n> <sup>5</sup> All modulo operations were conducted with `uint8_t` to allow compilers more optimization opportunities.\n> The C++ STL and StringZilla benchmarks used a 64-bit [Mersenne Twister][faq-mersenne-twister] as the generator.\n> For C, C++, and StringZilla, an in-place update of the string was used.\n> In Python every string had to be allocated as a new object, which makes it less fair.\n> <sup>6</sup> Contrary to the popular opinion, Python's default `sorted` function works faster than the C and C++ standard libraries.\n> That holds for large lists or tuples of strings, but fails as soon as you need more complex logic, like sorting dictionaries by a string key, or producing the \"sorted order\" permutation.\n> The latter is very common in database engines and is most similar to `numpy.argsort`.\n> Current StringZilla solution can be at least 4x faster without loss of generality.\n\n[faq-mersenne-twister]: https://en.wikipedia.org/wiki/Mersenne_Twister\n\n## Functionality\n\nStringZilla is compatible with most modern CPUs, and provides a broad range of functionality.\n\n- [x] works on both Little-Endian and Big-Endian architectures.\n- [x] works on 32-bit and 64-bit hardware architectures.\n- [x] compatible with ASCII and UTF-8 encoding.\n\nNot all features are available across all bindings.\nConsider contributing, if you need a feature that's not yet implemented.\n\n|                                | Maturity | C 99  | C++ 11 | Python | Swift | Rust  |\n| :----------------------------- | :------: | :---: | :----: | :----: | :---: | :---: |\n| Substring Search               |    🌳     |   ✅   |   ✅    |   ✅    |   ✅   |   ✅   |\n| Character Set Search           |    🌳     |   ✅   |   ✅    |   ✅    |   ✅   |   ✅   |\n| Edit Distances                 |    🧐     |   ✅   |   ✅    |   ✅    |   ✅   |   ⚪   |\n| Small String Class             |    🧐     |   ✅   |   ✅    |   ❌    |   ❌   |   ⚪   |\n| Sorting & Sequence Operations  |    🚧     |   ✅   |   ✅    |   ✅    |   ⚪   |   ⚪   |\n| Lazy Ranges, Compressed Arrays |    🧐     |   ⚪   |   ✅    |   ✅    |   ⚪   |   ⚪   |\n| Hashes & Fingerprints          |    🚧     |   ✅   |   ✅    |   ⚪    |   ⚪   |   ⚪   |\n\n> 🌳 parts are used in production.\n> 🧐 parts are in beta.\n> 🚧 parts are under active development, and are likely to break in subsequent releases.\n> ✅ are implemented.\n> ⚪ are considered.\n> ❌ are not intended.\n\n## Quick Start: Python 🐍\n\nPython bindings are available on PyPI, and can be installed with `pip`.\nYou can immediately check the installed version and the used hardware capabilities with following commands:\n\n```bash\npip install stringzilla\npython -c \"import stringzilla; print(stringzilla.__version__)\"\npython -c \"import stringzilla; print(stringzilla.__capabilities__)\"\n```\n\n### Basic Usage\n\nIf you've ever used the Python `str`, `bytes`, `bytearray`, `memoryview` class, you'll know what to expect.\nStringZilla's `Str` class is a hybrid of those two, providing `str`-like interface to byte-arrays.\n\n```python\nfrom stringzilla import Str, File\n\ntext_from_str = Str('some-string') # no copies, just a view\ntext_from_bytes = Str(b'some-array') # no copies, just a view\ntext_from_file = Str(File('some-file.txt')) # memory-mapped file\n\nimport numpy as np\nalphabet_array = np.arange(ord(\"a\"), ord(\"z\"), dtype=np.uint8)\ntext_from_array = Str(memoryview(alphabet_array))\n```\n\nThe `File` class memory-maps a file from persistent memory without loading its copy into RAM.\nThe contents of that file would remain immutable, and the mapping can be shared by multiple Python processes simultaneously.\nA standard dataset pre-processing use case would be to map a sizeable textual dataset like Common Crawl into memory, spawn child processes, and split the job between them.\n\n### Basic Operations\n\n- Length: `len(text) -> int`\n- Indexing: `text[42] -> str`\n- Slicing: `text[42:46] -> Str`\n- Substring check: `'substring' in text -> bool`\n- Hashing: `hash(text) -> int`\n- String conversion: `str(text) -> str`\n\n### Advanced Operations\n\n```py\nimport sys\n\nx: bool = text.contains('substring', start=0, end=sys.maxsize)\nx: int = text.find('substring', start=0, end=sys.maxsize)\nx: int = text.count('substring', start=0, end=sys.maxsize, allowoverlap=False)\nx: str = text.decode(encoding='utf-8', errors='strict')\nx: Strs = text.split(separator=' ', maxsplit=sys.maxsize, keepseparator=False)\nx: Strs = text.rsplit(separator=' ', maxsplit=sys.maxsize, keepseparator=False)\nx: Strs = text.splitlines(keeplinebreaks=False, maxsplit=sys.maxsize)\n```\n\nIt's important to note, that the last function behavior is slightly different from Python's `str.splitlines`.\nThe [native version][faq-splitlines] matches `\\n`, `\\r`, `\\v` or `\\x0b`, `\\f` or `\\x0c`, `\\x1c`, `\\x1d`, `\\x1e`, `\\x85`, `\\r\\n`, `\\u2028`, `\\u2029`, including 3x two-bytes-long runes.\nThe StringZilla version matches only `\\n`, `\\v`, `\\f`, `\\r`, `\\x1c`, `\\x1d`, `\\x1e`, `\\x85`, avoiding two-byte-long runes.\n\n[faq-splitlines]: https://docs.python.org/3/library/stdtypes.html#str.splitlines\n\n### Character Set Operations\n\nPython strings don't natively support character set operations.\nThis forces people to use regular expressions, which are slow and hard to read.\nTo avoid the need for `re.finditer`, StringZilla provides the following interfaces:\n\n```py\nx: int = text.find_first_of('chars', start=0, end=sys.maxsize)\nx: int = text.find_last_of('chars', start=0, end=sys.maxsize)\nx: int = text.find_first_not_of('chars', start=0, end=sys.maxsize)\nx: int = text.find_last_not_of('chars', start=0, end=sys.maxsize)\nx: Strs = text.split_charset(separator='chars', maxsplit=sys.maxsize, keepseparator=False)\nx: Strs = text.rsplit_charset(separator='chars', maxsplit=sys.maxsize, keepseparator=False)\n```\n\nYou can also transform the string using Look-Up Tables (LUTs), mapping it to a different character set.\nThis would result in a copy - `str` for `str` inputs and `bytes` for other types.\n\n```py\nx: str = text.translate('chars', {}, start=0, end=sys.maxsize, inplace=False)\nx: bytes = text.translate(b'chars', {}, start=0, end=sys.maxsize, inplace=False)\n```\n\nFor efficiency reasons, pass the LUT as a string or bytes object, not as a dictionary.\nThis can be useful in high-throughput applications dealing with binary data, including bioinformatics and image processing.\nHere is an example:\n\n```py\nimport stringzilla as sz\nlook_up_table = bytes(range(256)) # Identity LUT\nimage = open(\"/image/path.jpeg\", \"rb\").read()\nsz.translate(image, look_up_table, inplace=True)\n```\n\n### Collection-Level Operations\n\nOnce split into a `Strs` object, you can sort, shuffle, and reorganize the slices, with minimum memory footprint.\nIf all the chunks are located in consecutive memory regions, the memory overhead can be as low as 4 bytes per chunk.\n\n```python\nlines: Strs = text.split(separator='\\n') # 4 bytes per line overhead for under 4 GB of text\nbatch: Strs = lines.sample(seed=42) # 10x faster than `random.choices`\nlines.shuffle(seed=42) # or shuffle all lines in place and shard with slices\n# WIP: lines.sort() # explodes to 16 bytes per line overhead for any length text\n# WIP: sorted_order: tuple = lines.argsort() # similar to `numpy.argsort`\n```\n\nWorking on [RedPajama][redpajama], addressing 20 Billion annotated english documents, one will need only 160 GB of RAM instead of Terabytes.\nOnce loaded, the data will be memory-mapped, and can be reused between multiple Python processes without copies.\nAnd of course, you can use slices to navigate the dataset and shard it between multiple workers.\n\n```python\nlines[::3] # every third line\nlines[1::1] # every odd line\nlines[:-100:-1] # last 100 lines in reverse order\n```\n\n[redpajama]: https://github.com/togethercomputer/RedPajama-Data\n\n### Iterators and Memory Efficiency\n\nPython's operations like `split()` and `readlines()` immediately materialize a `list` of copied parts.\nThis can be very memory-inefficient for large datasets.\nStringZilla saves a lot of memory by viewing existing memory regions as substrings, but even more memory can be saved by using lazily evaluated iterators.\n\n```py\nx: SplitIterator[Str] = text.split_iter(separator=' ', keepseparator=False)\nx: SplitIterator[Str] = text.rsplit_iter(separator=' ', keepseparator=False)\nx: SplitIterator[Str] = text.split_charset_iter(separator='chars', keepseparator=False)\nx: SplitIterator[Str] = text.rsplit_charset_iter(separator='chars', keepseparator=False)\n```\n\nStringZilla can easily be 10x more memory efficient than native Python classes for tokenization.\nWith lazy operations, it practically becomes free.\n\n```py\nimport stringzilla as sz\n%load_ext memory_profiler\n\ntext = open(\"enwik9.txt\", \"r\").read() # 1 GB, mean word length 7.73 bytes\n%memit text.split() # increment: 8670.12 MiB (152 ms)\n%memit sz.split(text) # increment: 530.75 MiB (25 ms)\n%memit sum(1 for _ in sz.split_iter(text)) # increment: 0.00 MiB\n```\n\n### Low-Level Python API\n\nAside from calling the methods on the `Str` and `Strs` classes, you can also call the global functions directly on `str` and `bytes` instances.\nAssuming StringZilla CPython bindings are implemented [without any intermediate tools like SWIG or PyBind](https://ashvardanian.com/posts/pybind11-cpython-tutorial/), the call latency should be similar to native classes.\n\n```py\nimport stringzilla as sz\n\ncontains: bool = sz.contains(\"haystack\", \"needle\", start=0, end=sys.maxsize)\noffset: int = sz.find(\"haystack\", \"needle\", start=0, end=sys.maxsize)\ncount: int = sz.count(\"haystack\", \"needle\", start=0, end=sys.maxsize, allowoverlap=False)\n```\n\n### Edit Distances\n\n```py\nassert sz.edit_distance(\"apple\", \"aple\") == 1 # skip one ASCII character\nassert sz.edit_distance(\"αβγδ\", \"αγδ\") == 2 # skip two bytes forming one rune\nassert sz.edit_distance_unicode(\"αβγδ\", \"αγδ\") == 1 # one unicode rune\n```\n\nSeveral Python libraries provide edit distance computation.\nMost of them are implemented in C, but are not always as fast as StringZilla.\nTaking a 1'000 long proteins around 10'000 characters long, computing just a 100 distances:\n\n- [JellyFish](https://github.com/jamesturk/jellyfish): 62.3s\n- [EditDistance](https://github.com/roy-ht/editdistance): 32.9s\n- StringZilla: __0.8s__\n\nMoreover, you can pass custom substitution matrices to compute the Needleman-Wunsch alignment scores.\nThat task is very common in bioinformatics and computational biology.\nIt's natively supported in BioPython, and its BLOSUM matrices can be converted to StringZilla's format.\nAlternatively, you can construct an arbitrary 256 by 256 cost matrix using NumPy.\nDepending on arguments, the result may be equal to the negative Levenshtein distance.\n\n```py\nimport numpy as np\nimport stringzilla as sz\n\ncosts = np.zeros((256, 256), dtype=np.int8)\ncosts.fill(-1)\nnp.fill_diagonal(costs, 0)\n\nassert sz.alignment_score(\"first\", \"second\", substitution_matrix=costs, gap_score=-1) == -sz.edit_distance(a, b)\n```\n\nUsing the same proteins as for Levenshtein distance benchmarks:\n\n- [BioPython](https://github.com/biopython/biopython): 25.8s\n- StringZilla: __7.8s__\n\n<details>\n  <summary><b>§ Example converting from BioPython to StringZilla.</b></summary>\n\n```py\nimport numpy as np\nfrom Bio import Align\nfrom Bio.Align import substitution_matrices\n\naligner = Align.PairwiseAligner()\naligner.substitution_matrix = substitution_matrices.load(\"BLOSUM62\")\naligner.open_gap_score = 1\naligner.extend_gap_score = 1\n\n# Convert the matrix to NumPy\nsubs_packed = np.array(aligner.substitution_matrix).astype(np.int8)\nsubs_reconstructed = np.zeros((256, 256), dtype=np.int8)\n\n# Initialize all banned characters to a the largest possible penalty\nsubs_reconstructed.fill(127)\nfor packed_row, packed_row_aminoacid in enumerate(aligner.substitution_matrix.alphabet):\n    for packed_column, packed_column_aminoacid in enumerate(aligner.substitution_matrix.alphabet):\n        reconstructed_row = ord(packed_row_aminoacid)\n        reconstructed_column = ord(packed_column_aminoacid)\n        subs_reconstructed[reconstructed_row, reconstructed_column] = subs_packed[packed_row, packed_column]\n\n# Let's pick two examples for of tri-peptides (made of 3 aminoacids)\nglutathione = \"ECG\" # Need to rebuild human tissue?\nthyrotropin_releasing_hormone = \"QHP\" # Or to regulate your metabolism?\n\nassert sz.alignment_score(\n    glutathione,\n    thyrotropin_releasing_hormone, \n    substitution_matrix=subs_reconstructed, \n    gap_score=1) == aligner.score(glutathione, thyrotropin_releasing_hormone) # Equal to 6\n```\n\n</details>\n\n### Serialization\n\n#### Filesystem\n\nSimilar to how `File` can be used to read a large file, other interfaces can be used to dump strings to disk faster.\nThe `Str` class has `write_to` to write the string to a file, and `offset_within` to obtain integer offsets of substring view in larger string for navigation.\n\n```py\nweb_archive = Str(\"<html>...</html><html>...</html>\")\n_, end_tag, next_doc = web_archive.partition(\"</html>\") # or use `find`\nnext_doc_offset = next_doc.offset_within(web_archive)\nweb_archive.write_to(\"next_doc.html\") # no GIL, no copies, just a view\n```\n\n#### PyArrow\n\nA `Str` is easy to cast to [PyArrow](https://arrow.apache.org/docs/python/arrays.html#string-and-binary-types) buffers.\n\n```py\nfrom pyarrow import foreign_buffer\nfrom stringzilla import Str\n\noriginal = \"hello\"\nview = Str(native)\narrow = foreign_buffer(view.address, view.nbytes, view)\n```\n\nThat means you can convert `Str` to `pyarrow.Buffer` and `Strs` to `pyarrow.Array` without extra copies.\n\n## Quick Start: C/C++ 🛠️\n\nThe C library is header-only, so you can just copy the `stringzilla.h` header into your project.\nSame applies to C++, where you would copy the `stringzilla.hpp` header.\nAlternatively, add it as a submodule, and include it in your build system.\n\n```sh\ngit submodule add https://github.com/ashvardanian/stringzilla.git\n```\n\nOr using a pure CMake approach:\n\n```cmake\nFetchContent_Declare(stringzilla GIT_REPOSITORY https://github.com/ashvardanian/stringzilla.git)\nFetchContent_MakeAvailable(stringzilla)\n```\n\nLast, but not the least, you can also install it as a library, and link against it.\nThis approach is worse for inlining, but brings dynamic runtime dispatch for the most advanced CPU features.\n\n### Basic Usage with C 99 and Newer\n\nThere is a stable C 99 interface, where all function names are prefixed with `sz_`.\nMost interfaces are well documented, and come with self-explanatory names and examples.\nIn some cases, hardware specific overloads are available, like `sz_find_avx512` or `sz_find_neon`.\nBoth are companions of the `sz_find`, first for x86 CPUs with AVX-512 support, and second for Arm NEON-capable CPUs.\n\n```c\n#include <stringzilla/stringzilla.h>\n\n// Initialize your haystack and needle\nsz_string_view_t haystack = {your_text, your_text_length};\nsz_string_view_t needle = {your_subtext, your_subtext_length};\n\n// Perform string-level operations\nsz_size_t substring_position = sz_find(haystack.start, haystack.length, needle.start, needle.length);\nsz_size_t substring_position = sz_find_avx512(haystack.start, haystack.length, needle.start, needle.length);\nsz_size_t substring_position = sz_find_neon(haystack.start, haystack.length, needle.start, needle.length);\n\n// Hash strings\nsz_u64_t hash = sz_hash(haystack.start, haystack.length);\n\n// Perform collection level operations\nsz_sequence_t array = {your_order, your_count, your_get_start, your_get_length, your_handle};\nsz_sort(&array, &your_config);\n```\n\n<details>\n  <summary><b>§ Mapping from LibC to StringZilla.</b></summary>\n\nBy design, StringZilla has a couple of notable differences from LibC:\n\n1. all strings are expected to have a length, and are not necessarily null-terminated.\n2. every operations has a reverse order counterpart.\n\nThat way `sz_find` and `sz_rfind` are similar to `strstr` and `strrstr` in LibC.\nSimilarly, `sz_find_byte` and `sz_rfind_byte` replace `memchr` and `memrchr`.\nThe `sz_find_charset` maps to `strspn` and `strcspn`, while `sz_rfind_charset` has no sibling in LibC.\n\n<table>\n    <tr>\n        <th>LibC Functionality</th>\n        <th>StringZilla Equivalents</th>\n    </tr>\n    <tr>\n        <td><code>memchr(haystack, needle, haystack_length)</code>, <code>strchr</code></td>\n        <td><code>sz_find_byte(haystack, haystack_length, needle)</code></td>\n    </tr>\n    <tr>\n        <td><code>memrchr(haystack, needle, haystack_length)</code></td>\n        <td><code>sz_rfind_byte(haystack, haystack_length, needle)</code></td>\n    </tr>\n    <tr>\n        <td><code>memcmp</code>, <code>strcmp</code></td>\n        <td><code>sz_order</code>, <code>sz_equal</code></td>\n    </tr>\n    <tr>\n        <td><code>strlen(haystack)</code></td>\n        <td><code>sz_find_byte(haystack, haystack_length, needle)</code></td>\n    </tr>\n    <tr>\n        <td><code>strcspn(haystack, needles)</code></td>\n        <td><code>sz_rfind_charset(haystack, haystack_length, needles_bitset)</code></td>\n    </tr>\n    <tr>\n        <td><code>strspn(haystack, needles)</code></td>\n        <td><code>sz_find_charset(haystack, haystack_length, needles_bitset)</code></td>\n    </tr>\n    <tr>\n        <td><code>memmem(haystack, haystack_length, needle, needle_length)</code>, <code>strstr</code></td>\n        <td><code>sz_find(haystack, haystack_length, needle, needle_length)</code></td>\n    </tr>\n    <tr>\n        <td><code>memcpy(destination, source, destination_length)</code></td>\n        <td><code>sz_copy(destination, source, destination_length)</code></td>\n    </tr>\n    <tr>\n        <td><code>memmove(destination, source, destination_length)</code></td>\n        <td><code>sz_move(destination, source, destination_length)</code></td>\n    </tr>\n    <tr>\n        <td><code>memset(destination, value, destination_length)</code></td>\n        <td><code>sz_fill(destination, destination_length, value)</code></td>\n    </tr>\n</table>\n\n</details>\n\n### Basic Usage with C++ 11 and Newer\n\nThere is a stable C++ 11 interface available in the `ashvardanian::stringzilla` namespace.\nIt comes with two STL-like classes: `string_view` and `string`.\nThe first is a non-owning view of a string, and the second is a mutable string with a [Small String Optimization][faq-sso].\n\n```cpp\n#include <stringzilla/stringzilla.hpp>\n\nnamespace sz = ashvardanian::stringzilla;\n\nsz::string haystack = \"some string\";\nsz::string_view needle = sz::string_view(haystack).substr(0, 4);\n\nauto substring_position = haystack.find(needle); // Or `rfind`\nauto hash = std::hash<sz::string_view>{}(haystack); // Compatible with STL's `std::hash`\n\nhaystack.end() - haystack.begin() == haystack.size(); // Or `rbegin`, `rend`\nhaystack.find_first_of(\" \\v\\t\") == 4; // Or `find_last_of`, `find_first_not_of`, `find_last_not_of`\nhaystack.starts_with(needle) == true; // Or `ends_with`\nhaystack.remove_prefix(needle.size()); // Why is this operation in-place?!\nhaystack.contains(needle) == true; // STL has this only from C++ 23 onwards\nhaystack.compare(needle) == 1; // Or `haystack <=> needle` in C++ 20 and beyond\n```\n\nStringZilla also provides string literals for automatic type resolution, [similar to STL][stl-literal]:\n\n```cpp\nusing sz::literals::operator\"\"_sz;\nusing std::literals::operator\"\"sv;\n\nauto a = \"some string\"; // char const *\nauto b = \"some string\"sv; // std::string_view\nauto b = \"some string\"_sz; // sz::string_view\n```\n\n[stl-literal]: https://en.cppreference.com/w/cpp/string/basic_string_view/operator%22%22sv\n\n### Memory Ownership and Small String Optimization\n\nMost operations in StringZilla don't assume any memory ownership.\nBut in addition to the read-only search-like operations StringZilla provides a minimalistic C and C++ implementations for a memory owning string \"class\".\nLike other efficient string implementations, it uses the [Small String Optimization][faq-sso] (SSO) to avoid heap allocations for short strings.\n\n[faq-sso]: https://cpp-optimizations.netlify.app/small_strings/\n\n```c\ntypedef union sz_string_t {\n    struct internal {\n        sz_ptr_t start;\n        sz_u8_t length;\n        char chars[SZ_STRING_INTERNAL_SPACE]; /// Ends with a null-terminator.\n    } internal;\n\n    struct external {\n        sz_ptr_t start;\n        sz_size_t length;        \n        sz_size_t space; /// The length of the heap-allocated buffer.\n        sz_size_t padding;\n    } external;\n\n} sz_string_t;\n```\n\nAs one can see, a short string can be kept on the stack, if it fits within `internal.chars` array.\nBefore 2015 GCC string implementation was just 8 bytes, and could only fit 7 characters.\nDifferent STL implementations today have different thresholds for the Small String Optimization.\nSimilar to GCC, StringZilla is 32 bytes in size, and similar to Clang it can fit 22 characters on stack.\nOur layout might be preferential, if you want to avoid branches.\nIf you use a different compiler, you may want to check it's SSO buffer size with a [simple Gist](https://gist.github.com/ashvardanian/c197f15732d9855c4e070797adf17b21).\n\n|                       | `libstdc++` in  GCC 13 | `libc++` in Clang 17 | StringZilla |\n| :-------------------- | ---------------------: | -------------------: | ----------: |\n| `sizeof(std::string)` |                     32 |                   24 |          32 |\n| Small String Capacity |                     15 |               __22__ |      __22__ |\n\nThis design has been since ported to many high-level programming languages.\nSwift, for example, [can store 15 bytes](https://developer.apple.com/documentation/swift/substring/withutf8(_:)#discussion) in the `String` instance itself.\nStringZilla implements SSO at the C level, providing the `sz_string_t` union and a simple API for primary operations.\n\n```c\nsz_memory_allocator_t allocator;\nsz_string_t string;\n\n// Init and make sure we are on stack\nsz_string_init(&string);\nsz_string_is_on_stack(&string); // == sz_true_k\n\n// Optionally pre-allocate space on the heap for future insertions.\nsz_string_grow(&string, 100, &allocator); // == sz_true_k\n\n// Append, erase, insert into the string.\nsz_string_expand(&string, 0, \"_Hello_\", 7, &allocator); // == sz_true_k\nsz_string_expand(&string, SZ_SIZE_MAX, \"world\", 5, &allocator); // == sz_true_k\nsz_string_erase(&string, 0, 1);\n\n// Unpacking & introspection.\nsz_ptr_t string_start;\nsz_size_t string_length;\nsz_size_t string_space;\nsz_bool_t string_is_external;\nsz_string_unpack(string, &string_start, &string_length, &string_space, &string_is_external);\nsz_equal(string_start, \"Hello_world\", 11); // == sz_true_k\n\n// Reclaim some memory.\nsz_string_shrink_to_fit(&string, &allocator); // == sz_true_k\nsz_string_free(&string, &allocator);\n```\n\nUnlike the conventional C strings, the `sz_string_t` is allowed to contain null characters.\nTo safely print those, pass the `string_length` to `printf` as well.\n\n```c\nprintf(\"%.*s\\n\", (int)string_length, string_start);\n```\n\n### What's Wrong with the C Standard Library?\n\nStringZilla is not a drop-in replacement for the C Standard Library.\nIt's designed to be a safer and more modern alternative.\nConceptually:\n\n1. LibC strings are expected to be null-terminated, so to use the efficient LibC implementations on slices of larger strings, you'd have to copy them, which is more expensive than the original string operation.\n2. LibC functionality is asymmetric - you can find the first and the last occurrence of a character within a string, but you can't find the last occurrence of a substring.\n3. LibC function names are typically very short and cryptic.\n4. LibC lacks crucial functionality like hashing and doesn't provide primitives for less critical but relevant operations like fuzzy matching.\n\nSomething has to be said about its support for UTF8.\nAside from a single-byte `char` type, LibC provides `wchar_t`:\n\n- The size of `wchar_t` is not consistent across platforms. On Windows, it's typically 16 bits (suitable for UTF-16), while on Unix-like systems, it's usually 32 bits (suitable for UTF-32). This inconsistency can lead to portability issues when writing cross-platform code.\n- `wchar_t` is designed to represent wide characters in a fixed-width format (UTF-16 or UTF-32). In contrast, UTF-8 is a variable-length encoding, where each character can take from 1 to 4 bytes. This fundamental difference means that `wchar_t` and UTF-8 are incompatible.\n\nStringZilla [partially addresses those issues](#unicode-utf-8-and-wide-characters).\n\n### What's Wrong with the C++ Standard Library?\n\n| C++ Code                             | Evaluation Result | Invoked Signature              |\n| :----------------------------------- | :---------------- | :----------------------------- |\n| `\"Loose\"s.replace(2, 2, \"vath\"s, 1)` | `\"Loathe\"` 🤢      | `(pos1, count1, str2, pos2)`   |\n| `\"Loose\"s.replace(2, 2, \"vath\", 1)`  | `\"Love\"` 🥰        | `(pos1, count1, str2, count2)` |\n\nStringZilla is designed to be a drop-in replacement for the C++ Standard Templates Library.\nThat said, some of the design decisions of STL strings are highly controversial, error-prone, and expensive.\nMost notably:\n\n1. Argument order for `replace`, `insert`, `erase` and similar functions is impossible to guess.\n2. Bounds-checking exceptions for `substr`-like functions are only thrown for one side of the range.\n3. Returning string copies in `substr`-like functions results in absurd volume of allocations.\n4. Incremental construction via `push_back`-like functions goes through too many branches.\n5. Inconsistency between `string` and `string_view` methods, like the lack of `remove_prefix` and `remove_suffix`.\n\nCheck the following set of asserts validating the `std::string` specification.\nIt's not realistic to expect the average developer to remember the [14 overloads of `std::string::replace`][stl-replace].\n\n[stl-replace]: https://en.cppreference.com/w/cpp/string/basic_string/replace\n\n```cpp\nusing str = std::string;\n\nassert(str(\"hello world\").substr(6) == \"world\");\nassert(str(\"hello world\").substr(6, 100) == \"world\"); // 106 is beyond the length of the string, but its OK\nassert_throws(str(\"hello world\").substr(100), std::out_of_range);   // 100 is beyond the length of the string\nassert_throws(str(\"hello world\").substr(20, 5), std::out_of_range); // 20 is beyond the length of the string\nassert_throws(str(\"hello world\").substr(-1, 5), std::out_of_range); // -1 casts to unsigned without any warnings...\nassert(str(\"hello world\").substr(0, -1) == \"hello world\");          // -1 casts to unsigned without any warnings...\n\nassert(str(\"hello\").replace(1, 2, \"123\") == \"h123lo\");\nassert(str(\"hello\").replace(1, 2, str(\"123\"), 1) == \"h23lo\");\nassert(str(\"hello\").replace(1, 2, \"123\", 1) == \"h1lo\");\nassert(str(\"hello\").replace(1, 2, \"123\", 1, 1) == \"h2lo\");\nassert(str(\"hello\").replace(1, 2, str(\"123\"), 1, 1) == \"h2lo\");\nassert(str(\"hello\").replace(1, 2, 3, 'a') == \"haaalo\");\nassert(str(\"hello\").replace(1, 2, {'a', 'b'}) == \"hablo\");\n```\n\nTo avoid those issues, StringZilla provides an alternative consistent interface.\nIt supports signed arguments, and doesn't have more than 3 arguments per function or\nThe standard API and our alternative can be conditionally disabled with `SZ_SAFETY_OVER_COMPATIBILITY=1`.\nWhen it's enabled, the _~~subjectively~~_ risky overloads from the Standard will be disabled.\n\n```cpp\nusing str = sz::string;\n\nstr(\"a:b\").front(1) == \"a\"; // no checks, unlike `substr`\nstr(\"a:b\").front(2) == \"2\"; // take first 2 characters\nstr(\"a:b\").back(-1) == \"b\"; // accepting negative indices\nstr(\"a:b\").back(-2) == \":b\"; // similar to Python's `\"a:b\"[-2:]`\nstr(\"a:b\").sub(1, -1) == \":\"; // similar to Python's `\"a:b\"[1:-1]`\nstr(\"a:b\").sub(-2, -1) == \":\"; // similar to Python's `\"a:b\"[-2:-1]`\nstr(\"a:b\").sub(-2, 1) == \"\"; // similar to Python's `\"a:b\"[-2:1]`\n\"a:b\"_sz[{-2, -1}] == \":\"; // works on views and overloads `operator[]`\n```\n\nAssuming StringZilla is a header-only library you can use the full API in some translation units and gradually transition to safer restricted API in others.\nBonus - all the bound checking is branchless, so it has a constant cost and won't hurt your branch predictor.\n\n### Beyond the C++ Standard Library - Learning from Python\n\nPython is arguably the most popular programming language for data science.\nIn part, that's due to the simplicity of its standard interfaces.\nStringZilla brings some of that functionality to C++.\n\n- Content checks: `isalnum`, `isalpha`, `isascii`, `isdigit`, `islower`, `isspace`, `isupper`.\n- Trimming character sets: `lstrip`, `rstrip`, `strip`.\n- Trimming string matches: `remove_prefix`, `remove_suffix`.\n- Ranges of search results: `splitlines`, `split`, `rsplit`.\n- Number of non-overlapping substring matches: `count`.\n- Partitioning: `partition`, `rpartition`.\n\nFor example, when parsing documents, it is often useful to split it into substrings.\nMost often, after that, you would compute the length of the skipped part, the offset and the length of the remaining part.\nThis results in a lot of pointer arithmetic and is error-prone.\nStringZilla provides a convenient `partition` function, which returns a tuple of three string views, making the code cleaner.\n\n```cpp\nauto parts = haystack.partition(':'); // Matching a character\nauto [before, match, after] = haystack.partition(':'); // Structure unpacking\nauto [before, match, after] = haystack.partition(sz::char_set(\":;\")); // Character-set argument\nauto [before, match, after] = haystack.partition(\" : \"); // String argument\nauto [before, match, after] = haystack.rpartition(sz::whitespaces_set()); // Split around the last whitespace\n```\n\nCombining those with the `split` function, one can easily parse a CSV file or HTTP headers.\n\n```cpp\nfor (auto line : haystack.split(\"\\r\\n\")) {\n    auto [key, _, value] = line.partition(':');\n    headers[key.strip()] = value.strip();\n}\n```\n\nSome other extensions are not present in the Python standard library either.\nLet's go through the C++ functionality category by category.\n\n- [Splits and Ranges](#splits-and-ranges).\n- [Concatenating Strings without Allocations](#concatenating-strings-without-allocations).\n- [Random Generation](#random-generation).\n- [Edit Distances and Fuzzy Search](#levenshtein-edit-distance-and-alignment-scores).\n\nSome of the StringZilla interfaces are not available even Python's native `str` class.\nHere is a sneak peek of the most useful ones.\n\n```cpp\ntext.hash(); // -> 64 bit unsigned integer \ntext.ssize(); // -> 64 bit signed length to avoid `static_cast<std::ssize_t>(text.size())`\ntext.contains_only(\" \\w\\t\"); // == text.find_first_not_of(sz::char_set(\" \\w\\t\")) == npos;\ntext.contains(sz::whitespaces_set()); // == text.find(sz::char_set(sz::whitespaces_set())) != npos;\n\n// Simpler slicing than `substr`\ntext.front(10); // -> sz::string_view\ntext.back(10); // -> sz::string_view\n\n// Safe variants, which clamp the range into the string bounds\nusing sz::string::cap;\ntext.front(10, cap) == text.front(std::min(10, text.size()));\ntext.back(10, cap) == text.back(std::min(10, text.size()));\n\n// Character set filtering\ntext.lstrip(sz::whitespaces_set()).rstrip(sz::newlines_set()); // like Python\ntext.front(sz::whitespaces_set()); // all leading whitespaces\ntext.back(sz::digits_set()); // all numerical symbols forming the suffix\n\n// Incremental construction\nusing sz::string::unchecked;\ntext.push_back('x'); // no surprises here\ntext.push_back('x', unchecked); // no bounds checking, Rust style\ntext.try_push_back('x'); // returns `false` if the string is full and the allocation failed\n\nsz::concatenate(text, \"@\", domain, \".\", tld); // No allocations\n```\n\n### Splits and Ranges\n\nOne of the most common use cases is to split a string into a collection of substrings.\nWhich would often result in [StackOverflow lookups][so-split] and snippets like the one below.\n\n[so-split]: https://stackoverflow.com/questions/14265581/parse-split-a-string-in-c-using-string-delimiter-standard-c\n\n```cpp\nstd::vector<std::string> lines = split(haystack, \"\\r\\n\"); // string delimiter\nstd::vector<std::string> words = split(lines, ' '); // character delimiter\n```\n\nThose allocate memory for each string and the temporary vectors.\nEach allocation can be orders of magnitude more expensive, than even serial `for`-loop over characters.\nTo avoid those, StringZilla provides lazily-evaluated ranges, compatible with the [Range-v3][range-v3] library.\n\n[range-v3]: https://github.com/ericniebler/range-v3\n\n```cpp\nfor (auto line : haystack.split(\"\\r\\n\"))\n    for (auto word : line.split(sz::char_set(\" \\w\\t.,;:!?\")))\n        std::cout << word << std::endl;\n```\n\nEach of those is available in reverse order as well.\nIt also allows interleaving matches, if you want both inclusions of `xx` in `xxx`.\nDebugging pointer offsets is not a pleasant exercise, so keep the following functions in mind.\n\n- `haystack.[r]find_all(needle, interleaving)`\n- `haystack.[r]find_all(sz::char_set(\"\"))`\n- `haystack.[r]split(needle)`\n- `haystack.[r]split(sz::char_set(\"\"))`\n\nFor $N$ matches the split functions will report $N+1$ matches, potentially including empty strings.\nRanges have a few convenience methods as well:\n\n```cpp\nrange.size(); // -> std::size_t\nrange.empty(); // -> bool\nrange.template to<std::set<std::sting>>(); \nrange.template to<std::vector<std::sting_view>>(); \n```\n\n### Concatenating Strings without Allocations\n\nAnother common string operation is concatenation.\nThe STL provides `std::string::operator+` and `std::string::append`, but those are not very efficient, if multiple invocations are performed.\n\n```cpp\nstd::string name, domain, tld;\nauto email = name + \"@\" + domain + \".\" + tld; // 4 allocations\n```\n\nThe efficient approach would be to pre-allocate the memory and copy the strings into it.\n\n```cpp\nstd::string email;\nemail.reserve(name.size() + domain.size() + tld.size() + 2);\nemail.append(name), email.append(\"@\"), email.append(domain), email.append(\".\"), email.append(tld);\n```\n\nThat's mouthful and error-prone.\nStringZilla provides a more convenient `concatenate` function, which takes a variadic number of arguments.\nIt also overrides the `operator|` to concatenate strings lazily, without any allocations.\n\n```cpp\nauto email = sz::concatenate(name, \"@\", domain, \".\", tld);   // 0 allocations\nauto email = name | \"@\" | domain | \".\" | tld;                // 0 allocations\nsz::string email = name | \"@\" | domain | \".\" | tld;          // 1 allocations\n```\n\n### Random Generation\n\nSoftware developers often need to generate random strings for testing purposes.\nThe STL provides `std::generate` and `std::random_device`, that can be used with StringZilla.\n\n```cpp\nsz::string random_string(std::size_t length, char const *alphabet, std::size_t cardinality) {\n    sz::string result(length, '\\0');\n    static std::random_device seed_source; // Expensive to construct - due to system calls\n    static std::mt19937 generator(seed_source()); // Also expensive - due to the state size\n    std::uniform_int_distribution<std::size_t> distribution(0, cardinality);\n    std::generate(result.begin(), result.end(), [&]() { return alphabet[distribution(generator)]; });\n    return result;\n}\n```\n\nMouthful and slow.\nStringZilla provides a C native method - `sz_generate` and a convenient C++ wrapper - `sz::generate`.\nSimilar to Python it also defines the commonly used character sets.\n\n```cpp\nauto protein = sz::string::random(300, \"ARNDCQEGHILKMFPSTWYV\"); // static method\nauto dna = sz::basic_string<custom_allocator>::random(3_000_000_000, \"ACGT\");\n\ndna.randomize(\"ACGT\"); // `noexcept` pre-allocated version\ndna.randomize(&std::rand, \"ACGT\"); // pass any generator, like `std::mt19937`\n\nchar uuid[36];\nsz::randomize(sz::string_span(uuid, 36), \"0123456789abcdef-\"); // Overwrite any buffer\n```\n\n### Bulk Replacements\n\nIn text processing, it's often necessary to replace all occurrences of a specific substring or set of characters within a string.\nStandard library functions may not offer the most efficient or convenient methods for performing bulk replacements, especially when dealing with large strings or performance-critical applications.\n\n- `haystack.replace_all(needle_string, replacement_string)`\n- `haystack.replace_all(sz::char_set(\"\"), replacement_string)`\n- `haystack.try_replace_all(needle_string, replacement_string)`\n- `haystack.try_replace_all(sz::char_set(\"\"), replacement_string)`\n- `haystack.transform(sz::look_up_table::identity())`\n- `haystack.transform(sz::look_up_table::identity(), haystack.data())`\n\n### Levenshtein Edit Distance and Alignment Scores\n\nLevenshtein and Hamming edit distance are provided for both byte-strings and UTF-8 strings.\nThe latter will output the distance in Unicode code points, not bytes.\nNeedleman-Wunsch alignment scores are only defined for byte-strings.\n\n```cpp\n// Count number of substitutions in same length strings\nsz::hamming_distance(first, second[, upper_bound]) -> std::size_t;\nsz::hamming_distance_utf8(first, second[, upper_bound]) -> std::size_t;\n\n// Count number of insertions, deletions and substitutions\nsz::edit_distance(first, second[, upper_bound[, allocator]]) -> std::size_t;\nsz::edit_distance_utf8(first, second[, upper_bound[, allocator]]) -> std::size_t;\n\n// Substitution-parametrized Needleman-Wunsch global alignment score\nstd::int8_t costs[256][256]; // Substitution costs matrix\nsz::alignment_score(first, second, costs[, gap_score[, allocator]) -> std::ptrdiff_t;\n```\n\n### Sorting in C and C++\n\nLibC provides `qsort` and STL provides `std::sort`.\nBoth have their quarks.\nThe LibC standard has no way to pass a context to the comparison function, that's only possible with platform-specific extensions.\nThose have [different arguments order](https://stackoverflow.com/a/39561369) on every OS.\n\n```c\n// Linux: https://linux.die.net/man/3/qsort_r\nvoid qsort_r(void *elements, size_t count, size_t element_width, \n    int (*compare)(void const *left, void const *right, void *context),\n    void *context);\n// MacOS and FreeBSD: https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man3/qsort_r.3.html\nvoid qsort_r(void *elements, size_t count, size_t element_width, \n    void *context,\n    int (*compare)(void *context, void const *left, void const *right));\n// Windows conflicts with ISO `qsort_s`: https://learn.microsoft.com/en-us/cpp/c-runtime-library/reference/qsort-s?view=msvc-170\nvoid qsort_s(id *elements, size_t count, size_t element_width, \n    int (*compare)(void *context, void const *left, void const *right),\n    void *context);\n```\n\nC++ generic algorithm is not perfect either.\nThere is no guarantee in the standard that `std::sort` won't allocate any memory.\nIf you are running on embedded, in real-time or on 100+ CPU cores per node, you may want to avoid that.\nStringZilla doesn't solve the general case, but hopes to improve the performance for strings.\nUse `sz_sort`, or the high-level `sz::sorted_order`, which can be used sort any collection of elements convertible to `sz::string_view`.\n\n```cpp\nstd::vector<std::string> data({\"c\", \"b\", \"a\"});\nstd::vector<std::size_t> order = sz::sorted_order(data); //< Simple shortcut\n\n// Or, taking care of memory allocation:\nsz::sorted_order(data.begin(), data.end(), order.data(), [](auto const &x) -> sz::string_view { return x; });\n```\n\n### Standard C++ Containers with String Keys\n\nThe C++ Standard Templates Library provides several associative containers, often used with string keys.\n\n```cpp\nstd::map<std::string, int, std::less<std::string>> sorted_words;\nstd::unordered_map<std::string, int, std::hash<std::string>, std::equal_to<std::string>> words;\n```\n\nThe performance of those containers is often limited by the performance of the string keys, especially on reads.\nStringZilla can be used to accelerate containers with `std::string` keys, by overriding the default comparator and hash functions.\n\n```cpp\nstd::map<std::string, int, sz::string_view_less> sorted_words;\nstd::unordered_map<std::string, int, sz::string_view_hash, sz::string_view_equal_to> words;\n```\n\nAlternatively, a better approach would be to use the `sz::string` class as a key.\nThe right hash function and comparator would be automatically selected and the performance gains would be more noticeable if the keys are short.\n\n```cpp\nstd::map<sz::string, int> sorted_words;\nstd::unordered_map<sz::string, int> words;\n```\n\n### Compilation Settings and Debugging\n\n__`SZ_DEBUG`__:\n\n> For maximal performance, the C library does not perform any bounds checking in Release builds.\n> In C++, bounds checking happens only in places where the STL `std::string` would do it.\n> If you want to enable more aggressive bounds-checking, define `SZ_DEBUG` before including the header.\n> If not explicitly set, it will be inferred from the build type.\n\n__`SZ_USE_X86_AVX512`, `SZ_USE_X86_AVX2`, `SZ_USE_ARM_NEON`__:\n\n> One can explicitly disable certain families of SIMD instructions for compatibility purposes.\n> Default values are inferred at compile time.\n\n__`SZ_DYNAMIC_DISPATCH`__:\n\n> By default, StringZilla is a header-only library.\n> But if you are running on different generations of devices, it makes sense to pre-compile the library for all supported generations at once, and dispatch at runtime.\n> This flag does just that and is used to produce the `stringzilla.so` shared library, as well as the Python bindings.\n\n__`SZ_USE_MISALIGNED_LOADS`__:\n\n> By default, StringZilla avoids misaligned loads.\n> If supported, it replaces many byte-level operations with word-level ones.\n> Going from `char`-like types to `uint64_t`-like ones can significantly accelerate the serial (SWAR) backend.\n> So consider enabling it if you are building for some embedded device.\n\n__`SZ_AVOID_LIBC`__ and __`SZ_OVERRIDE_LIBC`__:\n\n> When using the C header-only library one can disable the use of LibC.\n> This may affect the type resolution system on obscure hardware platforms. \n> Moreover, one may let `stringzilla` override the common symbols like the `memcpy` and `memset` with its own implementations.\n> In that case you can use the [`LD_PRELOAD` trick][ld-preload-trick] to prioritize it's symbols over the ones from the LibC and accelerate existing string-heavy applications without recompiling them.\n> It also adds a layer of security, as the `stringzilla` isn't [undefined for NULL inputs][redhat-memcpy-ub] like `memcpy(NULL, NULL, 0)`.\n\n[ld-preload-trick]: https://ashvardanian.com/posts/ld-preload-libsee\n[redhat-memcpy-ub]: https://developers.redhat.com/articles/2024/12/11/making-memcpynull-null-0-well-defined\n\n__`SZ_AVOID_STL`__ and __`SZ_SAFETY_OVER_COMPATIBILITY`__:\n\n> When using the C++ interface one can disable implicit conversions from `std::string` to `sz::string` and back.\n> If not needed, the `<string>` and `<string_view>` headers will be excluded, reducing compilation time.\n> Moreover, if STL compatibility is a low priority, one can make the API safer by disabling the overloads, which are subjectively error prone.\n\n__`STRINGZILLA_BUILD_SHARED`, `STRINGZILLA_BUILD_TEST`, `STRINGZILLA_BUILD_BENCHMARK`, `STRINGZILLA_TARGET_ARCH`__ for CMake users:\n\n> When compiling the tests and benchmarks, you can explicitly set the target hardware architecture.\n> It's synonymous to GCC's `-march` flag and is used to enable/disable the appropriate instruction sets.\n> You can also disable the shared library build, if you don't need it.\n\n## Quick Start: Rust 🦀\n\nStringZilla is available as a Rust crate, with documentation available on [docs.rs/stringzilla](https://docs.rs/stringzilla).\nTo use the latest crate release in your project, add the following to your `Cargo.toml`:\n\n```toml\n[dependencies]\nstringzilla = \">=3\"\n```\n\nOr if you want to use the latest pre-release version from the repository:\n\n```toml\n[dependencies]\nstringzilla = { git = \"https://github.com/ashvardanian/stringzilla\", branch = \"main-dev\" }\n```\n\nOnce installed, all of the functionality is available through the `stringzilla` namespace.\nMany interfaces will look familiar to the users of the `memchr` crate.\n\n```rust\nuse stringzilla::sz;\n\n// Identical to `memchr::memmem::find` and `memchr::memmem::rfind` functions\nsz::find(\"Hello, world!\", \"world\") // 7\nsz::rfind(\"Hello, world!\", \"world\") // 7\n\n// Generalizations of `memchr::memrchr[123]`\nsz::find_char_from(\"Hello, world!\", \"world\") // 2\nsz::rfind_char_from(\"Hello, world!\", \"world\") // 11\n```\n\nUnlike `memchr`, the throughput of `stringzilla` is [high in both normal and reverse-order searches][memchr-benchmarks].\nIt also provides no constraints on the size of the character set, while `memchr` allows only 1, 2, or 3 characters.\nIn addition to global functions, `stringzilla` provides a `StringZilla` extension trait:\n\n```rust\nuse stringzilla::StringZilla;\n\nlet my_string: String = String::from(\"Hello, world!\");\nlet my_str = my_string.as_str();\nlet my_cow_str = Cow::from(&my_string);\n\n// Use the generic function with a String\nassert_eq!(my_string.sz_find(\"world\"), Some(7));\nassert_eq!(my_string.sz_rfind(\"world\"), Some(7));\nassert_eq!(my_string.sz_find_char_from(\"world\"), Some(2));\nassert_eq!(my_string.sz_rfind_char_from(\"world\"), Some(11));\nassert_eq!(my_string.sz_find_char_not_from(\"world\"), Some(0));\nassert_eq!(my_string.sz_rfind_char_not_from(\"world\"), Some(12));\n\n// Same works for &str and Cow<'_, str>\nassert_eq!(my_str.sz_find(\"world\"), Some(7));\nassert_eq!(my_cow_str.as_ref().sz_find(\"world\"), Some(7));\n```\n\nThe library also exposes Levenshtein and Hamming edit-distances for byte-arrays and UTF-8 strings, as well as Needleman-Wunch alignment scores.\n\n```rust\nuse stringzilla::sz;\n\n// Handling arbitrary byte arrays:\nsz::edit_distance(\"Hello, world!\", \"Hello, world?\"); // 1\nsz::hamming_distance(\"Hello, world!\", \"Hello, world?\"); // 1\nsz::alignment_score(\"Hello, world!\", \"Hello, world?\", sz::unary_substitution_costs(), -1); // -1\n\n// Handling UTF-8 strings:\nsz::hamming_distance_utf8(\"αβγδ\", \"αγγδ\") // 1\nsz::edit_distance_utf8(\"façade\", \"facade\") // 1\n```\n\n[memchr-benchmarks]: https://github.com/ashvardanian/memchr_vs_stringzilla\n\n## Quick Start: Swift 🍏\n\nStringZilla can be added as a dependency in the Swift Package Manager.\nIn your `Package.swift` file, add the following:\n\n```swift\ndependencies: [\n    .package(url: \"https://github.com/ashvardanian/stringzilla\")\n]\n```\n\nThe package currently covers only the most basic functionality, but is planned to be extended to cover the full C++ API.\n\n```swift\nvar s = \"Hello, world! Welcome to StringZilla. 👋\"\ns[s.findFirst(substring: \"world\")!...] // \"world! Welcome to StringZilla. 👋\")    \ns[s.findLast(substring: \"o\")!...] // \"o StringZilla. 👋\")\ns[s.findFirst(characterFrom: \"aeiou\")!...] // \"ello, world! Welcome to StringZilla. 👋\")\ns[s.findLast(characterFrom: \"aeiou\")!...] // \"a. 👋\")\ns[s.findFirst(characterNotFrom: \"aeiou\")!...] // \"Hello, world! Welcome to StringZilla. 👋\"\ns.editDistance(from: \"Hello, world!\")! // 29\n```\n\n## Algorithms & Design Decisions 📚\n\nStringZilla aims to optimize some of the slowest string operations.\nSome popular operations, however, like equality comparisons and relative order checking, almost always complete on some of the very first bytes in either string.\nIn such operations vectorization is almost useless, unless huge and very similar strings are considered.\nStringZilla implements those operations as well, but won't result in substantial speedups.\n\n### Exact Substring Search\n\nSubstring search algorithms are generally divided into: comparison-based, automaton-based, and bit-parallel.\nDifferent families are effective for different alphabet sizes and needle lengths.\nThe more operations are needed per-character - the more effective SIMD would be.\nThe longer the needle - the more effective the skip-tables are.\nStringZilla uses different exact substring search algorithms for different needle lengths and backends:\n\n- When no SIMD is available - SWAR (SIMD Within A Register) algorithms are used on 64-bit words.\n- Boyer-Moore-Horspool (BMH) algorithm with Raita heuristic variation for longer needles.\n- SIMD algorithms are randomized to look at different parts of the needle.\n\nOn very short needles, especially 1-4 characters long, brute force with SIMD is the fastest solution.\nOn mid-length needles, bit-parallel algorithms are effective, as the character masks fit into 32-bit or 64-bit words.\nEither way, if the needle is under 64-bytes long, on haystack traversal we will still fetch every CPU cache line.\nSo the only way to improve performance is to reduce the number of comparisons.\nThe snippet below shows how StringZilla accomplishes that for needles of length two.\n\nhttps://github.com/ashvardanian/StringZilla/blob/266c01710dddf71fc44800f36c2f992ca9735f87/include/stringzilla/stringzilla.h#L1585-L1637\n\nGoing beyond that, to long needles, Boyer-Moore (BM) and its variants are often the best choice.\nIt has two tables: the good-suffix shift and the bad-character shift.\nCommon choice is to use the simplified BMH algorithm, which only uses the bad-character shift table, reducing the pre-processing time.\nWe do the same for mid-length needles up to 256 bytes long.\nThat way the stack-allocated shift table remains small.\n\nhttps://github.com/ashvardanian/StringZilla/blob/46e957cd4f9ecd4945318dd3c48783dd11323f37/include/stringzilla/stringzilla.h#L1774-L1825\n\nIn the C++ Standards Library, the `std::string::find` function uses the BMH algorithm with Raita's heuristic.\nBefore comparing the entire string, it matches the first, last, and the middle character.\nVery practical, but can be slow for repetitive characters.\nBoth SWAR and SIMD backends of StringZilla have a cheap pre-processing step, where we locate unique characters.\nThis makes the library a lot more practical when dealing with non-English corpora.\n\nhttps://github.com/ashvardanian/StringZilla/blob/46e957cd4f9ecd4945318dd3c48783dd11323f37/include/stringzilla/stringzilla.h#L1398-L1431\n\nAll those, still, have $O(hn)$ worst case complexity.\nTo guarantee $O(h)$ worst case time complexity, the Apostolico-Giancarlo (AG) algorithm adds an additional skip-table.\nPreprocessing phase is $O(n+sigma)$ in time and space.\nOn traversal, performs from $(h/n)$ to $(3h/2)$ comparisons.\nIt however, isn't practical on modern CPUs.\nA simpler idea, the Galil-rule might be a more relevant optimizations, if many matches must be found.\n\nOther algorithms previously considered and deprecated:\n\n- Apostolico-Giancarlo algorithm for longer needles. _Control-flow is too complex for efficient vectorization._\n- Shift-Or-based Bitap algorithm for short needles. _Slower than SWAR._\n- Horspool-style bad-character check in SIMD backends. _Effective only for very long needles, and very uneven character distributions between the needle and the haystack. Faster \"character-in-set\" check needed to generalize._\n\n> § Reading materials.\n> [Exact String Matching Algorithms in Java](https://www-igm.univ-mlv.fr/~lecroq/string).\n> [SIMD-friendly algorithms for substring searching](http://0x80.pl/articles/simd-strfind.html).\n\n### Levenshtein Edit Distance\n\nLevenshtein distance is the best known edit-distance for strings, that checks, how many insertions, deletions, and substitutions are needed to transform one string to another.\nIt's extensively used in approximate string-matching, spell-checking, and bioinformatics.\n\nThe computational cost of the Levenshtein distance is $O(n * m)$, where $n$ and $m$ are the lengths of the string arguments.\nTo compute that, the naive approach requires $O(n * m)$ space to store the \"Levenshtein matrix\", the bottom-right corner of which will contain the Levenshtein distance.\nThe algorithm producing the matrix has been simultaneously studied/discovered by the Soviet mathematicians Vladimir Levenshtein in 1965, Taras Vintsyuk in 1968, and American computer scientists - Robert Wagner, David Sankoff, Michael J. Fischer in the following years.\nSeveral optimizations are known:\n\n1. __Space Optimization__: The matrix can be computed in $O(min(n,m))$ space, by only storing the last two rows of the matrix.\n2. __Divide and Conquer__: Hirschberg's algorithm can be applied to decompose the computation into subtasks.\n3. __Automata__: Levenshtein automata can be effective, if one of the strings doesn't change, and is a subject to many comparisons.\n4. __Shift-Or__: Bit-parallel algorithms transpose the matrix into a bit-matrix, and perform bitwise operations on it.\n\nThe last approach is quite powerful and performant, and is used by the great [RapidFuzz][rapidfuzz] library.\nIt's less known, than the others, derived from the Baeza-Yates-Gonnet algorithm, extended to bounded edit-distance search by Manber and Wu in 1990s, and further extended by Gene Myers in 1999 and Heikki Hyyro between 2002 and 2004.\n\nStringZilla introduces a different approach, extensively used in Unum's internal combinatorial optimization libraries.\nThe approach doesn't change the number of trivial operations, but performs them in a different order, removing the data dependency, that occurs when computing the insertion costs.\nThis results in much better vectorization for intra-core parallelism and potentially multi-core evaluation of a single request.\n\nNext design goals:\n\n- [ ] Generalize fast traversals to rectangular matrices.\n- [ ] Port x86 AVX-512 solution to Arm NEON.\n\n> § Reading materials.\n> [Faster Levenshtein Distances with a SIMD-friendly Traversal Order](https://ashvardanian.com/posts/levenshtein-diagonal).\n\n[rapidfuzz]: https://github.com/rapidfuzz/RapidFuzz\n\n### Needleman-Wunsch Alignment Score for Bioinformatics\n\nThe field of bioinformatics studies various representations of biological structures.\nThe \"primary\" representations are generally strings over sparse alphabets:\n\n- [DNA][faq-dna] sequences, where the alphabet is {A, C, G, T}, ranging from ~100 characters for short reads to 3 billion for the human genome.\n- [RNA][faq-rna] sequences, where the alphabet is {A, C, G, U}, ranging from ~50 characters for tRNA to thousands for mRNA.\n- [Proteins][faq-protein], where the alphabet is made of 22 amino acids, ranging from 2 characters for [dipeptide][faq-dipeptide] to 35,000 for [Titin][faq-titin], the longest protein.\n\nThe shorter the representation, the more often researchers may want to use custom substitution matrices.\nMeaning that the cost of a substitution between two characters may not be the same for all pairs.\n\nStringZilla adapts the fairly efficient two-row Wagner-Fisher algorithm as a baseline serial implementation of the Needleman-Wunsch score.\nIt supports arbitrary alphabets up to 256 characters, and can be used with either [BLOSUM][faq-blosum], [PAM][faq-pam], or other substitution matrices.\nIt also uses SIMD for hardware acceleration of the substitution lookups.\nThis however, does not __yet__ break the data-dependency for insertion costs, where 80% of the time is wasted.\nWith that solved, the SIMD implementation will become 5x faster than the serial one.\n\n[faq-dna]: https://en.wikipedia.org/wiki/DNA\n[faq-rna]: https://en.wikipedia.org/wiki/RNA\n[faq-protein]: https://en.wikipedia.org/wiki/Protein\n[faq-blosum]: https://en.wikipedia.org/wiki/BLOSUM\n[faq-pam]: https://en.wikipedia.org/wiki/Point_accepted_mutation\n[faq-dipeptide]: https://en.wikipedia.org/wiki/Dipeptide\n[faq-titin]: https://en.wikipedia.org/wiki/Titin\n\n### Memory Copying, Fills, and Moves\n\nA lot has been written about the time computers spend copying memory and how that operation is implemented in LibC.\nInterestingly, the operation can still be improved, as most Assembly implementations use outdated instructions.\nEven performance-oriented STL replacements, like Meta's [Folly v2024.09.23 focus on AVX2](https://github.com/facebook/folly/blob/main/folly/memset.S), and don't take advantage of the new masked instructions in AVX-512 or SVE.\n\nIn AVX-512, StringZilla uses non-temporal stores to avoid cache pollution, when dealing with very large strings.\nMoreover, it handles the unaligned head and the tails of the `target` buffer separately, ensuring that writes in big copies are always aligned to cache-line boundaries.\nThat's true for both AVX2 and AVX-512 backends.\n\nStringZilla also contains \"drafts\" of smarter, but less efficient algorithms, that minimize the number of unaligned loads, perfoming shuffles and permutations.\nThat's a topic for future research, as the performance gains are not yet satisfactory.\n\n> § Reading materials.\n> [`memset` benchmarks](https://github.com/nadavrot/memset_benchmark?tab=readme-ov-file) by Nadav Rotem.\n> [Cache Associativity](https://en.algorithmica.org/hpc/cpu-cache/associativity/) by Sergey Slotin.\n\n### Random Generation\n\nGenerating random strings from different alphabets is a very common operation.\nStringZilla accepts an arbitrary [Pseudorandom Number Generator][faq-prng] to produce noise, and an array of characters to sample from.\nSampling is optimized to avoid integer division, a costly operation on modern CPUs.\nFor that a 768-byte long lookup table is used to perform 2 lookups, 1 multiplication, 2 shifts, and 2 accumulations.\n\nhttps://github.com/ashvardanian/StringZilla/blob/266c01710dddf71fc44800f36c2f992ca9735f87/include/stringzilla/stringzilla.h#L2490-L2533\n\n[faq-prng]: https://en.wikipedia.org/wiki/Pseudorandom_number_generator\n\n### Sorting\n\nFor lexicographic sorting of strings, StringZilla uses a \"hybrid-hybrid\" approach with $O(n * log(n))$ and.\n\n1. Radix sort for first bytes exported into a continuous buffer for locality.\n2. IntroSort on partially ordered chunks to balance efficiency and worst-case performance.\n   1. IntroSort begins with a QuickSort.\n   2. If the recursion depth exceeds a certain threshold, it switches to a HeapSort.\n\nA better algorithm is in development.\nCheck #173 for design goals and progress updates.\n\n### Hashing\n\n> [!WARNING]\n> Hash functions are not cryptographically safe and are currently under active development.\n> They may change in future __minor__ releases.\n\nChoosing the right hashing algorithm for your application can be crucial from both performance and security standpoint.\nIn StringZilla a 64-bit rolling hash function is reused for both string hashes and substring hashes, Rabin-style fingerprints.\nRolling hashes take the same amount of time to compute hashes with different window sizes, and are fast to update.\nThose are not however perfect hashes, and collisions are frequent.\nStringZilla attempts to use SIMD, but the performance is not __yet__ satisfactory.\nOn Intel Sapphire Rapids, the following numbers can be expected for N-way parallel variants.\n\n- 4-way AVX2 throughput with 64-bit integer multiplication (no native support): 0.28 GB/s.\n- 4-way AVX2 throughput with 32-bit integer multiplication: 0.54 GB/s.\n- 4-way AVX-512DQ throughput with 64-bit integer multiplication: 0.46 GB/s.\n- 4-way AVX-512 throughput with 32-bit integer multiplication: 0.58 GB/s.\n- 8-way AVX-512 throughput with 32-bit integer multiplication: 0.11 GB/s.\n\nNext design goals:\n\n- [ ] Try gear-hash and other rolling approaches.\n\n#### Why not CRC32?\n\nCyclic Redundancy Check 32 is one of the most commonly used hash functions in Computer Science.\nIt has in-hardware support on both x86 and Arm, for both 8-bit, 16-bit, 32-bit, and 64-bit words.\nThe `0x1EDC6F41` polynomial is used in iSCSI, Btrfs, ext4, and the `0x04C11DB7` in SATA, Ethernet, Zlib, PNG.\nIn case of Arm more than one polynomial is supported.\nIt is, however, somewhat limiting for Big Data usecases, which often have to deal with more than 4 Billion strings, making collisions unavoidable.\nMoreover, the existing SIMD approaches are tricky, combining general purpose computations with specialized instructions, to utilize more silicon in every cycle.\n\n> § Reading materials.\n> [Comprehensive derivation of approaches](https://github.com/komrad36/CRC)\n> [Faster computation for 4 KB buffers on x86](https://www.corsix.org/content/fast-crc32c-4k)\n> [Comparing different lookup tables](https://create.stephan-brumme.com/crc32)\n> Great open-source implementations.\n> [By Peter Cawley](https://github.com/corsix/fast-crc32)\n> [By Stephan Brumme](https://github.com/stbrumme/crc32)\n\n#### Other Modern Alternatives\n\n[MurmurHash](https://github.com/aappleby/smhasher/blob/master/README.md) from 2008 by Austin Appleby is one of the best known non-cryptographic hashes.\nIt has a very short implementation and is capable of producing 32-bit and 128-bit hashes.\nThe [CityHash](https://opensource.googleblog.com/2011/04/introducing-cityhash) from 2011 by Google and the [xxHash](https://github.com/Cyan4973/xxHash) improve on that, better leveraging the super-scalar nature of modern CPUs and producing 64-bit and 128-bit hashes.\n\nNeither of those functions are cryptographic, unlike MD5, SHA, and BLAKE algorithms.\nMost of cryptographic hashes are based on the Merkle-Damgård construction, and aren't resistant to the length-extension attacks.\nCurrent state of the Art, might be the [BLAKE3](https://github.com/BLAKE3-team/BLAKE3) algorithm.\nIt's resistant to a broad range of attacks, can process 2 bytes per CPU cycle, and comes with a very optimized official implementation for C and Rust.\nIt has the same 128-bit security level as the BLAKE2, and achieves its performance gains by reducing the number of mixing rounds, and processing data in 1 KiB chunks, which is great for longer strings, but may result in poor performance on short ones.\n\nAll mentioned libraries have undergone extensive testing and are considered production-ready.\nThey can definitely accelerate your application, but so may the downstream mixer.\nFor instance, when a hash-table is constructed, the hashes are further shrunk to address table buckets.\nIf the mixer looses entropy, the performance gains from the hash function may be lost.\nAn example would be power-of-two modulo, which is a common mixer, but is known to be weak.\nOne alternative would be the [fastrange](https://github.com/lemire/fastrange) by Daniel Lemire.\nAnother one is the [Fibonacci hash trick](https://probablydance.com/2018/06/16/fibonacci-hashing-the-optimization-that-the-world-forgot-or-a-better-alternative-to-integer-modulo/) using the Golden Ratio, also used in StringZilla.\n\n### Unicode, UTF-8, and Wide Characters\n\nMost StringZilla operations are byte-level, so they work well with ASCII and UTF8 content out of the box.\nIn some cases, like edit-distance computation, the result of byte-level evaluation and character-level evaluation may differ.\nSo StringZilla provides following functions to work with Unicode:\n\n- `sz_edit_distance_utf8` - computes the Levenshtein distance between two UTF-8 strings.\n- `sz_hamming_distance_utf8` - computes the Hamming distance between two UTF-8 strings.\n\nJava, JavaScript, Python 2, C#, and Objective-C, however, use wide characters (`wchar`) - two byte long codes, instead of the more reasonable fixed-length UTF32 or variable-length UTF8.\nThis leads [to all kinds of offset-counting issues][wide-char-offsets] when facing four-byte long Unicode characters.\nSo consider transcoding with [simdutf](https://github.com/simdutf/simdutf), if you are coming from such environments.\n\n[wide-char-offsets]: https://josephg.com/blog/string-length-lies/\n\n## Contributing 👾\n\nPlease check out the [contributing guide](https://github.com/ashvardanian/StringZilla/blob/main/CONTRIBUTING.md) for more details on how to setup the development environment and contribute to this project.\nIf you like this project, you may also enjoy [USearch][usearch], [UCall][ucall], [UForm][uform], and [SimSIMD][simsimd]. 🤗\n\n[usearch]: https://github.com/unum-cloud/usearch\n[ucall]: https://github.com/unum-cloud/ucall\n[uform]: https://github.com/unum-cloud/uform\n[simsimd]: https://github.com/ashvardanian/simsimd\n\nIf you like strings and value efficiency, you may also enjoy the following projects:\n\n- [simdutf](https://github.com/simdutf/simdutf) - transcoding UTF8, UTF16, and UTF32 LE and BE.\n- [hyperscan](https://github.com/intel/hyperscan) - regular expressions with SIMD acceleration.\n- [pyahocorasick](https://github.com/WojciechMula/pyahocorasick) - Aho-Corasick algorithm in Python.\n- [rapidfuzz](https://github.com/rapidfuzz/RapidFuzz) - fast string matching in C++ and Python.\n\nIf you are looking for more reading materials on this topic, consider the following:\n\n- [5x faster strings with SIMD & SWAR](https://ashvardanian.com/posts/stringzilla/).\n- [The Painful Pitfalls of C++ STL Strings](https://ashvardanian.com/posts/painful-strings/).\n\n## License 📜\n\nFeel free to use the project under Apache 2.0 or the Three-clause BSD license at your preference.\n"
        },
        {
          "name": "VERSION",
          "type": "blob",
          "size": 0.0068359375,
          "content": "3.11.3\n"
        },
        {
          "name": "binding.gyp",
          "type": "blob",
          "size": 0.3994140625,
          "content": "{\n    \"targets\": [\n        {\n            \"target_name\": \"stringzilla\",\n            \"sources\": [\"javascript/lib.c\"],\n            \"include_dirs\": [\"include\"],\n            \"cflags\": [\n                \"-std=c99\",\n                \"-Wno-unknown-pragmas\",\n                \"-Wno-maybe-uninitialized\",\n                \"-Wno-cast-function-type\",\n                \"-Wno-unused-function\",\n            ],\n        }\n    ]\n}\n"
        },
        {
          "name": "build.rs",
          "type": "blob",
          "size": 3.1904296875,
          "content": "use std::env;\n\nfn main() {\n    let mut build = cc::Build::new();\n    build\n        .file(\"c/lib.c\")\n        .include(\"include\")\n        .warnings(false)\n        .flag_if_supported(\"-std=c99\")\n        .flag_if_supported(\"-fPIC\");\n\n    // Cargo will set different environment variables that we can use to properly configure the build.\n    // https://doc.rust-lang.org/cargo/reference/environment-variables.html#environment-variables-cargo-sets-for-build-scripts\n    let target_arch = env::var(\"CARGO_CFG_TARGET_ARCH\").unwrap_or_default();\n    let target_endian = env::var(\"CARGO_CFG_TARGET_ENDIAN\").unwrap_or_default();\n\n    // To get the operating system we can use the TARGET environment variable.\n    // To check the list of available targets, run `rustc --print target-list`.\n    let target = env::var(\"TARGET\").unwrap_or_default();\n\n    if target.contains(\"linux\") {\n        build.flag_if_supported(\"-fdiagnostics-color=always\");\n        build.flag_if_supported(\"-O3\");\n        build.flag_if_supported(\"-pedantic\");\n\n        // Set architecture-specific flags and macros\n        if target_arch == \"x86_64\" {\n            build.define(\"SZ_USE_X86_AVX512\", \"1\");\n            build.define(\"SZ_USE_X86_AVX2\", \"1\");\n        } else {\n            build.define(\"SZ_USE_X86_AVX512\", \"0\");\n            build.define(\"SZ_USE_X86_AVX2\", \"0\");\n        }\n\n        if target_arch == \"aarch64\" {\n            build.flag_if_supported(\"-march=armv8-a+simd\");\n            build.define(\"SZ_USE_ARM_SVE\", \"1\");\n            build.define(\"SZ_USE_ARM_NEON\", \"1\");\n        } else {\n            build.define(\"SZ_USE_ARM_SVE\", \"0\");\n            build.define(\"SZ_USE_ARM_NEON\", \"0\");\n        }\n    } else if target.contains(\"darwin\") {\n        build.flag_if_supported(\"-fcolor-diagnostics\");\n        build.flag_if_supported(\"-O3\");\n        build.flag_if_supported(\"-pedantic\");\n\n        if target_arch == \"x86_64\" {\n            // Assuming no AVX-512 support for Darwin as per setup.py logic\n            build.define(\"SZ_USE_X86_AVX512\", \"0\");\n            build.define(\"SZ_USE_X86_AVX2\", \"1\");\n        } else {\n            build.define(\"SZ_USE_X86_AVX512\", \"0\");\n            build.define(\"SZ_USE_X86_AVX2\", \"0\");\n        }\n\n        if target_arch == \"aarch64\" {\n            build.define(\"SZ_USE_ARM_SVE\", \"0\"); // Assuming no SVE support for Darwin\n            build.define(\"SZ_USE_ARM_NEON\", \"1\");\n        } else {\n            build.define(\"SZ_USE_ARM_SVE\", \"0\");\n            build.define(\"SZ_USE_ARM_NEON\", \"0\");\n        }\n    } else if target.contains(\"windows\") {\n        // Set architecture-specific flags and macros\n        if target_arch == \"x86_64\" {\n            build.define(\"SZ_USE_X86_AVX512\", \"1\");\n            build.define(\"SZ_USE_X86_AVX2\", \"1\");\n        } else {\n            build.define(\"SZ_USE_X86_AVX512\", \"0\");\n            build.define(\"SZ_USE_X86_AVX2\", \"0\");\n        }\n    }\n\n    // Set endian-specific macro\n    if target_endian == \"big\" {\n        build.define(\"SZ_DETECT_BIG_ENDIAN\", \"1\");\n    } else {\n        build.define(\"SZ_DETECT_BIG_ENDIAN\", \"0\");\n    }\n\n    build.compile(\"stringzilla\");\n\n    println!(\"cargo:rerun-if-changed=c/lib.c\");\n    println!(\"cargo:rerun-if-changed=rust/lib.rs\");\n    println!(\"cargo:rerun-if-changed=include/stringzilla/stringzilla.h\");\n}\n"
        },
        {
          "name": "c",
          "type": "tree",
          "content": null
        },
        {
          "name": "cli",
          "type": "tree",
          "content": null
        },
        {
          "name": "include",
          "type": "tree",
          "content": null
        },
        {
          "name": "javascript",
          "type": "tree",
          "content": null
        },
        {
          "name": "package.json",
          "type": "blob",
          "size": 0.814453125,
          "content": "{\n  \"name\": \"stringzilla\",\n  \"version\": \"3.11.3\",\n  \"description\": \"SIMD-accelerated string search, sort, hashes, fingerprints, & edit distances\",\n  \"author\": \"Ash Vardanian\",\n  \"license\": \"Apache 2.0\",\n  \"main\": \"javascript/stringzilla.js\",\n  \"type\": \"module\",\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"https://github.com/ashvardanian/stringzilla.git\"\n  },\n  \"gypfile\": true,\n  \"engines\": {\n    \"node\": \"~10 >=10.20 || >=12.17\"\n  },\n  \"dependencies\": {\n    \"@types/node\": \"^20.4.5\",\n    \"bindings\": \"~1.2.1\",\n    \"node-addon-api\": \"^3.0.0\"\n  },\n  \"scripts\": {\n    \"test\": \"node --test ./scripts/test.js\"\n  },\n  \"devDependencies\": {\n    \"@semantic-release/exec\": \"^6.0.3\",\n    \"@semantic-release/git\": \"^10.0.1\",\n    \"conventional-changelog-eslint\": \"^3.0.9\",\n    \"semantic-release\": \"^21.1.2\",\n    \"typescript\": \"^5.1.6\"\n  }\n}"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 4.2294921875,
          "content": "# This file configures wheels compilation for `cibuilwheel` for StringZilla CPython bindings.\n# On a good day it will produce:\n#   - `manylinux` and `musllinux` wheels for Linux on x86_64, aarch64, i686, ppc64le, s390x;\n#   - `macos` wheels for x86_64, arm64, and universal2;\n#   - `windows` wheels for AMD64, x86, and ARM64.\n#   * for Python versions from 3.6 to 3.13.\n#   * running over 5,000 tests on each wheel.\n#   = meaning 16 platforms * 8 Python versions = 128 builds.\n#   = meaning over 500,000 tests.\n[build-system]\nrequires = [\"setuptools>=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.pytest.ini_options]\nminversion = \"6.0\"\naddopts = [\"-ra\", \"--showlocals\", \"--strict-markers\", \"--strict-config\"]\nxfail_strict = true\nfilterwarnings = [\"error\"]\n\n[tool.cibuildwheel]\ntest-requires = [\"pytest\", \"pytest-repeat\"]\ntest-command = \"pytest {project}/scripts/test.py -x\"\nbuild-verbosity = 0\n\n# We need to build for all platforms:\n# - on Linux: x86_64, aarch64, i686, ppc64le, s390x\n# - on MacOS: x86_64, arm64, universal2\n# - on Windows: AMD64, x86, ARM64\n# https://cibuildwheel.readthedocs.io/en/stable/options/#archs\n#\n# Important to note, not all those paltforms have recent images.\n# The `manylinux_2_28` seems to be missing for `i686`.\n# The `i686` is 32-bit x86, and `x86_64` is 64-bit x86.\narchs = [\"all\"]\n\n# Add \"pp*\" to skip PyPy builds, but they should work fine these days :)\n# https://cibuildwheel.readthedocs.io/en/stable/options/#build-skip\n# https://cibuildwheel.readthedocs.io/en/stable/#what-does-it-do\nskip = []\n\n[tool.cibuildwheel.linux]\nbefore-build = [\"rm -rf {project}/build\"]\nrepair-wheel-command = \"auditwheel repair --lib-sdir . -w {dest_dir} {wheel}\"\n\n# Use more recent images for the most popular SIMD-capable CPU architectures, to have access to newer compilers.\n# Otherwise, prepare yourself to all kinds of AVX-512 issues and other SIMD-related pain.\n# You can keep track of the most recent images on Quay:\n# - for `manylinux`: https://quay.io/search?q=manylinux\n# - for `musllinux`: https://quay.io/search?q=musllinux\nmanylinux-x86_64-image = \"manylinux_2_28\"\nmanylinux-aarch64-image = \"manylinux_2_28\"\nmanylinux-s390x-image = \"manylinux_2_28\"\nmanylinux-ppc64le-image = \"manylinux_2_28\"\nmusllinux-x86_64-image = \"musllinux_1_2\"\nmusllinux-aarch64-image = \"musllinux_1_2\"\nmusllinux-s390x-image = \"musllinux_1_2\"\nmusllinux-ppc64le-image = \"musllinux_1_2\"\nmusllinux-i686-image = \"musllinux_1_2\"\n\n# On CentOS we have to use `yum`.\n# The healthy version would be: `apt-get update && apt-get install -y libc6-dev wget python3-dev`.\nbefore-all = [\"yum update -y && yum install -y glibc-devel wget python3-devel\"]\n\n# With `musl` builds, we obviously don't need the `glibc` and can't use `yum`.\n# This may also be handy for using custom dependencies for different Python versions:\n# https://cibuildwheel.readthedocs.io/en/stable/options/#overrides\n[[tool.cibuildwheel.overrides]]\nselect = \"*-musllinux*\"\nbefore-all = \"apk add --update wget python3-dev\"\n\n[tool.cibuildwheel.macos]\nbefore-build = [\"rm -rf {project}/build\"]\nrepair-wheel-command = \"delocate-wheel --require-archs {delocate_archs} -w {dest_dir} -v {wheel}\"\n\n[tool.cibuildwheel.windows]\nbefore-build = [\"rd /s /q {project}\\\\build || echo Done\"]\n\n# Detect x86 64-bit builds\n[[tool.cibuildwheel.overrides]]\nselect = \"*-win_amd64\"\ninherit.environment = \"append\"\nenvironment.SZ_X86_64 = \"1\"\n\n[[tool.cibuildwheel.overrides]]\nselect = \"*-manylinux*_x86_64\"\ninherit.environment = \"append\"\nenvironment.SZ_X86_64 = \"1\"\n\n[[tool.cibuildwheel.overrides]]\nselect = \"*-musllinux*_x86_64\"\ninherit.environment = \"append\"\nenvironment.SZ_X86_64 = \"1\"\n\n[[tool.cibuildwheel.overrides]]\nselect = \"*-macos*_x86_64\"\ninherit.environment = \"append\"\nenvironment.SZ_X86_64 = \"1\"\n\n# Detect ARM 64-bit builds\n[[tool.cibuildwheel.overrides]]\nselect = \"*-win_arm64\"\ninherit.environment = \"append\"\nenvironment.SZ_ARM64 = \"1\"\n\n[[tool.cibuildwheel.overrides]]\nselect = \"*-manylinux*_aarch64\"\ninherit.environment = \"append\"\nenvironment.SZ_ARM64 = \"1\"\n\n[[tool.cibuildwheel.overrides]]\nselect = \"*-musllinux*_aarch64\"\ninherit.environment = \"append\"\nenvironment.SZ_ARM64 = \"1\"\n\n[[tool.cibuildwheel.overrides]]\nselect = \"*-macos*_arm64\"\ninherit.environment = \"append\"\nenvironment.SZ_ARM64 = \"1\"\n\n[tool.cibuildwheel.macos.environment]\nMACOSX_DEPLOYMENT_TARGET = \"10.11\"\n"
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "rust",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 7.626953125,
          "content": "import os\nimport sys\nimport platform\nfrom setuptools import setup, find_packages, Extension\nfrom typing import List, Tuple\nimport sysconfig\nimport glob\n\n\ndef get_compiler() -> str:\n    if platform.python_implementation() == \"CPython\":\n        compiler = platform.python_compiler().lower()\n        return \"gcc\" if \"gcc\" in compiler else \"llvm\" if \"clang\" in compiler else \"\"\n    return \"\"\n\n\nusing_cibuildwheels = os.environ.get(\"CIBUILDWHEEL\", \"0\") == \"1\"\n\n\ndef is_64bit_x86() -> bool:\n    if using_cibuildwheels:\n        return \"SZ_X86_64\" in os.environ\n    arch = platform.machine()\n    return arch in [\"x86_64\", \"x64\", \"AMD64\"]\n\n\ndef is_64bit_arm() -> bool:\n    if using_cibuildwheels:\n        return \"SZ_ARM64\" in os.environ\n    arch = platform.machine()\n    return arch in [\"arm64\", \"aarch64\", \"ARM64\"]\n\n\ndef is_big_endian() -> bool:\n    return sys.byteorder == \"big\"\n\n\ndef linux_settings() -> Tuple[List[str], List[str], List[Tuple[str]]]:\n    compile_args = [\n        \"-std=c99\",  # use the C 99 language dialect\n        \"-pedantic\",  # stick close to the C language standard, avoid compiler extensions\n        \"-O3\",  # maximum optimization level\n        \"-fdiagnostics-color=always\",  # color console output\n        \"-Wno-unknown-pragmas\",  # like: `pragma region` and some unrolls\n        \"-Wno-unused-function\",  # like: ... declared ‘static’ but never defined\n        \"-Wno-incompatible-pointer-types\",  # like: passing argument 4 of ‘sz_export_prefix_u32’ from incompatible pointer type\n        \"-Wno-discarded-qualifiers\",  # like: passing argument 1 of ‘free’ discards ‘const’ qualifier from pointer target type\n        \"-fPIC\",  # to enable dynamic dispatch\n    ]\n    link_args = [\n        \"-fPIC\",  # to enable dynamic dispatch\n    ]\n\n    # GCC is our primary compiler, so when packaging the library, even if the current machine\n    # doesn't support AVX-512 or SVE, still precompile those.\n    macros_args = [\n        (\"SZ_USE_X86_AVX512\", \"1\" if is_64bit_x86() else \"0\"),\n        (\"SZ_USE_X86_AVX2\", \"1\" if is_64bit_x86() else \"0\"),\n        (\"SZ_USE_ARM_SVE\", \"1\" if is_64bit_arm() else \"0\"),\n        (\"SZ_USE_ARM_NEON\", \"1\" if is_64bit_arm() else \"0\"),\n        (\"SZ_DETECT_BIG_ENDIAN\", \"1\" if is_big_endian() else \"0\"),\n    ]\n\n    return compile_args, link_args, macros_args\n\n\ndef darwin_settings() -> Tuple[List[str], List[str], List[Tuple[str]]]:\n\n    compile_args = [\n        \"-std=c99\",  # use the C 99 language dialect\n        \"-pedantic\",  # stick close to the C language standard, avoid compiler extensions\n        \"-O3\",  # maximum optimization level\n        \"-fcolor-diagnostics\",  # color console output\n        \"-Wno-unknown-pragmas\",  # like: `pragma region` and some unrolls\n        \"-Wno-incompatible-function-pointer-types\",\n        \"-Wno-incompatible-pointer-types\",  # like: passing argument 4 of ‘sz_export_prefix_u32’ from incompatible pointer type\n        \"-Wno-discarded-qualifiers\",  # like: passing argument 1 of ‘free’ discards ‘const’ qualifier from pointer target type\n        \"-fPIC\",  # to enable dynamic dispatch\n        \"-mfloat-abi=hard\",  # NEON intrinsics not available with the soft-float ABI\n        \"-mmacosx-version-min=11.0\",  # minimum macOS version\n    ]\n    link_args = [\n        \"-fPIC\",  # to enable dynamic dispatch\n    ]\n\n    # Apple Clang doesn't support the `-march=native` argument,\n    # so we must pre-set the CPU generation. Technically the last Intel-based Apple\n    # product was the 2021 MacBook Pro, which had the \"Coffee Lake\" architecture.\n    # During Universal builds, however, even AVX header cause compilation errors.\n    can_use_avx2 = is_64bit_x86() and sysconfig.get_platform().startswith(\"universal\")\n    macros_args = [\n        (\"SZ_USE_X86_AVX512\", \"0\"),\n        (\"SZ_USE_X86_AVX2\", \"1\" if can_use_avx2 else \"0\"),\n        (\"SZ_USE_ARM_SVE\", \"0\"),\n        (\"SZ_USE_ARM_NEON\", \"1\" if is_64bit_arm() else \"0\"),\n    ]\n\n    return compile_args, link_args, macros_args\n\n\ndef windows_settings() -> Tuple[List[str], List[str], List[Tuple[str]]]:\n    compile_args = [\n        \"/std:c99\",  # use the C 99 language dialect\n        \"/Wall\",  # stick close to the C language standard, avoid compiler extensions\n        \"/O2\",  # maximum optimization level\n    ]\n\n    # When packaging the library, even if the current machine doesn't support AVX-512 or SVE, still precompile those.\n    macros_args = [\n        (\"SZ_USE_X86_AVX512\", \"1\" if is_64bit_x86() else \"0\"),\n        (\"SZ_USE_X86_AVX2\", \"1\" if is_64bit_x86() else \"0\"),\n        (\"SZ_USE_ARM_SVE\", \"0\"),\n        (\"SZ_USE_ARM_NEON\", \"1\" if is_64bit_arm() else \"0\"),\n        (\"SZ_DETECT_BIG_ENDIAN\", \"1\" if is_big_endian() else \"0\"),\n    ]\n\n    link_args = []\n    return compile_args, link_args, macros_args\n\n\nif sys.platform == \"linux\" or sys.platform.startswith('freebsd'):\n    compile_args, link_args, macros_args = linux_settings()\n\nelif sys.platform == \"darwin\":\n    compile_args, link_args, macros_args = darwin_settings()\n\nelif sys.platform == \"win32\":\n    compile_args, link_args, macros_args = windows_settings()\n\n# TODO: It would be great to infer available compilation flags on FreeBSD. They are likely similar to Linux\nelse:\n    compile_args, link_args, macros_args = [], [], []\n\next_modules = [\n    Extension(\n        \"stringzilla\",\n        [\"python/lib.c\"] + glob.glob(\"c/*.c\"),\n        # In the past I've used `np.get_include()` to include NumPy headers,\n        # but it's not necessary for this library.\n        include_dirs=[\"include\"],\n        extra_compile_args=compile_args,\n        extra_link_args=link_args,\n        define_macros=[(\"SZ_DYNAMIC_DISPATCH\", \"1\")] + macros_args,\n    ),\n]\n\n__version__ = open(\"VERSION\", \"r\").read().strip()\n__lib_name__ = \"stringzilla\"\n\n\nthis_directory = os.path.abspath(os.path.dirname(__file__))\nwith open(os.path.join(this_directory, \"README.md\"), \"r\", encoding=\"utf-8\") as f:\n    long_description = f.read()\n\n\nsetup(\n    name=__lib_name__,\n    version=__version__,\n    description=\"SIMD-accelerated string search, sort, hashes, fingerprints, & edit distances\",\n    author=\"Ash Vardanian\",\n    author_email=\"1983160+ashvardanian@users.noreply.github.com\",\n    url=\"https://github.com/ashvardanian/stringzilla\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    license=\"Apache-2.0\",\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Natural Language :: English\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Information Technology\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Programming Language :: C++\",\n        \"Programming Language :: Python :: 3 :: Only\",\n        \"Programming Language :: Python :: 3.6\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Programming Language :: Python :: 3.13\",\n        \"Programming Language :: Python :: Implementation :: CPython\",\n        \"Programming Language :: Python :: Implementation :: PyPy\",\n        \"Operating System :: OS Independent\",\n        \"Topic :: File Formats\",\n        \"Topic :: Internet :: Log Analysis\",\n        \"Topic :: Scientific/Engineering :: Information Analysis\",\n        \"Topic :: System :: Logging\",\n        \"Topic :: Text Processing :: General\",\n        \"Topic :: Text Processing :: Indexing\",\n    ],\n    include_dirs=[],\n    setup_requires=[],\n    ext_modules=ext_modules,\n    packages=find_packages(),\n    entry_points={\n        \"console_scripts\": [\n            \"sz_split=cli.split:main\",\n            \"sz_wc=cli.wc:main\",\n        ],\n    },\n)\n"
        },
        {
          "name": "swift",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}