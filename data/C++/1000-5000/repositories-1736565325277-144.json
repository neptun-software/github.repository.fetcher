{
  "metadata": {
    "timestamp": 1736565325277,
    "page": 144,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/StarSpace",
      "stars": 3950,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.263671875,
          "content": "# Prerequisites\n*.d\n\n# Compiled Object files\n*.slo\n*.lo\n*.o\n*.obj\n\n# Precompiled Headers\n*.gch\n*.pch\n\n# Compiled Dynamic libraries\n*.so\n*.dylib\n*.dll\n\n# Fortran module files\n*.mod\n*.smod\n\n# Compiled Static libraries\n*.lai\n*.la\n*.a\n*.lib\n\n# Executables\n*.exe\n*.out\n*.app\n"
        },
        {
          "name": "3rdparty",
          "type": "tree",
          "content": null
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 0.23828125,
          "content": "# Code of Conduct\n\nFacebook has adopted a Code of Conduct that we expect project participants to adhere to.\nPlease read the [full text](https://code.fb.com/codeofconduct/)\nso that you can understand what actions will and will not be tolerated.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.01953125,
          "content": "# Contributing to StarSpace\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `master`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\n## License\nBy contributing to StarSpace, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.060546875,
          "content": "MIT License\n\nCopyright (c) Facebook, Inc. and its affiliates.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MVS",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 25.32421875,
          "content": "<p align=\"center\"><img width=\"15%\" src=\"examples/starspace.png\" /></p>\n\n# StarSpace\n\nStarSpace is a general-purpose neural model for efficient learning of entity embeddings for solving a wide variety of problems: \n- Learning word, sentence or document level embeddings.\n- Information retrieval: ranking of sets of entities/documents or objects, e.g. ranking web documents.\n- Text classification, or any other labeling task.\n- Metric/similarity learning, e.g. learning sentence or document similarity.\n- Content-based or Collaborative filtering-based Recommendation, e.g. recommending music or videos.\n- Embedding graphs, e.g. multi-relational graphs such as Freebase.\n- Image classification, ranking or retrieval (e.g. by using existing ResNet features).\n\nIn the general case, it learns to represent objects of different types into a common vectorial embedding space,\nhence the star ('*', wildcard) and space in the name, and in that space compares them against each other.\nIt learns to rank a set of entities/documents or objects given a query entity/document or object, which is not necessarily the same type as the items in the set.\n\nSee the [paper](https://arxiv.org/abs/1709.03856) for more details on how it works.\n\n# News\n- <img width=\"5%\" src=\"examples/new2.gif\"> StarSpace is available in Python: check out the <a href=\"https://github.com/facebookresearch/StarSpace#building-starspace\">Building StarSpace</a> section for details. \n- <img width=\"5%\" src=\"examples/new2.gif\"> Support reading from compressed file: check out the <a href=\"https://github.com/facebookresearch/StarSpace/#compressed-file\">Compressed File</a> section for more details.\n- <img width=\"5%\" src=\"examples/new2.gif\"> New license and patents: now StarSpace is under MIT license. Checkout <a href=\"https://github.com/facebookresearch/StarSpace/blob/master/LICENSE.md\">LICENSE</a> for details.\n- StarSpace training is much faster now with mini batch training (setting batch size by \"-batchSize\" argument). Details in [#190](https://github.com/facebookresearch/StarSpace/pull/190).\n- We added support for real-valued input and label weights: checkout the <a href=\"https://github.com/facebookresearch/StarSpace/#file-format\">File Format</a> and <a href=\"https://github.com/facebookresearch/StarSpace/#imagespace-learning-image-and-label-embeddings\">ImageSpace</a> section for more details on how to use weights in input and label.\n\n# Requirements\n\nStarSpace builds on modern Mac OS, Windows, and Linux distributions. Since it uses C++11 features, it requires a compiler with good C++11 support. These include :\n\n* (gcc-4.6.3 or newer), (Visual Studio 2015), or (clang-3.3 or newer) \n\nCompilation is carried out using a Makefile, so you will need to have a working **make**.\n\nYou need to install <a href=http://www.boost.org/>Boost</a> library and specify the path of boost library in makefile in order to run StarSpace. Basically:\n\n    $wget https://dl.bintray.com/boostorg/release/1.63.0/source/boost_1_63_0.zip\n    $unzip boost_1_63_0.zip\n    $sudo mv boost_1_63_0 /usr/local/bin\n\nOptional: if one wishes to run the unit tests in src directory, <a href=https://github.com/google/googletest>google test</a> is required and its path needs to be specified in 'TEST_INCLUDES' in the makefile.\n\n# Building StarSpace\n\nIn order to build StarSpace on Mac OS or Linux, use the following:\n\n    git clone https://github.com/facebookresearch/Starspace.git\n    cd Starspace\n    make\n\nIn order to build StarSpace on Windows, open the following in Visual Studio:\n\n    MVS\\StarSpace.sln\nIn order to build StarSpace python wrapper, please refer <a href=\"https://github.com/facebookresearch/StarSpace/tree/master/python\">README</a> inside the directory <a href=\"https://github.com/facebookresearch/StarSpace/tree/master/python\">python</a>.\n\n# File Format\n\nStarSpace takes input files of the following format. \nEach line will be one input example, in the simplest case the input has k words, and each\nlabels 1..r is a single word:\n\n    word_1 word_2 ... word_k __label__1 ... __label__r\n\nThis file format is the same as in <a href=\"https://github.com/facebookresearch/fastText\">fastText</a>. It assumes by default that labels are words that are prefixed by the string \\_\\_label\\_\\_, and the prefix string can be set by \"-label\" argument. \n\nIn order to learn the embeddings, do:\n\n    $./starspace train -trainFile data.txt -model modelSaveFile\n\nwhere data.txt is a training file containing utf-8 encoded text. At the end of optimization the program will save two files: model and modelSaveFile.tsv. modelSaveFile.tsv is a standard tsv format file containing the entity embedding vectors, one per line. modelSaveFile is a binary file containing the parameters of the model along with the dictionary and all hyper parameters. The binary file can be used later to compute entity embedding vectors or to run evaluation tasks.\n\nIn the more general case, each label also consists of words:\n\n    word_1 word_2 ... word_k <tab> label_1_word_1 label_1_word_2 ... <tab> label_r_word_1 .. \n\nEmbedding vectors will be learned for each word and label to group similar inputs and labels together. \n\nIn order to learn the embeddings in the more general case where each label consists of words, one needs to specify the -fileFormat flag to be 'labelDoc', as follows:\n\n    $./starspace train -trainFile data.txt -model modelSaveFile -fileFormat labelDoc\n\nWe also extend the file format to support real-valued weights (in both input and label space) by setting argument \"-useWeight\" to true (default is false). If \"-useWeight\" is true, we support weights by the following format\n\n    word_1:wt_1 word_2:wt_2 ... word_k:wt_k __label__1:lwt_1 ...    __label__r:lwt_r\n    \ne.g.,\n\n    dog:0.1 cat:0.5 ...\n    \nThe default weight is 1 for any word / label that does not contain weights. \n\n# Compressed File\n\nStarSpace can also read from compressed file (currently only support gzip files). You can skip this part if you do not plan to use compressed input files. To run StarSpace with compressed input, first compile StarSpace using makefile_compress instead of makefile:\n\n    make -f makefile_compress\n\nThen in the train config, specify\n    \n    ./starspace -trainFile input -compressFile gzip -numGzFile 10 ...\n    \nIt assumes that there are input files with names \n\n    input00.gz, input01.gz, ..., input09.gz \n    \nand reads from those files.\n\nTo prepare data in this format, one can use the standard 'split' function to first split input file into multiple chunks, then compress them. For instance:\n\n    split -d -l xxx original_input.txt input && gzip input*\n\n\n## Training Mode\n\nTo explain how it works in different train modes, we call the input of a particular example as \"LHS\" (stands for left-hand-side) and the label as \"RHS\" (stands for right-hand-side). StarSpace supports the following training modes (the default is the first one): \n\n* trainMode = 0:\n    * Each example contains both input and labels.\n    * If fileFormat is 'fastText' then the labels are individuals features/words specified (e.g. with a prefix __label__, see file format above).\n    * **Use case:**  classification tasks, see _tagspace_ example below.\n    * If fileFormat is 'labelDoc' then the labels are bags of features, and one of those bags is selected (see file format, above).\n    * **Use case:**  retrieval/search tasks, each example consists of a query followed by a set of relevant documents.\n* trainMode = 1:\n    * Each example contains a collection of labels. At training time, one label from the collection is randomly picked as the RHS, and the rest of the labels in the collection become the LHS.\n    * **Use case:**  content-based or collaborative filtering-based recommendation, see _pagespace_ example below.\n* trainMode = 2:\n    * Each example contains a collection of labels. At training time, one label from the collection is randomly picked as the LHS, and the rest of the labels in the collection become the RHS.\n    * **Use case:** learning a mapping from an object to a set of objects of which it is a part, e.g. sentence (from within document) to document.\n* trainMode = 3:\n    * Each example contains a collection of labels. At training time, two labels from the collection are randomly picked as the LHS and RHS.\n    * **Use case:** learn pairwise similarity from collections of similar objects, e.g. sentence similiarity.\n* trainMode = 4:\n    * Each example contains two labels. At training time, the first label from the collection will be picked as the LHS and the second label will be picked as the RHS.\n    * **Use case:** learning from multi-relational graphs.\n* trainMode = 5:\n    * Each example contains only input. At training time, it generates multiple training examples: each feature from input is picked as the RHS, and other features surronding it (up to distance ws) are picked as the LHS.\n    * **Use case:** learn word embeddings in unsupervised way.\n\n# Example use cases\n\n## TagSpace word / tag embeddings\n\n**Setting:** Learning the mapping from a short text to relevant hashtags, e.g. as in <a href=\"https://research.fb.com/publications/tagspace-semantic-embeddings-from-hashtags/\">this paper</a>. This is a classical classification setting.\n\n**Model:** the mapping learnt goes from bags of words to bags of tags, by learning an embedding of both. \nFor instance,  the input “restaurant has great food <\\tab> #restaurant <\\tab> #yum” will be translated into the following graph. (Nodes in the graph are entities for which embeddings will be learned, and edges in the graph are relationships between the entities).\n\n![word-tag](https://github.com/facebookresearch/Starspace/blob/master/examples/tagspace.png)\n\n**Input file format**:\n\n    restaurant has great food #yum #restaurant\n\n**Command:**\n\n    $./starspace train -trainFile input.txt -model tagspace -label '#'\n\n### Example scripts:\nWe apply the model to the problem of text classification on <a href=\"https://github.com/mhjabreel/CharCNN/tree/master/data/ag_news_csv\">AG's News Topic Classification Dataset</a>. Here our tags are news article categories, and we use the hits@1 metric to measure classification accuracy. <a href=\"https://github.com/facebookresearch/Starspace/blob/master/examples/classification_ag_news.sh\">This example script</a> downloads the data and run StarSpace model on it under the examples directory:\n\n    $bash examples/classification_ag_news.sh\n    \n## PageSpace user / page embeddings \n\n**Setting:** On Facebook, users can fan (follow) public pages they're interested in. When a user fans a page, the user can receive all things the page posts on Facebook. We want to learn page embeddings based on users' fanning data, and use it to recommend users new pages they might be interested to fan (follow). This setting can be generalized to other recommendation problems: for instance, embedding and recommending movies to users based on movies watched in the past; embed and recommend restaurants to users based on the restaurants checked-in by users in the past, etc.\n\n**Model：** Users are represented as the bag of pages that they follow (fan). That is, we do not learn a direct embedding of users, instead, each user will have an embedding which is the average embedding of pages fanned by the user. Pages are embedded directly (with a unique feature in the dictionary). This setup can work better in the case where the number of users is larger than the number of pages, and the number of pages fanned by each user is small on average (i.e. the edges between user and page is relatively sparse). It also generalizes to new users without retraining. However, the more traditional recommendation setting can also be used.\n\n![user-page](https://github.com/facebookresearch/Starspace/blob/master/examples/user-page.png)\n\nEach user is represented by the bag-of-pages fanned by the user, and each training example is a single user.\n\n**Input file format:**\n\n    page_1 page_2 ... page_M\n\nAt training time, at each step for each example (user), one random page is selected as a label and the rest of bag of pages are selected as input. This can be achieved by setting flag -trainMode to 1. \n\n**Command:**\n\n    $./starspace train -trainFile input.txt -model pagespace -label 'page' -trainMode 1\n\n### Example scripts:\nTo provide an example script, we choose the Last.FM (http://www.lastfm.com)\n dataset from <a href=\"https://grouplens.org/datasets/hetrec-2011/\">HectRec 2011</a> and model it similarly as in the PageSpace setting: user is represented by the bag-of-artitsts listened by the user. \n \n     $bash examples/recomm_user_artists.sh\n\n## DocSpace document recommendation\n\n**Setting:** We want to embed and recommend web documents for users based on their historical likes/click data. \n\n**Model:** Each document is represented by a bag-of-words of the document. Each user is represented as a (bag of) the documents that they liked/clicked in the past. \nAt training time, at each step one random document is selected as the label and the rest of the bag of documents are selected as input. \n\n![user-doc](https://github.com/facebookresearch/Starspace/blob/master/examples/user-doc.png)\n\n\n**Input file format:**\n\n    roger federer loses <tab> venus williams wins <tab> world series ended\n    i love cats <tab> funny lolcat links <tab> how to be a petsitter  \n    \nEach line is a user, and each document (documents separated by tabs) are documents that they liked.\nSo the first user likes sports, and the second is interested in pets in this case.\n    \n**Command:**\n\n    ./starspace train -trainFile input.txt -model docspace -trainMode 1 -fileFormat labelDoc\n    \n    \n## GraphSpace: Link Prediction in Knowledge Bases ##\n\n**Setting:** Learning the mapping between entities and relations in <a href=\"http://www.freebase.com\">Freebase</a>. In freebase, data comes in the format \n\n    (head_entity, relation_type, tail_entity)\n\nPerforming link prediction can be formalized as filling in incomplete triples like \n\n    (head_entity, relation_type, ?) or (?, relation_type, tail_entity)\n\n**Model:** We learn the embeddings of all entities and relation types. For each relation_type, we learn two embeddings: one for predicting tail_entity given head_entity, one for predicting head_entity given tail_entity.\n\n![multi-rel](https://github.com/facebookresearch/StarSpace/blob/master/examples/multi-relations.png)\n\n### Example scripts:\n<a href=\"https://github.com/facebookresearch/Starspace/blob/master/examples/multi_relation_example.sh\">This example script</a> downloads the Freebase15k data from <a href=\"https://everest.hds.utc.fr/doku.php?id=en:transe\">here</a> and runs the StarSpace model on it:\n\n    $bash examples/multi_relation_example.sh\n   \n    \n## SentenceSpace: Learning Sentence Embeddings\n\n**Setting:** Learning the mapping between sentences. Given the embedding of one sentence, one can find semantically similar/relevant sentences.\n\n**Model:** Each example is a collection of sentences which are semantically related. Two are picked at random using trainMode 3: one as the input and one as the label, other sentences are picked as random negatives. One easy way to obtain semantically related sentences without labeling is to consider all sentences in the same document are related, and then train on those documents.\n\n![sentences](https://github.com/facebookresearch/StarSpace/blob/master/examples/sentences.png)\n\n### Example scripts:\n<a href=\"https://github.com/facebookresearch/Starspace/blob/master/examples/wikipedia_sentence_matching.sh\">This example script</a> downloads data where each example is a set of sentences from the same Wikipedia page and runs the StarSpace model on it:\n\n    $bash examples/wikipedia_sentence_matching.sh\n    \nTo run the full experiment on Wikipedia Sentence Matching presented in [this paper](https://arxiv.org/abs/1709.03856), \nuse <a href=\"https://github.com/facebookresearch/Starspace/blob/master/examples/wikipedia_sentence_matching_full.sh\">this script</a> (warning: it takes a long time to download data and train the model):\n\n    $bash examples/wikipedia_sentence_matching_full.sh\n    \n    \n## ArticleSpace: Learning Sentence and Article Embeddings\n\n**Setting:** Learning the mapping between sentences and articles. Given the embedding of one sentence, one can find the most relevant articles.\n\n**Model:** Each example is an article which contains multiple sentences. At training time, one sentence is picked at random as the input, the remaining sentences in the article becomes the label, other articles are picked as random negatives (trainMode 2).\n\n### Example scripts:\n<a href=\"https://github.com/facebookresearch/Starspace/blob/master/examples/wikipedia_article_search.sh\">This example script</a> downloads data where each example is a Wikipedia article and runs the StarSpace model on it:\n\n    $bash examples/wikipedia_article_search.sh\n    \nTo run the full experiment on Wikipedia Article Search presented in [this paper](https://arxiv.org/abs/1709.03856), \nuse <a href=\"https://github.com/facebookresearch/Starspace/blob/master/examples/wikipedia_article_search_full.sh\">this script</a> (warning: it takes a long time to download data and train the model):\n\n    $bash examples/wikipedia_article_search_full.sh\n    \n## ImageSpace: Learning Image and Label Embeddings\n\nWith the most recent update, StarSpace can also be used to learn joint embeddings with images and other entities. For instance, one can use ResNet features (the last layer of a pre-trained ResNet model) to represent an image, and embed images with other entities (words, hashtags, etc.). Just like other entities in Starspace, images can be either on the input or the label side, depending on your task.\n\nHere we give an example using <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10</a> to illustrate how we train images with other entities (in this example, image class): we train a <a href=\"https://github.com/facebookresearch/ResNeXt\">ResNeXt</a> model on CIFAR-10  which achieves 96.34% accuracy on test dataset, and use the last layer of ResNeXt as the features for each image. We embed 10 image classes together with image features in the same space using StarSpace. For an example image from class 1 with last layer (0.8, 0.5, ..., 1.2), we convert it to the following format:\n    \n    d0:0.8  d1:0.5   ...    d1023:1.2   __label__1\n\nAfter converting train and test examples of CIFAR-10 to the above format, we ran <a href=\"https://github.com/facebookresearch/StarSpace/blob/master/examples/image_feature_example_cifar10.sh\">this example script</a>:\n\n    $bash examples/image_feature_example_cifar10.sh\n\nand achieved 96.40% accuracy on an average of 5 runs.\n\n# Full Documentation of Parameters\n    \n    Run \"starspace train ...\"  or \"starspace test ...\"\n\n    The following arguments are mandatory for train: \n      -trainFile       training file path\n      -model           output model file path\n\n    The following arguments are mandatory for test: \n      -testFile        test file path\n      -model           model file path\n\n    The following arguments for the dictionary are optional:\n      -minCount        minimal number of word occurences [1]\n      -minCountLabel   minimal number of label occurences [1]\n      -ngrams          max length of word ngram [1]\n      -bucket          number of buckets [2000000]\n      -label           labels prefix [__label__]. See file format section.\n\n    The following arguments for training are optional:\n      -initModel       if not empty, it loads a previously trained model in -initModel and carry on training.\n      -trainMode       takes value in [0, 1, 2, 3, 4, 5], see Training Mode Section. [0]\n      -fileFormat      currently support 'fastText' and 'labelDoc', see File Format Section. [fastText]\n      -validationFile  validation file path\n      -validationPatience    number of iterations of validation where does not improve before we stop training [10]\n      -saveEveryEpoch  save intermediate models after each epoch [false]\n      -saveTempModel   save intermediate models after each epoch with an unique name including epoch number [false]\n      -lr              learning rate [0.01]\n      -dim             size of embedding vectors [100]\n      -epoch           number of epochs [5]\n      -maxTrainTime    max train time (secs) [8640000]\n      -negSearchLimit  number of negatives sampled [50]\n      -maxNegSamples   max number of negatives in a batch update [10]\n      -loss            loss function {hinge, softmax} [hinge]\n      -margin          margin parameter in hinge loss. It's only effective if hinge loss is used. [0.05]\n      -similarity      takes value in [cosine, dot]. Whether to use cosine or dot product as similarity function in  hinge loss.\n                       It's only effective if hinge loss is used. [cosine]\n      -p               normalization parameter: we normalize sum of embeddings by deviding Size^p, when p=1, it's equivalent to taking average of embeddings; when p=0, it's equivalent to taking sum of embeddings. [0.5]\n      -adagrad         whether to use adagrad in training [1]\n      -shareEmb        whether to use the same embedding matrix for LHS and RHS. [1]\n      -ws              only used in trainMode 5, the size of the context window for word level training. [5]\n      -dropoutLHS      dropout probability for LHS features. [0]\n      -dropoutRHS      dropout probability for RHS features. [0]\n      -initRandSd      initial values of embeddings are randomly generated from normal distribution with mean=0, standard deviation=initRandSd. [0.001]\n      -trainWord       whether to train word level together with other tasks (for multi-tasking). [0]\n      -wordWeight      if trainWord is true, wordWeight specifies example weight for word level training examples. [0.5]\n      -batchSize       size of mini batch in training. [5]\n\n    The following arguments for test are optional:\n      -basedoc         file path for a set of labels to compare against true label. It is required when -fileFormat='labelDoc'.\n                       In the case -fileFormat='fastText' and -basedoc is not provided, we compare true label with all other labels in the dictionary.\n      -predictionFile  file path for save predictions. If not empty, top K predictions for each example will be saved.\n      -K               if -predictionFile is not empty, top K predictions for each example will be saved.\n      -excludeLHS      exclude elements in the LHS from predictions\n\n    The following arguments are optional:\n      -normalizeText   whether to run basic text preprocess for input files [0]\n      -useWeight       whether input file contains weights [0]\n      -verbose         verbosity level [0]\n      -debug           whether it's in debug mode [0]\n      -thread          number of threads [10]\n\n\nNote: We use the same implementation of word n-grams for words as in <a href=\"https://github.com/facebookresearch/fastText\">fastText</a>. When \"-ngrams\" is set to be larger than 1, a hashing map of size specified by the \"-bucket\" argument is used for n-grams; when \"-ngrams\" is set to 1, no hash map is used, and the dictionary contains all words within the minCount and minCountLabel constraints.\n\n\n## Utility Functions\n\nWe also provide a few utility functions for StarSpace:\n### Show Predictions for Queries\n\nA simple way to check the quality of a trained embedding model is to inspect the predictions when typing in an input. To build and use this utility function, run the following commands:\n\n    make query_predict\n    ./query_predict <model> k [basedocs]\n    \nwhere \"\\<model\\>\" specifies a trained StarSpace model and the optional K specifies how many of the top predictions to show (top ranked first). \"basedocs\" points to the file of documents to rank, see also the argument of the same name in the starspace main above. If \"basedocs\" is not provided, the labels in the dictionary are used instead.\n\nAfter loading the model, it reads a line of entities (can be either a single word or a sentence / document), and outputs the predictions.\n\n### Nearest Neighbor Queries\n\nAnother simple way to check the quality of a trained embedding model is to inspect nearest neighbors of entities. To build and use this utility function, run the following commands:\n\n    make query_nn\n    ./query_nn <model> [k]\n    \nwhere \"\\<model\\>\" specifies a trained StarSpace model and the optional K (default value is 5) specifies how many nearest neighbors to search for.\n\nAfter loading the model, it reads a line of entities (can be either a single word or a sentence / document), and output the nearest entities in embedding space.\n\n### Print Ngrams\n\nAs the ngrams used in the model are not saved in tsv format, we also provide a separate function to output n-grams embeddings from the model. To use that, run the following commands:\n\n    make print_ngrams\n    ./print_ngrams <model>\n    \nwhere \"\\<model\\>\" specifies a trained StarSpace model with argument -ngrams > 1.\n\n### Print Sentence / Document Embedding\n\nSometimes it is useful to print out sentence / document embeddings from a trained model. To use that, run the following commands:\n\n    make embed_doc\n    ./embed_doc <model> [filename]\n    \nwhere \"\\<model\\>\" specifies a trained StarSpace model. If filename is provided, it reads each sentence / document from file, line by line, and outputs vector embeddings accordingly. If the filename is not provided, it reads each sentence / document from stdin.\n\n\n## Citation\n\nPlease cite the [arXiv paper](https://arxiv.org/abs/1709.03856) if you use StarSpace in your work:\n\n```\n@article{wu2017starspace,\n  title={StarSpace: Embed All The Things!},\n  author = {{Wu}, L. and {Fisch}, A. and {Chopra}, S. and {Adams}, K. and {Bordes}, A. and {Weston}, J.},\n  journal={arXiv preprint arXiv:{1709.03856}},\n  year={2017}\n}\n```\n## Contact\n* Facebook group: [StarSpace Users](https://www.facebook.com/groups/532005453808326)\n* emails: ledell@fb.com, jase@fb.com\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "makefile",
          "type": "blob",
          "size": 3.6689453125,
          "content": "#\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nCXX = g++\nCXXFLAGS = -pthread -std=gnu++11\n\nBOOST_DIR = /usr/local/bin/boost_1_63_0/\nGTEST_DIR = /usr/local/bin/googletest\n\nOBJS = normalize.o dict.o args.o proj.o parser.o data.o model.o starspace.o doc_parser.o doc_data.o utils.o\nTESTS = matrix_test proj_test\nINCLUDES = -I$(BOOST_DIR)\n\nopt: CXXFLAGS += -O3 -funroll-loops\nopt: starspace\n\ndebug: CXXFLAGS += -g -O0 -fno-inline\ndebug: starspace\n\n\nTEST_INCLUDES = -I$(GTEST_DIR)/include\n\nGTEST_HEADERS = $(GTEST_DIR)/include/gtest/*.h \\\n                $(GTEST_DIR)/include/gtest/internal/*.h\n\nCPPFLAGS += -isystem $(GTEST_DIR)/include\n\nGTEST_SRCS_ = $(GTEST_DIR)/src/*.cc $(GTEST_DIR)/src/*.h $(GTEST_HEADERS)\n\n# For simplicity and to avoid depending on Google Test's\n# implementation details, the dependencies specified below are\n# conservative and not optimized.  This is fine as Google Test\n# compiles fast and for ordinary users its source rarely changes.\n\ngtest-all.o : $(GTEST_SRCS_)\n\t$(CXX) $(CPPFLAGS) -I$(GTEST_DIR) $(CXXFLAGS) -c \\\n            $(GTEST_DIR)/src/gtest-all.cc\n\ngtest_main.o : $(GTEST_SRCS_)\n\t$(CXX) $(CPPFLAGS) -I$(GTEST_DIR) $(CXXFLAGS) -c \\\n            $(GTEST_DIR)/src/gtest_main.cc\n\ngtest.a : gtest-all.o\n\t$(AR) $(ARFLAGS) $@ $^\n\ngtest_main.a : gtest-all.o gtest_main.o\n\t$(AR) $(ARFLAGS) $@ $^\n\n\nnormalize.o: src/utils/normalize.cpp src/utils/normalize.h\n\t$(CXX) $(CXXFLAGS) -g -c src/utils/normalize.cpp\n\ndict.o: src/dict.cpp src/dict.h src/utils/args.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/dict.cpp\n\nargs.o: src/utils/args.cpp src/utils/args.h\n\t$(CXX) $(CXXFLAGS) -g -c src/utils/args.cpp\n\nmatrix_test.o: src/test/matrix_test.cpp src/matrix.h $(GTEST_HEADERS)\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) $(TEST_INCLUDES) -g -c src/test/matrix_test.cpp\n\nmodel.o: data.o src/model.cpp src/model.h src/utils/args.h src/proj.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/model.cpp\n\nmatrix_test: matrix_test.o gtest_main.a\n\t$(CXX) $(CPPFLAGS) $(CXXFLAGS) -lpthread $^ -o $@\n\nproj.o: src/proj.cpp src/proj.h src/matrix.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/proj.cpp\n\nproj_test.o: src/test/proj_test.cpp src/proj.h $(GTEST_HEADERS)\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) $(TEST_INCLUDES) -g -c src/test/proj_test.cpp\n\nproj_test: proj.o proj_test.o gtest_main.a\n\t$(CXX) $(CPPFLAGS) $(CXXFLAGS) -lpthread $^ -o $@\n\ndata.o: parser.o src/data.cpp src/data.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/data.cpp -o data.o\n\nutils.o: src/utils/utils.cpp src/utils/utils.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/utils/utils.cpp -o utils.o\n\ndoc_data.o: doc_parser.o data.o src/doc_data.cpp src/doc_data.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/doc_data.cpp -o doc_data.o\n\nparser.o: dict.o src/parser.cpp src/parser.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/parser.cpp -o parser.o\n\ndoc_parser.o: dict.o src/doc_parser.cpp src/doc_parser.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/doc_parser.cpp -o doc_parser.o\n\nstarspace.o: src/starspace.cpp src/starspace.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/starspace.cpp\n\nstarspace: $(OBJS)\n\t$(CXX) $(CXXFLAGS) $(OBJS) $(INCLUDES) -g src/main.cpp -o starspace\n\nquery_nn: $(OBJS)\n\t$(CXX) $(CXXFLAGS) $(OBJS) $(INCLUDES) -g src/apps/query_nn.cpp -o query_nn\n\nquery_predict: $(OBJS)\n\t$(CXX) $(CXXFLAGS) $(OBJS) $(INCLUDES) -g src/apps/query_predict.cpp -o query_predict\n\nprint_ngrams: $(OBJS)\n\t$(CXX) $(CXXFLAGS) $(OBJS) $(INCLUDES) -g src/apps/print_ngrams.cpp -o print_ngrams\n\nembed_doc: $(OBJS)\n\t$(CXX) $(CXXFLAGS) $(OBJS) $(INCLUDES) -g src/apps/embed_doc.cpp -o embed_doc\n\ntest: $(TESTS)\n\nclean:\n\trm -rf *.o starspace gtest.a gtest_main.a *_test query_nn print_ngrams\n"
        },
        {
          "name": "makefile_compress",
          "type": "blob",
          "size": 3.904296875,
          "content": "#\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nCXX = g++\nCXXFLAGS = -pthread -std=gnu++11 -D COMPRESS_FILE\n\nBOOST_DIR = /usr/local/bin/boost_1_63_0/\nGTEST_DIR = /usr/local/bin/googletest\n\nOBJS = normalize.o dict.o args.o proj.o parser.o data.o model.o starspace.o doc_parser.o doc_data.o utils.o\nTESTS = matrix_test proj_test\nINCLUDES = -I$(BOOST_DIR)\n\nopt: CXXFLAGS += -O3 -funroll-loops\nopt: starspace\n\ndebug: CXXFLAGS += -g -O0 -fno-inline\ndebug: starspace\n\n\nTEST_INCLUDES = -I$(GTEST_DIR)/include\n\nGTEST_HEADERS = $(GTEST_DIR)/include/gtest/*.h \\\n                $(GTEST_DIR)/include/gtest/internal/*.h\n\nCPPFLAGS += -isystem $(GTEST_DIR)/include\n\nGTEST_SRCS_ = $(GTEST_DIR)/src/*.cc $(GTEST_DIR)/src/*.h $(GTEST_HEADERS)\n\n# For simplicity and to avoid depending on Google Test's\n# implementation details, the dependencies specified below are\n# conservative and not optimized.  This is fine as Google Test\n# compiles fast and for ordinary users its source rarely changes.\n\ngtest-all.o : $(GTEST_SRCS_)\n\t$(CXX) $(CPPFLAGS) -I$(GTEST_DIR) $(CXXFLAGS) -c \\\n            $(GTEST_DIR)/src/gtest-all.cc\n\ngtest_main.o : $(GTEST_SRCS_)\n\t$(CXX) $(CPPFLAGS) -I$(GTEST_DIR) $(CXXFLAGS) -c \\\n            $(GTEST_DIR)/src/gtest_main.cc\n\ngtest.a : gtest-all.o\n\t$(AR) $(ARFLAGS) $@ $^\n\ngtest_main.a : gtest-all.o gtest_main.o\n\t$(AR) $(ARFLAGS) $@ $^\n\nnormalize.o: src/utils/normalize.cpp src/utils/normalize.h\n\t$(CXX) $(CXXFLAGS) -g -c src/utils/normalize.cpp\n\ndict.o: src/dict.cpp src/dict.h src/utils/args.h 3rdparty/zlib.cpp 3rdparty/gzip.cpp\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c -L/usr/local/lib -lz src/dict.cpp -o dict.o\n\nargs.o: src/utils/args.cpp src/utils/args.h\n\t$(CXX) $(CXXFLAGS) -g -c src/utils/args.cpp\n\nmatrix_test.o: src/test/matrix_test.cpp src/matrix.h $(GTEST_HEADERS)\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) $(TEST_INCLUDES) -g -c src/test/matrix_test.cpp\n\nmodel.o: data.o src/model.cpp src/model.h src/utils/args.h src/proj.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/model.cpp\n\nmatrix_test: matrix_test.o gtest_main.a\n\t$(CXX) $(CPPFLAGS) $(CXXFLAGS) -lpthread $^ -o $@\n\nproj.o: src/proj.cpp src/proj.h src/matrix.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/proj.cpp\n\nproj_test.o: src/test/proj_test.cpp src/proj.h $(GTEST_HEADERS)\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) $(TEST_INCLUDES) -g -c src/test/proj_test.cpp\n\nproj_test: proj.o proj_test.o gtest_main.a\n\t$(CXX) $(CPPFLAGS) $(CXXFLAGS) -lpthread $^ -o $@\n\ndata.o: parser.o utils.o src/data.cpp src/data.h 3rdparty/zlib.cpp 3rdparty/gzip.cpp\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c -L/usr/local/lib -lz src/data.cpp -o data.o\n\nutils.o: src/utils/utils.cpp src/utils/utils.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/utils/utils.cpp -o utils.o\n\ndoc_data.o: doc_parser.o data.o src/doc_data.cpp src/doc_data.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/doc_data.cpp -o doc_data.o\n\nparser.o: dict.o src/parser.cpp src/parser.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/parser.cpp -o parser.o\n\ndoc_parser.o: dict.o src/doc_parser.cpp src/doc_parser.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/doc_parser.cpp -o doc_parser.o\n\nstarspace.o: src/starspace.cpp src/starspace.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/starspace.cpp\n\nstarspace: $(OBJS) 3rdparty/zlib.cpp 3rdparty/gzip.cpp\n\t$(CXX) $(CXXFLAGS) $(OBJS) $(INCLUDES) -g -L/usr/local/lib -lz src/main.cpp 3rdparty/zlib.cpp 3rdparty/gzip.cpp -o starspace\n\nquery_nn: $(OBJS)\n\t$(CXX) $(CXXFLAGS) $(OBJS) $(INCLUDES) -g src/apps/query_nn.cpp -o query_nn\n\nquery_predict: $(OBJS)\n\t$(CXX) $(CXXFLAGS) $(OBJS) $(INCLUDES) -g src/apps/query_predict.cpp -o query_predict\n\nprint_ngrams: $(OBJS)\n\t$(CXX) $(CXXFLAGS) $(OBJS) $(INCLUDES) -g src/apps/print_ngrams.cpp -o print_ngrams\n\nembed_doc: $(OBJS)\n\t$(CXX) $(CXXFLAGS) $(OBJS) $(INCLUDES) -g src/apps/embed_doc.cpp -o embed_doc\n\ntest: $(TESTS)\n\nclean:\n\trm -rf *.o starspace gtest.a gtest_main.a *_test query_nn print_ngrams\n"
        },
        {
          "name": "makefile_py",
          "type": "blob",
          "size": 3.5595703125,
          "content": "CXX = g++\nCXXFLAGS = -pthread -std=gnu++11\n\nBOOST_DIR = /usr/local/bin/boost_1_63_0/\nGTEST_DIR = /usr/local/bin/googletest\n\nOBJS = normalize.o dict.o args.o proj.o parser.o data.o model.o starspace.o doc_parser.o doc_data.o utils.o\nTESTS = matrix_test proj_test\nINCLUDES = -I$(BOOST_DIR)\n\nopt: CXXFLAGS += -O3 -fPIC -funroll-loops\nopt: libstarspace.a\n\ndebug: CXXFLAGS += -g -O0 -fPIC -fno-inline\ndebug: libstarspace.a\n\n\nTEST_INCLUDES = -I$(GTEST_DIR)/include\n\nGTEST_HEADERS = $(GTEST_DIR)/include/gtest/*.h \\\n                $(GTEST_DIR)/include/gtest/internal/*.h\n\nCPPFLAGS += -isystem $(GTEST_DIR)/include\n\nGTEST_SRCS_ = $(GTEST_DIR)/src/*.cc $(GTEST_DIR)/src/*.h $(GTEST_HEADERS)\n\n# For simplicity and to avoid depending on Google Test's\n# implementation details, the dependencies specified below are\n# conservative and not optimized.  This is fine as Google Test\n# compiles fast and for ordinary users its source rarely changes.\n\ngtest-all.o : $(GTEST_SRCS_)\n\t$(CXX) $(CPPFLAGS) -I$(GTEST_DIR) $(CXXFLAGS) -c \\\n            $(GTEST_DIR)/src/gtest-all.cc\n\ngtest_main.o : $(GTEST_SRCS_)\n\t$(CXX) $(CPPFLAGS) -I$(GTEST_DIR) $(CXXFLAGS) -c \\\n            $(GTEST_DIR)/src/gtest_main.cc\n\ngtest.a : gtest-all.o\n\t$(AR) $(ARFLAGS) $@ $^\n\ngtest_main.a : gtest-all.o gtest_main.o\n\t$(AR) $(ARFLAGS) $@ $^\n\n\nnormalize.o: src/utils/normalize.cpp src/utils/normalize.h\n\t$(CXX) $(CXXFLAGS) -g -c src/utils/normalize.cpp\n\ndict.o: src/dict.cpp src/dict.h src/utils/args.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/dict.cpp\n\nargs.o: src/utils/args.cpp src/utils/args.h\n\t$(CXX) $(CXXFLAGS) -g -c src/utils/args.cpp\n\nmatrix_test.o: src/test/matrix_test.cpp src/matrix.h $(GTEST_HEADERS)\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) $(TEST_INCLUDES) -g -c src/test/matrix_test.cpp\n\nmodel.o: data.o src/model.cpp src/model.h src/utils/args.h src/proj.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/model.cpp\n\nmatrix_test: matrix_test.o gtest_main.a\n\t$(CXX) $(CPPFLAGS) $(CXXFLAGS) -lpthread $^ -o $@\n\nproj.o: src/proj.cpp src/proj.h src/matrix.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/proj.cpp\n\nproj_test.o: src/test/proj_test.cpp src/proj.h $(GTEST_HEADERS)\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) $(TEST_INCLUDES) -g -c src/test/proj_test.cpp\n\nproj_test: proj.o proj_test.o gtest_main.a\n\t$(CXX) $(CPPFLAGS) $(CXXFLAGS) -lpthread $^ -o $@\n\ndata.o: parser.o src/data.cpp src/data.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/data.cpp -o data.o\n\nutils.o: src/utils/utils.cpp src/utils/utils.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/utils/utils.cpp -o utils.o\n\ndoc_data.o: doc_parser.o data.o src/doc_data.cpp src/doc_data.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/doc_data.cpp -o doc_data.o\n\nparser.o: dict.o src/parser.cpp src/parser.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/parser.cpp -o parser.o\n\ndoc_parser.o: dict.o src/doc_parser.cpp src/doc_parser.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/doc_parser.cpp -o doc_parser.o\n\nstarspace.o: src/starspace.cpp src/starspace.h\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) -g -c src/starspace.cpp\n\nlibstarspace.a: $(OBJS)\n\tar rcs libstarspace.a $(OBJS)\n\nstarspace:\n\t$(CXX) $(CXXFLAGS) $(OBJS) $(INCLUDES) -g src/main.cpp -o starspace\n\nquery_nn: $(OBJS)\n\t$(CXX) $(CXXFLAGS) $(OBJS) $(INCLUDES) -g src/apps/query_nn.cpp -o query_nn\n\nquery_predict: $(OBJS)\n\t$(CXX) $(CXXFLAGS) $(OBJS) $(INCLUDES) -g src/apps/query_predict.cpp -o query_predict\n\nprint_ngrams: $(OBJS)\n\t$(CXX) $(CXXFLAGS) $(OBJS) $(INCLUDES) -g src/apps/print_ngrams.cpp -o print_ngrams\n\nembed_doc: $(OBJS)\n\t$(CXX) $(CXXFLAGS) $(OBJS) $(INCLUDES) -g src/apps/embed_doc.cpp -o embed_doc\n\ntest: $(TESTS)\n\nclean:\n\trm -rf *.o starspace gtest.a gtest_main.a *_test query_nn print_ngrams\n"
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}