{
  "metadata": {
    "timestamp": 1736565533524,
    "page": 409,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "li-plus/chatglm.cpp",
      "stars": 2963,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 0.0859375,
          "content": "BasedOnStyle: LLVM\nIndentWidth: 4\nColumnLimit: 120\nAlwaysBreakTemplateDeclarations: Yes\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.126953125,
          "content": "**/.git/\n.github/\n.hypothesis/\n.pytest_cache/\nbuild/\nchatglm_cpp.egg-info/\ndist/\n.dockerignore\nmodels/\nDockerfile\n**/__pycache__/\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.19921875,
          "content": "# ide\n.vscode/\n.vs/\n\n# macOS\n.DS_Store\n\n# python\n__pycache__/\n*.egg-info/\ndist/\n*.so\n*.whl\n.hypothesis/\n.venv\n\nchatglm_cpp/__init__.pyi\n\n# cpp\nbuild/\n\n# model\n*.bin\n\n# clangd\n.cache/\n\n# data\n/data/\n*.log\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.6455078125,
          "content": "[submodule \"third_party/sentencepiece\"]\n\tpath = third_party/sentencepiece\n\turl = https://github.com/google/sentencepiece.git\n[submodule \"third_party/pybind11\"]\n\tpath = third_party/pybind11\n\turl = https://github.com/pybind/pybind11.git\n\tbranch = stable\n[submodule \"third_party/ggml\"]\n\tpath = third_party/ggml\n\turl = https://github.com/ggerganov/ggml.git\n[submodule \"third_party/re2\"]\n\tpath = third_party/re2\n\turl = https://github.com/google/re2.git\n[submodule \"third_party/abseil-cpp\"]\n\tpath = third_party/abseil-cpp\n\turl = https://github.com/abseil/abseil-cpp.git\n[submodule \"third_party/stb\"]\n\tpath = third_party/stb\n\turl = https://github.com/nothings/stb.git\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 5.095703125,
          "content": "cmake_minimum_required(VERSION 3.12)\nproject(ChatGLM.cpp VERSION 0.0.1 LANGUAGES CXX)\n\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib CACHE STRING \"\")\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib CACHE STRING \"\")\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin CACHE STRING \"\")\n\nset(CMAKE_CXX_STANDARD 17)\n\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -g -Wall\")\nset(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -Wno-expansion-to-defined\")   # suppress ggml warnings\n\nif (NOT CMAKE_BUILD_TYPE)\n    set(CMAKE_BUILD_TYPE Release)\nendif ()\n\noption(CHATGLM_ENABLE_EXAMPLES \"chatglm: enable c++ examples\" ON)\noption(CHATGLM_ENABLE_PYBIND \"chatglm: enable python binding\" OFF)\noption(CHATGLM_ENABLE_TESTING \"chatglm: enable testing\" OFF)\n\nset(BUILD_SHARED_LIBS OFF CACHE BOOL \"\")\nif (CHATGLM_ENABLE_PYBIND)\n    set(BUILD_SHARED_LIBS OFF CACHE BOOL \"\" FORCE)\n    set(CMAKE_POSITION_INDEPENDENT_CODE ON)\nendif ()\n\n# third-party libraries\n\n# ggml\nif (GGML_CUDA)\n    add_compile_definitions(GGML_USE_CUDA)\n    enable_language(CUDA)\n    # ref: https://stackoverflow.com/questions/28932864/which-compute-capability-is-supported-by-which-cuda-versions\n    set(CUDA_ARCH_LIST \"52;61;70;75\")\n    if (CMAKE_CUDA_COMPILER_VERSION VERSION_GREATER_EQUAL \"11.0\")\n        set(CUDA_ARCH_LIST \"${CUDA_ARCH_LIST};80\")\n    endif ()\n    if (CMAKE_CUDA_COMPILER_VERSION VERSION_GREATER_EQUAL \"11.1\")\n        set(CUDA_ARCH_LIST \"${CUDA_ARCH_LIST};86\")\n    endif ()\n    if (CMAKE_CUDA_COMPILER_VERSION VERSION_GREATER_EQUAL \"11.8\")\n        set(CUDA_ARCH_LIST \"${CUDA_ARCH_LIST};89;90\")\n    endif ()\n    set(CMAKE_CUDA_ARCHITECTURES ${CUDA_ARCH_LIST} CACHE STRING \"\")\nendif ()\n\nif (GGML_METAL)\n    add_compile_definitions(GGML_USE_METAL)\n    set(GGML_METAL_EMBED_LIBRARY ON CACHE BOOL \"\" FORCE)\nendif ()\n\nif (GGML_PERF)\n    add_compile_definitions(GGML_PERF)\nendif ()\n\ninclude_directories(third_party/ggml/include/ggml third_party/ggml/src)\nadd_subdirectory(third_party/ggml)\n\n# sentencepiece\nset(SPM_ENABLE_SHARED OFF CACHE BOOL \"chatglm: disable sentencepiece shared libraries by default\")\nset(SPM_ENABLE_TCMALLOC OFF CACHE BOOL \"chatglm: disable tcmalloc by default\")\ninclude_directories(third_party/sentencepiece/src)\nadd_subdirectory(third_party/sentencepiece)\n\ninclude_directories(third_party/sentencepiece/third_party/protobuf-lite)\n\n# absl\nset(ABSL_ENABLE_INSTALL ON CACHE BOOL \"\" FORCE)\nset(ABSL_PROPAGATE_CXX_STD ON CACHE BOOL \"\" FORCE)\nadd_subdirectory(third_party/abseil-cpp)\n\n# re2\nadd_subdirectory(third_party/re2)\n\n# stb\ninclude_directories(third_party/stb)\n\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR})\n\nfile(GLOB CPP_SOURCES\n    ${PROJECT_SOURCE_DIR}/*.h\n    ${PROJECT_SOURCE_DIR}/*.cpp\n    ${PROJECT_SOURCE_DIR}/tests/*.cpp)\n\nadd_library(chatglm STATIC chatglm.cpp)\ntarget_link_libraries(chatglm PUBLIC ggml sentencepiece-static re2)\n\n# c++ examples\nif (CHATGLM_ENABLE_EXAMPLES)\n    add_executable(main main.cpp)\n    target_link_libraries(main PRIVATE chatglm)\n\n    find_package(OpenMP)\n    if (OpenMP_CXX_FOUND)\n        set(CHATGLM_OPENMP_TARGET OpenMP::OpenMP_CXX)\n    endif ()\n    add_executable(perplexity tests/perplexity.cpp)\n    target_link_libraries(perplexity PRIVATE chatglm ${CHATGLM_OPENMP_TARGET})\nendif ()\n\n# GoogleTest\nif (CHATGLM_ENABLE_TESTING)\n    enable_testing()\n\n    # ref: https://github.com/google/googletest/blob/main/googletest/README.md\n    include(FetchContent)\n    FetchContent_Declare(\n      googletest\n      # Specify the commit you depend on and update it regularly.\n      URL https://github.com/google/googletest/archive/refs/heads/main.zip\n    )\n    # For Windows: Prevent overriding the parent project's compiler/linker settings\n    set(gtest_force_shared_crt ON CACHE BOOL \"\" FORCE)\n    FetchContent_MakeAvailable(googletest)\n    include(GoogleTest)\n\n    # Now simply link against gtest or gtest_main as needed. Eg\n    add_executable(chatglm_test chatglm_test.cpp)\n    target_link_libraries(chatglm_test PRIVATE chatglm gtest_main)\n    gtest_discover_tests(chatglm_test)\nendif ()\n\nif (CHATGLM_ENABLE_PYBIND)\n    add_subdirectory(third_party/pybind11)\n    pybind11_add_module(_C chatglm_pybind.cpp)\n    target_link_libraries(_C PRIVATE chatglm)\nendif ()\n\n# lint\nfile(GLOB PY_SOURCES\n    ${PROJECT_SOURCE_DIR}/chatglm_cpp/*.py\n    ${PROJECT_SOURCE_DIR}/examples/*.py\n    ${PROJECT_SOURCE_DIR}/tests/*.py\n    ${PROJECT_SOURCE_DIR}/convert.py\n    ${PROJECT_SOURCE_DIR}/setup.py)\nadd_custom_target(lint\n    COMMAND clang-format -i ${CPP_SOURCES}\n    COMMAND isort ${PY_SOURCES}\n    COMMAND black ${PY_SOURCES} --verbose)\n\n# check all\nadd_custom_target(check-all\n    COMMAND cmake --build build -j\n    COMMAND ./build/bin/chatglm_test\n    COMMAND python3 setup.py develop\n    COMMAND python3 -m pytest --forked tests/test_chatglm_cpp.py\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}\n)\n\n# mypy\nadd_custom_target(mypy\n    mypy chatglm_cpp examples --exclude __init__.pyi\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}\n)\n\n# stub\nadd_custom_target(stub\n    pybind11-stubgen chatglm_cpp -o .\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}\n)\n\nif (MSVC)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall\")\n    add_definitions(\"/wd4267 /wd4244 /wd4305 /Zc:strictStrings /utf-8\")\nendif ()\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.7314453125,
          "content": "ARG BASE_IMAGE=ubuntu:20.04\n\nFROM ${BASE_IMAGE} AS build\n\nARG CMAKE_ARGS=\"-DGGML_CUDA=OFF\"\n\nWORKDIR /chatglm.cpp\n\n# apt\nRUN \\\n    sed -e \"s/archive.ubuntu.com/mirrors.tuna.tsinghua.edu.cn/g\" \\\n        -e \"s/security.ubuntu.com/mirrors.tuna.tsinghua.edu.cn/g\" -i /etc/apt/sources.list && \\\n    apt update && \\\n    DEBIAN_FRONTEND=noninteractive apt install -yq --no-install-recommends \\\n        gcc g++ make python3-dev python3-pip python3-venv && \\\n    rm -rf /var/lib/apt/lists/*\n\n# pip\nRUN \\\n    python3 -m pip install --upgrade -i https://pypi.tuna.tsinghua.edu.cn/simple pip && \\\n    python3 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple && \\\n    python3 -m pip install --no-cache-dir build cmake\n\nARG PATH=${PATH}:/usr/local/bin\n\nADD . .\n\n# build cpp binary\nRUN \\\n    cmake -B build ${CMAKE_ARGS} && \\\n    cmake --build build -j --config Release\n\n# build python binding\nRUN \\\n    CMAKE_ARGS=${CMAKE_ARGS} python3 -m build --wheel\n\nFROM ${BASE_IMAGE}\n\nWORKDIR /chatglm.cpp\n\nRUN \\\n    sed -e \"s/archive.ubuntu.com/mirrors.tuna.tsinghua.edu.cn/g\" \\\n        -e \"s/security.ubuntu.com/mirrors.tuna.tsinghua.edu.cn/g\" -i /etc/apt/sources.list && \\\n    apt update && \\\n    DEBIAN_FRONTEND=noninteractive apt install -yq --no-install-recommends \\\n        python3 python3-pip && \\\n    rm -rf /var/lib/apt/lists/*\n\nCOPY --from=build /chatglm.cpp/build/bin/main /chatglm.cpp/build/bin/main\nCOPY --from=build /chatglm.cpp/dist/ /chatglm.cpp/dist/\n\nADD examples examples\n\nRUN \\\n    python3 -m pip install --upgrade -i https://pypi.tuna.tsinghua.edu.cn/simple pip && \\\n    python3 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple && \\\n    python3 -m pip install --no-cache-dir -f dist 'chatglm-cpp[api]' && \\\n    rm -rf dist\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.041015625,
          "content": "MIT License\n\nCopyright (c) 2023 Jiahao Li\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.5205078125,
          "content": "global-include CMakeLists.txt *.cmake README.md LICENSE\ninclude *.cpp *.h\n\n# ggml\ngraft third_party/ggml/include\ngraft third_party/ggml/src\ninclude third_party/ggml/*\n\n# pybind11\ngraft third_party/pybind11/include\ngraft third_party/pybind11/tools\n\n# sentencepiece\ngraft third_party/sentencepiece/src\ngraft third_party/sentencepiece/third_party\ninclude third_party/sentencepiece/*\n\n# re2\ngraft third_party/re2\n\n# absl\ngraft third_party/abseil-cpp\n\n# stb\ninclude third_party/stb/stb_image.h\ninclude third_party/stb/stb_image_resize2.h\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 24.759765625,
          "content": "# ChatGLM.cpp\n\n[![CMake](https://github.com/li-plus/chatglm.cpp/actions/workflows/cmake.yml/badge.svg)](https://github.com/li-plus/chatglm.cpp/actions/workflows/cmake.yml)\n[![Python package](https://github.com/li-plus/chatglm.cpp/actions/workflows/python-package.yml/badge.svg)](https://github.com/li-plus/chatglm.cpp/actions/workflows/python-package.yml)\n[![PyPI](https://img.shields.io/pypi/v/chatglm-cpp)](https://pypi.org/project/chatglm-cpp/)\n![Python](https://img.shields.io/pypi/pyversions/chatglm-cpp)\n[![License: MIT](https://img.shields.io/badge/license-MIT-blue)](LICENSE)\n\nC++ implementation of [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B), [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B), [ChatGLM3](https://github.com/THUDM/ChatGLM3) and [GLM-4](https://github.com/THUDM/GLM-4)(V) for real-time chatting on your MacBook.\n\n![demo](docs/demo.gif)\n\n## Features\n\nHighlights:\n* Pure C++ implementation based on [ggml](https://github.com/ggerganov/ggml), working in the same way as [llama.cpp](https://github.com/ggerganov/llama.cpp).\n* Accelerated memory-efficient CPU inference with int4/int8 quantization, optimized KV cache and parallel computing.\n* P-Tuning v2 and LoRA finetuned models support.\n* Streaming generation with typewriter effect.\n* Python binding, web demo, api servers and more possibilities.\n\nSupport Matrix:\n* Hardwares: x86/arm CPU, NVIDIA GPU, Apple Silicon GPU\n* Platforms: Linux, MacOS, Windows\n* Models: [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B), [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B), [ChatGLM3](https://github.com/THUDM/ChatGLM3), [GLM-4](https://github.com/THUDM/GLM-4)(V), [CodeGeeX2](https://github.com/THUDM/CodeGeeX2)\n\n## Getting Started\n\n**Preparation**\n\nClone the ChatGLM.cpp repository into your local machine:\n```sh\ngit clone --recursive https://github.com/li-plus/chatglm.cpp.git && cd chatglm.cpp\n```\n\nIf you forgot the `--recursive` flag when cloning the repository, run the following command in the `chatglm.cpp` folder:\n```sh\ngit submodule update --init --recursive\n```\n\n**Quantize Model**\n\nInstall necessary packages for loading and quantizing Hugging Face models:\n```sh\npython3 -m pip install -U pip\npython3 -m pip install torch tabulate tqdm transformers accelerate sentencepiece\n```\n\nUse `convert.py` to transform ChatGLM-6B into quantized GGML format. For example, to convert the fp16 original model to q4_0 (quantized int4) GGML model, run:\n```sh\npython3 chatglm_cpp/convert.py -i THUDM/chatglm-6b -t q4_0 -o models/chatglm-ggml.bin\n```\n\nThe original model (`-i <model_name_or_path>`) can be a Hugging Face model name or a local path to your pre-downloaded model. Currently supported models are:\n* ChatGLM-6B: `THUDM/chatglm-6b`, `THUDM/chatglm-6b-int8`, `THUDM/chatglm-6b-int4`\n* ChatGLM2-6B: `THUDM/chatglm2-6b`, `THUDM/chatglm2-6b-int4`, `THUDM/chatglm2-6b-32k`, `THUDM/chatglm2-6b-32k-int4`\n* ChatGLM3-6B: `THUDM/chatglm3-6b`, `THUDM/chatglm3-6b-32k`, `THUDM/chatglm3-6b-128k`, `THUDM/chatglm3-6b-base`\n* ChatGLM4(V)-9B: `THUDM/glm-4-9b-chat`, `THUDM/glm-4-9b-chat-1m`, `THUDM/glm-4-9b`, `THUDM/glm-4v-9b`\n* CodeGeeX2: `THUDM/codegeex2-6b`, `THUDM/codegeex2-6b-int4`\n\nYou are free to try any of the below quantization types by specifying `-t <type>`:\n| type   | precision | symmetric |\n| ------ | --------- | --------- |\n| `q4_0` | int4      | true      |\n| `q4_1` | int4      | false     |\n| `q5_0` | int5      | true      |\n| `q5_1` | int5      | false     |\n| `q8_0` | int8      | true      |\n| `f16`  | half      |           |\n| `f32`  | float     |           |\n\nFor LoRA models, add `-l <lora_model_name_or_path>` flag to merge your LoRA weights into the base model. For example, run `python3 chatglm_cpp/convert.py -i THUDM/chatglm3-6b -t q4_0 -o models/chatglm3-ggml-lora.bin -l shibing624/chatglm3-6b-csc-chinese-lora` to merge public LoRA weights from Hugging Face.\n\nFor P-Tuning v2 models using the [official finetuning script](https://github.com/THUDM/ChatGLM3/tree/main/finetune_demo), additional weights are automatically detected by `convert.py`. If `past_key_values` is on the output weight list, the P-Tuning checkpoint is successfully converted.\n\n**Build & Run**\n\nCompile the project using CMake:\n```sh\ncmake -B build\ncmake --build build -j --config Release\n```\n\nNow you may chat with the quantized ChatGLM-6B model by running:\n```sh\n./build/bin/main -m models/chatglm-ggml.bin -p ‰Ω†Â•Ω\n# ‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\n```\n\nTo run the model in interactive mode, add the `-i` flag. For example:\n```sh\n./build/bin/main -m models/chatglm-ggml.bin -i\n```\nIn interactive mode, your chat history will serve as the context for the next-round conversation.\n\nRun `./build/bin/main -h` to explore more options!\n\n**Try Other Models**\n\n<details open>\n<summary>ChatGLM2-6B</summary>\n\n```sh\npython3 chatglm_cpp/convert.py -i THUDM/chatglm2-6b -t q4_0 -o models/chatglm2-ggml.bin\n./build/bin/main -m models/chatglm2-ggml.bin -p ‰Ω†Â•Ω --top_p 0.8 --temp 0.8\n# ‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM2-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\n```\n</details>\n\n<details open>\n<summary>ChatGLM3-6B</summary>\n\nChatGLM3-6B further supports function call and code interpreter in addition to chat mode.\n\nChat mode:\n```sh\npython3 chatglm_cpp/convert.py -i THUDM/chatglm3-6b -t q4_0 -o models/chatglm3-ggml.bin\n./build/bin/main -m models/chatglm3-ggml.bin -p ‰Ω†Â•Ω --top_p 0.8 --temp 0.8\n# ‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM3-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\n```\n\nSetting system prompt:\n```sh\n./build/bin/main -m models/chatglm3-ggml.bin -p ‰Ω†Â•Ω -s \"You are ChatGLM3, a large language model trained by Zhipu.AI. Follow the user's instructions carefully. Respond using markdown.\"\n# ‰Ω†Â•ΩüëãÔºÅÊàëÊòØ ChatGLM3ÔºåÊúâ‰ªÄ‰πàÈóÆÈ¢òÂèØ‰ª•Â∏ÆÊÇ®Ëß£Á≠îÂêóÔºü\n```\n\nFunction call:\n~~~\n$ ./build/bin/main -m models/chatglm3-ggml.bin --top_p 0.8 --temp 0.8 --sp examples/system/function_call.txt -i\nSystem   > Answer the following questions as best as you can. You have access to the following tools: ...\nPrompt   > ÁîüÊàê‰∏Ä‰∏™ÈöèÊú∫Êï∞\nChatGLM3 > random_number_generator\n```python\ntool_call(seed=42, range=(0, 100))\n```\nTool Call   > Please manually call function `random_number_generator` with args `tool_call(seed=42, range=(0, 100))` and provide the results below.\nObservation > 23\nChatGLM3 > Ê†πÊçÆÊÇ®ÁöÑË¶ÅÊ±ÇÔºåÊàë‰ΩøÁî®ÈöèÊú∫Êï∞ÁîüÊàêÂô®APIÁîüÊàê‰∫Ü‰∏Ä‰∏™ÈöèÊú∫Êï∞„ÄÇÊ†πÊçÆAPIËøîÂõûÁªìÊûúÔºåÁîüÊàêÁöÑÈöèÊú∫Êï∞‰∏∫23„ÄÇ\n~~~\n\nCode interpreter:\n~~~\n$ ./build/bin/main -m models/chatglm3-ggml.bin --top_p 0.8 --temp 0.8 --sp examples/system/code_interpreter.txt -i\nSystem   > ‰Ω†ÊòØ‰∏Ä‰ΩçÊô∫ËÉΩAIÂä©ÊâãÔºå‰Ω†Âè´ChatGLMÔºå‰Ω†ËøûÊé•ÁùÄ‰∏ÄÂè∞ÁîµËÑëÔºå‰ΩÜËØ∑Ê≥®ÊÑè‰∏çËÉΩËÅîÁΩë„ÄÇÂú®‰ΩøÁî®PythonËß£ÂÜ≥‰ªªÂä°Êó∂Ôºå‰Ω†ÂèØ‰ª•ËøêË°å‰ª£Á†ÅÂπ∂ÂæóÂà∞ÁªìÊûúÔºåÂ¶ÇÊûúËøêË°åÁªìÊûúÊúâÈîôËØØÔºå‰Ω†ÈúÄË¶ÅÂ∞ΩÂèØËÉΩÂØπ‰ª£Á†ÅËøõË°åÊîπËøõ„ÄÇ‰Ω†ÂèØ‰ª•Â§ÑÁêÜÁî®Êà∑‰∏ä‰º†Âà∞ÁîµËÑë‰∏äÁöÑÊñá‰ª∂ÔºåÊñá‰ª∂ÈªòËÆ§Â≠òÂÇ®Ë∑ØÂæÑÊòØ/mnt/data/„ÄÇ\nPrompt   > ÂàóÂá∫100‰ª•ÂÜÖÁöÑÊâÄÊúâË¥®Êï∞\nChatGLM3 > Â•ΩÁöÑÔºåÊàë‰ºö‰∏∫ÊÇ®ÂàóÂá∫100‰ª•ÂÜÖÁöÑÊâÄÊúâË¥®Êï∞„ÄÇ\n```python\ndef is_prime(n):\n   \"\"\"Check if a number is prime.\"\"\"\n   if n <= 1:\n       return False\n   if n <= 3:\n       return True\n   if n % 2 == 0 or n % 3 == 0:\n       return False\n   i = 5\n   while i * i <= n:\n       if n % i == 0 or n % (i + 2) == 0:\n           return False\n       i += 6\n   return True\n\nprimes_upto_100 = [i for i in range(2, 101) if is_prime(i)]\nprimes_upto_100\n```\n\nCode Interpreter > Please manually run the code and provide the results below.\nObservation      > [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\nChatGLM3 > 100‰ª•ÂÜÖÁöÑÊâÄÊúâË¥®Êï∞‰∏∫Ôºö\n\n$$\n2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97 \n$$\n~~~\n\n</details>\n\n<details open>\n<summary>ChatGLM4-9B</summary>\n\nChat mode:\n```sh\npython3 chatglm_cpp/convert.py -i THUDM/glm-4-9b-chat -t q4_0 -o models/chatglm4-ggml.bin\n./build/bin/main -m models/chatglm4-ggml.bin -p ‰Ω†Â•Ω --top_p 0.8 --temp 0.8\n# ‰Ω†Â•ΩüëãÔºÅÊúâ‰ªÄ‰πàÂèØ‰ª•Â∏ÆÂä©‰Ω†ÁöÑÂêóÔºü\n```\n\n</details>\n\n<details open>\n<summary>ChatGLM4V-9B</summary>\n\n[![03-Confusing-Pictures](examples/03-Confusing-Pictures.jpg)](https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg)\n\nYou may use `-vt <vision_type>` to set quantization type for the vision encoder. It is recommended to run GLM4V on GPU since vision encoding runs too slow on CPU even with 4-bit quantization.\n```sh\npython3 chatglm_cpp/convert.py -i THUDM/glm-4v-9b -t q4_0 -vt q4_0 -o models/chatglm4v-ggml.bin\n./build/bin/main -m models/chatglm4v-ggml.bin --image examples/03-Confusing-Pictures.jpg -p \"ËøôÂº†ÂõæÁâáÊúâ‰ªÄ‰πà‰∏çÂØªÂ∏∏ÁöÑÂú∞Êñπ\" --temp 0\n# ËøôÂº†ÂõæÁâá‰∏≠‰∏çÂØªÂ∏∏ÁöÑÂú∞ÊñπÂú®‰∫éÔºåÁî∑Â≠êÊ≠£Âú®‰∏ÄËæÜÈªÑËâ≤Âá∫ÁßüËΩ¶ÂêéÈù¢ÁÜ®Ë°£Êúç„ÄÇÈÄöÂ∏∏ÊÉÖÂÜµ‰∏ãÔºåÁÜ®Ë°£ÊòØÂú®ÂÆ∂‰∏≠ÊàñÊ¥óË°£Â∫óËøõË°åÁöÑÔºåËÄå‰∏çÊòØÂú®ËΩ¶ËæÜ‰∏ä„ÄÇÊ≠§Â§ñÔºåÂá∫ÁßüËΩ¶Âú®Ë°åÈ©∂‰∏≠ÔºåÁî∑Â≠êÂç¥ËÉΩÂ§üÁ®≥ÂÆöÂú∞ÁÜ®Ë°£ÔºåËøôÂ¢ûÂä†‰∫ÜÂú∫ÊôØÁöÑËçíËØûÊÑü„ÄÇ\n```\n\n</details>\n\n<details>\n<summary>CodeGeeX2</summary>\n\n```sh\n$ python3 chatglm_cpp/convert.py -i THUDM/codegeex2-6b -t q4_0 -o models/codegeex2-ggml.bin\n$ ./build/bin/main -m models/codegeex2-ggml.bin --temp 0 --mode generate -p \"\\\n# language: Python\n# write a bubble sort function\n\"\n\n\ndef bubble_sort(lst):\n    for i in range(len(lst) - 1):\n        for j in range(len(lst) - 1 - i):\n            if lst[j] > lst[j + 1]:\n                lst[j], lst[j + 1] = lst[j + 1], lst[j]\n    return lst\n\n\nprint(bubble_sort([5, 4, 3, 2, 1]))\n```\n</details>\n\n## Using BLAS\n\nBLAS library can be integrated to further accelerate matrix multiplication. However, in some cases, using BLAS may cause performance degradation. Whether to turn on BLAS should depend on the benchmarking result.\n\n**Accelerate Framework**\n\nAccelerate Framework is automatically enabled on macOS. To disable it, add the CMake flag `-DGGML_NO_ACCELERATE=ON`.\n\n**OpenBLAS**\n\nOpenBLAS provides acceleration on CPU. Add the CMake flag `-DGGML_OPENBLAS=ON` to enable it.\n```sh\ncmake -B build -DGGML_OPENBLAS=ON && cmake --build build -j\n```\n\n**CUDA**\n\nCUDA accelerates model inference on NVIDIA GPU. Add the CMake flag `-DGGML_CUDA=ON` to enable it.\n```sh\ncmake -B build -DGGML_CUDA=ON && cmake --build build -j\n```\n\nBy default, all kernels will be compiled for all possible CUDA architectures and it takes some time. To run on a specific type of device, you may specify `CMAKE_CUDA_ARCHITECTURES` to speed up the nvcc compilation. For example:\n```sh\ncmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=\"80\"       # for A100\ncmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=\"70;75\"    # compatible with both V100 and T4\n```\n\nTo find out the CUDA architecture of your GPU device, see [Your GPU Compute Capability](https://developer.nvidia.com/cuda-gpus).\n\n**Metal**\n\nMPS (Metal Performance Shaders) allows computation to run on powerful Apple Silicon GPU. Add the CMake flag `-DGGML_METAL=ON` to enable it.\n```sh\ncmake -B build -DGGML_METAL=ON && cmake --build build -j\n```\n\n## Python Binding\n\nThe Python binding provides high-level `chat` and `stream_chat` interface similar to the original Hugging Face ChatGLM(2)-6B.\n\n**Installation**\n\nInstall from PyPI (recommended): will trigger compilation on your platform.\n```sh\npip install -U chatglm-cpp\n```\n\nTo enable CUDA on NVIDIA GPU:\n```sh\nCMAKE_ARGS=\"-DGGML_CUDA=ON\" pip install -U chatglm-cpp\n```\n\nTo enable Metal on Apple silicon devices:\n```sh\nCMAKE_ARGS=\"-DGGML_METAL=ON\" pip install -U chatglm-cpp\n```\n\nYou may also install from source. Add the corresponding `CMAKE_ARGS` for acceleration.\n```sh\n# install from the latest source hosted on GitHub\npip install git+https://github.com/li-plus/chatglm.cpp.git@main\n# or install from your local source after git cloning the repo\npip install .\n```\n\nPre-built wheels for CPU backend on Linux / MacOS / Windows are published on [release](https://github.com/li-plus/chatglm.cpp/releases). For CUDA / Metal backends, please compile from source code or source distribution.\n\n**Using Pre-converted GGML Models**\n\nHere is a simple demo that uses `chatglm_cpp.Pipeline` to load the GGML model and chat with it. First enter the examples folder (`cd examples`) and launch a Python interactive shell:\n```python\n>>> import chatglm_cpp\n>>> \n>>> pipeline = chatglm_cpp.Pipeline(\"../models/chatglm-ggml.bin\")\n>>> pipeline.chat([chatglm_cpp.ChatMessage(role=\"user\", content=\"‰Ω†Â•Ω\")])\nChatMessage(role=\"assistant\", content=\"‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\", tool_calls=[])\n```\n\nTo chat in stream, run the below Python example:\n```sh\npython3 cli_demo.py -m ../models/chatglm-ggml.bin -i\n```\n\nLaunch a web demo to chat in your browser:\n```sh\npython3 web_demo.py -m ../models/chatglm-ggml.bin\n```\n\n![web_demo](docs/web_demo.jpg)\n\nFor other models:\n\n<details open>\n<summary>ChatGLM2-6B</summary>\n\n```sh\npython3 cli_demo.py -m ../models/chatglm2-ggml.bin -p ‰Ω†Â•Ω --temp 0.8 --top_p 0.8  # CLI demo\npython3 web_demo.py -m ../models/chatglm2-ggml.bin --temp 0.8 --top_p 0.8  # web demo\n```\n</details>\n\n<details open>\n<summary>ChatGLM3-6B</summary>\n\n**CLI Demo**\n\nChat mode:\n```sh\npython3 cli_demo.py -m ../models/chatglm3-ggml.bin -p ‰Ω†Â•Ω --temp 0.8 --top_p 0.8\n```\n\nFunction call:\n```sh\npython3 cli_demo.py -m ../models/chatglm3-ggml.bin --temp 0.8 --top_p 0.8 --sp system/function_call.txt -i\n```\n\nCode interpreter:\n```sh\npython3 cli_demo.py -m ../models/chatglm3-ggml.bin --temp 0.8 --top_p 0.8 --sp system/code_interpreter.txt -i\n```\n\n**Web Demo**\n\nInstall Python dependencies and the IPython kernel for code interpreter.\n```sh\npip install streamlit jupyter_client ipython ipykernel\nipython kernel install --name chatglm3-demo --user\n```\n\nLaunch the web demo:\n```sh\nstreamlit run chatglm3_demo.py\n```\n\n| Function Call               | Code Interpreter               |\n|-----------------------------|--------------------------------|\n| ![](docs/function_call.png) | ![](docs/code_interpreter.png) |\n\n</details>\n\n<details open>\n<summary>ChatGLM4-9B</summary>\n\nChat mode:\n```sh\npython3 cli_demo.py -m ../models/chatglm4-ggml.bin -p ‰Ω†Â•Ω --temp 0.8 --top_p 0.8\n```\n</details>\n\n<details open>\n<summary>ChatGLM4V-9B</summary>\n\nChat mode:\n```sh\npython3 cli_demo.py -m ../models/chatglm4v-ggml.bin --image 03-Confusing-Pictures.jpg -p \"ËøôÂº†ÂõæÁâáÊúâ‰ªÄ‰πà‰∏çÂØªÂ∏∏‰πãÂ§Ñ\" --temp 0\n```\n</details>\n\n<details>\n<summary>CodeGeeX2</summary>\n\n```sh\n# CLI demo\npython3 cli_demo.py -m ../models/codegeex2-ggml.bin --temp 0 --mode generate -p \"\\\n# language: Python\n# write a bubble sort function\n\"\n# web demo\npython3 web_demo.py -m ../models/codegeex2-ggml.bin --temp 0 --max_length 512 --mode generate --plain\n```\n</details>\n\n**Converting Hugging Face LLMs at Runtime**\n\nSometimes it might be inconvenient to convert and save the intermediate GGML models beforehand. Here is an option to directly load from the original Hugging Face model, quantize it into GGML models in a minute, and start serving. All you need is to replace the GGML model path with the Hugging Face model name or path.\n```python\n>>> import chatglm_cpp\n>>> \n>>> pipeline = chatglm_cpp.Pipeline(\"THUDM/chatglm-6b\", dtype=\"q4_0\")\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:10<00:00,  1.27s/it]\nProcessing model states: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 339/339 [00:23<00:00, 14.73it/s]\n...\n>>> pipeline.chat([chatglm_cpp.ChatMessage(role=\"user\", content=\"‰Ω†Â•Ω\")])\nChatMessage(role=\"assistant\", content=\"‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\", tool_calls=[])\n```\n\nLikewise, replace the GGML model path with Hugging Face model in any example script, and it just works. For example:\n```sh\npython3 cli_demo.py -m THUDM/chatglm-6b -p ‰Ω†Â•Ω -i\n```\n\n## API Server\n\nWe support various kinds of API servers to integrate with popular frontends. Extra dependencies can be installed by:\n```sh\npip install 'chatglm-cpp[api]'\n```\nRemember to add the corresponding `CMAKE_ARGS` to enable acceleration.\n\n**LangChain API**\n\nStart the api server for LangChain:\n```sh\nMODEL=./models/chatglm2-ggml.bin uvicorn chatglm_cpp.langchain_api:app --host 127.0.0.1 --port 8000\n```\n\nTest the api endpoint with `curl`:\n```sh\ncurl http://127.0.0.1:8000 -H 'Content-Type: application/json' -d '{\"prompt\": \"‰Ω†Â•Ω\"}'\n```\n\nRun with LangChain:\n```python\n>>> from langchain.llms import ChatGLM\n>>> \n>>> llm = ChatGLM(endpoint_url=\"http://127.0.0.1:8000\")\n>>> llm.predict(\"‰Ω†Â•Ω\")\n'‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM2-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ'\n```\n\nFor more options, please refer to [examples/langchain_client.py](examples/langchain_client.py) and [LangChain ChatGLM Integration](https://python.langchain.com/docs/integrations/llms/chatglm).\n\n**OpenAI API**\n\nStart an API server compatible with [OpenAI chat completions protocol](https://platform.openai.com/docs/api-reference/chat):\n```sh\nMODEL=./models/chatglm3-ggml.bin uvicorn chatglm_cpp.openai_api:app --host 127.0.0.1 --port 8000\n```\n\nTest your endpoint with `curl`:\n```sh\ncurl http://127.0.0.1:8000/v1/chat/completions -H 'Content-Type: application/json' \\\n    -d '{\"messages\": [{\"role\": \"user\", \"content\": \"‰Ω†Â•Ω\"}]}'\n```\n\nUse the OpenAI client to chat with your model:\n```python\n>>> from openai import OpenAI\n>>> \n>>> client = OpenAI(base_url=\"http://127.0.0.1:8000/v1\")\n>>> response = client.chat.completions.create(model=\"default-model\", messages=[{\"role\": \"user\", \"content\": \"‰Ω†Â•Ω\"}])\n>>> response.choices[0].message.content\n'‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM3-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ'\n```\n\nFor stream response, check out the example client script:\n```sh\npython3 examples/openai_client.py --base_url http://127.0.0.1:8000/v1 --stream --prompt ‰Ω†Â•Ω\n```\n\nTool calling is also supported:\n```sh\npython3 examples/openai_client.py --base_url http://127.0.0.1:8000/v1 --tool_call --prompt ‰∏äÊµ∑Â§©Ê∞îÊÄé‰πàÊ†∑\n```\n\nRequest GLM4V with image inputs:\n```sh\n# request with local image file\npython3 examples/openai_client.py --base_url http://127.0.0.1:8000/v1 --prompt \"ÊèèËø∞ËøôÂº†ÂõæÁâá\" \\\n    --image examples/03-Confusing-Pictures.jpg --temp 0\n# request with image url\npython3 examples/openai_client.py --base_url http://127.0.0.1:8000/v1 --prompt \"ÊèèËø∞ËøôÂº†ÂõæÁâá\" \\\n    --image https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg --temp 0\n```\n\nWith this API server as backend, ChatGLM.cpp models can be seamlessly integrated into any frontend that uses OpenAI-style API, including [mckaywrigley/chatbot-ui](https://github.com/mckaywrigley/chatbot-ui), [fuergaosi233/wechat-chatgpt](https://github.com/fuergaosi233/wechat-chatgpt), [Yidadaa/ChatGPT-Next-Web](https://github.com/Yidadaa/ChatGPT-Next-Web), and more.\n\n## Using Docker\n\n**Option 1: Building Locally**\n\nBuilding docker image locally and start a container to run inference on CPU:\n```sh\ndocker build . --network=host -t chatglm.cpp\n# cpp demo\ndocker run -it --rm -v $PWD/models:/chatglm.cpp/models chatglm.cpp ./build/bin/main -m models/chatglm-ggml.bin -p \"‰Ω†Â•Ω\"\n# python demo\ndocker run -it --rm -v $PWD/models:/chatglm.cpp/models chatglm.cpp python3 examples/cli_demo.py -m models/chatglm-ggml.bin -p \"‰Ω†Â•Ω\"\n# langchain api server\ndocker run -it --rm -v $PWD/models:/chatglm.cpp/models -p 8000:8000 -e MODEL=models/chatglm-ggml.bin chatglm.cpp \\\n    uvicorn chatglm_cpp.langchain_api:app --host 0.0.0.0 --port 8000\n# openai api server\ndocker run -it --rm -v $PWD/models:/chatglm.cpp/models -p 8000:8000 -e MODEL=models/chatglm-ggml.bin chatglm.cpp \\\n    uvicorn chatglm_cpp.openai_api:app --host 0.0.0.0 --port 8000\n```\n\nFor CUDA support, make sure [nvidia-docker](https://github.com/NVIDIA/nvidia-docker) is installed. Then run:\n```sh\ndocker build . --network=host -t chatglm.cpp-cuda \\\n    --build-arg BASE_IMAGE=nvidia/cuda:12.2.0-devel-ubuntu20.04 \\\n    --build-arg CMAKE_ARGS=\"-DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=80\"\ndocker run -it --rm --gpus all -v $PWD/models:/chatglm.cpp/models chatglm.cpp-cuda \\\n    ./build/bin/main -m models/chatglm-ggml.bin -p \"‰Ω†Â•Ω\"\n```\n\n**Option 2: Using Pre-built Image**\n\nThe pre-built image for CPU inference is published on both [Docker Hub](https://hub.docker.com/repository/docker/liplusx/chatglm.cpp) and [GitHub Container Registry (GHCR)](https://github.com/li-plus/chatglm.cpp/pkgs/container/chatglm.cpp).\n\nTo pull from Docker Hub and run demo:\n```sh\ndocker run -it --rm -v $PWD/models:/chatglm.cpp/models liplusx/chatglm.cpp:main \\\n    ./build/bin/main -m models/chatglm-ggml.bin -p \"‰Ω†Â•Ω\"\n```\n\nTo pull from GHCR and run demo:\n```sh\ndocker run -it --rm -v $PWD/models:/chatglm.cpp/models ghcr.io/li-plus/chatglm.cpp:main \\\n    ./build/bin/main -m models/chatglm-ggml.bin -p \"‰Ω†Â•Ω\"\n```\n\nPython demo and API servers are also supported in pre-built image. Use it in the same way as **Option 1**.\n\n## Performance\n\nEnvironment:\n* CPU backend performance is measured on a Linux server with Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz using 16 threads.\n* CUDA backend is measured on a V100-SXM2-32GB GPU using 1 thread.\n* MPS backend is measured on an Apple M2 Ultra device using 1 thread.\n\nChatGLM-6B:\n\n|                                | Q4_0  | Q4_1  | Q5_0  | Q5_1  | Q8_0  | F16   |\n|--------------------------------|-------|-------|-------|-------|-------|-------|\n| ms/token (CPU @ Platinum 8260) | 74    | 77    | 86    | 89    | 114   | 189   |\n| ms/token (CUDA @ V100 SXM2)    | 8.1   | 8.7   | 9.4   | 9.5   | 12.0  | 19.1  |\n| ms/token (MPS @ M2 Ultra)      | 11.5  | 12.3  | N/A   | N/A   | 16.1  | 24.4  |\n| file size                      | 3.3G  | 3.7G  | 4.0G  | 4.4G  | 6.2G  | 12G   |\n| mem usage                      | 4.0G  | 4.4G  | 4.7G  | 5.1G  | 6.9G  | 13G   |\n\nChatGLM2-6B / ChatGLM3-6B / CodeGeeX2:\n\n|                                | Q4_0  | Q4_1  | Q5_0  | Q5_1  | Q8_0  | F16   |\n|--------------------------------|-------|-------|-------|-------|-------|-------|\n| ms/token (CPU @ Platinum 8260) | 64    | 71    | 79    | 83    | 106   | 189   |\n| ms/token (CUDA @ V100 SXM2)    | 7.9   | 8.3   | 9.2   | 9.2   | 11.7  | 18.5  |\n| ms/token (MPS @ M2 Ultra)      | 10.0  | 10.8  | N/A   | N/A   | 14.5  | 22.2  |\n| file size                      | 3.3G  | 3.7G  | 4.0G  | 4.4G  | 6.2G  | 12G   |\n| mem usage                      | 3.4G  | 3.8G  | 4.1G  | 4.5G  | 6.2G  | 12G   |\n\nChatGLM4-9B:\n\n|                                | Q4_0 | Q4_1 | Q5_0 | Q5_1 | Q8_0 | F16  |\n|--------------------------------|------|------|------|------|------|------|\n| ms/token (CPU @ Platinum 8260) | 105  | 105  | 122  | 134  | 158  | 279  |\n| ms/token (CUDA @ V100 SXM2)    | 12.1 | 12.5 | 13.8 | 13.9 | 17.7 | 27.7 |\n| ms/token (MPS @ M2 Ultra)      | 14.4 | 15.3 | 19.6 | 20.1 | 20.7 | 32.4 |\n| file size                      | 5.0G | 5.5G | 6.1G | 6.6G | 9.4G | 18G  |\n\n## Model Quality\n\nWe measure model quality by evaluating the perplexity over the WikiText-2 test dataset, following the strided sliding window strategy in https://huggingface.co/docs/transformers/perplexity. Lower perplexity usually indicates a better model.\n\nDownload and unzip the dataset from [link](https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip). Measure the perplexity with a stride of 512 and max input length of 2048:\n```sh\n./build/bin/perplexity -m models/chatglm3-base-ggml.bin -f wikitext-2-raw/wiki.test.raw -s 512 -l 2048\n```\n\n|                         | Q4_0  | Q4_1  | Q5_0  | Q5_1  | Q8_0  | F16   |\n|-------------------------|-------|-------|-------|-------|-------|-------|\n| [ChatGLM3-6B-Base][1]   | 6.215 | 6.188 | 6.006 | 6.022 | 5.971 | 5.972 |\n| [ChatGLM4-9B-Base][2]   | 6.834 | 6.780 | 6.645 | 6.624 | 6.576 | 6.577 |\n\n[1]: https://huggingface.co/THUDM/chatglm3-6b-base\n[2]: https://huggingface.co/THUDM/glm-4-9b\n\n## Development\n\n**Unit Test & Benchmark**\n\nTo perform unit tests, add this CMake flag `-DCHATGLM_ENABLE_TESTING=ON` to enable testing. Recompile and run the unit test (including benchmark).\n```sh\nmkdir -p build && cd build\ncmake .. -DCHATGLM_ENABLE_TESTING=ON && make -j\n./bin/chatglm_test\n```\n\nFor benchmark only:\n```sh\n./bin/chatglm_test --gtest_filter='Benchmark.*'\n```\n\n**Lint**\n\nTo format the code, run `make lint` inside the `build` folder. You should have `clang-format`, `black` and `isort` pre-installed.\n\n**Performance**\n\nTo detect the performance bottleneck, add the CMake flag `-DGGML_PERF=ON`:\n```sh\ncmake .. -DGGML_PERF=ON && make -j\n```\nThis will print timing for each graph operation when running the model.\n\n## Acknowledgements\n\n* This project is greatly inspired by [@ggerganov](https://github.com/ggerganov)'s [llama.cpp](https://github.com/ggerganov/llama.cpp) and is based on his NN library [ggml](https://github.com/ggerganov/ggml).\n* Thank [@THUDM](https://github.com/THUDM) for the amazing [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B), [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B), [ChatGLM3](https://github.com/THUDM/ChatGLM3) and [GLM-4](https://github.com/THUDM/GLM-4) and for releasing the model sources and checkpoints.\n"
        },
        {
          "name": "chatglm.cpp",
          "type": "blob",
          "size": 95.2216796875,
          "content": "#include \"chatglm.h\"\n#include <algorithm>\n#include <codecvt>\n#include <cstring>\n#include <fcntl.h>\n#include <fstream>\n#include <functional>\n#include <ggml-quants.h>\n#include <google/protobuf/stubs/strutil.h>\n#include <iomanip>\n#include <iostream>\n#include <locale>\n#include <numeric>\n#include <random>\n#include <regex>\n#include <string>\n#include <sys/stat.h>\n#include <thread>\n\n#define STB_IMAGE_IMPLEMENTATION\n#define STB_IMAGE_RESIZE2_IMPLEMENTATION\n#include <stb_image.h>\n#include <stb_image_resize2.h>\n\n#ifdef __has_include\n#if __has_include(<unistd.h>)\n#include <unistd.h>\n#if defined(_POSIX_MAPPED_FILES)\n#include <sys/mman.h>\n#endif\n#if defined(_POSIX_MEMLOCK_RANGE)\n#include <sys/resource.h>\n#endif\n#endif\n#endif\n\n#if defined(_WIN32)\n#define WIN32_LEAN_AND_MEAN\n#ifndef NOMINMAX\n#define NOMINMAX\n#endif\n#include <io.h>\n#include <stdio.h>\n#include <windows.h>\n#endif\n\n#ifdef GGML_USE_CUDA\n#include <ggml-cuda.h>\n#endif\n\n#ifdef GGML_USE_METAL\n#include <ggml-metal.h>\n#endif\n\nnamespace chatglm {\n\nstatic std::string shape_to_string(ggml_tensor *tensor) {\n    std::ostringstream oss;\n    oss << '[';\n    for (int i = ggml_n_dims(tensor) - 1; i >= 0; i--) {\n        oss << tensor->ne[i] << (i > 0 ? \", \" : \"\");\n    }\n    oss << ']';\n    return oss.str();\n}\n\nstatic std::string strides_to_string(ggml_tensor *tensor) {\n    std::ostringstream oss;\n    oss << '[';\n    for (int i = ggml_n_dims(tensor) - 1; i >= 0; i--) {\n        oss << tensor->nb[i] << (i > 0 ? \", \" : \"\");\n    }\n    oss << ']';\n    return oss.str();\n}\n\nstd::string to_string(ggml_tensor *tensor, bool with_data) {\n    std::vector<char> buf(ggml_nbytes(tensor));\n    if (tensor->buffer) {\n        ggml_backend_tensor_get(tensor, buf.data(), 0, buf.size());\n    } else {\n        memcpy(buf.data(), tensor->data, buf.size());\n    }\n\n    std::vector<float> float_buf(ggml_nelements(tensor));\n\n    switch (tensor->type) {\n    case GGML_TYPE_F32:\n        memcpy(float_buf.data(), buf.data(), buf.size());\n        break;\n    case GGML_TYPE_F16:\n        ggml_fp16_to_fp32_row((ggml_fp16_t *)buf.data(), float_buf.data(), ggml_nelements(tensor));\n        break;\n    case GGML_TYPE_Q4_0:\n        dequantize_row_q4_0((block_q4_0 *)buf.data(), float_buf.data(), ggml_nelements(tensor));\n        break;\n    case GGML_TYPE_Q4_1:\n        dequantize_row_q4_1((block_q4_1 *)buf.data(), float_buf.data(), ggml_nelements(tensor));\n        break;\n    case GGML_TYPE_Q5_0:\n        dequantize_row_q5_0((block_q5_0 *)buf.data(), float_buf.data(), ggml_nelements(tensor));\n        break;\n    case GGML_TYPE_Q5_1:\n        dequantize_row_q5_1((block_q5_1 *)buf.data(), float_buf.data(), ggml_nelements(tensor));\n        break;\n    case GGML_TYPE_Q8_0:\n        dequantize_row_q8_0((block_q8_0 *)buf.data(), float_buf.data(), ggml_nelements(tensor));\n        break;\n    default:\n        CHATGLM_THROW << \"Unsupported dtype \" << tensor->type;\n    }\n\n    std::ostringstream oss;\n    oss << \"ggml_tensor(\";\n\n    if (with_data) {\n        const int n_dims = ggml_n_dims(tensor);\n        if (n_dims > 3)\n            oss << \"[\";\n        for (int i3 = 0; i3 < tensor->ne[3]; i3++) {\n            if (n_dims > 2)\n                oss << (i3 > 0 ? \",\\n\\n[\" : \"[\");\n            for (int i2 = 0; i2 < tensor->ne[2]; i2++) {\n                if (n_dims > 1)\n                    oss << (i2 > 0 ? \",\\n\\n[\" : \"[\");\n                for (int i1 = 0; i1 < tensor->ne[1]; i1++) {\n                    oss << (i1 > 0 ? \",\\n[\" : \"[\");\n                    for (int i0 = 0; i0 < tensor->ne[0]; i0++) {\n                        oss << (i0 > 0 ? \", \" : \"\");\n                        const int i = ((i3 * tensor->ne[2] + i2) * tensor->ne[1] + i1) * tensor->ne[0] + i0;\n                        oss << std::setw(7) << std::fixed << std::setprecision(4) << float_buf[i];\n                    }\n                    oss << \"]\";\n                }\n                if (n_dims > 1)\n                    oss << \"]\";\n            }\n            if (n_dims > 2)\n                oss << \"]\";\n        }\n        if (n_dims > 3)\n            oss << \"]\";\n        oss << \", \";\n    }\n\n    oss << \"shape=\" << shape_to_string(tensor) << \", stride=\" << strides_to_string(tensor) << \")\";\n    return oss.str();\n}\n\nconst std::string ToolCallMessage::TYPE_FUNCTION = \"function\";\nconst std::string ToolCallMessage::TYPE_CODE = \"code\";\n\nconst std::string ChatMessage::ROLE_USER = \"user\";\nconst std::string ChatMessage::ROLE_ASSISTANT = \"assistant\";\nconst std::string ChatMessage::ROLE_SYSTEM = \"system\";\nconst std::string ChatMessage::ROLE_OBSERVATION = \"observation\";\n\nvoid BaseTokenizer::check_chat_messages(const std::vector<ChatMessage> &messages) {\n    std::string target_role = ChatMessage::ROLE_USER;\n    for (size_t i = 0; i < messages.size(); i++) {\n        if (messages[i].role != ChatMessage::ROLE_USER && messages[i].role != ChatMessage::ROLE_ASSISTANT) {\n            continue;\n        }\n        CHATGLM_CHECK(messages[i].role == target_role)\n            << \"expect messages[\" << i << \"].role to be \" << target_role << \", but got \" << messages[i].role;\n        target_role = (target_role == ChatMessage::ROLE_USER) ? ChatMessage::ROLE_ASSISTANT : ChatMessage::ROLE_USER;\n    }\n    CHATGLM_CHECK(target_role == ChatMessage::ROLE_ASSISTANT)\n        << \"expect last message role to be \" << ChatMessage::ROLE_USER << \", but got \" << ChatMessage::ROLE_ASSISTANT;\n}\n\nstd::vector<ChatMessage> BaseTokenizer::filter_user_assistant_messages(const std::vector<ChatMessage> &messages) {\n    std::vector<ChatMessage> user_assistant_messages;\n    user_assistant_messages.reserve(messages.size());\n    for (const auto &msg : messages) {\n        if (msg.role == ChatMessage::ROLE_USER || msg.role == ChatMessage::ROLE_ASSISTANT) {\n            user_assistant_messages.emplace_back(msg);\n        }\n    }\n    return user_assistant_messages;\n}\n\n// for debugging purpose\n[[maybe_unused]] static inline ggml_tensor *add_zero(ggml_context *ctx, ggml_tensor *tensor) {\n    ggml_tensor *zeros = ggml_new_tensor(ctx, GGML_TYPE_F32, ggml_n_dims(tensor), tensor->ne);\n    ggml_set_f32(zeros, 0);\n    ggml_tensor *out = ggml_add(ctx, tensor, zeros);\n    return out;\n}\n\n// ===== streamer =====\n\nvoid StreamerGroup::put(const std::vector<int> &output_ids) {\n    for (auto &streamer : streamers_) {\n        streamer->put(output_ids);\n    }\n}\n\nvoid StreamerGroup::end() {\n    for (auto &streamer : streamers_) {\n        streamer->end();\n    }\n}\n\n// reference: https://stackoverflow.com/questions/216823/how-to-trim-a-stdstring\n\n// trim from start (in place)\nstatic inline void ltrim(std::string &s) {\n    s.erase(s.begin(), std::find_if(s.begin(), s.end(), [](unsigned char ch) { return !std::isspace(ch); }));\n}\n\n// trim from end (in place)\nstatic inline void rtrim(std::string &s) {\n    s.erase(std::find_if(s.rbegin(), s.rend(), [](unsigned char ch) { return !std::isspace(ch); }).base(), s.end());\n}\n\n// trim from both ends (in place)\nstatic inline void trim(std::string &s) {\n    rtrim(s);\n    ltrim(s);\n}\n\nvoid TextStreamer::put(const std::vector<int> &output_ids) {\n    if (is_prompt_) {\n        // skip prompt\n        is_prompt_ = false;\n        return;\n    }\n\n    static const std::vector<char> puncts{',', '!', ':', ';', '?'};\n\n    token_cache_.insert(token_cache_.end(), output_ids.begin(), output_ids.end());\n    std::string text = tokenizer_->decode(token_cache_);\n    if (is_first_line_) {\n        ltrim(text);\n    }\n    if (text.empty()) {\n        return;\n    }\n\n    std::string printable_text;\n    if (text.back() == '\\n') {\n        // flush the cache after newline\n        printable_text = text.substr(print_len_);\n        is_first_line_ = false;\n        token_cache_.clear();\n        print_len_ = 0;\n    } else if (std::find(puncts.begin(), puncts.end(), text.back()) != puncts.end()) {\n        // last symbol is a punctuation, hold on\n    } else if (text.size() >= 3 && text.compare(text.size() - 3, 3, \"ÔøΩ\") == 0) {\n        // ends with an incomplete token, hold on\n    } else {\n        printable_text = text.substr(print_len_);\n        print_len_ = text.size();\n    }\n\n    os_ << printable_text << std::flush;\n}\n\nvoid TextStreamer::end() {\n    std::string text = tokenizer_->decode(token_cache_);\n    if (is_first_line_) {\n        ltrim(text);\n    }\n    os_ << text.substr(print_len_) << std::endl;\n    is_prompt_ = true;\n    is_first_line_ = true;\n    token_cache_.clear();\n    print_len_ = 0;\n}\n\nvoid PerfStreamer::put(const std::vector<int> &output_ids) {\n    CHATGLM_CHECK(!output_ids.empty());\n    if (num_prompt_tokens_ == 0) {\n        // before prompt eval\n        start_us_ = ggml_time_us();\n        num_prompt_tokens_ = output_ids.size();\n    } else {\n        if (num_output_tokens_ == 0) {\n            // first new token\n            prompt_us_ = ggml_time_us();\n        }\n        num_output_tokens_ += output_ids.size();\n    }\n}\n\nvoid PerfStreamer::reset() {\n    start_us_ = prompt_us_ = end_us_ = 0;\n    num_prompt_tokens_ = num_output_tokens_ = 0;\n}\n\nstd::string PerfStreamer::to_string() const {\n    std::ostringstream oss;\n    oss << \"prompt time: \" << prompt_total_time_us() / 1000.f << \" ms / \" << num_prompt_tokens() << \" tokens (\"\n        << prompt_token_time_us() / 1000.f << \" ms/token)\\n\"\n        << \"output time: \" << output_total_time_us() / 1000.f << \" ms / \" << num_output_tokens() << \" tokens (\"\n        << output_token_time_us() / 1000.f << \" ms/token)\\n\"\n        << \"total time: \" << (prompt_total_time_us() + output_total_time_us()) / 1000.f << \" ms\";\n    return oss.str();\n}\n\n#ifdef _POSIX_MAPPED_FILES\nMappedFile::MappedFile(const std::string &path) {\n    int fd = open(path.c_str(), O_RDONLY);\n    CHATGLM_CHECK(fd > 0) << \"cannot open file \" << path << \": \" << strerror(errno);\n\n    struct stat sb;\n    CHATGLM_CHECK(fstat(fd, &sb) == 0) << strerror(errno);\n    size = sb.st_size;\n\n    data = (char *)mmap(nullptr, size, PROT_READ, MAP_SHARED, fd, 0);\n    CHATGLM_CHECK(data != MAP_FAILED) << strerror(errno);\n\n    CHATGLM_CHECK(close(fd) == 0) << strerror(errno);\n}\n\nMappedFile::~MappedFile() { CHATGLM_CHECK(munmap(data, size) == 0) << strerror(errno); }\n#elif defined(_WIN32)\nMappedFile::MappedFile(const std::string &path) {\n\n    int fd = open(path.c_str(), O_RDONLY);\n    CHATGLM_CHECK(fd > 0) << \"cannot open file \" << path << \": \" << strerror(errno);\n\n    struct _stat64 sb;\n    CHATGLM_CHECK(_fstat64(fd, &sb) == 0) << strerror(errno);\n    size = sb.st_size;\n\n    HANDLE hFile = (HANDLE)_get_osfhandle(fd);\n\n    HANDLE hMapping = CreateFileMappingA(hFile, NULL, PAGE_READONLY, 0, 0, NULL);\n    CHATGLM_CHECK(hMapping != NULL) << strerror(errno);\n\n    data = (char *)MapViewOfFile(hMapping, FILE_MAP_READ, 0, 0, 0);\n    CloseHandle(hMapping);\n\n    CHATGLM_CHECK(data != NULL) << strerror(errno);\n\n    CHATGLM_CHECK(close(fd) == 0) << strerror(errno);\n}\n\nMappedFile::~MappedFile() { CHATGLM_CHECK(UnmapViewOfFile(data)) << strerror(errno); }\n#endif\n\nvoid ModelLoader::seek(int64_t offset, int whence) {\n    if (whence == SEEK_SET) {\n        ptr = data + offset;\n    } else if (whence == SEEK_CUR) {\n        ptr += offset;\n    } else if (whence == SEEK_END) {\n        ptr = data + size + offset;\n    } else {\n        CHATGLM_THROW << \"invalid seek mode \" << whence;\n    }\n}\n\nstd::string ModelLoader::read_string(size_t length) {\n    std::string s(ptr, ptr + length);\n    ptr += length;\n    return s;\n}\n\nStateDict ModelLoader::read_state_dict() {\n    StateDict sd;\n    sd.ctx = make_unique_ggml_context(GGML_DEFAULT_GRAPH_SIZE * ggml_tensor_overhead(), nullptr, true);\n    sd.buf = unique_ggml_backend_buffer_t(ggml_backend_cpu_buffer_from_ptr(data, size));\n\n    // assume state dict is stored at the back of file\n    while (tell() < (int64_t)size) {\n        // tensor name\n        int name_size = read_basic<int>();\n        std::string weight_name = read_string(name_size);\n\n        // tensor shape\n        int64_t ne[4]{1, 1, 1, 1};\n        int ndim = read_basic<int>();\n        CHATGLM_CHECK(0 < ndim && ndim <= 4);\n        for (int i = ndim - 1; i >= 0; i--) {\n            ne[i] = read_basic<int>();\n        }\n\n        // tensor dtype\n        ggml_type dtype = (ggml_type)read_basic<int>();\n\n        // tensor data\n        ggml_tensor *tensor = ggml_new_tensor(sd.ctx.get(), dtype, ndim, ne);\n        constexpr int64_t MEM_ALIGNED = 16;\n        const int64_t data_offset = (tell() + (MEM_ALIGNED - 1)) & ~(MEM_ALIGNED - 1);\n        ggml_backend_tensor_alloc(sd.buf.get(), tensor, data + data_offset);\n        // tensor->data = data + data_offset;\n        seek(data_offset + ggml_nbytes(tensor), SEEK_SET);\n\n        // add to state dict\n        sd.kv.emplace(weight_name, tensor);\n    }\n    return sd;\n}\n\nImage Image::open(const std::string &path) {\n    int width, height, channels;\n    uint8_t *buffer = stbi_load(path.c_str(), &width, &height, &channels, 3);\n    CHATGLM_CHECK(channels == 3);\n    Image image(width, height, channels, buffer);\n    stbi_image_free(buffer);\n    return image;\n}\n\nImage Image::resize(size_t new_width, size_t new_height) const {\n    Image output(new_width, new_height, channels);\n    stbir_resize_uint8_srgb(pixels.data(), width, height, 0, output.pixels.data(), new_width, new_height, 0, STBIR_RGB);\n    return output;\n}\n\nModelContext::ModelContext()\n    : compute_meta(ggml_tensor_overhead() * (GGML_DEFAULT_GRAPH_SIZE * 4) + ggml_graph_overhead()),\n      ctx_w(make_unique_ggml_context(ggml_tensor_overhead() * GGML_DEFAULT_GRAPH_SIZE, nullptr, true)),\n      ctx_kv(make_unique_ggml_context(ggml_tensor_overhead() * GGML_DEFAULT_GRAPH_SIZE, nullptr, true)),\n      ctx_b(make_unique_ggml_context(compute_meta.size(), compute_meta.data(), true)),\n      gf(ggml_new_graph_custom(ctx_b.get(), GGML_DEFAULT_GRAPH_SIZE * 4, false)) {\n\n#if defined(GGML_USE_CUDA)\n    backend = unique_ggml_backend_t(ggml_backend_cuda_init(0));\n#elif defined(GGML_USE_METAL)\n    backend = unique_ggml_backend_t(ggml_backend_metal_init());\n#else\n    backend = unique_ggml_backend_t(ggml_backend_cpu_init());\n#endif\n    CHATGLM_CHECK(backend) << \"failed to initialize ggml backend\";\n\n    allocr = unique_ggml_gallocr_t(ggml_gallocr_new(ggml_backend_get_default_buffer_type(backend.get())));\n}\n\n// ===== modules =====\n\nggml_tensor *Embedding::forward(ModelContext *mctx, ggml_tensor *input) const {\n    ggml_tensor *output = ggml_get_rows(mctx->ctx_b.get(), weight, input);\n    return output;\n}\n\nggml_tensor *Linear::forward(ModelContext *mctx, ggml_tensor *input) const {\n    // input: [seqlen, in_features]\n    ggml_context *ctx = mctx->ctx_b.get();\n    ggml_tensor *output = ggml_mul_mat(ctx, weight, input); // [seqlen, out_features]\n    if (bias) {\n        output = ggml_add_inplace(ctx, output, bias);\n    }\n    return output;\n}\n\nggml_tensor *LayerNorm::forward(ModelContext *mctx, ggml_tensor *input) const {\n    // input: [seqlen, normalized_shape]\n    ggml_context *ctx = mctx->ctx_b.get();\n    ggml_tensor *output = ggml_norm(ctx, input, eps);\n    output = ggml_mul_inplace(ctx, output, weight);\n    output = ggml_add_inplace(ctx, output, bias);\n    return output;\n}\n\nggml_tensor *RMSNorm::forward(ModelContext *mctx, ggml_tensor *input) const {\n    ggml_context *ctx = mctx->ctx_b.get();\n    ggml_tensor *output = ggml_rms_norm(ctx, input, eps);\n    output = ggml_mul_inplace(ctx, output, weight);\n    return output;\n}\n\nstatic ggml_tensor *apply_activation_inplace(ggml_context *ctx, ggml_tensor *hidden_states, ActivationType hidden_act) {\n    switch (hidden_act) {\n    case ActivationType::GELU:\n        return ggml_gelu_inplace(ctx, hidden_states);\n    case ActivationType::SILU:\n        return ggml_silu_inplace(ctx, hidden_states);\n    default:\n        CHATGLM_THROW << \"Unknown activation type \" << (int)hidden_act;\n    }\n}\n\nggml_tensor *BasicMLP::forward(ModelContext *mctx, ggml_tensor *hidden_states) const {\n    ggml_context *ctx = mctx->ctx_b.get();\n    hidden_states = dense_h_to_4h.forward(mctx, hidden_states);\n    hidden_states = apply_activation_inplace(ctx, hidden_states, hidden_act);\n    hidden_states = dense_4h_to_h.forward(mctx, hidden_states);\n    return hidden_states;\n}\n\nggml_tensor *BasicGLU::forward(ModelContext *mctx, ggml_tensor *hidden_states) const {\n    ggml_context *ctx = mctx->ctx_b.get();\n    ggml_tensor *gate = gate_proj.forward(mctx, hidden_states);\n    gate = apply_activation_inplace(ctx, gate, hidden_act);\n    hidden_states = up_proj.forward(mctx, hidden_states);\n    hidden_states = ggml_mul_inplace(ctx, hidden_states, gate);\n    hidden_states = down_proj.forward(mctx, hidden_states);\n    return hidden_states;\n}\n\n// Adapted from https://github.com/ggerganov/llama.cpp/blob/master/common/common.cpp\nstatic int get_num_physical_cores() {\n    unsigned int n_threads = std::thread::hardware_concurrency();\n    return n_threads > 0 ? (n_threads <= 4 ? n_threads : n_threads / 2) : 4;\n}\n\nstatic void set_default_num_threads(ggml_backend_t backend, int num_tokens) {\n    int n_threads = 1;\n    if (ggml_backend_is_cpu(backend)) {\n        if (num_tokens > 1) {\n            // context\n            n_threads = get_num_physical_cores();\n        } else {\n            // decode\n            n_threads = std::min(get_num_physical_cores(), 16);\n        }\n    }\n    if (num_tokens >= 32 && ggml_cpu_has_blas() && !ggml_cpu_has_gpublas()) {\n        // BLAS is enabled\n        n_threads = std::min(4, n_threads);\n    }\n\n    if (ggml_backend_is_cpu(backend)) {\n        ggml_backend_cpu_set_n_threads(backend, n_threads);\n    }\n\n#ifdef GGML_USE_METAL\n    if (ggml_backend_is_metal(backend)) {\n        ggml_backend_metal_set_n_cb(backend, n_threads);\n    }\n#endif\n}\n\nstd::string to_string(ModelType model_type) {\n    static const std::unordered_map<ModelType, std::string> m{{ModelType::CHATGLM, \"ChatGLM\"},\n                                                              {ModelType::CHATGLM2, \"ChatGLM2\"},\n                                                              {ModelType::CHATGLM3, \"ChatGLM3\"},\n                                                              {ModelType::CHATGLM4, \"ChatGLM4\"},\n                                                              {ModelType::CHATGLM4V, \"ChatGLM4V\"}};\n    return m.at(model_type);\n}\n\nstatic ggml_tensor *apply_rotary_emb_basic(ModelContext *mctx, ggml_tensor *layer, ggml_tensor *position_ids,\n                                           RopeType rope_type, float rope_theta) {\n    // tensor a (activation) is of shape [s, #h, d]\n    // tensor b (position_ids) is of shape [s]\n    ggml_context *ctx = mctx->ctx_b.get();\n    if (ggml_cpu_has_cuda() && !ggml_is_contiguous(layer)) {\n        layer = ggml_cont(ctx, layer);\n    }\n    const int head_size = layer->ne[0];\n    layer = ggml_rope_ext_inplace(ctx, layer, position_ids, nullptr, head_size, (int)rope_type, 0, rope_theta, 1.0f,\n                                  0.0f, 1.0f, 0.0f, 0.0f); // [s, #h, d]\n    return layer;\n}\n\nstatic ggml_tensor *apply_rotary_emb_glm(ModelContext *mctx, ggml_tensor *layer, ggml_tensor *position_ids) {\n    // tensor a (activation) is of shape [s, #h, d]\n    // tensor b (position_ids) is of shape [2 * s]\n    ggml_context *ctx = mctx->ctx_b.get();\n\n    const int head_size = layer->ne[0];\n    const int num_heads = layer->ne[1];\n    const int qlen = layer->ne[2];\n    const int rope_dim = head_size / 2;\n\n    ggml_tensor *b1 = ggml_view_1d(ctx, position_ids, qlen, 0);\n    ggml_tensor *b2 = ggml_view_1d(ctx, position_ids, qlen, qlen * ggml_element_size(position_ids));\n\n    ggml_tensor *a1 = ggml_view_3d(ctx, layer, head_size / 2, num_heads, qlen, layer->nb[1], layer->nb[2], 0);\n    ggml_tensor *a2 = ggml_view_3d(ctx, layer, head_size / 2, num_heads, qlen, layer->nb[1], layer->nb[2],\n                                   head_size / 2 * ggml_element_size(layer));\n\n    ggml_tensor *a1_rope = a1;\n    ggml_tensor *a2_rope = a2;\n\n    if (ggml_cpu_has_cuda()) {\n        a1_rope = ggml_cont(ctx, a1_rope);\n        a2_rope = ggml_cont(ctx, a2_rope);\n    }\n\n    a1_rope = ggml_rope_inplace(ctx, a1_rope, b1, rope_dim, (int)RopeType::NEOX); // [s, #h, d/2]\n    a2_rope = ggml_rope_inplace(ctx, a2_rope, b2, rope_dim, (int)RopeType::NEOX); // [s, #h, d/2]\n\n    if (ggml_cpu_has_cuda()) {\n        a1_rope = ggml_cpy(ctx, a1_rope, a1);\n        a2_rope = ggml_cpy(ctx, a2_rope, a2);\n    }\n\n    ggml_build_forward_expand(mctx->gf, a1_rope);\n    ggml_build_forward_expand(mctx->gf, a2_rope);\n\n    return layer;\n}\n\nstatic ggml_tensor *apply_rotary_emb_glm2(ModelContext *mctx, ggml_tensor *layer, ggml_tensor *position_ids,\n                                          float rope_theta) {\n    // NOTE: ChatGLM2 applies RoPE only on half of the features. The remaining half is skipped.\n    // layer: [s, #h, d], position_ids: [s]\n    ggml_context *ctx = mctx->ctx_b.get();\n\n    const int head_size = layer->ne[0];\n    const int rope_dim = head_size / 2;\n\n    ggml_tensor *half_layer_view =\n        ggml_view_3d(ctx, layer, rope_dim, layer->ne[1], layer->ne[2], layer->nb[1], layer->nb[2], 0);\n\n    ggml_tensor *half_layer = half_layer_view;\n    if (ggml_cpu_has_cuda()) {\n        half_layer = ggml_cont(ctx, half_layer);\n    }\n    ggml_tensor *roped_half_layer =\n        ggml_rope_ext_inplace(ctx, half_layer, position_ids, nullptr, rope_dim, (int)RopeType::GPTJ, 0, rope_theta,\n                              1.0f, 0.0f, 1.0f, 0.0f, 0.0f); // [s, #h, d]\n    if (ggml_cpu_has_cuda()) {\n        roped_half_layer = ggml_cpy(ctx, roped_half_layer, half_layer_view);\n    }\n    ggml_build_forward_expand(mctx->gf, roped_half_layer);\n\n    return layer;\n}\n\nstatic ggml_tensor *apply_rotary_emb(ModelContext *mctx, ggml_tensor *layer, ggml_tensor *position_ids,\n                                     RopeType rope_type, float rope_theta) {\n    switch (rope_type) {\n    case RopeType::GPTJ:\n    case RopeType::NEOX:\n        return apply_rotary_emb_basic(mctx, layer, position_ids, rope_type, rope_theta);\n    case RopeType::CHATGLM:\n        return apply_rotary_emb_glm(mctx, layer, position_ids);\n    case RopeType::CHATGLM2:\n        return apply_rotary_emb_glm2(mctx, layer, position_ids, rope_theta);\n    case RopeType::DISABLED:\n        return layer;\n    default:\n        CHATGLM_THROW << \"Unknown rope type \" << (int)rope_type;\n    }\n}\n\nggml_tensor *BasicAttention::forward(ModelContext *mctx, ggml_tensor *hidden_states, ggml_tensor *attention_mask,\n                                     ggml_tensor *position_ids, int n_past) const {\n    ggml_context *ctx = mctx->ctx_b.get();\n\n    const int hidden_size = hidden_states->ne[0];\n    const int qlen = hidden_states->ne[1];\n    const int head_size = hidden_size / num_attention_heads;\n    const int num_shared_q_heads = num_attention_heads / num_key_value_heads;\n\n    ggml_tensor *qkv = query_key_value.forward(mctx, hidden_states); // [sq, (#h + 2 * #kvh) * d]\n\n    // split mixed qkv into separate query, key and value\n    ggml_tensor *query_layer; // [s, #h, d]\n    ggml_tensor *key_layer;   // [s, #kvh, d]\n    ggml_tensor *value_layer; // [s, #kvh, d]\n\n    if (interleaved_qkv) {\n        CHATGLM_CHECK(num_shared_q_heads == 1) << \"interleaved qkv is not supported for GQA\";\n        query_layer = ggml_view_3d(ctx, qkv, head_size, num_attention_heads, qlen,\n                                   3 * head_size * ggml_element_size(qkv), qkv->nb[1], 0);\n        key_layer = ggml_view_3d(ctx, qkv, head_size, num_attention_heads, qlen, 3 * head_size * ggml_element_size(qkv),\n                                 qkv->nb[1], head_size * ggml_element_size(qkv));\n        value_layer =\n            ggml_view_3d(ctx, qkv, head_size, num_attention_heads, qlen, 3 * head_size * ggml_element_size(qkv),\n                         qkv->nb[1], 2 * head_size * ggml_element_size(qkv));\n    } else {\n        query_layer = ggml_view_3d(ctx, qkv, head_size, num_attention_heads, qlen, head_size * ggml_element_size(qkv),\n                                   qkv->nb[1], 0);\n        key_layer = ggml_view_3d(ctx, qkv, head_size, num_key_value_heads, qlen, head_size * ggml_element_size(qkv),\n                                 qkv->nb[1], hidden_size * ggml_element_size(qkv));\n        value_layer =\n            ggml_view_3d(ctx, qkv, head_size, num_key_value_heads, qlen, head_size * ggml_element_size(qkv), qkv->nb[1],\n                         (hidden_size + head_size * num_key_value_heads) * ggml_element_size(qkv));\n    }\n\n    query_layer = apply_rotary_emb(mctx, query_layer, position_ids, rope_type, rope_theta);\n    key_layer = apply_rotary_emb(mctx, key_layer, position_ids, rope_type, rope_theta);\n\n    query_layer = ggml_cont(ctx, ggml_permute(ctx, query_layer, 0, 2, 1, 3)); // [#h, s, d]\n    if (num_shared_q_heads > 1) {\n        query_layer = ggml_reshape_3d(ctx, query_layer, head_size, num_shared_q_heads * qlen,\n                                      num_key_value_heads); // [#kvh, (#h/#kvh) * s, d]\n    }\n\n    key_layer = ggml_permute(ctx, key_layer, 0, 2, 1, 3);     // [#kvh, s, d]\n    value_layer = ggml_permute(ctx, value_layer, 1, 2, 0, 3); // [#kvh, d, s]\n\n    ggml_tensor *context_layer;\n    if (k_cache && v_cache) {\n        // store key & value to cache\n        ggml_tensor *k_cache_view =\n            ggml_view_3d(ctx, k_cache, head_size, qlen, num_key_value_heads, k_cache->nb[1], k_cache->nb[2],\n                         (num_virtual_tokens + n_past) * k_cache->nb[1]); // [#kvh, s, d]\n        ggml_tensor *v_cache_view =\n            ggml_view_3d(ctx, v_cache, qlen, head_size, num_key_value_heads, v_cache->nb[1], v_cache->nb[2],\n                         (num_virtual_tokens + n_past) * v_cache->nb[0]); // [#kvh, d, s]\n        ggml_build_forward_expand(mctx->gf, ggml_cpy(ctx, key_layer, k_cache_view));\n        ggml_build_forward_expand(mctx->gf, ggml_cpy(ctx, value_layer, v_cache_view));\n\n        // concat key & value with past kv\n        key_layer = ggml_view_3d(ctx, k_cache, head_size, num_virtual_tokens + n_past + qlen, num_key_value_heads,\n                                 k_cache->nb[1], k_cache->nb[2],\n                                 0); // [#kvh, kvs, d]\n        value_layer = ggml_view_3d(ctx, v_cache, num_virtual_tokens + n_past + qlen, head_size, num_key_value_heads,\n                                   v_cache->nb[1], v_cache->nb[2],\n                                   0); // [#kvh, d, kvs]\n\n        // attention\n        query_layer = ggml_scale_inplace(ctx, query_layer, 1.f / std::sqrt(head_size));\n        ggml_tensor *attn_scores = ggml_mul_mat(ctx, key_layer, query_layer); // [#kvh, (#h/#kvh) * s, kvs]\n\n        if (n_past == 0) {\n            // build attention mask for context input\n            if (num_shared_q_heads > 1) {\n                attn_scores = ggml_reshape_3d(ctx, attn_scores, num_virtual_tokens + n_past + qlen, qlen,\n                                              num_attention_heads); // [#h, s, kvs]\n            }\n\n            if (attention_mask) {\n                attn_scores = ggml_add_inplace(ctx, attn_scores, attention_mask);\n            }\n\n            if (num_shared_q_heads > 1) {\n                attn_scores =\n                    ggml_reshape_3d(ctx, attn_scores, num_virtual_tokens + n_past + qlen, num_shared_q_heads * qlen,\n                                    num_key_value_heads); // [#kvh, (#h/#kvh) * s, kvs]\n            }\n        }\n\n        ggml_tensor *attn_probs = ggml_soft_max_inplace(ctx, attn_scores); // [#kvh, (#h/#kvh) * s, kvs]\n\n        context_layer = ggml_mul_mat(ctx, value_layer, attn_probs); // [#kvh, (#h/#kvh) * s, d]\n        if (num_shared_q_heads > 1) {\n            context_layer = ggml_reshape_3d(ctx, context_layer, head_size, qlen,\n                                            num_attention_heads); // [#h, s, d]\n        }\n        context_layer = ggml_cont(ctx, ggml_permute(ctx, context_layer, 0, 2, 1, 3)); // [s, #h, d]\n    } else {\n        // qkv must be correctly padded\n        key_layer = ggml_cast(ctx, key_layer, GGML_TYPE_F16);                                    // [#kvh, s, d]\n        value_layer = ggml_cast(ctx, ggml_permute(ctx, value_layer, 1, 0, 2, 3), GGML_TYPE_F16); // [#kvh, s, d]\n        context_layer = ggml_flash_attn_ext(ctx, query_layer, key_layer, value_layer, attention_mask,\n                                            1.f / std::sqrt(head_size), 0);\n        ggml_flash_attn_ext_set_prec(context_layer, GGML_PREC_F32);\n    }\n\n    context_layer = ggml_reshape_2d(ctx, context_layer, hidden_size, qlen); // [s, #h * d]\n\n    ggml_tensor *attn_output = dense.forward(mctx, context_layer);\n    return attn_output;\n}\n\nBaseModelForCausalLM::BaseModelForCausalLM(ModelConfig config)\n    : config(config), mctx_(std::make_unique<ModelContext>()) {}\n\nggml_tensor *BaseModelForCausalLM::forward_graph_compute(const std::vector<int> &input_ids,\n                                                         const std::optional<Image> &image, int n_past, int n_ctx,\n                                                         bool is_decoding) {\n    mctx_->ctx_b = make_unique_ggml_context(mctx_->compute_meta.size(), mctx_->compute_meta.data(), true);\n    mctx_->gf = ggml_new_graph_custom(mctx_->ctx_b.get(), GGML_DEFAULT_GRAPH_SIZE * 4, false);\n\n    const int qlen = (n_past == 0) ? input_ids.size() : 1;\n\n    ggml_tensor *curr_input_ids = ggml_new_tensor_1d(mctx_->ctx_b.get(), GGML_TYPE_I32, qlen);\n    ggml_set_name(curr_input_ids, \"input_ids\");\n    ggml_set_input(curr_input_ids);\n\n    ggml_tensor *curr_image = nullptr;\n    if (n_past == 0 && image) {\n        curr_image = ggml_new_tensor_3d(mctx_->ctx_b.get(), GGML_TYPE_F32, config.vision.image_size,\n                                        config.vision.image_size, config.vision.in_channels);\n        ggml_set_name(curr_image, \"image\");\n        ggml_set_input(curr_image);\n    }\n\n    ggml_tensor *lm_logits = forward(mctx_.get(), curr_input_ids, curr_image, input_ids, n_past, is_decoding);\n    ggml_set_output(lm_logits);\n\n    ggml_build_forward_expand(mctx_->gf, lm_logits);\n    CHATGLM_CHECK(ggml_gallocr_alloc_graph(mctx_->allocr.get(), mctx_->gf));\n\n    // TODO: move into set_graph_inputs\n    curr_input_ids = ggml_graph_get_tensor(mctx_->gf, \"input_ids\");\n    if (curr_input_ids) {\n        ggml_backend_tensor_set(curr_input_ids, input_ids.data() + input_ids.size() - qlen, 0, qlen * sizeof(int));\n    }\n    set_graph_inputs(input_ids, image, n_past, n_ctx);\n\n    set_default_num_threads(mctx_->backend.get(), qlen);\n    CHATGLM_CHECK(ggml_backend_graph_compute(mctx_->backend.get(), mctx_->gf) == GGML_STATUS_SUCCESS);\n\n#ifdef GGML_PERF\n    ggml_graph_print(mctx_->gf);\n#endif\n\n    return lm_logits;\n}\n\nint BaseModelForCausalLM::generate_next_token(const std::vector<int> &input_ids, const std::optional<Image> &image,\n                                              const GenerationConfig &gen_config, int n_past, int n_ctx) {\n    ggml_tensor *lm_logits = forward_graph_compute(input_ids, image, n_past, n_ctx, true);\n    CHATGLM_CHECK(ggml_n_dims(lm_logits) == 1);\n\n    int vocab_size = lm_logits->ne[0];\n    std::vector<float> next_token_logits(vocab_size);\n    ggml_backend_tensor_get(lm_logits, next_token_logits.data(), 0, vocab_size * sizeof(float));\n\n    // check nan\n    for (int i = 0; i < vocab_size; i++) {\n        CHATGLM_CHECK(std::isfinite(next_token_logits[i])) << \"nan/inf encountered at lm_logits[\" << i << \"]\";\n    }\n\n    // logits pre-process\n    if (gen_config.repetition_penalty != 1.f) {\n        sampling_repetition_penalty(next_token_logits.data(), next_token_logits.data() + vocab_size, input_ids,\n                                    gen_config.repetition_penalty);\n    }\n\n    int next_token_id;\n    if (gen_config.do_sample) {\n        // temperature sampling\n        if (gen_config.temperature > 0) {\n            sampling_temperature(next_token_logits.data(), next_token_logits.data() + vocab_size,\n                                 gen_config.temperature);\n        }\n\n        std::vector<TokenIdScore> token_scores(vocab_size);\n        for (int i = 0; i < vocab_size; i++) {\n            token_scores[i] = TokenIdScore(i, next_token_logits[i]);\n        }\n\n        // top_k sampling\n        if (0 < gen_config.top_k && gen_config.top_k < (int)token_scores.size()) {\n            sampling_top_k(token_scores.data(), token_scores.data() + gen_config.top_k,\n                           token_scores.data() + token_scores.size());\n            token_scores.resize(gen_config.top_k);\n        }\n\n        // top_p sampling\n        if (0.f < gen_config.top_p && gen_config.top_p < 1.f) {\n            auto pos = sampling_top_p(token_scores.data(), token_scores.data() + token_scores.size(), gen_config.top_p);\n            token_scores.resize(pos - token_scores.data());\n        }\n\n        // sample next token\n        sampling_softmax_inplace(token_scores.data(), token_scores.data() + token_scores.size());\n        for (size_t i = 0; i < token_scores.size(); i++) {\n            next_token_logits[i] = token_scores[i].score;\n        }\n\n        thread_local std::random_device rd;\n        thread_local std::mt19937 gen(rd());\n\n        std::discrete_distribution<> dist(next_token_logits.data(), next_token_logits.data() + token_scores.size());\n        next_token_id = token_scores[dist(gen)].id;\n    } else {\n        // greedy search\n        next_token_id =\n            std::max_element(next_token_logits.begin(), next_token_logits.end()) - next_token_logits.begin();\n    }\n\n    return next_token_id;\n}\n\nvoid BaseModelForCausalLM::sampling_repetition_penalty(float *first, float *last, const std::vector<int> &input_ids,\n                                                       float penalty) {\n    CHATGLM_CHECK(penalty > 0) << \"penalty must be a positive float, but got \" << penalty;\n    const float inv_penalty = 1.f / penalty;\n    const int vocab_size = last - first;\n    std::vector<bool> occurrence(vocab_size, false);\n    for (const int id : input_ids) {\n        if (!occurrence[id]) {\n            first[id] *= (first[id] > 0) ? inv_penalty : penalty;\n        }\n        occurrence[id] = true;\n    }\n}\n\nvoid BaseModelForCausalLM::sampling_temperature(float *first, float *last, float temp) {\n    const float inv_temp = 1.f / temp;\n    for (float *it = first; it != last; it++) {\n        *it *= inv_temp;\n    }\n}\n\nvoid BaseModelForCausalLM::sampling_top_k(TokenIdScore *first, TokenIdScore *kth, TokenIdScore *last) {\n    std::nth_element(first, kth, last, std::greater<TokenIdScore>());\n}\n\nTokenIdScore *BaseModelForCausalLM::sampling_top_p(TokenIdScore *first, TokenIdScore *last, float top_p) {\n    // fast top_p in expected O(n) time complexity\n    sampling_softmax_inplace(first, last);\n\n    while (first + 1 < last) {\n        const float pivot_score = (last - 1)->score; // use mid score?\n        TokenIdScore *mid =\n            std::partition(first, last - 1, [pivot_score](const TokenIdScore &x) { return x.score > pivot_score; });\n        std::swap(*mid, *(last - 1));\n\n        const float prefix_sum =\n            std::accumulate(first, mid, 0.f, [](float sum, const TokenIdScore &x) { return sum + x.score; });\n        if (prefix_sum >= top_p) {\n            last = mid;\n        } else if (prefix_sum + mid->score < top_p) {\n            first = mid + 1;\n            top_p -= prefix_sum + mid->score;\n        } else {\n            return mid + 1;\n        }\n    }\n    return last;\n}\n\nvoid BaseModelForCausalLM::sampling_softmax_inplace(TokenIdScore *first, TokenIdScore *last) {\n    float max_score = std::max_element(first, last)->score;\n    float sum = 0.f;\n    for (TokenIdScore *p = first; p != last; p++) {\n        float s = std::exp(p->score - max_score);\n        p->score = s;\n        sum += s;\n    }\n    float inv_sum = 1.f / sum;\n    for (TokenIdScore *p = first; p != last; p++) {\n        p->score *= inv_sum;\n    }\n}\n\nstd::vector<int> BaseModelForCausalLM::generate(const std::vector<int> &input_ids, const std::optional<Image> &image,\n                                                const GenerationConfig &gen_config, BaseStreamer *streamer) {\n    CHATGLM_CHECK(gen_config.max_length <= config.max_length)\n        << \"Requested max_length (\" << gen_config.max_length << \") exceeds pre-configured model max_length (\"\n        << config.max_length << \")\";\n\n    std::vector<int> output_ids;\n    output_ids.reserve(gen_config.max_length);\n    output_ids = input_ids;\n    if (streamer) {\n        streamer->put(input_ids);\n    }\n\n    int n_past = 0;\n    const int n_ctx = input_ids.size();\n    const int max_new_tokens = (gen_config.max_new_tokens > 0) ? gen_config.max_new_tokens : gen_config.max_length;\n\n    while ((int)output_ids.size() < std::min(gen_config.max_length, n_ctx + max_new_tokens)) {\n        int next_token_id = generate_next_token(output_ids, image, gen_config, n_past, n_ctx);\n\n        n_past = count_tokens(output_ids, image);\n        output_ids.emplace_back(next_token_id);\n\n        if (streamer) {\n            streamer->put({next_token_id});\n        }\n\n        if (next_token_id == config.eos_token_id ||\n            std::find(config.extra_eos_token_ids.begin(), config.extra_eos_token_ids.end(), next_token_id) !=\n                config.extra_eos_token_ids.end()) {\n            break;\n        }\n    }\n\n    if (streamer) {\n        streamer->end();\n    }\n\n    return output_ids;\n}\n\n// ===== ChatGLM-6B =====\n\nChatGLMTokenizer::ChatGLMTokenizer(std::string_view serialized_model_proto) {\n    const auto status = sp.LoadFromSerializedProto(serialized_model_proto);\n    CHATGLM_CHECK(status.ok()) << status.ToString();\n\n    bos_token_id = sp.PieceToId(\"<sop>\");\n    eos_token_id = sp.PieceToId(\"<eop>\");\n    mask_token_id = sp.PieceToId(\"[MASK]\");\n    gmask_token_id = sp.PieceToId(\"[gMASK]\");\n    pad_token_id = sp.PieceToId(\"<pad>\");\n}\n\nstd::vector<int> ChatGLMTokenizer::encode(const std::string &text, int max_length) const {\n    std::string input = preprocess(text);\n    std::vector<int> ids;\n    sp.Encode(input, &ids);\n    ids.insert(ids.end(), {gmask_token_id, bos_token_id});\n    if ((int)ids.size() > max_length) {\n        // sliding window: always take the last max_length tokens\n        ids.erase(ids.begin(), ids.end() - max_length);\n    }\n    return ids;\n}\n\nstd::vector<int> ChatGLMTokenizer::apply_chat_template(const std::vector<ChatMessage> &messages, int max_length) const {\n    std::string prompt = apply_chat_template_text(messages);\n    std::vector<int> input_ids = encode(prompt, max_length);\n    return input_ids;\n}\n\nstd::string ChatGLMTokenizer::apply_chat_template_text(const std::vector<ChatMessage> &messages) {\n    check_chat_messages(messages);\n    std::vector<ChatMessage> user_assistant_messages = filter_user_assistant_messages(messages);\n\n    std::ostringstream oss_prompt;\n    if (user_assistant_messages.size() == 1) {\n        oss_prompt << user_assistant_messages.front().content;\n    } else {\n        for (size_t i = 0; i < user_assistant_messages.size(); i += 2) {\n            oss_prompt << \"[Round \" << i / 2 << \"]\\nÈóÆÔºö\" << user_assistant_messages[i].content << \"\\nÁ≠îÔºö\";\n            if (i + 1 < user_assistant_messages.size()) {\n                oss_prompt << user_assistant_messages[i + 1].content << \"\\n\";\n            }\n        }\n    }\n    return oss_prompt.str();\n}\n\nstd::string ChatGLMTokenizer::decode(const std::vector<int> &ids, bool skip_special_tokens) const {\n    CHATGLM_CHECK(skip_special_tokens) << \"unimplemented\";\n    std::string text;\n    sp.Decode(ids, &text);\n    text = postprocess(text);\n    return text;\n}\n\nstatic std::string regex_replace(const std::string &input, const std::regex &regex,\n                                 std::function<std::string(const std::smatch &)> format) {\n    std::ostringstream oss;\n    int last_index = 0;\n    for (auto it = std::sregex_iterator(input.begin(), input.end(), regex); it != std::sregex_iterator(); it++) {\n        oss << it->prefix() << format(*it);\n        last_index = it->position() + it->length();\n    }\n    oss << input.substr(last_index);\n    return oss.str();\n}\n\nstd::string ChatGLMTokenizer::preprocess(const std::string &text) {\n    std::string output;\n\n    // newline token\n    {\n        static const std::regex newline_regex(\"\\n\");\n        output = std::regex_replace(text, newline_regex, \"<n>\");\n    }\n    // tab token\n    {\n        static const std::regex tab_regex(\"\\t\");\n        output = std::regex_replace(output, tab_regex, \"<|tab|>\");\n    }\n    // blank tokens\n    {\n        static const std::regex pattern(R\"([ ]{2,80})\");\n        output = regex_replace(output, pattern, [](const std::smatch &sm) {\n            std::ostringstream oss;\n            oss << \"<|blank_\" << sm.str().size() << \"|>\";\n            return oss.str();\n        });\n    }\n\n    return output;\n}\n\nstatic inline std::string replace_punctuations(const std::string &text) {\n    // reference: https://stackoverflow.com/questions/37989081/how-to-use-unicode-range-in-c-regex\n    static std::wstring_convert<std::codecvt_utf8_utf16<wchar_t>> converter;\n    static const std::vector<std::pair<std::wregex, std::wstring>> punct_map{\n        {std::wregex(converter.from_bytes(R\"(([\\u4e00-\\u9fff]),)\")), converter.from_bytes(\"$1Ôºå\")},\n        {std::wregex(converter.from_bytes(R\"(,([\\u4e00-\\u9fff]))\")), converter.from_bytes(\"Ôºå$1\")},\n        {std::wregex(converter.from_bytes(R\"(([\\u4e00-\\u9fff])!)\")), converter.from_bytes(\"$1ÔºÅ\")},\n        {std::wregex(converter.from_bytes(R\"(!([\\u4e00-\\u9fff]))\")), converter.from_bytes(\"ÔºÅ$1\")},\n        {std::wregex(converter.from_bytes(R\"(([\\u4e00-\\u9fff]):)\")), converter.from_bytes(\"$1Ôºö\")},\n        {std::wregex(converter.from_bytes(R\"(:([\\u4e00-\\u9fff]))\")), converter.from_bytes(\"Ôºö$1\")},\n        {std::wregex(converter.from_bytes(R\"(([\\u4e00-\\u9fff]);)\")), converter.from_bytes(\"$1Ôºõ\")},\n        {std::wregex(converter.from_bytes(R\"(;([\\u4e00-\\u9fff]))\")), converter.from_bytes(\"Ôºõ$1\")},\n        {std::wregex(converter.from_bytes(R\"(([\\u4e00-\\u9fff])\\?)\")), converter.from_bytes(\"$1Ôºü\")},\n        {std::wregex(converter.from_bytes(R\"(\\?([\\u4e00-\\u9fff]))\")), converter.from_bytes(\"Ôºü$1\")},\n    };\n    std::wstring w_output = converter.from_bytes(text);\n    for (const auto &punct_pair : punct_map) {\n        w_output = std::regex_replace(w_output, punct_pair.first, punct_pair.second);\n    }\n    std::string output = converter.to_bytes(w_output);\n    return output;\n}\n\nstd::string ChatGLMTokenizer::postprocess(const std::string &text) {\n    std::string output;\n\n    // newline token\n    {\n        static const std::regex pattern(R\"(<n>)\");\n        output = std::regex_replace(text, pattern, \"\\n\");\n    }\n    // tab token\n    {\n        static const std::regex pattern(R\"(<\\|tab\\|>)\");\n        output = std::regex_replace(output, pattern, \"\\t\");\n    }\n    // blank tokens\n    {\n        static const std::regex pattern(R\"(<\\|blank_(\\d+)\\|>)\");\n        output = regex_replace(output, pattern,\n                               [](const std::smatch &sm) { return std::string(std::stoi(sm[1].str()), ' '); });\n    }\n    // punctuations\n    output = replace_punctuations(output);\n\n    return output;\n}\n\nggml_tensor *GLMBlock::forward(ModelContext *mctx, ggml_tensor *hidden_states, ggml_tensor *attention_mask,\n                               ggml_tensor *position_ids, int n_past) const {\n    ggml_context *ctx = mctx->ctx_b.get();\n\n    ggml_tensor *attn_input = input_layernorm.forward(mctx, hidden_states);\n    ggml_tensor *attn_output = attention.forward(mctx, attn_input, attention_mask, position_ids, n_past);\n    ggml_build_forward_expand(mctx->gf, attn_output);\n    attn_input = ggml_scale_inplace(ctx, attn_input, alpha);\n    hidden_states = ggml_add_inplace(ctx, attn_input, attn_output);\n\n    ggml_tensor *mlp_input = post_attention_layernorm.forward(mctx, hidden_states);\n    ggml_tensor *mlp_output = mlp.forward(mctx, mlp_input);\n    ggml_build_forward_expand(mctx->gf, mlp_output);\n    mlp_input = ggml_scale_inplace(ctx, mlp_input, alpha);\n    ggml_tensor *output = ggml_add_inplace(ctx, mlp_input, mlp_output);\n\n    return output;\n}\n\nstatic void alloc_weight_context(ModelContext *mctx, const ggml_backend_buffer_t sd_buf) {\n    void *sd_buf_base = ggml_backend_buffer_get_base(sd_buf);\n    const size_t sd_buf_size = ggml_backend_buffer_get_size(sd_buf);\n    if (ggml_backend_is_cpu(mctx->backend.get())) {\n        mctx->buf_w = unique_ggml_backend_buffer_t(ggml_backend_cpu_buffer_from_ptr(sd_buf_base, sd_buf_size));\n    }\n#ifdef GGML_USE_METAL\n    else if (ggml_backend_is_metal(mctx->backend.get())) {\n        const size_t max_size = ggml_get_max_tensor_size(mctx->ctx_w.get());\n        mctx->buf_w =\n            unique_ggml_backend_buffer_t(ggml_backend_metal_buffer_from_ptr(sd_buf_base, sd_buf_size, max_size));\n    }\n#endif\n    else {\n        mctx->buf_w =\n            unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx->ctx_w.get(), mctx->backend.get()));\n    }\n}\n\nvoid ChatGLMForCausalLM::load_state_dict(const StateDict &sd) {\n    alloc_weight_context(mctx_.get(), sd.buf.get());\n\n    StateDict self_sd = state_dict();\n    for (auto &item : self_sd.kv) {\n        const std::string &name = item.first;\n        ggml_tensor *self_weight = item.second;\n        ggml_tensor *ckpt_weight = sd.kv.at(name);\n        CHATGLM_CHECK(ggml_nbytes(self_weight) == ggml_nbytes(ckpt_weight));\n        if (ggml_backend_is_cpu(mctx_->backend.get()) || ggml_cpu_has_metal()) {\n            ggml_backend_tensor_alloc(mctx_->buf_w.get(), self_weight, ckpt_weight->data);\n        } else {\n            ggml_backend_tensor_set(self_weight, ckpt_weight->data, 0, ggml_nbytes(self_weight));\n        }\n    }\n}\n\nvoid ChatGLMModel::set_graph_inputs(ggml_cgraph *gf, const std::vector<int> &input_ids,\n                                    const std::optional<Image> &image, int n_past, int n_ctx) const {\n    // attention_mask: [s, kvs] auto broadcast to [#h, s, kvs]\n    // semantic: attn_scores[:, :-1, -1] = -inf\n    const int qlen = input_ids.size() - n_past;\n    if (n_past == 0) {\n        ggml_tensor *attention_mask = ggml_graph_get_tensor(gf, \"attention_mask\");\n        const int kvlen = attention_mask->ne[0];\n        std::vector<float> attention_mask_buffer(qlen * kvlen, 0.f);\n        CHATGLM_CHECK(ggml_nbytes(attention_mask) == attention_mask_buffer.size() * sizeof(float));\n        for (int i = 0; i < qlen - 1; i++) {\n            attention_mask_buffer[i * kvlen + (kvlen - 1)] = -INFINITY;\n        }\n        ggml_backend_tensor_set(attention_mask, attention_mask_buffer.data(), 0,\n                                attention_mask_buffer.size() * sizeof(float));\n    }\n\n    // position_ids: [2 * qlen]\n    ggml_tensor *position_ids = ggml_graph_get_tensor(gf, \"position_ids\");\n    CHATGLM_CHECK(ggml_n_dims(position_ids) == 1 && position_ids->ne[0] == 2 * qlen)\n        << \"invalid position ids size \" << position_ids->ne[0];\n\n    std::vector<int> position_ids_buffer(position_ids->ne[0]);\n    for (int i = 0; i < qlen; i++) {\n        const int p = n_past + i;\n        position_ids_buffer[i] = std::min(p, n_ctx - 2);\n        position_ids_buffer[qlen + i] = std::max(p - (n_ctx - 2), 0);\n    }\n    ggml_backend_tensor_set(position_ids, position_ids_buffer.data(), 0, position_ids_buffer.size() * sizeof(int));\n}\n\nStateDict ChatGLMForCausalLM::state_dict() const {\n    StateDict sd;\n    sd.kv.emplace(\"transformer.word_embeddings.weight\", transformer.word_embeddings.weight);\n    for (int i = 0; i < config.num_hidden_layers; i++) {\n        std::string layer_prefix = \"transformer.layers.\" + std::to_string(i) + '.';\n        sd.kv.emplace(layer_prefix + \"input_layernorm.weight\", transformer.layers[i].input_layernorm.weight);\n        sd.kv.emplace(layer_prefix + \"input_layernorm.bias\", transformer.layers[i].input_layernorm.bias);\n        sd.kv.emplace(layer_prefix + \"attention.query_key_value.weight\",\n                      transformer.layers[i].attention.query_key_value.weight);\n        sd.kv.emplace(layer_prefix + \"attention.query_key_value.bias\",\n                      transformer.layers[i].attention.query_key_value.bias);\n        sd.kv.emplace(layer_prefix + \"attention.dense.weight\", transformer.layers[i].attention.dense.weight);\n        sd.kv.emplace(layer_prefix + \"attention.dense.bias\", transformer.layers[i].attention.dense.bias);\n        sd.kv.emplace(layer_prefix + \"post_attention_layernorm.weight\",\n                      transformer.layers[i].post_attention_layernorm.weight);\n        sd.kv.emplace(layer_prefix + \"post_attention_layernorm.bias\",\n                      transformer.layers[i].post_attention_layernorm.bias);\n        sd.kv.emplace(layer_prefix + \"mlp.dense_h_to_4h.weight\", transformer.layers[i].mlp.dense_h_to_4h.weight);\n        sd.kv.emplace(layer_prefix + \"mlp.dense_h_to_4h.bias\", transformer.layers[i].mlp.dense_h_to_4h.bias);\n        sd.kv.emplace(layer_prefix + \"mlp.dense_4h_to_h.weight\", transformer.layers[i].mlp.dense_4h_to_h.weight);\n        sd.kv.emplace(layer_prefix + \"mlp.dense_4h_to_h.bias\", transformer.layers[i].mlp.dense_4h_to_h.bias);\n    }\n    sd.kv.emplace(\"transformer.final_layernorm.weight\", transformer.final_layernorm.weight);\n    sd.kv.emplace(\"transformer.final_layernorm.bias\", transformer.final_layernorm.bias);\n    return sd;\n}\n\n// ===== ChatGLM2-6B =====\n\nChatGLM2Tokenizer::ChatGLM2Tokenizer(std::string_view serialized_model_proto) {\n    const auto status = sp.LoadFromSerializedProto(serialized_model_proto);\n    CHATGLM_CHECK(status.ok()) << status.ToString();\n\n    int special_id = sp.GetPieceSize();\n    mask_token_id = special_id++;\n    gmask_token_id = special_id++;\n    smask_token_id = special_id++;\n    sop_token_id = special_id++;\n    eop_token_id = special_id++;\n}\n\nstd::vector<int> ChatGLM2Tokenizer::encode(const std::string &text, int max_length) const {\n    std::vector<int> ids;\n    sp.Encode(text, &ids);\n    ids.insert(ids.begin(), {gmask_token_id, sop_token_id}); // special prefix\n    if ((int)ids.size() > max_length) {\n        // sliding window: drop the least recent history while keeping the two special prefix tokens\n        int num_drop = (int)ids.size() - max_length;\n        ids.erase(ids.begin() + 2, ids.begin() + 2 + num_drop);\n    }\n    return ids;\n}\n\nstd::string ChatGLM2Tokenizer::decode(const std::vector<int> &ids, bool skip_special_tokens) const {\n    CHATGLM_CHECK(skip_special_tokens) << \"unimplemented\";\n    // filter out special tokens\n    std::vector<int> normal_ids(ids);\n    normal_ids.erase(std::remove_if(normal_ids.begin(), normal_ids.end(), [this](int id) { return is_special_id(id); }),\n                     normal_ids.end());\n\n    std::string text;\n    sp.Decode(normal_ids, &text);\n    text = replace_punctuations(text);\n    return text;\n}\n\nstd::vector<int> ChatGLM2Tokenizer::apply_chat_template(const std::vector<ChatMessage> &messages,\n                                                        int max_length) const {\n    std::string prompt = apply_chat_template_text(messages);\n    std::vector<int> input_ids = encode(prompt, max_length);\n    return input_ids;\n}\n\nstd::string ChatGLM2Tokenizer::apply_chat_template_text(const std::vector<ChatMessage> &messages) {\n    check_chat_messages(messages);\n    std::vector<ChatMessage> user_assistant_messages = filter_user_assistant_messages(messages);\n\n    std::ostringstream oss_prompt;\n    for (size_t i = 0; i < user_assistant_messages.size(); i += 2) {\n        oss_prompt << \"[Round \" << i / 2 + 1 << \"]\\n\\nÈóÆÔºö\" << user_assistant_messages[i].content << \"\\n\\nÁ≠îÔºö\";\n        if (i < user_assistant_messages.size() - 1) {\n            oss_prompt << user_assistant_messages[i + 1].content << \"\\n\\n\";\n        }\n    }\n    return oss_prompt.str();\n}\n\nbool ChatGLM2Tokenizer::is_special_id(int id) const {\n    return id == mask_token_id || id == gmask_token_id || id == smask_token_id || id == sop_token_id ||\n           id == eop_token_id;\n}\n\nvoid ChatGLM2ForCausalLM::load_state_dict(const StateDict &sd) {\n    alloc_weight_context(mctx_.get(), sd.buf.get());\n\n    if (config.num_virtual_tokens > 0) {\n        ggml_tensor *past_key_values = sd.kv.at(\"past_key_values\");\n        load_prefix_cache(past_key_values);\n    }\n\n    auto self_sd = state_dict();\n    load_state_dict(mctx_.get(), self_sd, sd);\n}\n\nvoid ChatGLM2ForCausalLM::load_state_dict(ModelContext *mctx, StateDict &dst, const StateDict &src) {\n    for (auto it = src.kv.begin(); it != src.kv.end(); it++) {\n        const std::string &name = it->first;\n        ggml_tensor *ckpt_weight = it->second;\n\n        if (name == \"past_key_values\") {\n            continue;\n        }\n\n        size_t pos = name.rfind(\"mlp.dense_h_to_4h.weight\");\n        if (pos != std::string::npos) {\n            // split dense_h_to_4h to gate & up\n            std::string gate_name = name.substr(0, pos) + \"mlp.gate_proj.weight\";\n            ggml_tensor *gate_proj = dst.kv.at(gate_name);\n\n            std::string up_name = name.substr(0, pos) + \"mlp.up_proj.weight\";\n            ggml_tensor *up_proj = dst.kv.at(up_name);\n\n            CHATGLM_CHECK(ggml_nbytes(ckpt_weight) == ggml_nbytes(gate_proj) + ggml_nbytes(up_proj)) << name;\n\n            if (ggml_backend_is_cpu(mctx->backend.get()) || ggml_cpu_has_metal()) {\n                ggml_backend_tensor_alloc(mctx->buf_w.get(), gate_proj, ckpt_weight->data);\n                ggml_backend_tensor_alloc(mctx->buf_w.get(), up_proj,\n                                          (char *)ckpt_weight->data + ggml_nbytes(gate_proj));\n            } else {\n                ggml_backend_tensor_set(gate_proj, ckpt_weight->data, 0, ggml_nbytes(gate_proj));\n                ggml_backend_tensor_set(up_proj, (char *)ckpt_weight->data + ggml_nbytes(gate_proj), 0,\n                                        ggml_nbytes(up_proj));\n            }\n        } else {\n            // normal weight\n            ggml_tensor *self_weight = dst.kv.at(name);\n            CHATGLM_CHECK(ggml_nbytes(self_weight) == ggml_nbytes(ckpt_weight)) << name;\n            if (ggml_backend_is_cpu(mctx->backend.get()) || ggml_cpu_has_metal()) {\n                ggml_backend_tensor_alloc(mctx->buf_w.get(), self_weight, ckpt_weight->data);\n            } else {\n                ggml_backend_tensor_set(self_weight, ckpt_weight->data, 0, ggml_nbytes(self_weight));\n            }\n        }\n    }\n}\n\nvoid ChatGLM2Model::set_graph_inputs(ggml_cgraph *gf, const std::vector<int> &input_ids,\n                                     const std::optional<Image> &image, int n_past, int n_ctx) const {\n    ggml_tensor *position_ids = ggml_graph_get_tensor(gf, \"position_ids\");\n    const int qlen = input_ids.size() - n_past;\n    CHATGLM_CHECK(ggml_n_dims(position_ids) == 1 && position_ids->ne[0] == qlen)\n        << \"invalid position ids size \" << position_ids->ne[0];\n\n    std::vector<int> position_ids_buffer(position_ids->ne[0]);\n    std::iota(position_ids_buffer.begin(), position_ids_buffer.end(), n_past);\n    ggml_backend_tensor_set(position_ids, position_ids_buffer.data(), 0, position_ids_buffer.size() * sizeof(int));\n\n    ggml_tensor *attention_mask = ggml_graph_get_tensor(gf, \"attention_mask\");\n    if (attention_mask) {\n        const int kvlen = attention_mask->ne[0];\n        const int qlen = attention_mask->ne[1];\n        std::vector<float> mask_buf(qlen * kvlen);\n        for (int i = 0; i < qlen; i++) {\n            for (int j = 0; j < kvlen; j++) {\n                mask_buf[i * kvlen + j] = (i < j + qlen - kvlen) ? -INFINITY : 0.f;\n            }\n        }\n        ggml_backend_tensor_set(attention_mask, mask_buf.data(), 0, ggml_nbytes(attention_mask));\n    }\n}\n\nStateDict ChatGLM2ForCausalLM::state_dict() const {\n    StateDict sd;\n    sd.kv.emplace(\"transformer.embedding.word_embeddings.weight\", transformer.word_embeddings.weight);\n    for (int i = 0; i < config.num_hidden_layers; i++) {\n        std::string layer_prefix = \"transformer.encoder.layers.\" + std::to_string(i) + '.';\n        sd.kv.emplace(layer_prefix + \"input_layernorm.weight\", transformer.layers[i].input_layernorm.weight);\n        sd.kv.emplace(layer_prefix + \"self_attention.query_key_value.weight\",\n                      transformer.layers[i].attention.query_key_value.weight);\n        sd.kv.emplace(layer_prefix + \"self_attention.query_key_value.bias\",\n                      transformer.layers[i].attention.query_key_value.bias);\n        sd.kv.emplace(layer_prefix + \"self_attention.dense.weight\", transformer.layers[i].attention.dense.weight);\n        sd.kv.emplace(layer_prefix + \"post_attention_layernorm.weight\",\n                      transformer.layers[i].post_attention_layernorm.weight);\n        sd.kv.emplace(layer_prefix + \"mlp.gate_proj.weight\", transformer.layers[i].mlp.gate_proj.weight);\n        sd.kv.emplace(layer_prefix + \"mlp.up_proj.weight\", transformer.layers[i].mlp.up_proj.weight);\n        // for compatibility\n        sd.kv.emplace(layer_prefix + \"mlp.dense_4h_to_h.weight\", transformer.layers[i].mlp.down_proj.weight);\n    }\n    sd.kv.emplace(\"transformer.encoder.final_layernorm.weight\", transformer.final_layernorm.weight);\n    sd.kv.emplace(\"transformer.output_layer.weight\", lm_head.weight);\n    return sd;\n}\n\n// ===== ChatGLM3-6B =====\n\nChatGLM3Tokenizer::ChatGLM3Tokenizer(std::string_view serialized_model_proto) {\n    const auto status = sp.LoadFromSerializedProto(serialized_model_proto);\n    CHATGLM_CHECK(status.ok()) << status.ToString();\n\n    int special_id = sp.GetPieceSize();\n    mask_token_id = special_id++;\n    gmask_token_id = special_id++;\n    smask_token_id = special_id++;\n    sop_token_id = special_id++;\n    eop_token_id = special_id++;\n    system_token_id = special_id++;\n    user_token_id = special_id++;\n    assistant_token_id = special_id++;\n    observation_token_id = special_id++;\n\n    special_tokens = {\n        {\"[MASK]\", mask_token_id},\n        {\"[gMASK]\", gmask_token_id},\n        {\"[sMASK]\", smask_token_id},\n        {\"sop\", sop_token_id},\n        {\"eop\", eop_token_id},\n        {\"<|system|>\", system_token_id},\n        {\"<|user|>\", user_token_id},\n        {\"<|assistant|>\", assistant_token_id},\n        {\"<|observation|>\", observation_token_id},\n    };\n\n    for (const auto &item : special_tokens) {\n        index_special_tokens[item.second] = item.first;\n    }\n}\n\nstd::vector<int> ChatGLM3Tokenizer::encode(const std::string &text, int max_length) const {\n    std::vector<int> ids;\n    sp.Encode(text, &ids);\n    ids.insert(ids.begin(), {gmask_token_id, sop_token_id}); // special prefix\n    truncate(ids, max_length);\n    return ids;\n}\n\nstd::string ChatGLM3Tokenizer::decode(const std::vector<int> &ids, bool skip_special_tokens) const {\n    std::vector<std::string> pieces;\n    for (int id : ids) {\n        auto pos = index_special_tokens.find(id);\n        if (pos != index_special_tokens.end()) {\n            // special tokens\n            pieces.emplace_back(pos->second);\n        } else {\n            // normal tokens\n            pieces.emplace_back(sp.IdToPiece(id));\n        }\n    }\n\n    std::string text = sp.DecodePieces(pieces);\n\n    if (skip_special_tokens) {\n        text = remove_special_tokens(text);\n    }\n\n    return text;\n}\n\nstd::string ChatGLM3Tokenizer::remove_special_tokens(const std::string &text) {\n    // R\"(<\\|assistant\\|> interpreter)\"\n    // R\"(<\\|assistant\\|> interpre)\"\n    static const std::regex re(R\"(<\\|assistant\\|>|<\\|user\\|>|<\\|observation\\|>)\");\n    std::string output = std::regex_replace(text, re, \"\");\n    return output;\n}\n\nstd::vector<int> ChatGLM3Tokenizer::encode_single_message(const std::string &role, const std::string &content) const {\n    std::vector<int> input_ids;\n    input_ids.emplace_back(get_command(\"<|\" + role + \"|>\"));\n    // TODO: support metadata\n    std::vector<int> newline_ids;\n    sp.Encode(\"\\n\", &newline_ids);\n    input_ids.insert(input_ids.end(), newline_ids.begin(), newline_ids.end());\n    std::vector<int> content_ids;\n    sp.Encode(content, &content_ids);\n    input_ids.insert(input_ids.end(), content_ids.begin(), content_ids.end());\n    return input_ids;\n}\n\nstd::vector<int> ChatGLM3Tokenizer::apply_chat_template(const std::vector<ChatMessage> &messages,\n                                                        int max_length) const {\n    std::vector<int> input_ids{gmask_token_id, sop_token_id};\n    for (const auto &msg : messages) {\n        auto msg_ids = encode_single_message(msg.role, msg.content);\n        input_ids.insert(input_ids.end(), msg_ids.begin(), msg_ids.end());\n\n        // encode code block into a separate message\n        if (!msg.tool_calls.empty() && msg.tool_calls.front().type == ToolCallMessage::TYPE_CODE) {\n            auto code_ids = encode_single_message(msg.role, msg.tool_calls.front().code.input);\n            input_ids.insert(input_ids.end(), code_ids.begin(), code_ids.end());\n        }\n    }\n    input_ids.emplace_back(assistant_token_id);\n    truncate(input_ids, max_length);\n    return input_ids;\n}\n\nChatMessage ChatGLM3Tokenizer::decode_message(const std::vector<int> &ids) const {\n    ChatMessage message;\n    if (!ids.empty() && ids.back() == observation_token_id) {\n        // insert an <|assistant|> token before content to match possible interpreter delimiter\n        std::vector<int> full_ids{assistant_token_id};\n        full_ids.insert(full_ids.end(), ids.begin(), ids.end());\n\n        std::string output = decode(full_ids, false);\n        const std::string ci_delim = \"<|assistant|> interpreter\";\n        size_t ci_pos = output.find(ci_delim);\n        if (ci_pos != std::string::npos) {\n            // code interpreter\n            std::string chat_output = output.substr(0, ci_pos);\n            chat_output = remove_special_tokens(chat_output);\n            trim(chat_output);\n            std::string code_output = output.substr(ci_pos + ci_delim.size());\n            code_output = remove_special_tokens(code_output);\n            trim(code_output);\n            message = ChatMessage(ChatMessage::ROLE_ASSISTANT, std::move(chat_output), std::nullopt,\n                                  {ToolCallMessage(CodeMessage(std::move(code_output)))});\n        } else {\n            // tool call\n            output = remove_special_tokens(output);\n\n            // parse tool name\n            std::string tool_name = \"PARSE_ERROR\";\n            size_t pos = output.find('\\n');\n            if (pos != std::string::npos) {\n                // split tool name and args by 1st linebreak\n                tool_name = output.substr(0, pos);\n                trim(tool_name);\n                output.erase(0, pos + 1);\n            }\n\n            // post process output\n            trim(output);\n\n            // extract args\n            std::string tool_args = \"PARSE_ERROR\";\n            static const std::regex args_regex(R\"(```.*?\\n(.*?)\\n```)\");\n            std::smatch sm;\n            if (std::regex_search(output, sm, args_regex)) {\n                CHATGLM_CHECK(sm.size() == 2) << \"unexpected regex match results\";\n                tool_args = sm[1];\n            }\n\n            message = ChatMessage(ChatMessage::ROLE_ASSISTANT, std::move(output), std::nullopt,\n                                  {ToolCallMessage(FunctionMessage(std::move(tool_name), std::move(tool_args)))});\n        }\n    } else {\n        // conversation\n        message = BaseTokenizer::decode_message(ids);\n        trim(message.content); // strip leading linebreak in conversation mode\n    }\n    return message;\n}\n\nint ChatGLM3Tokenizer::get_command(const std::string &token) const {\n    auto pos = special_tokens.find(token);\n    CHATGLM_CHECK(pos != special_tokens.end()) << token << \" is not a special token\";\n    return pos->second;\n}\n\nbool ChatGLM3Tokenizer::is_special_id(int id) const { return index_special_tokens.count(id) > 0; }\n\nvoid ChatGLM3Tokenizer::truncate(std::vector<int> &ids, int max_length) {\n    if ((int)ids.size() > max_length) {\n        // sliding window: drop the least recent history while keeping the two special prefix tokens\n        int num_drop = (int)ids.size() - max_length;\n        ids.erase(ids.begin() + 2, ids.begin() + 2 + num_drop);\n    }\n}\n\n// ===== ChatGLM4-9B =====\n\nTiktokenCoreBPE::TiktokenCoreBPE(std::unordered_map<std::string, int> encoder,\n                                 std::unordered_map<std::string, int> special_tokens_encoder,\n                                 const std::string &pattern)\n    : regex(std::make_unique<RE2>(\"(\" + pattern + \")\")), encoder(std::move(encoder)),\n      special_tokens_encoder(std::move(special_tokens_encoder)) {\n    CHATGLM_CHECK(regex->ok()) << regex->error();\n    CHATGLM_CHECK(regex->NumberOfCapturingGroups() <= 2) << \"unimplemented\";\n\n    decoder.reserve(this->encoder.size());\n    for (const auto &[token, rank] : this->encoder) {\n        decoder.emplace(rank, token);\n    }\n\n    special_tokens_decoder.reserve(this->special_tokens_encoder.size());\n    for (const auto &[token, rank] : this->special_tokens_encoder) {\n        special_tokens_decoder.emplace(rank, token);\n    }\n}\n\nstd::vector<std::pair<size_t, int>> TiktokenCoreBPE::_byte_pair_merge(const std::unordered_map<std::string, int> &ranks,\n                                                                      const std::string &piece) {\n    using rank_t = int;\n\n    std::vector<std::pair<size_t, rank_t>> parts; // (start, rank)\n    parts.reserve(piece.length() + 1);\n\n    auto min_rank = std::make_pair<rank_t, size_t>(std::numeric_limits<rank_t>::max(),\n                                                   std::numeric_limits<size_t>::max()); // (rank, start)\n\n    for (size_t i = 0; i < piece.length() - 1; i++) {\n        rank_t rank = std::numeric_limits<rank_t>::max();\n        if (auto it = ranks.find(piece.substr(i, 2)); it != ranks.end()) {\n            rank = it->second;\n        }\n        if (rank < min_rank.first) {\n            min_rank = std::make_pair(rank, i);\n        }\n        parts.emplace_back(std::make_pair(i, rank));\n    }\n    parts.emplace_back(std::make_pair(piece.length() - 1, std::numeric_limits<rank_t>::max()));\n    parts.emplace_back(std::make_pair(piece.length(), std::numeric_limits<rank_t>::max()));\n\n    auto get_rank = [&piece, &ranks](const std::vector<std::pair<size_t, rank_t>> &parts, size_t i) {\n        if (i + 3 < parts.size()) {\n            size_t start = parts[i].first;\n            size_t end = parts[i + 3].first;\n            if (auto it = ranks.find(piece.substr(start, end - start)); it != ranks.end()) {\n                return it->second;\n            }\n        }\n        return std::numeric_limits<rank_t>::max();\n    };\n\n    while (min_rank.first != std::numeric_limits<rank_t>::max()) {\n        size_t i = min_rank.second;\n        if (i > 0) {\n            parts[i - 1].second = get_rank(parts, i - 1);\n        }\n        parts[i].second = get_rank(parts, i);\n        parts.erase(parts.begin() + i + 1);\n\n        min_rank = std::make_pair(std::numeric_limits<rank_t>::max(), std::numeric_limits<size_t>::max());\n        for (size_t i = 0; i < parts.size() - 1; i++) {\n            rank_t rank = parts[i].second;\n            if (rank < min_rank.first) {\n                min_rank = std::make_pair(rank, i);\n            }\n        }\n    }\n\n    return parts;\n}\n\nstd::vector<int> TiktokenCoreBPE::byte_pair_encode(const std::string &piece,\n                                                   const std::unordered_map<std::string, int> &ranks) {\n    CHATGLM_CHECK(piece.length() > 1);\n\n    auto parts = _byte_pair_merge(ranks, piece);\n\n    std::vector<int> tokens;\n    tokens.reserve(parts.size() - 1);\n\n    for (size_t i = 1; i < parts.size(); i++) {\n        size_t start = parts[i - 1].first;\n        size_t end = parts[i].first;\n        int rank = ranks.at(piece.substr(start, end - start));\n        tokens.emplace_back(rank);\n    }\n\n    return tokens;\n}\n\nstd::vector<int> TiktokenCoreBPE::_encode_ordinary_native(const std::string &text) const {\n    std::vector<int> ret;\n    re2::StringPiece input(text);\n    re2::StringPiece prev_input(input);\n    std::string piece;\n    std::string piece2;\n    while (RE2::FindAndConsume(&input, *regex, &piece, &piece2)) {\n        if (!piece2.empty()) {\n            // workaround for lookahead: capture sub group and restore input\n            auto pos = prev_input.find(piece2);\n            input = prev_input.substr(pos + piece2.length());\n            piece = std::move(piece2);\n        }\n        if (auto it = encoder.find(piece); it != encoder.end()) {\n            ret.emplace_back(it->second);\n        } else {\n            std::vector<int> bpe_ids = byte_pair_encode(piece, encoder);\n            ret.insert(ret.end(), bpe_ids.begin(), bpe_ids.end());\n        }\n        prev_input = input;\n    }\n    return ret;\n}\n\nstd::string TiktokenCoreBPE::_decode_native(const std::vector<int> &tokens) const {\n    std::string ret;\n    ret.reserve(tokens.size() * 2);\n    for (int token : tokens) {\n        if (auto it = decoder.find(token); it != decoder.end()) {\n            ret.append(it->second);\n        } else if (auto it = special_tokens_decoder.find(token); it != special_tokens_decoder.end()) {\n            ret.append(it->second);\n        } else {\n            CHATGLM_THROW << \"Unknown token \" << token;\n        }\n    }\n    return ret;\n}\n\nChatGLM4Tokenizer::ChatGLM4Tokenizer(const std::string &vocab_text) {\n    std::istringstream in(vocab_text);\n    std::string base64_token;\n    int rank;\n    std::unordered_map<std::string, int> mergeable_ranks;\n    while (in >> base64_token >> rank) {\n        std::string token;\n        CHATGLM_CHECK(google::protobuf::Base64Unescape(base64_token, &token));\n        mergeable_ranks.emplace(std::move(token), rank);\n    }\n    size_t vocab_size = mergeable_ranks.size();\n\n    const std::vector<std::string> all_special_tokens = {\"<|endoftext|>\",\n                                                         \"[MASK]\",\n                                                         \"[gMASK]\",\n                                                         \"[sMASK]\",\n                                                         \"<sop>\",\n                                                         \"<eop>\",\n                                                         \"<|system|>\",\n                                                         \"<|user|>\",\n                                                         \"<|assistant|>\",\n                                                         \"<|observation|>\",\n                                                         \"<|begin_of_image|>\",\n                                                         \"<|end_of_image|>\",\n                                                         \"<|begin_of_video|>\",\n                                                         \"<|end_of_video|>\"};\n\n    std::unordered_map<std::string, int> special_tokens_encoder;\n    special_tokens_encoder.reserve(all_special_tokens.size());\n    for (const auto &token : all_special_tokens) {\n        special_tokens_encoder.emplace(token, vocab_size++);\n    }\n    // common special token ids\n    eos_token_id = special_tokens_encoder.at(\"<|endoftext|>\");\n    gmask_token_id = special_tokens_encoder.at(\"[gMASK]\");\n    sop_token_id = special_tokens_encoder.at(\"<sop>\");\n    user_token_id = special_tokens_encoder.at(\"<|user|>\");\n    assistant_token_id = special_tokens_encoder.at(\"<|assistant|>\");\n    observation_token_id = special_tokens_encoder.at(\"<|observation|>\");\n    boi_token_id = special_tokens_encoder.at(\"<|begin_of_image|>\");\n    eoi_token_id = special_tokens_encoder.at(\"<|end_of_image|>\");\n\n    const std::string pattern =\n        R\"((?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|(\\s+)(?:\\s)|\\s+)\";\n    core_bpe = TiktokenCoreBPE(std::move(mergeable_ranks), std::move(special_tokens_encoder), pattern);\n}\n\nstd::vector<int> ChatGLM4Tokenizer::encode(const std::string &text, int max_length) const {\n    std::vector<int> ids = core_bpe.encode_ordinary(text);\n    ids.insert(ids.begin(), {gmask_token_id, sop_token_id}); // special prefix\n    truncate(ids, max_length);\n    return ids;\n}\n\nstd::string ChatGLM4Tokenizer::decode(const std::vector<int> &ids, bool skip_special_tokens) const {\n    std::vector<int> valid_ids = ids;\n    if (skip_special_tokens) {\n        valid_ids.erase(std::remove_if(valid_ids.begin(), valid_ids.end(),\n                                       [this](int id) { return core_bpe.special_tokens_decoder.count(id) > 0; }),\n                        valid_ids.end());\n    }\n    return core_bpe.decode(valid_ids);\n}\n\nChatMessage ChatGLM4Tokenizer::decode_message(const std::vector<int> &ids) const {\n    // TODO: support tool call\n    ChatMessage message = BaseTokenizer::decode_message(ids);\n    trim(message.content); // strip leading linebreak in conversation mode\n    return message;\n}\n\nstd::vector<int> ChatGLM4Tokenizer::apply_chat_template(const std::vector<ChatMessage> &messages,\n                                                        int max_length) const {\n    std::vector<int> input_ids{gmask_token_id, sop_token_id};\n    std::vector<int> newline_ids = core_bpe.encode_ordinary(\"\\n\");\n    for (const auto &msg : messages) {\n        input_ids.emplace_back(core_bpe.special_tokens_encoder.at(\"<|\" + msg.role + \"|>\"));\n        input_ids.insert(input_ids.end(), newline_ids.begin(), newline_ids.end());\n        if (msg.image.has_value()) {\n            input_ids.insert(input_ids.end(), {boi_token_id, eos_token_id, eoi_token_id});\n        }\n        std::vector<int> content_ids = core_bpe.encode_ordinary(msg.content);\n        input_ids.insert(input_ids.end(), content_ids.begin(), content_ids.end());\n    }\n    input_ids.emplace_back(assistant_token_id);\n    truncate(input_ids, max_length);\n    return input_ids;\n}\n\nvoid ChatGLM4Tokenizer::truncate(std::vector<int> &ids, int max_length) {\n    if ((int)ids.size() > max_length) {\n        // sliding window: drop the least recent history while keeping the two special prefix tokens\n        int num_drop = (int)ids.size() - max_length;\n        ids.erase(ids.begin() + 2, ids.begin() + 2 + num_drop);\n    }\n}\n\n// ===== GLM4V-9B =====\n\nggml_tensor *Conv2d::forward(ModelContext *mctx, ggml_tensor *input) const {\n    // input: [b, c, h, w]\n    ggml_context *ctx = mctx->ctx_b.get();\n    ggml_tensor *output = ggml_conv_2d(ctx, weight, input, stride, stride, 0, 0, 1, 1); // [b, oc, oh, ow]\n    output = ggml_add_inplace(ctx, output, bias);\n    return output;\n}\n\nggml_tensor *PatchEmbedding::forward(ModelContext *mctx, ggml_tensor *input) const {\n    // input: [c, h, w]\n    ggml_context *ctx = mctx->ctx_b.get();\n    input = proj.forward(mctx, input);                                              // [oc, oh, ow]\n    input = ggml_reshape_2d(ctx, input, input->ne[0] * input->ne[1], input->ne[2]); // [oc, oh * ow]\n    input = ggml_permute(ctx, input, 1, 0, 2, 3);                                   // [oh * ow, oc] view as [s, h]\n\n    ggml_tensor *embeddings = ggml_new_tensor_2d(ctx, input->type, hidden_size(), num_positions());\n    ggml_set_name(embeddings, \"embeddings\");\n    ggml_set_input(embeddings);\n\n    // concat (cls, x)\n    ggml_tensor *cls_embedding_view =\n        ggml_cpy(ctx, cls_embedding, ggml_view_1d(ctx, embeddings, cls_embedding->ne[0], 0));\n    ggml_tensor *img_embedding_view = ggml_cpy(\n        ctx, input, ggml_view_2d(ctx, embeddings, input->ne[0], input->ne[1], embeddings->nb[1], embeddings->nb[1]));\n    ggml_build_forward_expand(mctx->gf, cls_embedding_view);\n    ggml_build_forward_expand(mctx->gf, img_embedding_view);\n\n    embeddings = ggml_add_inplace(ctx, embeddings, position_embedding.weight); // [s, h]\n    return embeddings;\n}\n\nggml_tensor *EVA2CLIPBlock::forward(ModelContext *mctx, ggml_tensor *hidden_states, ggml_tensor *attention_mask,\n                                    ggml_tensor *position_ids, int n_past) const {\n    ggml_context *ctx = mctx->ctx_b.get();\n\n    ggml_tensor *residual = hidden_states;\n    hidden_states = attention.forward(mctx, hidden_states, attention_mask, position_ids, n_past);\n    hidden_states = input_layernorm.forward(mctx, hidden_states);\n    hidden_states = ggml_add_inplace(ctx, hidden_states, residual);\n\n    residual = hidden_states;\n    hidden_states = mlp.forward(mctx, hidden_states);\n    hidden_states = post_attention_layernorm.forward(mctx, hidden_states);\n    hidden_states = ggml_add_inplace(ctx, hidden_states, residual);\n\n    return hidden_states;\n}\n\nEVA2CLIPTransformer::EVA2CLIPTransformer(ModelContext *mctx, const VisionModelConfig &config) {\n    layers.reserve(config.num_hidden_layers);\n    for (int layer_id = 0; layer_id < config.num_hidden_layers; layer_id++) {\n        layers.emplace_back(mctx, config.dtype, config.hidden_size, config.num_attention_heads,\n                            config.num_attention_heads, config.intermediate_size, config.num_positions, config.norm_eps,\n                            config.hidden_act, true, true, false, RopeType::DISABLED, -1, 0, false);\n    }\n}\n\nggml_tensor *EVA2CLIPTransformer::forward(ModelContext *mctx, ggml_tensor *hidden_states,\n                                          ggml_tensor *attention_mask) const {\n    for (const auto &layer : layers) {\n        hidden_states = layer.forward(mctx, hidden_states, attention_mask, nullptr, 0);\n    }\n    return hidden_states;\n}\n\nggml_tensor *EVA2CLIPModel::forward(ModelContext *mctx, ggml_tensor *input) const {\n    ggml_context *ctx = mctx->ctx_b.get();\n\n    ggml_tensor *hidden_states = patch_embedding.forward(mctx, input);\n\n    // padding for flash attn\n    const int pad_to_multiple_of = ggml_cpu_has_cuda() ? 256 : GGML_KQ_MASK_PAD;\n    const int pad_size = GGML_PAD(hidden_states->ne[1], pad_to_multiple_of) - hidden_states->ne[1];\n    if (pad_size) {\n        hidden_states = ggml_pad(ctx, hidden_states, 0, pad_size, 0, 0);\n    }\n\n    ggml_tensor *encoder_attention_mask =\n        ggml_new_tensor_2d(ctx, GGML_TYPE_F32, hidden_states->ne[1], hidden_states->ne[1]);\n    ggml_set_input(encoder_attention_mask);\n    ggml_set_name(encoder_attention_mask, \"encoder_attention_mask\");\n\n    encoder_attention_mask = ggml_cast(ctx, encoder_attention_mask, GGML_TYPE_F16);\n    hidden_states = transformer.forward(mctx, hidden_states, encoder_attention_mask); // [s, hd]\n\n    const int grid_size = std::round(std::sqrt(hidden_states->ne[1] - pad_size - 1));\n    hidden_states = ggml_view_3d(ctx, hidden_states, hidden_states->ne[0], grid_size, grid_size, hidden_states->nb[1],\n                                 grid_size * hidden_states->nb[1], hidden_states->nb[1]); // [g, g, hd]\n    hidden_states = ggml_cont(ctx, ggml_permute(ctx, hidden_states, 2, 0, 1, 3));         // [hd, g, g]\n    hidden_states = conv.forward(mctx, hidden_states);                                    // [hd, g/2, g/2]\n    hidden_states = ggml_reshape_2d(ctx, hidden_states, hidden_states->ne[0] * hidden_states->ne[1],\n                                    hidden_states->ne[2]);                        // [hd, s]\n    hidden_states = ggml_cont(ctx, ggml_permute(ctx, hidden_states, 1, 0, 2, 3)); // [s, hd]\n\n    hidden_states = linear_proj.forward(mctx, hidden_states);\n    hidden_states = norm1.forward(mctx, hidden_states);\n    hidden_states = ggml_gelu_inplace(ctx, hidden_states);\n    hidden_states = glu.forward(mctx, hidden_states);\n\n    ggml_tensor *output = ggml_new_tensor_2d(ctx, hidden_states->type, hidden_states->ne[0], hidden_states->ne[1] + 2);\n\n    // concat (boi, x, eoi)\n    ggml_tensor *boi_view = ggml_cpy(ctx, boi, ggml_view_1d(ctx, output, hidden_states->ne[0], 0));\n    ggml_tensor *hidden_states_view =\n        ggml_cpy(ctx, hidden_states,\n                 ggml_view_2d(ctx, output, hidden_states->ne[0], hidden_states->ne[1], output->nb[1], output->nb[1]));\n    ggml_tensor *eoi_view =\n        ggml_cpy(ctx, eoi, ggml_view_1d(ctx, output, hidden_states->ne[0], (hidden_states->ne[1] + 1) * output->nb[1]));\n    ggml_build_forward_expand(mctx->gf, boi_view);\n    ggml_build_forward_expand(mctx->gf, hidden_states_view);\n    ggml_build_forward_expand(mctx->gf, eoi_view);\n\n    output = ggml_scale_inplace(ctx, output, 1.f / scaling_factor);\n\n    return output;\n}\n\nggml_tensor *ChatGLM4VModel::forward_embeddings(ModelContext *mctx, ggml_tensor *input_ids, ggml_tensor *images,\n                                                const std::vector<int> &input_ids_vec, int n_past) const {\n    ggml_context *ctx = mctx->ctx_b.get();\n\n    ggml_tensor *text_emb = word_embeddings.forward(mctx, input_ids);\n\n    if (!(n_past == 0 && images)) {\n        return text_emb;\n    }\n\n    CHATGLM_CHECK(images->ne[0] == images->ne[1] && images->ne[2] == 3);\n\n    const int vision_begin =\n        std::find(input_ids_vec.begin(), input_ids_vec.end(), config.boi_token_id) - input_ids_vec.begin();\n    const int vision_end =\n        std::find(input_ids_vec.begin(), input_ids_vec.end(), config.eoi_token_id) - input_ids_vec.begin() + 1;\n    CHATGLM_CHECK(vision_begin < (int)input_ids_vec.size() && vision_end <= (int)input_ids_vec.size() &&\n                  vision_begin + 3 == vision_end);\n\n    ggml_tensor *vision_emb = vision.forward(mctx, images);\n\n    const int text_tokens = input_ids->ne[0];\n    const int total_tokens = text_tokens - 1 + num_vision_tokens();\n\n    ggml_tensor *embeddings = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, config.hidden_size, total_tokens);\n    // before boi\n    ggml_tensor *text_emb0_view =\n        ggml_cpy(ctx, ggml_view_2d(ctx, text_emb, config.hidden_size, vision_begin, text_emb->nb[1], 0),\n                 ggml_view_2d(ctx, embeddings, config.hidden_size, vision_begin, embeddings->nb[1], 0));\n    // vision tokens\n    ggml_tensor *vision_emb_view = ggml_cpy(ctx, vision_emb,\n                                            ggml_view_2d(ctx, embeddings, config.hidden_size, vision_emb->ne[1],\n                                                         embeddings->nb[1], vision_begin * embeddings->nb[1]));\n    // after eoi\n    ggml_tensor *text_emb1_view =\n        ggml_cpy(ctx,\n                 ggml_view_2d(ctx, text_emb, config.hidden_size, text_tokens - vision_end, text_emb->nb[1],\n                              vision_end * text_emb->nb[1]),\n                 ggml_view_2d(ctx, embeddings, config.hidden_size, text_tokens - vision_end, embeddings->nb[1],\n                              (total_tokens - (text_tokens - vision_end)) * embeddings->nb[1]));\n\n    ggml_build_forward_expand(mctx->gf, text_emb0_view);\n    ggml_build_forward_expand(mctx->gf, vision_emb_view);\n    ggml_build_forward_expand(mctx->gf, text_emb1_view);\n\n    return embeddings;\n}\n\nvoid ChatGLM4VModel::set_graph_inputs(ggml_cgraph *gf, const std::vector<int> &input_ids,\n                                      const std::optional<Image> &image, int n_past, int n_ctx) const {\n    // set position_ids\n    ggml_tensor *position_ids = ggml_graph_get_tensor(gf, \"position_ids\");\n    if (position_ids) {\n        std::vector<int> position_ids_buf(position_ids->ne[0]);\n        const auto vision_idx =\n            std::find(input_ids.begin(), input_ids.end(), config.boi_token_id) - input_ids.begin() + 1;\n        if (n_past == 0 && vision_idx < (int)input_ids.size()) {\n            std::iota(position_ids_buf.begin(), position_ids_buf.begin() + vision_idx, 0);\n            std::fill(position_ids_buf.begin() + vision_idx,\n                      position_ids_buf.begin() + vision_idx + num_vision_tokens(), vision_idx);\n            std::iota(position_ids_buf.begin() + vision_idx + num_vision_tokens(), position_ids_buf.end(),\n                      vision_idx + 1);\n        } else {\n            const int n_past_text = (n_past > 0) ? input_ids.size() - 1 : 0;\n            std::iota(position_ids_buf.begin(), position_ids_buf.end(), n_past_text);\n        }\n        ggml_backend_tensor_set(position_ids, position_ids_buf.data(), 0, position_ids_buf.size() * sizeof(int));\n    }\n\n    // set image\n    ggml_tensor *image_tensor = ggml_graph_get_tensor(gf, \"image\");\n    if (image_tensor) {\n        // resize\n        Image src_resized = image->resize(config.vision.image_size, config.vision.image_size);\n        // to float, normalize, channel first\n        std::vector<float> pixels_f32(src_resized.pixels.size());\n        const size_t num_pixels = src_resized.height * src_resized.width;\n        for (size_t i = 0; i < num_pixels; i++) {\n            pixels_f32[0 * num_pixels + i] =\n                (src_resized.pixels[i * src_resized.channels + 0] / 255.f - 0.48145466f) / 0.26862954f;\n            pixels_f32[1 * num_pixels + i] =\n                (src_resized.pixels[i * src_resized.channels + 1] / 255.f - 0.45782750f) / 0.26130258f;\n            pixels_f32[2 * num_pixels + i] =\n                (src_resized.pixels[i * src_resized.channels + 2] / 255.f - 0.40821073f) / 0.27577711f;\n        }\n        // copy to tensor\n        ggml_backend_tensor_set(image_tensor, pixels_f32.data(), 0, ggml_nbytes(image_tensor));\n    }\n\n    // attention_mask\n    ggml_tensor *attention_mask = ggml_graph_get_tensor(gf, \"attention_mask\");\n    if (attention_mask) {\n        const int kvlen = attention_mask->ne[0];\n        const int qlen = attention_mask->ne[1];\n        std::vector<float> mask_buf(qlen * kvlen);\n        for (int i = 0; i < qlen; i++) {\n            for (int j = 0; j < kvlen; j++) {\n                mask_buf[i * kvlen + j] = (i < j + qlen - kvlen) ? -INFINITY : 0.f;\n            }\n        }\n        ggml_backend_tensor_set(attention_mask, mask_buf.data(), 0, ggml_nbytes(attention_mask));\n    }\n\n    // encoder_attention_mask\n    ggml_tensor *encoder_attention_mask = ggml_graph_get_tensor(gf, \"encoder_attention_mask\");\n    if (encoder_attention_mask) {\n        const int valid_tokens = vision.patch_embedding.num_positions();\n        const int M = encoder_attention_mask->ne[1];\n        const int N = encoder_attention_mask->ne[0];\n        std::vector<float> encoder_mask_f32(M * N);\n        CHATGLM_CHECK((size_t)ggml_nelements(encoder_attention_mask) == encoder_mask_f32.size());\n        for (int i = 0; i < M; i++) {\n            for (int j = 0; j < N; j++) {\n                encoder_mask_f32[i * N + j] =\n                    (i < valid_tokens && j < valid_tokens) ? 0.f : -65504.f; // -INFINITY causes nan/inf logits\n            }\n        }\n        ggml_backend_tensor_set(encoder_attention_mask, encoder_mask_f32.data(), 0,\n                                ggml_nbytes(encoder_attention_mask));\n    }\n}\n\nint ChatGLM4VForCausalLM::count_tokens(const std::vector<int> &input_ids, const std::optional<Image> &image) const {\n    int token_cnt = input_ids.size();\n    if (image) {\n        token_cnt += transformer.num_vision_tokens() - 1;\n    }\n    return token_cnt;\n}\n\nvoid ChatGLM4VForCausalLM::load_state_dict(const StateDict &sd) {\n    alloc_weight_context(mctx_.get(), sd.buf.get());\n\n    auto self_sd = state_dict();\n    ChatGLM2ForCausalLM::load_state_dict(mctx_.get(), self_sd, sd);\n}\n\nStateDict ChatGLM4VForCausalLM::state_dict() const {\n    StateDict sd;\n\n    // vision\n    sd.kv.emplace(\"transformer.vision.patch_embedding.cls_embedding\", transformer.vision.patch_embedding.cls_embedding);\n    sd.kv.emplace(\"transformer.vision.patch_embedding.proj.weight\", transformer.vision.patch_embedding.proj.weight);\n    sd.kv.emplace(\"transformer.vision.patch_embedding.proj.bias\", transformer.vision.patch_embedding.proj.bias);\n    sd.kv.emplace(\"transformer.vision.patch_embedding.position_embedding.weight\",\n                  transformer.vision.patch_embedding.position_embedding.weight);\n\n    for (size_t i = 0; i < transformer.vision.transformer.layers.size(); i++) {\n        const std::string prefix = \"transformer.vision.transformer.layers.\" + std::to_string(i) + '.';\n        sd.kv.emplace(prefix + \"input_layernorm.weight\",\n                      transformer.vision.transformer.layers[i].input_layernorm.weight);\n        sd.kv.emplace(prefix + \"input_layernorm.bias\", transformer.vision.transformer.layers[i].input_layernorm.bias);\n        sd.kv.emplace(prefix + \"attention.query_key_value.weight\",\n                      transformer.vision.transformer.layers[i].attention.query_key_value.weight);\n        sd.kv.emplace(prefix + \"attention.query_key_value.bias\",\n                      transformer.vision.transformer.layers[i].attention.query_key_value.bias);\n        sd.kv.emplace(prefix + \"attention.dense.weight\",\n                      transformer.vision.transformer.layers[i].attention.dense.weight);\n        sd.kv.emplace(prefix + \"attention.dense.bias\", transformer.vision.transformer.layers[i].attention.dense.bias);\n        sd.kv.emplace(prefix + \"mlp.fc1.weight\", transformer.vision.transformer.layers[i].mlp.dense_h_to_4h.weight);\n        sd.kv.emplace(prefix + \"mlp.fc1.bias\", transformer.vision.transformer.layers[i].mlp.dense_h_to_4h.bias);\n        sd.kv.emplace(prefix + \"mlp.fc2.weight\", transformer.vision.transformer.layers[i].mlp.dense_4h_to_h.weight);\n        sd.kv.emplace(prefix + \"mlp.fc2.bias\", transformer.vision.transformer.layers[i].mlp.dense_4h_to_h.bias);\n        sd.kv.emplace(prefix + \"post_attention_layernorm.weight\",\n                      transformer.vision.transformer.layers[i].post_attention_layernorm.weight);\n        sd.kv.emplace(prefix + \"post_attention_layernorm.bias\",\n                      transformer.vision.transformer.layers[i].post_attention_layernorm.bias);\n    }\n    sd.kv.emplace(\"transformer.vision.conv.weight\", transformer.vision.conv.weight);\n    sd.kv.emplace(\"transformer.vision.conv.bias\", transformer.vision.conv.bias);\n    sd.kv.emplace(\"transformer.vision.linear_proj.linear_proj.weight\", transformer.vision.linear_proj.weight);\n    sd.kv.emplace(\"transformer.vision.linear_proj.norm1.weight\", transformer.vision.norm1.weight);\n    sd.kv.emplace(\"transformer.vision.linear_proj.norm1.bias\", transformer.vision.norm1.bias);\n    sd.kv.emplace(\"transformer.vision.linear_proj.gate_proj.weight\", transformer.vision.glu.gate_proj.weight);\n    sd.kv.emplace(\"transformer.vision.linear_proj.dense_h_to_4h.weight\", transformer.vision.glu.up_proj.weight);\n    sd.kv.emplace(\"transformer.vision.linear_proj.dense_4h_to_h.weight\", transformer.vision.glu.down_proj.weight);\n    sd.kv.emplace(\"transformer.vision.boi\", transformer.vision.boi);\n    sd.kv.emplace(\"transformer.vision.eoi\", transformer.vision.eoi);\n\n    // text\n    sd.kv.emplace(\"transformer.embedding.word_embeddings.weight\", transformer.word_embeddings.weight);\n    for (int i = 0; i < config.num_hidden_layers; i++) {\n        const std::string layer_prefix = \"transformer.encoder.layers.\" + std::to_string(i) + '.';\n        sd.kv.emplace(layer_prefix + \"input_layernorm.weight\", transformer.layers[i].input_layernorm.weight);\n        sd.kv.emplace(layer_prefix + \"self_attention.query_key_value.weight\",\n                      transformer.layers[i].attention.query_key_value.weight);\n        sd.kv.emplace(layer_prefix + \"self_attention.query_key_value.bias\",\n                      transformer.layers[i].attention.query_key_value.bias);\n        sd.kv.emplace(layer_prefix + \"self_attention.dense.weight\", transformer.layers[i].attention.dense.weight);\n        sd.kv.emplace(layer_prefix + \"post_attention_layernorm.weight\",\n                      transformer.layers[i].post_attention_layernorm.weight);\n        sd.kv.emplace(layer_prefix + \"mlp.gate_proj.weight\", transformer.layers[i].mlp.gate_proj.weight);\n        sd.kv.emplace(layer_prefix + \"mlp.up_proj.weight\", transformer.layers[i].mlp.up_proj.weight);\n        // for compatibility\n        sd.kv.emplace(layer_prefix + \"mlp.dense_4h_to_h.weight\", transformer.layers[i].mlp.down_proj.weight);\n    }\n    sd.kv.emplace(\"transformer.encoder.final_layernorm.weight\", transformer.final_layernorm.weight);\n    sd.kv.emplace(\"transformer.output_layer.weight\", lm_head.weight);\n    return sd;\n}\n\n// ===== pipeline =====\n\nPipeline::Pipeline(const std::string &path, int max_length) {\n    auto _update_config_max_length = [](ModelConfig &config, int max_length) {\n        if (max_length > 0) {\n            CHATGLM_CHECK(max_length <= config.max_length)\n                << \"Requested max_length (\" << max_length << \") exceeds the max possible model sequence length (\"\n                << config.max_length << \")\";\n            config.max_length = max_length;\n        }\n    };\n\n    mapped_file_ = std::make_unique<MappedFile>(path);\n    ModelLoader loader(mapped_file_->data, mapped_file_->size);\n\n    // load magic\n    std::string magic = loader.read_string(4);\n    CHATGLM_CHECK(magic == \"ggml\") << \"model file is broken (bad magic)\";\n\n    // load model type\n    ModelType model_type = (ModelType)loader.read_basic<int>();\n    // load version\n    int version = loader.read_basic<int>();\n    if (model_type == ModelType::CHATGLM) {\n        // load config\n        ModelConfig config;\n        if (version == 1) {\n            config = ModelConfig(model_type, loader.read_basic<ModelConfigRecordV1>(), 1e-5f, 10000.f, 0);\n        } else if (version == 2) {\n            config = ModelConfig(model_type, loader.read_basic<ModelConfigRecordV2>());\n        } else {\n            CHATGLM_THROW << \"only support version 1 or 2 for now but got \" << version;\n        }\n        _update_config_max_length(config, max_length);\n\n        // load tokenizer\n        int proto_size = loader.read_basic<int>();\n        std::string_view serialized_model_proto((char *)mapped_file_->data + loader.tell(), proto_size);\n        loader.seek(proto_size, SEEK_CUR);\n        tokenizer = std::make_unique<ChatGLMTokenizer>(serialized_model_proto);\n\n        // load model\n        model = std::make_unique<ChatGLMForCausalLM>(config);\n        StateDict sd = loader.read_state_dict();\n        model->load_state_dict(sd);\n    } else if (model_type == ModelType::CHATGLM2 || model_type == ModelType::CHATGLM3) {\n        // load config\n        ModelConfig config;\n        if (version == 1) {\n            config = ModelConfig(model_type, loader.read_basic<ModelConfigRecordV1GQA>(), 1e-5f, 10000.f, 0);\n        } else if (version == 2) {\n            config = ModelConfig(model_type, loader.read_basic<ModelConfigRecordV2>());\n        } else {\n            CHATGLM_THROW << \"only support version 1 or 2 for now but got \" << version;\n        }\n        _update_config_max_length(config, max_length);\n\n        // load tokenizer\n        int proto_size = loader.read_basic<int>();\n        std::string_view serialized_model_proto((char *)mapped_file_->data + loader.tell(), proto_size);\n        loader.seek(proto_size, SEEK_CUR);\n\n        if (model_type == ModelType::CHATGLM2) {\n            tokenizer = std::make_unique<ChatGLM2Tokenizer>(serialized_model_proto);\n            model = std::make_unique<ChatGLM2ForCausalLM>(config);\n        } else {\n            auto chatglm3_tokenizer = std::make_unique<ChatGLM3Tokenizer>(serialized_model_proto);\n            // TODO: read from checkpoint file\n            config.extra_eos_token_ids = {chatglm3_tokenizer->observation_token_id, chatglm3_tokenizer->user_token_id};\n            tokenizer = std::move(chatglm3_tokenizer);\n            model = std::make_unique<ChatGLM3ForCausalLM>(config);\n        }\n\n        // load model\n        StateDict sd = loader.read_state_dict();\n        model->load_state_dict(sd);\n    } else if (model_type == ModelType::CHATGLM4 || model_type == ModelType::CHATGLM4V) {\n        // load config\n        CHATGLM_CHECK(version == 2) << \"only support version 2 for now but got \" << version;\n        ModelConfig config(model_type, loader.read_basic<ModelConfigRecordV2>());\n        _update_config_max_length(config, max_length);\n\n        if (model_type == ModelType::CHATGLM4V) {\n            config.vision = VisionModelConfig(loader.read_basic<VisionModelConfigRecord>());\n        }\n\n        // load tokenizer\n        int vocab_text_size = loader.read_basic<int>();\n        std::string vocab_text = loader.read_string(vocab_text_size);\n        auto chatglm4_tokenizer = std::make_unique<ChatGLM4Tokenizer>(vocab_text);\n        config.extra_eos_token_ids = {chatglm4_tokenizer->observation_token_id, chatglm4_tokenizer->user_token_id};\n        config.boi_token_id = chatglm4_tokenizer->boi_token_id;\n        config.eoi_token_id = chatglm4_tokenizer->eoi_token_id;\n        tokenizer = std::move(chatglm4_tokenizer);\n\n        // load model\n        if (model_type == ModelType::CHATGLM4V) {\n            model = std::make_unique<ChatGLM4VForCausalLM>(config);\n        } else {\n            model = std::make_unique<ChatGLM4ForCausalLM>(config);\n        }\n        StateDict sd = loader.read_state_dict();\n        model->load_state_dict(sd);\n    } else {\n        CHATGLM_THROW << \"invalid model type \" << (int)model_type;\n    }\n}\n\nstd::vector<int> Pipeline::generate(const std::vector<int> &input_ids, const std::optional<Image> &image,\n                                    const GenerationConfig &gen_config, BaseStreamer *streamer) const {\n    std::vector<int> output_ids = model->generate(input_ids, image, gen_config, streamer);\n    std::vector<int> new_output_ids(output_ids.begin() + input_ids.size(), output_ids.end());\n    return new_output_ids;\n}\n\nstd::string Pipeline::generate(const std::string &prompt, const GenerationConfig &gen_config,\n                               BaseStreamer *streamer) const {\n    std::vector<int> input_ids = tokenizer->encode(prompt, gen_config.max_context_length);\n    std::vector<int> new_output_ids = generate(input_ids, std::nullopt, gen_config, streamer);\n    std::string output = tokenizer->decode(new_output_ids);\n    return output;\n}\n\nChatMessage Pipeline::chat(const std::vector<ChatMessage> &messages, const GenerationConfig &gen_config,\n                           BaseStreamer *streamer) const {\n    auto it =\n        std::find_if(messages.begin(), messages.end(), [](const ChatMessage &msg) { return msg.image.has_value(); });\n    const std::optional<Image> image = (it != messages.end()) ? it->image : std::nullopt;\n    std::vector<int> input_ids = tokenizer->apply_chat_template(messages, gen_config.max_context_length);\n    std::vector<int> new_output_ids = generate(input_ids, image, gen_config, streamer);\n    ChatMessage output = tokenizer->decode_message(new_output_ids);\n    return output;\n}\n\n} // namespace chatglm\n"
        },
        {
          "name": "chatglm.h",
          "type": "blob",
          "size": 51.296875,
          "content": "#pragma once\n\n#include <cmath>\n#include <ggml.h>\n#include <ggml/ggml-backend.h>\n#include <iomanip>\n#include <re2/re2.h>\n#include <sentencepiece_processor.h>\n#include <sstream>\n#include <unordered_map>\n\nnamespace chatglm {\n\n// ===== common =====\n\nclass LogMessageFatal {\n  public:\n    LogMessageFatal(const char *file, int line) { oss_ << file << ':' << line << ' '; }\n    [[noreturn]] ~LogMessageFatal() noexcept(false) { throw std::runtime_error(oss_.str()); }\n    std::ostringstream &stream() { return oss_; }\n\n  private:\n    std::ostringstream oss_;\n};\n\n#define CHATGLM_THROW ::chatglm::LogMessageFatal(__FILE__, __LINE__).stream()\n#define CHATGLM_CHECK(cond)                                                                                            \\\n    if (!(cond))                                                                                                       \\\n    CHATGLM_THROW << \"check failed (\" #cond \") \"\n\n#define CHATGLM_CHECK_CUDA(call)                                                                                       \\\n    do {                                                                                                               \\\n        cudaError_t error = (call);                                                                                    \\\n        CHATGLM_CHECK(error == cudaSuccess) << \"CUDA error: \" << cudaGetErrorString(error);                            \\\n    } while (0)\n\nstd::string to_string(ggml_tensor *tensor, bool with_data = true);\n\nenum class ModelType {\n    CHATGLM = 1,\n    CHATGLM2 = 2,\n    CHATGLM3 = 3,\n    CHATGLM4 = 4,\n    CHATGLM4V = 1004,\n};\n\nstd::string to_string(ModelType model_type);\n\n// For compatibility\nstruct ModelConfigRecordV1 {\n    // common attributes\n    ggml_type dtype;\n    int vocab_size;\n    int hidden_size;\n    int num_attention_heads;\n    int num_hidden_layers;\n    int intermediate_size;\n    // for sequence generation\n    int max_length;\n    // for tokenizer\n    int bos_token_id;\n    int eos_token_id;\n    int pad_token_id;\n    int sep_token_id;\n};\n\n// For compatibility\nstruct ModelConfigRecordV1GQA {\n    // ModelConfigRecordV1\n    ggml_type dtype;\n    int vocab_size;\n    int hidden_size;\n    int num_attention_heads;\n    int num_hidden_layers;\n    int intermediate_size;\n    int max_length;\n    int bos_token_id;\n    int eos_token_id;\n    int pad_token_id;\n    int sep_token_id;\n    // GQA\n    int num_key_value_heads;\n};\n\n// TODO: use json to serialize config\nstruct ModelConfigRecordV2 {\n    ggml_type dtype;\n    int vocab_size;\n    int hidden_size;\n    int num_attention_heads;\n    int num_key_value_heads;\n    int num_hidden_layers;\n    int intermediate_size;\n    float norm_eps;\n    int num_virtual_tokens;\n    float rope_theta;\n    int max_length;\n    int eos_token_id;\n    int pad_token_id;\n};\n\nenum class ActivationType {\n    GELU,\n    SILU,\n};\n\nenum class RopeType {\n    GPTJ = 0,\n    NEOX = 2,\n    CHATGLM = 4,\n    CHATGLM2 = 8,\n    DISABLED = 10000,\n};\n\nenum class AttentionMaskType {\n    BIDIRECTIONAL,\n    CAUSAL,\n    CHATGLM,\n};\n\nstruct VisionModelConfigRecord {\n    ggml_type dtype;\n    int hidden_size;\n    int image_size;\n    int in_channels;\n    int intermediate_size;\n    float norm_eps;\n    int num_attention_heads;\n    int num_hidden_layers;\n    int num_positions;\n    int patch_size;\n    float scaling_factor;\n};\n\nstruct VisionModelConfig {\n    ggml_type dtype;\n    ActivationType hidden_act;\n    int hidden_size;\n    int image_size;\n    int in_channels;\n    int intermediate_size;\n    float norm_eps;\n    int num_attention_heads;\n    int num_hidden_layers;\n    int num_positions;\n    int patch_size;\n    float scaling_factor;\n\n    VisionModelConfig() = default;\n\n    VisionModelConfig(ggml_type dtype, ActivationType hidden_act, int hidden_size, int image_size, int in_channels,\n                      int intermediate_size, float norm_eps, int num_attention_heads, int num_hidden_layers,\n                      int num_positions, int patch_size, float scaling_factor)\n        : dtype(dtype), hidden_act(hidden_act), hidden_size(hidden_size), image_size(image_size),\n          in_channels(in_channels), intermediate_size(intermediate_size), norm_eps(norm_eps),\n          num_attention_heads(num_attention_heads), num_hidden_layers(num_hidden_layers), num_positions(num_positions),\n          patch_size(patch_size), scaling_factor(scaling_factor) {}\n\n    VisionModelConfig(const VisionModelConfigRecord &rec)\n        : VisionModelConfig(rec.dtype, ActivationType::GELU, rec.hidden_size, rec.image_size, rec.in_channels,\n                            rec.intermediate_size, rec.norm_eps, rec.num_attention_heads, rec.num_hidden_layers,\n                            rec.num_positions, rec.patch_size, rec.scaling_factor) {}\n\n    friend std::ostream &operator<<(std::ostream &os, const VisionModelConfig &self) {\n        return os << \"VisionModelConfig(dtype=\" << self.dtype << \", hidden_act=\" << (int)self.hidden_act\n                  << \", hidden_size=\" << self.hidden_size << \", image_size=\" << self.image_size\n                  << \", in_channels=\" << self.in_channels << \", intermediate_size=\" << self.intermediate_size\n                  << \", norm_eps=\" << self.norm_eps << \", num_attention_heads=\" << self.num_attention_heads\n                  << \", num_hidden_layers=\" << self.num_hidden_layers << \", num_positions=\"\n                  << \", patch_size=\" << self.patch_size << \", scaling_factor=\" << self.scaling_factor << \")\";\n    }\n};\n\n// Should save kv record of ModelConfig in the future\nclass ModelConfig {\n  public:\n    ModelConfig() = default;\n\n    ModelConfig(ModelType model_type, ggml_type dtype, int vocab_size, int hidden_size, int num_attention_heads,\n                int num_key_value_heads, int num_hidden_layers, int intermediate_size, float norm_eps, float rope_theta,\n                int num_virtual_tokens, int max_length, int bos_token_id, int eos_token_id, int pad_token_id,\n                int sep_token_id, int boi_token_id, int eoi_token_id, std::vector<int> extra_eos_token_ids,\n                const VisionModelConfig &vision)\n        : model_type(model_type), dtype(dtype), vocab_size(vocab_size), hidden_size(hidden_size),\n          num_attention_heads(num_attention_heads), num_key_value_heads(num_key_value_heads),\n          num_hidden_layers(num_hidden_layers), intermediate_size(intermediate_size), norm_eps(norm_eps),\n          rope_theta(rope_theta), num_virtual_tokens(num_virtual_tokens), max_length(max_length),\n          bos_token_id(bos_token_id), eos_token_id(eos_token_id), pad_token_id(pad_token_id),\n          sep_token_id(sep_token_id), boi_token_id(boi_token_id), eoi_token_id(eoi_token_id),\n          extra_eos_token_ids(std::move(extra_eos_token_ids)), vision(vision) {\n        if (model_type == ModelType::CHATGLM) {\n            hidden_act = ActivationType::GELU;\n            use_qkv_bias = true;\n            use_dense_bias = true;\n            interleaved_qkv = true;\n            tie_word_embeddings = true;\n            rope_type = RopeType::CHATGLM;\n        } else {\n            hidden_act = ActivationType::SILU;\n            use_qkv_bias = true;\n            use_dense_bias = false;\n            interleaved_qkv = false;\n            tie_word_embeddings = false;\n            rope_type = RopeType::CHATGLM2;\n        }\n    }\n\n    ModelConfig(ModelType model_type, const ModelConfigRecordV1 &rec, float norm_eps, float rope_theta,\n                int num_virtual_tokens)\n        : ModelConfig(model_type, rec.dtype, rec.vocab_size, rec.hidden_size, rec.num_attention_heads,\n                      rec.num_attention_heads, rec.num_hidden_layers, rec.intermediate_size, norm_eps, rope_theta,\n                      num_virtual_tokens, rec.max_length, rec.bos_token_id, rec.eos_token_id, rec.pad_token_id,\n                      rec.sep_token_id, -1, -1, {}, {}) {}\n\n    ModelConfig(ModelType model_type, const ModelConfigRecordV1GQA &rec, float norm_eps, float rope_theta,\n                int num_virtual_tokens)\n        : ModelConfig(model_type, rec.dtype, rec.vocab_size, rec.hidden_size, rec.num_attention_heads,\n                      rec.num_key_value_heads, rec.num_hidden_layers, rec.intermediate_size, norm_eps, rope_theta,\n                      num_virtual_tokens, rec.max_length, rec.bos_token_id, rec.eos_token_id, rec.pad_token_id,\n                      rec.sep_token_id, -1, -1, {}, {}) {}\n\n    ModelConfig(ModelType model_type, const ModelConfigRecordV2 &rec)\n        : ModelConfig(model_type, rec.dtype, rec.vocab_size, rec.hidden_size, rec.num_attention_heads,\n                      rec.num_key_value_heads, rec.num_hidden_layers, rec.intermediate_size, rec.norm_eps,\n                      rec.rope_theta, rec.num_virtual_tokens, rec.max_length, -1, rec.eos_token_id, rec.pad_token_id,\n                      -1, -1, -1, {}, {}) {}\n\n    std::string model_type_name() const { return to_string(model_type); }\n\n    friend std::ostream &operator<<(std::ostream &os, const ModelConfig &self) {\n        os << \"ModelConfig(model_type=\" << (int)self.model_type << \", dtype=\" << self.dtype\n           << \", vocab_size=\" << self.vocab_size << \", hidden_size=\" << self.hidden_size\n           << \", num_attention_heads=\" << self.num_attention_heads\n           << \", num_key_value_heads=\" << self.num_key_value_heads << \", num_hidden_layers=\" << self.num_hidden_layers\n           << \", intermediate_size=\" << self.intermediate_size << \", norm_eps=\" << self.norm_eps\n           << \", hidden_act=\" << (int)self.hidden_act << \", use_qkv_bias=\" << self.use_qkv_bias\n           << \", use_dense_bias=\" << self.use_dense_bias << \", interleaved_qkv=\" << self.interleaved_qkv\n           << \", tie_word_embeddings=\" << self.tie_word_embeddings << \", rope_type=\" << (int)self.rope_type\n           << \", rope_theta=\" << self.rope_theta << \", num_virtual_tokens=\" << self.num_virtual_tokens\n           << \", max_length=\" << self.max_length << \", bos_token_id=\" << self.bos_token_id\n           << \", eos_token_id=\" << self.eos_token_id << \", pad_token_id=\" << self.pad_token_id\n           << \", sep_token_id=\" << self.sep_token_id << \", extra_eos_token_ids={\";\n        for (size_t i = 0; i < self.extra_eos_token_ids.size(); i++) {\n            os << (i > 0 ? \", \" : \"\") << self.extra_eos_token_ids[i];\n        }\n        return os << \"}, vision=\" << self.vision << \")\";\n    }\n\n  public:\n    ModelType model_type;\n    ggml_type dtype;\n    int vocab_size;\n    int hidden_size;\n    int num_attention_heads;\n    int num_key_value_heads;\n    int num_hidden_layers;\n    int intermediate_size;\n    float norm_eps;\n    ActivationType hidden_act;\n    bool use_qkv_bias;\n    bool use_dense_bias;\n    bool interleaved_qkv;\n    bool tie_word_embeddings;\n    RopeType rope_type;\n    float rope_theta;\n    int num_virtual_tokens;\n    int max_length;\n    int bos_token_id;\n    int eos_token_id;\n    int pad_token_id;\n    int sep_token_id;\n    int boi_token_id;\n    int eoi_token_id;\n    std::vector<int> extra_eos_token_ids;\n    VisionModelConfig vision;\n};\n\nstruct FunctionMessage {\n    std::string name;\n    std::string arguments;\n\n    FunctionMessage() = default;\n    FunctionMessage(std::string name, std::string arguments) : name(std::move(name)), arguments(std::move(arguments)) {}\n\n    friend std::ostream &operator<<(std::ostream &os, const FunctionMessage &self) {\n        return os << \"FunctionMessage(name=\" << std::quoted(self.name) << \", arguments=\" << std::quoted(self.arguments)\n                  << \")\";\n    }\n};\n\nstruct CodeMessage {\n    std::string input;\n\n    CodeMessage() = default;\n    CodeMessage(std::string input) : input(std::move(input)) {}\n\n    friend std::ostream &operator<<(std::ostream &os, const CodeMessage &self) {\n        return os << \"CodeMessage(input=\" << std::quoted(self.input) << \")\";\n    }\n};\n\nstruct ToolCallMessage {\n    std::string type;\n    FunctionMessage function;\n    CodeMessage code;\n\n    static const std::string TYPE_FUNCTION;\n    static const std::string TYPE_CODE;\n\n    ToolCallMessage(FunctionMessage function) : type(TYPE_FUNCTION), function(std::move(function)) {}\n\n    ToolCallMessage(CodeMessage code) : type(TYPE_CODE), code(std::move(code)) {}\n\n    friend std::ostream &operator<<(std::ostream &os, const ToolCallMessage &self) {\n        return os << \"ToolCallMessage(type=\" << std::quoted(self.type) << \", function=\" << self.function\n                  << \", code=\" << self.code << \")\";\n    }\n};\n\nstruct Image {\n    size_t width = 0;\n    size_t height = 0;\n    size_t channels = 0;\n    std::vector<uint8_t> pixels;\n\n    Image() = default;\n\n    Image(size_t width, size_t height, size_t channels)\n        : width(width), height(height), channels(channels), pixels(width * height * channels) {}\n\n    Image(size_t width, size_t height, size_t channels, uint8_t *data)\n        : width(width), height(height), channels(channels), pixels(data, data + width * height * channels) {}\n\n    Image(const Image &other) = default;\n\n    Image(Image &&other) { *this = std::move(other); }\n\n    Image &operator=(const Image &other) = default;\n\n    Image &operator=(Image &&other) {\n        width = other.width;\n        height = other.height;\n        channels = other.channels;\n        pixels = std::move(other.pixels);\n        other.clear();\n        return *this;\n    }\n\n    static Image open(const std::string &path);\n\n    Image resize(size_t new_width, size_t new_height) const;\n\n    void clear() {\n        width = height = channels = 0;\n        pixels.clear();\n    }\n\n    friend std::ostream &operator<<(std::ostream &os, const Image &self) {\n        return os << \"Image(mode=RGB, size=\" << self.width << \"x\" << self.height << \")\";\n    }\n};\n\nstruct ChatMessage {\n    std::string role;\n    std::string content;\n    std::optional<Image> image;\n    std::vector<ToolCallMessage> tool_calls;\n\n    static const std::string ROLE_USER;\n    static const std::string ROLE_ASSISTANT;\n    static const std::string ROLE_SYSTEM;\n    static const std::string ROLE_OBSERVATION;\n\n    ChatMessage() = default;\n\n    ChatMessage(std::string role, std::string content, std::optional<Image> image = std::nullopt,\n                std::vector<ToolCallMessage> tool_calls = {})\n        : role(std::move(role)), content(std::move(content)), image(std::move(image)),\n          tool_calls(std::move(tool_calls)) {}\n\n    friend std::ostream &operator<<(std::ostream &os, const ChatMessage &self) {\n        os << \"ChatMessage(role=\" << std::quoted(self.role) << \", content=\" << std::quoted(self.content);\n        if (self.image.has_value()) {\n            os << \", image=\" << *self.image;\n        }\n        os << \", tool_calls=[\";\n        for (size_t i = 0; i < self.tool_calls.size(); i++) {\n            os << (i > 0 ? \", \" : \"\") << self.tool_calls[i];\n        }\n        return os << \"])\";\n    }\n};\n\nclass BaseTokenizer {\n  public:\n    virtual ~BaseTokenizer() = default;\n\n    virtual std::vector<int> encode(const std::string &text, int max_length) const = 0;\n\n    virtual std::string decode(const std::vector<int> &ids, bool skip_special_tokens = true) const = 0;\n\n    virtual std::vector<int> apply_chat_template(const std::vector<ChatMessage> &messages, int max_length) const = 0;\n\n    virtual ChatMessage decode_message(const std::vector<int> &ids) const {\n        return {ChatMessage::ROLE_ASSISTANT, decode(ids)};\n    }\n\n  protected:\n    static void check_chat_messages(const std::vector<ChatMessage> &messages);\n\n    static std::vector<ChatMessage> filter_user_assistant_messages(const std::vector<ChatMessage> &messages);\n};\n\nstruct ggml_context_deleter_t {\n    void operator()(ggml_context *ctx) const noexcept { ggml_free(ctx); }\n};\n\nusing unique_ggml_context_t = std::unique_ptr<ggml_context, ggml_context_deleter_t>;\n\ninline unique_ggml_context_t make_unique_ggml_context(size_t mem_size, void *mem_buffer, bool no_alloc) {\n    return unique_ggml_context_t(ggml_init({mem_size, mem_buffer, no_alloc}));\n}\n\nstruct ggml_gallocr_deleter_t {\n    void operator()(ggml_gallocr *galloc) const noexcept { ggml_gallocr_free(galloc); }\n};\n\nusing unique_ggml_gallocr_t = std::unique_ptr<ggml_gallocr, ggml_gallocr_deleter_t>;\n\nstruct ggml_backend_deleter_t {\n    void operator()(ggml_backend_t backend) const noexcept { ggml_backend_free(backend); }\n};\n\nusing unique_ggml_backend_t = std::unique_ptr<ggml_backend, ggml_backend_deleter_t>;\n\nstruct ggml_backend_buffer_deleter_t {\n    void operator()(ggml_backend_buffer_t buffer) const noexcept { ggml_backend_buffer_free(buffer); }\n};\n\nusing unique_ggml_backend_buffer_t = std::unique_ptr<ggml_backend_buffer, ggml_backend_buffer_deleter_t>;\n\n// reference: https://github.com/ggerganov/llama.cpp/blob/master/llama.cpp\ntemplate <typename T>\nstruct no_init {\n    T value;\n    no_init() { /* do nothing */\n    }\n};\n\nstruct ModelContext {\n    std::vector<no_init<char>> compute_meta;\n\n    unique_ggml_context_t ctx_w;  // weight\n    unique_ggml_context_t ctx_kv; // kv cache\n    unique_ggml_context_t ctx_b;  // buffer\n\n    ggml_cgraph *gf;\n    unique_ggml_backend_t backend;\n    unique_ggml_gallocr_t allocr;\n\n    unique_ggml_backend_buffer_t buf_w;\n    unique_ggml_backend_buffer_t buf_kv;\n\n    ModelContext();\n};\n\nclass Embedding {\n  public:\n    Embedding() = default;\n\n    Embedding(ModelContext *mctx, ggml_type dtype, int num_embeddings, int embedding_dim)\n        : weight(ggml_new_tensor_2d(mctx->ctx_w.get(), dtype, embedding_dim, num_embeddings)) {}\n\n    int num_embeddings() const { return weight->ne[1]; }\n\n    int embedding_dim() const { return weight->ne[0]; }\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *input) const;\n\n  public:\n    ggml_tensor *weight = nullptr;\n};\n\nclass Linear {\n  public:\n    Linear() = default;\n\n    Linear(ModelContext *mctx, ggml_type dtype, int in_features, int out_features, bool use_bias = true)\n        : weight(ggml_new_tensor_2d(mctx->ctx_w.get(), dtype, in_features, out_features)),\n          bias(use_bias ? ggml_new_tensor_1d(mctx->ctx_w.get(), GGML_TYPE_F32, out_features) : nullptr) {}\n\n    int in_features() const { return weight->ne[0]; }\n    int out_features() const { return weight->ne[1]; }\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *input) const;\n\n  public:\n    ggml_tensor *weight = nullptr; // [out_features, in_features]\n    ggml_tensor *bias = nullptr;   // [out_features]\n};\n\nclass LayerNorm {\n  public:\n    LayerNorm() = default;\n\n    LayerNorm(ModelContext *mctx, int normalized_shape, float eps = 1e-5f)\n        : weight(ggml_new_tensor_1d(mctx->ctx_w.get(), GGML_TYPE_F32, normalized_shape)),\n          bias(ggml_new_tensor_1d(mctx->ctx_w.get(), GGML_TYPE_F32, normalized_shape)), eps(eps) {}\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *input) const;\n\n  public:\n    ggml_tensor *weight = nullptr; // [normalized_shape]\n    ggml_tensor *bias = nullptr;   // [normalized_shape]\n    float eps = 0.f;\n};\n\nclass RMSNorm {\n  public:\n    RMSNorm() = default;\n\n    RMSNorm(ModelContext *mctx, int normalized_shape, float eps = 1e-5f)\n        : weight(ggml_new_tensor_1d(mctx->ctx_w.get(), GGML_TYPE_F32, normalized_shape)), eps(eps) {}\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *input) const;\n\n  public:\n    ggml_tensor *weight = nullptr; // [normalized_shape]\n    float eps = 0.f;\n};\n\nclass BasicMLP {\n  public:\n    BasicMLP() = default;\n\n    BasicMLP(ModelContext *mctx, ggml_type dtype, int hidden_size, int intermediate_size, ActivationType hidden_act)\n        : dense_h_to_4h(mctx, dtype, hidden_size, intermediate_size),\n          dense_4h_to_h(mctx, dtype, intermediate_size, hidden_size), hidden_act(hidden_act) {}\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *hidden_states) const;\n\n  public:\n    Linear dense_h_to_4h;\n    Linear dense_4h_to_h;\n    ActivationType hidden_act;\n};\n\nclass BasicGLU {\n  public:\n    BasicGLU() = default;\n\n    BasicGLU(ModelContext *mctx, ggml_type dtype, int hidden_size, int intermediate_size, ActivationType hidden_act)\n        : gate_proj(mctx, dtype, hidden_size, intermediate_size, false),\n          up_proj(mctx, dtype, hidden_size, intermediate_size, false),\n          down_proj(mctx, dtype, intermediate_size, hidden_size, false), hidden_act(hidden_act) {}\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *hidden_states) const;\n\n  public:\n    Linear gate_proj;\n    Linear up_proj;\n    Linear down_proj;\n    ActivationType hidden_act;\n};\n\nclass BasicAttention {\n  public:\n    BasicAttention() = default;\n\n    BasicAttention(ModelContext *mctx, ggml_type dtype, int hidden_size, int num_attention_heads,\n                   int num_key_value_heads, int max_length, bool use_qkv_bias, bool use_dense_bias,\n                   bool interleaved_qkv, RopeType rope_type, float rope_theta, int num_virtual_tokens, bool use_cache)\n        : num_attention_heads(num_attention_heads), num_key_value_heads(num_key_value_heads),\n          interleaved_qkv(interleaved_qkv), rope_type(rope_type), rope_theta(rope_theta),\n          num_virtual_tokens(num_virtual_tokens),\n          query_key_value(mctx, dtype, hidden_size,\n                          hidden_size + 2 * (hidden_size / num_attention_heads) * num_key_value_heads, use_qkv_bias),\n          dense(mctx, dtype, hidden_size, hidden_size, use_dense_bias),\n          k_cache(use_cache ? ggml_new_tensor_3d(mctx->ctx_kv.get(), GGML_TYPE_F16, hidden_size / num_attention_heads,\n                                                 max_length + num_virtual_tokens, num_key_value_heads)\n                            : nullptr),\n          v_cache(use_cache ? ggml_new_tensor_3d(mctx->ctx_kv.get(), GGML_TYPE_F16, max_length + num_virtual_tokens,\n                                                 hidden_size / num_attention_heads, num_key_value_heads)\n                            : nullptr) {}\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *hidden_states, ggml_tensor *attention_mask,\n                         ggml_tensor *position_ids, int n_past) const;\n\n  public:\n    int num_attention_heads;\n    int num_key_value_heads;\n    bool interleaved_qkv;\n    RopeType rope_type;\n    float rope_theta;\n    int num_virtual_tokens;\n    Linear query_key_value;\n    Linear dense;\n    ggml_tensor *k_cache = nullptr; // [#kvh, s, d]\n    ggml_tensor *v_cache = nullptr; // [#kvh, d, s]\n};\n\ntemplate <typename Norm, typename MLP>\nclass BasicBlock {\n  public:\n    BasicBlock() = default;\n    BasicBlock(ModelContext *mctx, ggml_type dtype, int hidden_size, int num_attention_heads, int num_key_value_heads,\n               int intermediate_size, int max_length, float norm_eps, ActivationType hidden_act, bool use_qkv_bias,\n               bool use_dense_bias, bool interleaved_qkv, RopeType rope_type, float rope_theta, int num_virtual_tokens,\n               bool use_cache)\n        : input_layernorm(mctx, hidden_size, norm_eps),\n          attention(mctx, dtype, hidden_size, num_attention_heads, num_key_value_heads, max_length, use_qkv_bias,\n                    use_dense_bias, interleaved_qkv, rope_type, rope_theta, num_virtual_tokens, use_cache),\n          post_attention_layernorm(mctx, hidden_size, norm_eps),\n          mlp(mctx, dtype, hidden_size, intermediate_size, hidden_act) {}\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *hidden_states, ggml_tensor *attention_mask,\n                         ggml_tensor *position_ids, int n_past) const {\n        ggml_context *ctx = mctx->ctx_b.get();\n\n        ggml_tensor *residual = hidden_states;\n        hidden_states = input_layernorm.forward(mctx, hidden_states);\n        hidden_states = attention.forward(mctx, hidden_states, attention_mask, position_ids, n_past);\n        hidden_states = ggml_add_inplace(ctx, hidden_states, residual);\n\n        residual = hidden_states;\n        hidden_states = post_attention_layernorm.forward(mctx, hidden_states);\n        hidden_states = mlp.forward(mctx, hidden_states);\n        hidden_states = ggml_add_inplace(ctx, hidden_states, residual);\n\n        return hidden_states;\n    }\n\n  protected:\n    BasicBlock(Norm input_layernorm, BasicAttention attention, Norm post_attention_layernorm, MLP mlp)\n        : input_layernorm(input_layernorm), attention(attention), post_attention_layernorm(post_attention_layernorm),\n          mlp(mlp) {}\n\n  public:\n    Norm input_layernorm;\n    BasicAttention attention;\n    Norm post_attention_layernorm;\n    MLP mlp;\n};\n\nstruct NoopPositionIdsAllocator {\n    ggml_tensor *operator()(ggml_context *ctx, int qlen) const { return nullptr; }\n};\n\nstruct BasicPositionIdsAllocator {\n    ggml_tensor *operator()(ggml_context *ctx, int qlen) const { return ggml_new_tensor_1d(ctx, GGML_TYPE_I32, qlen); }\n};\n\nstruct GLMPositionIdsAllocator {\n    ggml_tensor *operator()(ggml_context *ctx, int qlen) const {\n        return ggml_new_tensor_1d(ctx, GGML_TYPE_I32, qlen * 2);\n    }\n};\n\ntemplate <typename Block, typename Norm, typename PositionIdsAllocator>\nclass BasicModel {\n  public:\n    BasicModel() = default;\n\n    BasicModel(Embedding word_embeddings, std::vector<Block> layers, Norm final_layernorm)\n        : word_embeddings(word_embeddings), layers(std::move(layers)), final_layernorm(final_layernorm) {}\n\n    BasicModel(ModelContext *mctx, const ModelConfig &config)\n        : word_embeddings(mctx, config.dtype, config.vocab_size, config.hidden_size),\n          layers(build_layers(mctx, config)), final_layernorm(mctx, config.hidden_size) {}\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *input_ids, ggml_tensor *images,\n                         const std::vector<int> &input_ids_vec, int n_past) const {\n        ggml_context *ctx = mctx->ctx_b.get();\n\n        ggml_tensor *hidden_states = forward_embeddings(mctx, input_ids, images, input_ids_vec, n_past);\n\n        const int qlen = hidden_states->ne[1];\n        const int kvlen = layers.front().attention.num_virtual_tokens + n_past + qlen;\n\n        ggml_tensor *position_ids = pos_ids_alloc_(ctx, qlen);\n        if (position_ids) {\n            ggml_set_name(position_ids, \"position_ids\");\n            ggml_set_input(position_ids);\n        }\n\n        ggml_tensor *attention_mask = nullptr;\n        if (n_past == 0) {\n            attention_mask = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, kvlen, qlen);\n            ggml_set_name(attention_mask, \"attention_mask\");\n            ggml_set_input(attention_mask);\n        }\n\n        for (const auto &layer : layers) {\n            hidden_states = layer.forward(mctx, hidden_states, attention_mask, position_ids, n_past);\n        }\n\n        hidden_states = final_layernorm.forward(mctx, hidden_states);\n        return hidden_states;\n    }\n\n    virtual ggml_tensor *forward_embeddings(ModelContext *mctx, ggml_tensor *input_ids, ggml_tensor *images,\n                                            const std::vector<int> &input_ids_vec, int n_past) const {\n        CHATGLM_CHECK(images == nullptr) << \"unimplemented\";\n        return word_embeddings.forward(mctx, input_ids);\n    }\n\n    void load_prefix_cache(const ModelConfig &config, ggml_tensor *past_key_values) {\n        // past_key_values: [l * 2, #h, v, d]\n        ModelContext mctx;\n\n        ggml_tensor *backend_past_key_values = ggml_new_tensor(mctx.ctx_kv.get(), past_key_values->type,\n                                                               ggml_n_dims(past_key_values), past_key_values->ne);\n        auto buf_kv =\n            unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx.ctx_kv.get(), mctx.backend.get()));\n        ggml_backend_tensor_set(backend_past_key_values, past_key_values->data, 0, ggml_nbytes(past_key_values));\n        past_key_values = backend_past_key_values;\n\n        const int head_size = config.hidden_size / config.num_attention_heads;\n        for (size_t i = 0; i < layers.size(); i++) {\n            auto &attn = layers[i].attention;\n            ggml_tensor *virtual_key =\n                ggml_view_3d(mctx.ctx_b.get(), past_key_values, head_size, config.num_virtual_tokens,\n                             config.num_key_value_heads, past_key_values->nb[1], past_key_values->nb[2],\n                             i * 2 * past_key_values->nb[3]); // [#h, v, d]\n            ggml_tensor *k_cache_view =\n                ggml_view_3d(mctx.ctx_b.get(), attn.k_cache, head_size, config.num_virtual_tokens,\n                             config.num_key_value_heads, attn.k_cache->nb[1], attn.k_cache->nb[2], 0); // [#h, v, d]\n            ggml_build_forward_expand(mctx.gf, ggml_cpy(mctx.ctx_b.get(), virtual_key, k_cache_view));\n\n            ggml_tensor *virtual_value = ggml_view_3d(\n                mctx.ctx_b.get(), past_key_values, head_size, config.num_virtual_tokens, config.num_key_value_heads,\n                past_key_values->nb[1], past_key_values->nb[2], (i * 2 + 1) * past_key_values->nb[3]); // [#h, v, d]\n            virtual_value = ggml_permute(mctx.ctx_b.get(), virtual_value, 1, 0, 2, 3);                 // [#h, d, v]\n            ggml_tensor *v_cache_view =\n                ggml_view_3d(mctx.ctx_b.get(), attn.v_cache, config.num_virtual_tokens, head_size,\n                             config.num_key_value_heads, attn.v_cache->nb[1], attn.v_cache->nb[2], 0); // [#h, d, v]\n            ggml_build_forward_expand(mctx.gf, ggml_cpy(mctx.ctx_b.get(), virtual_value, v_cache_view));\n        }\n\n        CHATGLM_CHECK(ggml_gallocr_alloc_graph(mctx.allocr.get(), mctx.gf));\n        CHATGLM_CHECK(ggml_backend_graph_compute(mctx.backend.get(), mctx.gf) == GGML_STATUS_SUCCESS);\n    }\n\n  private:\n    std::vector<Block> build_layers(ModelContext *mctx, const ModelConfig &config) {\n        std::vector<Block> layers;\n        layers.reserve(config.num_hidden_layers);\n        for (int layer_id = 0; layer_id < config.num_hidden_layers; layer_id++) {\n            layers.emplace_back(mctx, config.dtype, config.hidden_size, config.num_attention_heads,\n                                config.num_key_value_heads, config.intermediate_size, config.max_length,\n                                config.norm_eps, config.hidden_act, config.use_qkv_bias, config.use_dense_bias,\n                                config.interleaved_qkv, config.rope_type, config.rope_theta, config.num_virtual_tokens,\n                                true);\n        }\n        mctx->buf_kv =\n            unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx->ctx_kv.get(), mctx->backend.get()));\n        return layers;\n    }\n\n  public:\n    Embedding word_embeddings;\n    std::vector<Block> layers;\n    Norm final_layernorm;\n\n  private:\n    PositionIdsAllocator pos_ids_alloc_;\n};\n\nclass BaseStreamer {\n  public:\n    virtual ~BaseStreamer() = default;\n    virtual void put(const std::vector<int> &output_ids) = 0;\n    virtual void end() = 0;\n};\n\nclass StreamerGroup : public BaseStreamer {\n  public:\n    StreamerGroup(std::vector<std::shared_ptr<BaseStreamer>> streamers) : streamers_(std::move(streamers)) {}\n    void put(const std::vector<int> &output_ids) override;\n    void end() override;\n\n  private:\n    std::vector<std::shared_ptr<BaseStreamer>> streamers_;\n};\n\n// reference: https://github.com/huggingface/transformers/blob/main/src/transformers/generation/streamers.py\nclass TextStreamer : public BaseStreamer {\n  public:\n    TextStreamer(std::ostream &os, BaseTokenizer *tokenizer)\n        : os_(os), tokenizer_(tokenizer), is_prompt_(true), is_first_line_(true), print_len_(0) {}\n    void put(const std::vector<int> &output_ids) override;\n    void end() override;\n\n  private:\n    std::ostream &os_;\n    BaseTokenizer *tokenizer_;\n    bool is_prompt_;\n    bool is_first_line_;\n    std::vector<int> token_cache_;\n    int print_len_;\n};\n\nclass PerfStreamer : public BaseStreamer {\n  public:\n    PerfStreamer() : start_us_(0), prompt_us_(0), end_us_(0), num_prompt_tokens_(0), num_output_tokens_(0) {}\n\n    void put(const std::vector<int> &output_ids) override;\n    void end() override { end_us_ = ggml_time_us(); }\n\n    void reset();\n    std::string to_string() const;\n\n    int64_t num_prompt_tokens() const { return num_prompt_tokens_; }\n    int64_t prompt_total_time_us() const { return prompt_us_ - start_us_; }\n    int64_t prompt_token_time_us() const {\n        return num_prompt_tokens() ? prompt_total_time_us() / num_prompt_tokens() : 0;\n    }\n    int64_t num_output_tokens() const { return num_output_tokens_; }\n    int64_t output_total_time_us() const { return end_us_ - prompt_us_; }\n    int64_t output_token_time_us() const {\n        return num_output_tokens() ? output_total_time_us() / num_output_tokens() : 0;\n    }\n\n  private:\n    int64_t start_us_;\n    int64_t prompt_us_;\n    int64_t end_us_;\n    int64_t num_prompt_tokens_;\n    int64_t num_output_tokens_;\n};\n\nclass MappedFile {\n  public:\n    MappedFile(const std::string &path);\n    ~MappedFile();\n\n  public:\n    char *data;\n    size_t size;\n};\n\nstruct StateDict {\n    unique_ggml_context_t ctx;\n    unique_ggml_backend_buffer_t buf;\n    std::unordered_map<std::string, ggml_tensor *> kv;\n};\n\nclass ModelLoader {\n  public:\n    ModelLoader(char *data, size_t size) : data(data), size(size), ptr(data) {}\n\n    int64_t tell() const { return ptr - data; }\n\n    void seek(int64_t offset, int whence);\n\n    template <typename T>\n    T read_basic() {\n        T obj = *(T *)ptr;\n        ptr += sizeof(T);\n        return obj;\n    }\n\n    std::string read_string(size_t length);\n\n    StateDict read_state_dict();\n\n  private:\n    char *data;\n    size_t size;\n    char *ptr;\n};\n\n// ===== generation =====\n\nstruct GenerationConfig {\n    int max_length;\n    int max_new_tokens;\n    int max_context_length;\n    bool do_sample;\n    int top_k;\n    float top_p;\n    float temperature;\n    float repetition_penalty;\n\n    GenerationConfig(int max_length = 2048, int max_new_tokens = -1, int max_context_length = 512,\n                     bool do_sample = true, int top_k = 0, float top_p = 0.7, float temperature = 0.95,\n                     float repetition_penalty = 1.f)\n        : max_length(max_length), max_new_tokens(max_new_tokens), max_context_length(max_context_length),\n          do_sample(do_sample), top_k(top_k), top_p(top_p), temperature(temperature),\n          repetition_penalty(repetition_penalty) {}\n};\n\nstruct TokenIdScore {\n    int id;\n    float score;\n\n    TokenIdScore() = default;\n    TokenIdScore(int id, float score) : id(id), score(score) {}\n\n    bool operator<(const TokenIdScore &other) const { return score < other.score; }\n    bool operator>(const TokenIdScore &other) const { return score > other.score; }\n\n    friend std::ostream &operator<<(std::ostream &os, const TokenIdScore &self) {\n        return os << \"TokenIdScore(id=\" << self.id << \", score=\" << self.score << \")\";\n    }\n};\n\nclass BaseModelForCausalLM {\n  public:\n    BaseModelForCausalLM(ModelConfig config);\n    virtual ~BaseModelForCausalLM() = default;\n\n    virtual void load_state_dict(const StateDict &sd) = 0;\n\n    virtual ggml_tensor *forward(ModelContext *mctx, ggml_tensor *input_ids, ggml_tensor *images,\n                                 const std::vector<int> &input_ids_vec, int n_past, bool is_decoding) const = 0;\n\n    virtual void set_graph_inputs(const std::vector<int> &input_ids, const std::optional<Image> &image, int n_past,\n                                  int n_ctx) const = 0;\n\n    virtual int count_tokens(const std::vector<int> &input_ids, const std::optional<Image> &image) const = 0;\n\n    ggml_tensor *forward_graph_compute(const std::vector<int> &input_ids, const std::optional<Image> &image, int n_past,\n                                       int n_ctx, bool is_decoding);\n\n    std::vector<int> generate(const std::vector<int> &input_ids, const std::optional<Image> &image,\n                              const GenerationConfig &gen_config, BaseStreamer *streamer = nullptr);\n\n    int generate_next_token(const std::vector<int> &input_ids, const std::optional<Image> &image,\n                            const GenerationConfig &gen_config, int n_past, int n_ctx);\n\n    // logits processor\n    static void sampling_repetition_penalty(float *first, float *last, const std::vector<int> &input_ids,\n                                            float penalty);\n    // logits warper\n    static void sampling_temperature(float *first, float *last, float temp);\n    static void sampling_top_k(TokenIdScore *first, TokenIdScore *kth, TokenIdScore *last);\n    static TokenIdScore *sampling_top_p(TokenIdScore *first, TokenIdScore *last, float top_p);\n\n    static void sampling_softmax_inplace(TokenIdScore *first, TokenIdScore *last);\n\n  public:\n    ModelConfig config;\n\n  protected:\n    std::unique_ptr<ModelContext> mctx_;\n};\n\ntemplate <typename Model>\nclass BasicModelForCausalLM : public BaseModelForCausalLM {\n  protected:\n    BasicModelForCausalLM(const ModelConfig &config)\n        : BaseModelForCausalLM(config), transformer(mctx_.get(), config),\n          lm_head(mctx_.get(), config.dtype, config.hidden_size, config.vocab_size, false) {\n        if (config.tie_word_embeddings) {\n            lm_head.weight = transformer.word_embeddings.weight;\n        }\n    }\n\n  public:\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *input_ids, ggml_tensor *images,\n                         const std::vector<int> &input_ids_vec, int n_past, bool is_decoding) const override {\n        ggml_tensor *transformer_outputs = transformer.forward(mctx, input_ids, images, input_ids_vec, n_past);\n        // NOTE: only compute next token logits for decoding\n        if (is_decoding && transformer_outputs->ne[1] > 1) {\n            transformer_outputs = ggml_view_1d(mctx->ctx_b.get(), transformer_outputs, transformer_outputs->ne[0],\n                                               (transformer_outputs->ne[1] - 1) * transformer_outputs->nb[1]);\n        }\n        ggml_tensor *lm_logits = lm_head.forward(mctx, transformer_outputs);\n        return lm_logits;\n    }\n\n    void set_graph_inputs(const std::vector<int> &input_ids, const std::optional<Image> &image, int n_past,\n                          int n_ctx) const override {\n        transformer.set_graph_inputs(mctx_->gf, input_ids, image, n_past, n_ctx);\n    }\n\n    int count_tokens(const std::vector<int> &input_ids, const std::optional<Image> &image) const override {\n        CHATGLM_CHECK(!image) << \"unimplemented\";\n        return input_ids.size();\n    }\n\n    void load_prefix_cache(ggml_tensor *past_key_values) { transformer.load_prefix_cache(config, past_key_values); }\n\n  public:\n    Model transformer;\n    Linear lm_head;\n};\n\n// ===== ChatGLM-6B =====\n\nclass ChatGLMTokenizer : public BaseTokenizer {\n  public:\n    ChatGLMTokenizer(std::string_view serialized_model_proto);\n\n    std::vector<int> encode(const std::string &text, int max_length) const override;\n\n    std::string decode(const std::vector<int> &ids, bool skip_special_tokens = true) const override;\n\n    std::vector<int> apply_chat_template(const std::vector<ChatMessage> &messages, int max_length) const override;\n\n    static std::string apply_chat_template_text(const std::vector<ChatMessage> &messages);\n\n  private:\n    static std::string preprocess(const std::string &text);\n\n    static std::string postprocess(const std::string &text);\n\n  public:\n    sentencepiece::SentencePieceProcessor sp;\n    int bos_token_id;\n    int eos_token_id;\n    int mask_token_id;\n    int gmask_token_id;\n    int pad_token_id;\n};\n\n// NOTE: disable inplace norm since it causes nonsense on cuda when sequence length >= 144\nclass GLMBlock : public BasicBlock<LayerNorm, BasicMLP> {\n  public:\n    GLMBlock() = default;\n\n    GLMBlock(ModelContext *mctx, ggml_type dtype, int hidden_size, int num_attention_heads, int num_key_value_heads,\n             int intermediate_size, int max_length, float norm_eps, ActivationType hidden_act, bool use_qkv_bias,\n             bool use_dense_bias, bool interleaved_qkv, RopeType rope_type, float rope_theta, int num_virtual_tokens,\n             bool use_cache)\n        : BasicBlock(LayerNorm(mctx, hidden_size, norm_eps),\n                     BasicAttention(mctx, dtype, hidden_size, num_attention_heads, num_attention_heads, max_length,\n                                    use_qkv_bias, use_dense_bias, interleaved_qkv, rope_type, rope_theta,\n                                    num_virtual_tokens, use_cache),\n                     LayerNorm(mctx, hidden_size, norm_eps),\n                     BasicMLP(mctx, dtype, hidden_size, intermediate_size, hidden_act)),\n          alpha(std::sqrt(2.f * 28)) {}\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *hidden_states, ggml_tensor *attention_mask,\n                         ggml_tensor *position_ids, int n_past) const;\n\n  public:\n    float alpha;\n};\n\nclass ChatGLMModel : public BasicModel<GLMBlock, LayerNorm, GLMPositionIdsAllocator> {\n  public:\n    ChatGLMModel() = default;\n\n    ChatGLMModel(ModelContext *mctx, const ModelConfig &config) : BasicModel(mctx, config) {}\n\n    void set_graph_inputs(ggml_cgraph *gf, const std::vector<int> &input_ids, const std::optional<Image> &image,\n                          int n_past, int n_ctx) const;\n};\n\nclass ChatGLMForCausalLM : public BasicModelForCausalLM<ChatGLMModel> {\n  public:\n    ChatGLMForCausalLM(const ModelConfig &config) : BasicModelForCausalLM(config) {}\n\n    void load_state_dict(const StateDict &sd) override;\n\n  private:\n    StateDict state_dict() const;\n};\n\n// ===== ChatGLM2-6B =====\n\nclass ChatGLM2Tokenizer : public BaseTokenizer {\n  public:\n    ChatGLM2Tokenizer(std::string_view serialized_model_proto);\n\n    std::vector<int> encode(const std::string &text, int max_length) const override;\n\n    std::string decode(const std::vector<int> &ids, bool skip_special_tokens = true) const override;\n\n    std::vector<int> apply_chat_template(const std::vector<ChatMessage> &messages, int max_length) const override;\n\n    static std::string apply_chat_template_text(const std::vector<ChatMessage> &messages);\n\n  private:\n    bool is_special_id(int id) const;\n\n  public:\n    sentencepiece::SentencePieceProcessor sp;\n    int mask_token_id;\n    int gmask_token_id;\n    int smask_token_id;\n    int sop_token_id;\n    int eop_token_id;\n};\n\nusing GLM2Block = BasicBlock<RMSNorm, BasicGLU>;\n\nclass ChatGLM2Model : public BasicModel<GLM2Block, RMSNorm, BasicPositionIdsAllocator> {\n  public:\n    ChatGLM2Model() = default;\n\n    ChatGLM2Model(ModelContext *mctx, const ModelConfig &config) : BasicModel(mctx, config) {}\n\n    void set_graph_inputs(ggml_cgraph *gf, const std::vector<int> &input_ids, const std::optional<Image> &image,\n                          int n_past, int n_ctx) const;\n};\n\nclass ChatGLM2ForCausalLM : public BasicModelForCausalLM<ChatGLM2Model> {\n  public:\n    ChatGLM2ForCausalLM(const ModelConfig &config) : BasicModelForCausalLM(config) {}\n\n    void load_state_dict(const StateDict &sd) override;\n\n    static void load_state_dict(ModelContext *mctx, StateDict &dst, const StateDict &src);\n\n  private:\n    StateDict state_dict() const;\n};\n\n// ===== ChatGLM3-6B =====\n\nclass ChatGLM3Tokenizer : public BaseTokenizer {\n  public:\n    ChatGLM3Tokenizer(std::string_view serialized_model_proto);\n\n    std::vector<int> encode(const std::string &text, int max_length) const override;\n\n    std::string decode(const std::vector<int> &ids, bool skip_special_tokens = true) const override;\n\n    std::vector<int> apply_chat_template(const std::vector<ChatMessage> &messages, int max_length) const override;\n\n    ChatMessage decode_message(const std::vector<int> &ids) const override;\n\n  private:\n    std::vector<int> encode_single_message(const std::string &role, const std::string &content) const;\n\n    static std::string remove_special_tokens(const std::string &text);\n\n    int get_command(const std::string &token) const;\n\n    bool is_special_id(int id) const;\n\n    static void truncate(std::vector<int> &ids, int max_length);\n\n  public:\n    sentencepiece::SentencePieceProcessor sp;\n    int mask_token_id;\n    int gmask_token_id;\n    int smask_token_id;\n    int sop_token_id;\n    int eop_token_id;\n    int system_token_id;\n    int user_token_id;\n    int assistant_token_id;\n    int observation_token_id;\n    std::unordered_map<std::string, int> special_tokens;\n    std::unordered_map<int, std::string> index_special_tokens;\n};\n\nusing ChatGLM3Model = ChatGLM2Model;\n\nusing ChatGLM3ForCausalLM = ChatGLM2ForCausalLM;\n\n// ===== ChatGLM4-9B =====\n\n// C++ port of BPE algorithm from https://github.com/openai/tiktoken/blob/main/src/lib.rs\nclass TiktokenCoreBPE {\n  public:\n    TiktokenCoreBPE() = default;\n\n    TiktokenCoreBPE(std::unordered_map<std::string, int> encoder,\n                    std::unordered_map<std::string, int> special_tokens_encoder, const std::string &pattern);\n\n    std::vector<int> encode_ordinary(const std::string &text) const { return _encode_ordinary_native(text); }\n\n    std::string decode(const std::vector<int> &tokens) const { return _decode_native(tokens); }\n\n  private:\n    static std::vector<std::pair<size_t, int>> _byte_pair_merge(const std::unordered_map<std::string, int> &ranks,\n                                                                const std::string &piece);\n\n    static std::vector<int> byte_pair_encode(const std::string &piece,\n                                             const std::unordered_map<std::string, int> &ranks);\n\n    std::vector<int> _encode_ordinary_native(const std::string &text) const;\n\n    std::string _decode_native(const std::vector<int> &tokens) const;\n\n  public:\n    std::unique_ptr<RE2> regex;\n    std::unordered_map<std::string, int> encoder;\n    std::unordered_map<std::string, int> special_tokens_encoder;\n    std::unordered_map<int, std::string> decoder;\n    std::unordered_map<int, std::string> special_tokens_decoder;\n};\n\nclass ChatGLM4Tokenizer : public BaseTokenizer {\n  public:\n    ChatGLM4Tokenizer(const std::string &vocab_text);\n\n    std::vector<int> encode(const std::string &text, int max_length) const override;\n\n    std::string decode(const std::vector<int> &ids, bool skip_special_tokens = true) const override;\n\n    std::vector<int> apply_chat_template(const std::vector<ChatMessage> &messages, int max_length) const override;\n\n    ChatMessage decode_message(const std::vector<int> &ids) const override;\n\n  private:\n    static void truncate(std::vector<int> &ids, int max_length);\n\n  public:\n    TiktokenCoreBPE core_bpe;\n    int eos_token_id;\n    // int mask_token_id;\n    int gmask_token_id;\n    // int smask_token_id;\n    int sop_token_id;\n    // int eop_token_id;\n    // int system_token_id;\n    int user_token_id;\n    int assistant_token_id;\n    int observation_token_id;\n    int boi_token_id;\n    int eoi_token_id;\n};\n\nusing ChatGLM4Model = ChatGLM2Model;\n\nusing ChatGLM4ForCausalLM = ChatGLM2ForCausalLM;\n\n// ===== GLM4V-9B =====\n\nclass Conv2d {\n  public:\n    Conv2d() : weight(nullptr), bias(nullptr), stride(0) {}\n\n    Conv2d(ModelContext *mctx, int in_channels, int out_channels, int kernel_size, int stride)\n        : weight(ggml_new_tensor_4d(mctx->ctx_w.get(), GGML_TYPE_F16, kernel_size, kernel_size, in_channels,\n                                    out_channels)),\n          bias(ggml_new_tensor_3d(mctx->ctx_w.get(), GGML_TYPE_F32, 1, 1, out_channels)), stride(stride) {}\n\n    int in_channels() const { return weight->ne[2]; }\n\n    int out_channels() const { return weight->ne[3]; }\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *input) const;\n\n  public:\n    ggml_tensor *weight;\n    ggml_tensor *bias;\n    int stride;\n};\n\nclass PatchEmbedding {\n  public:\n    PatchEmbedding() = default;\n\n    PatchEmbedding(ModelContext *mctx, int in_channels, int hidden_size, int patch_size, int num_positions)\n        : proj(mctx, in_channels, hidden_size, patch_size, patch_size),\n          position_embedding(mctx, GGML_TYPE_F32, num_positions, hidden_size),\n          cls_embedding(ggml_new_tensor_1d(mctx->ctx_w.get(), GGML_TYPE_F16, hidden_size)) {}\n\n    int hidden_size() const { return proj.out_channels(); }\n\n    int num_positions() const { return position_embedding.num_embeddings(); }\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *input) const;\n\n  public:\n    Conv2d proj;\n    Embedding position_embedding;\n    ggml_tensor *cls_embedding = nullptr; // [H]\n};\n\nclass EVA2CLIPBlock : public BasicBlock<LayerNorm, BasicMLP> {\n  public:\n    EVA2CLIPBlock() = default;\n\n    EVA2CLIPBlock(ModelContext *mctx, ggml_type dtype, int hidden_size, int num_attention_heads,\n                  int num_key_value_heads, int intermediate_size, int max_length, float norm_eps,\n                  ActivationType hidden_act, bool use_qkv_bias, bool use_dense_bias, bool interleaved_qkv,\n                  RopeType rope_type, float rope_theta, int num_virtual_tokens, bool use_cache)\n        : BasicBlock(mctx, dtype, hidden_size, num_attention_heads, num_key_value_heads, intermediate_size, max_length,\n                     norm_eps, hidden_act, use_qkv_bias, use_dense_bias, interleaved_qkv, rope_type, rope_theta,\n                     num_virtual_tokens, use_cache) {}\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *hidden_states, ggml_tensor *attention_mask,\n                         ggml_tensor *position_ids, int n_past) const;\n};\n\nclass EVA2CLIPTransformer {\n  public:\n    EVA2CLIPTransformer() = default;\n\n    EVA2CLIPTransformer(ModelContext *mctx, const VisionModelConfig &config);\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *hidden_states, ggml_tensor *attention_mask) const;\n\n  public:\n    std::vector<EVA2CLIPBlock> layers;\n};\n\nclass EVA2CLIPModel {\n  public:\n    EVA2CLIPModel() = default;\n\n    EVA2CLIPModel(ModelContext *mctx, const ModelConfig &config)\n        : patch_embedding(mctx, config.vision.in_channels, config.vision.hidden_size, config.vision.patch_size,\n                          config.vision.num_positions),\n          transformer(mctx, config.vision), conv(mctx, config.vision.hidden_size, config.hidden_size, 2, 2),\n          linear_proj(mctx, config.vision.dtype, config.hidden_size, config.hidden_size, false),\n          norm1(mctx, config.hidden_size),\n          glu(mctx, config.vision.dtype, config.hidden_size, config.intermediate_size, ActivationType::SILU),\n          boi(ggml_new_tensor_1d(mctx->ctx_w.get(), GGML_TYPE_F16, config.hidden_size)),\n          eoi(ggml_new_tensor_1d(mctx->ctx_w.get(), GGML_TYPE_F16, config.hidden_size)),\n          scaling_factor(config.vision.scaling_factor) {}\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *input) const;\n\n  public:\n    PatchEmbedding patch_embedding;\n    EVA2CLIPTransformer transformer;\n    Conv2d conv;\n    Linear linear_proj;\n    LayerNorm norm1;\n    BasicGLU glu;\n    ggml_tensor *boi = nullptr;\n    ggml_tensor *eoi = nullptr;\n    float scaling_factor = 0.f;\n};\n\nclass ChatGLM4VModel : public ChatGLM4Model {\n  public:\n    ChatGLM4VModel() = default;\n\n    ChatGLM4VModel(ModelContext *mctx, const ModelConfig &config)\n        : ChatGLM4Model(mctx, config), config(config), vision(mctx, config) {}\n\n    ggml_tensor *forward_embeddings(ModelContext *mctx, ggml_tensor *input_ids, ggml_tensor *images,\n                                    const std::vector<int> &input_ids_vec, int n_past) const override;\n\n    int num_vision_tokens() const {\n        auto square = [](int x) { return x * x; };\n        return square(config.vision.image_size / config.vision.patch_size / 2);\n    }\n\n    void set_graph_inputs(ggml_cgraph *gf, const std::vector<int> &input_ids, const std::optional<Image> &image,\n                          int n_past, int n_ctx) const;\n\n  public:\n    ModelConfig config;\n    EVA2CLIPModel vision;\n};\n\nclass ChatGLM4VForCausalLM : public BasicModelForCausalLM<ChatGLM4VModel> {\n  public:\n    ChatGLM4VForCausalLM(const ModelConfig &config) : BasicModelForCausalLM(config) {}\n\n    int count_tokens(const std::vector<int> &input_ids, const std::optional<Image> &image) const override;\n\n    void load_state_dict(const StateDict &sd) override;\n\n  private:\n    StateDict state_dict() const;\n};\n\n// ===== pipeline =====\n\nclass Pipeline {\n  public:\n    Pipeline(const std::string &path, int max_length = -1);\n\n    std::vector<int> generate(const std::vector<int> &input_ids, const std::optional<Image> &image,\n                              const GenerationConfig &gen_config, BaseStreamer *streamer = nullptr) const;\n\n    std::string generate(const std::string &prompt, const GenerationConfig &gen_config,\n                         BaseStreamer *streamer = nullptr) const;\n\n    ChatMessage chat(const std::vector<ChatMessage> &messages, const GenerationConfig &gen_config,\n                     BaseStreamer *streamer = nullptr) const;\n\n  protected:\n    std::unique_ptr<MappedFile> mapped_file_;\n\n  public:\n    std::unique_ptr<BaseTokenizer> tokenizer;\n    std::unique_ptr<BaseModelForCausalLM> model;\n};\n\n} // namespace chatglm\n"
        },
        {
          "name": "chatglm_cpp",
          "type": "tree",
          "content": null
        },
        {
          "name": "chatglm_pybind.cpp",
          "type": "blob",
          "size": 10.07421875,
          "content": "#include \"chatglm.h\"\n#include <pybind11/pybind11.h>\n#include <pybind11/stl.h>\n\nnamespace chatglm {\n\nnamespace py = pybind11;\nusing namespace pybind11::literals;\n\nclass PyBaseTokenizer : public BaseTokenizer {\n  public:\n    using BaseTokenizer::BaseTokenizer;\n\n    std::vector<int> encode(const std::string &text, int max_length) const override {\n        PYBIND11_OVERRIDE_PURE(std::vector<int>, BaseTokenizer, encode, text, max_length);\n    }\n    std::string decode(const std::vector<int> &ids, bool skip_special_tokens) const override {\n        PYBIND11_OVERLOAD_PURE(std::string, BaseTokenizer, decode, ids, skip_special_tokens);\n    }\n    std::vector<int> apply_chat_template(const std::vector<ChatMessage> &messages, int max_length) const override {\n        PYBIND11_OVERLOAD_PURE(std::vector<int>, BaseTokenizer, apply_chat_template, messages, max_length);\n    }\n};\n\nclass PyBaseModelForCausalLM : public BaseModelForCausalLM {\n  public:\n    using BaseModelForCausalLM::BaseModelForCausalLM;\n\n    void load_state_dict(const StateDict &sd) override {\n        PYBIND11_OVERLOAD_PURE(void, PyBaseModelForCausalLM, load_state_dict, sd);\n    }\n\n    ggml_tensor *forward(ModelContext *mctx, ggml_tensor *input_ids, ggml_tensor *images,\n                         const std::vector<int> &input_ids_vec, int n_past, bool is_decoding) const override {\n        PYBIND11_OVERLOAD_PURE(ggml_tensor *, PyBaseModelForCausalLM, forward, mctx, input_ids, images, input_ids_vec,\n                               n_past, is_decoding);\n    }\n\n    void set_graph_inputs(const std::vector<int> &input_ids, const std::optional<Image> &image, int n_past,\n                          int n_ctx) const override {\n        PYBIND11_OVERLOAD_PURE(void, PyBaseModelForCausalLM, set_graph_inputs, input_ids, image, n_past, n_ctx);\n    }\n\n    int count_tokens(const std::vector<int> &input_ids, const std::optional<Image> &image) const override {\n        PYBIND11_OVERLOAD_PURE(int, PyBaseModelForCausalLM, count_tokens, input_ids, image);\n    }\n};\n\ntemplate <typename T>\nstatic inline std::string to_string(const T &obj) {\n    std::ostringstream oss;\n    oss << obj;\n    return oss.str();\n}\n\nPYBIND11_MODULE(_C, m) {\n    m.doc() = \"ChatGLM.cpp python binding\";\n\n    py::enum_<ModelType>(m, \"ModelType\")\n        .value(\"CHATGLM\", ModelType::CHATGLM)\n        .value(\"CHATGLM2\", ModelType::CHATGLM2)\n        .value(\"CHATGLM3\", ModelType::CHATGLM3)\n        .value(\"CHATGLM4\", ModelType::CHATGLM4);\n\n    py::class_<VisionModelConfig>(m, \"VisionModelConfig\")\n        // .def_readonly(\"dtype\", &VisionModelConfig::dtype)\n        // .def_readonly(\"hidden_act\", &VisionModelConfig::hidden_act)\n        .def_readonly(\"hidden_size\", &VisionModelConfig::hidden_size)\n        .def_readonly(\"image_size\", &VisionModelConfig::image_size)\n        .def_readonly(\"in_channels\", &VisionModelConfig::in_channels)\n        .def_readonly(\"intermediate_size\", &VisionModelConfig::intermediate_size)\n        .def_readonly(\"norm_eps\", &VisionModelConfig::norm_eps)\n        .def_readonly(\"num_attention_heads\", &VisionModelConfig::num_attention_heads)\n        .def_readonly(\"num_hidden_layers\", &VisionModelConfig::num_hidden_layers)\n        .def_readonly(\"num_positions\", &VisionModelConfig::num_positions)\n        .def_readonly(\"patch_size\", &VisionModelConfig::patch_size)\n        .def_readonly(\"scaling_factor\", &VisionModelConfig::scaling_factor);\n\n    py::class_<ModelConfig>(m, \"ModelConfig\")\n        .def_readonly(\"model_type\", &ModelConfig::model_type)\n        // .def_readonly(\"dtype\", &ModelConfig::dtype)\n        .def_readonly(\"vocab_size\", &ModelConfig::vocab_size)\n        .def_readonly(\"hidden_size\", &ModelConfig::hidden_size)\n        .def_readonly(\"num_attention_heads\", &ModelConfig::num_attention_heads)\n        .def_readonly(\"num_key_value_heads\", &ModelConfig::num_key_value_heads)\n        .def_readonly(\"num_hidden_layers\", &ModelConfig::num_hidden_layers)\n        .def_readonly(\"intermediate_size\", &ModelConfig::intermediate_size)\n        .def_readonly(\"norm_eps\", &ModelConfig::norm_eps)\n        .def_readonly(\"max_length\", &ModelConfig::max_length)\n        .def_readonly(\"bos_token_id\", &ModelConfig::bos_token_id)\n        .def_readonly(\"eos_token_id\", &ModelConfig::eos_token_id)\n        .def_readonly(\"pad_token_id\", &ModelConfig::pad_token_id)\n        .def_readonly(\"sep_token_id\", &ModelConfig::sep_token_id)\n        .def_readonly(\"extra_eos_token_ids\", &ModelConfig::extra_eos_token_ids)\n        .def_readonly(\"vision\", &ModelConfig::vision)\n        .def_property_readonly(\"model_type_name\", &ModelConfig::model_type_name);\n\n    py::class_<GenerationConfig>(m, \"GenerationConfig\")\n        .def(py::init<int, int, int, bool, int, float, float, float>(), \"max_length\"_a = 2048, \"max_new_tokens\"_a = -1,\n             \"max_context_length\"_a = 512, \"do_sample\"_a = true, \"top_k\"_a = 0, \"top_p\"_a = 0.7, \"temperature\"_a = 0.95,\n             \"repetition_penalty\"_a = 1.0)\n        .def_readwrite(\"max_length\", &GenerationConfig::max_length)\n        .def_readwrite(\"max_new_tokens\", &GenerationConfig::max_new_tokens)\n        .def_readwrite(\"max_context_length\", &GenerationConfig::max_context_length)\n        .def_readwrite(\"do_sample\", &GenerationConfig::do_sample)\n        .def_readwrite(\"top_k\", &GenerationConfig::top_k)\n        .def_readwrite(\"top_p\", &GenerationConfig::top_p)\n        .def_readwrite(\"temperature\", &GenerationConfig::temperature)\n        .def_readwrite(\"repetition_penalty\", &GenerationConfig::repetition_penalty);\n\n    py::class_<FunctionMessage>(m, \"FunctionMessage\")\n        .def(\"__repr__\", &to_string<FunctionMessage>)\n        .def(\"__str__\", &to_string<FunctionMessage>)\n        .def_readwrite(\"name\", &FunctionMessage::name)\n        .def_readwrite(\"arguments\", &FunctionMessage::arguments);\n\n    py::class_<CodeMessage>(m, \"CodeMessage\")\n        .def(\"__repr__\", &to_string<CodeMessage>)\n        .def(\"__str__\", &to_string<CodeMessage>)\n        .def_readwrite(\"input\", &CodeMessage::input);\n\n    py::class_<ToolCallMessage>(m, \"ToolCallMessage\")\n        .def(\"__repr__\", &to_string<ToolCallMessage>)\n        .def(\"__str__\", &to_string<ToolCallMessage>)\n        .def_readwrite(\"type\", &ToolCallMessage::type)\n        .def_readwrite(\"function\", &ToolCallMessage::function)\n        .def_readwrite(\"code\", &ToolCallMessage::code);\n\n    py::class_<Image>(m, \"Image\", py::buffer_protocol())\n        .def(py::init([](py::buffer b) {\n            py::buffer_info info = b.request();\n\n            CHATGLM_CHECK(info.format == py::format_descriptor<uint8_t>::format())\n                << \"Incompatible format: expect a byte array!\";\n            CHATGLM_CHECK(info.ndim == 3 && info.shape[2] == 3) << \"Only support RGB image for now\";\n\n            for (int i = 1; i < info.ndim; i++) {\n                CHATGLM_CHECK(info.strides[i] * info.shape[i] == info.strides[i - 1])\n                    << \"Only support contiguous array\";\n            }\n\n            return Image(info.shape[1], info.shape[0], info.shape[2], (uint8_t *)info.ptr);\n        }))\n        .def_buffer([](Image &self) {\n            return py::buffer_info(\n                self.pixels.data(), sizeof(uint8_t), py::format_descriptor<uint8_t>::format(), 3,\n                {self.height, self.width, self.channels},\n                {self.width * self.channels * sizeof(uint8_t), self.channels * sizeof(uint8_t), sizeof(uint8_t)});\n        })\n        .def(\"__repr__\", &to_string<Image>)\n        .def(\"__str__\", &to_string<Image>)\n        .def_readonly(\"width\", &Image::width)\n        .def_readonly(\"height\", &Image::height)\n        .def_readonly(\"channels\", &Image::channels)\n        .def_readonly(\"pixels\", &Image::pixels);\n\n    py::class_<ChatMessage>(m, \"ChatMessage\")\n        .def(py::init<std::string, std::string, std::optional<Image>, std::vector<ToolCallMessage>>(), \"role\"_a,\n             \"content\"_a, \"image\"_a = std::nullopt, \"tool_calls\"_a = std::vector<ToolCallMessage>{})\n        .def(\"__repr__\", &to_string<ChatMessage>)\n        .def(\"__str__\", &to_string<ChatMessage>)\n        .def_readonly_static(\"ROLE_SYSTEM\", &ChatMessage::ROLE_SYSTEM)\n        .def_readonly_static(\"ROLE_USER\", &ChatMessage::ROLE_USER)\n        .def_readonly_static(\"ROLE_ASSISTANT\", &ChatMessage::ROLE_ASSISTANT)\n        .def_readonly_static(\"ROLE_OBSERVATION\", &ChatMessage::ROLE_OBSERVATION)\n        .def_readwrite(\"role\", &ChatMessage::role)\n        .def_readwrite(\"content\", &ChatMessage::content)\n        .def_readwrite(\"image\", &ChatMessage::image)\n        .def_readwrite(\"tool_calls\", &ChatMessage::tool_calls);\n\n    py::class_<BaseTokenizer, PyBaseTokenizer>(m, \"BaseTokenizer\")\n        .def(\"encode\", &BaseTokenizer::encode, \"text\"_a, \"max_length\"_a)\n        .def(\"decode\", &BaseTokenizer::decode, \"ids\"_a, \"skip_special_tokens\"_a = true)\n        .def(\"apply_chat_template\", &BaseTokenizer::apply_chat_template, \"messages\"_a, \"max_length\"_a)\n        .def(\"decode_message\", &BaseTokenizer::decode_message, \"ids\"_a);\n\n    py::class_<BaseModelForCausalLM, PyBaseModelForCausalLM>(m, \"BaseModelForCausalLM\")\n        .def(\"generate_next_token\", &BaseModelForCausalLM::generate_next_token, \"input_ids\"_a, \"image\"_a,\n             \"gen_config\"_a, \"n_past\"_a, \"n_ctx\"_a)\n        .def(\"count_tokens\", &BaseModelForCausalLM::count_tokens, \"input_ids\"_a, \"image\"_a)\n        .def_readonly(\"config\", &BaseModelForCausalLM::config);\n\n    // ===== ChatGLM =====\n\n    py::class_<ChatGLMTokenizer, BaseTokenizer>(m, \"ChatGLMTokenizer\");\n\n    py::class_<ChatGLMForCausalLM, BaseModelForCausalLM>(m, \"ChatGLMForCausalLM\");\n\n    // ===== ChatGLM2 =====\n\n    py::class_<ChatGLM2Tokenizer, BaseTokenizer>(m, \"ChatGLM2Tokenizer\");\n\n    py::class_<ChatGLM2ForCausalLM, BaseModelForCausalLM>(m, \"ChatGLM2ForCausalLM\");\n\n    // ===== ChatGLM3 =====\n\n    py::class_<ChatGLM3Tokenizer, BaseTokenizer>(m, \"ChatGLM3Tokenizer\");\n\n    // ===== ChatGLM4 =====\n\n    py::class_<ChatGLM4Tokenizer, BaseTokenizer>(m, \"ChatGLM4Tokenizer\");\n\n    // ===== Pipeline ====\n\n    py::class_<Pipeline>(m, \"Pipeline\")\n        .def(py::init<const std::string &, int>(), \"path\"_a, \"max_length\"_a = -1)\n        .def_property_readonly(\"model\", [](const Pipeline &self) { return self.model.get(); })\n        .def_property_readonly(\"tokenizer\", [](const Pipeline &self) { return self.tokenizer.get(); });\n}\n\n} // namespace chatglm\n"
        },
        {
          "name": "chatglm_test.cpp",
          "type": "blob",
          "size": 82.134765625,
          "content": "#include \"chatglm.h\"\n#include <filesystem>\n#include <fstream>\n#include <gtest/gtest.h>\n#include <random>\n\n#ifdef GGML_USE_CUDA\n#include <cuda_runtime.h>\n#include <ggml-cuda.h>\n#endif\n\nnamespace chatglm {\n\nnamespace fs = std::filesystem;\n\nstatic inline void expect_all_close(ggml_tensor *a, ggml_tensor *b, float atol = 1e-5f, float rtol = 0.f) {\n    ASSERT_EQ(a->type, b->type);\n    ASSERT_EQ(a->type, GGML_TYPE_F32);\n    ASSERT_EQ(ggml_nelements(a), ggml_nelements(b));\n\n    int64_t numel = ggml_nelements(a);\n\n    std::vector<float> a_buf(numel);\n    ggml_backend_tensor_get(a, a_buf.data(), 0, numel * sizeof(float));\n\n    std::vector<float> b_buf(numel);\n    ggml_backend_tensor_get(b, b_buf.data(), 0, numel * sizeof(float));\n\n    float max_abs_diff = 0.f;\n    float max_rel_diff = 0.f;\n    int64_t num_mismatch = 0;\n    for (int64_t i = 0; i < numel; i++) {\n        float ai = a_buf[i];\n        float bi = b_buf[i];\n        EXPECT_TRUE(std::isfinite(ai) && std::isfinite(bi));\n        float abs_diff = std::abs(ai - bi);\n        max_abs_diff = std::max(max_abs_diff, abs_diff);\n        if (abs_diff >= atol + rtol * std::abs(bi)) {\n            num_mismatch++;\n        }\n        float rel_diff = abs_diff / std::abs(bi);\n        max_rel_diff = std::max(max_rel_diff, rel_diff);\n    }\n    EXPECT_TRUE(num_mismatch == 0) << \"Tensors are not close!\\n\\n\"\n                                   << \"Mismatched elements: \" << num_mismatch << \" / \" << numel << \" (\"\n                                   << num_mismatch * 100 / numel << \"%)\\n\"\n                                   << \"Greatest absolute difference: \" << max_abs_diff << \" (up to \" << std::scientific\n                                   << atol << std::defaultfloat << \" allowed)\\n\"\n                                   << \"Greatest relative difference: \" << max_rel_diff << \" (up to \" << std::scientific\n                                   << rtol << std::defaultfloat << \" allowed)\\n\";\n}\n\nstatic inline void read_backend_tensor_data(std::istream &is, ggml_tensor *tensor) {\n    std::vector<no_init<char>> buf(ggml_nbytes(tensor));\n    CHATGLM_CHECK(is.read((char *)buf.data(), buf.size()));\n    ggml_backend_tensor_set(tensor, buf.data(), 0, buf.size());\n}\n\nstatic inline void _fill(ggml_tensor *tensor, const std::vector<float> &values) {\n    switch (tensor->type) {\n    case GGML_TYPE_F32: {\n        ggml_backend_tensor_set(tensor, values.data(), 0, sizeof(float) * values.size());\n    } break;\n    case GGML_TYPE_F16: {\n        std::vector<ggml_fp16_t> fp16_buf(values.size());\n        ggml_fp32_to_fp16_row(values.data(), fp16_buf.data(), fp16_buf.size());\n        ggml_backend_tensor_set(tensor, fp16_buf.data(), 0, fp16_buf.size());\n    } break;\n    case GGML_TYPE_Q4_0:\n    case GGML_TYPE_Q4_1:\n    case GGML_TYPE_Q5_0:\n    case GGML_TYPE_Q5_1:\n    case GGML_TYPE_Q8_0: {\n        std::vector<no_init<char>> q_buf(ggml_nbytes(tensor));\n        ggml_quantize_chunk(tensor->type, values.data(), q_buf.data(), 0, ggml_nelements(tensor) / tensor->ne[0],\n                            tensor->ne[0], nullptr);\n        ggml_backend_tensor_set(tensor, q_buf.data(), 0, ggml_nbytes(tensor));\n    } break;\n    default:\n        CHATGLM_THROW << \"unsupported dtype \" << tensor->type;\n    }\n}\n\nstatic inline float random() { return rand() / (float)RAND_MAX; }\n\nstatic inline float random(float lo, float hi) { return lo + random() * (hi - lo); }\n\nstatic inline void random_(ggml_tensor *tensor) {\n    std::vector<float> values(ggml_nelements(tensor));\n    for (float &v : values) {\n        v = random();\n    }\n    _fill(tensor, values);\n}\n\nstatic inline float randn() {\n    thread_local std::random_device rd{};\n    thread_local std::mt19937 gen{rd()};\n    std::normal_distribution<float> d;\n    return d(gen);\n}\n\nstatic inline void randn_(ggml_tensor *tensor) {\n    std::vector<float> values(ggml_nelements(tensor));\n    for (float &v : values) {\n        v = randn();\n    }\n    _fill(tensor, values);\n}\n\n// return elapsed time in milliseconds\nstatic inline float timeit(std::function<void()> fn, int warmup, int active) {\n    for (int i = 0; i < warmup; i++) {\n        fn();\n    }\n\n#ifdef GGML_USE_CUDA\n    CHATGLM_CHECK_CUDA(cudaDeviceSynchronize());\n#endif\n    int64_t start_us = ggml_time_us();\n    for (int i = 0; i < active; i++) {\n        fn();\n    }\n#ifdef GGML_USE_CUDA\n    CHATGLM_CHECK_CUDA(cudaDeviceSynchronize());\n#endif\n    int64_t end_us = ggml_time_us();\n\n    float elapsed_ms = (end_us - start_us) / 1000.f;\n    return elapsed_ms / active;\n}\n\nstatic std::vector<int> extract_sorted_ids(std::vector<TokenIdScore> &token_scores) {\n    std::vector<int> token_ids(token_scores.size());\n    for (size_t i = 0; i < token_scores.size(); i++) {\n        token_ids[i] = token_scores[i].id;\n    }\n    std::sort(token_ids.begin(), token_ids.end());\n    return token_ids;\n}\n\nTEST(Sampling, RepetitionPenalty) {\n    constexpr float penalty = 1.2;\n    std::vector<float> logits{0.96, 1.2, -2, -0.8, 0, 2.4, -1};\n    std::vector<int> input_ids{0, 2, 5, 2};\n    // reference\n    std::vector<float> target{0.8, 1.2, -2.4, -0.8, 0, 2, -1};\n    // test\n    BaseModelForCausalLM::sampling_repetition_penalty(logits.data(), logits.data() + logits.size(), input_ids, penalty);\n    // compare\n    for (size_t i = 0; i < logits.size(); i++) {\n        EXPECT_FLOAT_EQ(logits[i], target[i]);\n    }\n}\n\nTEST(DISABLED_Sampling, BenchmarkRepetitionPenalty) {\n    const float penalty = 1.2;\n    constexpr size_t vocab_size = 128000;\n    constexpr int seq_len = 32000;\n    std::vector<float> logits(vocab_size);\n    for (auto &x : logits) {\n        x = random(-1, 1);\n    }\n    std::vector<int> input_ids(seq_len);\n    for (size_t i = 0; i < input_ids.size(); i++) {\n        input_ids[i] = i;\n    }\n\n    auto fn = [&logits, &input_ids, penalty] {\n        BaseModelForCausalLM::sampling_repetition_penalty(logits.data(), logits.data() + logits.size(), input_ids,\n                                                          penalty);\n    };\n    auto elapsed_ms = timeit(fn, 2, 100);\n    std::cout << \"[\" << ::testing::UnitTest::GetInstance()->current_test_info()->name() << \"] \" << elapsed_ms\n              << \" ms\\n\";\n}\n\nTEST(Sampling, Temperature) {\n    constexpr float temp = 0.7;\n    std::vector<float> logits(64);\n    for (float &v : logits) {\n        v = random();\n    }\n    // reference\n    std::vector<float> target = logits;\n    for (auto &v : target) {\n        v /= temp;\n    }\n    // test\n    BaseModelForCausalLM::sampling_temperature(logits.data(), logits.data() + logits.size(), temp);\n    // compare\n    for (size_t i = 0; i < logits.size(); i++) {\n        EXPECT_FLOAT_EQ(logits[i], target[i]);\n    }\n}\n\nTEST(Sampling, TopK) {\n    constexpr int top_k = 20;\n    std::vector<TokenIdScore> token_scores(64);\n    for (size_t i = 0; i < token_scores.size(); i++) {\n        token_scores[i] = TokenIdScore(i, random());\n    }\n\n    // reference\n    std::vector<TokenIdScore> target = token_scores;\n    std::sort(target.begin(), target.end(), std::greater<TokenIdScore>());\n    target.resize(top_k);\n\n    // test\n    BaseModelForCausalLM::sampling_top_k(token_scores.data(), token_scores.data() + top_k,\n                                         token_scores.data() + token_scores.size());\n    token_scores.resize(top_k);\n\n    // sort & compare\n    EXPECT_EQ(extract_sorted_ids(token_scores), extract_sorted_ids(target));\n}\n\nstatic void reference_top_p(std::vector<TokenIdScore> &token_scores, float top_p) {\n    std::sort(token_scores.begin(), token_scores.end(), std::greater<TokenIdScore>());\n    BaseModelForCausalLM::sampling_softmax_inplace(token_scores.data(), token_scores.data() + token_scores.size());\n    float cumsum = 0.f;\n    for (size_t i = 0; i < token_scores.size(); i++) {\n        cumsum += token_scores[i].score;\n        if (cumsum >= top_p) {\n            token_scores.resize(i + 1);\n            break;\n        }\n    }\n}\n\nTEST(Sampling, TopP) {\n    constexpr float top_p = 0.7;\n    for (int i = 0; i < 10; i++) {\n        std::vector<TokenIdScore> token_scores(1024);\n        for (size_t i = 0; i < token_scores.size(); i++) {\n            token_scores[i] = TokenIdScore(i, random());\n        }\n\n        // reference\n        std::vector<TokenIdScore> target = token_scores;\n        reference_top_p(target, top_p);\n        EXPECT_TRUE(!token_scores.empty());\n\n        // test\n        TokenIdScore *pos =\n            BaseModelForCausalLM::sampling_top_p(token_scores.data(), token_scores.data() + token_scores.size(), top_p);\n        token_scores.resize(pos - token_scores.data());\n\n        // sort & compare\n        auto output_ids = extract_sorted_ids(token_scores);\n        auto target_ids = extract_sorted_ids(target);\n        EXPECT_EQ(output_ids, target_ids);\n    }\n}\n\nstatic inline ggml_tensor *ggml_new_tensor_like(ggml_context *ctx, ggml_tensor *tensor) {\n    return ggml_new_tensor(ctx, tensor->type, ggml_n_dims(tensor), tensor->ne);\n}\n\nclass ChatGLMTest : public ::testing::Test {\n  protected:\n    std::unique_ptr<ModelContext> mctx_;\n\n    void SetUp() override { mctx_ = std::make_unique<ModelContext>(); }\n\n    float perf_graph_compute() {\n        auto fn = [this] {\n            CHATGLM_CHECK(ggml_backend_graph_compute(mctx_->backend.get(), mctx_->gf) == GGML_STATUS_SUCCESS);\n        };\n        if (ggml_backend_is_cpu(mctx_->backend.get())) {\n            return timeit(fn, 1, 3);\n        } else {\n            return timeit(fn, 10, 100);\n        }\n    }\n\n    template <typename Model>\n    void test_model(Model &model, const ModelConfig &config, const fs::path &data_path, int seq_len,\n                    const std::vector<ggml_tensor *> &all_weights) {\n        ASSERT_EQ(config.num_hidden_layers, 1);\n\n        std::ifstream ifs(data_path, std::ios::binary);\n        ASSERT_TRUE(ifs) << \"cannot open file \" << data_path;\n\n        ggml_tensor *x1 = ggml_new_tensor_1d(mctx_->ctx_b.get(), GGML_TYPE_I32, seq_len);\n        ggml_tensor *ref_y1 = ggml_new_tensor_2d(mctx_->ctx_b.get(), GGML_TYPE_F32, config.hidden_size, seq_len);\n        ggml_tensor *x2 = ggml_new_tensor_1d(mctx_->ctx_b.get(), GGML_TYPE_I32, 1);\n        ggml_tensor *ref_y2 = ggml_new_tensor_1d(mctx_->ctx_b.get(), GGML_TYPE_F32, config.hidden_size);\n        ggml_tensor *x3 = ggml_new_tensor_1d(mctx_->ctx_b.get(), GGML_TYPE_I32, 1);\n        ggml_tensor *ref_y3 = ggml_new_tensor_1d(mctx_->ctx_b.get(), GGML_TYPE_F32, config.hidden_size);\n\n        std::vector<ggml_tensor *> all_tensors = all_weights;\n        all_tensors.insert(all_tensors.end(), {x1, ref_y1, x2, ref_y2, x3, ref_y3});\n\n        ggml_tensor *past_key_values = nullptr;\n        if (config.num_virtual_tokens > 0) {\n            const int head_size = config.hidden_size / config.num_attention_heads;\n            past_key_values =\n                ggml_new_tensor_4d(mctx_->ctx_b.get(), GGML_TYPE_F16, head_size, config.num_virtual_tokens,\n                                   config.num_key_value_heads, config.num_hidden_layers * 2); // [l * 2, #h, v, d]\n        }\n\n        auto buf_b =\n            unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_b.get(), mctx_->backend.get()));\n        auto buf_w =\n            unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_w.get(), mctx_->backend.get()));\n\n        if (config.num_virtual_tokens > 0) {\n            read_backend_tensor_data(ifs, past_key_values);\n            model.load_prefix_cache(config, past_key_values);\n        }\n\n        for (auto tensor : all_tensors) {\n            read_backend_tensor_data(ifs, tensor);\n        }\n        ASSERT_TRUE(ifs.peek() == EOF);\n\n        auto input_ids_to_vec = [](ggml_tensor *input_ids) {\n            std::vector<int> input_ids_vec(ggml_nelements(input_ids));\n            ggml_backend_tensor_get(input_ids, input_ids_vec.data(), 0, ggml_nbytes(input_ids));\n            return input_ids_vec;\n        };\n\n        std::vector<int> input_ids;\n\n        // prefill\n        {\n            std::vector<int> x1_vec = input_ids_to_vec(x1);\n            input_ids.insert(input_ids.end(), x1_vec.begin(), x1_vec.end());\n            ggml_graph_clear(mctx_->gf);\n            ggml_tensor *out_y1 = model.forward(mctx_.get(), x1, nullptr, input_ids, 0);\n            ggml_build_forward_expand(mctx_->gf, out_y1);\n            CHATGLM_CHECK(ggml_gallocr_alloc_graph(mctx_->allocr.get(), mctx_->gf));\n            model.set_graph_inputs(mctx_->gf, input_ids, std::nullopt, 0, seq_len);\n            CHATGLM_CHECK(ggml_backend_graph_compute(mctx_->backend.get(), mctx_->gf) == GGML_STATUS_SUCCESS);\n\n            expect_all_close(ref_y1, out_y1, 5e-4);\n        }\n        // decode\n        {\n            std::vector<int> x2_vec = input_ids_to_vec(x2);\n            input_ids.insert(input_ids.end(), x2_vec.begin(), x2_vec.end());\n            ggml_graph_clear(mctx_->gf);\n            ggml_tensor *out_y2 = model.forward(mctx_.get(), x2, nullptr, input_ids, seq_len);\n            ggml_build_forward_expand(mctx_->gf, out_y2);\n            CHATGLM_CHECK(ggml_gallocr_alloc_graph(mctx_->allocr.get(), mctx_->gf));\n            model.set_graph_inputs(mctx_->gf, input_ids, std::nullopt, seq_len, seq_len);\n            CHATGLM_CHECK(ggml_backend_graph_compute(mctx_->backend.get(), mctx_->gf) == GGML_STATUS_SUCCESS);\n\n            expect_all_close(ref_y2, out_y2, 5e-4);\n        }\n        {\n            std::vector<int> x3_vec = input_ids_to_vec(x3);\n            input_ids.insert(input_ids.end(), x3_vec.begin(), x3_vec.end());\n            ggml_graph_clear(mctx_->gf);\n            ggml_tensor *out_y3 = model.forward(mctx_.get(), x3, nullptr, input_ids, seq_len + 1);\n            ggml_build_forward_expand(mctx_->gf, out_y3);\n            CHATGLM_CHECK(ggml_gallocr_alloc_graph(mctx_->allocr.get(), mctx_->gf));\n            model.set_graph_inputs(mctx_->gf, input_ids, std::nullopt, seq_len + 1, seq_len);\n            CHATGLM_CHECK(ggml_backend_graph_compute(mctx_->backend.get(), mctx_->gf) == GGML_STATUS_SUCCESS);\n\n            expect_all_close(ref_y3, out_y3, 5e-4);\n        }\n    }\n};\n\nTEST_F(ChatGLMTest, Embedding) {\n    float w_data[]{1.5410, -0.2934, -2.1788, 0.5684,  -1.0845, -1.3986,\n                   0.4033, 0.8380,  -0.7193, -0.4033, -0.5966, 0.1820};\n    int x_data[]{1, 3, 0, 2, 3};\n    float y_data[]{0.5684,  -1.0845, -1.3986, -0.4033, -0.5966, 0.1820,  1.5410, -0.2934,\n                   -2.1788, 0.4033,  0.8380,  -0.7193, -0.4033, -0.5966, 0.1820};\n\n    ggml_tensor *x = ggml_new_tensor_1d(mctx_->ctx_b.get(), GGML_TYPE_I32, 5);\n    Embedding model(mctx_.get(), GGML_TYPE_F32, 4, 3);\n    ggml_tensor *ref = ggml_new_tensor_2d(mctx_->ctx_b.get(), GGML_TYPE_F32, 3, 5);\n\n    auto buf_b = unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_b.get(), mctx_->backend.get()));\n    auto buf_w = unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_w.get(), mctx_->backend.get()));\n\n    ggml_backend_tensor_set(x, x_data, 0, sizeof(x_data));\n    ggml_backend_tensor_set(model.weight, w_data, 0, sizeof(w_data));\n    ggml_backend_tensor_set(ref, y_data, 0, sizeof(y_data));\n\n    ggml_tensor *out = model.forward(mctx_.get(), x);\n\n    ggml_build_forward_expand(mctx_->gf, out);\n    CHATGLM_CHECK(ggml_gallocr_alloc_graph(mctx_->allocr.get(), mctx_->gf));\n    CHATGLM_CHECK(ggml_backend_graph_compute(mctx_->backend.get(), mctx_->gf) == GGML_STATUS_SUCCESS);\n\n    expect_all_close(ref, out);\n}\n\nTEST_F(ChatGLMTest, Linear) {\n    fs::path test_path = fs::path(__FILE__).parent_path() / \"tests/data/linear.data\";\n    std::ifstream ifs(test_path, std::ios::binary);\n    ASSERT_TRUE(ifs) << \"cannot open file \" << test_path;\n\n    ggml_tensor *w = ggml_new_tensor_2d(mctx_->ctx_b.get(), GGML_TYPE_F32, 64, 32);\n    ggml_tensor *b = ggml_new_tensor_1d(mctx_->ctx_b.get(), GGML_TYPE_F32, 32);\n    ggml_tensor *x = ggml_new_tensor_2d(mctx_->ctx_b.get(), GGML_TYPE_F32, 64, 2);\n    ggml_tensor *ref = ggml_new_tensor_2d(mctx_->ctx_b.get(), GGML_TYPE_F32, 32, 2);\n\n    ggml_tensor *vec_x = ggml_new_tensor_1d(mctx_->ctx_b.get(), GGML_TYPE_F32, 64);\n    ggml_tensor *vec_ref = ggml_new_tensor_1d(mctx_->ctx_b.get(), GGML_TYPE_F32, 32);\n\n    auto buf_b = unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_b.get(), mctx_->backend.get()));\n\n    read_backend_tensor_data(ifs, w);\n    read_backend_tensor_data(ifs, b);\n    read_backend_tensor_data(ifs, x);\n    read_backend_tensor_data(ifs, ref);\n\n    read_backend_tensor_data(ifs, vec_x);\n    read_backend_tensor_data(ifs, vec_ref);\n\n    ASSERT_TRUE(ifs.peek() == EOF);\n\n    struct TestCase {\n        ggml_tensor *x;\n        ggml_tensor *ref;\n    };\n    std::vector<TestCase> cases{{x, ref}, {vec_x, vec_ref}};\n\n    struct TestConfig {\n        ggml_type dtype;\n        float atol;\n        float rtol;\n    };\n    std::vector<TestConfig> test_configs{\n        {GGML_TYPE_F32, 1e-5, 0},   {GGML_TYPE_F16, 1e-2, 5e-4}, {GGML_TYPE_Q8_0, 0.2, 5e-4},\n        {GGML_TYPE_Q5_0, 1.5, 0.1}, {GGML_TYPE_Q5_1, 1.5, 0.1},  {GGML_TYPE_Q4_1, 2.0, 0.2},\n        {GGML_TYPE_Q4_0, 2.0, 0.2},\n    };\n\n    for (const auto &config : test_configs) {\n        Linear model(mctx_.get(), config.dtype, 64, 32);\n        auto buf_w =\n            unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_w.get(), mctx_->backend.get()));\n\n        auto ctx = make_unique_ggml_context(1024 * 1024, nullptr, false);\n        ggml_tensor *w_cpu = ggml_new_tensor_like(ctx.get(), w);\n        ggml_backend_tensor_get(w, w_cpu->data, 0, ggml_nbytes(w));\n\n        ggml_tensor *wq_cpu = ggml_new_tensor_2d(ctx.get(), config.dtype, w_cpu->ne[0], w_cpu->ne[1]);\n        if (config.dtype == GGML_TYPE_F32) {\n            wq_cpu = w_cpu;\n        } else if (config.dtype == GGML_TYPE_F16) {\n            ggml_fp32_to_fp16_row((float *)w_cpu->data, (ggml_fp16_t *)wq_cpu->data, ggml_nelements(w_cpu));\n        } else {\n            ggml_quantize_chunk(config.dtype, (float *)w_cpu->data, wq_cpu->data, 0, w_cpu->ne[1], w_cpu->ne[0],\n                                nullptr);\n        }\n        ggml_backend_tensor_set(model.weight, wq_cpu->data, 0, ggml_nbytes(model.weight));\n        ggml_backend_tensor_copy(b, model.bias);\n\n        for (const auto &c : cases) {\n            ggml_graph_clear(mctx_->gf);\n            ggml_tensor *out = model.forward(mctx_.get(), c.x);\n\n            ggml_build_forward_expand(mctx_->gf, out);\n            CHATGLM_CHECK(ggml_gallocr_alloc_graph(mctx_->allocr.get(), mctx_->gf));\n            CHATGLM_CHECK(ggml_backend_graph_compute(mctx_->backend.get(), mctx_->gf) == GGML_STATUS_SUCCESS);\n\n            expect_all_close(c.ref, out, config.atol, config.rtol);\n        }\n    }\n}\n\nTEST_F(ChatGLMTest, BenchmarkLinear) {\n    constexpr int M = 64, N = 1024, K = 1024 * 3;\n    std::vector<ggml_type> dtypes{GGML_TYPE_F32,  GGML_TYPE_F16,  GGML_TYPE_Q8_0, GGML_TYPE_Q5_1,\n                                  GGML_TYPE_Q5_0, GGML_TYPE_Q4_1, GGML_TYPE_Q4_0};\n    for (ggml_type dtype : dtypes) {\n        mctx_ = std::make_unique<ModelContext>();\n\n        Linear m(mctx_.get(), dtype, K, N);\n        ggml_tensor *x = ggml_new_tensor_2d(mctx_->ctx_b.get(), GGML_TYPE_F32, K, M);\n\n        ggml_tensor *y = m.forward(mctx_.get(), x);\n        ggml_build_forward_expand(mctx_->gf, y);\n        CHATGLM_CHECK(ggml_gallocr_alloc_graph(mctx_->allocr.get(), mctx_->gf));\n\n        std::vector<ggml_tensor *> all_tensors{m.weight, m.bias, x};\n        for (auto tensor : all_tensors) {\n            randn_(tensor);\n        }\n\n        std::cout << \"[Benchmark] Linear \" << ggml_type_name(dtype) << \" time: \" << perf_graph_compute() << \" ms\\n\";\n    }\n}\n\nTEST_F(ChatGLMTest, LayerNorm) {\n    fs::path test_path = fs::path(__FILE__).parent_path() / \"tests/data/layer_norm.data\";\n    std::ifstream ifs(test_path, std::ios::binary);\n    ASSERT_TRUE(ifs) << \"cannot open file \" << test_path;\n\n    LayerNorm model(mctx_.get(), 64);\n    ggml_tensor *x = ggml_new_tensor_2d(mctx_->ctx_b.get(), GGML_TYPE_F32, 64, 3);\n    ggml_tensor *ref = ggml_new_tensor_2d(mctx_->ctx_b.get(), GGML_TYPE_F32, 64, 3);\n\n    auto buf_b = unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_b.get(), mctx_->backend.get()));\n    auto buf_w = unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_w.get(), mctx_->backend.get()));\n\n    std::vector<ggml_tensor *> all_tensors{model.weight, model.bias, x, ref};\n    for (auto tensor : all_tensors) {\n        read_backend_tensor_data(ifs, tensor);\n    }\n    ASSERT_TRUE(ifs.peek() == EOF);\n\n    ggml_tensor *out = model.forward(mctx_.get(), x);\n\n    ggml_build_forward_expand(mctx_->gf, out);\n    CHATGLM_CHECK(ggml_gallocr_alloc_graph(mctx_->allocr.get(), mctx_->gf));\n    CHATGLM_CHECK(ggml_backend_graph_compute(mctx_->backend.get(), mctx_->gf) == GGML_STATUS_SUCCESS);\n\n    expect_all_close(ref, out);\n}\n\nTEST_F(ChatGLMTest, BenchmarkLayerNorm) {\n    constexpr int seq_len = 64;\n    constexpr int hidden = 1024;\n\n    LayerNorm m(mctx_.get(), hidden);\n    ggml_tensor *x = ggml_new_tensor_2d(mctx_->ctx_b.get(), GGML_TYPE_F32, hidden, seq_len);\n\n    auto buffer =\n        unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_b.get(), mctx_->backend.get()));\n    auto buffer_w =\n        unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_w.get(), mctx_->backend.get()));\n\n    std::vector<ggml_tensor *> all_tensors{m.weight, m.bias, x};\n    for (auto tensor : all_tensors) {\n        random_(tensor);\n    }\n\n    ggml_tensor *y = m.forward(mctx_.get(), x);\n    ggml_build_forward_expand(mctx_->gf, y);\n    CHATGLM_CHECK(ggml_gallocr_alloc_graph(mctx_->allocr.get(), mctx_->gf));\n    std::cout << \"[Benchmark] LayerNorm \" << ggml_type_name(GGML_TYPE_F32) << \" time: \" << perf_graph_compute()\n              << \" ms\\n\";\n}\n\nTEST_F(ChatGLMTest, RMSNorm) {\n    fs::path test_path = fs::path(__FILE__).parent_path() / \"tests/data/rms_norm.data\";\n    std::ifstream ifs(test_path, std::ios::binary);\n    ASSERT_TRUE(ifs) << \"cannot open file \" << test_path;\n\n    RMSNorm model(mctx_.get(), 64);\n    ggml_tensor *x = ggml_new_tensor_2d(mctx_->ctx_b.get(), GGML_TYPE_F32, 64, 3);\n    ggml_tensor *ref = ggml_new_tensor_2d(mctx_->ctx_b.get(), GGML_TYPE_F32, 64, 3);\n\n    auto buf_b = unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_b.get(), mctx_->backend.get()));\n    auto buf_w = unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_w.get(), mctx_->backend.get()));\n\n    std::vector<ggml_tensor *> all_tensors{model.weight, x, ref};\n    for (auto tensor : all_tensors) {\n        read_backend_tensor_data(ifs, tensor);\n    }\n    ASSERT_TRUE(ifs.peek() == EOF);\n\n    ggml_tensor *out = model.forward(mctx_.get(), x);\n\n    ggml_build_forward_expand(mctx_->gf, out);\n    CHATGLM_CHECK(ggml_gallocr_alloc_graph(mctx_->allocr.get(), mctx_->gf));\n    CHATGLM_CHECK(ggml_backend_graph_compute(mctx_->backend.get(), mctx_->gf) == GGML_STATUS_SUCCESS);\n\n    expect_all_close(ref, out);\n}\n\nTEST_F(ChatGLMTest, BenchmarkRMSNorm) {\n    constexpr int seq_len = 64;\n    constexpr int hidden = 1024;\n\n    RMSNorm m(mctx_.get(), hidden);\n    ggml_tensor *x = ggml_new_tensor_2d(mctx_->ctx_b.get(), GGML_TYPE_F32, hidden, seq_len);\n\n    auto buffer =\n        unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_b.get(), mctx_->backend.get()));\n    auto buffer_w =\n        unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_w.get(), mctx_->backend.get()));\n\n    std::vector<ggml_tensor *> all_tensors{m.weight, x};\n    for (auto tensor : all_tensors) {\n        random_(tensor);\n    }\n\n    ggml_tensor *y = m.forward(mctx_.get(), x);\n    ggml_build_forward_expand(mctx_->gf, y);\n    CHATGLM_CHECK(ggml_gallocr_alloc_graph(mctx_->allocr.get(), mctx_->gf));\n    std::cout << \"[Benchmark] RMSNorm \" << ggml_type_name(GGML_TYPE_F32) << \" time: \" << perf_graph_compute()\n              << \" ms\\n\";\n}\n\nTEST_F(ChatGLMTest, GLMModel) {\n    fs::path data_path = fs::path(__FILE__).parent_path() / \"tests/data/glm_model.data\";\n\n    ModelConfig config(ModelType::CHATGLM, GGML_TYPE_F32, /*vocab_size=*/5, /*hidden_size=*/32,\n                       /*num_attention_heads=*/8, /*num_key_value_heads=*/8, /*num_hidden_layers=*/1,\n                       /*intermediate_size=*/128, /*norm_eps=*/1e-5f, /*rope_theta=*/10000.f, /*num_virtual_tokens=*/0,\n                       /*max_length=*/8, /*bos_token_id=*/-1, /*eos_token_id=*/-1, /*pad_token_id=*/-1,\n                       /*sep_token_id=*/-1, /*boi_token_id=*/-1, /*eoi_token_id=*/-1, /*extra_eos_token_ids=*/{},\n                       /*vision=*/{});\n\n    constexpr int seq_len = 3;\n\n    ChatGLMModel model(mctx_.get(), config);\n\n    std::vector<ggml_tensor *> all_weights{model.word_embeddings.weight,\n                                           model.layers[0].input_layernorm.weight,\n                                           model.layers[0].input_layernorm.bias,\n                                           model.layers[0].attention.query_key_value.weight,\n                                           model.layers[0].attention.query_key_value.bias,\n                                           model.layers[0].attention.dense.weight,\n                                           model.layers[0].attention.dense.bias,\n                                           model.layers[0].post_attention_layernorm.weight,\n                                           model.layers[0].post_attention_layernorm.bias,\n                                           model.layers[0].mlp.dense_h_to_4h.weight,\n                                           model.layers[0].mlp.dense_h_to_4h.bias,\n                                           model.layers[0].mlp.dense_4h_to_h.weight,\n                                           model.layers[0].mlp.dense_4h_to_h.bias,\n                                           model.final_layernorm.weight,\n                                           model.final_layernorm.bias};\n\n    test_model(model, config, data_path, seq_len, all_weights);\n}\n\nTEST_F(ChatGLMTest, GLMPTuningV2Model) {\n    fs::path data_path = fs::path(__FILE__).parent_path() / \"tests/data/glm_ptuning_v2_model.data\";\n\n    ModelConfig config(ModelType::CHATGLM, GGML_TYPE_F32, /*vocab_size=*/5, /*hidden_size=*/32,\n                       /*num_attention_heads=*/8, /*num_key_value_heads=*/8, /*num_hidden_layers=*/1,\n                       /*intermediate_size=*/128, /*norm_eps=*/1e-5f, /*rope_theta=*/10000.f, /*num_virtual_tokens=*/5,\n                       /*max_length=*/8, /*bos_token_id=*/-1, /*eos_token_id=*/-1, /*pad_token_id=*/-1,\n                       /*sep_token_id=*/-1, /*boi_token_id=*/-1, /*eoi_token_id=*/-1, /*extra_eos_token_ids=*/{},\n                       /*vision=*/{});\n\n    constexpr int seq_len = 3;\n\n    ChatGLMModel model(mctx_.get(), config);\n\n    std::vector<ggml_tensor *> all_weights{model.word_embeddings.weight,\n                                           model.layers[0].input_layernorm.weight,\n                                           model.layers[0].input_layernorm.bias,\n                                           model.layers[0].attention.query_key_value.weight,\n                                           model.layers[0].attention.query_key_value.bias,\n                                           model.layers[0].attention.dense.weight,\n                                           model.layers[0].attention.dense.bias,\n                                           model.layers[0].post_attention_layernorm.weight,\n                                           model.layers[0].post_attention_layernorm.bias,\n                                           model.layers[0].mlp.dense_h_to_4h.weight,\n                                           model.layers[0].mlp.dense_h_to_4h.bias,\n                                           model.layers[0].mlp.dense_4h_to_h.weight,\n                                           model.layers[0].mlp.dense_4h_to_h.bias,\n                                           model.final_layernorm.weight,\n                                           model.final_layernorm.bias};\n\n    test_model(model, config, data_path, seq_len, all_weights);\n}\n\nTEST_F(ChatGLMTest, GLM2Model) {\n    fs::path data_path = fs::path(__FILE__).parent_path() / \"tests/data/glm2_model.data\";\n\n    ModelConfig config(ModelType::CHATGLM2, GGML_TYPE_F32, /*vocab_size=*/5, /*hidden_size=*/32,\n                       /*num_attention_heads=*/8, /*num_key_value_heads=*/2, /*num_hidden_layers=*/1,\n                       /*intermediate_size=*/48, /*norm_eps=*/1e-5f, /*rope_theta=*/10000.f, /*num_virtual_tokens=*/0,\n                       /*max_length=*/8, /*bos_token_id=*/-1, /*eos_token_id=*/-1, /*pad_token_id=*/-1,\n                       /*sep_token_id=*/-1, /*boi_token_id=*/-1, /*eoi_token_id=*/-1, /*extra_eos_token_ids=*/{},\n                       /*vision=*/{});\n\n    constexpr int seq_len = 3;\n\n    ChatGLM2Model model(mctx_.get(), config);\n\n    std::vector<ggml_tensor *> all_weights{model.word_embeddings.weight,\n                                           model.layers[0].input_layernorm.weight,\n                                           model.layers[0].attention.query_key_value.weight,\n                                           model.layers[0].attention.query_key_value.bias,\n                                           model.layers[0].attention.dense.weight,\n                                           model.layers[0].post_attention_layernorm.weight,\n                                           model.layers[0].mlp.gate_proj.weight,\n                                           model.layers[0].mlp.up_proj.weight,\n                                           model.layers[0].mlp.down_proj.weight,\n                                           model.final_layernorm.weight};\n\n    test_model(model, config, data_path, seq_len, all_weights);\n}\n\nTEST_F(ChatGLMTest, GLM3Model) {\n    fs::path data_path = fs::path(__FILE__).parent_path() / \"tests/data/glm3_model.data\";\n\n    ModelConfig config(ModelType::CHATGLM3, GGML_TYPE_F32, /*vocab_size=*/5, /*hidden_size=*/32,\n                       /*num_attention_heads=*/8, /*num_key_value_heads=*/2, /*num_hidden_layers=*/1,\n                       /*intermediate_size=*/48, /*norm_eps=*/1e-5f, /*rope_theta=*/10000.f, /*num_virtual_tokens=*/0,\n                       /*max_length=*/8, /*bos_token_id=*/-1, /*eos_token_id=*/-1, /*pad_token_id=*/-1,\n                       /*sep_token_id=*/-1, /*boi_token_id=*/-1, /*eoi_token_id=*/-1, /*extra_eos_token_ids=*/{},\n                       /*vision=*/{});\n\n    constexpr int seq_len = 3;\n\n    ChatGLM3Model model(mctx_.get(), config);\n\n    std::vector<ggml_tensor *> all_weights{model.word_embeddings.weight,\n                                           model.layers[0].input_layernorm.weight,\n                                           model.layers[0].attention.query_key_value.weight,\n                                           model.layers[0].attention.query_key_value.bias,\n                                           model.layers[0].attention.dense.weight,\n                                           model.layers[0].post_attention_layernorm.weight,\n                                           model.layers[0].mlp.gate_proj.weight,\n                                           model.layers[0].mlp.up_proj.weight,\n                                           model.layers[0].mlp.down_proj.weight,\n                                           model.final_layernorm.weight};\n\n    test_model(model, config, data_path, seq_len, all_weights);\n}\n\nTEST_F(ChatGLMTest, GLM3PTuningV2Model) {\n    fs::path data_path = fs::path(__FILE__).parent_path() / \"tests/data/glm3_ptuning_v2_model.data\";\n\n    ModelConfig config(ModelType::CHATGLM3, GGML_TYPE_F32, /*vocab_size=*/5, /*hidden_size=*/32,\n                       /*num_attention_heads=*/8, /*num_key_value_heads=*/2, /*num_hidden_layers=*/1,\n                       /*intermediate_size=*/48, /*norm_eps=*/1e-5f, /*rope_theta=*/10000.f, /*num_virtual_tokens=*/5,\n                       /*max_length=*/8, /*bos_token_id=*/-1, /*eos_token_id=*/-1, /*pad_token_id=*/-1,\n                       /*sep_token_id=*/-1, /*boi_token_id=*/-1, /*eoi_token_id=*/-1, /*extra_eos_token_ids=*/{},\n                       /*vision=*/{});\n\n    constexpr int seq_len = 3;\n\n    ChatGLM3Model model(mctx_.get(), config);\n\n    std::vector<ggml_tensor *> all_weights{model.word_embeddings.weight,\n                                           model.layers[0].input_layernorm.weight,\n                                           model.layers[0].attention.query_key_value.weight,\n                                           model.layers[0].attention.query_key_value.bias,\n                                           model.layers[0].attention.dense.weight,\n                                           model.layers[0].post_attention_layernorm.weight,\n                                           model.layers[0].mlp.gate_proj.weight,\n                                           model.layers[0].mlp.up_proj.weight,\n                                           model.layers[0].mlp.down_proj.weight,\n                                           model.final_layernorm.weight};\n\n    test_model(model, config, data_path, seq_len, all_weights);\n}\n\nTEST_F(ChatGLMTest, GLM4Model) {\n    fs::path data_path = fs::path(__FILE__).parent_path() / \"tests/data/glm4_model.data\";\n\n    ModelConfig config(ModelType::CHATGLM4, GGML_TYPE_F32, /*vocab_size=*/5, /*hidden_size=*/32,\n                       /*num_attention_heads=*/8, /*num_key_value_heads=*/2, /*num_hidden_layers=*/1,\n                       /*intermediate_size=*/48, /*norm_eps=*/1e-5f, /*rope_theta=*/10000.f, /*num_virtual_tokens=*/0,\n                       /*max_length=*/8, /*bos_token_id=*/-1, /*eos_token_id=*/-1, /*pad_token_id=*/-1,\n                       /*sep_token_id=*/-1, /*boi_token_id=*/-1, /*eoi_token_id=*/-1, /*extra_eos_token_ids=*/{},\n                       /*vision=*/{});\n\n    constexpr int seq_len = 3;\n\n    ChatGLM4Model model(mctx_.get(), config);\n\n    std::vector<ggml_tensor *> all_weights{model.word_embeddings.weight,\n                                           model.layers[0].input_layernorm.weight,\n                                           model.layers[0].attention.query_key_value.weight,\n                                           model.layers[0].attention.query_key_value.bias,\n                                           model.layers[0].attention.dense.weight,\n                                           model.layers[0].post_attention_layernorm.weight,\n                                           model.layers[0].mlp.gate_proj.weight,\n                                           model.layers[0].mlp.up_proj.weight,\n                                           model.layers[0].mlp.down_proj.weight,\n                                           model.final_layernorm.weight};\n\n    test_model(model, config, data_path, seq_len, all_weights);\n}\n\nTEST_F(ChatGLMTest, GLM4VModelText) {\n    fs::path data_path = fs::path(__FILE__).parent_path() / \"tests/data/glm4v_model_text.data\";\n\n    VisionModelConfig vision(GGML_TYPE_F32, ActivationType::GELU, 128, 28, 3, 56, 1e-6, 2, 1, 17, 7, 8);\n\n    ModelConfig config(ModelType::CHATGLM4V, GGML_TYPE_F32, /*vocab_size=*/8, /*hidden_size=*/32,\n                       /*num_attention_heads=*/8, /*num_key_value_heads=*/2, /*num_hidden_layers=*/1,\n                       /*intermediate_size=*/48, /*norm_eps=*/0.00000015625f, /*rope_theta=*/10000.f,\n                       /*num_virtual_tokens=*/0,\n                       /*max_length=*/16, /*bos_token_id=*/-1, /*eos_token_id=*/4, /*pad_token_id=*/-1,\n                       /*sep_token_id=*/-1, /*boi_token_id=*/2, /*eoi_token_id=*/4, /*extra_eos_token_ids=*/{},\n                       /*vision=*/vision);\n\n    constexpr int seq_len = 3;\n\n    ChatGLM4VModel model(mctx_.get(), config);\n\n    std::vector<ggml_tensor *> all_weights{model.word_embeddings.weight,\n                                           model.layers[0].input_layernorm.weight,\n                                           model.layers[0].attention.query_key_value.weight,\n                                           model.layers[0].attention.query_key_value.bias,\n                                           model.layers[0].attention.dense.weight,\n                                           model.layers[0].post_attention_layernorm.weight,\n                                           model.layers[0].mlp.gate_proj.weight,\n                                           model.layers[0].mlp.up_proj.weight,\n                                           model.layers[0].mlp.down_proj.weight,\n                                           model.final_layernorm.weight};\n\n    test_model(model, config, data_path, seq_len, all_weights);\n}\n\nTEST_F(ChatGLMTest, GLM4VModel) {\n    fs::path data_path = fs::path(__FILE__).parent_path() / \"tests/data/glm4v_model.data\";\n\n    VisionModelConfig vision(GGML_TYPE_F32, ActivationType::GELU, 128, 28, 3, 56, 1e-6, 2, 1, 17, 7, 8);\n\n    ModelConfig config(ModelType::CHATGLM4V, GGML_TYPE_F32, /*vocab_size=*/8, /*hidden_size=*/32,\n                       /*num_attention_heads=*/8, /*num_key_value_heads=*/2, /*num_hidden_layers=*/1,\n                       /*intermediate_size=*/48, /*norm_eps=*/0.00000015625f, /*rope_theta=*/10000.f,\n                       /*num_virtual_tokens=*/0,\n                       /*max_length=*/16, /*bos_token_id=*/-1, /*eos_token_id=*/4, /*pad_token_id=*/-1,\n                       /*sep_token_id=*/-1, /*boi_token_id=*/2, /*eoi_token_id=*/4, /*extra_eos_token_ids=*/{},\n                       /*vision=*/vision);\n\n    std::ifstream fin(data_path, std::ios::binary);\n    ASSERT_TRUE(fin) << \"cannot open file \" << data_path;\n\n    ChatGLM4VModel model(mctx_.get(), config);\n\n    ggml_tensor *images = ggml_new_tensor_3d(mctx_->ctx_b.get(), GGML_TYPE_F32, config.vision.image_size,\n                                             config.vision.image_size, config.vision.in_channels);\n\n    const int vision_tokens = model.num_vision_tokens();\n\n    const int seq_len = 6;\n    ggml_tensor *x1 = ggml_new_tensor_1d(mctx_->ctx_b.get(), GGML_TYPE_I32, seq_len);\n    ggml_tensor *ref_y1 =\n        ggml_new_tensor_2d(mctx_->ctx_b.get(), GGML_TYPE_F32, config.hidden_size, seq_len - 1 + vision_tokens);\n    ggml_tensor *x2 = ggml_new_tensor_1d(mctx_->ctx_b.get(), GGML_TYPE_I32, 1);\n    ggml_tensor *ref_y2 = ggml_new_tensor_1d(mctx_->ctx_b.get(), GGML_TYPE_F32, config.hidden_size);\n    ggml_tensor *x3 = ggml_new_tensor_1d(mctx_->ctx_b.get(), GGML_TYPE_I32, 1);\n    ggml_tensor *ref_y3 = ggml_new_tensor_1d(mctx_->ctx_b.get(), GGML_TYPE_F32, config.hidden_size);\n\n    auto buf_b = unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_b.get(), mctx_->backend.get()));\n    auto buf_w = unique_ggml_backend_buffer_t(ggml_backend_alloc_ctx_tensors(mctx_->ctx_w.get(), mctx_->backend.get()));\n\n    std::vector<ggml_tensor *> all_tensors{\n        // vision\n        model.vision.patch_embedding.cls_embedding,\n        model.vision.patch_embedding.proj.weight,\n        model.vision.patch_embedding.proj.bias,\n        model.vision.patch_embedding.position_embedding.weight,\n        model.vision.transformer.layers[0].input_layernorm.weight,\n        model.vision.transformer.layers[0].input_layernorm.bias,\n        model.vision.transformer.layers[0].attention.query_key_value.weight,\n        model.vision.transformer.layers[0].attention.query_key_value.bias,\n        model.vision.transformer.layers[0].attention.dense.weight,\n        model.vision.transformer.layers[0].attention.dense.bias,\n        model.vision.transformer.layers[0].mlp.dense_h_to_4h.weight,\n        model.vision.transformer.layers[0].mlp.dense_h_to_4h.bias,\n        model.vision.transformer.layers[0].mlp.dense_4h_to_h.weight,\n        model.vision.transformer.layers[0].mlp.dense_4h_to_h.bias,\n        model.vision.transformer.layers[0].post_attention_layernorm.weight,\n        model.vision.transformer.layers[0].post_attention_layernorm.bias,\n        model.vision.conv.weight,\n        model.vision.conv.bias,\n        model.vision.linear_proj.weight,\n        model.vision.norm1.weight,\n        model.vision.norm1.bias,\n        model.vision.glu.gate_proj.weight,\n        model.vision.glu.up_proj.weight,\n        model.vision.glu.down_proj.weight,\n        model.vision.boi,\n        model.vision.eoi,\n        // text\n        model.word_embeddings.weight,\n        model.layers[0].input_layernorm.weight,\n        model.layers[0].attention.query_key_value.weight,\n        model.layers[0].attention.query_key_value.bias,\n        model.layers[0].attention.dense.weight,\n        model.layers[0].post_attention_layernorm.weight,\n        model.layers[0].mlp.gate_proj.weight,\n        model.layers[0].mlp.up_proj.weight,\n        model.layers[0].mlp.down_proj.weight,\n        model.final_layernorm.weight,\n        // inputs & outputs\n        images,\n        x1,\n        ref_y1,\n        x2,\n        ref_y2,\n        x3,\n        ref_y3,\n    };\n\n    for (ggml_tensor *tensor : all_tensors) {\n        read_backend_tensor_data(fin, tensor);\n    }\n    ASSERT_TRUE(fin.peek() == EOF);\n\n    auto input_ids_to_vec = [](ggml_tensor *input_ids) {\n        std::vector<int> input_ids_vec(ggml_nelements(input_ids));\n        ggml_backend_tensor_get(input_ids, input_ids_vec.data(), 0, ggml_nbytes(input_ids));\n        return input_ids_vec;\n    };\n\n    std::vector<int> input_ids;\n\n    // prefill\n    {\n        std::vector<int> x1_vec = input_ids_to_vec(x1);\n        input_ids.insert(input_ids.end(), x1_vec.begin(), x1_vec.end());\n        ggml_graph_clear(mctx_->gf);\n        ggml_tensor *out_y1 = model.forward(mctx_.get(), x1, images, input_ids, 0);\n        ggml_build_forward_expand(mctx_->gf, out_y1);\n        CHATGLM_CHECK(ggml_gallocr_alloc_graph(mctx_->allocr.get(), mctx_->gf));\n        model.set_graph_inputs(mctx_->gf, input_ids, std::nullopt, 0, ref_y1->ne[1]);\n        CHATGLM_CHECK(ggml_backend_graph_compute(mctx_->backend.get(), mctx_->gf) == GGML_STATUS_SUCCESS);\n\n        expect_all_close(ref_y1, out_y1, 1e-2);\n    }\n    // decode\n    {\n        std::vector<int> x2_vec = input_ids_to_vec(x2);\n        input_ids.insert(input_ids.end(), x2_vec.begin(), x2_vec.end());\n        ggml_graph_clear(mctx_->gf);\n        ggml_tensor *out_y2 =\n            model.forward(mctx_.get(), x2, images, input_ids, seq_len - 1 + model.num_vision_tokens());\n        ggml_build_forward_expand(mctx_->gf, out_y2);\n        CHATGLM_CHECK(ggml_gallocr_alloc_graph(mctx_->allocr.get(), mctx_->gf));\n        model.set_graph_inputs(mctx_->gf, input_ids, std::nullopt, seq_len, seq_len);\n        CHATGLM_CHECK(ggml_backend_graph_compute(mctx_->backend.get(), mctx_->gf) == GGML_STATUS_SUCCESS);\n\n        expect_all_close(ref_y2, out_y2, 1e-2);\n    }\n    {\n        std::vector<int> x3_vec = input_ids_to_vec(x3);\n        input_ids.insert(input_ids.end(), x3_vec.begin(), x3_vec.end());\n        ggml_graph_clear(mctx_->gf);\n        ggml_tensor *out_y3 = model.forward(mctx_.get(), x3, images, input_ids, seq_len + model.num_vision_tokens());\n        ggml_build_forward_expand(mctx_->gf, out_y3);\n        CHATGLM_CHECK(ggml_gallocr_alloc_graph(mctx_->allocr.get(), mctx_->gf));\n        model.set_graph_inputs(mctx_->gf, input_ids, std::nullopt, seq_len + 1, seq_len);\n        CHATGLM_CHECK(ggml_backend_graph_compute(mctx_->backend.get(), mctx_->gf) == GGML_STATUS_SUCCESS);\n\n        expect_all_close(ref_y3, out_y3, 1e-2);\n    }\n}\n\nTEST_F(ChatGLMTest, quantize) {\n    const float src_data[]{\n        -1.1258e+00, -1.1524e+00, -2.5058e-01, -4.3388e-01, 8.4871e-01,  6.9201e-01,  -3.1601e-01, -2.1152e+00,\n        3.2227e-01,  -1.2633e+00, 3.4998e-01,  3.0813e-01,  1.1984e-01,  1.2377e+00,  1.1168e+00,  -2.4728e-01,\n        -1.3527e+00, -1.6959e+00, 5.6665e-01,  7.9351e-01,  5.9884e-01,  -1.5551e+00, -3.4136e-01, 1.8530e+00,\n        7.5019e-01,  -5.8550e-01, -1.7340e-01, 1.8348e-01,  1.3894e+00,  1.5863e+00,  9.4630e-01,  -8.4368e-01,\n        -6.1358e-01, 3.1593e-02,  -4.9268e-01, 2.4841e-01,  4.3970e-01,  1.1241e-01,  6.4079e-01,  4.4116e-01,\n        -1.0231e-01, 7.9244e-01,  -2.8967e-01, 5.2507e-02,  5.2286e-01,  2.3022e+00,  -1.4689e+00, -1.5867e+00,\n        -6.7309e-01, 8.7283e-01,  1.0554e+00,  1.7784e-01,  -2.3034e-01, -3.9175e-01, 5.4329e-01,  -3.9516e-01,\n        -4.4622e-01, 7.4402e-01,  1.5210e+00,  3.4105e+00,  -1.5312e+00, -1.2341e+00, 1.8197e+00,  -5.5153e-01,\n        -5.6925e-01, 9.1997e-01,  1.1108e+00,  1.2899e+00,  -1.4782e+00, 2.5672e+00,  -4.7312e-01, 3.3555e-01,\n        -1.6293e+00, -5.4974e-01, -4.7983e-01, -4.9968e-01, -1.0670e+00, 1.1149e+00,  -1.4067e-01, 8.0575e-01,\n        -9.3348e-02, 6.8705e-01,  -8.3832e-01, 8.9182e-04,  8.4189e-01,  -4.0003e-01, 1.0395e+00,  3.5815e-01,\n        -2.4600e-01, 2.3025e+00,  -1.8817e+00, -4.9727e-02, -1.0450e+00, -9.5650e-01, 3.3532e-02,  7.1009e-01,\n        1.6459e+00,  -1.3602e+00, 3.4457e-01,  5.1987e-01,  -2.6133e+00, -1.6965e+00, -2.2824e-01, 2.7995e-01,\n        2.4693e-01,  7.6887e-02,  3.3801e-01,  4.5440e-01,  4.5694e-01,  -8.6537e-01, 7.8131e-01,  -9.2679e-01,\n        -2.1883e-01, -2.4351e+00, -7.2915e-02, -3.3986e-02, 9.6252e-01,  3.4917e-01,  -9.2146e-01, -5.6195e-02,\n        -6.2270e-01, -4.6372e-01, 1.9218e+00,  -4.0255e-01, 1.2390e-01,  1.1648e+00,  9.2337e-01,  1.3873e+00,\n        -8.8338e-01, -4.1891e-01, -8.0483e-01, 5.6561e-01,  6.1036e-01,  4.6688e-01,  1.9507e+00,  -1.0631e+00,\n        -7.7326e-02, 1.1640e-01,  -5.9399e-01, -1.2439e+00, -1.0209e-01, -1.0335e+00, -3.1264e-01, 2.4579e-01,\n        -2.5964e-01, 1.1834e-01,  2.4396e-01,  1.1646e+00,  2.8858e-01,  3.8660e-01,  -2.0106e-01, -1.1793e-01,\n        1.9220e-01,  -7.7216e-01, -1.9003e+00, 1.3068e-01,  -7.0429e-01, 3.1472e-01,  1.5739e-01,  3.8536e-01,\n        9.6715e-01,  -9.9108e-01, 3.0161e-01,  -1.0732e-01, 9.9846e-01,  -4.9871e-01, 7.6111e-01,  6.1830e-01,\n        3.1405e-01,  2.1333e-01,  -1.2005e-01, 3.6046e-01,  -3.1403e-01, -1.0787e+00, 2.4081e-01,  -1.3962e+00,\n        -6.6144e-02, -3.5836e-01, -1.5616e+00, -3.5464e-01, 1.0811e+00,  1.3148e-01,  1.5735e+00,  7.8143e-01,\n        -1.0787e+00, -7.2091e-01, 1.4708e+00,  2.7564e-01,  6.6678e-01,  -9.9439e-01, -1.1894e+00, -1.1959e+00,\n        -5.5963e-01, 5.3347e-01,  4.0689e-01,  3.9459e-01,  1.7151e-01,  8.7604e-01,  -2.8709e-01, 1.0216e+00,\n        -7.4395e-02, -1.0922e+00, 3.9203e-01,  5.9453e-01,  6.6227e-01,  -1.2063e+00, 6.0744e-01,  -5.4716e-01,\n        1.1711e+00,  9.7496e-02,  9.6337e-01,  8.4032e-01,  -1.2537e+00, 9.8684e-01,  -4.9466e-01, -1.2830e+00,\n        9.5522e-01,  1.2836e+00,  -6.6586e-01, 5.6513e-01,  2.8770e-01,  -3.3375e-02, -1.0619e+00, -1.1443e-01,\n        -3.4334e-01, 1.5713e+00,  1.9161e-01,  3.7994e-01,  -1.4476e-01, 6.3762e-01,  -2.8129e-01, -1.3299e+00,\n        -1.4201e-01, -5.3415e-01, -5.2338e-01, 8.6150e-01,  -8.8696e-01, 8.3877e-01,  1.1529e+00,  -1.7611e+00,\n        -1.4777e+00, -1.7557e+00, 7.6166e-02,  -1.0786e+00, 1.4403e+00,  -1.1059e-01, 5.7686e-01,  -1.6917e-01,\n        -6.4025e-02, 1.0384e+00,  9.0682e-01,  -4.7551e-01, -8.7074e-01, 1.4474e-01,  1.9029e+00,  3.9040e-01};\n\n    const int q8_ref[]{\n        68,  36,   -68, -69,  -15, -26,  51,  42,  -19, -127, 19,  -76, 21,  19,  7,   74,  67,   -15,  -81, -102, 34,\n        48,  36,   -93, -20,  111, 45,   -35, -10, 11,  83,   95,  57,  -51, -32, 38,  -23, 1,    -18,  9,   16,   4,\n        24,  16,   -4,  30,   -11, 2,    19,  86,  -55, -59,  -25, 33,  39,  7,   -9,  -15, 20,   -15,  -17, 28,   57,\n        127, -57,  -46, 68,   -21, 45,   37,  -28, 46,  55,   64,  -73, 127, -23, 17,  -81, -27,  -24,  -25, -53,  55,\n        -7,  40,   -5,  34,   -41, 0,    42,  -20, 51,  18,   -12, 114, -93, -2,  -52, -47, 2,    35,   69,  37,   80,\n        -66, 17,   25,  -127, -82, -11,  14,  12,  4,   16,   22,  22,  -42, 38,  -45, -11, -118, -4,   -2,  47,   17,\n        -45, -3,   -30, -23,  93,  -20,  6,   57,  45,  67,   -35, 35,  -58, -27, -52, 37,  40,   30,   127, -69,  -5,\n        8,   -39,  -81, -7,   -67, -20,  16,  -17, 8,   16,   76,  19,  25,  -13, -8,  13,  -50,  -124, 9,   -46,  20,\n        10,  25,   88,  34,   78,  -80,  24,  -9,  81,  -40,  61,  50,  25,  17,  -10, 29,  -25,  -87,  19,  -113, -5,\n        -29, -126, -29, 87,   11,  127,  63,  -87, -58, 119,  22,  54,  -80, -96, -97, 45,  33,   -55,  53,  40,   39,\n        17,  87,   -28, 101,  -7,  -108, 39,  59,  66,  -119, 60,  -54, 116, 10,  95,  83,  -124, 98,   -49, -127, 95,\n        127, -66,  56,  28,   -3,  -105, -11, -84, 35,  -23,  105, 13,  25,  -10, 43,  -19, -89,  -9,   -36, -35,  57,\n        -59, 56,   77,  -118, -99, -117, 5,   -72, 96,  -7,   38,  -11, -4,  69,  61,  -32, -58,  10,   127, 26};\n\n    const int q4_0_ref[]{59,   52,   52,   36,   -89,  -74,  -85,  43,   119,  -16,  -71,  99,   121,  -103, -40, -19,\n                         -52,  87,   -46,  -74,  -87,  104,  105,  -121, -105, -104, 118,  -105, -104, 102,  73,  8,\n                         -57,  -77,  75,   -100, 34,   -75,  -118, 101,  -75,  -124, 93,   -112, 89,   119,  -99, 26,\n                         -23,  -118, -69,  -75,  -120, 101,  58,   53,   125,  20,   -119, -118, -80,  -109, 87,  -119,\n                         105,  120,  -23,  121,  -119, -59,  -70,  -59,  -50,  -77,  -100, -118, 123,  54,   117, 102,\n                         -112, -116, 120,  -72,  -6,   125,  -72,  124,  121,  103,  75,   -78,  -125, -83,  -10, -87,\n                         51,   123,  4,    69,   -42,  -57,  25,   118,  90,   -35,  -25,  -17,  34,   -79,  27,  117,\n                         37,   54,   -9,   35,   -70,  -14,  40,   15,   -58,  68,   100,  -113, -12,  -101, -99, -77,\n                         -23,  -15,  -121, -42,  41,   -123, 105,  -98,  -119, 74,   74,   -92,  -52,  116,  3,   111};\n\n    const int q4_1_ref[]{60,   52,   59,   -64,  52,  36,   -89,  -74,  -85,  43,   119,  -16,  -71,  99,   121, -103,\n                         -40,  -19,  -52,  87,   85,  53,   89,   -66,  51,   117,  -125, 86,   70,   69,   103, 70,\n                         52,   119,  -108, -11,  6,   28,   -96,  48,   -65,  52,   -121, -65,  100,  -103, 74,  107,\n                         -111, 95,   -91,  -121, 97,  -28,  5,    101,  51,   58,   102,  -103, -42,  52,   58,  -63,\n                         -114, 20,   -118, -102, -64, -93,  104,  -118, 121,  121,  -6,   122,  -102, -58,  -53, -42,\n                         28,   52,   -102, -65,  100, -122, -124, -54,  -102, -103, 127,  115,  -121, 72,   5,   -125,\n                         87,   -109, -122, -104, -80, 50,   63,   -66,  124,  99,   9,    103,  -36,  -123, -5,  -70,\n                         41,   72,   -9,   -103, -74, 50,   41,   33,   122,  49,   34,   -67,  -28,  -117, -38, -54,\n                         9,    -35,  86,   13,   -41, -15,  74,   -69,  -101, 112,  27,   116,  -47,  51,   11,  -65,\n                         22,   14,   -120, 57,   -41, 122,  -90,  114,  119,  -75,  -75,  91,   68,   -117, -4,  -112};\n\n    const int q5_0_ref[]{\n        59,  48,  48,  125,  -100, 121, 103, 55,   78,   109,  86,   69,  -34, -32, 98,  -58, -13, 18,   -79,  -55,\n        120, -82, -46, -78,  7,    -51, -79, -79,  51,   -64,  -78,  -1,  30,  47,  -35, 46,  32,  -36,  -111, 0,\n        126, 101, 119, 55,   34,   -79, 81,  95,   45,   125,  20,   -54, 89,  8,   -71, 32,  -93, -18,  42,   35,\n        -61, 3,   119, 105,  1,    -53, 58,  49,   -115, 95,   -68,  -12, -6,  24,  2,   3,   96,  38,   -81,  2,\n        -62, -48, -62, -29,  19,   123, 101, -118, -50,  -81,  -121, 125, -63, 22,  39,  -13, -25, 107,  -21,  -36,\n        32,  25,  -31, 111,  -11,  -6,  97,  -40,  -13,  -34,  75,   -82, 42,  -76, 15,  -29, 22,  74,   -3,   65,\n        86,  -11, 8,   -118, -67,  126, 17,  -36,  -109, -85,  -50,  -50, 34,  -83, 65,  -93, -48, -28,  23,   -7,\n        75,  107, -2,  69,   100,  -13, 65,  14,   -117, -103, -56,  15,  -40, 23,  -99, -81, -47, -105, -85,  25,\n        -61, -13, -2,  -99,  65,   27,  -78, 27,   17,   116,  -124, 73,  119, -7,  6,   -33};\n\n    const int q5_1_ref[]{25,   48,   59,   -64, 48,   125,  -100, 121,  104,  56,   95,  125,  87,   70,  -18,  -16,\n                         99,   -57,  -13,  35,  -79,  -38,  -119, -81,  41,   49,   89,  -66,  0,    32,  4,    76,\n                         102,  -6,   7,    -69, -115, 123,  -34,  125,  121,  -17,  56,  -6,   13,   40,  81,   96,\n                         -104, 48,   -121, -65, 46,   -96,  -46,  -126, -55,  36,   117, -42,  51,   -81, 74,   15,\n                         -78,  -39,  10,   -38, 102,  101,  -36,  35,   -82,  48,   58,  -63,  -51,  95,  -67,  -12,\n                         13,   25,   20,   37,  -128, 70,   -64,  20,   -28,  -14,  -12, -11,  53,   -84, -121, -68,\n                         -13,  47,   -102, -65, 120,  -126, 62,   -23,  -40,  12,   25,  -108, 36,   35,  -17,  -25,\n                         31,   -112, 11,   5,   -82,  39,   29,   33,   121,  46,   63,  -66,  -43,  75,  -16,  28,\n                         -7,   -58,  2,    -50, -87,  27,   -9,   118,  83,   -126, -18, 35,   108,  101, 66,   66,\n                         76,   45,   34,   -67, -66,  92,   47,   27,   -23,  22,   -76, -92,  2,    -70, -84,  12,\n                         -65,  -14,  116,  103, 55,   -15,  55,   -23,  -112, 47,   11,  -65,  46,   104, 84,   -26,\n                         44,   12,   1,    98,  -66,  -28,  77,   -44,  -18,  -118, 122, -74,  -121, 6,   -7,   32};\n\n    auto ctx = make_unique_ggml_context(1024 * 1024, nullptr, false);\n\n    ggml_tensor *src = ggml_new_tensor_2d(ctx.get(), GGML_TYPE_F32, 128, 2);\n    memcpy(src->data, src_data, sizeof(src_data));\n\n    [[maybe_unused]] auto qtensor_to_string = [](ggml_tensor *tensor) {\n        std::ostringstream oss;\n        oss << \"Q8: [\";\n        for (size_t i = 0; i < ggml_nbytes(tensor); i++) {\n            oss << (i > 0 ? \", \" : \"\") << (int)((char *)tensor->data)[i];\n        }\n        oss << \"]\";\n        return oss.str();\n    };\n\n    // q8_0\n    {\n        ggml_tensor *q8_dst = ggml_new_tensor_2d(ctx.get(), GGML_TYPE_Q8_0, 128, 2);\n        ggml_quantize_chunk(GGML_TYPE_Q8_0, (float *)src->data, q8_dst->data, 0, src->ne[1], src->ne[0], nullptr);\n        // std::cout << qtensor_to_string(q8_dst) << '\\n';\n        EXPECT_TRUE(memcmp(q8_dst->data, q8_ref, sizeof(q8_ref)));\n    }\n    // q4_0\n    {\n        ggml_tensor *q4_dst = ggml_new_tensor_2d(ctx.get(), GGML_TYPE_Q4_0, 128, 2);\n        ggml_quantize_chunk(GGML_TYPE_Q4_0, (float *)src->data, q4_dst->data, 0, src->ne[1], src->ne[0], nullptr);\n        // std::cout << qtensor_to_string(q4_dst) << '\\n';\n        EXPECT_TRUE(memcmp(q4_dst->data, q4_0_ref, sizeof(q4_0_ref)));\n    }\n    // q4_1\n    {\n        ggml_tensor *q4_dst = ggml_new_tensor_2d(ctx.get(), GGML_TYPE_Q4_1, 128, 2);\n        ggml_quantize_chunk(GGML_TYPE_Q4_1, (float *)src->data, q4_dst->data, 0, src->ne[1], src->ne[0], nullptr);\n        // std::cout << qtensor_to_string(q4_dst) << '\\n';\n        EXPECT_TRUE(memcmp(q4_dst->data, q4_1_ref, sizeof(q4_1_ref)));\n    }\n    // q5_0\n    {\n        ggml_tensor *q5_dst = ggml_new_tensor_2d(ctx.get(), GGML_TYPE_Q5_0, 128, 2);\n        ggml_quantize_chunk(GGML_TYPE_Q5_0, (float *)src->data, q5_dst->data, 0, src->ne[1], src->ne[0], nullptr);\n        // std::cout << qtensor_to_string(q5_dst) << '\\n';\n        EXPECT_TRUE(memcmp(q5_dst->data, q5_0_ref, sizeof(q5_0_ref)));\n    }\n    // q5_1\n    {\n        ggml_tensor *q5_dst = ggml_new_tensor_2d(ctx.get(), GGML_TYPE_Q5_1, 128, 2);\n        ggml_quantize_chunk(GGML_TYPE_Q5_1, (float *)src->data, q5_dst->data, 0, src->ne[1], src->ne[0], nullptr);\n        // std::cout << qtensor_to_string(q5_dst) << '\\n';\n        EXPECT_TRUE(memcmp(q5_dst->data, q5_1_ref, sizeof(q5_1_ref)));\n    }\n}\n\nstruct TokenizerTestCase {\n    std::string prompt;\n    std::vector<int> input_ids;\n    bool skip_decode = false;\n};\n\nstatic void check_tokenizer(const BaseTokenizer *tokenizer, const std::vector<TokenizerTestCase> &cases) {\n    for (const auto &c : cases) {\n        // encode\n        std::vector<int> input_ids = tokenizer->encode(c.prompt, 2048);\n        EXPECT_EQ(input_ids, c.input_ids);\n        if (!c.skip_decode) {\n            // decode\n            std::string output = tokenizer->decode(c.input_ids);\n            EXPECT_EQ(output, c.prompt);\n        }\n    }\n}\n\nstatic void check_chat_format(const Pipeline &pipeline) {\n    GenerationConfig gen_config;\n    gen_config.max_new_tokens = 1;\n    EXPECT_THROW(\n        {\n            pipeline.chat({{ChatMessage::ROLE_USER, \"user\"}, {ChatMessage::ROLE_USER, \"user\"}}, gen_config);\n        },\n        std::runtime_error);\n    EXPECT_THROW({ pipeline.chat({{ChatMessage::ROLE_ASSISTANT, \"assistant\"}}, gen_config); }, std::runtime_error);\n    EXPECT_THROW(\n        {\n            pipeline.chat({{ChatMessage::ROLE_USER, \"user\"}, {ChatMessage::ROLE_ASSISTANT, \"assistant\"}}, gen_config);\n        },\n        std::runtime_error);\n    // never throw with system prompt\n    pipeline.chat({{ChatMessage::ROLE_SYSTEM, \"system\"}, {ChatMessage::ROLE_USER, \"user\"}}, gen_config);\n}\n\nTEST(Pipeline, ChatGLM) {\n    fs::path model_path = fs::path(__FILE__).parent_path() / \"models/chatglm-ggml.bin\";\n    if (!fs::exists(model_path)) {\n        GTEST_SKIP() << \"Skipping ChatGLM e2e test (ggml model not found)\";\n    }\n    Pipeline pipeline(model_path.string());\n    ASSERT_TRUE(dynamic_cast<ChatGLMTokenizer *>(pipeline.tokenizer.get()));\n    ASSERT_TRUE(dynamic_cast<ChatGLMForCausalLM *>(pipeline.model.get()));\n\n    // tokenizer\n    {\n        std::vector<TokenizerTestCase> cases{\n            {\"‰Ω†Â•Ω\", {5, 74874, 130001, 130004}},\n            {\"[Round 0]\\nÈóÆÔºö‰Ω†Â•Ω\\nÁ≠îÔºö‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\\n[Round \"\n             \"1]\\nÈóÆÔºöÊôö‰∏äÁù°‰∏çÁùÄÂ∫îËØ•ÊÄé‰πàÂäû\\nÁ≠îÔºö\",\n             {53,     6945,   5,      8,     42,    4,     64286,  12,    74874, 4,   67342,  12,    74874, 130328,\n              130247, 130233, 130227, 35,    65806, 68241, 75890,  14132, 5388,  340, 11,     21,    222,   6,\n              76693,  66877,  63852,  6,     66430, 68747, 102501, 63823, 4,     52,  6945,   5,     9,     42,\n              4,      64286,  12,     65450, 83400, 64213, 66846,  4,     67342, 12,  130001, 130004}},\n            {\"def main():\\n    print('hello world')\\t# greeting\",\n             {1616, 594, 125936, 4, 130011, 2274, 89, 7283, 398, 125686, 130008, 61, 25672, 130001, 130004}}};\n        check_tokenizer(pipeline.tokenizer.get(), cases);\n    }\n\n    // prompter\n    {\n        EXPECT_EQ(ChatGLMTokenizer::apply_chat_template_text({{ChatMessage::ROLE_USER, \"‰Ω†Â•Ω\"}}), \"‰Ω†Â•Ω\");\n        EXPECT_EQ(\n            ChatGLMTokenizer::apply_chat_template_text({\n                {ChatMessage::ROLE_USER, \"‰Ω†Â•Ω\"},\n                {ChatMessage::ROLE_ASSISTANT, \"‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\"},\n                {ChatMessage::ROLE_USER, \"Êôö‰∏äÁù°‰∏çÁùÄÂ∫îËØ•ÊÄé‰πàÂäû\"},\n            }),\n            \"[Round 0]\\nÈóÆÔºö‰Ω†Â•Ω\\nÁ≠îÔºö‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã \"\n            \"ChatGLM-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\\n[Round 1]\\nÈóÆÔºöÊôö‰∏äÁù°‰∏çÁùÄÂ∫îËØ•ÊÄé‰πàÂäû\\nÁ≠îÔºö\");\n    }\n\n    // chat\n    {\n        check_chat_format(pipeline);\n        GenerationConfig gen_config;\n        gen_config.do_sample = false;\n        std::vector<ChatMessage> messages{{ChatMessage::ROLE_USER, \"‰Ω†Â•Ω\"}};\n        ChatMessage output = pipeline.chat(messages, gen_config);\n        EXPECT_EQ(output.content, \"‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\");\n    }\n}\n\nTEST(Pipeline, ChatGLM2) {\n    fs::path model_path = fs::path(__FILE__).parent_path() / \"models/chatglm2-ggml.bin\";\n    if (!fs::exists(model_path)) {\n        GTEST_SKIP() << \"Skipping ChatGLM2 e2e test (ggml model not found)\";\n    }\n    Pipeline pipeline(model_path.string());\n    ASSERT_TRUE(dynamic_cast<ChatGLM2Tokenizer *>(pipeline.tokenizer.get()));\n    ASSERT_TRUE(dynamic_cast<ChatGLM2ForCausalLM *>(pipeline.model.get()));\n\n    // tokenizer\n    {\n        std::vector<TokenizerTestCase> cases{\n            {\"‰Ω†Â•Ω\", {64790, 64792, 36474, 54591}},\n            {\"[Round 1]\\n\\nÈóÆÔºö‰Ω†Â•Ω\\n\\nÁ≠îÔºö\",\n             {64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 39701, 13, 13, 55437, 31211}},\n            {\"[Round 1]\\n\\nÈóÆÔºö‰Ω†Â•Ω\\n\\nÁ≠îÔºö‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã \"\n             \"ChatGLM2-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\\n\\n[Round 2]\\n\\nÈóÆÔºöÊôö‰∏äÁù°‰∏çÁùÄÂ∫îËØ•ÊÄé‰πàÂäû\\n\\nÁ≠îÔºö\",\n             {64790, 64792, 790,   30951, 517,   30910, 30939, 30996, 13,    13,    54761, 31211, 39701,\n              13,    13,    55437, 31211, 39701, 243,   162,   148,   142,   31404, 33030, 34797, 42481,\n              22011, 10461, 30944, 30943, 30941, 30978, 30949, 31123, 48895, 35214, 54622, 31123, 32616,\n              39905, 31901, 31639, 31155, 13,    13,    30995, 30951, 517,   30910, 30943, 30996, 13,\n              13,    54761, 31211, 32820, 54266, 31876, 35153, 13,    13,    55437, 31211}},\n            {\"def main():\\n    print('hello world')\\t# greeting\",\n             {64790, 64792, 884, 1301, 9427, 13, 296, 4466, 2029, 15616, 30914, 993, 3387, 12, 31010, 30174}}};\n        check_tokenizer(pipeline.tokenizer.get(), cases);\n    }\n\n    // prompter\n    {\n        EXPECT_EQ(ChatGLM2Tokenizer::apply_chat_template_text({{ChatMessage::ROLE_USER, \"‰Ω†Â•Ω\"}}),\n                  \"[Round 1]\\n\\nÈóÆÔºö‰Ω†Â•Ω\\n\\nÁ≠îÔºö\");\n        EXPECT_EQ(\n            ChatGLM2Tokenizer::apply_chat_template_text({\n                {ChatMessage::ROLE_USER, \"‰Ω†Â•Ω\"},\n                {ChatMessage::ROLE_ASSISTANT, \"‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM2-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\"},\n                {ChatMessage::ROLE_USER, \"Êôö‰∏äÁù°‰∏çÁùÄÂ∫îËØ•ÊÄé‰πàÂäû\"},\n            }),\n            \"[Round 1]\\n\\nÈóÆÔºö‰Ω†Â•Ω\\n\\nÁ≠îÔºö‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã \"\n            \"ChatGLM2-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\\n\\n[Round 2]\\n\\nÈóÆÔºöÊôö‰∏äÁù°‰∏çÁùÄÂ∫îËØ•ÊÄé‰πàÂäû\\n\\nÁ≠îÔºö\");\n    }\n\n    // chat\n    {\n        check_chat_format(pipeline);\n        GenerationConfig gen_config;\n        gen_config.do_sample = false;\n        std::vector<ChatMessage> messages{{ChatMessage::ROLE_USER, \"‰Ω†Â•Ω\"}};\n        ChatMessage output = pipeline.chat(messages, gen_config);\n        EXPECT_EQ(output.content, \"‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM2-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\");\n    }\n}\n\nstatic inline std::string read_text(const fs::path &path) {\n    std::ifstream ifs(path);\n    std::ostringstream oss;\n    oss << ifs.rdbuf();\n    return oss.str();\n}\n\nTEST(Pipeline, ChatGLM3) {\n    fs::path model_path = fs::path(__FILE__).parent_path() / \"models/chatglm3-ggml.bin\";\n    if (!fs::exists(model_path)) {\n        GTEST_SKIP() << \"Skipping ChatGLM3 e2e test (ggml model not found)\";\n    }\n    Pipeline pipeline(model_path.string());\n    ASSERT_TRUE(dynamic_cast<ChatGLM3Tokenizer *>(pipeline.tokenizer.get()));\n    ASSERT_TRUE(dynamic_cast<ChatGLM3ForCausalLM *>(pipeline.model.get()));\n\n    const std::string system_tool_call =\n        read_text(fs::path(__FILE__).parent_path() / \"examples/system/function_call.txt\");\n    const std::string system_ci = read_text(fs::path(__FILE__).parent_path() / \"examples/system/code_interpreter.txt\");\n\n    // tokenizer\n    {\n        std::vector<int> target_ids{64790, 64792, 36474, 54591};\n        std::vector<int> input_ids = pipeline.tokenizer->encode(\"‰Ω†Â•Ω\", 2048);\n        EXPECT_EQ(input_ids, target_ids);\n    }\n    {\n        std::vector<ChatMessage> messages{{ChatMessage::ROLE_USER, \"‰Ω†Â•Ω\"}};\n        std::vector<int> input_ids = pipeline.tokenizer->apply_chat_template(messages, 2048);\n        std::vector<int> target_ids{64790, 64792, 64795, 30910, 13, 36474, 54591, 64796};\n        EXPECT_EQ(input_ids, target_ids);\n    }\n    {\n        std::vector<ChatMessage> messages{\n            {ChatMessage::ROLE_USER, \"‰Ω†Â•Ω\"},\n            {ChatMessage::ROLE_ASSISTANT, \"‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM3-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\"},\n            {ChatMessage::ROLE_USER, \"Êôö‰∏äÁù°‰∏çÁùÄÂ∫îËØ•ÊÄé‰πàÂäû\"},\n        };\n        std::vector<int> input_ids = pipeline.tokenizer->apply_chat_template(messages, 2048);\n        std::vector<int> target_ids{64790, 64792, 64795, 30910, 13,    36474, 54591, 64796, 30910, 13,    36474, 54591,\n                                    243,   162,   148,   142,   31404, 33030, 34797, 42481, 22011, 10461, 30944, 30966,\n                                    30941, 30978, 30949, 31123, 48895, 35214, 54622, 31123, 32616, 39905, 31901, 31639,\n                                    31155, 64795, 30910, 13,    30910, 32820, 54266, 31876, 35153, 64796};\n        EXPECT_EQ(input_ids, target_ids);\n    }\n    {\n        std::vector<ChatMessage> messages{\n            {ChatMessage::ROLE_SYSTEM, system_tool_call},\n            {ChatMessage::ROLE_USER, \"ÁîüÊàê‰∏Ä‰∏™ÈöèÊú∫Êï∞\"},\n        };\n        std::vector<int> input_ids = pipeline.tokenizer->apply_chat_template(messages, 2048);\n        std::vector<int> target_ids{\n            64790, 64792, 64794, 30910, 13,    20115, 267,   1762,  2554,  362,   1077,  362,   344,   457,   30930,\n            809,   431,   1675,  289,   267,   1762,  4159,  30954, 13,    30982, 13,    296,   30955, 16599, 30962,\n            11228, 30962, 7311,  1306,  2932,  729,   13,    352,   30955, 2323,  2932,  449,   16599, 30962, 11228,\n            30962, 7311,  1306,  1252,  13,    352,   30955, 16302, 2932,  449,   9398,  711,   260,   5402,  1276,\n            1994,  30932, 268,   30930, 30912, 30930, 2288,  30995, 30940, 30996, 14819, 1994,  906,   2288,  30995,\n            30939, 30996, 1252,  13,    352,   30955, 12209, 2932,  790,   13,    753,   30982, 13,    647,   30955,\n            2323,  2932,  449,   24794, 1252,  13,    647,   30955, 16302, 2932,  449,   1036,  5402,  9352,  1050,\n            422,   267,   17009, 1252,  13,    647,   30955, 3543,  2932,  449,   592,   1252,  13,    647,   30955,\n            20379, 2932,  2033,  13,    753,   4143,  13,    753,   30982, 13,    647,   30955, 2323,  2932,  449,\n            7855,  1252,  13,    647,   30955, 16302, 2932,  449,   1036,  2288,  290,   267,   7383,  3859,  1252,\n            13,    647,   30955, 3543,  2932,  449,   30912, 16471, 30995, 592,   30932, 558,   30996, 1252,  13,\n            647,   30955, 20379, 2932,  2033,  13,    753,   30983, 13,    352,   30996, 13,    296,   4143,  13,\n            296,   30955, 752,   30962, 27564, 2932,  729,   13,    352,   30955, 2323,  2932,  449,   752,   30962,\n            27564, 1252,  13,    352,   30955, 16302, 2932,  449,   4867,  267,   1465,  5100,  332,   4256,  17654,\n            30962, 2323,  31040, 1252,  13,    352,   30955, 12209, 2932,  790,   13,    753,   30982, 13,    647,\n            30955, 2323,  2932,  449,   17654, 30962, 2323,  1252,  13,    647,   30955, 16302, 2932,  449,   1036,\n            1462,  290,   267,   1911,  289,   330,   580,   266,   819,   1252,  13,    647,   30955, 3543,  2932,\n            449,   2069,  1252,  13,    647,   30955, 20379, 2932,  2033,  13,    753,   30983, 13,    352,   30996,\n            13,    296,   30983, 13,    30983, 64795, 30910, 13,    30910, 36454, 31623, 37853, 54744, 64796};\n        EXPECT_EQ(input_ids, target_ids);\n    }\n\n    // chat\n    {\n        // check_chat_format(pipeline);\n        GenerationConfig gen_config;\n        gen_config.do_sample = false;\n        std::vector<ChatMessage> messages{{ChatMessage::ROLE_USER, \"‰Ω†Â•Ω\"}};\n        ChatMessage output = pipeline.chat(messages, gen_config);\n        EXPECT_EQ(output.content, \"‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM3-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\");\n    }\n\n    // tool call\n    {\n        GenerationConfig gen_config;\n        gen_config.do_sample = false;\n        std::vector<ChatMessage> messages{\n            {ChatMessage::ROLE_SYSTEM, system_tool_call},\n            {ChatMessage::ROLE_USER, \"ÁîüÊàê‰∏Ä‰∏™ÈöèÊú∫Êï∞\"},\n        };\n        {\n            ChatMessage output = pipeline.chat(messages, gen_config);\n            EXPECT_EQ(output.role, ChatMessage::ROLE_ASSISTANT);\n            EXPECT_EQ(output.content, \"```python\\n\"\n                                      \"tool_call(seed=42, range=(0, 100))\\n\"\n                                      \"```\");\n            messages.emplace_back(std::move(output));\n        }\n        messages.emplace_back(ChatMessage::ROLE_OBSERVATION, \"22\");\n        {\n            ChatMessage output = pipeline.chat(messages, gen_config);\n            EXPECT_EQ(output.role, ChatMessage::ROLE_ASSISTANT);\n            EXPECT_EQ(output.content, \"Ê†πÊçÆÊÇ®ÁöÑË¶ÅÊ±ÇÔºåÊàë‰ΩøÁî®ÈöèÊú∫Êï∞ÁîüÊàêÂô®APIÁîüÊàê‰∫Ü‰∏Ä‰∏™Âú®0Âíå100‰πãÈó¥ÁöÑÈöèÊú∫Êï∞ÔºåÁªìÊûú‰∏∫22„ÄÇ\");\n        }\n    }\n\n    // code interpreter\n    {\n        GenerationConfig gen_config;\n        gen_config.do_sample = false;\n        std::vector<ChatMessage> messages{\n            {ChatMessage::ROLE_SYSTEM, system_ci},\n            {ChatMessage::ROLE_USER, \"Ê±ÇÂá∫100‰ª•ÂÜÖÁöÑÊâÄÊúâË¥®Êï∞\"},\n        };\n        {\n            ChatMessage output = pipeline.chat(messages, gen_config);\n            EXPECT_EQ(output.role, ChatMessage::ROLE_ASSISTANT);\n            EXPECT_EQ(output.content,\n                      R\"(Ë¥®Êï∞ÊòØÂè™ËÉΩË¢´1ÂíåËá™Ë∫´Êï¥Èô§ÁöÑÊ≠£Êï¥Êï∞„ÄÇÊàë‰ª¨ÂèØ‰ª•ÈÄöËøáÈÅçÂéÜ1Âà∞100ÁöÑÊâÄÊúâÊï∞Â≠óÊù•ÊâæÂá∫100‰ª•ÂÜÖÁöÑÊâÄÊúâË¥®Êï∞„ÄÇ\n\n‰∏ãÈù¢ÊòØÊâæÂá∫100‰ª•ÂÜÖÁöÑÊâÄÊúâË¥®Êï∞ÁöÑPython‰ª£Á†ÅÔºö)\");\n            EXPECT_EQ(output.tool_calls.at(0).code.input, R\"(```python\ndef is_prime(n):\n    \"\"\"Check if a number is prime.\"\"\"\n    if n <= 1:\n        return False\n    if n <= 3:\n        return True\n    if n % 2 == 0 or n % 3 == 0:\n        return False\n    i = 5\n    while i * i <= n:\n        if n % i == 0 or n % (i + 2) == 0:\n            return False\n        i += 6\n    return True\n\nprimes_upto_100 = [i for i in range(2, 101) if is_prime(i)]\nprimes_upto_100\n```)\");\n            messages.emplace_back(std::move(output));\n        }\n        messages.emplace_back(\n            ChatMessage::ROLE_OBSERVATION,\n            \"[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\");\n        {\n            ChatMessage output = pipeline.chat(messages, gen_config);\n            EXPECT_EQ(output.role, ChatMessage::ROLE_ASSISTANT);\n            EXPECT_EQ(output.content,\n                      R\"(100‰ª•ÂÜÖÁöÑÊâÄÊúâË¥®Êï∞‰∏∫Ôºö\n\n$$\n2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97 \n$$)\");\n        }\n    }\n}\n\nTEST(Pipeline, ChatGLM4) {\n    fs::path model_path = fs::path(__FILE__).parent_path() / \"models/chatglm4-ggml.bin\";\n    if (!fs::exists(model_path)) {\n        GTEST_SKIP() << \"Skipping ChatGLM4 e2e test (ggml model not found)\";\n    }\n    Pipeline pipeline(model_path.string());\n    ASSERT_TRUE(dynamic_cast<ChatGLM4Tokenizer *>(pipeline.tokenizer.get()));\n    ASSERT_TRUE(dynamic_cast<ChatGLM4ForCausalLM *>(pipeline.model.get()));\n    auto tokenizer = dynamic_cast<ChatGLM4Tokenizer *>(pipeline.tokenizer.get());\n\n    // const std::string system_tool_call =\n    //     read_text(fs::path(__FILE__).parent_path() / \"examples/system/function_call.txt\");\n    // const std::string system_ci = read_text(fs::path(__FILE__).parent_path() /\n    // \"examples/system/code_interpreter.txt\");\n\n    // tiktoken\n    {\n        // taken from:\n        // https://github.com/ggerganov/llama.cpp/blob/4bfe50f741479c1df1c377260c3ff5702586719e/convert-hf-to-gguf.py#L413\n        const std::string chktxt =\n            \"\\n \\n\\n \\n\\n\\n \\t \\t\\t \\t\\n  \\n   \\n    \\n     \\nüöÄ (normal) üò∂\\u200düå´Ô∏è (multiple emojis \"\n            \"concatenated) \"\n            \"‚úÖ ü¶ôü¶ô 3 33 333 3333 33333 333333 3333333 33333333 3.3 3..3 3...3 \"\n            \"·ûÄ·û∂·ûì·üã·ûè·üÇ·ûñ·û∑·ûü·üÅ·ûü·û¢·û∂·ûÖüòÅ \"\n            \"?ÊàëÊÉ≥Âú®appleÂ∑•‰Ωú1314151Â§©ÔΩû ------======= –Ω–µ—â–æ –Ω–∞ –ë—ä–ª–≥–∞—Ä—Å–∫–∏ ''''''```````\\\"\\\"\\\"\\\"......!!!!!!?????? I've \"\n            \"been 'told he's there, 'RE you sure? 'M not sure I'll make it, 'D you like some tea? We'Ve a'lL\";\n\n        const std::vector<int> ref_ids{\n            198,    4710,   14721, 65020,  7847,   1572,  2303,   78043,  10942, 9281,   248,    222,   320,    8251,\n            8,      26440,  114,   124564, 9281,   234,   104,    30423,  320,   35495,  98226,  96714, 8,      25442,\n            227,    11157,  99,    247,    9281,   99,    247,    220,    18,    220,    100702, 220,   121577, 220,\n            121577, 18,     220,   121577, 100702, 220,   121577, 121577, 220,   121577, 121577, 18,    220,    121577,\n            121577, 100702, 220,   18,     13,     18,    220,    18,     496,   18,     220,    18,    1112,   18,\n            220,    20833,  222,   96709,  241,    44002, 233,    20833,  237,   44002,  224,    20833, 244,    20833,\n            115,    20833,  253,   44002,  223,    20833, 253,    20833,  95,    96709,  227,    74764, 223,    937,\n            101446, 98319,  22320, 98538,  118901, 19,    99082,  16,     98411, 21168,  55088,  52883, 18625,  131040,\n            13065,  146335, 78377, 3355,   4605,   4605,  13865,  13865,  73022, 3014,   3014,   28052, 17066,  2928,\n            26524,  7646,   358,   3003,   1012,   364,   83,     813,    566,   594,    1052,   11,    364,    787,\n            498,    2704,   30,    364,    44,     537,   2704,   358,    3278,  1281,   432,    11,    364,    35,\n            498,    1075,   1045,  15231,  30,     1205,  6,      42368,  264,   63409,  43};\n\n        const std::vector<int> out_ids = tokenizer->core_bpe.encode_ordinary(chktxt);\n        EXPECT_EQ(ref_ids, out_ids);\n    }\n    {\n        const std::string text = R\"(\n```c++\n#include <iostream>\n\nint main() {\n    printf(\"hello world\\n\");    // say hello\n}\n```\n\n```python\nif __name__ == '__main__':\n    print('hello world')        # say hello\n```\n)\";\n        const std::vector<int> ref_ids = {198,   73022, 66,    22879, 1067,  366,   9661,  1339, 396,   1887, 368,\n                                          341,   262,   4100,  445,   14978, 1879,  1699,  5038, 262,   442,  1977,\n                                          23745, 198,   532,   13865, 19288, 73022, 12663, 198,  333,   1304, 606,\n                                          563,   621,   12106, 3817,  16165, 262,   1173,  492,  14978, 1879, 863,\n                                          286,   671,   1977,  23745, 198,   13865, 3989};\n        const std::vector<int> out_ids = tokenizer->core_bpe.encode_ordinary(text);\n        EXPECT_EQ(ref_ids, out_ids);\n    }\n    // tokenizer\n    {\n        std::vector<int> target_ids{151331, 151333, 109377};\n        std::vector<int> input_ids = pipeline.tokenizer->encode(\"‰Ω†Â•Ω\", 2048);\n        EXPECT_EQ(input_ids, target_ids);\n    }\n    {\n        std::vector<ChatMessage> messages{{ChatMessage::ROLE_USER, \"‰Ω†Â•Ω\"}};\n        std::vector<int> input_ids = pipeline.tokenizer->apply_chat_template(messages, 2048);\n        std::vector<int> target_ids{151331, 151333, 151336, 198, 109377, 151337};\n        EXPECT_EQ(input_ids, target_ids);\n    }\n    {\n        std::vector<ChatMessage> messages{{ChatMessage::ROLE_USER, \"‰Ω†Â•Ω\"},\n                                          {ChatMessage::ROLE_ASSISTANT, \"‰Ω†Â•ΩüëãÔºÅÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊúâ‰ªÄ‰πàÂèØ‰ª•Â∏ÆÂä©‰Ω†ÁöÑÂêóÔºü\"},\n                                          {ChatMessage::ROLE_USER, \"Êôö‰∏äÁù°‰∏çÁùÄÂ∫îËØ•ÊÄé‰πàÂäû\"}};\n        std::vector<int> input_ids = pipeline.tokenizer->apply_chat_template(messages, 2048);\n        std::vector<int> target_ids{151331, 151333, 151336, 198,    109377, 151337, 198,    109377, 9281,  239,\n                                    233,    6313,   118295, 103810, 98406,  3837,   101665, 110368, 99444, 99212,\n                                    11314,  151336, 198,    101160, 120410, 99379,  103298, 151337};\n        EXPECT_EQ(input_ids, target_ids);\n    }\n    // {\n    //     std::vector<ChatMessage> messages{\n    //         {ChatMessage::ROLE_SYSTEM, system_tool_call},\n    //         {ChatMessage::ROLE_USER, \"ÁîüÊàê‰∏Ä‰∏™ÈöèÊú∫Êï∞\"},\n    //     };\n    //     std::vector<int> input_ids = pipeline.tokenizer->apply_chat_template(messages, 2048);\n    //     std::vector<int> target_ids{\n    //         64790, 64792, 64794, 30910, 13,    20115, 267,   1762,  2554,  362,   1077,  362,   344,   457,   30930,\n    //         809,   431,   1675,  289,   267,   1762,  4159,  30954, 13,    30982, 13,    296,   30955, 16599, 30962,\n    //         11228, 30962, 7311,  1306,  2932,  729,   13,    352,   30955, 2323,  2932,  449,   16599, 30962, 11228,\n    //         30962, 7311,  1306,  1252,  13,    352,   30955, 16302, 2932,  449,   9398,  711,   260,   5402,  1276,\n    //         1994,  30932, 268,   30930, 30912, 30930, 2288,  30995, 30940, 30996, 14819, 1994,  906,   2288,  30995,\n    //         30939, 30996, 1252,  13,    352,   30955, 12209, 2932,  790,   13,    753,   30982, 13,    647,   30955,\n    //         2323,  2932,  449,   24794, 1252,  13,    647,   30955, 16302, 2932,  449,   1036,  5402,  9352,  1050,\n    //         422,   267,   17009, 1252,  13,    647,   30955, 3543,  2932,  449,   592,   1252,  13,    647,   30955,\n    //         20379, 2932,  2033,  13,    753,   4143,  13,    753,   30982, 13,    647,   30955, 2323,  2932,  449,\n    //         7855,  1252,  13,    647,   30955, 16302, 2932,  449,   1036,  2288,  290,   267,   7383,  3859,  1252,\n    //         13,    647,   30955, 3543,  2932,  449,   30912, 16471, 30995, 592,   30932, 558,   30996, 1252,  13,\n    //         647,   30955, 20379, 2932,  2033,  13,    753,   30983, 13,    352,   30996, 13,    296,   4143,  13,\n    //         296,   30955, 752,   30962, 27564, 2932,  729,   13,    352,   30955, 2323,  2932,  449,   752,   30962,\n    //         27564, 1252,  13,    352,   30955, 16302, 2932,  449,   4867,  267,   1465,  5100,  332,   4256,  17654,\n    //         30962, 2323,  31040, 1252,  13,    352,   30955, 12209, 2932,  790,   13,    753,   30982, 13,    647,\n    //         30955, 2323,  2932,  449,   17654, 30962, 2323,  1252,  13,    647,   30955, 16302, 2932,  449,   1036,\n    //         1462,  290,   267,   1911,  289,   330,   580,   266,   819,   1252,  13,    647,   30955, 3543,  2932,\n    //         449,   2069,  1252,  13,    647,   30955, 20379, 2932,  2033,  13,    753,   30983, 13,    352,   30996,\n    //         13,    296,   30983, 13,    30983, 64795, 30910, 13,    30910, 36454, 31623, 37853, 54744, 64796};\n    //     EXPECT_EQ(input_ids, target_ids);\n    // }\n\n    // chat\n    {\n        // check_chat_format(pipeline);\n        GenerationConfig gen_config;\n        gen_config.do_sample = false;\n        std::vector<ChatMessage> messages{{ChatMessage::ROLE_USER, \"‰Ω†Â•Ω\"}};\n        ChatMessage output = pipeline.chat(messages, gen_config);\n        EXPECT_EQ(output.content, \"‰Ω†Â•ΩüëãÔºÅÂæàÈ´òÂÖ¥ËÉΩÂ∏ÆÂä©‰Ω†ÔºåÊúâ‰ªÄ‰πàÈóÆÈ¢òÊàñËÄÖÈúÄË¶ÅÂ∏ÆÂä©ÁöÑÂú∞ÊñπÂêóÔºü\");\n    }\n}\n\nTEST(Pipeline, CodeGeeX2) {\n    fs::path model_path = fs::path(__FILE__).parent_path() / \"models/codegeex2-ggml.bin\";\n    if (!fs::exists(model_path)) {\n        GTEST_SKIP() << \"Skipping CodeGeeX2 e2e test (ggml model not found)\";\n    }\n    Pipeline pipeline(model_path.string());\n    ASSERT_TRUE(dynamic_cast<ChatGLM2Tokenizer *>(pipeline.tokenizer.get()));\n    ASSERT_TRUE(dynamic_cast<ChatGLM2ForCausalLM *>(pipeline.model.get()));\n\n    // tokenizer\n    {\n        std::vector<TokenizerTestCase> cases{\n            {\"# language: Python\\n# write a bubble sort function\\n\",\n             {64790, 64792, 31010, 3239, 30954, 16719, 13, 31010, 3072, 260, 17338, 3482, 1674, 13}}};\n        check_tokenizer(pipeline.tokenizer.get(), cases);\n    }\n\n    // generate\n    {\n        GenerationConfig gen_config;\n        gen_config.do_sample = false;\n        gen_config.max_length = 256;\n\n        std::string prompt = \"# language: Python\\n# write a bubble sort function\\n\";\n        std::string target = R\"(\n\ndef bubble_sort(lst):\n    for i in range(len(lst) - 1):\n        for j in range(len(lst) - 1 - i):\n            if lst[j] > lst[j + 1]:\n                lst[j], lst[j + 1] = lst[j + 1], lst[j]\n    return lst\n\n\nprint(bubble_sort([5, 4, 3, 2, 1])))\";\n\n        std::string output = pipeline.generate(prompt, gen_config);\n        EXPECT_EQ(output, target);\n    }\n}\n\nTEST(Pipeline, ChatGLM4V) {\n    fs::path model_path = fs::path(__FILE__).parent_path() / \"models/chatglm4v-ggml.bin\";\n    if (!fs::exists(model_path)) {\n        GTEST_SKIP() << \"Skipping ChatGLM4V e2e test (ggml model not found)\";\n    }\n    Pipeline pipeline(model_path.string());\n    ASSERT_TRUE(dynamic_cast<ChatGLM4Tokenizer *>(pipeline.tokenizer.get()));\n    ASSERT_TRUE(dynamic_cast<ChatGLM4VForCausalLM *>(pipeline.model.get()));\n\n    // tokenizer\n    {\n        fs::path image_path = fs::path(__FILE__).parent_path() / \"examples/03-Confusing-Pictures.jpg\";\n        Image image = Image::open(image_path.string());\n        std::vector<ChatMessage> messages{{ChatMessage::ROLE_USER, \"ÊèèËø∞ËøôÂº†ÂõæÁâá\", image}};\n        std::vector<int> target_ids{151331, 151333, 151336, 198,    151339, 151329,\n                                    151340, 100395, 108627, 100736, 151337};\n        std::vector<int> input_ids = pipeline.tokenizer->apply_chat_template(messages, 2048);\n        EXPECT_EQ(input_ids, target_ids);\n    }\n    // chat\n    {\n        // check_chat_format(pipeline);\n        GenerationConfig gen_config;\n        gen_config.do_sample = false;\n        std::vector<ChatMessage> messages{{ChatMessage::ROLE_USER, \"‰Ω†Â•Ω\"}};\n        ChatMessage output = pipeline.chat(messages, gen_config);\n        EXPECT_EQ(output.content, \"‰Ω†Â•ΩüëãÔºÅÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\");\n    }\n    // chat with image\n    {\n        GenerationConfig gen_config;\n        gen_config.do_sample = false;\n        fs::path image_path = fs::path(__FILE__).parent_path() / \"examples/03-Confusing-Pictures.jpg\";\n        Image image = Image::open(image_path.string());\n        std::vector<ChatMessage> messages{{ChatMessage::ROLE_USER, \"ËøôÂº†ÂõæÁâáÊúâ‰ªÄ‰πà‰∏çÂØªÂ∏∏ÁöÑÂú∞Êñπ\", std::move(image)}};\n        ChatMessage output = pipeline.chat(messages, gen_config);\n        EXPECT_EQ(output.content,\n                  \"ËøôÂº†ÂõæÁâá‰∏≠‰∏çÂØªÂ∏∏ÁöÑÂú∞ÊñπÂú®‰∫éÔºåÁî∑Â≠êÊ≠£Âú®‰∏ÄËæÜÈªÑËâ≤Âá∫ÁßüËΩ¶ÂêéÈù¢ÁÜ®Ë°£Êúç„ÄÇÈÄöÂ∏∏ÊÉÖÂÜµ‰∏ãÔºåÁÜ®Ë°£ÊòØÂú®ÂÆ∂‰∏≠ÊàñÊ¥óË°£Â∫óËøõË°åÁöÑ\"\n                  \"ÔºåËÄå‰∏çÊòØÂú®ËΩ¶ËæÜ‰∏ä„ÄÇÊ≠§Â§ñÔºåÂá∫ÁßüËΩ¶Âú®Ë°åÈ©∂‰∏≠ÔºåÁî∑Â≠êÂç¥ËÉΩÂ§üÁ®≥ÂÆöÂú∞ÁÜ®Ë°£ÔºåËøôÂ¢ûÂä†‰∫ÜÂú∫ÊôØÁöÑËçíËØûÊÑü„ÄÇ\");\n    }\n}\n\nstatic void run_benchmark(const fs::path &model_path) {\n    if (!fs::exists(model_path)) {\n        GTEST_SKIP() << \"Skipping benchmark test (model \" << model_path << \" not found)\";\n    }\n\n    ggml_time_init();\n    int64_t start_ms = ggml_time_ms();\n    Pipeline pipeline(model_path.string());\n    int64_t load_model_ms = ggml_time_ms() - start_ms;\n\n    start_ms = ggml_time_ms();\n    std::vector<ChatMessage> messages{\n        {ChatMessage::ROLE_USER, \"‰Ω†Â•Ω\"},\n        {ChatMessage::ROLE_ASSISTANT, \"‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\"},\n        {ChatMessage::ROLE_USER, \"Êôö‰∏äÁù°‰∏çÁùÄÂ∫îËØ•ÊÄé‰πàÂäû\"},\n    };\n\n    GenerationConfig gen_config;\n    gen_config.do_sample = false;\n\n    PerfStreamer streamer;\n    start_ms = ggml_time_ms();\n    pipeline.chat(messages, gen_config, &streamer);\n    int64_t gen_s = (ggml_time_ms() - start_ms) / 1000.f;\n\n    std::cout << \"======== benchmark results for \" << model_path.filename() << \" ========\\n\"\n              << \"model loaded within: \" << load_model_ms << \" ms\\n\"\n              << \"generation finished within: \" << gen_s << \" s\\n\"\n              << streamer.to_string() << \"\\n\"\n              << \"===========================================================\\n\";\n}\n\nTEST(Benchmark, ChatGLM) {\n    fs::path model_path = fs::path(__FILE__).parent_path() / \"models/chatglm-ggml.bin\";\n    run_benchmark(model_path);\n}\n\nTEST(Benchmark, ChatGLM2) {\n    fs::path model_path = fs::path(__FILE__).parent_path() / \"models/chatglm2-ggml.bin\";\n    run_benchmark(model_path);\n}\n\nTEST(Benchmark, ChatGLM4) {\n    fs::path model_path = fs::path(__FILE__).parent_path() / \"models/chatglm4-ggml.bin\";\n    run_benchmark(model_path);\n}\n\n} // namespace chatglm\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "main.cpp",
          "type": "blob",
          "size": 13.525390625,
          "content": "#include \"chatglm.h\"\n#include <fstream>\n#include <iomanip>\n#include <iostream>\n\n#ifdef _WIN32\n#include <codecvt>\n#include <fcntl.h>\n#include <io.h>\n#include <windows.h>\n#endif\n\nenum InferenceMode {\n    INFERENCE_MODE_CHAT,\n    INFERENCE_MODE_GENERATE,\n};\n\nstatic inline InferenceMode to_inference_mode(const std::string &s) {\n    static std::unordered_map<std::string, InferenceMode> m{{\"chat\", INFERENCE_MODE_CHAT},\n                                                            {\"generate\", INFERENCE_MODE_GENERATE}};\n    return m.at(s);\n}\n\nstruct Args {\n    std::string model_path = \"models/chatglm-ggml.bin\";\n    InferenceMode mode = INFERENCE_MODE_CHAT;\n    bool sync = false;\n    std::string prompt = \"‰Ω†Â•Ω\";\n    std::string system = \"\";\n    std::string image_path = \"\";\n    int max_length = 2048;\n    int max_new_tokens = -1;\n    int max_context_length = 512;\n    bool interactive = false;\n    int top_k = 0;\n    float top_p = 0.7;\n    float temp = 0.95;\n    float repeat_penalty = 1.0;\n    bool verbose = false;\n};\n\nstatic void usage(const std::string &prog) {\n    std::cout << \"Usage: \" << prog << R\"( [options]\n\noptions:\n  -h, --help            show this help message and exit\n  -m, --model PATH      model path (default: models/chatglm-ggml.bin)\n  --mode                inference mode chosen from {chat, generate} (default: chat)\n  --sync                synchronized generation without streaming\n  -p, --prompt PROMPT   prompt to start generation with (default: ‰Ω†Â•Ω)\n  --pp, --prompt_path   path to the plain text file that stores the prompt\n  -s, --system SYSTEM   system message to set the behavior of the assistant\n  --sp, --system_path   path to the plain text file that stores the system message\n  --image               path to the input image for visual language models\n  -i, --interactive     run in interactive mode\n  -l, --max_length N    max total length including prompt and output (default: 2048)\n  --max_new_tokens N    max number of tokens to generate, ignoring the number of prompt tokens\n  -c, --max_context_length N\n                        max context length (default: 512)\n  --top_k N             top-k sampling (default: 0)\n  --top_p N             top-p sampling (default: 0.7)\n  --temp N              temperature (default: 0.95)\n  --repeat_penalty N    penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n  -v, --verbose         display verbose output including config/system/performance info\n)\";\n}\n\nstatic std::string read_text(std::string path) {\n    std::ifstream fin(path);\n    CHATGLM_CHECK(fin) << \"cannot open file \" << path;\n    std::ostringstream oss;\n    oss << fin.rdbuf();\n    return oss.str();\n}\n\nstatic Args parse_args(const std::vector<std::string> &argv) {\n    Args args;\n\n    for (size_t i = 1; i < argv.size(); i++) {\n        const std::string &arg = argv.at(i);\n\n        if (arg == \"-h\" || arg == \"--help\") {\n            usage(argv.at(0));\n            exit(EXIT_SUCCESS);\n        } else if (arg == \"-m\" || arg == \"--model\") {\n            args.model_path = argv.at(++i);\n        } else if (arg == \"--mode\") {\n            args.mode = to_inference_mode(argv.at(++i));\n        } else if (arg == \"--sync\") {\n            args.sync = true;\n        } else if (arg == \"-p\" || arg == \"--prompt\") {\n            args.prompt = argv.at(++i);\n        } else if (arg == \"--pp\" || arg == \"--prompt_path\") {\n            args.prompt = read_text(argv.at(++i));\n        } else if (arg == \"-s\" || arg == \"--system\") {\n            args.system = argv.at(++i);\n        } else if (arg == \"--sp\" || arg == \"--system_path\") {\n            args.system = read_text(argv.at(++i));\n        } else if (arg == \"--image\") {\n            args.image_path = argv.at(++i);\n        } else if (arg == \"-i\" || arg == \"--interactive\") {\n            args.interactive = true;\n        } else if (arg == \"-l\" || arg == \"--max_length\") {\n            args.max_length = std::stoi(argv.at(++i));\n        } else if (arg == \"--max_new_tokens\") {\n            args.max_new_tokens = std::stoi(argv.at(++i));\n        } else if (arg == \"-c\" || arg == \"--max_context_length\") {\n            args.max_context_length = std::stoi(argv.at(++i));\n        } else if (arg == \"--top_k\") {\n            args.top_k = std::stoi(argv.at(++i));\n        } else if (arg == \"--top_p\") {\n            args.top_p = std::stof(argv.at(++i));\n        } else if (arg == \"--temp\") {\n            args.temp = std::stof(argv.at(++i));\n        } else if (arg == \"--repeat_penalty\") {\n            args.repeat_penalty = std::stof(argv.at(++i));\n        } else if (arg == \"-v\" || arg == \"--verbose\") {\n            args.verbose = true;\n        } else {\n            std::cerr << \"Unknown argument: \" << arg << std::endl;\n            usage(argv.at(0));\n            exit(EXIT_FAILURE);\n        }\n    }\n\n    return args;\n}\n\nstatic Args parse_args(int argc, char **argv) {\n    std::vector<std::string> argv_vec;\n    argv_vec.reserve(argc);\n\n#ifdef _WIN32\n    LPWSTR *wargs = CommandLineToArgvW(GetCommandLineW(), &argc);\n    CHATGLM_CHECK(wargs) << \"failed to retrieve command line arguments\";\n\n    std::wstring_convert<std::codecvt_utf8_utf16<wchar_t>> converter;\n    for (int i = 0; i < argc; i++) {\n        argv_vec.emplace_back(converter.to_bytes(wargs[i]));\n    }\n\n    LocalFree(wargs);\n#else\n    for (int i = 0; i < argc; i++) {\n        argv_vec.emplace_back(argv[i]);\n    }\n#endif\n\n    return parse_args(argv_vec);\n}\n\nstatic bool get_utf8_line(std::string &line) {\n#ifdef _WIN32\n    std::wstring wline;\n    bool ret = !!std::getline(std::wcin, wline);\n    std::wstring_convert<std::codecvt_utf8_utf16<wchar_t>> converter;\n    line = converter.to_bytes(wline);\n    return ret;\n#else\n    return !!std::getline(std::cin, line);\n#endif\n}\n\nstatic inline void print_message(const chatglm::ChatMessage &message) {\n    std::cout << message.content << \"\\n\";\n    if (!message.tool_calls.empty() && message.tool_calls.front().type == chatglm::ToolCallMessage::TYPE_CODE) {\n        std::cout << message.tool_calls.front().code.input << \"\\n\";\n    }\n}\n\nstatic void chat(Args &args) {\n    ggml_time_init();\n    int64_t start_load_us = ggml_time_us();\n    chatglm::Pipeline pipeline(args.model_path, args.max_length);\n    int64_t end_load_us = ggml_time_us();\n\n    std::string model_name = pipeline.model->config.model_type_name();\n\n    auto text_streamer = std::make_shared<chatglm::TextStreamer>(std::cout, pipeline.tokenizer.get());\n    auto perf_streamer = std::make_shared<chatglm::PerfStreamer>();\n    std::vector<std::shared_ptr<chatglm::BaseStreamer>> streamers{perf_streamer};\n    if (!args.sync) {\n        streamers.emplace_back(text_streamer);\n    }\n    auto streamer = std::make_unique<chatglm::StreamerGroup>(std::move(streamers));\n\n    chatglm::GenerationConfig gen_config(args.max_length, args.max_new_tokens, args.max_context_length, args.temp > 0,\n                                         args.top_k, args.top_p, args.temp, args.repeat_penalty);\n\n    if (args.verbose) {\n        std::cout << \"system info: | \"\n                  << \"AVX = \" << ggml_cpu_has_avx() << \" | \"\n                  << \"AVX2 = \" << ggml_cpu_has_avx2() << \" | \"\n                  << \"AVX512 = \" << ggml_cpu_has_avx512() << \" | \"\n                  << \"AVX512_VBMI = \" << ggml_cpu_has_avx512_vbmi() << \" | \"\n                  << \"AVX512_VNNI = \" << ggml_cpu_has_avx512_vnni() << \" | \"\n                  << \"FMA = \" << ggml_cpu_has_fma() << \" | \"\n                  << \"NEON = \" << ggml_cpu_has_neon() << \" | \"\n                  << \"ARM_FMA = \" << ggml_cpu_has_arm_fma() << \" | \"\n                  << \"F16C = \" << ggml_cpu_has_f16c() << \" | \"\n                  << \"FP16_VA = \" << ggml_cpu_has_fp16_va() << \" | \"\n                  << \"WASM_SIMD = \" << ggml_cpu_has_wasm_simd() << \" | \"\n                  << \"BLAS = \" << ggml_cpu_has_blas() << \" | \"\n                  << \"SSE3 = \" << ggml_cpu_has_sse3() << \" | \"\n                  << \"VSX = \" << ggml_cpu_has_vsx() << \" |\\n\";\n\n        std::cout << \"inference config: | \"\n                  << \"max_length = \" << args.max_length << \" | \"\n                  << \"max_context_length = \" << args.max_context_length << \" | \"\n                  << \"top_k = \" << args.top_k << \" | \"\n                  << \"top_p = \" << args.top_p << \" | \"\n                  << \"temperature = \" << args.temp << \" | \"\n                  << \"repetition_penalty = \" << args.repeat_penalty << \" |\\n\";\n\n        std::cout << \"loaded \" << pipeline.model->config.model_type_name() << \" model from \" << args.model_path\n                  << \" within: \" << (end_load_us - start_load_us) / 1000.f << \" ms\\n\";\n\n        std::cout << std::endl;\n    }\n\n    if (args.mode != INFERENCE_MODE_CHAT && args.interactive) {\n        std::cerr << \"interactive demo is only supported for chat mode, falling back to non-interactive one\\n\";\n        args.interactive = false;\n    }\n\n    if (!args.image_path.empty() && pipeline.model->config.model_type != chatglm::ModelType::CHATGLM4V) {\n        std::cerr << \"image is specified for model without visual ability, falling back to language mode\\n\";\n        args.image_path.clear();\n    }\n\n    std::optional<chatglm::Image> image;\n    if (!args.image_path.empty()) {\n        image = chatglm::Image::open(args.image_path);\n    }\n\n    std::vector<chatglm::ChatMessage> system_messages;\n    if (!args.system.empty()) {\n        system_messages.emplace_back(chatglm::ChatMessage::ROLE_SYSTEM, args.system);\n    }\n\n    if (args.interactive) {\n        std::cout << R\"(    ________          __  ________    __  ___                 )\" << '\\n'\n                  << R\"(   / ____/ /_  ____ _/ /_/ ____/ /   /  |/  /_________  ____  )\" << '\\n'\n                  << R\"(  / /   / __ \\/ __ `/ __/ / __/ /   / /|_/ // ___/ __ \\/ __ \\ )\" << '\\n'\n                  << R\"( / /___/ / / / /_/ / /_/ /_/ / /___/ /  / // /__/ /_/ / /_/ / )\" << '\\n'\n                  << R\"( \\____/_/ /_/\\__,_/\\__/\\____/_____/_/  /_(_)___/ .___/ .___/  )\" << '\\n'\n                  << R\"(                                              /_/   /_/       )\" << '\\n'\n                  << '\\n';\n\n        std::cout\n            << \"Welcome to ChatGLM.cpp! Ask whatever you want. Type 'clear' to clear context. Type 'stop' to exit.\\n\"\n            << \"\\n\";\n\n        std::vector<chatglm::ChatMessage> messages = system_messages;\n        if (!args.system.empty()) {\n            std::cout << std::setw(model_name.size()) << std::left << \"System\"\n                      << \" > \" << args.system << std::endl;\n        }\n        auto prompt_image = image;\n        while (true) {\n            std::string role;\n            if (!messages.empty() && !messages.back().tool_calls.empty()) {\n                const auto &tool_call = messages.back().tool_calls.front();\n                if (tool_call.type == chatglm::ToolCallMessage::TYPE_FUNCTION) {\n                    // function call\n                    std::cout << \"Function Call > Please manually call function `\" << tool_call.function.name\n                              << \"` with args `\" << tool_call.function.arguments << \"` and provide the results below.\\n\"\n                              << \"Observation   > \" << std::flush;\n                } else if (tool_call.type == chatglm::ToolCallMessage::TYPE_CODE) {\n                    // code interpreter\n                    std::cout << \"Code Interpreter > Please manually run the code and provide the results below.\\n\"\n                              << \"Observation      > \" << std::flush;\n                } else {\n                    CHATGLM_THROW << \"unexpected tool type \" << tool_call.type;\n                }\n                role = chatglm::ChatMessage::ROLE_OBSERVATION;\n            } else {\n                std::cout << std::setw(model_name.size()) << std::left << \"Prompt\"\n                          << \" > \" << std::flush;\n                role = chatglm::ChatMessage::ROLE_USER;\n            }\n            std::string prompt;\n            if (!get_utf8_line(prompt) || prompt == \"stop\") {\n                break;\n            }\n            if (prompt.empty()) {\n                continue;\n            }\n            if (prompt == \"clear\") {\n                messages = system_messages;\n                prompt_image = image;\n                continue;\n            }\n            messages.emplace_back(std::move(role), std::move(prompt), std::move(prompt_image));\n            prompt_image.reset();\n            std::cout << model_name << \" > \";\n            chatglm::ChatMessage output = pipeline.chat(messages, gen_config, streamer.get());\n            if (args.sync) {\n                print_message(output);\n            }\n            messages.emplace_back(std::move(output));\n            if (args.verbose) {\n                std::cout << \"\\n\" << perf_streamer->to_string() << \"\\n\\n\";\n            }\n            perf_streamer->reset();\n        }\n        std::cout << \"Bye\\n\";\n    } else {\n        if (args.mode == INFERENCE_MODE_CHAT) {\n            std::vector<chatglm::ChatMessage> messages = system_messages;\n            messages.emplace_back(chatglm::ChatMessage::ROLE_USER, args.prompt, std::move(image));\n            chatglm::ChatMessage output = pipeline.chat(messages, gen_config, streamer.get());\n            if (args.sync) {\n                print_message(output);\n            }\n        } else {\n            std::string output = pipeline.generate(args.prompt, gen_config, streamer.get());\n            if (args.sync) {\n                std::cout << output << \"\\n\";\n            }\n        }\n        if (args.verbose) {\n            std::cout << \"\\n\" << perf_streamer->to_string() << \"\\n\\n\";\n        }\n    }\n}\n\nint main(int argc, char **argv) {\n#ifdef _WIN32\n    SetConsoleOutputCP(CP_UTF8);\n    _setmode(_fileno(stdin), _O_WTEXT);\n#endif\n\n    try {\n        Args args = parse_args(argc, argv);\n        chat(args);\n    } catch (std::exception &e) {\n        std::cerr << e.what() << std::endl;\n        exit(EXIT_FAILURE);\n    }\n    return 0;\n}\n"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.2998046875,
          "content": "[build-system]\nrequires = [\n    \"setuptools>=42\",\n    \"cmake>=3.12\",\n]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"chatglm-cpp\"\nauthors = [\n    {name = \"Jiahao Li\", email = \"liplus17@163.com\"},\n]\nmaintainers = [\n    {name = \"Jiahao Li\", email = \"liplus17@163.com\"},\n]\ndescription = \"C++ implementation of ChatGLM family models\"\nreadme = \"README.md\"\nrequires-python = \">=3.7\"\nkeywords = [\"ChatGLM\", \"ChatGLM2\", \"ChatGLM3\", \"Large Language Model\"]\nlicense = {text = \"MIT License\"}\nclassifiers = [\n    \"Development Status :: 3 - Alpha\",\n    \"Intended Audience :: Science/Research\",\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n]\ndynamic = [\"version\"]\n\n[project.optional-dependencies]\napi = [\n    \"fastapi[all]\",\n    \"sse-starlette\",\n]\n\n[project.urls]\nHomepage = \"https://github.com/li-plus/chatglm.cpp\"\nRepository = \"https://github.com/li-plus/chatglm.cpp.git\"\n\n# reference: https://black.readthedocs.io/en/stable/usage_and_configuration/the_basics.html#configuration-format\n[tool.black]\nline-length = 120\ninclude = '\\.py$'\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 5.365234375,
          "content": "# reference: https://github.com/pybind/cmake_example\n\nimport os\nimport re\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nfrom setuptools import Extension, find_packages, setup\nfrom setuptools.command.build_ext import build_ext\n\n# Convert distutils Windows platform specifiers to CMake -A arguments\nPLAT_TO_CMAKE = {\n    \"win32\": \"Win32\",\n    \"win-amd64\": \"x64\",\n    \"win-arm32\": \"ARM\",\n    \"win-arm64\": \"ARM64\",\n}\n\n\n# A CMakeExtension needs a sourcedir instead of a file list.\n# The name must be the _single_ output extension from the CMake build.\n# If you need multiple extensions, see scikit-build.\nclass CMakeExtension(Extension):\n    def __init__(self, name: str, sourcedir: str = \"\") -> None:\n        super().__init__(name, sources=[])\n        self.sourcedir = os.fspath(Path(sourcedir).resolve())\n\n\nclass CMakeBuild(build_ext):\n    def build_extension(self, ext: CMakeExtension) -> None:\n        # Must be in this form due to bug in .resolve() only fixed in Python 3.10+\n        ext_fullpath = Path.cwd() / self.get_ext_fullpath(ext.name)\n        extdir = ext_fullpath.parent.resolve()\n\n        # Using this requires trailing slash for auto-detection & inclusion of\n        # auxiliary \"native\" libs\n\n        debug = int(os.environ.get(\"DEBUG\", 0)) if self.debug is None else self.debug\n        cfg = \"Debug\" if debug else \"Release\"\n\n        # CMake lets you override the generator - we need to check this.\n        # Can be set with Conda-Build, for example.\n        cmake_generator = os.environ.get(\"CMAKE_GENERATOR\", \"\")\n\n        # Set Python_EXECUTABLE instead if you use PYBIND11_FINDPYTHON\n        # EXAMPLE_VERSION_INFO shows you how to pass a value into the C++ code\n        # from Python.\n        cmake_args = [\n            f\"-DCMAKE_LIBRARY_OUTPUT_DIRECTORY={extdir}{os.sep}\",\n            f\"-DPYTHON_EXECUTABLE={sys.executable}\",\n            f\"-DCMAKE_BUILD_TYPE={cfg}\",  # not used on MSVC, but no harm\n            f\"-DCHATGLM_ENABLE_PYBIND=ON\",\n            f\"-DCHATGLM_ENABLE_EXAMPLES=OFF\",\n        ]\n        build_args = []\n        # Adding CMake arguments set as environment variable\n        # (needed e.g. to build for ARM OSx on conda-forge)\n        if \"CMAKE_ARGS\" in os.environ:\n            cmake_args += [item for item in os.environ[\"CMAKE_ARGS\"].split(\" \") if item]\n\n        if self.compiler.compiler_type != \"msvc\":\n            # Using Ninja-build since it a) is available as a wheel and b)\n            # multithreads automatically. MSVC would require all variables be\n            # exported for Ninja to pick it up, which is a little tricky to do.\n            # Users can override the generator with CMAKE_GENERATOR in CMake\n            # 3.15+.\n            if not cmake_generator or cmake_generator == \"Ninja\":\n                try:\n                    import ninja\n\n                    ninja_executable_path = Path(ninja.BIN_DIR) / \"ninja\"\n                    cmake_args += [\n                        \"-GNinja\",\n                        f\"-DCMAKE_MAKE_PROGRAM:FILEPATH={ninja_executable_path}\",\n                    ]\n                except ImportError:\n                    pass\n\n        else:\n            # Single config generators are handled \"normally\"\n            single_config = any(x in cmake_generator for x in {\"NMake\", \"Ninja\"})\n\n            # CMake allows an arch-in-generator style for backward compatibility\n            contains_arch = any(x in cmake_generator for x in {\"ARM\", \"Win64\"})\n\n            # Specify the arch if using MSVC generator, but only if it doesn't\n            # contain a backward-compatibility arch spec already in the\n            # generator name.\n            if not single_config and not contains_arch:\n                cmake_args += [\"-A\", PLAT_TO_CMAKE[self.plat_name]]\n\n            # Multi-config generators have a different way to specify configs\n            if not single_config:\n                cmake_args += [f\"-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_{cfg.upper()}={extdir}\"]\n                build_args += [\"--config\", cfg]\n\n        if sys.platform.startswith(\"darwin\"):\n            # Cross-compile support for macOS - respect ARCHFLAGS if set\n            archs = re.findall(r\"-arch (\\S+)\", os.environ.get(\"ARCHFLAGS\", \"\"))\n            if archs:\n                cmake_args += [\"-DCMAKE_OSX_ARCHITECTURES={}\".format(\";\".join(archs))]\n\n        # Set CMAKE_BUILD_PARALLEL_LEVEL to control the parallel build level\n        # across all generators.\n        # if \"CMAKE_BUILD_PARALLEL_LEVEL\" not in os.environ:\n        #     # self.parallel is a Python 3 only way to set parallel jobs by hand\n        #     # using -j in the build_ext call, not supported by pip or PyPA-build.\n        #     if hasattr(self, \"parallel\") and self.parallel:\n        #         # CMake 3.12+ only.\n        #         build_args += [f\"-j{self.parallel}\"]\n\n        # Compile in parallel by default\n        build_args += [f\"-j\"]\n\n        build_temp = Path(self.build_temp) / ext.name\n        if not build_temp.exists():\n            build_temp.mkdir(parents=True)\n\n        subprocess.run([\"cmake\", ext.sourcedir, *cmake_args], cwd=build_temp, check=True)\n        subprocess.run([\"cmake\", \"--build\", \".\", *build_args], cwd=build_temp, check=True)\n\n\nHERE = Path(__file__).resolve().parent\nversion = re.search(r'__version__ = \"(.*?)\"', (HERE / \"chatglm_cpp/__init__.py\").read_text(encoding=\"utf-8\")).group(1)\n\nsetup(\n    version=version,\n    packages=find_packages(),\n    ext_modules=[CMakeExtension(\"chatglm_cpp._C\")],\n    cmdclass={\"build_ext\": CMakeBuild},\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}