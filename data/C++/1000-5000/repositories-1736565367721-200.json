{
  "metadata": {
    "timestamp": 1736565367721,
    "page": 200,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "leejet/stable-diffusion.cpp",
      "stars": 3681,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 0.2783203125,
          "content": "BasedOnStyle: Chromium\nUseTab: Never\nIndentWidth: 4\nTabWidth: 4\nAllowShortIfStatementsOnASingleLine: false\nColumnLimit: 0\nAccessModifierOffset: -4\nNamespaceIndentation: All\nFixNamespaceComments: false\nAlignAfterOpenBracket: true\nAlignConsecutiveAssignments: true\nIndentCaseLabels: true"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.03515625,
          "content": "build*/\ntest/\n\n.cache/\n*.swp\nmodels/"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.09375,
          "content": "build*/\ntest/\n.vscode/\n.cache/\n*.swp\n.vscode/\n*.bat\n*.bin\n*.exe\n*.gguf\noutput*.png\nmodels*\n*.log"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.078125,
          "content": "[submodule \"ggml\"]\n    path = ggml\n\turl = https://github.com/ggerganov/ggml.git\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 3.8271484375,
          "content": "cmake_minimum_required(VERSION 3.12)\nproject(\"stable-diffusion\")\n\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n\nif (NOT XCODE AND NOT MSVC AND NOT CMAKE_BUILD_TYPE)\n    set(CMAKE_BUILD_TYPE Release CACHE STRING \"Build type\" FORCE)\n    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"MinSizeRel\" \"RelWithDebInfo\")\nendif()\n\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\n\nif(CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR)\n    set(SD_STANDALONE ON)\nelse()\n    set(SD_STANDALONE OFF)\nendif()\n\n#\n# Option list\n#\n\n# general\n#option(SD_BUILD_TESTS                \"sd: build tests\"    ${SD_STANDALONE})\noption(SD_BUILD_EXAMPLES             \"sd: build examples\" ${SD_STANDALONE})\noption(SD_CUDA                       \"sd: cuda backend\" OFF)\noption(SD_HIPBLAS                    \"sd: rocm backend\" OFF)\noption(SD_METAL                      \"sd: metal backend\" OFF)\noption(SD_VULKAN                     \"sd: vulkan backend\" OFF)\noption(SD_SYCL                       \"sd: sycl backend\" OFF)\noption(SD_MUSA                       \"sd: musa backend\" OFF)\noption(SD_FAST_SOFTMAX               \"sd: x1.5 faster softmax, indeterministic (sometimes, same seed don't generate same image), cuda only\" OFF)\noption(SD_BUILD_SHARED_LIBS          \"sd: build shared libs\" OFF)\n#option(SD_BUILD_SERVER               \"sd: build server example\"                           ON)\n\nif(SD_CUDA)\n    message(\"-- Use CUDA as backend stable-diffusion\")\n    set(GGML_CUDA ON)\n    add_definitions(-DSD_USE_CUDA)\nendif()\n\nif(SD_METAL)\n    message(\"-- Use Metal as backend stable-diffusion\")\n    set(GGML_METAL ON)\n    add_definitions(-DSD_USE_METAL)\nendif()\n\nif (SD_VULKAN)\n    message(\"-- Use Vulkan as backend stable-diffusion\")\n    set(GGML_VULKAN ON)\n    add_definitions(-DSD_USE_VULKAN)\nendif ()\n\nif (SD_HIPBLAS)\n    message(\"-- Use HIPBLAS as backend stable-diffusion\")\n    set(GGML_HIPBLAS ON)\n    add_definitions(-DSD_USE_CUDA)\n    if(SD_FAST_SOFTMAX)\n        set(GGML_CUDA_FAST_SOFTMAX ON)\n    endif()\nendif ()\n\nif(SD_MUSA)\n    message(\"-- Use MUSA as backend stable-diffusion\")\n    set(GGML_MUSA ON)\n    add_definitions(-DSD_USE_CUBLAS)\n    if(SD_FAST_SOFTMAX)\n        set(GGML_CUDA_FAST_SOFTMAX ON)\n    endif()\nendif()\n\nset(SD_LIB stable-diffusion)\n\nfile(GLOB SD_LIB_SOURCES\n    \"*.h\"\n    \"*.cpp\"\n    \"*.hpp\"\n)\n\n# we can get only one share lib\nif(SD_BUILD_SHARED_LIBS)\n    message(\"-- Build shared library\")\n    message(${SD_LIB_SOURCES})\n    set(BUILD_SHARED_LIBS OFF)\n    add_library(${SD_LIB} SHARED ${SD_LIB_SOURCES})\n    add_definitions(-DSD_BUILD_SHARED_LIB)\n    target_compile_definitions(${SD_LIB} PRIVATE -DSD_BUILD_DLL)\n    set(CMAKE_POSITION_INDEPENDENT_CODE ON)\nelse()\n    message(\"-- Build static library\")\n    set(BUILD_SHARED_LIBS OFF)\n    add_library(${SD_LIB} STATIC ${SD_LIB_SOURCES})\nendif()\n\nif(SD_SYCL)\n    message(\"-- Use SYCL as backend stable-diffusion\")\n    set(GGML_SYCL ON)\n    add_definitions(-DSD_USE_SYCL)\n    # disable fast-math on host, see:\n    # https://www.intel.com/content/www/us/en/docs/cpp-compiler/developer-guide-reference/2021-10/fp-model-fp.html\n    if (WIN32)\n        set(SYCL_COMPILE_OPTIONS /fp:precise)\n    else()\n        set(SYCL_COMPILE_OPTIONS -fp-model=precise)\n    endif()\n    message(\"-- Turn off fast-math for host in SYCL backend\")\n    target_compile_options(${SD_LIB} PRIVATE ${SYCL_COMPILE_OPTIONS})\nendif()\n\nset(CMAKE_POLICY_DEFAULT_CMP0077 NEW)\n\n# see https://github.com/ggerganov/ggml/pull/682\nadd_definitions(-DGGML_MAX_NAME=128)\n\n# deps\n# Only add ggml if it hasn't been added yet\nif (NOT TARGET ggml)\n    add_subdirectory(ggml)\nendif()\n\nadd_subdirectory(thirdparty)\n\ntarget_link_libraries(${SD_LIB} PUBLIC ggml zip)\ntarget_include_directories(${SD_LIB} PUBLIC . thirdparty)\ntarget_compile_features(${SD_LIB} PUBLIC cxx_std_11)\n\n\nif (SD_BUILD_EXAMPLES)\n    add_subdirectory(examples)\nendif()\n\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.33203125,
          "content": "ARG UBUNTU_VERSION=22.04\n\nFROM ubuntu:$UBUNTU_VERSION as build\n\nRUN apt-get update && apt-get install -y build-essential git cmake\n\nWORKDIR /sd.cpp\n\nCOPY . .\n\nRUN mkdir build && cd build && cmake .. && cmake --build . --config Release\n\nFROM ubuntu:$UBUNTU_VERSION as runtime\n\nCOPY --from=build /sd.cpp/build/bin/sd /sd\n\nENTRYPOINT [ \"/sd\" ]"
        },
        {
          "name": "Dockerfile.musa",
          "type": "blob",
          "size": 0.466796875,
          "content": "ARG MUSA_VERSION=rc3.1.0\n\nFROM mthreads/musa:${MUSA_VERSION}-devel-ubuntu22.04 as build\n\nRUN apt-get update && apt-get install -y cmake\n\nWORKDIR /sd.cpp\n\nCOPY . .\n\nRUN mkdir build && cd build && \\\n    cmake .. -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DSD_MUSA=ON -DCMAKE_BUILD_TYPE=Release && \\\n    cmake --build . --config Release\n\nFROM mthreads/musa:${MUSA_VERSION}-runtime-ubuntu22.04 as runtime\n\nCOPY --from=build /sd.cpp/build/bin/sd /sd\n\nENTRYPOINT [ \"/sd\" ]"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.037109375,
          "content": "MIT License\n\nCopyright (c) 2023 leejet\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.908203125,
          "content": "<p align=\"center\">\n  <img src=\"./assets/cat_with_sd_cpp_42.png\" width=\"360x\">\n</p>\n\n# stable-diffusion.cpp\n\nInference of Stable Diffusion and Flux in pure C/C++\n\n## Features\n\n- Plain C/C++ implementation based on [ggml](https://github.com/ggerganov/ggml), working in the same way as [llama.cpp](https://github.com/ggerganov/llama.cpp)\n- Super lightweight and without external dependencies\n- SD1.x, SD2.x, SDXL and [SD3/SD3.5](./docs/sd3.md) support\n    - !!!The VAE in SDXL encounters NaN issues under FP16, but unfortunately, the ggml_conv_2d only operates under FP16. Hence, a parameter is needed to specify the VAE that has fixed the FP16 NaN issue. You can find it here: [SDXL VAE FP16 Fix](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/blob/main/sdxl_vae.safetensors).\n- [Flux-dev/Flux-schnell Support](./docs/flux.md)\n\n- [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo) and [SDXL-Turbo](https://huggingface.co/stabilityai/sdxl-turbo) support\n- [PhotoMaker](https://github.com/TencentARC/PhotoMaker) support.\n- 16-bit, 32-bit float support\n- 2-bit, 3-bit, 4-bit, 5-bit and 8-bit integer quantization support\n- Accelerated memory-efficient CPU inference\n    - Only requires ~2.3GB when using txt2img with fp16 precision to generate a 512x512 image, enabling Flash Attention just requires ~1.8GB.\n- AVX, AVX2 and AVX512 support for x86 architectures\n- Full CUDA, Metal, Vulkan and SYCL backend for GPU acceleration.\n- Can load ckpt, safetensors and diffusers models/checkpoints. Standalone VAEs models\n    - No need to convert to `.ggml` or `.gguf` anymore!\n- Flash Attention for memory usage optimization\n- Original `txt2img` and `img2img` mode\n- Negative prompt\n- [stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui) style tokenizer (not all the features, only token weighting for now)\n- LoRA support, same as [stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#lora)\n- Latent Consistency Models support (LCM/LCM-LoRA)\n- Faster and memory efficient latent decoding with [TAESD](https://github.com/madebyollin/taesd)\n- Upscale images generated with [ESRGAN](https://github.com/xinntao/Real-ESRGAN)\n- VAE tiling processing for reduce memory usage\n- Control Net support with SD 1.5\n- Sampling method\n    - `Euler A`\n    - `Euler`\n    - `Heun`\n    - `DPM2`\n    - `DPM++ 2M`\n    - [`DPM++ 2M v2`](https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/8457)\n    - `DPM++ 2S a`\n    - [`LCM`](https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/13952)\n- Cross-platform reproducibility (`--rng cuda`, consistent with the `stable-diffusion-webui GPU RNG`)\n- Embedds generation parameters into png output as webui-compatible text string\n- Supported platforms\n    - Linux\n    - Mac OS\n    - Windows\n    - Android (via Termux)\n\n### TODO\n\n- [ ] More sampling methods\n- [ ] Make inference faster\n    - The current implementation of ggml_conv_2d is slow and has high memory usage\n- [ ] Continuing to reduce memory usage (quantizing the weights of ggml_conv_2d)\n- [ ] Implement Inpainting support\n\n## Usage\n\nFor most users, you can download the built executable program from the latest [release](https://github.com/leejet/stable-diffusion.cpp/releases/latest).\nIf the built product does not meet your requirements, you can choose to build it manually.\n\n### Get the Code\n\n```\ngit clone --recursive https://github.com/leejet/stable-diffusion.cpp\ncd stable-diffusion.cpp\n```\n\n- If you have already cloned the repository, you can use the following command to update the repository to the latest code.\n\n```\ncd stable-diffusion.cpp\ngit pull origin master\ngit submodule init\ngit submodule update\n```\n\n### Download weights\n\n- download original weights(.ckpt or .safetensors). For example\n    - Stable Diffusion v1.4 from https://huggingface.co/CompVis/stable-diffusion-v-1-4-original\n    - Stable Diffusion v1.5 from https://huggingface.co/runwayml/stable-diffusion-v1-5\n    - Stable Diffuison v2.1 from https://huggingface.co/stabilityai/stable-diffusion-2-1\n    - Stable Diffusion 3 2B from https://huggingface.co/stabilityai/stable-diffusion-3-medium\n\n    ```shell\n    curl -L -O https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt\n    # curl -L -O https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors\n    # curl -L -O https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-nonema-pruned.safetensors\n    # curl -L -O https://huggingface.co/stabilityai/stable-diffusion-3-medium/resolve/main/sd3_medium_incl_clips_t5xxlfp16.safetensors\n    ```\n\n### Build\n\n#### Build from scratch\n\n```shell\nmkdir build\ncd build\ncmake ..\ncmake --build . --config Release\n```\n\n##### Using OpenBLAS\n\n```\ncmake .. -DGGML_OPENBLAS=ON\ncmake --build . --config Release\n```\n\n##### Using CUDA\n\nThis provides BLAS acceleration using the CUDA cores of your Nvidia GPU. Make sure to have the CUDA toolkit installed. You can download it from your Linux distro's package manager (e.g. `apt install nvidia-cuda-toolkit`) or from here: [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads). Recommended to have at least 4 GB of VRAM.\n\n```\ncmake .. -DSD_CUDA=ON\ncmake --build . --config Release\n```\n\n##### Using HipBLAS\nThis provides BLAS acceleration using the ROCm cores of your AMD GPU. Make sure to have the ROCm toolkit installed.\n\nWindows User Refer to [docs/hipBLAS_on_Windows.md](docs%2FhipBLAS_on_Windows.md) for a comprehensive guide.\n\n```\ncmake .. -G \"Ninja\" -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DSD_HIPBLAS=ON -DCMAKE_BUILD_TYPE=Release -DAMDGPU_TARGETS=gfx1100\ncmake --build . --config Release\n```\n\n##### Using MUSA\n\nThis provides BLAS acceleration using the MUSA cores of your Moore Threads GPU. Make sure to have the MUSA toolkit installed.\n\n```bash\ncmake .. -DCMAKE_C_COMPILER=/usr/local/musa/bin/clang -DCMAKE_CXX_COMPILER=/usr/local/musa/bin/clang++ -DSD_MUSA=ON -DCMAKE_BUILD_TYPE=Release\ncmake --build . --config Release\n```\n\n##### Using Metal\n\nUsing Metal makes the computation run on the GPU. Currently, there are some issues with Metal when performing operations on very large matrices, making it highly inefficient at the moment. Performance improvements are expected in the near future.\n\n```\ncmake .. -DSD_METAL=ON\ncmake --build . --config Release\n```\n\n##### Using Vulkan\n\nInstall Vulkan SDK from https://www.lunarg.com/vulkan-sdk/.\n\n```\ncmake .. -DSD_VULKAN=ON\ncmake --build . --config Release\n```\n\n##### Using SYCL\n\nUsing SYCL makes the computation run on the Intel GPU. Please make sure you have installed the related driver and [Intel® oneAPI Base toolkit](https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit.html) before start. More details and steps can refer to [llama.cpp SYCL backend](https://github.com/ggerganov/llama.cpp/blob/master/docs/backend/SYCL.md#linux).\n\n```\n# Export relevant ENV variables\nsource /opt/intel/oneapi/setvars.sh\n\n# Option 1: Use FP32 (recommended for better performance in most cases)\ncmake .. -DSD_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx\n\n# Option 2: Use FP16\ncmake .. -DSD_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL_F16=ON\n\ncmake --build . --config Release\n```\n\nExample of text2img by using SYCL backend:\n\n- download `stable-diffusion` model weight, refer to [download-weight](#download-weights).\n\n- run `./bin/sd -m ../models/sd3_medium_incl_clips_t5xxlfp16.safetensors --cfg-scale 5 --steps 30 --sampling-method euler  -H 1024 -W 1024 --seed 42 -p \"fantasy medieval village world inside a glass sphere , high detail, fantasy, realistic, light effect, hyper detail, volumetric lighting, cinematic, macro, depth of field, blur, red light and clouds from the back, highly detailed epic cinematic concept art cg render made in maya, blender and photoshop, octane render, excellent composition, dynamic dramatic cinematic lighting, aesthetic, very inspirational, world inside a glass sphere by james gurney by artgerm with james jean, joe fenton and tristan eaton by ross tran, fine details, 4k resolution\"`\n\n<p align=\"center\">\n  <img src=\"./assets/sycl_sd3_output.png\" width=\"360x\">\n</p>\n\n\n\n##### Using Flash Attention\n\nEnabling flash attention for the diffusion model reduces memory usage by varying amounts of MB.\neg.:\n - flux 768x768 ~600mb\n - SD2 768x768 ~1400mb\n\nFor most backends, it slows things down, but for cuda it generally speeds it up too.\nAt the moment, it is only supported for some models and some backends (like cpu, cuda/rocm, metal).\n\nRun by adding `--diffusion-fa` to the arguments and watch for:\n```\n[INFO ] stable-diffusion.cpp:312  - Using flash attention in the diffusion model\n```\nand the compute buffer shrink in the debug log:\n```\n[DEBUG] ggml_extend.hpp:1004 - flux compute buffer size: 650.00 MB(VRAM)\n```\n\n### Run\n\n```\nusage: ./bin/sd [arguments]\n\narguments:\n  -h, --help                         show this help message and exit\n  -M, --mode [MODEL]                 run mode (txt2img or img2img or convert, default: txt2img)\n  -t, --threads N                    number of threads to use during computation (default: -1)\n                                     If threads <= 0, then threads will be set to the number of CPU physical cores\n  -m, --model [MODEL]                path to full model\n  --diffusion-model                  path to the standalone diffusion model\n  --clip_l                           path to the clip-l text encoder\n  --clip_g                           path to the clip-l text encoder\n  --t5xxl                            path to the the t5xxl text encoder\n  --vae [VAE]                        path to vae\n  --taesd [TAESD_PATH]               path to taesd. Using Tiny AutoEncoder for fast decoding (low quality)\n  --control-net [CONTROL_PATH]       path to control net model\n  --embd-dir [EMBEDDING_PATH]        path to embeddings\n  --stacked-id-embd-dir [DIR]        path to PHOTOMAKER stacked id embeddings\n  --input-id-images-dir [DIR]        path to PHOTOMAKER input id images dir\n  --normalize-input                  normalize PHOTOMAKER input id images\n  --upscale-model [ESRGAN_PATH]      path to esrgan model. Upscale images after generate, just RealESRGAN_x4plus_anime_6B supported by now\n  --upscale-repeats                  Run the ESRGAN upscaler this many times (default 1)\n  --type [TYPE]                      weight type (f32, f16, q4_0, q4_1, q5_0, q5_1, q8_0, q2_k, q3_k, q4_k)\n                                     If not specified, the default is the type of the weight file\n  --lora-model-dir [DIR]             lora model directory\n  -i, --init-img [IMAGE]             path to the input image, required by img2img\n  --control-image [IMAGE]            path to image condition, control net\n  -o, --output OUTPUT                path to write result image to (default: ./output.png)\n  -p, --prompt [PROMPT]              the prompt to render\n  -n, --negative-prompt PROMPT       the negative prompt (default: \"\")\n  --cfg-scale SCALE                  unconditional guidance scale: (default: 7.0)\n  --skip-layers LAYERS               Layers to skip for SLG steps: (default: [7,8,9])\n  --skip-layer-start START           SLG enabling point: (default: 0.01)\n  --skip-layer-end END               SLG disabling point: (default: 0.2)\n\t\t\t\t\t\t\t\t\t SLG will be enabled at step int([STEPS]*[START]) and disabled at int([STEPS]*[END])\n  --strength STRENGTH                strength for noising/unnoising (default: 0.75)\n  --style-ratio STYLE-RATIO          strength for keeping input identity (default: 20%)\n  --control-strength STRENGTH        strength to apply Control Net (default: 0.9)\n                                     1.0 corresponds to full destruction of information in init image\n  -H, --height H                     image height, in pixel space (default: 512)\n  -W, --width W                      image width, in pixel space (default: 512)\n  --sampling-method {euler, euler_a, heun, dpm2, dpm++2s_a, dpm++2m, dpm++2mv2, ipndm, ipndm_v, lcm}\n                                     sampling method (default: \"euler_a\")\n  --steps  STEPS                     number of sample steps (default: 20)\n  --rng {std_default, cuda}          RNG (default: cuda)\n  -s SEED, --seed SEED               RNG seed (default: 42, use random seed for < 0)\n  -b, --batch-count COUNT            number of images to generate\n  --schedule {discrete, karras, exponential, ays, gits} Denoiser sigma schedule (default: discrete)\n  --clip-skip N                      ignore last layers of CLIP network; 1 ignores none, 2 ignores one layer (default: -1)\n                                     <= 0 represents unspecified, will be 1 for SD1.x, 2 for SD2.x\n  --vae-tiling                       process vae in tiles to reduce memory usage\n  --vae-on-cpu                       keep vae in cpu (for low vram)\n  --clip-on-cpu                      keep clip in cpu (for low vram)\n  --diffusion-fa                     use flash attention in the diffusion model (for low vram)\n                                     Might lower quality, since it implies converting k and v to f16.\n                                     This might crash if it is not supported by the backend.\n  --control-net-cpu                  keep controlnet in cpu (for low vram)\n  --canny                            apply canny preprocessor (edge detection)\n  --color                            Colors the logging tags according to level\n  -v, --verbose                      print extra info\n```\n\n#### txt2img example\n\n```sh\n./bin/sd -m ../models/sd-v1-4.ckpt -p \"a lovely cat\"\n# ./bin/sd -m ../models/v1-5-pruned-emaonly.safetensors -p \"a lovely cat\"\n# ./bin/sd -m ../models/sd_xl_base_1.0.safetensors --vae ../models/sdxl_vae-fp16-fix.safetensors -H 1024 -W 1024 -p \"a lovely cat\" -v\n# ./bin/sd -m ../models/sd3_medium_incl_clips_t5xxlfp16.safetensors -H 1024 -W 1024 -p 'a lovely cat holding a sign says \\\"Stable Diffusion CPP\\\"' --cfg-scale 4.5 --sampling-method euler -v\n# ./bin/sd --diffusion-model  ../models/flux1-dev-q3_k.gguf --vae ../models/ae.sft --clip_l ../models/clip_l.safetensors --t5xxl ../models/t5xxl_fp16.safetensors  -p \"a lovely cat holding a sign says 'flux.cpp'\" --cfg-scale 1.0 --sampling-method euler -v\n# ./bin/sd -m  ..\\models\\sd3.5_large.safetensors --clip_l ..\\models\\clip_l.safetensors --clip_g ..\\models\\clip_g.safetensors --t5xxl ..\\models\\t5xxl_fp16.safetensors  -H 1024 -W 1024 -p 'a lovely cat holding a sign says \\\"Stable diffusion 3.5 Large\\\"' --cfg-scale 4.5 --sampling-method euler -v\n```\n\nUsing formats of different precisions will yield results of varying quality.\n\n| f32  | f16  |q8_0  |q5_0  |q5_1  |q4_0  |q4_1  |\n| ----  |----  |----  |----  |----  |----  |----  |\n| ![](./assets/f32.png) |![](./assets/f16.png) |![](./assets/q8_0.png) |![](./assets/q5_0.png) |![](./assets/q5_1.png) |![](./assets/q4_0.png) |![](./assets/q4_1.png) |\n\n#### img2img example\n\n- `./output.png` is the image generated from the above txt2img pipeline\n\n\n```\n./bin/sd --mode img2img -m ../models/sd-v1-4.ckpt -p \"cat with blue eyes\" -i ./output.png -o ./img2img_output.png --strength 0.4\n```\n\n<p align=\"center\">\n  <img src=\"./assets/img2img_output.png\" width=\"256x\">\n</p>\n\n## More Guides\n\n- [LoRA](./docs/lora.md)\n- [LCM/LCM-LoRA](./docs/lcm.md)\n- [Using PhotoMaker to personalize image generation](./docs/photo_maker.md)\n- [Using ESRGAN to upscale results](./docs/esrgan.md)\n- [Using TAESD to faster decoding](./docs/taesd.md)\n- [Docker](./docs/docker.md)\n- [Quantization and GGUF](./docs/quantization_and_gguf.md)\n\n## Bindings\n\nThese projects wrap `stable-diffusion.cpp` for easier use in other languages/frameworks.\n\n* Golang: [seasonjs/stable-diffusion](https://github.com/seasonjs/stable-diffusion)\n* C#: [DarthAffe/StableDiffusion.NET](https://github.com/DarthAffe/StableDiffusion.NET)\n* Python: [william-murray1204/stable-diffusion-cpp-python](https://github.com/william-murray1204/stable-diffusion-cpp-python)\n* Rust: [newfla/diffusion-rs](https://github.com/newfla/diffusion-rs)\n\n## UIs\n\nThese projects use `stable-diffusion.cpp` as a backend for their image generation.\n\n- [Jellybox](https://jellybox.com)\n- [Stable Diffusion GUI](https://github.com/fszontagh/sd.cpp.gui.wx)\n\n## Contributors\n\nThank you to all the people who have already contributed to stable-diffusion.cpp!\n\n[![Contributors](https://contrib.rocks/image?repo=leejet/stable-diffusion.cpp)](https://github.com/leejet/stable-diffusion.cpp/graphs/contributors)\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=leejet/stable-diffusion.cpp&type=Date)](https://star-history.com/#leejet/stable-diffusion.cpp&Date)\n\n## References\n\n- [ggml](https://github.com/ggerganov/ggml)\n- [stable-diffusion](https://github.com/CompVis/stable-diffusion)\n- [sd3-ref](https://github.com/Stability-AI/sd3-ref)\n- [stable-diffusion-stability-ai](https://github.com/Stability-AI/stablediffusion)\n- [stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)\n- [ComfyUI](https://github.com/comfyanonymous/ComfyUI)\n- [k-diffusion](https://github.com/crowsonkb/k-diffusion)\n- [latent-consistency-model](https://github.com/luosiallen/latent-consistency-model)\n- [generative-models](https://github.com/Stability-AI/generative-models/)\n- [PhotoMaker](https://github.com/TencentARC/PhotoMaker)\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "clip.hpp",
          "type": "blob",
          "size": 38.80078125,
          "content": "#ifndef __CLIP_HPP__\n#define __CLIP_HPP__\n\n#include \"ggml_extend.hpp\"\n#include \"model.h\"\n\n/*================================================== CLIPTokenizer ===================================================*/\n\nstd::pair<std::unordered_map<std::string, float>, std::string> extract_and_remove_lora(std::string text) {\n    std::regex re(\"<lora:([^:]+):([^>]+)>\");\n    std::smatch matches;\n    std::unordered_map<std::string, float> filename2multiplier;\n\n    while (std::regex_search(text, matches, re)) {\n        std::string filename = matches[1].str();\n        float multiplier     = std::stof(matches[2].str());\n\n        text = std::regex_replace(text, re, \"\", std::regex_constants::format_first_only);\n\n        if (multiplier == 0.f) {\n            continue;\n        }\n\n        if (filename2multiplier.find(filename) == filename2multiplier.end()) {\n            filename2multiplier[filename] = multiplier;\n        } else {\n            filename2multiplier[filename] += multiplier;\n        }\n    }\n\n    return std::make_pair(filename2multiplier, text);\n}\n\nstd::vector<std::pair<int, std::u32string>> bytes_to_unicode() {\n    std::vector<std::pair<int, std::u32string>> byte_unicode_pairs;\n    std::set<int> byte_set;\n    for (int b = static_cast<int>('!'); b <= static_cast<int>('~'); ++b) {\n        byte_set.insert(b);\n        byte_unicode_pairs.push_back(std::pair<int, std::u32string>(b, unicode_value_to_utf32(b)));\n    }\n    for (int b = 161; b <= 172; ++b) {\n        byte_set.insert(b);\n        byte_unicode_pairs.push_back(std::pair<int, std::u32string>(b, unicode_value_to_utf32(b)));\n    }\n    for (int b = 174; b <= 255; ++b) {\n        byte_set.insert(b);\n        byte_unicode_pairs.push_back(std::pair<int, std::u32string>(b, unicode_value_to_utf32(b)));\n    }\n    int n = 0;\n    for (int b = 0; b < 256; ++b) {\n        if (byte_set.find(b) == byte_set.end()) {\n            byte_unicode_pairs.push_back(std::pair<int, std::u32string>(b, unicode_value_to_utf32(n + 256)));\n            ++n;\n        }\n    }\n    // LOG_DEBUG(\"byte_unicode_pairs %d\", byte_unicode_pairs.size());\n    return byte_unicode_pairs;\n}\n\n// Ref: https://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py\n\ntypedef std::function<bool(std::string&, std::vector<int32_t>&)> on_new_token_cb_t;\n\nclass CLIPTokenizer {\nprivate:\n    std::map<int, std::u32string> byte_encoder;\n    std::map<std::u32string, int> byte_decoder;\n    std::map<std::u32string, int> encoder;\n    std::map<int, std::u32string> decoder;\n    std::map<std::pair<std::u32string, std::u32string>, int> bpe_ranks;\n    std::regex pat;\n    int encoder_len;\n    int bpe_len;\n\npublic:\n    const std::string UNK_TOKEN = \"<|endoftext|>\";\n    const std::string BOS_TOKEN = \"<|startoftext|>\";\n    const std::string EOS_TOKEN = \"<|endoftext|>\";\n    const std::string PAD_TOKEN = \"<|endoftext|>\";\n\n    const int UNK_TOKEN_ID = 49407;\n    const int BOS_TOKEN_ID = 49406;\n    const int EOS_TOKEN_ID = 49407;\n    const int PAD_TOKEN_ID = 49407;\n\nprivate:\n    static std::string strip(const std::string& str) {\n        std::string::size_type start = str.find_first_not_of(\" \\t\\n\\r\\v\\f\");\n        std::string::size_type end   = str.find_last_not_of(\" \\t\\n\\r\\v\\f\");\n\n        if (start == std::string::npos) {\n            // String contains only whitespace characters\n            return \"\";\n        }\n\n        return str.substr(start, end - start + 1);\n    }\n\n    static std::string whitespace_clean(std::string text) {\n        text = std::regex_replace(text, std::regex(R\"(\\s+)\"), \" \");\n        text = strip(text);\n        return text;\n    }\n\n    static std::set<std::pair<std::u32string, std::u32string>> get_pairs(const std::vector<std::u32string>& subwords) {\n        std::set<std::pair<std::u32string, std::u32string>> pairs;\n        if (subwords.size() == 0) {\n            return pairs;\n        }\n        std::u32string prev_subword = subwords[0];\n        for (int i = 1; i < subwords.size(); i++) {\n            std::u32string subword = subwords[i];\n            std::pair<std::u32string, std::u32string> pair(prev_subword, subword);\n            pairs.insert(pair);\n            prev_subword = subword;\n        }\n        return pairs;\n    }\n\npublic:\n    CLIPTokenizer(int pad_token_id = 49407, const std::string& merges_utf8_str = \"\")\n        : PAD_TOKEN_ID(pad_token_id) {\n        if (merges_utf8_str.size() > 0) {\n            load_from_merges(merges_utf8_str);\n        } else {\n            load_from_merges(ModelLoader::load_merges());\n        }\n    }\n\n    void load_from_merges(const std::string& merges_utf8_str) {\n        auto byte_unicode_pairs = bytes_to_unicode();\n        // printf(\"byte_unicode_pairs have %lu pairs \\n\", byte_unicode_pairs.size());\n        byte_encoder = std::map<int, std::u32string>(byte_unicode_pairs.begin(), byte_unicode_pairs.end());\n        for (auto& pair : byte_unicode_pairs) {\n            byte_decoder[pair.second] = pair.first;\n        }\n        // for (auto & pair: byte_unicode_pairs) {\n        //     std::cout << pair.first << \": \" << pair.second << std::endl;\n        // }\n        std::vector<std::u32string> merges;\n        size_t start = 0;\n        size_t pos;\n        std::u32string merges_utf32_str = utf8_to_utf32(merges_utf8_str);\n        while ((pos = merges_utf32_str.find('\\n', start)) != std::string::npos) {\n            merges.push_back(merges_utf32_str.substr(start, pos - start));\n            start = pos + 1;\n        }\n        // LOG_DEBUG(\"merges size %llu\", merges.size());\n        GGML_ASSERT(merges.size() == 48895);\n        merges = std::vector<std::u32string>(merges.begin() + 1, merges.end());\n        std::vector<std::pair<std::u32string, std::u32string>> merge_pairs;\n        for (const auto& merge : merges) {\n            size_t space_pos = merge.find(' ');\n            merge_pairs.emplace_back(merge.substr(0, space_pos), merge.substr(space_pos + 1));\n            // LOG_DEBUG(\"%s\", utf32_to_utf8(merge.substr(space_pos + 1)).c_str());\n            // printf(\"%s :: %s | %s \\n\", utf32_to_utf8(merge).c_str(), utf32_to_utf8(merge.substr(0, space_pos)).c_str(),\n            //                     utf32_to_utf8(merge.substr(space_pos + 1)).c_str());\n        }\n        std::vector<std::u32string> vocab;\n        for (const auto& pair : byte_unicode_pairs) {\n            vocab.push_back(pair.second);\n        }\n        for (const auto& pair : byte_unicode_pairs) {\n            vocab.push_back(pair.second + utf8_to_utf32(\"</w>\"));\n        }\n        for (const auto& merge : merge_pairs) {\n            vocab.push_back(merge.first + merge.second);\n        }\n        vocab.push_back(utf8_to_utf32(\"<|startoftext|>\"));\n        vocab.push_back(utf8_to_utf32(\"<|endoftext|>\"));\n        LOG_DEBUG(\"vocab size: %llu\", vocab.size());\n        int i = 0;\n        for (const auto& token : vocab) {\n            encoder[token] = i;\n            decoder[i]     = token;\n            i++;\n        }\n        encoder_len = i;\n\n        auto it = encoder.find(utf8_to_utf32(\"img</w>\"));\n        if (it != encoder.end()) {\n            LOG_DEBUG(\" trigger word img already in vocab\");\n        } else {\n            LOG_DEBUG(\" trigger word img not in vocab yet\");\n        }\n\n        int rank = 0;\n        for (const auto& merge : merge_pairs) {\n            bpe_ranks[merge] = rank++;\n        }\n        bpe_len = rank;\n    };\n\n    void add_token(const std::string& text) {\n        std::u32string token = utf8_to_utf32(text);\n        auto it              = encoder.find(token);\n        if (it != encoder.end()) {\n            encoder[token]       = encoder_len;\n            decoder[encoder_len] = token;\n            encoder_len++;\n        }\n    }\n\n    std::u32string bpe(const std::u32string& token) {\n        std::vector<std::u32string> word;\n\n        for (int i = 0; i < token.size() - 1; i++) {\n            word.emplace_back(1, token[i]);\n        }\n        word.push_back(token.substr(token.size() - 1) + utf8_to_utf32(\"</w>\"));\n\n        std::set<std::pair<std::u32string, std::u32string>> pairs = get_pairs(word);\n\n        if (pairs.empty()) {\n            return token + utf8_to_utf32(\"</w>\");\n        }\n\n        while (true) {\n            auto min_pair_iter = std::min_element(pairs.begin(),\n                                                  pairs.end(),\n                                                  [&](const std::pair<std::u32string, std::u32string>& a,\n                                                      const std::pair<std::u32string, std::u32string>& b) {\n                                                      if (bpe_ranks.find(a) == bpe_ranks.end()) {\n                                                          return false;\n                                                      } else if (bpe_ranks.find(b) == bpe_ranks.end()) {\n                                                          return true;\n                                                      }\n                                                      return bpe_ranks.at(a) < bpe_ranks.at(b);\n                                                  });\n\n            const std::pair<std::u32string, std::u32string>& bigram = *min_pair_iter;\n\n            if (bpe_ranks.find(bigram) == bpe_ranks.end()) {\n                break;\n            }\n\n            std::u32string first  = bigram.first;\n            std::u32string second = bigram.second;\n            std::vector<std::u32string> new_word;\n            int32_t i = 0;\n\n            while (i < word.size()) {\n                auto it = std::find(word.begin() + i, word.end(), first);\n                if (it == word.end()) {\n                    new_word.insert(new_word.end(), word.begin() + i, word.end());\n                    break;\n                }\n                new_word.insert(new_word.end(), word.begin() + i, it);\n                i = static_cast<int32_t>(std::distance(word.begin(), it));\n\n                if (word[i] == first && i < static_cast<int32_t>(word.size()) - 1 && word[i + 1] == second) {\n                    new_word.push_back(first + second);\n                    i += 2;\n                } else {\n                    new_word.push_back(word[i]);\n                    i += 1;\n                }\n            }\n\n            word = new_word;\n\n            if (word.size() == 1) {\n                break;\n            }\n            pairs = get_pairs(word);\n        }\n\n        std::u32string result;\n        for (int i = 0; i < word.size(); i++) {\n            result += word[i];\n            if (i != word.size() - 1) {\n                result += utf8_to_utf32(\" \");\n            }\n        }\n\n        return result;\n    }\n\n    std::vector<int> tokenize(std::string text,\n                              on_new_token_cb_t on_new_token_cb,\n                              size_t max_length = 0,\n                              bool padding      = false) {\n        std::vector<int32_t> tokens = encode(text, on_new_token_cb);\n\n        tokens.insert(tokens.begin(), BOS_TOKEN_ID);\n        if (max_length > 0) {\n            if (tokens.size() > max_length - 1) {\n                tokens.resize(max_length - 1);\n                tokens.push_back(EOS_TOKEN_ID);\n            } else {\n                tokens.push_back(EOS_TOKEN_ID);\n                if (padding) {\n                    tokens.insert(tokens.end(), max_length - tokens.size(), PAD_TOKEN_ID);\n                }\n            }\n        }\n\n        return tokens;\n    }\n\n    void pad_tokens(std::vector<int>& tokens,\n                    std::vector<float>& weights,\n                    size_t max_length = 0,\n                    bool padding      = false) {\n        if (max_length > 0 && padding) {\n            size_t n = std::ceil(tokens.size() * 1.0 / (max_length - 2));\n            if (n == 0) {\n                n = 1;\n            }\n            size_t length = max_length * n;\n            LOG_DEBUG(\"token length: %llu\", length);\n            std::vector<int> new_tokens;\n            std::vector<float> new_weights;\n            new_tokens.push_back(BOS_TOKEN_ID);\n            new_weights.push_back(1.0);\n            int token_idx = 0;\n            for (int i = 1; i < length; i++) {\n                if (token_idx >= tokens.size()) {\n                    break;\n                }\n                if (i % max_length == 0) {\n                    new_tokens.push_back(BOS_TOKEN_ID);\n                    new_weights.push_back(1.0);\n                } else if (i % max_length == max_length - 1) {\n                    new_tokens.push_back(EOS_TOKEN_ID);\n                    new_weights.push_back(1.0);\n                } else {\n                    new_tokens.push_back(tokens[token_idx]);\n                    new_weights.push_back(weights[token_idx]);\n                    token_idx++;\n                }\n            }\n\n            new_tokens.push_back(EOS_TOKEN_ID);\n            new_weights.push_back(1.0);\n            tokens  = new_tokens;\n            weights = new_weights;\n\n            if (padding) {\n                tokens.insert(tokens.end(), length - tokens.size(), PAD_TOKEN_ID);\n                weights.insert(weights.end(), length - weights.size(), 1.0);\n            }\n        }\n    }\n\n    std::string clean_up_tokenization(std::string& text) {\n        std::regex pattern(R\"( ,)\");\n        // Replace \" ,\" with \",\"\n        std::string result = std::regex_replace(text, pattern, \",\");\n        return result;\n    }\n\n    std::string decode(const std::vector<int>& tokens) {\n        std::string text = \"\";\n        for (int t : tokens) {\n            if (t == 49406 || t == 49407)\n                continue;\n            std::u32string ts = decoder[t];\n            // printf(\"%d, %s \\n\", t,  utf32_to_utf8(ts).c_str());\n            std::string s = utf32_to_utf8(ts);\n            if (s.length() >= 4) {\n                if (ends_with(s, \"</w>\")) {\n                    text += s.replace(s.length() - 4, s.length() - 1, \"\") + \" \";\n                } else {\n                    text += s;\n                }\n            } else {\n                text += \" \" + s;\n            }\n        }\n        // std::vector<unsigned char> bytes;\n        // for (auto c : text){\n        //     bytes.push_back(byte_decoder[c]);\n        // }\n\n        // std::string s((char *)bytes.data());\n        // std::string s = \"\";\n        text = clean_up_tokenization(text);\n        return trim(text);\n    }\n\n    std::vector<int> encode(std::string text, on_new_token_cb_t on_new_token_cb) {\n        std::string original_text = text;\n        std::vector<int32_t> bpe_tokens;\n        text = whitespace_clean(text);\n        std::transform(text.begin(), text.end(), text.begin(), [](unsigned char c) { return std::tolower(c); });\n\n        std::regex pat(R\"(<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[[:alpha:]]+|[[:digit:]]|[^[:space:][:alpha:][:digit:]]+)\",\n                       std::regex::icase);\n\n        std::smatch matches;\n        std::string str = text;\n        std::vector<std::string> token_strs;\n        while (std::regex_search(str, matches, pat)) {\n            bool skip = on_new_token_cb(str, bpe_tokens);\n            if (skip) {\n                continue;\n            }\n            for (auto& token : matches) {\n                std::string token_str = token.str();\n                std::u32string utf32_token;\n                for (int i = 0; i < token_str.length(); i++) {\n                    unsigned char b = token_str[i];\n                    utf32_token += byte_encoder[b];\n                }\n                auto bpe_strs = bpe(utf32_token);\n                size_t start  = 0;\n                size_t pos;\n                while ((pos = bpe_strs.find(' ', start)) != std::u32string::npos) {\n                    auto bpe_str = bpe_strs.substr(start, pos - start);\n                    bpe_tokens.push_back(encoder[bpe_str]);\n                    token_strs.push_back(utf32_to_utf8(bpe_str));\n\n                    start = pos + 1;\n                }\n                auto bpe_str = bpe_strs.substr(start, bpe_strs.size() - start);\n                bpe_tokens.push_back(encoder[bpe_str]);\n                token_strs.push_back(utf32_to_utf8(bpe_str));\n            }\n            str = matches.suffix();\n        }\n        std::stringstream ss;\n        ss << \"[\";\n        for (auto token : token_strs) {\n            ss << \"\\\"\" << token << \"\\\", \";\n        }\n        ss << \"]\";\n        // LOG_DEBUG(\"split prompt \\\"%s\\\" to tokens %s\", original_text.c_str(), ss.str().c_str());\n        // printf(\"split prompt \\\"%s\\\" to tokens %s \\n\", original_text.c_str(), ss.str().c_str());\n        return bpe_tokens;\n    }\n};\n\n/*================================================ FrozenCLIPEmbedder ================================================*/\n\n// Ref: https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py\n\nstruct CLIPMLP : public GGMLBlock {\nprotected:\n    bool use_gelu;\n\npublic:\n    CLIPMLP(int64_t d_model, int64_t intermediate_size) {\n        blocks[\"fc1\"] = std::shared_ptr<GGMLBlock>(new Linear(d_model, intermediate_size));\n        blocks[\"fc2\"] = std::shared_ptr<GGMLBlock>(new Linear(intermediate_size, d_model));\n\n        if (d_model == 1024 || d_model == 1280) {  // SD 2.x\n            use_gelu = true;\n        } else {  // SD 1.x\n            use_gelu = false;\n        }\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [N, n_token, d_model]\n        auto fc1 = std::dynamic_pointer_cast<Linear>(blocks[\"fc1\"]);\n        auto fc2 = std::dynamic_pointer_cast<Linear>(blocks[\"fc2\"]);\n\n        x = fc1->forward(ctx, x);\n        if (use_gelu) {\n            x = ggml_gelu_inplace(ctx, x);\n        } else {\n            x = ggml_gelu_quick_inplace(ctx, x);\n        }\n        x = fc2->forward(ctx, x);\n        return x;\n    }\n};\n\nstruct CLIPLayer : public GGMLBlock {\nprotected:\n    int64_t d_model;  // hidden_size/embed_dim\n    int64_t n_head;\n    int64_t intermediate_size;\n\npublic:\n    CLIPLayer(int64_t d_model,\n              int64_t n_head,\n              int64_t intermediate_size)\n        : d_model(d_model),\n          n_head(n_head),\n          intermediate_size(intermediate_size) {\n        blocks[\"self_attn\"] = std::shared_ptr<GGMLBlock>(new MultiheadAttention(d_model, n_head, true, true));\n\n        blocks[\"layer_norm1\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(d_model));\n        blocks[\"layer_norm2\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(d_model));\n\n        blocks[\"mlp\"] = std::shared_ptr<GGMLBlock>(new CLIPMLP(d_model, intermediate_size));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x, bool mask = true) {\n        // x: [N, n_token, d_model]\n        auto self_attn   = std::dynamic_pointer_cast<MultiheadAttention>(blocks[\"self_attn\"]);\n        auto layer_norm1 = std::dynamic_pointer_cast<LayerNorm>(blocks[\"layer_norm1\"]);\n        auto layer_norm2 = std::dynamic_pointer_cast<LayerNorm>(blocks[\"layer_norm2\"]);\n        auto mlp         = std::dynamic_pointer_cast<CLIPMLP>(blocks[\"mlp\"]);\n\n        x = ggml_add(ctx, x, self_attn->forward(ctx, layer_norm1->forward(ctx, x), mask));\n        x = ggml_add(ctx, x, mlp->forward(ctx, layer_norm2->forward(ctx, x)));\n        return x;\n    }\n};\n\nstruct CLIPEncoder : public GGMLBlock {\nprotected:\n    int64_t n_layer;\n\npublic:\n    CLIPEncoder(int64_t n_layer,\n                int64_t d_model,\n                int64_t n_head,\n                int64_t intermediate_size)\n        : n_layer(n_layer) {\n        for (int i = 0; i < n_layer; i++) {\n            std::string name = \"layers.\" + std::to_string(i);\n            blocks[name]     = std::shared_ptr<GGMLBlock>(new CLIPLayer(d_model, n_head, intermediate_size));\n        }\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x, int clip_skip = -1, bool mask = true) {\n        // x: [N, n_token, d_model]\n        int layer_idx = n_layer - 1;\n        // LOG_DEBUG(\"clip_skip %d\", clip_skip);\n        if (clip_skip > 0) {\n            layer_idx = n_layer - clip_skip;\n        }\n\n        for (int i = 0; i < n_layer; i++) {\n            // LOG_DEBUG(\"layer %d\", i);\n            if (i == layer_idx + 1) {\n                break;\n            }\n            std::string name = \"layers.\" + std::to_string(i);\n            auto layer       = std::dynamic_pointer_cast<CLIPLayer>(blocks[name]);\n            x                = layer->forward(ctx, x, mask);  // [N, n_token, d_model]\n            // LOG_DEBUG(\"layer %d\", i);\n        }\n        return x;\n    }\n};\n\nclass CLIPEmbeddings : public GGMLBlock {\nprotected:\n    int64_t embed_dim;\n    int64_t vocab_size;\n    int64_t num_positions;\n\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix = \"\") {\n        enum ggml_type token_wtype    = (tensor_types.find(prefix + \"token_embedding.weight\") != tensor_types.end()) ? tensor_types[prefix + \"token_embedding.weight\"] : GGML_TYPE_F32;\n        enum ggml_type position_wtype = GGML_TYPE_F32;  //(tensor_types.find(prefix + \"position_embedding.weight\") != tensor_types.end()) ? tensor_types[prefix + \"position_embedding.weight\"] : GGML_TYPE_F32;\n\n        params[\"token_embedding.weight\"]    = ggml_new_tensor_2d(ctx, token_wtype, embed_dim, vocab_size);\n        params[\"position_embedding.weight\"] = ggml_new_tensor_2d(ctx, position_wtype, embed_dim, num_positions);\n    }\n\npublic:\n    CLIPEmbeddings(int64_t embed_dim,\n                   int64_t vocab_size    = 49408,\n                   int64_t num_positions = 77)\n        : embed_dim(embed_dim),\n          vocab_size(vocab_size),\n          num_positions(num_positions) {\n    }\n\n    struct ggml_tensor* get_token_embed_weight() {\n        return params[\"token_embedding.weight\"];\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* input_ids,\n                                struct ggml_tensor* custom_embed_weight) {\n        // input_ids: [N, n_token]\n        auto token_embed_weight    = params[\"token_embedding.weight\"];\n        auto position_embed_weight = params[\"position_embedding.weight\"];\n\n        GGML_ASSERT(input_ids->ne[0] == position_embed_weight->ne[1]);\n        input_ids            = ggml_reshape_3d(ctx, input_ids, input_ids->ne[0], 1, input_ids->ne[1]);\n        auto token_embedding = ggml_get_rows(ctx, custom_embed_weight != NULL ? custom_embed_weight : token_embed_weight, input_ids);\n        token_embedding      = ggml_reshape_3d(ctx, token_embedding, token_embedding->ne[0], token_embedding->ne[1], token_embedding->ne[3]);\n\n        // token_embedding + position_embedding\n        auto x = ggml_add(ctx,\n                          token_embedding,\n                          position_embed_weight);  // [N, n_token, embed_dim]\n        return x;\n    }\n};\n\nclass CLIPVisionEmbeddings : public GGMLBlock {\nprotected:\n    int64_t embed_dim;\n    int64_t num_channels;\n    int64_t patch_size;\n    int64_t image_size;\n    int64_t num_patches;\n    int64_t num_positions;\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix = \"\") {\n        enum ggml_type patch_wtype    = GGML_TYPE_F16;  // tensor_types.find(prefix + \"patch_embedding.weight\") != tensor_types.end() ? tensor_types[prefix + \"patch_embedding.weight\"] : GGML_TYPE_F16;\n        enum ggml_type class_wtype    = GGML_TYPE_F32;  // tensor_types.find(prefix + \"class_embedding\") != tensor_types.end() ? tensor_types[prefix + \"class_embedding\"] : GGML_TYPE_F32;\n        enum ggml_type position_wtype = GGML_TYPE_F32;  // tensor_types.find(prefix + \"position_embedding.weight\") != tensor_types.end() ? tensor_types[prefix + \"position_embedding.weight\"] : GGML_TYPE_F32;\n\n        params[\"patch_embedding.weight\"]    = ggml_new_tensor_4d(ctx, patch_wtype, patch_size, patch_size, num_channels, embed_dim);\n        params[\"class_embedding\"]           = ggml_new_tensor_1d(ctx, class_wtype, embed_dim);\n        params[\"position_embedding.weight\"] = ggml_new_tensor_2d(ctx, position_wtype, embed_dim, num_positions);\n    }\n\npublic:\n    CLIPVisionEmbeddings(int64_t embed_dim,\n                         int64_t num_channels = 3,\n                         int64_t patch_size   = 14,\n                         int64_t image_size   = 224)\n        : embed_dim(embed_dim),\n          num_channels(num_channels),\n          patch_size(patch_size),\n          image_size(image_size) {\n        num_patches   = (image_size / patch_size) * (image_size / patch_size);\n        num_positions = num_patches + 1;\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* pixel_values) {\n        // pixel_values: [N, num_channels, image_size, image_size]\n        // return: [N, num_positions, embed_dim]\n        GGML_ASSERT(pixel_values->ne[0] == image_size && pixel_values->ne[1] == image_size && pixel_values->ne[2] == num_channels);\n\n        auto patch_embed_weight    = params[\"patch_embedding.weight\"];\n        auto class_embed_weight    = params[\"class_embedding\"];\n        auto position_embed_weight = params[\"position_embedding.weight\"];\n\n        // concat(patch_embedding, class_embedding) + position_embedding\n        struct ggml_tensor* patch_embedding;\n        int64_t N       = pixel_values->ne[3];\n        patch_embedding = ggml_nn_conv_2d(ctx, pixel_values, patch_embed_weight, NULL, patch_size, patch_size);  // [N, embed_dim, image_size // pacht_size, image_size // pacht_size]\n        patch_embedding = ggml_reshape_3d(ctx, patch_embedding, num_patches, embed_dim, N);                      // [N, embed_dim, num_patches]\n        patch_embedding = ggml_cont(ctx, ggml_permute(ctx, patch_embedding, 1, 0, 2, 3));                        // [N, num_patches, embed_dim]\n        patch_embedding = ggml_reshape_4d(ctx, patch_embedding, 1, embed_dim, num_patches, N);                   // [N, num_patches, embed_dim, 1]\n\n        struct ggml_tensor* class_embedding = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, embed_dim, N);\n        class_embedding                     = ggml_repeat(ctx, class_embed_weight, class_embedding);      // [N, embed_dim]\n        class_embedding                     = ggml_reshape_4d(ctx, class_embedding, 1, embed_dim, 1, N);  // [N, 1, embed_dim, 1]\n\n        struct ggml_tensor* x = ggml_concat(ctx, class_embedding, patch_embedding, 2);  // [N, num_positions, embed_dim, 1]\n        x                     = ggml_reshape_3d(ctx, x, embed_dim, num_positions, N);   // [N, num_positions, embed_dim]\n        x                     = ggml_add(ctx, x, position_embed_weight);\n        return x;  // [N, num_positions, embed_dim]\n    }\n};\n\n// OPENAI_CLIP_VIT_L_14: https://huggingface.co/openai/clip-vit-large-patch14/blob/main/config.json\n// OPEN_CLIP_VIT_H_14: https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K/blob/main/config.json\n// OPEN_CLIP_VIT_BIGG_14: https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k/blob/main/config.json (CLIPTextModelWithProjection)\n\nenum CLIPVersion {\n    OPENAI_CLIP_VIT_L_14,   // SD 1.x and SDXL\n    OPEN_CLIP_VIT_H_14,     // SD 2.x\n    OPEN_CLIP_VIT_BIGG_14,  // SDXL\n};\n\nclass CLIPTextModel : public GGMLBlock {\nprotected:\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix = \"\") {\n        if (version == OPEN_CLIP_VIT_BIGG_14) {\n            enum ggml_type wtype      = GGML_TYPE_F32;  // tensor_types.find(prefix + \"text_projection\") != tensor_types.end() ? tensor_types[prefix + \"text_projection\"] : GGML_TYPE_F32;\n            params[\"text_projection\"] = ggml_new_tensor_2d(ctx, wtype, projection_dim, hidden_size);\n        }\n    }\n\npublic:\n    CLIPVersion version = OPENAI_CLIP_VIT_L_14;\n    // network hparams\n    int32_t vocab_size        = 49408;\n    int32_t n_token           = 77;  // max_position_embeddings\n    int32_t hidden_size       = 768;\n    int32_t intermediate_size = 3072;\n    int32_t n_head            = 12;\n    int32_t n_layer           = 12;    // num_hidden_layers\n    int32_t projection_dim    = 1280;  // only for OPEN_CLIP_VIT_BIGG_14\n    int32_t clip_skip         = -1;\n    bool with_final_ln        = true;\n\n    CLIPTextModel(CLIPVersion version = OPENAI_CLIP_VIT_L_14,\n                  int clip_skip_value = -1,\n                  bool with_final_ln  = true)\n        : version(version), with_final_ln(with_final_ln) {\n        if (version == OPEN_CLIP_VIT_H_14) {\n            hidden_size       = 1024;\n            intermediate_size = 4096;\n            n_head            = 16;\n            n_layer           = 24;\n        } else if (version == OPEN_CLIP_VIT_BIGG_14) {  // CLIPTextModelWithProjection\n            hidden_size       = 1280;\n            intermediate_size = 5120;\n            n_head            = 20;\n            n_layer           = 32;\n        }\n        set_clip_skip(clip_skip_value);\n\n        blocks[\"embeddings\"]       = std::shared_ptr<GGMLBlock>(new CLIPEmbeddings(hidden_size, vocab_size, n_token));\n        blocks[\"encoder\"]          = std::shared_ptr<GGMLBlock>(new CLIPEncoder(n_layer, hidden_size, n_head, intermediate_size));\n        blocks[\"final_layer_norm\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(hidden_size));\n    }\n\n    void set_clip_skip(int skip) {\n        if (skip <= 0) {\n            return;\n        }\n        clip_skip = skip;\n    }\n\n    struct ggml_tensor* get_token_embed_weight() {\n        auto embeddings = std::dynamic_pointer_cast<CLIPEmbeddings>(blocks[\"embeddings\"]);\n        return embeddings->get_token_embed_weight();\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* input_ids,\n                                struct ggml_tensor* tkn_embeddings,\n                                size_t max_token_idx = 0,\n                                bool return_pooled   = false) {\n        // input_ids: [N, n_token]\n        auto embeddings       = std::dynamic_pointer_cast<CLIPEmbeddings>(blocks[\"embeddings\"]);\n        auto encoder          = std::dynamic_pointer_cast<CLIPEncoder>(blocks[\"encoder\"]);\n        auto final_layer_norm = std::dynamic_pointer_cast<LayerNorm>(blocks[\"final_layer_norm\"]);\n\n        auto x = embeddings->forward(ctx, input_ids, tkn_embeddings);  // [N, n_token, hidden_size]\n        x      = encoder->forward(ctx, x, return_pooled ? -1 : clip_skip, true);\n        if (return_pooled || with_final_ln) {\n            x = final_layer_norm->forward(ctx, x);\n        }\n\n        if (return_pooled) {\n            auto text_projection = params[\"text_projection\"];\n            ggml_tensor* pooled  = ggml_view_1d(ctx, x, hidden_size, x->nb[1] * max_token_idx);\n            if (text_projection != NULL) {\n                pooled = ggml_nn_linear(ctx, pooled, text_projection, NULL);\n            } else {\n                LOG_DEBUG(\"Missing text_projection matrix, assuming identity...\");\n            }\n            return pooled;  // [hidden_size, 1, 1]\n        }\n\n        return x;  // [N, n_token, hidden_size]\n    }\n};\n\nclass CLIPVisionModel : public GGMLBlock {\npublic:\n    // network hparams\n    int32_t num_channels      = 3;\n    int32_t patch_size        = 14;\n    int32_t image_size        = 224;\n    int32_t num_positions     = 257;  // (image_size / patch_size)^2 + 1\n    int32_t hidden_size       = 1024;\n    int32_t intermediate_size = 4096;\n    int32_t n_head            = 16;\n    int32_t n_layer           = 24;\n\npublic:\n    CLIPVisionModel(CLIPVersion version = OPENAI_CLIP_VIT_L_14) {\n        if (version == OPEN_CLIP_VIT_H_14) {\n            hidden_size       = 1280;\n            intermediate_size = 5120;\n            n_head            = 16;\n            n_layer           = 32;\n        } else if (version == OPEN_CLIP_VIT_BIGG_14) {\n            hidden_size       = 1664;\n            intermediate_size = 8192;\n            n_head            = 16;\n            n_layer           = 48;\n        }\n\n        blocks[\"embeddings\"]     = std::shared_ptr<GGMLBlock>(new CLIPVisionEmbeddings(hidden_size, num_channels, patch_size, image_size));\n        blocks[\"pre_layernorm\"]  = std::shared_ptr<GGMLBlock>(new LayerNorm(hidden_size));\n        blocks[\"encoder\"]        = std::shared_ptr<GGMLBlock>(new CLIPEncoder(n_layer, hidden_size, n_head, intermediate_size));\n        blocks[\"post_layernorm\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(hidden_size));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* pixel_values, bool return_pooled = true) {\n        // pixel_values: [N, num_channels, image_size, image_size]\n        auto embeddings     = std::dynamic_pointer_cast<CLIPVisionEmbeddings>(blocks[\"embeddings\"]);\n        auto pre_layernorm  = std::dynamic_pointer_cast<LayerNorm>(blocks[\"pre_layernorm\"]);\n        auto encoder        = std::dynamic_pointer_cast<CLIPEncoder>(blocks[\"encoder\"]);\n        auto post_layernorm = std::dynamic_pointer_cast<LayerNorm>(blocks[\"post_layernorm\"]);\n\n        auto x = embeddings->forward(ctx, pixel_values);  // [N, num_positions, embed_dim]\n        x      = pre_layernorm->forward(ctx, x);\n        x      = encoder->forward(ctx, x, -1, false);\n        // print_ggml_tensor(x, true, \"ClipVisionModel x: \");\n        auto last_hidden_state = x;\n        x                      = post_layernorm->forward(ctx, x);  // [N, n_token, hidden_size]\n\n        GGML_ASSERT(x->ne[3] == 1);\n        if (return_pooled) {\n            ggml_tensor* pooled = ggml_cont(ctx, ggml_view_2d(ctx, x, x->ne[0], x->ne[2], x->nb[2], 0));\n            return pooled;  // [N, hidden_size]\n        } else {\n            // return x;  // [N, n_token, hidden_size]\n            return last_hidden_state;  // [N, n_token, hidden_size]\n        }\n    }\n};\n\nclass CLIPProjection : public UnaryBlock {\nprotected:\n    int64_t in_features;\n    int64_t out_features;\n    bool transpose_weight;\n\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix = \"\") {\n        enum ggml_type wtype = tensor_types.find(prefix + \"weight\") != tensor_types.end() ? tensor_types[prefix + \"weight\"] : GGML_TYPE_F32;\n        if (transpose_weight) {\n            params[\"weight\"] = ggml_new_tensor_2d(ctx, wtype, out_features, in_features);\n        } else {\n            params[\"weight\"] = ggml_new_tensor_2d(ctx, wtype, in_features, out_features);\n        }\n    }\n\npublic:\n    CLIPProjection(int64_t in_features,\n                   int64_t out_features,\n                   bool transpose_weight = false)\n        : in_features(in_features),\n          out_features(out_features),\n          transpose_weight(transpose_weight) {}\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        struct ggml_tensor* w = params[\"weight\"];\n        if (transpose_weight) {\n            w = ggml_cont(ctx, ggml_transpose(ctx, w));\n        }\n        return ggml_nn_linear(ctx, x, w, NULL);\n    }\n};\n\nclass CLIPVisionModelProjection : public GGMLBlock {\npublic:\n    int32_t hidden_size    = 1024;\n    int32_t projection_dim = 768;\n    int32_t image_size     = 224;\n\npublic:\n    CLIPVisionModelProjection(CLIPVersion version   = OPENAI_CLIP_VIT_L_14,\n                              bool transpose_proj_w = false) {\n        if (version == OPEN_CLIP_VIT_H_14) {\n            hidden_size    = 1280;\n            projection_dim = 1024;\n        } else if (version == OPEN_CLIP_VIT_BIGG_14) {\n            hidden_size = 1664;\n        }\n\n        blocks[\"vision_model\"]      = std::shared_ptr<GGMLBlock>(new CLIPVisionModel(version));\n        blocks[\"visual_projection\"] = std::shared_ptr<GGMLBlock>(new CLIPProjection(hidden_size, projection_dim, transpose_proj_w));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* pixel_values) {\n        // pixel_values: [N, num_channels, image_size, image_size]\n        // return: [N, projection_dim]\n        auto vision_model      = std::dynamic_pointer_cast<CLIPVisionModel>(blocks[\"vision_model\"]);\n        auto visual_projection = std::dynamic_pointer_cast<CLIPProjection>(blocks[\"visual_projection\"]);\n\n        auto x = vision_model->forward(ctx, pixel_values);  // [N, hidden_size]\n        x      = visual_projection->forward(ctx, x);        // [N, projection_dim]\n\n        return x;  // [N, projection_dim]\n    }\n};\n\nstruct CLIPTextModelRunner : public GGMLRunner {\n    CLIPTextModel model;\n\n    CLIPTextModelRunner(ggml_backend_t backend,\n                        std::map<std::string, enum ggml_type>& tensor_types,\n                        const std::string prefix,\n                        CLIPVersion version = OPENAI_CLIP_VIT_L_14,\n                        int clip_skip_value = 1,\n                        bool with_final_ln  = true)\n        : GGMLRunner(backend), model(version, clip_skip_value, with_final_ln) {\n        model.init(params_ctx, tensor_types, prefix);\n    }\n\n    std::string get_desc() {\n        return \"clip\";\n    }\n\n    void set_clip_skip(int clip_skip) {\n        model.set_clip_skip(clip_skip);\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors, const std::string prefix) {\n        model.get_param_tensors(tensors, prefix);\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* input_ids,\n                                struct ggml_tensor* embeddings,\n                                size_t max_token_idx = 0,\n                                bool return_pooled   = false) {\n        size_t N       = input_ids->ne[1];\n        size_t n_token = input_ids->ne[0];\n        if (input_ids->ne[0] > model.n_token) {\n            GGML_ASSERT(input_ids->ne[0] % model.n_token == 0);\n            input_ids = ggml_reshape_2d(ctx, input_ids, model.n_token, input_ids->ne[0] / model.n_token);\n        }\n\n        return model.forward(ctx, input_ids, embeddings, max_token_idx, return_pooled);\n    }\n\n    struct ggml_cgraph* build_graph(struct ggml_tensor* input_ids,\n                                    int num_custom_embeddings    = 0,\n                                    void* custom_embeddings_data = NULL,\n                                    size_t max_token_idx         = 0,\n                                    bool return_pooled           = false) {\n        struct ggml_cgraph* gf = ggml_new_graph(compute_ctx);\n\n        input_ids = to_backend(input_ids);\n\n        struct ggml_tensor* embeddings = NULL;\n\n        if (num_custom_embeddings > 0 && custom_embeddings_data != NULL) {\n            auto token_embed_weight = model.get_token_embed_weight();\n            auto custom_embeddings  = ggml_new_tensor_2d(compute_ctx,\n                                                         token_embed_weight->type,\n                                                         model.hidden_size,\n                                                         num_custom_embeddings);\n            set_backend_tensor_data(custom_embeddings, custom_embeddings_data);\n\n            // concatenate custom embeddings\n            embeddings = ggml_concat(compute_ctx, token_embed_weight, custom_embeddings, 1);\n        }\n\n        struct ggml_tensor* hidden_states = forward(compute_ctx, input_ids, embeddings, max_token_idx, return_pooled);\n\n        ggml_build_forward_expand(gf, hidden_states);\n\n        return gf;\n    }\n\n    void compute(const int n_threads,\n                 struct ggml_tensor* input_ids,\n                 int num_custom_embeddings,\n                 void* custom_embeddings_data,\n                 size_t max_token_idx,\n                 bool return_pooled,\n                 ggml_tensor** output,\n                 ggml_context* output_ctx = NULL) {\n        auto get_graph = [&]() -> struct ggml_cgraph* {\n            return build_graph(input_ids, num_custom_embeddings, custom_embeddings_data, max_token_idx, return_pooled);\n        };\n        GGMLRunner::compute(get_graph, n_threads, true, output, output_ctx);\n    }\n};\n\n#endif  // __CLIP_HPP__\n"
        },
        {
          "name": "common.hpp",
          "type": "blob",
          "size": 21.3857421875,
          "content": "#ifndef __COMMON_HPP__\n#define __COMMON_HPP__\n\n#include \"ggml_extend.hpp\"\n\nclass DownSampleBlock : public GGMLBlock {\nprotected:\n    int channels;\n    int out_channels;\n    bool vae_downsample;\n\npublic:\n    DownSampleBlock(int channels,\n                    int out_channels,\n                    bool vae_downsample = false)\n        : channels(channels),\n          out_channels(out_channels),\n          vae_downsample(vae_downsample) {\n        if (vae_downsample) {\n            blocks[\"conv\"] = std::shared_ptr<GGMLBlock>(new Conv2d(channels, out_channels, {3, 3}, {2, 2}, {0, 0}));\n        } else {\n            blocks[\"op\"] = std::shared_ptr<GGMLBlock>(new Conv2d(channels, out_channels, {3, 3}, {2, 2}, {1, 1}));\n        }\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [N, channels, h, w]\n        if (vae_downsample) {\n            auto conv = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv\"]);\n\n            x = ggml_pad(ctx, x, 1, 1, 0, 0);\n            x = conv->forward(ctx, x);\n        } else {\n            auto conv = std::dynamic_pointer_cast<Conv2d>(blocks[\"op\"]);\n\n            x = conv->forward(ctx, x);\n        }\n        return x;  // [N, out_channels, h/2, w/2]\n    }\n};\n\nclass UpSampleBlock : public GGMLBlock {\nprotected:\n    int channels;\n    int out_channels;\n\npublic:\n    UpSampleBlock(int channels,\n                  int out_channels)\n        : channels(channels),\n          out_channels(out_channels) {\n        blocks[\"conv\"] = std::shared_ptr<GGMLBlock>(new Conv2d(channels, out_channels, {3, 3}, {1, 1}, {1, 1}));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [N, channels, h, w]\n        auto conv = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv\"]);\n\n        x = ggml_upscale(ctx, x, 2);  // [N, channels, h*2, w*2]\n        x = conv->forward(ctx, x);    // [N, out_channels, h*2, w*2]\n        return x;\n    }\n};\n\nclass ResBlock : public GGMLBlock {\nprotected:\n    // network hparams\n    int64_t channels;      // model_channels * (1, 1, 1, 2, 2, 4, 4, 4)\n    int64_t emb_channels;  // time_embed_dim\n    int64_t out_channels;  // mult * model_channels\n    std::pair<int, int> kernel_size;\n    int dims;\n    bool skip_t_emb;\n    bool exchange_temb_dims;\n\n    std::shared_ptr<GGMLBlock> conv_nd(int dims,\n                                       int64_t in_channels,\n                                       int64_t out_channels,\n                                       std::pair<int, int> kernel_size,\n                                       std::pair<int, int> padding) {\n        GGML_ASSERT(dims == 2 || dims == 3);\n        if (dims == 3) {\n            return std::shared_ptr<GGMLBlock>(new Conv3dnx1x1(in_channels, out_channels, kernel_size.first, 1, padding.first));\n        } else {\n            return std::shared_ptr<GGMLBlock>(new Conv2d(in_channels, out_channels, kernel_size, {1, 1}, padding));\n        }\n    }\n\npublic:\n    ResBlock(int64_t channels,\n             int64_t emb_channels,\n             int64_t out_channels,\n             std::pair<int, int> kernel_size = {3, 3},\n             int dims                        = 2,\n             bool exchange_temb_dims         = false,\n             bool skip_t_emb                 = false)\n        : channels(channels),\n          emb_channels(emb_channels),\n          out_channels(out_channels),\n          kernel_size(kernel_size),\n          dims(dims),\n          skip_t_emb(skip_t_emb),\n          exchange_temb_dims(exchange_temb_dims) {\n        std::pair<int, int> padding = {kernel_size.first / 2, kernel_size.second / 2};\n        blocks[\"in_layers.0\"]       = std::shared_ptr<GGMLBlock>(new GroupNorm32(channels));\n        // in_layer_1 is nn.SILU()\n        blocks[\"in_layers.2\"] = conv_nd(dims, channels, out_channels, kernel_size, padding);\n\n        if (!skip_t_emb) {\n            // emb_layer_0 is nn.SILU()\n            blocks[\"emb_layers.1\"] = std::shared_ptr<GGMLBlock>(new Linear(emb_channels, out_channels));\n        }\n\n        blocks[\"out_layers.0\"] = std::shared_ptr<GGMLBlock>(new GroupNorm32(out_channels));\n        // out_layer_1 is nn.SILU()\n        // out_layer_2 is nn.Dropout(), skip for inference\n        blocks[\"out_layers.3\"] = conv_nd(dims, out_channels, out_channels, kernel_size, padding);\n\n        if (out_channels != channels) {\n            blocks[\"skip_connection\"] = conv_nd(dims, channels, out_channels, {1, 1}, {0, 0});\n        }\n    }\n\n    virtual struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x, struct ggml_tensor* emb = NULL) {\n        // For dims==3, we reduce dimension from 5d to 4d by merging h and w, in order not to change ggml\n        // [N, c, t, h, w] => [N, c, t, h * w]\n        // x: [N, channels, h, w] if dims == 2 else [N, channels, t, h, w]\n        // emb: [N, emb_channels] if dims == 2 else [N, t, emb_channels]\n        auto in_layers_0  = std::dynamic_pointer_cast<GroupNorm32>(blocks[\"in_layers.0\"]);\n        auto in_layers_2  = std::dynamic_pointer_cast<UnaryBlock>(blocks[\"in_layers.2\"]);\n        auto out_layers_0 = std::dynamic_pointer_cast<GroupNorm32>(blocks[\"out_layers.0\"]);\n        auto out_layers_3 = std::dynamic_pointer_cast<UnaryBlock>(blocks[\"out_layers.3\"]);\n\n        if (emb == NULL) {\n            GGML_ASSERT(skip_t_emb);\n        }\n\n        // in_layers\n        auto h = in_layers_0->forward(ctx, x);\n        h      = ggml_silu_inplace(ctx, h);\n        h      = in_layers_2->forward(ctx, h);  // [N, out_channels, h, w] if dims == 2 else [N, out_channels, t, h, w]\n\n        // emb_layers\n        if (!skip_t_emb) {\n            auto emb_layer_1 = std::dynamic_pointer_cast<Linear>(blocks[\"emb_layers.1\"]);\n\n            auto emb_out = ggml_silu(ctx, emb);\n            emb_out      = emb_layer_1->forward(ctx, emb_out);  // [N, out_channels] if dims == 2 else [N, t, out_channels]\n\n            if (dims == 2) {\n                emb_out = ggml_reshape_4d(ctx, emb_out, 1, 1, emb_out->ne[0], emb_out->ne[1]);  // [N, out_channels, 1, 1]\n            } else {\n                emb_out = ggml_reshape_4d(ctx, emb_out, 1, emb_out->ne[0], emb_out->ne[1], emb_out->ne[2]);  // [N, t, out_channels, 1]\n                if (exchange_temb_dims) {\n                    // emb_out = rearrange(emb_out, \"b t c ... -> b c t ...\")\n                    emb_out = ggml_cont(ctx, ggml_permute(ctx, emb_out, 0, 2, 1, 3));  // [N, out_channels, t, 1]\n                }\n            }\n\n            h = ggml_add(ctx, h, emb_out);  // [N, out_channels, h, w] if dims == 2 else [N, out_channels, t, h, w]\n        }\n\n        // out_layers\n        h = out_layers_0->forward(ctx, h);\n        h = ggml_silu_inplace(ctx, h);\n        // dropout, skip for inference\n        h = out_layers_3->forward(ctx, h);\n\n        // skip connection\n        if (out_channels != channels) {\n            auto skip_connection = std::dynamic_pointer_cast<UnaryBlock>(blocks[\"skip_connection\"]);\n            x                    = skip_connection->forward(ctx, x);  // [N, out_channels, h, w] if dims == 2 else [N, out_channels, t, h, w]\n        }\n\n        h = ggml_add(ctx, h, x);\n        return h;  // [N, out_channels, h, w] if dims == 2 else [N, out_channels, t, h, w]\n    }\n};\n\nclass GEGLU : public GGMLBlock {\nprotected:\n    int64_t dim_in;\n    int64_t dim_out;\n\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, std::string prefix = \"\") {\n        enum ggml_type wtype      = (tensor_types.find(prefix + \"proj.weight\") != tensor_types.end()) ? tensor_types[prefix + \"proj.weight\"] : GGML_TYPE_F32;\n        enum ggml_type bias_wtype = GGML_TYPE_F32;  //(tensor_types.find(prefix + \"proj.bias\") != tensor_types.end()) ? tensor_types[prefix + \"proj.bias\"] : GGML_TYPE_F32;\n        params[\"proj.weight\"]     = ggml_new_tensor_2d(ctx, wtype, dim_in, dim_out * 2);\n        params[\"proj.bias\"]       = ggml_new_tensor_1d(ctx, bias_wtype, dim_out * 2);\n    }\n\npublic:\n    GEGLU(int64_t dim_in, int64_t dim_out)\n        : dim_in(dim_in), dim_out(dim_out) {}\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [ne3, ne2, ne1, dim_in]\n        // return: [ne3, ne2, ne1, dim_out]\n        struct ggml_tensor* w = params[\"proj.weight\"];\n        struct ggml_tensor* b = params[\"proj.bias\"];\n\n        auto x_w    = ggml_view_2d(ctx, w, w->ne[0], w->ne[1] / 2, w->nb[1], 0);                        // [dim_out, dim_in]\n        auto x_b    = ggml_view_1d(ctx, b, b->ne[0] / 2, 0);                                            // [dim_out, dim_in]\n        auto gate_w = ggml_view_2d(ctx, w, w->ne[0], w->ne[1] / 2, w->nb[1], w->nb[1] * w->ne[1] / 2);  // [dim_out, ]\n        auto gate_b = ggml_view_1d(ctx, b, b->ne[0] / 2, b->nb[0] * b->ne[0] / 2);                      // [dim_out, ]\n\n        auto x_in = x;\n        x         = ggml_nn_linear(ctx, x_in, x_w, x_b);        // [ne3, ne2, ne1, dim_out]\n        auto gate = ggml_nn_linear(ctx, x_in, gate_w, gate_b);  // [ne3, ne2, ne1, dim_out]\n\n        gate = ggml_gelu_inplace(ctx, gate);\n\n        x = ggml_mul(ctx, x, gate);  // [ne3, ne2, ne1, dim_out]\n\n        return x;\n    }\n};\n\nclass FeedForward : public GGMLBlock {\npublic:\n    FeedForward(int64_t dim,\n                int64_t dim_out,\n                int64_t mult = 4) {\n        int64_t inner_dim = dim * mult;\n\n        blocks[\"net.0\"] = std::shared_ptr<GGMLBlock>(new GEGLU(dim, inner_dim));\n        // net_1 is nn.Dropout(), skip for inference\n        blocks[\"net.2\"] = std::shared_ptr<GGMLBlock>(new Linear(inner_dim, dim_out));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [ne3, ne2, ne1, dim]\n        // return: [ne3, ne2, ne1, dim_out]\n\n        auto net_0 = std::dynamic_pointer_cast<GEGLU>(blocks[\"net.0\"]);\n        auto net_2 = std::dynamic_pointer_cast<Linear>(blocks[\"net.2\"]);\n\n        x = net_0->forward(ctx, x);  // [ne3, ne2, ne1, inner_dim]\n        x = net_2->forward(ctx, x);  // [ne3, ne2, ne1, dim_out]\n        return x;\n    }\n};\n\nclass CrossAttention : public GGMLBlock {\nprotected:\n    int64_t query_dim;\n    int64_t context_dim;\n    int64_t n_head;\n    int64_t d_head;\n    bool flash_attn;\n\npublic:\n    CrossAttention(int64_t query_dim,\n                   int64_t context_dim,\n                   int64_t n_head,\n                   int64_t d_head,\n                   bool flash_attn = false)\n        : n_head(n_head),\n          d_head(d_head),\n          query_dim(query_dim),\n          context_dim(context_dim),\n          flash_attn(flash_attn) {\n        int64_t inner_dim = d_head * n_head;\n\n        blocks[\"to_q\"] = std::shared_ptr<GGMLBlock>(new Linear(query_dim, inner_dim, false));\n        blocks[\"to_k\"] = std::shared_ptr<GGMLBlock>(new Linear(context_dim, inner_dim, false));\n        blocks[\"to_v\"] = std::shared_ptr<GGMLBlock>(new Linear(context_dim, inner_dim, false));\n\n        blocks[\"to_out.0\"] = std::shared_ptr<GGMLBlock>(new Linear(inner_dim, query_dim));\n        // to_out_1 is nn.Dropout(), skip for inference\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x, struct ggml_tensor* context) {\n        // x: [N, n_token, query_dim]\n        // context: [N, n_context, context_dim]\n        // return: [N, n_token, query_dim]\n        auto to_q     = std::dynamic_pointer_cast<Linear>(blocks[\"to_q\"]);\n        auto to_k     = std::dynamic_pointer_cast<Linear>(blocks[\"to_k\"]);\n        auto to_v     = std::dynamic_pointer_cast<Linear>(blocks[\"to_v\"]);\n        auto to_out_0 = std::dynamic_pointer_cast<Linear>(blocks[\"to_out.0\"]);\n\n        int64_t n         = x->ne[2];\n        int64_t n_token   = x->ne[1];\n        int64_t n_context = context->ne[1];\n        int64_t inner_dim = d_head * n_head;\n\n        auto q = to_q->forward(ctx, x);        // [N, n_token, inner_dim]\n        auto k = to_k->forward(ctx, context);  // [N, n_context, inner_dim]\n        auto v = to_v->forward(ctx, context);  // [N, n_context, inner_dim]\n\n        x = ggml_nn_attention_ext(ctx, q, k, v, n_head, NULL, false, false, flash_attn);  // [N, n_token, inner_dim]\n\n        x = to_out_0->forward(ctx, x);  // [N, n_token, query_dim]\n        return x;\n    }\n};\n\nclass BasicTransformerBlock : public GGMLBlock {\nprotected:\n    int64_t n_head;\n    int64_t d_head;\n    bool ff_in;\n\npublic:\n    BasicTransformerBlock(int64_t dim,\n                          int64_t n_head,\n                          int64_t d_head,\n                          int64_t context_dim,\n                          bool ff_in      = false,\n                          bool flash_attn = false)\n        : n_head(n_head), d_head(d_head), ff_in(ff_in) {\n        // disable_self_attn is always False\n        // disable_temporal_crossattention is always False\n        // switch_temporal_ca_to_sa is always False\n        // inner_dim is always None or equal to dim\n        // gated_ff is always True\n        blocks[\"attn1\"] = std::shared_ptr<GGMLBlock>(new CrossAttention(dim, dim, n_head, d_head, flash_attn));\n        blocks[\"attn2\"] = std::shared_ptr<GGMLBlock>(new CrossAttention(dim, context_dim, n_head, d_head, flash_attn));\n        blocks[\"ff\"]    = std::shared_ptr<GGMLBlock>(new FeedForward(dim, dim));\n        blocks[\"norm1\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(dim));\n        blocks[\"norm2\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(dim));\n        blocks[\"norm3\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(dim));\n\n        if (ff_in) {\n            blocks[\"norm_in\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(dim));\n            blocks[\"ff_in\"]   = std::shared_ptr<GGMLBlock>(new FeedForward(dim, dim));\n        }\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x, struct ggml_tensor* context) {\n        // x: [N, n_token, query_dim]\n        // context: [N, n_context, context_dim]\n        // return: [N, n_token, query_dim]\n\n        auto attn1 = std::dynamic_pointer_cast<CrossAttention>(blocks[\"attn1\"]);\n        auto attn2 = std::dynamic_pointer_cast<CrossAttention>(blocks[\"attn2\"]);\n        auto ff    = std::dynamic_pointer_cast<FeedForward>(blocks[\"ff\"]);\n        auto norm1 = std::dynamic_pointer_cast<LayerNorm>(blocks[\"norm1\"]);\n        auto norm2 = std::dynamic_pointer_cast<LayerNorm>(blocks[\"norm2\"]);\n        auto norm3 = std::dynamic_pointer_cast<LayerNorm>(blocks[\"norm3\"]);\n\n        if (ff_in) {\n            auto norm_in = std::dynamic_pointer_cast<LayerNorm>(blocks[\"norm_in\"]);\n            auto ff_in   = std::dynamic_pointer_cast<FeedForward>(blocks[\"ff_in\"]);\n\n            auto x_skip = x;\n            x           = norm_in->forward(ctx, x);\n            x           = ff_in->forward(ctx, x);\n            // self.is_res is always True\n            x = ggml_add(ctx, x, x_skip);\n        }\n\n        auto r = x;\n        x      = norm1->forward(ctx, x);\n        x      = attn1->forward(ctx, x, x);  // self-attention\n        x      = ggml_add(ctx, x, r);\n        r      = x;\n        x      = norm2->forward(ctx, x);\n        x      = attn2->forward(ctx, x, context);  // cross-attention\n        x      = ggml_add(ctx, x, r);\n        r      = x;\n        x      = norm3->forward(ctx, x);\n        x      = ff->forward(ctx, x);\n        x      = ggml_add(ctx, x, r);\n\n        return x;\n    }\n};\n\nclass SpatialTransformer : public GGMLBlock {\nprotected:\n    int64_t in_channels;  // mult * model_channels\n    int64_t n_head;\n    int64_t d_head;\n    int64_t depth       = 1;    // 1\n    int64_t context_dim = 768;  // hidden_size, 1024 for VERSION_SD2\n\npublic:\n    SpatialTransformer(int64_t in_channels,\n                       int64_t n_head,\n                       int64_t d_head,\n                       int64_t depth,\n                       int64_t context_dim,\n                       bool flash_attn = false)\n        : in_channels(in_channels),\n          n_head(n_head),\n          d_head(d_head),\n          depth(depth),\n          context_dim(context_dim) {\n        // We will convert unet transformer linear to conv2d 1x1 when loading the weights, so use_linear is always False\n        // disable_self_attn is always False\n        int64_t inner_dim = n_head * d_head;  // in_channels\n        blocks[\"norm\"]    = std::shared_ptr<GGMLBlock>(new GroupNorm32(in_channels));\n        blocks[\"proj_in\"] = std::shared_ptr<GGMLBlock>(new Conv2d(in_channels, inner_dim, {1, 1}));\n\n        for (int i = 0; i < depth; i++) {\n            std::string name = \"transformer_blocks.\" + std::to_string(i);\n            blocks[name]     = std::shared_ptr<GGMLBlock>(new BasicTransformerBlock(inner_dim, n_head, d_head, context_dim, false, flash_attn));\n        }\n\n        blocks[\"proj_out\"] = std::shared_ptr<GGMLBlock>(new Conv2d(inner_dim, in_channels, {1, 1}));\n    }\n\n    virtual struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x, struct ggml_tensor* context) {\n        // x: [N, in_channels, h, w]\n        // context: [N, max_position(aka n_token), hidden_size(aka context_dim)]\n        auto norm     = std::dynamic_pointer_cast<GroupNorm32>(blocks[\"norm\"]);\n        auto proj_in  = std::dynamic_pointer_cast<Conv2d>(blocks[\"proj_in\"]);\n        auto proj_out = std::dynamic_pointer_cast<Conv2d>(blocks[\"proj_out\"]);\n\n        auto x_in         = x;\n        int64_t n         = x->ne[3];\n        int64_t h         = x->ne[1];\n        int64_t w         = x->ne[0];\n        int64_t inner_dim = n_head * d_head;\n\n        x = norm->forward(ctx, x);\n        x = proj_in->forward(ctx, x);  // [N, inner_dim, h, w]\n\n        x = ggml_cont(ctx, ggml_permute(ctx, x, 1, 2, 0, 3));  // [N, h, w, inner_dim]\n        x = ggml_reshape_3d(ctx, x, inner_dim, w * h, n);      // [N, h * w, inner_dim]\n\n        for (int i = 0; i < depth; i++) {\n            std::string name       = \"transformer_blocks.\" + std::to_string(i);\n            auto transformer_block = std::dynamic_pointer_cast<BasicTransformerBlock>(blocks[name]);\n\n            x = transformer_block->forward(ctx, x, context);\n        }\n\n        x = ggml_cont(ctx, ggml_permute(ctx, x, 1, 0, 2, 3));  // [N, inner_dim, h * w]\n        x = ggml_reshape_4d(ctx, x, w, h, inner_dim, n);       // [N, inner_dim, h, w]\n\n        // proj_out\n        x = proj_out->forward(ctx, x);  // [N, in_channels, h, w]\n\n        x = ggml_add(ctx, x, x_in);\n        return x;\n    }\n};\n\nclass AlphaBlender : public GGMLBlock {\nprotected:\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, std::string prefix = \"\") {\n        // Get the type of the \"mix_factor\" tensor from the input tensors map with the specified prefix\n        enum ggml_type wtype = GGML_TYPE_F32;  //(tensor_types.ypes.find(prefix + \"mix_factor\") != tensor_types.end()) ? tensor_types[prefix + \"mix_factor\"] : GGML_TYPE_F32;\n        params[\"mix_factor\"] = ggml_new_tensor_1d(ctx, wtype, 1);\n    }\n\n    float get_alpha() {\n        // image_only_indicator is always tensor([0.]) and since mix_factor.shape is [1,]\n        // so learned_with_images is same as learned\n        float alpha = ggml_backend_tensor_get_f32(params[\"mix_factor\"]);\n        return sigmoid(alpha);\n    }\n\npublic:\n    AlphaBlender() {\n        // merge_strategy is always learned_with_images\n        // for inference, we don't need to set alpha\n        // since mix_factor.shape is [1,], we don't need rearrange using  rearrange_pattern\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* x_spatial,\n                                struct ggml_tensor* x_temporal) {\n        // image_only_indicator is always tensor([0.])\n        float alpha = get_alpha();\n        auto x      = ggml_add(ctx,\n                               ggml_scale(ctx, x_spatial, alpha),\n                               ggml_scale(ctx, x_temporal, 1.0f - alpha));\n        return x;\n    }\n};\n\nclass VideoResBlock : public ResBlock {\npublic:\n    VideoResBlock(int channels,\n                  int emb_channels,\n                  int out_channels,\n                  std::pair<int, int> kernel_size = {3, 3},\n                  int64_t video_kernel_size       = 3,\n                  int dims                        = 2)  // always 2\n        : ResBlock(channels, emb_channels, out_channels, kernel_size, dims) {\n        blocks[\"time_stack\"] = std::shared_ptr<GGMLBlock>(new ResBlock(out_channels, emb_channels, out_channels, kernel_size, 3, true));\n        blocks[\"time_mixer\"] = std::shared_ptr<GGMLBlock>(new AlphaBlender());\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* x,\n                                struct ggml_tensor* emb,\n                                int num_video_frames) {\n        // x: [N, channels, h, w] aka [b*t, channels, h, w]\n        // emb: [N, emb_channels] aka [b*t, emb_channels]\n        // image_only_indicator is always tensor([0.])\n        auto time_stack = std::dynamic_pointer_cast<ResBlock>(blocks[\"time_stack\"]);\n        auto time_mixer = std::dynamic_pointer_cast<AlphaBlender>(blocks[\"time_mixer\"]);\n\n        x = ResBlock::forward(ctx, x, emb);\n\n        int64_t T = num_video_frames;\n        int64_t B = x->ne[3] / T;\n        int64_t C = x->ne[2];\n        int64_t H = x->ne[1];\n        int64_t W = x->ne[0];\n\n        x          = ggml_reshape_4d(ctx, x, W * H, C, T, B);           // (b t) c h w -> b t c (h w)\n        x          = ggml_cont(ctx, ggml_permute(ctx, x, 0, 2, 1, 3));  // b t c (h w) -> b c t (h w)\n        auto x_mix = x;\n\n        emb = ggml_reshape_4d(ctx, emb, emb->ne[0], T, B, emb->ne[3]);  // (b t) ... -> b t ...\n\n        x = time_stack->forward(ctx, x, emb);  // b t c (h w)\n\n        x = time_mixer->forward(ctx, x_mix, x);  // b t c (h w)\n\n        x = ggml_cont(ctx, ggml_permute(ctx, x, 0, 2, 1, 3));  // b c t (h w) -> b t c (h w)\n        x = ggml_reshape_4d(ctx, x, W, H, C, T * B);           // b t c (h w) -> (b t) c h w\n\n        return x;\n    }\n};\n\n#endif  // __COMMON_HPP__\n"
        },
        {
          "name": "conditioner.hpp",
          "type": "blob",
          "size": 57.46484375,
          "content": "#ifndef __CONDITIONER_HPP__\n#define __CONDITIONER_HPP__\n\n#include \"clip.hpp\"\n#include \"t5.hpp\"\n\nstruct SDCondition {\n    struct ggml_tensor* c_crossattn = NULL;  // aka context\n    struct ggml_tensor* c_vector    = NULL;  // aka y\n    struct ggml_tensor* c_concat    = NULL;\n\n    SDCondition() = default;\n    SDCondition(struct ggml_tensor* c_crossattn, struct ggml_tensor* c_vector, struct ggml_tensor* c_concat)\n        : c_crossattn(c_crossattn), c_vector(c_vector), c_concat(c_concat) {}\n};\n\nstruct Conditioner {\n    virtual SDCondition get_learned_condition(ggml_context* work_ctx,\n                                              int n_threads,\n                                              const std::string& text,\n                                              int clip_skip,\n                                              int width,\n                                              int height,\n                                              int adm_in_channels        = -1,\n                                              bool force_zero_embeddings = false)                                             = 0;\n    virtual void alloc_params_buffer()                                                                                        = 0;\n    virtual void free_params_buffer()                                                                                         = 0;\n    virtual void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors)                                       = 0;\n    virtual size_t get_params_buffer_size()                                                                                   = 0;\n    virtual std::tuple<SDCondition, std::vector<bool>> get_learned_condition_with_trigger(ggml_context* work_ctx,\n                                                                                          int n_threads,\n                                                                                          const std::string& text,\n                                                                                          int clip_skip,\n                                                                                          int width,\n                                                                                          int height,\n                                                                                          int num_input_imgs,\n                                                                                          int adm_in_channels        = -1,\n                                                                                          bool force_zero_embeddings = false) = 0;\n    virtual std::string remove_trigger_from_prompt(ggml_context* work_ctx,\n                                                   const std::string& prompt)                                                 = 0;\n};\n\n// ldm.modules.encoders.modules.FrozenCLIPEmbedder\n// Ref: https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/cad87bf4e3e0b0a759afa94e933527c3123d59bc/modules/sd_hijack_clip.py#L283\nstruct FrozenCLIPEmbedderWithCustomWords : public Conditioner {\n    SDVersion version    = VERSION_SD1;\n    PMVersion pm_version = PM_VERSION_1;\n    CLIPTokenizer tokenizer;\n    std::shared_ptr<CLIPTextModelRunner> text_model;\n    std::shared_ptr<CLIPTextModelRunner> text_model2;\n\n    std::string trigger_word = \"img\";  // should be user settable\n    std::string embd_dir;\n    int32_t num_custom_embeddings = 0;\n    std::vector<uint8_t> token_embed_custom;\n    std::vector<std::string> readed_embeddings;\n\n    FrozenCLIPEmbedderWithCustomWords(ggml_backend_t backend,\n                                      std::map<std::string, enum ggml_type>& tensor_types,\n                                      const std::string& embd_dir,\n                                      SDVersion version = VERSION_SD1,\n                                      PMVersion pv      = PM_VERSION_1,\n                                      int clip_skip     = -1)\n        : version(version), pm_version(pv), tokenizer(sd_version_is_sd2(version) ? 0 : 49407), embd_dir(embd_dir) {\n        if (clip_skip <= 0) {\n            clip_skip = 1;\n            if (sd_version_is_sd2(version) || sd_version_is_sdxl(version)) {\n                clip_skip = 2;\n            }\n        }\n        if (sd_version_is_sd1(version)) {\n            text_model = std::make_shared<CLIPTextModelRunner>(backend, tensor_types, \"cond_stage_model.transformer.text_model\", OPENAI_CLIP_VIT_L_14, clip_skip);\n        } else if (sd_version_is_sd2(version)) {\n            text_model = std::make_shared<CLIPTextModelRunner>(backend, tensor_types, \"cond_stage_model.transformer.text_model\", OPEN_CLIP_VIT_H_14, clip_skip);\n        } else if (sd_version_is_sdxl(version)) {\n            text_model  = std::make_shared<CLIPTextModelRunner>(backend, tensor_types, \"cond_stage_model.transformer.text_model\", OPENAI_CLIP_VIT_L_14, clip_skip, false);\n            text_model2 = std::make_shared<CLIPTextModelRunner>(backend, tensor_types, \"cond_stage_model.1.transformer.text_model\", OPEN_CLIP_VIT_BIGG_14, clip_skip, false);\n        }\n    }\n\n    void set_clip_skip(int clip_skip) {\n        text_model->set_clip_skip(clip_skip);\n        if (sd_version_is_sdxl(version)) {\n            text_model2->set_clip_skip(clip_skip);\n        }\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors) {\n        text_model->get_param_tensors(tensors, \"cond_stage_model.transformer.text_model\");\n        if (sd_version_is_sdxl(version)) {\n            text_model2->get_param_tensors(tensors, \"cond_stage_model.1.transformer.text_model\");\n        }\n    }\n\n    void alloc_params_buffer() {\n        text_model->alloc_params_buffer();\n        if (sd_version_is_sdxl(version)) {\n            text_model2->alloc_params_buffer();\n        }\n    }\n\n    void free_params_buffer() {\n        text_model->free_params_buffer();\n        if (sd_version_is_sdxl(version)) {\n            text_model2->free_params_buffer();\n        }\n    }\n\n    size_t get_params_buffer_size() {\n        size_t buffer_size = text_model->get_params_buffer_size();\n        if (sd_version_is_sdxl(version)) {\n            buffer_size += text_model2->get_params_buffer_size();\n        }\n        return buffer_size;\n    }\n\n    bool load_embedding(std::string embd_name, std::string embd_path, std::vector<int32_t>& bpe_tokens) {\n        // the order matters\n        ModelLoader model_loader;\n        if (!model_loader.init_from_file(embd_path)) {\n            LOG_ERROR(\"embedding '%s' failed\", embd_name.c_str());\n            return false;\n        }\n        if (std::find(readed_embeddings.begin(), readed_embeddings.end(), embd_name) != readed_embeddings.end()) {\n            LOG_DEBUG(\"embedding already read in: %s\", embd_name.c_str());\n            return true;\n        }\n        struct ggml_init_params params;\n        params.mem_size               = 10 * 1024 * 1024;  // max for custom embeddings 10 MB\n        params.mem_buffer             = NULL;\n        params.no_alloc               = false;\n        struct ggml_context* embd_ctx = ggml_init(params);\n        struct ggml_tensor* embd      = NULL;\n        int64_t hidden_size           = text_model->model.hidden_size;\n        auto on_load                  = [&](const TensorStorage& tensor_storage, ggml_tensor** dst_tensor) {\n            if (tensor_storage.ne[0] != hidden_size) {\n                LOG_DEBUG(\"embedding wrong hidden size, got %i, expected %i\", tensor_storage.ne[0], hidden_size);\n                return false;\n            }\n            embd        = ggml_new_tensor_2d(embd_ctx, tensor_storage.type, hidden_size, tensor_storage.n_dims > 1 ? tensor_storage.ne[1] : 1);\n            *dst_tensor = embd;\n            return true;\n        };\n        model_loader.load_tensors(on_load, NULL);\n        readed_embeddings.push_back(embd_name);\n        token_embed_custom.resize(token_embed_custom.size() + ggml_nbytes(embd));\n        memcpy((void*)(token_embed_custom.data() + num_custom_embeddings * hidden_size * ggml_type_size(embd->type)),\n               embd->data,\n               ggml_nbytes(embd));\n        for (int i = 0; i < embd->ne[1]; i++) {\n            bpe_tokens.push_back(text_model->model.vocab_size + num_custom_embeddings);\n            // LOG_DEBUG(\"new custom token: %i\", text_model.vocab_size + num_custom_embeddings);\n            num_custom_embeddings++;\n        }\n        LOG_DEBUG(\"embedding '%s' applied, custom embeddings: %i\", embd_name.c_str(), num_custom_embeddings);\n        return true;\n    }\n\n    std::tuple<std::vector<int>, std::vector<float>, std::vector<bool>>\n    tokenize_with_trigger_token(std::string text,\n                                int num_input_imgs,\n                                int32_t image_token,\n                                bool padding = false) {\n        return tokenize_with_trigger_token(text, num_input_imgs, image_token,\n                                           text_model->model.n_token, padding);\n    }\n\n    std::vector<int> convert_token_to_id(std::string text) {\n        auto on_new_token_cb = [&](std::string& str, std::vector<int32_t>& bpe_tokens) -> bool {\n            size_t word_end       = str.find(\",\");\n            std::string embd_name = word_end == std::string::npos ? str : str.substr(0, word_end);\n            embd_name             = trim(embd_name);\n            std::string embd_path = get_full_path(embd_dir, embd_name + \".pt\");\n            if (embd_path.size() == 0) {\n                embd_path = get_full_path(embd_dir, embd_name + \".ckpt\");\n            }\n            if (embd_path.size() == 0) {\n                embd_path = get_full_path(embd_dir, embd_name + \".safetensors\");\n            }\n            if (embd_path.size() > 0) {\n                if (load_embedding(embd_name, embd_path, bpe_tokens)) {\n                    if (word_end != std::string::npos) {\n                        str = str.substr(word_end);\n                    } else {\n                        str = \"\";\n                    }\n                    return true;\n                }\n            }\n            return false;\n        };\n        std::vector<int> curr_tokens = tokenizer.encode(text, on_new_token_cb);\n        return curr_tokens;\n    }\n\n    std::string decode(const std::vector<int>& tokens) {\n        return tokenizer.decode(tokens);\n    }\n\n    std::tuple<std::vector<int>, std::vector<float>, std::vector<bool>>\n    tokenize_with_trigger_token(std::string text,\n                                int num_input_imgs,\n                                int32_t image_token,\n                                size_t max_length = 0,\n                                bool padding      = false) {\n        auto parsed_attention = parse_prompt_attention(text);\n\n        {\n            std::stringstream ss;\n            ss << \"[\";\n            for (const auto& item : parsed_attention) {\n                ss << \"['\" << item.first << \"', \" << item.second << \"], \";\n            }\n            ss << \"]\";\n            LOG_DEBUG(\"parse '%s' to %s\", text.c_str(), ss.str().c_str());\n        }\n\n        auto on_new_token_cb = [&](std::string& str, std::vector<int32_t>& bpe_tokens) -> bool {\n            size_t word_end       = str.find(\",\");\n            std::string embd_name = word_end == std::string::npos ? str : str.substr(0, word_end);\n            embd_name             = trim(embd_name);\n            std::string embd_path = get_full_path(embd_dir, embd_name + \".pt\");\n            if (embd_path.size() == 0) {\n                embd_path = get_full_path(embd_dir, embd_name + \".ckpt\");\n            }\n            if (embd_path.size() == 0) {\n                embd_path = get_full_path(embd_dir, embd_name + \".safetensors\");\n            }\n            if (embd_path.size() > 0) {\n                if (load_embedding(embd_name, embd_path, bpe_tokens)) {\n                    if (word_end != std::string::npos) {\n                        str = str.substr(word_end);\n                    } else {\n                        str = \"\";\n                    }\n                    return true;\n                }\n            }\n            return false;\n        };\n\n        std::vector<int> tokens;\n        std::vector<float> weights;\n        std::vector<bool> class_token_mask;\n        int32_t class_idx = -1, tokens_acc = 0;\n        for (const auto& item : parsed_attention) {\n            std::vector<int> class_token_index;\n            std::vector<int> clean_input_ids;\n            const std::string& curr_text = item.first;\n            float curr_weight            = item.second;\n            // printf(\" %s: %f \\n\", curr_text.c_str(), curr_weight);\n            std::vector<int> curr_tokens = tokenizer.encode(curr_text, on_new_token_cb);\n            int32_t clean_index          = 0;\n            for (uint32_t i = 0; i < curr_tokens.size(); i++) {\n                int token_id = curr_tokens[i];\n                if (token_id == image_token)\n                    class_token_index.push_back(clean_index - 1);\n                else {\n                    clean_input_ids.push_back(token_id);\n                    clean_index++;\n                }\n            }\n            // GGML_ASSERT(class_token_index.size() == 1); // PhotoMaker currently does not support multiple\n            //     trigger words in a single prompt.\n            if (class_token_index.size() == 1) {\n                // Expand the class word token and corresponding mask\n                int class_token = clean_input_ids[class_token_index[0]];\n                class_idx       = tokens_acc + class_token_index[0];\n                std::vector<int> clean_input_ids_tmp;\n                for (uint32_t i = 0; i < class_token_index[0]; i++)\n                    clean_input_ids_tmp.push_back(clean_input_ids[i]);\n                for (uint32_t i = 0; i < (pm_version == PM_VERSION_2 ? 2 * num_input_imgs : num_input_imgs); i++)\n                    clean_input_ids_tmp.push_back(class_token);\n                for (uint32_t i = class_token_index[0] + 1; i < clean_input_ids.size(); i++)\n                    clean_input_ids_tmp.push_back(clean_input_ids[i]);\n                clean_input_ids.clear();\n                clean_input_ids = clean_input_ids_tmp;\n            }\n            tokens_acc += clean_index;\n            tokens.insert(tokens.end(), clean_input_ids.begin(), clean_input_ids.end());\n            weights.insert(weights.end(), clean_input_ids.size(), curr_weight);\n        }\n        // BUG!! double couting, pad_tokens will add BOS at the beginning\n        // tokens.insert(tokens.begin(), tokenizer.BOS_TOKEN_ID);\n        // weights.insert(weights.begin(), 1.0);\n\n        tokenizer.pad_tokens(tokens, weights, max_length, padding);\n        int offset = pm_version == PM_VERSION_2 ? 2 * num_input_imgs : num_input_imgs;\n        for (uint32_t i = 0; i < tokens.size(); i++) {\n            // if (class_idx + 1 <= i && i < class_idx + 1 + 2*num_input_imgs) // photomaker V2 has num_tokens(=2)*num_input_imgs\n            if (class_idx + 1 <= i && i < class_idx + 1 + offset)  // photomaker V2 has num_tokens(=2)*num_input_imgs\n                                                                   // hardcode for now\n                class_token_mask.push_back(true);\n            else\n                class_token_mask.push_back(false);\n        }\n\n        // printf(\"[\");\n        // for (int i = 0; i < tokens.size(); i++) {\n        //     printf(\"%d, \", class_token_mask[i] ? 1 : 0);\n        // }\n        // printf(\"]\\n\");\n\n        // for (int i = 0; i < tokens.size(); i++) {\n        //     std::cout << tokens[i] << \":\" << weights[i] << \", \";\n        // }\n        // std::cout << std::endl;\n\n        return std::make_tuple(tokens, weights, class_token_mask);\n    }\n\n    std::pair<std::vector<int>, std::vector<float>> tokenize(std::string text,\n                                                             bool padding = false) {\n        return tokenize(text, text_model->model.n_token, padding);\n    }\n\n    std::pair<std::vector<int>, std::vector<float>> tokenize(std::string text,\n                                                             size_t max_length = 0,\n                                                             bool padding      = false) {\n        auto parsed_attention = parse_prompt_attention(text);\n\n        {\n            std::stringstream ss;\n            ss << \"[\";\n            for (const auto& item : parsed_attention) {\n                ss << \"['\" << item.first << \"', \" << item.second << \"], \";\n            }\n            ss << \"]\";\n            LOG_DEBUG(\"parse '%s' to %s\", text.c_str(), ss.str().c_str());\n        }\n\n        auto on_new_token_cb = [&](std::string& str, std::vector<int32_t>& bpe_tokens) -> bool {\n            size_t word_end       = str.find(\",\");\n            std::string embd_name = word_end == std::string::npos ? str : str.substr(0, word_end);\n            embd_name             = trim(embd_name);\n            std::string embd_path = get_full_path(embd_dir, embd_name + \".pt\");\n            if (embd_path.size() == 0) {\n                embd_path = get_full_path(embd_dir, embd_name + \".ckpt\");\n            }\n            if (embd_path.size() == 0) {\n                embd_path = get_full_path(embd_dir, embd_name + \".safetensors\");\n            }\n            if (embd_path.size() > 0) {\n                if (load_embedding(embd_name, embd_path, bpe_tokens)) {\n                    if (word_end != std::string::npos) {\n                        str = str.substr(word_end);\n                    } else {\n                        str = \"\";\n                    }\n                    return true;\n                }\n            }\n            return false;\n        };\n\n        std::vector<int> tokens;\n        std::vector<float> weights;\n        for (const auto& item : parsed_attention) {\n            const std::string& curr_text = item.first;\n            float curr_weight            = item.second;\n            std::vector<int> curr_tokens = tokenizer.encode(curr_text, on_new_token_cb);\n            tokens.insert(tokens.end(), curr_tokens.begin(), curr_tokens.end());\n            weights.insert(weights.end(), curr_tokens.size(), curr_weight);\n        }\n\n        tokenizer.pad_tokens(tokens, weights, max_length, padding);\n\n        // for (int i = 0; i < tokens.size(); i++) {\n        //     std::cout << tokens[i] << \":\" << weights[i] << \", \";\n        // }\n        // std::cout << std::endl;\n\n        return {tokens, weights};\n    }\n\n    SDCondition get_learned_condition_common(ggml_context* work_ctx,\n                                             int n_threads,\n                                             std::vector<int>& tokens,\n                                             std::vector<float>& weights,\n                                             int clip_skip,\n                                             int width,\n                                             int height,\n                                             int adm_in_channels        = -1,\n                                             bool force_zero_embeddings = false) {\n        set_clip_skip(clip_skip);\n        int64_t t0                               = ggml_time_ms();\n        struct ggml_tensor* hidden_states        = NULL;  // [N, n_token, hidden_size]\n        struct ggml_tensor* chunk_hidden_states  = NULL;  // [n_token, hidden_size] or [n_token, hidden_size + hidden_size2]\n        struct ggml_tensor* chunk_hidden_states1 = NULL;  // [n_token, hidden_size]\n        struct ggml_tensor* chunk_hidden_states2 = NULL;  // [n_token, hidden_size2]\n        struct ggml_tensor* pooled               = NULL;\n        std::vector<float> hidden_states_vec;\n\n        size_t chunk_len   = 77;\n        size_t chunk_count = tokens.size() / chunk_len;\n        for (int chunk_idx = 0; chunk_idx < chunk_count; chunk_idx++) {\n            std::vector<int> chunk_tokens(tokens.begin() + chunk_idx * chunk_len,\n                                          tokens.begin() + (chunk_idx + 1) * chunk_len);\n            std::vector<float> chunk_weights(weights.begin() + chunk_idx * chunk_len,\n                                             weights.begin() + (chunk_idx + 1) * chunk_len);\n\n            auto input_ids                 = vector_to_ggml_tensor_i32(work_ctx, chunk_tokens);\n            struct ggml_tensor* input_ids2 = NULL;\n            size_t max_token_idx           = 0;\n            if (sd_version_is_sdxl(version)) {\n                auto it = std::find(chunk_tokens.begin(), chunk_tokens.end(), tokenizer.EOS_TOKEN_ID);\n                if (it != chunk_tokens.end()) {\n                    std::fill(std::next(it), chunk_tokens.end(), 0);\n                }\n\n                max_token_idx = std::min<size_t>(std::distance(chunk_tokens.begin(), it), chunk_tokens.size() - 1);\n\n                input_ids2 = vector_to_ggml_tensor_i32(work_ctx, chunk_tokens);\n\n                // for (int i = 0; i < chunk_tokens.size(); i++) {\n                //     printf(\"%d \", chunk_tokens[i]);\n                // }\n                // printf(\"\\n\");\n            }\n\n            {\n                text_model->compute(n_threads,\n                                    input_ids,\n                                    num_custom_embeddings,\n                                    token_embed_custom.data(),\n                                    max_token_idx,\n                                    false,\n                                    &chunk_hidden_states1,\n                                    work_ctx);\n                if (sd_version_is_sdxl(version)) {\n                    text_model2->compute(n_threads,\n                                         input_ids2,\n                                         0,\n                                         NULL,\n                                         max_token_idx,\n                                         false,\n                                         &chunk_hidden_states2, work_ctx);\n                    // concat\n                    chunk_hidden_states = ggml_tensor_concat(work_ctx, chunk_hidden_states1, chunk_hidden_states2, 0);\n\n                    if (chunk_idx == 0) {\n                        text_model2->compute(n_threads,\n                                             input_ids2,\n                                             0,\n                                             NULL,\n                                             max_token_idx,\n                                             true,\n                                             &pooled,\n                                             work_ctx);\n                    }\n                } else {\n                    chunk_hidden_states = chunk_hidden_states1;\n                }\n            }\n\n            int64_t t1 = ggml_time_ms();\n            LOG_DEBUG(\"computing condition graph completed, taking %\" PRId64 \" ms\", t1 - t0);\n            ggml_tensor* result = ggml_dup_tensor(work_ctx, chunk_hidden_states);\n            {\n                float original_mean = ggml_tensor_mean(chunk_hidden_states);\n                for (int i2 = 0; i2 < chunk_hidden_states->ne[2]; i2++) {\n                    for (int i1 = 0; i1 < chunk_hidden_states->ne[1]; i1++) {\n                        for (int i0 = 0; i0 < chunk_hidden_states->ne[0]; i0++) {\n                            float value = ggml_tensor_get_f32(chunk_hidden_states, i0, i1, i2);\n                            value *= chunk_weights[i1];\n                            ggml_tensor_set_f32(result, value, i0, i1, i2);\n                        }\n                    }\n                }\n                float new_mean = ggml_tensor_mean(result);\n                ggml_tensor_scale(result, (original_mean / new_mean));\n            }\n            if (force_zero_embeddings) {\n                float* vec = (float*)result->data;\n                for (int i = 0; i < ggml_nelements(result); i++) {\n                    vec[i] = 0;\n                }\n            }\n            hidden_states_vec.insert(hidden_states_vec.end(), (float*)result->data, ((float*)result->data) + ggml_nelements(result));\n        }\n\n        hidden_states = vector_to_ggml_tensor(work_ctx, hidden_states_vec);\n        hidden_states = ggml_reshape_2d(work_ctx,\n                                        hidden_states,\n                                        chunk_hidden_states->ne[0],\n                                        ggml_nelements(hidden_states) / chunk_hidden_states->ne[0]);\n\n        ggml_tensor* vec = NULL;\n        if (sd_version_is_sdxl(version)) {\n            int out_dim = 256;\n            vec         = ggml_new_tensor_1d(work_ctx, GGML_TYPE_F32, adm_in_channels);\n            // [0:1280]\n            size_t offset = 0;\n            memcpy(vec->data, pooled->data, ggml_nbytes(pooled));\n            offset += ggml_nbytes(pooled);\n\n            // original_size_as_tuple\n            float orig_width             = (float)width;\n            float orig_height            = (float)height;\n            std::vector<float> timesteps = {orig_height, orig_width};\n\n            ggml_tensor* embed_view = ggml_view_2d(work_ctx, vec, out_dim, 2, ggml_type_size(GGML_TYPE_F32) * out_dim, offset);\n            offset += ggml_nbytes(embed_view);\n            set_timestep_embedding(timesteps, embed_view, out_dim);\n            // print_ggml_tensor(ggml_reshape_1d(work_ctx, embed_view, out_dim * 2));\n            // crop_coords_top_left\n            float crop_coord_top  = 0.f;\n            float crop_coord_left = 0.f;\n            timesteps             = {crop_coord_top, crop_coord_left};\n            embed_view            = ggml_view_2d(work_ctx, vec, out_dim, 2, ggml_type_size(GGML_TYPE_F32) * out_dim, offset);\n            offset += ggml_nbytes(embed_view);\n            set_timestep_embedding(timesteps, embed_view, out_dim);\n            // print_ggml_tensor(ggml_reshape_1d(work_ctx, embed_view, out_dim * 2));\n            // target_size_as_tuple\n            float target_width  = (float)width;\n            float target_height = (float)height;\n            timesteps           = {target_height, target_width};\n            embed_view          = ggml_view_2d(work_ctx, vec, out_dim, 2, ggml_type_size(GGML_TYPE_F32) * out_dim, offset);\n            offset += ggml_nbytes(embed_view);\n            set_timestep_embedding(timesteps, embed_view, out_dim);\n            // print_ggml_tensor(ggml_reshape_1d(work_ctx, embed_view, out_dim * 2));\n            GGML_ASSERT(offset == ggml_nbytes(vec));\n        }\n        // print_ggml_tensor(result);\n        return SDCondition(hidden_states, vec, NULL);\n    }\n\n    std::tuple<SDCondition, std::vector<bool>>\n    get_learned_condition_with_trigger(ggml_context* work_ctx,\n                                       int n_threads,\n                                       const std::string& text,\n                                       int clip_skip,\n                                       int width,\n                                       int height,\n                                       int num_input_imgs,\n                                       int adm_in_channels        = -1,\n                                       bool force_zero_embeddings = false) {\n        auto image_tokens = convert_token_to_id(trigger_word);\n        // if(image_tokens.size() == 1){\n        //     printf(\" image token id is: %d \\n\", image_tokens[0]);\n        // }\n        GGML_ASSERT(image_tokens.size() == 1);\n        auto tokens_and_weights     = tokenize_with_trigger_token(text,\n                                                                  num_input_imgs,\n                                                                  image_tokens[0],\n                                                                  true);\n        std::vector<int>& tokens    = std::get<0>(tokens_and_weights);\n        std::vector<float>& weights = std::get<1>(tokens_and_weights);\n        std::vector<bool>& clsm     = std::get<2>(tokens_and_weights);\n        // printf(\"tokens: \\n\");\n        // for(int i = 0; i < tokens.size(); ++i)\n        //    printf(\"%d \", tokens[i]);\n        // printf(\"\\n\");\n        // printf(\"clsm: \\n\");\n        // for(int i = 0; i < clsm.size(); ++i)\n        //    printf(\"%d \", clsm[i]?1:0);\n        // printf(\"\\n\");\n        auto cond = get_learned_condition_common(work_ctx, n_threads, tokens, weights, clip_skip, width, height, adm_in_channels, force_zero_embeddings);\n        return std::make_tuple(cond, clsm);\n    }\n\n    std::string remove_trigger_from_prompt(ggml_context* work_ctx,\n                                           const std::string& prompt) {\n        auto image_tokens = convert_token_to_id(trigger_word);\n        GGML_ASSERT(image_tokens.size() == 1);\n        auto tokens_and_weights  = tokenize(prompt, false);\n        std::vector<int>& tokens = tokens_and_weights.first;\n        auto it                  = std::find(tokens.begin(), tokens.end(), image_tokens[0]);\n        GGML_ASSERT(it != tokens.end());  // prompt must have trigger word\n        tokens.erase(it);\n        return decode(tokens);\n    }\n\n    SDCondition get_learned_condition(ggml_context* work_ctx,\n                                      int n_threads,\n                                      const std::string& text,\n                                      int clip_skip,\n                                      int width,\n                                      int height,\n                                      int adm_in_channels        = -1,\n                                      bool force_zero_embeddings = false) {\n        auto tokens_and_weights     = tokenize(text, true);\n        std::vector<int>& tokens    = tokens_and_weights.first;\n        std::vector<float>& weights = tokens_and_weights.second;\n        return get_learned_condition_common(work_ctx, n_threads, tokens, weights, clip_skip, width, height, adm_in_channels, force_zero_embeddings);\n    }\n};\n\nstruct FrozenCLIPVisionEmbedder : public GGMLRunner {\n    CLIPVisionModelProjection vision_model;\n\n    FrozenCLIPVisionEmbedder(ggml_backend_t backend, std::map<std::string, enum ggml_type>& tensor_types)\n        : vision_model(OPEN_CLIP_VIT_H_14, true), GGMLRunner(backend) {\n        vision_model.init(params_ctx, tensor_types, \"cond_stage_model.transformer\");\n    }\n\n    std::string get_desc() {\n        return \"clip_vision\";\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors) {\n        vision_model.get_param_tensors(tensors, \"cond_stage_model.transformer\");\n    }\n\n    struct ggml_cgraph* build_graph(struct ggml_tensor* pixel_values) {\n        struct ggml_cgraph* gf = ggml_new_graph(compute_ctx);\n\n        pixel_values = to_backend(pixel_values);\n\n        struct ggml_tensor* hidden_states = vision_model.forward(compute_ctx, pixel_values);\n\n        ggml_build_forward_expand(gf, hidden_states);\n\n        return gf;\n    }\n\n    void compute(const int n_threads,\n                 ggml_tensor* pixel_values,\n                 ggml_tensor** output,\n                 ggml_context* output_ctx) {\n        auto get_graph = [&]() -> struct ggml_cgraph* {\n            return build_graph(pixel_values);\n        };\n        GGMLRunner::compute(get_graph, n_threads, true, output, output_ctx);\n    }\n};\n\nstruct SD3CLIPEmbedder : public Conditioner {\n    CLIPTokenizer clip_l_tokenizer;\n    CLIPTokenizer clip_g_tokenizer;\n    T5UniGramTokenizer t5_tokenizer;\n    std::shared_ptr<CLIPTextModelRunner> clip_l;\n    std::shared_ptr<CLIPTextModelRunner> clip_g;\n    std::shared_ptr<T5Runner> t5;\n\n    SD3CLIPEmbedder(ggml_backend_t backend,\n                    std::map<std::string, enum ggml_type>& tensor_types,\n                    int clip_skip = -1)\n        : clip_g_tokenizer(0) {\n        if (clip_skip <= 0) {\n            clip_skip = 2;\n        }\n        clip_l = std::make_shared<CLIPTextModelRunner>(backend, tensor_types, \"text_encoders.clip_l.transformer.text_model\", OPENAI_CLIP_VIT_L_14, clip_skip, false);\n        clip_g = std::make_shared<CLIPTextModelRunner>(backend, tensor_types, \"text_encoders.clip_g.transformer.text_model\", OPEN_CLIP_VIT_BIGG_14, clip_skip, false);\n        t5     = std::make_shared<T5Runner>(backend, tensor_types, \"text_encoders.t5xxl.transformer\");\n    }\n\n    void set_clip_skip(int clip_skip) {\n        clip_l->set_clip_skip(clip_skip);\n        clip_g->set_clip_skip(clip_skip);\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors) {\n        clip_l->get_param_tensors(tensors, \"text_encoders.clip_l.transformer.text_model\");\n        clip_g->get_param_tensors(tensors, \"text_encoders.clip_g.transformer.text_model\");\n        t5->get_param_tensors(tensors, \"text_encoders.t5xxl.transformer\");\n    }\n\n    void alloc_params_buffer() {\n        clip_l->alloc_params_buffer();\n        clip_g->alloc_params_buffer();\n        t5->alloc_params_buffer();\n    }\n\n    void free_params_buffer() {\n        clip_l->free_params_buffer();\n        clip_g->free_params_buffer();\n        t5->free_params_buffer();\n    }\n\n    size_t get_params_buffer_size() {\n        size_t buffer_size = clip_l->get_params_buffer_size();\n        buffer_size += clip_g->get_params_buffer_size();\n        buffer_size += t5->get_params_buffer_size();\n        return buffer_size;\n    }\n\n    std::vector<std::pair<std::vector<int>, std::vector<float>>> tokenize(std::string text,\n                                                                          size_t max_length = 0,\n                                                                          bool padding      = false) {\n        auto parsed_attention = parse_prompt_attention(text);\n\n        {\n            std::stringstream ss;\n            ss << \"[\";\n            for (const auto& item : parsed_attention) {\n                ss << \"['\" << item.first << \"', \" << item.second << \"], \";\n            }\n            ss << \"]\";\n            LOG_DEBUG(\"parse '%s' to %s\", text.c_str(), ss.str().c_str());\n        }\n\n        auto on_new_token_cb = [&](std::string& str, std::vector<int32_t>& bpe_tokens) -> bool {\n            return false;\n        };\n\n        std::vector<int> clip_l_tokens;\n        std::vector<float> clip_l_weights;\n        std::vector<int> clip_g_tokens;\n        std::vector<float> clip_g_weights;\n        std::vector<int> t5_tokens;\n        std::vector<float> t5_weights;\n        for (const auto& item : parsed_attention) {\n            const std::string& curr_text = item.first;\n            float curr_weight            = item.second;\n\n            std::vector<int> curr_tokens = clip_l_tokenizer.encode(curr_text, on_new_token_cb);\n            clip_l_tokens.insert(clip_l_tokens.end(), curr_tokens.begin(), curr_tokens.end());\n            clip_l_weights.insert(clip_l_weights.end(), curr_tokens.size(), curr_weight);\n\n            curr_tokens = clip_g_tokenizer.encode(curr_text, on_new_token_cb);\n            clip_g_tokens.insert(clip_g_tokens.end(), curr_tokens.begin(), curr_tokens.end());\n            clip_g_weights.insert(clip_g_weights.end(), curr_tokens.size(), curr_weight);\n\n            curr_tokens = t5_tokenizer.Encode(curr_text, true);\n            t5_tokens.insert(t5_tokens.end(), curr_tokens.begin(), curr_tokens.end());\n            t5_weights.insert(t5_weights.end(), curr_tokens.size(), curr_weight);\n        }\n\n        clip_l_tokenizer.pad_tokens(clip_l_tokens, clip_l_weights, max_length, padding);\n        clip_g_tokenizer.pad_tokens(clip_g_tokens, clip_g_weights, max_length, padding);\n        t5_tokenizer.pad_tokens(t5_tokens, t5_weights, max_length, padding);\n\n        // for (int i = 0; i < clip_l_tokens.size(); i++) {\n        //     std::cout << clip_l_tokens[i] << \":\" << clip_l_weights[i] << \", \";\n        // }\n        // std::cout << std::endl;\n\n        // for (int i = 0; i < clip_g_tokens.size(); i++) {\n        //     std::cout << clip_g_tokens[i] << \":\" << clip_g_weights[i] << \", \";\n        // }\n        // std::cout << std::endl;\n\n        // for (int i = 0; i < t5_tokens.size(); i++) {\n        //     std::cout << t5_tokens[i] << \":\" << t5_weights[i] << \", \";\n        // }\n        // std::cout << std::endl;\n\n        return {{clip_l_tokens, clip_l_weights}, {clip_g_tokens, clip_g_weights}, {t5_tokens, t5_weights}};\n    }\n\n    SDCondition get_learned_condition_common(ggml_context* work_ctx,\n                                             int n_threads,\n                                             std::vector<std::pair<std::vector<int>, std::vector<float>>> token_and_weights,\n                                             int clip_skip,\n                                             bool force_zero_embeddings = false) {\n        set_clip_skip(clip_skip);\n        auto& clip_l_tokens  = token_and_weights[0].first;\n        auto& clip_l_weights = token_and_weights[0].second;\n        auto& clip_g_tokens  = token_and_weights[1].first;\n        auto& clip_g_weights = token_and_weights[1].second;\n        auto& t5_tokens      = token_and_weights[2].first;\n        auto& t5_weights     = token_and_weights[2].second;\n\n        int64_t t0                                 = ggml_time_ms();\n        struct ggml_tensor* hidden_states          = NULL;  // [N, n_token*2, 4096]\n        struct ggml_tensor* chunk_hidden_states    = NULL;  // [n_token*2, 4096]\n        struct ggml_tensor* chunk_hidden_states_l  = NULL;  // [n_token, hidden_size_l]\n        struct ggml_tensor* chunk_hidden_states_g  = NULL;  // [n_token, hidden_size_g]\n        struct ggml_tensor* chunk_hidden_states_t5 = NULL;  // [n_token, hidden_size_t5]\n        struct ggml_tensor* pooled                 = NULL;\n        struct ggml_tensor* pooled_l               = NULL;  // [768,]\n        struct ggml_tensor* pooled_g               = NULL;  // [1280,]\n        std::vector<float> hidden_states_vec;\n\n        size_t chunk_len   = 77;\n        size_t chunk_count = clip_l_tokens.size() / chunk_len;\n        for (int chunk_idx = 0; chunk_idx < chunk_count; chunk_idx++) {\n            // clip_l\n            {\n                std::vector<int> chunk_tokens(clip_l_tokens.begin() + chunk_idx * chunk_len,\n                                              clip_l_tokens.begin() + (chunk_idx + 1) * chunk_len);\n                std::vector<float> chunk_weights(clip_l_weights.begin() + chunk_idx * chunk_len,\n                                                 clip_l_weights.begin() + (chunk_idx + 1) * chunk_len);\n\n                auto input_ids       = vector_to_ggml_tensor_i32(work_ctx, chunk_tokens);\n                size_t max_token_idx = 0;\n\n                clip_l->compute(n_threads,\n                                input_ids,\n                                0,\n                                NULL,\n                                max_token_idx,\n                                false,\n                                &chunk_hidden_states_l,\n                                work_ctx);\n                {\n                    auto tensor         = chunk_hidden_states_l;\n                    float original_mean = ggml_tensor_mean(tensor);\n                    for (int i2 = 0; i2 < tensor->ne[2]; i2++) {\n                        for (int i1 = 0; i1 < tensor->ne[1]; i1++) {\n                            for (int i0 = 0; i0 < tensor->ne[0]; i0++) {\n                                float value = ggml_tensor_get_f32(tensor, i0, i1, i2);\n                                value *= chunk_weights[i1];\n                                ggml_tensor_set_f32(tensor, value, i0, i1, i2);\n                            }\n                        }\n                    }\n                    float new_mean = ggml_tensor_mean(tensor);\n                    ggml_tensor_scale(tensor, (original_mean / new_mean));\n                }\n\n                if (chunk_idx == 0) {\n                    auto it       = std::find(chunk_tokens.begin(), chunk_tokens.end(), clip_l_tokenizer.EOS_TOKEN_ID);\n                    max_token_idx = std::min<size_t>(std::distance(chunk_tokens.begin(), it), chunk_tokens.size() - 1);\n                    clip_l->compute(n_threads,\n                                    input_ids,\n                                    0,\n                                    NULL,\n                                    max_token_idx,\n                                    true,\n                                    &pooled_l,\n                                    work_ctx);\n                }\n            }\n\n            // clip_g\n            {\n                std::vector<int> chunk_tokens(clip_g_tokens.begin() + chunk_idx * chunk_len,\n                                              clip_g_tokens.begin() + (chunk_idx + 1) * chunk_len);\n                std::vector<float> chunk_weights(clip_g_weights.begin() + chunk_idx * chunk_len,\n                                                 clip_g_weights.begin() + (chunk_idx + 1) * chunk_len);\n\n                auto input_ids       = vector_to_ggml_tensor_i32(work_ctx, chunk_tokens);\n                size_t max_token_idx = 0;\n\n                clip_g->compute(n_threads,\n                                input_ids,\n                                0,\n                                NULL,\n                                max_token_idx,\n                                false,\n                                &chunk_hidden_states_g,\n                                work_ctx);\n\n                {\n                    auto tensor         = chunk_hidden_states_g;\n                    float original_mean = ggml_tensor_mean(tensor);\n                    for (int i2 = 0; i2 < tensor->ne[2]; i2++) {\n                        for (int i1 = 0; i1 < tensor->ne[1]; i1++) {\n                            for (int i0 = 0; i0 < tensor->ne[0]; i0++) {\n                                float value = ggml_tensor_get_f32(tensor, i0, i1, i2);\n                                value *= chunk_weights[i1];\n                                ggml_tensor_set_f32(tensor, value, i0, i1, i2);\n                            }\n                        }\n                    }\n                    float new_mean = ggml_tensor_mean(tensor);\n                    ggml_tensor_scale(tensor, (original_mean / new_mean));\n                }\n\n                if (chunk_idx == 0) {\n                    auto it       = std::find(chunk_tokens.begin(), chunk_tokens.end(), clip_g_tokenizer.EOS_TOKEN_ID);\n                    max_token_idx = std::min<size_t>(std::distance(chunk_tokens.begin(), it), chunk_tokens.size() - 1);\n                    clip_g->compute(n_threads,\n                                    input_ids,\n                                    0,\n                                    NULL,\n                                    max_token_idx,\n                                    true,\n                                    &pooled_g,\n                                    work_ctx);\n                }\n            }\n\n            // t5\n            {\n                std::vector<int> chunk_tokens(t5_tokens.begin() + chunk_idx * chunk_len,\n                                              t5_tokens.begin() + (chunk_idx + 1) * chunk_len);\n                std::vector<float> chunk_weights(t5_weights.begin() + chunk_idx * chunk_len,\n                                                 t5_weights.begin() + (chunk_idx + 1) * chunk_len);\n\n                auto input_ids = vector_to_ggml_tensor_i32(work_ctx, chunk_tokens);\n\n                t5->compute(n_threads,\n                            input_ids,\n                            &chunk_hidden_states_t5,\n                            work_ctx);\n                {\n                    auto tensor         = chunk_hidden_states_t5;\n                    float original_mean = ggml_tensor_mean(tensor);\n                    for (int i2 = 0; i2 < tensor->ne[2]; i2++) {\n                        for (int i1 = 0; i1 < tensor->ne[1]; i1++) {\n                            for (int i0 = 0; i0 < tensor->ne[0]; i0++) {\n                                float value = ggml_tensor_get_f32(tensor, i0, i1, i2);\n                                value *= chunk_weights[i1];\n                                ggml_tensor_set_f32(tensor, value, i0, i1, i2);\n                            }\n                        }\n                    }\n                    float new_mean = ggml_tensor_mean(tensor);\n                    ggml_tensor_scale(tensor, (original_mean / new_mean));\n                }\n            }\n\n            auto chunk_hidden_states_lg_pad = ggml_new_tensor_3d(work_ctx,\n                                                                 chunk_hidden_states_l->type,\n                                                                 4096,\n                                                                 chunk_hidden_states_l->ne[1],\n                                                                 chunk_hidden_states_l->ne[2]);  // [n_token, 4096]\n\n            for (int i2 = 0; i2 < chunk_hidden_states_lg_pad->ne[2]; i2++) {\n                for (int i1 = 0; i1 < chunk_hidden_states_lg_pad->ne[1]; i1++) {\n                    for (int i0 = 0; i0 < chunk_hidden_states_lg_pad->ne[0]; i0++) {\n                        float value = 0.f;\n                        if (i0 < chunk_hidden_states_l->ne[0]) {\n                            value = ggml_tensor_get_f32(chunk_hidden_states_l, i0, i1, i2);\n                        } else if (i0 < chunk_hidden_states_l->ne[0] + chunk_hidden_states_g->ne[0]) {\n                            value = ggml_tensor_get_f32(chunk_hidden_states_g, i0 - chunk_hidden_states_l->ne[0], i1, i2);\n                        }\n                        ggml_tensor_set_f32(chunk_hidden_states_lg_pad, value, i0, i1, i2);\n                    }\n                }\n            }\n\n            chunk_hidden_states = ggml_tensor_concat(work_ctx, chunk_hidden_states_lg_pad, chunk_hidden_states_t5, 1);  // [n_token*2, 4096]\n\n            if (chunk_idx == 0) {\n                pooled = ggml_tensor_concat(work_ctx, pooled_l, pooled_g, 0);  // [768 + 1280]\n            }\n\n            int64_t t1 = ggml_time_ms();\n            LOG_DEBUG(\"computing condition graph completed, taking %\" PRId64 \" ms\", t1 - t0);\n            if (force_zero_embeddings) {\n                float* vec = (float*)chunk_hidden_states->data;\n                for (int i = 0; i < ggml_nelements(chunk_hidden_states); i++) {\n                    vec[i] = 0;\n                }\n            }\n\n            hidden_states_vec.insert(hidden_states_vec.end(),\n                                     (float*)chunk_hidden_states->data,\n                                     ((float*)chunk_hidden_states->data) + ggml_nelements(chunk_hidden_states));\n        }\n\n        hidden_states = vector_to_ggml_tensor(work_ctx, hidden_states_vec);\n        hidden_states = ggml_reshape_2d(work_ctx,\n                                        hidden_states,\n                                        chunk_hidden_states->ne[0],\n                                        ggml_nelements(hidden_states) / chunk_hidden_states->ne[0]);\n        return SDCondition(hidden_states, pooled, NULL);\n    }\n\n    SDCondition get_learned_condition(ggml_context* work_ctx,\n                                      int n_threads,\n                                      const std::string& text,\n                                      int clip_skip,\n                                      int width,\n                                      int height,\n                                      int adm_in_channels        = -1,\n                                      bool force_zero_embeddings = false) {\n        auto tokens_and_weights = tokenize(text, 77, true);\n        return get_learned_condition_common(work_ctx, n_threads, tokens_and_weights, clip_skip, force_zero_embeddings);\n    }\n\n    std::tuple<SDCondition, std::vector<bool>> get_learned_condition_with_trigger(ggml_context* work_ctx,\n                                                                                  int n_threads,\n                                                                                  const std::string& text,\n                                                                                  int clip_skip,\n                                                                                  int width,\n                                                                                  int height,\n                                                                                  int num_input_imgs,\n                                                                                  int adm_in_channels        = -1,\n                                                                                  bool force_zero_embeddings = false) {\n        GGML_ASSERT(0 && \"Not implemented yet!\");\n    }\n\n    std::string remove_trigger_from_prompt(ggml_context* work_ctx,\n                                           const std::string& prompt) {\n        GGML_ASSERT(0 && \"Not implemented yet!\");\n    }\n};\n\nstruct FluxCLIPEmbedder : public Conditioner {\n    CLIPTokenizer clip_l_tokenizer;\n    T5UniGramTokenizer t5_tokenizer;\n    std::shared_ptr<CLIPTextModelRunner> clip_l;\n    std::shared_ptr<T5Runner> t5;\n\n    FluxCLIPEmbedder(ggml_backend_t backend,\n                     std::map<std::string, enum ggml_type>& tensor_types,\n                     int clip_skip = -1) {\n        if (clip_skip <= 0) {\n            clip_skip = 2;\n        }\n        clip_l = std::make_shared<CLIPTextModelRunner>(backend, tensor_types, \"text_encoders.clip_l.transformer.text_model\", OPENAI_CLIP_VIT_L_14, clip_skip, true);\n        t5     = std::make_shared<T5Runner>(backend, tensor_types, \"text_encoders.t5xxl.transformer\");\n    }\n\n    void set_clip_skip(int clip_skip) {\n        clip_l->set_clip_skip(clip_skip);\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors) {\n        clip_l->get_param_tensors(tensors, \"text_encoders.clip_l.transformer.text_model\");\n        t5->get_param_tensors(tensors, \"text_encoders.t5xxl.transformer\");\n    }\n\n    void alloc_params_buffer() {\n        clip_l->alloc_params_buffer();\n        t5->alloc_params_buffer();\n    }\n\n    void free_params_buffer() {\n        clip_l->free_params_buffer();\n        t5->free_params_buffer();\n    }\n\n    size_t get_params_buffer_size() {\n        size_t buffer_size = clip_l->get_params_buffer_size();\n        buffer_size += t5->get_params_buffer_size();\n        return buffer_size;\n    }\n\n    std::vector<std::pair<std::vector<int>, std::vector<float>>> tokenize(std::string text,\n                                                                          size_t max_length = 0,\n                                                                          bool padding      = false) {\n        auto parsed_attention = parse_prompt_attention(text);\n\n        {\n            std::stringstream ss;\n            ss << \"[\";\n            for (const auto& item : parsed_attention) {\n                ss << \"['\" << item.first << \"', \" << item.second << \"], \";\n            }\n            ss << \"]\";\n            LOG_DEBUG(\"parse '%s' to %s\", text.c_str(), ss.str().c_str());\n        }\n\n        auto on_new_token_cb = [&](std::string& str, std::vector<int32_t>& bpe_tokens) -> bool {\n            return false;\n        };\n\n        std::vector<int> clip_l_tokens;\n        std::vector<float> clip_l_weights;\n        std::vector<int> t5_tokens;\n        std::vector<float> t5_weights;\n        for (const auto& item : parsed_attention) {\n            const std::string& curr_text = item.first;\n            float curr_weight            = item.second;\n\n            std::vector<int> curr_tokens = clip_l_tokenizer.encode(curr_text, on_new_token_cb);\n            clip_l_tokens.insert(clip_l_tokens.end(), curr_tokens.begin(), curr_tokens.end());\n            clip_l_weights.insert(clip_l_weights.end(), curr_tokens.size(), curr_weight);\n\n            curr_tokens = t5_tokenizer.Encode(curr_text, true);\n            t5_tokens.insert(t5_tokens.end(), curr_tokens.begin(), curr_tokens.end());\n            t5_weights.insert(t5_weights.end(), curr_tokens.size(), curr_weight);\n        }\n\n        clip_l_tokenizer.pad_tokens(clip_l_tokens, clip_l_weights, 77, padding);\n        t5_tokenizer.pad_tokens(t5_tokens, t5_weights, max_length, padding);\n\n        // for (int i = 0; i < clip_l_tokens.size(); i++) {\n        //     std::cout << clip_l_tokens[i] << \":\" << clip_l_weights[i] << \", \";\n        // }\n        // std::cout << std::endl;\n\n        // for (int i = 0; i < t5_tokens.size(); i++) {\n        //     std::cout << t5_tokens[i] << \":\" << t5_weights[i] << \", \";\n        // }\n        // std::cout << std::endl;\n\n        return {{clip_l_tokens, clip_l_weights}, {t5_tokens, t5_weights}};\n    }\n\n    SDCondition get_learned_condition_common(ggml_context* work_ctx,\n                                             int n_threads,\n                                             std::vector<std::pair<std::vector<int>, std::vector<float>>> token_and_weights,\n                                             int clip_skip,\n                                             bool force_zero_embeddings = false) {\n        set_clip_skip(clip_skip);\n        auto& clip_l_tokens  = token_and_weights[0].first;\n        auto& clip_l_weights = token_and_weights[0].second;\n        auto& t5_tokens      = token_and_weights[1].first;\n        auto& t5_weights     = token_and_weights[1].second;\n\n        int64_t t0                              = ggml_time_ms();\n        struct ggml_tensor* hidden_states       = NULL;  // [N, n_token, 4096]\n        struct ggml_tensor* chunk_hidden_states = NULL;  // [n_token, 4096]\n        struct ggml_tensor* pooled              = NULL;  // [768,]\n        std::vector<float> hidden_states_vec;\n\n        size_t chunk_len   = 256;\n        size_t chunk_count = t5_tokens.size() / chunk_len;\n        for (int chunk_idx = 0; chunk_idx < chunk_count; chunk_idx++) {\n            // clip_l\n            if (chunk_idx == 0) {\n                size_t chunk_len_l = 77;\n                std::vector<int> chunk_tokens(clip_l_tokens.begin(),\n                                              clip_l_tokens.begin() + chunk_len_l);\n                std::vector<float> chunk_weights(clip_l_weights.begin(),\n                                                 clip_l_weights.begin() + chunk_len_l);\n\n                auto input_ids       = vector_to_ggml_tensor_i32(work_ctx, chunk_tokens);\n                size_t max_token_idx = 0;\n\n                auto it       = std::find(chunk_tokens.begin(), chunk_tokens.end(), clip_l_tokenizer.EOS_TOKEN_ID);\n                max_token_idx = std::min<size_t>(std::distance(chunk_tokens.begin(), it), chunk_tokens.size() - 1);\n\n                clip_l->compute(n_threads,\n                                input_ids,\n                                0,\n                                NULL,\n                                max_token_idx,\n                                true,\n                                &pooled,\n                                work_ctx);\n            }\n\n            // t5\n            {\n                std::vector<int> chunk_tokens(t5_tokens.begin() + chunk_idx * chunk_len,\n                                              t5_tokens.begin() + (chunk_idx + 1) * chunk_len);\n                std::vector<float> chunk_weights(t5_weights.begin() + chunk_idx * chunk_len,\n                                                 t5_weights.begin() + (chunk_idx + 1) * chunk_len);\n\n                auto input_ids = vector_to_ggml_tensor_i32(work_ctx, chunk_tokens);\n\n                t5->compute(n_threads,\n                            input_ids,\n                            &chunk_hidden_states,\n                            work_ctx);\n                {\n                    auto tensor         = chunk_hidden_states;\n                    float original_mean = ggml_tensor_mean(tensor);\n                    for (int i2 = 0; i2 < tensor->ne[2]; i2++) {\n                        for (int i1 = 0; i1 < tensor->ne[1]; i1++) {\n                            for (int i0 = 0; i0 < tensor->ne[0]; i0++) {\n                                float value = ggml_tensor_get_f32(tensor, i0, i1, i2);\n                                value *= chunk_weights[i1];\n                                ggml_tensor_set_f32(tensor, value, i0, i1, i2);\n                            }\n                        }\n                    }\n                    float new_mean = ggml_tensor_mean(tensor);\n                    ggml_tensor_scale(tensor, (original_mean / new_mean));\n                }\n            }\n\n            int64_t t1 = ggml_time_ms();\n            LOG_DEBUG(\"computing condition graph completed, taking %\" PRId64 \" ms\", t1 - t0);\n            if (force_zero_embeddings) {\n                float* vec = (float*)chunk_hidden_states->data;\n                for (int i = 0; i < ggml_nelements(chunk_hidden_states); i++) {\n                    vec[i] = 0;\n                }\n            }\n\n            hidden_states_vec.insert(hidden_states_vec.end(),\n                                     (float*)chunk_hidden_states->data,\n                                     ((float*)chunk_hidden_states->data) + ggml_nelements(chunk_hidden_states));\n        }\n\n        hidden_states = vector_to_ggml_tensor(work_ctx, hidden_states_vec);\n        hidden_states = ggml_reshape_2d(work_ctx,\n                                        hidden_states,\n                                        chunk_hidden_states->ne[0],\n                                        ggml_nelements(hidden_states) / chunk_hidden_states->ne[0]);\n        return SDCondition(hidden_states, pooled, NULL);\n    }\n\n    SDCondition get_learned_condition(ggml_context* work_ctx,\n                                      int n_threads,\n                                      const std::string& text,\n                                      int clip_skip,\n                                      int width,\n                                      int height,\n                                      int adm_in_channels        = -1,\n                                      bool force_zero_embeddings = false) {\n        auto tokens_and_weights = tokenize(text, 256, true);\n        return get_learned_condition_common(work_ctx, n_threads, tokens_and_weights, clip_skip, force_zero_embeddings);\n    }\n\n    std::tuple<SDCondition, std::vector<bool>> get_learned_condition_with_trigger(ggml_context* work_ctx,\n                                                                                  int n_threads,\n                                                                                  const std::string& text,\n                                                                                  int clip_skip,\n                                                                                  int width,\n                                                                                  int height,\n                                                                                  int num_input_imgs,\n                                                                                  int adm_in_channels        = -1,\n                                                                                  bool force_zero_embeddings = false) {\n        GGML_ASSERT(0 && \"Not implemented yet!\");\n    }\n\n    std::string remove_trigger_from_prompt(ggml_context* work_ctx,\n                                           const std::string& prompt) {\n        GGML_ASSERT(0 && \"Not implemented yet!\");\n    }\n};\n\n#endif"
        },
        {
          "name": "control.hpp",
          "type": "blob",
          "size": 19.7060546875,
          "content": "#ifndef __CONTROL_HPP__\n#define __CONTROL_HPP__\n\n#include \"common.hpp\"\n#include \"ggml_extend.hpp\"\n#include \"model.h\"\n\n#define CONTROL_NET_GRAPH_SIZE 1536\n\n/*\n    =================================== ControlNet ===================================\n    Reference: https://github.com/comfyanonymous/ComfyUI/blob/master/comfy/cldm/cldm.py\n\n*/\nclass ControlNetBlock : public GGMLBlock {\nprotected:\n    SDVersion version = VERSION_SD1;\n    // network hparams\n    int in_channels                        = 4;\n    int out_channels                       = 4;\n    int hint_channels                      = 3;\n    int num_res_blocks                     = 2;\n    std::vector<int> attention_resolutions = {4, 2, 1};\n    std::vector<int> channel_mult          = {1, 2, 4, 4};\n    std::vector<int> transformer_depth     = {1, 1, 1, 1};\n    int time_embed_dim                     = 1280;  // model_channels*4\n    int num_heads                          = 8;\n    int num_head_channels                  = -1;   // channels // num_heads\n    int context_dim                        = 768;  // 1024 for VERSION_SD2, 2048 for VERSION_SDXL\n\npublic:\n    int model_channels  = 320;\n    int adm_in_channels = 2816;  // only for VERSION_SDXL\n\n    ControlNetBlock(SDVersion version = VERSION_SD1)\n        : version(version) {\n        if (sd_version_is_sd2(version)) {\n            context_dim       = 1024;\n            num_head_channels = 64;\n            num_heads         = -1;\n        } else if (sd_version_is_sdxl(version)) {\n            context_dim           = 2048;\n            attention_resolutions = {4, 2};\n            channel_mult          = {1, 2, 4};\n            transformer_depth     = {1, 2, 10};\n            num_head_channels     = 64;\n            num_heads             = -1;\n        } else if (version == VERSION_SVD) {\n            in_channels       = 8;\n            out_channels      = 4;\n            context_dim       = 1024;\n            adm_in_channels   = 768;\n            num_head_channels = 64;\n            num_heads         = -1;\n        }\n\n        blocks[\"time_embed.0\"] = std::shared_ptr<GGMLBlock>(new Linear(model_channels, time_embed_dim));\n        // time_embed_1 is nn.SiLU()\n        blocks[\"time_embed.2\"] = std::shared_ptr<GGMLBlock>(new Linear(time_embed_dim, time_embed_dim));\n\n        if (sd_version_is_sdxl(version) || version == VERSION_SVD) {\n            blocks[\"label_emb.0.0\"] = std::shared_ptr<GGMLBlock>(new Linear(adm_in_channels, time_embed_dim));\n            // label_emb_1 is nn.SiLU()\n            blocks[\"label_emb.0.2\"] = std::shared_ptr<GGMLBlock>(new Linear(time_embed_dim, time_embed_dim));\n        }\n\n        // input_blocks\n        blocks[\"input_blocks.0.0\"] = std::shared_ptr<GGMLBlock>(new Conv2d(in_channels, model_channels, {3, 3}, {1, 1}, {1, 1}));\n\n        std::vector<int> input_block_chans;\n        input_block_chans.push_back(model_channels);\n        int ch              = model_channels;\n        int input_block_idx = 0;\n        int ds              = 1;\n\n        auto get_resblock = [&](int64_t channels, int64_t emb_channels, int64_t out_channels) -> ResBlock* {\n            return new ResBlock(channels, emb_channels, out_channels);\n        };\n\n        auto get_attention_layer = [&](int64_t in_channels,\n                                       int64_t n_head,\n                                       int64_t d_head,\n                                       int64_t depth,\n                                       int64_t context_dim) -> SpatialTransformer* {\n            return new SpatialTransformer(in_channels, n_head, d_head, depth, context_dim);\n        };\n\n        auto make_zero_conv = [&](int64_t channels) {\n            return new Conv2d(channels, channels, {1, 1});\n        };\n\n        blocks[\"zero_convs.0.0\"] = std::shared_ptr<GGMLBlock>(make_zero_conv(model_channels));\n\n        blocks[\"input_hint_block.0\"] = std::shared_ptr<GGMLBlock>(new Conv2d(hint_channels, 16, {3, 3}, {1, 1}, {1, 1}));\n        // nn.SiLU()\n        blocks[\"input_hint_block.2\"] = std::shared_ptr<GGMLBlock>(new Conv2d(16, 16, {3, 3}, {1, 1}, {1, 1}));\n        // nn.SiLU()\n        blocks[\"input_hint_block.4\"] = std::shared_ptr<GGMLBlock>(new Conv2d(16, 32, {3, 3}, {2, 2}, {1, 1}));\n        // nn.SiLU()\n        blocks[\"input_hint_block.6\"] = std::shared_ptr<GGMLBlock>(new Conv2d(32, 32, {3, 3}, {1, 1}, {1, 1}));\n        // nn.SiLU()\n        blocks[\"input_hint_block.8\"] = std::shared_ptr<GGMLBlock>(new Conv2d(32, 96, {3, 3}, {2, 2}, {1, 1}));\n        // nn.SiLU()\n        blocks[\"input_hint_block.10\"] = std::shared_ptr<GGMLBlock>(new Conv2d(96, 96, {3, 3}, {1, 1}, {1, 1}));\n        // nn.SiLU()\n        blocks[\"input_hint_block.12\"] = std::shared_ptr<GGMLBlock>(new Conv2d(96, 256, {3, 3}, {2, 2}, {1, 1}));\n        // nn.SiLU()\n        blocks[\"input_hint_block.14\"] = std::shared_ptr<GGMLBlock>(new Conv2d(256, model_channels, {3, 3}, {1, 1}, {1, 1}));\n\n        size_t len_mults = channel_mult.size();\n        for (int i = 0; i < len_mults; i++) {\n            int mult = channel_mult[i];\n            for (int j = 0; j < num_res_blocks; j++) {\n                input_block_idx += 1;\n                std::string name = \"input_blocks.\" + std::to_string(input_block_idx) + \".0\";\n                blocks[name]     = std::shared_ptr<GGMLBlock>(get_resblock(ch, time_embed_dim, mult * model_channels));\n\n                ch = mult * model_channels;\n                if (std::find(attention_resolutions.begin(), attention_resolutions.end(), ds) != attention_resolutions.end()) {\n                    int n_head = num_heads;\n                    int d_head = ch / num_heads;\n                    if (num_head_channels != -1) {\n                        d_head = num_head_channels;\n                        n_head = ch / d_head;\n                    }\n                    std::string name = \"input_blocks.\" + std::to_string(input_block_idx) + \".1\";\n                    blocks[name]     = std::shared_ptr<GGMLBlock>(get_attention_layer(ch,\n                                                                                      n_head,\n                                                                                      d_head,\n                                                                                      transformer_depth[i],\n                                                                                      context_dim));\n                }\n                blocks[\"zero_convs.\" + std::to_string(input_block_idx) + \".0\"] = std::shared_ptr<GGMLBlock>(make_zero_conv(ch));\n                input_block_chans.push_back(ch);\n            }\n            if (i != len_mults - 1) {\n                input_block_idx += 1;\n                std::string name = \"input_blocks.\" + std::to_string(input_block_idx) + \".0\";\n                blocks[name]     = std::shared_ptr<GGMLBlock>(new DownSampleBlock(ch, ch));\n\n                blocks[\"zero_convs.\" + std::to_string(input_block_idx) + \".0\"] = std::shared_ptr<GGMLBlock>(make_zero_conv(ch));\n\n                input_block_chans.push_back(ch);\n                ds *= 2;\n            }\n        }\n\n        // middle blocks\n        int n_head = num_heads;\n        int d_head = ch / num_heads;\n        if (num_head_channels != -1) {\n            d_head = num_head_channels;\n            n_head = ch / d_head;\n        }\n        blocks[\"middle_block.0\"] = std::shared_ptr<GGMLBlock>(get_resblock(ch, time_embed_dim, ch));\n        blocks[\"middle_block.1\"] = std::shared_ptr<GGMLBlock>(get_attention_layer(ch,\n                                                                                  n_head,\n                                                                                  d_head,\n                                                                                  transformer_depth[transformer_depth.size() - 1],\n                                                                                  context_dim));\n        blocks[\"middle_block.2\"] = std::shared_ptr<GGMLBlock>(get_resblock(ch, time_embed_dim, ch));\n\n        // middle_block_out\n        blocks[\"middle_block_out.0\"] = std::shared_ptr<GGMLBlock>(make_zero_conv(ch));\n    }\n\n    struct ggml_tensor* resblock_forward(std::string name,\n                                         struct ggml_context* ctx,\n                                         struct ggml_tensor* x,\n                                         struct ggml_tensor* emb) {\n        auto block = std::dynamic_pointer_cast<ResBlock>(blocks[name]);\n        return block->forward(ctx, x, emb);\n    }\n\n    struct ggml_tensor* attention_layer_forward(std::string name,\n                                                struct ggml_context* ctx,\n                                                struct ggml_tensor* x,\n                                                struct ggml_tensor* context) {\n        auto block = std::dynamic_pointer_cast<SpatialTransformer>(blocks[name]);\n        return block->forward(ctx, x, context);\n    }\n\n    struct ggml_tensor* input_hint_block_forward(struct ggml_context* ctx,\n                                                 struct ggml_tensor* hint,\n                                                 struct ggml_tensor* emb,\n                                                 struct ggml_tensor* context) {\n        int num_input_blocks = 15;\n        auto h               = hint;\n        for (int i = 0; i < num_input_blocks; i++) {\n            if (i % 2 == 0) {\n                auto block = std::dynamic_pointer_cast<Conv2d>(blocks[\"input_hint_block.\" + std::to_string(i)]);\n\n                h = block->forward(ctx, h);\n            } else {\n                h = ggml_silu_inplace(ctx, h);\n            }\n        }\n        return h;\n    }\n\n    std::vector<struct ggml_tensor*> forward(struct ggml_context* ctx,\n                                             struct ggml_tensor* x,\n                                             struct ggml_tensor* hint,\n                                             struct ggml_tensor* guided_hint,\n                                             struct ggml_tensor* timesteps,\n                                             struct ggml_tensor* context,\n                                             struct ggml_tensor* y = NULL) {\n        // x: [N, in_channels, h, w] or [N, in_channels/2, h, w]\n        // timesteps: [N,]\n        // context: [N, max_position, hidden_size] or [1, max_position, hidden_size]. for example, [N, 77, 768]\n        // y: [N, adm_in_channels] or [1, adm_in_channels]\n        if (context != NULL) {\n            if (context->ne[2] != x->ne[3]) {\n                context = ggml_repeat(ctx, context, ggml_new_tensor_3d(ctx, GGML_TYPE_F32, context->ne[0], context->ne[1], x->ne[3]));\n            }\n        }\n\n        if (y != NULL) {\n            if (y->ne[1] != x->ne[3]) {\n                y = ggml_repeat(ctx, y, ggml_new_tensor_2d(ctx, GGML_TYPE_F32, y->ne[0], x->ne[3]));\n            }\n        }\n\n        auto time_embed_0     = std::dynamic_pointer_cast<Linear>(blocks[\"time_embed.0\"]);\n        auto time_embed_2     = std::dynamic_pointer_cast<Linear>(blocks[\"time_embed.2\"]);\n        auto input_blocks_0_0 = std::dynamic_pointer_cast<Conv2d>(blocks[\"input_blocks.0.0\"]);\n        auto zero_convs_0     = std::dynamic_pointer_cast<Conv2d>(blocks[\"zero_convs.0.0\"]);\n\n        auto middle_block_out = std::dynamic_pointer_cast<Conv2d>(blocks[\"middle_block_out.0\"]);\n\n        auto t_emb = ggml_nn_timestep_embedding(ctx, timesteps, model_channels);  // [N, model_channels]\n\n        auto emb = time_embed_0->forward(ctx, t_emb);\n        emb      = ggml_silu_inplace(ctx, emb);\n        emb      = time_embed_2->forward(ctx, emb);  // [N, time_embed_dim]\n\n        // SDXL/SVD\n        if (y != NULL) {\n            auto label_embed_0 = std::dynamic_pointer_cast<Linear>(blocks[\"label_emb.0.0\"]);\n            auto label_embed_2 = std::dynamic_pointer_cast<Linear>(blocks[\"label_emb.0.2\"]);\n\n            auto label_emb = label_embed_0->forward(ctx, y);\n            label_emb      = ggml_silu_inplace(ctx, label_emb);\n            label_emb      = label_embed_2->forward(ctx, label_emb);  // [N, time_embed_dim]\n\n            emb = ggml_add(ctx, emb, label_emb);  // [N, time_embed_dim]\n        }\n\n        std::vector<struct ggml_tensor*> outs;\n\n        if (guided_hint == NULL) {\n            guided_hint = input_hint_block_forward(ctx, hint, emb, context);\n        }\n        outs.push_back(guided_hint);\n\n        // input_blocks\n\n        // input block 0\n        auto h = input_blocks_0_0->forward(ctx, x);\n        h      = ggml_add(ctx, h, guided_hint);\n        outs.push_back(zero_convs_0->forward(ctx, h));\n\n        // input block 1-11\n        size_t len_mults    = channel_mult.size();\n        int input_block_idx = 0;\n        int ds              = 1;\n        for (int i = 0; i < len_mults; i++) {\n            int mult = channel_mult[i];\n            for (int j = 0; j < num_res_blocks; j++) {\n                input_block_idx += 1;\n                std::string name = \"input_blocks.\" + std::to_string(input_block_idx) + \".0\";\n                h                = resblock_forward(name, ctx, h, emb);  // [N, mult*model_channels, h, w]\n                if (std::find(attention_resolutions.begin(), attention_resolutions.end(), ds) != attention_resolutions.end()) {\n                    std::string name = \"input_blocks.\" + std::to_string(input_block_idx) + \".1\";\n                    h                = attention_layer_forward(name, ctx, h, context);  // [N, mult*model_channels, h, w]\n                }\n\n                auto zero_conv = std::dynamic_pointer_cast<Conv2d>(blocks[\"zero_convs.\" + std::to_string(input_block_idx) + \".0\"]);\n\n                outs.push_back(zero_conv->forward(ctx, h));\n            }\n            if (i != len_mults - 1) {\n                ds *= 2;\n                input_block_idx += 1;\n\n                std::string name = \"input_blocks.\" + std::to_string(input_block_idx) + \".0\";\n                auto block       = std::dynamic_pointer_cast<DownSampleBlock>(blocks[name]);\n\n                h = block->forward(ctx, h);  // [N, mult*model_channels, h/(2^(i+1)), w/(2^(i+1))]\n\n                auto zero_conv = std::dynamic_pointer_cast<Conv2d>(blocks[\"zero_convs.\" + std::to_string(input_block_idx) + \".0\"]);\n\n                outs.push_back(zero_conv->forward(ctx, h));\n            }\n        }\n        // [N, 4*model_channels, h/8, w/8]\n\n        // middle_block\n        h = resblock_forward(\"middle_block.0\", ctx, h, emb);             // [N, 4*model_channels, h/8, w/8]\n        h = attention_layer_forward(\"middle_block.1\", ctx, h, context);  // [N, 4*model_channels, h/8, w/8]\n        h = resblock_forward(\"middle_block.2\", ctx, h, emb);             // [N, 4*model_channels, h/8, w/8]\n\n        // out\n        outs.push_back(middle_block_out->forward(ctx, h));\n        return outs;\n    }\n};\n\nstruct ControlNet : public GGMLRunner {\n    SDVersion version = VERSION_SD1;\n    ControlNetBlock control_net;\n\n    ggml_backend_buffer_t control_buffer = NULL;  // keep control output tensors in backend memory\n    ggml_context* control_ctx            = NULL;\n    std::vector<struct ggml_tensor*> controls;  // (12 input block outputs, 1 middle block output) SD 1.5\n    struct ggml_tensor* guided_hint = NULL;     // guided_hint cache, for faster inference\n    bool guided_hint_cached         = false;\n\n    ControlNet(ggml_backend_t backend,\n               std::map<std::string, enum ggml_type>& tensor_types,\n               SDVersion version = VERSION_SD1)\n        : GGMLRunner(backend), control_net(version) {\n        control_net.init(params_ctx, tensor_types, \"\");\n    }\n\n    ~ControlNet() {\n        free_control_ctx();\n    }\n\n    void alloc_control_ctx(std::vector<struct ggml_tensor*> outs) {\n        struct ggml_init_params params;\n        params.mem_size   = static_cast<size_t>(outs.size() * ggml_tensor_overhead()) + 1024 * 1024;\n        params.mem_buffer = NULL;\n        params.no_alloc   = true;\n        control_ctx       = ggml_init(params);\n\n        controls.resize(outs.size() - 1);\n\n        size_t control_buffer_size = 0;\n\n        guided_hint = ggml_dup_tensor(control_ctx, outs[0]);\n        control_buffer_size += ggml_nbytes(guided_hint);\n\n        for (int i = 0; i < outs.size() - 1; i++) {\n            controls[i] = ggml_dup_tensor(control_ctx, outs[i + 1]);\n            control_buffer_size += ggml_nbytes(controls[i]);\n        }\n\n        control_buffer = ggml_backend_alloc_ctx_tensors(control_ctx, backend);\n\n        LOG_DEBUG(\"control buffer size %.2fMB\", control_buffer_size * 1.f / 1024.f / 1024.f);\n    }\n\n    void free_control_ctx() {\n        if (control_buffer != NULL) {\n            ggml_backend_buffer_free(control_buffer);\n            control_buffer = NULL;\n        }\n        if (control_ctx != NULL) {\n            ggml_free(control_ctx);\n            control_ctx = NULL;\n        }\n        guided_hint        = NULL;\n        guided_hint_cached = false;\n        controls.clear();\n    }\n\n    std::string get_desc() {\n        return \"control_net\";\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors, const std::string prefix) {\n        control_net.get_param_tensors(tensors, prefix);\n    }\n\n    struct ggml_cgraph* build_graph(struct ggml_tensor* x,\n                                    struct ggml_tensor* hint,\n                                    struct ggml_tensor* timesteps,\n                                    struct ggml_tensor* context,\n                                    struct ggml_tensor* y = NULL) {\n        struct ggml_cgraph* gf = ggml_new_graph_custom(compute_ctx, CONTROL_NET_GRAPH_SIZE, false);\n\n        x = to_backend(x);\n        if (guided_hint_cached) {\n            hint = NULL;\n        } else {\n            hint = to_backend(hint);\n        }\n        context   = to_backend(context);\n        y         = to_backend(y);\n        timesteps = to_backend(timesteps);\n\n        auto outs = control_net.forward(compute_ctx,\n                                        x,\n                                        hint,\n                                        guided_hint_cached ? guided_hint : NULL,\n                                        timesteps,\n                                        context,\n                                        y);\n\n        if (control_ctx == NULL) {\n            alloc_control_ctx(outs);\n        }\n\n        ggml_build_forward_expand(gf, ggml_cpy(compute_ctx, outs[0], guided_hint));\n        for (int i = 0; i < outs.size() - 1; i++) {\n            ggml_build_forward_expand(gf, ggml_cpy(compute_ctx, outs[i + 1], controls[i]));\n        }\n\n        return gf;\n    }\n\n    void compute(int n_threads,\n                 struct ggml_tensor* x,\n                 struct ggml_tensor* hint,\n                 struct ggml_tensor* timesteps,\n                 struct ggml_tensor* context,\n                 struct ggml_tensor* y,\n                 struct ggml_tensor** output     = NULL,\n                 struct ggml_context* output_ctx = NULL) {\n        // x: [N, in_channels, h, w]\n        // timesteps: [N, ]\n        // context: [N, max_position, hidden_size]([N, 77, 768]) or [1, max_position, hidden_size]\n        // y: [N, adm_in_channels] or [1, adm_in_channels]\n        auto get_graph = [&]() -> struct ggml_cgraph* {\n            return build_graph(x, hint, timesteps, context, y);\n        };\n\n        GGMLRunner::compute(get_graph, n_threads, false, output, output_ctx);\n        guided_hint_cached = true;\n    }\n\n    bool load_from_file(const std::string& file_path) {\n        LOG_INFO(\"loading control net from '%s'\", file_path.c_str());\n        alloc_params_buffer();\n        std::map<std::string, ggml_tensor*> tensors;\n        control_net.get_param_tensors(tensors);\n        std::set<std::string> ignore_tensors;\n\n        ModelLoader model_loader;\n        if (!model_loader.init_from_file(file_path)) {\n            LOG_ERROR(\"init control net model loader from file failed: '%s'\", file_path.c_str());\n            return false;\n        }\n\n        bool success = model_loader.load_tensors(tensors, backend, ignore_tensors);\n\n        if (!success) {\n            LOG_ERROR(\"load control net tensors from model loader failed\");\n            return false;\n        }\n\n        LOG_INFO(\"control net model loaded\");\n        return success;\n    }\n};\n\n#endif  // __CONTROL_HPP__"
        },
        {
          "name": "denoiser.hpp",
          "type": "blob",
          "size": 38.6171875,
          "content": "#ifndef __DENOISER_HPP__\n#define __DENOISER_HPP__\n\n#include \"ggml_extend.hpp\"\n#include \"gits_noise.inl\"\n\n/*================================================= CompVisDenoiser ==================================================*/\n\n// Ref: https://github.com/crowsonkb/k-diffusion/blob/master/k_diffusion/external.py\n\n#define TIMESTEPS 1000\n#define FLUX_TIMESTEPS 1000\n\nstruct SigmaSchedule {\n    int version = 0;\n    typedef std::function<float(float)> t_to_sigma_t;\n\n    virtual std::vector<float> get_sigmas(uint32_t n, float sigma_min, float sigma_max, t_to_sigma_t t_to_sigma) = 0;\n};\n\nstruct DiscreteSchedule : SigmaSchedule {\n    std::vector<float> get_sigmas(uint32_t n, float sigma_min, float sigma_max, t_to_sigma_t t_to_sigma) {\n        std::vector<float> result;\n\n        int t_max = TIMESTEPS - 1;\n\n        if (n == 0) {\n            return result;\n        } else if (n == 1) {\n            result.push_back(t_to_sigma((float)t_max));\n            result.push_back(0);\n            return result;\n        }\n\n        float step = static_cast<float>(t_max) / static_cast<float>(n - 1);\n        for (uint32_t i = 0; i < n; ++i) {\n            float t = t_max - step * i;\n            result.push_back(t_to_sigma(t));\n        }\n        result.push_back(0);\n        return result;\n    }\n};\n\nstruct ExponentialSchedule : SigmaSchedule {\n    std::vector<float> get_sigmas(uint32_t n, float sigma_min, float sigma_max, t_to_sigma_t t_to_sigma) {\n        std::vector<float> sigmas;\n\n        // Calculate step size\n        float log_sigma_min = std::log(sigma_min);\n        float log_sigma_max = std::log(sigma_max);\n        float step          = (log_sigma_max - log_sigma_min) / (n - 1);\n\n        // Fill sigmas with exponential values\n        for (uint32_t i = 0; i < n; ++i) {\n            float sigma = std::exp(log_sigma_max - step * i);\n            sigmas.push_back(sigma);\n        }\n\n        sigmas.push_back(0.0f);\n\n        return sigmas;\n    }\n};\n\n/* interp and linear_interp adapted from dpilger26's NumCpp library:\n * https://github.com/dpilger26/NumCpp/tree/5e40aab74d14e257d65d3dc385c9ff9e2120c60e */\nconstexpr double interp(double left, double right, double perc) noexcept {\n    return (left * (1. - perc)) + (right * perc);\n}\n\n/* This will make the assumption that the reference x and y values are\n * already sorted in ascending order because they are being generated as\n * such in the calling function */\nstd::vector<double> linear_interp(std::vector<float> new_x,\n                                  const std::vector<float> ref_x,\n                                  const std::vector<float> ref_y) {\n    const size_t len_x = new_x.size();\n    size_t i           = 0;\n    size_t j           = 0;\n    std::vector<double> new_y(len_x);\n\n    if (ref_x.size() != ref_y.size()) {\n        LOG_ERROR(\"Linear Interpolation Failed: length mismatch\");\n        return new_y;\n    }\n\n    /* Adjusted bounds checking to ensure new_x is within ref_x range */\n    if (new_x[0] < ref_x[0]) {\n        new_x[0] = ref_x[0];\n    }\n    if (new_x.back() > ref_x.back()) {\n        new_x.back() = ref_x.back();\n    }\n\n    while (i < len_x) {\n        if ((ref_x[j] > new_x[i]) || (new_x[i] > ref_x[j + 1])) {\n            j++;\n            continue;\n        }\n\n        const double perc = static_cast<double>(new_x[i] - ref_x[j]) / static_cast<double>(ref_x[j + 1] - ref_x[j]);\n\n        new_y[i] = interp(ref_y[j], ref_y[j + 1], perc);\n        i++;\n    }\n\n    return new_y;\n}\n\nstd::vector<float> linear_space(const float start, const float end, const size_t num_points) {\n    std::vector<float> result(num_points);\n    const float inc = (end - start) / (static_cast<float>(num_points - 1));\n\n    if (num_points > 0) {\n        result[0] = start;\n\n        for (size_t i = 1; i < num_points; i++) {\n            result[i] = result[i - 1] + inc;\n        }\n    }\n\n    return result;\n}\n\nstd::vector<float> log_linear_interpolation(std::vector<float> sigma_in,\n                                            const size_t new_len) {\n    const size_t s_len        = sigma_in.size();\n    std::vector<float> x_vals = linear_space(0.f, 1.f, s_len);\n    std::vector<float> y_vals(s_len);\n\n    /* Reverses the input array to be ascending instead of descending,\n     * also hits it with a log, it is log-linear interpolation after all */\n    for (size_t i = 0; i < s_len; i++) {\n        y_vals[i] = std::log(sigma_in[s_len - i - 1]);\n    }\n\n    std::vector<float> new_x_vals  = linear_space(0.f, 1.f, new_len);\n    std::vector<double> new_y_vals = linear_interp(new_x_vals, x_vals, y_vals);\n    std::vector<float> results(new_len);\n\n    for (size_t i = 0; i < new_len; i++) {\n        results[i] = static_cast<float>(std::exp(new_y_vals[new_len - i - 1]));\n    }\n\n    return results;\n}\n\n/*\nhttps://research.nvidia.com/labs/toronto-ai/AlignYourSteps/howto.html\n*/\nstruct AYSSchedule : SigmaSchedule {\n    std::vector<float> get_sigmas(uint32_t n, float sigma_min, float sigma_max, t_to_sigma_t t_to_sigma) {\n        const std::vector<float> noise_levels[] = {\n            /* SD1.5 */\n            {14.6146412293f, 6.4745760956f, 3.8636745985f, 2.6946151520f,\n             1.8841921177f, 1.3943805092f, 0.9642583904f, 0.6523686016f,\n             0.3977456272f, 0.1515232662f, 0.0291671582f},\n            /* SDXL */\n            {14.6146412293f, 6.3184485287f, 3.7681790315f, 2.1811480769f,\n             1.3405244945f, 0.8620721141f, 0.5550693289f, 0.3798540708f,\n             0.2332364134f, 0.1114188177f, 0.0291671582f},\n            /* SVD */\n            {700.00f, 54.5f, 15.886f, 7.977f, 4.248f, 1.789f, 0.981f, 0.403f,\n             0.173f, 0.034f, 0.002f},\n        };\n\n        std::vector<float> inputs;\n        std::vector<float> results(n + 1);\n\n        switch (version) {\n            case VERSION_SD2: /* fallthrough */\n                LOG_WARN(\"AYS not designed for SD2.X models\");\n            case VERSION_SD1:\n                LOG_INFO(\"AYS using SD1.5 noise levels\");\n                inputs = noise_levels[0];\n                break;\n            case VERSION_SDXL:\n                LOG_INFO(\"AYS using SDXL noise levels\");\n                inputs = noise_levels[1];\n                break;\n            case VERSION_SVD:\n                LOG_INFO(\"AYS using SVD noise levels\");\n                inputs = noise_levels[2];\n                break;\n            default:\n                LOG_ERROR(\"Version not compatable with AYS scheduler\");\n                return results;\n        }\n\n        /* Stretches those pre-calculated reference levels out to the desired\n         * size using log-linear interpolation */\n        if ((n + 1) != inputs.size()) {\n            results = log_linear_interpolation(inputs, n + 1);\n        } else {\n            results = inputs;\n        }\n\n        /* Not sure if this is strictly neccessary */\n        results[n] = 0.0f;\n\n        return results;\n    }\n};\n\n/*\n * GITS Scheduler: https://github.com/zju-pi/diff-sampler/tree/main/gits-main\n */\nstruct GITSSchedule : SigmaSchedule {\n    std::vector<float> get_sigmas(uint32_t n, float sigma_min, float sigma_max, t_to_sigma_t t_to_sigma) {\n        if (sigma_max <= 0.0f) {\n            return std::vector<float>{};\n        }\n\n        std::vector<float> sigmas;\n\n        // Assume coeff is provided (replace 1.20 with your dynamic coeff)\n        float coeff = 1.20f;  // Default coefficient\n        // Normalize coeff to the closest value in the array (0.80 to 1.50)\n        coeff = std::round(coeff * 20.0f) / 20.0f;  // Round to the nearest 0.05\n        // Calculate the index based on the coefficient\n        int index = static_cast<int>((coeff - 0.80f) / 0.05f);\n        // Ensure the index is within bounds\n        index                                                 = std::max(0, std::min(index, static_cast<int>(GITS_NOISE.size() - 1)));\n        const std::vector<std::vector<float>>& selected_noise = *GITS_NOISE[index];\n\n        if (n <= 20) {\n            sigmas = (selected_noise)[n - 2];\n        } else {\n            sigmas = log_linear_interpolation(selected_noise.back(), n + 1);\n        }\n\n        sigmas[n] = 0.0f;\n        return sigmas;\n    }\n};\n\nstruct KarrasSchedule : SigmaSchedule {\n    std::vector<float> get_sigmas(uint32_t n, float sigma_min, float sigma_max, t_to_sigma_t t_to_sigma) {\n        // These *COULD* be function arguments here,\n        // but does anybody ever bother to touch them?\n        float rho = 7.f;\n\n        std::vector<float> result(n + 1);\n\n        float min_inv_rho = pow(sigma_min, (1.f / rho));\n        float max_inv_rho = pow(sigma_max, (1.f / rho));\n        for (uint32_t i = 0; i < n; i++) {\n            // Eq. (5) from Karras et al 2022\n            result[i] = pow(max_inv_rho + (float)i / ((float)n - 1.f) * (min_inv_rho - max_inv_rho), rho);\n        }\n        result[n] = 0.;\n        return result;\n    }\n};\n\nstruct Denoiser {\n    std::shared_ptr<SigmaSchedule> schedule                                                  = std::make_shared<DiscreteSchedule>();\n    virtual float sigma_min()                                                                = 0;\n    virtual float sigma_max()                                                                = 0;\n    virtual float sigma_to_t(float sigma)                                                    = 0;\n    virtual float t_to_sigma(float t)                                                        = 0;\n    virtual std::vector<float> get_scalings(float sigma)                                     = 0;\n    virtual ggml_tensor* noise_scaling(float sigma, ggml_tensor* noise, ggml_tensor* latent) = 0;\n    virtual ggml_tensor* inverse_noise_scaling(float sigma, ggml_tensor* latent)             = 0;\n\n    virtual std::vector<float> get_sigmas(uint32_t n) {\n        auto bound_t_to_sigma = std::bind(&Denoiser::t_to_sigma, this, std::placeholders::_1);\n        return schedule->get_sigmas(n, sigma_min(), sigma_max(), bound_t_to_sigma);\n    }\n};\n\nstruct CompVisDenoiser : public Denoiser {\n    float sigmas[TIMESTEPS];\n    float log_sigmas[TIMESTEPS];\n\n    float sigma_data = 1.0f;\n\n    float sigma_min() {\n        return sigmas[0];\n    }\n\n    float sigma_max() {\n        return sigmas[TIMESTEPS - 1];\n    }\n\n    float sigma_to_t(float sigma) {\n        float log_sigma = std::log(sigma);\n        std::vector<float> dists;\n        dists.reserve(TIMESTEPS);\n        for (float log_sigma_val : log_sigmas) {\n            dists.push_back(log_sigma - log_sigma_val);\n        }\n\n        int low_idx = 0;\n        for (size_t i = 0; i < TIMESTEPS; i++) {\n            if (dists[i] >= 0) {\n                low_idx++;\n            }\n        }\n        low_idx      = std::min(std::max(low_idx - 1, 0), TIMESTEPS - 2);\n        int high_idx = low_idx + 1;\n\n        float low  = log_sigmas[low_idx];\n        float high = log_sigmas[high_idx];\n        float w    = (low - log_sigma) / (low - high);\n        w          = std::max(0.f, std::min(1.f, w));\n        float t    = (1.0f - w) * low_idx + w * high_idx;\n\n        return t;\n    }\n\n    float t_to_sigma(float t) {\n        int low_idx     = static_cast<int>(std::floor(t));\n        int high_idx    = static_cast<int>(std::ceil(t));\n        float w         = t - static_cast<float>(low_idx);\n        float log_sigma = (1.0f - w) * log_sigmas[low_idx] + w * log_sigmas[high_idx];\n        return std::exp(log_sigma);\n    }\n\n    std::vector<float> get_scalings(float sigma) {\n        float c_skip = 1.0f;\n        float c_out  = -sigma;\n        float c_in   = 1.0f / std::sqrt(sigma * sigma + sigma_data * sigma_data);\n        return {c_skip, c_out, c_in};\n    }\n\n    // this function will modify noise/latent\n    ggml_tensor* noise_scaling(float sigma, ggml_tensor* noise, ggml_tensor* latent) {\n        ggml_tensor_scale(noise, sigma);\n        ggml_tensor_add(latent, noise);\n        return latent;\n    }\n\n    ggml_tensor* inverse_noise_scaling(float sigma, ggml_tensor* latent) {\n        return latent;\n    }\n};\n\nstruct CompVisVDenoiser : public CompVisDenoiser {\n    std::vector<float> get_scalings(float sigma) {\n        float c_skip = sigma_data * sigma_data / (sigma * sigma + sigma_data * sigma_data);\n        float c_out  = -sigma * sigma_data / std::sqrt(sigma * sigma + sigma_data * sigma_data);\n        float c_in   = 1.0f / std::sqrt(sigma * sigma + sigma_data * sigma_data);\n        return {c_skip, c_out, c_in};\n    }\n};\n\nfloat time_snr_shift(float alpha, float t) {\n    if (alpha == 1.0f) {\n        return t;\n    }\n    return alpha * t / (1 + (alpha - 1) * t);\n}\n\nstruct DiscreteFlowDenoiser : public Denoiser {\n    float sigmas[TIMESTEPS];\n    float shift = 3.0f;\n\n    float sigma_data = 1.0f;\n\n    DiscreteFlowDenoiser() {\n        set_parameters();\n    }\n\n    void set_parameters() {\n        for (int i = 1; i < TIMESTEPS + 1; i++) {\n            sigmas[i - 1] = t_to_sigma(i);\n        }\n    }\n\n    float sigma_min() {\n        return sigmas[0];\n    }\n\n    float sigma_max() {\n        return sigmas[TIMESTEPS - 1];\n    }\n\n    float sigma_to_t(float sigma) {\n        return sigma * 1000.f;\n    }\n\n    float t_to_sigma(float t) {\n        t = t + 1;\n        return time_snr_shift(shift, t / 1000.f);\n    }\n\n    std::vector<float> get_scalings(float sigma) {\n        float c_skip = 1.0f;\n        float c_out  = -sigma;\n        float c_in   = 1.0f;\n        return {c_skip, c_out, c_in};\n    }\n\n    // this function will modify noise/latent\n    ggml_tensor* noise_scaling(float sigma, ggml_tensor* noise, ggml_tensor* latent) {\n        ggml_tensor_scale(noise, sigma);\n        ggml_tensor_scale(latent, 1.0f - sigma);\n        ggml_tensor_add(latent, noise);\n        return latent;\n    }\n\n    ggml_tensor* inverse_noise_scaling(float sigma, ggml_tensor* latent) {\n        ggml_tensor_scale(latent, 1.0f / (1.0f - sigma));\n        return latent;\n    }\n};\n\nfloat flux_time_shift(float mu, float sigma, float t) {\n    return std::exp(mu) / (std::exp(mu) + std::pow((1.0 / t - 1.0), sigma));\n}\n\nstruct FluxFlowDenoiser : public Denoiser {\n    float sigmas[TIMESTEPS];\n    float shift = 1.15f;\n\n    float sigma_data = 1.0f;\n\n    FluxFlowDenoiser(float shift = 1.15f) {\n        set_parameters(shift);\n    }\n\n    void set_parameters(float shift = 1.15f) {\n        this->shift = shift;\n        for (int i = 1; i < TIMESTEPS + 1; i++) {\n            sigmas[i - 1] = t_to_sigma(i / TIMESTEPS * TIMESTEPS);\n        }\n    }\n\n    float sigma_min() {\n        return sigmas[0];\n    }\n\n    float sigma_max() {\n        return sigmas[TIMESTEPS - 1];\n    }\n\n    float sigma_to_t(float sigma) {\n        return sigma;\n    }\n\n    float t_to_sigma(float t) {\n        t = t + 1;\n        return flux_time_shift(shift, 1.0f, t / TIMESTEPS);\n    }\n\n    std::vector<float> get_scalings(float sigma) {\n        float c_skip = 1.0f;\n        float c_out  = -sigma;\n        float c_in   = 1.0f;\n        return {c_skip, c_out, c_in};\n    }\n\n    // this function will modify noise/latent\n    ggml_tensor* noise_scaling(float sigma, ggml_tensor* noise, ggml_tensor* latent) {\n        ggml_tensor_scale(noise, sigma);\n        ggml_tensor_scale(latent, 1.0f - sigma);\n        ggml_tensor_add(latent, noise);\n        return latent;\n    }\n\n    ggml_tensor* inverse_noise_scaling(float sigma, ggml_tensor* latent) {\n        ggml_tensor_scale(latent, 1.0f / (1.0f - sigma));\n        return latent;\n    }\n};\n\ntypedef std::function<ggml_tensor*(ggml_tensor*, float, int)> denoise_cb_t;\n\n// k diffusion reverse ODE: dx = (x - D(x;\\sigma)) / \\sigma dt; \\sigma(t) = t\nstatic void sample_k_diffusion(sample_method_t method,\n                               denoise_cb_t model,\n                               ggml_context* work_ctx,\n                               ggml_tensor* x,\n                               std::vector<float> sigmas,\n                               std::shared_ptr<RNG> rng) {\n    size_t steps = sigmas.size() - 1;\n    // sample_euler_ancestral\n    switch (method) {\n        case EULER_A: {\n            struct ggml_tensor* noise = ggml_dup_tensor(work_ctx, x);\n            struct ggml_tensor* d     = ggml_dup_tensor(work_ctx, x);\n\n            for (int i = 0; i < steps; i++) {\n                float sigma = sigmas[i];\n\n                // denoise\n                ggml_tensor* denoised = model(x, sigma, i + 1);\n\n                // d = (x - denoised) / sigma\n                {\n                    float* vec_d        = (float*)d->data;\n                    float* vec_x        = (float*)x->data;\n                    float* vec_denoised = (float*)denoised->data;\n\n                    for (int i = 0; i < ggml_nelements(d); i++) {\n                        vec_d[i] = (vec_x[i] - vec_denoised[i]) / sigma;\n                    }\n                }\n\n                // get_ancestral_step\n                float sigma_up   = std::min(sigmas[i + 1],\n                                            std::sqrt(sigmas[i + 1] * sigmas[i + 1] * (sigmas[i] * sigmas[i] - sigmas[i + 1] * sigmas[i + 1]) / (sigmas[i] * sigmas[i])));\n                float sigma_down = std::sqrt(sigmas[i + 1] * sigmas[i + 1] - sigma_up * sigma_up);\n\n                // Euler method\n                float dt = sigma_down - sigmas[i];\n                // x = x + d * dt\n                {\n                    float* vec_d = (float*)d->data;\n                    float* vec_x = (float*)x->data;\n\n                    for (int i = 0; i < ggml_nelements(x); i++) {\n                        vec_x[i] = vec_x[i] + vec_d[i] * dt;\n                    }\n                }\n\n                if (sigmas[i + 1] > 0) {\n                    // x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n                    ggml_tensor_set_f32_randn(noise, rng);\n                    // noise = load_tensor_from_file(work_ctx, \"./rand\" + std::to_string(i+1) + \".bin\");\n                    {\n                        float* vec_x     = (float*)x->data;\n                        float* vec_noise = (float*)noise->data;\n\n                        for (int i = 0; i < ggml_nelements(x); i++) {\n                            vec_x[i] = vec_x[i] + vec_noise[i] * sigma_up;\n                        }\n                    }\n                }\n            }\n        } break;\n        case EULER:  // Implemented without any sigma churn\n        {\n            struct ggml_tensor* d = ggml_dup_tensor(work_ctx, x);\n\n            for (int i = 0; i < steps; i++) {\n                float sigma = sigmas[i];\n\n                // denoise\n                ggml_tensor* denoised = model(x, sigma, i + 1);\n\n                // d = (x - denoised) / sigma\n                {\n                    float* vec_d        = (float*)d->data;\n                    float* vec_x        = (float*)x->data;\n                    float* vec_denoised = (float*)denoised->data;\n\n                    for (int j = 0; j < ggml_nelements(d); j++) {\n                        vec_d[j] = (vec_x[j] - vec_denoised[j]) / sigma;\n                    }\n                }\n\n                float dt = sigmas[i + 1] - sigma;\n                // x = x + d * dt\n                {\n                    float* vec_d = (float*)d->data;\n                    float* vec_x = (float*)x->data;\n\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        vec_x[j] = vec_x[j] + vec_d[j] * dt;\n                    }\n                }\n            }\n        } break;\n        case HEUN: {\n            struct ggml_tensor* d  = ggml_dup_tensor(work_ctx, x);\n            struct ggml_tensor* x2 = ggml_dup_tensor(work_ctx, x);\n\n            for (int i = 0; i < steps; i++) {\n                // denoise\n                ggml_tensor* denoised = model(x, sigmas[i], -(i + 1));\n\n                // d = (x - denoised) / sigma\n                {\n                    float* vec_d        = (float*)d->data;\n                    float* vec_x        = (float*)x->data;\n                    float* vec_denoised = (float*)denoised->data;\n\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        vec_d[j] = (vec_x[j] - vec_denoised[j]) / sigmas[i];\n                    }\n                }\n\n                float dt = sigmas[i + 1] - sigmas[i];\n                if (sigmas[i + 1] == 0) {\n                    // Euler step\n                    // x = x + d * dt\n                    float* vec_d = (float*)d->data;\n                    float* vec_x = (float*)x->data;\n\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        vec_x[j] = vec_x[j] + vec_d[j] * dt;\n                    }\n                } else {\n                    // Heun step\n                    float* vec_d  = (float*)d->data;\n                    float* vec_d2 = (float*)d->data;\n                    float* vec_x  = (float*)x->data;\n                    float* vec_x2 = (float*)x2->data;\n\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        vec_x2[j] = vec_x[j] + vec_d[j] * dt;\n                    }\n\n                    ggml_tensor* denoised = model(x2, sigmas[i + 1], i + 1);\n                    float* vec_denoised   = (float*)denoised->data;\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        float d2 = (vec_x2[j] - vec_denoised[j]) / sigmas[i + 1];\n                        vec_d[j] = (vec_d[j] + d2) / 2;\n                        vec_x[j] = vec_x[j] + vec_d[j] * dt;\n                    }\n                }\n            }\n        } break;\n        case DPM2: {\n            struct ggml_tensor* d  = ggml_dup_tensor(work_ctx, x);\n            struct ggml_tensor* x2 = ggml_dup_tensor(work_ctx, x);\n\n            for (int i = 0; i < steps; i++) {\n                // denoise\n                ggml_tensor* denoised = model(x, sigmas[i], i + 1);\n\n                // d = (x - denoised) / sigma\n                {\n                    float* vec_d        = (float*)d->data;\n                    float* vec_x        = (float*)x->data;\n                    float* vec_denoised = (float*)denoised->data;\n\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        vec_d[j] = (vec_x[j] - vec_denoised[j]) / sigmas[i];\n                    }\n                }\n\n                if (sigmas[i + 1] == 0) {\n                    // Euler step\n                    // x = x + d * dt\n                    float dt     = sigmas[i + 1] - sigmas[i];\n                    float* vec_d = (float*)d->data;\n                    float* vec_x = (float*)x->data;\n\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        vec_x[j] = vec_x[j] + vec_d[j] * dt;\n                    }\n                } else {\n                    // DPM-Solver-2\n                    float sigma_mid = exp(0.5f * (log(sigmas[i]) + log(sigmas[i + 1])));\n                    float dt_1      = sigma_mid - sigmas[i];\n                    float dt_2      = sigmas[i + 1] - sigmas[i];\n\n                    float* vec_d  = (float*)d->data;\n                    float* vec_x  = (float*)x->data;\n                    float* vec_x2 = (float*)x2->data;\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        vec_x2[j] = vec_x[j] + vec_d[j] * dt_1;\n                    }\n\n                    ggml_tensor* denoised = model(x2, sigma_mid, i + 1);\n                    float* vec_denoised   = (float*)denoised->data;\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        float d2 = (vec_x2[j] - vec_denoised[j]) / sigma_mid;\n                        vec_x[j] = vec_x[j] + d2 * dt_2;\n                    }\n                }\n            }\n\n        } break;\n        case DPMPP2S_A: {\n            struct ggml_tensor* noise = ggml_dup_tensor(work_ctx, x);\n            struct ggml_tensor* d     = ggml_dup_tensor(work_ctx, x);\n            struct ggml_tensor* x2    = ggml_dup_tensor(work_ctx, x);\n\n            for (int i = 0; i < steps; i++) {\n                // denoise\n                ggml_tensor* denoised = model(x, sigmas[i], i + 1);\n\n                // get_ancestral_step\n                float sigma_up   = std::min(sigmas[i + 1],\n                                            std::sqrt(sigmas[i + 1] * sigmas[i + 1] * (sigmas[i] * sigmas[i] - sigmas[i + 1] * sigmas[i + 1]) / (sigmas[i] * sigmas[i])));\n                float sigma_down = std::sqrt(sigmas[i + 1] * sigmas[i + 1] - sigma_up * sigma_up);\n                auto t_fn        = [](float sigma) -> float { return -log(sigma); };\n                auto sigma_fn    = [](float t) -> float { return exp(-t); };\n\n                if (sigma_down == 0) {\n                    // Euler step\n                    float* vec_d        = (float*)d->data;\n                    float* vec_x        = (float*)x->data;\n                    float* vec_denoised = (float*)denoised->data;\n\n                    for (int j = 0; j < ggml_nelements(d); j++) {\n                        vec_d[j] = (vec_x[j] - vec_denoised[j]) / sigmas[i];\n                    }\n\n                    // TODO: If sigma_down == 0, isn't this wrong?\n                    // But\n                    // https://github.com/crowsonkb/k-diffusion/blob/master/k_diffusion/sampling.py#L525\n                    // has this exactly the same way.\n                    float dt = sigma_down - sigmas[i];\n                    for (int j = 0; j < ggml_nelements(d); j++) {\n                        vec_x[j] = vec_x[j] + vec_d[j] * dt;\n                    }\n                } else {\n                    // DPM-Solver++(2S)\n                    float t      = t_fn(sigmas[i]);\n                    float t_next = t_fn(sigma_down);\n                    float h      = t_next - t;\n                    float s      = t + 0.5f * h;\n\n                    float* vec_d        = (float*)d->data;\n                    float* vec_x        = (float*)x->data;\n                    float* vec_x2       = (float*)x2->data;\n                    float* vec_denoised = (float*)denoised->data;\n\n                    // First half-step\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        vec_x2[j] = (sigma_fn(s) / sigma_fn(t)) * vec_x[j] - (exp(-h * 0.5f) - 1) * vec_denoised[j];\n                    }\n\n                    ggml_tensor* denoised = model(x2, sigmas[i + 1], i + 1);\n\n                    // Second half-step\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        vec_x[j] = (sigma_fn(t_next) / sigma_fn(t)) * vec_x[j] - (exp(-h) - 1) * vec_denoised[j];\n                    }\n                }\n\n                // Noise addition\n                if (sigmas[i + 1] > 0) {\n                    ggml_tensor_set_f32_randn(noise, rng);\n                    {\n                        float* vec_x     = (float*)x->data;\n                        float* vec_noise = (float*)noise->data;\n\n                        for (int i = 0; i < ggml_nelements(x); i++) {\n                            vec_x[i] = vec_x[i] + vec_noise[i] * sigma_up;\n                        }\n                    }\n                }\n            }\n        } break;\n        case DPMPP2M:  // DPM++ (2M) from Karras et al (2022)\n        {\n            struct ggml_tensor* old_denoised = ggml_dup_tensor(work_ctx, x);\n\n            auto t_fn = [](float sigma) -> float { return -log(sigma); };\n\n            for (int i = 0; i < steps; i++) {\n                // denoise\n                ggml_tensor* denoised = model(x, sigmas[i], i + 1);\n\n                float t                 = t_fn(sigmas[i]);\n                float t_next            = t_fn(sigmas[i + 1]);\n                float h                 = t_next - t;\n                float a                 = sigmas[i + 1] / sigmas[i];\n                float b                 = exp(-h) - 1.f;\n                float* vec_x            = (float*)x->data;\n                float* vec_denoised     = (float*)denoised->data;\n                float* vec_old_denoised = (float*)old_denoised->data;\n\n                if (i == 0 || sigmas[i + 1] == 0) {\n                    // Simpler step for the edge cases\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        vec_x[j] = a * vec_x[j] - b * vec_denoised[j];\n                    }\n                } else {\n                    float h_last = t - t_fn(sigmas[i - 1]);\n                    float r      = h_last / h;\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        float denoised_d = (1.f + 1.f / (2.f * r)) * vec_denoised[j] - (1.f / (2.f * r)) * vec_old_denoised[j];\n                        vec_x[j]         = a * vec_x[j] - b * denoised_d;\n                    }\n                }\n\n                // old_denoised = denoised\n                for (int j = 0; j < ggml_nelements(x); j++) {\n                    vec_old_denoised[j] = vec_denoised[j];\n                }\n            }\n        } break;\n        case DPMPP2Mv2:  // Modified DPM++ (2M) from https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/8457\n        {\n            struct ggml_tensor* old_denoised = ggml_dup_tensor(work_ctx, x);\n\n            auto t_fn = [](float sigma) -> float { return -log(sigma); };\n\n            for (int i = 0; i < steps; i++) {\n                // denoise\n                ggml_tensor* denoised = model(x, sigmas[i], i + 1);\n\n                float t                 = t_fn(sigmas[i]);\n                float t_next            = t_fn(sigmas[i + 1]);\n                float h                 = t_next - t;\n                float a                 = sigmas[i + 1] / sigmas[i];\n                float* vec_x            = (float*)x->data;\n                float* vec_denoised     = (float*)denoised->data;\n                float* vec_old_denoised = (float*)old_denoised->data;\n\n                if (i == 0 || sigmas[i + 1] == 0) {\n                    // Simpler step for the edge cases\n                    float b = exp(-h) - 1.f;\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        vec_x[j] = a * vec_x[j] - b * vec_denoised[j];\n                    }\n                } else {\n                    float h_last = t - t_fn(sigmas[i - 1]);\n                    float h_min  = std::min(h_last, h);\n                    float h_max  = std::max(h_last, h);\n                    float r      = h_max / h_min;\n                    float h_d    = (h_max + h_min) / 2.f;\n                    float b      = exp(-h_d) - 1.f;\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        float denoised_d = (1.f + 1.f / (2.f * r)) * vec_denoised[j] - (1.f / (2.f * r)) * vec_old_denoised[j];\n                        vec_x[j]         = a * vec_x[j] - b * denoised_d;\n                    }\n                }\n\n                // old_denoised = denoised\n                for (int j = 0; j < ggml_nelements(x); j++) {\n                    vec_old_denoised[j] = vec_denoised[j];\n                }\n            }\n        } break;\n        case IPNDM:  // iPNDM sampler from https://github.com/zju-pi/diff-sampler/tree/main/diff-solvers-main\n        {\n            int max_order       = 4;\n            ggml_tensor* x_next = x;\n            std::vector<ggml_tensor*> buffer_model;\n\n            for (int i = 0; i < steps; i++) {\n                float sigma      = sigmas[i];\n                float sigma_next = sigmas[i + 1];\n\n                ggml_tensor* x_cur = x_next;\n                float* vec_x_cur   = (float*)x_cur->data;\n                float* vec_x_next  = (float*)x_next->data;\n\n                // Denoising step\n                ggml_tensor* denoised = model(x_cur, sigma, i + 1);\n                float* vec_denoised   = (float*)denoised->data;\n                // d_cur = (x_cur - denoised) / sigma\n                struct ggml_tensor* d_cur = ggml_dup_tensor(work_ctx, x_cur);\n                float* vec_d_cur          = (float*)d_cur->data;\n\n                for (int j = 0; j < ggml_nelements(d_cur); j++) {\n                    vec_d_cur[j] = (vec_x_cur[j] - vec_denoised[j]) / sigma;\n                }\n\n                int order = std::min(max_order, i + 1);\n\n                // Calculate vec_x_next based on the order\n                switch (order) {\n                    case 1:  // First Euler step\n                        for (int j = 0; j < ggml_nelements(x_next); j++) {\n                            vec_x_next[j] = vec_x_cur[j] + (sigma_next - sigma) * vec_d_cur[j];\n                        }\n                        break;\n\n                    case 2:  // Use one history point\n                    {\n                        float* vec_d_prev1 = (float*)buffer_model.back()->data;\n                        for (int j = 0; j < ggml_nelements(x_next); j++) {\n                            vec_x_next[j] = vec_x_cur[j] + (sigma_next - sigma) * (3 * vec_d_cur[j] - vec_d_prev1[j]) / 2;\n                        }\n                    } break;\n\n                    case 3:  // Use two history points\n                    {\n                        float* vec_d_prev1 = (float*)buffer_model.back()->data;\n                        float* vec_d_prev2 = (float*)buffer_model[buffer_model.size() - 2]->data;\n                        for (int j = 0; j < ggml_nelements(x_next); j++) {\n                            vec_x_next[j] = vec_x_cur[j] + (sigma_next - sigma) * (23 * vec_d_cur[j] - 16 * vec_d_prev1[j] + 5 * vec_d_prev2[j]) / 12;\n                        }\n                    } break;\n\n                    case 4:  // Use three history points\n                    {\n                        float* vec_d_prev1 = (float*)buffer_model.back()->data;\n                        float* vec_d_prev2 = (float*)buffer_model[buffer_model.size() - 2]->data;\n                        float* vec_d_prev3 = (float*)buffer_model[buffer_model.size() - 3]->data;\n                        for (int j = 0; j < ggml_nelements(x_next); j++) {\n                            vec_x_next[j] = vec_x_cur[j] + (sigma_next - sigma) * (55 * vec_d_cur[j] - 59 * vec_d_prev1[j] + 37 * vec_d_prev2[j] - 9 * vec_d_prev3[j]) / 24;\n                        }\n                    } break;\n                }\n\n                // Manage buffer_model\n                if (buffer_model.size() == max_order - 1) {\n                    // Shift elements to the left\n                    for (int k = 0; k < max_order - 2; k++) {\n                        buffer_model[k] = buffer_model[k + 1];\n                    }\n                    buffer_model.back() = d_cur;  // Replace the last element with d_cur\n                } else {\n                    buffer_model.push_back(d_cur);\n                }\n            }\n        } break;\n        case IPNDM_V:  // iPNDM_v sampler from https://github.com/zju-pi/diff-sampler/tree/main/diff-solvers-main\n        {\n            int max_order = 4;\n            std::vector<ggml_tensor*> buffer_model;\n            ggml_tensor* x_next = x;\n\n            for (int i = 0; i < steps; i++) {\n                float sigma  = sigmas[i];\n                float t_next = sigmas[i + 1];\n\n                // Denoising step\n                ggml_tensor* denoised     = model(x, sigma, i + 1);\n                float* vec_denoised       = (float*)denoised->data;\n                struct ggml_tensor* d_cur = ggml_dup_tensor(work_ctx, x);\n                float* vec_d_cur          = (float*)d_cur->data;\n                float* vec_x              = (float*)x->data;\n\n                // d_cur = (x - denoised) / sigma\n                for (int j = 0; j < ggml_nelements(d_cur); j++) {\n                    vec_d_cur[j] = (vec_x[j] - vec_denoised[j]) / sigma;\n                }\n\n                int order   = std::min(max_order, i + 1);\n                float h_n   = t_next - sigma;\n                float h_n_1 = (i > 0) ? (sigma - sigmas[i - 1]) : h_n;\n\n                switch (order) {\n                    case 1:  // First Euler step\n                        for (int j = 0; j < ggml_nelements(x_next); j++) {\n                            vec_x[j] += vec_d_cur[j] * h_n;\n                        }\n                        break;\n\n                    case 2: {\n                        float* vec_d_prev1 = (float*)buffer_model.back()->data;\n                        for (int j = 0; j < ggml_nelements(x_next); j++) {\n                            vec_x[j] += h_n * ((2 + (h_n / h_n_1)) * vec_d_cur[j] - (h_n / h_n_1) * vec_d_prev1[j]) / 2;\n                        }\n                        break;\n                    }\n\n                    case 3: {\n                        float h_n_2        = (i > 1) ? (sigmas[i - 1] - sigmas[i - 2]) : h_n_1;\n                        float* vec_d_prev1 = (float*)buffer_model.back()->data;\n                        float* vec_d_prev2 = (buffer_model.size() > 1) ? (float*)buffer_model[buffer_model.size() - 2]->data : vec_d_prev1;\n                        for (int j = 0; j < ggml_nelements(x_next); j++) {\n                            vec_x[j] += h_n * ((23 * vec_d_cur[j] - 16 * vec_d_prev1[j] + 5 * vec_d_prev2[j]) / 12);\n                        }\n                        break;\n                    }\n\n                    case 4: {\n                        float h_n_2        = (i > 1) ? (sigmas[i - 1] - sigmas[i - 2]) : h_n_1;\n                        float h_n_3        = (i > 2) ? (sigmas[i - 2] - sigmas[i - 3]) : h_n_2;\n                        float* vec_d_prev1 = (float*)buffer_model.back()->data;\n                        float* vec_d_prev2 = (buffer_model.size() > 1) ? (float*)buffer_model[buffer_model.size() - 2]->data : vec_d_prev1;\n                        float* vec_d_prev3 = (buffer_model.size() > 2) ? (float*)buffer_model[buffer_model.size() - 3]->data : vec_d_prev2;\n                        for (int j = 0; j < ggml_nelements(x_next); j++) {\n                            vec_x[j] += h_n * ((55 * vec_d_cur[j] - 59 * vec_d_prev1[j] + 37 * vec_d_prev2[j] - 9 * vec_d_prev3[j]) / 24);\n                        }\n                        break;\n                    }\n                }\n\n                // Manage buffer_model\n                if (buffer_model.size() == max_order - 1) {\n                    buffer_model.erase(buffer_model.begin());\n                }\n                buffer_model.push_back(d_cur);\n\n                // Prepare the next d tensor\n                d_cur = ggml_dup_tensor(work_ctx, x_next);\n            }\n        } break;\n        case LCM:  // Latent Consistency Models\n        {\n            struct ggml_tensor* noise = ggml_dup_tensor(work_ctx, x);\n            struct ggml_tensor* d     = ggml_dup_tensor(work_ctx, x);\n\n            for (int i = 0; i < steps; i++) {\n                float sigma = sigmas[i];\n\n                // denoise\n                ggml_tensor* denoised = model(x, sigma, i + 1);\n\n                // x = denoised\n                {\n                    float* vec_x        = (float*)x->data;\n                    float* vec_denoised = (float*)denoised->data;\n                    for (int j = 0; j < ggml_nelements(x); j++) {\n                        vec_x[j] = vec_denoised[j];\n                    }\n                }\n\n                if (sigmas[i + 1] > 0) {\n                    // x += sigmas[i + 1] * noise_sampler(sigmas[i], sigmas[i + 1])\n                    ggml_tensor_set_f32_randn(noise, rng);\n                    // noise = load_tensor_from_file(res_ctx, \"./rand\" + std::to_string(i+1) + \".bin\");\n                    {\n                        float* vec_x     = (float*)x->data;\n                        float* vec_noise = (float*)noise->data;\n\n                        for (int j = 0; j < ggml_nelements(x); j++) {\n                            vec_x[j] = vec_x[j] + sigmas[i + 1] * vec_noise[j];\n                        }\n                    }\n                }\n            }\n        } break;\n\n        default:\n            LOG_ERROR(\"Attempting to sample with nonexisting sample method %i\", method);\n            abort();\n    }\n}\n\n#endif  // __DENOISER_HPP__\n"
        },
        {
          "name": "diffusion_model.hpp",
          "type": "blob",
          "size": 6.5859375,
          "content": "#ifndef __DIFFUSION_MODEL_H__\n#define __DIFFUSION_MODEL_H__\n\n#include \"flux.hpp\"\n#include \"mmdit.hpp\"\n#include \"unet.hpp\"\n\nstruct DiffusionModel {\n    virtual void compute(int n_threads,\n                         struct ggml_tensor* x,\n                         struct ggml_tensor* timesteps,\n                         struct ggml_tensor* context,\n                         struct ggml_tensor* c_concat,\n                         struct ggml_tensor* y,\n                         struct ggml_tensor* guidance,\n                         int num_video_frames                      = -1,\n                         std::vector<struct ggml_tensor*> controls = {},\n                         float control_strength                    = 0.f,\n                         struct ggml_tensor** output               = NULL,\n                         struct ggml_context* output_ctx           = NULL,\n                         std::vector<int> skip_layers              = std::vector<int>())             = 0;\n    virtual void alloc_params_buffer()                                                  = 0;\n    virtual void free_params_buffer()                                                   = 0;\n    virtual void free_compute_buffer()                                                  = 0;\n    virtual void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors) = 0;\n    virtual size_t get_params_buffer_size()                                             = 0;\n    virtual int64_t get_adm_in_channels()                                               = 0;\n};\n\nstruct UNetModel : public DiffusionModel {\n    UNetModelRunner unet;\n\n    UNetModel(ggml_backend_t backend,\n              std::map<std::string, enum ggml_type>& tensor_types,\n              SDVersion version = VERSION_SD1,\n              bool flash_attn   = false)\n        : unet(backend, tensor_types, \"model.diffusion_model\", version, flash_attn) {\n    }\n\n    void alloc_params_buffer() {\n        unet.alloc_params_buffer();\n    }\n\n    void free_params_buffer() {\n        unet.free_params_buffer();\n    }\n\n    void free_compute_buffer() {\n        unet.free_compute_buffer();\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors) {\n        unet.get_param_tensors(tensors, \"model.diffusion_model\");\n    }\n\n    size_t get_params_buffer_size() {\n        return unet.get_params_buffer_size();\n    }\n\n    int64_t get_adm_in_channels() {\n        return unet.unet.adm_in_channels;\n    }\n\n    void compute(int n_threads,\n                 struct ggml_tensor* x,\n                 struct ggml_tensor* timesteps,\n                 struct ggml_tensor* context,\n                 struct ggml_tensor* c_concat,\n                 struct ggml_tensor* y,\n                 struct ggml_tensor* guidance,\n                 int num_video_frames                      = -1,\n                 std::vector<struct ggml_tensor*> controls = {},\n                 float control_strength                    = 0.f,\n                 struct ggml_tensor** output               = NULL,\n                 struct ggml_context* output_ctx           = NULL,\n                 std::vector<int> skip_layers              = std::vector<int>()) {\n        (void)skip_layers;  // SLG doesn't work with UNet models\n        return unet.compute(n_threads, x, timesteps, context, c_concat, y, num_video_frames, controls, control_strength, output, output_ctx);\n    }\n};\n\nstruct MMDiTModel : public DiffusionModel {\n    MMDiTRunner mmdit;\n\n    MMDiTModel(ggml_backend_t backend,\n               std::map<std::string, enum ggml_type>& tensor_types)\n        : mmdit(backend, tensor_types, \"model.diffusion_model\") {\n    }\n\n    void alloc_params_buffer() {\n        mmdit.alloc_params_buffer();\n    }\n\n    void free_params_buffer() {\n        mmdit.free_params_buffer();\n    }\n\n    void free_compute_buffer() {\n        mmdit.free_compute_buffer();\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors) {\n        mmdit.get_param_tensors(tensors, \"model.diffusion_model\");\n    }\n\n    size_t get_params_buffer_size() {\n        return mmdit.get_params_buffer_size();\n    }\n\n    int64_t get_adm_in_channels() {\n        return 768 + 1280;\n    }\n\n    void compute(int n_threads,\n                 struct ggml_tensor* x,\n                 struct ggml_tensor* timesteps,\n                 struct ggml_tensor* context,\n                 struct ggml_tensor* c_concat,\n                 struct ggml_tensor* y,\n                 struct ggml_tensor* guidance,\n                 int num_video_frames                      = -1,\n                 std::vector<struct ggml_tensor*> controls = {},\n                 float control_strength                    = 0.f,\n                 struct ggml_tensor** output               = NULL,\n                 struct ggml_context* output_ctx           = NULL,\n                 std::vector<int> skip_layers              = std::vector<int>()) {\n        return mmdit.compute(n_threads, x, timesteps, context, y, output, output_ctx, skip_layers);\n    }\n};\n\nstruct FluxModel : public DiffusionModel {\n    Flux::FluxRunner flux;\n\n    FluxModel(ggml_backend_t backend,\n              std::map<std::string, enum ggml_type>& tensor_types,\n              SDVersion version = VERSION_FLUX,\n              bool flash_attn   = false)\n        : flux(backend, tensor_types, \"model.diffusion_model\", version, flash_attn) {\n    }\n\n    void alloc_params_buffer() {\n        flux.alloc_params_buffer();\n    }\n\n    void free_params_buffer() {\n        flux.free_params_buffer();\n    }\n\n    void free_compute_buffer() {\n        flux.free_compute_buffer();\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors) {\n        flux.get_param_tensors(tensors, \"model.diffusion_model\");\n    }\n\n    size_t get_params_buffer_size() {\n        return flux.get_params_buffer_size();\n    }\n\n    int64_t get_adm_in_channels() {\n        return 768;\n    }\n\n    void compute(int n_threads,\n                 struct ggml_tensor* x,\n                 struct ggml_tensor* timesteps,\n                 struct ggml_tensor* context,\n                 struct ggml_tensor* c_concat,\n                 struct ggml_tensor* y,\n                 struct ggml_tensor* guidance,\n                 int num_video_frames                      = -1,\n                 std::vector<struct ggml_tensor*> controls = {},\n                 float control_strength                    = 0.f,\n                 struct ggml_tensor** output               = NULL,\n                 struct ggml_context* output_ctx           = NULL,\n                 std::vector<int> skip_layers              = std::vector<int>()) {\n        return flux.compute(n_threads, x, timesteps, context, c_concat, y, guidance, output, output_ctx, skip_layers);\n    }\n};\n\n#endif\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "esrgan.hpp",
          "type": "blob",
          "size": 7.990234375,
          "content": "#ifndef __ESRGAN_HPP__\n#define __ESRGAN_HPP__\n\n#include \"ggml_extend.hpp\"\n#include \"model.h\"\n\n/*\n    ===================================    ESRGAN  ===================================\n    References:\n    https://github.com/xinntao/Real-ESRGAN/blob/master/inference_realesrgan.py\n    https://github.com/XPixelGroup/BasicSR/blob/v1.4.2/basicsr/archs/rrdbnet_arch.py\n\n*/\n\nclass ResidualDenseBlock : public GGMLBlock {\nprotected:\n    int num_feat;\n    int num_grow_ch;\n\npublic:\n    ResidualDenseBlock(int num_feat = 64, int num_grow_ch = 32)\n        : num_feat(num_feat), num_grow_ch(num_grow_ch) {\n        blocks[\"conv1\"] = std::shared_ptr<GGMLBlock>(new Conv2d(num_feat, num_grow_ch, {3, 3}, {1, 1}, {1, 1}));\n        blocks[\"conv2\"] = std::shared_ptr<GGMLBlock>(new Conv2d(num_feat + num_grow_ch, num_grow_ch, {3, 3}, {1, 1}, {1, 1}));\n        blocks[\"conv3\"] = std::shared_ptr<GGMLBlock>(new Conv2d(num_feat + 2 * num_grow_ch, num_grow_ch, {3, 3}, {1, 1}, {1, 1}));\n        blocks[\"conv4\"] = std::shared_ptr<GGMLBlock>(new Conv2d(num_feat + 3 * num_grow_ch, num_grow_ch, {3, 3}, {1, 1}, {1, 1}));\n        blocks[\"conv5\"] = std::shared_ptr<GGMLBlock>(new Conv2d(num_feat + 4 * num_grow_ch, num_feat, {3, 3}, {1, 1}, {1, 1}));\n    }\n\n    struct ggml_tensor* lrelu(struct ggml_context* ctx, struct ggml_tensor* x) {\n        return ggml_leaky_relu(ctx, x, 0.2f, true);\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [n, num_feat, h, w]\n        // return: [n, num_feat, h, w]\n\n        auto conv1 = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv1\"]);\n        auto conv2 = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv2\"]);\n        auto conv3 = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv3\"]);\n        auto conv4 = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv4\"]);\n        auto conv5 = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv5\"]);\n\n        auto x1    = lrelu(ctx, conv1->forward(ctx, x));\n        auto x_cat = ggml_concat(ctx, x, x1, 2);\n        auto x2    = lrelu(ctx, conv2->forward(ctx, x_cat));\n        x_cat      = ggml_concat(ctx, x_cat, x2, 2);\n        auto x3    = lrelu(ctx, conv3->forward(ctx, x_cat));\n        x_cat      = ggml_concat(ctx, x_cat, x3, 2);\n        auto x4    = lrelu(ctx, conv4->forward(ctx, x_cat));\n        x_cat      = ggml_concat(ctx, x_cat, x4, 2);\n        auto x5    = conv5->forward(ctx, x_cat);\n\n        x5 = ggml_add(ctx, ggml_scale(ctx, x5, 0.2f), x);\n        return x5;\n    }\n};\n\nclass RRDB : public GGMLBlock {\npublic:\n    RRDB(int num_feat, int num_grow_ch = 32) {\n        blocks[\"rdb1\"] = std::shared_ptr<GGMLBlock>(new ResidualDenseBlock(num_feat, num_grow_ch));\n        blocks[\"rdb2\"] = std::shared_ptr<GGMLBlock>(new ResidualDenseBlock(num_feat, num_grow_ch));\n        blocks[\"rdb3\"] = std::shared_ptr<GGMLBlock>(new ResidualDenseBlock(num_feat, num_grow_ch));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [n, num_feat, h, w]\n        // return: [n, num_feat, h, w]\n\n        auto rdb1 = std::dynamic_pointer_cast<ResidualDenseBlock>(blocks[\"rdb1\"]);\n        auto rdb2 = std::dynamic_pointer_cast<ResidualDenseBlock>(blocks[\"rdb2\"]);\n        auto rdb3 = std::dynamic_pointer_cast<ResidualDenseBlock>(blocks[\"rdb3\"]);\n\n        auto out = rdb1->forward(ctx, x);\n        out      = rdb2->forward(ctx, out);\n        out      = rdb3->forward(ctx, out);\n\n        out = ggml_add(ctx, ggml_scale(ctx, out, 0.2f), x);\n        return out;\n    }\n};\n\nclass RRDBNet : public GGMLBlock {\nprotected:\n    int scale       = 4;  // default RealESRGAN_x4plus_anime_6B\n    int num_block   = 6;  // default RealESRGAN_x4plus_anime_6B\n    int num_in_ch   = 3;\n    int num_out_ch  = 3;\n    int num_feat    = 64;  // default RealESRGAN_x4plus_anime_6B\n    int num_grow_ch = 32;  // default RealESRGAN_x4plus_anime_6B\n\npublic:\n    RRDBNet() {\n        blocks[\"conv_first\"] = std::shared_ptr<GGMLBlock>(new Conv2d(num_in_ch, num_feat, {3, 3}, {1, 1}, {1, 1}));\n        for (int i = 0; i < num_block; i++) {\n            std::string name = \"body.\" + std::to_string(i);\n            blocks[name]     = std::shared_ptr<GGMLBlock>(new RRDB(num_feat, num_grow_ch));\n        }\n        blocks[\"conv_body\"] = std::shared_ptr<GGMLBlock>(new Conv2d(num_feat, num_feat, {3, 3}, {1, 1}, {1, 1}));\n        // upsample\n        blocks[\"conv_up1\"]  = std::shared_ptr<GGMLBlock>(new Conv2d(num_feat, num_feat, {3, 3}, {1, 1}, {1, 1}));\n        blocks[\"conv_up2\"]  = std::shared_ptr<GGMLBlock>(new Conv2d(num_feat, num_feat, {3, 3}, {1, 1}, {1, 1}));\n        blocks[\"conv_hr\"]   = std::shared_ptr<GGMLBlock>(new Conv2d(num_feat, num_feat, {3, 3}, {1, 1}, {1, 1}));\n        blocks[\"conv_last\"] = std::shared_ptr<GGMLBlock>(new Conv2d(num_feat, num_out_ch, {3, 3}, {1, 1}, {1, 1}));\n    }\n\n    struct ggml_tensor* lrelu(struct ggml_context* ctx, struct ggml_tensor* x) {\n        return ggml_leaky_relu(ctx, x, 0.2f, true);\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [n, num_in_ch, h, w]\n        // return: [n, num_out_ch, h*4, w*4]\n        auto conv_first = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv_first\"]);\n        auto conv_body  = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv_body\"]);\n        auto conv_up1   = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv_up1\"]);\n        auto conv_up2   = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv_up2\"]);\n        auto conv_hr    = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv_hr\"]);\n        auto conv_last  = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv_last\"]);\n\n        auto feat      = conv_first->forward(ctx, x);\n        auto body_feat = feat;\n        for (int i = 0; i < num_block; i++) {\n            std::string name = \"body.\" + std::to_string(i);\n            auto block       = std::dynamic_pointer_cast<RRDB>(blocks[name]);\n\n            body_feat = block->forward(ctx, body_feat);\n        }\n        body_feat = conv_body->forward(ctx, body_feat);\n        feat      = ggml_add(ctx, feat, body_feat);\n        // upsample\n        feat     = lrelu(ctx, conv_up1->forward(ctx, ggml_upscale(ctx, feat, 2)));\n        feat     = lrelu(ctx, conv_up2->forward(ctx, ggml_upscale(ctx, feat, 2)));\n        auto out = conv_last->forward(ctx, lrelu(ctx, conv_hr->forward(ctx, feat)));\n        return out;\n    }\n};\n\nstruct ESRGAN : public GGMLRunner {\n    RRDBNet rrdb_net;\n    int scale     = 4;\n    int tile_size = 128;  // avoid cuda OOM for 4gb VRAM\n\n    ESRGAN(ggml_backend_t backend, std::map<std::string, enum ggml_type>& tensor_types)\n        : GGMLRunner(backend) {\n        rrdb_net.init(params_ctx, tensor_types, \"\");\n    }\n\n    std::string get_desc() {\n        return \"esrgan\";\n    }\n\n    bool load_from_file(const std::string& file_path) {\n        LOG_INFO(\"loading esrgan from '%s'\", file_path.c_str());\n\n        alloc_params_buffer();\n        std::map<std::string, ggml_tensor*> esrgan_tensors;\n        rrdb_net.get_param_tensors(esrgan_tensors);\n\n        ModelLoader model_loader;\n        if (!model_loader.init_from_file(file_path)) {\n            LOG_ERROR(\"init esrgan model loader from file failed: '%s'\", file_path.c_str());\n            return false;\n        }\n\n        bool success = model_loader.load_tensors(esrgan_tensors, backend);\n\n        if (!success) {\n            LOG_ERROR(\"load esrgan tensors from model loader failed\");\n            return false;\n        }\n\n        LOG_INFO(\"esrgan model loaded\");\n        return success;\n    }\n\n    struct ggml_cgraph* build_graph(struct ggml_tensor* x) {\n        struct ggml_cgraph* gf  = ggml_new_graph(compute_ctx);\n        x                       = to_backend(x);\n        struct ggml_tensor* out = rrdb_net.forward(compute_ctx, x);\n        ggml_build_forward_expand(gf, out);\n        return gf;\n    }\n\n    void compute(const int n_threads,\n                 struct ggml_tensor* x,\n                 ggml_tensor** output,\n                 ggml_context* output_ctx = NULL) {\n        auto get_graph = [&]() -> struct ggml_cgraph* {\n            return build_graph(x);\n        };\n        GGMLRunner::compute(get_graph, n_threads, false, output, output_ctx);\n    }\n};\n\n#endif  // __ESRGAN_HPP__"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "face_detect.py",
          "type": "blob",
          "size": 3.3271484375,
          "content": "import os\r\nimport sys\r\n\r\nimport numpy as np\r\nimport torch\r\nfrom diffusers.utils import load_image\r\n# pip install insightface==0.7.3\r\nfrom insightface.app import FaceAnalysis\r\nfrom insightface.data import get_image as ins_get_image\r\nfrom safetensors.torch import save_file\r\n\r\n### \r\n# https://github.com/cubiq/ComfyUI_IPAdapter_plus/issues/165#issue-2055829543\r\n###\r\nclass FaceAnalysis2(FaceAnalysis):\r\n    # NOTE: allows setting det_size for each detection call.\r\n    # the model allows it but the wrapping code from insightface\r\n    # doesn't show it, and people end up loading duplicate models\r\n    # for different sizes where there is absolutely no need to\r\n    def get(self, img, max_num=0, det_size=(640, 640)):\r\n        if det_size is not None:\r\n            self.det_model.input_size = det_size\r\n\r\n        return super().get(img, max_num)\r\n\r\ndef analyze_faces(face_analysis: FaceAnalysis, img_data: np.ndarray, det_size=(640, 640)):\r\n    # NOTE: try detect faces, if no faces detected, lower det_size until it does\r\n    detection_sizes = [None] + [(size, size) for size in range(640, 256, -64)] + [(256, 256)]\r\n\r\n    for size in detection_sizes:\r\n        faces = face_analysis.get(img_data, det_size=size)\r\n        if len(faces) > 0:\r\n            return faces\r\n\r\n    return []\r\n\r\nif __name__ == \"__main__\":\r\n    #face_detector = FaceAnalysis2(providers=['CUDAExecutionProvider'], allowed_modules=['detection', 'recognition'])\r\n    face_detector = FaceAnalysis2(providers=['CPUExecutionProvider'], allowed_modules=['detection', 'recognition'])\r\n    face_detector.prepare(ctx_id=0, det_size=(640, 640))\r\n    #input_folder_name = './scarletthead_woman'\r\n    input_folder_name = sys.argv[1]\r\n    image_basename_list = os.listdir(input_folder_name)\r\n    image_path_list = sorted([os.path.join(input_folder_name, basename) for basename in image_basename_list])\r\n\r\n    input_id_images = []\r\n    for image_path in image_path_list:\r\n        input_id_images.append(load_image(image_path))\r\n    \r\n    id_embed_list = []\r\n    \r\n    for img in input_id_images:\r\n        img = np.array(img)\r\n        img = img[:, :, ::-1]\r\n        faces = analyze_faces(face_detector, img)\r\n        if len(faces) > 0:\r\n            id_embed_list.append(torch.from_numpy((faces[0]['embedding'])))\r\n    \r\n    if len(id_embed_list) == 0:\r\n        raise ValueError(f\"No face detected in input image pool\")\r\n    \r\n    id_embeds = torch.stack(id_embed_list)    \r\n    \r\n    # for r in id_embeds:\r\n    #     print(r)\r\n    # #torch.save(id_embeds, input_folder_name+'/id_embeds.pt');\r\n    # weights = dict()\r\n    # weights[\"id_embeds\"] = id_embeds\r\n    # save_file(weights, input_folder_name+'/id_embeds.safetensors')\r\n\r\n    binary_data = id_embeds.numpy().tobytes()\r\n    two = 4\r\n    zero = 0\r\n    one = 1\r\n    tensor_name = \"id_embeds\"\r\n# Write binary data to a file\r\n    with open(input_folder_name+'/id_embeds.bin', \"wb\") as f:\r\n        f.write(two.to_bytes(4, byteorder='little'))\r\n        f.write((len(tensor_name)).to_bytes(4, byteorder='little'))\r\n        f.write(zero.to_bytes(4, byteorder='little'))\r\n        f.write((id_embeds.shape[1]).to_bytes(4, byteorder='little'))\r\n        f.write((id_embeds.shape[0]).to_bytes(4, byteorder='little'))\r\n        f.write(one.to_bytes(4, byteorder='little'))\r\n        f.write(one.to_bytes(4, byteorder='little'))\r\n        f.write(tensor_name.encode('ascii'))\r\n        f.write(binary_data)\r\n\r\n    "
        },
        {
          "name": "flux.hpp",
          "type": "blob",
          "size": 51.2421875,
          "content": "#ifndef __FLUX_HPP__\n#define __FLUX_HPP__\n\n#include <vector>\n\n#include \"ggml_extend.hpp\"\n#include \"model.h\"\n\n#define FLUX_GRAPH_SIZE 10240\n\nnamespace Flux {\n\n    struct MLPEmbedder : public UnaryBlock {\n    public:\n        MLPEmbedder(int64_t in_dim, int64_t hidden_dim) {\n            blocks[\"in_layer\"]  = std::shared_ptr<GGMLBlock>(new Linear(in_dim, hidden_dim, true));\n            blocks[\"out_layer\"] = std::shared_ptr<GGMLBlock>(new Linear(hidden_dim, hidden_dim, true));\n        }\n\n        struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n            // x: [..., in_dim]\n            // return: [..., hidden_dim]\n            auto in_layer  = std::dynamic_pointer_cast<Linear>(blocks[\"in_layer\"]);\n            auto out_layer = std::dynamic_pointer_cast<Linear>(blocks[\"out_layer\"]);\n\n            x = in_layer->forward(ctx, x);\n            x = ggml_silu_inplace(ctx, x);\n            x = out_layer->forward(ctx, x);\n            return x;\n        }\n    };\n\n    class RMSNorm : public UnaryBlock {\n    protected:\n        int64_t hidden_size;\n        float eps;\n\n        void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix = \"\") {\n            ggml_type wtype = GGML_TYPE_F32;  //(tensor_types.find(prefix + \"scale\") != tensor_types.end()) ? tensor_types[prefix + \"scale\"] : GGML_TYPE_F32;\n            params[\"scale\"] = ggml_new_tensor_1d(ctx, wtype, hidden_size);\n        }\n\n    public:\n        RMSNorm(int64_t hidden_size,\n                float eps = 1e-06f)\n            : hidden_size(hidden_size),\n              eps(eps) {}\n\n        struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n            struct ggml_tensor* w = params[\"scale\"];\n            x                     = ggml_rms_norm(ctx, x, eps);\n            x                     = ggml_mul(ctx, x, w);\n            return x;\n        }\n    };\n\n    struct QKNorm : public GGMLBlock {\n    public:\n        QKNorm(int64_t dim) {\n            blocks[\"query_norm\"] = std::shared_ptr<GGMLBlock>(new RMSNorm(dim));\n            blocks[\"key_norm\"]   = std::shared_ptr<GGMLBlock>(new RMSNorm(dim));\n        }\n\n        struct ggml_tensor* query_norm(struct ggml_context* ctx, struct ggml_tensor* x) {\n            // x: [..., dim]\n            // return: [..., dim]\n            auto norm = std::dynamic_pointer_cast<RMSNorm>(blocks[\"query_norm\"]);\n\n            x = norm->forward(ctx, x);\n            return x;\n        }\n\n        struct ggml_tensor* key_norm(struct ggml_context* ctx, struct ggml_tensor* x) {\n            // x: [..., dim]\n            // return: [..., dim]\n            auto norm = std::dynamic_pointer_cast<RMSNorm>(blocks[\"key_norm\"]);\n\n            x = norm->forward(ctx, x);\n            return x;\n        }\n    };\n\n    __STATIC_INLINE__ struct ggml_tensor* apply_rope(struct ggml_context* ctx,\n                                                     struct ggml_tensor* x,\n                                                     struct ggml_tensor* pe) {\n        // x: [N, L, n_head, d_head]\n        // pe: [L, d_head/2, 2, 2]\n        int64_t d_head = x->ne[0];\n        int64_t n_head = x->ne[1];\n        int64_t L      = x->ne[2];\n        int64_t N      = x->ne[3];\n        x              = ggml_cont(ctx, ggml_permute(ctx, x, 0, 2, 1, 3));       // [N, n_head, L, d_head]\n        x              = ggml_reshape_4d(ctx, x, 2, d_head / 2, L, n_head * N);  // [N * n_head, L, d_head/2, 2]\n        x              = ggml_cont(ctx, ggml_permute(ctx, x, 3, 0, 1, 2));       // [2, N * n_head, L, d_head/2]\n\n        int64_t offset = x->nb[2] * x->ne[2];\n        auto x_0       = ggml_view_3d(ctx, x, x->ne[0], x->ne[1], x->ne[2], x->nb[1], x->nb[2], offset * 0);  // [N * n_head, L, d_head/2]\n        auto x_1       = ggml_view_3d(ctx, x, x->ne[0], x->ne[1], x->ne[2], x->nb[1], x->nb[2], offset * 1);  // [N * n_head, L, d_head/2]\n        x_0            = ggml_reshape_4d(ctx, x_0, 1, x_0->ne[0], x_0->ne[1], x_0->ne[2]);                    // [N * n_head, L, d_head/2, 1]\n        x_1            = ggml_reshape_4d(ctx, x_1, 1, x_1->ne[0], x_1->ne[1], x_1->ne[2]);                    // [N * n_head, L, d_head/2, 1]\n        auto temp_x    = ggml_new_tensor_4d(ctx, x_0->type, 2, x_0->ne[1], x_0->ne[2], x_0->ne[3]);\n        x_0            = ggml_repeat(ctx, x_0, temp_x);  // [N * n_head, L, d_head/2, 2]\n        x_1            = ggml_repeat(ctx, x_1, temp_x);  // [N * n_head, L, d_head/2, 2]\n\n        pe        = ggml_cont(ctx, ggml_permute(ctx, pe, 3, 0, 1, 2));  // [2, L, d_head/2, 2]\n        offset    = pe->nb[2] * pe->ne[2];\n        auto pe_0 = ggml_view_3d(ctx, pe, pe->ne[0], pe->ne[1], pe->ne[2], pe->nb[1], pe->nb[2], offset * 0);  // [L, d_head/2, 2]\n        auto pe_1 = ggml_view_3d(ctx, pe, pe->ne[0], pe->ne[1], pe->ne[2], pe->nb[1], pe->nb[2], offset * 1);  // [L, d_head/2, 2]\n\n        auto x_out = ggml_add_inplace(ctx, ggml_mul(ctx, x_0, pe_0), ggml_mul(ctx, x_1, pe_1));  // [N * n_head, L, d_head/2, 2]\n        x_out      = ggml_reshape_3d(ctx, x_out, d_head, L, n_head * N);                         // [N*n_head, L, d_head]\n        return x_out;\n    }\n\n    __STATIC_INLINE__ struct ggml_tensor* attention(struct ggml_context* ctx,\n                                                    struct ggml_tensor* q,\n                                                    struct ggml_tensor* k,\n                                                    struct ggml_tensor* v,\n                                                    struct ggml_tensor* pe,\n                                                    bool flash_attn) {\n        // q,k,v: [N, L, n_head, d_head]\n        // pe: [L, d_head/2, 2, 2]\n        // return: [N, L, n_head*d_head]\n        q = apply_rope(ctx, q, pe);  // [N*n_head, L, d_head]\n        k = apply_rope(ctx, k, pe);  // [N*n_head, L, d_head]\n\n        auto x = ggml_nn_attention_ext(ctx, q, k, v, v->ne[1], NULL, false, true, flash_attn);  // [N, L, n_head*d_head]\n        return x;\n    }\n\n    struct SelfAttention : public GGMLBlock {\n    public:\n        int64_t num_heads;\n        bool flash_attn;\n\n    public:\n        SelfAttention(int64_t dim,\n                      int64_t num_heads = 8,\n                      bool qkv_bias     = false,\n                      bool flash_attn   = false)\n            : num_heads(num_heads) {\n            int64_t head_dim = dim / num_heads;\n            blocks[\"qkv\"]    = std::shared_ptr<GGMLBlock>(new Linear(dim, dim * 3, qkv_bias));\n            blocks[\"norm\"]   = std::shared_ptr<GGMLBlock>(new QKNorm(head_dim));\n            blocks[\"proj\"]   = std::shared_ptr<GGMLBlock>(new Linear(dim, dim));\n        }\n\n        std::vector<struct ggml_tensor*> pre_attention(struct ggml_context* ctx, struct ggml_tensor* x) {\n            auto qkv_proj = std::dynamic_pointer_cast<Linear>(blocks[\"qkv\"]);\n            auto norm     = std::dynamic_pointer_cast<QKNorm>(blocks[\"norm\"]);\n\n            auto qkv         = qkv_proj->forward(ctx, x);\n            auto qkv_vec     = split_qkv(ctx, qkv);\n            int64_t head_dim = qkv_vec[0]->ne[0] / num_heads;\n            auto q           = ggml_reshape_4d(ctx, qkv_vec[0], head_dim, num_heads, qkv_vec[0]->ne[1], qkv_vec[0]->ne[2]);\n            auto k           = ggml_reshape_4d(ctx, qkv_vec[1], head_dim, num_heads, qkv_vec[1]->ne[1], qkv_vec[1]->ne[2]);\n            auto v           = ggml_reshape_4d(ctx, qkv_vec[2], head_dim, num_heads, qkv_vec[2]->ne[1], qkv_vec[2]->ne[2]);\n            q                = norm->query_norm(ctx, q);\n            k                = norm->key_norm(ctx, k);\n            return {q, k, v};\n        }\n\n        struct ggml_tensor* post_attention(struct ggml_context* ctx, struct ggml_tensor* x) {\n            auto proj = std::dynamic_pointer_cast<Linear>(blocks[\"proj\"]);\n\n            x = proj->forward(ctx, x);  // [N, n_token, dim]\n            return x;\n        }\n\n        struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x, struct ggml_tensor* pe) {\n            // x: [N, n_token, dim]\n            // pe: [n_token, d_head/2, 2, 2]\n            // return [N, n_token, dim]\n            auto qkv = pre_attention(ctx, x);                                   // q,k,v: [N, n_token, n_head, d_head]\n            x        = attention(ctx, qkv[0], qkv[1], qkv[2], pe, flash_attn);  // [N, n_token, dim]\n            x        = post_attention(ctx, x);                                  // [N, n_token, dim]\n            return x;\n        }\n    };\n\n    struct ModulationOut {\n        ggml_tensor* shift = NULL;\n        ggml_tensor* scale = NULL;\n        ggml_tensor* gate  = NULL;\n\n        ModulationOut(ggml_tensor* shift = NULL, ggml_tensor* scale = NULL, ggml_tensor* gate = NULL)\n            : shift(shift), scale(scale), gate(gate) {}\n    };\n\n    struct Modulation : public GGMLBlock {\n    public:\n        bool is_double;\n        int multiplier;\n\n    public:\n        Modulation(int64_t dim, bool is_double)\n            : is_double(is_double) {\n            multiplier    = is_double ? 6 : 3;\n            blocks[\"lin\"] = std::shared_ptr<GGMLBlock>(new Linear(dim, dim * multiplier));\n        }\n\n        std::vector<ModulationOut> forward(struct ggml_context* ctx, struct ggml_tensor* vec) {\n            // x: [N, dim]\n            // return: [ModulationOut, ModulationOut]\n            auto lin = std::dynamic_pointer_cast<Linear>(blocks[\"lin\"]);\n\n            auto out = ggml_silu(ctx, vec);\n            out      = lin->forward(ctx, out);  // [N, multiplier*dim]\n\n            auto m = ggml_reshape_3d(ctx, out, vec->ne[0], multiplier, vec->ne[1]);  // [N, multiplier, dim]\n            m      = ggml_cont(ctx, ggml_permute(ctx, m, 0, 2, 1, 3));               // [multiplier, N, dim]\n\n            int64_t offset = m->nb[1] * m->ne[1];\n            auto shift_0   = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 0);  // [N, dim]\n            auto scale_0   = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 1);  // [N, dim]\n            auto gate_0    = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 2);  // [N, dim]\n\n            if (is_double) {\n                auto shift_1 = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 3);  // [N, dim]\n                auto scale_1 = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 4);  // [N, dim]\n                auto gate_1  = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 5);  // [N, dim]\n                return {ModulationOut(shift_0, scale_0, gate_0), ModulationOut(shift_1, scale_1, gate_1)};\n            }\n\n            return {ModulationOut(shift_0, scale_0, gate_0), ModulationOut()};\n        }\n    };\n\n    __STATIC_INLINE__ struct ggml_tensor* modulate(struct ggml_context* ctx,\n                                                   struct ggml_tensor* x,\n                                                   struct ggml_tensor* shift,\n                                                   struct ggml_tensor* scale) {\n        // x: [N, L, C]\n        // scale: [N, C]\n        // shift: [N, C]\n        scale = ggml_reshape_3d(ctx, scale, scale->ne[0], 1, scale->ne[1]);  // [N, 1, C]\n        shift = ggml_reshape_3d(ctx, shift, shift->ne[0], 1, shift->ne[1]);  // [N, 1, C]\n        x     = ggml_add(ctx, x, ggml_mul(ctx, x, scale));\n        x     = ggml_add(ctx, x, shift);\n        return x;\n    }\n\n    struct DoubleStreamBlock : public GGMLBlock {\n        bool flash_attn;\n\n    public:\n        DoubleStreamBlock(int64_t hidden_size,\n                          int64_t num_heads,\n                          float mlp_ratio,\n                          bool qkv_bias   = false,\n                          bool flash_attn = false)\n            : flash_attn(flash_attn) {\n            int64_t mlp_hidden_dim = hidden_size * mlp_ratio;\n            blocks[\"img_mod\"]      = std::shared_ptr<GGMLBlock>(new Modulation(hidden_size, true));\n            blocks[\"img_norm1\"]    = std::shared_ptr<GGMLBlock>(new LayerNorm(hidden_size, 1e-6f, false));\n            blocks[\"img_attn\"]     = std::shared_ptr<GGMLBlock>(new SelfAttention(hidden_size, num_heads, qkv_bias, flash_attn));\n\n            blocks[\"img_norm2\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(hidden_size, 1e-6f, false));\n            blocks[\"img_mlp.0\"] = std::shared_ptr<GGMLBlock>(new Linear(hidden_size, mlp_hidden_dim));\n            // img_mlp.1 is nn.GELU(approximate=\"tanh\")\n            blocks[\"img_mlp.2\"] = std::shared_ptr<GGMLBlock>(new Linear(mlp_hidden_dim, hidden_size));\n\n            blocks[\"txt_mod\"]   = std::shared_ptr<GGMLBlock>(new Modulation(hidden_size, true));\n            blocks[\"txt_norm1\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(hidden_size, 1e-6f, false));\n            blocks[\"txt_attn\"]  = std::shared_ptr<GGMLBlock>(new SelfAttention(hidden_size, num_heads, qkv_bias, flash_attn));\n\n            blocks[\"txt_norm2\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(hidden_size, 1e-6f, false));\n            blocks[\"txt_mlp.0\"] = std::shared_ptr<GGMLBlock>(new Linear(hidden_size, mlp_hidden_dim));\n            // img_mlp.1 is nn.GELU(approximate=\"tanh\")\n            blocks[\"txt_mlp.2\"] = std::shared_ptr<GGMLBlock>(new Linear(mlp_hidden_dim, hidden_size));\n        }\n\n        std::pair<struct ggml_tensor*, struct ggml_tensor*> forward(struct ggml_context* ctx,\n                                                                    struct ggml_tensor* img,\n                                                                    struct ggml_tensor* txt,\n                                                                    struct ggml_tensor* vec,\n                                                                    struct ggml_tensor* pe) {\n            // img: [N, n_img_token, hidden_size]\n            // txt: [N, n_txt_token, hidden_size]\n            // pe: [n_img_token + n_txt_token, d_head/2, 2, 2]\n            // return: ([N, n_img_token, hidden_size], [N, n_txt_token, hidden_size])\n\n            auto img_mod   = std::dynamic_pointer_cast<Modulation>(blocks[\"img_mod\"]);\n            auto img_norm1 = std::dynamic_pointer_cast<LayerNorm>(blocks[\"img_norm1\"]);\n            auto img_attn  = std::dynamic_pointer_cast<SelfAttention>(blocks[\"img_attn\"]);\n\n            auto img_norm2 = std::dynamic_pointer_cast<LayerNorm>(blocks[\"img_norm2\"]);\n            auto img_mlp_0 = std::dynamic_pointer_cast<Linear>(blocks[\"img_mlp.0\"]);\n            auto img_mlp_2 = std::dynamic_pointer_cast<Linear>(blocks[\"img_mlp.2\"]);\n\n            auto txt_mod   = std::dynamic_pointer_cast<Modulation>(blocks[\"txt_mod\"]);\n            auto txt_norm1 = std::dynamic_pointer_cast<LayerNorm>(blocks[\"txt_norm1\"]);\n            auto txt_attn  = std::dynamic_pointer_cast<SelfAttention>(blocks[\"txt_attn\"]);\n\n            auto txt_norm2 = std::dynamic_pointer_cast<LayerNorm>(blocks[\"txt_norm2\"]);\n            auto txt_mlp_0 = std::dynamic_pointer_cast<Linear>(blocks[\"txt_mlp.0\"]);\n            auto txt_mlp_2 = std::dynamic_pointer_cast<Linear>(blocks[\"txt_mlp.2\"]);\n\n            auto img_mods          = img_mod->forward(ctx, vec);\n            ModulationOut img_mod1 = img_mods[0];\n            ModulationOut img_mod2 = img_mods[1];\n            auto txt_mods          = txt_mod->forward(ctx, vec);\n            ModulationOut txt_mod1 = txt_mods[0];\n            ModulationOut txt_mod2 = txt_mods[1];\n\n            // prepare image for attention\n            auto img_modulated = img_norm1->forward(ctx, img);\n            img_modulated      = Flux::modulate(ctx, img_modulated, img_mod1.shift, img_mod1.scale);\n            auto img_qkv       = img_attn->pre_attention(ctx, img_modulated);  // q,k,v: [N, n_img_token, n_head, d_head]\n            auto img_q         = img_qkv[0];\n            auto img_k         = img_qkv[1];\n            auto img_v         = img_qkv[2];\n\n            // prepare txt for attention\n            auto txt_modulated = txt_norm1->forward(ctx, txt);\n            txt_modulated      = Flux::modulate(ctx, txt_modulated, txt_mod1.shift, txt_mod1.scale);\n            auto txt_qkv       = txt_attn->pre_attention(ctx, txt_modulated);  // q,k,v: [N, n_txt_token, n_head, d_head]\n            auto txt_q         = txt_qkv[0];\n            auto txt_k         = txt_qkv[1];\n            auto txt_v         = txt_qkv[2];\n\n            // run actual attention\n            auto q = ggml_concat(ctx, txt_q, img_q, 2);  // [N, n_txt_token + n_img_token, n_head, d_head]\n            auto k = ggml_concat(ctx, txt_k, img_k, 2);  // [N, n_txt_token + n_img_token, n_head, d_head]\n            auto v = ggml_concat(ctx, txt_v, img_v, 2);  // [N, n_txt_token + n_img_token, n_head, d_head]\n\n            auto attn         = attention(ctx, q, k, v, pe, flash_attn);              // [N, n_txt_token + n_img_token, n_head*d_head]\n            attn              = ggml_cont(ctx, ggml_permute(ctx, attn, 0, 2, 1, 3));  // [n_txt_token + n_img_token, N, hidden_size]\n            auto txt_attn_out = ggml_view_3d(ctx,\n                                             attn,\n                                             attn->ne[0],\n                                             attn->ne[1],\n                                             txt->ne[1],\n                                             attn->nb[1],\n                                             attn->nb[2],\n                                             0);                                              // [n_txt_token, N, hidden_size]\n            txt_attn_out      = ggml_cont(ctx, ggml_permute(ctx, txt_attn_out, 0, 2, 1, 3));  // [N, n_txt_token, hidden_size]\n            auto img_attn_out = ggml_view_3d(ctx,\n                                             attn,\n                                             attn->ne[0],\n                                             attn->ne[1],\n                                             img->ne[1],\n                                             attn->nb[1],\n                                             attn->nb[2],\n                                             attn->nb[2] * txt->ne[1]);                       // [n_img_token, N, hidden_size]\n            img_attn_out      = ggml_cont(ctx, ggml_permute(ctx, img_attn_out, 0, 2, 1, 3));  // [N, n_img_token, hidden_size]\n\n            // calculate the img bloks\n            img = ggml_add(ctx, img, ggml_mul(ctx, img_attn->post_attention(ctx, img_attn_out), img_mod1.gate));\n\n            auto img_mlp_out = img_mlp_0->forward(ctx, Flux::modulate(ctx, img_norm2->forward(ctx, img), img_mod2.shift, img_mod2.scale));\n            img_mlp_out      = ggml_gelu_inplace(ctx, img_mlp_out);\n            img_mlp_out      = img_mlp_2->forward(ctx, img_mlp_out);\n\n            img = ggml_add(ctx, img, ggml_mul(ctx, img_mlp_out, img_mod2.gate));\n\n            // calculate the txt bloks\n            txt = ggml_add(ctx, txt, ggml_mul(ctx, txt_attn->post_attention(ctx, txt_attn_out), txt_mod1.gate));\n\n            auto txt_mlp_out = txt_mlp_0->forward(ctx, Flux::modulate(ctx, txt_norm2->forward(ctx, txt), txt_mod2.shift, txt_mod2.scale));\n            txt_mlp_out      = ggml_gelu_inplace(ctx, txt_mlp_out);\n            txt_mlp_out      = txt_mlp_2->forward(ctx, txt_mlp_out);\n\n            txt = ggml_add(ctx, txt, ggml_mul(ctx, txt_mlp_out, txt_mod2.gate));\n\n            return {img, txt};\n        }\n    };\n\n    struct SingleStreamBlock : public GGMLBlock {\n    public:\n        int64_t num_heads;\n        int64_t hidden_size;\n        int64_t mlp_hidden_dim;\n        bool flash_attn;\n\n    public:\n        SingleStreamBlock(int64_t hidden_size,\n                          int64_t num_heads,\n                          float mlp_ratio = 4.0f,\n                          float qk_scale  = 0.f,\n                          bool flash_attn = false)\n            : hidden_size(hidden_size), num_heads(num_heads), flash_attn(flash_attn) {\n            int64_t head_dim = hidden_size / num_heads;\n            float scale      = qk_scale;\n            if (scale <= 0.f) {\n                scale = 1 / sqrt((float)head_dim);\n            }\n            mlp_hidden_dim = hidden_size * mlp_ratio;\n\n            blocks[\"linear1\"]  = std::shared_ptr<GGMLBlock>(new Linear(hidden_size, hidden_size * 3 + mlp_hidden_dim));\n            blocks[\"linear2\"]  = std::shared_ptr<GGMLBlock>(new Linear(hidden_size + mlp_hidden_dim, hidden_size));\n            blocks[\"norm\"]     = std::shared_ptr<GGMLBlock>(new QKNorm(head_dim));\n            blocks[\"pre_norm\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(hidden_size, 1e-6f, false));\n            // mlp_act is nn.GELU(approximate=\"tanh\")\n            blocks[\"modulation\"] = std::shared_ptr<GGMLBlock>(new Modulation(hidden_size, false));\n        }\n\n        struct ggml_tensor* forward(struct ggml_context* ctx,\n                                    struct ggml_tensor* x,\n                                    struct ggml_tensor* vec,\n                                    struct ggml_tensor* pe) {\n            // x: [N, n_token, hidden_size]\n            // pe: [n_token, d_head/2, 2, 2]\n            // return: [N, n_token, hidden_size]\n\n            auto linear1    = std::dynamic_pointer_cast<Linear>(blocks[\"linear1\"]);\n            auto linear2    = std::dynamic_pointer_cast<Linear>(blocks[\"linear2\"]);\n            auto norm       = std::dynamic_pointer_cast<QKNorm>(blocks[\"norm\"]);\n            auto pre_norm   = std::dynamic_pointer_cast<LayerNorm>(blocks[\"pre_norm\"]);\n            auto modulation = std::dynamic_pointer_cast<Modulation>(blocks[\"modulation\"]);\n\n            auto mods         = modulation->forward(ctx, vec);\n            ModulationOut mod = mods[0];\n\n            auto x_mod   = Flux::modulate(ctx, pre_norm->forward(ctx, x), mod.shift, mod.scale);\n            auto qkv_mlp = linear1->forward(ctx, x_mod);                            // [N, n_token, hidden_size * 3 + mlp_hidden_dim]\n            qkv_mlp      = ggml_cont(ctx, ggml_permute(ctx, qkv_mlp, 2, 0, 1, 3));  // [hidden_size * 3 + mlp_hidden_dim, N, n_token]\n\n            auto qkv = ggml_view_3d(ctx,\n                                    qkv_mlp,\n                                    qkv_mlp->ne[0],\n                                    qkv_mlp->ne[1],\n                                    hidden_size * 3,\n                                    qkv_mlp->nb[1],\n                                    qkv_mlp->nb[2],\n                                    0);                                     // [hidden_size * 3 , N, n_token]\n            qkv      = ggml_cont(ctx, ggml_permute(ctx, qkv, 1, 2, 0, 3));  // [N, n_token, hidden_size * 3]\n            auto mlp = ggml_view_3d(ctx,\n                                    qkv_mlp,\n                                    qkv_mlp->ne[0],\n                                    qkv_mlp->ne[1],\n                                    mlp_hidden_dim,\n                                    qkv_mlp->nb[1],\n                                    qkv_mlp->nb[2],\n                                    qkv_mlp->nb[2] * hidden_size * 3);      // [mlp_hidden_dim , N, n_token]\n            mlp      = ggml_cont(ctx, ggml_permute(ctx, mlp, 1, 2, 0, 3));  // [N, n_token, mlp_hidden_dim]\n\n            auto qkv_vec     = split_qkv(ctx, qkv);  // q,k,v: [N, n_token, hidden_size]\n            int64_t head_dim = hidden_size / num_heads;\n            auto q           = ggml_reshape_4d(ctx, qkv_vec[0], head_dim, num_heads, qkv_vec[0]->ne[1], qkv_vec[0]->ne[2]);  // [N, n_token, n_head, d_head]\n            auto k           = ggml_reshape_4d(ctx, qkv_vec[1], head_dim, num_heads, qkv_vec[1]->ne[1], qkv_vec[1]->ne[2]);  // [N, n_token, n_head, d_head]\n            auto v           = ggml_reshape_4d(ctx, qkv_vec[2], head_dim, num_heads, qkv_vec[2]->ne[1], qkv_vec[2]->ne[2]);  // [N, n_token, n_head, d_head]\n            q                = norm->query_norm(ctx, q);\n            k                = norm->key_norm(ctx, k);\n            auto attn        = attention(ctx, q, k, v, pe, flash_attn);  // [N, n_token, hidden_size]\n\n            auto attn_mlp = ggml_concat(ctx, attn, ggml_gelu_inplace(ctx, mlp), 0);  // [N, n_token, hidden_size + mlp_hidden_dim]\n            auto output   = linear2->forward(ctx, attn_mlp);                         // [N, n_token, hidden_size]\n\n            output = ggml_add(ctx, x, ggml_mul(ctx, output, mod.gate));\n            return output;\n        }\n    };\n\n    struct LastLayer : public GGMLBlock {\n    public:\n        LastLayer(int64_t hidden_size,\n                  int64_t patch_size,\n                  int64_t out_channels) {\n            blocks[\"norm_final\"]         = std::shared_ptr<GGMLBlock>(new LayerNorm(hidden_size, 1e-06f, false));\n            blocks[\"linear\"]             = std::shared_ptr<GGMLBlock>(new Linear(hidden_size, patch_size * patch_size * out_channels));\n            blocks[\"adaLN_modulation.1\"] = std::shared_ptr<GGMLBlock>(new Linear(hidden_size, 2 * hidden_size));\n        }\n\n        struct ggml_tensor* forward(struct ggml_context* ctx,\n                                    struct ggml_tensor* x,\n                                    struct ggml_tensor* c) {\n            // x: [N, n_token, hidden_size]\n            // c: [N, hidden_size]\n            // return: [N, n_token, patch_size * patch_size * out_channels]\n            auto norm_final         = std::dynamic_pointer_cast<LayerNorm>(blocks[\"norm_final\"]);\n            auto linear             = std::dynamic_pointer_cast<Linear>(blocks[\"linear\"]);\n            auto adaLN_modulation_1 = std::dynamic_pointer_cast<Linear>(blocks[\"adaLN_modulation.1\"]);\n\n            auto m = adaLN_modulation_1->forward(ctx, ggml_silu(ctx, c));  // [N, 2 * hidden_size]\n            m      = ggml_reshape_3d(ctx, m, c->ne[0], 2, c->ne[1]);       // [N, 2, hidden_size]\n            m      = ggml_cont(ctx, ggml_permute(ctx, m, 0, 2, 1, 3));     // [2, N, hidden_size]\n\n            int64_t offset = m->nb[1] * m->ne[1];\n            auto shift     = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 0);  // [N, hidden_size]\n            auto scale     = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 1);  // [N, hidden_size]\n\n            x = Flux::modulate(ctx, norm_final->forward(ctx, x), shift, scale);\n            x = linear->forward(ctx, x);\n\n            return x;\n        }\n    };\n\n    struct FluxParams {\n        int64_t in_channels         = 64;\n        int64_t out_channels        = 64;\n        int64_t vec_in_dim          = 768;\n        int64_t context_in_dim      = 4096;\n        int64_t hidden_size         = 3072;\n        float mlp_ratio             = 4.0f;\n        int64_t num_heads           = 24;\n        int64_t depth               = 19;\n        int64_t depth_single_blocks = 38;\n        std::vector<int> axes_dim   = {16, 56, 56};\n        int64_t axes_dim_sum        = 128;\n        int theta                   = 10000;\n        bool qkv_bias               = true;\n        bool guidance_embed         = true;\n        bool flash_attn             = true;\n    };\n\n    struct Flux : public GGMLBlock {\n    public:\n        std::vector<float> linspace(float start, float end, int num) {\n            std::vector<float> result(num);\n            float step = (end - start) / (num - 1);\n            for (int i = 0; i < num; ++i) {\n                result[i] = start + i * step;\n            }\n            return result;\n        }\n\n        std::vector<std::vector<float>> transpose(const std::vector<std::vector<float>>& mat) {\n            int rows = mat.size();\n            int cols = mat[0].size();\n            std::vector<std::vector<float>> transposed(cols, std::vector<float>(rows));\n            for (int i = 0; i < rows; ++i) {\n                for (int j = 0; j < cols; ++j) {\n                    transposed[j][i] = mat[i][j];\n                }\n            }\n            return transposed;\n        }\n\n        std::vector<float> flatten(const std::vector<std::vector<float>>& vec) {\n            std::vector<float> flat_vec;\n            for (const auto& sub_vec : vec) {\n                flat_vec.insert(flat_vec.end(), sub_vec.begin(), sub_vec.end());\n            }\n            return flat_vec;\n        }\n\n        std::vector<std::vector<float>> rope(const std::vector<float>& pos, int dim, int theta) {\n            assert(dim % 2 == 0);\n            int half_dim = dim / 2;\n\n            std::vector<float> scale = linspace(0, (dim * 1.0f - 2) / dim, half_dim);\n\n            std::vector<float> omega(half_dim);\n            for (int i = 0; i < half_dim; ++i) {\n                omega[i] = 1.0 / std::pow(theta, scale[i]);\n            }\n\n            int pos_size = pos.size();\n            std::vector<std::vector<float>> out(pos_size, std::vector<float>(half_dim));\n            for (int i = 0; i < pos_size; ++i) {\n                for (int j = 0; j < half_dim; ++j) {\n                    out[i][j] = pos[i] * omega[j];\n                }\n            }\n\n            std::vector<std::vector<float>> result(pos_size, std::vector<float>(half_dim * 4));\n            for (int i = 0; i < pos_size; ++i) {\n                for (int j = 0; j < half_dim; ++j) {\n                    result[i][4 * j]     = std::cos(out[i][j]);\n                    result[i][4 * j + 1] = -std::sin(out[i][j]);\n                    result[i][4 * j + 2] = std::sin(out[i][j]);\n                    result[i][4 * j + 3] = std::cos(out[i][j]);\n                }\n            }\n\n            return result;\n        }\n\n        // Generate IDs for image patches and text\n        std::vector<std::vector<float>> gen_ids(int h, int w, int patch_size, int bs, int context_len) {\n            int h_len = (h + (patch_size / 2)) / patch_size;\n            int w_len = (w + (patch_size / 2)) / patch_size;\n\n            std::vector<std::vector<float>> img_ids(h_len * w_len, std::vector<float>(3, 0.0));\n\n            std::vector<float> row_ids = linspace(0, h_len - 1, h_len);\n            std::vector<float> col_ids = linspace(0, w_len - 1, w_len);\n\n            for (int i = 0; i < h_len; ++i) {\n                for (int j = 0; j < w_len; ++j) {\n                    img_ids[i * w_len + j][1] = row_ids[i];\n                    img_ids[i * w_len + j][2] = col_ids[j];\n                }\n            }\n\n            std::vector<std::vector<float>> img_ids_repeated(bs * img_ids.size(), std::vector<float>(3));\n            for (int i = 0; i < bs; ++i) {\n                for (int j = 0; j < img_ids.size(); ++j) {\n                    img_ids_repeated[i * img_ids.size() + j] = img_ids[j];\n                }\n            }\n\n            std::vector<std::vector<float>> txt_ids(bs * context_len, std::vector<float>(3, 0.0));\n            std::vector<std::vector<float>> ids(bs * (context_len + img_ids.size()), std::vector<float>(3));\n            for (int i = 0; i < bs; ++i) {\n                for (int j = 0; j < context_len; ++j) {\n                    ids[i * (context_len + img_ids.size()) + j] = txt_ids[j];\n                }\n                for (int j = 0; j < img_ids.size(); ++j) {\n                    ids[i * (context_len + img_ids.size()) + context_len + j] = img_ids_repeated[i * img_ids.size() + j];\n                }\n            }\n\n            return ids;\n        }\n\n        // Generate positional embeddings\n        std::vector<float> gen_pe(int h, int w, int patch_size, int bs, int context_len, int theta, const std::vector<int>& axes_dim) {\n            std::vector<std::vector<float>> ids       = gen_ids(h, w, patch_size, bs, context_len);\n            std::vector<std::vector<float>> trans_ids = transpose(ids);\n            size_t pos_len                            = ids.size();\n            int num_axes                              = axes_dim.size();\n            for (int i = 0; i < pos_len; i++) {\n                // std::cout << trans_ids[0][i] << \" \" << trans_ids[1][i] << \" \" << trans_ids[2][i] << std::endl;\n            }\n\n            int emb_dim = 0;\n            for (int d : axes_dim)\n                emb_dim += d / 2;\n\n            std::vector<std::vector<float>> emb(bs * pos_len, std::vector<float>(emb_dim * 2 * 2, 0.0));\n            int offset = 0;\n            for (int i = 0; i < num_axes; ++i) {\n                std::vector<std::vector<float>> rope_emb = rope(trans_ids[i], axes_dim[i], theta);  // [bs*pos_len, axes_dim[i]/2 * 2 * 2]\n                for (int b = 0; b < bs; ++b) {\n                    for (int j = 0; j < pos_len; ++j) {\n                        for (int k = 0; k < rope_emb[0].size(); ++k) {\n                            emb[b * pos_len + j][offset + k] = rope_emb[j][k];\n                        }\n                    }\n                }\n                offset += rope_emb[0].size();\n            }\n\n            return flatten(emb);\n        }\n\n    public:\n        FluxParams params;\n        Flux() {}\n        Flux(FluxParams params)\n            : params(params) {\n            int64_t pe_dim = params.hidden_size / params.num_heads;\n\n            blocks[\"img_in\"]    = std::shared_ptr<GGMLBlock>(new Linear(params.in_channels, params.hidden_size, true));\n            blocks[\"time_in\"]   = std::shared_ptr<GGMLBlock>(new MLPEmbedder(256, params.hidden_size));\n            blocks[\"vector_in\"] = std::shared_ptr<GGMLBlock>(new MLPEmbedder(params.vec_in_dim, params.hidden_size));\n            if (params.guidance_embed) {\n                blocks[\"guidance_in\"] = std::shared_ptr<GGMLBlock>(new MLPEmbedder(256, params.hidden_size));\n            }\n            blocks[\"txt_in\"] = std::shared_ptr<GGMLBlock>(new Linear(params.context_in_dim, params.hidden_size, true));\n\n            for (int i = 0; i < params.depth; i++) {\n                blocks[\"double_blocks.\" + std::to_string(i)] = std::shared_ptr<GGMLBlock>(new DoubleStreamBlock(params.hidden_size,\n                                                                                                                params.num_heads,\n                                                                                                                params.mlp_ratio,\n                                                                                                                params.qkv_bias,\n                                                                                                                params.flash_attn));\n            }\n\n            for (int i = 0; i < params.depth_single_blocks; i++) {\n                blocks[\"single_blocks.\" + std::to_string(i)] = std::shared_ptr<GGMLBlock>(new SingleStreamBlock(params.hidden_size,\n                                                                                                                params.num_heads,\n                                                                                                                params.mlp_ratio,\n                                                                                                                0.f,\n                                                                                                                params.flash_attn));\n            }\n\n            blocks[\"final_layer\"] = std::shared_ptr<GGMLBlock>(new LastLayer(params.hidden_size, 1, params.out_channels));\n        }\n\n        struct ggml_tensor* patchify(struct ggml_context* ctx,\n                                     struct ggml_tensor* x,\n                                     int64_t patch_size) {\n            // x: [N, C, H, W]\n            // return: [N, h*w, C * patch_size * patch_size]\n            int64_t N = x->ne[3];\n            int64_t C = x->ne[2];\n            int64_t H = x->ne[1];\n            int64_t W = x->ne[0];\n            int64_t p = patch_size;\n            int64_t h = H / patch_size;\n            int64_t w = W / patch_size;\n\n            GGML_ASSERT(h * p == H && w * p == W);\n\n            x = ggml_reshape_4d(ctx, x, p, w, p, h * C * N);       // [N*C*h, p, w, p]\n            x = ggml_cont(ctx, ggml_permute(ctx, x, 0, 2, 1, 3));  // [N*C*h, w, p, p]\n            x = ggml_reshape_4d(ctx, x, p * p, w * h, C, N);       // [N, C, h*w, p*p]\n            x = ggml_cont(ctx, ggml_permute(ctx, x, 0, 2, 1, 3));  // [N, h*w, C, p*p]\n            x = ggml_reshape_3d(ctx, x, p * p * C, w * h, N);      // [N, h*w, C*p*p]\n            return x;\n        }\n\n        struct ggml_tensor* unpatchify(struct ggml_context* ctx,\n                                       struct ggml_tensor* x,\n                                       int64_t h,\n                                       int64_t w,\n                                       int64_t patch_size) {\n            // x: [N, h*w, C*patch_size*patch_size]\n            // return: [N, C, H, W]\n            int64_t N = x->ne[2];\n            int64_t C = x->ne[0] / patch_size / patch_size;\n            int64_t H = h * patch_size;\n            int64_t W = w * patch_size;\n            int64_t p = patch_size;\n\n            GGML_ASSERT(C * p * p == x->ne[0]);\n\n            x = ggml_reshape_4d(ctx, x, p * p, C, w * h, N);       // [N, h*w, C, p*p]\n            x = ggml_cont(ctx, ggml_permute(ctx, x, 0, 2, 1, 3));  // [N, C, h*w, p*p]\n            x = ggml_reshape_4d(ctx, x, p, p, w, h * C * N);       // [N*C*h, w, p, p]\n            x = ggml_cont(ctx, ggml_permute(ctx, x, 0, 2, 1, 3));  // [N*C*h, p, w, p]\n            x = ggml_reshape_4d(ctx, x, W, H, C, N);               // [N, C, h*p, w*p]\n\n            return x;\n        }\n\n        struct ggml_tensor* forward_orig(struct ggml_context* ctx,\n                                         struct ggml_tensor* img,\n                                         struct ggml_tensor* txt,\n                                         struct ggml_tensor* timesteps,\n                                         struct ggml_tensor* y,\n                                         struct ggml_tensor* guidance,\n                                         struct ggml_tensor* pe,\n                                         std::vector<int> skip_layers = std::vector<int>()) {\n            auto img_in      = std::dynamic_pointer_cast<Linear>(blocks[\"img_in\"]);\n            auto time_in     = std::dynamic_pointer_cast<MLPEmbedder>(blocks[\"time_in\"]);\n            auto vector_in   = std::dynamic_pointer_cast<MLPEmbedder>(blocks[\"vector_in\"]);\n            auto txt_in      = std::dynamic_pointer_cast<Linear>(blocks[\"txt_in\"]);\n            auto final_layer = std::dynamic_pointer_cast<LastLayer>(blocks[\"final_layer\"]);\n\n            img      = img_in->forward(ctx, img);\n            auto vec = time_in->forward(ctx, ggml_nn_timestep_embedding(ctx, timesteps, 256, 10000, 1000.f));\n\n            if (params.guidance_embed) {\n                GGML_ASSERT(guidance != NULL);\n                auto guidance_in = std::dynamic_pointer_cast<MLPEmbedder>(blocks[\"guidance_in\"]);\n                // bf16 and fp16 result is different\n                auto g_in = ggml_nn_timestep_embedding(ctx, guidance, 256, 10000, 1000.f);\n                vec       = ggml_add(ctx, vec, guidance_in->forward(ctx, g_in));\n            }\n\n            vec = ggml_add(ctx, vec, vector_in->forward(ctx, y));\n            txt = txt_in->forward(ctx, txt);\n\n            for (int i = 0; i < params.depth; i++) {\n                if (skip_layers.size() > 0 && std::find(skip_layers.begin(), skip_layers.end(), i) != skip_layers.end()) {\n                    continue;\n                }\n\n                auto block = std::dynamic_pointer_cast<DoubleStreamBlock>(blocks[\"double_blocks.\" + std::to_string(i)]);\n\n                auto img_txt = block->forward(ctx, img, txt, vec, pe);\n                img          = img_txt.first;   // [N, n_img_token, hidden_size]\n                txt          = img_txt.second;  // [N, n_txt_token, hidden_size]\n            }\n\n            auto txt_img = ggml_concat(ctx, txt, img, 1);  // [N, n_txt_token + n_img_token, hidden_size]\n            for (int i = 0; i < params.depth_single_blocks; i++) {\n                if (skip_layers.size() > 0 && std::find(skip_layers.begin(), skip_layers.end(), i + params.depth) != skip_layers.end()) {\n                    continue;\n                }\n                auto block = std::dynamic_pointer_cast<SingleStreamBlock>(blocks[\"single_blocks.\" + std::to_string(i)]);\n\n                txt_img = block->forward(ctx, txt_img, vec, pe);\n            }\n\n            txt_img = ggml_cont(ctx, ggml_permute(ctx, txt_img, 0, 2, 1, 3));  // [n_txt_token + n_img_token, N, hidden_size]\n            img     = ggml_view_3d(ctx,\n                                   txt_img,\n                                   txt_img->ne[0],\n                                   txt_img->ne[1],\n                                   img->ne[1],\n                                   txt_img->nb[1],\n                                   txt_img->nb[2],\n                                   txt_img->nb[2] * txt->ne[1]);           // [n_img_token, N, hidden_size]\n            img     = ggml_cont(ctx, ggml_permute(ctx, img, 0, 2, 1, 3));  // [N, n_img_token, hidden_size]\n\n            img = final_layer->forward(ctx, img, vec);  // (N, T, patch_size ** 2 * out_channels)\n\n            return img;\n        }\n\n        struct ggml_tensor* forward(struct ggml_context* ctx,\n                                    struct ggml_tensor* x,\n                                    struct ggml_tensor* timestep,\n                                    struct ggml_tensor* context,\n                                    struct ggml_tensor* c_concat,\n                                    struct ggml_tensor* y,\n                                    struct ggml_tensor* guidance,\n                                    struct ggml_tensor* pe,\n                                    std::vector<int> skip_layers = std::vector<int>()) {\n            // Forward pass of DiT.\n            // x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)\n            // timestep: (N,) tensor of diffusion timesteps\n            // context: (N, L, D)\n            // c_concat: NULL, or for (N,C+M, H, W) for Fill\n            // y: (N, adm_in_channels) tensor of class labels\n            // guidance: (N,)\n            // pe: (L, d_head/2, 2, 2)\n            // return: (N, C, H, W)\n\n            GGML_ASSERT(x->ne[3] == 1);\n\n            int64_t W          = x->ne[0];\n            int64_t H          = x->ne[1];\n            int64_t C          = x->ne[2];\n            int64_t patch_size = 2;\n            int pad_h          = (patch_size - H % patch_size) % patch_size;\n            int pad_w          = (patch_size - W % patch_size) % patch_size;\n            x                  = ggml_pad(ctx, x, pad_w, pad_h, 0, 0);  // [N, C, H + pad_h, W + pad_w]\n\n            // img = rearrange(x, \"b c (h ph) (w pw) -> b (h w) (c ph pw)\", ph=patch_size, pw=patch_size)\n            auto img = patchify(ctx, x, patch_size);  // [N, h*w, C * patch_size * patch_size]\n\n            if (c_concat != NULL) {\n                ggml_tensor* masked = ggml_view_4d(ctx, c_concat, c_concat->ne[0], c_concat->ne[1], C, 1, c_concat->nb[1], c_concat->nb[2], c_concat->nb[3], 0);\n                ggml_tensor* mask   = ggml_view_4d(ctx, c_concat, c_concat->ne[0], c_concat->ne[1], 8 * 8, 1, c_concat->nb[1], c_concat->nb[2], c_concat->nb[3], c_concat->nb[2] * C);\n\n                masked = ggml_pad(ctx, masked, pad_w, pad_h, 0, 0);\n                mask   = ggml_pad(ctx, mask, pad_w, pad_h, 0, 0);\n\n                masked = patchify(ctx, masked, patch_size);\n                mask   = patchify(ctx, mask, patch_size);\n\n                img = ggml_concat(ctx, img, ggml_concat(ctx, masked, mask, 0), 0);\n            }\n\n            auto out = forward_orig(ctx, img, context, timestep, y, guidance, pe, skip_layers);  // [N, h*w, C * patch_size * patch_size]\n\n            // rearrange(out, \"b (h w) (c ph pw) -> b c (h ph) (w pw)\", h=h_len, w=w_len, ph=2, pw=2)\n            out = unpatchify(ctx, out, (H + pad_h) / patch_size, (W + pad_w) / patch_size, patch_size);  // [N, C, H + pad_h, W + pad_w]\n\n            return out;\n        }\n    };\n\n    struct FluxRunner : public GGMLRunner {\n        static std::map<std::string, enum ggml_type> empty_tensor_types;\n\n    public:\n        FluxParams flux_params;\n        Flux flux;\n        std::vector<float> pe_vec;  // for cache\n\n        FluxRunner(ggml_backend_t backend,\n                   std::map<std::string, enum ggml_type>& tensor_types = empty_tensor_types,\n                   const std::string prefix                            = \"\",\n                   SDVersion version                                   = VERSION_FLUX,\n                   bool flash_attn                                     = false)\n            : GGMLRunner(backend) {\n            flux_params.flash_attn          = flash_attn;\n            flux_params.guidance_embed      = false;\n            flux_params.depth               = 0;\n            flux_params.depth_single_blocks = 0;\n            if (version == VERSION_FLUX_FILL) {\n                flux_params.in_channels = 384;\n            }\n            for (auto pair : tensor_types) {\n                std::string tensor_name = pair.first;\n                if (tensor_name.find(\"model.diffusion_model.\") == std::string::npos)\n                    continue;\n                if (tensor_name.find(\"guidance_in.in_layer.weight\") != std::string::npos) {\n                    // not schnell\n                    flux_params.guidance_embed = true;\n                }\n                size_t db = tensor_name.find(\"double_blocks.\");\n                if (db != std::string::npos) {\n                    tensor_name     = tensor_name.substr(db);  // remove prefix\n                    int block_depth = atoi(tensor_name.substr(14, tensor_name.find(\".\", 14)).c_str());\n                    if (block_depth + 1 > flux_params.depth) {\n                        flux_params.depth = block_depth + 1;\n                    }\n                }\n                size_t sb = tensor_name.find(\"single_blocks.\");\n                if (sb != std::string::npos) {\n                    tensor_name     = tensor_name.substr(sb);  // remove prefix\n                    int block_depth = atoi(tensor_name.substr(14, tensor_name.find(\".\", 14)).c_str());\n                    if (block_depth + 1 > flux_params.depth_single_blocks) {\n                        flux_params.depth_single_blocks = block_depth + 1;\n                    }\n                }\n            }\n\n            LOG_INFO(\"Flux blocks: %d double, %d single\", flux_params.depth, flux_params.depth_single_blocks);\n            if (!flux_params.guidance_embed) {\n                LOG_INFO(\"Flux guidance is disabled (Schnell mode)\");\n            }\n\n            flux = Flux(flux_params);\n            flux.init(params_ctx, tensor_types, prefix);\n        }\n\n        std::string get_desc() {\n            return \"flux\";\n        }\n\n        void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors, const std::string prefix) {\n            flux.get_param_tensors(tensors, prefix);\n        }\n\n        struct ggml_cgraph* build_graph(struct ggml_tensor* x,\n                                        struct ggml_tensor* timesteps,\n                                        struct ggml_tensor* context,\n                                        struct ggml_tensor* c_concat,\n                                        struct ggml_tensor* y,\n                                        struct ggml_tensor* guidance,\n                                        std::vector<int> skip_layers = std::vector<int>()) {\n            GGML_ASSERT(x->ne[3] == 1);\n            struct ggml_cgraph* gf = ggml_new_graph_custom(compute_ctx, FLUX_GRAPH_SIZE, false);\n\n            x       = to_backend(x);\n            context = to_backend(context);\n            if (c_concat != NULL) {\n                c_concat = to_backend(c_concat);\n            }\n            y         = to_backend(y);\n            timesteps = to_backend(timesteps);\n            if (flux_params.guidance_embed) {\n                guidance = to_backend(guidance);\n            }\n\n            pe_vec      = flux.gen_pe(x->ne[1], x->ne[0], 2, x->ne[3], context->ne[1], flux_params.theta, flux_params.axes_dim);\n            int pos_len = pe_vec.size() / flux_params.axes_dim_sum / 2;\n            // LOG_DEBUG(\"pos_len %d\", pos_len);\n            auto pe = ggml_new_tensor_4d(compute_ctx, GGML_TYPE_F32, 2, 2, flux_params.axes_dim_sum / 2, pos_len);\n            // pe->data = pe_vec.data();\n            // print_ggml_tensor(pe);\n            // pe->data = NULL;\n            set_backend_tensor_data(pe, pe_vec.data());\n\n            struct ggml_tensor* out = flux.forward(compute_ctx,\n                                                   x,\n                                                   timesteps,\n                                                   context,\n                                                   c_concat,\n                                                   y,\n                                                   guidance,\n                                                   pe,\n                                                   skip_layers);\n\n            ggml_build_forward_expand(gf, out);\n\n            return gf;\n        }\n\n        void compute(int n_threads,\n                     struct ggml_tensor* x,\n                     struct ggml_tensor* timesteps,\n                     struct ggml_tensor* context,\n                     struct ggml_tensor* c_concat,\n                     struct ggml_tensor* y,\n                     struct ggml_tensor* guidance,\n                     struct ggml_tensor** output     = NULL,\n                     struct ggml_context* output_ctx = NULL,\n                     std::vector<int> skip_layers    = std::vector<int>()) {\n            // x: [N, in_channels, h, w]\n            // timesteps: [N, ]\n            // context: [N, max_position, hidden_size]\n            // y: [N, adm_in_channels] or [1, adm_in_channels]\n            // guidance: [N, ]\n            auto get_graph = [&]() -> struct ggml_cgraph* {\n                return build_graph(x, timesteps, context, c_concat, y, guidance, skip_layers);\n            };\n\n            GGMLRunner::compute(get_graph, n_threads, false, output, output_ctx);\n        }\n\n        void test() {\n            struct ggml_init_params params;\n            params.mem_size   = static_cast<size_t>(20 * 1024 * 1024);  // 20 MB\n            params.mem_buffer = NULL;\n            params.no_alloc   = false;\n\n            struct ggml_context* work_ctx = ggml_init(params);\n            GGML_ASSERT(work_ctx != NULL);\n\n            {\n                // cpu f16:\n                // cuda f16: nan\n                // cuda q8_0: pass\n                auto x = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 16, 16, 16, 1);\n                ggml_set_f32(x, 0.01f);\n                // print_ggml_tensor(x);\n\n                std::vector<float> timesteps_vec(1, 999.f);\n                auto timesteps = vector_to_ggml_tensor(work_ctx, timesteps_vec);\n\n                std::vector<float> guidance_vec(1, 3.5f);\n                auto guidance = vector_to_ggml_tensor(work_ctx, guidance_vec);\n\n                auto context = ggml_new_tensor_3d(work_ctx, GGML_TYPE_F32, 4096, 256, 1);\n                ggml_set_f32(context, 0.01f);\n                // print_ggml_tensor(context);\n\n                auto y = ggml_new_tensor_2d(work_ctx, GGML_TYPE_F32, 768, 1);\n                ggml_set_f32(y, 0.01f);\n                // print_ggml_tensor(y);\n\n                struct ggml_tensor* out = NULL;\n\n                int t0 = ggml_time_ms();\n                compute(8, x, timesteps, context, NULL, y, guidance, &out, work_ctx);\n                int t1 = ggml_time_ms();\n\n                print_ggml_tensor(out);\n                LOG_DEBUG(\"flux test done in %dms\", t1 - t0);\n            }\n        }\n\n        static void load_from_file_and_test(const std::string& file_path) {\n            // ggml_backend_t backend    = ggml_backend_cuda_init(0);\n            ggml_backend_t backend           = ggml_backend_cpu_init();\n            ggml_type model_data_type        = GGML_TYPE_Q8_0;\n            std::shared_ptr<FluxRunner> flux = std::shared_ptr<FluxRunner>(new FluxRunner(backend));\n            {\n                LOG_INFO(\"loading from '%s'\", file_path.c_str());\n\n                flux->alloc_params_buffer();\n                std::map<std::string, ggml_tensor*> tensors;\n                flux->get_param_tensors(tensors, \"model.diffusion_model\");\n\n                ModelLoader model_loader;\n                if (!model_loader.init_from_file(file_path, \"model.diffusion_model.\")) {\n                    LOG_ERROR(\"init model loader from file failed: '%s'\", file_path.c_str());\n                    return;\n                }\n\n                bool success = model_loader.load_tensors(tensors, backend);\n\n                if (!success) {\n                    LOG_ERROR(\"load tensors from model loader failed\");\n                    return;\n                }\n\n                LOG_INFO(\"flux model loaded\");\n            }\n            flux->test();\n        }\n    };\n\n}  // namespace Flux\n\n#endif  // __FLUX_HPP__\n"
        },
        {
          "name": "format-code.sh",
          "type": "blob",
          "size": 0.087890625,
          "content": "clang-format -style=file -i *.cpp *.h *.hpp\nclang-format -style=file -i examples/cli/*.cpp"
        },
        {
          "name": "ggml",
          "type": "commit",
          "content": null
        },
        {
          "name": "ggml_extend.hpp",
          "type": "blob",
          "size": 59.736328125,
          "content": "#ifndef __GGML_EXTEND_HPP__\n#define __GGML_EXTEND_HPP__\n\n#include <assert.h>\n#include <inttypes.h>\n#include <stdarg.h>\n#include <algorithm>\n#include <cstring>\n#include <fstream>\n#include <functional>\n#include <iostream>\n#include <iterator>\n#include <map>\n#include <memory>\n#include <random>\n#include <regex>\n#include <set>\n#include <sstream>\n#include <string>\n#include <unordered_map>\n#include <vector>\n\n#include \"ggml-alloc.h\"\n#include \"ggml-backend.h\"\n#include \"ggml-cpu.h\"\n#include \"ggml.h\"\n\n#include \"model.h\"\n\n#ifdef SD_USE_CUDA\n#include \"ggml-cuda.h\"\n#endif\n\n#ifdef SD_USE_METAL\n#include \"ggml-metal.h\"\n#endif\n\n#ifdef SD_USE_VULKAN\n#include \"ggml-vulkan.h\"\n#endif\n\n#ifdef SD_USE_SYCL\n#include \"ggml-sycl.h\"\n#endif\n\n#include \"rng.hpp\"\n#include \"util.h\"\n\n#define EPS 1e-05f\n\n#ifndef __STATIC_INLINE__\n#define __STATIC_INLINE__ static inline\n#endif\n\n__STATIC_INLINE__ void ggml_log_callback_default(ggml_log_level level, const char* text, void* user_data) {\n    (void)level;\n    (void)user_data;\n    fputs(text, stderr);\n    fflush(stderr);\n}\n\n__STATIC_INLINE__ void ggml_tensor_set_f32_randn(struct ggml_tensor* tensor, std::shared_ptr<RNG> rng) {\n    uint32_t n                        = (uint32_t)ggml_nelements(tensor);\n    std::vector<float> random_numbers = rng->randn(n);\n    for (uint32_t i = 0; i < n; i++) {\n        ggml_set_f32_1d(tensor, i, random_numbers[i]);\n    }\n}\n\n// set tensor[i, j, k, l]\n// set tensor[l]\n// set tensor[k, l]\n// set tensor[j, k, l]\n__STATIC_INLINE__ void ggml_tensor_set_f32(struct ggml_tensor* tensor, float value, int l, int k = 0, int j = 0, int i = 0) {\n    GGML_ASSERT(tensor->nb[0] == sizeof(float));\n    *(float*)((char*)(tensor->data) + i * tensor->nb[3] + j * tensor->nb[2] + k * tensor->nb[1] + l * tensor->nb[0]) = value;\n}\n\n__STATIC_INLINE__ float ggml_tensor_get_f32(const ggml_tensor* tensor, int l, int k = 0, int j = 0, int i = 0) {\n    if (tensor->buffer != NULL) {\n        float value;\n        ggml_backend_tensor_get(tensor, &value, i * tensor->nb[3] + j * tensor->nb[2] + k * tensor->nb[1] + l * tensor->nb[0], sizeof(float));\n        return value;\n    }\n    GGML_ASSERT(tensor->nb[0] == sizeof(float));\n    return *(float*)((char*)(tensor->data) + i * tensor->nb[3] + j * tensor->nb[2] + k * tensor->nb[1] + l * tensor->nb[0]);\n}\n\n__STATIC_INLINE__ int ggml_tensor_get_i32(const ggml_tensor* tensor, int l, int k = 0, int j = 0, int i = 0) {\n    if (tensor->buffer != NULL) {\n        float value;\n        ggml_backend_tensor_get(tensor, &value, i * tensor->nb[3] + j * tensor->nb[2] + k * tensor->nb[1] + l * tensor->nb[0], sizeof(int));\n        return value;\n    }\n    GGML_ASSERT(tensor->nb[0] == sizeof(int));\n    return *(int*)((char*)(tensor->data) + i * tensor->nb[3] + j * tensor->nb[2] + k * tensor->nb[1] + l * tensor->nb[0]);\n}\n\n__STATIC_INLINE__ ggml_fp16_t ggml_tensor_get_f16(const ggml_tensor* tensor, int l, int k = 0, int j = 0, int i = 0) {\n    GGML_ASSERT(tensor->nb[0] == sizeof(ggml_fp16_t));\n    return *(ggml_fp16_t*)((char*)(tensor->data) + i * tensor->nb[3] + j * tensor->nb[2] + k * tensor->nb[1] + l * tensor->nb[0]);\n}\n\nstatic struct ggml_tensor* get_tensor_from_graph(struct ggml_cgraph* gf, const char* name) {\n    struct ggml_tensor* res = NULL;\n    for (int i = 0; i < ggml_graph_n_nodes(gf); i++) {\n        struct ggml_tensor* node = ggml_graph_node(gf, i);\n        // printf(\"%d, %s \\n\", i, ggml_get_name(node));\n        if (strcmp(ggml_get_name(node), name) == 0) {\n            res = node;\n            break;\n        }\n    }\n    return res;\n}\n\n__STATIC_INLINE__ void print_ggml_tensor(struct ggml_tensor* tensor, bool shape_only = false, const char* mark = \"\") {\n    printf(\"%s (%s): shape(%zu, %zu, %zu, %zu)\\n\", mark, ggml_type_name(tensor->type), tensor->ne[0], tensor->ne[1], tensor->ne[2], tensor->ne[3]);\n    fflush(stdout);\n    if (shape_only) {\n        return;\n    }\n    int range = 3;\n    for (int i = 0; i < tensor->ne[3]; i++) {\n        if (i >= range && i + range < tensor->ne[3]) {\n            continue;\n        }\n        for (int j = 0; j < tensor->ne[2]; j++) {\n            if (j >= range && j + range < tensor->ne[2]) {\n                continue;\n            }\n            for (int k = 0; k < tensor->ne[1]; k++) {\n                if (k >= range && k + range < tensor->ne[1]) {\n                    continue;\n                }\n                for (int l = 0; l < tensor->ne[0]; l++) {\n                    if (l >= range && l + range < tensor->ne[0]) {\n                        continue;\n                    }\n                    if (tensor->type == GGML_TYPE_F32) {\n                        printf(\"  [%d, %d, %d, %d] = %f\\n\", i, j, k, l, ggml_tensor_get_f32(tensor, l, k, j, i));\n                    } else if (tensor->type == GGML_TYPE_F16) {\n                        printf(\"  [%d, %d, %d, %d] = %i\\n\", i, j, k, l, ggml_tensor_get_f16(tensor, l, k, j, i));\n                    } else if (tensor->type == GGML_TYPE_I32) {\n                        printf(\"  [%d, %d, %d, %d] = %i\\n\", i, j, k, l, ggml_tensor_get_i32(tensor, l, k, j, i));\n                    }\n                    fflush(stdout);\n                }\n            }\n        }\n    }\n}\n\n__STATIC_INLINE__ ggml_tensor* load_tensor_from_file(ggml_context* ctx, const std::string& file_path) {\n    std::ifstream file(file_path, std::ios::binary);\n    if (!file.is_open()) {\n        LOG_ERROR(\"failed to open '%s'\", file_path.c_str());\n        return NULL;\n    }\n    int32_t n_dims;\n    int32_t length;\n    int32_t ttype;\n\n    file.read(reinterpret_cast<char*>(&n_dims), sizeof(n_dims));\n    file.read(reinterpret_cast<char*>(&length), sizeof(length));\n    file.read(reinterpret_cast<char*>(&ttype), sizeof(ttype));\n\n    if (file.eof()) {\n        LOG_ERROR(\"incomplete file '%s'\", file_path.c_str());\n        return NULL;\n    }\n\n    int32_t nelements = 1;\n    int32_t ne[4]     = {1, 1, 1, 1};\n    for (int i = 0; i < n_dims; ++i) {\n        file.read(reinterpret_cast<char*>(&ne[i]), sizeof(ne[i]));\n        nelements *= ne[i];\n    }\n    std::string name(length, 0);\n    file.read(&name[0], length);\n    ggml_tensor* tensor = ggml_new_tensor_4d(ctx, (ggml_type)ttype, ne[0], ne[1], ne[2], ne[3]);\n    const size_t bpe    = ggml_type_size(ggml_type(ttype));\n    file.read(reinterpret_cast<char*>(tensor->data), ggml_nbytes(tensor));\n    return tensor;\n}\n\n// __STATIC_INLINE__ void save_tensor_to_file(const std::string& file_name, ggml_tensor* tensor, const std::string & name) {\n//     std::string file_name_ = file_name + \".tensor\";\n//     std::string name_ = name;\n//     std::ofstream file(\"./\" + file_name_, std::ios::binary);\n//     file.write(reinterpret_cast<char*>(&tensor->n_dims), sizeof(tensor->n_dims));\n//     int len = (int)name_.size();\n//     file.write(reinterpret_cast<char*>(&len), sizeof(len));\n//     int ttype = (int)tensor->type;\n//     file.write(reinterpret_cast<char*>(&ttype), sizeof(ttype));\n//     for (int i = 0; i < tensor->n_dims; ++i) {\n//         int ne_ = (int) tensor->ne[i];\n//         file.write(reinterpret_cast<char*>(&ne_), sizeof(ne_));\n//     }\n//     file.write(&name_[0], len);\n//     char* data = nullptr;\n//     file.write((char*)tensor->data, ggml_nbytes(tensor));\n//     file.close();\n// }\n\n__STATIC_INLINE__ void copy_ggml_tensor(struct ggml_tensor* dst, struct ggml_tensor* src) {\n    if (dst->type == src->type) {\n        dst->nb[0] = src->nb[0];\n        dst->nb[1] = src->nb[1];\n        dst->nb[2] = src->nb[2];\n        dst->nb[3] = src->nb[3];\n\n        memcpy(((char*)dst->data), ((char*)src->data), ggml_nbytes(dst));\n        return;\n    }\n    struct ggml_init_params params;\n    params.mem_size          = 10 * 1024 * 1024;  // for padding\n    params.mem_buffer        = NULL;\n    params.no_alloc          = false;\n    struct ggml_context* ctx = ggml_init(params);\n    if (!ctx) {\n        LOG_ERROR(\"ggml_init() failed\");\n        return;\n    }\n    ggml_tensor* final = ggml_cpy(ctx, src, dst);\n\n    struct ggml_cgraph* graph = ggml_new_graph(ctx);\n    ggml_build_forward_expand(graph, final);\n    ggml_graph_compute_with_ctx(ctx, graph, 1);\n    ggml_free(ctx);\n}\n\n__STATIC_INLINE__ float sigmoid(float x) {\n    return 1 / (1.0f + expf(-x));\n}\n\n// SPECIAL OPERATIONS WITH TENSORS\n\n__STATIC_INLINE__ uint8_t* sd_tensor_to_image(struct ggml_tensor* input) {\n    int64_t width    = input->ne[0];\n    int64_t height   = input->ne[1];\n    int64_t channels = input->ne[2];\n    GGML_ASSERT(channels == 3 && input->type == GGML_TYPE_F32);\n    uint8_t* image_data = (uint8_t*)malloc(width * height * channels);\n    for (int iy = 0; iy < height; iy++) {\n        for (int ix = 0; ix < width; ix++) {\n            for (int k = 0; k < channels; k++) {\n                float value                                               = ggml_tensor_get_f32(input, ix, iy, k);\n                *(image_data + iy * width * channels + ix * channels + k) = (uint8_t)(value * 255.0f);\n            }\n        }\n    }\n    return image_data;\n}\n\n__STATIC_INLINE__ uint8_t* sd_tensor_to_mul_image(struct ggml_tensor* input, int idx) {\n    int64_t width    = input->ne[0];\n    int64_t height   = input->ne[1];\n    int64_t channels = input->ne[2];\n    GGML_ASSERT(channels == 3 && input->type == GGML_TYPE_F32);\n    uint8_t* image_data = (uint8_t*)malloc(width * height * channels);\n    for (int iy = 0; iy < height; iy++) {\n        for (int ix = 0; ix < width; ix++) {\n            for (int k = 0; k < channels; k++) {\n                float value                                               = ggml_tensor_get_f32(input, ix, iy, k, idx);\n                *(image_data + iy * width * channels + ix * channels + k) = (uint8_t)(value * 255.0f);\n            }\n        }\n    }\n    return image_data;\n}\n\n__STATIC_INLINE__ void sd_image_to_tensor(const uint8_t* image_data,\n                                          struct ggml_tensor* output,\n                                          bool scale = true) {\n    int64_t width    = output->ne[0];\n    int64_t height   = output->ne[1];\n    int64_t channels = output->ne[2];\n    GGML_ASSERT(channels == 3 && output->type == GGML_TYPE_F32);\n    for (int iy = 0; iy < height; iy++) {\n        for (int ix = 0; ix < width; ix++) {\n            for (int k = 0; k < channels; k++) {\n                float value = *(image_data + iy * width * channels + ix * channels + k);\n                if (scale) {\n                    value /= 255.f;\n                }\n                ggml_tensor_set_f32(output, value, ix, iy, k);\n            }\n        }\n    }\n}\n\n__STATIC_INLINE__ void sd_mask_to_tensor(const uint8_t* image_data,\n                                         struct ggml_tensor* output,\n                                         bool scale = true) {\n    int64_t width    = output->ne[0];\n    int64_t height   = output->ne[1];\n    int64_t channels = output->ne[2];\n    GGML_ASSERT(channels == 1 && output->type == GGML_TYPE_F32);\n    for (int iy = 0; iy < height; iy++) {\n        for (int ix = 0; ix < width; ix++) {\n            float value = *(image_data + iy * width * channels + ix);\n            if (scale) {\n                value /= 255.f;\n            }\n            ggml_tensor_set_f32(output, value, ix, iy);\n        }\n    }\n}\n\n__STATIC_INLINE__ void sd_apply_mask(struct ggml_tensor* image_data,\n                                     struct ggml_tensor* mask,\n                                     struct ggml_tensor* output) {\n    int64_t width    = output->ne[0];\n    int64_t height   = output->ne[1];\n    int64_t channels = output->ne[2];\n    GGML_ASSERT(output->type == GGML_TYPE_F32);\n    for (int ix = 0; ix < width; ix++) {\n        for (int iy = 0; iy < height; iy++) {\n            float m = ggml_tensor_get_f32(mask, ix, iy);\n            for (int k = 0; k < channels; k++) {\n                float value = ((float)(m < 254.5/255)) * (ggml_tensor_get_f32(image_data, ix, iy, k) - .5) + .5;\n                ggml_tensor_set_f32(output, value, ix, iy, k);\n            }\n        }\n    }\n}\n\n__STATIC_INLINE__ void sd_mul_images_to_tensor(const uint8_t* image_data,\n                                               struct ggml_tensor* output,\n                                               int idx,\n                                               float* mean = NULL,\n                                               float* std  = NULL) {\n    int64_t width    = output->ne[0];\n    int64_t height   = output->ne[1];\n    int64_t channels = output->ne[2];\n    GGML_ASSERT(channels == 3 && output->type == GGML_TYPE_F32);\n    for (int iy = 0; iy < height; iy++) {\n        for (int ix = 0; ix < width; ix++) {\n            for (int k = 0; k < channels; k++) {\n                int value       = *(image_data + iy * width * channels + ix * channels + k);\n                float pixel_val = value / 255.0f;\n                if (mean != NULL && std != NULL)\n                    pixel_val = (pixel_val - mean[k]) / std[k];\n                ggml_tensor_set_f32(output, pixel_val, ix, iy, k, idx);\n            }\n        }\n    }\n}\n\n__STATIC_INLINE__ void sd_image_f32_to_tensor(const float* image_data,\n                                              struct ggml_tensor* output,\n                                              bool scale = true) {\n    int64_t width    = output->ne[0];\n    int64_t height   = output->ne[1];\n    int64_t channels = output->ne[2];\n    GGML_ASSERT(channels == 3 && output->type == GGML_TYPE_F32);\n    for (int iy = 0; iy < height; iy++) {\n        for (int ix = 0; ix < width; ix++) {\n            for (int k = 0; k < channels; k++) {\n                int value = *(image_data + iy * width * channels + ix * channels + k);\n                if (scale) {\n                    value /= 255.f;\n                }\n                ggml_tensor_set_f32(output, value, ix, iy, k);\n            }\n        }\n    }\n}\n\n__STATIC_INLINE__ void ggml_split_tensor_2d(struct ggml_tensor* input,\n                                            struct ggml_tensor* output,\n                                            int x,\n                                            int y) {\n    int64_t width    = output->ne[0];\n    int64_t height   = output->ne[1];\n    int64_t channels = output->ne[2];\n    GGML_ASSERT(input->type == GGML_TYPE_F32 && output->type == GGML_TYPE_F32);\n    for (int iy = 0; iy < height; iy++) {\n        for (int ix = 0; ix < width; ix++) {\n            for (int k = 0; k < channels; k++) {\n                float value = ggml_tensor_get_f32(input, ix + x, iy + y, k);\n                ggml_tensor_set_f32(output, value, ix, iy, k);\n            }\n        }\n    }\n}\n\n// unclamped -> expects x in the range [0-1]\n__STATIC_INLINE__ float ggml_smootherstep_f32(const float x) {\n    GGML_ASSERT(x >= 0.f && x <= 1.f);\n    return x * x * x * (x * (6.0f * x - 15.0f) + 10.0f);\n}\n\n__STATIC_INLINE__ void ggml_merge_tensor_2d(struct ggml_tensor* input,\n                                            struct ggml_tensor* output,\n                                            int x,\n                                            int y,\n                                            int overlap) {\n    int64_t width    = input->ne[0];\n    int64_t height   = input->ne[1];\n    int64_t channels = input->ne[2];\n\n    int64_t img_width  = output->ne[0];\n    int64_t img_height = output->ne[1];\n\n    GGML_ASSERT(input->type == GGML_TYPE_F32 && output->type == GGML_TYPE_F32);\n    for (int iy = 0; iy < height; iy++) {\n        for (int ix = 0; ix < width; ix++) {\n            for (int k = 0; k < channels; k++) {\n                float new_value = ggml_tensor_get_f32(input, ix, iy, k);\n                if (overlap > 0) {  // blend colors in overlapped area\n                    float old_value = ggml_tensor_get_f32(output, x + ix, y + iy, k);\n\n                    const float x_f_0 = (x > 0) ? ix / float(overlap) : 1;\n                    const float x_f_1 = (x < (img_width - width)) ? (width - ix) / float(overlap) : 1;\n                    const float y_f_0 = (y > 0) ? iy / float(overlap) : 1;\n                    const float y_f_1 = (y < (img_height - height)) ? (height - iy) / float(overlap) : 1;\n\n                    const float x_f = std::min(std::min(x_f_0, x_f_1), 1.f);\n                    const float y_f = std::min(std::min(y_f_0, y_f_1), 1.f);\n\n                    ggml_tensor_set_f32(\n                        output,\n                        old_value + new_value * ggml_smootherstep_f32(y_f) * ggml_smootherstep_f32(x_f),\n                        x + ix, y + iy, k);\n                } else {\n                    ggml_tensor_set_f32(output, new_value, x + ix, y + iy, k);\n                }\n            }\n        }\n    }\n}\n\n__STATIC_INLINE__ float ggml_tensor_mean(struct ggml_tensor* src) {\n    float mean        = 0.0f;\n    int64_t nelements = ggml_nelements(src);\n    float* data       = (float*)src->data;\n    for (int i = 0; i < nelements; i++) {\n        mean += data[i] / nelements * 1.0f;\n    }\n    return mean;\n}\n\n// a = a+b\n__STATIC_INLINE__ void ggml_tensor_add(struct ggml_tensor* a, struct ggml_tensor* b) {\n    GGML_ASSERT(ggml_nelements(a) == ggml_nelements(b));\n    int64_t nelements = ggml_nelements(a);\n    float* vec_a      = (float*)a->data;\n    float* vec_b      = (float*)b->data;\n    for (int i = 0; i < nelements; i++) {\n        vec_a[i] = vec_a[i] + vec_b[i];\n    }\n}\n\n__STATIC_INLINE__ void ggml_tensor_scale(struct ggml_tensor* src, float scale) {\n    int64_t nelements = ggml_nelements(src);\n    float* data       = (float*)src->data;\n    for (int i = 0; i < nelements; i++) {\n        data[i] = data[i] * scale;\n    }\n}\n\n__STATIC_INLINE__ void ggml_tensor_clamp(struct ggml_tensor* src, float min, float max) {\n    int64_t nelements = ggml_nelements(src);\n    float* data       = (float*)src->data;\n    for (int i = 0; i < nelements; i++) {\n        float val = data[i];\n        data[i]   = val < min ? min : (val > max ? max : val);\n    }\n}\n\n__STATIC_INLINE__ struct ggml_tensor* ggml_tensor_concat(struct ggml_context* ctx,\n                                                         struct ggml_tensor* a,\n                                                         struct ggml_tensor* b,\n                                                         int dim) {\n    int64_t ne[GGML_MAX_DIMS];\n    for (int d = 0; d < GGML_MAX_DIMS; ++d) {\n        if (d == dim) {\n            ne[d] = a->ne[d] + b->ne[d];\n            continue;\n        }\n        GGML_ASSERT(a->ne[d] == b->ne[d]);\n        ne[d] = a->ne[d];\n    }\n    struct ggml_tensor* result = ggml_new_tensor(ctx, a->type, GGML_MAX_DIMS, ne);\n    int64_t o[4]               = {0, 0, 0, 0};\n    o[dim]                     = a->ne[dim];\n\n    float v;\n    for (int i3 = 0; i3 < result->ne[3]; i3++) {\n        for (int i2 = 0; i2 < result->ne[2]; i2++) {\n            for (int i1 = 0; i1 < result->ne[1]; i1++) {\n                for (int i0 = 0; i0 < result->ne[0]; i0++) {\n                    if (i0 < a->ne[0] && i1 < a->ne[1] && i2 < a->ne[2] && i3 < a->ne[3]) {\n                        v = ggml_tensor_get_f32(a, i0, i1, i2, i3);\n                    } else {\n                        v = ggml_tensor_get_f32(b, i0 - o[0], i1 - o[1], i2 - o[2], i3 - o[3]);\n                    }\n\n                    ggml_tensor_set_f32(result, v, i0, i1, i2, i3);\n                }\n            }\n        }\n    }\n    return result;\n}\n\n// convert values from [0, 1] to [-1, 1]\n__STATIC_INLINE__ void ggml_tensor_scale_input(struct ggml_tensor* src) {\n    int64_t nelements = ggml_nelements(src);\n    float* data       = (float*)src->data;\n    for (int i = 0; i < nelements; i++) {\n        float val = data[i];\n        data[i]   = val * 2.0f - 1.0f;\n    }\n}\n\n// convert values from [-1, 1] to [0, 1]\n__STATIC_INLINE__ void ggml_tensor_scale_output(struct ggml_tensor* src) {\n    int64_t nelements = ggml_nelements(src);\n    float* data       = (float*)src->data;\n    for (int i = 0; i < nelements; i++) {\n        float val = data[i];\n        data[i]   = (val + 1.0f) * 0.5f;\n    }\n}\n\ntypedef std::function<void(ggml_tensor*, ggml_tensor*, bool)> on_tile_process;\n\n// Tiling\n__STATIC_INLINE__ void sd_tiling(ggml_tensor* input, ggml_tensor* output, const int scale, const int tile_size, const float tile_overlap_factor, on_tile_process on_processing) {\n    int input_width   = (int)input->ne[0];\n    int input_height  = (int)input->ne[1];\n    int output_width  = (int)output->ne[0];\n    int output_height = (int)output->ne[1];\n    GGML_ASSERT(input_width % 2 == 0 && input_height % 2 == 0 && output_width % 2 == 0 && output_height % 2 == 0);  // should be multiple of 2\n\n    int tile_overlap     = (int32_t)(tile_size * tile_overlap_factor);\n    int non_tile_overlap = tile_size - tile_overlap;\n\n    struct ggml_init_params params = {};\n    params.mem_size += tile_size * tile_size * input->ne[2] * sizeof(float);                       // input chunk\n    params.mem_size += (tile_size * scale) * (tile_size * scale) * output->ne[2] * sizeof(float);  // output chunk\n    params.mem_size += 3 * ggml_tensor_overhead();\n    params.mem_buffer = NULL;\n    params.no_alloc   = false;\n\n    LOG_DEBUG(\"tile work buffer size: %.2f MB\", params.mem_size / 1024.f / 1024.f);\n\n    // draft context\n    struct ggml_context* tiles_ctx = ggml_init(params);\n    if (!tiles_ctx) {\n        LOG_ERROR(\"ggml_init() failed\");\n        return;\n    }\n\n    // tiling\n    ggml_tensor* input_tile  = ggml_new_tensor_4d(tiles_ctx, GGML_TYPE_F32, tile_size, tile_size, input->ne[2], 1);\n    ggml_tensor* output_tile = ggml_new_tensor_4d(tiles_ctx, GGML_TYPE_F32, tile_size * scale, tile_size * scale, output->ne[2], 1);\n    on_processing(input_tile, NULL, true);\n    int num_tiles = ceil((float)input_width / non_tile_overlap) * ceil((float)input_height / non_tile_overlap);\n    LOG_INFO(\"processing %i tiles\", num_tiles);\n    pretty_progress(1, num_tiles, 0.0f);\n    int tile_count = 1;\n    bool last_y = false, last_x = false;\n    float last_time = 0.0f;\n    for (int y = 0; y < input_height && !last_y; y += non_tile_overlap) {\n        if (y + tile_size >= input_height) {\n            y      = input_height - tile_size;\n            last_y = true;\n        }\n        for (int x = 0; x < input_width && !last_x; x += non_tile_overlap) {\n            if (x + tile_size >= input_width) {\n                x      = input_width - tile_size;\n                last_x = true;\n            }\n            int64_t t1 = ggml_time_ms();\n            ggml_split_tensor_2d(input, input_tile, x, y);\n            on_processing(input_tile, output_tile, false);\n            ggml_merge_tensor_2d(output_tile, output, x * scale, y * scale, tile_overlap * scale);\n            int64_t t2 = ggml_time_ms();\n            last_time  = (t2 - t1) / 1000.0f;\n            pretty_progress(tile_count, num_tiles, last_time);\n            tile_count++;\n        }\n        last_x = false;\n    }\n    if (tile_count < num_tiles) {\n        pretty_progress(num_tiles, num_tiles, last_time);\n    }\n    ggml_free(tiles_ctx);\n}\n\n__STATIC_INLINE__ struct ggml_tensor* ggml_group_norm_32(struct ggml_context* ctx,\n                                                         struct ggml_tensor* a) {\n    const float eps = 1e-6f;  // default eps parameter\n    return ggml_group_norm(ctx, a, 32, eps);\n}\n\n__STATIC_INLINE__ struct ggml_tensor* ggml_nn_linear(struct ggml_context* ctx,\n                                                     struct ggml_tensor* x,\n                                                     struct ggml_tensor* w,\n                                                     struct ggml_tensor* b) {\n    x = ggml_mul_mat(ctx, w, x);\n    if (b != NULL) {\n        x = ggml_add(ctx, x, b);\n    }\n    return x;\n}\n\n// w: [OC，IC, KH, KW]\n// x: [N, IC, IH, IW]\n// b: [OC,]\n// result: [N, OC, OH, OW]\n__STATIC_INLINE__ struct ggml_tensor* ggml_nn_conv_2d(struct ggml_context* ctx,\n                                                      struct ggml_tensor* x,\n                                                      struct ggml_tensor* w,\n                                                      struct ggml_tensor* b,\n                                                      int s0 = 1,\n                                                      int s1 = 1,\n                                                      int p0 = 0,\n                                                      int p1 = 0,\n                                                      int d0 = 1,\n                                                      int d1 = 1) {\n    x = ggml_conv_2d(ctx, w, x, s0, s1, p0, p1, d0, d1);\n    if (b != NULL) {\n        b = ggml_reshape_4d(ctx, b, 1, 1, b->ne[0], 1);\n        // b = ggml_repeat(ctx, b, x);\n        x = ggml_add(ctx, x, b);\n    }\n    return x;\n}\n\n// w: [OC，IC, KD, 1 * 1]\n// x: [N, IC, IH, IW]\n// b: [OC,]\n// result: [N, OC, OH, OW]\n__STATIC_INLINE__ struct ggml_tensor* ggml_nn_conv_3d_nx1x1_bak(struct ggml_context* ctx,\n                                                                struct ggml_tensor* x,\n                                                                struct ggml_tensor* w,\n                                                                struct ggml_tensor* b,\n                                                                int s2 = 1,\n                                                                int p2 = 1,\n                                                                int d2 = 1) {\n    GGML_ASSERT(w->ne[0] == 1);\n    // timesteps = x.shape[0]\n    // x = rearrange(x, \"(b t) c h w -> b c t h w\", t=timesteps)\n    // x = conv3d(x)\n    // return rearrange(x, \"b c t h w -> (b t) c h w\")\n    int64_t T = x->ne[3];\n    int64_t B = x->ne[3] / T;\n    int64_t C = x->ne[2];\n    int64_t H = x->ne[1];\n    int64_t W = x->ne[0];\n\n    x = ggml_reshape_4d(ctx, x, W * H, C, T, B);           // (b t) c h w -> b t c (h w)\n    x = ggml_cont(ctx, ggml_permute(ctx, x, 0, 2, 1, 3));  // b t c (h w) -> b c t (h w)\n    x = ggml_conv_2d(ctx, w, x, 1, s2, 0, p2, 1, d2);      // [B, OC, T, OH * OW]\n    if (b != NULL) {\n        b = ggml_reshape_4d(ctx, b, 1, 1, b->ne[0], 1);\n        x = ggml_add(ctx, x, b);\n    }\n    x = ggml_cont(ctx, ggml_permute(ctx, x, 0, 2, 1, 3));  // b c t (h w) -> b t c (h w)\n    x = ggml_reshape_4d(ctx, x, W, H, C, T * B);           // b t c (h w) -> (b t) c h w\n    return x;                                              // [B*T, OC, OH, OW]\n}\n\n// w: [OC，IC, KD, 1 * 1]\n// x: [N, IC, ID, IH*IW]\n// b: [OC,]\n// result: [N, OC, OD, OH*OW]\n__STATIC_INLINE__ struct ggml_tensor* ggml_nn_conv_3d_nx1x1(struct ggml_context* ctx,\n                                                            struct ggml_tensor* x,\n                                                            struct ggml_tensor* w,\n                                                            struct ggml_tensor* b,\n                                                            int s2 = 1,\n                                                            int p2 = 1,\n                                                            int d2 = 1) {\n    x = ggml_conv_2d(ctx, w, x, 1, s2, 0, p2, 1, d2);  // [N, OC, T, OH * OW]\n    if (b != NULL) {\n        b = ggml_reshape_4d(ctx, b, 1, 1, b->ne[0], 1);\n        x = ggml_add(ctx, x, b);\n    }\n    return x;  // [N, OC, T, OH * OW]\n}\n\n// qkv: [N, L, 3*C]\n// return: ([N, L, C], [N, L, C], [N, L, C])\n__STATIC_INLINE__ std::vector<struct ggml_tensor*> split_qkv(struct ggml_context* ctx,\n                                                             struct ggml_tensor* qkv) {\n    qkv = ggml_reshape_4d(ctx, qkv, qkv->ne[0] / 3, 3, qkv->ne[1], qkv->ne[2]);  // [N, L, 3, C]\n    qkv = ggml_cont(ctx, ggml_permute(ctx, qkv, 0, 3, 1, 2));                    // [3, N, L, C]\n\n    int64_t offset = qkv->nb[2] * qkv->ne[2];\n    auto q         = ggml_view_3d(ctx, qkv, qkv->ne[0], qkv->ne[1], qkv->ne[2], qkv->nb[1], qkv->nb[2], offset * 0);  // [N, L, C]\n    auto k         = ggml_view_3d(ctx, qkv, qkv->ne[0], qkv->ne[1], qkv->ne[2], qkv->nb[1], qkv->nb[2], offset * 1);  // [N, L, C]\n    auto v         = ggml_view_3d(ctx, qkv, qkv->ne[0], qkv->ne[1], qkv->ne[2], qkv->nb[1], qkv->nb[2], offset * 2);  // [N, L, C]\n    return {q, k, v};\n}\n\n// q: [N * n_head, n_token, d_head]\n// k: [N * n_head, n_k, d_head]\n// v: [N * n_head, d_head, n_k]\n// return: [N * n_head, n_token, d_head]\n__STATIC_INLINE__ struct ggml_tensor* ggml_nn_attention(struct ggml_context* ctx,\n                                                        struct ggml_tensor* q,\n                                                        struct ggml_tensor* k,\n                                                        struct ggml_tensor* v,\n                                                        bool mask = false) {\n#if defined(SD_USE_FLASH_ATTENTION) && !defined(SD_USE_CUDA) && !defined(SD_USE_METAL) && !defined(SD_USE_VULKAN) && !defined(SD_USE_SYCL)\n    struct ggml_tensor* kqv = ggml_flash_attn(ctx, q, k, v, false);  // [N * n_head, n_token, d_head]\n#else\n    float d_head           = (float)q->ne[0];\n    struct ggml_tensor* kq = ggml_mul_mat(ctx, k, q);  // [N * n_head, n_token, n_k]\n    kq                     = ggml_scale_inplace(ctx, kq, 1.0f / sqrt(d_head));\n    if (mask) {\n        kq = ggml_diag_mask_inf_inplace(ctx, kq, 0);\n    }\n    kq                      = ggml_soft_max_inplace(ctx, kq);\n    struct ggml_tensor* kqv = ggml_mul_mat(ctx, v, kq);  // [N * n_head, n_token, d_head]\n#endif\n    return kqv;\n}\n\n// q: [N, L_q, C] or [N*n_head, L_q, d_head]\n// k: [N, L_k, C] or [N*n_head, L_k, d_head]\n// v: [N, L_k, C] or [N, L_k, n_head, d_head]\n// return: [N, L_q, C]\n__STATIC_INLINE__ struct ggml_tensor* ggml_nn_attention_ext(struct ggml_context* ctx,\n                                                            struct ggml_tensor* q,\n                                                            struct ggml_tensor* k,\n                                                            struct ggml_tensor* v,\n                                                            int64_t n_head,\n                                                            struct ggml_tensor* mask = NULL,\n                                                            bool diag_mask_inf       = false,\n                                                            bool skip_reshape        = false,\n                                                            bool flash_attn          = false) {\n    int64_t L_q;\n    int64_t L_k;\n    int64_t C;\n    int64_t N;\n    int64_t d_head;\n    if (!skip_reshape) {\n        L_q    = q->ne[1];\n        L_k    = k->ne[1];\n        C      = q->ne[0];\n        N      = q->ne[2];\n        d_head = C / n_head;\n        q      = ggml_reshape_4d(ctx, q, d_head, n_head, L_q, N);   // [N, L_q, n_head, d_head]\n        q      = ggml_cont(ctx, ggml_permute(ctx, q, 0, 2, 1, 3));  // [N, n_head, L_q, d_head]\n        q      = ggml_reshape_3d(ctx, q, d_head, L_q, n_head * N);  // [N * n_head, L_q, d_head]\n\n        k = ggml_reshape_4d(ctx, k, d_head, n_head, L_k, N);   // [N, L_k, n_head, d_head]\n        k = ggml_cont(ctx, ggml_permute(ctx, k, 0, 2, 1, 3));  // [N, n_head, L_k, d_head]\n        k = ggml_reshape_3d(ctx, k, d_head, L_k, n_head * N);  // [N * n_head, L_k, d_head]\n\n        v = ggml_reshape_4d(ctx, v, d_head, n_head, L_k, N);  // [N, L_k, n_head, d_head]\n    } else {\n        L_q    = q->ne[1];\n        L_k    = k->ne[1];\n        d_head = v->ne[0];\n        N      = v->ne[3];\n        C      = d_head * n_head;\n    }\n\n    float scale = (1.0f / sqrt((float)d_head));\n\n    // if (flash_attn) {\n    //     LOG_DEBUG(\"attention_ext L_q:%d L_k:%d n_head:%d C:%d d_head:%d N:%d\", L_q, L_k, n_head, C, d_head, N);\n    // }\n    //  is there anything oddly shaped?? ping Green-Sky if you can trip this assert\n    GGML_ASSERT(((L_k % 256 == 0) && L_q == L_k) || !(L_k % 256 == 0));\n\n    bool can_use_flash_attn = true;\n    can_use_flash_attn      = can_use_flash_attn && L_k % 256 == 0;\n    can_use_flash_attn      = can_use_flash_attn && d_head % 64 == 0;  // double check\n\n    // cuda max d_head seems to be 256, cpu does seem to work with 512\n    can_use_flash_attn = can_use_flash_attn && d_head <= 256;  // double check\n\n    if (mask != nullptr) {\n        // TODO(Green-Sky): figure out if we can bend t5 to work too\n        can_use_flash_attn = can_use_flash_attn && mask->ne[2] == 1;\n        can_use_flash_attn = can_use_flash_attn && mask->ne[3] == 1;\n    }\n\n    // TODO(Green-Sky): more pad or disable for funny tensor shapes\n\n    ggml_tensor* kqv = nullptr;\n    // GGML_ASSERT((flash_attn && can_use_flash_attn) || !flash_attn);\n    if (can_use_flash_attn && flash_attn) {\n        // LOG_DEBUG(\"using flash attention\");\n        k = ggml_cast(ctx, k, GGML_TYPE_F16);\n\n        v = ggml_cont(ctx, ggml_permute(ctx, v, 0, 2, 1, 3));  // [N, n_head, L_k, d_head]\n        v = ggml_reshape_3d(ctx, v, d_head, L_k, n_head * N);  // [N * n_head, L_k, d_head]\n        v = ggml_cast(ctx, v, GGML_TYPE_F16);\n\n        kqv = ggml_flash_attn_ext(ctx, q, k, v, mask, scale, 0, 0);\n        ggml_flash_attn_ext_set_prec(kqv, GGML_PREC_F32);\n\n        // kqv = ggml_view_3d(ctx, kqv, d_head, n_head, L_k, kqv->nb[1], kqv->nb[2], 0);\n        kqv = ggml_view_3d(ctx, kqv, d_head, n_head, L_q, kqv->nb[1], kqv->nb[2], 0);\n    } else {\n        v = ggml_cont(ctx, ggml_permute(ctx, v, 1, 2, 0, 3));  // [N, n_head, d_head, L_k]\n        v = ggml_reshape_3d(ctx, v, L_k, d_head, n_head * N);  // [N * n_head, d_head, L_k]\n\n        auto kq = ggml_mul_mat(ctx, k, q);  // [N * n_head, L_q, L_k]\n        kq      = ggml_scale_inplace(ctx, kq, scale);\n        if (mask) {\n            kq = ggml_add(ctx, kq, mask);\n        }\n        if (diag_mask_inf) {\n            kq = ggml_diag_mask_inf_inplace(ctx, kq, 0);\n        }\n        kq = ggml_soft_max_inplace(ctx, kq);\n\n        kqv = ggml_mul_mat(ctx, v, kq);  // [N * n_head, L_q, d_head]\n\n        kqv = ggml_reshape_4d(ctx, kqv, d_head, L_q, n_head, N);  // [N, n_head, L_q, d_head]\n        kqv = ggml_permute(ctx, kqv, 0, 2, 1, 3);                 // [N, L_q, n_head, d_head]\n    }\n\n    kqv = ggml_cont(ctx, kqv);\n    kqv = ggml_reshape_3d(ctx, kqv, d_head * n_head, L_q, N);  // [N, L_q, C]\n\n    return kqv;\n}\n\n__STATIC_INLINE__ struct ggml_tensor* ggml_nn_layer_norm(struct ggml_context* ctx,\n                                                         struct ggml_tensor* x,\n                                                         struct ggml_tensor* w,\n                                                         struct ggml_tensor* b,\n                                                         float eps = EPS) {\n    x = ggml_norm(ctx, x, eps);\n    if (w != NULL) {\n        x = ggml_mul(ctx, x, w);\n        if (b != NULL) {\n            x = ggml_add(ctx, x, b);\n        }\n    }\n    return x;\n}\n\n__STATIC_INLINE__ struct ggml_tensor* ggml_nn_group_norm(struct ggml_context* ctx,\n                                                         struct ggml_tensor* x,\n                                                         struct ggml_tensor* w,\n                                                         struct ggml_tensor* b,\n                                                         int num_groups = 32) {\n    if (ggml_n_dims(x) >= 3 && w != NULL && b != NULL) {\n        w = ggml_reshape_4d(ctx, w, 1, 1, w->ne[0], 1);\n        b = ggml_reshape_4d(ctx, b, 1, 1, b->ne[0], 1);\n    }\n\n    const float eps = 1e-6f;  // default eps parameter\n    x               = ggml_group_norm(ctx, x, num_groups, eps);\n    if (w != NULL && b != NULL) {\n        x = ggml_mul(ctx, x, w);\n        // b = ggml_repeat(ctx, b, x);\n        x = ggml_add(ctx, x, b);\n    }\n    return x;\n}\n\n__STATIC_INLINE__ void ggml_backend_tensor_get_and_sync(ggml_backend_t backend, const struct ggml_tensor* tensor, void* data, size_t offset, size_t size) {\n#if defined(SD_USE_CUDA) || defined(SD_USE_SYCL)\n    if (!ggml_backend_is_cpu(backend)) {\n        ggml_backend_tensor_get_async(backend, tensor, data, offset, size);\n        ggml_backend_synchronize(backend);\n    } else {\n        ggml_backend_tensor_get(tensor, data, offset, size);\n    }\n#else\n    ggml_backend_tensor_get(tensor, data, offset, size);\n#endif\n}\n\n__STATIC_INLINE__ float ggml_backend_tensor_get_f32(ggml_tensor* tensor) {\n    GGML_ASSERT(tensor->type == GGML_TYPE_F32 || tensor->type == GGML_TYPE_F16);\n    float value;\n    if (tensor->type == GGML_TYPE_F32) {\n        ggml_backend_tensor_get(tensor, &value, 0, sizeof(value));\n    } else {  // GGML_TYPE_F16\n        ggml_fp16_t f16_value;\n        ggml_backend_tensor_get(tensor, &f16_value, 0, sizeof(f16_value));\n        value = ggml_fp16_to_fp32(f16_value);\n    }\n    return value;\n}\n\n__STATIC_INLINE__ struct ggml_tensor* vector_to_ggml_tensor(struct ggml_context* ctx,\n                                                            const std::vector<float>& vec) {\n    struct ggml_tensor* t = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, vec.size());\n    memcpy(t->data, (const void*)vec.data(), ggml_nbytes(t));\n    return t;\n}\n\n__STATIC_INLINE__ struct ggml_tensor* vector_to_ggml_tensor_i32(struct ggml_context* ctx,\n                                                                const std::vector<int>& vec) {\n    struct ggml_tensor* t = ggml_new_tensor_1d(ctx, GGML_TYPE_I32, vec.size());\n    memcpy(t->data, (const void*)vec.data(), ggml_nbytes(t));\n    return t;\n}\n\n__STATIC_INLINE__ std::vector<float> arange(float start, float end, float step = 1.f) {\n    std::vector<float> result;\n\n    for (float value = start; value < end; value += step) {\n        result.push_back(value);\n    }\n\n    return result;\n}\n\n// Ref: https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/util.py#L151\n__STATIC_INLINE__ std::vector<float> timestep_embedding(std::vector<float> timesteps,\n                                                        int dim,\n                                                        int max_period = 10000) {\n    // timesteps: [N,]\n    // embedding: [N, dim]\n    size_t N        = timesteps.size();\n    int acutual_dim = dim;\n    if (dim % 2 != 0) {\n        acutual_dim = dim + 1;\n    }\n    std::vector<float> embedding(N * acutual_dim, 0.f);\n    int half = dim / 2;\n    std::vector<float> freqs(half);\n    for (int i = 0; i < half; ++i) {\n        freqs[i] = (float)std::exp(-std::log(max_period) * i / half);\n    }\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < half; ++j) {\n            float arg                             = timesteps[i] * freqs[j];\n            embedding[i * acutual_dim + j]        = std::cos(arg);\n            embedding[i * acutual_dim + j + half] = std::sin(arg);\n        }\n    }\n    return embedding;\n}\n\n__STATIC_INLINE__ void set_timestep_embedding(std::vector<float> timesteps,\n                                              struct ggml_tensor* embedding,\n                                              int dim,\n                                              int max_period = 10000) {\n    std::vector<float> embedding_vec = timestep_embedding(timesteps, dim, max_period);\n    memcpy(((char*)embedding->data), ((char*)embedding_vec.data()), ggml_nbytes(embedding));\n}\n\n__STATIC_INLINE__ struct ggml_tensor* new_timestep_embedding(struct ggml_context* ctx,\n                                                             std::vector<float> timesteps,\n                                                             int dim,\n                                                             int max_period = 10000) {\n    // timesteps: [N,]\n    // embedding: [N, dim]\n    std::vector<float> embedding_vec = timestep_embedding(timesteps, dim, max_period);\n    int acutual_dim                  = dim;\n    if (dim % 2 != 0) {\n        acutual_dim = dim + 1;\n    }\n    struct ggml_tensor* embedding = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, acutual_dim, timesteps.size());\n    if (embedding->data != NULL) {\n        memcpy(((char*)embedding->data), ((char*)embedding_vec.data()), ggml_nbytes(embedding));\n    } else {\n        ggml_backend_tensor_set(embedding, embedding_vec.data(), 0, ggml_nbytes(embedding));\n    }\n    return embedding;\n}\n\n__STATIC_INLINE__ struct ggml_tensor* ggml_nn_timestep_embedding(\n    struct ggml_context* ctx,\n    struct ggml_tensor* timesteps,\n    int dim,\n    int max_period    = 10000,\n    float time_factor = 1.0f) {\n    timesteps = ggml_scale(ctx, timesteps, time_factor);\n    return ggml_timestep_embedding(ctx, timesteps, dim, max_period);\n}\n\n__STATIC_INLINE__ size_t ggml_tensor_num(ggml_context* ctx) {\n    size_t num = 0;\n    for (ggml_tensor* t = ggml_get_first_tensor(ctx); t != nullptr; t = ggml_get_next_tensor(ctx, t)) {\n        num++;\n    }\n    return num;\n}\n\n/* SDXL with LoRA requires more space */\n#define MAX_PARAMS_TENSOR_NUM 15360\n#define MAX_GRAPH_SIZE 15360\n\nstruct GGMLRunner {\nprotected:\n    typedef std::function<struct ggml_cgraph*()> get_graph_cb_t;\n\n    struct ggml_context* params_ctx     = NULL;\n    ggml_backend_buffer_t params_buffer = NULL;\n\n    struct ggml_context* compute_ctx    = NULL;\n    struct ggml_gallocr* compute_allocr = NULL;\n\n    std::map<struct ggml_tensor*, const void*> backend_tensor_data_map;\n\n    ggml_backend_t backend = NULL;\n\n    void alloc_params_ctx() {\n        struct ggml_init_params params;\n        params.mem_size   = static_cast<size_t>(MAX_PARAMS_TENSOR_NUM * ggml_tensor_overhead());\n        params.mem_buffer = NULL;\n        params.no_alloc   = true;\n\n        params_ctx = ggml_init(params);\n        GGML_ASSERT(params_ctx != NULL);\n    }\n\n    void free_params_ctx() {\n        if (params_ctx != NULL) {\n            ggml_free(params_ctx);\n            params_ctx = NULL;\n        }\n    }\n\n    void alloc_compute_ctx() {\n        struct ggml_init_params params;\n        params.mem_size   = static_cast<size_t>(ggml_tensor_overhead() * MAX_GRAPH_SIZE + ggml_graph_overhead());\n        params.mem_buffer = NULL;\n        params.no_alloc   = true;\n\n        compute_ctx = ggml_init(params);\n        GGML_ASSERT(compute_ctx != NULL);\n    }\n\n    void free_compute_ctx() {\n        if (compute_ctx != NULL) {\n            ggml_free(compute_ctx);\n            compute_ctx = NULL;\n        }\n    }\n\n    bool alloc_compute_buffer(get_graph_cb_t get_graph) {\n        if (compute_allocr != NULL) {\n            return true;\n        }\n        reset_compute_ctx();\n        struct ggml_cgraph* gf = get_graph();\n        backend_tensor_data_map.clear();\n        compute_allocr = ggml_gallocr_new(ggml_backend_get_default_buffer_type(backend));\n\n        if (!ggml_gallocr_reserve(compute_allocr, gf)) {\n            // failed to allocate the compute buffer\n            LOG_ERROR(\"%s: failed to allocate the compute buffer\\n\", get_desc().c_str());\n            free_compute_buffer();\n            return false;\n        }\n\n        // compute the required memory\n        size_t compute_buffer_size = ggml_gallocr_get_buffer_size(compute_allocr, 0);\n        LOG_DEBUG(\"%s compute buffer size: %.2f MB(%s)\",\n                  get_desc().c_str(),\n                  compute_buffer_size / 1024.0 / 1024.0,\n                  ggml_backend_is_cpu(backend) ? \"RAM\" : \"VRAM\");\n        return true;\n    }\n\n    void cpy_data_to_backend_tensor() {\n        for (auto& kv : backend_tensor_data_map) {\n            auto tensor = kv.first;\n            auto data   = kv.second;\n\n            ggml_backend_tensor_set(tensor, data, 0, ggml_nbytes(tensor));\n        }\n\n        backend_tensor_data_map.clear();\n    }\n\npublic:\n    virtual std::string get_desc() = 0;\n\n    GGMLRunner(ggml_backend_t backend)\n        : backend(backend) {\n        alloc_params_ctx();\n    }\n\n    virtual ~GGMLRunner() {\n        free_params_buffer();\n        free_compute_buffer();\n        free_params_ctx();\n        free_compute_ctx();\n    }\n\n    void reset_compute_ctx() {\n        free_compute_ctx();\n        alloc_compute_ctx();\n    }\n\n    bool alloc_params_buffer() {\n        size_t num_tensors = ggml_tensor_num(params_ctx);\n        params_buffer      = ggml_backend_alloc_ctx_tensors(params_ctx, backend);\n        if (params_buffer == NULL) {\n            LOG_ERROR(\"%s alloc params backend buffer failed, num_tensors = %i\",\n                      get_desc().c_str(),\n                      num_tensors);\n            return false;\n        }\n        size_t params_buffer_size = ggml_backend_buffer_get_size(params_buffer);\n        LOG_DEBUG(\"%s params backend buffer size = % 6.2f MB(%s) (%i tensors)\",\n                  get_desc().c_str(),\n                  params_buffer_size / (1024.0 * 1024.0),\n                  ggml_backend_is_cpu(backend) ? \"RAM\" : \"VRAM\",\n                  num_tensors);\n        // printf(\"%s params backend buffer size = % 6.2f MB(%s) (%i tensors)\\n\",\n        //           get_desc().c_str(),\n        //           params_buffer_size / (1024.0 * 1024.0),\n        //           ggml_backend_is_cpu(backend) ? \"RAM\" : \"VRAM\",\n        //           num_tensors);\n        return true;\n    }\n\n    void free_params_buffer() {\n        if (params_buffer != NULL) {\n            ggml_backend_buffer_free(params_buffer);\n            params_buffer = NULL;\n        }\n    }\n\n    size_t get_params_buffer_size() {\n        if (params_buffer != NULL) {\n            return ggml_backend_buffer_get_size(params_buffer);\n        }\n        return 0;\n    }\n\n    void free_compute_buffer() {\n        if (compute_allocr != NULL) {\n            ggml_gallocr_free(compute_allocr);\n            compute_allocr = NULL;\n        }\n    }\n\n    // do copy after alloc graph\n    void set_backend_tensor_data(struct ggml_tensor* tensor, const void* data) {\n        backend_tensor_data_map[tensor] = data;\n    }\n\n    struct ggml_tensor* to_backend(struct ggml_tensor* tensor) {\n        GGML_ASSERT(compute_ctx != NULL);\n        if (tensor == NULL) {\n            return NULL;\n        }\n        // it's performing a compute, check if backend isn't cpu\n        if (!ggml_backend_is_cpu(backend) && (tensor->buffer == NULL || ggml_backend_buffer_is_host(tensor->buffer))) {\n            // pass input tensors to gpu memory\n            auto backend_tensor = ggml_dup_tensor(compute_ctx, tensor);\n\n            set_backend_tensor_data(backend_tensor, tensor->data);\n            return backend_tensor;\n        } else {\n            return tensor;\n        }\n    }\n\n    void compute(get_graph_cb_t get_graph,\n                 int n_threads,\n                 bool free_compute_buffer_immediately = true,\n                 struct ggml_tensor** output          = NULL,\n                 struct ggml_context* output_ctx      = NULL) {\n        alloc_compute_buffer(get_graph);\n        reset_compute_ctx();\n        struct ggml_cgraph* gf = get_graph();\n        GGML_ASSERT(ggml_gallocr_alloc_graph(compute_allocr, gf));\n        cpy_data_to_backend_tensor();\n        if (ggml_backend_is_cpu(backend)) {\n            ggml_backend_cpu_set_n_threads(backend, n_threads);\n        }\n\n        ggml_backend_graph_compute(backend, gf);\n#ifdef GGML_PERF\n        ggml_graph_print(gf);\n#endif\n        if (output != NULL) {\n            auto result = ggml_graph_node(gf, -1);\n            if (*output == NULL && output_ctx != NULL) {\n                *output = ggml_dup_tensor(output_ctx, result);\n            }\n            if (*output != NULL) {\n                ggml_backend_tensor_get_and_sync(backend, result, (*output)->data, 0, ggml_nbytes(*output));\n            }\n        }\n\n        if (free_compute_buffer_immediately) {\n            free_compute_buffer();\n        }\n    }\n};\n\nclass GGMLBlock {\nprotected:\n    typedef std::unordered_map<std::string, struct ggml_tensor*> ParameterMap;\n    typedef std::unordered_map<std::string, std::shared_ptr<GGMLBlock>> GGMLBlockMap;\n    GGMLBlockMap blocks;\n    ParameterMap params;\n\n    void init_blocks(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix = \"\") {\n        for (auto& pair : blocks) {\n            auto& block = pair.second;\n            block->init(ctx, tensor_types, prefix + pair.first);\n        }\n    }\n\n    virtual void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix = \"\") {}\n\npublic:\n    void init(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, std::string prefix = \"\") {\n        if (prefix.size() > 0) {\n            prefix = prefix + \".\";\n        }\n        init_blocks(ctx, tensor_types, prefix);\n        init_params(ctx, tensor_types, prefix);\n    }\n\n    size_t get_params_num() {\n        size_t num_tensors = params.size();\n        for (auto& pair : blocks) {\n            auto& block = pair.second;\n\n            num_tensors += block->get_params_num();\n        }\n        return num_tensors;\n    };\n\n    size_t get_params_mem_size() {\n        size_t mem_size = 0;\n        for (auto& pair : blocks) {\n            auto& block = pair.second;\n\n            mem_size += block->get_params_mem_size();\n        }\n\n        for (auto& pair : params) {\n            mem_size += ggml_nbytes(pair.second);\n        }\n\n        return mem_size;\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors, std::string prefix = \"\") {\n        if (prefix.size() > 0) {\n            prefix = prefix + \".\";\n        }\n        for (auto& pair : blocks) {\n            auto& block = pair.second;\n            block->get_param_tensors(tensors, prefix + pair.first);\n        }\n\n        for (auto& pair : params) {\n            struct ggml_tensor* param    = pair.second;\n            tensors[prefix + pair.first] = pair.second;\n        }\n    }\n};\n\nclass UnaryBlock : public GGMLBlock {\npublic:\n    virtual struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) = 0;\n};\n\nclass Linear : public UnaryBlock {\nprotected:\n    int64_t in_features;\n    int64_t out_features;\n    bool bias;\n    bool force_f32;\n\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix = \"\") {\n        enum ggml_type wtype = (tensor_types.find(prefix + \"weight\") != tensor_types.end()) ? tensor_types[prefix + \"weight\"] : GGML_TYPE_F32;\n        if (in_features % ggml_blck_size(wtype) != 0 || force_f32) {\n            wtype = GGML_TYPE_F32;\n        }\n        params[\"weight\"] = ggml_new_tensor_2d(ctx, wtype, in_features, out_features);\n        if (bias) {\n            enum ggml_type wtype = GGML_TYPE_F32;  //(tensor_types.ypes.find(prefix + \"bias\") != tensor_types.end()) ? tensor_types[prefix + \"bias\"] : GGML_TYPE_F32;\n            params[\"bias\"]       = ggml_new_tensor_1d(ctx, wtype, out_features);\n        }\n    }\n\npublic:\n    Linear(int64_t in_features,\n           int64_t out_features,\n           bool bias      = true,\n           bool force_f32 = false)\n        : in_features(in_features),\n          out_features(out_features),\n          bias(bias),\n          force_f32(force_f32) {}\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        struct ggml_tensor* w = params[\"weight\"];\n        struct ggml_tensor* b = NULL;\n        if (bias) {\n            b = params[\"bias\"];\n        }\n        return ggml_nn_linear(ctx, x, w, b);\n    }\n};\n\nclass Embedding : public UnaryBlock {\nprotected:\n    int64_t embedding_dim;\n    int64_t num_embeddings;\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix = \"\") {\n        enum ggml_type wtype = (tensor_types.find(prefix + \"weight\") != tensor_types.end()) ? tensor_types[prefix + \"weight\"] : GGML_TYPE_F32;\n        params[\"weight\"]     = ggml_new_tensor_2d(ctx, wtype, embedding_dim, num_embeddings);\n    }\n\npublic:\n    Embedding(int64_t num_embeddings, int64_t embedding_dim)\n        : embedding_dim(embedding_dim),\n          num_embeddings(num_embeddings) {\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* input_ids) {\n        // input_ids: [N, n_token]\n        auto weight = params[\"weight\"];\n\n        // There are issues with ggml batch inference, so we are expanding it here first.\n        // TODO: fix ggml batch inference\n        int64_t n = input_ids->ne[1];\n        input_ids = ggml_reshape_1d(ctx, input_ids, input_ids->ne[0] * input_ids->ne[1]);\n\n        input_ids      = ggml_reshape_3d(ctx, input_ids, input_ids->ne[0], 1, input_ids->ne[1]);\n        auto embedding = ggml_get_rows(ctx, weight, input_ids);\n        embedding      = ggml_reshape_3d(ctx, embedding, embedding->ne[0], embedding->ne[1] / n, n);\n\n        // [N, n_token, embedding_dim]\n        return embedding;\n    }\n};\n\nclass Conv2d : public UnaryBlock {\nprotected:\n    int64_t in_channels;\n    int64_t out_channels;\n    std::pair<int, int> kernel_size;\n    std::pair<int, int> stride;\n    std::pair<int, int> padding;\n    std::pair<int, int> dilation;\n    bool bias;\n\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix = \"\") {\n        enum ggml_type wtype = GGML_TYPE_F16;  //(tensor_types.find(prefix + \"weight\") != tensor_types.end()) ? tensor_types[prefix + \"weight\"] : GGML_TYPE_F16;\n        params[\"weight\"]     = ggml_new_tensor_4d(ctx, wtype, kernel_size.second, kernel_size.first, in_channels, out_channels);\n        if (bias) {\n            enum ggml_type wtype = GGML_TYPE_F32;  // (tensor_types.find(prefix + \"bias\") != tensor_types.end()) ? tensor_types[prefix + \"bias\"] : GGML_TYPE_F32;\n            params[\"bias\"]       = ggml_new_tensor_1d(ctx, wtype, out_channels);\n        }\n    }\n\npublic:\n    Conv2d(int64_t in_channels,\n           int64_t out_channels,\n           std::pair<int, int> kernel_size,\n           std::pair<int, int> stride   = {1, 1},\n           std::pair<int, int> padding  = {0, 0},\n           std::pair<int, int> dilation = {1, 1},\n           bool bias                    = true)\n        : in_channels(in_channels),\n          out_channels(out_channels),\n          kernel_size(kernel_size),\n          stride(stride),\n          padding(padding),\n          dilation(dilation),\n          bias(bias) {}\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        struct ggml_tensor* w = params[\"weight\"];\n        struct ggml_tensor* b = NULL;\n        if (bias) {\n            b = params[\"bias\"];\n        }\n        return ggml_nn_conv_2d(ctx, x, w, b, stride.second, stride.first, padding.second, padding.first, dilation.second, dilation.first);\n    }\n};\n\nclass Conv3dnx1x1 : public UnaryBlock {\nprotected:\n    int64_t in_channels;\n    int64_t out_channels;\n    int64_t kernel_size;\n    int64_t stride;\n    int64_t padding;\n    int64_t dilation;\n    bool bias;\n\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix = \"\") {\n        enum ggml_type wtype = GGML_TYPE_F16;                                                              //(tensor_types.find(prefix + \"weight\") != tensor_types.end()) ? tensor_types[prefix + \"weight\"] : GGML_TYPE_F16;\n        params[\"weight\"]     = ggml_new_tensor_4d(ctx, wtype, 1, kernel_size, in_channels, out_channels);  // 5d => 4d\n        if (bias) {\n            enum ggml_type wtype = GGML_TYPE_F32;  //(tensor_types.find(prefix + \"bias\") != tensor_types.end()) ? tensor_types[prefix + \"bias\"] : GGML_TYPE_F32;\n            params[\"bias\"]       = ggml_new_tensor_1d(ctx, wtype, out_channels);\n        }\n    }\n\npublic:\n    Conv3dnx1x1(int64_t in_channels,\n                int64_t out_channels,\n                int64_t kernel_size,\n                int64_t stride   = 1,\n                int64_t padding  = 0,\n                int64_t dilation = 1,\n                bool bias        = true)\n        : in_channels(in_channels),\n          out_channels(out_channels),\n          kernel_size(kernel_size),\n          stride(stride),\n          padding(padding),\n          dilation(dilation),\n          bias(bias) {}\n\n    // x: [N, IC, ID, IH*IW]\n    // result: [N, OC, OD, OH*OW]\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        struct ggml_tensor* w = params[\"weight\"];\n        struct ggml_tensor* b = NULL;\n        if (bias) {\n            b = params[\"bias\"];\n        }\n        return ggml_nn_conv_3d_nx1x1(ctx, x, w, b, stride, padding, dilation);\n    }\n};\n\nclass LayerNorm : public UnaryBlock {\nprotected:\n    int64_t normalized_shape;\n    float eps;\n    bool elementwise_affine;\n    bool bias;\n\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix = \"\") {\n        if (elementwise_affine) {\n            enum ggml_type wtype = GGML_TYPE_F32;  //(tensor_types.ypes.find(prefix + \"weight\") != tensor_types.end()) ? tensor_types[prefix + \"weight\"] : GGML_TYPE_F32;\n            params[\"weight\"]     = ggml_new_tensor_1d(ctx, wtype, normalized_shape);\n            if (bias) {\n                enum ggml_type wtype = GGML_TYPE_F32;  //(tensor_types.ypes.find(prefix + \"bias\") != tensor_types.end()) ? tensor_types[prefix + \"bias\"] : GGML_TYPE_F32;\n                params[\"bias\"]       = ggml_new_tensor_1d(ctx, wtype, normalized_shape);\n            }\n        }\n    }\n\npublic:\n    LayerNorm(int64_t normalized_shape,\n              float eps               = 1e-05f,\n              bool elementwise_affine = true,\n              bool bias               = true)\n        : normalized_shape(normalized_shape),\n          eps(eps),\n          elementwise_affine(elementwise_affine),\n          bias(bias) {}\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        struct ggml_tensor* w = NULL;\n        struct ggml_tensor* b = NULL;\n\n        if (elementwise_affine) {\n            w = params[\"weight\"];\n            if (bias) {\n                b = params[\"bias\"];\n            }\n        }\n        return ggml_nn_layer_norm(ctx, x, w, b, eps);\n    }\n};\n\nclass GroupNorm : public GGMLBlock {\nprotected:\n    int64_t num_groups;\n    int64_t num_channels;\n    float eps;\n    bool affine;\n\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix = \"\") {\n        if (affine) {\n            enum ggml_type wtype      = GGML_TYPE_F32;  //(tensor_types.find(prefix + \"weight\") != tensor_types.end()) ? tensor_types[prefix + \"weight\"] : GGML_TYPE_F32;\n            enum ggml_type bias_wtype = GGML_TYPE_F32;  //(tensor_types.find(prefix + \"bias\") != tensor_types.end()) ? tensor_types[prefix + \"bias\"] : GGML_TYPE_F32;\n            params[\"weight\"]          = ggml_new_tensor_1d(ctx, wtype, num_channels);\n            params[\"bias\"]            = ggml_new_tensor_1d(ctx, bias_wtype, num_channels);\n        }\n    }\n\npublic:\n    GroupNorm(int64_t num_groups,\n              int64_t num_channels,\n              float eps   = 1e-05f,\n              bool affine = true)\n        : num_groups(num_groups),\n          num_channels(num_channels),\n          eps(eps),\n          affine(affine) {}\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        struct ggml_tensor* w = NULL;\n        struct ggml_tensor* b = NULL;\n        if (affine) {\n            w = params[\"weight\"];\n            b = params[\"bias\"];\n        }\n        return ggml_nn_group_norm(ctx, x, w, b, num_groups);\n    }\n};\n\nclass GroupNorm32 : public GroupNorm {\npublic:\n    GroupNorm32(int64_t num_channels)\n        : GroupNorm(32, num_channels, 1e-06f) {}\n};\n\nclass MultiheadAttention : public GGMLBlock {\nprotected:\n    int64_t embed_dim;\n    int64_t n_head;\n    std::string q_proj_name;\n    std::string k_proj_name;\n    std::string v_proj_name;\n    std::string out_proj_name;\n\npublic:\n    MultiheadAttention(int64_t embed_dim,\n                       int64_t n_head,\n                       bool qkv_proj_bias        = true,\n                       bool out_proj_bias        = true,\n                       std::string q_proj_name   = \"q_proj\",\n                       std::string k_proj_name   = \"k_proj\",\n                       std::string v_proj_name   = \"v_proj\",\n                       std::string out_proj_name = \"out_proj\")\n        : embed_dim(embed_dim),\n          n_head(n_head),\n          q_proj_name(q_proj_name),\n          k_proj_name(k_proj_name),\n          v_proj_name(v_proj_name),\n          out_proj_name(out_proj_name) {\n        blocks[q_proj_name]   = std::shared_ptr<GGMLBlock>(new Linear(embed_dim, embed_dim, qkv_proj_bias));\n        blocks[k_proj_name]   = std::shared_ptr<GGMLBlock>(new Linear(embed_dim, embed_dim, qkv_proj_bias));\n        blocks[v_proj_name]   = std::shared_ptr<GGMLBlock>(new Linear(embed_dim, embed_dim, qkv_proj_bias));\n        blocks[out_proj_name] = std::shared_ptr<GGMLBlock>(new Linear(embed_dim, embed_dim, out_proj_bias));\n    }\n\n    // x: [N, n_token, embed_dim]\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x, bool mask = false) {\n        auto q_proj   = std::dynamic_pointer_cast<Linear>(blocks[q_proj_name]);\n        auto k_proj   = std::dynamic_pointer_cast<Linear>(blocks[k_proj_name]);\n        auto v_proj   = std::dynamic_pointer_cast<Linear>(blocks[v_proj_name]);\n        auto out_proj = std::dynamic_pointer_cast<Linear>(blocks[out_proj_name]);\n\n        struct ggml_tensor* q = q_proj->forward(ctx, x);\n        struct ggml_tensor* k = k_proj->forward(ctx, x);\n        struct ggml_tensor* v = v_proj->forward(ctx, x);\n\n        x = ggml_nn_attention_ext(ctx, q, k, v, n_head, NULL, mask);  // [N, n_token, embed_dim]\n\n        x = out_proj->forward(ctx, x);  // [N, n_token, embed_dim]\n        return x;\n    }\n};\n\n#endif  // __GGML_EXTEND__HPP__\n"
        },
        {
          "name": "gits_noise.inl",
          "type": "blob",
          "size": 46.62109375,
          "content": "#ifndef GITS_NOISE_INL\n#define GITS_NOISE_INL\n\nconst std::vector<std::vector<float>> GITS_NOISE_0_80 = {\n    { 14.61464119f, 7.49001646f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 6.77309084f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 3.07277966f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.85520077f, 2.05039096f, 0.02916753f },\n    { 14.61464119f, 12.23089790f, 8.75849152f, 7.49001646f, 5.85520077f, 2.05039096f, 0.02916753f },\n    { 14.61464119f, 12.23089790f, 8.75849152f, 7.49001646f, 5.85520077f, 3.07277966f, 1.56271636f, 0.02916753f },\n    { 14.61464119f, 12.96784878f, 11.54541874f, 8.75849152f, 7.49001646f, 5.85520077f, 3.07277966f, 1.56271636f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.23089790f, 10.90732002f, 8.75849152f, 7.49001646f, 5.85520077f, 3.07277966f, 1.56271636f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 10.90732002f, 8.75849152f, 7.49001646f, 5.85520077f, 3.07277966f, 1.56271636f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 10.90732002f, 9.24142551f, 8.30717278f, 7.49001646f, 5.85520077f, 3.07277966f, 1.56271636f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 10.90732002f, 9.24142551f, 8.30717278f, 7.49001646f, 6.14220476f, 4.86714602f, 3.07277966f, 1.56271636f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.31284904f, 9.24142551f, 8.30717278f, 7.49001646f, 6.14220476f, 4.86714602f, 3.07277966f, 1.56271636f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.90732002f, 10.31284904f, 9.24142551f, 8.30717278f, 7.49001646f, 6.14220476f, 4.86714602f, 3.07277966f, 1.56271636f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.90732002f, 10.31284904f, 9.24142551f, 8.75849152f, 8.30717278f, 7.49001646f, 6.14220476f, 4.86714602f, 3.07277966f, 1.56271636f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.90732002f, 10.31284904f, 9.75859547f, 9.24142551f, 8.75849152f, 8.30717278f, 7.49001646f, 6.14220476f, 4.86714602f, 3.19567990f, 1.98035145f, 0.86115354f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.90732002f, 10.31284904f, 9.75859547f, 9.24142551f, 8.75849152f, 8.30717278f, 7.49001646f, 6.14220476f, 4.86714602f, 3.19567990f, 1.98035145f, 0.86115354f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.90732002f, 10.31284904f, 9.75859547f, 9.24142551f, 8.75849152f, 8.30717278f, 7.88507891f, 7.49001646f, 6.77309084f, 5.85520077f, 4.65472794f, 3.07277966f, 1.84880662f, 0.83188516f, 0.02916753f }\n};\n\nconst std::vector<std::vector<float>> GITS_NOISE_0_85 = {\n    { 14.61464119f, 7.49001646f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 1.84880662f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 6.77309084f, 1.56271636f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.11996698f, 3.07277966f, 1.24153244f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.09240818f, 2.84484982f, 0.95350921f, 0.02916753f },\n    { 14.61464119f, 12.23089790f, 8.75849152f, 7.49001646f, 5.09240818f, 2.84484982f, 0.95350921f, 0.02916753f },\n    { 14.61464119f, 12.23089790f, 8.75849152f, 7.49001646f, 5.58536053f, 3.19567990f, 1.84880662f, 0.803307f, 0.02916753f },\n    { 14.61464119f, 12.96784878f, 11.54541874f, 8.75849152f, 7.49001646f, 5.58536053f, 3.19567990f, 1.84880662f, 0.803307f, 0.02916753f },\n    { 14.61464119f, 12.96784878f, 11.54541874f, 8.75849152f, 7.49001646f, 6.14220476f, 4.65472794f, 3.07277966f, 1.84880662f, 0.803307f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.23089790f, 10.90732002f, 8.75849152f, 7.49001646f, 6.14220476f, 4.65472794f, 3.07277966f, 1.84880662f, 0.803307f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.23089790f, 10.90732002f, 9.24142551f, 8.30717278f, 7.49001646f, 6.14220476f, 4.65472794f, 3.07277966f, 1.84880662f, 0.803307f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 10.90732002f, 9.24142551f, 8.30717278f, 7.49001646f, 6.14220476f, 4.65472794f, 3.07277966f, 1.84880662f, 0.803307f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.31284904f, 9.24142551f, 8.30717278f, 7.49001646f, 6.14220476f, 4.65472794f, 3.07277966f, 1.84880662f, 0.803307f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.31284904f, 9.24142551f, 8.30717278f, 7.49001646f, 6.14220476f, 4.86714602f, 3.60512662f, 2.63833880f, 1.56271636f, 0.72133851f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.31284904f, 9.24142551f, 8.30717278f, 7.49001646f, 6.77309084f, 5.85520077f, 4.65472794f, 3.46139455f, 2.45070267f, 1.56271636f, 0.72133851f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.31284904f, 9.24142551f, 8.75849152f, 8.30717278f, 7.49001646f, 6.77309084f, 5.85520077f, 4.65472794f, 3.46139455f, 2.45070267f, 1.56271636f, 0.72133851f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.90732002f, 10.31284904f, 9.24142551f, 8.75849152f, 8.30717278f, 7.49001646f, 6.77309084f, 5.85520077f, 4.65472794f, 3.46139455f, 2.45070267f, 1.56271636f, 0.72133851f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.90732002f, 10.31284904f, 9.75859547f, 9.24142551f, 8.75849152f, 8.30717278f, 7.49001646f, 6.77309084f, 5.85520077f, 4.65472794f, 3.46139455f, 2.45070267f, 1.56271636f, 0.72133851f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.90732002f, 10.31284904f, 9.75859547f, 9.24142551f, 8.75849152f, 8.30717278f, 7.88507891f, 7.49001646f, 6.77309084f, 5.85520077f, 4.65472794f, 3.46139455f, 2.45070267f, 1.56271636f, 0.72133851f, 0.02916753f }\n};\n\nconst std::vector<std::vector<float>> GITS_NOISE_0_90 = {\n    { 14.61464119f, 6.77309084f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 1.56271636f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 3.07277966f, 0.95350921f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 2.54230714f, 0.89115214f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 4.86714602f, 2.54230714f, 0.89115214f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.09240818f, 3.07277966f, 1.61558151f, 0.69515091f, 0.02916753f },\n    { 14.61464119f, 12.23089790f, 8.75849152f, 7.11996698f, 4.86714602f, 3.07277966f, 1.61558151f, 0.69515091f, 0.02916753f },\n    { 14.61464119f, 12.23089790f, 8.75849152f, 7.49001646f, 5.85520077f, 4.45427561f, 2.95596409f, 1.61558151f, 0.69515091f, 0.02916753f },\n    { 14.61464119f, 12.23089790f, 8.75849152f, 7.49001646f, 5.85520077f, 4.45427561f, 3.19567990f, 2.19988537f, 1.24153244f, 0.57119018f, 0.02916753f },\n    { 14.61464119f, 12.96784878f, 10.90732002f, 8.75849152f, 7.49001646f, 5.85520077f, 4.45427561f, 3.19567990f, 2.19988537f, 1.24153244f, 0.57119018f, 0.02916753f },\n    { 14.61464119f, 12.96784878f, 11.54541874f, 9.24142551f, 8.30717278f, 7.49001646f, 5.85520077f, 4.45427561f, 3.19567990f, 2.19988537f, 1.24153244f, 0.57119018f, 0.02916753f },\n    { 14.61464119f, 12.96784878f, 11.54541874f, 9.24142551f, 8.30717278f, 7.49001646f, 6.14220476f, 4.86714602f, 3.75677586f, 2.84484982f, 1.84880662f, 1.08895338f, 0.52423614f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.23089790f, 10.90732002f, 9.24142551f, 8.30717278f, 7.49001646f, 6.14220476f, 4.86714602f, 3.75677586f, 2.84484982f, 1.84880662f, 1.08895338f, 0.52423614f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.23089790f, 10.90732002f, 9.24142551f, 8.30717278f, 7.49001646f, 6.44769001f, 5.58536053f, 4.45427561f, 3.32507086f, 2.45070267f, 1.61558151f, 0.95350921f, 0.45573691f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 10.90732002f, 9.24142551f, 8.30717278f, 7.49001646f, 6.44769001f, 5.58536053f, 4.45427561f, 3.32507086f, 2.45070267f, 1.61558151f, 0.95350921f, 0.45573691f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 10.90732002f, 9.24142551f, 8.30717278f, 7.49001646f, 6.77309084f, 5.85520077f, 4.86714602f, 3.91689563f, 3.07277966f, 2.27973175f, 1.56271636f, 0.95350921f, 0.45573691f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.31284904f, 9.24142551f, 8.30717278f, 7.49001646f, 6.77309084f, 5.85520077f, 4.86714602f, 3.91689563f, 3.07277966f, 2.27973175f, 1.56271636f, 0.95350921f, 0.45573691f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.31284904f, 9.24142551f, 8.75849152f, 8.30717278f, 7.49001646f, 6.77309084f, 5.85520077f, 4.86714602f, 3.91689563f, 3.07277966f, 2.27973175f, 1.56271636f, 0.95350921f, 0.45573691f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.96784878f, 12.23089790f, 11.54541874f, 10.31284904f, 9.24142551f, 8.75849152f, 8.30717278f, 7.49001646f, 6.77309084f, 5.85520077f, 5.09240818f, 4.45427561f, 3.60512662f, 2.95596409f, 2.19988537f, 1.51179266f, 0.89115214f, 0.43325692f, 0.02916753f }\n};\n\nconst std::vector<std::vector<float>> GITS_NOISE_0_95 = {\n    { 14.61464119f, 6.77309084f, 0.02916753f },\n    { 14.61464119f, 6.77309084f, 1.56271636f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 2.84484982f, 0.89115214f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 2.36326075f, 0.803307f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 2.95596409f, 1.56271636f, 0.64427125f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 4.86714602f, 2.95596409f, 1.56271636f, 0.64427125f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 4.86714602f, 3.07277966f, 1.91321158f, 1.08895338f, 0.50118381f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.85520077f, 4.45427561f, 3.07277966f, 1.91321158f, 1.08895338f, 0.50118381f, 0.02916753f },\n    { 14.61464119f, 12.23089790f, 8.75849152f, 7.49001646f, 5.85520077f, 4.45427561f, 3.07277966f, 1.91321158f, 1.08895338f, 0.50118381f, 0.02916753f },\n    { 14.61464119f, 12.23089790f, 8.75849152f, 7.49001646f, 5.85520077f, 4.45427561f, 3.19567990f, 2.19988537f, 1.41535246f, 0.803307f, 0.38853383f, 0.02916753f },\n    { 14.61464119f, 12.23089790f, 8.75849152f, 7.49001646f, 5.85520077f, 4.65472794f, 3.46139455f, 2.63833880f, 1.84880662f, 1.24153244f, 0.72133851f, 0.34370604f, 0.02916753f },\n    { 14.61464119f, 12.96784878f, 10.90732002f, 8.75849152f, 7.49001646f, 5.85520077f, 4.65472794f, 3.46139455f, 2.63833880f, 1.84880662f, 1.24153244f, 0.72133851f, 0.34370604f, 0.02916753f },\n    { 14.61464119f, 12.96784878f, 10.90732002f, 8.75849152f, 7.49001646f, 6.14220476f, 4.86714602f, 3.75677586f, 2.95596409f, 2.19988537f, 1.56271636f, 1.05362725f, 0.64427125f, 0.32104823f, 0.02916753f },\n    { 14.61464119f, 12.96784878f, 10.90732002f, 8.75849152f, 7.49001646f, 6.44769001f, 5.58536053f, 4.65472794f, 3.60512662f, 2.95596409f, 2.19988537f, 1.56271636f, 1.05362725f, 0.64427125f, 0.32104823f, 0.02916753f },\n    { 14.61464119f, 12.96784878f, 11.54541874f, 9.24142551f, 8.30717278f, 7.49001646f, 6.44769001f, 5.58536053f, 4.65472794f, 3.60512662f, 2.95596409f, 2.19988537f, 1.56271636f, 1.05362725f, 0.64427125f, 0.32104823f, 0.02916753f },\n    { 14.61464119f, 12.96784878f, 11.54541874f, 9.24142551f, 8.30717278f, 7.49001646f, 6.44769001f, 5.58536053f, 4.65472794f, 3.75677586f, 3.07277966f, 2.45070267f, 1.78698075f, 1.24153244f, 0.83188516f, 0.50118381f, 0.22545385f, 0.02916753f },\n    { 14.61464119f, 12.96784878f, 11.54541874f, 9.24142551f, 8.30717278f, 7.49001646f, 6.77309084f, 5.85520077f, 5.09240818f, 4.45427561f, 3.60512662f, 2.95596409f, 2.36326075f, 1.72759056f, 1.24153244f, 0.83188516f, 0.50118381f, 0.22545385f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.23089790f, 10.90732002f, 9.24142551f, 8.30717278f, 7.49001646f, 6.77309084f, 5.85520077f, 5.09240818f, 4.45427561f, 3.60512662f, 2.95596409f, 2.36326075f, 1.72759056f, 1.24153244f, 0.83188516f, 0.50118381f, 0.22545385f, 0.02916753f },\n    { 14.61464119f, 13.76078796f, 12.23089790f, 10.90732002f, 9.24142551f, 8.30717278f, 7.49001646f, 6.77309084f, 5.85520077f, 5.09240818f, 4.45427561f, 3.75677586f, 3.07277966f, 2.45070267f, 1.91321158f, 1.46270394f, 1.05362725f, 0.72133851f, 0.43325692f, 0.19894916f, 0.02916753f }\n};\n\nconst std::vector<std::vector<float>> GITS_NOISE_1_00 = {\n    { 14.61464119f, 1.56271636f, 0.02916753f },\n    { 14.61464119f, 6.77309084f, 0.95350921f, 0.02916753f },\n    { 14.61464119f, 6.77309084f, 2.36326075f, 0.803307f, 0.02916753f },\n    { 14.61464119f, 7.11996698f, 3.07277966f, 1.56271636f, 0.59516323f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 2.84484982f, 1.41535246f, 0.57119018f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 2.84484982f, 1.61558151f, 0.86115354f, 0.38853383f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 4.86714602f, 2.84484982f, 1.61558151f, 0.86115354f, 0.38853383f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 4.86714602f, 3.07277966f, 1.98035145f, 1.24153244f, 0.72133851f, 0.34370604f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.85520077f, 4.45427561f, 3.07277966f, 1.98035145f, 1.24153244f, 0.72133851f, 0.34370604f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.85520077f, 4.45427561f, 3.19567990f, 2.27973175f, 1.51179266f, 0.95350921f, 0.54755926f, 0.25053367f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.85520077f, 4.45427561f, 3.19567990f, 2.36326075f, 1.61558151f, 1.08895338f, 0.72133851f, 0.41087446f, 0.17026083f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 8.75849152f, 7.49001646f, 5.85520077f, 4.45427561f, 3.19567990f, 2.36326075f, 1.61558151f, 1.08895338f, 0.72133851f, 0.41087446f, 0.17026083f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 8.75849152f, 7.49001646f, 5.85520077f, 4.65472794f, 3.60512662f, 2.84484982f, 2.12350607f, 1.56271636f, 1.08895338f, 0.72133851f, 0.41087446f, 0.17026083f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 8.75849152f, 7.49001646f, 5.85520077f, 4.65472794f, 3.60512662f, 2.84484982f, 2.19988537f, 1.61558151f, 1.162866f, 0.803307f, 0.50118381f, 0.27464288f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 8.75849152f, 7.49001646f, 5.85520077f, 4.65472794f, 3.75677586f, 3.07277966f, 2.45070267f, 1.84880662f, 1.36964464f, 1.01931262f, 0.72133851f, 0.45573691f, 0.25053367f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 8.75849152f, 7.49001646f, 6.14220476f, 5.09240818f, 4.26497746f, 3.46139455f, 2.84484982f, 2.19988537f, 1.67050016f, 1.24153244f, 0.92192322f, 0.64427125f, 0.43325692f, 0.25053367f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 8.75849152f, 7.49001646f, 6.14220476f, 5.09240818f, 4.26497746f, 3.60512662f, 2.95596409f, 2.45070267f, 1.91321158f, 1.51179266f, 1.12534678f, 0.83188516f, 0.59516323f, 0.38853383f, 0.22545385f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 12.23089790f, 9.24142551f, 8.30717278f, 7.49001646f, 6.14220476f, 5.09240818f, 4.26497746f, 3.60512662f, 2.95596409f, 2.45070267f, 1.91321158f, 1.51179266f, 1.12534678f, 0.83188516f, 0.59516323f, 0.38853383f, 0.22545385f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 12.23089790f, 9.24142551f, 8.30717278f, 7.49001646f, 6.77309084f, 5.85520077f, 5.09240818f, 4.26497746f, 3.60512662f, 2.95596409f, 2.45070267f, 1.91321158f, 1.51179266f, 1.12534678f, 0.83188516f, 0.59516323f, 0.38853383f, 0.22545385f, 0.09824532f, 0.02916753f }\n};\n\nconst std::vector<std::vector<float>> GITS_NOISE_1_05 = {\n    { 14.61464119f, 0.95350921f, 0.02916753f },\n    { 14.61464119f, 6.77309084f, 0.89115214f, 0.02916753f },\n    { 14.61464119f, 6.77309084f, 2.05039096f, 0.72133851f, 0.02916753f },\n    { 14.61464119f, 6.77309084f, 2.84484982f, 1.28281462f, 0.52423614f, 0.02916753f },\n    { 14.61464119f, 6.77309084f, 3.07277966f, 1.61558151f, 0.803307f, 0.34370604f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 2.84484982f, 1.56271636f, 0.803307f, 0.34370604f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 2.84484982f, 1.61558151f, 0.95350921f, 0.52423614f, 0.22545385f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 3.07277966f, 1.98035145f, 1.24153244f, 0.74807048f, 0.41087446f, 0.17026083f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 3.19567990f, 2.27973175f, 1.51179266f, 0.95350921f, 0.59516323f, 0.34370604f, 0.13792117f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 5.09240818f, 3.46139455f, 2.45070267f, 1.61558151f, 1.08895338f, 0.72133851f, 0.45573691f, 0.25053367f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.09240818f, 3.46139455f, 2.45070267f, 1.61558151f, 1.08895338f, 0.72133851f, 0.45573691f, 0.25053367f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.85520077f, 4.45427561f, 3.19567990f, 2.36326075f, 1.61558151f, 1.08895338f, 0.72133851f, 0.45573691f, 0.25053367f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.85520077f, 4.45427561f, 3.19567990f, 2.45070267f, 1.72759056f, 1.24153244f, 0.86115354f, 0.59516323f, 0.38853383f, 0.22545385f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.85520077f, 4.65472794f, 3.60512662f, 2.84484982f, 2.19988537f, 1.61558151f, 1.162866f, 0.83188516f, 0.59516323f, 0.38853383f, 0.22545385f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.85520077f, 4.65472794f, 3.60512662f, 2.84484982f, 2.19988537f, 1.67050016f, 1.28281462f, 0.95350921f, 0.72133851f, 0.52423614f, 0.34370604f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.85520077f, 4.65472794f, 3.60512662f, 2.95596409f, 2.36326075f, 1.84880662f, 1.41535246f, 1.08895338f, 0.83188516f, 0.61951244f, 0.45573691f, 0.32104823f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.85520077f, 4.65472794f, 3.60512662f, 2.95596409f, 2.45070267f, 1.91321158f, 1.51179266f, 1.20157266f, 0.95350921f, 0.74807048f, 0.57119018f, 0.43325692f, 0.29807833f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 8.30717278f, 7.11996698f, 5.85520077f, 4.65472794f, 3.60512662f, 2.95596409f, 2.45070267f, 1.91321158f, 1.51179266f, 1.20157266f, 0.95350921f, 0.74807048f, 0.57119018f, 0.43325692f, 0.29807833f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 8.30717278f, 7.11996698f, 5.85520077f, 4.65472794f, 3.60512662f, 2.95596409f, 2.45070267f, 1.98035145f, 1.61558151f, 1.32549286f, 1.08895338f, 0.86115354f, 0.69515091f, 0.54755926f, 0.41087446f, 0.29807833f, 0.19894916f, 0.09824532f, 0.02916753f }\n};\n\nconst std::vector<std::vector<float>> GITS_NOISE_1_10 = {\n    { 14.61464119f, 0.89115214f, 0.02916753f },\n    { 14.61464119f, 2.36326075f, 0.72133851f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 1.61558151f, 0.57119018f, 0.02916753f },\n    { 14.61464119f, 6.77309084f, 2.45070267f, 1.08895338f, 0.45573691f, 0.02916753f },\n    { 14.61464119f, 6.77309084f, 2.95596409f, 1.56271636f, 0.803307f, 0.34370604f, 0.02916753f },\n    { 14.61464119f, 6.77309084f, 3.07277966f, 1.61558151f, 0.89115214f, 0.4783645f, 0.19894916f, 0.02916753f },\n    { 14.61464119f, 6.77309084f, 3.07277966f, 1.84880662f, 1.08895338f, 0.64427125f, 0.34370604f, 0.13792117f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 2.84484982f, 1.61558151f, 0.95350921f, 0.54755926f, 0.27464288f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 2.95596409f, 1.91321158f, 1.24153244f, 0.803307f, 0.4783645f, 0.25053367f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 3.07277966f, 2.05039096f, 1.41535246f, 0.95350921f, 0.64427125f, 0.41087446f, 0.22545385f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 3.19567990f, 2.27973175f, 1.61558151f, 1.12534678f, 0.803307f, 0.54755926f, 0.36617002f, 0.22545385f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 3.32507086f, 2.45070267f, 1.72759056f, 1.24153244f, 0.89115214f, 0.64427125f, 0.45573691f, 0.32104823f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 5.09240818f, 3.60512662f, 2.84484982f, 2.05039096f, 1.51179266f, 1.08895338f, 0.803307f, 0.59516323f, 0.43325692f, 0.29807833f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 5.09240818f, 3.60512662f, 2.84484982f, 2.12350607f, 1.61558151f, 1.24153244f, 0.95350921f, 0.72133851f, 0.54755926f, 0.41087446f, 0.29807833f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 5.85520077f, 4.45427561f, 3.19567990f, 2.45070267f, 1.84880662f, 1.41535246f, 1.08895338f, 0.83188516f, 0.64427125f, 0.50118381f, 0.36617002f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 5.85520077f, 4.45427561f, 3.19567990f, 2.45070267f, 1.91321158f, 1.51179266f, 1.20157266f, 0.95350921f, 0.74807048f, 0.59516323f, 0.45573691f, 0.34370604f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 5.85520077f, 4.45427561f, 3.46139455f, 2.84484982f, 2.19988537f, 1.72759056f, 1.36964464f, 1.08895338f, 0.86115354f, 0.69515091f, 0.54755926f, 0.43325692f, 0.34370604f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.85520077f, 4.45427561f, 3.46139455f, 2.84484982f, 2.19988537f, 1.72759056f, 1.36964464f, 1.08895338f, 0.86115354f, 0.69515091f, 0.54755926f, 0.43325692f, 0.34370604f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 11.54541874f, 7.49001646f, 5.85520077f, 4.45427561f, 3.46139455f, 2.84484982f, 2.19988537f, 1.72759056f, 1.36964464f, 1.08895338f, 0.89115214f, 0.72133851f, 0.59516323f, 0.4783645f, 0.38853383f, 0.29807833f, 0.22545385f, 0.17026083f, 0.09824532f, 0.02916753f }\n};\n\nconst std::vector<std::vector<float>> GITS_NOISE_1_15 = {\n    { 14.61464119f, 0.83188516f, 0.02916753f },\n    { 14.61464119f, 1.84880662f, 0.59516323f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 1.56271636f, 0.52423614f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 1.91321158f, 0.83188516f, 0.34370604f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.45070267f, 1.24153244f, 0.59516323f, 0.25053367f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.84484982f, 1.51179266f, 0.803307f, 0.41087446f, 0.17026083f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.84484982f, 1.56271636f, 0.89115214f, 0.50118381f, 0.25053367f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 6.77309084f, 3.07277966f, 1.84880662f, 1.12534678f, 0.72133851f, 0.43325692f, 0.22545385f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 6.77309084f, 3.07277966f, 1.91321158f, 1.24153244f, 0.803307f, 0.52423614f, 0.34370604f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 2.95596409f, 1.91321158f, 1.24153244f, 0.803307f, 0.52423614f, 0.34370604f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 3.07277966f, 2.05039096f, 1.36964464f, 0.95350921f, 0.69515091f, 0.4783645f, 0.32104823f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 3.07277966f, 2.12350607f, 1.51179266f, 1.08895338f, 0.803307f, 0.59516323f, 0.43325692f, 0.29807833f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 3.07277966f, 2.12350607f, 1.51179266f, 1.08895338f, 0.803307f, 0.59516323f, 0.45573691f, 0.34370604f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 3.07277966f, 2.19988537f, 1.61558151f, 1.24153244f, 0.95350921f, 0.74807048f, 0.59516323f, 0.45573691f, 0.34370604f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 3.19567990f, 2.45070267f, 1.78698075f, 1.32549286f, 1.01931262f, 0.803307f, 0.64427125f, 0.50118381f, 0.38853383f, 0.29807833f, 0.22545385f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 3.19567990f, 2.45070267f, 1.78698075f, 1.32549286f, 1.01931262f, 0.803307f, 0.64427125f, 0.52423614f, 0.41087446f, 0.32104823f, 0.25053367f, 0.19894916f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 3.19567990f, 2.45070267f, 1.84880662f, 1.41535246f, 1.12534678f, 0.89115214f, 0.72133851f, 0.59516323f, 0.4783645f, 0.38853383f, 0.32104823f, 0.25053367f, 0.19894916f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.86714602f, 3.19567990f, 2.45070267f, 1.84880662f, 1.41535246f, 1.12534678f, 0.89115214f, 0.72133851f, 0.59516323f, 0.50118381f, 0.41087446f, 0.34370604f, 0.29807833f, 0.25053367f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f }\n};\n\nconst std::vector<std::vector<float>> GITS_NOISE_1_20 = {\n    { 14.61464119f, 0.803307f, 0.02916753f },\n    { 14.61464119f, 1.56271636f, 0.52423614f, 0.02916753f },\n    { 14.61464119f, 2.36326075f, 0.92192322f, 0.36617002f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.24153244f, 0.59516323f, 0.25053367f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.05039096f, 0.95350921f, 0.45573691f, 0.17026083f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.45070267f, 1.24153244f, 0.64427125f, 0.29807833f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.45070267f, 1.36964464f, 0.803307f, 0.45573691f, 0.25053367f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.84484982f, 1.61558151f, 0.95350921f, 0.59516323f, 0.36617002f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.84484982f, 1.67050016f, 1.08895338f, 0.74807048f, 0.50118381f, 0.32104823f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.95596409f, 1.84880662f, 1.24153244f, 0.83188516f, 0.59516323f, 0.41087446f, 0.27464288f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 3.07277966f, 1.98035145f, 1.36964464f, 0.95350921f, 0.69515091f, 0.50118381f, 0.36617002f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 6.77309084f, 3.46139455f, 2.36326075f, 1.56271636f, 1.08895338f, 0.803307f, 0.59516323f, 0.45573691f, 0.34370604f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 6.77309084f, 3.46139455f, 2.45070267f, 1.61558151f, 1.162866f, 0.86115354f, 0.64427125f, 0.50118381f, 0.38853383f, 0.29807833f, 0.22545385f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.65472794f, 3.07277966f, 2.12350607f, 1.51179266f, 1.08895338f, 0.83188516f, 0.64427125f, 0.50118381f, 0.38853383f, 0.29807833f, 0.22545385f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.65472794f, 3.07277966f, 2.12350607f, 1.51179266f, 1.08895338f, 0.83188516f, 0.64427125f, 0.50118381f, 0.41087446f, 0.32104823f, 0.25053367f, 0.19894916f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.65472794f, 3.07277966f, 2.12350607f, 1.51179266f, 1.08895338f, 0.83188516f, 0.64427125f, 0.50118381f, 0.41087446f, 0.34370604f, 0.27464288f, 0.22545385f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.65472794f, 3.07277966f, 2.19988537f, 1.61558151f, 1.20157266f, 0.92192322f, 0.72133851f, 0.57119018f, 0.45573691f, 0.36617002f, 0.29807833f, 0.25053367f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.65472794f, 3.07277966f, 2.19988537f, 1.61558151f, 1.24153244f, 0.95350921f, 0.74807048f, 0.59516323f, 0.4783645f, 0.38853383f, 0.32104823f, 0.27464288f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 7.49001646f, 4.65472794f, 3.07277966f, 2.19988537f, 1.61558151f, 1.24153244f, 0.95350921f, 0.74807048f, 0.59516323f, 0.50118381f, 0.41087446f, 0.34370604f, 0.29807833f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f }\n};\n\nconst std::vector<std::vector<float>> GITS_NOISE_1_25 = {\n    { 14.61464119f, 0.72133851f, 0.02916753f },\n    { 14.61464119f, 1.56271636f, 0.50118381f, 0.02916753f },\n    { 14.61464119f, 2.05039096f, 0.803307f, 0.32104823f, 0.02916753f },\n    { 14.61464119f, 2.36326075f, 0.95350921f, 0.43325692f, 0.17026083f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.24153244f, 0.59516323f, 0.27464288f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 3.07277966f, 1.51179266f, 0.803307f, 0.43325692f, 0.22545385f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.36326075f, 1.24153244f, 0.72133851f, 0.41087446f, 0.22545385f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.45070267f, 1.36964464f, 0.83188516f, 0.52423614f, 0.34370604f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.84484982f, 1.61558151f, 0.98595673f, 0.64427125f, 0.43325692f, 0.27464288f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.84484982f, 1.67050016f, 1.08895338f, 0.74807048f, 0.52423614f, 0.36617002f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.84484982f, 1.72759056f, 1.162866f, 0.803307f, 0.59516323f, 0.45573691f, 0.34370604f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.95596409f, 1.84880662f, 1.24153244f, 0.86115354f, 0.64427125f, 0.4783645f, 0.36617002f, 0.27464288f, 0.19894916f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.95596409f, 1.84880662f, 1.28281462f, 0.92192322f, 0.69515091f, 0.52423614f, 0.41087446f, 0.32104823f, 0.25053367f, 0.19894916f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.95596409f, 1.91321158f, 1.32549286f, 0.95350921f, 0.72133851f, 0.54755926f, 0.43325692f, 0.34370604f, 0.27464288f, 0.22545385f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.95596409f, 1.91321158f, 1.32549286f, 0.95350921f, 0.72133851f, 0.57119018f, 0.45573691f, 0.36617002f, 0.29807833f, 0.25053367f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.95596409f, 1.91321158f, 1.32549286f, 0.95350921f, 0.74807048f, 0.59516323f, 0.4783645f, 0.38853383f, 0.32104823f, 0.27464288f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 3.07277966f, 2.05039096f, 1.41535246f, 1.05362725f, 0.803307f, 0.61951244f, 0.50118381f, 0.41087446f, 0.34370604f, 0.29807833f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 3.07277966f, 2.05039096f, 1.41535246f, 1.05362725f, 0.803307f, 0.64427125f, 0.52423614f, 0.43325692f, 0.36617002f, 0.32104823f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 3.07277966f, 2.05039096f, 1.46270394f, 1.08895338f, 0.83188516f, 0.66947293f, 0.54755926f, 0.45573691f, 0.38853383f, 0.34370604f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f }\n};\n\nconst std::vector<std::vector<float>> GITS_NOISE_1_30 = {\n    { 14.61464119f, 0.72133851f, 0.02916753f },\n    { 14.61464119f, 1.24153244f, 0.43325692f, 0.02916753f },\n    { 14.61464119f, 1.56271636f, 0.59516323f, 0.22545385f, 0.02916753f },\n    { 14.61464119f, 1.84880662f, 0.803307f, 0.36617002f, 0.13792117f, 0.02916753f },\n    { 14.61464119f, 2.36326075f, 1.01931262f, 0.52423614f, 0.25053367f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.36964464f, 0.74807048f, 0.41087446f, 0.22545385f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 3.07277966f, 1.56271636f, 0.89115214f, 0.54755926f, 0.34370604f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 3.07277966f, 1.61558151f, 0.95350921f, 0.61951244f, 0.41087446f, 0.27464288f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.45070267f, 1.36964464f, 0.83188516f, 0.54755926f, 0.36617002f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.45070267f, 1.41535246f, 0.92192322f, 0.64427125f, 0.45573691f, 0.34370604f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.6383388f, 1.56271636f, 1.01931262f, 0.72133851f, 0.50118381f, 0.36617002f, 0.27464288f, 0.19894916f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.84484982f, 1.61558151f, 1.05362725f, 0.74807048f, 0.54755926f, 0.41087446f, 0.32104823f, 0.25053367f, 0.19894916f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.84484982f, 1.61558151f, 1.08895338f, 0.77538133f, 0.57119018f, 0.43325692f, 0.34370604f, 0.27464288f, 0.22545385f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.84484982f, 1.61558151f, 1.08895338f, 0.803307f, 0.59516323f, 0.45573691f, 0.36617002f, 0.29807833f, 0.25053367f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.84484982f, 1.61558151f, 1.08895338f, 0.803307f, 0.59516323f, 0.4783645f, 0.38853383f, 0.32104823f, 0.27464288f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.84484982f, 1.72759056f, 1.162866f, 0.83188516f, 0.64427125f, 0.50118381f, 0.41087446f, 0.34370604f, 0.29807833f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.84484982f, 1.72759056f, 1.162866f, 0.83188516f, 0.64427125f, 0.52423614f, 0.43325692f, 0.36617002f, 0.32104823f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.84484982f, 1.78698075f, 1.24153244f, 0.92192322f, 0.72133851f, 0.57119018f, 0.45573691f, 0.38853383f, 0.34370604f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.84484982f, 1.78698075f, 1.24153244f, 0.92192322f, 0.72133851f, 0.57119018f, 0.4783645f, 0.41087446f, 0.36617002f, 0.32104823f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f }\n};\n\nconst std::vector<std::vector<float>> GITS_NOISE_1_35 = {\n    { 14.61464119f, 0.69515091f, 0.02916753f },\n    { 14.61464119f, 0.95350921f, 0.34370604f, 0.02916753f },\n    { 14.61464119f, 1.56271636f, 0.57119018f, 0.19894916f, 0.02916753f },\n    { 14.61464119f, 1.61558151f, 0.69515091f, 0.29807833f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 1.84880662f, 0.83188516f, 0.43325692f, 0.22545385f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.45070267f, 1.162866f, 0.64427125f, 0.36617002f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.36964464f, 0.803307f, 0.50118381f, 0.32104823f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.41535246f, 0.83188516f, 0.54755926f, 0.36617002f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.56271636f, 0.95350921f, 0.64427125f, 0.45573691f, 0.32104823f, 0.22545385f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.56271636f, 0.95350921f, 0.64427125f, 0.45573691f, 0.34370604f, 0.25053367f, 0.19894916f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 3.07277966f, 1.61558151f, 1.01931262f, 0.72133851f, 0.52423614f, 0.38853383f, 0.29807833f, 0.22545385f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 3.07277966f, 1.61558151f, 1.01931262f, 0.72133851f, 0.52423614f, 0.41087446f, 0.32104823f, 0.25053367f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 3.07277966f, 1.61558151f, 1.05362725f, 0.74807048f, 0.54755926f, 0.43325692f, 0.34370604f, 0.27464288f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 3.07277966f, 1.72759056f, 1.12534678f, 0.803307f, 0.59516323f, 0.45573691f, 0.36617002f, 0.29807833f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 3.07277966f, 1.72759056f, 1.12534678f, 0.803307f, 0.59516323f, 0.4783645f, 0.38853383f, 0.32104823f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.45070267f, 1.51179266f, 1.01931262f, 0.74807048f, 0.57119018f, 0.45573691f, 0.36617002f, 0.32104823f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.6383388f, 1.61558151f, 1.08895338f, 0.803307f, 0.61951244f, 0.50118381f, 0.41087446f, 0.34370604f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.6383388f, 1.61558151f, 1.08895338f, 0.803307f, 0.64427125f, 0.52423614f, 0.43325692f, 0.36617002f, 0.32104823f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 5.85520077f, 2.6383388f, 1.61558151f, 1.08895338f, 0.803307f, 0.64427125f, 0.52423614f, 0.45573691f, 0.38853383f, 0.34370604f, 0.32104823f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f }\n};\n\nconst std::vector<std::vector<float>> GITS_NOISE_1_40 = {\n    { 14.61464119f, 0.59516323f, 0.02916753f },\n    { 14.61464119f, 0.95350921f, 0.34370604f, 0.02916753f },\n    { 14.61464119f, 1.08895338f, 0.43325692f, 0.13792117f, 0.02916753f },\n    { 14.61464119f, 1.56271636f, 0.64427125f, 0.27464288f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 1.61558151f, 0.803307f, 0.43325692f, 0.22545385f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.05039096f, 0.95350921f, 0.54755926f, 0.34370604f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.45070267f, 1.24153244f, 0.72133851f, 0.43325692f, 0.27464288f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.45070267f, 1.24153244f, 0.74807048f, 0.50118381f, 0.34370604f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.45070267f, 1.28281462f, 0.803307f, 0.52423614f, 0.36617002f, 0.27464288f, 0.19894916f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.45070267f, 1.28281462f, 0.803307f, 0.54755926f, 0.38853383f, 0.29807833f, 0.22545385f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.41535246f, 0.86115354f, 0.59516323f, 0.43325692f, 0.32104823f, 0.25053367f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.51179266f, 0.95350921f, 0.64427125f, 0.45573691f, 0.34370604f, 0.27464288f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.51179266f, 0.95350921f, 0.64427125f, 0.4783645f, 0.36617002f, 0.29807833f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.56271636f, 0.98595673f, 0.69515091f, 0.52423614f, 0.41087446f, 0.34370604f, 0.29807833f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.56271636f, 1.01931262f, 0.72133851f, 0.54755926f, 0.43325692f, 0.36617002f, 0.32104823f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.61558151f, 1.05362725f, 0.74807048f, 0.57119018f, 0.45573691f, 0.38853383f, 0.34370604f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.61558151f, 1.08895338f, 0.803307f, 0.61951244f, 0.50118381f, 0.41087446f, 0.36617002f, 0.32104823f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.61558151f, 1.08895338f, 0.803307f, 0.61951244f, 0.50118381f, 0.43325692f, 0.38853383f, 0.34370604f, 0.32104823f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.61558151f, 1.08895338f, 0.803307f, 0.64427125f, 0.52423614f, 0.45573691f, 0.41087446f, 0.36617002f, 0.34370604f, 0.32104823f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f }\n};\n\nconst std::vector<std::vector<float>> GITS_NOISE_1_45 = {\n    { 14.61464119f, 0.59516323f, 0.02916753f },\n    { 14.61464119f, 0.803307f, 0.25053367f, 0.02916753f },\n    { 14.61464119f, 0.95350921f, 0.34370604f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 1.24153244f, 0.54755926f, 0.25053367f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 1.56271636f, 0.72133851f, 0.36617002f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 1.61558151f, 0.803307f, 0.45573691f, 0.27464288f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 1.91321158f, 0.95350921f, 0.57119018f, 0.36617002f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.19988537f, 1.08895338f, 0.64427125f, 0.41087446f, 0.27464288f, 0.19894916f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.45070267f, 1.24153244f, 0.74807048f, 0.50118381f, 0.34370604f, 0.25053367f, 0.19894916f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.45070267f, 1.24153244f, 0.74807048f, 0.50118381f, 0.36617002f, 0.27464288f, 0.22545385f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.45070267f, 1.28281462f, 0.803307f, 0.54755926f, 0.41087446f, 0.32104823f, 0.25053367f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.45070267f, 1.28281462f, 0.803307f, 0.57119018f, 0.43325692f, 0.34370604f, 0.27464288f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.45070267f, 1.28281462f, 0.83188516f, 0.59516323f, 0.45573691f, 0.36617002f, 0.29807833f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.45070267f, 1.28281462f, 0.83188516f, 0.59516323f, 0.45573691f, 0.36617002f, 0.32104823f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.51179266f, 0.95350921f, 0.69515091f, 0.52423614f, 0.41087446f, 0.34370604f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.51179266f, 0.95350921f, 0.69515091f, 0.52423614f, 0.43325692f, 0.36617002f, 0.32104823f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.56271636f, 0.98595673f, 0.72133851f, 0.54755926f, 0.45573691f, 0.38853383f, 0.34370604f, 0.32104823f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.56271636f, 1.01931262f, 0.74807048f, 0.57119018f, 0.4783645f, 0.41087446f, 0.36617002f, 0.34370604f, 0.32104823f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.84484982f, 1.56271636f, 1.01931262f, 0.74807048f, 0.59516323f, 0.50118381f, 0.43325692f, 0.38853383f, 0.36617002f, 0.34370604f, 0.32104823f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f }\n};\n\nconst std::vector<std::vector<float>> GITS_NOISE_1_50 = {\n    { 14.61464119f, 0.54755926f, 0.02916753f },\n    { 14.61464119f, 0.803307f, 0.25053367f, 0.02916753f },\n    { 14.61464119f, 0.86115354f, 0.32104823f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 1.24153244f, 0.54755926f, 0.25053367f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 1.56271636f, 0.72133851f, 0.36617002f, 0.19894916f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 1.61558151f, 0.803307f, 0.45573691f, 0.27464288f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 1.61558151f, 0.83188516f, 0.52423614f, 0.34370604f, 0.25053367f, 0.17026083f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 1.84880662f, 0.95350921f, 0.59516323f, 0.38853383f, 0.27464288f, 0.19894916f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 1.84880662f, 0.95350921f, 0.59516323f, 0.41087446f, 0.29807833f, 0.22545385f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 1.84880662f, 0.95350921f, 0.61951244f, 0.43325692f, 0.32104823f, 0.25053367f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.19988537f, 1.12534678f, 0.72133851f, 0.50118381f, 0.36617002f, 0.27464288f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.19988537f, 1.12534678f, 0.72133851f, 0.50118381f, 0.36617002f, 0.29807833f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.36326075f, 1.24153244f, 0.803307f, 0.57119018f, 0.43325692f, 0.34370604f, 0.29807833f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.36326075f, 1.24153244f, 0.803307f, 0.57119018f, 0.43325692f, 0.34370604f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.36326075f, 1.24153244f, 0.803307f, 0.59516323f, 0.45573691f, 0.36617002f, 0.32104823f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.36326075f, 1.24153244f, 0.803307f, 0.59516323f, 0.45573691f, 0.38853383f, 0.34370604f, 0.32104823f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.45070267f, 1.32549286f, 0.86115354f, 0.64427125f, 0.50118381f, 0.41087446f, 0.36617002f, 0.34370604f, 0.32104823f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.45070267f, 1.36964464f, 0.92192322f, 0.69515091f, 0.54755926f, 0.45573691f, 0.41087446f, 0.36617002f, 0.34370604f, 0.32104823f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f },\n    { 14.61464119f, 2.45070267f, 1.41535246f, 0.95350921f, 0.72133851f, 0.57119018f, 0.4783645f, 0.43325692f, 0.38853383f, 0.36617002f, 0.34370604f, 0.32104823f, 0.29807833f, 0.27464288f, 0.25053367f, 0.22545385f, 0.19894916f, 0.17026083f, 0.13792117f, 0.09824532f, 0.02916753f }\n};\n\nconst std::vector<const std::vector<std::vector<float>>*> GITS_NOISE = {\n    { &GITS_NOISE_0_80 },\n    { &GITS_NOISE_0_85 },\n    { &GITS_NOISE_0_90 },\n    { &GITS_NOISE_0_95 },\n    { &GITS_NOISE_1_00 },\n    { &GITS_NOISE_1_05 },\n    { &GITS_NOISE_1_10 },\n    { &GITS_NOISE_1_15 },\n    { &GITS_NOISE_1_20 },\n    { &GITS_NOISE_1_25 },\n    { &GITS_NOISE_1_30 },\n    { &GITS_NOISE_1_35 },\n    { &GITS_NOISE_1_40 },\n    { &GITS_NOISE_1_45 },\n    { &GITS_NOISE_1_50 }\n};\n\n#endif // GITS_NOISE_INL\n"
        },
        {
          "name": "lora.hpp",
          "type": "blob",
          "size": 34.587890625,
          "content": "#ifndef __LORA_HPP__\n#define __LORA_HPP__\n\n#include \"ggml_extend.hpp\"\n\n#define LORA_GRAPH_SIZE 10240\n\nstruct LoraModel : public GGMLRunner {\n    enum lora_t {\n        REGULAR      = 0,\n        DIFFUSERS    = 1,\n        DIFFUSERS_2  = 2,\n        DIFFUSERS_3  = 3,\n        TRANSFORMERS = 4,\n        LORA_TYPE_COUNT\n    };\n\n    const std::string lora_ups[LORA_TYPE_COUNT] = {\n        \".lora_up\",\n        \"_lora.up\",\n        \".lora_B\",\n        \".lora.up\",\n        \".lora_linear_layer.up\",\n    };\n\n    const std::string lora_downs[LORA_TYPE_COUNT] = {\n        \".lora_down\",\n        \"_lora.down\",\n        \".lora_A\",\n        \".lora.down\",\n        \".lora_linear_layer.down\",\n    };\n\n    const std::string lora_pre[LORA_TYPE_COUNT] = {\n        \"lora.\",\n        \"\",\n        \"\",\n        \"\",\n        \"\",\n    };\n\n    const std::map<std::string, std::string> alt_names = {\n        // mmdit\n        {\"final_layer.adaLN_modulation.1\", \"norm_out.linear\"},\n        {\"pos_embed\", \"pos_embed.proj\"},\n        {\"final_layer.linear\", \"proj_out\"},\n        {\"y_embedder.mlp.0\", \"time_text_embed.text_embedder.linear_1\"},\n        {\"y_embedder.mlp.2\", \"time_text_embed.text_embedder.linear_2\"},\n        {\"t_embedder.mlp.0\", \"time_text_embed.timestep_embedder.linear_1\"},\n        {\"t_embedder.mlp.2\", \"time_text_embed.timestep_embedder.linear_2\"},\n        {\"x_block.mlp.fc1\", \"ff.net.0.proj\"},\n        {\"x_block.mlp.fc2\", \"ff.net.2\"},\n        {\"context_block.mlp.fc1\", \"ff_context.net.0.proj\"},\n        {\"context_block.mlp.fc2\", \"ff_context.net.2\"},\n        {\"x_block.adaLN_modulation.1\", \"norm1.linear\"},\n        {\"context_block.adaLN_modulation.1\", \"norm1_context.linear\"},\n        {\"context_block.attn.proj\", \"attn.to_add_out\"},\n        {\"x_block.attn.proj\", \"attn.to_out.0\"},\n        {\"x_block.attn2.proj\", \"attn2.to_out.0\"},\n        // flux\n        // singlestream\n        {\"linear2\", \"proj_out\"},\n        {\"modulation.lin\", \"norm.linear\"},\n        // doublestream\n        {\"txt_attn.proj\", \"attn.to_add_out\"},\n        {\"img_attn.proj\", \"attn.to_out.0\"},\n        {\"txt_mlp.0\", \"ff_context.net.0.proj\"},\n        {\"txt_mlp.2\", \"ff_context.net.2\"},\n        {\"img_mlp.0\", \"ff.net.0.proj\"},\n        {\"img_mlp.2\", \"ff.net.2\"},\n        {\"txt_mod.lin\", \"norm1_context.linear\"},\n        {\"img_mod.lin\", \"norm1.linear\"},\n    };\n\n    const std::map<std::string, std::string> qkv_prefixes = {\n        // mmdit\n        {\"context_block.attn.qkv\", \"attn.add_\"},  // suffix \"_proj\"\n        {\"x_block.attn.qkv\", \"attn.to_\"},\n        {\"x_block.attn2.qkv\", \"attn2.to_\"},\n        // flux\n        // doublestream\n        {\"txt_attn.qkv\", \"attn.add_\"},  // suffix \"_proj\"\n        {\"img_attn.qkv\", \"attn.to_\"},\n    };\n    const std::map<std::string, std::string> qkvm_prefixes = {\n        // flux\n        // singlestream\n        {\"linear1\", \"\"},\n    };\n\n    const std::string* type_fingerprints = lora_ups;\n\n    float multiplier = 1.0f;\n    std::map<std::string, struct ggml_tensor*> lora_tensors;\n    std::string file_path;\n    ModelLoader model_loader;\n    bool load_failed                = false;\n    bool applied                    = false;\n    std::vector<int> zero_index_vec = {0};\n    ggml_tensor* zero_index         = NULL;\n    enum lora_t type                = REGULAR;\n\n    LoraModel(ggml_backend_t backend,\n              const std::string& file_path = \"\",\n              const std::string prefix     = \"\")\n        : file_path(file_path), GGMLRunner(backend) {\n        if (!model_loader.init_from_file(file_path, prefix)) {\n            load_failed = true;\n        }\n    }\n\n    std::string get_desc() {\n        return \"lora\";\n    }\n\n    bool load_from_file(bool filter_tensor = false) {\n        LOG_INFO(\"loading LoRA from '%s'\", file_path.c_str());\n\n        if (load_failed) {\n            LOG_ERROR(\"init lora model loader from file failed: '%s'\", file_path.c_str());\n            return false;\n        }\n\n        bool dry_run          = true;\n        auto on_new_tensor_cb = [&](const TensorStorage& tensor_storage, ggml_tensor** dst_tensor) -> bool {\n            const std::string& name = tensor_storage.name;\n\n            if (filter_tensor && !contains(name, \"lora\")) {\n                // LOG_INFO(\"skipping LoRA tesnor '%s'\", name.c_str());\n                return true;\n            }\n            // LOG_INFO(\"%s\", name.c_str());\n            for (int i = 0; i < LORA_TYPE_COUNT; i++) {\n                if (name.find(type_fingerprints[i]) != std::string::npos) {\n                    type = (lora_t)i;\n                    break;\n                }\n            }\n\n            if (dry_run) {\n                struct ggml_tensor* real = ggml_new_tensor(params_ctx,\n                                                           tensor_storage.type,\n                                                           tensor_storage.n_dims,\n                                                           tensor_storage.ne);\n                lora_tensors[name]       = real;\n            } else {\n                auto real   = lora_tensors[name];\n                *dst_tensor = real;\n            }\n\n            return true;\n        };\n\n        model_loader.load_tensors(on_new_tensor_cb, backend);\n        alloc_params_buffer();\n        // exit(0);\n        dry_run = false;\n        model_loader.load_tensors(on_new_tensor_cb, backend);\n\n        LOG_DEBUG(\"lora type: \\\"%s\\\"/\\\"%s\\\"\", lora_downs[type].c_str(), lora_ups[type].c_str());\n\n        LOG_DEBUG(\"finished loaded lora\");\n        return true;\n    }\n\n    ggml_tensor* to_f32(ggml_context* ctx, ggml_tensor* a) {\n        auto out = ggml_reshape_1d(ctx, a, ggml_nelements(a));\n        out      = ggml_get_rows(ctx, out, zero_index);\n        out      = ggml_reshape(ctx, out, a);\n        return out;\n    }\n\n    std::vector<std::string> to_lora_keys(std::string blk_name, SDVersion version) {\n        std::vector<std::string> keys;\n        // if (!sd_version_is_sd3(version) || blk_name != \"model.diffusion_model.pos_embed\") {\n        size_t k_pos = blk_name.find(\".weight\");\n        if (k_pos == std::string::npos) {\n            return keys;\n        }\n        blk_name = blk_name.substr(0, k_pos);\n        // }\n        keys.push_back(blk_name);\n        keys.push_back(\"lora.\" + blk_name);\n        if (sd_version_is_dit(version)) {\n            if (blk_name.find(\"model.diffusion_model\") != std::string::npos) {\n                blk_name.replace(blk_name.find(\"model.diffusion_model\"), sizeof(\"model.diffusion_model\") - 1, \"transformer\");\n            }\n\n            if (blk_name.find(\".single_blocks\") != std::string::npos) {\n                blk_name.replace(blk_name.find(\".single_blocks\"), sizeof(\".single_blocks\") - 1, \".single_transformer_blocks\");\n            }\n            if (blk_name.find(\".double_blocks\") != std::string::npos) {\n                blk_name.replace(blk_name.find(\".double_blocks\"), sizeof(\".double_blocks\") - 1, \".transformer_blocks\");\n            }\n\n            if (blk_name.find(\".joint_blocks\") != std::string::npos) {\n                blk_name.replace(blk_name.find(\".joint_blocks\"), sizeof(\".joint_blocks\") - 1, \".transformer_blocks\");\n            }\n\n            for (const auto& item : alt_names) {\n                size_t match = blk_name.find(item.first);\n                if (match != std::string::npos) {\n                    blk_name = blk_name.substr(0, match) + item.second;\n                }\n            }\n            for (const auto& prefix : qkv_prefixes) {\n                size_t match = blk_name.find(prefix.first);\n                if (match != std::string::npos) {\n                    std::string split_blk = \"SPLIT|\" + blk_name.substr(0, match) + prefix.second;\n                    keys.push_back(split_blk);\n                }\n            }\n            for (const auto& prefix : qkvm_prefixes) {\n                size_t match = blk_name.find(prefix.first);\n                if (match != std::string::npos) {\n                    std::string split_blk = \"SPLIT_L|\" + blk_name.substr(0, match) + prefix.second;\n                    keys.push_back(split_blk);\n                }\n            }\n        }\n        keys.push_back(blk_name);\n\n        std::vector<std::string> ret;\n        for (std::string& key : keys) {\n            ret.push_back(key);\n            replace_all_chars(key, '.', '_');\n            ret.push_back(key);\n        }\n        return ret;\n    }\n\n    struct ggml_cgraph* build_lora_graph(std::map<std::string, struct ggml_tensor*> model_tensors, SDVersion version) {\n        struct ggml_cgraph* gf = ggml_new_graph_custom(compute_ctx, LORA_GRAPH_SIZE, false);\n\n        zero_index = ggml_new_tensor_1d(compute_ctx, GGML_TYPE_I32, 1);\n        set_backend_tensor_data(zero_index, zero_index_vec.data());\n        ggml_build_forward_expand(gf, zero_index);\n\n        std::set<std::string> applied_lora_tensors;\n        for (auto it : model_tensors) {\n            std::string k_tensor       = it.first;\n            struct ggml_tensor* weight = model_tensors[it.first];\n\n            std::vector<std::string> keys = to_lora_keys(k_tensor, version);\n            if (keys.size() == 0)\n                continue;\n            ggml_tensor* lora_up   = NULL;\n            ggml_tensor* lora_down = NULL;\n            for (auto& key : keys) {\n                std::string alpha_name         = \"\";\n                std::string scale_name         = \"\";\n                std::string split_q_scale_name = \"\";\n                std::string lora_down_name     = \"\";\n                std::string lora_up_name       = \"\";\n\n                if (starts_with(key, \"SPLIT|\")) {\n                    key = key.substr(sizeof(\"SPLIT|\") - 1);\n                    // TODO: Handle alphas\n                    std::string suffix  = \"\";\n                    auto split_q_d_name = lora_pre[type] + key + \"q\" + suffix + lora_downs[type] + \".weight\";\n\n                    if (lora_tensors.find(split_q_d_name) == lora_tensors.end()) {\n                        suffix         = \"_proj\";\n                        split_q_d_name = lora_pre[type] + key + \"q\" + suffix + lora_downs[type] + \".weight\";\n                    }\n                    if (lora_tensors.find(split_q_d_name) != lora_tensors.end()) {\n                        // print_ggml_tensor(it.second, true);  //[3072, 21504, 1, 1]\n                        // find qkv and mlp up parts in LoRA model\n                        auto split_k_d_name = lora_pre[type] + key + \"k\" + suffix + lora_downs[type] + \".weight\";\n                        auto split_v_d_name = lora_pre[type] + key + \"v\" + suffix + lora_downs[type] + \".weight\";\n\n                        auto split_q_u_name = lora_pre[type] + key + \"q\" + suffix + lora_ups[type] + \".weight\";\n                        auto split_k_u_name = lora_pre[type] + key + \"k\" + suffix + lora_ups[type] + \".weight\";\n                        auto split_v_u_name = lora_pre[type] + key + \"v\" + suffix + lora_ups[type] + \".weight\";\n\n                        auto split_q_scale_name = lora_pre[type] + key + \"q\" + suffix + \".scale\";\n                        auto split_k_scale_name = lora_pre[type] + key + \"k\" + suffix + \".scale\";\n                        auto split_v_scale_name = lora_pre[type] + key + \"v\" + suffix + \".scale\";\n\n                        auto split_q_alpha_name = lora_pre[type] + key + \"q\" + suffix + \".alpha\";\n                        auto split_k_alpha_name = lora_pre[type] + key + \"k\" + suffix + \".alpha\";\n                        auto split_v_alpha_name = lora_pre[type] + key + \"v\" + suffix + \".alpha\";\n\n                        ggml_tensor* lora_q_down = NULL;\n                        ggml_tensor* lora_q_up   = NULL;\n                        ggml_tensor* lora_k_down = NULL;\n                        ggml_tensor* lora_k_up   = NULL;\n                        ggml_tensor* lora_v_down = NULL;\n                        ggml_tensor* lora_v_up   = NULL;\n\n                        lora_q_down = to_f32(compute_ctx, lora_tensors[split_q_d_name]);\n\n                        if (lora_tensors.find(split_q_u_name) != lora_tensors.end()) {\n                            lora_q_up = to_f32(compute_ctx, lora_tensors[split_q_u_name]);\n                        }\n\n                        if (lora_tensors.find(split_k_d_name) != lora_tensors.end()) {\n                            lora_k_down = to_f32(compute_ctx, lora_tensors[split_k_d_name]);\n                        }\n\n                        if (lora_tensors.find(split_k_u_name) != lora_tensors.end()) {\n                            lora_k_up = to_f32(compute_ctx, lora_tensors[split_k_u_name]);\n                        }\n\n                        if (lora_tensors.find(split_v_d_name) != lora_tensors.end()) {\n                            lora_v_down = to_f32(compute_ctx, lora_tensors[split_v_d_name]);\n                        }\n\n                        if (lora_tensors.find(split_v_u_name) != lora_tensors.end()) {\n                            lora_v_up = to_f32(compute_ctx, lora_tensors[split_v_u_name]);\n                        }\n\n                        float q_rank = lora_q_up->ne[0];\n                        float k_rank = lora_k_up->ne[0];\n                        float v_rank = lora_v_up->ne[0];\n\n                        float lora_q_scale = 1;\n                        float lora_k_scale = 1;\n                        float lora_v_scale = 1;\n\n                        if (lora_tensors.find(split_q_scale_name) != lora_tensors.end()) {\n                            lora_q_scale = ggml_backend_tensor_get_f32(lora_tensors[split_q_scale_name]);\n                            applied_lora_tensors.insert(split_q_scale_name);\n                        }\n                        if (lora_tensors.find(split_k_scale_name) != lora_tensors.end()) {\n                            lora_k_scale = ggml_backend_tensor_get_f32(lora_tensors[split_k_scale_name]);\n                            applied_lora_tensors.insert(split_k_scale_name);\n                        }\n                        if (lora_tensors.find(split_v_scale_name) != lora_tensors.end()) {\n                            lora_v_scale = ggml_backend_tensor_get_f32(lora_tensors[split_v_scale_name]);\n                            applied_lora_tensors.insert(split_v_scale_name);\n                        }\n\n                        if (lora_tensors.find(split_q_alpha_name) != lora_tensors.end()) {\n                            float lora_q_alpha = ggml_backend_tensor_get_f32(lora_tensors[split_q_alpha_name]);\n                            applied_lora_tensors.insert(split_q_alpha_name);\n                            lora_q_scale = lora_q_alpha / q_rank;\n                        }\n                        if (lora_tensors.find(split_k_alpha_name) != lora_tensors.end()) {\n                            float lora_k_alpha = ggml_backend_tensor_get_f32(lora_tensors[split_k_alpha_name]);\n                            applied_lora_tensors.insert(split_k_alpha_name);\n                            lora_k_scale = lora_k_alpha / k_rank;\n                        }\n                        if (lora_tensors.find(split_v_alpha_name) != lora_tensors.end()) {\n                            float lora_v_alpha = ggml_backend_tensor_get_f32(lora_tensors[split_v_alpha_name]);\n                            applied_lora_tensors.insert(split_v_alpha_name);\n                            lora_v_scale = lora_v_alpha / v_rank;\n                        }\n\n                        ggml_scale_inplace(compute_ctx, lora_q_down, lora_q_scale);\n                        ggml_scale_inplace(compute_ctx, lora_k_down, lora_k_scale);\n                        ggml_scale_inplace(compute_ctx, lora_v_down, lora_v_scale);\n\n                        // print_ggml_tensor(lora_q_down, true);  //[3072, R, 1, 1]\n                        // print_ggml_tensor(lora_k_down, true);  //[3072, R, 1, 1]\n                        // print_ggml_tensor(lora_v_down, true);  //[3072, R, 1, 1]\n                        // print_ggml_tensor(lora_q_up, true);    //[R, 3072, 1, 1]\n                        // print_ggml_tensor(lora_k_up, true);    //[R, 3072, 1, 1]\n                        // print_ggml_tensor(lora_v_up, true);    //[R, 3072, 1, 1]\n\n                        // these need to be stitched together this way:\n                        //                          |q_up,0   ,0   |\n                        //                          |0   ,k_up,0   |\n                        //                          |0   ,0   ,v_up|\n                        // (q_down,k_down,v_down) . (q   ,k   ,v)\n\n                        // up_concat will be [9216, R*3, 1, 1]\n                        // down_concat will be [R*3, 3072, 1, 1]\n                        ggml_tensor* lora_down_concat = ggml_concat(compute_ctx, ggml_concat(compute_ctx, lora_q_down, lora_k_down, 1), lora_v_down, 1);\n\n                        ggml_tensor* z = ggml_dup_tensor(compute_ctx, lora_q_up);\n                        ggml_scale(compute_ctx, z, 0);\n                        ggml_tensor* zz = ggml_concat(compute_ctx, z, z, 1);\n\n                        ggml_tensor* q_up = ggml_concat(compute_ctx, lora_q_up, zz, 1);\n                        ggml_tensor* k_up = ggml_concat(compute_ctx, ggml_concat(compute_ctx, z, lora_k_up, 1), z, 1);\n                        ggml_tensor* v_up = ggml_concat(compute_ctx, zz, lora_v_up, 1);\n                        // print_ggml_tensor(q_up, true);  //[R, 9216, 1, 1]\n                        // print_ggml_tensor(k_up, true);  //[R, 9216, 1, 1]\n                        // print_ggml_tensor(v_up, true);  //[R, 9216, 1, 1]\n                        ggml_tensor* lora_up_concat = ggml_concat(compute_ctx, ggml_concat(compute_ctx, q_up, k_up, 0), v_up, 0);\n                        // print_ggml_tensor(lora_up_concat, true);  //[R*3, 9216, 1, 1]\n\n                        lora_down = ggml_cont(compute_ctx, lora_down_concat);\n                        lora_up   = ggml_cont(compute_ctx, lora_up_concat);\n\n                        applied_lora_tensors.insert(split_q_u_name);\n                        applied_lora_tensors.insert(split_k_u_name);\n                        applied_lora_tensors.insert(split_v_u_name);\n\n                        applied_lora_tensors.insert(split_q_d_name);\n                        applied_lora_tensors.insert(split_k_d_name);\n                        applied_lora_tensors.insert(split_v_d_name);\n                    }\n                }\n                if (starts_with(key, \"SPLIT_L|\")) {\n                    key = key.substr(sizeof(\"SPLIT_L|\") - 1);\n\n                    auto split_q_d_name = lora_pre[type] + key + \"attn.to_q\" + lora_downs[type] + \".weight\";\n                    if (lora_tensors.find(split_q_d_name) != lora_tensors.end()) {\n                        // print_ggml_tensor(it.second, true);  //[3072, 21504, 1, 1]\n                        // find qkv and mlp up parts in LoRA model\n                        auto split_k_d_name = lora_pre[type] + key + \"attn.to_k\" + lora_downs[type] + \".weight\";\n                        auto split_v_d_name = lora_pre[type] + key + \"attn.to_v\" + lora_downs[type] + \".weight\";\n\n                        auto split_q_u_name = lora_pre[type] + key + \"attn.to_q\" + lora_ups[type] + \".weight\";\n                        auto split_k_u_name = lora_pre[type] + key + \"attn.to_k\" + lora_ups[type] + \".weight\";\n                        auto split_v_u_name = lora_pre[type] + key + \"attn.to_v\" + lora_ups[type] + \".weight\";\n\n                        auto split_m_d_name = lora_pre[type] + key + \"proj_mlp\" + lora_downs[type] + \".weight\";\n                        auto split_m_u_name = lora_pre[type] + key + \"proj_mlp\" + lora_ups[type] + \".weight\";\n\n                        auto split_q_scale_name = lora_pre[type] + key + \"attn.to_q\" + \".scale\";\n                        auto split_k_scale_name = lora_pre[type] + key + \"attn.to_k\" + \".scale\";\n                        auto split_v_scale_name = lora_pre[type] + key + \"attn.to_v\" + \".scale\";\n                        auto split_m_scale_name = lora_pre[type] + key + \"proj_mlp\" + \".scale\";\n\n                        auto split_q_alpha_name = lora_pre[type] + key + \"attn.to_q\" + \".alpha\";\n                        auto split_k_alpha_name = lora_pre[type] + key + \"attn.to_k\" + \".alpha\";\n                        auto split_v_alpha_name = lora_pre[type] + key + \"attn.to_v\" + \".alpha\";\n                        auto split_m_alpha_name = lora_pre[type] + key + \"proj_mlp\" + \".alpha\";\n\n                        ggml_tensor* lora_q_down = NULL;\n                        ggml_tensor* lora_q_up   = NULL;\n                        ggml_tensor* lora_k_down = NULL;\n                        ggml_tensor* lora_k_up   = NULL;\n                        ggml_tensor* lora_v_down = NULL;\n                        ggml_tensor* lora_v_up   = NULL;\n\n                        ggml_tensor* lora_m_down = NULL;\n                        ggml_tensor* lora_m_up   = NULL;\n\n                        lora_q_up = to_f32(compute_ctx, lora_tensors[split_q_u_name]);\n\n                        if (lora_tensors.find(split_q_d_name) != lora_tensors.end()) {\n                            lora_q_down = to_f32(compute_ctx, lora_tensors[split_q_d_name]);\n                        }\n\n                        if (lora_tensors.find(split_q_u_name) != lora_tensors.end()) {\n                            lora_q_up = to_f32(compute_ctx, lora_tensors[split_q_u_name]);\n                        }\n\n                        if (lora_tensors.find(split_k_d_name) != lora_tensors.end()) {\n                            lora_k_down = to_f32(compute_ctx, lora_tensors[split_k_d_name]);\n                        }\n\n                        if (lora_tensors.find(split_k_u_name) != lora_tensors.end()) {\n                            lora_k_up = to_f32(compute_ctx, lora_tensors[split_k_u_name]);\n                        }\n\n                        if (lora_tensors.find(split_v_d_name) != lora_tensors.end()) {\n                            lora_v_down = to_f32(compute_ctx, lora_tensors[split_v_d_name]);\n                        }\n\n                        if (lora_tensors.find(split_v_u_name) != lora_tensors.end()) {\n                            lora_v_up = to_f32(compute_ctx, lora_tensors[split_v_u_name]);\n                        }\n\n                        if (lora_tensors.find(split_m_d_name) != lora_tensors.end()) {\n                            lora_m_down = to_f32(compute_ctx, lora_tensors[split_m_d_name]);\n                        }\n\n                        if (lora_tensors.find(split_m_u_name) != lora_tensors.end()) {\n                            lora_m_up = to_f32(compute_ctx, lora_tensors[split_m_u_name]);\n                        }\n\n                        float q_rank = lora_q_up->ne[0];\n                        float k_rank = lora_k_up->ne[0];\n                        float v_rank = lora_v_up->ne[0];\n                        float m_rank = lora_v_up->ne[0];\n\n                        float lora_q_scale = 1;\n                        float lora_k_scale = 1;\n                        float lora_v_scale = 1;\n                        float lora_m_scale = 1;\n\n                        if (lora_tensors.find(split_q_scale_name) != lora_tensors.end()) {\n                            lora_q_scale = ggml_backend_tensor_get_f32(lora_tensors[split_q_scale_name]);\n                            applied_lora_tensors.insert(split_q_scale_name);\n                        }\n                        if (lora_tensors.find(split_k_scale_name) != lora_tensors.end()) {\n                            lora_k_scale = ggml_backend_tensor_get_f32(lora_tensors[split_k_scale_name]);\n                            applied_lora_tensors.insert(split_k_scale_name);\n                        }\n                        if (lora_tensors.find(split_v_scale_name) != lora_tensors.end()) {\n                            lora_v_scale = ggml_backend_tensor_get_f32(lora_tensors[split_v_scale_name]);\n                            applied_lora_tensors.insert(split_v_scale_name);\n                        }\n                        if (lora_tensors.find(split_m_scale_name) != lora_tensors.end()) {\n                            lora_m_scale = ggml_backend_tensor_get_f32(lora_tensors[split_m_scale_name]);\n                            applied_lora_tensors.insert(split_m_scale_name);\n                        }\n\n                        if (lora_tensors.find(split_q_alpha_name) != lora_tensors.end()) {\n                            float lora_q_alpha = ggml_backend_tensor_get_f32(lora_tensors[split_q_alpha_name]);\n                            applied_lora_tensors.insert(split_q_alpha_name);\n                            lora_q_scale = lora_q_alpha / q_rank;\n                        }\n                        if (lora_tensors.find(split_k_alpha_name) != lora_tensors.end()) {\n                            float lora_k_alpha = ggml_backend_tensor_get_f32(lora_tensors[split_k_alpha_name]);\n                            applied_lora_tensors.insert(split_k_alpha_name);\n                            lora_k_scale = lora_k_alpha / k_rank;\n                        }\n                        if (lora_tensors.find(split_v_alpha_name) != lora_tensors.end()) {\n                            float lora_v_alpha = ggml_backend_tensor_get_f32(lora_tensors[split_v_alpha_name]);\n                            applied_lora_tensors.insert(split_v_alpha_name);\n                            lora_v_scale = lora_v_alpha / v_rank;\n                        }\n                        if (lora_tensors.find(split_m_alpha_name) != lora_tensors.end()) {\n                            float lora_m_alpha = ggml_backend_tensor_get_f32(lora_tensors[split_m_alpha_name]);\n                            applied_lora_tensors.insert(split_m_alpha_name);\n                            lora_m_scale = lora_m_alpha / m_rank;\n                        }\n\n                        ggml_scale_inplace(compute_ctx, lora_q_down, lora_q_scale);\n                        ggml_scale_inplace(compute_ctx, lora_k_down, lora_k_scale);\n                        ggml_scale_inplace(compute_ctx, lora_v_down, lora_v_scale);\n                        ggml_scale_inplace(compute_ctx, lora_m_down, lora_m_scale);\n\n                        // print_ggml_tensor(lora_q_down, true);  //[3072, R, 1, 1]\n                        // print_ggml_tensor(lora_k_down, true);  //[3072, R, 1, 1]\n                        // print_ggml_tensor(lora_v_down, true);  //[3072, R, 1, 1]\n                        // print_ggml_tensor(lora_m_down, true);  //[3072, R, 1, 1]\n                        // print_ggml_tensor(lora_q_up, true);  //[R, 3072, 1, 1]\n                        // print_ggml_tensor(lora_k_up, true);  //[R, 3072, 1, 1]\n                        // print_ggml_tensor(lora_v_up, true);  //[R, 3072, 1, 1]\n                        // print_ggml_tensor(lora_m_up, true);  //[R, 12288, 1, 1]\n\n                        // these need to be stitched together this way:\n                        //                                 |q_up,0   ,0   ,0   |\n                        //                                 |0   ,k_up,0   ,0   |\n                        //                                 |0   ,0   ,v_up,0   |\n                        //                                 |0   ,0   ,0   ,m_up|\n                        // (q_down,k_down,v_down,m_down) . (q   ,k   ,v   ,m)\n\n                        // up_concat will be [21504, R*4, 1, 1]\n                        // down_concat will be [R*4, 3072, 1, 1]\n\n                        ggml_tensor* lora_down_concat = ggml_concat(compute_ctx, ggml_concat(compute_ctx, lora_q_down, lora_k_down, 1), ggml_concat(compute_ctx, lora_v_down, lora_m_down, 1), 1);\n                        // print_ggml_tensor(lora_down_concat, true);  //[3072, R*4, 1, 1]\n\n                        // this also means that if rank is bigger than 672, it is less memory efficient to do it this way (should be fine)\n                        // print_ggml_tensor(lora_q_up, true);  //[3072, R, 1, 1]\n                        ggml_tensor* z     = ggml_dup_tensor(compute_ctx, lora_q_up);\n                        ggml_tensor* mlp_z = ggml_dup_tensor(compute_ctx, lora_m_up);\n                        ggml_scale(compute_ctx, z, 0);\n                        ggml_scale(compute_ctx, mlp_z, 0);\n                        ggml_tensor* zz = ggml_concat(compute_ctx, z, z, 1);\n\n                        ggml_tensor* q_up = ggml_concat(compute_ctx, ggml_concat(compute_ctx, lora_q_up, zz, 1), mlp_z, 1);\n                        ggml_tensor* k_up = ggml_concat(compute_ctx, ggml_concat(compute_ctx, z, lora_k_up, 1), ggml_concat(compute_ctx, z, mlp_z, 1), 1);\n                        ggml_tensor* v_up = ggml_concat(compute_ctx, ggml_concat(compute_ctx, zz, lora_v_up, 1), mlp_z, 1);\n                        ggml_tensor* m_up = ggml_concat(compute_ctx, ggml_concat(compute_ctx, zz, z, 1), lora_m_up, 1);\n                        // print_ggml_tensor(q_up, true);  //[R, 21504, 1, 1]\n                        // print_ggml_tensor(k_up, true);  //[R, 21504, 1, 1]\n                        // print_ggml_tensor(v_up, true);  //[R, 21504, 1, 1]\n                        // print_ggml_tensor(m_up, true);  //[R, 21504, 1, 1]\n\n                        ggml_tensor* lora_up_concat = ggml_concat(compute_ctx, ggml_concat(compute_ctx, q_up, k_up, 0), ggml_concat(compute_ctx, v_up, m_up, 0), 0);\n                        // print_ggml_tensor(lora_up_concat, true);  //[R*4, 21504, 1, 1]\n\n                        lora_down = ggml_cont(compute_ctx, lora_down_concat);\n                        lora_up   = ggml_cont(compute_ctx, lora_up_concat);\n\n                        applied_lora_tensors.insert(split_q_u_name);\n                        applied_lora_tensors.insert(split_k_u_name);\n                        applied_lora_tensors.insert(split_v_u_name);\n                        applied_lora_tensors.insert(split_m_u_name);\n\n                        applied_lora_tensors.insert(split_q_d_name);\n                        applied_lora_tensors.insert(split_k_d_name);\n                        applied_lora_tensors.insert(split_v_d_name);\n                        applied_lora_tensors.insert(split_m_d_name);\n                    }\n                }\n                if (lora_up == NULL || lora_down == NULL) {\n                    lora_up_name = lora_pre[type] + key + lora_ups[type] + \".weight\";\n                    if (lora_tensors.find(lora_up_name) == lora_tensors.end()) {\n                        if (key == \"model_diffusion_model_output_blocks_2_2_conv\") {\n                            // fix for some sdxl lora, like lcm-lora-xl\n                            key          = \"model_diffusion_model_output_blocks_2_1_conv\";\n                            lora_up_name = lora_pre[type] + key + lora_ups[type] + \".weight\";\n                        }\n                    }\n\n                    lora_down_name = lora_pre[type] + key + lora_downs[type] + \".weight\";\n                    alpha_name     = lora_pre[type] + key + \".alpha\";\n                    scale_name     = lora_pre[type] + key + \".scale\";\n\n                    if (lora_tensors.find(lora_up_name) != lora_tensors.end()) {\n                        lora_up = lora_tensors[lora_up_name];\n                    }\n\n                    if (lora_tensors.find(lora_down_name) != lora_tensors.end()) {\n                        lora_down = lora_tensors[lora_down_name];\n                    }\n                    applied_lora_tensors.insert(lora_up_name);\n                    applied_lora_tensors.insert(lora_down_name);\n                    applied_lora_tensors.insert(alpha_name);\n                    applied_lora_tensors.insert(scale_name);\n                }\n\n                if (lora_up == NULL || lora_down == NULL) {\n                    continue;\n                }\n                // calc_scale\n                int64_t dim       = lora_down->ne[ggml_n_dims(lora_down) - 1];\n                float scale_value = 1.0f;\n                if (lora_tensors.find(scale_name) != lora_tensors.end()) {\n                    scale_value = ggml_backend_tensor_get_f32(lora_tensors[scale_name]);\n                } else if (lora_tensors.find(alpha_name) != lora_tensors.end()) {\n                    float alpha = ggml_backend_tensor_get_f32(lora_tensors[alpha_name]);\n                    scale_value = alpha / dim;\n                }\n                scale_value *= multiplier;\n\n                // flat lora tensors to multiply it\n                int64_t lora_up_rows   = lora_up->ne[ggml_n_dims(lora_up) - 1];\n                lora_up                = ggml_reshape_2d(compute_ctx, lora_up, ggml_nelements(lora_up) / lora_up_rows, lora_up_rows);\n                int64_t lora_down_rows = lora_down->ne[ggml_n_dims(lora_down) - 1];\n                lora_down              = ggml_reshape_2d(compute_ctx, lora_down, ggml_nelements(lora_down) / lora_down_rows, lora_down_rows);\n\n                // ggml_mul_mat requires tensor b transposed\n                lora_down                  = ggml_cont(compute_ctx, ggml_transpose(compute_ctx, lora_down));\n                struct ggml_tensor* updown = ggml_mul_mat(compute_ctx, lora_up, lora_down);\n                updown                     = ggml_cont(compute_ctx, ggml_transpose(compute_ctx, updown));\n                updown                     = ggml_reshape(compute_ctx, updown, weight);\n                GGML_ASSERT(ggml_nelements(updown) == ggml_nelements(weight));\n                updown = ggml_scale_inplace(compute_ctx, updown, scale_value);\n                ggml_tensor* final_weight;\n                if (weight->type != GGML_TYPE_F32 && weight->type != GGML_TYPE_F16) {\n                    // final_weight = ggml_new_tensor(compute_ctx, GGML_TYPE_F32, ggml_n_dims(weight), weight->ne);\n                    // final_weight = ggml_cpy(compute_ctx, weight, final_weight);\n                    final_weight = to_f32(compute_ctx, weight);\n                    final_weight = ggml_add_inplace(compute_ctx, final_weight, updown);\n                    final_weight = ggml_cpy(compute_ctx, final_weight, weight);\n                } else {\n                    final_weight = ggml_add_inplace(compute_ctx, weight, updown);\n                }\n                // final_weight = ggml_add_inplace(compute_ctx, weight, updown);  // apply directly\n                ggml_build_forward_expand(gf, final_weight);\n                break;\n            }\n        }\n        size_t total_lora_tensors_count   = 0;\n        size_t applied_lora_tensors_count = 0;\n\n        for (auto& kv : lora_tensors) {\n            total_lora_tensors_count++;\n            if (applied_lora_tensors.find(kv.first) == applied_lora_tensors.end()) {\n                LOG_WARN(\"unused lora tensor |%s|\", kv.first.c_str());\n                print_ggml_tensor(kv.second, true);\n                // exit(0);\n            } else {\n                applied_lora_tensors_count++;\n            }\n        }\n        /* Don't worry if this message shows up twice in the logs per LoRA,\n         * this function is called once to calculate the required buffer size\n         * and then again to actually generate a graph to be used */\n        if (applied_lora_tensors_count != total_lora_tensors_count) {\n            LOG_WARN(\"Only (%lu / %lu) LoRA tensors have been applied\",\n                     applied_lora_tensors_count, total_lora_tensors_count);\n        } else {\n            LOG_DEBUG(\"(%lu / %lu) LoRA tensors applied successfully\",\n                      applied_lora_tensors_count, total_lora_tensors_count);\n        }\n\n        return gf;\n    }\n\n    void apply(std::map<std::string, struct ggml_tensor*> model_tensors, SDVersion version, int n_threads) {\n        auto get_graph = [&]() -> struct ggml_cgraph* {\n            return build_lora_graph(model_tensors, version);\n        };\n        GGMLRunner::compute(get_graph, n_threads, true);\n    }\n};\n\n#endif  // __LORA_HPP__\n"
        },
        {
          "name": "mmdit.hpp",
          "type": "blob",
          "size": 45.302734375,
          "content": "#ifndef __MMDIT_HPP__\n#define __MMDIT_HPP__\n\n#include \"ggml_extend.hpp\"\n#include \"model.h\"\n\n#define MMDIT_GRAPH_SIZE 10240\n\nstruct Mlp : public GGMLBlock {\npublic:\n    Mlp(int64_t in_features,\n        int64_t hidden_features = -1,\n        int64_t out_features    = -1,\n        bool bias               = true) {\n        // act_layer is always lambda: nn.GELU(approximate=\"tanh\")\n        // norm_layer is always None\n        // use_conv is always False\n        if (hidden_features == -1) {\n            hidden_features = in_features;\n        }\n        if (out_features == -1) {\n            out_features = in_features;\n        }\n        blocks[\"fc1\"] = std::shared_ptr<GGMLBlock>(new Linear(in_features, hidden_features, bias));\n        blocks[\"fc2\"] = std::shared_ptr<GGMLBlock>(new Linear(hidden_features, out_features, bias));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [N, n_token, in_features]\n        auto fc1 = std::dynamic_pointer_cast<Linear>(blocks[\"fc1\"]);\n        auto fc2 = std::dynamic_pointer_cast<Linear>(blocks[\"fc2\"]);\n\n        x = fc1->forward(ctx, x);\n        x = ggml_gelu_inplace(ctx, x);\n        x = fc2->forward(ctx, x);\n        return x;\n    }\n};\n\nstruct PatchEmbed : public GGMLBlock {\n    // 2D Image to Patch Embedding\nprotected:\n    bool flatten;\n    bool dynamic_img_pad;\n    int patch_size;\n\npublic:\n    PatchEmbed(int64_t img_size     = 224,\n               int patch_size       = 16,\n               int64_t in_chans     = 3,\n               int64_t embed_dim    = 1536,\n               bool bias            = true,\n               bool flatten         = true,\n               bool dynamic_img_pad = true)\n        : patch_size(patch_size),\n          flatten(flatten),\n          dynamic_img_pad(dynamic_img_pad) {\n        // img_size is always None\n        // patch_size is always 2\n        // in_chans is always 16\n        // norm_layer is always False\n        // strict_img_size is always true, but not used\n\n        blocks[\"proj\"] = std::shared_ptr<GGMLBlock>(new Conv2d(in_chans,\n                                                               embed_dim,\n                                                               {patch_size, patch_size},\n                                                               {patch_size, patch_size},\n                                                               {0, 0},\n                                                               {1, 1},\n                                                               bias));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [N, C, H, W]\n        // return: [N, H*W, embed_dim]\n        auto proj = std::dynamic_pointer_cast<Conv2d>(blocks[\"proj\"]);\n\n        if (dynamic_img_pad) {\n            int64_t W = x->ne[0];\n            int64_t H = x->ne[1];\n            int pad_h = (patch_size - H % patch_size) % patch_size;\n            int pad_w = (patch_size - W % patch_size) % patch_size;\n            x         = ggml_pad(ctx, x, pad_w, pad_h, 0, 0);  // TODO: reflect pad mode\n        }\n        x = proj->forward(ctx, x);\n\n        if (flatten) {\n            x = ggml_reshape_3d(ctx, x, x->ne[0] * x->ne[1], x->ne[2], x->ne[3]);\n            x = ggml_cont(ctx, ggml_permute(ctx, x, 1, 0, 2, 3));\n        }\n        return x;\n    }\n};\n\nstruct TimestepEmbedder : public GGMLBlock {\n    // Embeds scalar timesteps into vector representations.\nprotected:\n    int64_t frequency_embedding_size;\n\npublic:\n    TimestepEmbedder(int64_t hidden_size,\n                     int64_t frequency_embedding_size = 256)\n        : frequency_embedding_size(frequency_embedding_size) {\n        blocks[\"mlp.0\"] = std::shared_ptr<GGMLBlock>(new Linear(frequency_embedding_size, hidden_size, true, true));\n        blocks[\"mlp.2\"] = std::shared_ptr<GGMLBlock>(new Linear(hidden_size, hidden_size, true, true));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* t) {\n        // t: [N, ]\n        // return: [N, hidden_size]\n        auto mlp_0 = std::dynamic_pointer_cast<Linear>(blocks[\"mlp.0\"]);\n        auto mlp_2 = std::dynamic_pointer_cast<Linear>(blocks[\"mlp.2\"]);\n\n        auto t_freq = ggml_nn_timestep_embedding(ctx, t, frequency_embedding_size);  // [N, frequency_embedding_size]\n\n        auto t_emb = mlp_0->forward(ctx, t_freq);\n        t_emb      = ggml_silu_inplace(ctx, t_emb);\n        t_emb      = mlp_2->forward(ctx, t_emb);\n        return t_emb;\n    }\n};\n\nstruct VectorEmbedder : public GGMLBlock {\n    // Embeds a flat vector of dimension input_dim\npublic:\n    VectorEmbedder(int64_t input_dim,\n                   int64_t hidden_size) {\n        blocks[\"mlp.0\"] = std::shared_ptr<GGMLBlock>(new Linear(input_dim, hidden_size, true, true));\n        blocks[\"mlp.2\"] = std::shared_ptr<GGMLBlock>(new Linear(hidden_size, hidden_size, true, true));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [N, input_dim]\n        // return: [N, hidden_size]\n        auto mlp_0 = std::dynamic_pointer_cast<Linear>(blocks[\"mlp.0\"]);\n        auto mlp_2 = std::dynamic_pointer_cast<Linear>(blocks[\"mlp.2\"]);\n\n        x = mlp_0->forward(ctx, x);\n        x = ggml_silu_inplace(ctx, x);\n        x = mlp_2->forward(ctx, x);\n        return x;\n    }\n};\n\nclass RMSNorm : public UnaryBlock {\nprotected:\n    int64_t hidden_size;\n    float eps;\n\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, std::string prefix = \"\") {\n        enum ggml_type wtype = GGML_TYPE_F32;  //(tensor_types.find(prefix + \"weight\") != tensor_types.end()) ? tensor_types[prefix + \"weight\"] : GGML_TYPE_F32;\n        params[\"weight\"]     = ggml_new_tensor_1d(ctx, wtype, hidden_size);\n    }\n\npublic:\n    RMSNorm(int64_t hidden_size,\n            float eps = 1e-06f)\n        : hidden_size(hidden_size),\n          eps(eps) {}\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        struct ggml_tensor* w = params[\"weight\"];\n        x                     = ggml_rms_norm(ctx, x, eps);\n        x                     = ggml_mul(ctx, x, w);\n        return x;\n    }\n};\n\nclass SelfAttention : public GGMLBlock {\npublic:\n    int64_t num_heads;\n    bool pre_only;\n    std::string qk_norm;\n\npublic:\n    SelfAttention(int64_t dim,\n                  int64_t num_heads   = 8,\n                  std::string qk_norm = \"\",\n                  bool qkv_bias       = false,\n                  bool pre_only       = false)\n        : num_heads(num_heads), pre_only(pre_only), qk_norm(qk_norm) {\n        int64_t d_head = dim / num_heads;\n        blocks[\"qkv\"]  = std::shared_ptr<GGMLBlock>(new Linear(dim, dim * 3, qkv_bias));\n        if (!pre_only) {\n            blocks[\"proj\"] = std::shared_ptr<GGMLBlock>(new Linear(dim, dim));\n        }\n        if (qk_norm == \"rms\") {\n            blocks[\"ln_q\"] = std::shared_ptr<GGMLBlock>(new RMSNorm(d_head, 1.0e-6));\n            blocks[\"ln_k\"] = std::shared_ptr<GGMLBlock>(new RMSNorm(d_head, 1.0e-6));\n        } else if (qk_norm == \"ln\") {\n            blocks[\"ln_q\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(d_head, 1.0e-6));\n            blocks[\"ln_k\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(d_head, 1.0e-6));\n        }\n    }\n\n    std::vector<struct ggml_tensor*> pre_attention(struct ggml_context* ctx, struct ggml_tensor* x) {\n        auto qkv_proj = std::dynamic_pointer_cast<Linear>(blocks[\"qkv\"]);\n\n        auto qkv         = qkv_proj->forward(ctx, x);\n        auto qkv_vec     = split_qkv(ctx, qkv);\n        int64_t head_dim = qkv_vec[0]->ne[0] / num_heads;\n        auto q           = ggml_reshape_4d(ctx, qkv_vec[0], head_dim, num_heads, qkv_vec[0]->ne[1], qkv_vec[0]->ne[2]);  // [N, n_token, n_head, d_head]\n        auto k           = ggml_reshape_4d(ctx, qkv_vec[1], head_dim, num_heads, qkv_vec[1]->ne[1], qkv_vec[1]->ne[2]);  // [N, n_token, n_head, d_head]\n        auto v           = qkv_vec[2];                                                                                   // [N, n_token, n_head*d_head]\n\n        if (qk_norm == \"rms\" || qk_norm == \"ln\") {\n            auto ln_q = std::dynamic_pointer_cast<UnaryBlock>(blocks[\"ln_q\"]);\n            auto ln_k = std::dynamic_pointer_cast<UnaryBlock>(blocks[\"ln_k\"]);\n            q         = ln_q->forward(ctx, q);\n            k         = ln_k->forward(ctx, k);\n        }\n\n        q = ggml_reshape_3d(ctx, q, q->ne[0] * q->ne[1], q->ne[2], q->ne[3]);  // [N, n_token, n_head*d_head]\n        k = ggml_reshape_3d(ctx, k, k->ne[0] * k->ne[1], k->ne[2], k->ne[3]);  // [N, n_token, n_head*d_head]\n\n        return {q, k, v};\n    }\n\n    struct ggml_tensor* post_attention(struct ggml_context* ctx, struct ggml_tensor* x) {\n        GGML_ASSERT(!pre_only);\n\n        auto proj = std::dynamic_pointer_cast<Linear>(blocks[\"proj\"]);\n\n        x = proj->forward(ctx, x);  // [N, n_token, dim]\n        return x;\n    }\n\n    // x: [N, n_token, dim]\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        auto qkv = pre_attention(ctx, x);\n        x        = ggml_nn_attention_ext(ctx, qkv[0], qkv[1], qkv[2], num_heads);  // [N, n_token, dim]\n        x        = post_attention(ctx, x);                                         // [N, n_token, dim]\n        return x;\n    }\n};\n\n__STATIC_INLINE__ struct ggml_tensor* modulate(struct ggml_context* ctx,\n                                               struct ggml_tensor* x,\n                                               struct ggml_tensor* shift,\n                                               struct ggml_tensor* scale) {\n    // x: [N, L, C]\n    // scale: [N, C]\n    // shift: [N, C]\n    scale = ggml_reshape_3d(ctx, scale, scale->ne[0], 1, scale->ne[1]);  // [N, 1, C]\n    shift = ggml_reshape_3d(ctx, shift, shift->ne[0], 1, shift->ne[1]);  // [N, 1, C]\n    x     = ggml_add(ctx, x, ggml_mul(ctx, x, scale));\n    x     = ggml_add(ctx, x, shift);\n    return x;\n}\n\nstruct DismantledBlock : public GGMLBlock {\n    // A DiT block with gated adaptive layer norm (adaLN) conditioning.\npublic:\n    int64_t num_heads;\n    bool pre_only;\n    bool self_attn;\n\npublic:\n    DismantledBlock(int64_t hidden_size,\n                    int64_t num_heads,\n                    float mlp_ratio     = 4.0,\n                    std::string qk_norm = \"\",\n                    bool qkv_bias       = false,\n                    bool pre_only       = false,\n                    bool self_attn      = false)\n        : num_heads(num_heads), pre_only(pre_only), self_attn(self_attn) {\n        // rmsnorm is always Flase\n        // scale_mod_only is always Flase\n        // swiglu is always Flase\n        blocks[\"norm1\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(hidden_size, 1e-06f, false));\n        blocks[\"attn\"]  = std::shared_ptr<GGMLBlock>(new SelfAttention(hidden_size, num_heads, qk_norm, qkv_bias, pre_only));\n\n        if (self_attn) {\n            blocks[\"attn2\"] = std::shared_ptr<GGMLBlock>(new SelfAttention(hidden_size, num_heads, qk_norm, qkv_bias, false));\n        }\n\n        if (!pre_only) {\n            blocks[\"norm2\"]        = std::shared_ptr<GGMLBlock>(new LayerNorm(hidden_size, 1e-06f, false));\n            int64_t mlp_hidden_dim = (int64_t)(hidden_size * mlp_ratio);\n            blocks[\"mlp\"]          = std::shared_ptr<GGMLBlock>(new Mlp(hidden_size, mlp_hidden_dim));\n        }\n\n        int64_t n_mods = 6;\n        if (pre_only) {\n            n_mods = 2;\n        }\n        if (self_attn) {\n            n_mods = 9;\n        }\n        blocks[\"adaLN_modulation.1\"] = std::shared_ptr<GGMLBlock>(new Linear(hidden_size, n_mods * hidden_size));\n    }\n\n    std::tuple<std::vector<struct ggml_tensor*>, std::vector<struct ggml_tensor*>, std::vector<struct ggml_tensor*>> pre_attention_x(struct ggml_context* ctx,\n                                                                                                                                     struct ggml_tensor* x,\n                                                                                                                                     struct ggml_tensor* c) {\n        GGML_ASSERT(self_attn);\n        // x: [N, n_token, hidden_size]\n        // c: [N, hidden_size]\n        auto norm1              = std::dynamic_pointer_cast<LayerNorm>(blocks[\"norm1\"]);\n        auto attn               = std::dynamic_pointer_cast<SelfAttention>(blocks[\"attn\"]);\n        auto attn2              = std::dynamic_pointer_cast<SelfAttention>(blocks[\"attn2\"]);\n        auto adaLN_modulation_1 = std::dynamic_pointer_cast<Linear>(blocks[\"adaLN_modulation.1\"]);\n\n        int64_t n_mods = 9;\n        auto m         = adaLN_modulation_1->forward(ctx, ggml_silu(ctx, c));  // [N, n_mods * hidden_size]\n        m              = ggml_reshape_3d(ctx, m, c->ne[0], n_mods, c->ne[1]);  // [N, n_mods, hidden_size]\n        m              = ggml_cont(ctx, ggml_permute(ctx, m, 0, 2, 1, 3));     // [n_mods, N, hidden_size]\n\n        int64_t offset = m->nb[1] * m->ne[1];\n        auto shift_msa = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 0);  // [N, hidden_size]\n        auto scale_msa = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 1);  // [N, hidden_size]\n        auto gate_msa  = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 2);  // [N, hidden_size]\n\n        auto shift_mlp = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 3);  // [N, hidden_size]\n        auto scale_mlp = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 4);  // [N, hidden_size]\n        auto gate_mlp  = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 5);  // [N, hidden_size]\n\n        auto shift_msa2 = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 6);  // [N, hidden_size]\n        auto scale_msa2 = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 7);  // [N, hidden_size]\n        auto gate_msa2  = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 8);  // [N, hidden_size]\n\n        auto x_norm = norm1->forward(ctx, x);\n\n        auto attn_in = modulate(ctx, x_norm, shift_msa, scale_msa);\n        auto qkv     = attn->pre_attention(ctx, attn_in);\n\n        auto attn2_in = modulate(ctx, x_norm, shift_msa2, scale_msa2);\n        auto qkv2     = attn2->pre_attention(ctx, attn2_in);\n\n        return {qkv, qkv2, {x, gate_msa, shift_mlp, scale_mlp, gate_mlp, gate_msa2}};\n    }\n\n    std::pair<std::vector<struct ggml_tensor*>, std::vector<struct ggml_tensor*>> pre_attention(struct ggml_context* ctx,\n                                                                                                struct ggml_tensor* x,\n                                                                                                struct ggml_tensor* c) {\n        // x: [N, n_token, hidden_size]\n        // c: [N, hidden_size]\n        auto norm1              = std::dynamic_pointer_cast<LayerNorm>(blocks[\"norm1\"]);\n        auto attn               = std::dynamic_pointer_cast<SelfAttention>(blocks[\"attn\"]);\n        auto adaLN_modulation_1 = std::dynamic_pointer_cast<Linear>(blocks[\"adaLN_modulation.1\"]);\n\n        int64_t n_mods = 6;\n        if (pre_only) {\n            n_mods = 2;\n        }\n        auto m = adaLN_modulation_1->forward(ctx, ggml_silu(ctx, c));  // [N, n_mods * hidden_size]\n        m      = ggml_reshape_3d(ctx, m, c->ne[0], n_mods, c->ne[1]);  // [N, n_mods, hidden_size]\n        m      = ggml_cont(ctx, ggml_permute(ctx, m, 0, 2, 1, 3));     // [n_mods, N, hidden_size]\n\n        int64_t offset = m->nb[1] * m->ne[1];\n        auto shift_msa = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 0);  // [N, hidden_size]\n        auto scale_msa = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 1);  // [N, hidden_size]\n        if (!pre_only) {\n            auto gate_msa  = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 2);  // [N, hidden_size]\n            auto shift_mlp = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 3);  // [N, hidden_size]\n            auto scale_mlp = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 4);  // [N, hidden_size]\n            auto gate_mlp  = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 5);  // [N, hidden_size]\n\n            auto attn_in = modulate(ctx, norm1->forward(ctx, x), shift_msa, scale_msa);\n\n            auto qkv = attn->pre_attention(ctx, attn_in);\n\n            return {qkv, {x, gate_msa, shift_mlp, scale_mlp, gate_mlp}};\n        } else {\n            auto attn_in = modulate(ctx, norm1->forward(ctx, x), shift_msa, scale_msa);\n            auto qkv     = attn->pre_attention(ctx, attn_in);\n\n            return {qkv, {NULL, NULL, NULL, NULL, NULL}};\n        }\n    }\n\n    struct ggml_tensor* post_attention_x(struct ggml_context* ctx,\n                                         struct ggml_tensor* attn_out,\n                                         struct ggml_tensor* attn2_out,\n                                         struct ggml_tensor* x,\n                                         struct ggml_tensor* gate_msa,\n                                         struct ggml_tensor* shift_mlp,\n                                         struct ggml_tensor* scale_mlp,\n                                         struct ggml_tensor* gate_mlp,\n                                         struct ggml_tensor* gate_msa2) {\n        // attn_out: [N, n_token, hidden_size]\n        // x: [N, n_token, hidden_size]\n        // gate_msa: [N, hidden_size]\n        // shift_mlp: [N, hidden_size]\n        // scale_mlp: [N, hidden_size]\n        // gate_mlp: [N, hidden_size]\n        // return: [N, n_token, hidden_size]\n        GGML_ASSERT(!pre_only);\n\n        auto attn  = std::dynamic_pointer_cast<SelfAttention>(blocks[\"attn\"]);\n        auto attn2 = std::dynamic_pointer_cast<SelfAttention>(blocks[\"attn2\"]);\n        auto norm2 = std::dynamic_pointer_cast<LayerNorm>(blocks[\"norm2\"]);\n        auto mlp   = std::dynamic_pointer_cast<Mlp>(blocks[\"mlp\"]);\n\n        gate_msa  = ggml_reshape_3d(ctx, gate_msa, gate_msa->ne[0], 1, gate_msa->ne[1]);     // [N, 1, hidden_size]\n        gate_mlp  = ggml_reshape_3d(ctx, gate_mlp, gate_mlp->ne[0], 1, gate_mlp->ne[1]);     // [N, 1, hidden_size]\n        gate_msa2 = ggml_reshape_3d(ctx, gate_msa2, gate_msa2->ne[0], 1, gate_msa2->ne[1]);  // [N, 1, hidden_size]\n\n        attn_out  = attn->post_attention(ctx, attn_out);\n        attn2_out = attn2->post_attention(ctx, attn2_out);\n\n        x            = ggml_add(ctx, x, ggml_mul(ctx, attn_out, gate_msa));\n        x            = ggml_add(ctx, x, ggml_mul(ctx, attn2_out, gate_msa2));\n        auto mlp_out = mlp->forward(ctx, modulate(ctx, norm2->forward(ctx, x), shift_mlp, scale_mlp));\n        x            = ggml_add(ctx, x, ggml_mul(ctx, mlp_out, gate_mlp));\n\n        return x;\n    }\n\n    struct ggml_tensor* post_attention(struct ggml_context* ctx,\n                                       struct ggml_tensor* attn_out,\n                                       struct ggml_tensor* x,\n                                       struct ggml_tensor* gate_msa,\n                                       struct ggml_tensor* shift_mlp,\n                                       struct ggml_tensor* scale_mlp,\n                                       struct ggml_tensor* gate_mlp) {\n        // attn_out: [N, n_token, hidden_size]\n        // x: [N, n_token, hidden_size]\n        // gate_msa: [N, hidden_size]\n        // shift_mlp: [N, hidden_size]\n        // scale_mlp: [N, hidden_size]\n        // gate_mlp: [N, hidden_size]\n        // return: [N, n_token, hidden_size]\n        GGML_ASSERT(!pre_only);\n\n        auto attn  = std::dynamic_pointer_cast<SelfAttention>(blocks[\"attn\"]);\n        auto norm2 = std::dynamic_pointer_cast<LayerNorm>(blocks[\"norm2\"]);\n        auto mlp   = std::dynamic_pointer_cast<Mlp>(blocks[\"mlp\"]);\n\n        gate_msa = ggml_reshape_3d(ctx, gate_msa, gate_msa->ne[0], 1, gate_msa->ne[1]);  // [N, 1, hidden_size]\n        gate_mlp = ggml_reshape_3d(ctx, gate_mlp, gate_mlp->ne[0], 1, gate_mlp->ne[1]);  // [N, 1, hidden_size]\n\n        attn_out = attn->post_attention(ctx, attn_out);\n\n        x            = ggml_add(ctx, x, ggml_mul(ctx, attn_out, gate_msa));\n        auto mlp_out = mlp->forward(ctx, modulate(ctx, norm2->forward(ctx, x), shift_mlp, scale_mlp));\n        x            = ggml_add(ctx, x, ggml_mul(ctx, mlp_out, gate_mlp));\n\n        return x;\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x, struct ggml_tensor* c) {\n        // x: [N, n_token, hidden_size]\n        // c: [N, hidden_size]\n        // return: [N, n_token, hidden_size]\n\n        auto attn = std::dynamic_pointer_cast<SelfAttention>(blocks[\"attn\"]);\n        if (self_attn) {\n            auto qkv_intermediates = pre_attention_x(ctx, x, c);\n            // auto qkv               = qkv_intermediates.first;\n            // auto intermediates     = qkv_intermediates.second;\n            // no longer a pair, but a tuple\n            auto qkv           = std::get<0>(qkv_intermediates);\n            auto qkv2          = std::get<1>(qkv_intermediates);\n            auto intermediates = std::get<2>(qkv_intermediates);\n\n            auto attn_out  = ggml_nn_attention_ext(ctx, qkv[0], qkv[1], qkv[2], num_heads);     // [N, n_token, dim]\n            auto attn2_out = ggml_nn_attention_ext(ctx, qkv2[0], qkv2[1], qkv2[2], num_heads);  // [N, n_token, dim]\n            x              = post_attention_x(ctx,\n                                              attn_out,\n                                              attn2_out,\n                                              intermediates[0],\n                                              intermediates[1],\n                                              intermediates[2],\n                                              intermediates[3],\n                                              intermediates[4],\n                                              intermediates[5]);\n            return x;  // [N, n_token, dim]\n        } else {\n            auto qkv_intermediates = pre_attention(ctx, x, c);\n            auto qkv               = qkv_intermediates.first;\n            auto intermediates     = qkv_intermediates.second;\n\n            auto attn_out = ggml_nn_attention_ext(ctx, qkv[0], qkv[1], qkv[2], num_heads);  // [N, n_token, dim]\n            x             = post_attention(ctx,\n                                           attn_out,\n                                           intermediates[0],\n                                           intermediates[1],\n                                           intermediates[2],\n                                           intermediates[3],\n                                           intermediates[4]);\n            return x;  // [N, n_token, dim]\n        }\n    }\n};\n\n__STATIC_INLINE__ std::pair<struct ggml_tensor*, struct ggml_tensor*>\nblock_mixing(struct ggml_context* ctx,\n             struct ggml_tensor* context,\n             struct ggml_tensor* x,\n             struct ggml_tensor* c,\n             std::shared_ptr<DismantledBlock> context_block,\n             std::shared_ptr<DismantledBlock> x_block) {\n    // context: [N, n_context, hidden_size]\n    // x: [N, n_token, hidden_size]\n    // c: [N, hidden_size]\n    auto context_qkv_intermediates = context_block->pre_attention(ctx, context, c);\n    auto context_qkv               = context_qkv_intermediates.first;\n    auto context_intermediates     = context_qkv_intermediates.second;\n\n    std::vector<ggml_tensor*> x_qkv, x_qkv2, x_intermediates;\n\n    if (x_block->self_attn) {\n        auto x_qkv_intermediates = x_block->pre_attention_x(ctx, x, c);\n        x_qkv                    = std::get<0>(x_qkv_intermediates);\n        x_qkv2                   = std::get<1>(x_qkv_intermediates);\n        x_intermediates          = std::get<2>(x_qkv_intermediates);\n    } else {\n        auto x_qkv_intermediates = x_block->pre_attention(ctx, x, c);\n        x_qkv                    = x_qkv_intermediates.first;\n        x_intermediates          = x_qkv_intermediates.second;\n    }\n    std::vector<struct ggml_tensor*> qkv;\n    for (int i = 0; i < 3; i++) {\n        qkv.push_back(ggml_concat(ctx, context_qkv[i], x_qkv[i], 1));\n    }\n\n    auto attn         = ggml_nn_attention_ext(ctx, qkv[0], qkv[1], qkv[2], x_block->num_heads);  // [N, n_context + n_token, hidden_size]\n    attn              = ggml_cont(ctx, ggml_permute(ctx, attn, 0, 2, 1, 3));                     // [n_context + n_token, N, hidden_size]\n    auto context_attn = ggml_view_3d(ctx,\n                                     attn,\n                                     attn->ne[0],\n                                     attn->ne[1],\n                                     context->ne[1],\n                                     attn->nb[1],\n                                     attn->nb[2],\n                                     0);                                              // [n_context, N, hidden_size]\n    context_attn      = ggml_cont(ctx, ggml_permute(ctx, context_attn, 0, 2, 1, 3));  // [N, n_context, hidden_size]\n    auto x_attn       = ggml_view_3d(ctx,\n                                     attn,\n                                     attn->ne[0],\n                                     attn->ne[1],\n                                     x->ne[1],\n                                     attn->nb[1],\n                                     attn->nb[2],\n                                     attn->nb[2] * context->ne[1]);             // [n_token, N, hidden_size]\n    x_attn            = ggml_cont(ctx, ggml_permute(ctx, x_attn, 0, 2, 1, 3));  // [N, n_token, hidden_size]\n\n    if (!context_block->pre_only) {\n        context = context_block->post_attention(ctx,\n                                                context_attn,\n                                                context_intermediates[0],\n                                                context_intermediates[1],\n                                                context_intermediates[2],\n                                                context_intermediates[3],\n                                                context_intermediates[4]);\n    } else {\n        context = NULL;\n    }\n\n    if (x_block->self_attn) {\n        auto attn2 = ggml_nn_attention_ext(ctx, x_qkv2[0], x_qkv2[1], x_qkv2[2], x_block->num_heads);  // [N, n_token, hidden_size]\n\n        x = x_block->post_attention_x(ctx,\n                                      x_attn,\n                                      attn2,\n                                      x_intermediates[0],\n                                      x_intermediates[1],\n                                      x_intermediates[2],\n                                      x_intermediates[3],\n                                      x_intermediates[4],\n                                      x_intermediates[5]);\n    } else {\n        x = x_block->post_attention(ctx,\n                                    x_attn,\n                                    x_intermediates[0],\n                                    x_intermediates[1],\n                                    x_intermediates[2],\n                                    x_intermediates[3],\n                                    x_intermediates[4]);\n    }\n\n    return {context, x};\n}\n\nstruct JointBlock : public GGMLBlock {\npublic:\n    JointBlock(int64_t hidden_size,\n               int64_t num_heads,\n               float mlp_ratio     = 4.0,\n               std::string qk_norm = \"\",\n               bool qkv_bias       = false,\n               bool pre_only       = false,\n               bool self_attn_x    = false) {\n        blocks[\"context_block\"] = std::shared_ptr<GGMLBlock>(new DismantledBlock(hidden_size, num_heads, mlp_ratio, qk_norm, qkv_bias, pre_only));\n        blocks[\"x_block\"]       = std::shared_ptr<GGMLBlock>(new DismantledBlock(hidden_size, num_heads, mlp_ratio, qk_norm, qkv_bias, false, self_attn_x));\n    }\n\n    std::pair<struct ggml_tensor*, struct ggml_tensor*> forward(struct ggml_context* ctx,\n                                                                struct ggml_tensor* context,\n                                                                struct ggml_tensor* x,\n                                                                struct ggml_tensor* c) {\n        auto context_block = std::dynamic_pointer_cast<DismantledBlock>(blocks[\"context_block\"]);\n        auto x_block       = std::dynamic_pointer_cast<DismantledBlock>(blocks[\"x_block\"]);\n\n        return block_mixing(ctx, context, x, c, context_block, x_block);\n    }\n};\n\nstruct FinalLayer : public GGMLBlock {\n    // The final layer of DiT.\npublic:\n    FinalLayer(int64_t hidden_size,\n               int64_t patch_size,\n               int64_t out_channels) {\n        // total_out_channels is always None\n        blocks[\"norm_final\"]         = std::shared_ptr<GGMLBlock>(new LayerNorm(hidden_size, 1e-06f, false));\n        blocks[\"linear\"]             = std::shared_ptr<GGMLBlock>(new Linear(hidden_size, patch_size * patch_size * out_channels, true, true));\n        blocks[\"adaLN_modulation.1\"] = std::shared_ptr<GGMLBlock>(new Linear(hidden_size, 2 * hidden_size));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* x,\n                                struct ggml_tensor* c) {\n        // x: [N, n_token, hidden_size]\n        // c: [N, hidden_size]\n        // return: [N, n_token, patch_size * patch_size * out_channels]\n        auto norm_final         = std::dynamic_pointer_cast<LayerNorm>(blocks[\"norm_final\"]);\n        auto linear             = std::dynamic_pointer_cast<Linear>(blocks[\"linear\"]);\n        auto adaLN_modulation_1 = std::dynamic_pointer_cast<Linear>(blocks[\"adaLN_modulation.1\"]);\n\n        auto m = adaLN_modulation_1->forward(ctx, ggml_silu(ctx, c));  // [N, 2 * hidden_size]\n        m      = ggml_reshape_3d(ctx, m, c->ne[0], 2, c->ne[1]);       // [N, 2, hidden_size]\n        m      = ggml_cont(ctx, ggml_permute(ctx, m, 0, 2, 1, 3));     // [2, N, hidden_size]\n\n        int64_t offset = m->nb[1] * m->ne[1];\n        auto shift     = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 0);  // [N, hidden_size]\n        auto scale     = ggml_view_2d(ctx, m, m->ne[0], m->ne[1], m->nb[1], offset * 1);  // [N, hidden_size]\n\n        x = modulate(ctx, norm_final->forward(ctx, x), shift, scale);\n        x = linear->forward(ctx, x);\n\n        return x;\n    }\n};\n\nstruct MMDiT : public GGMLBlock {\n    // Diffusion model with a Transformer backbone.\nprotected:\n    int64_t input_size               = -1;\n    int64_t patch_size               = 2;\n    int64_t in_channels              = 16;\n    int64_t d_self                   = -1;  // >=0 for MMdiT-X\n    int64_t depth                    = 24;\n    float mlp_ratio                  = 4.0f;\n    int64_t adm_in_channels          = 2048;\n    int64_t out_channels             = 16;\n    int64_t pos_embed_max_size       = 192;\n    int64_t num_patchs               = 36864;  // 192 * 192\n    int64_t context_size             = 4096;\n    int64_t context_embedder_out_dim = 1536;\n    int64_t hidden_size;\n    std::string qk_norm;\n\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, std::string prefix = \"\") {\n        enum ggml_type wtype = GGML_TYPE_F32;  //(tensor_types.find(prefix + \"pos_embed\") != tensor_types.end()) ? tensor_types[prefix + \"pos_embed\"] : GGML_TYPE_F32;\n        params[\"pos_embed\"]  = ggml_new_tensor_3d(ctx, wtype, hidden_size, num_patchs, 1);\n    }\n\npublic:\n    MMDiT(std::map<std::string, enum ggml_type>& tensor_types) {\n        // input_size is always None\n        // learn_sigma is always False\n        // register_length is alwalys 0\n        // rmsnorm is alwalys False\n        // scale_mod_only is alwalys False\n        // swiglu is alwalys False\n        // qkv_bias is always True\n        // context_processor_layers is always None\n        // pos_embed_scaling_factor is not used\n        // pos_embed_offset is not used\n        // context_embedder_config is always {'target': 'torch.nn.Linear', 'params': {'in_features': 4096, 'out_features': 1536}}\n\n        // read tensors from tensor_types\n        for (auto pair : tensor_types) {\n            std::string tensor_name = pair.first;\n            if (tensor_name.find(\"model.diffusion_model.\") == std::string::npos)\n                continue;\n            size_t jb = tensor_name.find(\"joint_blocks.\");\n            if (jb != std::string::npos) {\n                tensor_name     = tensor_name.substr(jb);  // remove prefix\n                int block_depth = atoi(tensor_name.substr(13, tensor_name.find(\".\", 13)).c_str());\n                if (block_depth + 1 > depth) {\n                    depth = block_depth + 1;\n                }\n                if (tensor_name.find(\"attn.ln\") != std::string::npos) {\n                    if (tensor_name.find(\".bias\") != std::string::npos) {\n                        qk_norm = \"ln\";\n                    } else {\n                        qk_norm = \"rms\";\n                    }\n                }\n                if (tensor_name.find(\"attn2\") != std::string::npos) {\n                    if (block_depth > d_self) {\n                        d_self = block_depth;\n                    }\n                }\n            }\n        }\n\n        if (d_self >= 0) {\n            pos_embed_max_size *= 2;\n            num_patchs *= 4;\n        }\n\n        LOG_INFO(\"MMDiT layers: %d (including %d MMDiT-x layers)\", depth, d_self + 1);\n\n        int64_t default_out_channels = in_channels;\n        hidden_size                  = 64 * depth;\n        context_embedder_out_dim     = 64 * depth;\n        int64_t num_heads            = depth;\n\n        blocks[\"x_embedder\"] = std::shared_ptr<GGMLBlock>(new PatchEmbed(input_size, patch_size, in_channels, hidden_size, true));\n        blocks[\"t_embedder\"] = std::shared_ptr<GGMLBlock>(new TimestepEmbedder(hidden_size));\n\n        if (adm_in_channels != -1) {\n            blocks[\"y_embedder\"] = std::shared_ptr<GGMLBlock>(new VectorEmbedder(adm_in_channels, hidden_size));\n        }\n\n        blocks[\"context_embedder\"] = std::shared_ptr<GGMLBlock>(new Linear(4096, context_embedder_out_dim, true, true));\n\n        for (int i = 0; i < depth; i++) {\n            blocks[\"joint_blocks.\" + std::to_string(i)] = std::shared_ptr<GGMLBlock>(new JointBlock(hidden_size,\n                                                                                                    num_heads,\n                                                                                                    mlp_ratio,\n                                                                                                    qk_norm,\n                                                                                                    true,\n                                                                                                    i == depth - 1,\n                                                                                                    i <= d_self));\n        }\n\n        blocks[\"final_layer\"] = std::shared_ptr<GGMLBlock>(new FinalLayer(hidden_size, patch_size, out_channels));\n    }\n\n    struct ggml_tensor*\n    cropped_pos_embed(struct ggml_context* ctx,\n                      int64_t h,\n                      int64_t w) {\n        auto pos_embed = params[\"pos_embed\"];\n\n        h = (h + 1) / patch_size;\n        w = (w + 1) / patch_size;\n\n        GGML_ASSERT(h <= pos_embed_max_size && h > 0);\n        GGML_ASSERT(w <= pos_embed_max_size && w > 0);\n\n        int64_t top  = (pos_embed_max_size - h) / 2;\n        int64_t left = (pos_embed_max_size - w) / 2;\n\n        auto spatial_pos_embed = ggml_reshape_3d(ctx, pos_embed, hidden_size, pos_embed_max_size, pos_embed_max_size);\n\n        // spatial_pos_embed = spatial_pos_embed[:, top : top + h, left : left + w, :]\n        spatial_pos_embed = ggml_view_3d(ctx,\n                                         spatial_pos_embed,\n                                         hidden_size,\n                                         pos_embed_max_size,\n                                         h,\n                                         spatial_pos_embed->nb[1],\n                                         spatial_pos_embed->nb[2],\n                                         spatial_pos_embed->nb[2] * top);                      // [h, pos_embed_max_size, hidden_size]\n        spatial_pos_embed = ggml_cont(ctx, ggml_permute(ctx, spatial_pos_embed, 0, 2, 1, 3));  // [pos_embed_max_size, h, hidden_size]\n        spatial_pos_embed = ggml_view_3d(ctx,\n                                         spatial_pos_embed,\n                                         hidden_size,\n                                         h,\n                                         w,\n                                         spatial_pos_embed->nb[1],\n                                         spatial_pos_embed->nb[2],\n                                         spatial_pos_embed->nb[2] * left);                     // [w, h, hidden_size]\n        spatial_pos_embed = ggml_cont(ctx, ggml_permute(ctx, spatial_pos_embed, 0, 2, 1, 3));  // [h, w, hidden_size]\n        spatial_pos_embed = ggml_reshape_3d(ctx, spatial_pos_embed, hidden_size, h * w, 1);    // [1, h*w, hidden_size]\n        return spatial_pos_embed;\n    }\n\n    struct ggml_tensor* unpatchify(struct ggml_context* ctx,\n                                   struct ggml_tensor* x,\n                                   int64_t h,\n                                   int64_t w) {\n        // x: [N, H*W, patch_size * patch_size * C]\n        // return: [N, C, H, W]\n        int64_t n = x->ne[2];\n        int64_t c = out_channels;\n        int64_t p = patch_size;\n        h         = (h + 1) / p;\n        w         = (w + 1) / p;\n\n        GGML_ASSERT(h * w == x->ne[1]);\n\n        x = ggml_reshape_4d(ctx, x, c, p * p, w * h, n);       // [N, H*W, P*P, C]\n        x = ggml_cont(ctx, ggml_permute(ctx, x, 2, 0, 1, 3));  // [N, C, H*W, P*P]\n        x = ggml_reshape_4d(ctx, x, p, p, w, h * c * n);       // [N*C*H, W, P, P]\n        x = ggml_cont(ctx, ggml_permute(ctx, x, 0, 2, 1, 3));  // [N*C*H, P, W, P]\n        x = ggml_reshape_4d(ctx, x, p * w, p * h, c, n);       // [N, C, H*P, W*P]\n        return x;\n    }\n\n    struct ggml_tensor* forward_core_with_concat(struct ggml_context* ctx,\n                                                 struct ggml_tensor* x,\n                                                 struct ggml_tensor* c_mod,\n                                                 struct ggml_tensor* context,\n                                                 std::vector<int> skip_layers = std::vector<int>()) {\n        // x: [N, H*W, hidden_size]\n        // context: [N, n_context, d_context]\n        // c: [N, hidden_size]\n        // return: [N, N*W, patch_size * patch_size * out_channels]\n        auto final_layer = std::dynamic_pointer_cast<FinalLayer>(blocks[\"final_layer\"]);\n\n        for (int i = 0; i < depth; i++) {\n            // skip iteration if i is in skip_layers\n            if (skip_layers.size() > 0 && std::find(skip_layers.begin(), skip_layers.end(), i) != skip_layers.end()) {\n                continue;\n            }\n\n            auto block = std::dynamic_pointer_cast<JointBlock>(blocks[\"joint_blocks.\" + std::to_string(i)]);\n\n            auto context_x = block->forward(ctx, context, x, c_mod);\n            context        = context_x.first;\n            x              = context_x.second;\n        }\n\n        x = final_layer->forward(ctx, x, c_mod);  // (N, T, patch_size ** 2 * out_channels)\n\n        return x;\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* x,\n                                struct ggml_tensor* t,\n                                struct ggml_tensor* y        = NULL,\n                                struct ggml_tensor* context  = NULL,\n                                std::vector<int> skip_layers = std::vector<int>()) {\n        // Forward pass of DiT.\n        // x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)\n        // t: (N,) tensor of diffusion timesteps\n        // y: (N, adm_in_channels) tensor of class labels\n        // context: (N, L, D)\n        // return: (N, C, H, W)\n        auto x_embedder = std::dynamic_pointer_cast<PatchEmbed>(blocks[\"x_embedder\"]);\n        auto t_embedder = std::dynamic_pointer_cast<TimestepEmbedder>(blocks[\"t_embedder\"]);\n\n        int64_t w = x->ne[0];\n        int64_t h = x->ne[1];\n\n        auto patch_embed = x_embedder->forward(ctx, x);            // [N, H*W, hidden_size]\n        auto pos_embed   = cropped_pos_embed(ctx, h, w);           // [1, H*W, hidden_size]\n        x                = ggml_add(ctx, patch_embed, pos_embed);  // [N, H*W, hidden_size]\n\n        auto c = t_embedder->forward(ctx, t);  // [N, hidden_size]\n        if (y != NULL && adm_in_channels != -1) {\n            auto y_embedder = std::dynamic_pointer_cast<VectorEmbedder>(blocks[\"y_embedder\"]);\n\n            y = y_embedder->forward(ctx, y);  // [N, hidden_size]\n            c = ggml_add(ctx, c, y);\n        }\n\n        if (context != NULL) {\n            auto context_embedder = std::dynamic_pointer_cast<Linear>(blocks[\"context_embedder\"]);\n\n            context = context_embedder->forward(ctx, context);  // [N, L, D] aka [N, L, 1536]\n        }\n\n        x = forward_core_with_concat(ctx, x, c, context, skip_layers);  // (N, H*W, patch_size ** 2 * out_channels)\n\n        x = unpatchify(ctx, x, h, w);  // [N, C, H, W]\n\n        return x;\n    }\n};\nstruct MMDiTRunner : public GGMLRunner {\n    MMDiT mmdit;\n\n    static std::map<std::string, enum ggml_type> empty_tensor_types;\n\n    MMDiTRunner(ggml_backend_t backend,\n                std::map<std::string, enum ggml_type>& tensor_types = empty_tensor_types,\n                const std::string prefix                            = \"\")\n        : GGMLRunner(backend), mmdit(tensor_types) {\n        mmdit.init(params_ctx, tensor_types, prefix);\n    }\n\n    std::string get_desc() {\n        return \"mmdit\";\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors, const std::string prefix) {\n        mmdit.get_param_tensors(tensors, prefix);\n    }\n\n    struct ggml_cgraph* build_graph(struct ggml_tensor* x,\n                                    struct ggml_tensor* timesteps,\n                                    struct ggml_tensor* context,\n                                    struct ggml_tensor* y,\n                                    std::vector<int> skip_layers = std::vector<int>()) {\n        struct ggml_cgraph* gf = ggml_new_graph_custom(compute_ctx, MMDIT_GRAPH_SIZE, false);\n\n        x         = to_backend(x);\n        context   = to_backend(context);\n        y         = to_backend(y);\n        timesteps = to_backend(timesteps);\n\n        struct ggml_tensor* out = mmdit.forward(compute_ctx,\n                                                x,\n                                                timesteps,\n                                                y,\n                                                context,\n                                                skip_layers);\n\n        ggml_build_forward_expand(gf, out);\n\n        return gf;\n    }\n\n    void compute(int n_threads,\n                 struct ggml_tensor* x,\n                 struct ggml_tensor* timesteps,\n                 struct ggml_tensor* context,\n                 struct ggml_tensor* y,\n                 struct ggml_tensor** output     = NULL,\n                 struct ggml_context* output_ctx = NULL,\n                 std::vector<int> skip_layers    = std::vector<int>()) {\n        // x: [N, in_channels, h, w]\n        // timesteps: [N, ]\n        // context: [N, max_position, hidden_size]([N, 154, 4096]) or [1, max_position, hidden_size]\n        // y: [N, adm_in_channels] or [1, adm_in_channels]\n        auto get_graph = [&]() -> struct ggml_cgraph* {\n            return build_graph(x, timesteps, context, y, skip_layers);\n        };\n\n        GGMLRunner::compute(get_graph, n_threads, false, output, output_ctx);\n    }\n\n    void test() {\n        struct ggml_init_params params;\n        params.mem_size   = static_cast<size_t>(10 * 1024 * 1024);  // 10 MB\n        params.mem_buffer = NULL;\n        params.no_alloc   = false;\n\n        struct ggml_context* work_ctx = ggml_init(params);\n        GGML_ASSERT(work_ctx != NULL);\n\n        {\n            // cpu f16: pass\n            // cpu f32: pass\n            // cuda f16: pass\n            // cuda f32: pass\n            auto x = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 128, 128, 16, 1);\n            std::vector<float> timesteps_vec(1, 999.f);\n            auto timesteps = vector_to_ggml_tensor(work_ctx, timesteps_vec);\n            ggml_set_f32(x, 0.01f);\n            // print_ggml_tensor(x);\n\n            auto context = ggml_new_tensor_3d(work_ctx, GGML_TYPE_F32, 4096, 154, 1);\n            ggml_set_f32(context, 0.01f);\n            // print_ggml_tensor(context);\n\n            auto y = ggml_new_tensor_2d(work_ctx, GGML_TYPE_F32, 2048, 1);\n            ggml_set_f32(y, 0.01f);\n            // print_ggml_tensor(y);\n\n            struct ggml_tensor* out = NULL;\n\n            int t0 = ggml_time_ms();\n            compute(8, x, timesteps, context, y, &out, work_ctx);\n            int t1 = ggml_time_ms();\n\n            print_ggml_tensor(out);\n            LOG_DEBUG(\"mmdit test done in %dms\", t1 - t0);\n        }\n    }\n\n    static void load_from_file_and_test(const std::string& file_path) {\n        // ggml_backend_t backend    = ggml_backend_cuda_init(0);\n        ggml_backend_t backend             = ggml_backend_cpu_init();\n        ggml_type model_data_type          = GGML_TYPE_F16;\n        std::shared_ptr<MMDiTRunner> mmdit = std::shared_ptr<MMDiTRunner>(new MMDiTRunner(backend));\n        {\n            LOG_INFO(\"loading from '%s'\", file_path.c_str());\n\n            mmdit->alloc_params_buffer();\n            std::map<std::string, ggml_tensor*> tensors;\n            mmdit->get_param_tensors(tensors, \"model.diffusion_model\");\n\n            ModelLoader model_loader;\n            if (!model_loader.init_from_file(file_path)) {\n                LOG_ERROR(\"init model loader from file failed: '%s'\", file_path.c_str());\n                return;\n            }\n\n            bool success = model_loader.load_tensors(tensors, backend);\n\n            if (!success) {\n                LOG_ERROR(\"load tensors from model loader failed\");\n                return;\n            }\n\n            LOG_INFO(\"mmdit model loaded\");\n        }\n        mmdit->test();\n    }\n};\n\n#endif"
        },
        {
          "name": "model.cpp",
          "type": "blob",
          "size": 77.001953125,
          "content": "#include <stdarg.h>\n#include <fstream>\n#include <regex>\n#include <set>\n#include <string>\n#include <unordered_map>\n#include <vector>\n\n#include \"model.h\"\n#include \"stable-diffusion.h\"\n#include \"util.h\"\n#include \"vocab.hpp\"\n\n#include \"ggml-alloc.h\"\n#include \"ggml-backend.h\"\n#include \"ggml-cpu.h\"\n#include \"ggml.h\"\n\n#include \"stable-diffusion.h\"\n\n#ifdef SD_USE_METAL\n#include \"ggml-metal.h\"\n#endif\n\n#ifdef SD_USE_VULKAN\n#include \"ggml-vulkan.h\"\n#endif\n\n#define ST_HEADER_SIZE_LEN 8\n\nuint64_t read_u64(uint8_t* buffer) {\n    // little endian\n    uint64_t value = 0;\n    value |= static_cast<int64_t>(buffer[7]) << 56;\n    value |= static_cast<int64_t>(buffer[6]) << 48;\n    value |= static_cast<int64_t>(buffer[5]) << 40;\n    value |= static_cast<int64_t>(buffer[4]) << 32;\n    value |= static_cast<int64_t>(buffer[3]) << 24;\n    value |= static_cast<int64_t>(buffer[2]) << 16;\n    value |= static_cast<int64_t>(buffer[1]) << 8;\n    value |= static_cast<int64_t>(buffer[0]);\n    return value;\n}\n\nint32_t read_int(uint8_t* buffer) {\n    // little endian\n    int value = 0;\n    value |= buffer[3] << 24;\n    value |= buffer[2] << 16;\n    value |= buffer[1] << 8;\n    value |= buffer[0];\n    return value;\n}\n\nuint16_t read_short(uint8_t* buffer) {\n    // little endian\n    uint16_t value = 0;\n    value |= buffer[1] << 8;\n    value |= buffer[0];\n    return value;\n}\n\n/*================================================= Preprocess ==================================================*/\n\nstd::string self_attn_names[] = {\n    \"self_attn.q_proj.weight\",\n    \"self_attn.k_proj.weight\",\n    \"self_attn.v_proj.weight\",\n    \"self_attn.q_proj.bias\",\n    \"self_attn.k_proj.bias\",\n    \"self_attn.v_proj.bias\",\n};\n\nconst char* unused_tensors[] = {\n    \"betas\",\n    \"alphas_cumprod_prev\",\n    \"sqrt_alphas_cumprod\",\n    \"sqrt_one_minus_alphas_cumprod\",\n    \"log_one_minus_alphas_cumprod\",\n    \"sqrt_recip_alphas_cumprod\",\n    \"sqrt_recipm1_alphas_cumprod\",\n    \"posterior_variance\",\n    \"posterior_log_variance_clipped\",\n    \"posterior_mean_coef1\",\n    \"posterior_mean_coef2\",\n    \"cond_stage_model.transformer.text_model.embeddings.position_ids\",\n    \"cond_stage_model.model.logit_scale\",\n    \"cond_stage_model.model.text_projection\",\n    \"conditioner.embedders.0.transformer.text_model.embeddings.position_ids\",\n    \"conditioner.embedders.0.model.logit_scale\",\n    \"conditioner.embedders.1.model.logit_scale\",\n    \"model.diffusion_model.time_embedding.cond_proj.weight\",\n    \"unet.time_embedding.cond_proj.weight\",\n    \"model_ema.decay\",\n    \"model_ema.num_updates\",\n    \"model_ema.diffusion_model\",\n    \"embedding_manager\",\n    \"denoiser.sigmas\",\n};\n\nbool is_unused_tensor(std::string name) {\n    for (int i = 0; i < sizeof(unused_tensors) / sizeof(const char*); i++) {\n        if (starts_with(name, unused_tensors[i])) {\n            return true;\n        }\n    }\n    return false;\n}\n\nstd::unordered_map<std::string, std::string> open_clip_to_hf_clip_model = {\n    {\"model.ln_final.bias\", \"transformer.text_model.final_layer_norm.bias\"},\n    {\"model.ln_final.weight\", \"transformer.text_model.final_layer_norm.weight\"},\n    {\"model.positional_embedding\", \"transformer.text_model.embeddings.position_embedding.weight\"},\n    {\"model.token_embedding.weight\", \"transformer.text_model.embeddings.token_embedding.weight\"},\n    {\"model.text_projection\", \"transformer.text_model.text_projection\"},\n    {\"model.visual.class_embedding\", \"transformer.vision_model.embeddings.class_embedding\"},\n    {\"model.visual.conv1.weight\", \"transformer.vision_model.embeddings.patch_embedding.weight\"},\n    {\"model.visual.ln_post.bias\", \"transformer.vision_model.post_layernorm.bias\"},\n    {\"model.visual.ln_post.weight\", \"transformer.vision_model.post_layernorm.weight\"},\n    {\"model.visual.ln_pre.bias\", \"transformer.vision_model.pre_layernorm.bias\"},\n    {\"model.visual.ln_pre.weight\", \"transformer.vision_model.pre_layernorm.weight\"},\n    {\"model.visual.positional_embedding\", \"transformer.vision_model.embeddings.position_embedding.weight\"},\n    {\"model.visual.proj\", \"transformer.visual_projection.weight\"},\n};\n\nstd::unordered_map<std::string, std::string> open_clip_to_hk_clip_resblock = {\n    {\"attn.out_proj.bias\", \"self_attn.out_proj.bias\"},\n    {\"attn.out_proj.weight\", \"self_attn.out_proj.weight\"},\n    {\"ln_1.bias\", \"layer_norm1.bias\"},\n    {\"ln_1.weight\", \"layer_norm1.weight\"},\n    {\"ln_2.bias\", \"layer_norm2.bias\"},\n    {\"ln_2.weight\", \"layer_norm2.weight\"},\n    {\"mlp.c_fc.bias\", \"mlp.fc1.bias\"},\n    {\"mlp.c_fc.weight\", \"mlp.fc1.weight\"},\n    {\"mlp.c_proj.bias\", \"mlp.fc2.bias\"},\n    {\"mlp.c_proj.weight\", \"mlp.fc2.weight\"},\n};\n\nstd::unordered_map<std::string, std::string> vae_decoder_name_map = {\n    {\"first_stage_model.decoder.mid.attn_1.to_k.bias\", \"first_stage_model.decoder.mid.attn_1.k.bias\"},\n    {\"first_stage_model.decoder.mid.attn_1.to_k.weight\", \"first_stage_model.decoder.mid.attn_1.k.weight\"},\n    {\"first_stage_model.decoder.mid.attn_1.to_out.0.bias\", \"first_stage_model.decoder.mid.attn_1.proj_out.bias\"},\n    {\"first_stage_model.decoder.mid.attn_1.to_out.0.weight\", \"first_stage_model.decoder.mid.attn_1.proj_out.weight\"},\n    {\"first_stage_model.decoder.mid.attn_1.to_q.bias\", \"first_stage_model.decoder.mid.attn_1.q.bias\"},\n    {\"first_stage_model.decoder.mid.attn_1.to_q.weight\", \"first_stage_model.decoder.mid.attn_1.q.weight\"},\n    {\"first_stage_model.decoder.mid.attn_1.to_v.bias\", \"first_stage_model.decoder.mid.attn_1.v.bias\"},\n    {\"first_stage_model.decoder.mid.attn_1.to_v.weight\", \"first_stage_model.decoder.mid.attn_1.v.weight\"},\n};\n\nstd::unordered_map<std::string, std::string> pmid_v2_name_map = {\n    {\"pmid.qformer_perceiver.perceiver_resampler.layers.0.1.1.weight\",\n     \"pmid.qformer_perceiver.perceiver_resampler.layers.0.1.1.fc1.weight\"},\n    {\"pmid.qformer_perceiver.perceiver_resampler.layers.0.1.3.weight\",\n     \"pmid.qformer_perceiver.perceiver_resampler.layers.0.1.1.fc2.weight\"},\n    {\"pmid.qformer_perceiver.perceiver_resampler.layers.1.1.1.weight\",\n     \"pmid.qformer_perceiver.perceiver_resampler.layers.1.1.1.fc1.weight\"},\n    {\"pmid.qformer_perceiver.perceiver_resampler.layers.1.1.3.weight\",\n     \"pmid.qformer_perceiver.perceiver_resampler.layers.1.1.1.fc2.weight\"},\n    {\"pmid.qformer_perceiver.perceiver_resampler.layers.2.1.1.weight\",\n     \"pmid.qformer_perceiver.perceiver_resampler.layers.2.1.1.fc1.weight\"},\n    {\"pmid.qformer_perceiver.perceiver_resampler.layers.2.1.3.weight\",\n     \"pmid.qformer_perceiver.perceiver_resampler.layers.2.1.1.fc2.weight\"},\n    {\"pmid.qformer_perceiver.perceiver_resampler.layers.3.1.1.weight\",\n     \"pmid.qformer_perceiver.perceiver_resampler.layers.3.1.1.fc1.weight\"},\n    {\"pmid.qformer_perceiver.perceiver_resampler.layers.3.1.3.weight\",\n     \"pmid.qformer_perceiver.perceiver_resampler.layers.3.1.1.fc2.weight\"},\n    {\"pmid.qformer_perceiver.token_proj.0.bias\",\n     \"pmid.qformer_perceiver.token_proj.fc1.bias\"},\n    {\"pmid.qformer_perceiver.token_proj.2.bias\",\n     \"pmid.qformer_perceiver.token_proj.fc2.bias\"},\n    {\"pmid.qformer_perceiver.token_proj.0.weight\",\n     \"pmid.qformer_perceiver.token_proj.fc1.weight\"},\n    {\"pmid.qformer_perceiver.token_proj.2.weight\",\n     \"pmid.qformer_perceiver.token_proj.fc2.weight\"},\n};\n\nstd::string convert_open_clip_to_hf_clip(const std::string& name) {\n    std::string new_name = name;\n    std::string prefix;\n    if (starts_with(new_name, \"conditioner.embedders.0.open_clip.\")) {\n        prefix   = \"cond_stage_model.\";\n        new_name = new_name.substr(strlen(\"conditioner.embedders.0.open_clip.\"));\n    } else if (starts_with(new_name, \"conditioner.embedders.0.\")) {\n        prefix   = \"cond_stage_model.\";\n        new_name = new_name.substr(strlen(\"conditioner.embedders.0.\"));\n    } else if (starts_with(new_name, \"conditioner.embedders.1.\")) {\n        prefix   = \"cond_stage_model.1.\";\n        new_name = new_name.substr(strlen(\"conditioner.embedders.0.\"));\n    } else if (starts_with(new_name, \"cond_stage_model.\")) {\n        prefix   = \"cond_stage_model.\";\n        new_name = new_name.substr(strlen(\"cond_stage_model.\"));\n    } else if (ends_with(new_name, \"vision_model.visual_projection.weight\")) {\n        prefix   = new_name.substr(0, new_name.size() - strlen(\"vision_model.visual_projection.weight\"));\n        new_name = prefix + \"visual_projection.weight\";\n        return new_name;\n    } else if (ends_with(new_name, \"transformer.text_projection.weight\")) {\n        prefix   = new_name.substr(0, new_name.size() - strlen(\"transformer.text_projection.weight\"));\n        new_name = prefix + \"transformer.text_model.text_projection\";\n        return new_name;\n    } else {\n        return new_name;\n    }\n\n    if (open_clip_to_hf_clip_model.find(new_name) != open_clip_to_hf_clip_model.end()) {\n        new_name = open_clip_to_hf_clip_model[new_name];\n    }\n\n    std::string open_clip_resblock_prefix = \"model.transformer.resblocks.\";\n    std::string hf_clip_resblock_prefix   = \"transformer.text_model.encoder.layers.\";\n\n    auto replace_suffix = [&]() {\n        if (new_name.find(open_clip_resblock_prefix) == 0) {\n            std::string remain = new_name.substr(open_clip_resblock_prefix.length());\n            std::string idx    = remain.substr(0, remain.find(\".\"));\n            std::string suffix = remain.substr(idx.length() + 1);\n\n            if (suffix == \"attn.in_proj_weight\" || suffix == \"attn.in_proj_bias\") {\n                new_name = hf_clip_resblock_prefix + idx + \".\" + suffix;\n            } else if (open_clip_to_hk_clip_resblock.find(suffix) != open_clip_to_hk_clip_resblock.end()) {\n                std::string new_suffix = open_clip_to_hk_clip_resblock[suffix];\n                new_name               = hf_clip_resblock_prefix + idx + \".\" + new_suffix;\n            }\n        }\n    };\n\n    replace_suffix();\n\n    open_clip_resblock_prefix = \"model.visual.transformer.resblocks.\";\n    hf_clip_resblock_prefix   = \"transformer.vision_model.encoder.layers.\";\n\n    replace_suffix();\n\n    return prefix + new_name;\n}\n\nstd::string convert_vae_decoder_name(const std::string& name) {\n    if (vae_decoder_name_map.find(name) != vae_decoder_name_map.end()) {\n        return vae_decoder_name_map[name];\n    }\n    return name;\n}\n\nstd::string convert_pmid_v2_name(const std::string& name) {\n    if (pmid_v2_name_map.find(name) != pmid_v2_name_map.end()) {\n        return pmid_v2_name_map[name];\n    }\n    return name;\n}\n\n/* If not a SDXL LoRA the unet\" prefix will have already been replaced by this\n * point and \"te2\" and \"te1\" don't seem to appear in non-SDXL only \"te_\" */\nstd::string convert_sdxl_lora_name(std::string tensor_name) {\n    const std::pair<std::string, std::string> sdxl_lora_name_lookup[] = {\n        {\"unet\", \"model_diffusion_model\"},\n        {\"te2\", \"cond_stage_model_1_transformer\"},\n        {\"te1\", \"cond_stage_model_transformer\"},\n        {\"text_encoder_2\", \"cond_stage_model_1_transformer\"},\n        {\"text_encoder\", \"cond_stage_model_transformer\"},\n    };\n    for (auto& pair_i : sdxl_lora_name_lookup) {\n        if (tensor_name.compare(0, pair_i.first.length(), pair_i.first) == 0) {\n            tensor_name = std::regex_replace(tensor_name, std::regex(pair_i.first), pair_i.second);\n            break;\n        }\n    }\n    return tensor_name;\n}\n\nstd::unordered_map<std::string, std::unordered_map<std::string, std::string>> suffix_conversion_underline = {\n    {\n        \"attentions\",\n        {\n            {\"to_k\", \"k\"},\n            {\"to_q\", \"q\"},\n            {\"to_v\", \"v\"},\n            {\"to_out_0\", \"proj_out\"},\n            {\"group_norm\", \"norm\"},\n        },\n    },\n    {\n        \"resnets\",\n        {\n            {\"conv1\", \"in_layers_2\"},\n            {\"conv2\", \"out_layers_3\"},\n            {\"norm1\", \"in_layers_0\"},\n            {\"norm2\", \"out_layers_0\"},\n            {\"time_emb_proj\", \"emb_layers_1\"},\n            {\"conv_shortcut\", \"skip_connection\"},\n        },\n    },\n};\n\nstd::unordered_map<std::string, std::unordered_map<std::string, std::string>> suffix_conversion_dot = {\n    {\n        \"attentions\",\n        {\n            {\"to_k\", \"k\"},\n            {\"to_q\", \"q\"},\n            {\"to_v\", \"v\"},\n            {\"to_out.0\", \"proj_out\"},\n            {\"group_norm\", \"norm\"},\n        },\n    },\n    {\n        \"resnets\",\n        {\n            {\"conv1\", \"in_layers.2\"},\n            {\"conv2\", \"out_layers.3\"},\n            {\"norm1\", \"in_layers.0\"},\n            {\"norm2\", \"out_layers.0\"},\n            {\"time_emb_proj\", \"emb_layers.1\"},\n            {\"conv_shortcut\", \"skip_connection\"},\n        },\n    },\n};\n\nstd::string convert_diffusers_name_to_compvis(std::string key, char seq) {\n    std::vector<std::string> m;\n\n    auto match = [](std::vector<std::string>& match_list, const std::regex& regex, const std::string& key) {\n        auto r = std::smatch{};\n        if (!std::regex_match(key, r, regex)) {\n            return false;\n        }\n\n        match_list.clear();\n        for (size_t i = 1; i < r.size(); ++i) {\n            match_list.push_back(r.str(i));\n        }\n        return true;\n    };\n\n    std::unordered_map<std::string, std::unordered_map<std::string, std::string>> suffix_conversion;\n    if (seq == '_') {\n        suffix_conversion = suffix_conversion_underline;\n    } else {\n        suffix_conversion = suffix_conversion_dot;\n    }\n\n    auto get_converted_suffix = [&suffix_conversion](const std::string& outer_key, const std::string& inner_key) {\n        auto outer_iter = suffix_conversion.find(outer_key);\n        if (outer_iter != suffix_conversion.end()) {\n            auto inner_iter = outer_iter->second.find(inner_key);\n            if (inner_iter != outer_iter->second.end()) {\n                return inner_iter->second;\n            }\n        }\n        return inner_key;\n    };\n\n    // convert attn to out\n    if (ends_with(key, \"to_out\")) {\n        key += format(\"%c0\", seq);\n    }\n\n    // unet\n    if (match(m, std::regex(format(\"unet%cconv_in(.*)\", seq)), key)) {\n        return format(\"model%cdiffusion_model%cinput_blocks%c0%c0\", seq, seq, seq, seq) + m[0];\n    }\n\n    if (match(m, std::regex(format(\"unet%cconv%cout(.*)\", seq, seq)), key)) {\n        return format(\"model%cdiffusion_model%cout%c2\", seq, seq, seq) + m[0];\n    }\n\n    if (match(m, std::regex(format(\"unet%cconv_norm_out(.*)\", seq)), key)) {\n        return format(\"model%cdiffusion_model%cout%c0\", seq, seq, seq) + m[0];\n    }\n\n    if (match(m, std::regex(format(\"unet%ctime_embedding%clinear_(\\\\d+)(.*)\", seq, seq)), key)) {\n        return format(\"model%cdiffusion_model%ctime_embed%c\", seq, seq, seq) + std::to_string(std::stoi(m[0]) * 2 - 2) + m[1];\n    }\n\n    if (match(m, std::regex(format(\"unet%cdown_blocks%c(\\\\d+)%c(attentions|resnets)%c(\\\\d+)%c(.+)\", seq, seq, seq, seq, seq)), key)) {\n        std::string suffix = get_converted_suffix(m[1], m[3]);\n        // LOG_DEBUG(\"%s %s %s %s\", m[0].c_str(), m[1].c_str(), m[2].c_str(), m[3].c_str());\n        return format(\"model%cdiffusion_model%cinput_blocks%c\", seq, seq, seq) + std::to_string(1 + std::stoi(m[0]) * 3 + std::stoi(m[2])) + seq +\n               (m[1] == \"attentions\" ? \"1\" : \"0\") + seq + suffix;\n    }\n\n    if (match(m, std::regex(format(\"unet%cmid_block%c(attentions|resnets)%c(\\\\d+)%c(.+)\", seq, seq, seq, seq)), key)) {\n        std::string suffix = get_converted_suffix(m[0], m[2]);\n        return format(\"model%cdiffusion_model%cmiddle_block%c\", seq, seq, seq) + (m[0] == \"attentions\" ? \"1\" : std::to_string(std::stoi(m[1]) * 2)) +\n               seq + suffix;\n    }\n\n    if (match(m, std::regex(format(\"unet%cup_blocks%c(\\\\d+)%c(attentions|resnets)%c(\\\\d+)%c(.+)\", seq, seq, seq, seq, seq)), key)) {\n        std::string suffix = get_converted_suffix(m[1], m[3]);\n        return format(\"model%cdiffusion_model%coutput_blocks%c\", seq, seq, seq) + std::to_string(std::stoi(m[0]) * 3 + std::stoi(m[2])) + seq +\n               (m[1] == \"attentions\" ? \"1\" : \"0\") + seq + suffix;\n    }\n\n    if (match(m, std::regex(format(\"unet%cdown_blocks%c(\\\\d+)%cdownsamplers%c0%cconv\", seq, seq, seq, seq, seq)), key)) {\n        return format(\"model%cdiffusion_model%cinput_blocks%c\", seq, seq, seq) + std::to_string(3 + std::stoi(m[0]) * 3) + seq + \"0\" + seq + \"op\";\n    }\n\n    if (match(m, std::regex(format(\"unet%cup_blocks%c(\\\\d+)%cupsamplers%c0%cconv\", seq, seq, seq, seq, seq)), key)) {\n        return format(\"model%cdiffusion_model%coutput_blocks%c\", seq, seq, seq) + std::to_string(2 + std::stoi(m[0]) * 3) + seq +\n               (std::stoi(m[0]) > 0 ? \"2\" : \"1\") + seq + \"conv\";\n    }\n\n    // clip\n    if (match(m, std::regex(format(\"te%ctext_model%cencoder%clayers%c(\\\\d+)%c(.+)\", seq, seq, seq, seq, seq)), key)) {\n        return format(\"cond_stage_model%ctransformer%ctext_model%cencoder%clayers%c\", seq, seq, seq, seq, seq) + m[0] + seq + m[1];\n    }\n\n    if (match(m, std::regex(format(\"te%ctext_model(.*)\", seq)), key)) {\n        return format(\"cond_stage_model%ctransformer%ctext_model\", seq, seq) + m[0];\n    }\n\n    // vae\n    if (match(m, std::regex(format(\"vae%c(.*)%cconv_norm_out(.*)\", seq, seq)), key)) {\n        return format(\"first_stage_model%c%s%cnorm_out%s\", seq, m[0].c_str(), seq, m[1].c_str());\n    }\n\n    if (match(m, std::regex(format(\"vae%c(.*)%cmid_block%c(attentions|resnets)%c(\\\\d+)%c(.+)\", seq, seq, seq, seq, seq)), key)) {\n        std::string suffix;\n        std::string block_name;\n        if (m[1] == \"attentions\") {\n            block_name = \"attn\";\n            suffix     = get_converted_suffix(m[1], m[3]);\n        } else {\n            block_name = \"block\";\n            suffix     = m[3];\n        }\n        return format(\"first_stage_model%c%s%cmid%c%s_%d%c%s\",\n                      seq, m[0].c_str(), seq, seq, block_name.c_str(), std::stoi(m[2]) + 1, seq, suffix.c_str());\n    }\n\n    if (match(m, std::regex(format(\"vae%c(.*)%cup_blocks%c(\\\\d+)%cresnets%c(\\\\d+)%c(.+)\", seq, seq, seq, seq, seq, seq)), key)) {\n        std::string suffix = m[3];\n        if (suffix == \"conv_shortcut\") {\n            suffix = \"nin_shortcut\";\n        }\n        return format(\"first_stage_model%c%s%cup%c%d%cblock%c%s%c%s\",\n                      seq, m[0].c_str(), seq, seq, 3 - std::stoi(m[1]), seq, seq, m[2].c_str(), seq, suffix.c_str());\n    }\n\n    if (match(m, std::regex(format(\"vae%c(.*)%cdown_blocks%c(\\\\d+)%cdownsamplers%c0%cconv\", seq, seq, seq, seq, seq, seq)), key)) {\n        return format(\"first_stage_model%c%s%cdown%c%d%cdownsample%cconv\",\n                      seq, m[0].c_str(), seq, seq, std::stoi(m[1]), seq, seq);\n    }\n\n    if (match(m, std::regex(format(\"vae%c(.*)%cdown_blocks%c(\\\\d+)%cresnets%c(\\\\d+)%c(.+)\", seq, seq, seq, seq, seq, seq)), key)) {\n        std::string suffix = m[3];\n        if (suffix == \"conv_shortcut\") {\n            suffix = \"nin_shortcut\";\n        }\n        return format(\"first_stage_model%c%s%cdown%c%d%cblock%c%s%c%s\",\n                      seq, m[0].c_str(), seq, seq, std::stoi(m[1]), seq, seq, m[2].c_str(), seq, suffix.c_str());\n    }\n\n    if (match(m, std::regex(format(\"vae%c(.*)%cup_blocks%c(\\\\d+)%cupsamplers%c0%cconv\", seq, seq, seq, seq, seq, seq)), key)) {\n        return format(\"first_stage_model%c%s%cup%c%d%cupsample%cconv\",\n                      seq, m[0].c_str(), seq, seq, 3 - std::stoi(m[1]), seq, seq);\n    }\n\n    if (match(m, std::regex(format(\"vae%c(.*)\", seq)), key)) {\n        return format(\"first_stage_model%c\", seq) + m[0];\n    }\n\n    return key;\n}\n\nstd::string convert_tensor_name(std::string name) {\n    if (starts_with(name, \"diffusion_model\")) {\n        name = \"model.\" + name;\n    }\n    // size_t pos = name.find(\"lora_A\");\n    // if (pos != std::string::npos) {\n    //     name.replace(pos, strlen(\"lora_A\"), \"lora_up\");\n    // }\n    // pos = name.find(\"lora_B\");\n    // if (pos != std::string::npos) {\n    //     name.replace(pos, strlen(\"lora_B\"), \"lora_down\");\n    // }\n    std::string new_name = name;\n    if (starts_with(name, \"cond_stage_model.\") || starts_with(name, \"conditioner.embedders.\") || starts_with(name, \"text_encoders.\") || ends_with(name, \".vision_model.visual_projection.weight\")) {\n        new_name = convert_open_clip_to_hf_clip(name);\n    } else if (starts_with(name, \"first_stage_model.decoder\")) {\n        new_name = convert_vae_decoder_name(name);\n    } else if (starts_with(name, \"pmid.qformer_perceiver\")) {\n        new_name = convert_pmid_v2_name(name);\n    } else if (starts_with(name, \"control_model.\")) {  // for controlnet pth models\n        size_t pos = name.find('.');\n        if (pos != std::string::npos) {\n            new_name = name.substr(pos + 1);\n        }\n    } else if (starts_with(name, \"lora_\")) {  // for lora\n        size_t pos = name.find('.');\n        if (pos != std::string::npos) {\n            std::string name_without_network_parts = name.substr(5, pos - 5);\n            std::string network_part               = name.substr(pos + 1);\n\n            // LOG_DEBUG(\"%s %s\", name_without_network_parts.c_str(), network_part.c_str());\n            std::string new_key = convert_diffusers_name_to_compvis(name_without_network_parts, '_');\n            /* For dealing with the new SDXL LoRA tensor naming convention */\n            new_key = convert_sdxl_lora_name(new_key);\n\n            if (new_key.empty()) {\n                new_name = name;\n            } else {\n                new_name = \"lora.\" + new_key + \".\" + network_part;\n            }\n        } else {\n            new_name = name;\n        }\n    } else if (contains(name, \"lora_up\") || contains(name, \"lora_down\") ||\n               contains(name, \"lora.up\") || contains(name, \"lora.down\") ||\n               contains(name, \"lora_linear\")) {\n        size_t pos = new_name.find(\".processor\");\n        if (pos != std::string::npos) {\n            new_name.replace(pos, strlen(\".processor\"), \"\");\n        }\n        // if (starts_with(new_name, \"transformer.transformer_blocks\") || starts_with(new_name, \"transformer.single_transformer_blocks\")) {\n        //     new_name = \"model.diffusion_model.\" + new_name;\n        // }\n        pos = new_name.rfind(\"lora\");\n        if (pos != std::string::npos) {\n            std::string name_without_network_parts = new_name.substr(0, pos - 1);\n            std::string network_part               = new_name.substr(pos);\n            // LOG_DEBUG(\"%s %s\", name_without_network_parts.c_str(), network_part.c_str());\n            std::string new_key = convert_diffusers_name_to_compvis(name_without_network_parts, '.');\n            new_key             = convert_sdxl_lora_name(new_key);\n            replace_all_chars(new_key, '.', '_');\n            size_t npos = network_part.rfind(\"_linear_layer\");\n            if (npos != std::string::npos) {\n                network_part.replace(npos, strlen(\"_linear_layer\"), \"\");\n            }\n            if (starts_with(network_part, \"lora.\")) {\n                network_part = \"lora_\" + network_part.substr(5);\n            }\n            if (new_key.size() > 0) {\n                new_name = \"lora.\" + new_key + \".\" + network_part;\n            }\n            // LOG_DEBUG(\"new name: %s\", new_name.c_str());\n        }\n    } else if (starts_with(name, \"unet\") || starts_with(name, \"vae\") || starts_with(name, \"te\")) {  // for diffuser\n        size_t pos = name.find_last_of('.');\n        if (pos != std::string::npos) {\n            std::string name_without_network_parts = name.substr(0, pos);\n            std::string network_part               = name.substr(pos + 1);\n            // LOG_DEBUG(\"%s %s\", name_without_network_parts.c_str(), network_part.c_str());\n            std::string new_key = convert_diffusers_name_to_compvis(name_without_network_parts, '.');\n            if (new_key.empty()) {\n                new_name = name;\n            } else {\n                new_name = new_key + \".\" + network_part;\n            }\n        } else {\n            new_name = name;\n        }\n    } else {\n        new_name = name;\n    }\n    // if (new_name != name) {\n    //     LOG_DEBUG(\"%s => %s\", name.c_str(), new_name.c_str());\n    // }\n    return new_name;\n}\n\nvoid preprocess_tensor(TensorStorage tensor_storage,\n                       std::vector<TensorStorage>& processed_tensor_storages) {\n    std::vector<TensorStorage> result;\n    std::string new_name = convert_tensor_name(tensor_storage.name);\n\n    // convert unet transformer linear to conv2d 1x1\n    if (starts_with(new_name, \"model.diffusion_model.\") &&\n        (ends_with(new_name, \"proj_in.weight\") || ends_with(new_name, \"proj_out.weight\"))) {\n        tensor_storage.unsqueeze();\n    }\n\n    // convert vae attn block linear to conv2d 1x1\n    if (starts_with(new_name, \"first_stage_model.\") && new_name.find(\"attn_1\") != std::string::npos) {\n        tensor_storage.unsqueeze();\n    }\n\n    tensor_storage.name = new_name;\n\n    if (new_name.find(\"cond_stage_model\") != std::string::npos &&\n        ends_with(new_name, \"attn.in_proj_weight\")) {\n        size_t prefix_size = new_name.find(\"attn.in_proj_weight\");\n        std::string prefix = new_name.substr(0, prefix_size);\n\n        std::vector<TensorStorage> chunks = tensor_storage.chunk(3);\n        chunks[0].name                    = prefix + \"self_attn.q_proj.weight\";\n        chunks[1].name                    = prefix + \"self_attn.k_proj.weight\";\n        chunks[2].name                    = prefix + \"self_attn.v_proj.weight\";\n\n        processed_tensor_storages.insert(processed_tensor_storages.end(), chunks.begin(), chunks.end());\n\n    } else if (new_name.find(\"cond_stage_model\") != std::string::npos &&\n               ends_with(new_name, \"attn.in_proj_bias\")) {\n        size_t prefix_size = new_name.find(\"attn.in_proj_bias\");\n        std::string prefix = new_name.substr(0, prefix_size);\n\n        std::vector<TensorStorage> chunks = tensor_storage.chunk(3);\n        chunks[0].name                    = prefix + \"self_attn.q_proj.bias\";\n        chunks[1].name                    = prefix + \"self_attn.k_proj.bias\";\n        chunks[2].name                    = prefix + \"self_attn.v_proj.bias\";\n\n        processed_tensor_storages.insert(processed_tensor_storages.end(), chunks.begin(), chunks.end());\n    } else {\n        processed_tensor_storages.push_back(tensor_storage);\n    }\n}\n\nfloat bf16_to_f32(uint16_t bfloat16) {\n    uint32_t val_bits = (static_cast<uint32_t>(bfloat16) << 16);\n    return *reinterpret_cast<float*>(&val_bits);\n}\n\nuint16_t f8_e4m3_to_f16(uint8_t f8) {\n    // do we need to support uz?\n\n    const uint32_t exponent_bias = 7;\n    if (f8 == 0xff) {\n        return ggml_fp32_to_fp16(-NAN);\n    } else if (f8 == 0x7f) {\n        return ggml_fp32_to_fp16(NAN);\n    }\n\n    uint32_t sign     = f8 & 0x80;\n    uint32_t exponent = (f8 & 0x78) >> 3;\n    uint32_t mantissa = f8 & 0x07;\n    uint32_t result   = sign << 24;\n    if (exponent == 0) {\n        if (mantissa > 0) {\n            exponent = 0x7f - exponent_bias;\n\n            // yes, 2 times\n            if ((mantissa & 0x04) == 0) {\n                mantissa &= 0x03;\n                mantissa <<= 1;\n                exponent -= 1;\n            }\n            if ((mantissa & 0x04) == 0) {\n                mantissa &= 0x03;\n                mantissa <<= 1;\n                exponent -= 1;\n            }\n\n            result |= (mantissa & 0x03) << 21;\n            result |= exponent << 23;\n        }\n    } else {\n        result |= mantissa << 20;\n        exponent += 0x7f - exponent_bias;\n        result |= exponent << 23;\n    }\n\n    return ggml_fp32_to_fp16(*reinterpret_cast<const float*>(&result));\n}\n\nuint16_t f8_e5m2_to_f16(uint8_t fp8) {\n    uint8_t sign     = (fp8 >> 7) & 0x1;\n    uint8_t exponent = (fp8 >> 2) & 0x1F;\n    uint8_t mantissa = fp8 & 0x3;\n\n    uint16_t fp16_sign = sign << 15;\n    uint16_t fp16_exponent;\n    uint16_t fp16_mantissa;\n\n    if (exponent == 0 && mantissa == 0) {  // zero\n        return fp16_sign;\n    }\n\n    if (exponent == 0x1F) {  // NAN and INF\n        fp16_exponent = 0x1F;\n        fp16_mantissa = mantissa ? (mantissa << 8) : 0;\n        return fp16_sign | (fp16_exponent << 10) | fp16_mantissa;\n    }\n\n    if (exponent == 0) {  // subnormal numbers\n        fp16_exponent = 0;\n        fp16_mantissa = (mantissa << 8);\n        return fp16_sign | fp16_mantissa;\n    }\n\n    // normal numbers\n    int16_t true_exponent = (int16_t)exponent - 15 + 15;\n    if (true_exponent <= 0) {\n        fp16_exponent = 0;\n        fp16_mantissa = (mantissa << 8);\n    } else if (true_exponent >= 0x1F) {\n        fp16_exponent = 0x1F;\n        fp16_mantissa = 0;\n    } else {\n        fp16_exponent = (uint16_t)true_exponent;\n        fp16_mantissa = mantissa << 8;\n    }\n\n    return fp16_sign | (fp16_exponent << 10) | fp16_mantissa;\n}\n\nvoid bf16_to_f32_vec(uint16_t* src, float* dst, int64_t n) {\n    // support inplace op\n    for (int64_t i = n - 1; i >= 0; i--) {\n        dst[i] = bf16_to_f32(src[i]);\n    }\n}\n\nvoid f8_e4m3_to_f16_vec(uint8_t* src, uint16_t* dst, int64_t n) {\n    // support inplace op\n    for (int64_t i = n - 1; i >= 0; i--) {\n        dst[i] = f8_e4m3_to_f16(src[i]);\n    }\n}\nvoid f8_e5m2_to_f16_vec(uint8_t* src, uint16_t* dst, int64_t n) {\n    // support inplace op\n    for (int64_t i = n - 1; i >= 0; i--) {\n        dst[i] = f8_e5m2_to_f16(src[i]);\n    }\n}\n\nvoid convert_tensor(void* src,\n                    ggml_type src_type,\n                    void* dst,\n                    ggml_type dst_type,\n                    int nrows,\n                    int n_per_row) {\n    int n = nrows * n_per_row;\n    if (src_type == dst_type) {\n        size_t nbytes = n * ggml_type_size(src_type) / ggml_blck_size(src_type);\n        memcpy(((char*)dst), ((char*)src), nbytes);\n    } else if (src_type == GGML_TYPE_F32) {\n        if (dst_type == GGML_TYPE_F16) {\n            ggml_fp32_to_fp16_row((float*)src, (ggml_fp16_t*)dst, n);\n        } else {\n            std::vector<float> imatrix(n_per_row, 1.0f);  // dummy importance matrix\n            const float* im = imatrix.data();\n            ggml_quantize_chunk(dst_type, (float*)src, dst, 0, nrows, n_per_row, im);\n        }\n    } else if (dst_type == GGML_TYPE_F32) {\n        if (src_type == GGML_TYPE_F16) {\n            ggml_fp16_to_fp32_row((ggml_fp16_t*)src, (float*)dst, n);\n        } else {\n            auto qtype = ggml_get_type_traits(src_type);\n            if (qtype->to_float == NULL) {\n                throw std::runtime_error(format(\"type %s unsupported for integer quantization: no dequantization available\",\n                                                ggml_type_name(src_type)));\n            }\n            qtype->to_float(src, (float*)dst, n);\n        }\n    } else {\n        // src_type == GGML_TYPE_F16 => dst_type is quantized\n        // src_type is quantized => dst_type == GGML_TYPE_F16 or dst_type is quantized\n        auto qtype = ggml_get_type_traits(src_type);\n        if (qtype->to_float == NULL) {\n            throw std::runtime_error(format(\"type %s unsupported for integer quantization: no dequantization available\",\n                                            ggml_type_name(src_type)));\n        }\n        std::vector<char> buf;\n        buf.resize(sizeof(float) * n);\n        char* src_data_f32 = buf.data();\n        qtype->to_float(src, (float*)src_data_f32, n);\n        if (dst_type == GGML_TYPE_F16) {\n            ggml_fp32_to_fp16_row((float*)src_data_f32, (ggml_fp16_t*)dst, n);\n        } else {\n            std::vector<float> imatrix(n_per_row, 1.0f);  // dummy importance matrix\n            const float* im = imatrix.data();\n            ggml_quantize_chunk(dst_type, (float*)src_data_f32, dst, 0, nrows, n_per_row, im);\n        }\n    }\n}\n\n/*================================================= ModelLoader ==================================================*/\n\n// ported from https://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py#L16\nstd::map<char, int> unicode_to_byte() {\n    std::map<int, char> byte_to_unicode;\n\n    // List of utf-8 byte ranges\n    for (int b = static_cast<int>('!'); b <= static_cast<int>('~'); ++b) {\n        byte_to_unicode[b] = static_cast<char>(b);\n    }\n\n    for (int b = 49825; b <= 49836; ++b) {\n        byte_to_unicode[b] = static_cast<char>(b);\n    }\n\n    for (int b = 49838; b <= 50111; ++b) {\n        byte_to_unicode[b] = static_cast<char>(b);\n    }\n    // printf(\"%d %d %d %d\\n\", static_cast<int>('¡'), static_cast<int>('¬'), static_cast<int>('®'), static_cast<int>('ÿ'));\n    // exit(1);\n\n    int n = 0;\n    for (int b = 0; b < 256; ++b) {\n        if (byte_to_unicode.find(b) == byte_to_unicode.end()) {\n            byte_to_unicode[b] = static_cast<char>(256 + n);\n            n++;\n        }\n    }\n\n    // byte_encoder = bytes_to_unicode()\n    // byte_decoder = {v: k for k, v in byte_encoder.items()}\n    std::map<char, int> byte_decoder;\n\n    for (const auto& entry : byte_to_unicode) {\n        byte_decoder[entry.second] = entry.first;\n    }\n\n    byte_to_unicode.clear();\n\n    return byte_decoder;\n}\n\nbool is_zip_file(const std::string& file_path) {\n    struct zip_t* zip = zip_open(file_path.c_str(), 0, 'r');\n    if (zip == NULL) {\n        return false;\n    }\n    zip_close(zip);\n    return true;\n}\n\nbool is_gguf_file(const std::string& file_path) {\n    std::ifstream file(file_path, std::ios::binary);\n    if (!file.is_open()) {\n        return false;\n    }\n\n    char magic[4];\n\n    file.read(magic, sizeof(magic));\n    if (!file) {\n        return false;\n    }\n    for (uint32_t i = 0; i < sizeof(magic); i++) {\n        if (magic[i] != GGUF_MAGIC[i]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n\nbool is_safetensors_file(const std::string& file_path) {\n    std::ifstream file(file_path, std::ios::binary);\n    if (!file.is_open()) {\n        return false;\n    }\n\n    // get file size\n    file.seekg(0, file.end);\n    size_t file_size_ = file.tellg();\n    file.seekg(0, file.beg);\n\n    // read header size\n    if (file_size_ <= ST_HEADER_SIZE_LEN) {\n        return false;\n    }\n\n    uint8_t header_size_buf[ST_HEADER_SIZE_LEN];\n    file.read((char*)header_size_buf, ST_HEADER_SIZE_LEN);\n    if (!file) {\n        return false;\n    }\n\n    size_t header_size_ = read_u64(header_size_buf);\n    if (header_size_ >= file_size_ || header_size_ <= 2) {\n        return false;\n    }\n\n    // read header\n    std::vector<char> header_buf;\n    header_buf.resize(header_size_ + 1);\n    header_buf[header_size_] = '\\0';\n    file.read(header_buf.data(), header_size_);\n    if (!file) {\n        return false;\n    }\n    nlohmann::json header_ = nlohmann::json::parse(header_buf.data());\n    if (header_.is_discarded()) {\n        return false;\n    }\n    return true;\n}\n\nbool ModelLoader::init_from_file(const std::string& file_path, const std::string& prefix) {\n    if (is_directory(file_path)) {\n        LOG_INFO(\"load %s using diffusers format\", file_path.c_str());\n        return init_from_diffusers_file(file_path, prefix);\n    } else if (is_gguf_file(file_path)) {\n        LOG_INFO(\"load %s using gguf format\", file_path.c_str());\n        return init_from_gguf_file(file_path, prefix);\n    } else if (is_safetensors_file(file_path)) {\n        LOG_INFO(\"load %s using safetensors format\", file_path.c_str());\n        return init_from_safetensors_file(file_path, prefix);\n    } else if (is_zip_file(file_path)) {\n        LOG_INFO(\"load %s using checkpoint format\", file_path.c_str());\n        return init_from_ckpt_file(file_path, prefix);\n    } else {\n        LOG_WARN(\"unknown format %s\", file_path.c_str());\n        return false;\n    }\n}\n\n/*================================================= GGUFModelLoader ==================================================*/\n\nbool ModelLoader::init_from_gguf_file(const std::string& file_path, const std::string& prefix) {\n    LOG_DEBUG(\"init from '%s'\", file_path.c_str());\n    file_paths_.push_back(file_path);\n    size_t file_index = file_paths_.size() - 1;\n\n    gguf_context* ctx_gguf_ = NULL;\n    ggml_context* ctx_meta_ = NULL;\n    ctx_gguf_               = gguf_init_from_file(file_path.c_str(), {true, &ctx_meta_});\n    if (!ctx_gguf_) {\n        LOG_ERROR(\"failed to open '%s'\", file_path.c_str());\n        return false;\n    }\n\n    int n_tensors = gguf_get_n_tensors(ctx_gguf_);\n\n    size_t total_size  = 0;\n    size_t data_offset = gguf_get_data_offset(ctx_gguf_);\n    for (int i = 0; i < n_tensors; i++) {\n        std::string name          = gguf_get_tensor_name(ctx_gguf_, i);\n        struct ggml_tensor* dummy = ggml_get_tensor(ctx_meta_, name.c_str());\n        size_t offset             = data_offset + gguf_get_tensor_offset(ctx_gguf_, i);\n\n        // LOG_DEBUG(\"%s\", name.c_str());\n\n        TensorStorage tensor_storage(prefix + name, dummy->type, dummy->ne, ggml_n_dims(dummy), file_index, offset);\n\n        GGML_ASSERT(ggml_nbytes(dummy) == tensor_storage.nbytes());\n\n        tensor_storages.push_back(tensor_storage);\n        tensor_storages_types[tensor_storage.name] = tensor_storage.type;\n    }\n\n    gguf_free(ctx_gguf_);\n    ggml_free(ctx_meta_);\n\n    return true;\n}\n\n/*================================================= SafeTensorsModelLoader ==================================================*/\n\nggml_type str_to_ggml_type(const std::string& dtype) {\n    ggml_type ttype = GGML_TYPE_COUNT;\n    if (dtype == \"F16\") {\n        ttype = GGML_TYPE_F16;\n    } else if (dtype == \"BF16\") {\n        ttype = GGML_TYPE_F32;\n    } else if (dtype == \"F32\") {\n        ttype = GGML_TYPE_F32;\n    } else if (dtype == \"F8_E4M3\") {\n        ttype = GGML_TYPE_F16;\n    } else if (dtype == \"F8_E5M2\") {\n        ttype = GGML_TYPE_F16;\n    }\n    return ttype;\n}\n\n// https://huggingface.co/docs/safetensors/index\nbool ModelLoader::init_from_safetensors_file(const std::string& file_path, const std::string& prefix) {\n    LOG_DEBUG(\"init from '%s'\", file_path.c_str());\n    file_paths_.push_back(file_path);\n    size_t file_index = file_paths_.size() - 1;\n    std::ifstream file(file_path, std::ios::binary);\n    if (!file.is_open()) {\n        LOG_ERROR(\"failed to open '%s'\", file_path.c_str());\n        return false;\n    }\n\n    // get file size\n    file.seekg(0, file.end);\n    size_t file_size_ = file.tellg();\n    file.seekg(0, file.beg);\n\n    // read header size\n    if (file_size_ <= ST_HEADER_SIZE_LEN) {\n        LOG_ERROR(\"invalid safetensor file '%s'\", file_path.c_str());\n        return false;\n    }\n\n    uint8_t header_size_buf[ST_HEADER_SIZE_LEN];\n    file.read((char*)header_size_buf, ST_HEADER_SIZE_LEN);\n    if (!file) {\n        LOG_ERROR(\"read safetensors header size failed: '%s'\", file_path.c_str());\n        return false;\n    }\n\n    size_t header_size_ = read_u64(header_size_buf);\n    if (header_size_ >= file_size_) {\n        LOG_ERROR(\"invalid safetensor file '%s'\", file_path.c_str());\n        return false;\n    }\n\n    // read header\n    std::vector<char> header_buf;\n    header_buf.resize(header_size_ + 1);\n    header_buf[header_size_] = '\\0';\n    file.read(header_buf.data(), header_size_);\n    if (!file) {\n        LOG_ERROR(\"read safetensors header failed: '%s'\", file_path.c_str());\n        return false;\n    }\n\n    nlohmann::json header_ = nlohmann::json::parse(header_buf.data());\n\n    for (auto& item : header_.items()) {\n        std::string name           = item.key();\n        nlohmann::json tensor_info = item.value();\n        // LOG_DEBUG(\"%s %s\\n\", name.c_str(), tensor_info.dump().c_str());\n\n        if (name == \"__metadata__\") {\n            continue;\n        }\n\n        if (is_unused_tensor(name)) {\n            continue;\n        }\n\n        std::string dtype    = tensor_info[\"dtype\"];\n        nlohmann::json shape = tensor_info[\"shape\"];\n\n        size_t begin = tensor_info[\"data_offsets\"][0].get<size_t>();\n        size_t end   = tensor_info[\"data_offsets\"][1].get<size_t>();\n\n        ggml_type type = str_to_ggml_type(dtype);\n        if (type == GGML_TYPE_COUNT) {\n            LOG_ERROR(\"unsupported dtype '%s' (tensor '%s')\", dtype.c_str(), name.c_str());\n            return false;\n        }\n\n        if (shape.size() > SD_MAX_DIMS) {\n            LOG_ERROR(\"invalid tensor '%s'\", name.c_str());\n            return false;\n        }\n\n        int n_dims              = (int)shape.size();\n        int64_t ne[SD_MAX_DIMS] = {1, 1, 1, 1, 1};\n        for (int i = 0; i < n_dims; i++) {\n            ne[i] = shape[i].get<int64_t>();\n        }\n\n        if (n_dims == 5) {\n            if (ne[3] == 1 && ne[4] == 1) {\n                n_dims = 4;\n            } else {\n                LOG_ERROR(\"invalid tensor '%s'\", name.c_str());\n                return false;\n            }\n        }\n\n        // ggml_n_dims returns 1 for scalars\n        if (n_dims == 0) {\n            n_dims = 1;\n        }\n\n        TensorStorage tensor_storage(prefix + name, type, ne, n_dims, file_index, ST_HEADER_SIZE_LEN + header_size_ + begin);\n        tensor_storage.reverse_ne();\n\n        size_t tensor_data_size = end - begin;\n\n        if (dtype == \"BF16\") {\n            tensor_storage.is_bf16 = true;\n            GGML_ASSERT(tensor_storage.nbytes() == tensor_data_size * 2);\n        } else if (dtype == \"F8_E4M3\") {\n            tensor_storage.is_f8_e4m3 = true;\n            // f8 -> f16\n            GGML_ASSERT(tensor_storage.nbytes() == tensor_data_size * 2);\n        } else if (dtype == \"F8_E5M2\") {\n            tensor_storage.is_f8_e5m2 = true;\n            // f8 -> f16\n            GGML_ASSERT(tensor_storage.nbytes() == tensor_data_size * 2);\n        } else {\n            GGML_ASSERT(tensor_storage.nbytes() == tensor_data_size);\n        }\n\n        tensor_storages.push_back(tensor_storage);\n        tensor_storages_types[tensor_storage.name] = tensor_storage.type;\n\n        // LOG_DEBUG(\"%s %s\", tensor_storage.to_string().c_str(), dtype.c_str());\n    }\n\n    return true;\n}\n\n/*================================================= DiffusersModelLoader ==================================================*/\n\nbool ModelLoader::init_from_diffusers_file(const std::string& file_path, const std::string& prefix) {\n    std::string unet_path = path_join(file_path, \"unet/diffusion_pytorch_model.safetensors\");\n    std::string vae_path  = path_join(file_path, \"vae/diffusion_pytorch_model.safetensors\");\n    std::string clip_path = path_join(file_path, \"text_encoder/model.safetensors\");\n\n    if (!init_from_safetensors_file(unet_path, \"unet.\")) {\n        return false;\n    }\n    if (!init_from_safetensors_file(vae_path, \"vae.\")) {\n        return false;\n    }\n    if (!init_from_safetensors_file(clip_path, \"te.\")) {\n        return false;\n    }\n    return true;\n}\n\n/*================================================= CkptModelLoader ==================================================*/\n\n// $ python -m pickletools sd-v1-4/archive/data.pkl | head -n 100\n//     0: \\x80 PROTO      2\n//     2: }    EMPTY_DICT\n//     3: q    BINPUT     0\n//     5: (    MARK\n//     6: X        BINUNICODE 'epoch'\n//    16: q        BINPUT     1\n//    18: K        BININT1    6\n//    20: X        BINUNICODE 'global_step'\n//    36: q        BINPUT     2\n//    38: J        BININT     470000\n//    43: X        BINUNICODE 'pytorch-lightning_version'\n//    73: q        BINPUT     3\n//    75: X        BINUNICODE '1.4.2'\n//    85: q        BINPUT     4\n//    87: X        BINUNICODE 'state_dict'\n//   102: q        BINPUT     5\n//   104: }        EMPTY_DICT\n//   105: q        BINPUT     6\n//   107: (        MARK\n//   108: X            BINUNICODE 'betas'\n//   118: q            BINPUT     7\n//   120: c            GLOBAL     'torch._utils _rebuild_tensor_v2'\n//   153: q            BINPUT     8\n//   155: (            MARK\n//   156: (                MARK\n//   157: X                    BINUNICODE 'storage'\n//   169: q                    BINPUT     9\n//   171: c                    GLOBAL     'torch FloatStorage'\n//   191: q                    BINPUT     10\n//   193: X                    BINUNICODE '0'\n//   199: q                    BINPUT     11\n//   201: X                    BINUNICODE 'cpu'\n//   209: q                    BINPUT     12\n//   211: M                    BININT2    1000\n//   214: t                    TUPLE      (MARK at 156)\n//   215: q                BINPUT     13\n//   217: Q                BINPERSID\n//   218: K                BININT1    0\n//   220: M                BININT2    1000\n//  ...............................\n//  3201: q            BINPUT     250\n//  3203: R            REDUCE\n//  3204: q            BINPUT     251\n//  3206: X            BINUNICODE 'model.diffusion_model.input_blocks.1.1.proj_in.weight'\n//  3264: q            BINPUT     252\n//  3266: h            BINGET     8\n//  3268: (            MARK\n//  3269: (                MARK\n//  3270: h                    BINGET     9\n//  3272: h                    BINGET     10\n//  3274: X                    BINUNICODE '30'\n//  3281: q                    BINPUT     253\n//  3283: h                    BINGET     12\n//  3285: J                    BININT     102400\n//  3290: t                    TUPLE      (MARK at 3269)\n//  3291: q                BINPUT     254\n//  3293: Q                BINPERSID\n//  3294: K                BININT1    0\n//  3296: (                MARK\n//  3297: M                    BININT2    320\n//  3300: M                    BININT2    320\n//  3303: K                    BININT1    1\n//  3305: K                    BININT1    1\n//  3307: t                    TUPLE      (MARK at 3296)\n//  3308: q                BINPUT     255\n//  3310: (                MARK\n//  3311: M                    BININT2    320\n//  3314: K                    BININT1    1\n//  3316: K                    BININT1    1\n//  3318: K                    BININT1    1\n//  3320: t                    TUPLE      (MARK at 3310)\n//  3321: r                LONG_BINPUT 256\n//  3326: \\x89             NEWFALSE\n//  3327: h                BINGET     16\n//  3329: )                EMPTY_TUPLE\n//  3330: R                REDUCE\n//  3331: r                LONG_BINPUT 257\n//  3336: t                TUPLE      (MARK at 3268)\n//  3337: r            LONG_BINPUT 258\n//  3342: R            REDUCE\n//  3343: r            LONG_BINPUT 259\n//  3348: X            BINUNICODE 'model.diffusion_model.input_blocks.1.1.proj_in.bias'\n//  3404: r            LONG_BINPUT 260\n//  3409: h            BINGET     8\n//  3411: (            MARK\n//  3412: (                MARK\n//  3413: h                    BINGET     9\n//  3415: h                    BINGET     10\n//  3417: X                    BINUNICODE '31'\n\nstruct PickleTensorReader {\n    enum ReadPhase {\n        READ_NAME,\n        READ_DATA,\n        CHECK_SIZE,\n        READ_DIMENS\n    };\n    ReadPhase phase   = READ_NAME;\n    size_t entry_size = 0;\n    int32_t nelements = 0;\n\n    TensorStorage tensor_storage;\n\n    static ggml_type global_type;  // all pickle_tensors data type\n    static bool read_global_type;\n\n    bool read_int_value(uint32_t value) {\n        if (phase == CHECK_SIZE) {\n            if (entry_size == value * ggml_type_size(tensor_storage.type)) {\n                nelements = value;\n                phase     = READ_DIMENS;\n                return true;\n            } else {\n                phase = READ_NAME;\n            }\n        } else if (phase == READ_DIMENS) {\n            if (tensor_storage.n_dims + 1 > SD_MAX_DIMS) {  // too many dimens\n                phase                 = READ_NAME;\n                tensor_storage.n_dims = 0;\n            }\n            if (nelements % value == 0) {\n                tensor_storage.ne[tensor_storage.n_dims] = value;\n                tensor_storage.n_dims++;\n            }\n        }\n        return false;\n    }\n\n    void read_global(const std::string& str) {\n        if (str == \"FloatStorage\") {\n            if (read_global_type) {\n                global_type      = GGML_TYPE_F32;\n                read_global_type = false;\n            }\n            tensor_storage.type = GGML_TYPE_F32;\n        } else if (str == \"HalfStorage\") {\n            if (read_global_type) {\n                global_type      = GGML_TYPE_F16;\n                read_global_type = false;\n            }\n            tensor_storage.type = GGML_TYPE_F16;\n        }\n    }\n\n    void read_string(const std::string& str, struct zip_t* zip, std::string dir) {\n        if (str == \"storage\") {\n            read_global_type = true;\n        } else if (str != \"state_dict\") {\n            if (phase == READ_DATA) {\n                std::string entry_name = dir + \"data/\" + std::string(str);\n\n                size_t i, n = zip_entries_total(zip);\n                for (i = 0; i < n; ++i) {\n                    zip_entry_openbyindex(zip, i);\n                    {\n                        std::string name = zip_entry_name(zip);\n                        if (name == entry_name) {\n                            tensor_storage.index_in_zip = (int)i;\n                            entry_size                  = zip_entry_size(zip);\n                            zip_entry_close(zip);\n                            break;\n                        }\n                    }\n                    zip_entry_close(zip);\n                }\n\n                phase = entry_size > 0 ? CHECK_SIZE : READ_NAME;\n            }\n            if (!read_global_type && phase == READ_NAME) {\n                tensor_storage.name = str;\n                phase               = READ_DATA;\n                tensor_storage.type = global_type;\n            }\n        }\n    }\n};\n\nggml_type PickleTensorReader::global_type = GGML_TYPE_F32;  // all pickle_tensors data type\nbool PickleTensorReader::read_global_type = false;\n\nint find_char(uint8_t* buffer, int len, char c) {\n    for (int pos = 0; pos < len; pos++) {\n        if (buffer[pos] == c) {\n            return pos;\n        }\n    }\n    return -1;\n}\n\n#define MAX_STRING_BUFFER 512\n\nbool ModelLoader::parse_data_pkl(uint8_t* buffer,\n                                 size_t buffer_size,\n                                 zip_t* zip,\n                                 std::string dir,\n                                 size_t file_index,\n                                 const std::string prefix) {\n    uint8_t* buffer_end = buffer + buffer_size;\n    if (buffer[0] == 0x80) {  // proto\n        if (buffer[1] != 2) {\n            LOG_ERROR(\"Unsupported protocol\\n\");\n            return false;\n        }\n        buffer += 2;  // 0x80 and version\n        char string_buffer[MAX_STRING_BUFFER];\n        bool finish = false;\n        PickleTensorReader reader;\n        // read pickle binary file\n        while (!finish && buffer < buffer_end) {\n            uint8_t opcode = *buffer;\n            buffer++;\n            // https://github.com/python/cpython/blob/3.7/Lib/pickletools.py#L1048\n            // https://github.com/python/cpython/blob/main/Lib/pickle.py#L105\n            switch (opcode) {\n                case '}':  // EMPTY_DICT     = b'}'   # push empty dict\n                    break;\n                case ']':  // EMPTY_LIST     = b']'   # push empty list\n                    break;\n                // skip unused sections\n                case 'h':  // BINGET         = b'h'   #   \"    \"    \"    \"   \"   \"  ;   \"    \" 1-byte arg\n                case 'q':  // BINPUT         = b'q'   #   \"     \"    \"   \"   \" ;   \"    \" 1-byte arg\n                case 'Q':  // BINPERSID      = b'Q'   #  \"       \"         \"  ;  \"  \"   \"     \"  stack\n                    buffer++;\n                    break;\n                case 'r':  // LONG_BINPUT    = b'r'   #   \"     \"    \"   \"   \" ;   \"    \" 4-byte arg\n                    buffer += 4;\n                    break;\n                case 0x95:  // FRAME            = b'\\x95'  # indicate the beginning of a new frame\n                    buffer += 8;\n                    break;\n                case 0x94:  // MEMOIZE          = b'\\x94'  # store top of the stack in memo\n                    break;\n                case '(':  // MARK           = b'('   # push special markobject on stack\n                    break;\n                case 'K':  // BININT1        = b'K'   # push 1-byte unsigned int\n                {\n                    uint8_t value = *buffer;\n                    if (reader.read_int_value(value)) {\n                        buffer++;\n                    }\n                    buffer++;\n                } break;\n                case 'M':  // BININT2        = b'M'   # push 2-byte unsigned int\n                {\n                    uint16_t value = read_short(buffer);\n                    if (reader.read_int_value(value)) {\n                        buffer++;\n                    }\n                    buffer += 2;\n                } break;\n                case 'J':  // BININT         = b'J'   # push four-byte signed int\n                {\n                    const int32_t value = read_int(buffer);\n                    if (reader.read_int_value(value)) {\n                        buffer++;  // skip tuple after read num_elements\n                    }\n                    buffer += 4;\n                } break;\n                case 'X':  // BINUNICODE     = b'X'   #   \"     \"       \"  ; counted UTF-8 string argument\n                {\n                    const int32_t len = read_int(buffer);\n                    buffer += 4;\n                    memset(string_buffer, 0, MAX_STRING_BUFFER);\n                    if (len > MAX_STRING_BUFFER) {\n                        LOG_WARN(\"tensor name very large\");\n                    }\n                    memcpy(string_buffer, buffer, len < MAX_STRING_BUFFER ? len : (MAX_STRING_BUFFER - 1));\n                    buffer += len;\n                    reader.read_string(string_buffer, zip, dir);\n                } break;\n                case 0x8C:  // SHORT_BINUNICODE = b'\\x8c'  # push short string; UTF-8 length < 256 bytes\n                {\n                    const int8_t len = *buffer;\n                    buffer++;\n                    memset(string_buffer, 0, MAX_STRING_BUFFER);\n                    memcpy(string_buffer, buffer, len);\n                    buffer += len;\n                    // printf(\"String: '%s'\\n\", string_buffer);\n                } break;\n                case 'c':  // GLOBAL         = b'c'   # push self.find_class(modname, name); 2 string args\n                {\n                    int len = find_char(buffer, MAX_STRING_BUFFER, '\\n');\n\n                    buffer += len + 1;\n                    len = find_char(buffer, MAX_STRING_BUFFER, '\\n');\n\n                    memset(string_buffer, 0, MAX_STRING_BUFFER);\n                    memcpy(string_buffer, buffer, len);\n                    buffer += len + 1;\n                    reader.read_global(string_buffer);\n                } break;\n                case 0x86:  // TUPLE2         = b'\\x86'  # build 2-tuple from two topmost stack items\n                case 0x85:  // TUPLE1         = b'\\x85'  # build 1-tuple from stack top\n                case 't':   // TUPLE          = b't'   # build tuple from topmost stack items\n                    if (reader.phase == PickleTensorReader::READ_DIMENS) {\n                        reader.tensor_storage.reverse_ne();\n                        reader.tensor_storage.file_index = file_index;\n                        // if(strcmp(prefix.c_str(), \"scarlett\") == 0)\n                        // printf(\" ZIP got tensor %s \\n \", reader.tensor_storage.name.c_str());\n                        reader.tensor_storage.name = prefix + reader.tensor_storage.name;\n                        tensor_storages.push_back(reader.tensor_storage);\n                        tensor_storages_types[reader.tensor_storage.name] = reader.tensor_storage.type;\n\n                        // LOG_DEBUG(\"%s\", reader.tensor_storage.name.c_str());\n                        // reset\n                        reader = PickleTensorReader();\n                    }\n                    break;\n                case '.':  // STOP           = b'.'   # every pickle ends with STOP\n                    finish = true;\n                    break;\n                default:\n                    break;\n            }\n        }\n    }\n    return true;\n}\n\nbool ModelLoader::init_from_ckpt_file(const std::string& file_path, const std::string& prefix) {\n    LOG_DEBUG(\"init from '%s'\", file_path.c_str());\n    file_paths_.push_back(file_path);\n    size_t file_index = file_paths_.size() - 1;\n\n    struct zip_t* zip = zip_open(file_path.c_str(), 0, 'r');\n    if (zip == NULL) {\n        LOG_ERROR(\"failed to open '%s'\", file_path.c_str());\n        return false;\n    }\n    int n = (int)zip_entries_total(zip);\n    for (int i = 0; i < n; ++i) {\n        zip_entry_openbyindex(zip, i);\n        {\n            std::string name = zip_entry_name(zip);\n            size_t pos       = name.find(\"data.pkl\");\n            if (pos != std::string::npos) {\n                std::string dir = name.substr(0, pos);\n                printf(\"ZIP %d, name = %s, dir = %s \\n\", i, name.c_str(), dir.c_str());\n                void* pkl_data = NULL;\n                size_t pkl_size;\n                zip_entry_read(zip, &pkl_data, &pkl_size);\n\n                // LOG_DEBUG(\"%lld\", pkl_size);\n\n                parse_data_pkl((uint8_t*)pkl_data, pkl_size, zip, dir, file_index, prefix);\n\n                free(pkl_data);\n            }\n        }\n        zip_entry_close(zip);\n    }\n    zip_close(zip);\n    return true;\n}\n\nSDVersion ModelLoader::get_sd_version() {\n    TensorStorage token_embedding_weight, input_block_weight;\n    bool input_block_checked = false;\n\n    bool has_multiple_encoders   = false;\n    bool is_unet = false;\n\n    bool is_xl = false;\n    bool is_flux = false;\n\n#define found_family (is_xl || is_flux)\n    for (auto& tensor_storage : tensor_storages) {\n        if (!found_family) {\n            if (tensor_storage.name.find(\"model.diffusion_model.double_blocks.\") != std::string::npos) {\n                is_flux = true;\n                if (input_block_checked) {\n                    break;\n                }\n            }\n            if (tensor_storage.name.find(\"model.diffusion_model.joint_blocks.\") != std::string::npos) {\n                return VERSION_SD3;\n            }\n            if (tensor_storage.name.find(\"model.diffusion_model.input_blocks.\") != std::string::npos) {\n                is_unet = true;\n                if(has_multiple_encoders){\n                    is_xl = true;\n                    if (input_block_checked) {\n                        break;\n                    }\n                }\n            }\n            if (tensor_storage.name.find(\"conditioner.embedders.1\") != std::string::npos || tensor_storage.name.find(\"cond_stage_model.1\") != std::string::npos) {\n                has_multiple_encoders = true;\n                if(is_unet){\n                    is_xl = true;\n                    if (input_block_checked) {\n                        break;\n                    }\n                }\n            }\n            if (tensor_storage.name.find(\"model.diffusion_model.input_blocks.8.0.time_mixer.mix_factor\") != std::string::npos) {\n                return VERSION_SVD;\n            }\n        }\n        if (tensor_storage.name == \"cond_stage_model.transformer.text_model.embeddings.token_embedding.weight\" ||\n            tensor_storage.name == \"cond_stage_model.model.token_embedding.weight\" ||\n            tensor_storage.name == \"text_model.embeddings.token_embedding.weight\" ||\n            tensor_storage.name == \"te.text_model.embeddings.token_embedding.weight\" ||\n            tensor_storage.name == \"conditioner.embedders.0.model.token_embedding.weight\" ||\n            tensor_storage.name == \"conditioner.embedders.0.transformer.text_model.embeddings.token_embedding.weight\") {\n            token_embedding_weight = tensor_storage;\n            // break;\n        }\n        if (tensor_storage.name == \"model.diffusion_model.input_blocks.0.0.weight\" || tensor_storage.name == \"model.diffusion_model.img_in.weight\") {\n            input_block_weight  = tensor_storage;\n            input_block_checked = true;\n            if (found_family) {\n                break;\n            }\n        }\n    }\n    bool is_inpaint = input_block_weight.ne[2] == 9;\n    if (is_xl) {\n        if (is_inpaint) {\n            return VERSION_SDXL_INPAINT;\n        }\n        return VERSION_SDXL;\n    }\n\n    if (is_flux) {\n        is_inpaint = input_block_weight.ne[0] == 384;\n        if (is_inpaint) {\n            return VERSION_FLUX_FILL;\n        }\n        return VERSION_FLUX;\n    }\n\n    if (token_embedding_weight.ne[0] == 768) {\n        if (is_inpaint) {\n            return VERSION_SD1_INPAINT;\n        }\n        return VERSION_SD1;\n    } else if (token_embedding_weight.ne[0] == 1024) {\n        if (is_inpaint) {\n            return VERSION_SD2_INPAINT;\n        }\n        return VERSION_SD2;\n    }\n    return VERSION_COUNT;\n}\n\nggml_type ModelLoader::get_sd_wtype() {\n    for (auto& tensor_storage : tensor_storages) {\n        if (is_unused_tensor(tensor_storage.name)) {\n            continue;\n        }\n\n        if (ggml_is_quantized(tensor_storage.type)) {\n            return tensor_storage.type;\n        }\n\n        if (tensor_should_be_converted(tensor_storage, GGML_TYPE_Q4_K)) {\n            return tensor_storage.type;\n        }\n    }\n    return GGML_TYPE_COUNT;\n}\n\nggml_type ModelLoader::get_conditioner_wtype() {\n    for (auto& tensor_storage : tensor_storages) {\n        if (is_unused_tensor(tensor_storage.name)) {\n            continue;\n        }\n\n        if ((tensor_storage.name.find(\"text_encoders\") == std::string::npos &&\n             tensor_storage.name.find(\"cond_stage_model\") == std::string::npos &&\n             tensor_storage.name.find(\"te.text_model.\") == std::string::npos &&\n             tensor_storage.name.find(\"conditioner\") == std::string::npos)) {\n            continue;\n        }\n\n        if (ggml_is_quantized(tensor_storage.type)) {\n            return tensor_storage.type;\n        }\n\n        if (tensor_should_be_converted(tensor_storage, GGML_TYPE_Q4_K)) {\n            return tensor_storage.type;\n        }\n    }\n    return GGML_TYPE_COUNT;\n}\n\nggml_type ModelLoader::get_diffusion_model_wtype() {\n    for (auto& tensor_storage : tensor_storages) {\n        if (is_unused_tensor(tensor_storage.name)) {\n            continue;\n        }\n\n        if (tensor_storage.name.find(\"model.diffusion_model.\") == std::string::npos) {\n            continue;\n        }\n\n        if (ggml_is_quantized(tensor_storage.type)) {\n            return tensor_storage.type;\n        }\n\n        if (tensor_should_be_converted(tensor_storage, GGML_TYPE_Q4_K)) {\n            return tensor_storage.type;\n        }\n    }\n    return GGML_TYPE_COUNT;\n}\n\nggml_type ModelLoader::get_vae_wtype() {\n    for (auto& tensor_storage : tensor_storages) {\n        if (is_unused_tensor(tensor_storage.name)) {\n            continue;\n        }\n\n        if (tensor_storage.name.find(\"vae.\") == std::string::npos &&\n            tensor_storage.name.find(\"first_stage_model\") == std::string::npos) {\n            continue;\n        }\n\n        if (ggml_is_quantized(tensor_storage.type)) {\n            return tensor_storage.type;\n        }\n\n        if (tensor_should_be_converted(tensor_storage, GGML_TYPE_Q4_K)) {\n            return tensor_storage.type;\n        }\n    }\n    return GGML_TYPE_COUNT;\n}\n\nvoid ModelLoader::set_wtype_override(ggml_type wtype, std::string prefix) {\n    for (auto& pair : tensor_storages_types) {\n        if (prefix.size() < 1 || pair.first.substr(0, prefix.size()) == prefix) {\n            for (auto& tensor_storage : tensor_storages) {\n                if (tensor_storage.name == pair.first) {\n                    if (tensor_should_be_converted(tensor_storage, wtype)) {\n                        pair.second = wtype;\n                    }\n                    break;\n                }\n            }\n        }\n    }\n}\n\nstd::string ModelLoader::load_merges() {\n    std::string merges_utf8_str(reinterpret_cast<const char*>(merges_utf8_c_str), sizeof(merges_utf8_c_str));\n    return merges_utf8_str;\n}\n\nstd::string ModelLoader::load_t5_tokenizer_json() {\n    std::string json_str(reinterpret_cast<const char*>(t5_tokenizer_json_str), sizeof(t5_tokenizer_json_str));\n    return json_str;\n}\n\nstd::vector<TensorStorage> remove_duplicates(const std::vector<TensorStorage>& vec) {\n    std::vector<TensorStorage> res;\n    std::unordered_map<std::string, size_t> name_to_index_map;\n\n    for (size_t i = 0; i < vec.size(); ++i) {\n        const std::string& current_name = vec[i].name;\n        auto it                         = name_to_index_map.find(current_name);\n\n        if (it != name_to_index_map.end()) {\n            res[it->second] = vec[i];\n        } else {\n            name_to_index_map[current_name] = i;\n            res.push_back(vec[i]);\n        }\n    }\n\n    // vec.resize(name_to_index_map.size());\n\n    return res;\n}\n\nbool ModelLoader::load_tensors(on_new_tensor_cb_t on_new_tensor_cb, ggml_backend_t backend) {\n    std::vector<TensorStorage> processed_tensor_storages;\n    for (auto& tensor_storage : tensor_storages) {\n        // LOG_DEBUG(\"%s\", name.c_str());\n\n        if (is_unused_tensor(tensor_storage.name)) {\n            continue;\n        }\n\n        preprocess_tensor(tensor_storage, processed_tensor_storages);\n    }\n    std::vector<TensorStorage> dedup = remove_duplicates(processed_tensor_storages);\n    processed_tensor_storages        = dedup;\n\n    bool success = true;\n    for (size_t file_index = 0; file_index < file_paths_.size(); file_index++) {\n        std::string file_path = file_paths_[file_index];\n        LOG_DEBUG(\"loading tensors from %s\", file_path.c_str());\n\n        std::ifstream file(file_path, std::ios::binary);\n        if (!file.is_open()) {\n            LOG_ERROR(\"failed to open '%s'\", file_path.c_str());\n            return false;\n        }\n\n        bool is_zip = false;\n        for (auto& tensor_storage : tensor_storages) {\n            if (tensor_storage.file_index != file_index) {\n                continue;\n            }\n            if (tensor_storage.index_in_zip >= 0) {\n                is_zip = true;\n                break;\n            }\n        }\n\n        struct zip_t* zip = NULL;\n        if (is_zip) {\n            zip = zip_open(file_path.c_str(), 0, 'r');\n            if (zip == NULL) {\n                LOG_ERROR(\"failed to open zip '%s'\", file_path.c_str());\n                return false;\n            }\n        }\n\n        std::vector<uint8_t> read_buffer;\n        std::vector<uint8_t> convert_buffer;\n\n        auto read_data = [&](const TensorStorage& tensor_storage, char* buf, size_t n) {\n            if (zip != NULL) {\n                zip_entry_openbyindex(zip, tensor_storage.index_in_zip);\n                size_t entry_size = zip_entry_size(zip);\n                if (entry_size != n) {\n                    read_buffer.resize(entry_size);\n                    zip_entry_noallocread(zip, (void*)read_buffer.data(), entry_size);\n                    memcpy((void*)buf, (void*)(read_buffer.data() + tensor_storage.offset), n);\n                } else {\n                    zip_entry_noallocread(zip, (void*)buf, n);\n                }\n                zip_entry_close(zip);\n            } else {\n                file.seekg(tensor_storage.offset);\n                file.read(buf, n);\n                if (!file) {\n                    LOG_ERROR(\"read tensor data failed: '%s'\", file_path.c_str());\n                    return false;\n                }\n            }\n            return true;\n        };\n        int tensor_count = 0;\n        int64_t t1       = ggml_time_ms();\n        for (auto& tensor_storage : processed_tensor_storages) {\n            if (tensor_storage.file_index != file_index) {\n                ++tensor_count;\n                continue;\n            }\n            ggml_tensor* dst_tensor = NULL;\n\n            success = on_new_tensor_cb(tensor_storage, &dst_tensor);\n            if (!success) {\n                LOG_WARN(\"process tensor failed: '%s'\", tensor_storage.name.c_str());\n                break;\n            }\n\n            if (dst_tensor == NULL) {\n                ++tensor_count;\n                continue;\n            }\n\n            size_t nbytes_to_read = tensor_storage.nbytes_to_read();\n\n            if (dst_tensor->buffer == NULL || ggml_backend_buffer_is_host(dst_tensor->buffer)) {\n                // for the CPU and Metal backend, we can copy directly into the tensor\n                if (tensor_storage.type == dst_tensor->type) {\n                    GGML_ASSERT(ggml_nbytes(dst_tensor) == tensor_storage.nbytes());\n                    read_data(tensor_storage, (char*)dst_tensor->data, nbytes_to_read);\n\n                    if (tensor_storage.is_bf16) {\n                        // inplace op\n                        bf16_to_f32_vec((uint16_t*)dst_tensor->data, (float*)dst_tensor->data, tensor_storage.nelements());\n                    } else if (tensor_storage.is_f8_e4m3) {\n                        // inplace op\n                        f8_e4m3_to_f16_vec((uint8_t*)dst_tensor->data, (uint16_t*)dst_tensor->data, tensor_storage.nelements());\n                    } else if (tensor_storage.is_f8_e5m2) {\n                        // inplace op\n                        f8_e5m2_to_f16_vec((uint8_t*)dst_tensor->data, (uint16_t*)dst_tensor->data, tensor_storage.nelements());\n                    }\n                } else {\n                    read_buffer.resize(tensor_storage.nbytes());\n                    read_data(tensor_storage, (char*)read_buffer.data(), nbytes_to_read);\n\n                    if (tensor_storage.is_bf16) {\n                        // inplace op\n                        bf16_to_f32_vec((uint16_t*)read_buffer.data(), (float*)read_buffer.data(), tensor_storage.nelements());\n                    } else if (tensor_storage.is_f8_e4m3) {\n                        // inplace op\n                        f8_e4m3_to_f16_vec((uint8_t*)read_buffer.data(), (uint16_t*)read_buffer.data(), tensor_storage.nelements());\n                    } else if (tensor_storage.is_f8_e5m2) {\n                        // inplace op\n                        f8_e5m2_to_f16_vec((uint8_t*)read_buffer.data(), (uint16_t*)read_buffer.data(), tensor_storage.nelements());\n                    }\n\n                    convert_tensor((void*)read_buffer.data(), tensor_storage.type, dst_tensor->data,\n                                   dst_tensor->type, (int)tensor_storage.nelements() / (int)tensor_storage.ne[0], (int)tensor_storage.ne[0]);\n                }\n            } else {\n                read_buffer.resize(tensor_storage.nbytes());\n                read_data(tensor_storage, (char*)read_buffer.data(), nbytes_to_read);\n\n                if (tensor_storage.is_bf16) {\n                    // inplace op\n                    bf16_to_f32_vec((uint16_t*)read_buffer.data(), (float*)read_buffer.data(), tensor_storage.nelements());\n                } else if (tensor_storage.is_f8_e4m3) {\n                    // inplace op\n                    f8_e4m3_to_f16_vec((uint8_t*)read_buffer.data(), (uint16_t*)read_buffer.data(), tensor_storage.nelements());\n                } else if (tensor_storage.is_f8_e5m2) {\n                    // inplace op\n                    f8_e5m2_to_f16_vec((uint8_t*)read_buffer.data(), (uint16_t*)read_buffer.data(), tensor_storage.nelements());\n                }\n\n                if (tensor_storage.type == dst_tensor->type) {\n                    // copy to device memory\n                    ggml_backend_tensor_set(dst_tensor, read_buffer.data(), 0, ggml_nbytes(dst_tensor));\n                } else {\n                    // convert first, then copy to device memory\n                    convert_buffer.resize(ggml_nbytes(dst_tensor));\n                    convert_tensor((void*)read_buffer.data(), tensor_storage.type,\n                                   (void*)convert_buffer.data(), dst_tensor->type,\n                                   (int)tensor_storage.nelements() / (int)tensor_storage.ne[0], (int)tensor_storage.ne[0]);\n                    ggml_backend_tensor_set(dst_tensor, convert_buffer.data(), 0, ggml_nbytes(dst_tensor));\n                }\n            }\n            int64_t t2 = ggml_time_ms();\n            pretty_progress(++tensor_count, processed_tensor_storages.size(), (t2 - t1) / 1000.0f);\n            t1 = t2;\n        }\n\n        if (zip != NULL) {\n            zip_close(zip);\n        }\n\n        if (!success) {\n            break;\n        }\n    }\n    return success;\n}\n\nbool ModelLoader::load_tensors(std::map<std::string, struct ggml_tensor*>& tensors,\n                               ggml_backend_t backend,\n                               std::set<std::string> ignore_tensors) {\n    std::set<std::string> tensor_names_in_file;\n    auto on_new_tensor_cb = [&](const TensorStorage& tensor_storage, ggml_tensor** dst_tensor) -> bool {\n        const std::string& name = tensor_storage.name;\n        // LOG_DEBUG(\"%s\", tensor_storage.to_string().c_str());\n        tensor_names_in_file.insert(name);\n\n        struct ggml_tensor* real;\n        if (tensors.find(name) != tensors.end()) {\n            real = tensors[name];\n        } else {\n            for (auto& ignore_tensor : ignore_tensors) {\n                if (starts_with(name, ignore_tensor)) {\n                    return true;\n                }\n            }\n            LOG_INFO(\"unknown tensor '%s' in model file\", tensor_storage.to_string().c_str());\n            return true;\n        }\n\n        if (\n            real->ne[0] != tensor_storage.ne[0] ||\n            real->ne[1] != tensor_storage.ne[1] ||\n            real->ne[2] != tensor_storage.ne[2] ||\n            real->ne[3] != tensor_storage.ne[3]) {\n            LOG_ERROR(\n                \"tensor '%s' has wrong shape in model file: \"\n                \"got [%d, %d, %d, %d], expected [%d, %d, %d, %d]\",\n                name.c_str(),\n                (int)tensor_storage.ne[0], (int)tensor_storage.ne[1], (int)tensor_storage.ne[2], (int)tensor_storage.ne[3],\n                (int)real->ne[0], (int)real->ne[1], (int)real->ne[2], (int)real->ne[3]);\n            return false;\n        }\n\n        *dst_tensor = real;\n\n        return true;\n    };\n\n    bool success = load_tensors(on_new_tensor_cb, backend);\n    if (!success) {\n        LOG_ERROR(\"load tensors from file failed\");\n        return false;\n    }\n\n    bool some_tensor_not_init = false;\n\n    for (auto pair : tensors) {\n        if (pair.first.find(\"cond_stage_model.transformer.text_model.encoder.layers.23\") != std::string::npos) {\n            continue;\n        }\n        if (pair.first.find(\"alphas_cumprod\") != std::string::npos) {\n            continue;\n        }\n\n        if (pair.first.find(\"alphas_cumprod\") != std::string::npos) {\n            continue;\n        }\n\n        if (tensor_names_in_file.find(pair.first) == tensor_names_in_file.end()) {\n            LOG_ERROR(\"tensor '%s' not in model file\", pair.first.c_str());\n            some_tensor_not_init = true;\n        }\n    }\n\n    if (some_tensor_not_init) {\n        return false;\n    }\n    return true;\n}\n\nbool ModelLoader::tensor_should_be_converted(const TensorStorage& tensor_storage, ggml_type type) {\n    const std::string& name = tensor_storage.name;\n    if (type != GGML_TYPE_COUNT) {\n        if (ggml_is_quantized(type) && tensor_storage.ne[0] % ggml_blck_size(type) != 0) {\n            // Pass, do not convert\n        } else if (ends_with(name, \".bias\")) {\n            // Pass, do not convert\n        } else if (ends_with(name, \".scale\")) {\n            // Pass, do not convert\n        } else if (contains(name, \"img_in.\") ||\n                   contains(name, \"txt_in.\") ||\n                   contains(name, \"time_in.\") ||\n                   contains(name, \"vector_in.\") ||\n                   contains(name, \"guidance_in.\") ||\n                   contains(name, \"final_layer.\")) {\n            // Pass, do not convert. For FLUX\n        } else if (contains(name, \"x_embedder.\") ||\n                   contains(name, \"t_embedder.\") ||\n                   contains(name, \"y_embedder.\") ||\n                   contains(name, \"pos_embed\") ||\n                   contains(name, \"context_embedder.\")) {\n            // Pass, do not convert. For MMDiT\n        } else if (contains(name, \"time_embed.\") || contains(name, \"label_emb.\")) {\n            // Pass, do not convert. For Unet\n        } else {\n            return true;\n        }\n    }\n    return false;\n}\n\nbool ModelLoader::save_to_gguf_file(const std::string& file_path, ggml_type type) {\n    auto backend    = ggml_backend_cpu_init();\n    size_t mem_size = 1 * 1024 * 1024;  // for padding\n    mem_size += tensor_storages.size() * ggml_tensor_overhead();\n    mem_size += get_params_mem_size(backend, type);\n    LOG_INFO(\"model tensors mem size: %.2fMB\", mem_size / 1024.f / 1024.f);\n    ggml_context* ggml_ctx = ggml_init({mem_size, NULL, false});\n\n    gguf_context* gguf_ctx = gguf_init_empty();\n\n    auto on_new_tensor_cb = [&](const TensorStorage& tensor_storage, ggml_tensor** dst_tensor) -> bool {\n        const std::string& name = tensor_storage.name;\n\n        ggml_type tensor_type = tensor_storage.type;\n        if (tensor_should_be_converted(tensor_storage, type)) {\n            tensor_type = type;\n        }\n\n        ggml_tensor* tensor = ggml_new_tensor(ggml_ctx, tensor_type, tensor_storage.n_dims, tensor_storage.ne);\n        if (tensor == NULL) {\n            LOG_ERROR(\"ggml_new_tensor failed\");\n            return false;\n        }\n        ggml_set_name(tensor, name.c_str());\n\n        // LOG_DEBUG(\"%s %d %s %d[%d %d %d %d] %d[%d %d %d %d]\", name.c_str(),\n        // ggml_nbytes(tensor), ggml_type_name(tensor_type),\n        // tensor_storage.n_dims,\n        // tensor_storage.ne[0], tensor_storage.ne[1], tensor_storage.ne[2], tensor_storage.ne[3],\n        // tensor->n_dims, tensor->ne[0], tensor->ne[1], tensor->ne[2], tensor->ne[3]);\n\n        *dst_tensor = tensor;\n\n        gguf_add_tensor(gguf_ctx, tensor);\n\n        return true;\n    };\n\n    bool success = load_tensors(on_new_tensor_cb, backend);\n    ggml_backend_free(backend);\n    LOG_INFO(\"load tensors done\");\n    LOG_INFO(\"trying to save tensors to %s\", file_path.c_str());\n    if (success) {\n        gguf_write_to_file(gguf_ctx, file_path.c_str(), false);\n    }\n    ggml_free(ggml_ctx);\n    gguf_free(gguf_ctx);\n    return success;\n}\n\nint64_t ModelLoader::get_params_mem_size(ggml_backend_t backend, ggml_type type) {\n    size_t alignment = 128;\n    if (backend != NULL) {\n        alignment = ggml_backend_get_alignment(backend);\n    }\n    int64_t mem_size = 0;\n    std::vector<TensorStorage> processed_tensor_storages;\n    for (auto& tensor_storage : tensor_storages) {\n        if (is_unused_tensor(tensor_storage.name)) {\n            continue;\n        }\n        preprocess_tensor(tensor_storage, processed_tensor_storages);\n    }\n\n    for (auto& tensor_storage : processed_tensor_storages) {\n        if (tensor_should_be_converted(tensor_storage, type)) {\n            tensor_storage.type = type;\n        }\n        mem_size += tensor_storage.nbytes() + alignment;\n    }\n\n    return mem_size;\n}\n\nbool convert(const char* input_path, const char* vae_path, const char* output_path, sd_type_t output_type) {\n    ModelLoader model_loader;\n\n    if (!model_loader.init_from_file(input_path)) {\n        LOG_ERROR(\"init model loader from file failed: '%s'\", input_path);\n        return false;\n    }\n\n    if (vae_path != NULL && strlen(vae_path) > 0) {\n        if (!model_loader.init_from_file(vae_path, \"vae.\")) {\n            LOG_ERROR(\"init model loader from file failed: '%s'\", vae_path);\n            return false;\n        }\n    }\n    bool success = model_loader.save_to_gguf_file(output_path, (ggml_type)output_type);\n    return success;\n}\n"
        },
        {
          "name": "model.h",
          "type": "blob",
          "size": 6.4697265625,
          "content": "#ifndef __MODEL_H__\n#define __MODEL_H__\n\n#include <functional>\n#include <map>\n#include <memory>\n#include <set>\n#include <sstream>\n#include <string>\n#include <tuple>\n#include <vector>\n\n#include \"ggml-backend.h\"\n#include \"ggml.h\"\n#include \"json.hpp\"\n#include \"zip.h\"\n\n#define SD_MAX_DIMS 5\n\nenum SDVersion {\n    VERSION_SD1,\n    VERSION_SD1_INPAINT,\n    VERSION_SD2,\n    VERSION_SD2_INPAINT,\n    VERSION_SDXL,\n    VERSION_SDXL_INPAINT,\n    VERSION_SVD,\n    VERSION_SD3,\n    VERSION_FLUX,\n    VERSION_FLUX_FILL,\n    VERSION_COUNT,\n};\n\nstatic inline bool sd_version_is_flux(SDVersion version) {\n    if (version == VERSION_FLUX || version == VERSION_FLUX_FILL) {\n        return true;\n    }\n    return false;\n}\n\nstatic inline bool sd_version_is_sd3(SDVersion version) {\n    if (version == VERSION_SD3) {\n        return true;\n    }\n    return false;\n}\n\nstatic inline bool sd_version_is_sd1(SDVersion version) {\n    if (version == VERSION_SD1 || version == VERSION_SD1_INPAINT) {\n        return true;\n    }\n    return false;\n}\n\nstatic inline bool sd_version_is_sd2(SDVersion version) {\n    if (version == VERSION_SD2 || version == VERSION_SD2_INPAINT) {\n        return true;\n    }\n    return false;\n}\n\nstatic inline bool sd_version_is_sdxl(SDVersion version) {\n    if (version == VERSION_SDXL || version == VERSION_SDXL_INPAINT) {\n        return true;\n    }\n    return false;\n}\n\nstatic inline bool sd_version_is_inpaint(SDVersion version) {\n    if (version == VERSION_SD1_INPAINT || version == VERSION_SD2_INPAINT || version == VERSION_SDXL_INPAINT || version == VERSION_FLUX_FILL) {\n        return true;\n    }\n    return false;\n}\n\nstatic inline bool sd_version_is_dit(SDVersion version) {\n    if (sd_version_is_flux(version) || sd_version_is_sd3(version)) {\n        return true;\n    }\n    return false;\n}\n\nenum PMVersion {\n    PM_VERSION_1,\n    PM_VERSION_2,\n};\n\nstruct TensorStorage {\n    std::string name;\n    ggml_type type          = GGML_TYPE_F32;\n    bool is_bf16            = false;\n    bool is_f8_e4m3         = false;\n    bool is_f8_e5m2         = false;\n    int64_t ne[SD_MAX_DIMS] = {1, 1, 1, 1, 1};\n    int n_dims              = 0;\n\n    size_t file_index = 0;\n    int index_in_zip  = -1;  // >= means stored in a zip file\n    size_t offset     = 0;   // offset in file\n\n    TensorStorage() = default;\n\n    TensorStorage(const std::string& name, ggml_type type, int64_t* ne, int n_dims, size_t file_index, size_t offset = 0)\n        : name(name), type(type), n_dims(n_dims), file_index(file_index), offset(offset) {\n        for (int i = 0; i < n_dims; i++) {\n            this->ne[i] = ne[i];\n        }\n    }\n\n    int64_t nelements() const {\n        int64_t n = 1;\n        for (int i = 0; i < SD_MAX_DIMS; i++) {\n            n *= ne[i];\n        }\n        return n;\n    }\n\n    int64_t nbytes() const {\n        return nelements() * ggml_type_size(type) / ggml_blck_size(type);\n    }\n\n    int64_t nbytes_to_read() const {\n        if (is_bf16 || is_f8_e4m3 || is_f8_e5m2) {\n            return nbytes() / 2;\n        } else {\n            return nbytes();\n        }\n    }\n\n    void unsqueeze() {\n        if (n_dims == 2) {\n            n_dims = 4;\n            ne[3]  = ne[1];\n            ne[2]  = ne[0];\n            ne[1]  = 1;\n            ne[0]  = 1;\n        }\n    }\n\n    std::vector<TensorStorage> chunk(size_t n) {\n        std::vector<TensorStorage> chunks;\n        size_t chunk_size = nbytes_to_read() / n;\n        // printf(\"%d/%d\\n\", chunk_size, nbytes_to_read());\n        reverse_ne();\n        for (int i = 0; i < n; i++) {\n            TensorStorage chunk_i = *this;\n            chunk_i.ne[0]         = ne[0] / n;\n            chunk_i.offset        = offset + i * chunk_size;\n            chunk_i.reverse_ne();\n            chunks.push_back(chunk_i);\n        }\n        reverse_ne();\n        return chunks;\n    }\n\n    void reverse_ne() {\n        int64_t new_ne[SD_MAX_DIMS] = {1, 1, 1, 1, 1};\n        for (int i = 0; i < n_dims; i++) {\n            new_ne[i] = ne[n_dims - 1 - i];\n        }\n        for (int i = 0; i < n_dims; i++) {\n            ne[i] = new_ne[i];\n        }\n    }\n\n    std::string to_string() const {\n        std::stringstream ss;\n        const char* type_name = ggml_type_name(type);\n        if (is_bf16) {\n            type_name = \"bf16\";\n        } else if (is_f8_e4m3) {\n            type_name = \"f8_e4m3\";\n        } else if (is_f8_e5m2) {\n            type_name = \"f8_e5m2\";\n        }\n        ss << name << \" | \" << type_name << \" | \";\n        ss << n_dims << \" [\";\n        for (int i = 0; i < SD_MAX_DIMS; i++) {\n            ss << ne[i];\n            if (i != SD_MAX_DIMS - 1) {\n                ss << \", \";\n            }\n        }\n        ss << \"]\";\n        return ss.str();\n    }\n};\n\ntypedef std::function<bool(const TensorStorage&, ggml_tensor**)> on_new_tensor_cb_t;\n\nclass ModelLoader {\nprotected:\n    std::vector<std::string> file_paths_;\n    std::vector<TensorStorage> tensor_storages;\n\n    bool parse_data_pkl(uint8_t* buffer,\n                        size_t buffer_size,\n                        zip_t* zip,\n                        std::string dir,\n                        size_t file_index,\n                        const std::string prefix);\n\n    bool init_from_gguf_file(const std::string& file_path, const std::string& prefix = \"\");\n    bool init_from_safetensors_file(const std::string& file_path, const std::string& prefix = \"\");\n    bool init_from_ckpt_file(const std::string& file_path, const std::string& prefix = \"\");\n    bool init_from_diffusers_file(const std::string& file_path, const std::string& prefix = \"\");\n\npublic:\n    std::map<std::string, enum ggml_type> tensor_storages_types;\n\n    bool init_from_file(const std::string& file_path, const std::string& prefix = \"\");\n    SDVersion get_sd_version();\n    ggml_type get_sd_wtype();\n    ggml_type get_conditioner_wtype();\n    ggml_type get_diffusion_model_wtype();\n    ggml_type get_vae_wtype();\n    void set_wtype_override(ggml_type wtype, std::string prefix = \"\");\n    bool load_tensors(on_new_tensor_cb_t on_new_tensor_cb, ggml_backend_t backend);\n    bool load_tensors(std::map<std::string, struct ggml_tensor*>& tensors,\n                      ggml_backend_t backend,\n                      std::set<std::string> ignore_tensors = {});\n\n    bool save_to_gguf_file(const std::string& file_path, ggml_type type);\n    bool tensor_should_be_converted(const TensorStorage& tensor_storage, ggml_type type);\n    int64_t get_params_mem_size(ggml_backend_t backend, ggml_type type = GGML_TYPE_COUNT);\n    ~ModelLoader() = default;\n\n    static std::string load_merges();\n    static std::string load_t5_tokenizer_json();\n};\n\n#endif  // __MODEL_H__\n"
        },
        {
          "name": "pmid.hpp",
          "type": "blob",
          "size": 37.6513671875,
          "content": "#ifndef __PMI_HPP__\n#define __PMI_HPP__\n\n#include \"ggml_extend.hpp\"\n\n#include \"clip.hpp\"\n#include \"lora.hpp\"\n\nstruct FuseBlock : public GGMLBlock {\n    // network hparams\n    int in_dim;\n    int out_dim;\n    int hidden_dim;\n    bool use_residue;\n\npublic:\n    FuseBlock(int i_d, int o_d, int h_d, bool use_residue = true)\n        : in_dim(i_d), out_dim(o_d), hidden_dim(h_d), use_residue(use_residue) {\n        blocks[\"fc1\"]       = std::shared_ptr<GGMLBlock>(new Linear(in_dim, hidden_dim, true));\n        blocks[\"fc2\"]       = std::shared_ptr<GGMLBlock>(new Linear(hidden_dim, out_dim, true));\n        blocks[\"layernorm\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(in_dim));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [N, channels, h, w]\n\n        auto fc1        = std::dynamic_pointer_cast<Linear>(blocks[\"fc1\"]);\n        auto fc2        = std::dynamic_pointer_cast<Linear>(blocks[\"fc2\"]);\n        auto layer_norm = std::dynamic_pointer_cast<LayerNorm>(blocks[\"layernorm\"]);\n\n        struct ggml_tensor* r = x;\n        // x = ggml_nn_layer_norm(ctx, x, ln_w, ln_b);\n        x = layer_norm->forward(ctx, x);\n        // x = ggml_add(ctx, ggml_mul_mat(ctx, fc1_w, x),  fc1_b);\n        x = fc1->forward(ctx, x);\n        x = ggml_gelu_inplace(ctx, x);\n        x = fc2->forward(ctx, x);\n        // x = ggml_add(ctx, ggml_mul_mat(ctx, fc2_w, x),  fc2_b);\n        if (use_residue)\n            x = ggml_add(ctx, x, r);\n        return x;\n    }\n};\n\n/*\nclass QFormerPerceiver(nn.Module):\n    def __init__(self, id_embeddings_dim, cross_attention_dim, num_tokens, embedding_dim=1024, use_residual=True, ratio=4):\n        super().__init__()\n\n        self.num_tokens = num_tokens\n        self.cross_attention_dim = cross_attention_dim\n        self.use_residual = use_residual\n        print(cross_attention_dim*num_tokens)\n        self.token_proj = nn.Sequential(\n            nn.Linear(id_embeddings_dim, id_embeddings_dim*ratio),\n            nn.GELU(),\n            nn.Linear(id_embeddings_dim*ratio, cross_attention_dim*num_tokens),\n        )\n        self.token_norm = nn.LayerNorm(cross_attention_dim)\n        self.perceiver_resampler = FacePerceiverResampler(\n            dim=cross_attention_dim,\n            depth=4,\n            dim_head=128,\n            heads=cross_attention_dim // 128,\n            embedding_dim=embedding_dim,\n            output_dim=cross_attention_dim,\n            ff_mult=4,\n        )\n\n    def forward(self, x, last_hidden_state):\n        x = self.token_proj(x)\n        x = x.reshape(-1, self.num_tokens, self.cross_attention_dim)\n        x = self.token_norm(x) # cls token\n        out = self.perceiver_resampler(x, last_hidden_state) # retrieve from patch tokens\n        if self.use_residual: # TODO: if use_residual is not true\n            out = x + 1.0 * out\n        return out\n*/\n\nstruct PMFeedForward : public GGMLBlock {\n    // network hparams\n    int dim;\n\npublic:\n    PMFeedForward(int d, int multi = 4)\n        : dim(d) {\n        int inner_dim = dim * multi;\n        blocks[\"0\"]   = std::shared_ptr<GGMLBlock>(new LayerNorm(dim));\n        blocks[\"1\"]   = std::shared_ptr<GGMLBlock>(new Mlp(dim, inner_dim, dim, false));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* x) {\n        auto norm = std::dynamic_pointer_cast<LayerNorm>(blocks[\"0\"]);\n        auto ff   = std::dynamic_pointer_cast<Mlp>(blocks[\"1\"]);\n\n        x = norm->forward(ctx, x);\n        x = ff->forward(ctx, x);\n        return x;\n    }\n};\n\nstruct PerceiverAttention : public GGMLBlock {\n    // network hparams\n    float scale;   // = dim_head**-0.5\n    int dim_head;  // = dim_head\n    int heads;     // = heads\npublic:\n    PerceiverAttention(int dim, int dim_h = 64, int h = 8)\n        : scale(powf(dim_h, -0.5)), dim_head(dim_h), heads(h) {\n        int inner_dim    = dim_head * heads;\n        blocks[\"norm1\"]  = std::shared_ptr<GGMLBlock>(new LayerNorm(dim));\n        blocks[\"norm2\"]  = std::shared_ptr<GGMLBlock>(new LayerNorm(dim));\n        blocks[\"to_q\"]   = std::shared_ptr<GGMLBlock>(new Linear(dim, inner_dim, false));\n        blocks[\"to_kv\"]  = std::shared_ptr<GGMLBlock>(new Linear(dim, inner_dim * 2, false));\n        blocks[\"to_out\"] = std::shared_ptr<GGMLBlock>(new Linear(inner_dim, dim, false));\n    }\n\n    struct ggml_tensor* reshape_tensor(struct ggml_context* ctx,\n                                       struct ggml_tensor* x,\n                                       int heads) {\n        int64_t ne[4];\n        for (int i = 0; i < 4; ++i)\n            ne[i] = x->ne[i];\n        // print_ggml_tensor(x, true, \"PerceiverAttention reshape x 0: \");\n        // printf(\"heads = %d \\n\", heads);\n        // x = ggml_view_4d(ctx, x, x->ne[0], x->ne[1], heads, x->ne[2]/heads,\n        //                          x->nb[1], x->nb[2], x->nb[3], 0);\n        x = ggml_reshape_4d(ctx, x, x->ne[0] / heads, heads, x->ne[1], x->ne[2]);\n        // x = ggml_view_4d(ctx, x, x->ne[0]/heads, heads, x->ne[1], x->ne[2],\n        //                          x->nb[1], x->nb[2], x->nb[3], 0);\n        // x = ggml_cont(ctx, x);\n        x = ggml_cont(ctx, ggml_permute(ctx, x, 0, 2, 1, 3));\n        // print_ggml_tensor(x, true, \"PerceiverAttention reshape x 1: \");\n        // x  = ggml_reshape_4d(ctx, x, ne[0], heads, ne[1], ne[2]/heads);\n        return x;\n    }\n\n    std::vector<struct ggml_tensor*> chunk_half(struct ggml_context* ctx,\n                                                struct ggml_tensor* x) {\n        auto tlo = ggml_view_4d(ctx, x, x->ne[0] / 2, x->ne[1], x->ne[2], x->ne[3], x->nb[1], x->nb[2], x->nb[3], 0);\n        auto tli = ggml_view_4d(ctx, x, x->ne[0] / 2, x->ne[1], x->ne[2], x->ne[3], x->nb[1], x->nb[2], x->nb[3], x->nb[0] * x->ne[0] / 2);\n        return {ggml_cont(ctx, tlo),\n                ggml_cont(ctx, tli)};\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* x,\n                                struct ggml_tensor* latents) {\n        // x (torch.Tensor): image features\n        //     shape (b, n1, D)\n        // latent (torch.Tensor): latent features\n        //     shape (b, n2, D)\n        int64_t ne[4];\n        for (int i = 0; i < 4; ++i)\n            ne[i] = latents->ne[i];\n\n        auto norm1 = std::dynamic_pointer_cast<LayerNorm>(blocks[\"norm1\"]);\n        auto norm2 = std::dynamic_pointer_cast<LayerNorm>(blocks[\"norm2\"]);\n        x          = norm1->forward(ctx, x);\n        latents    = norm2->forward(ctx, latents);\n        auto to_q  = std::dynamic_pointer_cast<Linear>(blocks[\"to_q\"]);\n        auto q     = to_q->forward(ctx, latents);\n\n        auto kv_input = ggml_concat(ctx, x, latents, 1);\n        auto to_kv    = std::dynamic_pointer_cast<Linear>(blocks[\"to_kv\"]);\n        auto kv       = to_kv->forward(ctx, kv_input);\n        auto k        = ggml_view_4d(ctx, kv, kv->ne[0] / 2, kv->ne[1], kv->ne[2], kv->ne[3], kv->nb[1] / 2, kv->nb[2] / 2, kv->nb[3] / 2, 0);\n        auto v        = ggml_view_4d(ctx, kv, kv->ne[0] / 2, kv->ne[1], kv->ne[2], kv->ne[3], kv->nb[1] / 2, kv->nb[2] / 2, kv->nb[3] / 2, kv->nb[0] * (kv->ne[0] / 2));\n        k             = ggml_cont(ctx, k);\n        v             = ggml_cont(ctx, v);\n        q             = reshape_tensor(ctx, q, heads);\n        k             = reshape_tensor(ctx, k, heads);\n        v             = reshape_tensor(ctx, v, heads);\n        scale         = 1.f / sqrt(sqrt((float)dim_head));\n        k             = ggml_scale_inplace(ctx, k, scale);\n        q             = ggml_scale_inplace(ctx, q, scale);\n        // auto weight = ggml_mul_mat(ctx, q, k);\n        auto weight = ggml_mul_mat(ctx, k, q);  // NOTE order of mul is opposite to pytorch\n\n        // GGML's softmax() is equivalent to pytorch's softmax(x, dim=-1)\n        // in this case, dimension along which Softmax will be computed is the last dim\n        // in torch and the first dim in GGML, consistent with the convention that pytorch's\n        // last dimension (varying most rapidly) corresponds to GGML's first (varying most rapidly).\n        // weight = ggml_soft_max(ctx, weight);\n        weight = ggml_soft_max_inplace(ctx, weight);\n        v      = ggml_cont(ctx, ggml_transpose(ctx, v));\n        // auto out = ggml_mul_mat(ctx, weight, v);\n        auto out    = ggml_mul_mat(ctx, v, weight);  // NOTE order of mul is opposite to pytorch\n        out         = ggml_cont(ctx, ggml_permute(ctx, out, 0, 2, 1, 3));\n        out         = ggml_reshape_3d(ctx, out, ne[0], ne[1], ggml_nelements(out) / (ne[0] * ne[1]));\n        auto to_out = std::dynamic_pointer_cast<Linear>(blocks[\"to_out\"]);\n        out         = to_out->forward(ctx, out);\n        return out;\n    }\n};\n\nstruct FacePerceiverResampler : public GGMLBlock {\n    // network hparams\n    int depth;\n\npublic:\n    FacePerceiverResampler(int dim           = 768,\n                           int d             = 4,\n                           int dim_head      = 64,\n                           int heads         = 16,\n                           int embedding_dim = 1280,\n                           int output_dim    = 768,\n                           int ff_mult       = 4)\n        : depth(d) {\n        blocks[\"proj_in\"]  = std::shared_ptr<GGMLBlock>(new Linear(embedding_dim, dim, true));\n        blocks[\"proj_out\"] = std::shared_ptr<GGMLBlock>(new Linear(dim, output_dim, true));\n        blocks[\"norm_out\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(output_dim));\n\n        for (int i = 0; i < depth; i++) {\n            std::string name = \"layers.\" + std::to_string(i) + \".0\";\n            blocks[name]     = std::shared_ptr<GGMLBlock>(new PerceiverAttention(dim, dim_head, heads));\n            name             = \"layers.\" + std::to_string(i) + \".1\";\n            blocks[name]     = std::shared_ptr<GGMLBlock>(new PMFeedForward(dim, ff_mult));\n        }\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* latents,\n                                struct ggml_tensor* x) {\n        // x: [N, channels, h, w]\n        auto proj_in  = std::dynamic_pointer_cast<Linear>(blocks[\"proj_in\"]);\n        auto proj_out = std::dynamic_pointer_cast<Linear>(blocks[\"proj_out\"]);\n        auto norm_out = std::dynamic_pointer_cast<LayerNorm>(blocks[\"norm_out\"]);\n\n        x = proj_in->forward(ctx, x);\n        for (int i = 0; i < depth; i++) {\n            std::string name = \"layers.\" + std::to_string(i) + \".0\";\n            auto attn        = std::dynamic_pointer_cast<PerceiverAttention>(blocks[name]);\n            name             = \"layers.\" + std::to_string(i) + \".1\";\n            auto ff          = std::dynamic_pointer_cast<PMFeedForward>(blocks[name]);\n            auto t           = attn->forward(ctx, x, latents);\n            latents          = ggml_add(ctx, t, latents);\n            t                = ff->forward(ctx, latents);\n            latents          = ggml_add(ctx, t, latents);\n        }\n        latents = proj_out->forward(ctx, latents);\n        latents = norm_out->forward(ctx, latents);\n        return latents;\n    }\n};\n\nstruct QFormerPerceiver : public GGMLBlock {\n    // network hparams\n    int num_tokens;\n    int cross_attention_dim;\n    bool use_residul;\n\npublic:\n    QFormerPerceiver(int id_embeddings_dim, int cross_attention_d, int num_t, int embedding_dim = 1024, bool use_r = true, int ratio = 4)\n        : cross_attention_dim(cross_attention_d), num_tokens(num_t), use_residul(use_r) {\n        blocks[\"token_proj\"]          = std::shared_ptr<GGMLBlock>(new Mlp(id_embeddings_dim,\n                                                                           id_embeddings_dim * ratio,\n                                                                           cross_attention_dim * num_tokens,\n                                                                           true));\n        blocks[\"token_norm\"]          = std::shared_ptr<GGMLBlock>(new LayerNorm(cross_attention_d));\n        blocks[\"perceiver_resampler\"] = std::shared_ptr<GGMLBlock>(new FacePerceiverResampler(\n            cross_attention_dim,\n            4,\n            128,\n            cross_attention_dim / 128,\n            embedding_dim,\n            cross_attention_dim,\n            4));\n    }\n\n    /*\n    def forward(self, x, last_hidden_state):\n        x = self.token_proj(x)\n        x = x.reshape(-1, self.num_tokens, self.cross_attention_dim)\n        x = self.token_norm(x) # cls token\n        out = self.perceiver_resampler(x, last_hidden_state) # retrieve from patch tokens\n        if self.use_residual: # TODO: if use_residual is not true\n            out = x + 1.0 * out\n        return out\n    */\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* x,\n                                struct ggml_tensor* last_hidden_state) {\n        // x: [N, channels, h, w]\n        auto token_proj          = std::dynamic_pointer_cast<Mlp>(blocks[\"token_proj\"]);\n        auto token_norm          = std::dynamic_pointer_cast<LayerNorm>(blocks[\"token_norm\"]);\n        auto perceiver_resampler = std::dynamic_pointer_cast<FacePerceiverResampler>(blocks[\"perceiver_resampler\"]);\n\n        x                       = token_proj->forward(ctx, x);\n        int64_t nel             = ggml_nelements(x);\n        x                       = ggml_reshape_3d(ctx, x, cross_attention_dim, num_tokens, nel / (cross_attention_dim * num_tokens));\n        x                       = token_norm->forward(ctx, x);\n        struct ggml_tensor* out = perceiver_resampler->forward(ctx, x, last_hidden_state);\n        if (use_residul)\n            out = ggml_add(ctx, x, out);\n        return out;\n    }\n};\n\n/*\nclass FacePerceiverResampler(torch.nn.Module):\n    def __init__(\n        self,\n        *,\n        dim=768,\n        depth=4,\n        dim_head=64,\n        heads=16,\n        embedding_dim=1280,\n        output_dim=768,\n        ff_mult=4,\n    ):\n        super().__init__()\n\n        self.proj_in = torch.nn.Linear(embedding_dim, dim)\n        self.proj_out = torch.nn.Linear(dim, output_dim)\n        self.norm_out = torch.nn.LayerNorm(output_dim)\n        self.layers = torch.nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(\n                torch.nn.ModuleList(\n                    [\n                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n                        FeedForward(dim=dim, mult=ff_mult),\n                    ]\n                )\n            )\n\n    def forward(self, latents, x):\n        x = self.proj_in(x)\n        for attn, ff in self.layers:\n            latents = attn(x, latents) + latents\n            latents = ff(latents) + latents\n        latents = self.proj_out(latents)\n        return self.norm_out(latents)\n*/\n\n/*\n\ndef FeedForward(dim, mult=4):\n    inner_dim = int(dim * mult)\n    return nn.Sequential(\n        nn.LayerNorm(dim),\n        nn.Linear(dim, inner_dim, bias=False),\n        nn.GELU(),\n        nn.Linear(inner_dim, dim, bias=False),\n    )\n\ndef reshape_tensor(x, heads):\n    bs, length, width = x.shape\n    # (bs, length, width) --> (bs, length, n_heads, dim_per_head)\n    x = x.view(bs, length, heads, -1)\n    # (bs, length, n_heads, dim_per_head) --> (bs, n_heads, length, dim_per_head)\n    x = x.transpose(1, 2)\n    # (bs, n_heads, length, dim_per_head) --> (bs*n_heads, length, dim_per_head)\n    x = x.reshape(bs, heads, length, -1)\n    return x\n\nclass PerceiverAttention(nn.Module):\n    def __init__(self, *, dim, dim_head=64, heads=8):\n        super().__init__()\n        self.scale = dim_head**-0.5\n        self.dim_head = dim_head\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n\n    def forward(self, x, latents):\n        \"\"\"\n        Args:\n            x (torch.Tensor): image features\n                shape (b, n1, D)\n            latent (torch.Tensor): latent features\n                shape (b, n2, D)\n        \"\"\"\n        x = self.norm1(x)\n        latents = self.norm2(latents)\n\n        b, l, _ = latents.shape\n\n        q = self.to_q(latents)\n        kv_input = torch.cat((x, latents), dim=-2)\n        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n\n        q = reshape_tensor(q, self.heads)\n        k = reshape_tensor(k, self.heads)\n        v = reshape_tensor(v, self.heads)\n\n        # attention\n        scale = 1 / math.sqrt(math.sqrt(self.dim_head))\n        weight = (q * scale) @ (k * scale).transpose(-2, -1)  # More stable with f16 than dividing afterwards\n        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n        out = weight @ v\n\n        out = out.permute(0, 2, 1, 3).reshape(b, l, -1)\n\n        return self.to_out(out)\n\n*/\n\nstruct FuseModule : public GGMLBlock {\n    // network hparams\n    int embed_dim;\n\npublic:\n    FuseModule(int imb_d)\n        : embed_dim(imb_d) {\n        blocks[\"mlp1\"]       = std::shared_ptr<GGMLBlock>(new FuseBlock(imb_d * 2, imb_d, imb_d, false));\n        blocks[\"mlp2\"]       = std::shared_ptr<GGMLBlock>(new FuseBlock(imb_d, imb_d, imb_d, true));\n        blocks[\"layer_norm\"] = std::shared_ptr<GGMLBlock>(new LayerNorm(embed_dim));\n    }\n\n    struct ggml_tensor* fuse_fn(struct ggml_context* ctx,\n                                struct ggml_tensor* prompt_embeds,\n                                struct ggml_tensor* id_embeds) {\n        auto mlp1       = std::dynamic_pointer_cast<FuseBlock>(blocks[\"mlp1\"]);\n        auto mlp2       = std::dynamic_pointer_cast<FuseBlock>(blocks[\"mlp2\"]);\n        auto layer_norm = std::dynamic_pointer_cast<LayerNorm>(blocks[\"layer_norm\"]);\n\n        // print_ggml_tensor(id_embeds, true, \"Fuseblock id_embeds: \");\n        // print_ggml_tensor(prompt_embeds, true, \"Fuseblock prompt_embeds: \");\n\n        // auto prompt_embeds0 = ggml_cont(ctx, ggml_permute(ctx, prompt_embeds, 2, 0, 1, 3));\n        // auto id_embeds0     = ggml_cont(ctx, ggml_permute(ctx, id_embeds, 2, 0, 1, 3));\n        // print_ggml_tensor(id_embeds0, true, \"Fuseblock id_embeds0: \");\n        // print_ggml_tensor(prompt_embeds0, true, \"Fuseblock prompt_embeds0: \");\n        // concat is along dim 2\n        // auto stacked_id_embeds = ggml_concat(ctx, prompt_embeds0, id_embeds0, 2);\n        auto stacked_id_embeds = ggml_concat(ctx, prompt_embeds, id_embeds, 0);\n        // print_ggml_tensor(stacked_id_embeds, true, \"Fuseblock stacked_id_embeds 0: \");\n        // stacked_id_embeds      = ggml_cont(ctx, ggml_permute(ctx, stacked_id_embeds, 1, 2, 0, 3));\n        // print_ggml_tensor(stacked_id_embeds, true, \"Fuseblock stacked_id_embeds 1: \");\n        // stacked_id_embeds = mlp1.forward(ctx, stacked_id_embeds);\n        // stacked_id_embeds = ggml_add(ctx, stacked_id_embeds, prompt_embeds);\n        // stacked_id_embeds = mlp2.forward(ctx, stacked_id_embeds);\n        // stacked_id_embeds = ggml_nn_layer_norm(ctx, stacked_id_embeds, ln_w, ln_b);\n\n        stacked_id_embeds = mlp1->forward(ctx, stacked_id_embeds);\n        stacked_id_embeds = ggml_add(ctx, stacked_id_embeds, prompt_embeds);\n        stacked_id_embeds = mlp2->forward(ctx, stacked_id_embeds);\n        stacked_id_embeds = layer_norm->forward(ctx, stacked_id_embeds);\n\n        // print_ggml_tensor(stacked_id_embeds, true, \"Fuseblock stacked_id_embeds 1: \");\n\n        return stacked_id_embeds;\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* prompt_embeds,\n                                struct ggml_tensor* id_embeds,\n                                struct ggml_tensor* class_tokens_mask,\n                                struct ggml_tensor* class_tokens_mask_pos,\n                                struct ggml_tensor* left,\n                                struct ggml_tensor* right) {\n        // x: [N, channels, h, w]\n\n        struct ggml_tensor* valid_id_embeds = id_embeds;\n        // # slice out the image token embeddings\n        // print_ggml_tensor(class_tokens_mask_pos, false);\n        ggml_set_name(class_tokens_mask_pos, \"class_tokens_mask_pos\");\n        ggml_set_name(prompt_embeds, \"prompt_embeds\");\n        // print_ggml_tensor(valid_id_embeds, true, \"valid_id_embeds\");\n        // print_ggml_tensor(class_tokens_mask_pos, true, \"class_tokens_mask_pos\");\n        struct ggml_tensor* image_token_embeds = ggml_get_rows(ctx, prompt_embeds, class_tokens_mask_pos);\n        ggml_set_name(image_token_embeds, \"image_token_embeds\");\n        valid_id_embeds                       = ggml_reshape_2d(ctx, valid_id_embeds, valid_id_embeds->ne[0],\n                                                                ggml_nelements(valid_id_embeds) / valid_id_embeds->ne[0]);\n        struct ggml_tensor* stacked_id_embeds = fuse_fn(ctx, image_token_embeds, valid_id_embeds);\n\n        // stacked_id_embeds = ggml_cont(ctx, ggml_permute(ctx, stacked_id_embeds, 0, 2, 1, 3));\n        // print_ggml_tensor(stacked_id_embeds, true, \"AA stacked_id_embeds\");\n        // print_ggml_tensor(left, true, \"AA left\");\n        // print_ggml_tensor(right, true, \"AA right\");\n        if (left && right) {\n            stacked_id_embeds = ggml_concat(ctx, left, stacked_id_embeds, 1);\n            stacked_id_embeds = ggml_concat(ctx, stacked_id_embeds, right, 1);\n        } else if (left) {\n            stacked_id_embeds = ggml_concat(ctx, left, stacked_id_embeds, 1);\n        } else if (right) {\n            stacked_id_embeds = ggml_concat(ctx, stacked_id_embeds, right, 1);\n        }\n        // print_ggml_tensor(stacked_id_embeds, true, \"BB stacked_id_embeds\");\n        // stacked_id_embeds                         = ggml_cont(ctx, ggml_permute(ctx, stacked_id_embeds, 0, 2, 1, 3));\n        // print_ggml_tensor(stacked_id_embeds, true, \"CC stacked_id_embeds\");\n        class_tokens_mask                         = ggml_cont(ctx, ggml_transpose(ctx, class_tokens_mask));\n        class_tokens_mask                         = ggml_repeat(ctx, class_tokens_mask, prompt_embeds);\n        prompt_embeds                             = ggml_mul(ctx, prompt_embeds, class_tokens_mask);\n        struct ggml_tensor* updated_prompt_embeds = ggml_add(ctx, prompt_embeds, stacked_id_embeds);\n        ggml_set_name(updated_prompt_embeds, \"updated_prompt_embeds\");\n        // print_ggml_tensor(updated_prompt_embeds, true, \"updated_prompt_embeds: \");\n        return updated_prompt_embeds;\n    }\n};\n\nstruct PhotoMakerIDEncoderBlock : public CLIPVisionModelProjection {\n    PhotoMakerIDEncoderBlock()\n        : CLIPVisionModelProjection(OPENAI_CLIP_VIT_L_14) {\n        blocks[\"visual_projection_2\"] = std::shared_ptr<GGMLBlock>(new Linear(1024, 1280, false));\n        blocks[\"fuse_module\"]         = std::shared_ptr<GGMLBlock>(new FuseModule(2048));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* id_pixel_values,\n                                struct ggml_tensor* prompt_embeds,\n                                struct ggml_tensor* class_tokens_mask,\n                                struct ggml_tensor* class_tokens_mask_pos,\n                                struct ggml_tensor* left,\n                                struct ggml_tensor* right) {\n        // x: [N, channels, h, w]\n        auto vision_model        = std::dynamic_pointer_cast<CLIPVisionModel>(blocks[\"vision_model\"]);\n        auto visual_projection   = std::dynamic_pointer_cast<CLIPProjection>(blocks[\"visual_projection\"]);\n        auto visual_projection_2 = std::dynamic_pointer_cast<Linear>(blocks[\"visual_projection_2\"]);\n        auto fuse_module         = std::dynamic_pointer_cast<FuseModule>(blocks[\"fuse_module\"]);\n\n        struct ggml_tensor* shared_id_embeds = vision_model->forward(ctx, id_pixel_values);          // [N, hidden_size]\n        struct ggml_tensor* id_embeds        = visual_projection->forward(ctx, shared_id_embeds);    // [N, proj_dim(768)]\n        struct ggml_tensor* id_embeds_2      = visual_projection_2->forward(ctx, shared_id_embeds);  // [N, 1280]\n\n        id_embeds   = ggml_cont(ctx, ggml_permute(ctx, id_embeds, 2, 0, 1, 3));\n        id_embeds_2 = ggml_cont(ctx, ggml_permute(ctx, id_embeds_2, 2, 0, 1, 3));\n\n        id_embeds = ggml_concat(ctx, id_embeds, id_embeds_2, 2);  // [batch_size, seq_length, 1, 2048] check whether concat at dim 2 is right\n        id_embeds = ggml_cont(ctx, ggml_permute(ctx, id_embeds, 1, 2, 0, 3));\n\n        struct ggml_tensor* updated_prompt_embeds = fuse_module->forward(ctx,\n                                                                         prompt_embeds,\n                                                                         id_embeds,\n                                                                         class_tokens_mask,\n                                                                         class_tokens_mask_pos,\n                                                                         left, right);\n        return updated_prompt_embeds;\n    }\n};\n\nstruct PhotoMakerIDEncoder_CLIPInsightfaceExtendtokenBlock : public CLIPVisionModelProjection {\n    int cross_attention_dim;\n    int num_tokens;\n\n    PhotoMakerIDEncoder_CLIPInsightfaceExtendtokenBlock(int id_embeddings_dim = 512)\n        : CLIPVisionModelProjection(OPENAI_CLIP_VIT_L_14),\n          cross_attention_dim(2048),\n          num_tokens(2) {\n        blocks[\"visual_projection_2\"] = std::shared_ptr<GGMLBlock>(new Linear(1024, 1280, false));\n        blocks[\"fuse_module\"]         = std::shared_ptr<GGMLBlock>(new FuseModule(2048));\n        /*\n        cross_attention_dim = 2048\n        # projection\n        self.num_tokens = 2\n        self.cross_attention_dim = cross_attention_dim\n        self.qformer_perceiver = QFormerPerceiver(\n                                    id_embeddings_dim,\n                                    cross_attention_dim,\n                                    self.num_tokens,\n                                )*/\n        blocks[\"qformer_perceiver\"] = std::shared_ptr<GGMLBlock>(new QFormerPerceiver(id_embeddings_dim,\n                                                                                      cross_attention_dim,\n                                                                                      num_tokens));\n    }\n\n    /*\n    def forward(self, id_pixel_values, prompt_embeds, class_tokens_mask, id_embeds):\n        b, num_inputs, c, h, w = id_pixel_values.shape\n        id_pixel_values = id_pixel_values.view(b * num_inputs, c, h, w)\n\n        last_hidden_state = self.vision_model(id_pixel_values)[0]\n        id_embeds = id_embeds.view(b * num_inputs, -1)\n\n        id_embeds = self.qformer_perceiver(id_embeds, last_hidden_state)\n        id_embeds = id_embeds.view(b, num_inputs, self.num_tokens, -1)\n        updated_prompt_embeds = self.fuse_module(prompt_embeds, id_embeds, class_tokens_mask)\n    */\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* id_pixel_values,\n                                struct ggml_tensor* prompt_embeds,\n                                struct ggml_tensor* class_tokens_mask,\n                                struct ggml_tensor* class_tokens_mask_pos,\n                                struct ggml_tensor* id_embeds,\n                                struct ggml_tensor* left,\n                                struct ggml_tensor* right) {\n        // x: [N, channels, h, w]\n        auto vision_model      = std::dynamic_pointer_cast<CLIPVisionModel>(blocks[\"vision_model\"]);\n        auto fuse_module       = std::dynamic_pointer_cast<FuseModule>(blocks[\"fuse_module\"]);\n        auto qformer_perceiver = std::dynamic_pointer_cast<QFormerPerceiver>(blocks[\"qformer_perceiver\"]);\n\n        // struct ggml_tensor* last_hidden_state = vision_model->forward(ctx, id_pixel_values);          // [N, hidden_size]\n        struct ggml_tensor* last_hidden_state = vision_model->forward(ctx, id_pixel_values, false);  // [N, hidden_size]\n        id_embeds                             = qformer_perceiver->forward(ctx, id_embeds, last_hidden_state);\n\n        struct ggml_tensor* updated_prompt_embeds = fuse_module->forward(ctx,\n                                                                         prompt_embeds,\n                                                                         id_embeds,\n                                                                         class_tokens_mask,\n                                                                         class_tokens_mask_pos,\n                                                                         left, right);\n        return updated_prompt_embeds;\n    }\n};\n\nstruct PhotoMakerIDEncoder : public GGMLRunner {\npublic:\n    SDVersion version    = VERSION_SDXL;\n    PMVersion pm_version = PM_VERSION_1;\n    PhotoMakerIDEncoderBlock id_encoder;\n    PhotoMakerIDEncoder_CLIPInsightfaceExtendtokenBlock id_encoder2;\n    float style_strength;\n\n    std::vector<float> ctm;\n    std::vector<ggml_fp16_t> ctmf16;\n    std::vector<int> ctmpos;\n\n    std::vector<ggml_fp16_t> zeros_left_16;\n    std::vector<float> zeros_left;\n    std::vector<ggml_fp16_t> zeros_right_16;\n    std::vector<float> zeros_right;\n\npublic:\n    PhotoMakerIDEncoder(ggml_backend_t backend, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix, SDVersion version = VERSION_SDXL, PMVersion pm_v = PM_VERSION_1, float sty = 20.f)\n        : GGMLRunner(backend),\n          version(version),\n          pm_version(pm_v),\n          style_strength(sty) {\n        if (pm_version == PM_VERSION_1) {\n            id_encoder.init(params_ctx, tensor_types, prefix);\n        } else if (pm_version == PM_VERSION_2) {\n            id_encoder2.init(params_ctx, tensor_types, prefix);\n        }\n    }\n\n    std::string get_desc() {\n        return \"pmid\";\n    }\n\n    PMVersion get_version() const {\n        return pm_version;\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors, const std::string prefix) {\n        if (pm_version == PM_VERSION_1)\n            id_encoder.get_param_tensors(tensors, prefix);\n        else if (pm_version == PM_VERSION_2)\n            id_encoder2.get_param_tensors(tensors, prefix);\n    }\n\n    struct ggml_cgraph* build_graph(  // struct ggml_allocr* allocr,\n        struct ggml_tensor* id_pixel_values,\n        struct ggml_tensor* prompt_embeds,\n        std::vector<bool>& class_tokens_mask,\n        struct ggml_tensor* id_embeds) {\n        ctm.clear();\n        ctmf16.clear();\n        ctmpos.clear();\n        zeros_left.clear();\n        zeros_left_16.clear();\n        zeros_right.clear();\n        zeros_right_16.clear();\n\n        ggml_context* ctx0 = compute_ctx;\n\n        struct ggml_cgraph* gf = ggml_new_graph(compute_ctx);\n\n        int64_t hidden_size = prompt_embeds->ne[0];\n        int64_t seq_length  = prompt_embeds->ne[1];\n        ggml_type type      = GGML_TYPE_F32;\n\n        struct ggml_tensor* class_tokens_mask_d = ggml_new_tensor_1d(ctx0, type, class_tokens_mask.size());\n\n        struct ggml_tensor* id_pixel_values_d = to_backend(id_pixel_values);\n        struct ggml_tensor* prompt_embeds_d   = to_backend(prompt_embeds);\n        struct ggml_tensor* id_embeds_d       = to_backend(id_embeds);\n\n        struct ggml_tensor* left  = NULL;\n        struct ggml_tensor* right = NULL;\n        for (int i = 0; i < class_tokens_mask.size(); i++) {\n            if (class_tokens_mask[i]) {\n                // printf(\" 1,\");\n                ctm.push_back(0.f);                        // here use 0.f instead of 1.f to make a scale mask\n                ctmf16.push_back(ggml_fp32_to_fp16(0.f));  // here use 0.f instead of 1.f to make a scale mask\n                ctmpos.push_back(i);\n            } else {\n                // printf(\" 0,\");\n                ctm.push_back(1.f);                        // here use 1.f instead of 0.f to make a scale mask\n                ctmf16.push_back(ggml_fp32_to_fp16(1.f));  // here use 0.f instead of 1.f to make a scale mask\n            }\n        }\n        // printf(\"\\n\");\n        if (ctmpos[0] > 0) {\n            // left = ggml_new_tensor_3d(ctx0, type, hidden_size, 1, ctmpos[0]);\n            left = ggml_new_tensor_3d(ctx0, type, hidden_size, ctmpos[0], 1);\n        }\n        if (ctmpos[ctmpos.size() - 1] < seq_length - 1) {\n            // right = ggml_new_tensor_3d(ctx0, type,\n            //                            hidden_size, 1, seq_length - ctmpos[ctmpos.size() - 1] - 1);\n            right = ggml_new_tensor_3d(ctx0, type,\n                                       hidden_size, seq_length - ctmpos[ctmpos.size() - 1] - 1, 1);\n        }\n        struct ggml_tensor* class_tokens_mask_pos = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, ctmpos.size());\n\n        {\n            if (type == GGML_TYPE_F16)\n                set_backend_tensor_data(class_tokens_mask_d, ctmf16.data());\n            else\n                set_backend_tensor_data(class_tokens_mask_d, ctm.data());\n            set_backend_tensor_data(class_tokens_mask_pos, ctmpos.data());\n            if (left) {\n                if (type == GGML_TYPE_F16) {\n                    for (int i = 0; i < ggml_nelements(left); ++i)\n                        zeros_left_16.push_back(ggml_fp32_to_fp16(0.f));\n                    set_backend_tensor_data(left, zeros_left_16.data());\n                } else {\n                    for (int i = 0; i < ggml_nelements(left); ++i)\n                        zeros_left.push_back(0.f);\n                    set_backend_tensor_data(left, zeros_left.data());\n                }\n            }\n            if (right) {\n                if (type == GGML_TYPE_F16) {\n                    for (int i = 0; i < ggml_nelements(right); ++i)\n                        zeros_right_16.push_back(ggml_fp32_to_fp16(0.f));\n                    set_backend_tensor_data(right, zeros_right_16.data());\n                } else {\n                    for (int i = 0; i < ggml_nelements(right); ++i)\n                        zeros_right.push_back(0.f);\n                    set_backend_tensor_data(right, zeros_right.data());\n                }\n            }\n        }\n        struct ggml_tensor* updated_prompt_embeds = NULL;\n        if (pm_version == PM_VERSION_1)\n            updated_prompt_embeds = id_encoder.forward(ctx0,\n                                                       id_pixel_values_d,\n                                                       prompt_embeds_d,\n                                                       class_tokens_mask_d,\n                                                       class_tokens_mask_pos,\n                                                       left, right);\n        else if (pm_version == PM_VERSION_2)\n            updated_prompt_embeds = id_encoder2.forward(ctx0,\n                                                        id_pixel_values_d,\n                                                        prompt_embeds_d,\n                                                        class_tokens_mask_d,\n                                                        class_tokens_mask_pos,\n                                                        id_embeds_d,\n                                                        left, right);\n\n        ggml_build_forward_expand(gf, updated_prompt_embeds);\n\n        return gf;\n    }\n\n    void compute(const int n_threads,\n                 struct ggml_tensor* id_pixel_values,\n                 struct ggml_tensor* prompt_embeds,\n                 struct ggml_tensor* id_embeds,\n                 std::vector<bool>& class_tokens_mask,\n                 struct ggml_tensor** updated_prompt_embeds,\n                 ggml_context* output_ctx) {\n        auto get_graph = [&]() -> struct ggml_cgraph* {\n            // return build_graph(compute_allocr, id_pixel_values, prompt_embeds, class_tokens_mask);\n            return build_graph(id_pixel_values, prompt_embeds, class_tokens_mask, id_embeds);\n        };\n\n        // GGMLRunner::compute(get_graph, n_threads, updated_prompt_embeds);\n        GGMLRunner::compute(get_graph, n_threads, true, updated_prompt_embeds, output_ctx);\n    }\n};\n\nstruct PhotoMakerIDEmbed : public GGMLRunner {\n    std::map<std::string, struct ggml_tensor*> tensors;\n    std::string file_path;\n    ModelLoader* model_loader;\n    bool load_failed = false;\n    bool applied     = false;\n\n    PhotoMakerIDEmbed(ggml_backend_t backend,\n                      ModelLoader* ml,\n                      const std::string& file_path = \"\",\n                      const std::string& prefix    = \"\")\n        : file_path(file_path), GGMLRunner(backend), model_loader(ml) {\n        if (!model_loader->init_from_file(file_path, prefix)) {\n            load_failed = true;\n        }\n    }\n\n    std::string get_desc() {\n        return \"id_embeds\";\n    }\n\n    bool load_from_file(bool filter_tensor = false) {\n        LOG_INFO(\"loading PhotoMaker ID Embeds from '%s'\", file_path.c_str());\n\n        if (load_failed) {\n            LOG_ERROR(\"init photomaker id embed from file failed: '%s'\", file_path.c_str());\n            return false;\n        }\n\n        bool dry_run          = true;\n        auto on_new_tensor_cb = [&](const TensorStorage& tensor_storage, ggml_tensor** dst_tensor) -> bool {\n            const std::string& name = tensor_storage.name;\n\n            if (filter_tensor && !contains(name, \"pmid.id_embeds\")) {\n                // LOG_INFO(\"skipping LoRA tesnor '%s'\", name.c_str());\n                return true;\n            }\n            if (dry_run) {\n                struct ggml_tensor* real = ggml_new_tensor(params_ctx,\n                                                           tensor_storage.type,\n                                                           tensor_storage.n_dims,\n                                                           tensor_storage.ne);\n                tensors[name]            = real;\n            } else {\n                auto real   = tensors[name];\n                *dst_tensor = real;\n            }\n\n            return true;\n        };\n\n        model_loader->load_tensors(on_new_tensor_cb, backend);\n        alloc_params_buffer();\n\n        dry_run = false;\n        model_loader->load_tensors(on_new_tensor_cb, backend);\n\n        LOG_DEBUG(\"finished loading PhotoMaker ID Embeds \");\n        return true;\n    }\n\n    struct ggml_tensor* get() {\n        std::map<std::string, struct ggml_tensor*>::iterator pos;\n        pos = tensors.find(\"pmid.id_embeds\");\n        if (pos != tensors.end())\n            return pos->second;\n        return NULL;\n    }\n};\n\n#endif  // __PMI_HPP__\n"
        },
        {
          "name": "preprocessing.hpp",
          "type": "blob",
          "size": 8.78125,
          "content": "#ifndef __PREPROCESSING_HPP__\n#define __PREPROCESSING_HPP__\n\n#include \"ggml_extend.hpp\"\n#define M_PI_ 3.14159265358979323846\n\nvoid convolve(struct ggml_tensor* input, struct ggml_tensor* output, struct ggml_tensor* kernel, int padding) {\n    struct ggml_init_params params;\n    params.mem_size                 = 20 * 1024 * 1024;  // 10\n    params.mem_buffer               = NULL;\n    params.no_alloc                 = false;\n    struct ggml_context* ctx0       = ggml_init(params);\n    struct ggml_tensor* kernel_fp16 = ggml_new_tensor_4d(ctx0, GGML_TYPE_F16, kernel->ne[0], kernel->ne[1], 1, 1);\n    ggml_fp32_to_fp16_row((float*)kernel->data, (ggml_fp16_t*)kernel_fp16->data, ggml_nelements(kernel));\n    ggml_tensor* h  = ggml_conv_2d(ctx0, kernel_fp16, input, 1, 1, padding, padding, 1, 1);\n    ggml_cgraph* gf = ggml_new_graph(ctx0);\n    ggml_build_forward_expand(gf, ggml_cpy(ctx0, h, output));\n    ggml_graph_compute_with_ctx(ctx0, gf, 1);\n    ggml_free(ctx0);\n}\n\nvoid gaussian_kernel(struct ggml_tensor* kernel) {\n    int ks_mid   = kernel->ne[0] / 2;\n    float sigma  = 1.4f;\n    float normal = 1.f / (2.0f * M_PI_ * powf(sigma, 2.0f));\n    for (int y = 0; y < kernel->ne[0]; y++) {\n        float gx = -ks_mid + y;\n        for (int x = 0; x < kernel->ne[1]; x++) {\n            float gy = -ks_mid + x;\n            float k_ = expf(-((gx * gx + gy * gy) / (2.0f * powf(sigma, 2.0f)))) * normal;\n            ggml_tensor_set_f32(kernel, k_, x, y);\n        }\n    }\n}\n\nvoid grayscale(struct ggml_tensor* rgb_img, struct ggml_tensor* grayscale) {\n    for (int iy = 0; iy < rgb_img->ne[1]; iy++) {\n        for (int ix = 0; ix < rgb_img->ne[0]; ix++) {\n            float r    = ggml_tensor_get_f32(rgb_img, ix, iy);\n            float g    = ggml_tensor_get_f32(rgb_img, ix, iy, 1);\n            float b    = ggml_tensor_get_f32(rgb_img, ix, iy, 2);\n            float gray = 0.2989f * r + 0.5870f * g + 0.1140f * b;\n            ggml_tensor_set_f32(grayscale, gray, ix, iy);\n        }\n    }\n}\n\nvoid prop_hypot(struct ggml_tensor* x, struct ggml_tensor* y, struct ggml_tensor* h) {\n    int n_elements = ggml_nelements(h);\n    float* dx      = (float*)x->data;\n    float* dy      = (float*)y->data;\n    float* dh      = (float*)h->data;\n    for (int i = 0; i < n_elements; i++) {\n        dh[i] = sqrtf(dx[i] * dx[i] + dy[i] * dy[i]);\n    }\n}\n\nvoid prop_arctan2(struct ggml_tensor* x, struct ggml_tensor* y, struct ggml_tensor* h) {\n    int n_elements = ggml_nelements(h);\n    float* dx      = (float*)x->data;\n    float* dy      = (float*)y->data;\n    float* dh      = (float*)h->data;\n    for (int i = 0; i < n_elements; i++) {\n        dh[i] = atan2f(dy[i], dx[i]);\n    }\n}\n\nvoid normalize_tensor(struct ggml_tensor* g) {\n    int n_elements = ggml_nelements(g);\n    float* dg      = (float*)g->data;\n    float max      = -INFINITY;\n    for (int i = 0; i < n_elements; i++) {\n        max = dg[i] > max ? dg[i] : max;\n    }\n    max = 1.0f / max;\n    for (int i = 0; i < n_elements; i++) {\n        dg[i] *= max;\n    }\n}\n\nvoid non_max_supression(struct ggml_tensor* result, struct ggml_tensor* G, struct ggml_tensor* D) {\n    for (int iy = 1; iy < result->ne[1] - 1; iy++) {\n        for (int ix = 1; ix < result->ne[0] - 1; ix++) {\n            float angle = ggml_tensor_get_f32(D, ix, iy) * 180.0f / M_PI_;\n            angle       = angle < 0.0f ? angle += 180.0f : angle;\n            float q     = 1.0f;\n            float r     = 1.0f;\n\n            // angle 0\n            if ((0 >= angle && angle < 22.5f) || (157.5f >= angle && angle <= 180)) {\n                q = ggml_tensor_get_f32(G, ix, iy + 1);\n                r = ggml_tensor_get_f32(G, ix, iy - 1);\n            }\n            // angle 45\n            else if (22.5f >= angle && angle < 67.5f) {\n                q = ggml_tensor_get_f32(G, ix + 1, iy - 1);\n                r = ggml_tensor_get_f32(G, ix - 1, iy + 1);\n            }\n            // angle 90\n            else if (67.5f >= angle && angle < 112.5) {\n                q = ggml_tensor_get_f32(G, ix + 1, iy);\n                r = ggml_tensor_get_f32(G, ix - 1, iy);\n            }\n            // angle 135\n            else if (112.5 >= angle && angle < 157.5f) {\n                q = ggml_tensor_get_f32(G, ix - 1, iy - 1);\n                r = ggml_tensor_get_f32(G, ix + 1, iy + 1);\n            }\n\n            float cur = ggml_tensor_get_f32(G, ix, iy);\n            if ((cur >= q) && (cur >= r)) {\n                ggml_tensor_set_f32(result, cur, ix, iy);\n            } else {\n                ggml_tensor_set_f32(result, 0.0f, ix, iy);\n            }\n        }\n    }\n}\n\nvoid threshold_hystersis(struct ggml_tensor* img, float high_threshold, float low_threshold, float weak, float strong) {\n    int n_elements = ggml_nelements(img);\n    float* imd     = (float*)img->data;\n    float max      = -INFINITY;\n    for (int i = 0; i < n_elements; i++) {\n        max = imd[i] > max ? imd[i] : max;\n    }\n    float ht = max * high_threshold;\n    float lt = ht * low_threshold;\n    for (int i = 0; i < n_elements; i++) {\n        float img_v = imd[i];\n        if (img_v >= ht) {  // strong pixel\n            imd[i] = strong;\n        } else if (img_v <= ht && img_v >= lt) {  // strong pixel\n            imd[i] = weak;\n        }\n    }\n\n    for (int iy = 0; iy < img->ne[1]; iy++) {\n        for (int ix = 0; ix < img->ne[0]; ix++) {\n            if (ix >= 3 && ix <= img->ne[0] - 3 && iy >= 3 && iy <= img->ne[1] - 3) {\n                ggml_tensor_set_f32(img, ggml_tensor_get_f32(img, ix, iy), ix, iy);\n            } else {\n                ggml_tensor_set_f32(img, 0.0f, ix, iy);\n            }\n        }\n    }\n\n    // hysteresis\n    for (int iy = 1; iy < img->ne[1] - 1; iy++) {\n        for (int ix = 1; ix < img->ne[0] - 1; ix++) {\n            float imd_v = ggml_tensor_get_f32(img, ix, iy);\n            if (imd_v == weak) {\n                if (ggml_tensor_get_f32(img, ix + 1, iy - 1) == strong || ggml_tensor_get_f32(img, ix + 1, iy) == strong ||\n                    ggml_tensor_get_f32(img, ix, iy - 1) == strong || ggml_tensor_get_f32(img, ix, iy + 1) == strong ||\n                    ggml_tensor_get_f32(img, ix - 1, iy - 1) == strong || ggml_tensor_get_f32(img, ix - 1, iy) == strong) {\n                    ggml_tensor_set_f32(img, strong, ix, iy);\n                } else {\n                    ggml_tensor_set_f32(img, 0.0f, ix, iy);\n                }\n            }\n        }\n    }\n}\n\nuint8_t* preprocess_canny(uint8_t* img, int width, int height, float high_threshold, float low_threshold, float weak, float strong, bool inverse) {\n    struct ggml_init_params params;\n    params.mem_size               = static_cast<size_t>(10 * 1024 * 1024);  // 10\n    params.mem_buffer             = NULL;\n    params.no_alloc               = false;\n    struct ggml_context* work_ctx = ggml_init(params);\n\n    if (!work_ctx) {\n        LOG_ERROR(\"ggml_init() failed\");\n        return NULL;\n    }\n\n    float kX[9] = {\n        -1, 0, 1,\n        -2, 0, 2,\n        -1, 0, 1};\n\n    float kY[9] = {\n        1, 2, 1,\n        0, 0, 0,\n        -1, -2, -1};\n\n    // generate kernel\n    int kernel_size             = 5;\n    struct ggml_tensor* gkernel = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, kernel_size, kernel_size, 1, 1);\n    struct ggml_tensor* sf_kx   = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 3, 3, 1, 1);\n    memcpy(sf_kx->data, kX, ggml_nbytes(sf_kx));\n    struct ggml_tensor* sf_ky = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 3, 3, 1, 1);\n    memcpy(sf_ky->data, kY, ggml_nbytes(sf_ky));\n    gaussian_kernel(gkernel);\n    struct ggml_tensor* image      = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, width, height, 3, 1);\n    struct ggml_tensor* image_gray = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, width, height, 1, 1);\n    struct ggml_tensor* iX         = ggml_dup_tensor(work_ctx, image_gray);\n    struct ggml_tensor* iY         = ggml_dup_tensor(work_ctx, image_gray);\n    struct ggml_tensor* G          = ggml_dup_tensor(work_ctx, image_gray);\n    struct ggml_tensor* tetha      = ggml_dup_tensor(work_ctx, image_gray);\n    sd_image_to_tensor(img, image);\n    grayscale(image, image_gray);\n    convolve(image_gray, image_gray, gkernel, 2);\n    convolve(image_gray, iX, sf_kx, 1);\n    convolve(image_gray, iY, sf_ky, 1);\n    prop_hypot(iX, iY, G);\n    normalize_tensor(G);\n    prop_arctan2(iX, iY, tetha);\n    non_max_supression(image_gray, G, tetha);\n    threshold_hystersis(image_gray, high_threshold, low_threshold, weak, strong);\n    // to RGB channels\n    for (int iy = 0; iy < height; iy++) {\n        for (int ix = 0; ix < width; ix++) {\n            float gray = ggml_tensor_get_f32(image_gray, ix, iy);\n            gray       = inverse ? 1.0f - gray : gray;\n            ggml_tensor_set_f32(image, gray, ix, iy);\n            ggml_tensor_set_f32(image, gray, ix, iy, 1);\n            ggml_tensor_set_f32(image, gray, ix, iy, 2);\n        }\n    }\n    free(img);\n    uint8_t* output = sd_tensor_to_image(image);\n    ggml_free(work_ctx);\n    return output;\n}\n\n#endif  // __PREPROCESSING_HPP__"
        },
        {
          "name": "rng.hpp",
          "type": "blob",
          "size": 0.783203125,
          "content": "#ifndef __RNG_H__\n#define __RNG_H__\n\n#include <random>\n#include <vector>\n\nclass RNG {\npublic:\n    virtual void manual_seed(uint64_t seed)      = 0;\n    virtual std::vector<float> randn(uint32_t n) = 0;\n};\n\nclass STDDefaultRNG : public RNG {\nprivate:\n    std::default_random_engine generator;\n\npublic:\n    void manual_seed(uint64_t seed) {\n        generator.seed((unsigned int)seed);\n    }\n\n    std::vector<float> randn(uint32_t n) {\n        std::vector<float> result;\n        float mean   = 0.0;\n        float stddev = 1.0;\n        std::normal_distribution<float> distribution(mean, stddev);\n        for (uint32_t i = 0; i < n; i++) {\n            float random_number = distribution(generator);\n            result.push_back(random_number);\n        }\n        return result;\n    }\n};\n\n#endif  // __RNG_H__"
        },
        {
          "name": "rng_philox.hpp",
          "type": "blob",
          "size": 4.255859375,
          "content": "#ifndef __RNG_PHILOX_H__\n#define __RNG_PHILOX_H__\n\n#include <cmath>\n#include <vector>\n\n#include \"rng.hpp\"\n\n// RNG imitiating torch cuda randn on CPU.\n// Port from: https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/5ef669de080814067961f28357256e8fe27544f4/modules/rng_philox.py\nclass PhiloxRNG : public RNG {\nprivate:\n    uint64_t seed;\n    uint32_t offset;\n\nprivate:\n    std::vector<uint32_t> philox_m = {0xD2511F53, 0xCD9E8D57};\n    std::vector<uint32_t> philox_w = {0x9E3779B9, 0xBB67AE85};\n    float two_pow32_inv            = 2.3283064e-10f;\n    float two_pow32_inv_2pi        = 2.3283064e-10f * 6.2831855f;\n\n    std::vector<uint32_t> uint32(uint64_t x) {\n        std::vector<uint32_t> result(2);\n        result[0] = static_cast<uint32_t>(x & 0xFFFFFFFF);\n        result[1] = static_cast<uint32_t>(x >> 32);\n        return result;\n    }\n\n    std::vector<std::vector<uint32_t>> uint32(const std::vector<uint64_t>& x) {\n        uint32_t N = (uint32_t)x.size();\n        std::vector<std::vector<uint32_t>> result(2, std::vector<uint32_t>(N));\n\n        for (uint32_t i = 0; i < N; ++i) {\n            result[0][i] = static_cast<uint32_t>(x[i] & 0xFFFFFFFF);\n            result[1][i] = static_cast<uint32_t>(x[i] >> 32);\n        }\n\n        return result;\n    }\n\n    //  A single round of the Philox 4x32 random number generator.\n    void philox4_round(std::vector<std::vector<uint32_t>>& counter,\n                       const std::vector<std::vector<uint32_t>>& key) {\n        uint32_t N = (uint32_t)counter[0].size();\n        for (uint32_t i = 0; i < N; i++) {\n            std::vector<uint32_t> v1 = uint32(static_cast<uint64_t>(counter[0][i]) * static_cast<uint64_t>(philox_m[0]));\n            std::vector<uint32_t> v2 = uint32(static_cast<uint64_t>(counter[2][i]) * static_cast<uint64_t>(philox_m[1]));\n\n            counter[0][i] = v2[1] ^ counter[1][i] ^ key[0][i];\n            counter[1][i] = v2[0];\n            counter[2][i] = v1[1] ^ counter[3][i] ^ key[1][i];\n            counter[3][i] = v1[0];\n        }\n    }\n\n    // Generates 32-bit random numbers using the Philox 4x32 random number generator.\n    // Parameters:\n    //     counter : A 4xN array of 32-bit integers representing the counter values (offset into generation).\n    //     key : A 2xN array of 32-bit integers representing the key values (seed).\n    //     rounds : The number of rounds to perform.\n    // Returns:\n    //     std::vector<std::vector<uint32_t>>: A 4xN array of 32-bit integers containing the generated random numbers.\n    std::vector<std::vector<uint32_t>> philox4_32(std::vector<std::vector<uint32_t>>& counter,\n                                                  std::vector<std::vector<uint32_t>>& key,\n                                                  int rounds = 10) {\n        uint32_t N = (uint32_t)counter[0].size();\n        for (int i = 0; i < rounds - 1; ++i) {\n            philox4_round(counter, key);\n\n            for (uint32_t j = 0; j < N; ++j) {\n                key[0][j] += philox_w[0];\n                key[1][j] += philox_w[1];\n            }\n        }\n\n        philox4_round(counter, key);\n        return counter;\n    }\n\n    float box_muller(float x, float y) {\n        float u = x * two_pow32_inv + two_pow32_inv / 2;\n        float v = y * two_pow32_inv_2pi + two_pow32_inv_2pi / 2;\n\n        float s = sqrt(-2.0f * log(u));\n\n        float r1 = s * sin(v);\n        return r1;\n    }\n\npublic:\n    PhiloxRNG(uint64_t seed = 0) {\n        this->seed   = seed;\n        this->offset = 0;\n    }\n\n    void manual_seed(uint64_t seed) {\n        this->seed   = seed;\n        this->offset = 0;\n    }\n\n    std::vector<float> randn(uint32_t n) {\n        std::vector<std::vector<uint32_t>> counter(4, std::vector<uint32_t>(n, 0));\n        for (uint32_t i = 0; i < n; i++) {\n            counter[0][i] = this->offset;\n        }\n\n        for (uint32_t i = 0; i < n; i++) {\n            counter[2][i] = i;\n        }\n        this->offset += 1;\n\n        std::vector<uint64_t> key(n, this->seed);\n        std::vector<std::vector<uint32_t>> key_uint32 = uint32(key);\n\n        std::vector<std::vector<uint32_t>> g = philox4_32(counter, key_uint32);\n\n        std::vector<float> result;\n        for (uint32_t i = 0; i < n; ++i) {\n            result.push_back(box_muller((float)g[0][i], (float)g[1][i]));\n        }\n        return result;\n    }\n};\n\n#endif  // __RNG_PHILOX_H__"
        },
        {
          "name": "stable-diffusion.cpp",
          "type": "blob",
          "size": 83.39453125,
          "content": "#include \"ggml_extend.hpp\"\n\n#include \"model.h\"\n#include \"rng.hpp\"\n#include \"rng_philox.hpp\"\n#include \"stable-diffusion.h\"\n#include \"util.h\"\n\n#include \"conditioner.hpp\"\n#include \"control.hpp\"\n#include \"denoiser.hpp\"\n#include \"diffusion_model.hpp\"\n#include \"esrgan.hpp\"\n#include \"lora.hpp\"\n#include \"pmid.hpp\"\n#include \"tae.hpp\"\n#include \"vae.hpp\"\n\n#define STB_IMAGE_IMPLEMENTATION\n#define STB_IMAGE_STATIC\n#include \"stb_image.h\"\n\n// #define STB_IMAGE_WRITE_IMPLEMENTATION\n// #define STB_IMAGE_WRITE_STATIC\n// #include \"stb_image_write.h\"\n\nconst char* model_version_to_str[] = {\n    \"SD 1.x\",\n    \"SD 1.x Inpaint\",\n    \"SD 2.x\",\n    \"SD 2.x Inpaint\",\n    \"SDXL\",\n    \"SDXL Inpaint\",\n    \"SVD\",\n    \"SD3.x\",\n    \"Flux\",\n    \"Flux Fill\"};\n\nconst char* sampling_methods_str[] = {\n    \"Euler A\",\n    \"Euler\",\n    \"Heun\",\n    \"DPM2\",\n    \"DPM++ (2s)\",\n    \"DPM++ (2M)\",\n    \"modified DPM++ (2M)\",\n    \"iPNDM\",\n    \"iPNDM_v\",\n    \"LCM\",\n};\n\n/*================================================== Helper Functions ================================================*/\n\nvoid calculate_alphas_cumprod(float* alphas_cumprod,\n                              float linear_start = 0.00085f,\n                              float linear_end   = 0.0120,\n                              int timesteps      = TIMESTEPS) {\n    float ls_sqrt = sqrtf(linear_start);\n    float le_sqrt = sqrtf(linear_end);\n    float amount  = le_sqrt - ls_sqrt;\n    float product = 1.0f;\n    for (int i = 0; i < timesteps; i++) {\n        float beta = ls_sqrt + amount * ((float)i / (timesteps - 1));\n        product *= 1.0f - powf(beta, 2.0f);\n        alphas_cumprod[i] = product;\n    }\n}\n\n/*=============================================== StableDiffusionGGML ================================================*/\n\nclass StableDiffusionGGML {\npublic:\n    ggml_backend_t backend             = NULL;  // general backend\n    ggml_backend_t clip_backend        = NULL;\n    ggml_backend_t control_net_backend = NULL;\n    ggml_backend_t vae_backend         = NULL;\n    ggml_type model_wtype              = GGML_TYPE_COUNT;\n    ggml_type conditioner_wtype        = GGML_TYPE_COUNT;\n    ggml_type diffusion_model_wtype    = GGML_TYPE_COUNT;\n    ggml_type vae_wtype                = GGML_TYPE_COUNT;\n\n    SDVersion version;\n    bool vae_decode_only         = false;\n    bool free_params_immediately = false;\n\n    std::shared_ptr<RNG> rng = std::make_shared<STDDefaultRNG>();\n    int n_threads            = -1;\n    float scale_factor       = 0.18215f;\n\n    std::shared_ptr<Conditioner> cond_stage_model;\n    std::shared_ptr<FrozenCLIPVisionEmbedder> clip_vision;  // for svd\n    std::shared_ptr<DiffusionModel> diffusion_model;\n    std::shared_ptr<AutoEncoderKL> first_stage_model;\n    std::shared_ptr<TinyAutoEncoder> tae_first_stage;\n    std::shared_ptr<ControlNet> control_net;\n    std::shared_ptr<PhotoMakerIDEncoder> pmid_model;\n    std::shared_ptr<LoraModel> pmid_lora;\n    std::shared_ptr<PhotoMakerIDEmbed> pmid_id_embeds;\n\n    std::string taesd_path;\n    bool use_tiny_autoencoder = false;\n    bool vae_tiling           = false;\n    bool stacked_id           = false;\n\n    std::map<std::string, struct ggml_tensor*> tensors;\n\n    std::string lora_model_dir;\n    // lora_name => multiplier\n    std::unordered_map<std::string, float> curr_lora_state;\n\n    std::shared_ptr<Denoiser> denoiser = std::make_shared<CompVisDenoiser>();\n\n    StableDiffusionGGML() = default;\n\n    StableDiffusionGGML(int n_threads,\n                        bool vae_decode_only,\n                        bool free_params_immediately,\n                        std::string lora_model_dir,\n                        rng_type_t rng_type)\n        : n_threads(n_threads),\n          vae_decode_only(vae_decode_only),\n          free_params_immediately(free_params_immediately),\n          lora_model_dir(lora_model_dir) {\n        if (rng_type == STD_DEFAULT_RNG) {\n            rng = std::make_shared<STDDefaultRNG>();\n        } else if (rng_type == CUDA_RNG) {\n            rng = std::make_shared<PhiloxRNG>();\n        }\n    }\n\n    ~StableDiffusionGGML() {\n        if (clip_backend != backend) {\n            ggml_backend_free(clip_backend);\n        }\n        if (control_net_backend != backend) {\n            ggml_backend_free(control_net_backend);\n        }\n        if (vae_backend != backend) {\n            ggml_backend_free(vae_backend);\n        }\n        ggml_backend_free(backend);\n    }\n\n    bool load_from_file(const std::string& model_path,\n                        const std::string& clip_l_path,\n                        const std::string& clip_g_path,\n                        const std::string& t5xxl_path,\n                        const std::string& diffusion_model_path,\n                        const std::string& vae_path,\n                        const std::string control_net_path,\n                        const std::string embeddings_path,\n                        const std::string id_embeddings_path,\n                        const std::string& taesd_path,\n                        bool vae_tiling_,\n                        ggml_type wtype,\n                        schedule_t schedule,\n                        bool clip_on_cpu,\n                        bool control_net_cpu,\n                        bool vae_on_cpu,\n                        bool diffusion_flash_attn) {\n        use_tiny_autoencoder = taesd_path.size() > 0;\n#ifdef SD_USE_CUDA\n        LOG_DEBUG(\"Using CUDA backend\");\n        backend = ggml_backend_cuda_init(0);\n#endif\n#ifdef SD_USE_METAL\n        LOG_DEBUG(\"Using Metal backend\");\n        ggml_log_set(ggml_log_callback_default, nullptr);\n        backend = ggml_backend_metal_init();\n#endif\n#ifdef SD_USE_VULKAN\n        LOG_DEBUG(\"Using Vulkan backend\");\n        for (int device = 0; device < ggml_backend_vk_get_device_count(); ++device) {\n            backend = ggml_backend_vk_init(device);\n        }\n        if (!backend) {\n            LOG_WARN(\"Failed to initialize Vulkan backend\");\n        }\n#endif\n#ifdef SD_USE_SYCL\n        LOG_DEBUG(\"Using SYCL backend\");\n        backend = ggml_backend_sycl_init(0);\n#endif\n\n        if (!backend) {\n            LOG_DEBUG(\"Using CPU backend\");\n            backend = ggml_backend_cpu_init();\n        }\n\n        ModelLoader model_loader;\n\n        vae_tiling = vae_tiling_;\n\n        if (model_path.size() > 0) {\n            LOG_INFO(\"loading model from '%s'\", model_path.c_str());\n            if (!model_loader.init_from_file(model_path)) {\n                LOG_ERROR(\"init model loader from file failed: '%s'\", model_path.c_str());\n            }\n        }\n\n        if (clip_l_path.size() > 0) {\n            LOG_INFO(\"loading clip_l from '%s'\", clip_l_path.c_str());\n            if (!model_loader.init_from_file(clip_l_path, \"text_encoders.clip_l.transformer.\")) {\n                LOG_WARN(\"loading clip_l from '%s' failed\", clip_l_path.c_str());\n            }\n        }\n\n        if (clip_g_path.size() > 0) {\n            LOG_INFO(\"loading clip_g from '%s'\", clip_g_path.c_str());\n            if (!model_loader.init_from_file(clip_g_path, \"text_encoders.clip_g.transformer.\")) {\n                LOG_WARN(\"loading clip_g from '%s' failed\", clip_g_path.c_str());\n            }\n        }\n\n        if (t5xxl_path.size() > 0) {\n            LOG_INFO(\"loading t5xxl from '%s'\", t5xxl_path.c_str());\n            if (!model_loader.init_from_file(t5xxl_path, \"text_encoders.t5xxl.transformer.\")) {\n                LOG_WARN(\"loading t5xxl from '%s' failed\", t5xxl_path.c_str());\n            }\n        }\n\n        if (diffusion_model_path.size() > 0) {\n            LOG_INFO(\"loading diffusion model from '%s'\", diffusion_model_path.c_str());\n            if (!model_loader.init_from_file(diffusion_model_path, \"model.diffusion_model.\")) {\n                LOG_WARN(\"loading diffusion model from '%s' failed\", diffusion_model_path.c_str());\n            }\n        }\n\n        if (vae_path.size() > 0) {\n            LOG_INFO(\"loading vae from '%s'\", vae_path.c_str());\n            if (!model_loader.init_from_file(vae_path, \"vae.\")) {\n                LOG_WARN(\"loading vae from '%s' failed\", vae_path.c_str());\n            }\n        }\n\n        version = model_loader.get_sd_version();\n        if (version == VERSION_COUNT) {\n            LOG_ERROR(\"get sd version from file failed: '%s'\", model_path.c_str());\n            return false;\n        }\n\n        LOG_INFO(\"Version: %s \", model_version_to_str[version]);\n        if (wtype == GGML_TYPE_COUNT) {\n            model_wtype = model_loader.get_sd_wtype();\n            if (model_wtype == GGML_TYPE_COUNT) {\n                model_wtype = GGML_TYPE_F32;\n                LOG_WARN(\"can not get mode wtype frome weight, use f32\");\n            }\n            conditioner_wtype = model_loader.get_conditioner_wtype();\n            if (conditioner_wtype == GGML_TYPE_COUNT) {\n                conditioner_wtype = wtype;\n            }\n            diffusion_model_wtype = model_loader.get_diffusion_model_wtype();\n            if (diffusion_model_wtype == GGML_TYPE_COUNT) {\n                diffusion_model_wtype = wtype;\n            }\n            vae_wtype = model_loader.get_vae_wtype();\n\n            if (vae_wtype == GGML_TYPE_COUNT) {\n                vae_wtype = wtype;\n            }\n        } else {\n            model_wtype           = wtype;\n            conditioner_wtype     = wtype;\n            diffusion_model_wtype = wtype;\n            vae_wtype             = wtype;\n            model_loader.set_wtype_override(wtype);\n        }\n\n        if (sd_version_is_sdxl(version)) {\n            vae_wtype = GGML_TYPE_F32;\n            model_loader.set_wtype_override(GGML_TYPE_F32, \"vae.\");\n        }\n\n        LOG_INFO(\"Weight type:                 %s\", model_wtype != SD_TYPE_COUNT ? ggml_type_name(model_wtype) : \"??\");\n        LOG_INFO(\"Conditioner weight type:     %s\", conditioner_wtype != SD_TYPE_COUNT ? ggml_type_name(conditioner_wtype) : \"??\");\n        LOG_INFO(\"Diffusion model weight type: %s\", diffusion_model_wtype != SD_TYPE_COUNT ? ggml_type_name(diffusion_model_wtype) : \"??\");\n        LOG_INFO(\"VAE weight type:             %s\", vae_wtype != SD_TYPE_COUNT ? ggml_type_name(vae_wtype) : \"??\");\n\n        LOG_DEBUG(\"ggml tensor size = %d bytes\", (int)sizeof(ggml_tensor));\n\n        if (sd_version_is_sdxl(version)) {\n            scale_factor = 0.13025f;\n            if (vae_path.size() == 0 && taesd_path.size() == 0) {\n                LOG_WARN(\n                    \"!!!It looks like you are using SDXL model. \"\n                    \"If you find that the generated images are completely black, \"\n                    \"try specifying SDXL VAE FP16 Fix with the --vae parameter. \"\n                    \"You can find it here: https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/blob/main/sdxl_vae.safetensors\");\n            }\n        } else if (sd_version_is_sd3(version)) {\n            scale_factor = 1.5305f;\n        } else if (sd_version_is_flux(version)) {\n            scale_factor = 0.3611;\n            // TODO: shift_factor\n        }\n\n        if (version == VERSION_SVD) {\n            clip_vision = std::make_shared<FrozenCLIPVisionEmbedder>(backend, model_loader.tensor_storages_types);\n            clip_vision->alloc_params_buffer();\n            clip_vision->get_param_tensors(tensors);\n\n            diffusion_model = std::make_shared<UNetModel>(backend, model_loader.tensor_storages_types, version);\n            diffusion_model->alloc_params_buffer();\n            diffusion_model->get_param_tensors(tensors);\n\n            first_stage_model = std::make_shared<AutoEncoderKL>(backend, model_loader.tensor_storages_types, \"first_stage_model\", vae_decode_only, true, version);\n            LOG_DEBUG(\"vae_decode_only %d\", vae_decode_only);\n            first_stage_model->alloc_params_buffer();\n            first_stage_model->get_param_tensors(tensors, \"first_stage_model\");\n        } else {\n            clip_backend   = backend;\n            bool use_t5xxl = false;\n            if (sd_version_is_dit(version)) {\n                use_t5xxl = true;\n            }\n            if (!ggml_backend_is_cpu(backend) && use_t5xxl && conditioner_wtype != GGML_TYPE_F32) {\n                clip_on_cpu = true;\n                LOG_INFO(\"set clip_on_cpu to true\");\n            }\n            if (clip_on_cpu && !ggml_backend_is_cpu(backend)) {\n                LOG_INFO(\"CLIP: Using CPU backend\");\n                clip_backend = ggml_backend_cpu_init();\n            }\n            if (diffusion_flash_attn) {\n                LOG_INFO(\"Using flash attention in the diffusion model\");\n            }\n            if (sd_version_is_sd3(version)) {\n                if (diffusion_flash_attn) {\n                    LOG_WARN(\"flash attention in this diffusion model is currently unsupported!\");\n                }\n                cond_stage_model = std::make_shared<SD3CLIPEmbedder>(clip_backend, model_loader.tensor_storages_types);\n                diffusion_model  = std::make_shared<MMDiTModel>(backend, model_loader.tensor_storages_types);\n            } else if (sd_version_is_flux(version)) {\n                cond_stage_model = std::make_shared<FluxCLIPEmbedder>(clip_backend, model_loader.tensor_storages_types);\n                diffusion_model  = std::make_shared<FluxModel>(backend, model_loader.tensor_storages_types, version, diffusion_flash_attn);\n            } else {\n                if (id_embeddings_path.find(\"v2\") != std::string::npos) {\n                    cond_stage_model = std::make_shared<FrozenCLIPEmbedderWithCustomWords>(clip_backend, model_loader.tensor_storages_types, embeddings_path, version, PM_VERSION_2);\n                } else {\n                    cond_stage_model = std::make_shared<FrozenCLIPEmbedderWithCustomWords>(clip_backend, model_loader.tensor_storages_types, embeddings_path, version);\n                }\n                diffusion_model = std::make_shared<UNetModel>(backend, model_loader.tensor_storages_types, version, diffusion_flash_attn);\n            }\n\n            cond_stage_model->alloc_params_buffer();\n            cond_stage_model->get_param_tensors(tensors);\n\n            diffusion_model->alloc_params_buffer();\n            diffusion_model->get_param_tensors(tensors);\n\n            if (!use_tiny_autoencoder) {\n                if (vae_on_cpu && !ggml_backend_is_cpu(backend)) {\n                    LOG_INFO(\"VAE Autoencoder: Using CPU backend\");\n                    vae_backend = ggml_backend_cpu_init();\n                } else {\n                    vae_backend = backend;\n                }\n                first_stage_model = std::make_shared<AutoEncoderKL>(vae_backend, model_loader.tensor_storages_types, \"first_stage_model\", vae_decode_only, false, version);\n                first_stage_model->alloc_params_buffer();\n                first_stage_model->get_param_tensors(tensors, \"first_stage_model\");\n            } else {\n                tae_first_stage = std::make_shared<TinyAutoEncoder>(backend, model_loader.tensor_storages_types, \"decoder.layers\", vae_decode_only, version);\n            }\n            // first_stage_model->get_param_tensors(tensors, \"first_stage_model.\");\n\n            if (control_net_path.size() > 0) {\n                ggml_backend_t controlnet_backend = NULL;\n                if (control_net_cpu && !ggml_backend_is_cpu(backend)) {\n                    LOG_DEBUG(\"ControlNet: Using CPU backend\");\n                    controlnet_backend = ggml_backend_cpu_init();\n                } else {\n                    controlnet_backend = backend;\n                }\n                control_net = std::make_shared<ControlNet>(controlnet_backend, model_loader.tensor_storages_types, version);\n            }\n\n            if (id_embeddings_path.find(\"v2\") != std::string::npos) {\n                pmid_model = std::make_shared<PhotoMakerIDEncoder>(backend, model_loader.tensor_storages_types, \"pmid\", version, PM_VERSION_2);\n                LOG_INFO(\"using PhotoMaker Version 2\");\n            } else {\n                pmid_model = std::make_shared<PhotoMakerIDEncoder>(backend, model_loader.tensor_storages_types, \"pmid\", version);\n            }\n            if (id_embeddings_path.size() > 0) {\n                pmid_lora = std::make_shared<LoraModel>(backend, id_embeddings_path, \"\");\n                if (!pmid_lora->load_from_file(true)) {\n                    LOG_WARN(\"load photomaker lora tensors from %s failed\", id_embeddings_path.c_str());\n                    return false;\n                }\n                LOG_INFO(\"loading stacked ID embedding (PHOTOMAKER) model file from '%s'\", id_embeddings_path.c_str());\n                if (!model_loader.init_from_file(id_embeddings_path, \"pmid.\")) {\n                    LOG_WARN(\"loading stacked ID embedding from '%s' failed\", id_embeddings_path.c_str());\n                } else {\n                    stacked_id = true;\n                }\n            }\n            if (stacked_id) {\n                if (!pmid_model->alloc_params_buffer()) {\n                    LOG_ERROR(\" pmid model params buffer allocation failed\");\n                    return false;\n                }\n                pmid_model->get_param_tensors(tensors, \"pmid\");\n            }\n        }\n\n        struct ggml_init_params params;\n        params.mem_size   = static_cast<size_t>(10 * 1024) * 1024;  // 10M\n        params.mem_buffer = NULL;\n        params.no_alloc   = false;\n        // LOG_DEBUG(\"mem_size %u \", params.mem_size);\n        struct ggml_context* ctx = ggml_init(params);  // for  alphas_cumprod and is_using_v_parameterization check\n        GGML_ASSERT(ctx != NULL);\n        ggml_tensor* alphas_cumprod_tensor = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, TIMESTEPS);\n        calculate_alphas_cumprod((float*)alphas_cumprod_tensor->data);\n\n        // load weights\n        LOG_DEBUG(\"loading weights\");\n\n        int64_t t0 = ggml_time_ms();\n\n        std::set<std::string> ignore_tensors;\n        tensors[\"alphas_cumprod\"] = alphas_cumprod_tensor;\n        if (use_tiny_autoencoder) {\n            ignore_tensors.insert(\"first_stage_model.\");\n        }\n        if (stacked_id) {\n            ignore_tensors.insert(\"lora.\");\n        }\n\n        if (vae_decode_only) {\n            ignore_tensors.insert(\"first_stage_model.encoder\");\n            ignore_tensors.insert(\"first_stage_model.quant\");\n        }\n        if (version == VERSION_SVD) {\n            ignore_tensors.insert(\"conditioner.embedders.3\");\n        }\n        bool success = model_loader.load_tensors(tensors, backend, ignore_tensors);\n        if (!success) {\n            LOG_ERROR(\"load tensors from model loader failed\");\n            ggml_free(ctx);\n            return false;\n        }\n\n        // LOG_DEBUG(\"model size = %.2fMB\", total_size / 1024.0 / 1024.0);\n\n        if (version == VERSION_SVD) {\n            // diffusion_model->test();\n            // first_stage_model->test();\n            // return false;\n        } else {\n            size_t clip_params_mem_size = cond_stage_model->get_params_buffer_size();\n            size_t unet_params_mem_size = diffusion_model->get_params_buffer_size();\n            size_t vae_params_mem_size  = 0;\n            if (!use_tiny_autoencoder) {\n                vae_params_mem_size = first_stage_model->get_params_buffer_size();\n            } else {\n                if (!tae_first_stage->load_from_file(taesd_path)) {\n                    return false;\n                }\n                vae_params_mem_size = tae_first_stage->get_params_buffer_size();\n            }\n            size_t control_net_params_mem_size = 0;\n            if (control_net) {\n                if (!control_net->load_from_file(control_net_path)) {\n                    return false;\n                }\n                control_net_params_mem_size = control_net->get_params_buffer_size();\n            }\n            size_t pmid_params_mem_size = 0;\n            if (stacked_id) {\n                pmid_params_mem_size = pmid_model->get_params_buffer_size();\n            }\n\n            size_t total_params_ram_size  = 0;\n            size_t total_params_vram_size = 0;\n            if (ggml_backend_is_cpu(clip_backend)) {\n                total_params_ram_size += clip_params_mem_size + pmid_params_mem_size;\n            } else {\n                total_params_vram_size += clip_params_mem_size + pmid_params_mem_size;\n            }\n\n            if (ggml_backend_is_cpu(backend)) {\n                total_params_ram_size += unet_params_mem_size;\n            } else {\n                total_params_vram_size += unet_params_mem_size;\n            }\n\n            if (ggml_backend_is_cpu(vae_backend)) {\n                total_params_ram_size += vae_params_mem_size;\n            } else {\n                total_params_vram_size += vae_params_mem_size;\n            }\n\n            if (ggml_backend_is_cpu(control_net_backend)) {\n                total_params_ram_size += control_net_params_mem_size;\n            } else {\n                total_params_vram_size += control_net_params_mem_size;\n            }\n\n            size_t total_params_size = total_params_ram_size + total_params_vram_size;\n            LOG_INFO(\n                \"total params memory size = %.2fMB (VRAM %.2fMB, RAM %.2fMB): \"\n                \"clip %.2fMB(%s), unet %.2fMB(%s), vae %.2fMB(%s), controlnet %.2fMB(%s), pmid %.2fMB(%s)\",\n                total_params_size / 1024.0 / 1024.0,\n                total_params_vram_size / 1024.0 / 1024.0,\n                total_params_ram_size / 1024.0 / 1024.0,\n                clip_params_mem_size / 1024.0 / 1024.0,\n                ggml_backend_is_cpu(clip_backend) ? \"RAM\" : \"VRAM\",\n                unet_params_mem_size / 1024.0 / 1024.0,\n                ggml_backend_is_cpu(backend) ? \"RAM\" : \"VRAM\",\n                vae_params_mem_size / 1024.0 / 1024.0,\n                ggml_backend_is_cpu(vae_backend) ? \"RAM\" : \"VRAM\",\n                control_net_params_mem_size / 1024.0 / 1024.0,\n                ggml_backend_is_cpu(control_net_backend) ? \"RAM\" : \"VRAM\",\n                pmid_params_mem_size / 1024.0 / 1024.0,\n                ggml_backend_is_cpu(clip_backend) ? \"RAM\" : \"VRAM\");\n        }\n\n        int64_t t1 = ggml_time_ms();\n        LOG_INFO(\"loading model from '%s' completed, taking %.2fs\", model_path.c_str(), (t1 - t0) * 1.0f / 1000);\n\n        // check is_using_v_parameterization_for_sd2\n        bool is_using_v_parameterization = false;\n        if (sd_version_is_sd2(version)) {\n            if (is_using_v_parameterization_for_sd2(ctx, sd_version_is_inpaint(version))) {\n                is_using_v_parameterization = true;\n            }\n        } else if (version == VERSION_SVD) {\n            // TODO: V_PREDICTION_EDM\n            is_using_v_parameterization = true;\n        }\n\n        if (sd_version_is_sd3(version)) {\n            LOG_INFO(\"running in FLOW mode\");\n            denoiser = std::make_shared<DiscreteFlowDenoiser>();\n        } else if (sd_version_is_flux(version)) {\n            LOG_INFO(\"running in Flux FLOW mode\");\n            float shift = 1.0f;  // TODO: validate\n            for (auto pair : model_loader.tensor_storages_types) {\n                if (pair.first.find(\"model.diffusion_model.guidance_in.in_layer.weight\") != std::string::npos) {\n                    shift = 1.15f;\n                    break;\n                }\n            }\n            denoiser = std::make_shared<FluxFlowDenoiser>(shift);\n        } else if (is_using_v_parameterization) {\n            LOG_INFO(\"running in v-prediction mode\");\n            denoiser = std::make_shared<CompVisVDenoiser>();\n        } else {\n            LOG_INFO(\"running in eps-prediction mode\");\n        }\n\n        if (schedule != DEFAULT) {\n            switch (schedule) {\n                case DISCRETE:\n                    LOG_INFO(\"running with discrete schedule\");\n                    denoiser->schedule = std::make_shared<DiscreteSchedule>();\n                    break;\n                case KARRAS:\n                    LOG_INFO(\"running with Karras schedule\");\n                    denoiser->schedule = std::make_shared<KarrasSchedule>();\n                    break;\n                case EXPONENTIAL:\n                    LOG_INFO(\"running exponential schedule\");\n                    denoiser->schedule = std::make_shared<ExponentialSchedule>();\n                    break;\n                case AYS:\n                    LOG_INFO(\"Running with Align-Your-Steps schedule\");\n                    denoiser->schedule          = std::make_shared<AYSSchedule>();\n                    denoiser->schedule->version = version;\n                    break;\n                case GITS:\n                    LOG_INFO(\"Running with GITS schedule\");\n                    denoiser->schedule          = std::make_shared<GITSSchedule>();\n                    denoiser->schedule->version = version;\n                    break;\n                case DEFAULT:\n                    // Don't touch anything.\n                    break;\n                default:\n                    LOG_ERROR(\"Unknown schedule %i\", schedule);\n                    abort();\n            }\n        }\n\n        auto comp_vis_denoiser = std::dynamic_pointer_cast<CompVisDenoiser>(denoiser);\n        if (comp_vis_denoiser) {\n            for (int i = 0; i < TIMESTEPS; i++) {\n                comp_vis_denoiser->sigmas[i]     = std::sqrt((1 - ((float*)alphas_cumprod_tensor->data)[i]) / ((float*)alphas_cumprod_tensor->data)[i]);\n                comp_vis_denoiser->log_sigmas[i] = std::log(comp_vis_denoiser->sigmas[i]);\n            }\n        }\n\n        LOG_DEBUG(\"finished loaded file\");\n        ggml_free(ctx);\n        return true;\n    }\n\n    bool is_using_v_parameterization_for_sd2(ggml_context* work_ctx, bool is_inpaint = false) {\n        struct ggml_tensor* x_t = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 8, 8, 4, 1);\n        ggml_set_f32(x_t, 0.5);\n        struct ggml_tensor* c = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 1024, 2, 1, 1);\n        ggml_set_f32(c, 0.5);\n\n        struct ggml_tensor* timesteps = ggml_new_tensor_1d(work_ctx, GGML_TYPE_F32, 1);\n        ggml_set_f32(timesteps, 999);\n\n        struct ggml_tensor* concat = is_inpaint ? ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 8, 8, 5, 1) : NULL;\n        ggml_set_f32(concat, 0);\n\n        int64_t t0              = ggml_time_ms();\n        struct ggml_tensor* out = ggml_dup_tensor(work_ctx, x_t);\n        diffusion_model->compute(n_threads, x_t, timesteps, c, concat, NULL, NULL, -1, {}, 0.f, &out);\n        diffusion_model->free_compute_buffer();\n\n        double result = 0.f;\n        {\n            float* vec_x   = (float*)x_t->data;\n            float* vec_out = (float*)out->data;\n\n            int64_t n = ggml_nelements(out);\n\n            for (int i = 0; i < n; i++) {\n                result += ((double)vec_out[i] - (double)vec_x[i]);\n            }\n            result /= n;\n        }\n        int64_t t1 = ggml_time_ms();\n        LOG_DEBUG(\"check is_using_v_parameterization_for_sd2, taking %.2fs\", (t1 - t0) * 1.0f / 1000);\n        return result < -1;\n    }\n\n    void apply_lora(const std::string& lora_name, float multiplier) {\n        int64_t t0                 = ggml_time_ms();\n        std::string st_file_path   = path_join(lora_model_dir, lora_name + \".safetensors\");\n        std::string ckpt_file_path = path_join(lora_model_dir, lora_name + \".ckpt\");\n        std::string file_path;\n        if (file_exists(st_file_path)) {\n            file_path = st_file_path;\n        } else if (file_exists(ckpt_file_path)) {\n            file_path = ckpt_file_path;\n        } else {\n            LOG_WARN(\"can not find %s or %s for lora %s\", st_file_path.c_str(), ckpt_file_path.c_str(), lora_name.c_str());\n            return;\n        }\n        LoraModel lora(backend, file_path);\n        if (!lora.load_from_file()) {\n            LOG_WARN(\"load lora tensors from %s failed\", file_path.c_str());\n            return;\n        }\n\n        lora.multiplier = multiplier;\n        // TODO: send version?\n        lora.apply(tensors, version, n_threads);\n        lora.free_params_buffer();\n\n        int64_t t1 = ggml_time_ms();\n\n        LOG_INFO(\"lora '%s' applied, taking %.2fs\", lora_name.c_str(), (t1 - t0) * 1.0f / 1000);\n    }\n\n    void apply_loras(const std::unordered_map<std::string, float>& lora_state) {\n        if (lora_state.size() > 0 && model_wtype != GGML_TYPE_F16 && model_wtype != GGML_TYPE_F32) {\n            LOG_WARN(\"In quantized models when applying LoRA, the images have poor quality.\");\n        }\n        std::unordered_map<std::string, float> lora_state_diff;\n        for (auto& kv : lora_state) {\n            const std::string& lora_name = kv.first;\n            float multiplier             = kv.second;\n\n            if (curr_lora_state.find(lora_name) != curr_lora_state.end()) {\n                float curr_multiplier = curr_lora_state[lora_name];\n                float multiplier_diff = multiplier - curr_multiplier;\n                if (multiplier_diff != 0.f) {\n                    lora_state_diff[lora_name] = multiplier_diff;\n                }\n            } else {\n                lora_state_diff[lora_name] = multiplier;\n            }\n        }\n\n        LOG_INFO(\"Attempting to apply %lu LoRAs\", lora_state.size());\n\n        for (auto& kv : lora_state_diff) {\n            apply_lora(kv.first, kv.second);\n        }\n\n        curr_lora_state = lora_state;\n    }\n\n    ggml_tensor* id_encoder(ggml_context* work_ctx,\n                            ggml_tensor* init_img,\n                            ggml_tensor* prompts_embeds,\n                            ggml_tensor* id_embeds,\n                            std::vector<bool>& class_tokens_mask) {\n        ggml_tensor* res = NULL;\n        pmid_model->compute(n_threads, init_img, prompts_embeds, id_embeds, class_tokens_mask, &res, work_ctx);\n        return res;\n    }\n\n    SDCondition get_svd_condition(ggml_context* work_ctx,\n                                  sd_image_t init_image,\n                                  int width,\n                                  int height,\n                                  int fps                    = 6,\n                                  int motion_bucket_id       = 127,\n                                  float augmentation_level   = 0.f,\n                                  bool force_zero_embeddings = false) {\n        // c_crossattn\n        int64_t t0                      = ggml_time_ms();\n        struct ggml_tensor* c_crossattn = NULL;\n        {\n            if (force_zero_embeddings) {\n                c_crossattn = ggml_new_tensor_1d(work_ctx, GGML_TYPE_F32, clip_vision->vision_model.projection_dim);\n                ggml_set_f32(c_crossattn, 0.f);\n            } else {\n                sd_image_f32_t image         = sd_image_t_to_sd_image_f32_t(init_image);\n                sd_image_f32_t resized_image = clip_preprocess(image, clip_vision->vision_model.image_size);\n                free(image.data);\n                image.data = NULL;\n\n                ggml_tensor* pixel_values = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, resized_image.width, resized_image.height, 3, 1);\n                sd_image_f32_to_tensor(resized_image.data, pixel_values, false);\n                free(resized_image.data);\n                resized_image.data = NULL;\n\n                // print_ggml_tensor(pixel_values);\n                clip_vision->compute(n_threads, pixel_values, &c_crossattn, work_ctx);\n                // print_ggml_tensor(c_crossattn);\n            }\n        }\n\n        // c_concat\n        struct ggml_tensor* c_concat = NULL;\n        {\n            if (force_zero_embeddings) {\n                c_concat = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, width / 8, height / 8, 4, 1);\n                ggml_set_f32(c_concat, 0.f);\n            } else {\n                ggml_tensor* init_img = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, width, height, 3, 1);\n\n                if (width != init_image.width || height != init_image.height) {\n                    sd_image_f32_t image         = sd_image_t_to_sd_image_f32_t(init_image);\n                    sd_image_f32_t resized_image = resize_sd_image_f32_t(image, width, height);\n                    free(image.data);\n                    image.data = NULL;\n                    sd_image_f32_to_tensor(resized_image.data, init_img, false);\n                    free(resized_image.data);\n                    resized_image.data = NULL;\n                } else {\n                    sd_image_to_tensor(init_image.data, init_img);\n                }\n                if (augmentation_level > 0.f) {\n                    struct ggml_tensor* noise = ggml_dup_tensor(work_ctx, init_img);\n                    ggml_tensor_set_f32_randn(noise, rng);\n                    // encode_pixels += torch.randn_like(pixels) * augmentation_level\n                    ggml_tensor_scale(noise, augmentation_level);\n                    ggml_tensor_add(init_img, noise);\n                }\n                ggml_tensor* moments = encode_first_stage(work_ctx, init_img);\n                c_concat             = get_first_stage_encoding(work_ctx, moments);\n            }\n        }\n\n        // y\n        struct ggml_tensor* y = NULL;\n        {\n            y                            = ggml_new_tensor_1d(work_ctx, GGML_TYPE_F32, diffusion_model->get_adm_in_channels());\n            int out_dim                  = 256;\n            int fps_id                   = fps - 1;\n            std::vector<float> timesteps = {(float)fps_id, (float)motion_bucket_id, augmentation_level};\n            set_timestep_embedding(timesteps, y, out_dim);\n        }\n        int64_t t1 = ggml_time_ms();\n        LOG_DEBUG(\"computing svd condition graph completed, taking %\" PRId64 \" ms\", t1 - t0);\n        return {c_crossattn, y, c_concat};\n    }\n\n    ggml_tensor* sample(ggml_context* work_ctx,\n                        ggml_tensor* init_latent,\n                        ggml_tensor* noise,\n                        SDCondition cond,\n                        SDCondition uncond,\n                        ggml_tensor* control_hint,\n                        float control_strength,\n                        float min_cfg,\n                        float cfg_scale,\n                        float guidance,\n                        sample_method_t method,\n                        const std::vector<float>& sigmas,\n                        int start_merge_step,\n                        SDCondition id_cond,\n                        std::vector<int> skip_layers = {},\n                        float slg_scale              = 0,\n                        float skip_layer_start       = 0.01,\n                        float skip_layer_end         = 0.2,\n                        ggml_tensor* noise_mask      = nullptr) {\n        LOG_DEBUG(\"Sample\");\n        struct ggml_init_params params;\n        size_t data_size = ggml_row_size(init_latent->type, init_latent->ne[0]);\n        for (int i = 1; i < 4; i++) {\n            data_size *= init_latent->ne[i];\n        }\n        data_size += 1024;\n        params.mem_size       = data_size * 3;\n        params.mem_buffer     = NULL;\n        params.no_alloc       = false;\n        ggml_context* tmp_ctx = ggml_init(params);\n\n        size_t steps = sigmas.size() - 1;\n        // noise = load_tensor_from_file(work_ctx, \"./rand0.bin\");\n        // print_ggml_tensor(noise);\n        struct ggml_tensor* x = ggml_dup_tensor(work_ctx, init_latent);\n        copy_ggml_tensor(x, init_latent);\n        x = denoiser->noise_scaling(sigmas[0], noise, x);\n\n        struct ggml_tensor* noised_input = ggml_dup_tensor(work_ctx, noise);\n\n        bool has_unconditioned = cfg_scale != 1.0 && uncond.c_crossattn != NULL;\n        bool has_skiplayer     = slg_scale != 0.0 && skip_layers.size() > 0;\n\n        // denoise wrapper\n        struct ggml_tensor* out_cond   = ggml_dup_tensor(work_ctx, x);\n        struct ggml_tensor* out_uncond = NULL;\n        struct ggml_tensor* out_skip   = NULL;\n\n        if (has_unconditioned) {\n            out_uncond = ggml_dup_tensor(work_ctx, x);\n        }\n        if (has_skiplayer) {\n            if (sd_version_is_dit(version)) {\n                out_skip = ggml_dup_tensor(work_ctx, x);\n            } else {\n                has_skiplayer = false;\n                LOG_WARN(\"SLG is incompatible with %s models\", model_version_to_str[version]);\n            }\n        }\n        struct ggml_tensor* denoised = ggml_dup_tensor(work_ctx, x);\n\n        auto denoise = [&](ggml_tensor* input, float sigma, int step) -> ggml_tensor* {\n            if (step == 1) {\n                pretty_progress(0, (int)steps, 0);\n            }\n            int64_t t0 = ggml_time_us();\n\n            std::vector<float> scaling = denoiser->get_scalings(sigma);\n            GGML_ASSERT(scaling.size() == 3);\n            float c_skip = scaling[0];\n            float c_out  = scaling[1];\n            float c_in   = scaling[2];\n\n            float t = denoiser->sigma_to_t(sigma);\n            std::vector<float> timesteps_vec(x->ne[3], t);  // [N, ]\n            auto timesteps = vector_to_ggml_tensor(work_ctx, timesteps_vec);\n            std::vector<float> guidance_vec(x->ne[3], guidance);\n            auto guidance_tensor = vector_to_ggml_tensor(work_ctx, guidance_vec);\n\n            copy_ggml_tensor(noised_input, input);\n            // noised_input = noised_input * c_in\n            ggml_tensor_scale(noised_input, c_in);\n\n            std::vector<struct ggml_tensor*> controls;\n\n            if (control_hint != NULL) {\n                control_net->compute(n_threads, noised_input, control_hint, timesteps, cond.c_crossattn, cond.c_vector);\n                controls = control_net->controls;\n                // print_ggml_tensor(controls[12]);\n                // GGML_ASSERT(0);\n            }\n\n            if (start_merge_step == -1 || step <= start_merge_step) {\n                // cond\n                diffusion_model->compute(n_threads,\n                                         noised_input,\n                                         timesteps,\n                                         cond.c_crossattn,\n                                         cond.c_concat,\n                                         cond.c_vector,\n                                         guidance_tensor,\n                                         -1,\n                                         controls,\n                                         control_strength,\n                                         &out_cond);\n            } else {\n                diffusion_model->compute(n_threads,\n                                         noised_input,\n                                         timesteps,\n                                         id_cond.c_crossattn,\n                                         cond.c_concat,\n                                         id_cond.c_vector,\n                                         guidance_tensor,\n                                         -1,\n                                         controls,\n                                         control_strength,\n                                         &out_cond);\n            }\n\n            float* negative_data = NULL;\n            if (has_unconditioned) {\n                // uncond\n                if (control_hint != NULL) {\n                    control_net->compute(n_threads, noised_input, control_hint, timesteps, uncond.c_crossattn, uncond.c_vector);\n                    controls = control_net->controls;\n                }\n                diffusion_model->compute(n_threads,\n                                         noised_input,\n                                         timesteps,\n                                         uncond.c_crossattn,\n                                         uncond.c_concat,\n                                         uncond.c_vector,\n                                         guidance_tensor,\n                                         -1,\n                                         controls,\n                                         control_strength,\n                                         &out_uncond);\n                negative_data = (float*)out_uncond->data;\n            }\n\n            int step_count         = sigmas.size();\n            bool is_skiplayer_step = has_skiplayer && step > (int)(skip_layer_start * step_count) && step < (int)(skip_layer_end * step_count);\n            float* skip_layer_data = NULL;\n            if (is_skiplayer_step) {\n                LOG_DEBUG(\"Skipping layers at step %d\\n\", step);\n                // skip layer (same as conditionned)\n                diffusion_model->compute(n_threads,\n                                         noised_input,\n                                         timesteps,\n                                         cond.c_crossattn,\n                                         cond.c_concat,\n                                         cond.c_vector,\n                                         guidance_tensor,\n                                         -1,\n                                         controls,\n                                         control_strength,\n                                         &out_skip,\n                                         NULL,\n                                         skip_layers);\n                skip_layer_data = (float*)out_skip->data;\n            }\n            float* vec_denoised  = (float*)denoised->data;\n            float* vec_input     = (float*)input->data;\n            float* positive_data = (float*)out_cond->data;\n            int ne_elements      = (int)ggml_nelements(denoised);\n            for (int i = 0; i < ne_elements; i++) {\n                float latent_result = positive_data[i];\n                if (has_unconditioned) {\n                    // out_uncond + cfg_scale * (out_cond - out_uncond)\n                    int64_t ne3 = out_cond->ne[3];\n                    if (min_cfg != cfg_scale && ne3 != 1) {\n                        int64_t i3  = i / out_cond->ne[0] * out_cond->ne[1] * out_cond->ne[2];\n                        float scale = min_cfg + (cfg_scale - min_cfg) * (i3 * 1.0f / ne3);\n                    } else {\n                        latent_result = negative_data[i] + cfg_scale * (positive_data[i] - negative_data[i]);\n                    }\n                }\n                if (is_skiplayer_step) {\n                    latent_result = latent_result + (positive_data[i] - skip_layer_data[i]) * slg_scale;\n                }\n                // v = latent_result, eps = latent_result\n                // denoised = (v * c_out + input * c_skip) or (input + eps * c_out)\n                vec_denoised[i] = latent_result * c_out + vec_input[i] * c_skip;\n            }\n            int64_t t1 = ggml_time_us();\n            if (step > 0) {\n                pretty_progress(step, (int)steps, (t1 - t0) / 1000000.f);\n                // LOG_INFO(\"step %d sampling completed taking %.2fs\", step, (t1 - t0) * 1.0f / 1000000);\n            }\n            if (noise_mask != nullptr) {\n                for (int64_t x = 0; x < denoised->ne[0]; x++) {\n                    for (int64_t y = 0; y < denoised->ne[1]; y++) {\n                        float mask = ggml_tensor_get_f32(noise_mask, x, y);\n                        for (int64_t k = 0; k < denoised->ne[2]; k++) {\n                            float init = ggml_tensor_get_f32(init_latent, x, y, k);\n                            float den  = ggml_tensor_get_f32(denoised, x, y, k);\n                            ggml_tensor_set_f32(denoised, init + mask * (den - init), x, y, k);\n                        }\n                    }\n                }\n            }\n\n            return denoised;\n        };\n\n        sample_k_diffusion(method, denoise, work_ctx, x, sigmas, rng);\n\n        x = denoiser->inverse_noise_scaling(sigmas[sigmas.size() - 1], x);\n\n        if (control_net) {\n            control_net->free_control_ctx();\n            control_net->free_compute_buffer();\n        }\n        diffusion_model->free_compute_buffer();\n        return x;\n    }\n\n    // ldm.models.diffusion.ddpm.LatentDiffusion.get_first_stage_encoding\n    ggml_tensor* get_first_stage_encoding(ggml_context* work_ctx, ggml_tensor* moments) {\n        // ldm.modules.distributions.distributions.DiagonalGaussianDistribution.sample\n        ggml_tensor* latent       = ggml_new_tensor_4d(work_ctx, moments->type, moments->ne[0], moments->ne[1], moments->ne[2] / 2, moments->ne[3]);\n        struct ggml_tensor* noise = ggml_dup_tensor(work_ctx, latent);\n        ggml_tensor_set_f32_randn(noise, rng);\n        // noise = load_tensor_from_file(work_ctx, \"noise.bin\");\n        {\n            float mean   = 0;\n            float logvar = 0;\n            float value  = 0;\n            float std_   = 0;\n            for (int i = 0; i < latent->ne[3]; i++) {\n                for (int j = 0; j < latent->ne[2]; j++) {\n                    for (int k = 0; k < latent->ne[1]; k++) {\n                        for (int l = 0; l < latent->ne[0]; l++) {\n                            mean   = ggml_tensor_get_f32(moments, l, k, j, i);\n                            logvar = ggml_tensor_get_f32(moments, l, k, j + (int)latent->ne[2], i);\n                            logvar = std::max(-30.0f, std::min(logvar, 20.0f));\n                            std_   = std::exp(0.5f * logvar);\n                            value  = mean + std_ * ggml_tensor_get_f32(noise, l, k, j, i);\n                            value  = value * scale_factor;\n                            // printf(\"%d %d %d %d -> %f\\n\", i, j, k, l, value);\n                            ggml_tensor_set_f32(latent, value, l, k, j, i);\n                        }\n                    }\n                }\n            }\n        }\n        return latent;\n    }\n\n    ggml_tensor* compute_first_stage(ggml_context* work_ctx, ggml_tensor* x, bool decode) {\n        int64_t W = x->ne[0];\n        int64_t H = x->ne[1];\n        int64_t C = 8;\n        if (use_tiny_autoencoder) {\n            C = 4;\n        } else {\n            if (sd_version_is_sd3(version)) {\n                C = 32;\n            } else if (sd_version_is_flux(version)) {\n                C = 32;\n            }\n        }\n        ggml_tensor* result = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32,\n                                                 decode ? (W * 8) : (W / 8),  // width\n                                                 decode ? (H * 8) : (H / 8),  // height\n                                                 decode ? 3 : C,\n                                                 x->ne[3]);  // channels\n        int64_t t0          = ggml_time_ms();\n        if (!use_tiny_autoencoder) {\n            if (decode) {\n                ggml_tensor_scale(x, 1.0f / scale_factor);\n            } else {\n                ggml_tensor_scale_input(x);\n            }\n            if (vae_tiling && decode) {  // TODO: support tiling vae encode\n                // split latent in 32x32 tiles and compute in several steps\n                auto on_tiling = [&](ggml_tensor* in, ggml_tensor* out, bool init) {\n                    first_stage_model->compute(n_threads, in, decode, &out);\n                };\n                sd_tiling(x, result, 8, 32, 0.5f, on_tiling);\n            } else {\n                first_stage_model->compute(n_threads, x, decode, &result);\n            }\n            first_stage_model->free_compute_buffer();\n            if (decode) {\n                ggml_tensor_scale_output(result);\n            }\n        } else {\n            if (vae_tiling && decode) {  // TODO: support tiling vae encode\n                // split latent in 64x64 tiles and compute in several steps\n                auto on_tiling = [&](ggml_tensor* in, ggml_tensor* out, bool init) {\n                    tae_first_stage->compute(n_threads, in, decode, &out);\n                };\n                sd_tiling(x, result, 8, 64, 0.5f, on_tiling);\n            } else {\n                tae_first_stage->compute(n_threads, x, decode, &result);\n            }\n            tae_first_stage->free_compute_buffer();\n        }\n\n        int64_t t1 = ggml_time_ms();\n        LOG_DEBUG(\"computing vae [mode: %s] graph completed, taking %.2fs\", decode ? \"DECODE\" : \"ENCODE\", (t1 - t0) * 1.0f / 1000);\n        if (decode) {\n            ggml_tensor_clamp(result, 0.0f, 1.0f);\n        }\n        return result;\n    }\n\n    ggml_tensor* encode_first_stage(ggml_context* work_ctx, ggml_tensor* x) {\n        return compute_first_stage(work_ctx, x, false);\n    }\n\n    ggml_tensor* decode_first_stage(ggml_context* work_ctx, ggml_tensor* x) {\n        return compute_first_stage(work_ctx, x, true);\n    }\n};\n\n/*================================================= SD API ==================================================*/\n\nstruct sd_ctx_t {\n    StableDiffusionGGML* sd = NULL;\n};\n\nsd_ctx_t* new_sd_ctx(const char* model_path_c_str,\n                     const char* clip_l_path_c_str,\n                     const char* clip_g_path_c_str,\n                     const char* t5xxl_path_c_str,\n                     const char* diffusion_model_path_c_str,\n                     const char* vae_path_c_str,\n                     const char* taesd_path_c_str,\n                     const char* control_net_path_c_str,\n                     const char* lora_model_dir_c_str,\n                     const char* embed_dir_c_str,\n                     const char* id_embed_dir_c_str,\n                     bool vae_decode_only,\n                     bool vae_tiling,\n                     bool free_params_immediately,\n                     int n_threads,\n                     enum sd_type_t wtype,\n                     enum rng_type_t rng_type,\n                     enum schedule_t s,\n                     bool keep_clip_on_cpu,\n                     bool keep_control_net_cpu,\n                     bool keep_vae_on_cpu,\n                     bool diffusion_flash_attn) {\n    sd_ctx_t* sd_ctx = (sd_ctx_t*)malloc(sizeof(sd_ctx_t));\n    if (sd_ctx == NULL) {\n        return NULL;\n    }\n    std::string model_path(model_path_c_str);\n    std::string clip_l_path(clip_l_path_c_str);\n    std::string clip_g_path(clip_g_path_c_str);\n    std::string t5xxl_path(t5xxl_path_c_str);\n    std::string diffusion_model_path(diffusion_model_path_c_str);\n    std::string vae_path(vae_path_c_str);\n    std::string taesd_path(taesd_path_c_str);\n    std::string control_net_path(control_net_path_c_str);\n    std::string embd_path(embed_dir_c_str);\n    std::string id_embd_path(id_embed_dir_c_str);\n    std::string lora_model_dir(lora_model_dir_c_str);\n\n    sd_ctx->sd = new StableDiffusionGGML(n_threads,\n                                         vae_decode_only,\n                                         free_params_immediately,\n                                         lora_model_dir,\n                                         rng_type);\n    if (sd_ctx->sd == NULL) {\n        return NULL;\n    }\n\n    if (!sd_ctx->sd->load_from_file(model_path,\n                                    clip_l_path,\n                                    clip_g_path,\n                                    t5xxl_path_c_str,\n                                    diffusion_model_path,\n                                    vae_path,\n                                    control_net_path,\n                                    embd_path,\n                                    id_embd_path,\n                                    taesd_path,\n                                    vae_tiling,\n                                    (ggml_type)wtype,\n                                    s,\n                                    keep_clip_on_cpu,\n                                    keep_control_net_cpu,\n                                    keep_vae_on_cpu,\n                                    diffusion_flash_attn)) {\n        delete sd_ctx->sd;\n        sd_ctx->sd = NULL;\n        free(sd_ctx);\n        return NULL;\n    }\n    return sd_ctx;\n}\n\nvoid free_sd_ctx(sd_ctx_t* sd_ctx) {\n    if (sd_ctx->sd != NULL) {\n        delete sd_ctx->sd;\n        sd_ctx->sd = NULL;\n    }\n    free(sd_ctx);\n}\n\nsd_image_t* generate_image(sd_ctx_t* sd_ctx,\n                           struct ggml_context* work_ctx,\n                           ggml_tensor* init_latent,\n                           std::string prompt,\n                           std::string negative_prompt,\n                           int clip_skip,\n                           float cfg_scale,\n                           float guidance,\n                           int width,\n                           int height,\n                           enum sample_method_t sample_method,\n                           const std::vector<float>& sigmas,\n                           int64_t seed,\n                           int batch_count,\n                           const sd_image_t* control_cond,\n                           float control_strength,\n                           float style_ratio,\n                           bool normalize_input,\n                           std::string input_id_images_path,\n                           std::vector<int> skip_layers = {},\n                           float slg_scale              = 0,\n                           float skip_layer_start       = 0.01,\n                           float skip_layer_end         = 0.2,\n                           ggml_tensor* masked_image    = NULL) {\n    if (seed < 0) {\n        // Generally, when using the provided command line, the seed is always >0.\n        // However, to prevent potential issues if 'stable-diffusion.cpp' is invoked as a library\n        // by a third party with a seed <0, let's incorporate randomization here.\n        srand((int)time(NULL));\n        seed = rand();\n    }\n\n    // for (auto v : sigmas) {\n    //     std::cout << v << \" \";\n    // }\n    // std::cout << std::endl;\n\n    int sample_steps = sigmas.size() - 1;\n\n    // Apply lora\n    auto result_pair                                = extract_and_remove_lora(prompt);\n    std::unordered_map<std::string, float> lora_f2m = result_pair.first;  // lora_name -> multiplier\n\n    for (auto& kv : lora_f2m) {\n        LOG_DEBUG(\"lora %s:%.2f\", kv.first.c_str(), kv.second);\n    }\n\n    prompt = result_pair.second;\n    LOG_DEBUG(\"prompt after extract and remove lora: \\\"%s\\\"\", prompt.c_str());\n\n    int64_t t0 = ggml_time_ms();\n    sd_ctx->sd->apply_loras(lora_f2m);\n    int64_t t1 = ggml_time_ms();\n    LOG_INFO(\"apply_loras completed, taking %.2fs\", (t1 - t0) * 1.0f / 1000);\n\n    // Photo Maker\n    std::string prompt_text_only;\n    ggml_tensor* init_img = NULL;\n    SDCondition id_cond;\n    std::vector<bool> class_tokens_mask;\n    if (sd_ctx->sd->stacked_id) {\n        if (!sd_ctx->sd->pmid_lora->applied) {\n            t0 = ggml_time_ms();\n            sd_ctx->sd->pmid_lora->apply(sd_ctx->sd->tensors, sd_ctx->sd->version, sd_ctx->sd->n_threads);\n            t1                             = ggml_time_ms();\n            sd_ctx->sd->pmid_lora->applied = true;\n            LOG_INFO(\"pmid_lora apply completed, taking %.2fs\", (t1 - t0) * 1.0f / 1000);\n            if (sd_ctx->sd->free_params_immediately) {\n                sd_ctx->sd->pmid_lora->free_params_buffer();\n            }\n        }\n        // preprocess input id images\n        std::vector<sd_image_t*> input_id_images;\n        bool pmv2 = sd_ctx->sd->pmid_model->get_version() == PM_VERSION_2;\n        if (sd_ctx->sd->pmid_model && input_id_images_path.size() > 0) {\n            std::vector<std::string> img_files = get_files_from_dir(input_id_images_path);\n            for (std::string img_file : img_files) {\n                int c = 0;\n                int width, height;\n                if (ends_with(img_file, \"safetensors\")) {\n                    continue;\n                }\n                uint8_t* input_image_buffer = stbi_load(img_file.c_str(), &width, &height, &c, 3);\n                if (input_image_buffer == NULL) {\n                    LOG_ERROR(\"PhotoMaker load image from '%s' failed\", img_file.c_str());\n                    continue;\n                } else {\n                    LOG_INFO(\"PhotoMaker loaded image from '%s'\", img_file.c_str());\n                }\n                sd_image_t* input_image = NULL;\n                input_image             = new sd_image_t{(uint32_t)width,\n                                             (uint32_t)height,\n                                             3,\n                                             input_image_buffer};\n                input_image             = preprocess_id_image(input_image);\n                if (input_image == NULL) {\n                    LOG_ERROR(\"preprocess input id image from '%s' failed\", img_file.c_str());\n                    continue;\n                }\n                input_id_images.push_back(input_image);\n            }\n        }\n        if (input_id_images.size() > 0) {\n            sd_ctx->sd->pmid_model->style_strength = style_ratio;\n            int32_t w                              = input_id_images[0]->width;\n            int32_t h                              = input_id_images[0]->height;\n            int32_t channels                       = input_id_images[0]->channel;\n            int32_t num_input_images               = (int32_t)input_id_images.size();\n            init_img                               = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, w, h, channels, num_input_images);\n            // TODO: move these to somewhere else and be user settable\n            float mean[] = {0.48145466f, 0.4578275f, 0.40821073f};\n            float std[]  = {0.26862954f, 0.26130258f, 0.27577711f};\n            for (int i = 0; i < num_input_images; i++) {\n                sd_image_t* init_image = input_id_images[i];\n                if (normalize_input)\n                    sd_mul_images_to_tensor(init_image->data, init_img, i, mean, std);\n                else\n                    sd_mul_images_to_tensor(init_image->data, init_img, i, NULL, NULL);\n            }\n            t0                            = ggml_time_ms();\n            auto cond_tup                 = sd_ctx->sd->cond_stage_model->get_learned_condition_with_trigger(work_ctx,\n                                                                                                             sd_ctx->sd->n_threads, prompt,\n                                                                                                             clip_skip,\n                                                                                                             width,\n                                                                                                             height,\n                                                                                                             num_input_images,\n                                                                                                             sd_ctx->sd->diffusion_model->get_adm_in_channels());\n            id_cond                       = std::get<0>(cond_tup);\n            class_tokens_mask             = std::get<1>(cond_tup);  //\n            struct ggml_tensor* id_embeds = NULL;\n            if (pmv2) {\n                // id_embeds = sd_ctx->sd->pmid_id_embeds->get();\n                id_embeds = load_tensor_from_file(work_ctx, path_join(input_id_images_path, \"id_embeds.bin\"));\n                // print_ggml_tensor(id_embeds, true, \"id_embeds:\");\n            }\n            id_cond.c_crossattn = sd_ctx->sd->id_encoder(work_ctx, init_img, id_cond.c_crossattn, id_embeds, class_tokens_mask);\n            t1                  = ggml_time_ms();\n            LOG_INFO(\"Photomaker ID Stacking, taking %\" PRId64 \" ms\", t1 - t0);\n            if (sd_ctx->sd->free_params_immediately) {\n                sd_ctx->sd->pmid_model->free_params_buffer();\n            }\n            // Encode input prompt without the trigger word for delayed conditioning\n            prompt_text_only = sd_ctx->sd->cond_stage_model->remove_trigger_from_prompt(work_ctx, prompt);\n            // printf(\"%s || %s \\n\", prompt.c_str(), prompt_text_only.c_str());\n            prompt = prompt_text_only;  //\n            // if (sample_steps < 50) {\n            //     LOG_INFO(\"sampling steps increases from %d to 50 for PHOTOMAKER\", sample_steps);\n            //     sample_steps = 50;\n            // }\n        } else {\n            LOG_WARN(\"Provided PhotoMaker model file, but NO input ID images\");\n            LOG_WARN(\"Turn off PhotoMaker\");\n            sd_ctx->sd->stacked_id = false;\n        }\n        for (sd_image_t* img : input_id_images) {\n            free(img->data);\n        }\n        input_id_images.clear();\n    }\n\n    // Get learned condition\n    t0               = ggml_time_ms();\n    SDCondition cond = sd_ctx->sd->cond_stage_model->get_learned_condition(work_ctx,\n                                                                           sd_ctx->sd->n_threads,\n                                                                           prompt,\n                                                                           clip_skip,\n                                                                           width,\n                                                                           height,\n                                                                           sd_ctx->sd->diffusion_model->get_adm_in_channels());\n\n    SDCondition uncond;\n    if (cfg_scale != 1.0) {\n        bool force_zero_embeddings = false;\n        if (sd_version_is_sdxl(sd_ctx->sd->version) && negative_prompt.size() == 0) {\n            force_zero_embeddings = true;\n        }\n        uncond = sd_ctx->sd->cond_stage_model->get_learned_condition(work_ctx,\n                                                                     sd_ctx->sd->n_threads,\n                                                                     negative_prompt,\n                                                                     clip_skip,\n                                                                     width,\n                                                                     height,\n                                                                     sd_ctx->sd->diffusion_model->get_adm_in_channels(),\n                                                                     force_zero_embeddings);\n    }\n    t1 = ggml_time_ms();\n    LOG_INFO(\"get_learned_condition completed, taking %\" PRId64 \" ms\", t1 - t0);\n\n    if (sd_ctx->sd->free_params_immediately) {\n        sd_ctx->sd->cond_stage_model->free_params_buffer();\n    }\n\n    // Control net hint\n    struct ggml_tensor* image_hint = NULL;\n    if (control_cond != NULL) {\n        image_hint = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, width, height, 3, 1);\n        sd_image_to_tensor(control_cond->data, image_hint);\n    }\n\n    // Sample\n    std::vector<struct ggml_tensor*> final_latents;  // collect latents to decode\n    int C = 4;\n    if (sd_version_is_sd3(sd_ctx->sd->version)) {\n        C = 16;\n    } else if (sd_version_is_flux(sd_ctx->sd->version)) {\n        C = 16;\n    }\n    int W = width / 8;\n    int H = height / 8;\n    LOG_INFO(\"sampling using %s method\", sampling_methods_str[sample_method]);\n    ggml_tensor* noise_mask = nullptr;\n    if (sd_version_is_inpaint(sd_ctx->sd->version)) {\n        if (masked_image == NULL) {\n            int64_t mask_channels = 1;\n            if (sd_ctx->sd->version == VERSION_FLUX_FILL) {\n                mask_channels = 8 * 8;  // flatten the whole mask\n            }\n            // no mask, set the whole image as masked\n            masked_image = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, init_latent->ne[0], init_latent->ne[1], mask_channels + init_latent->ne[2], 1);\n            for (int64_t x = 0; x < masked_image->ne[0]; x++) {\n                for (int64_t y = 0; y < masked_image->ne[1]; y++) {\n                    if (sd_ctx->sd->version == VERSION_FLUX_FILL) {\n                        // TODO: this might be wrong\n                        for (int64_t c = 0; c < init_latent->ne[2]; c++) {\n                            ggml_tensor_set_f32(masked_image, 0, x, y, c);\n                        }\n                        for (int64_t c = init_latent->ne[2]; c < masked_image->ne[2]; c++) {\n                            ggml_tensor_set_f32(masked_image, 1, x, y, c);\n                        }\n                    } else {\n                        ggml_tensor_set_f32(masked_image, 1, x, y, 0);\n                        for (int64_t c = 1; c < masked_image->ne[2]; c++) {\n                            ggml_tensor_set_f32(masked_image, 0, x, y, c);\n                        }\n                    }\n                }\n            }\n        }\n        cond.c_concat   = masked_image;\n        uncond.c_concat = masked_image;\n    } else {\n        noise_mask = masked_image;\n    }\n    for (int b = 0; b < batch_count; b++) {\n        int64_t sampling_start = ggml_time_ms();\n        int64_t cur_seed       = seed + b;\n        LOG_INFO(\"generating image: %i/%i - seed %\" PRId64, b + 1, batch_count, cur_seed);\n\n        sd_ctx->sd->rng->manual_seed(cur_seed);\n        struct ggml_tensor* x_t   = init_latent;\n        struct ggml_tensor* noise = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, W, H, C, 1);\n        ggml_tensor_set_f32_randn(noise, sd_ctx->sd->rng);\n\n        int start_merge_step = -1;\n        if (sd_ctx->sd->stacked_id) {\n            start_merge_step = int(sd_ctx->sd->pmid_model->style_strength / 100.f * sample_steps);\n            // if (start_merge_step > 30)\n            //     start_merge_step = 30;\n            LOG_INFO(\"PHOTOMAKER: start_merge_step: %d\", start_merge_step);\n        }\n\n        struct ggml_tensor* x_0 = sd_ctx->sd->sample(work_ctx,\n                                                     x_t,\n                                                     noise,\n                                                     cond,\n                                                     uncond,\n                                                     image_hint,\n                                                     control_strength,\n                                                     cfg_scale,\n                                                     cfg_scale,\n                                                     guidance,\n                                                     sample_method,\n                                                     sigmas,\n                                                     start_merge_step,\n                                                     id_cond,\n                                                     skip_layers,\n                                                     slg_scale,\n                                                     skip_layer_start,\n                                                     skip_layer_end,\n                                                     noise_mask);\n\n        // struct ggml_tensor* x_0 = load_tensor_from_file(ctx, \"samples_ddim.bin\");\n        // print_ggml_tensor(x_0);\n        int64_t sampling_end = ggml_time_ms();\n        LOG_INFO(\"sampling completed, taking %.2fs\", (sampling_end - sampling_start) * 1.0f / 1000);\n        final_latents.push_back(x_0);\n    }\n\n    if (sd_ctx->sd->free_params_immediately) {\n        sd_ctx->sd->diffusion_model->free_params_buffer();\n    }\n    int64_t t3 = ggml_time_ms();\n    LOG_INFO(\"generating %\" PRId64 \" latent images completed, taking %.2fs\", final_latents.size(), (t3 - t1) * 1.0f / 1000);\n\n    // Decode to image\n    LOG_INFO(\"decoding %zu latents\", final_latents.size());\n    std::vector<struct ggml_tensor*> decoded_images;  // collect decoded images\n    for (size_t i = 0; i < final_latents.size(); i++) {\n        t1                      = ggml_time_ms();\n        struct ggml_tensor* img = sd_ctx->sd->decode_first_stage(work_ctx, final_latents[i] /* x_0 */);\n        // print_ggml_tensor(img);\n        if (img != NULL) {\n            decoded_images.push_back(img);\n        }\n        int64_t t2 = ggml_time_ms();\n        LOG_INFO(\"latent %\" PRId64 \" decoded, taking %.2fs\", i + 1, (t2 - t1) * 1.0f / 1000);\n    }\n\n    int64_t t4 = ggml_time_ms();\n    LOG_INFO(\"decode_first_stage completed, taking %.2fs\", (t4 - t3) * 1.0f / 1000);\n    if (sd_ctx->sd->free_params_immediately && !sd_ctx->sd->use_tiny_autoencoder) {\n        sd_ctx->sd->first_stage_model->free_params_buffer();\n    }\n    sd_image_t* result_images = (sd_image_t*)calloc(batch_count, sizeof(sd_image_t));\n    if (result_images == NULL) {\n        ggml_free(work_ctx);\n        return NULL;\n    }\n\n    for (size_t i = 0; i < decoded_images.size(); i++) {\n        result_images[i].width   = width;\n        result_images[i].height  = height;\n        result_images[i].channel = 3;\n        result_images[i].data    = sd_tensor_to_image(decoded_images[i]);\n    }\n    ggml_free(work_ctx);\n\n    return result_images;\n}\n\nsd_image_t* txt2img(sd_ctx_t* sd_ctx,\n                    const char* prompt_c_str,\n                    const char* negative_prompt_c_str,\n                    int clip_skip,\n                    float cfg_scale,\n                    float guidance,\n                    int width,\n                    int height,\n                    enum sample_method_t sample_method,\n                    int sample_steps,\n                    int64_t seed,\n                    int batch_count,\n                    const sd_image_t* control_cond,\n                    float control_strength,\n                    float style_ratio,\n                    bool normalize_input,\n                    const char* input_id_images_path_c_str,\n                    int* skip_layers         = NULL,\n                    size_t skip_layers_count = 0,\n                    float slg_scale          = 0,\n                    float skip_layer_start   = 0.01,\n                    float skip_layer_end     = 0.2) {\n    std::vector<int> skip_layers_vec(skip_layers, skip_layers + skip_layers_count);\n    LOG_DEBUG(\"txt2img %dx%d\", width, height);\n    if (sd_ctx == NULL) {\n        return NULL;\n    }\n\n    struct ggml_init_params params;\n    params.mem_size = static_cast<size_t>(10 * 1024 * 1024);  // 10 MB\n    if (sd_version_is_sd3(sd_ctx->sd->version)) {\n        params.mem_size *= 3;\n    }\n    if (sd_version_is_flux(sd_ctx->sd->version)) {\n        params.mem_size *= 4;\n    }\n    if (sd_ctx->sd->stacked_id) {\n        params.mem_size += static_cast<size_t>(10 * 1024 * 1024);  // 10 MB\n    }\n    params.mem_size += width * height * 3 * sizeof(float);\n    params.mem_size *= batch_count;\n    params.mem_buffer = NULL;\n    params.no_alloc   = false;\n    // LOG_DEBUG(\"mem_size %u \", params.mem_size);\n\n    struct ggml_context* work_ctx = ggml_init(params);\n    if (!work_ctx) {\n        LOG_ERROR(\"ggml_init() failed\");\n        return NULL;\n    }\n\n    size_t t0 = ggml_time_ms();\n\n    std::vector<float> sigmas = sd_ctx->sd->denoiser->get_sigmas(sample_steps);\n\n    int C = 4;\n    if (sd_version_is_sd3(sd_ctx->sd->version)) {\n        C = 16;\n    } else if (sd_version_is_flux(sd_ctx->sd->version)) {\n        C = 16;\n    }\n    int W                    = width / 8;\n    int H                    = height / 8;\n    ggml_tensor* init_latent = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, W, H, C, 1);\n    if (sd_version_is_sd3(sd_ctx->sd->version)) {\n        ggml_set_f32(init_latent, 0.0609f);\n    } else if (sd_version_is_flux(sd_ctx->sd->version)) {\n        ggml_set_f32(init_latent, 0.1159f);\n    } else {\n        ggml_set_f32(init_latent, 0.f);\n    }\n\n    if (sd_version_is_inpaint(sd_ctx->sd->version)) {\n        LOG_WARN(\"This is an inpainting model, this should only be used in img2img mode with a mask\");\n    }\n\n    sd_image_t* result_images = generate_image(sd_ctx,\n                                               work_ctx,\n                                               init_latent,\n                                               prompt_c_str,\n                                               negative_prompt_c_str,\n                                               clip_skip,\n                                               cfg_scale,\n                                               guidance,\n                                               width,\n                                               height,\n                                               sample_method,\n                                               sigmas,\n                                               seed,\n                                               batch_count,\n                                               control_cond,\n                                               control_strength,\n                                               style_ratio,\n                                               normalize_input,\n                                               input_id_images_path_c_str,\n                                               skip_layers_vec,\n                                               slg_scale,\n                                               skip_layer_start,\n                                               skip_layer_end);\n\n    size_t t1 = ggml_time_ms();\n\n    LOG_INFO(\"txt2img completed in %.2fs\", (t1 - t0) * 1.0f / 1000);\n\n    return result_images;\n}\n\nsd_image_t* img2img(sd_ctx_t* sd_ctx,\n                    sd_image_t init_image,\n                    sd_image_t mask,\n                    const char* prompt_c_str,\n                    const char* negative_prompt_c_str,\n                    int clip_skip,\n                    float cfg_scale,\n                    float guidance,\n                    int width,\n                    int height,\n                    sample_method_t sample_method,\n                    int sample_steps,\n                    float strength,\n                    int64_t seed,\n                    int batch_count,\n                    const sd_image_t* control_cond,\n                    float control_strength,\n                    float style_ratio,\n                    bool normalize_input,\n                    const char* input_id_images_path_c_str,\n                    int* skip_layers         = NULL,\n                    size_t skip_layers_count = 0,\n                    float slg_scale          = 0,\n                    float skip_layer_start   = 0.01,\n                    float skip_layer_end     = 0.2) {\n    std::vector<int> skip_layers_vec(skip_layers, skip_layers + skip_layers_count);\n    LOG_DEBUG(\"img2img %dx%d\", width, height);\n    if (sd_ctx == NULL) {\n        return NULL;\n    }\n\n    struct ggml_init_params params;\n    params.mem_size = static_cast<size_t>(10 * 1024 * 1024);  // 10 MB\n    if (sd_version_is_sd3(sd_ctx->sd->version)) {\n        params.mem_size *= 2;\n    }\n    if (sd_version_is_flux(sd_ctx->sd->version)) {\n        params.mem_size *= 3;\n    }\n    if (sd_ctx->sd->stacked_id) {\n        params.mem_size += static_cast<size_t>(10 * 1024 * 1024);  // 10 MB\n    }\n    params.mem_size += width * height * 3 * sizeof(float) * 3;\n    params.mem_size *= batch_count;\n    params.mem_buffer = NULL;\n    params.no_alloc   = false;\n    // LOG_DEBUG(\"mem_size %u \", params.mem_size);\n\n    struct ggml_context* work_ctx = ggml_init(params);\n    if (!work_ctx) {\n        LOG_ERROR(\"ggml_init() failed\");\n        return NULL;\n    }\n\n    size_t t0 = ggml_time_ms();\n\n    if (seed < 0) {\n        srand((int)time(NULL));\n        seed = rand();\n    }\n    sd_ctx->sd->rng->manual_seed(seed);\n\n    ggml_tensor* init_img = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, width, height, 3, 1);\n    ggml_tensor* mask_img = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, width, height, 1, 1);\n\n    sd_mask_to_tensor(mask.data, mask_img);\n\n    sd_image_to_tensor(init_image.data, init_img);\n\n    ggml_tensor* masked_image;\n\n    if (sd_version_is_inpaint(sd_ctx->sd->version)) {\n        int64_t mask_channels = 1;\n        if (sd_ctx->sd->version == VERSION_FLUX_FILL) {\n            mask_channels = 8 * 8;  // flatten the whole mask\n        }\n        ggml_tensor* masked_img = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, width, height, 3, 1);\n        sd_apply_mask(init_img, mask_img, masked_img);\n        ggml_tensor* masked_image_0 = NULL;\n        if (!sd_ctx->sd->use_tiny_autoencoder) {\n            ggml_tensor* moments = sd_ctx->sd->encode_first_stage(work_ctx, masked_img);\n            masked_image_0       = sd_ctx->sd->get_first_stage_encoding(work_ctx, moments);\n        } else {\n            masked_image_0 = sd_ctx->sd->encode_first_stage(work_ctx, masked_img);\n        }\n        masked_image = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, masked_image_0->ne[0], masked_image_0->ne[1], mask_channels + masked_image_0->ne[2], 1);\n        for (int ix = 0; ix < masked_image_0->ne[0]; ix++) {\n            for (int iy = 0; iy < masked_image_0->ne[1]; iy++) {\n                int mx = ix * 8;\n                int my = iy * 8;\n                if (sd_ctx->sd->version == VERSION_FLUX_FILL) {\n                    for (int k = 0; k < masked_image_0->ne[2]; k++) {\n                        float v = ggml_tensor_get_f32(masked_image_0, ix, iy, k);\n                        ggml_tensor_set_f32(masked_image, v, ix, iy, k);\n                    }\n                    // \"Encode\" 8x8 mask chunks into a flattened 1x64 vector, and concatenate to masked image\n                    for (int x = 0; x < 8; x++) {\n                        for (int y = 0; y < 8; y++) {\n                            float m = ggml_tensor_get_f32(mask_img, mx + x, my + y);\n                            // TODO: check if the way the mask is flattened is correct (is it supposed to be x*8+y or x+8*y?)\n                            // python code was using \"b (h 8) (w 8) -> b (8 8) h w\"\n                            ggml_tensor_set_f32(masked_image, m, ix, iy, masked_image_0->ne[2] + x * 8 + y);\n                        }\n                    }\n                } else {\n                    float m = ggml_tensor_get_f32(mask_img, mx, my);\n                    ggml_tensor_set_f32(masked_image, m, ix, iy, 0);\n                    for (int k = 0; k < masked_image_0->ne[2]; k++) {\n                        float v = ggml_tensor_get_f32(masked_image_0, ix, iy, k);\n                        ggml_tensor_set_f32(masked_image, v, ix, iy, k + mask_channels);\n                    }\n                }\n            }\n        }\n    } else {\n        // LOG_WARN(\"Inpainting with a base model is not great\");\n        masked_image = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, width / 8, height / 8, 1, 1);\n        for (int ix = 0; ix < masked_image->ne[0]; ix++) {\n            for (int iy = 0; iy < masked_image->ne[1]; iy++) {\n                int mx  = ix * 8;\n                int my  = iy * 8;\n                float m = ggml_tensor_get_f32(mask_img, mx, my);\n                ggml_tensor_set_f32(masked_image, m, ix, iy);\n            }\n        }\n    }\n\n    ggml_tensor* init_latent = NULL;\n    if (!sd_ctx->sd->use_tiny_autoencoder) {\n        ggml_tensor* moments = sd_ctx->sd->encode_first_stage(work_ctx, init_img);\n        init_latent          = sd_ctx->sd->get_first_stage_encoding(work_ctx, moments);\n    } else {\n        init_latent = sd_ctx->sd->encode_first_stage(work_ctx, init_img);\n    }\n\n    print_ggml_tensor(init_latent, true);\n    size_t t1 = ggml_time_ms();\n    LOG_INFO(\"encode_first_stage completed, taking %.2fs\", (t1 - t0) * 1.0f / 1000);\n\n    std::vector<float> sigmas = sd_ctx->sd->denoiser->get_sigmas(sample_steps);\n    size_t t_enc              = static_cast<size_t>(sample_steps * strength);\n    if (t_enc == sample_steps)\n        t_enc--;\n    LOG_INFO(\"target t_enc is %zu steps\", t_enc);\n    std::vector<float> sigma_sched;\n    sigma_sched.assign(sigmas.begin() + sample_steps - t_enc - 1, sigmas.end());\n\n    sd_image_t* result_images = generate_image(sd_ctx,\n                                               work_ctx,\n                                               init_latent,\n                                               prompt_c_str,\n                                               negative_prompt_c_str,\n                                               clip_skip,\n                                               cfg_scale,\n                                               guidance,\n                                               width,\n                                               height,\n                                               sample_method,\n                                               sigma_sched,\n                                               seed,\n                                               batch_count,\n                                               control_cond,\n                                               control_strength,\n                                               style_ratio,\n                                               normalize_input,\n                                               input_id_images_path_c_str,\n                                               skip_layers_vec,\n                                               slg_scale,\n                                               skip_layer_start,\n                                               skip_layer_end,\n                                               masked_image);\n\n    size_t t2 = ggml_time_ms();\n\n    LOG_INFO(\"img2img completed in %.2fs\", (t1 - t0) * 1.0f / 1000);\n\n    return result_images;\n}\n\nSD_API sd_image_t* img2vid(sd_ctx_t* sd_ctx,\n                           sd_image_t init_image,\n                           int width,\n                           int height,\n                           int video_frames,\n                           int motion_bucket_id,\n                           int fps,\n                           float augmentation_level,\n                           float min_cfg,\n                           float cfg_scale,\n                           enum sample_method_t sample_method,\n                           int sample_steps,\n                           float strength,\n                           int64_t seed) {\n    if (sd_ctx == NULL) {\n        return NULL;\n    }\n\n    LOG_INFO(\"img2vid %dx%d\", width, height);\n\n    std::vector<float> sigmas = sd_ctx->sd->denoiser->get_sigmas(sample_steps);\n\n    struct ggml_init_params params;\n    params.mem_size = static_cast<size_t>(10 * 1024) * 1024;  // 10 MB\n    params.mem_size += width * height * 3 * sizeof(float) * video_frames;\n    params.mem_buffer = NULL;\n    params.no_alloc   = false;\n    // LOG_DEBUG(\"mem_size %u \", params.mem_size);\n\n    // draft context\n    struct ggml_context* work_ctx = ggml_init(params);\n    if (!work_ctx) {\n        LOG_ERROR(\"ggml_init() failed\");\n        return NULL;\n    }\n\n    if (seed < 0) {\n        seed = (int)time(NULL);\n    }\n\n    sd_ctx->sd->rng->manual_seed(seed);\n\n    int64_t t0 = ggml_time_ms();\n\n    SDCondition cond = sd_ctx->sd->get_svd_condition(work_ctx,\n                                                     init_image,\n                                                     width,\n                                                     height,\n                                                     fps,\n                                                     motion_bucket_id,\n                                                     augmentation_level);\n\n    auto uc_crossattn = ggml_dup_tensor(work_ctx, cond.c_crossattn);\n    ggml_set_f32(uc_crossattn, 0.f);\n\n    auto uc_concat = ggml_dup_tensor(work_ctx, cond.c_concat);\n    ggml_set_f32(uc_concat, 0.f);\n\n    auto uc_vector = ggml_dup_tensor(work_ctx, cond.c_vector);\n\n    SDCondition uncond = SDCondition(uc_crossattn, uc_vector, uc_concat);\n\n    int64_t t1 = ggml_time_ms();\n    LOG_INFO(\"get_learned_condition completed, taking %\" PRId64 \" ms\", t1 - t0);\n    if (sd_ctx->sd->free_params_immediately) {\n        sd_ctx->sd->clip_vision->free_params_buffer();\n    }\n\n    sd_ctx->sd->rng->manual_seed(seed);\n    int C                   = 4;\n    int W                   = width / 8;\n    int H                   = height / 8;\n    struct ggml_tensor* x_t = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, W, H, C, video_frames);\n    ggml_set_f32(x_t, 0.f);\n\n    struct ggml_tensor* noise = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, W, H, C, video_frames);\n    ggml_tensor_set_f32_randn(noise, sd_ctx->sd->rng);\n\n    LOG_INFO(\"sampling using %s method\", sampling_methods_str[sample_method]);\n    struct ggml_tensor* x_0 = sd_ctx->sd->sample(work_ctx,\n                                                 x_t,\n                                                 noise,\n                                                 cond,\n                                                 uncond,\n                                                 {},\n                                                 0.f,\n                                                 min_cfg,\n                                                 cfg_scale,\n                                                 0.f,\n                                                 sample_method,\n                                                 sigmas,\n                                                 -1,\n                                                 SDCondition(NULL, NULL, NULL));\n\n    int64_t t2 = ggml_time_ms();\n    LOG_INFO(\"sampling completed, taking %.2fs\", (t2 - t1) * 1.0f / 1000);\n    if (sd_ctx->sd->free_params_immediately) {\n        sd_ctx->sd->diffusion_model->free_params_buffer();\n    }\n\n    struct ggml_tensor* img = sd_ctx->sd->decode_first_stage(work_ctx, x_0);\n    if (sd_ctx->sd->free_params_immediately) {\n        sd_ctx->sd->first_stage_model->free_params_buffer();\n    }\n    if (img == NULL) {\n        ggml_free(work_ctx);\n        return NULL;\n    }\n\n    sd_image_t* result_images = (sd_image_t*)calloc(video_frames, sizeof(sd_image_t));\n    if (result_images == NULL) {\n        ggml_free(work_ctx);\n        return NULL;\n    }\n\n    for (size_t i = 0; i < video_frames; i++) {\n        auto img_i = ggml_view_3d(work_ctx, img, img->ne[0], img->ne[1], img->ne[2], img->nb[1], img->nb[2], img->nb[3] * i);\n\n        result_images[i].width   = width;\n        result_images[i].height  = height;\n        result_images[i].channel = 3;\n        result_images[i].data    = sd_tensor_to_image(img_i);\n    }\n    ggml_free(work_ctx);\n\n    int64_t t3 = ggml_time_ms();\n\n    LOG_INFO(\"img2vid completed in %.2fs\", (t3 - t0) * 1.0f / 1000);\n\n    return result_images;\n}\n"
        },
        {
          "name": "stable-diffusion.h",
          "type": "blob",
          "size": 7.4384765625,
          "content": "#ifndef __STABLE_DIFFUSION_H__\n#define __STABLE_DIFFUSION_H__\n\n#if defined(_WIN32) || defined(__CYGWIN__)\n#ifndef SD_BUILD_SHARED_LIB\n#define SD_API\n#else\n#ifdef SD_BUILD_DLL\n#define SD_API __declspec(dllexport)\n#else\n#define SD_API __declspec(dllimport)\n#endif\n#endif\n#else\n#if __GNUC__ >= 4\n#define SD_API __attribute__((visibility(\"default\")))\n#else\n#define SD_API\n#endif\n#endif\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n#include <stdbool.h>\n#include <stddef.h>\n#include <stdint.h>\n#include <string.h>\n\nenum rng_type_t {\n    STD_DEFAULT_RNG,\n    CUDA_RNG\n};\n\nenum sample_method_t {\n    EULER_A,\n    EULER,\n    HEUN,\n    DPM2,\n    DPMPP2S_A,\n    DPMPP2M,\n    DPMPP2Mv2,\n    IPNDM,\n    IPNDM_V,\n    LCM,\n    N_SAMPLE_METHODS\n};\n\nenum schedule_t {\n    DEFAULT,\n    DISCRETE,\n    KARRAS,\n    EXPONENTIAL,\n    AYS,\n    GITS,\n    N_SCHEDULES\n};\n\n// same as enum ggml_type\nenum sd_type_t {\n    SD_TYPE_F32  = 0,\n    SD_TYPE_F16  = 1,\n    SD_TYPE_Q4_0 = 2,\n    SD_TYPE_Q4_1 = 3,\n    // SD_TYPE_Q4_2 = 4, support has been removed\n    // SD_TYPE_Q4_3 = 5, support has been removed\n    SD_TYPE_Q5_0     = 6,\n    SD_TYPE_Q5_1     = 7,\n    SD_TYPE_Q8_0     = 8,\n    SD_TYPE_Q8_1     = 9,\n    SD_TYPE_Q2_K     = 10,\n    SD_TYPE_Q3_K     = 11,\n    SD_TYPE_Q4_K     = 12,\n    SD_TYPE_Q5_K     = 13,\n    SD_TYPE_Q6_K     = 14,\n    SD_TYPE_Q8_K     = 15,\n    SD_TYPE_IQ2_XXS  = 16,\n    SD_TYPE_IQ2_XS   = 17,\n    SD_TYPE_IQ3_XXS  = 18,\n    SD_TYPE_IQ1_S    = 19,\n    SD_TYPE_IQ4_NL   = 20,\n    SD_TYPE_IQ3_S    = 21,\n    SD_TYPE_IQ2_S    = 22,\n    SD_TYPE_IQ4_XS   = 23,\n    SD_TYPE_I8       = 24,\n    SD_TYPE_I16      = 25,\n    SD_TYPE_I32      = 26,\n    SD_TYPE_I64      = 27,\n    SD_TYPE_F64      = 28,\n    SD_TYPE_IQ1_M    = 29,\n    SD_TYPE_BF16     = 30,\n    SD_TYPE_Q4_0_4_4 = 31,\n    SD_TYPE_Q4_0_4_8 = 32,\n    SD_TYPE_Q4_0_8_8 = 33,\n    SD_TYPE_TQ1_0    = 34,\n    SD_TYPE_TQ2_0    = 35,\n    SD_TYPE_COUNT,\n};\n\nSD_API const char* sd_type_name(enum sd_type_t type);\n\nenum sd_log_level_t {\n    SD_LOG_DEBUG,\n    SD_LOG_INFO,\n    SD_LOG_WARN,\n    SD_LOG_ERROR\n};\n\ntypedef void (*sd_log_cb_t)(enum sd_log_level_t level, const char* text, void* data);\ntypedef void (*sd_progress_cb_t)(int step, int steps, float time, void* data);\n\nSD_API void sd_set_log_callback(sd_log_cb_t sd_log_cb, void* data);\nSD_API void sd_set_progress_callback(sd_progress_cb_t cb, void* data);\nSD_API int32_t get_num_physical_cores();\nSD_API const char* sd_get_system_info();\n\ntypedef struct {\n    uint32_t width;\n    uint32_t height;\n    uint32_t channel;\n    uint8_t* data;\n} sd_image_t;\n\ntypedef struct sd_ctx_t sd_ctx_t;\n\nSD_API sd_ctx_t* new_sd_ctx(const char* model_path,\n                            const char* clip_l_path,\n                            const char* clip_g_path,\n                            const char* t5xxl_path,\n                            const char* diffusion_model_path,\n                            const char* vae_path,\n                            const char* taesd_path,\n                            const char* control_net_path_c_str,\n                            const char* lora_model_dir,\n                            const char* embed_dir_c_str,\n                            const char* stacked_id_embed_dir_c_str,\n                            bool vae_decode_only,\n                            bool vae_tiling,\n                            bool free_params_immediately,\n                            int n_threads,\n                            enum sd_type_t wtype,\n                            enum rng_type_t rng_type,\n                            enum schedule_t s,\n                            bool keep_clip_on_cpu,\n                            bool keep_control_net_cpu,\n                            bool keep_vae_on_cpu,\n                            bool diffusion_flash_attn);\n\nSD_API void free_sd_ctx(sd_ctx_t* sd_ctx);\n\nSD_API sd_image_t* txt2img(sd_ctx_t* sd_ctx,\n                           const char* prompt,\n                           const char* negative_prompt,\n                           int clip_skip,\n                           float cfg_scale,\n                           float guidance,\n                           int width,\n                           int height,\n                           enum sample_method_t sample_method,\n                           int sample_steps,\n                           int64_t seed,\n                           int batch_count,\n                           const sd_image_t* control_cond,\n                           float control_strength,\n                           float style_strength,\n                           bool normalize_input,\n                           const char* input_id_images_path,\n                           int* skip_layers,\n                           size_t skip_layers_count,\n                           float slg_scale,\n                           float skip_layer_start,\n                           float skip_layer_end);\n\nSD_API sd_image_t* img2img(sd_ctx_t* sd_ctx,\n                           sd_image_t init_image,\n                           sd_image_t mask_image,\n                           const char* prompt,\n                           const char* negative_prompt,\n                           int clip_skip,\n                           float cfg_scale,\n                           float guidance,\n                           int width,\n                           int height,\n                           enum sample_method_t sample_method,\n                           int sample_steps,\n                           float strength,\n                           int64_t seed,\n                           int batch_count,\n                           const sd_image_t* control_cond,\n                           float control_strength,\n                           float style_strength,\n                           bool normalize_input,\n                           const char* input_id_images_path,\n                           int* skip_layers,\n                           size_t skip_layers_count,\n                           float slg_scale,\n                           float skip_layer_start,\n                           float skip_layer_end);\n\nSD_API sd_image_t* img2vid(sd_ctx_t* sd_ctx,\n                           sd_image_t init_image,\n                           int width,\n                           int height,\n                           int video_frames,\n                           int motion_bucket_id,\n                           int fps,\n                           float augmentation_level,\n                           float min_cfg,\n                           float cfg_scale,\n                           enum sample_method_t sample_method,\n                           int sample_steps,\n                           float strength,\n                           int64_t seed);\n\ntypedef struct upscaler_ctx_t upscaler_ctx_t;\n\nSD_API upscaler_ctx_t* new_upscaler_ctx(const char* esrgan_path,\n                                        int n_threads);\nSD_API void free_upscaler_ctx(upscaler_ctx_t* upscaler_ctx);\n\nSD_API sd_image_t upscale(upscaler_ctx_t* upscaler_ctx, sd_image_t input_image, uint32_t upscale_factor);\n\nSD_API bool convert(const char* input_path, const char* vae_path, const char* output_path, enum sd_type_t output_type);\n\nSD_API uint8_t* preprocess_canny(uint8_t* img,\n                                 int width,\n                                 int height,\n                                 float high_threshold,\n                                 float low_threshold,\n                                 float weak,\n                                 float strong,\n                                 bool inverse);\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif  // __STABLE_DIFFUSION_H__\n"
        },
        {
          "name": "t5.hpp",
          "type": "blob",
          "size": 39.376953125,
          "content": "#ifndef __T5_HPP__\n#define __T5_HPP__\n\n#include <float.h>\n#include <limits>\n#include <map>\n#include <memory>\n#include <regex>\n#include <sstream>\n#include <string>\n#include <unordered_map>\n\n#include \"darts.h\"\n#include \"ggml_extend.hpp\"\n#include \"json.hpp\"\n#include \"model.h\"\n\n// Port from: https://github.com/google/sentencepiece/blob/master/src/unigram_model.h\n// and https://github.com/google/sentencepiece/blob/master/src/unigram_model.h.\n// Original License: https://github.com/google/sentencepiece/blob/master/LICENSE\n//\n// Since tokenization is not the bottleneck in SD, performance was not a major consideration\n// during the migration.\nclass MetaspacePreTokenizer {\nprivate:\n    std::string replacement;\n    bool add_prefix_space;\n\npublic:\n    MetaspacePreTokenizer(const std::string replacement = \" \", bool add_prefix_space = true)\n        : replacement(replacement), add_prefix_space(add_prefix_space) {}\n\n    std::string tokenize(const std::string& input) const {\n        std::string tokens;\n        std::stringstream ss(input);\n\n        if (add_prefix_space) {\n            tokens += replacement;\n        }\n\n        std::string token;\n        bool firstToken = true;\n        while (std::getline(ss, token, ' ')) {\n            if (!firstToken)\n                tokens += replacement + token;\n            else\n                tokens += token;\n\n            firstToken = false;\n        }\n\n        return tokens;\n    }\n};\n\nusing EncodeResult = std::vector<std::pair<std::string, int>>;\nclass T5UniGramTokenizer {\npublic:\n    enum Status {\n        OK,\n        NO_PIECES_LOADED,\n        NO_ENTRY_FOUND,\n        BUILD_DOUBLE_ARRAY_FAILED,\n        PIECE_ALREADY_DEFINED,\n        INVLIAD_JSON\n    };\n\nprotected:\n    MetaspacePreTokenizer pre_tokenizer;\n\n    // all <piece, score> pairs\n    std::vector<std::pair<std::string, float>> piece_score_pairs;\n\n    float min_score_ = 0.0;\n    float max_score_ = 0.0;\n    std::unique_ptr<Darts::DoubleArray> trie_;\n\n    // Maximum size of the return value of Trie, which corresponds\n    // to the maximum size of shared common prefix in the sentence pieces.\n    int trie_results_size_;\n    // unknown id.\n    int unk_id_            = 2;\n    std::string eos_token_ = \"</s>\";\n    int eos_id_            = 1;\n    int pad_id_            = 0;\n    // status.\n    Status status_ = OK;\n\n    float kUnkPenalty = 10.0;\n\n    std::string replacement;\n    bool add_prefix_space = true;\n\n    void InitializePieces(const std::string& json_str) {\n        nlohmann::json data;\n\n        try {\n            data = nlohmann::json::parse(json_str);\n        } catch (const nlohmann::json::parse_error& e) {\n            status_ = INVLIAD_JSON;\n            return;\n        }\n        if (!data.contains(\"model\")) {\n            status_ = INVLIAD_JSON;\n            return;\n        }\n        nlohmann::json model = data[\"model\"];\n        if (!model.contains(\"vocab\")) {\n            status_ = INVLIAD_JSON;\n            return;\n        }\n        if (model.contains(\"unk_id\")) {\n            unk_id_ = model[\"unk_id\"];\n        }\n\n        replacement      = data[\"pre_tokenizer\"][\"replacement\"];\n        add_prefix_space = data[\"pre_tokenizer\"][\"add_prefix_space\"];\n\n        pre_tokenizer = MetaspacePreTokenizer(replacement, add_prefix_space);\n\n        for (const auto& item : model[\"vocab\"]) {\n            if (item.size() != 2 || !item[0].is_string() || !item[1].is_number_float()) {\n                status_ = INVLIAD_JSON;\n                return;\n            }\n            std::string piece = item[0];\n            float score       = item[1];\n            piece_score_pairs.emplace_back(piece, score);\n        }\n    }\n\n    // Builds a Trie index.\n    void BuildTrie(std::vector<std::pair<std::string, int>>* pieces) {\n        if (status_ != OK)\n            return;\n\n        if (pieces->empty()) {\n            status_ = NO_PIECES_LOADED;\n            return;\n        }\n\n        // sort by sentencepiece since DoubleArray::build()\n        // only accepts sorted strings.\n        sort(pieces->begin(), pieces->end());\n\n        // Makes key/value set for DoubleArrayTrie.\n        std::vector<const char*> key(pieces->size());\n        std::vector<int> value(pieces->size());\n        for (size_t i = 0; i < pieces->size(); ++i) {\n            key[i]   = (*pieces)[i].first.data();  // sorted piece.\n            value[i] = (*pieces)[i].second;        // vocab_id\n        }\n\n        trie_ = std::unique_ptr<Darts::DoubleArray>(new Darts::DoubleArray());\n        if (trie_->build(key.size(), const_cast<char**>(&key[0]), nullptr,\n                         &value[0]) != 0) {\n            status_ = BUILD_DOUBLE_ARRAY_FAILED;\n            return;\n        }\n\n        // Computes the maximum number of shared prefixes in the trie.\n        const int kMaxTrieResultsSize = 1024;\n        std::vector<Darts::DoubleArray::result_pair_type> results(\n            kMaxTrieResultsSize);\n        trie_results_size_ = 0;\n        for (const auto& p : *pieces) {\n            const int num_nodes = trie_->commonPrefixSearch(\n                p.first.data(), results.data(), results.size(), p.first.size());\n            trie_results_size_ = std::max(trie_results_size_, num_nodes);\n        }\n\n        if (trie_results_size_ == 0)\n            status_ = NO_ENTRY_FOUND;\n    }\n\n    // Non-virtual (inlined) implementation for faster execution.\n    inline float GetScoreInlined(int id) const {\n        return piece_score_pairs[id].second;\n    }\n\n    inline bool IsUnusedInlined(int id) const {\n        return false;  // TODO\n    }\n\n    inline bool IsUserDefinedInlined(int id) const {\n        return false;  // TODO\n    }\n\n    inline size_t OneCharLen(const char* src) const {\n        return \"\\1\\1\\1\\1\\1\\1\\1\\1\\1\\1\\1\\1\\2\\2\\3\\4\"[(*src & 0xFF) >> 4];\n    }\n\n    // The optimized Viterbi encode.\n    // Main differences from the original function:\n    // 1. Memorizes the best path at each postion so far,\n    // 2. No need to store the Lattice nodes,\n    // 3. Works in utf-8 directly,\n    // 4. Defines a new struct with fewer fields than Lattice,\n    // 5. Does not depend on `class Lattice` nor call `SetSentence()`,\n    // `PopulateNodes()`, or `Viterbi()`. It does everything in one function.\n    // For detailed explanations please see the comments inside the function body.\n    EncodeResult EncodeOptimized(const std::string& normalized) const {\n        // An optimized Viterbi algorithm for unigram language models. Benchmarking\n        // results show that it generates almost identical outputs and achieves 2.1x\n        // speedup on average for 102 languages compared to the original\n        // implementation. It's based on the following three ideas:\n        //\n        // 1. Because it uses the *unigram* model:\n        //     best_score(x1, x2, …, xt) = best_score(x1, x2, …, x{t-1}) + score(xt)\n        // Deciding the best path (and score) can be decoupled into two isolated\n        // terms: (a) the best path ended before the last token `best_score(x1, x2, …,\n        // x{t-1})`, and (b) the last token and its `score(xt)`. The two terms are\n        // not related to each other at all.\n        //\n        // Therefore, we can compute once and store the *best_path ending at\n        // each character position*. In this way, when we know best_path_ends_at[M],\n        // we can reuse it to compute all the best_path_ends_at_[...] where the last\n        // token starts at the same character position M.\n        //\n        // This improves the time complexity from O(n*k*k) to O(n*k) because it\n        // eliminates the extra loop of recomputing the best path ending at the same\n        // position, where n is the input length and k is the maximum number of tokens\n        // that can be recognized starting at each position.\n        //\n        // 2. Again, because it uses the *unigram* model, we don’t need to actually\n        // store the lattice nodes. We still recognize all the tokens and lattice\n        // nodes from the input, but along identifying them, we use and discard them\n        // on the fly. There is no need to actually store them for best path Viterbi\n        // decoding. The only thing we need to store is the best_path ending at\n        // each character position.\n        //\n        // This improvement reduces the things needed to store in memory from O(n*k)\n        // to O(n), where n is the input length and k is the maximum number of tokens\n        // that can be recognized starting at each position.\n        //\n        // It also avoids the need of dynamic-size lattice node pool, because the\n        // number of things to store is fixed as n.\n        //\n        // 3. SentencePiece is designed to work with unicode, taking utf-8 encoding\n        // inputs. In the original implementation, the lattice positions are based on\n        // unicode positions. A mapping from unicode position to the utf-8 position is\n        // maintained to recover the utf-8 string piece.\n        //\n        // We found that it is sufficient and beneficial to directly work with utf-8\n        // positions:\n        //\n        // Firstly, it saves the conversion and mapping between unicode positions and\n        // utf-8 positions.\n        //\n        // Secondly, it reduces the number of fields we need to maintain in the\n        // node/path structure. Specifically, there are 8 fields defined in\n        // `Lattice::Node` used by the original encoder, but here in the optimized\n        // encoder we only need to define 3 fields in `BestPathNode`.\n\n        if (status() != OK || normalized.empty()) {\n            return {};\n        }\n        // Represents the last node of the best path.\n        struct BestPathNode {\n            int id = -1;  // The vocab id. (maybe -1 for UNK)\n            float best_path_score =\n                0;  // The total score of the best path ending at this node.\n            int starts_at =\n                -1;  // The starting position (in utf-8) of this node. The entire best\n                     // path can be constructed by backtracking along this link.\n        };\n        const int size        = normalized.size();\n        const float unk_score = min_score() - kUnkPenalty;\n        // The ends are exclusive.\n        std::vector<BestPathNode> best_path_ends_at(size + 1);\n        // Generate lattice on-the-fly (not stored) and update best_path_ends_at.\n        int starts_at = 0;\n        while (starts_at < size) {\n            std::size_t node_pos = 0;\n            std::size_t key_pos  = starts_at;\n            const auto best_path_score_till_here =\n                best_path_ends_at[starts_at].best_path_score;\n            bool has_single_node = false;\n            const int mblen =\n                std::min<int>(OneCharLen(normalized.data() + starts_at),\n                              size - starts_at);\n            while (key_pos < size) {\n                const int ret =\n                    trie_->traverse(normalized.data(), node_pos, key_pos, key_pos + 1);\n                if (ret == -2)\n                    break;\n                if (ret >= 0) {\n                    if (IsUnusedInlined(ret))\n                        continue;\n                    // Update the best path node.\n                    auto& target_node = best_path_ends_at[key_pos];\n                    const auto length = (key_pos - starts_at);\n                    // User defined symbol receives extra bonus to always be selected.\n                    const auto score = IsUserDefinedInlined(ret)\n                                           ? (length * max_score_ - 0.1)\n                                           : GetScoreInlined(ret);\n                    const auto candidate_best_path_score =\n                        score + best_path_score_till_here;\n                    if (target_node.starts_at == -1 ||\n                        candidate_best_path_score > target_node.best_path_score) {\n                        target_node.best_path_score = candidate_best_path_score;\n                        target_node.starts_at       = starts_at;\n                        target_node.id              = ret;\n                    }\n                    if (!has_single_node && length == mblen) {\n                        has_single_node = true;\n                    }\n                }\n            }\n            if (!has_single_node) {\n                auto& target_node = best_path_ends_at[starts_at + mblen];\n                const auto candidate_best_path_score =\n                    unk_score + best_path_score_till_here;\n                if (target_node.starts_at == -1 ||\n                    candidate_best_path_score > target_node.best_path_score) {\n                    target_node.best_path_score = candidate_best_path_score;\n                    target_node.starts_at       = starts_at;\n                    target_node.id              = unk_id_;\n                }\n            }\n            // Move by one unicode character.\n            starts_at += mblen;\n        }\n        // Backtrack to identify the best path.\n        EncodeResult results;\n        int ends_at = size;\n        while (ends_at > 0) {\n            const auto& node = best_path_ends_at[ends_at];\n            results.emplace_back(\n                normalized.substr(node.starts_at, ends_at - node.starts_at), node.id);\n            ends_at = node.starts_at;\n        }\n        std::reverse(results.begin(), results.end());\n        return results;\n    }\n\npublic:\n    explicit T5UniGramTokenizer(const std::string& json_str = \"\") {\n        if (json_str.size() != 0) {\n            InitializePieces(json_str);\n        } else {\n            InitializePieces(ModelLoader::load_t5_tokenizer_json());\n        }\n\n        min_score_ = FLT_MAX;\n        max_score_ = FLT_MIN;\n\n        std::vector<std::pair<std::string, int>> pieces;\n        for (int i = 0; i < piece_score_pairs.size(); i++) {\n            const auto& sp = piece_score_pairs[i];\n\n            min_score_ = std::min(min_score_, sp.second);\n            max_score_ = std::max(max_score_, sp.second);\n\n            pieces.emplace_back(sp.first, i);\n        }\n\n        BuildTrie(&pieces);\n    }\n    ~T5UniGramTokenizer(){};\n\n    std::string Normalize(const std::string& input) const {\n        // Ref: https://github.com/huggingface/tokenizers/blob/1ff56c0c70b045f0cd82da1af9ac08cd4c7a6f9f/bindings/python/py_src/tokenizers/implementations/sentencepiece_unigram.py#L29\n        // TODO: nmt-nfkc\n        std::string normalized = std::regex_replace(input, std::regex(\" {2,}\"), \" \");\n        return normalized;\n    }\n\n    std::vector<int> Encode(const std::string& input, bool append_eos_if_not_present = true) const {\n        std::string normalized = Normalize(input);\n        normalized             = pre_tokenizer.tokenize(normalized);\n        EncodeResult result    = EncodeOptimized(normalized);\n        if (result.size() > 0 && append_eos_if_not_present) {\n            auto item = result[result.size() - 1];\n            if (item.first != eos_token_) {\n                result.emplace_back(eos_token_, eos_id_);\n            }\n        }\n        std::vector<int> tokens;\n        for (auto item : result) {\n            tokens.push_back(item.second);\n        }\n        return tokens;\n    }\n\n    void pad_tokens(std::vector<int>& tokens,\n                    std::vector<float>& weights,\n                    size_t max_length = 0,\n                    bool padding      = false) {\n        if (max_length > 0 && padding) {\n            size_t orig_token_num = tokens.size() - 1;\n            size_t n              = std::ceil(orig_token_num * 1.0 / (max_length - 1));\n            if (n == 0) {\n                n = 1;\n            }\n            size_t length = max_length * n;\n            LOG_DEBUG(\"token length: %llu\", length);\n            std::vector<int> new_tokens;\n            std::vector<float> new_weights;\n            int token_idx = 0;\n            for (int i = 0; i < length; i++) {\n                if (token_idx >= orig_token_num) {\n                    break;\n                }\n                if (i % max_length == max_length - 1) {\n                    new_tokens.push_back(eos_id_);\n                    new_weights.push_back(1.0);\n                } else {\n                    new_tokens.push_back(tokens[token_idx]);\n                    new_weights.push_back(weights[token_idx]);\n                    token_idx++;\n                }\n            }\n\n            new_tokens.push_back(eos_id_);\n            new_weights.push_back(1.0);\n            tokens  = new_tokens;\n            weights = new_weights;\n\n            if (padding) {\n                int pad_token_id = pad_id_;\n                tokens.insert(tokens.end(), length - tokens.size(), pad_token_id);\n                weights.insert(weights.end(), length - weights.size(), 1.0);\n            }\n        }\n    }\n\n    // Returns the minimum score in sentence pieces.\n    // min_score() - 10 is used for the cost of unknown sentence.\n    float min_score() const { return min_score_; }\n\n    // Returns the maximum score in sentence pieces.\n    // max_score() is used for the cost of user defined symbols.\n    float max_score() const { return max_score_; }\n\n    Status status() const { return status_; }\n};\n\nclass T5LayerNorm : public UnaryBlock {\nprotected:\n    int64_t hidden_size;\n    float eps;\n\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix = \"\") {\n        enum ggml_type wtype = GGML_TYPE_F32;  //(tensor_types.find(prefix + \"weight\") != tensor_types.end()) ? tensor_types[prefix + \"weight\"] : GGML_TYPE_F32;\n        params[\"weight\"]     = ggml_new_tensor_1d(ctx, wtype, hidden_size);\n    }\n\npublic:\n    T5LayerNorm(int64_t hidden_size,\n                float eps = 1e-06f)\n        : hidden_size(hidden_size),\n          eps(eps) {}\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        struct ggml_tensor* w = params[\"weight\"];\n        x                     = ggml_rms_norm(ctx, x, eps);\n        x                     = ggml_mul(ctx, x, w);\n        return x;\n    }\n};\n\nstruct T5DenseActDense : public UnaryBlock {\npublic:\n    T5DenseActDense(int64_t model_dim, int64_t ff_dim) {\n        blocks[\"wi\"] = std::shared_ptr<GGMLBlock>(new Linear(model_dim, ff_dim, false));\n        blocks[\"wo\"] = std::shared_ptr<GGMLBlock>(new Linear(ff_dim, model_dim, false));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [N, n_token, model_dim]\n        auto wi = std::dynamic_pointer_cast<Linear>(blocks[\"wi\"]);\n        auto wo = std::dynamic_pointer_cast<Linear>(blocks[\"wo\"]);\n\n        x = wi->forward(ctx, x);\n        x = ggml_relu_inplace(ctx, x);\n        x = wo->forward(ctx, x);\n        return x;\n    }\n};\n\nstruct T5DenseGatedActDense : public UnaryBlock {\npublic:\n    T5DenseGatedActDense(int64_t model_dim, int64_t ff_dim) {\n        blocks[\"wi_0\"] = std::shared_ptr<GGMLBlock>(new Linear(model_dim, ff_dim, false));\n        blocks[\"wi_1\"] = std::shared_ptr<GGMLBlock>(new Linear(model_dim, ff_dim, false));\n        blocks[\"wo\"]   = std::shared_ptr<GGMLBlock>(new Linear(ff_dim, model_dim, false));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [N, n_token, model_dim]\n        auto wi_0 = std::dynamic_pointer_cast<Linear>(blocks[\"wi_0\"]);\n        auto wi_1 = std::dynamic_pointer_cast<Linear>(blocks[\"wi_1\"]);\n        auto wo   = std::dynamic_pointer_cast<Linear>(blocks[\"wo\"]);\n\n        auto hidden_gelu   = ggml_gelu_inplace(ctx, wi_0->forward(ctx, x));\n        auto hidden_linear = wi_1->forward(ctx, x);\n        x                  = ggml_mul_inplace(ctx, hidden_gelu, hidden_linear);\n        x                  = wo->forward(ctx, x);\n        return x;\n    }\n};\n\nstruct T5LayerFF : public UnaryBlock {\npublic:\n    T5LayerFF(int64_t model_dim, int64_t ff_dim) {\n        blocks[\"DenseReluDense\"] = std::shared_ptr<GGMLBlock>(new T5DenseGatedActDense(model_dim, ff_dim));\n        blocks[\"layer_norm\"]     = std::shared_ptr<GGMLBlock>(new T5LayerNorm(model_dim));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [N, n_token, model_dim]\n        auto DenseReluDense = std::dynamic_pointer_cast<T5DenseGatedActDense>(blocks[\"DenseReluDense\"]);\n        auto layer_norm     = std::dynamic_pointer_cast<T5LayerNorm>(blocks[\"layer_norm\"]);\n\n        auto forwarded_states = layer_norm->forward(ctx, x);\n        forwarded_states      = DenseReluDense->forward(ctx, forwarded_states);\n        x                     = ggml_add_inplace(ctx, forwarded_states, x);\n        return x;\n    }\n};\n\nclass T5Attention : public GGMLBlock {\nprotected:\n    int64_t model_dim;\n    int64_t inner_dim;\n    int64_t num_heads;\n    bool using_relative_attention_bias;\n    int64_t relative_attention_num_buckets  = 32;\n    int64_t relative_attention_max_distance = 128;\n\npublic:\n    T5Attention(int64_t model_dim,\n                int64_t inner_dim,\n                int64_t num_heads,\n                bool using_relative_attention_bias = false)\n        : model_dim(model_dim),\n          inner_dim(inner_dim),\n          num_heads(num_heads),\n          using_relative_attention_bias(using_relative_attention_bias) {\n        blocks[\"q\"] = std::shared_ptr<GGMLBlock>(new Linear(model_dim, inner_dim, false));\n        blocks[\"k\"] = std::shared_ptr<GGMLBlock>(new Linear(model_dim, inner_dim, false));\n        blocks[\"v\"] = std::shared_ptr<GGMLBlock>(new Linear(model_dim, inner_dim, false));\n        blocks[\"o\"] = std::shared_ptr<GGMLBlock>(new Linear(inner_dim, model_dim, false));\n        if (using_relative_attention_bias) {\n            blocks[\"relative_attention_bias\"] = std::shared_ptr<GGMLBlock>(new Embedding(relative_attention_num_buckets, num_heads));\n        }\n    }\n\n    struct ggml_tensor* compute_bias(struct ggml_context* ctx,\n                                     struct ggml_tensor* relative_position_bucket) {\n        auto relative_attention_bias = std::dynamic_pointer_cast<Embedding>(blocks[\"relative_attention_bias\"]);\n\n        auto values = relative_attention_bias->forward(ctx, relative_position_bucket);  // shape (query_length, key_length, num_heads)\n        values      = ggml_cont(ctx, ggml_permute(ctx, values, 2, 0, 1, 3));            // shape (1, num_heads, query_length, key_length)\n        return values;\n    }\n\n    // x: [N, n_token, model_dim]\n    std::pair<struct ggml_tensor*, struct ggml_tensor*> forward(struct ggml_context* ctx,\n                                                                struct ggml_tensor* x,\n                                                                struct ggml_tensor* past_bias                = NULL,\n                                                                struct ggml_tensor* mask                     = NULL,\n                                                                struct ggml_tensor* relative_position_bucket = NULL) {\n        auto q_proj   = std::dynamic_pointer_cast<Linear>(blocks[\"q\"]);\n        auto k_proj   = std::dynamic_pointer_cast<Linear>(blocks[\"k\"]);\n        auto v_proj   = std::dynamic_pointer_cast<Linear>(blocks[\"v\"]);\n        auto out_proj = std::dynamic_pointer_cast<Linear>(blocks[\"o\"]);\n\n        int64_t n_head = num_heads;\n        int64_t d_head = inner_dim / n_head;\n\n        auto q = q_proj->forward(ctx, x);\n        auto k = k_proj->forward(ctx, x);\n        auto v = v_proj->forward(ctx, x);\n\n        if (using_relative_attention_bias && relative_position_bucket != NULL) {\n            past_bias = compute_bias(ctx, relative_position_bucket);\n        }\n        if (past_bias != NULL) {\n            if (mask != NULL) {\n                mask = ggml_add(ctx, mask, past_bias);\n            } else {\n                mask = past_bias;\n            }\n        }\n\n        k = ggml_scale_inplace(ctx, k, sqrt(d_head));\n\n        x = ggml_nn_attention_ext(ctx, q, k, v, num_heads, mask);  // [N, n_token, d_head * n_head]\n\n        x = out_proj->forward(ctx, x);  // [N, n_token, model_dim]\n        return {x, past_bias};\n    }\n};\n\nstruct T5LayerSelfAttention : public GGMLBlock {\npublic:\n    T5LayerSelfAttention(int64_t model_dim,\n                         int64_t inner_dim,\n                         int64_t ff_dim,\n                         int64_t num_heads,\n                         bool using_relative_attention_bias) {\n        blocks[\"SelfAttention\"] = std::shared_ptr<GGMLBlock>(new T5Attention(model_dim, inner_dim, num_heads, using_relative_attention_bias));\n        blocks[\"layer_norm\"]    = std::shared_ptr<GGMLBlock>(new T5LayerNorm(model_dim));\n    }\n\n    std::pair<struct ggml_tensor*, struct ggml_tensor*> forward(struct ggml_context* ctx,\n                                                                struct ggml_tensor* x,\n                                                                struct ggml_tensor* past_bias                = NULL,\n                                                                struct ggml_tensor* mask                     = NULL,\n                                                                struct ggml_tensor* relative_position_bucket = NULL) {\n        // x: [N, n_token, model_dim]\n        auto SelfAttention = std::dynamic_pointer_cast<T5Attention>(blocks[\"SelfAttention\"]);\n        auto layer_norm    = std::dynamic_pointer_cast<T5LayerNorm>(blocks[\"layer_norm\"]);\n\n        auto normed_hidden_state = layer_norm->forward(ctx, x);\n        auto ret                 = SelfAttention->forward(ctx, normed_hidden_state, past_bias, mask, relative_position_bucket);\n        auto output              = ret.first;\n        past_bias                = ret.second;\n\n        x = ggml_add_inplace(ctx, output, x);\n        return {x, past_bias};\n    }\n};\n\nstruct T5Block : public GGMLBlock {\npublic:\n    T5Block(int64_t model_dim, int64_t inner_dim, int64_t ff_dim, int64_t num_heads, bool using_relative_attention_bias) {\n        blocks[\"layer.0\"] = std::shared_ptr<GGMLBlock>(new T5LayerSelfAttention(model_dim, inner_dim, ff_dim, num_heads, using_relative_attention_bias));\n        blocks[\"layer.1\"] = std::shared_ptr<GGMLBlock>(new T5LayerFF(model_dim, ff_dim));\n    }\n\n    std::pair<struct ggml_tensor*, struct ggml_tensor*> forward(struct ggml_context* ctx,\n                                                                struct ggml_tensor* x,\n                                                                struct ggml_tensor* past_bias                = NULL,\n                                                                struct ggml_tensor* mask                     = NULL,\n                                                                struct ggml_tensor* relative_position_bucket = NULL) {\n        // x: [N, n_token, model_dim]\n        auto layer_0 = std::dynamic_pointer_cast<T5LayerSelfAttention>(blocks[\"layer.0\"]);\n        auto layer_1 = std::dynamic_pointer_cast<T5LayerFF>(blocks[\"layer.1\"]);\n\n        auto ret  = layer_0->forward(ctx, x, past_bias, mask, relative_position_bucket);\n        x         = ret.first;\n        past_bias = ret.second;\n        x         = layer_1->forward(ctx, x);\n        return {x, past_bias};\n    }\n};\n\nstruct T5Stack : public GGMLBlock {\n    int64_t num_layers;\n\npublic:\n    T5Stack(int64_t num_layers,\n            int64_t model_dim,\n            int64_t inner_dim,\n            int64_t ff_dim,\n            int64_t num_heads)\n        : num_layers(num_layers) {\n        for (int i = 0; i < num_layers; i++) {\n            blocks[\"block.\" + std::to_string(i)] = std::shared_ptr<GGMLBlock>(new T5Block(model_dim, inner_dim, ff_dim, num_heads, i == 0));\n        }\n\n        blocks[\"final_layer_norm\"] = std::shared_ptr<GGMLBlock>(new T5LayerNorm(model_dim));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* x,\n                                struct ggml_tensor* past_bias                = NULL,\n                                struct ggml_tensor* attention_mask           = NULL,\n                                struct ggml_tensor* relative_position_bucket = NULL) {\n        // x: [N, n_token, model_dim]\n        for (int i = 0; i < num_layers; i++) {\n            auto block = std::dynamic_pointer_cast<T5Block>(blocks[\"block.\" + std::to_string(i)]);\n\n            auto ret  = block->forward(ctx, x, past_bias, attention_mask, relative_position_bucket);\n            x         = ret.first;\n            past_bias = ret.second;\n        }\n\n        auto final_layer_norm = std::dynamic_pointer_cast<T5LayerNorm>(blocks[\"final_layer_norm\"]);\n\n        x = final_layer_norm->forward(ctx, x);\n        return x;\n    }\n};\n\nstruct T5 : public GGMLBlock {\npublic:\n    T5(int64_t num_layers,\n       int64_t model_dim,\n       int64_t ff_dim,\n       int64_t num_heads,\n       int64_t vocab_size) {\n        blocks[\"encoder\"] = std::shared_ptr<GGMLBlock>(new T5Stack(num_layers, model_dim, model_dim, ff_dim, num_heads));\n        blocks[\"shared\"]  = std::shared_ptr<GGMLBlock>(new Embedding(vocab_size, model_dim));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* input_ids,\n                                struct ggml_tensor* past_bias                = NULL,\n                                struct ggml_tensor* attention_mask           = NULL,\n                                struct ggml_tensor* relative_position_bucket = NULL) {\n        // input_ids: [N, n_token]\n\n        auto shared  = std::dynamic_pointer_cast<Embedding>(blocks[\"shared\"]);\n        auto encoder = std::dynamic_pointer_cast<T5Stack>(blocks[\"encoder\"]);\n\n        auto x = shared->forward(ctx, input_ids);\n        x      = encoder->forward(ctx, x, past_bias, attention_mask, relative_position_bucket);\n        return x;\n    }\n};\n\nstruct T5Runner : public GGMLRunner {\n    T5 model;\n    std::vector<int> relative_position_bucket_vec;\n\n    T5Runner(ggml_backend_t backend,\n             std::map<std::string, enum ggml_type>& tensor_types,\n             const std::string prefix,\n             int64_t num_layers = 24,\n             int64_t model_dim  = 4096,\n             int64_t ff_dim     = 10240,\n             int64_t num_heads  = 64,\n             int64_t vocab_size = 32128)\n        : GGMLRunner(backend), model(num_layers, model_dim, ff_dim, num_heads, vocab_size) {\n        model.init(params_ctx, tensor_types, prefix);\n    }\n\n    std::string get_desc() {\n        return \"t5\";\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors, const std::string prefix) {\n        model.get_param_tensors(tensors, prefix);\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* input_ids,\n                                struct ggml_tensor* relative_position_bucket) {\n        size_t N       = input_ids->ne[1];\n        size_t n_token = input_ids->ne[0];\n\n        auto hidden_states = model.forward(ctx, input_ids, NULL, NULL, relative_position_bucket);  // [N, n_token, model_dim]\n        return hidden_states;\n    }\n\n    struct ggml_cgraph* build_graph(struct ggml_tensor* input_ids) {\n        struct ggml_cgraph* gf = ggml_new_graph(compute_ctx);\n\n        input_ids = to_backend(input_ids);\n\n        relative_position_bucket_vec = compute_relative_position_bucket(input_ids->ne[0], input_ids->ne[0]);\n\n        // for (int i = 0; i < relative_position_bucket_vec.size(); i++) {\n        //     if (i % 77 == 0) {\n        //         printf(\"\\n\");\n        //     }\n        //     printf(\"%d \", relative_position_bucket_vec[i]);\n        // }\n\n        auto relative_position_bucket = ggml_new_tensor_2d(compute_ctx,\n                                                           GGML_TYPE_I32,\n                                                           input_ids->ne[0],\n                                                           input_ids->ne[0]);\n        set_backend_tensor_data(relative_position_bucket, relative_position_bucket_vec.data());\n\n        struct ggml_tensor* hidden_states = forward(compute_ctx, input_ids, relative_position_bucket);\n\n        ggml_build_forward_expand(gf, hidden_states);\n\n        return gf;\n    }\n\n    void compute(const int n_threads,\n                 struct ggml_tensor* input_ids,\n                 ggml_tensor** output,\n                 ggml_context* output_ctx = NULL) {\n        auto get_graph = [&]() -> struct ggml_cgraph* {\n            return build_graph(input_ids);\n        };\n        GGMLRunner::compute(get_graph, n_threads, true, output, output_ctx);\n    }\n\n    static std::vector<int> _relative_position_bucket(const std::vector<int>& relative_position,\n                                                      bool bidirectional = true,\n                                                      int num_buckets    = 32,\n                                                      int max_distance   = 128) {\n        std::vector<int> relative_buckets(relative_position.size(), 0);\n        std::vector<int> abs_relative_position = relative_position;\n\n        if (bidirectional) {\n            num_buckets = num_buckets / 2;\n            for (size_t i = 0; i < relative_position.size(); ++i) {\n                if (relative_position[i] > 0) {\n                    relative_buckets[i] += num_buckets;\n                }\n                abs_relative_position[i] = std::abs(relative_position[i]);\n            }\n        } else {\n            for (size_t i = 0; i < relative_position.size(); ++i) {\n                abs_relative_position[i] = std::max(-relative_position[i], 0);\n            }\n        }\n\n        int max_exact = num_buckets / 2;\n        std::vector<int> relative_position_if_large(relative_position.size(), 0);\n\n        for (size_t i = 0; i < relative_position.size(); ++i) {\n            if (abs_relative_position[i] < max_exact) {\n                relative_buckets[i] += abs_relative_position[i];\n            } else {\n                float log_pos                 = std::log(static_cast<float>(abs_relative_position[i]) / max_exact);\n                float log_base                = std::log(static_cast<float>(max_distance) / max_exact);\n                relative_position_if_large[i] = max_exact + static_cast<int>((log_pos / log_base) * (num_buckets - max_exact));\n                relative_position_if_large[i] = std::min(relative_position_if_large[i], num_buckets - 1);\n                relative_buckets[i] += relative_position_if_large[i];\n            }\n        }\n\n        return relative_buckets;\n    }\n\n    std::vector<int> compute_relative_position_bucket(int query_length,\n                                                      int key_length) {\n        std::vector<int> context_position(query_length);\n        std::vector<int> memory_position(key_length);\n\n        for (int i = 0; i < query_length; ++i) {\n            context_position[i] = i;\n        }\n        for (int i = 0; i < key_length; ++i) {\n            memory_position[i] = i;\n        }\n\n        std::vector<std::vector<int>> relative_position(query_length, std::vector<int>(key_length, 0));\n        for (int i = 0; i < query_length; ++i) {\n            for (int j = 0; j < key_length; ++j) {\n                relative_position[i][j] = memory_position[j] - context_position[i];\n            }\n        }\n\n        std::vector<int> relative_position_bucket;\n        for (int i = 0; i < query_length; ++i) {\n            std::vector<int> result = _relative_position_bucket(relative_position[i], true);\n            relative_position_bucket.insert(relative_position_bucket.end(), result.begin(), result.end());\n        }\n\n        return relative_position_bucket;\n    }\n};\n\nstruct T5Embedder {\n    T5UniGramTokenizer tokenizer;\n    T5Runner model;\n\n    static std::map<std::string, enum ggml_type> empty_tensor_types;\n\n    T5Embedder(ggml_backend_t backend,\n               std::map<std::string, enum ggml_type>& tensor_types = empty_tensor_types,\n               const std::string prefix                            = \"\",\n               int64_t num_layers                                  = 24,\n               int64_t model_dim                                   = 4096,\n               int64_t ff_dim                                      = 10240,\n               int64_t num_heads                                   = 64,\n               int64_t vocab_size                                  = 32128)\n        : model(backend, tensor_types, prefix, num_layers, model_dim, ff_dim, num_heads, vocab_size) {\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors, const std::string prefix) {\n        model.get_param_tensors(tensors, prefix);\n    }\n\n    void alloc_params_buffer() {\n        model.alloc_params_buffer();\n    }\n\n    std::pair<std::vector<int>, std::vector<float>> tokenize(std::string text,\n                                                             size_t max_length = 0,\n                                                             bool padding      = false) {\n        auto parsed_attention = parse_prompt_attention(text);\n\n        {\n            std::stringstream ss;\n            ss << \"[\";\n            for (const auto& item : parsed_attention) {\n                ss << \"['\" << item.first << \"', \" << item.second << \"], \";\n            }\n            ss << \"]\";\n            LOG_DEBUG(\"parse '%s' to %s\", text.c_str(), ss.str().c_str());\n        }\n\n        std::vector<int> tokens;\n        std::vector<float> weights;\n        for (const auto& item : parsed_attention) {\n            const std::string& curr_text = item.first;\n            float curr_weight            = item.second;\n            std::vector<int> curr_tokens = tokenizer.Encode(curr_text, false);\n            tokens.insert(tokens.end(), curr_tokens.begin(), curr_tokens.end());\n            weights.insert(weights.end(), curr_tokens.size(), curr_weight);\n        }\n\n        int EOS_TOKEN_ID = 1;\n        tokens.push_back(EOS_TOKEN_ID);\n        weights.push_back(1.0);\n\n        tokenizer.pad_tokens(tokens, weights, max_length, padding);\n\n        // for (int i = 0; i < tokens.size(); i++) {\n        //     std::cout << tokens[i] << \":\" << weights[i] << \", \";\n        // }\n        // std::cout << std::endl;\n\n        return {tokens, weights};\n    }\n\n    void test() {\n        struct ggml_init_params params;\n        params.mem_size   = static_cast<size_t>(10 * 1024 * 1024);  // 10 MB\n        params.mem_buffer = NULL;\n        params.no_alloc   = false;\n\n        struct ggml_context* work_ctx = ggml_init(params);\n        GGML_ASSERT(work_ctx != NULL);\n\n        {\n            // cpu f16: pass\n            // cpu f32: pass\n            // cuda f16: nan\n            // cuda f32: pass\n            // cuda q8_0: nan\n            // TODO: fix cuda nan\n            std::string text(\"a lovely cat\");\n            auto tokens_and_weights     = tokenize(text, 77, true);\n            std::vector<int>& tokens    = tokens_and_weights.first;\n            std::vector<float>& weights = tokens_and_weights.second;\n            for (auto token : tokens) {\n                printf(\"%d \", token);\n            }\n            printf(\"\\n\");\n            auto input_ids          = vector_to_ggml_tensor_i32(work_ctx, tokens);\n            struct ggml_tensor* out = NULL;\n\n            int t0 = ggml_time_ms();\n            model.compute(8, input_ids, &out, work_ctx);\n            int t1 = ggml_time_ms();\n\n            print_ggml_tensor(out);\n            LOG_DEBUG(\"t5 test done in %dms\", t1 - t0);\n        }\n    }\n\n    static void load_from_file_and_test(const std::string& file_path) {\n        // ggml_backend_t backend    = ggml_backend_cuda_init(0);\n        ggml_backend_t backend         = ggml_backend_cpu_init();\n        ggml_type model_data_type      = GGML_TYPE_F32;\n        std::shared_ptr<T5Embedder> t5 = std::shared_ptr<T5Embedder>(new T5Embedder(backend));\n        {\n            LOG_INFO(\"loading from '%s'\", file_path.c_str());\n\n            t5->alloc_params_buffer();\n            std::map<std::string, ggml_tensor*> tensors;\n            t5->get_param_tensors(tensors, \"\");\n\n            ModelLoader model_loader;\n            if (!model_loader.init_from_file(file_path)) {\n                LOG_ERROR(\"init model loader from file failed: '%s'\", file_path.c_str());\n                return;\n            }\n\n            bool success = model_loader.load_tensors(tensors, backend);\n\n            if (!success) {\n                LOG_ERROR(\"load tensors from model loader failed\");\n                return;\n            }\n\n            LOG_INFO(\"t5 model loaded\");\n        }\n        t5->test();\n    }\n};\n\n#endif  // __T5_HPP__"
        },
        {
          "name": "tae.hpp",
          "type": "blob",
          "size": 9.40234375,
          "content": "#ifndef __TAE_HPP__\n#define __TAE_HPP__\n\n#include \"ggml_extend.hpp\"\n\n#include \"model.h\"\n\n/*\n    ===================================    TinyAutoEncoder  ===================================\n    References:\n    https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoders/vae.py\n    https://github.com/madebyollin/taesd/blob/main/taesd.py\n\n*/\n\nclass TAEBlock : public UnaryBlock {\nprotected:\n    int n_in;\n    int n_out;\n\npublic:\n    TAEBlock(int n_in, int n_out)\n        : n_in(n_in), n_out(n_out) {\n        blocks[\"conv.0\"] = std::shared_ptr<GGMLBlock>(new Conv2d(n_in, n_out, {3, 3}, {1, 1}, {1, 1}));\n        blocks[\"conv.2\"] = std::shared_ptr<GGMLBlock>(new Conv2d(n_out, n_out, {3, 3}, {1, 1}, {1, 1}));\n        blocks[\"conv.4\"] = std::shared_ptr<GGMLBlock>(new Conv2d(n_out, n_out, {3, 3}, {1, 1}, {1, 1}));\n        if (n_in != n_out) {\n            blocks[\"skip\"] = std::shared_ptr<GGMLBlock>(new Conv2d(n_in, n_out, {1, 1}, {1, 1}, {1, 1}, {1, 1}, false));\n        }\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [n, n_in, h, w]\n        // return: [n, n_out, h, w]\n\n        auto conv_0 = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv.0\"]);\n        auto conv_2 = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv.2\"]);\n        auto conv_4 = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv.4\"]);\n\n        auto h = conv_0->forward(ctx, x);\n        h      = ggml_relu_inplace(ctx, h);\n        h      = conv_2->forward(ctx, h);\n        h      = ggml_relu_inplace(ctx, h);\n        h      = conv_4->forward(ctx, h);\n\n        if (n_in != n_out) {\n            auto skip = std::dynamic_pointer_cast<Conv2d>(blocks[\"skip\"]);\n            LOG_DEBUG(\"skip\");\n            x = skip->forward(ctx, x);\n        }\n\n        h = ggml_add(ctx, h, x);\n        h = ggml_relu_inplace(ctx, h);\n        return h;\n    }\n};\n\nclass TinyEncoder : public UnaryBlock {\n    int in_channels = 3;\n    int channels    = 64;\n    int z_channels  = 4;\n    int num_blocks  = 3;\n\npublic:\n    TinyEncoder(int z_channels = 4)\n        : z_channels(z_channels) {\n        int index                       = 0;\n        blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new Conv2d(in_channels, channels, {3, 3}, {1, 1}, {1, 1}));\n        blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new TAEBlock(channels, channels));\n\n        blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new Conv2d(channels, channels, {3, 3}, {2, 2}, {1, 1}, {1, 1}, false));\n        for (int i = 0; i < num_blocks; i++) {\n            blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new TAEBlock(channels, channels));\n        }\n\n        blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new Conv2d(channels, channels, {3, 3}, {2, 2}, {1, 1}, {1, 1}, false));\n        for (int i = 0; i < num_blocks; i++) {\n            blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new TAEBlock(channels, channels));\n        }\n\n        blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new Conv2d(channels, channels, {3, 3}, {2, 2}, {1, 1}, {1, 1}, false));\n        for (int i = 0; i < num_blocks; i++) {\n            blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new TAEBlock(channels, channels));\n        }\n\n        blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new Conv2d(channels, z_channels, {3, 3}, {1, 1}, {1, 1}));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [n, in_channels, h, w]\n        // return: [n, z_channels, h/8, w/8]\n\n        for (int i = 0; i < num_blocks * 3 + 6; i++) {\n            auto block = std::dynamic_pointer_cast<UnaryBlock>(blocks[std::to_string(i)]);\n\n            x = block->forward(ctx, x);\n        }\n\n        return x;\n    }\n};\n\nclass TinyDecoder : public UnaryBlock {\n    int z_channels   = 4;\n    int channels     = 64;\n    int out_channels = 3;\n    int num_blocks   = 3;\n\npublic:\n    TinyDecoder(int z_channels = 4)\n        : z_channels(z_channels) {\n        int index = 0;\n\n        blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new Conv2d(z_channels, channels, {3, 3}, {1, 1}, {1, 1}));\n        index++;  // nn.ReLU()\n\n        for (int i = 0; i < num_blocks; i++) {\n            blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new TAEBlock(channels, channels));\n        }\n        index++;  // nn.Upsample()\n        blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new Conv2d(channels, channels, {3, 3}, {1, 1}, {1, 1}, {1, 1}, false));\n\n        for (int i = 0; i < num_blocks; i++) {\n            blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new TAEBlock(channels, channels));\n        }\n        index++;  // nn.Upsample()\n        blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new Conv2d(channels, channels, {3, 3}, {1, 1}, {1, 1}, {1, 1}, false));\n\n        for (int i = 0; i < num_blocks; i++) {\n            blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new TAEBlock(channels, channels));\n        }\n        index++;  // nn.Upsample()\n        blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new Conv2d(channels, channels, {3, 3}, {1, 1}, {1, 1}, {1, 1}, false));\n\n        blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new TAEBlock(channels, channels));\n        blocks[std::to_string(index++)] = std::shared_ptr<GGMLBlock>(new Conv2d(channels, out_channels, {3, 3}, {1, 1}, {1, 1}));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* z) {\n        // z: [n, z_channels, h, w]\n        // return: [n, out_channels, h*8, w*8]\n\n        auto h = ggml_scale(ctx, z, 1.0f / 3.0f);\n        h      = ggml_tanh_inplace(ctx, h);\n        h      = ggml_scale(ctx, h, 3.0f);\n\n        for (int i = 0; i < num_blocks * 3 + 10; i++) {\n            if (blocks.find(std::to_string(i)) == blocks.end()) {\n                if (i == 1) {\n                    h = ggml_relu_inplace(ctx, h);\n                } else {\n                    h = ggml_upscale(ctx, h, 2);\n                }\n                continue;\n            }\n            auto block = std::dynamic_pointer_cast<UnaryBlock>(blocks[std::to_string(i)]);\n\n            h = block->forward(ctx, h);\n        }\n\n        return h;\n    }\n};\n\nclass TAESD : public GGMLBlock {\nprotected:\n    bool decode_only;\n\npublic:\n    TAESD(bool decode_only = true, SDVersion version = VERSION_SD1)\n        : decode_only(decode_only) {\n        int z_channels = 4;\n        if (sd_version_is_dit(version)) {\n            z_channels = 16;\n        }\n        blocks[\"decoder.layers\"] = std::shared_ptr<GGMLBlock>(new TinyDecoder(z_channels));\n\n        if (!decode_only) {\n            blocks[\"encoder.layers\"] = std::shared_ptr<GGMLBlock>(new TinyEncoder(z_channels));\n        }\n    }\n\n    struct ggml_tensor* decode(struct ggml_context* ctx, struct ggml_tensor* z) {\n        auto decoder = std::dynamic_pointer_cast<TinyDecoder>(blocks[\"decoder.layers\"]);\n        return decoder->forward(ctx, z);\n    }\n\n    struct ggml_tensor* encode(struct ggml_context* ctx, struct ggml_tensor* x) {\n        auto encoder = std::dynamic_pointer_cast<TinyEncoder>(blocks[\"encoder.layers\"]);\n        return encoder->forward(ctx, x);\n    }\n};\n\nstruct TinyAutoEncoder : public GGMLRunner {\n    TAESD taesd;\n    bool decode_only = false;\n\n    TinyAutoEncoder(ggml_backend_t backend,\n                    std::map<std::string, enum ggml_type>& tensor_types,\n                    const std::string prefix,\n                    bool decoder_only = true,\n                    SDVersion version = VERSION_SD1)\n        : decode_only(decoder_only),\n          taesd(decode_only, version),\n          GGMLRunner(backend) {\n        taesd.init(params_ctx, tensor_types, prefix);\n    }\n\n    std::string get_desc() {\n        return \"taesd\";\n    }\n\n    bool load_from_file(const std::string& file_path) {\n        LOG_INFO(\"loading taesd from '%s', decode_only = %s\", file_path.c_str(), decode_only ? \"true\" : \"false\");\n        alloc_params_buffer();\n        std::map<std::string, ggml_tensor*> taesd_tensors;\n        taesd.get_param_tensors(taesd_tensors);\n        std::set<std::string> ignore_tensors;\n        if (decode_only) {\n            ignore_tensors.insert(\"encoder.\");\n        }\n\n        ModelLoader model_loader;\n        if (!model_loader.init_from_file(file_path)) {\n            LOG_ERROR(\"init taesd model loader from file failed: '%s'\", file_path.c_str());\n            return false;\n        }\n\n        bool success = model_loader.load_tensors(taesd_tensors, backend, ignore_tensors);\n\n        if (!success) {\n            LOG_ERROR(\"load tae tensors from model loader failed\");\n            return false;\n        }\n\n        LOG_INFO(\"taesd model loaded\");\n        return success;\n    }\n\n    struct ggml_cgraph* build_graph(struct ggml_tensor* z, bool decode_graph) {\n        struct ggml_cgraph* gf  = ggml_new_graph(compute_ctx);\n        z                       = to_backend(z);\n        struct ggml_tensor* out = decode_graph ? taesd.decode(compute_ctx, z) : taesd.encode(compute_ctx, z);\n        ggml_build_forward_expand(gf, out);\n        return gf;\n    }\n\n    void compute(const int n_threads,\n                 struct ggml_tensor* z,\n                 bool decode_graph,\n                 struct ggml_tensor** output,\n                 struct ggml_context* output_ctx = NULL) {\n        auto get_graph = [&]() -> struct ggml_cgraph* {\n            return build_graph(z, decode_graph);\n        };\n\n        GGMLRunner::compute(get_graph, n_threads, false, output, output_ctx);\n    }\n};\n\n#endif  // __TAE_HPP__"
        },
        {
          "name": "thirdparty",
          "type": "tree",
          "content": null
        },
        {
          "name": "unet.hpp",
          "type": "blob",
          "size": 30.0380859375,
          "content": "#ifndef __UNET_HPP__\n#define __UNET_HPP__\n\n#include \"common.hpp\"\n#include \"ggml_extend.hpp\"\n#include \"model.h\"\n\n/*==================================================== UnetModel =====================================================*/\n\n#define UNET_GRAPH_SIZE 10240\n\nclass SpatialVideoTransformer : public SpatialTransformer {\nprotected:\n    int64_t time_depth;\n    int64_t max_time_embed_period;\n\npublic:\n    SpatialVideoTransformer(int64_t in_channels,\n                            int64_t n_head,\n                            int64_t d_head,\n                            int64_t depth,\n                            int64_t context_dim,\n                            int64_t time_depth            = 1,\n                            int64_t max_time_embed_period = 10000)\n        : SpatialTransformer(in_channels, n_head, d_head, depth, context_dim),\n          max_time_embed_period(max_time_embed_period) {\n        // We will convert unet transformer linear to conv2d 1x1 when loading the weights, so use_linear is always False\n        // use_spatial_context is always True\n        // merge_strategy is always learned_with_images\n        // merge_factor is loaded from weights\n        // time_context_dim is always None\n        // ff_in is always True\n        // disable_self_attn is always False\n        // disable_temporal_crossattention is always False\n\n        int64_t inner_dim = n_head * d_head;\n\n        GGML_ASSERT(depth == time_depth);\n        GGML_ASSERT(in_channels == inner_dim);\n\n        int64_t time_mix_d_head    = d_head;\n        int64_t n_time_mix_heads   = n_head;\n        int64_t time_mix_inner_dim = time_mix_d_head * n_time_mix_heads;  // equal to inner_dim\n        int64_t time_context_dim   = context_dim;\n\n        for (int i = 0; i < time_depth; i++) {\n            std::string name = \"time_stack.\" + std::to_string(i);\n            blocks[name]     = std::shared_ptr<GGMLBlock>(new BasicTransformerBlock(inner_dim,\n                                                                                    n_time_mix_heads,\n                                                                                    time_mix_d_head,\n                                                                                    time_context_dim,\n                                                                                    true));\n        }\n\n        int64_t time_embed_dim     = in_channels * 4;\n        blocks[\"time_pos_embed.0\"] = std::shared_ptr<GGMLBlock>(new Linear(in_channels, time_embed_dim));\n        // time_pos_embed.1 is nn.SiLU()\n        blocks[\"time_pos_embed.2\"] = std::shared_ptr<GGMLBlock>(new Linear(time_embed_dim, in_channels));\n\n        blocks[\"time_mixer\"] = std::shared_ptr<GGMLBlock>(new AlphaBlender());\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* x,\n                                struct ggml_tensor* context,\n                                int timesteps) {\n        // x: [N, in_channels, h, w] aka [b*t, in_channels, h, w], t == timesteps\n        // context: [N, max_position(aka n_context), hidden_size(aka context_dim)] aka [b*t, n_context, context_dim], t == timesteps\n        // t_emb: [N, in_channels] aka [b*t, in_channels]\n        // timesteps is num_frames\n        // time_context is always None\n        // image_only_indicator is always tensor([0.])\n        // transformer_options is not used\n        // GGML_ASSERT(ggml_n_dims(context) == 3);\n\n        auto norm             = std::dynamic_pointer_cast<GroupNorm32>(blocks[\"norm\"]);\n        auto proj_in          = std::dynamic_pointer_cast<Conv2d>(blocks[\"proj_in\"]);\n        auto proj_out         = std::dynamic_pointer_cast<Conv2d>(blocks[\"proj_out\"]);\n        auto time_pos_embed_0 = std::dynamic_pointer_cast<Linear>(blocks[\"time_pos_embed.0\"]);\n        auto time_pos_embed_2 = std::dynamic_pointer_cast<Linear>(blocks[\"time_pos_embed.2\"]);\n        auto time_mixer       = std::dynamic_pointer_cast<AlphaBlender>(blocks[\"time_mixer\"]);\n\n        auto x_in         = x;\n        int64_t n         = x->ne[3];\n        int64_t h         = x->ne[1];\n        int64_t w         = x->ne[0];\n        int64_t inner_dim = n_head * d_head;\n\n        GGML_ASSERT(n == timesteps);  // We compute cond and uncond separately, so batch_size==1\n\n        auto time_context    = context;  // [b*t, n_context, context_dim]\n        auto spatial_context = context;\n        // time_context_first_timestep = time_context[::timesteps]\n        auto time_context_first_timestep = ggml_view_3d(ctx,\n                                                        time_context,\n                                                        time_context->ne[0],\n                                                        time_context->ne[1],\n                                                        1,\n                                                        time_context->nb[1],\n                                                        time_context->nb[2],\n                                                        0);  // [b, n_context, context_dim]\n        time_context                     = ggml_new_tensor_3d(ctx, GGML_TYPE_F32,\n                                                              time_context_first_timestep->ne[0],\n                                                              time_context_first_timestep->ne[1],\n                                                              time_context_first_timestep->ne[2] * h * w);\n        time_context                     = ggml_repeat(ctx, time_context_first_timestep, time_context);  // [b*h*w, n_context, context_dim]\n\n        x = norm->forward(ctx, x);\n        x = proj_in->forward(ctx, x);  // [N, inner_dim, h, w]\n\n        x = ggml_cont(ctx, ggml_permute(ctx, x, 1, 2, 0, 3));  // [N, h, w, inner_dim]\n        x = ggml_reshape_3d(ctx, x, inner_dim, w * h, n);      // [N, h * w, inner_dim]\n\n        auto num_frames = ggml_arange(ctx, 0, timesteps, 1);\n        // since b is 1, no need to do repeat\n        auto t_emb = ggml_nn_timestep_embedding(ctx, num_frames, in_channels, max_time_embed_period);  // [N, in_channels]\n\n        auto emb = time_pos_embed_0->forward(ctx, t_emb);\n        emb      = ggml_silu_inplace(ctx, emb);\n        emb      = time_pos_embed_2->forward(ctx, emb);                   // [N, in_channels]\n        emb      = ggml_reshape_3d(ctx, emb, emb->ne[0], 1, emb->ne[1]);  // [N, 1, in_channels]\n\n        for (int i = 0; i < depth; i++) {\n            std::string transformer_name = \"transformer_blocks.\" + std::to_string(i);\n            std::string time_stack_name  = \"time_stack.\" + std::to_string(i);\n\n            auto block     = std::dynamic_pointer_cast<BasicTransformerBlock>(blocks[transformer_name]);\n            auto mix_block = std::dynamic_pointer_cast<BasicTransformerBlock>(blocks[time_stack_name]);\n\n            x = block->forward(ctx, x, spatial_context);  // [N, h * w, inner_dim]\n\n            // in_channels == inner_dim\n            auto x_mix = x;\n            x_mix      = ggml_add(ctx, x_mix, emb);  // [N, h * w, inner_dim]\n\n            int64_t N = x_mix->ne[2];\n            int64_t T = timesteps;\n            int64_t B = N / T;\n            int64_t S = x_mix->ne[1];\n            int64_t C = x_mix->ne[0];\n\n            x_mix = ggml_reshape_4d(ctx, x_mix, C, S, T, B);               // (b t) s c -> b t s c\n            x_mix = ggml_cont(ctx, ggml_permute(ctx, x_mix, 0, 2, 1, 3));  // b t s c -> b s t c\n            x_mix = ggml_reshape_3d(ctx, x_mix, C, T, S * B);              // b s t c -> (b s) t c\n\n            x_mix = mix_block->forward(ctx, x_mix, time_context);  // [B * h * w, T, inner_dim]\n\n            x_mix = ggml_reshape_4d(ctx, x_mix, C, T, S, B);               // (b s) t c -> b s t c\n            x_mix = ggml_cont(ctx, ggml_permute(ctx, x_mix, 0, 2, 1, 3));  // b s t c -> b t s c\n            x_mix = ggml_reshape_3d(ctx, x_mix, C, S, T * B);              // b t s c -> (b t) s c\n\n            x = time_mixer->forward(ctx, x, x_mix);  // [N, h * w, inner_dim]\n        }\n\n        x = ggml_cont(ctx, ggml_permute(ctx, x, 1, 0, 2, 3));  // [N, inner_dim, h * w]\n        x = ggml_reshape_4d(ctx, x, w, h, inner_dim, n);       // [N, inner_dim, h, w]\n\n        // proj_out\n        x = proj_out->forward(ctx, x);  // [N, in_channels, h, w]\n\n        x = ggml_add(ctx, x, x_in);\n        return x;\n    }\n};\n\n// ldm.modules.diffusionmodules.openaimodel.UNetModel\nclass UnetModelBlock : public GGMLBlock {\nprotected:\n    static std::map<std::string, enum ggml_type> empty_tensor_types;\n    SDVersion version = VERSION_SD1;\n    // network hparams\n    int in_channels                        = 4;\n    int out_channels                       = 4;\n    int num_res_blocks                     = 2;\n    std::vector<int> attention_resolutions = {4, 2, 1};\n    std::vector<int> channel_mult          = {1, 2, 4, 4};\n    std::vector<int> transformer_depth     = {1, 1, 1, 1};\n    int time_embed_dim                     = 1280;  // model_channels*4\n    int num_heads                          = 8;\n    int num_head_channels                  = -1;   // channels // num_heads\n    int context_dim                        = 768;  // 1024 for VERSION_SD2, 2048 for VERSION_SDXL\n\npublic:\n    int model_channels  = 320;\n    int adm_in_channels = 2816;  // only for VERSION_SDXL/SVD\n\n    UnetModelBlock(SDVersion version = VERSION_SD1, std::map<std::string, enum ggml_type>& tensor_types = empty_tensor_types, bool flash_attn = false)\n        : version(version) {\n        if (sd_version_is_sd2(version)) {\n            context_dim       = 1024;\n            num_head_channels = 64;\n            num_heads         = -1;\n        } else if (sd_version_is_sdxl(version)) {\n            context_dim           = 2048;\n            attention_resolutions = {4, 2};\n            channel_mult          = {1, 2, 4};\n            transformer_depth     = {1, 2, 10};\n            num_head_channels     = 64;\n            num_heads             = -1;\n        } else if (version == VERSION_SVD) {\n            in_channels       = 8;\n            out_channels      = 4;\n            context_dim       = 1024;\n            adm_in_channels   = 768;\n            num_head_channels = 64;\n            num_heads         = -1;\n        }\n        if (sd_version_is_inpaint(version)) {\n            in_channels = 9;\n        }\n\n        // dims is always 2\n        // use_temporal_attention is always True for SVD\n\n        blocks[\"time_embed.0\"] = std::shared_ptr<GGMLBlock>(new Linear(model_channels, time_embed_dim));\n        // time_embed_1 is nn.SiLU()\n        blocks[\"time_embed.2\"] = std::shared_ptr<GGMLBlock>(new Linear(time_embed_dim, time_embed_dim));\n\n        if (sd_version_is_sdxl(version) || version == VERSION_SVD) {\n            blocks[\"label_emb.0.0\"] = std::shared_ptr<GGMLBlock>(new Linear(adm_in_channels, time_embed_dim));\n            // label_emb_1 is nn.SiLU()\n            blocks[\"label_emb.0.2\"] = std::shared_ptr<GGMLBlock>(new Linear(time_embed_dim, time_embed_dim));\n        }\n\n        // input_blocks\n        blocks[\"input_blocks.0.0\"] = std::shared_ptr<GGMLBlock>(new Conv2d(in_channels, model_channels, {3, 3}, {1, 1}, {1, 1}));\n\n        std::vector<int> input_block_chans;\n        input_block_chans.push_back(model_channels);\n        int ch              = model_channels;\n        int input_block_idx = 0;\n        int ds              = 1;\n\n        auto get_resblock = [&](int64_t channels, int64_t emb_channels, int64_t out_channels) -> ResBlock* {\n            if (version == VERSION_SVD) {\n                return new VideoResBlock(channels, emb_channels, out_channels);\n            } else {\n                return new ResBlock(channels, emb_channels, out_channels);\n            }\n        };\n\n        auto get_attention_layer = [&](int64_t in_channels,\n                                       int64_t n_head,\n                                       int64_t d_head,\n                                       int64_t depth,\n                                       int64_t context_dim) -> SpatialTransformer* {\n            if (version == VERSION_SVD) {\n                return new SpatialVideoTransformer(in_channels, n_head, d_head, depth, context_dim);\n            } else {\n                return new SpatialTransformer(in_channels, n_head, d_head, depth, context_dim, flash_attn);\n            }\n        };\n\n        size_t len_mults = channel_mult.size();\n        for (int i = 0; i < len_mults; i++) {\n            int mult = channel_mult[i];\n            for (int j = 0; j < num_res_blocks; j++) {\n                input_block_idx += 1;\n                std::string name = \"input_blocks.\" + std::to_string(input_block_idx) + \".0\";\n                blocks[name]     = std::shared_ptr<GGMLBlock>(get_resblock(ch, time_embed_dim, mult * model_channels));\n\n                ch = mult * model_channels;\n                if (std::find(attention_resolutions.begin(), attention_resolutions.end(), ds) != attention_resolutions.end()) {\n                    int n_head = num_heads;\n                    int d_head = ch / num_heads;\n                    if (num_head_channels != -1) {\n                        d_head = num_head_channels;\n                        n_head = ch / d_head;\n                    }\n                    std::string name = \"input_blocks.\" + std::to_string(input_block_idx) + \".1\";\n                    blocks[name]     = std::shared_ptr<GGMLBlock>(get_attention_layer(ch,\n                                                                                      n_head,\n                                                                                      d_head,\n                                                                                      transformer_depth[i],\n                                                                                      context_dim));\n                }\n                input_block_chans.push_back(ch);\n            }\n            if (i != len_mults - 1) {\n                input_block_idx += 1;\n                std::string name = \"input_blocks.\" + std::to_string(input_block_idx) + \".0\";\n                blocks[name]     = std::shared_ptr<GGMLBlock>(new DownSampleBlock(ch, ch));\n\n                input_block_chans.push_back(ch);\n                ds *= 2;\n            }\n        }\n\n        // middle blocks\n        int n_head = num_heads;\n        int d_head = ch / num_heads;\n        if (num_head_channels != -1) {\n            d_head = num_head_channels;\n            n_head = ch / d_head;\n        }\n        blocks[\"middle_block.0\"] = std::shared_ptr<GGMLBlock>(get_resblock(ch, time_embed_dim, ch));\n        blocks[\"middle_block.1\"] = std::shared_ptr<GGMLBlock>(get_attention_layer(ch,\n                                                                                  n_head,\n                                                                                  d_head,\n                                                                                  transformer_depth[transformer_depth.size() - 1],\n                                                                                  context_dim));\n        blocks[\"middle_block.2\"] = std::shared_ptr<GGMLBlock>(get_resblock(ch, time_embed_dim, ch));\n\n        // output_blocks\n        int output_block_idx = 0;\n        for (int i = (int)len_mults - 1; i >= 0; i--) {\n            int mult = channel_mult[i];\n            for (int j = 0; j < num_res_blocks + 1; j++) {\n                int ich = input_block_chans.back();\n                input_block_chans.pop_back();\n\n                std::string name = \"output_blocks.\" + std::to_string(output_block_idx) + \".0\";\n                blocks[name]     = std::shared_ptr<GGMLBlock>(get_resblock(ch + ich, time_embed_dim, mult * model_channels));\n\n                ch                = mult * model_channels;\n                int up_sample_idx = 1;\n                if (std::find(attention_resolutions.begin(), attention_resolutions.end(), ds) != attention_resolutions.end()) {\n                    int n_head = num_heads;\n                    int d_head = ch / num_heads;\n                    if (num_head_channels != -1) {\n                        d_head = num_head_channels;\n                        n_head = ch / d_head;\n                    }\n                    std::string name = \"output_blocks.\" + std::to_string(output_block_idx) + \".1\";\n                    blocks[name]     = std::shared_ptr<GGMLBlock>(get_attention_layer(ch, n_head, d_head, transformer_depth[i], context_dim));\n\n                    up_sample_idx++;\n                }\n\n                if (i > 0 && j == num_res_blocks) {\n                    std::string name = \"output_blocks.\" + std::to_string(output_block_idx) + \".\" + std::to_string(up_sample_idx);\n                    blocks[name]     = std::shared_ptr<GGMLBlock>(new UpSampleBlock(ch, ch));\n\n                    ds /= 2;\n                }\n\n                output_block_idx += 1;\n            }\n        }\n\n        // out\n        blocks[\"out.0\"] = std::shared_ptr<GGMLBlock>(new GroupNorm32(ch));  // ch == model_channels\n        // out_1 is nn.SiLU()\n        blocks[\"out.2\"] = std::shared_ptr<GGMLBlock>(new Conv2d(model_channels, out_channels, {3, 3}, {1, 1}, {1, 1}));\n    }\n\n    struct ggml_tensor* resblock_forward(std::string name,\n                                         struct ggml_context* ctx,\n                                         struct ggml_tensor* x,\n                                         struct ggml_tensor* emb,\n                                         int num_video_frames) {\n        if (version == VERSION_SVD) {\n            auto block = std::dynamic_pointer_cast<VideoResBlock>(blocks[name]);\n\n            return block->forward(ctx, x, emb, num_video_frames);\n        } else {\n            auto block = std::dynamic_pointer_cast<ResBlock>(blocks[name]);\n\n            return block->forward(ctx, x, emb);\n        }\n    }\n\n    struct ggml_tensor* attention_layer_forward(std::string name,\n                                                struct ggml_context* ctx,\n                                                struct ggml_tensor* x,\n                                                struct ggml_tensor* context,\n                                                int timesteps) {\n        if (version == VERSION_SVD) {\n            auto block = std::dynamic_pointer_cast<SpatialVideoTransformer>(blocks[name]);\n\n            return block->forward(ctx, x, context, timesteps);\n        } else {\n            auto block = std::dynamic_pointer_cast<SpatialTransformer>(blocks[name]);\n\n            return block->forward(ctx, x, context);\n        }\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* x,\n                                struct ggml_tensor* timesteps,\n                                struct ggml_tensor* context,\n                                struct ggml_tensor* c_concat              = NULL,\n                                struct ggml_tensor* y                     = NULL,\n                                int num_video_frames                      = -1,\n                                std::vector<struct ggml_tensor*> controls = {},\n                                float control_strength                    = 0.f) {\n        // x: [N, in_channels, h, w] or [N, in_channels/2, h, w]\n        // timesteps: [N,]\n        // context: [N, max_position, hidden_size] or [1, max_position, hidden_size]. for example, [N, 77, 768]\n        // c_concat: [N, in_channels, h, w] or [1, in_channels, h, w]\n        // y: [N, adm_in_channels] or [1, adm_in_channels]\n        // return: [N, out_channels, h, w]\n        if (context != NULL) {\n            if (context->ne[2] != x->ne[3]) {\n                context = ggml_repeat(ctx, context, ggml_new_tensor_3d(ctx, GGML_TYPE_F32, context->ne[0], context->ne[1], x->ne[3]));\n            }\n        }\n\n        if (c_concat != NULL) {\n            if (c_concat->ne[3] != x->ne[3]) {\n                c_concat = ggml_repeat(ctx, c_concat, x);\n            }\n            x = ggml_concat(ctx, x, c_concat, 2);\n        }\n\n        if (y != NULL) {\n            if (y->ne[1] != x->ne[3]) {\n                y = ggml_repeat(ctx, y, ggml_new_tensor_2d(ctx, GGML_TYPE_F32, y->ne[0], x->ne[3]));\n            }\n        }\n\n        auto time_embed_0     = std::dynamic_pointer_cast<Linear>(blocks[\"time_embed.0\"]);\n        auto time_embed_2     = std::dynamic_pointer_cast<Linear>(blocks[\"time_embed.2\"]);\n        auto input_blocks_0_0 = std::dynamic_pointer_cast<Conv2d>(blocks[\"input_blocks.0.0\"]);\n\n        auto out_0 = std::dynamic_pointer_cast<GroupNorm32>(blocks[\"out.0\"]);\n        auto out_2 = std::dynamic_pointer_cast<Conv2d>(blocks[\"out.2\"]);\n\n        auto t_emb = ggml_nn_timestep_embedding(ctx, timesteps, model_channels);  // [N, model_channels]\n\n        auto emb = time_embed_0->forward(ctx, t_emb);\n        emb      = ggml_silu_inplace(ctx, emb);\n        emb      = time_embed_2->forward(ctx, emb);  // [N, time_embed_dim]\n\n        // SDXL/SVD\n        if (y != NULL) {\n            auto label_embed_0 = std::dynamic_pointer_cast<Linear>(blocks[\"label_emb.0.0\"]);\n            auto label_embed_2 = std::dynamic_pointer_cast<Linear>(blocks[\"label_emb.0.2\"]);\n\n            auto label_emb = label_embed_0->forward(ctx, y);\n            label_emb      = ggml_silu_inplace(ctx, label_emb);\n            label_emb      = label_embed_2->forward(ctx, label_emb);  // [N, time_embed_dim]\n\n            emb = ggml_add(ctx, emb, label_emb);  // [N, time_embed_dim]\n        }\n\n        // input_blocks\n        std::vector<struct ggml_tensor*> hs;\n\n        // input block 0\n        auto h = input_blocks_0_0->forward(ctx, x);\n\n        ggml_set_name(h, \"bench-start\");\n        hs.push_back(h);\n        // input block 1-11\n        size_t len_mults    = channel_mult.size();\n        int input_block_idx = 0;\n        int ds              = 1;\n        for (int i = 0; i < len_mults; i++) {\n            int mult = channel_mult[i];\n            for (int j = 0; j < num_res_blocks; j++) {\n                input_block_idx += 1;\n                std::string name = \"input_blocks.\" + std::to_string(input_block_idx) + \".0\";\n                h                = resblock_forward(name, ctx, h, emb, num_video_frames);  // [N, mult*model_channels, h, w]\n                if (std::find(attention_resolutions.begin(), attention_resolutions.end(), ds) != attention_resolutions.end()) {\n                    std::string name = \"input_blocks.\" + std::to_string(input_block_idx) + \".1\";\n                    h                = attention_layer_forward(name, ctx, h, context, num_video_frames);  // [N, mult*model_channels, h, w]\n                }\n                hs.push_back(h);\n            }\n            if (i != len_mults - 1) {\n                ds *= 2;\n                input_block_idx += 1;\n\n                std::string name = \"input_blocks.\" + std::to_string(input_block_idx) + \".0\";\n                auto block       = std::dynamic_pointer_cast<DownSampleBlock>(blocks[name]);\n\n                h = block->forward(ctx, h);  // [N, mult*model_channels, h/(2^(i+1)), w/(2^(i+1))]\n                hs.push_back(h);\n            }\n        }\n        // [N, 4*model_channels, h/8, w/8]\n\n        // middle_block\n        h = resblock_forward(\"middle_block.0\", ctx, h, emb, num_video_frames);             // [N, 4*model_channels, h/8, w/8]\n        h = attention_layer_forward(\"middle_block.1\", ctx, h, context, num_video_frames);  // [N, 4*model_channels, h/8, w/8]\n        h = resblock_forward(\"middle_block.2\", ctx, h, emb, num_video_frames);             // [N, 4*model_channels, h/8, w/8]\n\n        if (controls.size() > 0) {\n            auto cs = ggml_scale_inplace(ctx, controls[controls.size() - 1], control_strength);\n            h       = ggml_add(ctx, h, cs);  // middle control\n        }\n        int control_offset = controls.size() - 2;\n\n        // output_blocks\n        int output_block_idx = 0;\n        for (int i = (int)len_mults - 1; i >= 0; i--) {\n            for (int j = 0; j < num_res_blocks + 1; j++) {\n                auto h_skip = hs.back();\n                hs.pop_back();\n\n                if (controls.size() > 0) {\n                    auto cs = ggml_scale_inplace(ctx, controls[control_offset], control_strength);\n                    h_skip  = ggml_add(ctx, h_skip, cs);  // control net condition\n                    control_offset--;\n                }\n\n                h = ggml_concat(ctx, h, h_skip, 2);\n\n                std::string name = \"output_blocks.\" + std::to_string(output_block_idx) + \".0\";\n\n                h = resblock_forward(name, ctx, h, emb, num_video_frames);\n\n                int up_sample_idx = 1;\n                if (std::find(attention_resolutions.begin(), attention_resolutions.end(), ds) != attention_resolutions.end()) {\n                    std::string name = \"output_blocks.\" + std::to_string(output_block_idx) + \".1\";\n\n                    h = attention_layer_forward(name, ctx, h, context, num_video_frames);\n\n                    up_sample_idx++;\n                }\n\n                if (i > 0 && j == num_res_blocks) {\n                    std::string name = \"output_blocks.\" + std::to_string(output_block_idx) + \".\" + std::to_string(up_sample_idx);\n                    auto block       = std::dynamic_pointer_cast<UpSampleBlock>(blocks[name]);\n\n                    h = block->forward(ctx, h);\n\n                    ds /= 2;\n                }\n\n                output_block_idx += 1;\n            }\n        }\n\n        // out\n        h = out_0->forward(ctx, h);\n        h = ggml_silu_inplace(ctx, h);\n        h = out_2->forward(ctx, h);\n        ggml_set_name(h, \"bench-end\");\n        return h;  // [N, out_channels, h, w]\n    }\n};\n\nstruct UNetModelRunner : public GGMLRunner {\n    UnetModelBlock unet;\n\n    UNetModelRunner(ggml_backend_t backend,\n                    std::map<std::string, enum ggml_type>& tensor_types,\n                    const std::string prefix,\n                    SDVersion version = VERSION_SD1,\n                    bool flash_attn   = false)\n        : GGMLRunner(backend), unet(version, tensor_types, flash_attn) {\n        unet.init(params_ctx, tensor_types, prefix);\n    }\n\n    std::string get_desc() {\n        return \"unet\";\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors, const std::string prefix) {\n        unet.get_param_tensors(tensors, prefix);\n    }\n\n    struct ggml_cgraph* build_graph(struct ggml_tensor* x,\n                                    struct ggml_tensor* timesteps,\n                                    struct ggml_tensor* context,\n                                    struct ggml_tensor* c_concat              = NULL,\n                                    struct ggml_tensor* y                     = NULL,\n                                    int num_video_frames                      = -1,\n                                    std::vector<struct ggml_tensor*> controls = {},\n                                    float control_strength                    = 0.f) {\n        struct ggml_cgraph* gf = ggml_new_graph_custom(compute_ctx, UNET_GRAPH_SIZE, false);\n\n        if (num_video_frames == -1) {\n            num_video_frames = x->ne[3];\n        }\n\n        x         = to_backend(x);\n        context   = to_backend(context);\n        y         = to_backend(y);\n        timesteps = to_backend(timesteps);\n        c_concat  = to_backend(c_concat);\n\n        for (int i = 0; i < controls.size(); i++) {\n            controls[i] = to_backend(controls[i]);\n        }\n\n        struct ggml_tensor* out = unet.forward(compute_ctx,\n                                               x,\n                                               timesteps,\n                                               context,\n                                               c_concat,\n                                               y,\n                                               num_video_frames,\n                                               controls,\n                                               control_strength);\n\n        ggml_build_forward_expand(gf, out);\n\n        return gf;\n    }\n\n    void compute(int n_threads,\n                 struct ggml_tensor* x,\n                 struct ggml_tensor* timesteps,\n                 struct ggml_tensor* context,\n                 struct ggml_tensor* c_concat,\n                 struct ggml_tensor* y,\n                 int num_video_frames                      = -1,\n                 std::vector<struct ggml_tensor*> controls = {},\n                 float control_strength                    = 0.f,\n                 struct ggml_tensor** output               = NULL,\n                 struct ggml_context* output_ctx           = NULL) {\n        // x: [N, in_channels, h, w]\n        // timesteps: [N, ]\n        // context: [N, max_position, hidden_size]([N, 77, 768]) or [1, max_position, hidden_size]\n        // c_concat: [N, in_channels, h, w] or [1, in_channels, h, w]\n        // y: [N, adm_in_channels] or [1, adm_in_channels]\n        auto get_graph = [&]() -> struct ggml_cgraph* {\n            return build_graph(x, timesteps, context, c_concat, y, num_video_frames, controls, control_strength);\n        };\n\n        GGMLRunner::compute(get_graph, n_threads, false, output, output_ctx);\n    }\n\n    void test() {\n        struct ggml_init_params params;\n        params.mem_size   = static_cast<size_t>(10 * 1024 * 1024);  // 10 MB\n        params.mem_buffer = NULL;\n        params.no_alloc   = false;\n\n        struct ggml_context* work_ctx = ggml_init(params);\n        GGML_ASSERT(work_ctx != NULL);\n\n        {\n            // CPU, num_video_frames = 1, x{num_video_frames, 8, 8, 8}: Pass\n            // CUDA, num_video_frames = 1, x{num_video_frames, 8, 8, 8}: Pass\n            // CPU, num_video_frames = 3, x{num_video_frames, 8, 8, 8}: Wrong result\n            // CUDA, num_video_frames = 3, x{num_video_frames, 8, 8, 8}: nan\n            int num_video_frames = 3;\n\n            auto x = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 8, 8, 8, num_video_frames);\n            std::vector<float> timesteps_vec(num_video_frames, 999.f);\n            auto timesteps = vector_to_ggml_tensor(work_ctx, timesteps_vec);\n            ggml_set_f32(x, 0.5f);\n            // print_ggml_tensor(x);\n\n            auto context = ggml_new_tensor_3d(work_ctx, GGML_TYPE_F32, 1024, 1, num_video_frames);\n            ggml_set_f32(context, 0.5f);\n            // print_ggml_tensor(context);\n\n            auto y = ggml_new_tensor_2d(work_ctx, GGML_TYPE_F32, 768, num_video_frames);\n            ggml_set_f32(y, 0.5f);\n            // print_ggml_tensor(y);\n\n            struct ggml_tensor* out = NULL;\n\n            int t0 = ggml_time_ms();\n            compute(8, x, timesteps, context, NULL, y, num_video_frames, {}, 0.f, &out, work_ctx);\n            int t1 = ggml_time_ms();\n\n            print_ggml_tensor(out);\n            LOG_DEBUG(\"unet test done in %dms\", t1 - t0);\n        }\n    }\n};\n\n#endif  // __UNET_HPP__\n"
        },
        {
          "name": "upscaler.cpp",
          "type": "blob",
          "size": 4.8076171875,
          "content": "#include \"esrgan.hpp\"\n#include \"ggml_extend.hpp\"\n#include \"model.h\"\n#include \"stable-diffusion.h\"\n\nstruct UpscalerGGML {\n    ggml_backend_t backend    = NULL;  // general backend\n    ggml_type model_data_type = GGML_TYPE_F16;\n    std::shared_ptr<ESRGAN> esrgan_upscaler;\n    std::string esrgan_path;\n    int n_threads;\n\n    UpscalerGGML(int n_threads)\n        : n_threads(n_threads) {\n    }\n\n    bool load_from_file(const std::string& esrgan_path) {\n#ifdef SD_USE_CUDA\n        LOG_DEBUG(\"Using CUDA backend\");\n        backend = ggml_backend_cuda_init(0);\n#endif\n#ifdef SD_USE_METAL\n        LOG_DEBUG(\"Using Metal backend\");\n        ggml_log_set(ggml_log_callback_default, nullptr);\n        backend = ggml_backend_metal_init();\n#endif\n#ifdef SD_USE_VULKAN\n        LOG_DEBUG(\"Using Vulkan backend\");\n        backend = ggml_backend_vk_init(0);\n#endif\n#ifdef SD_USE_SYCL\n        LOG_DEBUG(\"Using SYCL backend\");\n        backend = ggml_backend_sycl_init(0);\n#endif\n        ModelLoader model_loader;\n        if (!model_loader.init_from_file(esrgan_path)) {\n            LOG_ERROR(\"init model loader from file failed: '%s'\", esrgan_path.c_str());\n        }\n        model_loader.set_wtype_override(model_data_type);\n        if (!backend) {\n            LOG_DEBUG(\"Using CPU backend\");\n            backend = ggml_backend_cpu_init();\n        }\n        LOG_INFO(\"Upscaler weight type: %s\", ggml_type_name(model_data_type));\n        esrgan_upscaler = std::make_shared<ESRGAN>(backend, model_loader.tensor_storages_types);\n        if (!esrgan_upscaler->load_from_file(esrgan_path)) {\n            return false;\n        }\n        return true;\n    }\n\n    sd_image_t upscale(sd_image_t input_image, uint32_t upscale_factor) {\n        // upscale_factor, unused for RealESRGAN_x4plus_anime_6B.pth\n        sd_image_t upscaled_image = {0, 0, 0, NULL};\n        int output_width          = (int)input_image.width * esrgan_upscaler->scale;\n        int output_height         = (int)input_image.height * esrgan_upscaler->scale;\n        LOG_INFO(\"upscaling from (%i x %i) to (%i x %i)\",\n                 input_image.width, input_image.height, output_width, output_height);\n\n        struct ggml_init_params params;\n        params.mem_size = output_width * output_height * 3 * sizeof(float) * 2;\n        params.mem_size += 2 * ggml_tensor_overhead();\n        params.mem_buffer = NULL;\n        params.no_alloc   = false;\n\n        // draft context\n        struct ggml_context* upscale_ctx = ggml_init(params);\n        if (!upscale_ctx) {\n            LOG_ERROR(\"ggml_init() failed\");\n            return upscaled_image;\n        }\n        LOG_DEBUG(\"upscale work buffer size: %.2f MB\", params.mem_size / 1024.f / 1024.f);\n        ggml_tensor* input_image_tensor = ggml_new_tensor_4d(upscale_ctx, GGML_TYPE_F32, input_image.width, input_image.height, 3, 1);\n        sd_image_to_tensor(input_image.data, input_image_tensor);\n\n        ggml_tensor* upscaled = ggml_new_tensor_4d(upscale_ctx, GGML_TYPE_F32, output_width, output_height, 3, 1);\n        auto on_tiling        = [&](ggml_tensor* in, ggml_tensor* out, bool init) {\n            esrgan_upscaler->compute(n_threads, in, &out);\n        };\n        int64_t t0 = ggml_time_ms();\n        sd_tiling(input_image_tensor, upscaled, esrgan_upscaler->scale, esrgan_upscaler->tile_size, 0.25f, on_tiling);\n        esrgan_upscaler->free_compute_buffer();\n        ggml_tensor_clamp(upscaled, 0.f, 1.f);\n        uint8_t* upscaled_data = sd_tensor_to_image(upscaled);\n        ggml_free(upscale_ctx);\n        int64_t t3 = ggml_time_ms();\n        LOG_INFO(\"input_image_tensor upscaled, taking %.2fs\", (t3 - t0) / 1000.0f);\n        upscaled_image = {\n            (uint32_t)output_width,\n            (uint32_t)output_height,\n            3,\n            upscaled_data,\n        };\n        return upscaled_image;\n    }\n};\n\nstruct upscaler_ctx_t {\n    UpscalerGGML* upscaler = NULL;\n};\n\nupscaler_ctx_t* new_upscaler_ctx(const char* esrgan_path_c_str,\n                                 int n_threads) {\n    upscaler_ctx_t* upscaler_ctx = (upscaler_ctx_t*)malloc(sizeof(upscaler_ctx_t));\n    if (upscaler_ctx == NULL) {\n        return NULL;\n    }\n    std::string esrgan_path(esrgan_path_c_str);\n\n    upscaler_ctx->upscaler = new UpscalerGGML(n_threads);\n    if (upscaler_ctx->upscaler == NULL) {\n        return NULL;\n    }\n\n    if (!upscaler_ctx->upscaler->load_from_file(esrgan_path)) {\n        delete upscaler_ctx->upscaler;\n        upscaler_ctx->upscaler = NULL;\n        free(upscaler_ctx);\n        return NULL;\n    }\n    return upscaler_ctx;\n}\n\nsd_image_t upscale(upscaler_ctx_t* upscaler_ctx, sd_image_t input_image, uint32_t upscale_factor) {\n    return upscaler_ctx->upscaler->upscale(input_image, upscale_factor);\n}\n\nvoid free_upscaler_ctx(upscaler_ctx_t* upscaler_ctx) {\n    if (upscaler_ctx->upscaler != NULL) {\n        delete upscaler_ctx->upscaler;\n        upscaler_ctx->upscaler = NULL;\n    }\n    free(upscaler_ctx);\n}\n"
        },
        {
          "name": "util.cpp",
          "type": "blob",
          "size": 21.5654296875,
          "content": "#include \"util.h\"\n#include <stdarg.h>\n#include <algorithm>\n#include <cmath>\n#include <codecvt>\n#include <fstream>\n#include <locale>\n#include <sstream>\n#include <string>\n#include <thread>\n#include <unordered_set>\n#include <vector>\n#include \"preprocessing.hpp\"\n\n#if defined(__APPLE__) && defined(__MACH__)\n#include <sys/sysctl.h>\n#include <sys/types.h>\n#endif\n\n#if !defined(_WIN32)\n#include <sys/ioctl.h>\n#include <unistd.h>\n#endif\n\n#include \"ggml-cpu.h\"\n#include \"ggml.h\"\n#include \"stable-diffusion.h\"\n\n#define STB_IMAGE_RESIZE_IMPLEMENTATION\n#include \"stb_image_resize.h\"\n\nbool ends_with(const std::string& str, const std::string& ending) {\n    if (str.length() >= ending.length()) {\n        return (str.compare(str.length() - ending.length(), ending.length(), ending) == 0);\n    } else {\n        return false;\n    }\n}\n\nbool starts_with(const std::string& str, const std::string& start) {\n    if (str.find(start) == 0) {\n        return true;\n    }\n    return false;\n}\n\nbool contains(const std::string& str, const std::string& substr) {\n    if (str.find(substr) != std::string::npos) {\n        return true;\n    }\n    return false;\n}\n\nvoid replace_all_chars(std::string& str, char target, char replacement) {\n    for (size_t i = 0; i < str.length(); ++i) {\n        if (str[i] == target) {\n            str[i] = replacement;\n        }\n    }\n}\n\nstd::string format(const char* fmt, ...) {\n    va_list ap;\n    va_list ap2;\n    va_start(ap, fmt);\n    va_copy(ap2, ap);\n    int size = vsnprintf(NULL, 0, fmt, ap);\n    std::vector<char> buf(size + 1);\n    int size2 = vsnprintf(buf.data(), size + 1, fmt, ap2);\n    va_end(ap2);\n    va_end(ap);\n    return std::string(buf.data(), size);\n}\n\n#ifdef _WIN32  // code for windows\n#include <windows.h>\n\nbool file_exists(const std::string& filename) {\n    DWORD attributes = GetFileAttributesA(filename.c_str());\n    return (attributes != INVALID_FILE_ATTRIBUTES && !(attributes & FILE_ATTRIBUTE_DIRECTORY));\n}\n\nbool is_directory(const std::string& path) {\n    DWORD attributes = GetFileAttributesA(path.c_str());\n    return (attributes != INVALID_FILE_ATTRIBUTES && (attributes & FILE_ATTRIBUTE_DIRECTORY));\n}\n\nstd::string get_full_path(const std::string& dir, const std::string& filename) {\n    std::string full_path = dir + \"\\\\\" + filename;\n\n    WIN32_FIND_DATA find_file_data;\n    HANDLE hFind = FindFirstFile(full_path.c_str(), &find_file_data);\n\n    if (hFind != INVALID_HANDLE_VALUE) {\n        FindClose(hFind);\n        return full_path;\n    } else {\n        return \"\";\n    }\n}\n\nstd::vector<std::string> get_files_from_dir(const std::string& dir) {\n    std::vector<std::string> files;\n\n    WIN32_FIND_DATA findFileData;\n    HANDLE hFind;\n\n    char currentDirectory[MAX_PATH];\n    GetCurrentDirectory(MAX_PATH, currentDirectory);\n\n    char directoryPath[MAX_PATH];  // this is absolute path\n    sprintf(directoryPath, \"%s\\\\%s\\\\*\", currentDirectory, dir.c_str());\n\n    // Find the first file in the directory\n    hFind = FindFirstFile(directoryPath, &findFileData);\n\n    // Check if the directory was found\n    if (hFind == INVALID_HANDLE_VALUE) {\n        printf(\"Unable to find directory.\\n\");\n        return files;\n    }\n\n    // Loop through all files in the directory\n    do {\n        // Check if the found file is a regular file (not a directory)\n        if (!(findFileData.dwFileAttributes & FILE_ATTRIBUTE_DIRECTORY)) {\n            files.push_back(std::string(currentDirectory) + \"\\\\\" + dir + \"\\\\\" + std::string(findFileData.cFileName));\n        }\n    } while (FindNextFile(hFind, &findFileData) != 0);\n\n    // Close the handle\n    FindClose(hFind);\n\n    sort(files.begin(), files.end());\n\n    return files;\n}\n\n#else  // Unix\n#include <dirent.h>\n#include <sys/stat.h>\n\nbool file_exists(const std::string& filename) {\n    struct stat buffer;\n    return (stat(filename.c_str(), &buffer) == 0 && S_ISREG(buffer.st_mode));\n}\n\nbool is_directory(const std::string& path) {\n    struct stat buffer;\n    return (stat(path.c_str(), &buffer) == 0 && S_ISDIR(buffer.st_mode));\n}\n\n// TODO: add windows version\nstd::string get_full_path(const std::string& dir, const std::string& filename) {\n    DIR* dp = opendir(dir.c_str());\n\n    if (dp != nullptr) {\n        struct dirent* entry;\n\n        while ((entry = readdir(dp)) != nullptr) {\n            if (strcasecmp(entry->d_name, filename.c_str()) == 0) {\n                closedir(dp);\n                return dir + \"/\" + entry->d_name;\n            }\n        }\n\n        closedir(dp);\n    }\n\n    return \"\";\n}\n\nstd::vector<std::string> get_files_from_dir(const std::string& dir) {\n    std::vector<std::string> files;\n\n    DIR* dp = opendir(dir.c_str());\n\n    if (dp != nullptr) {\n        struct dirent* entry;\n\n        while ((entry = readdir(dp)) != nullptr) {\n            std::string fname = dir + \"/\" + entry->d_name;\n            if (!is_directory(fname))\n                files.push_back(fname);\n        }\n        closedir(dp);\n    }\n\n    sort(files.begin(), files.end());\n\n    return files;\n}\n\n#endif\n\n// get_num_physical_cores is copy from\n// https://github.com/ggerganov/llama.cpp/blob/master/examples/common.cpp\n// LICENSE: https://github.com/ggerganov/llama.cpp/blob/master/LICENSE\nint32_t get_num_physical_cores() {\n#ifdef __linux__\n    // enumerate the set of thread siblings, num entries is num cores\n    std::unordered_set<std::string> siblings;\n    for (uint32_t cpu = 0; cpu < UINT32_MAX; ++cpu) {\n        std::ifstream thread_siblings(\"/sys/devices/system/cpu\" + std::to_string(cpu) + \"/topology/thread_siblings\");\n        if (!thread_siblings.is_open()) {\n            break;  // no more cpus\n        }\n        std::string line;\n        if (std::getline(thread_siblings, line)) {\n            siblings.insert(line);\n        }\n    }\n    if (siblings.size() > 0) {\n        return static_cast<int32_t>(siblings.size());\n    }\n#elif defined(__APPLE__) && defined(__MACH__)\n    int32_t num_physical_cores;\n    size_t len = sizeof(num_physical_cores);\n    int result = sysctlbyname(\"hw.perflevel0.physicalcpu\", &num_physical_cores, &len, NULL, 0);\n    if (result == 0) {\n        return num_physical_cores;\n    }\n    result = sysctlbyname(\"hw.physicalcpu\", &num_physical_cores, &len, NULL, 0);\n    if (result == 0) {\n        return num_physical_cores;\n    }\n#elif defined(_WIN32)\n    // TODO: Implement\n#endif\n    unsigned int n_threads = std::thread::hardware_concurrency();\n    return n_threads > 0 ? (n_threads <= 4 ? n_threads : n_threads / 2) : 4;\n}\n\nstatic sd_progress_cb_t sd_progress_cb = NULL;\nvoid* sd_progress_cb_data              = NULL;\n\nstd::u32string utf8_to_utf32(const std::string& utf8_str) {\n    std::wstring_convert<std::codecvt_utf8<char32_t>, char32_t> converter;\n    return converter.from_bytes(utf8_str);\n}\n\nstd::string utf32_to_utf8(const std::u32string& utf32_str) {\n    std::wstring_convert<std::codecvt_utf8<char32_t>, char32_t> converter;\n    return converter.to_bytes(utf32_str);\n}\n\nstd::u32string unicode_value_to_utf32(int unicode_value) {\n    std::u32string utf32_string = {static_cast<char32_t>(unicode_value)};\n    return utf32_string;\n}\n\nstatic std::string sd_basename(const std::string& path) {\n    size_t pos = path.find_last_of('/');\n    if (pos != std::string::npos) {\n        return path.substr(pos + 1);\n    }\n    pos = path.find_last_of('\\\\');\n    if (pos != std::string::npos) {\n        return path.substr(pos + 1);\n    }\n    return path;\n}\n\nstd::string path_join(const std::string& p1, const std::string& p2) {\n    if (p1.empty()) {\n        return p2;\n    }\n\n    if (p2.empty()) {\n        return p1;\n    }\n\n    if (p1[p1.length() - 1] == '/' || p1[p1.length() - 1] == '\\\\') {\n        return p1 + p2;\n    }\n\n    return p1 + \"/\" + p2;\n}\n\nstd::vector<std::string> splitString(const std::string& str, char delimiter) {\n    std::vector<std::string> result;\n    size_t start = 0;\n    size_t end   = str.find(delimiter);\n\n    while (end != std::string::npos) {\n        result.push_back(str.substr(start, end - start));\n        start = end + 1;\n        end   = str.find(delimiter, start);\n    }\n\n    // Add the last segment after the last delimiter\n    result.push_back(str.substr(start));\n\n    return result;\n}\n\nsd_image_t* preprocess_id_image(sd_image_t* img) {\n    int shortest_edge   = 224;\n    int size            = shortest_edge;\n    sd_image_t* resized = NULL;\n    uint32_t w          = img->width;\n    uint32_t h          = img->height;\n    uint32_t c          = img->channel;\n\n    // 1. do resize using stb_resize functions\n\n    unsigned char* buf = (unsigned char*)malloc(sizeof(unsigned char) * 3 * size * size);\n    if (!stbir_resize_uint8(img->data, w, h, 0,\n                            buf, size, size, 0,\n                            c)) {\n        fprintf(stderr, \"%s: resize operation failed \\n \", __func__);\n        return resized;\n    }\n\n    // 2. do center crop (likely unnecessary due to step 1)\n\n    // 3. do rescale\n\n    // 4. do normalize\n\n    // 3 and 4 will need to be done in float format.\n\n    resized = new sd_image_t{(uint32_t)shortest_edge,\n                             (uint32_t)shortest_edge,\n                             3,\n                             buf};\n    return resized;\n}\n\nvoid pretty_progress(int step, int steps, float time) {\n    if (sd_progress_cb) {\n        sd_progress_cb(step, steps, time, sd_progress_cb_data);\n        return;\n    }\n    if (step == 0) {\n        return;\n    }\n    std::string progress = \"  |\";\n    int max_progress     = 50;\n    int32_t current      = (int32_t)(step * 1.f * max_progress / steps);\n    for (int i = 0; i < 50; i++) {\n        if (i > current) {\n            progress += \" \";\n        } else if (i == current && i != max_progress - 1) {\n            progress += \">\";\n        } else {\n            progress += \"=\";\n        }\n    }\n    progress += \"|\";\n    printf(time > 1.0f ? \"\\r%s %i/%i - %.2fs/it\" : \"\\r%s %i/%i - %.2fit/s\\033[K\",\n           progress.c_str(), step, steps,\n           time > 1.0f || time == 0 ? time : (1.0f / time));\n    fflush(stdout);  // for linux\n    if (step == steps) {\n        printf(\"\\n\");\n    }\n}\n\nstd::string ltrim(const std::string& s) {\n    auto it = std::find_if(s.begin(), s.end(), [](int ch) {\n        return !std::isspace(ch);\n    });\n    return std::string(it, s.end());\n}\n\nstd::string rtrim(const std::string& s) {\n    auto it = std::find_if(s.rbegin(), s.rend(), [](int ch) {\n        return !std::isspace(ch);\n    });\n    return std::string(s.begin(), it.base());\n}\n\nstd::string trim(const std::string& s) {\n    return rtrim(ltrim(s));\n}\n\nstatic sd_log_cb_t sd_log_cb = NULL;\nvoid* sd_log_cb_data         = NULL;\n\n#define LOG_BUFFER_SIZE 1024\n\nvoid log_printf(sd_log_level_t level, const char* file, int line, const char* format, ...) {\n    va_list args;\n    va_start(args, format);\n\n    static char log_buffer[LOG_BUFFER_SIZE + 1];\n    int written = snprintf(log_buffer, LOG_BUFFER_SIZE, \"%s:%-4d - \", sd_basename(file).c_str(), line);\n\n    if (written >= 0 && written < LOG_BUFFER_SIZE) {\n        vsnprintf(log_buffer + written, LOG_BUFFER_SIZE - written, format, args);\n    }\n    strncat(log_buffer, \"\\n\", LOG_BUFFER_SIZE - strlen(log_buffer));\n\n    if (sd_log_cb) {\n        sd_log_cb(level, log_buffer, sd_log_cb_data);\n    }\n\n    va_end(args);\n}\n\nvoid sd_set_log_callback(sd_log_cb_t cb, void* data) {\n    sd_log_cb      = cb;\n    sd_log_cb_data = data;\n}\nvoid sd_set_progress_callback(sd_progress_cb_t cb, void* data) {\n    sd_progress_cb      = cb;\n    sd_progress_cb_data = data;\n}\nconst char* sd_get_system_info() {\n    static char buffer[1024];\n    std::stringstream ss;\n    ss << \"System Info: \\n\";\n    ss << \"    SSE3 = \" << ggml_cpu_has_sse3() << std::endl;\n    ss << \"    AVX = \" << ggml_cpu_has_avx() << std::endl;\n    ss << \"    AVX2 = \" << ggml_cpu_has_avx2() << std::endl;\n    ss << \"    AVX512 = \" << ggml_cpu_has_avx512() << std::endl;\n    ss << \"    AVX512_VBMI = \" << ggml_cpu_has_avx512_vbmi() << std::endl;\n    ss << \"    AVX512_VNNI = \" << ggml_cpu_has_avx512_vnni() << std::endl;\n    ss << \"    FMA = \" << ggml_cpu_has_fma() << std::endl;\n    ss << \"    NEON = \" << ggml_cpu_has_neon() << std::endl;\n    ss << \"    ARM_FMA = \" << ggml_cpu_has_arm_fma() << std::endl;\n    ss << \"    F16C = \" << ggml_cpu_has_f16c() << std::endl;\n    ss << \"    FP16_VA = \" << ggml_cpu_has_fp16_va() << std::endl;\n    ss << \"    WASM_SIMD = \" << ggml_cpu_has_wasm_simd() << std::endl;\n    ss << \"    VSX = \" << ggml_cpu_has_vsx() << std::endl;\n    snprintf(buffer, sizeof(buffer), \"%s\", ss.str().c_str());\n    return buffer;\n}\n\nconst char* sd_type_name(enum sd_type_t type) {\n    return ggml_type_name((ggml_type)type);\n}\n\nsd_image_f32_t sd_image_t_to_sd_image_f32_t(sd_image_t image) {\n    sd_image_f32_t converted_image;\n    converted_image.width   = image.width;\n    converted_image.height  = image.height;\n    converted_image.channel = image.channel;\n\n    // Allocate memory for float data\n    converted_image.data = (float*)malloc(image.width * image.height * image.channel * sizeof(float));\n\n    for (int i = 0; i < image.width * image.height * image.channel; i++) {\n        // Convert uint8_t to float\n        converted_image.data[i] = (float)image.data[i];\n    }\n\n    return converted_image;\n}\n\n// Function to perform double linear interpolation\nfloat interpolate(float v1, float v2, float v3, float v4, float x_ratio, float y_ratio) {\n    return v1 * (1 - x_ratio) * (1 - y_ratio) + v2 * x_ratio * (1 - y_ratio) + v3 * (1 - x_ratio) * y_ratio + v4 * x_ratio * y_ratio;\n}\n\nsd_image_f32_t resize_sd_image_f32_t(sd_image_f32_t image, int target_width, int target_height) {\n    sd_image_f32_t resized_image;\n    resized_image.width   = target_width;\n    resized_image.height  = target_height;\n    resized_image.channel = image.channel;\n\n    // Allocate memory for resized float data\n    resized_image.data = (float*)malloc(target_width * target_height * image.channel * sizeof(float));\n\n    for (int y = 0; y < target_height; y++) {\n        for (int x = 0; x < target_width; x++) {\n            float original_x = (float)x * image.width / target_width;\n            float original_y = (float)y * image.height / target_height;\n\n            int x1 = (int)original_x;\n            int y1 = (int)original_y;\n            int x2 = x1 + 1;\n            int y2 = y1 + 1;\n\n            for (int k = 0; k < image.channel; k++) {\n                float v1 = *(image.data + y1 * image.width * image.channel + x1 * image.channel + k);\n                float v2 = *(image.data + y1 * image.width * image.channel + x2 * image.channel + k);\n                float v3 = *(image.data + y2 * image.width * image.channel + x1 * image.channel + k);\n                float v4 = *(image.data + y2 * image.width * image.channel + x2 * image.channel + k);\n\n                float x_ratio = original_x - x1;\n                float y_ratio = original_y - y1;\n\n                float value = interpolate(v1, v2, v3, v4, x_ratio, y_ratio);\n\n                *(resized_image.data + y * target_width * image.channel + x * image.channel + k) = value;\n            }\n        }\n    }\n\n    return resized_image;\n}\n\nvoid normalize_sd_image_f32_t(sd_image_f32_t image, float means[3], float stds[3]) {\n    for (int y = 0; y < image.height; y++) {\n        for (int x = 0; x < image.width; x++) {\n            for (int k = 0; k < image.channel; k++) {\n                int index         = (y * image.width + x) * image.channel + k;\n                image.data[index] = (image.data[index] - means[k]) / stds[k];\n            }\n        }\n    }\n}\n\n// Constants for means and std\nfloat means[3] = {0.48145466, 0.4578275, 0.40821073};\nfloat stds[3]  = {0.26862954, 0.26130258, 0.27577711};\n\n// Function to clip and preprocess sd_image_f32_t\nsd_image_f32_t clip_preprocess(sd_image_f32_t image, int size) {\n    float scale = (float)size / fmin(image.width, image.height);\n\n    // Interpolation\n    int new_width       = (int)(scale * image.width);\n    int new_height      = (int)(scale * image.height);\n    float* resized_data = (float*)malloc(new_width * new_height * image.channel * sizeof(float));\n\n    for (int y = 0; y < new_height; y++) {\n        for (int x = 0; x < new_width; x++) {\n            float original_x = (float)x * image.width / new_width;\n            float original_y = (float)y * image.height / new_height;\n\n            int x1 = (int)original_x;\n            int y1 = (int)original_y;\n            int x2 = x1 + 1;\n            int y2 = y1 + 1;\n\n            for (int k = 0; k < image.channel; k++) {\n                float v1 = *(image.data + y1 * image.width * image.channel + x1 * image.channel + k);\n                float v2 = *(image.data + y1 * image.width * image.channel + x2 * image.channel + k);\n                float v3 = *(image.data + y2 * image.width * image.channel + x1 * image.channel + k);\n                float v4 = *(image.data + y2 * image.width * image.channel + x2 * image.channel + k);\n\n                float x_ratio = original_x - x1;\n                float y_ratio = original_y - y1;\n\n                float value = interpolate(v1, v2, v3, v4, x_ratio, y_ratio);\n\n                *(resized_data + y * new_width * image.channel + x * image.channel + k) = value;\n            }\n        }\n    }\n\n    // Clip and preprocess\n    int h = (new_height - size) / 2;\n    int w = (new_width - size) / 2;\n\n    sd_image_f32_t result;\n    result.width   = size;\n    result.height  = size;\n    result.channel = image.channel;\n    result.data    = (float*)malloc(size * size * image.channel * sizeof(float));\n\n    for (int k = 0; k < image.channel; k++) {\n        for (int i = 0; i < size; i++) {\n            for (int j = 0; j < size; j++) {\n                *(result.data + i * size * image.channel + j * image.channel + k) =\n                    fmin(fmax(*(resized_data + (i + h) * new_width * image.channel + (j + w) * image.channel + k), 0.0f), 255.0f) / 255.0f;\n            }\n        }\n    }\n\n    // Free allocated memory\n    free(resized_data);\n\n    // Normalize\n    for (int k = 0; k < image.channel; k++) {\n        for (int i = 0; i < size; i++) {\n            for (int j = 0; j < size; j++) {\n                // *(result.data + i * size * image.channel + j * image.channel + k) = 0.5f;\n                int offset  = i * size * image.channel + j * image.channel + k;\n                float value = *(result.data + offset);\n                value       = (value - means[k]) / stds[k];\n                // value = 0.5f;\n                *(result.data + offset) = value;\n            }\n        }\n    }\n\n    return result;\n}\n\n// Ref: https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/cad87bf4e3e0b0a759afa94e933527c3123d59bc/modules/prompt_parser.py#L345\n//\n// Parses a string with attention tokens and returns a list of pairs: text and its associated weight.\n// Accepted tokens are:\n//   (abc) - increases attention to abc by a multiplier of 1.1\n//   (abc:3.12) - increases attention to abc by a multiplier of 3.12\n//   [abc] - decreases attention to abc by a multiplier of 1.1\n//   \\( - literal character '('\n//   \\[ - literal character '['\n//   \\) - literal character ')'\n//   \\] - literal character ']'\n//   \\\\ - literal character '\\'\n//   anything else - just text\n//\n// >>> parse_prompt_attention('normal text')\n// [['normal text', 1.0]]\n// >>> parse_prompt_attention('an (important) word')\n// [['an ', 1.0], ['important', 1.1], [' word', 1.0]]\n// >>> parse_prompt_attention('(unbalanced')\n// [['unbalanced', 1.1]]\n// >>> parse_prompt_attention('\\(literal\\]')\n// [['(literal]', 1.0]]\n// >>> parse_prompt_attention('(unnecessary)(parens)')\n// [['unnecessaryparens', 1.1]]\n// >>> parse_prompt_attention('a (((house:1.3)) [on] a (hill:0.5), sun, (((sky))).')\n// [['a ', 1.0],\n//  ['house', 1.5730000000000004],\n//  [' ', 1.1],\n//  ['on', 1.0],\n//  [' a ', 1.1],\n//  ['hill', 0.55],\n//  [', sun, ', 1.1],\n//  ['sky', 1.4641000000000006],\n//  ['.', 1.1]]\nstd::vector<std::pair<std::string, float>> parse_prompt_attention(const std::string& text) {\n    std::vector<std::pair<std::string, float>> res;\n    std::vector<int> round_brackets;\n    std::vector<int> square_brackets;\n\n    float round_bracket_multiplier  = 1.1f;\n    float square_bracket_multiplier = 1 / 1.1f;\n\n    std::regex re_attention(R\"(\\\\\\(|\\\\\\)|\\\\\\[|\\\\\\]|\\\\\\\\|\\\\|\\(|\\[|:([+-]?[.\\d]+)\\)|\\)|\\]|[^\\\\()\\[\\]:]+|:)\");\n    std::regex re_break(R\"(\\s*\\bBREAK\\b\\s*)\");\n\n    auto multiply_range = [&](int start_position, float multiplier) {\n        for (int p = start_position; p < res.size(); ++p) {\n            res[p].second *= multiplier;\n        }\n    };\n\n    std::smatch m;\n    std::string remaining_text = text;\n\n    while (std::regex_search(remaining_text, m, re_attention)) {\n        std::string text   = m[0];\n        std::string weight = m[1];\n\n        if (text == \"(\") {\n            round_brackets.push_back((int)res.size());\n        } else if (text == \"[\") {\n            square_brackets.push_back((int)res.size());\n        } else if (!weight.empty()) {\n            if (!round_brackets.empty()) {\n                multiply_range(round_brackets.back(), std::stof(weight));\n                round_brackets.pop_back();\n            }\n        } else if (text == \")\" && !round_brackets.empty()) {\n            multiply_range(round_brackets.back(), round_bracket_multiplier);\n            round_brackets.pop_back();\n        } else if (text == \"]\" && !square_brackets.empty()) {\n            multiply_range(square_brackets.back(), square_bracket_multiplier);\n            square_brackets.pop_back();\n        } else if (text == \"\\\\(\") {\n            res.push_back({text.substr(1), 1.0f});\n        } else {\n            res.push_back({text, 1.0f});\n        }\n\n        remaining_text = m.suffix();\n    }\n\n    for (int pos : round_brackets) {\n        multiply_range(pos, round_bracket_multiplier);\n    }\n\n    for (int pos : square_brackets) {\n        multiply_range(pos, square_bracket_multiplier);\n    }\n\n    if (res.empty()) {\n        res.push_back({\"\", 1.0f});\n    }\n\n    int i = 0;\n    while (i + 1 < res.size()) {\n        if (res[i].second == res[i + 1].second) {\n            res[i].first += res[i + 1].first;\n            res.erase(res.begin() + i + 1);\n        } else {\n            ++i;\n        }\n    }\n\n    return res;\n}"
        },
        {
          "name": "util.h",
          "type": "blob",
          "size": 2.189453125,
          "content": "#ifndef __UTIL_H__\n#define __UTIL_H__\n\n#include <cstdint>\n#include <string>\n#include <vector>\n\n#include \"stable-diffusion.h\"\n\nbool ends_with(const std::string& str, const std::string& ending);\nbool starts_with(const std::string& str, const std::string& start);\nbool contains(const std::string& str, const std::string& substr);\n\nstd::string format(const char* fmt, ...);\n\nvoid replace_all_chars(std::string& str, char target, char replacement);\n\nbool file_exists(const std::string& filename);\nbool is_directory(const std::string& path);\nstd::string get_full_path(const std::string& dir, const std::string& filename);\n\nstd::vector<std::string> get_files_from_dir(const std::string& dir);\n\nstd::u32string utf8_to_utf32(const std::string& utf8_str);\nstd::string utf32_to_utf8(const std::u32string& utf32_str);\nstd::u32string unicode_value_to_utf32(int unicode_value);\n\nsd_image_t* preprocess_id_image(sd_image_t* img);\n\n// std::string sd_basename(const std::string& path);\n\ntypedef struct {\n    uint32_t width;\n    uint32_t height;\n    uint32_t channel;\n    float* data;\n} sd_image_f32_t;\n\nvoid normalize_sd_image_f32_t(sd_image_f32_t image, float means[3], float stds[3]);\n\nsd_image_f32_t sd_image_t_to_sd_image_f32_t(sd_image_t image);\n\nsd_image_f32_t resize_sd_image_f32_t(sd_image_f32_t image, int target_width, int target_height);\n\nsd_image_f32_t clip_preprocess(sd_image_f32_t image, int size);\n\nstd::string path_join(const std::string& p1, const std::string& p2);\nstd::vector<std::string> splitString(const std::string& str, char delimiter);\nvoid pretty_progress(int step, int steps, float time);\n\nvoid log_printf(sd_log_level_t level, const char* file, int line, const char* format, ...);\n\nstd::string trim(const std::string& s);\n\nstd::vector<std::pair<std::string, float>> parse_prompt_attention(const std::string& text);\n\n#define LOG_DEBUG(format, ...) log_printf(SD_LOG_DEBUG, __FILE__, __LINE__, format, ##__VA_ARGS__)\n#define LOG_INFO(format, ...) log_printf(SD_LOG_INFO, __FILE__, __LINE__, format, ##__VA_ARGS__)\n#define LOG_WARN(format, ...) log_printf(SD_LOG_WARN, __FILE__, __LINE__, format, ##__VA_ARGS__)\n#define LOG_ERROR(format, ...) log_printf(SD_LOG_ERROR, __FILE__, __LINE__, format, ##__VA_ARGS__)\n#endif  // __UTIL_H__\n"
        },
        {
          "name": "vae.hpp",
          "type": "blob",
          "size": 25.287109375,
          "content": "#ifndef __VAE_HPP__\n#define __VAE_HPP__\n\n#include \"common.hpp\"\n#include \"ggml_extend.hpp\"\n\n/*================================================== AutoEncoderKL ===================================================*/\n\n#define VAE_GRAPH_SIZE 20480\n\nclass ResnetBlock : public UnaryBlock {\nprotected:\n    int64_t in_channels;\n    int64_t out_channels;\n\npublic:\n    ResnetBlock(int64_t in_channels,\n                int64_t out_channels)\n        : in_channels(in_channels),\n          out_channels(out_channels) {\n        // temb_channels is always 0\n        blocks[\"norm1\"] = std::shared_ptr<GGMLBlock>(new GroupNorm32(in_channels));\n        blocks[\"conv1\"] = std::shared_ptr<GGMLBlock>(new Conv2d(in_channels, out_channels, {3, 3}, {1, 1}, {1, 1}));\n\n        blocks[\"norm2\"] = std::shared_ptr<GGMLBlock>(new GroupNorm32(out_channels));\n        blocks[\"conv2\"] = std::shared_ptr<GGMLBlock>(new Conv2d(out_channels, out_channels, {3, 3}, {1, 1}, {1, 1}));\n\n        if (out_channels != in_channels) {\n            blocks[\"nin_shortcut\"] = std::shared_ptr<GGMLBlock>(new Conv2d(in_channels, out_channels, {1, 1}));\n        }\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [N, in_channels, h, w]\n        // t_emb is always None\n        auto norm1 = std::dynamic_pointer_cast<GroupNorm32>(blocks[\"norm1\"]);\n        auto conv1 = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv1\"]);\n        auto norm2 = std::dynamic_pointer_cast<GroupNorm32>(blocks[\"norm2\"]);\n        auto conv2 = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv2\"]);\n\n        auto h = x;\n        h      = norm1->forward(ctx, h);\n        h      = ggml_silu_inplace(ctx, h);  // swish\n        h      = conv1->forward(ctx, h);\n        // return h;\n\n        h = norm2->forward(ctx, h);\n        h = ggml_silu_inplace(ctx, h);  // swish\n        // dropout, skip for inference\n        h = conv2->forward(ctx, h);\n\n        // skip connection\n        if (out_channels != in_channels) {\n            auto nin_shortcut = std::dynamic_pointer_cast<Conv2d>(blocks[\"nin_shortcut\"]);\n\n            x = nin_shortcut->forward(ctx, x);  // [N, out_channels, h, w]\n        }\n\n        h = ggml_add(ctx, h, x);\n        return h;  // [N, out_channels, h, w]\n    }\n};\n\nclass AttnBlock : public UnaryBlock {\nprotected:\n    int64_t in_channels;\n\npublic:\n    AttnBlock(int64_t in_channels)\n        : in_channels(in_channels) {\n        blocks[\"norm\"] = std::shared_ptr<GGMLBlock>(new GroupNorm32(in_channels));\n        blocks[\"q\"]    = std::shared_ptr<GGMLBlock>(new Conv2d(in_channels, in_channels, {1, 1}));\n        blocks[\"k\"]    = std::shared_ptr<GGMLBlock>(new Conv2d(in_channels, in_channels, {1, 1}));\n        blocks[\"v\"]    = std::shared_ptr<GGMLBlock>(new Conv2d(in_channels, in_channels, {1, 1}));\n\n        blocks[\"proj_out\"] = std::shared_ptr<GGMLBlock>(new Conv2d(in_channels, in_channels, {1, 1}));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [N, in_channels, h, w]\n        auto norm     = std::dynamic_pointer_cast<GroupNorm32>(blocks[\"norm\"]);\n        auto q_proj   = std::dynamic_pointer_cast<Conv2d>(blocks[\"q\"]);\n        auto k_proj   = std::dynamic_pointer_cast<Conv2d>(blocks[\"k\"]);\n        auto v_proj   = std::dynamic_pointer_cast<Conv2d>(blocks[\"v\"]);\n        auto proj_out = std::dynamic_pointer_cast<Conv2d>(blocks[\"proj_out\"]);\n\n        auto h_ = norm->forward(ctx, x);\n\n        const int64_t n = h_->ne[3];\n        const int64_t c = h_->ne[2];\n        const int64_t h = h_->ne[1];\n        const int64_t w = h_->ne[0];\n\n        auto q = q_proj->forward(ctx, h_);                          // [N, in_channels, h, w]\n        q      = ggml_cont(ctx, ggml_permute(ctx, q, 1, 2, 0, 3));  // [N, h, w, in_channels]\n        q      = ggml_reshape_3d(ctx, q, c, h * w, n);              // [N, h * w, in_channels]\n\n        auto k = k_proj->forward(ctx, h_);                          // [N, in_channels, h, w]\n        k      = ggml_cont(ctx, ggml_permute(ctx, k, 1, 2, 0, 3));  // [N, h, w, in_channels]\n        k      = ggml_reshape_3d(ctx, k, c, h * w, n);              // [N, h * w, in_channels]\n\n        auto v = v_proj->forward(ctx, h_);              // [N, in_channels, h, w]\n        v      = ggml_reshape_3d(ctx, v, h * w, c, n);  // [N, in_channels, h * w]\n\n        h_ = ggml_nn_attention(ctx, q, k, v, false);  // [N, h * w, in_channels]\n\n        h_ = ggml_cont(ctx, ggml_permute(ctx, h_, 1, 0, 2, 3));  // [N, in_channels, h * w]\n        h_ = ggml_reshape_4d(ctx, h_, w, h, c, n);               // [N, in_channels, h, w]\n\n        h_ = proj_out->forward(ctx, h_);  // [N, in_channels, h, w]\n\n        h_ = ggml_add(ctx, h_, x);\n        return h_;\n    }\n};\n\nclass AE3DConv : public Conv2d {\npublic:\n    AE3DConv(int64_t in_channels,\n             int64_t out_channels,\n             std::pair<int, int> kernel_size,\n             int64_t video_kernel_size    = 3,\n             std::pair<int, int> stride   = {1, 1},\n             std::pair<int, int> padding  = {0, 0},\n             std::pair<int, int> dilation = {1, 1},\n             bool bias                    = true)\n        : Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias) {\n        int64_t kernel_padding  = video_kernel_size / 2;\n        blocks[\"time_mix_conv\"] = std::shared_ptr<GGMLBlock>(new Conv3dnx1x1(out_channels,\n                                                                             out_channels,\n                                                                             video_kernel_size,\n                                                                             1,\n                                                                             kernel_padding));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx,\n                                struct ggml_tensor* x) {\n        // timesteps always None\n        // skip_video always False\n        // x: [N, IC, IH, IW]\n        // result: [N, OC, OH, OW]\n        auto time_mix_conv = std::dynamic_pointer_cast<Conv3dnx1x1>(blocks[\"time_mix_conv\"]);\n\n        x = Conv2d::forward(ctx, x);\n        // timesteps = x.shape[0]\n        // x = rearrange(x, \"(b t) c h w -> b c t h w\", t=timesteps)\n        // x = conv3d(x)\n        // return rearrange(x, \"b c t h w -> (b t) c h w\")\n        int64_t T = x->ne[3];\n        int64_t B = x->ne[3] / T;\n        int64_t C = x->ne[2];\n        int64_t H = x->ne[1];\n        int64_t W = x->ne[0];\n\n        x = ggml_reshape_4d(ctx, x, W * H, C, T, B);           // (b t) c h w -> b t c (h w)\n        x = ggml_cont(ctx, ggml_permute(ctx, x, 0, 2, 1, 3));  // b t c (h w) -> b c t (h w)\n        x = time_mix_conv->forward(ctx, x);                    // [B, OC, T, OH * OW]\n        x = ggml_cont(ctx, ggml_permute(ctx, x, 0, 2, 1, 3));  // b c t (h w) -> b t c (h w)\n        x = ggml_reshape_4d(ctx, x, W, H, C, T * B);           // b t c (h w) -> (b t) c h w\n        return x;                                              // [B*T, OC, OH, OW]\n    }\n};\n\nclass VideoResnetBlock : public ResnetBlock {\nprotected:\n    void init_params(struct ggml_context* ctx, std::map<std::string, enum ggml_type>& tensor_types, const std::string prefix = \"\") {\n        enum ggml_type wtype = (tensor_types.find(prefix + \"mix_factor\") != tensor_types.end()) ? tensor_types[prefix + \"mix_factor\"] : GGML_TYPE_F32;\n        params[\"mix_factor\"] = ggml_new_tensor_1d(ctx, wtype, 1);\n    }\n\n    float get_alpha() {\n        float alpha = ggml_backend_tensor_get_f32(params[\"mix_factor\"]);\n        return sigmoid(alpha);\n    }\n\npublic:\n    VideoResnetBlock(int64_t in_channels,\n                     int64_t out_channels,\n                     int video_kernel_size = 3)\n        : ResnetBlock(in_channels, out_channels) {\n        // merge_strategy is always learned\n        blocks[\"time_stack\"] = std::shared_ptr<GGMLBlock>(new ResBlock(out_channels, 0, out_channels, {video_kernel_size, 1}, 3, false, true));\n    }\n\n    struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [N, in_channels, h, w] aka [b*t, in_channels, h, w]\n        // return: [N, out_channels, h, w] aka [b*t, out_channels, h, w]\n        // t_emb is always None\n        // skip_video is always False\n        // timesteps is always None\n        auto time_stack = std::dynamic_pointer_cast<ResBlock>(blocks[\"time_stack\"]);\n\n        x = ResnetBlock::forward(ctx, x);  // [N, out_channels, h, w]\n        // return x;\n\n        int64_t T = x->ne[3];\n        int64_t B = x->ne[3] / T;\n        int64_t C = x->ne[2];\n        int64_t H = x->ne[1];\n        int64_t W = x->ne[0];\n\n        x          = ggml_reshape_4d(ctx, x, W * H, C, T, B);           // (b t) c h w -> b t c (h w)\n        x          = ggml_cont(ctx, ggml_permute(ctx, x, 0, 2, 1, 3));  // b t c (h w) -> b c t (h w)\n        auto x_mix = x;\n\n        x = time_stack->forward(ctx, x);  // b t c (h w)\n\n        float alpha = get_alpha();\n        x           = ggml_add(ctx,\n                               ggml_scale(ctx, x, alpha),\n                               ggml_scale(ctx, x_mix, 1.0f - alpha));\n\n        x = ggml_cont(ctx, ggml_permute(ctx, x, 0, 2, 1, 3));  // b c t (h w) -> b t c (h w)\n        x = ggml_reshape_4d(ctx, x, W, H, C, T * B);           // b t c (h w) -> (b t) c h w\n\n        return x;\n    }\n};\n\n// ldm.modules.diffusionmodules.model.Encoder\nclass Encoder : public GGMLBlock {\nprotected:\n    int ch                   = 128;\n    std::vector<int> ch_mult = {1, 2, 4, 4};\n    int num_res_blocks       = 2;\n    int in_channels          = 3;\n    int z_channels           = 4;\n    bool double_z            = true;\n\npublic:\n    Encoder(int ch,\n            std::vector<int> ch_mult,\n            int num_res_blocks,\n            int in_channels,\n            int z_channels,\n            bool double_z = true)\n        : ch(ch),\n          ch_mult(ch_mult),\n          num_res_blocks(num_res_blocks),\n          in_channels(in_channels),\n          z_channels(z_channels),\n          double_z(double_z) {\n        blocks[\"conv_in\"] = std::shared_ptr<GGMLBlock>(new Conv2d(in_channels, ch, {3, 3}, {1, 1}, {1, 1}));\n\n        size_t num_resolutions = ch_mult.size();\n\n        int block_in = 1;\n        for (int i = 0; i < num_resolutions; i++) {\n            if (i == 0) {\n                block_in = ch;\n            } else {\n                block_in = ch * ch_mult[i - 1];\n            }\n            int block_out = ch * ch_mult[i];\n            for (int j = 0; j < num_res_blocks; j++) {\n                std::string name = \"down.\" + std::to_string(i) + \".block.\" + std::to_string(j);\n                blocks[name]     = std::shared_ptr<GGMLBlock>(new ResnetBlock(block_in, block_out));\n                block_in         = block_out;\n            }\n            if (i != num_resolutions - 1) {\n                std::string name = \"down.\" + std::to_string(i) + \".downsample\";\n                blocks[name]     = std::shared_ptr<GGMLBlock>(new DownSampleBlock(block_in, block_in, true));\n            }\n        }\n\n        blocks[\"mid.block_1\"] = std::shared_ptr<GGMLBlock>(new ResnetBlock(block_in, block_in));\n        blocks[\"mid.attn_1\"]  = std::shared_ptr<GGMLBlock>(new AttnBlock(block_in));\n        blocks[\"mid.block_2\"] = std::shared_ptr<GGMLBlock>(new ResnetBlock(block_in, block_in));\n\n        blocks[\"norm_out\"] = std::shared_ptr<GGMLBlock>(new GroupNorm32(block_in));\n        blocks[\"conv_out\"] = std::shared_ptr<GGMLBlock>(new Conv2d(block_in, double_z ? z_channels * 2 : z_channels, {3, 3}, {1, 1}, {1, 1}));\n    }\n\n    virtual struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [N, in_channels, h, w]\n\n        auto conv_in     = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv_in\"]);\n        auto mid_block_1 = std::dynamic_pointer_cast<ResnetBlock>(blocks[\"mid.block_1\"]);\n        auto mid_attn_1  = std::dynamic_pointer_cast<AttnBlock>(blocks[\"mid.attn_1\"]);\n        auto mid_block_2 = std::dynamic_pointer_cast<ResnetBlock>(blocks[\"mid.block_2\"]);\n        auto norm_out    = std::dynamic_pointer_cast<GroupNorm32>(blocks[\"norm_out\"]);\n        auto conv_out    = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv_out\"]);\n\n        auto h = conv_in->forward(ctx, x);  // [N, ch, h, w]\n\n        // downsampling\n        size_t num_resolutions = ch_mult.size();\n        for (int i = 0; i < num_resolutions; i++) {\n            for (int j = 0; j < num_res_blocks; j++) {\n                std::string name = \"down.\" + std::to_string(i) + \".block.\" + std::to_string(j);\n                auto down_block  = std::dynamic_pointer_cast<ResnetBlock>(blocks[name]);\n\n                h = down_block->forward(ctx, h);\n            }\n            if (i != num_resolutions - 1) {\n                std::string name = \"down.\" + std::to_string(i) + \".downsample\";\n                auto down_sample = std::dynamic_pointer_cast<DownSampleBlock>(blocks[name]);\n\n                h = down_sample->forward(ctx, h);\n            }\n        }\n\n        // middle\n        h = mid_block_1->forward(ctx, h);\n        h = mid_attn_1->forward(ctx, h);\n        h = mid_block_2->forward(ctx, h);  // [N, block_in, h, w]\n\n        // end\n        h = norm_out->forward(ctx, h);\n        h = ggml_silu_inplace(ctx, h);  // nonlinearity/swish\n        h = conv_out->forward(ctx, h);  // [N, z_channels*2, h, w]\n        return h;\n    }\n};\n\n// ldm.modules.diffusionmodules.model.Decoder\nclass Decoder : public GGMLBlock {\nprotected:\n    int ch                   = 128;\n    int out_ch               = 3;\n    std::vector<int> ch_mult = {1, 2, 4, 4};\n    int num_res_blocks       = 2;\n    int z_channels           = 4;\n    bool video_decoder       = false;\n    int video_kernel_size    = 3;\n\n    virtual std::shared_ptr<GGMLBlock> get_conv_out(int64_t in_channels,\n                                                    int64_t out_channels,\n                                                    std::pair<int, int> kernel_size,\n                                                    std::pair<int, int> stride  = {1, 1},\n                                                    std::pair<int, int> padding = {0, 0}) {\n        if (video_decoder) {\n            return std::shared_ptr<GGMLBlock>(new AE3DConv(in_channels, out_channels, kernel_size, video_kernel_size, stride, padding));\n        } else {\n            return std::shared_ptr<GGMLBlock>(new Conv2d(in_channels, out_channels, kernel_size, stride, padding));\n        }\n    }\n\n    virtual std::shared_ptr<GGMLBlock> get_resnet_block(int64_t in_channels,\n                                                        int64_t out_channels) {\n        if (video_decoder) {\n            return std::shared_ptr<GGMLBlock>(new VideoResnetBlock(in_channels, out_channels, video_kernel_size));\n        } else {\n            return std::shared_ptr<GGMLBlock>(new ResnetBlock(in_channels, out_channels));\n        }\n    }\n\npublic:\n    Decoder(int ch,\n            int out_ch,\n            std::vector<int> ch_mult,\n            int num_res_blocks,\n            int z_channels,\n            bool video_decoder    = false,\n            int video_kernel_size = 3)\n        : ch(ch),\n          out_ch(out_ch),\n          ch_mult(ch_mult),\n          num_res_blocks(num_res_blocks),\n          z_channels(z_channels),\n          video_decoder(video_decoder),\n          video_kernel_size(video_kernel_size) {\n        size_t num_resolutions = ch_mult.size();\n        int block_in           = ch * ch_mult[num_resolutions - 1];\n\n        blocks[\"conv_in\"] = std::shared_ptr<GGMLBlock>(new Conv2d(z_channels, block_in, {3, 3}, {1, 1}, {1, 1}));\n\n        blocks[\"mid.block_1\"] = get_resnet_block(block_in, block_in);\n        blocks[\"mid.attn_1\"]  = std::shared_ptr<GGMLBlock>(new AttnBlock(block_in));\n        blocks[\"mid.block_2\"] = get_resnet_block(block_in, block_in);\n\n        for (int i = num_resolutions - 1; i >= 0; i--) {\n            int mult      = ch_mult[i];\n            int block_out = ch * mult;\n            for (int j = 0; j < num_res_blocks + 1; j++) {\n                std::string name = \"up.\" + std::to_string(i) + \".block.\" + std::to_string(j);\n                blocks[name]     = get_resnet_block(block_in, block_out);\n\n                block_in = block_out;\n            }\n            if (i != 0) {\n                std::string name = \"up.\" + std::to_string(i) + \".upsample\";\n                blocks[name]     = std::shared_ptr<GGMLBlock>(new UpSampleBlock(block_in, block_in));\n            }\n        }\n\n        blocks[\"norm_out\"] = std::shared_ptr<GGMLBlock>(new GroupNorm32(block_in));\n        blocks[\"conv_out\"] = get_conv_out(block_in, out_ch, {3, 3}, {1, 1}, {1, 1});\n    }\n\n    virtual struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* z) {\n        // z: [N, z_channels, h, w]\n        // alpha is always 0\n        // merge_strategy is always learned\n        // time_mode is always conv-only, so we need to replace conv_out_op/resnet_op to AE3DConv/VideoResBlock\n        // AttnVideoBlock will not be used\n        auto conv_in     = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv_in\"]);\n        auto mid_block_1 = std::dynamic_pointer_cast<ResnetBlock>(blocks[\"mid.block_1\"]);\n        auto mid_attn_1  = std::dynamic_pointer_cast<AttnBlock>(blocks[\"mid.attn_1\"]);\n        auto mid_block_2 = std::dynamic_pointer_cast<ResnetBlock>(blocks[\"mid.block_2\"]);\n        auto norm_out    = std::dynamic_pointer_cast<GroupNorm32>(blocks[\"norm_out\"]);\n        auto conv_out    = std::dynamic_pointer_cast<Conv2d>(blocks[\"conv_out\"]);\n\n        // conv_in\n        auto h = conv_in->forward(ctx, z);  // [N, block_in, h, w]\n\n        // middle\n        h = mid_block_1->forward(ctx, h);\n        // return h;\n\n        h = mid_attn_1->forward(ctx, h);\n        h = mid_block_2->forward(ctx, h);  // [N, block_in, h, w]\n\n        // upsampling\n        size_t num_resolutions = ch_mult.size();\n        for (int i = num_resolutions - 1; i >= 0; i--) {\n            for (int j = 0; j < num_res_blocks + 1; j++) {\n                std::string name = \"up.\" + std::to_string(i) + \".block.\" + std::to_string(j);\n                auto up_block    = std::dynamic_pointer_cast<ResnetBlock>(blocks[name]);\n\n                h = up_block->forward(ctx, h);\n            }\n            if (i != 0) {\n                std::string name = \"up.\" + std::to_string(i) + \".upsample\";\n                auto up_sample   = std::dynamic_pointer_cast<UpSampleBlock>(blocks[name]);\n\n                h = up_sample->forward(ctx, h);\n            }\n        }\n\n        h = norm_out->forward(ctx, h);\n        h = ggml_silu_inplace(ctx, h);  // nonlinearity/swish\n        h = conv_out->forward(ctx, h);  // [N, out_ch, h*8, w*8]\n        return h;\n    }\n};\n\n// ldm.models.autoencoder.AutoencoderKL\nclass AutoencodingEngine : public GGMLBlock {\nprotected:\n    bool decode_only       = true;\n    bool use_video_decoder = false;\n    bool use_quant         = true;\n    int embed_dim          = 4;\n    struct {\n        int z_channels           = 4;\n        int resolution           = 256;\n        int in_channels          = 3;\n        int out_ch               = 3;\n        int ch                   = 128;\n        std::vector<int> ch_mult = {1, 2, 4, 4};\n        int num_res_blocks       = 2;\n        bool double_z            = true;\n    } dd_config;\n\npublic:\n    AutoencodingEngine(bool decode_only       = true,\n                       bool use_video_decoder = false,\n                       SDVersion version      = VERSION_SD1)\n        : decode_only(decode_only), use_video_decoder(use_video_decoder) {\n        if (sd_version_is_dit(version)) {\n            dd_config.z_channels = 16;\n            use_quant            = false;\n        }\n        if (use_video_decoder) {\n            use_quant = false;\n        }\n        blocks[\"decoder\"] = std::shared_ptr<GGMLBlock>(new Decoder(dd_config.ch,\n                                                                   dd_config.out_ch,\n                                                                   dd_config.ch_mult,\n                                                                   dd_config.num_res_blocks,\n                                                                   dd_config.z_channels,\n                                                                   use_video_decoder));\n        if (use_quant) {\n            blocks[\"post_quant_conv\"] = std::shared_ptr<GGMLBlock>(new Conv2d(dd_config.z_channels,\n                                                                              embed_dim,\n                                                                              {1, 1}));\n        }\n        if (!decode_only) {\n            blocks[\"encoder\"] = std::shared_ptr<GGMLBlock>(new Encoder(dd_config.ch,\n                                                                       dd_config.ch_mult,\n                                                                       dd_config.num_res_blocks,\n                                                                       dd_config.in_channels,\n                                                                       dd_config.z_channels,\n                                                                       dd_config.double_z));\n            if (use_quant) {\n                int factor = dd_config.double_z ? 2 : 1;\n\n                blocks[\"quant_conv\"] = std::shared_ptr<GGMLBlock>(new Conv2d(embed_dim * factor,\n                                                                             dd_config.z_channels * factor,\n                                                                             {1, 1}));\n            }\n        }\n    }\n\n    struct ggml_tensor* decode(struct ggml_context* ctx, struct ggml_tensor* z) {\n        // z: [N, z_channels, h, w]\n        if (use_quant) {\n            auto post_quant_conv = std::dynamic_pointer_cast<Conv2d>(blocks[\"post_quant_conv\"]);\n            z                    = post_quant_conv->forward(ctx, z);  // [N, z_channels, h, w]\n        }\n        auto decoder = std::dynamic_pointer_cast<Decoder>(blocks[\"decoder\"]);\n\n        ggml_set_name(z, \"bench-start\");\n        auto h = decoder->forward(ctx, z);\n        ggml_set_name(h, \"bench-end\");\n        return h;\n    }\n\n    struct ggml_tensor* encode(struct ggml_context* ctx, struct ggml_tensor* x) {\n        // x: [N, in_channels, h, w]\n        auto encoder = std::dynamic_pointer_cast<Encoder>(blocks[\"encoder\"]);\n\n        auto h = encoder->forward(ctx, x);  // [N, 2*z_channels, h/8, w/8]\n        if (use_quant) {\n            auto quant_conv = std::dynamic_pointer_cast<Conv2d>(blocks[\"quant_conv\"]);\n            h               = quant_conv->forward(ctx, h);  // [N, 2*embed_dim, h/8, w/8]\n        }\n        return h;\n    }\n};\n\nstruct AutoEncoderKL : public GGMLRunner {\n    bool decode_only = true;\n    AutoencodingEngine ae;\n\n    AutoEncoderKL(ggml_backend_t backend,\n                  std::map<std::string, enum ggml_type>& tensor_types,\n                  const std::string prefix,\n                  bool decode_only       = false,\n                  bool use_video_decoder = false,\n                  SDVersion version      = VERSION_SD1)\n        : decode_only(decode_only), ae(decode_only, use_video_decoder, version), GGMLRunner(backend) {\n        ae.init(params_ctx, tensor_types, prefix);\n    }\n\n    std::string get_desc() {\n        return \"vae\";\n    }\n\n    void get_param_tensors(std::map<std::string, struct ggml_tensor*>& tensors, const std::string prefix) {\n        ae.get_param_tensors(tensors, prefix);\n    }\n\n    struct ggml_cgraph* build_graph(struct ggml_tensor* z, bool decode_graph) {\n        struct ggml_cgraph* gf = ggml_new_graph(compute_ctx);\n\n        z = to_backend(z);\n\n        struct ggml_tensor* out = decode_graph ? ae.decode(compute_ctx, z) : ae.encode(compute_ctx, z);\n\n        ggml_build_forward_expand(gf, out);\n\n        return gf;\n    }\n\n    void compute(const int n_threads,\n                 struct ggml_tensor* z,\n                 bool decode_graph,\n                 struct ggml_tensor** output,\n                 struct ggml_context* output_ctx = NULL) {\n        auto get_graph = [&]() -> struct ggml_cgraph* {\n            return build_graph(z, decode_graph);\n        };\n        // ggml_set_f32(z, 0.5f);\n        // print_ggml_tensor(z);\n        GGMLRunner::compute(get_graph, n_threads, true, output, output_ctx);\n    }\n\n    void test() {\n        struct ggml_init_params params;\n        params.mem_size   = static_cast<size_t>(10 * 1024 * 1024);  // 10 MB\n        params.mem_buffer = NULL;\n        params.no_alloc   = false;\n\n        struct ggml_context* work_ctx = ggml_init(params);\n        GGML_ASSERT(work_ctx != NULL);\n\n        {\n            // CPU, x{1, 3, 64, 64}: Pass\n            // CUDA, x{1, 3, 64, 64}: Pass, but sill get wrong result for some image, may be due to interlnal nan\n            // CPU, x{2, 3, 64, 64}: Wrong result\n            // CUDA, x{2, 3, 64, 64}: Wrong result, and different from CPU result\n            auto x = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 64, 64, 3, 2);\n            ggml_set_f32(x, 0.5f);\n            print_ggml_tensor(x);\n            struct ggml_tensor* out = NULL;\n\n            int t0 = ggml_time_ms();\n            compute(8, x, false, &out, work_ctx);\n            int t1 = ggml_time_ms();\n\n            print_ggml_tensor(out);\n            LOG_DEBUG(\"encode test done in %dms\", t1 - t0);\n        }\n\n        if (false) {\n            // CPU, z{1, 4, 8, 8}: Pass\n            // CUDA, z{1, 4, 8, 8}: Pass\n            // CPU, z{3, 4, 8, 8}: Wrong result\n            // CUDA, z{3, 4, 8, 8}: Wrong result, and different from CPU result\n            auto z = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 8, 8, 4, 1);\n            ggml_set_f32(z, 0.5f);\n            print_ggml_tensor(z);\n            struct ggml_tensor* out = NULL;\n\n            int t0 = ggml_time_ms();\n            compute(8, z, true, &out, work_ctx);\n            int t1 = ggml_time_ms();\n\n            print_ggml_tensor(out);\n            LOG_DEBUG(\"decode test done in %dms\", t1 - t0);\n        }\n    };\n};\n\n#endif\n"
        },
        {
          "name": "vocab.hpp",
          "type": "blob",
          "size": 28795.8310546875,
          "content": ""
        }
      ]
    }
  ]
}