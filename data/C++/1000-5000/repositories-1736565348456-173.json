{
  "metadata": {
    "timestamp": 1736565348456,
    "page": 173,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ravenscroftj/turbopilot",
      "stars": 3819,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0302734375,
          "content": "ggml/build\nbuild\nmodels\nassets\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.013671875,
          "content": "build/\nmodels/"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.353515625,
          "content": "[submodule \"ggml\"]\n\tpath = extern/ggml\n\turl = git@github.com:ggerganov/ggml.git\n[submodule \"extern/argparse\"]\n\tpath = extern/argparse\n\turl = https://github.com/p-ranav/argparse.git\n[submodule \"extern/sbdlog\"]\n\tpath = extern/spdlog\n\turl = https://github.com/gabime/spdlog.git\n[submodule \"extern/ggml\"]\n\tpath = extern/ggml\n\turl = https://github.com/ggerganov/ggml\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "BUILD.md",
          "type": "blob",
          "size": 2.21875,
          "content": "# Build TurboPilot\n\nTurboPilot is a C++ program that uses the [GGML](https://github.com/ggerganov/ggml) project to parse and run language models.\n\n### Dependencies\n\nTo build turbopilot you will need CMake, Libboost, a C++ toolchain and GNU Make.\n\n#### Ubuntu\n\nOn Ubuntu you can install these things with:\n\n```bash\nsudo apt-get update\nsudo apt-get install libboost-dev cmake build-essential\n```\n\n#### MacOS\n\nIf you use [brew](https://brew.sh/) you can simply add these dependencies by running:\n\n```bash\nbrew install cmake boost\n```\n\n### Checkout Submodules\n\nMake sure the ggml subproject is checked out with `git submodule init` and `git submodule update`\n\n### Prepare and Build\n\nConfigure cmake to build the project with the following:\n\n```bash\nmkdir build\ncd build\ncmake ..\n```\n\nIf you are running on linux you can optionally compile a static build with `cmake -D CMAKE_EXE_LINKER_FLAGS=\"-static\" ..` which should make your binary portable across different flavours of the OS.\n\nFrom here you can now build the components that make up turbopilot by running:\n\n```bash\nmake\n```\n\n### Building with OpenBLAS\n\n\n[BLAS](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) libraries accelerate mathematical operations. You can use the OpenBLAS implementation with Turbopilot to make generation faster - particularly for longer prompts.\n\nWhen you run cmake, you can additionally set `-D GGML_OPENBLAS=On` to enable BLAS support.\n\nE.g. `cmake .. -D GGML_OPENBLAS=On`\n\n### Building with CuBLAS\n\nCuBLAS is the [BLAS](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) library provided by nvidia that runs linear algebra code on your GPU. This can speed up the application significantly, especially when working with long prompts.\n\n#### Install Cuda SDK for your Operating System\n\nYou will need `nvcc` and the `libcublas-dev` dependencies as a bare minimum. Follow the guide from nvidia [here](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/) for more detailed installation instructions.\n\n#### Configuring Cmake with CuBLAS\n\nYou will need to set `-DGGML_CUBLAS=ON` and also pass the path to your `nvcc` executable with `-DCMAKE_CUDA_COMPILER=/path/to/nvcc`.\n\nFull example: `cmake -DGGML_CUBLAS=ON -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc ..`\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 2.0107421875,
          "content": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n- Fixed issue with static build in docker not working.\n- Integrated CUDA functonality from llama.cpp upstream which accelerates inference for long prompts\n\n## [0.0.4] 2023-04-14\n\n- Added multi-threaded server support which should prevent health checks aimed at `GET /` from failing during prediction.\n- Separated autocomplete lambda into a separate C++ function so that it can be bound to `/v1/completions`, `/v1/engines/copilot-codex/completions` and `/v1/engines/codegen/completions`\n- Removed `model` from completion input as required param which stops the official copilot plugin from freaking out\n- Integrate latest changes from upstream ggml including some fixes for ARM NEON processor\n- Added Mac builds as part of CI\n- Support for fork of [vscode-fauxpilot](https://github.com/ravenscroftj/vscode-fauxpilot) with a progress indicator is now available ([PR](https://github.com/Venthe/vscode-fauxpilot/pull/26) is open upstream, please react/vote for it).\n\n\n## [0.0.3] 2023-04-13\n\n- Added 350M parameter codegen model to Google Drive folder\n- Added multi-arch docker images so that users can now directly run on Apple silicon and even raspberry pi\n- Now support pre-tokenized inputs passed into the API from a Python tokenizer (thanks to @thakkarparth007 for their PR - https://github.com/ravenscroftj/ggml/pull/2)\n\n\n## [0.0.2] - 2023-04-12\n\n- Project now builds on Mac OS (Thanks to @Dimitrije-V for their PR https://github.com/ravenscroftj/ggml/pull/1 and @dabdine for contributing some clearer Mac build instructions)\n- Fix inability to load vocab.json on converting the 16B model due to encoding of the file not being set by @swanserquack in #5\n- Improve performance of model by incorporating changes to GGML library from @ggerganov\n\n\n## [0.0.1] 2023-04-10\n\n- Turbopilot is born!"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 2.01953125,
          "content": "cmake_minimum_required (VERSION 3.0)\nproject(turbopilot VERSION 0.1.0)\nset(CMAKE_CXX_STANDARD 11)\nset(CMAKE_CXX_STANDARD_REQUIRED true)\nset(CMAKE_C_STANDARD 11)\nset(CMAKE_C_STANDARD_REQUIRED true)\nset(THREADS_PREFER_PTHREAD_FLAG ON)\nfind_package(Threads REQUIRED)\n\n\n# option(BUILD_SHARED_LIBS \"Build using shared libraries\" OFF)\n\n\nset(CMAKE_EXPORT_COMPILE_COMMANDS \"on\")\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\nset(CMAKE_INSTALL_RPATH \"${CMAKE_INSTALL_PREFIX}/lib\")\n\noption(GGML_CLBLAST                 \"ggml: use clBLAST\"                  OFF)\noption(GGML_CUBLAS                  \"ggml: use cuBLAS\"                   OFF)\n\n\n\nif (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"arm\" OR ${CMAKE_SYSTEM_PROCESSOR} MATCHES \"aarch64\")\n    message(STATUS \"ARM detected\")\n    if (MSVC)\n        # TODO: arm msvc?\n        message(STATUS \"ARM+MSVC= :( \")\n    else()\n        if (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"armv6\")\n            # Raspberry Pi 1, Zero\n            add_compile_options(-mfpu=neon-fp-armv8 -mfp16-format=ieee -mno-unaligned-access)\n        endif()\n        if (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"armv7\")\n            # Raspberry Pi 2\n            add_compile_options(-mfpu=neon-fp-armv8 -mfp16-format=ieee -mno-unaligned-access -funsafe-math-optimizations)\n        endif()\n        if (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"armv8\")\n            # Raspberry Pi 3, 4, Zero 2 (32-bit)\n            add_compile_options(-mfp16-format=ieee -mno-unaligned-access)\n        endif()\n    endif()\nendif()\n\n\n\n\n\nadd_subdirectory(extern/ggml)\nadd_subdirectory(extern/argparse)\nadd_subdirectory(extern/spdlog)\n\nif (GGML_STATIC)\n    SET(CMAKE_FIND_LIBRARY_SUFFIXES \".a\")\n    SET(BUILD_SHARED_LIBS OFF)\n    SET(CMAKE_EXE_LINKER_FLAGS \"-static\")\nendif()\n\nif (GGML_CUBLAS)\n    cmake_minimum_required(VERSION 3.17)\n\n    find_package(CUDAToolkit)\n    if (CUDAToolkit_FOUND)\n        add_compile_definitions(GGML_USE_CUBLAS)\n    else()\n        message(WARNING \"cuBLAS not found\")\n    endif()\nendif()\n\n\n\nadd_subdirectory(src)\n\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)"
        },
        {
          "name": "Dockerfile.default",
          "type": "blob",
          "size": 1.32421875,
          "content": "ARG BUILD_BASE=\"ubuntu:22.04\"\nARG RUNTIME_BASE=\"ubuntu:22.04\"\n\nFROM ${BUILD_BASE} AS build\n\nARG EXTRA_DEPS=\"\"\nARG CMAKE_ARGS=\"\"\n\nRUN echo \"CMAKE_ARGS: ${CMAKE_ARGS}\"\nRUN echo \"EXTRA_DEPS: ${EXTRA_DEPS}\"\n\nENV DEBIAN_FRONTEND=noninteractive\n\n# inlude kitware apt repo to allow us to grab latest cmake\nRUN apt-get update && apt-get install -y ca-certificates gpg wget\nRUN wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | gpg --dearmor - | tee /usr/share/keyrings/kitware-archive-keyring.gpg >/dev/null\nRUN echo 'deb [signed-by=/usr/share/keyrings/kitware-archive-keyring.gpg] https://apt.kitware.com/ubuntu/ jammy main' | tee /etc/apt/sources.list.d/kitware.list >/dev/null\n\nRUN apt-get update && apt-get install -y build-essential cmake libboost-dev libboost-thread-dev ${EXTRA_DEPS}\n\nADD ./ /turbopilot\n\nRUN mkdir /turbopilot/build\n\nWORKDIR  /turbopilot/build\n\nRUN cmake .. ${CMAKE_ARGS}\nRUN make turbopilot\n\nFROM ${RUNTIME_BASE} AS runtime\n\nARG RUNTIME_DEPS=\"\"\n\nRUN if [[ -z \"${RUNTIME_DEPS}\" ]] ; then echo \"No runtime libs required\" ; else  apt-get update && apt-get install -y ${RUNTIME_DEPS} ; fi\n\n\nWORKDIR /app\n\nCOPY --from=build /turbopilot/build/bin/turbopilot /app/turbopilot\n\nENV THREADS=4\n\nENV MODEL=\"/models/codegen-2B-multi-ggml-4bit-quant.bin\"\n\nENV BATCHSIZE=8\n\nCOPY ./run.sh /app/\n\nEXPOSE 18080\n\nCMD /app/run.sh"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.4306640625,
          "content": "Copyright 2023 James Ravenscroft\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
        },
        {
          "name": "MODELS.md",
          "type": "blob",
          "size": 6.2333984375,
          "content": "# Models Directory\n\n## StableCode Instruct State-of-the-art for low Spec machines(Released 8th August 2023)\n\n[StableCode](https://stability.ai/blog/stablecode-llm-generative-ai-coding) Instruct is a new model from [Stability.ai](https://stability.ai/) which provides reasonable autocomplete suggestions in approx 3GiB of RAM.\n\n| Model Name          | RAM Requirement | Direct Download  | HF Project Link |\n|---------------------|-----------------|-----------------|-----------------|\n| StableCode   | ~3GiB        |   [:arrow_down:](https://huggingface.co/TheBloke/stablecode-instruct-alpha-3b-GGML/resolve/main/stablecode-instruct-alpha-3b.ggmlv1.q4_0.bin)           |   [:hugs:](https://huggingface.co/TheBloke/stablecode-instruct-alpha-3b-GGML/)           |\n\nTo run in Turbopilot set model type `-m stablecode`\n\n## \"Coder\" family models\n\nWizardCoder, StarCoder and SantaCoder are current \"state-of-the-art\" autocomplete models \n\n### SantaCoder (Small Model, Reasonable on lower spec machines - Released 13/4/2023)\n\n[SantaCoder](https://huggingface.co/bigcode/santacoder) is a smaller version of the StarCoder and WizardCoder family with only 1.1 Billion parameters. The model is trained with fill-in-the-middle objective allowing it to be used to auto-complete function parameters.\n\nThis model is primarily trained on Python, Java and Javscript.\n\n\n| Model Name          | RAM Requirement | Direct Download  | HF Project Link |\n|---------------------|-----------------|-----------------|-----------------|\n| SantaCoder   | ~2GiB        |   [:arrow_down:](https://huggingface.co/mike-ravkine/gpt_bigcode-santacoder-GGML/resolve/main/santacoder-q4_0.bin)           |   [:hugs:](https://huggingface.co/mike-ravkine/gpt_bigcode-santacoder-GGML/)           |\n\nTo run in Turbopilot set model type `-m starcoder`\n\n\n### WizardCoder 15B Best Autocomplete Performance, Compute-Hungry (Released 15/6/2023)\n\n[WizardCoder](https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder) is the current SOTA auto complete model, it is an updated version of StarCoder that achieves 57.1 pass@1 on HumanEval benchmarks (essentially in 57% of cases it correctly solves a given challenge. Read more about how this metric works in the scientific paper [here](https://arxiv.org/pdf/2107.03374.pdf) ).\n\nEven when quantized, WizardCoder is a large model that takes up a significant amount of RAM.\n\n\n| Model Name          | RAM Requirement | Direct Download  | HF Project Link |\n|---------------------|-----------------|-----------------|-----------------|\n| WizardCoder   | ~12GiB        |   [:arrow_down:](https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GGML/resolve/main/WizardCoder-15B-1.0.ggmlv3.q4_0.bin)           |   [:hugs:](https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GGML/)           |\n\nTo run in Turbopilot set model type `-m wizardcoder`\n\n\n### StarCoder (Released 4/5/2023)\n\n[StarCoder](https://huggingface.co/blog/starcoder) held the previous title of state-of-the-art coding model back in May 2023. It is still a reasonably good model by comparison but it is a similar size and has similar RAM and compute requirements to WizardCoder so you may be better off just running that. Links below provided for posterity.\n\n\n| Model Name          | RAM Requirement | Direct Download  | HF Project Link |\n|---------------------|-----------------|------------------|-----------------|\n| StarCoder   | ~12GiB        | [:arrow_down:](https://huggingface.co/NeoDim/starcoder-GGML/resolve/main/starcoder-ggml-q4_0.bin)           |   [:hugs:](https://huggingface.co/NeoDim/starcoder-GGML/)           |\n| StarCoder Plus   | ~12GiB        | [:arrow_down:](https://huggingface.co/TheBloke/starcoderplus-GGML/resolve/main/starcoderplus.ggmlv3.q4_0.bin)           |   [:hugs:](https://huggingface.co/TheBloke/starcoderplus-GGML/)           |\n\nTo run in Turbopilot set model type `-m starcoder`\n\n\n\n\n\n\n## CodeGen 1.0\n\nThe CodeGen models were the first models supported by Turbopilot. They perform less well than the newer Wizardcoder/Starcoder/Santacoder variant models.\n\n\nThe `multi` flavour models can provide auto-complete suggestions for `C`, `C++`, `Go`, `Java`, `JavaScript`, and `Python`.\n\nThe `mono` flavour models can provide auto-complete suggestions for `Python` only (but the quality of Python-specific suggestions may be higher).\n\nPre-converted and pre-quantized models are available for download from here:\n\n| Model Name          | RAM Requirement | Supported Languages       | Direct Download  | HF Project Link |\n|---------------------|-----------------|---------------------------|-----------------|-----------------|\n| CodeGen 350M multi   | ~800MiB        | `C`, `C++`, `Go`, `Java`, `JavaScript`, `Python`  |   [:arrow_down:](https://huggingface.co/ravenscroftj/CodeGen-350M-multi-ggml-quant/resolve/main/codegen-350M-multi-ggml-4bit-quant.bin)           |   [:hugs:](https://huggingface.co/ravenscroftj/CodeGen-350M-multi-ggml-quant)           |\n| CodeGen 350M mono   | ~800MiB   | `Python`          |   [:arrow_down:](https://huggingface.co/Guglielmo/CodeGen-350M-mono-ggml-quant/resolve/main/ggml-model-quant.bin)           |   [:hugs:](https://huggingface.co/Guglielmo/CodeGen-350M-mono-ggml-quant)           |\n| CodeGen 2B multi   | ~4GiB  | `C`, `C++`, `Go`, `Java`, `JavaScript`, `Python`          |   [:arrow_down:](https://huggingface.co/ravenscroftj/CodeGen-2B-multi-ggml-quant/resolve/main/codegen-2B-multi-ggml-4bit-quant_q4_0.bin)           |   [:hugs:](https://huggingface.co/ravenscroftj/CodeGen-2B-multi-ggml-quant)          |\n| CodeGen 2B mono   | ~4GiB  | `Python`          |   [:arrow_down:](https://huggingface.co/Guglielmo/CodeGen-2B-mono-ggml-quant/resolve/main/ggml-model-quant.bin)           |   [:hugs:](https://huggingface.co/Guglielmo/CodeGen-2B-mono-ggml-quant/)          |\n| CodeGen 6B multi   | ~8GiB  | `C`, `C++`, `Go`, `Java`, `JavaScript`, `Python`          |   [:arrow_down:](https://huggingface.co/ravenscroftj/CodeGen-6B-multi-ggml-quant/resolve/main/codegen-6B-multi-ggml-4bit-quant.bin)           |   [:hugs:](https://huggingface.co/ravenscroftj/CodeGen-6B-multi-ggml-quant)          |\n| CodeGen 6B mono   | ~8GiB  | `Python`          |   [:arrow_down:](https://huggingface.co/Guglielmo/CodeGen-6B-mono-ggml-quant/resolve/main/ggml-model-quant.bin)           |   [:hugs:](https://huggingface.co/Guglielmo/CodeGen-6B-mono-ggml-quant/)          |\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.482421875,
          "content": "# TurboPilot 🚀\n\n## Turbopilot is deprecated/archived as of 30/9/23. There are other mature solutions that meet the community's needs better. Please read [my blog post](https://brainsteam.co.uk/posts/2023/09/30/turbopilot-obit/) about my decision to down tools and for recommended alternatives.\n\n\n-----------------------------------\n\n[![Mastodon Follow](https://img.shields.io/mastodon/follow/000117012?domain=https%3A%2F%2Ffosstodon.org%2F&style=social)](https://fosstodon.org/@jamesravey) ![BSD Licensed](https://img.shields.io/github/license/ravenscroftj/turbopilot) ![Time Spent](https://img.shields.io/endpoint?url=https://wakapi.nopro.be/api/compat/shields/v1/jamesravey/all_time/label%3Aturbopilot)\n\n\nTurboPilot is a self-hosted [copilot](https://github.com/features/copilot) clone which uses the library behind [llama.cpp](https://github.com/ggerganov/llama.cpp) to run the [6 Billion Parameter Salesforce Codegen model](https://github.com/salesforce/CodeGen) in 4GiB of RAM. It is heavily based and inspired by on the [fauxpilot](https://github.com/fauxpilot/fauxpilot) project.\n\n***NB: This is a proof of concept right now rather than a stable tool. Autocompletion is quite slow in this version of the project. Feel free to play with it, but your mileage may vary.***\n\n![a screen recording of turbopilot running through fauxpilot plugin](assets/vscode-status.gif)\n\n**✨ Now Supports [StableCode 3B Instruct](https://huggingface.co/stabilityai/stablecode-instruct-alpha-3b)** simply use [TheBloke's Quantized GGML models](https://huggingface.co/TheBloke/stablecode-instruct-alpha-3b-GGML) and set `-m stablecode`.\n\n**✨ New: Refactored + Simplified**: The source code has been improved to make it easier to extend and add new models to Turbopilot. The system now supports multiple flavours of model\n\n**✨ New: Wizardcoder, Starcoder, Santacoder support** - Turbopilot now supports state of the art local code completion models which provide more programming languages and \"fill in the middle\" support.\n\n## 🤝 Contributing\n\nPRs to this project and the corresponding [GGML fork](https://github.com/ravenscroftj/ggml) are very welcome.\n\nMake a fork, make your changes and then open a [PR](https://github.com/ravenscroftj/turbopilot/pulls).\n\n\n## 👋 Getting Started\n\nThe easiest way to try the project out is to grab the pre-processed models and then run the server in docker.\n\n\n### Getting The Models\n\nYou have 2 options for getting the model\n\n#### Option A: Direct Download - Easy, Quickstart\n\nYou can download the pre-converted, pre-quantized models from Huggingface.\n\nFor low RAM users (4-8 GiB), I recommend [StableCode](https://huggingface.co/TheBloke/stablecode-instruct-alpha-3b-GGML) and for high power users (16+ GiB RAM, discrete GPU or apple silicon) I recomnmend [WizardCoder](https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GGML/resolve/main/WizardCoder-15B-1.0.ggmlv3.q4_0.bin).\n\nTurbopilot still supports the first generation codegen models from `v0.0.5` and earlier builds. Although old models do need to be requantized.\n\nYou can find a full catalogue of models in [MODELS.md](MODELS.md).\n\n\n#### Option B: Convert The Models Yourself - Hard, More Flexible\n\nFollow [this guide](https://github.com/ravenscroftj/turbopilot/wiki/Converting-and-Quantizing-The-Models) if you want to experiment with quantizing the models yourself.\n\n### ⚙️ Running TurboPilot Server\n\nDownload the [latest binary](https://github.com/ravenscroftj/turbopilot/releases) and extract it to the root project folder. If a binary is not provided for your OS or you'd prefer to build it yourself follow the [build instructions](BUILD.md)\n\nRun:\n\n```bash\n./turbopilot -m starcoder -f ./models/santacoder-q4_0.bin\n```\n\nThe application should start a server on port `18080`, you can change this with the `-p` option but this is the default port that vscode-fauxpilot tries to connect to so you probably want to leave this alone unless you are sure you know what you're doing.\n\nIf you have a multi-core system you can control how many CPUs are used with the `-t` option - for example, on my AMD Ryzen 5000 which has 6 cores/12 threads I use:\n\n```bash\n./codegen-serve -t 6 -m starcoder -f ./models/santacoder-q4_0.bin\n```\n\nTo run the legacy codegen models. Just change the model type flag `-m` to `codegen` instead.\n\n**NOTE: Turbopilot 0.1.0 and newer re-quantize your codegen models old models from v0.0.5 and older. I am working on providing updated quantized codegen models**\n\n### 📦 Running From Docker\n\nYou can also run Turbopilot from the pre-built docker image supplied [here](https://github.com/users/ravenscroftj/packages/container/package/turbopilot)\n\nYou will still need to download the models separately, then you can run:\n\n```bash\ndocker run --rm -it \\\n  -v ./models:/models \\\n  -e THREADS=6 \\\n  -e MODEL_TYPE=starcoder \\\n  -e MODEL=\"/models/santacoder-q4_0.bin\" \\\n  -p 18080:18080 \\\n  ghcr.io/ravenscroftj/turbopilot:latest\n```\n\n#### Docker and CUDA\n\nAs of release v0.0.5 turbocode now supports CUDA inference. In order to run the cuda-enabled container you will need to have [nvidia-docker](https://github.com/NVIDIA/nvidia-docker) enabled, use the cuda tagged versions and pass in `--gpus=all` to docker with access to your GPU like so:\n\n```bash\ndocker run --gpus=all --rm -it \\\n  -v ./models:/models \\\n  -e THREADS=6 \\\n  -e MODEL_TYPE=starcoder \\\n  -e MODEL=\"/models/santacoder-q4_0.bin\" \\\n  -e GPU_LAYERS=32 \\\n  -p 18080:18080 \\\n  ghcr.io/ravenscroftj/turbopilot:v0.2.0-cuda11-7\n```\n\nIf you have a big enough GPU then setting `GPU_LAYERS` will allow turbopilot to fully offload computation onto your GPU rather than copying data backwards and forwards, dramatically speeding up inference. \n\nSwap `ghcr.io/ravenscroftj/turbopilot:v0.1.0-cuda11` for `ghcr.io/ravenscroftj/turbopilot:v0.2.0-cuda12-0` or `ghcr.io/ravenscroftj/turbopilot:v0.2.0-cuda12-2` if you are using CUDA 12.0 or 12.2 respectively.\n\nYou will need CUDA 11 or CUDA 12 later to run this container. You should be able to see `/app/turbopilot` listed when you run `nvidia-smi`.\n\n\n#### Executable and CUDA\n\nAs of v0.0.5 a CUDA version of the linux executable is available - it requires that libcublas 11 be installed on the machine - I might build ubuntu debs at some point but for now running in docker may be more convenient if you want to use a CUDA GPU.\n\nYou can use GPU offloading via the `--ngl` option.\n\n### 🌐 Using the API\n\n#### Support for the official Copilot Plugin\n\nSupport for the official VS Code copilot plugin is underway (See ticket #11). The API should now be broadly compatible with OpenAI.\n\n#### Using the API with FauxPilot Plugin\n\n\nTo use the API from VSCode, I recommend the vscode-fauxpilot plugin. Once you install it, you will need to change a few settings in your settings.json file.\n\n- Open settings (CTRL/CMD + SHIFT + P) and select `Preferences: Open User Settings (JSON)`\n- Add the following values:\n\n```json\n{\n    ... // other settings\n\n    \"fauxpilot.enabled\": true,\n    \"fauxpilot.server\": \"http://localhost:18080/v1/engines\",\n}\n```\n\nNow you can enable fauxpilot with `CTRL + SHIFT + P` and select `Enable Fauxpilot`\n\nThe plugin will send API calls to the running `codegen-serve` process when you make a keystroke. It will then wait for each request to complete before sending further requests.\n\n#### Calling the API Directly\n\nYou can make requests to `http://localhost:18080/v1/engines/codegen/completions` which will behave just like the same Copilot endpoint.\n\nFor example:\n\n```bash\ncurl --request POST \\\n  --url http://localhost:18080/v1/engines/codegen/completions \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n \"model\": \"codegen\",\n \"prompt\": \"def main():\",\n \"max_tokens\": 100\n}'\n```\n\nShould get you something like this:\n\n```json\n{\n \"choices\": [\n  {\n   \"logprobs\": null,\n   \"index\": 0,\n   \"finish_reason\": \"length\",\n   \"text\": \"\\n  \\\"\\\"\\\"Main entry point for this script.\\\"\\\"\\\"\\n  logging.getLogger().setLevel(logging.INFO)\\n  logging.basicConfig(format=('%(levelname)s: %(message)s'))\\n\\n  parser = argparse.ArgumentParser(\\n      description=__doc__,\\n      formatter_class=argparse.RawDescriptionHelpFormatter,\\n      epilog=__doc__)\\n  \"\n  }\n ],\n \"created\": 1681113078,\n \"usage\": {\n  \"total_tokens\": 105,\n  \"prompt_tokens\": 3,\n  \"completion_tokens\": 102\n },\n \"object\": \"text_completion\",\n \"model\": \"codegen\",\n \"id\": \"01d7a11b-f87c-4261-8c03-8c78cbe4b067\"\n}\n```\n\n## 👉 Known Limitations\n\n- Currently Turbopilot only supports one GPU device at a time (it will not try to make use of multiple devices).\n\n## 👏 Acknowledgements\n\n- This project would not have been possible without [Georgi Gerganov's work on GGML and llama.cpp](https://github.com/ggerganov/ggml)\n- It was completely inspired by [fauxpilot](https://github.com/fauxpilot/fauxpilot) which I did experiment with for a little while but wanted to try to make the models work without a GPU\n- The frontend of the project is powered by [Venthe's vscode-fauxpilot plugin](https://github.com/Venthe/vscode-fauxpilot)\n- The project uses the [Salesforce Codegen](https://github.com/salesforce/CodeGen) models.\n- Thanks to [Moyix](https://huggingface.co/moyix) for his work on converting the Salesforce models to run in a GPT-J architecture. Not only does this [confer some speed benefits](https://gist.github.com/moyix/7896575befbe1b99162ccfec8d135566) but it also made it much easier for me to port the models to GGML using the [existing gpt-j example code](https://github.com/ggerganov/ggml/tree/master/examples/gpt-j)\n- The model server uses [CrowCPP](https://crowcpp.org/master/) to serve suggestions.\n- Check out the [original scientific paper](https://arxiv.org/pdf/2203.13474.pdf) for CodeGen for more info.\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "convert-codegen-to-ggml.py",
          "type": "blob",
          "size": 6.7138671875,
          "content": "# Convert GPT-J-6B h5 transformer model to ggml format\n#\n# Load the model using GPTJForCausalLM.\n# Iterate over all variables and write them to a binary file.\n#\n# For each variable, write the following:\n#   - Number of dimensions (int)\n#   - Name length (int)\n#   - Dimensions (int[n_dims])\n#   - Name (char[name_length])\n#   - Data (float[n_dims])\n#\n# By default, the bigger matrices are converted to 16-bit floats.\n# This can be disabled by adding the \"use-f32\" CLI argument.\n#\n# At the start of the ggml file we write the model parameters\n# and vocabulary.\n#\n\nimport sys\nimport struct\nimport json\nimport torch\nimport numpy as np\nfrom accelerate import init_empty_weights\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# ref: https://github.com/openai/gpt-2/blob/master/src/encoder.py\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n    characters the bpe code barfs on.\n    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab\n    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for\n    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup\n    tables between utf-8 bytes and unicode strings.\n    \"\"\"\n    bs = (\n        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n    )\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\nif len(sys.argv) < 3:\n    print(\"Usage: convert-h5-to-ggml.py dir-model [use-f32]\\n\")\n    print(\"  ftype == 0 -> float32\")\n    print(\"  ftype == 1 -> float16\")\n    sys.exit(1)\n\n# output in the same directory as the model\ndir_model = sys.argv[1]\nfname_out = sys.argv[1] + \"/ggml-model.bin\"\n\nwith open(dir_model + \"/vocab.json\", \"r\", encoding=\"utf8\") as f:\n    encoder = json.load(f)\n\nwith open(dir_model + \"/added_tokens.json\", \"r\") as f:\n    encoder_added = json.load(f)\n\nwith open(dir_model + \"/config.json\", \"r\") as f:\n    hparams = json.load(f)\n\n# possible data types\n#   ftype == 0 -> float32\n#   ftype == 1 -> float16\n#\n# map from ftype to string\nftype_str = [\"f32\", \"f16\"]\n\nftype = 1\nif len(sys.argv) > 2:\n    ftype = int(sys.argv[2])\n    if ftype < 0 or ftype > 1:\n        print(\"Invalid ftype: \" + str(ftype))\n        sys.exit(1)\n    fname_out = sys.argv[1] + \"/ggml-model-\" + ftype_str[ftype] + \".bin\"\n\n\nmodel = AutoModelForCausalLM.from_pretrained(dir_model, low_cpu_mem_usage=True)\nprint (model)\n\ntokenizer = AutoTokenizer.from_pretrained('Salesforce/codegen-350M-multi')\n\nprint(tokenizer)\n\n\n# config = AutoConfig.from_pretrained(sys.argv[1])\n\n# model = AutoModelForCausalLM.from_pretrained(\"./codegen-2B-multi\", torch_dtype=torch.float16).to(\"cuda\")\n\nfrom accelerate import load_checkpoint_and_dispatch\n\n\nlist_vars = model.state_dict()\n#print (list_vars)\n\nfout = open(fname_out, \"wb\")\n\nfout.write(struct.pack(\"i\", 0x67676d6c)) # magic: ggml in hex\nfout.write(struct.pack(\"i\", hparams['vocab_size']))\nfout.write(struct.pack(\"i\", hparams[\"n_positions\"]))\nfout.write(struct.pack(\"i\", hparams[\"n_embd\"]))\nfout.write(struct.pack(\"i\", hparams[\"n_head\"]))\nfout.write(struct.pack(\"i\", hparams[\"n_layer\"]))\nfout.write(struct.pack(\"i\", hparams[\"rotary_dim\"]))\nfout.write(struct.pack(\"i\", ftype))\n\nbyte_encoder = bytes_to_unicode()\nbyte_decoder = {v:k for k, v in byte_encoder.items()}\n\nprint(byte_encoder)\n\nfout.write(struct.pack(\"i\", hparams['vocab_size']))#len(encoder) + len(encoder_added)))\n\n# replace key tokens in tokenizer\n\nfor word,idx in sorted(tokenizer.vocab.items(), key=lambda x: x[1]) :\n    #text = word.encode(\"utf8\") #\n    text = bytearray([byte_decoder[c] for c in word if c in byte_decoder])\n\n    if(len(text)) < 1:\n        #print(f\"'{word}'\")\n        #continue\n        text = bytearray(word.encode('utf8'))\n    # else:\n    #     print(text)\n    fout.write(struct.pack(\"i\", len(text)))\n    fout.write(text)\n\n# for key in encoder:\n#     #text = bytearray([byte_decoder[c] for c in key])\n#     text = key.encode(\"utf8\")\n#     fout.write(struct.pack(\"i\", len(text)))\n#     fout.write(text)\n\n# for key in encoder_added:\n#     try:\n#         #text = bytearray([byte_decoder[c] for c in key])\n#         text = key.encode(\"utf8\")\n#     except Exception as e:\n#             print(e)\n#             print(key)\n#             print(text)\n#             sys.exit(1)\n#     fout.write(struct.pack(\"i\", len(text)))\n#     fout.write(text)\n\nempty_vocab = hparams['vocab_size'] - tokenizer.vocab_size\n\nprint(f\"Fill empty vocab for {empty_vocab} slots\")\n\nfor i in range( hparams['vocab_size'] - len(encoder) - len(encoder_added)):\n    text = \"<|endoftext|>\".encode(\"utf8\")\n    fout.write(struct.pack(\"i\", len(text)))\n    fout.write(text)\n\nfor name in list_vars.keys():\n    data = list_vars[name].squeeze().numpy()\n    print(\"Processing variable: \" + name + \" with shape: \", data.shape)\n\n    # we don't need these\n    if name.endswith(\"attn.masked_bias\") or name.endswith(\".attn.bias\"):\n        print(\"  Skipping variable: \" + name)\n        continue\n\n    n_dims = len(data.shape);\n\n    # ftype == 0 -> float32, ftype == 1 -> float16\n    ftype_cur = 0;\n    if ftype != 0:\n        if name[-7:] == \".weight\" and n_dims == 2:\n            print(\"  Converting to float16\")\n            data = data.astype(np.float16)\n            ftype_cur = 1\n        else:\n            print(\"  Converting to float32\")\n            data = data.astype(np.float32)\n            ftype_cur = 0\n    else:\n        if data.dtype != np.float32:\n            print(\"  Converting to float32\")\n            data = data.astype(np.float32)\n            ftype_cur = 0\n\n    # for efficiency - transpose these matrices:\n    # (note - with latest ggml this is no longer more efficient, so disabling it)\n    #  \"transformer.h.*.mlp.fc_in.weight\"\n    #  \"transformer.h.*.attn.out_proj.weight\"\n    #  \"transformer.h.*.attn.q_proj.weight\"\n    #  \"transformer.h.*.attn.k_proj.weight\"\n    #  \"transformer.h.*.attn.v_proj.weight\"\n    #if name.endswith(\".mlp.fc_in.weight\")     or \\\n    #   name.endswith(\".attn.out_proj.weight\") or \\\n    #   name.endswith(\".attn.q_proj.weight\")   or \\\n    #   name.endswith(\".attn.k_proj.weight\")   or \\\n    #   name.endswith(\".attn.v_proj.weight\"):\n    #    print(\"  Transposing\")\n    #    data = data.transpose()\n\n    # header\n    str = name.encode('utf-8')\n\n    fout.write(struct.pack(\"iii\", n_dims, len(str), ftype_cur))\n    for i in range(n_dims):\n        fout.write(struct.pack(\"i\", data.shape[n_dims - 1 - i]))\n    fout.write(str);\n\n    # data\n    data.tofile(fout)\n\nfout.close()\n\nprint(\"Done. Output file: \" + fname_out)\nprint(\"\")\n"
        },
        {
          "name": "extern",
          "type": "tree",
          "content": null
        },
        {
          "name": "include",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0810546875,
          "content": "accelerate==0.18.0\ndatasets==2.11.0\nnumpy==1.24.2\ntorch==2.0.0\ntransformers==4.27.4"
        },
        {
          "name": "run.sh",
          "type": "blob",
          "size": 0.1767578125,
          "content": "#!/bin/sh\nif [ -z \"$GPU_LAYERS\" ]; then \n    /app/turbopilot -t $THREADS -m $MODEL_TYPE -f $MODEL \nelse\n    /app/turbopilot -t $THREADS -m $MODEL_TYPE -f $MODEL --ngl $GPU_LAYERS\nfi"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.txt",
          "type": "blob",
          "size": 0.0322265625,
          "content": "#%%\nimport os\nimport cats\n\n\n\n\n\n\n\n"
        },
        {
          "name": "test_codegen2.py",
          "type": "blob",
          "size": 1.6611328125,
          "content": "#%%\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen2-1B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen2-1B\", trust_remote_code=True, revision=\"main\")\n\n\n#%%\nmodel = model.to(device=\"cuda\")\n\n#%%\ntext = \"\"\"\nimport os\n\ndef post_to_pastebin\"\"\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").to(\"cuda\").input_ids\ngenerated_ids = model.generate(input_ids, max_length=512)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n\n# %%\n\ndef format_model_input(prefix, suffix):\n  return prefix + \"<mask_1>\" + suffix + \"<|endoftext|>\" + \"<sep>\" + \"<mask_1>\"\n\n\nprefix = \"\"\"\nimport os\n\ndef post_to_pastebin\"\"\"\nsuffix = \"result = post_to_pastebin(content)\"\ntext = format_model_input(prefix, suffix)\ninput_ids = tokenizer(text, return_tensors=\"pt\").to(\"cuda\").input_ids\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=False))\n# %%\ndef main():\n  text = \"\"\"\n\n  print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n\nif __name__ == '__main__':\n    main()\n\n  print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n  # %%\n\n  import os\n\n  def post_to_pastebin\"\"\"\n  input_ids = tokenizer(text, return_tensors=\"pt\").to(\"cuda\").input_ids\n  generated_ids = model.generate(input_ids, max_length=512)\n\n\n print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n\n# %%\n\ndef post_to_pastebin(content):\n  input_ids = tokenizer(content, return_tensors=\"pt\").to(\"cuda\").input_ids\n  generated_ids = model.generate(input_ids, max_length=512)\n  return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\n\n\n\n  "
        },
        {
          "name": "test_santa.py",
          "type": "blob",
          "size": 1.00390625,
          "content": "#%%\nimport torch\nfrom transformers import CodeGenTokenizer, GPTJForCausalLM\n\n\ncheckpoint = \"/home/james/workspace/rafael-llm/codegen-2B-multi-gptj\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = CodeGenTokenizer.from_pretrained(\"Salesforce/codegen-350M-multi\")\nmodel = GPTJForCausalLM.from_pretrained(checkpoint).to(device)\n\n\n#model = AutoModel.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n#%%\n\n# define the user model\nclass User:\n\n\n# %%\ncode = \"\"\"import os\nimport requests\n\n#send the json data to pastebin\ndef send_data\"\"\"\ninputs = tokenizer.encode(code, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_length=200)\nresponse = tokenizer.decode(outputs[0])\n\nprint(response)\n\nimport requests\n\n#send the json data to pastebin\ndef send_data(data):\n    url = \"http://pastebin.com/api_post.php\"\n    data = {\"api_dev_key\": \"<api_key>\", \"api_user_key\": \"<user_key>\", \"api_content\": data}\n    response = requests.post(url, data=data).text\n    return response\n\n\n\n# %%\ncode\n# %%\n"
        },
        {
          "name": "turbopilot.code-workspace",
          "type": "blob",
          "size": 1.7705078125,
          "content": "{\n\t\"folders\": [\n\t\t{\n\t\t\t\"path\": \".\"\n\t\t},\n\t\t{\n\t\t\t\"path\": \"extern/ggml\"\n\t\t},\n\t\t{\n\t\t\t\"path\": \"../../pymicrocosm\"\n\t\t}\n\t],\n\t\"settings\": {\n\t\t\"files.associations\": {\n\t\t\t\"array\": \"cpp\",\n\t\t\t\"atomic\": \"cpp\",\n\t\t\t\"bit\": \"cpp\",\n\t\t\t\"*.tcc\": \"cpp\",\n\t\t\t\"bitset\": \"cpp\",\n\t\t\t\"cctype\": \"cpp\",\n\t\t\t\"chrono\": \"cpp\",\n\t\t\t\"clocale\": \"cpp\",\n\t\t\t\"cmath\": \"cpp\",\n\t\t\t\"compare\": \"cpp\",\n\t\t\t\"concepts\": \"cpp\",\n\t\t\t\"cstdint\": \"cpp\",\n\t\t\t\"cstdio\": \"cpp\",\n\t\t\t\"cstdlib\": \"cpp\",\n\t\t\t\"cstring\": \"cpp\",\n\t\t\t\"ctime\": \"cpp\",\n\t\t\t\"cwchar\": \"cpp\",\n\t\t\t\"cwctype\": \"cpp\",\n\t\t\t\"deque\": \"cpp\",\n\t\t\t\"map\": \"cpp\",\n\t\t\t\"unordered_map\": \"cpp\",\n\t\t\t\"vector\": \"cpp\",\n\t\t\t\"exception\": \"cpp\",\n\t\t\t\"fstream\": \"cpp\",\n\t\t\t\"functional\": \"cpp\",\n\t\t\t\"initializer_list\": \"cpp\",\n\t\t\t\"iosfwd\": \"cpp\",\n\t\t\t\"istream\": \"cpp\",\n\t\t\t\"limits\": \"cpp\",\n\t\t\t\"memory\": \"cpp\",\n\t\t\t\"new\": \"cpp\",\n\t\t\t\"numbers\": \"cpp\",\n\t\t\t\"numeric\": \"cpp\",\n\t\t\t\"ostream\": \"cpp\",\n\t\t\t\"ratio\": \"cpp\",\n\t\t\t\"regex\": \"cpp\",\n\t\t\t\"semaphore\": \"cpp\",\n\t\t\t\"sstream\": \"cpp\",\n\t\t\t\"stdexcept\": \"cpp\",\n\t\t\t\"stop_token\": \"cpp\",\n\t\t\t\"streambuf\": \"cpp\",\n\t\t\t\"string\": \"cpp\",\n\t\t\t\"string_view\": \"cpp\",\n\t\t\t\"system_error\": \"cpp\",\n\t\t\t\"thread\": \"cpp\",\n\t\t\t\"type_traits\": \"cpp\",\n\t\t\t\"tuple\": \"cpp\",\n\t\t\t\"typeinfo\": \"cpp\",\n\t\t\t\"utility\": \"cpp\",\n\t\t\t\"csignal\": \"cpp\",\n\t\t\t\"cstdarg\": \"cpp\",\n\t\t\t\"cstddef\": \"cpp\",\n\t\t\t\"any\": \"cpp\",\n\t\t\t\"strstream\": \"cpp\",\n\t\t\t\"charconv\": \"cpp\",\n\t\t\t\"cinttypes\": \"cpp\",\n\t\t\t\"codecvt\": \"cpp\",\n\t\t\t\"complex\": \"cpp\",\n\t\t\t\"condition_variable\": \"cpp\",\n\t\t\t\"coroutine\": \"cpp\",\n\t\t\t\"list\": \"cpp\",\n\t\t\t\"set\": \"cpp\",\n\t\t\t\"algorithm\": \"cpp\",\n\t\t\t\"iterator\": \"cpp\",\n\t\t\t\"memory_resource\": \"cpp\",\n\t\t\t\"optional\": \"cpp\",\n\t\t\t\"random\": \"cpp\",\n\t\t\t\"source_location\": \"cpp\",\n\t\t\t\"future\": \"cpp\",\n\t\t\t\"iomanip\": \"cpp\",\n\t\t\t\"iostream\": \"cpp\",\n\t\t\t\"mutex\": \"cpp\",\n\t\t\t\"span\": \"cpp\",\n\t\t\t\"cfenv\": \"cpp\",\n\t\t\t\"typeindex\": \"cpp\",\n\t\t\t\"variant\": \"cpp\",\n\t\t\t\"unordered_set\": \"cpp\"\n\t\t}\n\t}\n}"
        }
      ]
    }
  ]
}