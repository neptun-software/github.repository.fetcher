{
  "metadata": {
    "timestamp": 1736566377205,
    "page": 140,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NVIDIA/TensorRT",
      "stars": 11035,
      "defaultBranch": "release/10.7",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 2.359375,
          "content": "---\nAccessModifierOffset: -4\nAlignAfterOpenBracket: DontAlign\nAlignConsecutiveAssignments: false\nAlignConsecutiveDeclarations: false\nAlignEscapedNewlinesLeft: false\nAlignOperands:   false\nAlignTrailingComments: true\nAllowAllParametersOfDeclarationOnNextLine: true\nAllowShortBlocksOnASingleLine: false\nAllowShortCaseLabelsOnASingleLine: true\nAllowShortFunctionsOnASingleLine: Empty\nAllowShortIfStatementsOnASingleLine: false\nAllowShortLoopsOnASingleLine: false\nAlwaysBreakAfterDefinitionReturnType: None\nAlwaysBreakAfterReturnType: None\nAlwaysBreakBeforeMultilineStrings: true\nAlwaysBreakTemplateDeclarations: true\nBasedOnStyle: None\nBinPackArguments: true\nBinPackParameters: true\nBreakBeforeBinaryOperators: All\nBreakBeforeBraces: Allman\nBreakBeforeTernaryOperators: true\nBreakConstructorInitializersBeforeComma: true\nColumnLimit:     120\nCommentPragmas:  '^ IWYU pragma:'\nConstructorInitializerAllOnOneLineOrOnePerLine: false\nConstructorInitializerIndentWidth: 4\nContinuationIndentWidth: 4\nCpp11BracedListStyle: true\nDerivePointerAlignment: false\nDisableFormat:   false\nExperimentalAutoDetectBinPacking: false\nForEachMacros:   [ foreach, Q_FOREACH, BOOST_FOREACH ]\nIncludeBlocks: Preserve\nIncludeCategories:\n  - Regex:           '^\"(llvm|llvm-c|clang|clang-c)/'\n    Priority:        2\n  - Regex:           '^(<|\"(gtest|isl|json)/)'\n    Priority:        3\n  - Regex:           '.*'\n    Priority:        1\nIndentCaseLabels: false\nIndentWidth:     4\nIndentWrappedFunctionNames: false\nKeepEmptyLinesAtTheStartOfBlocks: true\nLanguage: Cpp\nMacroBlockBegin: ''\nMacroBlockEnd:   ''\nMaxEmptyLinesToKeep: 1\nNamespaceIndentation: None\nObjCBlockIndentWidth: 4\nObjCSpaceAfterProperty: true\nObjCSpaceBeforeProtocolList: true\nPenaltyBreakBeforeFirstCallParameter: 19\nPenaltyBreakComment: 300\nPenaltyBreakFirstLessLess: 120\nPenaltyBreakString: 1000\nPenaltyExcessCharacter: 1000000\nPenaltyReturnTypeOnItsOwnLine: 60\nPointerAlignment: Left\nPointerBindsToType: false\nReflowComments:  true\nSortIncludes:    true\nSpaceAfterCStyleCast: true\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeParens: ControlStatements\nSpaceInEmptyParentheses: false\nSpacesBeforeTrailingComments: 1\nSpacesInAngles:  false\nSpacesInCStyleCastParentheses: false\nSpacesInContainerLiterals: true\nSpacesInParentheses: false\nSpacesInSquareBrackets: false\nStandard:        Cpp11\nStatementMacros: [API_ENTRY_TRY,TRT_TRY]\nTabWidth:        4\nUseTab:          Never\n...\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0263671875,
          "content": "/.git*\nbuild*\n/third_party\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1357421875,
          "content": "build/\n/demo/BERT/models\n/demo/BERT/engines\n/demo/BERT/squad/*.json\n/docker/jetpack_files/*\n*.nvmk\n*.sln\n*.vcxproj\nexternals/\n**/.DS_Store\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.361328125,
          "content": "[submodule \"third_party/protobuf\"]\n\tpath = third_party/protobuf\n\turl = https://github.com/protocolbuffers/protobuf.git\n\tbranch = 3.20.x\n[submodule \"third_party/cub\"]\n\tpath = third_party/cub\n\turl = https://github.com/NVlabs/cub.git\n\tbranch = 1.8.0\n[submodule \"parsers/onnx\"]\n\tpath = parsers/onnx\n\turl = https://github.com/onnx/onnx-tensorrt.git\n\tbranch = release/10.7-GA\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 50.61328125,
          "content": "# TensorRT OSS Release Changelog\n\n## 10.7.0 GA - 2024-12-4\nKey Feature and Updates:\n\n- Demo Changes\n  - demoDiffusion\n    - Enabled low-vram for the Flux pipeline. Users can now run the pipelines on systems with 32GB VRAM.\n    - Added support for [FLUX.1-schnell](https://huggingface.co/black-forest-labs/FLUX.1-schnell) pipeline.\n    - Enabled weight streaming mode for Flux pipeline.\n\n- Plugin Changes\n  - On Blackwell and later platforms, TensorRT will drop cuDNN support on the following categories of plugins\n    - User-written `IPluginV2Ext`, `IPluginV2DynamicExt`, and `IPluginV2IOExt` plugins that are dependent on cuDNN handles provided by TensorRT (via the `attachToContext()` API).\n    - TensorRT standard plugins that use cuDNN, specifically:\n      - `InstanceNormalization_TRT` (version: 1, 2, and 3) present in `plugin/instanceNormalizationPlugin/`.\n      - `GroupNormalizationPlugin` (version: 1) present in `plugin/groupNormalizationPlugin/`.\n      - Note: These normalization plugins are superseded by TensorRT’s native `INormalizationLayer` ([C++](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_normalization_layer.html), [Python](https://docs.nvidia.com/deeplearning/tensorrt/operators/docs/Normalization.html)). TensorRT support for cuDNN-dependent plugins remain unchanged on pre-Blackwell platforms.\n\n- Parser Changes\n  - Now prioritizes using plugins over local functions when a corresponding plugin is available in the registry.\n  - Added dynamic axes support for `Squeeze` and `Unsqueeze` operations.\n  - Added support for parsing mixed-precision `BatchNormalization` nodes in strongly-typed mode.\n\n- Addressed Issues\n  - Fixed [4113](https://github.com/NVIDIA/TensorRT/issues/4113).\n\n## 10.6.0 GA - 2024-11-05\nKey Feature and Updates:\n- Demo Changes\n  - demoBERT: The use of `fcPlugin` in demoBERT has been removed.\n  - demoBERT: All TensorRT plugins now used in demoBERT (`CustomEmbLayerNormDynamic`, `CustomSkipLayerNormDynamic`, and `CustomQKVToContextDynamic`) now have versions that inherit from IPluginV3 interface classes. The user can opt-in to use these V3 plugins by specifying `--use-v3-plugins` to the builder scripts.\n    - Opting-in to use V3 plugins does not affect performance, I/O, or plugin attributes. \n    - There is a known issue in the V3 (version 4) of `CustomQKVToContextDynamic` plugin from TensorRT 10.6.0, causing an internal assertion error if either the batch or sequence dimensions differ at runtime from the ones used to serialize the engine. See the “known issues” section of the [TensorRT-10.6.0 release notes](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-10-6-0).\n    - For smoother migration, the default behavior is still using the  deprecated `IPluginV2DynamicExt`-derived plugins, when the flag: `--use-v3-plugins` isn't specified in the builder scripts. The flag `--use-deprecated-plugins` was added as an explicit way to enforce the default behavior, and is mutually exclusive with `--use-v3-plugins`.\n  - demoDiffusion\n    - Introduced BF16 and FP8 support for the [Flux.1-dev](demo/Diffusion#generate-an-image-guided-by-a-text-prompt-using-flux) pipeline.\n    - Expanded FP8 support on Ada platforms.\n    - Enabled LoRA adapter compatibility for SDv1.5, SDv2.1, and SDXL pipelines using Diffusers version 0.30.3.\n\n- Sample Changes\n  - Added the Python sample [quickly_deployable_plugins](samples/python/quickly_deployable_plugins), which demonstrates quickly deployable Python-based plugin definitions (QDPs) in TensorRT. QDPs are a simple and intuitive decorator-based approach to defining TensorRT plugins, requiring drastically less code.\n\n- Plugin Changes\n  - The `fcPlugin` has been deprecated. Its functionality has been superseded by the [IMatrixMultiplyLayer](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_matrix_multiply_layer.html) that is natively provided by TensorRT.\n  - Migrated `IPluginV2`-descendent version 1 of `CustomEmbLayerNormDynamic`, to version 6, which implements `IPluginV3`.\n    - The newer versions preserve the attributes and I/O of the corresponding older plugin version.\n    - The older plugin versions are deprecated and will be removed in a future release.\n\n- Parser Changes\n  - Updated ONNX submodule version to 1.17.0.\n  - Fixed issue where conditional layers were incorrectly being added.\n  - Updated local function metadata to contain more information.\n  - Added support for parsing nodes with Quickly Deployable Plugins.\n  - Fixed handling of optional outputs.\n\n- Tool Updates\n  - ONNX-Graphsurgeon updated to version 0.5.3\n  - Polygraphy updated to 0.49.14.\n\n## 10.5.0 GA - 2024-09-30\nKey Features and Updates:\n\n- Demo changes\n    - Added [Flux.1-dev](demo/Diffusion) pipeline\n- Sample changes\n    - None\n- Plugin changes\n    - Migrated `IPluginV2`-descendent versions of `bertQKVToContextPlugin` (1, 2, 3) to newer versions (4, 5, 6 respectively) which implement `IPluginV3`.\n    - Note:\n        - The newer versions preserve the attributes and I/O of the corresponding older plugin version\n        - The older plugin versions are deprecated and will be removed in a future release\n- Quickstart guide\n    - None\n- Parser changes\n    - Added support for real-valued `STFT` operations\n    - Improved error handling in `IParser`\n\nKnown issues:\n\n- Demos:\n    - TensorRT engine might not be build successfully when using `--fp8` flag on H100 GPUs.\n\n## 10.4.0 GA - 2024-09-18\nKey Features and Updates:\n\n- Demo changes\n    - Added [Stable Cascade](demo/Diffusion) pipeline.\n    - Enabled INT8 and FP8 quantization for Stable Diffusion v1.5, v2.0 and v2.1 pipelines.\n    - Enabled FP8 quantization for Stable Diffusion XL pipeline.\n- Sample changes\n    - Add a new python sample `aliased_io_plugin` which demonstrates how in-place updates to plugin inputs can be achieved through I/O aliasing.\n- Plugin changes\n    - Migrated IPluginV2-descendent versions (a) of the following plugins to newer versions (b) which implement IPluginV3 (a->b):\n        - scatterElementsPlugin (1->2)\n        - skipLayerNormPlugin (1->5, 2->6, 3->7, 4->8)\n        - embLayerNormPlugin (2->4, 3->5)\n        - bertQKVToContextPlugin (1->4, 2->5, 3->6)\n    - Note\n        - The newer versions preserve the corresponding attributes and I/O of the corresponding older plugin version.\n        - The older plugin versions are deprecated and will be removed in a future release.\n\n- Quickstart guide\n    - Updated deploy_to_triton guide and removed legacy APIs.\n    - Removed legacy TF-TRT code as the project is no longer supported.\n    - Removed quantization_tutorial as pytorch_quantization has been deprecated. Check out https://github.com/NVIDIA/TensorRT-Model-Optimizer for the latest quantization support. Check [Stable Diffusion XL (Base/Turbo) and Stable Diffusion 1.5 Quantization with Model Optimizer](https://github.com/NVIDIA/TensorRT-Model-Optimizer/tree/main/diffusers/quantization) for integration with TensorRT.\n- Parser changes\n    - Added support for tensor `axes` for `Pad` operations.\n    - Added support for `BlackmanWindow`, `HammingWindow`, and `HannWindow` operations.\n    - Improved error handling in `IParserRefitter`.\n    - Fixed kernel shape inference in multi-input convolutions.\n\n- Updated tooling\n    - polygraphy-extension-trtexec v0.0.9\n\n## 10.3.0 GA - 2024-08-02\n\nKey Features and Updates:\n\n - Demo changes\n   - Added [Stable Video Diffusion](demo/Diffusion)(`SVD`) pipeline.\n - Plugin changes\n   - Deprecated Version 1 of [ScatterElements plugin](plugin/scatterElementsPlugin). It is superseded by Version 2, which implements the `IPluginV3` interface.\n - Quickstart guide\n   - Updated the [SemanticSegmentation](quickstart/SemanticSegmentation) guide with latest APIs.\n - Parser changes\n   - Added support for tensor `axes` inputs for `Slice` node.\n   - Updated `ScatterElements` importer to use Version 2 of [ScatterElements plugin](plugin/scatterElementsPlugin), which implements the `IPluginV3` interface.\n - Updated tooling\n   - Polygraphy v0.49.13\n\n## 10.2.0 GA - 2024-07-09\n\nKey Features and Updates:\n\n - Demo changes\n   - Added [Stable Diffusion 3 demo](demo/Diffusion).\n - Plugin changes\n   - Version 3 of the [InstanceNormalization plugin](plugin/instanceNormalizationPlugin/) (`InstanceNormalization_TRT`) has been added. This version is based on the `IPluginV3` interface and is used by the TensorRT ONNX parser when native `InstanceNormalization` is disabled.\n - Tooling changes\n   - Pytorch Quantization development has transitioned to [TensorRT Model Optimizer](https://github.com/NVIDIA/TensorRT-Model-Optimizer). All developers are encouraged to use TensorRT Model Optimizer to benefit from the latest advancements on quantization and compression.\n - Build containers\n   - Updated default cuda versions to `12.5.0`.\n\n## 10.1.0 GA - 2024-06-17\n\nKey Features and Updates:\n\n - Parser changes\n   - Added `supportsModelV2` API\n   - Added support for `DeformConv` operation\n   - Added support for `PluginV3` TensorRT Plugins\n   - Marked all IParser and IParserRefitter APIs as `noexcept`\n - Plugin changes\n   - Added version 2 of ROIAlign_TRT plugin, which implements the IPluginV3 plugin interface. When importing an ONNX model with the RoiAlign op, this new version of the plugin will be inserted to the TRT network.\n - Samples changes\n   - Added a new sample [non_zero_plugin](samples/python/non_zero_plugin), which is a Python version of the C++ sample [sampleNonZeroPlugin](samples/sampleNonZeroPlugin).\n - Updated tooling\n   - Polygraphy v0.49.12\n   - ONNX-GraphSurgeon v0.5.3\n\n## 10.0.1 GA - 2024-04-24\n\nKey Features and Updates:\n\n - Parser changes\n   - Added support for building with `protobuf-lite`.\n   - Fixed issue when parsing and refitting models with nested `BatchNormalization` nodes.\n   - Added support for empty inputs in custom plugin nodes.\n - Demo changes\n   - The following demos have been removed: Jasper, Tacotron2, HuggingFace Diffusers notebook\n - Updated tooling\n   - Polygraphy v0.49.10\n   - ONNX-GraphSurgeon v0.5.2\n - Build Containers\n   - Updated default cuda versions to `12.4.0`.\n   - Added Rocky Linux 8 and Rocky Linux 9 build containers\n\n## 10.0.0 EA - 2024-03-27\n\nKey Features and Updates:\n\n - Samples changes\n   - Added a [sample](samples/python/sample_weight_stripping) showcasing weight-stripped engines.\n   - Added a [sample](samples/python/python_plugin/circ_pad_plugin_multi_tactic.py) demonstrating the use of custom tactics with IPluginV3.\n   - Added a [sample](samples/sampleNonZeroPlugin) to showcase plugins with data-dependent output shapes, using IPluginV3.\n - Parser changes\n   - Added a new class `IParserRefitter` that can be used to refit a TensorRT engine with the weights of an ONNX model.\n   - `kNATIVE_INSTANCENORM` is now set to ON by default.\n   - Added support for `IPluginV3` interfaces from TensorRT.\n   - Added support for `INT4` quantization.\n   - Added support for the `reduction` attribute in `ScatterElements`.\n   - Added support for `wrap` padding mode in `Pad`\n - Plugin changes\n   - A [new plugin](plugin/scatterElementsPlugin) has been added in compliance with [ONNX ScatterElements](https://github.com/onnx/onnx/blob/main/docs/Operators.md#ScatterElements).\n   - The TensorRT plugin library no longer has a load-time link dependency on cuBLAS or cuDNN libraries.\n   - All plugins which relied on cuBLAS/cuDNN handles passed through `IPluginV2Ext::attachToContext()` have moved to use cuBLAS/cuDNN resources initialized by the plugin library itself. This works by dynamically loading the required cuBLAS/cuDNN library. Additionally, plugins which independently initialized their cuBLAS/cuDNN resources have also moved to dynamically loading the required library. If the respective library is not discoverable through the library path(s), these plugins will not work.\n   - bertQKVToContextPlugin: Version 2 of this plugin now supports head sizes less than or equal to 32.\n   - reorgPlugin: Added a version 2 which implements IPluginV2DynamicExt.\n   - disentangledAttentionPlugin: Fixed a kernel bug.\n - Demo changes\n   - HuggingFace demos have been removed. For all users using TensorRT to accelerate Large Language Model inference, please use [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/).\n - Updated tooling\n   - Polygraphy v0.49.9\n   - ONNX-GraphSurgeon v0.5.1\n   - TensorRT Engine Explorer v0.1.8\n - Build Containers\n   - RedHat/CentOS 7.x are no longer officially supported starting with TensorRT 10.0. The corresponding container has been removed from TensorRT-OSS.\n\n## 9.3.0 GA - 2024-02-09\n\nKey Features and Updates:\n\n - Demo changes\n   - Faster Text-to-image using SDXL & INT8 quantization using AMMO\n - Updated tooling\n   - Polygraphy v0.49.7\n\n## 9.2.0 GA - 2023-11-27\n\nKey Features and Updates:\n\n - `trtexec` enhancement: Added `--weightless` flag to mark the engine as weightless.\n - Parser changes\n   - Added support for Hardmax operator.\n   - Changes to a few operator importers to ensure that TensorRT preserves the precision of operations when using strongly typed mode.\n - Plugin changes\n   - Explicit INT8 support added to `bertQKVToContextPlugin`.\n   - Various bug fixes.\n - Updated HuggingFace demo to use transformers v4.31.0 and PyTorch v2.1.0.\n\n\n## 9.1.0 GA - 2023-10-18\n\nKey Features and Updates:\n\n - Update the [trt_python_plugin](samples/python/python_plugin) sample.\n   - Python plugins API reference is part of the offical TRT Python API.\n - Added samples demonstrating the usage of the progress monitor API.\n   - Check [sampleProgressMonitor](samples/sampleProgressMonitor) for the C++ sample.\n   - Check [simple_progress_monitor](samples/python/simple_progress_monitor) for the Python sample.\n - Remove dependencies related to python<3.8 in python samples as we no longer support python<3.8 for python samples. \n - Demo changes\n   - Added LAMBADA dataset accuracy checks in the [HuggingFace](demo/HuggingFace) demo.\n   - Enabled structured sparsity and FP8 quantized batch matrix multiplication(BMM)s in attention in the [NeMo](demo/NeMo) demo.\n   - Replaced deprecated APIs in the [BERT](demo/BERT) demo.\n - Updated tooling\n   - Polygraphy v0.49.1\n\n\n## 9.0.1 GA - 2023-09-07\n\nKey Features and Updates:\n\n - TensorRT plugin autorhing in Python is now supported\n   - See the [trt_python_plugin](samples/python/python_plugin) sample for reference.\n - Updated default CUDA version to 12.2\n - Support for BLIP models, Seq2Seq and Vision2Seq abstractions in HuggingFace demo.\n - demoDiffusion refactoring and SDXL enhancements\n - Additional validation asserts for NV Plugins\n - Updated tooling\n   - TensorRT Engine Explorer v0.1.7: graph rendering for TensorRT 9.0 `kgen` kernels\n   - ONNX-GraphSurgeon v0.3.29\n   - PyTorch quantization toolkit v2.2.0\n\n\n## 9.0.0 EA - 2023-08-06\n\nKey Features and Updates:\n\n - Added the NeMo demo to demonstrate the performance benefit of using E4M3 FP8 data type with the GPT models trained with the [NVIDIA NeMo Toolkit](https://github.com/NVIDIA/NeMo) and [TransformerEngine](https://github.com/NVIDIA/TransformerEngine).\n - Demo Diffusion updates\n   - Added SDXL 1.0 txt2img pipeline\n   - Added ControlNet pipeline\n   - Huggingface demo updates\n     - Added Flan-T5, OPT, BLOOM, BLOOMZ, GPT-Neo, GPT-NeoX, Cerebras-GPT support with accuracy check\n     - Refactored code and extracted common utils into Seq2Seq class \n     - Optimized shape-changing overhead and achieved a >30% e2e performance gain\n     - Added stable KV-cache, beam search and fp16 support for all models\n     - Added dynamic batch size TRT inference\n     - Added uneven-length multi-batch inference with attention_mask support\n     - Added `chat` command – interactive CLI\n     - Upgraded PyTorch and HuggingFace version to support Hopper GPU\n     - Updated notebooks with much simplified demo API.\n\n  - Added two new TensorRT samples: sampleProgressMonitor (C++) and simple_progress_reporter (Python) that are examples for using Progress Monitor during engine build.\n  - The following plugins were deprecated:\n     - ``BatchedNMS_TRT``\n     - ``BatchedNMSDynamic_TRT``\n     - ``BatchTilePlugin_TRT``\n     - ``Clip_TRT``\n     - ``CoordConvAC``\n     - ``CropAndResize``\n     - ``EfficientNMS_ONNX_TRT``\n     - ``CustomGeluPluginDynamic``\n     - ``LReLU_TRT``\n     - ``NMSDynamic_TRT``\n     - ``NMS_TRT``\n     - ``Normalize_TRT``\n     - ``Proposal``\n     - ``SingleStepLSTMPlugin``\n     - ``SpecialSlice_TRT``\n     - ``Split``\n\n  - Ubuntu 18.04 has reached end of life and is no longer supported by TensorRT starting with 9.0, and the corresponding Dockerfile(s) have been removed.\n  - Support for aarch64 builds will not be available in this release, and the corresponding Dockerfiles have been removed.\n\n## [8.6.1 GA](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-6-1) - 2023-05-02\n\nTensorRT OSS release corresponding to TensorRT 8.6.1.6 GA release.\n- Updates since [TensorRT 8.6.0 EA release](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-6-0-EA).\n- Please refer to the [TensorRT 8.6.1.6 GA release notes](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-6-1) for more information.\n\nKey Features and Updates:\n\n- Added a new flag `--use-cuda-graph` to demoDiffusion to improve performance.\n- Optimized GPT2 and T5 HuggingFace demos to use fp16 I/O tensors for fp16 networks.\n\n## [8.6.0 EA](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-6-0-EA) - 2023-03-10\n\nTensorRT OSS release corresponding to TensorRT 8.6.0.12 EA release.\n- Updates since [TensorRT 8.5.3 GA release](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-5-3).\n- Please refer to the [TensorRT 8.6.0.12 EA release notes](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-6-0-EA) for more information.\n\nKey Features and Updates:\n\n- demoDiffusion acceleration is now supported out of the box in TensorRT without requiring plugins.\n  - The following plugins have been removed accordingly: GroupNorm, LayerNorm, MultiHeadCrossAttention, MultiHeadFlashAttention, SeqLen2Spatial, and SplitGeLU.\n- Added a new sample called onnx_custom_plugin.\n\n## [8.5.3 GA](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-5-3) - 2023-01-30\n\nTensorRT OSS release corresponding to TensorRT 8.5.3.1 GA release.\n- Updates since [TensorRT 8.5.2 GA release](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-5-2).\n- Please refer to the [TensorRT 8.5.3 GA release notes](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-5-3) for more information.\n\nKey Features and Updates:\n\n- Added the following HuggingFace demos: GPT-J-6B, GPT2-XL, and GPT2-Medium\n- Added nvinfer1::plugin namespace\n- Optimized KV Cache performance for T5\n\n## [8.5.2 GA](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-5-2) - 2022-12-12\n\nTensorRT OSS release corresponding to TensorRT 8.5.2.2 GA release.\n- Updates since [TensorRT 8.5.1 GA release](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-5-1).\n- Please refer to the [TensorRT 8.5.2 GA release notes](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-5-2) for more information.\n\nKey Features and Updates:\n\n- Plugin enhancements\n  - Added [LayerNormPlugin](plugin/layerNormPlugin), [SplitGeLUPlugin](plugin/splitGeLUPlugin), [GroupNormPlugin](plugin/groupNormPlugin), and [SeqLen2SpatialPlugin](plugin/seqLen2SpatialPlugin) to support [stable diffusion demo](demo/Diffusion).\n- KV-cache and beam search to GPT2 and T5 demos\n\n## [22.12](https://github.com/NVIDIA/TensorRT/releases/tag/22.12) - 2022-12-06\n\n### Added\n- Stable Diffusion demo using TensorRT Plugins\n- KV-cache and beam search to GPT2 and T5 demos\n- Perplexity calculation to all HF demos\n\n### Changed\n- Updated trex to v0.1.5\n- Increased default workspace size in demoBERT to build BS=128 fp32 engines\n- Use `avg_iter=8` and timing cache to make demoBERT perf more stable\n\n### Removed\n- None\n\n## [8.5.1 GA](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-5-1) - 2022-11-01\n\nTensorRT OSS release corresponding to TensorRT 8.5.1.7 GA release.\n- Updates since [TensorRT 8.4.1 GA release](https://github.com/NVIDIA/TensorRT/releases/tag/8.4.1).\n- Please refer to the [TensorRT 8.5.1 GA release notes](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-5-1) for more information.\n\nKey Features and Updates:\n\n- Samples enhancements\n  - Added [sampleNamedDimensions](samples/sampleNamedDimensions) which works with named dimensions.\n  - Updated `sampleINT8API` and `introductory_parser_samples` to use `ONNX` models over `Caffe`/`UFF`\n  - Removed UFF/Caffe samples including `sampleMNIST`, `end_to_end_tensorflow_mnist`, `sampleINT8`, `sampleMNISTAPI`, `sampleUffMNIST`, `sampleUffPluginV2Ext`, `engine_refit_mnist`, `int8_caffe_mnist`, `uff_custom_plugin`, `sampleFasterRCNN`, `sampleUffFasterRCNN`, `sampleGoogleNet`, `sampleSSD`, `sampleUffSSD`, `sampleUffMaskRCNN` and `uff_ssd`.\n\n- Plugin enhancements\n  - Added [GridAnchorRectPlugin](plugin/gridAnchorPlugin) to support rectangular feature maps in gridAnchorPlugin.\n  - Added [ROIAlignPlugin](plugin/roiAlignPlugin) to support the ONNX operator [RoiAlign](https://github.com/onnx/onnx/blob/main/docs/Operators.md#RoiAlign). The ONNX parser will automatically route ROIAlign ops through the plugin.\n  - Added Hopper support for the [BERTQKVToContextPlugin](plugin/bertQKVToContextPlugin) plugin.\n  - Exposed the **use_int8_scale_max** attribute in the [BERTQKVToContextPlugin](plugin/bertQKVToContextPlugin) plugin to allow users to disable the by-default usage of INT8 scale factors to optimize softmax MAX reduction in versions 2 and 3 of the plugin.\n\n- ONNX-TensorRT changes\n  - Added support for operator [Reciprocal](https://github.com/onnx/onnx/blob/main/docs/Operators.md#Reciprocal).\n\n- Build containers\n  - Updated default cuda versions to `11.8.0`.\n\n- Tooling enhancements\n  - Updated [onnx-graphsurgeon](tools/onnx-graphsurgeon) to v0.3.25.\n  - Updated [Polygraphy](tools/Polygraphy) to v0.43.1.\n  - Updated [polygraphy-extension-trtexec](tool/polygraphy-extension-trtexec) to v0.0.8.\n  - Updated [Tensorflow Quantization Toolkit](tools/tensorflow-quantization) to v0.2.0.\n\n## [22.08](https://github.com/NVIDIA/TensorRT/releases/tag/22.08) - 2022-08-16\n\nUpdated TensorRT version to 8.4.2 - see the [TensorRT 8.4.2 release notes](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-4-2) for more information\n\n### Changed\n- Updated default protobuf version to 3.20.x\n- Updated ONNX-TensorRT submodule version to `22.08` tag\n- Updated `sampleIOFormats` and `sampleAlgorithmSelector` to use `ONNX` models over `Caffe`\n\n### Fixes\n- Fixed missing serialization member in custom `ClipPlugin` plugin used in `uff_custom_plugin` sample\n- Fixed various Python import issues\n\n### Added\n- Added new DeBERTA demo\n- Added version 2 for `disentangledAttentionPlugin` to support DeBERTA v2\n\n### Removed\n- None\n\n## [22.07](https://github.com/NVIDIA/TensorRT/releases/tag/22.07) - 2022-07-21\n### Added\n- `polygraphy-trtexec-plugin` tool for Polygraphy\n- Multi-profile support for demoBERT\n- KV cache support for HF BART demo\n\n### Changed\n- Updated ONNX-GS to `v0.3.20`\n\n### Removed\n- None\n\n## [8.4.1 GA](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-4-1) - 2022-06-14\n\nTensorRT OSS release corresponding to TensorRT 8.4.1.5 GA release.\n- Updates since [TensorRT 8.2.1 GA release](https://github.com/NVIDIA/TensorRT/releases/tag/8.2.1).\n- Please refer to the [TensorRT 8.4.1 GA release notes](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-4-1) for more information.\n\nKey Features and Updates:\n\n- Samples enhancements\n  - Added [Detectron2 Mask R-CNN R50-FPN](samples/python/detectron2/README.md) python sample\n  - Added a [quickstart guide](quickstart/deploy_to_triton) for NVidia Triton deployment workflow.\n  - Added onnx export script for [sampleOnnxMnistCoordConvAC](samples/sampleOnnxMnistCoordConvAC)\n  - Removed `sampleNMT`.\n  - Removed usage of deprecated TensorRT APIs in samples.\n\n- EfficientDet sample\n  - Added support for EfficientDet Lite and AdvProp models.\n  - Added dynamic batch support.\n  - Added mixed precision engine builder.\n\n- HuggingFace transformer demo\n  - Added BART model.\n  - Performance speedup of GPT-2 greedy search using GPU implementation.\n  - Fixed GPT2 onnx export failure due to 2G file size limitation.\n  - Extended Megatron LayerNorm plugins to support larger hidden sizes.\n  - Added performance benchmarking mode.\n  - Enable tf32 format by default.\n\n- `demoBERT` enhancements\n  - Add `--duration` flag to perf benchmarking script.\n  - Fixed import of `nvinfer_plugins` library in demoBERT on Windows.\n\n- Torch-QAT toolkit\n  - `quant_bert.py` module removed. It is now upstreamed to [HuggingFace QDQBERT](https://huggingface.co/docs/transformers/model_doc/qdqbert).\n  - Use axis0 as default for deconv.\n  - [#1939](https://github.com/NVIDIA/TensorRT/issues/1939) - Fixed path in `classification_flow` example.\n\n- Plugin enhancements\n  - Added [Disentangled attention plugin](plugin/disentangledAttentionPlugin), `DisentangledAttention_TRT`, to support DeBERTa model.\n  - Added [Multiscale deformable attention plugin](plugin/multiscaleDeformableAttnPlugin), `MultiscaleDeformableAttnPlugin_TRT`, to support DDETR model.\n  - Added new plugins: [decodeBbox3DPlugin](plugin/decodeBbox3DPlugin), [pillarScatterPlugin](plugin/pillarScatterPlugin), and [voxelGeneratorPlugin](plugin/voxelGeneratorPlugin).\n  - Refactored [EfficientNMS plugin](plugin/efficientNMSPlugin) to support [TF-TRT](https://github.com/tensorflow/tensorrt) and implicit batch mode.\n  - `fp16` support for `pillarScatterPlugin`.\n\n- Build containers\n  - Updated default cuda versions to `11.6.2`.\n  - [CentOS Linux 8 has reached End-of-Life](https://www.centos.org/centos-linux-eol/) on Dec 31, 2021. The corresponding container has been removed from TensorRT-OSS.\n  - Install `devtoolset-8` for updated g++ versions in CentOS7 container.\n\n- Tooling enhancements\n  - Added [Tensorflow Quantization Toolkit](tools/tensorflow-quantization) v0.1.0 for Quantization-Aware-Training of Tensorflow 2.x Keras models.\n  - Added [TensorRT Engine Explorer](tools/experimental/trt-engine-explorer/README.md) v0.1.2 for inspecting TensorRT engine plans and associated inference profiling data.\n  - Updated [Polygraphy](tools/Polygraphy) to v0.38.0.\n  - Updated [onnx-graphsurgeon](tools/onnx-graphsurgeon) to v0.3.19.\n\n- `trtexec` enhancements\n  - Added `--layerPrecisions` and `--layerOutputTypes` flags for specifying layer-wise precision and output type constraints.\n  - Added `--memPoolSize` flag to specify the size of workspace as well as the DLA memory pools via a unified interface. Correspondingly the `--workspace` flag has been deprecated.\n  - \"End-To-End Host Latency\" metric has been removed. Use the “Host Latency” metric instead. For more information, refer to [Benchmarking Network](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#trtexec-benchmark) section in the TensorRT Developer Guide.\n  - Use `enqueueV2()` instead of `enqueue()` when engine has explicit batch dimensions.\n\n\n## [22.06](https://github.com/NVIDIA/TensorRT/releases/tag/22.06) - 2022-06-08\n### Added\n- None\n\n### Changed\n- Disentangled attention (DMHA) plugin refactored\n- ONNX parser updated to 8.2GA\n\n### Removed\n- None\n\n## [22.05](https://github.com/NVIDIA/TensorRT/releases/tag/22.05) - 2022-05-13\n### Added\n- Disentangled attention plugin for DeBERTa\n- DMHA (multiscaleDeformableAttnPlugin) plugin for DDETR\n- Performance benchmarking mode to HuggingFace demo\n\n### Changed\n- Updated base TensorRT version to 8.2.5.1\n- Updated onnx-graphsurgeon v0.3.19 [CHANGELOG](tools/onnx-graphsurgeon/CHANGELOG.md)\n- fp16 support for pillarScatterPlugin\n- [#1939](https://github.com/NVIDIA/TensorRT/issues/i1939) - Fixed path in quantization `classification_flow`\n- Fixed GPT2 onnx export failure due to 2G limitation\n- Use axis0 as default for deconv in pytorch-quantization toolkit\n- Updated onnx export script for CoordConvAC sample\n- Install devtoolset-8 for updated g++ version in CentOS7 container\n\n### Removed\n- Usage of deprecated TensorRT APIs in samples removed\n- `quant_bert.py` module removed from pytorch-quantization\n\n## [22.04](https://github.com/NVIDIA/TensorRT/releases/tag/22.04) - 2022-04-13\n### Added\n- TensorRT Engine Explorer v0.1.0 [README](tools/experimental/trt-engine-explorer/README.md)\n- Detectron 2 Mask R-CNN R50-FPN python [sample](samples/python/detectron2/README.md)\n- Model export script for sampleOnnxMnistCoordConvAC\n\n### Changed\n- Updated base TensorRT version to 8.2.4.2\n- Updated copyright headers with SPDX identifiers\n- Updated onnx-graphsurgeon v0.3.17 [CHANGELOG](tools/onnx-graphsurgeon/CHANGELOG.md)\n- `PyramidROIAlign` plugin refactor and bug fixes\n- Fixed `MultilevelCropAndResize` crashes on Windows\n- [#1583](https://github.com/NVIDIA/TensorRT/issues/1583) - sublicense ieee/half.h under Apache2\n- Updated demo/BERT performance tables for rel-8.2\n- [#1774](https://github.com/NVIDIA/TensorRT/issues/1774) Fix python hangs at IndexErrors when TF is imported after TensorRT\n- Various bugfixes in demos - BERT, Tacotron2 and HuggingFace GPT/T5 notebooks\n- Cleaned up sample READMEs\n\n### Removed\n- sampleNMT removed from samples\n\n## [22.03](https://github.com/NVIDIA/TensorRT/releases/tag/22.03) - 2022-03-23\n### Added\n- EfficientDet sample enhancements\n  - Added support for EfficientDet Lite and AdvProp models.\n  - Added dynamic batch support.\n  - Added mixed precision engine builder.\n\n### Changed\n- Better decoupling of HuggingFace demo tests\n\n## [22.02](https://github.com/NVIDIA/TensorRT/releases/tag/22.02) - 2022-02-04\n### Added\n- New plugins: [decodeBbox3DPlugin](plugin/decodeBbox3DPlugin), [pillarScatterPlugin](plugin/pillarScatterPlugin), and [voxelGeneratorPlugin](plugin/voxelGeneratorPlugin)\n\n### Changed\n- Extend Megatron LayerNorm plugins to support larger hidden sizes\n- Refactored EfficientNMS plugin for TFTRT and added implicit batch mode support\n- Update base TensorRT version to 8.2.3.0\n- GPT-2 greedy search speedup - now runs on GPU\n- Updates to TensorRT developer tools\n  - Polygraphy [v0.35.1](tools/Polygraphy/CHANGELOG.md#v0351-2022-01-14)\n  - onnx-graphsurgeon [v0.3.15](tools/onnx-graphsurgeon/CHANGELOG.md#v0315-2022-01-18)\n- Updated ONNX parser to v8.2.3.0\n- Minor updates and bugfixes\n  - Samples: TFOD, GPT-2, demo/BERT\n  - Plugins: proposalPlugin, geluPlugin, bertQKVToContextPlugin, batchedNMS\n\n### Removed\n- Unused source file(s) in demo/BERT\n\n## [8.2.1 GA](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-2-1) - 2021-11-24\n\nTensorRT OSS release corresponding to TensorRT 8.2.1.8 GA release.\n- Updates since [TensorRT 8.2.0 EA release](https://github.com/NVIDIA/TensorRT/releases/tag/8.2.0-EA).\n- Please refer to the [TensorRT 8.2.1 GA release notes](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-2-1) for more information.\n\n- ONNX parser [v8.2.1](https://github.com/onnx/onnx-tensorrt/releases/tag/release%2F8.2-GA)\n  - Removed duplicate constant layer checks that caused some performance regressions\n  - Fixed expand dynamic shape calculations\n  - Added parser-side checks for `Scatter` layer support\n\n- Sample updates\n  - Added [Tensorflow Object Detection API converter samples](samples/python/tensorflow_object_detection_api), including Single Shot Detector, Faster R-CNN and Mask R-CNN models\n  - Multiple enhancements in HuggingFace transformer demos\n    - Added multi-batch support\n    - Fixed resultant performance regression in batchsize=1\n    - Fixed T5 large/T5-3B accuracy issues\n    - Added [notebooks](demo/HuggingFace/notebooks) for T5 and GPT-2\n    - Added CPU benchmarking option\n  - Deprecated `kSTRICT_TYPES` (strict type constraints). Equivalent behaviour now achieved by setting `PREFER_PRECISION_CONSTRAINTS`, `DIRECT_IO`, and `REJECT_EMPTY_ALGORITHMS`\n  - Removed `sampleMovieLens`\n  - Renamed sampleReformatFreeIO to sampleIOFormats\n  - Add `idleTime` option for samples to control qps\n  - Specify default value for `precisionConstraints`\n  - Fixed reporting of TensorRT build version in trtexec\n  - Fixed `combineDescriptions` typo in trtexec/tracer.py\n  - Fixed usages of of `kDIRECT_IO`\n\n- Plugin updates\n  - `EfficientNMS` plugin support extended to TF-TRT, and for clang builds.\n  - Sanitize header definitions for BERT fused MHA plugin\n  - Separate C++ and cu files in `splitPlugin` to avoid PTX generation (required for CUDA enhanced compatibility support)\n  - Enable C++14 build for plugins\n\n- ONNX tooling updates\n  - [onnx-graphsurgeon](tools/onnx-graphsurgeon/CHANGELOG.md) upgraded to v0.3.14\n  - [Polygraphy](tools/Polygraphy/CHANGELOG.md) upgraded to v0.33.2\n  - [pytorch-quantization](tools/pytorch-quantization) toolkit upgraded to v2.1.2\n\n- Build and container fixes\n  - Add `SM86` target to default `GPU_ARCHS` for platforms with cuda-11.1+\n  - Remove deprecated `SM_35` and add `SM_60` to default `GPU_ARCHS`\n  - Skip CUB builds for cuda 11.0+ [#1455](https://github.com/NVIDIA/TensorRT/pull/1455)\n  - Fixed cuda-10.2 container build failures in Ubuntu 20.04\n  - Add native ARM server build container\n  - Install devtoolset-8 for updated g++ version in CentOS7\n  - Added a note on supporting c++14 builds for CentOS7\n  - Fixed docker build for large UIDs [#1373](https://github.com/NVIDIA/TensorRT/issues/1373)\n  - Updated README instructions for Jetpack builds\n\n- demo enhancements\n  - Updated Tacotron2 instructions and add CPU benchmarking\n  - Fixed issues in demoBERT python notebook\n\n- Documentation updates\n  - Updated Python documentation for `add_reduce`, `add_top_k`, and `ISoftMaxLayer`\n  - Renamed default GitHub branch to `main` and updated hyperlinks\n\n## [8.2.0 EA](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-2-0-EA) - 2021-10-05\n### Added\n- [Demo applications](demo/HuggingFace) showcasing TensorRT inference of [HuggingFace Transformers](https://huggingface.co/transformers).\n  - Support is currently extended to GPT-2 and T5 models.\n- Added support for the following ONNX operators:\n  - `Einsum`\n  - `IsNan`\n  - `GatherND`\n  - `Scatter`\n  - `ScatterElements`\n  - `ScatterND`\n  - `Sign`\n  - `Round`\n- Added support for building TensorRT Python API on Windows.\n\n### Updated\n- Notable API updates in TensorRT 8.2.0.6 EA release. See [TensorRT Developer Guide](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html) for details.\n  - Added three new APIs, `IExecutionContext: getEnqueueEmitsProfile()`, `setEnqueueEmitsProfile()`, and `reportToProfiler()` which can be used to collect layer profiling info when the inference is launched as a CUDA graph.\n  - Eliminated the global logger; each `Runtime`, `Builder` or `Refitter` now has its own logger.\n  - Added new operators: `IAssertionLayer`, `IConditionLayer`, `IEinsumLayer`, `IIfConditionalBoundaryLayer`, `IIfConditionalOutputLayer`, `IIfConditionalInputLayer`, and `IScatterLayer`.\n  - Added new `IGatherLayer` modes: `kELEMENT` and `kND`\n  - Added new `ISliceLayer` modes: `kFILL`, `kCLAMP`, and `kREFLECT`\n  - Added new `IUnaryLayer` operators: `kSIGN` and `kROUND`\n  - Added new runtime class `IEngineInspector` that can be used to inspect the detailed information of an engine, including the layer parameters, the chosen tactics, the precision used, etc.\n  - `ProfilingVerbosity` enums have been updated to show their functionality more explicitly.\n- Updated TensorRT OSS container defaults to cuda 11.4\n- CMake to target C++14 builds.\n- Updated following ONNX operators:\n  - `Gather` and `GatherElements` implementations to natively support negative indices\n  - `Pad` layer to support ND padding, along with `edge` and `reflect` padding mode support\n  - `If` layer with general performance improvements.\n\n### Removed\n- Removed `sampleMLP`.\n- Several flags of trtexec have been deprecated:\n  - `--explicitBatch` flag has been deprecated and has no effect. When the input model is in UFF or in Caffe prototxt format, the implicit batch dimension mode is used automatically; when the input model is in ONNX format, the explicit batch mode is used automatically.\n  - `--explicitPrecision` flag has been deprecated and has no effect. When the input ONNX model contains Quantization/Dequantization nodes, TensorRT automatically uses explicit precision mode.\n  - `--nvtxMode=[verbose|default|none]` has been deprecated in favor of `--profilingVerbosity=[detailed|layer_names_only|none]` to show its functionality more explicitly.\n\n## [21.10](https://github.com/NVIDIA/TensorRT/releases/tag/21.10) - 2021-10-05\n### Added\n- Benchmark script for demoBERT-Megatron\n- Dynamic Input Shape support for EfficientNMS plugin\n- Support empty dimensions in ONNX\n- INT32 and dynamic clips through elementwise in ONNX parser\n\n### Changed\n- Bump TensorRT version to 8.0.3.4\n- Use static shape for only single batch single sequence input in demo/BERT\n- Revert to using native FC layer in demo/BERT and FCPlugin only on older GPUs.\n- Update demo/Tacotron2 for TensorRT 8.0\n- Updates to TensorRT developer tools\n  - Polygraphy [v0.33.0](tools/Polygraphy/CHANGELOG.md#v0330-2021-09-16)\n    - Added various examples, a CLI User Guide and how-to guides.\n    - Added experimental support for DLA.\n    - Added a `data to-input` tool that can combine inputs/outputs created by `--save-inputs`/`--save-outputs`.\n    - Added a `PluginRefRunner` which provides CPU reference implementations for TensorRT plugins\n    - Made several performance improvements in the Polygraphy CUDA wrapper.\n    - Removed the `to-json` tool which was used to convert Pickled data generated by Polygraphy 0.26.1 and older to JSON.\n  - Bugfixes and documentation updates in pytorch-quantization toolkit.\n- Bumped up package versions: tensorflow-gpu 2.5.1, pillow 8.3.2\n- ONNX parser enhancements and bugfixes\n  - Update ONNX submodule to v1.8.0\n  - Update convDeconvMultiInput function to properly handle deconvs\n  - Update RNN documentation\n  - Update QDQ axis assertion\n  - Fix bidirectional activation alpha and beta values\n  - Fix opset10 `Resize`\n  - Fix shape tensor unsqueeze\n  - Mark BOOL tiles as unsupported\n  - Remove unnecessary shape tensor checks\n\n### Removed\n- N/A\n\n## [21.09](https://github.com/NVIDIA/TensorRT/releases/tag/21.09) - 2021-09-22\n### Added\n- Add `ONNX2TRT_VERSION` overwrite in CMake.\n\n### Changed\n- Updates to TensorRT developer tools\n  - ONNX-GraphSurgeon [v0.3.12](tools/onnx-graphsurgeon/CHANGELOG.md#v0312-2021-08-24)\n  - pytorch-quantization toolkit [v2.1.1](tools/pytorch-quantization)\n- Fix assertion in EfficientNMSPlugin\n\n### Removed\n- N/A\n\n## [21.08](https://github.com/NVIDIA/TensorRT/releases/tag/21.08) - 2021-08-05\n### Added\n\n- Add demoBERT and demoBERT-MT (sparsity) benchmark data for TensorRT 8.\n- Added example python notebooks\n  - [BERT - Q&A with TensorRT](demo/BERT/notebooks)\n  - [EfficientNet - Object Detection with TensorRT](demo/EfficientDet/notebooks)\n\n### Changed\n- Updated samples and plugins directory structure\n- Updates to TensorRT developer tools\n  - Polygraphy [v0.31.1](tools/Polygraphy/CHANGELOG.md#v0311-2021-07-16)\n  - ONNX-GraphSurgeon [v0.3.11](tools/onnx-graphsurgeon/CHANGELOG.md#v0311-2021-07-14)\n  - pytorch-quantization toolkit [v2.1.1](tools/pytorch-quantization)\n- README fix to update build command for native aarch64 builds.\n\n### Removed\n- N/A\n\n## [21.07](https://github.com/NVIDIA/TensorRT/releases/tag/21.07) - 2021-07-21\nIdentical to the TensorRT-OSS [8.0.1](https://github.com/NVIDIA/TensorRT/releases/tag/8.0.1) Release.\n\n## [8.0.1](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#tensorrt-8) - 2021-07-02\n### Added\n- Added support for the following ONNX operators: `Celu`, `CumSum`, `EyeLike`, `GatherElements`, `GlobalLpPool`, `GreaterOrEqual`, `LessOrEqual`, `LpNormalization`, `LpPool`, `ReverseSequence`, and `SoftmaxCrossEntropyLoss` [details]().\n- Rehauled `Resize` ONNX operator, now fully supporting the following modes:\n  - Coordinate Transformation modes: `half_pixel`, `pytorch_half_pixel`, `tf_half_pixel_for_nn`, `asymmetric`, and `align_corners`.\n  - Modes: `nearest`, `linear`.\n  - Nearest Modes: `floor`, `ceil`, `round_prefer_floor`, `round_prefer_ceil`.\n- Added support for multi-input ONNX `ConvTranpose` operator.\n- Added support for 3D spatial dimensions in ONNX `InstanceNormalization`.\n- Added support for generic 2D padding in ONNX.\n- ONNX `QuantizeLinear` and `DequantizeLinear` operators leverage `IQuantizeLayer` and `IDequantizeLayer`.\n  - Added support for tensor scales.\n  - Added support for per-axis quantization.\n- Added `EfficientNMS_TRT`, `EfficientNMS_ONNX_TRT` plugins and experimental support for ONNX `NonMaxSuppression` operator.\n- Added `ScatterND` plugin.\n- Added TensorRT [QuickStart Guide](https://github.com/NVIDIA/TensorRT/tree/main/quickstart).\n- Added new samples: [engine_refit_onnx_bidaf](https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#engine_refit_onnx_bidaf) builds an engine from ONNX BiDAF model and refits engine with new weights, [efficientdet](samples/python/efficientdet) and [efficientnet](samples/python/efficientnet) samples for demonstrating Object Detection using TensorRT.\n- Added support for Ubuntu20.04 and RedHat/CentOS 8.3.\n- Added Python 3.9 support.\n\n### Changed\n- Update Polygraphy to [v0.30.3](tools/Polygraphy/CHANGELOG.md#v0303-2021-06-25).\n- Update ONNX-GraphSurgeon to [v0.3.10](tools/onnx-graphsurgeon/CHANGELOG.md#v0310-2021-05-20).\n- Update Pytorch Quantization toolkit to v2.1.0.\n- Notable TensorRT API updates\n  - TensorRT now declares API’s with the `noexcept` keyword. All TensorRT classes that an application inherits from (such as IPluginV2) must guarantee that methods called by TensorRT do not throw uncaught exceptions, or the behavior is undefined.\n  - Destructors for classes with `destroy()` methods were previously protected. They are now public, enabling use of smart pointers for these classes. The `destroy()` methods are deprecated.\n- Moved `RefitMap` API from ONNX parser to core TensorRT.\n- Various bugfixes for plugins, samples and ONNX parser.\n- Port demoBERT to tensorflow2 and update UFF samples to leverage nvidia-tensorflow1 container.\n\n### Removed\n- `IPlugin` and `IPluginFactory` interfaces were deprecated in TensorRT 6.0 and have been removed in TensorRT 8.0. We recommend that you write new plugins or refactor existing ones to target the `IPluginV2DynamicExt` and `IPluginV2IOExt` interfaces. For more information, refer to [Migrating Plugins From TensorRT 6.x Or 7.x To TensorRT 8.x.x](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#migrating-plugins-6x-7x-to-8x).\n  - For plugins based on `IPluginV2DynamicExt` and `IPluginV2IOExt`, certain methods with legacy function signatures (derived from `IPluginV2` and `IPluginV2Ext` base classes) which were deprecated and marked for removal in TensorRT 8.0 will no longer be available.\n- Removed `samplePlugin` since it showcased IPluginExt interface, which is no longer supported in TensorRT 8.0.\n- Removed `sampleMovieLens` and `sampleMovieLensMPS`.\n- Removed Dockerfile for Ubuntu 16.04. TensorRT 8.0 debians for Ubuntu 16.04 require python 3.5 while minimum required python version for TensorRT OSS is 3.6.\n- Removed support for PowerPC builds, consistent with TensorRT GA releases.\n\n### Notes\n- We had deprecated the Caffe Parser and UFF Parser in TensorRT 7.0. They are still tested and functional in TensorRT 8.0, however, we plan to remove the support in a future release. Ensure you migrate your workflow to use `tf2onnx`, `keras2onnx` or [TensorFlow-TensorRT (TF-TRT)](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html).\n- Refer to [TensorRT 8.0.1 GA Release Notes](https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-801/release-notes/tensorrt-8.html#rel_8-0-1) for additional details\n\n\n## [21.06](https://github.com/NVIDIA/TensorRT/releases/tag/21.06) - 2021-06-23\n### Added\n- Add switch for batch-agnostic mode in NMS plugin\n- Add missing model.py in `uff_custom_plugin` sample\n\n### Changed\n- Update to [Polygraphy v0.29.2](tools/Polygraphy/CHANGELOG.md#v0292-2021-04-30)\n- Update to [ONNX-GraphSurgeon v0.3.9](tools/onnx-graphsurgeon/CHANGELOG.md#v039-2021-04-20)\n- Fix numerical errors for float type in NMS/batchedNMS plugins\n- Update demoBERT input dimensions to match Triton requirement [#1051](https://github.com/NVIDIA/TensorRT/pull/1051)\n- Optimize TLT MaskRCNN plugins:\n  - enable fp16 precision in multilevelCropAndResizePlugin and multilevelProposeROIPlugin\n  - Algorithms optimization for NMS kernels and ROIAlign kernel\n  - Fix invalid cuda config issue when bs is larger than 32\n  - Fix issues found  on Jetson NANO\n\n### Removed\n- Removed fcplugin from demoBERT to improve latency\n\n\n## [21.05](https://github.com/NVIDIA/TensorRT/releases/tag/21.05) - 2021-05-20\n### Added\n- Extended support for ONNX operator `InstanceNormalization` to 5D tensors\n- Support negative indices in ONNX `Gather` operator\n- Add support for importing ONNX double-typed weights as float\n- [ONNX-GraphSurgeon (v0.3.7)](tools/onnx-graphsurgeon/CHANGELOG.md#v037-2021-03-31) support for models with externally stored weights\n\n### Changed\n- Update ONNX-TensorRT to [21.05](https://github.com/onnx/onnx-tensorrt/releases/tag/21.05)\n- [Relicense ONNX-TensorRT](https://github.com/onnx/onnx-tensorrt/blob/master/LICENSE) under Apache2\n- demoBERT builder fixes for multi-batch\n- Speedup demoBERT build using global timing cache and disable cuDNN tactics\n- Standardize python package versions across OSS samples\n- Bugfixes in multilevelProposeROI and bertQKV plugin\n- Fix memleaks in samples logger\n\n\n## [21.04](https://github.com/NVIDIA/TensorRT/releases/tag/21.04) - 2021-04-12\n### Added\n- SM86 kernels for BERT MHA plugin\n- Added opset13 support for `SoftMax`, `LogSoftmax`, `Squeeze`, and `Unsqueeze`.\n- Added support for the `EyeLike` and `GatherElements` operators.\n\n### Changed\n- Updated TensorRT version to v7.2.3.4.\n- Update to ONNX-TensorRT [21.03](https://github.com/onnx/onnx-tensorrt/releases/tag/21.03)\n- ONNX-GraphSurgeon (v0.3.4) - updates fold_constants to correctly exit early.\n- Set default CUDA_INSTALL_DIR [#798](https://github.com/NVIDIA/TensorRT/pull/798)\n- Plugin bugfixes, qkv kernels for sm86\n- Fixed GroupNorm CMakeFile for cu sources [#1083](https://github.com/NVIDIA/TensorRT/pull/1083)\n- Permit groupadd with non-unique GID in build containers [#1091](https://github.com/NVIDIA/TensorRT/pull/1091)\n- Avoid `reinterpret_cast` [#146](https://github.com/NVIDIA/TensorRT/pull/146)\n- Clang-format plugins and samples\n- Avoid arithmetic on void pointer in multilevelProposeROIPlugin.cpp [#1028](https://github.com/NVIDIA/TensorRT/pull/1028)\n- Update BERT plugin documentation.\n\n### Removed\n- Removes extra terminate call in InstanceNorm\n\n\n## [21.03](https://github.com/NVIDIA/TensorRT/releases/tag/21.03) - 2021-03-09\n### Added\n- Optimized FP16 NMS/batchedNMS plugins with n-bit radix sort and based on `IPluginV2DynamicExt`\n- `ProposalDynamic` and `CropAndResizeDynamic` plugins based on `IPluginV2DynamicExt`\n\n### Changed\n- [ONNX-TensorRT v21.03 update](https://github.com/onnx/onnx-tensorrt/blob/master/docs/Changelog.md#2103-container-release---2021-03-09)\n- [ONNX-GraphSurgeon v0.3.3 update](tools/onnx-graphsurgeon/CHANGELOG.md#v03-2021-03-04)\n- Bugfix for `scaledSoftmax` kernel\n\n### Removed\n- N/A\n\n\n## [21.02](https://github.com/NVIDIA/TensorRT/releases/tag/21.02) - 2021-02-01\n### Added\n- [TensorRT Python API bindings](python)\n- [TensorRT Python samples](samples/python)\n- FP16 support to batchedNMSPlugin [#1002](https://github.com/NVIDIA/TensorRT/pull/1002)\n- Configurable input size for TLT MaskRCNN Plugin [#986](https://github.com/NVIDIA/TensorRT/pull/986)\n\n### Changed\n- TensorRT version updated to 7.2.2.3\n- [ONNX-TensorRT v21.02 update](https://github.com/onnx/onnx-tensorrt/blob/master/docs/Changelog.md#2102-container-release---2021-01-22)\n- [Polygraphy v0.21.1 update](tools/Polygraphy/CHANGELOG.md#v0211-2021-01-12)\n- [PyTorch-Quantization Toolkit](tools/pytorch-quantization) v2.1.0 update\n  - Documentation update, ONNX opset 13 support, ResNet example\n- [ONNX-GraphSurgeon v0.28 update](tools/onnx-graphsurgeon/CHANGELOG.md#v028-2020-10-08)\n- [demoBERT builder](demo/BERT) updated to work with Tensorflow2 (in compatibility mode)\n- Refactor [Dockerfiles](docker) for OSS container\n\n### Removed\n- N/A\n\n\n## [20.12](https://github.com/NVIDIA/TensorRT/releases/tag/20.12) - 2020-12-18\n### Added\n- Add configurable input size for TLT MaskRCNN Plugin\n\n### Changed\n- Update symbol export map for plugins\n- Correctly use channel dimension when creating Prelu node\n- Fix Jetson cross compilation CMakefile\n\n### Removed\n- N/A\n\n\n## [20.11](https://github.com/NVIDIA/TensorRT/releases/tag/20.11) - 2020-11-20\n### Added\n- API documentation for [ONNX-GraphSurgeon](https://github.com/NVIDIA/TensorRT/tree/main/tools/onnx-graphsurgeon/docs)\n\n### Changed\n- Support for SM86 in [demoBERT](https://github.com/NVIDIA/TensorRT/tree/main/demo/BERT)\n- Updated NGC checkpoint URLs for [demoBERT](https://github.com/NVIDIA/TensorRT/tree/main/demo/BERT) and [Tacotron2](https://github.com/NVIDIA/TensorRT/tree/main/demo/Tacotron2).\n\n### Removed\n- N/A\n\n\n## [20.10](https://github.com/NVIDIA/TensorRT/releases/tag/20.10) - 2020-10-22\n### Added\n- [Polygraphy](https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy) v0.20.13 - Deep Learning Inference Prototyping and Debugging Toolkit\n- [PyTorch-Quantization Toolkit](https://github.com/NVIDIA/TensorRT/tree/main/tools/pytorch-quantization) v2.0.0\n- Updated BERT plugins for [variable sequence length inputs](https://github.com/NVIDIA/TensorRT/tree/main/demo/BERT#variable-sequence-length)\n- Optimized kernels for sequence lengths of 64 and 96 added\n- Added Tacotron2 + Waveglow TTS demo [#677](https://github.com/NVIDIA/TensorRT/pull/677)\n- Re-enable `GridAnchorRect_TRT` plugin with rectangular feature maps [#679](https://github.com/NVIDIA/TensorRT/pull/679)\n- Update batchedNMS plugin to IPluginV2DynamicExt interface [#738](https://github.com/NVIDIA/TensorRT/pull/738)\n- Support 3D inputs in InstanceNormalization plugin [#745](https://github.com/NVIDIA/TensorRT/pull/745)\n- Added this CHANGELOG.md\n\n### Changed\n- ONNX GraphSurgeon - v0.2.7 with bugfixes, new examples.\n- demo/BERT bugfixes for Jetson Xavier\n- Updated build Dockerfile to cuda-11.1\n- Updated ClangFormat style specification according to TensorRT coding guidelines\n\n### Removed\n- N/A\n\n\n## [7.2.1](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/tensorrt-7.html#rel_7-2-1) - 2020-10-20\n### Added\n- [Polygraphy](tools/Polygraphy) v0.20.13 - Deep Learning Inference Prototyping and Debugging Toolkit\n- [PyTorch-Quantization Toolkit](tools/pytorch-quantization) v2.0.0\n- Updated BERT plugins for [variable sequence length inputs](demo/BERT#variable-sequence-length)\n  - Optimized kernels for sequence lengths of 64 and 96 added\n- Added Tacotron2 + Waveglow TTS demo [#677](https://github.com/NVIDIA/TensorRT/pull/677)\n- Re-enable `GridAnchorRect_TRT` plugin with rectangular feature maps [#679](https://github.com/NVIDIA/TensorRT/pull/679)\n- Update batchedNMS plugin to IPluginV2DynamicExt interface [#738](https://github.com/NVIDIA/TensorRT/pull/738)\n- Support 3D inputs in InstanceNormalization plugin [#745](https://github.com/NVIDIA/TensorRT/pull/745)\n- Added this CHANGELOG.md\n\n### Changed\n- ONNX GraphSurgeon - [v0.2.7](tools/onnx-graphsurgeon/CHANGELOG.md#v027-2020-09-29) with bugfixes, new examples.\n- demo/BERT bugfixes for Jetson Xavier\n- Updated build Dockerfile to cuda-11.1\n- Updated ClangFormat style specification according to TensorRT coding guidelines\n\n### Removed\n- N/A\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 8.2939453125,
          "content": "#\n# SPDX-FileCopyrightText: Copyright (c) 1993-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\ncmake_minimum_required(VERSION 3.13 FATAL_ERROR)\ninclude(cmake/modules/set_ifndef.cmake)\ninclude(cmake/modules/find_library_create_target.cmake)\n\nset_ifndef(TRT_LIB_DIR ${CMAKE_BINARY_DIR})\nset_ifndef(TRT_OUT_DIR ${CMAKE_BINARY_DIR})\n\n# Converts Windows paths\nif(CMAKE_VERSION VERSION_LESS 3.20)\n    file(TO_CMAKE_PATH \"${TRT_LIB_DIR}\" TRT_LIB_DIR)\n    file(TO_CMAKE_PATH \"${TRT_OUT_DIR}\" TRT_OUT_DIR)\nelse()\n    cmake_path(SET TRT_LIB_DIR ${TRT_LIB_DIR})\n    cmake_path(SET TRT_OUT_DIR ${TRT_OUT_DIR})\nendif()\n\n# Required to export symbols to build *.libs\nif(WIN32)\n    add_compile_definitions(TENSORRT_BUILD_LIB 1)\nendif()\n\n# Set output paths\nset(RUNTIME_OUTPUT_DIRECTORY ${TRT_OUT_DIR} CACHE PATH \"Output directory for runtime target files\")\nset(LIBRARY_OUTPUT_DIRECTORY ${TRT_OUT_DIR} CACHE PATH \"Output directory for library target files\")\nset(ARCHIVE_OUTPUT_DIRECTORY ${TRT_OUT_DIR} CACHE PATH \"Output directory for archive target files\")\n\nif(WIN32)\n    set(STATIC_LIB_EXT \"lib\")\nelse()\n    set(STATIC_LIB_EXT \"a\")\nendif()\n\nfile(STRINGS \"${CMAKE_CURRENT_SOURCE_DIR}/include/NvInferVersion.h\" VERSION_STRINGS REGEX \"#define NV_TENSORRT_.*\")\n\nforeach(TYPE MAJOR MINOR PATCH BUILD)\n    string(REGEX MATCH \"NV_TENSORRT_${TYPE} [0-9]+\" TRT_TYPE_STRING ${VERSION_STRINGS})\n    string(REGEX MATCH \"[0-9]+\" TRT_${TYPE} ${TRT_TYPE_STRING})\nendforeach(TYPE)\n\nset(TRT_VERSION \"${TRT_MAJOR}.${TRT_MINOR}.${TRT_PATCH}\" CACHE STRING \"TensorRT project version\")\nset(ONNX2TRT_VERSION \"${TRT_MAJOR}.${TRT_MINOR}.${TRT_PATCH}\" CACHE STRING \"ONNX2TRT project version\")\nset(TRT_SOVERSION \"${TRT_MAJOR}\" CACHE STRING \"TensorRT library so version\")\nmessage(\"Building for TensorRT version: ${TRT_VERSION}, library version: ${TRT_SOVERSION}\")\n\nif(NOT DEFINED CMAKE_TOOLCHAIN_FILE)\n    find_program(CMAKE_CXX_COMPILER NAMES $ENV{CXX} g++)\nendif()\n\nset(CMAKE_SKIP_BUILD_RPATH True)\n\nproject(TensorRT\n        LANGUAGES CXX CUDA\n        VERSION ${TRT_VERSION}\n        DESCRIPTION \"TensorRT is a C++ library that facilitates high-performance inference on NVIDIA GPUs and deep learning accelerators.\"\n        HOMEPAGE_URL \"https://github.com/NVIDIA/TensorRT\")\n\nif(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT)\n  set(CMAKE_INSTALL_PREFIX ${TRT_LIB_DIR}/../ CACHE PATH \"TensorRT installation\" FORCE)\nendif(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT)\n\noption(BUILD_PLUGINS \"Build TensorRT plugin\" ON)\noption(BUILD_PARSERS \"Build TensorRT parsers\" ON)\noption(BUILD_SAMPLES \"Build TensorRT samples\" ON)\n\n# C++14\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_EXTENSIONS OFF)\n\nif(NOT MSVC)\n    set(CMAKE_CXX_FLAGS \"-Wno-deprecated-declarations ${CMAKE_CXX_FLAGS} -DBUILD_SYSTEM=cmake_oss\")\nelse()\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DBUILD_SYSTEM=cmake_oss\")\nendif()\n\n############################################################################################\n# Cross-compilation settings\n\nset_ifndef(TRT_PLATFORM_ID \"x86_64\")\nmessage(STATUS \"Targeting TRT Platform: ${TRT_PLATFORM_ID}\")\n\n############################################################################################\n# Debug settings\n\nset(TRT_DEBUG_POSTFIX _debug CACHE STRING \"suffix for debug builds\")\n\nif (CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n    message(\"Building in debug mode ${DEBUG_POSTFIX}\")\nendif()\n\n############################################################################################\n# Dependencies\n\nset(DEFAULT_CUDA_VERSION 12.2.0)\nset(DEFAULT_CUDNN_VERSION 8.9)\nset(DEFAULT_PROTOBUF_VERSION 3.20.1)\n\n# Dependency Version Resolution\nset_ifndef(CUDA_VERSION ${DEFAULT_CUDA_VERSION})\nmessage(STATUS \"CUDA version set to ${CUDA_VERSION}\")\nset_ifndef(CUDNN_VERSION ${DEFAULT_CUDNN_VERSION})\nmessage(STATUS \"cuDNN version set to ${CUDNN_VERSION}\")\nset_ifndef(PROTOBUF_VERSION ${DEFAULT_PROTOBUF_VERSION})\nmessage(STATUS \"Protobuf version set to ${PROTOBUF_VERSION}\")\n\nset(THREADS_PREFER_PTHREAD_FLAG ON)\nfind_package(Threads REQUIRED)\nif (BUILD_PLUGINS OR BUILD_PARSERS)\n    include(third_party/protobuf.cmake)\nendif()\nif(NOT CUB_ROOT_DIR)\n  if (CUDA_VERSION VERSION_LESS 11.0)\n    set(CUB_ROOT_DIR ${CMAKE_CURRENT_SOURCE_DIR}/third_party/cub CACHE STRING \"directory of CUB installation\")\n  endif()\nendif()\n\n## find_package(CUDA) is broken for cross-compilation. Enable CUDA language instead.\nif(NOT DEFINED CMAKE_TOOLCHAIN_FILE)\n    find_package(CUDA ${CUDA_VERSION} REQUIRED)\nendif()\n\ninclude_directories(\n    ${CUDA_INCLUDE_DIRS}\n)\nif(BUILD_PARSERS)\n    configure_protobuf(${PROTOBUF_VERSION})\nendif()\n\n# Windows library names have major version appended.\nif (MSVC)\n    set(nvinfer_lib_name \"nvinfer_${TRT_SOVERSION}\")\n    set(nvinfer_plugin_lib_name \"nvinfer_plugin_${TRT_SOVERSION}\")\n    set(nvinfer_vc_plugin_lib_name \"nvinfer_vc_plugin_${TRT_SOVERSION}\")\n    set(nvonnxparser_lib_name \"nvonnxparser_${TRT_SOVERSION}\")\nelse()\n    set(nvinfer_lib_name \"nvinfer\")\n    set(nvinfer_plugin_lib_name \"nvinfer_plugin\")\n    set(nvinfer_vc_plugin_lib_name \"nvinfer_vc_plugin\")\n    set(nvonnxparser_lib_name \"nvonnxparser\")\nendif()\n\nfind_library_create_target(nvinfer ${nvinfer_lib_name} SHARED ${TRT_LIB_DIR})\n\nif (DEFINED USE_CUGFX)\n    find_library(CUDART_LIB cugfx_dll HINTS ${CUDA_TOOLKIT_ROOT_DIR} PATH_SUFFIXES lib lib/x64 lib64)\nelse()\n    find_library(CUDART_LIB cudart_static HINTS ${CUDA_TOOLKIT_ROOT_DIR} PATH_SUFFIXES lib lib/x64 lib64)\nendif()\n\nif (NOT MSVC)\n    find_library(RT_LIB rt)\nendif()\n\nset(CUDA_LIBRARIES ${CUDART_LIB})\n\n############################################################################################\n# CUDA targets\n\nif (DEFINED GPU_ARCHS)\n  message(STATUS \"GPU_ARCHS defined as ${GPU_ARCHS}. Generating CUDA code for SM ${GPU_ARCHS}\")\n  separate_arguments(GPU_ARCHS)\nelse()\n  list(APPEND GPU_ARCHS\n      75\n    )\n\n  find_file(IS_L4T_NATIVE nv_tegra_release PATHS /env/)\n  set (IS_L4T_CROSS \"False\")\n  if (DEFINED ENV{IS_L4T_CROSS})\n    set(IS_L4T_CROSS $ENV{IS_L4T_CROSS})\n  endif()\n\n  if (IS_L4T_NATIVE OR ${IS_L4T_CROSS} STREQUAL \"True\")\n    # Only Orin (SM87) supported\n    list(APPEND GPU_ARCHS 87)\n  endif()\n\n  if (CUDA_VERSION VERSION_GREATER_EQUAL 11.0)\n    # Ampere GPU (SM80) support is only available in CUDA versions > 11.0\n    list(APPEND GPU_ARCHS 80)\n  endif()\n  if (CUDA_VERSION VERSION_GREATER_EQUAL 11.1)\n    list(APPEND GPU_ARCHS 86)\n  endif()\n\n  message(STATUS \"GPU_ARCHS is not defined. Generating CUDA code for default SMs: ${GPU_ARCHS}\")\nendif()\nset(BERT_GENCODES)\n# Generate SASS for each architecture\nforeach(arch ${GPU_ARCHS})\n    if (${arch} GREATER_EQUAL 75)\n        set(BERT_GENCODES \"${BERT_GENCODES} -gencode arch=compute_${arch},code=sm_${arch}\")\n    endif()\n    set(GENCODES \"${GENCODES} -gencode arch=compute_${arch},code=sm_${arch}\")\nendforeach()\n\n# Generate PTX for the last architecture in the list.\nlist(GET GPU_ARCHS -1 LATEST_SM)\nset(GENCODES \"${GENCODES} -gencode arch=compute_${LATEST_SM},code=compute_${LATEST_SM}\")\nif (${LATEST_SM} GREATER_EQUAL 75)\n    set(BERT_GENCODES \"${BERT_GENCODES} -gencode arch=compute_${LATEST_SM},code=compute_${LATEST_SM}\")\nendif()\n\nif(NOT MSVC)\n    set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr -Xcompiler -Wno-deprecated-declarations\")\nelse()\n    set(CMAKE_CUDA_SEPARABLE_COMPILATION ON)\n    set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr -Xcompiler\")\nendif()\n\n############################################################################################\n# TensorRT\n\nif(BUILD_PLUGINS)\n    add_subdirectory(plugin)\nelse()\n    find_library_create_target(nvinfer_plugin ${nvinfer_plugin_lib_name} SHARED ${TRT_OUT_DIR} ${TRT_LIB_DIR})\nendif()\n\nif(BUILD_PARSERS)\n    add_subdirectory(parsers)\nelse()\n    find_library_create_target(nvonnxparser ${nvonnxparser_lib_name} SHARED ${TRT_OUT_DIR} ${TRT_LIB_DIR})\nendif()\n\nif(BUILD_SAMPLES)\n    add_subdirectory(samples)\nendif()\n"
        },
        {
          "name": "CODING-GUIDELINES.md",
          "type": "blob",
          "size": 17.6787109375,
          "content": "\n## TensorRT C++ Coding Guidelines\n\nThe TensorRT C++ Coding Guidelines are derived from several sources, primarily:\n\n- [AUTOSAR C++ 2014](https://www.autosar.org/fileadmin/user_upload/standards/adaptive/17-03/AUTOSAR_RS_CPP14Guidelines.pdf)\n- [MISRA C++ 2008](https://www.misra.org.uk/Activities/MISRAC/tabid/171/Default.aspx)\n- [Google C++ Style Guide](https://google.github.io/styleguide/cppguide.html)\n\n------\n\n#### Namespaces\n\n1. *MISRA C++: 2008 Rule 7-3-1*\n   Global namespace shall only contain main, namespace declarations and extern \"C\" declarations. Use explicit or anon namespaces for everything else.\n2. Closing braces of namespaces should have a comment saying the namespace it closes:\n```cpp\nnamespace foo\n{\n...\n} // namespace foo\n```\n\n#### Constants\n1. Prefer `const` or `constexpr` variables over `#defines` whenever possible, as the latter are not visible to the compiler.\n2. *MISRA C++: 2008 Rule 7-1-1 and 7-1-2*\n   A variable that is not modified after its initialization should be declared as `const`.\n3. For naming of constants, see the Naming section of this document.\n\n\n\n#### Literals\n1. Except `0`  (only used in comparison for checking signness/existence/emptiness) and `nullptr`, `true`, `false`, all other literals should only be used for variable initialization.\n   Example:\n```cpp\nif (nbInputs == 2U){/*...*/}\n```\n   Should be changed to:\n```cpp\nconstexpr size_t kNbInputsWBias = 2U;\nif (nbInputs == kNbInputsWBias) {/*...*/}\n```\n\n\n#### Brace Notation\n1. Use the [Allman indentation](https://en.wikipedia.org/wiki/Indent_style#Allman_style) style.\n2. Put the semicolon for an empty `for` or `while` loop in a new line.\n3. *AUTOSAR C++14 Rule 6.6.3*, *MISRA C++: 2008 6-3-1*\n   The statement forming the body of a `switch`, `while`, `do .. while` or `for` statement shall be a compound statement. (use brace-delimited statements)\n4. *AUTOSAR C++14 Rule 6.6.4*, *MISRA C++: 2008 Rule 6-4-1*\n   `If` and `else` should always be followed by brace-delimited statements, even if empty or a single statement.\n\n\n#### Naming\n1. Filenames\n   * Camel case with first letter lowercase: `thisIsASubDir` and `thisIsAFilename.cpp`\n   * *NOTE*: All files involved in the compilation of a compilation target (.exe/.so) must have filenames that are case-insensitive unique.\n\n2. Types\n   * All types (including, but not limited to, class names) are [camel case](https://en.wikipedia.org/wiki/Camel_case) with uppercase first letter. Example: `FooBarClass`\n\n3. Local variables, methods and namespaces\n   * Camel case with first letter lowercase. Example: `localFooBar`\n\n4. Non-magic-number global variables that are non-static and not defined in anonymous namespace\n   * Camel case prefixed by a lower case 'g'. Example: `gDontUseGlobalFoos`\n\n5. Non-magic-number global variables that are static or defined in an anonymous namespace\n   * Camel case prefixed by a lower case  's'. Example: `sMutableStaticGlobal`\n\n6. Locally visible static variable\n   * Camel case with lowercase prefix ''s\" as the first letter of the name. Example: `static std::once_flag sCaskInitOnce;`\n\n7. Public, private and protected class member variables\n   * Camelcase prefixed with an 'm': `mNbFooValues`.\n   * Public member variables do not require the 'm' prefix but it is highly encouraged to use the prefix when needed to improve code clarity, especially in cases where the class is a base class in an inheritance chain.\n\n8. Constants\n   * Enumerations, global constants, static constants at class-scope and function-scope magic-number/literal constants are uppercase snakecase with prefix 'k':\n```cpp\nconst int kDIGIT_NUM = 10;\n```\n> *NOTE*: Function-scope constants that are not magic numbers or literals are named like non-constant variables:\n```cpp\nconst bool pass = a && b;\n```\n\n9. Macros\n   * See [Constants](CODING-GUIDELINES.md#constants), which are preferred over `#define`.\n   * If you must use macros, however, follow uppercase snakecase: `FOO_VERSION`\n\nNotes:\n* In general we don't use [hungarian notation](https://en.wikipedia.org/wiki/Hungarian_notation), except for 'apps hungarian' in some cases such as 'nb' in a variable name to indicate count: `mNbTensorDescriptors`\n* If a constructor's parameter name `foo` conflicts with a public member name `foo`, add a trailing underscore to the parameter name: `foo_`.\n* *MISRA C++: 2008 Rule 2-13-4*\n  Literal suffixes should be upper case. For example, use `1234L` instead of `1234l`.\n\n\n#### Tabs vs Spaces\n1. Use only spaces. Do not use tabs.\n2. Indent 4 spaces at a time. This is enforced automatically if you format your code using our clang-format config.\n\n\n#### Formatting\n1. Use the [LLVM clang-format](https://clang.llvm.org/docs/ClangFormat.html) tool for formatting your changes prior to submitting the PR.\n2. Use a maximum of 120 characters per line. The auto formatting tool will wrap longer lines.\n3. Exceptions to formatting violations must be justified on a per-case basis. Bypassing the formatting rules is discouraged, but can be achieved for exceptions as follows:\n```cpp\n// clang-format off\n// .. Unformatted code ..\n// clang-format on\n```\n\n\n#### Pointers and Memory Allocation\n1. *AUTOSAR C++ 2014: 18-5-2/3*\n   Use smart pointers for allocating objects on the heap.\n2. When picking a smart pointer, prefer `unique_ptr` for single resource ownership and `shared_ptr` for shared resource ownership. Use `weak_ptr` only in exceptional cases.\n3. Do not use smart pointers that have been deprecated in C++11.\n\n\n#### Comments\n1. C++ comments are required. C comments are not allowed except for special cases (inline).\n2. C++ style for single-line comments. `// This is a single line comment`\n3. In function calls where parameters are not obvious from inspection, it can be helpful to use an inline C comment to document the parameter for readers:\n```cpp\ndoSomeOperation(/* checkForErrors = */ false);\n```\n4. If the comment is a full sentence, it should be capitalized i.e. start with capital letter and punctuated properly.\n5. Follow [Doxygen rules](http://www.doxygen.nl/manual/docblocks.html) for documenting new class interfaces and function prototypes.\n* For C++-style single-line comments use `//!`.\n* For class members, use `//!<`.\n```cpp\n//! This is a Doxygen comment\n//! in C++ style\n\nstruct Foo\n{\n    int x; //!< This is a Doxygen comment for members\n}\n```\n\n\n#### Disabling Code\n1. Use `#if` / `#endif` to disable code, preferably with a mnemonic condition like this:\n```cpp\n#if DEBUG_CONVOLUTION_INSTRUMENTATION\n// ...code to be disabled...\n#endif\n```\n\n```cpp\n// Alternative: use a macro which evaluates to a noop in release code.\n#if DEBUG_CONVOLUTION_INSTRUMENTATION\n# define DEBUG_CONV_CODE(x) x\n#else\n# define DEBUG_CONV_CODE(x)\n#endif\n```\n\n2. *MISRA C++: 2008 Rule 0-1-9*, *AutoSAR C++ 2014: 6-0-1*\n   Dead code is forbidden in safety-critical software - you may not use compile-time expressions and DCE to disable code. However, this technique can be useful elsewhere (e.g. tools, tests) to help prevent bitrot.\n\n```cpp\n// Not allowed in safety-critical code.\nconst bool gDisabledFeature = false;\n\nvoid foo()\n{\n   if (gDisabledFeature)\n   {\n       doSomething();\n   }\n}\n```\n\n3. *MISRA C++: 2008 Rule 2-7-2 and 2-7-3*\n   Do NOT use comments to disable code. Use comments to explain code, not hide it.\n\n\n#### Exceptions\n1.  Exceptions must not be thrown across library boundaries.\n\n\n#### Casts\n1. Use the least forceful cast necessary, or no cast if possible, to help the compiler diagnose unintended consequences.\n2. Casting a pointer to a `void*` should be implicit (except if removing `const`).\n3. *MISRA C++: 2008 Rule 5-2-5*\n   Casting should not remove any `const` or `volatile` qualification from the type of a pointer or reference.\n4. *MISRA C++: 2008 Rule 5-2-4*\n   Do not use C-style casts (other than void casts) and functional notation casts (other than explicit constructor calls).\n6. Casting from a `void*` to a `T*` should be done with `static_cast`, not `reinterpret_cast`, since the latter is more forceful.\n7. Use `reinterpret_cast` as a last resort, where `const_cast` and `static_cast` won't work.\n8. Avoid `dynamic_cast`.\n\n\n#### Expressions\n1. *MISRA C++: 2008 Rule 6-2-1*\n   Do not use assignment operator in subexpressions.\n```cpp\n// Not compliant\nx = y = z;\n\n// Not compliant\nif (x = y)\n{\n    // ...\n}\n```\n\n\n#### Ternary operator\n1. *AUTOSAR C++ 2014: 7-1-1*\n   Ternary operator should not be used as a sub-expression. Ternary operator expressions should be encapsulated with braces. Example:\n```cpp\nconst auto var = (condition0 ? a : (condition1 ? b : c));\n```\n   should be changed to:\n```cpp\nconst auto d = (condition1 ? b : c);\nconst auto var = (condition0 ? a : d);\n```\n\n\n#### Statements\n1. When practical, a `switch` statement controlled by an `enum` should have a case for each enum value and not have a default clause so that we get a compile-time error if a new enum value is added.\n2. *MISRA C++:2008 Rules 6-4-3, 6-4-4, and 6-4-5*\n   Switch statements should be well structured.  An informal guideline is to treat switch statements as structured multi-way branches and not \"glorified gotos\" such as:\n```cpp\n// Not compliant\nswitch (x) case 4: if (y) case 5: return 0; else default: return 1;\n```\n3. The \"well structured\" requirement prohibits fall-though except from one case label to another.   Each case clause must be terminated in a break or throw.  If a case clause has multiple statements, the braces are optional.  The following example illustrates these requirements:\n```cpp\nswitch (x)\n{\ncase 0:         // Fall-through allowed from case 0: to case 1: since case 0 is empty.\ncase 1:\n    a();\n    b();\n    break;\ncase 2:\ncase 4:\n{              // With optional braces\n    c();\n    d();\n    break;\n}\ncase 5:\n    c();\n    throw 42;  // Terminating with throw is okay\ndefault:\n    throw 42;\n}\n```\n\n4. *MISRA C++:2008 Rule 6-4-3*\n   Ending a case clause with return is not allowed.\n5. If a switch clause is a compound statement, put the break inside the braces.\n```cpp\nswitch (x)\n{\ncase 0:\ncase 1:\n{\n    y();\n    z();\n    break;\n}\n...other cases...\n}\n```\n\n#### Functions\n1. Avoid declaring large functions as `inline`, absent a quantifiable benefit.  Remember that functions defined in class declarations are implicitly inline.\n2. Rather than using the `static` keyword to mark a function as having internal linkage, prefer to use anonymous namespaces instead.\n3. *MISRA C++:2008 Rule 0-1-10*\n   Every defined function must be called at least once. That is, do not have unused methods.\n4. *MISRA C++:2008 Rule 8-4-2*\n   Parameter names should be consistent across function definition and corresponding function declarations.\n\n\n#### Forward declarations and extern variables\n\n1. *MISRA C++: 2008 Rule 3-2-3*\n   For safety critical code, a type, object or function that is used in multiple translation units shall be declared in one and only one file.\n   * This means we cannot forward declare incomplete types in files where they are needed. Instead, we should put forward declarations in header files, and include these header files as needed.\n\n\n#### Structures and Classes\n1. *MISRA C++: 2008 Rule 14-7-1*\n   All class templates, function templates, class template member functions and class template static members shall be instantiated at least once. This prevents use of uninitialized variables.\n2. *MISRA C++: 2008 Rule 11-01*\n   If class is not a *Plain Old Data Structure*, then its data members should be private.\n\n\n#### Preprocessor Directives\n1. *MISRA C++: 2008 Rule 16-0-2*\n   `#define` and `#undef` of macros should be done only at global namespace.\n2. Avoid the use of `#ifdef` and `#ifndef` directives (except in the case of header include guards). Prefer to use `#if defined(...)` or `#if !defined(...)` instead. The latter syntax is more consistent with C syntax, and allows you to use more complicated preprocessor conditionals, e.g.:\n```cpp\n#if defined(FOO) || defined(BAR)\nvoid foo();\n#endif // defined(FOO) || defined(BAR)\n```\n\n3. When nesting preprocessor directives, use indentation after the hash mark (#). For example:\n```cpp\n#if defined(FOO)\n# if FOO == 0\n#  define BAR 0\n# elif FOO == 1\n#  define BAR 5\n# else\n#  error \"invalid FOO value\"\n# endif\n#endif\n```\n\n4. Do not use `#pragma` once as include guard.\n5. Use a preprocessor guard. It's standard-conforming and modern compilers are smart enough to open the file only once.\n   * The guard name must have prefix `TRT_` followed by the filename, all in caps. For a header file named `FooBarHello.h`, name the symbol as `TRT_FOO_BAR_HELLO_H`.\n   * Only use the file name to create the symbol. Unlike the Google C++ guideline, we do not include the directory names in the symbol. This is because we ensure all filenames are unique in the compilation unit.\n   * Do not use prefix with underscore. Such symbols are reserved in C++ standard for compilers or implementation.\n   * Do not use trailing underscore for the symbol. We differ in this from Google C++ guideline, which uses trailing underscore: `TRT_FOO_BAR_HELLO_H_`\n```cpp\n#ifndef TRT_FOO_BAR_HELLO_H\n#define TRT_FOO_BAR_HELLO_H\n// ...\n#endif // TRT_FOO_BAR_HELLO_H\n```\n\n6. *AUTOSAR C++ 2014: 7-1-6*\n   Use `using` instead of `typedef`.\n\n\n#### Signed vs Unsigned Integers\n1. Use signed integers instead of unsigned, except for  the cases below.\n* The integer is a bitmap - use an unsigned type, since sign extension could lead to surprises.\n* The integer is being used with an external library that expects an unsigned integer.  A common example is a loop that compares against `std::vector::size()`, such as:\n```cpp\nfor (size_t i = 0; i < mTensors.size(); ++i) // preferred style\n```\n* Using only signed integers for the above would lead to prolixity and perhaps unsafe narrowing:\n```cpp\nfor (int i = 0; i < static_cast<int>(mTensors.size()); ++i)\n```\n\n\n#### Special Considerations for API\n1. The API consists, with very few exceptions, of methodless structs and pure virtual interface classes.\n2. API class methods should be either virtual or inline.\n3. The API does not use  integral types with platform-dependent sizes, other than `int`, `unsigned`, and `bool`.   `size_t` should be used only for sizes of memory buffers.\n4. The API does not use any aggregate types (e.g. `std::string`) which may be compiled differently with different compilers and libraries.\n5. The API minimizes dependencies on system headers - currently only `<cstddef>` and `<cstdint>`.\n6. Memory ownership may not be transferred across API boundaries - any memory allocated inside a library must be freed inside the library.\n7. The API should be C++03.\n8. New methods should be added at the end of interfaces so as to preserve v-table compatibility (compilers don't guarantee this, but de facto it works.)\n9. Avoid optional arguments to functions, since they can make it difficult to extend interfaces.\n10. Do not throw exceptions across library boundaries.\n11. Document all APIs with doxygen.\n\n\n#### Common Pitfalls\n\n1. C headers should not be used directly.\n   - Example: Use `<cstdint>` instead of  `<stdint.h>`\n2. Do not use C library functions, whenever possible.\n   * Use brace initialization or `std::fill_n()` instead of `memset()`. This is especially important when dealing with non-[POD types](http://en.cppreference.com/w/cpp/concept/PODType). In the example below, using `memset()` will corrupt the vtable of `Foo:`\n```cpp\nstruct Foo {\n    virtual int getX() { return x; }\n    int x;\n};\n...\n\n// Bad: use memset() to initialize Foo\n{\n    Foo foo;\n    memset(&foo, 0, sizeof(foo)); // Destroys hiddien virtual-function-table pointer!\n}\n// Good: use brace initialization to initialize Foo\n{\n    Foo foo = {};\n}\n```\n\n2. When specifying pointers to `const` data, the pointer itself may be `const`, in some usecases.\n```cpp\nchar const * const errStr = getErrorStr(status);\n```\n\n----\n\n## Appendix\n\n####  Abbreviation Words and Compound Words as Part of Names\n\n* Abbreviation words, which are usually fully-capitalized in literature, are treated as normal words without special capitalization, e.g. `gpuAllocator`, where GPU is converted to `gpu` before constructing the camel case name.\n* Compound words, which are usually used in full in literature, e.g. `runtime`, can be abbreviated into fully capitalized letters, e.g. `RT` in NvInferRT.h.\n\n####  Terminology\n\n* *CUDA code* is code that must be compiled with a CUDA compiler. Typically, it includes:\n   * Declaration or definition of global or static variables with one of the following CUDA keywords: `__device__`, `__managed__` and `__constant__`.\n   * Declaration or definition of device functions decorated with `__device__`.\n   * Declaration or definition of kernels decorated with `__global__`.\n   * Kernel launching with <<<...>>> syntax.\n\n> NOTE:\n   * Definition of kernel function pointer type aliases is not device code, e.g. `typedef __global__ void(*KernelFunc)(void* /*arg*/);`.\n   * Definition of pointers to kernel functions is not device code, either, e.g. `__global__ void(*KernelFunc)(void* /*arg*/) = getKernelFunc(parameters);` .\n   * Kernel launching with the CUDA runtime/driver API's, e.g. `cuLaunch` and `cudaLaunch`, is not CUDA code.\n\n----\n\n## NVIDIA Copyright\n\n1. All TensorRT Open Source Software code should contain an NVIDIA copyright header that includes the current year.  The following block of text should be prepended to the top of all OSS files.  This includes .cpp, .h, .cu, .py, and any other source files which are compiled or interpreted.\n```cpp\n/*\n * Copyright (c) 2021, NVIDIA CORPORATION. All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n```\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 7.06640625,
          "content": "\n## TensorRT OSS Contribution Rules\n\n#### Issue Tracking\n\n* All enhancement, bugfix, or change requests must begin with the creation of a [TensorRT Issue Request](https://github.com/nvidia/TensorRT/issues).\n  * The issue request must be reviewed by TensorRT engineers and approved prior to code review.\n\n\n#### Coding Guidelines\n\n- All source code contributions must strictly adhere to the [TensorRT Coding Guidelines](CODING-GUIDELINES.md).\n\n- In addition, please follow the existing conventions in the relevant file, submodule, module, and project when you add new code or when you extend/fix existing functionality.\n\n- To maintain consistency in code formatting and style, you should also run `clang-format` on the modified sources with the provided configuration file. This applies TensorRT code formatting rules to:\n  - class, function/method, and variable/field naming\n  - comment style\n  - indentation\n  - line length\n\n- Format git changes:\n  ```bash\n  # Commit ID is optional - if unspecified, run format on staged changes.\n  git-clang-format --style file [commit ID/reference]\n  ```\n\n- Format  individual source files:\n  ```bash\n  # -style=file : Obtain the formatting rules from .clang-format\n  # -i : In-place modification of the processed file\n  clang-format -style=file -i -fallback-style=none <file(s) to process>\n  ```\n\n- Format entire codebase (for project maintainers only):\n  ```bash\n  find samples plugin -iname *.h -o -iname *.c -o -iname *.cpp -o -iname *.hpp \\\n  | xargs clang-format -style=file -i -fallback-style=none\n  ```\n\n- Avoid introducing unnecessary complexity into existing code so that maintainability and readability are preserved.\n\n- Try to keep pull requests (PRs) as concise as possible:\n  - Avoid committing commented-out code.\n  - Wherever possible, each PR should address a single concern. If there are several otherwise-unrelated things that should be fixed to reach a desired endpoint, our recommendation is to open several PRs and indicate the dependencies in the description. The more complex the changes are in a single PR, the more time it will take to review those changes.\n\n- Write commit titles using imperative mood and [these rules](https://chris.beams.io/posts/git-commit/), and reference the Issue number corresponding to the PR. Following is the recommended format for commit texts:\n```\n#<Issue Number> - <Commit Title>\n\n<Commit Body>\n```\n\n- Ensure that the build log is clean, meaning no warnings or errors should be present.\n\n- Ensure that all `sample_*` tests pass prior to submitting your code.\n\n- All OSS components must contain accompanying documentation (READMEs) describing the functionality, dependencies, and known issues.\n\n  - See `README.md` for existing samples and plugins for reference.\n\n- All OSS components must have an accompanying test.\n\n  - If introducing a new component, such as a plugin, provide a test sample to verify the functionality.\n\n- To add or disable functionality:\n  - Add a CMake option with a default value that matches the existing behavior.\n  - Where entire files can be included/excluded based on the value of this option, selectively include/exclude the relevant files from compilation by modifying `CMakeLists.txt` rather than using `#if` guards around the entire body of each file.\n  - Where the functionality involves minor changes to existing files, use `#if` guards.\n\n- Make sure that you can contribute your work to open source (no license and/or patent conflict is introduced by your code). You will need to [`sign`](#signing-your-work) your commit.\n\n- Thanks in advance for your patience as we review your contributions; we do appreciate them!\n\n\n#### Pull Requests\nDeveloper workflow for code contributions is as follows:\n\n1. Developers must first [fork](https://help.github.com/en/articles/fork-a-repo) the [upstream](https://github.com/nvidia/TensorRT) TensorRT OSS repository.\n\n2. Git clone the forked repository and push changes to the personal fork.\n\n  ```bash\ngit clone https://github.com/YOUR_USERNAME/YOUR_FORK.git TensorRT\n# Checkout the targeted branch and commit changes\n# Push the commits to a branch on the fork (remote).\ngit push -u origin <local-branch>:<remote-branch>\n  ```\n\n3. Once the code changes are staged on the fork and ready for review, a [Pull Request](https://help.github.com/en/articles/about-pull-requests) (PR) can be [requested](https://help.github.com/en/articles/creating-a-pull-request) to merge the changes from a branch of the fork into a selected branch of upstream.\n  * Exercise caution when selecting the source and target branches for the PR.\n    Note that versioned releases of TensorRT OSS are posted to `release/` branches of the upstream repo.\n  * Creation of a PR creation kicks off the code review process.\n  * Atleast one TensorRT engineer will be assigned for the review.\n  * While under review, mark your PRs as work-in-progress by prefixing the PR title with [WIP].\n\n4. Since there is no CI/CD process in place yet, the PR will be accepted and the corresponding issue closed only after adequate testing has been completed, manually, by the developer and/or TensorRT engineer reviewing the code.\n\n\n#### Signing Your Work\n\n* We require that all contributors \"sign-off\" on their commits. This certifies that the contribution is your original work, or you have rights to submit it under the same license, or a compatible license.\n\n  * Any contribution which contains commits that are not Signed-Off will not be accepted.\n\n* To sign off on a commit you simply use the `--signoff` (or `-s`) option when committing your changes:\n  ```bash\n  $ git commit -s -m \"Add cool feature.\"\n  ```\n  This will append the following to your commit message:\n  ```\n  Signed-off-by: Your Name <your@email.com>\n  ```\n\n* Full text of the DCO:\n\n  ```\n    Developer Certificate of Origin\n    Version 1.1\n    \n    Copyright (C) 2004, 2006 The Linux Foundation and its contributors.\n    1 Letterman Drive\n    Suite D4700\n    San Francisco, CA, 94129\n    \n    Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n  ```\n\n  ```\n    Developer's Certificate of Origin 1.1\n    \n    By making a contribution to this project, I certify that:\n    \n    (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or\n    \n    (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or\n    \n    (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.\n    \n    (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.\n  ```\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 20.255859375,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   Copyright 2021 NVIDIA Corporation\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n\n   PORTIONS LICENSED AS FOLLOWS\n\n   > tools/pytorch-quantization/examples/torchvision/models/classification/resnet.py\n\n     BSD 3-Clause License\n\n     Copyright (c) Soumith Chintala 2016,\n     All rights reserved.\n\n     Redistribution and use in source and binary forms, with or without\n     modification, are permitted provided that the following conditions are met:\n\n     * Redistributions of source code must retain the above copyright notice, this\n       list of conditions and the following disclaimer.\n\n     * Redistributions in binary form must reproduce the above copyright notice,\n       this list of conditions and the following disclaimer in the documentation\n       and/or other materials provided with the distribution.\n\n     * Neither the name of the copyright holder nor the names of its\n       contributors may be used to endorse or promote products derived from\n       this software without specific prior written permission.\n\n     THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n     AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n     IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n     DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n     FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n     DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n     SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n     CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n     OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n     OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n   > samples/common/windows/getopt.c\n\n     Copyright (c) 2002 Todd C. Miller <Todd.Miller@courtesan.com>\n\n     Permission to use, copy, modify, and distribute this software for any\n     purpose with or without fee is hereby granted, provided that the above\n     copyright notice and this permission notice appear in all copies.\n\n     THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n     WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n     MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n     ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n     WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n     ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n     OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n     Sponsored in part by the Defense Advanced Research Projects\n     Agency (DARPA) and Air Force Research Laboratory, Air Force\n     Materiel Command, USAF, under agreement number F39502-99-1-0512.\n\n\n     Copyright (c) 2000 The NetBSD Foundation, Inc.\n     All rights reserved.\n\n     This code is derived from software contributed to The NetBSD Foundation\n     by Dieter Baron and Thomas Klausner.\n\n     Redistribution and use in source and binary forms, with or without\n     modification, are permitted provided that the following conditions\n     are met:\n     1. Redistributions of source code must retain the above copyright\n        notice, this list of conditions and the following disclaimer.\n     2. Redistributions in binary form must reproduce the above copyright\n        notice, this list of conditions and the following disclaimer in the\n        documentation and/or other materials provided with the distribution.\n\n     THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS\n     ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\n     TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n     PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS\n     BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n     CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n     SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n     INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n     CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n     ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n     POSSIBILITY OF SUCH DAMAGE.\n     - Copyright (c) 2002 Todd C. Miller <Todd.Miller@courtesan.com>\n     - Copyright (c) 2000 The NetBSD Foundation, Inc.\n\n\n   > parsers/common/ieee_half.h\n   > samples/common/half.h\n   > third_party/ieee/half.h\n\n     The MIT License\n\n     Copyright (c) 2012-2017 Christian Rau <rauy@users.sourceforge.net>\n\n     Permission is hereby granted, free of charge, to any person obtaining a\n     copy of this software and associated documentation files (the \"Software\"),\n     to deal in the Software without restriction, including without limitation\n     the rights to use, copy, modify, merge, publish, distribute, sublicense,\n     and/or sell copies of the Software, and to permit persons to whom the\n     Software is furnished to do so, subject to the following conditions:\n\n     The above copyright notice and this permission notice shall be included\n     in all copies or substantial portions of the Software.\n\n     THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n     IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n     FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n     THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n     LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n     DEALINGS IN THE SOFTWARE.\n\n   > plugin/multiscaleDeformableAttnPlugin/multiscaleDeformableAttn.cu\n   > plugin/multiscaleDeformableAttnPlugin/multiscaleDeformableAttn.h\n   > plugin/multiscaleDeformableAttnPlugin/multiscaleDeformableIm2ColCuda.cuh\n     \n     Copyright 2020 SenseTime\n\n     Licensed under the Apache License, Version 2.0 (the \"License\");\n     you may not use this file except in compliance with the License.\n     You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n     Unless required by applicable law or agreed to in writing, software\n     distributed under the License is distributed on an \"AS IS\" BASIS,\n     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n     See the License for the specific language governing permissions and\n     limitations under the License.\n\n     DETR\n\n     Copyright 2020 - present, Facebook, Inc\n\n     Licensed under the Apache License, Version 2.0 (the \"License\");\n     you may not use this file except in compliance with the License.\n     You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n     Unless required by applicable law or agreed to in writing, software\n     distributed under the License is distributed on an \"AS IS\" BASIS,\n     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n     See the License for the specific language governing permissions and\n     limitations under the License.\n\n   > demo/Diffusion/utilities.py\n   > demo/Diffusion/stable_video_diffusion_pipeline.py\n\n     HuggingFace diffusers library.\n\n     Copyright 2024 The HuggingFace Team.\n\n     Licensed under the Apache License, Version 2.0 (the \"License\");\n     you may not use this file except in compliance with the License.\n     You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n     Unless required by applicable law or agreed to in writing, software\n     distributed under the License is distributed on an \"AS IS\" BASIS,\n     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n     See the License for the specific language governing permissions and\n     limitations under the License.\n\n   > demo/Diffusion/utils_sd3/sd3_impls.py\n   > demo/Diffusion/utils_sd3/other_impls.py\n   > demo/Diffusion/utils_sd3/mmdit.py\n   > demo/Diffusion/stable_diffusion_3_pipeline.py\n\n      MIT License\n\n      Copyright (c) 2024 Stability AI\n\n      Permission is hereby granted, free of charge, to any person obtaining a copy\n      of this software and associated documentation files (the \"Software\"), to deal\n      in the Software without restriction, including without limitation the rights\n      to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n      copies of the Software, and to permit persons to whom the Software is\n      furnished to do so, subject to the following conditions:\n\n      The above copyright notice and this permission notice shall be included in all\n      copies or substantial portions of the Software.\n\n      THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n      IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n      FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n      AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n      LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n      OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n      SOFTWARE.\n\n   > demo/Diffusion/utilities.py\n\n      ModelScope library.\n\n      Copyright (c) Alibaba, Inc. and its affiliates.\n\n      Licensed under the Apache License, Version 2.0 (the \"License\");\n      you may not use this file except in compliance with the License.\n      You may obtain a copy of the License at\n\n         http://www.apache.org/licenses/LICENSE-2.0\n\n      Unless required by applicable law or agreed to in writing, software\n      distributed under the License is distributed on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n      See the License for the specific language governing permissions and\n      limitations under the License.\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 0.2890625,
          "content": "TensorRT Open Source Software\nCopyright (c) 2021, NVIDIA CORPORATION.\n\nThis product includes software developed at\nNVIDIA CORPORATION (https://www.nvidia.com/).\n\nThis software contains code derived by Soumith Chintala.\nBSD 3-Clause License (https://github.com/pytorch/vision/blob/master/LICENSE)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.2734375,
          "content": "[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) [![Documentation](https://img.shields.io/badge/TensorRT-documentation-brightgreen.svg)](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html) [![Roadmap](https://img.shields.io/badge/Roadmap-Q1_2025-brightgreen.svg)](documents/tensorrt_roadmap_2025q1.pdf)\n\n# TensorRT Open Source Software\nThis repository contains the Open Source Software (OSS) components of NVIDIA TensorRT. It includes the sources for TensorRT plugins and ONNX parser, as well as sample applications demonstrating usage and capabilities of the TensorRT platform. These open source software components are a subset of the TensorRT General Availability (GA) release with some extensions and bug-fixes.\n\n* For code contributions to TensorRT-OSS, please see our [Contribution Guide](CONTRIBUTING.md) and [Coding Guidelines](CODING-GUIDELINES.md).\n* For a summary of new additions and updates shipped with TensorRT-OSS releases, please refer to the [Changelog](CHANGELOG.md).\n* For business inquiries, please contact [researchinquiries@nvidia.com](mailto:researchinquiries@nvidia.com)\n* For press and other inquiries, please contact Hector Marinez at [hmarinez@nvidia.com](mailto:hmarinez@nvidia.com)\n\nNeed enterprise support? NVIDIA global support is available for TensorRT with the [NVIDIA AI Enterprise software suite](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/). Check out [NVIDIA LaunchPad](https://www.nvidia.com/en-us/launchpad/ai/ai-enterprise/) for free access to a set of hands-on labs with TensorRT hosted on NVIDIA infrastructure.\n\nJoin the [TensorRT and Triton community](https://www.nvidia.com/en-us/deep-learning-ai/triton-tensorrt-newsletter/) and stay current on the latest product updates, bug fixes, content, best practices, and more.\n\n# Prebuilt TensorRT Python Package\nWe provide the TensorRT Python package for an easy installation. \\\nTo install:\n```bash\npip install tensorrt\n```\nYou can skip the **Build** section to enjoy TensorRT with Python.\n\n# Build\n\n## Prerequisites\nTo build the TensorRT-OSS components, you will first need the following software packages.\n\n**TensorRT GA build**\n* TensorRT v10.7.0.23\n  * Available from direct download links listed below\n\n**System Packages**\n* [CUDA](https://developer.nvidia.com/cuda-toolkit)\n  * Recommended versions:\n  * cuda-12.6.0 + cuDNN-8.9\n  * cuda-11.8.0 + cuDNN-8.9\n* [GNU make](https://ftp.gnu.org/gnu/make/) >= v4.1\n* [cmake](https://github.com/Kitware/CMake/releases) >= v3.13\n* [python](<https://www.python.org/downloads/>) >= v3.8, <= v3.10.x\n* [pip](https://pypi.org/project/pip/#history) >= v19.0\n* Essential utilities\n  * [git](https://git-scm.com/downloads), [pkg-config](https://www.freedesktop.org/wiki/Software/pkg-config/), [wget](https://www.gnu.org/software/wget/faq.html#download)\n\n**Optional Packages**\n* Containerized build\n  * [Docker](https://docs.docker.com/install/) >= 19.03\n  * [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-docker)\n* PyPI packages (for demo applications/tests)\n  * [onnx](https://pypi.org/project/onnx/)\n  * [onnxruntime](https://pypi.org/project/onnxruntime/)\n  * [tensorflow-gpu](https://pypi.org/project/tensorflow/) >= 2.5.1\n  * [Pillow](https://pypi.org/project/Pillow/) >= 9.0.1\n  * [pycuda](https://pypi.org/project/pycuda/) < 2021.1\n  * [numpy](https://pypi.org/project/numpy/)\n  * [pytest](https://pypi.org/project/pytest/)\n* Code formatting tools (for contributors)\n  * [Clang-format](https://clang.llvm.org/docs/ClangFormat.html)\n  * [Git-clang-format](https://github.com/llvm-mirror/clang/blob/master/tools/clang-format/git-clang-format)\n\n  > NOTE: [onnx-tensorrt](https://github.com/onnx/onnx-tensorrt), [cub](http://nvlabs.github.io/cub/), and [protobuf](https://github.com/protocolbuffers/protobuf.git) packages are downloaded along with TensorRT OSS, and not required to be installed.\n\n## Downloading TensorRT Build\n\n1. #### Download TensorRT OSS\n\t```bash\n\tgit clone -b main https://github.com/nvidia/TensorRT TensorRT\n\tcd TensorRT\n\tgit submodule update --init --recursive\n\t```\n\n2. #### (Optional - if not using TensorRT container) Specify the TensorRT GA release build path\n\n    If using the TensorRT OSS build container, TensorRT libraries are preinstalled under `/usr/lib/x86_64-linux-gnu` and you may skip this step.\n\n    Else download and extract the TensorRT GA build from [NVIDIA Developer Zone](https://developer.nvidia.com) with the direct links below:\n      - [TensorRT 10.7.0.23 for CUDA 11.8, Linux x86_64](https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.7.0/tars/TensorRT-10.7.0.23.Linux.x86_64-gnu.cuda-11.8.tar.gz)\n      - [TensorRT 10.7.0.23 for CUDA 12.6, Linux x86_64](https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.7.0/tars/TensorRT-10.7.0.23.Linux.x86_64-gnu.cuda-12.6.tar.gz)\n      - [TensorRT 10.7.0.23 for CUDA 11.8, Windows x86_64](https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.7.0/zip/TensorRT-10.7.0.23.Windows.win10.cuda-11.8.zip)\n      - [TensorRT 10.7.0.23 for CUDA 12.6, Windows x86_64](https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.7.0/zip/TensorRT-10.7.0.23.Windows.win10.cuda-12.6.zip)\n\n\n    **Example: Ubuntu 20.04 on x86-64 with cuda-12.6**\n\n    ```bash\n    cd ~/Downloads\n    tar -xvzf TensorRT-10.7.0.23.Linux.x86_64-gnu.cuda-12.6.tar.gz\n    export TRT_LIBPATH=`pwd`/TensorRT-10.7.0.23\n    ```\n\n    **Example: Windows on x86-64 with cuda-12.6**\n\n    ```powershell\n    Expand-Archive -Path TensorRT-10.7.0.23.Windows.win10.cuda-12.6.zip\n    $env:TRT_LIBPATH=\"$pwd\\TensorRT-10.7.0.23\\lib\"\n    ```\n\n## Setting Up The Build Environment\n\nFor Linux platforms, we recommend that you generate a docker container for building TensorRT OSS as described below. For native builds, please install the [prerequisite](#prerequisites) *System Packages*.\n\n1. #### Generate the TensorRT-OSS build container.\n    The TensorRT-OSS build container can be generated using the supplied Dockerfiles and build scripts. The build containers are configured for building TensorRT OSS out-of-the-box.\n\n    **Example: Ubuntu 20.04 on x86-64 with cuda-12.6 (default)**\n    ```bash\n    ./docker/build.sh --file docker/ubuntu-20.04.Dockerfile --tag tensorrt-ubuntu20.04-cuda12.6\n    ```\n    **Example: Rockylinux8 on x86-64 with cuda-12.6**\n    ```bash\n    ./docker/build.sh --file docker/rockylinux8.Dockerfile --tag tensorrt-rockylinux8-cuda12.6\n    ```\n    **Example: Ubuntu 22.04 cross-compile for Jetson (aarch64) with cuda-12.6 (JetPack SDK)**\n    ```bash\n    ./docker/build.sh --file docker/ubuntu-cross-aarch64.Dockerfile --tag tensorrt-jetpack-cuda12.6\n    ```\n    **Example: Ubuntu 22.04 on aarch64 with cuda-12.6**\n    ```bash\n    ./docker/build.sh --file docker/ubuntu-22.04-aarch64.Dockerfile --tag tensorrt-aarch64-ubuntu22.04-cuda12.6\n    ```\n\n2. #### Launch the TensorRT-OSS build container.\n    **Example: Ubuntu 20.04 build container**\n\t```bash\n\t./docker/launch.sh --tag tensorrt-ubuntu20.04-cuda12.6 --gpus all\n\t```\n\t> NOTE:\n  <br> 1. Use the `--tag` corresponding to build container generated in Step 1.\n  <br> 2. [NVIDIA Container Toolkit](#prerequisites) is required for GPU access (running TensorRT applications) inside the build container.\n  <br> 3. `sudo` password for Ubuntu build containers is 'nvidia'.\n  <br> 4. Specify port number using `--jupyter <port>` for launching Jupyter notebooks.\n\n## Building TensorRT-OSS\n* Generate Makefiles and build.\n\n    **Example: Linux (x86-64) build with default cuda-12.6**\n\t```bash\n\tcd $TRT_OSSPATH\n\tmkdir -p build && cd build\n\tcmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out\n\tmake -j$(nproc)\n\t```\n    **Example: Linux (aarch64) build with default cuda-12.6**\n\t```bash\n\tcd $TRT_OSSPATH\n\tmkdir -p build && cd build\n\tcmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out -DCMAKE_TOOLCHAIN_FILE=$TRT_OSSPATH/cmake/toolchains/cmake_aarch64-native.toolchain\n\tmake -j$(nproc)\n\t```\n    **Example: Native build on Jetson (aarch64) with cuda-12.6**\n\t```bash\n\tcd $TRT_OSSPATH\n\tmkdir -p build && cd build\n\tcmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out -DTRT_PLATFORM_ID=aarch64 -DCUDA_VERSION=12.6\n  CC=/usr/bin/gcc make -j$(nproc)\n\t```\n  > NOTE: C compiler must be explicitly specified via CC= for native aarch64 builds of protobuf.\n\n    **Example: Ubuntu 22.04 Cross-Compile for Jetson (aarch64) with cuda-12.6 (JetPack)**\n\t```bash\n\tcd $TRT_OSSPATH\n\tmkdir -p build && cd build\n\tcmake .. -DCMAKE_TOOLCHAIN_FILE=$TRT_OSSPATH/cmake/toolchains/cmake_aarch64.toolchain -DCUDA_VERSION=12.6 -DCUDNN_LIB=/pdk_files/cudnn/usr/lib/aarch64-linux-gnu/libcudnn.so -DCUBLAS_LIB=/usr/local/cuda-12.6/targets/aarch64-linux/lib/stubs/libcublas.so -DCUBLASLT_LIB=/usr/local/cuda-12.6/targets/aarch64-linux/lib/stubs/libcublasLt.so -DTRT_LIB_DIR=/pdk_files/tensorrt/lib\n\tmake -j$(nproc)\n\t```\n\n    **Example: Native builds on Windows (x86) with cuda-12.6**\n\t```powershell\n\tcd $TRT_OSSPATH\n\tmkdir -p build\n\tcd -p build\n\tcmake .. -DTRT_LIB_DIR=\"$env:TRT_LIBPATH\" -DCUDNN_ROOT_DIR=\"$env:CUDNN_PATH\" -DTRT_OUT_DIR=\"$pwd\\\\out\"\n\tmsbuild TensorRT.sln /property:Configuration=Release -m:$env:NUMBER_OF_PROCESSORS\n\t```\n\n\t> NOTE:\n\t<br> 1. The default CUDA version used by CMake is 12.4.0. To override this, for example to 11.8, append `-DCUDA_VERSION=11.8` to the cmake command.\n* Required CMake build arguments are:\n\t- `TRT_LIB_DIR`: Path to the TensorRT installation directory containing libraries.\n\t- `TRT_OUT_DIR`: Output directory where generated build artifacts will be copied.\n* Optional CMake build arguments:\n\t- `CMAKE_BUILD_TYPE`: Specify if binaries generated are for release or debug (contain debug symbols). Values consists of [`Release`] | `Debug`\n\t- `CUDA_VERSION`: The version of CUDA to target, for example [`11.7.1`].\n\t- `CUDNN_VERSION`: The version of cuDNN to target, for example [`8.6`].\n\t- `PROTOBUF_VERSION`:  The version of Protobuf to use, for example [`3.0.0`]. Note: Changing this will not configure CMake to use a system version of Protobuf, it will configure CMake to download and try building that version.\n\t- `CMAKE_TOOLCHAIN_FILE`: The path to a toolchain file for cross compilation.\n\t- `BUILD_PARSERS`: Specify if the parsers should be built, for example [`ON`] | `OFF`.  If turned OFF, CMake will try to find precompiled versions of the parser libraries to use in compiling samples. First in `${TRT_LIB_DIR}`, then on the system. If the build type is Debug, then it will prefer debug builds of the libraries before release versions if available.\n\t- `BUILD_PLUGINS`: Specify if the plugins should be built, for example [`ON`] | `OFF`. If turned OFF, CMake will try to find a precompiled version of the plugin library to use in compiling samples. First in `${TRT_LIB_DIR}`, then on the system. If the build type is Debug, then it will prefer debug builds of the libraries before release versions if available.\n\t- `BUILD_SAMPLES`: Specify if the samples should be built, for example [`ON`] | `OFF`.\n\t- `GPU_ARCHS`: GPU (SM) architectures to target. By default we generate CUDA code for all major SMs. Specific SM versions can be specified here as a quoted space-separated list to reduce compilation time and binary size. Table of compute capabilities of NVIDIA GPUs can be found [here](https://developer.nvidia.com/cuda-gpus). Examples:\n        - NVidia A100: `-DGPU_ARCHS=\"80\"`\n        - Tesla T4, GeForce RTX 2080: `-DGPU_ARCHS=\"75\"`\n        - Titan V, Tesla V100: `-DGPU_ARCHS=\"70\"`\n        - Multiple SMs: `-DGPU_ARCHS=\"80 75\"`\n\t- `TRT_PLATFORM_ID`: Bare-metal build (unlike containerized cross-compilation). Currently supported options: `x86_64` (default).\n\n# References\n\n## TensorRT Resources\n\n* [TensorRT Developer Home](https://developer.nvidia.com/tensorrt)\n* [TensorRT QuickStart Guide](https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html)\n* [TensorRT Developer Guide](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html)\n* [TensorRT Sample Support Guide](https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html)\n* [TensorRT ONNX Tools](https://docs.nvidia.com/deeplearning/tensorrt/index.html#tools)\n* [TensorRT Discussion Forums](https://devtalk.nvidia.com/default/board/304/tensorrt/)\n* [TensorRT Release Notes](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html)\n\n## Known Issues\n\n* Please refer to [TensorRT Release Notes](https://docs.nvidia.com/deeplearning/tensorrt/release-notes)\n"
        },
        {
          "name": "VERSION",
          "type": "blob",
          "size": 0.009765625,
          "content": "10.7.0.23\n"
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "documents",
          "type": "tree",
          "content": null
        },
        {
          "name": "include",
          "type": "tree",
          "content": null
        },
        {
          "name": "parsers",
          "type": "tree",
          "content": null
        },
        {
          "name": "plugin",
          "type": "tree",
          "content": null
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "quickstart",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.828125,
          "content": "onnx>=1.14.0\ntensorflow[and-cuda]==2.12.0; (platform_machine==\"x86_64\" and sys.platform==\"linux\" and python_version>=\"3.7\")\nonnxruntime==1.8.1; python_version<\"3.10\"\nonnxruntime==1.12.1; python_version==\"3.10\"\n-f https://download.pytorch.org/whl/cu113/torch_stable.html\ntorch==1.10.2+cu113; python_version<\"3.10\" and platform_machine==\"x86_64\" and sys.platform==\"linux\"\ntorch==1.10.2; python_version<\"3.10\" and platform_machine==\"aarch64\" and sys.platform==\"linux\"\ntorch==1.11.0+cu113; python_version==\"3.10\" and platform_machine==\"x86_64\" and sys.platform==\"linux\"\ntorch==1.11.0; python_version==\"3.10\" and platform_machine==\"aarch64\" and sys.platform==\"linux\"\ntorchvision==0.11.3; python_version<\"3.10\"\ntorchvision==0.12.0; python_version==\"3.10\"\nPillow\nnumpy\npycuda==2024.1\npytest\n--extra-index-url https://pypi.ngc.nvidia.com\nonnx-graphsurgeon\n"
        },
        {
          "name": "samples",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}