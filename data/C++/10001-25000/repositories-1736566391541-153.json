{
  "metadata": {
    "timestamp": 1736566391541,
    "page": 153,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "google/sentencepiece",
      "stars": 10460,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.775390625,
          "content": "Makefile\nMakefile.in\n/ar-lib\n/mdate-sh\n/py-compile\n/test-driver\n/ylwrap\n/build\n\n/autom4te.cache\n/autoscan.log\n/autoscan-*.log\n/aclocal.m4\n/compile\n/config.guess\n/config.sub\n/configure\n/configure.scan\n/depcomp\n/install-sh\n/missing\n/stamp-h1\n/libtool\n/config.h\n/config.status\n/autogen.sh\n/ltmain.sh\n\nCMakeFiles\nCMakeCache.txt\nconfig.h\nsentencepiece.pc\nCPackConfig.cmake\nCTestTestfile.cmake\nCPackSourceConfig.cmake\nDartConfiguration.tcl\n\n*.o\n*.lo\n*.a\n*.la\n*.pyc\n\n.libs\n.deps\n\n*.m4\n*.log\n*.trs\n\ncompile_charsmap\n\nspm_decode\nspm_encode\nspm_export_vocab\nspm_train\nspm_normalize\nspm_test\n\n.DS_Store\n*.egg-info/\ndist/\n*.swp\n*.swo\n*.pyc\n\nm.model\nm.vocab\n\ncmake_install.cmake\nlibsentencepiece.so*\nlibsentencepiece_train.so*\npython/bundled\n_sentencepiece.*.so\nthird_party/abseil-cpp\n\npython/sentencepiece\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 7.2490234375,
          "content": "# Copyright 2018 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.!\n\ncmake_minimum_required(VERSION 3.5 FATAL_ERROR)\nfile(STRINGS \"VERSION.txt\" SPM_VERSION)\nmessage(STATUS \"VERSION: ${SPM_VERSION}\")\n\nif(POLICY CMP0091)\n  cmake_policy(SET CMP0091 NEW)\nendif()\n\nproject(sentencepiece VERSION ${SPM_VERSION} LANGUAGES C CXX)\n\noption(SPM_ENABLE_NFKC_COMPILE \"Enables NFKC compile\" OFF)\noption(SPM_ENABLE_SHARED \"Builds shared libaries in addition to static libraries.\" ON)\noption(SPM_BUILD_TEST \"Builds test binaries.\" OFF)\noption(SPM_COVERAGE \"Runs gcov to test coverage.\" OFF)\noption(SPM_ENABLE_TENSORFLOW_SHARED \"Makes a tensorflow compatible shared file.\" OFF)\noption(SPM_ENABLE_TCMALLOC \"Enable TCMalloc if available.\" ON)\noption(SPM_TCMALLOC_STATIC \"Link static library of TCMALLOC.\" OFF)\noption(SPM_NO_THREADLOCAL \"Disable thread_local operator\" OFF)\noption(SPM_ENABLE_MSVC_MT_BUILD, \"Use /MT flag in MSVC build\" OFF)\noption(SPM_CROSS_SYSTEM_PROCESSOR, \"Override system processor\" \"\")\n\nset(SPM_PROTOBUF_PROVIDER \"internal\" CACHE STRING \"Provider of protobuf library\")\nset_property(CACHE SPM_PROTOBUF_PROVIDER PROPERTY STRINGS \"internal\" \"package\")\nset(SPM_ABSL_PROVIDER \"internal\" CACHE STRING \"Provider of absl library\")\nset_property(CACHE SPM_ABSL_PROVIDER PROPERTY STRINGS \"internal\" \"module\" \"package\")\n\nif (SPM_CROSS_SYSTEM_PROCESSOR)\n set(CMAKE_SYSTEM_PROCESSOR ${SPM_CROSS_SYSTEM_PROCESSOR})\nendif()\n\n# Disable shared build on windows\nif(WIN32)\n  set(SPM_ENABLE_SHARED OFF)\nendif()\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\nif((CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\" AND\n     CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 10.0) OR\n   (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" AND\n     CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 8.0))\n  string(APPEND CMAKE_CXX_FLAGS \" -fmacro-prefix-map=${CMAKE_SOURCE_DIR}/=''\")\nendif()\n\nif (NOT DEFINED CMAKE_INSTALL_BINDIR)\n  set(CMAKE_INSTALL_BINDIR bin)\nendif()\n\nif (NOT DEFINED CMAKE_INSTALL_LIBDIR)\n  set(CMAKE_INSTALL_LIBDIR lib)\nendif()\n\nif (NOT DEFINED CMAKE_INSTALL_INCLUDEDIR)\n  set(CMAKE_INSTALL_INCLUDEDIR include)\nendif()\n\nif (UNIX)\n  include(GNUInstallDirs)\n  set(prefix ${CMAKE_INSTALL_PREFIX})\n  set(exec_prefix \"\\${prefix}\")\n  set(libdir \"\\${exec_prefix}/${CMAKE_INSTALL_LIBDIR}\")\n  set(includedir \"\\${prefix}/${CMAKE_INSTALL_INCLUDEDIR}\")\nelse()\n  set(prefix ${CMAKE_INSTALL_PREFIX})\n  set(exec_prefix \"\\${prefix}\")\n  set(libdir \"\\${exec_prefix}/lib\")\n  set(includedir \"\\${prefix}/include\")\nendif()\nset(GNUCXX_STD_SUPPORT_VERSION \"4.3\")\n\nif(${CMAKE_SYSTEM_NAME} STREQUAL \"FreeBSD\")\nadd_definitions(-D_FREEBSD)\nendif()\n\nif (SPM_USE_BUILTIN_PROTOBUF)\n  set(libprotobuf_lite \"\")\nelse()\n  set(libprotobuf_lite \"protobuf-lite\")\nendif()\n\nif (MSVC)\n  add_definitions(\"/wd4267 /wd4244 /wd4305 /Zc:strictStrings /utf-8\")\n  if (SPM_ENABLE_MSVC_MT_BUILD)\n    string(REPLACE \"/MD\" \"/MT\" CMAKE_CXX_FLAGS_DEBUG          ${CMAKE_CXX_FLAGS_DEBUG})\n    string(REPLACE \"/MD\" \"/MT\" CMAKE_CXX_FLAGS_MINSIZEREL     ${CMAKE_CXX_FLAGS_MINSIZEREL})\n    string(REPLACE \"/MD\" \"/MT\" CMAKE_CXX_FLAGS_RELEASE        ${CMAKE_CXX_FLAGS_RELEASE})\n    string(REPLACE \"/MD\" \"/MT\" CMAKE_CXX_FLAGS_RELWITHDEBINFO ${CMAKE_CXX_FLAGS_RELWITHDEBINFO})\n  endif()\nendif()\n\nif (APPLE)\n  set(CMAKE_MACOSX_RPATH ON)\n  set(CMAKE_SKIP_BUILD_RPATH FALSE)\n  set(CMAKE_BUILD_WITH_INSTALL_RPATH FALSE)\n  set(CMAKE_INSTALL_RPATH \"${CMAKE_INSTALL_PREFIX}/lib\")\n  set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)\n  list(FIND CMAKE_PLATFORM_IMPLICIT_LINK_DIRECTORIES \"${CMAKE_INSTALL_PREFIX}/lib\" isSystemDir)\n  if (\"${isSystemDir}\" STREQUAL \"-1\")\n    set(CMAKE_INSTALL_RPATH \"${CMAKE_INSTALL_PREFIX}/lib\")\n  endif()\nendif()\n\n# SPDX-License-Identifier: (MIT OR CC0-1.0)\n# Copyright 2020 Jan Tojnar\n# https://github.com/jtojnar/cmake-snips\n#\n# Modelled after Python’s os.path.join\n# https://docs.python.org/3.7/library/os.path.html#os.path.join\n# Windows not supported\nfunction(join_paths joined_path first_path_segment)\n    set(temp_path \"${first_path_segment}\")\n    foreach(current_segment IN LISTS ARGN)\n        if(NOT (\"${current_segment}\" STREQUAL \"\"))\n            if(IS_ABSOLUTE \"${current_segment}\")\n                set(temp_path \"${current_segment}\")\n            else()\n                set(temp_path \"${temp_path}/${current_segment}\")\n            endif()\n        endif()\n    endforeach()\n    set(${joined_path} \"${temp_path}\" PARENT_SCOPE)\nendfunction()\n\njoin_paths(libdir_for_pc_file \"\\${exec_prefix}\" \"${CMAKE_INSTALL_LIBDIR}\")\njoin_paths(includedir_for_pc_file \"\\${prefix}\" \"${CMAKE_INSTALL_INCLUDEDIR}\")\n\nconfigure_file(\"${PROJECT_SOURCE_DIR}/config.h.in\" \"config.h\")\nconfigure_file(\"${PROJECT_SOURCE_DIR}/sentencepiece.pc.in\" \"sentencepiece.pc\" @ONLY)\n\nif (NOT MSVC)\n  # suppress warning for C++11 features.\n#  add_definitions(\"-Wno-deprecated-declarations -Wno-deprecated-enum-enum-conversion\")\n  install(FILES \"${CMAKE_CURRENT_BINARY_DIR}/sentencepiece.pc\" DESTINATION ${CMAKE_INSTALL_LIBDIR}/pkgconfig)\nendif()\n\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR} ${PROJECT_BINARY_DIR})\n\nif (SPM_BUILD_TEST)\n  enable_testing()\nendif()\n\nif (SPM_ABSL_PROVIDER STREQUAL \"internal\")\n  include_directories(${CMAKE_CURRENT_SOURCE_DIR}/third_party/absl)\nelseif (SPM_ABSL_PROVIDER STREQUAL \"module\")\n  include(FetchContent)\n  FetchContent_Populate(abseil-cpp\n        GIT_REPOSITORY  https://github.com/abseil/abseil-cpp.git\n        SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/third_party/abseil-cpp\n        GIT_PROGRESS TRUE)\n  add_subdirectory(third_party/abseil-cpp)\n  if (NOT EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/third_party/absl.org)\n    file(RENAME ${CMAKE_CURRENT_SOURCE_DIR}/third_party/absl ${CMAKE_CURRENT_SOURCE_DIR}/third_party/absl.org)\n    execute_process(COMMAND ${CMAKE_COMMAND} -E create_symlink\n      ${CMAKE_CURRENT_SOURCE_DIR}/third_party/abseil-cpp/absl\n      ${CMAKE_CURRENT_SOURCE_DIR}/third_party/absl)\n  endif()\nelseif (SPM_ABSL_PROVIDER STREQUAL \"package\")\n  find_package(absl REQUIRED)\n  get_target_property(ABSL_INCLUDE_DIRS absl::base INTERFACE_INCLUDE_DIRECTORIES)\n  if (NOT EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/third_party/absl.org)\n    file(RENAME ${CMAKE_CURRENT_SOURCE_DIR}/third_party/absl ${CMAKE_CURRENT_SOURCE_DIR}/third_party/absl.org)\n    execute_process(COMMAND ${CMAKE_COMMAND} -E create_symlink\n        ${ABSL_INCLUDE_DIRS}/absl ${CMAKE_CURRENT_SOURCE_DIR}/third_party/absl)\n  endif()\n  include_directories(${ABSL_INCLUDE_DIRS})\nendif()\n\nadd_subdirectory(src)\nadd_subdirectory(third_party)\n\nset(CPACK_SOURCE_GENERATOR \"TXZ\")\nset(CPACK_GENERATOR \"7Z\")\nset(CPACK_PACKAGE_VERSION \"${SPM_VERSION}\")\nset(CPACK_STRIP_FILES TRUE)\nset(CPACK_RESOURCE_FILE_LICENSE \"${PROJECT_SOURCE_DIR}/LICENSE\")\nset(CPACK_RESOURCE_FILE_README \"${PROJECT_SOURCE_DIR}/README.md\")\nset(CPACK_PACKAGE_CONTACT \"taku@google.com\")\nset(CPACK_DEBIAN_PACKAGE_MAINTAINER \"Taku Kudo\")\nset(CPACK_SOURCE_IGNORE_FILES \"/build/;/.git/;/dist/;/sdist/;~$;${CPACK_SOURCE_IGNORE_FILES}\")\ninclude(CPack)\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.416015625,
          "content": "Want to contribute? Great! First, read this page (including the small print at the end).\n\n### Before you contribute\nBefore we can use your code, you must sign the\n[Google Individual Contributor License Agreement](https://cla.developers.google.com/about/google-individual)\n(CLA), which you can do online. The CLA is necessary mainly because you own the\ncopyright to your changes even after your contribution becomes part of our\ncodebase, so we need your permission to use and distribute your code. We also\nneed to be sure of various other things—for instance, that you'll tell us if you\nknow that your code infringes on other people's patents. You don't have to sign\nthe CLA until after you've submitted your code for review and a member has\napproved it, but you must do it before we can put your code into our codebase.\nBefore you start working on a larger contribution, you should get in touch with\nus first through the issue tracker with your idea so that we can help out and\npossibly guide you. Coordinating up-front makes it much easier to avoid\nfrustration later on.\n\n### Code reviews\nAll submissions, including submissions by project members, require review. We\nuse Github pull requests for this purpose.\n\n### The small print\nContributions made by corporations are covered by a different agreement than\nthe one above, the [Software Grant and Corporate Contributor License Agreement](https://cla.developers.google.com/about/google-corporate).\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.5029296875,
          "content": "# SentencePiece\n\n[![Build C++](https://github.com/google/sentencepiece/actions/workflows/cmake.yml/badge.svg)](https://github.com/google/sentencepiece/actions/workflows/cmake.yml)\n[![Build Wheels](https://github.com/google/sentencepiece/actions/workflows/wheel.yml/badge.svg)](https://github.com/google/sentencepiece/actions/workflows/wheel.yml)\n[![GitHub Issues](https://img.shields.io/github/issues/google/sentencepiece.svg)](https://github.com/google/sentencepiece/issues)\n[![PyPI version](https://badge.fury.io/py/sentencepiece.svg)](https://badge.fury.io/py/sentencepiece)\n[![PyPi downloads](https://img.shields.io/pypi/dm/sentencepiece?style=flat-square&logo=pypi&logoColor=white)](https://pypi.org/project/sentencepiece/)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![License](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://opensource.org/licenses/Apache-2.0)\n[![SLSA 3](https://slsa.dev/images/gh-badge-level3.svg)](https://slsa.dev)\n\nSentencePiece is an unsupervised text tokenizer and detokenizer mainly for\nNeural Network-based text generation systems where the vocabulary size\nis predetermined prior to the neural model training. SentencePiece implements\n**subword units** (e.g., **byte-pair-encoding (BPE)** [[Sennrich et al.](https://www.aclweb.org/anthology/P16-1162)]) and\n**unigram language model** [[Kudo.](https://arxiv.org/abs/1804.10959)])\nwith the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\n\n**This is not an official Google product.**\n\n## Technical highlights\n- **Purely data driven**: SentencePiece trains tokenization and detokenization\n  models from sentences. Pre-tokenization ([Moses tokenizer](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl)/[MeCab](http://taku910.github.io/mecab/)/[KyTea](http://www.phontron.com/kytea/)) is not always required.\n- **Language independent**: SentencePiece treats the sentences just as sequences of Unicode characters. There is no language-dependent logic.\n- **Multiple subword algorithms**: **BPE**  [[Sennrich et al.](https://www.aclweb.org/anthology/P16-1162)] and **unigram language model** [[Kudo.](https://arxiv.org/abs/1804.10959)] are supported.\n- **Subword regularization**: SentencePiece implements subword sampling for [subword regularization](https://arxiv.org/abs/1804.10959) and [BPE-dropout](https://arxiv.org/abs/1910.13267) which help to improve the robustness and accuracy of NMT models.\n- **Fast and lightweight**: Segmentation speed is around 50k sentences/sec, and memory footprint is around 6MB.\n- **Self-contained**: The same tokenization/detokenization is obtained as long as the same model file is used.\n- **Direct vocabulary id generation**: SentencePiece manages vocabulary to id mapping and can directly generate vocabulary id sequences from raw sentences.\n- **NFKC-based normalization**: SentencePiece performs NFKC-based text normalization.\n\nFor those unfamiliar with SentencePiece as a software/algorithm, one can read [a gentle introduction here](https://medium.com/@jacky2wong/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08).\n\n\n## Comparisons with other implementations\n|Feature|SentencePiece|[subword-nmt](https://github.com/rsennrich/subword-nmt)|[WordPiece](https://arxiv.org/pdf/1609.08144.pdf)|\n|:---|:---:|:---:|:---:|\n|Supported algorithm|BPE, unigram, char, word|BPE|BPE*|\n|OSS?|Yes|Yes|Google internal|\n|Subword regularization|[Yes](#subword-regularization-and-bpe-dropout)|No|No|\n|Python Library (pip)|[Yes](python/README.md)|No|N/A|\n|C++ Library|[Yes](doc/api.md)|No|N/A|\n|Pre-segmentation required?|[No](#whitespace-is-treated-as-a-basic-symbol)|Yes|Yes|\n|Customizable normalization (e.g., NFKC)|[Yes](doc/normalization.md)|No|N/A|\n|Direct id generation|[Yes](#end-to-end-example)|No|N/A|\n\nNote that BPE algorithm used in WordPiece is slightly different from the original BPE.\n\n## Overview\n### What is SentencePiece?\nSentencePiece is a re-implementation of **sub-word units**, an effective way to alleviate the open vocabulary\n  problems in neural machine translation. SentencePiece supports two segmentation algorithms, **byte-pair-encoding (BPE)** [[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)] and **unigram language model** [[Kudo.](https://arxiv.org/abs/1804.10959)]. Here are the high level differences from other implementations.\n\n#### The number of unique tokens is predetermined\nNeural Machine Translation models typically operate with a fixed\nvocabulary. Unlike most unsupervised word segmentation algorithms, which\nassume an infinite vocabulary, SentencePiece trains the segmentation model such\nthat the final vocabulary size is fixed, e.g., 8k, 16k, or 32k.\n\nNote that SentencePiece specifies the final vocabulary size for training, which is different from\n[subword-nmt](https://github.com/rsennrich/subword-nmt) that uses the number of merge operations.\nThe number of merge operations is a BPE-specific parameter and not applicable to other segmentation algorithms, including unigram, word and character.\n\n#### Trains from raw sentences\nPrevious sub-word implementations assume that the input sentences are pre-tokenized. This constraint was required for efficient training, but makes the preprocessing complicated as we have to run language dependent tokenizers in advance.\nThe implementation of SentencePiece is fast enough to train the model from raw sentences. This is useful for training the tokenizer and detokenizer for Chinese and Japanese where no explicit spaces exist between words.\n\n#### Whitespace is treated as a basic symbol\nThe first step of Natural Language processing is text tokenization. For\nexample, a standard English tokenizer would segment the text \"Hello world.\" into the\nfollowing three tokens.\n\n> [Hello] [World] [.]\n\nOne observation is that the original input and tokenized sequence are **NOT\nreversibly convertible**. For instance, the information that is no space between\n“World” and “.” is dropped from the tokenized sequence, since e.g., `Tokenize(“World.”) == Tokenize(“World .”)`\n\nSentencePiece treats the input text just as a sequence of Unicode characters. Whitespace is also handled as a normal symbol. To handle the whitespace as a basic token explicitly, SentencePiece first escapes the whitespace with a meta symbol \"▁\" (U+2581) as follows.\n\n> Hello▁World.\n\nThen, this text is segmented into small pieces, for example:\n\n> [Hello] [▁Wor] [ld] [.]\n\nSince the whitespace is preserved in the segmented text, we can detokenize the text without any ambiguities.\n\n```\n  detokenized = ''.join(pieces).replace('▁', ' ')\n```\n\nThis feature makes it possible to perform detokenization without relying on language-specific resources.\n\nNote that we cannot apply the same lossless conversions when splitting the\nsentence with standard word segmenters, since they treat the whitespace as a\nspecial symbol. Tokenized sequences do not preserve the necessary information to restore the original sentence.\n\n* (en) Hello world.   → [Hello] [World] [.]   \\(A space between Hello and World\\)\n* (ja) こんにちは世界。  → [こんにちは] [世界] [。] \\(No space between こんにちは and 世界\\)\n\n#### Subword regularization and BPE-dropout\nSubword regularization [[Kudo.](https://arxiv.org/abs/1804.10959)] and BPE-dropout [Provilkov et al](https://arxiv.org/abs/1910.13267) are simple regularization methods\nthat virtually augment training data with on-the-fly subword sampling, which helps to improve the accuracy as well as robustness of NMT models.\n\nTo enable subword regularization, you would like to integrate SentencePiece library\n([C++](doc/api.md#sampling-subword-regularization)/[Python](python/README.md)) into the NMT system to sample one segmentation for each parameter update, which is different from the standard off-line data preparations. Here's the example of [Python library](python/README.md). You can find that 'New York' is segmented differently on each ``SampleEncode (C++)`` or ``encode with enable_sampling=True (Python)`` calls. The details of sampling parameters are found in [sentencepiece_processor.h](src/sentencepiece_processor.h).\n\n```\n>>> import sentencepiece as spm\n>>> s = spm.SentencePieceProcessor(model_file='spm.model')\n>>> for n in range(5):\n...     s.encode('New York', out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)\n...\n['▁', 'N', 'e', 'w', '▁York']\n['▁', 'New', '▁York']\n['▁', 'New', '▁Y', 'o', 'r', 'k']\n['▁', 'New', '▁York']\n['▁', 'New', '▁York']\n```\n\n## Installation\n\n### Python module\nSentencePiece provides Python wrapper that supports both SentencePiece training and segmentation.\nYou can install Python binary package of SentencePiece with.\n\n```\npip install sentencepiece\n```\n\nFor more detail, see [Python module](python/README.md)\n\n### Build and install SentencePiece command line tools from C++ source\nThe following tools and libraries are required to build SentencePiece:\n\n* [cmake](https://cmake.org/)\n* C++11 compiler\n* [gperftools](https://github.com/gperftools/gperftools) library (optional, 10-40% performance improvement can be obtained.)\n\nOn Ubuntu, the build tools can be installed with apt-get:\n```\n% sudo apt-get install cmake build-essential pkg-config libgoogle-perftools-dev\n```\n\nThen, you can build and install command line tools as follows.\n```\n% git clone https://github.com/google/sentencepiece.git \n% cd sentencepiece\n% mkdir build\n% cd build\n% cmake ..\n% make -j $(nproc)\n% sudo make install\n% sudo ldconfig -v\n```\nOn OSX/macOS, replace the last command with `sudo update_dyld_shared_cache`\n\n### Build and install using vcpkg\n\nYou can download and install sentencepiece using the [vcpkg](https://github.com/Microsoft/vcpkg) dependency manager:\n\n    git clone https://github.com/Microsoft/vcpkg.git\n    cd vcpkg\n    ./bootstrap-vcpkg.sh\n    ./vcpkg integrate install\n    ./vcpkg install sentencepiece\n\nThe sentencepiece port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.\n\n### Download and install SentencePiece from signed released wheels\n\nYou can download the wheel from the [GitHub releases page](https://github.com/google/sentencepiece/releases/latest).\nWe generate [SLSA3 signatures](slsa.dev) using the OpenSSF's [slsa-framework/slsa-github-generator](https://github.com/slsa-framework/slsa-github-generator) during the release process. To verify a release binary:\n1. Install the verification tool from [slsa-framework/slsa-verifier#installation](https://github.com/slsa-framework/slsa-verifier#installation).\n2. Download the provenance file `attestation.intoto.jsonl` from the [GitHub releases page](https://github.com/google/sentencepiece/releases/latest).\n3. Run the verifier:\n```shell\nslsa-verifier -artifact-path <the-wheel> -provenance attestation.intoto.jsonl -source github.com/google/sentencepiece -tag <the-tag>\n```\n\npip install wheel_file.whl\n\n## Usage instructions\n### Train SentencePiece Model\n```\n% spm_train --input=<input> --model_prefix=<model_name> --vocab_size=8000 --character_coverage=1.0 --model_type=<type>\n```\n* `--input`: one-sentence-per-line **raw** corpus file. No need to run\n  tokenizer, normalizer or preprocessor. By default, SentencePiece normalizes\n  the input with Unicode NFKC. You can pass a comma-separated list of files.\n* `--model_prefix`: output model name prefix. `<model_name>.model` and `<model_name>.vocab` are generated.\n* `--vocab_size`: vocabulary size, e.g., 8000, 16000, or 32000\n* `--character_coverage`: amount of characters covered by the model, good defaults are: `0.9995` for languages with rich character set like Japanese or Chinese and `1.0` for other languages with small character set.\n* `--model_type`: model type. Choose from `unigram` (default), `bpe`, `char`, or `word`. The input sentence must be pretokenized when using `word` type.\n\nUse `--help` flag to display all parameters for training, or see [here](doc/options.md) for an overview.\n\n### Encode raw text into sentence pieces/ids\n```\n% spm_encode --model=<model_file> --output_format=piece < input > output\n% spm_encode --model=<model_file> --output_format=id < input > output\n```\n\nUse `--extra_options` flag to insert the BOS/EOS markers or reverse the input sequence.\n```\n% spm_encode --extra_options=eos (add </s> only)\n% spm_encode --extra_options=bos:eos (add <s> and </s>)\n% spm_encode --extra_options=reverse:bos:eos (reverse input and add <s> and </s>)\n```\n\nSentencePiece supports nbest segmentation and segmentation sampling with `--output_format=(nbest|sample)_(piece|id)` flags.\n```\n% spm_encode --model=<model_file> --output_format=sample_piece --nbest_size=-1 --alpha=0.5 < input > output\n% spm_encode --model=<model_file> --output_format=nbest_id --nbest_size=10 < input > output\n```\n\n### Decode sentence pieces/ids into raw text\n```\n% spm_decode --model=<model_file> --input_format=piece < input > output\n% spm_decode --model=<model_file> --input_format=id < input > output\n```\nUse `--extra_options` flag to decode the text in reverse order.\n```\n% spm_decode --extra_options=reverse < input > output\n```\n\n### End-to-End Example\n```\n% spm_train --input=data/botchan.txt --model_prefix=m --vocab_size=1000\nunigram_model_trainer.cc(494) LOG(INFO) Starts training with :\ninput: \"../data/botchan.txt\"\n... <snip>\nunigram_model_trainer.cc(529) LOG(INFO) EM sub_iter=1 size=1100 obj=10.4973 num_tokens=37630 num_tokens/piece=34.2091\ntrainer_interface.cc(272) LOG(INFO) Saving model: m.model\ntrainer_interface.cc(281) LOG(INFO) Saving vocabs: m.vocab\n\n% echo \"I saw a girl with a telescope.\" | spm_encode --model=m.model\n▁I ▁saw ▁a ▁girl ▁with ▁a ▁ te le s c o pe .\n\n% echo \"I saw a girl with a telescope.\" | spm_encode --model=m.model --output_format=id\n9 459 11 939 44 11 4 142 82 8 28 21 132 6\n\n% echo \"9 459 11 939 44 11 4 142 82 8 28 21 132 6\" | spm_decode --model=m.model --input_format=id\nI saw a girl with a telescope.\n```\nYou can find that the original input sentence is restored from the vocabulary id sequence.\n\n### Export vocabulary list\n```\n% spm_export_vocab --model=<model_file> --output=<output file>\n```\n```<output file>``` stores a list of vocabulary and emission log probabilities. The vocabulary id corresponds to the line number in this file.\n\n### Redefine special meta tokens\n  By default, SentencePiece uses Unknown (&lt;unk&gt;), BOS (&lt;s&gt;) and EOS (&lt;/s&gt;) tokens which have the ids of 0, 1, and 2 respectively. We can redefine this mapping in the training phase as follows.\n\n```\n% spm_train --bos_id=0 --eos_id=1 --unk_id=5 --input=... --model_prefix=... --character_coverage=...\n```\nWhen setting -1 id e.g., ```bos_id=-1```, this special token is disabled. Note that the unknown id cannot be disabled.  We can define an id for padding (&lt;pad&gt;) as ```--pad_id=3```.  \n\nIf you want to assign another special tokens, please see [Use custom symbols](doc/special_symbols.md).\n\n### Vocabulary restriction\n```spm_encode``` accepts a ```--vocabulary``` and a ```--vocabulary_threshold``` option so that ```spm_encode``` will only produce symbols which also appear in the vocabulary (with at least some frequency). The background of this feature is described in [subword-nmt page](https://github.com/rsennrich/subword-nmt#best-practice-advice-for-byte-pair-encoding-in-nmt).\n\nThe usage is basically the same as that of ```subword-nmt```. Assuming that L1 and L2 are the two languages (source/target languages), train the shared spm model, and get resulting vocabulary for each:\n\n```\n% cat {train_file}.L1 {train_file}.L2 | shuffle > train\n% spm_train --input=train --model_prefix=spm --vocab_size=8000 --character_coverage=0.9995\n% spm_encode --model=spm.model --generate_vocabulary < {train_file}.L1 > {vocab_file}.L1\n% spm_encode --model=spm.model --generate_vocabulary < {train_file}.L2 > {vocab_file}.L2\n```\n\n```shuffle``` command is used just in case because ```spm_train``` loads the first 10M lines of corpus by default.\n\n\nThen segment train/test corpus with ```--vocabulary``` option\n```\n% spm_encode --model=spm.model --vocabulary={vocab_file}.L1 --vocabulary_threshold=50 < {test_file}.L1 > {test_file}.seg.L1\n% spm_encode --model=spm.model --vocabulary={vocab_file}.L2 --vocabulary_threshold=50 < {test_file}.L2 > {test_file}.seg.L2\n```\n\n## Advanced topics\n\n* [SentencePiece Experiments](doc/experiments.md)\n* [SentencePieceProcessor C++ API](doc/api.md)\n* [Use custom text normalization rules](doc/normalization.md)\n* [Use custom symbols](doc/special_symbols.md)\n* [Python Module](python/README.md)\n* [Segmentation and training algorithms in detail]\n\n"
        },
        {
          "name": "VERSION.txt",
          "type": "blob",
          "size": 0.005859375,
          "content": "0.2.1\n"
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "config.h.in",
          "type": "blob",
          "size": 0.1650390625,
          "content": "#ifndef CONFIG_H_\n#define CONFIG_H_\n\n#define VERSION \"@PROJECT_VERSION@\"\n#define PACKAGE \"@PROJECT_NAME@\"\n#define PACKAGE_STRING \"@PROJECT_NAME@\"\n\n\n#endif  // CONFIG_H_\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "sentencepiece.pc.in",
          "type": "blob",
          "size": 0.3623046875,
          "content": "prefix=@prefix@\nexec_prefix=@exec_prefix@\nlibdir=@libdir_for_pc_file@\nincludedir=@includedir_for_pc_file@\n\nName: @PROJECT_NAME@\nDescription: Unsupervised text tokenizer and detokenizer for Neural Network-based text generation.\nVersion: @PROJECT_VERSION@\nLibs: -L${libdir} -lsentencepiece -lsentencepiece_train\nCflags: -I${includedir}\nRequires.private: @libprotobuf_lite@\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}