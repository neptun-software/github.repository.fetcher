{
  "metadata": {
    "timestamp": 1736566333746,
    "page": 85,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "scylladb/scylladb",
      "stars": 13940,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 5.357421875,
          "content": "---\nLanguage: Cpp\nAccessModifierOffset: -4\nAlignAfterOpenBracket: DontAlign\nAlignArrayOfStructures: None\nAlignConsecutiveAssignments:\n  Enabled: false\n  AcrossEmptyLines: false\n  AcrossComments: false\n  AlignCompound: false\n  PadOperators: true\nAlignConsecutiveBitFields:\n  Enabled: false\n  AcrossEmptyLines: false\n  AcrossComments: false\n  AlignCompound: false\n  PadOperators: false\nAlignConsecutiveDeclarations:\n  Enabled: false\n  AcrossEmptyLines: false\n  AcrossComments: false\n  AlignCompound: false\n  PadOperators: false\nAlignConsecutiveMacros:\n  Enabled: false\n  AcrossEmptyLines: false\n  AcrossComments: false\n  AlignCompound: false\n  PadOperators: false\nAlignConsecutiveShortCaseStatements:\n  Enabled: false\n  AcrossEmptyLines: false\n  AcrossComments: false\n  AlignCaseColons: false\nAlignEscapedNewlines: Right\nAlignOperands: Align\nAlignTrailingComments:\n  Kind: Always\n  OverEmptyLines: 0\nAllowAllArgumentsOnNextLine: true\nAllowAllParametersOfDeclarationOnNextLine: true\nAllowShortBlocksOnASingleLine: Never\nAllowShortCaseLabelsOnASingleLine: false\nAllowShortEnumsOnASingleLine: true\nAllowShortFunctionsOnASingleLine: None\nAllowShortIfStatementsOnASingleLine: Never\nAllowShortLambdasOnASingleLine: Empty\nAllowShortLoopsOnASingleLine: false\nAlwaysBreakAfterDefinitionReturnType: None\nAlwaysBreakAfterReturnType: None\nAlwaysBreakBeforeMultilineStrings: false\nAlwaysBreakTemplateDeclarations: Yes\nAttributeMacros:\n  - __capability\nBinPackArguments: true\nBinPackParameters: true\nBitFieldColonSpacing: Both\nBraceWrapping:\n  AfterCaseLabel: false\n  AfterClass: false\n  AfterControlStatement: Never\n  AfterEnum: false\n  AfterExternBlock: false\n  AfterFunction: false\n  AfterNamespace: false\n  AfterObjCDeclaration: false\n  AfterStruct: false\n  AfterUnion: false\n  BeforeCatch: false\n  BeforeElse: false\n  BeforeLambdaBody: false\n  BeforeWhile: false\n  IndentBraces: false\n  SplitEmptyFunction: true\n  SplitEmptyRecord: true\n  SplitEmptyNamespace: true\nBreakAfterAttributes: Never\nBreakAfterJavaFieldAnnotations: false\nBreakArrays: true\nBreakBeforeBinaryOperators: None\nBreakBeforeConceptDeclarations: Always\nBreakBeforeBraces: Attach\nBreakBeforeInlineASMColon: OnlyMultiline\nBreakBeforeTernaryOperators: true\nBreakConstructorInitializers: BeforeComma\nBreakInheritanceList: BeforeColon\nBreakStringLiterals: true\nColumnLimit: 160\nCommentPragmas: '^ IWYU pragma:'\nCompactNamespaces: false\nConstructorInitializerIndentWidth: 4\nContinuationIndentWidth: 8\nCpp11BracedListStyle: true\nDerivePointerAlignment: false\nDisableFormat: false\nEmptyLineAfterAccessModifier: Never\nEmptyLineBeforeAccessModifier: LogicalBlock\nExperimentalAutoDetectBinPacking: false\nFixNamespaceComments: true\nForEachMacros:\n  - foreach\n  - Q_FOREACH\n  - BOOST_FOREACH\nIfMacros:\n  - KJ_IF_MAYBE\nIndentAccessModifiers: false\nIndentCaseBlocks: false\nIndentCaseLabels: false\nIndentExternBlock: AfterExternBlock\nIndentGotoLabels: true\nIndentPPDirectives: None\nIndentRequiresClause: true\nIndentWidth: 4\nIndentWrappedFunctionNames: false\nInsertBraces: false\nInsertNewlineAtEOF: true\nInsertTrailingCommas: None\nIntegerLiteralSeparator:\n  Binary: 0\n  BinaryMinDigits: 0\n  Decimal: 0\n  DecimalMinDigits: 0\n  Hex: 0\n  HexMinDigits: 0\nJavaScriptQuotes: Leave\nJavaScriptWrapImports: true\nKeepEmptyLinesAtTheStartOfBlocks: true\nKeepEmptyLinesAtEOF: false\nLambdaBodyIndentation: Signature\nLineEnding: DeriveLF\nMacroBlockBegin: ''\nMacroBlockEnd: ''\nMaxEmptyLinesToKeep: 2\nNamespaceIndentation: None\nPackConstructorInitializers: BinPack\nPenaltyBreakAssignment: 2\nPenaltyBreakBeforeFirstCallParameter: 19\nPenaltyBreakComment: 300\nPenaltyBreakFirstLessLess: 120\nPenaltyBreakOpenParenthesis: 0\nPenaltyBreakString: 1000\nPenaltyBreakTemplateDeclaration: 10\nPenaltyExcessCharacter: 1000000\nPenaltyIndentedWhitespace: 0\nPenaltyReturnTypeOnItsOwnLine: 60\nPointerAlignment: Left\nPPIndentWidth: -1\nQualifierAlignment: Leave\nReferenceAlignment: Pointer\nReflowComments: true\nRemoveBracesLLVM: false\nRemoveParentheses: Leave\nRemoveSemicolon: false\nRequiresClausePosition: OwnLine\nRequiresExpressionIndentation: OuterScope\nSeparateDefinitionBlocks: Leave\nShortNamespaceLines: 1\nSortIncludes: Never\nSortJavaStaticImport: Before\nSortUsingDeclarations: Never\nSpaceAfterCStyleCast: false\nSpaceAfterLogicalNot: false\nSpaceAfterTemplateKeyword: true\nSpaceAroundPointerQualifiers: Default\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeCaseColon: false\nSpaceBeforeCpp11BracedList: false\nSpaceBeforeCtorInitializerColon: true\nSpaceBeforeInheritanceColon: true\nSpaceBeforeJsonColon: false\nSpaceBeforeParens: ControlStatements\nSpaceBeforeParensOptions:\n  AfterControlStatements: true\n  AfterForeachMacros: true\n  AfterFunctionDefinitionName: false\n  AfterFunctionDeclarationName: false\n  AfterIfMacros: true\n  AfterOverloadedOperator: false\n  AfterRequiresInClause: false\n  AfterRequiresInExpression: false\n  BeforeNonEmptyParentheses: false\nSpaceBeforeRangeBasedForLoopColon: true\nSpaceBeforeSquareBrackets: false\nSpaceInEmptyBlock: false\nSpacesBeforeTrailingComments: 1\nSpacesInAngles: Never\nSpacesInContainerLiterals: true\nSpacesInLineCommentPrefix:\n  Minimum: 1\n  Maximum: -1\nSpacesInParens: Never\nSpacesInParensOptions:\n  InCStyleCasts: false\n  InConditionalStatements: false\n  InEmptyParentheses: false\n  Other: false\nSpacesInSquareBrackets: false\nStandard: Latest\nTabWidth: 8\nUseTab: Never\nVerilogBreakBetweenInstancePorts: true\nWhitespaceSensitiveMacros:\n  - BOOST_PP_STRINGIZE\n  - CF_SWIFT_NAME\n  - NS_SWIFT_NAME\n  - PP_STRINGIZE\n  - STRINGIZE\n...\n\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0322265625,
          "content": ".git\nbuild\nseastar/build\ntestlog\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.119140625,
          "content": "*.cc diff=cpp\n*.hh diff=cpp\n*.svg binary\ndocs/_static/api/js/* binary\npgo/profiles/** filter=lfs diff=lfs merge=lfs -text\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.44140625,
          "content": ".cproject\n.project\n.settings\nbuild\nbuild.ninja\ncmake-build-*\nbuild.ninja.new\ncscope.*\n/debian/\ndist/ami/files/*.rpm\ndist/ami/variables.json\ndist/ami/scylla_deploy.sh\n*.pyc\nCql.tokens\n.kdev4\n*.kdev4\n.idea\nCMakeLists.txt.user\n.cache\n.tox\n*.egg-info\n__pycache__CMakeLists.txt.user\n.gdbinit\n/resources\n.pytest_cache\n/expressions.tokens\ntags\n!db/tags/\ntestlog\ntest/*/*.reject\n.vscode\ncompile_commands.json\n.ccls-cache/\n.mypy_cache\n.envrc\nclang_build\n.idea/\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.42578125,
          "content": "[submodule \"seastar\"]\n\tpath = seastar\n\turl = ../seastar\n\tignore = dirty\n[submodule \"swagger-ui\"]\n\tpath = swagger-ui\n\turl = ../scylla-swagger-ui\n\tignore = dirty\n[submodule \"abseil\"]\n\tpath = abseil\n\turl = ../abseil-cpp\n[submodule \"scylla-tools\"]\n\tpath = tools/java\n\turl = ../scylla-tools-java\n[submodule \"scylla-python3\"]\n\tpath = tools/python3\n\turl = ../scylla-python3\n[submodule \"tools/cqlsh\"]\n\tpath = tools/cqlsh\n\turl = ../scylla-cqlsh\n"
        },
        {
          "name": ".gitorderfile",
          "type": "blob",
          "size": 0.021484375,
          "content": "*.py\n*.hh\n*.rl\n*.cc\n*\n"
        },
        {
          "name": ".mailmap",
          "type": "blob",
          "size": 0.3251953125,
          "content": "Avi Kivity <avi@scylladb.com> Avi Kivity' via ScyllaDB development <scylladb-dev@googlegroups.com>\nRaphael S. Carvalho <raphaelsc@scylladb.com> Raphael S. Carvalho' via ScyllaDB development <scylladb-dev@googlegroups.com>\nPavel Emelyanov <xemul@scylladb.com> Pavel Emelyanov' via ScyllaDB development <scylladb-dev@googlegroups.com>\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 11.193359375,
          "content": "cmake_minimum_required(VERSION 3.27)\n\nproject(scylla)\n\nlist(APPEND CMAKE_MODULE_PATH\n  ${CMAKE_CURRENT_SOURCE_DIR}/cmake\n  ${CMAKE_CURRENT_SOURCE_DIR}/seastar/cmake)\n\n# Set the possible values of build type for cmake-gui\nset(scylla_build_types\n    \"Debug\" \"RelWithDebInfo\" \"Dev\" \"Sanitize\" \"Coverage\")\nif(DEFINED CMAKE_BUILD_TYPE)\n    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS\n     ${scylla_build_types})\n    if(NOT CMAKE_BUILD_TYPE)\n        set(CMAKE_BUILD_TYPE \"RelWithDebInfo\" CACHE\n            STRING \"Choose the type of build.\" FORCE)\n        message(WARNING \"CMAKE_BUILD_TYPE not specified, Using 'RelWithDebInfo'\")\n    elseif(NOT CMAKE_BUILD_TYPE IN_LIST scylla_build_types)\n        message(FATAL_ERROR \"Unknown CMAKE_BUILD_TYPE: ${CMAKE_BUILD_TYPE}. \"\n            \"Following types are supported: ${scylla_build_types}\")\n    endif()\nendif(DEFINED CMAKE_BUILD_TYPE)\n\noption(Scylla_ENABLE_LTO \"Turn on link-time optimization for the 'release' mode.\" ON)\n\ninclude(mode.common)\nget_property(is_multi_config GLOBAL PROPERTY GENERATOR_IS_MULTI_CONFIG)\nif(is_multi_config)\n    foreach(config ${CMAKE_CONFIGURATION_TYPES})\n        include(mode.${config})\n        list(APPEND scylla_build_modes ${scylla_build_mode_${config}})\n    endforeach()\n    add_custom_target(mode_list\n        COMMAND ${CMAKE_COMMAND} -E echo \"$<JOIN:${scylla_build_modes}, >\"\n        COMMENT \"List configured modes\"\n        BYPRODUCTS mode-list.phony.stamp\n        COMMAND_EXPAND_LISTS)\nelse()\n    include(mode.${CMAKE_BUILD_TYPE})\n    add_custom_target(mode_list\n        ${CMAKE_COMMAND} -E echo \"${scylla_build_mode}\"\n        COMMENT \"List configured modes\")\nendif()\n\ninclude(limit_jobs)\n\n# Configure Seastar compile options to align with Scylla\nset(CMAKE_CXX_STANDARD \"23\" CACHE INTERNAL \"\")\nset(CMAKE_CXX_EXTENSIONS ON CACHE INTERNAL \"\")\nset(CMAKE_CXX_SCAN_FOR_MODULES OFF CACHE INTERNAL \"\")\nset(CMAKE_CXX_VISIBILITY_PRESET hidden)\n\nif(is_multi_config)\n    find_package(Seastar)\n    # this is atypical compared to standard ExternalProject usage:\n    # - Seastar's build system should already be configured at this point.\n    # - We maintain separate project variants for each configuration type.\n    #\n    # Benefits of this approach:\n    # - Allows the parent project to consume the compile options exposed by\n    #   .pc file. as the compile options vary from one config to another.\n    # - Allows application of config-specific settings\n    # - Enables building Seastar within the parent project's build system\n    # - Facilitates linking of artifacts with the external project target,\n    #   establishing proper dependencies between them\n    include(ExternalProject)\n\n    ExternalProject_Add(Seastar\n        SOURCE_DIR \"${PROJECT_SOURCE_DIR}/seastar\"\n        BINARY_DIR \"${CMAKE_BINARY_DIR}/$<CONFIG>/seastar\"\n        CONFIGURE_COMMAND \"\"\n        BUILD_COMMAND ${CMAKE_COMMAND} --build <BINARY_DIR>\n          --target seastar\n          --target seastar_testing\n          --target seastar_perf_testing\n          --target app_iotune\n        BUILD_ALWAYS ON\n        BUILD_BYPRODUCTS\n          <BINARY_DIR>/libseastar.$<IF:$<CONFIG:Debug,Dev>,so,a>\n          <BINARY_DIR>/libseastar_testing.$<IF:$<CONFIG:Debug,Dev>,so,a>\n          <BINARY_DIR>/libseastar_perf_testing.$<IF:$<CONFIG:Debug,Dev>,so,a>\n          <BINARY_DIR>/apps/iotune/iotune\n          <BINARY_DIR>/gen/include/seastar/http/chunk_parsers.hh\n          <BINARY_DIR>/gen/include/seastar/http/request_parser.hh\n          <BINARY_DIR>/gen/include/seastar/http/response_parser.hh\n        INSTALL_COMMAND \"\")\n    add_dependencies(Seastar::seastar Seastar)\n    add_dependencies(Seastar::seastar_testing Seastar)\nelse()\n    set(Seastar_TESTING ON CACHE BOOL \"\" FORCE)\n    set(Seastar_API_LEVEL 7 CACHE STRING \"\" FORCE)\n    set(Seastar_DEPRECATED_OSTREAM_FORMATTERS OFF CACHE BOOL \"\" FORCE)\n    set(Seastar_APPS ON CACHE BOOL \"\" FORCE)\n    set(Seastar_EXCLUDE_APPS_FROM_ALL ON CACHE BOOL \"\" FORCE)\n    set(Seastar_EXCLUDE_TESTS_FROM_ALL ON CACHE BOOL \"\" FORCE)\n    set(Seastar_IO_URING ON CACHE BOOL \"\" FORCE)\n    set(Seastar_SCHEDULING_GROUPS_COUNT 19 CACHE STRING \"\" FORCE)\n    set(Seastar_UNUSED_RESULT_ERROR ON CACHE BOOL \"\" FORCE)\n    add_subdirectory(seastar)\n    target_compile_definitions (seastar\n      PRIVATE\n        SEASTAR_NO_EXCEPTION_HACK)\nendif()\n\nset(ABSL_PROPAGATE_CXX_STD ON CACHE BOOL \"\" FORCE)\n\nif(Scylla_ENABLE_LTO)\n    list(APPEND absl_cxx_flags $<$<CONFIG:RelWithDebInfo>:${CMAKE_CXX_COMPILE_OPTIONS_IPO};-ffat-lto-objects>)\nendif()\n\nfind_package(Sanitizers QUIET)\nlist(APPEND absl_cxx_flags\n    $<$<CONFIG:Debug,Sanitize>:$<TARGET_PROPERTY:Sanitizers::address,INTERFACE_COMPILE_OPTIONS>;$<TARGET_PROPERTY:Sanitizers::undefined_behavior,INTERFACE_COMPILE_OPTIONS>>)\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n    list(APPEND ABSL_GCC_FLAGS ${absl_cxx_flags})\nelseif(CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n    list(APPEND ABSL_LLVM_FLAGS ${absl_cxx_flags})\nendif()\nset(ABSL_DEFAULT_LINKOPTS\n    $<$<CONFIG:Debug,Sanitize>:$<TARGET_PROPERTY:Sanitizers::address,INTERFACE_LINK_LIBRARIES>;$<TARGET_PROPERTY:Sanitizers::undefined_behavior,INTERFACE_LINK_LIBRARIES>>)\nadd_subdirectory(abseil)\nadd_library(absl-headers INTERFACE)\ntarget_include_directories(absl-headers SYSTEM INTERFACE\n    \"${PROJECT_SOURCE_DIR}/abseil\")\nadd_library(absl::headers ALIAS absl-headers)\n\n# Exclude absl::strerror from the default \"all\" target since it's not\n# used in Scylla build and, moreover, makes use of deprecated glibc APIs,\n# such as sys_nerr, which are not exposed from \"stdio.h\" since glibc 2.32,\n# which happens to be the case for recent Fedora distribution versions.\n#\n# Need to use the internal \"absl_strerror\" target name instead of namespaced\n# variant because `set_target_properties` does not understand the latter form,\n# unfortunately.\nset_target_properties(absl_strerror PROPERTIES EXCLUDE_FROM_ALL TRUE)\n\n# System libraries dependencies\nfind_package(Boost REQUIRED\n    COMPONENTS filesystem program_options system thread regex unit_test_framework)\ntarget_link_libraries(Boost::regex\n  INTERFACE\n    ICU::i18n\n    ICU::uc)\nfind_package(Lua REQUIRED)\nfind_package(ZLIB REQUIRED)\nfind_package(ICU COMPONENTS uc i18n REQUIRED)\nfind_package(fmt 10.0.0 REQUIRED)\nfind_package(libdeflate REQUIRED)\nfind_package(libxcrypt REQUIRED)\nfind_package(Snappy REQUIRED)\nfind_package(RapidJSON REQUIRED)\nfind_package(xxHash REQUIRED)\nfind_package(yaml-cpp REQUIRED)\nfind_package(zstd REQUIRED)\nfind_package(lz4 REQUIRED)\n\nset(scylla_gen_build_dir \"${CMAKE_BINARY_DIR}/gen\")\nfile(MAKE_DIRECTORY \"${scylla_gen_build_dir}\")\n\ninclude(add_version_library)\ngenerate_scylla_version()\n\nadd_library(scylla-zstd STATIC\n    zstd.cc)\ntarget_link_libraries(scylla-zstd\n  PRIVATE\n    db\n    Seastar::seastar\n    zstd::libzstd)\n\nadd_library(scylla-main STATIC)\ntarget_sources(scylla-main\n  PRIVATE\n    absl-flat_hash_map.cc\n    bytes.cc\n    client_data.cc\n    clocks-impl.cc\n    collection_mutation.cc\n    compress.cc\n    converting_mutation_partition_applier.cc\n    counters.cc\n    direct_failure_detector/failure_detector.cc\n    duration.cc\n    exceptions/exceptions.cc\n    frozen_schema.cc\n    generic_server.cc\n    debug.cc\n    init.cc\n    keys.cc\n    multishard_mutation_query.cc\n    mutation_query.cc\n    node_ops/task_manager_module.cc\n    partition_slice_builder.cc\n    querier.cc\n    query.cc\n    query_ranges_to_vnodes.cc\n    query-result-set.cc\n    tombstone_gc_options.cc\n    tombstone_gc.cc\n    reader_concurrency_semaphore.cc\n    reader_concurrency_semaphore_group.cc\n    row_cache.cc\n    schema_mutations.cc\n    serializer.cc\n    sstables_loader.cc\n    table_helper.cc\n    tasks/task_handler.cc\n    tasks/task_manager.cc\n    timeout_config.cc\n    unimplemented.cc\n    validation.cc\n    vint-serialization.cc)\ntarget_link_libraries(scylla-main\n  PRIVATE\n    \"$<LINK_LIBRARY:WHOLE_ARCHIVE,scylla-zstd>\"\n    db\n    absl::headers\n    absl::btree\n    absl::hash\n    absl::raw_hash_set\n    Seastar::seastar\n    Snappy::snappy\n    systemd\n    ZLIB::ZLIB\n    lz4::lz4_static\n    zstd::zstd_static\n)\n\noption(Scylla_CHECK_HEADERS\n  \"Add check-headers target for checking the self-containness of headers\")\nif(Scylla_CHECK_HEADERS)\n    add_custom_target(check-headers)\n    # compatibility target used by CI, which builds \"check-headers\" only for\n    # the \"Dev\" mode.\n    # our CI currently builds \"dev-headers\" using ninja without specify a build\n    # mode. where \"dev\" is actually a prefix encoded in the target name for the\n    # underlying \"headers\" target. while we don't have this convention in CMake\n    # targets. in contrast, the \"check-headers\" which is built for all\n    # configurations defined by \"CMAKE_DEFAULT_CONFIGS\". however, we only need\n    # to build \"check-headers\" for the \"Dev\" configuration. Therefore, before\n    # updating the CI to use build \"check-headers:Dev\", let's add a new target\n    # that specifically builds \"check-headers\" only for Dev configuration. The\n    # new target will do nothing for other configurations.\n    add_custom_target(dev-headers\n        COMMAND ${CMAKE_COMMAND}\n          \"$<IF:$<CONFIG:Dev>,--build;${CMAKE_BINARY_DIR};--config;$<CONFIG>;--target;check-headers,-E;echo;skipping;dev-headers;in;$<CONFIG>>\"\n        COMMAND_EXPAND_LISTS)\nendif()\n\ninclude(check_headers)\ncheck_headers(check-headers scylla-main\n  GLOB ${CMAKE_CURRENT_SOURCE_DIR}/*.hh)\n\noption(Scylla_DIST\n  \"Build dist targets\"\n  ON)\n\nadd_custom_target(compiler-training)\n\nadd_subdirectory(api)\nadd_subdirectory(alternator)\nadd_subdirectory(db)\nadd_subdirectory(auth)\nadd_subdirectory(cdc)\nadd_subdirectory(compaction)\nadd_subdirectory(cql3)\nadd_subdirectory(data_dictionary)\nadd_subdirectory(dht)\nadd_subdirectory(gms)\nadd_subdirectory(idl)\nadd_subdirectory(index)\nadd_subdirectory(lang)\nadd_subdirectory(locator)\nadd_subdirectory(message)\nadd_subdirectory(mutation)\nadd_subdirectory(mutation_writer)\nadd_subdirectory(node_ops)\nadd_subdirectory(readers)\nadd_subdirectory(redis)\nadd_subdirectory(replica)\nadd_subdirectory(raft)\nadd_subdirectory(repair)\nadd_subdirectory(rust)\nadd_subdirectory(schema)\nadd_subdirectory(service)\nadd_subdirectory(sstables)\nadd_subdirectory(streaming)\nadd_subdirectory(test)\nadd_subdirectory(tools)\nadd_subdirectory(tracing)\nadd_subdirectory(transport)\nadd_subdirectory(types)\nadd_subdirectory(utils)\nadd_version_library(scylla_version\n    release.cc)\n\nadd_executable(scylla\n  main.cc)\nset(scylla_libs\n    scylla-main\n    api\n    auth\n    alternator\n    db\n    cdc\n    compaction\n    cql3\n    data_dictionary\n    dht\n    gms\n    idl\n    index\n    lang\n    locator\n    message\n    mutation\n    mutation_writer\n    raft\n    readers\n    redis\n    repair\n    replica\n    schema\n    scylla_version\n    service\n    sstables\n    streaming\n    test-perf\n    tools\n    tracing\n    transport\n    types\n    utils)\ntarget_link_libraries(scylla PRIVATE\n    ${scylla_libs})\n\nif(Scylla_ENABLE_LTO)\n  include(enable_lto)\n  foreach(target scylla ${scylla_libs})\n    enable_lto(${target})\n  endforeach()\nendif()\n\ntarget_link_libraries(scylla PRIVATE\n    Seastar::seastar\n    absl::headers\n    yaml-cpp::yaml-cpp\n    Boost::program_options)\n\ntarget_include_directories(scylla PRIVATE\n    \"${CMAKE_CURRENT_SOURCE_DIR}\"\n    \"${scylla_gen_build_dir}\")\n\nadd_custom_target(maybe-scylla\n  DEPENDS $<$<CONFIG:Dev>:$<TARGET_FILE:scylla>>)\nadd_dependencies(compiler-training\n  maybe-scylla)\n\nif(Scylla_DIST)\n  add_subdirectory(dist)\nendif()\n\nif(Scylla_BUILD_INSTRUMENTED)\n  add_subdirectory(pgo)\nendif()\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.2109375,
          "content": "# Contributing to Scylla\n\n## Asking questions or requesting help\n\nUse the [ScyllaDB Community Forum](https://forum.scylladb.com) or the [Slack workspace](http://slack.scylladb.com) for general questions and help.\n\nJoin the [Scylla Developers mailing list](https://groups.google.com/g/scylladb-dev) for deeper technical discussions and to discuss your ideas for contributions.\n\n## Reporting an issue\n\nPlease use the [issue tracker](https://github.com/scylladb/scylla/issues/) to report issues or to suggest features. Fill in as much information as you can in the issue template, especially for performance problems.\n\n## Contributing code to Scylla\n\nBefore you can contribute code to Scylla for the first time, you should sign the [Contributor License Agreement](https://www.scylladb.com/open-source/contributor-agreement/) and send the signed form cla@scylladb.com. You can then submit your changes as patches to the to the [scylladb-dev mailing list](https://groups.google.com/forum/#!forum/scylladb-dev) or as a pull request to the [Scylla project on github](https://github.com/scylladb/scylla).\nIf you need help formatting or sending patches, [check out these instructions](https://github.com/scylladb/scylla/wiki/Formatting-and-sending-patches).\n\nThe Scylla C++ source code uses the [Seastar coding style](https://github.com/scylladb/seastar/blob/master/coding-style.md) so please adhere to that in your patches. Note that Scylla code is written with `using namespace seastar`, so should not explicitly add the `seastar::` prefix to Seastar symbols. You will usually not need to add `using namespace seastar` to new source files, because most Scylla header files have `#include \"seastarx.hh\"`, which does this.\n\nHeader files in Scylla must be self-contained, i.e., each can be included without having to include specific other headers first. To verify that your change did not break this property, run `ninja dev-headers`. If you added or removed header files, you must `touch configure.py` first - this will cause `configure.py` to be automatically re-run to generate a fresh list of header files.\n\nFor more criteria on what reviewers consider good code, see the [review checklist](https://github.com/scylladb/scylla/blob/master/docs/dev/review-checklist.md).\n"
        },
        {
          "name": "Doxyfile",
          "type": "blob",
          "size": 100.6181640625,
          "content": "# Doxyfile 1.8.9.1\n\n# This file describes the settings to be used by the documentation system\n# doxygen (www.doxygen.org) for a project.\n#\n# All text after a double hash (##) is considered a comment and is placed in\n# front of the TAG it is preceding.\n#\n# All text after a single hash (#) is considered a comment and will be ignored.\n# The format is:\n# TAG = value [value, ...]\n# For lists, items can also be appended using:\n# TAG += value [value, ...]\n# Values that contain spaces should be placed between quotes (\\\" \\\").\n\n#---------------------------------------------------------------------------\n# Project related configuration options\n#---------------------------------------------------------------------------\n\n# This tag specifies the encoding used for all characters in the config file\n# that follow. The default is UTF-8 which is also the encoding used for all text\n# before the first occurrence of this tag. Doxygen uses libiconv (or the iconv\n# built into libc) for the transcoding. See http://www.gnu.org/software/libiconv\n# for the list of possible encodings.\n# The default value is: UTF-8.\n\nDOXYFILE_ENCODING      = UTF-8\n\n# The PROJECT_NAME tag is a single word (or a sequence of words surrounded by\n# double-quotes, unless you are using Doxywizard) that should identify the\n# project for which the documentation is generated. This name is used in the\n# title of most generated pages and in a few other places.\n# The default value is: My Project.\n\nPROJECT_NAME           = \"Seastar\"\n\n# The PROJECT_NUMBER tag can be used to enter a project or revision number. This\n# could be handy for archiving the generated documentation or if some version\n# control system is used.\n\nPROJECT_NUMBER         =\n\n# Using the PROJECT_BRIEF tag one can provide an optional one line description\n# for a project that appears at the top of each page and should give viewer a\n# quick idea about the purpose of the project. Keep the description short.\n\nPROJECT_BRIEF          = \"High performance C++ framework for concurrent servers\"\n\n# With the PROJECT_LOGO tag one can specify a logo or an icon that is included\n# in the documentation. The maximum height of the logo should not exceed 55\n# pixels and the maximum width should not exceed 200 pixels. Doxygen will copy\n# the logo to the output directory.\n\nPROJECT_LOGO           =\n\n# The OUTPUT_DIRECTORY tag is used to specify the (relative or absolute) path\n# into which the generated documentation will be written. If a relative path is\n# entered, it will be relative to the location where doxygen was started. If\n# left blank the current directory will be used.\n\nOUTPUT_DIRECTORY       = build/documentation\n\n# If the CREATE_SUBDIRS tag is set to YES then doxygen will create 4096 sub-\n# directories (in 2 levels) under the output directory of each output format and\n# will distribute the generated files over these directories. Enabling this\n# option can be useful when feeding doxygen a huge amount of source files, where\n# putting all generated files in the same directory would otherwise causes\n# performance problems for the file system.\n# The default value is: NO.\n\nCREATE_SUBDIRS         = NO\n\n# If the ALLOW_UNICODE_NAMES tag is set to YES, doxygen will allow non-ASCII\n# characters to appear in the names of generated files. If set to NO, non-ASCII\n# characters will be escaped, for example _xE3_x81_x84 will be used for Unicode\n# U+3044.\n# The default value is: NO.\n\nALLOW_UNICODE_NAMES    = YES\n\n# The OUTPUT_LANGUAGE tag is used to specify the language in which all\n# documentation generated by doxygen is written. Doxygen will use this\n# information to generate all constant output in the proper language.\n# Possible values are: Afrikaans, Arabic, Armenian, Brazilian, Catalan, Chinese,\n# Chinese-Traditional, Croatian, Czech, Danish, Dutch, English (United States),\n# Esperanto, Farsi (Persian), Finnish, French, German, Greek, Hungarian,\n# Indonesian, Italian, Japanese, Japanese-en (Japanese with English messages),\n# Korean, Korean-en (Korean with English messages), Latvian, Lithuanian,\n# Macedonian, Norwegian, Persian (Farsi), Polish, Portuguese, Romanian, Russian,\n# Serbian, Serbian-Cyrillic, Slovak, Slovene, Spanish, Swedish, Turkish,\n# Ukrainian and Vietnamese.\n# The default value is: English.\n\nOUTPUT_LANGUAGE        = English\n\n# If the BRIEF_MEMBER_DESC tag is set to YES, doxygen will include brief member\n# descriptions after the members that are listed in the file and class\n# documentation (similar to Javadoc). Set to NO to disable this.\n# The default value is: YES.\n\nBRIEF_MEMBER_DESC      = YES\n\n# If the REPEAT_BRIEF tag is set to YES, doxygen will prepend the brief\n# description of a member or function before the detailed description\n#\n# Note: If both HIDE_UNDOC_MEMBERS and BRIEF_MEMBER_DESC are set to NO, the\n# brief descriptions will be completely suppressed.\n# The default value is: YES.\n\nREPEAT_BRIEF           = YES\n\n# This tag implements a quasi-intelligent brief description abbreviator that is\n# used to form the text in various listings. Each string in this list, if found\n# as the leading text of the brief description, will be stripped from the text\n# and the result, after processing the whole list, is used as the annotated\n# text. Otherwise, the brief description is used as-is. If left blank, the\n# following values are used ($name is automatically replaced with the name of\n# the entity):The $name class, The $name widget, The $name file, is, provides,\n# specifies, contains, represents, a, an and the.\n\nABBREVIATE_BRIEF       =\n\n# If the ALWAYS_DETAILED_SEC and REPEAT_BRIEF tags are both set to YES then\n# doxygen will generate a detailed section even if there is only a brief\n# description.\n# The default value is: NO.\n\nALWAYS_DETAILED_SEC    = NO\n\n# If the INLINE_INHERITED_MEMB tag is set to YES, doxygen will show all\n# inherited members of a class in the documentation of that class as if those\n# members were ordinary class members. Constructors, destructors and assignment\n# operators of the base classes will not be shown.\n# The default value is: NO.\n\nINLINE_INHERITED_MEMB  = YES\n\n# If the FULL_PATH_NAMES tag is set to YES, doxygen will prepend the full path\n# before files name in the file list and in the header files. If set to NO the\n# shortest path that makes the file name unique will be used\n# The default value is: YES.\n\nFULL_PATH_NAMES        = YES\n\n# The STRIP_FROM_PATH tag can be used to strip a user-defined part of the path.\n# Stripping is only done if one of the specified strings matches the left-hand\n# part of the path. The tag can be used to show relative paths in the file list.\n# If left blank the directory from which doxygen is run is used as the path to\n# strip.\n#\n# Note that you can specify absolute paths here, but also relative paths, which\n# will be relative from the directory where doxygen is started.\n# This tag requires that the tag FULL_PATH_NAMES is set to YES.\n\nSTRIP_FROM_PATH        =\n\n# The STRIP_FROM_INC_PATH tag can be used to strip a user-defined part of the\n# path mentioned in the documentation of a class, which tells the reader which\n# header file to include in order to use a class. If left blank only the name of\n# the header file containing the class definition is used. Otherwise one should\n# specify the list of include paths that are normally passed to the compiler\n# using the -I flag.\n\nSTRIP_FROM_INC_PATH    =\n\n# If the SHORT_NAMES tag is set to YES, doxygen will generate much shorter (but\n# less readable) file names. This can be useful is your file systems doesn't\n# support long names like on DOS, Mac, or CD-ROM.\n# The default value is: NO.\n\nSHORT_NAMES            = NO\n\n# If the JAVADOC_AUTOBRIEF tag is set to YES then doxygen will interpret the\n# first line (until the first dot) of a Javadoc-style comment as the brief\n# description. If set to NO, the Javadoc-style will behave just like regular Qt-\n# style comments (thus requiring an explicit @brief command for a brief\n# description.)\n# The default value is: NO.\n\nJAVADOC_AUTOBRIEF      = NO\n\n# If the QT_AUTOBRIEF tag is set to YES then doxygen will interpret the first\n# line (until the first dot) of a Qt-style comment as the brief description. If\n# set to NO, the Qt-style will behave just like regular Qt-style comments (thus\n# requiring an explicit \\brief command for a brief description.)\n# The default value is: NO.\n\nQT_AUTOBRIEF           = NO\n\n# The MULTILINE_CPP_IS_BRIEF tag can be set to YES to make doxygen treat a\n# multi-line C++ special comment block (i.e. a block of //! or /// comments) as\n# a brief description. This used to be the default behavior. The new default is\n# to treat a multi-line C++ comment block as a detailed description. Set this\n# tag to YES if you prefer the old behavior instead.\n#\n# Note that setting this tag to YES also means that rational rose comments are\n# not recognized any more.\n# The default value is: NO.\n\nMULTILINE_CPP_IS_BRIEF = NO\n\n# If the INHERIT_DOCS tag is set to YES then an undocumented member inherits the\n# documentation from any documented member that it re-implements.\n# The default value is: YES.\n\nINHERIT_DOCS           = YES\n\n# If the SEPARATE_MEMBER_PAGES tag is set to YES then doxygen will produce a new\n# page for each member. If set to NO, the documentation of a member will be part\n# of the file/class/namespace that contains it.\n# The default value is: NO.\n\nSEPARATE_MEMBER_PAGES  = NO\n\n# The TAB_SIZE tag can be used to set the number of spaces in a tab. Doxygen\n# uses this value to replace tabs by spaces in code fragments.\n# Minimum value: 1, maximum value: 16, default value: 4.\n\nTAB_SIZE               = 8\n\n# This tag can be used to specify a number of aliases that act as commands in\n# the documentation. An alias has the form:\n# name=value\n# For example adding\n# \"sideeffect=@par Side Effects:\\n\"\n# will allow you to put the command \\sideeffect (or @sideeffect) in the\n# documentation, which will result in a user-defined paragraph with heading\n# \"Side Effects:\". You can put \\n's in the value part of an alias to insert\n# newlines.\n\nALIASES                =\n\n# This tag can be used to specify a number of word-keyword mappings (TCL only).\n# A mapping has the form \"name=value\". For example adding \"class=itcl::class\"\n# will allow you to use the command class in the itcl::class meaning.\n\nTCL_SUBST              =\n\n# Set the OPTIMIZE_OUTPUT_FOR_C tag to YES if your project consists of C sources\n# only. Doxygen will then generate output that is more tailored for C. For\n# instance, some of the names that are used will be different. The list of all\n# members will be omitted, etc.\n# The default value is: NO.\n\nOPTIMIZE_OUTPUT_FOR_C  = NO\n\n# Set the OPTIMIZE_OUTPUT_JAVA tag to YES if your project consists of Java or\n# Python sources only. Doxygen will then generate output that is more tailored\n# for that language. For instance, namespaces will be presented as packages,\n# qualified scopes will look different, etc.\n# The default value is: NO.\n\nOPTIMIZE_OUTPUT_JAVA   = NO\n\n# Set the OPTIMIZE_FOR_FORTRAN tag to YES if your project consists of Fortran\n# sources. Doxygen will then generate output that is tailored for Fortran.\n# The default value is: NO.\n\nOPTIMIZE_FOR_FORTRAN   = NO\n\n# Set the OPTIMIZE_OUTPUT_VHDL tag to YES if your project consists of VHDL\n# sources. Doxygen will then generate output that is tailored for VHDL.\n# The default value is: NO.\n\nOPTIMIZE_OUTPUT_VHDL   = NO\n\n# Doxygen selects the parser to use depending on the extension of the files it\n# parses. With this tag you can assign which parser to use for a given\n# extension. Doxygen has a built-in mapping, but you can override or extend it\n# using this tag. The format is ext=language, where ext is a file extension, and\n# language is one of the parsers supported by doxygen: IDL, Java, Javascript,\n# C#, C, C++, D, PHP, Objective-C, Python, Fortran (fixed format Fortran:\n# FortranFixed, free formatted Fortran: FortranFree, unknown formatted Fortran:\n# Fortran. In the later case the parser tries to guess whether the code is fixed\n# or free formatted code, this is the default for Fortran type files), VHDL. For\n# instance to make doxygen treat .inc files as Fortran files (default is PHP),\n# and .f files as C (default is Fortran), use: inc=Fortran f=C.\n#\n# Note: For files without extension you can use no_extension as a placeholder.\n#\n# Note that for custom extensions you also need to set FILE_PATTERNS otherwise\n# the files are not read by doxygen.\n\nEXTENSION_MAPPING      =\n\n# If the MARKDOWN_SUPPORT tag is enabled then doxygen pre-processes all comments\n# according to the Markdown format, which allows for more readable\n# documentation. See http://daringfireball.net/projects/markdown/ for details.\n# The output of markdown processing is further processed by doxygen, so you can\n# mix doxygen, HTML, and XML commands with Markdown formatting. Disable only in\n# case of backward compatibilities issues.\n# The default value is: YES.\n\nMARKDOWN_SUPPORT       = YES\n\n# When enabled doxygen tries to link words that correspond to documented\n# classes, or namespaces to their corresponding documentation. Such a link can\n# be prevented in individual cases by putting a % sign in front of the word or\n# globally by setting AUTOLINK_SUPPORT to NO.\n# The default value is: YES.\n\nAUTOLINK_SUPPORT       = YES\n\n# If you use STL classes (i.e. std::string, std::vector, etc.) but do not want\n# to include (a tag file for) the STL sources as input, then you should set this\n# tag to YES in order to let doxygen match functions declarations and\n# definitions whose arguments contain STL classes (e.g. func(std::string);\n# versus func(std::string) {}). This also make the inheritance and collaboration\n# diagrams that involve STL classes more complete and accurate.\n# The default value is: NO.\n\nBUILTIN_STL_SUPPORT    = YES\n\n# If you use Microsoft's C++/CLI language, you should set this option to YES to\n# enable parsing support.\n# The default value is: NO.\n\nCPP_CLI_SUPPORT        = NO\n\n# Set the SIP_SUPPORT tag to YES if your project consists of sip (see:\n# http://www.riverbankcomputing.co.uk/software/sip/intro) sources only. Doxygen\n# will parse them like normal C++ but will assume all classes use public instead\n# of private inheritance when no explicit protection keyword is present.\n# The default value is: NO.\n\nSIP_SUPPORT            = NO\n\n# For Microsoft's IDL there are propget and propput attributes to indicate\n# getter and setter methods for a property. Setting this option to YES will make\n# doxygen to replace the get and set methods by a property in the documentation.\n# This will only work if the methods are indeed getting or setting a simple\n# type. If this is not the case, or you want to show the methods anyway, you\n# should set this option to NO.\n# The default value is: YES.\n\nIDL_PROPERTY_SUPPORT   = YES\n\n# If member grouping is used in the documentation and the DISTRIBUTE_GROUP_DOC\n# tag is set to YES then doxygen will reuse the documentation of the first\n# member in the group (if any) for the other members of the group. By default\n# all members of a group must be documented explicitly.\n# The default value is: NO.\n\nDISTRIBUTE_GROUP_DOC   = NO\n\n# Set the SUBGROUPING tag to YES to allow class member groups of the same type\n# (for instance a group of public functions) to be put as a subgroup of that\n# type (e.g. under the Public Functions section). Set it to NO to prevent\n# subgrouping. Alternatively, this can be done per class using the\n# \\nosubgrouping command.\n# The default value is: YES.\n\nSUBGROUPING            = YES\n\n# When the INLINE_GROUPED_CLASSES tag is set to YES, classes, structs and unions\n# are shown inside the group in which they are included (e.g. using \\ingroup)\n# instead of on a separate page (for HTML and Man pages) or section (for LaTeX\n# and RTF).\n#\n# Note that this feature does not work in combination with\n# SEPARATE_MEMBER_PAGES.\n# The default value is: NO.\n\nINLINE_GROUPED_CLASSES = NO\n\n# When the INLINE_SIMPLE_STRUCTS tag is set to YES, structs, classes, and unions\n# with only public data fields or simple typedef fields will be shown inline in\n# the documentation of the scope in which they are defined (i.e. file,\n# namespace, or group documentation), provided this scope is documented. If set\n# to NO, structs, classes, and unions are shown on a separate page (for HTML and\n# Man pages) or section (for LaTeX and RTF).\n# The default value is: NO.\n\nINLINE_SIMPLE_STRUCTS  = YES\n\n# When TYPEDEF_HIDES_STRUCT tag is enabled, a typedef of a struct, union, or\n# enum is documented as struct, union, or enum with the name of the typedef. So\n# typedef struct TypeS {} TypeT, will appear in the documentation as a struct\n# with name TypeT. When disabled the typedef will appear as a member of a file,\n# namespace, or class. And the struct will be named TypeS. This can typically be\n# useful for C code in case the coding convention dictates that all compound\n# types are typedef'ed and only the typedef is referenced, never the tag name.\n# The default value is: NO.\n\nTYPEDEF_HIDES_STRUCT   = NO\n\n# The size of the symbol lookup cache can be set using LOOKUP_CACHE_SIZE. This\n# cache is used to resolve symbols given their name and scope. Since this can be\n# an expensive process and often the same symbol appears multiple times in the\n# code, doxygen keeps a cache of pre-resolved symbols. If the cache is too small\n# doxygen will become slower. If the cache is too large, memory is wasted. The\n# cache size is given by this formula: 2^(16+LOOKUP_CACHE_SIZE). The valid range\n# is 0..9, the default is 0, corresponding to a cache size of 2^16=65536\n# symbols. At the end of a run doxygen will report the cache usage and suggest\n# the optimal cache size from a speed point of view.\n# Minimum value: 0, maximum value: 9, default value: 0.\n\nLOOKUP_CACHE_SIZE      = 0\n\n#---------------------------------------------------------------------------\n# Build related configuration options\n#---------------------------------------------------------------------------\n\n# If the EXTRACT_ALL tag is set to YES, doxygen will assume all entities in\n# documentation are documented, even if no documentation was available. Private\n# class members and static file members will be hidden unless the\n# EXTRACT_PRIVATE respectively EXTRACT_STATIC tags are set to YES.\n# Note: This will also disable the warnings about undocumented members that are\n# normally produced when WARNINGS is set to YES.\n# The default value is: NO.\n\nEXTRACT_ALL            = NO\n\n# If the EXTRACT_PRIVATE tag is set to YES, all private members of a class will\n# be included in the documentation.\n# The default value is: NO.\n\nEXTRACT_PRIVATE        = NO\n\n# If the EXTRACT_PACKAGE tag is set to YES, all members with package or internal\n# scope will be included in the documentation.\n# The default value is: NO.\n\nEXTRACT_PACKAGE        = NO\n\n# If the EXTRACT_STATIC tag is set to YES, all static members of a file will be\n# included in the documentation.\n# The default value is: NO.\n\nEXTRACT_STATIC         = YES\n\n# If the EXTRACT_LOCAL_CLASSES tag is set to YES, classes (and structs) defined\n# locally in source files will be included in the documentation. If set to NO,\n# only classes defined in header files are included. Does not have any effect\n# for Java sources.\n# The default value is: YES.\n\nEXTRACT_LOCAL_CLASSES  = NO\n\n# This flag is only useful for Objective-C code. If set to YES, local methods,\n# which are defined in the implementation section but not in the interface are\n# included in the documentation. If set to NO, only methods in the interface are\n# included.\n# The default value is: NO.\n\nEXTRACT_LOCAL_METHODS  = NO\n\n# If this flag is set to YES, the members of anonymous namespaces will be\n# extracted and appear in the documentation as a namespace called\n# 'anonymous_namespace{file}', where file will be replaced with the base name of\n# the file that contains the anonymous namespace. By default anonymous namespace\n# are hidden.\n# The default value is: NO.\n\nEXTRACT_ANON_NSPACES   = NO\n\n# If the HIDE_UNDOC_MEMBERS tag is set to YES, doxygen will hide all\n# undocumented members inside documented classes or files. If set to NO these\n# members will be included in the various overviews, but no documentation\n# section is generated. This option has no effect if EXTRACT_ALL is enabled.\n# The default value is: NO.\n\nHIDE_UNDOC_MEMBERS     = NO\n\n# If the HIDE_UNDOC_CLASSES tag is set to YES, doxygen will hide all\n# undocumented classes that are normally visible in the class hierarchy. If set\n# to NO, these classes will be included in the various overviews. This option\n# has no effect if EXTRACT_ALL is enabled.\n# The default value is: NO.\n\nHIDE_UNDOC_CLASSES     = NO\n\n# If the HIDE_FRIEND_COMPOUNDS tag is set to YES, doxygen will hide all friend\n# (class|struct|union) declarations. If set to NO, these declarations will be\n# included in the documentation.\n# The default value is: NO.\n\nHIDE_FRIEND_COMPOUNDS  = YES\n\n# If the HIDE_IN_BODY_DOCS tag is set to YES, doxygen will hide any\n# documentation blocks found inside the body of a function. If set to NO, these\n# blocks will be appended to the function's detailed documentation block.\n# The default value is: NO.\n\nHIDE_IN_BODY_DOCS      = NO\n\n# The INTERNAL_DOCS tag determines if documentation that is typed after a\n# \\internal command is included. If the tag is set to NO then the documentation\n# will be excluded. Set it to YES to include the internal documentation.\n# The default value is: NO.\n\nINTERNAL_DOCS          = NO\n\n# If the CASE_SENSE_NAMES tag is set to NO then doxygen will only generate file\n# names in lower-case letters. If set to YES, upper-case letters are also\n# allowed. This is useful if you have classes or files whose names only differ\n# in case and if your file system supports case sensitive file names. Windows\n# and Mac users are advised to set this option to NO.\n# The default value is: system dependent.\n\nCASE_SENSE_NAMES       = YES\n\n# If the HIDE_SCOPE_NAMES tag is set to NO then doxygen will show members with\n# their full class and namespace scopes in the documentation. If set to YES, the\n# scope will be hidden.\n# The default value is: NO.\n\nHIDE_SCOPE_NAMES       = NO\n\n# If the HIDE_COMPOUND_REFERENCE tag is set to NO (default) then doxygen will\n# append additional text to a page's title, such as Class Reference. If set to\n# YES the compound reference will be hidden.\n# The default value is: NO.\n\nHIDE_COMPOUND_REFERENCE= NO\n\n# If the SHOW_INCLUDE_FILES tag is set to YES then doxygen will put a list of\n# the files that are included by a file in the documentation of that file.\n# The default value is: YES.\n\nSHOW_INCLUDE_FILES     = YES\n\n# If the SHOW_GROUPED_MEMB_INC tag is set to YES then Doxygen will add for each\n# grouped member an include statement to the documentation, telling the reader\n# which file to include in order to use the member.\n# The default value is: NO.\n\nSHOW_GROUPED_MEMB_INC  = NO\n\n# If the FORCE_LOCAL_INCLUDES tag is set to YES then doxygen will list include\n# files with double quotes in the documentation rather than with sharp brackets.\n# The default value is: NO.\n\nFORCE_LOCAL_INCLUDES   = NO\n\n# If the INLINE_INFO tag is set to YES then a tag [inline] is inserted in the\n# documentation for inline members.\n# The default value is: YES.\n\nINLINE_INFO            = YES\n\n# If the SORT_MEMBER_DOCS tag is set to YES then doxygen will sort the\n# (detailed) documentation of file and class members alphabetically by member\n# name. If set to NO, the members will appear in declaration order.\n# The default value is: YES.\n\nSORT_MEMBER_DOCS       = YES\n\n# If the SORT_BRIEF_DOCS tag is set to YES then doxygen will sort the brief\n# descriptions of file, namespace and class members alphabetically by member\n# name. If set to NO, the members will appear in declaration order. Note that\n# this will also influence the order of the classes in the class list.\n# The default value is: NO.\n\nSORT_BRIEF_DOCS        = NO\n\n# If the SORT_MEMBERS_CTORS_1ST tag is set to YES then doxygen will sort the\n# (brief and detailed) documentation of class members so that constructors and\n# destructors are listed first. If set to NO the constructors will appear in the\n# respective orders defined by SORT_BRIEF_DOCS and SORT_MEMBER_DOCS.\n# Note: If SORT_BRIEF_DOCS is set to NO this option is ignored for sorting brief\n# member documentation.\n# Note: If SORT_MEMBER_DOCS is set to NO this option is ignored for sorting\n# detailed member documentation.\n# The default value is: NO.\n\nSORT_MEMBERS_CTORS_1ST = NO\n\n# If the SORT_GROUP_NAMES tag is set to YES then doxygen will sort the hierarchy\n# of group names into alphabetical order. If set to NO the group names will\n# appear in their defined order.\n# The default value is: NO.\n\nSORT_GROUP_NAMES       = NO\n\n# If the SORT_BY_SCOPE_NAME tag is set to YES, the class list will be sorted by\n# fully-qualified names, including namespaces. If set to NO, the class list will\n# be sorted only by class name, not including the namespace part.\n# Note: This option is not very useful if HIDE_SCOPE_NAMES is set to YES.\n# Note: This option applies only to the class list, not to the alphabetical\n# list.\n# The default value is: NO.\n\nSORT_BY_SCOPE_NAME     = NO\n\n# If the STRICT_PROTO_MATCHING option is enabled and doxygen fails to do proper\n# type resolution of all parameters of a function it will reject a match between\n# the prototype and the implementation of a member function even if there is\n# only one candidate or it is obvious which candidate to choose by doing a\n# simple string match. By disabling STRICT_PROTO_MATCHING doxygen will still\n# accept a match between prototype and implementation in such cases.\n# The default value is: NO.\n\nSTRICT_PROTO_MATCHING  = NO\n\n# The GENERATE_TODOLIST tag can be used to enable (YES) or disable (NO) the todo\n# list. This list is created by putting \\todo commands in the documentation.\n# The default value is: YES.\n\nGENERATE_TODOLIST      = YES\n\n# The GENERATE_TESTLIST tag can be used to enable (YES) or disable (NO) the test\n# list. This list is created by putting \\test commands in the documentation.\n# The default value is: YES.\n\nGENERATE_TESTLIST      = YES\n\n# The GENERATE_BUGLIST tag can be used to enable (YES) or disable (NO) the bug\n# list. This list is created by putting \\bug commands in the documentation.\n# The default value is: YES.\n\nGENERATE_BUGLIST       = YES\n\n# The GENERATE_DEPRECATEDLIST tag can be used to enable (YES) or disable (NO)\n# the deprecated list. This list is created by putting \\deprecated commands in\n# the documentation.\n# The default value is: YES.\n\nGENERATE_DEPRECATEDLIST= YES\n\n# The ENABLED_SECTIONS tag can be used to enable conditional documentation\n# sections, marked by \\if <section_label> ... \\endif and \\cond <section_label>\n# ... \\endcond blocks.\n\nENABLED_SECTIONS       =\n\n# The MAX_INITIALIZER_LINES tag determines the maximum number of lines that the\n# initial value of a variable or macro / define can have for it to appear in the\n# documentation. If the initializer consists of more lines than specified here\n# it will be hidden. Use a value of 0 to hide initializers completely. The\n# appearance of the value of individual variables and macros / defines can be\n# controlled using \\showinitializer or \\hideinitializer command in the\n# documentation regardless of this setting.\n# Minimum value: 0, maximum value: 10000, default value: 30.\n\nMAX_INITIALIZER_LINES  = 30\n\n# Set the SHOW_USED_FILES tag to NO to disable the list of files generated at\n# the bottom of the documentation of classes and structs. If set to YES, the\n# list will mention the files that were used to generate the documentation.\n# The default value is: YES.\n\nSHOW_USED_FILES        = YES\n\n# Set the SHOW_FILES tag to NO to disable the generation of the Files page. This\n# will remove the Files entry from the Quick Index and from the Folder Tree View\n# (if specified).\n# The default value is: YES.\n\nSHOW_FILES             = YES\n\n# Set the SHOW_NAMESPACES tag to NO to disable the generation of the Namespaces\n# page. This will remove the Namespaces entry from the Quick Index and from the\n# Folder Tree View (if specified).\n# The default value is: YES.\n\nSHOW_NAMESPACES        = YES\n\n# The FILE_VERSION_FILTER tag can be used to specify a program or script that\n# doxygen should invoke to get the current version for each file (typically from\n# the version control system). Doxygen will invoke the program by executing (via\n# popen()) the command command input-file, where command is the value of the\n# FILE_VERSION_FILTER tag, and input-file is the name of an input file provided\n# by doxygen. Whatever the program writes to standard output is used as the file\n# version. For an example see the documentation.\n\nFILE_VERSION_FILTER    =\n\n# The LAYOUT_FILE tag can be used to specify a layout file which will be parsed\n# by doxygen. The layout file controls the global structure of the generated\n# output files in an output format independent way. To create the layout file\n# that represents doxygen's defaults, run doxygen with the -l option. You can\n# optionally specify a file name after the option, if omitted DoxygenLayout.xml\n# will be used as the name of the layout file.\n#\n# Note that if you run doxygen from a directory containing a file called\n# DoxygenLayout.xml, doxygen will parse it automatically even if the LAYOUT_FILE\n# tag is left empty.\n\nLAYOUT_FILE            =\n\n# The CITE_BIB_FILES tag can be used to specify one or more bib files containing\n# the reference definitions. This must be a list of .bib files. The .bib\n# extension is automatically appended if omitted. This requires the bibtex tool\n# to be installed. See also http://en.wikipedia.org/wiki/BibTeX for more info.\n# For LaTeX the style of the bibliography can be controlled using\n# LATEX_BIB_STYLE. To use this feature you need bibtex and perl available in the\n# search path. See also \\cite for info how to create references.\n\nCITE_BIB_FILES         =\n\n#---------------------------------------------------------------------------\n# Configuration options related to warning and progress messages\n#---------------------------------------------------------------------------\n\n# The QUIET tag can be used to turn on/off the messages that are generated to\n# standard output by doxygen. If QUIET is set to YES this implies that the\n# messages are off.\n# The default value is: NO.\n\nQUIET                  = NO\n\n# The WARNINGS tag can be used to turn on/off the warning messages that are\n# generated to standard error (stderr) by doxygen. If WARNINGS is set to YES\n# this implies that the warnings are on.\n#\n# Tip: Turn warnings on while writing the documentation.\n# The default value is: YES.\n\nWARNINGS               = YES\n\n# If the WARN_IF_UNDOCUMENTED tag is set to YES then doxygen will generate\n# warnings for undocumented members. If EXTRACT_ALL is set to YES then this flag\n# will automatically be disabled.\n# The default value is: YES.\n\nWARN_IF_UNDOCUMENTED   = YES\n\n# If the WARN_IF_DOC_ERROR tag is set to YES, doxygen will generate warnings for\n# potential errors in the documentation, such as not documenting some parameters\n# in a documented function, or documenting parameters that don't exist or using\n# markup commands wrongly.\n# The default value is: YES.\n\nWARN_IF_DOC_ERROR      = YES\n\n# This WARN_NO_PARAMDOC option can be enabled to get warnings for functions that\n# are documented, but have no documentation for their parameters or return\n# value. If set to NO, doxygen will only warn about wrong or incomplete\n# parameter documentation, but not about the absence of documentation.\n# The default value is: NO.\n\nWARN_NO_PARAMDOC       = NO\n\n# The WARN_FORMAT tag determines the format of the warning messages that doxygen\n# can produce. The string should contain the $file, $line, and $text tags, which\n# will be replaced by the file and line number from which the warning originated\n# and the warning text. Optionally the format may contain $version, which will\n# be replaced by the version of the file (if it could be obtained via\n# FILE_VERSION_FILTER)\n# The default value is: $file:$line: $text.\n\nWARN_FORMAT            = \"$file:$line: $text\"\n\n# The WARN_LOGFILE tag can be used to specify a file to which warning and error\n# messages should be written. If left blank the output is written to standard\n# error (stderr).\n\nWARN_LOGFILE           =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the input files\n#---------------------------------------------------------------------------\n\n# The INPUT tag is used to specify the files and/or directories that contain\n# documented source files. You may enter file names like myfile.cpp or\n# directories like /usr/src/myproject. Separate the files or directories with\n# spaces.\n# Note: If this tag is empty the current directory is searched.\n\nINPUT                  =\n\n# This tag can be used to specify the character encoding of the source files\n# that doxygen parses. Internally doxygen uses the UTF-8 encoding. Doxygen uses\n# libiconv (or the iconv built into libc) for the transcoding. See the libiconv\n# documentation (see: http://www.gnu.org/software/libiconv) for the list of\n# possible encodings.\n# The default value is: UTF-8.\n\nINPUT_ENCODING         = UTF-8\n\n# If the value of the INPUT tag contains directories, you can use the\n# FILE_PATTERNS tag to specify one or more wildcard patterns (like *.cpp and\n# *.h) to filter out the source-files in the directories. If left blank the\n# following patterns are tested:*.c, *.cc, *.cxx, *.cpp, *.c++, *.java, *.ii,\n# *.ixx, *.ipp, *.i++, *.inl, *.idl, *.ddl, *.odl, *.h, *.hh, *.hxx, *.hpp,\n# *.h++, *.cs, *.d, *.php, *.php4, *.php5, *.phtml, *.inc, *.m, *.markdown,\n# *.md, *.mm, *.dox, *.py, *.f90, *.f, *.for, *.tcl, *.vhd, *.vhdl, *.ucf,\n# *.qsf, *.as and *.js.\n\nFILE_PATTERNS          =\n\n# The RECURSIVE tag can be used to specify whether or not subdirectories should\n# be searched for input files as well.\n# The default value is: NO.\n\nRECURSIVE              = YES\n\n# The EXCLUDE tag can be used to specify files and/or directories that should be\n# excluded from the INPUT source files. This way you can easily exclude a\n# subdirectory from a directory tree whose root is specified with the INPUT tag.\n#\n# Note that relative paths are relative to the directory from which doxygen is\n# run.\n\nEXCLUDE                = build, dpdk\n\n# The EXCLUDE_SYMLINKS tag can be used to select whether or not files or\n# directories that are symbolic links (a Unix file system feature) are excluded\n# from the input.\n# The default value is: NO.\n\nEXCLUDE_SYMLINKS       = NO\n\n# If the value of the INPUT tag contains directories, you can use the\n# EXCLUDE_PATTERNS tag to specify one or more wildcard patterns to exclude\n# certain files from those directories.\n#\n# Note that the wildcards are matched against the file with absolute path, so to\n# exclude all test directories for example use the pattern */test/*\n\nEXCLUDE_PATTERNS       =\n\n# The EXCLUDE_SYMBOLS tag can be used to specify one or more symbol names\n# (namespaces, classes, functions, etc.) that should be excluded from the\n# output. The symbol name can be a fully qualified name, a word, or if the\n# wildcard * is used, a substring. Examples: ANamespace, AClass,\n# AClass::ANamespace, ANamespace::*Test\n#\n# Note that the wildcards are matched against the file with absolute path, so to\n# exclude all test directories use the pattern */test/*\n\nEXCLUDE_SYMBOLS        =\n\n# The EXAMPLE_PATH tag can be used to specify one or more files or directories\n# that contain example code fragments that are included (see the \\include\n# command).\n\nEXAMPLE_PATH           =\n\n# If the value of the EXAMPLE_PATH tag contains directories, you can use the\n# EXAMPLE_PATTERNS tag to specify one or more wildcard pattern (like *.cpp and\n# *.h) to filter out the source-files in the directories. If left blank all\n# files are included.\n\nEXAMPLE_PATTERNS       =\n\n# If the EXAMPLE_RECURSIVE tag is set to YES then subdirectories will be\n# searched for input files to be used with the \\include or \\dontinclude commands\n# irrespective of the value of the RECURSIVE tag.\n# The default value is: NO.\n\nEXAMPLE_RECURSIVE      = NO\n\n# The IMAGE_PATH tag can be used to specify one or more files or directories\n# that contain images that are to be included in the documentation (see the\n# \\image command).\n\nIMAGE_PATH             =\n\n# The INPUT_FILTER tag can be used to specify a program that doxygen should\n# invoke to filter for each input file. Doxygen will invoke the filter program\n# by executing (via popen()) the command:\n#\n# <filter> <input-file>\n#\n# where <filter> is the value of the INPUT_FILTER tag, and <input-file> is the\n# name of an input file. Doxygen will then use the output that the filter\n# program writes to standard output. If FILTER_PATTERNS is specified, this tag\n# will be ignored.\n#\n# Note that the filter must not add or remove lines; it is applied before the\n# code is scanned, but not when the output code is generated. If lines are added\n# or removed, the anchors will not be placed correctly.\n\nINPUT_FILTER           =\n\n# The FILTER_PATTERNS tag can be used to specify filters on a per file pattern\n# basis. Doxygen will compare the file name with each pattern and apply the\n# filter if there is a match. The filters are a list of the form: pattern=filter\n# (like *.cpp=my_cpp_filter). See INPUT_FILTER for further information on how\n# filters are used. If the FILTER_PATTERNS tag is empty or if none of the\n# patterns match the file name, INPUT_FILTER is applied.\n\nFILTER_PATTERNS        =\n\n# If the FILTER_SOURCE_FILES tag is set to YES, the input filter (if set using\n# INPUT_FILTER) will also be used to filter the input files that are used for\n# producing the source files to browse (i.e. when SOURCE_BROWSER is set to YES).\n# The default value is: NO.\n\nFILTER_SOURCE_FILES    = NO\n\n# The FILTER_SOURCE_PATTERNS tag can be used to specify source filters per file\n# pattern. A pattern will override the setting for FILTER_PATTERN (if any) and\n# it is also possible to disable source filtering for a specific pattern using\n# *.ext= (so without naming a filter).\n# This tag requires that the tag FILTER_SOURCE_FILES is set to YES.\n\nFILTER_SOURCE_PATTERNS =\n\n# If the USE_MDFILE_AS_MAINPAGE tag refers to the name of a markdown file that\n# is part of the input, its contents will be placed on the main page\n# (index.html). This can be useful if you have a project on for instance GitHub\n# and want to reuse the introduction page also for the doxygen output.\n\nUSE_MDFILE_AS_MAINPAGE =\n\n#---------------------------------------------------------------------------\n# Configuration options related to source browsing\n#---------------------------------------------------------------------------\n\n# If the SOURCE_BROWSER tag is set to YES then a list of source files will be\n# generated. Documented entities will be cross-referenced with these sources.\n#\n# Note: To get rid of all source code in the generated output, make sure that\n# also VERBATIM_HEADERS is set to NO.\n# The default value is: NO.\n\nSOURCE_BROWSER         = NO\n\n# Setting the INLINE_SOURCES tag to YES will include the body of functions,\n# classes and enums directly into the documentation.\n# The default value is: NO.\n\nINLINE_SOURCES         = NO\n\n# Setting the STRIP_CODE_COMMENTS tag to YES will instruct doxygen to hide any\n# special comment blocks from generated source code fragments. Normal C, C++ and\n# Fortran comments will always remain visible.\n# The default value is: YES.\n\nSTRIP_CODE_COMMENTS    = YES\n\n# If the REFERENCED_BY_RELATION tag is set to YES then for each documented\n# function all documented functions referencing it will be listed.\n# The default value is: NO.\n\nREFERENCED_BY_RELATION = NO\n\n# If the REFERENCES_RELATION tag is set to YES then for each documented function\n# all documented entities called/used by that function will be listed.\n# The default value is: NO.\n\nREFERENCES_RELATION    = NO\n\n# If the REFERENCES_LINK_SOURCE tag is set to YES and SOURCE_BROWSER tag is set\n# to YES then the hyperlinks from functions in REFERENCES_RELATION and\n# REFERENCED_BY_RELATION lists will link to the source code. Otherwise they will\n# link to the documentation.\n# The default value is: YES.\n\nREFERENCES_LINK_SOURCE = YES\n\n# If SOURCE_TOOLTIPS is enabled (the default) then hovering a hyperlink in the\n# source code will show a tooltip with additional information such as prototype,\n# brief description and links to the definition and documentation. Since this\n# will make the HTML file larger and loading of large files a bit slower, you\n# can opt to disable this feature.\n# The default value is: YES.\n# This tag requires that the tag SOURCE_BROWSER is set to YES.\n\nSOURCE_TOOLTIPS        = YES\n\n# If the USE_HTAGS tag is set to YES then the references to source code will\n# point to the HTML generated by the htags(1) tool instead of doxygen built-in\n# source browser. The htags tool is part of GNU's global source tagging system\n# (see http://www.gnu.org/software/global/global.html). You will need version\n# 4.8.6 or higher.\n#\n# To use it do the following:\n# - Install the latest version of global\n# - Enable SOURCE_BROWSER and USE_HTAGS in the config file\n# - Make sure the INPUT points to the root of the source tree\n# - Run doxygen as normal\n#\n# Doxygen will invoke htags (and that will in turn invoke gtags), so these\n# tools must be available from the command line (i.e. in the search path).\n#\n# The result: instead of the source browser generated by doxygen, the links to\n# source code will now point to the output of htags.\n# The default value is: NO.\n# This tag requires that the tag SOURCE_BROWSER is set to YES.\n\nUSE_HTAGS              = NO\n\n# If the VERBATIM_HEADERS tag is set the YES then doxygen will generate a\n# verbatim copy of the header file for each class for which an include is\n# specified. Set to NO to disable this.\n# See also: Section \\class.\n# The default value is: YES.\n\nVERBATIM_HEADERS       = YES\n\n#---------------------------------------------------------------------------\n# Configuration options related to the alphabetical class index\n#---------------------------------------------------------------------------\n\n# If the ALPHABETICAL_INDEX tag is set to YES, an alphabetical index of all\n# compounds will be generated. Enable this if the project contains a lot of\n# classes, structs, unions or interfaces.\n# The default value is: YES.\n\nALPHABETICAL_INDEX     = YES\n\n# The COLS_IN_ALPHA_INDEX tag can be used to specify the number of columns in\n# which the alphabetical index list will be split.\n# Minimum value: 1, maximum value: 20, default value: 5.\n# This tag requires that the tag ALPHABETICAL_INDEX is set to YES.\n\nCOLS_IN_ALPHA_INDEX    = 5\n\n# In case all classes in a project start with a common prefix, all classes will\n# be put under the same header in the alphabetical index. The IGNORE_PREFIX tag\n# can be used to specify a prefix (or a list of prefixes) that should be ignored\n# while generating the index headers.\n# This tag requires that the tag ALPHABETICAL_INDEX is set to YES.\n\nIGNORE_PREFIX          =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the HTML output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_HTML tag is set to YES, doxygen will generate HTML output\n# The default value is: NO.\n\nGENERATE_HTML          = YES\n\n# The HTML_OUTPUT tag is used to specify where the HTML docs will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: html.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_OUTPUT            = html\n\n# The HTML_FILE_EXTENSION tag can be used to specify the file extension for each\n# generated HTML page (for example: .htm, .php, .asp).\n# The default value is: .html.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_FILE_EXTENSION    = .html\n\n# The HTML_HEADER tag can be used to specify a user-defined HTML header file for\n# each generated HTML page. If the tag is left blank doxygen will generate a\n# standard header.\n#\n# To get valid HTML the header file that includes any scripts and style sheets\n# that doxygen needs, which is dependent on the configuration options used (e.g.\n# the setting GENERATE_TREEVIEW). It is highly recommended to start with a\n# default header using\n# doxygen -w html new_header.html new_footer.html new_stylesheet.css\n# YourConfigFile\n# and then modify the file new_header.html. See also section \"Doxygen usage\"\n# for information on how to generate the default header that doxygen normally\n# uses.\n# Note: The header is subject to change so you typically have to regenerate the\n# default header when upgrading to a newer version of doxygen. For a description\n# of the possible markers and block names see the documentation.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_HEADER            =\n\n# The HTML_FOOTER tag can be used to specify a user-defined HTML footer for each\n# generated HTML page. If the tag is left blank doxygen will generate a standard\n# footer. See HTML_HEADER for more information on how to generate a default\n# footer and what special commands can be used inside the footer. See also\n# section \"Doxygen usage\" for information on how to generate the default footer\n# that doxygen normally uses.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_FOOTER            =\n\n# The HTML_STYLESHEET tag can be used to specify a user-defined cascading style\n# sheet that is used by each HTML page. It can be used to fine-tune the look of\n# the HTML output. If left blank doxygen will generate a default style sheet.\n# See also section \"Doxygen usage\" for information on how to generate the style\n# sheet that doxygen normally uses.\n# Note: It is recommended to use HTML_EXTRA_STYLESHEET instead of this tag, as\n# it is more robust and this tag (HTML_STYLESHEET) will in the future become\n# obsolete.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_STYLESHEET        =\n\n# The HTML_EXTRA_STYLESHEET tag can be used to specify additional user-defined\n# cascading style sheets that are included after the standard style sheets\n# created by doxygen. Using this option one can overrule certain style aspects.\n# This is preferred over using HTML_STYLESHEET since it does not replace the\n# standard style sheet and is therefore more robust against future updates.\n# Doxygen will copy the style sheet files to the output directory.\n# Note: The order of the extra style sheet files is of importance (e.g. the last\n# style sheet in the list overrules the setting of the previous ones in the\n# list). For an example see the documentation.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_EXTRA_STYLESHEET  =\n\n# The HTML_EXTRA_FILES tag can be used to specify one or more extra images or\n# other source files which should be copied to the HTML output directory. Note\n# that these files will be copied to the base HTML output directory. Use the\n# $relpath^ marker in the HTML_HEADER and/or HTML_FOOTER files to load these\n# files. In the HTML_STYLESHEET file, use the file name only. Also note that the\n# files will be copied as-is; there are no commands or markers available.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_EXTRA_FILES       =\n\n# The HTML_COLORSTYLE_HUE tag controls the color of the HTML output. Doxygen\n# will adjust the colors in the style sheet and background images according to\n# this color. Hue is specified as an angle on a colorwheel, see\n# http://en.wikipedia.org/wiki/Hue for more information. For instance the value\n# 0 represents red, 60 is yellow, 120 is green, 180 is cyan, 240 is blue, 300\n# purple, and 360 is red again.\n# Minimum value: 0, maximum value: 359, default value: 220.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_COLORSTYLE_HUE    = 220\n\n# The HTML_COLORSTYLE_SAT tag controls the purity (or saturation) of the colors\n# in the HTML output. For a value of 0 the output will use grayscales only. A\n# value of 255 will produce the most vivid colors.\n# Minimum value: 0, maximum value: 255, default value: 100.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_COLORSTYLE_SAT    = 100\n\n# The HTML_COLORSTYLE_GAMMA tag controls the gamma correction applied to the\n# luminance component of the colors in the HTML output. Values below 100\n# gradually make the output lighter, whereas values above 100 make the output\n# darker. The value divided by 100 is the actual gamma applied, so 80 represents\n# a gamma of 0.8, The value 220 represents a gamma of 2.2, and 100 does not\n# change the gamma.\n# Minimum value: 40, maximum value: 240, default value: 80.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_COLORSTYLE_GAMMA  = 80\n\n# If the HTML_TIMESTAMP tag is set to YES then the footer of each generated HTML\n# page will contain the date and time when the page was generated. Setting this\n# to NO can help when comparing the output of multiple runs.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_TIMESTAMP         = NO\n\n# If the HTML_DYNAMIC_SECTIONS tag is set to YES then the generated HTML\n# documentation will contain sections that can be hidden and shown after the\n# page has loaded.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_DYNAMIC_SECTIONS  = NO\n\n# With HTML_INDEX_NUM_ENTRIES one can control the preferred number of entries\n# shown in the various tree structured indices initially; the user can expand\n# and collapse entries dynamically later on. Doxygen will expand the tree to\n# such a level that at most the specified number of entries are visible (unless\n# a fully collapsed tree already exceeds this amount). So setting the number of\n# entries 1 will produce a full collapsed tree by default. 0 is a special value\n# representing an infinite number of entries and will result in a full expanded\n# tree by default.\n# Minimum value: 0, maximum value: 9999, default value: 100.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_INDEX_NUM_ENTRIES = 100\n\n# If the GENERATE_DOCSET tag is set to YES, additional index files will be\n# generated that can be used as input for Apple's Xcode 3 integrated development\n# environment (see: http://developer.apple.com/tools/xcode/), introduced with\n# OSX 10.5 (Leopard). To create a documentation set, doxygen will generate a\n# Makefile in the HTML output directory. Running make will produce the docset in\n# that directory and running make install will install the docset in\n# ~/Library/Developer/Shared/Documentation/DocSets so that Xcode will find it at\n# startup. See http://developer.apple.com/tools/creatingdocsetswithdoxygen.html\n# for more information.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_DOCSET        = NO\n\n# This tag determines the name of the docset feed. A documentation feed provides\n# an umbrella under which multiple documentation sets from a single provider\n# (such as a company or product suite) can be grouped.\n# The default value is: Doxygen generated docs.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_FEEDNAME        = \"Doxygen generated docs\"\n\n# This tag specifies a string that should uniquely identify the documentation\n# set bundle. This should be a reverse domain-name style string, e.g.\n# com.mycompany.MyDocSet. Doxygen will append .docset to the name.\n# The default value is: org.doxygen.Project.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_BUNDLE_ID       = org.doxygen.Project\n\n# The DOCSET_PUBLISHER_ID tag specifies a string that should uniquely identify\n# the documentation publisher. This should be a reverse domain-name style\n# string, e.g. com.mycompany.MyDocSet.documentation.\n# The default value is: org.doxygen.Publisher.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_PUBLISHER_ID    = org.doxygen.Publisher\n\n# The DOCSET_PUBLISHER_NAME tag identifies the documentation publisher.\n# The default value is: Publisher.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_PUBLISHER_NAME  = Publisher\n\n# If the GENERATE_HTMLHELP tag is set to YES then doxygen generates three\n# additional HTML index files: index.hhp, index.hhc, and index.hhk. The\n# index.hhp is a project file that can be read by Microsoft's HTML Help Workshop\n# (see: http://www.microsoft.com/en-us/download/details.aspx?id=21138) on\n# Windows.\n#\n# The HTML Help Workshop contains a compiler that can convert all HTML output\n# generated by doxygen into a single compiled HTML file (.chm). Compiled HTML\n# files are now used as the Windows 98 help format, and will replace the old\n# Windows help format (.hlp) on all Windows platforms in the future. Compressed\n# HTML files also contain an index, a table of contents, and you can search for\n# words in the documentation. The HTML workshop also contains a viewer for\n# compressed HTML files.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_HTMLHELP      = NO\n\n# The CHM_FILE tag can be used to specify the file name of the resulting .chm\n# file. You can add a path in front of the file if the result should not be\n# written to the html output directory.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nCHM_FILE               =\n\n# The HHC_LOCATION tag can be used to specify the location (absolute path\n# including file name) of the HTML help compiler (hhc.exe). If non-empty,\n# doxygen will try to run the HTML help compiler on the generated index.hhp.\n# The file has to be specified with full path.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nHHC_LOCATION           =\n\n# The GENERATE_CHI flag controls if a separate .chi index file is generated\n# (YES) or that it should be included in the master .chm file (NO).\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nGENERATE_CHI           = NO\n\n# The CHM_INDEX_ENCODING is used to encode HtmlHelp index (hhk), content (hhc)\n# and project file content.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nCHM_INDEX_ENCODING     =\n\n# The BINARY_TOC flag controls whether a binary table of contents is generated\n# (YES) or a normal table of contents (NO) in the .chm file. Furthermore it\n# enables the Previous and Next buttons.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nBINARY_TOC             = NO\n\n# The TOC_EXPAND flag can be set to YES to add extra items for group members to\n# the table of contents of the HTML help documentation and to the tree view.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nTOC_EXPAND             = NO\n\n# If the GENERATE_QHP tag is set to YES and both QHP_NAMESPACE and\n# QHP_VIRTUAL_FOLDER are set, an additional index file will be generated that\n# can be used as input for Qt's qhelpgenerator to generate a Qt Compressed Help\n# (.qch) of the generated HTML documentation.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_QHP           = NO\n\n# If the QHG_LOCATION tag is specified, the QCH_FILE tag can be used to specify\n# the file name of the resulting .qch file. The path specified is relative to\n# the HTML output folder.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQCH_FILE               =\n\n# The QHP_NAMESPACE tag specifies the namespace to use when generating Qt Help\n# Project output. For more information please see Qt Help Project / Namespace\n# (see: http://qt-project.org/doc/qt-4.8/qthelpproject.html#namespace).\n# The default value is: org.doxygen.Project.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_NAMESPACE          = org.doxygen.Project\n\n# The QHP_VIRTUAL_FOLDER tag specifies the namespace to use when generating Qt\n# Help Project output. For more information please see Qt Help Project / Virtual\n# Folders (see: http://qt-project.org/doc/qt-4.8/qthelpproject.html#virtual-\n# folders).\n# The default value is: doc.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_VIRTUAL_FOLDER     = doc\n\n# If the QHP_CUST_FILTER_NAME tag is set, it specifies the name of a custom\n# filter to add. For more information please see Qt Help Project / Custom\n# Filters (see: http://qt-project.org/doc/qt-4.8/qthelpproject.html#custom-\n# filters).\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_CUST_FILTER_NAME   =\n\n# The QHP_CUST_FILTER_ATTRS tag specifies the list of the attributes of the\n# custom filter to add. For more information please see Qt Help Project / Custom\n# Filters (see: http://qt-project.org/doc/qt-4.8/qthelpproject.html#custom-\n# filters).\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_CUST_FILTER_ATTRS  =\n\n# The QHP_SECT_FILTER_ATTRS tag specifies the list of the attributes this\n# project's filter section matches. Qt Help Project / Filter Attributes (see:\n# http://qt-project.org/doc/qt-4.8/qthelpproject.html#filter-attributes).\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_SECT_FILTER_ATTRS  =\n\n# The QHG_LOCATION tag can be used to specify the location of Qt's\n# qhelpgenerator. If non-empty doxygen will try to run qhelpgenerator on the\n# generated .qhp file.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHG_LOCATION           =\n\n# If the GENERATE_ECLIPSEHELP tag is set to YES, additional index files will be\n# generated, together with the HTML files, they form an Eclipse help plugin. To\n# install this plugin and make it available under the help contents menu in\n# Eclipse, the contents of the directory containing the HTML and XML files needs\n# to be copied into the plugins directory of eclipse. The name of the directory\n# within the plugins directory should be the same as the ECLIPSE_DOC_ID value.\n# After copying Eclipse needs to be restarted before the help appears.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_ECLIPSEHELP   = NO\n\n# A unique identifier for the Eclipse help plugin. When installing the plugin\n# the directory name containing the HTML and XML files should also have this\n# name. Each documentation set should have its own identifier.\n# The default value is: org.doxygen.Project.\n# This tag requires that the tag GENERATE_ECLIPSEHELP is set to YES.\n\nECLIPSE_DOC_ID         = org.doxygen.Project\n\n# If you want full control over the layout of the generated HTML pages it might\n# be necessary to disable the index and replace it with your own. The\n# DISABLE_INDEX tag can be used to turn on/off the condensed index (tabs) at top\n# of each HTML page. A value of NO enables the index and the value YES disables\n# it. Since the tabs in the index contain the same information as the navigation\n# tree, you can set this option to YES if you also set GENERATE_TREEVIEW to YES.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nDISABLE_INDEX          = NO\n\n# The GENERATE_TREEVIEW tag is used to specify whether a tree-like index\n# structure should be generated to display hierarchical information. If the tag\n# value is set to YES, a side panel will be generated containing a tree-like\n# index structure (just like the one that is generated for HTML Help). For this\n# to work a browser that supports JavaScript, DHTML, CSS and frames is required\n# (i.e. any modern browser). Windows users are probably better off using the\n# HTML help feature. Via custom style sheets (see HTML_EXTRA_STYLESHEET) one can\n# further fine-tune the look of the index. As an example, the default style\n# sheet generated by doxygen has an example that shows how to put an image at\n# the root of the tree instead of the PROJECT_NAME. Since the tree basically has\n# the same information as the tab index, you could consider setting\n# DISABLE_INDEX to YES when enabling this option.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_TREEVIEW      = NO\n\n# The ENUM_VALUES_PER_LINE tag can be used to set the number of enum values that\n# doxygen will group on one line in the generated HTML documentation.\n#\n# Note that a value of 0 will completely suppress the enum values from appearing\n# in the overview section.\n# Minimum value: 0, maximum value: 20, default value: 4.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nENUM_VALUES_PER_LINE   = 4\n\n# If the treeview is enabled (see GENERATE_TREEVIEW) then this tag can be used\n# to set the initial width (in pixels) of the frame in which the tree is shown.\n# Minimum value: 0, maximum value: 1500, default value: 250.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nTREEVIEW_WIDTH         = 250\n\n# If the EXT_LINKS_IN_WINDOW option is set to YES, doxygen will open links to\n# external symbols imported via tag files in a separate window.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nEXT_LINKS_IN_WINDOW    = NO\n\n# Use this tag to change the font size of LaTeX formulas included as images in\n# the HTML documentation. When you change the font size after a successful\n# doxygen run you need to manually remove any form_*.png images from the HTML\n# output directory to force them to be regenerated.\n# Minimum value: 8, maximum value: 50, default value: 10.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nFORMULA_FONTSIZE       = 10\n\n# Use the FORMULA_TRANPARENT tag to determine whether or not the images\n# generated for formulas are transparent PNGs. Transparent PNGs are not\n# supported properly for IE 6.0, but are supported on all modern browsers.\n#\n# Note that when changing this option you need to delete any form_*.png files in\n# the HTML output directory before the changes have effect.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nFORMULA_TRANSPARENT    = YES\n\n# Enable the USE_MATHJAX option to render LaTeX formulas using MathJax (see\n# http://www.mathjax.org) which uses client side Javascript for the rendering\n# instead of using pre-rendered bitmaps. Use this if you do not have LaTeX\n# installed or if you want to formulas look prettier in the HTML output. When\n# enabled you may also need to install MathJax separately and configure the path\n# to it using the MATHJAX_RELPATH option.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nUSE_MATHJAX            = NO\n\n# When MathJax is enabled you can set the default output format to be used for\n# the MathJax output. See the MathJax site (see:\n# http://docs.mathjax.org/en/latest/output.html) for more details.\n# Possible values are: HTML-CSS (which is slower, but has the best\n# compatibility), NativeMML (i.e. MathML) and SVG.\n# The default value is: HTML-CSS.\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_FORMAT         = HTML-CSS\n\n# When MathJax is enabled you need to specify the location relative to the HTML\n# output directory using the MATHJAX_RELPATH option. The destination directory\n# should contain the MathJax.js script. For instance, if the mathjax directory\n# is located at the same level as the HTML output directory, then\n# MATHJAX_RELPATH should be ../mathjax. The default value points to the MathJax\n# Content Delivery Network so you can quickly see the result without installing\n# MathJax. However, it is strongly recommended to install a local copy of\n# MathJax from http://www.mathjax.org before deployment.\n# The default value is: http://cdn.mathjax.org/mathjax/latest.\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_RELPATH        = http://cdn.mathjax.org/mathjax/latest\n\n# The MATHJAX_EXTENSIONS tag can be used to specify one or more MathJax\n# extension names that should be enabled during MathJax rendering. For example\n# MATHJAX_EXTENSIONS = TeX/AMSmath TeX/AMSsymbols\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_EXTENSIONS     =\n\n# The MATHJAX_CODEFILE tag can be used to specify a file with javascript pieces\n# of code that will be used on startup of the MathJax code. See the MathJax site\n# (see: http://docs.mathjax.org/en/latest/output.html) for more details. For an\n# example see the documentation.\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_CODEFILE       =\n\n# When the SEARCHENGINE tag is enabled doxygen will generate a search box for\n# the HTML output. The underlying search engine uses javascript and DHTML and\n# should work on any modern browser. Note that when using HTML help\n# (GENERATE_HTMLHELP), Qt help (GENERATE_QHP), or docsets (GENERATE_DOCSET)\n# there is already a search function so this one should typically be disabled.\n# For large projects the javascript based search engine can be slow, then\n# enabling SERVER_BASED_SEARCH may provide a better solution. It is possible to\n# search using the keyboard; to jump to the search box use <access key> + S\n# (what the <access key> is depends on the OS and browser, but it is typically\n# <CTRL>, <ALT>/<option>, or both). Inside the search box use the <cursor down\n# key> to jump into the search results window, the results can be navigated\n# using the <cursor keys>. Press <Enter> to select an item or <escape> to cancel\n# the search. The filter options can be selected when the cursor is inside the\n# search box by pressing <Shift>+<cursor down>. Also here use the <cursor keys>\n# to select a filter and <Enter> or <escape> to activate or cancel the filter\n# option.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nSEARCHENGINE           = YES\n\n# When the SERVER_BASED_SEARCH tag is enabled the search engine will be\n# implemented using a web server instead of a web client using Javascript. There\n# are two flavors of web server based searching depending on the EXTERNAL_SEARCH\n# setting. When disabled, doxygen will generate a PHP script for searching and\n# an index file used by the script. When EXTERNAL_SEARCH is enabled the indexing\n# and searching needs to be provided by external tools. See the section\n# \"External Indexing and Searching\" for details.\n# The default value is: NO.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nSERVER_BASED_SEARCH    = NO\n\n# When EXTERNAL_SEARCH tag is enabled doxygen will no longer generate the PHP\n# script for searching. Instead the search results are written to an XML file\n# which needs to be processed by an external indexer. Doxygen will invoke an\n# external search engine pointed to by the SEARCHENGINE_URL option to obtain the\n# search results.\n#\n# Doxygen ships with an example indexer (doxyindexer) and search engine\n# (doxysearch.cgi) which are based on the open source search engine library\n# Xapian (see: http://xapian.org/).\n#\n# See the section \"External Indexing and Searching\" for details.\n# The default value is: NO.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nEXTERNAL_SEARCH        = NO\n\n# The SEARCHENGINE_URL should point to a search engine hosted by a web server\n# which will return the search results when EXTERNAL_SEARCH is enabled.\n#\n# Doxygen ships with an example indexer (doxyindexer) and search engine\n# (doxysearch.cgi) which are based on the open source search engine library\n# Xapian (see: http://xapian.org/). See the section \"External Indexing and\n# Searching\" for details.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nSEARCHENGINE_URL       =\n\n# When SERVER_BASED_SEARCH and EXTERNAL_SEARCH are both enabled the unindexed\n# search data is written to a file for indexing by an external tool. With the\n# SEARCHDATA_FILE tag the name of this file can be specified.\n# The default file is: searchdata.xml.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nSEARCHDATA_FILE        = searchdata.xml\n\n# When SERVER_BASED_SEARCH and EXTERNAL_SEARCH are both enabled the\n# EXTERNAL_SEARCH_ID tag can be used as an identifier for the project. This is\n# useful in combination with EXTRA_SEARCH_MAPPINGS to search through multiple\n# projects and redirect the results back to the right project.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nEXTERNAL_SEARCH_ID     =\n\n# The EXTRA_SEARCH_MAPPINGS tag can be used to enable searching through doxygen\n# projects other than the one defined by this configuration file, but that are\n# all added to the same external search index. Each project needs to have a\n# unique id set via EXTERNAL_SEARCH_ID. The search mapping then maps the id of\n# to a relative location where the documentation can be found. The format is:\n# EXTRA_SEARCH_MAPPINGS = tagname1=loc1 tagname2=loc2 ...\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nEXTRA_SEARCH_MAPPINGS  =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the LaTeX output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_LATEX tag is set to YES, doxygen will generate LaTeX output.\n# The default value is: YES.\n\nGENERATE_LATEX         = NO\n\n# The LATEX_OUTPUT tag is used to specify where the LaTeX docs will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: latex.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_OUTPUT           = latex\n\n# The LATEX_CMD_NAME tag can be used to specify the LaTeX command name to be\n# invoked.\n#\n# Note that when enabling USE_PDFLATEX this option is only used for generating\n# bitmaps for formulas in the HTML output, but not in the Makefile that is\n# written to the output directory.\n# The default file is: latex.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_CMD_NAME         = latex\n\n# The MAKEINDEX_CMD_NAME tag can be used to specify the command name to generate\n# index for LaTeX.\n# The default file is: makeindex.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nMAKEINDEX_CMD_NAME     = makeindex\n\n# If the COMPACT_LATEX tag is set to YES, doxygen generates more compact LaTeX\n# documents. This may be useful for small projects and may help to save some\n# trees in general.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nCOMPACT_LATEX          = NO\n\n# The PAPER_TYPE tag can be used to set the paper type that is used by the\n# printer.\n# Possible values are: a4 (210 x 297 mm), letter (8.5 x 11 inches), legal (8.5 x\n# 14 inches) and executive (7.25 x 10.5 inches).\n# The default value is: a4.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nPAPER_TYPE             = a4\n\n# The EXTRA_PACKAGES tag can be used to specify one or more LaTeX package names\n# that should be included in the LaTeX output. To get the times font for\n# instance you can specify\n# EXTRA_PACKAGES=times\n# If left blank no extra packages will be included.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nEXTRA_PACKAGES         =\n\n# The LATEX_HEADER tag can be used to specify a personal LaTeX header for the\n# generated LaTeX document. The header should contain everything until the first\n# chapter. If it is left blank doxygen will generate a standard header. See\n# section \"Doxygen usage\" for information on how to let doxygen write the\n# default header to a separate file.\n#\n# Note: Only use a user-defined header if you know what you are doing! The\n# following commands have a special meaning inside the header: $title,\n# $datetime, $date, $doxygenversion, $projectname, $projectnumber,\n# $projectbrief, $projectlogo. Doxygen will replace $title with the empty\n# string, for the replacement values of the other commands the user is referred\n# to HTML_HEADER.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_HEADER           =\n\n# The LATEX_FOOTER tag can be used to specify a personal LaTeX footer for the\n# generated LaTeX document. The footer should contain everything after the last\n# chapter. If it is left blank doxygen will generate a standard footer. See\n# LATEX_HEADER for more information on how to generate a default footer and what\n# special commands can be used inside the footer.\n#\n# Note: Only use a user-defined footer if you know what you are doing!\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_FOOTER           =\n\n# The LATEX_EXTRA_STYLESHEET tag can be used to specify additional user-defined\n# LaTeX style sheets that are included after the standard style sheets created\n# by doxygen. Using this option one can overrule certain style aspects. Doxygen\n# will copy the style sheet files to the output directory.\n# Note: The order of the extra style sheet files is of importance (e.g. the last\n# style sheet in the list overrules the setting of the previous ones in the\n# list).\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_EXTRA_STYLESHEET =\n\n# The LATEX_EXTRA_FILES tag can be used to specify one or more extra images or\n# other source files which should be copied to the LATEX_OUTPUT output\n# directory. Note that the files will be copied as-is; there are no commands or\n# markers available.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_EXTRA_FILES      =\n\n# If the PDF_HYPERLINKS tag is set to YES, the LaTeX that is generated is\n# prepared for conversion to PDF (using ps2pdf or pdflatex). The PDF file will\n# contain links (just like the HTML output) instead of page references. This\n# makes the output suitable for online browsing using a PDF viewer.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nPDF_HYPERLINKS         = YES\n\n# If the USE_PDFLATEX tag is set to YES, doxygen will use pdflatex to generate\n# the PDF file directly from the LaTeX files. Set this option to YES, to get a\n# higher quality PDF documentation.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nUSE_PDFLATEX           = YES\n\n# If the LATEX_BATCHMODE tag is set to YES, doxygen will add the \\batchmode\n# command to the generated LaTeX files. This will instruct LaTeX to keep running\n# if errors occur, instead of asking the user for help. This option is also used\n# when generating formulas in HTML.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_BATCHMODE        = NO\n\n# If the LATEX_HIDE_INDICES tag is set to YES then doxygen will not include the\n# index chapters (such as File Index, Compound Index, etc.) in the output.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_HIDE_INDICES     = NO\n\n# If the LATEX_SOURCE_CODE tag is set to YES then doxygen will include source\n# code with syntax highlighting in the LaTeX output.\n#\n# Note that which sources are shown also depends on other settings such as\n# SOURCE_BROWSER.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_SOURCE_CODE      = NO\n\n# The LATEX_BIB_STYLE tag can be used to specify the style to use for the\n# bibliography, e.g. plainnat, or ieeetr. See\n# http://en.wikipedia.org/wiki/BibTeX and \\cite for more info.\n# The default value is: plain.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_BIB_STYLE        = plain\n\n#---------------------------------------------------------------------------\n# Configuration options related to the RTF output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_RTF tag is set to YES, doxygen will generate RTF output. The\n# RTF output is optimized for Word 97 and may not look too pretty with other RTF\n# readers/editors.\n# The default value is: NO.\n\nGENERATE_RTF           = NO\n\n# The RTF_OUTPUT tag is used to specify where the RTF docs will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: rtf.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_OUTPUT             = rtf\n\n# If the COMPACT_RTF tag is set to YES, doxygen generates more compact RTF\n# documents. This may be useful for small projects and may help to save some\n# trees in general.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nCOMPACT_RTF            = NO\n\n# If the RTF_HYPERLINKS tag is set to YES, the RTF that is generated will\n# contain hyperlink fields. The RTF file will contain links (just like the HTML\n# output) instead of page references. This makes the output suitable for online\n# browsing using Word or some other Word compatible readers that support those\n# fields.\n#\n# Note: WordPad (write) and others do not support links.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_HYPERLINKS         = NO\n\n# Load stylesheet definitions from file. Syntax is similar to doxygen's config\n# file, i.e. a series of assignments. You only have to provide replacements,\n# missing definitions are set to their default value.\n#\n# See also section \"Doxygen usage\" for information on how to generate the\n# default style sheet that doxygen normally uses.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_STYLESHEET_FILE    =\n\n# Set optional variables used in the generation of an RTF document. Syntax is\n# similar to doxygen's config file. A template extensions file can be generated\n# using doxygen -e rtf extensionFile.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_EXTENSIONS_FILE    =\n\n# If the RTF_SOURCE_CODE tag is set to YES then doxygen will include source code\n# with syntax highlighting in the RTF output.\n#\n# Note that which sources are shown also depends on other settings such as\n# SOURCE_BROWSER.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_SOURCE_CODE        = NO\n\n#---------------------------------------------------------------------------\n# Configuration options related to the man page output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_MAN tag is set to YES, doxygen will generate man pages for\n# classes and files.\n# The default value is: NO.\n\nGENERATE_MAN           = NO\n\n# The MAN_OUTPUT tag is used to specify where the man pages will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it. A directory man3 will be created inside the directory specified by\n# MAN_OUTPUT.\n# The default directory is: man.\n# This tag requires that the tag GENERATE_MAN is set to YES.\n\nMAN_OUTPUT             = man\n\n# The MAN_EXTENSION tag determines the extension that is added to the generated\n# man pages. In case the manual section does not start with a number, the number\n# 3 is prepended. The dot (.) at the beginning of the MAN_EXTENSION tag is\n# optional.\n# The default value is: .3.\n# This tag requires that the tag GENERATE_MAN is set to YES.\n\nMAN_EXTENSION          = .3\n\n# The MAN_SUBDIR tag determines the name of the directory created within\n# MAN_OUTPUT in which the man pages are placed. If defaults to man followed by\n# MAN_EXTENSION with the initial . removed.\n# This tag requires that the tag GENERATE_MAN is set to YES.\n\nMAN_SUBDIR             =\n\n# If the MAN_LINKS tag is set to YES and doxygen generates man output, then it\n# will generate one additional man file for each entity documented in the real\n# man page(s). These additional files only source the real man page, but without\n# them the man command would be unable to find the correct page.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_MAN is set to YES.\n\nMAN_LINKS              = NO\n\n#---------------------------------------------------------------------------\n# Configuration options related to the XML output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_XML tag is set to YES, doxygen will generate an XML file that\n# captures the structure of the code including all documentation.\n# The default value is: NO.\n\nGENERATE_XML           = NO\n\n# The XML_OUTPUT tag is used to specify where the XML pages will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: xml.\n# This tag requires that the tag GENERATE_XML is set to YES.\n\nXML_OUTPUT             = xml\n\n# If the XML_PROGRAMLISTING tag is set to YES, doxygen will dump the program\n# listings (including syntax highlighting and cross-referencing information) to\n# the XML output. Note that enabling this will significantly increase the size\n# of the XML output.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_XML is set to YES.\n\nXML_PROGRAMLISTING     = YES\n\n#---------------------------------------------------------------------------\n# Configuration options related to the DOCBOOK output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_DOCBOOK tag is set to YES, doxygen will generate Docbook files\n# that can be used to generate PDF.\n# The default value is: NO.\n\nGENERATE_DOCBOOK       = NO\n\n# The DOCBOOK_OUTPUT tag is used to specify where the Docbook pages will be put.\n# If a relative path is entered the value of OUTPUT_DIRECTORY will be put in\n# front of it.\n# The default directory is: docbook.\n# This tag requires that the tag GENERATE_DOCBOOK is set to YES.\n\nDOCBOOK_OUTPUT         = docbook\n\n# If the DOCBOOK_PROGRAMLISTING tag is set to YES, doxygen will include the\n# program listings (including syntax highlighting and cross-referencing\n# information) to the DOCBOOK output. Note that enabling this will significantly\n# increase the size of the DOCBOOK output.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_DOCBOOK is set to YES.\n\nDOCBOOK_PROGRAMLISTING = NO\n\n#---------------------------------------------------------------------------\n# Configuration options for the AutoGen Definitions output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_AUTOGEN_DEF tag is set to YES, doxygen will generate an\n# AutoGen Definitions (see http://autogen.sf.net) file that captures the\n# structure of the code including all documentation. Note that this feature is\n# still experimental and incomplete at the moment.\n# The default value is: NO.\n\nGENERATE_AUTOGEN_DEF   = NO\n\n#---------------------------------------------------------------------------\n# Configuration options related to the Perl module output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_PERLMOD tag is set to YES, doxygen will generate a Perl module\n# file that captures the structure of the code including all documentation.\n#\n# Note that this feature is still experimental and incomplete at the moment.\n# The default value is: NO.\n\nGENERATE_PERLMOD       = NO\n\n# If the PERLMOD_LATEX tag is set to YES, doxygen will generate the necessary\n# Makefile rules, Perl scripts and LaTeX code to be able to generate PDF and DVI\n# output from the Perl module output.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_PERLMOD is set to YES.\n\nPERLMOD_LATEX          = NO\n\n# If the PERLMOD_PRETTY tag is set to YES, the Perl module output will be nicely\n# formatted so it can be parsed by a human reader. This is useful if you want to\n# understand what is going on. On the other hand, if this tag is set to NO, the\n# size of the Perl module output will be much smaller and Perl will parse it\n# just the same.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_PERLMOD is set to YES.\n\nPERLMOD_PRETTY         = YES\n\n# The names of the make variables in the generated doxyrules.make file are\n# prefixed with the string contained in PERLMOD_MAKEVAR_PREFIX. This is useful\n# so different doxyrules.make files included by the same Makefile don't\n# overwrite each other's variables.\n# This tag requires that the tag GENERATE_PERLMOD is set to YES.\n\nPERLMOD_MAKEVAR_PREFIX =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the preprocessor\n#---------------------------------------------------------------------------\n\n# If the ENABLE_PREPROCESSING tag is set to YES, doxygen will evaluate all\n# C-preprocessor directives found in the sources and include files.\n# The default value is: YES.\n\nENABLE_PREPROCESSING   = YES\n\n# If the MACRO_EXPANSION tag is set to YES, doxygen will expand all macro names\n# in the source code. If set to NO, only conditional compilation will be\n# performed. Macro expansion can be done in a controlled way by setting\n# EXPAND_ONLY_PREDEF to YES.\n# The default value is: NO.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nMACRO_EXPANSION        = NO\n\n# If the EXPAND_ONLY_PREDEF and MACRO_EXPANSION tags are both set to YES then\n# the macro expansion is limited to the macros specified with the PREDEFINED and\n# EXPAND_AS_DEFINED tags.\n# The default value is: NO.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nEXPAND_ONLY_PREDEF     = NO\n\n# If the SEARCH_INCLUDES tag is set to YES, the include files in the\n# INCLUDE_PATH will be searched if a #include is found.\n# The default value is: YES.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nSEARCH_INCLUDES        = YES\n\n# The INCLUDE_PATH tag can be used to specify one or more directories that\n# contain include files that are not input files but should be processed by the\n# preprocessor.\n# This tag requires that the tag SEARCH_INCLUDES is set to YES.\n\nINCLUDE_PATH           =\n\n# You can use the INCLUDE_FILE_PATTERNS tag to specify one or more wildcard\n# patterns (like *.h and *.hpp) to filter out the header-files in the\n# directories. If left blank, the patterns specified with FILE_PATTERNS will be\n# used.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nINCLUDE_FILE_PATTERNS  =\n\n# The PREDEFINED tag can be used to specify one or more macro names that are\n# defined before the preprocessor is started (similar to the -D option of e.g.\n# gcc). The argument of the tag is a list of macros of the form: name or\n# name=definition (no spaces). If the definition and the \"=\" are omitted, \"=1\"\n# is assumed. To prevent a macro definition from being undefined via #undef or\n# recursively expanded use the := operator instead of the = operator.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nPREDEFINED             =\n\n# If the MACRO_EXPANSION and EXPAND_ONLY_PREDEF tags are set to YES then this\n# tag can be used to specify a list of macro names that should be expanded. The\n# macro definition that is found in the sources will be used. Use the PREDEFINED\n# tag if you want to use a different macro definition that overrules the\n# definition found in the source code.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nEXPAND_AS_DEFINED      =\n\n# If the SKIP_FUNCTION_MACROS tag is set to YES then doxygen's preprocessor will\n# remove all references to function-like macros that are alone on a line, have\n# an all uppercase name, and do not end with a semicolon. Such function macros\n# are typically used for boiler-plate code, and will confuse the parser if not\n# removed.\n# The default value is: YES.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nSKIP_FUNCTION_MACROS   = YES\n\n#---------------------------------------------------------------------------\n# Configuration options related to external references\n#---------------------------------------------------------------------------\n\n# The TAGFILES tag can be used to specify one or more tag files. For each tag\n# file the location of the external documentation should be added. The format of\n# a tag file without this location is as follows:\n# TAGFILES = file1 file2 ...\n# Adding location for the tag files is done as follows:\n# TAGFILES = file1=loc1 \"file2 = loc2\" ...\n# where loc1 and loc2 can be relative or absolute paths or URLs. See the\n# section \"Linking to external documentation\" for more information about the use\n# of tag files.\n# Note: Each tag file must have a unique name (where the name does NOT include\n# the path). If a tag file is not located in the directory in which doxygen is\n# run, you must also specify the path to the tagfile here.\n\nTAGFILES               =\n\n# When a file name is specified after GENERATE_TAGFILE, doxygen will create a\n# tag file that is based on the input files it reads. See section \"Linking to\n# external documentation\" for more information about the usage of tag files.\n\nGENERATE_TAGFILE       =\n\n# If the ALLEXTERNALS tag is set to YES, all external class will be listed in\n# the class index. If set to NO, only the inherited external classes will be\n# listed.\n# The default value is: NO.\n\nALLEXTERNALS           = NO\n\n# If the EXTERNAL_GROUPS tag is set to YES, all external groups will be listed\n# in the modules index. If set to NO, only the current project's groups will be\n# listed.\n# The default value is: YES.\n\nEXTERNAL_GROUPS        = YES\n\n# If the EXTERNAL_PAGES tag is set to YES, all external pages will be listed in\n# the related pages index. If set to NO, only the current project's pages will\n# be listed.\n# The default value is: YES.\n\nEXTERNAL_PAGES         = YES\n\n# The PERL_PATH should be the absolute path and name of the perl script\n# interpreter (i.e. the result of 'which perl').\n# The default file (with absolute path) is: /usr/bin/perl.\n\nPERL_PATH              = /usr/bin/perl\n\n#---------------------------------------------------------------------------\n# Configuration options related to the dot tool\n#---------------------------------------------------------------------------\n\n# If the CLASS_DIAGRAMS tag is set to YES, doxygen will generate a class diagram\n# (in HTML and LaTeX) for classes with base or super classes. Setting the tag to\n# NO turns the diagrams off. Note that this option also works with HAVE_DOT\n# disabled, but it is recommended to install and use dot, since it yields more\n# powerful graphs.\n# The default value is: YES.\n\nCLASS_DIAGRAMS         = YES\n\n# You can define message sequence charts within doxygen comments using the \\msc\n# command. Doxygen will then run the mscgen tool (see:\n# http://www.mcternan.me.uk/mscgen/)) to produce the chart and insert it in the\n# documentation. The MSCGEN_PATH tag allows you to specify the directory where\n# the mscgen tool resides. If left empty the tool is assumed to be found in the\n# default search path.\n\nMSCGEN_PATH            =\n\n# You can include diagrams made with dia in doxygen documentation. Doxygen will\n# then run dia to produce the diagram and insert it in the documentation. The\n# DIA_PATH tag allows you to specify the directory where the dia binary resides.\n# If left empty dia is assumed to be found in the default search path.\n\nDIA_PATH               =\n\n# If set to YES the inheritance and collaboration graphs will hide inheritance\n# and usage relations if the target is undocumented or is not a class.\n# The default value is: YES.\n\nHIDE_UNDOC_RELATIONS   = YES\n\n# If you set the HAVE_DOT tag to YES then doxygen will assume the dot tool is\n# available from the path. This tool is part of Graphviz (see:\n# http://www.graphviz.org/), a graph visualization toolkit from AT&T and Lucent\n# Bell Labs. The other options in this section have no effect if this option is\n# set to NO\n# The default value is: NO.\n\nHAVE_DOT               = NO\n\n# The DOT_NUM_THREADS specifies the number of dot invocations doxygen is allowed\n# to run in parallel. When set to 0 doxygen will base this on the number of\n# processors available in the system. You can set it explicitly to a value\n# larger than 0 to get control over the balance between CPU load and processing\n# speed.\n# Minimum value: 0, maximum value: 32, default value: 0.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_NUM_THREADS        = 0\n\n# When you want a differently looking font in the dot files that doxygen\n# generates you can specify the font name using DOT_FONTNAME. You need to make\n# sure dot is able to find the font, which can be done by putting it in a\n# standard location or by setting the DOTFONTPATH environment variable or by\n# setting DOT_FONTPATH to the directory containing the font.\n# The default value is: Helvetica.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_FONTNAME           = Helvetica\n\n# The DOT_FONTSIZE tag can be used to set the size (in points) of the font of\n# dot graphs.\n# Minimum value: 4, maximum value: 24, default value: 10.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_FONTSIZE           = 10\n\n# By default doxygen will tell dot to use the default font as specified with\n# DOT_FONTNAME. If you specify a different font using DOT_FONTNAME you can set\n# the path where dot can find it using this tag.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_FONTPATH           =\n\n# If the CLASS_GRAPH tag is set to YES then doxygen will generate a graph for\n# each documented class showing the direct and indirect inheritance relations.\n# Setting this tag to YES will force the CLASS_DIAGRAMS tag to NO.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCLASS_GRAPH            = YES\n\n# If the COLLABORATION_GRAPH tag is set to YES then doxygen will generate a\n# graph for each documented class showing the direct and indirect implementation\n# dependencies (inheritance, containment, and class references variables) of the\n# class with other documented classes.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCOLLABORATION_GRAPH    = YES\n\n# If the GROUP_GRAPHS tag is set to YES then doxygen will generate a graph for\n# groups, showing the direct groups dependencies.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nGROUP_GRAPHS           = YES\n\n# If the UML_LOOK tag is set to YES, doxygen will generate inheritance and\n# collaboration diagrams in a style similar to the OMG's Unified Modeling\n# Language.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nUML_LOOK               = NO\n\n# If the UML_LOOK tag is enabled, the fields and methods are shown inside the\n# class node. If there are many fields or methods and many nodes the graph may\n# become too big to be useful. The UML_LIMIT_NUM_FIELDS threshold limits the\n# number of items for each type to make the size more manageable. Set this to 0\n# for no limit. Note that the threshold may be exceeded by 50% before the limit\n# is enforced. So when you set the threshold to 10, up to 15 fields may appear,\n# but if the number exceeds 15, the total amount of fields shown is limited to\n# 10.\n# Minimum value: 0, maximum value: 100, default value: 10.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nUML_LIMIT_NUM_FIELDS   = 10\n\n# If the TEMPLATE_RELATIONS tag is set to YES then the inheritance and\n# collaboration graphs will show the relations between templates and their\n# instances.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nTEMPLATE_RELATIONS     = NO\n\n# If the INCLUDE_GRAPH, ENABLE_PREPROCESSING and SEARCH_INCLUDES tags are set to\n# YES then doxygen will generate a graph for each documented file showing the\n# direct and indirect include dependencies of the file with other documented\n# files.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nINCLUDE_GRAPH          = YES\n\n# If the INCLUDED_BY_GRAPH, ENABLE_PREPROCESSING and SEARCH_INCLUDES tags are\n# set to YES then doxygen will generate a graph for each documented file showing\n# the direct and indirect include dependencies of the file with other documented\n# files.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nINCLUDED_BY_GRAPH      = YES\n\n# If the CALL_GRAPH tag is set to YES then doxygen will generate a call\n# dependency graph for every global function or class method.\n#\n# Note that enabling this option will significantly increase the time of a run.\n# So in most cases it will be better to enable call graphs for selected\n# functions only using the \\callgraph command.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCALL_GRAPH             = NO\n\n# If the CALLER_GRAPH tag is set to YES then doxygen will generate a caller\n# dependency graph for every global function or class method.\n#\n# Note that enabling this option will significantly increase the time of a run.\n# So in most cases it will be better to enable caller graphs for selected\n# functions only using the \\callergraph command.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCALLER_GRAPH           = NO\n\n# If the GRAPHICAL_HIERARCHY tag is set to YES then doxygen will graphical\n# hierarchy of all classes instead of a textual one.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nGRAPHICAL_HIERARCHY    = YES\n\n# If the DIRECTORY_GRAPH tag is set to YES then doxygen will show the\n# dependencies a directory has on other directories in a graphical way. The\n# dependency relations are determined by the #include relations between the\n# files in the directories.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDIRECTORY_GRAPH        = YES\n\n# The DOT_IMAGE_FORMAT tag can be used to set the image format of the images\n# generated by dot.\n# Note: If you choose svg you need to set HTML_FILE_EXTENSION to xhtml in order\n# to make the SVG files visible in IE 9+ (other browsers do not have this\n# requirement).\n# Possible values are: png, jpg, gif and svg.\n# The default value is: png.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_IMAGE_FORMAT       = png\n\n# If DOT_IMAGE_FORMAT is set to svg, then this option can be set to YES to\n# enable generation of interactive SVG images that allow zooming and panning.\n#\n# Note that this requires a modern browser other than Internet Explorer. Tested\n# and working are Firefox, Chrome, Safari, and Opera.\n# Note: For IE 9+ you need to set HTML_FILE_EXTENSION to xhtml in order to make\n# the SVG files visible. Older versions of IE do not have SVG support.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nINTERACTIVE_SVG        = NO\n\n# The DOT_PATH tag can be used to specify the path where the dot tool can be\n# found. If left blank, it is assumed the dot tool can be found in the path.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_PATH               =\n\n# The DOTFILE_DIRS tag can be used to specify one or more directories that\n# contain dot files that are included in the documentation (see the \\dotfile\n# command).\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOTFILE_DIRS           =\n\n# The MSCFILE_DIRS tag can be used to specify one or more directories that\n# contain msc files that are included in the documentation (see the \\mscfile\n# command).\n\nMSCFILE_DIRS           =\n\n# The DIAFILE_DIRS tag can be used to specify one or more directories that\n# contain dia files that are included in the documentation (see the \\diafile\n# command).\n\nDIAFILE_DIRS           =\n\n# When using plantuml, the PLANTUML_JAR_PATH tag should be used to specify the\n# path where java can find the plantuml.jar file. If left blank, it is assumed\n# PlantUML is not used or called during a preprocessing step. Doxygen will\n# generate a warning when it encounters a \\startuml command in this case and\n# will not generate output for the diagram.\n\nPLANTUML_JAR_PATH      =\n\n# When using plantuml, the specified paths are searched for files specified by\n# the !include statement in a plantuml block.\n\nPLANTUML_INCLUDE_PATH  =\n\n# The DOT_GRAPH_MAX_NODES tag can be used to set the maximum number of nodes\n# that will be shown in the graph. If the number of nodes in a graph becomes\n# larger than this value, doxygen will truncate the graph, which is visualized\n# by representing a node as a red box. Note that doxygen if the number of direct\n# children of the root node in a graph is already larger than\n# DOT_GRAPH_MAX_NODES then the graph will not be shown at all. Also note that\n# the size of a graph can be further restricted by MAX_DOT_GRAPH_DEPTH.\n# Minimum value: 0, maximum value: 10000, default value: 50.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_GRAPH_MAX_NODES    = 50\n\n# The MAX_DOT_GRAPH_DEPTH tag can be used to set the maximum depth of the graphs\n# generated by dot. A depth value of 3 means that only nodes reachable from the\n# root by following a path via at most 3 edges will be shown. Nodes that lay\n# further from the root node will be omitted. Note that setting this option to 1\n# or 2 may greatly reduce the computation time needed for large code bases. Also\n# note that the size of a graph can be further restricted by\n# DOT_GRAPH_MAX_NODES. Using a depth of 0 means no depth restriction.\n# Minimum value: 0, maximum value: 1000, default value: 0.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nMAX_DOT_GRAPH_DEPTH    = 0\n\n# Set the DOT_TRANSPARENT tag to YES to generate images with a transparent\n# background. This is disabled by default, because dot on Windows does not seem\n# to support this out of the box.\n#\n# Warning: Depending on the platform used, enabling this option may lead to\n# badly anti-aliased labels on the edges of a graph (i.e. they become hard to\n# read).\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_TRANSPARENT        = NO\n\n# Set the DOT_MULTI_TARGETS tag to YES to allow dot to generate multiple output\n# files in one run (i.e. multiple -o and -T options on the command line). This\n# makes dot run faster, but since only newer versions of dot (>1.8.10) support\n# this, this feature is disabled by default.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_MULTI_TARGETS      = NO\n\n# If the GENERATE_LEGEND tag is set to YES doxygen will generate a legend page\n# explaining the meaning of the various boxes and arrows in the dot generated\n# graphs.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nGENERATE_LEGEND        = YES\n\n# If the DOT_CLEANUP tag is set to YES, doxygen will remove the intermediate dot\n# files that are used to generate the various graphs.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_CLEANUP            = YES\n"
        },
        {
          "name": "HACKING.md",
          "type": "blob",
          "size": 19.4052734375,
          "content": "# Guidelines for developing Scylla\n\nThis document is intended to help developers and contributors to Scylla get started. The first part consists of general guidelines that make no assumptions about a development environment or tooling. The second part describes a particular environment and work-flow for exemplary purposes.\n\n## Overview\n\nThis section covers some high-level information about the Scylla source code and work-flow.\n\n### Getting the source code\n\nScylla uses [Git submodules](https://git-scm.com/book/en/v2/Git-Tools-Submodules) to manage its dependency on Seastar and other tools. Be sure that all submodules are correctly initialized when cloning the project:\n\n```bash\n$ git clone https://github.com/scylladb/scylla\n$ cd scylla\n$ git submodule update --init --recursive\n```\n\n### Dependencies\n\nScylla is fairly fussy about its build environment, requiring a very recent\nversion of the C++23 compiler and numerous tools and libraries to build.\n\nRun `./install-dependencies.sh` (as root) to use your Linux distributions's\npackage manager to install the appropriate packages on your build machine.\nHowever, this will only work on very recent distributions. For example,\ncurrently Fedora users must upgrade to Fedora 32 otherwise the C++ compiler\nwill be too old, and not support the new C++23 standard that Scylla uses.\n\nAlternatively, to avoid having to upgrade your build machine or install\nvarious packages on it, we provide another option - the **frozen toolchain**.\nThis is a script, `./tools/toolchain/dbuild`, that can execute build or run\ncommands inside a container that contains exactly the right build tools and\nlibraries. The `dbuild` technique is useful for beginners, but is also the way\nin which ScyllaDB produces official releases, so it is highly recommended.\n\nTo use `dbuild`, you simply prefix any build or run command with it. Building\nand running Scylla becomes as easy as:\n\n```bash\n$ ./tools/toolchain/dbuild ./configure.py\n$ ./tools/toolchain/dbuild ninja build/release/scylla\n$ ./tools/toolchain/dbuild ./build/release/scylla --developer-mode 1\n```\n\nNote: do not mix environemtns - either perform all your work with dbuild, or natively on the host.\nNote2: you can get to an interactive shell within dbuild by running it without any parameters:\n```bash\n$ ./tools/toolchain/dbuild\n```\n\n### Build system\n\n**Note**: Compiling Scylla requires, conservatively, 2 GB of memory per native\nthread, and up to 3 GB per native thread while linking. GCC >= 10 is\nrequired.\n\nScylla is built with [Ninja](https://ninja-build.org/), a low-level rule-based system. A Python script, `configure.py`, generates a Ninja file (`build.ninja`) based on configuration options.\n\nTo build for the first time:\n\n```bash\n$ ./configure.py\n$ ninja-build\n```\n\nAfterwards, it is sufficient to just execute Ninja.\n\nThe full suite of options for project configuration is available via\n\n```bash\n$ ./configure.py --help\n```\n\nThe most important option is:\n\n- `--enable-dpdk`: [DPDK](http://dpdk.org/) is a set of libraries and drivers for fast packet processing. During development, it's not necessary to enable support even if it is supported by your platform.\n\nSource files and build targets are tracked manually in `configure.py`, so the script needs to be updated when new files or targets are added or removed.\n\nTo save time -- for instance, to avoid compiling all unit tests -- you can also specify specific targets to Ninja. For example,\n\n```bash\n$ ninja-build build/release/tests/schema_change_test\n$ ninja-build build/release/service/storage_proxy.o\n```\n\nYou can also specify a single mode. For example\n\n```bash\n$ ninja-build release\n```\n\nWill build everytihng in release mode. The valid modes are\n\n* Debug: Enables [AddressSanitizer](https://github.com/google/sanitizers/wiki/AddressSanitizer)\n  and other sanity checks. It has no optimizations, which allows for debugging with tools like\n  GDB. Debugging builds are generally slower and generate much larger object files than release builds.\n* Release: Fewer checks and more optimizations. It still has debug info.\n* Dev: No optimizations or debug info. The objective is to compile and link as fast as possible.\n  This is useful for the first iterations of a patch.\n\n\nNote that by default unit tests binaries are stripped so they can't be used with gdb or seastar-addr2line.\nTo include debug information in the unit test binary, build the test binary with a `_g` suffix. For example,\n\n```bash\n$ ninja-build build/release/tests/schema_change_test_g\n```\n\n### Unit testing\n\nUnit tests live in the `/tests` directory. Like with application source files, test sources and executables are specified manually in `configure.py` and need to be updated when changes are made.\n\nA test target can be any executable. A non-zero return code indicates test failure.\n\nMost tests in the Scylla repository are built using the [Boost.Test](http://www.boost.org/doc/libs/1_64_0/libs/test/doc/html/index.html) library. Utilities for writing tests with Seastar futures are also included.\n\nRun all tests through the test execution wrapper with\n\n```bash\n$ ./test.py --mode={debug,release}\n```\n\nor, if you are using `dbuild`, you need to build the code and the tests and then you can run them at will:\n\n```bash\n$ ./tools/toolchain/dbuild ninja {debug,release,dev}-build\n$ ./tools/toolchain/dbuild ./test.py --mode {debug,release,dev}\n```\n\nThe `--name` argument can be specified to run a particular test.\n\nAlternatively, you can execute the test executable directly. For example,\n\n```bash\n$ build/release/tests/row_cache_test -- -c1 -m1G\n```\n\nThe `-c1 -m1G` arguments limit this Seastar-based test to a single system thread and 1 GB of memory.\n\n### Preparing patches\n\nAll changes to Scylla are submitted as patches to the public [mailing list](mailto:scylladb-dev@googlegroups.com). Once a patch is approved by one of the maintainers of the project, it is committed to the maintainers' copy of the repository at https://github.com/scylladb/scylla.\n\nDetailed instructions for formatting patches for the mailing list and advice on preparing good patches are available at the [ScyllaDB website](http://docs.scylladb.com/contribute/). There are also some guidelines that can help you make the patch review process smoother:\n\n1. Before generating patches, make sure your Git configuration points to `.gitorderfile`. You can do it by running\n\n```bash\n$ git config diff.orderfile .gitorderfile\n```\n\n2. If you are sending more than a single patch, push your changes into a new branch of your fork of Scylla on GitHub and add a URL pointing to this branch to your cover letter.\n\n3. If you are sending a new revision of an earlier patchset, add a brief summary of changes in this version, for example:\n```\nIn v3:\n    - declared move constructor and move assignment operator as noexcept\n    - used std::variant instead of a union\n    ...\n```\n\n4. Add information about the tests run with this fix. It can look like\n```\n\"Tests: unit ({mode}), dtest ({smp})\"\n```\n\nThe usual is \"Tests: unit (dev)\", although running debug tests is encouraged.\n\n5. When answering review comments, prefer inline quotes as they make it easier to track the conversation across multiple e-mails.\n\n6. The Linux kernel's [Submitting Patches](https://www.kernel.org/doc/html/v4.19/process/submitting-patches.html) document offers excellent advice on how to prepare patches and patchsets for review. Since the Scylla development process is derived from the kernel's, almost all of the advice there is directly applicable.\n\n### Finding a person to review and merge your patches\n\nYou can use the `scripts/find-maintainer` script to find a subsystem maintainer and/or reviewer for your patches. The script accepts a filename in the git source tree as an argument and outputs a list of subsystems the file belongs to and their respective maintainers and reviewers. For example, if you changed the `cql3/statements/create_view_statement.hh` file, run the script as follows:\n\n```bash\n$ ./scripts/find-maintainer cql3/statements/create_view_statement.hh\n```\n\nand you will get output like this:\n\n```\nCQL QUERY LANGUAGE\n  Tomasz Grabiec <tgrabiec@scylladb.com>   [maintainer]\nMATERIALIZED VIEWS\n  Nadav Har'El <nyh@scylladb.com>          [reviewer]\n```\n\n### Running Scylla\n\nOnce Scylla has been compiled, executing the (`debug` or `release`) target will start a running instance in the foreground:\n\n```bash\n$ build/release/scylla\n```\n\nThe `scylla` executable requires a configuration file, `scylla.yaml`. By default, this is read from `$SCYLLA_HOME/conf/scylla.yaml`. A good starting point for development is located in the repository at `/conf/scylla.yaml`.\n\nFor development, a directory at `$HOME/scylla` can be used for all Scylla-related files:\n\n```bash\n$ mkdir -p $HOME/scylla $HOME/scylla/conf\n$ cp conf/scylla.yaml $HOME/scylla/conf/scylla.yaml\n$ # Edit configuration options as appropriate\n$ SCYLLA_HOME=$HOME/scylla build/release/scylla\n```\n\nThe `scylla.yaml` file in the repository by default writes all database data to `/var/lib/scylla`, which likely requires root access. Change the `data_file_directories`, `commitlog_directory` and `schema_commitlog_directory` fields as appropriate.\n\nScylla has a number of requirements for the file-system and operating system to operate ideally and at peak performance. However, during development, these requirements can be relaxed with the `--developer-mode` flag.\n\nAdditionally, when running on under-powered platforms like portable laptops, the `--overprovisioned` flag is useful.\n\nOn a development machine, one might run Scylla as\n\n```bash\n$ SCYLLA_HOME=$HOME/scylla build/release/scylla --overprovisioned --developer-mode=yes\n```\n\nTo interact with scylla it is recommended to build our versions of\ncqlsh and nodetool. They are available at\nhttps://github.com/scylladb/scylla-tools-java and can be built with\n\n```bash\n$ sudo ./install-dependencies.sh\n$ ant jar\n```\n\ncqlsh should work out of the box, but nodetool depends on a running\nscylla-jmx (https://github.com/scylladb/scylla-jmx). It can be build\nwith\n\n```bash\n$ mvn package\n```\n\nand must be started with\n\n```bash\n$ ./scripts/scylla-jmx\n```\n\n### Branches and tags\n\nMultiple release branches are maintained on the Git repository at https://github.com/scylladb/scylla. Release 1.5, for instance, is tracked on the `branch-1.5` branch.\n\nSimilarly, tags are used to pin-point precise release versions, including hot-fix versions like 1.5.4. These are named `scylla-1.5.4`, for example.\n\nMost development happens on the `master` branch. Release branches are cut from `master` based on time and/or features. When a patch against `master` fixes a serious issue like a node crash or data loss, it is backported to a particular release branch with `git cherry-pick` by the project maintainers.\n\n## Example: development on Fedora 25\n\nThis section describes one possible work-flow for developing Scylla on a Fedora 25 system. It is presented as an example to help you to develop a work-flow and tools that you are comfortable with.\n\n### Preface\n\nThis guide will be written from the perspective of a fictitious developer, Taylor Smith.\n\n### Git work-flow\n\nHaving two Git remotes is useful:\n\n- A public clone of Seastar (`\"public\"`)\n- A private clone of Seastar (`\"private\"`) for in-progress work or work that is not yet ready to share\n\nThe first step to contributing a change to Scylla is to create a local branch dedicated to it. For example, a feature that fixes a bug in the CQL statement for creating tables could be called `ts/cql_create_table_error/v1`. The branch name is prefaced by the developer's initials and has a suffix indicating that this is the first version. The version suffix is useful when branches are shared publicly and changes are requested on the mailing list. Having a branch for each version of the patch (or patch set) shared publicly makes it easier to reference and compare the history of a change.\n\nSetting the upstream branch of your development branch to `master` is a useful way to track your changes. You can do this with\n\n```bash\n$ git branch -u master ts/cql_create_table_error/v1\n```\n\nAs a patch set is developed, you can periodically push the branch to the private remote to back-up work.\n\nOnce the patch set is ready to be reviewed, push the branch to the public remote and prepare an email to the `scylladb-dev` mailing list. Including a link to the branch on your public remote allows for reviewers to quickly test and explore your changes.\n\n### Development environment and source code navigation\n\nScylla includes a [CMake](https://cmake.org/) file, `CMakeLists.txt`, for use only with development environments (not for building) so that they can properly analyze the source code.\n\n[CLion](https://www.jetbrains.com/clion/) is a commercial IDE offers reasonably good source code navigation and advice for code hygiene, though its C++ parser sometimes makes errors and flags false issues.\n\nOther good options that directly parse CMake files are [KDevelop](https://www.kdevelop.org/) and [QtCreator](https://wiki.qt.io/Qt_Creator).\n\nTo use the `CMakeLists.txt` file with these programs, define the `FOR_IDE` CMake variable or shell environmental variable.\n\n[Eclipse](https://eclipse.org/cdt/) is another open-source option. It doesn't natively work with CMake projects, and its C++ parser has many similar issues as CLion.\n\n### Distributed compilation: `distcc` and `ccache`\n\nScylla's compilations times can be long. Two tools help somewhat:\n\n- [ccache](https://ccache.samba.org/) caches compiled object files on disk and re-uses them when possible\n- [distcc](https://github.com/distcc/distcc) distributes compilation jobs to remote machines\n\nA reasonably-powered laptop acts as the coordinator for compilation. A second, more powerful, machine acts as a passive compilation server.\n\nHaving a direct wired connection between the machines ensures that object files can be transmitted quickly and limits the overhead of remote compilation.\nThe coordinator has been assigned the static IP address `10.0.0.1` and the passive compilation machine has been assigned `10.0.0.2`.\n\nOn Fedora, installing the `ccache` package places symbolic links for `gcc` and `g++` in the `PATH`. This allows normal compilation to transparently invoke `ccache` for compilation and cache object files on the local file-system.\n\nNext, set `CCACHE_PREFIX` so that `ccache` is responsible for invoking `distcc` as necessary:\n\n```bash\nexport CCACHE_PREFIX=\"distcc\"\n```\n\nOn each host, edit `/etc/sysconfig/distccd` to include the allowed coordinators and the total number of jobs that the machine should accept.\nThis example is for the laptop, which has 2 physical cores (4 logical cores with hyper-threading):\n\n```\nOPTIONS=\"--allow 10.0.0.2 --allow 127.0.0.1 --jobs 4\"\n```\n\n`10.0.0.2` has 8 physical cores (16 logical cores) and 64 GB of memory.\n\nAs a rule-of-thumb, the number of jobs that a machine should be specified to support should be equal to the number of its native threads.\n\nRestart the `distccd` service on all machines.\n\nOn the coordinator machine, edit `$HOME/.distcc/hosts` with the available hosts for compilation. Order of the hosts indicates preference.\n\n```\n10.0.0.2/16 localhost/2\n```\n\nIn this example, `10.0.0.2` will be sent up to 16 jobs and the local machine will be sent up to 2. Allowing for two extra threads on the host machine for coordination, we run compilation with `16 + 2 + 2 = 20` jobs in total: `ninja-build -j20`.\n\nWhen a compilation is in progress, the status of jobs on all remote machines can be visualized in the terminal with `distccmon-text` or graphically as a GTK application with `distccmon-gnome`.\n\nOne thing to keep in mind is that linking object files happens on the coordinating machine, which can be a bottleneck. See the next sections speeding up this process.\n\n### Using the `gold` linker\n\nLinking Scylla can be slow. The gold linker can replace GNU ld and often speeds the linking process. On Fedora, you can switch the system linker using\n\n```bash\n$ sudo alternatives --config ld\n```\n\n### Using split dwarf\n\nWith debug info enabled, most of the link time is spent copying and\nrelocating it. It is possible to leave most of the debug info out of\nthe link by writing it to a side .dwo file. This is done by passing\n`-gsplit-dwarf` to gcc.\n\nUnfortunately just `-gsplit-dwarf` would slow down `gdb` startup. To\navoid that the gold linker can be told to create an index with\n`--gdb-index`.\n\nMore info at https://gcc.gnu.org/wiki/DebugFission.\n\nBoth options can be enable by passing `--split-dwarf` to configure.py.\n\nNote that distcc is *not* compatible with it, but icecream\n(https://github.com/icecc/icecream) is.\n\n### Testing changes in Seastar with Scylla\n\nSometimes Scylla development is closely tied with a feature being developed in Seastar. It can be useful to compile Scylla with a particular check-out of Seastar.\n\nOne way to do this it to create a local remote for the Seastar submodule in the Scylla repository:\n\n```bash\n$ cd $HOME/src/scylla\n$ cd seastar\n$ git remote add local /home/tsmith/src/seastar\n$ git remote update\n$ git checkout -t local/my_local_seastar_branch\n```\n\n### Generating code coverage report\n\nInstall dependencies:\n\n    $ dnf install llvm # for llvm-profdata and llvm-cov\n    $ dnf install lcov # for genhtml\n\nInstruct `configure.py` to generate build files for `coverage` mode:\n\n    $ ./configure.py --mode=coverage\n\nBuild the tests you want to run, then run them via `test.py` (important!):\n\n    $ ./test.py --mode=coverage [...]\n\nAlternatively, you can run individual tests via `./scripts/coverage.py --run`.\n\nOpen the link printed at the end. Be horrified. Go and write more tests.\n\nFor more details see `./scripts/coverage.py --help`.\n\n### Resolving stack backtraces\n\nScylla may print stack backtraces to the log for several reasons.\nFor example:\n- When aborting (e.g. due to assertion failure, internal error, or segfault)\n- When detecting seastar reactor stalls (where a seastar task runs for a long time without yielding the cpu to other tasks on that shard)\n\nThe backtraces contain code pointers so they are not very helpful without resolving into code locations.\nTo resolve the backtraces, one needs the scylla relocatable package that contains the scylla binary (with debug information),\nas well as the dynamic libraries it is linked against.\n\nBuilds from our automated build system are uploaded to the cloud\nand can be searched on http://backtrace.scylladb.com/\n\nMake sure you have the scylla server exact `build-id` to locate\nits respective relocatable package, required for decoding backtraces it prints.\n\nThe build-id is printed to the system log when scylla starts.\nIt can also be found by executing `scylla --build-id`, or\nby using the `file` utility, for example:\n```\n$ scylla --build-id\n4cba12e6eb290a406bfa4930918db23941fd4be3\n\n$ file scylla\nscylla: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////lib64/ld-linux-x86-64.so.2, for GNU/Linux 3.2.0, BuildID[sha1]=4cba12e6eb290a406bfa4930918db23941fd4be3, with debug_info, not stripped, too many notes (256)\n```\n\nTo find the build-id of a coredump, use the `eu-unstrip` utility as follows:\n```\n$ eu-unstrip -n --core <coredump> | awk '/scylla$/ { s=$2; sub(/@.*$/, \"\", s); print s; exit(0); }'\n4cba12e6eb290a406bfa4930918db23941fd4be3\n```\n\n### Core dump debugging\n\nSee [debugging.md](docs/dev/debugging.md).\n"
        },
        {
          "name": "LICENSE-ScyllaDB-Source-Available.md",
          "type": "blob",
          "size": 13.6318359375,
          "content": "## **SCYLLADB SOFTWARE LICENSE AGREEMENT**\n\n| Version: | 1.0 |\n| :---- | :---- |\n| Last updated: | December 18, 2024 |\n\n**Your Acceptance**\n\nBy utilizing or accessing the Software in any manner, You hereby confirm and agree to be bound by this ScyllaDB Software License Agreement (the \"**Agreement**\"), which sets forth the terms and conditions on which ScyllaDB Ltd. (\"**Licensor**\") makes the Software available to You, as the Licensee. If Licensee does not agree to the terms of this Agreement or cannot otherwise comply with the Agreement, Licensee shall not utilize or access the Software.\n\nThe terms \"**You**\" or \"**Licensee**\" refer to any individual accessing or using the Software under this Agreement (\"**Use**\"). In case that such individual is Using the Software on behalf of a legal entity, You hereby irrevocably represents and warrants that You have full legal capacity and authority to enter into this Agreement on behalf of such entity as well as bind such entity to this Agreement, and in such case, the term \"You\" or \"Licensee\" in this Agreement will refer to such entity.\n\n**Grant of License**\n\n* **Software Definitions:** Software means the ScyllaDB software provided by Licensor, including the source code, object code, and any accompanying documentation or tools, or any part thereof, as made available under this Agreement.\n* **Grant of License:** Subject to the terms and conditions of this Agreement, Licensor grants You a limited, non-exclusive, revocable, non-sublicensable, non-transferable, royalty free license to Use the Software, in each case solely for the purposes of:\n  1) Copying, distributing, evaluating (including performing benchmarking or comparative tests or evaluations , subject to the limitations below) and improving the Software and ScyllaDB; and\n  2) create a modified version of the Software (each, a \"**Licensed Work**\"); provided however, that each such Licensed Work keeps all or substantially all of the functions and features of the Software, and/or using all or substantially all of the source code of the Software. You hereby agree that all the Licensed Work are, upon creation, considered Licensed Work of the Licensor, shall be the sole property of the Licensor and its assignees, and the Licensor and its assignees shall be the sole owner of all rights of any kind or nature, in connection with such Licensed Work. You hereby irrevocably and unconditionally assign to the Licensor all the Licensed Work and any part thereof.  This License applies separately for each version of the Licensed Work, which shall be considered \"Software\" for the purpose of this Agreement.\n\n\n**License Limitations, Restrictions and Obligations:** The license grant above is subject to the following limitations, restrictions, and obligations. If Licensees Use of the Software does not comply with the above license grant or the terms of this section (including exceeding the Usage Limit set forth below), Licensee must: (i) refrain from any Use of the Software; and (ii) purchase a [commercial paid license](https://www.scylladb.com/scylladb-proprietary-software-license-agreement/) from the Licensor.\n\n* **Updates:** You shall be solely responsible for providing all equipment, systems, assets, access, and ancillary goods and services needed to access and Use the Software.  Licensor may modify or update the Software at any time, without notification, in its sole and absolute discretion.  After the effective date of each such update, Licensor shall bear no obligation to run, provide or support legacy versions of the Software.\n* **\"Usage Limit\":** Licensee's total overall available storage across all deployments and clusters of the Software and the Licensed Work under this License shall not exceed 10TB and/or an upper limit of 50 VCPUs (hyper threads).\n* **IP Markings:** Licensee must retain all copyright, trademark, and other proprietary notices contained in the Software. You will not modify, delete, alter, remove, or obscure any intellectual property, including without limitations licensing, copyright, trademark, or any other notices of Licensor in the Software.\n* **License Reproduction:** You must conspicuously display this Agreement on each copy of the Software. If You receive the Software from a third party, this Agreement still applies to Your Use of the Software. You will be responsible for any breach of this Agreement by any such third-party.\n* Distribution of any Licensed Works is permitted, provided that: (i) You must include in any Licensed Work prominent notices stating that You have modified the Software, (ii) You include a copy of this Agreement with the Licensed Work, and (iii) You clearly identify all modifications made in the Licensed Work and provides attribution to the Licensor as the original author(s) of the Software.\n* **Commercial Use Restrictions:** Licensee may not offer the Software as a software-as-a-service (SaaS) or commercial database-as-as-service (dBaaS) offering.  Licensee may not use the Software to compete with Licensor's existing or future products or services. If your Use of the Software does not comply with the requirements currently in effect as described in this License, you must purchase a commercial license from the Licensor, its affiliated entities, or you must refrain from using the Software and all Licensed Work. Furthermore, if You make any written claim of patent infringement relating to the Software, Your patent license for the Software granted under this Agreement terminates immediately.\n* Notwithstanding anything to the contrary, under the License granted hereunder, You shall not and shall not permit others to: (i) transfer the Software or any portions thereof to any other party except as expressly permitted herein; (ii) attempt to circumvent or overcome any technological protection measures incorporated into the Software; (iii) incorporate the Software into the structure, machinery or controls of any aircraft, other aerial device, military vehicle, hovercraft, waterborne craft or any medical equipment of any kind; or (iv) use the Software or any part thereof in any unlawful, harmful or illegal manner, or in a manner which infringes third parties rights in any way, including intellectual property rights.\n\n**Monitoring; Audit**\n\n* **License Key:** Licensor may implement a method of authentication, e.g., a unique license token (\"License Key\") as a condition of accessing or using the Software. Upon the implementation of such License Key, Licensee agrees to comply with Licensor terms and requirements with regards to such License Key\n* **Monitoring & Data Sharing:** Licensor do not collect customer data from its database. Notwithstanding, Licensee acknowledges and agrees that the License Key and Software may share telemetry metrics and information regarding the execution volume and statistics with Licensor regarding Licensees use of the same. Any disclosure or use of such information shall be subject to, and in accordance with, Licensors Privacy Policy and Data Processing Agreement, which can be found at [https://www.scylladb.com/policies-agreements](https://www.scylladb.com/policies-agreements).\n* **Information Requests; Audits:**  Licensee shall keep accurate records of its access to and use of any Software, and shall promptly respond to any Licensor requests for information regarding the same.  To ensure compliance with the terms of this Agreement, during the term of this Agreement and for a period of one (1) year thereafter, Licensor (or an agent bound by customary confidentiality undertakings on its behalf) may audit Licensees records which are related to its access to or use of the Software. The cost of such audit shall be borne by Licensor unless it is determined that Licensee has materially breached this Agreement.\n\n**Termination**\n\n* **Termination:** Licensor may immediately terminate this Agreement will automatically terminate if You for any reason, including without limitation for (i) Licensees breach of any term, condition, or restriction of this Agreement, unless such breach was cured to Licensors satisfaction within no more than 15 days from the date of the breach. Notwithstanding the foregoing, intentional; or (ii) if Licensee brings any claim, demand or repeated breaches lawsuit against Licensor.\n* **Obligations on Termination:**  Upon termination of this Agreement by You will cause Your licenses to terminate automatically and permanently, at Licensors sole discretion, Licensee must (i) immediately stop using any Software, (ii) return all copies of any tools or documentation provided by Licensor; and (iii) pay amount due to Licensor hereunder (e.g., audit costs).  All obligations which by their nature must survive the termination of this Agreement shall so survive.\n\n**Indemnity; Disclaimer; Limitation of Liability**\n\n* **Indemnity:** Licensee hereby agrees to indemnify, defend and hold harmless Licensor and its affiliates from any losses or damages incurred due to a third party claim arising out of: (i) Licensees breach of this Agreement; (ii) Licensees negligence, willful misconduct or violation of law, or (iii) Licensees products or services.\n* DISCLAIMER OF WARRANTIES:  LICENSEE AGREES THAT LICENSOR HAS MADE NO EXPRESS WARRANTIES REGARDING THE SOFTWARE AND THAT THE SOFTWARE IS BEING PROVIDED \"AS IS\" WITHOUT WARRANTY OF ANY KIND. LICENSOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THE SOFTWARE, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION, ANY IMPLIED WARRANTIES OF FITNESS FOR A PARTICULAR PURPOSE; TITLE; MERCHANTABILITY;  OR NON-INFRINGEMENT OF THIRD PARTY RIGHTS. LICENSOR DOES NOT WARRANT THAT THE SOFTWARE WILL OPERATE UNINTERRUPTED OR ERROR FREE, OR THAT ALL ERRORS WILL BE CORRECTED.  LICENSOR DOES NOT GUARANTEE ANY PARTICULAR RESULTS FROM THE USE OF THE SOFTWARE, AND DOES NOT WARRANT THAT THE SOFTWARE IS FIT FOR ANY PARTICULAR PURPOSE.\n* LIMITATION OF LIABILITY:  TO THE FULLEST EXTENT PERMISSIBLE UNDER APPLICABLE LAW, IN NO EVENT WILL LICENSOR AND/OR ITS AFFILIATES, EMPLOYEES, OFFICERS AND DIRECTORS BE LIABLE TO LICENSEE FOR (I) ANY LOSS OF USE OR DATA; INTERRUPTION OF BUSINESS; OR ANY INDIRECT; SPECIAL; INCIDENTAL; OR CONSEQUENTIAL DAMAGES OF ANY KIND (INCLUDING LOST PROFITS); AND (II) ANY DIRECT DAMAGES EXCEEDING THE TOTAL AMOUNT OF ONE THOUSAND US DOLLARS ($1,000).  THE FOREGOING PROVISIONS LIMITING THE LIABILITY OF LICENSOR SHALL APPLY REGARDLESS OF THE FORM OR CAUSE OF ACTION, WHETHER IN STRICT LIABILITY, CONTRACT OR TORT.\n\n**Proprietary Rights; No Other Rights**\n\n* **Ownership:** Licensor retains sole and exclusive ownership of all rights, interests and title in the Software and any scripts, processes, techniques, methodologies, inventions, know-how, concepts, formatting, arrangements, visual attributes, ideas, database rights, copyrights, patents, trade secrets, and other intellectual property related thereto, and all derivatives, enhancements, modifications and improvements thereof. Except for the limited license rights granted herein, Licensee has no rights in or to the Software and/ or Licensors trademarks, logo, or branding and You acknowledge that such Software, trademarks, logo, or branding is the sole property of Licensor.\n* **Feedback:** Licensee is not required to provide any suggestions, enhancement requests, recommendations or other feedback regarding the Software (\"Feedback\").  If, notwithstanding this policy, Licensee submits Feedback, Licensee understands and acknowledges that such Feedback is not submitted in confidence and Licensor assumes no obligation, expressed or implied, by considering it.  All right in any trademark or logo of Licensor or its affiliates and You shall make no claim of right to the Software or any part thereof to be supplied by Licensor hereunder and acknowledges that as between Licensor and You, such Software is the sole proprietary, title and interest in and to Licensor.such Feedback shall be assigned to, and shall become the sole and exclusive property of, Licensor upon its creation.\n* Except for the rights expressly granted to You under this Agreement, You are not granted any other licenses or rights in the Software or otherwise. This Agreement constitutes the entire agreement between the You and the Licensor with respect to the subject matter hereof and supersedes all prior or contemporaneous communications, representations, or agreements, whether oral or written.\n* **Third-Party Software:** Customer acknowledges that the Software may contain open and closed source components (OSS Components) that are governed separately by certain licenses, in each case as further provided by Company upon request. Any applicable OSS Component license is solely between Licensee and the applicable licensor of the OSS Component and Licensee shall comply with the applicable OSS Component license.\n* If any provision of this Agreement is held to be invalid or unenforceable, such provision shall be struck and the remaining provisions shall remain in full force and effect.\n\n**Miscellaneous**\n\n* **Miscellaneous:** This Agreement may be modified at any time by Licensor, and constitutes the entire agreement between the parties with respect to the subject matter hereof. Licensee may not assign or subcontract its rights or obligations under this Agreement.  This Agreement does not, and shall not be construed to create any relationship, partnership, joint venture, employer-employee, agency, or franchisor-franchisee relationship between the parties.\n* **Governing Law & Jurisdiction:** This Agreement shall be governed and construed in accordance with the laws of Israel, without giving effect to their respective conflicts of laws provisions, and the competent courts situated in Tel Aviv, Israel, shall have sole and exclusive jurisdiction over the parties and any conflict and/or dispute arising out of, or in connection to, this Agreement\n\n\\[*End of ScyllaDB Software License Agreement*\\]\n\n"
        },
        {
          "name": "NOTICE.txt",
          "type": "blob",
          "size": 0.720703125,
          "content": "This project includes code developed by the Apache Software Foundation (http://www.apache.org/),\nespecially Apache Cassandra.\n\nIt includes files from https://github.com/antonblanchard/crc32-vpmsum (author Anton Blanchard <anton@au.ibm.com>, IBM).\nThese files are located in utils/arch/powerpc/crc32-vpmsum. Their license may be found in licenses/LICENSE-crc32-vpmsum.TXT.\n\nIt includes modified code from https://gitbox.apache.org/repos/asf?p=cassandra-dtest.git (owned by The Apache Software Foundation)\n\nIt includes modified tests from https://github.com/etcd-io/etcd.git (owned by The etcd Authors)\n\nIt includes files from https://github.com/bytecodealliance/wasmtime-cpp (owned by Bytecode Alliance), licensed with Apache License 2.0.\n"
        },
        {
          "name": "ORIGIN",
          "type": "blob",
          "size": 4.5615234375,
          "content": "http://git-wip-us.apache.org/repos/asf/cassandra.git trunk (bf599fb5b062cbcc652da78b7d699e7a01b949ad)\n\nimport = bf599fb5b062cbcc652da78b7d699e7a01b949ad\nY      = Already in scylla\n\n$ git log --oneline import..cassandra-2.1.11 -- gms/\nY  484e645 Mark node as dead even if already left\n   d0c166f Add trampled commit back\n   ba5837e Merge branch 'cassandra-2.0' into cassandra-2.1\n   718e47f Forgot a damn c/r\n   a7282e4 Merge branch 'cassandra-2.0' into cassandra-2.1\nY  ae4cd69 Print versions for gossip states in gossipinfo.\nY  7fba3d2 Don't mark nodes down before the max local pause interval once paused.\n   c2142e6 Merge branch 'cassandra-2.0' into cassandra-2.1\n   ba9a69e checkForEndpointCollision fails for legitimate collisions, finalized list of statuses and nits, CASSANDRA-9765\n   54470a2 checkForEndpointCollision fails for legitimate collisions, improved version after CR, CASSANDRA-9765\n   2c9b490 checkForEndpointCollision fails for legitimate collisions, CASSANDRA-9765\n   4c15970 Merge branch 'cassandra-2.0' into cassandra-2.1\n   ad8047a ArrivalWindow should use primitives\nY  4012134 Failure detector detects and ignores local pauses\n   9bcdd0f Merge branch 'cassandra-2.0' into cassandra-2.1\n   cefaa4e Close incoming connections when MessagingService is stopped\n   ea1beda Merge branch 'cassandra-2.0' into cassandra-2.1\n   08dbbd6 Ignore gossip SYNs after shutdown\n   3c17ac6 Merge branch 'cassandra-2.0' into cassandra-2.1\n   a64bc43 lists work better when you initialize them\n   543a899 change list to arraylist\n   730d4d4 Merge branch 'cassandra-2.0' into cassandra-2.1\n   e3e2de0 change list to arraylist\n   f7884c5 Merge branch 'cassandra-2.0' into cassandra-2.1\nY  84b2846 remove redundant state\n   4f2c372 Merge branch 'cassandra-2.0' into cassandra-2.1\nY  b2c62bb Add shutdown gossip state to prevent timeouts during rolling restarts\nY  def4835 Add missing follow on fix for 7816 only applied to cassandra-2.1 branch in 763130bdbde2f4cec2e8973bcd5203caf51cc89f\nY  763130b Followup commit for 7816\n   1376b8e Merge branch 'cassandra-2.0' into cassandra-2.1\nY  2199a87 Fix duplicate up/down messages sent to native clients\n   136042e Merge branch 'cassandra-2.0' into cassandra-2.1\nY  eb9c5bb Improve FD logging when the arrival time is ignored.\n\n$ git log --oneline import..cassandra-2.1.11 -- service/StorageService.java\n   92c5787 Keep StorageServiceMBean interface stable\n   6039d0e Fix DC and Rack in nodetool info\n   a2f0da0 Merge branch 'cassandra-2.0' into cassandra-2.1\n   c4de752 Follow-up to CASSANDRA-10238\n   e889ee4 2i key cache load fails\n   4b1d59e Merge branch 'cassandra-2.0' into cassandra-2.1\n   257cdaa Fix consolidating racks violating the RF contract\nY  27754c0 refuse to decomission if not in state NORMAL patch by Jan Karlsson and Stefania for CASSANDRA-8741\nY  5bc56c3 refuse to decomission if not in state NORMAL patch by Jan Karlsson and Stefania for CASSANDRA-8741\nY  8f9ca07 Cannot replace token does not exist - DN node removed as Fat Client\n   c2142e6 Merge branch 'cassandra-2.0' into cassandra-2.1\n   54470a2 checkForEndpointCollision fails for legitimate collisions, improved version after CR, CASSANDRA-9765\n   1eccced Handle corrupt files on startup\n   2c9b490 checkForEndpointCollision fails for legitimate collisions, CASSANDRA-9765\n   c4b5260 Merge branch 'cassandra-2.0' into cassandra-2.1\nY  52dbc3f Can't transition from write survey to normal mode\n   9966419 Make rebuild only run one at a time\n   d693ca1 Merge branch 'cassandra-2.0' into cassandra-2.1\n   be9eff5 Add option to not validate atoms during scrub\n   2a4daaf followup fix for 8564\n   93478ab Wait for anticompaction to finish\n   9e9846e Fix for harmless exceptions being logged as ERROR\n   6d06f32 Fix anticompaction blocking ANTI_ENTROPY stage\n   4f2c372 Merge branch 'cassandra-2.0' into cassandra-2.1\nY  b2c62bb Add shutdown gossip state to prevent timeouts during rolling restarts\nY  cba1b68 Fix failed bootstrap/replace attempts being persisted in system.peers\n   f59df28 Allow takeColumnFamilySnapshot to take a list of tables patch by Sachin Jarin; reviewed by Nick Bailey for CASSANDRA-8348\nY  ac46747 Fix failed bootstrap/replace attempts being persisted in system.peers\n   5abab57 Merge branch 'cassandra-2.0' into cassandra-2.1\n   0ff9c3c Allow reusing snapshot tags across different column families.\n   f9c57a5 Merge branch 'cassandra-2.0' into cassandra-2.1\nY  b296c55 Fix MOVED_NODE client event\n   bbb3fc7 Merge branch 'cassandra-2.0' into cassandra-2.1\n   37eb2a0 Fix NPE in nodetool getendpoints with bad ks/cf\n   f8b43d4 Merge branch 'cassandra-2.0' into cassandra-2.1\n   e20810c Remove C* specific class from JMX API\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.1796875,
          "content": "# Scylla\n\n[![Slack](https://img.shields.io/badge/slack-scylla-brightgreen.svg?logo=slack)](http://slack.scylladb.com)\n[![Twitter](https://img.shields.io/twitter/follow/ScyllaDB.svg?style=social&label=Follow)](https://twitter.com/intent/follow?screen_name=ScyllaDB)\n\n## What is Scylla?\n\nScylla is the real-time big data database that is API-compatible with Apache Cassandra and Amazon DynamoDB.\nScylla embraces a shared-nothing approach that increases throughput and storage capacity to realize order-of-magnitude performance improvements and reduce hardware costs.\n\nFor more information, please see the [ScyllaDB web site].\n\n[ScyllaDB web site]: https://www.scylladb.com\n\n## Build Prerequisites\n\nScylla is fairly fussy about its build environment, requiring very recent\nversions of the C++23 compiler and of many libraries to build. The document\n[HACKING.md](HACKING.md) includes detailed information on building and\ndeveloping Scylla, but to get Scylla building quickly on (almost) any build\nmachine, Scylla offers a [frozen toolchain](tools/toolchain/README.md),\nThis is a pre-configured Docker image which includes recent versions of all\nthe required compilers, libraries and build tools. Using the frozen toolchain\nallows you to avoid changing anything in your build machine to meet Scylla's\nrequirements - you just need to meet the frozen toolchain's prerequisites\n(mostly, Docker or Podman being available).\n\n## Building Scylla\n\nBuilding Scylla with the frozen toolchain `dbuild` is as easy as:\n\n```bash\n$ git submodule update --init --force --recursive\n$ ./tools/toolchain/dbuild ./configure.py\n$ ./tools/toolchain/dbuild ninja build/release/scylla\n```\n\nFor further information, please see:\n\n* [Developer documentation] for more information on building Scylla.\n* [Build documentation] on how to build Scylla binaries, tests, and packages.\n* [Docker image build documentation] for information on how to build Docker images.\n\n[developer documentation]: HACKING.md\n[build documentation]: docs/dev/building.md\n[docker image build documentation]: dist/docker/debian/README.md\n\n## Running Scylla\n\nTo start Scylla server, run:\n\n```bash\n$ ./tools/toolchain/dbuild ./build/release/scylla --workdir tmp --smp 1 --developer-mode 1\n```\n\nThis will start a Scylla node with one CPU core allocated to it and data files stored in the `tmp` directory.\nThe `--developer-mode` is needed to disable the various checks Scylla performs at startup to ensure the machine is configured for maximum performance (not relevant on development workstations).\nPlease note that you need to run Scylla with `dbuild` if you built it with the frozen toolchain.\n\nFor more run options, run:\n\n```bash\n$ ./tools/toolchain/dbuild ./build/release/scylla --help\n```\n\n## Testing\n\n[![Build with the latest Seastar](https://github.com/scylladb/scylladb/actions/workflows/seastar.yaml/badge.svg)](https://github.com/scylladb/scylladb/actions/workflows/seastar.yaml) [![Check Reproducible Build](https://github.com/scylladb/scylladb/actions/workflows/reproducible-build.yaml/badge.svg)](https://github.com/scylladb/scylladb/actions/workflows/reproducible-build.yaml) [![clang-nightly](https://github.com/scylladb/scylladb/actions/workflows/clang-nightly.yaml/badge.svg)](https://github.com/scylladb/scylladb/actions/workflows/clang-nightly.yaml)\n\nSee [test.py manual](docs/dev/testing.md).\n\n## Scylla APIs and compatibility\nBy default, Scylla is compatible with Apache Cassandra and its API - CQL.\nThere is also support for the API of Amazon DynamoDB,\nwhich needs to be enabled and configured in order to be used. For more\ninformation on how to enable the DynamoDB API in Scylla,\nand the current compatibility of this feature as well as Scylla-specific extensions, see\n[Alternator](docs/alternator/alternator.md) and\n[Getting started with Alternator](docs/alternator/getting-started.md).\n\n## Documentation\n\nDocumentation can be found [here](docs/dev/README.md).\nSeastar documentation can be found [here](http://docs.seastar.io/master/index.html).\nUser documentation can be found [here](https://docs.scylladb.com/).\n\n## Training\n\nTraining material and online courses can be found at [Scylla University](https://university.scylladb.com/).\nThe courses are free, self-paced and include hands-on examples. They cover a variety of topics including Scylla data modeling,\nadministration, architecture, basic NoSQL concepts, using drivers for application development, Scylla setup, failover, compactions,\nmulti-datacenters and how Scylla integrates with third-party applications.\n\n## Contributing to Scylla\n\nIf you want to report a bug or submit a pull request or a patch, please read the [contribution guidelines].\n\nIf you are a developer working on Scylla, please read the [developer guidelines].\n\n[contribution guidelines]: CONTRIBUTING.md\n[developer guidelines]: HACKING.md\n\n## Contact\n\n* The [community forum] and [Slack channel] are for users to discuss configuration, management, and operations of the ScyllaDB open source.\n* The [developers mailing list] is for developers and people interested in following the development of ScyllaDB to discuss technical topics.\n\n[Community forum]: https://forum.scylladb.com/\n\n[Slack channel]: http://slack.scylladb.com/\n\n[Developers mailing list]: https://groups.google.com/forum/#!forum/scylladb-dev\n"
        },
        {
          "name": "SCYLLA-VERSION-GEN",
          "type": "blob",
          "size": 2.98046875,
          "content": "#!/bin/sh\n\nUSAGE=$(cat <<-END\nUsage: $(basename \"$0\") [-h|--help] [-o|--output-dir PATH] [--date-stamp DATE] -- generate Scylla version and build information files.\n\nOptions:\n  -h|--help show this help message.\n  -o|--output-dir PATH specify destination path at which the version files are to be created.\n  -d|--date-stamp DATE manually set date for release parameter\n  -v|--verbose also print out the version number\n\nBy default, the script will attempt to parse 'version' file\nin the current directory, which should contain a string of\n'\\$version-\\$release' form.\n\nOtherwise, it will call 'git log' on the source tree (the\ndirectory, which contains the script) to obtain current\ncommit hash and use it for building the version and release\nstrings.\n\nThe script assumes that it's called from the Scylla source\ntree.\n\nThe files created are:\n  SCYLLA-VERSION-FILE\n  SCYLLA-RELEASE-FILE\n  SCYLLA-PRODUCT-FILE\n\nBy default, these files are created in the 'build'\nsubdirectory under the directory containing the script.\nThe destination directory can be overridden by\nusing '-o PATH' option.\nEND\n)\n\nDATE=\"\"\nPRINT_VERSION=false\n\nwhile [ $# -gt 0 ]; do\n\topt=\"$1\"\n\tcase $opt in\n\t\t-h|--help)\n\t\t\techo \"$USAGE\"\n\t\t\texit 0\n\t\t\t;;\n\t\t-o|--output-dir)\n\t\t\tOUTPUT_DIR=\"$2\"\n\t\t\tshift\n\t\t\tshift\n\t\t\t;;\n\t\t--date-stamp)\n\t\t\tDATE=\"$2\"\n\t\t\tshift\n\t\t\tshift\n\t\t\t;;\n\t\t-v|--verbose)\n\t\t\tPRINT_VERSION=true\n\t\t\tshift\n\t\t\t;;\n\t\t*)\n\t\t\techo \"Unexpected argument found: $1\"\n\t\t\techo\n\t\t\techo \"$USAGE\"\n\t\t\texit 1\n\t\t\t;;\n\tesac\ndone\n\nSCRIPT_DIR=\"$(dirname \"$0\")\"\n\nif [ -z \"$OUTPUT_DIR\" ]; then\n\tOUTPUT_DIR=\"$SCRIPT_DIR/build\"\nfi\n\nif [ -z \"$DATE\" ]; then\n  DATE=$(date --utc +%Y%m%d)\nfi\n\n# Default scylla product/version tags\nPRODUCT=scylla\nVERSION=6.3.0-dev\n\nif test -f version\nthen\n\tSCYLLA_VERSION=$(cat version | awk -F'-' '{print $1}')\n\tSCYLLA_RELEASE=$(cat version | awk -F'-' '{print $2}')\nelse\n\tSCYLLA_VERSION=$VERSION\n\tif [ -z \"$SCYLLA_RELEASE\" ]; then\n\t\tGIT_COMMIT=$(git -C \"$SCRIPT_DIR\" log --pretty=format:'%h' -n 1 --abbrev=12)\n\t\t# For custom package builds, replace \"0\" with \"counter.yourname\",\n\t\t# where counter starts at 1 and increments for successive versions.\n\t\t# This ensures that the package manager will select your custom\n\t\t# package over the standard release.\n\t\t# Do not use any special characters like - or _ in the name above!\n\t\t# These characters either have special meaning or are illegal in\n\t\t# version strings.\n\t\tSCYLLA_BUILD=0\n\t\tSCYLLA_RELEASE=$SCYLLA_BUILD.$DATE.$GIT_COMMIT\n\telif [ -f \"$OUTPUT_DIR/SCYLLA-RELEASE-FILE\" ]; then\n\t\techo \"setting SCYLLA_RELEASE only makes sense in clean builds\" 1>&2\n\t\texit 1\n\tfi\nfi\n\nif [ -f \"$OUTPUT_DIR/SCYLLA-RELEASE-FILE\" ]; then\n\tGIT_COMMIT_FILE=$(cat \"$OUTPUT_DIR/SCYLLA-RELEASE-FILE\" | rev | cut -d . -f 1 | rev)\n\tif [ \"$GIT_COMMIT\" = \"$GIT_COMMIT_FILE\" ]; then\n\t\texit 0\n\tfi\nfi\n\nif $PRINT_VERSION; then\n\techo \"$SCYLLA_VERSION-$SCYLLA_RELEASE\"\nfi\nmkdir -p \"$OUTPUT_DIR\"\necho \"$SCYLLA_VERSION\" > \"$OUTPUT_DIR/SCYLLA-VERSION-FILE\"\necho \"$SCYLLA_RELEASE\" > \"$OUTPUT_DIR/SCYLLA-RELEASE-FILE\"\necho \"$PRODUCT\" > \"$OUTPUT_DIR/SCYLLA-PRODUCT-FILE\"\n"
        },
        {
          "name": "abseil",
          "type": "commit",
          "content": null
        },
        {
          "name": "absl-flat_hash_map.cc",
          "type": "blob",
          "size": 0.2685546875,
          "content": "/*\n * Copyright (C) 2020-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"absl-flat_hash_map.hh\"\n\nsize_t sstring_hash::operator()(std::string_view v) const noexcept {\n    return absl::Hash<std::string_view>{}(v);\n}\n"
        },
        {
          "name": "absl-flat_hash_map.hh",
          "type": "blob",
          "size": 0.7529296875,
          "content": "/*\n * Copyright (C) 2020-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <absl/container/flat_hash_map.h>\n#include <seastar/core/sstring.hh>\n\nusing namespace seastar;\n\nstruct sstring_hash {\n    using is_transparent = void;\n    size_t operator()(std::string_view v) const noexcept;\n};\n\nstruct sstring_eq {\n    using is_transparent = void;\n    bool operator()(std::string_view a, std::string_view b) const noexcept {\n        return a == b;\n    }\n};\n\ntemplate <typename K, typename V, typename... Ts>\nstruct flat_hash_map : public absl::flat_hash_map<K, V, Ts...> {\n};\n\ntemplate <typename V>\nstruct flat_hash_map<sstring, V>\n    : public absl::flat_hash_map<sstring, V, sstring_hash, sstring_eq> {};\n"
        },
        {
          "name": "alternator",
          "type": "tree",
          "content": null
        },
        {
          "name": "amplify.yml",
          "type": "blob",
          "size": 0.2744140625,
          "content": "version: 1\napplications:\n  - frontend:\n      phases:\n        build:\n          commands:\n            - make setupenv\n            - make dirhtml\n      artifacts:\n        baseDirectory: _build/dirhtml\n        files:\n          - '**/*'\n      cache:\n        paths: []\n    appRoot: docs\n"
        },
        {
          "name": "api",
          "type": "tree",
          "content": null
        },
        {
          "name": "auth",
          "type": "tree",
          "content": null
        },
        {
          "name": "backlog_controller.hh",
          "type": "blob",
          "size": 5.2294921875,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n#include <seastar/core/scheduling.hh>\n#include <seastar/core/timer.hh>\n#include <seastar/core/gate.hh>\n#include <seastar/core/file.hh>\n#include <chrono>\n#include <cmath>\n\n#include \"seastarx.hh\"\n\n// Simple proportional controller to adjust shares for processes for which a backlog can be clearly\n// defined.\n//\n// Goal is to consume the backlog as fast as we can, but not so fast that we steal all the CPU from\n// incoming requests, and at the same time minimize user-visible fluctuations in the quota.\n//\n// What that translates to is we'll try to keep the backlog's first derivative at 0 (IOW, we keep\n// backlog constant). As the backlog grows we increase CPU usage, decreasing CPU usage as the\n// backlog diminishes.\n//\n// The exact point at which the controller stops determines the desired CPU usage. As the backlog\n// grows and approach a maximum desired, we need to be more aggressive. We will therefore define two\n// thresholds, and increase the constant as we cross them.\n//\n// Doing that divides the range in three (before the first, between first and second, and after\n// second threshold), and we'll be slow to grow in the first region, grow normally in the second\n// region, and aggressively in the third region.\n//\n// The constants q1 and q2 are used to determine the proportional factor at each stage.\nclass backlog_controller {\npublic:\n    using scheduling_group = seastar::scheduling_group;\n\n    future<> shutdown() {\n        _update_timer.cancel();\n        return std::move(_inflight_update);\n    }\n\n    future<> update_static_shares(float static_shares) {\n        _static_shares = static_shares;\n        return make_ready_future<>();\n    }\n\nprotected:\n    struct control_point {\n        float input;\n        float output;\n    };\n\n    scheduling_group _scheduling_group;\n\n    std::vector<control_point> _control_points;\n\n    std::function<float()> _current_backlog;\n    timer<> _update_timer;\n    // updating shares for an I/O class may contact another shard and returns a future.\n    future<> _inflight_update;\n\n    // Used when the controllers are disabled and a static share is used\n    // When that option is deprecated we should remove this.\n    float _static_shares;\n\n    virtual void update_controller(float quota);\n\n    bool controller_disabled() const noexcept {\n        return _static_shares > 0;\n    }\n\n    void adjust();\n\n    backlog_controller(scheduling_group sg, std::chrono::milliseconds interval,\n                       std::vector<control_point> control_points, std::function<float()> backlog,\n                       float static_shares = 0)\n        : _scheduling_group(std::move(sg))\n        , _control_points()\n        , _current_backlog(std::move(backlog))\n        , _update_timer([this] { adjust(); })\n        , _inflight_update(make_ready_future<>())\n        , _static_shares(static_shares)\n    {\n        _control_points.insert(_control_points.end(), control_points.begin(), control_points.end());\n        _update_timer.arm_periodic(interval);\n    }\n\n    virtual ~backlog_controller() {}\npublic:\n    backlog_controller(backlog_controller&&) = default;\n    float backlog_of_shares(float shares) const;\n};\n\n// memtable flush CPU controller.\n//\n// - First threshold is the soft limit line,\n// - Maximum is the point in which we'd stop consuming request,\n// - Second threshold is halfway between them.\n//\n// Below the soft limit, we are in no particular hurry to flush, since it means we're set to\n// complete flushing before we a new memtable is ready. The quota is dirty * q1, and q1 is set to a\n// low number.\n//\n// The first half of the virtual dirty region is where we expect to be usually, so we have a low\n// slope corresponding to a sluggish response between q1 * soft_limit and q2.\n//\n// In the second half, we're getting close to the hard dirty limit so we increase the slope and\n// become more responsive, up to a maximum quota of qmax.\nclass flush_controller : public backlog_controller {\n    static constexpr float hard_dirty_limit = 1.0f;\npublic:\n    flush_controller(backlog_controller::scheduling_group sg, float static_shares, std::chrono::milliseconds interval, float soft_limit, std::function<float()> current_dirty)\n        : backlog_controller(std::move(sg), std::move(interval),\n          std::vector<backlog_controller::control_point>({{0.0, 0.0}, {soft_limit, 10}, {soft_limit + (hard_dirty_limit - soft_limit) / 2, 200} , {hard_dirty_limit, 1000}}),\n          std::move(current_dirty),\n          static_shares\n        )\n    {}\n};\n\nclass compaction_controller : public backlog_controller {\npublic:\n    static constexpr unsigned normalization_factor = 30;\n    static constexpr float disable_backlog = std::numeric_limits<double>::infinity();\n    static constexpr float backlog_disabled(float backlog) { return std::isinf(backlog); }\n    compaction_controller(backlog_controller::scheduling_group sg, float static_shares, std::chrono::milliseconds interval, std::function<float()> current_backlog)\n        : backlog_controller(std::move(sg), std::move(interval),\n          std::vector<backlog_controller::control_point>({{0.0, 50}, {1.5, 100} , {normalization_factor, 1000}}),\n          std::move(current_backlog),\n          static_shares\n        )\n    {}\n};\n"
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "build_mode.hh",
          "type": "blob",
          "size": 1.9970703125,
          "content": "\n/*\n * Copyright (C) 2022-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#ifndef SCYLLA_BUILD_MODE\n#error SCYLLA_BUILD_MODE must be defined\n#endif\n\n#ifndef STRINGIFY\n// We need two levels of indirection\n// to make a string out of the macro name.\n// The outer level expands the macro\n// and the inner level makes a string out of the expanded macro.\n#define STRINGIFY_VALUE(x) #x\n#define STRINGIFY_MACRO(x) STRINGIFY_VALUE(x)\n#endif\n\n#define SCYLLA_BUILD_MODE_STR STRINGIFY_MACRO(SCYLLA_BUILD_MODE)\n\n// We use plain macro definitions\n// so the preprocessor can expand them\n// inline in the #if directives below\n#define SCYLLA_BUILD_MODE_CODE_debug 0\n#define SCYLLA_BUILD_MODE_CODE_release 1\n#define SCYLLA_BUILD_MODE_CODE_dev 2\n#define SCYLLA_BUILD_MODE_CODE_sanitize 3\n#define SCYLLA_BUILD_MODE_CODE_coverage 4\n\n#define _SCYLLA_BUILD_MODE_CODE(sbm) SCYLLA_BUILD_MODE_CODE_ ## sbm\n#define SCYLLA_BUILD_MODE_CODE(sbm) _SCYLLA_BUILD_MODE_CODE(sbm)\n\n#if SCYLLA_BUILD_MODE_CODE(SCYLLA_BUILD_MODE) == SCYLLA_BUILD_MODE_CODE_debug\n#define SCYLLA_BUILD_MODE_DEBUG\n#elif SCYLLA_BUILD_MODE_CODE(SCYLLA_BUILD_MODE) == SCYLLA_BUILD_MODE_CODE_release\n#define SCYLLA_BUILD_MODE_RELEASE\n#elif SCYLLA_BUILD_MODE_CODE(SCYLLA_BUILD_MODE) == SCYLLA_BUILD_MODE_CODE_dev\n#define SCYLLA_BUILD_MODE_DEV\n#elif SCYLLA_BUILD_MODE_CODE(SCYLLA_BUILD_MODE) == SCYLLA_BUILD_MODE_CODE_sanitize\n#define SCYLLA_BUILD_MODE_SANITIZE\n#elif SCYLLA_BUILD_MODE_CODE(SCYLLA_BUILD_MODE) == SCYLLA_BUILD_MODE_CODE_coverage\n#define SCYLLA_BUILD_MODE_COVERAGE\n#else\n#error unrecognized SCYLLA_BUILD_MODE\n#endif\n\n#if (defined(SCYLLA_BUILD_MODE_RELEASE) || defined(SCYLLA_BUILD_MODE_DEV)) && defined(SEASTAR_DEBUG)\n#error SEASTAR_DEBUG is not expected to be defined when SCYLLA_BUILD_MODE is \"release\" or \"dev\"\n#endif\n\n#if (defined(SCYLLA_BUILD_MODE_DEBUG) || defined(SCYLLA_BUILD_MODE_SANITIZE)) && !defined(SEASTAR_DEBUG)\n#error SEASTAR_DEBUG is expected to be defined when SCYLLA_BUILD_MODE is \"debug\" or \"sanitize\"\n#endif\n"
        },
        {
          "name": "bytes.cc",
          "type": "blob",
          "size": 1.7783203125,
          "content": "/*\n * Copyright (C) 2014-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"bytes.hh\"\n#include <fmt/ostream.h>\n#include <seastar/core/format.hh>\n\nstatic inline int8_t hex_to_int(unsigned char c) {\n    switch (c) {\n        case '0': return 0;\n        case '1': return 1;\n        case '2': return 2;\n        case '3': return 3;\n        case '4': return 4;\n        case '5': return 5;\n        case '6': return 6;\n        case '7': return 7;\n        case '8': return 8;\n        case '9': return 9;\n        case 'a': case 'A': return 10;\n        case 'b': case 'B': return 11;\n        case 'c': case 'C': return 12;\n        case 'd': case 'D': return 13;\n        case 'e': case 'E': return 14;\n        case 'f': case 'F': return 15;\n        default:\n            return -1;\n    }\n}\n\nbytes from_hex(std::string_view s) {\n    if (s.length() % 2 == 1) {\n        throw std::invalid_argument(\"An hex string representing bytes must have an even length\");\n    }\n    bytes out{bytes::initialized_later(), s.length() / 2};\n    unsigned end = out.size();\n    for (unsigned i = 0; i != end; i++) {\n        auto half_byte1 = hex_to_int(s[i * 2]);\n        auto half_byte2 = hex_to_int(s[i * 2 + 1]);\n        if (half_byte1 == -1 || half_byte2 == -1) {\n            throw std::invalid_argument(fmt::format(\"Non-hex characters in {}\", s));\n        }\n        out[i] = (half_byte1 << 4) | half_byte2;\n    }\n    return out;\n}\n\nsstring to_hex(bytes_view b) {\n    return fmt::to_string(fmt_hex(b));\n}\n\nsstring to_hex(const bytes& b) {\n    return to_hex(bytes_view(b));\n}\n\nsstring to_hex(const bytes_opt& b) {\n    return !b ? \"null\" : to_hex(*b);\n}\n\nnamespace std {\n\nstd::ostream& operator<<(std::ostream& os, const bytes_view& b) {\n    fmt::print(os, \"{}\", fmt_hex(b));\n    return os;\n}\n\n}\n"
        },
        {
          "name": "bytes.hh",
          "type": "blob",
          "size": 5.0029296875,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"seastarx.hh\"\n#include <fmt/format.h>\n#include <seastar/core/sstring.hh>\n#include \"utils/hashing.hh\"\n#include <optional>\n#include <iosfwd>\n#include <functional>\n#include <compare>\n#include \"bytes_fwd.hh\"\n#include \"utils/mutable_view.hh\"\n#include \"utils/simple_hashers.hh\"\n\ninline bytes to_bytes(bytes&& b) {\n    return std::move(b);\n}\n\ninline std::string_view to_string_view(bytes_view view) {\n    return {reinterpret_cast<const char*>(view.data()), view.size()};\n}\n\ninline bytes_view to_bytes_view(std::string_view view) {\n    return {reinterpret_cast<const int8_t*>(view.data()), view.size()};\n}\n\nstruct fmt_hex {\n    const bytes_view& v;\n    fmt_hex(const bytes_view& v) noexcept : v(v) {}\n};\n\nbytes from_hex(std::string_view s);\nsstring to_hex(bytes_view b);\nsstring to_hex(const bytes& b);\nsstring to_hex(const bytes_opt& b);\n\ntemplate <>\nstruct fmt::formatter<fmt_hex> {\n    size_t _group_size_in_bytes = 0;\n    char _delimiter = ' ';\npublic:\n    // format_spec := [group_size[delimiter]]\n    // group_size := a char from '0' to '9'\n    // delimiter := a char other than '{'  or '}'\n    //\n    // by default, the given bytes are printed without delimiter, just\n    // like a string. so a string view of {0x20, 0x01, 0x0d, 0xb8} is\n    // printed like:\n    // \"20010db8\".\n    //\n    // but the format specifier can be used to customize how the bytes\n    // are printed. for instance, to print an bytes_view like IPv6. so\n    // the format specfier would be \"{:2:}\", where\n    // - \"2\": bytes are printed in groups of 2 bytes\n    // - \":\": each group is delimited by \":\"\n    // and the formatted output will look like:\n    // \"2001:0db8:0000\"\n    //\n    // or we can mimic how the default format of used by hexdump using\n    // \"{:2 }\", where\n    // - \"2\": bytes are printed in group of 2 bytes\n    // - \" \": each group is delimited by \" \"\n    // and the formatted output will look like:\n    // \"2001 0db8 0000\"\n    //\n    // or we can just print each bytes and separate them by a dash using\n    // \"{:1-}\"\n    // and the formatted output will look like:\n    // \"20-01-0b-b8-00-00\"\n    constexpr auto parse(fmt::format_parse_context& ctx) {\n        // get the delimiter if any\n        auto it = ctx.begin();\n        auto end = ctx.end();\n        if (it != end && *it != '}') {\n            int group_size = *it++ - '0';\n            if (group_size < 0 ||\n                static_cast<size_t>(group_size) > sizeof(uint64_t)) {\n                throw format_error(\"invalid group_size\");\n            }\n            _group_size_in_bytes = group_size;\n            if (it != end) {\n                // optional delimiter\n                _delimiter = *it++;\n            }\n        }\n        if (it != end && *it != '}') {\n            throw format_error(\"invalid format\");\n        }\n        return it;\n    }\n    template <typename FormatContext>\n    auto format(const ::fmt_hex& s, FormatContext& ctx) const {\n        auto out = ctx.out();\n        const auto& v = s.v;\n        if (_group_size_in_bytes > 0) {\n            for (size_t i = 0, size = v.size(); i < size; i++) {\n                if (i != 0 && i % _group_size_in_bytes == 0) {\n                    fmt::format_to(out, \"{}{:02x}\", _delimiter, std::byte(v[i]));\n                } else {\n                    fmt::format_to(out, \"{:02x}\", std::byte(v[i]));\n                }\n            }\n        } else {\n            for (auto b : v) {\n                fmt::format_to(out, \"{:02x}\", std::byte(b));\n            }\n        }\n        return out;\n    }\n};\n\ntemplate <>\nstruct fmt::formatter<bytes> : fmt::formatter<fmt_hex> {\n    template <typename FormatContext>\n    auto format(const ::bytes& s, FormatContext& ctx) const {\n        return fmt::formatter<::fmt_hex>::format(::fmt_hex(bytes_view(s)), ctx);\n    }\n};\n\nnamespace std {\n\n// Must be in std:: namespace, or ADL fails\nstd::ostream& operator<<(std::ostream& os, const bytes_view& b);\n\n}\n\ntemplate<>\nstruct appending_hash<bytes> {\n    template<typename Hasher>\n    void operator()(Hasher& h, const bytes& v) const {\n        feed_hash(h, v.size());\n        h.update(reinterpret_cast<const char*>(v.cbegin()), v.size() * sizeof(bytes::value_type));\n    }\n};\n\ntemplate<>\nstruct appending_hash<bytes_view> {\n    template<typename Hasher>\n    void operator()(Hasher& h, bytes_view v) const {\n        feed_hash(h, v.size());\n        h.update(reinterpret_cast<const char*>(v.begin()), v.size() * sizeof(bytes_view::value_type));\n    }\n};\n\nusing bytes_view_hasher = simple_xx_hasher;\n\nnamespace std {\ntemplate <>\nstruct hash<bytes_view> {\n    size_t operator()(bytes_view v) const {\n        bytes_view_hasher h;\n        appending_hash<bytes_view>{}(h, v);\n        return h.finalize();\n    }\n};\n} // namespace std\n\ninline std::strong_ordering compare_unsigned(bytes_view v1, bytes_view v2) {\n  auto size = std::min(v1.size(), v2.size());\n  if (size) {\n    auto n = memcmp(v1.begin(), v2.begin(), size);\n    if (n) {\n        return n <=> 0;\n    }\n  }\n    return v1.size() <=> v2.size();\n}\n"
        },
        {
          "name": "bytes_fwd.hh",
          "type": "blob",
          "size": 0.443359375,
          "content": "/*\n * Copyright (C) 2024-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <seastar/core/sstring.hh>\n\n#include \"utils/mutable_view.hh\"\n\nusing namespace seastar;\n\nusing bytes = basic_sstring<int8_t, uint32_t, 31, false>;\nusing bytes_view = std::basic_string_view<int8_t>;\nusing bytes_mutable_view = basic_mutable_view<bytes_view::value_type>;\nusing bytes_opt = std::optional<bytes>;\n"
        },
        {
          "name": "bytes_ostream.hh",
          "type": "blob",
          "size": 14.8779296875,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"bytes.hh\"\n#include \"utils/assert.hh\"\n#include \"utils/managed_bytes.hh\"\n#include <seastar/core/simple-stream.hh>\n#include <seastar/core/loop.hh>\n#include <bit>\n#include <concepts>\n#include <ranges>\n\n\nclass bytes_ostream_fragment_iterator {\npublic:\n    using iterator_category = std::input_iterator_tag;\n    using iterator_concept = std::input_iterator_tag;\n    using value_type = bytes_view;\n    using difference_type = std::ptrdiff_t;\n    using pointer = bytes_view*;\n    using reference = bytes_view&;\npublic:\n    using chunk = multi_chunk_blob_storage;\n    struct implementation {\n        chunk* current_chunk;\n    };\nprivate:\n    chunk* _current = nullptr;\npublic:\n    bytes_ostream_fragment_iterator() = default;\n    bytes_ostream_fragment_iterator(chunk* current) : _current(current) {}\n    bytes_ostream_fragment_iterator(const bytes_ostream_fragment_iterator&) = default;\n    bytes_ostream_fragment_iterator& operator=(const bytes_ostream_fragment_iterator&) = default;\n    bytes_view operator*() const {\n        return { _current->data, _current->frag_size };\n    }\n    bytes_view operator->() const {\n        return *(*this);\n    }\n    bytes_ostream_fragment_iterator& operator++() {\n        _current = _current->next;\n        return *this;\n    }\n    bytes_ostream_fragment_iterator operator++(int) {\n        bytes_ostream_fragment_iterator tmp(*this);\n        ++(*this);\n        return tmp;\n    }\n    bool operator==(const bytes_ostream_fragment_iterator&) const = default;\n    implementation extract_implementation() const {\n        return implementation {\n            .current_chunk = _current,\n        };\n    }\n};\n\n/**\n * Utility for writing data into a buffer when its final size is not known up front.\n *\n * Internally the data is written into a chain of chunks allocated on-demand.\n * No resizing of previously written data happens.\n *\n */\nclass bytes_ostream {\npublic:\n    using size_type = bytes::size_type;\n    using value_type = bytes::value_type;\n    using fragment_type = bytes_view;\n    static constexpr size_type max_chunk_size() { return max_alloc_size() - sizeof(chunk); }\nprivate:\n    static_assert(sizeof(value_type) == 1, \"value_type is assumed to be one byte long\");\n    // Note: while appending data, chunk::size refers to the allocated space in the chunk,\n    //       and chunk::frag_size refers to the currently occupied space in the chunk.\n    //       After building, the first chunk::size is the whole object size, and chunk::frag_size\n    //       doesn't change. This fits with managed_bytes interpretation.\n    using chunk = multi_chunk_blob_storage;\n    static constexpr size_type default_chunk_size{512};\n    static constexpr size_type max_alloc_size() { return 128 * 1024; }\nprivate:\n    chunk::ref_type _begin;\n    chunk* _current;\n    size_type _size;\n    size_type _initial_chunk_size = default_chunk_size;\npublic:\n    using fragment_iterator = bytes_ostream_fragment_iterator;\n    using const_iterator = fragment_iterator;\n\n    class output_iterator {\n    public:\n        using iterator_category = std::output_iterator_tag;\n        using difference_type = std::ptrdiff_t;\n        using value_type = bytes_ostream::value_type;\n        using pointer = bytes_ostream::value_type*;\n        using reference = bytes_ostream::value_type&;\n\n        friend class bytes_ostream;\n\n    private:\n        bytes_ostream* _ostream = nullptr;\n\n    private:\n        explicit output_iterator(bytes_ostream& os) : _ostream(&os) { }\n\n    public:\n        reference operator*() const { return *_ostream->write_place_holder(1); }\n        output_iterator& operator++() { return *this; }\n        output_iterator operator++(int) { return *this; }\n    };\nprivate:\n    inline size_type current_space_left() const {\n        if (!_current) {\n            return 0;\n        }\n        return _current->size - _current->frag_size;\n    }\n    // Figure out next chunk size.\n    //   - must be enough for data_size + sizeof(chunk)\n    //   - must be at least _initial_chunk_size\n    //   - try to double each time to prevent too many allocations\n    //   - should not exceed max_alloc_size, unless data_size requires so\n    //   - will be power-of-two so the allocated memory can be fully utilized.\n    size_type next_alloc_size(size_t data_size) const {\n        auto next_size = _current\n                ? _current->size * 2\n                : _initial_chunk_size;\n        next_size = std::min(next_size, max_alloc_size());\n        auto r = std::max<size_type>(next_size, data_size + sizeof(chunk));\n        return std::bit_ceil(r);\n    }\n    // Makes room for a contiguous region of given size.\n    // The region is accounted for as already written.\n    // size must not be zero.\n    [[gnu::always_inline]]\n    value_type* alloc(size_type size) {\n        if (__builtin_expect(size <= current_space_left(), true)) {\n            auto ret = _current->data + _current->frag_size;\n            _current->frag_size += size;\n            _size += size;\n            return ret;\n        } else {\n            return alloc_new(size);\n        }\n    }\n    [[gnu::noinline]]\n    value_type* alloc_new(size_type size) {\n            auto alloc_size = next_alloc_size(size);\n            auto space = malloc(alloc_size);\n            if (!space) {\n                throw std::bad_alloc();\n            }\n            auto backref = _current ? &_current->next : &_begin;\n            auto new_chunk = new (space) chunk(backref, alloc_size - sizeof(chunk), size);\n            _current = new_chunk;\n            _size += size;\n            return _current->data;\n    }\n    [[gnu::noinline]]\n    void free_chain(chunk* c) noexcept {\n        while (c) {\n            auto n = c->next;\n            c->~chunk();\n            ::free(c);\n            c = n;\n        }\n    }\npublic:\n    explicit bytes_ostream(size_t initial_chunk_size) noexcept\n        : _begin()\n        , _current(nullptr)\n        , _size(0)\n        , _initial_chunk_size(initial_chunk_size)\n    { }\n\n    bytes_ostream() noexcept : bytes_ostream(default_chunk_size) {}\n\n    bytes_ostream(bytes_ostream&& o) noexcept\n        : _begin(std::exchange(o._begin, {}))\n        , _current(o._current)\n        , _size(o._size)\n        , _initial_chunk_size(o._initial_chunk_size)\n    {\n        o._current = nullptr;\n        o._size = 0;\n    }\n\n    bytes_ostream(const bytes_ostream& o)\n        : _begin()\n        , _current(nullptr)\n        , _size(0)\n        , _initial_chunk_size(o._initial_chunk_size)\n    {\n        append(o);\n    }\n\n    ~bytes_ostream() {\n        free_chain(_begin.ptr);\n    }\n\n    bytes_ostream& operator=(const bytes_ostream& o) {\n        if (this != &o) {\n            auto x = bytes_ostream(o);\n            *this = std::move(x);\n        }\n        return *this;\n    }\n\n    bytes_ostream& operator=(bytes_ostream&& o) noexcept {\n        if (this != &o) {\n            this->~bytes_ostream();\n            new (this) bytes_ostream(std::move(o));\n        }\n        return *this;\n    }\n\n    template <typename T>\n    struct place_holder {\n        value_type* ptr;\n        // makes the place_holder looks like a stream\n        seastar::simple_output_stream get_stream() {\n            return seastar::simple_output_stream(reinterpret_cast<char*>(ptr), sizeof(T));\n        }\n    };\n\n    // Returns a place holder for a value to be written later.\n    template <std::integral T>\n    inline\n    place_holder<T>\n    write_place_holder() {\n        return place_holder<T>{alloc(sizeof(T))};\n    }\n\n    [[gnu::always_inline]]\n    value_type* write_place_holder(size_type size) {\n        return alloc(size);\n    }\n\n    // Writes given sequence of bytes\n    [[gnu::always_inline]]\n    inline void write(bytes_view v) {\n        if (v.empty()) {\n            return;\n        }\n\n        auto this_size = std::min(v.size(), size_t(current_space_left()));\n        if (__builtin_expect(this_size, true)) {\n            memcpy(_current->data + _current->frag_size, v.begin(), this_size);\n            _current->frag_size += this_size;\n            _size += this_size;\n            v.remove_prefix(this_size);\n        }\n\n        while (!v.empty()) {\n            auto this_size = std::min(v.size(), size_t(max_chunk_size()));\n            std::copy_n(v.begin(), this_size, alloc_new(this_size));\n            v.remove_prefix(this_size);\n        }\n    }\n\n    [[gnu::always_inline]]\n    void write(const char* ptr, size_t size) {\n        write(bytes_view(reinterpret_cast<const signed char*>(ptr), size));\n    }\n\n    bool is_linearized() const {\n        return !_begin || !_begin->next;\n    }\n\n    // Call only when is_linearized()\n    bytes_view view() const {\n        SCYLLA_ASSERT(is_linearized());\n        if (!_current) {\n            return bytes_view();\n        }\n\n        return bytes_view(_current->data, _size);\n    }\n\n    // Makes the underlying storage contiguous and returns a view to it.\n    // Invalidates all previously created placeholders.\n    bytes_view linearize() {\n        if (is_linearized()) {\n            return view();\n        }\n\n        auto space = malloc(_size + sizeof(chunk));\n        if (!space) {\n            throw std::bad_alloc();\n        }\n\n        auto old_begin = _begin;\n        auto new_chunk = new (space) chunk(&_begin, _size, _size);\n\n        auto dst = new_chunk->data;\n        auto r = old_begin.ptr;\n        while (r) {\n            auto next = r->next;\n            dst = std::copy_n(r->data, r->frag_size, dst);\n            r->~chunk();\n            ::free(r);\n            r = next;\n        }\n\n        _current = new_chunk;\n        _begin = std::move(new_chunk);\n        return bytes_view(_current->data, _size);\n    }\n\n    // Returns the amount of bytes written so far\n    size_type size() const {\n        return _size;\n    }\n\n    // For the FragmentRange concept\n    size_type size_bytes() const {\n        return _size;\n    }\n\n    bool empty() const {\n        return _size == 0;\n    }\n\n    void reserve(size_t size) {\n        // FIXME: implement\n    }\n\n    void append(const bytes_ostream& o) {\n        for (auto&& bv : o.fragments()) {\n            write(bv);\n        }\n    }\n\n    // Removes n bytes from the end of the bytes_ostream.\n    // Beware of O(n) algorithm.\n    void remove_suffix(size_t n) {\n        _size -= n;\n        auto left = _size;\n        auto current = _begin.ptr;\n        while (current) {\n            if (current->frag_size >= left) {\n                current->frag_size = left;\n                _current = current;\n                free_chain(current->next);\n                current->next = nullptr;\n                return;\n            }\n            left -= current->frag_size;\n            current = current->next;\n        }\n    }\n\n    // begin() and end() form an input range to bytes_view representing fragments.\n    // Any modification of this instance invalidates iterators.\n    fragment_iterator begin() const { return { _begin.ptr }; }\n    fragment_iterator end() const { return { nullptr }; }\n\n    output_iterator write_begin() { return output_iterator(*this); }\n\n    std::ranges::subrange<fragment_iterator> fragments() const {\n        return { begin(), end() };\n    }\n\n    struct position {\n        chunk* _chunk;\n        size_type _offset;\n    };\n\n    position pos() const {\n        return { _current, _current ? _current->frag_size : 0 };\n    }\n\n    // Returns the amount of bytes written since given position.\n    // \"pos\" must be valid.\n    size_type written_since(position pos) {\n        chunk* c = pos._chunk;\n        if (!c) {\n            return _size;\n        }\n        size_type total = c->frag_size - pos._offset;\n        c = c->next;\n        while (c) {\n            total += c->frag_size;\n            c = c->next;\n        }\n        return total;\n    }\n\n    // Rollbacks all data written after \"pos\".\n    // Invalidates all placeholders and positions created after \"pos\".\n    void retract(position pos) {\n        if (!pos._chunk) {\n            *this = {};\n            return;\n        }\n        _size -= written_since(pos);\n        _current = pos._chunk;\n        free_chain(_current->next);\n        _current->next = nullptr;\n        _current->frag_size = pos._offset;\n    }\n\n    void reduce_chunk_count() {\n        // FIXME: This is a simplified version. It linearizes the whole buffer\n        // if its size is below max_chunk_size. We probably could also gain\n        // some read performance by doing \"real\" reduction, i.e. merging\n        // all chunks until all but the last one is max_chunk_size.\n        if (size() < max_chunk_size()) {\n            linearize();\n        }\n    }\n\n    bool operator==(const bytes_ostream& other) const {\n        auto as = fragments().begin();\n        auto as_end = fragments().end();\n        auto bs = other.fragments().begin();\n        auto bs_end = other.fragments().end();\n\n        auto a = *as++;\n        auto b = *bs++;\n        while (!a.empty() || !b.empty()) {\n            auto now = std::min(a.size(), b.size());\n            if (!std::equal(a.begin(), a.begin() + now, b.begin(), b.begin() + now)) {\n                return false;\n            }\n            a.remove_prefix(now);\n            if (a.empty() && as != as_end) {\n                a = *as++;\n            }\n            b.remove_prefix(now);\n            if (b.empty() && bs != bs_end) {\n                b = *bs++;\n            }\n        }\n        return true;\n    }\n\n    // Makes this instance empty.\n    //\n    // The first buffer is not deallocated, so callers may rely on the\n    // fact that if they write less than the initial chunk size between\n    // the clear() calls then writes will not involve any memory allocations,\n    // except for the first write made on this instance.\n    void clear() {\n        if (_begin.ptr) {\n            _begin.ptr->frag_size = 0;\n            _size = 0;\n            free_chain(_begin.ptr->next);\n            _begin.ptr->next = nullptr;\n            _current = _begin.ptr;\n        }\n    }\n\n    managed_bytes to_managed_bytes() && {\n        if (_size) {\n            _begin.ptr->size = _size;\n            _current = nullptr;\n            _size = 0;\n            auto begin_ptr = _begin.ptr;\n            _begin.ptr = nullptr;\n            return managed_bytes(begin_ptr);\n        } else {\n            return managed_bytes();\n        }\n    }\n\n    // Makes this instance empty using async continuations, while allowing yielding.\n    //\n    // The first buffer is not deallocated, so callers may rely on the\n    // fact that if they write less than the initial chunk size between\n    // the clear() calls then writes will not involve any memory allocations,\n    // except for the first write made on this instance.\n    future<> clear_gently() noexcept {\n        if (!_begin.ptr) {\n            return make_ready_future<>();\n        }\n        _begin->frag_size = 0;\n        _current = _begin.ptr;\n        _size = 0;\n        return do_until([this] { return !_begin.ptr->next; }, [this] {\n            auto second_chunk = _begin.ptr->next;\n            auto next = second_chunk->next;\n            second_chunk->~chunk();\n            ::free(second_chunk);\n            _begin->next = std::move(next);\n            return make_ready_future<>();\n        });\n    }\n};\n"
        },
        {
          "name": "cache_mutation_reader.hh",
          "type": "blob",
          "size": 52.1650390625,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"utils/assert.hh\"\n#include <vector>\n#include \"row_cache.hh\"\n#include \"mutation/mutation_fragment.hh\"\n#include \"query-request.hh\"\n#include \"partition_snapshot_row_cursor.hh\"\n#include \"read_context.hh\"\n#include \"readers/delegating_v2.hh\"\n#include \"clustering_key_filter.hh\"\n\nnamespace cache {\n\nextern logging::logger clogger;\n\nclass cache_mutation_reader final : public mutation_reader::impl {\n    enum class state {\n        before_static_row,\n\n        // Invariants:\n        //  - position_range(_lower_bound, _upper_bound) covers all not yet emitted positions from current range\n        //  - if _next_row has valid iterators:\n        //    - _next_row points to the nearest row in cache >= _lower_bound\n        //    - _next_row_in_range = _next.position() < _upper_bound\n        //  - if _next_row doesn't have valid iterators, it has no meaning.\n        reading_from_cache,\n\n        // Starts reading from underlying reader.\n        // The range to read is position_range(_lower_bound, min(_next_row.position(), _upper_bound)).\n        // Invariants:\n        //  - _next_row_in_range = _next.position() < _upper_bound\n        move_to_underlying,\n\n        // Invariants:\n        // - Upper bound of the read is *_underlying_upper_bound\n        // - _next_row_in_range = _next.position() < _upper_bound\n        // - _last_row points at a direct predecessor of the next row which is going to be read.\n        //   Used for populating continuity.\n        // - _population_range_starts_before_all_rows is set accordingly\n        // - _underlying is engaged and fast-forwarded\n        reading_from_underlying,\n\n        end_of_stream\n    };\n    partition_snapshot_ptr _snp;\n\n    query::clustering_key_filter_ranges _ck_ranges; // Query schema domain, reversed reads use native order\n    query::clustering_row_ranges::const_iterator _ck_ranges_curr; // Query schema domain\n    query::clustering_row_ranges::const_iterator _ck_ranges_end; // Query schema domain\n\n    lsa_manager _lsa_manager;\n\n    partition_snapshot_row_weakref _last_row; // Table schema domain\n\n    // Holds the lower bound of a position range which hasn't been processed yet.\n    // Only rows with positions < _lower_bound have been emitted, and only\n    // range_tombstone_changes with positions <= _lower_bound.\n    //\n    // Invariant: !_lower_bound.is_clustering_row()\n    position_in_partition _lower_bound; // Query schema domain\n    // Invariant: !_upper_bound.is_clustering_row()\n    position_in_partition_view _upper_bound; // Query schema domain\n    std::optional<position_in_partition> _underlying_upper_bound; // Query schema domain\n\n    // cache_mutation_reader may be constructed either\n    // with a read_context&, where it knows that the read_context\n    // is owned externally, by the caller.  In this case\n    // _read_context_holder would be disengaged.\n    // Or, it could be constructed with a std::unique_ptr<read_context>,\n    // in which case it assumes ownership of the read_context\n    // and it is now responsible for closing it.\n    // In this case, _read_context_holder would be engaged\n    // and _read_context will reference its content.\n    std::unique_ptr<read_context> _read_context_holder;\n    read_context& _read_context;\n    partition_snapshot_row_cursor _next_row;\n\n    // Holds the currently active range tombstone of the output mutation fragment stream.\n    // While producing the stream, at any given time, _current_tombstone applies to the\n    // key range which extends at least to _lower_bound. When consuming subsequent interval,\n    // which will advance _lower_bound further, be it from underlying or from cache,\n    // a decision is made whether the range tombstone in the next interval is the same as\n    // the current one or not. If it is different, then range_tombstone_change is emitted\n    // with the old _lower_bound value (start of the next interval).\n    tombstone _current_tombstone;\n\n    state _state = state::before_static_row;\n\n    bool _next_row_in_range = false;\n    bool _has_rt = false;\n\n    // True iff current population interval starts at before_all_clustered_rows\n    // and _last_row is unset. (And the read isn't reverse).\n    //\n    // Rationale: in the \"most general\" step of cache population,\n    // we mark the `(_last_row, ...] `range as continuous, which can involve doing something to `_last_row`.\n    // But when populating the range `(before_all_clustered_rows, ...)`,\n    // a rows_entry at `before_all_clustered_rows` needn't exist.\n    // Thus this case needs a special treatment which doesn't involve `_last_row`.\n    // And for that, this case it has to be recognized (via this flag).\n    //\n    // We cannot just look at _lower_bound, because emission of range tombstones changes _lower_bound and\n    // because we mark clustering intervals as continuous when consuming a clustering_row, it would prevent\n    // us from marking the interval as continuous.\n    // Valid when _state == reading_from_underlying.\n    bool _population_range_starts_before_all_rows;\n\n    // Points to the underlying reader conforming to _schema,\n    // either to *_underlying_holder or _read_context.underlying().underlying().\n    mutation_reader* _underlying = nullptr;\n    mutation_reader_opt _underlying_holder;\n\n    gc_clock::time_point _read_time;\n    gc_clock::time_point _gc_before;\n\n    future<> do_fill_buffer();\n    future<> ensure_underlying();\n    void copy_from_cache_to_buffer();\n    future<> process_static_row();\n    void move_to_end();\n    void move_to_next_range();\n    void move_to_range(query::clustering_row_ranges::const_iterator);\n    void move_to_next_entry();\n    void maybe_drop_last_entry(tombstone) noexcept;\n    void add_to_buffer(const partition_snapshot_row_cursor&);\n    void add_clustering_row_to_buffer(mutation_fragment_v2&&);\n    void add_to_buffer(range_tombstone_change&&);\n    void offer_from_underlying(mutation_fragment_v2&&);\n    future<> read_from_underlying();\n    void start_reading_from_underlying();\n    bool after_current_range(position_in_partition_view position);\n    bool can_populate() const;\n    // Marks the range between _last_row (exclusive) and _next_row (exclusive) as continuous,\n    // provided that the underlying reader still matches the latest version of the partition.\n    // Invalidates _last_row.\n    void maybe_update_continuity();\n    // Tries to ensure that the lower bound of the current population range exists.\n    // Returns false if it failed and range cannot be populated.\n    // Assumes can_populate().\n    // If returns true then _last_row is refreshed and points to the population lower bound.\n    // if _read_context.is_reversed() then _last_row is always valid after this.\n    // if !_read_context.is_reversed() then _last_row is valid after this or the population lower bound\n    // is before all rows (so _last_row doesn't point at any entry).\n    bool ensure_population_lower_bound();\n    void maybe_add_to_cache(const mutation_fragment_v2& mf);\n    void maybe_add_to_cache(const clustering_row& cr);\n    bool maybe_add_to_cache(const range_tombstone_change& rtc);\n    void maybe_add_to_cache(const static_row& sr);\n    void maybe_set_static_row_continuous();\n    void set_rows_entry_continuous(rows_entry& e);\n    void restore_continuity_after_insertion(const mutation_partition::rows_type::iterator&);\n    void finish_reader() {\n        push_mutation_fragment(*_schema, _permit, partition_end());\n        _end_of_stream = true;\n        _state = state::end_of_stream;\n    }\n    const schema_ptr& snp_schema() const {\n        return _snp->schema();\n    }\n    void touch_partition();\n\n    position_in_partition_view to_table_domain(position_in_partition_view query_domain_pos) {\n        if (!_read_context.is_reversed()) [[likely]] {\n            return query_domain_pos;\n        }\n        return query_domain_pos.reversed();\n    }\n\n    range_tombstone to_table_domain(range_tombstone query_domain_rt) {\n        if (_read_context.is_reversed()) [[unlikely]] {\n            query_domain_rt.reverse();\n        }\n        return query_domain_rt;\n    }\n\n    position_in_partition_view to_query_domain(position_in_partition_view table_domain_pos) {\n        if (!_read_context.is_reversed()) [[likely]] {\n            return table_domain_pos;\n        }\n        return table_domain_pos.reversed();\n    }\n\n    const schema& table_schema() {\n        return *_snp->schema();\n    }\n\n    gc_clock::time_point get_read_time() {\n        return _read_context.tombstone_gc_state() ? gc_clock::now() : gc_clock::time_point::min();\n    }\n\n    gc_clock::time_point get_gc_before(const schema& schema, dht::decorated_key dk, const gc_clock::time_point query_time) {\n        auto gc_state = _read_context.tombstone_gc_state();\n        if (gc_state) {\n            return gc_state->with_commitlog_check_disabled().get_gc_before_for_key(schema.shared_from_this(), dk, query_time);\n        }\n\n        return gc_clock::time_point::min();\n    }\n\npublic:\n    cache_mutation_reader(schema_ptr s,\n                               dht::decorated_key dk,\n                               query::clustering_key_filter_ranges&& crr,\n                               read_context& ctx,\n                               partition_snapshot_ptr snp,\n                               row_cache& cache)\n        : mutation_reader::impl(std::move(s), ctx.permit())\n        , _snp(std::move(snp))\n        , _ck_ranges(std::move(crr))\n        , _ck_ranges_curr(_ck_ranges.begin())\n        , _ck_ranges_end(_ck_ranges.end())\n        , _lsa_manager(cache)\n        , _lower_bound(position_in_partition::before_all_clustered_rows())\n        , _upper_bound(position_in_partition_view::before_all_clustered_rows())\n        , _read_context_holder()\n        , _read_context(ctx)    // ctx is owned by the caller, who's responsible for closing it.\n        , _next_row(*_schema, *_snp, false, _read_context.is_reversed())\n        , _read_time(get_read_time())\n        , _gc_before(get_gc_before(*_schema, dk, _read_time))\n    {\n        clogger.trace(\"csm {}: table={}.{}, reversed={}, snap={}\", fmt::ptr(this), _schema->ks_name(), _schema->cf_name(), _read_context.is_reversed(),\n                      fmt::ptr(&*_snp));\n        push_mutation_fragment(*_schema, _permit, partition_start(std::move(dk), _snp->partition_tombstone()));\n    }\n    cache_mutation_reader(schema_ptr s,\n                               dht::decorated_key dk,\n                               query::clustering_key_filter_ranges&& crr,\n                               std::unique_ptr<read_context> unique_ctx,\n                               partition_snapshot_ptr snp,\n                               row_cache& cache)\n        : cache_mutation_reader(s, std::move(dk), std::move(crr), *unique_ctx, std::move(snp), cache)\n    {\n        // Assume ownership of the read_context.\n        // It is our responsibility to close it now.\n        _read_context_holder = std::move(unique_ctx);\n    }\n    cache_mutation_reader(const cache_mutation_reader&) = delete;\n    cache_mutation_reader(cache_mutation_reader&&) = delete;\n    virtual future<> fill_buffer() override;\n    virtual future<> next_partition() override {\n        clear_buffer_to_next_partition();\n        if (is_buffer_empty()) {\n            _end_of_stream = true;\n        }\n        return make_ready_future<>();\n    }\n    virtual future<> fast_forward_to(const dht::partition_range&) override {\n        clear_buffer();\n        _end_of_stream = true;\n        return make_ready_future<>();\n    }\n    virtual future<> fast_forward_to(position_range pr) override {\n        return make_exception_future<>(make_backtraced_exception_ptr<std::bad_function_call>());\n    }\n    virtual future<> close() noexcept override {\n        auto close_read_context = _read_context_holder ?  _read_context_holder->close() : make_ready_future<>();\n        auto close_underlying = _underlying_holder ? _underlying_holder->close() : make_ready_future<>();\n        return when_all_succeed(std::move(close_read_context), std::move(close_underlying)).discard_result();\n    }\n};\n\ninline\nfuture<> cache_mutation_reader::process_static_row() {\n    if (_snp->static_row_continuous()) {\n        _read_context.cache().on_row_hit();\n        static_row sr = _lsa_manager.run_in_read_section([this] {\n            return _snp->static_row(_read_context.digest_requested());\n        });\n        if (!sr.empty()) {\n            push_mutation_fragment(*_schema, _permit, std::move(sr));\n        }\n        return make_ready_future<>();\n    } else {\n        _read_context.cache().on_row_miss();\n        return ensure_underlying().then([this] {\n            return (*_underlying)().then([this] (mutation_fragment_v2_opt&& sr) {\n                if (sr) {\n                    SCYLLA_ASSERT(sr->is_static_row());\n                    maybe_add_to_cache(sr->as_static_row());\n                    push_mutation_fragment(std::move(*sr));\n                }\n                maybe_set_static_row_continuous();\n            });\n        });\n    }\n}\n\ninline\nvoid cache_mutation_reader::touch_partition() {\n    _snp->touch();\n}\n\ninline\nfuture<> cache_mutation_reader::fill_buffer() {\n    if (_state == state::before_static_row) {\n        touch_partition();\n        auto after_static_row = [this] {\n            if (_ck_ranges_curr == _ck_ranges_end) {\n                finish_reader();\n                return make_ready_future<>();\n            }\n            _state = state::reading_from_cache;\n            _lsa_manager.run_in_read_section([this] {\n                move_to_range(_ck_ranges_curr);\n            });\n            return fill_buffer();\n        };\n        if (_schema->has_static_columns()) {\n            return process_static_row().then(std::move(after_static_row));\n        } else {\n            return after_static_row();\n        }\n    }\n    clogger.trace(\"csm {}: fill_buffer(), range={}, lb={}\", fmt::ptr(this), *_ck_ranges_curr, _lower_bound);\n    return do_until([this] { return _end_of_stream || is_buffer_full(); }, [this] {\n        return do_fill_buffer();\n    });\n}\n\ninline\nfuture<> cache_mutation_reader::ensure_underlying() {\n    if (_underlying) {\n        return make_ready_future<>();\n    }\n    return _read_context.ensure_underlying().then([this] {\n        mutation_reader& ctx_underlying = _read_context.underlying().underlying();\n        if (ctx_underlying.schema() != _schema) {\n            _underlying_holder = make_delegating_reader(ctx_underlying);\n            _underlying_holder->upgrade_schema(_schema);\n            _underlying = &*_underlying_holder;\n        } else {\n            _underlying = &ctx_underlying;\n        }\n    });\n}\n\ninline\nfuture<> cache_mutation_reader::do_fill_buffer() {\n    if (_state == state::move_to_underlying) {\n        if (!_underlying) {\n            return ensure_underlying().then([this] {\n                return do_fill_buffer();\n            });\n        }\n        _state = state::reading_from_underlying;\n        _population_range_starts_before_all_rows = _lower_bound.is_before_all_clustered_rows(*_schema) && !_read_context.is_reversed() && !_last_row;\n        _underlying_upper_bound = _next_row_in_range ? position_in_partition::before_key(_next_row.position())\n                                                     : position_in_partition(_upper_bound);\n        if (!_read_context.partition_exists()) {\n            clogger.trace(\"csm {}: partition does not exist\", fmt::ptr(this));\n            if (_current_tombstone) {\n                clogger.trace(\"csm {}: move_to_underlying: emit rtc({}, null)\", fmt::ptr(this), _lower_bound);\n                push_mutation_fragment(mutation_fragment_v2(*_schema, _permit, range_tombstone_change(_lower_bound, {})));\n                _current_tombstone = {};\n            }\n            return read_from_underlying();\n        }\n        return _underlying->fast_forward_to(position_range{_lower_bound, *_underlying_upper_bound}).then([this] {\n            if (!_current_tombstone) {\n                return read_from_underlying();\n            }\n            return _underlying->peek().then([this] (mutation_fragment_v2* mf) {\n                position_in_partition::equal_compare eq(*_schema);\n                if (!mf || !mf->is_range_tombstone_change()\n                        || !eq(mf->as_range_tombstone_change().position(), _lower_bound)) {\n                    clogger.trace(\"csm {}: move_to_underlying: emit rtc({}, null)\", fmt::ptr(this), _lower_bound);\n                    push_mutation_fragment(mutation_fragment_v2(*_schema, _permit, range_tombstone_change(_lower_bound, {})));\n                    _current_tombstone = {};\n                }\n                return read_from_underlying();\n            });\n        });\n    }\n    if (_state == state::reading_from_underlying) {\n        return read_from_underlying();\n    }\n    // SCYLLA_ASSERT(_state == state::reading_from_cache)\n    return _lsa_manager.run_in_read_section([this] {\n        auto next_valid = _next_row.iterators_valid();\n        clogger.trace(\"csm {}: reading_from_cache, range=[{}, {}), next={}, valid={}, rt={}\", fmt::ptr(this), _lower_bound,\n            _upper_bound, _next_row.position(), next_valid, _current_tombstone);\n        // We assume that if there was eviction, and thus the range may\n        // no longer be continuous, the cursor was invalidated.\n        if (!next_valid) {\n            auto adjacent = _next_row.advance_to(_lower_bound);\n            _next_row_in_range = !after_current_range(_next_row.position());\n            if (!adjacent && !_next_row.continuous()) {\n                _last_row = nullptr; // We could insert a dummy here, but this path is unlikely.\n                start_reading_from_underlying();\n                return make_ready_future<>();\n            }\n        }\n        _next_row.maybe_refresh();\n        clogger.trace(\"csm {}: next={}\", fmt::ptr(this), _next_row);\n        while (_state == state::reading_from_cache) {\n            copy_from_cache_to_buffer();\n            if (need_preempt() || is_buffer_full()) {\n                break;\n            }\n        }\n        return make_ready_future<>();\n    });\n}\n\ninline\nfuture<> cache_mutation_reader::read_from_underlying() {\n    return consume_mutation_fragments_until(*_underlying,\n        [this] { return _state != state::reading_from_underlying || is_buffer_full(); },\n        [this] (mutation_fragment_v2 mf) {\n            _read_context.cache().on_row_miss();\n            offer_from_underlying(std::move(mf));\n        },\n        [this] {\n            _lower_bound = std::move(*_underlying_upper_bound);\n            _underlying_upper_bound.reset();\n            _state = state::reading_from_cache;\n            _lsa_manager.run_in_update_section([this] {\n                auto same_pos = _next_row.maybe_refresh();\n                clogger.trace(\"csm {}: underlying done, in_range={}, same={}, next={}\", fmt::ptr(this), _next_row_in_range, same_pos, _next_row);\n                if (!same_pos) {\n                    _read_context.cache().on_mispopulate(); // FIXME: Insert dummy entry at _lower_bound.\n                    _next_row_in_range = !after_current_range(_next_row.position());\n                    if (!_next_row.continuous()) {\n                        _last_row = nullptr; // We did not populate the full range up to _lower_bound, break continuity\n                        start_reading_from_underlying();\n                    }\n                    return;\n                }\n                if (_next_row_in_range) {\n                    maybe_update_continuity();\n                } else {\n                    if (can_populate()) {\n                        const schema& table_s = table_schema();\n                        rows_entry::tri_compare cmp(table_s);\n                        auto& rows = _snp->version()->partition().mutable_clustered_rows();\n                        if (query::is_single_row(*_schema, *_ck_ranges_curr)) {\n                            // If there are range tombstones which apply to the row then\n                            // we cannot insert an empty entry here because if those range\n                            // tombstones got evicted by now, we will insert an entry\n                            // with missing range tombstone information.\n                            // FIXME: try to set the range tombstone when possible.\n                            if (!_has_rt) {\n                            with_allocator(_snp->region().allocator(), [&] {\n                                auto e = alloc_strategy_unique_ptr<rows_entry>(\n                                    current_allocator().construct<rows_entry>(_ck_ranges_curr->start()->value()));\n                                // Use _next_row iterator only as a hint, because there could be insertions after _upper_bound.\n                                auto insert_result = rows.insert_before_hint(\n                                        _next_row.at_a_row() ? _next_row.get_iterator_in_latest_version() : rows.begin(),\n                                        std::move(e),\n                                        cmp);\n                                if (insert_result.second) {\n                                    auto it = insert_result.first;\n                                    _snp->tracker()->insert(*it);\n                                    auto next = std::next(it);\n                                    // Also works in reverse read mode.\n                                    // It preserves the continuity of the range the entry falls into.\n                                    it->set_continuous(next->continuous());\n                                    clogger.trace(\"csm {}: inserted empty row at {}, cont={}, rt={}\", fmt::ptr(this), it->position(), it->continuous(), it->range_tombstone());\n                                }\n                            });\n                            }\n                        } else if (ensure_population_lower_bound()) {\n                            with_allocator(_snp->region().allocator(), [&] {\n                                auto e = alloc_strategy_unique_ptr<rows_entry>(\n                                    current_allocator().construct<rows_entry>(table_s, to_table_domain(_upper_bound), is_dummy::yes, is_continuous::no));\n                                // Use _next_row iterator only as a hint, because there could be insertions after _upper_bound.\n                                auto insert_result = rows.insert_before_hint(\n                                        _next_row.at_a_row() ? _next_row.get_iterator_in_latest_version() : rows.begin(),\n                                        std::move(e),\n                                        cmp);\n                                if (insert_result.second) {\n                                    clogger.trace(\"csm {}: L{}: inserted dummy at {}\", fmt::ptr(this), __LINE__, _upper_bound);\n                                    _snp->tracker()->insert(*insert_result.first);\n                                    restore_continuity_after_insertion(insert_result.first);\n                                }\n                                if (_read_context.is_reversed()) [[unlikely]] {\n                                    clogger.trace(\"csm {}: set_continuous({}), prev={}, rt={}\", fmt::ptr(this), _last_row.position(), insert_result.first->position(), _current_tombstone);\n                                    set_rows_entry_continuous(*_last_row);\n                                    _last_row->set_range_tombstone(_current_tombstone);\n                                } else {\n                                    clogger.trace(\"csm {}: set_continuous({}), prev={}, rt={}\", fmt::ptr(this), insert_result.first->position(), _last_row.position(), _current_tombstone);\n                                    set_rows_entry_continuous(*insert_result.first);\n                                    insert_result.first->set_range_tombstone(_current_tombstone);\n                                }\n                                maybe_drop_last_entry(_current_tombstone);\n                            });\n                        }\n                    } else {\n                        _read_context.cache().on_mispopulate();\n                    }\n                    try {\n                        move_to_next_range();\n                    } catch (const std::bad_alloc&) {\n                        // We cannot reenter the section, since we may have moved to the new range\n                        _snp->region().allocator().invalidate_references(); // Invalidates _next_row\n                    }\n                }\n            });\n            return make_ready_future<>();\n        });\n}\n\ninline\nbool cache_mutation_reader::ensure_population_lower_bound() {\n    if (_population_range_starts_before_all_rows) {\n        return true;\n    }\n    if (!_last_row.refresh(*_snp)) {\n        return false;\n    }\n    // Continuity flag we will later set for the upper bound extends to the previous row in the same version,\n    // so we need to ensure we have an entry in the latest version.\n    if (!_last_row.is_in_latest_version()) {\n        rows_entry::tri_compare cmp(*_schema);\n        partition_snapshot_row_cursor cur(*_schema, *_snp, false, _read_context.is_reversed());\n\n        if (!cur.advance_to(to_query_domain(_last_row.position()))) {\n            return false;\n        }\n\n        if (cmp(cur.table_position(), _last_row.position()) != 0) {\n            return false;\n        }\n\n        auto res = with_allocator(_snp->region().allocator(), [&] {\n            return cur.ensure_entry_in_latest();\n        });\n\n        _last_row.set_latest(res.it);\n        if (res.inserted) {\n            clogger.trace(\"csm {}: inserted lower bound dummy at {}\", fmt::ptr(this), _last_row.position());\n        }\n    }\n\n    return true;\n}\n\ninline\nvoid cache_mutation_reader::maybe_update_continuity() {\n    position_in_partition::equal_compare eq(*_schema);\n    if (can_populate()\n            && ensure_population_lower_bound()\n            && !eq(_last_row.position(), _next_row.table_position())) {\n        with_allocator(_snp->region().allocator(), [&] {\n            rows_entry& e = _next_row.ensure_entry_in_latest().row;\n            auto& rows = _snp->version()->partition().mutable_clustered_rows();\n            const schema& table_s = table_schema();\n            rows_entry::tri_compare table_cmp(table_s);\n\n            if (_read_context.is_reversed()) [[unlikely]] {\n                if (_current_tombstone != _last_row->range_tombstone() && !_last_row->dummy()) {\n                    with_allocator(_snp->region().allocator(), [&] {\n                        auto e2 = alloc_strategy_unique_ptr<rows_entry>(\n                                current_allocator().construct<rows_entry>(table_s,\n                                                                          position_in_partition_view::before_key(_last_row->position()),\n                                                                          is_dummy::yes,\n                                                                          is_continuous::yes));\n                        auto insert_result = rows.insert(std::move(e2), table_cmp);\n                        if (insert_result.second) {\n                            clogger.trace(\"csm {}: L{}: inserted dummy at {}\", fmt::ptr(this), __LINE__, insert_result.first->position());\n                            _snp->tracker()->insert(*insert_result.first);\n                        }\n                        clogger.trace(\"csm {}: set_continuous({}), prev={}, rt={}\", fmt::ptr(this), insert_result.first->position(),\n                                      _last_row.position(), _current_tombstone);\n                        set_rows_entry_continuous(*insert_result.first);\n                        insert_result.first->set_range_tombstone(_current_tombstone);\n                        clogger.trace(\"csm {}: set_continuous({})\", fmt::ptr(this), _last_row.position());\n                        set_rows_entry_continuous(*_last_row);\n                    });\n                } else {\n                    clogger.trace(\"csm {}: set_continuous({}), rt={}\", fmt::ptr(this), _last_row.position(), _current_tombstone);\n                    set_rows_entry_continuous(*_last_row);\n                    _last_row->set_range_tombstone(_current_tombstone);\n                }\n            } else {\n                if (_current_tombstone != e.range_tombstone() && !e.dummy()) {\n                    with_allocator(_snp->region().allocator(), [&] {\n                        auto e2 = alloc_strategy_unique_ptr<rows_entry>(\n                                current_allocator().construct<rows_entry>(table_s,\n                                                                          position_in_partition_view::before_key(e.position()),\n                                                                          is_dummy::yes,\n                                                                          is_continuous::yes));\n                        // Use _next_row iterator only as a hint because there could be insertions before\n                        // _next_row.get_iterator_in_latest_version(), either from concurrent reads,\n                        // from _next_row.ensure_entry_in_latest().\n                        auto insert_result = rows.insert_before_hint(_next_row.get_iterator_in_latest_version(), std::move(e2), table_cmp);\n                        if (insert_result.second) {\n                            clogger.trace(\"csm {}: L{}: inserted dummy at {}\", fmt::ptr(this), __LINE__, insert_result.first->position());\n                            _snp->tracker()->insert(*insert_result.first);\n                            clogger.trace(\"csm {}: set_continuous({}), prev={}, rt={}\", fmt::ptr(this), insert_result.first->position(),\n                                          _last_row.position(), _current_tombstone);\n                            set_rows_entry_continuous(*insert_result.first);\n                            insert_result.first->set_range_tombstone(_current_tombstone);\n                        }\n                        clogger.trace(\"csm {}: set_continuous({})\", fmt::ptr(this), e.position());\n                        set_rows_entry_continuous(e);\n                    });\n                } else {\n                    clogger.trace(\"csm {}: set_continuous({}), rt={}\", fmt::ptr(this), e.position(), _current_tombstone);\n                    e.set_range_tombstone(_current_tombstone);\n                    set_rows_entry_continuous(e);\n                }\n            }\n            maybe_drop_last_entry(_current_tombstone);\n        });\n    } else {\n        _read_context.cache().on_mispopulate();\n    }\n}\n\ninline\nvoid cache_mutation_reader::maybe_add_to_cache(const clustering_row& cr) {\n    if (!can_populate()) {\n        _last_row = nullptr;\n        _population_range_starts_before_all_rows = false;\n        _read_context.cache().on_mispopulate();\n        return;\n    }\n    clogger.trace(\"csm {}: populate({}), rt={}\", fmt::ptr(this), clustering_row::printer(*_schema, cr), _current_tombstone);\n    _lsa_manager.run_in_update_section_with_allocator([this, &cr] {\n        mutation_partition_v2& mp = _snp->version()->partition();\n        rows_entry::tri_compare cmp(table_schema());\n\n        if (_read_context.digest_requested()) {\n            cr.cells().prepare_hash(*_schema, column_kind::regular_column);\n        }\n        auto new_entry = alloc_strategy_unique_ptr<rows_entry>(\n            current_allocator().construct<rows_entry>(table_schema(), cr.key(), cr.as_deletable_row()));\n        new_entry->set_continuous(false);\n        new_entry->set_range_tombstone(_current_tombstone);\n        auto it = _next_row.iterators_valid() && _next_row.at_a_row() ? _next_row.get_iterator_in_latest_version()\n                                              : mp.clustered_rows().lower_bound(cr.key(), cmp);\n        auto insert_result = mp.mutable_clustered_rows().insert_before_hint(it, std::move(new_entry), cmp);\n        it = insert_result.first;\n        if (insert_result.second) {\n            _snp->tracker()->insert(*it);\n            restore_continuity_after_insertion(it);\n        }\n\n        rows_entry& e = *it;\n        if (ensure_population_lower_bound()) {\n            if (_read_context.is_reversed()) [[unlikely]] {\n                clogger.trace(\"csm {}: set_continuous({})\", fmt::ptr(this), _last_row.position());\n                set_rows_entry_continuous(*_last_row);\n                // _current_tombstone must also apply to _last_row itself (if it's non-dummy)\n                // because otherwise there would be a rtc after it, either creating a different entry,\n                // or clearing _last_row if population did not happen.\n                _last_row->set_range_tombstone(_current_tombstone);\n            } else {\n                clogger.trace(\"csm {}: set_continuous({})\", fmt::ptr(this), e.position());\n                set_rows_entry_continuous(e);\n                e.set_range_tombstone(_current_tombstone);\n            }\n        } else {\n            _read_context.cache().on_mispopulate();\n        }\n        with_allocator(standard_allocator(), [&] {\n            _last_row = partition_snapshot_row_weakref(*_snp, it, true);\n        });\n        _population_range_starts_before_all_rows = false;\n    });\n}\n\ninline\nbool cache_mutation_reader::maybe_add_to_cache(const range_tombstone_change& rtc) {\n    rows_entry::tri_compare q_cmp(*_schema);\n\n    clogger.trace(\"csm {}: maybe_add_to_cache({})\", fmt::ptr(this), rtc);\n\n    // Don't emit the closing range tombstone change, we may continue from cache with the same tombstone.\n    // The following relies on !_underlying_upper_bound->is_clustering_row()\n    if (q_cmp(rtc.position(), *_underlying_upper_bound) == 0) {\n        _lower_bound = rtc.position();\n        return false;\n    }\n\n    auto prev = std::exchange(_current_tombstone, rtc.tombstone());\n    if (_current_tombstone == prev) {\n        return false;\n    }\n\n    if (!can_populate()) {\n        // _current_tombstone is now invalid and remains so for this reader. No need to change it.\n        _last_row = nullptr;\n        _population_range_starts_before_all_rows = false;\n        _read_context.cache().on_mispopulate();\n        return true;\n    }\n\n    _lsa_manager.run_in_update_section_with_allocator([&] {\n        mutation_partition_v2& mp = _snp->version()->partition();\n        rows_entry::tri_compare cmp(table_schema());\n\n        auto new_entry = alloc_strategy_unique_ptr<rows_entry>(\n                current_allocator().construct<rows_entry>(table_schema(), to_table_domain(rtc.position()), is_dummy::yes, is_continuous::no));\n        auto it = _next_row.iterators_valid() && _next_row.at_a_row() ? _next_row.get_iterator_in_latest_version()\n                                              : mp.clustered_rows().lower_bound(to_table_domain(rtc.position()), cmp);\n        auto insert_result = mp.mutable_clustered_rows().insert_before_hint(it, std::move(new_entry), cmp);\n        it = insert_result.first;\n        if (insert_result.second) {\n            _snp->tracker()->insert(*it);\n            restore_continuity_after_insertion(it);\n        }\n\n        rows_entry& e = *it;\n        if (ensure_population_lower_bound()) {\n            // underlying may emit range_tombstone_change fragments with the same position.\n            // In such case, the range to which the tombstone from the first fragment applies is empty and should be ignored.\n            //\n            // Note: we are using a query schema comparator to compare table schema positions here,\n            // but this is okay because we are only checking for equality,\n            // which is preserved by schema reversals.\n            if (q_cmp(_last_row.position(), it->position()) != 0) {\n                if (_read_context.is_reversed()) [[unlikely]] {\n                    clogger.trace(\"csm {}: set_continuous({}), rt={}\", fmt::ptr(this), _last_row.position(), prev);\n                    set_rows_entry_continuous(*_last_row);\n                    _last_row->set_range_tombstone(prev);\n                } else {\n                    clogger.trace(\"csm {}: set_continuous({}), rt={}\", fmt::ptr(this), e.position(), prev);\n                    set_rows_entry_continuous(e);\n                    e.set_range_tombstone(prev);\n                }\n            }\n        } else {\n            _read_context.cache().on_mispopulate();\n        }\n        with_allocator(standard_allocator(), [&] {\n            _last_row = partition_snapshot_row_weakref(*_snp, it, true);\n        });\n        _population_range_starts_before_all_rows = false;\n    });\n    return true;\n}\n\ninline\nbool cache_mutation_reader::after_current_range(position_in_partition_view p) {\n    position_in_partition::tri_compare cmp(*_schema);\n    return cmp(p, _upper_bound) >= 0;\n}\n\ninline\nvoid cache_mutation_reader::start_reading_from_underlying() {\n    clogger.trace(\"csm {}: start_reading_from_underlying(), range=[{}, {})\", fmt::ptr(this), _lower_bound, _next_row_in_range ? _next_row.position() : _upper_bound);\n    _state = state::move_to_underlying;\n    _next_row.touch();\n}\n\ninline\nvoid cache_mutation_reader::copy_from_cache_to_buffer() {\n    clogger.trace(\"csm {}: copy_from_cache, next_row_in_range={}, next={}\", fmt::ptr(this), _next_row_in_range, _next_row);\n    _next_row.touch();\n\n    if (_next_row.range_tombstone() != _current_tombstone) {\n        position_in_partition::equal_compare eq(*_schema);\n        auto upper_bound = _next_row_in_range ? position_in_partition_view::before_key(_next_row.position()) : _upper_bound;\n        if (!eq(_lower_bound, upper_bound)) {\n            position_in_partition new_lower_bound(upper_bound);\n            auto tomb = _next_row.range_tombstone();\n            clogger.trace(\"csm {}: rtc({}, {}) ...{}\", fmt::ptr(this), _lower_bound, tomb, new_lower_bound);\n            push_mutation_fragment(mutation_fragment_v2(*_schema, _permit, range_tombstone_change(_lower_bound, tomb)));\n            _current_tombstone = tomb;\n            _lower_bound = std::move(new_lower_bound);\n            _read_context.cache()._tracker.on_range_tombstone_read();\n        }\n    }\n\n    if (_next_row_in_range) {\n        bool remove_row = false;\n\n        if (_read_context.tombstone_gc_state() // do not compact rows when tombstone_gc_state is not set (used in some unit tests)\n            && !_next_row.dummy()\n            && _snp->at_latest_version()\n            && _snp->at_oldest_version()) {\n            deletable_row& row = _next_row.latest_row();\n            tombstone range_tomb = _next_row.range_tombstone_for_row();\n            auto t = row.deleted_at();\n            t.apply(range_tomb);\n\n            auto row_tomb_expired = [&](row_tombstone tomb) {\n                return (tomb && tomb.max_deletion_time() < _gc_before);\n            };\n\n            auto is_row_dead = [&](const deletable_row& row) {\n                auto& m = row.marker();\n                return (!m.is_missing() && m.is_dead(_read_time) && m.deletion_time() < _gc_before);\n            };\n\n            if (row_tomb_expired(t) || is_row_dead(row)) {\n                const schema& row_schema = _next_row.latest_row_schema();\n\n                _read_context.cache()._tracker.on_row_compacted();\n\n                with_allocator(_snp->region().allocator(), [&] {\n                    deletable_row row_copy(row_schema, row);\n                    row_copy.compact_and_expire(row_schema, t.tomb(), _read_time, always_gc, _gc_before, nullptr);\n                    std::swap(row, row_copy);\n                });\n                remove_row = row.empty();\n\n                auto tomb_expired = [&](tombstone tomb) {\n                    return (tomb && tomb.deletion_time < _gc_before);\n                };\n\n                auto latests_range_tomb = _next_row.get_iterator_in_latest_version()->range_tombstone();\n                if (tomb_expired(latests_range_tomb)) {\n                    _next_row.get_iterator_in_latest_version()->set_range_tombstone({});\n                }\n            }\n        }\n\n        if (_next_row.range_tombstone_for_row() != _current_tombstone) [[unlikely]] {\n            auto tomb = _next_row.range_tombstone_for_row();\n            auto new_lower_bound = position_in_partition::before_key(_next_row.position());\n            clogger.trace(\"csm {}: rtc({}, {})\", fmt::ptr(this), new_lower_bound, tomb);\n            push_mutation_fragment(mutation_fragment_v2(*_schema, _permit, range_tombstone_change(new_lower_bound, tomb)));\n            _lower_bound = std::move(new_lower_bound);\n            _current_tombstone = tomb;\n            _read_context.cache()._tracker.on_range_tombstone_read();\n        }\n\n        if (remove_row) {\n            _read_context.cache()._tracker.on_row_compacted_away();\n\n            _lower_bound = position_in_partition::after_key(*_schema, _next_row.position());\n\n            partition_snapshot_row_weakref row_ref(_next_row);\n            move_to_next_entry();\n\n            with_allocator(_snp->region().allocator(), [&] {\n                cache_tracker& tracker = _read_context.cache()._tracker;\n                if (row_ref->is_linked()) {\n                    tracker.get_lru().remove(*row_ref);\n                }\n                row_ref->on_evicted(tracker);\n            });\n\n            _snp->region().allocator().invalidate_references();\n            _next_row.force_valid();\n        } else {\n            // We add the row to the buffer even when it's full.\n            // This simplifies the code. For more info see #3139.\n            add_to_buffer(_next_row);\n            move_to_next_entry();\n        }\n    } else {\n        move_to_next_range();\n    }\n}\n\ninline\nvoid cache_mutation_reader::move_to_end() {\n    finish_reader();\n    clogger.trace(\"csm {}: eos\", fmt::ptr(this));\n}\n\ninline\nvoid cache_mutation_reader::move_to_next_range() {\n    if (_current_tombstone) {\n        clogger.trace(\"csm {}: move_to_next_range: emit rtc({}, null)\", fmt::ptr(this), _upper_bound);\n        push_mutation_fragment(mutation_fragment_v2(*_schema, _permit, range_tombstone_change(_upper_bound, {})));\n        _current_tombstone = {};\n    }\n    auto next_it = std::next(_ck_ranges_curr);\n    if (next_it == _ck_ranges_end) {\n        move_to_end();\n        _ck_ranges_curr = next_it;\n    } else {\n        move_to_range(next_it);\n    }\n}\n\ninline\nvoid cache_mutation_reader::move_to_range(query::clustering_row_ranges::const_iterator next_it) {\n    auto lb = position_in_partition::for_range_start(*next_it);\n    auto ub = position_in_partition_view::for_range_end(*next_it);\n    _last_row = nullptr;\n    _lower_bound = std::move(lb);\n    _upper_bound = std::move(ub);\n    _ck_ranges_curr = next_it;\n    auto adjacent = _next_row.advance_to(_lower_bound);\n    _next_row_in_range = !after_current_range(_next_row.position());\n    clogger.trace(\"csm {}: move_to_range(), range={}, lb={}, ub={}, next={}\", fmt::ptr(this), *_ck_ranges_curr, _lower_bound, _upper_bound, _next_row.position());\n    if (!adjacent && !_next_row.continuous()) {\n        // FIXME: We don't insert a dummy for singular range to avoid allocating 3 entries\n        // for a hit (before, at and after). If we supported the concept of an incomplete row,\n        // we could insert such a row for the lower bound if it's full instead, for both singular and\n        // non-singular ranges.\n        if (_ck_ranges_curr->start() && !query::is_single_row(*_schema, *_ck_ranges_curr)) {\n            // Insert dummy for lower bound\n            if (can_populate()) {\n                // FIXME: _lower_bound could be adjacent to the previous row, in which case we could skip this\n                clogger.trace(\"csm {}: insert dummy at {}\", fmt::ptr(this), _lower_bound);\n                auto insert_result = with_allocator(_lsa_manager.region().allocator(), [&] {\n                    rows_entry::tri_compare cmp(table_schema());\n                    auto& rows = _snp->version()->partition().mutable_clustered_rows();\n                    auto new_entry = alloc_strategy_unique_ptr<rows_entry>(current_allocator().construct<rows_entry>(table_schema(),\n                            to_table_domain(_lower_bound), is_dummy::yes, is_continuous::no));\n                    return rows.insert_before_hint(\n                            _next_row.at_a_row() ? _next_row.get_iterator_in_latest_version() : rows.begin(),\n                            std::move(new_entry),\n                            cmp);\n                });\n                auto it = insert_result.first;\n                if (insert_result.second) {\n                    _snp->tracker()->insert(*it);\n                }\n                _last_row = partition_snapshot_row_weakref(*_snp, it, true);\n            } else {\n                _read_context.cache().on_mispopulate();\n            }\n        }\n        start_reading_from_underlying();\n    }\n}\n\n// Drops _last_row entry when possible without changing logical contents of the partition.\n// Call only when _last_row and _next_row are valid.\n// Calling after ensure_population_lower_bound() is ok.\n// _next_row must have a greater position than _last_row.\n// Invalidates references but keeps the _next_row valid.\ninline\nvoid cache_mutation_reader::maybe_drop_last_entry(tombstone rt) noexcept {\n    // Drop dummy entry if it falls inside a continuous range.\n    // This prevents unnecessary dummy entries from accumulating in cache and slowing down scans.\n    //\n    // Eviction can happen only from oldest versions to preserve the continuity non-overlapping rule\n    // (See docs/dev/row_cache.md)\n    //\n    if (_last_row\n            && !_read_context.is_reversed() // FIXME\n            && _last_row->dummy()\n            && _last_row->continuous()\n            && _last_row->range_tombstone() == rt\n            && _snp->at_latest_version()\n            && _snp->at_oldest_version()) {\n\n        clogger.trace(\"csm {}: dropping unnecessary dummy at {}\", fmt::ptr(this), _last_row->position());\n\n        with_allocator(_snp->region().allocator(), [&] {\n            cache_tracker& tracker = _read_context.cache()._tracker;\n            tracker.get_lru().remove(*_last_row);\n            _last_row->on_evicted(tracker);\n        });\n        _last_row = nullptr;\n\n        // There could be iterators pointing to _last_row, invalidate them\n        _snp->region().allocator().invalidate_references();\n\n        // Don't invalidate _next_row, move_to_next_entry() expects it to be still valid.\n        _next_row.force_valid();\n    }\n}\n\n// _next_row must be inside the range.\ninline\nvoid cache_mutation_reader::move_to_next_entry() {\n    clogger.trace(\"csm {}: move_to_next_entry(), curr={}\", fmt::ptr(this), _next_row.position());\n    if (no_clustering_row_between(*_schema, _next_row.position(), _upper_bound)) {\n        move_to_next_range();\n    } else {\n        auto new_last_row = partition_snapshot_row_weakref(_next_row);\n        // In reverse mode, the cursor may fall out of the entries because there is no dummy before all rows.\n        // Hence !next() doesn't mean we can end the read. The cursor will be positioned before all rows and\n        // not point at any row. continuous() is still correctly set.\n        _next_row.next();\n        _last_row = std::move(new_last_row);\n        _next_row_in_range = !after_current_range(_next_row.position());\n        clogger.trace(\"csm {}: next={}, cont={}, in_range={}\", fmt::ptr(this), _next_row.position(), _next_row.continuous(), _next_row_in_range);\n        if (!_next_row.continuous()) {\n            start_reading_from_underlying();\n        } else {\n            maybe_drop_last_entry(_next_row.range_tombstone());\n        }\n    }\n}\n\ninline\nvoid cache_mutation_reader::offer_from_underlying(mutation_fragment_v2&& mf) {\n    clogger.trace(\"csm {}: offer_from_underlying({})\", fmt::ptr(this), mutation_fragment_v2::printer(*_schema, mf));\n    if (mf.is_clustering_row()) {\n        maybe_add_to_cache(mf.as_clustering_row());\n        add_clustering_row_to_buffer(std::move(mf));\n    } else {\n        SCYLLA_ASSERT(mf.is_range_tombstone_change());\n        auto& chg = mf.as_range_tombstone_change();\n        if (maybe_add_to_cache(chg)) {\n            add_to_buffer(std::move(mf).as_range_tombstone_change());\n        }\n    }\n}\n\ninline\nvoid cache_mutation_reader::add_to_buffer(const partition_snapshot_row_cursor& row) {\n    position_in_partition::less_compare less(*_schema);\n    if (!row.dummy()) {\n        _read_context.cache().on_row_hit();\n        if (_read_context.digest_requested()) {\n            row.latest_row_prepare_hash();\n        }\n        add_clustering_row_to_buffer(mutation_fragment_v2(*_schema, _permit, row.row()));\n    } else {\n        if (less(_lower_bound, row.position())) {\n            _lower_bound = row.position();\n        }\n        _read_context.cache()._tracker.on_dummy_row_hit();\n    }\n}\n\n// Maintains the following invariants, also in case of exception:\n//   (1) no fragment with position >= _lower_bound was pushed yet\n//   (2) If _lower_bound > mf.position(), mf was emitted\ninline\nvoid cache_mutation_reader::add_clustering_row_to_buffer(mutation_fragment_v2&& mf) {\n    clogger.trace(\"csm {}: add_clustering_row_to_buffer({})\", fmt::ptr(this), mutation_fragment_v2::printer(*_schema, mf));\n    auto& row = mf.as_clustering_row();\n    auto new_lower_bound = position_in_partition::after_key(*_schema, row.key());\n    push_mutation_fragment(std::move(mf));\n    _lower_bound = std::move(new_lower_bound);\n    if (row.tomb()) {\n        _read_context.cache()._tracker.on_row_tombstone_read();\n    }\n}\n\ninline\nvoid cache_mutation_reader::add_to_buffer(range_tombstone_change&& rtc) {\n    clogger.trace(\"csm {}: add_to_buffer({})\", fmt::ptr(this), rtc);\n    _has_rt = true;\n    position_in_partition::less_compare less(*_schema);\n    _lower_bound = position_in_partition(rtc.position());\n    push_mutation_fragment(*_schema, _permit, std::move(rtc));\n    _read_context.cache()._tracker.on_range_tombstone_read();\n}\n\ninline\nvoid cache_mutation_reader::maybe_add_to_cache(const static_row& sr) {\n    if (can_populate()) {\n        clogger.trace(\"csm {}: populate({})\", fmt::ptr(this), static_row::printer(*_schema, sr));\n        _read_context.cache().on_static_row_insert();\n        _lsa_manager.run_in_update_section_with_allocator([&] {\n            if (_read_context.digest_requested()) {\n                sr.cells().prepare_hash(*_schema, column_kind::static_column);\n            }\n            // Static row is the same under table and query schema\n            _snp->version()->partition().static_row().apply(table_schema(), column_kind::static_column, sr.cells());\n        });\n    } else {\n        _read_context.cache().on_mispopulate();\n    }\n}\n\ninline\nvoid cache_mutation_reader::maybe_set_static_row_continuous() {\n    if (can_populate()) {\n        clogger.trace(\"csm {}: set static row continuous\", fmt::ptr(this));\n        _snp->version()->partition().set_static_row_continuous(true);\n    } else {\n        _read_context.cache().on_mispopulate();\n    }\n}\n\n// Last dummies can exist in a quasi-evicted state, where they are unlinked from LRU,\n// but still alive.\n// But while in this state, they mustn't carry any information (i.e. continuity),\n// due to the \"older versions are evicted first\" rule of MVCC.\n// Thus, when we make an entry continuous, we must ensure that it isn't an\n// unlinked last dummy.\ninline\nvoid cache_mutation_reader::set_rows_entry_continuous(rows_entry& e) {\n    e.set_continuous(true);\n    if (!e.is_linked()) [[unlikely]] {\n        _snp->tracker()->touch(e);\n    }\n}\n\ninline\nvoid cache_mutation_reader::restore_continuity_after_insertion(const mutation_partition::rows_type::iterator& it) {\n    if (auto x = std::next(it); x->continuous()) {\n        it->set_continuous(true);\n        it->set_range_tombstone(x->range_tombstone());\n    }\n}\n\ninline\nbool cache_mutation_reader::can_populate() const {\n    return _snp->at_latest_version() && _read_context.cache().phase_of(_read_context.key()) == _read_context.phase();\n}\n\n} // namespace cache\n\n// pass a reference to ctx to cache_mutation_reader\n// keeping its ownership at caller's.\ninline mutation_reader make_cache_mutation_reader(schema_ptr s,\n                                                            dht::decorated_key dk,\n                                                            query::clustering_key_filter_ranges crr,\n                                                            row_cache& cache,\n                                                            cache::read_context& ctx,\n                                                            partition_snapshot_ptr snp)\n{\n    return make_mutation_reader<cache::cache_mutation_reader>(\n        std::move(s), std::move(dk), std::move(crr), ctx, std::move(snp), cache);\n}\n\n// transfer ownership of ctx to cache_mutation_reader\ninline mutation_reader make_cache_mutation_reader(schema_ptr s,\n                                                            dht::decorated_key dk,\n                                                            query::clustering_key_filter_ranges crr,\n                                                            row_cache& cache,\n                                                            std::unique_ptr<cache::read_context> unique_ctx,\n                                                            partition_snapshot_ptr snp)\n{\n    return make_mutation_reader<cache::cache_mutation_reader>(\n        std::move(s), std::move(dk), std::move(crr), std::move(unique_ctx), std::move(snp), cache);\n}\n"
        },
        {
          "name": "cache_temperature.hh",
          "type": "blob",
          "size": 0.6982421875,
          "content": "/*\n * Copyright 2018-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <stdint.h>\n\nnamespace ser {\n\ntemplate <typename T>\nclass serializer;\n\n};\n\nclass cache_temperature {\n    float hit_rate;\n    explicit cache_temperature(uint8_t hr) : hit_rate(hr/255.0f) {}\npublic:\n    uint8_t get_serialized_temperature() const {\n        return hit_rate * 255;\n    }\n    cache_temperature() : hit_rate(0) {}\n    explicit cache_temperature(float hr) : hit_rate(hr) {}\n    explicit operator float() const { return hit_rate; }\n    static cache_temperature invalid() { return cache_temperature(-1.0f); }\n    friend struct ser::serializer<cache_temperature>;\n};\n"
        },
        {
          "name": "cartesian_product.hh",
          "type": "blob",
          "size": 2.9365234375,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n *\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <vector>\n#include <sys/types.h>\n\n// Single-pass range over cartesian product of vectors.\n\n// Note:\n//    {a, b, c} x {1, 2} = {{a, 1}, {a, 2}, {b, 1}, {b, 2}, {c, 1}, {c, 2}}\ntemplate<typename T>\nstruct cartesian_product {\n    const std::vector<std::vector<T>>& _vec_of_vecs;\npublic:\n    class iterator {\n    public:\n        using iterator_category = std::forward_iterator_tag;\n        using value_type = std::vector<T>;\n        using difference_type = std::ptrdiff_t;\n        using pointer = std::vector<T>*;\n        using reference = std::vector<T>&;\n    private:\n        size_t _pos;\n        const std::vector<std::vector<T>>* _vec_of_vecs;\n        value_type _current;\n        std::vector<typename std::vector<T>::const_iterator> _iterators;\n    public:\n        struct end_tag {};\n        iterator(end_tag) : _pos(-1) {}\n        iterator(const std::vector<std::vector<T>>& vec_of_vecs) : _pos(0), _vec_of_vecs(&vec_of_vecs) {\n            _iterators.reserve(vec_of_vecs.size());\n            for (auto&& vec : vec_of_vecs) {\n                _iterators.push_back(vec.begin());\n                if (vec.empty()) {\n                    _pos = -1;\n                    break;\n                }\n            }\n        }\n        value_type& operator*() {\n            _current.clear();\n            _current.reserve(_vec_of_vecs->size());\n            for (auto& i : _iterators) {\n                _current.emplace_back(*i);\n            }\n            return _current;\n        }\n        void operator++() {\n            ++_pos;\n\n            for (ssize_t i = _iterators.size() - 1; i >= 0; --i) {\n                ++_iterators[i];\n                if (_iterators[i] != (*_vec_of_vecs)[i].end()) {\n                    return;\n                }\n                _iterators[i] = (*_vec_of_vecs)[i].begin();\n            }\n\n            // If we're here it means we've covered every combination\n            _pos = -1;\n        }\n        bool operator==(const iterator& o) const { return _pos == o._pos; }\n    };\npublic:\n    cartesian_product(const std::vector<std::vector<T>>& vec_of_vecs) : _vec_of_vecs(vec_of_vecs) {}\n    iterator begin() { return iterator(_vec_of_vecs); }\n    iterator end() { return iterator(typename iterator::end_tag()); }\n};\n\ntemplate<typename T>\ninline\nsize_t cartesian_product_size(const std::vector<std::vector<T>>& vec_of_vecs) {\n    size_t r = 1;\n    for (auto&& vec : vec_of_vecs) {\n        r *= vec.size();\n    }\n    return r;\n}\n\ntemplate<typename T>\ninline\nbool cartesian_product_is_empty(const std::vector<std::vector<T>>& vec_of_vecs) {\n    for (auto&& vec : vec_of_vecs) {\n        if (vec.empty()) {\n            return true;\n        }\n    }\n    return vec_of_vecs.empty();\n}\n\ntemplate<typename T>\ninline\ncartesian_product<T> make_cartesian_product(const std::vector<std::vector<T>>& vec_of_vecs) {\n    return cartesian_product<T>(vec_of_vecs);\n}\n"
        },
        {
          "name": "cdc",
          "type": "tree",
          "content": null
        },
        {
          "name": "cell_locking.hh",
          "type": "blob",
          "size": 19.0771484375,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <exception>\n\n#include <boost/intrusive/unordered_set.hpp>\n\n#include \"utils/assert.hh\"\n#include \"utils/small_vector.hh\"\n#include \"mutation/mutation_partition.hh\"\n#include \"utils/xx_hasher.hh\"\n\n#include \"db/timeout_clock.hh\"\n#include \"utils/log.hh\"\n\nextern logging::logger cell_locker_log;\n\nclass cells_range {\n    using ids_vector_type = utils::small_vector<column_id, 5>;\n\n    position_in_partition_view _position;\n    ids_vector_type _ids;\npublic:\n    using iterator = ids_vector_type::iterator;\n    using const_iterator = ids_vector_type::const_iterator;\n\n    cells_range()\n        : _position(position_in_partition_view(position_in_partition_view::static_row_tag_t())) { }\n\n    explicit cells_range(position_in_partition_view pos, const row& cells)\n        : _position(pos)\n    {\n        _ids.reserve(cells.size());\n        cells.for_each_cell([this] (auto id, auto&&) {\n            _ids.emplace_back(id);\n        });\n    }\n\n    position_in_partition_view position() const { return _position; }\n    bool empty() const { return _ids.empty(); }\n\n    auto begin() const { return _ids.begin(); }\n    auto end() const { return _ids.end(); }\n};\n\nclass partition_cells_range {\n    const mutation_partition& _mp;\npublic:\n    class iterator {\n        const mutation_partition& _mp;\n        std::optional<mutation_partition::rows_type::const_iterator> _position;\n        cells_range _current;\n    public:\n        explicit iterator(const mutation_partition& mp)\n            : _mp(mp)\n            , _current(position_in_partition_view(position_in_partition_view::static_row_tag_t()), mp.static_row().get())\n        { }\n\n        iterator(const mutation_partition& mp, mutation_partition::rows_type::const_iterator it)\n            : _mp(mp)\n            , _position(it)\n        { }\n\n        iterator& operator++() {\n            if (!_position) {\n                _position = _mp.clustered_rows().begin();\n            } else {\n                ++(*_position);\n            }\n            if (_position != _mp.clustered_rows().end()) {\n                auto it = *_position;\n                _current = cells_range(position_in_partition_view(position_in_partition_view::clustering_row_tag_t(), it->key()),\n                        it->row().cells());\n            }\n            return *this;\n        }\n\n        iterator operator++(int) {\n            iterator it(*this);\n            operator++();\n            return it;\n        }\n\n        cells_range& operator*() {\n            return _current;\n        }\n\n        cells_range* operator->() {\n            return &_current;\n        }\n\n        bool operator==(const iterator& other) const {\n            return _position == other._position;\n        }\n    };\npublic:\n    explicit partition_cells_range(const mutation_partition& mp) : _mp(mp) { }\n\n    iterator begin() const {\n        return iterator(_mp);\n    }\n    iterator end() const {\n        return iterator(_mp, _mp.clustered_rows().end());\n    }\n};\n\nclass locked_cell;\n\nstruct cell_locker_stats {\n    uint64_t lock_acquisitions = 0;\n    uint64_t operations_waiting_for_lock = 0;\n};\n\nclass cell_locker {\nprivate:\n    class partition_entry;\n\n    struct cell_address {\n        position_in_partition position;\n        column_id id;\n    };\n\n    class cell_entry : public bi::unordered_set_base_hook<bi::link_mode<bi::auto_unlink>>,\n                       public enable_lw_shared_from_this<cell_entry> {\n        partition_entry& _parent;\n        cell_address _address;\n        db::timeout_semaphore _semaphore { 0 };\n\n        friend class cell_locker;\n    public:\n        cell_entry(partition_entry& parent, position_in_partition position, column_id id)\n            : _parent(parent)\n            , _address { std::move(position), id }\n        { }\n\n        // Upgrades cell_entry to another schema.\n        // Changes the value of cell_address, so cell_entry has to be\n        // temporarily removed from its parent partition_entry.\n        // Returns true if the cell_entry still exist in the new schema and\n        // should be reinserted.\n        bool upgrade(const schema& from, const schema& to, column_kind kind) noexcept {\n            auto& old_column_mapping = from.get_column_mapping();\n            auto& column = old_column_mapping.column_at(kind, _address.id);\n            auto cdef = to.get_column_definition(column.name());\n            if (!cdef) {\n                return false;\n            }\n            _address.id = cdef->id;\n            return true;\n        }\n\n        const position_in_partition& position() const {\n            return _address.position;\n        }\n\n        future<> lock(db::timeout_clock::time_point _timeout) {\n            return _semaphore.wait(_timeout);\n        }\n        void unlock() {\n            _semaphore.signal();\n        }\n\n        ~cell_entry() {\n            if (!is_linked()) {\n                return;\n            }\n            unlink();\n            if (!--_parent._cell_count) {\n                delete &_parent;\n            }\n        }\n\n        class hasher {\n            const ::schema* _schema; // pointer instead of reference for default assignment\n        public:\n            explicit hasher(const schema& s) : _schema(&s) { }\n\n            size_t operator()(const cell_address& ca) const {\n                xx_hasher hasher;\n                ca.position.feed_hash(hasher, *_schema);\n                ::feed_hash(hasher, ca.id);\n                return static_cast<size_t>(hasher.finalize_uint64());\n            }\n            size_t operator()(const cell_entry& ce) const {\n                return operator()(ce._address);\n            }\n        };\n\n        class equal_compare {\n            position_in_partition::equal_compare _cmp;\n        private:\n            bool do_compare(const cell_address& a, const cell_address& b) const {\n                return a.id == b.id && _cmp(a.position, b.position);\n            }\n        public:\n            explicit equal_compare(const schema& s) : _cmp(s) { }\n            bool operator()(const cell_address& ca, const cell_entry& ce) const {\n                return do_compare(ca, ce._address);\n            }\n            bool operator()(const cell_entry& ce, const cell_address& ca) const {\n                return do_compare(ca, ce._address);\n            }\n            bool operator()(const cell_entry& a, const cell_entry& b) const {\n                return do_compare(a._address, b._address);\n            }\n        };\n    };\n\n    class partition_entry : public bi::unordered_set_base_hook<bi::link_mode<bi::auto_unlink>> {\n        using cells_type = bi::unordered_set<cell_entry,\n                                             bi::equal<cell_entry::equal_compare>,\n                                             bi::hash<cell_entry::hasher>,\n                                             bi::constant_time_size<false>>;\n\n        static constexpr size_t initial_bucket_count = 16;\n        using max_load_factor = std::ratio<3, 4>;\n        dht::decorated_key _key;\n        cell_locker& _parent;\n        size_t _rehash_at_size = compute_rehash_at_size(initial_bucket_count);\n        std::unique_ptr<cells_type::bucket_type[]> _buckets; // TODO: start with internal storage?\n        size_t _cell_count = 0; // cells_type::empty() is not O(1) if the hook is auto-unlink\n        cells_type::bucket_type _internal_buckets[initial_bucket_count];\n        cells_type _cells;\n        schema_ptr _schema;\n\n        friend class cell_entry;\n    private:\n        static constexpr size_t compute_rehash_at_size(size_t bucket_count) {\n            return bucket_count * max_load_factor::num / max_load_factor::den;\n        }\n\n        // Try to rehash the set, if needed.\n        // The function may fail silently on bad_alloc (logging a warning).\n        // Rehashing would be retried at a later time on failure.\n        void maybe_rehash() {\n            if (_cell_count >= _rehash_at_size) {\n                auto new_bucket_count = std::min(_cells.bucket_count() * 2, _cells.bucket_count() + 1024);\n                try {\n                    auto buckets = std::make_unique<cells_type::bucket_type[]>(new_bucket_count);\n\n                    _cells.rehash(cells_type::bucket_traits(buckets.get(), new_bucket_count));\n                    _buckets = std::move(buckets);\n                } catch (const std::bad_alloc&) {\n                    cell_locker_log.warn(\"Could not rehash cell_locker partition cells set: bucket_count={} new_bucket_count={}: {}\", _cells.bucket_count(), new_bucket_count, std::current_exception());\n                }\n                // Attempt rehash at the new size in both success and failure paths.\n                // On failure, we don't want to retry too soon since it may take some time\n                // for memory to free up.\n                _rehash_at_size = compute_rehash_at_size(new_bucket_count);\n            }\n        }\n    public:\n        partition_entry(schema_ptr s, cell_locker& parent, const dht::decorated_key& dk)\n            : _key(dk)\n            , _parent(parent)\n            , _cells(cells_type::bucket_traits(_internal_buckets, initial_bucket_count),\n                     cell_entry::hasher(*s), cell_entry::equal_compare(*s))\n            , _schema(s)\n        { }\n\n        ~partition_entry() {\n            if (is_linked()) {\n                _parent._partition_count--;\n            }\n        }\n\n        // Upgrades partition entry to new schema. Returns false if all\n        // cell_entries has been removed during the upgrade.\n        bool upgrade(schema_ptr new_schema);\n\n        void insert(lw_shared_ptr<cell_entry> cell) {\n            _cells.insert(*cell);\n            _cell_count++;\n            maybe_rehash();\n        }\n\n        cells_type& cells() {\n            return _cells;\n        }\n\n        struct hasher {\n            size_t operator()(const dht::decorated_key& dk) const {\n                return std::hash<dht::decorated_key>()(dk);\n            }\n            size_t operator()(const partition_entry& pe) const {\n                return operator()(pe._key);\n            }\n        };\n\n        class equal_compare {\n            dht::decorated_key_equals_comparator _cmp;\n        public:\n            explicit equal_compare(const schema& s) : _cmp(s) { }\n            bool operator()(const dht::decorated_key& dk, const partition_entry& pe) {\n                return _cmp(dk, pe._key);\n            }\n            bool operator()(const partition_entry& pe, const dht::decorated_key& dk) {\n                return _cmp(dk, pe._key);\n            }\n            bool operator()(const partition_entry& a, const partition_entry& b) {\n                return _cmp(a._key, b._key);\n            }\n        };\n    };\n\n    using partitions_type = bi::unordered_set<partition_entry,\n                                              bi::equal<partition_entry::equal_compare>,\n                                              bi::hash<partition_entry::hasher>,\n                                              bi::constant_time_size<false>>;\n\n    static constexpr size_t initial_bucket_count = 4 * 1024;\n    using max_load_factor = std::ratio<3, 4>;\n\n    std::unique_ptr<partitions_type::bucket_type[]> _buckets;\n    partitions_type _partitions;\n    size_t _partition_count = 0;\n    size_t _rehash_at_size = compute_rehash_at_size(initial_bucket_count);\n    schema_ptr _schema;\n\n    // partitions_type uses equality comparator which keeps a reference to the\n    // original schema, we must ensure that it doesn't die.\n    schema_ptr _original_schema;\n    cell_locker_stats& _stats;\n\n    friend class locked_cell;\nprivate:\n    struct locker;\n\n    static constexpr size_t compute_rehash_at_size(size_t bucket_count) {\n        return bucket_count * max_load_factor::num / max_load_factor::den;\n    }\n\n    // Try to rehash the set, if needed.\n    // The function may fail silently on bad_alloc (logging a warning).\n    // Rehashing would be retried at a later time on failure.\n    void maybe_rehash() {\n        if (_partition_count >= _rehash_at_size) {\n            auto new_bucket_count = std::min(_partitions.bucket_count() * 2, _partitions.bucket_count() + 64 * 1024);\n            try {\n                auto buckets = std::make_unique<partitions_type::bucket_type[]>(new_bucket_count);\n\n                _partitions.rehash(partitions_type::bucket_traits(buckets.get(), new_bucket_count));\n                _buckets = std::move(buckets);\n            } catch (const std::bad_alloc&) {\n                cell_locker_log.warn(\"Could not rehash cell_locker partitions set: bucket_count={} new_bucket_count={}: {}\", _partitions.bucket_count(), new_bucket_count, std::current_exception());\n            }\n            // Attempt rehash at the new size in both success and failure paths.\n            // On failure, we don't want to retry too soon since it may take some time\n            // for memory to free up.\n            _rehash_at_size = compute_rehash_at_size(new_bucket_count);\n        }\n    }\npublic:\n    explicit cell_locker(schema_ptr s, cell_locker_stats& stats)\n        : _buckets(std::make_unique<partitions_type::bucket_type[]>(initial_bucket_count))\n        , _partitions(partitions_type::bucket_traits(_buckets.get(), initial_bucket_count),\n                      partition_entry::hasher(), partition_entry::equal_compare(*s))\n        , _schema(s)\n        , _original_schema(std::move(s))\n        , _stats(stats)\n    { }\n\n    ~cell_locker() {\n        SCYLLA_ASSERT(_partitions.empty());\n    }\n\n    void set_schema(schema_ptr s) {\n        _schema = s;\n    }\n    schema_ptr schema() const {\n        return _schema;\n    }\n\n    // partition_cells_range is required to be in cell_locker::schema()\n    future<std::vector<locked_cell>> lock_cells(const dht::decorated_key& dk, partition_cells_range&& range,\n                                                db::timeout_clock::time_point timeout);\n};\n\n\nclass locked_cell {\n    lw_shared_ptr<cell_locker::cell_entry> _entry;\npublic:\n    explicit locked_cell(lw_shared_ptr<cell_locker::cell_entry> entry)\n        : _entry(std::move(entry)) { }\n\n    locked_cell(const locked_cell&) = delete;\n    locked_cell(locked_cell&&) = default;\n\n    ~locked_cell() {\n        if (_entry) {\n            _entry->unlock();\n        }\n    }\n};\n\nstruct cell_locker::locker {\n    cell_entry::hasher _hasher;\n    cell_entry::equal_compare _eq_cmp;\n    partition_entry& _partition_entry;\n\n    partition_cells_range _range;\n    partition_cells_range::iterator _current_ck;\n    cells_range::const_iterator _current_cell;\n\n    db::timeout_clock::time_point _timeout;\n    std::vector<locked_cell> _locks;\n    cell_locker_stats& _stats;\nprivate:\n    void update_ck() {\n        if (!is_done()) {\n            _current_cell = _current_ck->begin();\n        }\n    }\n\n    future<> lock_next();\n\n    bool is_done() const { return _current_ck == _range.end(); }\npublic:\n    explicit locker(const ::schema& s, cell_locker_stats& st, partition_entry& pe, partition_cells_range&& range, db::timeout_clock::time_point timeout)\n        : _hasher(s)\n        , _eq_cmp(s)\n        , _partition_entry(pe)\n        , _range(std::move(range))\n        , _current_ck(_range.begin())\n        , _timeout(timeout)\n        , _stats(st)\n    {\n        update_ck();\n    }\n\n    locker(const locker&) = delete;\n    locker(locker&&) = delete;\n\n    future<> lock_all() {\n        // Cannot defer before first call to lock_next().\n        return lock_next().then([this] {\n            return do_until([this] { return is_done(); }, [this] {\n                return lock_next();\n            });\n        });\n    }\n\n    std::vector<locked_cell> get() && { return std::move(_locks); }\n};\n\ninline\nfuture<std::vector<locked_cell>> cell_locker::lock_cells(const dht::decorated_key& dk, partition_cells_range&& range, db::timeout_clock::time_point timeout) {\n    partition_entry::hasher pe_hash;\n    partition_entry::equal_compare pe_eq(*_schema);\n\n    auto it = _partitions.find(dk, pe_hash, pe_eq);\n    std::unique_ptr<partition_entry> partition;\n    if (it == _partitions.end()) {\n        partition = std::make_unique<partition_entry>(_schema, *this, dk);\n    } else if (!it->upgrade(_schema)) {\n        partition = std::unique_ptr<partition_entry>(&*it);\n        _partition_count--;\n        _partitions.erase(it);\n    }\n\n    if (partition) {\n        std::vector<locked_cell> locks;\n        for (auto&& r : range) {\n            if (r.empty()) {\n                continue;\n            }\n            for (auto&& c : r) {\n                auto cell = make_lw_shared<cell_entry>(*partition, position_in_partition(r.position()), c);\n                _stats.lock_acquisitions++;\n                partition->insert(cell);\n                locks.emplace_back(std::move(cell));\n            }\n        }\n\n        if (!locks.empty()) {\n            _partitions.insert(*partition.release());\n            _partition_count++;\n            maybe_rehash();\n        }\n        return make_ready_future<std::vector<locked_cell>>(std::move(locks));\n    }\n\n    auto l = std::make_unique<locker>(*_schema, _stats, *it, std::move(range), timeout);\n    auto f = l->lock_all();\n    return f.then([l = std::move(l)] {\n        return std::move(*l).get();\n    });\n}\n\ninline\nfuture<> cell_locker::locker::lock_next() {\n    while (!is_done()) {\n        if (_current_cell == _current_ck->end()) {\n            ++_current_ck;\n            update_ck();\n            continue;\n        }\n\n        auto cid = *_current_cell++;\n\n        cell_address ca { position_in_partition(_current_ck->position()), cid };\n        auto it = _partition_entry.cells().find(ca, _hasher, _eq_cmp);\n        if (it != _partition_entry.cells().end()) {\n            _stats.operations_waiting_for_lock++;\n            return it->lock(_timeout).then([this, ce = it->shared_from_this()] () mutable {\n                _stats.operations_waiting_for_lock--;\n                _stats.lock_acquisitions++;\n                _locks.emplace_back(std::move(ce));\n            });\n        }\n\n        auto cell = make_lw_shared<cell_entry>(_partition_entry, position_in_partition(_current_ck->position()), cid);\n        _stats.lock_acquisitions++;\n        _partition_entry.insert(cell);\n        _locks.emplace_back(std::move(cell));\n    }\n    return make_ready_future<>();\n}\n\ninline\nbool cell_locker::partition_entry::upgrade(schema_ptr new_schema) {\n    if (_schema == new_schema) {\n        return true;\n    }\n\n    auto buckets = std::make_unique<cells_type::bucket_type[]>(_cells.bucket_count());\n    auto cells = cells_type(cells_type::bucket_traits(buckets.get(), _cells.bucket_count()),\n                            cell_entry::hasher(*new_schema), cell_entry::equal_compare(*new_schema));\n\n    _cells.clear_and_dispose([&] (cell_entry* cell_ptr) noexcept {\n        auto& cell = *cell_ptr;\n        auto kind = cell.position().is_static_row() ? column_kind::static_column\n                                                    : column_kind::regular_column;\n        auto reinsert = cell.upgrade(*_schema, *new_schema, kind);\n        if (reinsert) {\n            cells.insert(cell);\n        } else {\n            _cell_count--;\n        }\n    });\n\n    // bi::unordered_set move assignment is actually a swap.\n    // Original _buckets cannot be destroyed before the container using them is\n    // so we need to explicitly make sure that the original _cells is no more.\n    _cells = std::move(cells);\n    auto destroy = [] (auto) { };\n    destroy(std::move(cells));\n\n    _buckets = std::move(buckets);\n    _schema = new_schema;\n    return _cell_count;\n}\n"
        },
        {
          "name": "checked-file-impl.hh",
          "type": "blob",
          "size": 4.384765625,
          "content": "/*\n * Copyright (C) 2016-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <seastar/core/file.hh>\n#include <seastar/core/seastar.hh>\n#include \"utils/disk-error-handler.hh\"\n\n#include \"seastarx.hh\"\n\nclass checked_file_impl : public file_impl {\npublic:\n\n    checked_file_impl(const io_error_handler& error_handler, file f)\n            : file_impl(*get_file_impl(f)),  _error_handler(error_handler), _file(f) {\n    }\n\n    virtual future<size_t> write_dma(uint64_t pos, const void* buffer, size_t len, io_intent* intent) override {\n        return do_io_check(_error_handler, [&] {\n            return get_file_impl(_file)->write_dma(pos, buffer, len, intent);\n        });\n    }\n\n    virtual future<size_t> write_dma(uint64_t pos, std::vector<iovec> iov, io_intent* intent) override {\n        return do_io_check(_error_handler, [&] {\n            return get_file_impl(_file)->write_dma(pos, iov, intent);\n        });\n    }\n\n    virtual future<size_t> read_dma(uint64_t pos, void* buffer, size_t len, io_intent* intent) override {\n        return do_io_check(_error_handler, [&] {\n            return get_file_impl(_file)->read_dma(pos, buffer, len, intent);\n        });\n    }\n\n    virtual future<size_t> read_dma(uint64_t pos, std::vector<iovec> iov, io_intent* intent) override {\n        return do_io_check(_error_handler, [&] {\n            return get_file_impl(_file)->read_dma(pos, iov, intent);\n        });\n    }\n\n    virtual future<> flush(void) override {\n        return do_io_check(_error_handler, [&] {\n            return get_file_impl(_file)->flush();\n        });\n    }\n\n    virtual future<struct stat> stat(void) override {\n        return do_io_check(_error_handler, [&] {\n            return get_file_impl(_file)->stat();\n        });\n    }\n\n    virtual future<> truncate(uint64_t length) override {\n        return do_io_check(_error_handler, [&] {\n            return get_file_impl(_file)->truncate(length);\n        });\n    }\n\n    virtual future<> discard(uint64_t offset, uint64_t length) override {\n        return do_io_check(_error_handler, [&] {\n            return get_file_impl(_file)->discard(offset, length);\n        });\n    }\n\n    virtual future<> allocate(uint64_t position, uint64_t length) override {\n        return do_io_check(_error_handler, [&] {\n            return get_file_impl(_file)->allocate(position, length);\n        });\n    }\n\n    virtual future<uint64_t> size(void) override {\n        return do_io_check(_error_handler, [&] {\n            return get_file_impl(_file)->size();\n        });\n    }\n\n    virtual future<> close() override {\n        return do_io_check(_error_handler, [&] {\n            return get_file_impl(_file)->close();\n        });\n    }\n\n    // returns a handle for plain file, so make_checked_file() should be called\n    // on file returned by handle.\n    virtual std::unique_ptr<seastar::file_handle_impl> dup() override {\n        return get_file_impl(_file)->dup();\n    }\n\n    virtual subscription<directory_entry> list_directory(std::function<future<> (directory_entry de)> next) override {\n        return do_io_check(_error_handler, [&] {\n            return get_file_impl(_file)->list_directory(next);\n        });\n    }\n\n    virtual future<temporary_buffer<uint8_t>> dma_read_bulk(uint64_t offset, size_t range_size, io_intent* intent) override {\n        return do_io_check(_error_handler, [&] {\n            return get_file_impl(_file)->dma_read_bulk(offset, range_size, intent);\n        });\n    }\nprivate:\n    const io_error_handler& _error_handler;\n    file _file;\n};\n\ninline file make_checked_file(const io_error_handler& error_handler, file f)\n{\n    return file(::make_shared<checked_file_impl>(error_handler, f));\n}\n\nfuture<file>\ninline open_checked_file_dma(const io_error_handler& error_handler,\n                             sstring name, open_flags flags,\n                             file_open_options options = {})\n{\n    return do_io_check(error_handler, [&] {\n        return open_file_dma(name, flags, options).then([&] (file f) {\n            return make_ready_future<file>(make_checked_file(error_handler, f));\n        });\n    });\n}\n\nfuture<file>\ninline open_checked_directory(const io_error_handler& error_handler,\n                              sstring name)\n{\n    return do_io_check(error_handler, [&] {\n        return open_directory(name).then([&] (file f) {\n            return make_ready_future<file>(make_checked_file(error_handler, f));\n        });\n    });\n}\n"
        },
        {
          "name": "client_data.cc",
          "type": "blob",
          "size": 0.7783203125,
          "content": "/*\n * Copyright (C) 2019-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"client_data.hh\"\n#include <stdexcept>\n\nsstring to_string(client_type ct) {\n    switch (ct) {\n        case client_type::cql: return \"cql\";\n        case client_type::thrift: return \"thrift\";\n        case client_type::alternator: return \"alternator\";\n    }\n    throw std::runtime_error(\"Invalid client_type\");\n}\n\nsstring to_string(client_connection_stage ccs) {\n    switch (ccs) {\n        case client_connection_stage::established: return \"ESTABLISHED\";\n        case client_connection_stage::authenticating: return \"AUTHENTICATING\";\n        case client_connection_stage::ready: return \"READY\";\n    }\n    throw std::runtime_error(\"Invalid client_connection_stage\");\n}\n"
        },
        {
          "name": "client_data.hh",
          "type": "blob",
          "size": 1.345703125,
          "content": "/*\n * Copyright (C) 2019-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n#pragma once\n\n#include <seastar/net/inet_address.hh>\n#include <seastar/core/sstring.hh>\n#include \"seastarx.hh\"\n\n#include <optional>\n\nenum class client_type {\n    cql = 0,\n    thrift,\n    alternator,\n};\n\nsstring to_string(client_type ct);\n\nenum class client_connection_stage {\n    established = 0,\n    authenticating,\n    ready,\n};\n\nsstring to_string(client_connection_stage ct);\n\n// Representation of a row in `system.clients'. std::optionals are for nullable cells.\nstruct client_data {\n    net::inet_address ip;\n    int32_t port;\n    client_type ct = client_type::cql;\n    client_connection_stage connection_stage = client_connection_stage::established;\n    int32_t shard_id;  /// ID of server-side shard which is processing the connection.\n\n    std::optional<sstring> driver_name;\n    std::optional<sstring> driver_version;\n    std::optional<sstring> hostname;\n    std::optional<int32_t> protocol_version;\n    std::optional<sstring> ssl_cipher_suite;\n    std::optional<bool> ssl_enabled;\n    std::optional<sstring> ssl_protocol;\n    std::optional<sstring> username;\n    std::optional<sstring> scheduling_group_name;\n\n    sstring stage_str() const { return to_string(connection_stage); }\n    sstring client_type_str() const { return to_string(ct); }\n};\n"
        },
        {
          "name": "clocks-impl.cc",
          "type": "blob",
          "size": 0.4169921875,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include <fmt/chrono.h>\n\n#include \"timestamp.hh\"\n\n#include \"clocks-impl.hh\"\n\nstd::atomic<int64_t> clocks_offset;\n\nstd::string format_timestamp(api::timestamp_type ts) {\n    std::chrono::system_clock::time_point when{api::timestamp_clock::duration(ts)};\n    return fmt::format(\"{:%Y/%m/%d %T}\", when);\n}\n"
        },
        {
          "name": "clocks-impl.hh",
          "type": "blob",
          "size": 0.9716796875,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <algorithm>\n#include <atomic>\n#include <chrono>\n#include <cstdint>\n\nextern std::atomic<int64_t> clocks_offset;\n\ntemplate<typename Duration>\ninline void forward_jump_clocks(Duration delta)\n{\n    auto d = std::chrono::duration_cast<std::chrono::seconds>(delta).count();\n    clocks_offset.fetch_add(d, std::memory_order_relaxed);\n}\n\ninline std::chrono::seconds get_clocks_offset()\n{\n    auto off = clocks_offset.load(std::memory_order_relaxed);\n    return std::chrono::seconds(off);\n}\n\n// Returns a time point which is earlier from t by d, or minimum time point if it cannot be represented.\ntemplate<typename Clock, typename Duration, typename Rep, typename Period>\ninline\nauto saturating_subtract(std::chrono::time_point<Clock, Duration> t, std::chrono::duration<Rep, Period> d) -> decltype(t) {\n    return std::max(t, decltype(t)::min() + d) - d;\n}\n"
        },
        {
          "name": "clustering_bounds_comparator.hh",
          "type": "blob",
          "size": 6.55078125,
          "content": "\n/*\n * Copyright (C) 2016-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <functional>\n#include \"keys.hh\"\n#include \"schema/schema_fwd.hh\"\n#include \"interval.hh\"\n\n/**\n * Represents the kind of bound in a range tombstone.\n */\nenum class bound_kind : uint8_t {\n    excl_end = 0,\n    incl_start = 1,\n    // values 2 to 5 are reserved for forward Origin compatibility\n    incl_end = 6,\n    excl_start = 7,\n};\n\n// Swaps start <-> end && incl <-> excl\nbound_kind invert_kind(bound_kind k);\n// Swaps start <-> end\nbound_kind reverse_kind(bound_kind k);\nint32_t weight(bound_kind k);\n\nclass bound_view {\n    const static thread_local clustering_key _empty_prefix;\n    std::reference_wrapper<const clustering_key_prefix> _prefix;\n    bound_kind _kind;\npublic:\n    bound_view(const clustering_key_prefix& prefix, bound_kind kind)\n        : _prefix(prefix)\n        , _kind(kind)\n    { }\n    bound_view(const bound_view& other) noexcept = default;\n    bound_view& operator=(const bound_view& other) noexcept = default;\n\n    bound_kind kind() const { return _kind; }\n    const clustering_key_prefix& prefix() const { return _prefix; }\n\n    struct tri_compare {\n        // To make it assignable and to avoid taking a schema_ptr, we\n        // wrap the schema reference.\n        std::reference_wrapper<const schema> _s;\n        tri_compare(const schema& s) : _s(s)\n        { }\n        std::strong_ordering operator()(const clustering_key_prefix& p1, int32_t w1, const clustering_key_prefix& p2, int32_t w2) const {\n            auto type = _s.get().clustering_key_prefix_type();\n            auto res = prefix_equality_tri_compare(type->types().begin(),\n                type->begin(p1.representation()), type->end(p1.representation()),\n                type->begin(p2.representation()), type->end(p2.representation()),\n                ::tri_compare);\n            if (res != 0) {\n                return res;\n            }\n            auto d1 = p1.size(_s);\n            auto d2 = p2.size(_s);\n\n            /*\n             * The logic below is\n             *\n             * if d1 == d2 { return w1 <=> w2 }\n             * if d1 < d2  { if w1 <= 0 return less else return greater\n             * if d1 > d2  { if w2 <= 0 return greater else return less\n             *\n             * Those w vs 0 checks are effectively { w <=> 0.5 } which is the same\n             * as { 2*w <=> 1 }. Next, w1 <=> w2 is equivalent to 2*w1 <=> 2*w2, so\n             * all three branches can be collapsed into a single <=>.\n             *\n             * This looks like veiling the above ifs for no reason, but it really\n             * helps compiler generate jump-less assembly.\n             */\n\n            return ((d1 <= d2) ? w1 << 1 : 1) <=> ((d2 <= d1) ? w2 << 1 : 1);\n        }\n        std::strong_ordering operator()(const bound_view b, const clustering_key_prefix& p) const {\n            return operator()(b._prefix, weight(b._kind), p, 0);\n        }\n        std::strong_ordering operator()(const clustering_key_prefix& p, const bound_view b) const {\n            return operator()(p, 0, b._prefix, weight(b._kind));\n        }\n        std::strong_ordering operator()(const bound_view b1, const bound_view b2) const {\n            return operator()(b1._prefix, weight(b1._kind), b2._prefix, weight(b2._kind));\n        }\n    };\n    struct compare {\n        // To make it assignable and to avoid taking a schema_ptr, we\n        // wrap the schema reference.\n        tri_compare _cmp;\n        compare(const schema& s) : _cmp(s)\n        { }\n        bool operator()(const clustering_key_prefix& p1, int32_t w1, const clustering_key_prefix& p2, int32_t w2) const {\n            return _cmp(p1, w1, p2, w2) < 0;\n        }\n        bool operator()(const bound_view b, const clustering_key_prefix& p) const {\n            return operator()(b._prefix, weight(b._kind), p, 0);\n        }\n        bool operator()(const clustering_key_prefix& p, const bound_view b) const {\n            return operator()(p, 0, b._prefix, weight(b._kind));\n        }\n        bool operator()(const bound_view b1, const bound_view b2) const {\n            return operator()(b1._prefix, weight(b1._kind), b2._prefix, weight(b2._kind));\n        }\n    };\n    bool equal(const schema& s, const bound_view other) const {\n        return _kind == other._kind && _prefix.get().equal(s, other._prefix.get());\n    }\n    bool adjacent(const schema& s, const bound_view other) const {\n        return invert_kind(other._kind) == _kind && _prefix.get().equal(s, other._prefix.get());\n    }\n    static bound_view bottom() {\n        return {_empty_prefix, bound_kind::incl_start};\n    }\n    static bound_view top() {\n        return {_empty_prefix, bound_kind::incl_end};\n    }\n    template<template<typename> typename R>\n    requires Interval<R, clustering_key_prefix_view>\n    static bound_view from_range_start(const R<clustering_key_prefix>& range) {\n        return range.start()\n               ? bound_view(range.start()->value(), range.start()->is_inclusive() ? bound_kind::incl_start : bound_kind::excl_start)\n               : bottom();\n    }\n    template<template<typename> typename R>\n    requires Interval<R, clustering_key_prefix>\n    static bound_view from_range_end(const R<clustering_key_prefix>& range) {\n        return range.end()\n               ? bound_view(range.end()->value(), range.end()->is_inclusive() ? bound_kind::incl_end : bound_kind::excl_end)\n               : top();\n    }\n    template<template<typename> typename R>\n    requires Interval<R, clustering_key_prefix>\n    static std::pair<bound_view, bound_view> from_range(const R<clustering_key_prefix>& range) {\n        return {from_range_start(range), from_range_end(range)};\n    }\n    template<template<typename> typename R>\n    requires Interval<R, clustering_key_prefix_view>\n    static std::optional<typename R<clustering_key_prefix_view>::bound> to_interval_bound(const bound_view& bv) {\n        if (&bv._prefix.get() == &_empty_prefix) {\n            return {};\n        }\n        bool inclusive = bv._kind != bound_kind::excl_end && bv._kind != bound_kind::excl_start;\n        return {typename R<clustering_key_prefix_view>::bound(bv._prefix.get().view(), inclusive)};\n    }\n    friend fmt::formatter<bound_view>;\n};\n\ntemplate <> struct fmt::formatter<bound_kind> : fmt::formatter<string_view> {\n    auto format(bound_kind, fmt::format_context& ctx) const -> decltype(ctx.out());\n};\ntemplate <> struct fmt::formatter<bound_view> : fmt::formatter<string_view> {\n    auto format(const bound_view& b, fmt::format_context& ctx) const {\n        return fmt::format_to(ctx.out(), \"{{bound: prefix={},kind={}}}\", b._prefix.get(), b._kind);\n    }\n};\n"
        },
        {
          "name": "clustering_interval_set.hh",
          "type": "blob",
          "size": 5.3544921875,
          "content": "/*\n * Copyright (C) 2020-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"utils/assert.hh\"\n#include \"schema/schema_fwd.hh\"\n#include \"mutation/position_in_partition.hh\"\n#include <boost/icl/interval_set.hpp>\n\n// Represents a non-contiguous subset of clustering_key domain of a particular schema.\n// Can be treated like an ordered and non-overlapping sequence of position_range:s.\nclass clustering_interval_set {\n    // Needed to make position_in_partition comparable, required by boost::icl::interval_set.\n    class position_in_partition_with_schema {\n        schema_ptr _schema;\n        position_in_partition _pos;\n    public:\n        position_in_partition_with_schema()\n            : _pos(position_in_partition::for_static_row())\n        { }\n        position_in_partition_with_schema(schema_ptr s, position_in_partition pos)\n            : _schema(std::move(s))\n            , _pos(std::move(pos))\n        { }\n        bool operator<(const position_in_partition_with_schema& other) const {\n            return position_in_partition::less_compare(*_schema)(_pos, other._pos);\n        }\n        bool operator==(const position_in_partition_with_schema& other) const {\n            return position_in_partition::equal_compare(*_schema)(_pos, other._pos);\n        }\n        const position_in_partition& position() const { return _pos; }\n    };\nprivate:\n    // We want to represent intervals of clustering keys, not position_in_partitions,\n    // but clustering_key domain is not enough to represent all kinds of clustering ranges.\n    // All intervals in this set are of the form [x, y).\n    using set_type = boost::icl::interval_set<position_in_partition_with_schema>;\n    using interval = boost::icl::interval<position_in_partition_with_schema>;\n    set_type _set;\npublic:\n    clustering_interval_set() = default;\n    // Constructs from legacy clustering_row_ranges\n    clustering_interval_set(const schema& s, const query::clustering_row_ranges& ranges) {\n        for (auto&& r : ranges) {\n            add(s, position_range::from_range(r));\n        }\n    }\n    query::clustering_row_ranges to_clustering_row_ranges() const {\n        query::clustering_row_ranges result;\n        for (position_range r : *this) {\n            result.push_back(query::clustering_range::make(\n                {r.start().key(), r.start()._bound_weight != bound_weight::after_all_prefixed},\n                {r.end().key(), r.end()._bound_weight == bound_weight::after_all_prefixed}));\n        }\n        return result;\n    }\n    class position_range_iterator {\n    public:\n        using iterator_category = std::input_iterator_tag;\n        using value_type = const position_range;\n        using difference_type = std::ptrdiff_t;\n        using pointer = const position_range*;\n        using reference = const position_range&;\n    private:\n        set_type::iterator _i;\n    public:\n        position_range_iterator(set_type::iterator i) : _i(i) {}\n        position_range operator*() const {\n            // FIXME: Produce position_range view. Not performance critical yet.\n            const interval::interval_type& iv = *_i;\n            return position_range{iv.lower().position(), iv.upper().position()};\n        }\n        bool operator==(const position_range_iterator& other) const = default;\n        position_range_iterator& operator++() {\n            ++_i;\n            return *this;\n        }\n        position_range_iterator operator++(int) {\n            auto tmp = *this;\n            ++_i;\n            return tmp;\n        }\n    };\n    static interval::type make_interval(const schema& s, const position_range& r) {\n        SCYLLA_ASSERT(r.start().has_clustering_key());\n        SCYLLA_ASSERT(r.end().has_clustering_key());\n        return interval::right_open(\n            position_in_partition_with_schema(s.shared_from_this(), r.start()),\n            position_in_partition_with_schema(s.shared_from_this(), r.end()));\n    }\npublic:\n    bool equals(const schema& s, const clustering_interval_set& other) const {\n        return boost::equal(_set, other._set);\n    }\n    bool contains(const schema& s, position_in_partition_view pos) const {\n        // FIXME: Avoid copy\n        return _set.find(position_in_partition_with_schema(s.shared_from_this(), position_in_partition(pos))) != _set.end();\n    }\n    // Returns true iff this set is fully contained in the other set.\n    bool contained_in(clustering_interval_set& other) const {\n        return boost::icl::within(_set, other._set);\n    }\n    bool overlaps(const schema& s, const position_range& range) const {\n        // FIXME: Avoid copy\n        auto r = _set.equal_range(make_interval(s, range));\n        return r.first != r.second;\n    }\n    // Adds given clustering range to this interval set.\n    // The range may overlap with this set.\n    void add(const schema& s, const position_range& r) {\n        _set += make_interval(s, r);\n    }\n    void add(const schema& s, const clustering_interval_set& other) {\n        for (auto&& r : other) {\n            add(s, r);\n        }\n    }\n    position_range_iterator begin() const { return {_set.begin()}; }\n    position_range_iterator end() const { return {_set.end()}; }\n};\n\ntemplate <> struct fmt::formatter<clustering_interval_set> : fmt::formatter<string_view> {\n    auto format(const clustering_interval_set& set, fmt::format_context& ctx) const {\n        return fmt::format_to(ctx.out(), \"{{{}}}\", fmt::join(set, \",\\n  \"));\n    }\n};\n"
        },
        {
          "name": "clustering_key_filter.hh",
          "type": "blob",
          "size": 1.9365234375,
          "content": "/*\n * Copyright (C) 2016-present ScyllaDB\n *\n * Modified by ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"schema/schema_fwd.hh\"\n#include \"query-request.hh\"\n\nnamespace query {\n\nclass clustering_key_filter_ranges {\n    clustering_row_ranges _storage;\n    std::reference_wrapper<const clustering_row_ranges> _ref;\npublic:\n    clustering_key_filter_ranges(const clustering_row_ranges& ranges) : _ref(ranges) { }\n    clustering_key_filter_ranges(clustering_row_ranges&& ranges)\n        : _storage(std::make_move_iterator(ranges.begin()), std::make_move_iterator(ranges.end())), _ref(_storage) {}\n\n    clustering_key_filter_ranges(clustering_key_filter_ranges&& other) noexcept\n        : _storage(std::move(other._storage))\n        , _ref(&other._ref.get() == &other._storage ? _storage : other._ref.get())\n    { }\n\n    clustering_key_filter_ranges& operator=(clustering_key_filter_ranges&& other) noexcept {\n        if (this != &other) {\n            _storage = std::move(other._storage);\n            _ref = (&other._ref.get() == &other._storage) ? _storage : other._ref.get();\n        }\n        return *this;\n    }\n\n    auto begin() const { return _ref.get().begin(); }\n    auto end() const { return _ref.get().end(); }\n    bool empty() const { return _ref.get().empty(); }\n    size_t size() const { return _ref.get().size(); }\n    const clustering_row_ranges& ranges() const { return _ref; }\n\n    // Returns all clustering ranges determined by `slice` inside partition determined by `key`.\n    // The ranges will be returned in the same order as stored in the slice. For a reversed slice\n    // a reverse schema shall be provided.\n    static clustering_key_filter_ranges get_ranges(const schema& schema, const query::partition_slice& slice, const partition_key& key) {\n        const query::clustering_row_ranges& ranges = slice.row_ranges(schema, key);\n        return clustering_key_filter_ranges(ranges);\n    }\n};\n\n}\n"
        },
        {
          "name": "clustering_ranges_walker.hh",
          "type": "blob",
          "size": 11.548828125,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n *\n * Modified by ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"utils/assert.hh\"\n#include \"schema/schema.hh\"\n#include \"query-request.hh\"\n#include \"mutation/mutation_fragment.hh\"\n#include \"mutation/mutation_fragment_v2.hh\"\n\n#include <boost/range/iterator_range.hpp>\n\n// Utility for in-order checking of overlap with position ranges.\nclass clustering_ranges_walker {\n    const schema& _schema;\n    const query::clustering_row_ranges& _ranges;\n    boost::iterator_range<query::clustering_row_ranges::const_iterator> _current_range;\n    bool _in_current; // next position is known to be >= _current_start\n    bool _past_current; // next position is known to be >= _current_end\n    bool _using_clustering_range; // Whether current range comes from _current_range\n    bool _with_static_row;\n    position_in_partition_view _current_start;\n    position_in_partition_view _current_end;\n    std::optional<position_in_partition> _trim;\n    size_t _change_counter = 1;\n    tombstone _tombstone;\nprivate:\n    bool advance_to_next_range() {\n        _in_current = false;\n        _past_current = false;\n        if (_using_clustering_range) {\n            if (!_current_range) {\n                return false;\n            }\n            _current_range.advance_begin(1);\n        }\n        ++_change_counter;\n        _using_clustering_range = true;\n        if (!_current_range) {\n            _current_end = _current_start = position_in_partition_view::after_all_clustered_rows();\n            return false;\n        }\n        _current_start = position_in_partition_view::for_range_start(_current_range.front());\n        _current_end = position_in_partition_view::for_range_end(_current_range.front());\n        return true;\n    }\n\n    void set_current_positions() {\n        _using_clustering_range = false;\n         if (!_with_static_row) {\n            if (!_current_range) {\n                _current_start = position_in_partition_view::before_all_clustered_rows();\n            } else {\n                _current_start = position_in_partition_view::for_range_start(_current_range.front());\n                _current_end = position_in_partition_view::for_range_end(_current_range.front());\n                _using_clustering_range = true;\n            }\n        } else {\n             // If the first range is contiguous with the static row, then advance _current_end as much as we can\n             if (_current_range && !_current_range.front().start()) {\n                 _current_end = position_in_partition_view::for_range_end(_current_range.front());\n                 _using_clustering_range = true;\n             }\n        }\n    }\n\npublic:\n    clustering_ranges_walker(const schema& s, const query::clustering_row_ranges& ranges, bool with_static_row = true)\n            : _schema(s)\n            , _ranges(ranges)\n            , _current_range(ranges)\n            , _in_current(with_static_row)\n            , _past_current(false)\n            , _with_static_row(with_static_row)\n            , _current_start(position_in_partition_view::for_static_row())\n            , _current_end(position_in_partition_view::before_all_clustered_rows()) {\n        set_current_positions();\n    }\n\n    clustering_ranges_walker(const clustering_ranges_walker&) = delete;\n    clustering_ranges_walker(clustering_ranges_walker&&) = delete;\n\n    clustering_ranges_walker& operator=(const clustering_ranges_walker&) = delete;\n    clustering_ranges_walker& operator=(clustering_ranges_walker&&) = delete;\n\n    using range_tombstones = utils::small_vector<range_tombstone_change, 1>;\n\n    // Result of advancing to a given position.\n    struct progress {\n        // True iff the position is contained in requested ranges.\n        bool contained;\n\n        // Range tombstone changes to emit which reflect current range tombstone\n        // trimmed to requested ranges, up to the advanced-to position (inclusive).\n        //\n        // It is guaranteed that the sequence of tombstones returned from all\n        // advance_to() calls will be the same for a given ranges no matter at\n        // which positions you call advance_to(), provided that you change\n        // the current tombstone at the same positions.\n        // Redundant changes will not be generated.\n        // This is to support the guarantees of mutation_reader.\n        range_tombstones rts;\n    };\n\n    // Excludes positions smaller than pos from the ranges.\n    // pos should be monotonic.\n    // No constraints between pos and positions passed to advance_to().\n    //\n    // After the invocation, when !out_of_range(), lower_bound() returns the smallest position still contained.\n    //\n    // After this, advance_to(lower_bound()) will always emit a range tombstone change for pos\n    // if there is an active range tombstone and !out_of_range().\n    void trim_front(position_in_partition pos) {\n        position_in_partition::less_compare less(_schema);\n\n        do {\n            if (!less(_current_start, pos)) {\n                break;\n            }\n            if (less(pos, _current_end)) {\n                _trim = std::move(pos);\n                _current_start = *_trim;\n                _in_current = false;\n                ++_change_counter;\n                break;\n            }\n        } while (advance_to_next_range());\n    }\n\n    // Returns true if given position is contained.\n    // Must be called with monotonic positions.\n    // Idempotent.\n    bool advance_to(position_in_partition_view pos) {\n        return advance_to(pos, _tombstone).contained;\n    }\n\n    // Returns result of advancing over clustering restrictions.\n    // Must be called with monotonic positions.\n    //\n    // The walker tracks current clustered tombstone.\n    // The new_tombstone will be the current clustered tombstone after advancing, starting from pos (inclusive).\n    // The returned progress object contains range_tombstone_change fragments which reflect changes of\n    // the current clustered tombstone trimmed to the boundaries of requested ranges, up to the\n    // advanced-to position (inclusive).\n    progress advance_to(position_in_partition_view pos, tombstone new_tombstone) {\n        position_in_partition::less_compare less(_schema);\n        range_tombstones rts;\n\n        auto prev_tombstone = _tombstone;\n        _tombstone = new_tombstone;\n\n        do {\n            if (!_in_current && less(pos, _current_start)) {\n                break;\n            }\n\n            if (!_in_current && prev_tombstone) {\n                rts.push_back(range_tombstone_change(_current_start, prev_tombstone));\n            }\n\n            // All subsequent clustering keys are larger than the start of this\n            // range so there is no need to check that again.\n            _in_current = true;\n\n            if (less(pos, _current_end)) {\n                if (prev_tombstone != new_tombstone) {\n                    rts.push_back(range_tombstone_change(pos, new_tombstone));\n                }\n                return progress{.contained = true, .rts = std::move(rts)};\n            } else {\n                if (!_past_current && prev_tombstone) {\n                    rts.push_back(range_tombstone_change(_current_end, {}));\n                }\n                _past_current = true;\n            }\n        } while (advance_to_next_range());\n\n        return progress{.contained = false, .rts = std::move(rts)};\n    }\n\n    // Returns true if the range expressed by start and end (as in position_range) overlaps\n    // with clustering ranges.\n    // Must be called with monotonic start position. That position must also be greater than\n    // the last position passed to the other advance_to() overload.\n    // Idempotent.\n    // Breaks the tracking of current range tombstone, so don't use if you also use the advance_to()\n    // overload which tracks tombstones.\n    bool advance_to(position_in_partition_view start, position_in_partition_view end) {\n        position_in_partition::less_compare less(_schema);\n\n        do {\n            if (!less(_current_start, end)) {\n                break;\n            }\n            if (less(start, _current_end)) {\n                return true;\n            }\n        } while (advance_to_next_range());\n\n        return false;\n    }\n\n    // Returns true if the range tombstone expressed by start and end (as in position_range) overlaps\n    // with clustering ranges.\n    // No monotonicity restrictions on argument values across calls.\n    // Does not affect lower_bound().\n    // Idempotent.\n    bool contains_tombstone(position_in_partition_view start, position_in_partition_view end) const {\n        position_in_partition::less_compare less(_schema);\n\n        if (_trim && !less(*_trim, end)) {\n            return false;\n        }\n\n        for (const auto& rng : _current_range) {\n            auto range_start = position_in_partition_view::for_range_start(rng);\n            if (!less(range_start, end)) {\n                return false;\n            }\n            auto range_end = position_in_partition_view::for_range_end(rng);\n            if (less(start, range_end)) {\n                return true;\n            }\n        }\n\n        return false;\n    }\n\n    // Intersects rt with query ranges. The first overlap is returned and the rest is applied to dst.\n    // If returns a disengaged optional, there is no overlap and nothing was applied to dst.\n    // No monotonicity restrictions on argument values across calls.\n    // Does not affect lower_bound().\n    std::optional<range_tombstone> split_tombstone(range_tombstone rt, range_tombstone_stream& dst) const {\n        position_in_partition::less_compare less(_schema);\n\n        if (_trim && !rt.trim_front(_schema, *_trim)) {\n            return std::nullopt;\n        }\n\n        std::optional<range_tombstone> first;\n\n        for (const auto& rng : _current_range) {\n            auto range_start = position_in_partition_view::for_range_start(rng);\n            auto range_end = position_in_partition_view::for_range_end(rng);\n            if (!less(rt.position(), range_start) && !less(range_end, rt.end_position())) {\n                // Fully enclosed by this range.\n                SCYLLA_ASSERT(!first);\n                return std::move(rt);\n            }\n            auto this_range_rt = rt;\n            if (this_range_rt.trim(_schema, range_start, range_end)) {\n                if (first) {\n                    dst.apply(std::move(this_range_rt));\n                } else {\n                    first = std::move(this_range_rt);\n                }\n            }\n        }\n\n        return first;\n    }\n\n    tombstone current_tombstone() const {\n        return _tombstone;\n    }\n\n    void set_tombstone(tombstone t) {\n        _tombstone = t;\n    }\n\n    // Returns true if advanced past all contained positions. Any later advance_to() until reset() will return false.\n    bool out_of_range() const {\n        return !_in_current && !_current_range;\n    }\n\n    // Resets the state of the walker so that advance_to() can be now called for new sequence of positions.\n    // Any range trimmings still hold after this.\n    void reset() {\n        _current_range = _ranges;\n        _in_current = _with_static_row;\n        _past_current = false;\n        _current_start = position_in_partition_view::for_static_row();\n        _current_end = position_in_partition_view::before_all_clustered_rows();\n        set_current_positions();\n        ++_change_counter;\n        if (_trim) {\n            trim_front(*std::exchange(_trim, {}));\n        }\n    }\n\n    // Can be called only when !out_of_range()\n    position_in_partition_view lower_bound() const {\n        return _current_start;\n    }\n\n    // When lower_bound() changes, this also does\n    // Always > 0.\n    size_t lower_bound_change_counter() const {\n        return _change_counter;\n    }\n};\n"
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "collection_mutation.cc",
          "type": "blob",
          "size": 16.7685546875,
          "content": "/*\n * Copyright (C) 2019-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"utils/assert.hh\"\n#include \"types/collection.hh\"\n#include \"types/user.hh\"\n#include \"concrete_types.hh\"\n#include \"mutation/mutation_partition.hh\"\n#include \"compaction/compaction_garbage_collector.hh\"\n#include \"combine.hh\"\n\n#include \"collection_mutation.hh\"\n\n#include <boost/range/numeric.hpp>\n\nbytes_view collection_mutation_input_stream::read_linearized(size_t n) {\n    managed_bytes_view mbv = ::read_simple_bytes(_src, n);\n    if (mbv.is_linearized()) {\n        return mbv.current_fragment();\n    } else {\n        return _linearized.emplace_front(linearized(mbv));\n    }\n}\nmanaged_bytes_view collection_mutation_input_stream::read_fragmented(size_t n) {\n    return ::read_simple_bytes(_src, n);\n}\nbool collection_mutation_input_stream::empty() const {\n    return _src.empty();\n}\n\ncollection_mutation::collection_mutation(const abstract_type& type, collection_mutation_view v)\n    : _data(v.data) {}\n\ncollection_mutation::collection_mutation(const abstract_type& type, managed_bytes data)\n    : _data(std::move(data)) {}\n\ncollection_mutation::operator collection_mutation_view() const\n{\n    return collection_mutation_view{managed_bytes_view(_data)};\n}\n\ncollection_mutation_view atomic_cell_or_collection::as_collection_mutation() const {\n    return collection_mutation_view{managed_bytes_view(_data)};\n}\n\nbool collection_mutation_view::is_empty() const {\n    auto in = collection_mutation_input_stream(data);\n    auto has_tomb = in.read_trivial<uint8_t>();\n    return !has_tomb && in.read_trivial<uint32_t>() == 0;\n}\n\nbool collection_mutation_view::is_any_live(const abstract_type& type, tombstone tomb, gc_clock::time_point now) const {\n    auto in = collection_mutation_input_stream(data);\n    auto has_tomb = in.read_trivial<uint8_t>();\n    if (has_tomb) {\n        auto ts = in.read_trivial<api::timestamp_type>();\n        auto ttl = in.read_trivial<gc_clock::duration::rep>();\n        tomb.apply(tombstone{ts, gc_clock::time_point(gc_clock::duration(ttl))});\n    }\n\n    auto nr = in.read_trivial<uint32_t>();\n    for (uint32_t i = 0; i != nr; ++i) {\n        auto key_size = in.read_trivial<uint32_t>();\n        in.read_fragmented(key_size); // Skip\n        auto vsize = in.read_trivial<uint32_t>();\n        auto value = atomic_cell_view::from_bytes(type, in.read_fragmented(vsize));\n        if (value.is_live(tomb, now, false)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n\napi::timestamp_type collection_mutation_view::last_update(const abstract_type& type) const {\n    auto in = collection_mutation_input_stream(data);\n    api::timestamp_type max = api::missing_timestamp;\n    auto has_tomb = in.read_trivial<uint8_t>();\n    if (has_tomb) {\n        max = std::max(max, in.read_trivial<api::timestamp_type>());\n        (void)in.read_trivial<gc_clock::duration::rep>();\n    }\n\n    auto nr = in.read_trivial<uint32_t>();\n    for (uint32_t i = 0; i != nr; ++i) {\n        const auto key_size = in.read_trivial<uint32_t>();\n        in.read_fragmented(key_size); // Skip\n        auto vsize = in.read_trivial<uint32_t>();\n        auto value = atomic_cell_view::from_bytes(type, in.read_fragmented(vsize));\n        max = std::max(value.timestamp(), max);\n    }\n\n    return max;\n}\n\nauto fmt::formatter<collection_mutation_view::printer>::format(const collection_mutation_view::printer& cmvp, fmt::format_context& ctx) const\n    -> decltype(ctx.out()) {\n    auto out = ctx.out();\n    out = fmt::format_to(out, \"{{collection_mutation_view \");\n    cmvp._cmv.with_deserialized(cmvp._type, [&out, &type = cmvp._type] (const collection_mutation_view_description& cmvd) {\n        bool first = true;\n        out = fmt::format_to(out, \"tombstone {}\", cmvd.tomb);\n        visit(type, make_visitor(\n        [&] (const collection_type_impl& ctype) {\n            auto&& key_type = ctype.name_comparator();\n            auto&& value_type = ctype.value_comparator();\n            out = fmt::format_to(out, \" collection cells {{\");\n            for (auto&& [key, value] : cmvd.cells) {\n                if (!first) {\n                    out = fmt::format_to(out, \", \");\n                }\n                fmt::format_to(out, \"{}: {}\", key_type->to_string(key), atomic_cell_view::printer(*value_type, value));\n                first = false;\n            }\n            out = fmt::format_to(out, \"}}\");\n        },\n        [&] (const user_type_impl& utype) {\n            out = fmt::format_to(out, \" user-type cells {{\");\n            for (auto&& [raw_idx, value] : cmvd.cells) {\n                if (first) {\n                    out = fmt::format_to(out, \" \");\n                } else {\n                    out = fmt::format_to(out, \", \");\n                }\n                auto idx = deserialize_field_index(raw_idx);\n                out = fmt::format_to(out, \"{}: {}\", utype.field_name_as_string(idx), atomic_cell_view::printer(*utype.type(idx), value));\n                first = false;\n            }\n            out = fmt::format_to(out, \"}}\");\n        },\n        [&] (const abstract_type& o) {\n            // Not throwing exception in this likely-to-be debug context\n            out = fmt::format_to(out, \" attempted to pretty-print collection_mutation_view_description with type {}\", o.name());\n        }\n        ));\n    });\n    return fmt::format_to(out, \"}}\");\n}\n\n\ncollection_mutation_description\ncollection_mutation_view_description::materialize(const abstract_type& type) const {\n    collection_mutation_description m;\n    m.tomb = tomb;\n    m.cells.reserve(cells.size());\n\n    visit(type, make_visitor(\n    [&] (const collection_type_impl& ctype) {\n        auto& value_type = *ctype.value_comparator();\n        for (auto&& e : cells) {\n            m.cells.emplace_back(to_bytes(e.first), atomic_cell(value_type, e.second));\n        }\n    },\n    [&] (const user_type_impl& utype) {\n        for (auto&& e : cells) {\n            m.cells.emplace_back(to_bytes(e.first), atomic_cell(*utype.type(deserialize_field_index(e.first)), e.second));\n        }\n    },\n    [&] (const abstract_type& o) {\n        throw std::runtime_error(format(\"attempted to materialize collection_mutation_view_description with type {}\", o.name()));\n    }\n    ));\n\n    return m;\n}\n\ncompact_and_expire_result collection_mutation_description::compact_and_expire(column_id id, row_tombstone base_tomb, gc_clock::time_point query_time,\n    can_gc_fn& can_gc, gc_clock::time_point gc_before, compaction_garbage_collector* collector)\n{\n    compact_and_expire_result res{};\n    if (tomb) {\n        res.collection_tombstones++;\n    }\n    auto t = tomb;\n    tombstone purged_tomb;\n    if (tomb <= base_tomb.regular()) {\n        tomb = tombstone();\n    } else if (tomb.deletion_time < gc_before && can_gc(tomb, is_shadowable::no)) { // The collection tombstone is never shadowable\n        purged_tomb = tomb;\n        tomb = tombstone();\n    }\n    t.apply(base_tomb.regular());\n    utils::chunked_vector<std::pair<bytes, atomic_cell>> survivors;\n    utils::chunked_vector<std::pair<bytes, atomic_cell>> losers;\n    for (auto&& name_and_cell : cells) {\n        atomic_cell& cell = name_and_cell.second;\n        auto cannot_erase_cell = [&] {\n            // Only row tombstones can be shadowable, (collection) cell tombstones aren't\n            return cell.deletion_time() >= gc_before || !can_gc(tombstone(cell.timestamp(), cell.deletion_time()), is_shadowable::no);\n        };\n\n        if (cell.is_covered_by(t, false) || cell.is_covered_by(base_tomb.shadowable().tomb(), false)) {\n            res.dead_cells++;\n            continue;\n        }\n        if (cell.has_expired(query_time)) {\n            if (cannot_erase_cell()) {\n                survivors.emplace_back(std::make_pair(\n                    std::move(name_and_cell.first), atomic_cell::make_dead(cell.timestamp(), cell.deletion_time())));\n            } else if (collector) {\n                losers.emplace_back(std::pair(\n                        std::move(name_and_cell.first), atomic_cell::make_dead(cell.timestamp(), cell.deletion_time())));\n            }\n            res.dead_cells++;\n        } else if (!cell.is_live()) {\n            if (cannot_erase_cell()) {\n                survivors.emplace_back(std::move(name_and_cell));\n            } else if (collector) {\n                losers.emplace_back(std::move(name_and_cell));\n            }\n            res.dead_cells++;\n        } else {\n            survivors.emplace_back(std::move(name_and_cell));\n            res.live_cells++;\n        }\n    }\n    if (collector) {\n        collector->collect(id, collection_mutation_description{purged_tomb, std::move(losers)});\n    }\n    cells = std::move(survivors);\n    return res;\n}\n\ntemplate <typename Iterator>\nstatic collection_mutation serialize_collection_mutation(\n        const abstract_type& type,\n        const tombstone& tomb,\n        std::ranges::subrange<Iterator> cells) {\n    auto element_size = [] (size_t c, auto&& e) -> size_t {\n        return c + 8 + e.first.size() + e.second.serialize().size();\n    };\n    auto size = std::ranges::fold_left(cells, (size_t)4, element_size);\n    size += 1;\n    if (tomb) {\n        size += sizeof(int64_t) + sizeof(int64_t);\n    }\n    managed_bytes ret(managed_bytes::initialized_later(), size);\n    managed_bytes_mutable_view out(ret);\n    write<uint8_t>(out, uint8_t(bool(tomb)));\n    if (tomb) {\n        write<int64_t>(out, tomb.timestamp);\n        write<int64_t>(out, tomb.deletion_time.time_since_epoch().count());\n    }\n    auto writek = [&out] (bytes_view v) {\n        write<int32_t>(out, v.size());\n        write_fragmented(out, single_fragmented_view(v));\n    };\n    auto writev = [&out] (managed_bytes_view v) {\n        write<int32_t>(out, v.size());\n        write_fragmented(out, v);\n    };\n    // FIXME: overflow?\n    write<int32_t>(out, std::ranges::distance(cells));\n    for (auto&& kv : cells) {\n        auto&& k = kv.first;\n        auto&& v = kv.second;\n        writek(k);\n\n        writev(v.serialize());\n    }\n    return collection_mutation(type, ret);\n}\n\ncollection_mutation collection_mutation_description::serialize(const abstract_type& type) const {\n    return serialize_collection_mutation(type, tomb, std::ranges::subrange(cells.begin(), cells.end()));\n}\n\ncollection_mutation collection_mutation_view_description::serialize(const abstract_type& type) const {\n    return serialize_collection_mutation(type, tomb, std::ranges::subrange(cells.begin(), cells.end()));\n}\n\ntemplate <typename C>\nrequires std::is_base_of_v<abstract_type, std::remove_reference_t<C>>\nstatic collection_mutation_view_description\nmerge(collection_mutation_view_description a, collection_mutation_view_description b, C&& key_type) {\n    using element_type = std::pair<bytes_view, atomic_cell_view>;\n\n    auto compare = [&] (const element_type& e1, const element_type& e2) {\n        return key_type.less(e1.first, e2.first);\n    };\n\n    auto merge = [] (const element_type& e1, const element_type& e2) {\n        // FIXME: use std::max()?\n        return std::make_pair(e1.first, compare_atomic_cell_for_merge(e1.second, e2.second) > 0 ? e1.second : e2.second);\n    };\n\n    // applied to a tombstone, returns a predicate checking whether a cell is killed by\n    // the tombstone\n    auto cell_killed = [] (const std::optional<tombstone>& t) {\n        return [&t] (const element_type& e) {\n            if (!t) {\n                return false;\n            }\n            // tombstone wins if timestamps equal here, unlike row tombstones\n            if (t->timestamp < e.second.timestamp()) {\n                return false;\n            }\n            return true;\n            // FIXME: should we consider TTLs too?\n        };\n    };\n\n    collection_mutation_view_description merged;\n    merged.cells.reserve(a.cells.size() + b.cells.size());\n\n    combine(a.cells.begin(), std::remove_if(a.cells.begin(), a.cells.end(), cell_killed(b.tomb)),\n            b.cells.begin(), std::remove_if(b.cells.begin(), b.cells.end(), cell_killed(a.tomb)),\n            std::back_inserter(merged.cells),\n            compare,\n            merge);\n    merged.tomb = std::max(a.tomb, b.tomb);\n\n    return merged;\n}\n\ncollection_mutation merge(const abstract_type& type, collection_mutation_view a, collection_mutation_view b) {\n    return a.with_deserialized(type, [&] (collection_mutation_view_description a_view) {\n        return b.with_deserialized(type, [&] (collection_mutation_view_description b_view) {\n            return visit(type, make_visitor(\n            [&] (const collection_type_impl& ctype) {\n                return merge(std::move(a_view), std::move(b_view), *ctype.name_comparator());\n            },\n            [&] (const user_type_impl& utype) {\n                return merge(std::move(a_view), std::move(b_view), *short_type);\n            },\n            [] (const abstract_type& o) -> collection_mutation_view_description {\n                throw std::runtime_error(format(\"collection_mutation merge: unknown type: {}\", o.name()));\n            }\n            )).serialize(type);\n        });\n    });\n}\n\ntemplate <typename C>\nrequires std::is_base_of_v<abstract_type, std::remove_reference_t<C>>\nstatic collection_mutation_view_description\ndifference(collection_mutation_view_description a, collection_mutation_view_description b, C&& key_type)\n{\n    collection_mutation_view_description diff;\n    diff.cells.reserve(std::max(a.cells.size(), b.cells.size()));\n\n    auto it = b.cells.begin();\n    for (auto&& c : a.cells) {\n        while (it != b.cells.end() && key_type.less(it->first, c.first)) {\n            ++it;\n        }\n        if (it == b.cells.end() || !key_type.equal(it->first, c.first)\n            || compare_atomic_cell_for_merge(c.second, it->second) > 0) {\n\n            auto cell = std::make_pair(c.first, c.second);\n            diff.cells.emplace_back(std::move(cell));\n        }\n    }\n    if (a.tomb > b.tomb) {\n        diff.tomb = a.tomb;\n    }\n\n    return diff;\n}\n\ncollection_mutation difference(const abstract_type& type, collection_mutation_view a, collection_mutation_view b)\n{\n    return a.with_deserialized(type, [&] (collection_mutation_view_description a_view) {\n        return b.with_deserialized(type, [&] (collection_mutation_view_description b_view) {\n            return visit(type, make_visitor(\n            [&] (const collection_type_impl& ctype) {\n                return difference(std::move(a_view), std::move(b_view), *ctype.name_comparator());\n            },\n            [&] (const user_type_impl& utype) {\n                return difference(std::move(a_view), std::move(b_view), *short_type);\n            },\n            [] (const abstract_type& o) -> collection_mutation_view_description {\n                throw std::runtime_error(format(\"collection_mutation difference: unknown type: {}\", o.name()));\n            }\n            )).serialize(type);\n        });\n    });\n}\n\ntemplate <typename F>\nrequires std::is_invocable_r_v<std::pair<bytes_view, atomic_cell_view>, F, collection_mutation_input_stream&>\nstatic collection_mutation_view_description\ndeserialize_collection_mutation(collection_mutation_input_stream& in, F&& read_kv) {\n    collection_mutation_view_description ret;\n\n    auto has_tomb = in.read_trivial<uint8_t>();\n    if (has_tomb) {\n        auto ts = in.read_trivial<api::timestamp_type>();\n        auto ttl = in.read_trivial<gc_clock::duration::rep>();\n        ret.tomb = tombstone{ts, gc_clock::time_point(gc_clock::duration(ttl))};\n    }\n\n    auto nr = in.read_trivial<uint32_t>();\n    ret.cells.reserve(nr);\n    for (uint32_t i = 0; i != nr; ++i) {\n        ret.cells.push_back(read_kv(in));\n    }\n\n    SCYLLA_ASSERT(in.empty());\n    return ret;\n}\n\ncollection_mutation_view_description\ndeserialize_collection_mutation(const abstract_type& type, collection_mutation_input_stream& in) {\n    return visit(type, make_visitor(\n    [&] (const collection_type_impl& ctype) {\n        // value_comparator(), ugh\n        return deserialize_collection_mutation(in, [&ctype] (collection_mutation_input_stream& in) {\n            // FIXME: we could probably avoid the need for size\n            auto ksize = in.read_trivial<uint32_t>();\n            auto key = in.read_linearized(ksize);\n            auto vsize = in.read_trivial<uint32_t>();\n            auto value = atomic_cell_view::from_bytes(*ctype.value_comparator(), in.read_fragmented(vsize));\n            return std::make_pair(key, value);\n        });\n    },\n    [&] (const user_type_impl& utype) {\n        return deserialize_collection_mutation(in, [&utype] (collection_mutation_input_stream& in) {\n            // FIXME: we could probably avoid the need for size\n            auto ksize = in.read_trivial<uint32_t>();\n            auto key = in.read_linearized(ksize);\n            auto vsize = in.read_trivial<uint32_t>();\n            auto value = atomic_cell_view::from_bytes(*utype.type(deserialize_field_index(key)), in.read_fragmented(vsize));\n            return std::make_pair(key, value);\n        });\n    },\n    [&] (const abstract_type& o) -> collection_mutation_view_description {\n        throw std::runtime_error(format(\"deserialize_collection_mutation: unknown type {}\", o.name()));\n    }\n    ));\n}\n"
        },
        {
          "name": "collection_mutation.hh",
          "type": "blob",
          "size": 5.970703125,
          "content": "/*\n * Copyright (C) 2019-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"utils/chunked_vector.hh\"\n#include \"schema/schema_fwd.hh\"\n#include \"gc_clock.hh\"\n#include \"mutation/atomic_cell.hh\"\n#include \"mutation/compact_and_expire_result.hh\"\n#include \"compaction/compaction_garbage_collector.hh\"\n#include <iosfwd>\n#include <forward_list>\n\nclass abstract_type;\nclass compaction_garbage_collector;\nclass row_tombstone;\n\nclass collection_mutation;\n\n// An auxiliary struct used to (de)construct collection_mutations.\n// Unlike collection_mutation which is a serialized blob, this struct allows to inspect logical units of information\n// (tombstone and cells) inside the mutation easily.\nstruct collection_mutation_description {\n    tombstone tomb;\n    // FIXME: use iterators?\n    // we never iterate over `cells` more than once, so there is no need to store them in memory.\n    // In some cases instead of constructing the `cells` vector, it would be more efficient to provide\n    // a one-time-use forward iterator which returns the cells.\n    utils::chunked_vector<std::pair<bytes, atomic_cell>> cells;\n\n    // Expires cells based on query_time. Expires tombstones based on max_purgeable and gc_before.\n    // Removes cells covered by tomb or this->tomb.\n    compact_and_expire_result compact_and_expire(column_id id, row_tombstone tomb, gc_clock::time_point query_time,\n        can_gc_fn&, gc_clock::time_point gc_before, compaction_garbage_collector* collector = nullptr);\n\n    // Packs the data to a serialized blob.\n    collection_mutation serialize(const abstract_type&) const;\n};\n\n// Similar to collection_mutation_description, except that it doesn't store the cells' data, only observes it.\nstruct collection_mutation_view_description {\n    tombstone tomb;\n    // FIXME: use iterators? See the fixme in collection_mutation_description; the same considerations apply here.\n    utils::chunked_vector<std::pair<bytes_view, atomic_cell_view>> cells;\n\n    // Copies the observed data, storing it in a collection_mutation_description.\n    collection_mutation_description materialize(const abstract_type&) const;\n\n    // Packs the data to a serialized blob.\n    collection_mutation serialize(const abstract_type&) const;\n};\n\nclass collection_mutation_input_stream {\n    std::forward_list<bytes> _linearized;\n    managed_bytes_view _src;\npublic:\n    collection_mutation_input_stream(const managed_bytes_view& src) : _src(src) {}\n    template <Trivial T>\n    T read_trivial() {\n        return ::read_simple<T>(_src);\n    }\n    bytes_view read_linearized(size_t n);\n    managed_bytes_view read_fragmented(size_t n);\n    bool empty() const;\n};\n\n// Given a collection_mutation_view, returns an auxiliary struct allowing the inspection of each cell.\n// The function needs to be given the type of stored data to reconstruct the structural information.\ncollection_mutation_view_description deserialize_collection_mutation(const abstract_type&, collection_mutation_input_stream&);\n\nclass collection_mutation_view {\npublic:\n    managed_bytes_view data;\n\n    // Is this a noop mutation?\n    bool is_empty() const;\n\n    // Is any of the stored cells live (not deleted nor expired) at the time point `tp`,\n    // given the later of the tombstones `t` and the one stored in the mutation (if any)?\n    // Requires a type to reconstruct the structural information.\n    bool is_any_live(const abstract_type&, tombstone t = tombstone(), gc_clock::time_point tp = gc_clock::time_point::min()) const;\n\n    // The maximum of timestamps of the mutation's cells and tombstone.\n    api::timestamp_type last_update(const abstract_type&) const;\n\n    // Given a function that operates on a collection_mutation_view_description,\n    // calls it on the corresponding description of `this`.\n    template <typename F>\n    inline decltype(auto) with_deserialized(const abstract_type& type, F f) const {\n        collection_mutation_input_stream stream(data);\n        return f(deserialize_collection_mutation(type, stream));\n    }\n\n    class printer {\n        const abstract_type& _type;\n        const collection_mutation_view& _cmv;\n    public:\n        printer(const abstract_type& type, const collection_mutation_view& cmv)\n                : _type(type), _cmv(cmv) {}\n        friend fmt::formatter<printer>;\n    };\n};\n\n// A serialized mutation of a collection of cells.\n// Used to represent mutations of collections (lists, maps, sets) or non-frozen user defined types.\n// It contains a sequence of cells, each representing a mutation of a single entry (element or field) of the collection.\n// Each cell has an associated 'key' (or 'path'). The meaning of each (key, cell) pair is:\n//  for sets: the key is the serialized set element, the cell contains no data (except liveness information),\n//  for maps: the key is the serialized map element's key, the cell contains the serialized map element's value,\n//  for lists: the key is a timeuuid identifying the list entry, the cell contains the serialized value,\n//  for user types: the key is an index identifying the field, the cell contains the value of the field.\n//  The mutation may also contain a collection-wide tombstone.\nclass collection_mutation {\npublic:\n    managed_bytes _data;\n\n    collection_mutation() {}\n    collection_mutation(const abstract_type&, collection_mutation_view);\n    collection_mutation(const abstract_type&, managed_bytes);\n    operator collection_mutation_view() const;\n};\n\ncollection_mutation merge(const abstract_type&, collection_mutation_view, collection_mutation_view);\n\ncollection_mutation difference(const abstract_type&, collection_mutation_view, collection_mutation_view);\n\n// Serializes the given collection of cells to a sequence of bytes ready to be sent over the CQL protocol.\nbytes_ostream serialize_for_cql(const abstract_type&, collection_mutation_view);\n\ntemplate <>\nstruct fmt::formatter<collection_mutation_view::printer> : fmt::formatter<string_view> {\n    auto format(const collection_mutation_view::printer&, fmt::format_context& ctx) const\n      -> decltype(ctx.out());\n};\n"
        },
        {
          "name": "column_computation.hh",
          "type": "blob",
          "size": 6.0390625,
          "content": "/*\n * Copyright (C) 2019-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"bytes.hh\"\n#include <memory>\n\nclass schema;\nclass partition_key;\nstruct atomic_cell_view;\nstruct tombstone;\n\nnamespace db::view {\nstruct clustering_or_static_row;\nstruct view_key_and_action;\n}\n\nclass column_computation;\nusing column_computation_ptr = std::unique_ptr<column_computation>;\n\n/*\n * Column computation represents a computation performed in order to obtain a value for a computed column.\n * Computed columns description is also available at docs/dev/system_schema_keyspace.md. They hold values\n * not provided directly by the user, but rather computed: from other column values and possibly other sources.\n * This class is able to serialize/deserialize column computations and perform the computation itself,\n * based on given schema, and partition key. Responsibility for providing enough data\n * in the clustering row in order for computation to succeed belongs to the caller. In particular,\n * generating a value might involve performing a read-before-write if the computation is performed\n * on more values than are present in the update request.\n */\nclass column_computation {\npublic:\n    virtual ~column_computation() = default;\n\n    static column_computation_ptr deserialize(bytes_view raw);\n\n    virtual column_computation_ptr clone() const = 0;\n\n    virtual bytes serialize() const = 0;\n    virtual bytes compute_value(const schema& schema, const partition_key& key) const = 0;\n    /*\n     * depends_on_non_primary_key_column for a column computation is needed to\n     * detect a case where the primary key of a materialized view depends on a\n     * non primary key column from the base table, but at the same time, the view\n     * itself doesn't have non-primary key columns. This is an issue, since as\n     * for now, it was assumed that no non-primary key columns in view schema\n     * meant that the update cannot change the primary key of the view, and\n     * therefore the update path can be simplified.\n     */\n    virtual bool depends_on_non_primary_key_column() const {\n        return false;\n    }\n};\n\n/*\n * Computes token value of partition key and returns it as bytes.\n *\n * Should NOT be used (use token_column_computation), because ordering\n * of bytes is different than ordering of tokens (signed vs unsigned comparison).\n *\n * The type name stored for computations of this class is \"token\" - this was\n * the original implementation. (now deprecated for new tables)\n */\nclass legacy_token_column_computation : public column_computation {\npublic:\n    virtual column_computation_ptr clone() const override {\n        return std::make_unique<legacy_token_column_computation>(*this);\n    }\n    virtual bytes serialize() const override;\n    virtual bytes compute_value(const schema& schema, const partition_key& key) const override;\n};\n\n\n/*\n * Computes token value of partition key and returns it as long_type.\n * The return type means that it can be trivially sorted (for example\n * if computed column using this computation is a clustering key),\n * preserving the correct order of tokens (using signed comparisons).\n *\n * Please use this class instead of legacy_token_column_computation.\n * \n * The type name stored for computations of this class is \"token_v2\".\n * (the name \"token\" refers to the deprecated legacy_token_column_computation)\n */\nclass token_column_computation : public column_computation {\npublic:\n    virtual column_computation_ptr clone() const override {\n        return std::make_unique<token_column_computation>(*this);\n    }\n    virtual bytes serialize() const override;\n    virtual bytes compute_value(const schema& schema, const partition_key& key) const override;\n};\n\n/*\n * collection_column_computation is used for a secondary index on a collection\n * column. In this case we don't have a single value to compute, but rather we\n * want to return multiple values (e.g., all the keys in the collection).\n * So this class does not implement the base class's compute_value() -\n * instead it implements a new method compute_collection_values(), which\n * can return multiple values. This new method is currently called only from\n * the materialized-view code which uses collection_column_computation.\n */\nclass collection_column_computation final : public column_computation {\n    enum class kind {\n        keys,\n        values,\n        entries,\n    };\n    const bytes _collection_name;\n    const kind _kind;\n    collection_column_computation(const bytes& collection_name, kind kind) : _collection_name(collection_name), _kind(kind) {}\n\n    using collection_kv = std::pair<bytes_view, atomic_cell_view>;\n    void operate_on_collection_entries(\n            std::invocable<collection_kv*, collection_kv*, tombstone> auto&& old_and_new_row_func, const schema& schema,\n            const partition_key& key, const db::view::clustering_or_static_row& update, const std::optional<db::view::clustering_or_static_row>& existing) const;\n\npublic:\n    static collection_column_computation for_keys(const bytes& collection_name) {\n        return {collection_name, kind::keys};\n    }\n    static collection_column_computation for_values(const bytes& collection_name) {\n        return {collection_name, kind::values};\n    }\n    static collection_column_computation for_entries(const bytes& collection_name) {\n        return {collection_name, kind::entries};\n    }\n    static column_computation_ptr for_target_type(std::string_view type, const bytes& collection_name);\n\n    virtual bytes serialize() const override;\n    virtual bytes compute_value(const schema& schema, const partition_key& key) const override;\n    virtual column_computation_ptr clone() const override {\n        return std::make_unique<collection_column_computation>(*this);\n    }\n    virtual bool depends_on_non_primary_key_column() const override {\n        return true;\n    }\n\n    std::vector<db::view::view_key_and_action> compute_values_with_action(const schema& schema, const partition_key& key,\n            const db::view::clustering_or_static_row& row, const std::optional<db::view::clustering_or_static_row>& existing) const;\n};\n"
        },
        {
          "name": "combine.hh",
          "type": "blob",
          "size": 1.1982421875,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <algorithm>\n\n// combine two sorted uniqued sequences into a single sorted sequence\n// unique elements are copied, duplicate elements are merged with a\n// binary function.\ntemplate <typename InputIterator1,\n          typename InputIterator2,\n          typename OutputIterator,\n          typename Compare,\n          typename Merge>\nOutputIterator\ncombine(InputIterator1 begin1, InputIterator1 end1,\n        InputIterator2 begin2, InputIterator2 end2,\n        OutputIterator out,\n        Compare compare,\n        Merge merge) {\n    while (begin1 != end1 && begin2 != end2) {\n        std::iter_const_reference_t<InputIterator1> e1 = *begin1;\n        std::iter_const_reference_t<InputIterator2> e2 = *begin2;\n        if (compare(e1, e2)) {\n            *out++ = e1;\n            ++begin1;\n        } else if (compare(e2, e1)) {\n            *out++ = e2;\n            ++begin2;\n        } else {\n            *out++ = merge(e1, e2);\n            ++begin1;\n            ++begin2;\n        }\n    }\n    out = std::copy(begin1, end1, out);\n    out = std::copy(begin2, end2, out);\n    return out;\n}\n\n\n"
        },
        {
          "name": "compaction",
          "type": "tree",
          "content": null
        },
        {
          "name": "compound.hh",
          "type": "blob",
          "size": 10.79296875,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"types/types.hh\"\n#include <algorithm>\n#include <vector>\n#include <span>\n#include <ranges>\n#include \"utils/assert.hh\"\n#include \"utils/serialization.hh\"\n#include <seastar/util/backtrace.hh>\n\nenum class allow_prefixes { no, yes };\n\ntemplate<allow_prefixes AllowPrefixes = allow_prefixes::no>\nclass compound_type final {\nprivate:\n    const std::vector<data_type> _types;\n    const bool _byte_order_equal;\n    const bool _byte_order_comparable;\n    const bool _is_reversed;\npublic:\n    static constexpr bool is_prefixable = AllowPrefixes == allow_prefixes::yes;\n    using prefix_type = compound_type<allow_prefixes::yes>;\n    using value_type = std::vector<bytes>;\n    using size_type = uint16_t;\n\n    compound_type(std::vector<data_type> types)\n        : _types(std::move(types))\n        , _byte_order_equal(std::all_of(_types.begin(), _types.end(), [] (const auto& t) {\n                return t->is_byte_order_equal();\n            }))\n        , _byte_order_comparable(false)\n        , _is_reversed(_types.size() == 1 && _types[0]->is_reversed())\n    { }\n\n    compound_type(compound_type&&) = default;\n\n    auto const& types() const {\n        return _types;\n    }\n\n    bool is_singular() const {\n        return _types.size() == 1;\n    }\n\n    prefix_type as_prefix() {\n        return prefix_type(_types);\n    }\nprivate:\n    /*\n     * Format:\n     *   <len(value1)><value1><len(value2)><value2>...<len(value_n)><value_n>\n     *\n     */\n    template<typename RangeOfSerializedComponents, FragmentedMutableView Out>\n    static void serialize_value(RangeOfSerializedComponents&& values, Out out) {\n        for (auto&& val : values) {\n            using val_type = std::remove_cvref_t<decltype(val)>;\n            if constexpr (FragmentedView<val_type>) {\n                SCYLLA_ASSERT(val.size_bytes() <= std::numeric_limits<size_type>::max());\n                write<size_type>(out, size_type(val.size_bytes()));\n                write_fragmented(out, val);\n            } else if constexpr (std::same_as<val_type, managed_bytes>) {\n                SCYLLA_ASSERT(val.size() <= std::numeric_limits<size_type>::max());\n                write<size_type>(out, size_type(val.size()));\n                write_fragmented(out, managed_bytes_view(val));\n            } else {\n                SCYLLA_ASSERT(val.size() <= std::numeric_limits<size_type>::max());\n                write<size_type>(out, size_type(val.size()));\n                write_fragmented(out, single_fragmented_view(val));\n            }\n        }\n    }\n    template <typename RangeOfSerializedComponents>\n    static size_t serialized_size(RangeOfSerializedComponents&& values) {\n        size_t len = 0;\n        for (auto&& val : values) {\n            using val_type = std::remove_cvref_t<decltype(val)>;\n            if constexpr (FragmentedView<val_type>) {\n                len += sizeof(size_type) + val.size_bytes();\n            } else {\n                len += sizeof(size_type) + val.size();\n            }\n        }\n        return len;\n    }\npublic:\n    managed_bytes serialize_single(const managed_bytes& v) const {\n        return serialize_value(std::ranges::subrange(&v, 1+&v));\n    }\n    managed_bytes serialize_single(const bytes& v) const {\n        return serialize_value(std::ranges::subrange(&v, 1+&v));\n    }\n    template<typename RangeOfSerializedComponents>\n    static managed_bytes serialize_value(RangeOfSerializedComponents&& values) {\n        auto size = serialized_size(values);\n        if (size > std::numeric_limits<size_type>::max()) {\n            throw std::runtime_error(format(\"Key size too large: {:d} > {:d}\", size, std::numeric_limits<size_type>::max()));\n        }\n        managed_bytes b(managed_bytes::initialized_later(), size);\n        serialize_value(values, managed_bytes_mutable_view(b));\n        return b;\n    }\n    template<typename T>\n    static managed_bytes serialize_value(std::initializer_list<T> values) {\n        return serialize_value(std::span(values));\n    }\n    managed_bytes serialize_optionals(std::span<const bytes_opt> values) const {\n        return serialize_value(values | std::views::transform([] (const bytes_opt& bo) -> bytes_view {\n            if (!bo) {\n                throw std::logic_error(\"attempted to create key component from empty optional\");\n            }\n            return *bo;\n        }));\n    }\n    managed_bytes serialize_optionals(std::span<const managed_bytes_opt> values) const {\n        return serialize_value(values | std::views::transform([] (const managed_bytes_opt& bo) -> managed_bytes_view {\n            if (!bo) {\n                throw std::logic_error(\"attempted to create key component from empty optional\");\n            }\n            return managed_bytes_view(*bo);\n        }));\n    }\n    managed_bytes serialize_value_deep(const std::vector<data_value>& values) const {\n        // TODO: Optimize\n        std::vector<bytes> partial;\n        partial.reserve(values.size());\n        auto i = _types.begin();\n        for (auto&& component : values) {\n            SCYLLA_ASSERT(i != _types.end());\n            partial.push_back((*i++)->decompose(component));\n        }\n        return serialize_value(partial);\n    }\n    managed_bytes decompose_value(const value_type& values) const {\n        return serialize_value(values);\n    }\n    class iterator {\n    public:\n        using iterator_category = std::forward_iterator_tag;\n        using iterator_concept = std::forward_iterator_tag;\n        using value_type = const managed_bytes_view;\n        using difference_type = std::ptrdiff_t;\n    private:\n        managed_bytes_view _v;\n        managed_bytes_view _current;\n        size_t _remaining = 0;\n    private:\n        void read_current() {\n            _remaining = _v.size_bytes();\n            size_type len;\n            {\n                if (_v.empty()) {\n                    return;\n                }\n                len = read_simple<size_type>(_v);\n                if (_v.size() < len) {\n                    throw_with_backtrace<marshal_exception>(format(\"compound_type iterator - not enough bytes, expected {:d}, got {:d}\", len, _v.size()));\n                }\n            }\n            _current = _v.prefix(len);\n            _v.remove_prefix(_current.size_bytes());\n        }\n    public:\n        struct end_iterator_tag {};\n        iterator(const managed_bytes_view& v) : _v(v) {\n            read_current();\n        }\n        iterator(end_iterator_tag, const managed_bytes_view& v) : _v() {}\n        iterator() {}\n        iterator& operator++() {\n            read_current();\n            return *this;\n        }\n        iterator operator++(int) {\n            iterator i(*this);\n            ++(*this);\n            return i;\n        }\n        value_type operator*() const { return _current; }\n        bool operator==(const iterator& i) const { return _remaining == i._remaining; }\n    };\n    static iterator begin(managed_bytes_view v) {\n        return iterator(v);\n    }\n    static iterator end(managed_bytes_view v) {\n        return iterator(typename iterator::end_iterator_tag(), v);\n    }\n    static std::ranges::subrange<iterator> components(managed_bytes_view v) {\n        return std::ranges::subrange(begin(v), end(v));\n    }\n    value_type deserialize_value(managed_bytes_view v) const {\n        std::vector<bytes> result;\n        result.reserve(_types.size());\n        std::transform(begin(v), end(v), std::back_inserter(result), [] (auto&& v) {\n            return to_bytes(v);\n        });\n        return result;\n    }\n    bool less(managed_bytes_view b1, managed_bytes_view b2) const {\n        return with_linearized(b1, [&] (bytes_view bv1) {\n            return with_linearized(b2, [&] (bytes_view bv2) {\n                return less(bv1, bv2);\n            });\n        });\n    }\n    bool less(bytes_view b1, bytes_view b2) const {\n        return compare(b1, b2) < 0;\n    }\n    size_t hash(managed_bytes_view v) const{\n        return with_linearized(v, [&] (bytes_view v) {\n            return hash(v);\n        });\n    }\n    size_t hash(bytes_view v) const {\n        if (_byte_order_equal) {\n            return std::hash<bytes_view>()(v);\n        }\n        auto t = _types.begin();\n        size_t h = 0;\n        for (auto&& value : components(v)) {\n            h ^= (*t)->hash(value);\n            ++t;\n        }\n        return h;\n    }\n    std::strong_ordering compare(managed_bytes_view b1, managed_bytes_view b2) const {\n        return with_linearized(b1, [&] (bytes_view bv1) {\n            return with_linearized(b2, [&] (bytes_view bv2) {\n                return compare(bv1, bv2);\n            });\n        });\n    }\n    std::strong_ordering compare(bytes_view b1, bytes_view b2) const {\n        if (_byte_order_comparable) {\n            if (_is_reversed) {\n                return compare_unsigned(b2, b1);\n            } else {\n                return compare_unsigned(b1, b2);\n            }\n        }\n        return lexicographical_tri_compare(_types.begin(), _types.end(),\n            begin(b1), end(b1), begin(b2), end(b2), [] (auto&& type, auto&& v1, auto&& v2) {\n                return type->compare(v1, v2);\n            });\n    }\n    // Returns true iff given prefix has no missing components\n    bool is_full(managed_bytes_view v) const {\n        SCYLLA_ASSERT(AllowPrefixes == allow_prefixes::yes);\n        return std::distance(begin(v), end(v)) == (ssize_t)_types.size();\n    }\n    bool is_empty(managed_bytes_view v) const {\n        return v.empty();\n    }\n    bool is_empty(const managed_bytes& v) const {\n        return v.empty();\n    }\n    bool is_empty(bytes_view v) const {\n        return begin(v) == end(v);\n    }\n    void validate(managed_bytes_view v) const {\n        std::vector<managed_bytes_view> values(begin(v), end(v));\n        if (AllowPrefixes == allow_prefixes::no && values.size() < _types.size()) {\n            throw marshal_exception(fmt::format(\"compound::validate(): non-prefixable compound cannot be a prefix\"));\n        }\n        if (values.size() > _types.size()) {\n            throw marshal_exception(fmt::format(\"compound::validate(): cannot have more values than types, have {} values but only {} types\",\n                        values.size(), _types.size()));\n        }\n        for (size_t i = 0; i != values.size(); ++i) {\n            //FIXME: is it safe to assume internal serialization-format format?\n            _types[i]->validate(values[i]);\n        }\n    }\n    bool equal(managed_bytes_view v1, managed_bytes_view v2) const {\n        return with_linearized(v1, [&] (bytes_view bv1) {\n            return with_linearized(v2, [&] (bytes_view bv2) {\n                return equal(bv1, bv2);\n            });\n        });\n    }\n    bool equal(bytes_view v1, bytes_view v2) const {\n        if (_byte_order_equal) {\n            return compare_unsigned(v1, v2) == 0;\n        }\n        // FIXME: call equal() on each component\n        return compare(v1, v2) == 0;\n    }\n};\n\nusing compound_prefix = compound_type<allow_prefixes::yes>;\n"
        },
        {
          "name": "compound_compat.hh",
          "type": "blob",
          "size": 23.408203125,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <ranges>\n#include <compare>\n#include \"compound.hh\"\n#include \"schema/schema.hh\"\n#include \"sstables/version.hh\"\n#include \"utils/log.hh\"\n\n//FIXME: de-inline methods and define this as static in a .cc file.\nextern logging::logger compound_logger;\n\n//\n// This header provides adaptors between the representation used by our compound_type<>\n// and representation used by Origin.\n//\n// For single-component keys the legacy representation is equivalent\n// to the only component's serialized form. For composite keys it the following\n// (See org.apache.cassandra.db.marshal.CompositeType):\n//\n//   <representation> ::= ( <component> )+\n//   <component>      ::= <length> <value> <EOC>\n//   <length>         ::= <uint16_t>\n//   <EOC>            ::= <uint8_t>\n//\n//  <value> is component's value in serialized form. <EOC> is always 0 for partition key.\n//\n\n// Given a representation serialized using @CompoundType, provides a view on the\n// representation of the same components as they would be serialized by Origin.\n//\n// The view is exposed in a form of a byte range. For example of use see to_legacy() function.\ntemplate <typename CompoundType>\nclass legacy_compound_view {\n    static_assert(!CompoundType::is_prefixable, \"Legacy view not defined for prefixes\");\n    CompoundType& _type;\n    managed_bytes_view _packed;\npublic:\n    legacy_compound_view(CompoundType& c, managed_bytes_view packed)\n        : _type(c)\n        , _packed(packed)\n    { }\n\n    class iterator {\n    public:\n        using iterator_category = std::input_iterator_tag;\n        using value_type = bytes::value_type;\n        using difference_type = std::ptrdiff_t;\n        using pointer = bytes::value_type*;\n        using reference = bytes::value_type&;\n    private:\n        bool _singular;\n        // Offset within virtual output space of a component.\n        //\n        // Offset: -2             -1             0  ...  LEN-1 LEN\n        // Field:  [ length MSB ] [ length LSB ] [   VALUE   ] [ EOC ]\n        //\n        int32_t _offset;\n        typename CompoundType::iterator _i;\n    public:\n        struct end_tag {};\n\n        iterator(const legacy_compound_view& v)\n            : _singular(v._type.is_singular())\n            , _offset(_singular ? 0 : -2)\n            , _i(_singular && !(*v._type.begin(v._packed)).size() ?\n                    v._type.end(v._packed) : v._type.begin(v._packed))\n        { }\n\n        iterator(const legacy_compound_view& v, end_tag)\n            : _offset(v._type.is_singular() && !(*v._type.begin(v._packed)).size() ? 0 : -2)\n            , _i(v._type.end(v._packed))\n        { }\n\n        // Default constructor is incorrectly needed for c++20\n        // weakly_incrementable concept requires for ranges.\n        // Will be fixed by https://wg21.link/P2325R3 but still\n        // needed for now.\n        iterator() {}\n\n        value_type operator*() const {\n            auto tmp = *_i;\n            int32_t component_size = tmp.size();\n            if (_offset == -2) {\n                return (component_size >> 8) & 0xff;\n            } else if (_offset == -1) {\n                return component_size & 0xff;\n            } else if (_offset < component_size) {\n                return tmp[_offset];\n            } else { // _offset == component_size\n                return 0; // EOC field\n            }\n        }\n\n        iterator& operator++() {\n            auto tmp = *_i;\n            auto component_size = (int32_t) tmp.size();\n            if (_offset < component_size\n                // When _singular, we skip the EOC byte.\n                && (!_singular || _offset != (component_size - 1)))\n            {\n                ++_offset;\n            } else {\n                ++_i;\n                _offset = -2;\n            }\n            return *this;\n        }\n\n        iterator operator++(int) {\n            iterator i(*this);\n            ++(*this);\n            return i;\n        }\n\n        bool operator==(const iterator& other) const {\n            return _offset == other._offset && other._i == _i;\n        }\n    };\n\n    // A trichotomic comparator defined on @CompoundType representations which\n    // orders them according to lexicographical ordering of their corresponding\n    // legacy representations.\n    //\n    //   tri_comparator(t)(k1, k2)\n    //\n    // ...is equivalent to:\n    //\n    //   compare_unsigned(to_legacy(t, k1), to_legacy(t, k2))\n    //\n    // ...but more efficient.\n    //\n    struct tri_comparator {\n        const CompoundType& _type;\n\n        tri_comparator(const CompoundType& type)\n            : _type(type)\n        { }\n\n        // @k1 and @k2 must be serialized using @type, which was passed to the constructor.\n        std::strong_ordering operator()(managed_bytes_view k1, managed_bytes_view k2) const {\n            if (_type.is_singular()) {\n                return compare_unsigned(*_type.begin(k1), *_type.begin(k2));\n            }\n            return std::lexicographical_compare_three_way(\n                _type.begin(k1), _type.end(k1),\n                _type.begin(k2), _type.end(k2),\n                [] (const managed_bytes_view& c1, const managed_bytes_view& c2) -> std::strong_ordering {\n                    if (c1.size() != c2.size() || !c1.size()) {\n                        return c1.size() < c2.size() ? std::strong_ordering::less : c1.size() ? std::strong_ordering::greater : std::strong_ordering::equal;\n                    }\n                    return compare_unsigned(c1, c2);\n                });\n        }\n    };\n\n    // Equivalent to std::distance(begin(), end()), but computes faster\n    size_t size() const {\n        if (_type.is_singular()) {\n            return (*_type.begin(_packed)).size();\n        }\n        size_t s = 0;\n        for (auto&& component : _type.components(_packed)) {\n            s += 2 /* length field */ + component.size() + 1 /* EOC */;\n        }\n        return s;\n    }\n\n    iterator begin() const {\n        return iterator(*this);\n    }\n\n    iterator end() const {\n        return iterator(*this, typename iterator::end_tag());\n    }\n};\n\n// Converts compound_type<> representation to legacy representation\n// @packed is assumed to be serialized using supplied @type.\ntemplate <typename CompoundType>\ninline\nbytes to_legacy(CompoundType& type, managed_bytes_view packed) {\n    legacy_compound_view<CompoundType> lv(type, packed);\n    bytes legacy_form(bytes::initialized_later(), lv.size());\n    std::copy(lv.begin(), lv.end(), legacy_form.begin());\n    return legacy_form;\n}\n\nclass composite_view;\n\n// Represents a value serialized according to Origin's CompositeType.\n// If is_compound is true, then the value is one or more components encoded as:\n//\n//   <representation> ::= ( <component> )+\n//   <component>      ::= <length> <value> <EOC>\n//   <length>         ::= <uint16_t>\n//   <EOC>            ::= <uint8_t>\n//\n// If false, then it encodes a single value, without a prefix length or a suffix EOC.\nclass composite final {\n    bytes _bytes;\n    bool _is_compound;\npublic:\n    composite(bytes&& b, bool is_compound)\n            : _bytes(std::move(b))\n            , _is_compound(is_compound)\n    { }\n\n    explicit composite(bytes&& b)\n            : _bytes(std::move(b))\n            , _is_compound(true)\n    { }\n\n    explicit composite(const composite_view& v);\n\n    composite()\n            : _bytes()\n            , _is_compound(true)\n    { }\n\n    using size_type = uint16_t;\n    using eoc_type = int8_t;\n\n    /*\n     * The 'end-of-component' byte should always be 0 for actual column name.\n     * However, it can set to 1 for query bounds. This allows to query for the\n     * equivalent of 'give me the full range'. That is, if a slice query is:\n     *   start = <3><\"foo\".getBytes()><0>\n     *   end   = <3><\"foo\".getBytes()><1>\n     * then we'll return *all* the columns whose first component is \"foo\".\n     * If for a component, the 'end-of-component' is != 0, there should not be any\n     * following component. The end-of-component can also be -1 to allow\n     * non-inclusive query. For instance:\n     *   end = <3><\"foo\".getBytes()><-1>\n     * allows to query everything that is smaller than <3><\"foo\".getBytes()>, but\n     * not <3><\"foo\".getBytes()> itself.\n     */\n    enum class eoc : eoc_type {\n        start = -1,\n        none = 0,\n        end = 1\n    };\n\n    using component = std::pair<bytes, eoc>;\n    using component_view = std::pair<bytes_view, eoc>;\nprivate:\n    template<typename Value>\n    requires (!std::same_as<const data_value, std::decay_t<Value>>)\n    static size_t size(const Value& val) {\n        return val.size();\n    }\n    static size_t size(const data_value& val) {\n        return val.serialized_size();\n    }\n    template<typename Value, typename CharOutputIterator>\n    requires (!std::same_as<const data_value, std::decay_t<Value>>)\n    static void write_value(Value&& val, CharOutputIterator& out) {\n        out = std::copy(val.begin(), val.end(), out);\n    }\n    template<typename CharOutputIterator>\n    static void write_value(managed_bytes_view val, CharOutputIterator& out) {\n        for (bytes_view frag : fragment_range(val)) {\n            out = std::copy(frag.begin(), frag.end(), out);\n        }\n    }\n    template <typename CharOutputIterator>\n    static void write_value(const data_value& val, CharOutputIterator& out) {\n        val.serialize(out);\n    }\n    template<typename RangeOfSerializedComponents, typename CharOutputIterator>\n    static void serialize_value(RangeOfSerializedComponents&& values, CharOutputIterator& out, bool is_compound) {\n        if (!is_compound) {\n            auto it = values.begin();\n            write_value(std::forward<decltype(*it)>(*it), out);\n            return;\n        }\n\n        for (auto&& val : values) {\n            write<size_type>(out, static_cast<size_type>(size(val)));\n            write_value(std::forward<decltype(val)>(val), out);\n            // Range tombstones are not keys. For collections, only frozen\n            // values can be keys. Therefore, for as long as it is safe to\n            // assume that this code will be used to create keys, it is safe\n            // to assume the trailing byte is always zero.\n            write<eoc_type>(out, eoc_type(eoc::none));\n        }\n    }\n    template <typename RangeOfSerializedComponents>\n    static size_t serialized_size(RangeOfSerializedComponents&& values, bool is_compound) {\n        size_t len = 0;\n        auto it = values.begin();\n        if (it != values.end()) {\n            // CQL3 uses a specific prefix (0xFFFF) to encode \"static columns\"\n            // (CASSANDRA-6561). This does mean the maximum size of the first component of a\n            // composite is 65534, not 65535 (or we wouldn't be able to detect if the first 2\n            // bytes is the static prefix or not).\n            auto value_size = size(*it);\n            if (value_size > static_cast<size_type>(std::numeric_limits<size_type>::max() - uint8_t(is_compound))) {\n                throw std::runtime_error(format(\"First component size too large: {:d} > {:d}\", value_size, std::numeric_limits<size_type>::max() - is_compound));\n            }\n            if (!is_compound) {\n                return value_size;\n            }\n            len += sizeof(size_type) + value_size + sizeof(eoc_type);\n            ++it;\n        }\n        for ( ; it != values.end(); ++it) {\n            auto value_size = size(*it);\n            if (value_size > std::numeric_limits<size_type>::max()) {\n                throw std::runtime_error(format(\"Component size too large: {:d} > {:d}\", value_size, std::numeric_limits<size_type>::max()));\n            }\n            len += sizeof(size_type) + value_size + sizeof(eoc_type);\n        }\n        return len;\n    }\npublic:\n    template <typename Describer>\n    auto describe_type(sstables::sstable_version_types v, Describer f) const {\n        return f(const_cast<bytes&>(_bytes));\n    }\n\n    // marker is ignored if !is_compound\n    template<typename RangeOfSerializedComponents>\n    static composite serialize_value(RangeOfSerializedComponents&& values, bool is_compound = true, eoc marker = eoc::none) {\n        auto size = serialized_size(values, is_compound);\n        bytes b(bytes::initialized_later(), size);\n        auto i = b.begin();\n        serialize_value(std::forward<decltype(values)>(values), i, is_compound);\n        if (is_compound && !b.empty()) {\n            b.back() = eoc_type(marker);\n        }\n        return composite(std::move(b), is_compound);\n    }\n\n    template<typename RangeOfSerializedComponents>\n    static composite serialize_static(const schema& s, RangeOfSerializedComponents&& values) {\n        // FIXME: Optimize\n        auto b = bytes(size_t(2), bytes::value_type(0xff));\n        std::vector<bytes_view> sv(s.clustering_key_size() + std::ranges::distance(values));\n        std::ranges::copy(values, sv.begin() + s.clustering_key_size());\n        b += composite::serialize_value(sv, true).release_bytes();\n        return composite(std::move(b));\n    }\n\n    static eoc to_eoc(int8_t eoc_byte) {\n        return eoc_byte == 0 ? eoc::none : (eoc_byte < 0 ? eoc::start : eoc::end);\n    }\n\n    class iterator {\n    public:\n        using iterator_category = std::input_iterator_tag;\n        using value_type = const component_view;\n        using difference_type = std::ptrdiff_t;\n        using pointer = const component_view*;\n        using reference = const component_view&;\n    private:\n        bytes_view _v;\n        component_view _current;\n        bool _strict_mode = true;\n    private:\n        void do_read_current() {\n            size_type len;\n            {\n                if (_v.empty()) {\n                    _v = bytes_view(nullptr, 0);\n                    return;\n                }\n                len = read_simple<size_type>(_v);\n                if (_v.size() < len) {\n                    throw_with_backtrace<marshal_exception>(format(\"composite iterator - not enough bytes, expected {:d}, got {:d}\", len, _v.size()));\n                }\n            }\n            auto value = bytes_view(_v.begin(), len);\n            _v.remove_prefix(len);\n            _current = component_view(std::move(value), to_eoc(read_simple<eoc_type>(_v)));\n        }\n        void read_current() {\n            try {\n                do_read_current();\n            } catch (marshal_exception&) {\n                if (_strict_mode) {\n                    on_internal_error(compound_logger, std::current_exception());\n                } else {\n                    throw;\n                }\n            }\n        }\n\n        struct end_iterator_tag {};\n\n        // In strict-mode de-serialization errors will invoke `on_internal_error()`.\n        iterator(const bytes_view& v, bool is_compound, bool is_static, bool strict_mode = true)\n                : _v(v), _strict_mode(strict_mode) {\n            if (is_static) {\n                _v.remove_prefix(2);\n            }\n            if (is_compound) {\n                read_current();\n            } else {\n                _current = component_view(_v, eoc::none);\n                _v.remove_prefix(_v.size());\n            }\n        }\n\n        iterator(end_iterator_tag) : _v(nullptr, 0) {}\n\n    public:\n        iterator() : iterator(end_iterator_tag()) {}\n        iterator& operator++() {\n            read_current();\n            return *this;\n        }\n\n        iterator operator++(int) {\n            iterator i(*this);\n            ++(*this);\n            return i;\n        }\n\n        const value_type& operator*() const { return _current; }\n        const value_type* operator->() const { return &_current; }\n        bool operator==(const iterator& i) const { return _v.begin() == i._v.begin(); }\n\n        friend class composite;\n        friend class composite_view;\n    };\n\n    iterator begin() const {\n        return iterator(_bytes, _is_compound, is_static());\n    }\n\n    iterator end() const {\n        return iterator(iterator::end_iterator_tag());\n    }\n\n    std::ranges::subrange<iterator> components() const & {\n        return { begin(), end() };\n    }\n\n    auto values() const & {\n        return components() | std::views::transform(&component_view::first);\n    }\n\n    std::vector<component> components() const && {\n        std::vector<component> result;\n        std::transform(begin(), end(), std::back_inserter(result), [](auto&& p) {\n            return component(bytes(p.first.begin(), p.first.end()), p.second);\n        });\n        return result;\n    }\n\n    std::vector<bytes> values() const && {\n        std::vector<bytes> result;\n        std::ranges::copy(components() | std::views::transform([](auto&& c) { return to_bytes(c.first); }), std::back_inserter(result));\n        return result;\n    }\n\n    const bytes& get_bytes() const {\n        return _bytes;\n    }\n\n    bytes release_bytes() && {\n        return std::move(_bytes);\n    }\n\n    size_t size() const {\n        return _bytes.size();\n    }\n\n    bool empty() const {\n        return _bytes.empty();\n    }\n\n    static bool is_static(bytes_view bytes, bool is_compound) {\n        return is_compound && bytes.size() > 2 && (bytes[0] & bytes[1] & 0xff) == 0xff;\n    }\n\n    bool is_static() const {\n        return is_static(_bytes, _is_compound);\n    }\n\n    bool is_compound() const {\n        return _is_compound;\n    }\n\n    template <typename ClusteringElement>\n    static composite from_clustering_element(const schema& s, const ClusteringElement& ce) {\n        return serialize_value(ce.components(s), s.is_compound());\n    }\n\n    static composite from_exploded(const std::vector<bytes_view>& v, bool is_compound, eoc marker = eoc::none) {\n        if (v.size() == 0) {\n            return composite(bytes(size_t(1), bytes::value_type(marker)), is_compound);\n        }\n        return serialize_value(v, is_compound, marker);\n    }\n\n    static composite static_prefix(const schema& s) {\n        return serialize_static(s, std::vector<bytes_view>());\n    }\n\n    explicit operator bytes_view() const {\n        return _bytes;\n    }\n\n    template <typename Component>\n    friend inline std::ostream& operator<<(std::ostream& os, const std::pair<Component, eoc>& c) {\n        fmt::print(os, \"{}\", c);\n        return os;\n    }\n\n    struct tri_compare {\n        const std::vector<data_type>& _types;\n        tri_compare(const std::vector<data_type>& types) : _types(types) {}\n        std::strong_ordering operator()(const composite&, const composite&) const;\n        std::strong_ordering operator()(composite_view, composite_view) const;\n    };\n};\n\ntemplate <typename Component>\nstruct fmt::formatter<std::pair<Component, composite::eoc>> : fmt::formatter<string_view> {\n    template <typename FormatContext>\n    auto format(const std::pair<Component, composite::eoc>& c, FormatContext& ctx) const {\n        if constexpr (std::same_as<Component, bytes_view>) {\n            return fmt::format_to(ctx.out(), \"{{value={}; eoc={:#02x}}}\",\n                                  fmt_hex(c.first), composite::eoc_type(c.second) & 0xff);\n        } else {\n            return fmt::format_to(ctx.out(), \"{{value={}; eoc={:#02x}}}\",\n                                  c.first, composite::eoc_type(c.second) & 0xff);\n        }\n    }\n};\n\nclass composite_view final {\n    friend class composite;\n    bytes_view _bytes;\n    bool _is_compound;\npublic:\n    composite_view(bytes_view b, bool is_compound = true)\n            : _bytes(b)\n            , _is_compound(is_compound)\n    { }\n\n    composite_view(const composite& c)\n            : composite_view(static_cast<bytes_view>(c), c.is_compound())\n    { }\n\n    composite_view()\n            : _bytes(nullptr, 0)\n            , _is_compound(true)\n    { }\n\n    std::vector<bytes_view> explode() const {\n        if (!_is_compound) {\n            return { _bytes };\n        }\n\n        std::vector<bytes_view> ret;\n        ret.reserve(8);\n        for (auto it = begin(), e = end(); it != e; ) {\n            ret.push_back(it->first);\n            auto marker = it->second;\n            ++it;\n            if (it != e && marker != composite::eoc::none) {\n                throw runtime_exception(format(\"non-zero component divider found ({:#02x}) mid\", composite::eoc_type(marker) & 0xff));\n            }\n        }\n        return ret;\n    }\n\n    composite::iterator begin() const {\n        return composite::iterator(_bytes, _is_compound, is_static());\n    }\n\n    composite::iterator end() const {\n        return composite::iterator(composite::iterator::end_iterator_tag());\n    }\n\n    std::ranges::subrange<composite::iterator> components() const {\n        return { begin(), end() };\n    }\n\n    composite::eoc last_eoc() const {\n        if (!_is_compound || _bytes.empty()) {\n            return composite::eoc::none;\n        }\n        bytes_view v(_bytes);\n        v.remove_prefix(v.size() - 1);\n        return composite::to_eoc(read_simple<composite::eoc_type>(v));\n    }\n\n    auto values() const {\n        return components() | std::views::transform(&composite::component_view::first);\n    }\n\n    size_t size() const {\n        return _bytes.size();\n    }\n\n    bool empty() const {\n        return _bytes.empty();\n    }\n\n    bool is_static() const {\n        return composite::is_static(_bytes, _is_compound);\n    }\n\n    bool is_valid() const {\n        try {\n            auto it = composite::iterator(_bytes, _is_compound, is_static(), false);\n            const auto end = composite::iterator(composite::iterator::end_iterator_tag());\n            size_t s = 0;\n            for (; it != end; ++it) {\n                auto& c = *it;\n                s += c.first.size() + sizeof(composite::size_type) + sizeof(composite::eoc_type);\n            }\n            return s == _bytes.size();\n        } catch (marshal_exception&) {\n            return false;\n        }\n    }\n\n    explicit operator bytes_view() const {\n        return _bytes;\n    }\n\n    bool operator==(const composite_view& k) const { return k._bytes == _bytes && k._is_compound == _is_compound; }\n\n    friend fmt::formatter<composite_view>;\n};\n\ntemplate <>\nstruct fmt::formatter<composite_view> : fmt::formatter<string_view> {\n    template <typename FormatContext>\n    auto format(const composite_view& v, FormatContext& ctx) const {\n        return fmt::format_to(ctx.out(), \"{{{}, compound={}, static={}}}\",\n                              fmt::join(v.components(), \", \"), v._is_compound, v.is_static());\n    }\n};\n\ninline\ncomposite::composite(const composite_view& v)\n    : composite(bytes(v._bytes), v._is_compound)\n{ }\n\ntemplate <>\nstruct fmt::formatter<composite> : fmt::formatter<string_view> {\n    template <typename FormatContext>\n    auto format(const composite& v, FormatContext& ctx) const {\n        return fmt::format_to(ctx.out(), \"{}\", composite_view(v));\n    }\n};\n\ninline\nstd::strong_ordering composite::tri_compare::operator()(const composite& v1, const composite& v2) const {\n    return (*this)(composite_view(v1), composite_view(v2));\n}\n\ninline\nstd::strong_ordering composite::tri_compare::operator()(composite_view v1, composite_view v2) const {\n    // See org.apache.cassandra.db.composites.AbstractCType#compare\n    if (v1.empty()) {\n        return v2.empty() ? std::strong_ordering::equal : std::strong_ordering::less;\n    }\n    if (v2.empty()) {\n        return std::strong_ordering::greater;\n    }\n    if (v1.is_static() != v2.is_static()) {\n        return v1.is_static() ? std::strong_ordering::less : std::strong_ordering::greater;\n    }\n    auto a_values = v1.components();\n    auto b_values = v2.components();\n    auto cmp = [&](const data_type& t, component_view c1, component_view c2) {\n        // First by value, then by EOC\n        auto r = t->compare(c1.first, c2.first);\n        if (r != 0) {\n            return r;\n        }\n        return (static_cast<int>(c1.second) - static_cast<int>(c2.second)) <=> 0;\n    };\n    return lexicographical_tri_compare(_types.begin(), _types.end(),\n        a_values.begin(), a_values.end(),\n        b_values.begin(), b_values.end(),\n        cmp);\n}\n"
        },
        {
          "name": "compress.cc",
          "type": "blob",
          "size": 12.1142578125,
          "content": "/*\n * Copyright (C) 2016-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include <lz4.h>\n#include <zlib.h>\n#include <snappy-c.h>\n\n#include \"compress.hh\"\n#include \"exceptions/exceptions.hh\"\n#include \"utils/class_registrator.hh\"\n\nconst sstring compressor::namespace_prefix = \"org.apache.cassandra.io.compress.\";\n\nclass lz4_processor: public compressor {\npublic:\n    using compressor::compressor;\n\n    size_t uncompress(const char* input, size_t input_len, char* output,\n                    size_t output_len) const override;\n    size_t compress(const char* input, size_t input_len, char* output,\n                    size_t output_len) const override;\n    size_t compress_max_size(size_t input_len) const override;\n};\n\nclass snappy_processor: public compressor {\npublic:\n    using compressor::compressor;\n\n    size_t uncompress(const char* input, size_t input_len, char* output,\n                    size_t output_len) const override;\n    size_t compress(const char* input, size_t input_len, char* output,\n                    size_t output_len) const override;\n    size_t compress_max_size(size_t input_len) const override;\n};\n\nclass deflate_processor: public compressor {\npublic:\n    using compressor::compressor;\n\n    size_t uncompress(const char* input, size_t input_len, char* output,\n                    size_t output_len) const override;\n    size_t compress(const char* input, size_t input_len, char* output,\n                    size_t output_len) const override;\n    size_t compress_max_size(size_t input_len) const override;\n};\n\ncompressor::compressor(sstring name)\n    : _name(std::move(name))\n{}\n\nstd::set<sstring> compressor::option_names() const {\n    return {};\n}\n\nstd::map<sstring, sstring> compressor::options() const {\n    return {};\n}\n\ncompressor::ptr_type compressor::create(const sstring& name, const opt_getter& opts) {\n    if (name.empty()) {\n        return {};\n    }\n\n    qualified_name qn(namespace_prefix, name);\n\n    for (auto& c : { lz4, snappy, deflate }) {\n        if (c->name() == static_cast<const sstring&>(qn)) {\n            return c;\n        }\n    }\n\n    return compressor_registry::create(qn, opts);\n}\n\nshared_ptr<compressor> compressor::create(const std::map<sstring, sstring>& options) {\n    auto i = options.find(compression_parameters::SSTABLE_COMPRESSION);\n    if (i != options.end() && !i->second.empty()) {\n        return create(i->second, [&options](const sstring& key) -> opt_string {\n            auto i = options.find(key);\n            if (i == options.end()) {\n                return std::nullopt;\n            }\n            return { i->second };\n        });\n    }\n    return {};\n}\n\nthread_local const shared_ptr<compressor> compressor::lz4 = ::make_shared<lz4_processor>(namespace_prefix + \"LZ4Compressor\");\nthread_local const shared_ptr<compressor> compressor::snappy = ::make_shared<snappy_processor>(namespace_prefix + \"SnappyCompressor\");\nthread_local const shared_ptr<compressor> compressor::deflate = ::make_shared<deflate_processor>(namespace_prefix + \"DeflateCompressor\");\n\nconst sstring compression_parameters::SSTABLE_COMPRESSION = \"sstable_compression\";\nconst sstring compression_parameters::CHUNK_LENGTH_KB = \"chunk_length_in_kb\";\nconst sstring compression_parameters::CHUNK_LENGTH_KB_ERR = \"chunk_length_kb\";\nconst sstring compression_parameters::CRC_CHECK_CHANCE = \"crc_check_chance\";\n\ncompression_parameters::compression_parameters()\n    : compression_parameters(compressor::lz4)\n{}\n\ncompression_parameters::~compression_parameters()\n{}\n\ncompression_parameters::compression_parameters(compressor_ptr c)\n    : _compressor(std::move(c))\n{}\n\ncompression_parameters::compression_parameters(const std::map<sstring, sstring>& options) {\n    _compressor = compressor::create(options);\n\n    validate_options(options);\n\n    auto chunk_length = options.find(CHUNK_LENGTH_KB) != options.end() ?\n        options.find(CHUNK_LENGTH_KB) : options.find(CHUNK_LENGTH_KB_ERR);\n\n    if (chunk_length != options.end()) {\n        try {\n            _chunk_length = std::stoi(chunk_length->second) * 1024;\n        } catch (const std::exception& e) {\n            throw exceptions::syntax_exception(sstring(\"Invalid integer value \") + chunk_length->second + \" for \" + chunk_length->first);\n        }\n    }\n    auto crc_chance = options.find(CRC_CHECK_CHANCE);\n    if (crc_chance != options.end()) {\n        try {\n            _crc_check_chance = std::stod(crc_chance->second);\n        } catch (const std::exception& e) {\n            throw exceptions::syntax_exception(sstring(\"Invalid double value \") + crc_chance->second + \"for \" + CRC_CHECK_CHANCE);\n        }\n    }\n}\n\nvoid compression_parameters::validate() {\n    if (_chunk_length) {\n        auto chunk_length = _chunk_length.value();\n        if (chunk_length <= 0) {\n            throw exceptions::configuration_exception(\n                fmt::format(\"Invalid negative or null for {}/{}\", CHUNK_LENGTH_KB, CHUNK_LENGTH_KB_ERR));\n        }\n        // _chunk_length must be a power of two\n        if (chunk_length & (chunk_length - 1)) {\n            throw exceptions::configuration_exception(\n                fmt::format(\"{}/{} must be a power of 2.\", CHUNK_LENGTH_KB, CHUNK_LENGTH_KB_ERR));\n        }\n        // Excessive _chunk_length is pointless and can lead to allocation\n        // failures (see issue #9933)\n        if (chunk_length > 128 * 1024) {\n            throw exceptions::configuration_exception(\n                fmt::format(\"{}/{} must be 128 or less.\", CHUNK_LENGTH_KB, CHUNK_LENGTH_KB_ERR));\n        }\n    }\n    if (_crc_check_chance && (_crc_check_chance.value() < 0.0 || _crc_check_chance.value() > 1.0)) {\n        throw exceptions::configuration_exception(sstring(CRC_CHECK_CHANCE) + \" must be between 0.0 and 1.0.\");\n    }\n}\n\nstd::map<sstring, sstring> compression_parameters::get_options() const {\n    if (!_compressor) {\n        return std::map<sstring, sstring>();\n    }\n    auto opts = _compressor->options();\n\n    opts.emplace(compression_parameters::SSTABLE_COMPRESSION, _compressor->name());\n    if (_chunk_length) {\n        opts.emplace(sstring(CHUNK_LENGTH_KB), std::to_string(_chunk_length.value() / 1024));\n    }\n    if (_crc_check_chance) {\n        opts.emplace(sstring(CRC_CHECK_CHANCE), std::to_string(_crc_check_chance.value()));\n    }\n    return opts;\n}\n\nbool compression_parameters::operator==(const compression_parameters& other) const {\n    return _compressor == other._compressor\n           && _chunk_length == other._chunk_length\n           && _crc_check_chance == other._crc_check_chance;\n}\n\nvoid compression_parameters::validate_options(const std::map<sstring, sstring>& options) {\n    // currently, there are no options specific to a particular compressor\n    static std::set<sstring> keywords({\n        sstring(SSTABLE_COMPRESSION),\n        sstring(CHUNK_LENGTH_KB),\n        sstring(CHUNK_LENGTH_KB_ERR),\n        sstring(CRC_CHECK_CHANCE),\n    });\n    std::set<sstring> ckw;\n    if (_compressor) {\n        ckw = _compressor->option_names();\n    }\n    for (auto&& opt : options) {\n        if (!keywords.contains(opt.first) && !ckw.contains(opt.first)) {\n            throw exceptions::configuration_exception(format(\"Unknown compression option '{}'.\", opt.first));\n        }\n    }\n}\n\nsize_t lz4_processor::uncompress(const char* input, size_t input_len,\n                char* output, size_t output_len) const {\n    // We use LZ4_decompress_safe(). According to the documentation, the\n    // function LZ4_decompress_fast() is slightly faster, but maliciously\n    // crafted compressed data can cause it to overflow the output buffer.\n    // Theoretically, our compressed data is created by us so is not malicious\n    // (and accidental corruption is avoided by the compressed-data checksum),\n    // but let's not take that chance for now, until we've actually measured\n    // the performance benefit that LZ4_decompress_fast() would bring.\n\n    // Cassandra's LZ4Compressor prepends to the chunk its uncompressed length\n    // in 4 bytes little-endian (!) order. We don't need this information -\n    // we already know the uncompressed data is at most the given chunk size\n    // (and usually is exactly that, except in the last chunk). The advance\n    // knowledge of the uncompressed size could be useful if we used\n    // LZ4_decompress_fast(), but we prefer LZ4_decompress_safe() anyway...\n    input += 4;\n    input_len -= 4;\n\n    auto ret = LZ4_decompress_safe(input, output, input_len, output_len);\n    if (ret < 0) {\n        throw std::runtime_error(\"LZ4 uncompression failure\");\n    }\n    return ret;\n}\n\nsize_t lz4_processor::compress(const char* input, size_t input_len,\n                char* output, size_t output_len) const {\n    if (output_len < LZ4_COMPRESSBOUND(input_len) + 4) {\n        throw std::runtime_error(\"LZ4 compression failure: length of output is too small\");\n    }\n    // Write input_len (32-bit data) to beginning of output in little-endian representation.\n    output[0] = input_len & 0xFF;\n    output[1] = (input_len >> 8) & 0xFF;\n    output[2] = (input_len >> 16) & 0xFF;\n    output[3] = (input_len >> 24) & 0xFF;\n    auto ret = LZ4_compress_default(input, output + 4, input_len, LZ4_compressBound(input_len));\n    if (ret == 0) {\n        throw std::runtime_error(\"LZ4 compression failure: LZ4_compress() failed\");\n    }\n    return ret + 4;\n}\n\nsize_t lz4_processor::compress_max_size(size_t input_len) const {\n    return LZ4_COMPRESSBOUND(input_len) + 4;\n}\n\nsize_t deflate_processor::uncompress(const char* input,\n                size_t input_len, char* output, size_t output_len) const {\n    z_stream zs;\n    zs.zalloc = Z_NULL;\n    zs.zfree = Z_NULL;\n    zs.opaque = Z_NULL;\n    zs.avail_in = 0;\n    zs.next_in = Z_NULL;\n    if (inflateInit(&zs) != Z_OK) {\n        throw std::runtime_error(\"deflate uncompression init failure\");\n    }\n    // yuck, zlib is not const-correct, and also uses unsigned char while we use char :-(\n    zs.next_in = reinterpret_cast<unsigned char*>(const_cast<char*>(input));\n    zs.avail_in = input_len;\n    zs.next_out = reinterpret_cast<unsigned char*>(output);\n    zs.avail_out = output_len;\n    auto res = inflate(&zs, Z_FINISH);\n    inflateEnd(&zs);\n    if (res == Z_STREAM_END) {\n        return output_len - zs.avail_out;\n    } else {\n        throw std::runtime_error(\"deflate uncompression failure\");\n    }\n}\n\nsize_t deflate_processor::compress(const char* input,\n                size_t input_len, char* output, size_t output_len) const {\n    z_stream zs;\n    zs.zalloc = Z_NULL;\n    zs.zfree = Z_NULL;\n    zs.opaque = Z_NULL;\n    zs.avail_in = 0;\n    zs.next_in = Z_NULL;\n    if (deflateInit(&zs, Z_DEFAULT_COMPRESSION) != Z_OK) {\n        throw std::runtime_error(\"deflate compression init failure\");\n    }\n    zs.next_in = reinterpret_cast<unsigned char*>(const_cast<char*>(input));\n    zs.avail_in = input_len;\n    zs.next_out = reinterpret_cast<unsigned char*>(output);\n    zs.avail_out = output_len;\n    auto res = ::deflate(&zs, Z_FINISH);\n    deflateEnd(&zs);\n    if (res == Z_STREAM_END) {\n        return output_len - zs.avail_out;\n    } else {\n        throw std::runtime_error(\"deflate compression failure\");\n    }\n}\n\nsize_t deflate_processor::compress_max_size(size_t input_len) const {\n    z_stream zs;\n    zs.zalloc = Z_NULL;\n    zs.zfree = Z_NULL;\n    zs.opaque = Z_NULL;\n    zs.avail_in = 0;\n    zs.next_in = Z_NULL;\n    if (deflateInit(&zs, Z_DEFAULT_COMPRESSION) != Z_OK) {\n        throw std::runtime_error(\"deflate compression init failure\");\n    }\n    auto res = deflateBound(&zs, input_len);\n    deflateEnd(&zs);\n    return res;\n}\n\nsize_t snappy_processor::uncompress(const char* input, size_t input_len,\n                char* output, size_t output_len) const {\n    if (snappy_uncompress(input, input_len, output, &output_len)\n            == SNAPPY_OK) {\n        return output_len;\n    } else {\n        throw std::runtime_error(\"snappy uncompression failure\");\n    }\n}\n\nsize_t snappy_processor::compress(const char* input, size_t input_len,\n                char* output, size_t output_len) const {\n    auto ret = snappy_compress(input, input_len, output, &output_len);\n    if (ret != SNAPPY_OK) {\n        throw std::runtime_error(\"snappy compression failure: snappy_compress() failed\");\n    }\n    return output_len;\n}\n\nsize_t snappy_processor::compress_max_size(size_t input_len) const {\n    return snappy_max_compressed_length(input_len);\n}\n\n"
        },
        {
          "name": "compress.hh",
          "type": "blob",
          "size": 3.4853515625,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <map>\n#include <optional>\n#include <set>\n\n#include <seastar/core/future.hh>\n#include <seastar/core/shared_ptr.hh>\n#include <seastar/core/sstring.hh>\n#include \"seastarx.hh\"\n\nclass compressor {\n    sstring _name;\npublic:\n    compressor(sstring);\n\n    virtual ~compressor() {}\n\n    /**\n     * Unpacks data in \"input\" to output. If output_len is of insufficient size,\n     * exception is thrown. I.e. you should keep track of the uncompressed size.\n     */\n    virtual size_t uncompress(const char* input, size_t input_len, char* output,\n                    size_t output_len) const = 0;\n    /**\n     * Packs data in \"input\" to output. If output_len is of insufficient size,\n     * exception is thrown. Maximum required size is obtained via \"compress_max_size\"\n     */\n    virtual size_t compress(const char* input, size_t input_len, char* output,\n                    size_t output_len) const = 0;\n    /**\n     * Returns the maximum output size for compressing data on \"input_len\" size.\n     */\n    virtual size_t compress_max_size(size_t input_len) const = 0;\n\n    /**\n     * Returns accepted option names for this compressor\n     */\n    virtual std::set<sstring> option_names() const;\n    /**\n     * Returns original options used in instantiating this compressor\n     */\n    virtual std::map<sstring, sstring> options() const;\n\n    /**\n     * Compressor class name.\n     */\n    const sstring& name() const {\n        return _name;\n    }\n\n    // to cheaply bridge sstable compression options / maps\n    using opt_string = std::optional<sstring>;\n    using opt_getter = std::function<opt_string(const sstring&)>;\n    using ptr_type = shared_ptr<compressor>;\n\n    static ptr_type create(const sstring& name, const opt_getter&);\n    static ptr_type create(const std::map<sstring, sstring>&);\n\n    static thread_local const ptr_type lz4;\n    static thread_local const ptr_type snappy;\n    static thread_local const ptr_type deflate;\n\n    static const sstring namespace_prefix;\n};\n\ntemplate<typename BaseType, typename... Args>\nclass class_registry;\n\nusing compressor_ptr = compressor::ptr_type;\nusing compressor_registry = class_registry<compressor, const typename compressor::opt_getter&>;\n\nclass compression_parameters {\npublic:\n    static constexpr int32_t DEFAULT_CHUNK_LENGTH = 4 * 1024;\n    static constexpr double DEFAULT_CRC_CHECK_CHANCE = 1.0;\n\n    static const sstring SSTABLE_COMPRESSION;\n    static const sstring CHUNK_LENGTH_KB;\n    static const sstring CHUNK_LENGTH_KB_ERR;\n    static const sstring CRC_CHECK_CHANCE;\nprivate:\n    compressor_ptr _compressor;\n    std::optional<int> _chunk_length;\n    std::optional<double> _crc_check_chance;\npublic:\n    compression_parameters();\n    compression_parameters(compressor_ptr);\n    compression_parameters(const std::map<sstring, sstring>& options);\n    ~compression_parameters();\n\n    compressor_ptr get_compressor() const { return _compressor; }\n    int32_t chunk_length() const { return _chunk_length.value_or(int(DEFAULT_CHUNK_LENGTH)); }\n    double crc_check_chance() const { return _crc_check_chance.value_or(double(DEFAULT_CRC_CHECK_CHANCE)); }\n\n    void validate();\n    std::map<sstring, sstring> get_options() const;\n    bool operator==(const compression_parameters& other) const;\n\n    static compression_parameters no_compression() {\n        return compression_parameters(nullptr);\n    }\nprivate:\n    void validate_options(const std::map<sstring, sstring>&);\n};\n"
        },
        {
          "name": "concrete_types.hh",
          "type": "blob",
          "size": 10.7275390625,
          "content": "/*\n * Copyright (C) 2019-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <seastar/net/inet_address.hh>\n\n#include \"types/types.hh\"\n#include \"types/list.hh\"\n#include \"types/map.hh\"\n#include \"types/set.hh\"\n#include \"types/tuple.hh\"\n#include \"types/user.hh\"\n#include \"utils/big_decimal.hh\"\n\nstruct empty_type_impl final : public abstract_type {\n    using native_type = empty_type_representation;\n    empty_type_impl();\n};\n\nstruct counter_type_impl final : public abstract_type {\n    counter_type_impl();\n};\n\ntemplate <typename T>\nstruct simple_type_impl : public concrete_type<T> {\n    simple_type_impl(abstract_type::kind k, sstring name, std::optional<uint32_t> value_length_if_fixed);\n};\n\ntemplate<typename T>\nstruct integer_type_impl : public simple_type_impl<T> {\n    integer_type_impl(abstract_type::kind k, sstring name, std::optional<uint32_t> value_length_if_fixed);\n};\n\nstruct byte_type_impl final : public integer_type_impl<int8_t> {\n    byte_type_impl();\n};\n\nstruct short_type_impl final : public integer_type_impl<int16_t> {\n    short_type_impl();\n};\n\nstruct int32_type_impl final : public integer_type_impl<int32_t> {\n    int32_type_impl();\n};\n\nstruct long_type_impl final : public integer_type_impl<int64_t> {\n    long_type_impl();\n};\n\nstruct boolean_type_impl final : public simple_type_impl<bool> {\n    boolean_type_impl();\n};\n\ntemplate <typename T>\nstruct floating_type_impl : public simple_type_impl<T> {\n    floating_type_impl(abstract_type::kind k, sstring name, std::optional<uint32_t> value_length_if_fixed);\n};\n\nstruct double_type_impl final : public floating_type_impl<double> {\n    double_type_impl();\n};\n\nstruct float_type_impl final : public floating_type_impl<float> {\n    float_type_impl();\n};\n\nstruct decimal_type_impl final : public concrete_type<big_decimal> {\n    decimal_type_impl();\n};\n\nstruct duration_type_impl final : public concrete_type<cql_duration> {\n    duration_type_impl();\n};\n\nstruct timestamp_type_impl final : public simple_type_impl<db_clock::time_point> {\n    timestamp_type_impl();\n    static db_clock::time_point from_string_view(std::string_view s);\n};\n\nstruct simple_date_type_impl final : public simple_type_impl<uint32_t> {\n    simple_date_type_impl();\n    static uint32_t from_string_view(std::string_view s);\n};\n\nstruct time_type_impl final : public simple_type_impl<int64_t> {\n    time_type_impl();\n    static int64_t from_string_view(std::string_view s);\n};\n\nstruct string_type_impl : public concrete_type<sstring> {\n    string_type_impl(kind k, sstring name);\n};\n\nstruct ascii_type_impl final : public string_type_impl {\n    ascii_type_impl();\n};\n\nstruct utf8_type_impl final : public string_type_impl {\n    utf8_type_impl();\n};\n\nstruct bytes_type_impl final : public concrete_type<bytes> {\n    bytes_type_impl();\n};\n\n// This is the old version of timestamp_type_impl, but has been replaced as it\n// wasn't comparing pre-epoch timestamps correctly. This is kept for backward\n// compatibility but shouldn't be used in new code.\nstruct date_type_impl final : public concrete_type<db_clock::time_point> {\n    date_type_impl();\n};\n\nusing timestamp_date_base_class = concrete_type<db_clock::time_point>;\n\nsstring timestamp_to_json_string(const timestamp_date_base_class& t, const bytes_view& bv);\n\nstruct timeuuid_type_impl final : public concrete_type<utils::UUID> {\n    timeuuid_type_impl();\n    static utils::UUID from_string_view(std::string_view s);\n};\n\nstruct varint_type_impl final : public concrete_type<utils::multiprecision_int> {\n    varint_type_impl();\n};\n\nstruct inet_addr_type_impl final : public concrete_type<seastar::net::inet_address> {\n    inet_addr_type_impl();\n    static seastar::net::inet_address from_string_view(std::string_view s);\n};\n\nstruct uuid_type_impl final : public concrete_type<utils::UUID> {\n    uuid_type_impl();\n    static utils::UUID from_string_view(std::string_view s);\n};\n\ntemplate <typename Func> using visit_ret_type = std::invoke_result_t<Func, const ascii_type_impl&>;\n\ntemplate <typename Func> concept CanHandleAllTypes = requires(Func f) {\n    { f(*static_cast<const ascii_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const boolean_type_impl*>(nullptr)) }     -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const byte_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const bytes_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const counter_type_impl*>(nullptr)) }     -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const date_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const decimal_type_impl*>(nullptr)) }     -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const double_type_impl*>(nullptr)) }      -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const duration_type_impl*>(nullptr)) }    -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const empty_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const float_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const inet_addr_type_impl*>(nullptr)) }   -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const int32_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const list_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const long_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const map_type_impl*>(nullptr)) }         -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const reversed_type_impl*>(nullptr)) }    -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const set_type_impl*>(nullptr)) }         -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const short_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const simple_date_type_impl*>(nullptr)) } -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const time_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const timestamp_type_impl*>(nullptr)) }   -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const timeuuid_type_impl*>(nullptr)) }    -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const tuple_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const user_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const utf8_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const uuid_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;\n    { f(*static_cast<const varint_type_impl*>(nullptr)) }      -> std::same_as<visit_ret_type<Func>>;\n};\n\ntemplate<typename Func>\nrequires CanHandleAllTypes<Func>\ninline visit_ret_type<Func> visit(const abstract_type& t, Func&& f) {\n    switch (t.get_kind()) {\n    case abstract_type::kind::ascii:\n        return f(*static_cast<const ascii_type_impl*>(&t));\n    case abstract_type::kind::boolean:\n        return f(*static_cast<const boolean_type_impl*>(&t));\n    case abstract_type::kind::byte:\n        return f(*static_cast<const byte_type_impl*>(&t));\n    case abstract_type::kind::bytes:\n        return f(*static_cast<const bytes_type_impl*>(&t));\n    case abstract_type::kind::counter:\n        return f(*static_cast<const counter_type_impl*>(&t));\n    case abstract_type::kind::date:\n        return f(*static_cast<const date_type_impl*>(&t));\n    case abstract_type::kind::decimal:\n        return f(*static_cast<const decimal_type_impl*>(&t));\n    case abstract_type::kind::double_kind:\n        return f(*static_cast<const double_type_impl*>(&t));\n    case abstract_type::kind::duration:\n        return f(*static_cast<const duration_type_impl*>(&t));\n    case abstract_type::kind::empty:\n        return f(*static_cast<const empty_type_impl*>(&t));\n    case abstract_type::kind::float_kind:\n        return f(*static_cast<const float_type_impl*>(&t));\n    case abstract_type::kind::inet:\n        return f(*static_cast<const inet_addr_type_impl*>(&t));\n    case abstract_type::kind::int32:\n        return f(*static_cast<const int32_type_impl*>(&t));\n    case abstract_type::kind::list:\n        return f(*static_cast<const list_type_impl*>(&t));\n    case abstract_type::kind::long_kind:\n        return f(*static_cast<const long_type_impl*>(&t));\n    case abstract_type::kind::map:\n        return f(*static_cast<const map_type_impl*>(&t));\n    case abstract_type::kind::reversed:\n        return f(*static_cast<const reversed_type_impl*>(&t));\n    case abstract_type::kind::set:\n        return f(*static_cast<const set_type_impl*>(&t));\n    case abstract_type::kind::short_kind:\n        return f(*static_cast<const short_type_impl*>(&t));\n    case abstract_type::kind::simple_date:\n        return f(*static_cast<const simple_date_type_impl*>(&t));\n    case abstract_type::kind::time:\n        return f(*static_cast<const time_type_impl*>(&t));\n    case abstract_type::kind::timestamp:\n        return f(*static_cast<const timestamp_type_impl*>(&t));\n    case abstract_type::kind::timeuuid:\n        return f(*static_cast<const timeuuid_type_impl*>(&t));\n    case abstract_type::kind::tuple:\n        return f(*static_cast<const tuple_type_impl*>(&t));\n    case abstract_type::kind::user:\n        return f(*static_cast<const user_type_impl*>(&t));\n    case abstract_type::kind::utf8:\n        return f(*static_cast<const utf8_type_impl*>(&t));\n    case abstract_type::kind::uuid:\n        return f(*static_cast<const uuid_type_impl*>(&t));\n    case abstract_type::kind::varint:\n        return f(*static_cast<const varint_type_impl*>(&t));\n    }\n    __builtin_unreachable();\n}\n\ntemplate <typename Func> struct data_value_visitor {\n    const void* v;\n    Func& f;\n    auto operator()(const empty_type_impl& t) { return f(t, v); }\n    auto operator()(const counter_type_impl& t) { return f(t, v); }\n    auto operator()(const reversed_type_impl& t) { return f(t, v); }\n    template <typename T> auto operator()(const T& t) {\n        return f(t, reinterpret_cast<const typename T::native_type*>(v));\n    }\n};\n\n// Given an abstract_type and a void pointer to an object of that\n// type, call f with the runtime type of t and v casted to the\n// corresponding native type.\n// This takes an abstract_type and a void pointer instead of a\n// data_value to support reversed_type_impl without requiring that\n// each visitor create a new data_value just to recurse.\ntemplate <typename Func> inline auto visit(const abstract_type& t, const void* v, Func&& f) {\n    return ::visit(t, data_value_visitor<Func>{v, f});\n}\n\ntemplate <typename Func> inline auto visit(const data_value& v, Func&& f) {\n    return ::visit(*v.type(), v._value, f);\n}\n"
        },
        {
          "name": "conf",
          "type": "tree",
          "content": null
        },
        {
          "name": "configure.py",
          "type": "blob",
          "size": 131.595703125,
          "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2015-present ScyllaDB\n#\n\n#\n# SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n#\n\nimport argparse\nimport copy\nimport os\nimport pathlib\nimport platform\nimport re\nimport shlex\nimport subprocess\nimport sys\nimport tempfile\nimport textwrap\nfrom shutil import which\nfrom typing import NamedTuple\n\n\nconfigure_args = str.join(' ', [shlex.quote(x) for x in sys.argv[1:] if not x.startswith('--out=') and not x.startswith('--out-final-name=')])\n\n# distribution \"internationalization\", converting package names.\n# Fedora name is key, values is distro -> package name dict.\ni18n_xlat = {\n    'boost-devel': {\n        'debian': 'libboost-dev',\n        'ubuntu': 'libboost-dev (libboost1.55-dev on 14.04)',\n    },\n}\n\npython3_dependencies = subprocess.run('./install-dependencies.sh --print-python3-runtime-packages', shell=True, capture_output=True, encoding='utf-8').stdout.strip()\npip_dependencies = subprocess.run('./install-dependencies.sh --print-pip-runtime-packages', shell=True, capture_output=True, encoding='utf-8').stdout.strip()\npip_symlinks = subprocess.run('./install-dependencies.sh --print-pip-symlinks', shell=True, capture_output=True, encoding='utf-8').stdout.strip()\nnode_exporter_filename = subprocess.run('./install-dependencies.sh --print-node-exporter-filename', shell=True, capture_output=True, encoding='utf-8').stdout.strip()\nnode_exporter_dirname = os.path.basename(node_exporter_filename).rstrip('.tar.gz')\n\n\ndef get_os_ids():\n    if os.environ.get('NIX_CC'):\n        return ['linux']\n\n    if not os.path.exists('/etc/os-release'):\n        return ['unknown']\n\n    os_ids = []\n    for line in open('/etc/os-release'):\n        key, _, value = line.partition('=')\n        value = value.strip().strip('\"')\n        if key == 'ID':\n            os_ids = [value]\n        if key == 'ID_LIKE':\n            os_ids += value.split(' ')\n    if os_ids:\n        return os_ids\n    return ['linux']  # default ID per os-release(5)\n\n\ndef pkgname(name):\n    if name in i18n_xlat:\n        dict = i18n_xlat[name]\n        for id in get_os_ids():\n            if id in dict:\n                return dict[id]\n    return name\n\n\ndef get_flags():\n    with open('/proc/cpuinfo') as f:\n        for line in f:\n            if line.strip():\n                if line.rstrip('\\n').startswith('flags'):\n                    return re.sub(r'^flags\\s+: ', '', line).split()\n\n\ndef add_tristate(arg_parser, name, dest, help, default=None):\n    arg_parser.add_argument('--enable-' + name, dest=dest, action='store_true', default=default,\n                            help='Enable ' + help)\n    arg_parser.add_argument('--disable-' + name, dest=dest, action='store_false', default=None,\n                            help='Disable ' + help)\n\n\ndef apply_tristate(var, test, note, missing):\n    if (var is None) or var:\n        if test():\n            return True\n        elif var is True:\n            print(missing)\n            sys.exit(1)\n        else:\n            print(note)\n            return False\n    return False\n\n\ndef have_pkg(package):\n    return subprocess.call(['pkg-config', package]) == 0\n\n\ndef pkg_config(package, *options):\n    pkg_config_path = os.environ.get('PKG_CONFIG_PATH', '')\n    # Add the directory containing the package to the search path, if a file is\n    # specified instead of a name.\n    if package.endswith('.pc'):\n        local_path = os.path.dirname(package)\n        pkg_config_path = '{}:{}'.format(local_path, pkg_config_path)\n\n    output = subprocess.check_output(['pkg-config'] + list(options) + [package],\n                                     env = {**os.environ,\n                                            'PKG_CONFIG_PATH': pkg_config_path})\n\n    return output.decode('utf-8').strip()\n\n\ndef try_compile(compiler, source='', flags=[]):\n    return try_compile_and_link(compiler, source, flags=flags + ['-c'])\n\n\ndef try_compile_and_link(compiler, source='', flags=[], verbose=False):\n    os.makedirs(tempfile.tempdir, exist_ok=True)\n    with tempfile.NamedTemporaryFile() as sfile:\n        ofd, ofile = tempfile.mkstemp()\n        os.close(ofd)\n        try:\n            sfile.file.write(bytes(source, 'utf-8'))\n            sfile.file.flush()\n            ret = subprocess.run([compiler, '-x', 'c++', '-o', ofile, sfile.name] + args.user_cflags.split() + flags,\n                                 capture_output=True)\n            if verbose:\n                print(f\"Compilation failed: {compiler} -x c++ -o {ofile} {sfile.name} {args.user_cflags} {flags}\")\n                print(source)\n                print(ret.stdout.decode('utf-8'))\n                print(ret.stderr.decode('utf-8'))\n            return ret.returncode == 0\n        finally:\n            if os.path.exists(ofile):\n                os.unlink(ofile)\n\n\ndef flag_supported(flag, compiler):\n    # gcc ignores -Wno-x even if it is not supported\n    adjusted = re.sub('^-Wno-', '-W', flag)\n    split = adjusted.split(' ')\n    return try_compile(flags=['-Werror'] + split, compiler=compiler)\n\n\ndef linker_flags(compiler):\n    src_main = 'int main(int argc, char **argv) { return 0; }'\n    link_flags = ['-fuse-ld=lld']\n    if try_compile_and_link(source=src_main, flags=link_flags, compiler=compiler):\n        print('Note: using the lld linker')\n        return ' '.join(link_flags)\n    link_flags = ['-fuse-ld=gold']\n    if try_compile_and_link(source=src_main, flags=link_flags, compiler=compiler):\n        print('Note: using the gold linker')\n        threads_flag = '-Wl,--threads'\n        if try_compile_and_link(source=src_main, flags=link_flags + [threads_flag], compiler=compiler):\n            link_flags.append(threads_flag)\n        return ' '.join(link_flags)\n    else:\n        linker = ''\n        try:\n            subprocess.call([\"gold\", \"-v\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n            linker = 'gold'\n        except:\n            pass\n        try:\n            subprocess.call([\"lld\", \"-v\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n            linker = 'lld'\n        except:\n            pass\n        if linker:\n            print(f'Linker {linker} found, but the compilation attempt failed, defaulting to default system linker')\n        else:\n            print('Note: neither lld nor gold found; using default system linker')\n        return ''\n\n\ndef maybe_static(flag, libs):\n    if flag:\n        libs = '-Wl,-Bstatic {} -Wl,-Bdynamic'.format(libs)\n    return libs\n\n\nclass Source(object):\n    def __init__(self, source, hh_prefix, cc_prefix):\n        self.source = source\n        self.hh_prefix = hh_prefix\n        self.cc_prefix = cc_prefix\n\n    def headers(self, gen_dir):\n        return [x for x in self.generated(gen_dir) if x.endswith(self.hh_prefix)]\n\n    def sources(self, gen_dir):\n        return [x for x in self.generated(gen_dir) if x.endswith(self.cc_prefix)]\n\n    def objects(self, gen_dir):\n        return [x.replace(self.cc_prefix, '.o') for x in self.sources(gen_dir)]\n\n    def endswith(self, end):\n        return self.source.endswith(end)\n\n\ndef default_target_arch():\n    if platform.machine() in ['i386', 'i686', 'x86_64']:\n        return 'westmere'   # support PCLMUL\n    elif platform.machine() == 'aarch64':\n        return 'armv8-a+crc+crypto'\n    else:\n        return ''\n\n\nclass Antlr3Grammar(Source):\n    def __init__(self, source):\n        Source.__init__(self, source, '.hpp', '.cpp')\n\n    def generated(self, gen_dir):\n        basename = os.path.splitext(self.source)[0]\n        files = [basename + ext\n                 for ext in ['Lexer.cpp', 'Lexer.hpp', 'Parser.cpp', 'Parser.hpp']]\n        return [os.path.join(gen_dir, file) for file in files]\n\nclass Json2Code(Source):\n    def __init__(self, source):\n        Source.__init__(self, source, '.hh', '.cc')\n\n    def generated(self, gen_dir):\n        return [os.path.join(gen_dir, self.source + '.hh'), os.path.join(gen_dir, self.source + '.cc')]\n\ndef find_headers(repodir, excluded_dirs):\n    walker = os.walk(repodir)\n\n    _, dirs, files = next(walker)\n    for excl_dir in excluded_dirs:\n        try:\n            dirs.remove(excl_dir)\n        except ValueError:\n            # Ignore complaints about excl_dir not being in dirs\n            pass\n\n    is_hh = lambda f: f.endswith('.hh')\n    headers = list(filter(is_hh, files))\n\n    for dirpath, _, files in walker:\n        if dirpath.startswith('./'):\n            dirpath = dirpath[2:]\n        headers += [os.path.join(dirpath, hh) for hh in filter(is_hh, files)]\n\n    return sorted(headers)\n\n\ndef generate_compdb(compdb, ninja, buildfile, modes):\n    # per-mode compdbs are built by taking the relevant entries from the\n    # output of \"ninja -t compdb\" and combining them with the CMake-made\n    # compdbs for Seastar in the relevant mode.\n    #\n    # \"ninja -t compdb\" output has to be filtered because\n    # - it contains rules for all selected modes, and several entries for\n    #   the same source file usually confuse indexers\n    # - it contains lots of irrelevant entries (for linker invocations,\n    #   header-only compilations, etc.)\n    os.makedirs(tempfile.tempdir, exist_ok=True)\n    with tempfile.NamedTemporaryFile() as ninja_compdb:\n        subprocess.run([ninja, '-f', buildfile, '-t', 'compdb'], stdout=ninja_compdb.file.fileno())\n        ninja_compdb.file.flush()\n\n        # build mode-specific compdbs\n        for mode in modes:\n            mode_out = outdir + '/' + mode\n            submodule_compdbs = [mode_out + '/' + submodule + '/' + compdb for submodule in ['seastar', 'abseil']]\n            with open(mode_out + '/' + compdb, 'w+b') as combined_mode_specific_compdb:\n                subprocess.run(['./scripts/merge-compdb.py', outdir + '/' + mode,\n                                ninja_compdb.name] + submodule_compdbs, stdout=combined_mode_specific_compdb)\n\n    # sort modes by supposed indexing speed\n    for mode in ['dev', 'debug', 'release', 'sanitize']:\n        compdb_target = outdir + '/' + mode + '/' + compdb\n        if os.path.exists(compdb_target):\n            try:\n                os.symlink(compdb_target, compdb)\n            except FileExistsError:\n                # if there is already a valid compile_commands.json link in the\n                # source root, we are done.\n                pass\n            return\n\n\ndef check_for_minimal_compiler_version(cxx):\n    compiler_test_src = '''\n\n// clang pretends to be gcc (defined __GNUC__), so we\n// must check it first\n#ifdef __clang__\n\n#if __clang_major__ < 10\n    #error \"MAJOR\"\n#endif\n\n#elif defined(__GNUC__)\n\n#if __GNUC__ < 10\n    #error \"MAJOR\"\n#elif __GNUC__ == 10\n    #if __GNUC_MINOR__ < 1\n        #error \"MINOR\"\n    #elif __GNUC_MINOR__ == 1\n        #if __GNUC_PATCHLEVEL__ < 1\n            #error \"PATCHLEVEL\"\n        #endif\n    #endif\n#endif\n\n#else\n\n#error \"Unrecognized compiler\"\n\n#endif\n\nint main() { return 0; }\n'''\n    if try_compile_and_link(compiler=cxx, source=compiler_test_src):\n        return\n    try_compile_and_link(compiler=cxx, source=compiler_test_src, verbose=True)\n    print('Wrong compiler version or incorrect flags. '\n          'Scylla needs GCC >= 10.1.1 with coroutines (-fcoroutines) or '\n          'clang >= 10.0.0 to compile.')\n    sys.exit(1)\n\n\ndef check_for_boost(cxx):\n    pkg_name = pkgname(\"boost-devel\")\n    if not try_compile(compiler=cxx, source='#include <boost/version.hpp>'):\n        print(f'Boost not installed.  Please install {pkg_name}.')\n        sys.exit(1)\n\n    if not try_compile(compiler=cxx, source='''\\\n            #include <boost/version.hpp>\n            #if BOOST_VERSION < 105500\n            #error Boost version too low\n            #endif\n            '''):\n        print(f'Installed boost version too old.  Please update {pkg_name}.')\n        sys.exit(1)\n\n\ndef check_for_lz4(cxx, cflags):\n    if not try_compile(cxx, source=textwrap.dedent('''\\\n        #include <lz4.h>\n\n        void m() {\n            LZ4_compress_default(static_cast<const char*>(0), static_cast<char*>(0), 0, 0);\n        }\n        '''), flags=cflags.split()):\n        print('Installed lz4-devel is too old. Please upgrade it to r129 / v1.73 and up')\n        sys.exit(1)\n\n\ndef find_ninja():\n    ninja = which('ninja') or which('ninja-build')\n    if ninja:\n        return ninja\n    print('Ninja executable (ninja or ninja-build) not found on PATH\\n')\n    sys.exit(1)\n\n\nmodes = {\n    'debug': {\n        'cxxflags': '-DDEBUG -DSANITIZE -DDEBUG_LSA_SANITIZER -DSCYLLA_ENABLE_ERROR_INJECTION',\n        'cxx_ld_flags': '',\n        'stack-usage-threshold': 1024*40,\n        'optimization-level': 'g',\n        'per_src_extra_cxxflags': {},\n        'cmake_build_type': 'Debug',\n        'can_have_debug_info': True,\n        'build_seastar_shared_libs': True,\n        'default': True,\n        'description': 'a mode with no optimizations, with sanitizers, and with additional debug checks enabled, used for testing',\n        'advanced_optimizations': False,\n    },\n    'release': {\n        'cxxflags': '-ffunction-sections -fdata-sections ',\n        'cxx_ld_flags': '-Wl,--gc-sections',\n        'stack-usage-threshold': 1024*13,\n        'optimization-level': '3',\n        'per_src_extra_cxxflags': {},\n        'cmake_build_type': 'RelWithDebInfo',\n        'can_have_debug_info': True,\n        'build_seastar_shared_libs': False,\n        'default': True,\n        'description': 'a mode with optimizations and no debug checks, used for production builds',\n        'advanced_optimizations': True,\n    },\n    'dev': {\n        'cxxflags': '-DDEVEL -DSEASTAR_ENABLE_ALLOC_FAILURE_INJECTION -DSCYLLA_ENABLE_ERROR_INJECTION -DSCYLLA_ENABLE_PREEMPTION_SOURCE',\n        'cxx_ld_flags': '',\n        'stack-usage-threshold': 1024*21,\n        'optimization-level': '2',\n        'per_src_extra_cxxflags': {},\n        'cmake_build_type': 'Dev',\n        'can_have_debug_info': False,\n        'build_seastar_shared_libs': True,\n        'default': True,\n        'description': 'a mode with no optimizations and no debug checks, optimized for fast build times, used for development',\n        'advanced_optimizations': False,\n    },\n    'sanitize': {\n        'cxxflags': '-DDEBUG -DSANITIZE -DDEBUG_LSA_SANITIZER -DSCYLLA_ENABLE_ERROR_INJECTION',\n        'cxx_ld_flags': '',\n        'stack-usage-threshold': 1024*50,\n        'optimization-level': 's',\n        'per_src_extra_cxxflags': {},\n        'cmake_build_type': 'Sanitize',\n        'can_have_debug_info': True,\n        'build_seastar_shared_libs': False,\n        'default': False,\n        'description': 'a mode with optimizations and sanitizers enabled, used for finding memory errors',\n        'advanced_optimizations': False,\n    },\n    'coverage': {\n        'cxxflags': '-fprofile-instr-generate -fcoverage-mapping -g -gz',\n        'cxx_ld_flags': '-fprofile-instr-generate -fcoverage-mapping',\n        'stack-usage-threshold': 1024*40,\n        'optimization-level': 'g',\n        'per_src_extra_cxxflags': {},\n        'cmake_build_type': 'Debug',\n        'can_have_debug_info': True,\n        'build_seastar_shared_libs': False,\n        'default': False,\n        'description': 'a mode exclusively used for generating test coverage reports',\n        'advanced_optimizations': False,\n    },\n}\n\nscylla_tests = set([\n    'test/boost/combined_tests',\n    'test/boost/UUID_test',\n    'test/boost/advanced_rpc_compressor_test',\n    'test/boost/allocation_strategy_test',\n    'test/boost/alternator_unit_test',\n    'test/boost/anchorless_list_test',\n    'test/boost/auth_passwords_test',\n    'test/boost/auth_resource_test',\n    'test/boost/big_decimal_test',\n    'test/boost/bloom_filter_test',\n    'test/boost/bptree_test',\n    'test/boost/broken_sstable_test',\n    'test/boost/btree_test',\n    'test/boost/bytes_ostream_test',\n    'test/boost/cache_mutation_reader_test',\n    'test/boost/cached_file_test',\n    'test/boost/caching_options_test',\n    'test/boost/canonical_mutation_test',\n    'test/boost/cartesian_product_test',\n    'test/boost/cdc_generation_test',\n    'test/boost/cell_locker_test',\n    'test/boost/checksum_utils_test',\n    'test/boost/chunked_managed_vector_test',\n    'test/boost/chunked_vector_test',\n    'test/boost/clustering_ranges_walker_test',\n    'test/boost/compaction_group_test',\n    'test/boost/compound_test',\n    'test/boost/compress_test',\n    'test/boost/config_test',\n    'test/boost/continuous_data_consumer_test',\n    'test/boost/counter_test',\n    'test/boost/cql_auth_syntax_test',\n    'test/boost/crc_test',\n    'test/boost/dict_trainer_test',\n    'test/boost/dirty_memory_manager_test',\n    'test/boost/double_decker_test',\n    'test/boost/duration_test',\n    'test/boost/dynamic_bitset_test',\n    'test/boost/enum_option_test',\n    'test/boost/enum_set_test',\n    'test/boost/estimated_histogram_test',\n    'test/boost/exception_container_test',\n    'test/boost/exceptions_fallback_test',\n    'test/boost/exceptions_optimized_test',\n    'test/boost/expr_test',\n    'test/boost/flush_queue_test',\n    'test/boost/fragmented_temporary_buffer_test',\n    'test/boost/frozen_mutation_test',\n    'test/boost/generic_server_test',\n    'test/boost/gossiping_property_file_snitch_test',\n    'test/boost/hash_test',\n    'test/boost/hashers_test',\n    'test/boost/hint_test',\n    'test/boost/idl_test',\n    'test/boost/incremental_compaction_test',\n    'test/boost/index_reader_test',\n    'test/boost/input_stream_test',\n    'test/boost/intrusive_array_test',\n    'test/boost/json_test',\n    'test/boost/keys_test',\n    'test/boost/like_matcher_test',\n    'test/boost/limiting_data_source_test',\n    'test/boost/linearizing_input_stream_test',\n    'test/boost/lister_test',\n    'test/boost/locator_topology_test',\n    'test/boost/log_heap_test',\n    'test/boost/logalloc_standard_allocator_segment_pool_backend_test',\n    'test/boost/logalloc_test',\n    'test/boost/managed_bytes_test',\n    'test/boost/managed_vector_test',\n    'test/boost/map_difference_test',\n    'test/boost/murmur_hash_test',\n    'test/boost/mutation_fragment_test',\n    'test/boost/mutation_query_test',\n    'test/boost/mutation_reader_another_test',\n    'test/boost/mutation_test',\n    'test/boost/mvcc_test',\n    'test/boost/nonwrapping_interval_test',\n    'test/boost/observable_test',\n    'test/boost/partitioner_test',\n    'test/boost/pretty_printers_test',\n    'test/boost/radix_tree_test',\n    'test/boost/range_tombstone_list_test',\n    'test/boost/rate_limiter_test',\n    'test/boost/recent_entries_map_test',\n    'test/boost/reservoir_sampling_test',\n    'test/boost/result_utils_test',\n    'test/boost/reusable_buffer_test',\n    'test/boost/rust_test',\n    'test/boost/s3_test',\n    'test/boost/aws_errors_test',\n    'test/boost/aws_error_injection_test',\n    'test/boost/schema_changes_test',\n    'test/boost/schema_loader_test',\n    'test/boost/serialization_test',\n    'test/boost/serialized_action_test',\n    'test/boost/service_level_controller_test',\n    'test/boost/small_vector_test',\n    'test/boost/snitch_reset_test',\n    'test/boost/sorting_test',\n    'test/boost/sstable_3_x_test',\n    'test/boost/sstable_conforms_to_mutation_source_test',\n    'test/boost/sstable_datafile_test',\n    'test/boost/sstable_generation_test',\n    'test/boost/sstable_move_test',\n    'test/boost/sstable_mutation_test',\n    'test/boost/sstable_partition_index_cache_test',\n    'test/boost/sstable_resharding_test',\n    'test/boost/sstable_test',\n    'test/boost/stall_free_test',\n    'test/boost/stream_compressor_test',\n    'test/boost/string_format_test',\n    'test/boost/summary_test',\n    'test/boost/tagged_integer_test',\n    'test/boost/token_metadata_test',\n    'test/boost/top_k_test',\n    'test/boost/transport_test',\n    'test/boost/types_test',\n    'test/boost/utf8_test',\n    'test/boost/vint_serialization_test',\n    'test/boost/virtual_table_mutation_source_test',\n    'test/boost/wasm_alloc_test',\n    'test/boost/wasm_test',\n    'test/boost/wrapping_interval_test',\n    'test/manual/ec2_snitch_test',\n    'test/manual/enormous_table_scan_test',\n    'test/manual/gce_snitch_test',\n    'test/manual/gossip',\n    'test/manual/hint_test',\n    'test/manual/message',\n    'test/manual/partition_data_test',\n    'test/manual/row_locker_test',\n    'test/manual/streaming_histogram_test',\n    'test/manual/sstable_scan_footprint_test',\n    'test/perf/memory_footprint_test',\n    'test/perf/perf_cache_eviction',\n    'test/perf/perf_commitlog',\n    'test/perf/perf_cql_parser',\n    'test/perf/perf_hash',\n    'test/perf/perf_mutation',\n    'test/perf/perf_collection',\n    'test/perf/perf_row_cache_reads',\n    'test/perf/logalloc',\n    'test/perf/perf_s3_client',\n    'test/unit/lsa_async_eviction_test',\n    'test/unit/lsa_sync_eviction_test',\n    'test/unit/row_cache_alloc_stress_test',\n    'test/unit/row_cache_stress_test',\n    'test/unit/cross_shard_barrier_test',\n    'test/boost/address_map_test',\n])\n\nperf_tests = set([\n    'test/perf/perf_mutation_readers',\n    'test/perf/perf_checksum',\n    'test/perf/perf_mutation_fragment',\n    'test/perf/perf_idl',\n    'test/perf/perf_vint',\n    'test/perf/perf_big_decimal',\n    'test/perf/perf_sort_by_proximity',\n])\n\nraft_tests = set([\n    'test/raft/replication_test',\n    'test/raft/randomized_nemesis_test',\n    'test/raft/many_test',\n    'test/raft/raft_server_test',\n    'test/raft/fsm_test',\n    'test/raft/etcd_test',\n    'test/raft/raft_sys_table_storage_test',\n    'test/raft/discovery_test',\n    'test/raft/failure_detector_test',\n])\n\nwasms = set([\n    'wasm/return_input.wat',\n    'wasm/test_complex_null_values.wat',\n    'wasm/test_fib_called_on_null.wat',\n    'wasm/test_functions_with_frozen_types.wat',\n    'wasm/test_mem_grow.wat',\n    'wasm/test_pow.wat',\n    'wasm/test_short_ints.wat',\n    'wasm/test_types_with_and_without_nulls.wat',\n    'wasm/test_UDA_final.wat',\n    'wasm/test_UDA_scalar.wat',\n    'wasm/test_word_double.wat',\n])\n\napps = set([\n    'scylla',\n])\n\nlto_binaries = set([\n    'scylla'\n])\n\ntests = scylla_tests | perf_tests | raft_tests\n\nother = set([\n    'iotune',\n])\n\nall_artifacts = apps | tests | other | wasms\n\narg_parser = argparse.ArgumentParser('Configure scylla', add_help=False, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\narg_parser.add_argument('--out', dest='buildfile', action='store', default='build.ninja',\n                        help='Output build-file name (by default build.ninja)')\narg_parser.add_argument('--out-final-name', dest=\"buildfile_final_name\", action='store',\n                        help='If set, rules will be generated as if this were the actual name of the file instead of the name passed by the --out option. \\\n                              This option is rather not useful for developers, it is intended to be used by Ninja when it decides to regenerate the makefile \\\n                              (a makefile with the same name but with a \".new\" suffix is generated, then it is renamed to overwrite the old file; \\\n                              the new file\\'s regeneration rule itself needs to refer to the correct filename).')\narg_parser.add_argument('--mode', action='append', choices=list(modes.keys()), dest='selected_modes',\n                        help=\"Build modes to generate ninja files for. The available build modes are:\\n{}\".format(\"; \".join([\"{} - {}\".format(m, cfg['description']) for m, cfg in modes.items()])))\narg_parser.add_argument('--with', dest='artifacts', action='append', default=[],\n                        help=\"Specify the artifacts to build, invoke {} with --list-artifacts to list all available artifacts, if unspecified all artifacts are built\".format(sys.argv[0]))\narg_parser.add_argument('--with-seastar', action='store', dest='seastar_path', default='seastar', help='Path to Seastar sources')\nadd_tristate(arg_parser, name='dist', dest='enable_dist',\n                        help='build dist targets')\narg_parser.add_argument('--dist-only', dest='dist_only', action='store_true', default=False,\n                        help='skip compiling code and run dist targets only')\n\narg_parser.add_argument('--cflags', action='store', dest='user_cflags', default='',\n                        help='Extra flags for the C++ compiler')\narg_parser.add_argument('--ldflags', action='store', dest='user_ldflags', default='',\n                        help='Extra flags for the linker')\narg_parser.add_argument('--target', action='store', dest='target', default=default_target_arch(),\n                        help='Target architecture (-march)')\narg_parser.add_argument('--compiler', action='store', dest='cxx', default='clang++',\n                        help='C++ compiler path')\narg_parser.add_argument('--c-compiler', action='store', dest='cc', default='clang',\n                        help='C compiler path')\nadd_tristate(arg_parser, name='dpdk', dest='dpdk',\n                        help='Use dpdk (from seastar dpdk sources) (default=True for release builds)')\narg_parser.add_argument('--dpdk-target', action='store', dest='dpdk_target', default='',\n                        help='Path to DPDK SDK target location (e.g. <DPDK SDK dir>/x86_64-native-linuxapp-gcc)')\narg_parser.add_argument('--debuginfo', action='store', dest='debuginfo', type=int, default=1,\n                        help='Enable(1)/disable(0)compiler debug information generation')\narg_parser.add_argument('--optimization-level', action='append', dest='mode_o_levels', metavar='MODE=LEVEL', default=[],\n                        help=f'Override default compiler optimization level for mode (defaults: {\" \".join([x+\"=\"+modes[x][\"optimization-level\"] for x in modes])})')\narg_parser.add_argument('--static-stdc++', dest='staticcxx', action='store_true',\n                        help='Link libgcc and libstdc++ statically')\narg_parser.add_argument('--static-boost', dest='staticboost', action='store_true',\n                        help='Link boost statically')\narg_parser.add_argument('--static-yaml-cpp', dest='staticyamlcpp', action='store_true',\n                        help='Link libyaml-cpp statically')\narg_parser.add_argument('--tests-debuginfo', action='store', dest='tests_debuginfo', type=int, default=0,\n                        help='Enable(1)/disable(0)compiler debug information generation for tests')\narg_parser.add_argument('--perf-tests-debuginfo', action='store', dest='perf_tests_debuginfo', type=int, default=0,\n                        help='Enable(1)/disable(0)compiler debug information generation for perf tests')\narg_parser.add_argument('--split-dwarf', dest='split_dwarf', action='store_true', default=False,\n                        help='use of split dwarf (https://gcc.gnu.org/wiki/DebugFission) to speed up linking')\narg_parser.add_argument('--enable-alloc-failure-injector', dest='alloc_failure_injector', action='store_true', default=False,\n                        help='enable allocation failure injection')\narg_parser.add_argument('--enable-seastar-debug-allocations', dest='seastar_debug_allocations', action='store_true', default=False,\n                        help='enable seastar debug allocations')\narg_parser.add_argument('--with-antlr3', dest='antlr3_exec', action='store', default=\"antlr3\",\n                        help='path to antlr3 executable')\narg_parser.add_argument('--with-ragel', dest='ragel_exec', action='store', default='ragel',\n        help='path to ragel executable')\nadd_tristate(arg_parser, name='stack-guards', dest='stack_guards', help='Use stack guards')\narg_parser.add_argument('--verbose', dest='verbose', action='store_true',\n                        help='Make configure.py output more verbose (useful for debugging the build process itself)')\narg_parser.add_argument('--test-repeat', dest='test_repeat', action='store', type=str, default='1',\n                         help='Set number of times to repeat each unittest.')\narg_parser.add_argument('--test-timeout', dest='test_timeout', action='store', type=str, default='7200')\narg_parser.add_argument('--clang-inline-threshold', action='store', type=int, dest='clang_inline_threshold', default=-1,\n                        help=\"LLVM-specific inline threshold compilation parameter\")\narg_parser.add_argument('--list-artifacts', dest='list_artifacts', action='store_true', default=False,\n                        help='List all available build artifacts, that can be passed to --with')\narg_parser.add_argument('--date-stamp', dest='date_stamp', type=str,\n                        help='Set datestamp for SCYLLA-VERSION-GEN')\nadd_tristate(arg_parser, name='lto', dest='lto', default=True,\n                        help='link-time optimization.')\narg_parser.add_argument('--use-profile', dest='use_profile', action='store',\n                        help='Path to the (optional) profile file to be used in the build. Meant to be used with the profile file (build/release/profiles/merged.profdata) generated during a previous build of build/release/scylla with --pgo (--cspgo).')\narg_parser.add_argument('--pgo', dest='pgo', action='store_true', default=False,\n                        help='Generate and use fresh PGO profiles when building Scylla. Only supported with clang for now.')\narg_parser.add_argument('--cspgo', dest='cspgo', action='store_true', default=False,\n                        help='Generate and use fresh CSPGO profiles when building Scylla. A clang-specific optional addition to --pgo.')\narg_parser.add_argument('--experimental-pgo', dest='experimental_pgo', action='store_true', default=False,\n                        help='When building with PGO, enable nonconservative (potentially pessimizing) optimizations. Only supported with clang for now. Not recommended.')\narg_parser.add_argument('--use-cmake', action=argparse.BooleanOptionalAction, default=False, help='Whether to use CMake as the build system')\narg_parser.add_argument('--coverage', action = 'store_true', help = 'Compile scylla with coverage instrumentation')\narg_parser.add_argument('--build-dir', action='store', default='build',\n                        help='Build directory path')\narg_parser.add_argument('-h', '--help', action='store_true', help='show this help message and exit')\nargs = arg_parser.parse_args()\nif args.help:\n    arg_parser.print_help()\n    arg_parser.exit()\n\nPROFILES_LIST_FILE_NAME = \"coverage_sources.list\"\n\noutdir = args.build_dir\ntempfile.tempdir = f\"{outdir}/tmp\"\n\nif args.list_artifacts:\n    for artifact in sorted(all_artifacts):\n        print(artifact)\n    exit(0)\n\ndefines = ['XXH_PRIVATE_API',\n           'SEASTAR_TESTING_MAIN',\n]\n\nscylla_raft_core = [\n    'raft/raft.cc',\n    'raft/server.cc',\n    'raft/fsm.cc',\n    'raft/tracker.cc',\n    'raft/log.cc',\n]\n\nscylla_core = (['message/messaging_service.cc',\n                'replica/database.cc',\n                'replica/table.cc',\n                'replica/tablets.cc',\n                'replica/distributed_loader.cc',\n                'replica/memtable.cc',\n                'replica/exceptions.cc',\n                'replica/dirty_memory_manager.cc',\n                'replica/mutation_dump.cc',\n                'mutation/atomic_cell.cc',\n                'mutation/canonical_mutation.cc',\n                'mutation/frozen_mutation.cc',\n                'mutation/mutation.cc',\n                'mutation/mutation_fragment.cc',\n                'mutation/mutation_fragment_stream_validator.cc',\n                'mutation/mutation_partition.cc',\n                'mutation/mutation_partition_v2.cc',\n                'mutation/mutation_partition_view.cc',\n                'mutation/mutation_partition_serializer.cc',\n                'mutation/partition_version.cc',\n                'mutation/range_tombstone.cc',\n                'mutation/range_tombstone_list.cc',\n                'mutation/async_utils.cc',\n                'absl-flat_hash_map.cc',\n                'collection_mutation.cc',\n                'client_data.cc',\n                'debug.cc',\n                'schema/caching_options.cc',\n                'schema/schema.cc',\n                'schema/schema_registry.cc',\n                'frozen_schema.cc',\n                'bytes.cc',\n                'timeout_config.cc',\n                'row_cache.cc',\n                'schema_mutations.cc',\n                'generic_server.cc',\n                'utils/alien_worker.cc',\n                'utils/array-search.cc',\n                'utils/base64.cc',\n                'utils/logalloc.cc',\n                'utils/large_bitset.cc',\n                'utils/buffer_input_stream.cc',\n                'utils/limiting_data_source.cc',\n                'utils/updateable_value.cc',\n                'utils/dict_trainer.cc',\n                'message/dictionary_service.cc',\n                'utils/directories.cc',\n                'gms/generation-number.cc',\n                'utils/rjson.cc',\n                'utils/human_readable.cc',\n                'utils/histogram_metrics_helper.cc',\n                'utils/on_internal_error.cc',\n                'utils/pretty_printers.cc',\n                'utils/stream_compressor.cc',\n                'converting_mutation_partition_applier.cc',\n                'readers/combined.cc',\n                'readers/multishard.cc',\n                'readers/mutation_reader.cc',\n                'readers/mutation_readers.cc',\n                'mutation_query.cc',\n                'keys.cc',\n                'counters.cc',\n                'compress.cc',\n                'zstd.cc',\n                'sstables/sstables.cc',\n                'sstables/sstables_manager.cc',\n                'sstables/sstable_set.cc',\n                'sstables/storage.cc',\n                'sstables/mx/partition_reversing_data_source.cc',\n                'sstables/mx/reader.cc',\n                'sstables/mx/writer.cc',\n                'sstables/kl/reader.cc',\n                'sstables/sstable_version.cc',\n                'sstables/compress.cc',\n                'sstables/checksummed_data_source.cc',\n                'sstables/sstable_mutation_reader.cc',\n                'compaction/compaction.cc',\n                'compaction/compaction_strategy.cc',\n                'compaction/size_tiered_compaction_strategy.cc',\n                'compaction/leveled_compaction_strategy.cc',\n                'compaction/task_manager_module.cc',\n                'compaction/time_window_compaction_strategy.cc',\n                'compaction/compaction_manager.cc',\n                'compaction/incremental_compaction_strategy.cc',\n                'compaction/incremental_backlog_tracker.cc',\n                'sstables/integrity_checked_file_impl.cc',\n                'sstables/prepended_input_stream.cc',\n                'sstables/m_format_read_helpers.cc',\n                'sstables/sstable_directory.cc',\n                'sstables/random_access_reader.cc',\n                'sstables/metadata_collector.cc',\n                'sstables/writer.cc',\n                'transport/cql_protocol_extension.cc',\n                'transport/event.cc',\n                'transport/event_notifier.cc',\n                'transport/server.cc',\n                'transport/controller.cc',\n                'transport/messages/result_message.cc',\n                'cdc/cdc_partitioner.cc',\n                'cdc/log.cc',\n                'cdc/split.cc',\n                'cdc/generation.cc',\n                'cdc/metadata.cc',\n                'cql3/type_json.cc',\n                'cql3/attributes.cc',\n                'cql3/cf_name.cc',\n                'cql3/cql3_type.cc',\n                'cql3/description.cc',\n                'cql3/operation.cc',\n                'cql3/index_name.cc',\n                'cql3/keyspace_element_name.cc',\n                'cql3/lists.cc',\n                'cql3/sets.cc',\n                'cql3/maps.cc',\n                'cql3/values.cc',\n                'cql3/expr/expression.cc',\n                'cql3/expr/restrictions.cc',\n                'cql3/expr/prepare_expr.cc',\n                'cql3/functions/user_function.cc',\n                'cql3/functions/functions.cc',\n                'cql3/functions/aggregate_fcts.cc',\n                'cql3/functions/castas_fcts.cc',\n                'cql3/functions/error_injection_fcts.cc',\n                'cql3/statements/cf_prop_defs.cc',\n                'cql3/statements/cf_statement.cc',\n                'cql3/statements/authentication_statement.cc',\n                'cql3/statements/create_keyspace_statement.cc',\n                'cql3/statements/create_table_statement.cc',\n                'cql3/statements/create_view_statement.cc',\n                'cql3/statements/create_type_statement.cc',\n                'cql3/statements/create_function_statement.cc',\n                'cql3/statements/create_aggregate_statement.cc',\n                'cql3/statements/drop_index_statement.cc',\n                'cql3/statements/drop_keyspace_statement.cc',\n                'cql3/statements/drop_table_statement.cc',\n                'cql3/statements/drop_view_statement.cc',\n                'cql3/statements/drop_type_statement.cc',\n                'cql3/statements/drop_function_statement.cc',\n                'cql3/statements/drop_aggregate_statement.cc',\n                'cql3/statements/schema_altering_statement.cc',\n                'cql3/statements/ks_prop_defs.cc',\n                'cql3/statements/function_statement.cc',\n                'cql3/statements/modification_statement.cc',\n                'cql3/statements/cas_request.cc',\n                'cql3/statements/raw/parsed_statement.cc',\n                'cql3/statements/property_definitions.cc',\n                'cql3/statements/update_statement.cc',\n                'cql3/statements/strongly_consistent_modification_statement.cc',\n                'cql3/statements/strongly_consistent_select_statement.cc',\n                'cql3/statements/delete_statement.cc',\n                'cql3/statements/prune_materialized_view_statement.cc',\n                'cql3/statements/batch_statement.cc',\n                'cql3/statements/select_statement.cc',\n                'cql3/statements/use_statement.cc',\n                'cql3/statements/index_prop_defs.cc',\n                'cql3/statements/index_target.cc',\n                'cql3/statements/create_index_statement.cc',\n                'cql3/statements/truncate_statement.cc',\n                'cql3/statements/alter_table_statement.cc',\n                'cql3/statements/alter_view_statement.cc',\n                'cql3/statements/list_users_statement.cc',\n                'cql3/statements/authorization_statement.cc',\n                'cql3/statements/permission_altering_statement.cc',\n                'cql3/statements/list_permissions_statement.cc',\n                'cql3/statements/grant_statement.cc',\n                'cql3/statements/revoke_statement.cc',\n                'cql3/statements/alter_type_statement.cc',\n                'cql3/statements/alter_keyspace_statement.cc',\n                'cql3/statements/role-management-statements.cc',\n                'cql3/statements/service_level_statement.cc',\n                'cql3/statements/create_service_level_statement.cc',\n                'cql3/statements/alter_service_level_statement.cc',\n                'cql3/statements/sl_prop_defs.cc',\n                'cql3/statements/drop_service_level_statement.cc',\n                'cql3/statements/attach_service_level_statement.cc',\n                'cql3/statements/detach_service_level_statement.cc',\n                'cql3/statements/list_service_level_statement.cc',\n                'cql3/statements/list_service_level_attachments_statement.cc',\n                'cql3/statements/list_effective_service_level_statement.cc',\n                'cql3/statements/describe_statement.cc',\n                'cql3/update_parameters.cc',\n                'cql3/util.cc',\n                'cql3/ut_name.cc',\n                'cql3/role_name.cc',\n                'data_dictionary/data_dictionary.cc',\n                'utils/runtime.cc',\n                'utils/murmur_hash.cc',\n                'utils/uuid.cc',\n                'utils/big_decimal.cc',\n                'types/types.cc',\n                'validation.cc',\n                'service/migration_manager.cc',\n                'service/tablet_allocator.cc',\n                'service/storage_proxy.cc',\n                'query_ranges_to_vnodes.cc',\n                'service/mapreduce_service.cc',\n                'service/paxos/proposal.cc',\n                'service/paxos/prepare_response.cc',\n                'service/paxos/paxos_state.cc',\n                'service/paxos/prepare_summary.cc',\n                'cql3/column_identifier.cc',\n                'cql3/column_specification.cc',\n                'cql3/constants.cc',\n                'cql3/query_processor.cc',\n                'cql3/query_options.cc',\n                'cql3/user_types.cc',\n                'cql3/untyped_result_set.cc',\n                'cql3/selection/selectable.cc',\n                'cql3/selection/selection.cc',\n                'cql3/selection/selector.cc',\n                'cql3/restrictions/statement_restrictions.cc',\n                'cql3/result_set.cc',\n                'cql3/prepare_context.cc',\n                'db/consistency_level.cc',\n                'db/system_keyspace.cc',\n                'db/virtual_table.cc',\n                'db/virtual_tables.cc',\n                'db/system_distributed_keyspace.cc',\n                'db/size_estimates_virtual_reader.cc',\n                'db/schema_applier.cc',\n                'db/schema_tables.cc',\n                'db/cql_type_parser.cc',\n                'db/legacy_schema_migrator.cc',\n                'db/commitlog/commitlog.cc',\n                'db/commitlog/commitlog_replayer.cc',\n                'db/commitlog/commitlog_entry.cc',\n                'db/data_listeners.cc',\n                'db/functions/function.cc',\n                'db/hints/internal/hint_endpoint_manager.cc',\n                'db/hints/internal/hint_sender.cc',\n                'db/hints/internal/hint_storage.cc',\n                'db/hints/manager.cc',\n                'db/hints/resource_manager.cc',\n                'db/hints/host_filter.cc',\n                'db/hints/sync_point.cc',\n                'db/config.cc',\n                'db/extensions.cc',\n                'db/heat_load_balance.cc',\n                'db/large_data_handler.cc',\n                'db/marshal/type_parser.cc',\n                'db/batchlog_manager.cc',\n                'db/tags/utils.cc',\n                'db/view/view.cc',\n                'db/view/view_update_generator.cc',\n                'db/view/row_locking.cc',\n                'db/sstables-format-selector.cc',\n                'db/snapshot-ctl.cc',\n                'db/rate_limiter.cc',\n                'db/per_partition_rate_limit_options.cc',\n                'db/snapshot/backup_task.cc',\n                'index/secondary_index_manager.cc',\n                'index/secondary_index.cc',\n                'utils/UUID_gen.cc',\n                'utils/i_filter.cc',\n                'utils/bloom_filter.cc',\n                'utils/bloom_calculations.cc',\n                'utils/rate_limiter.cc',\n                'utils/file_lock.cc',\n                'utils/dynamic_bitset.cc',\n                'utils/managed_bytes.cc',\n                'utils/exceptions.cc',\n                'utils/config_file.cc',\n                'utils/multiprecision_int.cc',\n                'utils/gz/crc_combine.cc',\n                'utils/gz/crc_combine_table.cc',\n                'utils/s3/aws_error.cc',\n                'utils/s3/client.cc',\n                'utils/s3/retry_strategy.cc',\n                'utils/advanced_rpc_compressor.cc',\n                'gms/version_generator.cc',\n                'gms/versioned_value.cc',\n                'gms/gossiper.cc',\n                'gms/feature_service.cc',\n                'gms/gossip_digest_syn.cc',\n                'gms/gossip_digest_ack.cc',\n                'gms/gossip_digest_ack2.cc',\n                'gms/endpoint_state.cc',\n                'gms/application_state.cc',\n                'gms/inet_address.cc',\n                'dht/i_partitioner.cc',\n                'dht/token.cc',\n                'dht/murmur3_partitioner.cc',\n                'dht/boot_strapper.cc',\n                'dht/range_streamer.cc',\n                'unimplemented.cc',\n                'query.cc',\n                'query-result-set.cc',\n                'locator/abstract_replication_strategy.cc',\n                'locator/tablets.cc',\n                'locator/azure_snitch.cc',\n                'locator/simple_strategy.cc',\n                'locator/local_strategy.cc',\n                'locator/network_topology_strategy.cc',\n                'locator/everywhere_replication_strategy.cc',\n                'locator/token_metadata.cc',\n                'locator/snitch_base.cc',\n                'locator/simple_snitch.cc',\n                'locator/rack_inferring_snitch.cc',\n                'locator/gossiping_property_file_snitch.cc',\n                'locator/production_snitch_base.cc',\n                'locator/ec2_snitch.cc',\n                'locator/ec2_multi_region_snitch.cc',\n                'locator/gce_snitch.cc',\n                'locator/topology.cc',\n                'locator/util.cc',\n                'service/client_state.cc',\n                'service/storage_service.cc',\n                'service/session.cc',\n                'service/task_manager_module.cc',\n                'service/misc_services.cc',\n                'service/pager/paging_state.cc',\n                'service/pager/query_pagers.cc',\n                'service/qos/qos_common.cc',\n                'service/qos/service_level_controller.cc',\n                'service/qos/standard_service_level_distributed_data_accessor.cc',\n                'service/qos/raft_service_level_distributed_data_accessor.cc',\n                'streaming/stream_task.cc',\n                'streaming/stream_session.cc',\n                'streaming/stream_request.cc',\n                'streaming/stream_summary.cc',\n                'streaming/stream_transfer_task.cc',\n                'streaming/stream_receive_task.cc',\n                'streaming/stream_plan.cc',\n                'streaming/progress_info.cc',\n                'streaming/session_info.cc',\n                'streaming/stream_coordinator.cc',\n                'streaming/stream_manager.cc',\n                'streaming/stream_result_future.cc',\n                'streaming/stream_session_state.cc',\n                'streaming/consumer.cc',\n                'clocks-impl.cc',\n                'partition_slice_builder.cc',\n                'init.cc',\n                'utils/lister.cc',\n                'repair/repair.cc',\n                'repair/row_level.cc',\n                'repair/table_check.cc',\n                'exceptions/exceptions.cc',\n                'auth/allow_all_authenticator.cc',\n                'auth/allow_all_authorizer.cc',\n                'auth/authenticated_user.cc',\n                'auth/authenticator.cc',\n                'auth/common.cc',\n                'auth/default_authorizer.cc',\n                'auth/resource.cc',\n                'auth/roles-metadata.cc',\n                'auth/passwords.cc',\n                'auth/password_authenticator.cc',\n                'auth/permission.cc',\n                'auth/permissions_cache.cc',\n                'auth/service.cc',\n                'auth/standard_role_manager.cc',\n                'auth/transitional.cc',\n                'auth/maintenance_socket_role_manager.cc',\n                'auth/role_or_anonymous.cc',\n                'auth/sasl_challenge.cc',\n                'auth/certificate_authenticator.cc',\n                'tracing/tracing.cc',\n                'tracing/trace_keyspace_helper.cc',\n                'tracing/trace_state.cc',\n                'tracing/traced_file.cc',\n                'table_helper.cc',\n                'tombstone_gc_options.cc',\n                'tombstone_gc.cc',\n                'utils/disk-error-handler.cc',\n                'utils/hashers.cc',\n                'utils/aws_sigv4.cc',\n                'duration.cc',\n                'vint-serialization.cc',\n                'utils/arch/powerpc/crc32-vpmsum/crc32_wrapper.cc',\n                'querier.cc',\n                'mutation_writer/multishard_writer.cc',\n                'multishard_mutation_query.cc',\n                'reader_concurrency_semaphore.cc',\n                'sstables_loader.cc',\n                'utils/utf8.cc',\n                'utils/ascii.cc',\n                'utils/like_matcher.cc',\n                'utils/error_injection.cc',\n                'utils/build_id.cc',\n                'mutation_writer/timestamp_based_splitting_writer.cc',\n                'mutation_writer/shard_based_splitting_writer.cc',\n                'mutation_writer/partition_based_splitting_writer.cc',\n                'mutation_writer/token_group_based_splitting_writer.cc',\n                'mutation_writer/feed_writers.cc',\n                'lang/manager.cc',\n                'lang/lua.cc',\n                'lang/wasm.cc',\n                'lang/wasm_alien_thread_runner.cc',\n                'lang/wasm_instance_cache.cc',\n                'service/raft/group0_state_id_handler.cc',\n                'service/raft/group0_state_machine.cc',\n                'service/raft/group0_state_machine_merger.cc',\n                'service/raft/raft_sys_table_storage.cc',\n                'serializer.cc',\n                'release.cc',\n                'service/raft/raft_rpc.cc',\n                'service/raft/raft_group_registry.cc',\n                'service/raft/discovery.cc',\n                'service/raft/raft_group0.cc',\n                'direct_failure_detector/failure_detector.cc',\n                'service/raft/raft_group0_client.cc',\n                'service/broadcast_tables/experimental/lang.cc',\n                'tasks/task_handler.cc',\n                'tasks/task_manager.cc',\n                'rust/wasmtime_bindings/src/lib.rs',\n                'utils/to_string.cc',\n                'service/topology_state_machine.cc',\n                'service/topology_mutation.cc',\n                'service/topology_coordinator.cc',\n                'node_ops/node_ops_ctl.cc',\n                'node_ops/task_manager_module.cc',\n                'reader_concurrency_semaphore_group.cc',\n                ] + [Antlr3Grammar('cql3/Cql.g')] \\\n                  + scylla_raft_core\n               )\n\napi = ['api/api.cc',\n       Json2Code('api/api-doc/storage_service.json'),\n       Json2Code('api/api-doc/lsa.json'),\n       'api/storage_service.cc',\n       'api/token_metadata.cc',\n       Json2Code('api/api-doc/commitlog.json'),\n       'api/commitlog.cc',\n       Json2Code('api/api-doc/gossiper.json'),\n       'api/gossiper.cc',\n       Json2Code('api/api-doc/failure_detector.json'),\n       'api/failure_detector.cc',\n       Json2Code('api/api-doc/column_family.json'),\n       'api/column_family.cc',\n       'api/messaging_service.cc',\n       Json2Code('api/api-doc/messaging_service.json'),\n       Json2Code('api/api-doc/storage_proxy.json'),\n       'api/storage_proxy.cc',\n       Json2Code('api/api-doc/cache_service.json'),\n       'api/cache_service.cc',\n       Json2Code('api/api-doc/collectd.json'),\n       'api/collectd.cc',\n       Json2Code('api/api-doc/endpoint_snitch_info.json'),\n       'api/endpoint_snitch.cc',\n       Json2Code('api/api-doc/compaction_manager.json'),\n       'api/compaction_manager.cc',\n       Json2Code('api/api-doc/hinted_handoff.json'),\n       'api/hinted_handoff.cc',\n       Json2Code('api/api-doc/utils.json'),\n       'api/lsa.cc',\n       Json2Code('api/api-doc/stream_manager.json'),\n       'api/stream_manager.cc',\n       Json2Code('api/api-doc/system.json'),\n       'api/system.cc',\n       Json2Code('api/api-doc/tasks.json'),\n       'api/tasks.cc',\n       Json2Code('api/api-doc/task_manager.json'),\n       'api/task_manager.cc',\n       Json2Code('api/api-doc/task_manager_test.json'),\n       'api/task_manager_test.cc',\n       'api/config.cc',\n       Json2Code('api/api-doc/config.json'),\n       Json2Code('api/api-doc/metrics.json'),\n       'api/error_injection.cc',\n       Json2Code('api/api-doc/error_injection.json'),\n       'api/authorization_cache.cc',\n       Json2Code('api/api-doc/authorization_cache.json'),\n       'api/raft.cc',\n       Json2Code('api/api-doc/raft.json'),\n       Json2Code('api/api-doc/cql_server_test.json'),\n       'api/cql_server_test.cc',\n       'api/service_levels.cc',\n       Json2Code('api/api-doc/service_levels.json'),\n       ]\n\nalternator = [\n       'alternator/controller.cc',\n       'alternator/server.cc',\n       'alternator/executor.cc',\n       'alternator/stats.cc',\n       'alternator/serialization.cc',\n       'alternator/expressions.cc',\n       Antlr3Grammar('alternator/expressions.g'),\n       'alternator/conditions.cc',\n       'alternator/consumed_capacity.cc',\n       'alternator/auth.cc',\n       'alternator/streams.cc',\n       'alternator/ttl.cc',\n]\n\nredis = [\n        'redis/controller.cc',\n        'redis/server.cc',\n        'redis/query_processor.cc',\n        'redis/protocol_parser.rl',\n        'redis/keyspace_utils.cc',\n        'redis/options.cc',\n        'redis/stats.cc',\n        'redis/mutation_utils.cc',\n        'redis/query_utils.cc',\n        'redis/abstract_command.cc',\n        'redis/command_factory.cc',\n        'redis/commands.cc',\n        'redis/lolwut.cc',\n        ]\n\nidls = ['idl/gossip_digest.idl.hh',\n        'idl/uuid.idl.hh',\n        'idl/range.idl.hh',\n        'idl/keys.idl.hh',\n        'idl/read_command.idl.hh',\n        'idl/token.idl.hh',\n        'idl/ring_position.idl.hh',\n        'idl/result.idl.hh',\n        'idl/frozen_mutation.idl.hh',\n        'idl/reconcilable_result.idl.hh',\n        'idl/streaming.idl.hh',\n        'idl/paging_state.idl.hh',\n        'idl/frozen_schema.idl.hh',\n        'idl/repair.idl.hh',\n        'idl/replay_position.idl.hh',\n        'idl/mutation.idl.hh',\n        'idl/query.idl.hh',\n        'idl/idl_test.idl.hh',\n        'idl/commitlog.idl.hh',\n        'idl/tracing.idl.hh',\n        'idl/consistency_level.idl.hh',\n        'idl/cache_temperature.idl.hh',\n        'idl/view.idl.hh',\n        'idl/messaging_service.idl.hh',\n        'idl/paxos.idl.hh',\n        'idl/raft.idl.hh',\n        'idl/raft_storage.idl.hh',\n        'idl/group0.idl.hh',\n        'idl/hinted_handoff.idl.hh',\n        'idl/storage_proxy.idl.hh',\n        'idl/group0_state_machine.idl.hh',\n        'idl/mapreduce_request.idl.hh',\n        'idl/replica_exception.idl.hh',\n        'idl/per_partition_rate_limit_info.idl.hh',\n        'idl/position_in_partition.idl.hh',\n        'idl/experimental/broadcast_tables_lang.idl.hh',\n        'idl/storage_service.idl.hh',\n        'idl/join_node.idl.hh',\n        'idl/utils.idl.hh',\n        'idl/gossip.idl.hh',\n        'idl/migration_manager.idl.hh',\n        \"idl/node_ops.idl.hh\",\n\n        ]\n\nscylla_tests_generic_dependencies = [\n    'test/lib/cql_test_env.cc',\n    'test/lib/test_services.cc',\n    'test/lib/log.cc',\n    'test/lib/test_utils.cc',\n    'test/lib/tmpdir.cc',\n    'test/lib/sstable_run_based_compaction_strategy_for_tests.cc',\n]\n\nscylla_tests_dependencies = scylla_core + alternator + idls + scylla_tests_generic_dependencies + [\n    'test/lib/cql_assertions.cc',\n    'test/lib/result_set_assertions.cc',\n    'test/lib/mutation_source_test.cc',\n    'test/lib/sstable_utils.cc',\n    'test/lib/data_model.cc',\n    'test/lib/exception_utils.cc',\n    'test/lib/random_schema.cc',\n    'test/lib/key_utils.cc',\n]\n\nscylla_raft_dependencies = scylla_raft_core + ['utils/uuid.cc', 'utils/error_injection.cc', 'utils/exceptions.cc']\n\nscylla_tools = ['tools/read_mutation.cc',\n                'tools/scylla-types.cc',\n                'tools/scylla-sstable.cc',\n                'tools/scylla-nodetool.cc',\n                'tools/schema_loader.cc',\n                'tools/load_system_tablets.cc',\n                'tools/utils.cc',\n                'tools/lua_sstable_consumer.cc']\nscylla_perfs = ['test/perf/perf_alternator.cc',\n                'test/perf/perf_fast_forward.cc',\n                'test/perf/perf_row_cache_update.cc',\n                'test/perf/perf_simple_query.cc',\n                'test/perf/perf_sstable.cc',\n                'test/perf/perf_tablets.cc',\n                'test/perf/tablet_load_balancing.cc',\n                'test/perf/perf.cc',\n                'test/lib/alternator_test_env.cc',\n                'test/lib/cql_test_env.cc',\n                'test/lib/log.cc',\n                'test/lib/test_services.cc',\n                'test/lib/test_utils.cc',\n                'test/lib/tmpdir.cc',\n                'test/lib/key_utils.cc',\n                'test/lib/random_schema.cc',\n                'test/lib/data_model.cc',\n                'seastar/tests/perf/linux_perf_event.cc']\n\ndeps = {\n    'scylla': idls + ['main.cc'] + scylla_core + api + alternator + redis + scylla_tools + scylla_perfs,\n}\n\npure_boost_tests = set([\n    'test/boost/anchorless_list_test',\n    'test/boost/auth_passwords_test',\n    'test/boost/auth_resource_test',\n    'test/boost/big_decimal_test',\n    'test/boost/caching_options_test',\n    'test/boost/cartesian_product_test',\n    'test/boost/checksum_utils_test',\n    'test/boost/chunked_vector_test',\n    'test/boost/compress_test',\n    'test/boost/cql_auth_syntax_test',\n    'test/boost/crc_test',\n    'test/boost/duration_test',\n    'test/boost/dynamic_bitset_test',\n    'test/boost/enum_option_test',\n    'test/boost/enum_set_test',\n    'test/boost/idl_test',\n    'test/boost/json_test',\n    'test/boost/keys_test',\n    'test/boost/like_matcher_test',\n    'test/boost/linearizing_input_stream_test',\n    'test/boost/map_difference_test',\n    'test/boost/nonwrapping_interval_test',\n    'test/boost/observable_test',\n    'test/boost/wrapping_interval_test',\n    'test/boost/range_tombstone_list_test',\n    'test/boost/reservoir_sampling_test',\n    'test/boost/serialization_test',\n    'test/boost/small_vector_test',\n    'test/boost/top_k_test',\n    'test/boost/vint_serialization_test',\n    'test/boost/utf8_test',\n    'test/boost/string_format_test',\n    'test/manual/streaming_histogram_test',\n])\n\ntests_not_using_seastar_test_framework = set([\n    'test/boost/alternator_unit_test',\n    'test/boost/small_vector_test',\n    'test/manual/gossip',\n    'test/manual/message',\n    'test/perf/memory_footprint_test',\n    'test/perf/perf_cache_eviction',\n    'test/perf/perf_cql_parser',\n    'test/perf/perf_hash',\n    'test/perf/perf_mutation',\n    'test/perf/perf_collection',\n    'test/perf/logalloc',\n    'test/unit/lsa_async_eviction_test',\n    'test/unit/lsa_sync_eviction_test',\n    'test/unit/row_cache_alloc_stress_test',\n    'test/manual/sstable_scan_footprint_test',\n    'test/unit/cross_shard_barrier_test',\n]) | pure_boost_tests\n\n\nCOVERAGE_INST_FLAGS = ['-fprofile-instr-generate', '-fcoverage-mapping', f'-fprofile-list=./{PROFILES_LIST_FILE_NAME}']\nif args.coverage:\n    for _, mode in filter(lambda m: m[0] != \"coverage\", modes.items()):\n        mode['cxx_ld_flags'] += ' ' + ' '.join(COVERAGE_INST_FLAGS)\n        mode['cxx_ld_flags'] = mode['cxx_ld_flags'].strip()\n        mode['cxxflags'] += ' ' + ' '.join(COVERAGE_INST_FLAGS)\n        mode['cxxflags'] = mode['cxxflags'].strip()\n\nfor t in tests_not_using_seastar_test_framework:\n    if t not in scylla_tests:\n        raise Exception(\"Test %s not found in scylla_tests\" % (t))\n\nfor t in sorted(scylla_tests):\n    deps[t] = [t + '.cc']\n    if t not in tests_not_using_seastar_test_framework:\n        deps[t] += scylla_tests_dependencies\n    else:\n        deps[t] += scylla_core + alternator + idls + scylla_tests_generic_dependencies\n\nperf_tests_seastar_deps = [\n    'seastar/tests/perf/perf_tests.cc'\n]\n\nfor t in sorted(perf_tests):\n    deps[t] = [t + '.cc'] + scylla_tests_dependencies + perf_tests_seastar_deps\n    deps[t] += ['test/perf/perf.cc', 'seastar/tests/perf/linux_perf_event.cc']\n\ndeps['test/boost/combined_tests'] += [\n    'test/boost/aggregate_fcts_test.cc',\n    'test/boost/auth_test.cc',\n    'test/boost/batchlog_manager_test.cc',\n    'test/boost/cache_algorithm_test.cc',\n    'test/boost/castas_fcts_test.cc',\n    'test/boost/cdc_test.cc',\n    'test/boost/column_mapping_test.cc',\n    'test/boost/commitlog_cleanup_test.cc',\n    'test/boost/commitlog_test.cc',\n    'test/boost/cql_auth_query_test.cc',\n    'test/boost/cql_functions_test.cc',\n    'test/boost/cql_query_group_test.cc',\n    'test/boost/cql_query_large_test.cc',\n    'test/boost/cql_query_like_test.cc',\n    'test/boost/cql_query_test.cc',\n    'test/boost/database_test.cc',\n    'test/boost/data_listeners_test.cc',\n    'test/boost/error_injection_test.cc',\n    'test/boost/extensions_test.cc',\n    'test/boost/filtering_test.cc',\n    'test/boost/group0_cmd_merge_test.cc',\n    'test/boost/group0_test.cc',\n    'test/boost/index_with_paging_test.cc',\n    'test/boost/json_cql_query_test.cc',\n    'test/boost/large_paging_state_test.cc',\n    'test/boost/loading_cache_test.cc',\n    'test/boost/memtable_test.cc',\n    'test/boost/multishard_combining_reader_as_mutation_source_test.cc',\n    'test/boost/multishard_mutation_query_test.cc',\n    'test/boost/mutation_reader_test.cc',\n    'test/boost/mutation_writer_test.cc',\n    'test/boost/network_topology_strategy_test.cc',\n    'test/boost/per_partition_rate_limit_test.cc',\n    'test/boost/querier_cache_test.cc',\n    'test/boost/query_processor_test.cc',\n    'test/boost/reader_concurrency_semaphore_test.cc',\n    'test/boost/repair_test.cc',\n    'test/boost/restrictions_test.cc',\n    'test/boost/role_manager_test.cc',\n    'test/boost/row_cache_test.cc',\n    'test/boost/schema_change_test.cc',\n    'test/boost/schema_registry_test.cc',\n    'test/boost/secondary_index_test.cc',\n    'test/boost/sessions_test.cc',\n    'test/boost/sstable_compaction_test.cc',\n    'test/boost/sstable_directory_test.cc',\n    'test/boost/sstable_set_test.cc',\n    'test/boost/statement_restrictions_test.cc',\n    'test/boost/storage_proxy_test.cc',\n    'test/boost/tablets_test.cc',\n    'test/boost/tracing_test.cc',\n    'test/boost/user_function_test.cc',\n    'test/boost/user_types_test.cc',\n    'test/boost/view_build_test.cc',\n    'test/boost/view_complex_test.cc',\n    'test/boost/view_schema_ckey_test.cc',\n    'test/boost/view_schema_pkey_test.cc',\n    'test/boost/view_schema_test.cc',\n    'test/boost/virtual_reader_test.cc',\n    'test/boost/virtual_table_test.cc',\n    'tools/schema_loader.cc',\n    'tools/read_mutation.cc',\n    'test/lib/expr_test_utils.cc',\n    'test/lib/dummy_sharder.cc',\n]\n\ndeps['test/boost/bytes_ostream_test'] = [\n    \"test/boost/bytes_ostream_test.cc\",\n    \"bytes.cc\",\n    \"utils/managed_bytes.cc\",\n    \"utils/logalloc.cc\",\n    \"utils/dynamic_bitset.cc\",\n    \"test/lib/log.cc\",\n]\ndeps['test/boost/input_stream_test'] = ['test/boost/input_stream_test.cc']\ndeps['test/boost/UUID_test'] = ['clocks-impl.cc', 'utils/UUID_gen.cc', 'test/boost/UUID_test.cc', 'utils/uuid.cc', 'utils/dynamic_bitset.cc', 'utils/hashers.cc', 'utils/on_internal_error.cc']\ndeps['test/boost/murmur_hash_test'] = ['bytes.cc', 'utils/murmur_hash.cc', 'test/boost/murmur_hash_test.cc']\ndeps['test/boost/allocation_strategy_test'] = ['test/boost/allocation_strategy_test.cc', 'utils/logalloc.cc', 'utils/dynamic_bitset.cc']\ndeps['test/boost/log_heap_test'] = ['test/boost/log_heap_test.cc']\ndeps['test/boost/estimated_histogram_test'] = ['test/boost/estimated_histogram_test.cc']\ndeps['test/boost/summary_test'] = ['test/boost/summary_test.cc']\ndeps['test/boost/anchorless_list_test'] = ['test/boost/anchorless_list_test.cc']\ndeps['test/perf/perf_commitlog'] += ['test/perf/perf.cc', 'seastar/tests/perf/linux_perf_event.cc']\ndeps['test/perf/perf_row_cache_reads'] += ['test/perf/perf.cc', 'seastar/tests/perf/linux_perf_event.cc']\ndeps['test/boost/reusable_buffer_test'] = [\n    \"test/boost/reusable_buffer_test.cc\",\n    \"test/lib/log.cc\",\n]\ndeps['test/boost/utf8_test'] = ['utils/utf8.cc', 'test/boost/utf8_test.cc']\ndeps['test/boost/small_vector_test'] = ['test/boost/small_vector_test.cc']\ndeps['test/boost/vint_serialization_test'] = ['test/boost/vint_serialization_test.cc', 'vint-serialization.cc', 'bytes.cc']\ndeps['test/boost/linearizing_input_stream_test'] = [\n    \"test/boost/linearizing_input_stream_test.cc\",\n    \"test/lib/log.cc\",\n]\ndeps['test/boost/expr_test'] = ['test/boost/expr_test.cc', 'test/lib/expr_test_utils.cc'] + scylla_core\ndeps['test/boost/rate_limiter_test'] = ['test/boost/rate_limiter_test.cc', 'db/rate_limiter.cc']\ndeps['test/boost/exceptions_optimized_test'] = ['test/boost/exceptions_optimized_test.cc', 'utils/exceptions.cc']\ndeps['test/boost/exceptions_fallback_test'] = ['test/boost/exceptions_fallback_test.cc', 'utils/exceptions.cc']\n\ndeps['test/boost/duration_test'] += ['test/lib/exception_utils.cc']\ndeps['test/boost/schema_loader_test'] += ['tools/schema_loader.cc', 'tools/read_mutation.cc']\ndeps['test/boost/rust_test'] += ['rust/inc/src/lib.rs']\n\ndeps['test/raft/replication_test'] = ['test/raft/replication_test.cc', 'test/raft/replication.cc', 'test/raft/helpers.cc'] + scylla_raft_dependencies\ndeps['test/raft/raft_server_test'] = ['test/raft/raft_server_test.cc', 'test/raft/replication.cc', 'test/raft/helpers.cc'] + scylla_raft_dependencies\ndeps['test/raft/randomized_nemesis_test'] = ['test/raft/randomized_nemesis_test.cc', 'direct_failure_detector/failure_detector.cc', 'test/raft/helpers.cc'] + scylla_raft_dependencies\ndeps['test/raft/failure_detector_test'] = ['test/raft/failure_detector_test.cc', 'direct_failure_detector/failure_detector.cc', 'test/raft/helpers.cc'] + scylla_raft_dependencies\ndeps['test/raft/many_test'] = ['test/raft/many_test.cc', 'test/raft/replication.cc', 'test/raft/helpers.cc'] + scylla_raft_dependencies\ndeps['test/raft/fsm_test'] =  ['test/raft/fsm_test.cc', 'test/raft/helpers.cc', 'test/lib/log.cc'] + scylla_raft_dependencies\ndeps['test/raft/etcd_test'] =  ['test/raft/etcd_test.cc', 'test/raft/helpers.cc', 'test/lib/log.cc'] + scylla_raft_dependencies\ndeps['test/raft/raft_sys_table_storage_test'] = ['test/raft/raft_sys_table_storage_test.cc'] + \\\n    scylla_core + scylla_tests_generic_dependencies\ndeps['test/boost/address_map_test'] = ['test/boost/address_map_test.cc'] + scylla_core\ndeps['test/raft/discovery_test'] =  ['test/raft/discovery_test.cc',\n                                     'test/raft/helpers.cc',\n                                     'test/lib/log.cc',\n                                     'service/raft/discovery.cc'] + scylla_raft_dependencies\n\nwasm_deps = {}\n\nwasm_deps['wasm/return_input.wat'] = 'test/resource/wasm/rust/return_input.rs'\nwasm_deps['wasm/test_short_ints.wat'] = 'test/resource/wasm/rust/test_short_ints.rs'\nwasm_deps['wasm/test_complex_null_values.wat'] = 'test/resource/wasm/rust/test_complex_null_values.rs'\nwasm_deps['wasm/test_functions_with_frozen_types.wat'] = 'test/resource/wasm/rust/test_functions_with_frozen_types.rs'\nwasm_deps['wasm/test_types_with_and_without_nulls.wat'] = 'test/resource/wasm/rust/test_types_with_and_without_nulls.rs'\n\nwasm_deps['wasm/test_fib_called_on_null.wat'] = 'test/resource/wasm/c/test_fib_called_on_null.c'\nwasm_deps['wasm/test_mem_grow.wat'] = 'test/resource/wasm/c/test_mem_grow.c'\nwasm_deps['wasm/test_pow.wat'] = 'test/resource/wasm/c/test_pow.c'\nwasm_deps['wasm/test_UDA_final.wat'] = 'test/resource/wasm/c/test_UDA_final.c'\nwasm_deps['wasm/test_UDA_scalar.wat'] = 'test/resource/wasm/c/test_UDA_scalar.c'\nwasm_deps['wasm/test_word_double.wat'] = 'test/resource/wasm/c/test_word_double.c'\n\n\ndef get_warning_options(cxx):\n    warnings = [\n        '-Wall',\n        '-Werror',\n        '-Wextra',\n        '-Wimplicit-fallthrough',\n        '-Wno-mismatched-tags',  # clang-only\n        '-Wno-c++11-narrowing',\n        '-Wno-overloaded-virtual',\n        '-Wno-unused-parameter',\n        '-Wno-unsupported-friend',\n        '-Wno-missing-field-initializers',\n        '-Wno-deprecated-copy',\n        '-Wno-enum-constexpr-conversion',\n    ]\n\n    warnings = [w\n                for w in warnings\n                if flag_supported(flag=w, compiler=cxx)]\n\n    return ' '.join(warnings + ['-Wno-error=deprecated-declarations'])\n\n\ndef get_clang_inline_threshold():\n    if args.clang_inline_threshold != -1:\n        return args.clang_inline_threshold\n    else:\n        return 2500\n\nfor mode_level in args.mode_o_levels:\n    ( mode, level ) = mode_level.split('=', 2)\n    if mode not in modes:\n        raise Exception(f'Mode {mode} is missing, cannot configure optimization level for it')\n    modes[mode]['optimization-level'] = level\n\nlinker_flags = linker_flags(compiler=args.cxx)\n\ntests_link_rule = 'link' if args.tests_debuginfo else 'link_stripped'\nperf_tests_link_rule = 'link' if args.perf_tests_debuginfo else 'link_stripped'\n\n# Strip if debuginfo is disabled, otherwise we end up with partial\n# debug info from the libraries we static link with\nregular_link_rule = 'link' if args.debuginfo else 'link_stripped'\n\nhas_sanitize_address_use_after_scope = try_compile(compiler=args.cxx, flags=['-fsanitize-address-use-after-scope'], source='int f() {}')\n\ndefines = ' '.join(['-D' + d for d in defines])\n\nglobals().update(vars(args))\n\ntotal_memory = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')\nlink_pool_depth = max(int(total_memory / 7e9), 1)\n\nselected_modes = args.selected_modes or modes.keys()\ndefault_modes = args.selected_modes or [mode for mode, mode_cfg in modes.items() if mode_cfg[\"default\"]]\nbuild_modes =  {m: modes[m] for m in selected_modes}\n\nbuildfile_final_name = args.buildfile_final_name or args.buildfile\n\nif args.artifacts:\n    build_artifacts = set()\n    for artifact in args.artifacts:\n        if artifact in all_artifacts:\n            build_artifacts.add(artifact)\n        else:\n            print(\"Ignoring unknown build artifact: {}\".format(artifact))\n    if not build_artifacts:\n        print(\"No artifacts to build, exiting\")\n        exit(1)\nelse:\n    build_artifacts = all_artifacts\n\n\ndef generate_version(date_stamp):\n    date_stamp_opt = ''\n    if date_stamp:\n        date_stamp_opt = f'--date-stamp {date_stamp}'\n    status = subprocess.call(f\"./SCYLLA-VERSION-GEN --output-dir {outdir} {date_stamp_opt}\", shell=True)\n    if status != 0:\n        print('Version file generation failed')\n        sys.exit(1)\n\n    with open(f'{outdir}/SCYLLA-VERSION-FILE', 'r') as f:\n        scylla_version = f.read().strip().replace('-', '~')\n    with open(f'{outdir}/SCYLLA-RELEASE-FILE', 'r') as f:\n        scylla_release = f.read().strip()\n    with open(f'{outdir}/SCYLLA-PRODUCT-FILE', 'r') as f:\n        scylla_product = f.read().strip()\n    return scylla_product, scylla_version, scylla_release\n\n\n# The relocatable package includes its own dynamic linker. We don't\n# know the path it will be installed to, so for now use a very long\n# path so that patchelf doesn't need to edit the program headers.  The\n# kernel imposes a limit of 4096 bytes including the null. The other\n# constraint is that the build-id has to be in the first page, so we\n# can't use all 4096 bytes for the dynamic linker.\n# In here we just guess that 2000 extra / should be enough to cover\n# any path we get installed to but not so large that the build-id is\n# pushed to the second page.\n# At the end of the build we check that the build-id is indeed in the\n# first page. At install time we check that patchelf doesn't modify\n# the program headers.\ndef dynamic_linker_option():\n    gcc_linker_output = subprocess.check_output(['gcc', '-###', '/dev/null', '-o', 't'], stderr=subprocess.STDOUT).decode('utf-8')\n    original_dynamic_linker = re.search('-dynamic-linker ([^ ]*)', gcc_linker_output).groups()[0]\n\n    employ_ld_trickery = True\n    # distro-specific setup\n    if os.environ.get('NIX_CC'):\n        employ_ld_trickery = False\n\n    if employ_ld_trickery:\n        # gdb has a SO_NAME_MAX_PATH_SIZE of 512, so limit the path size to\n        # that. The 512 includes the null at the end, hence the 511 below.\n        dynamic_linker = '/' * (511 - len(original_dynamic_linker)) + original_dynamic_linker\n    else:\n        dynamic_linker = original_dynamic_linker\n    return f'--dynamic-linker={dynamic_linker}'\n\nforced_ldflags = '-Wl,'\n\n# The default build-id used by lld is xxhash, which is 8 bytes long, but RPM\n# requires build-ids to be at least 16 bytes long\n# (https://github.com/rpm-software-management/rpm/issues/950), so let's\n# explicitly ask for SHA1 build-ids.\nforced_ldflags += '--build-id=sha1,'\n\nforced_ldflags += dynamic_linker_option()\n\nuser_ldflags = forced_ldflags + ' ' + args.user_ldflags\n\ncurdir = os.getcwd()\nuser_cflags = args.user_cflags + f\" -ffile-prefix-map={curdir}=.\"\n\n# Since gcc 13, libgcc doesn't need the exception workaround\nuser_cflags += ' -DSEASTAR_NO_EXCEPTION_HACK'\n\nif args.target != '':\n    user_cflags += ' -march=' + args.target\n\nfor mode in modes:\n    # Those flags are passed not only to Scylla objects, but also to libraries\n    # that we compile ourselves.\n    modes[mode]['lib_cflags'] = user_cflags\n    modes[mode]['lib_ldflags'] = user_ldflags + linker_flags\n\n\ndef prepare_advanced_optimizations(*, modes, build_modes, args):\n    for mode in modes:\n        modes[mode]['has_lto'] = False\n        modes[mode]['is_profile'] = False\n\n    profile_modes = {}\n\n    for mode in modes:\n        if not modes[mode]['advanced_optimizations']:\n            continue\n\n        # When building with PGO, -Wbackend-plugin generates a warning for every\n        # function which changed its control flow graph since the profile was\n        # taken.\n        # We allow stale profiles, so these warnings are just noise to us.\n        # Let's silence them.\n        modes[mode]['lib_cflags'] += ' -Wno-backend-plugin'\n\n        if args.lto:\n            modes[mode]['has_lto'] = True\n            modes[mode]['lib_cflags'] += ' -flto=thin -ffat-lto-objects'\n\n        # Absolute path (in case of the initial profile) or path\n        # beginning with $builddir (in case of generated profiles),\n        # for use in ninja dependency rules.\n        # Using absolute paths only would work too, but we use\n        # $builddir for consistency with all other ninja targets.\n        profile_target = None\n        # Absolute path to the profile, for use in compiler flags.\n        # Can't use $builddir here because the flags are also passed\n        # to seastar, which doesn't understand ninja variables.\n        profile_path = None\n\n        if args.use_profile:\n            profile_path = os.path.abspath(args.use_profile)\n            profile_target = profile_path\n        elif args.use_profile is None:\n            # Use the default profile. There is a rule in later part of configure.py\n            # which extracts the default profile from an archive in pgo/profiles,\n            # (stored in git LFS) to build/\n\n            default_profile_archive_path = f\"pgo/profiles/{platform.machine()}/profile.profdata.xz\"\n            default_profile_filename = pathlib.Path(default_profile_archive_path).stem\n\n            # We are checking whether the profile archive is compressed,\n            # instead of just checking for its existence, because of how git LFS works.\n            #\n            # When a file is stored in LFS, the underlying git repository only receives a text file stub\n            # containing some metadata of the actual file. On checkout, LFS filters download the actual\n            # file based on that metadata and substitute it for the stub.\n            # If LFS is disabled or not installed, git will simply check out the stub,\n            # which will be a regular text file.\n            #\n            # By ignoring existing but uncompressed profile files we are accommodating users who don't\n            # have LFS installed yet, or don't want to be forced to use it.\n            #\n            validate_archive = subprocess.run([\"file\", default_profile_archive_path], capture_output=True)\n            if \"compressed data\" in validate_archive.stdout.decode():\n                default_profile_filename = pathlib.Path(default_profile_archive_path).stem\n                profile_path = os.path.abspath(\"build/\" + default_profile_filename)\n                profile_target = \"$builddir/\" + default_profile_filename\n                modes[mode].setdefault('profile_recipe', '')\n                modes[mode]['profile_recipe'] += textwrap.dedent(f\"\"\"\\\n                    rule xz_uncompress\n                        command = xz --uncompress --stdout $in > $out\n                        description = XZ_UNCOMPRESS $in to $out\n                    build {profile_target}: xz_uncompress {default_profile_archive_path}\n                    \"\"\")\n            else:\n                # Avoid breaking existing pipelines without git-lfs installed.\n                print(f\"WARNING: {default_profile_archive_path} is not an archive. Building without a profile.\", file=sys.stderr)\n        else:\n            # Passing --use-profile=\"\" explicitly disables the default profile.\n            pass\n\n        # pgso (profile-guided size-optimization) adds optsize hints (-Os) to cold code.\n        # We don't want to optimize anything for size, because that's a potential source\n        # of performance regressions, and the benefits are dubious. Let's disable pgso\n        # by default. (Currently is enabled in Clang by default.)\n        #\n        # Value profiling allows the compiler to track not only the outcomes of branches\n        # but also the values of variables at interesting decision points.\n        # Currently Clang uses value profiling for two things: specializing for the most\n        # common sizes of memory ops (e.g. memcpy, memcmp) and specializing for the most\n        # common targets of indirect branches.\n        # It's valuable in general, but our training suite is not realistic and exhaustive\n        # enough to be confident about value profiling. Let's also keep it disabled by\n        # default, conservatively. (Currently it is enabled in Clang by default.)\n        conservative_opts = \"\" if args.experimental_pgo else \"-mllvm -pgso=false -mllvm -enable-value-profiling=false\"\n\n        llvm_instr_types = []\n        if args.pgo:\n            llvm_instr_types += [\"\"]\n        if args.cspgo:\n            llvm_instr_types += [\"cs-\"]\n        for it in llvm_instr_types:\n            submode = copy.deepcopy(modes[mode])\n            submode_name = f'{mode}-{it}pgo'\n            submode['parent_mode'] = mode\n            if profile_path is not None:\n                submode['lib_cflags'] += f\" -fprofile-use={profile_path}\"\n                submode['cxx_ld_flags'] += f\" -fprofile-use={profile_path}\"\n                submode['profile_target'] = profile_target\n            submode['lib_cflags'] += f\" -f{it}profile-generate={os.path.realpath(outdir)}/{submode_name} {conservative_opts}\"\n            submode['cxx_ld_flags'] += f\" -f{it}profile-generate={os.path.realpath(outdir)}/{submode_name} {conservative_opts}\"\n            # Profile collection depends on java tools because we use cassandra-stress as the load.\n            submode['profile_recipe'] = textwrap.dedent(f\"\"\"\\\n                build $builddir/{submode_name}/profiles/prof.profdata: train $builddir/{submode_name}/scylla | dist-tools-tar\n                build $builddir/{submode_name}/profiles/merged.profdata: merge_profdata $builddir/{submode_name}/profiles/prof.profdata {profile_target or str()}\n                \"\"\")\n            submode['is_profile'] = True\n            profile_path = f\"{os.path.realpath(outdir)}/{submode_name}/profiles/merged.profdata\"\n            profile_target = f\"$builddir/{submode_name}/profiles/merged.profdata\"\n\n            profile_modes[submode_name] = submode\n\n        if profile_path is not None:\n            modes[mode]['lib_cflags'] += f\" -fprofile-use={profile_path} {conservative_opts}\"\n            modes[mode]['cxx_ld_flags'] += f\" -fprofile-use={profile_path} {conservative_opts}\"\n            modes[mode]['profile_target'] = profile_target\n            modes[mode].setdefault('profile_recipe', \"\")\n            modes[mode]['profile_recipe'] += textwrap.dedent(f\"\"\"\\\n                build $builddir/{mode}/profiles/merged.profdata: copy {profile_target or profile_path or str()}\n                \"\"\")\n\n    modes.update(profile_modes)\n    build_modes.update(profile_modes)\n\n\n# cmake likes to separate things with semicolons\ndef semicolon_separated(*flags):\n    # original flags may be space separated, so convert to string still\n    # using spaces\n    f = ' '.join(flags)\n    return re.sub(' +', ';', f)\n\ndef real_relpath(path, start):\n    return os.path.relpath(os.path.realpath(path), os.path.realpath(start))\n\ndef configure_seastar(build_dir, mode, mode_config):\n    seastar_cxx_ld_flags = mode_config['cxx_ld_flags']\n    # We want to \"undo\" coverage for seastar if we have it enabled.\n    if args.coverage:\n        for flag in COVERAGE_INST_FLAGS:\n            seastar_cxx_ld_flags = seastar_cxx_ld_flags.replace(' ' + flag, '')\n            seastar_cxx_ld_flags = seastar_cxx_ld_flags.replace(flag, '')\n    seastar_cmake_args = [\n        '-DCMAKE_BUILD_TYPE={}'.format(mode_config['cmake_build_type']),\n        '-DCMAKE_C_COMPILER={}'.format(args.cc),\n        '-DCMAKE_CXX_COMPILER={}'.format(args.cxx),\n        '-DCMAKE_EXPORT_NO_PACKAGE_REGISTRY=ON',\n        '-DCMAKE_CXX_STANDARD=23',\n        '-DCMAKE_CXX_EXTENSIONS=ON',\n        '-DSeastar_CXX_FLAGS=SHELL:{}'.format(mode_config['lib_cflags']),\n        '-DSeastar_LD_FLAGS={}'.format(semicolon_separated(mode_config['lib_ldflags'], seastar_cxx_ld_flags)),\n        '-DSeastar_API_LEVEL=7',\n        '-DSeastar_DEPRECATED_OSTREAM_FORMATTERS=OFF',\n        '-DSeastar_UNUSED_RESULT_ERROR=ON',\n        '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON',\n        '-DSeastar_SCHEDULING_GROUPS_COUNT=19',\n        '-DSeastar_IO_URING=ON',\n    ]\n\n    if args.stack_guards is not None:\n        stack_guards = 'ON' if args.stack_guards else 'OFF'\n        seastar_cmake_args += ['-DSeastar_STACK_GUARDS={}'.format(stack_guards)]\n\n    dpdk = args.dpdk\n    if dpdk is None:\n        dpdk = platform.machine() == 'x86_64' and mode == 'release'\n    if dpdk:\n        seastar_cmake_args += ['-DSeastar_DPDK=ON', '-DSeastar_DPDK_MACHINE=westmere']\n    if args.split_dwarf:\n        seastar_cmake_args += ['-DSeastar_SPLIT_DWARF=ON']\n    if args.alloc_failure_injector:\n        seastar_cmake_args += ['-DSeastar_ALLOC_FAILURE_INJECTION=ON']\n    if args.seastar_debug_allocations:\n        seastar_cmake_args += ['-DSeastar_DEBUG_ALLOCATIONS=ON']\n    if mode_config['build_seastar_shared_libs']:\n        seastar_cmake_args += ['-DBUILD_SHARED_LIBS=ON']\n\n    cmake_args = seastar_cmake_args[:]\n    seastar_build_dir = os.path.join(build_dir, mode, 'seastar')\n    seastar_cmd = ['cmake', '-G', 'Ninja', real_relpath(args.seastar_path, seastar_build_dir)] + cmake_args\n    cmake_dir = seastar_build_dir\n    if dpdk:\n        # need to cook first\n        cmake_dir = args.seastar_path # required by cooking.sh\n        relative_seastar_build_dir = os.path.join('..', seastar_build_dir)  # relative to seastar/\n        seastar_cmd = ['./cooking.sh', '-i', 'dpdk', '-d', relative_seastar_build_dir, '--'] + seastar_cmd[4:]\n\n    if args.verbose:\n        print(\" \\\\\\n  \".join(seastar_cmd))\n    os.makedirs(seastar_build_dir, exist_ok=True)\n    subprocess.check_call(seastar_cmd, shell=False, cwd=cmake_dir)\n\n\ndef configure_abseil(build_dir, mode, mode_config):\n    abseil_cflags = mode_config['lib_cflags']\n    cxx_flags = mode_config['cxxflags']\n    if '-DSANITIZE' in cxx_flags:\n        abseil_cflags += ' -fsanitize=address -fsanitize=undefined -fno-sanitize=vptr'\n\n    # We want to \"undo\" coverage for abseil if we have it enabled, as we are not\n    # interested in the coverage of the abseil library. these flags were previously\n    # added to cxx_ld_flags\n    if args.coverage:\n        for flag in COVERAGE_INST_FLAGS:\n            cxx_flags = cxx_flags.replace(f' {flag}', '')\n\n    cxx_flags += ' ' + abseil_cflags.strip()\n    cmake_mode = mode_config['cmake_build_type']\n    abseil_cmake_args = [\n        '-DCMAKE_BUILD_TYPE={}'.format(cmake_mode),\n        '-DCMAKE_INSTALL_PREFIX={}'.format(build_dir + '/inst'), # just to avoid a warning from absl\n        '-DCMAKE_C_COMPILER={}'.format(args.cc),\n        '-DCMAKE_CXX_COMPILER={}'.format(args.cxx),\n        '-DCMAKE_CXX_FLAGS_{}={}'.format(cmake_mode.upper(), cxx_flags),\n        '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON',\n        '-DCMAKE_CXX_STANDARD=23',\n        '-DABSL_PROPAGATE_CXX_STD=ON',\n    ]\n\n    cmake_args = abseil_cmake_args[:]\n    abseil_build_dir = os.path.join(build_dir, mode, 'abseil')\n    abseil_cmd = ['cmake', '-G', 'Ninja', real_relpath('abseil', abseil_build_dir)] + cmake_args\n\n    if args.verbose:\n        print(' \\\\\\n  '.join(abseil_cmd))\n    os.makedirs(abseil_build_dir, exist_ok=True)\n    subprocess.check_call(abseil_cmd, shell=False, cwd=abseil_build_dir)\n\nabseil_libs = ['absl/' + lib for lib in [\n    'container/libabsl_hashtablez_sampler.a',\n    'container/libabsl_raw_hash_set.a',\n    'synchronization/libabsl_synchronization.a',\n    'synchronization/libabsl_graphcycles_internal.a',\n    'debugging/libabsl_stacktrace.a',\n    'debugging/libabsl_symbolize.a',\n    'debugging/libabsl_debugging_internal.a',\n    'debugging/libabsl_demangle_internal.a',\n    'time/libabsl_time.a',\n    'time/libabsl_time_zone.a',\n    'numeric/libabsl_int128.a',\n    'hash/libabsl_hash.a',\n    'hash/libabsl_city.a',\n    'hash/libabsl_low_level_hash.a',\n    'base/libabsl_malloc_internal.a',\n    'base/libabsl_spinlock_wait.a',\n    'base/libabsl_base.a',\n    'base/libabsl_raw_logging_internal.a',\n    'profiling/libabsl_exponential_biased.a',\n    'strings/libabsl_strings.a',\n    'strings/libabsl_strings_internal.a',\n    'base/libabsl_throw_delegate.a']]\n\n\ndef query_seastar_flags(pc_file, use_shared_libs, link_static_cxx=False):\n    if use_shared_libs:\n        opt = '--shared'\n    else:\n        opt = '--static'\n    cflags = pkg_config(pc_file, '--cflags', opt)\n    libs = pkg_config(pc_file, '--libs', opt)\n    if use_shared_libs:\n        rpath = os.path.dirname(libs.split()[0])\n        libs = f\"-Wl,-rpath='{rpath}' {libs}\"\n    if link_static_cxx:\n        libs = libs.replace('-lstdc++ ', '')\n\n    testing_libs = pkg_config(pc_file.replace('seastar.pc', 'seastar-testing.pc'), '--libs', '--static')\n    return {'seastar_cflags': cflags,\n            'seastar_libs': libs,\n            'seastar_testing_libs': testing_libs}\n\npkgs = ['libsystemd',\n        'jsoncpp']\n# Lua can be provided by lua53 package on Debian-like\n# systems and by Lua on others.\npkgs.append('lua53' if have_pkg('lua53') else 'lua')\n\n\nlibs = ' '.join([maybe_static(args.staticyamlcpp, '-lyaml-cpp'), '-latomic', '-lz', '-lsnappy',\n                 ' -lstdc++fs', ' -lcrypt', ' -lcryptopp', ' -lpthread',\n                 # Must link with static version of libzstd, since\n                 # experimental APIs that we use are only present there.\n                 maybe_static(True, '-lzstd'),\n                 maybe_static(True, '-llz4'),\n                 maybe_static(args.staticboost, '-lboost_date_time -lboost_regex -licuuc -licui18n'),\n                 '-lxxhash',\n                 '-ldeflate',\n                ])\n\nif not args.staticboost:\n    user_cflags += ' -DBOOST_ALL_DYN_LINK'\n\nfor pkg in pkgs:\n    user_cflags += ' ' + pkg_config(pkg, '--cflags')\n    libs += ' ' + pkg_config(pkg, '--libs')\nuser_cflags += ' -fvisibility=hidden'\nuser_ldflags += ' -fvisibility=hidden'\nif args.staticcxx:\n    user_ldflags += \" -static-libstdc++\"\n\n\ndef get_extra_cxxflags(mode, mode_config, cxx, debuginfo):\n    cxxflags = []\n\n    optimization_level = mode_config['optimization-level']\n    cxxflags.append(f'-O{optimization_level}')\n\n    if mode == 'release':\n        optimization_flags = [\n            '--param inline-unit-growth=300', # gcc\n            f'-mllvm -inline-threshold={get_clang_inline_threshold()}',  # clang\n            # clang generates 16-byte loads that break store-to-load forwarding\n            # gcc also has some trouble: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103554\n            '-fno-slp-vectorize',\n        ]\n        optimization_flags = [o\n                              for o in optimization_flags\n                              if flag_supported(flag=o, compiler=cxx)]\n        cxxflags += optimization_flags\n\n    if flag_supported(flag='-Wstack-usage=4096', compiler=cxx):\n        stack_usage_threshold = mode_config['stack-usage-threshold']\n        cxxflags += [f'-Wstack-usage={stack_usage_threshold}',\n                     '-Wno-error=stack-usage=']\n\n    cxxflags.append(f'-DSCYLLA_BUILD_MODE={mode}')\n\n    if debuginfo and mode_config['can_have_debug_info']:\n        cxxflags += ['-g', '-gz']\n\n    # Since AssignmentTracking was enabled by default in clang\n    # (llvm/llvm-project@de6da6ad55d3ca945195d1cb109cb8efdf40a52a)\n    # coroutine frame debugging info (`coro_frame_ty`) is broken.\n    # \n    # It seems that we aren't losing much by disabling AssigmentTracking,\n    # so for now we choose to disable it to get `coro_frame_ty` back.\n    cxxflags.append('-Xclang -fexperimental-assignment-tracking=disabled')\n\n    return cxxflags\n\n\ndef get_release_cxxflags(scylla_product,\n                         scylla_version,\n                         scylla_release):\n    definitions = {'SCYLLA_PRODUCT': scylla_product,\n                   'SCYLLA_VERSION': scylla_version,\n                   'SCYLLA_RELEASE': scylla_release}\n    return [f'-D{name}=\"\\\\\"{value}\\\\\"\"' for name, value in definitions.items()]\n\n\ndef write_build_file(f,\n                     arch,\n                     ninja,\n                     scylla_product,\n                     scylla_version,\n                     scylla_release,\n                     args):\n    warnings = get_warning_options(args.cxx)\n    f.write(textwrap.dedent('''\\\n        configure_args = {configure_args}\n        builddir = {outdir}\n        cxx = {cxx}\n        cxxflags = -std=gnu++23 {user_cflags} {warnings} {defines}\n        ldflags = {linker_flags} {user_ldflags}\n        ldflags_build = {linker_flags}\n        libs = {libs}\n        pool link_pool\n            depth = {link_pool_depth}\n        pool submodule_pool\n            depth = 1\n        rule gen\n            command = echo -e $text > $out\n            description = GEN $out\n        rule swagger\n            command = {seastar_path}/scripts/seastar-json2code.py --create-cc -f $in -o $out\n            description = SWAGGER $out\n        rule serializer\n            command = ./idl-compiler.py --ns ser -f $in -o $out\n            description = IDL compiler $out\n        rule ninja\n            command = {ninja} -C $subdir $target\n            restat = 1\n            description = NINJA $out\n        rule ragel\n            # sed away a bug in ragel 7 that emits some extraneous _nfa* variables\n            # (the $$ is collapsed to a single one by ninja)\n            command = {ragel_exec} -G2 -o $out $in && sed -i -e '1h;2,$$H;$$!d;g' -re 's/static const char _nfa[^;]*;//g' $out\n            description = RAGEL $out\n        rule run\n            command = $in > $out\n            description = GEN $out\n        rule copy\n            command = cp --reflink=auto $in $out\n            description = COPY $out\n        rule strip\n            command = scripts/strip.sh $in\n        rule package\n            command = scripts/create-relocatable-package.py --build-dir $builddir/$mode --node-exporter-dir $builddir/node_exporter --debian-dir $builddir/debian/debian $out\n        rule stripped_package\n            command = scripts/create-relocatable-package.py --stripped --build-dir $builddir/$mode --node-exporter-dir $builddir/node_exporter --debian-dir $builddir/debian/debian $out\n        rule debuginfo_package\n            command = dist/debuginfo/scripts/create-relocatable-package.py --build-dir $builddir/$mode --node-exporter-dir $builddir/node_exporter $out\n        rule rpmbuild\n            command = reloc/build_rpm.sh --reloc-pkg $in --builddir $out\n        rule debbuild\n            command = reloc/build_deb.sh --reloc-pkg $in --builddir $out\n        rule unified\n            command = unified/build_unified.sh --build-dir $builddir/$mode --unified-pkg $out\n        rule rust_header\n            command = cxxbridge --include rust/cxx.h --header $in > $out\n            description = RUST_HEADER $out\n        rule rust_source\n            command = cxxbridge --include rust/cxx.h $in > $out\n            description = RUST_SOURCE $out\n        rule cxxbridge_header\n            command = cxxbridge --header > $out\n        rule c2wasm\n            command = clang --target=wasm32 --no-standard-libraries -Wl,--export-all -Wl,--no-entry $in -o $out\n            description = C2WASM $out\n        rule rust2wasm\n            command = cargo build --target=wasm32-wasi --example=$example --locked --manifest-path=test/resource/wasm/rust/Cargo.toml --target-dir=$builddir/wasm/ $\n                && wasm-opt -Oz $builddir/wasm/wasm32-wasi/debug/examples/$example.wasm -o $builddir/wasm/$example.wasm $\n                && wasm-strip $builddir/wasm/$example.wasm\n            description = RUST2WASM $out\n        rule wasm2wat\n            command = wasm2wat $in > $out\n            description = WASM2WAT $out\n        rule run_profile\n          command = rm -r `dirname $out` && pgo/run_all $in `dirname $out` $type\n        rule train\n          command = rm -r `dirname $out` && pgo/train `realpath $in` `realpath -m $out` `realpath -m $builddir/pgo_datasets`\n          pool = console\n        rule merge_profdata\n          command = llvm-profdata merge $in -output=$out\n        ''').format(configure_args=configure_args,\n                    outdir=outdir,\n                    cxx=args.cxx,\n                    user_cflags=user_cflags,\n                    warnings=warnings,\n                    defines=defines,\n                    linker_flags=linker_flags,\n                    user_ldflags=user_ldflags,\n                    libs=libs,\n                    link_pool_depth=link_pool_depth,\n                    seastar_path=args.seastar_path,\n                    ninja=ninja,\n                    ragel_exec=args.ragel_exec))\n\n    for binary in sorted(wasms):\n        src = wasm_deps[binary]\n        wasm = binary[:-4] + '.wasm'\n        if src.endswith('.rs'):\n            f.write(f'build $builddir/{wasm}: rust2wasm {src} | test/resource/wasm/rust/Cargo.lock test/resource/wasm/rust/build.rs\\n')\n            example_name = binary[binary.rindex('/')+1:-4]\n            f.write(f'   example = {example_name}\\n')\n        else:\n            f.write(f'build $builddir/{wasm}: c2wasm {src}\\n')\n        f.write(f'build $builddir/{binary}: wasm2wat $builddir/{wasm}\\n')\n\n    for mode in build_modes:\n        modeval = modes[mode]\n\n        fmt_lib = 'fmt'\n        f.write(textwrap.dedent('''\\\n            cxx_ld_flags_{mode} = {cxx_ld_flags}\n            ld_flags_{mode} = $cxx_ld_flags_{mode} {lib_ldflags}\n            cxxflags_{mode} = {lib_cflags} {cxxflags} -iquote. -iquote $builddir/{mode}/gen\n            libs_{mode} = -l{fmt_lib}\n            seastar_libs_{mode} = {seastar_libs}\n            seastar_testing_libs_{mode} = {seastar_testing_libs}\n            rule cxx.{mode}\n              command = $cxx -MD -MT $out -MF $out.d {seastar_cflags} $cxxflags_{mode} $cxxflags $obj_cxxflags -c -o $out $in\n              description = CXX $out\n              depfile = $out.d\n            rule link.{mode}\n              command = $cxx  $ld_flags_{mode} $ldflags -o $out $in $libs $libs_{mode}\n              description = LINK $out\n              pool = link_pool\n            rule link_stripped.{mode}\n              command = $cxx  $ld_flags_{mode} -s $ldflags -o $out $in $libs $libs_{mode}\n              description = LINK (stripped) $out\n              pool = link_pool\n            rule link_build.{mode}\n              command = $cxx  $ld_flags_{mode} $ldflags_build -o $out $in $libs $libs_{mode}\n              description = LINK (build) $out\n              pool = link_pool\n            rule ar.{mode}\n              command = rm -f $out; ar cr $out $in; ranlib $out\n              description = AR $out\n            rule antlr3.{mode}\n                # We replace many local `ExceptionBaseType* ex` variables with a single function-scope one.\n                # Because we add such a variable to every function, and because `ExceptionBaseType` is not a global\n                # name, we also add a global typedef to avoid compilation errors.\n                command = sed -e '/^#if 0/,/^#endif/d' $in > $builddir/{mode}/gen/$in $\n                     && {antlr3_exec} $builddir/{mode}/gen/$in $\n                     && sed -i -e '/^.*On :.*$$/d' $builddir/{mode}/gen/${{stem}}Lexer.hpp $\n                     && sed -i -e '/^.*On :.*$$/d' $builddir/{mode}/gen/${{stem}}Lexer.cpp $\n                     && sed -i -e '/^.*On :.*$$/d' $builddir/{mode}/gen/${{stem}}Parser.hpp $\n                     && sed -i -e 's/^\\\\( *\\\\)\\\\(ImplTraits::CommonTokenType\\\\* [a-zA-Z0-9_]* = NULL;\\\\)$$/\\\\1const \\\\2/' $\n                        -e '/^.*On :.*$$/d' $\n                        -e '1i using ExceptionBaseType = int;' $\n                        -e 's/^{{/{{ ExceptionBaseType\\\\* ex = nullptr;/; $\n                            s/ExceptionBaseType\\\\* ex = new/ex = new/; $\n                            s/exceptions::syntax_exception e/exceptions::syntax_exception\\\\& e/' $\n                        $builddir/{mode}/gen/${{stem}}Parser.cpp\n                description = ANTLR3 $in\n            rule checkhh.{mode}\n              command = $cxx -MD -MT $out -MF $out.d {seastar_cflags} $cxxflags $cxxflags_{mode} $obj_cxxflags --include $in -c -o $out $builddir/{mode}/gen/empty.cc\n              description = CHECKHH $in\n              depfile = $out.d\n            rule test.{mode}\n              command = ./test.py --mode={mode} --repeat={test_repeat} --timeout={test_timeout}\n              pool = console\n              description = TEST {mode}\n            # This rule is unused for PGO stages. They use the rust lib from the parent mode.\n            rule rust_lib.{mode}\n              command = CARGO_BUILD_DEP_INFO_BASEDIR='.' cargo build --locked --manifest-path=rust/Cargo.toml --target-dir=$builddir/{mode} --profile=rust-{mode} $\n                        && touch $out\n              description = RUST_LIB $out\n            ''').format(mode=mode, antlr3_exec=args.antlr3_exec, fmt_lib=fmt_lib, test_repeat=args.test_repeat, test_timeout=args.test_timeout, **modeval))\n        f.write(\n            'build {mode}-build: phony {artifacts} {wasms}\\n'.format(\n                mode=mode,\n                artifacts=str.join(' ', ['$builddir/' + mode + '/' + x for x in sorted(build_artifacts - wasms)]),\n                wasms = str.join(' ', ['$builddir/' + x for x in sorted(build_artifacts & wasms)]),\n            )\n        )\n        if profile_recipe := modes[mode].get('profile_recipe'):\n            f.write(profile_recipe)\n        include_cxx_target = f'{mode}-build' if not args.dist_only else ''\n        include_dist_target = f'dist-{mode}' if args.enable_dist is None or args.enable_dist else ''\n        f.write(f'build {mode}: phony {include_cxx_target} {include_dist_target}\\n')\n        compiles = {}\n        swaggers = set()\n        serializers = {}\n        ragels = {}\n        antlr3_grammars = set()\n        rust_headers = {}\n\n        # We want LTO, but with the regular LTO, clang generates special LLVM IR files instead of\n        # regular ELF objects after the compile phase, and these special LLVM bitcode can only be\n        # used for LTO builds. The cost of compiling all tests with LTO is prohibitively high, so\n        # we can't use these IR files for tests -- we need to compile regular ELF objects as well.\n        # Therefore, we build FatLTO objects, which contain LTO compatible IR and the regular\n        # object code. And we enable LTO when linking the main Scylla executable, while disable\n        # it when linking anything else.\n\n        seastar_lib_ext = 'so' if modeval['build_seastar_shared_libs'] else 'a'\n        for binary in sorted(build_artifacts):\n            if modeval['is_profile'] and binary != \"scylla\":\n                # Just to avoid clutter in build.ninja\n                continue\n            profile_dep = modes[mode].get('profile_target', \"\")\n\n            if binary in other or binary in wasms:\n                continue\n            srcs = deps[binary]\n            objs = ['$builddir/' + mode + '/' + src.replace('.cc', '.o')\n                    for src in srcs\n                    if src.endswith('.cc')]\n            objs.append('$builddir/../utils/arch/powerpc/crc32-vpmsum/crc32.S')\n            has_rust = False\n            for dep in deps[binary]:\n                if isinstance(dep, Antlr3Grammar):\n                    objs += dep.objects(f'$builddir/{mode}/gen')\n                if isinstance(dep, Json2Code):\n                    objs += dep.objects(f'$builddir/{mode}/gen')\n                if dep.endswith('.rs'):\n                    has_rust = True\n                    idx = dep.rindex('/src/')\n                    obj = dep[:idx].replace('rust/','') + '.o'\n                    objs.append(f'$builddir/{mode}/gen/rust/{obj}')\n            if has_rust:\n                parent_mode = modes[mode].get('parent_mode', mode)\n                objs.append(f'$builddir/{parent_mode}/rust-{parent_mode}/librust_combined.a')\n\n            do_lto = modes[mode]['has_lto'] and binary in lto_binaries\n            seastar_dep = f'$builddir/{mode}/seastar/libseastar.{seastar_lib_ext}'\n            seastar_testing_dep = f'$builddir/{mode}/seastar/libseastar_testing.{seastar_lib_ext}'\n            abseil_dep = ' '.join(f'$builddir/{mode}/abseil/{lib}' for lib in abseil_libs)\n            seastar_testing_libs = f'$seastar_testing_libs_{mode}'\n\n            local_libs = f'$seastar_libs_{mode} $libs'\n            objs.extend([f'$builddir/{mode}/abseil/{lib}' for lib in abseil_libs])\n\n            if do_lto:\n                local_libs += ' -flto=thin -ffat-lto-objects'\n            else:\n                local_libs += ' -fno-lto'\n            if binary in tests:\n                if binary in pure_boost_tests:\n                    local_libs += ' ' + maybe_static(args.staticboost, '-lboost_unit_test_framework')\n                if binary not in tests_not_using_seastar_test_framework:\n                    local_libs += f' {seastar_testing_libs}'\n                else:\n                    local_libs += ' ' + '-lgnutls' + ' ' + '-lboost_unit_test_framework'\n                # Our code's debugging information is huge, and multiplied\n                # by many tests yields ridiculous amounts of disk space.\n                # So we strip the tests by default; The user can very\n                # quickly re-link the test unstripped by adding a \"_g\"\n                # to the test name, e.g., \"ninja build/release/testname_g\"\n                link_rule = perf_tests_link_rule if binary.startswith('test/perf/') else tests_link_rule\n                f.write('build $builddir/{}/{}: {}.{} {} | {} {} {}\\n'.format(mode, binary, link_rule, mode, str.join(' ', objs), seastar_dep, seastar_testing_dep, abseil_dep))\n                f.write('   libs = {}\\n'.format(local_libs))\n                f.write('build $builddir/{}/{}_g: {}.{} {} | {} {} {}\\n'.format(mode, binary, regular_link_rule, mode, str.join(' ', objs), seastar_dep, seastar_testing_dep, abseil_dep))\n                f.write('   libs = {}\\n'.format(local_libs))\n            else:\n                if binary == 'scylla':\n                    local_libs += f' {seastar_testing_libs}'\n                f.write('build $builddir/{}/{}: {}.{} {} | {} {} {}\\n'.format(mode, binary, regular_link_rule, mode, str.join(' ', objs), seastar_dep, seastar_testing_dep, abseil_dep))\n                f.write('   libs = {}\\n'.format(local_libs))\n                f.write(f'build $builddir/{mode}/{binary}.stripped: strip $builddir/{mode}/{binary}\\n')\n                f.write(f'build $builddir/{mode}/{binary}.debug: phony $builddir/{mode}/{binary}.stripped\\n')\n            for src in srcs:\n                if src.endswith('.cc'):\n                    obj = '$builddir/' + mode + '/' + src.replace('.cc', '.o')\n                    compiles[obj] = src\n                elif src.endswith('.idl.hh'):\n                    hh = '$builddir/' + mode + '/gen/' + src.replace('.idl.hh', '.dist.hh')\n                    serializers[hh] = src\n                elif src.endswith('.json'):\n                    swaggers.add(src)\n                elif src.endswith('.rl'):\n                    hh = '$builddir/' + mode + '/gen/' + src.replace('.rl', '.hh')\n                    ragels[hh] = src\n                elif src.endswith('.g'):\n                    antlr3_grammars.add(src)\n                elif src.endswith('.rs'):\n                    idx = src.rindex('/src/')\n                    hh = '$builddir/' + mode + '/gen/' + src[:idx] + '.hh'\n                    rust_headers[hh] = src\n                else:\n                    raise Exception('No rule for ' + src)\n        f.write(\n            'build {mode}-objects: phony {objs}\\n'.format(\n                mode=mode,\n                objs=' '.join(compiles)\n            )\n        )\n\n        headers = find_headers('.', excluded_dirs=['idl', 'build', 'seastar', '.git'])\n        f.write(\n            'build {mode}-headers: phony {header_objs}\\n'.format(\n                mode=mode,\n                header_objs=' '.join([\"$builddir/{mode}/{hh}.o\".format(mode=mode, hh=hh) for hh in headers])\n            )\n        )\n\n        f.write(\n            'build {mode}-test: test.{mode} {test_executables} $builddir/{mode}/scylla {wasms}\\n'.format(\n                mode=mode,\n                test_executables=' '.join(['$builddir/{}/{}'.format(mode, binary) for binary in sorted(tests)]),\n                wasms=' '.join([f'$builddir/{binary}' for binary in sorted(wasms)]),\n            )\n        )\n        f.write(\n            'build {mode}-check: phony {mode}-headers {mode}-test\\n'.format(\n                mode=mode,\n            )\n        )\n        compiler_training_artifacts=[]\n        if mode == 'dev':\n            compiler_training_artifacts.append(f'$builddir/{mode}/scylla')\n        elif mode == 'release' or mode == 'debug':\n            compiler_training_artifacts.append(f'$builddir/{mode}/service/storage_proxy.o')\n        f.write(\n            'build {mode}-compiler-training: phony {artifacts}\\n'.format(\n                mode=mode,\n                artifacts=str.join(' ', compiler_training_artifacts)\n            )\n        )\n\n        gen_dir = '$builddir/{}/gen'.format(mode)\n        gen_headers = []\n        for g in antlr3_grammars:\n            gen_headers += g.headers('$builddir/{}/gen'.format(mode))\n        for g in swaggers:\n            gen_headers += g.headers('$builddir/{}/gen'.format(mode))\n        gen_headers += list(serializers.keys())\n        gen_headers += list(ragels.keys())\n        gen_headers += list(rust_headers.keys())\n        gen_headers.append('$builddir/{}/gen/rust/cxx.h'.format(mode))\n        gen_headers_dep = ' '.join(gen_headers)\n\n        for hh in rust_headers:\n            src = rust_headers[hh]\n            f.write('build {}: rust_header {}\\n'.format(hh, src))\n            cc = hh.replace('.hh', '.cc')\n            f.write('build {}: rust_source {}\\n'.format(cc, src))\n            obj = cc.replace('.cc', '.o')\n            compiles[obj] = cc\n        for obj in compiles:\n            src = compiles[obj]\n            seastar_dep = f'$builddir/{mode}/seastar/libseastar.{seastar_lib_ext}'\n            abseil_dep = ' '.join(f'$builddir/{mode}/abseil/{lib}' for lib in abseil_libs)\n            f.write(f'build {obj}: cxx.{mode} {src} | {profile_dep} || {seastar_dep} {abseil_dep} {gen_headers_dep}\\n')\n            if src in modeval['per_src_extra_cxxflags']:\n                f.write('    cxxflags = {seastar_cflags} $cxxflags $cxxflags_{mode} {extra_cxxflags}\\n'.format(mode=mode, extra_cxxflags=modeval[\"per_src_extra_cxxflags\"][src], **modeval))\n        for swagger in swaggers:\n            hh = swagger.headers(gen_dir)[0]\n            cc = swagger.sources(gen_dir)[0]\n            obj = swagger.objects(gen_dir)[0]\n            src = swagger.source\n            f.write('build {} | {} : swagger {} | {}/scripts/seastar-json2code.py\\n'.format(hh, cc, src, args.seastar_path))\n            f.write(f'build {obj}: cxx.{mode} {cc} | {profile_dep}\\n')\n        for hh in serializers:\n            src = serializers[hh]\n            f.write('build {}: serializer {} | idl-compiler.py\\n'.format(hh, src))\n        for hh in ragels:\n            src = ragels[hh]\n            f.write('build {}: ragel {}\\n'.format(hh, src))\n        f.write('build {}: cxxbridge_header\\n'.format('$builddir/{}/gen/rust/cxx.h'.format(mode)))\n        if 'parent_mode' not in modes[mode]:\n            librust = '$builddir/{}/rust-{}/librust_combined'.format(mode, mode)\n            f.write('build {}.a: rust_lib.{} rust/Cargo.lock\\n  depfile={}.d\\n'.format(librust, mode, librust))\n        for grammar in antlr3_grammars:\n            outs = ' '.join(grammar.generated('$builddir/{}/gen'.format(mode)))\n            f.write('build {}: antlr3.{} {}\\n  stem = {}\\n'.format(outs, mode, grammar.source,\n                                                                   grammar.source.rsplit('.', 1)[0]))\n            for cc in grammar.sources('$builddir/{}/gen'.format(mode)):\n                obj = cc.replace('.cpp', '.o')\n                f.write(f'build {obj}: cxx.{mode} {cc} | {profile_dep} || {\" \".join(serializers)}\\n')\n                flags = '-Wno-parentheses-equality'\n                if cc.endswith('Parser.cpp'):\n                    # Unoptimized parsers end up using huge amounts of stack space and overflowing their stack\n                    flags += ' -O1' if modes[mode]['optimization-level'] in ['0', 'g', 's'] else ''\n\n                    if '-DSANITIZE' in modeval['cxxflags'] and has_sanitize_address_use_after_scope:\n                        flags += ' -fno-sanitize-address-use-after-scope'\n                f.write('  obj_cxxflags = %s\\n' % flags)\n        f.write(f'build $builddir/{mode}/gen/empty.cc: gen\\n')\n        for hh in headers:\n            f.write('build $builddir/{mode}/{hh}.o: checkhh.{mode} {hh} | $builddir/{mode}/gen/empty.cc {profile_dep} || {gen_headers_dep}\\n'.format(\n                    mode=mode, hh=hh, gen_headers_dep=gen_headers_dep, profile_dep=profile_dep))\n\n        seastar_dep = f'$builddir/{mode}/seastar/libseastar.{seastar_lib_ext}'\n        seastar_testing_dep = f'$builddir/{mode}/seastar/libseastar_testing.{seastar_lib_ext}'\n        f.write('build {seastar_dep}: ninja $builddir/{mode}/seastar/build.ninja | always {profile_dep}\\n'\n                .format(**locals()))\n        f.write('  pool = submodule_pool\\n')\n        f.write('  subdir = $builddir/{mode}/seastar\\n'.format(**locals()))\n        f.write('  target = seastar\\n'.format(**locals()))\n        f.write('build {seastar_testing_dep}: ninja $builddir/{mode}/seastar/build.ninja | always {profile_dep}\\n'\n                .format(**locals()))\n        f.write('  pool = submodule_pool\\n')\n        f.write('  subdir = $builddir/{mode}/seastar\\n'.format(**locals()))\n        f.write('  target = seastar_testing\\n'.format(**locals()))\n        f.write('  profile_dep = {profile_dep}\\n'.format(**locals()))\n\n        for lib in abseil_libs:\n            f.write('build $builddir/{mode}/abseil/{lib}: ninja $builddir/{mode}/abseil/build.ninja | always {profile_dep}\\n'.format(**locals()))\n            f.write('  pool = submodule_pool\\n')\n            f.write('  subdir = $builddir/{mode}/abseil\\n'.format(**locals()))\n            f.write('  target = {lib}\\n'.format(**locals()))\n            f.write('  profile_dep = {profile_dep}\\n'.format(**locals()))\n\n        f.write('build $builddir/{mode}/seastar/apps/iotune/iotune: ninja $builddir/{mode}/seastar/build.ninja | $builddir/{mode}/seastar/libseastar.{seastar_lib_ext}\\n'\n                .format(**locals()))\n        f.write('  pool = submodule_pool\\n')\n        f.write('  subdir = $builddir/{mode}/seastar\\n'.format(**locals()))\n        f.write('  target = iotune\\n'.format(**locals()))\n        f.write('  profile_dep = {profile_dep}\\n'.format(**locals()))\n        f.write(textwrap.dedent('''\\\n            build $builddir/{mode}/iotune: copy $builddir/{mode}/seastar/apps/iotune/iotune\n            build $builddir/{mode}/iotune.stripped: strip $builddir/{mode}/iotune\n            build $builddir/{mode}/iotune.debug: phony $builddir/{mode}/iotune.stripped\n            ''').format(**locals()))\n        if args.dist_only:\n            include_scylla_and_iotune = ''\n            include_scylla_and_iotune_stripped = ''\n            include_scylla_and_iotune_debug = ''\n        else:\n            include_scylla_and_iotune = f'$builddir/{mode}/scylla $builddir/{mode}/iotune'\n            include_scylla_and_iotune_stripped = f'$builddir/{mode}/scylla.stripped $builddir/{mode}/iotune.stripped'\n            include_scylla_and_iotune_debug = f'$builddir/{mode}/scylla.debug $builddir/{mode}/iotune.debug'\n        f.write('build $builddir/{mode}/dist/tar/{scylla_product}-unstripped-{scylla_version}-{scylla_release}.{arch}.tar.gz: package {include_scylla_and_iotune} $builddir/SCYLLA-RELEASE-FILE $builddir/SCYLLA-VERSION-FILE $builddir/debian/debian $builddir/node_exporter/node_exporter | always\\n'.format(**locals()))\n        f.write('  mode = {mode}\\n'.format(**locals()))\n        f.write('build $builddir/{mode}/dist/tar/{scylla_product}-{scylla_version}-{scylla_release}.{arch}.tar.gz: stripped_package {include_scylla_and_iotune_stripped} $builddir/SCYLLA-RELEASE-FILE $builddir/SCYLLA-VERSION-FILE $builddir/debian/debian $builddir/node_exporter/node_exporter.stripped | always\\n'.format(**locals()))\n        f.write('  mode = {mode}\\n'.format(**locals()))\n        f.write('build $builddir/{mode}/dist/tar/{scylla_product}-debuginfo-{scylla_version}-{scylla_release}.{arch}.tar.gz: debuginfo_package {include_scylla_and_iotune_debug} $builddir/SCYLLA-RELEASE-FILE $builddir/SCYLLA-VERSION-FILE $builddir/debian/debian $builddir/node_exporter/node_exporter.debug | always\\n'.format(**locals()))\n        f.write('  mode = {mode}\\n'.format(**locals()))\n        f.write('build $builddir/{mode}/dist/tar/{scylla_product}-package.tar.gz: copy $builddir/{mode}/dist/tar/{scylla_product}-{scylla_version}-{scylla_release}.{arch}.tar.gz\\n'.format(**locals()))\n        f.write('  mode = {mode}\\n'.format(**locals()))\n        f.write('build $builddir/{mode}/dist/tar/{scylla_product}-{arch}-package.tar.gz: copy $builddir/{mode}/dist/tar/{scylla_product}-{scylla_version}-{scylla_release}.{arch}.tar.gz\\n'.format(**locals()))\n        f.write('  mode = {mode}\\n'.format(**locals()))\n\n        f.write(f'build $builddir/dist/{mode}/redhat: rpmbuild $builddir/{mode}/dist/tar/{scylla_product}-unstripped-{scylla_version}-{scylla_release}.{arch}.tar.gz\\n')\n        f.write(f'  mode = {mode}\\n')\n        f.write(f'build $builddir/dist/{mode}/debian: debbuild $builddir/{mode}/dist/tar/{scylla_product}-unstripped-{scylla_version}-{scylla_release}.{arch}.tar.gz\\n')\n        f.write(f'  mode = {mode}\\n')\n        f.write(f'build dist-server-{mode}: phony $builddir/dist/{mode}/redhat $builddir/dist/{mode}/debian\\n')\n        f.write(f'build dist-server-debuginfo-{mode}: phony $builddir/{mode}/dist/tar/{scylla_product}-debuginfo-{scylla_version}-{scylla_release}.{arch}.tar.gz\\n')\n        f.write(f'build dist-tools-{mode}: phony $builddir/{mode}/dist/tar/{scylla_product}-tools-{scylla_version}-{scylla_release}.noarch.tar.gz dist-tools-rpm dist-tools-deb\\n')\n        f.write(f'build dist-cqlsh-{mode}: phony $builddir/{mode}/dist/tar/{scylla_product}-cqlsh-{scylla_version}-{scylla_release}.{arch}.tar.gz dist-cqlsh-rpm dist-cqlsh-deb\\n')\n        f.write(f'build dist-python3-{mode}: phony dist-python3-tar dist-python3-rpm dist-python3-deb\\n')\n        f.write(f'build dist-unified-{mode}: phony $builddir/{mode}/dist/tar/{scylla_product}-unified-{scylla_version}-{scylla_release}.{arch}.tar.gz\\n')\n        f.write(f'build $builddir/{mode}/dist/tar/{scylla_product}-unified-{scylla_version}-{scylla_release}.{arch}.tar.gz: unified $builddir/{mode}/dist/tar/{scylla_product}-{scylla_version}-{scylla_release}.{arch}.tar.gz $builddir/{mode}/dist/tar/{scylla_product}-python3-{scylla_version}-{scylla_release}.{arch}.tar.gz $builddir/{mode}/dist/tar/{scylla_product}-tools-{scylla_version}-{scylla_release}.noarch.tar.gz $builddir/{mode}/dist/tar/{scylla_product}-cqlsh-{scylla_version}-{scylla_release}.{arch}.tar.gz | always\\n')\n        f.write(f'  mode = {mode}\\n')\n        f.write(f'build $builddir/{mode}/dist/tar/{scylla_product}-unified-package-{scylla_version}-{scylla_release}.tar.gz: copy $builddir/{mode}/dist/tar/{scylla_product}-unified-{scylla_version}-{scylla_release}.{arch}.tar.gz\\n')\n        f.write(f'build $builddir/{mode}/dist/tar/{scylla_product}-unified-{arch}-package-{scylla_version}-{scylla_release}.tar.gz: copy $builddir/{mode}/dist/tar/{scylla_product}-unified-{scylla_version}-{scylla_release}.{arch}.tar.gz\\n')\n\n    checkheaders_mode = 'dev' if 'dev' in modes else modes.keys()[0]\n    f.write('build checkheaders: phony || {}\\n'.format(' '.join(['$builddir/{}/{}.o'.format(checkheaders_mode, hh) for hh in headers])))\n\n    f.write(\n            'build build: phony {}\\n'.format(' '.join([f'{mode}-build' for mode in default_modes]))\n    )\n    f.write(\n            'build test: phony {}\\n'.format(' '.join(['{mode}-test'.format(mode=mode) for mode in default_modes]))\n    )\n    f.write(\n            'build check: phony {}\\n'.format(' '.join(['{mode}-check'.format(mode=mode) for mode in default_modes]))\n    )\n    f.write(\n            'build wasm: phony {}\\n'.format(' '.join([f'$builddir/{binary}' for binary in sorted(wasms)]))\n    )\n    f.write(\n            'build compiler-training: phony {}\\n'.format(' '.join(['{mode}-compiler-training'.format(mode=mode) for mode in default_modes]))\n    )\n\n    f.write(textwrap.dedent(f'''\\\n        build dist-unified-tar: phony {' '.join([f'$builddir/{mode}/dist/tar/{scylla_product}-unified-{scylla_version}-{scylla_release}.{arch}.tar.gz' for mode in default_modes])}\n        build dist-unified: phony dist-unified-tar\n\n        build dist-server-deb: phony {' '.join(['$builddir/dist/{mode}/debian'.format(mode=mode) for mode in default_modes])}\n        build dist-server-rpm: phony {' '.join(['$builddir/dist/{mode}/redhat'.format(mode=mode) for mode in default_modes])}\n        build dist-server-tar: phony {' '.join(['$builddir/{mode}/dist/tar/{scylla_product}-{scylla_version}-{scylla_release}.{arch}.tar.gz'.format(mode=mode, scylla_product=scylla_product, arch=arch, scylla_version=scylla_version, scylla_release=scylla_release) for mode in default_modes])}\n        build dist-server-debuginfo: phony {' '.join(['$builddir/{mode}/dist/tar/{scylla_product}-debuginfo-{scylla_version}-{scylla_release}.{arch}.tar.gz'.format(mode=mode, scylla_product=scylla_product, arch=arch, scylla_version=scylla_version, scylla_release=scylla_release) for mode in default_modes])}\n        build dist-server: phony dist-server-tar dist-server-debuginfo dist-server-rpm dist-server-deb\n\n        rule build-submodule-reloc\n          command = cd $reloc_dir && ./reloc/build_reloc.sh --version $$(<../../$builddir/SCYLLA-PRODUCT-FILE)-$$(sed 's/-/~/' <../../$builddir/SCYLLA-VERSION-FILE)-$$(<../../$builddir/SCYLLA-RELEASE-FILE) --nodeps $args\n        rule build-submodule-rpm\n          command = cd $dir && ./reloc/build_rpm.sh --reloc-pkg $artifact\n        rule build-submodule-deb\n          command = cd $dir && ./reloc/build_deb.sh --reloc-pkg $artifact\n\n        build tools/java/build/{scylla_product}-tools-{scylla_version}-{scylla_release}.noarch.tar.gz: build-submodule-reloc | $builddir/SCYLLA-PRODUCT-FILE $builddir/SCYLLA-VERSION-FILE $builddir/SCYLLA-RELEASE-FILE\n          reloc_dir = tools/java\n        build dist-tools-rpm: build-submodule-rpm tools/java/build/{scylla_product}-tools-{scylla_version}-{scylla_release}.noarch.tar.gz\n          dir = tools/java\n          artifact = build/{scylla_product}-tools-{scylla_version}-{scylla_release}.noarch.tar.gz\n        build dist-tools-deb: build-submodule-deb tools/java/build/{scylla_product}-tools-{scylla_version}-{scylla_release}.noarch.tar.gz\n          dir = tools/java\n          artifact = build/{scylla_product}-tools-{scylla_version}-{scylla_release}.noarch.tar.gz\n        build dist-tools-tar: phony {' '.join(['$builddir/{mode}/dist/tar/{scylla_product}-tools-{scylla_version}-{scylla_release}.noarch.tar.gz'.format(mode=mode, scylla_product=scylla_product, scylla_version=scylla_version, scylla_release=scylla_release) for mode in default_modes])}\n        build dist-tools: phony dist-tools-tar dist-tools-rpm dist-tools-deb\n\n        build tools/cqlsh/build/{scylla_product}-cqlsh-{scylla_version}-{scylla_release}.{arch}.tar.gz: build-submodule-reloc | $builddir/SCYLLA-PRODUCT-FILE $builddir/SCYLLA-VERSION-FILE $builddir/SCYLLA-RELEASE-FILE\n          reloc_dir = tools/cqlsh\n        build dist-cqlsh-rpm: build-submodule-rpm tools/cqlsh/build/{scylla_product}-cqlsh-{scylla_version}-{scylla_release}.{arch}.tar.gz\n          dir = tools/cqlsh\n          artifact = build/{scylla_product}-cqlsh-{scylla_version}-{scylla_release}.{arch}.tar.gz\n        build dist-cqlsh-deb: build-submodule-deb tools/cqlsh/build/{scylla_product}-cqlsh-{scylla_version}-{scylla_release}.{arch}.tar.gz\n          dir = tools/cqlsh\n          artifact = build/{scylla_product}-cqlsh-{scylla_version}-{scylla_release}.{arch}.tar.gz\n        build dist-cqlsh-tar: phony {' '.join(['$builddir/{mode}/dist/tar/{scylla_product}-cqlsh-{scylla_version}-{scylla_release}.{arch}.tar.gz'.format(mode=mode, scylla_product=scylla_product, scylla_version=scylla_version, scylla_release=scylla_release, arch=arch) for mode in default_modes])}\n        build dist-cqlsh: phony dist-cqlsh-tar dist-cqlsh-rpm dist-cqlsh-deb\n\n        build tools/python3/build/{scylla_product}-python3-{scylla_version}-{scylla_release}.{arch}.tar.gz: build-submodule-reloc | $builddir/SCYLLA-PRODUCT-FILE $builddir/SCYLLA-VERSION-FILE $builddir/SCYLLA-RELEASE-FILE\n          reloc_dir = tools/python3\n          args = --packages \"{python3_dependencies}\" --pip-packages \"{pip_dependencies}\" --pip-symlinks \"{pip_symlinks}\"\n        build dist-python3-rpm: build-submodule-rpm tools/python3/build/{scylla_product}-python3-{scylla_version}-{scylla_release}.{arch}.tar.gz\n          dir = tools/python3\n          artifact = build/{scylla_product}-python3-{scylla_version}-{scylla_release}.{arch}.tar.gz\n        build dist-python3-deb: build-submodule-deb tools/python3/build/{scylla_product}-python3-{scylla_version}-{scylla_release}.{arch}.tar.gz\n          dir = tools/python3\n          artifact = build/{scylla_product}-python3-{scylla_version}-{scylla_release}.{arch}.tar.gz\n        build dist-python3-tar: phony {' '.join(['$builddir/{mode}/dist/tar/{scylla_product}-python3-{scylla_version}-{scylla_release}.{arch}.tar.gz'.format(mode=mode, scylla_product=scylla_product, arch=arch, scylla_version=scylla_version, scylla_release=scylla_release) for mode in default_modes])}\n        build dist-python3: phony dist-python3-tar dist-python3-rpm dist-python3-deb\n        build dist-deb: phony dist-server-deb dist-python3-deb dist-tools-deb dist-cqlsh-deb\n        build dist-rpm: phony dist-server-rpm dist-python3-rpm dist-tools-rpm dist-cqlsh-rpm\n        build dist-tar: phony dist-unified-tar dist-server-tar dist-python3-tar dist-tools-tar dist-cqlsh-tar\n\n        build dist: phony dist-unified dist-server dist-python3 dist-tools dist-cqlsh\n        '''))\n\n    f.write(textwrap.dedent(f'''\\\n        build dist-check: phony {' '.join(['dist-check-{mode}'.format(mode=mode) for mode in default_modes])}\n        rule dist-check\n          command = ./tools/testing/dist-check/dist-check.sh --mode $mode\n        '''))\n    for mode in build_modes:\n        f.write(textwrap.dedent(f'''\\\n        build $builddir/{mode}/dist/tar/{scylla_product}-python3-{scylla_version}-{scylla_release}.{arch}.tar.gz: copy tools/python3/build/{scylla_product}-python3-{scylla_version}-{scylla_release}.{arch}.tar.gz\n        build $builddir/{mode}/dist/tar/{scylla_product}-python3-package.tar.gz: copy tools/python3/build/{scylla_product}-python3-{scylla_version}-{scylla_release}.{arch}.tar.gz\n        build $builddir/{mode}/dist/tar/{scylla_product}-python3-{arch}-package.tar.gz: copy tools/python3/build/{scylla_product}-python3-{scylla_version}-{scylla_release}.{arch}.tar.gz\n        build $builddir/{mode}/dist/tar/{scylla_product}-tools-{scylla_version}-{scylla_release}.noarch.tar.gz: copy tools/java/build/{scylla_product}-tools-{scylla_version}-{scylla_release}.noarch.tar.gz\n        build $builddir/{mode}/dist/tar/{scylla_product}-tools-package.tar.gz: copy tools/java/build/{scylla_product}-tools-{scylla_version}-{scylla_release}.noarch.tar.gz\n        build $builddir/{mode}/dist/tar/{scylla_product}-cqlsh-{scylla_version}-{scylla_release}.{arch}.tar.gz: copy tools/cqlsh/build/{scylla_product}-cqlsh-{scylla_version}-{scylla_release}.{arch}.tar.gz\n        build $builddir/{mode}/dist/tar/{scylla_product}-cqlsh-package.tar.gz: copy tools/cqlsh/build/{scylla_product}-cqlsh-{scylla_version}-{scylla_release}.{arch}.tar.gz\n\n        build {mode}-dist: phony dist-server-{mode} dist-server-debuginfo-{mode} dist-python3-{mode} dist-tools-{mode} dist-unified-{mode} dist-cqlsh-{mode}\n        build dist-{mode}: phony {mode}-dist\n        build dist-check-{mode}: dist-check\n          mode = {mode}\n            '''))\n\n\n    build_ninja_files=[]\n    for mode in build_modes:\n        build_ninja_files += [f'{outdir}/{mode}/seastar/build.ninja']\n        build_ninja_files += [f'{outdir}/{mode}/abseil/build.ninja']\n\n    f.write(textwrap.dedent('''\\\n        rule configure\n          command = ./configure.py --out={buildfile_final_name}.new --out-final-name={buildfile_final_name} $configure_args && mv {buildfile_final_name}.new {buildfile_final_name}\n          generator = 1\n          description = CONFIGURE $configure_args\n        build {buildfile_final_name} {build_ninja_list}: configure | configure.py SCYLLA-VERSION-GEN $builddir/SCYLLA-PRODUCT-FILE $builddir/SCYLLA-VERSION-FILE $builddir/SCYLLA-RELEASE-FILE {args.seastar_path}/CMakeLists.txt\n        rule cscope\n            command = find -name '*.[chS]' -o -name \"*.cc\" -o -name \"*.hh\" | cscope -bq -i-\n            description = CSCOPE\n        build cscope: cscope\n        rule clean\n            command = rm -rf build\n            description = CLEAN\n        build clean: clean\n        rule mode_list\n            command = echo {modes_list}\n            description = List configured modes\n        build mode_list: mode_list\n        default {modes_list}\n        ''').format(modes_list=' '.join(default_modes), build_ninja_list=\" \".join(build_ninja_files), **globals()))\n    unit_test_list = set(test for test in build_artifacts if test in set(tests))\n    f.write(textwrap.dedent('''\\\n        rule unit_test_list\n            command = /usr/bin/env echo -e '{unit_test_list}'\n            description = List configured unit tests\n        build unit_test_list: unit_test_list\n        ''').format(unit_test_list=\"\\\\n\".join(sorted(unit_test_list))))\n    f.write(textwrap.dedent('''\\\n        build always: phony\n        rule scylla_version_gen\n            command = ./SCYLLA-VERSION-GEN --output-dir $builddir\n            restat = 1\n        build $builddir/SCYLLA-RELEASE-FILE $builddir/SCYLLA-VERSION-FILE: scylla_version_gen | always\n        rule debian_files_gen\n            command = ./dist/debian/debian_files_gen.py --build-dir $builddir\n        build $builddir/debian/debian: debian_files_gen | always\n        rule extract_node_exporter\n            command = tar -C $builddir -xvpf {node_exporter_filename} --no-same-owner && rm -rfv $builddir/node_exporter && mv -v $builddir/{node_exporter_dirname} $builddir/node_exporter\n        build $builddir/node_exporter/node_exporter: extract_node_exporter | always\n        build $builddir/node_exporter/node_exporter.stripped: strip $builddir/node_exporter/node_exporter\n        build $builddir/node_exporter/node_exporter.debug: phony $builddir/node_exporter/node_exporter.stripped\n        rule print_help\n             command = ./scripts/build-help.sh\n        build help: print_help | always\n        ''').format(**globals()))\n\n\ndef create_build_system(args):\n    check_for_minimal_compiler_version(args.cxx)\n    check_for_boost(args.cxx)\n    check_for_lz4(args.cxx, args.user_cflags)\n\n    os.makedirs(outdir, exist_ok=True)\n\n    scylla_product, scylla_version, scylla_release = generate_version(args.date_stamp)\n\n    for mode, mode_config in build_modes.items():\n        extra_cxxflags = ' '.join(get_extra_cxxflags(mode, mode_config, args.cxx, args.debuginfo))\n        mode_config['cxxflags'] += f' {extra_cxxflags}'\n\n        mode_config['per_src_extra_cxxflags']['release.cc'] = ' '.join(get_release_cxxflags(scylla_product, scylla_version, scylla_release))\n\n    prepare_advanced_optimizations(modes=modes, build_modes=build_modes, args=args)\n\n    if not args.dist_only:\n        global user_cflags, libs\n        # args.buildfile builds seastar with the rules of\n        # {outdir}/{mode}/seastar/build.ninja, and\n        # {outdir}/{mode}/seastar/seastar.pc is queried for building flags\n        for mode, mode_config in build_modes.items():\n            configure_seastar(outdir, mode, mode_config)\n            configure_abseil(outdir, mode, mode_config)\n        user_cflags += ' -isystem abseil'\n\n    for mode, mode_config in build_modes.items():\n        mode_config.update(query_seastar_flags(f'{outdir}/{mode}/seastar/seastar.pc',\n                                               mode_config['build_seastar_shared_libs'],\n                                               args.staticcxx))\n\n    ninja = find_ninja()\n    with open(args.buildfile, 'w') as f:\n        arch = platform.machine()\n        write_build_file(f,\n                         arch,\n                         ninja,\n                         scylla_product,\n                         scylla_version,\n                         scylla_release,\n                         args)\n    generate_compdb('compile_commands.json', ninja, args.buildfile, selected_modes)\n\n\nclass BuildType(NamedTuple):\n    build_by_default: bool\n    cmake_build_type: str\n\n\ndef generate_compdb_for_cmake_build(source_dir, build_dir):\n    # Since Seastar and Scylla are configured as separate projects, their compilation\n    # databases need to be merged into a single database for tooling consumption.\n    compdb = 'compile_commands.json'\n    scylla_compdb_path = os.path.join(build_dir, compdb)\n    seastar_compdb_path = ''\n    # sort build types by supposed indexing speed\n    for build_type in ['Dev', 'Debug', 'RelWithDebInfo', 'Sanitize']:\n        seastar_compdb_path = os.path.join(build_dir, build_type, 'seastar', compdb)\n        if os.path.exists(seastar_compdb_path):\n            break\n    assert seastar_compdb_path, \"Seasetar's building system is not configured yet.\"\n    # if the file exists, just overwrite it so we can keep it updated\n    with open(os.path.join(source_dir, compdb), 'w+b') as merged_compdb:\n        # \"merge-compdb.py\" considers all object files under the \"--prefix\"\n        # directory as relevant. Since CMake generates .o files in\n        # \"CMakeFiles\" directories, we preserve the compilation rules for\n        # these generated files.\n        prefix = \"\"\n        subprocess.run([os.path.join(source_dir, 'scripts/merge-compdb.py'),\n                        prefix,\n                        scylla_compdb_path,\n                        seastar_compdb_path],\n                       stdout=merged_compdb,\n                       check=True)\n\n\ndef configure_using_cmake(args):\n    # all supported build modes, and if they are built by default if selected\n    build_modes = {'debug': BuildType(True, 'Debug'),\n                   'release': BuildType(True, 'RelWithDebInfo'),\n                   'dev': BuildType(True, 'Dev'),\n                   'sanitize': BuildType(False, 'Sanitize'),\n                   'coverage': BuildType(False, 'Coverage')}\n    default_modes = list(name for name, mode in build_modes.items()\n                         if mode.build_by_default)\n    selected_modes = args.selected_modes or default_modes\n    selected_configs = ';'.join(build_modes[mode].cmake_build_type for mode\n                                in selected_modes)\n    settings = {\n        'CMAKE_CONFIGURATION_TYPES': selected_configs,\n        'CMAKE_CROSS_CONFIGS': selected_configs,\n        'CMAKE_DEFAULT_CONFIGS': selected_configs,\n        'CMAKE_C_COMPILER': args.cc,\n        'CMAKE_CXX_COMPILER': args.cxx,\n        'CMAKE_CXX_FLAGS': args.user_cflags,\n        'CMAKE_EXE_LINKER_FLAGS': semicolon_separated(args.user_ldflags),\n        'CMAKE_EXPORT_COMPILE_COMMANDS': 'ON',\n        'Scylla_CHECK_HEADERS': 'ON',\n        'Scylla_DIST': 'ON' if args.enable_dist in (None, True) else 'OFF',\n        'Scylla_TEST_TIMEOUT': args.test_timeout,\n        'Scylla_TEST_REPEAT': args.test_repeat,\n        'Scylla_ENABLE_LTO': 'ON' if args.lto else 'OFF',\n    }\n    if args.date_stamp:\n        settings['Scylla_DATE_STAMP'] = args.date_stamp\n    if args.staticboost:\n        settings['Boost_USE_STATIC_LIBS'] = 'ON'\n    if args.clang_inline_threshold != -1:\n        settings['Scylla_CLANG_INLINE_THRESHOLD'] = args.clang_inline_threshold\n    if args.cspgo:\n        settings['Scylla_BUILD_INSTRUMENTED'] = \"CSIR\"\n    elif args.pgo:\n        settings['Scylla_BUILD_INSTRUMENTED'] = \"IR\"\n    if args.use_profile:\n        settings['Scylla_PROFDATA_FILE'] = args.use_profile\n    elif args.use_profile is None:\n        profile_archive_path = f\"pgo/profiles/{platform.machine()}/profile.profdata.xz\"\n        if \"compressed data\" in subprocess.check_output([\"file\", profile_archive_path], text=True):\n            settings['Scylla_PROFDATA_COMPRESSED_FILE'] = profile_archive_path\n        else:\n            # Avoid breaking existing pipelines without git-lfs installed.\n            print(f\"WARNING: {profile_archive_path} is not an archive. Building without a profile.\", file=sys.stderr)\n    # scripts/refresh-pgo-profiles.sh does not specify the path to the profile\n    # so we don't define Scylla_PROFDATA_COMPRESSED_FILE, and use the default\n    # value\n\n    source_dir = os.path.realpath(os.path.dirname(__file__))\n    if os.path.isabs(args.build_dir):\n        build_dir = args.build_dir\n    else:\n        build_dir = os.path.join(source_dir, args.build_dir)\n\n    if not args.dist_only:\n        for mode in selected_modes:\n            configure_seastar(build_dir, build_modes[mode].cmake_build_type, modes[mode])\n\n    cmake_command = ['cmake']\n    cmake_command += [f'-D{var}={value}' for var, value in settings.items()]\n    cmake_command += ['-G', 'Ninja Multi-Config',\n                      '-B', build_dir,\n                      '-S', source_dir]\n    subprocess.check_call(cmake_command, shell=False, cwd=source_dir)\n    generate_compdb_for_cmake_build(source_dir, build_dir)\n\n\nif __name__ == '__main__':\n    if args.use_cmake:\n        prepare_advanced_optimizations(modes=modes, build_modes=build_modes, args=args)\n        configure_using_cmake(args)\n    else:\n        create_build_system(args)\n"
        },
        {
          "name": "converting_mutation_partition_applier.cc",
          "type": "blob",
          "size": 6.8115234375,
          "content": "/*\n * Copyright (C) 2020-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"utils/assert.hh\"\n#include \"converting_mutation_partition_applier.hh\"\n#include \"concrete_types.hh\"\n\n#include \"mutation/mutation_partition_view.hh\"\n#include \"mutation/mutation_partition.hh\"\n#include \"schema/schema.hh\"\n\nbool\nconverting_mutation_partition_applier::is_compatible(const column_definition& new_def, const abstract_type& old_type, column_kind kind) {\n    return ::is_compatible(new_def.kind, kind) && new_def.type->is_value_compatible_with(old_type);\n}\n\natomic_cell\nconverting_mutation_partition_applier::upgrade_cell(const abstract_type& new_type, const abstract_type& old_type, atomic_cell_view cell,\n                                atomic_cell::collection_member cm) {\n    if (cell.is_live() && !old_type.is_counter()) {\n        if (cell.is_live_and_has_ttl()) {\n            return atomic_cell::make_live(new_type, cell.timestamp(), cell.value(), cell.expiry(), cell.ttl(), cm);\n        }\n        return atomic_cell::make_live(new_type, cell.timestamp(), cell.value(), cm);\n    } else {\n        return atomic_cell(new_type, cell);\n    }\n}\n\nvoid\nconverting_mutation_partition_applier::accept_cell(row& dst, column_kind kind, const column_definition& new_def, const abstract_type& old_type, atomic_cell_view cell) {\n    if (!is_compatible(new_def, old_type, kind) || cell.timestamp() <= new_def.dropped_at()) {\n        return;\n    }\n    dst.apply(new_def, upgrade_cell(*new_def.type, old_type, cell));\n}\n\nvoid\nconverting_mutation_partition_applier::accept_cell(row& dst, column_kind kind, const column_definition& new_def, const abstract_type& old_type, collection_mutation_view cell) {\n    if (!is_compatible(new_def, old_type, kind)) {\n        return;\n    }\n\n  cell.with_deserialized(old_type, [&] (collection_mutation_view_description old_view) {\n    collection_mutation_description new_view;\n    if (old_view.tomb.timestamp > new_def.dropped_at()) {\n        new_view.tomb = old_view.tomb;\n    }\n\n    visit(old_type, make_visitor(\n        [&] (const collection_type_impl& old_ctype) {\n            SCYLLA_ASSERT(new_def.type->is_collection()); // because is_compatible\n            auto& new_ctype = static_cast<const collection_type_impl&>(*new_def.type);\n\n            auto& new_value_type = *new_ctype.value_comparator();\n            auto& old_value_type = *old_ctype.value_comparator();\n\n            for (auto& c : old_view.cells) {\n                if (c.second.timestamp() > new_def.dropped_at()) {\n                    new_view.cells.emplace_back(c.first, upgrade_cell(\n                            new_value_type, old_value_type, c.second, atomic_cell::collection_member::yes));\n                }\n            }\n        },\n        [&] (const user_type_impl& old_utype) {\n            SCYLLA_ASSERT(new_def.type->is_user_type()); // because is_compatible\n            auto& new_utype = static_cast<const user_type_impl&>(*new_def.type);\n\n            for (auto& c : old_view.cells) {\n                if (c.second.timestamp() > new_def.dropped_at()) {\n                    auto idx = deserialize_field_index(c.first);\n                    SCYLLA_ASSERT(idx < new_utype.size() && idx < old_utype.size());\n\n                    new_view.cells.emplace_back(c.first, upgrade_cell(\n                            *new_utype.type(idx), *old_utype.type(idx), c.second, atomic_cell::collection_member::yes));\n                }\n            }\n        },\n        [&] (const abstract_type& o) {\n            throw std::runtime_error(format(\"not a multi-cell type: {}\", o.name()));\n        }\n    ));\n\n    if (new_view.tomb || !new_view.cells.empty()) {\n        dst.apply(new_def, new_view.serialize(*new_def.type));\n    }\n  });\n}\n\nconverting_mutation_partition_applier::converting_mutation_partition_applier(\n        const column_mapping& visited_column_mapping,\n        const schema& target_schema,\n        mutation_partition& target)\n    : _p_schema(target_schema)\n    , _p(target)\n    , _visited_column_mapping(visited_column_mapping)\n{ }\n\n\nvoid\nconverting_mutation_partition_applier::accept_partition_tombstone(tombstone t) {\n    _p.apply(t);\n}\n\nvoid\nconverting_mutation_partition_applier::accept_static_cell(column_id id, atomic_cell cell) {\n    return accept_static_cell(id, atomic_cell_view(cell));\n}\n\nvoid\nconverting_mutation_partition_applier::accept_static_cell(column_id id, atomic_cell_view cell) {\n    const column_mapping_entry& col = _visited_column_mapping.static_column_at(id);\n    const column_definition* def = _p_schema.get_column_definition(col.name());\n    if (def) {\n        accept_cell(_p._static_row.maybe_create(), column_kind::static_column, *def, *col.type(), cell);\n    }\n}\n\nvoid\nconverting_mutation_partition_applier::accept_static_cell(column_id id, collection_mutation_view collection) {\n    const column_mapping_entry& col = _visited_column_mapping.static_column_at(id);\n    const column_definition* def = _p_schema.get_column_definition(col.name());\n    if (def) {\n        accept_cell(_p._static_row.maybe_create(), column_kind::static_column, *def, *col.type(), collection);\n    }\n}\n\nvoid\nconverting_mutation_partition_applier::accept_row_tombstone(const range_tombstone& rt) {\n    _p.apply_row_tombstone(_p_schema, rt);\n}\n\nvoid\nconverting_mutation_partition_applier::accept_row(position_in_partition_view key, const row_tombstone& deleted_at, const row_marker& rm, is_dummy dummy, is_continuous continuous) {\n    deletable_row& r = _p.clustered_row(_p_schema, key, dummy, continuous);\n    r.apply(rm);\n    r.apply(deleted_at);\n    _current_row = &r;\n}\n\nvoid\nconverting_mutation_partition_applier::accept_row_cell(column_id id, atomic_cell cell) {\n    return accept_row_cell(id, atomic_cell_view(cell));\n}\n\nvoid\nconverting_mutation_partition_applier::accept_row_cell(column_id id, atomic_cell_view cell) {\n    const column_mapping_entry& col = _visited_column_mapping.regular_column_at(id);\n    const column_definition* def = _p_schema.get_column_definition(col.name());\n    if (def) {\n        accept_cell(_current_row->cells(), column_kind::regular_column, *def, *col.type(), cell);\n    }\n}\n\nvoid\nconverting_mutation_partition_applier::accept_row_cell(column_id id, collection_mutation_view collection) {\n    const column_mapping_entry& col = _visited_column_mapping.regular_column_at(id);\n    const column_definition* def = _p_schema.get_column_definition(col.name());\n    if (def) {\n        accept_cell(_current_row->cells(), column_kind::regular_column, *def, *col.type(), collection);\n    }\n}\n\nvoid\nconverting_mutation_partition_applier::append_cell(row& dst, column_kind kind, const column_definition& new_def, const column_definition& old_def, const atomic_cell_or_collection& cell) {\n    if (new_def.is_atomic()) {\n        accept_cell(dst, kind, new_def, *old_def.type, cell.as_atomic_cell(old_def));\n    } else {\n        accept_cell(dst, kind, new_def, *old_def.type, cell.as_collection_mutation());\n    }\n}\n"
        },
        {
          "name": "converting_mutation_partition_applier.hh",
          "type": "blob",
          "size": 2.763671875,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"mutation/mutation_partition_visitor.hh\"\n#include \"mutation/atomic_cell.hh\"\n#include \"schema/schema.hh\" // temporary: bring in definition of `column_kind`\n\nclass schema;\nclass row;\nclass mutation_partition;\nclass column_mapping;\nclass deletable_row;\nclass column_definition;\nclass abstract_type;\nclass atomic_cell_or_collection;\n\n// Mutation partition visitor which applies visited data into\n// existing mutation_partition. The visited data may be of a different schema.\n// Data which is not representable in the new schema is dropped.\n// Weak exception guarantees.\nclass converting_mutation_partition_applier : public mutation_partition_visitor {\n    const schema& _p_schema;\n    mutation_partition& _p;\n    const column_mapping& _visited_column_mapping;\n    deletable_row* _current_row;\nprivate:\n    static bool is_compatible(const column_definition& new_def, const abstract_type& old_type, column_kind kind);\n    static atomic_cell upgrade_cell(const abstract_type& new_type, const abstract_type& old_type, atomic_cell_view cell,\n                                    atomic_cell::collection_member cm = atomic_cell::collection_member::no);\n    static void accept_cell(row& dst, column_kind kind, const column_definition& new_def, const abstract_type& old_type, atomic_cell_view cell);\n    static void accept_cell(row& dst, column_kind kind, const column_definition& new_def, const abstract_type& old_type, collection_mutation_view cell);public:\n    converting_mutation_partition_applier(\n            const column_mapping& visited_column_mapping,\n            const schema& target_schema,\n            mutation_partition& target);\n    virtual void accept_partition_tombstone(tombstone t) override;\n    void accept_static_cell(column_id id, atomic_cell cell);\n    virtual void accept_static_cell(column_id id, atomic_cell_view cell) override;\n    virtual void accept_static_cell(column_id id, collection_mutation_view collection) override;\n    virtual void accept_row_tombstone(const range_tombstone& rt) override;\n    virtual void accept_row(position_in_partition_view key, const row_tombstone& deleted_at, const row_marker& rm, is_dummy dummy, is_continuous continuous) override;\n    void accept_row_cell(column_id id, atomic_cell cell);\n    virtual void accept_row_cell(column_id id, atomic_cell_view cell) override;\n    virtual void accept_row_cell(column_id id, collection_mutation_view collection) override;\n\n    // Appends the cell to dst upgrading it to the new schema.\n    // Cells must have monotonic names.\n    static void append_cell(row& dst, column_kind kind, const column_definition& new_def, const column_definition& old_def, const atomic_cell_or_collection& cell);\n};\n"
        },
        {
          "name": "counters.cc",
          "type": "blob",
          "size": 9.52734375,
          "content": "/*\n * Copyright (C) 2016-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"utils/assert.hh\"\n#include \"counters.hh\"\n#include \"mutation/mutation.hh\"\n#include \"combine.hh\"\n#include \"utils/log.hh\"\n\nlogging::logger cell_locker_log(\"cell_locker\");\n\nauto fmt::formatter<counter_shard_view>::format(const counter_shard_view& csv,\n                                                fmt::format_context& ctx) const -> decltype(ctx.out()) {\n    return fmt::format_to(ctx.out(), \"{{global_shard id: {} value: {}, clock: {}}}\",\n                          csv.id(), csv.value(), csv.logical_clock());\n}\n\nauto fmt::formatter<counter_cell_view>::format(const counter_cell_view& ccv,\n                                               fmt::format_context& ctx) const -> decltype(ctx.out()) {\n    return fmt::format_to(ctx.out(), \"{{counter_cell timestamp: {} shards: {{{}}}}}\",\n                          ccv.timestamp(), fmt::join(ccv.shards(), \", \"));\n}\n\nvoid counter_cell_builder::do_sort_and_remove_duplicates()\n{\n    std::ranges::sort(_shards, std::ranges::less(), std::mem_fn(&counter_shard::id));\n\n    std::vector<counter_shard> new_shards;\n    new_shards.reserve(_shards.size());\n    for (auto& cs : _shards) {\n        if (new_shards.empty() || new_shards.back().id() != cs.id()) {\n            new_shards.emplace_back(cs);\n        } else {\n            new_shards.back().apply(cs);\n        }\n    }\n    _shards = std::move(new_shards);\n    _sorted = true;\n}\n\nstatic bool apply_in_place(const column_definition& cdef, atomic_cell_mutable_view dst, atomic_cell_mutable_view src)\n{\n    auto dst_ccmv = counter_cell_mutable_view(dst);\n    auto src_ccmv = counter_cell_mutable_view(src);\n    auto dst_shards = dst_ccmv.shards();\n    auto src_shards = src_ccmv.shards();\n\n    auto dst_it = dst_shards.begin();\n    auto src_it = src_shards.begin();\n\n    while (src_it != src_shards.end()) {\n        while (dst_it != dst_shards.end() && dst_it->id() < src_it->id()) {\n            ++dst_it;\n        }\n        if (dst_it == dst_shards.end() || dst_it->id() != src_it->id()) {\n            // Fast-path failed. Revert and fall back to the slow path.\n            if (dst_it == dst_shards.end()) {\n                --dst_it;\n            }\n            while (src_it != src_shards.begin()) {\n                --src_it;\n                while (dst_it->id() != src_it->id()) {\n                    --dst_it;\n                }\n                src_it->swap_value_and_clock(*dst_it);\n            }\n            return false;\n        }\n        if (dst_it->logical_clock() < src_it->logical_clock()) {\n            dst_it->swap_value_and_clock(*src_it);\n        } else {\n            src_it->set_value_and_clock(*dst_it);\n        }\n        ++src_it;\n    }\n\n    auto dst_ts = dst_ccmv.timestamp();\n    auto src_ts = src_ccmv.timestamp();\n    dst_ccmv.set_timestamp(std::max(dst_ts, src_ts));\n    src_ccmv.set_timestamp(dst_ts);\n    return true;\n}\n\nvoid counter_cell_view::apply(const column_definition& cdef, atomic_cell_or_collection& dst, atomic_cell_or_collection& src)\n{\n    auto dst_ac = dst.as_atomic_cell(cdef);\n    auto src_ac = src.as_atomic_cell(cdef);\n\n    if (!dst_ac.is_live() || !src_ac.is_live()) {\n        if (dst_ac.is_live() || (!src_ac.is_live() && compare_atomic_cell_for_merge(dst_ac, src_ac) < 0)) {\n            std::swap(dst, src);\n        }\n        return;\n    }\n\n    if (dst_ac.is_counter_update() && src_ac.is_counter_update()) {\n        auto src_v = src_ac.counter_update_value();\n        auto dst_v = dst_ac.counter_update_value();\n        dst = atomic_cell::make_live_counter_update(std::max(dst_ac.timestamp(), src_ac.timestamp()),\n                                                    src_v + dst_v);\n        return;\n    }\n\n    SCYLLA_ASSERT(!dst_ac.is_counter_update());\n    SCYLLA_ASSERT(!src_ac.is_counter_update());\n\n    auto src_ccv = counter_cell_view(src_ac);\n    auto dst_ccv = counter_cell_view(dst_ac);\n    if (dst_ccv.shard_count() >= src_ccv.shard_count()) {\n        auto dst_amc = dst.as_mutable_atomic_cell(cdef);\n        auto src_amc = src.as_mutable_atomic_cell(cdef);\n        if (apply_in_place(cdef, dst_amc, src_amc)) {\n            return;\n        }\n    }\n\n    auto dst_shards = dst_ccv.shards();\n    auto src_shards = src_ccv.shards();\n\n    counter_cell_builder result;\n    combine(dst_shards.begin(), dst_shards.end(), src_shards.begin(), src_shards.end(),\n            result.inserter(), counter_shard_view::less_compare_by_id(), [] (auto& x, auto& y) {\n                return x.logical_clock() < y.logical_clock() ? y : x;\n            });\n\n    auto cell = result.build(std::max(dst_ac.timestamp(), src_ac.timestamp()));\n    src = std::exchange(dst, atomic_cell_or_collection(std::move(cell)));\n}\n\nstd::optional<atomic_cell> counter_cell_view::difference(atomic_cell_view a, atomic_cell_view b)\n{\n    SCYLLA_ASSERT(!a.is_counter_update());\n    SCYLLA_ASSERT(!b.is_counter_update());\n\n    if (!b.is_live() || !a.is_live()) {\n        if (b.is_live() || (!a.is_live() && compare_atomic_cell_for_merge(b, a) < 0)) {\n            return atomic_cell(*counter_type, a);\n        }\n        return { };\n    }\n\n    auto a_ccv = counter_cell_view(a);\n    auto b_ccv = counter_cell_view(b);\n    auto a_shards = a_ccv.shards();\n    auto b_shards = b_ccv.shards();\n\n    auto a_it = a_shards.begin();\n    auto a_end = a_shards.end();\n    auto b_it = b_shards.begin();\n    auto b_end = b_shards.end();\n\n    counter_cell_builder result;\n    while (a_it != a_end) {\n        while (b_it != b_end && (*b_it).id() < (*a_it).id()) {\n            ++b_it;\n        }\n        if (b_it == b_end || (*a_it).id() != (*b_it).id() || (*a_it).logical_clock() > (*b_it).logical_clock()) {\n            result.add_shard(counter_shard(*a_it));\n        }\n        ++a_it;\n    }\n\n    std::optional<atomic_cell> diff;\n    if (!result.empty()) {\n        diff = result.build(std::max(a.timestamp(), b.timestamp()));\n    } else if (a.timestamp() > b.timestamp()) {\n        diff = atomic_cell::make_live(*counter_type, a.timestamp(), bytes_view());\n    }\n    return diff;\n}\n\n\nvoid transform_counter_updates_to_shards(mutation& m, const mutation* current_state, uint64_t clock_offset, locator::host_id local_host_id) {\n    // FIXME: allow current_state to be frozen_mutation\n\n    utils::UUID local_id = local_host_id.uuid();\n\n    auto transform_new_row_to_shards = [&s = *m.schema(), clock_offset, local_id] (column_kind kind, auto& cells) {\n        cells.for_each_cell([&] (column_id id, atomic_cell_or_collection& ac_o_c) {\n            auto& cdef = s.column_at(kind, id);\n            auto acv = ac_o_c.as_atomic_cell(cdef);\n            if (!acv.is_live()) {\n                return; // continue -- we are in lambda\n            }\n            auto delta = acv.counter_update_value();\n            auto cs = counter_shard(counter_id(local_id), delta, clock_offset + 1);\n            ac_o_c = counter_cell_builder::from_single_shard(acv.timestamp(), cs);\n        });\n    };\n\n    if (!current_state) {\n        transform_new_row_to_shards(column_kind::static_column, m.partition().static_row());\n        for (auto& cr : m.partition().clustered_rows()) {\n            transform_new_row_to_shards(column_kind::regular_column, cr.row().cells());\n        }\n        return;\n    }\n\n    clustering_key::less_compare cmp(*m.schema());\n\n    auto transform_row_to_shards = [&s = *m.schema(), clock_offset, local_id] (column_kind kind, auto& transformee, auto& state) {\n        std::deque<std::pair<column_id, counter_shard>> shards;\n        state.for_each_cell([&] (column_id id, const atomic_cell_or_collection& ac_o_c) {\n            auto& cdef = s.column_at(kind, id);\n            auto acv = ac_o_c.as_atomic_cell(cdef);\n            if (!acv.is_live()) {\n                return; // continue -- we are in lambda\n            }\n            auto ccv = counter_cell_view(acv);\n            auto cs = ccv.get_shard(counter_id(local_id));\n            if (!cs) {\n                return; // continue\n            }\n            shards.emplace_back(std::make_pair(id, counter_shard(*cs)));\n          });\n\n        transformee.for_each_cell([&] (column_id id, atomic_cell_or_collection& ac_o_c) {\n            auto& cdef = s.column_at(kind, id);\n            auto acv = ac_o_c.as_atomic_cell(cdef);\n            if (!acv.is_live()) {\n                return; // continue -- we are in lambda\n            }\n            while (!shards.empty() && shards.front().first < id) {\n                shards.pop_front();\n            }\n\n            auto delta = acv.counter_update_value();\n\n            if (shards.empty() || shards.front().first > id) {\n                auto cs = counter_shard(counter_id(local_id), delta, clock_offset + 1);\n                ac_o_c = counter_cell_builder::from_single_shard(acv.timestamp(), cs);\n            } else {\n                auto& cs = shards.front().second;\n                cs.update(delta, clock_offset + 1);\n                ac_o_c = counter_cell_builder::from_single_shard(acv.timestamp(), cs);\n                shards.pop_front();\n            }\n        });\n    };\n\n    transform_row_to_shards(column_kind::static_column, m.partition().static_row(), current_state->partition().static_row());\n\n    auto& cstate = current_state->partition();\n    auto it = cstate.clustered_rows().begin();\n    auto end = cstate.clustered_rows().end();\n    for (auto& cr : m.partition().clustered_rows()) {\n        while (it != end && cmp(it->key(), cr.key())) {\n            ++it;\n        }\n        if (it == end || cmp(cr.key(), it->key())) {\n            transform_new_row_to_shards(column_kind::regular_column, cr.row().cells());\n            continue;\n        }\n\n        transform_row_to_shards(column_kind::regular_column, cr.row().cells(), it->row().cells());\n    }\n}\n"
        },
        {
          "name": "counters.hh",
          "type": "blob",
          "size": 12.927734375,
          "content": "/*\n * Copyright (C) 2016-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"utils/assert.hh\"\n\n#include \"mutation/atomic_cell.hh\"\n#include \"types/types.hh\"\n#include \"locator/host_id.hh\"\n\nclass mutation;\nclass atomic_cell_or_collection;\n\nusing counter_id = utils::tagged_uuid<struct counter_id_tag>;\n\ntemplate<mutable_view is_mutable>\nclass basic_counter_shard_view {\n    enum class offset : unsigned {\n        id = 0u,\n        value = unsigned(id) + sizeof(counter_id),\n        logical_clock = unsigned(value) + sizeof(int64_t),\n        total_size = unsigned(logical_clock) + sizeof(int64_t),\n    };\nprivate:\n    managed_bytes_basic_view<is_mutable> _base;\nprivate:\n    template<typename T>\n    T read(offset off) const {\n        auto v = _base;\n        v.remove_prefix(size_t(off));\n        return read_simple_native<T>(v);\n    }\npublic:\n    static constexpr auto size = size_t(offset::total_size);\npublic:\n    basic_counter_shard_view() = default;\n    explicit basic_counter_shard_view(managed_bytes_basic_view<is_mutable> v) noexcept\n        : _base(v) { }\n\n    counter_id id() const { return read<counter_id>(offset::id); }\n    int64_t value() const { return read<int64_t>(offset::value); }\n    int64_t logical_clock() const { return read<int64_t>(offset::logical_clock); }\n\n    void swap_value_and_clock(basic_counter_shard_view other) noexcept {\n        static constexpr size_t off = size_t(offset::value);\n        static constexpr size_t size = size_t(offset::total_size) - off;\n\n        signed char tmp[size];\n        auto tmp_view = single_fragmented_mutable_view(bytes_mutable_view(std::data(tmp), std::size(tmp)));\n\n        managed_bytes_mutable_view this_view = _base.substr(off, size);\n        managed_bytes_mutable_view other_view = other._base.substr(off, size);\n\n        copy_fragmented_view(tmp_view, this_view);\n        copy_fragmented_view(this_view, other_view);\n        copy_fragmented_view(other_view, tmp_view);\n    }\n\n    void set_value_and_clock(const basic_counter_shard_view& other) noexcept {\n        static constexpr size_t off = size_t(offset::value);\n        static constexpr size_t size = size_t(offset::total_size) - off;\n\n        managed_bytes_mutable_view this_view = _base.substr(off, size);\n        managed_bytes_mutable_view other_view = other._base.substr(off, size);\n\n        copy_fragmented_view(this_view, other_view);\n    }\n\n    bool operator==(const basic_counter_shard_view& other) const {\n        return id() == other.id() && value() == other.value()\n               && logical_clock() == other.logical_clock();\n    }\n\n    struct less_compare_by_id {\n        bool operator()(const basic_counter_shard_view& x, const basic_counter_shard_view& y) const {\n            return x.id() < y.id();\n        }\n    };\n};\n\nusing counter_shard_view = basic_counter_shard_view<mutable_view::no>;\n\ntemplate <>\nstruct fmt::formatter<counter_shard_view> : fmt::formatter<string_view> {\n    auto format(const counter_shard_view&, fmt::format_context& ctx) const -> decltype(ctx.out());\n};\n\nclass counter_shard {\n    counter_id _id;\n    int64_t _value;\n    int64_t _logical_clock;\nprivate:\n    // Shared logic for applying counter_shards and counter_shard_views.\n    // T is either counter_shard or basic_counter_shard_view<U>.\n    template<typename T>\n    requires requires(T shard) {\n        { shard.value() } -> std::same_as<int64_t>;\n        { shard.logical_clock() } -> std::same_as<int64_t>;\n    }\n    counter_shard& do_apply(T&& other) noexcept {\n        auto other_clock = other.logical_clock();\n        if (_logical_clock < other_clock) {\n            _logical_clock = other_clock;\n            _value = other.value();\n        }\n        return *this;\n    }\npublic:\n    counter_shard(counter_id id, int64_t value, int64_t logical_clock) noexcept\n        : _id(id)\n        , _value(value)\n        , _logical_clock(logical_clock)\n    { }\n\n    explicit counter_shard(counter_shard_view csv) noexcept\n        : _id(csv.id())\n        , _value(csv.value())\n        , _logical_clock(csv.logical_clock())\n    { }\n\n    counter_id id() const { return _id; }\n    int64_t value() const { return _value; }\n    int64_t logical_clock() const { return _logical_clock; }\n\n    counter_shard& update(int64_t value_delta, int64_t clock_increment) noexcept {\n        _value = uint64_t(_value) + uint64_t(value_delta); // signed int overflow is undefined hence the cast\n        _logical_clock += clock_increment;\n        return *this;\n    }\n\n    counter_shard& apply(counter_shard_view other) noexcept {\n        return do_apply(other);\n    }\n\n    counter_shard& apply(const counter_shard& other) noexcept {\n        return do_apply(other);\n    }\n\n    static constexpr size_t serialized_size() {\n        return counter_shard_view::size;\n    }\n    void serialize(atomic_cell_value_mutable_view& out) const {\n        write_native<counter_id>(out, _id);\n        write_native<int64_t>(out, _value);\n        write_native<int64_t>(out, _logical_clock);\n    }\n};\n\nclass counter_cell_builder {\n    std::vector<counter_shard> _shards;\n    bool _sorted = true;\nprivate:\n    void do_sort_and_remove_duplicates();\npublic:\n    counter_cell_builder() = default;\n    counter_cell_builder(size_t shard_count) {\n        _shards.reserve(shard_count);\n    }\n\n    void add_shard(const counter_shard& cs) {\n        _shards.emplace_back(cs);\n    }\n\n    void add_maybe_unsorted_shard(const counter_shard& cs) {\n        add_shard(cs);\n        if (_sorted && _shards.size() > 1) {\n            auto current = _shards.rbegin();\n            auto previous = std::next(current);\n            _sorted = current->id() > previous->id();\n        }\n    }\n\n    void sort_and_remove_duplicates() {\n        if (!_sorted) {\n            do_sort_and_remove_duplicates();\n        }\n    }\n\n    size_t serialized_size() const {\n        return _shards.size() * counter_shard::serialized_size();\n    }\n    void serialize(atomic_cell_value_mutable_view& out) const {\n        for (auto&& cs : _shards) {\n            cs.serialize(out);\n        }\n    }\n\n    bool empty() const {\n        return _shards.empty();\n    }\n\n    atomic_cell build(api::timestamp_type timestamp) const {\n        auto ac = atomic_cell::make_live_uninitialized(*counter_type, timestamp, serialized_size());\n\n        auto dst = ac.value();\n        for (auto&& cs : _shards) {\n            cs.serialize(dst);\n        }\n        return ac;\n    }\n\n    static atomic_cell from_single_shard(api::timestamp_type timestamp, const counter_shard& cs) {\n        auto ac = atomic_cell::make_live_uninitialized(*counter_type, timestamp, counter_shard::serialized_size());\n        auto dst = ac.value();\n        cs.serialize(dst);\n        return ac;\n    }\n\n    class inserter_iterator {\n    public:\n        using iterator_category = std::output_iterator_tag;\n        using value_type = counter_shard;\n        using difference_type = std::ptrdiff_t;\n        using pointer = counter_shard*;\n        using reference = counter_shard&;\n    private:\n        counter_cell_builder* _builder;\n    public:\n        explicit inserter_iterator(counter_cell_builder& b) : _builder(&b) { }\n        inserter_iterator& operator=(const counter_shard& cs) {\n            _builder->add_shard(cs);\n            return *this;\n        }\n        inserter_iterator& operator=(const counter_shard_view& csv) {\n            return this->operator=(counter_shard(csv));\n        }\n        inserter_iterator& operator++() { return *this; }\n        inserter_iterator& operator++(int) { return *this; }\n        inserter_iterator& operator*() { return *this; };\n    };\n\n    inserter_iterator inserter() {\n        return inserter_iterator(*this);\n    }\n};\n\n// <counter_id>   := <int64_t><int64_t>\n// <shard>        := <counter_id><int64_t:value><int64_t:logical_clock>\n// <counter_cell> := <shard>*\ntemplate<mutable_view is_mutable>\nclass basic_counter_cell_view {\nprotected:\n    basic_atomic_cell_view<is_mutable> _cell;\nprivate:\n    class shard_iterator {\n    public:\n        using iterator_category = std::bidirectional_iterator_tag;\n        using iterator_concept = std::bidirectional_iterator_tag;\n        using value_type = basic_counter_shard_view<is_mutable>;\n        using difference_type = std::ptrdiff_t;\n        using pointer = basic_counter_shard_view<is_mutable>*;\n    private:\n        managed_bytes_basic_view<is_mutable> _current;\n        basic_counter_shard_view<is_mutable> _current_view;\n        size_t _pos = 0;\n    public:\n        shard_iterator() noexcept = default;\n        shard_iterator(managed_bytes_basic_view<is_mutable> v, size_t offset) noexcept\n            : _current(v), _current_view(_current), _pos(offset) { }\n\n        value_type operator*() const noexcept {\n            return _current_view;\n        }\n\n        pointer operator->() noexcept {\n            return &_current_view;\n        }\n        shard_iterator& operator++() noexcept {\n            _pos += counter_shard_view::size;\n            _current_view = basic_counter_shard_view<is_mutable>(_current.substr(_pos, counter_shard_view::size));\n            return *this;\n        }\n        shard_iterator operator++(int) noexcept {\n            auto it = *this;\n            operator++();\n            return it;\n        }\n        shard_iterator& operator--() noexcept {\n            _pos -= counter_shard_view::size;\n            _current_view = basic_counter_shard_view<is_mutable>(_current.substr(_pos, counter_shard_view::size));\n            return *this;\n        }\n        shard_iterator operator--(int) noexcept {\n            auto it = *this;\n            operator--();\n            return it;\n        }\n        bool operator==(const shard_iterator& other) const noexcept {\n            return _pos == other._pos;\n        }\n    };\npublic:\n    std::ranges::subrange<shard_iterator> shards() const {\n        auto value = _cell.value();\n        auto begin = shard_iterator(value, 0);\n        auto end = shard_iterator(value, value.size());\n        return {begin, end};\n    }\n\n    size_t shard_count() const {\n        return _cell.value().size() / counter_shard_view::size;\n    }\npublic:\n    // ac must be a live counter cell\n    explicit basic_counter_cell_view(basic_atomic_cell_view<is_mutable> ac) noexcept\n        : _cell(ac)\n    {\n        SCYLLA_ASSERT(_cell.is_live());\n        SCYLLA_ASSERT(!_cell.is_counter_update());\n    }\n\n    api::timestamp_type timestamp() const { return _cell.timestamp(); }\n\n    static data_type total_value_type() { return long_type; }\n\n    int64_t total_value() const {\n        return std::ranges::fold_left(shards(), int64_t(0), [] (int64_t v, counter_shard_view cs) {\n            return v + cs.value();\n        });\n    }\n\n    std::optional<counter_shard_view> get_shard(const counter_id& id) const {\n        auto it = std::ranges::find_if(shards(), [&id] (counter_shard_view csv) {\n            return csv.id() == id;\n        });\n        if (it == shards().end()) {\n            return { };\n        }\n        return *it;\n    }\n\n    bool operator==(const basic_counter_cell_view& other) const {\n        return timestamp() == other.timestamp() && std::ranges::equal(shards(), other.shards());\n    }\n};\n\nstruct counter_cell_view : basic_counter_cell_view<mutable_view::no> {\n    using basic_counter_cell_view::basic_counter_cell_view;\n\n    // Reversibly applies two counter cells, at least one of them must be live.\n    static void apply(const column_definition& cdef, atomic_cell_or_collection& dst, atomic_cell_or_collection& src);\n\n    // Computes a counter cell containing minimal amount of data which, when\n    // applied to 'b' returns the same cell as 'a' and 'b' applied together.\n    static std::optional<atomic_cell> difference(atomic_cell_view a, atomic_cell_view b);\n};\n\ntemplate <>\nstruct fmt::formatter<counter_cell_view> : fmt::formatter<string_view> {\n    auto format(const counter_cell_view&, fmt::format_context& ctx) const -> decltype(ctx.out());\n};\n\nstruct counter_cell_mutable_view : basic_counter_cell_view<mutable_view::yes> {\n    using basic_counter_cell_view::basic_counter_cell_view;\n\n    explicit counter_cell_mutable_view(atomic_cell_mutable_view ac) noexcept\n        : basic_counter_cell_view<mutable_view::yes>(ac)\n    {\n    }\n\n    void set_timestamp(api::timestamp_type ts) { _cell.set_timestamp(ts); }\n};\n\n// Transforms mutation dst from counter updates to counter shards using state\n// stored in current_state.\n// If current_state is present it has to be in the same schema as dst.\nvoid transform_counter_updates_to_shards(mutation& dst, const mutation* current_state, uint64_t clock_offset, locator::host_id local_id);\n\ntemplate<>\nstruct appending_hash<counter_shard_view> {\n    template<typename Hasher>\n    void operator()(Hasher& h, const counter_shard_view& cshard) const {\n        ::feed_hash(h, cshard.id());\n        ::feed_hash(h, cshard.value());\n        ::feed_hash(h, cshard.logical_clock());\n    }\n};\n\ntemplate<>\nstruct appending_hash<counter_cell_view> {\n    template<typename Hasher>\n    void operator()(Hasher& h, const counter_cell_view& cell) const {\n        ::feed_hash(h, true); // is_live\n        ::feed_hash(h, cell.timestamp());\n        for (auto&& csv : cell.shards()) {\n            ::feed_hash(h, csv);\n        }\n    }\n};\n"
        },
        {
          "name": "coverage_excludes.txt",
          "type": "blob",
          "size": 0.513671875,
          "content": "# This file contains regx patterns that should be\n# excluded from coverage reports. The file exists for easy\n# synchronization between different runs of coverage or different\n# frameworks running tests for unified coverage report.\n# The format is simple:\n# 1. lines that start with # are comments and should be ignored\n# 2. empty lines should be ignored\n# 3. every other line contains a single regx pattern of files to exclude\n# Note if the string should start with '#' simply put (#) instead.\n\n(.*/)?seastar/.*\n(.*/)?test/.*\n"
        },
        {
          "name": "coverage_sources.list",
          "type": "blob",
          "size": 0.28125,
          "content": "# Those tests are testing header files so we have no choice but to instrument them\nsource:test/boost/small_vector_test\\.cc=allow\nsource:test/boost/anchorless_list_test\\.cc=allow\n\n# Don't instrument files that are part of the testing framework itself.\nsource:test/*\\.*=skip\n\ndefault:allow\n"
        },
        {
          "name": "cql3",
          "type": "tree",
          "content": null
        },
        {
          "name": "cql_serialization_format.hh",
          "type": "blob",
          "size": 1.1552734375,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <iostream>\n#include <cstdint>\n#include <exception>\n\nusing cql_protocol_version_type = uint8_t;\n\n// Abstraction of transport protocol-dependent serialization format\n// Protocols v1, v2 used 16 bits for collection sizes, while v3 and\n// above use 32 bits.  But letting every bit of the code know what\n// transport protocol we're using (and in some cases, we aren't using\n// any transport -- it's for internal storage) is bad, so abstract it\n// away here.\n\nclass cql_serialization_format {\n    cql_protocol_version_type _version;\npublic:\n    static constexpr cql_protocol_version_type latest_version = 4;\n    explicit cql_serialization_format(cql_protocol_version_type version) : _version(version) {}\n    static cql_serialization_format latest() { return cql_serialization_format{latest_version}; }\n    cql_protocol_version_type protocol_version() const { return _version; }\n    void ensure_supported() const {\n        if (_version < 3) {\n            throw std::runtime_error(\"cql protocol version must be 3 or later\");\n        }\n    }\n};\n"
        },
        {
          "name": "data_dictionary",
          "type": "tree",
          "content": null
        },
        {
          "name": "db",
          "type": "tree",
          "content": null
        },
        {
          "name": "db_clock.hh",
          "type": "blob",
          "size": 2.134765625,
          "content": "/*\n * Copyright (C) 2014-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"clocks-impl.hh\"\n#include \"gc_clock.hh\"\n\n#include <chrono>\n#include <cstdint>\n#include <ratio>\n#include <type_traits>\n#include <fmt/chrono.h>\n\n// the database clock follows Java - 1ms granularity, 64-bit counter, 1970 epoch\n\nclass db_clock final {\npublic:\n    using base = std::chrono::system_clock;\n    using rep = int64_t;\n    using period = std::ratio<1, 1000>; // milliseconds\n    using duration = std::chrono::duration<rep, period>;\n    using time_point = std::chrono::time_point<db_clock, duration>;\n\n    static constexpr bool is_steady = base::is_steady;\n    static constexpr std::time_t to_time_t(time_point t) {\n        return std::chrono::duration_cast<std::chrono::seconds>(t.time_since_epoch()).count();\n    }\n    static constexpr time_point from_time_t(std::time_t t) {\n        return time_point(std::chrono::duration_cast<duration>(std::chrono::seconds(t)));\n    }\n    static time_point now() noexcept {\n        return time_point(std::chrono::duration_cast<duration>(base::now().time_since_epoch())) + get_clocks_offset();\n    }\n};\n\ninline\ngc_clock::time_point to_gc_clock(db_clock::time_point tp) noexcept {\n    // Converting time points through `std::time_t` means that we don't have to make any assumptions about the epochs\n    // of `gc_clock` and `db_clock`, though we require that that the period of `gc_clock` is also 1 s like\n    // `std::time_t` to avoid loss of information.\n    {\n        using second = std::ratio<1, 1>;\n        static_assert(\n                std::is_same<gc_clock::period, second>::value,\n                \"Conversion via std::time_t would lose information.\");\n    }\n\n    return gc_clock::from_time_t(db_clock::to_time_t(tp));\n}\n\n/* For debugging and log messages. */\ntemplate <>\nstruct fmt::formatter<db_clock::time_point> : fmt::formatter<string_view> {\n    template <typename FormatContext>\n    auto format(const db_clock::time_point& tp, FormatContext& ctx) const {\n        auto t = db_clock::to_time_t(tp);\n        return fmt::format_to(ctx.out(), \"{:%Y/%m/%d %T}\", fmt::gmtime(t));\n    }\n};\n"
        },
        {
          "name": "debug.cc",
          "type": "blob",
          "size": 0.22265625,
          "content": "/*\n * Copyright (C) 2023-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"debug.hh\"\n\nnamespace debug {\n\nseastar::sharded<replica::database>* the_database = nullptr;\n\n}\n"
        },
        {
          "name": "debug.hh",
          "type": "blob",
          "size": 0.2880859375,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <seastar/core/sharded.hh>\n\nnamespace replica {\nclass database;\n}\n\nnamespace debug {\n\nextern seastar::sharded<replica::database>* the_database;\n\n\n}\n\n"
        },
        {
          "name": "debug",
          "type": "tree",
          "content": null
        },
        {
          "name": "default.nix",
          "type": "blob",
          "size": 4.478515625,
          "content": "# Copyright (C) 2021-present ScyllaDB\n#\n\n#\n# SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n#\n\n#\n# * \"nix build\" is unsupported (or rather supported up to and not including\n#   installPhase), so basically all this is just for \"nix develop\"\n#\n# * IMPORTANT: to avoid using up ungodly amounts of disk space under\n#   /nix/store/ when you are not using flakes, make sure to move the\n#   actual build directory outside this tree and make ./build a\n#   symlink to it.  Or use flakes (seriously, just use flakes).\n#\n\n{ flake ? false\n, shell ? false\n, pkgs ? import <nixpkgs> { system = builtins.currentSystem; overlays = [ (import ./dist/nix/overlay.nix <nixpkgs>) ]; }\n, srcPath ? builtins.path { path = ./.; name = \"scylla\"; }\n, repl ? null\n, mode ? \"release\"\n, verbose ? false\n\n# shell env will want to add stuff to the environment, and the way\n# for it to do so is to pass us a function with this signatire:\n, devInputs ? ({ pkgs, llvm }: [])\n}:\n\nlet\n  inherit (builtins)\n    baseNameOf\n    head\n    match\n    split\n  ;\n\n  inherit (import (builtins.fetchTarball {\n    url = \"https://github.com/hercules-ci/gitignore/archive/5b9e0ff9d3b551234b4f3eb3983744fa354b17f1.tar.gz\";\n    sha256 = \"01l4phiqgw9xgaxr6jr456qmww6kzghqrnbc7aiiww3h6db5vw53\";\n  }) { inherit (pkgs) lib; })\n    gitignoreSource;\n\n  # all later Boost versions are problematic one way or another\n  boost = pkgs.boost175;\n\n  llvm = pkgs.llvmPackages_15;\n\n  stdenvUnwrapped = llvm.stdenv;\n\n  # define custom ccache- and distcc-aware wrappers for all relevant\n  # compile drivers (used only in shell env)\n  cc-wrappers = pkgs.callPackage ./dist/nix/pkg/custom/ccache-distcc-wrap {\n    cc = stdenvUnwrapped.cc;\n    clang = llvm.clang;\n    inherit (pkgs) gcc;\n  };\n\n  stdenv = if shell then pkgs.overrideCC stdenvUnwrapped cc-wrappers\n           else stdenvUnwrapped;\n\n  noNix = path: type: type != \"regular\" || (match \".*\\.nix\" path) == null;\n  src = builtins.filterSource noNix (if flake then srcPath\n                                     else gitignoreSource srcPath);\n\n  derive = if shell then pkgs.mkShell.override { inherit stdenv; }\n           else stdenv.mkDerivation;\n\nin derive ({\n  name = \"scylla\";\n  inherit src;\n\n  # since Scylla build, as it exists, is not cross-capable, the\n  # nativeBuildInputs/buildInputs distinction below ranges, depending\n  # on how charitable one feels, from \"pedantic\" through\n  # \"aspirational\" all the way to \"cargo cult ritual\" -- i.e. not\n  # expected to be actually correct or verifiable.  but it's the\n  # thought that counts!\n  nativeBuildInputs = with pkgs; [\n    ant\n    antlr3\n    boost\n    cargo\n    cmake\n    cxx-rs\n    gcc\n    openjdk11_headless\n    libtool\n    llvm.bintools\n    maven\n    ninja\n    pkg-config\n    python2\n    (python3.withPackages (ps: with ps; [\n      aiohttp\n      boto3\n      colorama\n      distro\n      magic\n      psutil\n      pyparsing\n      pytest\n      pytest-asyncio\n      pyudev\n      pyyaml\n      requests\n      scylla-driver\n      setuptools\n      tabulate\n      urwid\n    ]))\n    ragel\n    rustc\n    stow\n  ] ++ (devInputs { inherit pkgs llvm; });\n\n  buildInputs = with pkgs; [\n    abseil-cpp\n    antlr3\n    boost\n    c-ares\n    cryptopp\n    fmt\n    gmp\n    gnutls\n    hwloc\n    icu\n    jsoncpp\n    libdeflate\n    libidn2\n    libp11\n    libsystemtap\n    libtasn1\n    libunistring\n    liburing\n    libxcrypt\n    libxfs\n    libxml2\n    libyamlcpp\n    llvm.compiler-rt\n    lksctp-tools\n    lua54Packages.lua\n    lz4\n    nettle\n    numactl\n    openssl\n    p11-kit\n    protobuf\n    rapidjson\n    snappy\n    systemd\n    valgrind\n    xorg.libpciaccess\n    xxHash\n    zlib\n    zstd\n  ];\n\n  JAVA8_HOME = \"${pkgs.openjdk8_headless}/lib/openjdk\";\n  JAVA_HOME = \"${pkgs.openjdk11_headless}/lib/openjdk\";\n\n}\n// (if shell then {\n\n  configurePhase = \"./configure.py${if verbose then \" --verbose\" else \"\"} --disable-dpdk\";\n\n} else {\n\n  # sha256 of the filtered source tree:\n  SCYLLA_RELEASE = head (split \"-\" (baseNameOf src));\n\n  postPatch = ''\n    patchShebangs ./configure.py\n    patchShebangs ./seastar/scripts/seastar-json2code.py\n    patchShebangs ./seastar/cooking.sh\n  '';\n\n  configurePhase = \"./configure.py${if verbose then \" --verbose\" else \"\"} --mode=${mode}\";\n\n  buildPhase = ''\n    ${pkgs.ninja}/bin/ninja \\\n      build/${mode}/scylla \\\n      build/${mode}/iotune \\\n\n  '';\n   #   build/${mode}/dist/tar/scylla-tools-package.tar.gz \\\n   #   build/${mode}/dist/tar/scylla-jmx-package.tar.gz \\\n\n  installPhase = ''\n    echo not implemented 1>&2\n    exit 1\n  '';\n\n})\n// (if !shell || repl == null then {} else {\n\n  REPL = repl;\n\n})\n)\n"
        },
        {
          "name": "dht",
          "type": "tree",
          "content": null
        },
        {
          "name": "direct_failure_detector",
          "type": "tree",
          "content": null
        },
        {
          "name": "dist",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "duration.cc",
          "type": "blob",
          "size": 14.9052734375,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"duration.hh\"\n\n#include <boost/lexical_cast.hpp>\n#include <seastar/core/format.hh>\n\n#include <cctype>\n#include <optional>\n#include <limits>\n#include <boost/regex.hpp>\n#include <sstream>\n#include <string>\n#include <unordered_map>\n\nnamespace {\n\n//\n// Helper for retrieving the counter based on knowing its type.\n//\ntemplate<class Counter>\nconstexpr typename Counter::value_type& counter_ref(cql_duration &) noexcept;\n\ntemplate<>\nconstexpr months_counter::value_type& counter_ref<months_counter>(cql_duration &d) noexcept {\n    return d.months;\n}\n\ntemplate<>\nconstexpr days_counter::value_type& counter_ref<days_counter>(cql_duration &d) noexcept {\n    return d.days;\n}\n\ntemplate<>\nconstexpr nanoseconds_counter::value_type& counter_ref<nanoseconds_counter>(cql_duration &d) noexcept {\n    return d.nanoseconds;\n}\n\n// Unit for a component of a duration. For example, years.\nclass duration_unit {\npublic:\n    using index_type = uint8_t;\n    using common_counter_type = cql_duration::common_counter_type;\n\n    virtual ~duration_unit() = default;\n\n    // Units with larger indices are greater. For example, \"months\" have a greater index than \"days\".\n    virtual index_type index() const noexcept = 0;\n\n    virtual const char* short_name() const noexcept = 0;\n\n    virtual const char* long_name() const noexcept = 0;\n\n    // Increment the appropriate counter in the duration instance based on a count of this unit.\n    virtual void increment_count(cql_duration&, common_counter_type) const noexcept = 0;\n\n    // The remaining capacity (in terms of this unit) of the appropriate counter in the duration instance.\n    virtual common_counter_type available_count(const cql_duration&) const noexcept = 0;\n};\n\n// `_index` is the assigned index of this unit.\n// `Counter` is the counter type in the `cql_duration` instance that is used to store this unit.\n// `_factor` is the conversion factor of one count of this unit to the corresponding count in `Counter`.\ntemplate <uint8_t _index, class Counter, cql_duration::common_counter_type _factor>\nclass duration_unit_impl : public duration_unit {\npublic:\n    static constexpr auto factor = _factor;\n\n    virtual ~duration_unit_impl() = default;\n\n    index_type index() const noexcept override {\n        return _index;\n    }\n\n    void increment_count(cql_duration &d, common_counter_type c) const noexcept override {\n        counter_ref<Counter>(d) += (c * factor);\n    }\n\n    common_counter_type available_count(const cql_duration& d) const noexcept override {\n        const auto limit = std::numeric_limits<typename Counter::value_type>::max();\n        return {(limit - counter_ref<Counter>(const_cast<cql_duration&>(d))) / factor};\n    }\n};\n\nstruct nanosecond_unit final : public duration_unit_impl<0, nanoseconds_counter , 1> {\n    const char* short_name() const noexcept override { return \"ns\"; }\n    const char* long_name() const noexcept override { return \"nanoseconds\"; }\n} const nanosecond{};\n\nstruct microsecond_unit final : public duration_unit_impl<1, nanoseconds_counter, 1000> {\n    const char* short_name() const noexcept override { return \"us\"; }\n    const char* long_name() const noexcept override { return \"microseconds\"; }\n} const microsecond{};\n\nstruct millisecond_unit final : public duration_unit_impl<2, nanoseconds_counter, microsecond_unit::factor * 1000> {\n    const char* short_name() const noexcept override { return \"ms\"; }\n    const char* long_name() const noexcept override { return \"milliseconds\"; }\n} const millisecond{};\n\nstruct second_unit final : public duration_unit_impl<3, nanoseconds_counter, millisecond_unit::factor * 1000> {\n    const char* short_name() const noexcept override { return \"s\"; }\n    const char* long_name() const noexcept override { return \"seconds\"; }\n} const second{};\n\nstruct minute_unit final : public duration_unit_impl<4, nanoseconds_counter, second_unit::factor * 60> {\n    const char* short_name() const noexcept override { return \"m\"; }\n    const char* long_name() const noexcept override { return \"minutes\"; }\n} const minute{};\n\nstruct hour_unit final : public duration_unit_impl<5, nanoseconds_counter, minute_unit::factor * 60> {\n    const char* short_name() const noexcept override { return \"h\"; }\n    const char* long_name() const noexcept override { return \"hours\"; }\n} const hour{};\n\nstruct day_unit final : public duration_unit_impl<6, days_counter, 1> {\n    const char* short_name() const noexcept override { return \"d\"; }\n    const char* long_name() const noexcept override { return \"days\"; }\n} const day{};\n\nstruct week_unit final : public duration_unit_impl<7, days_counter, 7> {\n    const char* short_name() const noexcept override { return \"w\"; }\n    const char* long_name() const noexcept override { return \"weeks\"; }\n} const week{};\n\nstruct month_unit final : public duration_unit_impl<8, months_counter, 1> {\n    const char* short_name() const noexcept override { return \"mo\"; }\n    const char* long_name() const noexcept override { return \"months\"; }\n} const month{};\n\nstruct year_unit final : public duration_unit_impl<9, months_counter, 12> {\n    const char* short_name() const noexcept override { return \"y\"; }\n    const char* long_name() const noexcept override { return \"years\"; }\n} const year{};\n\nconst auto unit_table = std::unordered_map<std::string_view, std::reference_wrapper<const duration_unit>>{\n        {year.short_name(), year},\n        {month.short_name(), month},\n        {week.short_name(), week},\n        {day.short_name(), day},\n        {hour.short_name(), hour},\n        {minute.short_name(), minute},\n        {second.short_name(), second},\n        {millisecond.short_name(), millisecond},\n        {microsecond.short_name(), microsecond}, {\"s\", microsecond},\n        {nanosecond.short_name(), nanosecond}\n};\n\n//\n// Convenient helper to parse the indexed sub-expression from a match group as a duration counter.\n//\n// Throws `std::out_of_range` if a counter is out of range.\n//\ntemplate <class Match, class Index = typename Match::size_type>\ncql_duration::common_counter_type parse_count(const Match& m, Index group_index) {\n    try {\n        return boost::lexical_cast<cql_duration::common_counter_type>(m[group_index].str());\n    } catch (const boost::bad_lexical_cast&) {\n        throw std::out_of_range(\"duration counter\");\n    }\n}\n\n//\n// Build up a duration unit-by-unit.\n//\n// We support overflow detection on construction for convenience and compatibility with Cassandra.\n//\n// We maintain some additional state over a `cql_duration` in order to track the order in which components are added when\n// parsing the standard format.\n//\nclass duration_builder final {\npublic:\n    duration_builder& add(cql_duration::common_counter_type count, const duration_unit& unit) {\n        validate_addition(count, unit);\n        validate_and_update_order(unit);\n\n        unit.increment_count(_duration, count);\n        return *this;\n    }\n\n    template <class Match, class Index = typename Match::size_type>\n    duration_builder& add_parsed_count(const Match& m, Index group_index, const duration_unit& unit) {\n        cql_duration::common_counter_type count;\n\n        try {\n            count = parse_count(m, group_index);\n        } catch (const std::out_of_range&) {\n            throw cql_duration_error(seastar::format(\"Invalid duration. The count for the {} is out of range\", unit.long_name()));\n        }\n\n        return add(count, unit);\n    }\n\n    cql_duration build() const noexcept {\n        return _duration;\n    }\n\nprivate:\n    const duration_unit* _current_unit{nullptr};\n\n    cql_duration _duration{};\n\n    //\n    // Throws `cql_duration_error` if the addition of a quantity of the designated unit would overflow one of the\n    // counters.\n    //\n    void validate_addition(typename cql_duration::common_counter_type count, const duration_unit& unit) const {\n        const auto available = unit.available_count(_duration);\n\n        if (count > available) {\n            throw cql_duration_error(\n                    seastar::format(\"Invalid duration. The number of {} must be less than or equal to {}\",\n                           unit.long_name(),\n                           available));\n        }\n    }\n\n    //\n    // Validate that an addition of a quantity of the designated unit is not out of order. We require that units are\n    // added in decreasing size.\n    //\n    // This function also updates the last-observed unit for the next invocation.\n    //\n    // Throws `cql_duration_error` for order violations.\n    //\n    void validate_and_update_order(const duration_unit& unit) {\n        const auto index = unit.index();\n\n        if (_current_unit != nullptr) {\n            if (index == _current_unit->index()) {\n                throw cql_duration_error(seastar::format(\"Invalid duration. The {} are specified multiple times\", unit.long_name()));\n            } else if (index > _current_unit->index()) {\n                throw cql_duration_error(\n                        seastar::format(\"Invalid duration. The {} should be after {}\",\n                               _current_unit->long_name(),\n                               unit.long_name()));\n            }\n        }\n\n        _current_unit = &unit;\n    }\n};\n\n//\n// These functions assume no sign information ('-). That is left to the `cql_duration` constructor.\n//\n\nstd::optional<cql_duration> parse_duration_standard_format(std::string_view s) {\n\n    //\n    // We parse one component (pair of a count and unit) at a time in order to give more precise error messages when\n    // units are specified multiple times or out of order rather than just \"parse error\".\n    //\n    // The other formats are more strict and complain less helpfully.\n    //\n\n    static const auto pattern =\n            boost::regex(\"(\\\\d+)(y|Y|mo|MO|mO|Mo|w|W|d|D|h|H|s|S|ms|MS|mS|Ms|us|US|uS|Us|s|S|ns|NS|nS|Ns|m|M)\");\n\n    auto iter = s.cbegin();\n    boost::cmatch match;\n\n    duration_builder b;\n\n    // `match_continuous` ensures that the entire string must be included in a match.\n    while (boost::regex_search(iter, s.end(), match, pattern, boost::regex_constants::match_continuous)) {\n        iter += match.length();\n\n        auto symbol = match[2].str();\n\n        // Special case for mu.\n        {\n            auto view = std::string_view(symbol);\n            view.remove_suffix(1);\n\n            if (view == reinterpret_cast<const char*>(u8\"\")) {\n                b.add_parsed_count(match, 1, microsecond);\n                continue;\n            }\n        }\n\n        // Otherwise, we can just convert to lower-case for look-up.\n        std::transform(symbol.begin(), symbol.end(), symbol.begin(), [](char ch) { return std::tolower(ch); });\n        b.add_parsed_count(match, 1, unit_table.at(symbol));\n    }\n\n    if (iter != s.cend()) {\n        // There is unconsumed input.\n        return {};\n    }\n\n    return b.build();\n}\n\nstd::optional<cql_duration> parse_duration_iso8601_format(std::string_view s) {\n    static const auto pattern = boost::regex(\"P((\\\\d+)Y)?((\\\\d+)M)?((\\\\d+)D)?(T((\\\\d+)H)?((\\\\d+)M)?((\\\\d+)S)?)?\");\n\n    boost::cmatch match;\n    if (!boost::regex_match(s.data(), match, pattern)) {\n        return {};\n    }\n\n    duration_builder b;\n\n    if (match[1].matched) {\n        b.add_parsed_count(match, 2, year);\n    }\n\n    if (match[3].matched) {\n        b.add_parsed_count(match, 4, month);\n    }\n\n    if (match[5].matched) {\n        b.add_parsed_count(match, 6, day);\n    }\n\n    // Optional, more granular, information.\n    if (match[7].matched) {\n        if (match[8].matched) {\n            b.add_parsed_count(match, 9, hour);\n        }\n\n        if (match[10].matched) {\n            b.add_parsed_count(match, 11, minute);\n        }\n\n        if (match[12].matched) {\n            b.add_parsed_count(match, 13, second);\n        }\n    }\n\n    return b.build();\n}\n\nstd::optional<cql_duration> parse_duration_iso8601_alternative_format(std::string_view s) {\n    static const auto pattern = boost::regex(\"P(\\\\d{4})-(\\\\d{2})-(\\\\d{2})T(\\\\d{2}):(\\\\d{2}):(\\\\d{2})\");\n\n    boost::cmatch match;\n    if (!boost::regex_match(s.data(), match, pattern)) {\n        return {};\n    }\n\n    return duration_builder()\n            .add_parsed_count(match, 1, year)\n            .add_parsed_count(match, 2, month)\n            .add_parsed_count(match, 3, day)\n            .add_parsed_count(match, 4, hour)\n            .add_parsed_count(match, 5, minute)\n            .add_parsed_count(match, 6, second)\n            .build();\n}\n\nstd::optional<cql_duration> parse_duration_iso8601_week_format(std::string_view s) {\n    static const auto pattern = boost::regex(\"P(\\\\d+)W\");\n\n    boost::cmatch match;\n    if (!boost::regex_match(s.data(), match, pattern)) {\n        return {};\n    }\n\n    return duration_builder()\n            .add_parsed_count(match, 1, week)\n            .build();\n}\n\n// Parse a duration string without sign information assuming one of the supported formats.\nstd::optional<cql_duration> parse_duration(std::string_view s) {\n    if (s.length() == 0u) {\n        return {};\n    }\n\n    if (s.front() == 'P') {\n        if (s.back() == 'W') {\n            return parse_duration_iso8601_week_format(s);\n        }\n\n        if (s.find('-') != s.npos) {\n            return parse_duration_iso8601_alternative_format(s);\n        }\n\n        return parse_duration_iso8601_format(s);\n    }\n\n    return parse_duration_standard_format(s);\n}\n\n}\n\ncql_duration::cql_duration(std::string_view s) {\n    const bool is_negative = (s.length() != 0) && (s[0] == '-');\n\n    // Without any sign indicator ('-').\n    const auto ps = (is_negative ? s.cbegin() + 1 : s.cbegin());\n\n    const auto d = parse_duration(ps);\n    if (!d) {\n        throw cql_duration_error(seastar::format(\"Unable to convert '{}' to a duration\", s));\n    }\n\n    *this = *d;\n\n    if (is_negative) {\n        months = -months;\n        days = -days;\n        nanoseconds = -nanoseconds;\n    }\n}\n\nstd::ostream& operator<<(std::ostream& os, const cql_duration& d) {\n    if ((d.months < 0) || (d.days < 0) || (d.nanoseconds < 0)) {\n        os << '-';\n    }\n\n    // If a non-zero integral component of the count can be expressed in `unit`, then append it to the stream with its\n    // unit.\n    //\n    // Returns the remaining count.\n    const auto append = [&os](cql_duration::common_counter_type count, auto&& unit) {\n        const auto divider = unit.factor;\n\n        if ((count == 0) || (count < divider)) {\n            return count;\n        }\n\n        os << (count / divider) << unit.short_name();\n        return count % divider;\n    };\n\n    const auto month_remainder = append(std::abs(d.months), year);\n    append(month_remainder, month);\n\n    append(std::abs(d.days), day);\n\n    auto nanosecond_remainder = append(std::abs(d.nanoseconds), hour);\n    nanosecond_remainder = append(nanosecond_remainder, minute);\n    nanosecond_remainder = append(nanosecond_remainder, second);\n    nanosecond_remainder = append(nanosecond_remainder, millisecond);\n    nanosecond_remainder = append(nanosecond_remainder, microsecond);\n    append(nanosecond_remainder, nanosecond);\n\n    return os;\n}\n\nseastar::sstring to_string(const cql_duration& d) {\n    std::ostringstream ss;\n    ss << d;\n    return ss.str();\n}\n"
        },
        {
          "name": "duration.hh",
          "type": "blob",
          "size": 4.4501953125,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <seastar/core/sstring.hh>\n\n#include <cstdint>\n#include <string_view>\n#include <ostream>\n#include <stdexcept>\n\n// Wrapper for a value with a type-tag for differentiating instances.\ntemplate <class Value, class Tag>\nclass cql_duration_counter final {\npublic:\n    using value_type = Value;\n\n    explicit constexpr cql_duration_counter(value_type count) noexcept : _count(count) {}\n\n    constexpr operator value_type() const noexcept { return _count; }\nprivate:\n    value_type _count;\n};\n\nusing months_counter = cql_duration_counter<int32_t, struct month_tag>;\nusing days_counter = cql_duration_counter<int32_t, struct day_tag>;\nusing nanoseconds_counter = cql_duration_counter<int64_t, struct nanosecond_tag>;\n\nclass cql_duration_error : public std::invalid_argument {\npublic:\n    explicit cql_duration_error(std::string_view what) : std::invalid_argument(what.data()) {}\n\n    virtual ~cql_duration_error() = default;\n};\n\n//\n// A duration of time.\n//\n// Three counters represent the time: the number of months, of days, and of nanoseconds. This is necessary because\n// the number hours in a day can vary during daylight savings and because the number of days in a month vary.\n//\n// As a consequence of this representation, there can exist no total ordering relation on durations. To see why,\n// consider a duration `1mo5s` (1 month and 5 seconds). In a month with 30 days, this represents a smaller duration of\n// time than in a month with 31 days.\n//\n// The primary use of this type is to manipulate absolute time-stamps with relative offsets. For example,\n// `\"Jan. 31 2005 at 23:15\" + 3mo5d`.\n//\nclass cql_duration final {\npublic:\n    using common_counter_type = int64_t;\n\n    static_assert(\n            (sizeof(common_counter_type) >= sizeof(months_counter::value_type)) &&\n            (sizeof(common_counter_type) >= sizeof(days_counter::value_type)) &&\n            (sizeof(common_counter_type) >= sizeof(nanoseconds_counter::value_type)),\n            \"The common counter type is smaller than one of the component counter types.\");\n\n    // A zero-valued duration.\n    constexpr cql_duration() noexcept = default;\n\n    // Construct a duration with explicit values for its three counters.\n    constexpr cql_duration(months_counter m, days_counter d, nanoseconds_counter n) noexcept :\n            months(m),\n            days(d),\n            nanoseconds(n) {}\n\n    //\n    // Parse a duration string.\n    //\n    // Three formats for durations are supported:\n    //\n    // 1. \"Standard\" format. This consists of one or more pairs of a count and a unit specifier. Examples are \"23d1mo\"\n    //    and \"5h23m10s\". Components of the total duration must be written in decreasing order. That is, \"5h2y\" is\n    //    an invalid duration string.\n    //\n    //    The allowed units are:\n    //      - \"y\": years\n    //      - \"mo\": months\n    //      - \"w\": weeks\n    //      - \"d\": days\n    //      - \"h\": hours\n    //      - \"m\": minutes\n    //      - \"s\": seconds\n    //      - \"ms\": milliseconds\n    //      - \"us\" or \"s\": microseconds\n    //      - \"ns\": nanoseconds\n    //\n    //    Units are case-insensitive.\n    //\n    // 2. ISO-8601 format. \"P[n]Y[n]M[n]DT[n]H[n]M[n]S\" or \"P[n]W\". All specifiers are optional. Examples are\n    //    \"P23Y1M\" or \"P10W\".\n    //\n    // 3. ISO-8601 alternate format. \"P[YYYY]-[MM]-[DD]T[hh]:[mm]:[ss]\". All specifiers are mandatory. An example is\n    //    \"P2000-10-14T07:22:30\".\n    //\n    // For all formats, a negative duration is indicated by beginning the string with the '-' symbol. For example,\n    // \"-2y10ns\".\n    //\n    // Throws `cql_duration_error` in the event of a parsing error.\n    //\n    explicit cql_duration(std::string_view s);\n\n    months_counter::value_type months{0};\n    days_counter::value_type days{0};\n    nanoseconds_counter::value_type nanoseconds{0};\n\n    //\n    // Note that equality comparison is based on exact counter matches. It is not valid to expect equivalency across\n    // counters like months and days. See the documentation for `duration` for more.\n    //\n    friend bool operator==(const cql_duration&, const cql_duration&) noexcept = default;\n};\n\n//\n// Pretty-print a duration using the standard format.\n//\n// Durations are simplified during printing so that `duration(24, 0, 0)` is printed as \"2y\".\n//\nstd::ostream& operator<<(std::ostream& os, const cql_duration& d);\n\n// See above.\nseastar::sstring to_string(const cql_duration&);\n"
        },
        {
          "name": "encoding_stats.hh",
          "type": "blob",
          "size": 2.7939453125,
          "content": "/*\n * Copyright (C) 2018-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"gc_clock.hh\"\n#include \"timestamp.hh\"\n#include \"utils/extremum_tracking.hh\"\n\n// Stores statistics on all the updates done to a memtable\n// The collected statistics are used for flushing memtable to the disk\nstruct encoding_stats {\n\n    // The fixed epoch corresponds to the one used by Origin - 22/09/2015, 00:00:00, GMT-0:\n    //        Calendar c = Calendar.getInstance(TimeZone.getTimeZone(\"GMT-0\"), Locale.US);\n    //        c.set(Calendar.YEAR, 2015);\n    //        c.set(Calendar.MONTH, Calendar.SEPTEMBER);\n    //        c.set(Calendar.DAY_OF_MONTH, 22);\n    //        c.set(Calendar.HOUR_OF_DAY, 0);\n    //        c.set(Calendar.MINUTE, 0);\n    //        c.set(Calendar.SECOND, 0);\n    //        c.set(Calendar.MILLISECOND, 0);\n    //\n    //        long TIMESTAMP_EPOCH = c.getTimeInMillis() * 1000; // timestamps should be in microseconds by convention\n    //        int DELETION_TIME_EPOCH = (int)(c.getTimeInMillis() / 1000); // local deletion times are in seconds\n    // Encoding stats are used for delta-encoding, so we want some default values\n    // that are just good enough so we take some recent date in the past\n    static constexpr int32_t deletion_time_epoch = 1442880000;\n    static constexpr api::timestamp_type timestamp_epoch = api::timestamp_type(deletion_time_epoch) * 1000 * 1000;\n    static constexpr int32_t ttl_epoch = 0;\n\n    api::timestamp_type min_timestamp = timestamp_epoch;\n    gc_clock::time_point min_local_deletion_time = gc_clock::time_point(gc_clock::duration(deletion_time_epoch));\n    gc_clock::duration min_ttl = gc_clock::duration(ttl_epoch);\n};\n\nclass encoding_stats_collector {\nprivate:\n    min_tracker<api::timestamp_type> min_timestamp;\n    min_tracker<gc_clock::time_point> min_local_deletion_time;\n    min_tracker<gc_clock::duration> min_ttl;\n\nprotected:\n    void update_timestamp(api::timestamp_type ts) noexcept {\n        min_timestamp.update(ts);\n    }\n\n    void update_local_deletion_time(gc_clock::time_point local_deletion_time) noexcept {\n        min_local_deletion_time.update(local_deletion_time);\n    }\n\n    void update_ttl(gc_clock::duration ttl) noexcept {\n        min_ttl.update(ttl);\n    }\n\npublic:\n    encoding_stats_collector() noexcept\n        : min_timestamp(api::max_timestamp)\n        , min_local_deletion_time(gc_clock::time_point::max())\n        , min_ttl(gc_clock::duration::max())\n    {}\n\n    void update(const encoding_stats& other) noexcept {\n        update_timestamp(other.min_timestamp);\n        update_local_deletion_time(other.min_local_deletion_time);\n        update_ttl(other.min_ttl);\n    }\n\n    encoding_stats get() const noexcept {\n        return { min_timestamp.get(), min_local_deletion_time.get(), min_ttl.get() };\n    }\n};\n"
        },
        {
          "name": "enum_set.hh",
          "type": "blob",
          "size": 7.6162109375,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"utils/assert.hh\"\n#include <boost/iterator/transform_iterator.hpp>\n#include <seastar/core/bitset-iter.hh>\n\n#include <algorithm>\n#include <cstddef>\n#include <optional>\n#include <stdexcept>\n#include <type_traits>\n#include <limits>\n\n/**\n *\n * Allows to take full advantage of compile-time information when operating\n * on a set of enum values.\n *\n * Examples:\n *\n *   enum class x { A, B, C };\n *   using my_enum = super_enum<x, x::A, x::B, x::C>;\n *   using my_enumset = enum_set<my_enum>;\n *\n *   static_assert(my_enumset::frozen<x::A, x::B>::contains<x::A>(), \"it should...\");\n *\n *   SCYLLA_ASSERT(my_enumset::frozen<x::A, x::B>::contains(my_enumset::prepare<x::A>()));\n *\n *   SCYLLA_ASSERT(my_enumset::frozen<x::A, x::B>::contains(x::A));\n *\n */\n\n\ntemplate<typename EnumType, EnumType... Items>\nstruct super_enum {\n    using enum_type = EnumType;\n\n    template<enum_type... values>\n    struct max {\n        static constexpr enum_type max_of(enum_type a, enum_type b) {\n            return a > b ? a : b;\n        }\n\n        template<enum_type first, enum_type second, enum_type... rest>\n        static constexpr enum_type get() {\n            return max_of(first, get<second, rest...>());\n        }\n\n        template<enum_type first>\n        static constexpr enum_type get() { return first; }\n\n        static constexpr enum_type value = get<values...>();\n    };\n\n    template<enum_type... values>\n    struct min {\n        static constexpr enum_type min_of(enum_type a, enum_type b) {\n            return a < b ? a : b;\n        }\n\n        template<enum_type first, enum_type second, enum_type... rest>\n        static constexpr enum_type get() {\n            return min_of(first, get<second, rest...>());\n        }\n\n        template<enum_type first>\n        static constexpr enum_type get() { return first; }\n\n        static constexpr enum_type value = get<values...>();\n    };\n\n    using sequence_type = typename std::underlying_type<enum_type>::type;\n\n    template <enum_type first, enum_type... rest>\n    struct valid_sequence {\n        static constexpr bool apply(sequence_type v) noexcept {\n            return (v == static_cast<sequence_type>(first)) || valid_sequence<rest...>::apply(v);\n        }\n    };\n\n    template <enum_type first>\n    struct valid_sequence<first> {\n        static constexpr bool apply(sequence_type v) noexcept {\n            return v == static_cast<sequence_type>(first);\n        }\n    };\n\n    static constexpr bool is_valid_sequence(sequence_type v) noexcept {\n        return valid_sequence<Items...>::apply(v);\n    }\n\n    template<enum_type Elem>\n    static constexpr sequence_type sequence_for() {\n        return static_cast<sequence_type>(Elem);\n    }\n\n    static sequence_type sequence_for(enum_type elem) {\n        return static_cast<sequence_type>(elem);\n    }\n\n    static constexpr sequence_type max_sequence = sequence_for<max<Items...>::value>();\n    static constexpr sequence_type min_sequence = sequence_for<min<Items...>::value>();\n\n    static_assert(min_sequence >= 0, \"negative enum values unsupported\");\n};\n\nclass bad_enum_set_mask : public std::invalid_argument {\npublic:\n    bad_enum_set_mask() : std::invalid_argument(\"Bit mask contains invalid enumeration indices.\") {\n    }\n};\n\ntemplate<typename Enum>\nclass enum_set {\npublic:\n    using mask_type = size_t; // TODO: use the smallest sufficient type\n    using enum_type = typename Enum::enum_type;\n\nprivate:\n    static constexpr int mask_digits = std::numeric_limits<mask_type>::digits;\n    using mask_iterator = seastar::bitsets::set_iterator<mask_digits>;\n\n    mask_type _mask;\n    constexpr enum_set(mask_type mask) : _mask(mask) {}\n\n    template<enum_type Elem>\n    static constexpr unsigned shift_for() {\n        return Enum::template sequence_for<Elem>();\n    }\n\n    static auto make_iterator(mask_iterator iter) {\n        return boost::make_transform_iterator(std::move(iter), [](typename Enum::sequence_type s) {\n            return enum_type(s);\n        });\n    }\n\npublic:\n    using iterator = std::invoke_result_t<decltype(&enum_set::make_iterator), mask_iterator>;\n\n    constexpr enum_set() : _mask(0) {}\n\n    /**\n     * \\throws \\ref bad_enum_set_mask\n     */\n    static constexpr enum_set from_mask(mask_type mask) {\n        const auto bit_range = seastar::bitsets::for_each_set(std::bitset<mask_digits>(mask));\n\n        if (!std::all_of(bit_range.begin(), bit_range.end(), &Enum::is_valid_sequence)) {\n            throw bad_enum_set_mask();\n        }\n\n        return enum_set(mask);\n    }\n\n    static constexpr mask_type full_mask() {\n        return ~(std::numeric_limits<mask_type>::max() << (Enum::max_sequence + 1));\n    }\n\n    static constexpr enum_set full() {\n        return enum_set(full_mask());\n    }\n\n    static inline mask_type mask_for(enum_type e) {\n        return mask_type(1) << Enum::sequence_for(e);\n    }\n\n    template<enum_type Elem>\n    static constexpr mask_type mask_for() {\n        return mask_type(1) << shift_for<Elem>();\n    }\n\n    struct prepared {\n        mask_type mask;\n        bool operator==(const prepared& o) const {\n            return mask == o.mask;\n        }\n    };\n\n    static prepared prepare(enum_type e) {\n        return {mask_for(e)};\n    }\n\n    template<enum_type e>\n    static constexpr prepared prepare() {\n        return {mask_for<e>()};\n    }\n\n    static_assert(std::numeric_limits<mask_type>::max() >= ((size_t)1 << Enum::max_sequence), \"mask type too small\");\n\n    template<enum_type e>\n    bool contains() const {\n        return bool(_mask & mask_for<e>());\n    }\n\n    bool contains(enum_type e) const {\n        return bool(_mask & mask_for(e));\n    }\n\n    template<enum_type e>\n    void remove() {\n        _mask &= ~mask_for<e>();\n    }\n\n    void remove(enum_type e) {\n        _mask &= ~mask_for(e);\n    }\n\n    template<enum_type e>\n    void set() {\n        _mask |= mask_for<e>();\n    }\n\n    template<enum_type e>\n    void set_if(bool condition) {\n        _mask |= mask_type(condition) << shift_for<e>();\n    }\n\n    void set(enum_type e) {\n        _mask |= mask_for(e);\n    }\n\n    template<enum_type e>\n    void toggle() {\n        _mask ^= mask_for<e>();\n    }\n\n    void toggle(enum_type e) {\n        _mask ^= mask_for(e);\n    }\n\n    void add(const enum_set& other) {\n        _mask |= other._mask;\n    }\n\n    explicit operator bool() const {\n        return bool(_mask);\n    }\n\n    mask_type mask() const {\n        return _mask;\n    }\n\n    iterator begin() const {\n        return make_iterator(mask_iterator(_mask));\n    }\n\n    iterator end() const {\n        return make_iterator(mask_iterator(0));\n    }\n\n    template<enum_type... items>\n    struct frozen {\n        template<enum_type first>\n        static constexpr mask_type make_mask() {\n            return mask_for<first>();\n        }\n\n        static constexpr mask_type make_mask() {\n            return 0;\n        }\n\n        template<enum_type first, enum_type second, enum_type... rest>\n        static constexpr mask_type make_mask() {\n            return mask_for<first>() | make_mask<second, rest...>();\n        }\n\n        static constexpr mask_type mask = make_mask<items...>();\n\n        template<enum_type Elem>\n        static constexpr bool contains() {\n            return mask & mask_for<Elem>();\n        }\n\n        static bool contains(enum_type e) {\n            return mask & mask_for(e);\n        }\n\n        static bool contains(prepared e) {\n            return mask & e.mask;\n        }\n\n        static constexpr enum_set<Enum> unfreeze() {\n            return enum_set<Enum>(mask);\n        }\n    };\n\n    template<enum_type... items>\n    static constexpr enum_set<Enum> of() {\n        return frozen<items...>::unfreeze();\n    }\n};\n"
        },
        {
          "name": "exceptions",
          "type": "tree",
          "content": null
        },
        {
          "name": "fix_system_distributed_tables.py",
          "type": "blob",
          "size": 4.625,
          "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# Copyright 2017-present ScyllaDB\n#\n#\n# SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n#\nimport argparse\nimport sys\n\nfrom cassandra.cluster import Cluster\nfrom cassandra.auth import PlainTextAuthProvider\n\nevents_cols = {\n    'session_id': 'uuid',\n    'event_id': 'timeuuid',\n    'activity': 'text',\n    'source': 'inet',\n    'source_elapsed': 'int',\n    'thread': 'text',\n    'scylla_span_id': 'bigint',\n    'scylla_parent_id': 'bigint'\n}\n\nsessions_cols = {\n    'session_id': 'uuid',\n    'command': 'text',\n    'client': 'inet',\n    'coordinator': 'inet',\n    'duration': 'int',\n    'parameters': 'map<text, text>',\n    'request': 'text',\n    'started_at': 'timestamp',\n    'request_size': 'int',\n    'response_size': 'int',\n    'username': 'text'\n}\n\nslow_query_log_cols = {\n    'node_ip': 'inet',\n    'shard': 'int',\n    'session_id': 'uuid',\n    'date': 'timestamp',\n    'start_time': 'timeuuid',\n    'command': 'text',\n    'duration': 'int',\n    'parameters': 'map<text, text>',\n    'source_ip': 'inet',\n    'table_names': 'set<text>',\n    'username': 'text'\n}\n\ntraces_tables_defs = {\n    'events': events_cols,\n    'sessions': sessions_cols,\n    'node_slow_log': slow_query_log_cols\n}\n################################################################################\ncredentials_cols = {\n    'username': 'text',\n    'options': 'map<text, text>',\n    'salted_hash': 'text'\n}\n\npermissions_cols = {\n    'username': 'text',\n    'resource': 'text',\n    'permissions': 'set<text>'\n}\n\nusers_cols = {\n    'name': 'text',\n    'super': 'boolean'\n}\n\nauth_tables_defs = {\n    'credentials': credentials_cols,\n    'permissions': permissions_cols,\n    'users': users_cols\n}\n################################################################################\nks_defs = {\n    'system_traces': traces_tables_defs,\n    'system_auth': auth_tables_defs\n}\n################################################################################\n\n\ndef validate_and_fix(args):\n    res = True\n    if args.user:\n        auth_provider = PlainTextAuthProvider(username=args.user, password=args.password)\n        cluster = Cluster(auth_provider=auth_provider, contact_points=[args.node], port=args.port)\n    else:\n        cluster = Cluster(contact_points=[args.node], port=args.port)\n\n    try:\n        session = cluster.connect()\n        cluster_meta = session.cluster.metadata\n        for ks, tables_defs in ks_defs.items():\n            if ks not in cluster_meta.keyspaces:\n                print(\"keyspace {} doesn't exist - skipping\".format(ks))\n                continue\n\n            ks_meta = cluster_meta.keyspaces[ks]\n            for table_name, table_cols in tables_defs.items():\n\n                if table_name not in ks_meta.tables:\n                    print(\"{}.{} doesn't exist - skipping\".format(ks, table_name))\n                    continue\n\n                print(\"Adjusting {}.{}\".format(ks, table_name))\n\n                table_meta = ks_meta.tables[table_name]\n                for column_name, column_type in table_cols.items():\n                    if column_name in table_meta.columns:\n                        column_meta = table_meta.columns[column_name]\n                        if column_meta.cql_type != column_type:\n                            print(\"ERROR: {}.{}::{} column has an unexpected column type: expected '{}' found '{}'\".format(ks, table_name, column_name, column_type, column_meta.cql_type))\n                            res = False\n                    else:\n                        try:\n                            session.execute(\"ALTER TABLE {}.{} ADD {} {}\".format(ks, table_name, column_name, column_type))\n                            print(\"{}.{}: added column '{}' of the type '{}'\".format(ks, table_name, column_name, column_type))\n                        except Exception:\n                            print(\"ERROR: {}.{}: failed to add column '{}' with type '{}': {}\".format(ks, table_name, column_name, column_type, sys.exc_info()))\n                            res = False\n    except Exception:\n        print(\"ERROR: {}\".format(sys.exc_info()))\n        res = False\n\n    return res\n\n\n################################################################################\nif __name__ == '__main__':\n    argp = argparse.ArgumentParser(description='Validate distributed system keyspaces')\n    argp.add_argument('--user', '-u')\n    argp.add_argument('--password', '-p', default='none')\n    argp.add_argument('--node', default='127.0.0.1', help='Node to connect to.')\n    argp.add_argument('--port', default=9042, help='Port to connect to.', type=int)\n\n    args = argp.parse_args()\n    res = validate_and_fix(args)\n    if res:\n        sys.exit(0)\n    else:\n        sys.exit(1)\n"
        },
        {
          "name": "flake.lock",
          "type": "blob",
          "size": 1.0029296875,
          "content": "{\n  \"nodes\": {\n    \"flake-utils\": {\n      \"locked\": {\n        \"lastModified\": 1667395993,\n        \"narHash\": \"sha256-nuEHfE/LcWyuSWnS8t12N1wc105Qtau+/OdUAjtQ0rA=\",\n        \"owner\": \"numtide\",\n        \"repo\": \"flake-utils\",\n        \"rev\": \"5aed5285a952e0b949eb3ba02c12fa4fcfef535f\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"numtide\",\n        \"repo\": \"flake-utils\",\n        \"type\": \"github\"\n      }\n    },\n    \"nixpkgs\": {\n      \"locked\": {\n        \"lastModified\": 1673947312,\n        \"narHash\": \"sha256-xx/2nRwRy3bXrtry6TtydKpJpqHahjuDB5sFkQ/XNDE=\",\n        \"owner\": \"NixOS\",\n        \"repo\": \"nixpkgs\",\n        \"rev\": \"2d38b664b4400335086a713a0036aafaa002c003\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"NixOS\",\n        \"ref\": \"nixpkgs-unstable\",\n        \"repo\": \"nixpkgs\",\n        \"type\": \"github\"\n      }\n    },\n    \"root\": {\n      \"inputs\": {\n        \"flake-utils\": \"flake-utils\",\n        \"nixpkgs\": \"nixpkgs\"\n      }\n    }\n  },\n  \"root\": \"root\",\n  \"version\": 7\n}\n"
        },
        {
          "name": "flake.nix",
          "type": "blob",
          "size": 1.2412109375,
          "content": "{\n  description = \"Monstrously Fast + Scalable NoSQL\";\n\n  inputs = {\n    nixpkgs.url = \"github:NixOS/nixpkgs/nixpkgs-unstable\";\n    flake-utils.url = \"github:numtide/flake-utils\";\n  };\n\n  outputs = { self, nixpkgs, flake-utils }: {\n    overlays.default = import ./dist/nix/overlay.nix nixpkgs;\n\n    lib = {\n      _attrs = system: let\n        pkgs = import nixpkgs {\n          inherit system;\n          overlays = [ self.overlays.default ];\n        };\n\n        repl = pkgs.writeText \"repl\" ''\n          let\n            self = builtins.getFlake (toString ${self.outPath});\n            attrs = self.lib._attrs \"${system}\";\n          in {\n            inherit self;\n            inherit (attrs) pkgs;\n          }\n        '';\n\n        args = {\n          flake = true;\n          srcPath = \"${self}\";\n          inherit pkgs repl;\n        };\n\n        package = import ./default.nixpkgs args;\n        devShell = import ./shell.nix args;\n      in {\n        inherit pkgs args package devShell;\n      };\n    };\n  }\n  // (flake-utils.lib.eachDefaultSystem (system: let\n    packageName = \"scylla\";\n    attrs = self.lib._attrs system;\n  in {\n    packages.${packageName} = attrs.package;\n    defaultPackage = self.packages.${system}.${packageName};\n\n    inherit (attrs) devShell;\n  }));\n}\n"
        },
        {
          "name": "frozen_schema.cc",
          "type": "blob",
          "size": 1.2919921875,
          "content": "/*\n * Copyright 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"frozen_schema.hh\"\n#include \"db/schema_tables.hh\"\n#include \"mutation/canonical_mutation.hh\"\n#include \"schema_mutations.hh\"\n#include \"idl/frozen_schema.dist.hh\"\n#include \"idl/frozen_schema.dist.impl.hh\"\n\nfrozen_schema::frozen_schema(const schema_ptr& s)\n    : _data([&s] {\n        schema_mutations sm = db::schema_tables::make_schema_mutations(s, api::new_timestamp(), true);\n        bytes_ostream out;\n        ser::writer_of_schema<bytes_ostream> wr(out);\n        std::move(wr).write_version(s->version())\n                     .write_mutations(sm)\n                     .end_schema();\n        return out;\n    }())\n{ }\n\nschema_ptr frozen_schema::unfreeze(const db::schema_ctxt& ctxt) const {\n    auto in = ser::as_input_stream(_data);\n    auto sv = ser::deserialize(in, std::type_identity<ser::schema_view>());\n    return sv.mutations().is_view()\n         ? db::schema_tables::create_view_from_mutations(ctxt, sv.mutations(), sv.version())\n         : db::schema_tables::create_table_from_mutations(ctxt, sv.mutations(), sv.version());\n}\n\nfrozen_schema::frozen_schema(bytes_ostream b)\n    : _data(std::move(b))\n{ }\n\nconst bytes_ostream& frozen_schema::representation() const\n{\n    return _data;\n}\n"
        },
        {
          "name": "frozen_schema.hh",
          "type": "blob",
          "size": 0.8125,
          "content": "/*\n * Copyright 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"schema/schema_fwd.hh\"\n#include \"mutation/frozen_mutation.hh\"\n#include \"bytes_ostream.hh\"\n\nnamespace db {\nclass schema_ctxt;\n}\n\n// Transport for schema_ptr across shards/nodes.\n// It's safe to access from another shard by const&.\nclass frozen_schema {\n    bytes_ostream _data;\npublic:\n    explicit frozen_schema(bytes_ostream);\n    frozen_schema(const schema_ptr&);\n    frozen_schema(frozen_schema&&) = default;\n    frozen_schema(const frozen_schema&) = default;\n    frozen_schema& operator=(const frozen_schema&) = default;\n    frozen_schema& operator=(frozen_schema&&) = default;\n    schema_ptr unfreeze(const db::schema_ctxt&) const;\n    const bytes_ostream& representation() const;\n};\n"
        },
        {
          "name": "full_position.hh",
          "type": "blob",
          "size": 1.5947265625,
          "content": "/*\n * Copyright (C) 2022-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"keys.hh\"\n#include \"mutation/position_in_partition.hh\"\n\nstruct full_position;\n\nstruct full_position_view {\n    const partition_key_view partition;\n    const position_in_partition_view position;\n\n    full_position_view(const full_position&);\n    full_position_view(const partition_key&, const position_in_partition_view);\n};\n\nstruct full_position {\n    partition_key partition;\n    position_in_partition position;\n\n    full_position(full_position_view);\n    full_position(partition_key, position_in_partition);\n\n    operator full_position_view() {\n        return full_position_view(partition, position);\n    }\n\n    static std::strong_ordering cmp(const schema& s, const full_position& a, const full_position& b) {\n        partition_key::tri_compare pk_cmp(s);\n        if (auto res = pk_cmp(a.partition, b.partition); res != 0) {\n            return res;\n        }\n        position_in_partition::tri_compare pos_cmp(s);\n        return pos_cmp(a.position, b.position);\n    }\n};\n\ninline full_position_view::full_position_view(const full_position& fp) : partition(fp.partition), position(fp.position) { }\ninline full_position_view::full_position_view(const partition_key& pk, const position_in_partition_view pos) : partition(pk), position(pos) { }\n\ninline full_position::full_position(full_position_view fpv) : partition(fpv.partition), position(fpv.position) { }\ninline full_position::full_position(partition_key pk, position_in_partition pos) : partition(std::move(pk)), position(pos) { }\n"
        },
        {
          "name": "gc_clock.hh",
          "type": "blob",
          "size": 3.068359375,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"clocks-impl.hh\"\n#include \"utils/hashing.hh\"\n\n#include <seastar/core/lowres_clock.hh>\n\n#include <chrono>\n#include <optional>\n\nclass gc_clock final {\npublic:\n    using base = seastar::lowres_system_clock;\n    using rep = int64_t;\n    using period = std::ratio<1, 1>; // seconds\n    using duration = std::chrono::duration<rep, period>;\n    using time_point = std::chrono::time_point<gc_clock, duration>;\n\n    static constexpr auto is_steady = base::is_steady;\n\n    static constexpr std::time_t to_time_t(time_point t) {\n        return std::chrono::duration_cast<std::chrono::seconds>(t.time_since_epoch()).count();\n    }\n\n    static constexpr time_point from_time_t(std::time_t t) {\n        return time_point(std::chrono::duration_cast<duration>(std::chrono::seconds(t)));\n    }\n\n    static time_point now() noexcept {\n        return time_point(std::chrono::duration_cast<duration>(base::now().time_since_epoch())) + get_clocks_offset();\n    }\n\n    static int32_t as_int32(duration d) {\n        auto count = d.count();\n        int32_t count_32 = static_cast<int32_t>(count);\n        if (count_32 != count) {\n            throw std::runtime_error(\"Duration too big\");\n        }\n        return count_32;\n    }\n\n    static int32_t as_int32(time_point tp) {\n        return as_int32(tp.time_since_epoch());\n    }\n};\n\nusing expiry_opt = std::optional<gc_clock::time_point>;\nusing ttl_opt = std::optional<gc_clock::duration>;\n\n// 20 years in seconds\nstatic constexpr gc_clock::duration max_ttl = gc_clock::duration{20 * 365 * 24 * 60 * 60};\n\ntemplate<>\nstruct appending_hash<gc_clock::time_point> {\n    template<typename Hasher>\n    void operator()(Hasher& h, gc_clock::time_point t) const noexcept {\n        // Remain backwards-compatible with the 32-bit duration::rep (refs #4460).\n        uint64_t d64 = t.time_since_epoch().count();\n        feed_hash(h, uint32_t(d64 & 0xffff'ffff));\n        uint32_t msb = d64 >> 32;\n        if (msb) {\n            feed_hash(h, msb);\n        }\n    }\n};\n\n\nnamespace ser {\n\n// Forward-declaration - defined in serializer.hh, to avoid including it here.\n\ntemplate <typename Output>\nvoid serialize_gc_clock_duration_value(Output& out, int64_t value);\n\ntemplate <typename Input>\nint64_t deserialize_gc_clock_duration_value(Input& in);\n\ntemplate <typename T>\nstruct serializer;\n\ntemplate <>\nstruct serializer<gc_clock::duration> {\n    template <typename Input>\n    static gc_clock::duration read(Input& in) {\n        return gc_clock::duration(deserialize_gc_clock_duration_value(in));\n    }\n\n    template <typename Output>\n    static void write(Output& out, gc_clock::duration d) {\n        serialize_gc_clock_duration_value(out, d.count());\n    }\n\n    template <typename Input>\n    static void skip(Input& in) {\n        read(in);\n    }\n};\n\n}\n\ntemplate<>\nstruct fmt::formatter<gc_clock::time_point> {\n    constexpr auto parse(format_parse_context& ctx) { return ctx.begin(); }\n    auto format(gc_clock::time_point, fmt::format_context& ctx) const -> decltype(ctx.out());\n};\n"
        },
        {
          "name": "gdbinit",
          "type": "blob",
          "size": 0.6025390625,
          "content": "# Recommended .gdbinit for debugging scylla\n# See docs/dev/debugging.md for more information\nhandle SIG34 pass noprint\nhandle SIG35 pass noprint\nhandle SIGUSR1 pass noprint\nset print pretty\nset python print-stack full\nset auto-load safe-path /opt/scylladb/libreloc\nadd-auto-load-safe-path /lib64\nadd-auto-load-safe-path /usr/lib64\nset debug libthread-db 1\n\n# Register pretty-printer helpers for printing common\n# std-c++ stl containers.\npython\nimport glob\nsys.path.insert(0, glob.glob('/usr/share/gcc-*/python')[0])\nfrom libstdcxx.v6.printers import register_libstdcxx_printers\nregister_libstdcxx_printers (None)\nend\n"
        },
        {
          "name": "gen_segmented_compress_params.py",
          "type": "blob",
          "size": 6.0185546875,
          "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2017-present ScyllaDB\n#\n\n#\n# SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n#\n\n'''\nGenerates compression segmentation parameters.\n\nThese parameters are used to reduce the memory footprint of the\nin-memory compression database. See sstables/compress.hh for more\ndetails.\n'''\n\nimport argparse\nimport math\n\n\ndef data_size_range_log2():\n    return range(4, 51)\n\n\ndef chunk_size_range_log2():\n    return range(4, 31)\n\n\ndef base_offset_size(data_size, chunk_size, n):\n    return int(math.ceil(math.log2(data_size)))\n\n\ndef relative_offset_size(data_size, chunk_size, n):\n    if n == 1:\n        return int(0)\n    else:\n        return int(math.ceil(math.log2((n - 1) * (chunk_size + 64))))\n\n\ndef segment_size(data_size, chunk_size, n):\n    return base_offset_size(data_size, chunk_size, n) + (n - 1) * relative_offset_size(data_size, chunk_size, n)\n\n\ndef no_of_segments(data_size, chunk_size, n):\n    return int(math.ceil((data_size / chunk_size) / n))\n\n\ndef n_for(data_size, chunk_size, n_values):\n    nominal_data_size = int(math.ceil(math.log2(data_size)))\n    chunk_size_log2 = int(math.ceil(math.log2(chunk_size)))\n    return next(filter(lambda x: x[0] == nominal_data_size and x[1] == chunk_size_log2, n_values))[2]\n\n\ndef size_deque(data_size, chunk_size):\n    return int(math.ceil(data_size / chunk_size)) * 64\n\n\ndef size_grouped_segments(data_size, chunk_size, n):\n    return no_of_segments(data_size, chunk_size, n) * segment_size(data_size, chunk_size, n)\n\n\ndef best_nominal_data_size_for_bucket_size(chunk_size, bucket_size, n_values):\n    def addressable_space(data_size_log2):\n        data_size = 2**data_size_log2\n        n = n_for(data_size, chunk_size, n_values)\n        bucket_size_bits = bucket_size * 8\n        total_size_bits = size_grouped_segments(data_size, chunk_size, n)\n\n        if bucket_size_bits >= total_size_bits:\n            return data_size, data_size_log2\n        else:\n            segments_pb = segments_per_bucket(data_size, chunk_size, n, bucket_size)\n            return n * segments_pb * chunk_size, data_size_log2\n\n    space = map(addressable_space, data_size_range_log2())\n    return max(space, key=lambda x: x[0])[1]\n\n\ndef segments_per_bucket(data_size, chunk_size, n, bucket_size):\n    # A safety padding of 7 bytes has to be reserved at the end\n    bucket_size_bits = bucket_size * 8 - 56\n    segment_size_bits = segment_size(data_size, chunk_size, n)\n\n    fits = int(math.floor(bucket_size_bits / segment_size_bits))\n\n    # We can't have more segments than the sizes support\n    return min(no_of_segments(data_size, chunk_size, n), fits)\n\n\ndef all_n_values():\n    optimal_sizes = {}\n\n    for f in data_size_range_log2():\n        for c in chunk_size_range_log2():\n            optimal_size = None\n            for n in range(1, 201):\n                s = size_grouped_segments(2**f, 2**c, n)\n\n                if optimal_size is None or optimal_size[3] > s:\n                    optimal_size = (f, c, n, s)\n\n            optimal_sizes[(f, c)] = optimal_size\n\n    n_values = []\n    for k in sorted(optimal_sizes.keys()):\n        f, c, n, s = optimal_sizes[k]\n        n_values.append((f, c, n))\n\n    return n_values\n\n\nfile_str = \"\"\"\n/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n// SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n\n/*\n * This file was autogenerated by gen_segmented_compress_params.py.\n */\n\n#include \"compress.hh\"\n\n#include <array>\n\nnamespace sstables {{\n\nconst uint64_t bucket_size{{{bucket_size}}};\n\nstruct bucket_info {{\n    uint64_t chunk_size_log2;\n    uint64_t best_data_size_log2;\n    uint64_t segments_per_bucket;\n}};\n\n// The largest data chunk from the file a bucketful of offsets can\n// cover, precalculated for different chunk sizes, plus the number\n// of segments that are needed to address the whole area.\nconst std::array<bucket_info, {bucket_infos_size}> bucket_infos{{{{\n{bucket_infos}}}}};\n\nstruct segment_info {{\n    uint8_t data_size_log2;\n    uint8_t chunk_size_log2;\n    uint8_t grouped_offsets;\n}};\n\n// Precomputed optimal segment information for different data and chunk sizes.\nconst std::array<segment_info, {segment_infos_size}> segment_infos{{{{\n{segment_infos}}}}};\n\n}} // namespace sstables\n\"\"\"\n\nif __name__ == '__main__':\n    cmdline_parser = argparse.ArgumentParser()\n    cmdline_parser.add_argument('--bucket-size-log2', action='store', help='specify bucket size (defaults to 4K)')\n\n    args = cmdline_parser.parse_args()\n\n    if args.bucket_size_log2 is not None:\n        bucket_size_log2 = int(args.bucket_size_log2)\n\n        if bucket_size_log2 < 10 or bucket_size_log2 > 30:\n            print(\"Bucket size is either too large or too small\")\n            exit(1)\n    else:\n        bucket_size_log2 = 12  # 4K\n\n    bucket_size = 2**bucket_size_log2\n\n    n_values = all_n_values()\n\n    with open(\"sstables/segmented_compress_params.hh\", \"w\") as infos_file:\n        bucket_infos = []\n        data_sizes = []\n        for chunk_size_log2 in chunk_size_range_log2():\n            chunk_size = 2**chunk_size_log2\n            data_size_log2 = best_nominal_data_size_for_bucket_size(chunk_size, bucket_size, n_values)\n            data_size = 2**data_size_log2\n            n = n_for(data_size, chunk_size, n_values)\n\n            bucket_infos.append(\"    {{{}, {}, {} /*out of the max of {}*/}}\".format(\n                chunk_size_log2,\n                data_size_log2,\n                segments_per_bucket(data_size, chunk_size, n, bucket_size),  # no of segments that fit into the bucket\n                no_of_segments(data_size, chunk_size, n)))  # normal no of segments for these sizes\n            data_sizes.append(data_size_log2)\n\n        segment_infos = []\n        for n_value in n_values:\n            if n_value[0] in data_sizes:\n                segment_infos.append(\"    {{{}, {}, {}}}\".format(*n_value))\n\n        infos_file.write(file_str.format(\n            bucket_size=bucket_size,\n            bucket_infos=\",\\n\".join(bucket_infos),\n            bucket_infos_size=len(bucket_infos),\n            segment_infos=\",\\n\".join(segment_infos),\n            segment_infos_size=len(segment_infos)))\n"
        },
        {
          "name": "generic_server.cc",
          "type": "blob",
          "size": 8.37109375,
          "content": "/*\n * Copyright (C) 2021-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"generic_server.hh\"\n\n\n#include <fmt/ranges.h>\n#include <seastar/core/when_all.hh>\n#include <seastar/coroutine/parallel_for_each.hh>\n#include <seastar/core/reactor.hh>\n\nnamespace generic_server {\n\nconnection::connection(server& server, connected_socket&& fd)\n    : _server{server}\n    , _fd{std::move(fd)}\n    , _read_buf(_fd.input())\n    , _write_buf(_fd.output())\n    , _hold_server(_server._gate)\n{\n    ++_server._total_connections;\n    _server._connections_list.push_back(*this);\n}\n\nconnection::~connection()\n{\n    server::connections_list_t::iterator iter = _server._connections_list.iterator_to(*this);\n    for (auto&& gi : _server._gentle_iterators) {\n        if (gi.iter == iter) {\n            gi.iter++;\n        }\n    }\n    _server._connections_list.erase(iter);\n}\n\nconnection::execute_under_tenant_type\nconnection::no_tenant() {\n    // return a function that runs the process loop with no scheduling group games\n    return [] (connection_process_loop loop) {\n        return loop();\n    };\n}\n\nvoid connection::switch_tenant(execute_under_tenant_type exec) {\n    _execute_under_current_tenant = std::move(exec);\n    _tenant_switch = true;\n}\n\nfuture<> server::for_each_gently(noncopyable_function<void(connection&)> fn) {\n    _gentle_iterators.emplace_front(*this);\n    std::list<gentle_iterator>::iterator gi = _gentle_iterators.begin();\n    return seastar::do_until([ gi ] { return gi->iter == gi->end; },\n        [ gi, fn = std::move(fn) ] {\n            fn(*(gi->iter++));\n            return make_ready_future<>();\n        }\n    ).finally([ this, gi ] { _gentle_iterators.erase(gi); });\n}\n\nstatic bool is_broken_pipe_or_connection_reset(std::exception_ptr ep) {\n    try {\n        std::rethrow_exception(ep);\n    } catch (const std::system_error& e) {\n        return (e.code().category() == std::system_category()\n            && (e.code().value() == EPIPE || e.code().value() == ECONNRESET))\n            // tls version:\n            || (e.code().category() == tls::error_category()\n            && (e.code().value() == tls::ERROR_PREMATURE_TERMINATION))\n            ;\n    } catch (...) {}\n    return false;\n}\n\nfuture<> connection::process_until_tenant_switch() {\n    _tenant_switch = false;\n    {\n        return do_until([this] {\n            return _read_buf.eof() || _tenant_switch;\n        }, [this] {\n            return process_request();\n        });\n    }\n}\n\nfuture<> connection::process()\n{\n    return with_gate(_pending_requests_gate, [this] {\n        return do_until([this] {\n            return _read_buf.eof();\n        }, [this] {\n            return _execute_under_current_tenant([this] {\n                return process_until_tenant_switch();\n            });\n        }).then_wrapped([this] (future<> f) {\n            handle_error(std::move(f));\n        });\n    }).finally([this] {\n        return _pending_requests_gate.close().then([this] {\n            on_connection_close();\n            return _ready_to_respond.handle_exception([] (std::exception_ptr ep) {\n                if (is_broken_pipe_or_connection_reset(ep)) {\n                    // expected if another side closes a connection or we're shutting down\n                    return;\n                }\n                std::rethrow_exception(ep);\n            }).finally([this] {\n                 return _write_buf.close();\n            });\n        });\n    });\n}\n\nvoid connection::on_connection_close()\n{\n}\n\nfuture<> connection::shutdown()\n{\n    try {\n        _fd.shutdown_input();\n        _fd.shutdown_output();\n    } catch (...) {\n    }\n    return make_ready_future<>();\n}\n\nserver::server(const sstring& server_name, logging::logger& logger)\n    : _server_name{server_name}\n    , _logger{logger}\n{\n}\n\nserver::~server()\n{\n}\n\nfuture<> server::stop() {\n    co_await shutdown();\n    co_await std::exchange(_all_connections_stopped, make_ready_future<>());\n}\n\nfuture<> server::shutdown() {\n    if (_gate.is_closed()) {\n        co_return;\n    }\n    _all_connections_stopped = _gate.close();\n    size_t nr = 0;\n    size_t nr_total = _listeners.size();\n    _logger.debug(\"abort accept nr_total={}\", nr_total);\n    for (auto&& l : _listeners) {\n        l.abort_accept();\n        _logger.debug(\"abort accept {} out of {} done\", ++nr, nr_total);\n    }\n    size_t nr_conn = 0;\n    auto nr_conn_total = _connections_list.size();\n    _logger.debug(\"shutdown connection nr_total={}\", nr_conn_total);\n    co_await coroutine::parallel_for_each(_connections_list, [&] (auto&& c) -> future<> {\n        co_await c.shutdown();\n        _logger.debug(\"shutdown connection {} out of {} done\", ++nr_conn, nr_conn_total);\n    });\n    co_await std::move(_listeners_stopped);\n}\n\nfuture<>\nserver::listen(socket_address addr, std::shared_ptr<seastar::tls::credentials_builder> builder, bool is_shard_aware, bool keepalive, std::optional<file_permissions> unix_domain_socket_permissions) {\n    shared_ptr<seastar::tls::server_credentials> creds = nullptr;\n    if (builder) {\n        creds = co_await builder->build_reloadable_server_credentials([this](const std::unordered_set<sstring>& files, std::exception_ptr ep) {\n            if (ep) {\n                _logger.warn(\"Exception loading {}: {}\", files, ep);\n            } else {\n                _logger.info(\"Reloaded {}\", files);\n            }\n        });\n    }\n    listen_options lo;\n    lo.reuse_address = true;\n    lo.unix_domain_socket_permissions = unix_domain_socket_permissions;\n    if (is_shard_aware) {\n        lo.lba = server_socket::load_balancing_algorithm::port;\n    }\n    server_socket ss;\n    try {\n        ss = creds\n            ? seastar::tls::listen(std::move(creds), addr, lo)\n            : seastar::listen(addr, lo);\n    } catch (...) {\n        throw std::runtime_error(format(\"{} error while listening on {} -> {}\", _server_name, addr, std::current_exception()));\n    }\n    _listeners.emplace_back(std::move(ss));\n    _listeners_stopped = when_all(std::move(_listeners_stopped), do_accepts(_listeners.size() - 1, keepalive, addr)).discard_result();\n}\n\nfuture<> server::do_accepts(int which, bool keepalive, socket_address server_addr) {\n    return repeat([this, which, keepalive, server_addr] {\n        seastar::gate::holder holder(_gate);\n        return _listeners[which].accept().then_wrapped([this, keepalive, server_addr, holder = std::move(holder)] (future<accept_result> f_cs_sa) mutable {\n            if (_gate.is_closed()) {\n                f_cs_sa.ignore_ready_future();\n                return stop_iteration::yes;\n            }\n            auto cs_sa = f_cs_sa.get();\n            auto fd = std::move(cs_sa.connection);\n            auto addr = std::move(cs_sa.remote_address);\n            fd.set_nodelay(true);\n            fd.set_keepalive(keepalive);\n            auto conn = make_connection(server_addr, std::move(fd), std::move(addr));\n            // Move the processing into the background.\n            (void)futurize_invoke([this, conn] {\n                return advertise_new_connection(conn); // Notify any listeners about new connection.\n            }).then_wrapped([this, conn] (future<> f) {\n                try {\n                    f.get();\n                } catch (...) {\n                    _logger.info(\"exception while advertising new connection: {}\", std::current_exception());\n                }\n                // Block while monitoring for lifetime/errors.\n                return conn->process().then_wrapped([this, conn] (auto f) {\n                    try {\n                        f.get();\n                    } catch (...) {\n                        auto ep = std::current_exception();\n                        if (!is_broken_pipe_or_connection_reset(ep)) {\n                            // some exceptions are expected if another side closes a connection\n                            // or we're shutting down\n                            _logger.info(\"exception while processing connection: {}\", ep);\n                        }\n                    }\n                    return unadvertise_connection(conn);\n                });\n            });\n            return stop_iteration::no;\n        }).handle_exception([this] (auto ep) {\n            _logger.debug(\"accept failed: {}\", ep);\n            return stop_iteration::no;\n        });\n    });\n}\n\nfuture<>\nserver::advertise_new_connection(shared_ptr<generic_server::connection> raw_conn) {\n    return make_ready_future<>();\n}\n\nfuture<>\nserver::unadvertise_connection(shared_ptr<generic_server::connection> raw_conn) {\n    return make_ready_future<>();\n}\n\n}\n"
        },
        {
          "name": "generic_server.hh",
          "type": "blob",
          "size": 4.45703125,
          "content": "/*\n * Copyright (C) 2021-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"utils/log.hh\"\n\n#include \"seastarx.hh\"\n\n#include <list>\n\n#include <seastar/core/file-types.hh>\n#include <seastar/core/future.hh>\n#include <seastar/core/gate.hh>\n#include <seastar/util/noncopyable_function.hh>\n#include <seastar/net/api.hh>\n#include <seastar/net/tls.hh>\n\n#include <boost/intrusive/list.hpp>\n\nnamespace generic_server {\n\nclass server;\n\n// A generic TCP connection.\n//\n// This class is used in tandem with the `server`class to implement a protocol\n// specific TCP connection.\n//\n// Protocol specific classes are expected to override the `process_request`\n// member function to perform request processing. This base class provides a\n// `_read_buf` and a `_write_buf` for reading requests and writing responses.\nclass connection : public boost::intrusive::list_base_hook<> {\npublic:\n    using connection_process_loop = noncopyable_function<future<> ()>;\n    using execute_under_tenant_type = noncopyable_function<future<> (connection_process_loop)>;\n    bool _tenant_switch = false;\n    execute_under_tenant_type _execute_under_current_tenant = no_tenant();\nprotected:\n    server& _server;\n    connected_socket _fd;\n    input_stream<char> _read_buf;\n    output_stream<char> _write_buf;\n    future<> _ready_to_respond = make_ready_future<>();\n    seastar::gate _pending_requests_gate;\n    seastar::gate::holder _hold_server;\n\nprivate:\n    future<> process_until_tenant_switch();\npublic:\n    connection(server& server, connected_socket&& fd);\n    virtual ~connection();\n\n    virtual future<> process();\n\n    virtual void handle_error(future<>&& f) = 0;\n\n    virtual future<> process_request() = 0;\n\n    virtual void on_connection_close();\n\n    virtual future<> shutdown();\n\n    void switch_tenant(execute_under_tenant_type execute);\n\n    static execute_under_tenant_type no_tenant();\n};\n\n// A generic TCP socket server.\n//\n// This class can be used as a base for a protocol specific TCP socket server\n// that listens to incoming connections and processes requests coming over the\n// connection.\n//\n// The provides a `listen` member function that creates a TCP server socket and\n// registers it to the Seastar reactor. The class also provides a `stop` member\n// function that can be used to safely stop the server.\n//\n// Protocol specific classes that inherit `server` are expected to also inherit\n// a connection class from `connection` and override the `make_connection` member\n// function to create a protocol specific connection upon `accept`.\nclass server {\n    friend class connection;\n\nprotected:\n    sstring _server_name;\n    logging::logger& _logger;\n    seastar::gate _gate;\n    future<> _all_connections_stopped = make_ready_future<>();\n    uint64_t _total_connections = 0;\n    future<> _listeners_stopped = make_ready_future<>();\n    using connections_list_t = boost::intrusive::list<connection>;\n    connections_list_t _connections_list;\n    struct gentle_iterator {\n        connections_list_t::iterator iter, end;\n        gentle_iterator(server& s) : iter(s._connections_list.begin()), end(s._connections_list.end()) {}\n        gentle_iterator(const gentle_iterator&) = delete;\n        gentle_iterator(gentle_iterator&&) = delete;\n    };\n    std::list<gentle_iterator> _gentle_iterators;\n    std::vector<server_socket> _listeners;\n\npublic:\n    server(const sstring& server_name, logging::logger& logger);\n\n    virtual ~server();\n\n    // Makes sure listening sockets no longer generate new connections and aborts the\n    // connected sockets, so that new requests are not served and existing requests don't\n    // send responses back.\n    //\n    // It does _not_ wait for any internal activity started by the established connections\n    // to finish. It's the .stop() method that does it\n    future<> shutdown();\n    future<> stop();\n\n    future<> listen(socket_address addr, std::shared_ptr<seastar::tls::credentials_builder> creds, bool is_shard_aware, bool keepalive, std::optional<file_permissions> unix_domain_socket_permissions);\n\n    future<> do_accepts(int which, bool keepalive, socket_address server_addr);\n\nprotected:\n    virtual seastar::shared_ptr<connection> make_connection(socket_address server_addr, connected_socket&& fd, socket_address addr) = 0;\n\n    virtual future<> advertise_new_connection(shared_ptr<connection> conn);\n\n    virtual future<> unadvertise_connection(shared_ptr<connection> conn);\n\n    future<> for_each_gently(noncopyable_function<void(connection&)>);\n};\n\n}\n"
        },
        {
          "name": "gms",
          "type": "tree",
          "content": null
        },
        {
          "name": "hashing_partition_visitor.hh",
          "type": "blob",
          "size": 2.28515625,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"mutation/mutation_partition_visitor.hh\"\n#include \"utils/hashing.hh\"\n#include \"schema/schema.hh\"\n#include \"mutation/atomic_cell_hash.hh\"\n#include \"mutation/position_in_partition.hh\"\n\n// Calculates a hash of a mutation_partition which is consistent with\n// mutation equality. For any equal mutations, no matter which schema\n// version they were generated under, the hash fed will be the same for both of them.\ntemplate<typename Hasher>\nclass hashing_partition_visitor : public mutation_partition_visitor {\n    Hasher& _h;\n    const schema& _s;\npublic:\n    hashing_partition_visitor(Hasher& h, const schema& s)\n        : _h(h)\n        , _s(s)\n    { }\n\n    virtual void accept_partition_tombstone(tombstone t) override {\n        feed_hash(_h, t);\n    }\n\n    virtual void accept_static_cell(column_id id, atomic_cell_view cell) override {\n        auto&& col = _s.static_column_at(id);\n        feed_hash(_h, col.name());\n        feed_hash(_h, col.type->name());\n        feed_hash(_h, cell, col);\n    }\n\n    virtual void accept_static_cell(column_id id, collection_mutation_view cell) override {\n        auto&& col = _s.static_column_at(id);\n        feed_hash(_h, col.name());\n        feed_hash(_h, col.type->name());\n        feed_hash(_h, cell, col);\n    }\n\n    virtual void accept_row_tombstone(const range_tombstone& rt) override {\n        feed_hash(_h, rt, _s);\n    }\n\n    virtual void accept_row(position_in_partition_view pos, const row_tombstone& deleted_at, const row_marker& rm, is_dummy dummy, is_continuous continuous) override {\n        if (dummy) {\n            return;\n        }\n        feed_hash(_h, pos.key(), _s);\n        feed_hash(_h, deleted_at);\n        feed_hash(_h, rm);\n    }\n\n    virtual void accept_row_cell(column_id id, atomic_cell_view cell) override {\n        auto&& col = _s.regular_column_at(id);\n        feed_hash(_h, col.name());\n        feed_hash(_h, col.type->name());\n        feed_hash(_h, cell, col);\n    }\n\n    virtual void accept_row_cell(column_id id, collection_mutation_view cell) override {\n        auto&& col = _s.regular_column_at(id);\n        feed_hash(_h, col.name());\n        feed_hash(_h, col.type->name());\n        feed_hash(_h, cell, col);\n    }\n};\n"
        },
        {
          "name": "idl-compiler.py",
          "type": "blob",
          "size": 63.701171875,
          "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# Copyright 2016-present ScyllaDB\n#\n\n#\n# SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n\nimport argparse\nimport pyparsing as pp\nfrom functools import reduce\nimport textwrap\nfrom numbers import Number\nfrom pprint import pformat\nfrom copy import copy\nfrom typing import List\nimport os.path\n\nEXTENSION = '.idl.hh'\nREAD_BUFF = 'input_buffer'\nWRITE_BUFF = 'output_buffer'\nSERIALIZER = 'serialize'\nDESERIALIZER = 'deserialize'\nSETSIZE = 'set_size'\nSIZETYPE = 'size_type'\n\n\ndef reindent(indent, text):\n    return textwrap.indent(textwrap.dedent(text), ' ' * indent)\n\n\ndef fprint(f, *args):\n    for arg in args:\n        f.write(arg)\n\n\ndef fprintln(f, *args):\n    for arg in args:\n        f.write(arg)\n    f.write('\\n')\n\n\ndef print_cw(f):\n    fprintln(f, \"\"\"\n/*\n * Copyright 2016-present ScyllaDB\n */\n\n// SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n\n /*\n  * This is an auto-generated code, do not modify directly.\n  */\n#pragma once\n \"\"\")\n\n\n###\n### AST Nodes\n###\nclass ASTBase:\n    name: str\n    ns_context: List[str]\n\n    def __init__(self, name):\n        self.name = name\n\n    @staticmethod\n    def combine_ns(namespaces):\n        return \"::\".join(namespaces)\n\n    def ns_qualified_name(self):\n        return self.name if not self.ns_context \\\n            else self.combine_ns(self.ns_context) + \"::\" + self.name\n\nclass Include(ASTBase):\n    '''AST node representing a single `include file/module`.'''\n\n    def __init__(self, name):\n        super().__init__(name)\n        sfx = '.idl.hh'\n        self.is_module = name.endswith(sfx)\n        if self.is_module:\n            self.module_name = self.name[0:-len(sfx)]\n\n    def __str__(self):\n        return f\"<Include(name={self.name}, is_module={self.is_module})>\"\n\n    def __repr__(self):\n        return self.__str__()\n\nclass BasicType(ASTBase):\n    '''AST node that represents terminal grammar nodes for the non-template\n    types, defined either inside or outside the IDL.\n\n    These can appear either in the definition of the class fields or as a part of\n    template types (template arguments).\n\n    Basic type nodes can also be marked as `const` when used inside a template type,\n    e.g. `lw_shared_ptr<const T>`. When an IDL-defined type `T` appears somewhere\n    with a `const` specifier, an additional `serializer<const T>` specialization\n    is generated for it.'''\n    def __init__(self, name, is_const=False):\n        super().__init__(name)\n        self.is_const = is_const\n\n    def __str__(self):\n        return f\"<BasicType(name={self.name}, is_const={self.is_const})>\"\n\n    def __repr__(self):\n        return self.__str__()\n\n    def to_string(self):\n        if self.is_const:\n            return 'const ' + self.name\n        return self.name\n\n\nclass TemplateType(ASTBase):\n    '''AST node representing template types, for example: `std::vector<T>`.\n\n    These can appear either in the definition of the class fields or as a part of\n    template types (template arguments).\n\n    Such types can either be defined inside or outside the IDL.'''\n    def __init__(self, name, template_parameters):\n        super().__init__(name)\n        # FIXME: dirty hack to translate non-type template parameters (numbers) to BasicType objects\n        self.template_parameters = [\n            t if isinstance(t, BasicType) or isinstance(t, TemplateType) else BasicType(name=str(t)) \\\n                for t in template_parameters]\n\n    def __str__(self):\n        return f\"<TemplateType(name={self.name}, args={pformat(self.template_parameters)}>\"\n\n    def __repr__(self):\n        return self.__str__()\n\n    def to_string(self):\n        res = self.name + '<'\n        res += ', '.join([p.to_string() for p in self.template_parameters])\n        res += '>'\n        return res\n\n\nclass EnumValue(ASTBase):\n    '''AST node representing a single `name=value` enumerator in the enum.\n\n    Initializer part is optional, the same as in C++ enums.'''\n    def __init__(self, name, initializer=None):\n        super().__init__(name)\n        self.initializer = initializer\n\n    def __str__(self):\n        return f\"<EnumValue(name={self.name}, initializer={self.initializer})>\"\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass EnumDef(ASTBase):\n    '''AST node representing C++ `enum class` construct.\n\n    Consists of individual initializers in form of `EnumValue` objects.\n    Should have an underlying type explicitly specified.'''\n    def __init__(self, name, underlying_type, members):\n        super().__init__(name)\n        self.underlying_type = underlying_type\n        self.members = members\n\n    def __str__(self):\n        return f\"<EnumDef(name={self.name}, underlying_type={self.underlying_type}, members={pformat(self.members)})>\";\n\n    def __repr__(self):\n        return self.__str__()\n\n    def serializer_write_impl(self, cout):\n        name = self.ns_qualified_name()\n\n        fprintln(cout, f\"\"\"\n{self.template_declaration}\ntemplate <typename Output>\nvoid serializer<{name}>::write(Output& buf, const {name}& v) {{\n  serialize(buf, static_cast<{self.underlying_type}>(v));\n}}\"\"\")\n\n\n    def serializer_read_impl(self, cout):\n        name = self.ns_qualified_name()\n\n        fprintln(cout, f\"\"\"\n{self.template_declaration}\ntemplate<typename Input>\n{name} serializer<{name}>::read(Input& buf) {{\n  return static_cast<{name}>(deserialize(buf, std::type_identity<{self.underlying_type}>()));\n}}\"\"\")\n\n\nclass Attributes(ASTBase):\n    ''' AST node for representing class and field attributes.\n\n    The following attributes are supported:\n     - `[[writable]]` class attribute, triggers generation of writers and views\n       for a class.\n     - `[[version id]] field attribute, marks that a field is available starting\n       from a specific version.'''\n    def __init__(self, attr_items=[]):\n        super().__init__('attributes')\n        self.attr_items = attr_items\n\n    def __str__(self):\n        return f\"[[{', '.join([a for a in self.attr_items])}]]\"\n\n    def __repr__(self):\n        return self.__str__()\n\n    def empty(self):\n        return not self.attr_items\n\n\nclass DataClassMember(ASTBase):\n    '''AST node representing a data field in a class.\n\n    Can optionally have a version attribute and a default value specified.'''\n    def __init__(self, type, name, attribute=None, default_value=None):\n        super().__init__(name)\n        self.type = type\n        self.attribute = attribute\n        self.default_value = default_value\n\n    def __str__(self):\n        return f\"<DataClassMember(type={self.type}, name={self.name}, attribute={self.attribute}, default_value={self.default_value})>\"\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass FunctionClassMember(ASTBase):\n    '''AST node representing getter function in a class definition.\n\n    Can optionally have a version attribute and a default value specified.\n\n    Getter functions should be used whenever it's needed to access private\n    members of a class.'''\n    def __init__(self, type, name, attribute=None, default_value=None):\n        super().__init__(name)\n        self.type = type\n        self.attribute = attribute\n        self.default_value = default_value\n\n    def __str__(self):\n        return f\"<FunctionClassMember(type={self.type}, name={self.name}, attribute={self.attribute}, default_value={self.default_value})>\"\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass ClassTemplateParam(ASTBase):\n    '''AST node representing a single template argument of a class template\n    definition, such as `typename T`.'''\n    def __init__(self, typename, name):\n        super().__init__(name)\n        self.typename = typename\n\n    def __str__(self):\n        return f\"<ClassTemplateParam(typename={self.typename}, name={self.name})>\"\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass ClassDef(ASTBase):\n    '''AST node representing a class definition. Can use either `class` or `struct`\n    keyword to define a class.\n\n    The following specifiers are allowed in a class declaration:\n     - `final` -- if a class is marked with this keyword it will not contain a\n       size argument. Final classes cannot be extended by a future version, so\n       it should be used with care.\n     - `stub` -- no code will be generated for the class, it's only there for\n       documentation.\n    Also it's possible to specify a `[[writable]]` attribute for a class, which\n    means that writers and views will be generated for the class.\n\n    Classes are also can be declared as template classes, much the same as in C++.\n    In this case the template declaration syntax mimics C++ templates.'''\n    def __init__(self, name, members, final, stub, attribute, template_params):\n        super().__init__(name)\n        self.members = members\n        self.final = final\n        self.stub = stub\n        self.attribute = attribute\n        self.template_params = template_params\n\n    def __str__(self):\n        return f\"<ClassDef(name={self.name}, members={pformat(self.members)}, final={self.final}, stub={self.stub}, attribute={self.attribute}, template_params={self.template_params})>\"\n\n    def __repr__(self):\n        return self.__str__()\n\n    def serializer_write_impl(self, cout):\n        name = self.ns_qualified_name()\n        full_name = name + self.template_param_names_str\n\n        fprintln(cout, f\"\"\"\n{self.template_declaration}\ntemplate <typename Output>\nvoid serializer<{full_name}>::write(Output& buf, const {full_name}& obj) {{\"\"\")\n        if not self.final:\n            fprintln(cout, f\"\"\"  {SETSIZE}(buf, obj);\"\"\")\n        for member in self.members:\n            if isinstance(member, ClassDef) or isinstance(member, EnumDef):\n                continue\n            fprintln(cout, f\"\"\"  static_assert(is_equivalent<decltype(obj.{member.name}), {param_type(member.type)}>::value, \"member value has a wrong type\");\n  {SERIALIZER}(buf, obj.{member.name});\"\"\")\n        fprintln(cout, \"}\")\n\n\n    def serializer_read_impl(self, cout):\n        name = self.ns_qualified_name()\n\n        fprintln(cout, f\"\"\"\n{self.template_declaration}\ntemplate <typename Input>\n{name}{self.template_param_names_str} serializer<{name}{self.template_param_names_str}>::read(Input& buf) {{\n return seastar::with_serialized_stream(buf, [] (auto& buf) {{\"\"\")\n        if not self.members:\n            if not self.final:\n                fprintln(cout, f\"\"\"  {SIZETYPE} size = {DESERIALIZER}(buf, std::type_identity<{SIZETYPE}>());\n  buf.skip(size - sizeof({SIZETYPE}));\"\"\")\n        elif not self.final:\n            fprintln(cout, f\"\"\"  {SIZETYPE} size = {DESERIALIZER}(buf, std::type_identity<{SIZETYPE}>());\n  auto in = buf.read_substream(size - sizeof({SIZETYPE}));\"\"\")\n        else:\n            fprintln(cout, \"\"\"  auto& in = buf;\"\"\")\n        params = []\n        local_names = {}\n        for index, param in enumerate(self.members):\n            if isinstance(param, ClassDef) or isinstance(param, EnumDef):\n                continue\n            local_param = \"__local_\" + str(index)\n            local_names[param.name] = local_param\n            if param.attribute:\n                deflt = param_type(param.type) + \"()\"\n                if param.default_value:\n                    deflt = param.default_value\n                if deflt in local_names:\n                    deflt = local_names[deflt]\n                fprintln(cout, f\"\"\"  auto {local_param} = (in.size()>0) ?\n    {DESERIALIZER}(in, std::type_identity<{param_type(param.type)}>()) : {deflt};\"\"\")\n            else:\n                fprintln(cout, f\"\"\"  auto {local_param} = {DESERIALIZER}(in, std::type_identity<{param_type(param.type)}>());\"\"\")\n            params.append(\"std::move(\" + local_param + \")\")\n        fprintln(cout, f\"\"\"\n  {name}{self.template_param_names_str} res {{{\", \".join(params)}}};\n  return res;\n }});\n}}\"\"\")\n\n\n    def serializer_skip_impl(self, cout):\n        name = self.ns_qualified_name()\n\n        fprintln(cout, f\"\"\"\n{self.template_declaration}\ntemplate <typename Input>\nvoid serializer<{name}{self.template_param_names_str}>::skip(Input& buf) {{\n seastar::with_serialized_stream(buf, [] (auto& buf) {{\"\"\")\n        if not self.final:\n            fprintln(cout, f\"\"\"  {SIZETYPE} size = {DESERIALIZER}(buf, std::type_identity<{SIZETYPE}>());\n  buf.skip(size - sizeof({SIZETYPE}));\"\"\")\n        else:\n            for m in get_members(self):\n                full_type = param_view_type(m.type)\n                fprintln(cout, f\"  ser::skip(buf, std::type_identity<{full_type}>());\")\n        fprintln(cout, \"\"\" });\\n}\"\"\")\n\n\nclass RpcVerbParam(ASTBase):\n    \"\"\"AST element representing a single argument in an RPC verb declaration.\n    Consists of:\n    * Argument type\n    * Argument name (optional)\n    * Additional attributes (only [[version]] attribute is supported).\n\n    If the name is omitted, then this argument will have a placeholder name of form `_N`, where N is the index\n    of the argument in the argument list for an RPC verb.\n\n    If the [[version]] attribute is specified, then handler function signature for an RPC verb will contain this\n    argument as an `rpc::optional<>`.\n    If the [[unique_ptr]] attribute is specified then handler function signature for an RPC verb will contain this\n    argument as an `foreign_ptr<unique_ptr<>>`\n    If the [[lw_shared_ptr]] attribute is specified then handler function signature for an RPC verb will contain this\n    argument as an `foreign_ptr<lw_shared_ptr<>>`\n    If the [[ref]] attribute is specified the send function signature will contain this type as const reference\"\"\"\n    def __init__(self, type, name, attributes=Attributes()):\n        self.type = type\n        self.name = name\n        self.attributes = attributes\n\n    def __str__(self):\n        return f\"<RpcVerbParam(type={self.type}, name={self.name}, attributes={self.attributes})>\"\n\n    def __repr__(self):\n        return self.__str__()\n\n    def is_optional(self):\n        return True in [a.startswith('version') for a in self.attributes.attr_items]\n\n    def is_lw_shared(self):\n        return True in [a.startswith('lw_shared_ptr') for a in self.attributes.attr_items]\n\n    def is_unique(self):\n        return True in [a.startswith('unique_ptr') for a in self.attributes.attr_items]\n\n    def is_ref(self):\n        return True in [a.startswith('ref') for a in self.attributes.attr_items]\n\n\n    def to_string(self):\n        res = self.type.to_string()\n        if self.is_optional():\n            res = 'rpc::optional<' + res + '>'\n        if self.name:\n            res += ' '\n            res += self.name\n        return res\n\n    def to_string_send_fn_signature(self):\n        res = self.type.to_string()\n        if self.is_ref():\n            res = 'const ' + res + '&'\n        if self.name:\n            res += ' '\n            res += self.name\n        return res\n\n    def to_string_handle_ret_value(self):\n        res = self.type.to_string()\n        if self.is_unique():\n            res = 'foreign_ptr<std::unique_ptr<' + res + '>>'\n        elif self.is_lw_shared():\n            res = 'foreign_ptr<lw_shared_ptr<' + res + '>>'\n        return res\n\n\nclass RpcVerb(ASTBase):\n    \"\"\"AST element representing an RPC verb declaration.\n\n    `my_verb` RPC verb declaration corresponds to the\n    `netw::messaging_verb::MY_VERB` enumeration value to identify the\n    new RPC verb.\n\n    For a given `idl_module.idl.hh` file, a registrator class named\n    `idl_module_rpc_verbs` will be created if there are any RPC verbs\n    registered within the IDL module file.\n\n    These are the methods being created for each RPC verb:\n\n            static void register_my_verb(netw::messaging_service* ms, std::function<return_value(args...)>&&);\n            static future<> unregister_my_verb(netw::messaging_service* ms);\n            static future<> send_my_verb(netw::messaging_service* ms, netw::msg_addr id, args...);\n            static future<> send_my_verb(netw::messaging_service* ms, locator::host_id id, args...);\n\n    Each method accepts a pointer to an instance of messaging_service\n    object, which contains the underlying seastar RPC protocol\n    implementation, that is used to register verbs and pass messages.\n\n    There is also a method to unregister all verbs at once:\n\n            static future<> unregister(netw::messaging_service* ms);\n\n    The following attributes are supported when declaring an RPC verb\n    in the IDL:\n\n    - [[with_client_info]] - the handler will contain a const reference to\n      an `rpc::client_info` as the first argument.\n    - [[with_timeout]] - an additional time_point parameter is supplied\n      to the handler function and send* method uses send_message_*_timeout\n      variant of internal function to actually send the message.\n      Incompatible with [[cancellable]].\n    - [[cancellable]] - an additional abort_source& parameter is supplied\n      to the handler function and send* method uses send_message_*_cancellable\n      variant of internal function to actually send the message.\n      Incompatible with [[with_timeout]].\n    - [[one_way]] - the handler function is annotated by\n      future<rpc::no_wait_type> return type to designate that a client\n      doesn't need to wait for an answer.\n\n    The `-> return_values` clause is optional for two-way messages. If omitted,\n    the return type is set to be `future<>`.\n    For one-way verbs, the use of return clause is prohibited and the\n    signature of `send*` function always returns `future<>`.\"\"\"\n    def __init__(self, name, parameters, return_values, with_client_info, with_timeout, cancellable, one_way):\n        super().__init__(name)\n        self.params = parameters\n        self.return_values = return_values\n        self.with_client_info = with_client_info\n        self.with_timeout = with_timeout\n        self.cancellable = cancellable\n        self.one_way = one_way\n\n    def __str__(self):\n        return f\"<RpcVerb(name={self.name}, params={self.params}, return_values={self.return_values}, with_client_info={self.with_client_info}, with_timeout={self.with_timeout}, cancellable={self.cancellable}, one_way={self.one_way})>\"\n\n    def __repr__(self):\n        return self.__str__()\n\n    def send_function_name(self):\n        send_fn = 'send_message'\n        if self.one_way:\n            send_fn += '_oneway'\n        if self.with_timeout:\n            send_fn += '_timeout'\n        if self.cancellable:\n            send_fn += '_cancellable'\n        return send_fn\n\n    def handler_function_return_values(self):\n        if self.one_way:\n            return 'future<rpc::no_wait_type>'\n        if not self.return_values:\n            return 'future<>'\n        l = len(self.return_values)\n        ret = 'rpc::tuple<' if l > 1 else ''\n        for t in self.return_values:\n            ret = ret + t.to_string_handle_ret_value() + ', '\n        ret = ret[:-2]\n        if l > 1:\n            ret = ret + '>'\n        return f\"future<{ret}>\"\n\n    def send_function_return_type(self):\n        if self.one_way or not self.return_values:\n            return 'future<>'\n        l = len(self.return_values)\n        ret = 'rpc::tuple<' if l > 1 else ''\n        for t in self.return_values:\n            ret = ret + t.to_string() + ', '\n        ret = ret[:-2]\n        if l > 1:\n            ret = ret + '>'\n        return f\"future<{ret}>\"\n\n    def messaging_verb_enum_case(self):\n        return f'netw::messaging_verb::{self.name.upper()}'\n\n    def handler_function_parameters_str(self):\n        res = []\n        if self.with_client_info:\n            res.append(RpcVerbParam(type=BasicType(name='rpc::client_info&', is_const=True), name='info'))\n        if self.with_timeout:\n            res.append(RpcVerbParam(type=BasicType(name='rpc::opt_time_point'), name='timeout'))\n        if self.params:\n            res.extend(self.params)\n        return ', '.join([p.to_string() for p in res])\n\n    def send_function_signature_params_list(self, include_placeholder_names, dst_type):\n        res = f'netw::messaging_service* ms, {dst_type} id'\n        if self.with_timeout:\n            res += ', netw::messaging_service::clock_type::time_point timeout'\n        if self.cancellable:\n            res += ', abort_source& as'\n        if self.params:\n            for idx, p in enumerate(self.params):\n                res += ', ' + p.to_string_send_fn_signature()\n                if include_placeholder_names and not p.name:\n                    res += f' _{idx + 1}'\n        return res\n\n    def send_message_argument_list(self):\n        res = f'ms, {self.messaging_verb_enum_case()}, id'\n        if self.with_timeout:\n            res += ', timeout'\n        if self.cancellable:\n            res += ', as'\n        if self.params:\n            for idx, p in enumerate(self.params):\n                res += ', ' + f'std::move({p.name if p.name else f\"_{idx + 1}\"})'\n        return res\n\n    def send_function_invocation(self):\n        res = 'return ' + self.send_function_name()\n        if not (self.one_way):\n            res += '<' + self.send_function_return_type() + '>'\n        res += '(' + self.send_message_argument_list() + ');'\n        return res\n\nclass NamespaceDef(ASTBase):\n    '''AST node representing a namespace scope.\n\n    It has the same meaning as in C++ or other languages with similar facilities.\n\n    A namespace can contain one of the following top-level constructs:\n     - namespaces\n     - class definitions\n     - enum definitions'''\n    def __init__(self, name, members):\n        super().__init__(name)\n        self.members = members\n\n    def __str__(self):\n        return f\"<NamespaceDef(name={self.name}, members={pformat(self.members)})>\"\n\n    def __repr__(self):\n        return self.__str__()\n\n###\n### Parse actions, which transform raw tokens into structured representation: specialized AST nodes\n###\ndef basic_type_parse_action(tokens):\n    return BasicType(name=tokens[0])\n\ndef include_parse_action(tokens):\n    return Include(name=tokens[0])\n\ndef template_type_parse_action(tokens):\n    return TemplateType(name=tokens['template_name'], template_parameters=tokens[\"template_parameters\"].asList())\n\n\ndef type_parse_action(tokens):\n    if len(tokens) == 1:\n        return tokens[0]\n    # If we have two tokens in type parse action then\n    # it's because we have BasicType production with `const`\n    # NOTE: template types cannot have `const` modifier at the moment,\n    # this wouldn't parse.\n    tokens[1].is_const = True\n    return tokens[1]\n\n\ndef enum_value_parse_action(tokens):\n    initializer = None\n    if len(tokens) == 2:\n        initializer = tokens[1]\n    return EnumValue(name=tokens[0], initializer=initializer)\n\n\ndef enum_def_parse_action(tokens):\n    return EnumDef(name=tokens['name'], underlying_type=tokens['underlying_type'], members=tokens['enum_values'].asList())\n\n\ndef attributes_parse_action(tokens):\n    items = []\n    for attr_clause in tokens:\n        # Split individual attributes inside each attribute clause by commas and strip extra whitespace characters\n        items += [arg.strip() for arg in attr_clause.split(',')]\n    return Attributes(attr_items=items)\n\n\ndef class_member_parse_action(tokens):\n    member_name = tokens['name']\n    raw_attrs = tokens['attributes']\n    attribute = raw_attrs.attr_items[0] if not raw_attrs.empty() else None\n    default = tokens['default'][0] if 'default' in tokens else None\n    if not isinstance(member_name, str): # accessor function declaration\n        return FunctionClassMember(type=tokens[\"type\"], name=member_name[0], attribute=attribute, default_value=default)\n    # data member\n    return DataClassMember(type=tokens[\"type\"], name=member_name, attribute=attribute, default_value=default)\n\n\ndef class_def_parse_action(tokens):\n    is_final = 'final' in tokens\n    is_stub = 'stub' in tokens\n    class_members = tokens['members'].asList() if 'members' in tokens else []\n    raw_attrs = tokens['attributes']\n    attribute = raw_attrs.attr_items[0] if not raw_attrs.empty() else None\n    template_params = None\n    if 'template' in tokens:\n        template_params = [ClassTemplateParam(typename=tp[0], name=tp[1]) for tp in tokens['template']]\n    return ClassDef(name=tokens['name'], members=class_members, final=is_final, stub=is_stub, attribute=attribute, template_params=template_params)\n\n\ndef rpc_verb_param_parse_action(tokens):\n    type = tokens['type']\n    name = tokens['ident'] if 'ident' in tokens else None\n    attrs = tokens['attrs']\n    return RpcVerbParam(type=type, name=name, attributes=attrs)\n\ndef rpc_verb_return_val_parse_action(tokens):\n    type = tokens['type']\n    attrs = tokens['attrs']\n    return RpcVerbParam(type=type, name='', attributes=attrs)\n\ndef rpc_verb_parse_action(tokens):\n    name = tokens['name']\n    raw_attrs = tokens['attributes']\n    params = tokens['params'] if 'params' in tokens else []\n    with_timeout = not raw_attrs.empty() and 'with_timeout' in raw_attrs.attr_items\n    cancellable = not raw_attrs.empty() and 'cancellable' in raw_attrs.attr_items\n    with_client_info = not raw_attrs.empty() and 'with_client_info' in raw_attrs.attr_items\n    one_way = not raw_attrs.empty() and 'one_way' in raw_attrs.attr_items\n    if one_way and 'return_values' in tokens:\n        raise Exception(f\"Invalid return type specification for one-way RPC verb '{name}'\")\n    if with_timeout and cancellable:\n        raise Exception(f\"Error in verb {name}: [[with_timeout]] cannot be used together with [[cancellable]] in the same verb\")\n    return RpcVerb(name=name, parameters=params, return_values=tokens.get('return_values'), with_client_info=with_client_info, with_timeout=with_timeout, cancellable=cancellable, one_way=one_way)\n\n\ndef namespace_parse_action(tokens):\n    return NamespaceDef(name=tokens['name'], members=tokens['ns_members'].asList())\n\n\ndef parse_file(file_name):\n    '''Parse the input from the file using IDL grammar syntax and generate AST'''\n\n    number = pp.pyparsing_common.signed_integer\n    identifier = pp.pyparsing_common.identifier\n\n    include_kw = pp.Keyword('#include').suppress()\n    lbrace = pp.Literal('{').suppress()\n    rbrace = pp.Literal('}').suppress()\n    cls = pp.Keyword('class').suppress()\n    colon = pp.Literal(\":\").suppress()\n    semi = pp.Literal(\";\").suppress()\n    langle = pp.Literal(\"<\").suppress()\n    rangle = pp.Literal(\">\").suppress()\n    equals = pp.Literal(\"=\").suppress()\n    comma = pp.Literal(\",\").suppress()\n    lparen = pp.Literal(\"(\")\n    rparen = pp.Literal(\")\")\n    lbrack = pp.Literal(\"[\").suppress()\n    rbrack = pp.Literal(\"]\").suppress()\n    struct = pp.Keyword('struct').suppress()\n    template = pp.Keyword('template').suppress()\n    final = pp.Keyword('final')\n    stub = pp.Keyword('stub')\n    const = pp.Keyword('const')\n    dcolon = pp.Literal(\"::\")\n    ns_qualified_ident = pp.Combine(pp.Optional(dcolon) + pp.delimitedList(identifier, \"::\", combine=True))\n    enum_lit = pp.Keyword('enum').suppress()\n    ns = pp.Keyword(\"namespace\").suppress()\n    verb = pp.Keyword(\"verb\").suppress()\n\n    btype = ns_qualified_ident.copy()\n    btype.setParseAction(basic_type_parse_action)\n\n    type = pp.Forward()\n\n    tmpl = ns_qualified_ident(\"template_name\") + langle + pp.Group(pp.delimitedList(type | number))(\"template_parameters\") + rangle\n    tmpl.setParseAction(template_type_parse_action)\n\n    type <<= tmpl | (pp.Optional(const) + btype)\n    type.setParseAction(type_parse_action)\n\n    include_stmt = include_kw - pp.QuotedString('\"')\n    include_stmt.setParseAction(include_parse_action)\n\n    enum_class = enum_lit - cls\n\n    enum_init = equals - number\n    enum_value = identifier - pp.Optional(enum_init)\n    enum_value.setParseAction(enum_value_parse_action)\n\n    enum_values = lbrace - pp.delimitedList(enum_value) - pp.Optional(comma) - rbrace\n    enum = enum_class - identifier(\"name\") - colon - identifier(\"underlying_type\") - enum_values(\"enum_values\") + pp.Optional(semi)\n    enum.setParseAction(enum_def_parse_action)\n\n    content = pp.Forward()\n\n    attrib = lbrack - lbrack - pp.SkipTo(']') - rbrack - rbrack\n    opt_attributes = pp.ZeroOrMore(attrib)(\"attributes\")\n    opt_attributes.setParseAction(attributes_parse_action)\n\n    default_value = equals - pp.SkipTo(';')\n    member_name = pp.Combine(identifier - pp.Optional(lparen - rparen)(\"function_marker\"))\n    class_member = type(\"type\") - member_name(\"name\") - opt_attributes - pp.Optional(default_value)(\"default\") - semi\n    class_member.setParseAction(class_member_parse_action)\n\n    template_param = pp.Group(identifier(\"type\") - identifier(\"name\"))\n    template_def = template - langle - pp.delimitedList(template_param)(\"params\") - rangle\n    class_content = pp.Forward()\n    class_def = pp.Optional(template_def)(\"template\") + (cls | struct) - ns_qualified_ident(\"name\") - \\\n        pp.Optional(final)(\"final\") - pp.Optional(stub)(\"stub\") - opt_attributes - \\\n        lbrace - pp.ZeroOrMore(class_content)(\"members\") - rbrace - pp.Optional(semi)\n    class_content <<= enum | class_def | class_member\n    class_def.setParseAction(class_def_parse_action)\n\n    rpc_verb_param = type(\"type\") - pp.Optional(identifier)(\"ident\") - opt_attributes(\"attrs\")\n    rpc_verb_param.setParseAction(rpc_verb_param_parse_action)\n    rpc_verb_params = pp.delimitedList(rpc_verb_param)\n\n    rpc_verb_return_val = type(\"type\") - opt_attributes(\"attrs\")\n    rpc_verb_return_val.setParseAction(rpc_verb_return_val_parse_action)\n    rpc_verb_return_vals = pp.delimitedList(rpc_verb_return_val)\n\n    rpc_verb = verb - opt_attributes - identifier(\"name\") - \\\n        lparen.suppress() - pp.Optional(rpc_verb_params(\"params\")) - rparen.suppress() - \\\n        pp.Optional(pp.Literal(\"->\").suppress() - rpc_verb_return_vals(\"return_values\")) - pp.Optional(semi)\n    rpc_verb.setParseAction(rpc_verb_parse_action)\n\n    namespace = ns - identifier(\"name\") - lbrace - pp.OneOrMore(content)(\"ns_members\") - rbrace\n    namespace.setParseAction(namespace_parse_action)\n\n    content <<= include_stmt | enum | class_def | rpc_verb | namespace\n\n    for varname in (\"include_stmt\", \"enum\", \"class_def\", \"class_member\", \"content\", \"namespace\", \"template_def\"):\n        locals()[varname].setName(varname)\n\n    rt = pp.OneOrMore(content)\n    rt.ignore(pp.cppStyleComment)\n    return rt.parseFile(file_name, parseAll=True)\n\n\ndef declare_methods(hout, name, template_param=\"\"):\n    fprintln(hout, f\"\"\"\ntemplate <{template_param}>\nstruct serializer<{name}> {{\n  template <typename Output>\n  static void write(Output& buf, const {name}& v);\n\n  template <typename Input>\n  static {name} read(Input& buf);\n\n  template <typename Input>\n  static void skip(Input& buf);\n}};\n\"\"\")\n\n    fprintln(hout, f\"\"\"\ntemplate <{template_param}>\nstruct serializer<const {name}> : public serializer<{name}>\n{{}};\n\"\"\")\n\n\ndef template_params_str(template_params):\n    if not template_params:\n        return \"\"\n    return \", \".join(map(lambda param: param.typename + \" \" + param.name, template_params))\n\n\ndef handle_enum(enum, hout, cout):\n    '''Generate serializer declarations and definitions for an IDL enum'''\n    temp_def = template_params_str(enum.parent_template_params)\n    name = enum.ns_qualified_name()\n    declare_methods(hout, name, temp_def)\n\n    enum.serializer_write_impl(cout)\n    enum.serializer_read_impl(cout)\n\n\ndef join_template(template_params):\n    return \"<\" + \", \".join([param_type(p) for p in template_params]) + \">\"\n\n\ndef param_type(t):\n    if isinstance(t, BasicType):\n        return 'const ' + t.name if t.is_const else t.name\n    elif isinstance(t, TemplateType):\n        return t.name + join_template(t.template_parameters)\n\n\ndef flat_type(t):\n    if isinstance(t, BasicType):\n        return t.name\n    elif isinstance(t, TemplateType):\n        return (t.name + \"__\" + \"_\".join([flat_type(p) for p in t.template_parameters])).replace('::', '__')\n\n\nlocal_types = {}\nlocal_writable_types = {}\nrpc_verbs = {}\n\n\ndef resolve_basic_type_ref(type: BasicType):\n    if type.name not in local_types:\n        raise KeyError(f\"Failed to resolve type reference for '{type.name}'\")\n    return local_types[type.name]\n\n\ndef list_types(t):\n    if isinstance(t, BasicType):\n        return [t.name]\n    elif isinstance(t, TemplateType):\n        return reduce(lambda a, b: a + b, [list_types(p) for p in t.template_parameters])\n\n\ndef list_local_writable_types(t):\n    return {l for l in list_types(t) if l in local_writable_types}\n\n\ndef is_basic_type(t):\n    return isinstance(t, BasicType) and t.name not in local_writable_types\n\n\ndef is_local_writable_type(t):\n    if isinstance(t, str): # e.g. `t` is a local class name\n        return t in local_writable_types\n    return t.name in local_writable_types\n\n\ndef get_template_name(lst):\n    return lst[\"template_name\"] if not isinstance(lst, str) and len(lst) > 1 else None\n\n\ndef is_vector(t):\n    return isinstance(t, TemplateType) and (t.name == \"std::vector\" or t.name == \"utils::chunked_vector\")\n\n\ndef is_variant(t):\n    return isinstance(t, TemplateType) and (t.name == \"boost::variant\" or t.name == \"std::variant\")\n\n\ndef is_optional(t):\n    return isinstance(t, TemplateType) and t.name == \"std::optional\"\n\n\ncreated_writers = set()\n\n\ndef get_member_name(name):\n    return name if not name.endswith('()') else name[:-2]\n\n\ndef get_members(cls):\n    return [p for p in cls.members if not isinstance(p, ClassDef) and not isinstance(p, EnumDef)]\n\n\ndef get_variant_type(t):\n    if is_variant(t):\n        return \"variant\"\n    return param_type(t)\n\n\ndef variant_to_member(template_parameters):\n    return [DataClassMember(name=get_variant_type(x), type=x) for x in template_parameters if is_local_writable_type(x) or is_variant(x)]\n\n\ndef variant_info(cls, template_parameters):\n    variant_info_cls = copy(cls) # shallow copy of cls\n    variant_info_cls.members = variant_to_member(template_parameters)\n    return variant_info_cls\n\n\nstubs = set()\n\n\ndef is_stub(cls):\n    return cls in stubs\n\n\ndef handle_visitors_state(cls, cout, classes=[]):\n    name = \"__\".join(classes) if classes else cls.name\n    frame = \"empty_frame\" if cls.final else \"frame\"\n    fprintln(cout, f\"\"\"\ntemplate<typename Output>\nstruct state_of_{name} {{\n    {frame}<Output> f;\"\"\")\n    if classes:\n        local_state = \"state_of_\" + \"__\".join(classes[:-1]) + '<Output>'\n        fprintln(cout, f\"    {local_state} _parent;\")\n        if cls.final:\n            fprintln(cout, f\"    state_of_{name}({local_state} parent) : _parent(parent) {{}}\")\n    fprintln(cout, \"};\")\n    members = get_members(cls)\n    member_class = classes if classes else [cls.name]\n    for m in members:\n        if is_local_writable_type(m.type):\n            handle_visitors_state(local_writable_types[param_type(m.type)], cout, member_class + [m.name])\n        if is_variant(m.type):\n            handle_visitors_state(variant_info(cls, m.type.template_parameters), cout, member_class + [m.name])\n\n\ndef get_dependency(cls):\n    members = get_members(cls)\n    return reduce(lambda a, b: a | b, [list_local_writable_types(m.type) for m in members], set())\n\n\ndef optional_add_methods(typ):\n    res = reindent(4, \"\"\"\n    void skip()  {\n        serialize(_out, false);\n    }\"\"\")\n    if is_basic_type(typ):\n        added_type = typ\n    elif is_local_writable_type(typ):\n        added_type = param_type(typ) + \"_view\"\n    else:\n        print(\"non supported optional type \", typ)\n        raise \"non supported optional type \" + param_type(typ)\n    res = res + reindent(4, f\"\"\"\n    void write(const {added_type}& obj) {{\n        serialize(_out, true);\n        serialize(_out, obj);\n    }}\"\"\")\n    if is_local_writable_type(typ):\n        res = res + reindent(4, f\"\"\"\n    writer_of_{param_type(typ)}<Output> write() {{\n        serialize(_out, true);\n        return {{_out}};\n    }}\"\"\")\n    return res\n\n\ndef vector_add_method(current, base_state):\n    typ = current.type\n    res = \"\"\n    if is_basic_type(typ.template_parameters[0]):\n        res = res + f\"\"\"\n  void add_{current.name}({param_type(typ.template_parameters[0])} t)  {{\n        serialize(_out, t);\n        _count++;\n  }}\"\"\"\n    else:\n        res = res + f\"\"\"\n  writer_of_{flat_type(typ.template_parameters[0])}<Output> add() {{\n        _count++;\n        return {{_out}};\n  }}\"\"\"\n        res = res + f\"\"\"\n  void add({param_view_type(typ.template_parameters[0])} v) {{\n        serialize(_out, v);\n        _count++;\n  }}\"\"\"\n    return res + f\"\"\"\n  after_{base_state}__{current.name}<Output> end_{current.name}() && {{\n        _size.set(_out, _count);\n        return {{ _out, std::move(_state) }};\n  }}\n\n  vector_position pos() const {{\n        return vector_position{{_out.pos(), _count}};\n  }}\n\n  void rollback(const vector_position& vp) {{\n        _out.retract(vp.pos);\n        _count = vp.count;\n  }}\"\"\"\n\n\ndef add_param_writer_basic_type(name, base_state, typ, var_type=\"\", var_index=None, root_node=False):\n    if isinstance(var_index, Number):\n        var_index = \"uint32_t(\" + str(var_index) + \")\"\n    create_variant_state = f\"auto state = state_of_{base_state}__{name}<Output> {{ start_frame(_out), std::move(_state) }};\" if var_index and root_node else \"\"\n    set_variant_index = f\"serialize(_out, {var_index});\\n\" if var_index is not None else \"\"\n    set_command = (\"_state.f.end(_out);\" if not root_node else \"state.f.end(_out);\") if var_type != \"\" else \"\"\n    return_command = \"{ _out, std::move(_state._parent) }\" if var_type != \"\" and not root_node else \"{ _out, std::move(_state) }\"\n\n    allow_fragmented = False\n    if typ.name in ['bytes', 'sstring']:\n        typename = typ.name + '_view'\n        allow_fragmented = True\n    else:\n        typename = 'const ' + typ.name + '&'\n\n    writer = reindent(4, \"\"\"\n        after_{base_state}__{name}<Output> write_{name}{var_type}({typename} t) && {{\n            {create_variant_state}\n            {set_variant_index}\n            serialize(_out, t);\n            {set_command}\n            return {return_command};\n        }}\"\"\").format(**locals())\n    if allow_fragmented:\n        writer += reindent(4, \"\"\"\n        template<typename FragmentedBuffer>\n        requires FragmentRange<FragmentedBuffer>\n        after_{base_state}__{name}<Output> write_fragmented_{name}{var_type}(FragmentedBuffer&& fragments) && {{\n            {set_variant_index}\n            serialize_fragmented(_out, std::forward<FragmentedBuffer>(fragments));\n            {set_command}\n            return {return_command};\n        }}\"\"\").format(**locals())\n    return writer\n\n\ndef add_param_writer_object(name, base_state, typ, var_type=\"\", var_index=None, root_node=False):\n    var_type1 = \"_\" + var_type if var_type != \"\" else \"\"\n    if isinstance(var_index, Number):\n        var_index = \"uint32_t(\" + str(var_index) + \")\"\n    create_variant_state = f\"auto state = state_of_{base_state}__{name}<Output> {{ start_frame(_out), std::move(_state) }};\" if var_index and root_node else \"\"\n    set_variant_index = f\"serialize(_out, {var_index});\\n\" if var_index is not None else \"\"\n    state = \"std::move(_state)\" if not var_index or not root_node else \"std::move(state)\"\n    ret = reindent(4, \"\"\"\n        {base_state}__{name}{var_type1}<Output> start_{name}{var_type}() && {{\n            {create_variant_state}\n            {set_variant_index}\n            return {{ _out, {state} }};\n        }}\n    \"\"\").format(**locals())\n    if not is_stub(typ.name) and is_local_writable_type(typ):\n        ret += add_param_writer_basic_type(name, base_state, typ, var_type, var_index, root_node)\n    if is_stub(typ.name):\n        typename = typ.name\n        set_command = \"_state.f.end(_out);\" if var_type != \"\" else \"\"\n        return_command = \"{ _out, std::move(_state._parent) }\" if var_type != \"\" and not root_node else \"{ _out, std::move(_state) }\"\n        ret += reindent(4, \"\"\"\n            template<typename Serializer>\n            after_{base_state}__{name}<Output> {name}{var_type}(Serializer&& f) && {{\n                {set_variant_index}\n                f(writer_of_{typename}<Output>(_out));\n                {set_command}\n                return {return_command};\n            }}\"\"\").format(**locals())\n        ret += reindent(4, \"\"\"\n            template<typename AsyncSerializer>\n            future<after_{base_state}__{name}<Output>> {name}{var_type}_gently(AsyncSerializer&& f) && {{\n                {set_variant_index}\n                return futurize_invoke(f, writer_of_{typename}<Output>(_out)).then([this] {{\n                    {set_command}\n                    return after_{base_state}__{name}<Output>{return_command};\n                }});\n            }}\"\"\").format(**locals())\n    return ret\n\n\ndef add_param_write(current, base_state, vector=False, root_node=False):\n    typ = current.type\n    res = \"\"\n    name = get_member_name(current.name)\n    if is_basic_type(typ):\n        res = res + add_param_writer_basic_type(name, base_state, typ)\n    elif is_optional(typ):\n        res = res + reindent(4, f\"\"\"\n    after_{base_state}__{name}<Output> skip_{name}() && {{\n        serialize(_out, false);\n        return {{ _out, std::move(_state) }};\n    }}\"\"\")\n        if is_basic_type(typ.template_parameters[0]):\n            res = res + add_param_writer_basic_type(name, base_state, typ.template_parameters[0], \"\", \"true\")\n        elif is_local_writable_type(typ.template_parameters[0]):\n            res = res + add_param_writer_object(name, base_state[0][1], typ, \"\", \"true\")\n        else:\n            print(\"non supported optional type \", typ.template_parameters[0])\n    elif is_vector(typ):\n        set_size = \"_size.set(_out, 0);\" if vector else \"serialize(_out, size_type(0));\"\n\n        res = res + f\"\"\"\n    {base_state}__{name}<Output> start_{name}() && {{\n        return {{ _out, std::move(_state) }};\n    }}\n\n    after_{base_state}__{name}<Output> skip_{name}() && {{\n        {set_size}\n        return {{ _out, std::move(_state) }};\n    }}\n\"\"\"\n    elif is_local_writable_type(typ):\n        res = res + add_param_writer_object(name, base_state, typ)\n    elif is_variant(typ):\n        for idx, p in enumerate(typ.template_parameters):\n            if is_basic_type(p):\n                res = res + add_param_writer_basic_type(name, base_state, p, \"_\" + param_type(p), idx, root_node)\n            elif is_variant(p):\n                res = res + add_param_writer_object(name, base_state, p, '_' + \"variant\", idx, root_node)\n            elif is_local_writable_type(p):\n                res = res + add_param_writer_object(name, base_state, p, '_' + param_type(p), idx, root_node)\n    else:\n        print(\"something is wrong with type\", typ)\n    return res\n\n\ndef get_return_struct(variant_node, classes):\n    if not variant_node:\n        return classes\n    if classes[-2] == \"variant\":\n        return classes[:-2]\n    return classes[:-1]\n\n\ndef add_variant_end_method(base_state, name, classes):\n\n    return_struct = \"after_\" + base_state + '<Output>'\n    return f\"\"\"\n    {return_struct}  end_{name}() && {{\n        _state.f.end(_out);\n        _state._parent.f.end(_out);\n        return {{ _out, std::move(_state._parent._parent) }};\n    }}\n\"\"\"\n\n\ndef add_end_method(parents, name, variant_node=False, return_value=True):\n    if variant_node:\n        return add_variant_end_method(parents, name, return_value)\n    base_state = parents + \"__\" + name\n    if return_value:\n        return_struct = \"after_\" + base_state + '<Output>'\n        return f\"\"\"\n    {return_struct}  end_{name}() && {{\n        _state.f.end(_out);\n        return {{ _out, std::move(_state._parent) }};\n    }}\n\"\"\"\n    return f\"\"\"\n    void  end_{name}() {{\n        _state.f.end(_out);\n    }}\n\"\"\"\n\n\ndef add_vector_placeholder():\n    return \"\"\"    place_holder<Output> _size;\n    size_type _count = 0;\"\"\"\n\n\ndef add_node(cout, name, member, base_state, prefix, parents, fun, is_type_vector=False, is_type_final=False):\n    struct_name = prefix + name\n    if member and is_type_vector:\n        vector_placeholder = add_vector_placeholder()\n        vector_init = \"\\n            , _size(start_place_holder(out))\"\n    else:\n        vector_placeholder = \"\"\n        vector_init = \"\"\n    if vector_init != \"\" or prefix == \"\":\n        state_init = \"_state{start_frame(out), std::move(state)}\" if parents != base_state and not is_type_final else \"_state(state)\"\n    else:\n        if member and is_variant(member) and parents != base_state:\n            state_init = \"_state{start_frame(out), std::move(state)}\"\n        else:\n            state_init = \"\"\n    if prefix == \"writer_of_\":\n        constructor = f\"\"\"{struct_name}(Output& out)\n            : _out(out)\n            , _state{{start_frame(out)}}{vector_init}\n            {{}}\"\"\"\n    elif state_init != \"\":\n        constructor = f\"\"\"{struct_name}(Output& out, state_of_{parents}<Output> state)\n            : _out(out)\n            , {state_init}{vector_init}\n            {{}}\"\"\"\n    else:\n        constructor = \"\"\n    fprintln(cout, f\"\"\"\ntemplate<typename Output>\nstruct {struct_name} {{\n    Output& _out;\n    state_of_{base_state}<Output> _state;\n    {vector_placeholder}\n    {constructor}\n    {fun}\n}};\"\"\")\n\n\ndef add_vector_node(cout, member, base_state, parents):\n    if member.type.template_parameters[0].name:\n        add_template_writer_node(cout, member.type.template_parameters[0])\n    add_node(cout, base_state + \"__\" + member.name, member.type, base_state, \"\", parents, vector_add_method(member, base_state), True)\n\n\noptional_nodes = set()\n\n\ndef add_optional_node(cout, typ):\n    global optional_nodes\n    full_type = flat_type(typ)\n    if full_type in optional_nodes:\n        return\n    optional_nodes.add(full_type)\n    fprintln(cout, reindent(0, f\"\"\"\ntemplate<typename Output>\nstruct writer_of_{full_type} {{\n    Output& _out;\n    {optional_add_methods(typ.template_parameters[0])}\n}};\"\"\"))\n\n\ndef add_variant_nodes(cout, member, param, base_state, parents, classes):\n    par = base_state + \"__\" + member.name\n    for typ in param.template_parameters:\n        if is_local_writable_type(typ):\n            handle_visitors_nodes(local_writable_types[param_type(typ)], cout, True, classes + [member.name, local_writable_types[param_type(typ)].name])\n        if is_variant(typ):\n            name = base_state + \"__\" + member.name + \"__variant\"\n            new_member = copy(member) # shallow copy\n            new_member.type = typ\n            new_member.name = \"variant\"\n            return_struct = \"after_\" + par\n            end_method = f\"\"\"\n    {return_struct}<Output>  end_variant() && {{\n        _state.f.end(_out);\n        return {{ _out, std::move(_state._parent) }};\n    }}\n\"\"\"\n            add_node(cout, name, None, base_state + \"__\" + member.name, \"after_\", name, end_method)\n            add_variant_nodes(cout, new_member, typ, par, name, classes + [member.name])\n            add_node(cout, name, typ, name, \"\", par, add_param_write(new_member, par))\n\n\nwriters = set()\n\n\ndef add_template_writer_node(cout, typ):\n    if is_optional(typ):\n        add_optional_node(cout, typ)\n\n\ndef add_nodes_when_needed(cout, member, base_state_name, parents, member_classes):\n    if is_vector(member.type):\n        add_vector_node(cout, member, base_state_name, base_state_name)\n    elif is_variant(member.type):\n        add_variant_nodes(cout, member, member.type, base_state_name, parents, member_classes)\n    elif is_local_writable_type(member.type):\n        handle_visitors_nodes(local_writable_types[member.type.name], cout, False, member_classes + [member.name])\n\n\ndef handle_visitors_nodes(cls, cout, variant_node=False, classes=[]):\n    global writers\n    # for root node, only generate once\n    if not classes:\n        if cls.name in writers:\n            return\n        writers.add(cls.name)\n\n    members = get_members(cls)\n    if classes:\n        base_state_name = \"__\".join(classes)\n        if variant_node:\n            parents = \"__\".join(classes[:-1])\n        else:\n            parents = \"__\".join(classes[:-1])\n        current_name = classes[-1]\n    else:\n        base_state_name = cls.name\n        current_name = cls.name\n        parents = \"\"\n    member_classes = classes if classes else [current_name]\n    prefix = \"\" if classes else \"writer_of_\"\n    if not members:\n        add_node(cout, base_state_name, None, base_state_name, prefix, parents, add_end_method(parents, current_name, variant_node, classes), False, cls.final)\n        return\n    add_node(cout, base_state_name + \"__\" + get_member_name(members[-1].name), members[-1].type, base_state_name, \"after_\", base_state_name, add_end_method(parents, current_name, variant_node, classes))\n    # Create writer and reader for include class\n    if not variant_node:\n        for member in get_dependency(cls):\n            handle_visitors_nodes(local_writable_types[member], cout)\n    for ind in reversed(range(1, len(members))):\n        member = members[ind]\n        add_nodes_when_needed(cout, member, base_state_name, parents, member_classes)\n        variant_state = base_state_name + \"__\" + get_member_name(member.name) if is_variant(member.type) else base_state_name\n        add_node(cout, base_state_name + \"__\" + get_member_name(members[ind - 1].name), member.type, variant_state, \"after_\", base_state_name, add_param_write(member, base_state_name), False)\n    member = members[0]\n    add_nodes_when_needed(cout, member, base_state_name, parents, member_classes)\n    add_node(cout, base_state_name, member.type, base_state_name, prefix, parents, add_param_write(member, base_state_name, False, not classes), False, cls.final)\n\n\ndef register_local_type(cls):\n    global local_types\n    local_types[cls.name] = cls\n\n\ndef register_writable_local_type(cls):\n    global local_writable_types\n    global stubs\n    if not cls.attribute or cls.attribute != 'writable':\n        return\n    local_writable_types[cls.name] = cls\n    if cls.stub:\n        stubs.add(cls.name)\n\n\ndef register_rpc_verb(verb):\n    global rpc_verbs\n    rpc_verbs[verb.name] = verb\n\n\ndef sort_dependencies():\n    dep_tree = {}\n    res = []\n    for k in local_writable_types:\n        cls = local_writable_types[k]\n        dep_tree[k] = get_dependency(cls)\n    while (len(dep_tree) > 0):\n        found = sorted(k for k in dep_tree if not dep_tree[k])\n        res = res + [k for k in found]\n        for k in found:\n            dep_tree.pop(k)\n        for k in dep_tree:\n            if dep_tree[k]:\n                dep_tree[k].difference_update(found)\n\n    return res\n\n\ndef join_template_view(lst, more_types=[]):\n    return \"<\" + \", \".join([param_view_type(l) for l in lst] + more_types) + \">\"\n\n\ndef to_view(val):\n    if val in local_writable_types:\n        return val + \"_view\"\n    return val\n\n\ndef param_view_type(t):\n    if isinstance(t, BasicType):\n        return to_view(t.name)\n    elif isinstance(t, TemplateType):\n        additional_types = []\n        if t.name == \"boost::variant\" or t.name == \"std::variant\":\n            additional_types.append(\"unknown_variant_type\")\n        return t.name + join_template_view(t.template_parameters, additional_types)\n\n\ndef element_type(t):\n    assert isinstance(t, TemplateType)\n    assert len(t.template_parameters) == 1\n    assert t.name != \"boost::variant\" and t.name != \"std::variant\"\n    return param_view_type(t.template_parameters[0])\n\n\nread_sizes = set()\n\n\ndef add_variant_read_size(hout, typ):\n    global read_sizes\n    t = param_view_type(typ)\n    if t in read_sizes:\n        return\n    if not is_variant(typ):\n        return\n    for p in typ.template_parameters:\n        if is_variant(p):\n            add_variant_read_size(hout, p)\n    read_sizes.add(t)\n    fprintln(hout, f\"\"\"\ntemplate<typename Input>\ninline void skip(Input& v, std::type_identity<{t}>) {{\n  return seastar::with_serialized_stream(v, [] (auto& v) {{\n    size_type ln = deserialize(v, std::type_identity<size_type>());\n    v.skip(ln - sizeof(size_type));\n  }});\n}}\"\"\")\n\n    fprintln(hout, f\"\"\"\ntemplate<typename Input>\n{t} deserialize(Input& v, std::type_identity<{t}>) {{\n  return seastar::with_serialized_stream(v, [] (auto& v) {{\n    auto in = v;\n    deserialize(in, std::type_identity<size_type>());\n    size_type o = deserialize(in, std::type_identity<size_type>());\n    \"\"\")\n    for index, param in enumerate(typ.template_parameters):\n        fprintln(hout, f\"\"\"\n    if (o == {index}) {{\n        v.skip(sizeof(size_type)*2);\n        return {t}(deserialize(v, std::type_identity<{param_view_type(param)}>()));\n    }}\"\"\")\n    fprintln(hout, f'    return {t}(deserialize(v, std::type_identity<unknown_variant_type>()));\\n  }});\\n}}')\n\n\ndef add_view(cout, cls):\n    members = get_members(cls)\n    for m in members:\n        add_variant_read_size(cout, m.type)\n\n    fprintln(cout, f\"\"\"struct {cls.name}_view {{\n    utils::input_stream v;\n    \"\"\")\n\n    if not is_stub(cls.name) and is_local_writable_type(cls.name):\n        fprintln(cout, reindent(4, f\"\"\"\n            operator {cls.name}() const {{\n               auto in = v;\n               return deserialize(in, std::type_identity<{cls.name}>());\n            }}\n        \"\"\"))\n\n    skip = \"\" if cls.final else \"ser::skip(in, std::type_identity<size_type>());\"\n    local_names = {}\n    for m in members:\n        name = get_member_name(m.name)\n        local_names[name] = \"this->\" + name + \"()\"\n        full_type = param_view_type(m.type)\n        if m.attribute:\n            deflt = m.default_value if m.default_value else param_type(m.type) + \"()\"\n            if deflt in local_names:\n                deflt = local_names[deflt]\n            deser = f\"(in.size()>0) ? {DESERIALIZER}(in, std::type_identity<{full_type}>()) : {deflt}\"\n        else:\n            deser = f\"{DESERIALIZER}(in, std::type_identity<{full_type}>())\"\n\n        if is_vector(m.type):\n            elem_type = element_type(m.type)\n            fprintln(cout, reindent(4, \"\"\"\n                auto {name}() const {{\n                  return seastar::with_serialized_stream(v, [] (auto& v) {{\n                   auto in = v;\n                   {skip}\n                   return vector_deserializer<{elem_type}>(in);\n                  }});\n                }}\n            \"\"\").format(f=DESERIALIZER, **locals()))\n        else:\n            fprintln(cout, reindent(4, \"\"\"\n                auto {name}() const {{\n                  return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype({f}(std::declval<utils::input_stream&>(), std::type_identity<{full_type}>())) {{\n                   std::ignore = this;\n                   auto in = v;\n                   {skip}\n                   return {deser};\n                  }});\n                }}\n            \"\"\").format(f=DESERIALIZER, **locals()))\n\n        skip = skip + f\"\\n       ser::skip(in, std::type_identity<{full_type}>());\"\n\n    fprintln(cout, \"};\")\n    skip_impl = \"auto& in = v;\\n       \" + skip if cls.final else \"v.skip(read_frame_size(v));\"\n    if skip == \"\":\n        skip_impl = \"\"\n\n    fprintln(cout, f\"\"\"\ntemplate<>\nstruct serializer<{cls.name}_view> {{\n    template<typename Input>\n    static {cls.name}_view read(Input& v) {{\n      return seastar::with_serialized_stream(v, [] (auto& v) {{\n        auto v_start = v;\n        auto start_size = v.size();\n        skip(v);\n        return {cls.name}_view{{v_start.read_substream(start_size - v.size())}};\n      }});\n    }}\n    template<typename Output>\n    static void write(Output& out, {cls.name}_view v) {{\n        v.v.copy_to(out);\n    }}\n    template<typename Input>\n    static void skip(Input& v) {{\n      return seastar::with_serialized_stream(v, [] (auto& v) {{\n        {skip_impl}\n      }});\n    }}\n}};\n\"\"\")\n\n\ndef add_views(cout):\n    for k in sort_dependencies():\n        add_view(cout, local_writable_types[k])\n\n\ndef add_visitors(cout):\n    if not local_writable_types:\n        return\n    add_views(cout)\n    fprintln(cout, \"\\n////// State holders\")\n    for k in local_writable_types:\n        handle_visitors_state(local_writable_types[k], cout)\n    fprintln(cout, \"\\n////// Nodes\")\n    for k in sort_dependencies():\n        handle_visitors_nodes(local_writable_types[k], cout)\n\n\ndef handle_include(obj, hout, cout):\n    '''Generate #include directives.\n    '''\n\n    if obj.is_module:\n        fprintln(hout, f'#include \"{obj.module_name}.dist.hh\"')\n        fprintln(cout, f'#include \"{obj.module_name}.dist.impl.hh\"')\n    else:\n        fprintln(hout, f'#include \"{obj.name}\"')\n\ndef handle_includes(tree, hout, cout):\n    for obj in tree:\n        if isinstance(obj, Include):\n            handle_include(obj, hout, cout)\n        else:\n            pass\n\ndef handle_class(cls, hout, cout):\n    '''Generate serializer class declarations and definitions for a class\n    defined in IDL.\n    '''\n    if cls.stub:\n        return\n    is_tpl = cls.template_params is not None\n    template_param_list = cls.template_params if is_tpl else []\n    template_params = template_params_str(template_param_list + cls.parent_template_params)\n    template_class_param = \"<\" + \",\".join(map(lambda a: a.name, template_param_list)) + \">\" if is_tpl else \"\"\n\n    name = cls.ns_qualified_name()\n    full_name = name + template_class_param\n    # Handle sub-types: can be either enum or class\n    for member in cls.members:\n        if isinstance(member, ClassDef):\n            handle_class(member, hout, cout)\n        elif isinstance(member, EnumDef):\n            handle_enum(member, hout, cout)\n    declare_methods(hout, full_name, template_params)\n\n    cls.serializer_write_impl(cout)\n    cls.serializer_read_impl(cout)\n    cls.serializer_skip_impl(cout)\n\n\ndef handle_objects(tree, hout, cout):\n    '''Main generation procedure: traverse AST and generate serializers for\n    classes/enums defined in the current IDL.\n    '''\n    for obj in tree:\n        if isinstance(obj, ClassDef):\n            handle_class(obj, hout, cout)\n        elif isinstance(obj, EnumDef):\n            handle_enum(obj, hout, cout)\n        elif isinstance(obj, NamespaceDef):\n            handle_objects(obj.members, hout, cout)\n        elif isinstance(obj, RpcVerb):\n            pass\n        elif isinstance(obj, Include):\n            pass\n        else:\n            print(f\"Unknown type: {obj}\")\n\n\ndef generate_rpc_verbs_declarations(hout, module_name):\n    fprintln(hout, f\"\\n// RPC verbs defined in the '{module_name}' module\\n\")\n    fprintln(hout, f'struct {module_name}_rpc_verbs {{')\n    for name, verb in rpc_verbs.items():\n        fprintln(hout, reindent(4, f'''static void register_{name}(netw::messaging_service* ms,\n    std::function<{verb.handler_function_return_values()} ({verb.handler_function_parameters_str()})>&&);\nstatic future<> unregister_{name}(netw::messaging_service* ms);\nstatic {verb.send_function_return_type()} send_{name}({verb.send_function_signature_params_list(include_placeholder_names=False, dst_type=\"netw::msg_addr\")});\nstatic {verb.send_function_return_type()} send_{name}({verb.send_function_signature_params_list(include_placeholder_names=False, dst_type=\"locator::host_id\")});\n'''))\n\n    fprintln(hout, reindent(4, 'static future<> unregister(netw::messaging_service* ms);'))\n    fprintln(hout, '};\\n')\n\n\ndef generate_rpc_verbs_definitions(cout, module_name):\n    fprintln(cout, f\"\\n// RPC verbs defined in the '{module_name}' module\")\n    for name, verb in rpc_verbs.items():\n        fprintln(cout, f'''\nvoid {module_name}_rpc_verbs::register_{name}(netw::messaging_service* ms,\n        std::function<{verb.handler_function_return_values()} ({verb.handler_function_parameters_str()})>&& f) {{\n    register_handler(ms, {verb.messaging_verb_enum_case()}, std::move(f));\n}}\n\nfuture<> {module_name}_rpc_verbs::unregister_{name}(netw::messaging_service* ms) {{\n    return ms->unregister_handler({verb.messaging_verb_enum_case()});\n}}\n\n{verb.send_function_return_type()} {module_name}_rpc_verbs::send_{name}({verb.send_function_signature_params_list(include_placeholder_names=True, dst_type=\"netw::msg_addr\")}) {{\n    {verb.send_function_invocation()}\n}}\n\n{verb.send_function_return_type()} {module_name}_rpc_verbs::send_{name}({verb.send_function_signature_params_list(include_placeholder_names=True, dst_type=\"locator::host_id\")}) {{\n    {verb.send_function_invocation()}\n}}''')\n\n    fprintln(cout, f'''\nfuture<> {module_name}_rpc_verbs::unregister(netw::messaging_service* ms) {{\n    return when_all_succeed({', '.join([f'unregister_{v}(ms)' for v in rpc_verbs.keys()])}).discard_result();\n}}\n''')\n\n\ndef generate_rpc_verbs(hout, cout, module_name):\n    if not rpc_verbs:\n        return\n    generate_rpc_verbs_declarations(hout, module_name)\n    generate_rpc_verbs_definitions(cout, module_name)\n\n\ndef handle_types(tree):\n    '''Traverse AST and record all locally defined types, i.e. defined in\n    the currently processed IDL file.\n    '''\n    for obj in tree:\n        if isinstance(obj, ClassDef):\n            register_local_type(obj)\n            register_writable_local_type(obj)\n        elif isinstance(obj, RpcVerb):\n            register_rpc_verb(obj)\n        elif isinstance(obj, EnumDef):\n            pass\n        elif isinstance(obj, NamespaceDef):\n            handle_types(obj.members)\n        elif isinstance(obj, Include):\n            pass\n        else:\n            print(f\"Unknown object type: {obj}\")\n\n\ndef setup_additional_metadata(tree, ns_context = [], parent_template_params=[]):\n    '''Cache additional metadata for each type declaration directly in the AST node.\n\n    This currently includes namespace info and template parameters for the\n    parent scope (applicable only to enums and classes).'''\n    for obj in tree:\n        if isinstance(obj, NamespaceDef):\n            setup_additional_metadata(obj.members, ns_context + [obj.name])\n        elif isinstance(obj, EnumDef):\n            obj.ns_context = ns_context\n\n            obj.parent_template_params = parent_template_params\n\n            obj.template_declaration = \"template <\" + template_params_str(parent_template_params) + \">\" \\\n                if parent_template_params else \"\"\n        elif isinstance(obj, ClassDef):\n            obj.ns_context = ns_context\n            # need to account for nested types\n            current_scope = obj.name\n            if obj.template_params:\n                # current scope name should consider template classes as well\n                current_scope += \"<\" + \",\".join(tp.name for tp in obj.template_params) + \">\"\n\n            obj.template_param_names_str = \"<\" + \",\".join(map(lambda a: a.name, obj.template_params)) + \">\" \\\n                if obj.template_params else \"\"\n\n            obj.parent_template_params = parent_template_params\n\n            obj.template_declaration = \"template <\" + template_params_str(obj.template_params + obj.parent_template_params) + \">\" \\\n                if obj.template_params else \"\"\n\n            nested_template_params = parent_template_params + obj.template_params if obj.template_params else []\n\n            setup_additional_metadata(obj.members, ns_context + [current_scope], nested_template_params)\n\n\ndef load_file(name):\n    cname = config.o.replace('.hh', '.impl.hh') if config.o else name.replace(EXTENSION, '.dist.impl.hh')\n    hname = config.o or name.replace(EXTENSION, '.dist.hh')\n    cout = open(cname, \"w+\")\n    hout = open(hname, \"w+\")\n    print_cw(hout)\n    fprintln(hout, \"\"\"\n /*\n  * The generate code should be included in a header file after\n  * The object definition\n  */\n    \"\"\")\n    fprintln(hout, \"#include \\\"serializer.hh\\\"\\n\")\n\n    print_cw(cout)\n    fprintln(cout, f\"#include \\\"{os.path.basename(hname)}\\\"\")\n    fprintln(cout, \"#include \\\"serializer_impl.hh\\\"\")\n    fprintln(cout, \"#include \\\"serialization_visitors.hh\\\"\")\n\n    def maybe_open_namespace(printed=False):\n        if config.ns != '' and not printed:\n            fprintln(hout, f\"namespace {config.ns} {{\")\n            fprintln(cout, f\"namespace {config.ns} {{\")\n            printed = True\n        return printed\n\n    data = parse_file(name)\n    if data:\n        handle_includes(data, hout, cout)\n        printed = maybe_open_namespace()\n        setup_additional_metadata(data)\n        handle_types(data)\n        handle_objects(data, hout, cout)\n\n        module_name = os.path.basename(name)\n        module_name = module_name[:module_name.find('.')]\n        generate_rpc_verbs(hout, cout, module_name)\n    maybe_open_namespace(printed)\n    add_visitors(cout)\n    if config.ns != '':\n        fprintln(hout, f\"}} // {config.ns}\")\n        fprintln(cout, f\"}} // {config.ns}\")\n    cout.close()\n    hout.close()\n\n\ndef general_include(files):\n    '''Write serialization-related header includes in the generated files'''\n    name = config.o if config.o else \"serializer.dist.hh\"\n    # Header file containing implementation of serializers and other supporting classes \n    cout = open(name.replace('.hh', '.impl.hh'), \"w+\")\n    # Header file with serializer declarations\n    hout = open(name, \"w+\")\n    print_cw(cout)\n    print_cw(hout)\n    for n in files:\n        fprintln(hout, '#include \"' + n + '\"')\n        fprintln(cout, '#include \"' + n.replace(\".dist.hh\", '.dist.impl.hh') + '\"')\n    cout.close()\n    hout.close()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"\"\"Generate serializer helper function\"\"\")\n\n    parser.add_argument('-o', help='Output file', default='')\n    parser.add_argument('-f', help='input file', default='')\n    parser.add_argument('--ns', help=\"\"\"namespace, when set function will be created\n    under the given namespace\"\"\", default='')\n    parser.add_argument('file', nargs='*', help=\"combine one or more file names for the general include files\")\n\n    config = parser.parse_args()\n    if config.file:\n        general_include(config.file)\n    elif config.f != '':\n        load_file(config.f)\n"
        },
        {
          "name": "idl",
          "type": "tree",
          "content": null
        },
        {
          "name": "index",
          "type": "tree",
          "content": null
        },
        {
          "name": "inet_address_vectors.hh",
          "type": "blob",
          "size": 0.55078125,
          "content": "/*\n * Copyright (C) 2021-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"gms/inet_address.hh\"\n#include \"locator/host_id.hh\"\n#include \"utils/small_vector.hh\"\n\nusing inet_address_vector_replica_set = utils::small_vector<gms::inet_address, 3>;\n\nusing inet_address_vector_topology_change = utils::small_vector<gms::inet_address, 1>;\n\nusing host_id_vector_replica_set = utils::small_vector<locator::host_id, 3>;\n\nusing host_id_vector_topology_change = utils::small_vector<locator::host_id, 1>;\n"
        },
        {
          "name": "init.cc",
          "type": "blob",
          "size": 4.8857421875,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"init.hh\"\n#include \"gms/inet_address.hh\"\n#include \"seastarx.hh\"\n#include \"db/config.hh\"\n\n#include <boost/algorithm/string/trim.hpp>\n#include <seastar/core/coroutine.hh>\n\nlogging::logger startlog(\"init\");\n\nstd::set<gms::inet_address> get_seeds_from_db_config(const db::config& cfg,\n                                                     const gms::inet_address broadcast_address,\n                                                     const bool fail_on_lookup_error) {\n    auto preferred = cfg.listen_interface_prefer_ipv6() ? std::make_optional(net::inet_address::family::INET6) : std::nullopt;\n    auto family = cfg.enable_ipv6_dns_lookup() || preferred ? std::nullopt : std::make_optional(net::inet_address::family::INET);\n    const auto listen = gms::inet_address::lookup(cfg.listen_address(), family).get();\n    auto seed_provider = cfg.seed_provider();\n\n    std::set<gms::inet_address> seeds;\n    if (seed_provider.parameters.contains(\"seeds\")) {\n        size_t begin = 0;\n        size_t next = 0;\n        sstring seeds_str = seed_provider.parameters.find(\"seeds\")->second;\n        while (begin < seeds_str.length() && begin != (next=seeds_str.find(\",\",begin))) {\n            auto seed = boost::trim_copy(seeds_str.substr(begin,next-begin));\n            try {\n                seeds.emplace(gms::inet_address::lookup(seed, family, preferred).get());\n            } catch (...) {\n                if (fail_on_lookup_error) {\n                    startlog.error(\"Bad configuration: invalid value in 'seeds': '{}': {}\", seed, std::current_exception());\n                    throw bad_configuration_error();\n                }\n                startlog.warn(\"Bad configuration: invalid value in 'seeds': '{}': {}. Node will continue booting since already bootstrapped.\",\n                               seed,\n                               std::current_exception());\n            }\n            begin = next+1;\n        }\n    }\n    if (seeds.empty()) {\n        seeds.emplace(gms::inet_address(\"127.0.0.1\"));\n    }\n    startlog.info(\"seeds={{{}}}, listen_address={}, broadcast_address={}\",\n            fmt::join(seeds, \", \"), listen, broadcast_address);\n    if (broadcast_address != listen && seeds.contains(listen)) {\n        startlog.error(\"Use broadcast_address instead of listen_address for seeds list\");\n        throw std::runtime_error(\"Use broadcast_address for seeds list\");\n    }\n    if (!cfg.replace_node_first_boot().empty() && seeds.contains(broadcast_address)) {\n        startlog.error(\"Bad configuration: replace-node-first-boot is not allowed for seed nodes\");\n        throw bad_configuration_error();\n    }\n    if ((!cfg.replace_address_first_boot().empty() || !cfg.replace_address().empty()) && seeds.contains(broadcast_address)) {\n        startlog.error(\"Bad configuration: replace-address and replace-address-first-boot are not allowed for seed nodes\");\n        throw bad_configuration_error();\n    }\n\n    return seeds;\n}\n\n\nstd::vector<std::reference_wrapper<configurable>>& configurable::configurables() {\n    static std::vector<std::reference_wrapper<configurable>> configurables;\n    return configurables;\n}\n\nvoid configurable::register_configurable(configurable & c) {\n    configurables().emplace_back(std::ref(c));\n}\n\nvoid configurable::append_all(db::config& cfg, boost::program_options::options_description_easy_init& init) {\n    for (configurable& c : configurables()) {\n        c.append_options(cfg, init);\n    }\n}\n\nfuture<configurable::notify_set> configurable::init_all(const boost::program_options::variables_map& opts, const db::config& cfg, db::extensions& exts, const service_set& services) {\n    notify_set res;\n    for (auto& c : configurables()) {\n        auto f = co_await c.get().initialize_ex(opts, cfg, exts, services);\n        if (f) {\n            res._listeners.emplace_back(std::move(f));\n        }\n    }\n    co_return res;\n}\n\nfuture<configurable::notify_set> configurable::init_all(const db::config& cfg, db::extensions& exts, const service_set& services) {\n    return do_with(boost::program_options::variables_map{}, [&](auto& opts) {\n        return init_all(opts, cfg, exts, services);\n    });\n}\n\nfuture<> configurable::notify_set::notify_all(system_state e) {\n    co_return co_await do_for_each(_listeners, [e](const notify_func& c) {\n        return c(e);\n    });\n}\n\nclass service_set::impl {\npublic:\n    void add(std::any value) {\n        _services.emplace(value.type(), value);\n    }\n    std::any find(const std::type_info& type) const {\n        return _services.at(type);\n    }\nprivate:\n    std::unordered_map<std::type_index, std::any> _services;\n};\n\nservice_set::service_set()\n    : _impl(std::make_unique<impl>())\n{}\nservice_set::~service_set() = default;\n\nvoid service_set::add(std::any value) {\n    _impl->add(std::move(value));\n} \n\nstd::any service_set::find(const std::type_info& type) const {\n    return _impl->find(type);\n}\n"
        },
        {
          "name": "init.hh",
          "type": "blob",
          "size": 3.9677734375,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n#pragma once\n\n#include <any>\n\n#include <seastar/core/sstring.hh>\n#include <seastar/core/future.hh>\n#include <seastar/core/distributed.hh>\n#include <seastar/core/abort_source.hh>\n#include \"utils/log.hh\"\n#include \"seastarx.hh\"\n#include <boost/program_options.hpp>\n\nnamespace db {\nclass extensions;\nclass seed_provider_type;\nclass config;\nnamespace view {\nclass view_update_generator;\n}\n}\n\nnamespace gms {\nclass feature_service;\nclass inet_address;\n}\n\nextern logging::logger startlog;\n\nclass bad_configuration_error : public std::exception {};\n\n[[nodiscard]] std::set<gms::inet_address> get_seeds_from_db_config(const db::config& cfg,\n                                                                   gms::inet_address broadcast_address,\n                                                                   bool fail_on_lookup_error);\n\nclass service_set {\npublic:\n    service_set();\n    ~service_set();\n    template<typename... Args>\n    service_set(seastar::sharded<Args>&... args) \n        : service_set()\n    {\n        (..., add(args));\n    }\n    template<typename T>\n    void add(seastar::sharded<T>& t) {\n        add(std::any{std::addressof(t)});\n    }\n    template<typename T>\n    seastar::sharded<T>& find() const {\n        return *std::any_cast<seastar::sharded<T>*>(find(typeid(seastar::sharded<T>*)));\n    }\nprivate:\n    void add(std::any);\n    std::any find(const std::type_info&) const;\n\n    class impl;\n    std::unique_ptr<impl> _impl;\n};\n\n/**\n * Very simplistic config registry. Allows hooking in a config object\n * to the \"main\" sequence.\n */\nclass configurable {\npublic:\n    configurable() {\n        // We auto register. Not that like cycle is assumed to be forever\n        // and scope should be managed elsewhere.\n        register_configurable(*this);\n    }\n    virtual ~configurable()\n    {}\n    // Hook to add command line options and/or add main config options\n    virtual void append_options(db::config&, boost::program_options::options_description_easy_init&)\n    {};\n    // Called after command line is parsed and db/config populated.\n    // Hooked config can for example take this opportunity to load any file(s).\n    virtual future<> initialize(const boost::program_options::variables_map&) {\n        return make_ready_future();\n    }\n    virtual future<> initialize(const boost::program_options::variables_map& map, const db::config& cfg, db::extensions& exts) {\n        return initialize(map);\n    }\n\n    /**\n     * State of initiating system for optional \n     * notification callback to objects created by\n     * `initialize`\n     */\n    enum class system_state {\n        started,\n        stopped,\n    };\n\n    using notify_func = std::function<future<>(system_state)>;\n\n    /** Hooks should override this to allow adding a notification function to the startup sequence. */\n    virtual future<notify_func> initialize_ex(const boost::program_options::variables_map& map, const db::config& cfg, db::extensions& exts) {\n        return initialize(map, cfg, exts).then([] { return notify_func{}; });\n    }\n\n    virtual future<notify_func> initialize_ex(const boost::program_options::variables_map& map, const db::config& cfg, db::extensions& exts, const service_set&) {\n        return initialize_ex(map, cfg, exts);\n    }\n\n    class notify_set {\n    public:\n        future<> notify_all(system_state);\n    private:\n        friend class configurable;\n        std::vector<notify_func> _listeners;\n    };\n\n    // visible for testing\n    static std::vector<std::reference_wrapper<configurable>>& configurables();\n    static future<notify_set> init_all(const boost::program_options::variables_map&, const db::config&, db::extensions&, const service_set& = {});\n    static future<notify_set> init_all(const db::config&, db::extensions&, const service_set& = {});\n    static void append_all(db::config&, boost::program_options::options_description_easy_init&);\nprivate:\n    static void register_configurable(configurable &);\n};\n"
        },
        {
          "name": "install-dependencies.sh",
          "type": "blob",
          "size": 11.349609375,
          "content": "#!/bin/bash -e\n#\n# This file is open source software, licensed to you under the terms\n# of the Apache License, Version 2.0 (the \"License\").  See the NOTICE file\n# distributed with this work for additional information regarding copyright\n# ownership.  You may not use this file except in compliance with the License.\n#\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n# os-release may be missing in container environment by default.\nif [ -f \"/etc/os-release\" ]; then\n    . /etc/os-release\nelif [ -f \"/etc/arch-release\" ]; then\n    export ID=arch\nelse\n    echo \"/etc/os-release missing.\"\n    exit 1\nfi\n\ndebian_base_packages=(\n    clang\n    gdb\n    cargo\n    wabt\n    liblua5.3-dev\n    python3-aiohttp\n    python3-pyparsing\n    python3-colorama\n    python3-tabulate\n    libsnappy-dev\n    libjsoncpp-dev\n    rapidjson-dev\n    scylla-antlr35-c++-dev\n    git\n    pigz\n    libunistring-dev\n    libzstd-dev\n    libdeflate-dev\n    librapidxml-dev\n    libcrypto++-dev\n    libxxhash-dev\n    slapd\n    ldap-utils\n    libcpp-jwt-dev\n)\n\nfedora_packages=(\n    clang\n    compiler-rt\n    libasan\n    libubsan\n    gdb\n    lua-devel\n    yaml-cpp-devel\n    antlr3-tool\n    antlr3-C++-devel\n    jsoncpp-devel\n    rapidjson-devel\n    snappy-devel\n    libdeflate-devel\n    systemd-devel\n    cryptopp-devel\n    git\n    git-lfs\n    python\n    sudo\n    patchelf\n    python3\n    python3-aiohttp\n    python3-pip\n    python3-magic\n    python3-colorama\n    python3-tabulate\n    python3-boto3\n    python3-pytest\n    python3-pytest-asyncio\n    python3-redis\n    python3-unidiff\n    python3-humanfriendly\n    python3-jinja2\n    dnf-utils\n    pigz\n    net-tools\n    tar\n    gzip\n    gawk\n    util-linux\n    ethtool\n    hwloc\n    glibc-langpack-en\n    xxhash-devel\n    makeself\n    libzstd-static libzstd-devel\n    lz4-static lz4-devel\n    rpm-build\n    devscripts\n    debhelper\n    fakeroot\n    file\n    dpkg-dev\n    curl\n    rust\n    cargo\n    rapidxml-devel\n    rust-std-static-wasm32-wasi\n    wabt\n    binaryen\n    lcov\n    java-11-openjdk-devel # for tools/java\n\n    llvm-bolt\n    moreutils\n    iproute\n    llvm\n    openldap-servers\n    openldap-devel\n    toxiproxy\n    cyrus-sasl\n    fipscheck\n    cpp-jwt-devel\n)\n\n# lld is not available on s390x, see\n# https://src.fedoraproject.org/rpms/lld/c/aa6e69df60747496f8f22121ae8cc605c9d3498a?branch=rawhide\nif [ \"$(uname -m)\" != \"s390x\" ]; then\n    fedora_packages+=(lld)\nfi\n\nfedora_python3_packages=(\n    python3-pyyaml\n    python3-urwid\n    python3-pyparsing\n    python3-requests\n    python3-setuptools\n    python3-psutil\n    python3-distro\n    python3-click\n    python3-six\n    python3-pyudev\n)\n\n# an associative array from packages to constrains\ndeclare -A pip_packages=(\n    [scylla-driver]=\n    [geomet]=\"<0.3,>=0.1\"\n    [traceback-with-variables]=\n    [scylla-api-client]=\n    [treelib]=\n    [allure-pytest]=\n    [pytest-xdist]=\n    [pykmip]=\n)\n\npip_symlinks=(\n    scylla-api-client\n)\n\ncentos_packages=(\n    gdb\n    yaml-cpp-devel\n    scylla-antlr35-tool\n    scylla-antlr35-C++-devel\n    jsoncpp-devel snappy-devel\n    rapidjson-devel\n    scylla-boost163-static\n    scylla-python34-pyparsing20\n    systemd-devel\n    pigz\n    openldap-servers\n    openldap-devel\n    cpp-jwt-devel\n)\n\n# 1) glibc 2.30-3 has sys/sdt.h (systemtap include)\n#    some old containers may contain glibc older,\n#    so enforce update on that one.\n# 2) if problems with signatures, ensure having fresh\n#    archlinux-keyring: pacman -Sy archlinux-keyring && pacman -Syyu\n# 3) aur installations require having sudo and being\n#    a sudoer. makepkg does not work otherwise.\n#\n# aur: antlr3, antlr3-cpp-headers-git\narch_packages=(\n    gdb\n    base-devel\n    filesystem\n    git\n    glibc\n    jsoncpp\n    lua\n    python-pyparsing\n    python3\n    rapidjson\n    snappy\n)\n\ngo_arch() {\n    local -A GO_ARCH=(\n        [\"x86_64\"]=amd64\n        [\"aarch64\"]=arm64\n        [\"s390x\"]=s390x\n    )\n    echo ${GO_ARCH[\"$(arch)\"]}\n}\n\nNODE_EXPORTER_VERSION=1.8.2\ndeclare -A NODE_EXPORTER_CHECKSUM=(\n    [\"x86_64\"]=6809dd0b3ec45fd6e992c19071d6b5253aed3ead7bf0686885a51d85c6643c66\n    [\"aarch64\"]=627382b9723c642411c33f48861134ebe893e70a63bcc8b3fc0619cd0bfac4be\n    [\"s390x\"]=971481f06a985e9fcaee9bcd8da99a830d5b9e5f21e5225694de7e23401327c4\n)\nNODE_EXPORTER_DIR=/opt/scylladb/dependencies\n\nnode_exporter_filename() {\n    echo \"node_exporter-$NODE_EXPORTER_VERSION.linux-$(go_arch).tar.gz\"\n}\n\nnode_exporter_fullpath() {\n    echo \"$NODE_EXPORTER_DIR/$(node_exporter_filename)\"\n}\n\nnode_exporter_checksum() {\n    sha256sum \"$(node_exporter_fullpath)\" | while read -r sum _; do [[ \"$sum\" == \"${NODE_EXPORTER_CHECKSUM[\"$(arch)\"]}\" ]]; done\n}\n\nnode_exporter_url() {\n    echo \"https://github.com/prometheus/node_exporter/releases/download/v$NODE_EXPORTER_VERSION/$(node_exporter_filename)\"\n}\n\nMINIO_BINARIES_DIR=/usr/local/bin\n\nminio_server_url() {\n    echo \"https://dl.minio.io/server/minio/release/linux-$(go_arch)/minio\"\n}\n\nminio_client_url() {\n    echo \"https://dl.min.io/client/mc/release/linux-$(go_arch)/mc\"\n}\n\nminio_download_jobs() {\n    cfile=$(mktemp)\n    echo $(curl -s \"$(minio_server_url).sha256sum\" | cut -f1 -d' ') \"${MINIO_BINARIES_DIR}/minio\" > $cfile\n    echo $(curl -s \"$(minio_client_url).sha256sum\" | cut -f1 -d' ') \"${MINIO_BINARIES_DIR}/mc\" >> $cfile\n    sha256sum -c $cfile | grep -F FAILED | sed \\\n        -e 's/:.*$//g' \\\n        -e \"s#${MINIO_BINARIES_DIR}/minio#$(minio_server_url) -o ${MINIO_BINARIES_DIR}/minio#\" \\\n        -e \"s#${MINIO_BINARIES_DIR}/mc#$(minio_client_url) -o ${MINIO_BINARIES_DIR}/mc#\"\n    rm -f ${cfile}\n}\n\nprint_usage() {\n    echo \"Usage: install-dependencies.sh [OPTION]...\"\n    echo \"\"\n    echo \"  --print-python3-runtime-packages Print required python3 packages for Scylla\"\n    echo \"  --print-pip-runtime-packages Print required pip packages for Scylla\"\n    echo \"  --print-pip-symlinks Print list of pip provided commands which need to install to /usr/bin\"\n    echo \"  --print-node-exporter-filename Print node_exporter filename\"\n    exit 1\n}\n\nPRINT_PYTHON3=false\nPRINT_PIP=false\nPRINT_PIP_SYMLINK=false\nPRINT_NODE_EXPORTER=false\nwhile [ $# -gt 0 ]; do\n    case \"$1\" in\n        \"--print-python3-runtime-packages\")\n            PRINT_PYTHON3=true\n            shift 1\n            ;;\n        \"--print-pip-runtime-packages\")\n            PRINT_PIP=true\n            shift 1\n            ;;\n        \"--print-pip-symlinks\")\n            PRINT_PIP_SYMLINK=true\n            shift 1\n            ;;\n        \"--print-node-exporter-filename\")\n            PRINT_NODE_EXPORTER=true\n            shift 1\n            ;;\n         *)\n            print_usage\n            ;;\n    esac\ndone\n\nif $PRINT_PYTHON3; then\n    if [ \"$ID\" != \"fedora\" ]; then\n        echo \"Unsupported Distribution: $ID\"\n        exit 1\n    fi\n    echo \"${fedora_python3_packages[@]}\"\n    exit 0\nfi\n\nif $PRINT_PIP; then\n    echo \"${!pip_packages[@]}\"\n    exit 0\nfi\n\nif $PRINT_PIP_SYMLINK; then\n    echo \"${pip_symlinks[@]}\"\n    exit 0\nfi\n\nif $PRINT_NODE_EXPORTER; then\n    node_exporter_fullpath\n    exit 0\nfi\n\numask 0022\n\n./seastar/install-dependencies.sh\n./tools/java/install-dependencies.sh\n\nif [ \"$ID\" = \"ubuntu\" ] || [ \"$ID\" = \"debian\" ]; then\n    apt-get -y install \"${debian_base_packages[@]}\"\n    if [ \"$VERSION_ID\" = \"8\" ]; then\n        apt-get -y install libsystemd-dev scylla-antlr35 libyaml-cpp-dev\n    elif [ \"$VERSION_ID\" = \"14.04\" ]; then\n        apt-get -y install scylla-antlr35 libyaml-cpp-dev\n    elif [ \"$VERSION_ID\" = \"9\" ]; then\n        apt-get -y install libsystemd-dev antlr3 scylla-libyaml-cpp05-dev\n    else\n        apt-get -y install libsystemd-dev antlr3 libyaml-cpp-dev\n    fi\n    apt-get -y install libssl-dev\n\n    echo -e \"Configure example:\\n\\t./configure.py --enable-dpdk --mode=release --static-boost --static-yaml-cpp --compiler=/opt/scylladb/bin/g++-7 --cflags=\\\"-I/opt/scylladb/include -L/opt/scylladb/lib/x86-linux-gnu/\\\" --ldflags=\\\"-Wl,-rpath=/opt/scylladb/lib\\\"\"\nelif [ \"$ID\" = \"fedora\" ]; then\n    fedora_packages+=(openssl-devel)\n    if rpm -q --quiet yum-utils; then\n        echo\n        echo \"This script will install dnf-utils package, witch will conflict with currently installed package: yum-utils\"\n        echo \"Please remove the package and try to run this script again.\"\n        exit 1\n    fi\n    dnf install -y \"${fedora_packages[@]}\" \"${fedora_python3_packages[@]}\"\n    PIP_DEFAULT_ARGS=\"--only-binary=:all: -v\"\n    pip_constrained_packages=\"\"\n    for package in \"${!pip_packages[@]}\"\n    do\n        pip_constrained_packages=\"${pip_constrained_packages} ${package}${pip_packages[$package]}\"\n    done\n    pip3 install \"$PIP_DEFAULT_ARGS\" $pip_constrained_packages\n\n    if [ -f \"$(node_exporter_fullpath)\" ] && node_exporter_checksum; then\n        echo \"$(node_exporter_filename) already exists, skipping download\"\n    else\n        mkdir -p \"$NODE_EXPORTER_DIR\"\n        curl -fSL -o \"$(node_exporter_fullpath)\" \"$(node_exporter_url)\"\n        if ! node_exporter_checksum; then\n            echo \"$(node_exporter_filename) download failed\"\n            exit 1\n        fi\n    fi\nelif [ \"$ID\" = \"centos\" ]; then\n    centos_packages+=(openssl-devel)\n    dnf install -y \"${centos_packages[@]}\"\n    echo -e \"Configure example:\\n\\tpython3.4 ./configure.py --enable-dpdk --mode=release --static-boost --compiler=/opt/scylladb/bin/g++-7.3 --python python3.4 --ldflag=-Wl,-rpath=/opt/scylladb/lib64 --cflags=-I/opt/scylladb/include --with-antlr3=/opt/scylladb/bin/antlr3\"\nelif [ \"$ID\" == \"arch\" ]; then\n    # main\n    if [ \"$EUID\" -eq \"0\" ]; then\n        pacman -Sy --needed --noconfirm \"${arch_packages[@]}\"\n    else\n        echo \"scylla: You now ran $0 as non-root. Run it again as root to execute the pacman part of the installation.\" 1>&2\n    fi\n\n    # aur\n    if [ ! -x /usr/bin/antlr3 ]; then\n        echo \"Installing aur/antlr3...\"\n        if (( EUID == 0 )); then\n            echo \"You now ran $0 as root. This can only update dependencies with pacman. Please run again it as non-root to complete the AUR part of the installation.\" 1>&2\n            exit 1\n        fi\n        TEMP=$(mktemp -d)\n        pushd \"$TEMP\" > /dev/null || exit 1\n        git clone --depth 1 https://aur.archlinux.org/antlr3.git\n        cd antlr3 || exit 1\n        makepkg -si\n        popd > /dev/null || exit 1\n    fi\n    if [ ! -f /usr/include/antlr3.hpp ]; then\n        echo \"Installing aur/antlr3-cpp-headers-git...\"\n        if (( EUID == 0 )); then\n            echo \"You now ran $0 as root. This can only update dependencies with pacman. Please run again it as non-root to complete the AUR part of the installation.\" 1>&2\n            exit 1\n        fi\n        TEMP=$(mktemp -d)\n        pushd \"$TEMP\" > /dev/null || exit 1\n        git clone --depth 1 https://aur.archlinux.org/antlr3-cpp-headers-git.git\n        cd antlr3-cpp-headers-git || exit 1\n        makepkg -si\n        popd > /dev/null || exit 1\n    fi\n    echo -e \"Configure example:\\n\\t./configure.py\\n\\tninja release\"\nfi\n\ncargo --config net.git-fetch-with-cli=true install cxxbridge-cmd --root /usr/local\n\nCURL_ARGS=$(minio_download_jobs)\nif [ ! -z \"${CURL_ARGS}\" ]; then\n    curl -fSL --remove-on-error --parallel --parallel-immediate ${CURL_ARGS}\n    chmod +x \"${MINIO_BINARIES_DIR}/minio\"\n    chmod +x \"${MINIO_BINARIES_DIR}/mc\"\nelse\n    echo \"Minio server and client are up-to-date, skipping download\"\nfi\n"
        },
        {
          "name": "install.sh",
          "type": "blob",
          "size": 21.623046875,
          "content": "#!/bin/bash\n#\n# Copyright (C) 2018-present ScyllaDB\n#\n\n#\n# SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n#\n\nset -e\n\nif [ -z \"$BASH_VERSION\" ]; then\n    echo \"Unsupported shell, please run this script on bash.\"\n    exit 1\nfi\n\nprint_usage() {\n    cat <<EOF\nUsage: install.sh [options]\n\nOptions:\n  --root /path/to/root     alternative install root (default /)\n  --prefix /prefix         directory prefix (default /usr)\n  --python3 /opt/python3   path of the python3 interpreter relative to install root (default /opt/scylladb/python3/bin/python3)\n  --housekeeping           enable housekeeping service\n  --nonroot                install Scylla without required root privilege\n  --sysconfdir /etc/sysconfig   specify sysconfig directory name\n  --packaging               use install.sh for packaging\n  --upgrade                 upgrade existing scylla installation (don't overwrite config files)\n  --supervisor             enable supervisor to manage scylla processes\n  --supervisor-log-to-stdout logging to stdout on supervisor\n  --without-systemd         skip installing systemd units\n  --help                   this helpful message\nEOF\n    exit 1\n}\n\n# Some words about pathnames in this script.\n#\n# A pathname has three components: \"$root/$prefix/$rest\".\n#\n# $root is used to point at the entire installed hierarchy, so you can perform\n# an install to a temporary directory without modifying your system, with the intent\n# that the files are copied later to the real /. So, if \"$root\"=\"/tmp/xyz\", you'd get\n# a standard hierarchy under /tmp/xyz: /tmp/xyz/etc, /tmp/xyz/var, and \n# /tmp/xyz/opt/scylladb/bin/scylla. This is used by rpmbuild --root to create a filesystem\n# image to package.\n#\n# When this script creates a file, it must use \"$root\" to refer to the file. When this\n# script inserts a file name into a file, it must not use \"$root\", because in the installed\n# system \"$root\" is stripped out. Example:\n#\n#    echo \"This file's name is /a/b/c. > \"$root/a/b/c\"\n#\n# The second component is \"$prefix\". It is used by non-root install to place files into\n# a directory of the user's choice (typically somewhere under their home directory). In theory\n# all files should be always under \"$prefix\", but in practice /etc files are not under \"$prefix\"\n# for standard installs (we use /etc not /usr/etc) and are under \"$prefix\" for non-root installs.\n# Another exception is files that go under /opt/scylladb in a standard install go under \"$prefix\"\n# for a non-root install.\n#\n# The last component is the rest of the file name, which doesn't matter for this script and\n# isn't changed by it.\n\nroot=/\nhousekeeping=false\nnonroot=false\npackaging=false\nupgrade=false\nsupervisor=false\nsupervisor_log_to_stdout=false\nwithout_systemd=false\nskip_systemd_check=false\n\nwhile [ $# -gt 0 ]; do\n    case \"$1\" in\n        \"--root\")\n            root=\"$(realpath \"$2\")\"\n            shift 2\n            ;;\n        \"--prefix\")\n            prefix=\"$2\"\n            shift 2\n            ;;\n        \"--housekeeping\")\n            housekeeping=true\n            shift 1\n            ;;\n        \"--python3\")\n            python3=\"$2\"\n            shift 2\n            ;;\n        \"--nonroot\")\n            nonroot=true\n            shift 1\n            ;;\n        \"--sysconfdir\")\n            sysconfdir=\"$2\"\n            shift 2\n            ;;\n        \"--packaging\")\n            packaging=true\n            skip_systemd_check=true\n            shift 1\n            ;;\n        \"--upgrade\")\n            upgrade=true\n            shift 1\n            ;;\n        \"--supervisor\")\n            supervisor=true\n            skip_systemd_check=true\n            shift 1\n            ;;\n        \"--supervisor-log-to-stdout\")\n            supervisor_log_to_stdout=true\n            shift 1\n            ;;\n        \"--without-systemd\")\n            without_systemd=true\n            skip_systemd_check=true\n            shift 1\n            ;;\n        \"--help\")\n            shift 1\n\t    print_usage\n            ;;\n        *)\n            print_usage\n            ;;\n    esac\ndone\n\npatchelf() {\n    # patchelf comes from the build system, so it needs the build system's ld.so and\n    # shared libraries. We can't use patchelf on patchelf itself, so invoke it via\n    # ld.so.\n    LD_LIBRARY_PATH=\"$PWD/libreloc\" libreloc/ld.so libexec/patchelf \"$@\"\n}\n\nremove_rpath() {\n    local file=\"$1\"\n    local rpath\n    # $file might not be an elf image\n    if rpath=$(patchelf --print-rpath \"$file\" 2>/dev/null); then\n      if [ -n \"$rpath\" ]; then\n        echo \"remove rpath from $file\"\n        patchelf --remove-rpath \"$file\"\n      fi\n    fi\n}\n\nadjust_bin() {\n    local bin=\"$1\"\n    # We could add --set-rpath too, but then debugedit (called by rpmbuild) barfs\n    # on the result. So use LD_LIBRARY_PATH in the thunk, below.\n    patchelf \\\n\t--set-interpreter \"$prefix/libreloc/ld.so\" \\\n\t\"$root/$prefix/libexec/$bin\"\n    cat > \"$root/$prefix/bin/$bin\" <<EOF\n#!/bin/bash -e\n[[ -z \"\\$LD_PRELOAD\" ]] || { echo \"\\$0: not compatible with LD_PRELOAD\" >&2; exit 110; }\nexport GNUTLS_SYSTEM_PRIORITY_FILE=\"\\${GNUTLS_SYSTEM_PRIORITY_FILE-$prefix/libreloc/gnutls.config}\"\nexport LD_LIBRARY_PATH=\"$prefix/libreloc\"\nexport UBSAN_OPTIONS=\"${UBSAN_OPTIONS:+$UBSAN_OPTIONS:}suppressions=$prefix/libexec/ubsan-suppressions.supp\"\nexec -a \"\\$0\" \"$prefix/libexec/$bin\" \"\\$@\"\nEOF\n    chmod 755 \"$root/$prefix/bin/$bin\"\n}\n\nrelocate_python3() {\n    local script=\"$2\"\n    local scriptname=\"$(basename \"$script\")\"\n    local installdir=\"$1\"\n    local install=\"$installdir/$scriptname\"\n    local relocateddir=\"$installdir/libexec\"\n    local pythoncmd=$(realpath -ms --relative-to \"$installdir\" \"$rpython3\")\n    local pythonpath=\"$(dirname \"$pythoncmd\")\"\n\n    if [ ! -x \"$script\" ]; then\n        install -m755 \"$script\" \"$install\"\n        return\n    fi\n    install -d -m755 \"$relocateddir\"\n    install -m755 \"$script\" \"$relocateddir\"\n    cat > \"$install\"<<EOF\n#!/usr/bin/env bash\n[[ -z \"\\$LD_PRELOAD\" ]] || { echo \"\\$0: not compatible with LD_PRELOAD\" >&2; exit 110; }\nx=\"\\$(readlink -f \"\\$0\")\"\nb=\"\\$(basename \"\\$x\")\"\nd=\"\\$(dirname \"\\$x\")\"\nCENTOS_SSL_CERT_FILE=\"/etc/pki/tls/cert.pem\"\nif [ -f \"\\${CENTOS_SSL_CERT_FILE}\" ]; then\n  c=\\${CENTOS_SSL_CERT_FILE}\nfi\nDEBIAN_SSL_CERT_FILE=\"/etc/ssl/certs/ca-certificates.crt\"\nif [ -f \"\\${DEBIAN_SSL_CERT_FILE}\" ]; then\n  c=\\${DEBIAN_SSL_CERT_FILE}\nfi\nPYTHONPATH=\"\\${d}:\\${d}/libexec:\\$PYTHONPATH\" PATH=\"\\${d}/../bin:\\${d}/$pythonpath:\\${PATH}\" SSL_CERT_FILE=\"\\${c}\" exec -a \"\\$0\" \"\\${d}/libexec/\\${b}\" \"\\$@\"\nEOF\n    chmod 755 \"$install\"\n}\n\ninstall() {\n    command install -Z \"$@\"\n}\n\ninstallconfig() {\n    local perm=\"$1\"\n    local src=\"$2\"\n    local dest=\"$3\"\n    local bname=$(basename \"$src\")\n\n    # do not overwrite config file when upgrade mode\n    if $upgrade && [ -e \"$dest/$bname\" ]; then\n        local oldsum=$(md5sum \"$dest/$bname\" | cut -f 1 -d \" \")\n        local newsum=$(md5sum \"$src\" | cut -f 1 -d \" \")\n        # if old one and new one are same, we can skip installing\n        if [ \"$oldsum\" != \"$newsum\" ]; then\n            install \"-m$perm\" \"$src\" -T \"$dest/$bname.new\"\n        fi\n    else\n        install \"-m$perm\" \"$src\" -Dt \"$dest\"\n    fi\n}\n\ncheck_usermode_support() {\n    user=$(systemctl --help|grep -e '--user')\n    [ -n \"$user\" ]\n}\n\n. /etc/os-release\n\nis_debian_variant() {\n    [ \"$ID_LIKE\" = \"debian\" -o \"$ID\" = \"debian\" ]\n}\n\nis_alpine() {\n    [ \"$ID\" = \"alpine\" ]\n}\n\nsupervisor_dir() {\n    local etcdir=\"$1\"\n    if is_debian_variant; then\n        echo \"$etcdir\"/supervisor/conf.d\n    elif is_alpine; then\n        echo \"$etcdir\"/supervisor.d\n    else\n        echo \"$etcdir\"/supervisord.d\n    fi\n}\n\nsupervisor_conf() {\n    local etcdir=\"$1\"\n    local service=\"$2\"\n    if is_debian_variant; then\n        echo `supervisor_dir \"$etcdir\"`/\"$service\".conf\n    else\n        echo `supervisor_dir \"$etcdir\"`/\"$service\".ini\n    fi\n}\n\nif ! $skip_systemd_check && [ ! -d /run/systemd/system/ ]; then\n    echo \"systemd is not detected, unsupported distribution.\"\n    exit 1\nfi\n\n# change directory to the package's root directory\ncd \"$(dirname \"$0\")\"\n\nproduct=\"$(cat ./SCYLLA-PRODUCT-FILE)\"\n\nif [ -z \"$prefix\" ]; then\n    if $nonroot; then\n        prefix=~/scylladb\n    else\n        prefix=/opt/scylladb\n    fi\nfi\n\nrprefix=$(realpath -m \"$root/$prefix\")\n\nif [ -f \"/etc/os-release\" ]; then\n    . /etc/os-release\nfi\n\nif [ -z \"$sysconfdir\" ]; then\n    sysconfdir=/etc/sysconfig\n    if ! $nonroot; then\n        if [ \"$ID\" = \"ubuntu\" ] || [ \"$ID\" = \"debian\" ]; then\n            sysconfdir=/etc/default\n        fi\n    fi\nfi\n\nif [ -z \"$python3\" ]; then\n    python3=$prefix/python3/bin/python3\nfi\nrpython3=$(realpath -m \"$root/$python3\")\nif ! $nonroot; then\n    retc=$(realpath -m \"$root/etc\")\n    rsysconfdir=$(realpath -m \"$root/$sysconfdir\")\n    rusr=$(realpath -m \"$root/usr\")\n    rsystemd=$(realpath -m \"$rusr/lib/systemd/system\")\n    rdoc=\"$rprefix/share/doc\"\n    rdata=$(realpath -m \"$root/var/lib/scylla\")\n    rhkdata=$(realpath -m \"$root/var/lib/scylla-housekeeping\")\nelse\n    retc=\"$rprefix/etc\"\n    rsysconfdir=\"$rprefix/$sysconfdir\"\n    rsystemd=\"$HOME/.config/systemd/user\"\n    rdoc=\"$rprefix/share/doc\"\n    rdata=\"$rprefix\"\nfi\n\n# scylla-conf\ninstall -d -m755 \"$retc\"/scylla\ninstall -d -m755 \"$retc\"/scylla.d\n\nscylla_yaml_dir=$(mktemp -d)\nscylla_yaml=$scylla_yaml_dir/scylla.yaml\ngrep -v api_ui_dir conf/scylla.yaml | grep -v api_doc_dir > $scylla_yaml\necho \"api_ui_dir: /opt/scylladb/swagger-ui/dist/\" >> $scylla_yaml\necho \"api_doc_dir: /opt/scylladb/api/api-doc/\" >> $scylla_yaml\ninstallconfig 644 $scylla_yaml \"$retc\"/scylla\nrm -rf $scylla_yaml_dir\n\ninstallconfig 644 conf/cassandra-rackdc.properties \"$retc\"/scylla\nif $housekeeping; then\n    installconfig 644 conf/housekeeping.cfg \"$retc\"/scylla.d\nfi\n# scylla-kernel-conf\nif ! $nonroot; then\n    install -m755 -d \"$rusr/lib/sysctl.d\"\n    for file in dist/common/sysctl.d/*.conf; do\n        installconfig 644 \"$file\" \"$rusr\"/lib/sysctl.d\n    done\nfi\ninstall -d -m755 -d \"$rprefix\"/kernel_conf\ninstall -m755 dist/common/kernel_conf/scylla_tune_sched -Dt \"$rprefix\"/kernel_conf\ninstall -m755 dist/common/kernel_conf/post_install.sh \"$rprefix\"/kernel_conf\nif ! $without_systemd; then\n    install -m644 dist/common/systemd/scylla-tune-sched.service -Dt \"$rsystemd\"\n    if ! $nonroot && [ \"$prefix\" != \"/opt/scylladb\" ]; then\n        install -d -m755 \"$retc\"/systemd/system/scylla-tune-sched.service.d\n        cat << EOS > \"$retc\"/systemd/system/scylla-tune-sched.service.d/execpath.conf\n[Service]\nExecStart=\nExecStart=$prefix/kernel_conf/scylla_tune_sched\nEOS\n        chmod 644 \"$retc\"/systemd/system/scylla-tune-sched.service.d/execpath.conf\n    fi\nfi\nrelocate_python3 \"$rprefix\"/kernel_conf dist/common/kernel_conf/scylla_tune_sched\n# scylla-node-exporter\nif ! $without_systemd; then\n    install -d -m755 \"$rsystemd\"\nfi\ninstall -d -m755 \"$rsysconfdir\"\ninstall -d -m755 \"$rprefix\"/node_exporter\ninstall -d -m755 \"$rprefix\"/node_exporter/licenses\ninstall -m755 node_exporter/node_exporter \"$rprefix\"/node_exporter\ninstall -m644 node_exporter/LICENSE \"$rprefix\"/node_exporter/licenses\ninstall -m644 node_exporter/NOTICE \"$rprefix\"/node_exporter/licenses\nif ! $without_systemd; then\n    install -m644 dist/common/systemd/scylla-node-exporter.service -Dt \"$rsystemd\"\nfi\ninstallconfig 644 dist/common/sysconfig/scylla-node-exporter \"$rsysconfdir\"\nif ! $nonroot && ! $without_systemd; then\n    install -d -m755 \"$retc\"/systemd/system/scylla-node-exporter.service.d\n    install -m644 dist/common/systemd/scylla-node-exporter.service.d/dependencies.conf -Dt \"$retc\"/systemd/system/scylla-node-exporter.service.d\n    if [ \"$sysconfdir\" != \"/etc/sysconfig\" ]; then\n        cat << EOS > \"$retc\"/systemd/system/scylla-node-exporter.service.d/sysconfdir.conf\n[Service]\nEnvironmentFile=\nEnvironmentFile=$sysconfdir/scylla-node-exporter\nEOS\n        chmod 644 \"$retc\"/systemd/system/scylla-node-exporter.service.d/sysconfdir.conf\n    fi\nelif ! $without_systemd; then\n    install -d -m755 \"$rsystemd\"/scylla-node-exporter.service.d\n    cat << EOS > \"$rsystemd\"/scylla-node-exporter.service.d/nonroot.conf\n[Service]\nEnvironmentFile=\nEnvironmentFile=$(realpath -m \"$rsysconfdir/scylla-node-exporter\")\nExecStart=\nExecStart=$rprefix/node_exporter/node_exporter $SCYLLA_NODE_EXPORTER_ARGS\nUser=\nGroup=\nEOS\n    chmod 644 \"$rsystemd\"/scylla-node-exporter.service.d/nonroot.conf\nfi\n\n# scylla-server\ninstall -m755 -d \"$rprefix\"\ninstall -m755 -d \"$retc/scylla.d\"\ninstallconfig 644 dist/common/sysconfig/scylla-housekeeping \"$rsysconfdir\"\ninstallconfig 644 dist/common/sysconfig/scylla-server \"$rsysconfdir\"\nfor file in dist/common/scylla.d/*.conf; do\n    installconfig 644 \"$file\" \"$retc\"/scylla.d\ndone\n\ninstall -d -m755 \"$retc\"/scylla \"$rprefix/bin\" \"$rprefix/libexec\" \"$rprefix/libreloc\" \"$rprefix/scripts\" \"$rprefix/bin\"\nif ! $without_systemd; then\n    install -m644 dist/common/systemd/scylla-fstrim.service -Dt \"$rsystemd\"\n    install -m644 dist/common/systemd/scylla-housekeeping-daily.service -Dt \"$rsystemd\"\n    install -m644 dist/common/systemd/scylla-housekeeping-restart.service -Dt \"$rsystemd\"\n    install -m644 dist/common/systemd/scylla-server.service -Dt \"$rsystemd\"\n    install -m644 dist/common/systemd/*.slice -Dt \"$rsystemd\"\n    install -m644 dist/common/systemd/*.timer -Dt \"$rsystemd\"\nfi\ninstall -m755 seastar/scripts/seastar-cpu-map.sh -Dt \"$rprefix\"/scripts\ninstall -m755 seastar/dpdk/usertools/dpdk-devbind.py -Dt \"$rprefix\"/scripts\ninstall -m755 libreloc/* -Dt \"$rprefix/libreloc\"\nfor lib in libreloc/*; do\n    remove_rpath \"$rprefix/$lib\"\ndone\n# some files in libexec are symlinks, which \"install\" dereferences\n# use cp -P for the symlinks instead.\ninstall -m755 libexec/* -Dt \"$rprefix/libexec\"\nfor bin in libexec/*; do\n    remove_rpath \"$rprefix/$bin\"\n    adjust_bin \"${bin#libexec/}\"\ndone\ninstall -m644 ubsan-suppressions.supp -Dt \"$rprefix/libexec\"\n\ninstall -d -m755 \"$rdoc\"/scylla\ninstall -m644 README.md -Dt \"$rdoc\"/scylla/\ninstall -m644 NOTICE.txt -Dt \"$rdoc\"/scylla/\ninstall -m644 ORIGIN -Dt \"$rdoc\"/scylla/\ninstall -d -m755 -d \"$rdoc\"/scylla/licenses/\ninstall -m644 licenses/* -Dt \"$rdoc\"/scylla/licenses/\ninstall -m755 -d \"$rdata\"\ninstall -m755 -d \"$rdata\"/data\ninstall -m755 -d \"$rdata\"/commitlog\ninstall -m755 -d \"$rdata\"/hints\ninstall -m755 -d \"$rdata\"/view_hints\ninstall -m755 -d \"$rdata\"/coredump\ninstall -m755 -d \"$rprefix\"/swagger-ui\ncp -pr swagger-ui/dist \"$rprefix\"/swagger-ui\ninstall -d -m755 -d \"$rprefix\"/api\ncp -pr api/api-doc \"$rprefix\"/api\ninstall -d -m755 -d \"$rprefix\"/scyllatop\ncp -pr tools/scyllatop/* \"$rprefix\"/scyllatop\ninstall -d -m755 -d \"$rprefix\"/scripts\ncp -pr dist/common/scripts/* \"$rprefix\"/scripts\nln -srf \"$rprefix/scyllatop/scyllatop.py\" \"$rprefix/bin/scyllatop\"\nif $supervisor; then\n    install -d -m755 \"$rprefix\"/supervisor\n    install -m755 dist/common/supervisor/* -Dt \"$rprefix\"/supervisor\nfi\n\n# scylla tools\ninstall -d -m755 \"$retc\"/bash_completion.d\ninstall -m644 dist/common/nodetool-completion \"$retc\"/bash_completion.d\ninstall -m755 bin/nodetool \"$rprefix/bin\"\n\nSBINFILES=$(cd dist/common/scripts/; ls scylla_*setup node_health_check scylla_kernel_check)\nSBINFILES+=\" $(cd seastar/scripts; ls seastar-cpu-map.sh)\"\n\ncat << EOS > \"$rprefix\"/scripts/scylla_product.py\nPRODUCT=\"$product\"\nEOS\nchmod 644 \"$rprefix\"/scripts/scylla_product.py\n\nif ! $nonroot && ! $without_systemd; then\n    install -d -m755 \"$retc\"/systemd/system/scylla-server.service.d\n    install -m644 dist/common/systemd/scylla-server.service.d/dependencies.conf -Dt \"$retc\"/systemd/system/scylla-server.service.d\n    if [ \"$sysconfdir\" != \"/etc/sysconfig\" ]; then\n        cat << EOS > \"$retc\"/systemd/system/scylla-server.service.d/sysconfdir.conf\n[Service]\nEnvironmentFile=\nEnvironmentFile=$sysconfdir/scylla-server\nEnvironmentFile=/etc/scylla.d/*.conf\nEOS\n        chmod 644 \"$retc\"/systemd/system/scylla-server.service.d/sysconfdir.conf\n        for i in daily restart; do\n            install -d -m755 \"$retc\"/systemd/system/scylla-housekeeping-$i.service.d\n            cat << EOS > \"$retc\"/systemd/system/scylla-housekeeping-$i.service.d/sysconfdir.conf\n[Service]\nEnvironmentFile=\nEnvironmentFile=$sysconfdir/scylla-housekeeping\nEOS\n        done\n    fi\nelif ! $without_systemd; then\n    install -d -m755 \"$rsystemd\"/scylla-server.service.d\n    if [ -d /var/log/journal ]; then\n        cat << EOS > \"$rsystemd\"/scylla-server.service.d/nonroot.conf\n[Service]\nEnvironmentFile=\nEnvironmentFile=$(realpath -m \"$rsysconfdir/scylla-server\")\nEnvironmentFile=$retc/scylla.d/*.conf\nExecStartPre=\nExecStart=\nExecStart=$rprefix/bin/scylla \\$SCYLLA_ARGS \\$SEASTAR_IO \\$DEV_MODE \\$CPUSET\nExecStopPost=\nUser=\nAmbientCapabilities=\nEOS\n        chmod 644 \"$rsystemd\"/scylla-server.service.d/nonroot.conf\n    else\n        cat << EOS > \"$rsystemd\"/scylla-server.service.d/nonroot.conf\n[Service]\nEnvironmentFile=\nEnvironmentFile=$(realpath -m \"$rsysconfdir/scylla-server\")\nEnvironmentFile=$retc/scylla.d/*.conf\nExecStartPre=\nExecStartPre=$rprefix/scripts/scylla_logrotate\nExecStart=\nExecStart=$rprefix/bin/scylla \\$SCYLLA_ARGS \\$SEASTAR_IO \\$DEV_MODE \\$CPUSET\nExecStopPost=\nUser=\nAmbientCapabilities=\nStandardOutput=\nStandardOutput=file:$rprefix/scylla-server.log\nStandardError=\nStandardError=inherit\nEOS\n        chmod 644 \"$rsystemd\"/scylla-server.service.d/nonroot.conf\n    fi\nfi\n\n\nif ! $nonroot; then\n    if [ \"$sysconfdir\" != \"/etc/sysconfig\" ]; then\n        cat << EOS > \"$rprefix\"/scripts/scylla_sysconfdir.py\nSYSCONFDIR=\"$sysconfdir\"\nEOS\n        chmod 644 \"$rprefix\"/scripts/scylla_sysconfdir.py\n    fi\n    install -m755 -d \"$rusr/bin\"\n    install -m755 -d \"$rhkdata\"\n    ln -srf \"$rprefix/bin/scylla\" \"$rusr/bin/scylla\"\n    ln -srf \"$rprefix/bin/iotune\" \"$rusr/bin/iotune\"\n    ln -srf \"$rprefix/bin/scyllatop\" \"$rusr/bin/scyllatop\"\n    ln -srf \"$rprefix/bin/nodetool\" \"$rusr/bin/nodetool\"\n    install -d -m755 \"$rusr\"/sbin\n    for i in $SBINFILES; do\n        ln -srf \"$rprefix/scripts/$i\" \"$rusr/sbin/$i\"\n    done\n\n    # we need keep /usr/lib/scylla directory to support upgrade/downgrade\n    # without error, so we need to create symlink for each script on the\n    # directory\n    install -m755 -d \"$rusr\"/lib/scylla/scyllatop/views\n    for i in $(find \"$rprefix\"/scripts/ -maxdepth 1 -type f); do\n        ln -srf $i \"$rusr\"/lib/scylla/\n    done\n    for i in $(find \"$rprefix\"/scyllatop/ -maxdepth 1 -type f); do\n        ln -srf $i \"$rusr\"/lib/scylla/scyllatop\n    done\n    for i in $(find \"$rprefix\"/scyllatop/views -maxdepth 1 -type f); do\n        ln -srf $i \"$rusr\"/lib/scylla/scyllatop/views\n    done\nelse\n    install -m755 -d \"$rdata\"/saved_caches\n    cat << EOS > \"$rprefix\"/scripts/scylla_sysconfdir.py\nSYSCONFDIR=\"$sysconfdir\"\nEOS\n    chmod 644 \"$rprefix\"/scripts/scylla_sysconfdir.py\n    install -d -m755 \"$rprefix\"/sbin\n    for i in $SBINFILES; do\n        ln -srf \"$rprefix/scripts/$i\" \"$rprefix/sbin/$i\"\n    done\nfi\n\n\n\ninstall -m755 scylla-gdb.py -Dt \"$rprefix\"/scripts/\n\nPYSCRIPTS=$(find dist/common/scripts/ -maxdepth 1 -type f -exec grep -Pls '\\A#!/usr/bin/env python3' {} +)\nfor i in $PYSCRIPTS; do\n    relocate_python3 \"$rprefix\"/scripts \"$i\"\ndone\nfor i in seastar/scripts/{perftune.py,addr2line.py,seastar-addr2line}; do\n    relocate_python3 \"$rprefix\"/scripts \"$i\"\ndone\nrelocate_python3 \"$rprefix\"/scyllatop tools/scyllatop/scyllatop.py\nrelocate_python3 \"$rprefix\"/scripts fix_system_distributed_tables.py\n\nif $supervisor; then\n    install -d -m755 `supervisor_dir $retc`\n    for service in scylla-server scylla-jmx scylla-node-exporter; do\n        if [ \"$service\" = \"scylla-server\" ]; then\n            program=\"scylla\"\n        else\n            program=$service\n        fi\n        cat << EOS > `supervisor_conf $retc $service`\n[program:$program]\ndirectory=$rprefix\ncommand=/bin/bash -c './supervisor/$service.sh'\nEOS\n        chmod 644 `supervisor_conf $retc $service`\n        if [ \"$service\" != \"scylla-server\" ]; then\n            cat << EOS >> `supervisor_conf $retc $service`\nuser=scylla\nEOS\n            chmod 644 `supervisor_conf $retc $service`\n        fi\n        if $supervisor_log_to_stdout; then\n            cat << EOS >> `supervisor_conf $retc $service`\nstdout_logfile=/dev/stdout\nstdout_logfile_maxbytes=0\nstderr_logfile=/dev/stderr\nstderr_logfile_maxbytes=0\nEOS\n            chmod 644 `supervisor_conf $retc $service`\n        fi\n    done\nfi\n\nif $nonroot; then\n    sed -i -e \"s#/var/lib/scylla#$rprefix#g\" $rsysconfdir/scylla-server\n    sed -i -e \"s#/etc/scylla#$retc/scylla#g\" $rsysconfdir/scylla-server\n    sed -i -e \"s#^SCYLLA_ARGS=\\\"#SCYLLA_ARGS=\\\"--workdir $rprefix #g\" $rsysconfdir/scylla-server\n    if [ ! -d /var/log/journal ] || $supervisor_log_to_stdout; then\n        sed -i -e \"s#--log-to-stdout 0#--log-to-stdout 1#g\" $rsysconfdir/scylla-server\n    fi\n    # nonroot install is also 'offline install'\n    touch $rprefix/SCYLLA-OFFLINE-FILE\n    chmod 644 $rprefix/SCYLLA-OFFLINE-FILE\n    touch $rprefix/SCYLLA-NONROOT-FILE\n    chmod 644 $rprefix/SCYLLA-NONROOT-FILE\n    if ! $skip_systemd_check && check_usermode_support; then\n        systemctl --user daemon-reload\n    fi\n    echo \"Scylla non-root install completed.\"\nelif ! $packaging; then\n    if $supervisor_log_to_stdout; then\n        sed -i -e \"s#--log-to-stdout 0#--log-to-stdout 1#g\" $rsysconfdir/scylla-server\n    fi\n    # run install.sh without --packaging is 'offline install'\n    touch $rprefix/SCYLLA-OFFLINE-FILE\n    chmod 644 $rprefix/SCYLLA-OFFLINE-FILE\n    nousr=\n    nogrp=\n    getent passwd scylla || nousr=1\n    getent group scylla || nogrp=1\n\n    # this handles both case group is not exist || group already exists\n    if [ $nousr ]; then\n        useradd -r -d /var/lib/scylla -M scylla\n    # only group is not exist, create it and add user to the group\n    elif [ $nogrp ]; then\n        groupadd -r scylla\n        usermod -g scylla scylla\n    fi\n    chown -R scylla:scylla $rdata\n    chown -R scylla:scylla $rhkdata\n\n    for file in dist/common/sysctl.d/*.conf; do\n        bn=$(basename \"$file\")\n        # ignore error since some kernel may not have specified parameter\n        sysctl -p \"$rusr\"/lib/sysctl.d/\"$bn\" || :\n    done\n    if ! $supervisor; then\n        $rprefix/scripts/scylla_post_install.sh\n        $rprefix/kernel_conf/post_install.sh\n    fi\n    echo \"Scylla offline install completed.\"\nfi\n"
        },
        {
          "name": "interval.hh",
          "type": "blob",
          "size": 32.6318359375,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"utils/assert.hh\"\n#include <algorithm>\n#include <list>\n#include <vector>\n#include <optional>\n#include <iosfwd>\n#include <compare>\n#include <ranges>\n#include <fmt/format.h>\n\ntemplate <typename Comparator, typename T>\nconcept IntervalComparatorFor = requires (T a, T b, Comparator& cmp) {\n    { cmp(a, b) } -> std::same_as<std::strong_ordering>;\n};\n\ntemplate <typename LessComparator, typename T>\nconcept IntervalLessComparatorFor = requires (T a, T b, LessComparator& cmp) {\n    { cmp(a, b) } -> std::same_as<bool>;\n};\n\ninline\nbool\nrequire_ordering_and_on_equal_return(\n        std::strong_ordering input,\n        std::strong_ordering match_to_return_true,\n        bool return_if_equal) {\n    if (input == match_to_return_true) {\n        return true;\n    } else if (input == std::strong_ordering::equal) {\n        return return_if_equal;\n    } else {\n        return false;\n    }\n}\n\ntemplate<typename T>\nclass interval_bound {\n    T _value;\n    bool _inclusive;\npublic:\n    interval_bound(T value, bool inclusive = true)\n              : _value(std::move(value))\n              , _inclusive(inclusive)\n    { }\n    const T& value() const & { return _value; }\n    T&& value() && { return std::move(_value); }\n    bool is_inclusive() const { return _inclusive; }\n    bool operator==(const interval_bound& other) const {\n        return (_value == other._value) && (_inclusive == other._inclusive);\n    }\n    bool equal(const interval_bound& other, IntervalComparatorFor<T> auto&& cmp) const {\n        return _inclusive == other._inclusive && cmp(_value, other._value) == 0;\n    }\n};\n\ntemplate<typename T>\nclass interval;\n\n// An interval which can have inclusive, exclusive or open-ended bounds on each end.\n// The end bound can be smaller than the start bound.\ntemplate<typename T>\nclass wrapping_interval {\n    template <typename U>\n    using optional = std::optional<U>;\npublic:\n    using bound = interval_bound<T>;\n\n    template <typename Transformer>\n    using transformed_type = typename std::remove_cv_t<std::remove_reference_t<std::invoke_result_t<Transformer, T>>>;\nprivate:\n    optional<bound> _start;\n    optional<bound> _end;\n    bool _singular;\npublic:\n    wrapping_interval(optional<bound> start, optional<bound> end, bool singular = false)\n        : _start(std::move(start))\n        , _singular(singular) {\n        if (!_singular) {\n            _end = std::move(end);\n        }\n    }\n    wrapping_interval(T value)\n        : _start(bound(std::move(value), true))\n        , _end()\n        , _singular(true)\n    { }\n    constexpr wrapping_interval() : wrapping_interval({}, {}) { }\nprivate:\n    // Bound wrappers for compile-time dispatch and safety.\n    struct start_bound_ref { const optional<bound>& b; };\n    struct end_bound_ref { const optional<bound>& b; };\n\n    start_bound_ref start_bound() const { return { start() }; }\n    end_bound_ref end_bound() const { return { end() }; }\n\n    static bool greater_than_or_equal(end_bound_ref end, start_bound_ref start, IntervalComparatorFor<T> auto&& cmp) {\n        return !end.b || !start.b || require_ordering_and_on_equal_return(\n                cmp(end.b->value(), start.b->value()),\n                std::strong_ordering::greater,\n                end.b->is_inclusive() && start.b->is_inclusive());\n    }\n\n    static bool less_than(end_bound_ref end, start_bound_ref start, IntervalComparatorFor<T> auto&& cmp) {\n        return !greater_than_or_equal(end, start, cmp);\n    }\n\n    static bool less_than_or_equal(start_bound_ref first, start_bound_ref second, IntervalComparatorFor<T> auto&& cmp) {\n        return !first.b || (second.b && require_ordering_and_on_equal_return(\n                cmp(first.b->value(), second.b->value()),\n                std::strong_ordering::less,\n                first.b->is_inclusive() || !second.b->is_inclusive()));\n    }\n\n    static bool less_than(start_bound_ref first, start_bound_ref second, IntervalComparatorFor<T> auto&& cmp) {\n        return second.b && (!first.b || require_ordering_and_on_equal_return(\n                cmp(first.b->value(), second.b->value()),\n                std::strong_ordering::less,\n                first.b->is_inclusive() && !second.b->is_inclusive()));\n    }\n\n    static bool greater_than_or_equal(end_bound_ref first, end_bound_ref second, IntervalComparatorFor<T> auto&& cmp) {\n        return !first.b || (second.b && require_ordering_and_on_equal_return(\n                cmp(first.b->value(), second.b->value()),\n                std::strong_ordering::greater,\n                first.b->is_inclusive() || !second.b->is_inclusive()));\n    }\npublic:\n    // the point is before the interval (works only for non wrapped intervals)\n    // Comparator must define a total ordering on T.\n    bool before(const T& point, IntervalComparatorFor<T> auto&& cmp) const {\n        SCYLLA_ASSERT(!is_wrap_around(cmp));\n        if (!start()) {\n            return false; //open start, no points before\n        }\n        auto r = cmp(point, start()->value());\n        if (r < 0) {\n            return true;\n        }\n        if (!start()->is_inclusive() && r == 0) {\n            return true;\n        }\n        return false;\n    }\n    // the other interval is before this interval (works only for non wrapped intervals)\n    // Comparator must define a total ordering on T.\n    bool other_is_before(const wrapping_interval<T>& o, IntervalComparatorFor<T> auto&& cmp) const {\n        SCYLLA_ASSERT(!is_wrap_around(cmp));\n        SCYLLA_ASSERT(!o.is_wrap_around(cmp));\n        if (!start() || !o.end()) {\n            return false;\n        }\n        auto r = cmp(o.end()->value(), start()->value());\n        if (r < 0) {\n            return true;\n        }\n        if (r > 0) {\n            return false;\n        }\n        // o.end()->value() == start()->value(), we decide based on inclusiveness\n        const auto ei = o.end()->is_inclusive();\n        const auto si = start()->is_inclusive();\n        if (!ei && !si) {\n            return true;\n        }\n        // At least one is inclusive, check that the other isn't\n        if (ei != si) {\n            return true;\n        }\n        return false;\n    }\n    // the point is after the interval (works only for non wrapped intervals)\n    // Comparator must define a total ordering on T.\n    bool after(const T& point, IntervalComparatorFor<T> auto&& cmp) const {\n        SCYLLA_ASSERT(!is_wrap_around(cmp));\n        if (!end()) {\n            return false; //open end, no points after\n        }\n        auto r = cmp(end()->value(), point);\n        if (r < 0) {\n            return true;\n        }\n        if (!end()->is_inclusive() && r == 0) {\n            return true;\n        }\n        return false;\n    }\n    // check if two intervals overlap.\n    // Comparator must define a total ordering on T.\n    bool overlaps(const wrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {\n        bool this_wraps = is_wrap_around(cmp);\n        bool other_wraps = other.is_wrap_around(cmp);\n\n        if (this_wraps && other_wraps) {\n            return true;\n        } else if (this_wraps) {\n            auto unwrapped = unwrap();\n            return other.overlaps(unwrapped.first, cmp) || other.overlaps(unwrapped.second, cmp);\n        } else if (other_wraps) {\n            auto unwrapped = other.unwrap();\n            return overlaps(unwrapped.first, cmp) || overlaps(unwrapped.second, cmp);\n        }\n\n        // No interval should reach this point as wrap around.\n        SCYLLA_ASSERT(!this_wraps);\n        SCYLLA_ASSERT(!other_wraps);\n\n        // if both this and other have an open start, the two intervals will overlap.\n        if (!start() && !other.start()) {\n            return true;\n        }\n\n        return greater_than_or_equal(end_bound(), other.start_bound(), cmp)\n            && greater_than_or_equal(other.end_bound(), start_bound(), cmp);\n    }\n    static wrapping_interval make(bound start, bound end) {\n        return wrapping_interval({std::move(start)}, {std::move(end)});\n    }\n    static constexpr wrapping_interval make_open_ended_both_sides() {\n        return {{}, {}};\n    }\n    static wrapping_interval make_singular(T value) {\n        return {std::move(value)};\n    }\n    static wrapping_interval make_starting_with(bound b) {\n        return {{std::move(b)}, {}};\n    }\n    static wrapping_interval make_ending_with(bound b) {\n        return {{}, {std::move(b)}};\n    }\n    bool is_singular() const {\n        return _singular;\n    }\n    bool is_full() const {\n        return !_start && !_end;\n    }\n    void reverse() {\n        if (!_singular) {\n            std::swap(_start, _end);\n        }\n    }\n    const optional<bound>& start() const {\n        return _start;\n    }\n    const optional<bound>& end() const {\n        return _singular ? _start : _end;\n    }\n    // Range is a wrap around if end value is smaller than the start value\n    // or they're equal and at least one bound is not inclusive.\n    // Comparator must define a total ordering on T.\n    bool is_wrap_around(IntervalComparatorFor<T> auto&& cmp) const {\n        if (_end && _start) {\n            auto r = cmp(end()->value(), start()->value());\n            return r < 0\n                   || (r == 0 && (!start()->is_inclusive() || !end()->is_inclusive()));\n        } else {\n            return false; // open ended interval or singular interval don't wrap around\n        }\n    }\n    // Converts a wrap-around interval to two non-wrap-around intervals.\n    // The returned intervals are not overlapping and ordered.\n    // Call only when is_wrap_around().\n    std::pair<wrapping_interval, wrapping_interval> unwrap() const {\n        return {\n            { {}, end() },\n            { start(), {} }\n        };\n    }\n    // the point is inside the interval\n    // Comparator must define a total ordering on T.\n    bool contains(const T& point, IntervalComparatorFor<T> auto&& cmp) const {\n        if (is_wrap_around(cmp)) {\n            auto unwrapped = unwrap();\n            return unwrapped.first.contains(point, cmp)\n                   || unwrapped.second.contains(point, cmp);\n        } else {\n            return !before(point, cmp) && !after(point, cmp);\n        }\n    }\n    // Returns true iff all values contained by other are also contained by this.\n    // Comparator must define a total ordering on T.\n    bool contains(const wrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {\n        bool this_wraps = is_wrap_around(cmp);\n        bool other_wraps = other.is_wrap_around(cmp);\n\n        if (this_wraps && other_wraps) {\n            return require_ordering_and_on_equal_return(\n                            cmp(start()->value(), other.start()->value()),\n                            std::strong_ordering::less,\n                            start()->is_inclusive() || !other.start()->is_inclusive())\n                && require_ordering_and_on_equal_return(\n                            cmp(end()->value(), other.end()->value()),\n                            std::strong_ordering::greater,\n                            end()->is_inclusive() || !other.end()->is_inclusive());\n        }\n\n        if (!this_wraps && !other_wraps) {\n            return less_than_or_equal(start_bound(), other.start_bound(), cmp)\n                    && greater_than_or_equal(end_bound(), other.end_bound(), cmp);\n        }\n\n        if (other_wraps) { // && !this_wraps\n            return !start() && !end();\n        }\n\n        // !other_wraps && this_wraps\n        return (other.start() && require_ordering_and_on_equal_return(\n                                    cmp(start()->value(), other.start()->value()),\n                                    std::strong_ordering::less,\n                                    start()->is_inclusive() || !other.start()->is_inclusive()))\n                || (other.end() && (require_ordering_and_on_equal_return(\n                                        cmp(end()->value(), other.end()->value()),\n                                        std::strong_ordering::greater,\n                                        end()->is_inclusive() || !other.end()->is_inclusive())));\n    }\n    // Returns intervals which cover all values covered by this interval but not covered by the other interval.\n    // Ranges are not overlapping and ordered.\n    // Comparator must define a total ordering on T.\n    std::vector<wrapping_interval> subtract(const wrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {\n        std::vector<wrapping_interval> result;\n        std::list<wrapping_interval> left;\n        std::list<wrapping_interval> right;\n\n        if (is_wrap_around(cmp)) {\n            auto u = unwrap();\n            left.emplace_back(std::move(u.first));\n            left.emplace_back(std::move(u.second));\n        } else {\n            left.push_back(*this);\n        }\n\n        if (other.is_wrap_around(cmp)) {\n            auto u = other.unwrap();\n            right.emplace_back(std::move(u.first));\n            right.emplace_back(std::move(u.second));\n        } else {\n            right.push_back(other);\n        }\n\n        // left and right contain now non-overlapping, ordered intervals\n\n        while (!left.empty() && !right.empty()) {\n            auto& r1 = left.front();\n            auto& r2 = right.front();\n            if (less_than(r2.end_bound(), r1.start_bound(), cmp)) {\n                right.pop_front();\n            } else if (less_than(r1.end_bound(), r2.start_bound(), cmp)) {\n                result.emplace_back(std::move(r1));\n                left.pop_front();\n            } else { // Overlap\n                auto tmp = std::move(r1);\n                left.pop_front();\n                if (!greater_than_or_equal(r2.end_bound(), tmp.end_bound(), cmp)) {\n                    left.push_front({bound(r2.end()->value(), !r2.end()->is_inclusive()), tmp.end()});\n                }\n                if (!less_than_or_equal(r2.start_bound(), tmp.start_bound(), cmp)) {\n                    left.push_front({tmp.start(), bound(r2.start()->value(), !r2.start()->is_inclusive())});\n                }\n            }\n        }\n\n        std::ranges::copy(left, std::back_inserter(result));\n\n        // TODO: Merge adjacent intervals (optimization)\n        return result;\n    }\n    // split interval in two around a split_point. split_point has to be inside the interval\n    // split_point will belong to first interval\n    // Comparator must define a total ordering on T.\n    std::pair<wrapping_interval<T>, wrapping_interval<T>> split(const T& split_point, IntervalComparatorFor<T> auto&& cmp) const {\n        SCYLLA_ASSERT(contains(split_point, std::forward<decltype(cmp)>(cmp)));\n        wrapping_interval left(start(), bound(split_point));\n        wrapping_interval right(bound(split_point, false), end());\n        return std::make_pair(std::move(left), std::move(right));\n    }\n    // Create a sub-interval including values greater than the split_point. Returns std::nullopt if\n    // split_point is after the end (but not included in the interval, in case of wraparound intervals)\n    // Comparator must define a total ordering on T.\n    std::optional<wrapping_interval<T>> split_after(const T& split_point, IntervalComparatorFor<T> auto&& cmp) const {\n        if (contains(split_point, std::forward<decltype(cmp)>(cmp))\n                && (!end() || cmp(split_point, end()->value()) != 0)) {\n            return wrapping_interval(bound(split_point, false), end());\n        } else if (end() && cmp(split_point, end()->value()) >= 0) {\n            // whether to return std::nullopt or the full interval is not\n            // well-defined for wraparound intervals; we return nullopt\n            // if split_point is after the end.\n            return std::nullopt;\n        } else {\n            return *this;\n        }\n    }\n    template<typename Bound, typename Transformer, typename U = transformed_type<Transformer>>\n    static std::optional<typename wrapping_interval<U>::bound> transform_bound(Bound&& b, Transformer&& transformer) {\n        if (b) {\n            return { { transformer(std::forward<Bound>(b).value().value()), b->is_inclusive() } };\n        };\n        return {};\n    }\n    // Transforms this interval into a new interval of a different value type\n    // Supplied transformer should transform value of type T (the old type) into value of type U (the new type).\n    template<typename Transformer, typename U = transformed_type<Transformer>>\n    wrapping_interval<U> transform(Transformer&& transformer) && {\n        return wrapping_interval<U>(transform_bound(std::move(_start), transformer), transform_bound(std::move(_end), transformer), _singular);\n    }\n    template<typename Transformer, typename U = transformed_type<Transformer>>\n    wrapping_interval<U> transform(Transformer&& transformer) const & {\n        return wrapping_interval<U>(transform_bound(_start, transformer), transform_bound(_end, transformer), _singular);\n    }\n    bool equal(const wrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {\n        return bool(_start) == bool(other._start)\n               && bool(_end) == bool(other._end)\n               && (!_start || _start->equal(*other._start, cmp))\n               && (!_end || _end->equal(*other._end, cmp))\n               && _singular == other._singular;\n    }\n    bool operator==(const wrapping_interval& other) const {\n        return (_start == other._start) && (_end == other._end) && (_singular == other._singular);\n    }\n\nprivate:\n    friend class interval<T>;\n};\n\ntemplate<typename U>\nstruct fmt::formatter<wrapping_interval<U>> : fmt::formatter<string_view> {\n    auto format(const wrapping_interval<U>& r, fmt::format_context& ctx) const {\n        auto out = ctx.out();\n        if (r.is_singular()) {\n            return fmt::format_to(out, \"{{{}}}\", r.start()->value());\n        }\n\n        if (!r.start()) {\n            out = fmt::format_to(out, \"(-inf, \");\n        } else {\n            if (r.start()->is_inclusive()) {\n                out = fmt::format_to(out, \"[\");\n            } else {\n                out = fmt::format_to(out, \"(\");\n            }\n            out = fmt::format_to(out, \"{},\", r.start()->value());\n        }\n\n        if (!r.end()) {\n            out = fmt::format_to(out, \"+inf)\");\n        } else {\n            out = fmt::format_to(out, \"{}\", r.end()->value());\n            if (r.end()->is_inclusive()) {\n                out = fmt::format_to(out, \"]\");\n            } else {\n                out = fmt::format_to(out, \")\");\n            }\n        }\n\n        return out;\n    }\n};\n\ntemplate<typename U>\nstd::ostream& operator<<(std::ostream& out, const wrapping_interval<U>& r) {\n    fmt::print(out, \"{}\", r);\n    return out;\n}\n\n// An interval which can have inclusive, exclusive or open-ended bounds on each end.\n// The end bound can never be smaller than the start bound.\ntemplate<typename T>\nclass interval {\n    template <typename U>\n    using optional = std::optional<U>;\npublic:\n    using bound = interval_bound<T>;\n\n    template <typename Transformer>\n    using transformed_type = typename wrapping_interval<T>::template transformed_type<Transformer>;\nprivate:\n    wrapping_interval<T> _interval;\npublic:\n    interval(T value)\n        : _interval(std::move(value))\n    { }\n    constexpr interval() : interval({}, {}) { }\n    // Can only be called if start <= end. IDL ctor.\n    interval(optional<bound> start, optional<bound> end, bool singular = false)\n        : _interval(std::move(start), std::move(end), singular)\n    { }\n    // Can only be called if !r.is_wrap_around().\n    explicit interval(wrapping_interval<T>&& r)\n        : _interval(std::move(r))\n    { }\n    // Can only be called if !r.is_wrap_around().\n    explicit interval(const wrapping_interval<T>& r)\n        : _interval(r)\n    { }\n    operator wrapping_interval<T>() const & {\n        return _interval;\n    }\n    operator wrapping_interval<T>() && {\n        return std::move(_interval);\n    }\n\n    // the point is before the interval.\n    // Comparator must define a total ordering on T.\n    bool before(const T& point, IntervalComparatorFor<T> auto&& cmp) const {\n        return _interval.before(point, std::forward<decltype(cmp)>(cmp));\n    }\n    // the other interval is before this interval.\n    // Comparator must define a total ordering on T.\n    bool other_is_before(const interval<T>& o, IntervalComparatorFor<T> auto&& cmp) const {\n        return _interval.other_is_before(o, std::forward<decltype(cmp)>(cmp));\n    }\n    // the point is after the interval.\n    // Comparator must define a total ordering on T.\n    bool after(const T& point, IntervalComparatorFor<T> auto&& cmp) const {\n        return _interval.after(point, std::forward<decltype(cmp)>(cmp));\n    }\n    // check if two intervals overlap.\n    // Comparator must define a total ordering on T.\n    bool overlaps(const interval& other, IntervalComparatorFor<T> auto&& cmp) const {\n        // if both this and other have an open start, the two intervals will overlap.\n        if (!start() && !other.start()) {\n            return true;\n        }\n\n        return wrapping_interval<T>::greater_than_or_equal(_interval.end_bound(), other._interval.start_bound(), cmp)\n            && wrapping_interval<T>::greater_than_or_equal(other._interval.end_bound(), _interval.start_bound(), cmp);\n    }\n    static interval make(bound start, bound end) {\n        return interval({std::move(start)}, {std::move(end)});\n    }\n    static constexpr interval make_open_ended_both_sides() {\n        return {{}, {}};\n    }\n    static interval make_singular(T value) {\n        return {std::move(value)};\n    }\n    static interval make_starting_with(bound b) {\n        return {{std::move(b)}, {}};\n    }\n    static interval make_ending_with(bound b) {\n        return {{}, {std::move(b)}};\n    }\n    bool is_singular() const {\n        return _interval.is_singular();\n    }\n    bool is_full() const {\n        return _interval.is_full();\n    }\n    const optional<bound>& start() const {\n        return _interval.start();\n    }\n    const optional<bound>& end() const {\n        return _interval.end();\n    }\n    // the point is inside the interval\n    // Comparator must define a total ordering on T.\n    bool contains(const T& point, IntervalComparatorFor<T> auto&& cmp) const {\n        return !before(point, cmp) && !after(point, cmp);\n    }\n    // Returns true iff all values contained by other are also contained by this.\n    // Comparator must define a total ordering on T.\n    bool contains(const interval& other, IntervalComparatorFor<T> auto&& cmp) const {\n        return wrapping_interval<T>::less_than_or_equal(_interval.start_bound(), other._interval.start_bound(), cmp)\n                && wrapping_interval<T>::greater_than_or_equal(_interval.end_bound(), other._interval.end_bound(), cmp);\n    }\n    // Returns intervals which cover all values covered by this interval but not covered by the other interval.\n    // Ranges are not overlapping and ordered.\n    // Comparator must define a total ordering on T.\n    std::vector<interval> subtract(const interval& other, IntervalComparatorFor<T> auto&& cmp) const {\n        auto subtracted = _interval.subtract(other._interval, std::forward<decltype(cmp)>(cmp));\n        return subtracted | std::views::transform([](auto&& r) {\n            return interval(std::move(r));\n        }) | std::ranges::to<std::vector>();\n    }\n    // split interval in two around a split_point. split_point has to be inside the interval\n    // split_point will belong to first interval\n    // Comparator must define a total ordering on T.\n    std::pair<interval<T>, interval<T>> split(const T& split_point, IntervalComparatorFor<T> auto&& cmp) const {\n        SCYLLA_ASSERT(contains(split_point, std::forward<decltype(cmp)>(cmp)));\n        interval left(start(), bound(split_point));\n        interval right(bound(split_point, false), end());\n        return std::make_pair(std::move(left), std::move(right));\n    }\n    // Create a sub-interval including values greater than the split_point. If split_point is after\n    // the end, returns std::nullopt.\n    std::optional<interval> split_after(const T& split_point, IntervalComparatorFor<T> auto&& cmp) const {\n        if (end() && cmp(split_point, end()->value()) >= 0) {\n            return std::nullopt;\n        } else if (start() && cmp(split_point, start()->value()) < 0) {\n            return *this;\n        } else {\n            return interval(interval_bound<T>(split_point, false), end());\n        }\n    }\n    // Creates a new sub-interval which is the intersection of this interval and an interval starting with \"start\".\n    // If there is no overlap, returns std::nullopt.\n    std::optional<interval> trim_front(std::optional<bound>&& start, IntervalComparatorFor<T> auto&& cmp) const {\n        return intersection(interval(std::move(start), {}), cmp);\n    }\n    // Transforms this interval into a new interval of a different value type\n    // Supplied transformer should transform value of type T (the old type) into value of type U (the new type).\n    template<typename Transformer, typename U = transformed_type<Transformer>>\n    interval<U> transform(Transformer&& transformer) && {\n        return interval<U>(std::move(_interval).transform(std::forward<Transformer>(transformer)));\n    }\n    template<typename Transformer, typename U = transformed_type<Transformer>>\n    interval<U> transform(Transformer&& transformer) const & {\n        return interval<U>(_interval.transform(std::forward<Transformer>(transformer)));\n    }\n    bool equal(const interval& other, IntervalComparatorFor<T> auto&& cmp) const {\n        return _interval.equal(other._interval, std::forward<decltype(cmp)>(cmp));\n    }\n    bool operator==(const interval& other) const {\n        return _interval == other._interval;\n    }\n    // Takes a vector of possibly overlapping intervals and returns a vector containing\n    // a set of non-overlapping intervals covering the same values.\n    template<IntervalComparatorFor<T> Comparator, typename IntervalVec>\n    requires requires (IntervalVec vec) {\n        { vec.begin() } -> std::random_access_iterator;\n        { vec.end() } -> std::random_access_iterator;\n        { vec.reserve(1) };\n        { vec.front() } -> std::same_as<interval&>;\n    }\n    static IntervalVec deoverlap(IntervalVec intervals, Comparator&& cmp) {\n        auto size = intervals.size();\n        if (size <= 1) {\n            return intervals;\n        }\n\n        std::sort(intervals.begin(), intervals.end(), [&](auto&& r1, auto&& r2) {\n            return wrapping_interval<T>::less_than(r1._interval.start_bound(), r2._interval.start_bound(), cmp);\n        });\n\n        IntervalVec deoverlapped_intervals;\n        deoverlapped_intervals.reserve(size);\n\n        auto&& current = intervals[0];\n        for (auto&& r : intervals | std::views::drop(1)) {\n            bool includes_end = wrapping_interval<T>::greater_than_or_equal(r._interval.end_bound(), current._interval.start_bound(), cmp)\n                                && wrapping_interval<T>::greater_than_or_equal(current._interval.end_bound(), r._interval.end_bound(), cmp);\n            if (includes_end) {\n                continue; // last.start <= r.start <= r.end <= last.end\n            }\n            bool includes_start = wrapping_interval<T>::greater_than_or_equal(current._interval.end_bound(), r._interval.start_bound(), cmp);\n            if (includes_start) {\n                current = interval(std::move(current.start()), std::move(r.end()));\n            } else {\n                deoverlapped_intervals.emplace_back(std::move(current));\n                current = std::move(r);\n            }\n        }\n\n        deoverlapped_intervals.emplace_back(std::move(current));\n        return deoverlapped_intervals;\n    }\n\nprivate:\n    // These private functions optimize the case where a sequence supports the\n    // lower and upper bound operations more efficiently, as is the case with\n    // some boost containers.\n    struct std_ {};\n    struct built_in_ : std_ {};\n\n    template<typename Range, IntervalLessComparatorFor<T> LessComparator,\n             typename = decltype(std::declval<Range>().lower_bound(std::declval<T>(), std::declval<LessComparator>()))>\n    typename std::ranges::const_iterator_t<Range> do_lower_bound(const T& value, Range&& r, LessComparator&& cmp, built_in_) const {\n        return r.lower_bound(value, std::forward<LessComparator>(cmp));\n    }\n\n    template<typename Range, IntervalLessComparatorFor<T> LessComparator,\n             typename = decltype(std::declval<Range>().upper_bound(std::declval<T>(), std::declval<LessComparator>()))>\n    typename std::ranges::const_iterator_t<Range> do_upper_bound(const T& value, Range&& r, LessComparator&& cmp, built_in_) const {\n        return r.upper_bound(value, std::forward<LessComparator>(cmp));\n    }\n\n    template<typename Range, IntervalLessComparatorFor<T> LessComparator>\n    typename std::ranges::const_iterator_t<Range> do_lower_bound(const T& value, Range&& r, LessComparator&& cmp, std_) const {\n        return std::lower_bound(r.begin(), r.end(), value, std::forward<LessComparator>(cmp));\n    }\n\n    template<typename Range, IntervalLessComparatorFor<T> LessComparator>\n    typename std::ranges::const_iterator_t<Range> do_upper_bound(const T& value, Range&& r, LessComparator&& cmp, std_) const {\n        return std::upper_bound(r.begin(), r.end(), value, std::forward<LessComparator>(cmp));\n    }\npublic:\n    // Return the lower bound of the specified sequence according to these bounds.\n    template<typename Range, IntervalLessComparatorFor<T> LessComparator>\n    typename std::ranges::const_iterator_t<Range> lower_bound(Range&& r, LessComparator&& cmp) const {\n        return start()\n            ? (start()->is_inclusive()\n                ? do_lower_bound(start()->value(), std::forward<Range>(r), std::forward<LessComparator>(cmp), built_in_())\n                : do_upper_bound(start()->value(), std::forward<Range>(r), std::forward<LessComparator>(cmp), built_in_()))\n            : std::ranges::begin(r);\n    }\n    // Return the upper bound of the specified sequence according to these bounds.\n    template<typename Range, IntervalLessComparatorFor<T> LessComparator>\n    typename std::ranges::const_iterator_t<Range> upper_bound(Range&& r, LessComparator&& cmp) const {\n        return end()\n             ? (end()->is_inclusive()\n                ? do_upper_bound(end()->value(), std::forward<Range>(r), std::forward<LessComparator>(cmp), built_in_())\n                : do_lower_bound(end()->value(), std::forward<Range>(r), std::forward<LessComparator>(cmp), built_in_()))\n             : (is_singular()\n                ? do_upper_bound(start()->value(), std::forward<Range>(r), std::forward<LessComparator>(cmp), built_in_())\n                : std::ranges::end(r));\n    }\n    // Returns a subset of the range that is within these bounds.\n    template<typename Range, IntervalLessComparatorFor<T> LessComparator>\n    std::ranges::subrange<std::ranges::const_iterator_t<Range>>\n    slice(Range&& range, LessComparator&& cmp) const {\n        return std::ranges::subrange(lower_bound(range, cmp), upper_bound(range, cmp));\n    }\n\n    // Returns the intersection between this interval and other.\n    std::optional<interval> intersection(const interval& other, IntervalComparatorFor<T> auto&& cmp) const {\n        auto p = std::minmax(_interval, other._interval, [&cmp] (auto&& a, auto&& b) {\n            return wrapping_interval<T>::less_than(a.start_bound(), b.start_bound(), cmp);\n        });\n        if (wrapping_interval<T>::greater_than_or_equal(p.first.end_bound(), p.second.start_bound(), cmp)) {\n            auto end = std::min(p.first.end_bound(), p.second.end_bound(), [&cmp] (auto&& a, auto&& b) {\n                return !wrapping_interval<T>::greater_than_or_equal(a, b, cmp);\n            });\n            return interval(p.second.start(), end.b);\n        }\n        return {};\n    }\n\n    friend class fmt::formatter<interval<T>>;\n};\n\ntemplate<typename U>\nstruct fmt::formatter<interval<U>> : fmt::formatter<string_view> {\n    auto format(const interval<U>& r, fmt::format_context& ctx) const {\n        return fmt::format_to(ctx.out(), \"{}\", r._interval);\n    }\n};\n\ntemplate<typename U>\nstd::ostream& operator<<(std::ostream& out, const interval<U>& r) {\n    fmt::print(out, \"{}\", r);\n    return out;\n}\n\ntemplate<template<typename> typename T, typename U>\nconcept Interval = std::is_same<T<U>, wrapping_interval<U>>::value || std::is_same<T<U>, interval<U>>::value;\n\n// Allow using interval<T> in a hash table. The hash function 31 * left +\n// right is the same one used by Cassandra's AbstractBounds.hashCode().\nnamespace std {\n\ntemplate<typename T>\nstruct hash<wrapping_interval<T>> {\n    using argument_type = wrapping_interval<T>;\n    using result_type = decltype(std::hash<T>()(std::declval<T>()));\n    result_type operator()(argument_type const& s) const {\n        auto hash = std::hash<T>();\n        auto left = s.start() ? hash(s.start()->value()) : 0;\n        auto right = s.end() ? hash(s.end()->value()) : 0;\n        return 31 * left + right;\n    }\n};\n\ntemplate<typename T>\nstruct hash<interval<T>> {\n    using argument_type = interval<T>;\n    using result_type = decltype(std::hash<T>()(std::declval<T>()));\n    result_type operator()(argument_type const& s) const {\n        return hash<wrapping_interval<T>>()(s);\n    }\n};\n\n}\n"
        },
        {
          "name": "keys.cc",
          "type": "blob",
          "size": 3.5859375,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include <iostream>\n\n#include \"keys.hh\"\n#include \"dht/i_partitioner.hh\"\n#include \"clustering_bounds_comparator.hh\"\n#include <boost/algorithm/string.hpp>\n\nlogging::logger klog(\"keys\");\n\nconst legacy_compound_view<partition_key_view::c_type>\npartition_key_view::legacy_form(const schema& s) const {\n    return { *get_compound_type(s), _bytes };\n}\n\nstd::strong_ordering\npartition_key_view::legacy_tri_compare(const schema& s, partition_key_view o) const {\n    auto cmp = legacy_compound_view<c_type>::tri_comparator(*get_compound_type(s));\n    return cmp(this->representation(), o.representation());\n}\n\nstd::strong_ordering\npartition_key_view::ring_order_tri_compare(const schema& s, partition_key_view k2) const {\n    auto t1 = dht::get_token(s, *this);\n    auto t2 = dht::get_token(s, k2);\n    if (t1 != t2) {\n        return t1 < t2 ? std::strong_ordering::less : std::strong_ordering::greater;\n    }\n    return legacy_tri_compare(s, k2);\n}\n\npartition_key partition_key::from_nodetool_style_string(const schema_ptr s, const sstring& key) {\n    std::vector<sstring> vec;\n    boost::split(vec, key, boost::is_any_of(\":\"));\n\n    auto it = std::begin(vec);\n    if (vec.size() != s->partition_key_type()->types().size()) {\n        throw std::invalid_argument(\"partition key '\" + key + \"' has mismatch number of components\");\n    }\n    std::vector<bytes> r;\n    r.reserve(vec.size());\n    for (auto t : s->partition_key_type()->types()) {\n        r.emplace_back(t->from_string(*it++));\n    }\n    return partition_key::from_range(std::move(r));\n}\n\nauto fmt::formatter<bound_kind>::format(bound_kind k, fmt::format_context& ctx) const\n        -> decltype(ctx.out()) {\n    std::string_view name;\n    switch (k) {\n    case bound_kind::excl_end:\n        name = \"excl end\";\n        break;\n    case bound_kind::incl_start:\n        name = \"incl start\";\n        break;\n    case bound_kind::incl_end:\n        name = \"incl end\";\n        break;\n    case bound_kind::excl_start:\n        name = \"excl start\";\n        break;\n    }\n    return fmt::format_to(ctx.out(), \"{}\", name);\n}\n\nbound_kind invert_kind(bound_kind k) {\n    switch (k) {\n    case bound_kind::excl_start: return bound_kind::incl_end;\n    case bound_kind::incl_start: return bound_kind::excl_end;\n    case bound_kind::excl_end:   return bound_kind::incl_start;\n    case bound_kind::incl_end:   return bound_kind::excl_start;\n    }\n    abort();\n}\n\nbound_kind reverse_kind(bound_kind k) {\n    switch (k) {\n    case bound_kind::excl_start: return bound_kind::excl_end;\n    case bound_kind::incl_start: return bound_kind::incl_end;\n    case bound_kind::excl_end:   return bound_kind::excl_start;\n    case bound_kind::incl_end:   return bound_kind::incl_start;\n    }\n    on_internal_error(klog, format(\"reverse_kind(): invalid value for `bound_kind`: {}\", static_cast<std::underlying_type_t<bound_kind>>(k)));\n}\n\nint32_t weight(bound_kind k) {\n    switch (k) {\n    case bound_kind::excl_end:\n        return -2;\n    case bound_kind::incl_start:\n        return -1;\n    case bound_kind::incl_end:\n        return 1;\n    case bound_kind::excl_start:\n        return 2;\n    }\n    abort();\n}\n\nconst thread_local clustering_key_prefix bound_view::_empty_prefix = clustering_key::make_empty();\n\nstd::ostream&\noperator<<(std::ostream& os, const exploded_clustering_prefix& ecp) {\n    // Can't pass to_hex() to transform(), since it is overloaded, so wrap:\n    auto enhex = [] (auto&& x) { return fmt_hex(x); };\n    fmt::print(os, \"prefix{{{}}}\", fmt::join(ecp._v | std::views::transform(enhex), \":\"));\n    return os;\n}\n"
        },
        {
          "name": "keys.hh",
          "type": "blob",
          "size": 33.6474609375,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"bytes.hh\"\n#include \"types/types.hh\"\n#include \"compound_compat.hh\"\n#include \"utils/managed_bytes.hh\"\n#include \"utils/hashing.hh\"\n#include \"utils/utf8.hh\"\n#include \"replica/database_fwd.hh\"\n#include \"schema/schema_fwd.hh\"\n#include <compare>\n#include <span>\n\n//\n// This header defines type system for primary key holders.\n//\n// We distinguish partition keys and clustering keys. API-wise they are almost\n// the same, but they're separate type hierarchies.\n//\n// Clustering keys are further divided into prefixed and non-prefixed (full).\n// Non-prefixed keys always have full component set, as defined by schema.\n// Prefixed ones can have any number of trailing components missing. They may\n// differ in underlying representation.\n//\n// The main classes are:\n//\n//   partition_key           - full partition key\n//   clustering_key          - full clustering key\n//   clustering_key_prefix   - clustering key prefix\n//\n// These classes wrap only the minimum information required to store the key\n// (the key value itself). Any information which can be inferred from schema\n// is not stored. Therefore accessors need to be provided with a pointer to\n// schema, from which information about structure is extracted.\n\n// Abstracts a view to serialized compound.\ntemplate <typename TopLevelView>\nclass compound_view_wrapper {\nprotected:\n    managed_bytes_view _bytes;\nprotected:\n    compound_view_wrapper(managed_bytes_view v)\n        : _bytes(v)\n    { }\n\n    static inline const auto& get_compound_type(const schema& s) {\n        return TopLevelView::get_compound_type(s);\n    }\npublic:\n    std::vector<bytes> explode(const schema& s) const {\n        return get_compound_type(s)->deserialize_value(_bytes);\n    }\n\n    managed_bytes_view representation() const {\n        return _bytes;\n    }\n\n    struct less_compare {\n        typename TopLevelView::compound _t;\n        less_compare(const schema& s) : _t(get_compound_type(s)) {}\n        bool operator()(const TopLevelView& k1, const TopLevelView& k2) const {\n            return _t->less(k1.representation(), k2.representation());\n        }\n    };\n\n    struct tri_compare {\n        typename TopLevelView::compound _t;\n        tri_compare(const schema &s) : _t(get_compound_type(s)) {}\n        std::strong_ordering operator()(const TopLevelView& k1, const TopLevelView& k2) const {\n            return _t->compare(k1.representation(), k2.representation());\n        }\n    };\n\n    struct hashing {\n        typename TopLevelView::compound _t;\n        hashing(const schema& s) : _t(get_compound_type(s)) {}\n        size_t operator()(const TopLevelView& o) const {\n            return _t->hash(o.representation());\n        }\n    };\n\n    struct equality {\n        typename TopLevelView::compound _t;\n        equality(const schema& s) : _t(get_compound_type(s)) {}\n        bool operator()(const TopLevelView& o1, const TopLevelView& o2) const {\n            return _t->equal(o1.representation(), o2.representation());\n        }\n    };\n\n    bool equal(const schema& s, const TopLevelView& other) const {\n        return get_compound_type(s)->equal(representation(), other.representation());\n    }\n\n    // begin() and end() return iterators over components of this compound. The iterator yields a managed_bytes_view to the component.\n    // The iterators satisfy InputIterator concept.\n    auto begin() const {\n        return TopLevelView::compound::element_type::begin(representation());\n    }\n\n    // See begin()\n    auto end() const {\n        return TopLevelView::compound::element_type::end(representation());\n    }\n\n    // begin() and end() return iterators over components of this compound. The iterator yields a managed_bytes_view to the component.\n    // The iterators satisfy InputIterator concept.\n    auto begin(const schema& s) const {\n        return begin();\n    }\n\n    // See begin()\n    auto end(const schema& s) const {\n        return end();\n    }\n\n    // Returns a range of managed_bytes_view\n    auto components() const {\n        return TopLevelView::compound::element_type::components(representation());\n    }\n\n    // Returns a range of managed_bytes_view\n    auto components(const schema& s) const {\n        return components();\n    }\n\n    bool is_empty() const {\n        return _bytes.empty();\n    }\n\n    explicit operator bool() const {\n        return !is_empty();\n    }\n\n    // For backward compatibility with existing code.\n    bool is_empty(const schema& s) const {\n        return is_empty();\n    }\n};\n\ntemplate <typename TopLevel, typename TopLevelView>\nclass compound_wrapper {\nprotected:\n    managed_bytes _bytes;\nprotected:\n    compound_wrapper(managed_bytes&& b) : _bytes(std::move(b)) {}\n\n    static inline const auto& get_compound_type(const schema& s) {\n        return TopLevel::get_compound_type(s);\n    }\nprivate:\n    static const data_type& get_singular_type(const schema& s) {\n        const auto& ct = get_compound_type(s);\n        if (!ct->is_singular()) {\n            throw std::invalid_argument(\"compound is not singular\");\n        }\n        return ct->types()[0];\n    }\npublic:\n    struct with_schema_wrapper {\n        with_schema_wrapper(const schema& s, const TopLevel& key) : s(s), key(key) {}\n        const schema& s;\n        const TopLevel& key;\n    };\n\n    with_schema_wrapper with_schema(const schema& s) const {\n        return with_schema_wrapper(s, *static_cast<const TopLevel*>(this));\n    }\n\n    static TopLevel make_empty() {\n        return from_exploded(std::vector<bytes>());\n    }\n\n    static TopLevel make_empty(const schema&) {\n        return make_empty();\n    }\n\n    template<typename RangeOfSerializedComponents>\n    static TopLevel from_exploded(RangeOfSerializedComponents&& v) {\n        return TopLevel::from_range(std::forward<RangeOfSerializedComponents>(v));\n    }\n\n    static TopLevel from_exploded(const schema& s, const std::vector<bytes>& v) {\n        return from_exploded(v);\n    }\n    static TopLevel from_exploded(const schema& s, const std::vector<managed_bytes>& v) {\n        return from_exploded(v);\n    }\n    static TopLevel from_exploded_view(const std::vector<bytes_view>& v) {\n        return from_exploded(v);\n    }\n\n    // We don't allow optional values, but provide this method as an efficient adaptor\n    static TopLevel from_optional_exploded(const schema& s, std::span<const bytes_opt> v) {\n        return TopLevel::from_bytes(get_compound_type(s)->serialize_optionals(v));\n    }\n    static TopLevel from_optional_exploded(const schema& s, std::span<const managed_bytes_opt> v) {\n        return TopLevel::from_bytes(get_compound_type(s)->serialize_optionals(v));\n    }\n\n    static TopLevel from_deeply_exploded(const schema& s, const std::vector<data_value>& v) {\n        return TopLevel::from_bytes(get_compound_type(s)->serialize_value_deep(v));\n    }\n\n    static TopLevel from_single_value(const schema& s, const bytes& v) {\n        return TopLevel::from_bytes(get_compound_type(s)->serialize_single(v));\n    }\n\n    static TopLevel from_single_value(const schema& s, const managed_bytes& v) {\n        return TopLevel::from_bytes(get_compound_type(s)->serialize_single(v));\n    }\n\n    template <typename T>\n    static\n    TopLevel from_singular(const schema& s, const T& v) {\n        const auto& type = get_singular_type(s);\n        return from_single_value(s, type->decompose(v));\n    }\n\n    static TopLevel from_singular_bytes(const schema& s, const bytes& b) {\n        get_singular_type(s); // validation\n        return from_single_value(s, b);\n    }\n\n    TopLevelView view() const {\n        return TopLevelView::from_bytes(_bytes);\n    }\n\n    operator TopLevelView() const {\n        return view();\n    }\n\n    // FIXME: return views\n    std::vector<bytes> explode(const schema& s) const {\n        return get_compound_type(s)->deserialize_value(_bytes);\n    }\n\n    std::vector<bytes> explode() const {\n        std::vector<bytes> result;\n        for (managed_bytes_view c : components()) {\n            result.emplace_back(to_bytes(c));\n        }\n        return result;\n    }\n\n    std::vector<managed_bytes> explode_fragmented() const {\n        std::vector<managed_bytes> result;\n        for (managed_bytes_view c : components()) {\n            result.emplace_back(managed_bytes(c));\n        }\n        return result;\n    }\n\n    struct tri_compare {\n        typename TopLevel::compound _t;\n        tri_compare(const schema& s) : _t(get_compound_type(s)) {}\n        std::strong_ordering operator()(const TopLevel& k1, const TopLevel& k2) const {\n            return _t->compare(k1.representation(), k2.representation());\n        }\n        std::strong_ordering operator()(const TopLevelView& k1, const TopLevel& k2) const {\n            return _t->compare(k1.representation(), k2.representation());\n        }\n        std::strong_ordering operator()(const TopLevel& k1, const TopLevelView& k2) const {\n            return _t->compare(k1.representation(), k2.representation());\n        }\n    };\n\n    struct less_compare {\n        typename TopLevel::compound _t;\n        less_compare(const schema& s) : _t(get_compound_type(s)) {}\n        bool operator()(const TopLevel& k1, const TopLevel& k2) const {\n            return _t->less(k1.representation(), k2.representation());\n        }\n        bool operator()(const TopLevelView& k1, const TopLevel& k2) const {\n            return _t->less(k1.representation(), k2.representation());\n        }\n        bool operator()(const TopLevel& k1, const TopLevelView& k2) const {\n            return _t->less(k1.representation(), k2.representation());\n        }\n    };\n\n    struct hashing {\n        typename TopLevel::compound _t;\n        hashing(const schema& s) : _t(get_compound_type(s)) {}\n        size_t operator()(const TopLevel& o) const {\n            return _t->hash(o.representation());\n        }\n        size_t operator()(const TopLevelView& o) const {\n            return _t->hash(o.representation());\n        }\n    };\n\n    struct equality {\n        typename TopLevel::compound _t;\n        equality(const schema& s) : _t(get_compound_type(s)) {}\n        bool operator()(const TopLevel& o1, const TopLevel& o2) const {\n            return _t->equal(o1.representation(), o2.representation());\n        }\n        bool operator()(const TopLevelView& o1, const TopLevel& o2) const {\n            return _t->equal(o1.representation(), o2.representation());\n        }\n        bool operator()(const TopLevel& o1, const TopLevelView& o2) const {\n            return _t->equal(o1.representation(), o2.representation());\n        }\n    };\n\n    bool equal(const schema& s, const TopLevel& other) const {\n        return get_compound_type(s)->equal(representation(), other.representation());\n    }\n\n    bool equal(const schema& s, const TopLevelView& other) const {\n        return get_compound_type(s)->equal(representation(), other.representation());\n    }\n\n    operator managed_bytes_view() const\n    {\n        return _bytes;\n    }\n\n    const managed_bytes& representation() const {\n        return _bytes;\n    }\n\n    // begin() and end() return iterators over components of this compound. The iterator yields a managed_bytes_view to the component.\n    // The iterators satisfy InputIterator concept.\n    auto begin(const schema& s) const {\n        return get_compound_type(s)->begin(_bytes);\n    }\n\n    // See begin()\n    auto end(const schema& s) const {\n        return get_compound_type(s)->end(_bytes);\n    }\n\n    bool is_empty() const {\n        return _bytes.empty();\n    }\n\n    explicit operator bool() const {\n        return !is_empty();\n    }\n\n    // For backward compatibility with existing code.\n    bool is_empty(const schema& s) const {\n        return is_empty();\n    }\n\n    // Returns a range of managed_bytes_view\n    auto components() const {\n        return TopLevelView::compound::element_type::components(representation());\n    }\n\n    // Returns a range of managed_bytes_view\n    auto components(const schema& s) const {\n        return components();\n    }\n\n    managed_bytes_view get_component(const schema& s, size_t idx) const {\n        auto it = begin(s);\n        std::advance(it, idx);\n        return *it;\n    }\n\n    // Returns the number of components of this compound.\n    size_t size(const schema& s) const {\n        return std::distance(begin(s), end(s));\n    }\n\n    size_t minimal_external_memory_usage() const {\n        return _bytes.minimal_external_memory_usage();\n    }\n\n    size_t external_memory_usage() const noexcept {\n        return _bytes.external_memory_usage();\n    }\n\n    size_t memory_usage() const noexcept {\n        return sizeof(*this) + external_memory_usage();\n    }\n};\n\ntemplate <typename TopLevel, typename PrefixTopLevel>\nclass prefix_view_on_full_compound {\npublic:\n    using iterator = typename compound_type<allow_prefixes::no>::iterator;\nprivate:\n    bytes_view _b;\n    unsigned _prefix_len;\n    iterator _begin;\n    iterator _end;\npublic:\n    prefix_view_on_full_compound(const schema& s, bytes_view b, unsigned prefix_len)\n        : _b(b)\n        , _prefix_len(prefix_len)\n        , _begin(TopLevel::get_compound_type(s)->begin(_b))\n        , _end(_begin)\n    {\n        std::advance(_end, prefix_len);\n    }\n\n    iterator begin() const { return _begin; }\n    iterator end() const { return _end; }\n\n    struct less_compare_with_prefix {\n        typename PrefixTopLevel::compound prefix_type;\n\n        less_compare_with_prefix(const schema& s)\n            : prefix_type(PrefixTopLevel::get_compound_type(s))\n        { }\n\n        bool operator()(const prefix_view_on_full_compound& k1, const PrefixTopLevel& k2) const {\n            return lexicographical_tri_compare(\n                prefix_type->types().begin(), prefix_type->types().end(),\n                k1.begin(), k1.end(),\n                prefix_type->begin(k2), prefix_type->end(k2),\n                tri_compare) < 0;\n        }\n\n        bool operator()(const PrefixTopLevel& k1, const prefix_view_on_full_compound& k2) const {\n            return lexicographical_tri_compare(\n                prefix_type->types().begin(), prefix_type->types().end(),\n                prefix_type->begin(k1), prefix_type->end(k1),\n                k2.begin(), k2.end(),\n                tri_compare) < 0;\n        }\n    };\n};\n\ntemplate <typename TopLevel>\nclass prefix_view_on_prefix_compound {\npublic:\n    using iterator = typename compound_type<allow_prefixes::yes>::iterator;\nprivate:\n    bytes_view _b;\n    unsigned _prefix_len;\n    iterator _begin;\n    iterator _end;\npublic:\n    prefix_view_on_prefix_compound(const schema& s, bytes_view b, unsigned prefix_len)\n        : _b(b)\n        , _prefix_len(prefix_len)\n        , _begin(TopLevel::get_compound_type(s)->begin(_b))\n        , _end(_begin)\n    {\n        std::advance(_end, prefix_len);\n    }\n\n    iterator begin() const { return _begin; }\n    iterator end() const { return _end; }\n\n    struct less_compare_with_prefix {\n        typename TopLevel::compound prefix_type;\n\n        less_compare_with_prefix(const schema& s)\n            : prefix_type(TopLevel::get_compound_type(s))\n        { }\n\n        bool operator()(const prefix_view_on_prefix_compound& k1, const TopLevel& k2) const {\n            return lexicographical_tri_compare(\n                prefix_type->types().begin(), prefix_type->types().end(),\n                k1.begin(), k1.end(),\n                prefix_type->begin(k2), prefix_type->end(k2),\n                tri_compare) < 0;\n        }\n\n        bool operator()(const TopLevel& k1, const prefix_view_on_prefix_compound& k2) const {\n            return lexicographical_tri_compare(\n                prefix_type->types().begin(), prefix_type->types().end(),\n                prefix_type->begin(k1), prefix_type->end(k1),\n                k2.begin(), k2.end(),\n                tri_compare) < 0;\n        }\n    };\n};\n\ntemplate <typename TopLevel, typename TopLevelView, typename PrefixTopLevel>\nclass prefixable_full_compound : public compound_wrapper<TopLevel, TopLevelView> {\n    using base = compound_wrapper<TopLevel, TopLevelView>;\nprotected:\n    prefixable_full_compound(bytes&& b) : base(std::move(b)) {}\npublic:\n    using prefix_view_type = prefix_view_on_full_compound<TopLevel, PrefixTopLevel>;\n\n    bool is_prefixed_by(const schema& s, const PrefixTopLevel& prefix) const {\n        const auto& t = base::get_compound_type(s);\n        const auto& prefix_type = PrefixTopLevel::get_compound_type(s);\n        return ::is_prefixed_by(t->types().begin(),\n            t->begin(*this), t->end(*this),\n            prefix_type->begin(prefix), prefix_type->end(prefix),\n            ::equal);\n    }\n\n    struct less_compare_with_prefix {\n        typename PrefixTopLevel::compound prefix_type;\n        typename TopLevel::compound full_type;\n\n        less_compare_with_prefix(const schema& s)\n            : prefix_type(PrefixTopLevel::get_compound_type(s))\n            , full_type(TopLevel::get_compound_type(s))\n        { }\n\n        bool operator()(const TopLevel& k1, const PrefixTopLevel& k2) const {\n            return lexicographical_tri_compare(\n                prefix_type->types().begin(), prefix_type->types().end(),\n                full_type->begin(k1), full_type->end(k1),\n                prefix_type->begin(k2), prefix_type->end(k2),\n                tri_compare) < 0;\n        }\n\n        bool operator()(const PrefixTopLevel& k1, const TopLevel& k2) const {\n            return lexicographical_tri_compare(\n                prefix_type->types().begin(), prefix_type->types().end(),\n                prefix_type->begin(k1), prefix_type->end(k1),\n                full_type->begin(k2), full_type->end(k2),\n                tri_compare) < 0;\n        }\n    };\n\n    // In prefix equality two sequences are equal if any of them is a prefix\n    // of the other. Otherwise lexicographical ordering is applied.\n    // Note: full compounds sorted according to lexicographical ordering are also\n    // sorted according to prefix equality ordering.\n    struct prefix_equality_less_compare {\n        typename PrefixTopLevel::compound prefix_type;\n        typename TopLevel::compound full_type;\n\n        prefix_equality_less_compare(const schema& s)\n            : prefix_type(PrefixTopLevel::get_compound_type(s))\n            , full_type(TopLevel::get_compound_type(s))\n        { }\n\n        bool operator()(const TopLevel& k1, const PrefixTopLevel& k2) const {\n            return prefix_equality_tri_compare(prefix_type->types().begin(),\n                full_type->begin(k1), full_type->end(k1),\n                prefix_type->begin(k2), prefix_type->end(k2),\n                tri_compare) < 0;\n        }\n\n        bool operator()(const PrefixTopLevel& k1, const TopLevel& k2) const {\n            return prefix_equality_tri_compare(prefix_type->types().begin(),\n                prefix_type->begin(k1), prefix_type->end(k1),\n                full_type->begin(k2), full_type->end(k2),\n                tri_compare) < 0;\n        }\n    };\n\n    prefix_view_type prefix_view(const schema& s, unsigned prefix_len) const {\n        return { s, this->representation(), prefix_len };\n    }\n};\n\ntemplate <typename TopLevel, typename FullTopLevel>\nclass prefix_compound_view_wrapper : public compound_view_wrapper<TopLevel> {\n    using base = compound_view_wrapper<TopLevel>;\nprotected:\n    prefix_compound_view_wrapper(managed_bytes_view v)\n        : compound_view_wrapper<TopLevel>(v)\n    { }\n\npublic:\n    bool is_full(const schema& s) const {\n        return TopLevel::get_compound_type(s)->is_full(base::_bytes);\n    }\n};\n\ntemplate <typename TopLevel, typename TopLevelView, typename FullTopLevel>\nclass prefix_compound_wrapper : public compound_wrapper<TopLevel, TopLevelView> {\n    using base = compound_wrapper<TopLevel, TopLevelView>;\nprotected:\n    prefix_compound_wrapper(managed_bytes&& b) : base(std::move(b)) {}\npublic:\n    using prefix_view_type = prefix_view_on_prefix_compound<TopLevel>;\n\n    prefix_view_type prefix_view(const schema& s, unsigned prefix_len) const {\n        return { s, this->representation(), prefix_len };\n    }\n\n    bool is_full(const schema& s) const {\n        return TopLevel::get_compound_type(s)->is_full(base::_bytes);\n    }\n\n    // Can be called only if is_full()\n    FullTopLevel to_full(const schema& s) const {\n        return FullTopLevel::from_exploded(s, base::explode(s));\n    }\n\n    bool is_prefixed_by(const schema& s, const TopLevel& prefix) const {\n        const auto& t = base::get_compound_type(s);\n        return ::is_prefixed_by(t->types().begin(),\n            t->begin(*this), t->end(*this),\n            t->begin(prefix), t->end(prefix),\n            equal);\n    }\n\n    // In prefix equality two sequences are equal if any of them is a prefix\n    // of the other. Otherwise lexicographical ordering is applied.\n    // Note: full compounds sorted according to lexicographical ordering are also\n    // sorted according to prefix equality ordering.\n    struct prefix_equality_less_compare {\n        typename TopLevel::compound prefix_type;\n\n        prefix_equality_less_compare(const schema& s)\n            : prefix_type(TopLevel::get_compound_type(s))\n        { }\n\n        bool operator()(const TopLevel& k1, const TopLevel& k2) const {\n            return prefix_equality_tri_compare(prefix_type->types().begin(),\n                prefix_type->begin(k1.representation()), prefix_type->end(k1.representation()),\n                prefix_type->begin(k2.representation()), prefix_type->end(k2.representation()),\n                tri_compare) < 0;\n        }\n    };\n\n    // See prefix_equality_less_compare.\n    struct prefix_equal_tri_compare {\n        typename TopLevel::compound prefix_type;\n\n        prefix_equal_tri_compare(const schema& s)\n            : prefix_type(TopLevel::get_compound_type(s))\n        { }\n\n        std::strong_ordering operator()(const TopLevel& k1, const TopLevel& k2) const {\n            return prefix_equality_tri_compare(prefix_type->types().begin(),\n                prefix_type->begin(k1.representation()), prefix_type->end(k1.representation()),\n                prefix_type->begin(k2.representation()), prefix_type->end(k2.representation()),\n                tri_compare);\n        }\n    };\n};\n\nclass partition_key_view : public compound_view_wrapper<partition_key_view> {\npublic:\n    using c_type = compound_type<allow_prefixes::no>;\nprivate:\n    partition_key_view(managed_bytes_view v)\n        : compound_view_wrapper<partition_key_view>(v)\n    { }\npublic:\n    using compound = lw_shared_ptr<c_type>;\n\n    static partition_key_view from_bytes(managed_bytes_view v) {\n        return { v };\n    }\n\n    static const compound& get_compound_type(const schema& s) {\n        return s.partition_key_type();\n    }\n\n    // Returns key's representation which is compatible with Origin.\n    // The result is valid as long as the schema is live.\n    const legacy_compound_view<c_type> legacy_form(const schema& s) const;\n\n    // A trichotomic comparator for ordering compatible with Origin.\n    std::strong_ordering legacy_tri_compare(const schema& s, partition_key_view o) const;\n\n    // Checks if keys are equal in a way which is compatible with Origin.\n    bool legacy_equal(const schema& s, partition_key_view o) const {\n        return legacy_tri_compare(s, o) == 0;\n    }\n\n    void validate(const schema& s) const {\n        return s.partition_key_type()->validate(representation());\n    }\n\n    // A trichotomic comparator which orders keys according to their ordering on the ring.\n    std::strong_ordering ring_order_tri_compare(const schema& s, partition_key_view o) const;\n};\n\ntemplate <>\nstruct fmt::formatter<partition_key_view> : fmt::formatter<string_view> {\n    template <typename FormatContext>\n    auto format(const partition_key_view& pk, FormatContext& ctx) const {\n        return with_linearized(pk.representation(), [&] (bytes_view v) {\n            return fmt::format_to(ctx.out(), \"pk{{{}}}\", fmt_hex(v));\n        });\n    }\n};\n\nclass partition_key : public compound_wrapper<partition_key, partition_key_view> {\n    explicit partition_key(managed_bytes&& b)\n        : compound_wrapper<partition_key, partition_key_view>(std::move(b))\n    { }\npublic:\n    using c_type = compound_type<allow_prefixes::no>;\n\n    template<typename RangeOfSerializedComponents>\n    static partition_key from_range(RangeOfSerializedComponents&& v) {\n        return partition_key(managed_bytes(c_type::serialize_value(std::forward<RangeOfSerializedComponents>(v))));\n    }\n\n    /*!\n     * \\brief create a partition_key from a nodetool style string\n     * takes a nodetool style string representation of a partition key and returns a partition_key.\n     * With composite keys, columns are concatenate using ':'.\n     * For example if a composite key is has two columns (col1, col2) to get the partition key that\n     * have col1=val1 and col2=val2 use the string 'val1:val2'\n     */\n    static partition_key from_nodetool_style_string(const schema_ptr s, const sstring& key);\n\n    partition_key(std::vector<bytes> v)\n        : compound_wrapper(managed_bytes(c_type::serialize_value(std::move(v))))\n    { }\n    partition_key(std::initializer_list<bytes> v) : partition_key(std::vector(v)) {}    \n\n    partition_key(partition_key&& v) = default;\n    partition_key(const partition_key& v) = default;\n    partition_key(partition_key& v) = default;\n    partition_key& operator=(const partition_key&) = default;\n    partition_key& operator=(partition_key&) = default;\n    partition_key& operator=(partition_key&&) = default;\n\n    partition_key(partition_key_view key)\n        : partition_key(managed_bytes(key.representation()))\n    { }\n\n    using compound = lw_shared_ptr<c_type>;\n\n    static partition_key from_bytes(managed_bytes_view b) {\n        return partition_key(managed_bytes(b));\n    }\n    static partition_key from_bytes(managed_bytes&& b) {\n        return partition_key(std::move(b));\n    }\n    static partition_key from_bytes(bytes_view b) {\n        return partition_key(managed_bytes(b));\n    }\n\n    static const compound& get_compound_type(const schema& s) {\n        return s.partition_key_type();\n    }\n\n    // Returns key's representation which is compatible with Origin.\n    // The result is valid as long as the schema is live.\n    const legacy_compound_view<c_type> legacy_form(const schema& s) const {\n        return view().legacy_form(s);\n    }\n\n    // A trichotomic comparator for ordering compatible with Origin.\n    std::strong_ordering legacy_tri_compare(const schema& s, const partition_key& o) const {\n        return view().legacy_tri_compare(s, o);\n    }\n\n    // Checks if keys are equal in a way which is compatible with Origin.\n    bool legacy_equal(const schema& s, const partition_key& o) const {\n        return view().legacy_equal(s, o);\n    }\n\n    void validate(const schema& s) const {\n        return s.partition_key_type()->validate(representation());\n    }\n};\n\ntemplate <>\nstruct fmt::formatter<partition_key> : fmt::formatter<string_view> {\n    template <typename FormatContext>\n    auto format(const partition_key& pk, FormatContext& ctx) const {\n        return fmt::format_to(ctx.out(), \"pk{{{}}}\", managed_bytes_view(pk.representation()));\n    }\n};\n\nnamespace detail {\n\ntemplate <typename WithSchemaWrapper, typename FormatContext>\nauto format_pk(const WithSchemaWrapper& pk, FormatContext& ctx) {\n    const auto& [schema, key] = pk;\n    auto type_iterator = key.get_compound_type(schema)->types().begin();\n    bool first = true;\n    auto out = ctx.out();\n    for (auto&& component : key.components(schema)) {\n        if (!first) {\n            out = fmt::format_to(out, \"{}\", \":\");\n        }\n        first = false;\n        auto key = (*type_iterator++)->to_string(to_bytes(component));\n        if (utils::utf8::validate((const uint8_t *) key.data(), key.size())) {\n            out = fmt::format_to(out, \"{}\", key);\n        } else {\n            out = fmt::format_to(out, \"{}\", \"<non-utf8-key>\");\n        }\n    }\n    return out;\n}\n} // namespace detail\n\ntemplate <>\nstruct fmt::formatter<partition_key::with_schema_wrapper> : fmt::formatter<string_view> {\n    template <typename FormatContext>\n    auto format(const partition_key::with_schema_wrapper& pk, FormatContext& ctx) const {\n        return ::detail::format_pk(pk, ctx);\n    }\n};\n\nclass exploded_clustering_prefix {\n    std::vector<bytes> _v;\npublic:\n    exploded_clustering_prefix(std::vector<bytes>&& v) : _v(std::move(v)) {}\n    exploded_clustering_prefix() {}\n    size_t size() const {\n        return _v.size();\n    }\n    auto const& components() const {\n        return _v;\n    }\n    explicit operator bool() const {\n        return !_v.empty();\n    }\n    bool is_full(const schema& s) const {\n        return _v.size() == s.clustering_key_size();\n    }\n    friend std::ostream& operator<<(std::ostream& os, const exploded_clustering_prefix& ecp);\n};\n\nclass clustering_key_prefix_view : public prefix_compound_view_wrapper<clustering_key_prefix_view, clustering_key> {\n    clustering_key_prefix_view(managed_bytes_view v)\n        : prefix_compound_view_wrapper<clustering_key_prefix_view, clustering_key>(v)\n    { }\npublic:\n    static clustering_key_prefix_view from_bytes(const managed_bytes& v) {\n        return { v };\n    }\n    static clustering_key_prefix_view from_bytes(managed_bytes_view v) {\n        return { v };\n    }\n    static clustering_key_prefix_view from_bytes(bytes_view v) {\n        return { v };\n    }\n\n    using compound = lw_shared_ptr<compound_type<allow_prefixes::yes>>;\n\n    static const compound& get_compound_type(const schema& s) {\n        return s.clustering_key_prefix_type();\n    }\n\n    static clustering_key_prefix_view make_empty() {\n        return { bytes_view() };\n    }\n};\n\nclass clustering_key_prefix : public prefix_compound_wrapper<clustering_key_prefix, clustering_key_prefix_view, clustering_key> {\n    explicit clustering_key_prefix(managed_bytes&& b)\n            : prefix_compound_wrapper<clustering_key_prefix, clustering_key_prefix_view, clustering_key>(std::move(b))\n    { }\npublic:\n    template<typename RangeOfSerializedComponents>\n    static clustering_key_prefix from_range(RangeOfSerializedComponents&& v) {\n        return clustering_key_prefix(compound::element_type::serialize_value(std::forward<RangeOfSerializedComponents>(v)));\n    }\n\n    clustering_key_prefix(std::vector<bytes> v)\n        : prefix_compound_wrapper(compound::element_type::serialize_value(std::move(v)))\n    { }\n    clustering_key_prefix(std::vector<managed_bytes> v)\n        : prefix_compound_wrapper(compound::element_type::serialize_value(std::move(v)))\n    { }\n    clustering_key_prefix(std::initializer_list<bytes> v) : clustering_key_prefix(std::vector(v)) {}\n\n    clustering_key_prefix(clustering_key_prefix&& v) = default;\n    clustering_key_prefix(const clustering_key_prefix& v) = default;\n    clustering_key_prefix(clustering_key_prefix& v) = default;\n    clustering_key_prefix& operator=(const clustering_key_prefix&) = default;\n    clustering_key_prefix& operator=(clustering_key_prefix&) = default;\n    clustering_key_prefix& operator=(clustering_key_prefix&&) = default;\n\n    clustering_key_prefix(clustering_key_prefix_view v)\n        : clustering_key_prefix(managed_bytes(v.representation()))\n    { }\n\n    using compound = lw_shared_ptr<compound_type<allow_prefixes::yes>>;\n\n    static clustering_key_prefix from_bytes(const managed_bytes& b) { return clustering_key_prefix(managed_bytes(b)); }\n    static clustering_key_prefix from_bytes(managed_bytes&& b) { return clustering_key_prefix(std::move(b)); }\n    static clustering_key_prefix from_bytes(managed_bytes_view b) { return clustering_key_prefix(managed_bytes(b)); }\n    static clustering_key_prefix from_bytes(bytes_view b) {\n        return clustering_key_prefix(managed_bytes(b));\n    }\n\n    static const compound& get_compound_type(const schema& s) {\n        return s.clustering_key_prefix_type();\n    }\n\n    static clustering_key_prefix from_clustering_prefix(const schema& s, const exploded_clustering_prefix& prefix) {\n        return from_exploded(s, prefix.components());\n    }\n\n    /* This function makes the passed clustering key full by filling its\n     * missing trailing components with empty values.\n     * This is used to represesent clustering keys of rows in compact tables that may be non-full.\n     * Returns whether a key wasn't full before the call.\n     */\n    static bool make_full(const schema& s, clustering_key_prefix& ck) {\n        if (!ck.is_full(s)) {\n            // TODO: avoid unnecessary copy here\n            auto full_ck_size = s.clustering_key_columns().size();\n            auto exploded = ck.explode(s);\n            exploded.resize(full_ck_size);\n            ck = clustering_key_prefix::from_exploded(std::move(exploded));\n            return true;\n        }\n        return false;\n    }\n};\n\ntemplate <>\nstruct fmt::formatter<clustering_key_prefix> : fmt::formatter<string_view> {\n    template <typename FormatContext>\n    auto format(const clustering_key_prefix& ckp, FormatContext& ctx) const {\n        return fmt::format_to(ctx.out(), \"ckp{{{}}}\", managed_bytes_view(ckp.representation()));\n    }\n};\n\ntemplate <>\nstruct fmt::formatter<clustering_key_prefix::with_schema_wrapper> : fmt::formatter<string_view> {\n    template <typename FormatContext>\n    auto format(const clustering_key_prefix::with_schema_wrapper& pk, FormatContext& ctx) const {\n        return ::detail::format_pk(pk, ctx);\n    }\n};\n\ntemplate<>\nstruct appending_hash<partition_key_view> {\n    template<typename Hasher>\n    void operator()(Hasher& h, const partition_key_view& pk, const schema& s) const {\n        for (managed_bytes_view v : pk.components(s)) {\n            ::feed_hash(h, v);\n        }\n    }\n};\n\ntemplate<>\nstruct appending_hash<partition_key> {\n    template<typename Hasher>\n    void operator()(Hasher& h, const partition_key& pk, const schema& s) const {\n        appending_hash<partition_key_view>()(h, pk.view(), s);\n    }\n};\n\ntemplate<>\nstruct appending_hash<clustering_key_prefix_view> {\n    template<typename Hasher>\n    void operator()(Hasher& h, const clustering_key_prefix_view& ck, const schema& s) const {\n        for (managed_bytes_view v : ck.components(s)) {\n            ::feed_hash(h, v);\n        }\n    }\n};\n\ntemplate<>\nstruct appending_hash<clustering_key_prefix> {\n    template<typename Hasher>\n    void operator()(Hasher& h, const clustering_key_prefix& ck, const schema& s) const {\n        appending_hash<clustering_key_prefix_view>()(h, ck.view(), s);\n    }\n};\n"
        },
        {
          "name": "lang",
          "type": "tree",
          "content": null
        },
        {
          "name": "licenses",
          "type": "tree",
          "content": null
        },
        {
          "name": "locator",
          "type": "tree",
          "content": null
        },
        {
          "name": "main.cc",
          "type": "blob",
          "size": 120.953125,
          "content": "/*\n * Copyright (C) 2014-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include <algorithm>\n#include <functional>\n#include <yaml-cpp/yaml.h>\n#include <fmt/ranges.h>\n\n#include <seastar/util/closeable.hh>\n#include <seastar/core/abort_source.hh>\n#include \"gms/inet_address.hh\"\n#include \"auth/allow_all_authenticator.hh\"\n#include \"auth/allow_all_authorizer.hh\"\n#include \"auth/maintenance_socket_role_manager.hh\"\n#include <seastar/core/future.hh>\n#include <seastar/core/signal.hh>\n#include <seastar/core/timer.hh>\n#include \"service/qos/raft_service_level_distributed_data_accessor.hh\"\n#include \"tasks/task_manager.hh\"\n#include \"utils/assert.hh\"\n#include \"utils/build_id.hh\"\n#include \"supervisor.hh\"\n#include \"replica/database.hh\"\n#include <seastar/core/reactor.hh>\n#include <seastar/core/app-template.hh>\n#include <seastar/core/distributed.hh>\n#include \"transport/server.hh\"\n#include <seastar/http/httpd.hh>\n#include \"api/api_init.hh\"\n#include \"db/config.hh\"\n#include \"db/extensions.hh\"\n#include \"db/legacy_schema_migrator.hh\"\n#include \"service/storage_service.hh\"\n#include \"service/migration_manager.hh\"\n#include \"service/tablet_allocator.hh\"\n#include \"service/load_meter.hh\"\n#include \"service/view_update_backlog_broker.hh\"\n#include \"service/qos/service_level_controller.hh\"\n#include \"streaming/stream_session.hh\"\n#include \"db/system_keyspace.hh\"\n#include \"db/system_distributed_keyspace.hh\"\n#include \"db/batchlog_manager.hh\"\n#include \"db/commitlog/commitlog.hh\"\n#include \"db/hints/manager.hh\"\n#include \"db/commitlog/commitlog_replayer.hh\"\n#include \"db/view/view_builder.hh\"\n#include \"utils/class_registrator.hh\"\n#include \"utils/error_injection.hh\"\n#include \"utils/runtime.hh\"\n#include \"utils/log.hh\"\n#include \"utils/directories.hh\"\n#include \"debug.hh\"\n#include \"auth/common.hh\"\n#include \"init.hh\"\n#include \"release.hh\"\n#include \"repair/repair.hh\"\n#include \"repair/row_level.hh\"\n#include <cstdio>\n#include <seastar/core/file.hh>\n#include <unistd.h>\n#include <sys/time.h>\n#include <sys/resource.h>\n#include <sys/prctl.h>\n#include \"tracing/tracing.hh\"\n#include <seastar/core/prometheus.hh>\n#include \"message/messaging_service.hh\"\n#include \"db/sstables-format-selector.hh\"\n#include \"db/snapshot-ctl.hh\"\n#include \"cql3/query_processor.hh\"\n#include <seastar/net/dns.hh>\n#include <seastar/core/io_queue.hh>\n#include <seastar/core/abort_on_ebadf.hh>\n#include <csignal>\n\n#include \"db/view/view_update_generator.hh\"\n#include \"service/cache_hitrate_calculator.hh\"\n#include \"compaction/compaction_manager.hh\"\n#include \"sstables/sstables.hh\"\n#include \"gms/feature_service.hh\"\n#include \"replica/distributed_loader.hh\"\n#include \"sstables_loader.hh\"\n#include \"cql3/cql_config.hh\"\n#include \"transport/controller.hh\"\n#include \"service/memory_limiter.hh\"\n#include \"service/endpoint_lifecycle_subscriber.hh\"\n#include \"db/schema_tables.hh\"\n\n#include \"redis/controller.hh\"\n#include \"cdc/log.hh\"\n#include \"cdc/cdc_extension.hh\"\n#include \"cdc/generation_service.hh\"\n#include \"tombstone_gc_extension.hh\"\n#include \"db/tags/extension.hh\"\n#include \"db/paxos_grace_seconds_extension.hh\"\n#include \"service/qos/standard_service_level_distributed_data_accessor.hh\"\n#include \"service/storage_proxy.hh\"\n#include \"service/mapreduce_service.hh\"\n#include \"alternator/controller.hh\"\n#include \"alternator/ttl.hh\"\n#include \"tools/entry_point.hh\"\n#include \"test/perf/entry_point.hh\"\n#include \"db/per_partition_rate_limit_extension.hh\"\n#include \"lang/manager.hh\"\n#include \"sstables/sstables_manager.hh\"\n#include \"db/virtual_tables.hh\"\n\n#include \"service/raft/raft_group_registry.hh\"\n#include \"service/raft/raft_group0_client.hh\"\n#include \"service/raft/raft_group0.hh\"\n#include \"gms/gossip_address_map.hh\"\n#include \"utils/alien_worker.hh\"\n#include \"utils/advanced_rpc_compressor.hh\"\n#include \"utils/shared_dict.hh\"\n#include \"message/dictionary_service.hh\"\n\nseastar::metrics::metric_groups app_metrics;\n\nusing namespace std::chrono_literals;\n\nnamespace bpo = boost::program_options;\n\nlogging::logger diaglog(\"diagnostics\");\n\n// Must live in a seastar::thread\nclass stop_signal {\n    bool _caught = false;\n    condition_variable _cond;\n    sharded<abort_source> _abort_sources;\n    future<> _broadcasts_to_abort_sources_done = make_ready_future<>();\nprivate:\n    void signaled() {\n        if (_caught) {\n            return;\n        }\n        _caught = true;\n        _cond.broadcast();\n        _broadcasts_to_abort_sources_done = _broadcasts_to_abort_sources_done.then([this] {\n            return _abort_sources.invoke_on_all(&abort_source::request_abort);\n        });\n    }\npublic:\n    stop_signal() {\n        _abort_sources.start().get();\n        handle_signal(SIGINT, [this] { signaled(); });\n        handle_signal(SIGTERM, [this] { signaled(); });\n    }\n    ~stop_signal() {\n        // There's no way to unregister a handler yet, so register a no-op handler instead.\n        handle_signal(SIGINT, [] {});\n        handle_signal(SIGTERM, [] {});\n        _broadcasts_to_abort_sources_done.get();\n        _abort_sources.stop().get();\n    }\n    future<> wait() {\n        return _cond.wait([this] { return _caught; });\n    }\n    bool stopping() const {\n        return _caught;\n    }\n    abort_source& as_local_abort_source() { return _abort_sources.local(); }\n    sharded<abort_source>& as_sharded_abort_source() { return _abort_sources; }\n};\n\nstruct object_storage_endpoint_param {\n    sstring endpoint;\n    s3::endpoint_config config;\n};\n\nnamespace YAML {\ntemplate<>\nstruct convert<::object_storage_endpoint_param> {\n    static bool decode(const Node& node, ::object_storage_endpoint_param& ep) {\n        ep.endpoint = node[\"name\"].as<std::string>();\n        ep.config.port = node[\"port\"].as<unsigned>();\n        ep.config.use_https = node[\"https\"].as<bool>(false);\n        if (node[\"aws_region\"] || std::getenv(\"AWS_DEFAULT_REGION\")) {\n            ep.config.aws.emplace();\n\n            // https://github.com/scylladb/scylla-pkg/issues/3845\n            // Allow picking up aws values via standard env vars as well.\n            // Value in config has prio, but fall back to env.\n            // This has the added benefit of potentially reducing the amount of\n            // sensitive data in config files (i.e. credentials)\n            auto get_node_value_or_env = [&](const char* key, const char* var) {\n                auto child = node[key];\n                if (child) {\n                    return child.as<std::string>();\n                }\n                auto val = std::getenv(var);\n                if (val) {\n                    return std::string(val);\n                }\n                return std::string{};\n            };\n            ep.config.aws->region = get_node_value_or_env(\"aws_region\", \"AWS_DEFAULT_REGION\");\n            ep.config.aws->access_key_id = get_node_value_or_env(\"aws_access_key_id\", \"AWS_ACCESS_KEY_ID\");\n            ep.config.aws->secret_access_key = get_node_value_or_env(\"aws_secret_access_key\", \"AWS_SECRET_ACCESS_KEY\");\n            ep.config.aws->session_token = get_node_value_or_env(\"aws_session_token\", \"AWS_SESSION_TOKEN\");\n        }\n        return true;\n    }\n};\n}\n\nstatic future<> read_object_storage_config(db::config& db_cfg) {\n    sstring cfg_name;\n    if (!db_cfg.object_storage_config_file().empty()) {\n        cfg_name = db_cfg.object_storage_config_file();\n    } else {\n        cfg_name = db::config::get_conf_sub(\"object_storage.yaml\").native();\n        if (!co_await file_accessible(cfg_name, access_flags::exists)) {\n            co_return;\n        }\n    }\n\n    auto cfg_file = co_await open_file_dma(cfg_name, open_flags::ro);\n    sstring data;\n    std::exception_ptr ex;\n\n    try {\n        auto sz = co_await cfg_file.size();\n        data = seastar::to_sstring(co_await cfg_file.dma_read_exactly<char>(0, sz));\n    } catch (...) {\n        ex = std::current_exception();\n    }\n    co_await cfg_file.close();\n    if (ex) {\n        co_await coroutine::return_exception_ptr(ex);\n    }\n\n    std::unordered_map<sstring, s3::endpoint_config> cfg;\n    YAML::Node doc = YAML::Load(data.c_str());\n    for (auto&& section : doc) {\n        auto sec_name = section.first.as<std::string>();\n        if (sec_name != \"endpoints\") {\n            co_await coroutine::return_exception(std::runtime_error(fmt::format(\"While parsing object_storage config: section {} currently unsupported.\", sec_name)));\n        }\n\n        auto endpoints = section.second.as<std::vector<object_storage_endpoint_param>>();\n        for (auto&& ep : endpoints) {\n            cfg[ep.endpoint] = std::move(ep.config);\n        }\n    }\n\n    db_cfg.object_storage_config.set(std::move(cfg));\n}\n\nstatic future<>\nread_config(bpo::variables_map& opts, db::config& cfg) {\n    sstring file;\n\n    if (opts.contains(\"options-file\")) {\n        file = opts[\"options-file\"].as<sstring>();\n    } else {\n        file = db::config::get_conf_sub(\"scylla.yaml\").string();\n    }\n    try {\n        co_await check_direct_io_support(file);\n        co_await cfg.read_from_file(file, [](auto & opt, auto & msg, auto status) {\n            auto level = log_level::warn;\n            if (auto value = status.value_or(db::config::value_status::Invalid);\n                value != db::config::value_status::Invalid && value != db::config::value_status::Deprecated) {\n                level = log_level::error;\n            }\n            startlog.log(level, \"{} : {}\", msg, opt);\n        });\n        co_await read_object_storage_config(cfg);\n    } catch (...) {\n        auto ep = std::current_exception();\n        startlog.error(\"Could not read configuration file {}: {}\", file, ep);\n        std::rethrow_exception(ep);\n    }\n}\n\n#ifdef SCYLLA_ENABLE_ERROR_INJECTION\nstatic future<>\nenable_initial_error_injections(const db::config& cfg) {\n    return smp::invoke_on_all([&cfg] {\n        auto& injector = utils::get_local_injector();\n        for (const auto& inj : cfg.error_injections_at_startup()) {\n            injector.enable(inj.name, inj.one_shot, inj.parameters);\n        }\n    });\n}\n#endif\n\n// Handles SIGHUP, using it to trigger re-reading of the configuration file. Should\n// only be constructed on shard 0.\nclass sighup_handler {\n    bpo::variables_map& _opts;\n    db::config& _cfg;\n    condition_variable _cond;\n    bool _pending = false; // if asked to reread while already reading\n    bool _stopping = false;\n    future<> _done = do_work();  // Launch main work loop, capture completion future\npublic:\n    // Installs the signal handler. Must call stop() (and wait for it) before destruction.\n    sighup_handler(bpo::variables_map& opts, db::config& cfg) : _opts(opts), _cfg(cfg) {\n        startlog.info(\"installing SIGHUP handler\");\n        handle_signal(SIGHUP, [this] { reread_config(); });\n    }\nprivate:\n    void reread_config() {\n        if (_stopping) {\n            return;\n        }\n        _pending = true;\n        _cond.broadcast();\n    }\n    // Main work loop. Waits for either _stopping or _pending to be raised, and\n    // re-reads the configuration file if _pending. We use a repeat loop here to\n    // avoid having multiple reads of the configuration file happening in parallel\n    // (this can cause an older read to overwrite the results of a younger read).\n    future<> do_work() {\n        return repeat([this] {\n            return _cond.wait([this] { return _pending || _stopping; }).then([this] {\n                return async([this] {\n                    if (_stopping) {\n                        return stop_iteration::yes;\n                    } else if (_pending) {\n                        _pending = false;\n                        try {\n                            startlog.info(\"re-reading configuration file\");\n                            read_config(_opts, _cfg).get();\n                            _cfg.broadcast_to_all_shards().get();\n                            startlog.info(\"completed re-reading configuration file\");\n                        } catch (...) {\n                            startlog.error(\"failed to re-read configuration file: {}\", std::current_exception());\n                        }\n                    }\n                    return stop_iteration::no;\n                });\n            });\n        });\n    }\npublic:\n    // Signals the main work loop to stop, and waits for it (and any in-progress work)\n    // to complete. After this is waited for, the object can be destroyed.\n    future<> stop() {\n        // No way to unregister yet\n        handle_signal(SIGHUP, [] {});\n        _pending = false;\n        _stopping = true;\n        _cond.broadcast();\n        return std::move(_done);\n    }\n};\n\nclass sigquit_handler {\n    bool _stopping = false;\n    bool _pending = false;\n    condition_variable _cond;\n    future<> _done;\n\n    sharded<replica::database>& _db;\n\n    future<> execution_loop() {\n        while (!_stopping) {\n            co_await _cond.wait([this] { return _pending || _stopping; });\n\n            if (_stopping) {\n                break;\n            }\n            _pending = false;\n\n            try {\n                co_await _db.invoke_on_all([] (replica::database& db) -> future<> {\n                    diaglog.info(\"Diagnostics dump requested via SIGQUIT:\\n{}\", memory::generate_memory_diagnostics_report());\n\n                    co_await db.foreach_reader_concurrency_semaphore([] (reader_concurrency_semaphore& semaphore) {\n                        diaglog.info(\"Diagnostics dump requested via SIGQUIT:\\n{}\", semaphore.dump_diagnostics());\n                        return make_ready_future<>();\n                    });\n                });\n            } catch (...) {\n                diaglog.error(\"Failed to dump diagnostics: {}\", std::current_exception());\n            }\n        }\n    }\n\npublic:\n    explicit sigquit_handler(sharded<replica::database>& db)\n        : _done(execution_loop())\n        , _db(db)\n    {\n        handle_signal(SIGQUIT, [this] {\n            _pending = true;\n            _cond.broadcast();\n        });\n    }\n\n    ~sigquit_handler() {\n        handle_signal(SIGQUIT, [] {});\n        _pending = false;\n        _stopping = true;\n        _cond.broadcast();\n        _done.get();\n    }\n};\n\nstatic\nvoid\nadjust_and_verify_rlimit(bool developer_mode) {\n    struct rlimit lim;\n    int r = getrlimit(RLIMIT_NOFILE, &lim);\n    if (r == -1) {\n        throw std::system_error(errno, std::system_category());\n    }\n\n    // First, try to increase the soft limit to the hard limit\n    // Ref: http://0pointer.net/blog/file-descriptor-limits.html\n\n    if (lim.rlim_cur < lim.rlim_max) {\n        lim.rlim_cur = lim.rlim_max;\n        r = setrlimit(RLIMIT_NOFILE, &lim);\n        if (r == -1) {\n            startlog.warn(\"adjusting RLIMIT_NOFILE failed with {}\", std::system_error(errno, std::system_category()));\n        }\n    }\n\n    auto recommended = 200'000U;\n    auto min = 10'000U;\n    if (lim.rlim_cur < min) {\n        if (developer_mode) {\n            startlog.warn(\"NOFILE rlimit too low (recommended setting {}, minimum setting {};\"\n                          \" you may run out of file descriptors.\", recommended, min);\n        } else {\n            startlog.error(\"NOFILE rlimit too low (recommended setting {}, minimum setting {};\"\n                          \" refusing to start.\", recommended, min);\n            throw std::runtime_error(\"NOFILE rlimit too low\");\n        }\n    }\n}\n\nstatic bool cpu_sanity() {\n#if defined(__x86_64__) || defined(__i386__)\n    if (!__builtin_cpu_supports(\"sse4.2\") || !__builtin_cpu_supports(\"pclmul\")) {\n        std::cerr << \"Scylla requires a processor with SSE 4.2 and PCLMUL support\\n\";\n        return false;\n    }\n#endif\n    return true;\n}\n\nstatic void tcp_syncookies_sanity() {\n    try {\n        auto f = file_desc::open(\"/proc/sys/net/ipv4/tcp_syncookies\", O_RDONLY | O_CLOEXEC);\n        char buf[128] = {};\n        f.read(buf, 128);\n        if (sstring(buf) == \"0\\n\") {\n            startlog.warn(\"sysctl entry net.ipv4.tcp_syncookies is set to 0.\\n\"\n                          \"For better performance, set following parameter on sysctl is strongly recommended:\\n\"\n                          \"net.ipv4.tcp_syncookies=1\");\n        }\n    } catch (const std::system_error& e) {\n            startlog.warn(\"Unable to check if net.ipv4.tcp_syncookies is set {}\", e);\n    }\n}\n\nstatic void tcp_timestamps_sanity() {\n    try {\n        auto f = file_desc::open(\"/proc/sys/net/ipv4/tcp_timestamps\", O_RDONLY | O_CLOEXEC);\n        char buf[128] = {};\n        f.read(buf, 128);\n        if (sstring(buf) == \"0\\n\") {\n            startlog.warn(\"sysctl entry net.ipv4.tcp_timestamps is set to 0.\\n\"\n                          \"To performance suffer less in a presence of packet loss, set following parameter on sysctl is strongly recommended:\\n\"\n                          \"net.ipv4.tcp_timestamps=1\");\n        }\n    } catch (const std::system_error& e) {\n        startlog.warn(\"Unable to check if net.ipv4.tcp_timestamps is set {}\", e);\n    }\n}\n\nstatic void\nverify_seastar_io_scheduler(const boost::program_options::variables_map& opts, bool developer_mode) {\n    auto note_bad_conf = [developer_mode] (sstring cause) {\n        sstring msg = \"I/O Scheduler is not properly configured! This is a non-supported setup, and performance is expected to be unpredictably bad.\\n Reason found: \"\n                    + cause + \"\\n\"\n                    + \"To properly configure the I/O Scheduler, run the scylla_io_setup utility shipped with Scylla.\\n\";\n\n        sstring devmode_msg = msg + \"To ignore this, see the developer-mode configuration option.\";\n        if (developer_mode) {\n            startlog.warn(\"{}\", msg.c_str());\n        } else {\n            startlog.error(\"{}\", devmode_msg.c_str());\n            throw std::runtime_error(\"Bad I/O Scheduler configuration\");\n        }\n    };\n\n    if (!(opts.contains(\"io-properties\") || opts.contains(\"io-properties-file\"))) {\n        note_bad_conf(\"none of --io-properties and --io-properties-file are set.\");\n    }\n}\n\nstatic\nvoid\nverify_adequate_memory_per_shard(bool developer_mode) {\n    auto shard_mem = memory::stats().total_memory();\n    if (shard_mem >= (1 << 30)) {\n        return;\n    }\n    if (developer_mode) {\n        startlog.warn(\"Only {} MiB per shard; this is below the recommended minimum of 1 GiB/shard;\"\n                \" continuing since running in developer mode\", shard_mem >> 20);\n    } else {\n        startlog.error(\"Only {} MiB per shard; this is below the recommended minimum of 1 GiB/shard; terminating.\"\n                \"Configure more memory (--memory option) or decrease shard count (--smp option).\", shard_mem >> 20);\n        throw std::runtime_error(\"configuration (memory per shard too low)\");\n    }\n}\n\nclass memory_threshold_guard {\n    seastar::memory::scoped_large_allocation_warning_threshold _slawt;\npublic:\n    explicit memory_threshold_guard(size_t threshold) : _slawt(threshold)  {}\n    future<> stop() { return make_ready_future<>(); }\n};\n\n// Formats parsed program options into a string as follows:\n// \"[key1: value1_1 value1_2 ..., key2: value2_1 value 2_2 ..., (positional) value3, ...]\"\nstd::string format_parsed_options(const std::vector<bpo::option>& opts) {\n    return fmt::format(\"[{}]\",\n        fmt::join(opts | std::views::transform([] (const bpo::option& opt) {\n            if (opt.value.empty()) {\n                return opt.string_key;\n            }\n\n            return fmt::format(\"{}{}\",\n                opt.string_key.empty() ?  \"(positional) \" : fmt::format(\"{}: \", opt.string_key),\n                fmt::join(opt.value, \" \"));\n        }), \", \")\n    );\n}\n\nstatic constexpr char startup_msg[] = \"Scylla version {} with build-id {} starting ...\\n\";\n\nvoid print_starting_message(int ac, char** av, const bpo::parsed_options& opts) {\n    fmt::print(startup_msg, scylla_version(), get_build_id());\n    if (ac) {\n        fmt::print(\"command used: \\\"{}\", av[0]);\n        for (int i = 1; i < ac; ++i) {\n            fmt::print(\" {}\", av[i]);\n        }\n        fmt::print(\"\\\"\\n\");\n    }\n    fmt::print(\"pid: {}\\n\", getpid());\n\n    fmt::print(\"parsed command line options: {}\\n\", format_parsed_options(opts.options));\n}\n\ntemplate <typename Func>\nstatic auto defer_verbose_shutdown(const char* what, Func&& func) {\n    auto vfunc = [what, func = std::forward<Func>(func)] () mutable {\n        startlog.info(\"Shutting down {}\", what);\n        try {\n            func();\n            startlog.info(\"Shutting down {} was successful\", what);\n        } catch (...) {\n            auto ex = std::current_exception();\n            bool do_abort = true;\n            try {\n                std::rethrow_exception(ex);\n            } catch (const std::system_error& e) {\n                // System error codes we consider \"environmental\",\n                // i.e. not scylla's fault, therefore there is no point in\n                // aborting and dumping core.\n                for (int i : {EIO, EACCES, EDQUOT, ENOSPC}) {\n                    if (e.code() == std::error_code(i, std::system_category())) {\n                        do_abort = false;\n                        break;\n                    }\n                }\n            } catch (const storage_io_error& e) {\n                do_abort = false;\n            } catch (...) {\n            }\n            auto msg = fmt::format(\"Unexpected error shutting down {}: {}\", what, ex);\n            if (do_abort) {\n                startlog.error(\"{}: aborting\", msg);\n                abort();\n            } else {\n                startlog.error(\"{}: exiting, at {}\", msg, current_backtrace());\n\n                // Call _exit() rather than exit() to exit immediately\n                // without calling exit handlers, avoiding\n                // boost::intrusive::detail::destructor_impl SCYLLA_ASSERT failure\n                // from ~segment_pool exit handler.\n                _exit(255);\n            }\n        }\n    };\n\n    auto ret = deferred_action(std::move(vfunc));\n    return ::make_shared<decltype(ret)>(std::move(ret));\n}\n\nnamespace debug {\nsharded<netw::messaging_service>* the_messaging_service;\nsharded<cql3::query_processor>* the_query_processor;\nsharded<qos::service_level_controller>* the_sl_controller;\nsharded<service::migration_manager>* the_migration_manager;\nsharded<service::storage_service>* the_storage_service;\nsharded<streaming::stream_manager> *the_stream_manager;\nsharded<gms::feature_service> *the_feature_service;\nsharded<gms::gossiper> *the_gossiper;\nsharded<locator::snitch_ptr> *the_snitch;\nsharded<service::storage_proxy> *the_storage_proxy;\n}\n\n// This is used by perf-alternator to allow running scylla together with the tool\n// in a single process. So that it's easier to measure internals. It's not added\n// to main_func_type to not complicate common flow as no other tool needs such logic.\nstd::function<void(lw_shared_ptr<db::config>)> after_init_func;\n\nstatic locator::host_id initialize_local_info_thread(sharded<db::system_keyspace>& sys_ks,\n        sharded<locator::snitch_ptr>& snitch,\n        const gms::inet_address& listen_address,\n        const db::config& cfg,\n        gms::inet_address broadcast_address,\n        gms::inet_address broadcast_rpc_address)\n{\n    auto linfo = sys_ks.local().load_local_info().get();\n    if (linfo.cluster_name.empty()) {\n        linfo.cluster_name = cfg.cluster_name();\n    } else if (linfo.cluster_name != cfg.cluster_name()) {\n        throw exceptions::configuration_exception(\"Saved cluster name \" + linfo.cluster_name + \" != configured name \" + cfg.cluster_name());\n    }\n    if (!linfo.host_id) {\n        linfo.host_id = locator::host_id::create_random_id();\n        startlog.info(\"Setting local host id to {}\", linfo.host_id);\n    }\n\n    linfo.listen_address = listen_address;\n    const auto host_id = linfo.host_id;\n    sys_ks.local().save_local_info(std::move(linfo), snitch.local()->get_location(), broadcast_address, broadcast_rpc_address).get();\n    return host_id;\n}\n\nextern \"C\" void __attribute__((weak)) __llvm_profile_dump();\n\n[[gnu::noinline]]\nvoid dump_performance_profiles() {\n    if (__llvm_profile_dump) {\n        startlog.info(\"Calling __llvm_profile_dump()\");\n        __llvm_profile_dump();\n    }\n}\n\nstatic int scylla_main(int ac, char** av) {\n    // Allow core dumps. The would be disabled by default if\n    // CAP_SYS_NICE was added to the binary, as is suggested by the\n    // epoll backend.\n    int r = prctl(PR_SET_DUMPABLE, 1, 0, 0, 0);\n    if (r) {\n        std::cerr << \"Could not make scylla dumpable\\n\";\n        exit(1);\n    }\n\n  try {\n    runtime::init_uptime();\n    std::setvbuf(stdout, nullptr, _IOLBF, 1000);\n    app_template::seastar_options app_cfg;\n    app_cfg.name = \"Scylla\";\n    app_cfg.description =\nR\"(scylla - NoSQL data store using the seastar framework\n\nFor more information, see https://github.com/scylladb/scylla.\n\nThe scylla executable hosts tools in addition to the main scylla server, these\ncan be invoked as: scylla {tool_name} [...]\n\nFor a list of available tools, run: scylla --list-tools\nFor more information about individual tools, run: scylla {tool_name} --help\n\nTo start the scylla server proper, simply invoke as: scylla server (or just scylla).\n)\";\n#ifdef DEBUG\n    // Increase the task quota to improve work:poll ratio in slow debug mode.\n    app_cfg.reactor_opts.task_quota_ms.set_default_value(5);\n#else\n    app_cfg.reactor_opts.task_quota_ms.set_default_value(0.5);\n#endif\n    app_cfg.auto_handle_sigint_sigterm = false;\n    app_cfg.reactor_opts.max_networking_io_control_blocks.set_default_value(50000);\n    {\n        const auto candidates = app_cfg.reactor_opts.reactor_backend.get_candidate_names();\n\n        // We don't want ScyllaDB to run with the io_uring backend.\n        // So select the default reactor backend explicitly here.\n        if (std::ranges::contains(candidates, \"linux-aio\")) {\n            app_cfg.reactor_opts.reactor_backend.select_default_candidate(\"linux-aio\");\n        } else {\n            app_cfg.reactor_opts.reactor_backend.select_default_candidate(\"epoll\");\n        }\n\n        // Leave some reserve IOCBs for scylla-nodetool and other native tool apps.\n        if (std::ranges::contains(candidates, \"io_uring\")) {\n            app_cfg.reactor_opts.reserve_io_control_blocks.set_default_value(10);\n        } else {\n            startlog.warn(\"Need to leave extra IOCBs in reserve for tools because the io_uring reactor backend is not available.\"\n                    \" Tools will fall-back to the epoll reactor backend, which requires more IOCBs to function.\");\n            app_cfg.reactor_opts.reserve_io_control_blocks.set_default_value(1024);\n        }\n    }\n    // We need to have the entire app config to run the app, but we need to\n    // run the app to read the config file with UDF specific options so that\n    // we know whether we need to reserve additional memory for UDFs.\n    app_cfg.smp_opts.reserve_additional_memory_per_shard = db::config::wasm_udf_reserved_memory;\n    app_template app(std::move(app_cfg));\n\n    auto ext = std::make_shared<db::extensions>();\n    ext->add_schema_extension<db::tags_extension>(db::tags_extension::NAME);\n    ext->add_schema_extension<cdc::cdc_extension>(cdc::cdc_extension::NAME);\n    ext->add_schema_extension<db::paxos_grace_seconds_extension>(db::paxos_grace_seconds_extension::NAME);\n    ext->add_schema_extension<tombstone_gc_extension>(tombstone_gc_extension::NAME);\n    ext->add_schema_extension<db::per_partition_rate_limit_extension>(db::per_partition_rate_limit_extension::NAME);\n\n    auto cfg = make_lw_shared<db::config>(ext);\n    auto init = app.get_options_description().add_options();\n\n    init(\"version\", bpo::bool_switch(), \"print version number and exit\");\n    init(\"build-id\", bpo::bool_switch(), \"print build-id and exit\");\n    init(\"build-mode\", bpo::bool_switch(), \"print build mode and exit\");\n    init(\"list-tools\", bpo::bool_switch(), \"list included tools and exit\");\n\n    bpo::options_description deprecated_options(\"Deprecated options - ignored\");\n    auto deprecated_options_easy_init = deprecated_options.add_options();\n    cfg->add_deprecated_options(deprecated_options_easy_init);\n    app.get_options_description().add(deprecated_options);\n\n    // TODO : default, always read?\n    init(\"options-file\", bpo::value<sstring>(), \"configuration file (i.e. <SCYLLA_HOME>/conf/scylla.yaml)\");\n\n    configurable::append_all(*cfg, init);\n    cfg->add_options(init);\n\n    // If --version is requested, print it out and exit immediately to avoid\n    // Seastar-specific warnings that may occur when running the app\n    if (!isatty(fileno(stdin))) {\n        auto parsed_opts = bpo::command_line_parser(ac, av).options(app.get_options_description()).allow_unregistered().run();\n        print_starting_message(ac, av, parsed_opts);\n    }\n\n    sharded<locator::shared_token_metadata> token_metadata;\n    sharded<locator::effective_replication_map_factory> erm_factory;\n    sharded<service::migration_notifier> mm_notifier;\n    sharded<service::endpoint_lifecycle_notifier> lifecycle_notifier;\n    sharded<compaction_manager> cm;\n    sharded<sstables::storage_manager> sstm;\n    distributed<replica::database> db;\n    seastar::sharded<service::cache_hitrate_calculator> cf_cache_hitrate_calculator;\n    service::load_meter load_meter;\n    sharded<service::storage_proxy> proxy;\n    sharded<service::storage_service> ss;\n    sharded<service::migration_manager> mm;\n    sharded<tasks::task_manager> task_manager;\n    api::http_context ctx(db);\n    httpd::http_server_control prometheus_server;\n    std::optional<utils::directories> dirs = {};\n    sharded<gms::feature_service> feature_service;\n    sharded<db::snapshot_ctl> snapshot_ctl;\n    sharded<netw::messaging_service> messaging;\n    sharded<cql3::query_processor> qp;\n    sharded<db::batchlog_manager> bm;\n    sharded<sstables::directory_semaphore> sst_dir_semaphore;\n    sharded<service::raft_group_registry> raft_gr;\n    sharded<service::memory_limiter> service_memory_limiter;\n    sharded<repair_service> repair;\n    sharded<sstables_loader> sst_loader;\n    sharded<streaming::stream_manager> stream_manager;\n    sharded<service::mapreduce_service> mapreduce_service;\n    sharded<gms::gossiper> gossiper;\n    sharded<locator::snitch_ptr> snitch;\n\n    // This worker wasn't designed to be used from multiple threads.\n    // If you are attempting to do that, make sure you know what you are doing.\n    // (It uses a std::mutex-based queue under the hood, so it may cause\n    // performance problems under contention).\n    //\n    // Note: we are creating this thread before app.run so that it doesn't\n    // inherit Seastar's CPU affinity masks. We want this thread to be free\n    // to migrate between CPUs; we think that's what makes the most sense.\n    auto rpc_dict_training_worker = utils::alien_worker(startlog, 19);\n\n    return app.run(ac, av, [&] () -> future<int> {\n\n        auto&& opts = app.configuration();\n\n        namespace sm = seastar::metrics;\n        app_metrics.add_group(\"scylladb\", {\n            sm::make_gauge(\"current_version\", sm::description(\"Current ScyllaDB version.\"), { sm::label_instance(\"version\", scylla_version()), sm::shard_label(\"\") }, [] { return 0; })\n        });\n\n        for (auto& opt: deprecated_options.options()) {\n            if (opts.contains(opt->long_name())) {\n                startlog.warn(\"{} option ignored (deprecated)\", opt->long_name());\n            }\n        }\n\n        // Check developer mode before even reading the config file, because we may not be\n        // able to read it if we need to disable strict dma mode.\n        // We'll redo this later and apply it to all reactors.\n        if (opts.contains(\"developer-mode\")) {\n            engine().set_strict_dma(false);\n        }\n\n        tcp_syncookies_sanity();\n        tcp_timestamps_sanity();\n\n        return seastar::async([&app, cfg, ext, &cm, &sstm, &db, &qp, &bm, &proxy, &mapreduce_service, &mm, &mm_notifier, &ctx, &opts, &dirs,\n                &prometheus_server, &cf_cache_hitrate_calculator, &load_meter, &feature_service, &gossiper, &snitch,\n                &token_metadata, &erm_factory, &snapshot_ctl, &messaging, &sst_dir_semaphore, &raft_gr, &service_memory_limiter,\n                &repair, &sst_loader, &ss, &lifecycle_notifier, &stream_manager, &task_manager, &rpc_dict_training_worker] {\n          try {\n              if (opts.contains(\"relabel-config-file\") && !opts[\"relabel-config-file\"].as<sstring>().empty()) {\n                  // calling update_relabel_config_from_file can cause an exception that would stop startup\n                  // that's on purpose, it means the configuration is broken and needs to be fixed\n                  utils::update_relabel_config_from_file(opts[\"relabel-config-file\"].as<sstring>()).get();\n              }\n            // disable reactor stall detection during startup\n            auto blocked_reactor_notify_ms = engine().get_blocked_reactor_notify_ms();\n            smp::invoke_on_all([] {\n                engine().update_blocked_reactor_notify_ms(10000h);\n            }).get();\n\n            ::stop_signal stop_signal; // we can move this earlier to support SIGINT during initialization\n            read_config(opts, *cfg).get();\n#ifdef SCYLLA_ENABLE_ERROR_INJECTION\n            enable_initial_error_injections(*cfg).get();\n#endif\n            auto notify_set = configurable::init_all(opts, *cfg, *ext, service_set(\n                db, ss, mm, proxy, feature_service, messaging, qp, bm\n            )).get();\n\n            auto stop_configurables = defer_verbose_shutdown(\"configurables\", [&] {\n                notify_set.notify_all(configurable::system_state::stopped).get();\n            });\n\n            cfg->setup_directories();\n\n            // We're writing to a non-atomic variable here. But bool writes are atomic\n            // in all supported architectures, and the broadcast_to_all_shards().get() below\n            // will apply the required memory barriers anyway.\n            ser::gc_clock_using_3_1_0_serialization = cfg->enable_3_1_0_compatibility_mode();\n\n            cfg->broadcast_to_all_shards().get();\n\n            // We pass this piece of config through a global as a temporary hack.\n            // See the comment at the definition of sstables::global_cache_index_pages.\n            smp::invoke_on_all([&cfg] {\n                sstables::global_cache_index_pages = cfg->cache_index_pages.operator utils::updateable_value<bool>();\n            }).get();\n\n            ::sighup_handler sighup_handler(opts, *cfg);\n            auto stop_sighup_handler = defer_verbose_shutdown(\"sighup\", [&] {\n                sighup_handler.stop().get();\n            });\n\n            logalloc::prime_segment_pool(memory::stats().total_memory(), memory::min_free_memory()).get();\n            logging::apply_settings(cfg->logging_settings(app.options().log_opts));\n\n            startlog.info(startup_msg, scylla_version(), get_build_id());\n\n            // Set the default scheduling_group, i.e., the main scheduling\n            // group to a lower shares. Subsystems needs higher shares\n            // should set it explicitly. This prevents code that is supposed to\n            // run inside its own scheduling group leaking to main group and\n            // causing latency issues.\n            smp::invoke_on_all([] {\n                auto default_sg = default_scheduling_group();\n                default_sg.set_shares(200);\n            }).get();\n\n            adjust_and_verify_rlimit(cfg->developer_mode());\n            verify_adequate_memory_per_shard(cfg->developer_mode());\n            verify_seastar_io_scheduler(opts, cfg->developer_mode());\n            if (cfg->partitioner() != \"org.apache.cassandra.dht.Murmur3Partitioner\") {\n                if (cfg->enable_deprecated_partitioners()) {\n                    startlog.warn(\"The partitioner {} is deprecated and will be removed in a future version.\"\n                            \"  Contact scylladb-users@googlegroups.com if you are using it in production\", cfg->partitioner());\n                } else {\n                    startlog.error(\"The partitioner {} is deprecated and will be removed in a future version.\"\n                            \"  To enable it, add \\\"enable_deprecated_partitioners: true\\\" to scylla.yaml\"\n                            \"  Contact scylladb-users@googlegroups.com if you are using it in production\", cfg->partitioner());\n                    throw bad_configuration_error();\n                }\n            }\n\n            auto unused_features = cfg->experimental_features() | std::views::filter([] (auto& f) {\n                return f == db::experimental_features_t::feature::UNUSED;\n            });\n            if (!unused_features.empty()) {\n                startlog.warn(\"Ignoring unused features found in config: {}\", unused_features);\n            }\n\n            gms::feature_config fcfg = gms::feature_config_from_db_config(*cfg);\n\n            debug::the_feature_service = &feature_service;\n            feature_service.start(fcfg).get();\n            // FIXME storage_proxy holds a reference on it and is not yet stopped.\n            // also the proxy leaves range_slice_read_executor-s hanging around\n            // and willing to find out if the cluster_supports_digest_multipartition_reads\n            //\n            //auto stop_feature_service = defer_verbose_shutdown(\"feature service\", [&feature_service] {\n            //    feature_service.stop().get();\n            //});\n\n            schema::set_default_partitioner(cfg->partitioner(), cfg->murmur3_partitioner_ignore_msb_bits());\n            auto make_sched_group = [&] (sstring name, sstring short_name, unsigned shares) {\n                if (cfg->cpu_scheduler()) {\n                    return seastar::create_scheduling_group(name, short_name, shares).get();\n                } else {\n                    return seastar::scheduling_group();\n                }\n            };\n            auto background_reclaim_scheduling_group = make_sched_group(\"background_reclaim\", \"bgre\", 50);\n            auto maintenance_scheduling_group = make_sched_group(\"streaming\", \"strm\", 200);\n\n            smp::invoke_on_all([&cfg, background_reclaim_scheduling_group] {\n                logalloc::tracker::config st_cfg;\n                st_cfg.defragment_on_idle = cfg->defragment_memory_on_idle();\n                st_cfg.abort_on_lsa_bad_alloc = cfg->abort_on_lsa_bad_alloc();\n                st_cfg.lsa_reclamation_step = cfg->lsa_reclamation_step();\n                st_cfg.background_reclaim_sched_group = background_reclaim_scheduling_group;\n                st_cfg.sanitizer_report_backtrace = cfg->sanitizer_report_backtrace();\n                logalloc::shard_tracker().configure(st_cfg);\n            }).get();\n\n            auto stop_lsa_background_reclaim = defer([&] () noexcept {\n                smp::invoke_on_all([&] {\n                    return logalloc::shard_tracker().stop();\n                }).get();\n            });\n\n            if (cfg->broadcast_address().empty() && cfg->listen_address().empty()) {\n                startlog.error(\"Bad configuration: neither listen_address nor broadcast_address are defined\\n\");\n                throw bad_configuration_error();\n            }\n\n            if (cfg->broadcast_rpc_address().empty() && cfg->rpc_address() == \"0.0.0.0\") {\n                startlog.error(\"If rpc_address is set to a wildcard address {}, then you must set broadcast_rpc_address\", cfg->rpc_address());\n                throw bad_configuration_error();\n            }\n\n            // We want to ensure a node is zero-token if and only if join_ring=false, so all the logic can rely on it.\n            if (cfg->join_ring() && cfg->num_tokens() == 0 && cfg->initial_token().empty()) {\n                startlog.error(\n                        \"Bad configuration: cannot join the ring with zero tokens. If you do not want the node \"\n                        \"to join the ring, use join-ring=false\");\n                throw bad_configuration_error();\n            }\n\n            auto preferred = cfg->listen_interface_prefer_ipv6() ? std::make_optional(net::inet_address::family::INET6) : std::nullopt;\n            auto family = cfg->enable_ipv6_dns_lookup() || preferred ? std::nullopt : std::make_optional(net::inet_address::family::INET);\n\n            auto broadcast_addr = utils::resolve(cfg->broadcast_address || cfg->listen_address, family, preferred).get();\n            auto broadcast_rpc_addr = utils::resolve(cfg->broadcast_rpc_address || cfg->rpc_address, family, preferred).get();\n\n            ctx.api_dir = cfg->api_ui_dir();\n            if (!ctx.api_dir.empty() && ctx.api_dir.back() != '/') {\n                // The api_dir should end with a backslash, add it if it's missing\n                ctx.api_dir.append(\"/\", 1);\n            }\n            ctx.api_doc = cfg->api_doc_dir();\n            if (!ctx.api_doc.empty() && ctx.api_doc.back() != '/') {\n                // The api_doc should end with a backslash, add it if it's missing\n                ctx.api_doc.append(\"/\", 1);\n            }\n            const auto hinted_handoff_enabled = cfg->hinted_handoff_enabled();\n\n            auto api_addr = utils::resolve(cfg->api_address || cfg->rpc_address, family, preferred).get();\n            supervisor::notify(\"starting API server\");\n            ctx.http_server.start(\"API\").get();\n            auto stop_http_server = defer_verbose_shutdown(\"API server\", [&ctx] {\n                ctx.http_server.stop().get();\n            });\n            api::set_server_init(ctx).get();\n\n            supervisor::notify(\"starting prometheus API server\");\n            std::any stop_prometheus;\n            if (cfg->prometheus_port()) {\n                prometheus_server.start(\"prometheus\").get();\n                stop_prometheus = defer_verbose_shutdown(\"prometheus API server\", [&prometheus_server] {\n                    prometheus_server.stop().get();\n                });\n\n                auto ip = utils::resolve(cfg->prometheus_address || cfg->listen_address, family, preferred).get();\n\n                prometheus::config pctx;\n                pctx.metric_help = \"Scylla server statistics\";\n                pctx.prefix = cfg->prometheus_prefix();\n                pctx.allow_protobuf = cfg->prometheus_allow_protobuf();\n                prometheus::start(prometheus_server, pctx).get();\n                with_scheduling_group(maintenance_scheduling_group, [&] {\n                  return prometheus_server.listen(socket_address{ip, cfg->prometheus_port()}).handle_exception([&ip, &cfg] (auto ep) {\n                    startlog.error(\"Could not start Prometheus API server on {}:{}: {}\", ip, cfg->prometheus_port(), ep);\n                    return make_exception_future<>(ep);\n                  });\n                }).get();\n            }\n\n            using namespace locator;\n            // Re-apply strict-dma after we've read the config file, this time\n            // to all reactors\n            if (opts.contains(\"developer-mode\")) {\n                smp::invoke_on_all([] { engine().set_strict_dma(false); }).get();\n            }\n\n            auto abort_on_internal_error_observer = cfg->abort_on_internal_error.observe([] (bool val) {\n                set_abort_on_internal_error(val);\n            });\n            set_abort_on_internal_error(cfg->abort_on_internal_error());\n\n            supervisor::notify(\"creating snitch\");\n            debug::the_snitch = &snitch;\n            snitch_config snitch_cfg;\n            snitch_cfg.name = cfg->endpoint_snitch();\n            snitch_cfg.listen_address = utils::resolve(cfg->listen_address, family).get();\n            snitch_cfg.broadcast_address = broadcast_addr;\n            snitch.start(snitch_cfg).get();\n            auto stop_snitch = defer_verbose_shutdown(\"snitch\", [&snitch] {\n                snitch.stop().get();\n            });\n            snitch.invoke_on_all(&locator::snitch_ptr::start).get();\n            // #293 - do not stop anything (unless snitch.on_all(start) fails)\n            stop_snitch->cancel();\n\n            api::set_server_snitch(ctx, snitch).get();\n            auto stop_snitch_api = defer_verbose_shutdown(\"snitch API\", [&ctx] {\n                api::unset_server_snitch(ctx).get();\n            });\n\n            if (auto opt_public_address = snitch.local()->get_public_address()) {\n                // Use the Public IP as broadcast_address to other nodes\n                // and the broadcast_rpc_address (for client CQL connections).\n                //\n                // Cassandra 2.1 manual explicitly instructs to set broadcast_address\n                // value to a public address in cassandra.yaml.\n                //\n                broadcast_addr = *opt_public_address;\n                if (cfg->broadcast_rpc_address().empty()) {\n                    broadcast_rpc_addr = *opt_public_address;\n                }\n            }\n\n            supervisor::notify(\"starting tokens manager\");\n            locator::token_metadata::config tm_cfg;\n            tm_cfg.topo_cfg.this_endpoint = broadcast_addr;\n            tm_cfg.topo_cfg.this_cql_address = broadcast_rpc_addr;\n            tm_cfg.topo_cfg.local_dc_rack = snitch.local()->get_location();\n            if (snitch.local()->get_name() == \"org.apache.cassandra.locator.SimpleSnitch\") {\n                //\n                // Simple snitch wants sort_by_proximity() not to reorder nodes anyhow\n                //\n                // \"Making all endpoints equal ensures we won't change the original\n                // ordering.\" - quote from C* code.\n                //\n                // The snitch_base implementation should handle the above case correctly.\n                // I'm leaving the this implementation anyway since it's the C*'s\n                // implementation and some installations may depend on it.\n                //\n                tm_cfg.topo_cfg.disable_proximity_sorting = true;\n            }\n            token_metadata.start([] () noexcept { return db::schema_tables::hold_merge_lock(); }, tm_cfg).get();\n            token_metadata.invoke_on_all([&] (auto& stm) {\n                stm.set_stall_detector_threshold(\n                        std::chrono::duration_cast<std::chrono::steady_clock::duration>(\n                                std::chrono::duration<double>(cfg->topology_barrier_stall_detector_threshold_seconds())));\n            }).get();\n            // storage_proxy holds a reference on it and is not yet stopped.\n            // what's worse is that the calltrace\n            //   storage_proxy::do_query \n            //                ::query_partition_key_range\n            //                ::query_partition_key_range_concurrent\n            // leaves unwaited futures on the reactor and once it gets there\n            // the token_metadata instance is accessed and ...\n            //\n            //auto stop_token_metadata = defer_verbose_shutdown(\"token metadata\", [ &token_metadata ] {\n            //    token_metadata.stop().get();\n            //});\n\n            api::set_server_token_metadata(ctx, token_metadata).get();\n            auto stop_tokens_api = defer_verbose_shutdown(\"token metadata API\", [&ctx] {\n                api::unset_server_token_metadata(ctx).get();\n            });\n\n\n            supervisor::notify(\"starting effective_replication_map factory\");\n            erm_factory.start().get();\n            auto stop_erm_factory = deferred_stop(erm_factory);\n\n            supervisor::notify(\"starting migration manager notifier\");\n            mm_notifier.start().get();\n            auto stop_mm_notifier = defer_verbose_shutdown(\"migration manager notifier\", [ &mm_notifier ] {\n                mm_notifier.stop().get();\n            });\n\n            supervisor::notify(\"starting per-shard database core\");\n\n            sst_dir_semaphore.start(cfg->initial_sstable_loading_concurrency()).get();\n            auto stop_sst_dir_sem = defer_verbose_shutdown(\"sst_dir_semaphore\", [&sst_dir_semaphore] {\n                sst_dir_semaphore.stop().get();\n            });\n\n            service_memory_limiter.start(memory::stats().total_memory()).get();\n            auto stop_mem_limiter = defer_verbose_shutdown(\"service_memory_limiter\", [] {\n                // Uncomment this once services release all the memory on stop\n                // service_memory_limiter.stop().get();\n            });\n\n            supervisor::notify(\"creating and verifying directories\");\n            utils::directories::set dir_set;\n            dir_set.add(cfg->commitlog_directory());\n            dir_set.add(cfg->schema_commitlog_directory());\n            dirs.emplace(cfg->developer_mode());\n            dirs->create_and_verify(std::move(dir_set)).get();\n\n            // The data directories are handled separately to prevent memory\n            // fragmentation in the dentry/inode cache. The top-level\n            // directories are verified first, and then the contents are\n            // verified in such a way that the files that will be closed\n            // immediately are located separately from those that will remain\n            // open for a longer duration in the dentry/inode cache.\n            utils::directories::set data_dir_set;\n            data_dir_set.add(cfg->data_file_directories());\n            dirs->create_and_verify(data_dir_set, utils::directories::recursive::no).get();\n            utils::directories::verify_owner_and_mode_of_data_dir(std::move(data_dir_set)).get();\n\n            auto hints_dir_initializer = db::hints::directory_initializer::make(*dirs, cfg->hints_directory()).get();\n            auto view_hints_dir_initializer = db::hints::directory_initializer::make(*dirs, cfg->view_hints_directory()).get();\n            if (!hinted_handoff_enabled.is_disabled_for_all()) {\n                hints_dir_initializer.ensure_created_and_verified().get();\n            }\n            view_hints_dir_initializer.ensure_created_and_verified().get();\n\n            auto get_tm_cfg = sharded_parameter([&] {\n                return tasks::task_manager::config {\n                    .task_ttl = cfg->task_ttl_seconds,\n                    .user_task_ttl = cfg->user_task_ttl_seconds,\n                    .broadcast_address = broadcast_addr\n                };\n            });\n            task_manager.start(std::move(get_tm_cfg), std::ref(stop_signal.as_sharded_abort_source())).get();\n            auto stop_task_manager = defer_verbose_shutdown(\"task_manager\", [&task_manager] {\n                task_manager.stop().get();\n            });\n\n            api::set_server_task_manager(ctx, task_manager, cfg).get();\n            auto stop_tm_api = defer_verbose_shutdown(\"task manager API\", [&ctx] {\n                api::unset_server_task_manager(ctx).get();\n            });\n#ifndef SCYLLA_BUILD_MODE_RELEASE\n            api::set_server_task_manager_test(ctx, task_manager).get();\n            auto stop_tm_test_api = defer_verbose_shutdown(\"task manager API\", [&ctx] {\n                api::unset_server_task_manager_test(ctx).get();\n            });\n#endif\n\n            // Note: changed from using a move here, because we want the config object intact.\n            replica::database_config dbcfg;\n            dbcfg.compaction_scheduling_group = make_sched_group(\"compaction\", \"comp\", 1000);\n            dbcfg.memory_compaction_scheduling_group = make_sched_group(\"mem_compaction\", \"mcmp\", 1000);\n            dbcfg.streaming_scheduling_group = maintenance_scheduling_group;\n            dbcfg.statement_scheduling_group = make_sched_group(\"statement\", \"stmt\", 1000);\n            dbcfg.memtable_scheduling_group = make_sched_group(\"memtable\", \"mt\", 1000);\n            dbcfg.memtable_to_cache_scheduling_group = make_sched_group(\"memtable_to_cache\", \"mt2c\", 200);\n            dbcfg.gossip_scheduling_group = make_sched_group(\"gossip\", \"gms\", 1000);\n            dbcfg.commitlog_scheduling_group = make_sched_group(\"commitlog\", \"clog\", 1000);\n            dbcfg.schema_commitlog_scheduling_group = make_sched_group(\"schema_commitlog\", \"sclg\", 1000);\n            dbcfg.available_memory = memory::stats().total_memory();\n\n            // Make sure to initialize the scheduling group keys at a point where we are sure\n            // that nobody will be creating scheduling groups (e.g. service levels controller can do that).\n            // Creating a scheduling group and scheduling group key concurrently\n            // doesn't work properly. See: https://github.com/scylladb/seastar/issues/2231\n            scheduling_group_key_config maintenance_cql_sg_stats_cfg =\n            make_scheduling_group_key_config<cql_transport::cql_sg_stats>(maintenance_socket_enabled::yes);\n            auto maintenance_cql_sg_stats_key = scheduling_group_key_create(maintenance_cql_sg_stats_cfg).get();\n            scheduling_group_key_config cql_sg_stats_cfg = make_scheduling_group_key_config<cql_transport::cql_sg_stats>(maintenance_socket_enabled::no);\n            auto cql_sg_stats_key = scheduling_group_key_create(cql_sg_stats_cfg).get();\n\n            supervisor::notify(\"starting compaction_manager\");\n            // get_cm_cfg is called on each shard when starting a sharded<compaction_manager>\n            // we need the getter since updateable_value is not shard-safe (#7316)\n            auto get_cm_cfg = sharded_parameter([&] {\n                return compaction_manager::config {\n                    .compaction_sched_group = compaction_manager::scheduling_group{dbcfg.compaction_scheduling_group},\n                    .maintenance_sched_group = compaction_manager::scheduling_group{dbcfg.streaming_scheduling_group},\n                    .available_memory = dbcfg.available_memory,\n                    .static_shares = cfg->compaction_static_shares,\n                    .throughput_mb_per_sec = cfg->compaction_throughput_mb_per_sec,\n                    .flush_all_tables_before_major = cfg->compaction_flush_all_tables_before_major_seconds() * 1s,\n                };\n            });\n            cm.start(std::move(get_cm_cfg), std::ref(stop_signal.as_sharded_abort_source()), std::ref(task_manager)).get();\n            auto stop_cm = defer_verbose_shutdown(\"compaction_manager\", [&cm] {\n               cm.stop().get();\n            });\n\n            sstables::storage_manager::config stm_cfg;\n            stm_cfg.s3_clients_memory = std::clamp<size_t>(memory::stats().total_memory() * 0.01, 10 << 20, 100 << 20);\n            sstm.start(std::ref(*cfg), stm_cfg).get();\n            auto stop_sstm = defer_verbose_shutdown(\"sstables storage manager\", [&sstm] {\n                sstm.stop().get();\n            });\n\n            static sharded<auth::service> auth_service;\n            static sharded<auth::service> maintenance_auth_service;\n            static sharded<qos::service_level_controller> sl_controller;\n            debug::the_sl_controller = &sl_controller;\n\n            //starting service level controller\n            qos::service_level_options default_service_level_configuration;\n            default_service_level_configuration.shares = 1000;\n            sl_controller.start(std::ref(auth_service), std::ref(token_metadata), std::ref(stop_signal.as_sharded_abort_source()), default_service_level_configuration, dbcfg.statement_scheduling_group).get();\n            sl_controller.invoke_on_all(&qos::service_level_controller::start).get();\n            auto stop_sl_controller = defer_verbose_shutdown(\"service level controller\", [] {\n                sl_controller.stop().get();\n            });\n\n            lang::manager::config lang_config;\n            lang_config.lua.max_bytes = cfg->user_defined_function_allocation_limit_bytes();\n            lang_config.lua.max_contiguous = cfg->user_defined_function_contiguous_allocation_limit_bytes();\n            lang_config.lua.timeout = std::chrono::milliseconds(cfg->user_defined_function_time_limit_ms());\n            if (cfg->enable_user_defined_functions() && cfg->check_experimental(db::experimental_features_t::feature::UDF)) {\n                lang_config.wasm = lang::manager::wasm_config {\n                    .udf_memory_limit = cfg->wasm_udf_memory_limit(),\n                    .cache_size = dbcfg.available_memory * cfg->wasm_cache_memory_fraction(),\n                    .cache_instance_size = cfg->wasm_cache_instance_size_limit(),\n                    .cache_timer_period = std::chrono::milliseconds(cfg->wasm_cache_timeout_in_ms()),\n                    .yield_fuel = cfg->wasm_udf_yield_fuel(),\n                    .total_fuel = cfg->wasm_udf_total_fuel(),\n                };\n            }\n\n            static sharded<lang::manager> langman;\n            langman.start(lang_config).get();\n            // don't stop for real until query_processor stops\n            auto stop_lang_man = defer_verbose_shutdown(\"lang manager\", [] { langman.invoke_on_all(&lang::manager::stop).get(); });\n            langman.invoke_on_all(&lang::manager::start).get();\n\n            supervisor::notify(\"starting database\");\n            debug::the_database = &db;\n            db.start(std::ref(*cfg), dbcfg, std::ref(mm_notifier), std::ref(feature_service), std::ref(token_metadata),\n                    std::ref(cm), std::ref(sstm), std::ref(langman), std::ref(sst_dir_semaphore), std::ref(stop_signal.as_sharded_abort_source()), utils::cross_shard_barrier()).get();\n            auto stop_database_and_sstables = defer_verbose_shutdown(\"database\", [&db] {\n                // #293 - do not stop anything - not even db (for real)\n                //return db.stop();\n                // call stop on each db instance, but leave the shareded<database> pointers alive.\n                db.invoke_on_all(&replica::database::stop).get();\n            });\n\n            // We need to init commitlog on shard0 before it is inited on other shards\n            // because it obtains the list of pre-existing segments for replay, which must\n            // not include reserve segments created by active commitlogs.\n            db.local().init_commitlog().get();\n            db.invoke_on_all(&replica::database::start, std::ref(sl_controller)).get();\n\n            ::sigquit_handler sigquit_handler(db);\n\n// FIXME: The stall detector uses glibc backtrace function to\n// collect backtraces, this causes ASAN failures on ARM.\n// For now we just disable the stall detector in this configuration,\n// the ticket about migrating to libunwind: scylladb/seastar#1878\n#if !defined(SEASTAR_DEBUG) || !defined(__aarch64__)\n            smp::invoke_on_all([blocked_reactor_notify_ms] {\n                engine().update_blocked_reactor_notify_ms(blocked_reactor_notify_ms);\n            }).get();\n#else\n            (void)blocked_reactor_notify_ms;\n#endif\n            debug::the_storage_proxy = &proxy;\n            supervisor::notify(\"starting storage proxy\");\n            service::storage_proxy::config spcfg {\n                .hints_directory_initializer = hints_dir_initializer,\n            };\n            spcfg.hinted_handoff_enabled = hinted_handoff_enabled;\n            spcfg.available_memory = memory::stats().total_memory();\n            smp_service_group_config storage_proxy_smp_service_group_config;\n            // Assuming less than 1kB per queued request, this limits storage_proxy submit_to() queues to 5MB or less\n            storage_proxy_smp_service_group_config.max_nonlocal_requests = 5000;\n            spcfg.read_smp_service_group = create_smp_service_group(storage_proxy_smp_service_group_config).get();\n            spcfg.write_smp_service_group = create_smp_service_group(storage_proxy_smp_service_group_config).get();\n            spcfg.write_mv_smp_service_group = create_smp_service_group(storage_proxy_smp_service_group_config).get();\n            spcfg.hints_write_smp_service_group = create_smp_service_group(storage_proxy_smp_service_group_config).get();\n            spcfg.write_ack_smp_service_group = create_smp_service_group(storage_proxy_smp_service_group_config).get();\n            static db::view::node_update_backlog node_backlog(smp::count, 10ms);\n            scheduling_group_key_config storage_proxy_stats_cfg =\n                    make_scheduling_group_key_config<service::storage_proxy_stats::stats>();\n            storage_proxy_stats_cfg.constructor = [plain_constructor = storage_proxy_stats_cfg.constructor] (void* ptr) {\n                plain_constructor(ptr);\n                reinterpret_cast<service::storage_proxy_stats::stats*>(ptr)->register_stats();\n                reinterpret_cast<service::storage_proxy_stats::stats*>(ptr)->register_split_metrics_local();\n            };\n            storage_proxy_stats_cfg.rename = [] (void* ptr) {\n                reinterpret_cast<service::storage_proxy_stats::stats*>(ptr)->register_stats();\n                reinterpret_cast<service::storage_proxy_stats::stats*>(ptr)->register_split_metrics_local();\n            };\n            proxy.start(std::ref(db), spcfg, std::ref(node_backlog),\n                    scheduling_group_key_create(storage_proxy_stats_cfg).get(),\n                    std::ref(feature_service), std::ref(token_metadata), std::ref(erm_factory)).get();\n\n            // #293 - do not stop anything\n            // engine().at_exit([&proxy] { return proxy.stop(); });\n            api::set_server_storage_proxy(ctx, proxy).get();\n            auto stop_sp_api = defer_verbose_shutdown(\"storage proxy API\", [&ctx] {\n                api::unset_server_storage_proxy(ctx).get();\n            });\n\n            static sharded<cql3::cql_config> cql_config;\n            cql_config.start(std::ref(*cfg)).get();\n\n            supervisor::notify(\"starting query processor\");\n            cql3::query_processor::memory_config qp_mcfg = {memory::stats().total_memory() / 256, memory::stats().total_memory() / 2560};\n            debug::the_query_processor = &qp;\n            auto local_data_dict = seastar::sharded_parameter([] (const replica::database& db) { return db.as_data_dictionary(); }, std::ref(db));\n\n            utils::loading_cache_config auth_prep_cache_config;\n            auth_prep_cache_config.max_size = qp_mcfg.authorized_prepared_cache_size;\n            auth_prep_cache_config.expiry = std::min(std::chrono::milliseconds(cfg->permissions_validity_in_ms()),\n                                                     std::chrono::duration_cast<std::chrono::milliseconds>(cql3::prepared_statements_cache::entry_expiry));\n            auth_prep_cache_config.refresh = std::chrono::milliseconds(cfg->permissions_update_interval_in_ms());\n\n            qp.start(std::ref(proxy), std::move(local_data_dict), std::ref(mm_notifier), qp_mcfg, std::ref(cql_config), std::move(auth_prep_cache_config), std::ref(langman)).get();\n\n            supervisor::notify(\"starting lifecycle notifier\");\n            lifecycle_notifier.start().get();\n            auto stop_lifecycle_notifier = defer_verbose_shutdown(\"lifecycle notifier\", [ &lifecycle_notifier ] {\n                lifecycle_notifier.stop().get();\n            });\n\n            supervisor::notify(\"creating tracing\");\n            sharded<tracing::tracing>& tracing = tracing::tracing::tracing_instance();\n            tracing.start(sstring(\"trace_keyspace_helper\")).get();\n            auto destroy_tracing = defer_verbose_shutdown(\"tracing instance\", [&tracing] {\n                tracing.stop().get();\n            });\n\n            with_scheduling_group(maintenance_scheduling_group, [&] {\n                return ctx.http_server.listen(socket_address{api_addr, cfg->api_port()});\n            }).get();\n            startlog.info(\"Scylla API server listening on {}:{} ...\", api_addr, cfg->api_port());\n\n            api::set_server_config(ctx, *cfg).get();\n            auto stop_config_api = defer_verbose_shutdown(\"config API\", [&ctx] {\n                api::unset_server_config(ctx).get();\n            });\n\n            static sharded<db::system_distributed_keyspace> sys_dist_ks;\n            static sharded<db::system_keyspace> sys_ks;\n            static sharded<db::view::view_update_generator> view_update_generator;\n            static sharded<db::view::view_builder> view_builder;\n            static sharded<cdc::generation_service> cdc_generation_service;\n\n            db::sstables_format_selector sst_format_selector(db);\n\n            api::set_format_selector(ctx, sst_format_selector).get();\n            auto stop_format_seletor_api = defer_verbose_shutdown(\"sstables format selector API\", [&ctx] {\n                api::unset_format_selector(ctx).get();\n            });\n\n            supervisor::notify(\"starting system keyspace\");\n            sys_ks.start(std::ref(qp), std::ref(db)).get();\n            // TODO: stop()?\n\n            // Initialization of a keyspace is done by shard 0 only. For system\n            // keyspace, the procedure  will go through the hardcoded column\n            // families, and in each of them, it will load the sstables for all\n            // shards using distributed database object.\n            // Iteration through column family directory for sstable loading is\n            // done only by shard 0, so we'll no longer face race conditions as\n            // described here: https://github.com/scylladb/scylla/issues/1014\n            supervisor::notify(\"loading system sstables\");\n            replica::distributed_loader::init_system_keyspace(sys_ks, erm_factory, db).get();\n\n            utils::get_local_injector().inject(\"stop_after_init_of_system_ks\",\n                [] { std::raise(SIGSTOP); });\n\n            // 1. Here we notify dependent services that system tables have been loaded,\n            //    and they in turn can load the necessary data from them;\n            // 2. This notification is important for the services that are needed\n            //    during schema commitlog replay;\n            // 3. The data this services load should be written with accompanying\n            //    table.flush, since commitlog is not replayed yet;\n            // 4. This is true for the following services:\n            //   * features_service: we need to re-enable previously enabled features,\n            //     this should be done before commitlog starts replaying\n            //     since some features affect storage.\n            //   * sstables_format_selector: we need to choose the appropriate format,\n            //     since schema commitlog replay can write to sstables.\n            when_all_succeed(feature_service.local().on_system_tables_loaded(sys_ks.local()),\n                sst_format_selector.on_system_tables_loaded(sys_ks.local())).get();\n\n            db.local().init_schema_commitlog();\n\n            utils::get_local_injector().inject(\"stop_after_init_of_schema_commitlog\",\n                [] { std::raise(SIGSTOP); });\n\n            // Mark all the system tables writable and assign the proper commitlog to them.\n            sys_ks.invoke_on_all(&db::system_keyspace::mark_writable).get();\n\n            auto sch_cl = db.local().schema_commitlog();\n            if (sch_cl != nullptr) {\n              auto paths = sch_cl->get_segments_to_replay().get();\n              if (!paths.empty()) {\n                  supervisor::notify(\"replaying schema commit log\");\n                  auto rp = db::commitlog_replayer::create_replayer(db, sys_ks).get();\n                  rp.recover(paths, db::schema_tables::COMMITLOG_FILENAME_PREFIX).get();\n                  supervisor::notify(\"replaying schema commit log - flushing memtables\");\n                  // The schema commitlog lives only on the null shard.\n                  // This is enforced when the table is marked to use\n                  // it - schema_static_props::enable_schema_commitlog function\n                  // also sets the use_null_sharder property.\n                  // This means only the local memtables need to be flushed.\n                  db.local().flush_all_memtables().get();\n                  supervisor::notify(\"replaying schema commit log - removing old commitlog segments\");\n                  //FIXME: discarded future\n                  (void)sch_cl->delete_segments(std::move(paths));\n              }\n            }\n\n            static sharded<gms::gossip_address_map> gossip_address_map;\n            supervisor::notify(\"starting gossip address map\");\n            gossip_address_map.start().get();\n            auto stop_gossip_address_map = defer_verbose_shutdown(\"gossip_address_map\", [] {\n                gossip_address_map.stop().get();\n            });\n\n            sys_ks.local().build_bootstrap_info().get();\n\n            const auto listen_address = utils::resolve(cfg->listen_address, family).get();\n            const auto host_id = initialize_local_info_thread(sys_ks, snitch, listen_address, *cfg, broadcast_addr, broadcast_rpc_addr);\n\n          shared_token_metadata::mutate_on_all_shards(token_metadata, [host_id, endpoint = broadcast_addr] (locator::token_metadata& tm) {\n              // Makes local host id available in topology cfg as soon as possible.\n              // Raft topology discard the endpoint-to-id map, so the local id can\n              // still be found in the config.\n              tm.get_topology().set_host_id_cfg(host_id);\n              tm.get_topology().add_or_update_endpoint(host_id, endpoint);\n              return make_ready_future<>();\n          }).get();\n\n            utils::dict_sampler dict_sampler;\n            auto arct_cfg = [&] {\n                return utils::advanced_rpc_compressor::tracker::config{\n                    .zstd_min_msg_size = cfg->internode_compression_zstd_min_message_size,\n                    .zstd_max_msg_size = cfg->internode_compression_zstd_max_message_size,\n                    .zstd_quota_fraction = cfg->internode_compression_zstd_max_cpu_fraction,\n                    .zstd_quota_refresh_ms = cfg->internode_compression_zstd_cpu_quota_refresh_period_ms,\n                    .zstd_longterm_quota_fraction = cfg->internode_compression_zstd_max_longterm_cpu_fraction,\n                    .zstd_longterm_quota_refresh_ms = cfg->internode_compression_zstd_longterm_cpu_quota_refresh_period_ms,\n                    .algo_config = cfg->internode_compression_algorithms,\n                    .register_metrics = cfg->internode_compression_enable_advanced(),\n                    .checksumming = cfg->internode_compression_checksumming,\n                };\n            };\n            static sharded<utils::walltime_compressor_tracker> compressor_tracker;\n            compressor_tracker.start(arct_cfg).get();\n            auto stop_compressor_tracker = defer_verbose_shutdown(\"compressor_tracker\", [] { compressor_tracker.stop().get(); });\n            compressor_tracker.local().attach_to_dict_sampler(&dict_sampler);\n\n            netw::messaging_service::config mscfg;\n\n            mscfg.id = host_id;\n            mscfg.ip = listen_address;\n            mscfg.broadcast_address = broadcast_addr;\n            mscfg.port = cfg->storage_port();\n            mscfg.ssl_port = cfg->ssl_storage_port();\n            mscfg.listen_on_broadcast_address = cfg->listen_on_broadcast_address();\n            mscfg.rpc_memory_limit = std::max<size_t>(0.08 * memory::stats().total_memory(), mscfg.rpc_memory_limit);\n            if (snitch.local()->prefer_local()) {\n                mscfg.preferred_ips = sys_ks.local().get_preferred_ips().get();\n            }\n            mscfg.maintenance_mode = maintenance_mode_enabled{cfg->maintenance_mode()};\n\n            const auto& seo = cfg->server_encryption_options();\n            auto encrypt = utils::get_or_default(seo, \"internode_encryption\", \"none\");\n\n            if (utils::is_true(utils::get_or_default(seo, \"require_client_auth\", \"false\"))) {\n                if (encrypt == \"dc\" || encrypt == \"rack\") {\n                    startlog.warn(\"Setting require_client_auth is incompatible with 'rack' and 'dc' internode_encryption values.\"\n                        \" To ensure that mutual TLS authentication is enforced, please set internode_encryption to 'all'. Continuing with\"\n                        \" potentially insecure configuration.\"\n                    );\n                }\n            }\n\n            sstring compress_what = cfg->internode_compression();\n            if (compress_what == \"all\") {\n                mscfg.compress = netw::messaging_service::compress_what::all;\n            } else if (compress_what == \"dc\") {\n                mscfg.compress = netw::messaging_service::compress_what::dc;\n            }\n            mscfg.enable_advanced_rpc_compression = cfg->internode_compression_enable_advanced();\n\n            if (encrypt == \"all\") {\n                mscfg.encrypt = netw::messaging_service::encrypt_what::all;\n            } else if (encrypt == \"dc\") {\n                mscfg.encrypt = netw::messaging_service::encrypt_what::dc;\n            } else if (encrypt == \"rack\") {\n                mscfg.encrypt = netw::messaging_service::encrypt_what::rack;\n            } else if (encrypt == \"transitional\") {\n                mscfg.encrypt = netw::messaging_service::encrypt_what::transitional;\n            }\n\n            if (!cfg->inter_dc_tcp_nodelay()) {\n                mscfg.tcp_nodelay = netw::messaging_service::tcp_nodelay_what::local;\n            }\n\n            netw::messaging_service::scheduling_config scfg;\n            scfg.statement_tenants = {\n                    {default_scheduling_group(), \"$system\"},\n                    {dbcfg.streaming_scheduling_group, \"$maintenance\", false}\n            };\n            scfg.streaming = dbcfg.streaming_scheduling_group;\n            scfg.gossip = dbcfg.gossip_scheduling_group;\n\n            supervisor::notify(\"starting messaging service\");\n            debug::the_messaging_service = &messaging;\n\n            std::shared_ptr<seastar::tls::credentials_builder> creds;\n            if (mscfg.encrypt != netw::messaging_service::encrypt_what::none \n                || (cfg->ssl_storage_port() != 0 && seo.contains(\"certificate\"))\n            ) {\n                creds = std::make_shared<seastar::tls::credentials_builder>();\n                utils::configure_tls_creds_builder(*creds, seo).get();\n            }\n\n            // Delay listening messaging_service until gossip message handlers are registered\n            messaging.start(mscfg, scfg, creds, std::ref(feature_service), std::ref(gossip_address_map), std::ref(compressor_tracker), std::ref(sl_controller)).get();\n            auto stop_ms = defer_verbose_shutdown(\"messaging service\", [&messaging] {\n                messaging.invoke_on_all(&netw::messaging_service::stop).get();\n            });\n\n            // #14299 - do early init of messaging_service (or rather its TLS structures)\n            // since other things (failure_detector) might try to send messages via it\n            // before start_listen is called.\n            messaging.invoke_on_all(&netw::messaging_service::start).get();\n\n            api::set_server_messaging_service(ctx, messaging).get();\n            auto stop_messaging_api = defer_verbose_shutdown(\"messaging service API\", [&ctx] {\n                api::unset_server_messaging_service(ctx).get();\n            });\n\n            // Task manager's messaging handlers need to be set like this because of dependency chain:\n            // messaging -(needs)-> sys_ks -> db -> cm -> task_manager.\n            task_manager.invoke_on_all([&] (auto& tm) {\n                tm.init_ms_handlers(messaging.local());\n            }).get();\n            auto uninit_tm_ms_handlers = defer([&task_manager] () {\n                task_manager.invoke_on_all([] (auto& tm) {\n                    return tm.uninit_ms_handlers();\n                }).get();\n            });\n\n            supervisor::notify(\"starting gossiper\");\n            auto cluster_name = cfg->cluster_name();\n            if (cluster_name.empty()) {\n                cluster_name = \"Test Cluster\";\n                startlog.warn(\"Using default cluster name is not recommended. Using a unique cluster name will reduce the chance of adding nodes to the wrong cluster by mistake\");\n            }\n            auto group0_id = sys_ks.local().get_raft_group0_id().get();\n\n            // Fail on a gossiper seeds lookup error only if the node is not bootstrapped.\n            const bool fail_on_lookup_error = !sys_ks.local().bootstrap_complete();\n\n            auto gossiper_seeds = get_seeds_from_db_config(*cfg, broadcast_addr, fail_on_lookup_error);\n\n            auto get_gossiper_cfg = sharded_parameter([&] {\n                gms::gossip_config gcfg;\n                gcfg.gossip_scheduling_group = dbcfg.gossip_scheduling_group;\n                gcfg.seeds = gossiper_seeds;\n                gcfg.cluster_name = cluster_name;\n                gcfg.partitioner = cfg->partitioner();\n                gcfg.ring_delay_ms = cfg->ring_delay_ms();\n                gcfg.shadow_round_ms = cfg->shadow_round_ms();\n                gcfg.shutdown_announce_ms = cfg->shutdown_announce_in_ms();\n                gcfg.skip_wait_for_gossip_to_settle = cfg->skip_wait_for_gossip_to_settle();\n                gcfg.group0_id = group0_id;\n                gcfg.host_id = host_id;\n                gcfg.failure_detector_timeout_ms = cfg->failure_detector_timeout_in_ms;\n                gcfg.force_gossip_generation = cfg->force_gossip_generation;\n                return gcfg;\n            });\n\n            debug::the_gossiper = &gossiper;\n            gossiper.start(std::ref(stop_signal.as_sharded_abort_source()), std::ref(token_metadata), std::ref(messaging), std::move(get_gossiper_cfg), std::ref(gossip_address_map)).get();\n            auto stop_gossiper = defer_verbose_shutdown(\"gossiper\", [&gossiper] {\n                // call stop on each instance, but leave the sharded<> pointers alive\n                gossiper.invoke_on_all(&gms::gossiper::stop).get();\n            });\n            gossiper.invoke_on_all(&gms::gossiper::start).get();\n\n            utils::get_local_injector().inject(\"stop_after_starting_gossiper\",\n                [] { std::raise(SIGSTOP); });\n\n            api::set_server_gossip(ctx, gossiper).get();\n            auto stop_gossip_api = defer_verbose_shutdown(\"gossiper API\", [&ctx] {\n                api::unset_server_gossip(ctx).get();\n            });\n\n            static sharded<service::direct_fd_pinger> fd_pinger;\n            supervisor::notify(\"starting direct failure detector pinger service\");\n            fd_pinger.start(std::ref(messaging)).get();\n\n            auto stop_fd_pinger = defer_verbose_shutdown(\"fd_pinger\", [] {\n                fd_pinger.stop().get();\n            });\n\n            static service::direct_fd_clock fd_clock;\n            static sharded<direct_failure_detector::failure_detector> fd;\n            supervisor::notify(\"starting direct failure detector service\");\n            fd.start(\n                std::ref(fd_pinger), std::ref(fd_clock),\n                service::direct_fd_clock::base::duration{std::chrono::milliseconds{100}}.count(),\n                service::direct_fd_clock::base::duration{std::chrono::milliseconds{cfg->direct_failure_detector_ping_timeout_in_ms()}}.count()).get();\n\n            auto stop_fd = defer_verbose_shutdown(\"direct_failure_detector\", [] {\n                fd.stop().get();\n            });\n\n            raft_gr.start(raft::server_id{host_id.id}, std::ref(messaging), std::ref(fd)).get();\n\n            // group0 client exists only on shard 0.\n            // The client has to be created before `stop_raft` since during\n            // destruction it has to exist until raft_gr.stop() completes.\n            service::raft_group0_client group0_client{raft_gr.local(), sys_ks.local(), token_metadata.local(), maintenance_mode_enabled{cfg->maintenance_mode()}};\n\n            service::raft_group0 group0_service{\n                    stop_signal.as_local_abort_source(), raft_gr.local(), messaging,\n                    gossiper.local(), feature_service.local(), sys_ks.local(), group0_client, dbcfg.gossip_scheduling_group};\n\n            service::tablet_allocator::config tacfg;\n            tacfg.initial_tablets_scale = cfg->tablets_initial_scale_factor();\n            distributed<service::tablet_allocator> tablet_allocator;\n            tablet_allocator.start(tacfg, std::ref(mm_notifier), std::ref(db)).get();\n            auto stop_tablet_allocator = defer_verbose_shutdown(\"tablet allocator\", [&tablet_allocator] {\n                tablet_allocator.stop().get();\n            });\n\n            supervisor::notify(\"starting mapreduce service\");\n            mapreduce_service.start(std::ref(messaging), std::ref(proxy), std::ref(db), std::ref(token_metadata), std::ref(stop_signal.as_sharded_abort_source())).get();\n            auto stop_mapreduce_service_handlers = defer_verbose_shutdown(\"mapreduce service\", [&mapreduce_service] {\n                mapreduce_service.stop().get();\n            });\n\n            supervisor::notify(\"starting migration manager\");\n            debug::the_migration_manager = &mm;\n            mm.start(std::ref(mm_notifier), std::ref(feature_service), std::ref(messaging), std::ref(proxy), std::ref(gossiper), std::ref(group0_client), std::ref(sys_ks)).get();\n            auto stop_migration_manager = defer_verbose_shutdown(\"migration manager\", [&mm] {\n                mm.stop().get();\n            });\n\n            utils::get_local_injector().inject(\"stop_after_starting_migration_manager\",\n                [] { std::raise(SIGSTOP); });\n\n            // XXX: stop_raft has to happen before query_processor and migration_manager\n            // is stopped, since some groups keep using the query\n            // processor until are stopped inside stop_raft.\n            auto stop_raft = defer_verbose_shutdown(\"Raft\", [&raft_gr] {\n                raft_gr.stop().get();\n            });\n\n            sharded<service::topology_state_machine> tsm;\n            tsm.start().get();\n            auto stop_tsm = defer_verbose_shutdown(\"topology_state_machine\", [&tsm] {\n                tsm.stop().get();\n            });\n\n            auto compression_dict_updated_callback = [] () -> future<> {\n                auto dict = co_await sys_ks.local().query_dict();\n                co_await utils::announce_dict_to_shards(compressor_tracker, std::move(dict));\n            };\n\n            supervisor::notify(\"initializing storage service\");\n            debug::the_storage_service = &ss;\n            ss.start(std::ref(stop_signal.as_sharded_abort_source()),\n                std::ref(db), std::ref(gossiper), std::ref(sys_ks), std::ref(sys_dist_ks),\n                std::ref(feature_service), std::ref(mm), std::ref(token_metadata), std::ref(erm_factory),\n                std::ref(messaging), std::ref(repair),\n                std::ref(stream_manager), std::ref(lifecycle_notifier), std::ref(bm), std::ref(snitch),\n                std::ref(tablet_allocator), std::ref(cdc_generation_service), std::ref(view_builder), std::ref(qp), std::ref(sl_controller),\n                std::ref(tsm), std::ref(task_manager), std::ref(gossip_address_map),\n                compression_dict_updated_callback\n            ).get();\n\n            auto stop_storage_service = defer_verbose_shutdown(\"storage_service\", [&] {\n                ss.stop().get();\n            });\n\n            api::set_server_storage_service(ctx, ss, group0_client).get();\n            auto stop_ss_api = defer_verbose_shutdown(\"storage service API\", [&ctx] {\n                api::unset_server_storage_service(ctx).get();\n            });\n\n            supervisor::notify(\"initializing query processor remote part\");\n            // TODO: do this together with proxy.start_remote(...)\n            qp.invoke_on_all(&cql3::query_processor::start_remote, std::ref(mm), std::ref(mapreduce_service),\n                             std::ref(ss), std::ref(group0_client)).get();\n            auto stop_qp_remote = defer_verbose_shutdown(\"query processor remote part\", [&qp] {\n                qp.invoke_on_all(&cql3::query_processor::stop_remote).get();\n            });\n\n            supervisor::notify(\"initializing virtual tables\");\n            smp::invoke_on_all([&] {\n                return db::initialize_virtual_tables(db, ss, gossiper, raft_gr, sys_ks, *cfg);\n            }).get();\n\n            // #293 - do not stop anything\n            // engine().at_exit([&qp] { return qp.stop(); });\n            sstables::init_metrics().get();\n\n            db::sstables_format_listener sst_format_listener(gossiper.local(), feature_service, sst_format_selector);\n\n            sst_format_listener.start().get();\n            auto stop_format_listener = defer_verbose_shutdown(\"sstables format listener\", [&sst_format_listener] {\n                sst_format_listener.stop().get();\n            });\n\n            supervisor::notify(\"starting Raft Group Registry service\");\n            raft_gr.invoke_on_all(&service::raft_group_registry::start).get();\n\n            api::set_server_raft(ctx, raft_gr).get();\n            auto stop_raft_api = defer_verbose_shutdown(\"Raft API\", [&ctx] {\n                api::unset_server_raft(ctx).get();\n            });\n\n            group0_client.init().get();\n\n            // schema migration, if needed, is also done on shard 0\n            db::legacy_schema_migrator::migrate(proxy, db, sys_ks, qp.local()).get();\n            db::schema_tables::save_system_schema(qp.local()).get();\n            db::schema_tables::recalculate_schema_version(sys_ks, proxy, feature_service.local()).get();\n\n            // making compaction manager api available, after system keyspace has already been established.\n            api::set_server_compaction_manager(ctx, cm).get();\n            auto stop_cm_api = defer_verbose_shutdown(\"compaction manager API\", [&ctx] {\n                api::unset_server_compaction_manager(ctx).get();\n            });\n\n            cm.invoke_on_all([&](compaction_manager& cm) {\n                auto cl = db.local().commitlog();\n                auto scl = db.local().schema_commitlog();\n                if (cl && scl) {\n                    cm.get_tombstone_gc_state().set_gc_time_min_source([cl, scl](const table_id& id) {\n                        return std::min(cl->min_gc_time(id), scl->min_gc_time(id));\n                    });\n                } else if (cl) {\n                    cm.get_tombstone_gc_state().set_gc_time_min_source([cl](const table_id& id) {\n                        return cl->min_gc_time(id);\n                    });\n                } else if (scl) {\n                    cm.get_tombstone_gc_state().set_gc_time_min_source([scl](const table_id& id) {\n                        return scl->min_gc_time(id);\n                    });\n                }\n            }).get();\n\n            supervisor::notify(\"loading tablet metadata\");\n            try {\n                ss.local().load_tablet_metadata({}).get();\n            } catch (...) {\n                if (!cfg->maintenance_mode()) {\n                    throw;\n                }\n                startlog.error(\"Failed to load tablet metadata (ignoring due to maintenance mode): {}\", std::current_exception());\n            }\n\n            // We do not support tablet re-sharding yet, see https://github.com/scylladb/scylladb/issues/16739.\n            // To avoid undefined behaviour due to violated assumptions, that\n            // each tablet has a valid shard replica, we check this assumption here, and refuse startup if violated.\n            if (!locator::check_tablet_replica_shards(ss.local().get_token_metadata_ptr()->tablets(), host_id).get()) {\n                throw std::runtime_error(\"Detected a tablet with invalid replica shard, reducing shard count with tablet-enabled tables is not yet supported. Replace the node instead.\");\n            }\n\n            supervisor::notify(\"loading non-system sstables\");\n            replica::distributed_loader::init_non_system_keyspaces(db, proxy, sys_ks).get();\n\n            sys_dist_ks.start(std::ref(qp), std::ref(mm), std::ref(proxy)).get();\n            auto stop_sdks = defer_verbose_shutdown(\"system distributed keyspace\", [] {\n                sys_dist_ks.invoke_on_all(&db::system_distributed_keyspace::stop).get();\n            });\n\n            group0_service.start().get();\n            auto stop_group0_service = defer_verbose_shutdown(\"group 0 service\", [&group0_service] {\n                sl_controller.local().abort_group0_operations();\n                group0_service.abort().get();\n            });\n\n            utils::get_local_injector().inject(\"stop_after_starting_group0_service\",\n                [] { std::raise(SIGSTOP); });\n\n            // Set up group0 service earlier since it is needed by group0 setup just below\n            ss.local().set_group0(group0_service);\n\n            supervisor::notify(\"starting view update generator\");\n            view_update_generator.start(std::ref(db), std::ref(proxy), std::ref(stop_signal.as_sharded_abort_source())).get();\n            auto stop_view_update_generator = defer_verbose_shutdown(\"view update generator\", [] {\n                view_update_generator.stop().get();\n            });\n\n            supervisor::notify(\"starting the view builder\");\n            view_builder.start(std::ref(db), std::ref(sys_ks), std::ref(sys_dist_ks), std::ref(mm_notifier), std::ref(view_update_generator), std::ref(group0_client), std::ref(qp)).get();\n            auto stop_view_builder = defer_verbose_shutdown(\"view builder\", [cfg] {\n                view_builder.stop().get();\n            });\n\n            supervisor::notify(\"starting commit log\");\n            auto cl = db.local().commitlog();\n\n            utils::get_local_injector().inject(\"stop_after_starting_commitlog\",\n                [] { std::raise(SIGSTOP); });\n\n            if (cl != nullptr) {\n                auto paths = cl->get_segments_to_replay().get();\n                if (!paths.empty()) {\n                    supervisor::notify(\"replaying commit log\");\n                    auto rp = db::commitlog_replayer::create_replayer(db, sys_ks).get();\n                    rp.recover(paths, db::commitlog::descriptor::FILENAME_PREFIX).get();\n                    supervisor::notify(\"replaying commit log - flushing memtables\");\n                    db.invoke_on_all(&replica::database::flush_all_memtables).get();\n                    supervisor::notify(\"replaying commit log - removing old commitlog segments\");\n                    //FIXME: discarded future\n                    (void)cl->delete_segments(std::move(paths));\n                }\n            }\n\n            // Once stuff is replayed, we can empty RP:s from truncation records. \n            // This ensures we can't mis-mash older records with a newer crashed run.\n            // I.e: never keep replay_positions alive across a restart cycle.\n            sys_ks.local().drop_truncation_rp_records().get();\n            sys_ks.local().drop_all_commitlog_cleanup_records().get();\n\n            db.invoke_on_all([] (replica::database& db) {\n                db.get_tables_metadata().for_each_table([] (table_id, lw_shared_ptr<replica::table> table) {\n                    replica::table& t = *table;\n                    t.enable_auto_compaction();\n                });\n            }).get();\n\n            api::set_server_column_family(ctx, sys_ks).get();\n            auto stop_cf_api = defer_verbose_shutdown(\"column family API\", [&ctx] {\n                api::unset_server_column_family(ctx).get();\n            });\n            static seastar::sharded<memory_threshold_guard> mtg;\n            mtg.start(cfg->large_memory_allocation_warning_threshold()).get();\n            supervisor::notify(\"initializing storage proxy RPC verbs\");\n            proxy.invoke_on_all(&service::storage_proxy::start_remote, std::ref(messaging), std::ref(gossiper), std::ref(mm), std::ref(sys_ks), std::ref(group0_client), std::ref(tsm)).get();\n            auto stop_proxy_handlers = defer_verbose_shutdown(\"storage proxy RPC verbs\", [&proxy] {\n                proxy.invoke_on_all(&service::storage_proxy::stop_remote).get();\n            });\n\n            debug::the_stream_manager = &stream_manager;\n            supervisor::notify(\"starting streaming service\");\n            stream_manager.start(std::ref(*cfg), std::ref(db), std::ref(view_builder), std::ref(messaging), std::ref(mm), std::ref(gossiper), maintenance_scheduling_group).get();\n            auto stop_stream_manager = defer_verbose_shutdown(\"stream manager\", [&stream_manager] {\n                // FIXME -- keep the instances alive, just call .stop on them\n                stream_manager.invoke_on_all(&streaming::stream_manager::stop).get();\n            });\n\n            stream_manager.invoke_on_all([&stop_signal] (streaming::stream_manager& sm) {\n                return sm.start(stop_signal.as_local_abort_source());\n            }).get();\n\n            api::set_server_stream_manager(ctx, stream_manager).get();\n            auto stop_stream_manager_api = defer_verbose_shutdown(\"stream manager api\", [&ctx] {\n                api::unset_server_stream_manager(ctx).get();\n            });\n\n            supervisor::notify(\"starting hinted handoff manager\");\n            if (!hinted_handoff_enabled.is_disabled_for_all()) {\n                hints_dir_initializer.ensure_rebalanced().get();\n            }\n            view_hints_dir_initializer.ensure_rebalanced().get();\n\n            proxy.invoke_on_all([&lifecycle_notifier] (service::storage_proxy& local_proxy) {\n                lifecycle_notifier.local().register_subscriber(&local_proxy);\n            }).get();\n\n            auto unsubscribe_proxy = defer_verbose_shutdown(\"unsubscribe storage proxy\", [&proxy, &lifecycle_notifier] {\n                proxy.invoke_on_all([&lifecycle_notifier] (service::storage_proxy& local_proxy) mutable {\n                    return lifecycle_notifier.local().unregister_subscriber(&local_proxy);\n                }).get();\n            });\n\n            // ATTN -- sharded repair reference already sits on storage_service and if\n            // it calls repair.local() before this place it'll crash (now it doesn't do\n            // both)\n            supervisor::notify(\"starting repair service\");\n            auto max_memory_repair = memory::stats().total_memory() * 0.1;\n            repair.start(std::ref(tsm), std::ref(gossiper), std::ref(messaging), std::ref(db), std::ref(proxy), std::ref(bm), std::ref(sys_ks), std::ref(view_builder), std::ref(task_manager), std::ref(mm), max_memory_repair).get();\n            auto stop_repair_service = defer_verbose_shutdown(\"repair service\", [&repair] {\n                repair.stop().get();\n            });\n            repair.invoke_on_all(&repair_service::start).get();\n            api::set_server_repair(ctx, repair, gossip_address_map).get();\n            auto stop_repair_api = defer_verbose_shutdown(\"repair API\", [&ctx] {\n                api::unset_server_repair(ctx).get();\n            });\n\n            utils::get_local_injector().inject(\"stop_after_starting_repair\",\n                [] { std::raise(SIGSTOP); });\n\n            supervisor::notify(\"starting CDC Generation Management service\");\n            /* This service uses the system distributed keyspace.\n             * It will only do that *after* the node has joined the token ring, and the token ring joining\n             * procedure (`storage_service::init_server`) is responsible for initializing sys_dist_ks.\n             * Hence the service will start using sys_dist_ks only after it was initialized.\n             *\n             * However, there is a problem with the service shutdown order: sys_dist_ks is stopped\n             * *before* CDC generation service is stopped (`storage_service::drain_on_shutdown` below),\n             * so CDC generation service takes sharded<db::sys_dist_ks> and must check local_is_initialized()\n             * every time it accesses it (because it may have been stopped already), then take local_shared()\n             * which will prevent sys_dist_ks from being destroyed while the service operates on it.\n             */\n            cdc::generation_service::config cdc_config;\n            cdc_config.ignore_msb_bits = cfg->murmur3_partitioner_ignore_msb_bits();\n            cdc_config.ring_delay = std::chrono::milliseconds(cfg->ring_delay_ms());\n            cdc_config.dont_rewrite_streams = cfg->cdc_dont_rewrite_streams();\n            cdc_generation_service.start(std::move(cdc_config), std::ref(gossiper), std::ref(sys_dist_ks), std::ref(sys_ks),\n                    std::ref(stop_signal.as_sharded_abort_source()), std::ref(token_metadata), std::ref(feature_service), std::ref(db),\n                    [&ss] () -> bool { return ss.local().raft_topology_change_enabled(); }).get();\n            auto stop_cdc_generation_service = defer_verbose_shutdown(\"CDC Generation Management service\", [] {\n                cdc_generation_service.stop().get();\n            });\n\n            utils::get_local_injector().inject(\"stop_after_starting_cdc_generation_service\",\n                [] { std::raise(SIGSTOP); });\n\n            auto get_cdc_metadata = [] (cdc::generation_service& svc) { return std::ref(svc.get_cdc_metadata()); };\n\n            supervisor::notify(\"starting CDC log service\");\n            static sharded<cdc::cdc_service> cdc;\n            cdc.start(std::ref(proxy), sharded_parameter(get_cdc_metadata, std::ref(cdc_generation_service)), std::ref(mm_notifier)).get();\n            auto stop_cdc_service = defer_verbose_shutdown(\"cdc log service\", [] {\n                cdc.stop().get();\n            });\n\n            supervisor::notify(\"starting storage service\", true);\n\n            gossiper.local().register_(ss.local().shared_from_this());\n            auto stop_listening = defer_verbose_shutdown(\"storage service notifications\", [&gossiper, &ss] {\n                gossiper.local().unregister_(ss.local().shared_from_this()).get();\n            });\n\n            gossiper.local().register_(mm.local().shared_from_this());\n            auto stop_mm_listening = defer_verbose_shutdown(\"migration manager notifications\", [&gossiper, &mm] {\n                gossiper.local().unregister_(mm.local().shared_from_this()).get();\n            });\n\n            utils::loading_cache_config perm_cache_config;\n            perm_cache_config.max_size = cfg->permissions_cache_max_entries();\n            perm_cache_config.expiry = std::chrono::milliseconds(cfg->permissions_validity_in_ms());\n            perm_cache_config.refresh = std::chrono::milliseconds(cfg->permissions_update_interval_in_ms());\n\n            auto start_auth_service = [&mm] (sharded<auth::service>& auth_service, std::any& stop_auth_service, const char* what) {\n                supervisor::notify(fmt::format(\"starting {}\", what));\n                auth_service.invoke_on_all(&auth::service::start, std::ref(mm), std::ref(sys_ks)).get();\n\n                stop_auth_service = defer_verbose_shutdown(what, [&auth_service] {\n                    auth_service.stop().get();\n                });\n            };\n\n            auto start_cql = [] (cql_transport::controller& controller, std::any& stop_cql, const char* what) {\n                supervisor::notify(fmt::format(\"starting {}\", what));\n                controller.start_server().get();\n                // FIXME -- this should be done via client hooks instead\n                stop_cql = defer_verbose_shutdown(what, [&controller] {\n                    controller.stop_server().get();\n                });\n            };\n\n            auth::service_config maintenance_auth_config;\n            maintenance_auth_config.authorizer_java_name = sstring{auth::allow_all_authorizer_name};\n            maintenance_auth_config.authenticator_java_name = sstring{auth::allow_all_authenticator_name};\n            maintenance_auth_config.role_manager_java_name = sstring{auth::maintenance_socket_role_manager_name};\n\n            maintenance_auth_service.start(perm_cache_config, std::ref(qp), std::ref(group0_client),  std::ref(mm_notifier), std::ref(mm), maintenance_auth_config, maintenance_socket_enabled::yes).get();\n\n            cql_transport::controller cql_maintenance_server_ctl(maintenance_auth_service, mm_notifier, gossiper, qp, service_memory_limiter, sl_controller, lifecycle_notifier, *cfg, maintenance_cql_sg_stats_key, maintenance_socket_enabled::yes, dbcfg.statement_scheduling_group);\n\n            std::any stop_maintenance_auth_service;\n            std::any stop_maintenance_cql;\n\n            if (cfg->maintenance_socket() != \"ignore\") {\n                start_auth_service(maintenance_auth_service, stop_maintenance_auth_service, \"maintenance auth service\");\n                start_cql(cql_maintenance_server_ctl, stop_maintenance_cql, \"maintenance native server\");\n            }\n\n            db::snapshot_ctl::config snap_cfg = {\n                .backup_sched_group = dbcfg.streaming_scheduling_group,\n            };\n            snapshot_ctl.start(std::ref(db), std::ref(task_manager), std::ref(sstm), snap_cfg).get();\n            auto stop_snapshot_ctl = defer_verbose_shutdown(\"snapshots\", [&snapshot_ctl] {\n                snapshot_ctl.stop().get();\n            });\n\n            api::set_server_snapshot(ctx, snapshot_ctl).get();\n            auto stop_api_snapshots = defer_verbose_shutdown(\"snapshots API\", [&ctx] {\n                api::unset_server_snapshot(ctx).get();\n            });\n\n            api::set_server_tasks_compaction_module(ctx, ss, snapshot_ctl).get();\n            auto stop_tasks_api = defer_verbose_shutdown(\"tasks API\", [&ctx] {\n                api::unset_server_tasks_compaction_module(ctx).get();\n            });\n\n            api::set_server_cache(ctx).get();\n            auto stop_cache_api = defer_verbose_shutdown(\"cache API\", [&ctx] {\n                api::unset_server_cache(ctx).get();\n            });\n\n            api::set_server_commitlog(ctx, db).get();\n            auto stop_commitlog_api = defer_verbose_shutdown(\"commitlog API\", [&ctx] {\n                api::unset_server_commitlog(ctx).get();\n            });\n\n            if (cfg->maintenance_mode()) {\n                startlog.info(\"entering maintenance mode.\");\n\n                ss.local().start_maintenance_mode().get();\n\n                seastar::set_abort_on_ebadf(cfg->abort_on_ebadf());\n                api::set_server_done(ctx).get();\n                {\n                    auto do_drain = defer_verbose_shutdown(\"local storage\", [&ss] {\n                        // Flush all memtables and stop ongoing compactions\n                        ss.local().drain_on_shutdown().get();\n                    });\n\n                    startlog.info(\"Scylla version {} initialization completed (maintenance mode).\", scylla_version());\n                    stop_signal.wait().get();\n                    startlog.info(\"Signal received; shutting down\");\n                }\n                startlog.info(\"Scylla version {} shutdown complete.\", scylla_version());\n                _exit(0);\n                return 0;\n            }\n\n            // Register storage_service to migration_notifier so we can update\n            // pending ranges when keyspace is changed\n            mm_notifier.local().register_listener(&ss.local());\n            auto stop_mm_listener = defer_verbose_shutdown(\"storage service notifications\", [&mm_notifier, &ss] {\n                mm_notifier.local().unregister_listener(&ss.local()).get();\n            });\n\n            supervisor::notify(\"starting sstables loader\");\n            sst_loader.start(std::ref(db), std::ref(messaging), std::ref(view_builder), std::ref(task_manager), std::ref(sstm), maintenance_scheduling_group).get();\n            auto stop_sst_loader = defer_verbose_shutdown(\"sstables loader\", [&sst_loader] {\n                sst_loader.stop().get();\n            });\n            api::set_server_sstables_loader(ctx, sst_loader).get();\n            auto stop_sstl_api = defer_verbose_shutdown(\"sstables loader API\", [&ctx] {\n                api::unset_server_sstables_loader(ctx).get();\n            });\n\n            /*\n             * FIXME. In bb07678346 commit the API toggle for autocompaction was\n             * (partially) delayed until system prepared to join the ring. Probably\n             * it was an overkill and it can be enabled earlier, even as early as\n             * 'by default'. E.g. the per-table toggle was 'enabled' right after\n             * the system keyspace started and nobody seemed to have any troubles.\n             */\n            db.local().enable_autocompaction_toggle();\n\n            // Load address_map from system.peers and subscribe to gossiper events to keep it updated.\n            ss.local().init_address_map(gossip_address_map.local()).get();\n            auto cancel_address_map_subscription = defer_verbose_shutdown(\"storage service uninit address map\", [&ss] {\n                ss.local().uninit_address_map().get();\n            });\n\n            // Need to make sure storage service stopped using group0 before running group0_service.abort()\n            // Normally it is done in storage_service::do_drain(), but in case start up fail we need to do it\n            // here as well\n            auto stop_group0_usage_in_storage_service = defer_verbose_shutdown(\"group 0 usage in local storage\", [&ss] {\n               ss.local().wait_for_group0_stop().get();\n            });\n\n            // Setup group0 early in case the node is bootstrapped already and the group exists.\n            // Need to do it before allowing incoming messaging service connections since\n            // storage proxy's and migration manager's verbs may access group0.\n            // This will also disable migration manager schema pulls if needed.\n            group0_service.setup_group0_if_exist(sys_ks.local(), ss.local(), qp.local(), mm.local()).get();\n\n            with_scheduling_group(maintenance_scheduling_group, [&] {\n                return messaging.invoke_on_all(&netw::messaging_service::start_listen, std::ref(token_metadata));\n            }).get();\n\n            const auto generation_number = gms::generation_type(sys_ks.local().increment_and_get_generation().get());\n\n            with_scheduling_group(maintenance_scheduling_group, [&] {\n                return ss.local().join_cluster(sys_dist_ks, proxy, service::start_hint_manager::yes, generation_number);\n            }).get();\n\n            dictionary_service dict_service(\n                dict_sampler,\n                sys_ks.local(),\n                rpc_dict_training_worker,\n                group0_client,\n                group0_service,\n                stop_signal.as_local_abort_source(),\n                feature_service.local(),\n                dictionary_service::config{\n                    .our_host_id = host_id,\n                    .rpc_dict_training_min_time_seconds = cfg->rpc_dict_training_min_time_seconds,\n                    .rpc_dict_training_min_bytes = cfg->rpc_dict_training_min_bytes,\n                    .rpc_dict_training_when = cfg->rpc_dict_training_when,\n                }\n            );\n            auto stop_dict_service = defer_verbose_shutdown(\"dictionary training\", [&] {\n                dict_service.stop().get();\n            });\n\n            supervisor::notify(\"starting tracing\");\n            tracing.invoke_on_all(&tracing::tracing::start, std::ref(qp), std::ref(mm)).get();\n            auto stop_tracing = defer_verbose_shutdown(\"tracing\", [&tracing] {\n                tracing.invoke_on_all(&tracing::tracing::shutdown).get();\n            });\n\n            startlog.info(\"SSTable data integrity checker is {}.\",\n                    cfg->enable_sstable_data_integrity_check() ? \"enabled\" : \"disabled\");\n\n            // This implicitly depends on node joining the cluster (join_cluster())\n            // with raft leader elected as only then service level mutation is put\n            // into scylla_local table. Calling it here avoids starting new cluster with\n            // older version only to immediately migrate it to the latest in the background.\n            sl_controller.invoke_on_all([&qp, &group0_client] (qos::service_level_controller& controller) -> future<> {\n                return controller.reload_distributed_data_accessor(\n                        qp.local(), group0_client, sys_ks.local(), sys_dist_ks.local());\n            }).get();\n\n            sl_controller.local().maybe_start_legacy_update_from_distributed_data([cfg] () {\n                return std::chrono::duration_cast<steady_clock_type::duration>(std::chrono::milliseconds(cfg->service_levels_interval()));\n            }, ss.local(), group0_client);\n\n            // Initialize virtual table in system_distributed keyspace after joining the cluster, so\n            // that the keyspace is ready\n            view_builder.invoke_on_all([] (db::view::view_builder& vb) {\n                vb.init_virtual_table();\n            }).get();\n\n            const qualified_name qualified_authorizer_name(auth::meta::AUTH_PACKAGE_NAME, cfg->authorizer());\n            const qualified_name qualified_authenticator_name(auth::meta::AUTH_PACKAGE_NAME, cfg->authenticator());\n            const qualified_name qualified_role_manager_name(auth::meta::AUTH_PACKAGE_NAME, cfg->role_manager());\n\n            auth::service_config auth_config;\n            auth_config.authorizer_java_name = qualified_authorizer_name;\n            auth_config.authenticator_java_name = qualified_authenticator_name;\n            auth_config.role_manager_java_name = qualified_role_manager_name;\n\n            auth_service.start(std::move(perm_cache_config), std::ref(qp), std::ref(group0_client), std::ref(mm_notifier), std::ref(mm), auth_config, maintenance_socket_enabled::no).get();\n\n            std::any stop_auth_service;\n            // Has to be called after node joined the cluster (join_cluster())\n            // with raft leader elected as only then auth version mutation is put\n            // in scylla_local table. This allows to know the version at auth service\n            // startup also when creating a new cluster.\n            start_auth_service(auth_service, stop_auth_service, \"auth service\");\n\n            utils::get_local_injector().inject(\"stop_after_starting_auth_service\",\n                [] { std::raise(SIGSTOP); });\n\n            api::set_server_authorization_cache(ctx, auth_service).get();\n            auto stop_authorization_cache_api = defer_verbose_shutdown(\"authorization cache api\", [&ctx] {\n                api::unset_server_authorization_cache(ctx).get();\n            });\n\n            // update the service level cache after the SL data accessor and auth service are initialized.\n            if (sl_controller.local().is_v2()) {\n                sl_controller.local().update_cache(qos::update_both_cache_levels::yes).get();\n            }\n\n            sl_controller.invoke_on_all([&lifecycle_notifier] (qos::service_level_controller& controller) {\n                lifecycle_notifier.local().register_subscriber(&controller);\n            }).get();\n            auto unsubscribe_sl_controller = defer_verbose_shutdown(\"service level controller subscription\", [&lifecycle_notifier] {\n                sl_controller.invoke_on_all([&lifecycle_notifier] (qos::service_level_controller& controller) {\n                    return lifecycle_notifier.local().unregister_subscriber(&controller);\n                }).get();\n            });\n\n            supervisor::notify(\"starting batchlog manager\");\n            db::batchlog_manager_config bm_cfg;\n            bm_cfg.write_request_timeout = cfg->write_request_timeout_in_ms() * 1ms;\n            bm_cfg.replay_rate = cfg->batchlog_replay_throttle_in_kb() * 1000;\n            bm_cfg.delay = std::chrono::milliseconds(cfg->ring_delay_ms());\n            bm_cfg.replay_cleanup_after_replays = cfg->batchlog_replay_cleanup_after_replays();\n\n            bm.start(std::ref(qp), std::ref(sys_ks), bm_cfg).get();\n            auto stop_batchlog_manager = defer_verbose_shutdown(\"batchlog manager\", [&bm] {\n                bm.stop().get();\n            });\n\n            supervisor::notify(\"starting load meter\");\n            load_meter.init(db, gossiper.local()).get();\n            auto stop_load_meter = defer_verbose_shutdown(\"load meter\", [&load_meter] {\n                load_meter.exit().get();\n            });\n\n            api::set_load_meter(ctx, load_meter).get();\n            auto stop_load_meter_api = defer_verbose_shutdown(\"load meter API\", [&ctx] {\n                api::unset_load_meter(ctx).get();\n            });\n\n            supervisor::notify(\"starting cf cache hit rate calculator\");\n            cf_cache_hitrate_calculator.start(std::ref(db), std::ref(gossiper)).get();\n            auto stop_cache_hitrate_calculator = defer_verbose_shutdown(\"cf cache hit rate calculator\",\n                    [&cf_cache_hitrate_calculator] {\n                        return cf_cache_hitrate_calculator.stop().get();\n                    }\n            );\n            cf_cache_hitrate_calculator.local().run_on(this_shard_id());\n\n            supervisor::notify(\"starting view update backlog broker\");\n            static sharded<service::view_update_backlog_broker> view_backlog_broker;\n            view_backlog_broker.start(std::ref(proxy), std::ref(gossiper)).get();\n            view_backlog_broker.invoke_on_all(&service::view_update_backlog_broker::start).get();\n            auto stop_view_backlog_broker = defer_verbose_shutdown(\"view update backlog broker\", [] {\n                view_backlog_broker.stop().get();\n            });\n\n            if (!ss.local().raft_topology_change_enabled()) {\n                startlog.info(\"Waiting for gossip to settle before accepting client requests...\");\n                gossiper.local().wait_for_gossip_to_settle().get();\n            }\n\n            supervisor::notify(\"allow replaying hints\");\n            proxy.invoke_on_all(&service::storage_proxy::allow_replaying_hints).get();\n\n            api::set_hinted_handoff(ctx, proxy).get();\n            auto stop_hinted_handoff_api = defer_verbose_shutdown(\"hinted handoff API\", [&ctx] {\n                api::unset_hinted_handoff(ctx).get();\n            });\n\n            if (cfg->view_building()) {\n                supervisor::notify(\"Launching generate_mv_updates for non system tables\");\n                with_scheduling_group(maintenance_scheduling_group, [] {\n                    return view_update_generator.invoke_on_all(&db::view::view_update_generator::start);\n                }).get();\n            }\n\n            if (cfg->view_building()) {\n                view_builder.invoke_on_all(&db::view::view_builder::start, std::ref(mm), utils::cross_shard_barrier()).get();\n            }\n\n            api::set_server_view_builder(ctx, view_builder).get();\n            auto stop_vb_api = defer_verbose_shutdown(\"view builder API\", [&ctx] {\n                api::unset_server_view_builder(ctx).get();\n            });\n\n            sharded<alternator::expiration_service> es;\n            std::any stop_expiration_service;\n\n            if (cfg->alternator_port() || cfg->alternator_https_port()) {\n                // Start the expiration service on all shards.\n                // Currently we only run it if Alternator is enabled, because\n                // only Alternator uses it for its TTL feature. But in the\n                // future if we add a CQL interface to it, we may want to\n                // start this outside the Alternator if().\n                supervisor::notify(\"starting the expiration service\");\n                es.start(seastar::sharded_parameter([] (const replica::database& db) { return db.as_data_dictionary(); }, std::ref(db)),\n                         std::ref(proxy), std::ref(gossiper)).get();\n                stop_expiration_service = defer_verbose_shutdown(\"expiration service\", [&es] {\n                    es.stop().get();\n                });\n                with_scheduling_group(maintenance_scheduling_group, [&es] {\n                    return es.invoke_on_all(&alternator::expiration_service::start);\n                }).get();\n            }\n\n            db.invoke_on_all(&replica::database::revert_initial_system_read_concurrency_boost).get();\n            notify_set.notify_all(configurable::system_state::started).get();\n            seastar::set_abort_on_ebadf(cfg->abort_on_ebadf());\n            api::set_server_done(ctx).get();\n\n            // Create controllers before drain_on_shutdown() below, so that it destructs\n            // after drain stops them in stop_transport()\n            // Register controllers after drain_on_shutdown() below, so that even on start\n            // failure drain is called and stops controllers\n            cql_transport::controller cql_server_ctl(auth_service, mm_notifier, gossiper, qp, service_memory_limiter, sl_controller, lifecycle_notifier, *cfg, cql_sg_stats_key, maintenance_socket_enabled::no, dbcfg.statement_scheduling_group);\n\n            api::set_server_service_levels(ctx, cql_server_ctl, qp).get();\n\n            alternator::controller alternator_ctl(gossiper, proxy, mm, sys_dist_ks, cdc_generation_service, service_memory_limiter, auth_service, sl_controller, *cfg, dbcfg.statement_scheduling_group);\n            redis::controller redis_ctl(proxy, auth_service, mm, *cfg, gossiper, dbcfg.statement_scheduling_group);\n\n            // Register at_exit last, so that storage_service::drain_on_shutdown will be called first\n            auto do_drain = defer_verbose_shutdown(\"local storage\", [&ss] {\n                ss.local().drain_on_shutdown().get();\n            });\n\n            auth_service.local().ensure_superuser_is_created().get();\n            ss.local().register_protocol_server(cql_server_ctl, cfg->start_native_transport()).get();\n            api::set_transport_controller(ctx, cql_server_ctl).get();\n            auto stop_transport_controller = defer_verbose_shutdown(\"transport controller API\", [&ctx] {\n                api::unset_transport_controller(ctx).get();\n            });\n\n            api::set_thrift_controller(ctx).get();\n            auto stop_thrift_controller = defer_verbose_shutdown(\"thrift controller API\", [&ctx] {\n                api::unset_thrift_controller(ctx).get();\n            });\n\n#ifndef SCYLLA_BUILD_MODE_RELEASE\n            api::set_server_cql_server_test(ctx, cql_server_ctl).get();\n            auto stop_cql_server_test_api = defer_verbose_shutdown(\"cql server API\", [&ctx] {\n                api::unset_server_cql_server_test(ctx).get();\n            });\n#endif\n\n            if (bool enabled = cfg->alternator_port() || cfg->alternator_https_port()) {\n                ss.local().register_protocol_server(alternator_ctl, enabled).get();\n            }\n\n            if (bool enabled = cfg->redis_port() || cfg->redis_ssl_port()) {\n                ss.local().register_protocol_server(redis_ctl, enabled).get();\n            }\n\n            supervisor::notify(\"serving\");\n\n            startlog.info(\"Scylla version {} initialization completed.\", scylla_version());\n            if (after_init_func) {\n                after_init_func(cfg);\n            }\n            stop_signal.wait().get();\n            startlog.info(\"Signal received; shutting down\");\n\t    // At this point, all objects destructors and all shutdown hooks registered with defer() are executed\n          } catch (const sleep_aborted&) {\n            startlog.info(\"Startup interrupted\");\n            // This happens when scylla gets SIGINT in the middle of join_cluster(), so\n            // just ignore it and exit normally\n            _exit(0);\n            return 0;\n          } catch (const abort_requested_exception&) {\n            startlog.info(\"Startup interrupted\");\n            // This happens when scylla gets SIGINT in the middle of join_cluster(), so\n            // just ignore it and exit normally\n            _exit(0);\n            return 0;\n          } catch (...) {\n            startlog.error(\"Startup failed: {}\", std::current_exception());\n            // We should be returning 1 here, but the system is not yet prepared for orderly rollback of main() objects\n            // and thread_local variables.\n            _exit(1);\n            return 1;\n          }\n          startlog.info(\"Scylla version {} shutdown complete.\", scylla_version());\n\n          // With -fprofile-generate, LLVM inserts an exit hook which saves the profile counters to disk.\n          // So does BOLT's instrumentation.\n          // But since we exit abruptly and skip those hooks, we have to trigger the dump manually.\n          dump_performance_profiles();\n\n          // We should be returning 0 here, but the system is not yet prepared for orderly rollback of main() objects\n          // and thread_local variables.\n          _exit(0);\n          return 0;\n        });\n    });\n  } catch (...) {\n      // reactor may not have been initialized, so can't use logger\n      fmt::print(std::cerr, \"FATAL: Exception during startup, aborting: {}\\n\", std::current_exception());\n      return 7; // 1 has a special meaning for upstart\n  }\n}\n\nint main(int ac, char** av) {\n    // early check to avoid triggering\n    if (!cpu_sanity()) {\n        _exit(71);\n    }\n\n    std::string exec_name;\n    if (ac >= 2) {\n        exec_name = av[1];\n    }\n\n    using main_func_type = std::function<int(int, char**)>;\n    struct tool {\n        std::string_view name;\n        main_func_type func;\n        std::string_view desc;\n    };\n    const tool tools[] = {\n        {\"server\", scylla_main, \"the scylladb server\"},\n        {\"types\", tools::scylla_types_main, \"a command-line tool to examine values belonging to scylla types\"},\n        {\"sstable\", tools::scylla_sstable_main, \"a multifunctional command-line tool to examine the content of sstables\"},\n        {\"nodetool\", tools::scylla_nodetool_main, \"a command-line tool to administer local or remote ScyllaDB nodes\"},\n        {\"perf-fast-forward\", perf::scylla_fast_forward_main, \"run performance tests by fast forwarding the reader on this server\"},\n        {\"perf-row-cache-update\", perf::scylla_row_cache_update_main, \"run performance tests by updating row cache on this server\"},\n        {\"perf-tablets\", perf::scylla_tablets_main, \"run performance tests of tablet metadata management\"},\n        {\"perf-load-balancing\", perf::scylla_tablet_load_balancing_main, \"run tablet load balancer tests\"},\n        {\"perf-simple-query\", perf::scylla_simple_query_main, \"run performance tests by sending simple queries to this server\"},\n        {\"perf-sstable\", perf::scylla_sstable_main, \"run performance tests by exercising sstable related operations on this server\"},\n        {\"perf-alternator\", perf::alternator(scylla_main, &after_init_func), \"run performance tests on full alternator stack\"}\n    };\n\n    main_func_type main_func;\n    if (exec_name.empty() || exec_name[0] == '-') {\n        main_func = scylla_main;\n    } else if (auto tool = std::ranges::find_if(tools, [exec_name] (auto& tool) {\n                               return tool.name == exec_name;\n                           });\n               tool != std::ranges::end(tools)) {\n        main_func = tool->func;\n        // shift args to consume the recognized tool name\n        std::shift_left(av + 1, av + ac, 1);\n        --ac;\n    } else {\n        fmt::print(\"error: unrecognized first argument: expected it to be \\\"server\\\", a regular command-line argument or a valid tool name (see `scylla --list-tools`), but got {}\\n\", exec_name);\n        return 1;\n    }\n\n    // Even on the environment which causes errors during Scylla initialization,\n    // \"scylla --version\" should be able to run without error.\n    // To do so, we need to parse and execute these options before\n    // initializing Scylla/Seastar classes.\n    bpo::options_description preinit_description(\"Scylla options\");\n    bpo::variables_map preinit_vm;\n    preinit_description.add_options()\n        (\"version\", bpo::bool_switch(), \"print version number and exit\")\n        (\"build-id\", bpo::bool_switch(), \"print build-id and exit\")\n        (\"build-mode\", bpo::bool_switch(), \"print build mode and exit\")\n        (\"list-tools\", bpo::bool_switch(), \"list included tools and exit\");\n    auto preinit_parsed_opts = bpo::command_line_parser(ac, av).options(preinit_description).allow_unregistered().run();\n    bpo::store(preinit_parsed_opts, preinit_vm);\n    if (preinit_vm[\"version\"].as<bool>()) {\n        fmt::print(\"{}\\n\", scylla_version());\n        return 0;\n    }\n    if (preinit_vm[\"build-id\"].as<bool>()) {\n        fmt::print(\"{}\\n\", get_build_id());\n        return 0;\n    }\n    if (preinit_vm[\"build-mode\"].as<bool>()) {\n        fmt::print(\"{}\\n\", scylla_build_mode());\n        return 0;\n    }\n    if (preinit_vm[\"list-tools\"].as<bool>()) {\n        for (auto& tool : tools) {\n            fmt::print(\"{} - {}\\n\", tool.name, tool.desc);\n        }\n        return 0;\n    }\n\n    return main_func(ac, av);\n}\n"
        },
        {
          "name": "map_difference.hh",
          "type": "blob",
          "size": 2.0556640625,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <set>\n\ntemplate<typename Key>\nstruct map_difference {\n    // Entries in left map whose keys don't exist in the right map.\n    std::set<Key> entries_only_on_left;\n\n    // Entries in right map whose keys don't exist in the left map.\n    std::set<Key> entries_only_on_right;\n\n    // Entries that appear in both maps with the same value.\n    std::set<Key> entries_in_common;\n\n    // Entries that appear in both maps but have different values.\n    std::set<Key> entries_differing;\n\n    map_difference()\n        : entries_only_on_left{}\n        , entries_only_on_right{}\n        , entries_in_common{}\n        , entries_differing{}\n    { }\n};\n\n/**\n * Produces a map_difference between the two specified maps, with Key keys and\n * Tp values, using the provided equality function. In order to work with any\n * map type, such as std::map and std::unordered_map, Args holds the remaining\n * type parameters of the particular map type.\n */\ntemplate<template<typename...> class Map,\n         typename Key,\n         typename Tp,\n         typename Eq = std::equal_to<Tp>,\n         typename... Args>\ninline\nmap_difference<Key>\ndifference(const Map<Key, Tp, Args...>& left,\n           const Map<Key, Tp, Args...>& right,\n           Eq equals = Eq())\n{\n    map_difference<Key> diff{};\n    for (auto&& kv : right) {\n        diff.entries_only_on_right.emplace(kv.first);\n    }\n    for (auto&& kv : left) {\n        auto&& left_key = kv.first;\n        auto&& it = right.find(left_key);\n        if (it != right.end()) {\n            diff.entries_only_on_right.erase(left_key);\n            const Tp& left_value = kv.second;\n            const Tp& right_value = it->second;\n            if (equals(left_value, right_value)) {\n                diff.entries_in_common.emplace(left_key);\n            } else {\n                diff.entries_differing.emplace(left_key);\n            }\n        } else {\n            diff.entries_only_on_left.emplace(left_key);\n        }\n    }\n    return diff;\n}\n"
        },
        {
          "name": "marshal_exception.hh",
          "type": "blob",
          "size": 0.8486328125,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <seastar/core/sstring.hh>\n\n#include \"seastarx.hh\"\n\nclass marshal_exception : public std::exception {\n    sstring _why;\npublic:\n    marshal_exception() = delete;\n    marshal_exception(sstring why) : _why(sstring(\"marshaling error: \") + why) {}\n    virtual const char* what() const noexcept override { return _why.c_str(); }\n};\n\n// Speed up compilation of code using throw_with_backtrace<marshal_exception,\n// sstring> by compiling it only once (in types.cc), and elsewhere say that\n// it is extern and not compile it again.\nnamespace seastar {\ntemplate <class Exc, typename... Args> [[noreturn]] void throw_with_backtrace(Args&&... args);\nextern template void throw_with_backtrace<marshal_exception, sstring>(sstring&&);\n}\n"
        },
        {
          "name": "message",
          "type": "tree",
          "content": null
        },
        {
          "name": "multishard_mutation_query.cc",
          "type": "blob",
          "size": 42.2333984375,
          "content": "/*\n * Copyright (C) 2018-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"schema/schema_registry.hh\"\n#include \"multishard_mutation_query.hh\"\n#include \"mutation_query.hh\"\n#include \"replica/database.hh\"\n#include \"db/config.hh\"\n#include \"query-result-writer.hh\"\n#include \"query_result_merger.hh\"\n#include \"readers/multishard.hh\"\n\n#include <fmt/core.h>\n#include <seastar/core/coroutine.hh>\n#include <seastar/coroutine/as_future.hh>\n\n#include <fmt/ostream.h>\n\nlogging::logger mmq_log(\"multishard_mutation_query\");\n\ntemplate <typename T>\nusing foreign_unique_ptr = foreign_ptr<std::unique_ptr<T>>;\n\n/// Context object for a multishard read.\n///\n/// Handles logic related to looking up, creating, saving and cleaning up remote\n/// (shard) readers for the `multishard_mutation_reader`.\n/// Has a state machine for each of the shard readers. See the state transition\n/// diagram below, above the declaration of `reader state`.\n/// The `read_context` is a short-lived object that is only kept around for the\n/// duration of a single page. A new `read_context` is created on each page and\n/// is discarded at the end of the page, after the readers are either saved\n/// or the process of their safe disposal was started in the background.\n/// Intended usage:\n/// * Create the `read_context`.\n/// * Call `read_context::lookup_readers()` to find any saved readers from the\n///   previous page.\n/// * Create the `multishard_mutation_reader`.\n/// * Fill the page.\n/// * Destroy the `multishard_mutation_reader` to trigger the disposal of the\n///   shard readers.\n/// * Call `read_context::save_readers()` if the read didn't finish yet, that is\n///   more pages are expected.\n/// * Call `read_context::stop()` to initiate the cleanup of any unsaved readers\n///   and their dependencies.\n/// * Destroy the `read_context`.\n///\n/// Note:\n/// 1) Each step can only be started when the previous phase has finished.\n/// 2) This usage is implemented in the `do_query()` function below.\n/// 3) Both, `read_context::lookup_readers()` and `read_context::save_readers()`\n///    knows to do nothing when the query is not stateful and just short\n///    circuit.\nclass read_context : public reader_lifecycle_policy_v2 {\n\n    //              ( )    (O)\n    //               |      ^\n    //               |      |\n    //         +--- inexistent ---+\n    //         |                  |\n    //     (1) |              (3) |\n    //         |                  |\n    //  successful_lookup         |\n    //     |         |            |\n    //     |         |            |\n    //     |         |    (3)     |\n    //     |         +---------> used\n    // (2) |                      |\n    //     |                  (4) |\n    //     |                      |\n    //     +---------------> saving_state\n    //                            |\n    //                           (O)\n    //\n    //  1) lookup_readers()\n    //  2) save_readers()\n    //  3) create_reader()\n    //  4) destroy_reader()\n    enum class reader_state {\n        inexistent,\n        successful_lookup,\n        used,\n        saving,\n    };\n\n    struct reader_meta {\n        struct remote_parts {\n            reader_permit permit;\n            lw_shared_ptr<const dht::partition_range> range;\n            std::unique_ptr<const query::partition_slice> slice;\n            utils::phased_barrier::operation read_operation;\n            std::optional<reader_concurrency_semaphore::inactive_read_handle> handle;\n            std::optional<mutation_reader::tracked_buffer> buffer;\n\n            remote_parts(\n                    reader_permit permit,\n                    lw_shared_ptr<const dht::partition_range> range = nullptr,\n                    std::unique_ptr<const query::partition_slice> slice = nullptr,\n                    utils::phased_barrier::operation read_operation = {},\n                    std::optional<reader_concurrency_semaphore::inactive_read_handle> handle = {})\n                : permit(std::move(permit))\n                , range(std::move(range))\n                , slice(std::move(slice))\n                , read_operation(std::move(read_operation))\n                , handle(std::move(handle)) {\n            }\n        };\n\n        reader_state state = reader_state::inexistent;\n        foreign_unique_ptr<remote_parts> rparts;\n        std::optional<mutation_reader::tracked_buffer> dismantled_buffer;\n\n        reader_meta() = default;\n\n        // Remote constructor.\n        reader_meta(reader_state s, std::optional<remote_parts> rp = {})\n            : state(s) {\n            if (rp) {\n                rparts = make_foreign(std::make_unique<remote_parts>(std::move(*rp)));\n            }\n        }\n\n        mutation_reader::tracked_buffer& get_dismantled_buffer(const reader_permit& permit) {\n            if (!dismantled_buffer) {\n                dismantled_buffer.emplace(permit);\n            }\n            return *dismantled_buffer;\n        }\n    };\n\n    struct dismantle_buffer_stats {\n        size_t partitions = 0;\n        size_t fragments = 0;\n        size_t bytes = 0;\n        size_t discarded_partitions = 0;\n        size_t discarded_fragments = 0;\n        size_t discarded_bytes = 0;\n\n        void add(const mutation_fragment_v2& mf) {\n            partitions += unsigned(mf.is_partition_start());\n            ++fragments;\n            bytes += mf.memory_usage();\n        }\n        void add(const schema& s, const range_tombstone_change& rtc) {\n            ++fragments;\n            bytes += rtc.memory_usage(s);\n        }\n        void add(const schema& s, const static_row& sr) {\n            ++fragments;\n            bytes += sr.memory_usage(s);\n        }\n        void add(const schema& s, const partition_start& ps) {\n            ++partitions;\n            ++fragments;\n            bytes += ps.memory_usage(s);\n        }\n        void add_discarded(const mutation_fragment_v2& mf) {\n            discarded_partitions += unsigned(mf.is_partition_start());\n            ++discarded_fragments;\n            discarded_bytes += mf.memory_usage();\n        }\n        void add_discarded(const schema& s, const range_tombstone_change& rtc) {\n            ++discarded_fragments;\n            discarded_bytes += rtc.memory_usage(s);\n        }\n        void add_discarded(const schema& s, const static_row& sr) {\n            ++discarded_fragments;\n            discarded_bytes += sr.memory_usage(s);\n        }\n        void add_discarded(const schema& s, const partition_start& ps) {\n            ++discarded_partitions;\n            ++discarded_fragments;\n            discarded_bytes += ps.memory_usage(s);\n        }\n    };\n\n    distributed<replica::database>& _db;\n    schema_ptr _schema;\n    locator::effective_replication_map_ptr _erm;\n    reader_permit _permit;\n    const query::read_command& _cmd;\n    const dht::partition_range_vector& _ranges;\n    tracing::trace_state_ptr _trace_state;\n\n    // One for each shard. Index is shard id.\n    std::vector<reader_meta> _readers;\n    std::vector<reader_concurrency_semaphore*> _semaphores;\n\n    static std::string_view reader_state_to_string(reader_state rs);\n\n    dismantle_buffer_stats dismantle_combined_buffer(mutation_reader::tracked_buffer combined_buffer, const dht::decorated_key& pkey);\n    dismantle_buffer_stats dismantle_compaction_state(detached_compaction_state compaction_state);\n    future<> save_reader(shard_id shard, full_position_view last_pos);\n\n    friend fmt::formatter<dismantle_buffer_stats>;\n\npublic:\n    read_context(distributed<replica::database>& db, schema_ptr s, locator::effective_replication_map_ptr erm,\n                 const query::read_command& cmd, const dht::partition_range_vector& ranges,\n                 tracing::trace_state_ptr trace_state, db::timeout_clock::time_point timeout)\n            : _db(db)\n            , _schema(std::move(s))\n            , _erm(std::move(erm))\n            , _permit(_db.local().get_reader_concurrency_semaphore().make_tracking_only_permit(_schema, \"multishard-mutation-query\", timeout, trace_state))\n            , _cmd(cmd)\n            , _ranges(ranges)\n            , _trace_state(std::move(trace_state))\n            , _semaphores(smp::count, nullptr) {\n        _readers.resize(smp::count);\n        _permit.set_max_result_size(get_max_result_size());\n\n        if (!_erm->get_replication_strategy().is_vnode_based()) {\n            // The algorithm is full of assumptions about shard assignment being round-robin, static, and full.\n            // This does not hold for tablets. We chose to avoid this algorithm rather than adapting it.\n            // The coordinator should split ranges accordingly.\n            on_internal_error(mmq_log, format(\"multishard reader cannot be used on tables with non-static sharding: {}.{}\",\n                              _schema->ks_name(), _schema->cf_name()));\n        }\n    }\n\n    read_context(read_context&&) = delete;\n    read_context(const read_context&) = delete;\n\n    read_context& operator=(read_context&&) = delete;\n    read_context& operator=(const read_context&) = delete;\n\n    distributed<replica::database>& db() {\n        return _db;\n    }\n\n    reader_permit permit() const {\n        return _permit;\n    }\n\n    const locator::effective_replication_map_ptr& erm() const {\n        return _erm;\n    }\n\n    query::max_result_size get_max_result_size() {\n        return _cmd.max_result_size ? *_cmd.max_result_size : _db.local().get_query_max_result_size();\n    }\n\n    virtual mutation_reader create_reader(\n            schema_ptr schema,\n            reader_permit permit,\n            const dht::partition_range& pr,\n            const query::partition_slice& ps,\n            tracing::trace_state_ptr trace_state,\n            mutation_reader::forwarding fwd_mr) override;\n\n    virtual const dht::partition_range* get_read_range() const override;\n\n    virtual void update_read_range(lw_shared_ptr<const dht::partition_range> range) override;\n\n    virtual future<> destroy_reader(stopped_reader reader) noexcept override;\n\n    virtual reader_concurrency_semaphore& semaphore() override {\n        const auto shard = this_shard_id();\n        if (!_semaphores[shard]) {\n            _semaphores[shard] = &_db.local().get_reader_concurrency_semaphore();\n        }\n        return *_semaphores[shard];\n    }\n\n    virtual future<reader_permit> obtain_reader_permit(schema_ptr schema, const char* const description, db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr) override {\n        const auto shard = this_shard_id();\n        auto& rm = _readers[shard];\n        if (rm.state == reader_state::successful_lookup) {\n            rm.rparts->permit.set_max_result_size(get_max_result_size());\n            co_return rm.rparts->permit;\n        }\n        auto permit = co_await _db.local().obtain_reader_permit(std::move(schema), description, timeout, std::move(trace_ptr));\n        permit.set_max_result_size(get_max_result_size());\n        co_return permit;\n    }\n\n    future<> lookup_readers(db::timeout_clock::time_point timeout) noexcept;\n\n    future<> save_readers(mutation_reader::tracked_buffer unconsumed_buffer, std::optional<detached_compaction_state> compaction_state,\n            full_position last_pos) noexcept;\n\n    future<> stop();\n};\n\ntemplate <> struct fmt::formatter<read_context::dismantle_buffer_stats> : fmt::formatter<string_view> {\n    auto format(const read_context::dismantle_buffer_stats& s, fmt::format_context& ctx) const {\n        return fmt::format_to(ctx.out(),\n                              \"kept {} partitions/{} fragments/{} bytes, discarded {} partitions/{} fragments/{} bytes\",\n                              s.partitions,\n                              s.fragments,\n                              s.bytes,\n                              s.discarded_partitions,\n                              s.discarded_fragments,\n                              s.discarded_bytes);\n    }\n};\n\nstd::string_view read_context::reader_state_to_string(reader_state rs) {\n    switch (rs) {\n        case reader_state::inexistent:\n            return \"inexistent\";\n        case reader_state::successful_lookup:\n            return \"successful_lookup\";\n        case reader_state::used:\n            return \"used\";\n        case reader_state::saving:\n            return \"saving\";\n    }\n    // If we got here, we are logging an error anyway, so the above layers\n    // (should) have detected the invalid state.\n    return \"invalid\";\n}\n\nmutation_reader read_context::create_reader(\n        schema_ptr schema,\n        reader_permit permit,\n        const dht::partition_range& pr,\n        const query::partition_slice& ps,\n        tracing::trace_state_ptr trace_state,\n        mutation_reader::forwarding fwd_mr) {\n    const auto shard = this_shard_id();\n    auto& rm = _readers[shard];\n\n    if (rm.state != reader_state::used && rm.state != reader_state::successful_lookup && rm.state != reader_state::inexistent) {\n        auto msg = seastar::format(\"Unexpected request to create reader for shard {}.\"\n                \" The reader is expected to be in either `used`, `successful_lookup` or `inexistent` state,\"\n                \" but is in `{}` state instead.\", shard, reader_state_to_string(rm.state));\n        mmq_log.warn(\"{}\", msg);\n        throw std::logic_error(msg.c_str());\n    }\n\n    // The reader is either in inexistent or successful lookup state.\n    if (rm.state == reader_state::successful_lookup) {\n        if (auto reader_opt = semaphore().unregister_inactive_read(std::move(*rm.rparts->handle))) {\n            rm.state = reader_state::used;\n            // The saved reader permit is expected to be the same one passed to create_reader,\n            // as returned from obtain_reader_permit()\n            if (reader_opt->permit() != permit) {\n                on_internal_error(mmq_log, \"read_context::create_reader(): passed-in permit is different than saved reader's permit\");\n            }\n            permit.set_trace_state(trace_state);\n            return std::move(*reader_opt);\n        }\n    }\n\n    auto& table = _db.local().find_column_family(schema);\n\n    auto remote_parts = reader_meta::remote_parts(\n            std::move(permit),\n            make_lw_shared<const dht::partition_range>(pr),\n            std::make_unique<const query::partition_slice>(ps),\n            table.read_in_progress());\n\n    if (!rm.rparts) {\n        rm.rparts = make_foreign(std::make_unique<reader_meta::remote_parts>(std::move(remote_parts)));\n    } else {\n        *rm.rparts = std::move(remote_parts);\n    }\n\n    rm.state = reader_state::used;\n\n    return table.as_mutation_source().make_reader_v2(std::move(schema), rm.rparts->permit, *rm.rparts->range, *rm.rparts->slice,\n            std::move(trace_state), streamed_mutation::forwarding::no, fwd_mr);\n}\n\nconst dht::partition_range* read_context::get_read_range() const {\n    auto& rm = _readers[this_shard_id()];\n    if (rm.rparts) {\n        return rm.rparts->range.get();\n    }\n    return nullptr;\n}\n\nvoid read_context::update_read_range(lw_shared_ptr<const dht::partition_range> range) {\n    auto& rm = _readers[this_shard_id()];\n    rm.rparts->range = std::move(range);\n}\n\nfuture<> read_context::destroy_reader(stopped_reader reader) noexcept {\n    auto& rm = _readers[this_shard_id()];\n\n    if (rm.state == reader_state::used) {\n        rm.state = reader_state::saving;\n        rm.rparts->handle = std::move(reader.handle);\n        rm.rparts->buffer = std::move(reader.unconsumed_fragments);\n    } else {\n        mmq_log.warn(\n                \"Unexpected request to dismantle reader in state `{}`.\"\n                \" Reader was not created nor is in the process of being created.\",\n                reader_state_to_string(rm.state));\n    }\n    return make_ready_future<>();\n}\n\nfuture<> read_context::stop() {\n    return parallel_for_each(smp::all_cpus(), [this] (unsigned shard) {\n        if (_readers[shard].rparts) {\n            return _db.invoke_on(shard, [&rparts_fptr = _readers[shard].rparts] (replica::database& db) mutable {\n                auto rparts = rparts_fptr.release();\n                if (rparts->handle) {\n                    auto reader_opt = rparts->permit.semaphore().unregister_inactive_read(std::move(*rparts->handle));\n                    if (reader_opt) {\n                        return reader_opt->close().then([rparts = std::move(rparts)] { });\n                    }\n                }\n                return make_ready_future<>();\n            });\n        }\n        return make_ready_future<>();\n    });\n}\n\nread_context::dismantle_buffer_stats read_context::dismantle_combined_buffer(mutation_reader::tracked_buffer combined_buffer,\n        const dht::decorated_key& pkey) {\n    auto& sharder = _erm->get_sharder(*_schema);\n\n    std::vector<mutation_fragment_v2> tmp_buffer;\n    dismantle_buffer_stats stats;\n\n    auto rit = std::reverse_iterator(combined_buffer.end());\n    const auto rend = std::reverse_iterator(combined_buffer.begin());\n    for (;rit != rend; ++rit) {\n        if (rit->is_partition_start()) {\n            const auto shard = sharder.shard_for_reads(rit->as_partition_start().key().token());\n\n            // It is possible that the reader this partition originates from\n            // does not exist anymore. Either because we failed stopping it or\n            // because it was evicted.\n            if (_readers[shard].state != reader_state::saving) {\n                for (auto& smf : tmp_buffer) {\n                    stats.add_discarded(smf);\n                }\n                stats.add_discarded(*rit);\n                tmp_buffer.clear();\n                continue;\n            }\n\n            auto& shard_buffer = _readers[shard].get_dismantled_buffer(_permit);\n            for (auto& smf : tmp_buffer) {\n                stats.add(smf);\n                shard_buffer.emplace_front(std::move(smf));\n            }\n            stats.add(*rit);\n            shard_buffer.emplace_front(std::move(*rit));\n            tmp_buffer.clear();\n        } else {\n            tmp_buffer.emplace_back(std::move(*rit));\n        }\n    }\n\n    const auto shard = sharder.shard_for_reads(pkey.token());\n    auto& shard_buffer = _readers[shard].get_dismantled_buffer(_permit);\n    for (auto& smf : tmp_buffer) {\n        stats.add(smf);\n        shard_buffer.emplace_front(std::move(smf));\n    }\n\n    return stats;\n}\n\nread_context::dismantle_buffer_stats read_context::dismantle_compaction_state(detached_compaction_state compaction_state) {\n    auto stats = dismantle_buffer_stats();\n    auto& sharder = _erm->get_sharder(*_schema);\n    const auto shard = sharder.shard_for_reads(compaction_state.partition_start.key().token());\n\n    auto& rtc_opt = compaction_state.current_tombstone;\n\n    // It is possible that the reader this partition originates from does not\n    // exist anymore. Either because we failed stopping it or because it was\n    // evicted.\n    if (_readers[shard].state != reader_state::saving) {\n        if (rtc_opt) {\n            stats.add_discarded(*_schema, *rtc_opt);\n        }\n        if (compaction_state.static_row) {\n            stats.add_discarded(*_schema, *compaction_state.static_row);\n        }\n        stats.add_discarded(*_schema, compaction_state.partition_start);\n        return stats;\n    }\n\n    auto& shard_buffer = _readers[shard].get_dismantled_buffer(_permit);\n\n    if (rtc_opt) {\n        stats.add(*_schema, *rtc_opt);\n        shard_buffer.emplace_front(*_schema, _permit, std::move(*rtc_opt));\n    }\n\n    if (compaction_state.static_row) {\n        stats.add(*_schema, *compaction_state.static_row);\n        shard_buffer.emplace_front(*_schema, _permit, std::move(*compaction_state.static_row));\n    }\n\n    stats.add(*_schema, compaction_state.partition_start);\n    shard_buffer.emplace_front(*_schema, _permit, std::move(compaction_state.partition_start));\n\n    return stats;\n}\n\nfuture<> read_context::save_reader(shard_id shard, full_position_view last_pos) {\n  return do_with(std::exchange(_readers[shard], {}), [this, shard, last_pos] (reader_meta& rm) mutable {\n    return _db.invoke_on(shard, [query_uuid = _cmd.query_uuid, query_ranges = _ranges, &rm,\n            last_pos, gts = tracing::global_trace_state_ptr(_trace_state)] (replica::database& db) mutable {\n        try {\n            auto rparts = rm.rparts.release(); // avoid another round-trip when destroying rparts\n            auto reader_opt = rparts->permit.semaphore().unregister_inactive_read(std::move(*rparts->handle));\n\n            if (!reader_opt) {\n                return make_ready_future<>();\n            }\n            mutation_reader_opt reader = std::move(*reader_opt);\n\n            size_t fragments = 0;\n            const auto size_before = reader->buffer_size();\n            const auto& schema = *reader->schema();\n\n            if (rparts->buffer) {\n                fragments += rparts->buffer->size();\n                auto rit = std::reverse_iterator(rparts->buffer->end());\n                auto rend = std::reverse_iterator(rparts->buffer->begin());\n                for (; rit != rend; ++rit) {\n                    reader->unpop_mutation_fragment(std::move(*rit));\n                }\n            }\n            if (rm.dismantled_buffer) {\n                fragments += rm.dismantled_buffer->size();\n                auto rit = std::reverse_iterator(rm.dismantled_buffer->cend());\n                auto rend = std::reverse_iterator(rm.dismantled_buffer->cbegin());\n                for (; rit != rend; ++rit) {\n                    // Copy the fragment, the buffer is on another shard.\n                    reader->unpop_mutation_fragment(mutation_fragment_v2(schema, rparts->permit, *rit));\n                }\n            }\n\n            const auto size_after = reader->buffer_size();\n\n            auto querier = query::shard_mutation_querier(\n                    std::move(query_ranges),\n                    std::move(rparts->range),\n                    std::move(rparts->slice),\n                    std::move(*reader),\n                    std::move(rparts->permit),\n                    last_pos);\n\n            db.get_querier_cache().insert_shard_querier(query_uuid, std::move(querier), gts.get());\n\n            db.get_stats().multishard_query_unpopped_fragments += fragments;\n            db.get_stats().multishard_query_unpopped_bytes += (size_after - size_before);\n            return make_ready_future<>();\n        } catch (...) {\n            // We don't want to fail a read just because of a failure to\n            // save any of the readers.\n            mmq_log.debug(\"Failed to save reader: {}\", std::current_exception());\n            ++db.get_stats().multishard_query_failed_reader_saves;\n            return make_ready_future<>();\n        }\n    }).handle_exception([this, shard] (std::exception_ptr e) {\n        // We don't want to fail a read just because of a failure to\n        // save any of the readers.\n        mmq_log.debug(\"Failed to save reader on shard {}: {}\", shard, e);\n        // This will account the failure on the local shard but we don't\n        // know where exactly the failure happened anyway.\n        ++_db.local().get_stats().multishard_query_failed_reader_saves;\n    });\n  });\n}\n\nfuture<> read_context::lookup_readers(db::timeout_clock::time_point timeout) noexcept {\n    if (!_cmd.query_uuid || _cmd.is_first_page) {\n        return make_ready_future<>();\n    }\n    try {\n        return _db.invoke_on_all([this, cmd = &_cmd, ranges = &_ranges, gs = global_schema_ptr(_schema),\n                gts = tracing::global_trace_state_ptr(_trace_state), timeout] (replica::database& db) mutable -> future<> {\n            auto schema = gs.get();\n            auto& table = db.find_column_family(schema);\n            auto& semaphore = this->semaphore();\n            auto shard = this_shard_id();\n\n            auto querier_opt = db.get_querier_cache().lookup_shard_mutation_querier(cmd->query_uuid, *schema, *ranges, cmd->slice, semaphore, gts.get(), timeout);\n\n            if (!querier_opt) {\n                _readers[shard] = reader_meta(reader_state::inexistent);\n                co_return;\n            }\n\n            auto& q = *querier_opt;\n            auto handle = semaphore.register_inactive_read(std::move(q).reader());\n            _readers[shard] = reader_meta(\n                    reader_state::successful_lookup,\n                    reader_meta::remote_parts(q.permit(), std::move(q).reader_range(), std::move(q).reader_slice(), table.read_in_progress(),\n                            std::move(handle)));\n        });\n    } catch (...) {\n        return current_exception_as_future();\n    }\n}\n\nfuture<> read_context::save_readers(mutation_reader::tracked_buffer unconsumed_buffer, std::optional<detached_compaction_state> compaction_state,\n            full_position last_pos) noexcept {\n    if (!_cmd.query_uuid) {\n        co_return;\n    }\n\n    const auto cb_stats = dismantle_combined_buffer(std::move(unconsumed_buffer), dht::decorate_key(*_schema, last_pos.partition));\n    tracing::trace(_trace_state, \"Dismantled combined buffer: {}\", cb_stats);\n\n    auto cs_stats = dismantle_buffer_stats{};\n    if (compaction_state) {\n        cs_stats = dismantle_compaction_state(std::move(*compaction_state));\n        tracing::trace(_trace_state, \"Dismantled compaction state: {}\", cs_stats);\n    } else {\n        tracing::trace(_trace_state, \"No compaction state to dismantle, partition exhausted\", cs_stats);\n    }\n\n    co_await parallel_for_each(std::views::iota(0u, smp::count), [this, &last_pos] (shard_id shard) {\n        auto& rm = _readers[shard];\n        if (rm.state == reader_state::successful_lookup || rm.state == reader_state::saving) {\n            return save_reader(shard, last_pos);\n        }\n\n        return make_ready_future<>();\n    });\n}\n\nnamespace {\n\ntemplate <typename ResultBuilder>\nrequires std::is_nothrow_move_constructible_v<typename ResultBuilder::result_type>\nstruct page_consume_result {\n    typename ResultBuilder::result_type result;\n    mutation_reader::tracked_buffer unconsumed_fragments;\n    lw_shared_ptr<compact_for_query_state_v2> compaction_state;\n\n    page_consume_result(typename ResultBuilder::result_type&& result, mutation_reader::tracked_buffer&& unconsumed_fragments,\n            lw_shared_ptr<compact_for_query_state_v2>&& compaction_state) noexcept\n        : result(std::move(result))\n        , unconsumed_fragments(std::move(unconsumed_fragments))\n        , compaction_state(std::move(compaction_state)) {\n    }\n};\n\n// A special-purpose multi-range reader for multishard reads\n//\n// It is different from the \"stock\" multi-range reader\n// (make_flat_multi_range_reader()) in the following ways:\n// * It guarantees that a buffer never crosses two ranges.\n// * It guarantees that after calling `fill_buffer()` the underlying reader's\n//   buffer's *entire* content is moved into its own buffer. In other words,\n//   calling detach_buffer() after fill_buffer() is guranted to get all\n//   fragments fetched in that call, none will be left in the underlying\n//   reader's one.\nclass multi_range_reader : public mutation_reader::impl {\n    mutation_reader _reader;\n    dht::partition_range_vector::const_iterator _it;\n    const dht::partition_range_vector::const_iterator _end;\n\npublic:\n    multi_range_reader(schema_ptr s, reader_permit permit, mutation_reader rd, const dht::partition_range_vector& ranges)\n        : impl(std::move(s), std::move(permit)) , _reader(std::move(rd)) , _it(ranges.begin()) , _end(ranges.end()) { }\n\n    virtual future<> fill_buffer() override {\n        if (is_end_of_stream()) {\n            co_return;\n        }\n        while (is_buffer_empty()) {\n            if (_reader.is_buffer_empty() && _reader.is_end_of_stream()) {\n                if (++_it == _end) {\n                    _end_of_stream = true;\n                    break;\n                } else {\n                    co_await _reader.fast_forward_to(*_it);\n                }\n            }\n            if (_reader.is_buffer_empty()) {\n                co_await _reader.fill_buffer();\n            }\n            _reader.move_buffer_content_to(*this);\n        }\n    }\n\n    virtual future<> fast_forward_to(const dht::partition_range&) override {\n        return make_exception_future<>(make_backtraced_exception_ptr<std::bad_function_call>());\n    }\n\n    virtual future<> fast_forward_to(position_range) override {\n        return make_exception_future<>(make_backtraced_exception_ptr<std::bad_function_call>());\n    }\n\n    virtual future<> next_partition() override {\n        clear_buffer_to_next_partition();\n        if (is_buffer_empty() && !is_end_of_stream()) {\n            return _reader.next_partition();\n        }\n        return make_ready_future<>();\n    }\n\n    virtual future<> close() noexcept override {\n        return _reader.close();\n    }\n};\n\n\n} // anonymous namespace\n\ntemplate <typename ResultBuilder>\nfuture<page_consume_result<ResultBuilder>> read_page(\n        shared_ptr<read_context> ctx,\n        schema_ptr s,\n        const query::read_command& cmd,\n        const dht::partition_range_vector& ranges,\n        tracing::trace_state_ptr trace_state,\n        noncopyable_function<ResultBuilder()> result_builder_factory) {\n    auto compaction_state = make_lw_shared<compact_for_query_state_v2>(*s, cmd.timestamp, cmd.slice, cmd.get_row_limit(),\n            cmd.partition_limit);\n\n    auto reader = make_multishard_combining_reader_v2(ctx, s, ctx->erm(), ctx->permit(), ranges.front(), cmd.slice,\n            trace_state, mutation_reader::forwarding(ranges.size() > 1));\n    if (ranges.size() > 1) {\n        reader = make_mutation_reader<multi_range_reader>(s, ctx->permit(), std::move(reader), ranges);\n    }\n\n    // Use coroutine::as_future to prevent exception on timesout.\n    auto f = co_await coroutine::as_future(query::consume_page(reader, compaction_state, cmd.slice, result_builder_factory(), cmd.get_row_limit(),\n                cmd.partition_limit, cmd.timestamp));\n    if (!f.failed()) {\n        // no exceptions are thrown in this block\n        auto result = std::move(f).get();\n        if (compaction_state->are_limits_reached() || result.is_short_read()) {\n            ResultBuilder::maybe_set_last_position(result, compaction_state->current_full_position());\n        }\n        const auto& cstats = compaction_state->stats();\n        tracing::trace(trace_state, \"Page stats: {} partition(s), {} static row(s) ({} live, {} dead), {} clustering row(s) ({} live, {} dead) and {} range tombstone(s)\",\n                cstats.partitions,\n                cstats.static_rows.total(),\n                cstats.static_rows.live,\n                cstats.static_rows.dead,\n                cstats.clustering_rows.total(),\n                cstats.clustering_rows.live,\n                cstats.clustering_rows.dead,\n                cstats.range_tombstones);\n        auto buffer = reader.detach_buffer();\n        co_await reader.close();\n        // page_consume_result cannot fail so there's no risk of double-closing reader.\n        co_return page_consume_result<ResultBuilder>(std::move(result), std::move(buffer), std::move(compaction_state));\n    }\n\n    co_await reader.close();\n    co_return coroutine::exception(f.get_exception());\n}\n\ntemplate <typename ResultBuilder>\nfuture<foreign_ptr<lw_shared_ptr<typename ResultBuilder::result_type>>> do_query_vnodes(\n        distributed<replica::database>& db,\n        schema_ptr s,\n        const query::read_command& cmd,\n        const dht::partition_range_vector& ranges,\n        tracing::trace_state_ptr trace_state,\n        db::timeout_clock::time_point timeout,\n        noncopyable_function<ResultBuilder()> result_builder_factory) {\n    auto& table = db.local().find_column_family(s);\n    auto erm = table.get_effective_replication_map();\n    auto ctx = seastar::make_shared<read_context>(db, s, erm, cmd, ranges, trace_state, timeout);\n\n    // Use coroutine::as_future to prevent exception on timesout.\n    auto f = co_await coroutine::as_future(ctx->lookup_readers(timeout).then([&, result_builder_factory = std::move(result_builder_factory)] () mutable {\n        return read_page<ResultBuilder>(ctx, s, cmd, ranges, trace_state, std::move(result_builder_factory));\n    }).then([&] (page_consume_result<ResultBuilder> r) -> future<foreign_ptr<lw_shared_ptr<typename ResultBuilder::result_type>>> {\n        if (r.compaction_state->are_limits_reached() || r.result.is_short_read()) {\n            // Must call before calling `detach_state()`.\n            auto last_pos = *r.compaction_state->current_full_position();\n            co_await ctx->save_readers(std::move(r.unconsumed_fragments), std::move(*r.compaction_state).detach_state(), std::move(last_pos));\n        }\n        co_return make_foreign(make_lw_shared<typename ResultBuilder::result_type>(std::move(r.result)));\n    }));\n    co_await ctx->stop();\n    if (f.failed()) {\n        co_return coroutine::exception(f.get_exception());\n    }\n    co_return f.get();\n}\n\ntemplate <typename ResultBuilder>\nfuture<foreign_ptr<lw_shared_ptr<typename ResultBuilder::result_type>>> do_query_tablets(\n        distributed<replica::database>& db,\n        schema_ptr s,\n        const query::read_command& cmd,\n        const dht::partition_range_vector& ranges,\n        tracing::trace_state_ptr trace_state,\n        db::timeout_clock::time_point timeout,\n        noncopyable_function<ResultBuilder()> result_builder_factory) {\n    auto& table = db.local().find_column_family(s);\n    auto erm = table.get_effective_replication_map();\n    const auto& token_metadata = erm->get_token_metadata();\n    const auto& tablets = token_metadata.tablets().get_tablet_map(s->id());\n    const auto this_node_id = token_metadata.get_topology().this_node()->host_id();\n\n    auto query_cmd = cmd;\n    auto result_builder = result_builder_factory();\n\n    std::vector<foreign_ptr<lw_shared_ptr<typename ResultBuilder::result_type>>> results;\n    locator::tablet_range_splitter range_splitter{s, tablets, this_node_id, ranges};\n    while (auto range_opt = range_splitter()) {\n        auto& r = *results.emplace_back(co_await db.invoke_on(range_opt->shard,\n                    [&result_builder, gs = global_schema_ptr(s), &query_cmd, &range_opt, gts = tracing::global_trace_state_ptr(trace_state), timeout] (replica::database& db) {\n            return result_builder.query(db, gs, query_cmd, range_opt->range, gts, timeout);\n        }));\n\n        // Subtract result from limit, watch for underflow\n        query_cmd.partition_limit -= std::min(query_cmd.partition_limit, ResultBuilder::get_partition_count(r));\n        query_cmd.set_row_limit(query_cmd.get_row_limit() - std::min(query_cmd.get_row_limit(), ResultBuilder::get_row_count(r)));\n\n        if (!query_cmd.partition_limit || !query_cmd.get_row_limit() || r.is_short_read()) {\n            break;\n        }\n    }\n\n    co_return result_builder.merge(std::move(results));\n}\n\ntemplate <typename ResultBuilder>\nstatic future<std::tuple<foreign_ptr<lw_shared_ptr<typename ResultBuilder::result_type>>, cache_temperature>> do_query_on_all_shards(\n        distributed<replica::database>& db,\n        schema_ptr s,\n        const query::read_command& cmd,\n        const dht::partition_range_vector& ranges,\n        tracing::trace_state_ptr trace_state,\n        db::timeout_clock::time_point timeout,\n        std::function<ResultBuilder(query::result_memory_accounter&&)> result_builder_factory) {\n    if (cmd.get_row_limit() == 0 || cmd.slice.partition_row_limit() == 0 || cmd.partition_limit == 0) {\n        co_return std::tuple(\n                make_foreign(make_lw_shared<typename ResultBuilder::result_type>()),\n                db.local().find_column_family(s).get_global_cache_hit_rate());\n    }\n\n    auto& local_db = db.local();\n    auto& stats = local_db.get_stats();\n    const auto short_read_allowed = query::short_read(cmd.slice.options.contains<query::partition_slice::option::allow_short_read>());\n\n    const auto& keyspace = local_db.find_keyspace(s->ks_name());\n    auto query_method = keyspace.get_replication_strategy().uses_tablets() ? do_query_tablets<ResultBuilder> : do_query_vnodes<ResultBuilder>;\n\n    try {\n        auto accounter = co_await local_db.get_result_memory_limiter().new_mutation_read(*cmd.max_result_size, short_read_allowed);\n\n        auto result = co_await query_method(db, s, cmd, ranges, std::move(trace_state), timeout,\n                [result_builder_factory, accounter = std::move(accounter)] () mutable {\n\t\t\treturn result_builder_factory(std::move(accounter));\n\t\t});\n\n        ++stats.total_reads;\n        stats.short_mutation_queries += bool(result->is_short_read());\n        auto hit_rate = local_db.find_column_family(s).get_global_cache_hit_rate();\n        co_return std::tuple(std::move(result), hit_rate);\n    } catch (...) {\n        ++stats.total_reads_failed;\n        throw;\n    }\n}\n\nnamespace {\n\nclass mutation_query_result_builder {\npublic:\n    using result_type = reconcilable_result;\n\nprivate:\n    reconcilable_result_builder _builder;\n    schema_ptr _s;\n\npublic:\n    mutation_query_result_builder(const schema& s, const query::partition_slice& slice, query::result_memory_accounter&& accounter)\n        : _builder(s, slice, std::move(accounter))\n        , _s(s.shared_from_this())\n     { }\n\n    void consume_new_partition(const dht::decorated_key& dk) { _builder.consume_new_partition(dk); }\n    void consume(tombstone t) { _builder.consume(t); }\n    stop_iteration consume(static_row&& sr, tombstone t, bool is_alive) { return _builder.consume(std::move(sr), t, is_alive); }\n    stop_iteration consume(clustering_row&& cr, row_tombstone t, bool is_alive) { return _builder.consume(std::move(cr), t, is_alive); }\n    stop_iteration consume(range_tombstone_change&& rtc) { return _builder.consume(std::move(rtc)); }\n    stop_iteration consume_end_of_partition()  { return _builder.consume_end_of_partition(); }\n    result_type consume_end_of_stream() { return _builder.consume_end_of_stream(); }\n\n    future<foreign_ptr<lw_shared_ptr<result_type>>> query(\n            replica::database& db,\n            schema_ptr schema,\n            const query::read_command& cmd,\n            const dht::partition_range& range,\n            tracing::trace_state_ptr trace_state,\n            db::timeout_clock::time_point timeout) {\n        auto res = co_await db.query_mutations(std::move(schema), cmd, range, std::move(trace_state), timeout);\n        co_return make_foreign(make_lw_shared<result_type>(std::get<0>(std::move(res))));\n    }\n\n    foreign_ptr<lw_shared_ptr<result_type>> merge(std::vector<foreign_ptr<lw_shared_ptr<result_type>>> results) {\n        if (results.empty()) {\n            return make_foreign(make_lw_shared<result_type>());\n        }\n        auto& first = results.front();\n        for (auto it = results.begin() + 1; it != results.end(); ++it) {\n            first->merge_disjoint(_s, **it);\n        }\n        return std::move(first);\n    }\n\n    static void maybe_set_last_position(result_type& r, std::optional<full_position> full_position) { }\n    static uint32_t get_partition_count(result_type& r) { return r.partitions().size(); }\n    static uint64_t get_row_count(result_type& r) { return r.row_count(); }\n};\n\nclass data_query_result_builder {\npublic:\n    using result_type = query::result;\n\nprivate:\n    std::unique_ptr<query::result::builder> _res_builder;\n    query_result_builder _builder;\n    query::result_options _opts;\n\npublic:\n    data_query_result_builder(const schema& s, const query::partition_slice& slice, query::result_options opts,\n            query::result_memory_accounter&& accounter, uint64_t tombstone_limit)\n        : _res_builder(std::make_unique<query::result::builder>(slice, opts, std::move(accounter), tombstone_limit))\n        , _builder(s, *_res_builder)\n        , _opts(opts)\n    { }\n\n    void consume_new_partition(const dht::decorated_key& dk) { _builder.consume_new_partition(dk); }\n    void consume(tombstone t) { _builder.consume(t); }\n    stop_iteration consume(static_row&& sr, tombstone t, bool is_alive) { return _builder.consume(std::move(sr), t, is_alive); }\n    stop_iteration consume(clustering_row&& cr, row_tombstone t, bool is_alive) { return _builder.consume(std::move(cr), t, is_alive); }\n    stop_iteration consume(range_tombstone_change&& rtc) { return _builder.consume(std::move(rtc)); }\n    stop_iteration consume_end_of_partition()  { return _builder.consume_end_of_partition(); }\n    result_type consume_end_of_stream() {\n        _builder.consume_end_of_stream();\n        return _res_builder->build();\n    }\n\n    future<foreign_ptr<lw_shared_ptr<result_type>>> query(\n            replica::database& db,\n            schema_ptr schema,\n            const query::read_command& cmd,\n            const dht::partition_range& range,\n            tracing::trace_state_ptr trace_state,\n            db::timeout_clock::time_point timeout) {\n        dht::partition_range_vector ranges;\n        ranges.emplace_back(range);\n        auto res = co_await db.query(std::move(schema), cmd, _opts, ranges, std::move(trace_state), timeout);\n        co_return std::get<0>(std::move(res));\n    }\n\n    foreign_ptr<lw_shared_ptr<result_type>> merge(std::vector<foreign_ptr<lw_shared_ptr<result_type>>> results) {\n        if (results.empty()) {\n            return make_foreign(make_lw_shared<result_type>());\n        }\n        query::result_merger merger(query::max_rows, query::max_partitions);\n        merger.reserve(results.size());\n        for (auto&& r: results) {\n            merger(std::move(r));\n        }\n        return merger.get();\n    }\n\n    static void maybe_set_last_position(result_type& r, std::optional<full_position> full_position) {\n        r.set_last_position(std::move(full_position));\n    }\n    static uint32_t get_partition_count(result_type& r) {\n        r.ensure_counts();\n        return *r.partition_count();\n    }\n    static uint64_t get_row_count(result_type& r) {\n        r.ensure_counts();\n        return *r.row_count();\n    }\n};\n\n} // anonymous namespace\n\nfuture<std::tuple<foreign_ptr<lw_shared_ptr<reconcilable_result>>, cache_temperature>> query_mutations_on_all_shards(\n        distributed<replica::database>& db,\n        schema_ptr query_schema,\n        const query::read_command& cmd,\n        const dht::partition_range_vector& ranges,\n        tracing::trace_state_ptr trace_state,\n        db::timeout_clock::time_point timeout) {\n    return do_query_on_all_shards<mutation_query_result_builder>(db, query_schema, cmd, ranges, std::move(trace_state), timeout,\n            [query_schema, &cmd] (query::result_memory_accounter&& accounter) {\n        return mutation_query_result_builder(*query_schema, cmd.slice, std::move(accounter));\n    });\n}\n\nfuture<std::tuple<foreign_ptr<lw_shared_ptr<query::result>>, cache_temperature>> query_data_on_all_shards(\n        distributed<replica::database>& db,\n        schema_ptr query_schema,\n        const query::read_command& cmd,\n        const dht::partition_range_vector& ranges,\n        query::result_options opts,\n        tracing::trace_state_ptr trace_state,\n        db::timeout_clock::time_point timeout) {\n    return do_query_on_all_shards<data_query_result_builder>(db, query_schema, cmd, ranges, std::move(trace_state), timeout,\n            [query_schema, &cmd, opts] (query::result_memory_accounter&& accounter) {\n        return data_query_result_builder(*query_schema, cmd.slice, opts, std::move(accounter), cmd.tombstone_limit);\n    });\n}\n"
        },
        {
          "name": "multishard_mutation_query.hh",
          "type": "blob",
          "size": 3.27734375,
          "content": "/*\n * Copyright (C) 2018-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"replica/database_fwd.hh\"\n#include \"schema/schema_fwd.hh\"\n#include \"cache_temperature.hh\"\n#include \"db/timeout_clock.hh\"\n#include \"dht/i_partitioner_fwd.hh\"\n\n#include <seastar/core/distributed.hh>\n\n#include \"seastarx.hh\"\n\nclass reconcilable_result;\n\nnamespace query {\n\nclass read_command;\nclass result;\nclass result_options;\n\n} // namespace query\n\nnamespace tracing {\n    class trace_state_ptr;\n} // namespace tracing\n\n/// Run the mutation query on all shards.\n///\n/// Under the hood it uses a multishard_combining_reader for reading the\n/// range(s) from all shards.\n///\n/// The query uses paging. The read will stop after reaching one of the page\n/// size limits. Page size is determined by the read_command (row and partition\n/// limits) and by the max_size parameter (max memory size of results).\n///\n/// Optionally the query can be stateful. This means that after filling the\n/// page, the shard readers are saved in the `querier_cache` on their home shard\n/// (wrapped in a `shard_mutation_querier`). Fragments already read from\n/// the shard readers, but not consumed by the results builder (due to\n/// reaching the limit), are extracted from the `multishard_combining_reader`'s\n/// (and the foreign readers wrapping the shard readers) buffers and pushed back\n/// into the shard reader they originated from. This way only the shard readers\n/// have to be cached in order to continue the query.\n/// When reading the next page these querier objects are looked up from\n/// their respective shard's `querier_cache`, instead of creating new shard\n/// readers.\n/// To enable stateful queries set the `query_uuid` field of the read command\n/// to an id unique to the query. This can be easily achieved by generating a\n/// random uuid with `utils::make_random_uuid()`.\n/// It is advisable that the `is_first_page` flag of the read command is set on\n/// the first page of the query so that a pointless lookup is avoided.\n///\n/// Note: params passed by reference are expected to be kept alive by the caller\n/// for the duration of the query. Params passed by const reference are expected\n/// to *not* change during the query, as they will possibly be accessed from\n/// other shards.\n///\n/// \\see multishard_combined_reader\n/// \\see querier_cache\nfuture<std::tuple<foreign_ptr<lw_shared_ptr<reconcilable_result>>, cache_temperature>> query_mutations_on_all_shards(\n        distributed<replica::database>& db,\n        schema_ptr s,\n        const query::read_command& cmd,\n        const dht::partition_range_vector& ranges,\n        tracing::trace_state_ptr trace_state,\n        db::timeout_clock::time_point timeout);\n\n/// Run the data query on all shards.\n///\n/// Identical to `query_mutations_on_all_shards()` except that it builds results\n/// in the `query::result` format instead of in the `reconcilable_result` one.\nfuture<std::tuple<foreign_ptr<lw_shared_ptr<query::result>>, cache_temperature>> query_data_on_all_shards(\n        distributed<replica::database>& db,\n        schema_ptr s,\n        const query::read_command& cmd,\n        const dht::partition_range_vector& ranges,\n        query::result_options opts,\n        tracing::trace_state_ptr trace_state,\n        db::timeout_clock::time_point timeout);\n"
        },
        {
          "name": "mutation",
          "type": "tree",
          "content": null
        },
        {
          "name": "mutation_query.cc",
          "type": "blob",
          "size": 3.3134765625,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include <seastar/coroutine/maybe_yield.hh>\n\n#include \"mutation_query.hh\"\n#include \"schema/schema_registry.hh\"\n\n#include <boost/range/algorithm/equal.hpp>\n\nreconcilable_result::~reconcilable_result() {}\n\nreconcilable_result::reconcilable_result()\n    : _row_count_low_bits(0)\n    , _row_count_high_bits(0)\n{ }\n\nreconcilable_result::reconcilable_result(uint32_t row_count_low_bits, utils::chunked_vector<partition> p, query::short_read short_read,\n                                         uint32_t row_count_high_bits, query::result_memory_tracker memory_tracker)\n    : _row_count_low_bits(row_count_low_bits)\n    , _short_read(short_read)\n    , _memory_tracker(std::move(memory_tracker))\n    , _partitions(std::move(p))\n    , _row_count_high_bits(row_count_high_bits)\n{ }\n\nreconcilable_result::reconcilable_result(uint64_t row_count, utils::chunked_vector<partition> p, query::short_read short_read,\n                                         query::result_memory_tracker memory_tracker)\n    : reconcilable_result(static_cast<uint32_t>(row_count), std::move(p), short_read, static_cast<uint32_t>(row_count >> 32), std::move(memory_tracker))\n{ }\n\nconst utils::chunked_vector<partition>& reconcilable_result::partitions() const {\n    return _partitions;\n}\n\nutils::chunked_vector<partition>& reconcilable_result::partitions() {\n    return _partitions;\n}\n\nbool\nreconcilable_result::operator==(const reconcilable_result& other) const {\n    return boost::equal(_partitions, other._partitions);\n}\n\nvoid\nreconcilable_result::merge_disjoint(schema_ptr schema, const reconcilable_result& other) {\n    std::copy(other._partitions.begin(), other._partitions.end(), std::back_inserter(_partitions));\n    _short_read = _short_read || other._short_read;\n    uint64_t row_count = this->row_count() + other.row_count();\n    _row_count_low_bits = static_cast<uint32_t>(row_count);\n    _row_count_high_bits = static_cast<uint32_t>(row_count >> 32);\n}\n\nauto fmt::formatter<reconcilable_result::printer>::format(\n    const reconcilable_result::printer& pr,\n    fmt::format_context& ctx) const -> decltype(ctx.out()) {\n    auto out = ctx.out();\n    out = fmt::format_to(out,\n                         \"{{rows={}, short_read={}, \",\n                         pr.self.row_count(),\n                         pr.self.is_short_read());\n    bool first = true;\n    for (const partition& p : pr.self.partitions()) {\n        if (!first) {\n            out = fmt::format_to(out, \", \");\n        }\n        first = false;\n        out = fmt::format_to(out,\n                             \"{{rows={}, {}}}\",\n                             p.row_count(),\n                             p._m.pretty_printer(pr.schema));\n    }\n    return fmt::format_to(out, \"]}}\");\n}\n\nreconcilable_result::printer reconcilable_result::pretty_printer(schema_ptr s) const {\n    return { *this, std::move(s) };\n}\n\nfuture<foreign_ptr<lw_shared_ptr<reconcilable_result>>> reversed(foreign_ptr<lw_shared_ptr<reconcilable_result>> result)\n{\n    for (auto& partition : result->partitions())\n    {\n        auto& m = partition.mut();\n        auto schema = local_schema_registry().get(m.schema_version());\n        m = frozen_mutation(reverse(m.unfreeze(schema)));\n        co_await coroutine::maybe_yield();\n    }\n\n    co_return std::move(result);\n}\n"
        },
        {
          "name": "mutation_query.hh",
          "type": "blob",
          "size": 6.62890625,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"query-request.hh\"\n#include \"query-result.hh\"\n#include \"mutation/frozen_mutation.hh\"\n#include \"db/timeout_clock.hh\"\n#include \"mutation/mutation.hh\"\n#include \"utils/chunked_vector.hh\"\n#include \"mutation/range_tombstone_assembler.hh\"\n\nclass reconcilable_result;\nclass frozen_reconcilable_result;\nclass mutation_source;\n\n// Can be read by other cores after publishing.\nstruct partition {\n    uint32_t _row_count_low_bits;\n    frozen_mutation _m; // FIXME: We don't need cf UUID, which frozen_mutation includes.\n    uint32_t _row_count_high_bits;\n    partition(uint32_t row_count_low_bits, frozen_mutation m, uint32_t row_count_high_bits)\n        : _row_count_low_bits(row_count_low_bits)\n        , _m(std::move(m))\n        , _row_count_high_bits(row_count_high_bits)\n    { }\n\n    partition(uint64_t row_count, frozen_mutation m)\n        : _row_count_low_bits(static_cast<uint32_t>(row_count))\n        , _m(std::move(m))\n        , _row_count_high_bits(static_cast<uint32_t>(row_count >> 32))\n    { }\n\n    uint32_t row_count_low_bits() const {\n        return _row_count_low_bits;\n    }\n\n    uint32_t row_count_high_bits() const {\n        return _row_count_high_bits;\n    }\n    \n    uint64_t row_count() const {\n        return (static_cast<uint64_t>(_row_count_high_bits) << 32) | _row_count_low_bits;\n    }\n\n    const frozen_mutation& mut() const {\n        return _m;\n    }\n\n    frozen_mutation& mut() {\n        return _m;\n    }\n\n\n    bool operator==(const partition& other) const {\n        return row_count() == other.row_count() && _m.representation() == other._m.representation();\n    }\n};\n\n// The partitions held by this object are ordered according to dht::decorated_key ordering and non-overlapping.\n// Each mutation must have different key.\n//\n// Can be read by other cores after publishing.\nclass reconcilable_result {\n    uint32_t _row_count_low_bits;\n    query::short_read _short_read;\n    query::result_memory_tracker _memory_tracker;\n    utils::chunked_vector<partition> _partitions;\n    uint32_t _row_count_high_bits;\npublic:\n    ~reconcilable_result();\n    reconcilable_result();\n    reconcilable_result(reconcilable_result&&) = default;\n    reconcilable_result& operator=(reconcilable_result&&) = default;\n    reconcilable_result(uint32_t row_count_low_bits, utils::chunked_vector<partition> partitions, query::short_read short_read,\n                        uint32_t row_count_high_bits, query::result_memory_tracker memory_tracker = { });\n    reconcilable_result(uint64_t row_count, utils::chunked_vector<partition> partitions, query::short_read short_read,\n                        query::result_memory_tracker memory_tracker = { });\n\n    const utils::chunked_vector<partition>& partitions() const;\n    utils::chunked_vector<partition>& partitions();\n\n    uint32_t row_count_low_bits() const {\n        return _row_count_low_bits;\n    }\n\n    uint32_t row_count_high_bits() const {\n        return _row_count_high_bits;\n    }\n\n    uint64_t row_count() const {\n        return (static_cast<uint64_t>(_row_count_high_bits) << 32) | _row_count_low_bits;\n    }\n\n    query::short_read is_short_read() const {\n        return _short_read;\n    }\n\n    size_t memory_usage() const {\n        return _memory_tracker.used_memory();\n    }\n\n    bool operator==(const reconcilable_result& other) const;\n\n    // other must be disjoint with this\n    // does not merge or update memory trackers\n    void merge_disjoint(schema_ptr schema, const reconcilable_result& other);\n\n    struct printer {\n        const reconcilable_result& self;\n        schema_ptr schema;\n        friend std::ostream& operator<<(std::ostream&, const printer&);\n    };\n\n    printer pretty_printer(schema_ptr) const;\n};\n\n// Reverse reconcilable_result by reversing mutations for all partitions.\nfuture<foreign_ptr<lw_shared_ptr<reconcilable_result>>> reversed(foreign_ptr<lw_shared_ptr<reconcilable_result>> result);\n\ntemplate <>\nstruct fmt::formatter<reconcilable_result::printer> {\n    constexpr auto parse(format_parse_context& ctx) { return ctx.begin(); }\n    auto format(const reconcilable_result::printer&, fmt::format_context& ctx) const\n        -> decltype(ctx.out());\n};\n\n\nclass reconcilable_result_builder {\n    schema_ptr _query_schema;\n    const query::partition_slice& _slice;\n\n    bool _return_static_content_on_partition_with_no_rows{};\n    bool _static_row_is_alive{};\n    uint64_t _total_live_rows = 0;\n    query::result_memory_accounter _memory_accounter;\n    stop_iteration _stop;\n    std::optional<streamed_mutation_freezer> _mutation_consumer;\n    range_tombstone_assembler _rt_assembler;\n\n    uint64_t _live_rows{};\n    // make this the last member so it is destroyed first. #7240\n    utils::chunked_vector<partition> _result;\n    size_t _used_at_entry;\n\nprivate:\n    stop_iteration consume(range_tombstone&& rt);\n\npublic:\n    // Expects reversed schema and reversed slice when building results for reverse query.\n    reconcilable_result_builder(const schema& query_schema, const query::partition_slice& slice,\n                                query::result_memory_accounter&& accounter) noexcept\n        : _query_schema(query_schema.shared_from_this()), _slice(slice)\n        , _memory_accounter(std::move(accounter))\n    { }\n\n    void consume_new_partition(const dht::decorated_key& dk);\n    void consume(tombstone t);\n    stop_iteration consume(static_row&& sr, tombstone, bool is_alive);\n    stop_iteration consume(clustering_row&& cr, row_tombstone, bool is_alive);\n    stop_iteration consume(range_tombstone_change&& rtc);\n    stop_iteration consume_end_of_partition();\n    reconcilable_result consume_end_of_stream();\n};\n\nfuture<query::result> to_data_query_result(\n        const reconcilable_result&,\n        schema_ptr,\n        const query::partition_slice&,\n        uint64_t row_limit,\n        uint32_t partition_limit,\n        query::result_options opts = query::result_options::only_result());\n\n// Query the content of the mutation.\n//\n// The mutation is destroyed in the process, see `mutation::consume()`.\nquery::result query_mutation(\n        mutation&& m,\n        const query::partition_slice& slice,\n        uint64_t row_limit = query::max_rows,\n        gc_clock::time_point now = gc_clock::now(),\n        query::result_options opts = query::result_options::only_result());\n\n// Performs a query for counter updates.\nfuture<mutation_opt> counter_write_query(schema_ptr, const mutation_source&, reader_permit permit,\n                                         const dht::decorated_key& dk,\n                                         const query::partition_slice& slice,\n                                         tracing::trace_state_ptr trace_ptr);\n\n"
        },
        {
          "name": "mutation_writer",
          "type": "tree",
          "content": null
        },
        {
          "name": "node_ops",
          "type": "tree",
          "content": null
        },
        {
          "name": "noexcept_traits.hh",
          "type": "blob",
          "size": 1.376953125,
          "content": "/*\n * Copyright 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include <type_traits>\n#include <memory>\n#include <seastar/core/future.hh>\n\n#include \"seastarx.hh\"\n\n#pragma once\n\n//\n// Utility for adapting types which are not nothrow move constructible into such\n// by wrapping them if necessary.\n//\n// Example usage:\n//\n//   T val{};\n//   using traits = noexcept_movable<T>;\n//   auto f = make_ready_future<typename traits::type>(traits::wrap(std::move(val)));\n//   T val2 = traits::unwrap(f.get());\n//\n\ntemplate<typename T>\nstruct noexcept_movable;\n\ntemplate<typename T>\nrequires std::is_nothrow_move_constructible_v<T>\nstruct noexcept_movable<T> {\n    using type = T;\n\n    static type wrap(T&& v) {\n        return std::move(v);\n    }\n\n    static future<T> wrap(future<T>&& v) {\n        return std::move(v);\n    }\n\n    static T unwrap(type&& v) {\n        return std::move(v);\n    }\n\n    static future<T> unwrap(future<type>&& v) {\n        return std::move(v);\n    }\n};\n\ntemplate<typename T>\nrequires (!std::is_nothrow_move_constructible_v<T>)\nstruct noexcept_movable<T> {\n    using type = std::unique_ptr<T>;\n\n    static type wrap(T&& v) {\n        return std::make_unique<T>(std::move(v));\n    }\n\n    static T unwrap(type&& v) {\n        return std::move(*v);\n    }\n};\n\ntemplate<typename T>\nusing noexcept_movable_t = typename noexcept_movable<T>::type;\n"
        },
        {
          "name": "partition_builder.hh",
          "type": "blob",
          "size": 2.6572265625,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"schema/schema.hh\"\n#include \"mutation/mutation_partition.hh\"\n#include \"mutation/mutation_partition_visitor.hh\"\n#include \"mutation/tombstone.hh\"\n#include \"mutation/atomic_cell.hh\"\n#include \"mutation/range_tombstone.hh\"\n#include \"collection_mutation.hh\"\n\n// Partition visitor which builds mutation_partition corresponding to the data its fed with.\nclass partition_builder final : public mutation_partition_visitor {\nprivate:\n    const schema& _schema;\n    mutation_partition& _partition;\n    deletable_row* _current_row;\npublic:\n    // @p will hold the result of building.\n    // @p must be empty.\n    partition_builder(const schema& s, mutation_partition& p)\n        : _schema(s)\n        , _partition(p)\n    { }\n\n    virtual void accept_partition_tombstone(tombstone t) override {\n        _partition.apply(t);\n    }\n\n    virtual void accept_static_cell(column_id id, atomic_cell_view cell) override {\n        auto& cdef = _schema.static_column_at(id);\n        accept_static_cell(id, atomic_cell(*cdef.type, cell));\n    }\n\n    void accept_static_cell(column_id id, atomic_cell&& cell) {\n        row& r = _partition.static_row().maybe_create();\n        r.append_cell(id, atomic_cell_or_collection(std::move(cell)));\n    }\n\n    virtual void accept_static_cell(column_id id, collection_mutation_view collection) override {\n        row& r = _partition.static_row().maybe_create();\n        r.append_cell(id, collection_mutation(*_schema.static_column_at(id).type, std::move(collection)));\n    }\n\n    virtual void accept_row_tombstone(const range_tombstone& rt) override {\n        _partition.apply_row_tombstone(_schema, rt);\n    }\n\n    virtual void accept_row(position_in_partition_view key, const row_tombstone& deleted_at, const row_marker& rm, is_dummy dummy, is_continuous continuous) override {\n        deletable_row& r = _partition.append_clustered_row(_schema, key, dummy, continuous);\n        r.apply(rm);\n        r.apply(deleted_at);\n        _current_row = &r;\n    }\n\n    virtual void accept_row_cell(column_id id, atomic_cell_view cell) override {\n        auto& cdef = _schema.regular_column_at(id);\n        accept_row_cell(id, atomic_cell(*cdef.type, cell));\n    }\n\n    void accept_row_cell(column_id id, atomic_cell&& cell) {\n        row& r = _current_row->cells();\n        r.append_cell(id, std::move(cell));\n    }\n\n    virtual void accept_row_cell(column_id id, collection_mutation_view collection) override {\n        row& r = _current_row->cells();\n        r.append_cell(id, collection_mutation(*_schema.regular_column_at(id).type, std::move(collection)));\n    }\n};\n"
        },
        {
          "name": "partition_range_compat.hh",
          "type": "blob",
          "size": 5.0458984375,
          "content": "/*\n * Copyright 2016-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n\n#pragma once\n\n#include <vector>\n#include \"interval.hh\"\n#include \"dht/ring_position.hh\"\n#include <boost/range/iterator_range_core.hpp>\n\nnamespace compat {\n\nusing wrapping_partition_range = wrapping_interval<dht::ring_position>;\n\n\n// unwraps a vector of wrapping ranges into a vector of nonwrapping ranges\n// if the vector happens to be sorted by the left bound, it remains sorted\ntemplate <typename T, typename Comparator>\nstd::vector<interval<T>>\nunwrap(std::vector<wrapping_interval<T>>&& v, Comparator&& cmp) {\n    std::vector<interval<T>> ret;\n    ret.reserve(v.size() + 1);\n    for (auto&& wr : v) {\n        if (wr.is_wrap_around(cmp)) {\n            auto&& p = std::move(wr).unwrap();\n            ret.insert(ret.begin(), interval<T>(std::move(p.first)));\n            ret.emplace_back(std::move(p.second));\n        } else {\n            ret.emplace_back(std::move(wr));\n        }\n    }\n    return ret;\n}\n\n// unwraps a vector of wrapping ranges into a vector of nonwrapping ranges\n// if the vector happens to be sorted by the left bound, it remains sorted\ntemplate <typename T, typename Comparator>\nstd::vector<interval<T>>\nunwrap(const std::vector<wrapping_interval<T>>& v, Comparator&& cmp) {\n    std::vector<interval<T>> ret;\n    ret.reserve(v.size() + 1);\n    for (auto&& wr : v) {\n        if (wr.is_wrap_around(cmp)) {\n            auto&& p = wr.unwrap();\n            ret.insert(ret.begin(), interval<T>(p.first));\n            ret.emplace_back(p.second);\n        } else {\n            ret.emplace_back(wr);\n        }\n    }\n    return ret;\n}\n\ntemplate <typename T>\nstd::vector<wrapping_interval<T>>\nwrap(const std::vector<interval<T>>& v) {\n    // re-wrap (-inf,x) ... (y, +inf) into (y, x):\n    if (v.size() >= 2 && !v.front().start() && !v.back().end()) {\n        auto ret = std::vector<wrapping_interval<T>>();\n        ret.reserve(v.size() - 1);\n        std::copy(v.begin() + 1, v.end() - 1, std::back_inserter(ret));\n        ret.emplace_back(v.back().start(), v.front().end());\n        return ret;\n    }\n    return v | std::ranges::to<std::vector<wrapping_interval<T>>>();\n}\n\ntemplate <typename T>\nstd::vector<wrapping_interval<T>>\nwrap(std::vector<interval<T>>&& v) {\n    // re-wrap (-inf,x) ... (y, +inf) into (y, x):\n    if (v.size() >= 2 && !v.front().start() && !v.back().end()) {\n        auto ret = std::vector<wrapping_interval<T>>();\n        ret.reserve(v.size() - 1);\n        std::move(v.begin() + 1, v.end() - 1, std::back_inserter(ret));\n        ret.emplace_back(std::move(v.back()).start(), std::move(v.front()).end());\n        return ret;\n    }\n    return std::ranges::owning_view(std::move(v)) | std::ranges::to<std::vector>();\n}\n\ninline\ndht::token_range_vector\nunwrap(const std::vector<wrapping_interval<dht::token>>& v) {\n    return unwrap(v, dht::token_comparator());\n}\n\ninline\ndht::token_range_vector\nunwrap(std::vector<wrapping_interval<dht::token>>&& v) {\n    return unwrap(std::move(v), dht::token_comparator());\n}\n\n\nclass one_or_two_partition_ranges : public std::pair<dht::partition_range, std::optional<dht::partition_range>> {\n    using pair = std::pair<dht::partition_range, std::optional<dht::partition_range>>;\npublic:\n    explicit one_or_two_partition_ranges(dht::partition_range&& f)\n        : pair(std::move(f), std::nullopt) {\n    }\n    explicit one_or_two_partition_ranges(dht::partition_range&& f, dht::partition_range&& s)\n        : pair(std::move(f), std::move(s)) {\n    }\n    operator dht::partition_range_vector() const & {\n        auto ret = dht::partition_range_vector();\n        // not reserving, since ret.size() is likely to be 1\n        ret.push_back(first);\n        if (second) {\n            ret.push_back(*second);\n        }\n        return ret;\n    }\n    operator dht::partition_range_vector() && {\n        auto ret = dht::partition_range_vector();\n        // not reserving, since ret.size() is likely to be 1\n        ret.push_back(std::move(first));\n        if (second) {\n            ret.push_back(std::move(*second));\n        }\n        return ret;\n    }\n};\n\ninline\none_or_two_partition_ranges\nunwrap(wrapping_partition_range pr, const schema& s) {\n    if (pr.is_wrap_around(dht::ring_position_comparator(s))) {\n        auto unw = std::move(pr).unwrap();\n        // Preserve ring order\n        return one_or_two_partition_ranges(\n                dht::partition_range(std::move(unw.second)),\n                dht::partition_range(std::move(unw.first)));\n    } else {\n        return one_or_two_partition_ranges(dht::partition_range(std::move(pr)));\n    }\n}\n\n// Unwraps `range` and calls `func` with its components, with an unwrapped\n// range type, as a parameter (once or twice)\ntemplate <typename T, typename Comparator, typename Func>\nvoid\nunwrap_into(wrapping_interval<T>&& range, const Comparator& cmp, Func&& func) {\n    if (range.is_wrap_around(cmp)) {\n        auto&& unw = range.unwrap();\n        // Preserve ring order\n        func(interval<T>(std::move(unw.second)));\n        func(interval<T>(std::move(unw.first)));\n    } else {\n        func(interval<T>(std::move(range)));\n    }\n}\n\n}\n"
        },
        {
          "name": "partition_slice_builder.cc",
          "type": "blob",
          "size": 5.310546875,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include <boost/range/algorithm/copy.hpp>\n#include <boost/range/algorithm_ext/push_back.hpp>\n\n#include \"partition_slice_builder.hh\"\n\npartition_slice_builder::partition_slice_builder(const schema& schema, query::partition_slice slice)\n    : _regular_columns(std::move(slice.regular_columns))\n    , _static_columns(std::move(slice.static_columns))\n    , _row_ranges(std::move(slice._row_ranges))\n    , _specific_ranges(std::move(slice._specific_ranges))\n    , _schema(schema)\n    , _options(std::move(slice.options))\n{\n}\n\npartition_slice_builder::partition_slice_builder(const schema& schema)\n    : _schema(schema)\n{\n    _options.set<query::partition_slice::option::send_partition_key>();\n    _options.set<query::partition_slice::option::send_clustering_key>();\n    _options.set<query::partition_slice::option::send_timestamp>();\n    _options.set<query::partition_slice::option::send_expiry>();\n}\n\nquery::partition_slice\npartition_slice_builder::build() {\n    std::vector<query::clustering_range> ranges;\n    if (_row_ranges) {\n        ranges = std::move(*_row_ranges);\n    } else {\n        ranges.emplace_back(query::clustering_range::make_open_ended_both_sides());\n    }\n\n    query::column_id_vector static_columns;\n    if (_static_columns) {\n        static_columns = std::move(*_static_columns);\n    } else {\n        static_columns =\n            _schema.static_columns() | std::views::transform(std::mem_fn(&column_definition::id)) | std::ranges::to<query::column_id_vector>();\n    }\n\n    query::column_id_vector regular_columns;\n    if (_regular_columns) {\n        regular_columns = std::move(*_regular_columns);\n    } else {\n        regular_columns = \n            _schema.regular_columns() | std::views::transform(std::mem_fn(&column_definition::id)) | std::ranges::to<query::column_id_vector>();\n    }\n\n    return {\n        std::move(ranges),\n        std::move(static_columns),\n        std::move(regular_columns),\n        std::move(_options),\n        std::move(_specific_ranges),\n        _partition_row_limit,\n    };\n}\n\npartition_slice_builder&\npartition_slice_builder::with_range(query::clustering_range range) {\n    if (!_row_ranges) {\n        _row_ranges = std::vector<query::clustering_range>();\n    }\n    _row_ranges->emplace_back(std::move(range));\n    return *this;\n}\n\npartition_slice_builder&\npartition_slice_builder::with_ranges(std::vector<query::clustering_range> ranges) {\n    if (!_row_ranges) {\n        _row_ranges = std::move(ranges);\n    } else {\n        for (auto&& r : ranges) {\n            with_range(std::move(r));\n        }\n    }\n    return *this;\n}\n\npartition_slice_builder&\npartition_slice_builder::mutate_ranges(std::function<void(std::vector<query::clustering_range>&)> func) {\n    if (_row_ranges) {\n        func(*_row_ranges);\n    }\n    return *this;\n}\n\npartition_slice_builder&\npartition_slice_builder::mutate_specific_ranges(std::function<void(query::specific_ranges&)> func) {\n    if (_specific_ranges) {\n        func(*_specific_ranges);\n    }\n    return *this;\n}\n\npartition_slice_builder&\npartition_slice_builder::set_specific_ranges(query::specific_ranges ranges) {\n    _specific_ranges = std::make_unique<query::specific_ranges>(std::move(ranges));\n    return *this;\n}\n\npartition_slice_builder&\npartition_slice_builder::with_no_regular_columns() {\n    _regular_columns = query::column_id_vector();\n    return *this;\n}\n\npartition_slice_builder&\npartition_slice_builder::with_regular_column(bytes name) {\n    if (!_regular_columns) {\n        _regular_columns = query::column_id_vector();\n    }\n\n    const column_definition* def = _schema.get_column_definition(name);\n    if (!def) {\n        throw std::runtime_error(format(\"No such column: {}\", _schema.regular_column_name_type()->to_string(name)));\n    }\n    if (!def->is_regular()) {\n        throw std::runtime_error(format(\"Column is not regular: {}\", _schema.column_name_type(*def)->to_string(name)));\n    }\n    _regular_columns->push_back(def->id);\n    return *this;\n}\n\npartition_slice_builder&\npartition_slice_builder::with_no_static_columns() {\n    _static_columns = query::column_id_vector();\n    return *this;\n}\n\npartition_slice_builder&\npartition_slice_builder::with_static_column(bytes name) {\n    if (!_static_columns) {\n        _static_columns = query::column_id_vector();\n    }\n\n    const column_definition* def = _schema.get_column_definition(name);\n    if (!def) {\n        throw std::runtime_error(format(\"No such column: {}\", utf8_type->to_string(name)));\n    }\n    if (!def->is_static()) {\n        throw std::runtime_error(format(\"Column is not static: {}\", utf8_type->to_string(name)));\n    }\n    _static_columns->push_back(def->id);\n    return *this;\n}\n\npartition_slice_builder&\npartition_slice_builder::reversed() {\n    _options.set<query::partition_slice::option::reversed>();\n    return *this;\n}\n\npartition_slice_builder&\npartition_slice_builder::without_partition_key_columns() {\n    _options.remove<query::partition_slice::option::send_partition_key>();\n    return *this;\n}\n\npartition_slice_builder&\npartition_slice_builder::without_clustering_key_columns() {\n    _options.remove<query::partition_slice::option::send_clustering_key>();\n    return *this;\n}\n\npartition_slice_builder& partition_slice_builder::with_partition_row_limit(uint64_t limit) {\n    _partition_row_limit = limit;\n    return *this;\n}\n"
        },
        {
          "name": "partition_slice_builder.hh",
          "type": "blob",
          "size": 2.5,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <optional>\n#include <vector>\n\n#include \"query-request.hh\"\n#include \"schema/schema_fwd.hh\"\n\n//\n// Fluent builder for query::partition_slice.\n//\n// Selects everything by default, unless restricted. Each property can be\n// restricted separately. For example, by default all static columns are\n// selected, but if with_static_column() is called then only that column will\n// be included. Still, all regular columns and the whole clustering range will\n// be selected (unless restricted).\n//\nclass partition_slice_builder {\n    std::optional<query::column_id_vector> _regular_columns;\n    std::optional<query::column_id_vector> _static_columns;\n    std::optional<std::vector<query::clustering_range>> _row_ranges;\n    std::unique_ptr<query::specific_ranges> _specific_ranges;\n    const schema& _schema;\n    query::partition_slice::option_set _options;\n    uint64_t _partition_row_limit = query::partition_max_rows;\npublic:\n    partition_slice_builder(const schema& schema);\n    partition_slice_builder(const schema& schema, query::partition_slice slice);\n\n    partition_slice_builder& with_static_column(bytes name);\n    partition_slice_builder& with_no_static_columns();\n    partition_slice_builder& with_regular_column(bytes name);\n    partition_slice_builder& with_no_regular_columns();\n    partition_slice_builder& with_range(query::clustering_range range);\n    partition_slice_builder& with_ranges(std::vector<query::clustering_range>);\n    // noop if no ranges have been set yet\n    partition_slice_builder& mutate_ranges(std::function<void(std::vector<query::clustering_range>&)>);\n    // noop if no specific ranges have been set yet\n    partition_slice_builder& mutate_specific_ranges(std::function<void(query::specific_ranges&)>);\n    partition_slice_builder& set_specific_ranges(query::specific_ranges);\n    partition_slice_builder& without_partition_key_columns();\n    partition_slice_builder& without_clustering_key_columns();\n    partition_slice_builder& reversed();\n    template <query::partition_slice::option OPTION>\n    partition_slice_builder& with_option() {\n        _options.set<OPTION>();\n        return *this;\n    }\n    template <query::partition_slice::option OPTION>\n    partition_slice_builder& with_option_toggled() {\n        _options.toggle<OPTION>();\n        return *this;\n    }\n\n    partition_slice_builder& with_partition_row_limit(uint64_t limit);\n\n    query::partition_slice build();\n};\n"
        },
        {
          "name": "partition_snapshot_reader.hh",
          "type": "blob",
          "size": 12.6474609375,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"mutation/partition_version.hh\"\n#include \"readers/mutation_reader_fwd.hh\"\n#include \"readers/mutation_reader.hh\"\n#include \"readers/range_tombstone_change_merger.hh\"\n#include \"clustering_key_filter.hh\"\n#include \"query-request.hh\"\n#include \"partition_snapshot_row_cursor.hh\"\n#include <any>\n\nextern seastar::logger mplog;\n\ntemplate <bool Reversing, typename Accounter>\nclass partition_snapshot_flat_reader : public mutation_reader::impl, public Accounter {\n    struct row_info {\n        mutation_fragment_v2 row;\n        tombstone rt_for_row;\n    };\n\n    // Represents a subset of mutations for some clustering key range.\n    //\n    // The range of the interval starts at the upper bound of the previous\n    // interval and its end depends on the contents of info:\n    //    - position_in_partition: holds the upper bound of the interval\n    //    - row_info: after_key(row_info::row.as_clustering_row().key())\n    //    - monostate: upper bound is the end of the current clustering key range\n    //\n    // All positions in query schema domain.\n    struct interval_info {\n        // Applies to the whole range of the interval.\n        tombstone range_tombstone;\n\n        // monostate means no more rows (end of range).\n        // position_in_partition means there is no row, it is the upper bound of the interval.\n        // if row_info, the upper bound is after_key(row_info::row.as_clustering_row().key()).\n        std::variant<row_info, position_in_partition, std::monostate> info;\n    };\n\n    // The part of the reader that accesses LSA memory directly and works\n    // with reclamation disabled. The state is either immutable (comparators,\n    // snapshot, references to region and alloc section) or dropped on any\n    // allocation section retry (_clustering_rows).\n    class lsa_partition_reader {\n        // _query_schema can be used to retrieve the clustering key order which is used\n        // for result ordering. This schema is passed from the query and is reversed iff\n        // the query was reversed (i.e. `Reversing==true`).\n        const schema& _query_schema;\n        reader_permit _permit;\n        partition_snapshot_ptr _snapshot;\n        logalloc::region& _region;\n        logalloc::allocating_section& _read_section;\n        partition_snapshot_row_cursor _cursor;\n        bool _digest_requested;\n        bool _done = false;\n    private:\n        template<typename Function>\n        decltype(auto) in_alloc_section(Function&& fn) {\n            return _read_section.with_reclaiming_disabled(_region, [&] {\n                return fn();\n            });\n        }\n    public:\n        explicit lsa_partition_reader(const schema& s, reader_permit permit, partition_snapshot_ptr snp,\n                                      logalloc::region& region, logalloc::allocating_section& read_section,\n                                      bool digest_requested)\n            : _query_schema(s)\n            , _permit(permit)\n            , _snapshot(std::move(snp))\n            , _region(region)\n            , _read_section(read_section)\n            , _cursor(s, *_snapshot, false, Reversing, digest_requested)\n            , _digest_requested(digest_requested)\n        { }\n\n        void on_new_range(position_in_partition_view lower_bound) {\n            in_alloc_section([&] {\n                _done = false;\n                _cursor.advance_to(lower_bound);\n                mplog.trace(\"on_new_range({}): {}\", lower_bound, _cursor);\n            });\n        }\n\n        template<typename Function>\n        decltype(auto) with_reserve(Function&& fn) {\n            return _read_section.with_reserve(_region, std::forward<Function>(fn));\n        }\n\n        tombstone partition_tombstone() {\n            logalloc::reclaim_lock guard(_region);\n            return _snapshot->partition_tombstone();\n        }\n\n        static_row get_static_row() {\n            return in_alloc_section([&] {\n                return _snapshot->static_row(_digest_requested);\n            });\n        }\n\n        // Returns mutations for the next interval in the range.\n        interval_info next_interval(const query::clustering_range& ck_range_query) {\n            return in_alloc_section([&]() -> interval_info {\n                position_in_partition::tri_compare cmp(_query_schema);\n\n                // Result is ignored because memtables don't lose information. If the entry is missing,\n                // it must have been redundant, and we can as well look at the next entry.\n                _cursor.maybe_refresh();\n\n                auto rt_before_row = _cursor.range_tombstone();\n                mplog.trace(\"next_interval(): range={}, rt={}, cursor={}\", ck_range_query, rt_before_row, _cursor);\n\n                if (_done || cmp(_cursor.position(), position_in_partition::for_range_end(ck_range_query)) >= 0) {\n                    mplog.trace(\"next_interval(): done\");\n                    return interval_info{rt_before_row, std::monostate{}};\n                }\n\n                if (_cursor.dummy()) {\n                    mplog.trace(\"next_interval(): pos={}, rt={}\", _cursor.position(), rt_before_row);\n                    auto res = interval_info{rt_before_row, position_in_partition(_cursor.position())};\n                    _done = !_cursor.next();\n                    return res;\n                }\n\n                tombstone rt_for_row = _cursor.range_tombstone_for_row();\n                mplog.trace(\"next_interval(): row, pos={}, rt={}, rt_for_row={}\", _cursor.position(), rt_before_row, rt_for_row);\n                auto result = mutation_fragment_v2(_query_schema, _permit, _cursor.row());\n                _done = !_cursor.next();\n                return interval_info{rt_before_row, row_info{std::move(result), rt_for_row}};\n            });\n        }\n    };\nprivate:\n    // Keeps shared pointer to the container we read mutation from to make sure\n    // that its lifetime is appropriately extended.\n    std::any _container_guard;\n\n    // Each range from _ck_ranges are taken to be in snapshot clustering key\n    // order, i.e. given a comparator derived from snapshot schema, for each ck_range from\n    // _ck_ranges, begin(ck_range) <= end(ck_range).\n    query::clustering_key_filter_ranges _ck_ranges;\n    query::clustering_row_ranges::const_iterator _current_ck_range;\n    query::clustering_row_ranges::const_iterator _ck_range_end;\n\n    std::optional<position_in_partition> _lower_bound;\n\n    // Last emitted range_tombstone_change.\n    tombstone _current_tombstone;\n\n    lsa_partition_reader _reader;\n    bool _static_row_done = false;\n\n    Accounter& accounter() {\n        return *this;\n    }\nprivate:\n    void push_static_row() {\n        auto sr = _reader.get_static_row();\n        if (!sr.empty()) {\n            emplace_mutation_fragment(mutation_fragment_v2(*_schema, _permit, std::move(sr)));\n        }\n    }\n\n    // We use the names ck_range_snapshot and ck_range_query to denote clustering order.\n    // ck_range_snapshot uses the snapshot order, while ck_range_query uses the\n    // query order. These two differ if the query was reversed (`Reversing==true`).\n    const query::clustering_range& current_ck_range_query() {\n        return *_current_ck_range;\n    }\n\n    void emit_next_interval() {\n        interval_info next = _reader.next_interval(current_ck_range_query());\n\n        if (next.range_tombstone != _current_tombstone) {\n            _current_tombstone = next.range_tombstone;\n            emplace_mutation_fragment(mutation_fragment_v2(*_schema, _permit,\n                range_tombstone_change(*_lower_bound, _current_tombstone)));\n        }\n\n        std::visit(make_visitor([&] (row_info&& info) {\n            auto pos_view = info.row.as_clustering_row().position();\n            _lower_bound = position_in_partition::after_key(*_schema, pos_view);\n            if (info.rt_for_row != _current_tombstone) {\n                _current_tombstone = info.rt_for_row;\n                emplace_mutation_fragment(mutation_fragment_v2(*_schema, _permit,\n                    range_tombstone_change(\n                        position_in_partition::before_key(info.row.as_clustering_row().key()), _current_tombstone)));\n            }\n            emplace_mutation_fragment(std::move(info.row));\n        }, [&] (position_in_partition&& pos) {\n            _lower_bound = std::move(pos);\n        }, [&] (std::monostate) {\n            if (_current_tombstone) {\n                _current_tombstone = {};\n                emplace_mutation_fragment(mutation_fragment_v2(*_schema, _permit,\n                    range_tombstone_change(position_in_partition_view::for_range_end(current_ck_range_query()), _current_tombstone)));\n            }\n            _current_ck_range = std::next(_current_ck_range);\n            on_new_range();\n        }), std::move(next.info));\n    }\n\n    void emplace_mutation_fragment(mutation_fragment_v2&& mfopt) {\n        mfopt.visit(accounter());\n        push_mutation_fragment(std::move(mfopt));\n    }\n\n    void on_new_range() {\n        if (_current_ck_range == _ck_range_end) {\n            _end_of_stream = true;\n            push_mutation_fragment(mutation_fragment_v2(*_schema, _permit, partition_end()));\n        } else {\n            _lower_bound = position_in_partition_view::for_range_start(current_ck_range_query());\n            _reader.on_new_range(*_lower_bound);\n        }\n    }\n\n    void do_fill_buffer() {\n        while (!is_end_of_stream() && !is_buffer_full()) {\n            emit_next_interval();\n            if (need_preempt()) {\n                break;\n            }\n        }\n    }\npublic:\n    template <typename... Args>\n    partition_snapshot_flat_reader(schema_ptr s, reader_permit permit, dht::decorated_key dk, partition_snapshot_ptr snp,\n                              query::clustering_key_filter_ranges crr, bool digest_requested,\n                              logalloc::region& region, logalloc::allocating_section& read_section,\n                              std::any pointer_to_container, Args&&... args)\n        : impl(std::move(s), std::move(permit))\n        , Accounter(std::forward<Args>(args)...)\n        , _container_guard(std::move(pointer_to_container))\n        , _ck_ranges(std::move(crr))\n        , _current_ck_range(_ck_ranges.begin())\n        , _ck_range_end(_ck_ranges.end())\n        , _reader(*_schema, _permit, std::move(snp), region, read_section, digest_requested)\n    {\n        _reader.with_reserve([&] {\n            push_mutation_fragment(*_schema, _permit, partition_start(std::move(dk), _reader.partition_tombstone()));\n        });\n    }\n\n    virtual future<> fill_buffer() override {\n        return do_until([this] { return is_end_of_stream() || is_buffer_full(); }, [this] {\n            _reader.with_reserve([&] {\n                if (!_static_row_done) {\n                    push_static_row();\n                    on_new_range();\n                    _static_row_done = true;\n                }\n                do_fill_buffer();\n            });\n            return make_ready_future<>();\n        });\n    }\n    virtual future<> next_partition() override {\n        clear_buffer_to_next_partition();\n        if (is_buffer_empty()) {\n            _end_of_stream = true;\n        }\n        return make_ready_future<>();\n    }\n    virtual future<> fast_forward_to(const dht::partition_range& pr) override {\n        throw std::runtime_error(\"This reader can't be fast forwarded to another partition.\");\n    };\n    virtual future<> fast_forward_to(position_range cr) override {\n        throw std::runtime_error(\"This reader can't be fast forwarded to another position.\");\n    };\n    virtual future<> close() noexcept override {\n        return make_ready_future<>();\n    }\n};\n\ntemplate <bool Reversing, typename Accounter, typename... Args>\ninline mutation_reader\nmake_partition_snapshot_flat_reader(schema_ptr s,\n                                    reader_permit permit,\n                                    dht::decorated_key dk,\n                                    query::clustering_key_filter_ranges crr,\n                                    partition_snapshot_ptr snp,\n                                    bool digest_requested,\n                                    logalloc::region& region,\n                                    logalloc::allocating_section& read_section,\n                                    std::any pointer_to_container,\n                                    streamed_mutation::forwarding fwd,\n                                    Args&&... args)\n{\n    auto res = make_mutation_reader<partition_snapshot_flat_reader<Reversing, Accounter>>(std::move(s), std::move(permit), std::move(dk),\n            snp, std::move(crr), digest_requested, region, read_section, std::move(pointer_to_container), std::forward<Args>(args)...);\n    if (fwd) {\n        return make_forwardable(std::move(res)); // FIXME: optimize\n    } else {\n        return res;\n    }\n}\n"
        },
        {
          "name": "partition_snapshot_row_cursor.hh",
          "type": "blob",
          "size": 35.044921875,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"mutation/partition_version.hh\"\n#include \"row_cache.hh\"\n#include \"utils/assert.hh\"\n#include \"utils/small_vector.hh\"\n#include <fmt/core.h>\n#include <ranges>\n\nclass partition_snapshot_row_cursor;\n\n// A non-owning reference to a row inside partition_snapshot which\n// maintains it's position and thus can be kept across reference invalidation points.\nclass partition_snapshot_row_weakref final {\n    mutation_partition::rows_type::iterator _it;\n    partition_snapshot::change_mark _change_mark;\n    position_in_partition _pos = position_in_partition::min();\n    bool _in_latest = false;\npublic:\n    partition_snapshot_row_weakref() = default;\n    // Makes this object point to a row pointed to by given partition_snapshot_row_cursor.\n    explicit partition_snapshot_row_weakref(const partition_snapshot_row_cursor&);\n    explicit partition_snapshot_row_weakref(std::nullptr_t) {}\n    partition_snapshot_row_weakref(partition_snapshot& snp, mutation_partition::rows_type::iterator it, bool in_latest)\n        : _it(it)\n        , _change_mark(snp.get_change_mark())\n        , _pos(it->position())\n        , _in_latest(in_latest)\n    { }\n    partition_snapshot_row_weakref& operator=(const partition_snapshot_row_cursor&);\n    partition_snapshot_row_weakref& operator=(std::nullptr_t) noexcept {\n        _change_mark = {};\n        return *this;\n    }\n    // Returns true iff the pointer is pointing at a row.\n    explicit operator bool() const { return _change_mark != partition_snapshot::change_mark(); }\npublic:\n    // Sets the iterator in latest version for the current position.\n    void set_latest(mutation_partition::rows_type::iterator it) {\n        _it = std::move(it);\n        _in_latest = true;\n    }\npublic:\n    // Returns the position of the row.\n    // Call only when pointing at a row.\n    const position_in_partition& position() const { return _pos; }\n    // Returns true iff the object is valid.\n    bool valid(partition_snapshot& snp) { return snp.get_change_mark() == _change_mark; }\n    // Call only when valid.\n    bool is_in_latest_version() const { return _in_latest; }\n    // Brings the object back to validity and returns true iff the snapshot contains the row.\n    // When not pointing at a row, returns false.\n    bool refresh(partition_snapshot& snp) {\n        auto snp_cm = snp.get_change_mark();\n        if (snp_cm == _change_mark) {\n            return true;\n        }\n        if (!_change_mark) {\n            return false;\n        }\n        _change_mark = snp_cm;\n        rows_entry::tri_compare cmp(*snp.schema());\n        _in_latest = true;\n        for (auto&& v : snp.versions()) {\n            auto rows = v.partition().clustered_rows();\n            _it = rows.find(_pos, cmp);\n            if (_it != rows.end()) {\n                return true;\n            }\n            _in_latest = false;\n        }\n        return false;\n    }\n    rows_entry* operator->() const {\n        return &*_it;\n    }\n    rows_entry& operator*() const {\n        return *_it;\n    }\n};\n\n// Allows iterating over rows of mutation_partition represented by given partition_snapshot.\n//\n// The cursor initially has a position before all rows and is not pointing at any row.\n// To position the cursor, use advance_to().\n//\n// All methods should be called with the region of the snapshot locked. The cursor is invalidated\n// when that lock section is left, or if the snapshot is modified.\n//\n// When the cursor is invalidated, it still maintains its previous position. It can be brought\n// back to validity by calling maybe_refresh(), or advance_to().\n//\n// Insertion of row entries after cursor's position invalidates the cursor.\n// Exceptions thrown from mutators invalidate the cursor.\n//\n// Range tombstone information is accessible via range_tombstone() and range_tombstone_for_row()\n// functions. range_tombstone() returns the tombstone for the interval which strictly precedes\n// the current row, and range_tombstone_for_row() returns the information for the row itself.\n// If the interval which precedes the row is not continuous, then range_tombstone() is empty.\n// If range_tombstone() is not empty then the interval is continuous.\nclass partition_snapshot_row_cursor final {\n    friend class partition_snapshot_row_weakref;\n    struct position_in_version {\n        mutation_partition::rows_type::iterator it;\n        utils::immutable_collection<mutation_partition::rows_type> rows;\n        int version_no;\n        const schema* schema;\n        bool unique_owner = false;\n        is_continuous continuous = is_continuous::no; // Range continuity in the direction of lower keys (in cursor schema domain).\n\n        // Range tombstone in the direction of lower keys (in cursor schema domain).\n        // Excludes the row. In the reverse mode, the row may have a different range tombstone.\n        tombstone rt;\n    };\n\n    const schema& _schema; // query domain\n    partition_snapshot& _snp;\n\n    // _heap contains iterators which are ahead of the cursor.\n    // _current_row contains iterators which are directly below the cursor.\n    utils::small_vector<position_in_version, 2> _heap; // query domain order\n    utils::small_vector<position_in_version, 2> _current_row;\n\n    // For !_reversed cursors points to the entry which\n    // is the lower_bound() of the current position in table schema order.\n    // For _reversed cursors it can be either lower_bound() in table order\n    // or lower_bound() in cursor's order, so should not be relied upon.\n    // if current entry is in the latest version then _latest_it points to it,\n    // also in _reversed mode.\n    std::optional<mutation_partition::rows_type::iterator> _latest_it;\n\n    // Continuity and range tombstone corresponding to ranges which are not represented in _heap because the cursor\n    // went pass all the entries in those versions.\n    bool _background_continuity = false;\n    tombstone _background_rt;\n\n    bool _continuous{};\n    bool _dummy{};\n    const bool _unique_owner;\n    const bool _reversed;\n    const bool _digest_requested;\n    tombstone _range_tombstone;\n    tombstone _range_tombstone_for_row;\n    position_in_partition _position; // table domain\n    partition_snapshot::change_mark _change_mark;\n\n    position_in_partition_view to_table_domain(position_in_partition_view pos) const {\n        if (_reversed) [[unlikely]] {\n            return pos.reversed();\n        }\n        return pos;\n    }\n\n    position_in_partition_view to_query_domain(position_in_partition_view pos) const {\n        if (_reversed) [[unlikely]] {\n            return pos.reversed();\n        }\n        return pos;\n    }\n\n    struct version_heap_less_compare {\n        rows_entry::tri_compare _cmp;\n        partition_snapshot_row_cursor& _cur;\n    public:\n        explicit version_heap_less_compare(partition_snapshot_row_cursor& cur)\n            : _cmp(cur._schema)\n            , _cur(cur)\n        { }\n\n        bool operator()(const position_in_version& a, const position_in_version& b) {\n            auto res = _cmp(_cur.to_query_domain(a.it->position()), _cur.to_query_domain(b.it->position()));\n            return res > 0 || (res == 0 && a.version_no > b.version_no);\n        }\n    };\n\n    // Removes the next row from _heap and puts it into _current_row\n    bool recreate_current_row() {\n        _current_row.clear();\n        _continuous = _background_continuity;\n        _range_tombstone = _background_rt;\n        _range_tombstone_for_row = _background_rt;\n        _dummy = true;\n        if (_heap.empty()) {\n            if (_reversed) {\n                _position = position_in_partition::before_all_clustered_rows();\n            } else {\n                _position = position_in_partition::after_all_clustered_rows();\n            }\n            return false;\n        }\n        version_heap_less_compare heap_less(*this);\n        position_in_partition::equal_compare eq(*_snp.schema());\n        do {\n            std::ranges::pop_heap(_heap, heap_less);\n            memory::on_alloc_point();\n            position_in_version& v = _heap.back();\n            rows_entry& e = *v.it;\n            if (_digest_requested) {\n                e.row().cells().prepare_hash(*v.schema, column_kind::regular_column);\n            }\n            _dummy &= bool(e.dummy());\n            _continuous |= bool(v.continuous);\n            _range_tombstone_for_row.apply(e.range_tombstone());\n            if (v.continuous) {\n                _range_tombstone.apply(v.rt);\n            }\n            _current_row.push_back(v);\n            _heap.pop_back();\n        } while (!_heap.empty() && eq(_current_row[0].it->position(), _heap[0].it->position()));\n\n        // FIXME: Optimize by dropping dummy() entries.\n        for (position_in_version& v : _heap) {\n            _continuous |= bool(v.continuous);\n            if (v.continuous) {\n                _range_tombstone.apply(v.rt);\n                _range_tombstone_for_row.apply(v.rt);\n            }\n        }\n\n        _position = position_in_partition(_current_row[0].it->position());\n        return true;\n    }\n\n    // lower_bound is in the query schema domain\n    void prepare_heap(position_in_partition_view lower_bound) {\n        lower_bound = to_table_domain(lower_bound);\n        memory::on_alloc_point();\n        rows_entry::tri_compare cmp(*_snp.schema());\n        version_heap_less_compare heap_less(*this);\n        _heap.clear();\n        _latest_it.reset();\n        _background_continuity = false;\n        _background_rt = {};\n        int version_no = 0;\n        bool unique_owner = _unique_owner;\n        bool first = true;\n        for (auto&& v : _snp.versions()) {\n            unique_owner = unique_owner && (first || !v.is_referenced());\n            auto rows = v.partition().clustered_rows();\n            auto pos = rows.lower_bound(lower_bound, cmp);\n            if (first) {\n                _latest_it = pos;\n            }\n            if (pos) {\n                is_continuous cont;\n                tombstone rt;\n                if (_reversed) [[unlikely]] {\n                    if (cmp(pos->position(), lower_bound) != 0) {\n                        cont = pos->continuous();\n                        rt = pos->range_tombstone();\n                        if (pos != rows.begin()) {\n                            --pos;\n                        } else {\n                            _background_continuity |= bool(cont);\n                            if (cont) {\n                                _background_rt = rt;\n                            }\n                            pos = {};\n                        }\n                    } else {\n                        auto next_entry = std::next(pos);\n                        if (next_entry == rows.end()) {\n                            // Positions past last dummy are complete since mutation sources\n                            // can't contain any keys which are larger.\n                            cont = is_continuous::yes;\n                            rt = {};\n                        } else {\n                            cont = next_entry->continuous();\n                            rt = next_entry->range_tombstone();\n                        }\n                    }\n                } else {\n                    cont = pos->continuous();\n                    rt = pos->range_tombstone();\n                }\n                if (pos) [[likely]] {\n                    _heap.emplace_back(position_in_version{pos, std::move(rows), version_no, v.get_schema().get(), unique_owner, cont, rt});\n                }\n            } else {\n                if (_reversed) [[unlikely]] {\n                    if (!rows.empty()) {\n                        pos = std::prev(rows.end());\n                    } else {\n                        _background_continuity = true;\n                    }\n                } else {\n                    _background_continuity = true; // Default continuity past the last entry\n                }\n                if (pos) [[likely]] {\n                    _heap.emplace_back(position_in_version{pos, std::move(rows), version_no, v.get_schema().get(), unique_owner, is_continuous::yes});\n                }\n            }\n            ++version_no;\n            first = false;\n        }\n        std::ranges::make_heap(_heap, heap_less);\n        _change_mark = _snp.get_change_mark();\n    }\n\n    // Advances the cursor to the next row.\n    // The @keep denotes whether the entries should be kept in partition version.\n    // If there is no next row, returns false and the cursor is no longer pointing at a row.\n    // Can be only called on a valid cursor pointing at a row.\n    // When throws, the cursor is invalidated and its position is not changed.\n    bool advance(bool keep) {\n        memory::on_alloc_point();\n        version_heap_less_compare heap_less(*this);\n        SCYLLA_ASSERT(iterators_valid());\n        for (auto&& curr : _current_row) {\n            if (!keep && curr.unique_owner) {\n                mutation_partition::rows_type::key_grabber kg(curr.it);\n                kg.release(current_deleter<rows_entry>());\n                if (_reversed && curr.it) [[unlikely]] {\n                    if (curr.rows.begin() == curr.it) {\n                        _background_continuity |= bool(curr.it->continuous());\n                        if (curr.it->continuous()) {\n                            _background_rt.apply(curr.it->range_tombstone());\n                        }\n                        curr.it = {};\n                    } else {\n                        curr.continuous = curr.it->continuous();\n                        curr.rt = curr.it->range_tombstone();\n                        --curr.it;\n                    }\n                }\n            } else {\n                if (_reversed) [[unlikely]] {\n                    if (curr.rows.begin() == curr.it) {\n                        _background_continuity |= bool(curr.it->continuous());\n                        if (curr.it->continuous()) {\n                            _background_rt.apply(curr.it->range_tombstone());\n                        }\n                        curr.it = {};\n                    } else {\n                        curr.continuous = curr.it->continuous();\n                        curr.rt = curr.it->range_tombstone();\n                        --curr.it;\n                    }\n                } else {\n                    ++curr.it;\n                    if (curr.it) {\n                        curr.continuous = curr.it->continuous();\n                        curr.rt = curr.it->range_tombstone();\n                    }\n                }\n            }\n            if (curr.it) {\n                if (curr.version_no == 0) {\n                    _latest_it = curr.it;\n                }\n                _heap.push_back(curr);\n                std::ranges::push_heap(_heap, heap_less);\n            }\n        }\n        return recreate_current_row();\n    }\n\n    bool is_in_latest_version() const noexcept { return at_a_row() && _current_row[0].version_no == 0; }\n\npublic:\n    // When reversed is true then the cursor will operate in reversed direction.\n    // When reversed, s must be a reversed schema relative to snp->schema()\n    // Positions and fragments accepted and returned by the cursor are from the domain of s.\n    // Iterators are from the table's schema domain.\n    partition_snapshot_row_cursor(const schema& s, partition_snapshot& snp, bool unique_owner = false, bool reversed = false, bool digest_requested = false)\n        : _schema(s)\n        , _snp(snp)\n        , _unique_owner(unique_owner)\n        , _reversed(reversed)\n        , _digest_requested(digest_requested)\n        , _position(position_in_partition::static_row_tag_t{})\n    { }\n\n    // If is_in_latest_version() then this returns an iterator to the entry under cursor in the latest version.\n    mutation_partition::rows_type::iterator get_iterator_in_latest_version() const {\n        SCYLLA_ASSERT(_latest_it);\n        return *_latest_it;\n    }\n\n    // Returns true iff the iterators obtained since the cursor was last made valid\n    // are still valid. Note that this doesn't mean that the cursor itself is valid.\n    bool iterators_valid() const {\n        return _snp.get_change_mark() == _change_mark;\n    }\n\n    // Marks the iterators as valid without refreshing them.\n    // Call only when the iterators are known to be valid.\n    void force_valid() {\n        _change_mark = _snp.get_change_mark();\n    }\n\n    // Advances cursor to the first entry with position >= pos, if such entry exists.\n    // If no such entry exists, the cursor is positioned at an extreme position in the direction of\n    // the cursor (min for reversed cursor, max for forward cursor) and not pointing at a row\n    // but still valid.\n    //\n    // continuous() is always valid after the call, even if not pointing at a row.\n    // Returns true iff the cursor is pointing at a row after the call.\n    bool maybe_advance_to(position_in_partition_view pos) {\n        prepare_heap(pos);\n        return recreate_current_row();\n    }\n\n    // Brings back the cursor to validity.\n    // Can be only called when cursor is pointing at a row.\n    //\n    // Semantically equivalent to:\n    //\n    //   advance_to(position());\n    //\n    // but avoids work if not necessary.\n    //\n    // Changes to attributes of the current row (e.g. continuity) don't have to be reflected.\n    bool maybe_refresh() {\n        if (!iterators_valid()) {\n            auto pos = position_in_partition(position()); // advance_to() modifies position() so copy\n            return advance_to(pos);\n        }\n        // Refresh latest version's iterator in case there was an insertion\n        // before it and after cursor's position. There cannot be any\n        // insertions for non-latest versions, so we don't have to update them.\n        if (!is_in_latest_version()) {\n            rows_entry::tri_compare cmp(*_snp.schema());\n            version_heap_less_compare heap_less(*this);\n            auto rows = _snp.version()->partition().clustered_rows();\n            bool match;\n            auto it = rows.lower_bound(_position, match, cmp);\n            _latest_it = it;\n            auto heap_i = std::ranges::find_if(_heap, [](auto&& v) { return v.version_no == 0; });\n\n            is_continuous cont;\n            tombstone rt;\n            if (it) {\n                cont = it->continuous();\n                rt = it->range_tombstone();\n                if (_reversed) [[unlikely]] {\n                    if (!match) {\n                        // lower_bound() in reverse order points to predecessor of it unless the keys are equal.\n                        if (it == rows.begin()) {\n                            if (it->continuous()) {\n                                _background_continuity = true;\n                                _background_rt.apply(it->range_tombstone());\n                            }\n                            it = {};\n                        } else {\n                            --it;\n                        }\n                    } else {\n                        // We can put anything in the match case since this continuity will not be used\n                        // when advancing the cursor. Same applies to rt.\n                        cont = is_continuous::no;\n                        rt = {};\n                    }\n                }\n            } else {\n                _background_continuity = true; // Default continuity\n            }\n\n            if (!it) {\n                if (heap_i != _heap.end()) {\n                    _heap.erase(heap_i);\n                    std::ranges::make_heap(_heap, heap_less);\n                }\n            } else if (match) {\n                _current_row.insert(_current_row.begin(), position_in_version{\n                    it, std::move(rows), 0, _snp.version()->get_schema().get(), _unique_owner, cont, rt});\n                if (heap_i != _heap.end()) {\n                    _heap.erase(heap_i);\n                    std::ranges::make_heap(_heap, heap_less);\n                }\n            } else {\n                if (heap_i != _heap.end()) {\n                    heap_i->it = it;\n                    heap_i->continuous = cont;\n                    heap_i->rt = rt;\n                    std::ranges::make_heap(_heap, heap_less);\n                } else {\n                    _heap.push_back(position_in_version{\n                        it, std::move(rows), 0, _snp.version()->get_schema().get(), _unique_owner, cont, rt});\n                    std::ranges::push_heap(_heap, heap_less);\n                }\n            }\n        }\n        return true;\n    }\n\n    // Brings back the cursor to validity, pointing at the first row with position not smaller\n    // than the current position. Returns false iff no such row exists.\n    // Assumes that rows are not inserted into the snapshot (static). They can be removed.\n    bool maybe_refresh_static() {\n        if (!iterators_valid()) {\n            return maybe_advance_to(position());\n        }\n        return true;\n    }\n\n    // Moves the cursor to the first entry with position >= pos.\n    // If no such entry exists, the cursor is still moved, although\n    // it won't be pointing at a row. Still, continuous() will be valid.\n    //\n    // Returns true iff there can't be any clustering row entries\n    // between lower_bound (inclusive) and the position to which the cursor\n    // was advanced.\n    //\n    // May be called when cursor is not valid.\n    // The cursor is valid after the call.\n    // Must be called under reclaim lock.\n    // When throws, the cursor is invalidated and its position is not changed.\n    bool advance_to(position_in_partition_view lower_bound) {\n        maybe_advance_to(lower_bound);\n        return no_clustering_row_between_weak(_schema, lower_bound, position());\n    }\n\n    // Call only when valid.\n    // Returns true iff the cursor is pointing at a row.\n    bool at_a_row() const { return !_current_row.empty(); }\n\n    // Advances to the next row, if any.\n    // If there is no next row, advances to the extreme position in the direction of the cursor\n    // (position_in_partition::before_all_clustering_rows() or position_in_partition::after_all_clustering_rows)\n    // and does not point at a row.\n    // Information about the range, continuous() and range_tombstone(), is still valid in this case.\n    // Call only when valid, not necessarily pointing at a row.\n    bool next() { return advance(true); }\n\n    bool erase_and_advance() { return advance(false); }\n\n    // Can be called when cursor is pointing at a row.\n    // Returns true iff the key range adjacent to the cursor's position from the side of smaller keys\n    // is marked as continuous.\n    bool continuous() const { return _continuous; }\n\n    // Can be called when cursor is valid, not necessarily pointing at a row.\n    // Returns the range tombstone for the key range adjacent to the cursor's position from the side of smaller keys.\n    // Excludes the range for the row itself. That information is returned by range_tombstone_for_row().\n    // It's possible that range_tombstone() is empty and range_tombstone_for_row() is not empty.\n    tombstone range_tombstone() const { return _range_tombstone; }\n\n    // Can be called when cursor is pointing at a row.\n    // Returns the range tombstone covering the row under the cursor.\n    tombstone range_tombstone_for_row() const { return _range_tombstone_for_row; }\n\n    // Can be called when cursor is pointing at a row.\n    bool dummy() const { return _dummy; }\n\n    // Can be called only when cursor is valid and pointing at a row, and !dummy().\n    const clustering_key& key() const { return _position.key(); }\n\n    // Can be called only when cursor is valid and pointing at a row.\n    clustering_row row() const {\n        // Note: if the precondition (\"cursor is valid and pointing at a row\") is fulfilled\n        // then _current_row is not empty, so the below is valid.\n        clustering_row cr(key(), deletable_row(_schema, *_current_row[0].schema, _current_row[0].it->row()));\n        for (size_t i = 1; i < _current_row.size(); ++i) {\n            cr.apply(_schema, *_current_row[i].schema, _current_row[i].it->row());\n        }\n        return cr;\n    }\n\n    const schema& latest_row_schema() const noexcept {\n        return *_current_row[0].schema;\n    }\n\n    // Can be called only when cursor is valid and pointing at a row.\n    deletable_row& latest_row() const noexcept {\n        return _current_row[0].it->row();\n    }\n\n    // Can be called only when cursor is valid and pointing at a row.\n    void latest_row_prepare_hash() const {\n        _current_row[0].it->row().cells().prepare_hash(*_current_row[0].schema, column_kind::regular_column);\n    }\n\n    // Can be called only when cursor is valid and pointing at a row.\n    // Monotonic exception guarantees.\n    template <typename Consumer>\n    requires std::is_invocable_v<Consumer, deletable_row&&>\n    void consume_row(Consumer&& consumer) {\n        for (position_in_version& v : _current_row) {\n            if (v.unique_owner && (_schema.version() == v.schema->version())) [[likely]] {\n                consumer(std::move(v.it->row()));\n            } else {\n                consumer(deletable_row(_schema, *v.schema, v.it->row()));\n            }\n        }\n    }\n\n    // Returns memory footprint of row entries under the cursor.\n    // Can be called only when cursor is valid and pointing at a row.\n    size_t memory_usage() const {\n        size_t result = 0;\n        for (const position_in_version& v : _current_row) {\n            result += v.it->memory_usage(*v.schema);\n        }\n        return result;\n    }\n\n    struct ensure_result {\n        rows_entry& row;\n        mutation_partition_v2::rows_type::iterator it;\n        bool inserted = false;\n    };\n\n    // Makes sure that a rows_entry for the row under the cursor exists in the latest version.\n    // Doesn't change logical value or continuity of the snapshot.\n    // Can be called only when cursor is valid and pointing at a row.\n    // The cursor remains valid after the call and points at the same row as before.\n    // Use only with evictable snapshots.\n    ensure_result ensure_entry_in_latest() {\n        auto&& rows = _snp.version()->partition().mutable_clustered_rows();\n        if (is_in_latest_version()) {\n            auto latest_i = get_iterator_in_latest_version();\n            rows_entry& latest = *latest_i;\n            if (_snp.at_latest_version()) {\n                _snp.tracker()->touch(latest);\n            }\n            return {latest, latest_i, false};\n        } else {\n            // Copy row from older version because rows in evictable versions must\n            // hold values which are independently complete to be consistent on eviction.\n            auto e = [&] {\n                if (!at_a_row()) {\n                    return alloc_strategy_unique_ptr<rows_entry>(\n                            current_allocator().construct<rows_entry>(*_snp.schema(), _position,\n                                                                      is_dummy(!_position.is_clustering_row()), is_continuous::no));\n                } else {\n                    return alloc_strategy_unique_ptr<rows_entry>(\n                            current_allocator().construct<rows_entry>(*_snp.schema(), *_current_row[0].schema, *_current_row[0].it));\n                }\n            }();\n            rows_entry& re = *e;\n            if (_reversed) { // latest_i is not reliably a successor\n                re.set_continuous(false);\n                e->set_range_tombstone(range_tombstone_for_row());\n                rows_entry::tri_compare cmp(*_snp.schema());\n                auto res = rows.insert(std::move(e), cmp);\n                if (auto l = std::next(res.first); l != rows.end() && l->continuous()) {\n                    re.set_continuous(true);\n                    re.set_range_tombstone(l->range_tombstone());\n                }\n                if (res.second) {\n                    _snp.tracker()->insert(re);\n                }\n                return {*res.first, res.first, res.second};\n            } else {\n                auto latest_i = get_iterator_in_latest_version();\n                if (latest_i && latest_i->continuous()) {\n                    e->set_continuous(true);\n                    // See the \"information monotonicity\" rule.\n                    e->set_range_tombstone(latest_i->range_tombstone());\n                } else {\n                    e->set_continuous(false);\n                    e->set_range_tombstone(range_tombstone_for_row());\n                }\n                auto i = rows.insert_before(latest_i, std::move(e));\n                _snp.tracker()->insert(re);\n                return {re, i, true};\n            }\n        }\n    }\n\n    // Returns a pointer to rows_entry with given position in latest version or\n    // creates a neutral one, provided that it belongs to a continuous range.\n    // Otherwise returns nullptr.\n    // Doesn't change logical value of mutation_partition or continuity of the snapshot.\n    // The cursor doesn't have to be valid.\n    // The cursor is invalid after the call.\n    // When returns an engaged optional, the attributes of the cursor: continuous() and range_tombstone()\n    // are valid, as if the cursor was advanced to the requested position.\n    // Assumes the snapshot is evictable and not populated by means other than ensure_entry_if_complete().\n    // Subsequent calls to ensure_entry_if_complete() or advance_to() must be given weakly monotonically increasing\n    // positions unless iterators are invalidated across the calls.\n    // The cursor must not be a reversed-order cursor.\n    // Use only with evictable snapshots.\n    std::optional<ensure_result> ensure_entry_if_complete(position_in_partition_view pos) {\n        if (_reversed) { // latest_i is unreliable\n            throw_with_backtrace<std::logic_error>(\"ensure_entry_if_complete() called on reverse cursor\");\n        }\n        position_in_partition::less_compare less(_schema);\n        if (!iterators_valid() || less(position(), pos)) {\n            auto has_entry = maybe_advance_to(pos);\n            SCYLLA_ASSERT(has_entry); // evictable snapshots must have a dummy after all rows.\n        }\n        auto&& rows = _snp.version()->partition().mutable_clustered_rows();\n        auto latest_i = get_iterator_in_latest_version();\n        position_in_partition::equal_compare eq(_schema);\n        if (eq(position(), pos)) {\n            // Check if entry was already inserted by previous call to ensure_entry_if_complete()\n            if (latest_i != rows.begin()) {\n                auto prev_i = std::prev(latest_i);\n                if (eq(prev_i->position(), pos)) {\n                    return ensure_result{*prev_i, prev_i, false};\n                }\n            }\n            return ensure_entry_in_latest();\n        } else if (!continuous()) {\n            return std::nullopt;\n        }\n        // Check if entry was already inserted by previous call to ensure_entry_if_complete()\n        if (latest_i != rows.begin()) {\n            auto prev_i = std::prev(latest_i);\n            if (eq(prev_i->position(), pos)) {\n                return ensure_result{*prev_i, prev_i, false};\n            }\n        }\n        auto e = alloc_strategy_unique_ptr<rows_entry>(\n                current_allocator().construct<rows_entry>(*_snp.version()->get_schema(), pos,\n                    is_dummy(!pos.is_clustering_row()),\n                    is_continuous::no));\n        if (latest_i && latest_i->continuous()) {\n            e->set_continuous(true);\n            e->set_range_tombstone(latest_i->range_tombstone()); // See the \"information monotonicity\" rule.\n        } else {\n            // Even if the range in the latest version is not continuous, the row itself is assumed to be complete,\n            // so it must inherit the current range tombstone.\n            e->set_range_tombstone(range_tombstone());\n        }\n        auto e_i = rows.insert_before(latest_i, std::move(e));\n        _snp.tracker()->insert(*e_i);\n        return ensure_result{*e_i, e_i, true};\n    }\n\n    // Brings the entry pointed to by the cursor to the front of the LRU\n    // Cursor must be valid and pointing at a row.\n    // Use only with evictable snapshots.\n    void touch() {\n        // We cannot bring entries from non-latest versions to the front because that\n        // could result violate ordering invariant for the LRU, which states that older versions\n        // must be evicted first. Needed to keep the snapshot consistent.\n        if (_snp.at_latest_version() && is_in_latest_version()) {\n            _snp.tracker()->touch(*get_iterator_in_latest_version());\n        }\n    }\n\n    // Position of the cursor in the cursor schema domain.\n    // Can be called when cursor is pointing at a row, even when invalid, or when valid.\n    position_in_partition_view position() const {\n        return to_query_domain(_position);\n    }\n\n    // Position of the cursor in the table schema domain.\n    // Can be called when cursor is pointing at a row, even when invalid, or when valid.\n    position_in_partition_view table_position() const {\n        return _position;\n    }\n\n    friend fmt::formatter<partition_snapshot_row_cursor>;\n};\n\ntemplate <> struct fmt::formatter<partition_snapshot_row_cursor> : fmt::formatter<string_view> {\n    auto format(const  partition_snapshot_row_cursor& cur, fmt::format_context& ctx) const {\n        auto out = ctx.out();\n        out = fmt::format_to(out, \"{{cursor: position={}, cont={}, rt={}}}\",\n                   cur._position, cur.continuous(), cur.range_tombstone());\n        if (cur.range_tombstone() != cur.range_tombstone_for_row()) {\n            out = fmt::format_to(out, \", row_rt={}\", cur.range_tombstone_for_row());\n        }\n        out = fmt::format_to(out, \", \");\n        if (cur._reversed) {\n            out = fmt::format_to(out, \"reversed, \");\n        }\n        if (!cur.iterators_valid()) {\n            return fmt::format_to(out, \" iterators invalid}}\");\n        }\n        out = fmt::format_to(out, \"snp={}, current=[\", fmt::ptr(&cur._snp));\n        bool first = true;\n        for (auto&& v : cur._current_row) {\n            if (!first) {\n                out = fmt::format_to(out, \", \");\n            }\n            first = false;\n            out = fmt::format_to(out, \"{{v={}, pos={}, cont={}, rt={}, row_rt={}}}\",\n                       v.version_no, v.it->position(), v.continuous, v.rt, v.it->range_tombstone());\n        }\n        out = fmt::format_to(out, \"], heap[\\n  \");\n        first = true;\n        for (auto&& v : cur._heap) {\n            if (!first) {\n                out = fmt::format_to(out, \",\\n  \");\n            }\n            first = false;\n            out = fmt::format_to(out, \"{{v={}, pos={}, cont={}, rt={}, row_rt={}}}\",\n                       v.version_no, v.it->position(), v.continuous, v.rt, v.it->range_tombstone());\n        }\n        out = fmt::format_to(out, \"], latest_iterator=[\");\n        if (cur._latest_it) {\n            mutation_partition::rows_type::iterator i = *cur._latest_it;\n            if (!i) {\n                out = fmt::format_to(out, \"end\");\n            } else {\n                out = fmt::format_to(out, \"{}\", i->position());\n            }\n        } else {\n            out = fmt::format_to(out, \"<none>\");\n        }\n        return fmt::format_to(out, \"]}}\");\n    }\n};\n\ninline\npartition_snapshot_row_weakref::partition_snapshot_row_weakref(const partition_snapshot_row_cursor& c)\n    : _it(c._current_row[0].it)\n    , _change_mark(c._change_mark)\n    , _pos(c._position)\n    , _in_latest(c.is_in_latest_version())\n{ }\n\ninline\npartition_snapshot_row_weakref& partition_snapshot_row_weakref::operator=(const partition_snapshot_row_cursor& c) {\n    auto tmp = partition_snapshot_row_weakref(c);\n    this->~partition_snapshot_row_weakref();\n    new (this) partition_snapshot_row_weakref(std::move(tmp));\n    return *this;\n}\n"
        },
        {
          "name": "pgo",
          "type": "tree",
          "content": null
        },
        {
          "name": "protocol_server.hh",
          "type": "blob",
          "size": 1.8583984375,
          "content": "/*\n * Copyright (C) 2021-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"seastarx.hh\"\n#include <seastar/core/future.hh>\n#include <seastar/net/socket_defs.hh>\n#include <vector>\n#include \"client_data.hh\"\n#include \"utils/chunked_vector.hh\"\n\nstruct client_data;\n\n// Abstraction for a server serving some kind of user-facing protocol.\nclass protocol_server {\nprotected:\n    seastar::scheduling_group _sched_group;\npublic:\n    virtual ~protocol_server() = default;\n    /// Name of the server, can be different or the same as than the protocol it serves.\n    virtual sstring name() const = 0;\n    /// Name of the protocol served.\n    virtual sstring protocol() const = 0;\n    /// Version of the protocol served (if any -- not all protocols are versioned).\n    virtual sstring protocol_version() const = 0;\n    /// Addresses the server is listening on, shall be empty when server is not running.\n    virtual std::vector<socket_address> listen_addresses() const = 0;\n    /// Start the server.\n    /// Can be called multiple times, in any state of the server.\n    virtual future<> start_server() = 0;\n    /// Stop the server.\n    /// Can be called multiple times, in any state of the server.\n    /// This variant is used on shutdown and therefore it must succeed.\n    virtual future<> stop_server() = 0;\n    /// Stop the server. Weaker variant of \\ref stop_server().\n    /// Can be called multiple times, in any state of the server.\n    /// This variant is used by the REST API so failure is acceptable.\n    virtual future<> request_stop_server() = 0;\n\n    virtual future<utils::chunked_vector<client_data>> get_client_data() {\n        return make_ready_future<utils::chunked_vector<client_data>>(utils::chunked_vector<client_data>());\n    }\n\n    protocol_server(seastar::scheduling_group sg) noexcept : _sched_group(std::move(sg)) {}\n};\n"
        },
        {
          "name": "querier.cc",
          "type": "blob",
          "size": 19.171875,
          "content": "/*\n * Copyright (C) 2018-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include <seastar/core/coroutine.hh>\n\n#include \"querier.hh\"\n#include \"dht/i_partitioner.hh\"\n#include \"reader_concurrency_semaphore.hh\"\n#include \"schema/schema.hh\"\n#include \"utils/log.hh\"\n#include \"utils/error_injection.hh\"\n\n#include <boost/range/adaptor/map.hpp>\n\nnamespace query {\n\nlogging::logger qlogger(\"querier_cache\");\nlogging::logger qrlogger(\"querier\");\n\nenum class can_use {\n    yes,\n    no_schema_version_mismatch,\n    no_ring_pos_mismatch,\n    no_clustering_pos_mismatch,\n    no_scheduling_group_mismatch,\n    no_fatal_semaphore_mismatch\n};\n\nstatic sstring cannot_use_reason(can_use cu)\n{\n    switch (cu)\n    {\n        case can_use::yes:\n            return \"can be used\";\n        case can_use::no_schema_version_mismatch:\n            return \"schema version mismatch\";\n        case can_use::no_ring_pos_mismatch:\n            return \"ring pos mismatch\";\n        case can_use::no_clustering_pos_mismatch:\n            return \"clustering pos mismatch\";\n        case can_use::no_scheduling_group_mismatch:\n            return \"scheduling group mismatch\";\n        case can_use::no_fatal_semaphore_mismatch:\n            return \"fatal semaphore mismatch\";\n    }\n    return \"unknown reason\";\n}\n\nstatic bool ring_position_matches(const schema& s, const dht::partition_range& range, const query::partition_slice& slice,\n        full_position_view pos) {\n    const auto is_reversed = slice.is_reversed();\n\n    const auto expected_start = dht::ring_position(dht::decorate_key(s, pos.partition));\n    // If there are no clustering columns or the select is distinct we don't\n    // have clustering rows at all. In this case we can be sure we won't have\n    // anything more in the last page's partition and thus the start bound is\n    // exclusive. Otherwise there might be clustering rows still and it is\n    // inclusive.\n    const auto expected_inclusiveness = s.clustering_key_size() > 0 &&\n        !slice.options.contains<query::partition_slice::option::distinct>() &&\n        pos.position.region() == partition_region::clustered;\n    const auto comparator = dht::ring_position_comparator(s);\n\n    if (is_reversed && !range.is_singular()) {\n        const auto& end = range.end();\n        return end && comparator(end->value(), expected_start) == 0 && end->is_inclusive() == expected_inclusiveness;\n    }\n\n    const auto& start = range.start();\n    return start && comparator(start->value(), expected_start) == 0 && start->is_inclusive() == expected_inclusiveness;\n}\n\nstatic bool clustering_position_matches(const schema& s, const query::partition_slice& slice, full_position_view pos) {\n    const auto& row_ranges = slice.row_ranges(s, pos.partition);\n\n    if (row_ranges.empty()) {\n        // This is a valid slice on the last page of a query with\n        // clustering restrictions. It simply means the query is\n        // effectively over, no further results are expected. We\n        // can assume the clustering position matches.\n        return true;\n    }\n\n    if (pos.position.region() != partition_region::clustered) {\n        // We stopped at a non-clustering position so the partition's clustering\n        // row ranges should be the default row ranges.\n        return &row_ranges == &slice.default_row_ranges();\n    }\n\n    clustering_key_prefix::equality eq(s);\n\n    const auto is_reversed = slice.is_reversed();\n\n    // If the page ended mid-partition the first partition range should start\n    // with the last clustering key (exclusive).\n    const auto& first_row_range = row_ranges.front();\n    const auto& start = is_reversed ? first_row_range.end() : first_row_range.start();\n    if (!start) {\n        return false;\n    }\n    return !start->is_inclusive() && eq(start->value(), pos.position.key());\n}\n\nstatic bool ranges_match(const schema& s, const dht::partition_range& original_range, const dht::partition_range& new_range) {\n    if (original_range.is_singular() != new_range.is_singular()) {\n        return false;\n    }\n\n    const auto cmp = dht::ring_position_comparator(s);\n    const auto bound_eq = [&] (const std::optional<dht::partition_range::bound>& a, const std::optional<dht::partition_range::bound>& b) {\n        return bool(a) == bool(b) && (!a || a->equal(*b, cmp));\n    };\n\n    // For singular ranges end() == start() so they are interchangeable.\n    // For non-singular ranges we check only the end().\n    return bound_eq(original_range.end(), new_range.end());\n}\n\nstatic bool ranges_match(const schema& s, dht::partition_ranges_view original_ranges, dht::partition_ranges_view new_ranges) {\n    if (new_ranges.empty()) {\n        return false;\n    }\n    if (original_ranges.size() == 1) {\n        if (new_ranges.size() != 1) {\n            return false;\n        }\n        return ranges_match(s, original_ranges.front(), new_ranges.front());\n    }\n\n    // As the query progresses the number of to-be-read ranges can never surpass\n    // that of the original ranges.\n    if (original_ranges.size() < new_ranges.size()) {\n        return false;\n    }\n\n    // If there is a difference in the size of the range lists we assume we\n    // already read ranges from the original list and these ranges are missing\n    // from the head of the new list.\n    auto new_ranges_it = new_ranges.begin();\n    auto original_ranges_it = original_ranges.begin() + (original_ranges.size() - new_ranges.size());\n\n    // The first range in the new list can be partially read so we only check\n    // that one of its bounds match that of its original counterpart, just like\n    // we do with single ranges.\n    if (!ranges_match(s, *original_ranges_it++, *new_ranges_it++)) {\n        return false;\n    }\n\n    const auto cmp = dht::ring_position_comparator(s);\n\n    // The rest of the list, those ranges that we didn't even started reading\n    // yet should be *identical* to their original counterparts.\n    return std::equal(original_ranges_it, original_ranges.end(), new_ranges_it,\n            [&cmp] (const dht::partition_range& a, const dht::partition_range& b) { return a.equal(b, cmp); });\n}\n\ntemplate <typename Querier>\nstatic can_use can_be_used_for_page(querier_cache::is_user_semaphore_func& is_user_semaphore, Querier& q, const schema& s, const dht::partition_range& range, const query::partition_slice& slice, reader_concurrency_semaphore& current_sem) {\n    if (s.version() != q.schema().version()) {\n        return can_use::no_schema_version_mismatch;\n    }\n\n    auto& querier_sem = q.permit().semaphore();\n    if (&querier_sem != &current_sem) {\n        if (is_user_semaphore(querier_sem) && is_user_semaphore(current_sem)) {\n            return can_use::no_scheduling_group_mismatch;\n        }\n        return can_use::no_fatal_semaphore_mismatch;\n    }\n\n    const auto pos_opt = q.current_position();\n    if (!pos_opt) {\n        // There was nothing read so far so we assume we are ok.\n        return can_use::yes;\n    }\n\n    if (!ring_position_matches(s, range, slice, *pos_opt)) {\n        return can_use::no_ring_pos_mismatch;\n    }\n    if (!clustering_position_matches(s, slice, *pos_opt)) {\n        return can_use::no_clustering_pos_mismatch;\n    }\n    return can_use::yes;\n}\n\n// The time-to-live of a cache-entry.\nconst std::chrono::seconds querier_cache::default_entry_ttl{10};\n\nstatic std::unique_ptr<querier_base> find_querier(querier_cache::index& index, query_id key,\n        dht::partition_ranges_view ranges, tracing::trace_state_ptr trace_state) {\n    const auto queriers = index.equal_range(key);\n\n    if (queriers.first == index.end()) {\n        tracing::trace(trace_state, \"Found no cached querier for key {}\", key);\n        return nullptr;\n    }\n\n    const auto it = std::find_if(queriers.first, queriers.second, [&] (const querier_cache::index::value_type& e) {\n        return ranges_match(e.second->schema(), e.second->ranges(), ranges);\n    });\n\n    if (it == queriers.second) {\n        tracing::trace(trace_state, \"Found cached querier(s) for key {} but none matches the query range(s) {}\", key, ranges);\n        return nullptr;\n    }\n    tracing::trace(trace_state, \"Found cached querier for key {} and range(s) {}\", key, ranges);\n    auto ptr = std::move(it->second);\n    index.erase(it);\n    return ptr;\n}\n\nquerier_cache::querier_cache(is_user_semaphore_func is_user_semaphore_func, std::chrono::seconds entry_ttl)\n    : _entry_ttl(entry_ttl), _is_user_semaphore_func(is_user_semaphore_func) {\n}\n\nstruct querier_utils {\n    static mutation_reader get_reader(querier_base& q) noexcept {\n        return std::move(std::get<mutation_reader>(q._reader));\n    }\n    static reader_concurrency_semaphore::inactive_read_handle get_inactive_read_handle(querier_base& q) noexcept {\n        return std::move(std::get<reader_concurrency_semaphore::inactive_read_handle>(q._reader));\n    }\n    static void set_reader(querier_base& q, mutation_reader r) noexcept {\n        q._reader = std::move(r);\n    }\n    static void set_inactive_read_handle(querier_base& q, reader_concurrency_semaphore::inactive_read_handle h) noexcept {\n        q._reader = std::move(h);\n    }\n};\n\ntemplate <typename Querier>\nvoid querier_cache::insert_querier(\n        query_id key,\n        querier_cache::index& index,\n        querier_cache::stats& stats,\n        Querier&& q,\n        std::chrono::seconds ttl,\n        tracing::trace_state_ptr trace_state) {\n    // FIXME: see #3159\n    // In reverse mode mutation_reader drops any remaining rows of the\n    // current partition when the page ends so it cannot be reused across\n    // pages.\n    if (q.is_reversed()) {\n        (void)with_gate(_closing_gate, [q = std::move(q)] () mutable {\n            return q.close().finally([q = std::move(q)] {});\n        });\n        return;\n    }\n\n    ++stats.inserts;\n\n    tracing::trace(trace_state, \"Caching querier with key {}\", key);\n\n    auto& sem = q.permit().semaphore();\n\n    auto irh = sem.register_inactive_read(querier_utils::get_reader(q));\n    if (!irh) {\n        ++stats.resource_based_evictions;\n        return;\n    }\n  try {\n    auto cleanup_irh = defer([&] () noexcept {\n        sem.unregister_inactive_read(std::move(irh));\n    });\n\n    auto it = index.emplace(key, std::make_unique<Querier>(std::move(q)));\n\n    ++stats.population;\n    auto cleanup_index = defer([&] () noexcept {\n        index.erase(it);\n        --stats.population;\n    });\n\n    auto notify_handler = [&stats, &index, it] (reader_concurrency_semaphore::evict_reason reason) {\n        index.erase(it);\n        switch (reason) {\n            case reader_concurrency_semaphore::evict_reason::permit:\n                ++stats.resource_based_evictions;\n                break;\n            case reader_concurrency_semaphore::evict_reason::time:\n                ++stats.time_based_evictions;\n                break;\n            case reader_concurrency_semaphore::evict_reason::manual:\n                break;\n        }\n        --stats.population;\n    };\n\n\n    if (const auto override_ttl = utils::get_local_injector().inject_parameter<uint64_t>(\"querier-cache-ttl-seconds\"); override_ttl) {\n        ttl = std::chrono::seconds(*override_ttl);\n    }\n\n    sem.set_notify_handler(irh, std::move(notify_handler), ttl);\n    querier_utils::set_inactive_read_handle(*it->second, std::move(irh));\n    cleanup_index.cancel();\n    cleanup_irh.cancel();\n  } catch (...) {\n    // It is okay to swallow the exception since\n    // we're allowed to drop the reader upon registration\n    // due to lack of resources - in which case we already\n    // drop the querier.\n    qlogger.warn(\"Failed to insert querier into index: {}. Ignored as if it was evicted upon registration\", std::current_exception());\n  }\n}\n\nvoid querier_cache::insert_data_querier(query_id key, querier&& q, tracing::trace_state_ptr trace_state) {\n    insert_querier(key, _data_querier_index, _stats, std::move(q), _entry_ttl, std::move(trace_state));\n}\n\nvoid querier_cache::insert_mutation_querier(query_id key, querier&& q, tracing::trace_state_ptr trace_state) {\n    insert_querier(key, _mutation_querier_index, _stats, std::move(q), _entry_ttl, std::move(trace_state));\n}\n\nvoid querier_cache::insert_shard_querier(query_id key, shard_mutation_querier&& q, tracing::trace_state_ptr trace_state) {\n    insert_querier(key, _shard_mutation_querier_index, _stats, std::move(q), _entry_ttl, std::move(trace_state));\n}\n\ntemplate <typename Querier>\nstd::optional<Querier> querier_cache::lookup_querier(\n        querier_cache::index& index,\n        query_id key,\n        const schema& s,\n        dht::partition_ranges_view ranges,\n        const query::partition_slice& slice,\n        reader_concurrency_semaphore& current_sem,\n        tracing::trace_state_ptr trace_state,\n        db::timeout_clock::time_point timeout) {\n    auto base_ptr = find_querier(index, key, ranges, trace_state);\n    auto& stats = _stats;\n    ++stats.lookups;\n    if (!base_ptr) {\n        ++stats.misses;\n        return std::nullopt;\n    }\n\n    auto* q_ptr = dynamic_cast<Querier*>(base_ptr.get());\n    if (!q_ptr) {\n        throw std::runtime_error(\"lookup_querier(): found querier is not of the expected type\");\n    }\n    auto& q = *q_ptr;\n    auto reader_opt = q.permit().semaphore().unregister_inactive_read(querier_utils::get_inactive_read_handle(q));\n    if (!reader_opt) {\n        throw std::runtime_error(\"lookup_querier(): found querier that is evicted\");\n    }\n    reader_opt->set_timeout(timeout);\n    querier_utils::set_reader(q, std::move(*reader_opt));\n    --stats.population;\n\n    const auto can_be_used = can_be_used_for_page(_is_user_semaphore_func, q, s, ranges.front(), slice, current_sem);\n    if (can_be_used == can_use::yes) {\n        tracing::trace(trace_state, \"Reusing querier\");\n        return std::optional<Querier>(std::move(q));\n    }\n\n    tracing::trace(trace_state, \"Dropping querier because {}\", cannot_use_reason(can_be_used));\n    ++stats.drops;\n\n    auto permit = q.permit();\n\n    // Save semaphore name and address for later to use it in\n    // error/warning message\n    auto q_semaphore_name = permit.semaphore().name();\n    auto q_semaphore_address = reinterpret_cast<uintptr_t>(&permit.semaphore());\n\n    // Close and drop the querier in the background.\n    // It is safe to do so, since _closing_gate is closed and\n    // waited on in querier_cache::stop()\n    (void)with_gate(_closing_gate, [q = std::move(q)] () mutable {\n        return q.close().finally([q = std::move(q)] {});\n    });\n\n    if (can_be_used == can_use::no_scheduling_group_mismatch) {\n        ++stats.scheduling_group_mismatches;\n        qlogger.warn(\"user semaphore mismatch detected, dropping reader {}: \"\n                \"reader belongs to {} (0x{:x}) but the query class appropriate is {} (0x{:x})\",\n                    permit.description(),\n                    q_semaphore_name,\n                    q_semaphore_address,\n                    current_sem.name(),\n                    reinterpret_cast<uintptr_t>(&current_sem));\n    }\n    else if (can_be_used == can_use::no_fatal_semaphore_mismatch) {\n        on_internal_error(qlogger, seastar::format(\"semaphore mismatch detected, dropping reader {}: \"\n                \"reader belongs to {} (0x{:x}) but the query class appropriate is {} (0x{:x})\",\n                permit.description(),\n                q_semaphore_name,\n                q_semaphore_address,\n                current_sem.name(),\n                reinterpret_cast<uintptr_t>(&current_sem)));\n    }\n\n    return std::nullopt;\n}\n\nstd::optional<querier> querier_cache::lookup_data_querier(query_id key,\n        const schema& s,\n        const dht::partition_range& range,\n        const query::partition_slice& slice,\n        reader_concurrency_semaphore& current_sem,\n        tracing::trace_state_ptr trace_state,\n        db::timeout_clock::time_point timeout) {\n    return lookup_querier<querier>(_data_querier_index, key, s, range, slice, current_sem, std::move(trace_state), timeout);\n}\n\nstd::optional<querier> querier_cache::lookup_mutation_querier(query_id key,\n        const schema& s,\n        const dht::partition_range& range,\n        const query::partition_slice& slice,\n        reader_concurrency_semaphore& current_sem,\n        tracing::trace_state_ptr trace_state,\n        db::timeout_clock::time_point timeout) {\n    return lookup_querier<querier>(_mutation_querier_index, key, s, range, slice, current_sem, std::move(trace_state), timeout);\n}\n\nstd::optional<shard_mutation_querier> querier_cache::lookup_shard_mutation_querier(query_id key,\n        const schema& s,\n        const dht::partition_range_vector& ranges,\n        const query::partition_slice& slice,\n        reader_concurrency_semaphore& current_sem,\n        tracing::trace_state_ptr trace_state,\n        db::timeout_clock::time_point timeout) {\n    return lookup_querier<shard_mutation_querier>(_shard_mutation_querier_index, key, s, ranges, slice, current_sem,\n            std::move(trace_state), timeout);\n}\n\nfuture<> querier_base::close() noexcept {\n    struct variant_closer {\n        querier_base& q;\n        future<> operator()(mutation_reader& reader) {\n            return reader.close();\n        }\n        future<> operator()(reader_concurrency_semaphore::inactive_read_handle& irh) {\n            auto reader_opt = q.permit().semaphore().unregister_inactive_read(std::move(irh));\n            return reader_opt ? reader_opt->close() : make_ready_future<>();\n        }\n    };\n    return std::visit(variant_closer{*this}, _reader);\n}\n\nthread_local logger::rate_limit querier::row_tombstone_warn_rate_limit{std::chrono::seconds(10)};\nthread_local logger::rate_limit querier::cell_tombstone_warn_rate_limit{std::chrono::seconds(10)};\n\nvoid querier::maybe_log_tombstone_warning(std::string_view what, uint64_t live, uint64_t dead, logger::rate_limit& rl) {\n    if (!_qr_config.tombstone_warn_threshold || dead < _qr_config.tombstone_warn_threshold) {\n        return;\n    }\n    if (_range->is_singular()) {\n        qrlogger.log(log_level::warn, rl, \"Read {} live {} and {} dead {}/tombstones for {}.{} partition key \\\"{}\\\" {} (see tombstone_warn_threshold)\",\n                      live, what, dead, what, _schema->ks_name(), _schema->cf_name(), _range->start()->value().key()->with_schema(*_schema), (*_range));\n    } else {\n        qrlogger.log(log_level::warn, rl, \"Read {} live {} and {} dead {}/tombstones for {}.{} <partition-range-scan> {} (see tombstone_warn_threshold)\",\n                      live, what, dead, what, _schema->ks_name(), _schema->cf_name(), (*_range));\n    }\n}\n\nvoid querier_cache::set_entry_ttl(std::chrono::seconds entry_ttl) {\n    _entry_ttl = entry_ttl;\n}\n\nfuture<bool> querier_cache::evict_one() noexcept {\n    for (auto ip : {&_data_querier_index, &_mutation_querier_index, &_shard_mutation_querier_index}) {\n        auto& idx = *ip;\n        if (idx.empty()) {\n            continue;\n        }\n        auto it = idx.begin();\n        auto reader_opt = it->second->permit().semaphore().unregister_inactive_read(querier_utils::get_inactive_read_handle(*it->second));\n        idx.erase(it);\n        ++_stats.resource_based_evictions;\n        --_stats.population;\n        if (reader_opt) {\n            co_await reader_opt->close();\n        }\n        co_return true;\n    }\n    co_return false;\n}\n\nfuture<> querier_cache::stop() noexcept {\n    co_await _closing_gate.close();\n\n    for (auto* ip : {&_data_querier_index, &_mutation_querier_index, &_shard_mutation_querier_index}) {\n        auto& idx = *ip;\n        for (auto it = idx.begin(); it != idx.end(); it = idx.erase(it)) {\n            co_await it->second->close();\n            --_stats.population;\n        }\n    }\n}\n\n} // namespace query\n"
        },
        {
          "name": "querier.hh",
          "type": "blob",
          "size": 17.4638671875,
          "content": "/*\n * Copyright (C) 2018-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <seastar/util/closeable.hh>\n\n#include \"mutation/mutation_compactor.hh\"\n#include \"reader_concurrency_semaphore.hh\"\n#include \"readers/mutation_source.hh\"\n#include \"full_position.hh\"\n\n#include <boost/intrusive/set.hpp>\n\n#include <variant>\n\nnamespace query {\n\nextern logging::logger qrlogger;\n\n/// Consume a page worth of data from the reader.\n///\n/// Uses `compaction_state` for compacting the fragments and `consumer` for\n/// building the results.\n/// Returns a future containing a tuple with the last consumed clustering key,\n/// or std::nullopt if the last row wasn't a clustering row, and whatever the\n/// consumer's `consume_end_of_stream()` method returns.\ntemplate <typename Consumer>\nrequires CompactedFragmentsConsumerV2<Consumer>\nauto consume_page(mutation_reader& reader,\n        lw_shared_ptr<compact_for_query_state_v2> compaction_state,\n        const query::partition_slice& slice,\n        Consumer&& consumer,\n        uint64_t row_limit,\n        uint32_t partition_limit,\n        gc_clock::time_point query_time) {\n    return reader.peek().then([=, &reader, consumer = std::move(consumer)] (\n                mutation_fragment_v2* next_fragment) mutable {\n        const auto next_fragment_region = next_fragment ? next_fragment->position().region() : partition_region::partition_start;\n        compaction_state->start_new_page(row_limit, partition_limit, query_time, next_fragment_region, consumer);\n\n        auto reader_consumer = compact_for_query_v2<Consumer>(compaction_state, std::move(consumer));\n\n        return reader.consume(std::move(reader_consumer));\n    });\n}\n\nclass querier_base {\n    friend class querier_utils;\n\npublic:\n    struct querier_config {\n        uint32_t tombstone_warn_threshold {0}; // 0 disabled\n        querier_config() = default;\n        explicit querier_config(uint32_t warn)\n            : tombstone_warn_threshold(warn) {}\n    };\n\nprotected:\n    schema_ptr _schema;\n    reader_permit _permit;\n    lw_shared_ptr<const dht::partition_range> _range;\n    std::unique_ptr<const query::partition_slice> _slice;\n    std::variant<mutation_reader, reader_concurrency_semaphore::inactive_read_handle> _reader;\n    dht::partition_ranges_view _query_ranges;\n    querier_config _qr_config;\n\npublic:\n    querier_base(reader_permit permit, lw_shared_ptr<const dht::partition_range> range,\n            std::unique_ptr<const query::partition_slice> slice, mutation_reader reader, dht::partition_ranges_view query_ranges)\n        : _schema(reader.schema())\n        , _permit(std::move(permit))\n        , _range(std::move(range))\n        , _slice(std::move(slice))\n        , _reader(std::move(reader))\n        , _query_ranges(query_ranges)\n    { }\n\n    querier_base(schema_ptr schema, reader_permit permit, dht::partition_range range,\n            query::partition_slice slice, const mutation_source& ms, tracing::trace_state_ptr trace_ptr,\n            querier_config config)\n        : _schema(std::move(schema))\n        , _permit(std::move(permit))\n        , _range(make_lw_shared<const dht::partition_range>(std::move(range)))\n        , _slice(std::make_unique<const query::partition_slice>(std::move(slice)))\n        , _reader(ms.make_reader_v2(_schema, _permit, *_range, *_slice, std::move(trace_ptr), streamed_mutation::forwarding::no, mutation_reader::forwarding::no))\n        , _query_ranges(*_range)\n        , _qr_config(std::move(config))\n    { }\n\n    querier_base(querier_base&&) = default;\n    querier_base& operator=(querier_base&&) = default;\n\n    virtual ~querier_base() = default;\n\n    const ::schema& schema() const {\n        return *_schema;\n    }\n\n    reader_permit& permit() {\n        return _permit;\n    }\n\n    bool is_reversed() const {\n        return _slice->is_reversed();\n    }\n\n    virtual std::optional<full_position_view> current_position() const = 0;\n\n    dht::partition_ranges_view ranges() const {\n        return _query_ranges;\n    }\n\n    size_t memory_usage() const {\n        return _permit.consumed_resources().memory;\n    }\n\n    future<> close() noexcept;\n};\n\n/// One-stop object for serving queries.\n///\n/// Encapsulates all state and logic for serving all pages for a given range\n/// of a query on a given shard. Can be used with any CompactedMutationsConsumer\n/// certified result-builder.\n/// Intended to be created on the first page of a query then saved and reused on\n/// subsequent pages.\n/// (1) Create with the parameters of your query.\n/// (2) Call consume_page() with your consumer to consume the contents of the\n///     next page.\n/// (3) At the end of the page save the querier if you expect more pages.\n///     The are_limits_reached() method can be used to determine whether the\n///     page was filled or not. Also check your result builder for short reads.\n///     Most result builders have memory-accounters that will stop the read\n///     once some memory limit was reached. This is called a short read as the\n///     read stops before the row and/or partition limits are reached.\n/// (4) At the beginning of the next page validate whether it can be used with\n///     the page's schema and start position. In case a schema or position\n///     mismatch is detected the querier shouldn't be used to produce the next\n///     page. It should be dropped instead and a new one should be created\n///     instead.\nclass querier : public querier_base {\n    static thread_local logger::rate_limit row_tombstone_warn_rate_limit;\n    static thread_local logger::rate_limit cell_tombstone_warn_rate_limit;\n\n    lw_shared_ptr<compact_for_query_state_v2> _compaction_state;\n\nprivate:\n    void maybe_log_tombstone_warning(std::string_view what, uint64_t live, uint64_t dead, logger::rate_limit& rl);\n\npublic:\n    querier(const mutation_source& ms,\n            schema_ptr schema,\n            reader_permit permit,\n            dht::partition_range range,\n            query::partition_slice slice,\n            tracing::trace_state_ptr trace_ptr,\n            querier_config config = {})\n        : querier_base(schema, permit, std::move(range), std::move(slice), ms, std::move(trace_ptr), std::move(config))\n        , _compaction_state(make_lw_shared<compact_for_query_state_v2>(*schema, gc_clock::time_point{}, *_slice, 0, 0)) {\n    }\n\n    bool are_limits_reached() const {\n        return  _compaction_state->are_limits_reached();\n    }\n\n    template <typename Consumer>\n    requires CompactedFragmentsConsumerV2<Consumer>\n    auto consume_page(Consumer&& consumer,\n            uint64_t row_limit,\n            uint32_t partition_limit,\n            gc_clock::time_point query_time,\n            tracing::trace_state_ptr trace_ptr = {}) {\n        return ::query::consume_page(std::get<mutation_reader>(_reader), _compaction_state, *_slice, std::move(consumer), row_limit,\n                partition_limit, query_time).then_wrapped([this, trace_ptr = std::move(trace_ptr)] (auto&& fut) {\n            const auto& cstats = _compaction_state->stats();\n            tracing::trace(trace_ptr, \"Page stats: {} partition(s), {} static row(s) ({} live, {} dead), {} clustering row(s) ({} live, {} dead), {} range tombstone(s) and {} cell(s) ({} live, {} dead)\",\n                    cstats.partitions,\n                    cstats.static_rows.total(),\n                    cstats.static_rows.live,\n                    cstats.static_rows.dead,\n                    cstats.clustering_rows.total(),\n                    cstats.clustering_rows.live,\n                    cstats.clustering_rows.dead,\n                    cstats.range_tombstones,\n                    cstats.live_cells() + cstats.dead_cells(),\n                    cstats.live_cells(),\n                    cstats.dead_cells());\n            maybe_log_tombstone_warning(\n                    \"rows\",\n                    cstats.static_rows.live + cstats.clustering_rows.live,\n                    cstats.static_rows.dead + cstats.clustering_rows.dead + cstats.range_tombstones,\n                    row_tombstone_warn_rate_limit);\n            maybe_log_tombstone_warning(\"cells\", cstats.live_cells(), cstats.dead_cells(), cell_tombstone_warn_rate_limit);\n            return std::move(fut);\n        });\n    }\n\n    virtual std::optional<full_position_view> current_position() const override {\n        const dht::decorated_key* dk = _compaction_state->current_partition();\n        if (!dk) {\n            return {};\n        }\n        return full_position_view(dk->key(), _compaction_state->current_position());\n    }\n};\n\n/// Local state of a multishard query.\n///\n/// This querier is not intended to be used directly to read pages. Instead it\n/// is merely a shard local state of a suspended multishard query and is\n/// intended to be used for storing the state of the query on each shard where\n/// it executes. It stores the local reader and the referenced parameters it was\n/// created with (similar to other queriers).\n/// For position validation purposes (at lookup) the reader's position is\n/// considered to be the same as that of the query.\nclass shard_mutation_querier : public querier_base {\n    std::unique_ptr<const dht::partition_range_vector> _query_ranges;\n    full_position _nominal_pos;\n\nprivate:\n    shard_mutation_querier(\n            std::unique_ptr<const dht::partition_range_vector> query_ranges,\n            lw_shared_ptr<const dht::partition_range> reader_range,\n            std::unique_ptr<const query::partition_slice> reader_slice,\n            mutation_reader reader,\n            reader_permit permit,\n            full_position nominal_pos)\n        : querier_base(permit, std::move(reader_range), std::move(reader_slice), std::move(reader), *query_ranges)\n        , _query_ranges(std::move(query_ranges))\n        , _nominal_pos(std::move(nominal_pos)) {\n    }\n\n\npublic:\n    shard_mutation_querier(\n            const dht::partition_range_vector query_ranges,\n            lw_shared_ptr<const dht::partition_range> reader_range,\n            std::unique_ptr<const query::partition_slice> reader_slice,\n            mutation_reader reader,\n            reader_permit permit,\n            full_position nominal_pos)\n        : shard_mutation_querier(std::make_unique<const dht::partition_range_vector>(std::move(query_ranges)), std::move(reader_range),\n                std::move(reader_slice), std::move(reader), std::move(permit), std::move(nominal_pos)) {\n    }\n\n    virtual std::optional<full_position_view> current_position() const override {\n        return _nominal_pos;\n    }\n\n    lw_shared_ptr<const dht::partition_range> reader_range() && {\n        return std::move(_range);\n    }\n\n    std::unique_ptr<const query::partition_slice> reader_slice() && {\n        return std::move(_slice);\n    }\n\n    mutation_reader reader() && {\n        return std::move(std::get<mutation_reader>(_reader));\n    }\n};\n\n/// Special-purpose cache for saving queriers between pages.\n///\n/// Queriers are saved at the end of the page and looked up at the beginning of\n/// the next page. The lookup() always removes the querier from the cache, it\n/// has to be inserted again at the end of the page.\n/// Lookup provides the following extra logic, special to queriers:\n/// * It accepts a factory function which is used to create a new querier if\n///     the lookup fails (see below). This allows for simple call sites.\n/// * It does range matching. A query sometimes will result in multiple querier\n///     objects executing on the same node and shard parallelly. To identify the\n///     appropriate querier lookup() will consider - in addition to the lookup\n///     key - the read range.\n/// * It does schema version and position checking. In some case a subsequent\n///     page will have a different schema version or will start from a position\n///     that is before the end position of the previous page. lookup() will\n///     recognize these cases and drop the previous querier and create a new one.\n///\n/// Inserted queriers will have a TTL. When this expires the querier is\n/// evicted. This is to avoid excess and unnecessary resource usage due to\n/// abandoned queriers.\n/// Registers cached readers with the reader concurrency semaphore, as inactive\n/// readers, so the latter can evict them if needed.\n/// Keeps the total memory consumption of cached queriers\n/// below max_queriers_memory_usage by evicting older entries upon inserting\n/// new ones if the the memory consupmtion would go above the limit.\nclass querier_cache {\npublic:\n    static const std::chrono::seconds default_entry_ttl;\n\n    struct stats {\n        // The number of inserts into the cache.\n        uint64_t inserts = 0;\n        // The number of cache lookups.\n        uint64_t lookups = 0;\n        // The subset of lookups that missed.\n        uint64_t misses = 0;\n        // The subset of lookups that hit but the looked up querier had to be\n        // dropped due to position mismatch.\n        uint64_t drops = 0;\n        // The number of queriers evicted due to their TTL expiring.\n        uint64_t time_based_evictions = 0;\n        // The number of queriers evicted to free up resources to be able to\n        // create new readers.\n        uint64_t resource_based_evictions = 0;\n        // The number of queriers currently in the cache.\n        uint64_t population = 0;\n        // The number of queries dropped due to scheduling group mismatch\n        // between semaphores\n        uint64_t scheduling_group_mismatches = 0;\n    };\n\n    using index = std::unordered_multimap<query_id, std::unique_ptr<querier_base>>;\n    using is_user_semaphore_func = std::function<bool(const reader_concurrency_semaphore&)>;\n\nprivate:\n    index _data_querier_index;\n    index _mutation_querier_index;\n    index _shard_mutation_querier_index;\n    std::chrono::seconds _entry_ttl;\n    stats _stats;\n    gate _closing_gate;\n    is_user_semaphore_func _is_user_semaphore_func;\n\nprivate:\n    template <typename Querier>\n    void insert_querier(\n            query_id key,\n            querier_cache::index& index,\n            querier_cache::stats& stats,\n            Querier&& q,\n            std::chrono::seconds ttl,\n            tracing::trace_state_ptr trace_state);\n\n    template <typename Querier>\n    std::optional<Querier> lookup_querier(\n        querier_cache::index& index,\n        query_id key,\n        const schema& s,\n        dht::partition_ranges_view ranges,\n        const query::partition_slice& slice,\n        reader_concurrency_semaphore& current_sem,\n        tracing::trace_state_ptr trace_state,\n        db::timeout_clock::time_point timeout);\n\npublic:\n    querier_cache(is_user_semaphore_func is_user_semaphore_func, std::chrono::seconds entry_ttl = default_entry_ttl);\n\n    querier_cache(const querier_cache&) = delete;\n    querier_cache& operator=(const querier_cache&) = delete;\n\n    // this is captured\n    querier_cache(querier_cache&&) = delete;\n    querier_cache& operator=(querier_cache&&) = delete;\n\n    void insert_data_querier(query_id key, querier&& q, tracing::trace_state_ptr trace_state);\n\n    void insert_mutation_querier(query_id key, querier&& q, tracing::trace_state_ptr trace_state);\n\n    void insert_shard_querier(query_id key, shard_mutation_querier&& q, tracing::trace_state_ptr trace_state);\n\n    /// Lookup a data querier in the cache.\n    ///\n    /// Queriers are found based on `key` and `range`. There may be multiple\n    /// queriers for the same `key` differentiated by their read range. Since\n    /// each subsequent page may have a narrower read range then the one before\n    /// it ranges cannot be simply matched based on equality. For matching we\n    /// use the fact that the coordinator splits the query range into\n    /// non-overlapping ranges. Thus both bounds of any range, or in case of\n    /// singular ranges only the start bound are guaranteed to be unique.\n    ///\n    /// The found querier is checked for a matching position and schema version.\n    /// The start position of the querier is checked against the start position\n    /// of the page using the `range' and `slice'.\n    std::optional<querier> lookup_data_querier(query_id key,\n            const schema& s,\n            const dht::partition_range& range,\n            const query::partition_slice& slice,\n            reader_concurrency_semaphore& current_sem,\n            tracing::trace_state_ptr trace_state,\n            db::timeout_clock::time_point timeout);\n\n    /// Lookup a mutation querier in the cache.\n    ///\n    /// See \\ref lookup_data_querier().\n    std::optional<querier> lookup_mutation_querier(query_id key,\n            const schema& s,\n            const dht::partition_range& range,\n            const query::partition_slice& slice,\n            reader_concurrency_semaphore& current_sem,\n            tracing::trace_state_ptr trace_state,\n            db::timeout_clock::time_point timeout);\n\n    /// Lookup a shard mutation querier in the cache.\n    ///\n    /// See \\ref lookup_data_querier().\n    std::optional<shard_mutation_querier> lookup_shard_mutation_querier(query_id key,\n            const schema& s,\n            const dht::partition_range_vector& ranges,\n            const query::partition_slice& slice,\n            reader_concurrency_semaphore& current_sem,\n            tracing::trace_state_ptr trace_state,\n            db::timeout_clock::time_point timeout);\n\n    /// Change the ttl of cache entries\n    ///\n    /// Applies only to entries inserted after the change.\n    void set_entry_ttl(std::chrono::seconds entry_ttl);\n\n    /// Evict a querier.\n    ///\n    /// Return true if a querier was evicted and false otherwise (if the cache\n    /// is empty).\n    future<bool> evict_one() noexcept;\n\n    /// Close all queriers and wait on background work.\n    ///\n    /// Should be used before destroying the querier_cache.\n    future<> stop() noexcept;\n\n    const stats& get_stats() const {\n        return _stats;\n    }\n};\n\n} // namespace query\n"
        },
        {
          "name": "query-request.hh",
          "type": "blob",
          "size": 21.2216796875,
          "content": "\n/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <memory>\n#include <optional>\n#include <fmt/ostream.h>\n\n#include \"db/functions/function_name.hh\"\n#include \"db/functions/function.hh\"\n#include \"db/functions/aggregate_function.hh\"\n#include \"db/consistency_level_type.hh\"\n#include \"keys.hh\"\n#include \"dht/ring_position.hh\"\n#include \"enum_set.hh\"\n#include \"interval.hh\"\n#include \"tracing/tracing.hh\"\n#include \"utils/small_vector.hh\"\n#include \"db/per_partition_rate_limit_info.hh\"\n#include \"query_id.hh\"\n#include \"bytes.hh\"\n#include \"cql_serialization_format.hh\"\n\nclass position_in_partition_view;\nclass position_in_partition;\nclass partition_slice_builder;\n\nnamespace ser {\n\ntemplate <typename T>\nclass serializer;\n\n};\n\nnamespace query {\n\nusing column_id_vector = utils::small_vector<column_id, 8>;\n\ntemplate <typename T>\nusing range = wrapping_interval<T>;\n\nusing ring_position = dht::ring_position;\n\n// Note: the bounds of a  clustering range don't necessarily satisfy `rb.end()->value() >= lb.end()->value()`,\n// where `lb`, `rb` are the left and right bound respectively, if the bounds use non-full clustering\n// key prefixes. Inclusiveness of the range's bounds must be taken into account during comparisons.\n// For example, consider clustering key type consisting of two ints. Then [0:1, 0:] is a valid non-empty range\n// (e.g. it includes the key 0:2) even though 0: < 0:1 w.r.t the clustering prefix order.\nusing clustering_range = interval<clustering_key_prefix>;\n\n// If `range` was supposed to be used with a comparator `cmp`, then\n// `reverse(range)` is supposed to be used with a reversed comparator `c`.\n// For instance, if it does make sense to do\n//   range.contains(point, cmp);\n// then it also makes sense to do\n//   reversed(range).contains(point, [](auto x, auto y) { return cmp(y, x); });\n// but it doesn't make sense to do\n//   reversed(range).contains(point, cmp);\nclustering_range reverse(const clustering_range& range);\n\nextern const dht::partition_range full_partition_range;\nextern const clustering_range full_clustering_range;\n\ninline\nbool is_single_partition(const dht::partition_range& range) {\n    return range.is_singular() && range.start()->value().has_key();\n}\n\ninline\nbool is_single_row(const schema& s, const query::clustering_range& range) {\n    return range.is_singular() && range.start()->value().is_full(s);\n}\n\ntypedef std::vector<clustering_range> clustering_row_ranges;\n\n/// Trim the clustering ranges.\n///\n/// Equivalent of intersecting each clustering range with [pos, +inf) position\n/// in partition range. Ranges that do not intersect are dropped. Ranges that\n/// partially overlap are trimmed.\n/// Result: each range will overlap fully with [pos, +inf).\n/// Works both with forward schema and ranges, and reversed schema and native reversed ranges\nvoid trim_clustering_row_ranges_to(const schema& s, clustering_row_ranges& ranges, position_in_partition pos);\n\n/// Trim the clustering ranges.\n///\n/// Equivalent of intersecting each clustering range with (key, +inf) clustering\n/// range. Ranges that do not intersect are dropped. Ranges that partially overlap\n/// are trimmed.\n/// Result: each range will overlap fully with (key, +inf).\n/// Works both with forward schema and ranges, and reversed schema and native reversed ranges\nvoid trim_clustering_row_ranges_to(const schema& s, clustering_row_ranges& ranges, const clustering_key& key);\n\nclass specific_ranges {\npublic:\n    specific_ranges(partition_key pk, clustering_row_ranges ranges)\n            : _pk(std::move(pk)), _ranges(std::move(ranges)) {\n    }\n    specific_ranges(const specific_ranges&) = default;\n\n    void add(const schema& s, partition_key pk, clustering_row_ranges ranges) {\n        if (!_pk.equal(s, pk)) {\n            throw std::runtime_error(\"Only single specific range supported currently\");\n        }\n        _pk = std::move(pk);\n        _ranges = std::move(ranges);\n    }\n    bool contains(const schema& s, const partition_key& pk) {\n        return _pk.equal(s, pk);\n    }\n    size_t size() const {\n        return 1;\n    }\n    const clustering_row_ranges* range_for(const schema& s, const partition_key& key) const {\n        if (_pk.equal(s, key)) {\n            return &_ranges;\n        }\n        return nullptr;\n    }\n    const partition_key& pk() const {\n        return _pk;\n    }\n    const clustering_row_ranges& ranges() const {\n        return _ranges;\n    }\n    clustering_row_ranges& ranges() {\n        return _ranges;\n    }\nprivate:\n    friend std::ostream& operator<<(std::ostream& out, const specific_ranges& r);\n\n    partition_key _pk;\n    clustering_row_ranges _ranges;\n};\n\nconstexpr auto max_rows = std::numeric_limits<uint64_t>::max();\nconstexpr auto partition_max_rows = std::numeric_limits<uint64_t>::max();\nconstexpr auto max_rows_if_set = std::numeric_limits<uint32_t>::max();\n\n// Specifies subset of rows, columns and cell attributes to be returned in a query.\n// Can be accessed across cores.\n// Schema-dependent.\nclass partition_slice {\n    friend class ::partition_slice_builder;\npublic:\n    enum class option {\n        send_clustering_key,\n        send_partition_key,\n        send_timestamp,\n        send_expiry,\n        reversed,\n        distinct,\n        collections_as_maps,\n        send_ttl,\n        allow_short_read,\n        with_digest,\n        bypass_cache,\n        // Normally, we don't return static row if the request has clustering\n        // key restrictions and the partition doesn't have any rows matching\n        // the restrictions, see #589. This flag overrides this behavior.\n        always_return_static_content,\n        // Use the new data range scan variant, which builds query::result\n        // directly, bypassing the intermediate reconcilable_result format used\n        // in pre 4.5 range scans.\n        range_scan_data_variant,\n        // When set, mutation query can end a page even if there is no live row in the\n        // final reconcilable_result. This prevents exchanging large pages when there\n        // is a lot of dead rows. This flag is needed during rolling upgrades to support\n        // old coordinators which do not tolerate pages with no live rows.\n        allow_mutation_read_page_without_live_row,\n    };\n    using option_set = enum_set<super_enum<option,\n        option::send_clustering_key,\n        option::send_partition_key,\n        option::send_timestamp,\n        option::send_expiry,\n        option::reversed,\n        option::distinct,\n        option::collections_as_maps,\n        option::send_ttl,\n        option::allow_short_read,\n        option::with_digest,\n        option::bypass_cache,\n        option::always_return_static_content,\n        option::range_scan_data_variant,\n        option::allow_mutation_read_page_without_live_row>>;\n    clustering_row_ranges _row_ranges;\npublic:\n    column_id_vector static_columns; // TODO: consider using bitmap\n    column_id_vector regular_columns;  // TODO: consider using bitmap\n    option_set options;\nprivate:\n    std::unique_ptr<specific_ranges> _specific_ranges;\n    uint32_t _partition_row_limit_low_bits;\n    uint32_t _partition_row_limit_high_bits;\npublic:\n    partition_slice(clustering_row_ranges row_ranges, column_id_vector static_columns,\n        column_id_vector regular_columns, option_set options,\n        std::unique_ptr<specific_ranges> specific_ranges,\n        cql_serialization_format,\n        uint32_t partition_row_limit_low_bits,\n        uint32_t partition_row_limit_high_bits);\n    partition_slice(clustering_row_ranges row_ranges, column_id_vector static_columns,\n        column_id_vector regular_columns, option_set options,\n        std::unique_ptr<specific_ranges> specific_ranges = nullptr,\n        uint64_t partition_row_limit = partition_max_rows);\n    partition_slice(clustering_row_ranges ranges, const schema& schema, const column_set& mask, option_set options);\n    partition_slice(const partition_slice&);\n    partition_slice(partition_slice&&);\n    ~partition_slice();\n\n    partition_slice& operator=(partition_slice&& other) noexcept;\n\n    const clustering_row_ranges& row_ranges(const schema&, const partition_key&) const;\n    void set_range(const schema&, const partition_key&, clustering_row_ranges);\n    void clear_range(const schema&, const partition_key&);\n    void clear_ranges() {\n        _specific_ranges = nullptr;\n    }\n    // FIXME: possibly make this function return a const ref instead.\n    clustering_row_ranges get_all_ranges() const;\n\n    const clustering_row_ranges& default_row_ranges() const {\n        return _row_ranges;\n    }\n    const std::unique_ptr<specific_ranges>& get_specific_ranges() const {\n        return _specific_ranges;\n    }\n    const cql_serialization_format cql_format() const {\n        return cql_serialization_format(4); // For IDL compatibility\n    }\n    uint32_t partition_row_limit_low_bits() const {\n        return _partition_row_limit_low_bits;\n    }\n    uint32_t partition_row_limit_high_bits() const {\n        return _partition_row_limit_high_bits;\n    }\n    uint64_t partition_row_limit() const {\n        return (static_cast<uint64_t>(_partition_row_limit_high_bits) << 32) | _partition_row_limit_low_bits;\n    }\n    void set_partition_row_limit(uint64_t limit) {\n        _partition_row_limit_low_bits = static_cast<uint64_t>(limit);\n        _partition_row_limit_high_bits = static_cast<uint64_t>(limit >> 32);\n    }\n\n    [[nodiscard]]\n    bool is_reversed() const {\n        return options.contains<query::partition_slice::option::reversed>();\n    }\n\n    friend std::ostream& operator<<(std::ostream& out, const partition_slice& ps);\n    friend std::ostream& operator<<(std::ostream& out, const specific_ranges& ps);\n};\n\n// See docs/dev/reverse-reads.md\n// In the following functions, `schema` may be reversed or not (both work).\npartition_slice legacy_reverse_slice_to_native_reverse_slice(const schema& schema, partition_slice slice);\npartition_slice native_reverse_slice_to_legacy_reverse_slice(const schema& schema, partition_slice slice);\n// Fully reverse slice (forward to native reverse or native reverse to forward).\n// Also toggles the reversed bit in `partition_slice::options`.\npartition_slice reverse_slice(const schema& schema, partition_slice slice);\n\nconstexpr auto max_partitions = std::numeric_limits<uint32_t>::max();\nconstexpr auto max_tombstones = std::numeric_limits<uint64_t>::max();\n\n// Tagged integers to disambiguate constructor arguments.\nenum class row_limit : uint64_t { max = max_rows };\nenum class partition_limit : uint32_t { max = max_partitions };\nenum class tombstone_limit : uint64_t { max = max_tombstones };\n\nusing is_first_page = bool_class<class is_first_page_tag>;\n\n/*\n * This struct is used in two incompatible ways.\n *\n * SEPARATE_PAGE_SIZE_AND_SAFETY_LIMIT cluster feature determines which way is\n * used.\n *\n * 1. If SEPARATE_PAGE_SIZE_AND_SAFETY_LIMIT is not enabled on the cluster then\n *    `page_size` field is ignored. Depending on the query type the meaning of\n *    the remaining two fields is:\n *\n *    a. For unpaged queries or for reverse queries:\n *\n *          * `soft_limit` is used to warn about queries that result exceeds\n *            this limit. If the limit is exceeded, a warning will be written to\n *            the log.\n *\n *          * `hard_limit` is used to terminate a query which result exceeds\n *            this limit. If the limit is exceeded, the operation will end with\n *            an exception.\n *\n *    b. For all other queries, `soft_limit` == `hard_limit` and their value is\n *       really a page_size in bytes. If the page is not previously cut by the\n *       page row limit then reaching the size of `soft_limit`/`hard_limit`\n *       bytes will cause a page to be finished.\n *\n * 2. If SEPARATE_PAGE_SIZE_AND_SAFETY_LIMIT is enabled on the cluster then all\n *    three fields are always set. They are used in different places:\n *\n *    a. `soft_limit` and `hard_limit` are used for unpaged queries and in a\n *       reversing reader used for reading KA/LA sstables. Their meaning is the\n *       same as in (1.a) above.\n *\n *    b. all other queries use `page_size` field only and the meaning of the\n *       field is the same ase in (1.b) above.\n *\n * Two interpretations of the `max_result_size` struct are not compatible so we\n * need to take care of handling a mixed clusters.\n *\n * As long as SEPARATE_PAGE_SIZE_AND_SAFETY_LIMIT cluster feature is not\n * supported by all nodes in the clustser, new nodes will always use the\n * interpretation described in the point (1). `soft_limit` and `hard_limit`\n * fields will be set appropriately to the query type and `page_size` field\n * will be set to 0. Old nodes will ignare `page_size` anyways and new nodes\n * will know to ignore it as well when it's set to 0. Old nodes will never set\n * `page_size` and that means new nodes will give it a default value of 0 and\n * ignore it for messages that miss this field.\n *\n * Once SEPARATE_PAGE_SIZE_AND_SAFETY_LIMIT cluster feature becomes supported by\n * the whole cluster, new nodes will start to set `page_size` to the right value\n * according to the interpretation described in the point (2).\n *\n * For each request, only the coordinator looks at\n * SEPARATE_PAGE_SIZE_AND_SAFETY_LIMIT and based on it decides for this request\n * whether it will be handled with interpretation (1) or (2). Then all the\n * replicas can check the decision based only on the message they receive.\n * If page_size is set to 0 or not set at all then the request will be handled\n * using the interpretation (1). Otherwise, interpretation (2) will be used.\n */\nstruct max_result_size {\n    uint64_t soft_limit;\n    uint64_t hard_limit;\nprivate:\n    uint64_t page_size = 0;\npublic:\n\n    max_result_size() = delete;\n    explicit max_result_size(uint64_t max_size) : soft_limit(max_size), hard_limit(max_size) { }\n    explicit max_result_size(uint64_t soft_limit, uint64_t hard_limit) : soft_limit(soft_limit), hard_limit(hard_limit) { }\n    max_result_size(uint64_t soft_limit, uint64_t hard_limit, uint64_t page_size)\n            : soft_limit(soft_limit)\n            , hard_limit(hard_limit)\n            , page_size(page_size)\n    { }\n    uint64_t get_page_size() const {\n        return page_size == 0 ? hard_limit : page_size;\n    }\n    max_result_size without_page_limit() const {\n        return max_result_size(soft_limit, hard_limit, 0);\n    }\n    bool operator==(const max_result_size&) const = default;\n    friend class ser::serializer<query::max_result_size>;\n};\n\n// Full specification of a query to the database.\n// Intended for passing across replicas.\n// Can be accessed across cores.\nclass read_command {\npublic:\n    table_id cf_id;\n    table_schema_version schema_version; // TODO: This should be enough, drop cf_id\n    partition_slice slice;\n    uint32_t row_limit_low_bits;\n    gc_clock::time_point timestamp;\n    std::optional<tracing::trace_info> trace_info;\n    uint32_t partition_limit; // The maximum number of live partitions to return.\n    // The \"query_uuid\" field is useful in pages queries: It tells the replica\n    // that when it finishes the read request prematurely, i.e., reached the\n    // desired number of rows per page, it should not destroy the reader object,\n    // rather it should keep it alive - at its current position - and save it\n    // under the unique key \"query_uuid\". Later, when we want to resume\n    // the read at exactly the same position (i.e., to request the next page)\n    // we can pass this same unique id in that query's \"query_uuid\" field.\n    query_id query_uuid;\n    // Signal to the replica that this is the first page of a (maybe) paged\n    // read request as far the replica is concerned. Can be used by the replica\n    // to avoid doing work normally done on paged requests, e.g. attempting to\n    // reused suspended readers.\n    query::is_first_page is_first_page;\n    // The maximum size of the query result, for all queries.\n    // We use the entire value range, so we need an optional for the case when\n    // the remote doesn't send it.\n    std::optional<query::max_result_size> max_result_size;\n    uint32_t row_limit_high_bits;\n    // Cut the page after processing this many tombstones (even if the page is empty).\n    uint64_t tombstone_limit;\n    api::timestamp_type read_timestamp; // not serialized\n    db::allow_per_partition_rate_limit allow_limit; // not serialized\npublic:\n    // IDL constructor\n    read_command(table_id cf_id,\n                 table_schema_version schema_version,\n                 partition_slice slice,\n                 uint32_t row_limit_low_bits,\n                 gc_clock::time_point now,\n                 std::optional<tracing::trace_info> ti,\n                 uint32_t partition_limit,\n                 query_id query_uuid,\n                 query::is_first_page is_first_page,\n                 std::optional<query::max_result_size> max_result_size,\n                 uint32_t row_limit_high_bits,\n                 uint64_t tombstone_limit)\n        : cf_id(std::move(cf_id))\n        , schema_version(std::move(schema_version))\n        , slice(std::move(slice))\n        , row_limit_low_bits(row_limit_low_bits)\n        , timestamp(now)\n        , trace_info(std::move(ti))\n        , partition_limit(partition_limit)\n        , query_uuid(query_uuid)\n        , is_first_page(is_first_page)\n        , max_result_size(max_result_size)\n        , row_limit_high_bits(row_limit_high_bits)\n        , tombstone_limit(tombstone_limit)\n        , read_timestamp(api::new_timestamp())\n        , allow_limit(db::allow_per_partition_rate_limit::no)\n    { }\n\n    read_command(table_id cf_id,\n            table_schema_version schema_version,\n            partition_slice slice,\n            query::max_result_size max_result_size,\n            query::tombstone_limit tombstone_limit,\n            query::row_limit row_limit = query::row_limit::max,\n            query::partition_limit partition_limit = query::partition_limit::max,\n            gc_clock::time_point now = gc_clock::now(),\n            std::optional<tracing::trace_info> ti = std::nullopt,\n            query_id query_uuid = query_id::create_null_id(),\n            query::is_first_page is_first_page = query::is_first_page::no,\n            api::timestamp_type rt = api::new_timestamp(),\n            db::allow_per_partition_rate_limit allow_limit = db::allow_per_partition_rate_limit::no)\n        : cf_id(std::move(cf_id))\n        , schema_version(std::move(schema_version))\n        , slice(std::move(slice))\n        , row_limit_low_bits(static_cast<uint32_t>(row_limit))\n        , timestamp(now)\n        , trace_info(std::move(ti))\n        , partition_limit(static_cast<uint32_t>(partition_limit))\n        , query_uuid(query_uuid)\n        , is_first_page(is_first_page)\n        , max_result_size(max_result_size)\n        , row_limit_high_bits(static_cast<uint32_t>(static_cast<uint64_t>(row_limit) >> 32))\n        , tombstone_limit(static_cast<uint64_t>(tombstone_limit))\n        , read_timestamp(rt)\n        , allow_limit(allow_limit)\n    { }\n\n\n    uint64_t get_row_limit() const {\n        return (static_cast<uint64_t>(row_limit_high_bits) << 32) | row_limit_low_bits;\n    }\n    void set_row_limit(uint64_t new_row_limit) {\n        row_limit_low_bits = static_cast<uint32_t>(new_row_limit);\n        row_limit_high_bits = static_cast<uint32_t>(new_row_limit >> 32);\n    }\n    friend std::ostream& operator<<(std::ostream& out, const read_command& r);\n};\n\n// Reverse read_command by reversing the schema version and transforming the slice from\n// the legacy reversed format to native reversed format. Shall be called with reversed\n// queries only.\nlw_shared_ptr<query::read_command> reversed(lw_shared_ptr<query::read_command>&& cmd);\n\nstruct mapreduce_request {\n    enum class reduction_type {\n        count,\n        aggregate\n    };\n    struct aggregation_info {\n        db::functions::function_name name;\n        std::vector<sstring> column_names;\n    };\n    struct reductions_info {\n        // Used by selector_factries to prepare reductions information\n        std::vector<reduction_type> types;\n        std::vector<aggregation_info> infos;\n    };\n\n    std::vector<reduction_type> reduction_types;\n\n    query::read_command cmd;\n    dht::partition_range_vector pr;\n\n    db::consistency_level cl;\n    lowres_system_clock::time_point timeout;\n    std::optional<std::vector<aggregation_info>> aggregation_infos;\n};\n\nstd::ostream& operator<<(std::ostream& out, const mapreduce_request& r);\nstd::ostream& operator<<(std::ostream& out, const mapreduce_request::reduction_type& r);\nstd::ostream& operator<<(std::ostream& out, const mapreduce_request::aggregation_info& a);\n\nstruct mapreduce_result {\n    // vector storing query result for each selected column\n    std::vector<bytes_opt> query_results;\n\n    struct printer {\n        const std::vector<::shared_ptr<db::functions::aggregate_function>> functions;\n        const query::mapreduce_result& res;\n    };\n};\n\nstd::ostream& operator<<(std::ostream& out, const query::mapreduce_result::printer&);\n}\n\n\ntemplate <> struct fmt::formatter<query::specific_ranges> : fmt::ostream_formatter {};\ntemplate <> struct fmt::formatter<query::partition_slice> : fmt::ostream_formatter {};\ntemplate <> struct fmt::formatter<query::read_command> : fmt::ostream_formatter {};\ntemplate <> struct fmt::formatter<query::mapreduce_request> : fmt::ostream_formatter {};\ntemplate <> struct fmt::formatter<query::mapreduce_request::reduction_type> : fmt::ostream_formatter {};\ntemplate <> struct fmt::formatter<query::mapreduce_request::aggregation_info> : fmt::ostream_formatter {};\ntemplate <> struct fmt::formatter<query::mapreduce_result::printer> : fmt::ostream_formatter {};\n"
        },
        {
          "name": "query-result-reader.hh",
          "type": "blob",
          "size": 6.1494140625,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"utils/assert.hh\"\n\n#include \"query-result.hh\"\n#include \"full_position.hh\"\n\n#include \"idl/query.dist.hh\"\n#include \"idl/query.dist.impl.hh\"\n\nnamespace query {\n\nusing result_bytes_view = ser::buffer_view<bytes_ostream::fragment_iterator>;\n\nclass result_atomic_cell_view {\n    ser::qr_cell_view _view;\npublic:\n    result_atomic_cell_view(ser::qr_cell_view view)\n        : _view(view) { }\n\n    api::timestamp_type timestamp() const {\n        return _view.timestamp().value_or(api::missing_timestamp);\n    }\n\n    expiry_opt expiry() const {\n        return _view.expiry();\n    }\n\n    ttl_opt ttl() const {\n        return _view.ttl();\n    }\n\n    result_bytes_view value() const {\n        return _view.value().view();\n    }\n};\n\n// Contains cells in the same order as requested by partition_slice.\n// Contains only live cells.\nclass result_row_view {\n    ser::qr_row_view _v;\npublic:\n    result_row_view(ser::qr_row_view v) : _v(v) {}\n\n    class iterator_type {\n        using cells_deserializer = ser::vector_deserializer<std::optional<ser::qr_cell_view>>;\n        cells_deserializer _cells;\n        cells_deserializer::iterator _i;\n    public:\n        iterator_type(const ser::qr_row_view& v)\n            : _cells(v.cells())\n            , _i(_cells.begin())\n        { }\n        std::optional<result_atomic_cell_view> next_atomic_cell() {\n            auto cell_opt = *_i++;\n            if (!cell_opt) {\n                return {};\n            }\n            return {result_atomic_cell_view(*cell_opt)};\n        }\n        std::optional<result_bytes_view> next_collection_cell() {\n            auto cell_opt = *_i++;\n            if (!cell_opt) {\n                return {};\n            }\n            ser::qr_cell_view v = *cell_opt;\n            return {v.value().view()};\n        };\n        void skip(const column_definition& def) {\n            ++_i;\n        }\n    };\n\n    iterator_type iterator() const {\n        return iterator_type(_v);\n    }\n};\n\n// Describes expectations about the ResultVisitor concept.\n//\n// Interaction flow:\n//   -> accept_new_partition()\n//   -> accept_new_row()\n//   -> accept_new_row()\n//   -> accept_partition_end()\n//   -> accept_new_partition()\n//   -> accept_new_row()\n//   -> accept_new_row()\n//   -> accept_new_row()\n//   -> accept_partition_end()\n//   ...\n//\nstruct result_visitor {\n    void accept_new_partition(\n        const partition_key& key, // FIXME: use view for the key\n        uint64_t row_count) {}\n\n    void accept_new_partition(uint64_t row_count) {}\n\n    void accept_new_row(\n        const clustering_key& key, // FIXME: use view for the key\n        const result_row_view& static_row,\n        const result_row_view& row) {}\n\n    void accept_new_row(const result_row_view& static_row, const result_row_view& row) {}\n\n    void accept_partition_end(const result_row_view& static_row) {}\n};\n\ntemplate<typename Visitor>\nconcept ResultVisitor = requires(Visitor visitor, const partition_key& pkey,\n                                      uint64_t row_count, const clustering_key& ckey,\n                                      const result_row_view& static_row, const result_row_view& row)\n{\n    visitor.accept_new_partition(pkey, row_count);\n    visitor.accept_new_partition(row_count);\n    visitor.accept_new_row(ckey, static_row, row);\n    visitor.accept_new_row(static_row, row);\n    visitor.accept_partition_end(static_row);\n};\n\nclass result_view {\n    ser::query_result_view _v;\n    friend class result_merger;\npublic:\n    result_view(const bytes_ostream& v) : _v(ser::query_result_view{ser::as_input_stream(v)}) {}\n    result_view(ser::query_result_view v) : _v(v) {}\n    explicit result_view(const query::result& res) : result_view(res.buf()) { }\n\n    template <typename Func>\n    static auto do_with(const query::result& res, Func&& func) {\n        result_view view(res.buf());\n        return func(view);\n    }\n\n    template <typename ResultVisitor>\n    static void consume(const query::result& res, const partition_slice& slice, ResultVisitor&& visitor) {\n        result_view(res).consume(slice, visitor);\n    }\n\n    template <typename Visitor>\n    requires ResultVisitor<Visitor>\n    void consume(const partition_slice& slice, Visitor&& visitor) const {\n        for (auto&& p : _v.partitions()) {\n            auto rows = p.rows();\n            auto row_count = rows.size();\n            if (slice.options.contains<partition_slice::option::send_partition_key>()) {\n                auto key = *p.key();\n                visitor.accept_new_partition(key, row_count);\n            } else {\n                visitor.accept_new_partition(row_count);\n            }\n\n            result_row_view static_row(p.static_row());\n\n            for (auto&& row : rows) {\n                result_row_view view(row.cells());\n                if (slice.options.contains<partition_slice::option::send_clustering_key>()) {\n                    visitor.accept_new_row(*row.key(), static_row, view);\n                } else {\n                    visitor.accept_new_row(static_row, view);\n                }\n            }\n\n            visitor.accept_partition_end(static_row);\n        }\n    }\n\n    std::tuple<uint32_t, uint64_t> count_partitions_and_rows() const {\n        auto ps = _v.partitions();\n        uint64_t rows = 0;\n        for (auto p : ps) {\n            rows += std::max(p.rows().size(), size_t(1));\n        }\n        return std::make_tuple(ps.size(), rows);\n    }\n\n    full_position calculate_last_position() const {\n        auto ps = _v.partitions();\n        SCYLLA_ASSERT(!ps.empty());\n        auto pit = ps.begin();\n        auto pnext = pit;\n        while (++pnext != ps.end()) {\n            pit = pnext;\n        }\n        auto p = *pit;\n        auto rs = p.rows();\n        auto pos = position_in_partition::for_partition_start();\n        if (!rs.empty()) {\n            auto rit = rs.begin();\n            auto rnext = rit;\n            while (++rnext != rs.end()) {\n                rit = rnext;\n            }\n            const auto& key_opt = (*rit).key();\n            if (key_opt) {\n                pos = position_in_partition::for_key(*key_opt);\n            }\n        }\n        return { p.key().value(), std::move(pos) };\n    }\n};\n\n}\n"
        },
        {
          "name": "query-result-set.cc",
          "type": "blob",
          "size": 7.6826171875,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"query-result-set.hh\"\n#include \"query-result-reader.hh\"\n#include \"partition_slice_builder.hh\"\n#include \"mutation/mutation.hh\"\n#include \"types/map.hh\"\n#include \"mutation_query.hh\"\n\n#include <fmt/format.h>\n\nnamespace query {\n\nclass deserialization_error : public std::runtime_error {\npublic:\n    using runtime_error::runtime_error;\n};\n\n// Result set builder is passed as a visitor to query_result::consume()\n// function. You can call the build() method to obtain a result set that\n// contains cells from the visited results.\nclass result_set_builder {\n    schema_ptr _schema;\n    const partition_slice& _slice;\n    std::vector<result_set_row> _rows;\n    std::unordered_map<sstring, non_null_data_value> _pkey_cells;\n    uint64_t _row_count;\npublic:\n    // Keep slice live as long as the builder is used.\n    result_set_builder(schema_ptr schema, const partition_slice& slice);\n    result_set build();\n    void accept_new_partition(const partition_key& key, uint64_t row_count);\n    void accept_new_partition(uint64_t row_count);\n    void accept_new_row(const clustering_key& key, const result_row_view& static_row, const result_row_view& row);\n    void accept_new_row(const result_row_view &static_row, const result_row_view &row);\n    void accept_partition_end(const result_row_view& static_row);\nprivate:\n    std::unordered_map<sstring, non_null_data_value> deserialize(const partition_key& key);\n    std::unordered_map<sstring, non_null_data_value> deserialize(const clustering_key& key);\n    std::unordered_map<sstring, non_null_data_value> deserialize(const result_row_view& row, bool is_static);\n};\n\nstd::ostream& operator<<(std::ostream& out, const result_set_row& row) {\n    for (auto&& cell : row._cells) {\n        auto&& type = static_cast<const data_value&>(cell.second).type();\n        auto&& value = cell.second;\n        out << cell.first << \"=\\\"\" << type->to_string(type->decompose(value)) << \"\\\" \";\n    }\n    return out;\n}\n\nstd::ostream& operator<<(std::ostream& out, const result_set& rs) {\n    for (auto&& row : rs._rows) {\n        out << row << std::endl;\n    }\n    return out;\n}\n\nstatic logging::logger query_result_log(\"query_result_log\");\n\nnon_null_data_value::non_null_data_value(data_value&& v) : _v(std::move(v)) {\n    if (_v.is_null()) {\n        on_internal_error(query_result_log, \"Trying to add a null data_value to a result_set_row\");\n    }\n}\n\nresult_set_builder::result_set_builder(schema_ptr schema, const partition_slice& slice)\n    : _schema{schema}, _slice(slice)\n{ }\n\nresult_set result_set_builder::build() {\n    return { _schema, std::move(_rows) };\n}\n\nvoid result_set_builder::accept_new_partition(const partition_key& key, uint64_t row_count)\n{\n    _pkey_cells = deserialize(key);\n    accept_new_partition(row_count);\n}\n\nvoid result_set_builder::accept_new_partition(uint64_t row_count)\n{\n    _row_count = row_count;\n}\n\nvoid result_set_builder::accept_new_row(const clustering_key& key, const result_row_view& static_row, const result_row_view& row)\n{\n    auto ckey_cells = deserialize(key);\n    auto static_cells = deserialize(static_row, true);\n    auto regular_cells = deserialize(row, false);\n\n    std::unordered_map<sstring, non_null_data_value> cells;\n    cells.insert(_pkey_cells.begin(), _pkey_cells.end());\n    cells.insert(ckey_cells.begin(), ckey_cells.end());\n    cells.insert(static_cells.begin(), static_cells.end());\n    cells.insert(regular_cells.begin(), regular_cells.end());\n    _rows.emplace_back(_schema, std::move(cells));\n}\n\nvoid result_set_builder::accept_new_row(const query::result_row_view &static_row, const query::result_row_view &row)\n{\n    auto static_cells = deserialize(static_row, true);\n    auto regular_cells = deserialize(row, false);\n\n    std::unordered_map<sstring, non_null_data_value> cells;\n    cells.insert(_pkey_cells.begin(), _pkey_cells.end());\n    cells.insert(static_cells.begin(), static_cells.end());\n    cells.insert(regular_cells.begin(), regular_cells.end());\n    _rows.emplace_back(_schema, std::move(cells));\n}\n\nvoid result_set_builder::accept_partition_end(const result_row_view& static_row)\n{\n    if (_row_count == 0) {\n        auto static_cells = deserialize(static_row, true);\n        std::unordered_map<sstring, non_null_data_value> cells;\n        cells.insert(_pkey_cells.begin(), _pkey_cells.end());\n        cells.insert(static_cells.begin(), static_cells.end());\n        _rows.emplace_back(_schema, std::move(cells));\n    }\n    _pkey_cells.clear();\n}\n\nstd::unordered_map<sstring, non_null_data_value>\nresult_set_builder::deserialize(const partition_key& key)\n{\n    std::unordered_map<sstring, non_null_data_value> cells;\n    auto i = key.begin(*_schema);\n    for (auto&& col : _schema->partition_key_columns()) {\n        cells.emplace(col.name_as_text(), col.type->deserialize_value(*i));\n        ++i;\n    }\n    return cells;\n}\n\nstd::unordered_map<sstring, non_null_data_value>\nresult_set_builder::deserialize(const clustering_key& key)\n{\n    std::unordered_map<sstring, non_null_data_value> cells;\n    auto i = key.begin(*_schema);\n    for (auto&& col : _schema->clustering_key_columns()) {\n        if (i == key.end(*_schema)) {\n            break;\n        }\n        cells.emplace(col.name_as_text(), col.type->deserialize_value(*i));\n        ++i;\n    }\n    return cells;\n}\n\nstd::unordered_map<sstring, non_null_data_value>\nresult_set_builder::deserialize(const result_row_view& row, bool is_static)\n{\n    std::unordered_map<sstring, non_null_data_value> cells;\n    auto i = row.iterator();\n    auto column_ids = is_static ? _slice.static_columns : _slice.regular_columns;\n    auto columns = column_ids | std::views::transform([this, is_static] (column_id id) -> const column_definition& {\n        if (is_static) {\n            return _schema->static_column_at(id);\n        } else {\n            return _schema->regular_column_at(id);\n        }\n    });\n    size_t index = 0;\n    for (auto &&col : columns) {\n      try {\n        if (col.is_atomic()) {\n            auto cell = i.next_atomic_cell();\n            if (cell) {\n                cells.emplace(col.name_as_text(), col.type->deserialize_value(cell->value()));\n            }\n        } else {\n            auto cell = i.next_collection_cell();\n            if (cell) {\n                    if (col.type->is_collection()) {\n                        auto ctype = static_pointer_cast<const collection_type_impl>(col.type);\n                        if (_slice.options.contains<partition_slice::option::collections_as_maps>()) {\n                            ctype = map_type_impl::get_instance(ctype->name_comparator(), ctype->value_comparator(), true);\n                        }\n\n                        cells.emplace(col.name_as_text(), ctype->deserialize_value(*cell));\n                    } else {\n                        cells.emplace(col.name_as_text(), col.type->deserialize_value(*cell));\n                    }\n            }\n        }\n        index++;\n      } catch (...) {\n            throw deserialization_error(fmt::format(FMT_STRING(\"failed on column {}.{}#{} (version: {}, id: {}, index: {}, type: {}): {}\"),\n                _schema->ks_name(), _schema->cf_name(), col.name_as_text(), _schema->version(), col.id, index, col.type->name(), std::current_exception()));\n      }\n    }\n    return cells;\n}\n\nresult_set\nresult_set::from_raw_result(schema_ptr s, const partition_slice& slice, const result& r) {\n    result_set_builder builder{std::move(s), slice};\n    result_view::consume(r, slice, builder);\n    return builder.build();\n}\n\nresult_set::result_set(const mutation& m) : result_set([&m] {\n    auto slice = partition_slice_builder(*m.schema()).build();\n    auto qr = query_mutation(mutation(m), slice);\n    return result_set::from_raw_result(m.schema(), slice, qr);\n}())\n{ }\n\n}\n"
        },
        {
          "name": "query-result-set.hh",
          "type": "blob",
          "size": 4.0068359375,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n\n#include <seastar/core/shared_ptr.hh>\n#include <fmt/ostream.h>\n#include \"types/types.hh\"\n#include \"schema/schema.hh\"\n\n#include <optional>\n#include <stdexcept>\n\nclass mutation;\n\nnamespace query {\n\nclass result;\n\nclass no_value : public std::runtime_error {\npublic:\n    using runtime_error::runtime_error;\n};\n\nclass non_null_data_value {\n    data_value _v;\n\npublic:\n    explicit non_null_data_value(data_value&& v);\n    operator const data_value&() const {\n        return _v;\n    }\n};\n\ninline bool operator==(const non_null_data_value& x, const non_null_data_value& y) {\n    return static_cast<const data_value&>(x) == static_cast<const data_value&>(y);\n}\n\n// Result set row is a set of cells that are associated with a row\n// including regular column cells, partition keys, as well as static values.\nclass result_set_row {\n    schema_ptr _schema;\n    const std::unordered_map<sstring, non_null_data_value> _cells;\npublic:\n    result_set_row(schema_ptr schema, std::unordered_map<sstring, non_null_data_value>&& cells)\n        : _schema{schema}\n        , _cells{std::move(cells)}\n    { }\n    // Look up a deserialized row cell value by column name\n    const data_value*\n    get_data_value(const sstring& column_name) const {\n        auto it = _cells.find(column_name);\n        if (it == _cells.end()) {\n            return nullptr;\n        }\n        return &static_cast<const data_value&>(it->second);\n    }\n    // Look up a deserialized row cell value by column name\n    template<typename T>\n    std::optional<T>\n    get(const sstring& column_name) const {\n        if (const auto *value = get_ptr<T>(column_name)) {\n            return std::optional(*value);\n        }\n        return std::nullopt;\n    }\n    template<typename T>\n    const T*\n    get_ptr(const sstring& column_name) const {\n        const auto *value = get_data_value(column_name);\n        if (value == nullptr) {\n            return nullptr;\n        }\n        return &value_cast<T>(*value);\n    }\n    // throws no_value on error\n    template<typename T>\n    const T& get_nonnull(const sstring& column_name) const {\n        auto v = get_ptr<std::remove_reference_t<T>>(column_name);\n        if (v) {\n            return *v;\n        }\n        throw no_value(column_name);\n    }\n    const std::unordered_map<sstring, non_null_data_value>& cells() const { return _cells; }\n    friend inline bool operator==(const result_set_row& x, const result_set_row& y) = default;\n    friend std::ostream& operator<<(std::ostream& out, const result_set_row& row);\n};\n\n// Result set is an in-memory representation of query results in\n// deserialized format. To obtain a result set, use the result_set_builder\n// class as a visitor to query_result::consume() function.\nclass result_set {\n    schema_ptr _schema;\n    std::vector<result_set_row> _rows;\npublic:\n    static result_set from_raw_result(schema_ptr, const partition_slice&, const result&);\n    result_set(schema_ptr s, std::vector<result_set_row>&& rows)\n        : _schema(std::move(s)), _rows{std::move(rows)}\n    { }\n    explicit result_set(const mutation&);\n    bool empty() const {\n        return _rows.empty();\n    }\n    // throws std::out_of_range on error\n    const result_set_row& row(size_t idx) const {\n        if (idx >= _rows.size()) {\n            throw std::out_of_range(\"no such row in result set: \" + std::to_string(idx));\n        }\n        return _rows[idx];\n    }\n    const std::vector<result_set_row>& rows() const {\n        return _rows;\n    }\n    const schema_ptr& schema() const {\n        return _schema;\n    }\n    friend inline bool operator==(const result_set& x, const result_set& y);\n    friend std::ostream& operator<<(std::ostream& out, const result_set& rs);\n};\n\ninline bool operator==(const result_set& x, const result_set& y) {\n    return x._rows == y._rows;\n}\n\n}\n\ntemplate <> struct fmt::formatter<query::result_set> : fmt::ostream_formatter {};\ntemplate <> struct fmt::formatter<query::result_set_row> : fmt::ostream_formatter {};\n"
        },
        {
          "name": "query-result-writer.hh",
          "type": "blob",
          "size": 8.21875,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"types/types.hh\"\n#include \"query-request.hh\"\n#include \"query-result.hh\"\n#include \"utils/digest_algorithm.hh\"\n#include \"utils/digester.hh\"\n#include \"full_position.hh\"\n#include \"mutation/tombstone.hh\"\n#include \"idl/query.dist.hh\"\n#include \"idl/query.dist.impl.hh\"\n\nnamespace query {\n\nclass result::partition_writer {\n    result_request _request;\n    ser::after_qr_partition__key<bytes_ostream> _w;\n    const partition_slice& _slice;\n    // We are tasked with keeping track of the range\n    // as well, since we are the primary \"context\"\n    // when iterating \"inside\" a partition\n    const clustering_row_ranges& _ranges;\n    ser::query_result__partitions<bytes_ostream>& _pw;\n    ser::vector_position _pos;\n    digester& _digest;\n    digester _digest_pos;\n    uint64_t& _row_count;\n    uint32_t& _partition_count;\n    api::timestamp_type& _last_modified;\npublic:\n    partition_writer(\n        result_request request,\n        const partition_slice& slice,\n        const clustering_row_ranges& ranges,\n        ser::query_result__partitions<bytes_ostream>& pw,\n        ser::vector_position pos,\n        ser::after_qr_partition__key<bytes_ostream> w,\n        digester& digest,\n        uint64_t& row_count,\n        uint32_t& partition_count,\n        api::timestamp_type& last_modified)\n        : _request(request)\n        , _w(std::move(w))\n        , _slice(slice)\n        , _ranges(ranges)\n        , _pw(pw)\n        , _pos(std::move(pos))\n        , _digest(digest)\n        , _digest_pos(digest)\n        , _row_count(row_count)\n        , _partition_count(partition_count)\n        , _last_modified(last_modified)\n    { }\n\n    bool requested_digest() const {\n        return _request != result_request::only_result;\n    }\n\n    bool requested_result() const {\n        return _request != result_request::only_digest;\n    }\n\n    ser::after_qr_partition__key<bytes_ostream> start() {\n        return std::move(_w);\n    }\n\n    // Cancels the whole partition element.\n    // Can be called at any stage of writing before this element is finalized.\n    // Do not use this writer after that.\n    void retract() {\n        _digest = _digest_pos;\n        _pw.rollback(_pos);\n    }\n\n    const clustering_row_ranges& ranges() const {\n        return _ranges;\n    }\n    const partition_slice& slice() const {\n        return _slice;\n    }\n    digester& digest() {\n        return _digest;\n    }\n    uint64_t& row_count() {\n        return _row_count;\n    }\n    uint32_t& partition_count() {\n        return _partition_count;\n    }\n    api::timestamp_type& last_modified() {\n        return _last_modified;\n    }\n\n};\n\nclass result::builder {\n    bytes_ostream _out;\n    const partition_slice& _slice;\n    ser::query_result__partitions<bytes_ostream> _w;\n    result_request _request;\n    uint64_t _row_count = 0;\n    uint32_t _partition_count = 0;\n    api::timestamp_type _last_modified = api::missing_timestamp;\n    short_read _short_read;\n    digester _digest;\n    result_memory_accounter _memory_accounter;\n    const uint64_t _tombstone_limit = query::max_tombstones;\n    uint64_t _tombstones = 0;\npublic:\n    builder(const partition_slice& slice, result_options options, result_memory_accounter memory_accounter, uint64_t tombstone_limit)\n        : _slice(slice)\n        , _w(ser::writer_of_query_result<bytes_ostream>(_out).start_partitions())\n        , _request(options.request)\n        , _digest(digester(options.digest_algo))\n        , _memory_accounter(std::move(memory_accounter))\n        , _tombstone_limit(tombstone_limit)\n    { }\n    builder(builder&&) = delete; // _out is captured by reference\n\n    void mark_as_short_read() { _short_read = short_read::yes; }\n    short_read is_short_read() const { return _short_read; }\n\n    result_memory_accounter& memory_accounter() { return _memory_accounter; }\n\n    stop_iteration bump_and_check_tombstone_limit() {\n        ++_tombstones;\n        if (_tombstones < _tombstone_limit) {\n            return stop_iteration::no;\n        }\n        if (!_slice.options.contains<partition_slice::option::allow_short_read>()) {\n            // The read is unpaged, we cannot interrupt it early without failing it.\n            // Better let it continue.\n            return stop_iteration::no;\n        }\n        return stop_iteration::yes;\n    }\n\n    const partition_slice& slice() const { return _slice; }\n\n    uint64_t row_count() const {\n        return _row_count;\n    }\n\n    uint32_t partition_count() const {\n        return _partition_count;\n    }\n\n    // Starts new partition and returns a builder for its contents.\n    // Invalidates all previously obtained builders\n    partition_writer add_partition(const schema& s, const partition_key& key) {\n        auto pos = _w.pos();\n        // fetch the row range for this partition already.\n        auto& ranges = _slice.row_ranges(s, key);\n        auto after_key = [this, pw = _w.add(), &key] () mutable {\n            if (_slice.options.contains<partition_slice::option::send_partition_key>()) {\n                return std::move(pw).write_key(key);\n            } else {\n                return std::move(pw).skip_key();\n            }\n        }();\n        if (_request != result_request::only_result) {\n            _digest.feed_hash(key, s);\n        }\n        return partition_writer(_request, _slice, ranges, _w, std::move(pos), std::move(after_key), _digest, _row_count,\n                                _partition_count, _last_modified);\n    }\n\n    result build(std::optional<full_position> last_pos = {}) {\n        std::move(_w).end_partitions().end_query_result();\n        switch (_request) {\n        case result_request::only_result:\n            return result(std::move(_out), _short_read, _row_count, _partition_count, std::move(last_pos), std::move(_memory_accounter).done());\n        case result_request::only_digest: {\n            bytes_ostream buf;\n            ser::writer_of_query_result<bytes_ostream>(buf).start_partitions().end_partitions().end_query_result();\n            return result(std::move(buf), result_digest(_digest.finalize_array()), _last_modified, _short_read, {}, {}, std::move(last_pos));\n        }\n        case result_request::result_and_digest:\n            return result(std::move(_out), result_digest(_digest.finalize_array()),\n                          _last_modified, _short_read, _row_count, _partition_count, std::move(last_pos), std::move(_memory_accounter).done());\n        }\n        abort();\n    }\n};\n\n}\n\nclass row;\nclass static_row;\nclass clustering_row;\nclass range_tombstone_change;\n\n// Adds mutation to query::result.\nclass mutation_querier {\n    const schema& _schema;\n    query::result_memory_accounter& _memory_accounter;\n    query::result::partition_writer _pw;\n    ser::qr_partition__static_row__cells<bytes_ostream> _static_cells_wr;\n    bool _live_data_in_static_row{};\n    uint64_t _live_clustering_rows = 0;\n    std::optional<ser::qr_partition__rows<bytes_ostream>> _rows_wr;\nprivate:\n    void query_static_row(const row& r, tombstone current_tombstone);\n    void prepare_writers();\npublic:\n    mutation_querier(const schema& s, query::result::partition_writer pw,\n                     query::result_memory_accounter& memory_accounter);\n    void consume(tombstone) { }\n    // Requires that sr.has_any_live_data()\n    stop_iteration consume(static_row&& sr, tombstone current_tombstone);\n    // Requires that cr.has_any_live_data()\n    stop_iteration consume(clustering_row&& cr, row_tombstone current_tombstone);\n    stop_iteration consume(range_tombstone_change&&) { return stop_iteration::no; }\n    uint64_t consume_end_of_stream();\n};\n\nclass query_result_builder {\n    const schema& _schema;\n    query::result::builder& _rb;\n    std::optional<mutation_querier> _mutation_consumer;\n    // We need to remember that we requested stop, to mark the read as short in the end.\n    stop_iteration _stop;\npublic:\n    query_result_builder(const schema& s, query::result::builder& rb) noexcept;\n\n    void consume_new_partition(const dht::decorated_key& dk);\n    void consume(tombstone t);\n    stop_iteration consume(static_row&& sr, tombstone t, bool);\n    stop_iteration consume(clustering_row&& cr, row_tombstone t, bool);\n    stop_iteration consume(range_tombstone_change&& rtc);\n    stop_iteration consume_end_of_partition();\n    void consume_end_of_stream();\n};\n"
        },
        {
          "name": "query-result.hh",
          "type": "blob",
          "size": 17.1015625,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"bytes_ostream.hh\"\n#include \"utils/digest_algorithm.hh\"\n#include \"query-request.hh\"\n#include \"full_position.hh\"\n#include <optional>\n#include <fmt/ostream.h>\n#include <seastar/util/bool_class.hh>\n#include \"seastarx.hh\"\n\nnamespace query {\n\nstruct short_read_tag { };\nusing short_read = bool_class<short_read_tag>;\n\n// result_memory_limiter, result_memory_accounter and result_memory_tracker\n// form an infrastructure for limiting size of query results.\n//\n// result_memory_limiter is a shard-local object which ensures that all results\n// combined do not use more than 10% of the shard memory.\n//\n// result_memory_accounter is used by result producers, updates the shard-local\n// limits as well as keeps track of the individual maximum result size limit\n// which is 1 MB.\n//\n// result_memory_tracker is just an object that makes sure the\n// result_memory_limiter is notified when memory is released (but not sooner).\n\nclass result_memory_accounter;\n\nclass result_memory_limiter {\n    const size_t _maximum_total_result_memory;\n    semaphore _memory_limiter;\npublic:\n    static constexpr size_t minimum_result_size = 4 * 1024;\n    static constexpr size_t maximum_result_size = 1 * 1024 * 1024;\n    static constexpr size_t unlimited_result_size = std::numeric_limits<size_t>::max();\npublic:\n    explicit result_memory_limiter(size_t maximum_total_result_memory)\n        : _maximum_total_result_memory(maximum_total_result_memory)\n        , _memory_limiter(_maximum_total_result_memory)\n    { }\n\n    result_memory_limiter(const result_memory_limiter&) = delete;\n    result_memory_limiter(result_memory_limiter&&) = delete;\n\n    ssize_t total_used_memory() const {\n        return _maximum_total_result_memory - _memory_limiter.available_units();\n    }\n\n    // Reserves minimum_result_size and creates new memory accounter for\n    // mutation query. Uses the specified maximum result size and may be\n    // stopped before reaching it due to memory pressure on shard.\n    future<result_memory_accounter> new_mutation_read(query::max_result_size max_result_size, short_read short_read_allowed);\n\n    // Reserves minimum_result_size and creates new memory accounter for\n    // data query. Uses the specified maximum result size, result will *not*\n    // be stopped due to on shard memory pressure in order to avoid digest\n    // mismatches.\n    future<result_memory_accounter> new_data_read(query::max_result_size max_result_size, short_read short_read_allowed);\n\n    // Creates a memory accounter for digest reads. Such accounter doesn't\n    // contribute to the shard memory usage, but still stops producing the\n    // result after individual limit has been reached.\n    future<result_memory_accounter> new_digest_read(query::max_result_size max_result_size, short_read short_read_allowed);\n\n    // Checks whether the result can grow any more, takes into account only\n    // the per shard limit.\n    stop_iteration check() const {\n        return stop_iteration(_memory_limiter.current() <= 0);\n    }\n\n    // Consumes n bytes from memory limiter and checks whether the result\n    // can grow any more (considering just the per-shard limit).\n    stop_iteration update_and_check(size_t n) {\n        _memory_limiter.consume(n);\n        return check();\n    }\n\n    void release(size_t n) noexcept {\n        _memory_limiter.signal(n);\n    }\n\n    semaphore& sem() noexcept { return _memory_limiter; }\n};\n\n\nclass result_memory_tracker {\n    semaphore_units<> _units;\n    size_t _used_memory;\nprivate:\n    static thread_local semaphore _dummy;\npublic:\n    result_memory_tracker() noexcept : _units(_dummy, 0), _used_memory(0) { }\n    result_memory_tracker(semaphore& sem, size_t blocked, size_t used) noexcept\n        : _units(sem, blocked), _used_memory(used) { }\n    size_t used_memory() const { return _used_memory; }\n};\n\nclass result_memory_accounter {\n    result_memory_limiter* _limiter = nullptr;\n    size_t _blocked_bytes = 0;\n    size_t _used_memory = 0;\n    size_t _total_used_memory = 0;\n    query::max_result_size _maximum_result_size;\n    stop_iteration _stop_on_global_limit;\n    short_read _short_read_allowed;\n    mutable bool _below_soft_limit = true;\nprivate:\n    // Mutation query accounter. Uses provided individual result size limit and\n    // will stop when shard memory pressure grows too high.\n    struct mutation_query_tag { };\n    explicit result_memory_accounter(mutation_query_tag, result_memory_limiter& limiter, query::max_result_size max_size, short_read short_read_allowed) noexcept\n        : _limiter(&limiter)\n        , _blocked_bytes(result_memory_limiter::minimum_result_size)\n        , _maximum_result_size(max_size)\n        , _stop_on_global_limit(true)\n        , _short_read_allowed(short_read_allowed)\n    { }\n\n    // Data query accounter. Uses provided individual result size limit and\n    // will *not* stop even though shard memory pressure grows too high.\n    struct data_query_tag { };\n    explicit result_memory_accounter(data_query_tag, result_memory_limiter& limiter, query::max_result_size max_size, short_read short_read_allowed) noexcept\n        : _limiter(&limiter)\n        , _blocked_bytes(result_memory_limiter::minimum_result_size)\n        , _maximum_result_size(max_size)\n        , _short_read_allowed(short_read_allowed)\n    { }\n\n    // Digest query accounter. Uses provided individual result size limit and\n    // will *not* stop even though shard memory pressure grows too high. This\n    // accounter does not contribute to the shard memory limits.\n    struct digest_query_tag { };\n    explicit result_memory_accounter(digest_query_tag, result_memory_limiter&, query::max_result_size max_size, short_read short_read_allowed) noexcept\n        : _blocked_bytes(0)\n        , _maximum_result_size(max_size)\n        , _short_read_allowed(short_read_allowed)\n    { }\n\n    stop_iteration check_local_limit() const;\n\n    friend class result_memory_limiter;\npublic:\n    explicit result_memory_accounter(size_t max_size) noexcept\n        : _blocked_bytes(0)\n        , _maximum_result_size(max_size) {\n    }\n\n    result_memory_accounter(result_memory_accounter&& other) noexcept\n        : _limiter(std::exchange(other._limiter, nullptr))\n        , _blocked_bytes(other._blocked_bytes)\n        , _used_memory(other._used_memory)\n        , _total_used_memory(other._total_used_memory)\n        , _maximum_result_size(other._maximum_result_size)\n        , _stop_on_global_limit(other._stop_on_global_limit)\n        , _short_read_allowed(other._short_read_allowed)\n        , _below_soft_limit(other._below_soft_limit)\n    { }\n\n    result_memory_accounter& operator=(result_memory_accounter&& other) noexcept {\n        if (this != &other) {\n            this->~result_memory_accounter();\n            new (this) result_memory_accounter(std::move(other));\n        }\n        return *this;\n    }\n\n    ~result_memory_accounter() {\n        if (_limiter) {\n            _limiter->release(_blocked_bytes);\n        }\n    }\n\n    size_t used_memory() const { return _used_memory; }\n\n    // Consume n more bytes for the result. Returns stop_iteration::yes if\n    // the result cannot grow any more (taking into account both individual\n    // and per-shard limits).\n    stop_iteration update_and_check(size_t n) {\n        _used_memory += n;\n        _total_used_memory += n;\n        auto stop = check_local_limit();\n        if (_limiter && _used_memory > _blocked_bytes) {\n            auto to_block = std::min(_used_memory - _blocked_bytes, n);\n            _blocked_bytes += to_block;\n            stop = (_limiter->update_and_check(to_block) && _stop_on_global_limit) || stop;\n            if (stop && !_short_read_allowed) {\n                // If we are here we stopped because of the global limit.\n                throw std::runtime_error(\"Maximum amount of memory for building query results is exhausted, unpaged query cannot be finished\");\n            }\n        }\n        return stop;\n    }\n\n    // Checks whether the result can grow any more.\n    stop_iteration check() const {\n        auto stop = check_local_limit();\n        if (!stop && _used_memory >= _blocked_bytes && _limiter) {\n            return _limiter->check() && _stop_on_global_limit;\n        }\n        return stop;\n    }\n\n    // Consume n more bytes for the result.\n    void update(size_t n) {\n        update_and_check(n);\n    }\n\n    result_memory_tracker done() && {\n        if (!_limiter) {\n            return { };\n        }\n        auto& sem = std::exchange(_limiter, nullptr)->sem();\n        return result_memory_tracker(sem, _blocked_bytes, _used_memory);\n    }\n};\n\ninline future<result_memory_accounter> result_memory_limiter::new_mutation_read(query::max_result_size max_size, short_read short_read_allowed) {\n    return _memory_limiter.wait(minimum_result_size).then([this, max_size, short_read_allowed] {\n        return result_memory_accounter(result_memory_accounter::mutation_query_tag(), *this, max_size, short_read_allowed);\n    });\n}\n\ninline future<result_memory_accounter> result_memory_limiter::new_data_read(query::max_result_size max_size, short_read short_read_allowed) {\n    return _memory_limiter.wait(minimum_result_size).then([this, max_size, short_read_allowed] {\n        return result_memory_accounter(result_memory_accounter::data_query_tag(), *this, max_size, short_read_allowed);\n    });\n}\n\ninline future<result_memory_accounter> result_memory_limiter::new_digest_read(query::max_result_size max_size, short_read short_read_allowed) {\n    return make_ready_future<result_memory_accounter>(result_memory_accounter(result_memory_accounter::digest_query_tag(), *this, max_size, short_read_allowed));\n}\n\nenum class result_request {\n    only_result,\n    only_digest,\n    result_and_digest,\n};\n\nstruct result_options {\n    result_request request = result_request::only_result;\n    digest_algorithm digest_algo = query::digest_algorithm::none;\n\n    static result_options only_result() {\n        return result_options{};\n    }\n\n    static result_options only_digest(digest_algorithm da) {\n        return {result_request::only_digest, da};\n    }\n};\n\nclass result_digest {\npublic:\n    using type = std::array<uint8_t, 16>;\nprivate:\n    type _digest;\npublic:\n    result_digest() = default;\n    result_digest(type&& digest) : _digest(std::move(digest)) {}\n    const type& get() const { return _digest; }\n    bool operator==(const result_digest& rh) const = default;\n};\n\n//\n// The query results are stored in a serialized form. This is in order to\n// address the following problems, which a structured format has:\n//\n//   - high level of indirection (vector of vectors of vectors of blobs), which\n//     is not CPU cache friendly\n//\n//   - high allocation rate due to fine-grained object structure\n//\n// On replica side, the query results are probably going to be serialized in\n// the transport layer anyway, so serializing the results up-front doesn't add\n// net work. There is no processing of the query results on replica other than\n// concatenation in case of range queries and checksum calculation. If query\n// results are collected in serialized form from different cores, we can\n// concatenate them without copying by simply appending the fragments into the\n// packet.\n//\n// On coordinator side, the query results would have to be parsed from the\n// transport layer buffers anyway, so the fact that iterators parse it also\n// doesn't add net work, but again saves allocations and copying. The CQL\n// server doesn't need complex data structures to process the results, it just\n// goes over it linearly consuming it.\n//\n// The coordinator side could be optimized even further for CQL queries which\n// do not need processing (eg. select * from cf where ...). We could make the\n// replica send the query results in the format which is expected by the CQL\n// binary protocol client. So in the typical case the coordinator would just\n// pass the data using zero-copy to the client, prepending a header.\n//\n// Users which need more complex structure of query results can convert this\n// to query::result_set.\n//\n// Related headers:\n//  - query-result-reader.hh\n//  - query-result-writer.hh\n\nclass result {\n    bytes_ostream _w;\n    std::optional<result_digest> _digest;\n    std::optional<uint32_t> _row_count_low_bits;\n    api::timestamp_type _last_modified = api::missing_timestamp;\n    short_read _short_read;\n    query::result_memory_tracker _memory_tracker;\n    std::optional<uint32_t> _partition_count;\n    std::optional<uint32_t> _row_count_high_bits;\n    std::optional<full_position> _last_position;\npublic:\n    class builder;\n    class partition_writer;\n    friend class result_merger;\n\n    result();\n    result(bytes_ostream&& w, short_read sr, std::optional<uint32_t> c_low_bits, std::optional<uint32_t> pc,\n           std::optional<uint32_t> c_high_bits, std::optional<full_position> last_position, result_memory_tracker memory_tracker = { })\n        : _w(std::move(w))\n        , _row_count_low_bits(c_low_bits)\n        , _short_read(sr)\n        , _memory_tracker(std::move(memory_tracker))\n        , _partition_count(pc)\n        , _row_count_high_bits(c_high_bits)\n        , _last_position(std::move(last_position))\n    {\n        w.reduce_chunk_count();\n    }\n    result(bytes_ostream&& w, std::optional<result_digest> d, api::timestamp_type last_modified,\n           short_read sr, std::optional<uint32_t> c_low_bits, std::optional<uint32_t> pc, std::optional<uint32_t> c_high_bits,\n           std::optional<full_position> last_position, result_memory_tracker memory_tracker = { })\n        : _w(std::move(w))\n        , _digest(d)\n        , _row_count_low_bits(c_low_bits)\n        , _last_modified(last_modified)\n        , _short_read(sr)\n        , _memory_tracker(std::move(memory_tracker))\n        , _partition_count(pc)\n        , _row_count_high_bits(c_high_bits)\n        , _last_position(std::move(last_position))\n    {\n        w.reduce_chunk_count();\n    }\n    result(bytes_ostream&& w, short_read sr, uint64_t c, std::optional<uint32_t> pc,\n           std::optional<full_position> last_position, result_memory_tracker memory_tracker = { })\n        : _w(std::move(w))\n        , _row_count_low_bits(static_cast<uint32_t>(c))\n        , _short_read(sr)\n        , _memory_tracker(std::move(memory_tracker))\n        , _partition_count(pc)\n        , _row_count_high_bits(static_cast<uint32_t>(c >> 32))\n        , _last_position(std::move(last_position))\n    {\n        w.reduce_chunk_count();\n    }\n    result(bytes_ostream&& w, std::optional<result_digest> d, api::timestamp_type last_modified,\n           short_read sr, uint64_t c, std::optional<uint32_t> pc, std::optional<full_position> last_position, result_memory_tracker memory_tracker = { })\n        : _w(std::move(w))\n        , _digest(d)\n        , _row_count_low_bits(static_cast<uint32_t>(c))\n        , _last_modified(last_modified)\n        , _short_read(sr)\n        , _memory_tracker(std::move(memory_tracker))\n        , _partition_count(pc)\n        , _row_count_high_bits(static_cast<uint32_t>(c >> 32))\n        , _last_position(std::move(last_position))\n    {\n        w.reduce_chunk_count();\n    }\n    result(result&&) = default;\n    result& operator=(result&&) = default;\n\n    const bytes_ostream& buf() const {\n        return _w;\n    }\n\n    const std::optional<result_digest>& digest() const {\n        return _digest;\n    }\n\n    const std::optional<uint32_t> row_count_low_bits() const {\n        return _row_count_low_bits;\n    }\n\n    const std::optional<uint32_t> row_count_high_bits() const {\n        return _row_count_high_bits;\n    }\n\n    const std::optional<uint64_t> row_count() const {\n        if (!_row_count_low_bits) {\n            return _row_count_low_bits;\n        }\n        return (static_cast<uint64_t>(_row_count_high_bits.value_or(0)) << 32) | _row_count_low_bits.value();\n    }\n\n    void set_row_count(std::optional<uint64_t> row_count) {\n        if (!row_count) {\n            _row_count_low_bits = std::nullopt;\n            _row_count_high_bits = std::nullopt;\n        } else {\n            _row_count_low_bits = std::make_optional(static_cast<uint32_t>(row_count.value()));\n            _row_count_high_bits = std::make_optional(static_cast<uint32_t>(row_count.value() >> 32));\n        }\n    }\n\n    api::timestamp_type last_modified() const {\n        return _last_modified;\n    }\n\n    short_read is_short_read() const {\n        return _short_read;\n    }\n\n    const std::optional<uint32_t>& partition_count() const {\n        return _partition_count;\n    }\n\n    void ensure_counts();\n\n    const std::optional<full_position>& last_position() const {\n        return _last_position;\n    }\n\n    void set_last_position(std::optional<full_position> last_position) {\n        _last_position = std::move(last_position);\n    }\n\n    // Return _last_position if replica filled it, otherwise calculate it based\n    // on the content (by looking up the last row in the last partition).\n    full_position get_or_calculate_last_position() const;\n\n    struct printer {\n        schema_ptr s;\n        const query::partition_slice& slice;\n        const query::result& res;\n    };\n\n    sstring pretty_print(schema_ptr, const query::partition_slice&) const;\n    printer pretty_printer(schema_ptr, const query::partition_slice&) const;\n};\n\nstd::ostream& operator<<(std::ostream& os, const query::result::printer&);\n}\n\ntemplate <> struct fmt::formatter<query::result::printer> : fmt::ostream_formatter {};\n"
        },
        {
          "name": "query.cc",
          "type": "blob",
          "size": 17.001953125,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include <limits>\n#include <memory>\n#include <stdexcept>\n#include <fmt/ranges.h>\n#include \"query-request.hh\"\n#include \"query-result.hh\"\n#include \"query-result-writer.hh\"\n#include \"query-result-set.hh\"\n#include <seastar/core/shared_ptr.hh>\n#include <seastar/core/thread.hh>\n#include \"bytes.hh\"\n#include \"mutation/mutation_partition_serializer.hh\"\n#include \"query-result-reader.hh\"\n#include \"query_result_merger.hh\"\n#include \"partition_slice_builder.hh\"\n#include \"schema/schema_registry.hh\"\n#include \"utils/assert.hh\"\n#include \"utils/overloaded_functor.hh\"\n\nnamespace query {\n\nstatic logging::logger qlogger(\"query\");\n\nconstexpr size_t result_memory_limiter::minimum_result_size;\nconstexpr size_t result_memory_limiter::maximum_result_size;\nconstexpr size_t result_memory_limiter::unlimited_result_size;\n\nthread_local semaphore result_memory_tracker::_dummy { 0 };\n\nconst dht::partition_range full_partition_range = dht::partition_range::make_open_ended_both_sides();\nconst clustering_range full_clustering_range = clustering_range::make_open_ended_both_sides();\n\nstd::ostream& operator<<(std::ostream& out, const specific_ranges& s);\n\nstd::ostream& operator<<(std::ostream& out, const partition_slice& ps) {\n    fmt::print(out,\n               \"{{regular_cols=[{}], static_cols=[{}], rows=[{}]\",\n               fmt::join(ps.regular_columns, \", \"),\n               fmt::join(ps.static_columns, \", \"),\n               ps._row_ranges);\n    if (ps._specific_ranges) {\n        fmt::print(out, \", specific=[{}]\", *ps._specific_ranges);\n    }\n    // FIXME: pretty print options\n    fmt::print(out, \", options={:x}, , partition_row_limit={}}}\",\n               ps.options.mask(), ps.partition_row_limit());\n    return out;\n}\n\nstd::ostream& operator<<(std::ostream& out, const read_command& r) {\n    fmt::print(out, \"read_command{{cf_id={}, version={}, slice={}, limit={}, timestamp={}, partition_limit={}, query_uuid={}, is_first_page={}, read_timestamp={}}}\",\n               r.cf_id, r.schema_version, r.slice, r.get_row_limit(), r.timestamp.time_since_epoch().count(), r.partition_limit, r.query_uuid, r.is_first_page, r.read_timestamp);\n    return out;\n}\n\nlw_shared_ptr<query::read_command> reversed(lw_shared_ptr<query::read_command>&& cmd)\n{\n    auto schema = local_schema_registry().get(cmd->schema_version)->get_reversed();\n    cmd->schema_version = schema->version();\n    cmd->slice = query::legacy_reverse_slice_to_native_reverse_slice(*schema, cmd->slice);\n\n    return std::move(cmd);\n}\n\nstd::ostream& operator<<(std::ostream& out, const mapreduce_request::reduction_type& r) {\n    out << \"reduction_type{\";\n    switch (r) {\n        case mapreduce_request::reduction_type::count:\n            out << \"count\";\n            break;\n        case mapreduce_request::reduction_type::aggregate:\n            out << \"aggregate\";\n            break;\n    }\n    return out << \"}\";\n}\n\nstd::ostream& operator<<(std::ostream& out, const mapreduce_request::aggregation_info& a) {\n    fmt::print(out, \"aggregation_info{{, name={}, column_names=[{}]}}\",\n               a.name, fmt::join(a.column_names, \",\"));;\n    return out;\n}\n\nstd::ostream& operator<<(std::ostream& out, const mapreduce_request& r) {\n    auto ms = std::chrono::time_point_cast<std::chrono::milliseconds>(r.timeout).time_since_epoch().count();\n    fmt::print(out, \"mapreduce_request{{reductions=[{}]\",\n               fmt::join(r.reduction_types, \",\"));\n    if (r.aggregation_infos) {\n        fmt::print(out, \", aggregation_infos=[{}]\",\n                   fmt::join(r.aggregation_infos.value(), \",\"));\n    }\n    fmt::print(out, \"cmd={}, pr={}, cl={}, timeout(ms)={}}}\",\n               r.cmd, r.pr, r.cl, ms);\n    return out;\n}\n\n\nstd::ostream& operator<<(std::ostream& out, const specific_ranges& s) {\n    fmt::print(out, \"{{{} : {}}}\", s._pk, fmt::join(s._ranges, \", \"));\n    return out;\n}\n\nvoid trim_clustering_row_ranges_to(const schema& s, clustering_row_ranges& ranges, position_in_partition pos) {\n    auto cmp = position_in_partition::composite_tri_compare(s);\n\n    auto it = ranges.begin();\n    while (it != ranges.end()) {\n        auto end_bound = position_in_partition_view::for_range_end(*it);\n        if (cmp(end_bound, pos) <= 0) {\n            it = ranges.erase(it);\n            continue;\n        } else if (auto start_bound = position_in_partition_view::for_range_start(*it); cmp(start_bound, pos) <= 0) {\n            SCYLLA_ASSERT(cmp(pos, end_bound) < 0);\n            *it = clustering_range(clustering_range::bound(pos.key(), pos.get_bound_weight() != bound_weight::after_all_prefixed), it->end());\n        }\n        ++it;\n    }\n}\n\nvoid trim_clustering_row_ranges_to(const schema& s, clustering_row_ranges& ranges, const clustering_key& key) {\n    return trim_clustering_row_ranges_to(s, ranges, position_in_partition::after_key(s, key));\n}\n\n\nclustering_range reverse(const clustering_range& range) {\n    if (range.is_singular()) {\n        return range;\n    }\n    return clustering_range(range.end(), range.start());\n}\n\n\nstatic void reverse_clustering_ranges_bounds(clustering_row_ranges& ranges) {\n    for (auto& range : ranges) {\n        range = reverse(range);\n    }\n}\n\npartition_slice legacy_reverse_slice_to_native_reverse_slice(const schema& schema, partition_slice slice) {\n    return partition_slice_builder(schema, std::move(slice))\n        .mutate_ranges([] (clustering_row_ranges& ranges) { reverse_clustering_ranges_bounds(ranges); })\n        .mutate_specific_ranges([] (specific_ranges& ranges) { reverse_clustering_ranges_bounds(ranges.ranges()); })\n        .build();\n}\n\npartition_slice native_reverse_slice_to_legacy_reverse_slice(const schema& schema, partition_slice slice) {\n    // They are the same, we give them different names to express intent\n    return legacy_reverse_slice_to_native_reverse_slice(schema, std::move(slice));\n}\n\npartition_slice reverse_slice(const schema& schema, partition_slice slice) {\n    return partition_slice_builder(schema, std::move(slice))\n        .mutate_ranges([] (clustering_row_ranges& ranges) {\n            std::reverse(ranges.begin(), ranges.end());\n            reverse_clustering_ranges_bounds(ranges);\n        })\n        .mutate_specific_ranges([] (specific_ranges& sranges) {\n            auto& ranges = sranges.ranges();\n            std::reverse(ranges.begin(), ranges.end());\n            reverse_clustering_ranges_bounds(ranges);\n        })\n        .with_option_toggled<partition_slice::option::reversed>()\n        .build();\n}\n\npartition_slice::partition_slice(clustering_row_ranges row_ranges,\n    query::column_id_vector static_columns,\n    query::column_id_vector regular_columns,\n    option_set options,\n    std::unique_ptr<specific_ranges> specific_ranges,\n    cql_serialization_format cql_format,\n    uint32_t partition_row_limit_low_bits,\n    uint32_t partition_row_limit_high_bits)\n    : _row_ranges(std::move(row_ranges))\n    , static_columns(std::move(static_columns))\n    , regular_columns(std::move(regular_columns))\n    , options(options)\n    , _specific_ranges(std::move(specific_ranges))\n    , _partition_row_limit_low_bits(partition_row_limit_low_bits)\n    , _partition_row_limit_high_bits(partition_row_limit_high_bits)\n{\n    cql_format.ensure_supported();\n}\n\npartition_slice::partition_slice(clustering_row_ranges row_ranges,\n    query::column_id_vector static_columns,\n    query::column_id_vector regular_columns,\n    option_set options,\n    std::unique_ptr<specific_ranges> specific_ranges,\n    uint64_t partition_row_limit)\n    : partition_slice(std::move(row_ranges), std::move(static_columns), std::move(regular_columns), options,\n            std::move(specific_ranges), cql_serialization_format::latest(), static_cast<uint32_t>(partition_row_limit),\n            static_cast<uint32_t>(partition_row_limit >> 32))\n{}\n\npartition_slice::partition_slice(clustering_row_ranges ranges, const schema& s, const column_set& columns, option_set options)\n    : partition_slice(ranges, query::column_id_vector{}, query::column_id_vector{}, options)\n{\n    regular_columns.reserve(columns.count());\n    for (ordinal_column_id id = columns.find_first(); id != column_set::npos; id = columns.find_next(id)) {\n        const column_definition& def = s.column_at(id);\n        if (def.is_static()) {\n            static_columns.push_back(def.id);\n        } else if (def.is_regular()) {\n            regular_columns.push_back(def.id);\n        } // else clustering or partition key column - skip, these are controlled by options\n    }\n}\n\npartition_slice::partition_slice(partition_slice&&) = default;\n\npartition_slice& partition_slice::operator=(partition_slice&& other) noexcept = default;\n\n// Only needed because selection_statement::execute does copies of its read_command\n// in the map-reduce op.\npartition_slice::partition_slice(const partition_slice& s)\n    : _row_ranges(s._row_ranges)\n    , static_columns(s.static_columns)\n    , regular_columns(s.regular_columns)\n    , options(s.options)\n    , _specific_ranges(s._specific_ranges ? std::make_unique<specific_ranges>(*s._specific_ranges) : nullptr)\n    , _partition_row_limit_low_bits(s._partition_row_limit_low_bits)\n    , _partition_row_limit_high_bits(s._partition_row_limit_high_bits)\n{}\n\npartition_slice::~partition_slice()\n{}\n\nconst clustering_row_ranges& partition_slice::row_ranges(const schema& s, const partition_key& k) const {\n    auto* r = _specific_ranges ? _specific_ranges->range_for(s, k) : nullptr;\n    return r ? *r : _row_ranges;\n}\n\nvoid partition_slice::set_range(const schema& s, const partition_key& k, clustering_row_ranges range) {\n    if (!_specific_ranges) {\n        _specific_ranges = std::make_unique<specific_ranges>(k, std::move(range));\n    } else {\n        _specific_ranges->add(s, k, std::move(range));\n    }\n}\n\nvoid partition_slice::clear_range(const schema& s, const partition_key& k) {\n    if (_specific_ranges && _specific_ranges->contains(s, k)) {\n        // just in case someone changes the impl above,\n        // we should do actual remove if specific_ranges suddenly\n        // becomes an actual map\n        SCYLLA_ASSERT(_specific_ranges->size() == 1);\n        _specific_ranges = nullptr;\n    }\n}\n\nclustering_row_ranges partition_slice::get_all_ranges() const {\n    auto all_ranges = default_row_ranges();\n    const auto& specific_ranges = get_specific_ranges();\n    if (specific_ranges) {\n        all_ranges.insert(all_ranges.end(), specific_ranges->ranges().begin(), specific_ranges->ranges().end());\n    }\n    return all_ranges;\n}\n\nsstring\nresult::pretty_print(schema_ptr s, const query::partition_slice& slice) const {\n    std::ostringstream out;\n    out << \"{ result: \" << result_set::from_raw_result(s, slice, *this);\n    out << \" digest: \";\n    if (_digest) {\n        out << std::hex << std::setw(2);\n        for (auto&& c : _digest->get()) {\n            out << unsigned(c) << \" \";\n        }\n    } else {\n        out << \"{}\";\n    }\n    out << \", short_read=\" << is_short_read() << \" }\";\n    return out.str();\n}\n\nquery::result::printer\nresult::pretty_printer(schema_ptr s, const query::partition_slice& slice) const {\n    return query::result::printer{s, slice, *this};\n}\n\nstd::ostream& operator<<(std::ostream& os, const query::result::printer& p) {\n    os << p.res.pretty_print(p.s, p.slice);\n    return os;\n}\n\nvoid result::ensure_counts() {\n    if (!_partition_count || !row_count()) {\n        uint64_t row_count;\n        std::tie(_partition_count, row_count) = result_view::do_with(*this, [] (auto&& view) {\n            return view.count_partitions_and_rows();\n        });\n        set_row_count(row_count);\n    }\n}\n\nfull_position result::get_or_calculate_last_position() const {\n    if (_last_position) {\n        return *_last_position;\n    }\n    return result_view::do_with(*this, [] (const result_view& v) {\n        return v.calculate_last_position();\n    });\n}\n\nresult::result()\n    : result([] {\n        bytes_ostream out;\n        ser::writer_of_query_result<bytes_ostream>(out).skip_partitions().end_query_result();\n        return out;\n    }(), short_read::no, 0, 0, {})\n{ }\n\nstatic void write_partial_partition(ser::writer_of_qr_partition<bytes_ostream>&& pw, const ser::qr_partition_view& pv, uint64_t rows_to_include) {\n    auto key = pv.key();\n    auto static_cells_wr = (key ? std::move(pw).write_key(*key) : std::move(pw).skip_key())\n            .start_static_row()\n            .start_cells();\n    for (auto&& cell : pv.static_row().cells()) {\n        static_cells_wr.add(cell);\n    }\n    auto rows_wr = std::move(static_cells_wr)\n            .end_cells()\n            .end_static_row()\n            .start_rows();\n    auto rows = pv.rows();\n    // rows.size() can be 0 is there's a single static row\n    auto it = rows.begin();\n    for (uint64_t i = 0; i < std::min(rows.size(), rows_to_include); ++i) {\n        rows_wr.add(*it++);\n    }\n    std::move(rows_wr).end_rows().end_qr_partition();\n}\n\nforeign_ptr<lw_shared_ptr<query::result>> result_merger::get() {\n    if (_partial.size() == 1) {\n        return std::move(_partial[0]);\n    }\n\n    bytes_ostream w;\n    auto partitions = ser::writer_of_query_result<bytes_ostream>(w).start_partitions();\n    uint64_t row_count = 0;\n    short_read is_short_read;\n    uint32_t partition_count = 0;\n\n    std::optional<full_position> last_position;\n\n    for (auto&& r : _partial) {\n        result_view::do_with(*r, [&] (result_view rv) {\n            last_position.reset();\n            for (auto&& pv : rv._v.partitions()) {\n                auto rows = pv.rows();\n                // If rows.empty(), then there's a static row, or there wouldn't be a partition\n                const uint64_t rows_in_partition = rows.size() ? : 1;\n                const uint64_t rows_to_include = std::min(_max_rows - row_count, rows_in_partition);\n                row_count += rows_to_include;\n                if (rows_to_include >= rows_in_partition) {\n                    partitions.add(pv);\n                    if (++partition_count >= _max_partitions) {\n                        return;\n                    }\n                } else if (rows_to_include > 0) {\n                    ++partition_count;\n                    write_partial_partition(partitions.add(), pv, rows_to_include);\n                    return;\n                } else {\n                    return;\n                }\n            }\n            last_position = r->last_position();\n        });\n        if (r->is_short_read()) {\n            is_short_read = short_read::yes;\n            break;\n        }\n        if (row_count >= _max_rows || partition_count >= _max_partitions) {\n            break;\n        }\n    }\n\n    std::move(partitions).end_partitions().end_query_result();\n    return make_foreign(make_lw_shared<query::result>(std::move(w), is_short_read, row_count, partition_count, std::move(last_position)));\n}\n\nstd::ostream& operator<<(std::ostream& out, const query::mapreduce_result::printer& p) {\n    if (p.functions.size() != p.res.query_results.size()) {\n        return out << \"[malformed mapreduce_result (\" << p.res.query_results.size()\n            << \" results, \" << p.functions.size() << \" aggregates)]\";\n    }\n\n    out << \"[\";\n    for (size_t i = 0; i < p.functions.size(); i++) {\n        auto& return_type = p.functions[i]->return_type();\n        out << return_type->to_string(bytes_view(*p.res.query_results[i]));\n\n        if (i + 1 < p.functions.size()) {\n            out << \", \";\n        }\n    }\n    return out << \"]\";\n}\n\n}\n\nstd::optional<query::clustering_range> position_range_to_clustering_range(const position_range& r, const schema& s) {\n    SCYLLA_ASSERT(r.start().get_type() == partition_region::clustered);\n    SCYLLA_ASSERT(r.end().get_type() == partition_region::clustered);\n\n    if (r.start().has_key() && r.end().has_key()\n            && clustering_key_prefix::equality(s)(r.start().key(), r.end().key())) {\n        SCYLLA_ASSERT(r.start().get_bound_weight() != r.end().get_bound_weight());\n\n        if (r.end().get_bound_weight() == bound_weight::after_all_prefixed\n                && r.start().get_bound_weight() != bound_weight::after_all_prefixed) {\n            // [before x, after x) and [for x, after x) get converted to [x, x].\n            return query::clustering_range::make_singular(r.start().key());\n        }\n\n        // [before x, for x) does not contain any keys.\n        return std::nullopt;\n    }\n\n    // position_range -> clustering_range\n    // (recall that position_ranges are always left-closed, right opened):\n    // [before x, ...), [for x, ...) -> [x, ...\n    // [after x, ...) -> (x, ...\n    // [..., before x), [..., for x) -> ..., x)\n    // [..., after x) -> ..., x]\n\n    auto to_bound = [&s] (const position_in_partition& p, bool left) -> std::optional<query::clustering_range::bound> {\n        if (p.is_before_all_clustered_rows(s)) {\n            SCYLLA_ASSERT(left);\n            return {};\n        }\n\n        if (p.is_after_all_clustered_rows(s)) {\n            SCYLLA_ASSERT(!left);\n            return {};\n        }\n\n        SCYLLA_ASSERT(p.has_key());\n\n        auto bw = p.get_bound_weight();\n        bool inclusive = left\n            ? bw != bound_weight::after_all_prefixed\n            : bw == bound_weight::after_all_prefixed;\n\n        return query::clustering_range::bound{p.key(), inclusive};\n    };\n\n    return query::clustering_range{to_bound(r.start(), true), to_bound(r.end(), false)};\n}\n"
        },
        {
          "name": "query_id.hh",
          "type": "blob",
          "size": 0.2021484375,
          "content": "// Copyright (C) 2023-present ScyllaDB\n// SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n\n#pragma once\n\n#include \"utils/UUID.hh\"\n\nusing query_id = utils::tagged_uuid<struct query_id_tag>;\n"
        },
        {
          "name": "query_ranges_to_vnodes.cc",
          "type": "blob",
          "size": 3.96484375,
          "content": "/*\n * Copyright (C) 2021-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"query_ranges_to_vnodes.hh\"\n\nstatic inline\nconst dht::token& start_token(const dht::partition_range& r) {\n    static const dht::token min_token = dht::minimum_token();\n    return r.start() ? r.start()->value().token() : min_token;\n}\n\nstatic inline\nconst dht::token& end_token(const dht::partition_range& r) {\n    static const dht::token max_token = dht::maximum_token();\n    return r.end() ? r.end()->value().token() : max_token;\n}\n\nquery_ranges_to_vnodes_generator::query_ranges_to_vnodes_generator(std::unique_ptr<locator::token_range_splitter> splitter, schema_ptr s, dht::partition_range_vector ranges, bool local) :\n        _s(s), _ranges(std::move(ranges)), _i(_ranges.begin()), _local(local), _splitter(std::move(splitter)) {}\n\ndht::partition_range_vector query_ranges_to_vnodes_generator::operator()(size_t n) {\n    n = std::min(n, size_t(1024));\n\n    dht::partition_range_vector result;\n    result.reserve(n);\n    while (_i != _ranges.end() && result.size() != n) {\n        process_one_range(n, result);\n    }\n    return result;\n}\n\nbool query_ranges_to_vnodes_generator::empty() const {\n    return _ranges.end() == _i;\n}\n\n/**\n * Compute all ranges we're going to query, in sorted order. Nodes can be replica destinations for many ranges,\n * so we need to restrict each scan to the specific range we want, or else we'd get duplicate results.\n */\nvoid query_ranges_to_vnodes_generator::process_one_range(size_t n, dht::partition_range_vector& ranges) {\n    dht::ring_position_comparator cmp(*_s);\n    dht::partition_range& cr = *_i;\n\n    auto get_remainder = [this, &cr] {\n        _i++;\n       return std::move(cr);\n    };\n\n    auto add_range = [&ranges] (dht::partition_range&& r) {\n        ranges.emplace_back(std::move(r));\n    };\n\n    if (_local) { // if the range is local no need to divide to vnodes\n        add_range(get_remainder());\n        return;\n    }\n\n    // special case for bounds containing exactly 1 token\n    if (start_token(cr) == end_token(cr)) {\n        if (start_token(cr).is_minimum()) {\n            _i++; // empty range? Move to the next one\n            return;\n        }\n        add_range(get_remainder());\n        return;\n    }\n\n    // divide the queryRange into pieces delimited by the ring\n    _splitter->reset(dht::ring_position_view::for_range_start(cr));\n    while (auto token_opt = _splitter->next_token()) {\n        auto upper_bound_token = *token_opt;\n        /*\n         * remainder can be a range/bounds of token _or_ keys and we want to split it with a token:\n         *   - if remainder is tokens, then we'll just split using the provided token.\n         *   - if remainder is keys, we want to split using token.upperBoundKey. For instance, if remainder\n         *     is [DK(10, 'foo'), DK(20, 'bar')], and we have 3 nodes with tokens 0, 15, 30. We want to\n         *     split remainder to A=[DK(10, 'foo'), 15] and B=(15, DK(20, 'bar')]. But since we can't mix\n         *     tokens and keys at the same time in a range, we uses 15.upperBoundKey() to have A include all\n         *     keys having 15 as token and B include none of those (since that is what our node owns).\n         * asSplitValue() abstracts that choice.\n         */\n\n        dht::ring_position split_point(upper_bound_token, dht::ring_position::token_bound::end);\n\n        if (cmp(dht::ring_position_view::for_range_end(cr), split_point) <= 0) {\n            break; // no more splits\n        }\n\n        if (cmp(dht::ring_position_view::for_range_start(cr), split_point) >= 0) {\n            continue; // avoid empty splits\n        }\n\n        std::pair<dht::partition_range, dht::partition_range> splits =\n                cr.split(split_point, cmp);\n\n        add_range(std::move(splits.first));\n        cr = std::move(splits.second);\n        if (ranges.size() == n) {\n            // we have enough ranges\n            break;\n        }\n    }\n\n    if (ranges.size() < n) {\n        add_range(get_remainder());\n    }\n}\n"
        },
        {
          "name": "query_ranges_to_vnodes.hh",
          "type": "blob",
          "size": 1.173828125,
          "content": "/*\n * Copyright (C) 2021-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"dht/i_partitioner_fwd.hh\"\n#include \"locator/token_range_splitter.hh\"\n\nclass query_ranges_to_vnodes_generator {\n    schema_ptr _s;\n    dht::partition_range_vector _ranges;\n    dht::partition_range_vector::iterator _i; // iterator to current range in _ranges\n    bool _local;\n    std::unique_ptr<locator::token_range_splitter> _splitter;\n    void process_one_range(size_t n, dht::partition_range_vector& ranges);\npublic:\n    query_ranges_to_vnodes_generator(std::unique_ptr<locator::token_range_splitter> splitter, schema_ptr s, dht::partition_range_vector ranges, bool local = false);\n    query_ranges_to_vnodes_generator(const query_ranges_to_vnodes_generator&) = delete;\n    query_ranges_to_vnodes_generator(query_ranges_to_vnodes_generator&&) = default;\n    // generate next 'n' vnodes, may return less than requested number of ranges\n    // which means either that there are no more ranges\n    // (in which case empty() == true), or too many ranges\n    // are requested\n    dht::partition_range_vector operator()(size_t n);\n    bool empty() const;\n};\n"
        },
        {
          "name": "query_result_merger.hh",
          "type": "blob",
          "size": 1.19140625,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <seastar/core/distributed.hh>\n#include \"query-result.hh\"\n\nnamespace query {\n\n// Merges non-overlapping results into one\n// Implements @Reducer concept from distributed.hh\nclass result_merger {\n    std::vector<foreign_ptr<lw_shared_ptr<query::result>>> _partial;\n    const uint64_t _max_rows;\n    const uint32_t _max_partitions;\npublic:\n    explicit result_merger(uint64_t max_rows, uint32_t max_partitions)\n            : _max_rows(max_rows)\n            , _max_partitions(max_partitions)\n    { }\n\n    void reserve(size_t size) {\n        _partial.reserve(size);\n    }\n\n    void operator()(foreign_ptr<lw_shared_ptr<query::result>> r) {\n        if (!_partial.empty() && _partial.back()->is_short_read()) {\n            return;\n        }\n        _partial.emplace_back(std::move(r));\n    }\n\n    // FIXME: Eventually we should return a composite_query_result here\n    // which holds the vector of query results and which can be quickly turned\n    // into packet fragments by the transport layer without copying the data.\n    foreign_ptr<lw_shared_ptr<query::result>> get();\n};\n\n}\n"
        },
        {
          "name": "raft",
          "type": "tree",
          "content": null
        },
        {
          "name": "read_context.hh",
          "type": "blob",
          "size": 8.953125,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"utils/assert.hh\"\n#include \"schema/schema_fwd.hh\"\n#include \"query-request.hh\"\n#include \"mutation/mutation_fragment.hh\"\n#include \"mutation/partition_version.hh\"\n#include \"tracing/tracing.hh\"\n#include \"row_cache.hh\"\n\nnamespace cache {\n\n/*\n* Represent a flat reader to the underlying source.\n* This reader automatically makes sure that it's up to date with all cache updates\n*/\nclass autoupdating_underlying_reader final {\n    row_cache& _cache;\n    read_context& _read_context;\n    mutation_reader_opt _reader;\n    utils::phased_barrier::phase_type _reader_creation_phase = 0;\n    dht::partition_range _range = { };\n    std::optional<dht::decorated_key> _last_key;\n    std::optional<dht::decorated_key> _new_last_key;\n\n    future<> close_reader() noexcept {\n        return _reader ? _reader->close() : make_ready_future<>();\n    }\npublic:\n    autoupdating_underlying_reader(row_cache& cache, read_context& context)\n        : _cache(cache)\n        , _read_context(context)\n    { }\n    future<mutation_fragment_v2_opt> move_to_next_partition() {\n        _last_key = std::move(_new_last_key);\n        auto start = population_range_start();\n        auto phase = _cache.phase_of(start);\n        if (!_reader || _reader_creation_phase != phase) {\n            if (_last_key) {\n                auto cmp = dht::ring_position_comparator(*_cache._schema);\n                auto&& new_range = _range.split_after(*_last_key, cmp);\n                if (!new_range) {\n                    co_await close_reader();\n                    co_return std::nullopt;\n                }\n                _range = std::move(*new_range);\n                _last_key = {};\n            }\n            if (_reader) {\n                ++_cache._tracker._stats.underlying_recreations;\n            }\n            auto old_reader = std::move(*_reader);\n            std::exception_ptr ex;\n            try {\n                _reader = _cache.create_underlying_reader(_read_context, _cache.snapshot_for_phase(phase), _range);\n                _reader_creation_phase = phase;\n            } catch(...) {\n                  ex = std::current_exception();\n            }\n            co_await old_reader.close();\n            maybe_rethrow_exception(std::move(ex));\n        }\n        co_await _reader->next_partition();\n        if (_reader->is_end_of_stream() && _reader->is_buffer_empty()) {\n            co_return std::nullopt;\n        }\n        auto mfopt = co_await (*_reader)();\n        if (mfopt) {\n            SCYLLA_ASSERT(mfopt->is_partition_start());\n            _new_last_key = mfopt->as_partition_start().key();\n        }\n        co_return std::move(mfopt);\n    }\n    future<> fast_forward_to(dht::partition_range&& range) {\n        auto snapshot_and_phase = _cache.snapshot_of(dht::ring_position_view::for_range_start(_range));\n        return fast_forward_to(std::move(range), snapshot_and_phase.snapshot, snapshot_and_phase.phase);\n    }\n    future<> fast_forward_to(dht::partition_range&& range, mutation_source& snapshot, row_cache::phase_type phase) {\n        _range = std::move(range);\n        _last_key = { };\n        _new_last_key = { };\n        if (_reader) {\n            if (_reader_creation_phase == phase) {\n                ++_cache._tracker._stats.underlying_partition_skips;\n                return _reader->fast_forward_to(_range);\n            } else {\n                ++_cache._tracker._stats.underlying_recreations;\n            }\n        }\n        return close_reader().then([this, snapshot, phase] () mutable {\n            _reader = _cache.create_underlying_reader(_read_context, snapshot, _range);\n            _reader_creation_phase = phase;\n        });\n    }\n    future<> close() noexcept {\n        return close_reader();\n    }\n    utils::phased_barrier::phase_type creation_phase() const {\n        return _reader_creation_phase;\n    }\n    const dht::partition_range& range() const {\n        return _range;\n    }\n    mutation_reader& underlying() { return *_reader; }\n    dht::ring_position_view population_range_start() const {\n        return _last_key ? dht::ring_position_view::for_after_key(*_last_key)\n                         : dht::ring_position_view::for_range_start(_range);\n    }\n};\n\nclass read_context final : public enable_lw_shared_from_this<read_context> {\n    row_cache& _cache;\n    schema_ptr _schema;\n    reader_permit _permit;\n    const dht::partition_range& _range;\n    const query::partition_slice& _slice;\n    tracing::trace_state_ptr _trace_state;\n    mutation_reader::forwarding _fwd_mr;\n    bool _range_query;\n    const tombstone_gc_state* _tombstone_gc_state;\n    // When reader enters a partition, it must be set up for reading that\n    // partition from the underlying mutation source (_underlying) in one of two ways:\n    //\n    //  1) either _underlying is already in that partition\n    //\n    //  2) _underlying is before the partition, then _underlying_snapshot and _key\n    //     are set so that _underlying_flat can be fast forwarded to the right partition.\n    //\n    autoupdating_underlying_reader _underlying;\n    uint64_t _underlying_created = 0;\n\n    mutation_source_opt _underlying_snapshot;\n    dht::partition_range _sm_range;\n    std::optional<dht::decorated_key> _key;\n    bool _partition_exists;\n    row_cache::phase_type _phase;\npublic:\n    read_context(row_cache& cache,\n            schema_ptr schema,\n            reader_permit permit,\n            const dht::partition_range& range,\n            const query::partition_slice& slice,\n            const tombstone_gc_state* gc_state,\n            tracing::trace_state_ptr trace_state,\n            mutation_reader::forwarding fwd_mr)\n        : _cache(cache)\n        , _schema(std::move(schema))\n        , _permit(std::move(permit))\n        , _range(range)\n        , _slice(slice)\n        , _trace_state(std::move(trace_state))\n        , _fwd_mr(fwd_mr)\n        , _range_query(!query::is_single_partition(range))\n        , _tombstone_gc_state(gc_state)\n        , _underlying(_cache, *this)\n    {\n        ++_cache._tracker._stats.reads;\n        if (!_range_query) {\n            _key = range.start()->value().as_decorated_key();\n        }\n    }\n    ~read_context() {\n        ++_cache._tracker._stats.reads_done;\n        if (_underlying_created) {\n            _cache._stats.reads_with_misses.mark();\n            ++_cache._tracker._stats.reads_with_misses;\n        } else {\n            _cache._stats.reads_with_no_misses.mark();\n        }\n    }\n    read_context(const read_context&) = delete;\n    row_cache& cache() { return _cache; }\n    const schema_ptr& schema() const { return _schema; }\n    reader_permit permit() const { return _permit; }\n    const dht::partition_range& range() const { return _range; }\n    const query::partition_slice& slice() const { return _slice; }\n    bool is_reversed() const { return _slice.is_reversed(); }\n    // Returns a slice in the native format (for reversed reads, in native-reversed format).\n    const query::partition_slice& native_slice() const { return _slice; }\n    tracing::trace_state_ptr trace_state() const { return _trace_state; }\n    mutation_reader::forwarding fwd_mr() const { return _fwd_mr; }\n    bool is_range_query() const { return _range_query; }\n    autoupdating_underlying_reader& underlying() { return _underlying; }\n    row_cache::phase_type phase() const { return _phase; }\n    const dht::decorated_key& key() const { return *_key; }\n    bool partition_exists() const { return _partition_exists; }\n    void on_underlying_created() { ++_underlying_created; }\n    bool digest_requested() const { return _slice.options.contains<query::partition_slice::option::with_digest>(); }\n    const tombstone_gc_state* tombstone_gc_state() const { return _tombstone_gc_state; }\npublic:\n    future<> ensure_underlying() {\n        if (_underlying_snapshot) {\n            return create_underlying().then([this] {\n                return _underlying.underlying()().then([this] (mutation_fragment_v2_opt&& mfopt) {\n                    _partition_exists = bool(mfopt);\n                });\n            });\n        }\n        // We know that partition exists because all the callers of\n        // enter_partition(const dht::decorated_key&, row_cache::phase_type)\n        // check that and there's no other way of setting _underlying_snapshot\n        // to empty. Except for calling create_underlying.\n        _partition_exists = true;\n        return make_ready_future<>();\n    }\npublic:\n    future<> create_underlying();\n    void enter_partition(const dht::decorated_key& dk, mutation_source& snapshot, row_cache::phase_type phase) {\n        _phase = phase;\n        _underlying_snapshot = snapshot;\n        _key = dk;\n    }\n    // Precondition: each caller needs to make sure that partition with |dk| key\n    //               exists in underlying before calling this function.\n    void enter_partition(const dht::decorated_key& dk, row_cache::phase_type phase) {\n        _phase = phase;\n        _underlying_snapshot = {};\n        _key = dk;\n    }\n    future<> close() noexcept {\n        return _underlying.close();\n    }\n};\n\n}\n"
        },
        {
          "name": "reader_concurrency_semaphore.cc",
          "type": "blob",
          "size": 64.275390625,
          "content": "/*\n * Copyright (C) 2018-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include <seastar/core/seastar.hh>\n#include <seastar/core/format.hh>\n#include <seastar/core/file.hh>\n#include <seastar/util/lazy.hh>\n#include <seastar/util/log.hh>\n#include <seastar/core/coroutine.hh>\n#include <seastar/coroutine/maybe_yield.hh>\n#include <seastar/core/metrics.hh>\n#include <utility>\n\n#include \"reader_concurrency_semaphore.hh\"\n#include \"query-result.hh\"\n#include \"readers/mutation_reader.hh\"\n#include \"utils/assert.hh\"\n#include \"utils/exceptions.hh\"\n#include \"schema/schema.hh\"\n#include \"utils/human_readable.hh\"\n#include \"utils/memory_limit_reached.hh\"\n\n#include <boost/range/algorithm/for_each.hpp>\n\nlogger rcslog(\"reader_concurrency_semaphore\");\n\nstruct reader_concurrency_semaphore::inactive_read {\n    mutation_reader reader;\n    const dht::partition_range* range = nullptr;\n    eviction_notify_handler notify_handler;\n    timer<lowres_clock> ttl_timer;\n    inactive_read_handle* handle = nullptr;\n\n    explicit inactive_read(mutation_reader reader_, const dht::partition_range* range_) noexcept\n        : reader(std::move(reader_))\n        , range(range_)\n    { }\n    inactive_read(inactive_read&& o)\n        : reader(std::move(o.reader))\n        , range(o.range)\n        , notify_handler(std::move(o.notify_handler))\n        , ttl_timer(std::move(o.ttl_timer))\n        , handle(o.handle)\n    {\n        o.handle = nullptr;\n    }\n    ~inactive_read() {\n        detach();\n    }\n    void detach() noexcept {\n        if (handle) {\n            handle->_permit = {};\n            handle = nullptr;\n        }\n    }\n};\n\ntemplate <>\nstruct fmt::formatter<reader_concurrency_semaphore::evict_reason> : fmt::formatter<string_view> {\n    template <typename FormatContext>\n    auto format(const reader_concurrency_semaphore::evict_reason& reason, FormatContext& ctx) const {\n        static const char* value_table[] = {\"permit\", \"time\", \"manual\"};\n        return fmt::format_to(ctx.out(), \"{}\", value_table[static_cast<int>(reason)]);\n    }\n};\n\nnamespace {\n\nvoid maybe_dump_reader_permit_diagnostics(const reader_concurrency_semaphore& semaphore, std::string_view problem, reader_permit::impl* permit) noexcept;\n\n}\n\nauto fmt::formatter<reader_resources>::format(const reader_resources& r, fmt::format_context& ctx) const\n        -> decltype(ctx.out()) {\n    return fmt::format_to(ctx.out(), \"{{{}, {}}}\", r.count, r.memory);\n}\n\nreader_permit::resource_units::resource_units(reader_permit permit, reader_resources res, already_consumed_tag)\n    : _permit(std::move(permit)), _resources(res) {\n}\n\nreader_permit::resource_units::resource_units(reader_permit permit, reader_resources res)\n    : _permit(std::move(permit)) {\n    _permit.consume(res);\n    _resources = res;\n}\n\nreader_permit::resource_units::resource_units(resource_units&& o) noexcept\n    : _permit(std::move(o._permit))\n    , _resources(std::exchange(o._resources, {})) {\n}\n\nreader_permit::resource_units::~resource_units() {\n    reset_to_zero();\n}\n\nreader_permit::resource_units& reader_permit::resource_units::operator=(resource_units&& o) noexcept {\n    if (&o == this) {\n        return *this;\n    }\n    reset_to_zero();\n    _permit = std::move(o._permit);\n    _resources = std::exchange(o._resources, {});\n    return *this;\n}\n\nvoid reader_permit::resource_units::add(resource_units&& o) {\n    SCYLLA_ASSERT(_permit == o._permit);\n    _resources += std::exchange(o._resources, {});\n}\n\nvoid reader_permit::resource_units::reset_to(reader_resources res) {\n    if (res == _resources) {\n        return;\n    }\n    if (res.count < _resources.count && res.memory < _resources.memory) {\n        _permit.signal(reader_resources{_resources.count - res.count, _resources.memory - res.memory});\n        _resources = res;\n        return;\n    }\n\n    if (res.non_zero()) {\n        _permit.consume(res);\n    }\n    if (_resources.non_zero()) {\n        _permit.signal(_resources);\n    }\n    _resources = res;\n}\n\nvoid reader_permit::resource_units::reset_to_zero() noexcept {\n    if (_resources.non_zero()) {\n        _permit.signal(_resources);\n        _resources = {};\n    }\n}\n\nclass reader_permit::impl\n        : public boost::intrusive::list_base_hook<boost::intrusive::link_mode<boost::intrusive::auto_unlink>>\n        , public enable_shared_from_this<reader_permit::impl> {\npublic:\n    struct auxiliary_data {\n        promise<> pr;\n        std::optional<shared_future<>> fut;\n        reader_concurrency_semaphore::read_func func;\n        // Self reference to keep the permit alive while queued for execution.\n        // Must be cleared on all code-paths, otherwise it will keep the permit alive in perpetuity.\n        reader_permit_opt permit_keepalive;\n        std::optional<reader_concurrency_semaphore::inactive_read> ir;\n    };\n\nprivate:\n    reader_concurrency_semaphore& _semaphore;\n    schema_ptr _schema;\n\n    sstring _op_name;\n    std::string_view _op_name_view;\n    reader_resources _base_resources;\n    bool _base_resources_consumed = false;\n    reader_resources _resources;\n    reader_permit::state _state = reader_permit::state::active;\n    uint64_t _need_cpu_branches = 0;\n    bool _marked_as_need_cpu = false;\n    uint64_t _awaits_branches = 0;\n    bool _marked_as_awaits = false;\n    std::exception_ptr _ex; // exception the permit was aborted with, nullptr if not aborted\n    timer<db::timeout_clock> _ttl_timer;\n    query::max_result_size _max_result_size{query::result_memory_limiter::unlimited_result_size};\n    uint64_t _sstables_read = 0;\n    size_t _requested_memory = 0;\n    uint64_t _oom_kills = 0;\n    tracing::trace_state_ptr _trace_ptr;\n\n    // Not strictly related to the permit.\n    // Used by the semaphore to to manage the permit.\n    auxiliary_data _aux_data;\n\nprivate:\n    void on_permit_need_cpu() {\n        _semaphore.on_permit_need_cpu();\n        _marked_as_need_cpu = true;\n    }\n    void on_permit_not_need_cpu() {\n        _semaphore.on_permit_not_need_cpu();\n        _marked_as_need_cpu = false;\n    }\n    void on_permit_awaits() {\n        _semaphore.on_permit_awaits();\n        _marked_as_awaits = true;\n    }\n    void on_permit_not_awaits() {\n        _semaphore.on_permit_not_awaits();\n        _marked_as_awaits = false;\n    }\n    void on_permit_active() {\n        if (_need_cpu_branches) {\n            _state = reader_permit::state::active_need_cpu;\n            on_permit_need_cpu();\n            if (_awaits_branches) {\n                _state = reader_permit::state::active_await;\n                on_permit_awaits();\n            }\n        } else {\n            _state = reader_permit::state::active;\n        }\n    }\n\n    void on_permit_inactive(reader_permit::state st) {\n        // If the permit is registered as inactive, while waiting for memory,\n        // clear the memory amount, the requests are failed anyway.\n        if (_state == reader_permit::state::waiting_for_memory) {\n            _requested_memory = {};\n        }\n        _state = st;\n        if (_marked_as_awaits) {\n            on_permit_not_awaits();\n        }\n        if (_marked_as_need_cpu) {\n            on_permit_not_need_cpu();\n        }\n    }\n\n    void on_timeout() {\n        auto keepalive = std::exchange(_aux_data.permit_keepalive, std::nullopt);\n\n        auto ex = named_semaphore_timed_out(_semaphore._name);\n        _ex = std::make_exception_ptr(ex);\n\n        switch (_state) {\n            case state::waiting_for_admission:\n            case state::waiting_for_memory:\n            case state::waiting_for_execution:\n                _aux_data.pr.set_exception(ex);\n                maybe_dump_reader_permit_diagnostics(_semaphore, \"timed out\", this);\n                _semaphore.dequeue_permit(*this);\n                break;\n            case state::active:\n            case state::active_need_cpu:\n            case state::active_await:\n                maybe_dump_reader_permit_diagnostics(_semaphore, \"timed out\", this);\n                break;\n            case state::inactive:\n                _semaphore.evict(*this, reader_concurrency_semaphore::evict_reason::time);\n                break;\n            case state::evicted:\n                break;\n        }\n    }\n\npublic:\n    struct value_tag {};\n\n    impl(reader_concurrency_semaphore& semaphore, schema_ptr schema, const std::string_view& op_name, reader_resources base_resources, db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr)\n        : _semaphore(semaphore)\n        , _schema(std::move(schema))\n        , _op_name_view(op_name)\n        , _base_resources(base_resources)\n        , _ttl_timer([this] { on_timeout(); })\n        , _trace_ptr(std::move(trace_ptr))\n    {\n        set_timeout(timeout);\n        _semaphore.on_permit_created(*this);\n    }\n    impl(reader_concurrency_semaphore& semaphore, schema_ptr schema, sstring&& op_name, reader_resources base_resources, db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr)\n        : _semaphore(semaphore)\n        , _schema(std::move(schema))\n        , _op_name(std::move(op_name))\n        , _op_name_view(_op_name)\n        , _base_resources(base_resources)\n        , _ttl_timer([this] { on_timeout(); })\n        , _trace_ptr(std::move(trace_ptr))\n    {\n        set_timeout(timeout);\n        _semaphore.on_permit_created(*this);\n    }\n    ~impl() {\n        if (_base_resources_consumed) {\n            signal(_base_resources);\n        }\n\n        if (_resources.non_zero()) {\n            on_internal_error_noexcept(rcslog, format(\"reader_permit::impl::~impl(): permit {} detected a leak of {{count={}, memory={}}} resources\",\n                        description(),\n                        _resources.count,\n                        _resources.memory));\n            signal(_resources);\n        }\n\n        if (_need_cpu_branches) {\n            on_internal_error_noexcept(rcslog, seastar::format(\"reader_permit::impl::~impl(): permit {}.{}:{} destroyed with {} need_cpu branches\",\n                        _schema ? _schema->ks_name() : \"*\",\n                        _schema ? _schema->cf_name() : \"*\",\n                        _op_name_view,\n                        _need_cpu_branches));\n            _semaphore.on_permit_not_need_cpu();\n        }\n\n        if (_awaits_branches) {\n            on_internal_error_noexcept(rcslog, seastar::format(\"reader_permit::impl::~impl(): permit {}.{}:{} destroyed with {} awaits branches\",\n                        _schema ? _schema->ks_name() : \"*\",\n                        _schema ? _schema->cf_name() : \"*\",\n                        _op_name_view,\n                        _awaits_branches));\n            _semaphore.on_permit_not_awaits();\n        }\n\n        // Should probably make a scene here, but its not worth it.\n        _semaphore._stats.sstables_read -= _sstables_read;\n        _semaphore._stats.disk_reads -= bool(_sstables_read);\n\n        _semaphore.on_permit_destroyed(*this);\n    }\n\n    reader_concurrency_semaphore& semaphore() {\n        return _semaphore;\n    }\n\n    const schema_ptr& get_schema() const {\n        return _schema;\n    }\n\n    std::string_view get_op_name() const {\n        return _op_name_view;\n    }\n\n    reader_permit::state get_state() const {\n        return _state;\n    }\n\n    auxiliary_data& aux_data() {\n        return _aux_data;\n    }\n\n    void on_waiting_for_admission() {\n        on_permit_inactive(reader_permit::state::waiting_for_admission);\n    }\n\n    void on_waiting_for_memory() {\n        on_permit_inactive(reader_permit::state::waiting_for_memory);\n    }\n\n    void on_waiting_for_execution() {\n        on_permit_inactive(reader_permit::state::waiting_for_execution);\n    }\n\n    void on_admission() {\n        SCYLLA_ASSERT(_state != reader_permit::state::active_await);\n        on_permit_active();\n        consume(_base_resources);\n        _base_resources_consumed = true;\n    }\n\n    void on_granted_memory() {\n        if (_state == reader_permit::state::waiting_for_memory) {\n            on_permit_active();\n        }\n        consume({0, std::exchange(_requested_memory, 0)});\n    }\n\n    void on_executing() {\n        on_permit_active();\n    }\n\n    void on_register_as_inactive() {\n        SCYLLA_ASSERT(_state == reader_permit::state::active || _state == reader_permit::state::active_need_cpu || _state == reader_permit::state::waiting_for_memory);\n        on_permit_inactive(reader_permit::state::inactive);\n    }\n\n    void on_unregister_as_inactive() {\n        SCYLLA_ASSERT(_state == reader_permit::state::inactive);\n        on_permit_active();\n    }\n\n    void on_evicted() {\n        SCYLLA_ASSERT(_state == reader_permit::state::inactive);\n        _state = reader_permit::state::evicted;\n        if (_base_resources_consumed) {\n            signal(_base_resources);\n            _base_resources_consumed = false;\n        }\n    }\n\n    void consume(reader_resources res) {\n        _semaphore.consume(*this, res);\n        _resources += res;\n    }\n\n    void signal(reader_resources res) {\n        _resources -= res;\n        _semaphore.signal(res);\n    }\n\n    future<resource_units> request_memory(size_t memory) {\n        _requested_memory += memory;\n        return _semaphore.request_memory(*this, memory).then([this, memory] {\n            return resource_units(reader_permit(shared_from_this()), {0, ssize_t(memory)}, resource_units::already_consumed_tag{});\n        });\n    }\n\n    reader_resources resources() const {\n        return _resources;\n    }\n\n    reader_resources base_resources() const {\n        return _base_resources;\n    }\n\n    void release_base_resources() noexcept {\n        if (_base_resources_consumed) {\n            _resources -= _base_resources;\n            _base_resources_consumed = false;\n        }\n        _semaphore.signal(std::exchange(_base_resources, {}));\n    }\n\n    sstring description() const {\n        return seastar::format(\"{}.{}:{}\",\n                _schema ? _schema->ks_name() : \"*\",\n                _schema ? _schema->cf_name() : \"*\",\n                _op_name_view);\n    }\n\n    void mark_need_cpu() noexcept {\n        ++_need_cpu_branches;\n        if (!_marked_as_need_cpu && _state == reader_permit::state::active) {\n            _state = reader_permit::state::active_need_cpu;\n            on_permit_need_cpu();\n            if (_awaits_branches && !_marked_as_awaits) {\n                _state = reader_permit::state::active_await;\n                on_permit_awaits();\n            }\n        }\n    }\n\n    void mark_not_need_cpu() noexcept {\n        SCYLLA_ASSERT(_need_cpu_branches);\n        --_need_cpu_branches;\n        if (_marked_as_need_cpu && !_need_cpu_branches) {\n            // When an exception is thrown, need_cpu and awaits guards might be\n            // destroyed out-of-order. Force the state out of awaits state here\n            // so that we maintain awaits >= need_cpu.\n            if (_marked_as_awaits) {\n                on_permit_not_awaits();\n            }\n            _state = reader_permit::state::active;\n            on_permit_not_need_cpu();\n        }\n    }\n\n    void mark_awaits() noexcept {\n        ++_awaits_branches;\n        if (_awaits_branches == 1 && _state == reader_permit::state::active_need_cpu) {\n            _state = reader_permit::state::active_await;\n            on_permit_awaits();\n        }\n    }\n\n    void mark_not_awaits() noexcept {\n        SCYLLA_ASSERT(_awaits_branches);\n        --_awaits_branches;\n        if (_marked_as_awaits && !_awaits_branches) {\n            _state = reader_permit::state::active_need_cpu;\n            on_permit_not_awaits();\n        }\n    }\n\n    bool needs_readmission() const {\n        return _state == reader_permit::state::evicted;\n    }\n\n    future<> wait_readmission() {\n        return _semaphore.do_wait_admission(*this);\n    }\n\n    db::timeout_clock::time_point timeout() const noexcept {\n        return _ttl_timer.armed() ? _ttl_timer.get_timeout() : db::no_timeout;\n    }\n\n    void set_timeout(db::timeout_clock::time_point timeout) noexcept {\n        _ttl_timer.cancel();\n        if (timeout != db::no_timeout) {\n            _ttl_timer.arm(timeout);\n        }\n    }\n\n    const tracing::trace_state_ptr& trace_state() const noexcept {\n        return _trace_ptr;\n    }\n\n    void set_trace_state(tracing::trace_state_ptr trace_ptr) noexcept {\n        if (_trace_ptr) {\n            // Create a continuation trace point\n            tracing::trace(trace_ptr, \"Continuing paged query, previous page's trace session is {}\", _trace_ptr->session_id());\n        }\n        _trace_ptr = std::move(trace_ptr);\n    }\n\n    void check_abort() {\n        if (_ex) {\n            std::rethrow_exception(_ex);\n        }\n    }\n\n    query::max_result_size max_result_size() const {\n        return _max_result_size;\n    }\n\n    void set_max_result_size(query::max_result_size s) {\n        _max_result_size = std::move(s);\n    }\n\n    void on_start_sstable_read() noexcept {\n        if (!_sstables_read) {\n            ++_semaphore._stats.disk_reads;\n        }\n        ++_sstables_read;\n        ++_semaphore._stats.sstables_read;\n    }\n\n    void on_finish_sstable_read() noexcept {\n        --_sstables_read;\n        --_semaphore._stats.sstables_read;\n        if (!_sstables_read) {\n            --_semaphore._stats.disk_reads;\n        }\n    }\n\n    bool on_oom_kill() noexcept {\n        return !bool(_oom_kills++);\n    }\n};\n\nstatic_assert(std::is_nothrow_copy_constructible_v<reader_permit>);\nstatic_assert(std::is_nothrow_move_constructible_v<reader_permit>);\n\nreader_permit::reader_permit(shared_ptr<impl> impl) : _impl(std::move(impl))\n{\n}\n\nreader_permit::reader_permit(reader_concurrency_semaphore& semaphore, schema_ptr schema, std::string_view op_name,\n        reader_resources base_resources, db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr)\n    : _impl(::seastar::make_shared<reader_permit::impl>(semaphore, std::move(schema), op_name, base_resources, timeout, std::move(trace_ptr)))\n{\n}\n\nreader_permit::reader_permit(reader_concurrency_semaphore& semaphore, schema_ptr schema, sstring&& op_name,\n        reader_resources base_resources, db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr)\n    : _impl(::seastar::make_shared<reader_permit::impl>(semaphore, std::move(schema), std::move(op_name), base_resources, timeout, std::move(trace_ptr)))\n{\n}\n\nreader_permit::~reader_permit() {\n}\n\nreader_concurrency_semaphore& reader_permit::semaphore() {\n    return _impl->semaphore();\n}\n\nconst schema_ptr& reader_permit::get_schema() const {\n    return _impl->get_schema();\n}\n\nstd::string_view reader_permit::get_op_name() const {\n    return _impl->get_op_name();\n}\n\nreader_permit::state reader_permit::get_state() const {\n    return _impl->get_state();\n}\n\nbool reader_permit::needs_readmission() const {\n    return _impl->needs_readmission();\n}\n\nfuture<> reader_permit::wait_readmission() {\n    return _impl->wait_readmission();\n}\n\nvoid reader_permit::consume(reader_resources res) {\n    _impl->consume(res);\n}\n\nvoid reader_permit::signal(reader_resources res) {\n    _impl->signal(res);\n}\n\nreader_permit::resource_units reader_permit::consume_memory(size_t memory) {\n    return consume_resources(reader_resources{0, ssize_t(memory)});\n}\n\nreader_permit::resource_units reader_permit::consume_resources(reader_resources res) {\n    return resource_units(*this, res);\n}\n\nfuture<reader_permit::resource_units> reader_permit::request_memory(size_t memory) {\n    return _impl->request_memory(memory);\n}\n\nreader_resources reader_permit::consumed_resources() const {\n    return _impl->resources();\n}\n\nreader_resources reader_permit::base_resources() const {\n    return _impl->base_resources();\n}\n\nvoid reader_permit::release_base_resources() noexcept {\n    return _impl->release_base_resources();\n}\n\nsstring reader_permit::description() const {\n    return _impl->description();\n}\n\nvoid reader_permit::mark_need_cpu() noexcept {\n    _impl->mark_need_cpu();\n}\n\nvoid reader_permit::mark_not_need_cpu() noexcept {\n    _impl->mark_not_need_cpu();\n}\n\nvoid reader_permit::mark_awaits() noexcept {\n    _impl->mark_awaits();\n}\n\nvoid reader_permit::mark_not_awaits() noexcept {\n    _impl->mark_not_awaits();\n}\n\ndb::timeout_clock::time_point reader_permit::timeout() const noexcept {\n    return _impl->timeout();\n}\n\nvoid reader_permit::set_timeout(db::timeout_clock::time_point timeout) noexcept {\n    _impl->set_timeout(timeout);\n}\n\nconst tracing::trace_state_ptr& reader_permit::trace_state() const noexcept {\n    return _impl->trace_state();\n}\n\nvoid reader_permit::set_trace_state(tracing::trace_state_ptr trace_ptr) noexcept {\n    _impl->set_trace_state(std::move(trace_ptr));\n}\n\nvoid reader_permit::check_abort() {\n    return _impl->check_abort();\n}\n\nquery::max_result_size reader_permit::max_result_size() const {\n    return _impl->max_result_size();\n}\n\nvoid reader_permit::set_max_result_size(query::max_result_size s) {\n    _impl->set_max_result_size(std::move(s));\n}\n\nvoid reader_permit::on_start_sstable_read() noexcept {\n    _impl->on_start_sstable_read();\n}\n\nvoid reader_permit::on_finish_sstable_read() noexcept {\n    _impl->on_finish_sstable_read();\n}\n\nauto fmt::formatter<reader_permit::state>::format(reader_permit::state s, fmt::format_context& ctx) const\n        -> decltype(ctx.out()) {\n    std::string_view name;\n    switch (s) {\n        case reader_permit::state::waiting_for_admission:\n            name = \"waiting_for_admission\";\n            break;\n        case reader_permit::state::waiting_for_memory:\n            name = \"waiting_for_memory\";\n            break;\n        case reader_permit::state::waiting_for_execution:\n            name = \"waiting_for_execution\";\n            break;\n        case reader_permit::state::active:\n            name = \"active\";\n            break;\n        case reader_permit::state::active_need_cpu:\n            name = \"active/need_cpu\";\n            break;\n        case reader_permit::state::active_await:\n            name = \"active/await\";\n            break;\n        case reader_permit::state::inactive:\n            name= \"inactive\";\n            break;\n        case reader_permit::state::evicted:\n            name = \"evicted\";\n            break;\n    }\n    return formatter<string_view>::format(name, ctx);\n}\n\nnamespace {\n\nstruct permit_stats {\n    uint64_t permits = 0;\n    reader_resources resources;\n\n    void add(const reader_permit::impl& permit) {\n        ++permits;\n        resources += permit.resources();\n    }\n\n    permit_stats& operator+=(const permit_stats& o) {\n        permits += o.permits;\n        resources += o.resources;\n        return *this;\n    }\n};\n\nusing permit_group_key = std::tuple<const schema*, std::string_view, reader_permit::state>;\n\nstruct permit_group_key_hash {\n    size_t operator()(const permit_group_key& k) const {\n        using underlying_type = std::underlying_type_t<reader_permit::state>;\n        return std::hash<uintptr_t>()(reinterpret_cast<uintptr_t>(std::get<0>(k)))\n            ^ std::hash<std::string_view>()(std::get<1>(k))\n            ^ std::hash<underlying_type>()(static_cast<underlying_type>(std::get<2>(k)));\n    }\n};\n\nusing permit_groups = std::unordered_map<permit_group_key, permit_stats, permit_group_key_hash>;\n\nstatic permit_stats do_dump_reader_permit_diagnostics(std::ostream& os, const permit_groups& permits, unsigned max_lines = 20) {\n    struct permit_summary {\n        const schema* s;\n        std::string_view op_name;\n        reader_permit::state state;\n        uint64_t permits;\n        reader_resources resources;\n    };\n\n    std::vector<permit_summary> permit_summaries;\n    for (const auto& [k, v] : permits) {\n        const auto& [s, op_name, k_state] = k;\n        permit_summaries.emplace_back(permit_summary{s, op_name, k_state, v.permits, v.resources});\n    }\n\n    std::ranges::sort(permit_summaries, [] (const permit_summary& a, const permit_summary& b) {\n        return a.resources.memory > b.resources.memory;\n    });\n\n    permit_stats total;\n    unsigned lines = 0;\n    permit_stats omitted_permit_stats;\n\n    auto print_line = [&os] (auto col1, auto col2, auto col3, auto col4) {\n        fmt::print(os, \"{}\\t{}\\t{}\\t{}\\n\", col1, col2, col3, col4);\n    };\n\n    print_line(\"permits\", \"count\", \"memory\", \"table/operation/state\");\n    for (const auto& summary : permit_summaries) {\n        total.permits += summary.permits;\n        total.resources += summary.resources;\n        if (!max_lines || lines++ < max_lines) {\n            print_line(summary.permits, summary.resources.count, utils::to_hr_size(summary.resources.memory), fmt::format(\"{}.{}/{}/{}\",\n                        summary.s ? summary.s->ks_name() : \"*\",\n                        summary.s ? summary.s->cf_name() : \"*\",\n                        summary.op_name,\n                        summary.state));\n        } else {\n            omitted_permit_stats.permits += summary.permits;\n            omitted_permit_stats.resources += summary.resources;\n        }\n    }\n    if (max_lines && lines > max_lines) {\n        print_line(omitted_permit_stats.permits, omitted_permit_stats.resources.count, utils::to_hr_size(omitted_permit_stats.resources.memory), \"permits omitted for brevity\");\n    }\n    fmt::print(os, \"\\n\");\n    print_line(total.permits, total.resources.count, utils::to_hr_size(total.resources.memory), \"total\");\n    return total;\n}\n\nstatic void do_dump_reader_permit_diagnostics(std::ostream& os, const reader_concurrency_semaphore& semaphore, std::string_view problem,\n        reader_permit::impl* permit, unsigned max_lines = 20) {\n    permit_groups permits;\n\n    semaphore.foreach_permit([&] (const reader_permit::impl& permit) {\n        permits[permit_group_key(permit.get_schema().get(), permit.get_op_name(), permit.get_state())].add(permit);\n    });\n\n    permit_stats total;\n\n    fmt::print(os, \"Semaphore {} with {}/{} count and {}/{} memory resources: {}, dumping permit diagnostics:\\n\",\n            semaphore.name(),\n            semaphore.initial_resources().count - semaphore.available_resources().count,\n            semaphore.initial_resources().count,\n            semaphore.initial_resources().memory - semaphore.available_resources().memory,\n            semaphore.initial_resources().memory,\n            problem);\n\n    if (permit) {\n        const auto& schema = permit->get_schema();\n        fmt::print(os, \"Trigger permit: count={}, memory={}, table={}.{}, operation={}, state={}\\n\",\n            permit->resources().count,\n            permit->resources().memory,\n            schema ? permit->get_schema()->ks_name() : \"*\",\n            schema ? permit->get_schema()->cf_name() : \"*\",\n            permit->get_op_name(),\n            permit->get_state());\n    }\n\n    std::vector<sstring> bottlenecks;\n    if (semaphore.get_stats().need_cpu_permits > 0) {\n        bottlenecks.push_back(\"CPU\");\n    }\n    if (semaphore.available_resources().memory <= 0) {\n        bottlenecks.push_back(\"memory\");\n    }\n    if (semaphore.available_resources().count <= 0) {\n        bottlenecks.push_back(\"disk\");\n    }\n\n    if (!bottlenecks.empty()) {\n        fmt::print(os, \"Identified bottleneck(s): {}\\n\", fmt::join(bottlenecks, \", \"));\n    }\n\n    fmt::print(os, \"\\n\");\n    total += do_dump_reader_permit_diagnostics(os, permits, max_lines);\n    fmt::print(os, \"\\n\");\n    const auto& stats = semaphore.get_stats();\n    fmt::print(os, \"Stats:\\n\"\n            \"permit_based_evictions: {}\\n\"\n            \"time_based_evictions: {}\\n\"\n            \"inactive_reads: {}\\n\"\n            \"total_successful_reads: {}\\n\"\n            \"total_failed_reads: {}\\n\"\n            \"total_reads_shed_due_to_overload: {}\\n\"\n            \"total_reads_killed_due_to_kill_limit: {}\\n\"\n            \"reads_admitted: {}\\n\"\n            \"reads_enqueued_for_admission: {}\\n\"\n            \"reads_enqueued_for_memory: {}\\n\"\n            \"reads_admitted_immediately: {}\\n\"\n            \"reads_queued_because_ready_list: {}\\n\"\n            \"reads_queued_because_need_cpu_permits: {}\\n\"\n            \"reads_queued_because_memory_resources: {}\\n\"\n            \"reads_queued_because_count_resources: {}\\n\"\n            \"reads_queued_with_eviction: {}\\n\"\n            \"total_permits: {}\\n\"\n            \"current_permits: {}\\n\"\n            \"need_cpu_permits: {}\\n\"\n            \"awaits_permits: {}\\n\"\n            \"disk_reads: {}\\n\"\n            \"sstables_read: {}\",\n            stats.permit_based_evictions,\n            stats.time_based_evictions,\n            stats.inactive_reads,\n            stats.total_successful_reads,\n            stats.total_failed_reads,\n            stats.total_reads_shed_due_to_overload,\n            stats.total_reads_killed_due_to_kill_limit,\n            stats.reads_admitted,\n            stats.reads_enqueued_for_admission,\n            stats.reads_enqueued_for_memory,\n            stats.reads_admitted_immediately,\n            stats.reads_queued_because_ready_list,\n            stats.reads_queued_because_need_cpu_permits,\n            stats.reads_queued_because_memory_resources,\n            stats.reads_queued_because_count_resources,\n            stats.reads_queued_with_eviction,\n            stats.total_permits,\n            stats.current_permits,\n            stats.need_cpu_permits,\n            stats.awaits_permits,\n            stats.disk_reads,\n            stats.sstables_read);\n}\n\nvoid maybe_dump_reader_permit_diagnostics(const reader_concurrency_semaphore& semaphore, std::string_view problem, reader_permit::impl* permit) noexcept {\n    static thread_local logger::rate_limit rate_limit(std::chrono::seconds(30));\n\n    rcslog.log(log_level::info, rate_limit, \"{}\", value_of([&] {\n        std::ostringstream os;\n        do_dump_reader_permit_diagnostics(os, semaphore, problem, permit);\n        return std::move(os).str();\n    }));\n}\n\n} // anonymous namespace\n\nvoid reader_concurrency_semaphore::inactive_read_handle::abandon() noexcept {\n    if (_permit) {\n        auto& permit = **_permit;\n        auto& sem = permit.semaphore();\n        sem.close_reader(std::move(permit.aux_data().ir->reader));\n        sem.dequeue_permit(permit);\n        // Break the handle <-> inactive read connection, to prevent the inactive\n        // read attempting to detach(). Not only is that unnecessary (the handle\n        // is abandoning the inactive read), but detach() will reset _permit,\n        // which might be the last permit instance alive. Destroying it could\n        // yank out *this from under our feet.\n        permit.aux_data().ir->handle = nullptr;\n        permit.aux_data().ir.reset();\n    }\n}\n\nreader_concurrency_semaphore::inactive_read_handle::inactive_read_handle(reader_permit permit) noexcept\n    : _permit(permit) {\n    (*_permit)->aux_data().ir->handle = this;\n}\n\nreader_concurrency_semaphore::inactive_read_handle::inactive_read_handle(inactive_read_handle&& o) noexcept\n    : _permit(std::exchange(o._permit, std::nullopt)) {\n    if (_permit) {\n        (*_permit)->aux_data().ir->handle = this;\n    }\n}\n\nreader_concurrency_semaphore::inactive_read_handle&\nreader_concurrency_semaphore::inactive_read_handle::operator=(inactive_read_handle&& o) noexcept {\n    if (this == &o) {\n        return *this;\n    }\n    abandon();\n    _permit = std::exchange(o._permit, std::nullopt);\n    if (_permit) {\n        (*_permit)->aux_data().ir->handle = this;\n    }\n    return *this;\n}\n\nvoid reader_concurrency_semaphore::wait_queue::push_to_admission_queue(reader_permit::impl& p) {\n    p.unlink();\n    _admission_queue.push_back(p);\n}\n\nvoid reader_concurrency_semaphore::wait_queue::push_to_memory_queue(reader_permit::impl& p) {\n    p.unlink();\n    _memory_queue.push_back(p);\n}\n\nreader_permit::impl& reader_concurrency_semaphore::wait_queue::front() {\n    if (_memory_queue.empty()) {\n        return _admission_queue.front();\n    } else {\n        return _memory_queue.front();\n    }\n}\n\nconst reader_permit::impl& reader_concurrency_semaphore::wait_queue::front() const {\n    return const_cast<wait_queue&>(*this).front();\n}\n\nnamespace {\n\nstruct stop_execution_loop {\n};\n\n}\n\nfuture<> reader_concurrency_semaphore::execution_loop() noexcept {\n    while (true) {\n        try {\n            co_await _ready_list_cv.when();\n        } catch (stop_execution_loop) {\n            co_return;\n        }\n\n        maybe_admit_waiters();\n\n        while (!_ready_list.empty()) {\n            auto& permit = _ready_list.front();\n            dequeue_permit(permit);\n            permit.on_executing();\n            auto e = std::move(permit.aux_data());\n\n            tracing::trace(permit.trace_state(), \"[reader concurrency semaphore {}] executing read\", _name);\n\n            try {\n                e.func(reader_permit(permit.shared_from_this())).forward_to(std::move(e.pr));\n            } catch (...) {\n                e.pr.set_exception(std::current_exception());\n            }\n\n            // We now possibly have >= CPU concurrency, so even if the above read\n            // didn't release any resources, just dequeueing it from the\n            // _ready_list could allow us to admit new reads.\n            maybe_admit_waiters();\n\n            if (need_preempt()) {\n                co_await coroutine::maybe_yield();\n            }\n        }\n    }\n}\n\nuint64_t reader_concurrency_semaphore::get_serialize_limit() const {\n    if (!_serialize_limit_multiplier() || _serialize_limit_multiplier() == std::numeric_limits<uint32_t>::max() || is_unlimited()) [[unlikely]] {\n        return std::numeric_limits<uint64_t>::max();\n    }\n    return _initial_resources.memory * _serialize_limit_multiplier();\n}\n\nuint64_t reader_concurrency_semaphore::get_kill_limit() const {\n    if (!_kill_limit_multiplier() || _kill_limit_multiplier() == std::numeric_limits<uint32_t>::max() || is_unlimited()) [[unlikely]] {\n        return std::numeric_limits<uint64_t>::max();\n    }\n    return _initial_resources.memory * _kill_limit_multiplier();\n}\n\nvoid reader_concurrency_semaphore::consume(reader_permit::impl& permit, resources r) {\n    // We check whether we even reached the memory limit first.\n    // This is a cheap check and should be false most of the time, providing a\n    // cheap short-circuit.\n    if (_resources.memory <= 0 && std::cmp_greater_equal(consumed_resources().memory + r.memory, get_kill_limit())) [[unlikely]] {\n        if (permit.on_oom_kill()) {\n            ++_stats.total_reads_killed_due_to_kill_limit;\n        }\n        maybe_dump_reader_permit_diagnostics(*this, \"kill limit triggered\", &permit);\n        throw utils::memory_limit_reached(format(\"kill limit triggered on semaphore {} by permit {}\", _name, permit.description()));\n    }\n    _resources -= r;\n}\n\nvoid reader_concurrency_semaphore::signal(const resources& r) noexcept {\n    _resources += r;\n    maybe_wake_execution_loop();\n}\n\nnamespace sm = seastar::metrics;\nstatic const sm::label class_label(\"class\");\n\nreader_concurrency_semaphore::reader_concurrency_semaphore(\n        utils::updateable_value<int> count,\n        ssize_t memory,\n        sstring name,\n        size_t max_queue_length,\n        utils::updateable_value<uint32_t> serialize_limit_multiplier,\n        utils::updateable_value<uint32_t> kill_limit_multiplier,\n        utils::updateable_value<uint32_t> cpu_concurrency,\n        register_metrics metrics)\n    : _initial_resources(count, memory)\n    , _resources(count, memory)\n    , _count_observer(count.observe([this] (const int& new_count) { set_resources({new_count, _initial_resources.memory}); }))\n    , _name(std::move(name))\n    , _max_queue_length(max_queue_length)\n    , _serialize_limit_multiplier(std::move(serialize_limit_multiplier))\n    , _kill_limit_multiplier(std::move(kill_limit_multiplier))\n    , _cpu_concurrency(cpu_concurrency)\n{\n    if (metrics == register_metrics::yes) {\n        _metrics.emplace();\n        _metrics->add_group(\"database\", {\n                sm::make_counter(\"sstable_read_queue_overloads\", _stats.total_reads_shed_due_to_overload,\n                               sm::description(\"Counts the number of times the sstable read queue was overloaded. \"\n                                               \"A non-zero value indicates that we have to drop read requests because they arrive faster than we can serve them.\"),\n                               {class_label(_name)}),\n\n                sm::make_gauge(\"active_reads\", [this] { return active_reads(); },\n                               sm::description(\"Holds the number of currently active read operations. \"),\n                               {class_label(_name)}),\n\n                sm::make_gauge(\"reads_memory_consumption\", [this] { return consumed_resources().memory; },\n                               sm::description(\"Holds the amount of memory consumed by current read operations. \"),\n                               {class_label(_name)}),\n\n                sm::make_gauge(\"queued_reads\", _stats.waiters,\n                               sm::description(\"Holds the number of currently queued read operations.\"),\n                               {class_label(_name)}),\n\n                sm::make_gauge(\"paused_reads\", _stats.inactive_reads,\n                               sm::description(\"The number of currently active reads that are temporarily paused.\"),\n                               {class_label(_name)}),\n\n                sm::make_counter(\"paused_reads_permit_based_evictions\", _stats.permit_based_evictions,\n                               sm::description(\"The number of paused reads evicted to free up permits.\"\n                                               \" Permits are required for new reads to start, and the database will evict paused reads (if any)\"\n                                               \" to be able to admit new ones, if there is a shortage of permits.\"),\n                               {class_label(_name)}),\n\n                sm::make_counter(\"reads_shed_due_to_overload\", _stats.total_reads_shed_due_to_overload,\n                               sm::description(\"The number of reads shed because the admission queue reached its max capacity.\"\n                                               \" When the queue is full, excessive reads are shed to avoid overload.\"),\n                               {class_label(_name)}),\n\n                sm::make_gauge(\"disk_reads\", _stats.disk_reads,\n                               sm::description(\"Holds the number of currently active disk read operations. \"),\n                               {class_label(_name)}),\n\n                sm::make_gauge(\"sstables_read\", _stats.sstables_read,\n                               sm::description(\"Holds the number of currently read sstables. \"),\n                               {class_label(_name)}),\n\n                sm::make_counter(\"total_reads\", _stats.total_successful_reads,\n                               sm::description(\"Counts the total number of successful user reads on this shard.\"),\n                               {class_label(_name)}),\n\n                sm::make_counter(\"total_reads_failed\", _stats.total_failed_reads,\n                               sm::description(\"Counts the total number of failed user read operations. \"\n                                               \"Add the total_reads to this value to get the total amount of reads issued on this shard.\"),\n                               {class_label(_name)}),\n                });\n    }\n}\n\nreader_concurrency_semaphore::reader_concurrency_semaphore(no_limits, sstring name, register_metrics metrics)\n    : reader_concurrency_semaphore(\n            utils::updateable_value(std::numeric_limits<int>::max()),\n            std::numeric_limits<ssize_t>::max(),\n            std::move(name),\n            std::numeric_limits<size_t>::max(),\n            utils::updateable_value(std::numeric_limits<uint32_t>::max()),\n            utils::updateable_value(std::numeric_limits<uint32_t>::max()),\n            utils::updateable_value(uint32_t(1)),\n            metrics) {}\n\nreader_concurrency_semaphore::~reader_concurrency_semaphore() {\n    SCYLLA_ASSERT(!_stats.waiters);\n    if (!_stats.total_permits) {\n        // We allow destroy without stop() when the semaphore wasn't used at all yet.\n        return;\n    }\n    if (!_stopped) {\n        on_internal_error_noexcept(rcslog, format(\"~reader_concurrency_semaphore(): semaphore {} not stopped before destruction\", _name));\n        // With the below conditions, we can get away with the semaphore being\n        // unstopped. In this case don't force an abort.\n        SCYLLA_ASSERT(_inactive_reads.empty() && !_close_readers_gate.get_count() && !_permit_gate.get_count() && !_execution_loop_future);\n        broken();\n    }\n}\n\nreader_concurrency_semaphore::inactive_read_handle reader_concurrency_semaphore::register_inactive_read(mutation_reader reader,\n        const dht::partition_range* range) noexcept {\n    auto& permit = reader.permit();\n    if (permit->get_state() == reader_permit::state::waiting_for_memory) {\n        // Kill all outstanding memory requests, the read is going to be evicted.\n        permit->aux_data().pr.set_exception(std::make_exception_ptr(std::bad_alloc{}));\n        dequeue_permit(*permit);\n    }\n    permit->on_register_as_inactive();\n    if (_blessed_permit == &*permit) {\n        _blessed_permit = nullptr;\n        maybe_wake_execution_loop();\n    }\n    if (!should_evict_inactive_read()) {\n      try {\n        permit->aux_data().ir.emplace(std::move(reader), range);\n        permit->unlink();\n        _inactive_reads.push_back(*permit);\n        ++_stats.inactive_reads;\n        return inactive_read_handle(permit);\n      } catch (...) {\n        // It is okay to swallow the exception since\n        // we're allowed to drop the reader upon registration\n        // due to lack of resources. Returning an empty\n        // i_r_h here rather than throwing simplifies the caller's\n        // error handling.\n        rcslog.warn(\"Registering inactive read failed: {}. Ignored as if it was evicted.\", std::current_exception());\n      }\n    } else {\n        permit->on_evicted();\n        ++_stats.permit_based_evictions;\n    }\n    close_reader(std::move(reader));\n    return inactive_read_handle();\n}\n\nvoid reader_concurrency_semaphore::set_notify_handler(inactive_read_handle& irh, eviction_notify_handler&& notify_handler, std::optional<std::chrono::seconds> ttl_opt) {\n    auto& ir = *(*irh._permit)->aux_data().ir;\n    ir.notify_handler = std::move(notify_handler);\n    if (ttl_opt) {\n        ir.ttl_timer.set_callback([this, permit = *irh._permit] () mutable {\n            evict(*permit, evict_reason::time);\n        });\n        ir.ttl_timer.arm(lowres_clock::now() + *ttl_opt);\n    }\n}\n\nmutation_reader_opt reader_concurrency_semaphore::unregister_inactive_read(inactive_read_handle irh) {\n    if (!irh) {\n        return {};\n    }\n    auto& permit = **irh._permit;\n    auto irp = std::move(permit.aux_data().ir);\n\n    if (&permit.semaphore() != this) {\n        // unregister from the other semaphore\n        // and close the reader, in case on_internal_error\n        // doesn't abort.\n        auto& sem = permit.semaphore();\n        sem.close_reader(std::move(irp->reader));\n        on_internal_error(rcslog, fmt::format(\n                    \"reader_concurrency_semaphore::unregister_inactive_read(): \"\n                    \"attempted to unregister an inactive read with a handle belonging to another semaphore: \"\n                    \"this is {} (0x{:x}) but the handle belongs to {} (0x{:x})\",\n                    name(),\n                    reinterpret_cast<uintptr_t>(this),\n                    sem.name(),\n                    reinterpret_cast<uintptr_t>(&sem)));\n    }\n\n    dequeue_permit(permit);\n    permit.on_unregister_as_inactive();\n    return std::move(irp->reader);\n}\n\nbool reader_concurrency_semaphore::try_evict_one_inactive_read(evict_reason reason) {\n    if (_inactive_reads.empty()) {\n        return false;\n    }\n    evict(_inactive_reads.front(), reason);\n    return true;\n}\n\nvoid reader_concurrency_semaphore::clear_inactive_reads() {\n    while (!_inactive_reads.empty()) {\n        evict(_inactive_reads.front(), evict_reason::manual);\n    }\n}\n\nfuture<> reader_concurrency_semaphore::evict_inactive_reads_for_table(table_id id, const dht::partition_range* range) noexcept {\n    auto overlaps_with_range = [range] (const reader_concurrency_semaphore::inactive_read& ir) {\n        if (!range || !ir.range) {\n            return true;\n        }\n        return ir.range->overlaps(*range, dht::ring_position_comparator(*ir.reader.schema()));\n    };\n\n    permit_list_type evicted_readers;\n    auto it = _inactive_reads.begin();\n    while (it != _inactive_reads.end()) {\n        auto& permit = *it;\n        auto& ir = *permit.aux_data().ir;\n        ++it;\n        if (ir.reader.schema()->id() == id && overlaps_with_range(ir)) {\n            do_detach_inactive_reader(permit, evict_reason::manual);\n            permit.unlink();\n            evicted_readers.push_back(permit);\n        }\n    }\n    while (!evicted_readers.empty()) {\n        auto& permit = evicted_readers.front();\n        auto irp = std::move(permit.aux_data().ir);\n        permit.unlink();\n        _permit_list.push_back(permit);\n        // Closing the reader might destroy the last permit instance, killing the\n        // permit itself, so this close has to be last in this scope.\n        co_await irp->reader.close();\n    }\n}\n\nstd::runtime_error reader_concurrency_semaphore::stopped_exception() {\n    return std::runtime_error(format(\"{} was stopped\", _name));\n}\n\nfuture<> reader_concurrency_semaphore::stop() noexcept {\n    SCYLLA_ASSERT(!_stopped);\n    _stopped = true;\n    co_await stop_ext_pre();\n    clear_inactive_reads();\n    co_await _permit_gate.close();\n    // Gate for closing readers is only closed after waiting for all reads, as the evictable\n    // readers might take the inactive registration path and find the gate closed.\n    co_await _close_readers_gate.close();\n    _ready_list_cv.broken(std::make_exception_ptr(stop_execution_loop{}));\n    if (_execution_loop_future) {\n        co_await std::move(*_execution_loop_future);\n    }\n    broken(std::make_exception_ptr(stopped_exception()));\n    co_await stop_ext_post();\n    co_return;\n}\n\nvoid reader_concurrency_semaphore::do_detach_inactive_reader(reader_permit::impl& permit, evict_reason reason) noexcept {\n    dequeue_permit(permit);\n    auto& ir = *permit.aux_data().ir;\n    ir.ttl_timer.cancel();\n    ir.detach();\n    ir.reader.permit()->on_evicted();\n    tracing::trace(permit.trace_state(), \"[reader_concurrency_semaphore {}] evicted, reason: {}\", _name, reason);\n    try {\n        if (ir.notify_handler) {\n            ir.notify_handler(reason);\n        }\n    } catch (...) {\n        rcslog.error(\"[semaphore {}] evict(): notify handler failed for inactive read evicted due to {}: {}\", _name, static_cast<int>(reason), std::current_exception());\n    }\n    switch (reason) {\n        case evict_reason::permit:\n            ++_stats.permit_based_evictions;\n            break;\n        case evict_reason::time:\n            ++_stats.time_based_evictions;\n            break;\n        case evict_reason::manual:\n            break;\n    }\n}\n\nmutation_reader reader_concurrency_semaphore::detach_inactive_reader(reader_permit::impl& permit, evict_reason reason) noexcept {\n    do_detach_inactive_reader(permit, reason);\n    auto irp = std::move(permit.aux_data().ir);\n    return std::move(irp->reader);\n}\n\nvoid reader_concurrency_semaphore::evict(reader_permit::impl& permit, evict_reason reason) noexcept {\n    close_reader(detach_inactive_reader(permit, reason));\n}\n\nvoid reader_concurrency_semaphore::close_reader(mutation_reader reader) {\n    // It is safe to discard the future since it is waited on indirectly\n    // by closing the _close_readers_gate in stop().\n    (void)with_gate(_close_readers_gate, [reader = std::move(reader)] () mutable {\n        return reader.close();\n    });\n}\n\nbool reader_concurrency_semaphore::has_available_units(const resources& r) const {\n    // Special case: when there is no active reader (based on count) admit one\n    // regardless of availability of memory.\n    return (_resources.non_zero() && _resources.count >= r.count && _resources.memory >= r.memory) || _resources.count == _initial_resources.count;\n}\n\nbool reader_concurrency_semaphore::cpu_concurrency_limit_reached() const {\n    return (_stats.need_cpu_permits - _stats.awaits_permits) >= _cpu_concurrency();\n}\n\nstd::exception_ptr reader_concurrency_semaphore::check_queue_size(std::string_view queue_name) {\n    if (_stats.waiters >= _max_queue_length) {\n        _stats.total_reads_shed_due_to_overload++;\n        maybe_dump_reader_permit_diagnostics(*this, fmt::format(\"{} queue overload\", queue_name), nullptr);\n        return std::make_exception_ptr(std::runtime_error(fmt::format(\"{}: {} queue overload\", _name, queue_name)));\n    }\n    return {};\n}\n\nfuture<> reader_concurrency_semaphore::enqueue_waiter(reader_permit::impl& permit, wait_on wait) {\n    if (auto ex = check_queue_size(\"wait\")) {\n        return make_exception_future<>(std::move(ex));\n    }\n    auto& ad = permit.aux_data();\n    ad.pr = {};\n    auto fut = ad.pr.get_future();\n    if (wait == wait_on::admission) {\n        permit.on_waiting_for_admission();\n        _wait_list.push_to_admission_queue(permit);\n        ++_stats.reads_enqueued_for_admission;\n    } else {\n        permit.on_waiting_for_memory();\n        ad.fut.emplace(std::move(fut));\n        fut = ad.fut->get_future();\n        _wait_list.push_to_memory_queue(permit);\n        ++_stats.reads_enqueued_for_memory;\n    }\n    ++_stats.waiters;\n    return fut;\n}\n\nvoid reader_concurrency_semaphore::evict_readers_in_background() {\n    if (_evicting) {\n        return;\n    }\n    _evicting = true;\n    // Evict inactive readers in the background while wait list isn't empty\n    // This is safe since stop() closes _gate;\n    (void)with_gate(_close_readers_gate, [this] {\n        return repeat([this] {\n            if (_inactive_reads.empty() || !should_evict_inactive_read()) {\n                _evicting = false;\n                return make_ready_future<stop_iteration>(stop_iteration::yes);\n            }\n            return detach_inactive_reader(_inactive_reads.front(), evict_reason::permit).close().then([] {\n                return stop_iteration::no;\n            });\n        });\n    });\n}\n\nreader_concurrency_semaphore::admit_result\nreader_concurrency_semaphore::can_admit_read(const reader_permit::impl& permit) const noexcept {\n    if (_resources.memory < 0) [[unlikely]] {\n        const auto consumed_memory = consumed_resources().memory;\n        if (std::cmp_greater_equal(consumed_memory, get_kill_limit())) {\n            return {can_admit::no, reason::memory_resources};\n        }\n        if (std::cmp_greater_equal(consumed_memory, get_serialize_limit())) {\n            if (_blessed_permit) {\n                // blessed permit is never in the wait list\n                return {can_admit::no, reason::memory_resources};\n            } else {\n                if (permit.get_state() == reader_permit::state::waiting_for_memory) {\n                    return {can_admit::yes, reason::all_ok};\n                } else {\n                    return {can_admit::no, reason::memory_resources};\n                }\n            }\n        }\n    }\n\n    if (permit.get_state() == reader_permit::state::waiting_for_memory) {\n        return {can_admit::yes, reason::all_ok};\n    }\n\n    if (!_ready_list.empty()) {\n        return {can_admit::no, reason::ready_list};\n    }\n\n    if (cpu_concurrency_limit_reached()) {\n        return {can_admit::no, reason::need_cpu_permits};\n    }\n\n    if (!has_available_units(permit.base_resources())) {\n        auto reason = _resources.memory >= permit.base_resources().memory ? reason::count_resources : reason::memory_resources;\n        if (_inactive_reads.empty()) {\n            return {can_admit::no, reason};\n        } else {\n            return {can_admit::maybe, reason};\n        }\n    }\n\n    return {can_admit::yes, reason::all_ok};\n}\n\nbool reader_concurrency_semaphore::should_evict_inactive_read() const noexcept {\n    if (_resources.memory < 0 || _resources.count < 0) {\n        return true;\n    }\n    if (_wait_list.empty()) {\n        return false;\n    }\n    const auto r = can_admit_read(_wait_list.front()).why;\n    return r == reason::memory_resources || r == reason::count_resources;\n}\n\nfuture<> reader_concurrency_semaphore::do_wait_admission(reader_permit::impl& permit) {\n    if (!_execution_loop_future) {\n        _execution_loop_future.emplace(execution_loop());\n    }\n\n    static uint64_t stats::*stats_table[] = {\n        &stats::reads_admitted_immediately,\n        &stats::reads_queued_because_ready_list,\n        &stats::reads_queued_because_need_cpu_permits,\n        &stats::reads_queued_because_memory_resources,\n        &stats::reads_queued_because_count_resources\n    };\n\n    static const char* result_as_string[] = {\n        \"admitted immediately\",\n        \"queued because of non-empty ready list\",\n        \"queued because of need_cpu permits\",\n        \"queued because of memory resources\",\n        \"queued because of count resources\"\n    };\n\n    const auto [admit, why] = can_admit_read(permit);\n    ++(_stats.*stats_table[static_cast<int>(why)]);\n    tracing::trace(permit.trace_state(), \"[reader concurrency semaphore {}] {}\", _name, result_as_string[static_cast<int>(why)]);\n    if (admit != can_admit::yes || !_wait_list.empty()) {\n        auto fut = enqueue_waiter(permit, wait_on::admission);\n        if (admit == can_admit::yes && !_wait_list.empty()) {\n            // This is a contradiction: the semaphore could admit waiters yet it has waiters.\n            // Normally, the semaphore should admit waiters as soon as it can.\n            // So at any point in time, there should either be no waiters, or it\n            // shouldn't be able to admit new reads. Otherwise something went wrong.\n            maybe_dump_reader_permit_diagnostics(*this, \"semaphore could admit new reads yet there are waiters\", nullptr);\n            maybe_wake_execution_loop();\n        } else if (admit == can_admit::maybe) {\n            tracing::trace(permit.trace_state(), \"[reader concurrency semaphore {}] evicting inactive reads in the background to free up resources\", _name);\n            ++_stats.reads_queued_with_eviction;\n            evict_readers_in_background();\n        }\n        return fut;\n    }\n\n    permit.on_admission();\n    ++_stats.reads_admitted;\n    if (permit.aux_data().func) {\n        return with_ready_permit(permit);\n    }\n    return make_ready_future<>();\n}\n\nvoid reader_concurrency_semaphore::maybe_admit_waiters() noexcept {\n    auto admit = can_admit::no;\n    while (!_wait_list.empty() && (admit = can_admit_read(_wait_list.front()).decision) == can_admit::yes) {\n        auto& permit = _wait_list.front();\n        dequeue_permit(permit);\n        try {\n            if (permit.get_state() == reader_permit::state::waiting_for_memory) {\n                _blessed_permit = &permit;\n                permit.on_granted_memory();\n            } else {\n                permit.on_admission();\n                ++_stats.reads_admitted;\n            }\n            if (permit.aux_data().func) {\n                permit.unlink();\n                _ready_list.push_back(permit);\n                _ready_list_cv.signal();\n                permit.on_waiting_for_execution();\n                ++_stats.waiters;\n            } else {\n                permit.aux_data().pr.set_value();\n            }\n        } catch (...) {\n            permit.aux_data().pr.set_exception(std::current_exception());\n        }\n    }\n    if (admit == can_admit::maybe) {\n        // Evicting readers will trigger another call to `maybe_admit_waiters()` from `signal()`.\n        evict_readers_in_background();\n    }\n}\n\nvoid reader_concurrency_semaphore::maybe_wake_execution_loop() noexcept {\n    if (!_wait_list.empty()) {\n        _ready_list_cv.signal();\n    }\n}\n\nfuture<> reader_concurrency_semaphore::request_memory(reader_permit::impl& permit, size_t memory) {\n    // Already blocked on memory?\n    if (permit.get_state() == reader_permit::state::waiting_for_memory) {\n        return permit.aux_data().fut->get_future();\n    }\n\n    if (_resources.memory > 0 || (consumed_resources().memory + memory) < get_serialize_limit()) {\n        permit.on_granted_memory();\n        return make_ready_future<>();\n    }\n\n    if (!_blessed_permit) {\n        _blessed_permit = &permit;\n    }\n\n    if (_blessed_permit == &permit) {\n        permit.on_granted_memory();\n        return make_ready_future<>();\n    }\n\n    return enqueue_waiter(permit, wait_on::memory);\n}\n\nvoid reader_concurrency_semaphore::dequeue_permit(reader_permit::impl& permit) {\n    switch (permit.get_state()) {\n        case reader_permit::state::waiting_for_admission:\n        case reader_permit::state::waiting_for_memory:\n        case reader_permit::state::waiting_for_execution:\n            --_stats.waiters;\n            break;\n        case reader_permit::state::inactive:\n        case reader_permit::state::evicted:\n            --_stats.inactive_reads;\n            break;\n        case reader_permit::state::active:\n        case reader_permit::state::active_need_cpu:\n        case reader_permit::state::active_await:\n            on_internal_error_noexcept(rcslog, format(\"reader_concurrency_semaphore::dequeue_permit(): unrecognized queued state: {}\", permit.get_state()));\n    }\n    permit.unlink();\n    _permit_list.push_back(permit);\n}\n\nvoid reader_concurrency_semaphore::on_permit_created(reader_permit::impl& permit) {\n    _permit_gate.enter();\n    _permit_list.push_back(permit);\n    ++_stats.total_permits;\n    ++_stats.current_permits;\n}\n\nvoid reader_concurrency_semaphore::on_permit_destroyed(reader_permit::impl& permit) noexcept {\n    permit.unlink();\n    _permit_gate.leave();\n    --_stats.current_permits;\n    if (_blessed_permit == &permit) {\n        _blessed_permit = nullptr;\n        maybe_wake_execution_loop();\n    }\n}\n\nvoid reader_concurrency_semaphore::on_permit_need_cpu() noexcept {\n    ++_stats.need_cpu_permits;\n}\n\nvoid reader_concurrency_semaphore::on_permit_not_need_cpu() noexcept {\n    SCYLLA_ASSERT(_stats.need_cpu_permits);\n    --_stats.need_cpu_permits;\n    SCYLLA_ASSERT(_stats.need_cpu_permits >= _stats.awaits_permits);\n    maybe_wake_execution_loop();\n}\n\nvoid reader_concurrency_semaphore::on_permit_awaits() noexcept {\n    ++_stats.awaits_permits;\n    SCYLLA_ASSERT(_stats.need_cpu_permits >= _stats.awaits_permits);\n    maybe_wake_execution_loop();\n}\n\nvoid reader_concurrency_semaphore::on_permit_not_awaits() noexcept {\n    SCYLLA_ASSERT(_stats.awaits_permits);\n    --_stats.awaits_permits;\n}\n\nfuture<reader_permit> reader_concurrency_semaphore::obtain_permit(schema_ptr schema, const char* const op_name, size_t memory,\n        db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr) {\n    auto permit = reader_permit(*this, std::move(schema), std::string_view(op_name), {1, static_cast<ssize_t>(memory)}, timeout, std::move(trace_ptr));\n    return do_wait_admission(*permit).then([permit] () mutable {\n        return std::move(permit);\n    });\n}\n\nfuture<reader_permit> reader_concurrency_semaphore::obtain_permit(schema_ptr schema, sstring&& op_name, size_t memory,\n        db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr) {\n    auto permit = reader_permit(*this, std::move(schema), std::move(op_name), {1, static_cast<ssize_t>(memory)}, timeout, std::move(trace_ptr));\n    return do_wait_admission(*permit).then([permit] () mutable {\n        return std::move(permit);\n    });\n}\n\nreader_permit reader_concurrency_semaphore::make_tracking_only_permit(schema_ptr schema, const char* const op_name,\n        db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr) {\n    return reader_permit(*this, std::move(schema), std::string_view(op_name), {}, timeout, std::move(trace_ptr));\n}\n\nreader_permit reader_concurrency_semaphore::make_tracking_only_permit(schema_ptr schema, sstring&& op_name,\n        db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr) {\n    return reader_permit(*this, std::move(schema), std::move(op_name), {}, timeout, std::move(trace_ptr));\n}\n\nfuture<> reader_concurrency_semaphore::with_permit(schema_ptr schema, const char* const op_name, size_t memory,\n        db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr, read_func func) {\n    auto permit = reader_permit(*this, std::move(schema), std::string_view(op_name), {1, static_cast<ssize_t>(memory)}, timeout, std::move(trace_ptr));\n    permit->aux_data().func = std::move(func);\n    permit->aux_data().permit_keepalive = permit;\n    return do_wait_admission(*permit);\n}\n\nfuture<> reader_concurrency_semaphore::with_ready_permit(reader_permit::impl& permit) {\n    if (auto ex = check_queue_size(\"ready\")) {\n        return make_exception_future<>(std::move(ex));\n    }\n    auto& ad = permit.aux_data();\n    ad.pr = {};\n    auto fut = ad.pr.get_future();\n    permit.unlink();\n    _ready_list.push_back(permit);\n    permit.on_waiting_for_execution();\n    ++_stats.waiters;\n    _ready_list_cv.signal();\n    return fut;\n}\n\nfuture<> reader_concurrency_semaphore::with_ready_permit(reader_permit permit, read_func func) {\n    permit->aux_data().func = std::move(func);\n    return with_ready_permit(*permit);\n}\n\nvoid reader_concurrency_semaphore::set_resources(resources r) {\n    auto delta = r - _initial_resources;\n    _initial_resources = r;\n    _resources += delta;\n    maybe_wake_execution_loop();\n}\n\nvoid reader_concurrency_semaphore::broken(std::exception_ptr ex) {\n    if (!ex) {\n        ex = std::make_exception_ptr(broken_semaphore{});\n    }\n    while (!_wait_list.empty()) {\n        auto& permit = _wait_list.front();\n        permit.aux_data().pr.set_exception(ex);\n        dequeue_permit(permit);\n    }\n}\n\nstd::string reader_concurrency_semaphore::dump_diagnostics(unsigned max_lines) const {\n    std::ostringstream os;\n    do_dump_reader_permit_diagnostics(os, *this, \"user request\", nullptr, max_lines);\n    return std::move(os).str();\n}\n\nvoid reader_concurrency_semaphore::foreach_permit(noncopyable_function<void(const reader_permit::impl&)> func) const {\n    boost::for_each(_permit_list, std::ref(func));\n    boost::for_each(_wait_list._admission_queue, std::ref(func));\n    boost::for_each(_wait_list._memory_queue, std::ref(func));\n    boost::for_each(_ready_list, std::ref(func));\n}\n\nvoid reader_concurrency_semaphore::foreach_permit(noncopyable_function<void(const reader_permit&)> func) const {\n    foreach_permit([func = std::move(func)] (const reader_permit::impl& p) {\n        // We cast away const to construct a reader_permit but the resulting\n        // object is passed as const& so there is no const violation here.\n        func(reader_permit(const_cast<reader_permit::impl&>(p).shared_from_this()));\n    });\n}\n\n// A file that tracks the memory usage of buffers resulting from read\n// operations.\nclass tracking_file_impl : public file_impl {\n    file _tracked_file;\n    reader_permit _permit;\n\npublic:\n    tracking_file_impl(file file, reader_permit permit)\n        : file_impl(*get_file_impl(file))\n        , _tracked_file(std::move(file))\n        , _permit(std::move(permit)) {\n    }\n\n    tracking_file_impl(const tracking_file_impl&) = delete;\n    tracking_file_impl& operator=(const tracking_file_impl&) = delete;\n    tracking_file_impl(tracking_file_impl&&) = default;\n    tracking_file_impl& operator=(tracking_file_impl&&) = default;\n\n    virtual future<size_t> write_dma(uint64_t pos, const void* buffer, size_t len, io_intent* intent) override {\n        return get_file_impl(_tracked_file)->write_dma(pos, buffer, len, intent);\n    }\n\n    virtual future<size_t> write_dma(uint64_t pos, std::vector<iovec> iov, io_intent* intent) override {\n        return get_file_impl(_tracked_file)->write_dma(pos, std::move(iov), intent);\n    }\n\n    virtual future<size_t> read_dma(uint64_t pos, void* buffer, size_t len, io_intent* intent) override {\n        return get_file_impl(_tracked_file)->read_dma(pos, buffer, len, intent);\n    }\n\n    virtual future<size_t> read_dma(uint64_t pos, std::vector<iovec> iov, io_intent* intent) override {\n        return get_file_impl(_tracked_file)->read_dma(pos, iov, intent);\n    }\n\n    virtual future<> flush(void) override {\n        return get_file_impl(_tracked_file)->flush();\n    }\n\n    virtual future<struct stat> stat(void) override {\n        return get_file_impl(_tracked_file)->stat();\n    }\n\n    virtual future<> truncate(uint64_t length) override {\n        return get_file_impl(_tracked_file)->truncate(length);\n    }\n\n    virtual future<> discard(uint64_t offset, uint64_t length) override {\n        return get_file_impl(_tracked_file)->discard(offset, length);\n    }\n\n    virtual future<> allocate(uint64_t position, uint64_t length) override {\n        return get_file_impl(_tracked_file)->allocate(position, length);\n    }\n\n    virtual future<uint64_t> size(void) override {\n        return get_file_impl(_tracked_file)->size();\n    }\n\n    virtual future<> close() override {\n        return get_file_impl(_tracked_file)->close();\n    }\n\n    virtual std::unique_ptr<file_handle_impl> dup() override {\n        return get_file_impl(_tracked_file)->dup();\n    }\n\n    virtual subscription<directory_entry> list_directory(std::function<future<> (directory_entry de)> next) override {\n        return get_file_impl(_tracked_file)->list_directory(std::move(next));\n    }\n\n    virtual future<temporary_buffer<uint8_t>> dma_read_bulk(uint64_t offset, size_t range_size, io_intent* intent) override {\n        return _permit.request_memory(range_size).then([this, offset, range_size, intent] (reader_permit::resource_units units) {\n            return get_file_impl(_tracked_file)->dma_read_bulk(offset, range_size, intent).then([units = std::move(units)] (temporary_buffer<uint8_t> buf) mutable {\n                return make_ready_future<temporary_buffer<uint8_t>>(make_tracked_temporary_buffer(std::move(buf), std::move(units)));\n            });\n        });\n    }\n};\n\nfile make_tracked_file(file f, reader_permit p) {\n    return file(make_shared<tracking_file_impl>(f, std::move(p)));\n}\n"
        },
        {
          "name": "reader_concurrency_semaphore.hh",
          "type": "blob",
          "size": 22.76953125,
          "content": "/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n#pragma once\n\n#include <boost/intrusive/list.hpp>\n#include <seastar/core/future.hh>\n#include <seastar/core/gate.hh>\n#include <seastar/core/condition-variable.hh>\n#include <seastar/core/metrics_registration.hh>\n#include \"reader_permit.hh\"\n#include \"utils/updateable_value.hh\"\n#include \"dht/i_partitioner_fwd.hh\"\n\nnamespace bi = boost::intrusive;\n\nusing namespace seastar;\n\nclass mutation_reader;\nusing mutation_reader_opt = optimized_optional<mutation_reader>;\n\n/// Specific semaphore for controlling reader concurrency\n///\n/// Use `make_permit()` to create a permit to track the resource consumption\n/// of a specific read. The permit should be created before the read is even\n/// started so it is available to track resource consumption from the start.\n/// Reader concurrency is dual limited by count and memory.\n/// The semaphore can be configured with the desired limits on\n/// construction. New readers will only be admitted when there is both\n/// enough count and memory units available. Readers are admitted in\n/// FIFO order.\n/// Semaphore's `name` must be provided in ctor and its only purpose is\n/// to increase readability of exceptions: both timeout exceptions and\n/// queue overflow exceptions (read below) include this `name` in messages.\n/// It's also possible to specify the maximum allowed number of waiting\n/// readers by the `max_queue_length` constructor parameter. When the\n/// number of waiting readers becomes equal or greater than\n/// `max_queue_length` (upon calling `obtain_permit()`) an exception of\n/// type `std::runtime_error` is thrown. Optionally, some additional\n/// code can be executed just before throwing (`prethrow_action` \n/// constructor parameter).\n///\n/// The semaphore has 3 layers of defense against consuming more memory\n/// than desired:\n/// 1) After memory consumption is larger than the configured memory limit,\n///    no more reads are admitted\n/// 2) After memory consumption is larger than `_serialize_limit_multiplier`\n///    times the configured memory limit, reads are serialized: only one of them\n///    is allowed to make progress, the rest is made to wait before they can\n///    consume more memory. Enforced via `request_memory()`.\n/// 4) After memory consumption is larger than `_kill_limit_multiplier`\n///    times the configured memory limit, reads are killed, by `consume()`\n///    throwing `std::bad_alloc`.\n///\n/// This makes `_kill_limit_multiplier` times the memory limit the effective\n/// upper bound of the memory consumed by reads.\n///\n/// The semaphore also acts as an execution stage for reads. This\n/// functionality is exposed via \\ref with_permit() and \\ref\n/// with_ready_permit().\nclass reader_concurrency_semaphore {\npublic:\n    using resources = reader_resources;\n\n    friend class reader_permit;\n\n    enum class evict_reason {\n        permit, // evicted due to permit shortage\n        time, // evicted due to expiring ttl\n        manual, // evicted manually via `try_evict_one_inactive_read()`\n    };\n\n    using eviction_notify_handler = noncopyable_function<void(evict_reason)>;\n\n    struct stats {\n        // The number of inactive reads evicted to free up permits.\n        uint64_t permit_based_evictions = 0;\n        // The number of inactive reads evicted due to expiring.\n        uint64_t time_based_evictions = 0;\n        // The number of inactive reads currently registered.\n        uint64_t inactive_reads = 0;\n        // Total number of successful reads executed through this semaphore.\n        uint64_t total_successful_reads = 0;\n        // Total number of failed reads executed through this semaphore.\n        uint64_t total_failed_reads = 0;\n        // Total number of reads rejected because the admission queue reached its max capacity\n        uint64_t total_reads_shed_due_to_overload = 0;\n        // Total number of reads killed due to the memory consumption reaching the kill limit.\n        uint64_t total_reads_killed_due_to_kill_limit = 0;\n        // Total number of reads admitted, via all admission paths.\n        uint64_t reads_admitted = 0;\n        // Total number of reads enqueued to wait for admission.\n        uint64_t reads_enqueued_for_admission = 0;\n        // Total number of reads enqueued to wait for memory.\n        uint64_t reads_enqueued_for_memory = 0;\n        // Total number of reads admitted immediately, without queueing\n        uint64_t reads_admitted_immediately = 0;\n        // Total number of reads enqueued because ready_list wasn't empty\n        uint64_t reads_queued_because_ready_list = 0;\n        // Total number of reads enqueued because there are permits who need CPU to make progress\n        uint64_t reads_queued_because_need_cpu_permits = 0;\n        // Total number of reads enqueued because there weren't enough memory resources\n        uint64_t reads_queued_because_memory_resources = 0;\n        // Total number of reads enqueued because there weren't enough count resources\n        uint64_t reads_queued_because_count_resources = 0;\n        // Total number of reads enqueued to be maybe admitted after evicting some inactive reads\n        uint64_t reads_queued_with_eviction = 0;\n        // Total number of permits created so far.\n        uint64_t total_permits = 0;\n        // Current number of permits.\n        uint64_t current_permits = 0;\n        // Current number permits needing CPU to make progress.\n        uint64_t need_cpu_permits = 0;\n        // Current number of permits awaiting I/O or an operation running on a remote shard.\n        uint64_t awaits_permits = 0;\n        // Current number of reads reading from the disk.\n        uint64_t disk_reads = 0;\n        // The number of sstables read currently.\n        uint64_t sstables_read = 0;\n        // Permits waiting on something: admission, memory or execution\n        uint64_t waiters = 0;\n\n        friend auto operator<=>(const stats&, const stats&) = default;\n    };\n\n    using permit_list_type = bi::list<\n            reader_permit::impl,\n            bi::base_hook<bi::list_base_hook<bi::link_mode<bi::auto_unlink>>>,\n            bi::constant_time_size<false>>;\n\n    using read_func = noncopyable_function<future<>(reader_permit)>;\n\nprivate:\n    struct inactive_read;\n\npublic:\n    class inactive_read_handle {\n        reader_permit_opt _permit;\n\n        friend class reader_concurrency_semaphore;\n\n    private:\n        void abandon() noexcept;\n\n        explicit inactive_read_handle(reader_permit permit) noexcept;\n    public:\n        inactive_read_handle() = default;\n        inactive_read_handle(inactive_read_handle&& o) noexcept;\n        inactive_read_handle& operator=(inactive_read_handle&& o) noexcept;\n        ~inactive_read_handle() {\n            abandon();\n        }\n        explicit operator bool() const noexcept {\n            return bool(_permit);\n        }\n    };\n\nprivate:\n    resources _initial_resources;\n    resources _resources;\n    utils::observer<int> _count_observer;\n\n    struct wait_queue {\n        // Stores entries for permits waiting to be admitted.\n        permit_list_type _admission_queue;\n        // Stores entries for serialized permits waiting to obtain memory.\n        permit_list_type _memory_queue;\n    public:\n        bool empty() const {\n            return _admission_queue.empty() && _memory_queue.empty();\n        }\n        void push_to_admission_queue(reader_permit::impl& p);\n        void push_to_memory_queue(reader_permit::impl& p);\n        reader_permit::impl& front();\n        const reader_permit::impl& front() const;\n    };\n\n    wait_queue _wait_list;\n    permit_list_type _ready_list;\n    condition_variable _ready_list_cv;\n    permit_list_type _inactive_reads;\n    // Stores permits that are not in any of the above list.\n    permit_list_type _permit_list;\n\n    sstring _name;\n    size_t _max_queue_length = std::numeric_limits<size_t>::max();\n    utils::updateable_value<uint32_t> _serialize_limit_multiplier;\n    utils::updateable_value<uint32_t> _kill_limit_multiplier;\n    utils::updateable_value<uint32_t> _cpu_concurrency;\n    stats _stats;\n    std::optional<seastar::metrics::metric_groups> _metrics;\n    bool _stopped = false;\n    bool _evicting = false;\n    gate _close_readers_gate;\n    gate _permit_gate;\n    std::optional<future<>> _execution_loop_future;\n    reader_permit::impl* _blessed_permit = nullptr;\n\nprivate:\n    void do_detach_inactive_reader(reader_permit::impl&, evict_reason reason) noexcept;\n    [[nodiscard]] mutation_reader detach_inactive_reader(reader_permit::impl&, evict_reason reason) noexcept;\n    void evict(reader_permit::impl&, evict_reason reason) noexcept;\n\n    bool has_available_units(const resources& r) const;\n\n    bool cpu_concurrency_limit_reached() const;\n\n    [[nodiscard]] std::exception_ptr check_queue_size(std::string_view queue_name);\n\n    // Add the permit to the wait queue and return the future which resolves when\n    // the permit is admitted (popped from the queue).\n    enum class wait_on { admission, memory };\n    future<> enqueue_waiter(reader_permit::impl& permit, wait_on wait);\n    void evict_readers_in_background();\n    future<> do_wait_admission(reader_permit::impl& permit);\n\n    // Check whether permit can be admitted or not.\n    // The wait list is not taken into consideration, this is the caller's\n    // responsibility.\n    // A return value of can_admit::maybe means admission might be possible if\n    // some of the inactive readers are evicted.\n    enum class can_admit { no, maybe, yes };\n    enum class reason { all_ok = 0, ready_list, need_cpu_permits, memory_resources, count_resources };\n    struct admit_result { can_admit decision; reason why; };\n    admit_result can_admit_read(const reader_permit::impl& permit) const noexcept;\n\n    bool should_evict_inactive_read() const noexcept;\n\n    void maybe_admit_waiters() noexcept;\n\n    void maybe_wake_execution_loop() noexcept;\n\n    // Request more memory for the permit.\n    // Request is instantly granted while memory consumption of all reads is\n    // below _kill_limit_multiplier.\n    // After memory consumption goes above the above limit, only one reader\n    // (permit) is allowed to make progress, this method will block for all other\n    // one, until:\n    // * The blessed read finishes and a new blessed permit is chosen.\n    // * Memory consumption falls below the limit.\n    future<> request_memory(reader_permit::impl& permit, size_t memory);\n\n    void dequeue_permit(reader_permit::impl&);\n\n    void on_permit_created(reader_permit::impl&);\n    void on_permit_destroyed(reader_permit::impl&) noexcept;\n\n    void on_permit_need_cpu() noexcept;\n    void on_permit_not_need_cpu() noexcept;\n\n    void on_permit_awaits() noexcept;\n    void on_permit_not_awaits() noexcept;\n\n    std::runtime_error stopped_exception();\n\n    // closes reader in the background.\n    void close_reader(mutation_reader reader);\n\n    future<> execution_loop() noexcept;\n\n    uint64_t get_serialize_limit() const;\n    uint64_t get_kill_limit() const;\n\n    // Throws std::bad_alloc if memory consumed is oom_kill_limit_multiply_threshold more than the memory limit.\n    void consume(reader_permit::impl& permit, resources r);\n    void signal(const resources& r) noexcept;\n\n    future<> with_ready_permit(reader_permit::impl& permit);\n\npublic:\n    struct no_limits { };\n    using register_metrics = bool_class<class register_metrics_clas>;\n\n    /// Create a semaphore with the specified limits\n    ///\n    /// The semaphore's name has to be unique!\n    reader_concurrency_semaphore(\n            utils::updateable_value<int> count,\n            ssize_t memory,\n            sstring name,\n            size_t max_queue_length,\n            utils::updateable_value<uint32_t> serialize_limit_multiplier,\n            utils::updateable_value<uint32_t> kill_limit_multiplier,\n            utils::updateable_value<uint32_t> cpu_concurrency,\n            register_metrics metrics);\n\n    reader_concurrency_semaphore(\n            int count,\n            ssize_t memory,\n            sstring name,\n            size_t max_queue_length,\n            utils::updateable_value<uint32_t> serialize_limit_multiplier,\n            utils::updateable_value<uint32_t> kill_limit_multiplier,\n            register_metrics metrics)\n        : reader_concurrency_semaphore(utils::updateable_value(count), memory, std::move(name), max_queue_length,\n                std::move(serialize_limit_multiplier), std::move(kill_limit_multiplier), utils::updateable_value<uint32_t>(1), metrics)\n    { }\n\n    /// Create a semaphore with practically unlimited count and memory.\n    ///\n    /// And conversely, no queue limit either.\n    /// The semaphore's name has to be unique!\n    explicit reader_concurrency_semaphore(no_limits, sstring name, register_metrics metrics);\n\n    /// A helper constructor *only for tests* that supplies default arguments.\n    /// The other constructors have default values removed so 'production-code'\n    /// is forced to specify all of them manually to avoid bugs.\n    struct for_tests{};\n    reader_concurrency_semaphore(for_tests, sstring name,\n            int count = std::numeric_limits<int>::max(),\n            ssize_t memory = std::numeric_limits<ssize_t>::max(),\n            size_t max_queue_length = std::numeric_limits<size_t>::max(),\n            utils::updateable_value<uint32_t> serialize_limit_multipler = utils::updateable_value(std::numeric_limits<uint32_t>::max()),\n            utils::updateable_value<uint32_t> kill_limit_multipler = utils::updateable_value(std::numeric_limits<uint32_t>::max()),\n            utils::updateable_value<uint32_t> cpu_concurrency = utils::updateable_value<uint32_t>(1),\n            register_metrics metrics = register_metrics::no)\n        : reader_concurrency_semaphore(utils::updateable_value(count), memory, std::move(name), max_queue_length, std::move(serialize_limit_multipler),\n                std::move(kill_limit_multipler), std::move(cpu_concurrency), metrics)\n    {}\n\n    virtual ~reader_concurrency_semaphore();\n\n    reader_concurrency_semaphore(const reader_concurrency_semaphore&) = delete;\n    reader_concurrency_semaphore& operator=(const reader_concurrency_semaphore&) = delete;\n\n    reader_concurrency_semaphore(reader_concurrency_semaphore&&) = delete;\n    reader_concurrency_semaphore& operator=(reader_concurrency_semaphore&&) = delete;\n\n    /// Returns the name of the semaphore\n    ///\n    /// If the semaphore has no name, \"unnamed reader concurrency semaphore\" is returned.\n    std::string_view name() const {\n        return _name.empty() ? \"unnamed reader concurrency semaphore\" : std::string_view(_name);\n    }\n\n    /// Register an inactive read.\n    ///\n    /// The semaphore will evict this read when there is a shortage of\n    /// permits. This might be immediate, during this register call.\n    /// Clients can use the returned handle to unregister the read, when it\n    /// stops being inactive and hence evictable, or to set the optional\n    /// notify_handler and ttl.\n    ///\n    /// The semaphore takes ownership of the passed in reader for the duration\n    /// of its inactivity and it may evict it to free up resources if necessary.\n    inactive_read_handle register_inactive_read(mutation_reader ir, const dht::partition_range* range = nullptr) noexcept;\n\n    /// Set the inactive read eviction notification handler and optionally eviction ttl.\n    ///\n    /// The semaphore may evict this read when there is a shortage of\n    /// permits or after the given ttl expired.\n    ///\n    /// The notification handler will be called when the inactive read is evicted\n    /// passing with the reason it was evicted to the handler.\n    ///\n    /// Note that the inactive read might have already been evicted if\n    /// the caller may yield after the register_inactive_read returned the handle\n    /// and before calling set_notify_handler. In this case, the caller must revalidate\n    /// the inactive_read_handle before calling this function.\n    void set_notify_handler(inactive_read_handle& irh, eviction_notify_handler&& handler, std::optional<std::chrono::seconds> ttl);\n\n    /// Unregister the previously registered inactive read.\n    ///\n    /// If the read was not evicted, the inactive read object, passed in to the\n    /// register call, will be returned. Otherwise a nullptr is returned.\n    mutation_reader_opt unregister_inactive_read(inactive_read_handle irh);\n\n    /// Try to evict an inactive read.\n    ///\n    /// Return true if an inactive read was evicted and false otherwise\n    /// (if there was no reader to evict).\n    bool try_evict_one_inactive_read(evict_reason = evict_reason::manual);\n\n    /// Clear all inactive reads.\n    void clear_inactive_reads();\n\n    /// Evict all inactive reads the belong to the table designated by the id.\n    /// If a range is provided, only inactive reads whose range overlaps with the\n    /// range are evicted.\n    /// The range of the inactive read is provided in register_inactive_read().\n    /// If the range for an inactive read was not provided, all reads for the\n    /// table are evicted.\n    future<> evict_inactive_reads_for_table(table_id id, const dht::partition_range* range = nullptr) noexcept;\nprivate:\n    // The following two functions are extension points for\n    // future inheriting classes that needs to run some stop\n    // logic just before or just after the current stop logic.\n    virtual future<> stop_ext_pre() {\n        return make_ready_future<>();\n    }\n    virtual future<> stop_ext_post() {\n        return make_ready_future<>();\n    }\npublic:\n    /// Stop the reader_concurrency_semaphore and clear all inactive reads.\n    ///\n    /// Wait on all async background work to complete.\n    future<> stop() noexcept;\n\n    const stats& get_stats() const {\n        return _stats;\n    }\n\n    stats& get_stats() {\n        return _stats;\n    }\n\n    /// Make an admitted permit\n    ///\n    /// The permit is already in an admitted state after being created, this\n    /// method includes waiting for admission.\n    /// The permit is associated with a schema, which is the schema of the table\n    /// the read is executed against, and the operation name, which should be a\n    /// name such that we can identify the operation which created this permit.\n    /// Ideally this should be a unique enough name that we not only can identify\n    /// the kind of read, but the exact code-path that was taken.\n    ///\n    /// Some permits cannot be associated with any table, so passing nullptr as\n    /// the schema parameter is allowed.\n    future<reader_permit> obtain_permit(schema_ptr schema, const char* const op_name, size_t memory, db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr);\n    future<reader_permit> obtain_permit(schema_ptr schema, sstring&& op_name, size_t memory, db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr);\n\n    /// Make a tracking only permit\n    ///\n    /// The permit is not admitted. It is intended for reads that bypass the\n    /// normal concurrency control, but whose resource usage we still want to\n    /// keep track of, as part of that concurrency control.\n    /// The permit is associated with a schema, which is the schema of the table\n    /// the read is executed against, and the operation name, which should be a\n    /// name such that we can identify the operation which created this permit.\n    /// Ideally this should be a unique enough name that we not only can identify\n    /// the kind of read, but the exact code-path that was taken.\n    ///\n    /// Some permits cannot be associated with any table, so passing nullptr as\n    /// the schema parameter is allowed.\n    reader_permit make_tracking_only_permit(schema_ptr schema, const char* const op_name, db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr);\n    reader_permit make_tracking_only_permit(schema_ptr schema, sstring&& op_name, db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr);\n\n    /// Run the function through the semaphore's execution stage with an admitted permit\n    ///\n    /// First a permit is obtained via the normal admission route, as if\n    /// it was created  with \\ref obtain_permit(), then func is enqueued to be\n    /// run by the semaphore's execution loop. This emulates an execution stage,\n    /// as it allows batching multiple funcs to be run together. Unlike an\n    /// execution stage, with_permit() accepts a type-erased function, which\n    /// allows for more flexibility in what functions are batched together.\n    /// Use only functions that share most of their code to benefit from the\n    /// instruction-cache warm-up!\n    ///\n    /// The permit is associated with a schema, which is the schema of the table\n    /// the read is executed against, and the operation name, which should be a\n    /// name such that we can identify the operation which created this permit.\n    /// Ideally this should be a unique enough name that we not only can identify\n    /// the kind of read, but the exact code-path that was taken.\n    ///\n    /// Some permits cannot be associated with any table, so passing nullptr as\n    /// the schema parameter is allowed.\n    future<> with_permit(schema_ptr schema, const char* const op_name, size_t memory, db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr, read_func func);\n\n    /// Run the function through the semaphore's execution stage with a pre-admitted permit\n    ///\n    /// Same as \\ref with_permit(), but it uses an already admitted\n    /// permit. Should only be used when a permit is already readily\n    /// available, e.g. when resuming a saved read. Using\n    /// \\ref obtain_permit(), then \\ref with_ready_permit() is less\n    /// optimal then just using \\ref with_permit().\n    future<> with_ready_permit(reader_permit permit, read_func func);\n\n    /// Set the total resources of the semaphore to \\p r.\n    ///\n    /// After this call, \\ref initial_resources() will reflect the new value.\n    /// Available resources will be adjusted by the delta.\n    void set_resources(resources r);\n\n    const resources initial_resources() const {\n        return _initial_resources;\n    }\n\n    bool is_unlimited() const {\n        return _initial_resources == reader_resources{std::numeric_limits<int>::max(), std::numeric_limits<ssize_t>::max()};\n    }\n\n    const resources available_resources() const {\n        return _resources;\n    }\n\n    const resources consumed_resources() const {\n        return _initial_resources - _resources;\n    }\n\n    void broken(std::exception_ptr ex = {});\n\n    /// Dump diagnostics printout\n    ///\n    /// Use max-lines to cap the number of (permit) lines in the report.\n    /// Use 0 for unlimited.\n    std::string dump_diagnostics(unsigned max_lines = 0) const;\n\n    void set_max_queue_length(size_t size) {\n        _max_queue_length = size;\n    }\n\n    uint64_t active_reads() const noexcept {\n        return _stats.current_permits - _stats.inactive_reads - _stats.waiters;\n    }\n\n    void foreach_permit(noncopyable_function<void(const reader_permit::impl&)> func) const;\n    void foreach_permit(noncopyable_function<void(const reader_permit&)> func) const;\n\n    uintptr_t get_blessed_permit() const noexcept { return reinterpret_cast<uintptr_t>(_blessed_permit); }\n};\n"
        },
        {
          "name": "reader_concurrency_semaphore_group.cc",
          "type": "blob",
          "size": 4.025390625,
          "content": "/*\n * Copyright (C) 2021-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"reader_concurrency_semaphore_group.hh\"\n\n// Calling adjust is serialized since 2 adjustments can't happen simultaneously,\n// if they did the behaviour would be undefined.\nfuture<> reader_concurrency_semaphore_group::adjust() {\n    return with_semaphore(_operations_serializer, 1, [this] () {\n        ssize_t distributed_memory = 0;\n        for (auto& [sg, wsem] : _semaphores) {\n            const ssize_t memory_share = std::floor((double(wsem.weight) / double(_total_weight)) * _total_memory);\n            wsem.sem.set_resources({_max_concurrent_reads, memory_share});\n            distributed_memory += memory_share;\n        }\n        // Slap the remainder on one of the semaphores.\n        // This will be a few bytes, doesn't matter where we add it.\n        auto& sem = _semaphores.begin()->second.sem;\n        sem.set_resources(sem.initial_resources() + reader_resources{0, _total_memory - distributed_memory});\n    });\n}\n\n// The call to change_weight is serialized as a consequence of the call to adjust.\nfuture<> reader_concurrency_semaphore_group::change_weight(weighted_reader_concurrency_semaphore& sem, size_t new_weight) {\n    auto diff = new_weight - sem.weight;\n    if (diff) {\n        sem.weight += diff;\n        _total_weight += diff;\n        return adjust();\n    }\n    return make_ready_future<>();\n}\n\nfuture<> reader_concurrency_semaphore_group::wait_adjust_complete() {\n    return with_semaphore(_operations_serializer, 1, [] {\n        return make_ready_future<>();\n    });\n}\n\nfuture<> reader_concurrency_semaphore_group::stop() noexcept {\n    return parallel_for_each(_semaphores, [] (auto&& item) {\n        return item.second.sem.stop();\n    }).then([this] {\n        _semaphores.clear();\n    });\n}\n\nreader_concurrency_semaphore& reader_concurrency_semaphore_group::get(scheduling_group sg) {\n    return _semaphores.at(sg).sem;\n}\nreader_concurrency_semaphore* reader_concurrency_semaphore_group::get_or_null(scheduling_group sg) {\n    auto it = _semaphores.find(sg);\n    if (it == _semaphores.end()) {\n        return nullptr;\n    } else {\n        return &(it->second.sem);\n    }\n}\nreader_concurrency_semaphore& reader_concurrency_semaphore_group::add_or_update(scheduling_group sg, size_t shares) {\n    auto result = _semaphores.try_emplace(\n            sg,\n            0,\n            _max_concurrent_reads,\n            _name_prefix ? format(\"{}_{}\", *_name_prefix, sg.name()) : sg.name(),\n            _max_queue_length,\n            _serialize_limit_multiplier,\n            _kill_limit_multiplier,\n            _cpu_concurrency\n        );\n    auto&& it = result.first;\n    // since we serialize all group changes this change wait will be queues and no further operations\n    // will be executed until this adjustment ends.\n    (void)change_weight(it->second, shares);\n    return it->second.sem;\n}\n\nfuture<> reader_concurrency_semaphore_group::remove(scheduling_group sg) {\n    auto node_handle =  _semaphores.extract(sg);\n    if (!node_handle.empty()) {\n        weighted_reader_concurrency_semaphore& sem = node_handle.mapped();\n        return sem.sem.stop().then([this, &sem] {\n            return change_weight(sem, 0);\n        }).finally([node_handle = std::move(node_handle)] () {\n            // this holds on to the node handle until we destroy it only after the semaphore\n            // is stopped properly.\n        });\n    }\n    return make_ready_future();\n}\n\nsize_t reader_concurrency_semaphore_group::size() {\n    return _semaphores.size();\n}\n\nvoid reader_concurrency_semaphore_group::foreach_semaphore(std::function<void(scheduling_group, reader_concurrency_semaphore&)> func) {\n    for (auto& [sg, wsem] : _semaphores) {\n        func(sg, wsem.sem);\n    }\n}\n\nfuture<>\nreader_concurrency_semaphore_group::foreach_semaphore_async(std::function<future<> (scheduling_group, reader_concurrency_semaphore&)> func) {\n    auto units = co_await get_units(_operations_serializer, 1);\n    for (auto& [sg, wsem] : _semaphores) {\n        co_await func(sg, wsem.sem);\n    }\n}\n"
        },
        {
          "name": "reader_concurrency_semaphore_group.hh",
          "type": "blob",
          "size": 3.994140625,
          "content": "/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n/*\n * Copyright (C) 2021-present ScyllaDB\n */\n\n#pragma once\n\n#include <unordered_map>\n#include <optional>\n#include \"reader_concurrency_semaphore.hh\"\n#include <boost/range/adaptor/map.hpp>\n#include <boost/range/numeric.hpp>\n\n// The reader_concurrency_semaphore_group is a group of semaphores that shares a common pool of memory,\n// the memory is dynamically divided between them according to a relative slice of shares each semaphore\n// is given.\n// All of the mutating operations on the group are asynchronic and serialized. The semaphores are created\n// and managed by the group.\n\nclass reader_concurrency_semaphore_group {\n    size_t _total_memory;\n    size_t _total_weight;\n    size_t _max_concurrent_reads;\n    size_t _max_queue_length;\n    utils::updateable_value<uint32_t> _serialize_limit_multiplier;\n    utils::updateable_value<uint32_t> _kill_limit_multiplier;\n    utils::updateable_value<uint32_t> _cpu_concurrency;\n\n    friend class database_test_wrapper;\n\n    struct weighted_reader_concurrency_semaphore {\n        size_t weight;\n        ssize_t memory_share;\n        reader_concurrency_semaphore sem;\n        weighted_reader_concurrency_semaphore(size_t shares, int count, sstring name, size_t max_queue_length,\n                utils::updateable_value<uint32_t> serialize_limit_multiplier,\n                utils::updateable_value<uint32_t> kill_limit_multiplier,\n                utils::updateable_value<uint32_t> cpu_concurrency)\n                : weight(shares)\n                , memory_share(0)\n                , sem(utils::updateable_value(count), 0, name, max_queue_length, std::move(serialize_limit_multiplier), std::move(kill_limit_multiplier),\n                      std::move(cpu_concurrency), reader_concurrency_semaphore::register_metrics::yes) {}\n    };\n\n    std::unordered_map<scheduling_group, weighted_reader_concurrency_semaphore> _semaphores;\n    seastar::semaphore _operations_serializer;\n    std::optional<sstring> _name_prefix;\n\n    future<> change_weight(weighted_reader_concurrency_semaphore& sem, size_t new_weight);\n\npublic:\n    reader_concurrency_semaphore_group(size_t memory, size_t max_concurrent_reads, size_t max_queue_length,\n            utils::updateable_value<uint32_t> serialize_limit_multiplier,\n            utils::updateable_value<uint32_t> kill_limit_multiplier,\n            utils::updateable_value<uint32_t> cpu_concurrency,\n            std::optional<sstring> name_prefix = std::nullopt)\n            : _total_memory(memory)\n            , _total_weight(0)\n            , _max_concurrent_reads(max_concurrent_reads)\n            ,  _max_queue_length(max_queue_length)\n            , _serialize_limit_multiplier(std::move(serialize_limit_multiplier))\n            , _kill_limit_multiplier(std::move(kill_limit_multiplier))\n            , _cpu_concurrency(std::move(cpu_concurrency))\n            , _operations_serializer(1)\n            , _name_prefix(std::move(name_prefix)) { }\n\n    ~reader_concurrency_semaphore_group() {\n        assert(_semaphores.empty());\n    }\n    future<> adjust();\n    future<> wait_adjust_complete();\n\n    future<> stop() noexcept;\n    reader_concurrency_semaphore& get(scheduling_group sg);\n    reader_concurrency_semaphore* get_or_null(scheduling_group sg);\n    reader_concurrency_semaphore& add_or_update(scheduling_group sg, size_t shares);\n    future<> remove(scheduling_group sg);\n    size_t size();\n    void foreach_semaphore(std::function<void(scheduling_group, reader_concurrency_semaphore&)> func);\n\n    future<> foreach_semaphore_async(std::function<future<> (scheduling_group, reader_concurrency_semaphore&)> func);\n\n    auto sum_read_concurrency_sem_var(std::invocable<reader_concurrency_semaphore&> auto member) {\n        using ret_type = std::invoke_result_t<decltype(member), reader_concurrency_semaphore&>;\n        return boost::accumulate(_semaphores | boost::adaptors::map_values | boost::adaptors::transformed([=] (weighted_reader_concurrency_semaphore& wrcs) { return std::invoke(member, wrcs.sem); }), ret_type(0));\n    }\n};\n"
        },
        {
          "name": "reader_permit.hh",
          "type": "blob",
          "size": 9.908203125,
          "content": "/*\n * Copyright (C) 2019-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <seastar/util/optimized_optional.hh>\n#include \"seastarx.hh\"\n\n#include \"db/timeout_clock.hh\"\n#include \"schema/schema_fwd.hh\"\n#include \"tracing/trace_state.hh\"\n\nnamespace query {\n\nstruct max_result_size;\n\n}\n\nnamespace seastar {\n    class file;\n} // namespace seastar\n\nstruct reader_resources {\n    int count = 0;\n    ssize_t memory = 0;\n\n    static reader_resources with_memory(ssize_t memory) { return reader_resources(0, memory); }\n\n    reader_resources() = default;\n\n    reader_resources(int count, ssize_t memory)\n        : count(count)\n        , memory(memory) {\n    }\n\n    reader_resources operator-(const reader_resources& other) const {\n        return reader_resources{count - other.count, memory - other.memory};\n    }\n\n    reader_resources& operator-=(const reader_resources& other) {\n        count -= other.count;\n        memory -= other.memory;\n        return *this;\n    }\n\n    reader_resources operator+(const reader_resources& other) const {\n        return reader_resources{count + other.count, memory + other.memory};\n    }\n\n    reader_resources& operator+=(const reader_resources& other) {\n        count += other.count;\n        memory += other.memory;\n        return *this;\n    }\n\n    bool non_zero() const {\n        return count > 0 || memory > 0;\n    }\n};\n\ninline bool operator==(const reader_resources& a, const reader_resources& b) {\n    return a.count == b.count && a.memory == b.memory;\n}\n\nclass reader_concurrency_semaphore;\n\n/// A permit for a specific read.\n///\n/// Used to track the read's resource consumption. Use `consume_memory()` to\n/// register memory usage, which returns a `resource_units` RAII object that\n/// should be held onto while the respective resources are in use.\nclass reader_permit {\n    friend class reader_concurrency_semaphore;\n    friend class tracking_allocator_base;\n\npublic:\n    class resource_units;\n    class need_cpu_guard;\n    class awaits_guard;\n\n    enum class state {\n        waiting_for_admission,\n        waiting_for_memory,\n        waiting_for_execution,\n        active,\n        active_need_cpu,\n        active_await,\n        inactive,\n        evicted,\n    };\n\n    class impl;\n\nprivate:\n    shared_ptr<impl> _impl;\n\nprivate:\n    reader_permit() = default;\n    reader_permit(shared_ptr<impl>);\n    explicit reader_permit(reader_concurrency_semaphore& semaphore, schema_ptr schema, std::string_view op_name,\n            reader_resources base_resources, db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr);\n    explicit reader_permit(reader_concurrency_semaphore& semaphore, schema_ptr schema, sstring&& op_name,\n            reader_resources base_resources, db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr);\n\n    reader_permit::impl& operator*() { return *_impl; }\n    reader_permit::impl* operator->() { return _impl.get(); }\n\n    void mark_need_cpu() noexcept;\n\n    void mark_not_need_cpu() noexcept;\n\n    void mark_awaits() noexcept;\n\n    void mark_not_awaits() noexcept;\n\n    operator bool() const { return bool(_impl); }\n\n    friend class optimized_optional<reader_permit>;\n\n    void consume(reader_resources res);\n\n    void signal(reader_resources res);\n\npublic:\n    ~reader_permit();\n\n    reader_permit(const reader_permit&) = default;\n    reader_permit(reader_permit&&) = default;\n\n    reader_permit& operator=(const reader_permit&) = default;\n    reader_permit& operator=(reader_permit&&) = default;\n\n    bool operator==(const reader_permit& o) const {\n        return _impl == o._impl;\n    }\n\n    reader_concurrency_semaphore& semaphore();\n\n    const schema_ptr& get_schema() const;\n    std::string_view get_op_name() const;\n    state get_state() const;\n\n    bool needs_readmission() const;\n\n    // Call only when needs_readmission() = true.\n    future<> wait_readmission();\n\n    resource_units consume_memory(size_t memory = 0);\n\n    resource_units consume_resources(reader_resources res);\n\n    future<resource_units> request_memory(size_t memory);\n\n    reader_resources consumed_resources() const;\n\n    reader_resources base_resources() const;\n\n    void release_base_resources() noexcept;\n\n    sstring description() const;\n\n    db::timeout_clock::time_point timeout() const noexcept;\n\n    void set_timeout(db::timeout_clock::time_point timeout) noexcept;\n\n    const tracing::trace_state_ptr& trace_state() const noexcept;\n\n    void set_trace_state(tracing::trace_state_ptr trace_ptr) noexcept;\n\n    // If the read was aborted, throw the exception the read was aborted with.\n    // Otherwise no-op.\n    void check_abort();\n\n    query::max_result_size max_result_size() const;\n    void set_max_result_size(query::max_result_size);\n\n    void on_start_sstable_read() noexcept;\n    void on_finish_sstable_read() noexcept;\n\n    uintptr_t id() { return reinterpret_cast<uintptr_t>(_impl.get()); }\n};\n\nusing reader_permit_opt = optimized_optional<reader_permit>;\n\nclass reader_permit::resource_units {\n    reader_permit _permit;\n    reader_resources _resources;\n\n    friend class reader_permit;\n    friend class reader_concurrency_semaphore;\nprivate:\n    class already_consumed_tag {};\n    resource_units(reader_permit permit, reader_resources res, already_consumed_tag);\n    resource_units(reader_permit permit, reader_resources res);\npublic:\n    resource_units(const resource_units&) = delete;\n    resource_units(resource_units&&) noexcept;\n    ~resource_units();\n    resource_units& operator=(const resource_units&) = delete;\n    resource_units& operator=(resource_units&&) noexcept;\n    void add(resource_units&& o);\n    void reset_to(reader_resources res);\n    void reset_to_zero() noexcept;\n    reader_permit permit() const { return _permit; }\n    reader_resources resources() const { return _resources; }\n};\n\n/// Mark a permit as needing CPU.\n///\n/// Conceptually, a permit is considered as needing CPU, when at least one reader\n/// associated with it has an ongoing foreground operation initiated by\n/// its consumer. E.g. a pending `fill_buffer()` call.\n/// This class is an RAII need_cpu marker meant to be used by keeping it alive\n/// while the reader is in need of CPU.\nclass reader_permit::need_cpu_guard {\n    reader_permit_opt _permit;\npublic:\n    explicit need_cpu_guard(reader_permit permit) noexcept : _permit(std::move(permit)) {\n        _permit->mark_need_cpu();\n    }\n    need_cpu_guard(need_cpu_guard&&) noexcept = default;\n    need_cpu_guard(const need_cpu_guard&) = delete;\n    ~need_cpu_guard() {\n        if (_permit) {\n            _permit->mark_not_need_cpu();\n        }\n    }\n    need_cpu_guard& operator=(need_cpu_guard&&) = delete;\n    need_cpu_guard& operator=(const need_cpu_guard&) = delete;\n};\n\n/// Mark a permit as awaiting I/O or an operation running on a remote shard.\n///\n/// Conceptually, a permit is considered awaiting, when at least one reader\n/// associated with it is waiting on I/O or a remote shard as part of a\n/// foreground operation initiated by its consumer. E.g. an sstable reader\n/// waiting on a disk read as part of its `fill_buffer()` call.\n/// This class is an RAII awaits marker meant to be used by keeping it alive\n/// until said awaited event completes.\nclass reader_permit::awaits_guard {\n    reader_permit_opt _permit;\npublic:\n    explicit awaits_guard(reader_permit permit) noexcept : _permit(std::move(permit)) {\n        _permit->mark_awaits();\n    }\n    awaits_guard(awaits_guard&&) noexcept = default;\n    awaits_guard(const awaits_guard&) = delete;\n    ~awaits_guard() {\n        if (_permit) {\n            _permit->mark_not_awaits();\n        }\n    }\n    awaits_guard& operator=(awaits_guard&&) = delete;\n    awaits_guard& operator=(const awaits_guard&) = delete;\n};\n\ntemplate <typename Char>\ntemporary_buffer<Char> make_tracked_temporary_buffer(temporary_buffer<Char> buf, reader_permit::resource_units units) {\n    return temporary_buffer<Char>(buf.get_write(), buf.size(), make_object_deleter(buf.release(), std::move(units)));\n}\n\ninline temporary_buffer<char> make_new_tracked_temporary_buffer(size_t size, reader_permit& permit) {\n    auto buf = temporary_buffer<char>(size);\n    return temporary_buffer<char>(buf.get_write(), buf.size(), make_object_deleter(buf.release(), permit.consume_memory(size)));\n}\n\nfile make_tracked_file(file f, reader_permit p);\n\nclass tracking_allocator_base {\n    reader_permit _permit;\nprotected:\n    tracking_allocator_base(reader_permit permit) noexcept : _permit(std::move(permit)) { }\n    void consume(size_t memory) {\n        _permit.consume(reader_resources::with_memory(memory));\n    }\n    void signal(size_t memory) {\n        _permit.signal(reader_resources::with_memory(memory));\n    }\n};\n\ntemplate <typename T>\nclass tracking_allocator : public tracking_allocator_base {\npublic:\n    using value_type = T;\n    using propagate_on_container_move_assignment = std::true_type;\n    using is_always_equal = std::false_type;\n\nprivate:\n    std::allocator<T> _alloc;\n\npublic:\n    tracking_allocator(reader_permit permit) noexcept : tracking_allocator_base(std::move(permit)) { }\n\n    T* allocate(size_t n) {\n        auto p = _alloc.allocate(n);\n        try {\n            consume(n * sizeof(T));\n        } catch (...) {\n            _alloc.deallocate(p, n);\n            throw;\n        }\n        return p;\n    }\n    void deallocate(T* p, size_t n) {\n        _alloc.deallocate(p, n);\n        if (n) {\n            signal(n * sizeof(T));\n        }\n    }\n\n    template <typename U>\n    friend bool operator==(const tracking_allocator<U>& a, const tracking_allocator<U>& b);\n};\n\ntemplate <typename T>\nbool operator==(const tracking_allocator<T>& a, const tracking_allocator<T>& b) {\n    return a._semaphore == b._semaphore;\n}\n\ntemplate <> struct fmt::formatter<reader_permit::state> : fmt::formatter<string_view> {\n    auto format(reader_permit::state, fmt::format_context& ctx) const -> decltype(ctx.out());\n};\ntemplate <> struct fmt::formatter<reader_resources> : fmt::formatter<string_view> {\n    auto format(const reader_resources&, fmt::format_context& ctx) const -> decltype(ctx.out());\n};\n"
        },
        {
          "name": "readers",
          "type": "tree",
          "content": null
        },
        {
          "name": "real_dirty_memory_accounter.hh",
          "type": "blob",
          "size": 5.9697265625,
          "content": "/*\n * Copyright (C) 2018-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"replica/memtable.hh\"\n#include \"row_cache.hh\"\n#include \"replica/dirty_memory_manager.hh\"\n\n// # Flush control overview\n//\n// It is good for memtables to grow as big as possible before they are flushed\n// to disk, because this reduces the number of sstables, which in turn reduces\n// the number of disk reads on queries and/or reduces the necessary compaction\n// work.\n//\n// On the other hand, the flush should start early enough to leave enough space\n// in RAM for new data incoming during the flush. Otherwise RAM could be\n// overfilled and the database would have to be throttled until some space is\n// freed.\n//\n// It is good for a memtable flush to happen as slow as possible, because it's\n// a non-interactive task, which should take as little resources from\n// interactive tasks as possible.\n//\n// On the other hand, the flush has to progress fast enough to keep up with\n// incoming writes, otherwise the size of data in RAM will keep growing\n// until RAM is overfilled.\n//\n// The balance of the above is kept by dirty_memory_manager and flush_controller.\n// They attempt to make flushes as delayed and slow as possible without risking\n// an OOM situation.\n//\n// # Flush control implementation\n//\n// Flush delay and speed is based on some formulas involving total available RAM,\n// total (\"real\") memtable memory and \"unspooled\" (not flushed to disk yet) memtable\n// memory. See dirty_memory_manager for details.\n//\n// (\"Dirty\" is a term borrowed from general caching terminology, which usually means\n// cache entries which were modified and have to be written back before being discarded.\n// In Scylla, \"dirty memory\" simply means \"memory taken up by memtable data\").\n//\n// (In some context the implementation might find it more natural to talk about\n// \"spooled\" memory instead of \"unspooled\". \"Spooled\" is just the difference between\n// \"real\" and \"unspooled\").\n//\n// Every memtable is kept in its own LSA region which tracks exact (the allocator is\n// the source of truth about memory usage) \"real\" changes for that memtable.\n// While the memtable is active, its \"unspooled\" is (obviously) equal to its \"real\".\n//\n// Once flush starts, remaining \"unspooled\" is tracked by flush_memory_accounter.\n// As flush_reader reads a memtable and passes its data to an sstable writer,\n// it asks flush_memory_accounter to decrement \"unspooled\" memory counters.\n//\n// The accounting by flush_memory_accounter is not exact (for example it\n// doesn't account the memtable tree nodes, only the contents stored there) but\n// it doesn't have to. It is only used by flush control formulas, so it only\n// has to be accurate enough for the heurisitcs to work.\n//\n// When memtable flush is finished, the amount of \"unspooled\" memory is corrected\n// from its inexact value to now-exact 0.\n//\n// After a memtable flush finishes, the memtable has to be merged into the\n// cache, to update or invalidate existing cache entries, so combined RAM data\n// stays up to date with sstables after the memtable disappears.\n//\n// During this merge, \"real\" decreases, and this has to be accounted for.\n// Unfortunately we can't rely on LSA's counters (which are the source of\n// truth for memory accounting) for \"real\", because the merge\n// requires the memtable LSA region to be merged into the cache LSA region.\n// As soon as the merge starts, all memtable data is considered a part of the cache\n// by the LSA.\n//\n// So, similarly as with flush_memory_accounter earlier, we have real_dirty_memory_accounter\n// whose job is to incrementally decrease \"real\" as data is merged into the cache.\n// This might also be slightly inexact.\n//\n// As row_cache::update() progresses, it estimates the amount of processed memory\n// as well as it can, and asks real_dirty_memory_accounter to \"unpin\" it from \"real\".\n//\n// Once the merge finishes, the \"real\" for the merged memtable is reduced from\n// its slightly inexact value to now-exact 0.\n//\nclass real_dirty_memory_accounter {\n    replica::dirty_memory_manager& _mgr;\n    cache_tracker& _tracker;\n    uint64_t _bytes;\n    uint64_t _uncommitted = 0;\npublic:\n    real_dirty_memory_accounter(replica::dirty_memory_manager& mgr, cache_tracker& tracker, size_t size);\n    real_dirty_memory_accounter(replica::memtable& m, cache_tracker& tracker);\n    ~real_dirty_memory_accounter();\n    real_dirty_memory_accounter(real_dirty_memory_accounter&& c);\n    real_dirty_memory_accounter(const real_dirty_memory_accounter& c) = delete;\n    // Needs commit() to take effect, or when this object is destroyed.\n    void unpin_memory(uint64_t bytes) { _uncommitted += bytes; }\n    void commit();\n};\n\ninline\nreal_dirty_memory_accounter::real_dirty_memory_accounter(replica::dirty_memory_manager& mgr, cache_tracker& tracker, size_t size)\n    : _mgr(mgr)\n    , _tracker(tracker)\n    , _bytes(size) {\n    _mgr.pin_real_dirty_memory(_bytes);\n}\n\ninline\nreal_dirty_memory_accounter::real_dirty_memory_accounter(replica::memtable& m, cache_tracker& tracker)\n    : real_dirty_memory_accounter(m.get_dirty_memory_manager(), tracker, m.occupancy().used_space())\n{ }\n\ninline\nreal_dirty_memory_accounter::~real_dirty_memory_accounter() {\n    _mgr.unpin_real_dirty_memory(_bytes);\n}\n\ninline\nreal_dirty_memory_accounter::real_dirty_memory_accounter(real_dirty_memory_accounter&& c)\n    : _mgr(c._mgr), _tracker(c._tracker), _bytes(c._bytes), _uncommitted(c._uncommitted) {\n    c._bytes = 0;\n    c._uncommitted = 0;\n}\n\ninline\nvoid real_dirty_memory_accounter::commit() {\n    auto bytes = std::exchange(_uncommitted, 0);\n    // this should never happen - if it does it is a bug. But we'll try to recover and log\n    // instead of asserting. Once it happens, though, it can keep happening until the update is\n    // done. So using metrics is better-suited than printing to the logs\n    if (bytes > _bytes) {\n        _tracker.pinned_dirty_memory_overload(bytes - _bytes);\n    }\n    auto delta = std::min(bytes, _bytes);\n    _bytes -= delta;\n    _mgr.unpin_real_dirty_memory(delta);\n}\n"
        },
        {
          "name": "redis",
          "type": "tree",
          "content": null
        },
        {
          "name": "release.cc",
          "type": "blob",
          "size": 1.75390625,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"utils/assert.hh\"\n#include \"version.hh\"\n#include \"build_mode.hh\"\n\n#include <boost/algorithm/string/classification.hpp>\n#include <boost/algorithm/string/split.hpp>\n\n#include <seastar/core/format.hh>\n\nstatic const char scylla_product_str[] = SCYLLA_PRODUCT;\nstatic const char scylla_version_str[] = SCYLLA_VERSION;\nstatic const char scylla_release_str[] = SCYLLA_RELEASE;\nstatic const char scylla_build_mode_str[] = SCYLLA_BUILD_MODE_STR;\n\nstd::string scylla_version()\n{\n    return seastar::format(\"{}-{}\", scylla_version_str, scylla_release_str);\n}\n\nstd::string scylla_build_mode()\n{\n    return seastar::format(\"{}\", scylla_build_mode_str);\n}\n\nstd::string doc_link(std::string_view url_tail) {\n    const std::string_view product = scylla_product_str;\n    const std::string_view version = scylla_version_str;\n\n    const auto prefix = product == \"scylla-enterprise\" ? \"enterprise\" : \"opensource\";\n\n    std::string branch = product == \"scylla-enterprise\" ? \"enterprise\" : \"master\";\n    if (!version.ends_with(\"~dev\")) {\n        std::vector<std::string> components;\n        boost::split(components, version, boost::algorithm::is_any_of(\".\"));\n        // Version is compiled into the binary, testing will step on this immediately.\n        SCYLLA_ASSERT(components.size() >= 2);\n        branch = fmt::format(\"branch-{}.{}\", components[0], components[1]);\n    }\n\n    return fmt::format(\"https://{}.docs.scylladb.com/{}/{}\", prefix, branch, url_tail);\n}\n\n// get the version number into writeable memory, so we can grep for it if we get a core dump\nstd::string version_stamp_for_core\n    = \"VERSION VERSION VERSION $Id: \" + scylla_version() + \" $ VERSION VERSION VERSION\";\n"
        },
        {
          "name": "release.hh",
          "type": "blob",
          "size": 0.52734375,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <string>\n#include \"build_mode.hh\"\n\nstd::string scylla_version();\n\nstd::string scylla_build_mode();\n\n// Generate the documentation link, which is appropriate for the current version\n// and product (open-source or enterprise).\n//\n// Will return a documentation URL like this:\n//      https://${product}.docs.scylladb.com/${branch}/${url_tail}\n//\nstd::string doc_link(std::string_view url_tail);\n"
        },
        {
          "name": "reloc",
          "type": "tree",
          "content": null
        },
        {
          "name": "repair",
          "type": "tree",
          "content": null
        },
        {
          "name": "replica",
          "type": "tree",
          "content": null
        },
        {
          "name": "reversibly_mergeable.hh",
          "type": "blob",
          "size": 1.474609375,
          "content": "/*\n * Copyright (C) 2016-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"utils/allocation_strategy.hh\"\n\n//\n// ~~ Definitions ~~\n//\n// Mergeable type is a type which has an associated \"apply\" binary operation (T x T -> T)\n// which forms a commutative semigroup with instances of that type.\n//\n// ReversiblyMergeable type is a Mergeable type which has two binary operations associated,\n// \"apply_reversibly\" and \"revert\", both working on objects of that type (T x T -> T x T)\n// with the following properties:\n//\n//   apply_reversibly(x, y) = (x', y')\n//   revert(x', y') = (x'', y'')\n//\n//   x'  = apply(x, y)\n//   x'' = x\n//   apply(x'', y'') = apply(x, y)\n//\n// Note that it is not guaranteed that y'' = y and the state of y' is unspecified.\n//\n// ~~ API ~~\n//\n// \"apply_reversibly\" and \"revert\" are usually implemented as instance methods or functions\n// mutating both arguments to store the result of the operation in them.\n//\n// \"revert\" is not allowed to throw. If \"apply_reversibly\" throws the objects on which it operates\n// are left in valid states, with guarantees the same as if a successful apply_reversibly() was\n// followed by revert().\n//\n\n\ntemplate<typename T>\nstruct default_reversible_applier {\n    void operator()(T& dst, T& src) const {\n        dst.apply_reversibly(src);\n    }\n};\n\ntemplate<typename T>\nstruct default_reverter {\n    void operator()(T& dst, T& src) const noexcept {\n        dst.revert(src);\n    }\n};\n"
        },
        {
          "name": "row_cache.cc",
          "type": "blob",
          "size": 62.5732421875,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"row_cache.hh\"\n#include <fmt/ranges.h>\n#include <seastar/core/memory.hh>\n#include <seastar/core/future-util.hh>\n#include <seastar/core/metrics.hh>\n#include <seastar/core/thread.hh>\n#include <seastar/core/coroutine.hh>\n#include <seastar/coroutine/as_future.hh>\n#include <seastar/util/defer.hh>\n#include \"replica/memtable.hh\"\n#include <boost/version.hpp>\n#include <sys/sdt.h>\n#include \"read_context.hh\"\n#include \"real_dirty_memory_accounter.hh\"\n#include \"readers/delegating_v2.hh\"\n#include \"readers/forwardable_v2.hh\"\n#include \"readers/nonforwardable.hh\"\n#include \"cache_mutation_reader.hh\"\n#include \"partition_snapshot_reader.hh\"\n#include \"clustering_key_filter.hh\"\n#include \"utils/assert.hh\"\n#include \"utils/updateable_value.hh\"\n\nnamespace cache {\n\nlogging::logger clogger(\"cache\");\n\n}\n\nusing namespace std::chrono_literals;\nusing namespace cache;\n\nstatic schema_ptr to_query_domain(const query::partition_slice& slice, schema_ptr table_domain_schema) {\n    if (slice.is_reversed()) [[unlikely]] {\n        return table_domain_schema->make_reversed();\n    }\n    return table_domain_schema;\n}\n\nmutation_reader\nrow_cache::create_underlying_reader(read_context& ctx, mutation_source& src, const dht::partition_range& pr) {\n    schema_ptr entry_schema = to_query_domain(ctx.slice(), _schema);\n    auto reader = src.make_reader_v2(entry_schema, ctx.permit(), pr, ctx.slice(), ctx.trace_state(), streamed_mutation::forwarding::yes);\n    ctx.on_underlying_created();\n    return reader;\n}\n\nstatic thread_local mutation_application_stats dummy_app_stats;\nstatic thread_local utils::updateable_value<double> dummy_index_cache_fraction(1.0);\n\ncache_tracker::cache_tracker()\n    : cache_tracker(dummy_index_cache_fraction, dummy_app_stats, register_metrics::no)\n{}\n\ncache_tracker::cache_tracker(utils::updateable_value<double> index_cache_fraction, register_metrics with_metrics)\n    : cache_tracker(std::move(index_cache_fraction), dummy_app_stats, with_metrics)\n{}\n\nstatic thread_local cache_tracker* current_tracker;\n\ncache_tracker::cache_tracker(utils::updateable_value<double> index_cache_fraction, mutation_application_stats& app_stats, register_metrics with_metrics)\n    : _garbage(_region, this, app_stats)\n    , _memtable_cleaner(_region, nullptr, app_stats)\n    , _app_stats(app_stats)\n    , _index_cache_fraction(std::move(index_cache_fraction))\n{\n    if (with_metrics) {\n        setup_metrics();\n    }\n\n    _region.make_evictable([this] {\n        return with_allocator(_region.allocator(), [this] () noexcept {\n            if (!_garbage.empty()) {\n                _garbage.clear_some();\n                return memory::reclaiming_result::reclaimed_something;\n            }\n            if (!_memtable_cleaner.empty()) {\n                _memtable_cleaner.clear_some();\n                return memory::reclaiming_result::reclaimed_something;\n            }\n            current_tracker = this;\n\n            // Cache replacement algorithm:\n            //\n            // if sstable index caches occupy more than index_cache_fraction of cache memory:\n            //     evict the least recently used index entry\n            // else:\n            //     evict the least recently used entry (data or index)\n            //\n            // This algorithm has the following good properties:\n            // 1. When index and data entries contend for cache space, it prevents\n            //    index cache from taking more than index_cache_fraction of memory.\n            //    This makes sure that the index cache doesn't catastrophically\n            //    deprive the data cache of memory in small-partition workloads.\n            // 2. Since it doesn't enforce a lower limit on the index space, but only\n            //    the upper limit, the parameter shouldn't require careful balancing.\n            //    In workloads where it makes sense to cache the index (usually: where\n            //    the index is small enough to fit in RAM), setting the fraction to any big number (e.g. 1.0)\n            //    should work well enough. In workloads where it doesn't make sense to cache the index,\n            //    setting the fraction to any small number (e.g. 0.0) should work well enough.\n            //    Setting it to a medium number (something like 0.2) should work well enough\n            //    for both extremes, although it might be suboptimal for non-extremes.\n            // 3. The parameter is trivially live-updateable.\n            //\n            // Perhaps this logic should be encapsulated somewhere else, maybe in `class lru` itself.\n            size_t total_cache_space = _region.occupancy().total_space();\n            size_t index_cache_space = _partition_index_cache_stats.used_bytes + _index_cached_file_stats.cached_bytes;\n            bool should_evict_index = index_cache_space > total_cache_space * _index_cache_fraction.get();\n\n            return _lru.evict(should_evict_index);\n        });\n    });\n}\n\ncache_tracker::~cache_tracker() {\n    clear();\n}\n\nmemory::reclaiming_result cache_tracker::evict_from_lru_shallow() noexcept {\n    return with_allocator(_region.allocator(), [this] () noexcept {\n        current_tracker = this;\n        return _lru.evict_shallow();\n    });\n}\n\nvoid cache_tracker::set_compaction_scheduling_group(seastar::scheduling_group sg) {\n    _memtable_cleaner.set_scheduling_group(sg);\n    _garbage.set_scheduling_group(sg);\n}\n\nnamespace sstables {\nvoid register_index_page_cache_metrics(seastar::metrics::metric_groups&, cached_file_stats&);\nvoid register_index_page_metrics(seastar::metrics::metric_groups&, partition_index_cache_stats&);\n};\n\nvoid\ncache_tracker::setup_metrics() {\n    namespace sm = seastar::metrics;\n    _metrics.add_group(\"cache\", {\n        sm::make_gauge(\"bytes_used\", sm::description(\"current bytes used by the cache out of the total size of memory\"), [this] { return _region.occupancy().used_space(); }),\n        sm::make_gauge(\"bytes_total\", sm::description(\"total size of memory for the cache\"), [this] { return _region.occupancy().total_space(); }),\n        sm::make_counter(\"partition_hits\", sm::description(\"number of partitions needed by reads and found in cache\"), _stats.partition_hits),\n        sm::make_counter(\"partition_misses\", sm::description(\"number of partitions needed by reads and missing in cache\"), _stats.partition_misses),\n        sm::make_counter(\"partition_insertions\", sm::description(\"total number of partitions added to cache\"), _stats.partition_insertions),\n        sm::make_counter(\"row_hits\", sm::description(\"total number of rows needed by reads and found in cache\"), _stats.row_hits),\n        sm::make_counter(\"dummy_row_hits\", sm::description(\"total number of dummy rows touched by reads in cache\"), _stats.dummy_row_hits),\n        sm::make_counter(\"row_misses\", sm::description(\"total number of rows needed by reads and missing in cache\"), _stats.row_misses),\n        sm::make_counter(\"row_insertions\", sm::description(\"total number of rows added to cache\"), _stats.row_insertions),\n        sm::make_counter(\"row_evictions\", sm::description(\"total number of rows evicted from cache\"), _stats.row_evictions),\n        sm::make_counter(\"row_removals\", sm::description(\"total number of invalidated rows\"), _stats.row_removals),\n        sm::make_counter(\"rows_dropped_by_tombstones\", _app_stats.rows_dropped_by_tombstones, sm::description(\"Number of rows dropped in cache by a tombstone write\")),\n        sm::make_counter(\"rows_compacted_with_tombstones\", _app_stats.rows_compacted_with_tombstones, sm::description(\"Number of rows scanned during write of a tombstone for the purpose of compaction in cache\")),\n        sm::make_counter(\"static_row_insertions\", sm::description(\"total number of static rows added to cache\"), _stats.static_row_insertions),\n        sm::make_counter(\"concurrent_misses_same_key\", sm::description(\"total number of operation with misses same key\"), _stats.concurrent_misses_same_key),\n        sm::make_counter(\"partition_merges\", sm::description(\"total number of partitions merged\"), _stats.partition_merges),\n        sm::make_counter(\"partition_evictions\", sm::description(\"total number of evicted partitions\"), _stats.partition_evictions),\n        sm::make_counter(\"partition_removals\", sm::description(\"total number of invalidated partitions\"), _stats.partition_removals),\n        sm::make_counter(\"mispopulations\", sm::description(\"number of entries not inserted by reads\"), _stats.mispopulations),\n        sm::make_gauge(\"partitions\", sm::description(\"total number of cached partitions\"), _stats.partitions),\n        sm::make_gauge(\"rows\", sm::description(\"total number of cached rows\"), _stats.rows),\n        sm::make_counter(\"reads\", sm::description(\"number of started reads\"), _stats.reads),\n        sm::make_counter(\"reads_with_misses\", sm::description(\"number of reads which had to read from sstables\"), _stats.reads_with_misses),\n        sm::make_gauge(\"active_reads\", sm::description(\"number of currently active reads\"), [this] { return _stats.active_reads(); }),\n        sm::make_counter(\"sstable_reader_recreations\", sm::description(\"number of times sstable reader was recreated due to memtable flush\"), _stats.underlying_recreations),\n        sm::make_counter(\"sstable_partition_skips\", sm::description(\"number of times sstable reader was fast forwarded across partitions\"), _stats.underlying_partition_skips),\n        sm::make_counter(\"sstable_row_skips\", sm::description(\"number of times sstable reader was fast forwarded within a partition\"), _stats.underlying_row_skips),\n        sm::make_counter(\"pinned_dirty_memory_overload\", sm::description(\"amount of pinned bytes that we tried to unpin over the limit. This should sit constantly at 0, and any number different than 0 is indicative of a bug\"), _stats.pinned_dirty_memory_overload),\n        sm::make_counter(\"rows_processed_from_memtable\", _stats.rows_processed_from_memtable,\n            sm::description(\"total number of rows in memtables which were processed during cache update on memtable flush\")),\n        sm::make_counter(\"rows_dropped_from_memtable\", _stats.rows_dropped_from_memtable,\n            sm::description(\"total number of rows in memtables which were dropped during cache update on memtable flush\")),\n        sm::make_counter(\"rows_merged_from_memtable\", _stats.rows_merged_from_memtable,\n            sm::description(\"total number of rows in memtables which were merged with existing rows during cache update on memtable flush\")),\n        sm::make_counter(\"range_tombstone_reads\", _stats.range_tombstone_reads,\n            sm::description(\"total amount of range tombstones processed during read\")),\n        sm::make_counter(\"row_tombstone_reads\", _stats.row_tombstone_reads,\n            sm::description(\"total amount of row tombstones processed during read\")),\n        sm::make_counter(\"rows_compacted\", _stats.rows_compacted,\n            sm::description(\"total amount of attempts to compact expired rows during read\")),\n        sm::make_counter(\"rows_compacted_away\", _stats.rows_compacted_away,\n            sm::description(\"total amount of compacted and removed rows during read\")),\n    });\n    sstables::register_index_page_cache_metrics(_metrics, _index_cached_file_stats);\n    sstables::register_index_page_metrics(_metrics, _partition_index_cache_stats);\n}\n\nvoid cache_tracker::clear() {\n    auto partitions_before = _stats.partitions;\n    auto rows_before = _stats.rows;\n    // We need to clear garbage first because garbage versions cannot be evicted from,\n    // mutation_partition::clear_gently() destroys intrusive tree invariants.\n    with_allocator(_region.allocator(), [this] {\n        _garbage.clear();\n        _memtable_cleaner.clear();\n        current_tracker = this;\n        _lru.evict_all();\n        // Eviction could have produced garbage.\n        _garbage.clear();\n        _memtable_cleaner.clear();\n    });\n    _stats.partition_removals += partitions_before;\n    _stats.row_removals += rows_before;\n    allocator().invalidate_references();\n}\n\nvoid cache_tracker::touch(rows_entry& e) {\n    // last dummy may not be linked if evicted\n    if (e.is_linked()) {\n        _lru.remove(e);\n    }\n    _lru.add(e);\n}\n\nvoid cache_tracker::insert(cache_entry& entry) {\n    insert(entry.partition());\n    ++_stats.partition_insertions;\n    ++_stats.partitions;\n    // partition_range_cursor depends on this to detect invalidation of _end\n    _region.allocator().invalidate_references();\n}\n\nvoid cache_tracker::on_partition_erase() noexcept {\n    --_stats.partitions;\n    ++_stats.partition_removals;\n    allocator().invalidate_references();\n}\n\nvoid cache_tracker::on_partition_merge() noexcept {\n    ++_stats.partition_merges;\n}\n\nvoid cache_tracker::on_partition_hit() noexcept {\n    ++_stats.partition_hits;\n}\n\nvoid cache_tracker::on_partition_miss() noexcept {\n    ++_stats.partition_misses;\n}\n\nvoid cache_tracker::on_partition_eviction() noexcept {\n    --_stats.partitions;\n    ++_stats.partition_evictions;\n}\n\nvoid cache_tracker::on_row_eviction() noexcept {\n    --_stats.rows;\n    ++_stats.row_evictions;\n}\n\nvoid cache_tracker::on_row_hit() noexcept {\n    ++_stats.row_hits;\n}\n\nvoid cache_tracker::on_dummy_row_hit() noexcept {\n    ++_stats.dummy_row_hits;\n}\n\nvoid cache_tracker::on_row_miss() noexcept {\n    ++_stats.row_misses;\n}\n\nvoid cache_tracker::on_mispopulate() noexcept {\n    ++_stats.mispopulations;\n}\n\nvoid cache_tracker::on_miss_already_populated() noexcept {\n    ++_stats.concurrent_misses_same_key;\n}\n\nvoid cache_tracker::pinned_dirty_memory_overload(uint64_t bytes) noexcept {\n    _stats.pinned_dirty_memory_overload += bytes;\n}\n\nallocation_strategy& cache_tracker::allocator() noexcept {\n    return _region.allocator();\n}\n\nlogalloc::region& cache_tracker::region() noexcept {\n    return _region;\n}\n\nconst logalloc::region& cache_tracker::region() const noexcept {\n    return _region;\n}\n\n// Stable cursor over partition entries from given range.\n//\n// Must be accessed with reclaim lock held on the cache region.\n// The position of the cursor is always valid, but cache entry reference\n// is not always valid. It remains valid as long as the iterators\n// into _cache._partitions remain valid. Cache entry reference can be\n// brought back to validity by calling refresh().\n//\nclass partition_range_cursor final {\n    std::reference_wrapper<row_cache> _cache;\n    row_cache::partitions_type::iterator _it;\n    row_cache::partitions_type::iterator _end;\n    dht::ring_position_view _start_pos;\n    dht::ring_position_view _end_pos;\n    std::optional<dht::decorated_key> _last;\n    uint64_t _last_reclaim_count;\nprivate:\n    void set_position(cache_entry& e) {\n        // FIXME: make ring_position_view convertible to ring_position, so we can use e.position()\n        if (e.is_dummy_entry()) {\n            _last = {};\n            _start_pos = dht::ring_position_view::max();\n        } else {\n            _last = e.key();\n            _start_pos = dht::ring_position_view(*_last);\n        }\n    }\npublic:\n    // Creates a cursor positioned at the lower bound of the range.\n    // The cache entry reference is not valid.\n    // The range reference must remain live as long as this instance is used.\n    partition_range_cursor(row_cache& cache, const dht::partition_range& range)\n        : _cache(cache)\n        , _start_pos(dht::ring_position_view::for_range_start(range))\n        , _end_pos(dht::ring_position_view::for_range_end(range))\n        , _last_reclaim_count(std::numeric_limits<uint64_t>::max())\n    { }\n\n    // Returns true iff the cursor is valid\n    bool valid() const {\n        return _cache.get().get_cache_tracker().allocator().invalidate_counter() == _last_reclaim_count;\n    }\n\n    // Repositions the cursor to the first entry with position >= pos.\n    // Returns true iff the position of the cursor is equal to pos.\n    // Can be called on invalid cursor, in which case it brings it back to validity.\n    // Strong exception guarantees.\n    bool advance_to(dht::ring_position_view pos) {\n        dht::ring_position_comparator cmp(*_cache.get()._schema);\n        if (cmp(_end_pos, pos) < 0) { // next() may have moved _start_pos past the _end_pos.\n            _end_pos = pos;\n        }\n        _end = _cache.get()._partitions.lower_bound(_end_pos, cmp);\n        _it = _cache.get()._partitions.lower_bound(pos, cmp);\n        auto same = cmp(pos, _it->position()) >= 0;\n        set_position(*_it);\n        _last_reclaim_count = _cache.get().get_cache_tracker().allocator().invalidate_counter();\n        return same;\n    }\n\n    // Ensures that cache entry reference is valid.\n    // The cursor will point at the first entry with position >= the current position.\n    // Returns true if and only if the position of the cursor did not change.\n    // Strong exception guarantees.\n    bool refresh() {\n        if (valid()) {\n            return true;\n        }\n        return advance_to(_start_pos);\n    }\n\n    // Positions the cursor at the next entry.\n    // May advance past the requested range. Use in_range() after the call to determine that.\n    // Call only when in_range() and cache entry reference is valid.\n    // Strong exception guarantees.\n    void next() {\n        auto next = std::next(_it);\n        set_position(*next);\n        _it = std::move(next);\n    }\n\n    // Valid only after refresh() and before _cache._partitions iterators are invalidated.\n    // Points inside the requested range if in_range().\n    cache_entry& entry() {\n        return *_it;\n    }\n\n    // Call only when cache entry reference is valid.\n    bool in_range() {\n        return _it != _end;\n    }\n\n    // Returns current position of the cursor.\n    // Result valid as long as this instance is valid and not advanced.\n    dht::ring_position_view position() const {\n        return _start_pos;\n    }\n};\n\nfuture<> read_context::create_underlying() {\n    if (_range_query) {\n        // FIXME: Singular-range mutation readers don't support fast_forward_to(), so need to use a wide range\n        // here in case the same reader will need to be fast forwarded later.\n        _sm_range = dht::partition_range({dht::ring_position(*_key)}, {dht::ring_position(*_key)});\n    } else {\n        _sm_range = dht::partition_range::make_singular({dht::ring_position(*_key)});\n    }\n    return _underlying.fast_forward_to(std::move(_sm_range), *_underlying_snapshot, _phase).then([this] {\n        _underlying_snapshot = {};\n    });\n}\n\nstatic mutation_reader read_directly_from_underlying(read_context& reader, mutation_fragment_v2 partition_start) {\n    auto res = make_delegating_reader(reader.underlying().underlying());\n    res.unpop_mutation_fragment(std::move(partition_start));\n    res.upgrade_schema(reader.schema());\n    return make_nonforwardable(std::move(res), true);\n}\n\n// Reader which populates the cache using data from the delegate.\nclass single_partition_populating_reader final : public mutation_reader::impl {\n    row_cache& _cache;\n    std::unique_ptr<read_context> _read_context;\n    mutation_reader_opt _reader;\nprivate:\n    future<> create_reader() {\n        auto src_and_phase = _cache.snapshot_of(_read_context->range().start()->value());\n        auto phase = src_and_phase.phase;\n        _read_context->enter_partition(_read_context->range().start()->value().as_decorated_key(), src_and_phase.snapshot, phase);\n        return _read_context->create_underlying().then([this, phase] {\n          return _read_context->underlying().underlying()().then([this, phase] (auto&& mfopt) {\n            if (!mfopt) {\n                if (phase == _cache.phase_of(_read_context->range().start()->value())) {\n                    _cache._read_section(_cache._tracker.region(), [this] {\n                        _cache.find_or_create_missing(_read_context->key());\n                    });\n                } else {\n                    _cache._tracker.on_mispopulate();\n                }\n                _end_of_stream = true;\n            } else if (phase == _cache.phase_of(_read_context->range().start()->value())) {\n                _reader = _cache._read_section(_cache._tracker.region(), [&] {\n                    cache_entry& e = _cache.find_or_create_incomplete(mfopt->as_partition_start(), phase);\n                    return e.read(_cache, *_read_context, phase);\n                });\n            } else {\n                _cache._tracker.on_mispopulate();\n                _reader = read_directly_from_underlying(*_read_context, std::move(*mfopt));\n            }\n          });\n        });\n    }\npublic:\n    single_partition_populating_reader(row_cache& cache,\n            std::unique_ptr<read_context> context)\n        : impl(context->schema(), context->permit())\n        , _cache(cache)\n        , _read_context(std::move(context))\n    { }\n\n    virtual future<> fill_buffer() override {\n        if (!_reader) {\n            return create_reader().then([this] {\n                if (_end_of_stream) {\n                    return make_ready_future<>();\n                }\n                return fill_buffer();\n            });\n        }\n        return do_until([this] { return is_end_of_stream() || is_buffer_full(); }, [this] {\n            return fill_buffer_from(*_reader).then([this] (bool reader_finished) {\n                if (reader_finished) {\n                    _end_of_stream = true;\n                }\n            });\n        });\n    }\n    virtual future<> next_partition() override {\n        if (_reader) {\n            clear_buffer();\n            _end_of_stream = true;\n        }\n        return make_ready_future<>();\n    }\n    virtual future<> fast_forward_to(const dht::partition_range&) override {\n        clear_buffer();\n        _end_of_stream = true;\n        return make_ready_future<>();\n    }\n    virtual future<> fast_forward_to(position_range pr) override {\n        return make_exception_future<>(make_backtraced_exception_ptr<std::bad_function_call>());\n    }\n    virtual future<> close() noexcept override {\n        auto close_reader = _reader ? _reader->close() : make_ready_future<>();\n        auto close_read_context = _read_context->close();\n        return when_all_succeed(std::move(close_reader), std::move(close_read_context)).discard_result();\n    }\n};\n\nvoid cache_tracker::clear_continuity(cache_entry& ce) noexcept {\n    ce.set_continuous(false);\n}\n\nvoid row_cache::on_partition_hit() {\n    _tracker.on_partition_hit();\n}\n\nvoid row_cache::on_partition_miss() {\n    _tracker.on_partition_miss();\n}\n\nvoid row_cache::on_row_hit() {\n    _stats.hits.mark();\n    _tracker.on_row_hit();\n}\n\nvoid row_cache::on_mispopulate() {\n    _tracker.on_mispopulate();\n}\n\nvoid row_cache::on_row_miss() {\n    _stats.misses.mark();\n    _tracker.on_row_miss();\n}\n\nvoid row_cache::on_static_row_insert() {\n    ++_tracker._stats.static_row_insertions;\n}\n\nclass range_populating_reader {\n    row_cache& _cache;\n    autoupdating_underlying_reader& _reader;\n    std::optional<row_cache::previous_entry_pointer> _last_key;\n    read_context& _read_context;\nprivate:\n    bool can_set_continuity() const {\n        return _last_key && _reader.creation_phase() == _cache.phase_of(_reader.population_range_start());\n    }\n    void handle_end_of_stream() {\n        if (!can_set_continuity()) {\n            _cache.on_mispopulate();\n            return;\n        }\n        if (!_reader.range().end() || !_reader.range().end()->is_inclusive()) {\n            dht::ring_position_comparator cmp(*_cache._schema);\n            auto it = _reader.range().end() ? _cache._partitions.find(_reader.range().end()->value(), cmp)\n                                           : std::prev(_cache._partitions.end());\n            if (it != _cache._partitions.end()) {\n                if (it == _cache._partitions.begin()) {\n                    if (!_last_key->_key) {\n                        it->set_continuous(true);\n                    } else {\n                        _cache.on_mispopulate();\n                    }\n                } else {\n                    auto prev = std::prev(it);\n                    if (prev->key().equal(*_cache._schema, *_last_key->_key)) {\n                        it->set_continuous(true);\n                    } else {\n                        _cache.on_mispopulate();\n                    }\n                }\n            }\n        }\n    }\npublic:\n    range_populating_reader(row_cache& cache, read_context& ctx)\n        : _cache(cache)\n        , _reader(ctx.underlying())\n        , _read_context(ctx)\n    {}\n\n    future<mutation_reader_opt> operator()() {\n        return _reader.move_to_next_partition().then([this] (auto&& mfopt) mutable {\n            {\n                if (!mfopt) {\n                    return _cache._read_section(_cache._tracker.region(), [&] {\n                        this->handle_end_of_stream();\n                        return make_ready_future<mutation_reader_opt>(std::nullopt);\n                    });\n                }\n                _cache.on_partition_miss();\n                const partition_start& ps = mfopt->as_partition_start();\n                const dht::decorated_key& key = ps.key();\n                if (_reader.creation_phase() == _cache.phase_of(key)) {\n                    return _cache._read_section(_cache._tracker.region(), [&] {\n                        cache_entry& e = _cache.find_or_create_incomplete(ps, _reader.creation_phase(),\n                                                               this->can_set_continuity() ? &*_last_key : nullptr);\n                        _last_key = row_cache::previous_entry_pointer(key);\n                        return make_ready_future<mutation_reader_opt>(e.read(_cache, _read_context, _reader.creation_phase()));\n                    });\n                } else {\n                    _cache._tracker.on_mispopulate();\n                    _last_key = row_cache::previous_entry_pointer(key);\n                    return make_ready_future<mutation_reader_opt>(read_directly_from_underlying(_read_context, std::move(*mfopt)));\n                }\n            }\n        });\n    }\n\n    future<> fast_forward_to(dht::partition_range&& pr) {\n        if (!pr.start()) {\n            _last_key = row_cache::previous_entry_pointer();\n        } else if (!pr.start()->is_inclusive() && pr.start()->value().has_key()) {\n            _last_key = row_cache::previous_entry_pointer(pr.start()->value().as_decorated_key());\n        } else {\n            // Inclusive start bound, cannot set continuity flag.\n            _last_key = {};\n        }\n\n        return _reader.fast_forward_to(std::move(pr));\n    }\n    future<> close() noexcept {\n        return _reader.close();\n    }\n};\n\nclass scanning_and_populating_reader final : public mutation_reader::impl {\n    const dht::partition_range* _pr;\n    row_cache& _cache;\n    std::unique_ptr<read_context> _read_context;\n    partition_range_cursor _primary;\n    range_populating_reader _secondary_reader;\n    bool _read_next_partition = false;\n    bool _secondary_in_progress = false;\n    bool _advance_primary = false;\n    std::optional<dht::partition_range::bound> _lower_bound;\n    dht::partition_range _secondary_range;\n    mutation_reader_opt _reader;\nprivate:\n    mutation_reader read_from_entry(cache_entry& ce) {\n        _cache.upgrade_entry(ce);\n        _cache.on_partition_hit();\n        return ce.read(_cache, *_read_context);\n    }\n\n    static dht::ring_position_view as_ring_position_view(const std::optional<dht::partition_range::bound>& lower_bound) {\n        return lower_bound ? dht::ring_position_view(lower_bound->value(), dht::ring_position_view::after_key(!lower_bound->is_inclusive()))\n                           : dht::ring_position_view::min();\n    }\n\n    mutation_reader_opt do_read_from_primary() {\n        return _cache._read_section(_cache._tracker.region(), [this] () -> mutation_reader_opt {\n            bool not_moved = true;\n            if (!_primary.valid()) {\n                not_moved = _primary.advance_to(as_ring_position_view(_lower_bound));\n            }\n\n            if (_advance_primary && not_moved) {\n                _primary.next();\n                not_moved = false;\n            }\n            _advance_primary = false;\n\n            if (not_moved || _primary.entry().continuous()) {\n                if (!_primary.in_range()) {\n                    return std::nullopt;\n                }\n                cache_entry& e = _primary.entry();\n                auto fr = read_from_entry(e);\n                _lower_bound = dht::partition_range::bound{e.key(), false};\n                // Delay the call to next() so that we don't see stale continuity on next invocation.\n                _advance_primary = true;\n                return mutation_reader_opt(std::move(fr));\n            } else {\n                if (_primary.in_range()) {\n                    cache_entry& e = _primary.entry();\n                    _secondary_range = dht::partition_range(_lower_bound,\n                        dht::partition_range::bound{e.key(), false});\n                    _lower_bound = dht::partition_range::bound{e.key(), true};\n                    _secondary_in_progress = true;\n                    return std::nullopt;\n                } else {\n                    dht::ring_position_comparator cmp(*_read_context->schema());\n                    auto range = _pr->trim_front(std::optional<dht::partition_range::bound>(_lower_bound), cmp);\n                    if (!range) {\n                        return std::nullopt;\n                    }\n                    _lower_bound = dht::partition_range::bound{dht::ring_position::max()};\n                    _secondary_range = std::move(*range);\n                    _secondary_in_progress = true;\n                    return std::nullopt;\n                }\n            }\n        });\n    }\n\n    future<mutation_reader_opt> read_from_primary() {\n        auto reader = do_read_from_primary();\n        if (!_secondary_in_progress) {\n            return make_ready_future<mutation_reader_opt>(std::move(reader));\n        }\n        return _secondary_reader.fast_forward_to(std::move(_secondary_range)).then([this] {\n            return read_from_secondary();\n        });\n    }\n\n    future<mutation_reader_opt> read_from_secondary() {\n        return _secondary_reader().then([this] (mutation_reader_opt&& fropt) {\n            if (fropt) {\n                return make_ready_future<mutation_reader_opt>(std::move(fropt));\n            } else {\n                _secondary_in_progress = false;\n                return read_from_primary();\n            }\n        });\n    }\n    future<> read_next_partition() {\n      auto close_reader = _reader ? _reader->close() : make_ready_future<>();\n      return close_reader.then([this] {\n        _read_next_partition = false;\n        return (_secondary_in_progress ? read_from_secondary() : read_from_primary()).then([this] (auto&& fropt) {\n            if (bool(fropt)) {\n                _reader = std::move(fropt);\n            } else {\n                _end_of_stream = true;\n            }\n        });\n      });\n    }\npublic:\n    scanning_and_populating_reader(row_cache& cache,\n                                   const dht::partition_range& range,\n                                   std::unique_ptr<read_context> context)\n        : impl(context->schema(), context->permit())\n        , _pr(&range)\n        , _cache(cache)\n        , _read_context(std::move(context))\n        , _primary(cache, range)\n        , _secondary_reader(cache, *_read_context)\n        , _lower_bound(range.start())\n    { }\n    virtual future<> fill_buffer() override {\n        return do_until([this] { return is_end_of_stream() || is_buffer_full(); }, [this] {\n            if (!_reader || _read_next_partition) {\n                return read_next_partition();\n            } else {\n                return fill_buffer_from(*_reader).then([this] (bool reader_finished) {\n                    if (reader_finished) {\n                        _read_next_partition = true;\n                    }\n                });\n            }\n        });\n    }\n    virtual future<> next_partition() override {\n        clear_buffer_to_next_partition();\n        if (_reader && is_buffer_empty()) {\n            return _reader->next_partition();\n        }\n        return make_ready_future<>();\n    }\n    virtual future<> fast_forward_to(const dht::partition_range& pr) override {\n        clear_buffer();\n        _end_of_stream = false;\n        _secondary_in_progress = false;\n        _advance_primary = false;\n        _pr = &pr;\n        _primary = partition_range_cursor{_cache, pr};\n        _lower_bound = pr.start();\n        return _reader->close();\n    }\n    virtual future<> fast_forward_to(position_range cr) override {\n        return make_exception_future<>(make_backtraced_exception_ptr<std::bad_function_call>());\n    }\n    virtual future<> close() noexcept override {\n        auto close_reader = _reader ? _reader->close() : make_ready_future<>();\n        auto close_secondary_reader = _secondary_reader.close();\n        auto close_read_context = _read_context->close();\n        return when_all_succeed(std::move(close_reader), std::move(close_secondary_reader), std::move(close_read_context)).discard_result();\n    }\n};\n\nmutation_reader\nrow_cache::make_scanning_reader(const dht::partition_range& range, std::unique_ptr<read_context> context) {\n    return make_mutation_reader<scanning_and_populating_reader>(*this, range, std::move(context));\n}\n\nmutation_reader_opt\nrow_cache::make_reader_opt(schema_ptr s,\n                       reader_permit permit,\n                       const dht::partition_range& range,\n                       const query::partition_slice& slice,\n                       const tombstone_gc_state* gc_state,\n                       tracing::trace_state_ptr trace_state,\n                       streamed_mutation::forwarding fwd,\n                       mutation_reader::forwarding fwd_mr)\n{\n    auto make_context = [&] {\n        return std::make_unique<read_context>(*this, s, permit, range, slice, gc_state, trace_state, fwd_mr);\n    };\n\n    if (query::is_single_partition(range) && !fwd_mr) {\n        tracing::trace(trace_state, \"Querying cache for range {} and slice {}\",\n                range, seastar::value_of([&slice] { return slice.get_all_ranges(); }));\n        auto mr = _read_section(_tracker.region(), [&] () -> mutation_reader_opt {\n            dht::ring_position_comparator cmp(*_schema);\n            auto&& pos = range.start()->value();\n            partitions_type::bound_hint hint;\n            auto i = _partitions.lower_bound(pos, cmp, hint);\n            if (hint.match) {\n                cache_entry& e = *i;\n                upgrade_entry(e);\n                on_partition_hit();\n                return e.read(*this, make_context());\n            } else if (i->continuous()) {\n                return {};\n            } else {\n                tracing::trace(trace_state, \"Range {} not found in cache\", range);\n                on_partition_miss();\n                return make_mutation_reader<single_partition_populating_reader>(*this, make_context());\n            }\n        });\n\n        if (mr && fwd == streamed_mutation::forwarding::yes) {\n            return make_forwardable(std::move(*mr));\n        } else {\n            return mr;\n        }\n    }\n\n    tracing::trace(trace_state, \"Scanning cache for range {} and slice {}\",\n                   range, seastar::value_of([&slice] { return slice.get_all_ranges(); }));\n    auto mr = make_scanning_reader(range, make_context());\n    if (fwd == streamed_mutation::forwarding::yes) {\n        return make_forwardable(std::move(mr));\n    } else {\n        return mr;\n    }\n}\n\nmutation_reader row_cache::make_nonpopulating_reader(schema_ptr schema, reader_permit permit, const dht::partition_range& range,\n        const query::partition_slice& slice, tracing::trace_state_ptr ts) {\n    if (!range.is_singular()) {\n        throw std::runtime_error(\"row_cache::make_nonpopulating_reader(): only singular ranges are supported\");\n    }\n    struct dummy_accounter {\n        void operator()(const clustering_row&) {}\n        void operator()(const static_row&) {}\n        void operator()(const range_tombstone_change&) {}\n        void operator()(const partition_start&) {}\n        void operator()(const partition_end&) {}\n    };\n    return _read_section(_tracker.region(), [&] () -> mutation_reader {\n        dht::ring_position_comparator cmp(*_schema);\n        auto&& pos = range.start()->value();\n        partitions_type::bound_hint hint;\n        auto i = _partitions.lower_bound(pos, cmp, hint);\n        if (hint.match) {\n            cache_entry& e = *i;\n            upgrade_entry(e);\n            tracing::trace(ts, \"Reading partition {} from cache\", pos);\n            return make_partition_snapshot_flat_reader<false, dummy_accounter>(\n                    schema,\n                    std::move(permit),\n                    e.key(),\n                    query::clustering_key_filter_ranges(slice.row_ranges(*schema, e.key().key())),\n                    e.partition().read(_tracker.region(), _tracker.memtable_cleaner(), nullptr, phase_of(pos)),\n                    false,\n                    _tracker.region(),\n                    _read_section,\n                    {},\n                    streamed_mutation::forwarding::no);\n        } else {\n            tracing::trace(ts, \"Partition {} is not found in cache\", pos);\n            return make_empty_flat_reader_v2(std::move(schema), std::move(permit));\n        }\n    });\n}\n\nvoid row_cache::clear_on_destruction() noexcept {\n    with_allocator(_tracker.allocator(), [this] {\n        _partitions.clear_and_dispose([this] (cache_entry* p) mutable noexcept {\n            if (!p->is_dummy_entry()) {\n                _tracker.on_partition_erase();\n            }\n            p->evict(_tracker);\n        });\n    });\n}\n\nrow_cache::~row_cache() {\n    clear_on_destruction();\n}\n\nvoid row_cache::clear_now() noexcept {\n    with_allocator(_tracker.allocator(), [this] {\n        auto it = _partitions.erase_and_dispose(_partitions.begin(), partitions_end(), [this] (cache_entry* p) noexcept {\n            _tracker.on_partition_erase();\n            p->evict(_tracker);\n        });\n        _tracker.clear_continuity(*it);\n    });\n}\n\ntemplate<typename CreateEntry, typename VisitEntry>\nrequires requires(CreateEntry create, VisitEntry visit, row_cache::partitions_type::iterator it, row_cache::partitions_type::bound_hint hint) {\n    { create(it, hint) } -> std::same_as<row_cache::partitions_type::iterator>;\n    { visit(it) } -> std::same_as<void>;\n}\ncache_entry& row_cache::do_find_or_create_entry(const dht::decorated_key& key,\n    const previous_entry_pointer* previous, CreateEntry&& create_entry, VisitEntry&& visit_entry)\n{\n    return with_allocator(_tracker.allocator(), [&] () -> cache_entry& {\n        partitions_type::bound_hint hint;\n        dht::ring_position_comparator cmp(*_schema);\n        auto i = _partitions.lower_bound(key, cmp, hint);\n        if (i == _partitions.end() || !hint.match) {\n            i = create_entry(i, hint);\n        } else {\n            visit_entry(i);\n        }\n\n        if (!previous) {\n            return *i;\n        }\n\n        if ((!previous->_key && i == _partitions.begin())\n            || (previous->_key && i != _partitions.begin()\n                && std::prev(i)->key().equal(*_schema, *previous->_key))) {\n            i->set_continuous(true);\n        } else {\n            on_mispopulate();\n        }\n\n        return *i;\n    });\n}\n\ncache_entry& row_cache::find_or_create_incomplete(const partition_start& ps, row_cache::phase_type phase, const previous_entry_pointer* previous) {\n    return do_find_or_create_entry(ps.key(), previous, [&] (auto i, const partitions_type::bound_hint& hint) { // create\n        // Create an fully discontinuous, except for the partition tombstone, entry\n        mutation_partition mp = mutation_partition::make_incomplete(*_schema, ps.partition_tombstone());\n        partitions_type::iterator entry = _partitions.emplace_before(i, ps.key().token().raw(), hint,\n                _schema, ps.key(), std::move(mp));\n        _tracker.insert(*entry);\n        return entry;\n    }, [&] (auto i) { // visit\n        _tracker.on_miss_already_populated();\n        cache_entry& e = *i;\n        e.partition().open_version(*e.schema(), &_tracker, phase).partition().apply(ps.partition_tombstone());\n        upgrade_entry(e);\n    });\n}\n\ncache_entry& row_cache::find_or_create_missing(const dht::decorated_key& key) {\n    return do_find_or_create_entry(key, nullptr, [&] (auto i, const partitions_type::bound_hint& hint) {\n        mutation_partition mp(*_schema);\n        bool cont = i->continuous();\n        partitions_type::iterator entry = _partitions.emplace_before(i, key.token().raw(), hint,\n                _schema, key, std::move(mp));\n        _tracker.insert(*entry);\n        entry->set_continuous(cont);\n        return entry;\n    }, [&] (auto i) {\n        _tracker.on_miss_already_populated();\n    });\n}\n\nvoid row_cache::populate(const mutation& m, const previous_entry_pointer* previous) {\n  _populate_section(_tracker.region(), [&] {\n    do_find_or_create_entry(m.decorated_key(), previous, [&] (auto i, const partitions_type::bound_hint& hint) {\n        partitions_type::iterator entry = _partitions.emplace_before(i, m.decorated_key().token().raw(), hint,\n                m.schema(), m.decorated_key(), m.partition());\n        _tracker.insert(*entry);\n        entry->set_continuous(i->continuous());\n        upgrade_entry(*entry);\n        return entry;\n    }, [&] (auto i) {\n        throw std::runtime_error(format(\"cache already contains entry for {}\", m.key()));\n    });\n  });\n}\n\ncache_entry& row_cache::lookup(const dht::decorated_key& key) {\n    return do_find_or_create_entry(key, nullptr, [&] (auto i, const partitions_type::bound_hint& hint) {\n        throw std::runtime_error(format(\"cache doesn't contain entry for {}\", key));\n        return i;\n    }, [&] (auto i) {\n        _tracker.on_miss_already_populated();\n        upgrade_entry(*i);\n    });\n}\n\nmutation_source& row_cache::snapshot_for_phase(phase_type phase) {\n    if (phase == _underlying_phase) {\n        return _underlying;\n    } else {\n        if (phase + 1 < _underlying_phase) {\n            throw std::runtime_error(format(\"attempted to read from retired phase {} (current={})\", phase, _underlying_phase));\n        }\n        return *_prev_snapshot;\n    }\n}\n\nrow_cache::snapshot_and_phase row_cache::snapshot_of(dht::ring_position_view pos) {\n    dht::ring_position_less_comparator less(*_schema);\n    if (!_prev_snapshot_pos || less(pos, *_prev_snapshot_pos)) {\n        return {_underlying, _underlying_phase};\n    }\n    return {*_prev_snapshot, _underlying_phase - 1};\n}\n\nvoid row_cache::invalidate_sync(replica::memtable& m) noexcept {\n    with_allocator(_tracker.allocator(), [&m, this] () {\n        logalloc::reclaim_lock _(_tracker.region());\n        bool blow_cache = false;\n        m.partitions.clear_and_dispose([this, &m, &blow_cache] (replica::memtable_entry* entry) noexcept {\n            try {\n                invalidate_locked(entry->key());\n            } catch (...) {\n                blow_cache = true;\n            }\n            m.evict_entry(*entry, _tracker.memtable_cleaner());\n        });\n        if (blow_cache) {\n            // We failed to invalidate the key. Recover using clear_now(), which doesn't throw.\n            clear_now();\n        }\n    });\n}\n\nrow_cache::phase_type row_cache::phase_of(dht::ring_position_view pos) {\n    dht::ring_position_less_comparator less(*_schema);\n    if (!_prev_snapshot_pos || less(pos, *_prev_snapshot_pos)) {\n        return _underlying_phase;\n    }\n    return _underlying_phase - 1;\n}\n\nthread_local preemption_source row_cache::default_preemption_source;\n\ntemplate <typename Updater>\nfuture<> row_cache::do_update(external_updater eu, replica::memtable& m, Updater updater, preemption_source& preempt_src) {\n  return do_update(std::move(eu), [this, &m, &preempt_src, updater = std::move(updater)] {\n    real_dirty_memory_accounter real_dirty_acc(m, _tracker);\n    m.on_detach_from_region_group();\n    _tracker.region().merge(m); // Now all data in memtable belongs to cache\n    _tracker.memtable_cleaner().merge(m._cleaner);\n    STAP_PROBE(scylla, row_cache_update_start);\n    auto cleanup = defer([&m, this] () noexcept {\n        invalidate_sync(m);\n        STAP_PROBE(scylla, row_cache_update_end);\n    });\n\n    return seastar::async([this, &m, &preempt_src, updater = std::move(updater), real_dirty_acc = std::move(real_dirty_acc)] () mutable {\n        size_t size_entry;\n        // In case updater fails, we must bring the cache to consistency without deferring.\n        auto cleanup = defer([&m, this] () noexcept {\n            invalidate_sync(m);\n            _prev_snapshot_pos = {};\n            _prev_snapshot = {};\n        });\n        utils::coroutine update; // Destroy before cleanup to release snapshots before invalidating.\n        auto destroy_update = defer([&] {\n            with_allocator(_tracker.allocator(), [&] {\n                update = {};\n            });\n        });\n        partition_presence_checker is_present = _prev_snapshot->make_partition_presence_checker();\n        while (!m.partitions.empty()) {\n            with_allocator(_tracker.allocator(), [&] () {\n                auto cmp = dht::ring_position_comparator(*_schema);\n                {\n                    size_t partition_count = 0;\n                    {\n                        STAP_PROBE(scylla, row_cache_update_one_batch_start);\n                        do {\n                          STAP_PROBE(scylla, row_cache_update_partition_start);\n                          {\n                            if (!update) {\n                                _update_section(_tracker.region(), [&] {\n                                    replica::memtable_entry& mem_e = *m.partitions.begin();\n                                    size_entry = mem_e.size_in_allocator_without_rows(_tracker.allocator());\n                                    partitions_type::bound_hint hint;\n                                    auto cache_i = _partitions.lower_bound(mem_e.key(), cmp, hint);\n                                    update = updater(_update_section, cache_i, mem_e, is_present, real_dirty_acc, hint, preempt_src);\n                                });\n                            }\n                            // We use cooperative deferring instead of futures so that\n                            // this layer has a chance to restore invariants before deferring,\n                            // in particular set _prev_snapshot_pos to the correct value.\n                            if (update.run() == stop_iteration::no) {\n                                break;\n                            }\n                            update = {};\n                            real_dirty_acc.unpin_memory(size_entry);\n                            _update_section(_tracker.region(), [&] {\n                                auto i = m.partitions.begin();\n                                i.erase_and_dispose(dht::raw_token_less_comparator{}, [&] (replica::memtable_entry* e) noexcept {\n                                    m.evict_entry(*e, _tracker.memtable_cleaner());\n                                });\n                            });\n                            ++partition_count;\n                          }\n                          STAP_PROBE(scylla, row_cache_update_partition_end);\n                        } while (!m.partitions.empty() && !preempt_src.should_preempt());\n                        with_allocator(standard_allocator(), [&] {\n                            if (m.partitions.empty()) {\n                                _prev_snapshot_pos = {};\n                            } else {\n                                _update_section(_tracker.region(), [&] {\n                                    _prev_snapshot_pos = m.partitions.begin()->key();\n                                });\n                            }\n                        });\n                        STAP_PROBE1(scylla, row_cache_update_one_batch_end, partition_count);\n                    }\n                }\n            });\n            real_dirty_acc.commit();\n            preempt_src.thread_yield();\n        }\n    }).finally([cleanup = std::move(cleanup)] {});\n  });\n}\n\nfuture<> row_cache::update(external_updater eu, replica::memtable& m, preemption_source& preempt_src) {\n    return do_update(std::move(eu), m, [this] (logalloc::allocating_section& alloc,\n            row_cache::partitions_type::iterator cache_i, replica::memtable_entry& mem_e, partition_presence_checker& is_present,\n            real_dirty_memory_accounter& acc, const partitions_type::bound_hint& hint, preemption_source& preempt_src) mutable {\n        // If cache doesn't contain the entry we cannot insert it because the mutation may be incomplete.\n        // FIXME: keep a bitmap indicating which sstables we do cover, so we don't have to\n        //        search it.\n        if (cache_i != partitions_end() && hint.match) {\n            cache_entry& entry = *cache_i;\n            upgrade_entry(entry);\n            SCYLLA_ASSERT(entry.schema() == _schema);\n            _tracker.on_partition_merge();\n            mem_e.upgrade_schema(_tracker.region(), _schema, _tracker.memtable_cleaner());\n            return entry.partition().apply_to_incomplete(*_schema, std::move(mem_e.partition()), _tracker.memtable_cleaner(),\n                alloc, _tracker.region(), _tracker, _underlying_phase, acc, preempt_src);\n        } else if (cache_i->continuous()\n                   || with_allocator(standard_allocator(), [&] { return is_present(mem_e.key()); })\n                      == partition_presence_checker_result::definitely_doesnt_exist) {\n            // Partition is absent in underlying. First, insert a neutral partition entry.\n            partitions_type::iterator entry = _partitions.emplace_before(cache_i, mem_e.key().token().raw(), hint,\n                cache_entry::evictable_tag(), _schema, dht::decorated_key(mem_e.key()),\n                partition_entry::make_evictable(*_schema, mutation_partition(*_schema)));\n            entry->set_continuous(cache_i->continuous());\n            _tracker.insert(*entry);\n            mem_e.upgrade_schema(_tracker.region(), _schema, _tracker.memtable_cleaner());\n            return entry->partition().apply_to_incomplete(*_schema, std::move(mem_e.partition()), _tracker.memtable_cleaner(),\n                alloc, _tracker.region(), _tracker, _underlying_phase, acc, preempt_src);\n        } else {\n            return utils::make_empty_coroutine();\n        }\n    }, preempt_src);\n}\n\nfuture<> row_cache::update_invalidating(external_updater eu, replica::memtable& m) {\n    return do_update(std::move(eu), m, [this] (logalloc::allocating_section& alloc,\n        row_cache::partitions_type::iterator cache_i, replica::memtable_entry& mem_e, partition_presence_checker& is_present,\n        real_dirty_memory_accounter& acc, const partitions_type::bound_hint&, preemption_source&)\n    {\n        if (cache_i != partitions_end() && cache_i->key().equal(*_schema, mem_e.key())) {\n            // FIXME: Invalidate only affected row ranges.\n            // This invalidates all information about the partition.\n            cache_entry& e = *cache_i;\n            e.evict(_tracker);\n            e.on_evicted(_tracker);\n        } else {\n            _tracker.clear_continuity(*cache_i);\n        }\n        // FIXME: subtract gradually from acc.\n        return utils::make_empty_coroutine();\n    }, default_preemption_source);\n}\n\nvoid row_cache::refresh_snapshot() {\n    _underlying = _snapshot_source();\n}\n\nvoid row_cache::touch(const dht::decorated_key& dk) {\n _read_section(_tracker.region(), [&] {\n    auto i = _partitions.find(dk, dht::ring_position_comparator(*_schema));\n    if (i != _partitions.end()) {\n        for (partition_version& pv : i->partition().versions_from_oldest()) {\n            for (rows_entry& row : pv.partition().clustered_rows()) {\n                _tracker.touch(row);\n            }\n        }\n    }\n });\n}\n\nvoid row_cache::unlink_from_lru(const dht::decorated_key& dk) {\n    _read_section(_tracker.region(), [&] {\n        auto i = _partitions.find(dk, dht::ring_position_comparator(*_schema));\n        if (i != _partitions.end()) {\n            for (partition_version& pv : i->partition().versions_from_oldest()) {\n                for (rows_entry& row : pv.partition().clustered_rows()) {\n                    // Last dummy may already be unlinked.\n                    if (row.is_linked()) {\n                        _tracker.get_lru().remove(row);\n                    }\n                }\n            }\n        }\n    });\n}\n\nvoid row_cache::invalidate_locked(const dht::decorated_key& dk) {\n    auto pos = _partitions.lower_bound(dk, dht::ring_position_comparator(*_schema));\n    if (pos == partitions_end() || !pos->key().equal(*_schema, dk)) {\n        _tracker.clear_continuity(*pos);\n    } else {\n        auto it = pos.erase_and_dispose(dht::raw_token_less_comparator{},\n            [this](cache_entry* p) mutable noexcept {\n                _tracker.on_partition_erase();\n                p->evict(_tracker);\n            });\n        _tracker.clear_continuity(*it);\n    }\n}\n\nfuture<> row_cache::invalidate(external_updater eu, const dht::decorated_key& dk) {\n    return invalidate(std::move(eu), dht::partition_range::make_singular(dk));\n}\n\nfuture<> row_cache::invalidate(external_updater eu, const dht::partition_range& range) {\n    return invalidate(std::move(eu), dht::partition_range_vector({range}));\n}\n\nfuture<> row_cache::invalidate(external_updater eu, dht::partition_range_vector&& ranges) {\n    return do_update(std::move(eu), [this, ranges = std::move(ranges)] {\n        return seastar::async([this, ranges = std::move(ranges)] {\n            auto on_failure = defer([this] () noexcept {\n                this->clear_now();\n                _prev_snapshot_pos = {};\n                _prev_snapshot = {};\n            });\n\n            for (auto&& range : ranges) {\n                _prev_snapshot_pos = dht::ring_position_view::for_range_start(range);\n                seastar::thread::maybe_yield();\n\n                while (true) {\n                    auto done = _update_section(_tracker.region(), [&] {\n                        auto cmp = dht::ring_position_comparator(*_schema);\n                        auto it = _partitions.lower_bound(*_prev_snapshot_pos, cmp);\n                        auto end = _partitions.lower_bound(dht::ring_position_view::for_range_end(range), cmp);\n                        return with_allocator(_tracker.allocator(), [&] {\n                            while (it != end) {\n                                it = it.erase_and_dispose(dht::raw_token_less_comparator{},\n                                    [&] (cache_entry* p) mutable noexcept {\n                                        _tracker.on_partition_erase();\n                                        p->evict(_tracker);\n                                    });\n                                // it != end is necessary for correctness. We cannot set _prev_snapshot_pos to end->position()\n                                // because after resuming something may be inserted before \"end\" which falls into the next range.\n                                if (need_preempt() && it != end) {\n                                    with_allocator(standard_allocator(), [&] {\n                                        _prev_snapshot_pos = it->key();\n                                    });\n                                    break;\n                                }\n                            }\n                            SCYLLA_ASSERT(it != _partitions.end());\n                            _tracker.clear_continuity(*it);\n                            return stop_iteration(it == end);\n                        });\n                    });\n                    if (done == stop_iteration::yes) {\n                        break;\n                    }\n                    // _prev_snapshot_pos must be updated at this point such that every position < _prev_snapshot_pos\n                    // is already invalidated and >= _prev_snapshot_pos is not yet invalidated.\n                    seastar::thread::yield();\n                }\n            }\n\n            on_failure.cancel();\n        });\n    });\n}\n\nvoid row_cache::evict() {\n    while (_tracker.region().evict_some() == memory::reclaiming_result::reclaimed_something) {}\n}\n\nrow_cache::row_cache(schema_ptr s, snapshot_source src, cache_tracker& tracker, is_continuous cont)\n    : _tracker(tracker)\n    , _schema(std::move(s))\n    , _partitions(dht::raw_token_less_comparator{})\n    , _underlying(src())\n    , _snapshot_source(std::move(src))\n{\n  try {\n    with_allocator(_tracker.allocator(), [this, cont] {\n        cache_entry entry(cache_entry::dummy_entry_tag{});\n        entry.set_continuous(bool(cont));\n        auto raw_token = entry.position().token().raw();\n        _partitions.insert(raw_token, std::move(entry), dht::ring_position_comparator{*_schema});\n    });\n  } catch (...) {\n    // The code above might have allocated something in _partitions.\n    // The destructor of _partitions will be called with the wrong allocator,\n    // so we have to clear _partitions manually here, before it is destroyed.\n    clear_on_destruction();\n    throw;\n  }\n}\n\ncache_entry::cache_entry(cache_entry&& o) noexcept\n    : _key(std::move(o._key))\n    , _pe(std::move(o._pe))\n    , _flags(o._flags)\n{\n}\n\ncache_entry::~cache_entry() {\n}\n\nvoid cache_entry::evict(cache_tracker& tracker) noexcept {\n    _pe.evict(tracker.cleaner());\n}\n\nvoid row_cache::set_schema(schema_ptr new_schema) noexcept {\n    _schema = std::move(new_schema);\n}\n\nvoid cache_entry::on_evicted(cache_tracker& tracker) noexcept {\n    row_cache::partitions_type::iterator it(this);\n    std::next(it)->set_continuous(false);\n    evict(tracker);\n    tracker.on_partition_eviction();\n    it.erase(dht::raw_token_less_comparator{});\n}\n\nstatic\nmutation_partition_v2::rows_type::iterator on_evicted_shallow(rows_entry& e, cache_tracker& tracker) noexcept {\n    mutation_partition_v2::rows_type::iterator it(&e);\n    if (e.is_last_dummy()) {\n        // Every evictable partition entry must have a dummy entry at the end,\n        // so don't remove it, just unlink from the LRU.\n        // That dummy is linked in the LRU, because there may be partitions\n        // with no regular rows, and we need to track them.\n\n        // We still need to break continuity in order to preserve the \"older versions are evicted first\"\n        // invariant.\n        it->set_continuous(false);\n    } else {\n        // When evicting a dummy with both sides continuous we don't need to break continuity.\n        //\n        auto still_continuous = e.continuous() && e.dummy();\n        auto old_rt = e.range_tombstone();\n        mutation_partition_v2::rows_type::key_grabber kg(it);\n        kg.release(current_deleter<rows_entry>());\n        if (!still_continuous || old_rt != it->range_tombstone()) {\n            it->set_continuous(false);\n        }\n        tracker.on_row_eviction();\n    }\n    return it;\n}\n\nvoid rows_entry::on_evicted(cache_tracker& tracker) noexcept {\n    auto it = ::on_evicted_shallow(*this, tracker);\n\n    mutation_partition_v2::rows_type* rows = it.tree_if_singular();\n    if (rows != nullptr) {\n        SCYLLA_ASSERT(it->is_last_dummy());\n        partition_version& pv = partition_version::container_of(mutation_partition_v2::container_of(*rows));\n        if (pv.is_referenced_from_entry()) {\n            partition_entry& pe = partition_entry::container_of(pv);\n            if (!pe.is_locked()) {\n                cache_entry& ce = cache_entry::container_of(pe);\n                ce.on_evicted(tracker);\n            }\n        }\n    }\n}\n\nvoid rows_entry::on_evicted() noexcept {\n    on_evicted(*current_tracker);\n}\n\nvoid rows_entry::on_evicted_shallow() noexcept {\n    ::on_evicted_shallow(*this, *current_tracker);\n}\n\nmutation_reader cache_entry::read(row_cache& rc, read_context& reader) {\n    auto source_and_phase = rc.snapshot_of(_key);\n    reader.enter_partition(_key, source_and_phase.snapshot, source_and_phase.phase);\n    return do_read(rc, reader);\n}\n\nmutation_reader cache_entry::read(row_cache& rc, read_context& reader, row_cache::phase_type phase) {\n    reader.enter_partition(_key, phase);\n    return do_read(rc, reader);\n}\n\nmutation_reader cache_entry::read(row_cache& rc, std::unique_ptr<read_context> unique_ctx) {\n    auto source_and_phase = rc.snapshot_of(_key);\n    unique_ctx->enter_partition(_key, source_and_phase.snapshot, source_and_phase.phase);\n    return do_read(rc, std::move(unique_ctx));\n}\n\nmutation_reader cache_entry::read(row_cache& rc, std::unique_ptr<read_context> unique_ctx, row_cache::phase_type phase) {\n    unique_ctx->enter_partition(_key, phase);\n    return do_read(rc, std::move(unique_ctx));\n}\n\n// Assumes reader is in the corresponding partition\nmutation_reader cache_entry::do_read(row_cache& rc, read_context& reader) {\n    auto snp = _pe.read(rc._tracker.region(), rc._tracker.cleaner(), &rc._tracker, reader.phase());\n    auto ckr = query::clustering_key_filter_ranges::get_ranges(*schema(), reader.native_slice(), _key.key());\n    schema_ptr entry_schema = to_query_domain(reader.slice(), schema());\n    auto r = make_cache_mutation_reader(entry_schema, _key, std::move(ckr), rc, reader, std::move(snp));\n    r.upgrade_schema(to_query_domain(reader.slice(), rc.schema()));\n    r.upgrade_schema(reader.schema());\n    return r;\n}\n\nmutation_reader cache_entry::do_read(row_cache& rc, std::unique_ptr<read_context> unique_ctx) {\n    auto snp = _pe.read(rc._tracker.region(), rc._tracker.cleaner(), &rc._tracker, unique_ctx->phase());\n    auto ckr = query::clustering_key_filter_ranges::get_ranges(*schema(), unique_ctx->native_slice(), _key.key());\n    schema_ptr reader_schema = unique_ctx->schema();\n    schema_ptr entry_schema = to_query_domain(unique_ctx->slice(), schema());\n    auto rc_schema = to_query_domain(unique_ctx->slice(), rc.schema());\n    auto r = make_cache_mutation_reader(entry_schema, _key, std::move(ckr), rc, std::move(unique_ctx), std::move(snp));\n    r.upgrade_schema(rc_schema);\n    r.upgrade_schema(reader_schema);\n    return r;\n}\n\nconst schema_ptr& row_cache::schema() const {\n    return _schema;\n}\n\nvoid row_cache::upgrade_entry(cache_entry& e) {\n    if (e.schema() != _schema && !e.partition().is_locked()) {\n        auto& r = _tracker.region();\n        SCYLLA_ASSERT(!r.reclaiming_enabled());\n        e.partition().upgrade(r, _schema, _tracker.cleaner(), &_tracker);\n    }\n}\n\nstd::ostream& operator<<(std::ostream& out, row_cache& rc) {\n    rc._read_section(rc._tracker.region(), [&] {\n        fmt::print(out, \"{{row_cache: {}}}\", fmt::join(rc._partitions.begin(), rc._partitions.end(), \", \"));\n    });\n    return out;\n}\n\nfuture<> row_cache::do_update(row_cache::external_updater eu, row_cache::internal_updater iu) noexcept {\n    auto permit = co_await get_units(_update_sem, 1);\n    co_await eu.prepare();\n    {\n        try {\n            eu.execute();\n        } catch (...) {\n            // Any error from execute is considered fatal\n            // to enforce exception safety.\n            on_fatal_internal_error(clogger, fmt::format(\"Fatal error during cache update: {}\", std::current_exception()));\n        }\n        [&] () noexcept {\n            _prev_snapshot_pos = dht::ring_position::min();\n            _prev_snapshot = std::exchange(_underlying, _snapshot_source());\n            ++_underlying_phase;\n        }();\n        auto f = co_await coroutine::as_future(futurize_invoke([&iu] {\n            return iu();\n        }));\n        {\n            _prev_snapshot_pos = {};\n            _prev_snapshot = {};\n            if (f.failed()) {\n                clogger.warn(\"Failure during cache update: {}\", f.get_exception());\n            }\n        }\n    }\n}\n\nauto fmt::formatter<cache_entry>::format(const cache_entry& e, fmt::format_context& ctx) const\n        -> decltype(ctx.out()) {\n    return fmt::format_to(ctx.out(), \"{{cache_entry: {}, cont={}, dummy={}, {}}}\",\n               e.position(), e.continuous(), e.is_dummy_entry(),\n               partition_entry::printer(e.partition()));\n}\n"
        },
        {
          "name": "row_cache.hh",
          "type": "blob",
          "size": 22.021484375,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <boost/intrusive/list.hpp>\n#include <boost/intrusive/set.hpp>\n#include <boost/intrusive/parent_from_member.hpp>\n\n#include <seastar/core/memory.hh>\n#include <seastar/util/noncopyable_function.hh>\n\n#include \"mutation/mutation_partition.hh\"\n#include \"utils/phased_barrier.hh\"\n#include \"utils/histogram.hh\"\n#include \"mutation/partition_version.hh\"\n#include <seastar/core/metrics_registration.hh>\n#include \"utils/double-decker.hh\"\n#include \"db/cache_tracker.hh\"\n#include \"readers/empty_v2.hh\"\n#include \"readers/mutation_source.hh\"\n\nnamespace bi = boost::intrusive;\n\nclass row_cache;\nclass cache_tracker;\nclass mutation_reader;\n\nnamespace replica {\nclass memtable_entry;\n}\n\nnamespace tracing { class trace_state_ptr; }\n\nnamespace cache {\n\nclass autoupdating_underlying_reader;\nclass cache_mutation_reader;\nclass read_context;\nclass lsa_manager;\n\n}\n\n// Intrusive set entry which holds partition data.\n//\n// TODO: Make memtables use this format too.\nclass cache_entry {\n    dht::decorated_key _key;\n    partition_entry _pe;\n    // True when we know that there is nothing between this entry and the previous one in cache\n    struct {\n        bool _continuous : 1;\n        bool _dummy_entry : 1;\n        bool _head : 1;\n        bool _tail : 1;\n        bool _train : 1;\n    } _flags{};\n    friend class size_calculator;\n\n    mutation_reader do_read(row_cache&, cache::read_context& ctx);\n    mutation_reader do_read(row_cache&, std::unique_ptr<cache::read_context> unique_ctx);\npublic:\n    friend class row_cache;\n    friend class cache_tracker;\n\n    bool is_head() const noexcept { return _flags._head; }\n    void set_head(bool v) noexcept { _flags._head = v; }\n    bool is_tail() const noexcept { return _flags._tail; }\n    void set_tail(bool v) noexcept { _flags._tail = v; }\n    bool with_train() const noexcept { return _flags._train; }\n    void set_train(bool v) noexcept { _flags._train = v; }\n\n    struct dummy_entry_tag{};\n    struct evictable_tag{};\n\n    cache_entry(dummy_entry_tag)\n        : _key{dht::token(), partition_key::make_empty()}\n    {\n        _flags._dummy_entry = true;\n    }\n\n    cache_entry(schema_ptr s, const dht::decorated_key& key, const mutation_partition& p)\n        : _key(key)\n        , _pe(partition_entry::make_evictable(*s, mutation_partition(*s, p)))\n    { }\n\n    cache_entry(schema_ptr s, dht::decorated_key&& key, mutation_partition&& p)\n        : cache_entry(evictable_tag(), s, std::move(key),\n            partition_entry::make_evictable(*s, std::move(p)))\n    { }\n\n    // It is assumed that pe is fully continuous\n    // pe must be evictable.\n    cache_entry(evictable_tag, schema_ptr s, dht::decorated_key&& key, partition_entry&& pe) noexcept\n        : _key(std::move(key))\n        , _pe(std::move(pe))\n    { }\n\n    cache_entry(cache_entry&&) noexcept;\n    ~cache_entry();\n\n    static cache_entry& container_of(partition_entry& pe) {\n        return *boost::intrusive::get_parent_from_member(&pe, &cache_entry::_pe);\n    }\n\n    // Called when all contents have been evicted.\n    // This object should unlink and destroy itself from the container.\n    void on_evicted(cache_tracker&) noexcept;\n    // Evicts contents of this entry.\n    // The caller is still responsible for unlinking and destroying this entry.\n    void evict(cache_tracker&) noexcept;\n\n    const dht::decorated_key& key() const noexcept { return _key; }\n    dht::ring_position_view position() const noexcept {\n        if (is_dummy_entry()) {\n            return dht::ring_position_view::max();\n        }\n        return _key;\n    }\n\n    friend dht::ring_position_view ring_position_view_to_compare(const cache_entry& ce) noexcept { return ce.position(); }\n\n    const partition_entry& partition() const noexcept { return _pe; }\n    partition_entry& partition() { return _pe; }\n    const schema_ptr& schema() const noexcept { return _pe.get_schema(); }\n    mutation_reader read(row_cache&, cache::read_context&);\n    mutation_reader read(row_cache&, std::unique_ptr<cache::read_context>);\n    mutation_reader read(row_cache&, cache::read_context&, utils::phased_barrier::phase_type);\n    mutation_reader read(row_cache&, std::unique_ptr<cache::read_context>, utils::phased_barrier::phase_type);\n    bool continuous() const noexcept { return _flags._continuous; }\n    void set_continuous(bool value) noexcept { _flags._continuous = value; }\n\n    bool is_dummy_entry() const noexcept { return _flags._dummy_entry; }\n};\n\n//\n// A data source which wraps another data source such that data obtained from the underlying data source\n// is cached in-memory in order to serve queries faster.\n//\n// Cache populates itself automatically during misses.\n//\n// All updates to the underlying mutation source must be performed through one of the synchronizing methods.\n// Those are the methods which accept external_updater, e.g. update(), invalidate().\n// All synchronizers have strong exception guarantees. If they fail, the set of writes represented by\n// cache didn't change.\n// Synchronizers can be invoked concurrently with each other and other operations on cache.\n//\nclass row_cache final {\npublic:\n    using phase_type = utils::phased_barrier::phase_type;\n    using partitions_type = double_decker<int64_t, cache_entry,\n                            dht::raw_token_less_comparator, dht::ring_position_comparator,\n                            16, bplus::key_search::linear>;\n    static_assert(bplus::SimpleLessCompare<int64_t, dht::raw_token_less_comparator>);\n    friend class cache::autoupdating_underlying_reader;\n    friend class single_partition_populating_reader;\n    friend class cache_entry;\n    friend class cache::cache_mutation_reader;\n    friend class cache::lsa_manager;\n    friend class cache::read_context;\n    friend class partition_range_cursor;\n    friend class cache_tester;\n\n    // A function which adds new writes to the underlying mutation source.\n    // All invocations of external_updater on given cache instance are serialized internally.\n    // Must have strong exception guarantees. If throws, the underlying mutation source\n    // must be left in the state in which it was before the call.\n    class external_updater_impl {\n    public:\n        virtual ~external_updater_impl() {}\n        // Prepare may return an exceptional future\n        // and the error is propagated to the row_cache::update caller.\n        // Hence, it must provide strong exception safety guarantees.\n        //\n        // Typically, `prepare` creates only temporary state\n        // to be atomically applied by `execute`, or, alternatively\n        // it must undo any side-effects on failure.\n        virtual future<> prepare() { return make_ready_future<>(); }\n        // FIXME: make execute() noexcept, that will require every updater to make execution exception safe,\n        // also change function signature.\n        // See https://github.com/scylladb/scylladb/issues/15576\n        //\n        // For now, scylla aborts on any exception from `execute` \n        virtual void execute() = 0;\n    };\n\n    class external_updater {\n        class non_prepared : public external_updater_impl {\n            using Func = seastar::noncopyable_function<void()>;\n            Func _func;\n        public:\n            explicit non_prepared(Func func) : _func(std::move(func)) {}\n            virtual void execute() override {\n                _func();\n            }\n        };\n        std::unique_ptr<external_updater_impl> _impl;\n    public:\n        external_updater(seastar::noncopyable_function<void()> f) : _impl(std::make_unique<non_prepared>(std::move(f))) {}\n        external_updater(std::unique_ptr<external_updater_impl> impl) : _impl(std::move(impl)) {}\n\n        future<> prepare() { return _impl->prepare(); }\n        void execute() { _impl->execute(); }\n    };\npublic:\n    struct stats {\n        utils::timed_rate_moving_average hits;\n        utils::timed_rate_moving_average misses;\n        utils::timed_rate_moving_average reads_with_misses;\n        utils::timed_rate_moving_average reads_with_no_misses;\n    };\nprivate:\n    cache_tracker& _tracker;\n    stats _stats{};\n    schema_ptr _schema;\n    partitions_type _partitions; // Cached partitions are complete.\n\n    // The snapshots used by cache are versioned. The version number of a snapshot is\n    // called the \"population phase\", or simply \"phase\". Between updates, cache\n    // represents the same snapshot.\n    //\n    // Update doesn't happen atomically. Before it completes, some entries reflect\n    // the old snapshot, while others reflect the new snapshot. After update\n    // completes, all entries must reflect the new snapshot. There is a race between the\n    // update process and populating reads. Since after the update all entries must\n    // reflect the new snapshot, reads using the old snapshot cannot be allowed to\n    // insert data which will no longer be reached by the update process. The whole\n    // range can be therefore divided into two sub-ranges, one which was already\n    // processed by the update and one which hasn't. Each key can be assigned a\n    // population phase which determines to which range it belongs, as well as which\n    // snapshot it reflects. The methods snapshot_of() and phase_of() can\n    // be used to determine this.\n    //\n    // In general, reads are allowed to populate given range only if the phase\n    // of the snapshot they use matches the phase of all keys in that range\n    // when the population is committed. This guarantees that the range will\n    // be reached by the update process or already has been in its entirety.\n    // In case of phase conflict, current solution is to give up on\n    // population. Since the update process is a scan, it's sufficient to\n    // check when committing the population if the start and end of the range\n    // have the same phases and that it's the same phase as that of the start\n    // of the range at the time when reading began.\n\n    mutation_source _underlying;\n    phase_type _underlying_phase = partition_snapshot::min_phase;\n    mutation_source_opt _prev_snapshot;\n\n    // Positions >= than this are using _prev_snapshot, the rest is using _underlying.\n    std::optional<dht::ring_position_ext> _prev_snapshot_pos;\n\n    snapshot_source _snapshot_source;\n\n    // There can be at most one update in progress.\n    seastar::semaphore _update_sem = {1};\n\n    logalloc::allocating_section _update_section;\n    logalloc::allocating_section _populate_section;\n    logalloc::allocating_section _read_section;\n    mutation_reader create_underlying_reader(cache::read_context&, mutation_source&, const dht::partition_range&);\n    mutation_reader make_scanning_reader(const dht::partition_range&, std::unique_ptr<cache::read_context>);\n    void on_partition_hit();\n    void on_partition_miss();\n    void on_row_hit();\n    void on_row_miss();\n    void on_static_row_insert();\n    void on_mispopulate();\n    void upgrade_entry(cache_entry&);\n    void invalidate_locked(const dht::decorated_key&);\n    void clear_now() noexcept;\n    void clear_on_destruction() noexcept;\n\n    struct previous_entry_pointer {\n        std::optional<dht::decorated_key> _key;\n\n        previous_entry_pointer() = default; // Represents dht::ring_position_view::min()\n        previous_entry_pointer(dht::decorated_key key) : _key(std::move(key)) {};\n\n        // TODO: store iterator here to avoid key comparison\n    };\n\n    template<typename CreateEntry, typename VisitEntry>\n    requires requires(CreateEntry create, VisitEntry visit, partitions_type::iterator it, partitions_type::bound_hint hint) {\n        { create(it, hint) } -> std::same_as<partitions_type::iterator>;\n        { visit(it) } -> std::same_as<void>;\n    }\n    // Must be run under reclaim lock\n    cache_entry& do_find_or_create_entry(const dht::decorated_key& key, const previous_entry_pointer* previous,\n                                 CreateEntry&& create_entry, VisitEntry&& visit_entry);\n\n    // Ensures that partition entry for given key exists in cache and returns a reference to it.\n    // Prepares the entry for reading. \"phase\" must match the current phase of the entry.\n    //\n    // Since currently every entry has to have a complete tombstone, it has to be provided here.\n    // The entry which is returned will have the tombstone applied to it.\n    //\n    // Must be run under reclaim lock\n    cache_entry& find_or_create_incomplete(const partition_start& ps, row_cache::phase_type phase, const previous_entry_pointer* previous = nullptr);\n\n    // Creates (or touches) a cache entry for missing partition so that sstables are not\n    // poked again for it.\n    cache_entry& find_or_create_missing(const dht::decorated_key& key);\n\n    partitions_type::iterator partitions_end() {\n        return std::prev(_partitions.end());\n    }\n\n    // Only active phases are accepted.\n    // Reference valid only until next deferring point.\n    mutation_source& snapshot_for_phase(phase_type);\n\n    // Returns population phase for given position in the ring.\n    // snapshot_for_phase() can be called to obtain mutation_source for given phase, but\n    // only until the next deferring point.\n    // Should be only called outside update().\n    phase_type phase_of(dht::ring_position_view);\n\n    struct snapshot_and_phase {\n        mutation_source& snapshot;\n        phase_type phase;\n    };\n\n    // Optimized version of:\n    //\n    //  { snapshot_for_phase(phase_of(pos)), phase_of(pos) };\n    //\n    snapshot_and_phase snapshot_of(dht::ring_position_view pos);\n\n    static thread_local preemption_source default_preemption_source;\n\n    // Merges the memtable into cache with configurable logic for handling memtable entries.\n    // The Updater gets invoked for every entry in the memtable with a lower bound iterator\n    // into _partitions (cache_i), and the memtable entry.\n    // It is invoked inside allocating section and in the context of cache's allocator.\n    // All memtable entries will be removed.\n    template <typename Updater>\n    future<> do_update(external_updater, replica::memtable& m, Updater func, preemption_source&);\n\n    // Clears given memtable invalidating any affected cache elements.\n    void invalidate_sync(replica::memtable&) noexcept;\n\n    // A function which updates cache to the current snapshot.\n    // It's responsible for advancing _prev_snapshot_pos between deferring points.\n    //\n    // Must have strong failure guarantees. Upon failure, it should still leave the cache\n    // in a state consistent with the update it is performing.\n    using internal_updater = std::function<future<>()>;\n\n    // Atomically updates the underlying mutation source and synchronizes the cache.\n    //\n    // Strong failure guarantees. If returns a failed future, the underlying mutation\n    // source was and cache are not modified.\n    //\n    // internal_updater is only kept alive until its invocation returns.\n    future<> do_update(external_updater eu, internal_updater iu) noexcept;\n\npublic:\n    ~row_cache();\n    row_cache(schema_ptr, snapshot_source, cache_tracker&, is_continuous = is_continuous::no);\n    row_cache(row_cache&&) = default;\n    row_cache(const row_cache&) = delete;\npublic:\n    // Implements mutation_source for this cache, see mutation_reader.hh\n    // User needs to ensure that the row_cache object stays alive\n    // as long as the reader is used.\n    // The range must not wrap around.\n\n    mutation_reader make_reader(schema_ptr s,\n                                     reader_permit permit,\n                                     const dht::partition_range& range,\n                                     const query::partition_slice& slice,\n                                     tracing::trace_state_ptr trace_state = nullptr,\n                                     streamed_mutation::forwarding fwd = streamed_mutation::forwarding::no,\n                                     mutation_reader::forwarding fwd_mr = mutation_reader::forwarding::no,\n                                     const tombstone_gc_state* gc_state = nullptr) {\n        if (auto reader_opt = make_reader_opt(s, permit, range, slice, gc_state, std::move(trace_state), fwd, fwd_mr)) {\n            return std::move(*reader_opt);\n        }\n        [[unlikely]] return make_empty_flat_reader_v2(std::move(s), std::move(permit));\n    }\n    // Same as make_reader, but returns an empty optional instead of a no-op reader when there is nothing to\n    // read. This is an optimization.\n    mutation_reader_opt make_reader_opt(schema_ptr,\n                                     reader_permit permit,\n                                     const dht::partition_range&,\n                                     const query::partition_slice&,\n                                     const tombstone_gc_state*,\n                                     tracing::trace_state_ptr trace_state = nullptr,\n                                     streamed_mutation::forwarding fwd = streamed_mutation::forwarding::no,\n                                     mutation_reader::forwarding fwd_mr = mutation_reader::forwarding::no);\n\n    mutation_reader make_reader(schema_ptr s,\n                                    reader_permit permit,\n                                    const dht::partition_range& range = query::full_partition_range,\n                                    const tombstone_gc_state* gc_state = nullptr) {\n        auto& full_slice = s->full_slice();\n        return make_reader(std::move(s), std::move(permit), range, full_slice, nullptr,\n                streamed_mutation::forwarding::no, mutation_reader::forwarding::no, gc_state);\n    }\n\n    // Only reads what is in the cache, doesn't populate.\n    // Supports reading singular ranges only, for now.\n    // Does not support reading in reverse.\n    mutation_reader make_nonpopulating_reader(schema_ptr s, reader_permit permit, const dht::partition_range& range,\n            const query::partition_slice& slice, tracing::trace_state_ptr ts);\n\n    const stats& stats() const { return _stats; }\npublic:\n    // Populate cache from given mutation, which must be fully continuous.\n    // Intended to be used only in tests.\n    // Can only be called prior to any reads.\n    void populate(const mutation& m, const previous_entry_pointer* previous = nullptr);\n\n    // Finds the entry in cache for a given key.\n    // Intended to be used only in tests.\n    cache_entry& lookup(const dht::decorated_key& key);\n\n    // Synchronizes cache with the underlying data source from a memtable which\n    // has just been flushed to the underlying data source.\n    // The memtable can be queried during the process, but must not be written.\n    // After the update is complete, memtable is empty.\n    future<> update(external_updater, replica::memtable&, preemption_source& preempt = default_preemption_source);\n\n    // Like update(), synchronizes cache with an incremental change to the underlying\n    // mutation source, but instead of inserting and merging data, invalidates affected ranges.\n    // Can be thought of as a more fine-grained version of invalidate(), which invalidates\n    // as few elements as possible.\n    future<> update_invalidating(external_updater, replica::memtable&);\n\n    // Refreshes snapshot. Must only be used if logical state in the underlying data\n    // source hasn't changed.\n    void refresh_snapshot();\n\n    // Moves given partition to the front of LRU if present in cache.\n    void touch(const dht::decorated_key&);\n\n    // Detaches current contents of given partition from LRU, so\n    // that they are not evicted by memory reclaimer.\n    void unlink_from_lru(const dht::decorated_key&);\n\n    // Synchronizes cache with the underlying mutation source\n    // by invalidating ranges which were modified. This will force\n    // them to be re-read from the underlying mutation source\n    // during next read overlapping with the invalidated ranges.\n    //\n    // The ranges passed to invalidate() must include all\n    // data which changed since last synchronization. Failure\n    // to do so may result in reads seeing partial writes,\n    // which would violate write atomicity.\n    //\n    // Guarantees that readers created after invalidate()\n    // completes will see all writes from the underlying\n    // mutation source made prior to the call to invalidate().\n    future<> invalidate(external_updater, const dht::decorated_key&);\n    future<> invalidate(external_updater, const dht::partition_range& = query::full_partition_range);\n    future<> invalidate(external_updater, dht::partition_range_vector&&);\n\n    // Evicts entries from cache.\n    //\n    // Note that this does not synchronize with the underlying source,\n    // it is assumed that the underlying source didn't change.\n    // If it did, use invalidate() instead.\n    void evict();\n\n    const cache_tracker& get_cache_tracker() const {\n        return _tracker;\n    }\n    cache_tracker& get_cache_tracker() {\n        return _tracker;\n    }\n\n    void set_schema(schema_ptr) noexcept;\n    const schema_ptr& schema() const;\n\n    friend std::ostream& operator<<(std::ostream&, row_cache&);\n\n    friend class just_cache_scanning_reader;\n    friend class scanning_and_populating_reader;\n    friend class range_populating_reader;\n    friend class cache_tracker;\n    friend class mark_end_as_continuous;\n};\n\nnamespace cache {\n\nclass lsa_manager {\n    row_cache &_cache;\npublic:\n    lsa_manager(row_cache &cache) : _cache(cache) {}\n\n    template<typename Func>\n    decltype(auto) run_in_read_section(const Func &func) {\n        return _cache._read_section(_cache._tracker.region(), [&func]() {\n            return func();\n        });\n    }\n\n    template<typename Func>\n    decltype(auto) run_in_update_section(const Func &func) {\n        return _cache._update_section(_cache._tracker.region(), [&func]() {\n            return func();\n        });\n    }\n\n    template<typename Func>\n    void run_in_update_section_with_allocator(Func &&func) {\n        return _cache._update_section(_cache._tracker.region(), [this, &func]() {\n            return with_allocator(_cache._tracker.region().allocator(), [&func]() mutable {\n                return func();\n            });\n        });\n    }\n\n    logalloc::region &region() { return _cache._tracker.region(); }\n\n    logalloc::allocating_section &read_section() { return _cache._read_section; }\n};\n\n}\n\ntemplate <> struct fmt::formatter<cache_entry> : fmt::formatter<string_view> {\n    auto format(const cache_entry&, fmt::format_context& ctx) const -> decltype(ctx.out());\n};\n"
        },
        {
          "name": "rust",
          "type": "tree",
          "content": null
        },
        {
          "name": "schema",
          "type": "tree",
          "content": null
        },
        {
          "name": "schema_mutations.cc",
          "type": "blob",
          "size": 7.1904296875,
          "content": "/*\n * Copyright 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"schema_mutations.hh\"\n#include \"mutation/canonical_mutation.hh\"\n#include \"db/schema_tables.hh\"\n#include \"utils/hashers.hh\"\n#include \"utils/UUID_gen.hh\"\n\nschema_mutations::schema_mutations(canonical_mutation columnfamilies,\n                                   canonical_mutation columns,\n                                   bool is_view,\n                                   std::optional<canonical_mutation> indices,\n                                   std::optional<canonical_mutation> dropped_columns,\n                                   std::optional<canonical_mutation> scylla_tables,\n                                   std::optional<canonical_mutation> view_virtual_columns,\n                                   std::optional<canonical_mutation> computed_columns)\n    : _columnfamilies(columnfamilies.to_mutation(is_view ? db::schema_tables::views() : db::schema_tables::tables()))\n    , _columns(columns.to_mutation(db::schema_tables::columns()))\n    , _view_virtual_columns(view_virtual_columns ? mutation_opt{view_virtual_columns.value().to_mutation(db::schema_tables::view_virtual_columns())} : std::nullopt)\n    , _computed_columns(computed_columns ? mutation_opt{computed_columns.value().to_mutation(db::schema_tables::computed_columns())} : std::nullopt)\n    , _indices(indices ? mutation_opt{indices.value().to_mutation(db::schema_tables::indexes())} : std::nullopt)\n    , _dropped_columns(dropped_columns ? mutation_opt{dropped_columns.value().to_mutation(db::schema_tables::dropped_columns())} : std::nullopt)\n    , _scylla_tables(scylla_tables ? mutation_opt{scylla_tables.value().to_mutation(db::schema_tables::scylla_tables())} : std::nullopt)\n{}\n\nvoid schema_mutations::copy_to(std::vector<mutation>& dst) const {\n    dst.push_back(_columnfamilies);\n    dst.push_back(_columns);\n    if (_view_virtual_columns) {\n        dst.push_back(*_view_virtual_columns);\n    }\n    if (_computed_columns) {\n        dst.push_back(*_computed_columns);\n    }\n    if (_indices) {\n        dst.push_back(*_indices);\n    }\n    if (_dropped_columns) {\n        dst.push_back(*_dropped_columns);\n    }\n    if (_scylla_tables) {\n        dst.push_back(*_scylla_tables);\n    }\n}\n\ntable_schema_version schema_mutations::digest(db::schema_features sf) const {\n    if (_scylla_tables) {\n        auto rs = query::result_set(*_scylla_tables);\n        if (!rs.empty()) {\n            auto&& row = rs.row(0);\n            auto val = row.get<utils::UUID>(\"version\");\n            if (val) {\n                return table_schema_version(*val);\n            }\n        }\n    }\n\n    md5_hasher h;\n\n    if (!sf.contains<db::schema_feature::TABLE_DIGEST_INSENSITIVE_TO_EXPIRY>()) {\n        // Disable this feature so that the digest remains compactible with Scylla\n        // versions prior to this feature.\n        // This digest affects the table schema version calculation and it's important\n        // that all nodes arrive at the same table schema version to avoid needless schema version\n        // pulls. It used to be the case that when table schema versions were calculated on boot we\n        // didn't yet know all the cluster features, so we could get different table versions after reboot\n        // in an already upgraded cluster. However, they are now available, and if\n        // TABLE_DIGEST_INSENSITIVE_TO_EXPIRY is enabled, we can compute with DIGEST_INSENSITIVE_TO_EXPIRY\n        // enabled.\n        sf.remove<db::schema_feature::DIGEST_INSENSITIVE_TO_EXPIRY>();\n    }\n\n    db::schema_tables::feed_hash_for_schema_digest(h, _columnfamilies, sf);\n    db::schema_tables::feed_hash_for_schema_digest(h, _columns, sf);\n    if (_view_virtual_columns && !_view_virtual_columns->partition().empty()) {\n        db::schema_tables::feed_hash_for_schema_digest(h, *_view_virtual_columns, sf);\n    }\n    if (_computed_columns && !_computed_columns->partition().empty()) {\n        db::schema_tables::feed_hash_for_schema_digest(h, *_computed_columns, sf);\n    }\n    if (_indices && !_indices->partition().empty()) {\n        db::schema_tables::feed_hash_for_schema_digest(h, *_indices, sf);\n    }\n    if (_dropped_columns && !_dropped_columns->partition().empty()) {\n        db::schema_tables::feed_hash_for_schema_digest(h, *_dropped_columns, sf);\n    }\n    if (_scylla_tables) {\n        db::schema_tables::feed_hash_for_schema_digest(h, *_scylla_tables, sf);\n    }\n    return table_schema_version(utils::UUID_gen::get_name_UUID(h.finalize()));\n}\n\nstd::optional<sstring> schema_mutations::partitioner() const {\n    if (_scylla_tables) {\n        auto rs = query::result_set(*_scylla_tables);\n        if (!rs.empty()) {\n            return rs.row(0).get<sstring>(\"partitioner\");\n        }\n    }\n    return { };\n}\n\nstatic mutation_opt compact(const mutation_opt& m) {\n    if (!m) {\n        return m;\n    }\n    return db::schema_tables::compact_for_schema_digest(*m);\n}\n\nstatic mutation_opt compact(const mutation& m) {\n    return db::schema_tables::compact_for_schema_digest(m);\n}\n\nbool schema_mutations::operator==(const schema_mutations& other) const {\n    return compact(_columnfamilies) == compact(other._columnfamilies)\n           && compact(_columns) == compact(other._columns)\n           && compact(_view_virtual_columns) == compact(other._view_virtual_columns)\n           && compact(_computed_columns) == compact(other._computed_columns)\n           && compact(_indices) == compact(other._indices)\n           && compact(_dropped_columns) == compact(other._dropped_columns)\n           && compact(_scylla_tables) == compact(other._scylla_tables)\n           ;\n}\n\nbool schema_mutations::live() const {\n    return _columnfamilies.live_row_count() > 0 || _columns.live_row_count() > 0 ||\n            (_view_virtual_columns && _view_virtual_columns->live_row_count() > 0) ||\n            (_computed_columns && _computed_columns->live_row_count() > 0);\n}\n\nbool schema_mutations::is_view() const {\n    return _columnfamilies.schema() == db::schema_tables::views();\n}\n\nauto fmt::formatter<schema_mutations>::format(const schema_mutations& sm, fmt::format_context& ctx) const\n        -> decltype(ctx.out()) {\n    auto out = fmt::format_to(ctx.out(), \"schema_mutations{{\\n\");\n    out = fmt::format_to(out, \" tables={},\\n\", sm.columnfamilies_mutation());\n    out = fmt::format_to(out, \" scylla_tables={},\\n\", sm.scylla_tables());\n    out = fmt::format_to(out, \" tables={},\\n\", sm.columns_mutation());\n    out = fmt::format_to(out, \" dropped_columns={},\\n\", sm.dropped_columns_mutation());\n    out = fmt::format_to(out, \" indices={},\\n\", sm.indices_mutation());\n    out = fmt::format_to(out, \" computed_columns={},\\n\", sm.computed_columns_mutation());\n    out = fmt::format_to(out, \" view_virtual_columns={},\\n\", sm.view_virtual_columns_mutation());\n    return fmt::format_to(out, \"}}\");\n}\n\nschema_mutations& schema_mutations::operator+=(schema_mutations&& sm) {\n    _columnfamilies += std::move(sm._columnfamilies);\n    _columns += std::move(sm._columns);\n    apply(_computed_columns, std::move(sm._computed_columns));\n    apply(_view_virtual_columns, std::move(sm._view_virtual_columns));\n    apply(_indices, std::move(sm._indices));\n    apply(_dropped_columns, std::move(sm._dropped_columns));\n    apply(_scylla_tables, std::move(sm._scylla_tables));\n    return *this;\n}\n"
        },
        {
          "name": "schema_mutations.hh",
          "type": "blob",
          "size": 4.5087890625,
          "content": "/*\n * Copyright 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <vector>\n#include \"mutation/mutation.hh\"\n#include \"schema/schema_fwd.hh\"\n#include \"mutation/canonical_mutation.hh\"\n#include \"db/schema_features.hh\"\n\n// Commutative representation of table schema\n// Equality ignores tombstones.\nclass schema_mutations {\n    mutation _columnfamilies;\n    mutation _columns;\n    mutation_opt _view_virtual_columns;\n    mutation_opt _computed_columns;\n    mutation_opt _indices;\n    mutation_opt _dropped_columns;\n    mutation_opt _scylla_tables;\npublic:\n    schema_mutations(mutation columnfamilies, mutation columns, mutation_opt view_virtual_columns, mutation_opt computed_columns, mutation_opt indices, mutation_opt dropped_columns,\n        mutation_opt scylla_tables)\n            : _columnfamilies(std::move(columnfamilies))\n            , _columns(std::move(columns))\n            , _view_virtual_columns(std::move(view_virtual_columns))\n            , _computed_columns(std::move(computed_columns))\n            , _indices(std::move(indices))\n            , _dropped_columns(std::move(dropped_columns))\n            , _scylla_tables(std::move(scylla_tables))\n    { }\n    schema_mutations(canonical_mutation columnfamilies,\n                     canonical_mutation columns,\n                     bool is_view,\n                     std::optional<canonical_mutation> indices,\n                     std::optional<canonical_mutation> dropped_columns,\n                     std::optional<canonical_mutation> scylla_tables,\n                     std::optional<canonical_mutation> view_virtual_columns,\n                     std::optional<canonical_mutation> computed_columns);\n\n    schema_mutations(schema_mutations&&) = default;\n    schema_mutations& operator=(schema_mutations&&) = default;\n    schema_mutations(const schema_mutations&) = default;\n    schema_mutations& operator=(const schema_mutations&) = default;\n\n    void copy_to(std::vector<mutation>& dst) const;\n\n    const mutation& columnfamilies_mutation() const {\n        return _columnfamilies;\n    }\n\n    const mutation& columns_mutation() const {\n        return _columns;\n    }\n\n    const mutation_opt& view_virtual_columns_mutation() const {\n        return _view_virtual_columns;\n    }\n\n    const mutation_opt& computed_columns_mutation() const {\n        return _computed_columns;\n    }\n\n    const mutation_opt& scylla_tables() const {\n        return _scylla_tables;\n    }\n\n    mutation_opt& scylla_tables() {\n        return _scylla_tables;\n    }\n\n    const mutation_opt& indices_mutation() const {\n        return _indices;\n    }\n    const mutation_opt& dropped_columns_mutation() const {\n        return _dropped_columns;\n    }\n\n    canonical_mutation columnfamilies_canonical_mutation() const {\n        return canonical_mutation(_columnfamilies);\n    }\n\n    canonical_mutation columns_canonical_mutation() const {\n        return canonical_mutation(_columns);\n    }\n\n    std::optional<canonical_mutation> view_virtual_columns_canonical_mutation() const {\n        if (_view_virtual_columns) {\n            return canonical_mutation(*_view_virtual_columns);\n        }\n        return {};\n    }\n\n    std::optional<canonical_mutation> computed_columns_canonical_mutation() const {\n        if (_computed_columns) {\n            return canonical_mutation(*_computed_columns);\n        }\n        return {};\n    }\n\n    std::optional<canonical_mutation> indices_canonical_mutation() const {\n        if (_indices) {\n            return canonical_mutation(*_indices);\n        }\n        return {};\n    }\n    std::optional<canonical_mutation> dropped_columns_canonical_mutation() const {\n        if (_dropped_columns) {\n            return canonical_mutation(*_dropped_columns);\n        }\n        return {};\n    }\n    std::optional<canonical_mutation> scylla_tables_canonical_mutation() const {\n        if (_scylla_tables) {\n            return canonical_mutation(*_scylla_tables);\n        }\n        return {};\n    }\n\n    bool is_view() const;\n\n    table_schema_version digest(db::schema_features) const;\n    std::optional<sstring> partitioner() const;\n\n    bool operator==(const schema_mutations&) const;\n    schema_mutations& operator+=(schema_mutations&&);\n\n    // Returns true iff any mutations contain any live cells\n    bool live() const;\n\n    friend std::ostream& operator<<(std::ostream&, const schema_mutations&);\n};\n\ntemplate <> struct fmt::formatter<schema_mutations> : fmt::formatter<string_view> {\n    auto format(const schema_mutations&, fmt::format_context& ctx) const -> decltype(ctx.out());\n};\n"
        },
        {
          "name": "schema_upgrader.hh",
          "type": "blob",
          "size": 4.123046875,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"mutation/mutation_fragment.hh\"\n#include \"mutation/mutation_fragment_v2.hh\"\n#include \"converting_mutation_partition_applier.hh\"\n\n// A StreamedMutationTransformer which transforms the stream to a different schema\nclass schema_upgrader {\n    schema_ptr _prev;\n    schema_ptr _new;\n    std::optional<reader_permit> _permit;\nprivate:\n    row transform(row&& r, column_kind kind) {\n        row new_row;\n        r.for_each_cell([&] (column_id id, atomic_cell_or_collection& cell) {\n            const column_definition& col = _prev->column_at(kind, id);\n            const column_definition* new_col = _new->get_column_definition(col.name());\n            if (new_col) {\n                converting_mutation_partition_applier::append_cell(new_row, kind, *new_col, col, std::move(cell));\n            }\n        });\n        return new_row;\n    }\npublic:\n    schema_upgrader(schema_ptr s)\n        : _new(std::move(s))\n    { }\n    schema_ptr operator()(schema_ptr old) {\n        _prev = std::move(old);\n        return _new;\n    }\n    mutation_fragment consume(static_row&& row) {\n        return mutation_fragment(*_new, std::move(*_permit), static_row(transform(std::move(row.cells()), column_kind::static_column)));\n    }\n    mutation_fragment consume(clustering_row&& row) {\n        return mutation_fragment(*_new, std::move(*_permit), clustering_row(row.key(), row.tomb(), row.marker(),\n            transform(std::move(row.cells()), column_kind::regular_column)));\n    }\n    mutation_fragment consume(range_tombstone&& rt) {\n        return mutation_fragment(*_new, std::move(*_permit), std::move(rt));\n    }\n    mutation_fragment consume(partition_start&& ph) {\n        return mutation_fragment(*_new, std::move(*_permit), std::move(ph));\n    }\n    mutation_fragment consume(partition_end&& eop) {\n        return mutation_fragment(*_new, std::move(*_permit), std::move(eop));\n    }\n    mutation_fragment operator()(mutation_fragment&& mf) {\n        _permit = mf.permit();\n        return std::move(mf).consume(*this);\n    }\n};\n\n// A StreamedMutationTransformer which transforms the stream to a different schema\nclass schema_upgrader_v2 {\n    schema_ptr _prev;\n    schema_ptr _new;\n    std::optional<reader_permit> _permit;\nprivate:\n    row transform(row&& r, column_kind kind) {\n        row new_row;\n        r.for_each_cell([&] (column_id id, atomic_cell_or_collection& cell) {\n            const column_definition& col = _prev->column_at(kind, id);\n            const column_definition* new_col = _new->get_column_definition(col.name());\n            if (new_col) {\n                converting_mutation_partition_applier::append_cell(new_row, kind, *new_col, col, std::move(cell));\n            }\n        });\n        return new_row;\n    }\npublic:\n    schema_upgrader_v2(schema_ptr s)\n        : _new(std::move(s))\n    { }\n    schema_ptr operator()(schema_ptr old) {\n        _prev = std::move(old);\n        return _new;\n    }\n    mutation_fragment_v2 consume(static_row&& row) {\n        return mutation_fragment_v2(*_new, std::move(*_permit), static_row(transform(std::move(row.cells()), column_kind::static_column)));\n    }\n    mutation_fragment_v2 consume(clustering_row&& row) {\n        return mutation_fragment_v2(*_new, std::move(*_permit), clustering_row(row.key(), row.tomb(), row.marker(),\n            transform(std::move(row.cells()), column_kind::regular_column)));\n    }\n    mutation_fragment_v2 consume(range_tombstone_change&& rt) {\n        return mutation_fragment_v2(*_new, std::move(*_permit), std::move(rt));\n    }\n    mutation_fragment_v2 consume(partition_start&& ph) {\n        return mutation_fragment_v2(*_new, std::move(*_permit), std::move(ph));\n    }\n    mutation_fragment_v2 consume(partition_end&& eop) {\n        return mutation_fragment_v2(*_new, std::move(*_permit), std::move(eop));\n    }\n    mutation_fragment_v2 operator()(mutation_fragment_v2&& mf) {\n        _permit = mf.permit();\n        return std::move(mf).consume(*this);\n    }\n};\n\nstatic_assert(StreamedMutationTranformer<schema_upgrader>);\nstatic_assert(StreamedMutationTranformerV2<schema_upgrader_v2>);\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "scylla-gdb.py",
          "type": "blob",
          "size": 238.2607421875,
          "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport gdb\nimport gdb.printing\nimport uuid\nimport argparse\nimport datetime\nimport functools\nimport re\nfrom operator import attrgetter\nfrom collections import defaultdict\nimport sys\nimport struct\nimport random\nimport bisect\nimport os\nimport subprocess\nimport time\nimport socket\nimport string\nimport math\n\n\ndef align_up(ptr, alignment):\n    res = ptr % alignment\n    if not res:\n        return ptr\n    return ptr + alignment - res\n\n\ndef template_arguments(gdb_type):\n    n = 0\n    while True:\n        try:\n            yield gdb_type.template_argument(n)\n            n += 1\n        except RuntimeError:\n            return\n\n\ndef get_template_arg_with_prefix(gdb_type, prefix):\n    for arg in template_arguments(gdb_type):\n        if str(arg).startswith(prefix):\n            return arg\n\n\ndef get_base_class_offset(gdb_type, base_class_name):\n    name_pattern = re.escape(base_class_name) + \"(<.*>)?$\"\n    for field in gdb_type.fields():\n        if field.is_base_class:\n            field_offset = int(field.bitpos / 8)\n            if re.match(name_pattern, field.type.strip_typedefs().name):\n                return field_offset\n            offset = get_base_class_offset(field.type, base_class_name)\n            if offset is not None:\n                return field_offset + offset\n\n\ndef get_field_offset(gdb_type, name):\n    for field in gdb_type.fields():\n        if field.name == name:\n            return int(field.bitpos / 8)\n\n@functools.cache\ndef size_t():\n    return gdb.lookup_type('size_t')\n\n@functools.cache\ndef _vptr_type():\n    return gdb.lookup_type('uintptr_t').pointer()\n\n# Defer initialization to first use\n# Don't prevent loading `scylla-gdb.py` due to any problem in the init code.\nvtable_symbol_pattern = None\nvptr_type = None\n\n\ndef _check_vptr(ptr):\n    global vtable_symbol_pattern\n    global vptr_type\n    symbol_name = resolve(ptr.reinterpret_cast(vptr_type).dereference(), startswith='vtable for ')\n    if symbol_name is None:\n        raise ValueError(\"Failed to resolve first word of virtual object @ {} as a vtable symbol\".format(int(ptr)))\n\n    m = re.match(vtable_symbol_pattern, symbol_name)\n    if m is None:\n        raise ValueError(\"Failed to extract type name from symbol name `{}'\".format(symbol_name))\n\n    return m\n\n\ndef downcast_vptr(ptr):\n    global vtable_symbol_pattern\n    global vptr_type\n    if vtable_symbol_pattern is None:\n        vtable_symbol_pattern = re.compile(r'vtable for (.*) \\+ ([0-9]+).*')\n        vptr_type = gdb.lookup_type('uintptr_t').pointer()\n\n    if not isinstance(ptr, gdb.Value):\n        ptr = gdb.parse_and_eval(ptr)\n\n    m = _check_vptr(ptr)\n\n    actual_type = gdb.lookup_type(m.group(1))\n    actual_type_ptr = actual_type.pointer()\n\n    if int(m.group(2)) == 16:\n        return ptr.reinterpret_cast(actual_type_ptr)\n\n    # We are most likely dealing with multiple inheritance and a pointer to a\n    # non-first base-class type.\n    base_class_field = actual_type.fields()[0]\n    assert base_class_field.is_base_class\n    base_classes = list(base_class_field.type.fields())\n\n    # The pointer is surely not to the first base-class, we would have found\n    # the expected m.group(2) == 16 offset otherwise.\n    for bc in base_classes[1:]:\n        speculative_ptr = gdb.Value(int(ptr) - int(bc.bitpos / 8))\n        m = _check_vptr(speculative_ptr)\n        if int(m.group(2)) == 16:\n            return speculative_ptr.reinterpret_cast(actual_type_ptr)\n\n    return None\n\n\nclass intrusive_list:\n    def __init__(self, list_ref, link=None):\n        list_type = list_ref.type.strip_typedefs()\n        self.node_type = list_type.template_argument(0)\n        rps = list_ref['data_']['root_plus_size_']\n        try:\n            self.root = rps['root_']\n        except gdb.error:\n            # Some boost versions have this instead\n            self.root = rps['m_header']\n        if link is not None:\n            self.link_offset = get_field_offset(self.node_type, link)\n        else:\n            member_hook = get_template_arg_with_prefix(list_type, \"boost::intrusive::member_hook\")\n            if not member_hook:\n                member_hook = get_template_arg_with_prefix(list_type, \"struct boost::intrusive::member_hook\")\n            if member_hook:\n                self.link_offset = member_hook.template_argument(2).cast(size_t())\n            else:\n                self.link_offset = get_base_class_offset(self.node_type, \"boost::intrusive::list_base_hook\")\n                if self.link_offset is None:\n                    raise Exception(\"Class does not extend list_base_hook: \" + str(self.node_type))\n\n    def __iter__(self):\n        hook = self.root['next_']\n        while hook and hook != self.root.address:\n            node_ptr = hook.cast(size_t()) - self.link_offset\n            yield node_ptr.cast(self.node_type.pointer()).dereference()\n            hook = hook['next_']\n\n    def __nonzero__(self):\n        return self.root['next_'] != self.root.address\n\n    def __bool__(self):\n        return self.__nonzero__()\n\n    def __len__(self):\n        return len(list(iter(self)))\n\n\nclass intrusive_slist:\n    def __init__(self, list_ref, link=None):\n        list_type = list_ref.type.strip_typedefs()\n        self.node_type = list_type.template_argument(0)\n        rps = list_ref['data_']['root_plus_size_']\n        self.root = rps['header_holder_']\n\n        if link is not None:\n            self.link_offset = get_field_offset(self.node_type, link)\n        else:\n            member_hook = get_template_arg_with_prefix(list_type, \"struct boost::intrusive::member_hook\")\n            if member_hook:\n                self.link_offset = member_hook.template_argument(2).cast(size_t())\n            else:\n                self.link_offset = get_base_class_offset(self.node_type, \"boost::intrusive::slist_base_hook\")\n                if self.link_offset is None:\n                    raise Exception(\"Class does not extend slist_base_hook: \" + str(self.node_type))\n\n    def __iter__(self):\n        hook = self.root['next_']\n        while hook != self.root.address:\n            node_ptr = hook.cast(size_t()) - self.link_offset\n            yield node_ptr.cast(self.node_type.pointer()).dereference()\n            hook = hook['next_']\n\n    def __nonzero__(self):\n        return self.root['next_'] != self.root.address\n\n    def __bool__(self):\n        return self.__nonzero__()\n\n    def __len__(self):\n        return len(list(iter(self)))\n\n\nclass std_optional:\n    def __init__(self, ref):\n        self.ref = ref\n\n    def get(self):\n        return self.ref['_M_payload']['_M_payload']['_M_value']\n\n    def __bool__(self):\n        return self.__nonzero__()\n\n    def __nonzero__(self):\n        return bool(self.ref['_M_payload']['_M_engaged'])\n\n\nclass std_tuple:\n    def __init__(self, ref):\n        self.ref = ref\n        self.members = []\n\n        t = self.ref[self.ref.type.fields()[0]]\n        while len(t.type.fields()) == 2:\n            tail, head = t.type.fields()\n            self.members.append(t[head]['_M_head_impl'])\n            t = t[tail]\n\n        self.members.append(t[t.type.fields()[0]]['_M_head_impl'])\n\n    def __len__(self):\n        return len(self.members)\n\n    def __iter__(self):\n        return iter(self.members)\n\n    def __getitem__(self, item):\n        return self.members[item]\n\n\nclass intrusive_set:\n    def __init__(self, ref, link=None):\n        container_type = ref.type.strip_typedefs()\n        self.node_type = container_type.template_argument(0)\n        if link is not None:\n            self.link_offset = get_field_offset(self.node_type, link)\n        else:\n            member_hook = get_template_arg_with_prefix(container_type, \"boost::intrusive::member_hook\")\n            if not member_hook:\n                raise Exception('Expected member_hook<> option not found in container\\'s template parameters')\n            self.link_offset = member_hook.template_argument(2).cast(size_t())\n        self.root = ref['holder']['root']['parent_']\n\n    def __visit(self, node):\n        if node:\n            for n in self.__visit(node['left_']):\n                yield n\n\n            node_ptr = node.cast(size_t()) - self.link_offset\n            yield node_ptr.cast(self.node_type.pointer()).dereference()\n\n            for n in self.__visit(node['right_']):\n                yield n\n\n    def __iter__(self):\n        for n in self.__visit(self.root):\n            yield n\n\n\nclass compact_radix_tree:\n    def __init__(self, ref):\n        self.root = ref['_root']['_v']\n\n    def to_string(self):\n        if self.root['_base_layout'] == 0:\n            return '<empty>'\n\n        # Compiler optimizes-away lots of critical stuff, so\n        # for now just show where the tree is\n        return 'compact radix tree @ 0x%x' % self.root\n\n\nclass intrusive_btree:\n    def __init__(self, ref):\n        container_type = ref.type.strip_typedefs()\n        self.tree = ref\n        self.leaf_node_flag = gdb.parse_and_eval('intrusive_b::node_base::NODE_LEAF')\n        self.key_type = container_type.template_argument(0)\n\n    def __visit_node_base(self, base, kids):\n        for i in range(0, base['num_keys']):\n            if kids:\n                for r in self.__visit_node(kids[i]):\n                    yield r\n\n            yield base['keys'][i].cast(self.key_type.pointer()).dereference()\n\n        if kids:\n            for r in self.__visit_node(kids[base['num_keys']]):\n                yield r\n\n    def __visit_node(self, node):\n        base = node['_base']\n        kids = node['_kids'] if not base['flags'] & self.leaf_node_flag else None\n\n        for r in self.__visit_node_base(base, kids):\n            yield r\n\n    def __iter__(self):\n        if self.tree['_root']:\n            for r in self.__visit_node(self.tree['_root']):\n                yield r\n        else:\n            for r in self.__visit_node_base(self.tree['_inline'], None):\n                yield r\n\n\nclass bplus_tree:\n    def __init__(self, ref):\n        self.tree = ref\n        self.leaf_node_flag = int(gdb.parse_and_eval(self.tree.type.name + \"::node::NODE_LEAF\"))\n        self.rightmost_leaf_flag = int(gdb.parse_and_eval(self.tree.type.name + \"::node::NODE_RIGHTMOST\"))\n\n    def __len__(self):\n        i = 0\n        for _ in self:\n            i += 1\n        return i\n\n    def __iter__(self):\n        node_p = self.tree['_left']\n        while node_p:\n            node = node_p.dereference()\n            if not node['_flags'] & self.leaf_node_flag:\n                raise ValueError(\"Expected B+ leaf node\")\n\n            for i in range(0, node['_num_keys']):\n                yield node['_kids'][i+1]['d'].dereference()['value']\n\n            if node['_flags'] & self.rightmost_leaf_flag:\n                node_p = None\n            else:\n                node_p = node['__next']\n\n\nclass double_decker:\n    def __init__(self, ref):\n        self.tree = ref['_tree']\n        self.leaf_node_flag = int(gdb.parse_and_eval(self.tree.type.name + \"::node::NODE_LEAF\"))\n        self.rightmost_leaf_flag = int(gdb.parse_and_eval(self.tree.type.name + \"::node::NODE_RIGHTMOST\"))\n        self.max_conflicting_partitions = 128\n\n    def __iter__(self):\n        node_p = self.tree['_left']\n        while node_p:\n            node = node_p.dereference()\n            if not node['_flags'] & self.leaf_node_flag:\n                raise ValueError(\"Expected B+ leaf node\")\n\n            for i in range(0, node['_num_keys']):\n                parts = node['_kids'][i+1]['d'].dereference()['value']\n                p = 0\n                while True:\n                    ce = parts['_data'][p]['object']\n                    if p == 0 and not ce['_flags']['_head']:\n                        raise ValueError(\"Expected head cache_entry\")\n                    yield ce\n                    if ce['_flags']['_tail']:\n                        break\n                    if p >= self.max_conflicting_partitions:\n                        raise ValueError(\"Too many conflicting partitions\")\n                    p += 1\n\n            if node['_flags'] & self.rightmost_leaf_flag:\n                node_p = None\n            else:\n                node_p = node['__next']\n\n\nclass boost_variant:\n    def __init__(self, ref):\n        self.ref = ref\n\n    def which(self):\n        return self.ref['which_']\n\n    def type(self):\n        return self.ref.type.template_argument(self.ref['which_'])\n\n    def get(self):\n        return self.ref['storage_'].address.cast(self.type().pointer())\n\n\nclass std_variant:\n    \"\"\"Wrapper around and std::variant.\n\n    Call get() to access the current value.\n    \"\"\"\n    def __init__(self, ref):\n        self.ref = ref\n        self.member_types = list(template_arguments(self.ref.type))\n\n    def index(self):\n        return int(self.ref['_M_index'])\n\n    # workaround for when template arguments refuses to work\n    def get_with_type(self, current_type):\n        index = self.index()\n        variadic_union = self.ref['_M_u']\n        for i in range(index):\n            variadic_union = variadic_union['_M_rest']\n\n        wrapper = variadic_union['_M_first']['_M_storage']\n        # literal types are stored directly in `_M_storage`.\n        if wrapper.type.strip_typedefs() == current_type:\n            return wrapper\n\n        # non-literal types are stored via a __gnu_cxx::__aligned_membuf\n        return wrapper['_M_storage'].reinterpret_cast(current_type.pointer()).dereference()\n\n    def get(self):\n        index = self.index()\n        current_type = self.member_types[index].strip_typedefs()\n        return self.get_with_type(index, current_type)\n\n\nclass std_map:\n    def __init__(self, ref):\n        container_type = ref.type.strip_typedefs()\n        kt = container_type.template_argument(0)\n        vt = container_type.template_argument(1)\n        self.value_type = gdb.lookup_type('::std::pair<{} const, {} >'.format(str(kt), str(vt)))\n        self.root = ref['_M_t']['_M_impl']['_M_header']['_M_parent']\n        self.size = int(ref['_M_t']['_M_impl']['_M_node_count'])\n\n    def __visit(self, node):\n        if node:\n            for n in self.__visit(node['_M_left']):\n                yield n\n\n            value = (node + 1).cast(self.value_type.pointer()).dereference()\n            yield value['first'], value['second']\n\n            for n in self.__visit(node['_M_right']):\n                yield n\n\n    def __iter__(self):\n        for n in self.__visit(self.root):\n            yield n\n\n    def __len__(self):\n        return self.size\n\n\nclass std_set:\n    def __init__(self, ref):\n        container_type = ref.type.strip_typedefs()\n        self.value_type = gdb.parse_and_eval(f'(({container_type}::value_type*)0)').type.target()\n        self.root = ref['_M_t']['_M_impl']['_M_header']['_M_parent']\n        self.size = int(ref['_M_t']['_M_impl']['_M_node_count'])\n\n    def __visit(self, node):\n        if node:\n            for n in self.__visit(node['_M_left']):\n                yield n\n\n            value = (node + 1).cast(self.value_type.pointer()).dereference()\n            yield value\n\n            for n in self.__visit(node['_M_right']):\n                yield n\n\n    def __iter__(self):\n        for n in self.__visit(self.root):\n            yield n\n\n    def __len__(self):\n        return self.size\n\n\nclass std_array:\n    def __init__(self, ref):\n        self.ref = ref\n\n    def __len__(self):\n        elems = self.ref['_M_elems']\n        return elems.type.sizeof / elems[0].type.sizeof\n\n    def __iter__(self):\n        elems = self.ref['_M_elems']\n        count = self.__len__()\n        i = 0\n        while i < count:\n            yield elems[i]\n            i += 1\n\n    def __nonzero__(self):\n        return self.__len__() > 0\n\n    def __bool__(self):\n        return self.__nonzero__()\n\n    def __getitem__(self, i):\n        return self.ref['_M_elems'][int(i)]\n\n\nclass std_vector:\n    def __init__(self, ref):\n        self.ref = ref\n\n    def __len__(self):\n        return int(self.ref['_M_impl']['_M_finish'] - self.ref['_M_impl']['_M_start'])\n\n    def __iter__(self):\n        i = self.ref['_M_impl']['_M_start']\n        end = self.ref['_M_impl']['_M_finish']\n        while i != end:\n            yield i.dereference()\n            i += 1\n\n    def __getitem__(self, item):\n        return (self.ref['_M_impl']['_M_start'] + item).dereference()\n\n    def __nonzero__(self):\n        return self.__len__() > 0\n\n    def __bool__(self):\n        return self.__nonzero__()\n\n    def external_memory_footprint(self):\n        return int(self.ref['_M_impl']['_M_end_of_storage']) - int(self.ref['_M_impl']['_M_start'])\n\n\nclass std_unordered_set:\n    def __init__(self, ref):\n        self.ht = ref['_M_h']\n        value_type = ref.type.template_argument(0)\n        _, node_type = lookup_type(['::std::__detail::_Hash_node<{}, {}>'.format(value_type.name, cache)\n                                    for cache in ('false', 'true')])\n        self.node_ptr_type = node_type.pointer()\n        self.value_ptr_type = value_type.pointer()\n\n    def __len__(self):\n        return self.ht['_M_element_count']\n\n    def __iter__(self):\n        p = self.ht['_M_before_begin']['_M_nxt']\n        while p:\n            pc = p.cast(self.node_ptr_type)['_M_storage']['_M_storage']['__data'].cast(self.value_ptr_type)\n            yield pc.dereference()\n            p = p['_M_nxt']\n\n    def __nonzero__(self):\n        return self.__len__() > 0\n\n    def __bool__(self):\n        return self.__nonzero__()\n\n\nclass std_unordered_map:\n    def __init__(self, ref):\n        self.ht = ref['_M_h']\n        kt = ref.type.template_argument(0)\n        vt = ref.type.template_argument(1)\n        value_type = gdb.lookup_type('::std::pair<{} const, {} >'.format(str(kt), str(vt)))\n        _, node_type = lookup_type(['::std::__detail::_Hash_node<{}, {}>'.format(value_type.name, cache)\n                                    for cache in ('false', 'true')])\n        self.node_ptr_type = node_type.pointer()\n        self.value_ptr_type = value_type.pointer()\n\n    def __len__(self):\n        return self.ht['_M_element_count']\n\n    def __iter__(self):\n        p = self.ht['_M_before_begin']['_M_nxt']\n        while p:\n            pc = p.cast(self.node_ptr_type)['_M_storage']['_M_storage']['__data'].cast(self.value_ptr_type)\n            yield (pc['first'], pc['second'])\n            p = p['_M_nxt']\n\n    def __nonzero__(self):\n        return self.__len__() > 0\n\n    def __bool__(self):\n        return self.__nonzero__()\n\n\nclass flat_hash_map:\n    def __init__(self, ref):\n        kt = ref.type.template_argument(0)\n        vt = ref.type.template_argument(1)\n        slot_ptr_type = gdb.lookup_type('::std::pair<const {}, {} >'.format(str(kt), str(vt))).pointer()\n        self.slots = ref['slots_'].cast(slot_ptr_type)\n        self.size = ref['size_']\n\n    def __len__(self):\n        return self.size\n\n    def __iter__(self):\n        size = self.size\n        slot = self.slots\n        while size > 0:\n            yield (slot['first'], slot['second'])\n            slot += 1\n            size -= 1\n\n    def __nonzero__(self):\n        return self.__len__() > 0\n\n    def __bool__(self):\n        return self.__nonzero__()\n\n\nclass absl_container:\n    # absl_container is the underlying type for flat_hash_map and flat_hash_set\n    # if we need to print flat_hash_set, we should yield the element of set\n    # instead of <key, value> tuple in `__iter__()`\n    def __init__(self, ref):\n        self.val = ref\n        HasInfozShift = 1\n        self.size = ref[\"settings_\"][\"value\"][\"size_\"] >> HasInfozShift\n\n    def __len__(self):\n        return self.size\n\n    def __iter__(self):\n        if self.size == 0:\n            return\n        capacity = int(self.val[\"settings_\"][\"value\"][\"capacity_\"])\n        control = self.val[\"settings_\"][\"value\"][\"control_\"]\n        # for the map the slot_type is std::pair<K, V>\n        slot_type = gdb.lookup_type(str(self.val.type.strip_typedefs()) + \"::slot_type\")\n        slots = self.val[\"settings_\"][\"value\"][\"slots_\"].cast(slot_type.pointer())\n        for i in range(capacity):\n            ctrl_t = int(control[i])\n            # if the control is empty or deleted, its value is less than -1, see\n            # https://github.com/abseil/abseil-cpp/blob/c1e1b47d989978cde8c5a2a219df425b785a0c47/absl/container/internal/raw_hash_set.h#L487-L503\n            if ctrl_t == -1:\n                break\n            if ctrl_t >= 0:\n                # NOTE: this only works for flat_hash_map\n                if slots[i]['value'].type.name.find(\"::map_slot_type\") != -1:\n                    yield slots[i]['key'], slots[i]['value']['second']\n                else:\n                    yield slots[i]['key'], slots[i]['value']\n\n    def __nonzero__(self):\n        return self.size > 0\n\n    def __bool__(self):\n        return self.size > 0\n\n\ndef unordered_map(ref):\n    if ref.type.name.startswith('flat_hash_map'):\n        try:\n            return flat_hash_map(ref)\n        except gdb.error:\n            # newer absl container uses a different memory layout\n            return absl_container(ref)\n    else:\n        return std_unordered_map(ref)\n\n\ndef std_priority_queue(ref):\n    return std_vector(ref['c'])\n\n\nclass std_deque:\n    # should reflect the value of _GLIBCXX_DEQUE_BUF_SIZE\n    DEQUE_BUF_SIZE = 512\n\n    def __init__(self, ref):\n        self.ref = ref\n        self.value_type = self.ref.type.strip_typedefs().template_argument(0)\n        if self.value_type.sizeof < std_deque.DEQUE_BUF_SIZE:\n            self.buf_size = int(std_deque.DEQUE_BUF_SIZE / self.value_type.sizeof)\n        else:\n            self.buf_size = 1\n\n    def _foreach_node(self):\n        node_it = self.ref['_M_impl']['_M_start']\n        node_end = self.ref['_M_impl']['_M_finish']\n        node = node_it['_M_node']\n        it = node_it['_M_first']\n        end = node_it['_M_last']\n\n        if not bool(node):\n            return\n\n        while node != node_end['_M_node']:\n            yield it, end\n            node = node + 1\n            it = node.dereference()\n            end = it + self.buf_size\n\n        yield node_end['_M_first'], node_end['_M_cur']\n\n    def __len__(self):\n        l = 0\n        for start, end in self._foreach_node():\n            l += (end - start)\n        return l\n\n    def __nonzero__(self):\n        return self.__len__() > 0\n\n    def __bool__(self):\n        return self.__len__() > 0\n\n    def __iter__(self):\n        for it, end in self._foreach_node():\n            while it != end:\n                yield it.dereference()\n                it += 1\n\n    def __str__(self):\n        items = [str(item) for item in self]\n        return \"{{size={}, [{}]}}\".format(len(self), \", \".join(items))\n\n\nclass static_vector:\n    def __init__(self, ref):\n        self.ref = ref\n\n    def __len__(self):\n        return int(self.ref['m_holder']['m_size'])\n\n    def __iter__(self):\n        t = self.ref.type.strip_typedefs()\n        value_type = t.template_argument(0)\n        data = self.ref['m_holder']['storage']['data'].cast(value_type.pointer())\n        for i in range(self.__len__()):\n            yield data[i]\n\n    def __nonzero__(self):\n        return self.__len__() > 0\n\n    def __bool__(self):\n        return self.__nonzero__()\n\n\nclass std_list:\n    \"\"\"Make `std::list` usable in python as a read-only container.\"\"\"\n\n    @staticmethod\n    def _make_dereference_func(value_type):\n        list_node_type = gdb.lookup_type('std::_List_node<{}>'.format(str(value_type))).pointer()\n\n        def deref(node):\n            list_node = node.cast(list_node_type)\n            return list_node['_M_storage']['_M_storage'].cast(value_type.pointer()).dereference()\n\n        return deref\n\n    def __init__(self, ref):\n        self.ref = ref\n        self._dereference_node = std_list._make_dereference_func(self.ref.type.strip_typedefs().template_argument(0))\n\n    def __len__(self):\n        try:\n            return int(self.ref['_M_impl']['_M_node']['_M_size'])\n        except gdb.error:\n            i = 0\n            for _ in self:\n                i += 1\n            return i\n\n    def __nonzero__(self):\n        return self.__len__() > 0\n\n    def __bool__(self):\n        return self.__nonzero__()\n\n    def __getitem__(self, item):\n        if not isinstance(item, int):\n            raise ValueError(\"Invalid index: expected `{}`, got: `{}`\".format(int, type(item)))\n\n        if item >= len(self):\n            raise ValueError(\"Index out of range: expected < {}, got {}\".format(len(self), item))\n\n        i = 0\n        it = iter(self)\n        val = next(it)\n        while i != item:\n            i += 1\n            val = next(it)\n\n        return val\n\n    def __iter__(self):\n        class std_list_iterator:\n            def __init__(self, lst):\n                self._list = lst\n                node_header = self._list.ref['_M_impl']['_M_node']\n                self._node = node_header['_M_next']\n                self._end = node_header['_M_next']['_M_prev']\n\n            def __next__(self):\n                if self._node == self._end:\n                    raise StopIteration()\n\n                val = self._list._dereference_node(self._node)\n                self._node = self._node['_M_next']\n                return val\n\n            # python2 compatibility\n            def next(self):\n                return self.__next__()\n\n        return std_list_iterator(self)\n\n    @staticmethod\n    def dereference_iterator(it):\n        deref = std_list._make_dereference_func(it.type.strip_typedefs().template_argument(0))\n        return deref(it['_M_node'])\n\n\nclass managed_vector:\n    def __init__(self, ref):\n        self._ref = ref\n\n    def __len__(self):\n        return int(self._ref['_size'])\n\n    def __nonzero__(self):\n        return self.__len__() > 0\n\n    def __bool__(self):\n        return self.__len__() > 0\n\n    def __iter__(self):\n        for i in range(len(self)):\n            yield self._ref['_data'][i]\n\n\nclass chunked_managed_vector:\n    def __init__(self, ref):\n        self._ref = ref\n\n    def __len__(self):\n        return int(self._ref['_size'])\n\n    def __nonzero__(self):\n        return self.__len__() > 0\n\n    def __bool__(self):\n        return self.__len__() > 0\n\n    def __iter__(self):\n        for chunk in managed_vector(self._ref['_chunks']):\n            for e in managed_vector(chunk):\n                yield e\n\n\nclass chunked_fifo:\n    class chunk:\n        def __init__(self, ref):\n            self.items = ref['items']\n            self.begin = ref['begin']\n            self.end = ref['end']\n            self.next_one = ref['next']\n\n        def __len__(self):\n            return self.end - self.begin\n\n    class iterator:\n        def __init__(self, fifo, chunk):\n            self.fifo = fifo\n            self.chunk = chunk\n            self.index = self.chunk.begin if chunk else 0\n\n        def __next__(self):\n            if self.index == 0:\n                raise StopIteration\n            if self.index == self.chunk.end:\n                if not self.chunk.next_one:\n                    raise StopIteration\n                self.chunk = chunked_fifo.chunk(self.chunk.next_one)\n                self.index = self.chunk.begin\n            index = self.index\n            self.index += 1\n            return self.chunk.items[self.fifo.mask(index)]['data']\n\n    def __init__(self, ref):\n        self._ref = ref\n        # try to access the member variable, so the constructor throws if the\n        # inspected variable is of the wrong type\n        _ = self.front_chunk\n\n    def mask(self, index):\n        return index & (self.items_per_chunk - 1)\n\n    @property\n    def items_per_chunk(self):\n        return self._ref.type.template_argument(1)\n\n    @property\n    def front_chunk(self):\n        return self._ref['_front_chunk']\n\n    @property\n    def back_chunk(self):\n        return self._ref['_back_chunk']\n\n    def __len__(self):\n        if not self.front_chunk:\n            return 0\n        if self.back_chunk == self.front_chunk:\n            front_chunk = self.chunk(self.front_chunk.dereference())\n            return len(front_chunk)\n        else:\n            front_chunk = self.chunk(self.front_chunk.dereference())\n            back_chunk = self.chunk(self.back_chunk.dereference())\n            num_chunks = self._ref['_nchunks'] - 2\n            return (len(front_chunk) +\n                    len(back_chunk) +\n                    num_chunks * self.items_per_chunk)\n\n    def __iter__(self):\n        return self.iterator(self, self.front_chunk)\n\n\nclass sstring:\n    def __init__(self, ref):\n        self.ref = ref\n\n    @staticmethod\n    def to_hex(data, size):\n        inf = gdb.selected_inferior()\n        return bytes(inf.read_memory(data, size)).hex()\n\n    def as_hex(self):\n        return self.to_hex(self.data(), len(self))\n\n    def is_internal(self):\n        return self.ref['u']['internal']['size'] >= 0\n\n    def __len__(self):\n        if self.is_internal():\n            return self.ref['u']['internal']['size']\n        else:\n            return self.ref['u']['external']['size']\n\n    def data(self):\n        if self.is_internal():\n            return self.ref['u']['internal']['str']\n        else:\n            return self.ref['u']['external']['str']\n\n    def __str__(self):\n        return self.as_hex()\n\n\ndef uint64_t(val):\n    val = int(val)\n    if val < 0:\n        val += 1 << 64\n    return val\n\n\nclass inet_address_printer(gdb.printing.PrettyPrinter):\n    'print a gms::inet_address'\n\n    def __init__(self, val):\n        self.val = val\n\n    def to_string(self):\n        family = str(self.val['_addr']['_in_family'])\n        if family == \"seastar::net::inet_address::family::INET\":\n            raw = self.val['_addr']['_in']['s_addr']\n            ipv4 = socket.inet_ntop(socket.AF_INET, struct.pack('=L', raw))\n            return str(ipv4)\n        else:\n            raw = self.val['_addr']['_in6']['__in6_u']['__u6_addr32']\n            ipv6 = socket.inet_ntop(socket.AF_INET6, struct.pack('=LLLL', raw[0], raw[1], raw[2], raw[3]))\n            return str(ipv6)\n\n    def display_hint(self):\n        return 'string'\n\n\nclass sstring_printer(gdb.printing.PrettyPrinter):\n    'print an sstring'\n\n    def __init__(self, val):\n        self.val = val\n\n    def to_string(self):\n        if self.val['u']['internal']['size'] >= 0:\n            array = self.val['u']['internal']['str']\n            len = int(self.val['u']['internal']['size'])\n            return ''.join([chr(array[x]) for x in range(len)])\n        else:\n            return self.val['u']['external']['str']\n\n    def display_hint(self):\n        return 'string'\n\n\nclass string_view_printer(gdb.printing.PrettyPrinter):\n    'print an std::string_view'\n\n    def __init__(self, val):\n        self.val = val\n\n    def to_string(self):\n        inf = gdb.selected_inferior()\n        return str(inf.read_memory(self.val['_M_str'], self.val['_M_len']), encoding='utf-8')\n\n    def display_hint(self):\n        return 'string'\n\n\nclass managed_bytes_printer(gdb.printing.PrettyPrinter):\n    'print a managed_bytes'\n\n    def __init__(self, val):\n        self.val = val\n\n    def pure_bytes(self):\n        inf = gdb.selected_inferior()\n\n        def to_bytes(data, size):\n            return bytes(inf.read_memory(data, size))\n\n        if self.val['_inline_size'] >= 0:\n            return to_bytes(self.val['_u']['inline_data'], int(self.val['_inline_size']))\n        elif self.val['_inline_size'] == -1:\n            return to_bytes(self.val['_u']['single_chunk_ref']['ptr']['data'], int(self.val['_u']['single_chunk_ref']['size']))\n        else:\n            ref = self.val['_u']['multi_chunk_ref']['ptr']\n            chunks = list()\n            while ref['ptr']:\n                chunks.append(to_bytes(ref['ptr']['data'], int(ref['ptr']['frag_size'])))\n                ref = ref['ptr']['next']\n            return b''.join(chunks)\n\n    def bytes_as_hex(self):\n        return self.pure_bytes().hex()\n\n    def bytes(self):\n        return self.bytes_as_hex()\n\n    def to_string(self):\n        return str(self.bytes())\n\n    def display_hint(self):\n        return 'managed_bytes'\n\n\nclass optional_printer(gdb.printing.PrettyPrinter):\n    def __init__(self, val):\n        self.val = std_optional(val)\n\n    def to_string(self):\n        if not self.val:\n            return 'std::nullopt'\n        return str(self.val.get())\n\n    def display_hint(self):\n        return 'std::optional'\n\n\nclass partition_entry_printer(gdb.printing.PrettyPrinter):\n    def __init__(self, val):\n        self.val = val\n\n    def to_string(self):\n        versions = list()\n        v = self.val['_version']['_version']\n        while v:\n            versions.append('@%s: %s' % (v, v.dereference()))\n            v = v['_next']\n        return '{_snapshot=%s, _version=%s, versions=[\\n%s\\n]}' % (self.val['_snapshot'], self.val['_version'], ',\\n'.join(versions))\n\n    def display_hint(self):\n        return 'partition_entry'\n\n\nclass mutation_partition_printer(gdb.printing.PrettyPrinter):\n    def __init__(self, val):\n        self.val = val\n\n    def __rows(self):\n        return intrusive_btree(self.val['_rows'])\n\n    def to_string(self):\n        rows = list(str(r) for r in self.__rows())\n        range_tombstones = list(str(r) for r in intrusive_set(self.val['_row_tombstones']['_tombstones'], link='_link'))\n        return '{_tombstone=%s, _static_row=%s (cont=%s), _row_tombstones=[%s], _rows=[%s]}' % (\n            self.val['_tombstone'],\n            self.val['_static_row'],\n            ('no', 'yes')[self.val['_static_row_continuous']],\n            '\\n' + ',\\n'.join(range_tombstones) + '\\n' if range_tombstones else '',\n            '\\n' + ',\\n'.join(rows) + '\\n' if rows else '')\n\n    def display_hint(self):\n        return 'mutation_partition'\n\n\nclass row_printer(gdb.printing.PrettyPrinter):\n    def __init__(self, val):\n        self.val = val\n\n    def __to_string_legacy(self):\n        if self.val['_type'] == gdb.parse_and_eval('row::storage_type::vector'):\n            cells = str(self.val['_storage']['vector'])\n        elif self.val['_type'] == gdb.parse_and_eval('row::storage_type::set'):\n            cells = '[%s]' % (', '.join(str(cell) for cell in intrusive_set(self.val['_storage']['set'])))\n        else:\n            raise Exception('Unsupported storage type: ' + self.val['_type'])\n        return '{type=%s, cells=%s}' % (self.val['_type'], cells)\n\n    def to_string(self):\n        try:\n            return '{cells=[%s]}' % compact_radix_tree(self.val['_cells']).to_string()\n        except gdb.error:\n            return self.__to_string_legacy()\n\n    def display_hint(self):\n        return 'row'\n\n\nclass managed_vector_printer(gdb.printing.PrettyPrinter):\n    def __init__(self, val):\n        self.val = val\n\n    def to_string(self):\n        size = int(self.val['_size'])\n        items = list()\n        for i in range(size):\n            items.append(str(self.val['_data'][i]))\n        return '{size=%d, items=[%s]}' % (size, ', '.join(items))\n\n    def display_hint(self):\n        return 'managed_vector'\n\n\nclass uuid_printer(gdb.printing.PrettyPrinter):\n    'print a uuid'\n\n    def __init__(self, val):\n        self.val = val\n\n    def to_string(self):\n        msb = uint64_t(self.val['most_sig_bits'])\n        lsb = uint64_t(self.val['least_sig_bits'])\n        return str(uuid.UUID(int=(msb << 64) | lsb))\n\n\nclass sstable_generation_printer(gdb.printing.PrettyPrinter):\n    'print an sstables::generation_type'\n    BASE36_ALPHABET = string.digits + string.ascii_lowercase\n    DECIMICRO_RATIO = 10_000_000\n\n    def __init__(self, val):\n        self.val = val['_value']\n\n    def _to_uuid(self):\n        msb = uint64_t(self.val['most_sig_bits'])\n        lsb = uint64_t(self.val['least_sig_bits'])\n        bytes = msb.to_bytes(8) + lsb.to_bytes(8)\n        return uuid.UUID(bytes=bytes)\n\n    @classmethod\n    def _encode_n_with_base36(cls, n):\n        assert n >= 0\n        output = ''\n        alphabet_len = len(cls.BASE36_ALPHABET)\n        while n:\n            n, index = divmod(n, alphabet_len)\n            output += cls.BASE36_ALPHABET[index]\n        return output[::-1]\n\n    @classmethod\n    def _encode_uuid_with_base36(cls, timeuuid):\n        # see also scripts/base36-uuid.py for more context on the encoding\n        # of the sstable (generation) identifiers.\n        seconds, decimicro = divmod(timeuuid.time, cls.DECIMICRO_RATIO)\n        delta = datetime.timedelta(seconds=seconds)\n        encoded_days = cls._encode_n_with_base36(delta.days)\n        encoded_seconds = cls._encode_n_with_base36(delta.seconds)\n        encoded_decimicro = cls._encode_n_with_base36(decimicro)\n        lsb = int.from_bytes(timeuuid.bytes[8:])\n        encoded_lsb = cls._encode_n_with_base36(lsb)\n        return (f'{encoded_days:0>4}_'\n                f'{encoded_seconds:0>4}_'\n                f'{encoded_decimicro:0>5}'\n                f'{encoded_lsb:0>13}')\n\n    def to_string(self):\n        if self.val.type == gdb.lookup_type('int64_t'):\n            # before the uuid-generation change\n            return str(self.val)\n\n        # after the uuid-generation change\n        assert self.val.type == gdb.lookup_type('utils::UUID')\n        timeuuid = self._to_uuid()\n        # a uuid encoded generation can present one of the following types:\n        # 1. null: the generation is empty.\n        # 2. an integer: if the sstable is created with\n        #    \"uuid_sstable_identifiers_enabled\" option disabled.\n        # 3. a uuid: if the option above is enabled as it is by default.\n        if timeuuid.int == 0:\n            return \"<null>\"\n        elif timeuuid.time == 0:\n            # encodes an integer\n            # just for the sake of correctness, as always use int64_t for\n            # representing sstable generation even the generations are\n            # always positive.\n            lsb = int.from_bytes(timeuuid.bytes[8:], signed=True)\n            return str(lsb)\n        else:\n            # encodes a uuid\n            return self._encode_uuid_with_base36(timeuuid)\n\n\nclass boost_intrusive_list_printer(gdb.printing.PrettyPrinter):\n    def __init__(self, val):\n        self.val = intrusive_list(val)\n\n    def to_string(self):\n        items = ['@' + str(v.address) + '=' + str(v) for v in self.val]\n        ptrs = [str(v.address) for v in self.val]\n        return 'boost::intrusive::list of size {} = [{}] = [{}]'.format(len(items), ', '.join(ptrs), ', '.join(items))\n\n\nclass interval_printer(gdb.printing.PrettyPrinter):\n    def __init__(self, val):\n        self.val = val['_interval']\n\n    def inspect_bound(self, bound_opt):\n        bound = std_optional(bound_opt)\n        if not bound:\n            return False, False, None\n\n        bound = bound.get()\n\n        return True, bool(bound['_inclusive']), bound['_value']\n\n    def to_string(self):\n        has_start, start_inclusive, start_value = self.inspect_bound(self.val['_start'])\n        has_end, end_inclusive, end_value = self.inspect_bound(self.val['_end'])\n\n        if self.val['_singular']:\n            return '{{{}}}'.format(str(start_value))\n\n        return '{}{}, {}{}'.format(\n            '[' if start_inclusive else '(',\n            str(start_value) if has_start else '-inf',\n            str(end_value) if has_end else '+inf',\n            ']' if end_inclusive else ')',\n        )\n\n\nclass ring_position_printer(gdb.printing.PrettyPrinter):\n    def __init__(self, val):\n        self.val = val\n\n    def to_string(self):\n        pkey = std_optional(self.val['_key'])\n        if pkey:\n            # we can assume token_kind == token_kind::key\n            return '{{{}, {}}}'.format(str(self.val['_token']['_data']), str(pkey.get()['_bytes']))\n\n        token_bound = int(self.val['_token_bound'])\n        token_kind = str(self.val['_token']['_kind'])[17:] # ignore the dht::token_kind:: prefix\n        if token_kind == 'key':\n            token = str(self.val['_token']['_data'])\n        else:\n            token = token_kind\n\n        return '{{{}, {}}}'.format(token, token_bound)\n\n\ndef build_pretty_printer():\n    pp = gdb.printing.RegexpCollectionPrettyPrinter('scylla')\n    pp.add_printer('sstring', r'^seastar::basic_sstring<char,.*>$', sstring_printer)\n    pp.add_printer('std::string_view', r'^std::basic_string_view<char,.*>$', string_view_printer)\n    pp.add_printer('bytes', r'^seastar::basic_sstring<signed char, unsigned int, 31, false>$', sstring_printer)\n    pp.add_printer('managed_bytes', r'^managed_bytes$', managed_bytes_printer)\n    pp.add_printer('partition_entry', r'^partition_entry$', partition_entry_printer)\n    pp.add_printer('mutation_partition', r'^mutation_partition$', mutation_partition_printer)\n    pp.add_printer('row', r'^row$', row_printer)\n    pp.add_printer('managed_vector', r'^managed_vector<.*>$', managed_vector_printer)\n    pp.add_printer('uuid', r'^utils::UUID$', uuid_printer)\n    pp.add_printer('sstable_generation', r'^sstables::generation_type$', sstable_generation_printer)\n    pp.add_printer('boost_intrusive_list', r'^boost::intrusive::list<.*>$', boost_intrusive_list_printer)\n    pp.add_printer('inet_address_printer', r'^gms::inet_address$', inet_address_printer)\n    pp.add_printer('interval', r'^interval<.*$', interval_printer)\n    pp.add_printer('nonwrapping_range', r'^nonwrapping_range<.*$', interval_printer) # scylla < 4.3 calls it nonwrapping_range\n    pp.add_printer('ring_position', r'^dht::ring_position$', ring_position_printer)\n    pp.add_printer('optional', r'^std::_Optional_base.*', optional_printer)\n    return pp\n\n\ngdb.printing.register_pretty_printer(gdb.current_objfile(), build_pretty_printer(), replace=True)\n\n\ndef cpus():\n    return int(gdb.parse_and_eval('::seastar::smp::count'))\n\n\ndef current_shard():\n    return int(gdb.parse_and_eval('\\'seastar\\'::local_engine->_id'))\n\n\nclass sharded:\n    def __init__(self, val):\n        self.val = val\n        self.instances = std_vector(self.val['_instances'])\n\n    def instance(self, shard=None):\n        idx = shard or current_shard()\n        if idx >= len(self.instances):\n            return None\n        return self.instances[idx]['service']['_p']\n\n    def local(self):\n        return self.instance()\n\n\ndef get_lsa_segment_pool():\n    try:\n        tracker = gdb.parse_and_eval('\\'logalloc::tracker_instance\\'')\n        tracker_impl = std_unique_ptr(tracker[\"_impl\"]).get().dereference()\n        return std_unique_ptr(tracker_impl[\"_segment_pool\"]).get().dereference()\n    except gdb.error:\n        return gdb.parse_and_eval('\\'logalloc::shard_segment_pool\\'')\n\n\ndef find_db(shard=None):\n    try:\n        db = gdb.parse_and_eval('::debug::the_database')\n    except Exception as e:\n        try:\n            db = gdb.parse_and_eval('::debug::db')\n        except:\n            return None\n\n    return sharded(db).instance(shard)\n\n\ndef find_dbs():\n    return [find_db(shard) for shard in range(cpus())]\n\n\ndef for_each_table(db=None):\n    \"\"\"Returns pointers to table objects which exist on current shard\"\"\"\n    if not db:\n        db = find_db()\n\n    try:\n        tables = unordered_map(db['_tables_metadata']['_column_families'])\n    except gdb.error:\n        tables = unordered_map(db['_column_families'])\n\n    for (key, value) in tables:\n        yield seastar_lw_shared_ptr(value).get().dereference()\n\n\ndef lookup_type(type_names):\n    for type_name in type_names:\n        try:\n            return (type_name, gdb.lookup_type(type_name))\n        except gdb.error:\n            continue\n    raise gdb.error('none of the types found')\n\n\ndef get_text_ranges():\n    try:\n        vptr_type = gdb.lookup_type('uintptr_t').pointer()\n        reactor_backend = gdb.parse_and_eval('seastar::local_engine->_backend')\n        # 2019.1 has value member, >=3.0 has std::unique_ptr<>\n        if reactor_backend.type.strip_typedefs().name.startswith('std::unique_ptr<'):\n            reactor_backend = std_unique_ptr(reactor_backend).get()\n        else:\n            reactor_backend = gdb.parse_and_eval('&seastar::local_engine->_backend')\n        known_vptr = int(reactor_backend.reinterpret_cast(vptr_type).dereference())\n    except Exception as e:\n        gdb.write(\"get_text_ranges(): Falling back to locating .rodata section because lookup to reactor backend to use as known vptr failed: {}\\n\".format(e))\n        known_vptr = None\n\n    def section_bounds(line):\n        items = line.split()\n        start = int(items[0], 16)\n        end = int(items[2], 16)\n        return (start, end)\n\n    ret = []\n    sections = gdb.execute('info files', False, True).split('\\n')\n    for line in sections:\n        # add .text for coroutines\n        if line.endswith(\"is .text\"):\n            ret.append(section_bounds(line))\n        elif known_vptr and \" is .\" in line:\n            bounds = section_bounds(line)\n            if bounds[0] <= known_vptr and known_vptr <= bounds[1]:\n                ret.append(bounds)\n        # otherwise vptrs are in .rodata section\n        elif line.endswith(\"is .rodata\"):\n            ret.append(section_bounds(line))\n\n    if len(ret) == 0:\n        raise Exception(\"Failed to find plausible text sections\")\n    return ret\n\n\ndef addr_in_ranges(ranges, addr):\n    for r in ranges:\n        if addr >= r[0] and addr <= r[1]:\n            return True\n    return False\n\n\nclass histogram:\n    \"\"\"Simple histogram.\n\n    Aggregate items by their count and present them in a histogram format.\n    Example:\n\n        h = histogram()\n        h['item1'] = 20 # Set an absolute value\n        h.add('item2') # Equivalent to h['item2'] += 1\n        h.add('item2')\n        h.add('item3')\n        h.print_to_console()\n\n    Would print:\n        4 item1 ++++++++++++++++++++++++++++++++++++++++\n        2 item2 ++++\n        1 item1 ++\n\n    Note that the number of indicators ('+') is does not correspond to the\n    actual number of items, rather it is supposed to illustrate their relative\n    counts.\n    \"\"\"\n    _column_count = 40\n\n    def __init__(self, counts = None, print_indicators = True, formatter=None, limit=None):\n        \"\"\"Constructor.\n\n        Params:\n        * counts: initial counts (default to empty).\n        * print_indicators: print the '+' characters to illustrate relative\n            count. Can be turned off when the item names are very long and would\n            thus make indicators unreadable.\n        * formatter: a callable that receives the item as its argument and is\n            expected to return the string to be printed in the second column.\n            By default, items are printed verbatim.\n        * limit: limit the number of printed items to the top limit ones.\n        \"\"\"\n        if counts is None:\n            self._counts = defaultdict(int)\n        else:\n            self._counts = counts\n        self._print_indicators = print_indicators\n\n        def default_formatter(value):\n            return str(value)\n        if formatter is None:\n            self._formatter = default_formatter\n        else:\n            self._formatter = formatter\n\n        if limit is None:\n            self._limit = -1\n        else:\n            self._limit = limit\n\n    def __len__(self):\n        return len(self._counts)\n\n    def __nonzero__(self):\n        return bool(len(self))\n\n    def __getitem__(self, item):\n        return self._counts[item]\n\n    def __setitem__(self, item, value):\n        self._counts[item] = value\n\n    def add(self, item):\n        self._counts[item] += 1\n\n    def __str__(self):\n        if not self._counts:\n            return ''\n\n        by_counts = defaultdict(list)\n        for k, v in self._counts.items():\n            by_counts[v].append(k)\n\n        counts_sorted = list(reversed(sorted(by_counts.keys())))\n        max_count = counts_sorted[0]\n\n        if max_count == 0:\n            count_per_column = 0\n        else:\n            count_per_column = self._column_count / max_count\n\n        lines = []\n        limit = self._limit\n\n        for count in counts_sorted:\n            items = by_counts[count]\n            if self._print_indicators:\n                indicator = '+' * max(1, int(count * count_per_column))\n            else:\n                indicator = ''\n            for item in items:\n                try:\n                    lines.append('{:9d} {} {}'.format(count, self._formatter(item), indicator))\n                except:\n                    gdb.write(\"error: failed to format item `{}': {}\\n\".format(item, sys.exc_info()[1]))\n            limit -= 1\n            if not limit:\n                break\n\n        return '\\n'.join(lines)\n\n    def __repr__(self):\n        return 'histogram({})'.format(self._counts)\n\n    def print_to_console(self):\n        gdb.write(str(self) + '\\n')\n\n\nclass task_symbol_matcher:\n    def __init__(self):\n        self._coro_pattern = re.compile(r'\\)( \\[clone \\.\\w+\\])?$')\n\n        # List of whitelisted symbol names. Each symbol is a tuple, where each\n        # element is a component of the name, the last element being the class\n        # name itself.\n        # We can't just merge them as `info symbol` might return mangled names too.\n        self._whitelist = task_symbol_matcher._make_symbol_matchers([\n                (\"seastar\", \"continuation\"),\n                (\"seastar\", \"future\", \"thread_wake_task\"), # backward compatibility with older versions\n                (\"seastar\", \"(anonymous namespace)\", \"thread_wake_task\"),\n                (\"seastar\", \"thread_context\"),\n                (\"seastar\", \"internal\", \"do_until_state\"),\n                (\"seastar\", \"internal\", \"do_with_state\"),\n                (\"seastar\", \"internal\", \"do_for_each_state\"),\n                (\"seastar\", \"parallel_for_each_state\"),\n                (\"seastar\", \"internal\", \"repeat_until_value_state\"),\n                (\"seastar\", \"internal\", \"repeater\"),\n                (\"seastar\", \"internal\", \"when_all_state\"),\n                (\"seastar\", \"internal\", \"when_all_state_component\"),\n                (\"seastar\", \"internal\", \"coroutine_traits_base\", \"promise_type\"),\n                (\"seastar\", \"lambda_task\"),\n                (\"seastar\", \"smp_message_queue\", \"async_work_item\"),\n        ])\n\n    @staticmethod\n    def _make_symbol_matchers(symbol_specs):\n        return list(map(task_symbol_matcher._make_symbol_matcher, symbol_specs))\n\n    @staticmethod\n    def _make_symbol_matcher(symbol_spec):\n        unmangled_prefix = 'vtable for {}'.format('::'.join(symbol_spec))\n        def matches_symbol(name):\n            if name.startswith(unmangled_prefix):\n                return True\n\n            try:\n                positions = [name.index(part) for part in symbol_spec]\n                return sorted(positions) == positions\n            except ValueError:\n                return False\n\n        return matches_symbol\n\n    def __call__(self, name):\n        name = name.strip()\n        if re.search(self._coro_pattern, name) is not None:\n            return True\n\n        for matcher in self._whitelist:\n            if matcher(name):\n                return True\n        return False\n\n\nclass scylla(gdb.Command):\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND, True)\n\n\nclass scylla_databases(gdb.Command):\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla databases', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def invoke(self, arg, from_tty):\n        for shard in range(cpus()):\n            db = find_db(shard)\n            gdb.write('{:5} (replica::database*){}\\n'.format(shard, db))\n\n\nclass scylla_commitlog(gdb.Command):\n    \"\"\"Prints info about commitlog segment manager and managed segments\n    \"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla commitlog', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def invoke(self, arg, from_tty):\n        db = find_db()\n        cmtlog = std_unique_ptr(db['_commitlog']).get()\n        gdb.write('(db::commitlog*){}\\n'.format(cmtlog))\n        segmgr = seastar_shared_ptr(cmtlog['_segment_manager']).get()\n        gdb.write('(db::commitlog::segment_manager*){}\\n'.format(segmgr))\n        segs = std_vector(segmgr['_segments'])\n        gdb.write('segments ({}):\\n'.format(len(segs)))\n        for seg_p in segs:\n            seg = seastar_shared_ptr(seg_p).get()\n            gdb.write('(db::commitlog::segment*){}\\n'.format(seg))\n\n\nclass scylla_keyspaces(gdb.Command):\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla keyspaces', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def invoke(self, arg, from_tty):\n        for shard in range(cpus()):\n            db = find_db(shard)\n            keyspaces = db['_keyspaces']\n            for (key, value) in unordered_map(keyspaces):\n                gdb.write('{:5} {:20} (replica::keyspace*){}\\n'.format(shard, str(key), value.address))\n\n\nclass scylla_tables(gdb.Command):\n    \"\"\"Lists tables (column-families)\"\"\"\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla tables', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def invoke(self, arg, from_tty):\n        parser = argparse.ArgumentParser(description=\"scylla tables\")\n        parser.add_argument(\"-a\", \"--all\", action=\"store_true\", default=False,\n                help=\"List tables from all shards\")\n        parser.add_argument(\"-u\", \"--user\", action=\"store_true\",\n                help=\"List only user tables\")\n        parser.add_argument(\"-k\", \"--keyspace\", action=\"store\",\n                help=\"List tables only for provided keyspace\")\n\n        try:\n            args = parser.parse_args(arg.split())\n        except SystemExit:\n            return\n\n        if args.all:\n            shards = list(range(cpus()))\n        else:\n            shards = [current_shard()]\n\n        for shard in shards:\n            for value in for_each_table():\n                schema = schema_ptr(value['_schema'])\n                if args.user and schema.is_system():\n                    continue\n                if args.keyspace and schema.ks_name != args.keyspace:\n                    continue\n                schema_id = str(schema['_raw']['_id'])\n                schema_version = str(schema['_raw']['_version'])\n                gdb.write('{:5} {} v={} {:45} (replica::table*){}\\n'.format(shard, schema_id, schema_version, schema.table_name(), value.address))\n\n\nclass scylla_table(gdb.Command):\n    \"\"\"Prints various info about individual table\n        Example:\n          scylla table ks.cf\n    \"\"\"\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla table', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def _find_table(self, ks, cf):\n        for value in for_each_table():\n            schema = schema_ptr(value['_schema'])\n            if schema.ks_name == ks and schema.cf_name == cf:\n                return value, schema\n\n        return None, None\n\n    @staticmethod\n    def phased_barrier_count(table, barrier_name):\n        g = seastar_lw_shared_ptr(table[barrier_name]['_gate']).get()\n        return int(g['_count'])\n\n    def invoke(self, arg, from_tty):\n        if arg is None or arg == '':\n            gdb.write(\"Specify keyspace.table argument\\n\")\n            return\n\n        [ ks, cf ] = arg.split('.', 2)\n        [ table, schema ] = self._find_table(ks, cf)\n        if table is None:\n            gdb.write(\"No such table\\n\")\n            return\n\n        gdb.write(f'(replica::table*){table.address}\\n')\n        gdb.write(f'schema version: {schema[\"_raw\"][\"_version\"]}\\n')\n        gdb.write('pending ops:')\n        for barrier_name in ['_pending_flushes_phaser', '_pending_writes_phaser', '_pending_reads_phaser', '_pending_streams_phaser']:\n            gdb.write(f' {barrier_name}: {scylla_table.phased_barrier_count(table, barrier_name)}')\n        gdb.write('\\n')\n\n\nclass scylla_task_histogram(gdb.Command):\n    \"\"\"Print a histogram of the virtual objects found in memory.\n\n    Sample the virtual objects in memory and create a histogram with the results.\n    By default up to 20000 samples will be collected and the top 30 items will\n    be shown. The number of collected samples, as well as number of items shown\n    can be customized by command line arguments. The sampling can also be\n    constrained to objects of a certain size. For more details invoke:\n\n        scylla task_histogram --help\n\n    Example:\n     12280: 0x4bc5878 vtable for seastar::file_data_source_impl + 16\n      9352: 0x4be2cf0 vtable for seastar::continuation<seastar::future<seasta...\n      9352: 0x4bc59a0 vtable for seastar::continuation<seastar::future<seasta...\n     (1)    (2)       (3)\n\n     Where:\n     (1): Number of objects of this type.\n     (2): The address of the class's vtable.\n     (3): The name of the class's vtable symbol.\n    \"\"\"\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla task_histogram', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def invoke(self, arg, from_tty):\n        parser = argparse.ArgumentParser(description=\"scylla task_histogram\")\n        parser.add_argument(\"-m\", \"--samples\", action=\"store\", type=int, default=20000,\n                help=\"The number of samples to collect. Defaults to 20000. Set to 0 to sample all objects. Ignored when `--all` is used.\"\n                \" Note that due to this limit being checked only after scanning an entire page, in practice it will always be overshot.\")\n        parser.add_argument(\"-c\", \"--count\", action=\"store\", type=int, default=30,\n                help=\"Show only the top COUNT elements of the histogram. Defaults to 30. Set to 0 to show all items. Ignored when `--all` is used.\")\n        parser.add_argument(\"-a\", \"--all\", action=\"store_true\", default=False,\n                help=\"Sample all pages and show all results. Equivalent to -m=0 -c=0.\")\n        parser.add_argument(\"-s\", \"--size\", type=int, action=\"store\", default=0,\n                help=\"The size of objects to sample. When set, only objects of this size will be sampled. A size of 0 (the default value) means no size restrictions.\")\n        parser.add_argument(\"-f\", \"--filter-tasks\", action=\"store_true\",\n                help=\"Include only task objects in the histogram, reduces noise but might exclude items due to inexact filtering.\")\n        parser.add_argument(\"-g\", \"--scheduling-groups\", action=\"store_true\",\n                help=\"Histogram is made from the scheduling groups of the sampled task objects. Implies -f.\")\n\n        try:\n            args = parser.parse_args(arg.split())\n        except SystemExit:\n            return\n\n        size = args.size\n        cpu_mem = gdb.parse_and_eval('\\'seastar::memory::cpu_mem\\'')\n        page_size = int(gdb.parse_and_eval('\\'seastar::memory::page_size\\''))\n        mem_start = cpu_mem['memory']\n\n        vptr_type = gdb.lookup_type('uintptr_t').pointer()\n        task_type = gdb.lookup_type('seastar::task')\n        task_fields = {f.name: f for f in task_type.fields()}\n        sg_offset = int(task_fields['_sg'].bitpos / 8)\n        sg_ptr = gdb.lookup_type('unsigned').pointer()\n\n        pages = cpu_mem['pages']\n        nr_pages = int(cpu_mem['nr_pages'])\n        page_samples = range(0, nr_pages) if args.all else random.sample(range(0, nr_pages), nr_pages)\n\n        text_ranges = get_text_ranges()\n\n        scheduling_group_names = {int(tq['_id']): str(tq['_name']) for tq in get_local_task_queues()}\n\n        def formatter(o):\n            if args.scheduling_groups:\n                return \"{:02} {}\".format(o, scheduling_group_names[o])\n            else:\n                return \"0x{:x} {}\".format(o, resolve(o))\n\n        limit = None if args.all or args.count == 0 else args.count\n        h = histogram(print_indicators=False, formatter=formatter, limit=limit)\n        symbol_matcher = task_symbol_matcher()\n\n        sc = span_checker()\n        vptr_count = defaultdict(int)\n        scanned_pages = 0\n        for idx in page_samples:\n            span = sc.get_span(mem_start + idx * page_size)\n            if not span or span.index != idx or not span.is_small():\n                continue\n            pool = span.pool()\n            if int(pool.dereference()['_object_size']) != size and size != 0:\n                continue\n            scanned_pages += 1\n            objsize = size if size != 0 else int(pool.dereference()['_object_size'])\n            span_size = span.used_span_size() * page_size\n            for idx2 in range(0, int(span_size / objsize)):\n                obj_addr = span.start + idx2 * objsize\n                addr = int(gdb.Value(obj_addr).reinterpret_cast(vptr_type).dereference())\n                if not addr_in_ranges(text_ranges, addr):\n                    continue\n                if args.filter_tasks:\n                    sym = resolve(addr)\n                    if not sym or not symbol_matcher(sym):\n                        continue # we only want tasks\n                if args.scheduling_groups:\n                    # This is a dirty trick to make this reasonably fast even with 100K+ or 1M+ objects\n                    # We have two good choices here:\n                    # 1) gdb.Value(obj_addr).reinterpret_cast(task_type).dereference()['_sg']['_id']\n                    #    This prints tons of warning about missing RTTI symbols\n                    #    and is quite slow.\n                    # 2) gdb.parse_and_eval(\"(seastar::task*)0x{:x}\".format(obj_addr))\n                    #    Works well but is horrendously slow: one type lookup per\n                    #    obj + parsing is too much apparently.\n                    #\n                    # So we bypass casting to seastar::task* and use the known\n                    # offset of the _id field instead directly.\n                    key = int(gdb.Value(obj_addr + sg_offset).reinterpret_cast(sg_ptr).dereference())\n                    # Task matching is not exact, we'll have some non-task\n                    # objects here, with invalid sg derived, ignore these.\n                    if key not in scheduling_group_names:\n                        continue\n                else:\n                    key = addr\n                h[key] += 1\n            if args.all or args.samples == 0:\n                continue\n            if scanned_pages >= args.samples or len(vptr_count) >= args.samples:\n                break\n\n        h.print_to_console()\n\n\ndef find_vptrs():\n    cpu_mem = gdb.parse_and_eval('\\'seastar::memory::cpu_mem\\'')\n    page_size = int(gdb.parse_and_eval('\\'seastar::memory::page_size\\''))\n    mem_start = cpu_mem['memory']\n    vptr_type = gdb.lookup_type('uintptr_t').pointer()\n    pages = cpu_mem['pages']\n    nr_pages = int(cpu_mem['nr_pages'])\n\n    text_ranges = get_text_ranges()\n\n    def is_vptr(addr):\n        return addr_in_ranges(text_ranges, addr)\n\n    idx = 0\n    while idx < nr_pages:\n        if pages[idx]['free']:\n            idx += pages[idx]['span_size']\n            continue\n        pool = pages[idx]['pool']\n        if not pool or pages[idx]['offset_in_span'] != 0:\n            idx += 1\n            continue\n        objsize = int(pool.dereference()['_object_size'])\n        span_size = pages[idx]['span_size'] * page_size\n        for idx2 in range(0, int(span_size / objsize) + 1):\n            obj_addr = mem_start + idx * page_size + idx2 * objsize\n            vptr = obj_addr.reinterpret_cast(vptr_type).dereference()\n            if is_vptr(vptr):\n                yield obj_addr, vptr\n        idx += pages[idx]['span_size']\n\n\ndef find_vptrs_of_type(vptr=None, typename=None):\n    \"\"\"\n    Return virtual objects whose vtable pointer equals vptr and/or matches typename.\n    typename has to be a prefix of the fully qualified name of the type\n    \"\"\"\n    for obj_addr, vtable_addr in find_vptrs():\n        if vptr is not None and vtable_addr != vptr:\n            continue\n        symbol_name = resolve(vtable_addr, startswith=typename)\n        if symbol_name is not None:\n            yield obj_addr, vtable_addr, symbol_name\n\n\ndef find_single_sstable_readers():\n    def _lookup_type(type_names):\n        n, t = lookup_type(type_names)\n        return (n, t.pointer())\n\n    types = []\n    try:\n        return intrusive_list(gdb.parse_and_eval('sstables::_reader_tracker._readers'), link='_tracker_link')\n    except gdb.error:\n        try:\n            # for Scylla <= 4.5\n            types = [_lookup_type(['sstables::sstable_mutation_reader<sstables::data_consume_rows_context_m, sstables::mp_row_consumer_m>',\n                                   'sstables::sstable_mutation_reader<sstables::mx::data_consume_rows_context_m, sstables::mx::mp_row_consumer_m>']),\n                     _lookup_type(['sstables::sstable_mutation_reader<sstables::data_consume_rows_context, sstables::mp_row_consumer_k_l>',\n                                   'sstables::sstable_mutation_reader<sstables::kl::data_consume_rows_context, sstables::kl::mp_row_consumer_k_l>'])]\n        except gdb.error:\n            types = [_lookup_type(['sstables::mx::mx_sstable_mutation_reader']),\n                     _lookup_type(['sstables::kl::sstable_mutation_reader'])]\n\n    def _lookup_obj(obj_addr, vtable_addr):\n        vtable_pfx = 'vtable for '\n        name = resolve(vtable_addr, startswith=vtable_pfx)\n        if not name:\n            return None\n        name = name[len(vtable_pfx):]\n        for type_name, ptr_type in types:\n            if name.startswith(type_name):\n                return obj_addr.reinterpret_cast(ptr_type)\n\n    for obj_addr, vtable_addr in find_vptrs():\n        obj = _lookup_obj(obj_addr, vtable_addr)\n        if obj:\n            yield obj\n\n\ndef find_active_sstables():\n    \"\"\" Yields sstable* once for each active sstable reader. \"\"\"\n    sstable_ptr_type = gdb.lookup_type('sstables::sstable').pointer()\n    for reader in find_single_sstable_readers():\n        sstable_ptr = reader['_sst']['_p']\n        yield sstable_ptr.reinterpret_cast(sstable_ptr_type)\n\n\nclass schema_ptr:\n    def __init__(self, ptr):\n        schema_ptr_type = gdb.lookup_type('schema').pointer()\n        self.ptr = ptr['_p'].reinterpret_cast(schema_ptr_type)\n\n    @property\n    def ks_name(self):\n        return str(self.ptr['_raw']['_ks_name'])[1:-1]\n\n    @property\n    def cf_name(self):\n        return str(self.ptr['_raw']['_cf_name'])[1:-1]\n\n    def table_name(self):\n        return '%s.%s' % (self.ptr['_raw']['_ks_name'], self.ptr['_raw']['_cf_name'])\n\n    def __getitem__(self, item):\n        return self.ptr[item]\n\n    def is_system(self):\n        return self.ks_name in [\"system\", \"system_schema\", \"system_distributed\", \"system_traces\", \"system_auth\", \"audit\"]\n\n\nclass scylla_active_sstables(gdb.Command):\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla active-sstables', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def invoke(self, arg, from_tty):\n        try:\n            sizeof_index_entry = int(gdb.parse_and_eval('sizeof(sstables::index_entry)'))\n            sizeof_entry = int(gdb.parse_and_eval('sizeof(sstables::partition_index_cache::entry)'))\n\n            def count_index_lists(sst):\n                index_lists_size = 0\n                for key, entry in intrusive_btree(std_unique_ptr(sst['_index_cache']).get()['_entries']):\n                    index_entries = std_vector(entry['list'])\n                    index_lists_size += sizeof_entry\n                    for e in index_entries:\n                        index_lists_size += sizeof_index_entry\n                        index_lists_size += e['_key']['_size']\n                        index_lists_size += e['_promoted_index_bytes']['_size']\n                return index_lists_size\n        except Exception:\n            count_index_lists = None\n\n        sstables = dict()  # name -> sstable*\n        for sst in find_active_sstables():\n            schema = schema_ptr(sst['_schema'])\n            id = '%s#%d' % (schema.table_name(), sst['_generation'])\n            if id in sstables:\n                sst, count = sstables[id]\n                sstables[id] = (sst, count + 1)\n                continue\n            sstables[id] = (sst, 1)\n\n        total_index_lists_size = 0\n        for id, (sst, count) in sstables.items():\n            if count_index_lists:\n                total_index_lists_size += count_index_lists(sst)\n            gdb.write('sstable %s, readers=%d data_file_size=%d\\n' % (id, count, sst['_data_file_size']))\n\n        gdb.write('sstable_count=%d, total_index_lists_size=%d\\n' % (len(sstables), total_index_lists_size))\n\n\nclass seastar_shared_ptr():\n    def __init__(self, ref):\n        self.ref = ref\n\n    def get(self):\n        return self.ref['_p']\n\n\nclass std_shared_ptr():\n    def __init__(self, ref):\n        self.ref = ref\n\n    def get(self):\n        return self.ref['_M_ptr']\n\n\nclass std_atomic():\n    def __init__(self, ref):\n        self.ref = ref\n\n    def get(self):\n        return self.ref['_M_i']\n\n\ndef has_enable_lw_shared_from_this(type):\n    for f in type.fields():\n        if f.is_base_class and 'enable_lw_shared_from_this' in f.name:\n            return True\n    return False\n\n\ndef remove_prefix(s, prefix):\n    if s.startswith(prefix):\n        return s[len(prefix):]\n    return s\n\n\nclass seastar_lw_shared_ptr():\n    def __init__(self, ref):\n        self.ref = ref\n        self.elem_type = ref.type.template_argument(0)\n\n    def _no_esft_type(self):\n        try:\n            return gdb.lookup_type('seastar::lw_shared_ptr_no_esft<%s>' % remove_prefix(str(self.elem_type.unqualified()), 'class ')).pointer()\n        except:\n            return gdb.lookup_type('seastar::shared_ptr_no_esft<%s>' % remove_prefix(str(self.elem_type.unqualified()), 'class ')).pointer()\n\n    def get(self):\n        if has_enable_lw_shared_from_this(self.elem_type):\n            return self.ref['_p'].cast(self.elem_type.pointer())\n        else:\n            return self.ref['_p'].cast(self._no_esft_type())['_value'].address\n\n\nclass lsa_region():\n    def __init__(self, region):\n        impl_ptr_type = gdb.lookup_type('logalloc::region_impl').pointer()\n        self.region = seastar_shared_ptr(region['_impl']).get().cast(impl_ptr_type)\n        self.segment_size = int(gdb.parse_and_eval('\\'logalloc::segment::size\\''))\n\n    def total(self):\n        size = int(self.region['_closed_occupancy']['_total_space'])\n        if int(self.region['_active_offset']) > 0:\n            size += self.segment_size\n        return size\n\n    def free(self):\n        return int(self.region['_closed_occupancy']['_free_space'])\n\n    def used(self):\n        return self.total() - self.free()\n\n    def impl(self):\n        return self.region\n\n\nclass dirty_mem_mgr():\n    def __init__(self, ref):\n        self.ref = ref\n\n    def real_dirty(self):\n        return int(self.ref['_region_group']['_real_total_memory'])\n\n    def unspooled(self):\n        return int(self.ref['_region_group']['_unspooled_total_memory'])\n\n\ndef find_instances(type_name):\n    \"\"\"\n    A generator for pointers to live objects of virtual type 'type_name'.\n    Only objects located at the beginning of allocation block are returned.\n    This is true, for instance, for all objects allocated using std::make_unique().\n    \"\"\"\n    ptr_type = gdb.lookup_type(type_name).pointer()\n    vtable_name = 'vtable for %s ' % type_name\n    for obj_addr, vtable_addr in find_vptrs():\n        name = resolve(vtable_addr, startswith=vtable_name)\n        if name:\n            yield gdb.Value(obj_addr).cast(ptr_type)\n\n\nclass span(object):\n    \"\"\"\n    Represents seastar allocator's memory span\n    \"\"\"\n\n    def __init__(self, index, start, page):\n        \"\"\"\n        :param index: index into cpu_mem.pages of the first page of the span\n        :param start: memory address of the first page of the span\n        :param page: seastar::memory::page* for the first page of the span\n        \"\"\"\n        self.index = index\n        self.start = start\n        self.page = page\n\n    def is_free(self):\n        return self.page['free']\n\n    def pool(self):\n        \"\"\"\n        Returns seastar::memory::small_pool* of this span.\n        Valid only when is_small().\n        \"\"\"\n        return self.page['pool']\n\n    def is_small(self):\n        return not self.is_free() and self.page['pool']\n\n    def is_large(self):\n        return not self.is_free() and not self.page['pool']\n\n    def size(self):\n        return int(self.page['span_size'])\n\n    def used_span_size(self):\n        \"\"\"\n        Returns the number of pages at the front of the span which are used by the allocator.\n\n        Due to https://github.com/scylladb/seastar/issues/625 there may be some\n        pages at the end of the span which are not used by the small pool.\n        We try to detect this. It's not 100% accurate but should work in most cases.\n\n        Returns 0 for free spans.\n        \"\"\"\n        n_pages = 0\n        pool = self.page['pool']\n        if self.page['free']:\n            return 0\n        if not pool:\n            return self.page['span_size']\n        for idx in range(int(self.page['span_size'])):\n            page = self.page.address + idx\n            if not page['pool'] or page['pool'] != pool or page['offset_in_span'] != idx:\n                break\n            n_pages += 1\n        return n_pages\n\n\ndef spans():\n    cpu_mem = gdb.parse_and_eval('\\'seastar::memory::cpu_mem\\'')\n    page_size = int(gdb.parse_and_eval('\\'seastar::memory::page_size\\''))\n    nr_pages = int(cpu_mem['nr_pages'])\n    pages = cpu_mem['pages']\n    mem_start = int(cpu_mem['memory'])\n    idx = 1\n    while idx < nr_pages:\n        page = pages[idx]\n        span_size = int(page['span_size'])\n        if span_size == 0:\n            idx += 1\n            continue\n        last_page = pages[idx + span_size - 1]\n        addr = mem_start + idx * page_size\n        yield span(idx, addr, page)\n        idx += span_size\n\n\nclass span_checker(object):\n    def __init__(self):\n        self._page_size = int(gdb.parse_and_eval('\\'seastar::memory::page_size\\''))\n        span_list = list(spans())\n        self._start_to_span = dict((s.start, s) for s in span_list)\n        self._starts = list(s.start for s in span_list)\n\n    def spans(self):\n        return self._start_to_span.values()\n\n    def get_span(self, ptr):\n        idx = bisect.bisect_right(self._starts, ptr)\n        if idx == 0:\n            return None\n        span_start = self._starts[idx - 1]\n        s = self._start_to_span[span_start]\n        if span_start + s.page['span_size'] * self._page_size <= ptr:\n            return None\n        return s\n\n\nclass scylla_memory(gdb.Command):\n    \"\"\"Summarize the state of the shard's memory.\n\n    The goal of this summary is to provide a starting point when investigating\n    memory issues.\n\n    The summary consists of two parts:\n    * A high level overview.\n    * A per size-class population statistics.\n\n    In an OOM situation the latter usually shows the immediate symptoms, one\n    or more heavily populated size classes eating up all memory. The overview\n    can be used to identify the subsystem that owns these problematic objects.\n    \"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla memory', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    @staticmethod\n    def summarize_inheriting_execution_stage(ies):\n        scheduling_group_names = {int(tq['_id']): str(tq['_name']) for tq in get_local_task_queues()}\n        per_sg_stages = []\n        i = 0\n        for es_opt in std_vector(ies['_stage_for_group']):\n            es_opt = std_optional(es_opt)\n            if not es_opt:\n                continue\n            es = es_opt.get()\n            enqueued = int(es['_stats']['function_calls_enqueued'])\n            executed = int(es['_stats']['function_calls_executed'])\n            size = enqueued - executed\n            if size > 0:\n                per_sg_stages.append((i, scheduling_group_names[i], size))\n            i += 1\n\n        return per_sg_stages\n\n    @staticmethod\n    def summarize_table_phased_barrier_users(db, barrier_name):\n        tables_by_count = defaultdict(list)\n        for table in for_each_table():\n            schema = schema_ptr(table['_schema'])\n            count = scylla_table.phased_barrier_count(table, barrier_name)\n            if count > 0:\n                tables_by_count[count].append(str(schema.table_name()).replace('\"', ''))\n\n        return [(c, tables_by_count[c]) for c in reversed(sorted(tables_by_count.keys()))]\n\n    @staticmethod\n    def summarize_storage_proxy_coordinator_stats(sp):\n        try:\n            return sp['_stats'], {None: sp['_stats']}\n        except gdb.error: # > 3.3\n            pass\n\n        stats_ptr_type = gdb.lookup_type('service::storage_proxy::stats').pointer()\n        key_id = int(sp['_stats_key']['_id'])\n        per_sg_stats = {}\n        reactor = gdb.parse_and_eval('seastar::local_engine')\n        for tq in get_local_task_queues():\n            sched_group_specific = std_array(reactor['_scheduling_group_specific_data']['per_scheduling_group_data'])[int(tq['_id'])]['specific_vals']\n            stats = std_vector(sched_group_specific)[key_id].reinterpret_cast(stats_ptr_type).dereference()\n            if int(stats['writes']) == 0 and int(stats['background_writes']) == 0 and int(stats['foreground_reads']) == 0 and int(stats['reads']) == 0:\n                continue\n            per_sg_stats[tq] = stats\n\n        return sp['_global_stats'], per_sg_stats\n\n    @staticmethod\n    def print_coordinator_stats():\n        try:\n            sp = sharded(gdb.parse_and_eval('debug::the_storage_proxy')).local()\n        except gdb.error:\n            sp = sharded(gdb.parse_and_eval('service::_the_storage_proxy')).local()\n        if not sp:\n            return\n        global_sp_stats, per_sg_sp_stats = scylla_memory.summarize_storage_proxy_coordinator_stats(sp)\n\n        hm = sp['_hints_manager']\n        view_hm = sp['_hints_for_views_manager']\n\n        gdb.write('Coordinator:\\n'\n                '  bg write bytes: {bg_wr_bytes:>13} B\\n'\n                '  hints:          {regular:>13} B\\n'\n                '  view hints:     {views:>13} B\\n'\n                .format(\n                        bg_wr_bytes=int(global_sp_stats['background_write_bytes']),\n                        regular=int(hm['_stats']['size_of_hints_in_progress']),\n                        views=int(view_hm['_stats']['size_of_hints_in_progress'])))\n\n        for sg_tq, stats in per_sg_sp_stats.items():\n            if sg_tq is None:\n                sg_header = ''\n            else:\n                sg_header = '  {sg_id:02} {sg_name}\\n'.format(sg_id=int(sg_tq['_id']), sg_name=str(sg_tq['_name']))\n\n            gdb.write(\n                    '{sg_header}'\n                    '    fg writes:  {fg_wr:>13}\\n'\n                    '    bg writes:  {bg_wr:>13}\\n'\n                    '    fg reads:   {fg_rd:>13}\\n'\n                    '    bg reads:   {bg_rd:>13}\\n'\n                    .format(\n                        sg_header=sg_header,\n                        fg_wr=int(stats['writes']) - int(stats['background_writes']),\n                        bg_wr=int(stats['background_writes']),\n                        fg_rd=int(stats['foreground_reads']),\n                        bg_rd=int(stats['reads']) - int(stats['foreground_reads'])))\n\n        gdb.write('\\n')\n\n    @staticmethod\n    def format_semaphore_stats(semaphore):\n        # older versions had names like \"_user_concurrency_semaphore\"\n        semaphore_name = \"{}:\".format(str(semaphore['_name']).removeprefix('_').removesuffix('_concurrency_semaphore'))\n        initial_count = int(semaphore[\"_initial_resources\"][\"count\"])\n        initial_memory = int(semaphore[\"_initial_resources\"][\"memory\"])\n        used_count = initial_count - int(semaphore[\"_resources\"][\"count\"])\n        used_memory = initial_memory - int(semaphore[\"_resources\"][\"memory\"])\n        waiters = int(semaphore[\"_stats\"][\"waiters\"])\n        return f'{semaphore_name:<16} {used_count:>3}/{initial_count:>3}, {used_memory:>13}/{initial_memory:>13}, queued: {waiters}'\n\n    @staticmethod\n    def print_replica_stats():\n        db = find_db()\n        if not db:\n            return\n\n        per_service_level_sem = []\n        for sg, sem in unordered_map(db[\"_reader_concurrency_semaphores_group\"][\"_semaphores\"]):\n            per_service_level_sem.append(scylla_memory.format_semaphore_stats(sem[\"sem\"]))\n\n        per_service_level_vu_sem = []\n        for sg, sem in unordered_map(db[\"_view_update_read_concurrency_semaphores_group\"][\"_semaphores\"]):\n            per_service_level_vu_sem.append(scylla_memory.format_semaphore_stats(sem[\"sem\"]))\n\n        database_typename = lookup_type(['replica::database', 'database'])[1].name\n        gdb.write('Replica:\\n')\n        gdb.write('  Read Concurrency Semaphores:\\n    {}\\n    {}\\n    {}\\n    {}\\n'.format(\n                '\\n    '.join(per_service_level_sem),\n                scylla_memory.format_semaphore_stats(db['_streaming_concurrency_sem']),\n                scylla_memory.format_semaphore_stats(db['_system_read_concurrency_sem']),\n                '\\n    '.join(per_service_level_vu_sem)))\n\n        gdb.write('  Execution Stages:\\n')\n        for es_path in [('_apply_stage',)]:\n            machine_name = es_path[0]\n            human_name = machine_name.replace('_', ' ').strip()\n            total = 0\n\n            gdb.write('    {}:\\n'.format(human_name))\n            es = db\n            for path_component in es_path:\n                try:\n                    es = es[path_component]\n                except gdb.error:\n                    break\n\n            for sg_id, sg_name, count in scylla_memory.summarize_inheriting_execution_stage(es):\n                total += count\n                gdb.write('      {:02} {:32} {}\\n'.format(sg_id, sg_name, count))\n            gdb.write('         {:32} {}\\n'.format('Total', total))\n\n        gdb.write('  Tables - Ongoing Operations:\\n')\n        for machine_name in ['_pending_writes_phaser', '_pending_reads_phaser', '_pending_streams_phaser']:\n            human_name = machine_name.replace('_', ' ').strip()\n            gdb.write('    {} (top 10):\\n'.format(human_name))\n            total = 0\n            i = 0\n            for count, tables in scylla_memory.summarize_table_phased_barrier_users(db, machine_name):\n                total += count\n                if i < 10:\n                    gdb.write('      {:9} {}\\n'.format(count, ', '.join(tables)))\n                i += 1\n            gdb.write('      {:9} Total (all)\\n'.format(total))\n        gdb.write('\\n')\n\n    def invoke(self, arg, from_tty):\n        cpu_mem = gdb.parse_and_eval('\\'seastar::memory::cpu_mem\\'')\n        page_size = int(gdb.parse_and_eval('\\'seastar::memory::page_size\\''))\n        free_mem = int(cpu_mem['nr_free_pages']) * page_size\n        total_mem = int(cpu_mem['nr_pages']) * page_size\n        gdb.write('Used memory: {used_mem:>13}\\nFree memory: {free_mem:>13}\\nTotal memory: {total_mem:>12}\\n\\n'\n                  .format(used_mem=total_mem - free_mem, free_mem=free_mem, total_mem=total_mem))\n\n        lsa = get_lsa_segment_pool()\n        segment_size = int(gdb.parse_and_eval('\\'logalloc::segment::size\\''))\n        lsa_free = int(lsa['_free_segments']) * segment_size\n        non_lsa_mem = int(lsa['_non_lsa_memory_in_use'])\n        lsa_used = int(lsa['_segments_in_use']) * segment_size + non_lsa_mem\n        lsa_allocated = lsa_used + lsa_free\n\n        gdb.write('LSA:\\n'\n                  '  allocated: {lsa:>13}\\n'\n                  '  used:      {lsa_used:>13}\\n'\n                  '  free:      {lsa_free:>13}\\n\\n'\n                  .format(lsa=lsa_allocated, lsa_used=lsa_used, lsa_free=lsa_free))\n\n        db = find_db()\n        cache_region = lsa_region(db['_row_cache_tracker']['_region'])\n\n        gdb.write('Cache:\\n'\n                  '  total:     {cache_total:>13}\\n'\n                  '  used:      {cache_used:>13}\\n'\n                  '  free:      {cache_free:>13}\\n\\n'\n                  .format(cache_total=cache_region.total(), cache_used=cache_region.used(), cache_free=cache_region.free()))\n\n        gdb.write('Memtables:\\n'\n                  ' total:       {total:>13}\\n'\n                  ' Regular:\\n'\n                  '  real dirty: {reg_real_dirty:>13}\\n'\n                  '  unspooled:  {reg_unspooled:>13}\\n'\n                  ' System:\\n'\n                  '  real dirty: {sys_real_dirty:>13}\\n'\n                  '  unspooled:  {sys_unspooled:>13}\\n\\n'\n                  .format(total=(lsa_allocated-cache_region.total()),\n                          reg_real_dirty=dirty_mem_mgr(db['_dirty_memory_manager']).real_dirty(),\n                          reg_unspooled=dirty_mem_mgr(db['_dirty_memory_manager']).unspooled(),\n                          sys_real_dirty=dirty_mem_mgr(db['_system_dirty_memory_manager']).real_dirty(),\n                          sys_unspooled=dirty_mem_mgr(db['_system_dirty_memory_manager']).unspooled()))\n\n        scylla_memory.print_coordinator_stats()\n        scylla_memory.print_replica_stats()\n\n        gdb.write('Small pools:\\n')\n        small_pools = cpu_mem['small_pools']\n        nr = small_pools['nr_small_pools']\n        gdb.write('{objsize:>5} {span_size:>6} {use_count:>10} {memory:>12} {unused:>12} {wasted_percent:>5}\\n'\n                  .format(objsize='objsz', span_size='spansz', use_count='usedobj', memory='memory',\n                          unused='unused', wasted_percent='wst%'))\n        total_small_bytes = 0\n        sc = span_checker()\n        free_object_size = gdb.parse_and_eval('sizeof(\\'seastar::memory::free_object\\')')\n        for i in range(int(nr)):\n            sp = small_pools['_u']['a'][i]\n            object_size = int(sp['_object_size'])\n            # Skip pools that are smaller than sizeof(free_object), they won't have any content\n            if object_size < free_object_size:\n                continue\n            span_size = int(sp['_span_sizes']['preferred']) * page_size\n            free_count = int(sp['_free_count'])\n            pages_in_use = 0\n            use_count = 0\n            for s in sc.spans():\n                if not s.is_free() and s.pool() == sp.address:\n                    pages_in_use += s.size()\n                    use_count += int(s.used_span_size() * page_size / object_size)\n            memory = pages_in_use * page_size\n            total_small_bytes += memory\n            use_count -= free_count\n            wasted = free_count * object_size\n            unused = memory - use_count * object_size\n            wasted_percent = wasted * 100.0 / memory if memory else 0\n            gdb.write('{objsize:5} {span_size:6} {use_count:10} {memory:12} {unused:12} {wasted_percent:5.1f}\\n'\n                      .format(objsize=object_size, span_size=span_size, use_count=use_count, memory=memory, unused=unused,\n                              wasted_percent=wasted_percent))\n        gdb.write('Small allocations: %d [B]\\n' % total_small_bytes)\n\n        large_allocs = defaultdict(int) # key: span size [B], value: span count\n        for s in sc.spans():\n            span_size = s.size()\n            if s.is_large():\n                large_allocs[span_size * page_size] += 1\n\n        gdb.write('Page spans:\\n')\n        gdb.write('{index:5} {size:>13} {total:>13} {allocated_size:>13} {allocated_count:>7}\\n'.format(\n            index=\"index\", size=\"size [B]\", total=\"free [B]\", allocated_size=\"large [B]\", allocated_count=\"[spans]\"))\n        total_large_bytes = 0\n        for index in range(int(cpu_mem['nr_span_lists'])):\n            span_list = cpu_mem['free_spans'][index]\n            front = int(span_list['_front'])\n            pages = cpu_mem['pages']\n            total = 0\n            while front:\n                span = pages[front]\n                total += int(span['span_size'])\n                front = int(span['link']['_next'])\n            span_size = (1 << index) * page_size\n            allocated_size = large_allocs[span_size] * span_size\n            total_large_bytes += allocated_size\n            gdb.write('{index:5} {size:13} {total:13} {allocated_size:13} {allocated_count:7}\\n'.format(index=index, size=span_size, total=total * page_size,\n                                                                allocated_count=large_allocs[span_size],\n                                                                allocated_size=allocated_size))\n        gdb.write('Large allocations: %d [B]\\n' % total_large_bytes)\n\n\nclass TreeNode(object):\n    def __init__(self, key):\n        self.key = key\n        self.children_by_key = {}\n\n    def get_or_add(self, key):\n        node = self.children_by_key.get(key, None)\n        if not node:\n            node = self.__class__(key)\n            self.add(node)\n        return node\n\n    def add(self, node):\n        self.children_by_key[node.key] = node\n\n    def squash_child(self):\n        assert self.has_only_one_child()\n        self.children_by_key = next(iter(self.children)).children_by_key\n\n    @property\n    def children(self):\n        return self.children_by_key.values()\n\n    def has_only_one_child(self):\n        return len(self.children_by_key) == 1\n\n    def has_children(self):\n        return bool(self.children_by_key)\n\n    def remove_all(self):\n        self.children_by_key.clear()\n\n\nclass ProfNode(TreeNode):\n    def __init__(self, key):\n        super(ProfNode, self).__init__(key)\n        self.size = 0\n        self.count = 0\n        self.tail = []\n\n    @property\n    def attributes(self):\n        return {\n            'size': self.size,\n            'count': self.count\n        }\n\n\ndef collapse_similar(node):\n    while node.has_only_one_child():\n        child = next(iter(node.children))\n        if node.attributes == child.attributes:\n            node.squash_child()\n            node.tail.append(child.key)\n        else:\n            break\n\n    for child in node.children:\n        collapse_similar(child)\n\n\ndef strip_level(node, level):\n    if level <= 0:\n        node.remove_all()\n    else:\n        for child in node.children:\n            strip_level(child, level - 1)\n\n\ndef print_tree(root_node,\n               formatter=attrgetter('key'),\n               order_by=attrgetter('key'),\n               printer=sys.stdout.write,\n               node_filter=None):\n\n    def print_node(node, is_last_history):\n        stems = (\" |   \", \"     \")\n        branches = (\" |-- \", r\" \\-- \")\n\n        label_lines = formatter(node).rstrip('\\n').split('\\n')\n        prefix_without_branch = ''.join(map(stems.__getitem__, is_last_history[:-1]))\n\n        if is_last_history:\n            printer(prefix_without_branch)\n            printer(branches[is_last_history[-1]])\n        printer(\"%s\\n\" % label_lines[0])\n\n        for line in label_lines[1:]:\n            printer(''.join(map(stems.__getitem__, is_last_history)))\n            printer(\"%s\\n\" % line)\n\n        children = sorted(filter(node_filter, node.children), key=order_by)\n        if children:\n            for child in children[:-1]:\n                print_node(child, is_last_history + [False])\n            print_node(children[-1], is_last_history + [True])\n\n        is_last = not is_last_history or is_last_history[-1]\n        if not is_last:\n            printer(\"%s%s\\n\" % (prefix_without_branch, stems[False]))\n\n    if not node_filter or node_filter(root_node):\n        print_node(root_node, [])\n\n\nclass scylla_heapprof(gdb.Command):\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla heapprof', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def invoke(self, arg, from_tty):\n        parser = argparse.ArgumentParser(description=\"scylla heapprof\")\n        parser.add_argument(\"-G\", \"--inverted\", action=\"store_true\",\n                            help=\"Compute caller-first profile instead of callee-first\")\n        parser.add_argument(\"-a\", \"--addresses\", action=\"store_true\",\n                            help=\"Show raw addresses before resolved symbol names\")\n        parser.add_argument(\"--no-symbols\", action=\"store_true\",\n                            help=\"Show only raw addresses\")\n        parser.add_argument(\"--flame\", action=\"store_true\",\n                            help=\"Write flamegraph data to heapprof.stacks instead of showing the profile\")\n        parser.add_argument(\"--min\", action=\"store\", type=int, default=0,\n                            help=\"Drop branches allocating less than given amount\")\n        try:\n            args = parser.parse_args(arg.split())\n        except SystemExit:\n            return\n\n        root = ProfNode(None)\n        cpu_mem = gdb.parse_and_eval('\\'seastar::memory::cpu_mem\\'')\n        site = cpu_mem['alloc_site_list_head']\n\n        while site:\n            size = int(site['size'])\n            count = int(site['count'])\n            if size:\n                n = root\n                n.size += size\n                n.count += count\n                bt = site['backtrace']['_main']\n                addresses = list(int(f['addr']) for f in static_vector(bt['_frames']))\n                addresses.pop(0)  # drop memory::get_backtrace()\n                if args.inverted:\n                    seq = reversed(addresses)\n                else:\n                    seq = addresses\n                for addr in seq:\n                    n = n.get_or_add(addr)\n                    n.size += size\n                    n.count += count\n            site = site['next']\n\n        def resolver(addr):\n            if args.no_symbols:\n                return '0x%x' % addr\n            if args.addresses:\n                return '0x%x %s' % (addr, resolve(addr) or '')\n            return resolve(addr) or ('0x%x' % addr)\n\n        if args.flame:\n            file_name = 'heapprof.stacks'\n            with open(file_name, 'w') as out:\n                trace = list()\n\n                def print_node(n):\n                    if n.key:\n                        trace.append(n.key)\n                        trace.extend(n.tail)\n                    for c in n.children:\n                        print_node(c)\n                    if not n.has_children():\n                        out.write(\"%s %d\\n\" % (';'.join(map(lambda x: '%s' % (x), map(resolver, trace))), n.size))\n                    if n.key:\n                        del trace[-1 - len(n.tail):]\n                print_node(root)\n            gdb.write('Wrote %s\\n' % (file_name))\n        else:\n            def node_formatter(n):\n                if n.key is None:\n                    name = \"All\"\n                else:\n                    name = resolver(n.key)\n                return \"%s (%d, #%d)\\n%s\" % (name, n.size, n.count, '\\n'.join(map(resolver, n.tail)))\n\n            def node_filter(n):\n                return n.size >= args.min\n\n            collapse_similar(root)\n            print_tree(root,\n                       formatter=node_formatter,\n                       order_by=lambda n: -n.size,\n                       node_filter=node_filter,\n                       printer=gdb.write)\n\n\ndef get_seastar_memory_start_and_size():\n    cpu_mem = gdb.parse_and_eval('\\'seastar::memory::cpu_mem\\'')\n    page_size = int(gdb.parse_and_eval('\\'seastar::memory::page_size\\''))\n    total_mem = int(cpu_mem['nr_pages']) * page_size\n    start = int(cpu_mem['memory'])\n    return start, total_mem\n\n\ndef seastar_memory_layout():\n    results = []\n    for t in reactor_threads():\n        start, total_mem = get_seastar_memory_start_and_size()\n        results.append((t, start, total_mem))\n    return results\n\n\ndef get_thread_owning_memory(ptr):\n    for t in reactor_threads():\n        start, size = get_seastar_memory_start_and_size()\n        if start <= ptr < start + size:\n            return t\n\n\nclass pointer_metadata(object):\n    def __init__(self, ptr, *args):\n        if isinstance(args[0], gdb.InferiorThread):\n            self._init_seastar_ptr(ptr, *args)\n        else:\n            self._init_generic_ptr(ptr, *args)\n\n    def _init_seastar_ptr(self, ptr, thread):\n        self.ptr = ptr\n        self.thread = thread\n        self._is_containing_page_free = False\n        self.is_small = False\n        self.is_live = False\n        self.is_lsa = False\n        self.size = 0\n        self.offset_in_object = 0\n\n    def _init_generic_ptr(self, ptr, speculative_size):\n        self.ptr = ptr\n        self.thread = None\n        self._is_containing_page_free = None\n        self.is_small = None\n        self.is_live = None\n        self.is_lsa = None\n        self.size = speculative_size\n        self.offset_in_object = 0\n\n    def is_managed_by_seastar(self):\n        return self.thread is not None\n\n    @property\n    def is_containing_page_free(self):\n        return self._is_containing_page_free\n\n    def mark_free(self):\n        self._is_containing_page_free = True\n        self._is_live = False\n\n    @property\n    def obj_ptr(self):\n        return self.ptr - self.offset_in_object\n\n    def __str__(self):\n        if not self.is_managed_by_seastar():\n            return \"0x{:x} (default allocator)\".format(self.ptr)\n\n        msg = \"thread %d\" % self.thread.num\n\n        if self.is_containing_page_free:\n            msg += ', page is free'\n            return msg\n\n        if self.is_small:\n            msg += ', small (size <= %d)' % self.size\n        else:\n            msg += ', large (size=%d)' % self.size\n\n        if self.is_live:\n            msg += ', live (0x%x +%d)' % (self.obj_ptr, self.offset_in_object)\n        else:\n            msg += ', free (0x%x +%d)' % (self.obj_ptr, self.offset_in_object)\n\n        if self.is_lsa:\n            msg += ', LSA-managed'\n\n        return msg\n\n\ndef get_segment_base(segment_pool):\n    try:\n        segment_store = segment_pool[\"_store\"]\n        try:\n            return int(std_unique_ptr(segment_store[\"_backend\"]).get()[\"_segments_base\"])\n        except gdb.error:\n            return int(segment_store[\"_segments_base\"])\n    except gdb.error:\n        return int(segment_pool[\"_segments_base\"])\n\n\nclass scylla_ptr(gdb.Command):\n    _is_seastar_allocator_used = None\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla ptr', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    @staticmethod\n    def is_seastar_allocator_used():\n        if scylla_ptr._is_seastar_allocator_used is not None:\n            return scylla_ptr._is_seastar_allocator_used\n\n        try:\n            gdb.parse_and_eval('&\\'seastar::memory::cpu_mem\\'')\n            scylla_ptr._is_seastar_allocator_used = True\n            return True\n        except:\n            scylla_ptr._is_seastar_allocator_used = False\n            return False\n\n    @staticmethod\n    def _do_analyze(ptr):\n        owning_thread = None\n        for t, start, size in seastar_memory_layout():\n            if ptr >= start and ptr < start + size:\n                owning_thread = t\n                break\n\n        ptr_meta = pointer_metadata(ptr, owning_thread)\n\n        if not owning_thread:\n            return ptr_meta\n\n        owning_thread.switch()\n\n        cpu_mem = gdb.parse_and_eval('\\'seastar::memory::cpu_mem\\'')\n        page_size = int(gdb.parse_and_eval('\\'seastar::memory::page_size\\''))\n        offset = ptr - int(cpu_mem['memory'])\n        ptr_page_idx = offset / page_size\n        pages = cpu_mem['pages']\n        page = pages[ptr_page_idx]\n\n        span = span_checker().get_span(ptr)\n        offset_in_span = ptr - span.start\n        if offset_in_span >= span.used_span_size() * page_size:\n            ptr_meta.mark_free()\n        elif span.is_small():\n            pool = span.pool()\n            object_size = int(pool['_object_size'])\n            ptr_meta.size = object_size\n            ptr_meta.is_small = True\n            offset_in_object = offset_in_span % object_size\n            free_object_ptr = gdb.lookup_type('void').pointer().pointer()\n            char_ptr = gdb.lookup_type('char').pointer()\n            # pool's free list\n            next_free = pool['_free']\n            free = False\n            while next_free:\n                if ptr >= next_free and ptr < next_free.reinterpret_cast(char_ptr) + object_size:\n                    free = True\n                    break\n                next_free = next_free.reinterpret_cast(free_object_ptr).dereference()\n            if not free:\n                # span's free list\n                first_page_in_span = span.page\n                next_free = first_page_in_span['freelist']\n                while next_free:\n                    if ptr >= next_free and ptr < next_free.reinterpret_cast(char_ptr) + object_size:\n                        free = True\n                        break\n                    next_free = next_free.reinterpret_cast(free_object_ptr).dereference()\n            ptr_meta.offset_in_object = offset_in_object\n            ptr_meta.is_live = not free\n        else:\n            ptr_meta.is_small = False\n            ptr_meta.is_live = not span.is_free()\n            ptr_meta.size = span.size() * page_size\n            ptr_meta.offset_in_object = ptr - span.start\n\n        # FIXME: handle debug-mode build\n        segment_pool = get_lsa_segment_pool()\n        segments_base = get_segment_base(segment_pool)\n        segment_size = int(gdb.parse_and_eval('\\'logalloc::segment\\'::size'))\n        index = int((int(ptr) - segments_base) / segment_size)\n        desc = std_vector(segment_pool[\"_segments\"])[index]\n        ptr_meta.is_lsa = bool(desc['_region'])\n\n        return ptr_meta\n\n    @staticmethod\n    def analyze(ptr):\n        orig = gdb.selected_thread()\n        try:\n            return scylla_ptr._do_analyze(ptr)\n        finally:\n            orig.switch()\n\n    def invoke(self, arg, from_tty):\n        ptr = int(gdb.parse_and_eval(arg))\n\n        ptr_meta = self.analyze(ptr)\n\n        gdb.write(\"{}\\n\".format(str(ptr_meta)))\n\n\nclass segment_descriptor:\n    def __init__(self, ref):\n        self.ref = ref\n\n    def region(self):\n        return self.ref['_region']\n\n    def is_lsa(self):\n        return bool(self.ref['_region'])\n\n    def free_space(self):\n        return int(gdb.parse_and_eval('%d & (\\'logalloc\\'::segment::size_mask)'\n                                      % (self.ref['_free_space'])))\n\n    @property\n    def address(self):\n        return self.ref.address\n\n\nclass scylla_segment_descs(gdb.Command):\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla segment-descs', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def invoke(self, arg, from_tty):\n        segment_size = int(gdb.parse_and_eval('\\'logalloc\\'::segment::size'))\n        segment_pool = get_lsa_segment_pool()\n        base = get_segment_base(segment_pool)\n\n        def print_desc(seg_addr, desc):\n            if desc.is_lsa():\n                gdb.write('0x%x: lsa free=%-6d used=%-6d %6.2f%% region=0x%x\\n' % (seg_addr, desc.free_space(),\n                    segment_size - int(desc.free_space()),\n                    float(segment_size - int(desc.free_space())) * 100 / segment_size,\n                    int(desc.region())))\n            else:\n                gdb.write('0x%x: std\\n' % (seg_addr))\n\n        if base is None: # debug mode build\n            segs = std_vector(segment_pool[\"_store\"][\"_segments\"])\n            descs = std_vector(segment_pool[\"_segments\"])\n\n            for seg, desc_ref in zip(segs, descs):\n                desc = segment_descriptor(desc_ref)\n                print_desc(int(seg), desc)\n        else:\n            addr = base\n            for desc in std_vector(segment_pool[\"_segments\"]):\n                desc = segment_descriptor(desc)\n                print_desc(addr, desc)\n                addr += segment_size\n\n\ndef shard_of(ptr):\n    return (int(ptr) >> 36) & 0xff\n\n\nclass scylla_lsa_check(gdb.Command):\n    \"\"\"\n    Runs consistency checks on the LSA state on current shard.\n    \"\"\"\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla lsa-check', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def invoke(self, arg, from_tty):\n        segment_size = int(gdb.parse_and_eval('\\'logalloc\\'::segment::size'))\n\n        db = find_db()\n\n        # Collect segment* which are present in _segment_descs\n        in_buckets = set()\n        cache_region = lsa_region(db['_row_cache_tracker']['_region'])\n        shard = shard_of(cache_region.impl())\n        for bucket in std_array(cache_region.impl()['_segment_descs']['_buckets']):\n            for desc in intrusive_list(bucket):\n                if shard_of(int(desc.address)) != shard:\n                    gdb.write('ERROR: Out-of-shard segment: (logalloc::segment_descriptor*)0x%x, free_space=%d\\n'\n                              % (int(desc.address), desc.free_space()))\n                in_buckets.add(int(desc.address))\n\n        # Scan shard's segment_descriptor:s for anomalies:\n        #  - detect segments owned by cache which are not in cache region's _segment_descs\n        #  - compute segment occupancy statistics for comparison with region's stored ones\n        segment_pool = get_lsa_segment_pool()\n        base = get_segment_base(segment_pool)\n        desc_free_space = 0\n        desc_total_space = 0\n        for desc in std_vector(segment_pool[\"_segments\"]):\n            desc = segment_descriptor(desc)\n            if desc.is_lsa() and desc.region() == cache_region.impl():\n                if not int(desc.address) in in_buckets:\n                    if cache_region.impl()['_active'] != base and cache_region.impl()['_buf_active'] != base:\n                        # stray = not in _closed_segments\n                        gdb.write('ERROR: Stray segment: (logalloc::segment*)0x%x, (logalloc::segment_descriptor*)0x%x, free_space=%d\\n'\n                              % (base, int(desc.address), desc.free_space()))\n                else:\n                    desc_free_space += desc.free_space()\n                    desc_total_space += segment_size\n            base += segment_size\n\n        region_free_space = int(cache_region.impl()['_closed_occupancy']['_free_space'])\n        if region_free_space != desc_free_space:\n            gdb.write(\"ERROR: free space in closed segments according to region: %d\\n\" % region_free_space)\n            gdb.write(\"ERROR: free space in closed segments according to segment descriptors: %d (diff: %d)\\n\"\n                      % (desc_free_space, desc_free_space - region_free_space))\n\n        region_total_space = int(cache_region.impl()['_closed_occupancy']['_total_space'])\n        if region_total_space != desc_total_space:\n            gdb.write(\"ERROR: total space in closed segments according to region: %d\\n\" % region_total_space)\n            gdb.write(\"ERROR: total space in closed segments according to segment descriptors: %d (diff: %d)\\n\"\n                      % (desc_total_space, desc_total_space - region_total_space))\n\n\nclass scylla_lsa(gdb.Command):\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla lsa', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def invoke(self, arg, from_tty):\n        lsa = get_lsa_segment_pool()\n        segment_size = int(gdb.parse_and_eval('\\'logalloc::segment::size\\''))\n\n        lsa_mem = int(lsa['_segments_in_use']) * segment_size\n        non_lsa_mem = int(lsa['_non_lsa_memory_in_use'])\n        total_mem = lsa_mem + non_lsa_mem\n        gdb.write('Log Structured Allocator\\n\\nLSA memory in use: {lsa_mem:>16}\\n'\n                  'Non-LSA memory in use: {non_lsa_mem:>12}\\nTotal memory in use: {total_mem:>14}\\n\\n'\n                  .format(lsa_mem=lsa_mem, non_lsa_mem=non_lsa_mem, total_mem=total_mem))\n\n        er_goal = int(lsa['_current_emergency_reserve_goal'])\n        er_max = int(lsa['_emergency_reserve_max'])\n        free_segments = int(lsa['_free_segments'])\n        gdb.write('Emergency reserve goal: {er_goal:>11}\\n'\n                  'Emergency reserve max: {er_max:>12}\\n'\n                  'Free segments:         {free_segments:>12}\\n\\n'\n                  .format(er_goal=er_goal, er_max=er_max, free_segments=free_segments))\n\n        for region in lsa_regions():\n            gdb.write('    Region #{r_id} (logalloc::region_impl*) {r_addr}\\n      - reclaimable: {r_en:>14}\\n'\n                      '      - evictable: {r_ev:16}\\n      - non-LSA memory: {r_non_lsa:>11}\\n'\n                      '      - closed LSA memory: {r_lsa:>8}\\n      - unused memory: {r_unused:>12}\\n'\n                      .format(r_addr=str(region.dereference()), r_id=int(region['_id']), r_en=bool(region['_reclaiming_enabled']),\n                              r_ev=bool(region['_evictable']),\n                              r_non_lsa=int(region['_non_lsa_occupancy']['_total_space']),\n                              r_lsa=int(region['_closed_occupancy']['_total_space']),\n                              r_unused=int(region['_closed_occupancy']['_free_space'])))\n\n\nnames = {}  # addr (int) -> name (str)\n\n\ndef resolve(addr, cache=True, startswith=None):\n    if addr in names:\n        return names[addr]\n\n    infosym = gdb.execute('info symbol 0x%x' % (addr), False, True)\n    if infosym.startswith('No symbol'):\n        return None\n\n    name = infosym[:infosym.find('in section')]\n    if startswith and not name.startswith(startswith):\n        return None\n    if cache:\n        names[addr] = name\n    return name\n\n\nclass lsa_regions(object):\n    def __init__(self):\n        lsa_tracker = std_unique_ptr(gdb.parse_and_eval('\\'logalloc::tracker_instance\\'._impl'))\n        self._regions = lsa_tracker['_regions']\n        self._region = self._regions['_M_impl']['_M_start']\n\n    def __iter__(self):\n        while self._region != self._regions['_M_impl']['_M_finish']:\n            yield self._region\n            self._region = self._region + 1\n\n\nclass lsa_object_descriptor(object):\n    @staticmethod\n    def decode(pos):\n        start_pos = pos\n        b = pos.dereference() & 0xff\n        pos += 1\n        if not (b & 0x40):\n            raise Exception('object descriptor at 0x%x does not start with 0x40: 0x%x' % (int(start_pos), int(b)))\n        value = b & 0x3f\n        shift = 0\n        while not (b & 0x80):\n            shift += 6\n            b = pos.dereference() & 0xff\n            pos += 1\n            value |= (b & 0x3f) << shift\n        return lsa_object_descriptor(value, start_pos, pos)\n    mig_re = re.compile(r'.* standard_migrator<(.*)>\\+16>,')\n\n    def __init__(self, value, desc_pos, obj_pos):\n        self.value = value\n        self.desc_pos = desc_pos\n        self.obj_pos = obj_pos\n\n    def is_live(self):\n        return (self.value & 1) == 1\n\n    def dead_size(self):\n        return self.value / 2\n\n    def migrator_ptr(self):\n        static_migrators = gdb.parse_and_eval(\"'::debug::static_migrators'\")\n        return static_migrators['_migrators']['_M_impl']['_M_start'][self.value >> 1]\n\n    def migrator(self):\n        return self.migrator_ptr().dereference()\n\n    def migrator_str(self):\n        mig = str(self.migrator())\n        m = re.match(self.mig_re, mig)\n        return m.group(1)\n\n    def live_size(self):\n        mig = self.migrator_ptr()\n        obj = int(self.obj_pos)\n        cmd = f'((migrate_fn_type*){mig})->size((const void*){obj})'\n        res = gdb.parse_and_eval(cmd)\n        return int(res)\n\n    def end_pos(self):\n        if self.is_live():\n            return self.obj_pos + self.live_size()\n        else:\n            return self.desc_pos + self.dead_size()\n\n    def __str__(self):\n        if self.is_live():\n            return '0x%x: live %s @ 0x%x size=%d' % (int(self.desc_pos), self.migrator(),\n                                                     int(self.obj_pos), self.live_size())\n        else:\n            return '0x%x: dead size=%d' % (int(self.desc_pos), self.dead_size())\n\n\nclass scylla_lsa_segment(gdb.Command):\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla lsa-segment', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def invoke(self, arg, from_tty):\n        # See logalloc::region_impl::for_each_live()\n\n        try:\n            logalloc_alignment = gdb.parse_and_eval(\"'::debug::logalloc_alignment'\")\n        except gdb.error:\n            # optimized-out and/or garbage-collected by ld, which\n            # _probably_ means the mode is not \"sanitize\", so:\n            logalloc_alignment = 1\n\n        logalloc_alignment_mask = logalloc_alignment - 1\n\n        ptr = int(arg, 0)\n        seg = gdb.parse_and_eval('(char*)(%d & ~(\\'logalloc\\'::segment::size - 1))' % (ptr))\n        segment_size = int(gdb.parse_and_eval('\\'logalloc\\'::segment::size'))\n        seg_end = seg + segment_size\n        while seg < seg_end:\n            desc = lsa_object_descriptor.decode(seg)\n            print(desc)\n            seg = desc.end_pos()\n            seg = gdb.parse_and_eval('(char*)((%d + %d) & ~%d)' % (seg, logalloc_alignment_mask, logalloc_alignment_mask))\n\n\nclass scylla_timers(gdb.Command):\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla timers', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def invoke(self, arg, from_tty):\n        gdb.write('Timers:\\n')\n        timer_set = gdb.parse_and_eval('\\'seastar\\'::local_engine->_timers')\n        for timer_list in std_array(timer_set['_buckets']):\n            for t in intrusive_list(timer_list, link='_link'):\n                gdb.write('(%s*) %s = %s\\n' % (t.type, t.address, t))\n        timer_set = gdb.parse_and_eval('\\'seastar\\'::local_engine->_lowres_timers')\n        for timer_list in std_array(timer_set['_buckets']):\n            for t in intrusive_list(timer_list, link='_link'):\n                gdb.write('(%s*) %s = %s\\n' % (t.type, t.address, t))\n\n\ndef has_reactor():\n    if gdb.parse_and_eval('\\'seastar\\'::local_engine'):\n        return True\n    return False\n\n\ndef reactor_threads():\n    orig = gdb.selected_thread()\n    for t in gdb.selected_inferior().threads():\n        t.switch()\n        if has_reactor():\n            yield t\n    orig.switch()\n\n\ndef reactors():\n    orig = gdb.selected_thread()\n    for t in gdb.selected_inferior().threads():\n        t.switch()\n        reactor = gdb.parse_and_eval('\\'seastar\\'::local_engine')\n        if reactor:\n            yield reactor.dereference()\n    orig.switch()\n\n\ndef switch_to_shard(shard):\n    for r in reactors():\n        if int(r['_id']) == shard:\n            return\n\n\nclass scylla_apply(gdb.Command):\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla apply', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def invoke(self, arg, from_tty):\n        orig = gdb.selected_thread()\n        try:\n            for r in reactors():\n                gdb.write(\"\\nShard %d: \\n\\n\" % (r['_id']))\n                gdb.execute(arg)\n        finally:\n            orig.switch()\n\n\nclass scylla_shard(gdb.Command):\n    \"\"\"Retrieves information on the current shard and allows switching shards.\n\n    Run without any parameters to print the current thread's shard number.\n    Run with a numeric parameter <N> to switch to the thread which runs shard <N>.\n\n    Example:\n      scylla shard 0\n    \"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla shard', gdb.COMMAND_USER, gdb.COMPLETE_NONE)\n\n    def invoke(self, arg, from_tty):\n        if arg is None or arg == '':\n            gdb.write('Current shard is %d\\n' % current_shard())\n            return\n        id = 0\n        try:\n            id = int(arg)\n        except:\n            gdb.write('Error: %s is not a valid shard number\\n' % arg)\n            return\n        orig = gdb.selected_thread()\n        for t in gdb.selected_inferior().threads():\n            t.switch()\n            reactor = gdb.parse_and_eval('\\'seastar\\'::local_engine')\n            if reactor and reactor['_id'] == id:\n                gdb.write('Switched to thread %d\\n' % t.num)\n                return\n        orig.switch()\n        gdb.write('Error: Shard %d not found\\n' % (id))\n\n\nclass scylla_mem_ranges(gdb.Command):\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla mem-ranges', gdb.COMMAND_USER, gdb.COMPLETE_NONE)\n\n    def invoke(self, arg, from_tty):\n        for t, start, total_mem in seastar_memory_layout():\n            gdb.write('0x%x +%d\\n' % (start, total_mem))\n\n\nclass scylla_mem_range(gdb.Command):\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla mem-range', gdb.COMMAND_USER, gdb.COMPLETE_NONE)\n\n    def invoke(self, arg, from_tty):\n        if not has_reactor():\n            gdb.write('Not a reactor thread')\n            return\n        gdb.write('0x%x +%d\\n' % get_seastar_memory_start_and_size())\n\n\nclass thread_switched_in(object):\n    def __init__(self, gdb_thread):\n        self.new = gdb_thread\n\n    def __enter__(self):\n        self.old = gdb.selected_thread()\n        self.new.switch()\n\n    def __exit__(self, *_):\n        self.old.switch()\n\n\nclass seastar_thread_context(object):\n    ulong_type = gdb.lookup_type('unsigned long')\n\n    # FIXME: The jmpbuf interpreting code targets x86_64 and glibc 2.19\n    # Offsets taken from sysdeps/x86_64/jmpbuf-offsets.h.\n    jmpbuf_offsets = {\n        'rbx': 0,\n        'rbp': 1,\n        'r12': 2,\n        'r13': 3,\n        'r14': 4,\n        'r15': 5,\n        'rsp': 6,\n        'rip': 7,\n    }\n    mangled_registers = ['rip', 'rsp', 'rbp']\n\n    def save_regs(self):\n        result = {}\n        for reg in self.jmpbuf_offsets.keys():\n            result[reg] = gdb.parse_and_eval('$%s' % reg).cast(self.ulong_type)\n        return result\n\n    def restore_regs(self, values):\n        gdb.newest_frame().select()\n        for reg, value in values.items():\n            gdb.execute('set $%s = %s' % (reg, value))\n\n    def get_fs_base(self):\n        return gdb.parse_and_eval('$fs_base')\n\n    def regs(self):\n        return self.regs_from_jmpbuf(self.thread_ctx['_context']['jmpbuf'])\n\n    def regs_from_jmpbuf(self, jmpbuf):\n        canary = gdb.Value(self.get_fs_base()).reinterpret_cast(self.ulong_type.pointer())[6]\n        result = {}\n        for reg, offset in self.jmpbuf_offsets.items():\n            value = jmpbuf['__jmpbuf'][offset].cast(self.ulong_type)\n            if reg in self.mangled_registers:\n                # glibc mangles by doing:\n                #   xor %reg, %fs:0x30\n                #   rol %reg, $0x11\n                bits = 64\n                shift = 0x11\n                value = (value << (bits - shift)) & (2**bits - 1) | (value >> shift)\n                value = value ^ canary\n            result[reg] = value\n        return result\n\n    def is_switched_in(self):\n        jmpbuf_link_ptr = gdb.parse_and_eval('seastar::g_current_context')\n        if jmpbuf_link_ptr['thread'] == self.thread_ctx.address:\n            return True\n        return False\n\n    def __init__(self, thread_ctx):\n        self.thread_ctx = thread_ctx\n        self.old_frame = gdb.selected_frame()\n        self.old_regs = self.save_regs()\n        self.old_gdb_thread = gdb.selected_thread()\n        self.gdb_thread = get_thread_owning_memory(thread_ctx.address)\n        self.new_regs = None\n\n    def __enter__(self):\n        gdb.write('Switched to thread %d, (seastar::thread_context*) 0x%x\\n' % (self.gdb_thread.num, int(self.thread_ctx.address)))\n        self.gdb_thread.switch()\n        if not self.is_switched_in():\n            self.new_regs = self.regs_from_jmpbuf(self.thread_ctx['_context']['jmpbuf'])\n            self.restore_regs(self.new_regs)\n\n    def __exit__(self, *_):\n        if self.new_regs:\n            self.gdb_thread.switch()\n            self.restore_regs(self.old_regs)\n        self.old_gdb_thread.switch()\n        self.old_frame.select()\n        gdb.write('Switched to thread %d\\n' % self.old_gdb_thread.num)\n\n\nactive_thread_context = None\n\n\ndef exit_thread_context():\n    global active_thread_context\n    if active_thread_context:\n        active_thread_context.__exit__()\n        active_thread_context = None\n\n\ndef seastar_threads_on_current_shard():\n    return intrusive_list(gdb.parse_and_eval('seastar::thread_context::_all_threads'), link='_all_link')\n\n\nclass scylla_thread(gdb.Command):\n    \"\"\"Operations with seastar::threads.\n\n    The following operations are supported:\n    * Switch to seastar thread (see also `scylla unthread`);\n    * Execute command in the context of all existing seastar::threads.\n    * Print saved registers for given thread\n\n    Run `scylla thread --help` for more information on usage.\n\n    DISCLAIMER: This is a dangerous command with the potential to crash the\n    process if anything goes wrong!\n    \"\"\"\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla thread', gdb.COMMAND_USER,\n                             gdb.COMPLETE_COMMAND, True)\n\n    def invoke_apply_all(self, args):\n        for r in reactors():\n            for t in seastar_threads_on_current_shard():\n                gdb.write('\\n[shard %2d] (seastar::thread_context*) 0x%x:\\n\\n' % (r['_id'], int(t.address)))\n                with seastar_thread_context(t):\n                    gdb.execute(' '.join(args))\n\n    def invoke(self, arg, for_tty):\n        parser = argparse.ArgumentParser(description=\"scylla thread\")\n        parser.add_argument(\"--iamsure\", action=\"store_true\", default=False,\n                help=\"I know what I'm doing and I want to run this command knowing that it might *crash* this process.\")\n        parser.add_argument(\"-s\", \"--switch\", action=\"store_true\", default=False,\n                help=\"Switch to the given thread.\"\n                \" The corresponding `seastar::thread_context*` is expected to be provided as the positional argument.\"\n                \" Any valid gdb expression that evaluates to such a pointer is accepted.\"\n                \" To leave the thread later, use the `scylla unthread` command.\")\n        parser.add_argument(\"-a\", \"--apply-all\", action=\"store_true\", default=False,\n                help=\"Execute the command in the context of each seastar::thread.\"\n                \" The command (and its arguments) to execute is expected to be provided as the positional argument.\")\n        parser.add_argument(\"-p\", \"--print-regs\", action=\"store_true\", default=False,\n                help=\"Print unmangled registers saved in the jump buffer\")\n        parser.add_argument(\"arg\", nargs='+')\n\n        try:\n            args = parser.parse_args(arg.split())\n        except SystemExit:\n            return\n\n        if not args.print_regs and not args.iamsure:\n            gdb.write(\"DISCLAIMER: This is a dangerous command with the potential to crash the process if anything goes wrong!\"\n                    \" Please pass the `--iamsure` flag to acknowledge being fine with this risk.\\n\")\n            return\n\n        if sum([arg for arg in [args.apply_all, args.switch, args.print_regs]]) > 1:\n            gdb.write(\"Only one of `--apply-all`, `--switch` or `--print-regs` can be used.\\n\")\n            return\n\n        if not args.apply_all and not args.switch and not args.print_regs:\n            gdb.write(\"No command specified, need one of `--apply-all`, `--switch` or `--print_regs`.\\n\")\n            return\n\n        if args.apply_all:\n            self.invoke_apply_all(args.arg)\n            return\n\n        if len(args.arg) > 1:\n            gdb.write(\"Expected only one argument for the `--switch` command.\\n\")\n            return\n\n        addr = gdb.parse_and_eval(args.arg[0])\n        ctx = addr.reinterpret_cast(gdb.lookup_type('seastar::thread_context').pointer()).dereference()\n        if args.print_regs:\n            for reg, val in seastar_thread_context(ctx).regs().items():\n                gdb.write(\"%s: 0x%x\\n\" % (reg, val))\n            return\n        exit_thread_context()\n        global active_thread_context\n        active_thread_context = seastar_thread_context(ctx)\n        active_thread_context.__enter__()\n\n\nclass scylla_unthread(gdb.Command):\n    \"\"\"Leave the current seastar::thread context.\n\n    Does nothing when there is no active seastar::thread context.\n    \"\"\"\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla unthread', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n\n    def invoke(self, arg, for_tty):\n        exit_thread_context()\n\n\nclass scylla_threads(gdb.Command):\n    \"\"\"Find and list seastar::threads\n\n    Without any arguments, lists threads on the current shard. You can also list\n    threads on a given shard with `-s $shard` or on all shards with `-a` (old\n    behaviour).\n    Pass `-v` to also add the functor vtable symbol to the listing, this might\n    or might not tell you what the main function of the thread is.\n    Invoke with `--help` to see all available options.\n    \"\"\"\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla threads', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n\n    def _do_list_threads(self, shard, include_vtable):\n        for t in seastar_threads_on_current_shard():\n            stack = std_unique_ptr(t['_stack']).get()\n            if include_vtable:\n                maybe_vtable = ' vtable: {}'.format(t['_func']['_vtable'])\n            else:\n                maybe_vtable = ''\n            gdb.write('[shard {}] (seastar::thread_context*) 0x{:x}, stack: 0x{:x}{}\\n'.format(\n                shard, int(t.address), int(stack), maybe_vtable))\n\n    def invoke(self, arg, for_tty):\n        parser = argparse.ArgumentParser(description=\"scylla threads\")\n        parser.add_argument(\"-a\", \"--all\", action=\"store_true\",\n                help=\"list threads on all shards\")\n        parser.add_argument(\"-s\", \"--shard\", action=\"store\", type=int, default=-1,\n                help=\"list threads on the specified shard\")\n        parser.add_argument(\"-v\", \"--vtable\", action=\"store_true\",\n                help=\"include functor vtable in the printout; could be useful for identifying the thread, but makes the output very verbose\")\n\n        try:\n            args = parser.parse_args(arg.split())\n        except SystemExit:\n            return\n\n        if args.all:\n            shards = list(range(cpus()))\n        elif args.shard != -1:\n            shards = [args.shard]\n        else:\n            shards = [int(gdb.parse_and_eval('seastar::local_engine._id'))]\n\n        for r in reactors():\n            shard = int(r['_id'])\n            if shard in shards:\n                self._do_list_threads(shard, args.vtable)\n\n\nclass circular_buffer(object):\n    def __init__(self, ref):\n        self.ref = ref\n\n    def _mask(self, i):\n        return i & (int(self.ref['_impl']['capacity']) - 1)\n\n    def __iter__(self):\n        impl = self.ref['_impl']\n        st = impl['storage']\n        cap = impl['capacity']\n        i = impl['begin']\n        end = impl['end']\n        while i < end:\n            yield st[self._mask(i)]\n            i += 1\n\n    def size(self):\n        impl = self.ref['_impl']\n        return int(impl['end']) - int(impl['begin'])\n\n    def __len__(self):\n        return self.size()\n\n    def __getitem__(self, item):\n        impl = self.ref['_impl']\n        return (impl['storage'] + self._mask(int(impl['begin']) + item)).dereference()\n\n    def external_memory_footprint(self):\n        impl = self.ref['_impl']\n        return int(impl['capacity']) * self.ref.type.template_argument(0).sizeof\n\n\nclass small_vector(object):\n    def __init__(self, ref):\n        self.ref = ref\n\n    def __len__(self):\n        return self.ref['_begin'] - self.ref['_end']\n\n    def __iter__(self):\n        e = self.ref['_begin']\n        while e != self.ref['_end']:\n            yield e.dereference()\n            e += 1\n\n    def external_memory_footprint(self):\n        if self.ref['_begin'] == self.ref['_internal']['storage'].address:\n            return 0\n        return int(self.ref['_capacity_end']) - int(self.ref['_begin'])\n\n\nclass boost_small_vector(object):\n    def __init__(self, ref):\n        self.ref = ref\n\n    def __iter__(self):\n        e = self.ref['m_holder']['m_start']\n        for i in range(0, self.ref['m_holder']['m_size']):\n            yield e.dereference()\n            e += 1\n\n    def __len__(self):\n        return self.ref['m_holder']['m_size']\n\n\nclass chunked_vector(object):\n    def __init__(self, ref):\n        self.ref = ref\n        self._max_contiguous_allocation = int(list(template_arguments(self.ref.type))[1])\n\n    def max_chunk_capacity(self):\n        return max(self._max_contiguous_allocation / self.ref.type.sizeof, 1);\n\n    def __len__(self):\n        return int(self.ref['_size'])\n\n    def __iter__(self):\n        sz = len(self)\n        for chunk in small_vector(self.ref['_chunks']):\n            chunk = std_unique_ptr(chunk).get()\n            for i in range(min(sz, self.max_chunk_capacity())):\n                yield chunk[i]\n                sz -= 1\n\n    def external_memory_footprint(self):\n        return int(self.ref['_capacity']) * self.ref.type.template_argument(0).sizeof \\\n               + small_vector(self.ref['_chunks']).external_memory_footprint()\n\n\ndef get_local_task_queues():\n    \"\"\" Return a list of task pointers for the local reactor. \"\"\"\n    for tq_ptr in static_vector(gdb.parse_and_eval('\\'seastar\\'::local_engine._task_queues')):\n        yield std_unique_ptr(tq_ptr).dereference()\n\ndef get_local_io_queues():\n    \"\"\" Return a list of io queues for the local reactor. \"\"\"\n    for dev, ioq in unordered_map(gdb.parse_and_eval('\\'seastar\\'::local_engine._io_queues')):\n        yield dev, std_unique_ptr(ioq).dereference()\n\n\ndef get_local_tasks(tq_id = None):\n    \"\"\" Return a list of task pointers for the local reactor. \"\"\"\n    if tq_id is not None:\n        tqs = filter(lambda x: x['_id'] == tq_id, get_local_task_queues())\n    else:\n        tqs = get_local_task_queues()\n\n    for tq in tqs:\n        for t in circular_buffer(tq['_q']):\n            yield t\n\n\nclass scylla_task_stats(gdb.Command):\n    \"\"\" Prints histogram of task types in reactor's pending task queue.\n\n    Example:\n    (gdb) scylla task-stats\n       16243: 0x18904f0 vtable for lambda_task<yield()::{lambda()#1}> + 16\n       16091: 0x197fc60 _ZTV12continuationIZN6futureIJEE12then_wrappedIZNS1_16handle_exception...\n       16090: 0x19bab50 _ZTV12continuationIZN6futureIJEE12then_wrappedINS1_12finally_bodyIZN7s...\n       14280: 0x1b36940 _ZTV12continuationIZN6futureIJEE12then_wrappedIZN17smp_message_queue15...\n\n       ^      ^         ^\n       |      |         '-- symbol name for vtable pointer\n       |      '------------ vtable pointer for the object pointed to by task*\n       '------------------- task count\n    \"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla task-stats', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n\n    def invoke(self, arg, for_tty):\n        vptr_count = defaultdict(int)\n        vptr_type = gdb.lookup_type('uintptr_t').pointer()\n        for ptr in get_local_tasks():\n            vptr = int(ptr.reinterpret_cast(vptr_type).dereference())\n            vptr_count[vptr] += 1\n        for vptr, count in sorted(vptr_count.items(), key=lambda e: -e[1]):\n            gdb.write('%10d: 0x%x %s\\n' % (count, vptr, resolve(vptr)))\n\n\nclass scylla_tasks(gdb.Command):\n    \"\"\" Prints contents of reactor pending tasks queue.\n\n    Example:\n    (gdb) scylla tasks\n    (task*) 0x60017d8c7f88  _ZTV12continuationIZN6futureIJEE12then_wrappedIZN17smp_message_queu...\n    (task*) 0x60019a391730  _ZTV12continuationIZN6futureIJEE12then_wrappedIZNS1_16handle_except...\n    (task*) 0x60018fac2208  vtable for lambda_task<yield()::{lambda()#1}> + 16\n    (task*) 0x60016e8b7428  _ZTV12continuationIZN6futureIJEE12then_wrappedINS1_12finally_bodyIZ...\n    (task*) 0x60017e5bece8  _ZTV12continuationIZN6futureIJEE12then_wrappedINS1_12finally_bodyIZ...\n    (task*) 0x60017e7f8aa0  _ZTV12continuationIZN6futureIJEE12then_wrappedIZNS1_16handle_except...\n    (task*) 0x60018fac21e0  vtable for lambda_task<yield()::{lambda()#1}> + 16\n    (task*) 0x60016e8b7540  _ZTV12continuationIZN6futureIJEE12then_wrappedINS1_12finally_bodyIZ...\n    (task*) 0x600174c34d58  _ZTV12continuationIZN6futureIJEE12then_wrappedINS1_12finally_bodyIZ...\n\n            ^               ^\n            |               |\n            |               '------------ symbol name for task's vtable pointer\n            '---------------------------- task pointer\n    \"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla tasks', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n\n    def invoke(self, arg, for_tty):\n        vptr_type = gdb.lookup_type('uintptr_t').pointer()\n        for ptr in get_local_tasks():\n            vptr = int(ptr.reinterpret_cast(vptr_type).dereference())\n            gdb.write('(task*) 0x%x  %s\\n' % (ptr, resolve(vptr)))\n\n\nclass scylla_task_queues(gdb.Command):\n    \"\"\" Print a summary of the reactor's task queues.\n\n    Example:\n       id name                             shares  tasks\n     A 00 \"main\"                           1000.00 4\n       01 \"atexit\"                         1000.00 0\n       02 \"streaming\"                       200.00 0\n     A 03 \"compaction\"                      171.51 1\n       04 \"mem_compaction\"                 1000.00 0\n    *A 05 \"statement\"                      1000.00 2\n       06 \"memtable\"                          8.02 0\n       07 \"memtable_to_cache\"               200.00 0\n\n    Where:\n        * id: seastar::reactor::task_queue::_id\n        * name: seastar::reactor::task_queue::_name\n        * shares: seastar::reactor::task_queue::_shares\n        * tasks: seastar::reactor::task_queue::_q.size()\n        * A: seastar::reactor::task_queue::_active == true\n        * *: seastar::reactor::task_queue::_current == true\n    \"\"\"\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla task-queues', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n\n    @staticmethod\n    def _active(a):\n        if a:\n            return 'A'\n        return ' '\n\n    @staticmethod\n    def _current(c):\n        if c:\n            return '*'\n        return ' '\n\n    def invoke(self, arg, for_tty):\n        current_sg = gdb.parse_and_eval('(seastar::scheduling_group) \\'seastar::internal::current_scheduling_group_ptr()::sg\\'')\n        gdb.write('   {:2} {:32} {:7} {}\\n'.format(\"id\", \"name\", \"shares\", \"tasks\"))\n        for tq in get_local_task_queues():\n            gdb.write('{}{} {:02} {:32} {:>7.2f} {}\\n'.format(\n                    self._current(current_sg['_id'] == tq['_id']),\n                    self._active(bool(tq['_active'])),\n                    int(tq['_id']),\n                    str(tq['_name']),\n                    float(tq['_shares']),\n                    len(circular_buffer(tq['_q']))))\n\n\nclass scylla_io_queues(gdb.Command):\n    \"\"\" Print a summary of the reactor's IO queues.\n\n    Example:\n    Dev 0:\n        Class:                  |shares:         |ptr:            \n        --------------------------------------------------------\n        \"default\"               |1               |0x6000002c6500  \n        \"commitlog\"             |1000            |0x6000003ad940  \n        \"memtable_flush\"        |1000            |0x6000005cb300  \n        \"streaming\"             |200             |0x0             \n        \"query\"                 |1000            |0x600000718580  \n        \"compaction\"            |1000            |0x6000030ef0c0  \n\n        Max request size:    2147483647\n        Max capacity:        Ticket(weight: 4194303, size: 4194303)\n        Capacity tail:       Ticket(weight: 73168384, size: 100561888)\n        Capacity head:       Ticket(weight: 77360511, size: 104242143)\n\n        Resources executing: Ticket(weight: 2176, size: 514048)\n        Resources queued:    Ticket(weight: 384, size: 98304)\n        Handles: (1)\n            Class 0x6000005d7278:\n                Ticket(weight: 128, size: 32768)\n                Ticket(weight: 128, size: 32768)\n                Ticket(weight: 128, size: 32768)\n        Pending in sink: (0)\n\n\n    \"\"\"\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla io-queues', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n\n    class ticket:\n        def __init__(self, ref):\n            self.ref = ref\n\n        def __str__(self):\n            return f\"Ticket(weight: {self.ref['_weight']}, size: {self.ref['_size']})\"\n\n    @staticmethod\n    def _print_io_priority_class(pclass, names_from_ptrs, indent = '\\t\\t'):\n        gdb.write(\"{}Class {}:\\n\".format(indent, names_from_ptrs.get(pclass.address, pclass.address)))\n        slist = intrusive_slist(pclass['_queue'], link='_hook')\n        for entry in slist:\n            gdb.write(\"{}\\t{}\\n\".format(indent, scylla_io_queues.ticket(entry['_ticket'])))\n\n    def _get_classes_infos(self, ioq):\n        # Starting from 5.3 priority classes are removed and IO inherits its name and\n        # shares from the respective sched group, so should the infos.  Not to rely on\n        # gdb.parse_and_eval() to fail searching for _infos, use the secret knowledge:\n        # commitlog gained its sched group at the same time, so if not present, we're\n        # on some older version and should fallback to io-prio _infos (or worse)\n        infos = {}\n        commitlog_met = False\n        for tq in get_local_task_queues():\n            name = str(tq['_name'])\n            if name == '\"commitlog\"':\n                commitlog_met = True\n            infos[int(tq['_id'])] = { 'name': name, 'shares': int(tq['_shares']) }\n        if commitlog_met:\n            return infos\n\n        try:\n            return std_array(gdb.parse_and_eval('seastar::io_priority_class::_infos'))\n        except gdb.error:\n            # Compatibility: io_queue::_registered_... stuff moved onto io_priority_class in version 4.6\n            return [ { 'name': x[0], 'shares': x[1] } for x in zip(std_array(ioq['_registered_names']), std_array(ioq['_registered_shares'])) ]\n\n    def invoke(self, arg, for_tty):\n        for dev, ioq in get_local_io_queues():\n            gdb.write(\"Dev {}:\\n\".format(dev))\n\n            infos = self._get_classes_infos(ioq)\n            pclasses = std_vector(ioq['_priority_classes'])\n\n            names_from_ptrs = {}\n\n            gdb.write(\"\\t{:24}|{:16}|{:46}\\n\".format(\"Class:\", \"shares:\", \"ptr:\"))\n            gdb.write(\"\\t\" + '-'*64 + \"\\n\")\n            for i, pclass in enumerate(pclasses):\n                pclass_ptr = std_unique_ptr(pclass).get()\n                names_from_ptrs[pclass_ptr] = infos[i]['name']\n                gdb.write(\"\\t{:24}|{:16}|({:30}){:16}\\n\".format(str(infos[i]['name']), str(infos[i]['shares']), str(pclass_ptr.type), str(pclass_ptr)))\n            gdb.write(\"\\n\")\n\n            group = std_shared_ptr(ioq['_group']).get().dereference()\n            while True:\n                # in order to avoid nested try-catch block, use a single while block\n                try:\n                    f_groups = static_vector(group['_fgs'])\n                    _ = len(f_groups)\n                    f_queues = static_vector(ioq['_streams'])\n                    _ = len(f_queues)\n                    fq_pclass = lambda x: x.dereference()\n                    break\n                except gdb.error:\n                    # try harder\n                    pass\n                try:\n                    f_groups = [std_unique_ptr(x) for x in std_vector(group['_fgs'])]\n                    f_queues = boost_small_vector(ioq['_streams'])\n                    fq_pclass = lambda x: x.dereference()\n                    break\n                except gdb.error:\n                    # try harder\n                    pass\n                try:\n                    f_groups = [group['_fg']]\n                    f_queues = [ioq['_fq']]\n                    fq_pclass = lambda x: seastar_lw_shared_ptr(x).get().dereference()\n                    break\n                except gdb.error:\n                    # give up\n                    raise\n\n            gdb.write(\"\\t{} streams\\n\".format(len(f_groups)))\n            gdb.write(\"\\n\")\n\n            for fg, fq in zip(f_groups, f_queues):\n                try:\n                    try:\n                        gdb.write(\"\\tCapacity tail:       {}\\n\".format(std_atomic(fg['_token_bucket']['_rovers']['tail']).get()))\n                        gdb.write(\"\\tCapacity head:       {}\\n\".format(std_atomic(fg['_token_bucket']['_rovers']['head']).get()))\n                        try:\n                            gdb.write(\"\\tCapacity ceil:       {}\\n\".format(std_atomic(fg['_token_bucket']['_rovers']['ceil']).get()))\n                        except gdb.error:\n                            pass\n                    except gdb.error:\n                        gdb.write(\"\\tCapacity tail:       {}\\n\".format(std_atomic(fg['_capacity_tail']).get()))\n                        gdb.write(\"\\tCapacity head:       {}\\n\".format(std_atomic(fg['_capacity_head']).get()))\n                        gdb.write(\"\\tCapacity ceil:       {}\\n\".format(std_atomic(fg['_capacity_ceil']).get()))\n                    gdb.write(\"\\n\")\n                except gdb.error:\n                    gdb.write(\"\\tMax capacity:        {}\\n\".format(self.ticket(fg['_maximum_capacity'])))\n                    gdb.write(\"\\tCapacity tail:       {}\\n\".format(self.ticket(std_atomic(fg['_capacity_tail']).get())))\n                    gdb.write(\"\\tCapacity head:       {}\\n\".format(self.ticket(std_atomic(fg['_capacity_head']).get())))\n                    gdb.write(\"\\n\")\n\n                gdb.write(\"\\tResources executing: {}\\n\".format(self.ticket(fq['_resources_executing'])))\n                gdb.write(\"\\tResources queued:    {}\\n\".format(self.ticket(fq['_resources_queued'])))\n                handles = std_priority_queue(fq['_handles'])\n                gdb.write(\"\\tHandles: ({})\\n\".format(len(handles)))\n                for pclass_ptr in handles:\n                    self._print_io_priority_class(fq_pclass(pclass_ptr), names_from_ptrs)\n\n            try:\n                pending = chunked_fifo(ioq['_sink']['_pending_io'])\n            except gdb.error:\n                pending = circular_buffer(ioq['_sink']['_pending_io'])\n            gdb.write(\"\\tPending in sink: ({})\\n\".format(len(pending)))\n            for op in pending:\n                gdb.write(\"Completion {}\\n\".format(op['_completion']))\n\n\nclass scylla_fiber(gdb.Command):\n    \"\"\" Walk the continuation chain starting from the given task\n\n    Example (cropped for brevity):\n    (gdb) scylla fiber 0x600016217c80\n    [shard  0] #-1 (task*) 0x000060001a305910 0x0000000004aa5260 vtable for seastar::continuation<...> + 16\n    [shard  0] #0  (task*) 0x0000600016217c80 0x0000000004aa5288 vtable for seastar::continuation<...> + 16\n    [shard  0] #1  (task*) 0x000060000ac42940 0x0000000004aa2aa0 vtable for seastar::continuation<...> + 16\n    [shard  0] #2  (task*) 0x0000600023f59a50 0x0000000004ac1b30 vtable for seastar::continuation<...> + 16\n     ^          ^          ^                  ^                  ^\n    (1)        (2)        (3)                (4)                (5)\n\n    1) Shard the task lives on (continuation chains can spawn multiple shards)\n    2) Task index:\n        - 0 is the task passed to the command\n        - < 0 are tasks this task waits on\n        - > 0 are tasks waiting on this task\n    3) Pointer to the task object.\n    4) Pointer to the task's vtable.\n    5) Symbol name of the task's vtable.\n\n    Invoke `scylla fiber --help` for more information on usage.\n    \"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla fiber', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n        self._task_symbol_matcher = task_symbol_matcher()\n        self._thread_map = None\n\n    def _name_is_on_whitelist(self, name):\n        return self._task_symbol_matcher(name)\n\n    def _maybe_log(self, msg, verbose):\n        if verbose:\n            gdb.write(msg)\n\n    def _probe_pointer(self, ptr, scanned_region_size, using_seastar_allocator, verbose):\n        \"\"\" Check if the pointer is a task pointer\n\n        The pattern we are looking for is:\n        ptr -> vtable ptr for a symbol that matches our whitelist\n\n        In addition, ptr has to point to an allocation block, managed by\n        seastar, that contains a live object.\n        \"\"\"\n        # Save work if caller already analyzed the pointer\n        if isinstance(ptr, pointer_metadata):\n            ptr_meta = ptr\n            ptr = ptr.ptr\n        else:\n            ptr_meta = None\n\n        try:\n            maybe_vptr = int(gdb.Value(ptr).reinterpret_cast(_vptr_type()).dereference())\n            self._maybe_log(\"\\t-> 0x{:016x}\\n\".format(maybe_vptr), verbose)\n        except gdb.MemoryError:\n            self._maybe_log(\"\\tNot a pointer\\n\", verbose)\n            return\n\n        resolved_symbol = resolve(maybe_vptr, False)\n        if resolved_symbol is None:\n            self._maybe_log(\"\\t\\tNot a vtable ptr\\n\", verbose)\n            return\n\n        self._maybe_log(\"\\t\\t=> {}\\n\".format(resolved_symbol), verbose)\n\n        if not self._name_is_on_whitelist(resolved_symbol):\n            self._maybe_log(\"\\t\\t\\tSymbol name doesn't match whitelisted symbols\\n\", verbose)\n            return\n\n        # The promise object starts on the third `uintptr_t` in the frame.\n        # The resume_fn pointer is the first `uintptr_t`.\n        # So if the task is a coroutine, we should be able to find the resume function via offsetting by -2.\n        # AFAIK both major compilers respect this convention.\n        if resolved_symbol.startswith('vtable for seastar::internal::coroutine_traits_base'):\n            if block := gdb.block_for_pc((gdb.Value(ptr).cast(_vptr_type()) - 2).dereference()):\n                resume = block.function\n                resolved_symbol += f\" ({resume.print_name} at {resume.symtab.filename}:{resume.line})\"\n            else:\n                resolved_symbol += f\" (unknown coroutine)\"\n\n        if using_seastar_allocator:\n            if ptr_meta is None:\n                ptr_meta = scylla_ptr.analyze(ptr)\n            if not ptr_meta.is_managed_by_seastar() or not ptr_meta.is_live:\n                self._maybe_log(\"\\t\\t\\tNot a live object\\n\", verbose)\n                return\n        else:\n            ptr_meta = pointer_metadata(ptr, scanned_region_size)\n\n        self._maybe_log(\"\\t\\t\\tTask found\\n\", verbose)\n\n        return ptr_meta, maybe_vptr, resolved_symbol\n\n    # Find futures waiting on this task\n    def _walk_forward(self, ptr_meta, name, i, max_depth, scanned_region_size, using_seastar_allocator, verbose):\n        ptr = ptr_meta.ptr\n\n        # thread_context has a self reference in its `_func` field which will be\n        # found before the ref to the next task if the whole object is scanned.\n        # Exploit that thread_context is a type we can cast to and scan the\n        # `_done` field explicitly to avoid that.\n        if 'thread_context' in name:\n            self._maybe_log(\"Current task is a thread, using its _done field to continue\\n\", verbose)\n            thread_context_ptr_type = gdb.lookup_type('seastar::thread_context').pointer()\n            ctxt = gdb.Value(int(ptr_meta.ptr)).reinterpret_cast(thread_context_ptr_type).dereference()\n            pr = ctxt['_done']\n            region_start = pr.address\n            region_end = region_start + pr.type.sizeof\n        else:\n            region_start = ptr + _vptr_type().sizeof # ignore our own vtable\n            region_end = region_start + (ptr_meta.size - ptr_meta.size % _vptr_type().sizeof)\n\n        self._maybe_log(\"Scanning task #{} @ 0x{:016x}: {}\\n\".format(i, ptr, str(ptr_meta)), verbose)\n\n        for it in range(region_start, region_end, _vptr_type().sizeof):\n            maybe_tptr = int(gdb.Value(it).reinterpret_cast(_vptr_type()).dereference())\n            self._maybe_log(\"0x{:016x}+0x{:04x} -> 0x{:016x}\\n\".format(ptr, it - ptr, maybe_tptr), verbose)\n\n            res = self._probe_pointer(maybe_tptr, scanned_region_size, using_seastar_allocator, verbose)\n\n            if res is None:\n                continue\n\n            if int(res[0].ptr) == int(ptr):\n                self._maybe_log(\"Rejecting self reference\\n\", verbose)\n                continue\n\n            return res\n\n        return None\n\n    # Find futures waited-on by this task\n    def _walk_backward(self, ptr_meta, name, i, max_depth, scanned_region_size, using_seastar_allocator, verbose):\n        orig = gdb.selected_thread()\n        res = None\n\n        # Threads need special handling as they allocate the thread_wait_task object on their stack.\n        if 'thread_context' in name:\n            context = gdb.parse_and_eval('(seastar::thread_context*)0x{:x}'.format(ptr_meta.ptr))\n            stack_ptr = int(std_unique_ptr(context['_stack']).get())\n            self._maybe_log(\"Current task is a thread, trying to find the thread_wake_task on its stack: 0x{:x}\\n\".format(stack_ptr), verbose)\n            stack_meta = scylla_ptr.analyze(stack_ptr)\n            # stack grows downwards, so walk from end of buffer towards the beginning\n            for maybe_tptr in range(stack_ptr + stack_meta.size - _vptr_type().sizeof, stack_ptr - _vptr_type().sizeof, -_vptr_type().sizeof):\n                res = self._probe_pointer(maybe_tptr, scanned_region_size, using_seastar_allocator, verbose)\n                if res is not None and 'thread_wake_task' in res[2]:\n                    return res\n            return None\n\n        if name.startswith('vtable for seastar::internal::when_all_state'):\n            when_all_state_base_ptr_type = gdb.lookup_type('seastar::internal::when_all_state_base').pointer()\n            when_all_state_base = gdb.Value(int(ptr_meta.ptr)).reinterpret_cast(when_all_state_base_ptr_type)\n            ptr = int(when_all_state_base['_continuation'])\n            self._maybe_log(\"Current task is a when_all_state, looking for references to its continuation field 0x{:x}\\n\".format(ptr), verbose)\n            return self._probe_pointer(ptr, scanned_region_size, using_seastar_allocator, verbose)\n\n        # Async work items will have references on the remote shard, so we first\n        # have to find out which is the remote shard and then switch to it.\n        # Have to match the start of the name, as async_work_item kicks off\n        # continuation and so it will be found in the name of those lambdas.\n        if name.startswith('vtable for seastar::smp_message_queue::async_work_item'):\n            self._maybe_log(\"Current task is a async work item, trying to deduce the remote shard\\n\", verbose)\n            smp_mmessage_queue_ptr_type = gdb.lookup_type('seastar::smp_message_queue').pointer()\n            work_item_type = gdb.lookup_type('seastar::smp_message_queue::work_item')\n            # Casts to templates with lambda template arguments just don't work.\n            # We know the offset of the message queue reference in\n            # async_work_item (first field) so we calculate and cast a pointer to it.\n            q_ptr = align_up(int(ptr_meta.ptr) + work_item_type.sizeof, _vptr_type().sizeof)\n            q = gdb.Value(q_ptr).reinterpret_cast(smp_mmessage_queue_ptr_type.pointer()).dereference()\n            shard = int(q['_pending']['remote']['_id'])\n            self._maybe_log(\"Deduced shard is {} (message queue 0x{:x} @ 0x{:x} + {})\\n\".format(shard, int(q), int(ptr_meta.ptr), q_ptr - int(ptr_meta.ptr)), verbose)\n            # Sanity check.\n            if shard < 0 or shard >= cpus():\n                return None\n            switch_to_shard(shard)\n        else:\n            ptr_meta.thread.switch()\n\n        try:\n            for maybe_tptr_meta, _ in scylla_find.find(ptr_meta.ptr):\n                maybe_tptr_meta.ptr -= maybe_tptr_meta.offset_in_object\n                res = self._probe_pointer(maybe_tptr_meta.ptr, scanned_region_size, using_seastar_allocator, verbose)\n                if res is None:\n                    continue\n\n                if int(res[0].ptr) == int(ptr_meta.ptr):\n                    self._maybe_log(\"Rejecting self reference\\n\", verbose)\n                    continue\n\n                return res\n        finally:\n            orig.switch()\n\n        return res\n\n    def _walk(self, walk_method, tptr_meta, name, max_depth, scanned_region_size, using_seastar_allocator, verbose):\n        i = 0\n        fiber = []\n        known_tasks = set([tptr_meta.ptr])\n        while True:\n            if max_depth > -1 and i >= max_depth:\n                break\n\n            self._maybe_log(\"_walk() 0x{:x} {}\\n\".format(int(tptr_meta.ptr), name), verbose)\n            res = walk_method(tptr_meta, name, i + 1, max_depth, scanned_region_size, using_seastar_allocator, verbose)\n            if res is None:\n                break\n\n            tptr_meta, vptr, name = res\n\n            if tptr_meta.ptr in known_tasks:\n                gdb.write(\"Stopping because loop is detected: task 0x{:016x} was seen before.\\n\".format(tptr_meta.ptr))\n                break\n\n            known_tasks.add(tptr_meta.ptr)\n\n            fiber.append((tptr_meta, vptr, name))\n\n            i += 1\n\n        return fiber\n\n    def invoke(self, arg, for_tty):\n        parser = argparse.ArgumentParser(description=\"scylla fiber\")\n        parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", default=False,\n                help=\"Make the command more verbose about what it is doing\")\n        parser.add_argument(\"-d\", \"--max-depth\", action=\"store\", type=int, default=-1,\n                help=\"Maximum depth to traverse on the continuation chain\")\n        parser.add_argument(\"-s\", \"--scanned-region-size\", action=\"store\", type=int, default=512,\n                help=\"The size of the memory region to be scanned when examining a task object.\"\n                \" Only used in fallback-mode. Fallback mode is used either when the default allocator is used by the application\"\n                \" (and hence pointer-metadata is not available) or when `scylla fiber` was invoked with `--force-fallback-mode`.\")\n        parser.add_argument(\"--force-fallback-mode\", action=\"store_true\", default=False,\n                help=\"Force fallback mode to be used, that is, scan a fixed-size region of memory\"\n                \" (configurable via --scanned-region-size), instead of relying on `scylla ptr` for determining the size of the task objects.\")\n        parser.add_argument(\"task\", action=\"store\", help=\"An expression that evaluates to a valid `seastar::task*` value. Cannot contain white-space.\")\n\n        try:\n            args = parser.parse_args(arg.split())\n        except SystemExit:\n            return\n\n        if self._thread_map is None:\n            self._thread_map = {}\n            for r in reactors():\n                self._thread_map[gdb.selected_thread().num] = int(r['_id'])\n\n        def format_task_line(i, task_info):\n            tptr_meta, vptr, name = task_info\n            tptr = tptr_meta.ptr\n            shard = self._thread_map[tptr_meta.thread.num]\n            gdb.write(\"[shard {:2}] #{:<2d} (task*) 0x{:016x} 0x{:016x} {}\\n\".format(shard, i, int(tptr), int(vptr), name))\n\n        try:\n            using_seastar_allocator = not args.force_fallback_mode and scylla_ptr.is_seastar_allocator_used()\n            if not using_seastar_allocator:\n                gdb.write(\"Not using the seastar allocator, falling back to scanning a fixed-size region of memory\\n\")\n\n            initial_task_ptr = int(gdb.parse_and_eval(args.task))\n            this_task = self._probe_pointer(initial_task_ptr, args.scanned_region_size, using_seastar_allocator, args.verbose)\n            if this_task is None:\n                gdb.write(\"Provided pointer 0x{:016x} is not an object managed by seastar or not a task pointer\\n\".format(initial_task_ptr))\n                return\n\n            backwards_fiber = self._walk(self._walk_backward, this_task[0], this_task[2], args.max_depth, args.scanned_region_size, using_seastar_allocator, args.verbose)\n\n            for i, task_info in enumerate(reversed(backwards_fiber)):\n                format_task_line(i - len(backwards_fiber), task_info)\n\n            format_task_line(0, this_task)\n\n            forward_fiber = self._walk(self._walk_forward, this_task[0], this_task[2], args.max_depth, args.scanned_region_size, using_seastar_allocator, args.verbose)\n\n            for i, task_info in enumerate(forward_fiber):\n                format_task_line(i + 1, task_info)\n\n            gdb.write(\"\\nFound no further pointers to task objects.\\n\")\n            if not backwards_fiber and not forward_fiber:\n                gdb.write(\"If this is unexpected, run `scylla fiber 0x{:016x} --verbose` to learn more.\\n\".format(initial_task_ptr))\n            else:\n                gdb.write(\"If you think there should be more, run `scylla fiber 0x{:016x} --verbose` to learn more.\\n\"\n                          \"Note that continuation across user-created seastar::promise<> objects are not detected by scylla-fiber.\\n\".format(int(this_task[0].ptr)))\n        except KeyboardInterrupt:\n            return\n\n\ndef find_objects(mem_start, mem_size, value, size_selector='g', only_live=True):\n    for line in gdb.execute(\"find/%s 0x%x, +0x%x, 0x%x\" % (size_selector, mem_start, mem_size, value), to_string=True).split('\\n'):\n        if line.startswith('0x'):\n            ptr_meta = scylla_ptr.analyze(int(line, base=16))\n            if not only_live or ptr_meta.is_live:\n                yield ptr_meta\n\n\nclass scylla_find(gdb.Command):\n    \"\"\" Finds live objects on seastar heap of current shard which contain given value.\n    Prints results in 'scylla ptr' format.\n\n    See `scylla find --help` for more details on usage.\n\n    Example:\n\n      (gdb) scylla find 0x600005321900\n      thread 1, small (size <= 512), live (0x6000000f3800 +48)\n      thread 1, small (size <= 56), live (0x6000008a1230 +32)\n    \"\"\"\n\n    _size_char_to_size = {\n        'b': 8,\n        'h': 16,\n        'w': 32,\n        'g': 64,\n    }\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla find', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n\n    @staticmethod\n    def find(value, size_selector='g', value_range=0, find_all=False, only_live=True):\n        step = int(scylla_find._size_char_to_size[size_selector] / 8)\n        offset = 0\n        mem_start, mem_size = get_seastar_memory_start_and_size()\n        it = iter(find_objects(mem_start, mem_size, value, size_selector, only_live))\n\n        # Find the first value in the range for which the search has results.\n        while offset < value_range:\n            try:\n                yield next(it), offset\n                if not find_all:\n                    break\n            except StopIteration:\n                offset += step\n                it = iter(find_objects(mem_start, mem_size, value + offset, size_selector, only_live))\n\n        # List the rest of the results for value.\n        try:\n            while True:\n                yield next(it), offset\n        except StopIteration:\n            pass\n\n\n    def invoke(self, arg, for_tty):\n        parser = argparse.ArgumentParser(description=\"scylla find\")\n        parser.add_argument(\"-s\", \"--size\", action=\"store\", choices=['b', 'h', 'w', 'g', '8', '16', '32', '64'],\n                default='g',\n                help=\"Size of the searched value.\"\n                    \" Accepted values are the size expressed in number of bits: 8, 16, 32 and 64.\"\n                    \" GDB's size classes are also accepted: b(byte), h(half-word), w(word) and g(giant-word).\"\n                    \" Defaults to g (64 bits).\")\n        parser.add_argument(\"-r\", \"--resolve\", action=\"store_true\",\n                help=\"Attempt to resolve the first pointer in the found objects as vtable pointer. \"\n                \" If the resolve is successful the vtable pointer as well as the vtable symbol name will be printed in the listing.\")\n        parser.add_argument(\"--value-range\", action=\"store\", type=int, default=0,\n                help=\"Find a range of values of the specified size.\"\n                \" Will start from VALUE, then use SIZE increments until VALUE + VALUE_RANGE is reached, or at least one usage is found.\"\n                \" By default only VALUE is searched.\")\n        parser.add_argument(\"-a\", \"--find-all\", action=\"store_true\",\n                help=\"Find all references, don't stop at the first offset which has usages. See --value-range.\")\n        parser.add_argument(\"-f\", \"--include-free\", action=\"store_true\", help=\"Include freed object in the result.\")\n        parser.add_argument(\"value\", action=\"store\", help=\"The value to be searched.\")\n\n        try:\n            args = parser.parse_args(arg.split())\n        except SystemExit:\n            return\n\n        size_arg_to_size_char = {\n            'b': 'b',\n            '8': 'b',\n            'h': 'h',\n            '16': 'h',\n            'w': 'w',\n            '32': 'w',\n            'g': 'g',\n            '64': 'g',\n        }\n\n        size_char = size_arg_to_size_char[args.size]\n\n        for ptr_meta, offset in scylla_find.find(int(gdb.parse_and_eval(args.value)), size_char, args.value_range, find_all=args.find_all,\n                only_live=(not args.include_free)):\n            if args.value_range and offset:\n                formatted_offset = \"+{}; \".format(offset)\n            else:\n                formatted_offset = \"\"\n            if args.resolve:\n                maybe_vptr = int(gdb.Value(ptr_meta.obj_ptr).reinterpret_cast(_vptr_type()).dereference())\n                symbol = resolve(maybe_vptr, cache=False)\n                if symbol is None:\n                    gdb.write('{}{}\\n'.format(formatted_offset, ptr_meta))\n                else:\n                    gdb.write('{}{} 0x{:016x} {}\\n'.format(formatted_offset, ptr_meta, maybe_vptr, symbol))\n            else:\n                gdb.write('{}{}\\n'.format(formatted_offset, ptr_meta))\n\n\nclass std_unique_ptr:\n    def __init__(self, obj):\n        self.obj = obj\n\n    def get(self):\n        try:\n            std_tuple(self.obj['_M_t']['_M_t'])[0].dereference()\n            return std_tuple(self.obj['_M_t']['_M_t'])[0]\n        except:\n            return self.obj['_M_t']['_M_t']['_M_head_impl']\n\n    def dereference(self):\n        return self.get().dereference()\n\n    def __getitem__(self, item):\n        return self.dereference()[item]\n\n    def address(self):\n        return self.get()\n\n    def __nonzero__(self):\n        return bool(self.get())\n\n    def __bool__(self):\n        return self.__nonzero__()\n\n\ndef ip_to_str(val, byteorder):\n    return '%d.%d.%d.%d' % (struct.unpack('BBBB', val.to_bytes(4, byteorder=byteorder))[::-1])\n\n\ndef get_ip(ep):\n    try:\n        return ep['_addr']['ip']['raw']\n    except gdb.error:\n        return ep['_addr']['_in']['s_addr']\n\n\nclass scylla_netw(gdb.Command):\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla netw', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n\n    def invoke(self, arg, for_tty):\n        mss = sharded(gdb.parse_and_eval('debug::the_messaging_service'))\n        if not mss.instances:\n            gdb.write('debug::the_messaging_service does not exist (yet?)\\n')\n            return\n\n        ms = mss.local()\n        gdb.write('Dropped messages: %s\\n' % ms['_dropped_messages'])\n        gdb.write('Outgoing connections:\\n')\n        idx = 0\n        for clients in std_vector(ms['_clients']):\n            for (addr, shard_info) in unordered_map(clients):\n                ip = ip_to_str(int(get_ip(addr['addr'])), byteorder='big')\n                client = shard_info['rpc_client']['_p']\n                rpc_client = std_unique_ptr(client['_p'])\n                gdb.write('[%d] IP: %s, (netw::messaging_service::rpc_protocol_client_wrapper*) %s:\\n' % (idx, ip, client))\n                gdb.write('  stats: %s\\n' % rpc_client['_stats'])\n                gdb.write('  outstanding: %d\\n' % int(rpc_client['_outstanding']['_M_h']['_M_element_count']))\n            idx += 1\n\n        servers = [\n            std_unique_ptr(ms['_server']['_M_elems'][0]),\n            std_unique_ptr(ms['_server']['_M_elems'][1]),\n            std_unique_ptr(ms['_server_tls']['_M_elems'][0]),\n            std_unique_ptr(ms['_server_tls']['_M_elems'][1]),\n        ]\n        for srv in servers:\n            if srv:\n                gdb.write('Server: resources=%s\\n' % srv['_resources_available'])\n                gdb.write('Incoming connections:\\n')\n                for k, clnt in unordered_map(srv['_conns']):\n                    conn = clnt['_p'].cast(clnt.type.template_argument(0).pointer())\n                    ip = ip_to_str(int(conn['_info']['addr']['u']['in']['sin_addr']['s_addr']), byteorder='big')\n                    port = int(conn['_info']['addr']['u']['in']['sin_port'])\n                    gdb.write('%s:%d: \\n' % (ip, port))\n                    gdb.write('   %s\\n' % (conn['_stats']))\n\n\ndef get_tagged_integer_type(i):\n    try:\n        return i['_value']\n    except gdb.error:\n        return i\n\n\ndef get_gms_generation_or_version(i):\n    return get_tagged_integer_type(i)\n\n\ndef get_gms_versioned_value(vv):\n    try:\n        return {\n            'version': get_gms_generation_or_version(vv['_version']),\n            'value': vv['_value'],\n        }\n    except gdb.error:\n        return {\n            'version': vv['version'],\n            'value': vv['value'],\n        }\n\n\nclass scylla_gms(gdb.Command):\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla gms', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n\n    def invoke(self, arg, for_tty):\n        try:\n            gossiper = sharded(gdb.parse_and_eval('debug::the_gossiper')).local()\n            state_map = gossiper['_endpoint_state_map']\n        except Exception as e:\n            gossiper = sharded(gdb.parse_and_eval('gms::_the_gossiper')).local()\n            state_map = gossiper['endpoint_state_map']\n        for (endpoint, state) in unordered_map(state_map):\n            try:\n                state_ptr = seastar_lw_shared_ptr(state)\n                state = state_ptr.get().dereference()\n            except Exception:\n                pass\n            ip = ip_to_str(int(get_ip(endpoint)), byteorder=sys.byteorder)\n            gdb.write('%s: (gms::endpoint_state*) %s (%s)\\n' % (ip, state.address, state['_heart_beat_state']))\n            try:\n                app_states_map = std_unordered_map(state['_application_state'])\n            except:\n                app_states_map = std_map(state['_application_state'])\n            for app_state, vv in app_states_map:\n                value = get_gms_versioned_value(vv)\n                gdb.write('  %s: {version=%d, value=%s}\\n' % (app_state, value['version'], value['value']))\n\n\nclass scylla_cache(gdb.Command):\n    \"\"\"Prints contents of the cache on current shard\"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla cache', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def __partitions(self, table):\n        try:\n            return double_decker(table['_cache']['_partitions'])\n        except gdb.error:\n            # Compatibility, the row-cache was switched to B+ tree at some point\n            return intrusive_set(table['_cache']['_partitions'])\n\n    def invoke(self, arg, from_tty):\n        schema_ptr_type = gdb.lookup_type('schema').pointer()\n        for table in for_each_table():\n            schema = table['_schema']['_p'].reinterpret_cast(schema_ptr_type)\n            name = '%s.%s' % (schema['_raw']['_ks_name'], schema['_raw']['_cf_name'])\n            gdb.write(\"%s:\\n\" % (name))\n            for e in self.__partitions(table):\n                gdb.write('  (cache_entry*) 0x%x {_key=%s, _flags=%s, _pe=%s}\\n' % (\n                    int(e.address), e['_key'], e['_flags'], e['_pe']))\n            gdb.write(\"\\n\")\n\n\ndef find_sstables_attached_to_tables():\n    for table in for_each_table():\n        for sst_ptr in std_unordered_set(seastar_lw_shared_ptr(seastar_lw_shared_ptr(table['_sstables']).get()['_all']).get().dereference()):\n            yield seastar_lw_shared_ptr(sst_ptr).get()\n\n\ndef find_sstables():\n    \"\"\"A generator which yields pointers to all live sstable objects on current shard.\"\"\"\n    db = find_db(current_shard())\n    user_sstables_manager = std_unique_ptr(db[\"_user_sstables_manager\"]).get()\n    system_sstables_manager = std_unique_ptr(db[\"_system_sstables_manager\"]).get()\n    for manager in (user_sstables_manager, system_sstables_manager):\n        for sst_list_name in (\"_active\", \"_undergoing_close\"):\n            for sst in intrusive_list(manager[sst_list_name], link=\"_manager_list_link\"):\n                yield sst.address\n\n\nclass scylla_sstables(gdb.Command):\n    \"\"\"Lists all sstable objects on currents shard together with useful information like on-disk and in-memory size.\"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla sstables', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    @staticmethod\n    def filename(sst):\n        \"\"\"The name of the sstable.\n\n        Should mirror `sstables::sstable::component_basename()`.\n        \"\"\"\n        old_format = '{keyspace}-{table}-{version}-{generation}-Data.db'\n        new_format = '{version}-{generation}-{format}-Data.db'\n        version_to_format = {\n            'ka': old_format,\n            'la': new_format,\n            'mc': new_format,\n            'md': new_format,\n            'me': new_format\n        }\n        format_to_str = ['big']\n        schema = schema_ptr(sst['_schema'])\n        int_type = gdb.lookup_type('int')\n        version_number = int(sst['_version'])\n        version_name = list(version_to_format)[version_number]\n        generation = sst['_generation']\n        return version_to_format[version_name].format(\n                keyspace=str(schema.ks_name)[1:-1],\n                table=str(schema.cf_name)[1:-1],\n                version=version_name,\n                generation=generation,\n                format=format_to_str[int(sst['_format'].cast(int_type))],\n            )\n\n    def invoke(self, arg, from_tty):\n        parser = argparse.ArgumentParser(description=\"scylla sstables\")\n        parser.add_argument(\"-t\", \"--tables\", action=\"store_true\", help=\"Only consider sstables attached to tables\")\n        parser.add_argument(\"--histogram\", action=\"store_true\", help=\"Instead of printing all sstables, print a histogram of the number of sstables per table\")\n        try:\n            args = parser.parse_args(arg.split())\n        except SystemExit:\n            return\n\n        filter_type = gdb.lookup_type('utils::filter::murmur3_bloom_filter')\n        cpu_id = current_shard()\n        total_size = 0 # in memory\n        total_on_disk_size = 0\n        count = 0\n\n        sstable_generator = find_sstables_attached_to_tables if args.tables else find_sstables\n        sstable_histogram = histogram(print_indicators=False)\n\n        for sst in sstable_generator():\n            try:\n                is_open = sst['_open']\n            except gdb.error:\n                is_open = bool(std_optional(sst['_open_mode']))\n            if not is_open:\n                continue\n\n            count += 1\n            size = 0\n\n            sc = seastar_lw_shared_ptr(sst['_components']['_value']).get()\n            local = sst['_components']['_cpu'] == cpu_id\n            size += sc.dereference().type.sizeof\n\n            bf = std_unique_ptr(sc['filter']).get().cast(filter_type.pointer())\n            bf_size = bf.dereference().type.sizeof + chunked_vector(bf['_bitset']['_storage']).external_memory_footprint()\n            size += bf_size\n\n            summary_size = std_vector(sc['summary']['_summary_data']).external_memory_footprint()\n            summary_size += chunked_vector(sc['summary']['entries']).external_memory_footprint()\n            summary_size += chunked_vector(sc['summary']['positions']).external_memory_footprint()\n            for e in std_vector(sc['summary']['_summary_data']):\n                summary_size += e['_size'] + e.type.sizeof\n            # FIXME: include external memory footprint of summary entries\n            size += summary_size\n\n            sm_size = 0\n            sm = std_optional(sc['scylla_metadata'])\n            if sm:\n                for tag, value in unordered_map(sm.get()['data']['data']):\n                    bv = boost_variant(value)\n                    # FIXME: only gdb.Type.template_argument(0) works for boost::variant<>\n                    if bv.which() != 0:\n                        continue\n                    val = bv.get()['value']\n                    if str(val.type) == 'sstables::sharding_metadata':\n                        sm_size += chunked_vector(val['token_ranges']['elements']).external_memory_footprint()\n            size += sm_size\n\n            # FIXME: Include compression info\n\n            data_file_size = sst['_data_file_size']\n            schema = schema_ptr(sst['_schema'])\n            if args.histogram:\n                sstable_histogram.add(schema.table_name())\n            else:\n                gdb.write('(sstables::sstable*) 0x%x: local=%d data_file=%d, in_memory=%d (bf=%d, summary=%d, sm=%d) %s filename=%s\\n'\n                          % (int(sst), local, data_file_size, size, bf_size, summary_size, sm_size, schema.table_name(), scylla_sstables.filename(sst)))\n\n            if local:\n                total_size += size\n                total_on_disk_size += data_file_size\n\n        if args.histogram:\n           sstable_histogram.print_to_console()\n\n        gdb.write('total (shard-local): count=%d, data_file=%d, in_memory=%d\\n' % (count, total_on_disk_size, total_size))\n\n\nclass scylla_memtables(gdb.Command):\n    \"\"\"Lists basic information about all memtable objects on current shard.\"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla memtables', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    @staticmethod\n    def dump_memtable_list(memtable_list):\n        region_ptr_type = gdb.lookup_type('logalloc::region').pointer()\n        for mt_ptr in std_vector(memtable_list['_memtables']):\n            mt = seastar_lw_shared_ptr(mt_ptr).get()\n            reg = lsa_region(mt.cast(region_ptr_type))\n            gdb.write('  (memtable*) 0x%x: total=%d, used=%d, free=%d, flushed=%d\\n' % (mt, reg.total(), reg.used(), reg.free(), mt['_flushed_memory']))\n\n    @staticmethod\n    def dump_compaction_group_memtables(compaction_group):\n        scylla_memtables.dump_memtable_list(seastar_lw_shared_ptr(compaction_group['_memtables']).get())\n\n    def invoke(self, arg, from_tty):\n        for table in for_each_table():\n            gdb.write('table %s:\\n' % schema_ptr(table['_schema']).table_name())\n            try:\n                sg_manager = std_unique_ptr(table[\"_sg_manager\"]).get().dereference()\n                for (sg_id, sg_ptr) in absl_container(sg_manager[\"_storage_groups\"]):\n                    sg = seastar_lw_shared_ptr(sg_ptr).get()\n                    scylla_memtables.dump_compaction_group_memtables(seastar_lw_shared_ptr(sg[\"_main_cg\"]).get())\n                    for cg_ptr in std_vector(sg[\"_split_ready_groups\"]):\n                        scylla_memtables.dump_compaction_group_memtables(seastar_lw_shared_ptr(cg_ptr).get())\n                return\n            except gdb.error:\n                pass\n\n            try:\n                sg_manager = std_unique_ptr(table[\"_sg_manager\"]).get().dereference()\n                for cg in intrusive_list(sg_manager[\"_compaction_groups\"], link='_list_hook'):\n                    scylla_memtables.dump_compaction_group_memtables(cg)\n                return\n            except gdb.error:\n                pass\n\n            try:\n                for cg in intrusive_list(table[\"_compaction_groups\"], link='_list_hook'):\n                    scylla_memtables.dump_compaction_group_memtables(cg)\n                return\n            except gdb.error:\n                pass\n\n            try:\n                for cg_ptr in chunked_vector(table[\"_compaction_groups\"]):\n                    scylla_memtables.dump_compaction_group_memtables(std_unique_ptr(cg_ptr).get())\n                return\n            except gdb.error:\n                pass\n\n            try:\n                for cg_ptr in std_vector(table[\"_compaction_groups\"]):\n                    scylla_memtables.dump_compaction_group_memtables(std_unique_ptr(cg_ptr).get())\n                return\n            except gdb.error:\n                pass\n\n            scylla_memtables.dump_compaction_group_memtables(std_unique_ptr(table[\"_compaction_group\"]).get())\n\ndef escape_html(s):\n    return s.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')\n\n\nclass scylla_generate_object_graph(gdb.Command):\n    \"\"\"Generate an object graph for an object.\n\n    The object graph is a directed graph, where vertices are objects and edges\n    are references between them, going from referrers to the referee. The\n    vertices contain information, like the address of the object, its size,\n    whether it is a live or not and if applies, the address and symbol name of\n    its vtable. The edges contain the list of offsets the referrer has references\n    at. The generated graph is an image, which allows the visual inspection of the\n    object graph.\n\n    The graph is generated with the help of `graphviz`. The command\n    generates `.dot` files which can be converted to images with the help of\n    the `dot` utility. The command can do this if the output file is one of\n    the supported image formats (e.g. `png`), otherwise only the `.dot` file\n    is generated, leaving the actual image generation to the user. When that is\n    the case, the generated `.dot` file can be converted to an image with the\n    following command:\n\n        dot -Tpng graph.dot -o graph.png\n\n    The `.dot` file is always generated, regardless of the specified output. This\n    file will contain the full name of vtable symbols. The graph will only contain\n    cropped versions of those to keep the size reasonable.\n\n    See `scylla generate_object_graph --help` for more details on usage.\n    Also see `man dot` for more information on supported output formats.\n\n    \"\"\"\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla generate-object-graph', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    @staticmethod\n    def _traverse_object_graph_breadth_first(address, max_depth, max_vertices, timeout_seconds, value_range_override):\n        vertices = dict() # addr -> obj info (ptr metadata, vtable symbol)\n        edges = defaultdict(set) # (referrer, referee) -> {(prev_offset1, next_offset1), (prev_offset2, next_offset2), ...}\n\n        vptr_type = gdb.lookup_type('uintptr_t').pointer()\n\n        vertices[address] = (scylla_ptr.analyze(address), resolve(gdb.Value(address).reinterpret_cast(vptr_type).dereference(), cache=False))\n\n        current_objects = [address]\n        next_objects = []\n        depth = 0\n        start_time = time.time()\n        stop = False\n\n        while not stop:\n            depth += 1\n            for current_obj in current_objects:\n                if value_range_override == -1:\n                    value_range = vertices[current_obj][0].size\n                else:\n                    value_range = value_range_override\n                for ptr_meta, to_off in scylla_find.find(current_obj, value_range=value_range, find_all=True):\n                    if timeout_seconds > 0:\n                        current_time = time.time()\n                        if current_time - start_time > timeout_seconds:\n                            stop = True\n                            break\n\n                    next_obj, next_off = ptr_meta.ptr, ptr_meta.offset_in_object\n                    next_obj -= next_off\n                    edges[(next_obj, current_obj)].add((next_off, to_off))\n                    if next_obj in vertices:\n                        continue\n\n                    symbol_name = resolve(gdb.Value(next_obj).reinterpret_cast(vptr_type).dereference(), cache=False)\n                    vertices[next_obj] = (ptr_meta, symbol_name)\n\n                    next_objects.append(next_obj)\n\n                    if max_vertices > 0 and len(vertices) >= max_vertices:\n                        stop = True\n                        break\n\n            if max_depth > 0 and depth == max_depth:\n                stop = True\n                break\n\n            current_objects = next_objects\n            next_objects = []\n\n        return edges, vertices\n\n    @staticmethod\n    def _do_generate_object_graph(address, output_file, max_depth, max_vertices, timeout_seconds, value_range_override):\n        edges, vertices = scylla_generate_object_graph._traverse_object_graph_breadth_first(address, max_depth,\n                max_vertices, timeout_seconds, value_range_override)\n\n        vptr_type = gdb.lookup_type('uintptr_t').pointer()\n        prefix_len = len('vtable for ')\n\n        for addr, obj_info in vertices.items():\n            ptr_meta, vtable_symbol_name = obj_info\n            size = ptr_meta.size\n            state = \"L\" if ptr_meta.is_live else \"F\"\n\n            addr_str = \"<b>0x{:x}</b>\".format(addr) if addr == address else \"0x{:x}\".format(addr)\n\n            if vtable_symbol_name:\n                symbol_name = vtable_symbol_name[prefix_len:] if len(vtable_symbol_name) > prefix_len else vtable_symbol_name\n                output_file.write('{} [label=<{} ({}, {})<br/>{}>]; // {}\\n'.format(addr, addr_str, size, state,\n                    escape_html(symbol_name[:32]), vtable_symbol_name))\n            else:\n                output_file.write('{} [label=<{} ({}, {})>];\\n'.format(addr, addr_str, size, state, ptr_meta))\n\n        for edge, offsets in edges.items():\n            a, b = edge\n            output_file.write('{} -> {} [label=\"{}\"];\\n'.format(a, b, offsets))\n\n    @staticmethod\n    def generate_object_graph(address, output_file, max_depth, max_vertices, timeout_seconds, value_range_override):\n        with open(output_file, 'w') as f:\n            f.write('digraph G {\\n')\n            scylla_generate_object_graph._do_generate_object_graph(address, f, max_depth, max_vertices, timeout_seconds, value_range_override)\n            f.write('}')\n\n    def invoke(self, arg, from_tty):\n        parser = argparse.ArgumentParser(description=\"scylla generate-object-graph\")\n        parser.add_argument(\"-o\", \"--output-file\", action=\"store\", type=str, default=\"graph.dot\",\n                help=\"Output file. Supported extensions are: dot, png, jpg, jpeg, svg and pdf.\"\n                \" Regardless of the extension, a `.dot` file will always be generated.\"\n                \" If the output is one of the graphic formats the command will convert the `.dot` file using the `dot` utility.\"\n                \" In this case the dot utility from the graphviz suite has to be installed on the machine.\"\n                \" To manually convert the `.dot` file do: `dot -Tpng graph.dot -o graph.png`.\")\n        parser.add_argument(\"-d\", \"--max-depth\", action=\"store\", type=int, default=5,\n                help=\"Maximum depth to traverse the object graph. Set to -1 for unlimited depth. Default is 5.\")\n        parser.add_argument(\"-v\", \"--max-vertices\", action=\"store\", type=int, default=-1,\n                help=\"Maximum amount of vertices (objects) to add to the object graph. Set to -1 to unlimited. Default is -1 (unlimited).\")\n        parser.add_argument(\"-t\", \"--timeout\", action=\"store\", type=int, default=-1,\n                help=\"Maximum amount of seconds to spend building the graph. Set to -1 for no timeout. Default is -1 (unlimited).\")\n        parser.add_argument(\"-r\", \"--value-range-override\", action=\"store\", type=int, default=-1,\n                help=\"The portion of the object to find references to. Same as --value-range for `scylla find`.\"\n                \" This can greatly speed up the graph generation when the graph has large objects but references are likely to point to their first X bytes.\"\n                \" Default value is -1, meaning the entire object is scanned (--value-range=sizeof(object)).\")\n        parser.add_argument(\"object\", action=\"store\", help=\"The object that is the starting point of the graph.\")\n\n        try:\n            args = parser.parse_args(arg.split())\n        except SystemExit:\n            return\n\n        supported_extensions = {'dot', 'png', 'jpg', 'jpeg', 'svg', 'pdf'}\n        head, tail = os.path.split(args.output_file)\n        filename, extension = tail.split('.')\n\n        if not extension in supported_extensions:\n            raise ValueError(\"The output file `{}' has unsupported extension `{}'. Supported extensions are: {}\".format(\n                args.output_file, extension, supported_extensions))\n\n        if extension != 'dot':\n            dot_file = os.path.join(head, filename + '.dot')\n        else:\n            dot_file = args.output_file\n\n        if args.max_depth == -1 and args.max_vertices == -1 and args.timeout == -1:\n            raise ValueError(\"The search has to be limited by at least one of: MAX_DEPTH, MAX_VERTICES or TIMEOUT\")\n\n        scylla_generate_object_graph.generate_object_graph(int(gdb.parse_and_eval(args.object)), dot_file,\n                args.max_depth, args.max_vertices, args.timeout, args.value_range_override)\n\n        if extension != 'dot':\n            subprocess.check_call(['dot', '-T' + extension, dot_file, '-o', args.output_file])\n\n\nclass scylla_smp_queues(gdb.Command):\n    \"\"\"Summarize the shard's outgoing smp queues.\n\n    The summary takes the form of a histogram. Example:\n\n    (gdb) scylla smp-queues\n        10747 17 ->  3 ++++++++++++++++++++++++++++++++++++++++\n          721 17 -> 19 ++\n          247 17 -> 20 +\n          233 17 -> 10 +\n          210 17 -> 14 +\n          205 17 ->  4 +\n          204 17 ->  5 +\n          198 17 -> 16 +\n          197 17 ->  6 +\n          189 17 -> 11 +\n          181 17 ->  1 +\n          179 17 -> 13 +\n          176 17 ->  2 +\n          173 17 ->  0 +\n          163 17 ->  8 +\n        1 17 ->  9 +\n\n    Each line has the following format\n\n        count from -> to ++++\n\n    Where:\n        count: the number of items in the queue;\n        from: the shard, from which the message was sent (this shard);\n        to: the shard, to which the message is sent;\n        ++++: visual illustration of the relative size of this queue;\n\n    See `scylla smp-queues --help` for more details on usage.\n    \"\"\"\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla smp-queues', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n        self.queues = set()\n\n    def _init(self):\n        qs = gdb.parse_and_eval('seastar::smp::_qs')\n        if qs.type.code != gdb.TYPE_CODE_PTR:\n            # older Seastar use std::unique_ptr for this variable\n            qs = std_unique_ptr(qs).get()\n        for i in range(cpus()):\n            for j in range(cpus()):\n                self.queues.add(qs[i][j])\n        self._queue_type = gdb.lookup_type('seastar::smp_message_queue').pointer()\n        self._ptr_type = gdb.lookup_type('uintptr_t').pointer()\n        self._unsigned_type = gdb.lookup_type('unsigned')\n        self._queue_size = 128\n        self._item_ptr_array_type = gdb.lookup_type('seastar::smp_message_queue::work_item').pointer().pointer()\n\n    def invoke(self, arg, from_tty):\n        if not self.queues:\n            self._init()\n\n        parser = argparse.ArgumentParser(description=\"scylla smp-queues\")\n        parser.add_argument(\"-f\", \"--from\", action=\"store\", type=int, required=False, dest=\"from_cpu\",\n                help=\"Filter for queues going from the given CPU\")\n        parser.add_argument(\"-t\", \"--to\", action=\"store\", type=int, required=False, dest=\"to_cpu\",\n                help=\"Filter for queues going to the given CPU\")\n        parser.add_argument(\"-g\", \"--scheduling-group\", action=\"store\", type=str, required=False,\n                help=\"Filter for work items belonging to the specified scheduling group (by name or id)\")\n        parser.add_argument(\"-c\", \"--content\", action=\"store_true\",\n                help=\"Create a histogram from the content of the queues, rather than the number of items in them\")\n\n        try:\n            args = parser.parse_args(arg.split())\n        except SystemExit:\n            return\n\n        sg_id = None\n        if args.scheduling_group:\n            for tq in get_local_task_queues():\n                id_str = \"{}\".format(tq['_id'].cast(self._unsigned_type))\n                name_str = str(tq['_name']).replace('\"', '')\n                if args.scheduling_group == id_str or args.scheduling_group == name_str:\n                    sg_id = int(tq['_id'])\n\n            if sg_id is None:\n                gdb.write(\"error: non-existent scheduling-group provided for filtering: `{}', run `scylla task-queues` for a list of valid scheduling group names and ids\\n\".format(args.scheduling_group))\n                return\n\n        def formatter(q):\n            if args.content:\n                return \"0x{:x} {}\".format(q, resolve(q))\n            else:\n                return '{:2} -> {:2}'.format(*q)\n\n        h = histogram(formatter=formatter, print_indicators=not args.content)\n        empty_queues = 0\n\n        def add_to_histogram(a, b, key=None, count=1):\n            if sg_id is not None and int(key.dereference()['_sg']['_id']) != sg_id:\n                return\n\n            if args.content:\n                vptr = int(gdb.Value(key).reinterpret_cast(self._ptr_type).dereference())\n                h[vptr] += count\n            else:\n                h[(a, b)] += count\n\n        for q in self.queues:\n            a = int(q['_completed']['remote']['_id'])\n            b = int(q['_pending']['remote']['_id'])\n            if args.from_cpu is not None and a != args.from_cpu:\n                continue\n            if args.to_cpu is not None and b != args.to_cpu:\n                continue\n\n            tx_queue = std_deque(q['_tx']['a']['pending_fifo'])\n            pending_queue = q['_pending']\n            pending_start = int(pending_queue['read_index_']['_M_i'])\n            pending_end = int(pending_queue['write_index_']['_M_i'])\n            if pending_end < pending_start:\n                pending_end += self._queue_size\n\n            if args.content or args.scheduling_group:\n                for item_ptr in tx_queue:\n                    add_to_histogram(a, b, key=item_ptr)\n\n                # Boost uses an aligned (to 8 bytes) buffer here.\n                # We'll just assume alignment is right.\n                buf = gdb.Value(pending_queue['storage_']['data_']['buf']).reinterpret_cast(self._item_ptr_array_type)\n\n                for i in range(pending_start, pending_end):\n                    add_to_histogram(a, b, key=buf[i % self._queue_size])\n            else:\n                count = len(tx_queue) + pending_end - pending_start\n                if count != 0:\n                    add_to_histogram(a, b, count=count)\n                else:\n                    empty_queues += 1\n\n        gdb.write('{}\\n'.format(h))\n        if empty_queues > 0:\n            gdb.write('omitted {} empty queues\\n'.format(empty_queues))\n\n\nclass scylla_small_objects(gdb.Command):\n    \"\"\"List live objects from one of the seastar allocator's small pools\n\n    The pool is selected with the `-o|--object-size` flag. Results are paginated by\n    default as there can be millions of objects. Default page size is 20.\n    To list a certain page, use the `-p|--page` flag. To find out the number of\n    total objects and pages, use `--summarize`.\n    To sample random pages, use `--random-page`.\n\n    If objects have a vtable, its type is resolved and this will appear in the\n    listing.\n\n    Note that to reach a certain page, the command has to traverse the memory\n    spans belonging to the pool linearly, until the desired range of object is\n    found. This can take a long time for well populated pools. To speed this\n    up, the span iterator is saved and reused when possible. This caching can\n    only be exploited within the same pool and only with monotonically\n    increasing pages.\n\n    For usage see: scylla small-objects --help\n\n    Examples:\n\n    (gdb) scylla small-objects -o 32 --summarize\n    number of objects: 60196912\n    page size        : 20\n    number of pages  : 3009845\n\n    (gdb) scylla small-objects -o 32 -p 100\n    page 100: 2000-2019\n    [2000] 0x635002ecba00\n    [2001] 0x635002ecba20\n    [2002] 0x635002ecba40\n    [2003] 0x635002ecba60\n    [2004] 0x635002ecba80\n    [2005] 0x635002ecbaa0\n    [2006] 0x635002ecbac0\n    [2007] 0x635002ecbae0\n    [2008] 0x635002ecbb00\n    [2009] 0x635002ecbb20\n    [2010] 0x635002ecbb40\n    [2011] 0x635002ecbb60\n    [2012] 0x635002ecbb80\n    [2013] 0x635002ecbba0\n    [2014] 0x635002ecbbc0\n    [2015] 0x635002ecbbe0\n    [2016] 0x635002ecbc00\n    [2017] 0x635002ecbc20\n    [2018] 0x635002ecbc40\n    [2019] 0x635002ecbc60\n    \"\"\"\n    class small_object_iterator():\n        def __init__(self, small_pool, resolve_symbols):\n            self._small_pool = small_pool\n            self._resolve_symbols = resolve_symbols\n\n            self._text_ranges = get_text_ranges()\n            self._free_object_ptr = gdb.lookup_type('void').pointer().pointer()\n            self._page_size = int(gdb.parse_and_eval('\\'seastar::memory::page_size\\''))\n            self._free_in_pool = set()\n            self._free_in_span = set()\n\n            pool_next_free = self._small_pool['_free']\n            while pool_next_free:\n                self._free_in_pool.add(int(pool_next_free))\n                pool_next_free = pool_next_free.reinterpret_cast(self._free_object_ptr).dereference()\n\n            self._span_it = iter(spans())\n            self._obj_it = iter([]) # initialize to exhausted iterator\n\n        def _next_span(self):\n            # Let any StopIteration bubble up, as it signals we are done with\n            # all spans.\n            span = next(self._span_it)\n            while span.pool() != self._small_pool.address:\n                span = next(self._span_it)\n\n            self._free_in_span = set()\n            span_start = int(span.start)\n            span_end = int(span_start + span.size() * self._page_size)\n\n            # span's free list\n            span_next_free = span.page['freelist']\n            while span_next_free:\n                self._free_in_span.add(int(span_next_free))\n                span_next_free = span_next_free.reinterpret_cast(self._free_object_ptr).dereference()\n\n            return span_start, span_end\n\n        def _next_obj(self):\n            try:\n                return next(self._obj_it)\n            except StopIteration:\n                # Don't call self._next_span() here as it might throw another StopIteration.\n                pass\n\n            span_start, span_end = self._next_span()\n            self._obj_it = iter(range(span_start, span_end, int(self._small_pool['_object_size'])))\n            return next(self._obj_it)\n\n        def __next__(self):\n            obj = self._next_obj()\n            while obj in self._free_in_span or obj in self._free_in_pool:\n                obj = self._next_obj()\n\n            if self._resolve_symbols:\n                addr = gdb.Value(obj).reinterpret_cast(_vptr_type()).dereference()\n                if addr_in_ranges(self._text_ranges, addr):\n                    return (obj, resolve(addr))\n                else:\n                    return (obj, None)\n            else:\n                return (obj, None)\n\n        def __iter__(self):\n            return self\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla small-objects', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n        self._parser = None\n        self._iterator = None\n        self._last_pos = 0\n        self._last_object_size = None\n\n    @staticmethod\n    def get_object_sizes():\n        cpu_mem = gdb.parse_and_eval('\\'seastar::memory::cpu_mem\\'')\n        small_pools = cpu_mem['small_pools']\n        nr = int(small_pools['nr_small_pools'])\n        return [int(small_pools['_u']['a'][i]['_object_size']) for i in range(nr)]\n\n    @staticmethod\n    def find_small_pool(object_size):\n        cpu_mem = gdb.parse_and_eval('\\'seastar::memory::cpu_mem\\'')\n        small_pools = cpu_mem['small_pools']\n        nr = int(small_pools['nr_small_pools'])\n        for i in range(nr):\n            sp = small_pools['_u']['a'][i]\n            if object_size == int(sp['_object_size']):\n                return sp\n\n        return None\n\n    def init_parser(self):\n        parser = argparse.ArgumentParser(description=\"scylla small-objects\")\n        parser.add_argument(\"-o\", \"--object-size\", action=\"store\", type=int, required=True,\n                help=\"Object size, valid sizes are: {}\".format(scylla_small_objects.get_object_sizes()))\n        parser.add_argument(\"-p\", \"--page\", action=\"store\", type=int, default=0, help=\"Page to show.\")\n        parser.add_argument(\"-s\", \"--page-size\", action=\"store\", type=int, default=20,\n                help=\"Number of objects in a page. A page size of 0 turns off paging.\")\n        parser.add_argument(\"--random-page\", action=\"store_true\", help=\"Show a random page.\")\n        parser.add_argument(\"--summarize\", action=\"store_true\",\n                help=\"Print the number of objects and pages in the pool.\")\n        parser.add_argument(\"--verbose\", action=\"store_true\",\n                help=\"Print additional details on what is going on.\")\n\n        self._parser = parser\n\n    def get_objects(self, small_pool, offset=0, count=0, resolve_symbols=False, verbose=False):\n        if self._last_object_size != int(small_pool['_object_size']) or offset < self._last_pos:\n            self._last_pos = 0\n            self._iterator = scylla_small_objects.small_object_iterator(small_pool, resolve_symbols)\n\n        skip = offset - self._last_pos\n        if verbose:\n            gdb.write('get_objects(): offset={}, count={}, last_pos={}, skip={}\\n'.format(offset, count, self._last_pos, skip))\n\n        for _ in range(skip):\n            next(self._iterator)\n\n        if count:\n            objects = []\n            for _ in range(count):\n                objects.append(next(self._iterator))\n        else:\n            objects = list(self._iterator)\n\n        self._last_pos += skip\n        self._last_pos += len(objects)\n\n        return objects\n\n    def invoke(self, arg, from_tty):\n        if self._parser is None:\n            self.init_parser()\n\n        try:\n            args = self._parser.parse_args(arg.split())\n        except SystemExit:\n            return\n\n        small_pool = scylla_small_objects.find_small_pool(args.object_size)\n        if small_pool is None:\n            raise ValueError(\"{} is not a valid object size for any small pools, valid object sizes are: {}\", scylla_small_objects.get_object_sizes())\n\n        if args.summarize:\n            if self._last_object_size != args.object_size:\n                if args.verbose:\n                    gdb.write(\"Object size changed ({} -> {}), scanning pool.\\n\".format(self._last_object_size, args.object_size))\n                self._num_objects = len(self.get_objects(small_pool, verbose=args.verbose))\n                self._last_object_size = args.object_size\n            gdb.write(\"number of objects: {}\\n\"\n                      \"page size        : {}\\n\"\n                      \"number of pages  : {}\\n\"\n                .format(\n                    self._num_objects,\n                    args.page_size,\n                    int(self._num_objects / args.page_size)))\n            return\n\n        if args.random_page:\n            if self._last_object_size != args.object_size:\n                if args.verbose:\n                    gdb.write(\"Object size changed ({} -> {}), scanning pool.\\n\".format(self._last_object_size, args.object_size))\n                self._num_objects = len(self.get_objects(small_pool, verbose=args.verbose))\n                self._last_object_size = args.object_size\n            page = random.randint(0, int(self._num_objects / args.page_size) - 1)\n        else:\n            page = args.page\n\n        offset = page * args.page_size\n        gdb.write(\"page {}: {}-{}\\n\".format(page, offset, offset + args.page_size - 1))\n        for i, (obj, sym) in enumerate(self.get_objects(small_pool, offset, args.page_size, resolve_symbols=True, verbose=args.verbose)):\n            if sym is None:\n                sym_text = \"\"\n            else:\n                sym_text = sym\n            gdb.write(\"[{}] 0x{:x} {}\\n\".format(offset + i, obj, sym_text))\n\n\nclass scylla_large_objects(gdb.Command):\n    \"\"\"List live objects from one of the seastar allocator's large pools\n\n    The pool is selected with the `-o|--object-size` flag. Results are paginated by\n    default as there can be thousands of objects. Default page size is 20.\n    To list a certain page, use the `-p|--page` flag. To find out the number of\n    total objects and pages, use `--summarize`.\n    To sample random pages, use `--random-page`.\n\n    For usage see: scylla large-objects --help\n\n    Examples:\n\n    (gdb) scylla large-objects -o 32768 --summarize\n    number of objects: 9737\n    page size        : 20\n    number of pages  : 487\n\n    (gdb) scylla large-objects -o 131072 --random-page\n    [40] 0x60001cb40000\n    [41] 0x60001cb60000\n    [42] 0x60001cb80000\n    [43] 0x60001cba0000\n    [44] 0x60001cbc0000\n    [45] 0x60001cbe0000\n    [46] 0x60001cc00000\n    [47] 0x60001cc20000\n    [48] 0x60001cc40000\n    [49] 0x60001cc60000\n    [50] 0x60001cc80000\n    [51] 0x60001cca0000\n    [52] 0x60001ccc0000\n    [53] 0x60001cce0000\n    [54] 0x60001cd00000\n    [55] 0x60001cd20000\n    [56] 0x60001cd40000\n    [57] 0x60001cd60000\n    [58] 0x60001cd80000\n    [59] 0x60001cda0000\n    \"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla large-objects', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n        self._parser = None\n        self._objects = None\n        self._object_size = None\n\n    def init_parser(self):\n        parser = argparse.ArgumentParser(description=\"scylla large-objects\")\n        parser.add_argument(\"-o\", \"--object-size\", action=\"store\", type=int, required=True,\n                            help=\"Object size, valid sizes are powers of two, above {}\".format(int(gdb.parse_and_eval(\"seastar::memory::max_small_allocation\"))))\n        parser.add_argument(\"-p\", \"--page\", action=\"store\", type=int, default=0, help=\"Page to show.\")\n        parser.add_argument(\"-s\", \"--page-size\", action=\"store\", type=int, default=20,\n                help=\"Number of objects in a page. A page size of 0 turns off paging.\")\n        parser.add_argument(\"--random-page\", action=\"store_true\", help=\"Show a random page.\")\n        parser.add_argument(\"--summarize\", action=\"store_true\",\n                help=\"Print the number of objects and pages in the pool.\")\n        parser.add_argument(\"--verbose\", action=\"store_true\",\n                help=\"Print additional details on what is going on.\")\n\n        self._parser = parser\n\n    def invoke(self, arg, from_tty):\n        if self._parser is None:\n            self.init_parser()\n\n        try:\n            args = self._parser.parse_args(arg.split())\n        except SystemExit:\n            return\n\n        object_size = int(args.object_size / int(gdb.parse_and_eval('\\'seastar::memory::page_size\\'')))\n\n        if self._objects is None or object_size != self._object_size:\n            if args.verbose:\n                gdb.write(f\"Object size changed ({self._object_size} pages -> {object_size} pages), scanning spans.\\n\")\n            # We materialize all relevant spans. There aren't usually that many\n            # of them. Rewrite this to be lazy if this proves problematic with\n            # certain cores.\n            self._objects = [s for s in spans() if s.is_large() and not s.is_free() and s.size() == object_size]\n\n        self._object_size = object_size\n\n        if args.summarize:\n            gdb.write(\"number of objects: {}\\n\"\n                      \"page size        : {}\\n\"\n                      \"number of pages  : {}\\n\"\n                      .format(\n                          len(self._objects),\n                          args.page_size,\n                          math.ceil(len(self._objects) / args.page_size)))\n            return\n\n        if args.random_page:\n            page = random.randint(0, int(len(self._objects) / args.page_size) - 1)\n        else:\n            page = args.page\n\n        if page >= len(self._objects)/args.page_size:\n            return\n\n        start = page * args.page_size\n        end = start + args.page_size\n        for index, span in enumerate(self._objects[start:end]):\n            gdb.write(\"[{}] 0x{:x}\\n\".format(start + index, span.start))\n\n\nclass scylla_compaction_tasks(gdb.Command):\n    \"\"\"Summarize the compaction::compaction_task_executor instances.\n\n    The summary is created based on compaction_manager::_tasks and it takes the\n    form of a histogram with the compaction type and compaction running and\n    table name as keys. Example:\n\n    (gdb) scylla compaction-task\n         2116 type=sstables::compaction_type::Compaction, running=false, \"cdc_test\".\"test_table_postimage_scylla_cdc_log\"\n          769 type=sstables::compaction_type::Compaction, running=false, \"cdc_test\".\"test_table_scylla_cdc_log\"\n          750 type=sstables::compaction_type::Compaction, running=false, \"cdc_test\".\"test_table_preimage_postimage_scylla_cdc_log\"\n          731 type=sstables::compaction_type::Compaction, running=false, \"cdc_test\".\"test_table_preimage_scylla_cdc_log\"\n          293 type=sstables::compaction_type::Compaction, running=false, \"cdc_test\".\"test_table\"\n          286 type=sstables::compaction_type::Compaction, running=false, \"cdc_test\".\"test_table_preimage\"\n          230 type=sstables::compaction_type::Compaction, running=false, \"cdc_test\".\"test_table_postimage\"\n           58 type=sstables::compaction_type::Compaction, running=false, \"cdc_test\".\"test_table_preimage_postimage\"\n        4 type=sstables::compaction_type::Compaction, running=true , \"cdc_test\".\"test_table_postimage_scylla_cdc_log\"\n        2 type=sstables::compaction_type::Compaction, running=true , \"cdc_test\".\"test_table\"\n        2 type=sstables::compaction_type::Compaction, running=true , \"cdc_test\".\"test_table_preimage_postimage_scylla_cdc_log\"\n        2 type=sstables::compaction_type::Compaction, running=true , \"cdc_test\".\"test_table_preimage\"\n        1 type=sstables::compaction_type::Compaction, running=true , \"cdc_test\".\"test_table_preimage_postimage\"\n        1 type=sstables::compaction_type::Compaction, running=true , \"cdc_test\".\"test_table_scylla_cdc_log\"\n        1 type=sstables::compaction_type::Compaction, running=true , \"cdc_test\".\"test_table_preimage_scylla_cdc_log\"\n    Total: 5246 instances of compaction::compaction_task_executor\n    \"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla compaction-tasks', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def invoke(self, arg, from_tty):\n        db = find_db()\n        cm = db['_compaction_manager']\n        try:\n            aux = std_unique_ptr(db['_compaction_manager']).get().dereference()\n            cm = aux\n        except:\n            pass\n        task_hist = histogram(print_indicators=False)\n\n        try:\n            task_list = list(intrusive_list(cm['_tasks']))\n        except gdb.error: # 6.2 compatibility\n            task_list = list(std_list(cm['_tasks']))\n\n        for task in task_list:\n            task = seastar_shared_ptr(task).get().dereference()\n            schema = schema_ptr(task['_compacting_table'].dereference()['_schema'])\n            key = 'type={}, state={:5}, {}'.format(task['_type'], str(task['_state']), schema.table_name())\n            task_hist.add(key)\n\n        task_hist.print_to_console()\n        gdb.write('Total: {} instances of compaction::compaction_task_executor\\n'.format(len(task_list)))\n\n\nclass Schema(object):\n    def __init__(self, schema, partition_key_types, clustering_key_types):\n        self.schema = schema\n        self.partition_key_types = partition_key_types\n        self.clustering_key_types = clustering_key_types\n        self.cql_type_deserializers = dict()\n\n    def get_cql_type_deserializer(self, type_name):\n        if type_name in self.cql_type_deserializers:\n            return self.cql_type_deserializers[type_name]\n\n        if type_name == '\"org.apache.cassandra.db.marshal.UTF8Type\"':\n            d = lambda b: b.decode('utf-8')\n        else:\n            raise Exception(f'Deserializer for {type_name} not implemented')\n\n        self.cql_type_deserializers[type_name] = d\n        return d\n\n    \"\"\"key must be a reference to an instance of clustering_key.\n    Returns a list of values of key components. \n    The object sorts the same as the clustering key.\n    \"\"\"\n    def parse_clustering_key(self, key):\n        b = managed_bytes_printer(key['_bytes']).pure_bytes()\n        return self.parse_clustering_key_bytes(b)\n\n    \"\"\"key must be a reference to an instance of partition_key.\n    \n    Returns a list of values of key components. \n    The object sorts the same as the partition key.\n    \"\"\"\n    def parse_partition_key(self, key):\n        b = managed_bytes_printer(key['_bytes']).pure_bytes()\n        return self.parse_partition_key_bytes(b)\n\n    \"\"\"b is a bytes object representing a clustering_key in memory\"\"\"\n    def parse_clustering_key_bytes(self, b):\n        return self.parse_key_bytes(self.clustering_key_types, b)\n\n    \"\"\"b is a bytes object representing a partition_key in memory\"\"\"\n    def parse_partition_key_bytes(self, b):\n        return self.parse_key_bytes(self.partition_key_types, b)\n\n    def deserialize_cql_value(self, abstract_type, b):\n        return self.get_cql_type_deserializer(str(abstract_type['_name']))(b)\n\n    \"\"\"b is a byte representation of a compound object\"\"\"\n    def parse_key_bytes(self, types, b):\n        offset = 0\n        type_nr = 0\n        components = list()\n        while offset < len(b):\n            component_size = int.from_bytes(b[offset:(offset+2)], byteorder='big')\n            offset += 2\n            components.append(self.deserialize_cql_value(types[type_nr], b[offset:(offset+component_size)]))\n            offset += component_size\n            type_nr += 1\n        return components\n\n\ncurrent_schema = None\n\n\ndef get_current_schema():\n    if not current_schema:\n        raise Exception('Current schema not set. Use \\\"scylla set-schema\\\" to set it.')\n    return current_schema\n\n\nclass scylla_set_schema(gdb.Command):\n    \"\"\"Sets the current schema to be used by schema-aware commands.\n\n    Setting the schema allows some commands and printers to interpret schema-dependent objects and present them\n    in a more friendly form.\n\n    Some commands require schema to work and will fail otherwise.\n    \"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla set-schema', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def get_abstract_types(self, key_type):\n        return [seastar_shared_ptr(key_type).get().dereference() for key_type in std_vector(key_type['_types'])]\n\n    def invoke(self, schema, from_tty):\n        global current_schema\n        if not schema:\n            current_schema = None\n            return\n        schema = gdb.parse_and_eval(schema)\n        typ = schema.type.strip_typedefs()\n        if typ.code == gdb.TYPE_CODE_PTR or typ.code == gdb.TYPE_CODE_REF or typ.code == gdb.TYPE_CODE_RVALUE_REF:\n            schema = schema.referenced_value()\n            typ = schema.type\n        if typ.name.startswith('seastar::lw_shared_ptr<'):\n            schema = seastar_lw_shared_ptr(schema).get().dereference()\n            typ = schema.type\n\n        raw_schema = schema['_raw']\n\n        gdb.write('(schema*) 0x{:x} ks={} cf={} id={} version={}\\n'.format(int(schema.address), raw_schema['_ks_name'], raw_schema['_cf_name'], raw_schema['_id'], raw_schema['_version']))\n\n        partition_key_type = self.get_abstract_types(seastar_lw_shared_ptr(schema['_partition_key_type']).get().dereference())\n        clustering_key_type = self.get_abstract_types(seastar_lw_shared_ptr(schema['_clustering_key_type']).get().dereference())\n        current_schema = Schema(schema, partition_key_type, clustering_key_type)\n\n\nclass scylla_schema(gdb.Command):\n    \"\"\"Pretty print a schema\n\n    Example:\n    (gdb) scylla schema $s\n    (schema*) 0x604009352380 ks=\"scylla_bench\" cf=\"test\" id=a3eadd80-f2a7-11ea-853c-000000000004 version=47e0bf13-6cc8-3421-93c6-a9fe169b1689\n\n    partition key: byte_order_equal=true byte_order_comparable=false is_reversed=false\n        \"org.apache.cassandra.db.marshal.LongType\"\n\n    clustering key: byte_order_equal=true byte_order_comparable=false is_reversed=true\n        \"org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.LongType)\"\n\n    columns:\n        column_kind::partition_key  id=0 ordinal_id=0 \"pk\" \"org.apache.cassandra.db.marshal.LongType\" is_atomic=true is_counter=false\n        column_kind::clustering_key id=0 ordinal_id=1 \"ck\" \"org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.LongType)\" is_atomic=true is_counter=false\n        column_kind::regular_column id=0 ordinal_id=2 \"v\" \"org.apache.cassandra.db.marshal.BytesType\" is_atomic=true is_counter=false\n\n    Argument is an expression that evaluates to a schema value, reference,\n    pointer or shared pointer.\n    \"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla schema', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def print_key_type(self, key_type, key_name):\n        gdb.write('{} key: byte_order_equal={} byte_order_comparable={} is_reversed={}\\n'.format(\n                key_name, key_type['_byte_order_equal'], key_type['_byte_order_comparable'], key_type['_is_reversed']))\n\n        for key_type in std_vector(key_type['_types']):\n            key_type = seastar_shared_ptr(key_type).get().dereference()\n            gdb.write('    {}\\n'.format(key_type['_name']))\n\n    def invoke(self, schema, from_tty):\n        schema = gdb.parse_and_eval(schema)\n        typ = schema.type.strip_typedefs()\n        if typ.code == gdb.TYPE_CODE_PTR or typ.code == gdb.TYPE_CODE_REF or typ.code == gdb.TYPE_CODE_RVALUE_REF:\n            schema = schema.referenced_value()\n            typ = schema.type\n        if typ.name.startswith('seastar::lw_shared_ptr<'):\n            schema = seastar_lw_shared_ptr(schema).get().dereference()\n            typ = schema.type\n\n        raw_schema = schema['_raw']\n\n        gdb.write('(schema*) 0x{:x} ks={} cf={} id={} version={}\\n'.format(int(schema.address), raw_schema['_ks_name'], raw_schema['_cf_name'], raw_schema['_id'], raw_schema['_version']))\n\n        gdb.write('\\n')\n        self.print_key_type(seastar_lw_shared_ptr(schema['_partition_key_type']).get().dereference(), 'partition')\n\n        gdb.write('\\n')\n        self.print_key_type(seastar_lw_shared_ptr(schema['_clustering_key_type']).get().dereference(), 'clustering')\n\n        gdb.write('\\n')\n        gdb.write(\"columns:\\n\")\n        for cdef in std_vector(raw_schema['_columns']):\n            gdb.write(\"    {:27} id={} ordinal_id={} {} {} is_atomic={} is_counter={}\\n\".format(\n                    str(cdef['kind']),\n                    cdef['id'],\n                    int(cdef['ordinal_id']),\n                    cdef['_name'],\n                    seastar_shared_ptr(cdef['type']).get().dereference()['_name'],\n                    cdef['_is_atomic'],\n                    cdef['_is_counter']))\n\n\nclass scylla_sstable_summary(gdb.Command):\n    \"\"\"Print content of sstable summary\n\n    Usage: scylla sstable-summary $sst\n\n    Where $sst has to be a sstables::sstable value or reference, or a\n    sstables::shared_sstable instance.\n\n    Example:\n\n        (gdb) scylla sstable-summary $sst\n        header: {min_index_interval = 128, size = 1, memory_size = 21, sampling_level = 128, size_at_full_sampling = 0}\n        first_key: 63617373616e647261\n        last_key: 63617373616e647261\n        [0]: {\n          token: 356242581507269238,\n          key: 63617373616e647261,\n          position: 0}\n\n    Keys are printed in the hexadecimal notation.\n    \"\"\"\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla sstable-summary', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n        self.inf = gdb.selected_inferior()\n\n    def to_hex(self, data, size):\n        return bytes(self.inf.read_memory(data, size)).hex()\n\n    def invoke(self, arg, for_tty):\n        arg = gdb.parse_and_eval(arg)\n        typ = arg.type.strip_typedefs()\n        if typ.name.startswith(\"seastar::lw_shared_ptr\"):\n            sst = seastar_lw_shared_ptr(arg).get().dereference()\n        else:\n            sst = arg\n        summary = seastar_lw_shared_ptr(sst['_components']['_value']).get().dereference()['summary']\n\n        gdb.write(\"header: {}\\n\".format(summary['header']))\n        gdb.write(\"first_key: {}\\n\".format(sstring(summary['first_key']['value'])))\n        gdb.write(\"last_key: {}\\n\".format(sstring(summary['last_key']['value'])))\n\n        entries = list(chunked_vector(summary['entries']))\n        for i in range(len(entries)):\n            e = entries[i]\n            gdb.write(\"[{}]: {{\\n  token: {},\\n  key: {},\\n  position: {}}}\\n\".format(i,\n                e['token']['_data'],\n                self.to_hex(e['key']['_M_str'], int(e['key']['_M_len'])),\n                e['position']))\n\n\nclass scylla_sstable_index_cache(gdb.Command):\n    \"\"\"Print content of sstable partition-index cache\n\n    Prints index pages present in the partition-cache. Promoted index is\n    not printed.\n\n    Usage: scylla sstable-index-cache $sst\n\n    Where $sst has to be a sstables::sstable value or reference, or a\n    sstables::shared_sstable instance.\n\n    Output format is:\n\n        [$summary_index] : [\n            { key: $partition_key, token: $token, position: $position_in_data_file }\n            ...\n        ]\n        ...\n\n    Example:\n\n        (gdb) scylla sstable-index-cache $sst\n        [0]: [\n          { key: 63617373616e647261, token: 356242581507269238, position: 0 }\n        ]\n        Total: 1 page(s) (1 loaded, 0 loading)\n\n    Keys are printed in the hexadecimal notation.\n    \"\"\"\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla sstable-index-cache', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n\n    def invoke(self, arg, for_tty):\n        arg = gdb.parse_and_eval(arg)\n        typ = arg.type\n        if typ.code == gdb.TYPE_CODE_PTR or typ.code == gdb.TYPE_CODE_REF or typ.code == gdb.TYPE_CODE_RVALUE_REF:\n            typ = typ.target().strip_typedefs()\n        if typ.name == \"sstables::partition_index_cache\":\n            cache = arg\n        else:\n            if typ.name.startswith(\"seastar::lw_shared_ptr\"):\n                sst = seastar_lw_shared_ptr(arg).get()\n            else:\n                sst = arg\n            cache = std_unique_ptr(sst['_index_cache']).get().dereference()\n        tree = bplus_tree(cache['_cache'])\n        loaded_pages = 0\n        loading_pages = 0\n        partition_index_page_type = gdb.lookup_type('sstables::partition_index_page')\n        for n in tree:\n            t = n['_page'].type\n            page = std_variant(n['_page'])\n            if page.index() == 0:\n                loading_pages += 1\n                continue\n            loaded_pages += 1\n            page = page.get_with_type(partition_index_page_type)\n            gdb.write(\"[{}]: [\\n\".format(int(n['_key'])))\n            for entry in chunked_managed_vector(page['_entries']):\n                entry = entry['_ptr'].dereference()['_value']\n                key = entry['_key']\n                token = std_optional(entry['_token'])\n                if token:\n                    token = str(int(token.get()['_data']))\n                else:\n                    token = 'null'\n                position = int(entry['_position'])\n\n                gdb.write(\"  {{ key: {}, token: {}, position: {} }}\\n\".format(key, token, position))\n            gdb.write(']\\n')\n\n        gdb.write(\"Total: {} page(s) ({} loaded, {} loading)\\n\".format(loaded_pages + loading_pages, loaded_pages, loading_pages))\n\n\nclass cached_file:\n    def __init__(self, file):\n        self.file = file\n        self.pages = dict((int(page['idx']), page['_lsa_buf']['_buf']) for page in bplus_tree(file['_cache']))\n        self.page_size = 4096\n\n    def has_page(self, page):\n        return page in self.pages\n\n    def read(self, offset, size):\n        page = offset // self.page_size\n        page_offset = offset % self.page_size\n        if page_offset + size > self.page_size:\n            size1 = self.page_size - page_offset\n            return self.read(offset, size1) + self.read(offset + size1, size - size1)\n        return self.get_page(page)[page_offset:page_offset + size]\n\n    def get_page(self, page_idx):\n        inf = gdb.selected_inferior()\n        return bytes(inf.read_memory(self.pages[page_idx], self.page_size))\n\n\nclass scylla_sstable_promoted_index(gdb.Command):\n    \"\"\"\n    Prints promoted index contents using cached index file contents\n\n    Usage:\n\n        $1 = (sstables::cached_promoted_index*) 0x7f7f7f7f7f7f\n        (gdb) scylla sstable-promoted-index *$1\n\n    \"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla sstable-promoted-index', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n\n    def invoke(self, arg, for_tty):\n        pi = gdb.parse_and_eval(arg)\n\n        pi_start = int(pi['_promoted_index_start'])\n\n        offsets_end = pi_start + int(pi['_promoted_index_size'])\n        offsets_start = offsets_end - int(pi['_blocks_count']) * 4\n        cf = cached_file(pi['_cached_file'])\n\n        idx = 0\n        offsets = dict()\n        for off in range(offsets_start, offsets_end, 4):\n            try:\n                offset_bytes = cf.read(off, 4)\n                offsets[idx] = int.from_bytes(offset_bytes, byteorder='big')\n            except KeyError:\n                pass\n            idx += 1\n\n        last_block = int(pi['_blocks_count']) - 1\n        for i, off in offsets.items():\n            if i == last_block:\n                end = offsets_end\n            elif not i + 1 in offsets:\n                continue\n            else:\n                end = offsets[i + 1]\n            try:\n                idx_off = pi_start + off\n                buf = cf.read(idx_off, end - off)\n                gdb.write(f'[{i}] offset={off} {buf.hex()}\\n')\n            except KeyError:\n                pass\n\n\nclass scylla_sstable_dump_cached_index(gdb.Command):\n    \"\"\"\n    Dumps cached index contents to a file named \"index.dump\"\n    Uncached pages will contain zeros.\n    \"\"\"\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla sstable-dump-cached-index', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n\n    def invoke(self, arg, for_tty):\n        pi = gdb.parse_and_eval(arg)\n        cf = cached_file(pi['_cached_file'])\n\n        with open('./index.dump', 'wb') as file:\n            for i in cf.pages.keys():\n                file.seek(i * cf.page_size)\n                file.write(cf.get_page(i))\n\n\nclass permit_stats:\n    def __init__(self, *args):\n        if len(args) == 2:\n            self.permits = 1\n            self.resource_count = args[0]\n            self.resource_memory = args[1]\n        elif len(args) == 0: # default constructor\n            self.permits = 0\n            self.resource_count = 0\n            self.resource_memory = 0\n        else:\n            raise TypeError(\"permit_stats.__init__(): expected 0 or 2 arguments, got {}\".format(len(args)))\n\n    def add(self, o):\n        self.permits += o.permits\n        self.resource_count += o.resource_count\n        self.resource_memory += o.resource_memory\n\n\nclass scylla_read_stats(gdb.Command):\n    \"\"\"Summarize all active reads for the given semaphores.\n\n    The command accepts multiple semaphores as arguments to summarize reads\n    from. If no semaphores are provided, the command uses the semaphores\n    from the local database instance. Reads are discovered through the\n    reader_permit they own. A read is considered to be the group readers\n    that belong to the same permit. Semaphores, which have no associated\n    reads are omitted from the printout.\n\n    Example:\n    (gdb) scylla read-stats\n    Semaphore _read_concurrency_sem with: 1/100 count and 14334414/14302576 memory resources, queued: 0, inactive=1\n       permits count       memory table/description/state\n             1     1     14279738 multishard_mutation_query_test.fuzzy_test/fuzzy-test/active\n            16     0        53532 multishard_mutation_query_test.fuzzy_test/shard-reader/active\n             1     0         1144 multishard_mutation_query_test.fuzzy_test/shard-reader/inactive\n             1     0            0 *.*/view_builder/active\n             1     0            0 multishard_mutation_query_test.fuzzy_test/multishard-mutation-query/active\n            20     1     14334414 Total\n    \"\"\"\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla read-stats', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    @staticmethod\n    def dump_reads_from_semaphore(semaphore):\n        try:\n            permit_list = semaphore['_permit_list']\n        except gdb.error:\n            gdb.write(\"Scylla version doesn't seem to have the permits linked yet, cannot list reads.\")\n            raise\n\n        state_prefix_len = len('reader_permit::state::')\n\n        # (table, description, state) -> stats\n        permit_summaries = defaultdict(permit_stats)\n        total = permit_stats()\n\n        for permit in intrusive_list(permit_list):\n            try:\n                schema = permit['_schema']['_p']\n            except:\n                # schema is already a raw pointer in older versions\n                schema = permit['_schema']\n\n            if schema:\n                raw_schema = schema.dereference()['_raw']\n                schema_name = \"{}.{}\".format(str(raw_schema['_ks_name']).replace('\"', ''), str(raw_schema['_cf_name']).replace('\"', ''))\n            else:\n                schema_name = \"*.*\"\n\n            description = str(permit['_op_name_view'])[1:-1]\n            state = str(permit['_state'])[state_prefix_len:]\n            summary = permit_stats(int(permit['_resources']['count']), int(permit['_resources']['memory']))\n\n            permit_summaries[(schema_name, description, state)].add(summary)\n            total.add(summary)\n\n        if not permit_summaries:\n            return\n\n        semaphore_name = str(semaphore['_name'])[1:-1]\n        initial_count = int(semaphore['_initial_resources']['count'])\n        initial_memory = int(semaphore['_initial_resources']['memory'])\n        inactive_read_count = len(intrusive_list(semaphore['_inactive_reads']))\n        waiters = int(semaphore[\"_stats\"][\"waiters\"])\n\n        gdb.write(\"Semaphore {} with: {}/{} count and {}/{} memory resources, queued: {}, inactive={}\\n\".format(\n                semaphore_name,\n                initial_count - int(semaphore['_resources']['count']), initial_count,\n                initial_memory - int(semaphore['_resources']['memory']), initial_memory,\n                waiters, inactive_read_count))\n\n        gdb.write(\"{:>10} {:5} {:>12} {}\\n\".format('permits', 'count', 'memory', 'table/description/state'))\n\n        permit_summaries_sorted = [(t, d, s, v) for (t, d, s), v in permit_summaries.items()]\n        permit_summaries_sorted.sort(key=lambda x: x[3].resource_memory, reverse=True)\n\n        for table, description, state, stats in permit_summaries_sorted:\n            gdb.write(\"{:10} {:5} {:12} {}/{}/{}\\n\".format(stats.permits, stats.resource_count, stats.resource_memory, table, description, state))\n\n        gdb.write(\"{:10} {:5} {:12} Total\\n\".format(total.permits, total.resource_count, total.resource_memory))\n\n    def invoke(self, args, from_tty):\n        if args:\n            semaphores = [gdb.parse_and_eval(arg) for arg in args.split(' ')]\n        else:\n            db = find_db()\n            semaphores = [db[\"_streaming_concurrency_sem\"], db[\"_system_read_concurrency_sem\"]]\n            semaphores.append(db[\"_compaction_concurrency_sem\"])\n            try:\n                semaphores += [weighted_sem[\"sem\"] for (_, weighted_sem) in unordered_map(db[\"_reader_concurrency_semaphores_group\"][\"_semaphores\"])]\n            except gdb.error:\n                # compatibility with code before per-scheduling-group semaphore\n                pass\n            try:\n                semaphores += [weighted_sem[\"sem\"] for (_, weighted_sem) in unordered_map(db[\"_view_update_read_concurrency_semaphores_group\"][\"_semaphores\"])]\n            except gdb.error:\n                # 2024.2 compatibility\n                pass\n\n        for semaphore in semaphores:\n            scylla_read_stats.dump_reads_from_semaphore(semaphore)\n\n\nclass scylla_get_config_value(gdb.Command):\n    \"\"\"Obtain the config item and print its value and metadata\n\n    Example:\n        (gdb) scylla get-config-value compaction_static_shares\n        value: 100, type: \"float\", source: SettingsFile, status: Used, live: MustRestart\n    \"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla get-config-value', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    def invoke(self, cfg_item_name, from_tty):\n        shard_id = current_shard()\n        db = find_db(shard_id)\n        cfg = db['_cfg']\n        try:\n            cfg_item = cfg[cfg_item_name]\n        except gdb.error:\n            gdb.write(\"Failed to obtain config item with name {}: {}\\n\".format(cfg_item_name, sys.exc_info()[1]))\n            return\n\n        cfg_type = cfg_item['_type']\n        cfg_src = cfg_item['_source']\n        cfg_status = cfg_item['_value_status']\n        cfg_liveness = cfg_item['_liveness']\n        cfg_value = std_vector(cfg_item['_cf']['_per_shard_values'])[shard_id]\n        cfg_value = std_vector(cfg_value)[int(cfg_item['_per_shard_values_offset'])]\n        cfg_value = std_unique_ptr(cfg_value).get()\n        cfg_value = downcast_vptr(cfg_value)['value']['_value']\n\n        def unqualify(name):\n            name = str(name)\n            return name.split('::')[-1]\n\n        gdb.write(\"value: {}, type: {}, source: {}, status: {}, live: {}\\n\".format(\n            cfg_value,\n            cfg_type['_name'],\n            unqualify(cfg_src),\n            unqualify(cfg_status),\n            unqualify(cfg_liveness)))\n\n\nclass scylla_range_tombstones(gdb.Command):\n    \"\"\"Prints and validates range tombstones in a given container.\n\n    Currently supported containers:\n        - mutation_partition\n\n    Example:\n\n        (gdb) scylla range-tombstones $mp\n        {\n          start: ['a', 'b'],\n          kind: bound_kind::excl_start,\n          end: ['a', 'b'],\n          kind: bound_kind::incl_end,\n          t: {timestamp = 1672546889091665, deletion_time = {__d = {__r = 1672546889}}}\n        }\n        {\n          start: ['a', 'b'],\n          kind: bound_kind::excl_start,\n          end: ['a', 'c']\n          kind: bound_kind::incl_end,\n          t: {timestamp = 1673731764010123, deletion_time = {__d = {__r = 1673731764}}}\n        }\n\n    \"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla range-tombstones', gdb.COMMAND_USER, gdb.COMPLETE_COMMAND)\n\n    \"\"\"value is an instance of clustering_key\"\"\"\n    def parse_clustering(self, value):\n        return get_current_schema().parse_clustering_key(value)\n\n    def invoke(self, partition, from_tty):\n        p = gdb.parse_and_eval(partition)\n\n        prev_end = None\n        for r in intrusive_set(p['_row_tombstones']['_tombstones']):\n            t = r['_tombstone']\n            start = self.parse_clustering(t['start'])\n            end = self.parse_clustering(t['end'])\n            gdb.write('{\\n  start: %s,\\n  kind: %s,\\n  end: %s,\\n  kind: %s,\\n  t: %s\\n}\\n' % (\n                start, t['start_kind'], end, t['end_kind'], t['tomb']))\n\n            # Detect crossed boundaries or empty ranges\n            if start > end or (start == end and (str(t['start_kind']) != 'bound_kind::incl_start'\n                                            or str(t['end_kind']) != 'bound_kind::incl_end')):\n                gdb.write('*** Invalid boundaries! ***\\n')\n                return\n\n            # Detect out of order ranges\n            if prev_end:\n                if start < prev_end or \\\n                        (start == prev_end and (str(prev['end_kind']) == 'bound_kind::incl_end'\n                                                and str(t['start_kind']) == 'bound_kind::incl_start')):\n                    gdb.write('*** Order violated! ***\\n')\n                    return\n            prev_end = end\n            prev = t\n\n\nclass scylla_gdb_func_dereference_smart_ptr(gdb.Function):\n    \"\"\"Dereference the pointer guarded by the smart pointer instance.\n\n    Supported smart pointers are:\n    * std::unique_ptr\n    * seastar::lw_shared_ptr\n    * seastar::foreign_ptr [1]\n\n    [1] Note that `seastar::foreign_ptr` wraps another smart pointer type which\n    is also dereferenced so its type has to be supported.\n\n    Usage:\n    $dereference_smart_ptr($ptr)\n\n    Where:\n    $ptr - a convenience variable or any gdb expression that evaluates\n        to a smart pointer instance of a supported type.\n\n    Returns:\n    The value pointed to by the guarded pointer.\n\n    Example:\n    (gdb) p $1._read_context\n    $2 = {_p = 0x60b00b068600}\n    (gdb) p $dereference_smart_ptr($1._read_context)\n    $3 = {<seastar::enable_lw_shared_from_this<cache::read_context>> = {<seastar::lw_shared_ptr_counter_base> = {_count = 1}, ...\n    \"\"\"\n\n    def __init__(self):\n        super(scylla_gdb_func_dereference_smart_ptr, self).__init__('dereference_smart_ptr')\n\n    def invoke(self, expr):\n        if isinstance(expr, gdb.Value):\n            ptr = expr\n        else:\n            ptr = gdb.parse_and_eval(expr)\n\n        typ = ptr.type.strip_typedefs()\n        if typ.name.startswith('seastar::shared_ptr<'):\n            return ptr['_p'].dereference()\n        if typ.name.startswith('seastar::lw_shared_ptr<'):\n            return seastar_lw_shared_ptr(ptr).get().dereference()\n        elif typ.name.startswith('seastar::foreign_ptr<'):\n            return self.invoke(ptr['_value'])\n        elif typ.name.startswith('std::unique_ptr<'):\n            return std_unique_ptr(ptr).get().dereference()\n\n        raise ValueError(\"Unsupported smart pointer type: {}\".format(typ.name))\n\n\nclass scylla_gdb_func_downcast_vptr(gdb.Function):\n    \"\"\"Downcast a ptr to a virtual object to a ptr of the actual object\n\n    Usage:\n    $downcast_vptr($ptr)\n\n    Where:\n    $ptr - an integer literal, a convenience variable or any gdb\n        expression that evaluates to an pointer, which points to an\n        virtual object.\n\n    Returns:\n    The pointer to the actual concrete object.\n\n    Example:\n    (gdb) p $1\n    $2 = (mutation_reader::impl *) 0x60b03363b900\n    (gdb) p $downcast_vptr(0x60b03363b900)\n    $3 = (combined_mutation_reader *) 0x60b03363b900\n    # The return value can also be dereferenced on the spot.\n    (gdb) p *$downcast_vptr($1)\n    $4 = {<mutation_reader::impl> = {_vptr.impl = 0x46a3ea8 <vtable for combined_mutation_reader+16>, _buffer = {_impl = {<std::allocator<mutation_fragment>> = ...\n    \"\"\"\n\n    def __init__(self):\n        super(scylla_gdb_func_downcast_vptr, self).__init__('downcast_vptr')\n\n    def invoke(self, ptr):\n        return downcast_vptr(ptr)\n\nclass reference_wrapper:\n    def __init__(self, ref):\n        self.ref = ref\n\n    def get(self):\n        return self.ref['_M_data']\n\n\nclass scylla_features(gdb.Command):\n    \"\"\"Prints state of Scylla gossiper features on current shard.\n\n    Example:\n    (gdb) scylla features\n    \"LWT\": false\n    \"HINTED_HANDOFF_SEPARATE_CONNECTION\": true\n    \"NONFROZEN_UDTS\": true\n    \"XXHASH\": true\n    \"CORRECT_COUNTER_ORDER\": true\n    \"WRITE_FAILURE_REPLY\": true\n    \"CDC\": false\n    \"CORRECT_NON_COMPOUND_RANGE_TOMBSTONES\": true\n    \"RANGE_TOMBSTONES\": true\n    \"VIEW_VIRTUAL_COLUMNS\": true\n    \"SCHEMA_TABLES_V3\": true\n    \"LARGE_PARTITIONS\": true\n    \"UDF\": false\n    \"INDEXES\": true\n    \"MATERIALIZED_VIEWS\": true\n    \"DIGEST_MULTIPARTITION_READ\": true\n    \"COUNTERS\": true\n    \"UNBOUNDED_RANGE_TOMBSTONES\": true\n    \"ROLES\": true\n    \"LA_SSTABLE_FORMAT\": true\n    \"MC_SSTABLE_FORMAT\": true\n    \"MD_SSTABLE_FORMAT\": true\n    \"STREAM_WITH_RPC_STREAM\": true\n    \"ROW_LEVEL_REPAIR\": true\n    \"TRUNCATION_TABLE\": true\n    \"CORRECT_STATIC_COMPACT_IN_MC\": true\n    \"DIGEST_INSENSITIVE_TO_EXPIRY\": true\n    \"COMPUTED_COLUMNS\": true\n    \"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla features', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n\n    def invoke(self, arg, for_tty):\n        try:\n            feature_service = sharded(gdb.parse_and_eval('debug::the_feature_service')).local()\n        except Exception as e:\n            gossiper = sharded(gdb.parse_and_eval('gms::_the_gossiper')).local()\n            feature_service = gossiper['_feature_service']\n\n        for (name, f) in unordered_map(feature_service['_registered_features']):\n            f = reference_wrapper(f).get()\n            gdb.write('%s: %s\\n' % (f['_name'], f['_enabled']))\n\nclass scylla_repairs(gdb.Command):\n    \"\"\" List all active repair instances for both repair masters and followers.\n\n    Example:\n\n       (repair_meta*) for masters: addr = 0x600005abf830, table = myks2.standard1, ip = 127.0.0.1, states = ['127.0.0.1->repair_state::get_sync_boundary_started', '127.0.0.3->repair_state::get_sync_boundary_finished'], repair_meta = (repair_meta*) 0x60400af3f8e0\n       (repair_meta*) for masters: addr = 0x60000521f830, table = myks2.standard1, ip = 127.0.0.1, states = ['127.0.0.1->repair_state::get_sync_boundary_started', '127.0.0.2->repair_state::get_sync_boundary_started'], repair_meta = (repair_meta*) 0x6040103df8e0\n       (repair_meta*) for follower: addr = 0x60000432a808, table = myks2.standard1, ip = 127.0.0.1, states = ['127.0.0.1->repair_state::get_sync_boundary_started', '127.0.0.2->repair_state::unknown'], repair_meta = (repair_meta*) 0x60400d73f8e0\n    \"\"\"\n\n    def __init__(self):\n        gdb.Command.__init__(self, 'scylla repairs', gdb.COMMAND_USER, gdb.COMPLETE_NONE, True)\n\n    def process(self, master, rm):\n        schema = rm['_schema']\n        table = schema_ptr(schema).table_name().replace('\"', '')\n        all_nodes_state = []\n        ip = str(rm['_myip']).replace('\"', '')\n        for n in std_vector(rm['_all_node_states']):\n            all_nodes_state.append(str(n['node']).replace('\"', '') + \"->\" + str(n['state']))\n        gdb.write('(%s*) for %s: addr = %s, table = %s, ip = %s, states = %s, repair_meta = (repair_meta*) %s\\n' % (rm.type, master, str(rm.address), table, ip, all_nodes_state, rm.address))\n\n    def invoke(self, arg, for_tty):\n        for rm in intrusive_list(gdb.parse_and_eval('debug::repair_meta_for_masters._repair_metas'), link='_tracker_link'):\n            self.process(\"masters\", rm)\n        for rm in intrusive_list(gdb.parse_and_eval('debug::repair_meta_for_followers._repair_metas'), link='_tracker_link'):\n            self.process(\"follower\", rm)\n\nclass scylla_gdb_func_collection_element(gdb.Function):\n    \"\"\"Return the element at the specified index/key from the container.\n\n    Usage:\n    $collection_element($col, $key)\n\n    Where:\n    $col - a variable, or an expression that evaluates to a value of any\n    supported container type.\n    $key - a literal, or an expression that evaluates to an index/key type\n    appropriate for the container.\n\n    Supported container types are:\n    * std::vector<> - key must be integer\n    * std::list<> - key must be integer\n    \"\"\"\n    def __init__(self):\n        super(scylla_gdb_func_collection_element, self).__init__('collection_element')\n\n    def invoke(self, collection, key):\n        typ = collection.type.strip_typedefs()\n        if typ.code == gdb.TYPE_CODE_PTR or typ.code == gdb.TYPE_CODE_REF or typ.code == gdb.TYPE_CODE_RVALUE_REF:\n            typ = typ.target().strip_typedefs()\n        if typ.name.startswith('std::vector<'):\n            return std_vector(collection)[int(key)]\n        elif typ.name.startswith('std::tuple<'):\n            return std_tuple(collection)[int(key)]\n        elif typ.name.startswith('std::__cxx11::list<'):\n            return std_list(collection)[int(key)]\n        elif typ.name.startswith('seastar::circular_buffer<'):\n            return circular_buffer(collection)[int(key)]\n        elif typ.name.startswith('boost::intrusive::list<'):\n            return list(intrusive_list(collection))[int(key)]\n\n        raise ValueError(\"Unsupported container type: {}\".format(typ.name))\n\n\nclass scylla_gdb_func_sharded_local(gdb.Function):\n    \"\"\"Get the local instance of a sharded object\n\n    Usage:\n    $sharded_local($obj)\n\n    Where:\n    $obj - a variable, or an expression that evaluates to any `seastar::sharded`\n    instance.\n\n    Example:\n    (gdb) p $sharded_local(cql3::_the_query_processor)\n    $1 = (cql3::query_processor *) 0x6350001f2390\n    \"\"\"\n\n    def __init__(self):\n        super(scylla_gdb_func_sharded_local, self).__init__('sharded_local')\n\n    def invoke(self, obj):\n        return sharded(obj).local()\n\n\nclass scylla_gdb_func_variant_member(gdb.Function):\n    \"\"\"Get the active member of an std::variant\n\n    Usage:\n    $variant_member($obj)\n\n    Where:\n    $obj - a variable, or an expression that evaluates to any `std::variant`\n    instance.\n\n    Example:\n    # $1 = (cql3::raw_value_view *) 0x6060365a7a50\n    (gdb) p &$variant_member($1->_data)\n    $2 = (cql3::null_value *) 0x6060365a7a50\n    \"\"\"\n\n    def __init__(self):\n        super(scylla_gdb_func_variant_member, self).__init__('variant_member')\n\n    def invoke(self, obj):\n        return std_variant(obj).get()\n\nclass scylla_gdb_func_coro_frame(gdb.Function):\n    \"\"\"Given a seastar::task* pointing to a coroutine task, convert it to a pointer to the coroutine frame.\n\n    Usage:\n    $coro_frame($ptr)\n\n    Where:\n    $ptr - any expression that evaluates to an integer. It will be interpreted as seastar::task*.\n\n    Returns:\n    The (typed) pointer to the coroutine frame.\n\n    Example:\n\n    1. Print the task chain:\n\n    (gdb) scylla fiber seastar::local_engine->_current_task\n    [shard  1] #0  (task*) 0x0000601008e8e970 0x000000000047aae0 vtable for seastar::internal::coroutine_traits_base<void>::promise_type + 16  (sstables::parse<unsigned int, std::pair<sstables::metadata_type, unsigned int> >(schema const&, sstables::sstable_version_types, sstables::random_access_reader&, sstables::disk_array<unsigned int, std::pair<sstables::metadata_type, unsigned int> >&) at sstables/sstables.cc:352)\n    [shard  1] #1  (task*) 0x00006010092acf10 0x000000000047aae0 vtable for seastar::internal::coroutine_traits_base<void>::promise_type + 16  (sstables::parse(schema const&, sstables::sstable_version_types, sstables::random_access_reader&, sstables::statistics&) at sstables/sstables.cc:570)\n    ...\n\n    2. Examine the coroutine frame of task #1:\n\n    (gdb) p *$coro_frame(0x00006010092acf10)\n    $1 = {\n      __resume_fn = 0x2485f80 <sstables::parse(schema const&, sstables::sstable_version_types, sstables::random_access_reader&, sstables::statistics&)>,\n      ...\n      PointerType_7 = 0x601008e67880,\n      PointerType_8 = 0x601008e64900,\n      ...\n      __int_32_23 = 4,\n      ...\n      __coro_index = 0 '\\000'\n      ...\n\n    3. Examine coroutine arguments.\n    (Needs some know-how. They should be at lowest-indexed PointerType* or __int* entries, depending on their type. Not so easy to automate):\n\n    (gdb) p $downcast_vptr($coro_frame(0x00006010092acf10)->PointerType_7)\n    $2 = (schema *) 0x601008e67880\n    (gdb) p (sstables::sstable_version_types)($coro_frame(0x00006010092acf10)->__int_32_23)\n    $3 = sstables::sstable_version_types::me\n    (gdb) p $downcast_vptr($coro_frame(0x00006010092acf10)->PointerType_8)\n    $4 = (sstables::file_random_access_reader *) 0x601008e64900\n    \"\"\"\n\n    def __init__(self):\n        super(scylla_gdb_func_coro_frame, self).__init__('coro_frame')\n\n    def invoke(self, ptr_raw):\n        vptr_type = gdb.lookup_type('uintptr_t').pointer()\n        ptr = gdb.Value(ptr_raw).reinterpret_cast(vptr_type)\n        maybe_vptr = int(ptr.dereference())\n\n        # Validate that the task is a coroutine.\n        with gdb.with_parameter(\"print demangle\", \"on\"):\n            symname = gdb.execute(f\"info symbol {maybe_vptr}\", False, True)\n        if not symname.startswith('vtable for seastar::internal::coroutine_traits_base'):\n            gdb.write(f\"0x{maybe_vptr:x} does not seem to point to a coroutine task.\")\n            return None\n\n        # The promise object starts on the third `uintptr_t` in the frame.\n        # The resume_fn pointer is the first `uintptr_t`.\n        # So if the task is a coroutine, we should be able to find the resume function via offsetting by -2.\n        # AFAIK both major compilers respect this convention.\n        block = gdb.block_for_pc((ptr - 2).dereference())\n\n        # Look up the coroutine frame type.\n        # I don't understand why, but gdb has problems looking up the coro_frame_ty type if demangling is enabled.\n        with gdb.with_parameter(\"demangle-style\", \"none\"):\n            coro_ty = gdb.lookup_type(f\"{block.function.linkage_name}.coro_frame_ty\").pointer()\n\n        return (ptr - 2).cast(coro_ty)\n\n# Commands\nscylla()\nscylla_databases()\nscylla_commitlog()\nscylla_keyspaces()\nscylla_tables()\nscylla_memory()\nscylla_ptr()\nscylla_mem_ranges()\nscylla_mem_range()\nscylla_heapprof()\nscylla_lsa()\nscylla_lsa_segment()\nscylla_lsa_check()\nscylla_segment_descs()\nscylla_timers()\nscylla_apply()\nscylla_shard()\nscylla_thread()\nscylla_unthread()\nscylla_threads()\nscylla_task_stats()\nscylla_tasks()\nscylla_task_queues()\nscylla_io_queues()\nscylla_fiber()\nscylla_find()\nscylla_task_histogram()\nscylla_active_sstables()\nscylla_netw()\nscylla_gms()\nscylla_cache()\nscylla_sstable_summary()\nscylla_sstable_index_cache()\nscylla_sstables()\nscylla_memtables()\nscylla_generate_object_graph()\nscylla_smp_queues()\nscylla_features()\nscylla_repairs()\nscylla_small_objects()\nscylla_large_objects()\nscylla_compaction_tasks()\nscylla_set_schema()\nscylla_schema()\nscylla_read_stats()\nscylla_get_config_value()\nscylla_range_tombstones()\nscylla_sstable_promoted_index()\nscylla_sstable_dump_cached_index()\n\n\n# Convenience functions\n#\n# List them inside `gdb` with\n#   (gdb) help function\n#\n# To get the usage of an individual function:\n#   (gdb) help function $function_name\nscylla_gdb_func_dereference_smart_ptr()\nscylla_gdb_func_downcast_vptr()\nscylla_gdb_func_coro_frame()\nscylla_gdb_func_collection_element()\nscylla_gdb_func_sharded_local()\nscylla_gdb_func_variant_member()\n\ngdb.execute('set language c++')\n"
        },
        {
          "name": "scylla_post_install.sh",
          "type": "blob",
          "size": 1.6767578125,
          "content": "#!/bin/bash\n#\n# Copyright (C) 2019-present ScyllaDB\n#\n\n#\n# SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n#\n\nif [ ! -d /run/systemd/system ]; then\n    exit 0\nfi\n\nversion_ge() {\n    [  \"$2\" = \"`echo -e \"$1\\n$2\" | sort -V | head -n1`\" ]\n}\n\n# Install capabilities.conf when AmbientCapabilities supported\n. /etc/os-release\n\nKERNEL_VER=$(uname -r)\n\n# CAP_PERFMON is only available on linux-5.8+\nif version_ge $KERNEL_VER 5.8; then\n    AMB_CAPABILITIES=\"$AMB_CAPABILITIES CAP_PERFMON\"\n    mkdir -p /etc/systemd/system/scylla-server.service.d/\n    cat << EOS > /etc/systemd/system/scylla-server.service.d/capabilities.conf\n[Service]\nAmbientCapabilities=CAP_PERFMON\nEOS\nfi\n\n# For systems with not a lot of memory, override default reservations for the slices\n# seastar has a minimum reservation of 1.5GB that kicks in, and 21GB * 0.07 = 1.5GB.\n# So for anything smaller than that we will not use percentages in the helper slice\nMEMTOTAL=$(cat /proc/meminfo |grep -e \"^MemTotal:\"|sed -s 's/^MemTotal:\\s*\\([0-9]*\\) kB$/\\1/')\nMEMTOTAL_BYTES=$(($MEMTOTAL * 1024))\nif [ $MEMTOTAL_BYTES -lt 23008753371 ]; then\n    mkdir -p /etc/systemd/system/scylla-helper.slice.d/\n    cat << EOS > /etc/systemd/system/scylla-helper.slice.d/memory.conf\n[Slice]\nMemoryHigh=1200M\nMemoryMax=1400M\nEOS\nfi\n\nif [ -e /etc/systemd/system/systemd-coredump@.service.d/timeout.conf ]; then\n    COREDUMP_RUNTIME_MAX=$(grep RuntimeMaxSec /etc/systemd/system/systemd-coredump@.service.d/timeout.conf)\n    if [ -z $COREDUMP_RUNTIME_MAX ]; then\n    cat << EOS > /etc/systemd/system/systemd-coredump@.service.d/timeout.conf\n[Service]\nRuntimeMaxSec=infinity\nTimeoutSec=infinity\nEOS\n    fi\nfi\n\nsystemctl --system daemon-reload >/dev/null || true\n"
        },
        {
          "name": "seastar",
          "type": "commit",
          "content": null
        },
        {
          "name": "seastarx.hh",
          "type": "blob",
          "size": 0.349609375,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\nnamespace seastar {\n\ntemplate <typename T>\nclass shared_ptr;\n\ntemplate <typename T, typename... A>\nshared_ptr<T> make_shared(A&&... a);\n\n}\n\nusing namespace seastar;\nusing seastar::shared_ptr;\nusing seastar::make_shared;\n"
        },
        {
          "name": "serialization_visitors.hh",
          "type": "blob",
          "size": 5.244140625,
          "content": "/*\n * Copyright 2016 ScylaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n#pragma once\n\n#include \"bytes_ostream.hh\"\n#include \"serializer.hh\"\n\nnamespace ser {\n\n// frame represents a place holder for object size which will be known later\n\ntemplate<typename Output>\nstruct place_holder { };\n\ntemplate<typename Output>\nstruct frame { };\n\ntemplate<>\nstruct place_holder<bytes_ostream> {\n    bytes_ostream::place_holder<size_type> ph;\n\n    place_holder(bytes_ostream::place_holder<size_type> ph) : ph(ph) { }\n\n    void set(bytes_ostream& out, size_type v) {\n        auto stream = ph.get_stream();\n        serialize(stream, v);\n    }\n};\n\ntemplate<>\nstruct frame<bytes_ostream> : public place_holder<bytes_ostream> {\n    bytes_ostream::size_type offset;\n\n    frame(bytes_ostream::place_holder<size_type> ph, bytes_ostream::size_type offset)\n        : place_holder(ph), offset(offset) { }\n\n    void end(bytes_ostream& out) {\n        set(out, out.size() - offset);\n    }\n};\n\nstruct vector_position {\n    bytes_ostream::position pos;\n    size_type count;\n};\n\n//empty frame, behave like a place holder, but is used when no place holder is needed\ntemplate<typename Output>\nstruct empty_frame {\n    void end(Output&) {}\n    empty_frame() = default;\n    empty_frame(const frame<Output>&){}\n};\n\ninline place_holder<bytes_ostream> start_place_holder(bytes_ostream& out) {\n    auto size_ph = out.write_place_holder<size_type>();\n    return { size_ph};\n}\n\ninline frame<bytes_ostream> start_frame(bytes_ostream& out) {\n    auto offset = out.size();\n    auto size_ph = out.write_place_holder<size_type>();\n    {\n        auto out = size_ph.get_stream();\n        serialize(out, (size_type)0);\n    }\n    return frame<bytes_ostream> { size_ph, offset };\n}\n\ntemplate<typename Input>\nsize_type read_frame_size(Input& in) {\n    auto sz = deserialize(in, std::type_identity<size_type>());\n    if (sz < sizeof(size_type)) {\n        throw std::runtime_error(fmt::format(\"IDL frame truncated: expected to have at least {} bytes, got {}\", sizeof(size_type), sz));\n    }\n    return sz - sizeof(size_type);\n}\n\n\ntemplate<>\nstruct place_holder<seastar::measuring_output_stream> {\n    void set(seastar::measuring_output_stream&, size_type) { }\n};\n\ntemplate<>\nstruct frame<seastar::measuring_output_stream> : public place_holder<seastar::measuring_output_stream> {\n    void end(seastar::measuring_output_stream& out) { }\n};\n\ninline place_holder<seastar::measuring_output_stream> start_place_holder(seastar::measuring_output_stream& out) {\n    serialize(out, size_type());\n    return { };\n}\n\ninline frame<seastar::measuring_output_stream> start_frame(seastar::measuring_output_stream& out) {\n    serialize(out, size_type());\n    return { };\n}\n\ntemplate<>\nclass place_holder<seastar::simple_output_stream> {\n    seastar::simple_output_stream _substream;\npublic:\n    place_holder(seastar::simple_output_stream substream)\n        : _substream(substream) { }\n\n    void set(seastar::simple_output_stream& out, size_type v) {\n        serialize(_substream, v);\n    }\n};\n\ntemplate<>\nclass frame<seastar::simple_output_stream> : public place_holder<seastar::simple_output_stream> {\n    char* _start;\npublic:\n    frame(seastar::simple_output_stream ph, char* start)\n        : place_holder(ph), _start(start) { }\n\n    void end(seastar::simple_output_stream& out) {\n        set(out, out.begin() - _start);\n    }\n};\n\ninline place_holder<seastar::simple_output_stream> start_place_holder(seastar::simple_output_stream& out) {\n    return { out.write_substream(sizeof(size_type)) };\n}\n\ninline frame<seastar::simple_output_stream> start_frame(seastar::simple_output_stream& out) {\n    auto start = out.begin();\n    auto substream = out.write_substream(sizeof(size_type));\n    {\n        auto sstr = substream;\n        serialize(sstr, size_type(0));\n    }\n    return frame<seastar::simple_output_stream>(substream, start);\n}\n\ntemplate<typename Iterator>\nclass place_holder<seastar::memory_output_stream<Iterator>> {\n    seastar::memory_output_stream<Iterator> _substream;\npublic:\n    place_holder(seastar::memory_output_stream<Iterator> substream)\n        : _substream(substream) { }\n\n    void set(seastar::memory_output_stream<Iterator>& out, size_type v) {\n        serialize(_substream, v);\n    }\n};\n\ntemplate<typename Iterator>\nclass frame<seastar::memory_output_stream<Iterator>> : public place_holder<seastar::memory_output_stream<Iterator>> {\n    size_t _start_left;\npublic:\n    frame(seastar::memory_output_stream<Iterator> ph, size_t start_left)\n        : place_holder<seastar::memory_output_stream<Iterator>>(ph), _start_left(start_left) { }\n\n    void end(seastar::memory_output_stream<Iterator>& out) {\n        this->set(out, _start_left - out.size());\n    }\n};\n\ntemplate<typename Iterator>\ninline place_holder<seastar::memory_output_stream<Iterator>> start_place_holder(seastar::memory_output_stream<Iterator>& out) {\n    return { out.write_substream(sizeof(size_type)) };\n}\n\ntemplate<typename Iterator>\ninline frame<seastar::memory_output_stream<Iterator>> start_frame(seastar::memory_output_stream<Iterator>& out) {\n    auto start_left = out.size();\n    auto substream = out.write_substream(sizeof(size_type));\n    {\n        auto sstr = substream;\n        serialize(sstr, size_type(0));\n    }\n    return frame<seastar::memory_output_stream<Iterator>>(substream, start_left);\n}\n\n}\n"
        },
        {
          "name": "serializer.cc",
          "type": "blob",
          "size": 0.8154296875,
          "content": "/*\n * Copyright (C) 2021-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"serializer_impl.hh\"\n\nnamespace ser {\n\nlogging::logger serlog(\"serializer\");\n\n} // namespace ser\n\nnamespace utils {\n\nmanaged_bytes_view\nbuffer_view_to_managed_bytes_view(ser::buffer_view<bytes_ostream::fragment_iterator> bv) {\n    auto impl = bv.extract_implementation();\n    return build_managed_bytes_view_from_internals(\n            impl.current,\n            impl.next.extract_implementation().current_chunk,\n            impl.size\n    );\n}\n\nmanaged_bytes_view_opt\nbuffer_view_to_managed_bytes_view(std::optional<ser::buffer_view<bytes_ostream::fragment_iterator>> bvo) {\n    if (!bvo) {\n        return std::nullopt;\n    }\n    return buffer_view_to_managed_bytes_view(*bvo);\n}\n\n\n} // namespace utils\n"
        },
        {
          "name": "serializer.hh",
          "type": "blob",
          "size": 11.4482421875,
          "content": "/*\n * Copyright 2016-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n#pragma once\n\n#include <seastar/core/sstring.hh>\n#include <optional>\n#include \"utils/assert.hh\"\n#include \"utils/managed_bytes.hh\"\n#include \"bytes_ostream.hh\"\n#include <seastar/core/simple-stream.hh>\n#include \"boost/variant/variant.hpp\"\n#include \"bytes_ostream.hh\"\n#include \"utils/fragment_range.hh\"\n#include <variant>\n\n#include <type_traits>\n\nnamespace ser {\n\n/// A fragmented view of an opaque buffer in a stream of serialised data\n///\n/// This class allows reading large, fragmented blobs serialised by the IDL\n/// infrastructure without linearising or copying them. The view remains valid\n/// as long as the underlying IDL-serialised buffer is alive.\n///\n/// Satisfies FragmentRange concept.\ntemplate<typename FragmentIterator>\nclass buffer_view {\n    bytes_view _first;\n    size_t _total_size;\n    FragmentIterator _next;\npublic:\n    using fragment_type = bytes_view;\n\n    struct implementation {\n        bytes_view current;\n        FragmentIterator next;\n        size_t size;\n    };\n\n    class iterator {\n        bytes_view _current;\n        size_t _left = 0;\n        FragmentIterator _next;\n    public:\n        using iterator_category\t= std::input_iterator_tag;\n        using value_type = bytes_view;\n        using pointer = const bytes_view*;\n        using reference = const bytes_view&;\n        using difference_type = std::ptrdiff_t;\n\n        iterator() = default;\n        iterator(bytes_view current, size_t left, FragmentIterator next)\n            : _current(current), _left(left), _next(next) { }\n\n        bytes_view operator*() const {\n            return _current;\n        }\n        const bytes_view* operator->() const {\n            return &_current;\n        }\n\n        iterator& operator++() {\n            _left -= _current.size();\n            if (_left) {\n                auto next_view = bytes_view(reinterpret_cast<const bytes::value_type*>((*_next).begin()),\n                                            (*_next).size());\n                auto next_size = std::min(_left, next_view.size());\n                _current = bytes_view(next_view.data(), next_size);\n                ++_next;\n            }\n            return *this;\n        }\n        iterator operator++(int) {\n            iterator it(*this);\n            operator++();\n            return it;\n        }\n\n        bool operator==(const iterator& other) const {\n            return _left == other._left;\n        }\n    };\n    using const_iterator = iterator;\n\n    explicit buffer_view(bytes_view current)\n        : _first(current), _total_size(current.size()) { }\n\n    buffer_view(bytes_view current, size_t size, FragmentIterator it)\n        : _first(current), _total_size(size), _next(it)\n    {\n        if (_first.size() > _total_size) {\n            _first.remove_suffix(_first.size() - _total_size);\n        }\n    }\n\n    explicit buffer_view(typename seastar::memory_input_stream<FragmentIterator>::simple stream)\n        : buffer_view(bytes_view(reinterpret_cast<const int8_t*>(stream.begin()), stream.size()))\n    { }\n\n    explicit buffer_view(typename seastar::memory_input_stream<FragmentIterator>::fragmented stream)\n        : buffer_view(bytes_view(reinterpret_cast<const int8_t*>(stream.first_fragment_data()), stream.first_fragment_size()),\n                      stream.size(), stream.fragment_iterator())\n    { }\n\n    iterator begin() const {\n        return iterator(_first, _total_size, _next);\n    }\n    iterator end() const {\n        return iterator();\n    }\n\n    size_t size_bytes() const {\n        return _total_size;\n    }\n    bool empty() const {\n        return !_total_size;\n    }\n\n    // FragmentedView implementation\n    void remove_prefix(size_t n) {\n        while (n >= _first.size() && n > 0) {\n            n -= _first.size();\n            remove_current();\n        }\n        _total_size -= n;\n        _first.remove_prefix(n);\n    }\n    void remove_current() {\n        _total_size -= _first.size();\n        if (_total_size) {\n            auto next_data = reinterpret_cast<const bytes::value_type*>((*_next).begin());\n            size_t next_size = std::min(_total_size, (*_next).size());\n            _first = bytes_view(next_data, next_size);\n            ++_next;\n        } else {\n            _first = bytes_view();\n        }\n    }\n    buffer_view prefix(size_t n) const {\n        auto tmp = *this;\n        tmp._total_size = std::min(tmp._total_size, n);\n        tmp._first = tmp._first.substr(0, n);\n        return tmp;\n    }\n    bytes_view current_fragment() {\n        return _first;\n    }\n\n    bytes linearize() const {\n        bytes b(bytes::initialized_later(), size_bytes());\n        auto dst = b.begin();\n        for (bytes_view fragment : *this) {\n            dst = std::copy(fragment.begin(), fragment.end(), dst);\n        }\n        return b;\n    }\n\n    template<typename Function>\n    decltype(auto) with_linearized(Function&& fn) const\n    {\n        bytes b;\n        bytes_view bv;\n        if (_first.size() != _total_size) {\n            b = linearize();\n            bv = b;\n        } else {\n            bv = _first;\n        }\n        return fn(bv);\n    }\n\n    implementation extract_implementation() const {\n        return implementation {\n            .current = _first,\n            .next = _next,\n            .size = _total_size,\n        };\n    }\n};\nstatic_assert(FragmentedView<buffer_view<bytes_ostream::fragment_iterator>>);\n\nusing size_type = uint32_t;\n\ntemplate<typename T, typename Input>\nrequires std::is_integral_v<T>\ninline T deserialize_integral(Input& input) {\n    T data;\n    input.read(reinterpret_cast<char*>(&data), sizeof(T));\n    return le_to_cpu(data);\n}\n\ntemplate<typename T, typename Output>\nrequires std::is_integral_v<T>\ninline void serialize_integral(Output& output, T data) {\n    data = cpu_to_le(data);\n    output.write(reinterpret_cast<const char*>(&data), sizeof(T));\n}\n\ntemplate<typename T>\nstruct serializer;\n\ntemplate<typename T>\nstruct integral_serializer {\n    template<typename Input>\n    static T read(Input& v) {\n        return deserialize_integral<T>(v);\n    }\n    template<typename Output>\n    static void write(Output& out, T v) {\n        serialize_integral(out, v);\n    }\n    template<typename Input>\n    static void skip(Input& v) {\n        read(v);\n    }\n};\n\ntemplate<> struct serializer<bool> {\n    template <typename Input>\n    static bool read(Input& i) {\n        return deserialize_integral<uint8_t>(i);\n    }\n    template< typename Output>\n    static void write(Output& out, bool v) {\n        serialize_integral(out, uint8_t(v));\n    }\n    template <typename Input>\n    static void skip(Input& i) {\n        read(i);\n    }\n\n};\ntemplate<> struct serializer<int8_t> : public integral_serializer<int8_t> {};\ntemplate<> struct serializer<uint8_t> : public integral_serializer<uint8_t> {};\ntemplate<> struct serializer<int16_t> : public integral_serializer<int16_t> {};\ntemplate<> struct serializer<uint16_t> : public integral_serializer<uint16_t> {};\ntemplate<> struct serializer<int32_t> : public integral_serializer<int32_t> {};\ntemplate<> struct serializer<uint32_t> : public integral_serializer<uint32_t> {};\ntemplate<> struct serializer<int64_t> : public integral_serializer<int64_t> {};\ntemplate<> struct serializer<uint64_t> : public integral_serializer<uint64_t> {};\n\ntemplate<typename Output>\nvoid safe_serialize_as_uint32(Output& output, uint64_t data);\n\ntemplate<typename T, typename Output>\ninline void serialize(Output& out, const T& v) {\n    serializer<T>::write(out, v);\n};\n\ntemplate<typename T, typename Output>\ninline void serialize(Output& out, const std::reference_wrapper<T> v) {\n    serializer<T>::write(out, v.get());\n}\n\ntemplate<typename T, typename Input>\ninline auto deserialize(Input& in, std::type_identity<T> t) {\n    return serializer<T>::read(in);\n}\n\ntemplate<typename T, typename Input>\ninline void skip(Input& v, std::type_identity<T>) {\n    return serializer<T>::skip(v);\n}\n\ntemplate<typename T>\nsize_type get_sizeof(const T& obj);\n\ntemplate<typename T>\nvoid set_size(seastar::measuring_output_stream& os, const T& obj);\n\ntemplate<typename Stream, typename T>\nvoid set_size(Stream& os, const T& obj);\n\ntemplate<typename Buffer, typename T>\nBuffer serialize_to_buffer(const T& v, size_t head_space = 0);\n\ntemplate<typename T, typename Buffer>\nT deserialize_from_buffer(const Buffer&, std::type_identity<T>, size_t head_space = 0);\n\ntemplate<typename Output, typename ...T>\nvoid serialize(Output& out, const boost::variant<T...>& v);\n\ntemplate<typename Input, typename ...T>\nboost::variant<T...> deserialize(Input& in, std::type_identity<boost::variant<T...>>);\n\ntemplate<typename Output, typename ...T>\nvoid serialize(Output& out, const std::variant<T...>& v);\n\ntemplate<typename Input, typename ...T>\nstd::variant<T...> deserialize(Input& in, std::type_identity<std::variant<T...>>);\n\nstruct unknown_variant_type {\n    size_type index;\n    sstring data;\n};\n\ntemplate<typename Output>\nvoid serialize(Output& out, const unknown_variant_type& v);\n\ntemplate<typename Input>\nunknown_variant_type deserialize(Input& in, std::type_identity<unknown_variant_type>);\n\ntemplate <typename T>\nstruct normalize {\n    using type = T;\n};\n\ntemplate <>\nstruct normalize<bytes_view> {\n     using type = bytes;\n};\n\ntemplate <>\nstruct normalize<managed_bytes> {\n     using type = bytes;\n};\n\ntemplate <>\nstruct normalize<bytes_ostream> {\n    using type = bytes;\n};\n\ntemplate <typename T, typename U>\nstruct is_equivalent : std::is_same<typename normalize<std::remove_const_t<std::remove_reference_t<T>>>::type, typename normalize<std::remove_const_t <std::remove_reference_t<U>>>::type> {\n};\n\ntemplate <typename T, typename U>\nstruct is_equivalent<std::reference_wrapper<T>, U> : is_equivalent<T, U> {\n};\n\ntemplate <typename T, typename U>\nstruct is_equivalent<T, std::reference_wrapper<U>> : is_equivalent<T, U> {\n};\n\ntemplate <typename T, typename U>\nstruct is_equivalent<std::optional<T>, std::optional<U>> : is_equivalent<T, U> {\n};\n\ntemplate <typename T, typename U, bool>\nstruct is_equivalent_arity;\n\ntemplate <typename ...T, typename ...U>\nstruct is_equivalent_arity<std::tuple<T...>, std::tuple<U...>, false> : std::false_type {\n};\n\ntemplate <typename ...T, typename ...U>\nstruct is_equivalent_arity<std::tuple<T...>, std::tuple<U...>, true> {\n    static constexpr bool value = (is_equivalent<T, U>::value && ...);\n};\n\ntemplate <typename ...T, typename ...U>\nstruct is_equivalent<std::tuple<T...>, std::tuple<U...>> : is_equivalent_arity<std::tuple<T...>, std::tuple<U...>, sizeof...(T) == sizeof...(U)> {\n};\n\ntemplate <typename ...T, typename ...U>\nstruct is_equivalent<std::variant<T...>, std::variant<U...>> : is_equivalent<std::tuple<T...>, std::tuple<U...>> {\n};\n\n// gc_clock duration values were serialized as 32-bit prior to 3.1, and\n// are serialized as 64-bit in 3.1.0.\n//\n// TTL values are capped to 20 years, which fits into 32 bits, so\n// truncation is not a concern.\n\ninline bool gc_clock_using_3_1_0_serialization = false;\n\ntemplate <typename Output>\nvoid\nserialize_gc_clock_duration_value(Output& out, int64_t v) {\n    if (!gc_clock_using_3_1_0_serialization) {\n        // This should have been caught by the CQL layer, so this is just\n        // for extra safety.\n        SCYLLA_ASSERT(int32_t(v) == v);\n        serializer<int32_t>::write(out, v);\n    } else {\n        serializer<int64_t>::write(out, v);\n    }\n}\n\ntemplate <typename Input>\nint64_t\ndeserialize_gc_clock_duration_value(Input& in) {\n    if (!gc_clock_using_3_1_0_serialization) {\n        return serializer<int32_t>::read(in);\n    } else {\n        return serializer<int64_t>::read(in);\n    }\n}\n\n}\n\n/*\n * Import the auto generated forward declaration code\n */\n"
        },
        {
          "name": "serializer_impl.hh",
          "type": "blob",
          "size": 30.140625,
          "content": "/*\n * Copyright 2016-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <list>\n#include <unordered_set>\n\n#include \"serializer.hh\"\n#include \"enum_set.hh\"\n#include \"utils/chunked_vector.hh\"\n#include \"utils/input_stream.hh\"\n#include <seastar/util/bool_class.hh>\n#include \"utils/small_vector.hh\"\n#include <absl/container/btree_set.h>\n#include <seastar/core/shared_ptr.hh>\n#include <seastar/core/temporary_buffer.hh>\n#include <seastar/core/on_internal_error.hh>\n#include \"utils/log.hh\"\n\nnamespace seastar {\nextern logger seastar_logger;\n}\n\nnamespace ser {\n\ntemplate<typename T>\nvoid set_size(seastar::measuring_output_stream& os, const T& obj) {\n    serialize(os, uint32_t(0));\n}\n\ntemplate<typename Stream, typename T>\nvoid set_size(Stream& os, const T& obj) {\n    serialize(os, get_sizeof(obj));\n}\n\n\ntemplate<typename Output>\nvoid safe_serialize_as_uint32(Output& out, uint64_t data) {\n    if (data > std::numeric_limits<uint32_t>::max()) {\n        throw std::runtime_error(\"Size is too big for serialization\");\n    }\n    serialize(out, uint32_t(data));\n}\n\ntemplate<typename T>\nconstexpr bool can_serialize_fast() {\n    return !std::is_same<T, bool>::value && std::is_integral<T>::value && (sizeof(T) == 1 || __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__);\n}\n\ntemplate<bool Fast, typename T>\nstruct serialize_array_helper;\n\ntemplate<typename T>\nstruct serialize_array_helper<true, T> {\n    template<typename Container, typename Output>\n    static void doit(Output& out, const Container& v) {\n        out.write(reinterpret_cast<const char*>(v.data()), v.size() * sizeof(T));\n    }\n};\n\ntemplate<typename T>\nstruct serialize_array_helper<false, T> {\n    template<typename Container, typename Output>\n    static void doit(Output& out, const Container& v) {\n        for (auto&& e : v) {\n            serialize(out, e);\n        }\n    }\n};\n\ntemplate<typename T, typename Container, typename Output>\ninline void serialize_array(Output& out, const Container& v) {\n    serialize_array_helper<can_serialize_fast<T>(), T>::doit(out, v);\n}\n\ntemplate<typename Container>\nstruct container_traits;\n\ntemplate<typename T>\nstruct container_traits<absl::btree_set<T>> {\n    struct back_emplacer {\n        absl::btree_set<T>& c;\n        back_emplacer(absl::btree_set<T>& c_) : c(c_) {}\n        void operator()(T&& v) {\n            c.emplace(std::move(v));\n        }\n    };\n};\n\ntemplate<typename T, typename... Args>\nstruct container_traits<std::unordered_set<T, Args...>> {\n    struct back_emplacer {\n        std::unordered_set<T, Args...>& c;\n        back_emplacer(std::unordered_set<T, Args...>& c_) : c(c_) {}\n        void operator()(T&& v) {\n            c.emplace(std::move(v));\n        }\n    };\n};\n\ntemplate<typename T>\nstruct container_traits<std::list<T>> {\n    struct back_emplacer {\n        std::list<T>& c;\n        back_emplacer(std::list<T>& c_) : c(c_) {}\n        void operator()(T&& v) {\n            c.emplace_back(std::move(v));\n        }\n    };\n    void resize(std::list<T>& c, size_t size) {\n        c.resize(size);\n    }\n};\n\ntemplate<typename T>\nstruct container_traits<std::vector<T>> {\n    struct back_emplacer {\n        std::vector<T>& c;\n        back_emplacer(std::vector<T>& c_) : c(c_) {}\n        void operator()(T&& v) {\n            c.emplace_back(std::move(v));\n        }\n    };\n    void resize(std::vector<T>& c, size_t size) {\n        c.resize(size);\n    }\n};\n\ntemplate<typename T, size_t N>\nstruct container_traits<utils::small_vector<T, N>> {\n    struct back_emplacer {\n        utils::small_vector<T, N>& c;\n        back_emplacer(utils::small_vector<T, N>& c_) : c(c_) {}\n        void operator()(T&& v) {\n            c.emplace_back(std::move(v));\n        }\n    };\n    void resize(utils::small_vector<T, N>& c, size_t size) {\n        c.resize(size);\n    }\n};\n\ntemplate<typename T>\nstruct container_traits<utils::chunked_vector<T>> {\n    struct back_emplacer {\n        utils::chunked_vector<T>& c;\n        back_emplacer(utils::chunked_vector<T>& c_) : c(c_) {}\n        void operator()(T&& v) {\n            c.emplace_back(std::move(v));\n        }\n    };\n    void resize(utils::chunked_vector<T>& c, size_t size) {\n        c.resize(size);\n    }\n};\n\ntemplate<typename T, size_t N>\nstruct container_traits<std::array<T, N>> {\n    struct back_emplacer {\n        std::array<T, N>& c;\n        size_t idx = 0;\n        back_emplacer(std::array<T, N>& c_) : c(c_) {}\n        void operator()(T&& v) {\n            c[idx++] = std::move(v);\n        }\n    };\n    void resize(std::array<T, N>& c, size_t size) {}\n};\n\ntemplate<bool Fast, typename T>\nstruct deserialize_array_helper;\n\ntemplate<typename T>\nstruct deserialize_array_helper<true, T> {\n    template<typename Input, typename Container>\n    static void doit(Input& in, Container& v, size_t sz) {\n        container_traits<Container> t;\n        t.resize(v, sz);\n        in.read(reinterpret_cast<char*>(v.data()), v.size() * sizeof(T));\n    }\n    template<typename Input>\n    static void skip(Input& in, size_t sz) {\n        in.skip(sz * sizeof(T));\n    }\n};\n\ntemplate<typename T>\nstruct deserialize_array_helper<false, T> {\n    template<typename Input, typename Container>\n    static void doit(Input& in, Container& v, size_t sz) {\n        typename container_traits<Container>::back_emplacer be(v);\n        while (sz--) {\n            be(deserialize(in, std::type_identity<T>()));\n        }\n    }\n    template<typename Input>\n    static void skip(Input& in, size_t sz) {\n        while (sz--) {\n            serializer<T>::skip(in);\n        }\n    }\n};\n\ntemplate<typename T, typename Input, typename Container>\ninline void deserialize_array(Input& in, Container& v, size_t sz) {\n    deserialize_array_helper<can_serialize_fast<T>(), T>::doit(in, v, sz);\n}\n\ntemplate<typename T, typename Input>\ninline void skip_array(Input& in, size_t sz) {\n    deserialize_array_helper<can_serialize_fast<T>(), T>::skip(in, sz);\n}\n\nnamespace idl::serializers::internal {\n\ntemplate<typename Vector>\nstruct vector_serializer {\n    using value_type = typename Vector::value_type;\n    template<typename Input>\n    static Vector read(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        Vector v;\n        v.reserve(sz);\n        deserialize_array<value_type>(in, v, sz);\n        return v;\n    }\n    template<typename Output>\n    static void write(Output& out, const Vector& v) {\n        safe_serialize_as_uint32(out, v.size());\n        serialize_array<value_type>(out, v);\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        skip_array<value_type>(in, sz);\n    }\n};\n\n}\n\ntemplate<typename T>\nstruct serializer<std::list<T>> {\n    template<typename Input>\n    static std::list<T> read(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        std::list<T> v;\n        deserialize_array_helper<false, T>::doit(in, v, sz);\n        return v;\n    }\n    template<typename Output>\n    static void write(Output& out, const std::list<T>& v) {\n        safe_serialize_as_uint32(out, v.size());\n        serialize_array_helper<false, T>::doit(out, v);\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        skip_array<T>(in, sz);\n    }\n};\n\ntemplate<typename T>\nstruct serializer<absl::btree_set<T>> {\n    template<typename Input>\n    static absl::btree_set<T> read(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        absl::btree_set<T> v;\n        deserialize_array_helper<false, T>::doit(in, v, sz);\n        return v;\n    }\n    template<typename Output>\n    static void write(Output& out, const absl::btree_set<T>& v) {\n        safe_serialize_as_uint32(out, v.size());\n        serialize_array_helper<false, T>::doit(out, v);\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        skip_array<T>(in, sz);\n    }\n};\n\ntemplate<typename T, typename... Args>\nstruct serializer<std::unordered_set<T, Args...>> {\n    template<typename Input>\n    static std::unordered_set<T, Args...> read(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        std::unordered_set<T, Args...> v;\n        v.reserve(sz);\n        deserialize_array_helper<false, T>::doit(in, v, sz);\n        return v;\n    }\n    template<typename Output>\n    static void write(Output& out, const std::unordered_set<T, Args...>& v) {\n        safe_serialize_as_uint32(out, v.size());\n        serialize_array_helper<false, T>::doit(out, v);\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        skip_array<T>(in, sz);\n    }\n};\n\ntemplate<typename T>\nstruct serializer<std::vector<T>>\n    : idl::serializers::internal::vector_serializer<std::vector<T>>\n{ };\n\ntemplate<typename T>\nstruct serializer<utils::chunked_vector<T>>\n    : idl::serializers::internal::vector_serializer<utils::chunked_vector<T>>\n{ };\n\ntemplate<typename T, size_t N>\nstruct serializer<utils::small_vector<T, N>>\n    : idl::serializers::internal::vector_serializer<utils::small_vector<T, N>>\n{ };\n\ntemplate<typename T, typename Ratio>\nstruct serializer<std::chrono::duration<T, Ratio>> {\n    template<typename Input>\n    static std::chrono::duration<T, Ratio> read(Input& in) {\n        return std::chrono::duration<T, Ratio>(deserialize(in, std::type_identity<T>()));\n    }\n    template<typename Output>\n    static void write(Output& out, const std::chrono::duration<T, Ratio>& d) {\n        serialize(out, d.count());\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        read(in);\n    }\n};\n\ntemplate<typename Clock, typename Duration>\nstruct serializer<std::chrono::time_point<Clock, Duration>> {\n    using value_type = std::chrono::time_point<Clock, Duration>;\n\n    template<typename Input>\n    static value_type read(Input& in) {\n        return typename Clock::time_point(Duration(deserialize(in, std::type_identity<uint64_t>())));\n    }\n    template<typename Output>\n    static void write(Output& out, const value_type& v) {\n        serialize(out, uint64_t(v.time_since_epoch().count()));\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        read(in);\n    }\n};\n\ntemplate<size_t N, typename T>\nstruct serializer<std::array<T, N>> {\n    template<typename Input>\n    static std::array<T, N> read(Input& in) {\n        std::array<T, N> v;\n        deserialize_array<T>(in, v, N);\n        return v;\n    }\n    template<typename Output>\n    static void write(Output& out, const std::array<T, N>& v) {\n        serialize_array<T>(out, v);\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        skip_array<T>(in, N);\n    }\n};\n\ntemplate<typename K, typename V>\nstruct serializer<std::map<K, V>> {\n    template<typename Input>\n    static std::map<K, V> read(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        std::map<K, V> m;\n        while (sz--) {\n            K k = deserialize(in, std::type_identity<K>());\n            V v = deserialize(in, std::type_identity<V>());\n            m[k] = v;\n        }\n        return m;\n    }\n    template<typename Output>\n    static void write(Output& out, const std::map<K, V>& v) {\n        safe_serialize_as_uint32(out, v.size());\n        for (auto&& e : v) {\n            serialize(out, e.first);\n            serialize(out, e.second);\n        }\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        while (sz--) {\n            serializer<K>::skip(in);\n            serializer<V>::skip(in);\n        }\n    }\n};\n\ntemplate<typename K, typename V>\nstruct serializer<std::unordered_map<K, V>> {\n    template<typename Input>\n    static std::unordered_map<K, V> read(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        std::unordered_map<K, V> m;\n        m.reserve(sz);\n        while (sz--) {\n            auto k = deserialize(in, std::type_identity<K>());\n            auto v = deserialize(in, std::type_identity<V>());\n            m.emplace(std::move(k), std::move(v));\n        }\n        return m;\n    }\n    template<typename Output>\n    static void write(Output& out, const std::unordered_map<K, V>& v) {\n        safe_serialize_as_uint32(out, v.size());\n        for (auto&& e : v) {\n            serialize(out, e.first);\n            serialize(out, e.second);\n        }\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        while (sz--) {\n            serializer<K>::skip(in);\n            serializer<V>::skip(in);\n        }\n    }\n};\n\ntemplate<typename Tag>\nstruct serializer<bool_class<Tag>> {\n    template<typename Input>\n    static bool_class<Tag> read(Input& in) {\n        return bool_class<Tag>(deserialize(in, std::type_identity<bool>()));\n    }\n\n    template<typename Output>\n    static void write(Output& out, bool_class<Tag> v) {\n        serialize(out, bool(v));\n    }\n\n    template<typename Input>\n    static void skip(Input& in) {\n        read(in);\n    }\n};\n\ntemplate<typename Stream>\nclass deserialized_bytes_proxy {\n    Stream _stream;\n\n    template<typename OtherStream>\n    friend class deserialized_bytes_proxy;\npublic:\n    explicit deserialized_bytes_proxy(Stream stream)\n        : _stream(std::move(stream)) { }\n\n    template<typename OtherStream>\n    requires std::convertible_to<OtherStream, Stream>\n    deserialized_bytes_proxy(deserialized_bytes_proxy<OtherStream> proxy)\n        : _stream(std::move(proxy._stream)) { }\n\n    auto view() const {\n      if constexpr (std::is_same_v<Stream, simple_input_stream>) {\n        return bytes_view(reinterpret_cast<const int8_t*>(_stream.begin()), _stream.size());\n      } else {\n        using iterator_type = typename Stream::iterator_type ;\n        static_assert(FragmentRange<buffer_view<iterator_type>>);\n        return seastar::with_serialized_stream(_stream, seastar::make_visitor(\n            [&] (typename seastar::memory_input_stream<iterator_type >::simple stream) {\n                return buffer_view<iterator_type>(bytes_view(reinterpret_cast<const int8_t*>(stream.begin()),\n                                                        stream.size()));\n            },\n            [&] (typename seastar::memory_input_stream<iterator_type >::fragmented stream) {\n                return buffer_view<iterator_type>(bytes_view(reinterpret_cast<const int8_t*>(stream.first_fragment_data()),\n                                                        stream.first_fragment_size()),\n                                             stream.size(), stream.fragment_iterator());\n            }\n        ));\n      }\n    }\n\n    [[gnu::always_inline]]\n    operator bytes() && {\n        bytes v(bytes::initialized_later(), _stream.size());\n        _stream.read(reinterpret_cast<char*>(v.begin()), _stream.size());\n        return v;\n    }\n\n    [[gnu::always_inline]]\n    operator managed_bytes() && {\n        managed_bytes mb(managed_bytes::initialized_later(), _stream.size());\n        for (bytes_mutable_view frag : fragment_range(managed_bytes_mutable_view(mb))) {\n            _stream.read(reinterpret_cast<char*>(frag.data()), frag.size());\n        }\n        return mb;\n    }\n\n    [[gnu::always_inline]]\n    operator bytes_ostream() && {\n        bytes_ostream v;\n        _stream.copy_to(v);\n        return v;\n    }\n};\n\ntemplate<>\nstruct serializer<temporary_buffer<char>> {\n    template<typename Input>\n    static temporary_buffer<char> read(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n\t\tauto v = temporary_buffer<char>(sz);\n        in.read(reinterpret_cast<char*>(v.get_write()), sz);\n        return v;\n    }\n    template<typename Output>\n    static void write(Output& out, const temporary_buffer<char>& v) {\n        safe_serialize_as_uint32(out, uint32_t(v.size()));\n        out.write(reinterpret_cast<const char*>(v.get()), v.size());\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        in.skip(sz);\n    }\n};\n\n\ntemplate<>\nstruct serializer<bytes> {\n    template<typename Input>\n    static deserialized_bytes_proxy<Input> read(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        return deserialized_bytes_proxy<Input>(in.read_substream(sz));\n    }\n    template<typename Output>\n    static void write(Output& out, bytes_view v) {\n        safe_serialize_as_uint32(out, uint32_t(v.size()));\n        out.write(reinterpret_cast<const char*>(v.begin()), v.size());\n    }\n    template<typename Output>\n    static void write(Output& out, const bytes& v) {\n        write(out, static_cast<bytes_view>(v));\n    }\n    template<typename Output>\n    static void write(Output& out, const managed_bytes& mb) {\n        safe_serialize_as_uint32(out, uint32_t(mb.size()));\n        for (bytes_view frag : fragment_range(managed_bytes_view(mb))) {\n            out.write(reinterpret_cast<const char*>(frag.data()), frag.size());\n        }\n    }\n    template<typename Output>\n    static void write(Output& out, const bytes_ostream& v) {\n        safe_serialize_as_uint32(out, uint32_t(v.size()));\n        for (bytes_view frag : v.fragments()) {\n            out.write(reinterpret_cast<const char*>(frag.begin()), frag.size());\n        }\n    }\n    template<typename Output, typename FragmentedBuffer>\n    requires FragmentRange<FragmentedBuffer>\n    static void write_fragmented(Output& out, FragmentedBuffer&& fragments) {\n        safe_serialize_as_uint32(out, uint32_t(fragments.size_bytes()));\n        for (bytes_view frag : fragments) {\n            out.write(reinterpret_cast<const char*>(frag.begin()), frag.size());\n        }\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        in.skip(sz);\n    }\n};\n\ntemplate<typename Output>\nvoid serialize(Output& out, const bytes_view& v) {\n    serializer<bytes>::write(out, v);\n}\ntemplate<typename Output>\nvoid serialize(Output& out, const managed_bytes& v) {\n    serializer<bytes>::write(out, v);\n}\ntemplate<typename Output>\nvoid serialize(Output& out, const bytes_ostream& v) {\n    serializer<bytes>::write(out, v);\n}\ntemplate<typename Input>\nbytes_ostream deserialize(Input& in, std::type_identity<bytes_ostream>) {\n    return serializer<bytes>::read(in);\n}\ntemplate<typename Output, typename FragmentedBuffer>\nrequires FragmentRange<FragmentedBuffer>\nvoid serialize_fragmented(Output& out, FragmentedBuffer&& v) {\n    serializer<bytes>::write_fragmented(out, std::forward<FragmentedBuffer>(v));\n}\n\ntemplate<typename T>\nstruct serializer<std::optional<T>> {\n    template<typename Input>\n    static std::optional<T> read(Input& in) {\n        std::optional<T> v;\n        auto b = deserialize(in, std::type_identity<bool>());\n        if (b) {\n            v.emplace(deserialize(in, std::type_identity<T>()));\n        }\n        return v;\n    }\n    template<typename Output>\n    static void write(Output& out, const std::optional<T>& v) {\n        serialize(out, bool(v));\n        if (v) {\n            serialize(out, v.value());\n        }\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        auto present = deserialize(in, std::type_identity<bool>());\n        if (present) {\n            serializer<T>::skip(in);\n        }\n    }\n};\n\nextern logging::logger serlog;\n\n// Warning: assumes that pointer is never null\ntemplate<typename T>\nstruct serializer<seastar::lw_shared_ptr<T>> {\n    template<typename Input>\n    static seastar::lw_shared_ptr<T> read(Input& in) {\n        return seastar::make_lw_shared<T>(deserialize(in, std::type_identity<T>()));\n    }\n    template<typename Output>\n    static void write(Output& out, const seastar::lw_shared_ptr<T>& v) {\n        if (!v) {\n            on_internal_error(serlog, \"Unexpected nullptr while serializing a pointer\");\n        }\n        serialize(out, *v);\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        serializer<T>::skip(in);\n    }\n};\n\ntemplate<>\nstruct serializer<sstring> {\n    template<typename Input>\n    static sstring read(Input& in) {\n        auto sz = deserialize(in, std::type_identity<uint32_t>());\n        sstring v = uninitialized_string(sz);\n        in.read(v.data(), sz);\n        return v;\n    }\n    template<typename Output>\n    static void write(Output& out, const sstring& v) {\n        safe_serialize_as_uint32(out, uint32_t(v.size()));\n        out.write(v.data(), v.size());\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        in.skip(deserialize(in, std::type_identity<size_type>()));\n    }\n};\n\ntemplate<typename T>\nstruct serializer<std::unique_ptr<T>> {\n    template<typename Input>\n    static std::unique_ptr<T> read(Input& in) {\n        std::unique_ptr<T> v;\n        auto b = deserialize(in, std::type_identity<bool>());\n        if (b) {\n            v = std::make_unique<T>(deserialize(in, std::type_identity<T>()));\n        }\n        return v;\n    }\n    template<typename Output>\n    static void write(Output& out, const std::unique_ptr<T>& v) {\n        serialize(out, bool(v));\n        if (v) {\n            serialize(out, *v);\n        }\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        auto present = deserialize(in, std::type_identity<bool>());\n        if (present) {\n            serializer<T>::skip(in);\n        }\n    }\n};\n\ntemplate<typename Enum>\nstruct serializer<enum_set<Enum>> {\n    template<typename Input>\n    static enum_set<Enum> read(Input& in) {\n        return enum_set<Enum>::from_mask(deserialize(in, std::type_identity<uint64_t>()));\n    }\n    template<typename Output>\n    static void write(Output& out, enum_set<Enum> v) {\n        serialize(out, uint64_t(v.mask()));\n    }\n    template<typename Input>\n    static void skip(Input& in) {\n        read(in);\n    }\n};\n\ntemplate<>\nstruct serializer<std::monostate> {\n    template<typename Input>\n    static std::monostate read(Input& in) {\n        return std::monostate{};\n    }\n    template<typename Output>\n    static void write(Output& out, std::monostate v) {}\n    template<typename Input>\n    static void skip(Input& in) {\n    }\n};\n\ntemplate<typename T>\nsize_type get_sizeof(const T& obj) {\n    seastar::measuring_output_stream ms;\n    serialize(ms, obj);\n    auto size = ms.size();\n    if (size > std::numeric_limits<size_type>::max()) {\n        throw std::runtime_error(\"Object is too big for get_sizeof\");\n    }\n    return size;\n}\n\ntemplate<typename Buffer, typename T>\nBuffer serialize_to_buffer(const T& v, size_t head_space) {\n    seastar::measuring_output_stream measure;\n    ser::serialize(measure, v);\n    Buffer ret(typename Buffer::initialized_later(), measure.size() + head_space);\n    seastar::simple_output_stream out(reinterpret_cast<char*>(ret.begin()), ret.size(), head_space);\n    ser::serialize(out, v);\n    return ret;\n}\n\ntemplate<typename T, typename Buffer>\nT deserialize_from_buffer(const Buffer& buf, std::type_identity<T> type, size_t head_space) {\n    seastar::simple_input_stream in(reinterpret_cast<const char*>(buf.begin() + head_space), buf.size() - head_space);\n    return deserialize(in, std::move(type));\n}\n\ninline\nutils::input_stream as_input_stream(bytes_view b) {\n    return utils::input_stream::simple(reinterpret_cast<const char*>(b.begin()), b.size());\n}\n\ninline\nutils::input_stream as_input_stream(const bytes_ostream& b) {\n    if (b.is_linearized()) {\n        return as_input_stream(b.view());\n    }\n    return utils::input_stream::fragmented(b.fragments().begin(), b.size());\n}\n\ntemplate<typename Output, typename ...T>\nvoid serialize(Output& out, const boost::variant<T...>& v) {}\n\ntemplate<typename Input, typename ...T>\nboost::variant<T...> deserialize(Input& in, std::type_identity<boost::variant<T...>>) {\n    return boost::variant<T...>();\n}\n\ntemplate<typename Output, typename ...T>\nvoid serialize(Output& out, const std::variant<T...>& v) {\n    static_assert(std::variant_size_v<std::variant<T...>> < 256);\n    size_t type_index = v.index();\n    serialize(out, uint8_t(type_index));\n    std::visit([&out] (const auto& member) {\n        serialize(out, member);\n    }, v);\n}\n\ntemplate<typename Input, typename T, size_t... I>\nT deserialize_std_variant(Input& in, std::type_identity<T> t,  size_t idx, std::index_sequence<I...>) {\n    T v;\n    (void)((I == idx ? v.template emplace<I>(deserialize(in, std::type_identity<std::variant_alternative_t<I, T>>())), true : false) || ...);\n    return v;\n}\n\ntemplate<typename Input, typename ...T>\nstd::variant<T...> deserialize(Input& in, std::type_identity<std::variant<T...>> v) {\n    size_t idx = deserialize(in, std::type_identity<uint8_t>());\n    return deserialize_std_variant(in, v, idx, std::make_index_sequence<sizeof...(T)>());\n}\n\ntemplate<typename Output>\nvoid serialize(Output& out, const unknown_variant_type& v) {\n    out.write(v.data.begin(), v.data.size());\n}\ntemplate<typename Input>\nunknown_variant_type deserialize(Input& in, std::type_identity<unknown_variant_type>) {\n    return seastar::with_serialized_stream(in, [] (auto& in) {\n        auto size = deserialize(in, std::type_identity<size_type>());\n        auto index = deserialize(in, std::type_identity<size_type>());\n        auto sz = size - sizeof(size_type) * 2;\n        sstring v = uninitialized_string(sz);\n        in.read(v.data(), sz);\n        return unknown_variant_type{ index, std::move(v) };\n    });\n}\n\n// Class for iteratively deserializing a frozen vector\n// using a range.\n// Use begin() and end() to iterate through the frozen vector,\n// deserializing (or skipping) one element at a time.\ntemplate <typename T, bool IsForward=true>\nclass vector_deserializer {\npublic:\n    using value_type = T;\n    using input_stream = utils::input_stream;\n\nprivate:\n    input_stream _in;\n    size_t _size;\n    utils::chunked_vector<input_stream> _substreams;\n\n    void fill_substreams() requires (!IsForward) {\n        input_stream in = _in;\n        input_stream in2 = _in;\n        for (size_t i = 0; i < size(); ++i) {\n            size_t old_size = in.size();\n            serializer<T>::skip(in);\n            size_t new_size = in.size();\n\n            _substreams.push_back(in2.read_substream(old_size - new_size));\n        }\n    }\n\n    struct forward_iterator_data {\n        input_stream _in = simple_input_stream();\n        void skip() {\n            serializer<T>::skip(_in);\n        }\n        value_type deserialize_next() {\n            return deserialize(_in, std::type_identity<T>());\n        }\n    };\n    struct reverse_iterator_data {\n        std::reverse_iterator<utils::chunked_vector<input_stream>::const_iterator> _substream_it;\n        void skip() {\n            ++_substream_it;\n        }\n        value_type deserialize_next() {\n            input_stream is = *_substream_it;\n            ++_substream_it;\n            return deserialize(is, std::type_identity<T>());\n        }\n    };\n\n\npublic:\n    vector_deserializer() noexcept\n        : _in(simple_input_stream())\n        , _size(0)\n    { }\n\n    explicit vector_deserializer(input_stream in)\n        : _in(std::move(in))\n        , _size(deserialize(_in, std::type_identity<uint32_t>()))\n    {\n        if constexpr (!IsForward) {\n            fill_substreams();\n        }\n    }\n\n    // Get the number of items in the vector\n    size_t size() const noexcept {\n        return _size;\n    }\n\n    bool empty() const noexcept {\n        return _size == 0;\n    }\n\n    // Input iterator\n    class iterator {\n        // _idx is the distance from .begin(). It is used only for comparing iterators.\n        size_t _idx = 0;\n        bool _consumed = false;\n        std::conditional_t<IsForward, forward_iterator_data, reverse_iterator_data> _data;\n\n        iterator(input_stream in, size_t idx) noexcept requires(IsForward)\n            : _idx(idx)\n            , _data{in}\n        { }\n        iterator(decltype(reverse_iterator_data::_substream_it) substreams, size_t idx) noexcept requires(!IsForward)\n            : _idx(idx)\n            , _data{substreams}\n        { }\n\n        friend class vector_deserializer;\n   public:\n        using iterator_category = std::input_iterator_tag;\n        using value_type = T;\n        using pointer = value_type*;\n        using reference = value_type&;\n        using difference_type = ssize_t;\n\n        iterator() noexcept = default;\n\n        bool operator==(const iterator& it) const noexcept {\n            return _idx == it._idx;\n        }\n\n        // Deserializes and returns the item, effectively incrementing the iterator..\n        value_type operator*() const {\n            auto zis = const_cast<iterator*>(this);\n            zis->_idx++;\n            zis->_consumed = true;\n            return zis->_data.deserialize_next();\n        }\n\n        iterator& operator++() {\n            if (!_consumed) {\n                _data.skip();\n                ++_idx;\n            } else {\n                _consumed = false;\n            }\n            return *this;\n        }\n        iterator operator++(int) {\n            auto pre = *this;\n            ++*this;\n            return pre;\n        }\n\n        ssize_t operator-(const iterator& it) const noexcept {\n            return _idx - it._idx;\n        }\n    };\n\n    using const_iterator = iterator;\n\n    static_assert(std::input_iterator<iterator>);\n    static_assert(std::sentinel_for<iterator, iterator>);\n\n    iterator begin() noexcept requires(IsForward) {\n        return {_in, 0};\n    }\n    const_iterator begin() const noexcept requires(IsForward) {\n        return {_in, 0};\n    }\n    const_iterator cbegin() const noexcept requires(IsForward) {\n        return {_in, 0};\n    }\n\n    iterator end() noexcept requires(IsForward) {\n        return {_in, _size};\n    }\n    const_iterator end() const noexcept requires(IsForward) {\n        return {_in, _size};\n    }\n    const_iterator cend() const noexcept requires(IsForward) {\n        return {_in, _size};\n    }\n\n    iterator begin() noexcept requires(!IsForward) {\n        return {_substreams.crbegin(), 0};\n    }\n    const_iterator begin() const noexcept requires(!IsForward) {\n        return {_substreams.crbegin(), 0};\n    }\n    const_iterator cbegin() const noexcept requires(!IsForward) {\n        return {_substreams.crbegin(), 0};\n    }\n\n    iterator end() noexcept requires(!IsForward) {\n        return {_substreams.crend(), _size};\n    }\n    const_iterator end() const noexcept requires(!IsForward) {\n        return {_substreams.crend(), _size};\n    }\n    const_iterator cend() const noexcept requires(!IsForward) {\n        return {_substreams.crend(), _size};\n    }\n\n};\n\nstatic_assert(std::ranges::range<vector_deserializer<int>>);\n\n}\n"
        },
        {
          "name": "service",
          "type": "tree",
          "content": null
        },
        {
          "name": "service_permit.hh",
          "type": "blob",
          "size": 0.8671875,
          "content": "/*\n * Copyright (C) 2019-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <seastar/core/semaphore.hh>\n#include <seastar/core/shared_ptr.hh>\n\nclass service_permit {\n    seastar::lw_shared_ptr<seastar::semaphore_units<>> _permit;\n    service_permit(seastar::semaphore_units<>&& u) : _permit(seastar::make_lw_shared<seastar::semaphore_units<>>(std::move(u))) {}\n    friend service_permit make_service_permit(seastar::semaphore_units<>&& permit);\n    friend service_permit empty_service_permit();\npublic:\n    size_t count() const { return _permit ? _permit->count() : 0; };\n};\n\ninline service_permit make_service_permit(seastar::semaphore_units<>&& permit) {\n    return service_permit(std::move(permit));\n}\n\ninline service_permit empty_service_permit() {\n    return make_service_permit(seastar::semaphore_units<>());\n}\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.7216796875,
          "content": "from setuptools import find_packages, setup\n\nsetup(\n    name=\"scylla\",\n    description='NoSQL data store using the seastar framework, compatible with Apache Cassandra',\n    url='https://github.com/scylladb/scylla',\n    download_url='https://github.com/scylladb/scylla/tags',\n    license='ScyllaDB-Source-Available-1.0',\n    platforms='any',\n    packages=find_packages(),\n    include_package_data=True,\n    install_requires=[],\n    classifiers=[\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.4\",\n        \"Programming Language :: Python :: 3.5\",\n        \"Programming Language :: Python :: 3.6\",\n        \"Programming Language :: Python :: 3.7\",\n    ],\n)\n"
        },
        {
          "name": "shell.nix",
          "type": "blob",
          "size": 0.482421875,
          "content": "# Copyright (C) 2021-present ScyllaDB\n#\n\n#\n# SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n\nargs:\n\nimport ./default.nix (args // {\n  shell = true;\n\n  devInputs = { pkgs, llvm }: with pkgs; [\n    # for impure building\n    ccache\n    distcc\n\n    # for debugging\n    binutils  # addr2line etc.\n    elfutils\n\n    gdbWithGreenThreadSupport\n\n    llvm.llvm\n    lz4       # coredumps on modern Systemd installations are lz4-compressed\n\n    # etc\n    diffutils\n    colordiff\n  ];\n})\n"
        },
        {
          "name": "sstables",
          "type": "tree",
          "content": null
        },
        {
          "name": "sstables_loader.cc",
          "type": "blob",
          "size": 28.314453125,
          "content": "/*\n * Copyright (C) 2021-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include <fmt/ranges.h>\n#include <seastar/core/coroutine.hh>\n#include <seastar/coroutine/maybe_yield.hh>\n#include <seastar/coroutine/switch_to.hh>\n#include <seastar/coroutine/parallel_for_each.hh>\n#include <seastar/rpc/rpc.hh>\n#include \"sstables_loader.hh\"\n#include \"replica/distributed_loader.hh\"\n#include \"replica/database.hh\"\n#include \"sstables/sstables_manager.hh\"\n#include \"sstables/sstables.hh\"\n#include \"gms/inet_address.hh\"\n#include \"streaming/stream_mutation_fragments_cmd.hh\"\n#include \"streaming/stream_reason.hh\"\n#include \"readers/mutation_fragment_v1_stream.hh\"\n#include \"locator/abstract_replication_strategy.hh\"\n#include \"message/messaging_service.hh\"\n\n#include <cfloat>\n#include <algorithm>\n\n#include <boost/range/adaptor/transformed.hpp>\n\nstatic logging::logger llog(\"sstables_loader\");\n\nnamespace {\n\nclass send_meta_data {\n    locator::host_id _node;\n    seastar::rpc::sink<frozen_mutation_fragment, streaming::stream_mutation_fragments_cmd> _sink;\n    seastar::rpc::source<int32_t> _source;\n    bool _error_from_peer = false;\n    size_t _num_partitions_sent = 0;\n    size_t _num_bytes_sent = 0;\n    future<> _receive_done;\nprivate:\n    future<> do_receive() {\n        int32_t status = 0;\n        while (auto status_opt = co_await _source()) {\n            status = std::get<0>(*status_opt);\n            llog.debug(\"send_meta_data: got error code={}, from node={}\", status, _node);\n            if (status == -1) {\n                _error_from_peer = true;\n            }\n        }\n        llog.debug(\"send_meta_data: finished reading source from node={}\", _node);\n        if (_error_from_peer) {\n            throw std::runtime_error(format(\"send_meta_data: got error code={} from node={}\", status, _node));\n        }\n        co_return;\n    }\npublic:\n    send_meta_data(locator::host_id node,\n            seastar::rpc::sink<frozen_mutation_fragment, streaming::stream_mutation_fragments_cmd> sink,\n            seastar::rpc::source<int32_t> source)\n        : _node(std::move(node))\n        , _sink(std::move(sink))\n        , _source(std::move(source))\n        , _receive_done(make_ready_future<>()) {\n    }\n    void receive() {\n        _receive_done = do_receive();\n    }\n    future<> send(const frozen_mutation_fragment& fmf, bool is_partition_start) {\n        if (_error_from_peer) {\n            throw std::runtime_error(format(\"send_meta_data: got error from peer node={}\", _node));\n        }\n        auto size = fmf.representation().size();\n        if (is_partition_start) {\n            ++_num_partitions_sent;\n        }\n        _num_bytes_sent += size;\n        llog.trace(\"send_meta_data: send mf to node={}, size={}\", _node, size);\n        co_return co_await _sink(fmf, streaming::stream_mutation_fragments_cmd::mutation_fragment_data);\n    }\n    future<> finish(bool failed) {\n        std::exception_ptr eptr;\n        try {\n            if (failed) {\n                co_await _sink(frozen_mutation_fragment(bytes_ostream()), streaming::stream_mutation_fragments_cmd::error);\n            } else {\n                co_await _sink(frozen_mutation_fragment(bytes_ostream()), streaming::stream_mutation_fragments_cmd::end_of_stream);\n            }\n        } catch (...) {\n            eptr = std::current_exception();\n            llog.warn(\"send_meta_data: failed to send {} to node={}, err={}\",\n                    failed ? \"stream_mutation_fragments_cmd::error\" : \"stream_mutation_fragments_cmd::end_of_stream\", _node, eptr);\n        }\n        try {\n            co_await _sink.close();\n        } catch (...)  {\n            eptr = std::current_exception();\n            llog.warn(\"send_meta_data: failed to close sink to node={}, err={}\", _node, eptr);\n        }\n        try {\n            co_await std::move(_receive_done);\n        } catch (...)  {\n            eptr = std::current_exception();\n            llog.warn(\"send_meta_data: failed to process source from node={}, err={}\", _node, eptr);\n        }\n        if (eptr) {\n            std::rethrow_exception(eptr);\n        }\n        co_return;\n    }\n    size_t num_partitions_sent() {\n        return _num_partitions_sent;\n    }\n    size_t num_bytes_sent() {\n        return _num_bytes_sent;\n    }\n};\n\n} // anonymous namespace\n\nusing primary_replica_only = bool_class<struct primary_replica_only_tag>;\nusing unlink_sstables = bool_class<struct unlink_sstables_tag>;\n\nclass sstable_streamer {\nprotected:\n    using stream_scope = sstables_loader::stream_scope;\n    netw::messaging_service& _ms;\n    replica::database& _db;\n    replica::table& _table;\n    locator::effective_replication_map_ptr _erm;\n    std::vector<sstables::shared_sstable> _sstables;\n    const primary_replica_only _primary_replica_only;\n    const unlink_sstables _unlink_sstables;\n    const stream_scope _stream_scope;\npublic:\n    sstable_streamer(netw::messaging_service& ms, replica::database& db, ::table_id table_id, std::vector<sstables::shared_sstable> sstables, primary_replica_only primary, unlink_sstables unlink, stream_scope scope)\n            : _ms(ms)\n            , _db(db)\n            , _table(db.find_column_family(table_id))\n            , _erm(_table.get_effective_replication_map())\n            , _sstables(std::move(sstables))\n            , _primary_replica_only(primary)\n            , _unlink_sstables(unlink)\n            , _stream_scope(scope)\n    {\n        if (_primary_replica_only && _stream_scope != stream_scope::all) {\n            throw std::runtime_error(\"Scoped streaming of primary replica only is not supported yet\");\n        }\n        // By sorting SSTables by their primary key, we allow SSTable runs to be\n        // incrementally streamed.\n        // Overlapping run fragments can have their content deduplicated, reducing\n        // the amount of data we need to put on the wire.\n        // Elements are popped off from the back of the vector, therefore we're sorting\n        // it in descending order, to start from the smaller tokens.\n        std::ranges::sort(_sstables, [] (const sstables::shared_sstable& x, const sstables::shared_sstable& y) {\n            return x->compare_by_first_key(*y) > 0;\n        });\n    }\n\n    virtual ~sstable_streamer() {}\n\n    virtual future<> stream(std::function<void(unsigned)> on_streamed);\n    host_id_vector_replica_set get_endpoints(const dht::token& token) const;\n    future<> stream_sstable_mutations(streaming::plan_id, const dht::partition_range&, std::vector<sstables::shared_sstable>);\nprotected:\n    virtual host_id_vector_replica_set get_primary_endpoints(const dht::token& token) const;\n    future<> stream_sstables(const dht::partition_range&, std::vector<sstables::shared_sstable>, std::function<void(unsigned)> on_streamed);\nprivate:\n    host_id_vector_replica_set get_all_endpoints(const dht::token& token) const;\n};\n\nclass tablet_sstable_streamer : public sstable_streamer {\n    const locator::tablet_map& _tablet_map;\npublic:\n    tablet_sstable_streamer(netw::messaging_service& ms, replica::database& db, ::table_id table_id, std::vector<sstables::shared_sstable> sstables, primary_replica_only primary, unlink_sstables unlink, stream_scope scope)\n        : sstable_streamer(ms, db, table_id, std::move(sstables), primary, unlink, scope)\n        , _tablet_map(_erm->get_token_metadata().tablets().get_tablet_map(table_id)) {\n    }\n\n    virtual future<> stream(std::function<void(unsigned)> on_streamed) override;\n    virtual host_id_vector_replica_set get_primary_endpoints(const dht::token& token) const override;\n\nprivate:\n    host_id_vector_replica_set to_replica_set(const locator::tablet_replica_set& replicas) const {\n        host_id_vector_replica_set result;\n        result.reserve(replicas.size());\n        for (auto&& replica : replicas) {\n            result.push_back(replica.host);\n        }\n        return result;\n    }\n\n    future<> stream_fully_contained_sstables(const dht::partition_range& pr, std::vector<sstables::shared_sstable> sstables, std::function<void(unsigned)> on_streamed) {\n        // FIXME: fully contained sstables can be optimized.\n        return stream_sstables(pr, std::move(sstables), std::move(on_streamed));\n    }\n\n    bool tablet_in_scope(locator::tablet_id) const;\n};\n\nhost_id_vector_replica_set sstable_streamer::get_endpoints(const dht::token& token) const {\n    return get_all_endpoints(token) | std::views::filter([&topo = _erm->get_topology(), scope = _stream_scope] (const auto& ep) {\n        switch (scope) {\n        case stream_scope::all:\n            return true;\n        case stream_scope::dc:\n            return topo.get_datacenter(ep) == topo.get_datacenter();\n        case stream_scope::rack:\n            return topo.get_location(ep) == topo.get_location();\n        case stream_scope::node:\n            return topo.is_me(ep);\n        }\n    }) | std::ranges::to<host_id_vector_replica_set>();\n}\n\nhost_id_vector_replica_set sstable_streamer::get_all_endpoints(const dht::token& token) const {\n    if (_primary_replica_only) {\n        return get_primary_endpoints(token);\n    }\n    auto current_targets = _erm->get_natural_replicas(token);\n    auto pending = _erm->get_pending_replicas(token);\n    std::move(pending.begin(), pending.end(), std::back_inserter(current_targets));\n    return current_targets;\n}\n\nhost_id_vector_replica_set sstable_streamer::get_primary_endpoints(const dht::token& token) const {\n    auto current_targets = _erm->get_natural_replicas(token);\n    current_targets.resize(1);\n    return current_targets;\n}\n\nhost_id_vector_replica_set tablet_sstable_streamer::get_primary_endpoints(const dht::token& token) const {\n    auto tid = _tablet_map.get_tablet_id(token);\n    auto replicas = locator::get_primary_replicas(_tablet_map.get_tablet_info(tid), _tablet_map.get_tablet_transition_info(tid));\n    return to_replica_set(replicas);\n}\n\nfuture<> sstable_streamer::stream(std::function<void(unsigned)> on_streamed) {\n    const auto full_partition_range = dht::partition_range::make_open_ended_both_sides();\n\n    co_await stream_sstables(full_partition_range, std::move(_sstables), std::move(on_streamed));\n}\n\nbool tablet_sstable_streamer::tablet_in_scope(locator::tablet_id tid) const {\n    if (_stream_scope == stream_scope::all) {\n        return true;\n    }\n\n    const auto& topo = _erm->get_topology();\n    for (const auto& r : _tablet_map.get_tablet_info(tid).replicas) {\n        switch (_stream_scope) {\n        case stream_scope::node:\n            if (topo.is_me(r.host)) {\n                return true;\n            }\n            break;\n        case stream_scope::rack:\n            if (topo.get_location(r.host) == topo.get_location()) {\n                return true;\n            }\n            break;\n        case stream_scope::dc:\n            if (topo.get_datacenter(r.host) == topo.get_datacenter()) {\n                return true;\n            }\n            break;\n        case stream_scope::all: // checked above already, but still need it here\n            return true;\n        }\n    }\n    return false;\n}\n\nfuture<> tablet_sstable_streamer::stream(std::function<void(unsigned)> on_streamed) {\n    // sstables are sorted by first key in reverse order.\n    auto sstable_it = _sstables.rbegin();\n\n    for (auto tablet_id : _tablet_map.tablet_ids() | std::views::filter([this] (auto tid) { return tablet_in_scope(tid); })) {\n        auto tablet_range = _tablet_map.get_token_range(tablet_id);\n\n        auto sstable_token_range = [] (const sstables::shared_sstable& sst) {\n            return dht::token_range(sst->get_first_decorated_key().token(),\n                                    sst->get_last_decorated_key().token());\n        };\n\n        std::vector<sstables::shared_sstable> sstables_fully_contained;\n        std::vector<sstables::shared_sstable> sstables_partially_contained;\n\n        // sstable is exhausted if its last key is before the current tablet range\n        auto exhausted = [&tablet_range] (const sstables::shared_sstable& sst) {\n            return tablet_range.before(sst->get_last_decorated_key().token(), dht::token_comparator{});\n        };\n        while (sstable_it != _sstables.rend() && exhausted(*sstable_it)) {\n            sstable_it++;\n        }\n\n        for (auto sst_it = sstable_it; sst_it != _sstables.rend(); sst_it++) {\n            auto sst_token_range = sstable_token_range(*sst_it);\n            // sstables are sorted by first key, so we're done with current tablet when\n            // the next sstable doesn't overlap with its owned token range.\n            if (!tablet_range.overlaps(sst_token_range, dht::token_comparator{})) {\n                break;\n            }\n\n            if (tablet_range.contains(sst_token_range, dht::token_comparator{})) {\n                sstables_fully_contained.push_back(*sst_it);\n            } else {\n                sstables_partially_contained.push_back(*sst_it);\n            }\n            co_await coroutine::maybe_yield();\n        }\n\n        auto tablet_pr = dht::to_partition_range(tablet_range);\n        co_await stream_sstables(tablet_pr, std::move(sstables_partially_contained), on_streamed);\n        co_await stream_fully_contained_sstables(tablet_pr, std::move(sstables_fully_contained), on_streamed);\n    }\n}\n\nfuture<> sstable_streamer::stream_sstables(const dht::partition_range& pr, std::vector<sstables::shared_sstable> sstables, std::function<void(unsigned)> on_streamed) {\n    size_t nr_sst_total = _sstables.size();\n    size_t nr_sst_current = 0;\n\n    while (!sstables.empty()) {\n        const size_t batch_sst_nr = std::min(16uz, sstables.size());\n        auto sst_processed = sstables\n            | std::views::reverse\n            | std::views::take(batch_sst_nr)\n            | std::ranges::to<std::vector>();\n        sstables.erase(sstables.end() - batch_sst_nr, sstables.end());\n\n        auto ops_uuid = streaming::plan_id{utils::make_random_uuid()};\n        llog.info(\"load_and_stream: started ops_uuid={}, process [{}-{}] out of {} sstables=[{}]\",\n            ops_uuid, nr_sst_current, nr_sst_current + sst_processed.size(), nr_sst_total,\n            fmt::join(sst_processed | boost::adaptors::transformed([] (auto sst) { return sst->get_filename(); }), \", \"));\n        nr_sst_current += sst_processed.size();\n        co_await stream_sstable_mutations(ops_uuid, pr, std::move(sst_processed));\n        if (on_streamed) {\n            std::invoke(on_streamed, batch_sst_nr);\n        }\n    }\n}\n\nfuture<> sstable_streamer::stream_sstable_mutations(streaming::plan_id ops_uuid, const dht::partition_range& pr, std::vector<sstables::shared_sstable> sstables) {\n    const auto token_range = pr.transform(std::mem_fn(&dht::ring_position::token));\n    auto s = _table.schema();\n    const auto cf_id = s->id();\n    const auto reason = streaming::stream_reason::repair;\n\n    auto sst_set = make_lw_shared<sstables::sstable_set>(sstables::make_partitioned_sstable_set(s, false));\n    size_t estimated_partitions = 0;\n    for (auto& sst : sstables) {\n        estimated_partitions += sst->estimated_keys_for_range(token_range);\n        sst_set->insert(sst);\n    }\n\n    auto start_time = std::chrono::steady_clock::now();\n    host_id_vector_replica_set current_targets;\n    std::unordered_map<locator::host_id, send_meta_data> metas;\n    size_t num_partitions_processed = 0;\n    size_t num_bytes_read = 0;\n    auto permit = co_await _db.obtain_reader_permit(_table, \"sstables_loader::load_and_stream()\", db::no_timeout, {});\n    auto reader = mutation_fragment_v1_stream(_table.make_streaming_reader(s, std::move(permit), pr, sst_set, gc_clock::now()));\n    std::exception_ptr eptr;\n    bool failed = false;\n\n    try {\n        while (auto mf = co_await reader()) {\n            bool is_partition_start = mf->is_partition_start();\n            if (is_partition_start) {\n                ++num_partitions_processed;\n                auto& start = mf->as_partition_start();\n                const auto& current_dk = start.key();\n\n                current_targets = get_endpoints(current_dk.token());\n                llog.trace(\"load_and_stream: ops_uuid={}, current_dk={}, current_targets={}\", ops_uuid,\n                        current_dk.token(), current_targets);\n                for (auto& node : current_targets) {\n                    if (!metas.contains(node)) {\n                        auto [sink, source] = co_await _ms.make_sink_and_source_for_stream_mutation_fragments(reader.schema()->version(),\n                                ops_uuid, cf_id, estimated_partitions, reason, service::default_session_id, node);\n                        llog.debug(\"load_and_stream: ops_uuid={}, make sink and source for node={}\", ops_uuid, node);\n                        metas.emplace(node, send_meta_data(node, std::move(sink), std::move(source)));\n                        metas.at(node).receive();\n                    }\n                }\n            }\n            frozen_mutation_fragment fmf = freeze(*s, *mf);\n            num_bytes_read += fmf.representation().size();\n            co_await coroutine::parallel_for_each(current_targets, [&metas, &fmf, is_partition_start] (const locator::host_id& node) {\n                return metas.at(node).send(fmf, is_partition_start);\n            });\n        }\n    } catch (...) {\n        failed = true;\n        eptr = std::current_exception();\n        llog.warn(\"load_and_stream: ops_uuid={}, ks={}, table={}, send_phase, err={}\",\n                ops_uuid, s->ks_name(), s->cf_name(), eptr);\n    }\n    co_await reader.close();\n    try {\n        co_await coroutine::parallel_for_each(metas.begin(), metas.end(), [failed] (std::pair<const locator::host_id, send_meta_data>& pair) {\n            auto& meta = pair.second;\n            return meta.finish(failed);\n        });\n    } catch (...) {\n        failed = true;\n        eptr = std::current_exception();\n        llog.warn(\"load_and_stream: ops_uuid={}, ks={}, table={}, finish_phase, err={}\",\n                ops_uuid, s->ks_name(), s->cf_name(), eptr);\n    }\n    if (!failed && _unlink_sstables) {\n        try {\n            co_await coroutine::parallel_for_each(sstables, [&] (sstables::shared_sstable& sst) {\n                llog.debug(\"load_and_stream: ops_uuid={}, ks={}, table={}, remove sst={}\",\n                        ops_uuid, s->ks_name(), s->cf_name(), sst->component_filenames());\n                return sst->unlink();\n            });\n        } catch (...) {\n            failed = true;\n            eptr = std::current_exception();\n            llog.warn(\"load_and_stream: ops_uuid={}, ks={}, table={}, del_sst_phase, err={}\",\n                    ops_uuid, s->ks_name(), s->cf_name(), eptr);\n        }\n    }\n    auto duration = std::chrono::duration_cast<std::chrono::duration<float>>(std::chrono::steady_clock::now() - start_time).count();\n    for (auto& [node, meta] : metas) {\n        llog.info(\"load_and_stream: ops_uuid={}, ks={}, table={}, target_node={}, num_partitions_sent={}, num_bytes_sent={}\",\n                ops_uuid, s->ks_name(), s->cf_name(), node, meta.num_partitions_sent(), meta.num_bytes_sent());\n    }\n    auto partition_rate = std::fabs(duration) > FLT_EPSILON ? num_partitions_processed / duration : 0;\n    auto bytes_rate = std::fabs(duration) > FLT_EPSILON ? num_bytes_read / duration / 1024 / 1024 : 0;\n    auto status = failed ? \"failed\" : \"succeeded\";\n    llog.info(\"load_and_stream: finished ops_uuid={}, ks={}, table={}, partitions_processed={} partitions, bytes_processed={} bytes, partitions_per_second={} partitions/s, bytes_per_second={} MiB/s, duration={} s, status={}\",\n            ops_uuid, s->ks_name(), s->cf_name(), num_partitions_processed, num_bytes_read, partition_rate, bytes_rate, duration, status);\n    if (failed) {\n        std::rethrow_exception(eptr);\n    }\n}\n\ntemplate <typename... Args>\nstatic std::unique_ptr<sstable_streamer> make_sstable_streamer(bool uses_tablets, Args&&... args) {\n    if (uses_tablets) {\n        return std::make_unique<tablet_sstable_streamer>(std::forward<Args>(args)...);\n    }\n    return std::make_unique<sstable_streamer>(std::forward<Args>(args)...);\n}\n\nfuture<> sstables_loader::load_and_stream(sstring ks_name, sstring cf_name,\n        ::table_id table_id, std::vector<sstables::shared_sstable> sstables, bool primary, bool unlink, stream_scope scope,\n        std::function<void(unsigned)> on_streamed) {\n    // streamer guarantees topology stability, for correctness, by holding effective_replication_map\n    // throughout its lifetime.\n    auto streamer = make_sstable_streamer(_db.local().find_column_family(table_id).uses_tablets(),\n                                          _messaging, _db.local(), table_id, std::move(sstables),\n                                          primary_replica_only(primary), unlink_sstables(unlink), scope);\n\n    co_await streamer->stream(on_streamed);\n}\n\n// For more details, see distributed_loader::process_upload_dir().\n// All the global operations are going to happen here, and just the reloading happens\n// in there.\nfuture<> sstables_loader::load_new_sstables(sstring ks_name, sstring cf_name,\n    bool load_and_stream, bool primary_replica_only, stream_scope scope) {\n    if (_loading_new_sstables) {\n        throw std::runtime_error(\"Already loading SSTables. Try again later\");\n    } else {\n        _loading_new_sstables = true;\n    }\n\n    co_await coroutine::switch_to(_sched_group);\n\n    sstring load_and_stream_desc = fmt::format(\"{}\", load_and_stream);\n    const auto& rs = _db.local().find_keyspace(ks_name).get_replication_strategy();\n    if (rs.is_per_table() && !load_and_stream) {\n        load_and_stream = true;\n        load_and_stream_desc = \"auto-enabled-for-tablets\";\n    }\n\n    llog.info(\"Loading new SSTables for keyspace={}, table={}, load_and_stream={}, primary_replica_only={}\",\n            ks_name, cf_name, load_and_stream_desc, primary_replica_only);\n    try {\n        if (load_and_stream) {\n            ::table_id table_id;\n            std::vector<std::vector<sstables::shared_sstable>> sstables_on_shards;\n            // Load-and-stream reads the entire content from SSTables, therefore it can afford to discard the bloom filter\n            // that might otherwise consume a significant amount of memory.\n            sstables::sstable_open_config cfg {\n                .load_bloom_filter = false,\n            };\n            std::tie(table_id, sstables_on_shards) = co_await replica::distributed_loader::get_sstables_from_upload_dir(_db, ks_name, cf_name, cfg);\n            co_await container().invoke_on_all([&sstables_on_shards, ks_name, cf_name, table_id, primary_replica_only, scope] (sstables_loader& loader) mutable -> future<> {\n                co_await loader.load_and_stream(ks_name, cf_name, table_id, std::move(sstables_on_shards[this_shard_id()]), primary_replica_only, true, scope, {});\n            });\n        } else {\n            co_await replica::distributed_loader::process_upload_dir(_db, _view_builder, ks_name, cf_name);\n        }\n    } catch (...) {\n        llog.warn(\"Done loading new SSTables for keyspace={}, table={}, load_and_stream={}, primary_replica_only={}, status=failed: {}\",\n                ks_name, cf_name, load_and_stream, primary_replica_only, std::current_exception());\n        _loading_new_sstables = false;\n        throw;\n    }\n    llog.info(\"Done loading new SSTables for keyspace={}, table={}, load_and_stream={}, primary_replica_only={}, status=succeeded\",\n            ks_name, cf_name, load_and_stream, primary_replica_only);\n    _loading_new_sstables = false;\n    co_return;\n}\n\nclass sstables_loader::download_task_impl : public tasks::task_manager::task::impl {\n    sharded<sstables_loader>& _loader;\n    sstring _endpoint;\n    sstring _bucket;\n    sstring _ks;\n    sstring _cf;\n    sstring _prefix;\n    sstables_loader::stream_scope _scope;\n    std::vector<sstring> _sstables;\n    std::vector<unsigned> _num_sstables_processed;\n\nprotected:\n    virtual future<> run() override;\n\npublic:\n    download_task_impl(tasks::task_manager::module_ptr module, sharded<sstables_loader>& loader,\n            sstring endpoint, sstring bucket,\n            sstring ks, sstring cf, sstring prefix, std::vector<sstring> sstables, sstables_loader::stream_scope scope) noexcept\n        : tasks::task_manager::task::impl(module, tasks::task_id::create_random_id(), 0, \"node\", ks, \"\", \"\", tasks::task_id::create_null_id())\n        , _loader(loader)\n        , _endpoint(std::move(endpoint))\n        , _bucket(std::move(bucket))\n        , _ks(std::move(ks))\n        , _cf(std::move(cf))\n        , _prefix(std::move(prefix))\n        , _scope(scope)\n        , _sstables(std::move(sstables))\n        , _num_sstables_processed(smp::count)\n    {\n        _status.progress_units = \"sstables\";\n    }\n\n    virtual std::string type() const override {\n        return \"download_sstables\";\n    }\n\n    virtual tasks::is_internal is_internal() const noexcept override {\n        return tasks::is_internal::no;\n    }\n\n    virtual tasks::is_user_task is_user_task() const noexcept override {\n        return tasks::is_user_task::yes;\n    }\n\n    tasks::is_abortable is_abortable() const noexcept override {\n        return tasks::is_abortable::yes;\n    }\n\n    virtual future<tasks::task_manager::task::progress> get_progress() const override {\n        llog.debug(\"get_progress: {}\", _num_sstables_processed);\n        unsigned processed = co_await _loader.map_reduce(adder<unsigned>(), [this] (auto&) {\n            return _num_sstables_processed[this_shard_id()];\n        });\n        co_return tasks::task_manager::task::progress {\n            .completed = processed,\n            .total = _sstables.size(),\n        };\n    }\n};\n\nfuture<> sstables_loader::download_task_impl::run() {\n    // Load-and-stream reads the entire content from SSTables, therefore it can afford to discard the bloom filter\n    // that might otherwise consume a significant amount of memory.\n    sstables::sstable_open_config cfg {\n        .load_bloom_filter = false,\n    };\n    llog.debug(\"Loading sstables from {}({}/{})\", _endpoint, _bucket, _prefix);\n\n    std::vector<seastar::abort_source> shard_aborts(smp::count);\n    auto [ table_id, sstables_on_shards ] = co_await replica::distributed_loader::get_sstables_from_object_store(_loader.local()._db, _ks, _cf, _sstables, _endpoint, _bucket, _prefix, cfg, [&] {\n        return &shard_aborts[this_shard_id()];\n    });\n    llog.debug(\"Streaming sstables from {}({}/{})\", _endpoint, _bucket, _prefix);\n    std::exception_ptr ex;\n    gate g;\n    try {\n        _as.check();\n\n        auto s = _as.subscribe([&]() noexcept {\n            try {\n                auto h = g.hold();\n                (void)smp::invoke_on_all([&shard_aborts, ex = _as.abort_requested_exception_ptr()] {\n                    shard_aborts[this_shard_id()].request_abort_ex(ex);\n                }).finally([h = std::move(h)] {});\n            } catch (...) {\n            }\n        });\n\n        co_await _loader.invoke_on_all([this, &sstables_on_shards, table_id] (sstables_loader& loader) mutable -> future<> {\n            co_await loader.load_and_stream(_ks, _cf, table_id, std::move(sstables_on_shards[this_shard_id()]), false, false, _scope, [this] (unsigned num_streamed) {\n                _num_sstables_processed[this_shard_id()] += num_streamed;\n            });\n        });\n    } catch (...) {\n        ex = std::current_exception();\n    }\n\n    co_await g.close();\n\n    if (_as.abort_requested()) {\n        if (!ex) {\n            ex = _as.abort_requested_exception_ptr();\n        }\n    }\n\n    if (ex) {\n        co_await _loader.invoke_on_all([&sstables_on_shards] (sstables_loader&) {\n            sstables_on_shards[this_shard_id()] = {}; // clear on correct shard\n        });\n        co_await coroutine::return_exception_ptr(std::move(ex));\n    }\n}\n\nsstables_loader::sstables_loader(sharded<replica::database>& db,\n        netw::messaging_service& messaging,\n        sharded<db::view::view_builder>& vb,\n        tasks::task_manager& tm,\n        sstables::storage_manager& sstm,\n        seastar::scheduling_group sg)\n    : _db(db)\n    , _messaging(messaging)\n    , _view_builder(vb)\n    , _task_manager_module(make_shared<task_manager_module>(tm))\n    , _storage_manager(sstm)\n    , _sched_group(std::move(sg))\n{\n    tm.register_module(\"sstables_loader\", _task_manager_module);\n}\n\nfuture<> sstables_loader::stop() {\n    co_await _task_manager_module->stop();\n}\n\nfuture<tasks::task_id> sstables_loader::download_new_sstables(sstring ks_name, sstring cf_name,\n            sstring prefix, std::vector<sstring> sstables,\n            sstring endpoint, sstring bucket, stream_scope scope) {\n    if (!_storage_manager.is_known_endpoint(endpoint)) {\n        throw std::invalid_argument(format(\"endpoint {} not found\", endpoint));\n    }\n    llog.info(\"Restore sstables from {}({}) to {}\", endpoint, prefix, ks_name);\n\n    auto task = co_await _task_manager_module->make_and_start_task<download_task_impl>({}, container(), std::move(endpoint), std::move(bucket), std::move(ks_name), std::move(cf_name), std::move(prefix), std::move(sstables), scope);\n    co_return task->id();\n}\n"
        },
        {
          "name": "sstables_loader.hh",
          "type": "blob",
          "size": 3.29296875,
          "content": "/*\n * Copyright (C) 2021-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <vector>\n#include <seastar/core/sharded.hh>\n#include \"schema/schema_fwd.hh\"\n#include \"sstables/shared_sstable.hh\"\n#include \"tasks/task_manager.hh\"\n\nusing namespace seastar;\n\nnamespace replica {\nclass database;\n}\n\nnamespace sstables { class storage_manager; }\n\nnamespace netw { class messaging_service; }\nnamespace db {\nnamespace view {\nclass view_builder;\n}\n}\n\n// The handler of the 'storage_service/load_new_ss_tables' endpoint which, in\n// turn, is the target of the 'nodetool refresh' command.\n// Gets sstables from the upload directory and makes them available in the\n// system. Built on top of the distributed_loader functionality.\nclass sstables_loader : public seastar::peering_sharded_service<sstables_loader> {\npublic:\n    enum class stream_scope { all, dc, rack, node };\n    class task_manager_module : public tasks::task_manager::module {\n        public:\n            task_manager_module(tasks::task_manager& tm) noexcept : tasks::task_manager::module(tm, \"sstables_loader\") {}\n    };\n\nprivate:\n    sharded<replica::database>& _db;\n    netw::messaging_service& _messaging;\n    sharded<db::view::view_builder>& _view_builder;\n    shared_ptr<task_manager_module> _task_manager_module;\n    sstables::storage_manager& _storage_manager;\n    seastar::scheduling_group _sched_group;\n\n    // Note that this is obviously only valid for the current shard. Users of\n    // this facility should elect a shard to be the coordinator based on any\n    // given objective criteria\n    //\n    // It shouldn't be impossible to actively serialize two callers if the need\n    // ever arise.\n    bool _loading_new_sstables = false;\n\n    future<> load_and_stream(sstring ks_name, sstring cf_name,\n            table_id, std::vector<sstables::shared_sstable> sstables,\n            bool primary_replica_only, bool unlink_sstables, stream_scope scope,\n            std::function<void(unsigned)> on_streamed);\n\npublic:\n    sstables_loader(sharded<replica::database>& db,\n            netw::messaging_service& messaging,\n            sharded<db::view::view_builder>& vb,\n            tasks::task_manager& tm,\n            sstables::storage_manager& sstm,\n            seastar::scheduling_group sg);\n\n    future<> stop();\n\n    /**\n     * Load new SSTables not currently tracked by the system\n     *\n     * This can be called, for instance, after copying a batch of SSTables to a CF directory.\n     *\n     * This should not be called in parallel for the same keyspace / column family, and doing\n     * so will throw an std::runtime_exception.\n     *\n     * @param ks_name the keyspace in which to search for new SSTables.\n     * @param cf_name the column family in which to search for new SSTables.\n     * @return a future<> when the operation finishes.\n     */\n    future<> load_new_sstables(sstring ks_name, sstring cf_name,\n            bool load_and_stream, bool primary_replica_only, stream_scope scope);\n\n    /**\n     * Download new SSTables not currently tracked by the system from object store\n     */\n    future<tasks::task_id> download_new_sstables(sstring ks_name, sstring cf_name,\n            sstring prefix, std::vector<sstring> sstables,\n            sstring endpoint, sstring bucket, stream_scope scope);\n\n    class download_task_impl;\n};\n"
        },
        {
          "name": "streaming",
          "type": "tree",
          "content": null
        },
        {
          "name": "supervisor.hh",
          "type": "blob",
          "size": 1.2001953125,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <seastar/core/sstring.hh>\n#include <seastar/core/format.hh>\n#include <seastar/util/log.hh>\n#include \"seastarx.hh\"\n#include <systemd/sd-daemon.h>\n\nextern logger startlog;\n\nclass supervisor {\npublic:\n    static constexpr auto systemd_ready_msg = \"READY=1\";\n    /** A systemd status message has a format <status message prefix>=<message> */\n    static constexpr auto systemd_status_msg_prefix = \"STATUS\";\npublic:\n    /**\n     * @brief Notify the Supervisor with the given message.\n     * @param msg message to notify the Supervisor with\n     * @param ready set to TRUE when scylla service becomes ready\n     */\n    static inline void notify(sstring msg, bool ready = false) {\n        startlog.info(\"{}\", msg);\n        try_notify_systemd(msg, ready);\n    }\n\nprivate:\n    static inline void try_notify_systemd(sstring msg, bool ready) {\n        if (ready) {\n            sd_notify(0, format(\"{}\\n{}={}\\n\", systemd_ready_msg, systemd_status_msg_prefix, msg).c_str());\n        } else {\n            sd_notify(0, format(\"{}={}\\n\", systemd_status_msg_prefix, msg).c_str());\n        }\n    }\n};\n"
        },
        {
          "name": "swagger-ui",
          "type": "commit",
          "content": null
        },
        {
          "name": "table_helper.cc",
          "type": "blob",
          "size": 8.65234375,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n *\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"utils/assert.hh\"\n#include <seastar/core/coroutine.hh>\n#include <seastar/coroutine/parallel_for_each.hh>\n#include \"table_helper.hh\"\n#include \"cql3/query_processor.hh\"\n#include \"cql3/statements/create_table_statement.hh\"\n#include \"cql3/statements/modification_statement.hh\"\n#include \"replica/database.hh\"\n#include \"service/migration_manager.hh\"\n\nstatic logging::logger tlogger(\"table_helper\");\n\nstatic schema_ptr parse_new_cf_statement(cql3::query_processor& qp, const sstring& create_cql) {\n    auto db = qp.db();\n\n    auto parsed = cql3::query_processor::parse_statement(create_cql, cql3::dialect{});\n\n    cql3::statements::raw::cf_statement* parsed_cf_stmt = static_cast<cql3::statements::raw::cf_statement*>(parsed.get());\n    (void)parsed_cf_stmt->keyspace(); // This will SCYLLA_ASSERT if cql statement did not contain keyspace\n    ::shared_ptr<cql3::statements::create_table_statement> statement =\n                    static_pointer_cast<cql3::statements::create_table_statement>(\n                                    parsed_cf_stmt->prepare(db, qp.get_cql_stats())->statement);\n    auto schema = statement->get_cf_meta_data(db);\n\n    // Generate the CF UUID based on its KF names. This is needed to ensure that\n    // all Nodes that create it would create it with the same UUID and we don't\n    // hit the #420 issue.\n    auto uuid = generate_legacy_id(schema->ks_name(), schema->cf_name());\n\n    schema_builder b(schema);\n    b.set_uuid(uuid);\n\n    return b.build();\n}\n\nfuture<> table_helper::setup_table(cql3::query_processor& qp, service::migration_manager& mm, const sstring& create_cql) {\n    auto db = qp.db();\n\n    auto schema = parse_new_cf_statement(qp, create_cql);\n\n    if (db.has_schema(schema->ks_name(), schema->cf_name())) {\n        co_return;\n    }\n\n    auto group0_guard = co_await mm.start_group0_operation();\n    auto ts = group0_guard.write_timestamp();\n\n    if (db.has_schema(schema->ks_name(), schema->cf_name())) { // re-check after read barrier\n        co_return;\n    }\n\n    // We don't care it it fails really - this may happen due to concurrent\n    // \"CREATE TABLE\" invocation on different Nodes.\n    // The important thing is that it will converge eventually (some traces may\n    // be lost in a process but that's ok).\n    try {\n        co_return co_await mm.announce(co_await service::prepare_new_column_family_announcement(qp.proxy(), schema, ts),\n                std::move(group0_guard), format(\"table_helper: create {} table\", schema->cf_name()));\n    } catch (...) {}\n}\n\nfuture<bool> table_helper::try_prepare(bool fallback, cql3::query_processor& qp, service::query_state& qs, cql3::dialect dialect) {\n    // Note: `_insert_cql_fallback` is known to be engaged if `fallback` is true, see cache_table_info below.\n    auto& stmt = fallback ? _insert_cql_fallback.value() : _insert_cql;\n    try {\n        shared_ptr<cql_transport::messages::result_message::prepared> msg_ptr = co_await qp.prepare(stmt, qs.get_client_state(), dialect);\n        _prepared_stmt = std::move(msg_ptr->get_prepared());\n        shared_ptr<cql3::cql_statement> cql_stmt = _prepared_stmt->statement;\n        _insert_stmt = dynamic_pointer_cast<cql3::statements::modification_statement>(cql_stmt);\n        _is_fallback_stmt = fallback;\n        co_return true;\n    } catch (exceptions::invalid_request_exception& eptr) {\n        // the non-fallback statement can't be prepared, and there is no possible fallback\n        if (!fallback && !_insert_cql_fallback) {\n            throw;\n        }\n        // We're trying to prepare the fallback statement, but it can't be prepared; signal an\n        // unrecoverable error\n        if (fallback) {\n            throw;\n        }\n\n        // There's still a chance to prepare the fallback statement\n        co_return false;\n    }\n}\n\nfuture<> table_helper::cache_table_info(cql3::query_processor& qp, service::migration_manager& mm, service::query_state& qs) {\n    if (!_prepared_stmt) {\n        // if prepared statement has been invalidated - drop cached pointers\n        _insert_stmt = nullptr;\n    } else if (!_is_fallback_stmt) {\n        // we've already prepared the non-fallback statement\n        co_return;\n    }\n\n    try {\n        bool success = co_await try_prepare(false, qp, qs, cql3::internal_dialect());\n        if (_is_fallback_stmt && _prepared_stmt) {\n            co_return;\n        }\n        if (!success) {\n            co_await try_prepare(true, qp, qs, cql3::internal_dialect()); // Can only return true or exception when preparing the fallback statement\n        }\n    } catch (...) {\n        auto eptr = std::current_exception();\n\n        // One of the possible causes for an error here could be the table that doesn't exist.\n        //FIXME: discarded future.\n        (void)qp.container().invoke_on(0, [&mm = mm.container(), create_cql = _create_cql] (cql3::query_processor& qp) -> future<> {\n            co_return co_await table_helper::setup_table(qp, mm.local(), create_cql);\n        });\n\n        // We throw the bad_column_family exception because the caller\n        // expects and accounts this type of errors.\n        try {\n            std::rethrow_exception(eptr);\n        } catch (std::exception& e) {\n            throw bad_column_family(_keyspace, _name, e);\n        } catch (...) {\n            throw bad_column_family(_keyspace, _name);\n        }\n    }\n}\n\nfuture<> table_helper::insert(cql3::query_processor& qp, service::migration_manager& mm, service::query_state& qs, noncopyable_function<cql3::query_options ()> opt_maker) {\n    co_await cache_table_info(qp, mm, qs);\n    auto opts = opt_maker();\n    opts.prepare(_prepared_stmt->bound_names);\n    co_await _insert_stmt->execute(qp, qs, opts, std::nullopt);\n}\n\nfuture<> table_helper::setup_keyspace(cql3::query_processor& qp, service::migration_manager& mm, std::string_view keyspace_name, sstring replication_factor, service::query_state& qs, std::vector<table_helper*> tables) {\n    if (this_shard_id() != 0) {\n        co_return;\n    }\n\n    // FIXME: call `announce` once (`announce` keyspace and tables together)\n    //\n    // Note that the CQL code in `parse_new_cf_statement` assumes that the keyspace exists.\n    // To solve this problem, we could, for example, use `schema_builder` instead of the\n    // CQL statements to create tables in `table_helper`.\n\n    if (std::any_of(tables.begin(), tables.end(), [&] (table_helper* t) { return t->_keyspace != keyspace_name; })) {\n        throw std::invalid_argument(\"setup_keyspace called with table_helper for different keyspace\");\n    }\n\n    data_dictionary::database db = qp.db();\n\n    std::map<sstring, sstring> opts;\n    opts[\"replication_factor\"] = replication_factor;\n    auto ksm = keyspace_metadata::new_keyspace(keyspace_name, \"org.apache.cassandra.locator.SimpleStrategy\", std::move(opts), std::nullopt);\n\n    while (!db.has_keyspace(keyspace_name)) {\n        auto group0_guard = co_await mm.start_group0_operation();\n        auto ts = group0_guard.write_timestamp();\n\n        if (!db.has_keyspace(keyspace_name)) {\n            try {\n                co_await mm.announce(service::prepare_new_keyspace_announcement(db.real_database(), ksm, ts),\n                        std::move(group0_guard), seastar::format(\"table_helper: create {} keyspace\", keyspace_name));\n            } catch (service::group0_concurrent_modification&) {\n                tlogger.info(\"Concurrent operation is detected while creating {} keyspace, retrying.\", keyspace_name);\n            }\n        }\n    }\n\n    qs.get_client_state().set_keyspace(db.real_database(), keyspace_name);\n\n    while (std::any_of(tables.begin(), tables.end(), [db] (table_helper* t) { return !db.has_schema(t->_keyspace, t->_name); })) {\n        auto group0_guard = co_await mm.start_group0_operation();\n        auto ts = group0_guard.write_timestamp();\n        std::vector<mutation> table_mutations;\n\n        co_await coroutine::parallel_for_each(tables, [&] (auto&& table) -> future<> {\n            auto schema = parse_new_cf_statement(qp, table->_create_cql);\n            if (!db.has_schema(schema->ks_name(), schema->cf_name())) {\n                co_return co_await service::prepare_new_column_family_announcement(table_mutations, qp.proxy(), *ksm, schema, ts);\n            }\n        });\n\n        if (table_mutations.empty()) {\n            co_return;\n        }\n\n        try {\n            co_return co_await mm.announce(std::move(table_mutations), std::move(group0_guard),\n                    seastar::format(\"table_helper: create tables for {} keyspace\", keyspace_name));\n        } catch (service::group0_concurrent_modification&) {\n            tlogger.info(\"Concurrent operation is detected while creating tables for {} keyspace, retrying.\", keyspace_name);\n        }\n    }\n}\n"
        },
        {
          "name": "table_helper.hh",
          "type": "blob",
          "size": 5.79296875,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n *\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"cql3/statements/prepared_statement.hh\"\n#include \"service/query_state.hh\"\n\nnamespace service {\nclass migration_manager;\n}\n\nnamespace cql3 {\nclass query_processor;\nclass dialect;\nnamespace statements {\nclass modification_statement;\n}}\n\n/**\n * \\class table_helper\n * \\brief A helper class that unites the operations on a single table under the same roof.\n */\nclass table_helper {\nprivate:\n    const sstring _keyspace; /** a keyspace name */\n    const sstring _name; /** a table name */\n    const sstring _create_cql; /** a CQL CREATE TABLE statement for the table */\n    const sstring _insert_cql; /** a CQL INSERT statement */\n    const std::optional<sstring> _insert_cql_fallback; /** a fallback CQL INSERT statement */\n\n    cql3::statements::prepared_statement::checked_weak_ptr _prepared_stmt; /** a raw prepared statement object (containing the INSERT statement) */\n    shared_ptr<cql3::statements::modification_statement> _insert_stmt; /** INSERT prepared statement */\n    /*\n     * Tells whether the _insert_stmt is a prepared fallback INSERT statement or the regular one.\n     * Should be changed alongside every _insert_stmt reassignment\n     * */\n    bool _is_fallback_stmt = false;\nprivate:\n    // Returns true is prepare succeeded, false if failed and there's still a chance to recover, exception if prepare failed and it's not possible to recover\n    future<bool> try_prepare(bool fallback, cql3::query_processor& qp, service::query_state& qs, cql3::dialect dialect);\npublic:\n    table_helper(std::string_view keyspace, std::string_view name, sstring create_cql, sstring insert_cql, std::optional<sstring> insert_cql_fallback = std::nullopt)\n        : _keyspace(keyspace)\n        , _name(name)\n        , _create_cql(std::move(create_cql))\n        , _insert_cql(std::move(insert_cql))\n        , _insert_cql_fallback(std::move(insert_cql_fallback)) {}\n\n    /**\n     * Tries to create a table using create_cql command.\n     *\n     * @return A future that resolves when the operation is complete. Any\n     *         possible errors are ignored.\n     */\n    static future<> setup_table(cql3::query_processor& qp, service::migration_manager& mm, const sstring& create_cql);\n\n    /**\n     * @return a future that resolves when the given t_helper is ready to be used for\n     * data insertion.\n     */\n    future<> cache_table_info(cql3::query_processor& qp, service::migration_manager& mm, service::query_state&);\n\n    /**\n     * @return The table name\n     */\n    const sstring& name() const {\n        return _name;\n    }\n\n    /**\n     * @return A pointer to the INSERT prepared statement\n     */\n    shared_ptr<cql3::statements::modification_statement> insert_stmt() const {\n        return _insert_stmt;\n    }\n\n    /**\n     * Execute a single insertion into the table.\n     *\n     * @tparam OptMaker cql_options maker functor type\n     * @tparam Args OptMaker arguments' types\n     * @param opt_maker cql_options maker functor\n     * @param opt_maker_args opt_maker arguments\n     */\n    template <typename OptMaker, typename... Args>\n    requires seastar::CanInvoke<OptMaker, Args...>\n    future<> insert(cql3::query_processor& qp, service::migration_manager& mm, service::query_state& qs, OptMaker opt_maker, Args... opt_maker_args) {\n        return insert(qp, mm, qs, noncopyable_function<cql3::query_options ()>([opt_maker = std::move(opt_maker), args = std::make_tuple(std::move(opt_maker_args)...)] () mutable {\n            return apply(opt_maker, std::move(args));\n        }));\n    }\n\n    future<> insert(cql3::query_processor& qp, service::migration_manager& mm, service::query_state& qs, noncopyable_function<cql3::query_options ()> opt_maker);\n\n    static future<> setup_keyspace(cql3::query_processor& qp, service::migration_manager& mm, std::string_view keyspace_name, sstring replication_factor, service::query_state& qs, std::vector<table_helper*> tables);\n\n    /**\n     * Makes a monotonically increasing value in 100ns (\"nanos\") based on the given time\n     * stamp and the \"nanos\" value of the previous event.\n     *\n     * If the amount of 100s of ns evaluated from the @param tp is equal to the\n     * given @param last_event_nanos increment @param last_event_nanos by one\n     * and return a time point based its new value.\n     *\n     * @param last_event_nanos a reference to the last nanos to align the given time point to.\n     * @param tp the amount of time passed since the Epoch that will be used for the calculation.\n     *\n     * @return the monotonically increasing value in 100s of ns based on the\n     * given time stamp and on the \"nanos\" value of the previous event.\n     */\n    static std::chrono::system_clock::time_point make_monotonic_UUID_tp(int64_t& last_event_nanos, std::chrono::system_clock::time_point tp) {\n        using namespace std::chrono;\n\n        auto tp_nanos = duration_cast<nanoseconds>(tp.time_since_epoch()).count() / 100;\n        if (tp_nanos > last_event_nanos) {\n            last_event_nanos = tp_nanos;\n            return tp;\n        } else {\n            return std::chrono::system_clock::time_point(nanoseconds((++last_event_nanos) * 100));\n        }\n    }\n};\n\nstruct bad_column_family : public std::exception {\nprivate:\n    sstring _keyspace;\n    sstring _cf;\n    sstring _what;\npublic:\n    bad_column_family(const sstring& keyspace, const sstring& cf)\n        : _keyspace(keyspace)\n        , _cf(cf)\n        , _what(format(\"{}.{} doesn't meet expected schema.\", _keyspace, _cf))\n    { }\n    bad_column_family(const sstring& keyspace, const sstring& cf, const std::exception& e)\n        : _keyspace(keyspace)\n        , _cf(cf)\n        , _what(format(\"{}.{} doesn't meet expected schema: {}\", _keyspace, _cf, e.what()))\n    { }\n    const char* what() const noexcept override {\n        return _what.c_str();\n    }\n};\n"
        },
        {
          "name": "tasks",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 102.720703125,
          "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2015-present ScyllaDB\n#\n#\n# SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n#\nimport argparse\nimport asyncio\nimport collections\nimport colorama\nimport difflib\nimport filecmp\nimport glob\nimport itertools\nimport logging\nimport multiprocessing\nimport os\nimport pathlib\nimport re\nimport resource\nimport shlex\nimport shutil\nimport signal\nimport subprocess\nimport sys\nimport time\nimport traceback\nimport xml.etree.ElementTree as ET\nimport yaml\n\nfrom abc import ABC, abstractmethod\nfrom io import StringIO\nfrom scripts import coverage    # type: ignore\nfrom test.pylib.artifact_registry import ArtifactRegistry\nfrom test.pylib.host_registry import HostRegistry\nfrom test.pylib.pool import Pool\nfrom test.pylib.s3_proxy import S3ProxyServer\nfrom test.pylib.s3_server_mock import MockS3Server\nfrom test.pylib.resource_gather import setup_cgroup, run_resource_watcher, get_resource_gather\nfrom test.pylib.util import LogPrefixAdapter\nfrom test.pylib.scylla_cluster import ScyllaServer, ScyllaCluster, get_cluster_manager, merge_cmdline_options\nfrom test.pylib.minio_server import MinioServer\nfrom typing import Dict, List, Callable, Any, Iterable, Optional, Awaitable, Union\nimport logging\nfrom test.pylib import coverage_utils\nimport humanfriendly\nimport treelib\n\nlaunch_time = time.monotonic()\n\noutput_is_a_tty = sys.stdout.isatty()\n\nall_modes = {'debug': 'Debug',\n             'release': 'RelWithDebInfo',\n             'dev': 'Dev',\n             'sanitize': 'Sanitize',\n             'coverage': 'Coverage'}\ndebug_modes = {'debug', 'sanitize'}\n\ndef create_formatter(*decorators) -> Callable[[Any], str]:\n    \"\"\"Return a function which decorates its argument with the given\n    color/style if stdout is a tty, and leaves intact otherwise.\"\"\"\n    def color(arg: Any) -> str:\n        return \"\".join(decorators) + str(arg) + colorama.Style.RESET_ALL\n\n    def nocolor(arg: Any) -> str:\n        return str(arg)\n    return color if output_is_a_tty else nocolor\n\n\nclass palette:\n    \"\"\"Color palette for formatting terminal output\"\"\"\n    ok = create_formatter(colorama.Fore.GREEN, colorama.Style.BRIGHT)\n    fail = create_formatter(colorama.Fore.RED, colorama.Style.BRIGHT)\n    new = create_formatter(colorama.Fore.BLUE)\n    skip = create_formatter(colorama.Style.DIM)\n    path = create_formatter(colorama.Style.BRIGHT)\n    diff_in = create_formatter(colorama.Fore.GREEN)\n    diff_out = create_formatter(colorama.Fore.RED)\n    diff_mark = create_formatter(colorama.Fore.MAGENTA)\n    warn = create_formatter(colorama.Fore.YELLOW)\n    crit = create_formatter(colorama.Fore.RED, colorama.Style.BRIGHT)\n    ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n    @staticmethod\n    def nocolor(text: str) -> str:\n        return palette.ansi_escape.sub('', text)\n\n\ndef path_to(mode, *components):\n    \"\"\"Resolve path to built executable\"\"\"\n    build_dir = 'build'\n    if os.path.exists(os.path.join(build_dir, 'build.ninja')):\n        *dir_components, basename = components\n        return os.path.join(build_dir, *dir_components, all_modes[mode], basename)\n    return os.path.join(build_dir, mode, *components)\n\n\ndef ninja(target):\n    \"\"\"Build specified target using ninja\"\"\"\n    build_dir = 'build'\n    args = ['ninja', target]\n    if os.path.exists(os.path.join(build_dir, 'build.ninja')):\n        args = ['ninja', '-C', build_dir, target]\n    return subprocess.Popen(args, stdout=subprocess.PIPE).communicate()[0].decode()\n\n\nclass TestSuite(ABC):\n    \"\"\"A test suite is a folder with tests of the same type.\n    E.g. it can be unit tests, boost tests, or CQL tests.\"\"\"\n\n    # All existing test suites, one suite per path/mode.\n    suites: Dict[str, 'TestSuite'] = dict()\n    artifacts = ArtifactRegistry()\n    hosts = HostRegistry()\n    FLAKY_RETRIES = 5\n    _next_id = collections.defaultdict(int) # (test_key -> id)\n\n    def __init__(self, path: str, cfg: dict, options: argparse.Namespace, mode: str) -> None:\n        self.suite_path = pathlib.Path(path)\n        self.name = str(self.suite_path.name)\n        self.cfg = cfg\n        self.options = options\n        self.mode = mode\n        self.suite_key = os.path.join(path, mode)\n        self.tests: List['Test'] = []\n        self.pending_test_count = 0\n        # The number of failed tests\n        self.n_failed = 0\n\n        self.run_first_tests = set(cfg.get(\"run_first\", []))\n        self.no_parallel_cases = set(cfg.get(\"no_parallel_cases\", []))\n        # Skip tests disabled in suite.yaml\n        self.disabled_tests = set(self.cfg.get(\"disable\", []))\n        # Skip tests disabled in specific mode.\n        self.disabled_tests.update(self.cfg.get(\"skip_in_\" + mode, []))\n        self.flaky_tests = set(self.cfg.get(\"flaky\", []))\n        # If this mode is one of the debug modes, and there are\n        # tests disabled in a debug mode, add these tests to the skip list.\n        if mode in debug_modes:\n            self.disabled_tests.update(self.cfg.get(\"skip_in_debug_modes\", []))\n        # If a test is listed in run_in_<mode>, it should only be enabled in\n        # this mode. Tests not listed in any run_in_<mode> directive should\n        # run in all modes. Inversing this, we should disable all tests\n        # which are listed explicitly in some run_in_<m> where m != mode\n        # This of course may create ambiguity with skip_* settings,\n        # since the priority of the two is undefined, but oh well.\n        run_in_m = set(self.cfg.get(\"run_in_\" + mode, []))\n        for a in all_modes:\n            if a == mode:\n                continue\n            skip_in_m = set(self.cfg.get(\"run_in_\" + a, []))\n            self.disabled_tests.update(skip_in_m - run_in_m)\n        # environment variables that should be the base of all processes running in this suit\n        self.base_env = {}\n        if self.need_coverage():\n            # Set the coverage data from each instrumented object to use the same file (and merged into it with locking)\n            # as long as we don't need test specific coverage data, this looks sufficient. The benefit of doing this in\n            # this way is that the storage will not be bloated with coverage files (each can weigh 10s of MBs so for several\n            # thousands of tests it can easily reach 10 of GBs)\n            # ref: https://clang.llvm.org/docs/SourceBasedCodeCoverage.html#running-the-instrumented-program\n            self.base_env[\"LLVM_PROFILE_FILE\"] = os.path.join(options.tmpdir,self.mode, \"coverage\", self.name, \"%m.profraw\")\n    # Generate a unique ID for `--repeat`ed tests\n    # We want these tests to have different XML IDs so test result\n    # processors (Jenkins) don't merge results for different iterations of\n    # the same test. We also don't want the ids to be too random, because then\n    # there is no correlation between test identifiers across multiple\n    # runs of test.py, and so it's hard to understand failure trends. The\n    # compromise is for next_id() results to be unique only within a particular\n    # test case. That is, we'll have a.1, a.2, a.3, b.1, b.2, b.3 rather than\n    # a.1 a.2 a.3 b.4 b.5 b.6.\n    def next_id(self, test_key) -> int:\n        TestSuite._next_id[test_key] += 1\n        return TestSuite._next_id[test_key]\n\n    @staticmethod\n    def test_count() -> int:\n        return sum(TestSuite._next_id.values())\n\n    @staticmethod\n    def load_cfg(path: str) -> dict:\n        with open(os.path.join(path, \"suite.yaml\"), \"r\") as cfg_file:\n            cfg = yaml.safe_load(cfg_file.read())\n            if not isinstance(cfg, dict):\n                raise RuntimeError(\"Failed to load tests in {}: suite.yaml is empty\".format(path))\n            return cfg\n\n    @staticmethod\n    def opt_create(path: str, options: argparse.Namespace, mode: str) -> 'TestSuite':\n        \"\"\"Return a subclass of TestSuite with name cfg[\"type\"].title + TestSuite.\n        Ensures there is only one suite instance per path.\"\"\"\n        suite_key = os.path.join(path, mode)\n        suite = TestSuite.suites.get(suite_key)\n        if not suite:\n            cfg = TestSuite.load_cfg(path)\n            kind = cfg.get(\"type\")\n            if kind is None:\n                raise RuntimeError(\"Failed to load tests in {}: suite.yaml has no suite type\".format(path))\n\n            def suite_type_to_class_name(suite_type: str) -> str:\n                if suite_type.casefold() == \"Approval\".casefold():\n                    suite_type = \"CQLApproval\"\n                else:\n                    suite_type = suite_type.title()\n                return suite_type + \"TestSuite\"\n\n            SpecificTestSuite = globals().get(suite_type_to_class_name(kind))\n            if not SpecificTestSuite:\n                raise RuntimeError(\"Failed to load tests in {}: suite type '{}' not found\".format(path, kind))\n            suite = SpecificTestSuite(path, cfg, options, mode)\n            assert suite is not None\n            TestSuite.suites[suite_key] = suite\n        return suite\n\n    @staticmethod\n    def all_tests() -> Iterable['Test']:\n        return itertools.chain(*(suite.tests for suite in\n                                 TestSuite.suites.values()))\n\n    @property\n    @abstractmethod\n    def pattern(self) -> str:\n        pass\n\n    @abstractmethod\n    async def add_test(self, shortname: str, casename: str) -> None:\n        pass\n\n    async def run(self, test: 'Test', options: argparse.Namespace):\n        try:\n            test.started = True\n            for i in range(1, self.FLAKY_RETRIES):\n                if i > 1:\n                    test.is_flaky_failure = True\n                    logging.info(\"Retrying test %s after a flaky fail, retry %d\", test.uname, i)\n                    test.reset()\n                await test.run(options)\n                if test.success or not test.is_flaky or test.is_cancelled:\n                    break\n        except asyncio.CancelledError:\n            test.is_cancelled = True\n            raise\n        finally:\n            self.pending_test_count -= 1\n            self.n_failed += int(test.failed)\n            if self.pending_test_count == 0:\n                await TestSuite.artifacts.cleanup_after_suite(self, self.n_failed > 0)\n        return test\n\n    def junit_tests(self):\n        \"\"\"Tests which participate in a consolidated junit report\"\"\"\n        return self.tests\n\n    def boost_tests(self):\n        return []\n\n    def build_test_list(self) -> List[str]:\n        return [os.path.splitext(t.relative_to(self.suite_path))[0] for t in\n                self.suite_path.glob(self.pattern)]\n\n    async def add_test_list(self) -> None:\n        options = self.options\n        lst = self.build_test_list()\n        if lst:\n            # Some tests are long and are better to be started earlier,\n            # so pop them up while sorting the list\n            lst.sort(key=lambda x: (x not in self.run_first_tests, x))\n\n        pending = set()\n        for shortname in lst:\n            testname = os.path.join(self.name, shortname)\n            casename = None\n\n            # Check opt-out lists\n            if shortname in self.disabled_tests:\n                continue\n            if options.skip_patterns:\n                if any(skip_pattern in testname for skip_pattern in options.skip_patterns):\n                    continue\n\n            # Check opt-in list\n            if options.name:\n                for p in options.name:\n                    pn = p.split('::', 2)\n                    if len(pn) == 1 and p in testname:\n                        break\n                    if len(pn) == 2 and pn[0] == testname:\n                        if pn[1] != \"*\":\n                            casename = pn[1]\n                        break\n                else:\n                    continue\n\n            async def add_test(shortname, casename) -> None:\n                # Add variants of the same test sequentially\n                # so that case cache has a chance to populate\n                for i in range(options.repeat):\n                    await self.add_test(shortname, casename)\n                    self.pending_test_count += 1\n\n            pending.add(asyncio.create_task(add_test(shortname, casename)))\n\n        if len(pending) == 0:\n            return\n        try:\n            await asyncio.gather(*pending)\n        except asyncio.CancelledError:\n            for task in pending:\n                task.cancel()\n            await asyncio.gather(*pending, return_exceptions=True)\n            raise\n    def need_coverage(self):\n        return self.options.coverage and (self.mode in self.options.coverage_modes) and bool(self.cfg.get(\"coverage\",True))\n\nclass UnitTestSuite(TestSuite):\n    \"\"\"TestSuite instantiation for non-boost unit tests\"\"\"\n\n    def __init__(self, path: str, cfg: dict, options: argparse.Namespace, mode: str) -> None:\n        super().__init__(path, cfg, options, mode)\n        # Map of custom test command line arguments, if configured\n        self.custom_args = cfg.get(\"custom_args\", {})\n        # Map of tests that cannot run with compaction groups\n        self.all_can_run_compaction_groups_except = cfg.get(\"all_can_run_compaction_groups_except\")\n\n    async def create_test(self, shortname, casename, suite, args):\n        exe = path_to(suite.mode, \"test\", suite.name, shortname)\n        if not os.access(exe, os.X_OK):\n            print(palette.warn(f\"Unit test executable {exe} not found.\"))\n            return\n        test = UnitTest(self.next_id((shortname, self.suite_key)), shortname, suite, args)\n        self.tests.append(test)\n\n    async def add_test(self, shortname, casename) -> None:\n        \"\"\"Create a UnitTest class with possibly custom command line\n        arguments and add it to the list of tests\"\"\"\n        # Skip tests which are not configured, and hence are not built\n        if os.path.join(\"test\", self.name, shortname) not in self.options.tests:\n            return\n\n        # Default seastar arguments, if not provided in custom test options,\n        # are two cores and 2G of RAM\n        args = self.custom_args.get(shortname, [\"-c2 -m2G\"])\n        args = merge_cmdline_options(args, self.options.extra_scylla_cmdline_options)\n        for a in args:\n            await self.create_test(shortname, casename, self, a)\n\n    @property\n    def pattern(self) -> str:\n        # This should only match individual tests and not combined_tests.cc\n        # file of the combined test.\n        # It is because combined_tests.cc itself does not contain any tests.\n        # To keep the code simple, we have avoided this by renaming\n        # combined test file to _tests.cc instead of changing the match\n        # pattern.\n        return \"*_test.cc\"\n\n\nclass BoostTestSuite(UnitTestSuite):\n    \"\"\"TestSuite for boost unit tests\"\"\"\n\n    # A cache of individual test cases, for which we have called\n    # --list_content. Static to share across all modes.\n    _case_cache: Dict[str, List[str]] = dict()\n\n    _exec_name_cache: Dict[str, str] = dict()\n\n    def _generate_cache(self, exec_path, exec_name) -> None:\n        res = subprocess.run(\n                [exec_path, '--list_content'],\n                check=True,\n                capture_output=True,\n                env=dict(os.environ,\n                         **{\"ASAN_OPTIONS\": \"halt_on_error=0\"}),\n            )\n        testname = None\n        fqname = None\n        for line in res.stderr.decode().splitlines():\n            if not line.startswith('    '):\n                testname = line.strip().rstrip('*')\n                fqname = os.path.join(self.mode, self.name, testname)\n                self._exec_name_cache[fqname] = exec_name\n                self._case_cache[fqname] = []\n            else:\n                casename = line.strip().rstrip('*')\n                if casename.startswith('_'):\n                    continue\n                self._case_cache[fqname].append(casename)\n\n    def __init__(self, path, cfg: dict, options: argparse.Namespace, mode) -> None:\n        super().__init__(path, cfg, options, mode)\n        exec_name = 'combined_tests'\n        exec_path = path_to(self.mode, \"test\", self.name, exec_name)\n        # Apply combined test only for test/boost,\n        # cache the tests only if the executable exists, so we can\n        # run test.py with a partially built tree\n        if self.name == 'boost' and os.path.exists(exec_path):\n            self._generate_cache(exec_path, exec_name)\n\n    async def create_test(self, shortname: str, casename: str, suite, args) -> None:\n        fqname = os.path.join(self.mode, self.name, shortname)\n        if fqname in self._exec_name_cache:\n            execname = self._exec_name_cache[fqname]\n            combined_test = True\n        else:\n            execname = None\n            combined_test = False\n        exe = path_to(suite.mode, \"test\", suite.name, execname if combined_test else shortname)\n        if not os.access(exe, os.X_OK):\n            print(palette.warn(f\"Boost test executable {exe} not found.\"))\n            return\n        options = self.options\n        allows_compaction_groups = self.all_can_run_compaction_groups_except != None and shortname not in self.all_can_run_compaction_groups_except\n        if options.parallel_cases and (shortname not in self.no_parallel_cases) and casename is None:\n            fqname = os.path.join(self.mode, self.name, shortname)\n            # since combined tests are preloaded to self._case_cache, this will\n            # only run in non-combined test mode\n            if fqname not in self._case_cache:\n                process = await asyncio.create_subprocess_exec(\n                    exe, *['--list_content'],\n                    stderr=asyncio.subprocess.PIPE,\n                    stdout=asyncio.subprocess.PIPE,\n                    env=dict(os.environ,\n                             **{\"ASAN_OPTIONS\": \"halt_on_error=0\"}),\n                    preexec_fn=os.setsid,\n                )\n                _, stderr = await asyncio.wait_for(process.communicate(), options.timeout)\n                # --list_content produces the list of all test cases in the file. When BOOST_DATA_TEST_CASE is used it\n                # will additionally produce the lines with numbers for each case preserving the function name like this:\n                # test_singular_tree_ptr_sz*\n                #     _0*\n                #     _1*\n                #     _2*\n                # however, it's only possible to run test_singular_tree_ptr_sz that will execute all test cases\n                # this line catches only test function name ignoring unrelated lines like '_0'\n                # Note: this will ignore any test case starting with a '_' symbol\n                case_list = [case[:-1] for case in stderr.decode().splitlines() if case.endswith('*') and not case.strip().startswith('_')]\n                self._case_cache[fqname] = case_list\n\n            case_list = self._case_cache[fqname]\n            if len(case_list) == 1:\n                test = BoostTest(self.next_id((shortname, self.suite_key)), shortname, suite, args, None, allows_compaction_groups, execname)\n                self.tests.append(test)\n            else:\n                for case in case_list:\n                    test = BoostTest(self.next_id((shortname, self.suite_key, case)), shortname, suite, args, case, allows_compaction_groups, execname)\n                    self.tests.append(test)\n        else:\n            test = BoostTest(self.next_id((shortname, self.suite_key)), shortname, suite, args, casename, allows_compaction_groups, execname)\n            self.tests.append(test)\n\n    async def add_test(self, shortname, casename) -> None:\n        \"\"\"Create a UnitTest class with possibly custom command line\n        arguments and add it to the list of tests\"\"\"\n        fqname = os.path.join(self.mode, self.name, shortname)\n        if fqname in self._exec_name_cache:\n            execname = self._exec_name_cache[fqname]\n            combined_test = True\n        else:\n            combined_test = False\n        # Skip tests which are not configured, and hence are not built\n        if os.path.join(\"test\", self.name, execname if combined_test else shortname) not in self.options.tests:\n                return\n\n        # Default seastar arguments, if not provided in custom test options,\n        # are two cores and 2G of RAM\n        args = self.custom_args.get(shortname, [\"-c2 -m2G\"])\n        args = merge_cmdline_options(args, self.options.extra_scylla_cmdline_options)\n        for a in args:\n            await self.create_test(shortname, casename, self, a)\n\n    def junit_tests(self) -> Iterable['Test']:\n        \"\"\"Boost tests produce an own XML output, so are not included in a junit report\"\"\"\n        return []\n\n    def boost_tests(self) -> Iterable['Tests']:\n        return self.tests\n\nclass PythonTestSuite(TestSuite):\n    \"\"\"A collection of Python pytests against a single Scylla instance\"\"\"\n\n    def __init__(self, path, cfg: dict, options: argparse.Namespace, mode: str) -> None:\n        super().__init__(path, cfg, options, mode)\n        self.scylla_exe = path_to(self.mode, \"scylla\")\n        self.scylla_env = dict(self.base_env)\n        if self.mode == \"coverage\":\n            self.scylla_env.update(coverage.env(self.scylla_exe, distinct_id=self.name))\n        self.scylla_env['SCYLLA'] = self.scylla_exe\n\n        cluster_cfg = self.cfg.get(\"cluster\", {\"initial_size\": 1})\n        cluster_size = cluster_cfg[\"initial_size\"]\n        env_pool_size = os.getenv(\"CLUSTER_POOL_SIZE\")\n        if options.cluster_pool_size is not None:\n            pool_size = options.cluster_pool_size\n        elif env_pool_size is not None:\n            pool_size = int(env_pool_size)\n        else:\n            pool_size = cfg.get(\"pool_size\", 2)\n        self.dirties_cluster = set(cfg.get(\"dirties_cluster\", []))\n\n        self.create_cluster = self.get_cluster_factory(cluster_size, options)\n        async def recycle_cluster(cluster: ScyllaCluster) -> None:\n            \"\"\"When a dirty cluster is returned to the cluster pool,\n               stop it and release the used IPs. We don't necessarily uninstall() it yet,\n               which would delete the log file and directory - we might want to preserve\n               these if it came from a failed test.\n            \"\"\"\n            for srv in cluster.running.values():\n                srv.log_file.close()\n                srv.maintenance_socket_dir.cleanup()\n            await cluster.stop()\n            await cluster.release_ips()\n\n        self.clusters = Pool(pool_size, self.create_cluster, recycle_cluster)\n\n    def get_cluster_factory(self, cluster_size: int, options: argparse.Namespace) -> Callable[..., Awaitable]:\n        def create_server(create_cfg: ScyllaCluster.CreateServerParams):\n            cmdline_options = self.cfg.get(\"extra_scylla_cmdline_options\", [])\n            if type(cmdline_options) == str:\n                cmdline_options = [cmdline_options]\n            cmdline_options = merge_cmdline_options(cmdline_options, create_cfg.cmdline_from_test)\n            cmdline_options = merge_cmdline_options(cmdline_options, options.extra_scylla_cmdline_options)\n            # There are multiple sources of config options, with increasing priority\n            # (if two sources provide the same config option, the higher priority one wins):\n            # 1. the defaults\n            # 2. suite-specific config options (in \"extra_scylla_config_options\")\n            # 3. config options from tests (when servers are added during a test)\n            default_config_options = \\\n                    {\"authenticator\": \"PasswordAuthenticator\",\n                     \"authorizer\": \"CassandraAuthorizer\"}\n            config_options = default_config_options | \\\n                             self.cfg.get(\"extra_scylla_config_options\", {}) | \\\n                             create_cfg.config_from_test\n\n            server = ScyllaServer(\n                mode=self.mode,\n                exe=self.scylla_exe,\n                vardir=os.path.join(self.options.tmpdir, self.mode),\n                logger=create_cfg.logger,\n                cluster_name=create_cfg.cluster_name,\n                ip_addr=create_cfg.ip_addr,\n                seeds=create_cfg.seeds,\n                cmdline_options=cmdline_options,\n                config_options=config_options,\n                property_file=create_cfg.property_file,\n                append_env=self.base_env,\n                server_encryption=create_cfg.server_encryption)\n\n            return server\n\n        async def create_cluster(logger: Union[logging.Logger, logging.LoggerAdapter]) -> ScyllaCluster:\n            cluster = ScyllaCluster(logger, self.hosts, cluster_size, create_server)\n\n            async def stop() -> None:\n                await cluster.stop()\n\n            # Suite artifacts are removed when\n            # the entire suite ends successfully.\n            self.artifacts.add_suite_artifact(self, stop)\n            if not self.options.save_log_on_success:\n                # If a test fails, we might want to keep the data dirs.\n                async def uninstall() -> None:\n                    await cluster.uninstall()\n\n                self.artifacts.add_suite_artifact(self, uninstall)\n            self.artifacts.add_exit_artifact(self, stop)\n\n            await cluster.install_and_start()\n            return cluster\n\n        return create_cluster\n\n    def build_test_list(self) -> List[str]:\n        \"\"\"For pytest, search for directories recursively\"\"\"\n        path = self.suite_path\n        pytests = itertools.chain(path.rglob(\"*_test.py\"), path.rglob(\"test_*.py\"))\n        return [os.path.splitext(t.relative_to(self.suite_path))[0] for t in pytests]\n\n    @property\n    def pattern(self) -> str:\n        assert False\n\n    async def add_test(self, shortname, casename) -> None:\n        test = PythonTest(self.next_id((shortname, self.suite_key)), shortname, casename, self)\n        self.tests.append(test)\n\n    async def run(self, test: 'Test', options: argparse.Namespace):\n        if not os.access(self.scylla_exe, os.F_OK):\n            raise FileNotFoundError(f\"{self.scylla_exe} does not exist.\")\n        if not os.access(self.scylla_exe, os.X_OK):\n            raise PermissionError(f\"{self.scylla_exe} is not executable.\")\n        return await super().run(test, options)\n\n\nclass CQLApprovalTestSuite(PythonTestSuite):\n    \"\"\"Run CQL commands against a single Scylla instance\"\"\"\n\n    def __init__(self, path, cfg, options: argparse.Namespace, mode) -> None:\n        super().__init__(path, cfg, options, mode)\n\n    def build_test_list(self) -> List[str]:\n        return TestSuite.build_test_list(self)\n\n    async def add_test(self, shortname: str, casename: str) -> None:\n        test = CQLApprovalTest(self.next_id((shortname, self.suite_key)), shortname, self)\n        self.tests.append(test)\n\n    @property\n    def pattern(self) -> str:\n        return \"*test.cql\"\n\n\nclass TopologyTestSuite(PythonTestSuite):\n    \"\"\"A collection of Python pytests against Scylla instances dealing with topology changes.\n       Instead of using a single Scylla cluster directly, there is a cluster manager handling\n       the lifecycle of clusters and bringing up new ones as needed. The cluster health checks\n       are done per test case.\n    \"\"\"\n\n    def build_test_list(self) -> List[str]:\n        \"\"\"Build list of Topology python tests\"\"\"\n        return TestSuite.build_test_list(self)\n\n    async def add_test(self, shortname: str, casename: str) -> None:\n        \"\"\"Add test to suite\"\"\"\n        test = TopologyTest(self.next_id((shortname, 'topology', self.mode)), shortname, casename, self)\n        self.tests.append(test)\n\n    @property\n    def pattern(self) -> str:\n        \"\"\"Python pattern\"\"\"\n        return \"test_*.py\"\n\n    def junit_tests(self):\n        \"\"\"Return an empty list, since topology tests are excluded from an aggregated Junit report to prevent double\n        count in the CI report\"\"\"\n        return []\n\n\nclass RunTestSuite(TestSuite):\n    \"\"\"TestSuite for test directory with a 'run' script \"\"\"\n\n    def __init__(self, path: str, cfg, options: argparse.Namespace, mode: str) -> None:\n        super().__init__(path, cfg, options, mode)\n        self.scylla_exe = path_to(self.mode, \"scylla\")\n        self.scylla_env = dict(self.base_env)\n        if self.mode == \"coverage\":\n            self.scylla_env = coverage.env(self.scylla_exe, distinct_id=self.name)\n\n        self.scylla_env['SCYLLA'] = self.scylla_exe\n\n    async def add_test(self, shortname, casename) -> None:\n        test = RunTest(self.next_id((shortname, self.suite_key)), shortname, self)\n        self.tests.append(test)\n\n    @property\n    def pattern(self) -> str:\n        return \"run\"\n\n\nclass ToolTestSuite(TestSuite):\n    \"\"\"A collection of Python pytests that test tools\n\n    These tests do not need an cluster setup for them. They invoke scylla\n    manually, in tool mode.\n    \"\"\"\n\n    def __init__(self, path, cfg: dict, options: argparse.Namespace, mode: str) -> None:\n        super().__init__(path, cfg, options, mode)\n\n    def build_test_list(self) -> List[str]:\n        \"\"\"For pytest, search for directories recursively\"\"\"\n        path = self.suite_path\n        pytests = itertools.chain(path.rglob(\"*_test.py\"), path.rglob(\"test_*.py\"))\n        return [os.path.splitext(t.relative_to(self.suite_path))[0] for t in pytests]\n\n    @property\n    def pattern(self) -> str:\n        assert False\n\n    async def add_test(self, shortname, casename) -> None:\n        test = ToolTest(self.next_id((shortname, self.suite_key)), shortname, self)\n        self.tests.append(test)\n\n\nclass Test:\n    \"\"\"Base class for CQL, Unit and Boost tests\"\"\"\n    def __init__(self, test_no: int, shortname: str, suite) -> None:\n        self.id = test_no\n        self.path = \"\"\n        self.args: List[str] = []\n        # Arguments which are required by a program regardless of additional test specific arguments\n        self.core_args : List[str] = []\n        self.valid_exit_codes = [0]\n        # Name with test suite name\n        self.name = os.path.join(suite.name, shortname.split('.')[0])\n        # Name within the suite\n        self.shortname = shortname\n        self.mode = suite.mode\n        self.suite = suite\n        self.allure_dir = pathlib.Path(suite.options.tmpdir) / self.mode / 'allure'\n        # Unique file name, which is also readable by human, as filename prefix\n        self.uname = \"{}.{}.{}\".format(self.suite.name, self.shortname, self.id)\n        self.log_filename = pathlib.Path(suite.options.tmpdir) / self.mode / (self.uname + \".log\")\n        self.log_filename.parent.mkdir(parents=True, exist_ok=True)\n        self.is_flaky = self.shortname in suite.flaky_tests\n        # True if the test was retried after it failed\n        self.is_flaky_failure = False\n        # True if the test was cancelled by a ctrl-c or timeout, so\n        # shouldn't be retried, even if it is flaky\n        self.is_cancelled = False\n        self.env = dict(self.suite.base_env)\n        self.started = False\n        self.success = False\n        self.time_start: float = 0\n        self.time_end: float = 0\n\n    def reset(self) -> None:\n        \"\"\"Reset the test before a retry, if it is retried as flaky\"\"\"\n        self.success = False\n        self.time_start = 0\n        self.time_end = 0\n\n    @property\n    def failed(self):\n        \"\"\"Returns True, if this test Failed\"\"\"\n        return self.started and not self.success and not self.is_cancelled\n\n    @property\n    def did_not_run(self):\n        \"\"\"Returns True, if this test did not run correctly, i.e. was canceled either during or before execution\"\"\"\n        return not self.started or self.is_cancelled\n\n    @abstractmethod\n    async def run(self, options: argparse.Namespace) -> 'Test':\n        pass\n\n    @abstractmethod\n    def print_summary(self) -> None:\n        pass\n\n    def check_log(self, trim: bool) -> None:\n        \"\"\"Check and trim logs and xml output for tests which have it\"\"\"\n        if trim:\n            self.log_filename.unlink()\n        pass\n\n    def write_junit_failure_report(self, xml_res: ET.Element) -> None:\n        assert not self.success\n        xml_fail = ET.SubElement(xml_res, 'failure')\n        xml_fail.text = \"Test {} {} failed, check the log at {}\".format(\n            self.path,\n            \" \".join(self.args),\n            self.log_filename)\n        if self.log_filename.exists():\n            system_out = ET.SubElement(xml_res, 'system-out')\n            system_out.text = read_log(self.log_filename)\n\n\nclass UnitTest(Test):\n    standard_args = shlex.split(\"--overprovisioned --unsafe-bypass-fsync 1 \"\n                                \"--kernel-page-cache 1 \"\n                                \"--blocked-reactor-notify-ms 2000000 --collectd 0 \"\n                                \"--max-networking-io-control-blocks=100 \")\n\n    def __init__(self, test_no: int, shortname: str, suite, args: str) -> None:\n        super().__init__(test_no, shortname, suite)\n        self.path = path_to(self.mode, \"test\", suite.name, shortname.split('.')[0])\n        self.args = shlex.split(args) + UnitTest.standard_args\n        if self.mode == \"coverage\":\n            self.env.update(coverage.env(self.path))\n\n    def print_summary(self) -> None:\n        print(\"Output of {} {}:\".format(self.path, \" \".join(self.args)))\n        print(read_log(self.log_filename))\n\n    async def run(self, options) -> Test:\n        self.success = await run_test(self, options, env=self.env)\n        logging.info(\"Test %s %s\", self.uname, \"succeeded\" if self.success else \"failed \")\n        return self\n\n\nTestPath = collections.namedtuple('TestPath', ['suite_name', 'test_name', 'case_name'])\n\nclass BoostTest(Test):\n    \"\"\"A unit test which can produce its own XML output\"\"\"\n\n    standard_args = shlex.split(\"--overprovisioned --unsafe-bypass-fsync 1 \"\n                                \"--kernel-page-cache 1 \"\n                                \"--blocked-reactor-notify-ms 2000000 --collectd 0 \"\n                                \"--max-networking-io-control-blocks=100 \")\n\n    def __init__(self, test_no: int, shortname: str, suite, args: str,\n                 casename: Optional[str], allows_compaction_groups : bool, execname: Optional[str]) -> None:\n        boost_args = []\n        combined_test = True if execname else False\n        _shortname = shortname\n        if casename:\n            shortname += '.' + casename\n            if combined_test:\n                boost_args += ['--run_test=' + _shortname + '/' + casename]\n            else:\n                boost_args += ['--run_test=' + casename]\n        else:\n            if combined_test:\n                boost_args += ['--run_test=' + _shortname]\n\n        super().__init__(test_no, shortname, suite)\n        if combined_test:\n            self.path = path_to(self.mode, \"test\", suite.name, execname)\n        else:\n            self.path = path_to(self.mode, \"test\", suite.name, shortname.split('.')[0])\n        self.args = shlex.split(args) + UnitTest.standard_args\n        if self.mode == \"coverage\":\n            self.env.update(coverage.env(self.path))\n\n        self.xmlout = os.path.join(suite.options.tmpdir, self.mode, \"xml\", self.uname + \".xunit.xml\")\n        boost_args += ['--report_level=no',\n                       '--logger=HRF,test_suite:XML,test_suite,' + self.xmlout]\n        boost_args += ['--catch_system_errors=no']  # causes undebuggable cores\n        boost_args += ['--color_output=false']\n        boost_args += ['--']\n        self.args = boost_args + self.args\n        self.casename = casename\n        self.__test_case_elements: list[ET.Element] = []\n        self.allows_compaction_groups = allows_compaction_groups\n\n    def reset(self) -> None:\n        \"\"\"Reset the test before a retry, if it is retried as flaky\"\"\"\n        super().reset()\n        self.__test_case_elements = []\n\n    def get_test_cases(self) -> list[ET.Element]:\n        if not self.__test_case_elements:\n            self.__parse_logger()\n        return self.__test_case_elements\n\n    @staticmethod\n    def test_path_of_element(test: ET.Element) -> TestPath:\n        path = test.attrib['path']\n        prefix, case_name = path.rsplit('::', 1)\n        suite_name, test_name = prefix.split('.', 1)\n        return TestPath(suite_name, test_name, case_name)\n\n    def __parse_logger(self) -> None:\n        def attach_path_and_mode(test):\n            # attach the \"path\" to the test so we can group the tests by this string\n            test_name = test.attrib['name']\n            prefix = self.name.replace(os.path.sep, '.')\n            test.attrib['path'] = f'{prefix}::{test_name}'\n            test.attrib['mode'] = self.mode\n            return test\n\n        try:\n            root = ET.parse(self.xmlout).getroot()\n            # only keep the tests which actually ran, the skipped ones do not have\n            # TestingTime tag in the corresponding TestCase tag.\n            self.__test_case_elements = map(attach_path_and_mode,\n                                            root.findall(\".//TestCase[TestingTime]\"))\n            os.unlink(self.xmlout)\n        except ET.ParseError as e:\n            message = palette.crit(f\"failed to parse XML output '{self.xmlout}': {e}\")\n            if e.msg.__contains__(\"no element found\"):\n                message = palette.crit(f\"Empty testcase XML output, possibly caused by a crash in the cql_test_env.cc, \"\n                                       f\"details: '{self.xmlout}': {e}\")\n            print(f\"error: {self.name}: {message}\")\n\n    def check_log(self, trim: bool) -> None:\n        self.__parse_logger()\n        super().check_log(trim)\n\n    async def run(self, options):\n        if options.random_seed:\n            self.args += ['--random-seed', options.random_seed]\n        if self.allows_compaction_groups and options.x_log2_compaction_groups:\n            self.args += [ \"--x-log2-compaction-groups\", str(options.x_log2_compaction_groups) ]\n        self.success = await run_test(self, options, env=self.env)\n        logging.info(\"Test %s %s\", self.uname, \"succeeded\" if self.success else \"failed \")\n        return self\n\n    def write_junit_failure_report(self, xml_res: ET.Element) -> None:\n        \"\"\"Does not write junit report for Jenkins legacy reasons\"\"\"\n        assert False\n\n    def print_summary(self) -> None:\n        print(\"Output of {} {}:\".format(self.path, \" \".join(self.args)))\n        print(read_log(self.log_filename))\n\n\nclass CQLApprovalTest(Test):\n    \"\"\"Run a sequence of CQL commands against a standalone Scylla\"\"\"\n\n    def __init__(self, test_no: int, shortname: str, suite) -> None:\n        super().__init__(test_no, shortname, suite)\n        # Path to cql_repl driver, in the given build mode\n        self.path = \"pytest\"\n        self.cql = suite.suite_path / (self.shortname + \".cql\")\n        self.result = suite.suite_path / (self.shortname + \".result\")\n        self.tmpfile = os.path.join(suite.options.tmpdir, self.mode, self.uname + \".reject\")\n        self.reject = suite.suite_path / (self.shortname + \".reject\")\n        self.server_log: Optional[str] = None\n        self.server_log_filename: Optional[pathlib.Path] = None\n        self.is_before_test_ok = False\n        self.is_executed_ok = False\n        self.is_new = False\n        self.is_after_test_ok = False\n        self.is_equal_result = False\n        self.summary = \"not run\"\n        self.unidiff: Optional[str] = None\n        self.server_log = None\n        self.server_log_filename = None\n        self.env: Dict[str, str] = dict()\n        self._prepare_args(suite.options)\n\n    def reset(self) -> None:\n        \"\"\"Reset the test before a retry, if it is retried as flaky\"\"\"\n        super().reset()\n        self.is_before_test_ok = False\n        self.is_executed_ok = False\n        self.is_new = False\n        self.is_after_test_ok = False\n        self.is_equal_result = False\n        self.summary = \"not run\"\n        self.unidiff = None\n        self.server_log = None\n        self.server_log_filename = None\n        self.env = dict()\n        old_tmpfile = pathlib.Path(self.tmpfile)\n        if old_tmpfile.exists():\n            old_tmpfile.unlink()\n\n\n    def _prepare_args(self, options: argparse.Namespace):\n        self.args = [\n            \"-s\",  # don't capture print() inside pytest\n            \"test/pylib/cql_repl/cql_repl.py\",\n            \"--input={}\".format(self.cql),\n            \"--output={}\".format(self.tmpfile),\n            \"--run_id={}\".format(self.id),\n            \"--mode={}\".format(self.mode),\n        ]\n        self.args.append(f\"--alluredir={self.allure_dir}\")\n        if not options.save_log_on_success:\n            self.args.append(\"--allure-no-capture\")\n\n    async def run(self, options: argparse.Namespace) -> Test:\n        self.success = False\n        self.summary = \"failed\"\n\n        loggerPrefix = self.mode + '/' + self.uname\n        logger = LogPrefixAdapter(logging.getLogger(loggerPrefix), {'prefix': loggerPrefix})\n        def set_summary(summary):\n            self.summary = summary\n            log_func = logger.info if self.success else logger.error\n            log_func(\"Test %s %s\", self.uname, summary)\n            if self.server_log is not None:\n                logger.info(\"Server log:\\n%s\", self.server_log)\n\n        # TODO: consider dirty_on_exception=True\n        async with (cm := self.suite.clusters.instance(False, logger)) as cluster:\n            try:\n                cluster.before_test(self.uname)\n                logger.info(\"Leasing Scylla cluster %s for test %s\", cluster, self.uname)\n                self.args.insert(1, \"--host={}\".format(cluster.endpoint()))\n                # If pre-check fails, e.g. because Scylla failed to start\n                # or crashed between two tests, fail entire test.py\n                self.is_before_test_ok = True\n                cluster.take_log_savepoint()\n                self.is_executed_ok = await run_test(self, options, env=self.env)\n                cluster.after_test(self.uname, self.is_executed_ok)\n                cm.dirty = cluster.is_dirty\n                self.is_after_test_ok = True\n\n                if not self.is_executed_ok:\n                    set_summary(\"\"\"returned non-zero return status.\\n\nCheck test log at {}.\"\"\".format(self.log_filename))\n                elif not os.path.isfile(self.tmpfile):\n                    set_summary(\"failed: no output file\")\n                elif not os.path.isfile(self.result):\n                    set_summary(\"failed: no result file\")\n                    self.is_new = True\n                else:\n                    self.is_equal_result = filecmp.cmp(self.result, self.tmpfile)\n                    if not self.is_equal_result:\n                        self.unidiff = format_unidiff(str(self.result), self.tmpfile)\n                        set_summary(\"failed: test output does not match expected result\")\n                        assert self.unidiff is not None\n                        logger.info(\"\\n{}\".format(palette.nocolor(self.unidiff)))\n                    else:\n                        self.success = True\n                        set_summary(\"succeeded\")\n            except Exception as e:\n                # Server log bloats the output if we produce it in all\n                # cases. So only grab it when it's relevant:\n                # 1) failed pre-check, e.g. start failure\n                # 2) failed test execution.\n                if not self.is_executed_ok:\n                    self.server_log = cluster.read_server_log()\n                    self.server_log_filename = cluster.server_log_filename()\n                    if not self.is_before_test_ok:\n                        set_summary(\"pre-check failed: {}\".format(e))\n                        print(\"Test {} {}\".format(self.name, self.summary))\n                        print(\"Server log  of the first server:\\n{}\".format(self.server_log))\n                        # Don't try to continue if the cluster is broken\n                        raise\n                set_summary(\"failed: {}\".format(e))\n            finally:\n                if os.path.exists(self.tmpfile):\n                    if self.is_executed_ok and (self.is_new or not self.is_equal_result):\n                        # Move the .reject file close to the .result file\n                        # so that it's easy to analyze the diff or overwrite .result\n                        # with .reject.\n                        shutil.move(self.tmpfile, self.reject)\n                    else:\n                        pathlib.Path(self.tmpfile).unlink()\n\n        return self\n\n    def print_summary(self) -> None:\n        print(\"Test {} ({}) {}\".format(palette.path(self.name), self.mode,\n                                       self.summary))\n        if not self.is_executed_ok:\n            print(read_log(self.log_filename))\n            if self.server_log is not None:\n                print(\"Server log of the first server:\")\n                print(self.server_log)\n        elif not self.is_equal_result and self.unidiff:\n            print(self.unidiff)\n\n    def write_junit_failure_report(self, xml_res: ET.Element) -> None:\n        assert not self.success\n        xml_fail = ET.SubElement(xml_res, 'failure')\n        xml_fail.text = self.summary\n        if not self.is_executed_ok:\n            if self.log_filename.exists():\n                system_out = ET.SubElement(xml_res, 'system-out')\n                system_out.text = read_log(self.log_filename)\n            if self.server_log_filename:\n                system_err = ET.SubElement(xml_res, 'system-err')\n                system_err.text = read_log(self.server_log_filename)\n        elif self.unidiff:\n            system_out = ET.SubElement(xml_res, 'system-out')\n            system_out.text = palette.nocolor(self.unidiff)\n\n\nclass RunTest(Test):\n    \"\"\"Run tests in a directory started by a run script\"\"\"\n\n    def __init__(self, test_no: int, shortname: str, suite) -> None:\n        super().__init__(test_no, shortname, suite)\n        self.path = suite.suite_path / shortname\n        self.xmlout = os.path.join(suite.options.tmpdir, self.mode, \"xml\", self.uname + \".xunit.xml\")\n        self.args = [\n            \"--junit-xml={}\".format(self.xmlout),\n            \"-o\",\n            \"junit_suite_name={}\".format(self.suite.name)\n        ]\n        self.args.append(f\"--alluredir={self.allure_dir}\")\n        if not suite.options.save_log_on_success:\n            self.args.append(\"--allure-no-capture\")\n\n    def print_summary(self) -> None:\n        print(\"Output of {} {}:\".format(self.path, \" \".join(self.args)))\n        print(read_log(self.log_filename))\n\n    async def run(self, options: argparse.Namespace) -> Test:\n        # This test can and should be killed gently, with SIGTERM, not with SIGKILL\n        self.success = await run_test(self, options, gentle_kill=True, env=self.suite.scylla_env)\n        logging.info(\"Test %s %s\", self.uname, \"succeeded\" if self.success else \"failed \")\n        return self\n\n\nclass PythonTest(Test):\n    \"\"\"Run a pytest collection of cases against a standalone Scylla\"\"\"\n\n    def __init__(self, test_no: int, shortname: str, casename: str, suite) -> None:\n        super().__init__(test_no, shortname, suite)\n        self.path = \"python\"\n        self.core_args = [\"-m\", \"pytest\"]\n        self.casename = casename\n        self.xmlout = os.path.join(self.suite.options.tmpdir, self.mode, \"xml\", self.uname + \".xunit.xml\")\n        self.server_log: Optional[str] = None\n        self.server_log_filename: Optional[pathlib.Path] = None\n        self.is_before_test_ok = False\n        self.is_after_test_ok = False\n\n    def _prepare_pytest_params(self, options: argparse.Namespace):\n        self.args = [\n            \"-s\",  # don't capture print() output inside pytest\n            \"--log-level=DEBUG\",   # Capture logs\n            \"-o\",\n            \"junit_family=xunit2\",\n            \"-o\",\n            \"junit_suite_name={}\".format(self.suite.name),\n            \"--junit-xml={}\".format(self.xmlout),\n            \"-rs\",\n            \"--run_id={}\".format(self.id),\n            \"--mode={}\".format(self.mode)\n        ]\n        self.args.append(f\"--alluredir={self.allure_dir}\")\n        if not options.save_log_on_success:\n            self.args.append(\"--allure-no-capture\")\n        if options.markers:\n            self.args.append(f\"-m={options.markers}\")\n\n            # https://docs.pytest.org/en/7.1.x/reference/exit-codes.html\n            no_tests_selected_exit_code = 5\n            self.valid_exit_codes = [0, no_tests_selected_exit_code]\n\n        arg = str(self.suite.suite_path / (self.shortname + \".py\"))\n        if self.casename is not None:\n            arg += '::' + self.casename\n        self.args.append(arg)\n\n    def reset(self) -> None:\n        \"\"\"Reset the test before a retry, if it is retried as flaky\"\"\"\n        super().reset()\n        self.server_log = None\n        self.server_log_filename = None\n        self.is_before_test_ok = False\n        self.is_after_test_ok = False\n\n    def print_summary(self) -> None:\n        print(\"Output of {} {}:\".format(self.path, \" \".join(self.args)))\n        print(read_log(self.log_filename))\n        if self.server_log is not None:\n            print(\"Server log of the first server:\")\n            print(self.server_log)\n\n    async def run(self, options: argparse.Namespace) -> Test:\n\n        self._prepare_pytest_params(options)\n\n        loggerPrefix = self.mode + '/' + self.uname\n        logger = LogPrefixAdapter(logging.getLogger(loggerPrefix), {'prefix': loggerPrefix})\n        cluster = await self.suite.clusters.get(logger)\n        try:\n            cluster.before_test(self.uname)\n            prepare_cql = self.suite.cfg.get(\"prepare_cql\", None)\n            if prepare_cql and not hasattr(cluster, 'prepare_cql_executed'):\n                cc = next(iter(cluster.running.values())).control_connection\n                if not isinstance(prepare_cql, collections.abc.Iterable):\n                    prepare_cql = [prepare_cql]\n                for stmt in prepare_cql:\n                    cc.execute(stmt)\n                cluster.prepare_cql_executed = True\n            logger.info(\"Leasing Scylla cluster %s for test %s\", cluster, self.uname)\n            self.args.insert(0, \"--host={}\".format(cluster.endpoint()))\n            self.is_before_test_ok = True\n            cluster.take_log_savepoint()\n            status = await run_test(self, options, env=self.suite.scylla_env)\n            if self.shortname in self.suite.dirties_cluster:\n                cluster.is_dirty = True\n            cluster.after_test(self.uname, status)\n            self.is_after_test_ok = True\n            self.success = status\n        except Exception as e:\n            self.server_log = cluster.read_server_log()\n            self.server_log_filename = cluster.server_log_filename()\n            if not self.is_before_test_ok:\n                print(\"Test {} pre-check failed: {}\".format(self.name, str(e)))\n                print(\"Server log of the first server:\\n{}\".format(self.server_log))\n                logger.info(f\"Discarding cluster after failed start for test %s...\", self.name)\n            elif not self.is_after_test_ok:\n                print(\"Test {} post-check failed: {}\".format(self.name, str(e)))\n                print(\"Server log of the first server:\\n{}\".format(self.server_log))\n                logger.info(f\"Discarding cluster after failed test %s...\", self.name)\n        await self.suite.clusters.put(cluster, is_dirty=cluster.is_dirty)\n        logger.info(\"Test %s %s\", self.uname, \"succeeded\" if self.success else \"failed \")\n        return self\n\n    def write_junit_failure_report(self, xml_res: ET.Element) -> None:\n        super().write_junit_failure_report(xml_res)\n        if self.server_log_filename is not None:\n            system_err = ET.SubElement(xml_res, 'system-err')\n            system_err.text = read_log(self.server_log_filename)\n\n\nclass TopologyTest(PythonTest):\n    \"\"\"Run a pytest collection of cases against Scylla clusters handling topology changes\"\"\"\n    status: bool\n\n    def __init__(self, test_no: int, shortname: str, casename: str, suite) -> None:\n        super().__init__(test_no, shortname, casename, suite)\n\n    async def run(self, options: argparse.Namespace) -> Test:\n\n        self._prepare_pytest_params(options)\n\n        test_path = os.path.join(self.suite.options.tmpdir, self.mode)\n        async with get_cluster_manager(self.mode + '/' + self.uname, self.suite.clusters, test_path) as manager:\n            self.args.insert(0, \"--tmpdir={}\".format(options.tmpdir))\n            self.args.insert(0, \"--manager-api={}\".format(manager.sock_path))\n            if options.artifacts_dir_url:\n                self.args.insert(0, \"--artifacts_dir_url={}\".format(options.artifacts_dir_url))\n\n            try:\n                # Note: start manager here so cluster (and its logs) is available in case of failure\n                await manager.start()\n                self.success = await run_test(self, options)\n            except Exception as e:\n                self.server_log = manager.cluster.read_server_log()\n                self.server_log_filename = manager.cluster.server_log_filename()\n                if not manager.is_before_test_ok:\n                    print(\"Test {} pre-check failed: {}\".format(self.name, str(e)))\n                    print(\"Server log of the first server:\\n{}\".format(self.server_log))\n                    # Don't try to continue if the cluster is broken\n                    raise\n            manager.logger.info(\"Test %s %s\", self.uname, \"succeeded\" if self.success else \"failed \")\n        return self\n\n\nclass ToolTest(Test):\n    \"\"\"Run a collection of pytest test cases\n\n    That do not need a scylla cluster set-up for them.\"\"\"\n\n    def __init__(self, test_no: int, shortname: str, suite) -> None:\n        super().__init__(test_no, shortname, suite)\n        launcher = self.suite.cfg.get(\"launcher\", \"pytest\")\n        self.path = launcher.split(maxsplit=1)[0]\n        self.xmlout = os.path.join(self.suite.options.tmpdir, self.mode, \"xml\", self.uname + \".xunit.xml\")\n\n    def _prepare_pytest_params(self, options: argparse.Namespace):\n        launcher = self.suite.cfg.get(\"launcher\", \"pytest\")\n        self.args = launcher.split()[1:]\n        self.args += [\n            \"-s\",  # don't capture print() output inside pytest\n            \"--log-level=DEBUG\",   # Capture logs\n            \"-o\",\n            \"junit_family=xunit2\",\n            \"--junit-xml={}\".format(self.xmlout),\n            \"--mode={}\".format(self.mode),\n            \"--run_id={}\".format(self.id)\n        ]\n        self.args.append(f\"--alluredir={self.allure_dir}\")\n        if not options.save_log_on_success:\n            self.args.append(\"--allure-no-capture\")\n        if options.markers:\n            self.args.append(f\"-m={options.markers}\")\n\n            # https://docs.pytest.org/en/7.1.x/reference/exit-codes.html\n            no_tests_selected_exit_code = 5\n            self.valid_exit_codes = [0, no_tests_selected_exit_code]\n        self.args.append(str(self.suite.suite_path / (self.shortname + \".py\")))\n\n    def print_summary(self) -> None:\n        print(\"Output of {} {}:\".format(self.path, \" \".join(self.args)))\n\n    async def run(self, options: argparse.Namespace) -> Test:\n        self._prepare_pytest_params(options)\n\n        loggerPrefix = self.mode + '/' + self.uname\n        logger = LogPrefixAdapter(logging.getLogger(loggerPrefix), {'prefix': loggerPrefix})\n        self.success = await run_test(self, options)\n        logger.info(\"Test %s %s\", self.uname, \"succeeded\" if self.success else \"failed \")\n        return self\n\n    def write_junit_failure_report(self, xml_res: ET.Element) -> None:\n        super().write_junit_failure_report(xml_res)\n\n\nclass TabularConsoleOutput:\n    \"\"\"Print test progress to the console\"\"\"\n\n    def __init__(self, verbose: bool, test_count: int) -> None:\n        self.verbose = verbose\n        self.test_count = test_count\n        self.last_test_no = 0\n\n    def print_start_blurb(self) -> None:\n        print(\"=\"*80)\n        print(\"{:10s} {:^8s} {:^7s} {:8s} {}\".format(\"[N/TOTAL]\", \"SUITE\", \"MODE\", \"RESULT\", \"TEST\"))\n        print(\"-\"*78)\n        print(\"\")\n\n    def print_end_blurb(self) -> None:\n        print(\"-\"*78)\n\n    def print_progress(self, test: Test) -> None:\n        self.last_test_no += 1\n        status = \"\"\n        if test.success:\n            logging.debug(\"Test {} is flaky {}\".format(test.uname,\n                                                       test.is_flaky_failure))\n            if test.is_flaky_failure:\n                status = palette.warn(\"[ FLKY ]\")\n            else:\n                status = palette.ok(\"[ PASS ]\")\n        else:\n            status = palette.fail(\"[ FAIL ]\")\n        msg = \"{:10s} {:^8s} {:^7s} {:8s} {}\".format(\n            f\"[{self.last_test_no}/{self.test_count}]\",\n            test.suite.name, test.mode[:7],\n            status,\n            test.uname\n        )\n        if not self.verbose:\n            if test.success:\n                print(\"\\033[A\", end=\"\\r\")\n                print(\"\\033[K\", end=\"\\r\")\n                print(msg)\n            else:\n                print(\"\\033[A\", end=\"\\r\")\n                print(\"\\033[K\", end=\"\\r\")\n                print(f\"{msg}\\n\")\n        else:\n            msg += \" {:.2f}s\".format(test.time_end - test.time_start)\n            print(msg)\n\n\nasync def run_test(test: Test, options: argparse.Namespace, gentle_kill=False, env=dict()) -> bool:\n    \"\"\"Run test program, return True if success else False\"\"\"\n\n    with test.log_filename.open(\"wb\") as log:\n\n        def report_error(error):\n            msg = \"=== TEST.PY SUMMARY START ===\\n\"\n            msg += \"{}\\n\".format(error)\n            msg += \"=== TEST.PY SUMMARY END ===\\n\"\n            log.write(msg.encode(encoding=\"UTF-8\"))\n        process = None\n        stdout = None\n        logging.info(\"Starting test %s: %s %s\", test.uname, test.path, \" \".join(test.args))\n        UBSAN_OPTIONS = [\n            \"halt_on_error=1\",\n            \"abort_on_error=1\",\n            f\"suppressions={os.getcwd()}/ubsan-suppressions.supp\",\n            os.getenv(\"UBSAN_OPTIONS\"),\n        ]\n        ASAN_OPTIONS = [\n            \"disable_coredump=0\",\n            \"abort_on_error=1\",\n            \"detect_stack_use_after_return=1\",\n            os.getenv(\"ASAN_OPTIONS\"),\n        ]\n        try:\n            resource_gather = get_resource_gather(options.gather_metrics, test, options.tmpdir)\n            resource_gather.make_cgroup()\n            log.write(\"=== TEST.PY STARTING TEST {} ===\\n\".format(test.uname).encode(encoding=\"UTF-8\"))\n            log.write(\"export UBSAN_OPTIONS='{}'\\n\".format(\n                \":\".join(filter(None, UBSAN_OPTIONS))).encode(encoding=\"UTF-8\"))\n            log.write(\"export ASAN_OPTIONS='{}'\\n\".format(\n                \":\".join(filter(None, ASAN_OPTIONS))).encode(encoding=\"UTF-8\"))\n            log.write(\"{} {}\\n\".format(test.path, \" \".join(test.args)).encode(encoding=\"UTF-8\"))\n            log.write(\"=== TEST.PY TEST {} OUTPUT ===\\n\".format(test.uname).encode(encoding=\"UTF-8\"))\n            log.flush()\n            test.time_start = time.time()\n            test.time_end = 0\n\n            path = test.path\n            args = test.core_args + test.args\n            if options.cpus:\n                path = 'taskset'\n                args = ['-c', options.cpus, test.path, *args]\n\n            test_running_event = asyncio.Event()\n            test_resource_watcher = resource_gather.cgroup_monitor(test_event=test_running_event)\n\n            process = await asyncio.create_subprocess_exec(\n                path, *args,\n                stderr=log,\n                stdout=log,\n                env=dict(os.environ,\n                         UBSAN_OPTIONS=\":\".join(filter(None, UBSAN_OPTIONS)),\n                         ASAN_OPTIONS=\":\".join(filter(None, ASAN_OPTIONS)),\n                         # TMPDIR env variable is used by any seastar/scylla\n                         # test for directory to store test temporary data.\n                         TMPDIR=os.path.join(options.tmpdir, test.mode),\n                         SCYLLA_TEST_ENV='yes',\n                         **env,\n                         ),\n                preexec_fn=resource_gather.put_process_to_cgroup,\n            )\n            stdout, _ = await asyncio.wait_for(process.communicate(), options.timeout)\n            test_running_event.set()\n            test.time_end = time.time()\n\n            metrics = resource_gather.get_test_metrics()\n            try:\n                async with asyncio.timeout(2):\n                    await test_resource_watcher\n            except TimeoutError:\n                log.write(f'Metrics for {test.name} can be inaccurate, job reached timeout'.encode(encoding='UTF-8'))\n            finally:\n                resource_gather.remove_cgroup()\n\n            if process.returncode not in test.valid_exit_codes:\n                report_error('Test exited with code {code}\\n'.format(code=process.returncode))\n                resource_gather.write_metrics_to_db(metrics)\n                return False\n            try:\n                test.check_log(not options.save_log_on_success)\n            except Exception as e:\n                print(\"\")\n                print(test.name + \": \" + palette.crit(\"failed to parse XML output: {}\".format(e)))\n                resource_gather.write_metrics_to_db(metrics)\n                return False\n            resource_gather.write_metrics_to_db(metrics, True)\n            return True\n        except (asyncio.TimeoutError, asyncio.CancelledError) as e:\n            test.is_cancelled = True\n            if process is not None:\n                if gentle_kill:\n                    process.terminate()\n                else:\n                    process.kill()\n                stdout, _ = await process.communicate()\n            if isinstance(e, asyncio.TimeoutError):\n                report_error(\"Test timed out\")\n            elif isinstance(e, asyncio.CancelledError):\n                print(test.shortname, end=\" \")\n                report_error(\"Test was cancelled: the parent process is exiting\")\n        except Exception as e:\n            report_error(\"Failed to run the test:\\n{e}\".format(e=e))\n    return False\n\n\ndef setup_signal_handlers(loop, signaled) -> None:\n\n    async def shutdown(loop, signo, signaled):\n        print(\"\\nShutdown requested... Aborting tests:\"),\n        signaled.signo = signo\n        signaled.set()\n\n    # Use a lambda to avoid creating a coroutine until\n    # the signal is delivered to the loop - otherwise\n    # the coroutine will be dangling when the loop is over,\n    # since it's never going to be invoked\n    for signo in [signal.SIGINT, signal.SIGTERM]:\n        loop.add_signal_handler(signo, lambda: asyncio.create_task(shutdown(loop, signo, signaled)))\n\n\ndef parse_cmd_line() -> argparse.Namespace:\n    \"\"\" Print usage and process command line options. \"\"\"\n\n    parser = argparse.ArgumentParser(description=\"Scylla test runner\")\n    parser.add_argument(\n        \"name\",\n        nargs=\"*\",\n        action=\"store\",\n        help=\"\"\"Can be empty. List of test names, to look for in\n                suites. Each name is used as a substring to look for in the\n                path to test file, e.g. \"mem\" will run all tests that have\n                \"mem\" in their name in all suites, \"boost/mem\" will only enable\n                tests starting with \"mem\" in \"boost\" suite, and\n                \"boost/memtable_test::test_hash_is_cached\" to narrow down to\n                a certain test case. Default: run all tests in all suites.\"\"\",\n    )\n    parser.add_argument(\n        \"--tmpdir\",\n        action=\"store\",\n        default=\"testlog\",\n        help=\"\"\"Path to temporary test data and log files. The data is\n        further segregated per build mode. Default: ./testlog.\"\"\",\n    )\n    parser.add_argument(\"--gather-metrics\", action=argparse.BooleanOptionalAction, default=True)\n    parser.add_argument(\"--max-failures\", type=int, default=-1, help=\"Maximum number of failures to tolerate before cancelling rest of tests.\")\n    parser.add_argument('--mode', choices=all_modes.keys(), action=\"append\", dest=\"modes\",\n                        help=\"Run only tests for given build mode(s)\")\n    parser.add_argument('--repeat', action=\"store\", default=\"1\", type=int,\n                        help=\"number of times to repeat test execution\")\n    parser.add_argument('--timeout', action=\"store\", default=\"24000\", type=int,\n                        help=\"timeout value for test execution\")\n    parser.add_argument('--verbose', '-v', action='store_true', default=False,\n                        help='Verbose reporting')\n    parser.add_argument('--jobs', '-j', action=\"store\", type=int,\n                        help=\"Number of jobs to use for running the tests\")\n    parser.add_argument('--save-log-on-success', \"-s\", default=False,\n                        dest=\"save_log_on_success\", action=\"store_true\",\n                        help=\"Save test log output on success.\")\n    parser.add_argument('--list', dest=\"list_tests\", action=\"store_true\", default=False,\n                        help=\"Print list of tests instead of executing them\")\n    parser.add_argument('--skip',\n                        dest=\"skip_patterns\", action=\"append\",\n                        help=\"Skip tests which match the provided pattern\")\n    parser.add_argument('--no-parallel-cases', dest=\"parallel_cases\", action=\"store_false\", default=True,\n                        help=\"Do not run individual test cases in parallel\")\n    parser.add_argument('--cpus', action=\"store\",\n                        help=\"Run the tests on those CPUs only (in taskset\"\n                        \" acceptable format). Consider using --jobs too\")\n    parser.add_argument('--log-level', action=\"store\",\n                        help=\"Log level for Python logging module. The log \"\n                        \"is in {tmpdir}/test.py.log. Default: INFO\",\n                        default=\"INFO\",\n                        choices=[\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\",\n                                 \"DEBUG\"],\n                        dest=\"log_level\")\n    parser.add_argument('--markers', action='store', metavar='MARKEXPR',\n                        help=\"Only run tests that match the given mark expression. The syntax is the same \"\n                             \"as in pytest, for example: --markers 'mark1 and not mark2'. The parameter \"\n                             \"is only supported by python tests for now, other tests ignore it. \"\n                             \"By default, the marker filter is not applied and all tests will be run without exception.\"\n                             \"To exclude e.g. slow tests you can write --markers 'not slow'.\")\n    parser.add_argument('--coverage', action = 'store_true', default = False,\n                        help=\"When running code instrumented with coverage support\"\n                             \"Will route the profiles to `tmpdir`/mode/coverage/`suite` and post process them in order to generate \"\n                             \"lcov file per suite, lcov file per mode, and an lcov file for the entire run, \"\n                             \"The lcov files can eventually be used for generating coverage reports\")\n    parser.add_argument(\"--coverage-mode\",action = 'append', type = str, dest = \"coverage_modes\",\n                        help = \"Collect and process coverage only for the modes specified. implies: --coverage, default: All built modes\")\n    parser.add_argument(\"--coverage-keep-raw\",action = 'store_true',\n                        help = \"Do not delete llvm raw profiles when processing coverage reports.\")\n    parser.add_argument(\"--coverage-keep-indexed\",action = 'store_true',\n                        help = \"Do not delete llvm indexed profiles when processing coverage reports.\")\n    parser.add_argument(\"--coverage-keep-lcovs\",action = 'store_true',\n                        help = \"Do not delete intermediate lcov traces when processing coverage reports.\")\n    parser.add_argument(\"--artifacts_dir_url\", action='store', type=str, default=None, dest=\"artifacts_dir_url\",\n                        help=\"Provide the URL to artifacts directory to generate the link to failed tests directory \"\n                             \"with logs\")\n    parser.add_argument(\"--cluster-pool-size\", action=\"store\", default=None, type=int,\n                        help=\"Set the pool_size for PythonTest and its descendants. Alternatively environment variable \"\n                             \"CLUSTER_POOL_SIZE can be used to achieve the same\")\n    scylla_additional_options = parser.add_argument_group('Additional options for Scylla tests')\n    scylla_additional_options.add_argument('--x-log2-compaction-groups', action=\"store\", default=\"0\", type=int,\n                             help=\"Controls number of compaction groups to be used by Scylla tests. Value of 3 implies 8 groups.\")\n    scylla_additional_options.add_argument('--extra-scylla-cmdline-options', action=\"store\", default=[], type=str,\n                                           help=\"Passing extra scylla cmdline options for all tests. Options should be space separated:\"\n                                                \"'--logger-log-level raft=trace --default-log-level error'\")\n\n    boost_group = parser.add_argument_group('boost suite options')\n    boost_group.add_argument('--random-seed', action=\"store\",\n                             help=\"Random number generator seed to be used by boost tests\")\n\n    args = parser.parse_args()\n\n    if not args.jobs:\n        if not args.cpus:\n            nr_cpus = multiprocessing.cpu_count()\n        else:\n            nr_cpus = int(subprocess.check_output(\n                ['taskset', '-c', args.cpus, 'python3', '-c',\n                 'import os; print(len(os.sched_getaffinity(0)))']))\n\n        cpus_per_test_job = 1\n        sysmem = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')\n        testmem = 6e9 if os.sysconf('SC_PAGE_SIZE') > 4096 else 2e9\n        default_num_jobs_mem = ((sysmem - 4e9) // testmem)\n        args.jobs = min(default_num_jobs_mem, nr_cpus // cpus_per_test_job)\n\n    if not output_is_a_tty:\n        args.verbose = True\n\n    if not args.modes:\n        try:\n            out = ninja('mode_list')\n            # [1/1] List configured modes\n            # debug release dev\n            args.modes = re.sub(r'.* List configured modes\\n(.*)\\n', r'\\1',\n                                out, count=1, flags=re.DOTALL).split(\"\\n\")[-1].split(' ')\n        except Exception:\n            print(palette.fail(\"Failed to read output of `ninja mode_list`: please run ./configure.py first\"))\n            raise\n\n    if not args.coverage_modes and args.coverage:\n        args.coverage_modes = list(args.modes)\n        if \"coverage\" in args.coverage_modes:\n            args.coverage_modes.remove(\"coverage\")\n        if not args.coverage_modes:\n            args.coverage = False\n    elif args.coverage_modes:\n        if \"coverage\" in args.coverage_modes:\n            raise RuntimeError(\"'coverage' mode is not allowed in --coverage-mode\")\n        missing_coverage_modes = set(args.coverage_modes).difference(set(args.modes))\n        if len(missing_coverage_modes) > 0:\n            raise RuntimeError(f\"The following modes weren't built or ran (using the '--mode' option): {missing_coverage_modes}\")\n        args.coverage = True\n    def prepare_dir(dirname, pattern):\n        # Ensure the dir exists\n        pathlib.Path(dirname).mkdir(parents=True, exist_ok=True)\n        # Remove old artifacts\n        for p in glob.glob(os.path.join(dirname, pattern), recursive=True):\n            pathlib.Path(p).unlink()\n\n    args.tmpdir = os.path.abspath(args.tmpdir)\n    prepare_dir(args.tmpdir, \"*.log\")\n\n    for mode in args.modes:\n        prepare_dir(os.path.join(args.tmpdir, mode), \"*.log\")\n        prepare_dir(os.path.join(args.tmpdir, mode), \"*.reject\")\n        prepare_dir(os.path.join(args.tmpdir, mode, \"xml\"), \"*.xml\")\n        shutil.rmtree(os.path.join(args.tmpdir, mode, \"failed_test\"), ignore_errors=True)\n        prepare_dir(os.path.join(args.tmpdir, mode, \"failed_test\"), \"*\")\n        prepare_dir(os.path.join(args.tmpdir, mode, \"allure\"), \"*.xml\")\n\n    # Get the list of tests configured by configure.py\n    try:\n        out = ninja('unit_test_list')\n        # [1/1] List configured unit tests\n        args.tests = set(re.sub(r'.* List configured unit tests\\n(.*)\\n', r'\\1', out, count=1, flags=re.DOTALL).split(\"\\n\"))\n    except Exception:\n        print(palette.fail(\"Failed to read output of `ninja unit_test_list`: please run ./configure.py first\"))\n        raise\n\n    if args.extra_scylla_cmdline_options:\n        args.extra_scylla_cmdline_options = args.extra_scylla_cmdline_options.split()\n\n    return args\n\n\nasync def find_tests(options: argparse.Namespace) -> None:\n\n    for f in glob.glob(os.path.join(\"test\", \"*\")):\n        if os.path.isdir(f) and os.path.isfile(os.path.join(f, \"suite.yaml\")):\n            for mode in options.modes:\n                suite = TestSuite.opt_create(f, options, mode)\n                await suite.add_test_list()\n\n    if not TestSuite.test_count():\n        if len(options.name):\n            print(\"Test {} not found\".format(palette.path(options.name[0])))\n            sys.exit(1)\n        else:\n            print(palette.warn(\"No tests found. Please enable tests in ./configure.py first.\"))\n            sys.exit(0)\n\n    logging.info(\"Found %d tests, repeat count is %d, starting %d concurrent jobs\",\n                 TestSuite.test_count(), options.repeat, options.jobs)\n    print(\"Found {} tests.\".format(TestSuite.test_count()))\n\n\nasync def run_all_tests(signaled: asyncio.Event, options: argparse.Namespace) -> None:\n    console = TabularConsoleOutput(options.verbose, TestSuite.test_count())\n    signaled_task = asyncio.create_task(signaled.wait())\n    pending = {signaled_task}\n\n    async def cancel(pending, msg):\n        for task in pending:\n            task.cancel(msg)\n        await asyncio.wait(pending, return_when=asyncio.ALL_COMPLETED)\n        print(\"...done\")\n\n    async def reap(done, pending, signaled):\n        nonlocal console\n        if signaled.is_set():\n            await cancel(pending, \"Signal received\")\n        failed = 0\n        for coro in done:\n            result = coro.result()\n            if isinstance(result, bool):\n                continue    # skip signaled task result\n            if not result.success:\n                failed += 1\n            if not result.did_not_run:\n                console.print_progress(result)\n        return failed\n\n    ms = MinioServer(options.tmpdir, '127.0.0.1', LogPrefixAdapter(logging.getLogger('minio'), {'prefix': 'minio'}))\n    await ms.start()\n    TestSuite.artifacts.add_exit_artifact(None, ms.stop)\n\n    hosts = HostRegistry()\n    mock_s3_server = MockS3Server(await hosts.lease_host(), 2012,\n                                  LogPrefixAdapter(logging.getLogger('s3_mock'), {'prefix': 's3_mock'}))\n    await mock_s3_server.start()\n    TestSuite.artifacts.add_exit_artifact(None, mock_s3_server.stop)\n\n    minio_uri = \"http://\" + os.environ[ms.ENV_ADDRESS] + \":\" + os.environ[ms.ENV_PORT]\n    proxy_s3_server = S3ProxyServer(await hosts.lease_host(), 9002, minio_uri, 3, int(time.time()),\n                                    LogPrefixAdapter(logging.getLogger('s3_proxy'), {'prefix': 's3_proxy'}))\n    await proxy_s3_server.start()\n    TestSuite.artifacts.add_exit_artifact(None, proxy_s3_server.stop)\n\n    console.print_start_blurb()\n    max_failures = options.max_failures\n    failed = 0\n    try:\n        TestSuite.artifacts.add_exit_artifact(None, TestSuite.hosts.cleanup)\n        for test in TestSuite.all_tests():\n            # +1 for 'signaled' event\n            if len(pending) > options.jobs:\n                # Wait for some task to finish\n                done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n                failed += await reap(done, pending, signaled)\n                if 0 < max_failures <= failed:\n                    print(\"Too much failures, stopping\")\n                    await cancel(pending, \"Too much failures, stopping\")\n            pending.add(asyncio.create_task(test.suite.run(test, options)))\n        # Wait & reap ALL tasks but signaled_task\n        # Do not use asyncio.ALL_COMPLETED to print a nice progress report\n        while len(pending) > 1:\n            done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n            failed += await reap(done, pending, signaled)\n            if 0 < max_failures <= failed:\n                print(\"Too much failures, stopping\")\n                await cancel(pending, \"Too much failures, stopping\")\n    except asyncio.CancelledError:\n        return\n    finally:\n        await TestSuite.artifacts.cleanup_before_exit()\n\n    console.print_end_blurb()\n\n\ndef read_log(log_filename: pathlib.Path) -> str:\n    \"\"\"Intelligently read test log output\"\"\"\n    try:\n        with log_filename.open(\"r\") as log:\n            msg = log.read()\n            return msg if len(msg) else \"===Empty log output===\"\n    except FileNotFoundError:\n        return \"===Log {} not found===\".format(log_filename)\n    except OSError as e:\n        return \"===Error reading log {}===\".format(e)\n\n\ndef print_summary(failed_tests: List[\"Test\"], cancelled_tests: int, options: argparse.Namespace) -> None:\n    rusage = resource.getrusage(resource.RUSAGE_CHILDREN)\n    cpu_used = rusage.ru_stime + rusage.ru_utime\n    cpu_available = (time.monotonic() - launch_time) * multiprocessing.cpu_count()\n    utilization = cpu_used / cpu_available\n    print(f\"CPU utilization: {utilization*100:.1f}%\")\n    if failed_tests:\n        print(\"The following test(s) have failed: {}\".format(\n            palette.path(\" \".join([t.name for t in failed_tests]))))\n        if options.verbose:\n            for test in failed_tests:\n                test.print_summary()\n                print(\"-\"*78)\n        if cancelled_tests > 0:\n            print(f\"Summary: {len(failed_tests)} of the total {TestSuite.test_count()} tests failed, {cancelled_tests} cancelled\")\n        else:\n            print(f\"Summary: {len(failed_tests)} of the total {TestSuite.test_count()} tests failed\")\n\n\ndef format_unidiff(fromfile: str, tofile: str) -> str:\n    with open(fromfile, \"r\") as frm, open(tofile, \"r\") as to:\n        buf = StringIO()\n        diff = difflib.unified_diff(\n            frm.readlines(),\n            to.readlines(),\n            fromfile=fromfile,\n            tofile=tofile,\n            fromfiledate=time.ctime(os.stat(fromfile).st_mtime),\n            tofiledate=time.ctime(os.stat(tofile).st_mtime),\n            n=10)           # Number of context lines\n\n        for i, line in enumerate(diff):\n            if i > 60:\n                break\n            if line.startswith('+'):\n                line = palette.diff_in(line)\n            elif line.startswith('-'):\n                line = palette.diff_out(line)\n            elif line.startswith('@'):\n                line = palette.diff_mark(line)\n            buf.write(line)\n        return buf.getvalue()\n\n\ndef write_junit_report(tmpdir: str, mode: str) -> None:\n    junit_filename = os.path.join(tmpdir, mode, \"xml\", \"junit.xml\")\n    total = 0\n    failed = 0\n    skipped = 0\n    xml_results = ET.Element(\"testsuite\", name=\"non-boost tests\", errors=\"0\")\n    for suite in TestSuite.suites.values():\n        for test in suite.junit_tests():\n            if test.mode != mode:\n                continue\n            total += 1\n            test_time = f\"{test.time_end - test.time_start:.3f}\"\n            # add the suite name to disambiguate tests named \"run\"\n            xml_res = ET.SubElement(xml_results, 'testcase',\n                                    name=\"{}.{}.{}.{}\".format(test.suite.name, test.shortname, mode, test.id),\n                                    time=test_time)\n            if test.failed:\n                failed += 1\n                test.write_junit_failure_report(xml_res)\n            if test.did_not_run:\n                skipped += 1\n    if total == 0:\n        return\n    xml_results.set(\"tests\", str(total))\n    xml_results.set(\"failures\", str(failed))\n    xml_results.set(\"skipped\", str(skipped))\n    with open(junit_filename, \"w\") as f:\n        ET.ElementTree(xml_results).write(f, encoding=\"unicode\")\n\n\ndef summarize_boost_tests(tests):\n    # in case we run a certain test multiple times\n    # - if any of the runs failed, the test is considered failed, and\n    #   the last failed run is returned.\n    # - otherwise, the last successful run is returned\n    failed_test = None\n    passed_test = None\n    num_failed_tests = collections.defaultdict(int)\n    num_passed_tests = collections.defaultdict(int)\n    for test in tests:\n        error = None\n        for tag in ['Error', 'FatalError', 'Exception']:\n            error = test.find(tag)\n            if error is not None:\n                break\n        mode = test.attrib['mode']\n        if error is None:\n            passed_test = test\n            num_passed_tests[mode] += 1\n        else:\n            failed_test = test\n            num_failed_tests[mode] += 1\n\n    if failed_test is not None:\n        test = failed_test\n    else:\n        test = passed_test\n\n    num_failed = sum(num_failed_tests.values())\n    num_passed = sum(num_passed_tests.values())\n    num_total = num_failed + num_passed\n    if num_total == 1:\n        return test\n    if num_failed == 0:\n        return test\n    # we repeated this test for multiple times.\n    #\n    # Boost::test's XML logger schema does not allow us to put text directly in a\n    # TestCase tag, so create a dummy Message tag in the TestCase for carrying the\n    # summary. and the schema requires that the tags should be listed in following order:\n    # 1. TestSuite\n    # 2. Info\n    # 3. Error\n    # 3. FatalError\n    # 4. Message\n    # 5. Exception\n    # 6. Warning\n    # and both \"file\" and \"line\" are required in an \"Info\" tag, so appease it. assuming\n    # there is no TestSuite under tag TestCase, we always add Info as the first subelements\n    if num_passed == 0:\n        message = ET.Element('Info', file=test.attrib['file'], line=test.attrib['line'])\n        message.text = f'The test failed {num_failed}/{num_total} times'\n        test.insert(0, message)\n    else:\n        message = ET.Element('Info', file=test.attrib['file'], line=test.attrib['line'])\n        modes = ', '.join(f'{mode}={n}' for mode, n in num_failed_tests.items())\n        message.text = f'failed: {modes}'\n        test.insert(0, message)\n\n        message = ET.Element('Info', file=test.attrib['file'], line=test.attrib['line'])\n        modes = ', '.join(f'{mode}={n}' for mode, n in num_passed_tests.items())\n        message.text = f'passed: {modes}'\n        test.insert(0, message)\n\n        message = ET.Element('Info', file=test.attrib['file'], line=test.attrib['line'])\n        message.text = f'{num_failed} out of {num_total} times failed.'\n        test.insert(0, message)\n    return test\n\n\ndef write_consolidated_boost_junit_xml(tmpdir: str, mode: str) -> str:\n    # collects all boost tests sorted by their full names\n    boost_tests = itertools.chain.from_iterable(suite.boost_tests()\n                                                for suite in TestSuite.suites.values())\n    test_cases = itertools.chain.from_iterable(test.get_test_cases()\n                                               for test in boost_tests)\n    test_cases = sorted(test_cases, key=BoostTest.test_path_of_element)\n\n    xml = ET.Element(\"TestLog\")\n    for full_path, tests in itertools.groupby(\n            test_cases,\n            key=BoostTest.test_path_of_element):\n        # dedup the tests with the same name, so only the representative one is\n        # preserved\n        test_case = summarize_boost_tests(tests)\n        test_case.attrib.pop('path')\n        test_case.attrib.pop('mode')\n\n        suite_name, test_name, _ = full_path\n        suite = xml.find(f\"./TestSuite[@name='{suite_name}']\")\n        if suite is None:\n            suite = ET.SubElement(xml, 'TestSuite', name=suite_name)\n        test = suite.find(f\"./TestSuite[@name='{test_name}']\")\n        if test is None:\n            test = ET.SubElement(suite, 'TestSuite', name=test_name)\n        test.append(test_case)\n    et = ET.ElementTree(xml)\n    xunit_file = f'{tmpdir}/{mode}/xml/boost.xunit.xml'\n    et.write(xunit_file, encoding='unicode')\n    return xunit_file\n\n\ndef boost_to_junit(boost_xml, junit_xml):\n    boost_root = ET.parse(boost_xml).getroot()\n    junit_root = ET.Element('testsuites')\n\n    def parse_tag_output(test_case_element: ET.Element, tag_output: str) -> str:\n        text_template = '''\n            [{level}] - {level_message}\n            [FILE] - {file_name}\n            [LINE] - {line_number}\n            '''\n        text = ''\n        tag_outputs = test_case_element.findall(tag_output)\n        if tag_outputs:\n            for tag_output in tag_outputs:\n                text += text_template.format(level='Info', level_message=tag_output.text,\n                                             file_name=tag_output.get('file'), line_number=tag_output.get('line'))\n        return text\n\n    # report produced {write_consolidated_boost_junit_xml} have the nested structure suite_boost -> [suite1, suite2, ...]\n    # so we are excluding the upper suite with name boost\n    for test_suite in boost_root.findall('./TestSuite/TestSuite'):\n        suite_time = 0.0\n        suite_test_total = 0\n        suite_test_fails_number = 0\n\n        junit_test_suite = ET.SubElement(junit_root, 'testsuite')\n        junit_test_suite.attrib['name'] = test_suite.attrib['name']\n\n        test_cases = test_suite.findall('TestCase')\n        for test_case in test_cases:\n            # convert the testing time: boost uses microseconds and Junit uses seconds\n            test_case_time = int(test_case.find('TestingTime').text) / 1_000_000\n            suite_time += test_case_time\n            suite_test_total += 1\n\n            junit_test_case = ET.SubElement(junit_test_suite, 'testcase')\n            junit_test_case.set('name', test_case.get('name'))\n            junit_test_case.set('time', str(test_case_time))\n            junit_test_case.set('file', test_case.get('file'))\n            junit_test_case.set('line', test_case.get('line'))\n\n            system_out = ET.SubElement(junit_test_case, 'system-out')\n            system_out.text = ''\n            for tag in ['Info', 'Message', 'Exception']:\n                output = parse_tag_output(test_case, tag)\n                if output:\n                    system_out.text += output\n\n        junit_test_suite.set('tests', str(suite_test_total))\n        junit_test_suite.set('time', str(suite_time))\n        junit_test_suite.set('failures', str(suite_test_fails_number))\n    ET.ElementTree(junit_root).write(junit_xml, encoding='UTF-8')\n\n\ndef open_log(tmpdir: str, log_file_name: str, log_level: str) -> None:\n    pathlib.Path(tmpdir).mkdir(parents=True, exist_ok=True)\n    logging.basicConfig(\n        filename=os.path.join(tmpdir, log_file_name),\n        filemode=\"w\",\n        level=log_level,\n        format=\"%(asctime)s.%(msecs)03d %(levelname)s> %(message)s\",\n        datefmt=\"%H:%M:%S\",\n    )\n    logging.critical(\"Started %s\", \" \".join(sys.argv))\n\n\nasync def main() -> int:\n\n    options = parse_cmd_line()\n\n    open_log(options.tmpdir, f\"test.py.{'-'.join(options.modes)}.log\", options.log_level)\n    setup_cgroup(options.gather_metrics)\n    await find_tests(options)\n    if options.list_tests:\n        print('\\n'.join([f\"{t.suite.mode:<8} {type(t.suite).__name__[:-9]:<11} {t.name}\"\n                         for t in TestSuite.all_tests()]))\n        return 0\n\n    signaled = asyncio.Event()\n    stop_event = asyncio.Event()\n    resource_watcher = run_resource_watcher(options.gather_metrics, signaled, stop_event, options.tmpdir)\n\n    setup_signal_handlers(asyncio.get_running_loop(), signaled)\n\n    try:\n        await run_all_tests(signaled, options)\n        stop_event.set()\n        async with asyncio.timeout(5):\n            await resource_watcher\n    except Exception as e:\n        print(palette.fail(e))\n        raise\n\n    if signaled.is_set():\n        return -signaled.signo      # type: ignore\n\n    failed_tests = [test for test in TestSuite.all_tests() if test.failed]\n    cancelled_tests = sum(1 for test in TestSuite.all_tests() if test.did_not_run)\n\n    print_summary(failed_tests, cancelled_tests, options)\n\n    for mode in options.modes:\n        junit_file = f\"{options.tmpdir}/{mode}/allure/boost.junit.xml\"\n        write_junit_report(options.tmpdir, mode)\n        xunit_file = write_consolidated_boost_junit_xml(options.tmpdir, mode)\n        boost_to_junit(xunit_file, junit_file)\n\n    if 'coverage' in options.modes:\n        coverage.generate_coverage_report(path_to(\"coverage\", \"tests\"))\n\n    if options.coverage:\n        await process_coverage(options)\n\n    # Note: failure codes must be in the ranges 0-124, 126-127,\n    #       to cooperate with git bisect's expectations\n    return 0 if not failed_tests else 1\n\nasync def process_coverage(options):\n    total_processing_time = time.time()\n    logger = LogPrefixAdapter(logging.getLogger(\"coverage\"), {'prefix' : 'coverage'})\n    modes_for_coverage = options.coverage_modes\n    # use about 75% of the machine's processing power.\n    concurrency = max(int(multiprocessing.cpu_count() * 0.75), 1)\n    logger.info(f\"Processing coverage information for modes: {modes_for_coverage}, using {concurrency} cpus\")\n    semaphore = asyncio.Semaphore(concurrency)\n    build_paths = [pathlib.Path(f\"build/{mode}\") for mode in modes_for_coverage]\n    paths_for_id_search = [bp / p for bp, p in itertools.product(build_paths, [\"scylla\", \"test\", \"seastar\"])]\n    logger.info(\"Getting binary ids for coverage conversion...\")\n    files_to_ids_map = await coverage_utils.get_binary_ids_map(paths = paths_for_id_search,\n                                                               filter = coverage_utils.PROFILED_ELF_TYPES,\n                                                               semaphore = semaphore,\n                                                               logger = logger)\n    logger.debug(f\"Binary ids map is: {files_to_ids_map}\")\n    logger.info(\"Done getting binary ids for coverage conversion\")\n    # get the suits that have actually been ran\n    suits_to_exclude = [\"pylib_test\", \"nodetool\"]\n    sources_to_exclude = [line for line in open(\"coverage_excludes.txt\", 'r').read().split('\\n') if line and not line.startswith('#')]\n    ran_suites = list({test.suite for test in TestSuite.all_tests() if test.suite.need_coverage()})\n\n    def suite_coverage_path(suite) -> pathlib.Path:\n        return pathlib.Path(suite.options.tmpdir) / suite.mode / 'coverage' / suite.name\n\n    def pathsize(path : pathlib.Path):\n        if path.is_file():\n            return os.path.getsize(path)\n        elif path.is_dir():\n            return sum([os.path.getsize(f) for f in path.glob(\"**/*\") if f.is_file()])\n        else:\n            return 0\n    class Stats:\n        def __init__(self, name = \"\", size = 0, time = 0) -> None:\n            self.name = name\n            self.size = size\n            self.time = time\n        def __add__(self, other):\n            return Stats(self.name,\n                         size = self.size + other.size,\n                         time = self.time + other.time)\n        def __str__(self):\n            name = f\"{self.name} - \" if self.name else \"\"\n            fields = []\n            if self.size:\n                fields.append(f\"size: {humanfriendly.format_size(self.size)}\")\n            if self.time:\n                fields.append(f\"time: {humanfriendly.format_timespan(self.time)}\")\n            fields = ', '.join(fields)\n            return f\"{name}{fields}\"\n        @property\n        def asstring(self):\n            return str(self)\n\n    # a nested map of: mode -> suite -> unified_coverage_file\n    suits_trace_files = {}\n    stats = treelib.Tree()\n\n    RAW_PROFILE_STATS = \"raw profiles\"\n    INDEXED_PROFILE_STATS = \"indexed profiles\"\n    LCOV_CONVERSION_STATS = \"lcov conversion\"\n    LCOV_SUITES_MEREGE_STATS = \"lcov per suite merge\"\n    LCOV_MODES_MERGE_STATS = \"lcov merge for mode\"\n    LCOV_MERGE_ALL_STATS = \"lcov merge all stats\"\n    ROOT_NODE = stats.create_node(tag = time.time(),\n                                  identifier = \"root\",\n                                  data = Stats(\"Coverage Processing Stats\", 0, 0))\n\n    for suite in list(ran_suites):\n        coverage_path = suite_coverage_path(suite)\n        if not coverage_path.exists():\n            logger.warning(f\"Coverage dir for suite '{suite.name}' in mode '{suite.mode}' wasn't found, common reasons:\\n\\t\"\n                \"1. The suite doesn't use any instrumented binaries.\\n\\t\"\n                \"2. The binaries weren't compiled with coverage instrumentation.\")\n            continue\n\n        # 1. Transform every suite raw profiles into indexed profiles\n        raw_profiles = list(coverage_path.glob(\"*.profraw\"))\n        if len(raw_profiles) == 0:\n            logger.warning(f\"Couldn't find any raw profiles for suite '{suite.name}' in mode '{suite.mode}' ({coverage_path}):\\n\\t\"\n                \"1. The binaries are killed instead of terminating which bypasses profile dump.\\n\\t\"\n                \"2. The suite tempres with the LLVM_PROFILE_FILE which causes the profile to be dumped\\n\\t\"\n                \"   to somewhere else.\")\n            continue\n        mode_stats = stats.get_node(suite.mode)\n        if not mode_stats:\n            mode_stats = stats.create_node(tag = time.time(),\n                                           identifier = suite.mode,\n                                           parent = ROOT_NODE,\n                                           data = Stats(f\"{suite.mode} mode processing stats\", 0, 0))\n\n        raw_stats_node = stats.get_node(mode_stats.identifier + RAW_PROFILE_STATS)\n        if not raw_stats_node:\n            raw_stats_node = stats.create_node(tag = time.time(),\n                                               identifier = mode_stats.identifier + RAW_PROFILE_STATS,\n                                               parent = mode_stats,\n                                               data = Stats(RAW_PROFILE_STATS, 0, 0))\n        stat = stats.create_node(tag = time.time(),\n                                 identifier = raw_stats_node.identifier + suite.name,\n                                 parent = raw_stats_node,\n                                 data = Stats(suite.name, pathsize(coverage_path), 0))\n        raw_stats_node.data += stat.data\n        mode_stats.data.time += stat.data.time\n        mode_stats.data.size = max(mode_stats.data.size, raw_stats_node.data.size)\n\n\n        logger.info(f\"{suite.name}: Converting raw profiles into indexed profiles - {stat.data}.\")\n        start_time = time.time()\n        merge_result = await coverage_utils.merge_profiles(profiles = raw_profiles,\n                                            path_for_merged = coverage_path,\n                                            clear_on_success = (not options.coverage_keep_raw),\n                                            semaphore = semaphore,\n                                            logger = logger)\n        indexed_stats_node = stats.get_node(mode_stats.identifier +INDEXED_PROFILE_STATS)\n        if not indexed_stats_node:\n            indexed_stats_node = stats.create_node(tag = time.time(),\n                                                   identifier = mode_stats.identifier +INDEXED_PROFILE_STATS,\n                                                   parent = mode_stats,\n                                                   data = Stats(INDEXED_PROFILE_STATS, 0, 0))\n        stat = stats.create_node(tag = time.time(),\n                                 identifier = indexed_stats_node.identifier + suite.name,\n                                 parent = indexed_stats_node,\n                                 data = Stats(suite.name, pathsize(coverage_path), time.time() - start_time))\n        indexed_stats_node.data += stat.data\n        mode_stats.data.time += stat.data.time\n        mode_stats.data.size = max(mode_stats.data.size, indexed_stats_node.data.size)\n\n        logger.info(f\"{suite.name}: Done converting raw profiles into indexed profiles - {humanfriendly.format_timespan(stat.data.time)}.\")\n\n        # 2. Transform every indexed profile into an lcov trace file,\n        #    after this step, the dependency upon the build artifacts\n        #    ends and processing of the files can be done using the source\n        #    code only.\n\n        logger.info(f\"{suite.name}: Converting indexed profiles into lcov trace files.\")\n        start_time = time.time()\n        if len(merge_result.errors) > 0:\n            raise RuntimeError(merge_result.errors)\n        await coverage_utils.profdata_to_lcov(profiles = merge_result.generated_profiles,\n                                              excludes = sources_to_exclude,\n                                              known_file_ids = files_to_ids_map,\n                                              clear_on_success = (not options.coverage_keep_indexed),\n                                              semaphore = semaphore,\n                                              logger = logger\n                                              )\n        lcov_conversion_stats_node = stats.get_node(mode_stats.identifier + LCOV_CONVERSION_STATS)\n        if not lcov_conversion_stats_node:\n            lcov_conversion_stats_node = stats.create_node(tag = time.time(),\n                                                           identifier = mode_stats.identifier + LCOV_CONVERSION_STATS,\n                                                           parent = mode_stats,\n                                                           data = Stats(LCOV_CONVERSION_STATS, 0, 0))\n        stat = stats.create_node(tag = time.time(),\n                                 identifier = lcov_conversion_stats_node.identifier + suite.name,\n                                 parent = lcov_conversion_stats_node,\n                                 data = Stats(suite.name, pathsize(coverage_path), time.time() - start_time))\n        lcov_conversion_stats_node.data += stat.data\n        mode_stats.data.time += stat.data.time\n        mode_stats.data.size = max(mode_stats.data.size, lcov_conversion_stats_node.data.size)\n\n        logger.info(f\"{suite.name}: Done converting indexed profiles into lcov trace files - {humanfriendly.format_timespan(stat.data.time)}.\")\n\n        # 3. combine all tracefiles\n        logger.info(f\"{suite.name} in mode {suite.mode}: Combinig lcov trace files.\")\n        start_time = time.time()\n        trace_files = list(coverage_path.glob(\"**/*.info\"))\n        target_trace_file = coverage_path / (suite.name + \".info\")\n        if len(trace_files) == 0: # No coverage data, can skip\n            logger.warning(f\"{suite.name} in mode  {suite.mode}: No coverage tracefiles found\")\n        elif len(trace_files) == 1: # No need to merge, we can just rename the file\n            trace_files[0].rename(str(target_trace_file))\n        else:\n            await coverage_utils.lcov_combine_traces(lcovs = trace_files,\n                                                     output_lcov = target_trace_file,\n                                                     clear_on_success = (not options.coverage_keep_lcovs),\n                                                     files_per_chunk = 10,\n                                                     semaphore = semaphore,\n                                                     logger = logger)\n        lcov_merge_stats_node = stats.get_node(mode_stats.identifier + LCOV_SUITES_MEREGE_STATS)\n        if not lcov_merge_stats_node:\n            lcov_merge_stats_node = stats.create_node(tag = time.time(),\n                                                      identifier = mode_stats.identifier + LCOV_SUITES_MEREGE_STATS,\n                                                      parent = mode_stats,\n                                                      data = Stats(LCOV_SUITES_MEREGE_STATS, 0, 0))\n        stat = stats.create_node(tag = time.time(),\n                                 identifier = lcov_merge_stats_node.identifier + suite.name,\n                                 parent = lcov_merge_stats_node,\n                                 data = Stats(suite.name, pathsize(coverage_path), time.time() - start_time))\n        lcov_merge_stats_node.data += stat.data\n        mode_stats.data.time += stat.data.time\n        mode_stats.data.size = max(mode_stats.data.size, lcov_merge_stats_node.data.size)\n\n        suits_trace_files.setdefault(suite.mode, {})[suite.name] = target_trace_file\n        logger.info(f\"{suite.name}: Done combinig lcov trace files - {humanfriendly.format_timespan(stat.data.time)}\")\n\n    #4. combine the suite lcovs into per mode trace files\n    modes_trace_files  = {}\n    for mode, suite_traces in suits_trace_files.items():\n\n        target_trace_file = pathlib.Path(options.tmpdir) / mode / \"coverage\" / f\"{mode}_coverage.info\"\n        start_time = time.time()\n        logger.info(f\"Consolidating trace files for mode {mode}.\")\n        await coverage_utils.lcov_combine_traces(lcovs = suite_traces.values(),\n                                                 output_lcov = target_trace_file,\n                                                 clear_on_success = False,\n                                                 files_per_chunk = 10,\n                                                 semaphore = semaphore,\n                                                 logger = logger)\n        mode_stats = stats[mode]\n        stat = stats.create_node(tag = time.time(),\n                                 identifier = mode_stats.identifier + LCOV_MODES_MERGE_STATS,\n                                 parent = mode_stats,\n                                 data = Stats(LCOV_MODES_MERGE_STATS, None, time.time() - start_time))\n        mode_stats.data.time += stat.data.time\n        ROOT_NODE.data.size += mode_stats.data.size\n        modes_trace_files[mode] = target_trace_file\n        logger.info(f\"Done consolidating trace files for mode {mode} - time: {humanfriendly.format_timespan(stat.data.time)}.\")\n    #5. create one consolidated file with all trace information\n    logger.info(f\"Consolidating all trace files for this run.\")\n    start_time = time.time()\n    target_trace_file = pathlib.Path(options.tmpdir) / \"test_coverage.info\"\n    await coverage_utils.lcov_combine_traces(lcovs = modes_trace_files.values(),\n                                             output_lcov = target_trace_file,\n                                             clear_on_success = False,\n                                             files_per_chunk = 10,\n                                             semaphore = semaphore,\n                                             logger = logger)\n    stats.create_node(tag = time.time(),\n                      identifier = LCOV_MERGE_ALL_STATS,\n                      parent = ROOT_NODE,\n                      data = Stats(LCOV_MERGE_ALL_STATS, None, time.time() - start_time))\n    logger.info(f\"Done consolidating all trace files for this run - time: {humanfriendly.format_timespan(time.time() - start_time)}.\")\n\n    logger.info(f\"Creating textual report.\")\n    proc = await asyncio.create_subprocess_shell(f\"lcov --summary --rc lcov_branch_coverage=1 {options.tmpdir}/test_coverage.info 2>/dev/null > {options.tmpdir}/test_coverage_report.txt\")\n    await proc.wait()\n    with open(pathlib.Path(options.tmpdir) /\"test_coverage_report.txt\") as f:\n        summary = f.readlines()\n    proc = await asyncio.create_subprocess_shell(f\"lcov --list --rc lcov_branch_coverage=1 {options.tmpdir}/test_coverage.info  2>/dev/null >> {options.tmpdir}/test_coverage_report.txt\")\n    await proc.wait()\n    logger.info(f\"Done creating textual report. ({options.tmpdir}/test_coverage_report.txt)\")\n    total_processing_time = time.time() - total_processing_time\n    ROOT_NODE.data.time = total_processing_time\n\n\n\n\n    stats_str =\"\\n\" + stats.show(stdout=False,\n                                 data_property=\"asstring\")\n    summary = [\"\\n\" + l for l in summary]\n    logger.info(stats_str)\n    logger.info(\"\".join(summary))\n\nasync def workaround_python26789() -> int:\n    \"\"\"Workaround for https://bugs.python.org/issue26789.\n    We'd like to print traceback if there is an internal error\n    in test.py. However, traceback module calls asyncio\n    default_exception_handler which in turns calls logging\n    module after it has been shut down. This leads to a nested\n    exception. Until 3.10 is in widespread use, reset the\n    asyncio exception handler before printing traceback.\"\"\"\n    try:\n        code = await main()\n    except (Exception, KeyboardInterrupt):\n        def noop(x, y):\n            return None\n        asyncio.get_running_loop().set_exception_handler(noop)\n        traceback.print_exc()\n        # Clear the custom handler\n        asyncio.get_running_loop().set_exception_handler(None)\n        return -1\n    return code\n\n\nif __name__ == \"__main__\":\n    colorama.init()\n    # gh-16583: ignore the inherited client host's ScyllaDB environment,\n    # since it may break the tests\n    if \"SCYLLA_CONF\" in os.environ:\n        del os.environ[\"SCYLLA_CONF\"]\n    if \"SCYLLA_HOME\" in os.environ:\n        del os.environ[\"SCYLLA_HOME\"]\n\n    if sys.version_info < (3, 7):\n        print(\"Python 3.7 or newer is required to run this program\")\n        sys.exit(-1)\n    sys.exit(asyncio.run(workaround_python26789()))\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "timeout_config.cc",
          "type": "blob",
          "size": 1.2470703125,
          "content": "/*\n * Copyright (C) 2020-present ScyllaDB\n *\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include \"timeout_config.hh\"\n#include \"db/config.hh\"\n#include <chrono>\n#include <seastar/core/future.hh>\n\nusing namespace std::chrono_literals;\n\nupdateable_timeout_config::updateable_timeout_config(const db::config& cfg)\n    : read_timeout_in_ms(cfg.read_request_timeout_in_ms)\n    , write_timeout_in_ms(cfg.write_request_timeout_in_ms)\n    , range_read_timeout_in_ms(cfg.range_request_timeout_in_ms)\n    , counter_write_timeout_in_ms(cfg.counter_write_request_timeout_in_ms)\n    , truncate_timeout_in_ms(cfg.truncate_request_timeout_in_ms)\n    , cas_timeout_in_ms(cfg.cas_contention_timeout_in_ms)\n    , other_timeout_in_ms(cfg.request_timeout_in_ms)\n{}\n\ntimeout_config updateable_timeout_config::current_values() const {\n    return {\n        std::chrono::milliseconds(read_timeout_in_ms),\n        std::chrono::milliseconds(write_timeout_in_ms),\n        std::chrono::milliseconds(range_read_timeout_in_ms),\n        std::chrono::milliseconds(counter_write_timeout_in_ms),\n        std::chrono::milliseconds(truncate_timeout_in_ms),\n        std::chrono::milliseconds(cas_timeout_in_ms),\n        std::chrono::milliseconds(other_timeout_in_ms),\n    };\n}\n"
        },
        {
          "name": "timeout_config.hh",
          "type": "blob",
          "size": 1.623046875,
          "content": "/*\n * Copyright (C) 2018-present ScyllaDB\n *\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"db/timeout_clock.hh\"\n#include \"utils/updateable_value.hh\"\n\nnamespace db { class config; }\n\nclass updateable_timeout_config;\n\n/// timeout_config represents a snapshot of the options stored in it when\n/// an instance of this class is created. so far this class is only used by\n/// client_state. so either these classes are obliged to\n/// update it by themselves, or they are fine with using the maybe-updated\n/// options in the lifecycle of a client / connection even if some of these\n/// options are changed whtn the client / connection is still alive.\nstruct timeout_config {\n    using duration_t = db::timeout_clock::duration;\n\n    duration_t read_timeout;\n    duration_t write_timeout;\n    duration_t range_read_timeout;\n    duration_t counter_write_timeout;\n    duration_t truncate_timeout;\n    duration_t cas_timeout;\n    duration_t other_timeout;\n};\n\nstruct updateable_timeout_config {\n    using timeout_option_t = utils::updateable_value<uint32_t>;\n\n    timeout_option_t read_timeout_in_ms;\n    timeout_option_t write_timeout_in_ms;\n    timeout_option_t range_read_timeout_in_ms;\n    timeout_option_t counter_write_timeout_in_ms;\n    timeout_option_t truncate_timeout_in_ms;\n    timeout_option_t cas_timeout_in_ms;\n    timeout_option_t other_timeout_in_ms;\n\n    explicit updateable_timeout_config(const db::config& cfg);\n\n    timeout_config current_values() const;\n};\n\n\nusing timeout_config_selector = db::timeout_clock::duration (timeout_config::*);\n\nextern const timeout_config infinite_timeout_config;\n"
        },
        {
          "name": "timestamp.hh",
          "type": "blob",
          "size": 1.3759765625,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <cstdint>\n#include <limits>\n#include <chrono>\n#include <string>\n#include \"clocks-impl.hh\"\n\nnamespace api {\n\nusing timestamp_type = int64_t;\ntimestamp_type constexpr missing_timestamp = std::numeric_limits<timestamp_type>::min();\ntimestamp_type constexpr min_timestamp = std::numeric_limits<timestamp_type>::min() + 1;\ntimestamp_type constexpr max_timestamp = std::numeric_limits<timestamp_type>::max();\n\n// Used for generating server-side mutation timestamps.\n// Same epoch as Java's System.currentTimeMillis() for compatibility.\n// Satisfies requirements of Clock.\nclass timestamp_clock final {\n    using base = std::chrono::system_clock;\npublic:\n    using rep = timestamp_type;\n    using duration = std::chrono::microseconds;\n    using period = typename duration::period;\n    using time_point = std::chrono::time_point<timestamp_clock, duration>;\n\n    static constexpr bool is_steady = base::is_steady;\n\n    static time_point now() {\n        return time_point(std::chrono::duration_cast<duration>(base::now().time_since_epoch())) + get_clocks_offset();\n    }\n};\n\ninline\ntimestamp_type new_timestamp() {\n    return timestamp_clock::now().time_since_epoch().count();\n}\n\n}\n\n/* For debugging and log messages. */\nstd::string format_timestamp(api::timestamp_type);\n"
        },
        {
          "name": "tombstone_gc-internals.hh",
          "type": "blob",
          "size": 0.7431640625,
          "content": "// Copyright (C) 2024-present ScyllaDB\n// SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n\n#include \"tombstone_gc.hh\"\n#include <boost/icl/interval_map.hpp>\n\nusing repair_history_map = boost::icl::interval_map<dht::token, gc_clock::time_point, boost::icl::partial_absorber, std::less, boost::icl::inplace_max>;\n\nclass repair_history_map_ptr {\n    lw_shared_ptr<repair_history_map> _ptr;\npublic:\n    repair_history_map_ptr() = default;\n    repair_history_map_ptr(lw_shared_ptr<repair_history_map> ptr) : _ptr(std::move(ptr)) {}\n    repair_history_map& operator*() const { return _ptr.operator*(); }\n    repair_history_map* operator->() const { return _ptr.operator->(); }\n    explicit operator bool() const { return _ptr.operator bool(); }\n};\n\n\n"
        },
        {
          "name": "tombstone_gc.cc",
          "type": "blob",
          "size": 11.4765625,
          "content": "/*\n * Copyright (C) 2021-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include <chrono>\n#include <boost/icl/interval.hpp>\n#include \"schema/schema.hh\"\n#include \"gc_clock.hh\"\n#include \"tombstone_gc.hh\"\n#include \"tombstone_gc-internals.hh\"\n#include \"locator/token_metadata.hh\"\n#include \"exceptions/exceptions.hh\"\n#include \"locator/abstract_replication_strategy.hh\"\n#include \"replica/database.hh\"\n#include \"data_dictionary/data_dictionary.hh\"\n#include \"gms/feature_service.hh\"\n\nextern logging::logger dblog;\n\nrepair_history_map_ptr tombstone_gc_state::get_or_create_repair_history_for_table(const table_id& id) {\n    if (!_reconcile_history_maps) {\n        return {};\n    }\n    auto& reconcile_history_maps = _reconcile_history_maps->_repair_maps;\n    auto it = reconcile_history_maps.find(id);\n    if (it != reconcile_history_maps.end()) {\n        return it->second;\n    }\n    reconcile_history_maps[id] = seastar::make_lw_shared<repair_history_map>();\n    return reconcile_history_maps[id];\n}\n\nrepair_history_map_ptr tombstone_gc_state::get_repair_history_for_table(const table_id& id) const {\n    if (!_reconcile_history_maps) {\n        return {};\n    }\n    auto& reconcile_history_maps = _reconcile_history_maps->_repair_maps;\n    auto it = reconcile_history_maps.find(id);\n    if (it != reconcile_history_maps.end()) {\n        return it->second;\n    }\n    return {};\n}\n\nseastar::lw_shared_ptr<gc_clock::time_point> tombstone_gc_state::get_or_create_group0_gc_time() {\n    if (!_reconcile_history_maps) {\n        return {};\n    }\n    if (!_reconcile_history_maps->_group0_gc_time) {\n        _reconcile_history_maps->_group0_gc_time = seastar::make_lw_shared<gc_clock::time_point>();\n    }\n    return _reconcile_history_maps->_group0_gc_time;\n}\n\nseastar::lw_shared_ptr<gc_clock::time_point> tombstone_gc_state::get_group0_gc_time() const {\n    if (!_reconcile_history_maps) {\n        return {};\n    }\n    if (!_reconcile_history_maps->_group0_gc_time) {\n        return {};\n    }\n    return _reconcile_history_maps->_group0_gc_time;\n}\n\ngc_clock::time_point tombstone_gc_state::get_gc_before_for_group0(schema_ptr s) const {\n    // use the reconcile mode for group0 tables with 0 propagation delay\n    auto gc_before = gc_clock::time_point::min();\n    auto m = get_group0_gc_time();\n    if (m) {\n        gc_before = *m;\n    }\n    return check_min(s, gc_before);\n}\n\nvoid tombstone_gc_state::drop_repair_history_for_table(const table_id& id) {\n    _reconcile_history_maps->_repair_maps.erase(id);\n}\n\n// This is useful for a sstable to query a gc_before for a range. The range is\n// defined by the first and last key in the sstable.\n//\n// The min_gc_before and max_gc_before returned are the min and max gc_before for all the keys in the range.\n//\n// The knows_entire_range is set to true:\n// 1) if the tombstone_gc_mode is not repair, since we have the same value for all the keys in the ranges.\n// 2) if the tombstone_gc_mode is repair, and the range is a sub range of a range in the repair history map.\ntombstone_gc_state::get_gc_before_for_range_result tombstone_gc_state::get_gc_before_for_range(schema_ptr s, const dht::token_range& range, const gc_clock::time_point& query_time) const {\n    bool knows_entire_range = true;\n\n    if (s->static_props().is_group0_table) {\n        const auto gc_before = get_gc_before_for_group0(s);\n        dblog.trace(\"Get gc_before for ks={}, table={}, range={}, mode=reconcile, gc_before={}\", s->ks_name(), s->cf_name(), range, gc_before);\n        return {gc_before, gc_before, knows_entire_range};\n    }\n\n    const auto& options = s->tombstone_gc_options();\n    switch (options.mode()) {\n    case tombstone_gc_mode::timeout: {\n        dblog.trace(\"Get gc_before for ks={}, table={}, range={}, mode=timeout\", s->ks_name(), s->cf_name(), range);\n        auto gc_before = check_min(s, saturating_subtract(query_time, s->gc_grace_seconds()));\n        return {gc_before, gc_before, knows_entire_range};\n    }\n    case tombstone_gc_mode::disabled: {\n        dblog.trace(\"Get gc_before for ks={}, table={}, range={}, mode=disabled\", s->ks_name(), s->cf_name(), range);\n        return {gc_clock::time_point::min(), gc_clock::time_point::min(), knows_entire_range};\n    }\n    case tombstone_gc_mode::immediate: {\n        dblog.trace(\"Get gc_before for ks={}, table={}, range={}, mode=immediate\", s->ks_name(), s->cf_name(), range);\n        auto t = check_min(s, query_time);\n        return {t, t, knows_entire_range};\n    }\n    case tombstone_gc_mode::repair: {\n        const std::chrono::seconds& propagation_delay = options.propagation_delay_in_seconds();\n        auto min_gc_before = gc_clock::time_point::min();\n        auto max_gc_before = gc_clock::time_point::min();\n        auto min_repair_timestamp = gc_clock::time_point::min();\n        auto max_repair_timestamp = gc_clock::time_point::min();\n        int hits = 0;\n        knows_entire_range = false;\n        auto m = get_repair_history_for_table(s->id());\n        if (m) {\n            auto interval = locator::token_metadata::range_to_interval(range);\n            auto min = gc_clock::time_point::max();\n            auto max = gc_clock::time_point::min();\n            bool contains_all = false;\n            for (const auto& [i, s] = m->equal_range(interval); auto& x : std::ranges::subrange(i, s)) {\n                auto r = locator::token_metadata::interval_to_range(x.first);\n                min = std::min(x.second, min);\n                max = std::max(x.second, max);\n                if (++hits == 1 && r.contains(range, dht::token_comparator{})) {\n                    contains_all = true;\n                }\n            }\n            if (hits == 0) {\n                min_repair_timestamp = gc_clock::time_point::min();\n                max_repair_timestamp = gc_clock::time_point::min();\n            } else {\n                knows_entire_range = hits == 1 && contains_all;\n                min_repair_timestamp = min;\n                max_repair_timestamp = max;\n            }\n            min_gc_before = check_min(s, saturating_subtract(min_repair_timestamp, propagation_delay));\n            max_gc_before = check_min(s, saturating_subtract(max_repair_timestamp, propagation_delay));\n        };\n        dblog.trace(\"Get gc_before for ks={}, table={}, range={}, mode=repair, min_repair_timestamp={}, max_repair_timestamp={}, propagation_delay={}, min_gc_before={}, max_gc_before={}, hits={}, knows_entire_range={}\",\n                s->ks_name(), s->cf_name(), range, min_repair_timestamp, max_repair_timestamp, propagation_delay.count(), min_gc_before, max_gc_before, hits, knows_entire_range);\n        return {min_gc_before, max_gc_before, knows_entire_range};\n    }\n    }\n    std::abort();\n}\n\nbool tombstone_gc_state::cheap_to_get_gc_before(const schema& s) const noexcept {\n    return s.tombstone_gc_options().mode() != tombstone_gc_mode::repair;\n}\n\ngc_clock::time_point tombstone_gc_state::check_min(schema_ptr s, gc_clock::time_point t) const {\n    if (_gc_min_source && t != gc_clock::time_point::min()) {\n        return std::min(t, _gc_min_source(s->id()));\n    }\n    return t;\n}\n\ngc_clock::time_point tombstone_gc_state::get_gc_before_for_key(schema_ptr s, const dht::decorated_key& dk, const gc_clock::time_point& query_time) const {\n    if (s->static_props().is_group0_table) {\n        const auto gc_before = get_gc_before_for_group0(s);\n        dblog.trace(\"Get gc_before for ks={}, table={}, dk={}, mode=reconcile, gc_before={}\", s->ks_name(), s->cf_name(), dk, gc_before);\n        return gc_before;\n    }\n\n    // if mode = timeout    // default option, if user does not specify tombstone_gc options\n    // if mode = disabled   // never gc tombstone\n    // if mode = immediate  // can gc tombstone immediately\n    // if mode = repair     // gc after repair\n    const auto& options = s->tombstone_gc_options();\n    switch (options.mode()) {\n    case tombstone_gc_mode::timeout:\n        dblog.trace(\"Get gc_before for ks={}, table={}, dk={}, mode=timeout\", s->ks_name(), s->cf_name(), dk);\n        return check_min(s, saturating_subtract(query_time, s->gc_grace_seconds()));\n    case tombstone_gc_mode::disabled:\n        dblog.trace(\"Get gc_before for ks={}, table={}, dk={}, mode=disabled\", s->ks_name(), s->cf_name(), dk);\n        return gc_clock::time_point::min();\n    case tombstone_gc_mode::immediate:\n        dblog.trace(\"Get gc_before for ks={}, table={}, dk={}, mode=immediate\", s->ks_name(), s->cf_name(), dk);\n        return check_min(s, query_time);\n    case tombstone_gc_mode::repair:\n        const std::chrono::seconds& propagation_delay = options.propagation_delay_in_seconds();\n        auto gc_before = gc_clock::time_point::min();\n        auto repair_timestamp = gc_clock::time_point::min();\n        auto m = get_repair_history_for_table(s->id());\n        if (m) {\n            const auto it = m->find(dk.token());\n            if (it == m->end()) {\n                gc_before = gc_clock::time_point::min();\n            } else {\n                repair_timestamp = it->second;\n                gc_before = saturating_subtract(repair_timestamp, propagation_delay);\n            }\n        }\n        gc_before = check_min(s, gc_before);\n        dblog.trace(\"Get gc_before for ks={}, table={}, dk={}, mode=repair, repair_timestamp={}, propagation_delay={}, gc_before={}\",\n                s->ks_name(), s->cf_name(), dk, repair_timestamp, propagation_delay.count(), gc_before);\n        return gc_before;\n    }\n    std::abort();\n}\n\nvoid tombstone_gc_state::update_repair_time(table_id id, const dht::token_range& range, gc_clock::time_point repair_time) {\n    auto m = get_or_create_repair_history_for_table(id);\n    if (!m) {\n        on_fatal_internal_error(dblog, \"repair_history_map not found/created\");\n    }\n    *m += std::make_pair(locator::token_metadata::range_to_interval(range), repair_time);\n}\n\nvoid tombstone_gc_state::update_group0_refresh_time(gc_clock::time_point refresh_time) {\n    auto m = get_or_create_group0_gc_time();\n    if (!m) {\n        on_fatal_internal_error(dblog, \"group0_gc_time not found/created\");\n    }\n    *m = refresh_time;\n}\n\nstatic bool needs_repair_before_gc(const replica::database& db, sstring ks_name) {\n    // If a table uses local replication strategy or rf one, there is no\n    // need to run repair even if tombstone_gc mode = repair.\n    auto& ks = db.find_keyspace(ks_name);\n    auto& rs = ks.get_replication_strategy();\n    bool needs_repair = rs.get_type() != locator::replication_strategy_type::local\n            && rs.get_replication_factor(db.get_token_metadata()) != 1;\n    return needs_repair;\n}\n\nstatic bool requires_repair_before_gc(data_dictionary::database db, sstring ks_name) {\n    auto real_db_ptr = db.real_database_ptr();\n    if (!real_db_ptr) {\n        return false;\n    }\n\n    const auto& rs = db.find_keyspace(ks_name).get_replication_strategy();\n    return rs.uses_tablets() && needs_repair_before_gc(*real_db_ptr, ks_name);\n}\n\nstd::map<sstring, sstring> get_default_tombstonesonte_gc_mode(data_dictionary::database db, sstring ks_name) {\n    return {{\"mode\", requires_repair_before_gc(db, ks_name) ? \"repair\" : \"timeout\"}};\n}\n\nvoid validate_tombstone_gc_options(const tombstone_gc_options* options, data_dictionary::database db, sstring ks_name) {\n    if (!options) {\n        return;\n    }\n    if (!db.features().tombstone_gc_options) {\n        throw exceptions::configuration_exception(\"tombstone_gc option not supported by the cluster\");\n    }\n\n    if (options->mode() == tombstone_gc_mode::repair && !needs_repair_before_gc(db.real_database(), ks_name)) {\n        throw exceptions::configuration_exception(\"tombstone_gc option with mode = repair not supported for table with RF one or local replication strategy\");\n    }\n}\n"
        },
        {
          "name": "tombstone_gc.hh",
          "type": "blob",
          "size": 3.7177734375,
          "content": "/*\n * Copyright (C) 2021-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <seastar/core/shared_ptr.hh>\n#include \"gc_clock.hh\"\n#include \"dht/token.hh\"\n#include \"schema/schema_fwd.hh\"\n#include \"interval.hh\"\n\nnamespace dht {\n\nclass decorated_key;\n\nusing token_range = interval<token>;\n\n}\n\nnamespace data_dictionary {\n\nclass database;\n\n}\n\n// The map stores the repair time for each token range in the table.\n// The repair time is the time of the last \"repair\" operation on the table together with the token range.\n//\n// The map is used to determine the time when the tombstones can be safely removed from the table (for the tables with\n// the \"repair\" tombstone GC mode).\nclass repair_history_map_ptr;\n\nclass per_table_history_maps {\npublic:\n    std::unordered_map<table_id, repair_history_map_ptr> _repair_maps;\n\n    // Separating the group0 GC time - it is not kept per table, but for the whole group0:\n    // - the state_id of the last mutation applies to all group0 tables wrt. the tombstone GC\n    // - we also always use the full token range for the group0 tables (so we don't need to store the token ranges)\n    seastar::lw_shared_ptr<gc_clock::time_point> _group0_gc_time;\n};\n\nclass tombstone_gc_options;\n\nusing gc_time_min_source = std::function<gc_clock::time_point(const table_id&)>;\n\nclass tombstone_gc_state {\n    gc_time_min_source _gc_min_source;\n    per_table_history_maps* _reconcile_history_maps;\n    [[nodiscard]] gc_clock::time_point check_min(schema_ptr, gc_clock::time_point) const;\n\n    [[nodiscard]] repair_history_map_ptr get_repair_history_for_table(const table_id& id) const;\n    [[nodiscard]] repair_history_map_ptr get_or_create_repair_history_for_table(const table_id& id);\n\n    [[nodiscard]] seastar::lw_shared_ptr<gc_clock::time_point> get_group0_gc_time() const;\n    [[nodiscard]] seastar::lw_shared_ptr<gc_clock::time_point> get_or_create_group0_gc_time();\n\n    [[nodiscard]] gc_clock::time_point get_gc_before_for_group0(schema_ptr s) const;\n\npublic:\n    tombstone_gc_state() = delete;\n    explicit tombstone_gc_state(per_table_history_maps* maps) noexcept : _reconcile_history_maps(maps) {}\n\n    explicit operator bool() const noexcept {\n        return _reconcile_history_maps != nullptr;\n    }\n\n    void set_gc_time_min_source(gc_time_min_source src) {\n        _gc_min_source = std::move(src);\n    }\n\n    // Returns true if it's cheap to retrieve gc_before, e.g. the mode will not require accessing a system table.\n    [[nodiscard]] bool cheap_to_get_gc_before(const schema& s) const noexcept;\n\n    void drop_repair_history_for_table(const table_id& id);\n\n    struct get_gc_before_for_range_result {\n        gc_clock::time_point min_gc_before;\n        gc_clock::time_point max_gc_before;\n        bool knows_entire_range{};\n    };\n\n    [[nodiscard]] get_gc_before_for_range_result get_gc_before_for_range(schema_ptr s, const dht::token_range& range, const gc_clock::time_point& query_time) const;\n\n    [[nodiscard]] gc_clock::time_point get_gc_before_for_key(schema_ptr s, const dht::decorated_key& dk, const gc_clock::time_point& query_time) const;\n\n    void update_repair_time(table_id id, const dht::token_range& range, gc_clock::time_point repair_time);\n    void update_group0_refresh_time(gc_clock::time_point refresh_time);\n\n    // returns a tombstone_gc_state copy with the commitlog check disabled (i.e.) without _gc_min_source.\n    [[nodiscard]] tombstone_gc_state with_commitlog_check_disabled() const { return tombstone_gc_state(_reconcile_history_maps); }\n};\n\nstd::map<sstring, sstring> get_default_tombstonesonte_gc_mode(data_dictionary::database db, sstring ks_name);\nvoid validate_tombstone_gc_options(const tombstone_gc_options* options, data_dictionary::database db, sstring ks_name);\n"
        },
        {
          "name": "tombstone_gc_extension.hh",
          "type": "blob",
          "size": 1.421875,
          "content": "/*\n * Copyright 2021-present ScyllaDB\n */\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <map>\n\n#include <seastar/core/sstring.hh>\n\n#include \"bytes.hh\"\n#include \"schema/schema.hh\"\n#include \"serializer_impl.hh\"\n#include \"tombstone_gc_options.hh\"\n\nclass tombstone_gc_extension : public schema_extension {\n    tombstone_gc_options _tombstone_gc_options;\npublic:\n    static constexpr auto NAME = \"tombstone_gc\";\n\n    tombstone_gc_extension() = default;\n    tombstone_gc_extension(const tombstone_gc_options& opts) : _tombstone_gc_options(opts) {}\n    explicit tombstone_gc_extension(std::map<seastar::sstring, seastar::sstring> tags) : _tombstone_gc_options(std::move(tags)) {}\n    explicit tombstone_gc_extension(const bytes& b) : _tombstone_gc_options(tombstone_gc_extension::deserialize(b)) {}\n    explicit tombstone_gc_extension(const seastar::sstring& s) {\n        throw std::logic_error(\"Cannot create tombstone_gc_extension info from string\");\n    }\n    bytes serialize() const override {\n        return ser::serialize_to_buffer<bytes>(_tombstone_gc_options.to_map());\n    }\n    static std::map<seastar::sstring, seastar::sstring> deserialize(const bytes_view& buffer) {\n        return ser::deserialize_from_buffer(buffer, std::type_identity<std::map<seastar::sstring, seastar::sstring>>());\n    }\n    const tombstone_gc_options& get_options() const {\n        return _tombstone_gc_options;\n    }\n};\n\n"
        },
        {
          "name": "tombstone_gc_options.cc",
          "type": "blob",
          "size": 2.69140625,
          "content": "/*\n * Copyright (C) 2021-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n\n#include \"tombstone_gc_options.hh\"\n#include \"exceptions/exceptions.hh\"\n#include <boost/lexical_cast.hpp>\n#include <seastar/core/sstring.hh>\n#include <map>\n#include \"utils/rjson.hh\"\n\ntombstone_gc_options::tombstone_gc_options(const std::map<seastar::sstring, seastar::sstring>& map) {\n    for (const auto& x : map) {\n        if (x.first == \"mode\") {\n            if (x.second == \"disabled\") {\n                _mode = tombstone_gc_mode::disabled;\n            } else if (x.second == \"repair\") {\n                _mode = tombstone_gc_mode::repair;\n            } else if (x.second == \"timeout\") {\n                _mode = tombstone_gc_mode::timeout;\n            } else if (x.second == \"immediate\") {\n                _mode = tombstone_gc_mode::immediate;\n            } else  {\n                throw exceptions::configuration_exception(format(\"Invalid value for tombstone_gc option mode: {}\", x.second));\n            }\n        } else if (x.first == \"propagation_delay_in_seconds\") {\n            try {\n                auto seconds = boost::lexical_cast<int64_t>(x.second);\n                if (seconds < 0) {\n                    throw exceptions::configuration_exception(format(\"Invalid value for tombstone_gc option propagation_delay_in_seconds: {}\", x.second));\n                }\n                _propagation_delay_in_seconds = std::chrono::seconds(seconds);\n            } catch (...) {\n                throw exceptions::configuration_exception(format(\"Invalid value for tombstone_gc option propagation_delay_in_seconds: {}\", x.second));\n            }\n        } else {\n            throw exceptions::configuration_exception(format(\"Invalid tombstone_gc option: {}\", x.first));\n        }\n    }\n}\n\nstd::map<seastar::sstring, seastar::sstring> tombstone_gc_options::to_map() const {\n    std::map<seastar::sstring, seastar::sstring> res = {\n        {\"mode\", format(\"{}\", _mode)},\n        {\"propagation_delay_in_seconds\", format(\"{}\", _propagation_delay_in_seconds.count())},\n    };\n    return res;\n}\n\nseastar::sstring tombstone_gc_options::to_sstring() const {\n    return rjson::print(rjson::from_string_map(to_map()));\n}\n\nauto fmt::formatter<tombstone_gc_mode>::format(tombstone_gc_mode mode, fmt::format_context& ctx) const\n        -> decltype(ctx.out()) {\n    std::string_view name = \"unknown\";\n    switch (mode) {\n    case tombstone_gc_mode::timeout:     name = \"timeout\"; break;\n    case tombstone_gc_mode::disabled:    name = \"disabled\"; break;\n    case tombstone_gc_mode::immediate:   name = \"immediate\"; break;\n    case tombstone_gc_mode::repair:      name = \"repair\"; break;\n    }\n    return formatter<string_view>::format(name, ctx);\n}\n"
        },
        {
          "name": "tombstone_gc_options.hh",
          "type": "blob",
          "size": 1.1455078125,
          "content": "/*\n * Copyright (C) 2021-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <map>\n#include <chrono>\n#include <fmt/core.h>\n#include <seastar/core/sstring.hh>\n\nenum class tombstone_gc_mode : uint8_t { timeout, disabled, immediate, repair };\n\nclass tombstone_gc_options {\nprivate:\n    tombstone_gc_mode _mode = tombstone_gc_mode::timeout;\n    std::chrono::seconds _propagation_delay_in_seconds = std::chrono::seconds(3600);\npublic:\n    tombstone_gc_options() = default;\n    const tombstone_gc_mode& mode() const { return _mode; }\n    explicit tombstone_gc_options(const std::map<seastar::sstring, seastar::sstring>& map);\n    const std::chrono::seconds& propagation_delay_in_seconds() const {\n        return _propagation_delay_in_seconds;\n    }\n    std::map<seastar::sstring, seastar::sstring> to_map() const;\n    seastar::sstring to_sstring() const;\n    bool operator==(const tombstone_gc_options&) const = default;\n};\n\ntemplate <> struct fmt::formatter<tombstone_gc_mode> : fmt::formatter<string_view> {\n    auto format(tombstone_gc_mode mode, fmt::format_context& ctx) const -> decltype(ctx.out());\n};\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 0.2451171875,
          "content": "[tox]\nusedevelop=True\nskipdist=True\n\n[testenv]\ndeps =\n    pytest\n    pytest-flake8\n\ncommands =\n    pytest --flake8 -k 'not tests'\n\n[flake8]\nmax-line-length = 120\nexclude = .ropeproject,.tox,tests\nshow-source = False\n\n[pytest]\nflake8-ignore =\n    E501\n"
        },
        {
          "name": "tracing",
          "type": "tree",
          "content": null
        },
        {
          "name": "transport",
          "type": "tree",
          "content": null
        },
        {
          "name": "types",
          "type": "tree",
          "content": null
        },
        {
          "name": "ubsan-suppressions.supp",
          "type": "blob",
          "size": 0.2197265625,
          "content": "# https://github.com/Tencent/rapidjson/commit/16872af88915176f49e389defb167f899e2c230a\npointer-overflow:rapidjson/internal/stack.h\n# https://github.com/boostorg/container/issues/171\npointer-overflow:boost/container/deque.hpp\n"
        },
        {
          "name": "unified",
          "type": "tree",
          "content": null
        },
        {
          "name": "unimplemented.cc",
          "type": "blob",
          "size": 2.1435546875,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include <unordered_map>\n#include \"unimplemented.hh\"\n#include <seastar/core/sstring.hh>\n#include <seastar/core/enum.hh>\n#include \"utils/log.hh\"\n#include \"seastarx.hh\"\n\nnamespace unimplemented {\n\nstatic thread_local std::unordered_map<cause, bool> _warnings;\n\nstatic logging::logger ulogger(\"unimplemented\");\n\nstd::string_view format_as(cause c) {\n    switch (c) {\n        case cause::INDEXES: return \"INDEXES\";\n        case cause::LWT: return \"LWT\";\n        case cause::PAGING: return \"PAGING\";\n        case cause::AUTH: return \"AUTH\";\n        case cause::PERMISSIONS: return \"PERMISSIONS\";\n        case cause::TRIGGERS: return \"TRIGGERS\";\n        case cause::COUNTERS: return \"COUNTERS\";\n        case cause::METRICS: return \"METRICS\";\n        case cause::MIGRATIONS: return \"MIGRATIONS\";\n        case cause::GOSSIP: return \"GOSSIP\";\n        case cause::TOKEN_RESTRICTION: return \"TOKEN_RESTRICTION\";\n        case cause::LEGACY_COMPOSITE_KEYS: return \"LEGACY_COMPOSITE_KEYS\";\n        case cause::COLLECTION_RANGE_TOMBSTONES: return \"COLLECTION_RANGE_TOMBSTONES\";\n        case cause::RANGE_DELETES: return \"RANGE_DELETES\";\n        case cause::VALIDATION: return \"VALIDATION\";\n        case cause::REVERSED: return \"REVERSED\";\n        case cause::COMPRESSION: return \"COMPRESSION\";\n        case cause::NONATOMIC: return \"NONATOMIC\";\n        case cause::CONSISTENCY: return \"CONSISTENCY\";\n        case cause::HINT: return \"HINT\";\n        case cause::SUPER: return \"SUPER\";\n        case cause::WRAP_AROUND: return \"WRAP_AROUND\";\n        case cause::STORAGE_SERVICE: return \"STORAGE_SERVICE\";\n        case cause::API: return \"API\";\n        case cause::SCHEMA_CHANGE: return \"SCHEMA_CHANGE\";\n        case cause::MIXED_CF: return \"MIXED_CF\";\n        case cause::SSTABLE_FORMAT_M: return \"SSTABLE_FORMAT_M\";\n    }\n    abort();\n}\n\nvoid warn(cause c) {\n    if (!_warnings.contains(c)) {\n        _warnings.insert({c, true});\n        ulogger.debug(\"{}\", format_as(c));\n    }\n}\n\nvoid fail(cause c) {\n    throw std::runtime_error(fmt::format(\"Not implemented: {}\", format_as(c)));\n}\n\n}\n"
        },
        {
          "name": "unimplemented.hh",
          "type": "blob",
          "size": 0.958984375,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <seastar/core/format.hh>\n#include <seastar/core/sstring.hh>\n#include <seastar/core/enum.hh>\n\nnamespace unimplemented {\n\nenum class cause {\n    API,\n    INDEXES,\n    LWT,\n    PAGING,\n    AUTH,\n    PERMISSIONS,\n    TRIGGERS,\n    COUNTERS,\n    METRICS,\n    MIGRATIONS,\n    GOSSIP,\n    TOKEN_RESTRICTION,\n    LEGACY_COMPOSITE_KEYS,\n    COLLECTION_RANGE_TOMBSTONES,\n    RANGE_DELETES,\n    VALIDATION,\n    REVERSED,\n    COMPRESSION,\n    NONATOMIC,\n    CONSISTENCY,\n    HINT,\n    SUPER,\n    WRAP_AROUND, // Support for handling wrap around ranges in queries on database level and below\n    STORAGE_SERVICE,\n    SCHEMA_CHANGE,\n    MIXED_CF,\n    SSTABLE_FORMAT_M,\n};\n\n[[noreturn]] void fail(cause what);\nvoid warn(cause what);\n\n}\n\nnamespace std {\n\ntemplate <>\nstruct hash<unimplemented::cause> : seastar::enum_hash<unimplemented::cause> {};\n\n}\n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "validation.cc",
          "type": "blob",
          "size": 2.3271484375,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n *\n * Modified by ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: (LicenseRef-ScyllaDB-Source-Available-1.0 and Apache-2.0)\n */\n\n#include \"validation.hh\"\n#include \"schema/schema.hh\"\n#include \"keys.hh\"\n#include \"data_dictionary/data_dictionary.hh\"\n#include \"exceptions/exceptions.hh\"\n\nnamespace validation {\n\n/**\n * Based on org.apache.cassandra.thrift.ThriftValidation#validate_key()\n */\nstd::optional<sstring> is_cql_key_invalid(const schema& schema, partition_key_view key) {\n    // C* validates here that the thrift key is not empty.\n    // It can only be empty if it is not composite and its only component in CQL form is empty.\n    if (schema.partition_key_size() == 1 && (*key.begin(schema)).empty()) {\n        return sstring(\"Key may not be empty\");\n    }\n\n    // check that key can be handled by FBUtilities.writeShortByteArray\n    auto b = key.representation();\n    if (b.size() > max_key_size) {\n        return format(\"Key length of {:d} is longer than maximum of {:d}\", b.size(), max_key_size);\n    }\n\n    try {\n        key.validate(schema);\n    } catch (const marshal_exception& e) {\n        return sstring(e.what());\n    }\n\n    return std::nullopt;\n}\n\nvoid\nvalidate_cql_key(const schema& schema, partition_key_view key) {\n    if (const auto err = is_cql_key_invalid(schema, key); err) {\n        throw exceptions::invalid_request_exception(std::move(*err));\n    }\n}\n\n/**\n * Based on org.apache.cassandra.thrift.ThriftValidation#validateColumnFamily(java.lang.String, java.lang.String)\n */\nschema_ptr\nvalidate_column_family(data_dictionary::database db, const sstring& keyspace_name, const sstring& cf_name) {\n    validate_keyspace(db, keyspace_name);\n\n    if (cf_name.empty()) {\n        throw exceptions::invalid_request_exception(\"non-empty table is required\");\n    }\n\n    auto t = db.try_find_table(keyspace_name, cf_name);\n    if (!t) {\n        throw exceptions::invalid_request_exception(format(\"unconfigured table {}\", cf_name));\n    }\n\n    return t->schema();\n}\n\nvoid validate_keyspace(data_dictionary::database db, const sstring& keyspace_name) {\n    if (keyspace_name.empty()) {\n        throw exceptions::invalid_request_exception(\"Keyspace not set\");\n    }\n\n    if (!db.has_keyspace(keyspace_name)) {\n        throw exceptions::keyspace_not_defined_exception(format(\"Keyspace {} does not exist\", keyspace_name));\n    }\n}\n\n}\n"
        },
        {
          "name": "validation.hh",
          "type": "blob",
          "size": 0.8671875,
          "content": "/*\n * Copyright (C) 2015-present ScyllaDB\n *\n * Modified by ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: (LicenseRef-ScyllaDB-Source-Available-1.0 and Apache-2.0)\n */\n\n#pragma once\n\n#include <seastar/core/sstring.hh>\n#include \"schema/schema_fwd.hh\"\n\nusing namespace seastar;\n\nclass partition_key_view;\n\nnamespace data_dictionary {\nclass database;\n}\n\nnamespace validation {\n\nconstexpr size_t max_key_size = std::numeric_limits<uint16_t>::max();\n\n// Returns an error string if key is invalid, a disengaged optional otherwise.\nstd::optional<sstring> is_cql_key_invalid(const schema& schema, partition_key_view key);\nvoid validate_cql_key(const schema& schema, partition_key_view key);\nschema_ptr validate_column_family(data_dictionary::database db, const sstring& keyspace_name, const sstring& cf_name);\nvoid validate_keyspace(data_dictionary::database db, const sstring& keyspace_name);\n\n}\n"
        },
        {
          "name": "version.hh",
          "type": "blob",
          "size": 0.8662109375,
          "content": "\n/*\n * Copyright (C) 2015-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include <seastar/core/sstring.hh>\n#include <seastar/core/format.hh>\n#include <tuple>\n\nnamespace version {\nclass version {\n    std::tuple<uint16_t, uint16_t, uint16_t> _version;\npublic:\n    version(uint16_t x, uint16_t y = 0, uint16_t z = 0): _version(std::make_tuple(x, y, z)) {}\n\n    seastar::sstring to_sstring() {\n        return seastar::format(\"{:d}.{:d}.{:d}\", std::get<0>(_version), std::get<1>(_version), std::get<2>(_version));\n    }\n\n    static version current() {\n        static version v(3, 0, 8);\n        return v;\n    }\n\n    std::strong_ordering operator<=>(const version&) const = default;\n};\n\ninline const seastar::sstring& release() {\n    static thread_local auto str_ver = version::current().to_sstring();\n    return str_ver;\n}\n}\n"
        },
        {
          "name": "view_info.hh",
          "type": "blob",
          "size": 3.2216796875,
          "content": "/*\n * Copyright (C) 2017-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#pragma once\n\n#include \"query-request.hh\"\n#include \"schema/schema_fwd.hh\"\n#include \"db/view/view.hh\"\n\nnamespace cql3::statements {\nclass select_statement;\n}\n\nclass view_info final {\n    const schema& _schema;\n    raw_view_info _raw;\n    // The following fields are used to select base table rows.\n    mutable shared_ptr<cql3::statements::select_statement> _select_statement;\n    mutable std::optional<query::partition_slice> _partition_slice;\n    db::view::base_info_ptr _base_info;\n    mutable bool _has_computed_column_depending_on_base_non_primary_key;\n\n    // True if the partition key columns of the view are the same as the\n    // partition key columns of the base, maybe in a different order.\n    mutable bool _is_partition_key_permutation_of_base_partition_key;\npublic:\n    view_info(const schema& schema, const raw_view_info& raw_view_info);\n\n    const raw_view_info& raw() const {\n        return _raw;\n    }\n\n    const table_id& base_id() const {\n        return _raw.base_id();\n    }\n\n    const sstring& base_name() const {\n        return _raw.base_name();\n    }\n\n    bool include_all_columns() const {\n        return _raw.include_all_columns();\n    }\n\n    const sstring& where_clause() const {\n        return _raw.where_clause();\n    }\n\n    cql3::statements::select_statement& select_statement(data_dictionary::database) const;\n    const query::partition_slice& partition_slice(data_dictionary::database) const;\n    const column_definition* view_column(const schema& base, column_kind kind, column_id base_id) const;\n    const column_definition* view_column(const column_definition& base_def) const;\n    bool has_base_non_pk_columns_in_view_pk() const;\n    bool has_computed_column_depending_on_base_non_primary_key() const {\n        return _has_computed_column_depending_on_base_non_primary_key;\n    }\n\n    bool is_partition_key_permutation_of_base_partition_key() const {\n        return _is_partition_key_permutation_of_base_partition_key;\n    }\n\n    /// Returns a pointer to the base_dependent_view_info which matches the current\n    /// schema of the base table.\n    ///\n    /// base_dependent_view_info lives separately from the view schema.\n    /// It can change without the view schema changing its value.\n    /// This pointer is updated on base table schema changes as long as this view_info\n    /// corresponds to the current schema of the view. After that the pointer stops tracking\n    /// the base table schema.\n    ///\n    /// The snapshot of both the view schema and base_dependent_view_info is represented\n    /// by view_and_base. See with_base_info_snapshot().\n    const db::view::base_info_ptr& base_info() const { return _base_info; }\n    void set_base_info(db::view::base_info_ptr);\n    db::view::base_info_ptr make_base_dependent_view_info(const schema& base_schema) const;\n\n    friend bool operator==(const view_info& x, const view_info& y) {\n        return x._raw == y._raw;\n    }\n    friend fmt::formatter<view_info>;\n};\n\ntemplate <> struct fmt::formatter<view_info> : fmt::formatter<string_view> {\n    auto format(const view_info& view, fmt::format_context& ctx) const {\n        return fmt::format_to(ctx.out(), \"{}\", view._raw);\n    }\n};\n"
        },
        {
          "name": "vint-serialization.cc",
          "type": "blob",
          "size": 4.630859375,
          "content": "/*\n * Copyright 2017-present ScyllaDB\n *\n * Modified by ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: (LicenseRef-ScyllaDB-Source-Available-1.0 and Apache-2.0)\n */\n\n#include \"vint-serialization.hh\"\n\n#include <seastar/core/bitops.hh>\n\n#include <algorithm>\n#include <array>\n#include <limits>\n\nstatic_assert(-1 == ~0, \"Not a twos-complement architecture\");\n\n// Accounts for the case that all bits are zero.\nstatic vint_size_type count_leading_zero_bits(uint64_t n) noexcept {\n    if (n == 0) {\n        return vint_size_type(std::numeric_limits<uint64_t>::digits);\n    }\n\n    return vint_size_type(count_leading_zeros(n));\n}\n\nstatic constexpr uint64_t encode_zigzag(int64_t n) noexcept {\n    // The right shift has to be arithmetic and not logical.\n    return (static_cast<uint64_t>(n) << 1) ^ static_cast<uint64_t>(n >> 63);\n}\n\nstatic constexpr int64_t decode_zigzag(uint64_t n) noexcept {\n    return static_cast<int64_t>((n >> 1) ^ -(n & 1));\n}\n\n// Mask for extracting from the first byte the part that is not used for indicating the total number of bytes.\nstatic uint64_t first_byte_value_mask(vint_size_type extra_bytes_size) {\n    // Include the sentinel zero bit in the mask.\n    return uint64_t(0xff) >> extra_bytes_size;\n}\n\nvint_size_type signed_vint::serialize(int64_t value, bytes::iterator out) {\n    return unsigned_vint::serialize(encode_zigzag(value), out);\n}\n\nvint_size_type signed_vint::serialized_size(int64_t value) noexcept {\n    return unsigned_vint::serialized_size(encode_zigzag(value));\n}\n\nint64_t signed_vint::deserialize(bytes_view v) {\n    const auto un = unsigned_vint::deserialize(v);\n    return decode_zigzag(un);\n}\n\nvint_size_type signed_vint::serialized_size_from_first_byte(bytes::value_type first_byte) {\n    return unsigned_vint::serialized_size_from_first_byte(first_byte);\n}\n\n// The number of additional bytes that we need to read.\nstatic vint_size_type count_extra_bytes(int8_t first_byte) {\n    // Sign extension.\n    const int64_t v(first_byte);\n\n    return count_leading_zero_bits(static_cast<uint64_t>(~v)) - vint_size_type(64 - 8);\n}\n\nstatic void encode(uint64_t value, vint_size_type size, bytes::iterator out) {\n    std::array<int8_t, 9> buffer({});\n\n    // `size` is always in the range [1, 9].\n    const auto extra_bytes_size = size - 1;\n\n    for (vint_size_type i = 0; i <= extra_bytes_size; ++i) {\n        buffer[extra_bytes_size - i] = static_cast<int8_t>(value & 0xff);\n        value >>= 8;\n    }\n\n    buffer[0] |= ~first_byte_value_mask(extra_bytes_size);\n    std::copy_n(buffer.cbegin(), size, out);\n}\n\nvint_size_type unsigned_vint::serialize(uint64_t value, bytes::iterator out) {\n    const auto size = serialized_size(value);\n\n    if (size == 1) {\n        *out = static_cast<int8_t>(value & 0xff);\n        return 1;\n    }\n\n    encode(value, size, out);\n    return size;\n}\n\nvint_size_type unsigned_vint::serialized_size(uint64_t value) noexcept {\n    // No need for the overhead of checking that all bits are zero.\n    //\n    // A signed quantity, to allow the case of `magnitude == 0` to result in a value of 9 below.\n    const auto magnitude = static_cast<int64_t>(count_leading_zeros(value | uint64_t(1)));\n\n    return vint_size_type(9) - vint_size_type((magnitude - 1) / 7);\n}\n\nuint64_t unsigned_vint::deserialize(bytes_view v) {\n    auto src = v.data();\n    auto len = v.size();\n    const int8_t first_byte = *src;\n\n    // No additional bytes, since the most significant bit is not set.\n    if (first_byte >= 0) {\n        return uint64_t(first_byte);\n    }\n\n    const auto extra_bytes_size = count_extra_bytes(first_byte);\n\n    // Extract the bits not used for counting bytes.\n    auto result = uint64_t(first_byte) & first_byte_value_mask(extra_bytes_size);\n\n#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n    uint64_t value;\n    // If we can overread do that. It is cheaper to have a single 64-bit read and\n    // then mask out the unneeded part than to do 8x 1 byte reads.\n    if (__builtin_expect(len >= sizeof(uint64_t) + 1, true)) {\n        std::copy_n(src + 1, sizeof(uint64_t), reinterpret_cast<int8_t*>(&value));\n    } else {\n        value = 0;\n        std::copy_n(src + 1, extra_bytes_size, reinterpret_cast<int8_t*>(&value));\n    }\n    value = be_to_cpu(value << (64 - (extra_bytes_size * 8)));\n    result <<= (extra_bytes_size * 8) % 64;\n    result |= value;\n#else\n    for (vint_size_type index = 0; index < extra_bytes_size; ++index) {\n        result <<= 8;\n        result |= (uint64_t(v[index + 1]) & uint64_t(0xff));\n    }\n#endif\n    return result;\n}\n\nvint_size_type unsigned_vint::serialized_size_from_first_byte(bytes::value_type first_byte) {\n    int8_t first_byte_casted = first_byte;\n    return 1 + (first_byte_casted >= 0 ? 0 : count_extra_bytes(first_byte_casted));\n}\n"
        },
        {
          "name": "vint-serialization.hh",
          "type": "blob",
          "size": 2.62890625,
          "content": "/*\n * Copyright 2017-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n//\n// For reference, see https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v5.spec.\n//\n// Relevant excerpt:\n//\n//     [unsigned vint]   An unsigned variable length integer. A vint is encoded with the most significant byte (MSB) first.\n//                       The most significant byte will contains the information about how many extra bytes need to be read\n//                       as well as the most significant bits of the integer.\n//                       The number of extra bytes to read is encoded as 1 bits on the left side.\n//                       For example, if we need to read 2 more bytes the first byte will start with 110\n//                       (e.g. 256 000 will be encoded on 3 bytes as [110]00011 11101000 00000000)\n//                       If the encoded integer is 8 bytes long the vint will be encoded on 9 bytes and the first\n//                       byte will be: 11111111\n//\n//    [vint]             A signed variable length integer. This is encoded using zig-zag encoding and then sent\n//                       like an [unsigned vint]. Zig-zag encoding converts numbers as follows:\n//                       0 = 0, -1 = 1, 1 = 2, -2 = 3, 2 = 4, -3 = 5, 3 = 6 and so forth.\n//                       The purpose is to send small negative values as small unsigned values, so that we save bytes on the wire.\n//                       To encode a value n use \"(n >> 31) ^ (n << 1)\" for 32 bit values, and \"(n >> 63) ^ (n << 1)\"\n//                       for 64 bit values where \"^\" is the xor operation, \"<<\" is the left shift operation and \">>\" is\n//                       the arithmetic right shift operation (highest-order bit is replicated).\n//                       Decode with \"(n >> 1) ^ -(n & 1)\".\n//\n\n#pragma once\n\n#include \"bytes.hh\"\n\n#include <cstdint>\n\nusing vint_size_type = bytes::size_type;\n\nstatic constexpr size_t max_vint_length = 9;\n\nstruct unsigned_vint final {\n    using value_type = uint64_t;\n\n    static vint_size_type serialized_size(value_type) noexcept;\n\n    static vint_size_type serialize(value_type, bytes::iterator out);\n\n    static value_type deserialize(bytes_view v);\n\n    static vint_size_type serialized_size_from_first_byte(bytes::value_type first_byte);\n};\n\nstruct signed_vint final {\n    using value_type = int64_t;\n\n    static vint_size_type serialized_size(value_type) noexcept;\n\n    static vint_size_type serialize(value_type, bytes::iterator out);\n\n    static value_type deserialize(bytes_view v);\n\n    static vint_size_type serialized_size_from_first_byte(bytes::value_type first_byte);\n};\n"
        },
        {
          "name": "zstd.cc",
          "type": "blob",
          "size": 6.265625,
          "content": "/*\n * Copyright (C) 2019-present ScyllaDB\n */\n\n/*\n * SPDX-License-Identifier: LicenseRef-ScyllaDB-Source-Available-1.0\n */\n\n#include <seastar/core/aligned_buffer.hh>\n\n// We need to use experimental features of the zstd library (to allocate compression/decompression context),\n// which are available only when the library is linked statically.\n#define ZSTD_STATIC_LINKING_ONLY\n#include <zstd.h>\n\n#include \"compress.hh\"\n#include \"exceptions/exceptions.hh\"\n#include \"utils/class_registrator.hh\"\n#include \"utils/reusable_buffer.hh\"\n#include <concepts>\n\nstatic const sstring COMPRESSION_LEVEL = \"compression_level\";\nstatic const sstring COMPRESSOR_NAME = compressor::namespace_prefix + \"ZstdCompressor\";\nstatic const size_t DCTX_SIZE = ZSTD_estimateDCtxSize();\n\nclass zstd_processor : public compressor {\n    int _compression_level = 3;\n    size_t _cctx_size;\n\n    static auto with_dctx(std::invocable<ZSTD_DCtx*> auto f) {\n        // The decompression context has a fixed size of ~128 KiB,\n        // so we don't bother ever resizing it the way we do with\n        // the compression context.\n        static thread_local std::unique_ptr<char[]> buf = std::invoke([&] {\n            auto ptr = std::unique_ptr<char[]>(new char[DCTX_SIZE]);\n            auto dctx = ZSTD_initStaticDCtx(ptr.get(), DCTX_SIZE);\n            if (!dctx) {\n                // Barring a bug, this should never happen.\n                throw std::runtime_error(\"Unable to initialize ZSTD decompression context\");\n            }\n            return ptr;\n        });\n        return f(reinterpret_cast<ZSTD_DCtx*>(buf.get()));\n    }\n\n    static auto with_cctx(size_t cctx_size, std::invocable<ZSTD_CCtx*> auto f) {\n        // See the comments to reusable_buffer for a rationale of using it for compression.\n        static thread_local utils::reusable_buffer<lowres_clock> buf(std::chrono::seconds(600));\n        static thread_local size_t last_seen_reallocs = buf.reallocs();\n        auto guard = utils::reusable_buffer_guard(buf);\n        // Note that the compression context isn't initialized with a particular\n        // compression config, but only with a particular size. As long as\n        // it is big enough, we can reuse a context initialized by an\n        // unrelated instance of zstd_processor without reinitializing it.\n        //\n        // If the existing context isn't big enough, the reusable buffer will\n        // be resized by the next line, and the following `if` will notice that\n        // and reinitialize the context.\n        auto view = guard.get_temporary_buffer(cctx_size);\n        if (last_seen_reallocs != buf.reallocs()) {\n            // Either the buffer just grew because we requested a buffer bigger\n            // than its last capacity, or it was shrunk some time ago by a timer.\n            // Either way, the resize destroyed the contents of the buffer and\n            // we have to initialize the context anew.\n            auto cctx = ZSTD_initStaticCCtx(view.data(), buf.size());\n            if (!cctx) {\n                // Barring a bug, this should never happen.\n                throw std::runtime_error(\"Unable to initialize ZSTD compression context\");\n            }\n            last_seen_reallocs = buf.reallocs();\n        }\n        return f(reinterpret_cast<ZSTD_CCtx*>(view.data()));\n    }\n\npublic:\n    zstd_processor(const opt_getter&);\n\n    size_t uncompress(const char* input, size_t input_len, char* output,\n                    size_t output_len) const override;\n    size_t compress(const char* input, size_t input_len, char* output,\n                    size_t output_len) const override;\n    size_t compress_max_size(size_t input_len) const override;\n\n    std::set<sstring> option_names() const override;\n    std::map<sstring, sstring> options() const override;\n};\n\nzstd_processor::zstd_processor(const opt_getter& opts)\n    : compressor(COMPRESSOR_NAME) {\n    auto level = opts(COMPRESSION_LEVEL);\n    if (level) {\n        try {\n            _compression_level = std::stoi(*level);\n        } catch (const std::exception& e) {\n            throw exceptions::syntax_exception(\n                format(\"Invalid integer value {} for {}\", *level, COMPRESSION_LEVEL));\n        }\n\n        auto min_level = ZSTD_minCLevel();\n        auto max_level = ZSTD_maxCLevel();\n        if (min_level > _compression_level || _compression_level > max_level) {\n            throw exceptions::configuration_exception(\n                format(\"{} must be between {} and {}, got {}\", COMPRESSION_LEVEL, min_level, max_level, _compression_level));\n        }\n    }\n\n    auto chunk_len_kb = opts(compression_parameters::CHUNK_LENGTH_KB);\n    if (!chunk_len_kb) {\n        chunk_len_kb = opts(compression_parameters::CHUNK_LENGTH_KB_ERR);\n    }\n    auto chunk_len = chunk_len_kb\n       // This parameter has already been validated.\n       ? std::stoi(*chunk_len_kb) * 1024\n       : compression_parameters::DEFAULT_CHUNK_LENGTH;\n\n    // We assume that the uncompressed input length is always <= chunk_len.\n    auto cparams = ZSTD_getCParams(_compression_level, chunk_len, 0);\n    _cctx_size = ZSTD_estimateCCtxSize_usingCParams(cparams);\n\n}\n\nsize_t zstd_processor::uncompress(const char* input, size_t input_len, char* output, size_t output_len) const {\n    auto ret = with_dctx([&] (ZSTD_DCtx* dctx) {\n        return ZSTD_decompressDCtx(dctx, output, output_len, input, input_len);\n    });\n    if (ZSTD_isError(ret)) {\n        throw std::runtime_error( format(\"ZSTD decompression failure: {}\", ZSTD_getErrorName(ret)));\n    }\n    return ret;\n}\n\n\nsize_t zstd_processor::compress(const char* input, size_t input_len, char* output, size_t output_len) const {\n    auto ret = with_cctx(_cctx_size, [&] (ZSTD_CCtx* cctx) {\n        return ZSTD_compressCCtx(cctx, output, output_len, input, input_len, _compression_level);\n    });\n    if (ZSTD_isError(ret)) {\n        throw std::runtime_error( format(\"ZSTD compression failure: {}\", ZSTD_getErrorName(ret)));\n    }\n    return ret;\n}\n\nsize_t zstd_processor::compress_max_size(size_t input_len) const {\n    return ZSTD_compressBound(input_len);\n}\n\nstd::set<sstring> zstd_processor::option_names() const {\n    return {COMPRESSION_LEVEL};\n}\n\nstd::map<sstring, sstring> zstd_processor::options() const {\n    return {{COMPRESSION_LEVEL, std::to_string(_compression_level)}};\n}\n\nstatic const class_registrator<compressor, zstd_processor, const compressor::opt_getter&>\n    registrator(COMPRESSOR_NAME);\n"
        }
      ]
    }
  ]
}