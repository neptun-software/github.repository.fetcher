{
  "metadata": {
    "timestamp": 1736566350971,
    "page": 107,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjExMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "microsoft/BitNet",
      "stars": 12578,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3232421875,
          "content": "# Extensions\n\n*.a\n*.bat\n*.bin\n*.dll\n*.dot\n*.etag\n*.exe\n*.gcda\n*.gcno\n*.gcov\n*.gguf\n*.gguf.json\n*.lastModified\n*.log\n*.metallib\n*.o\n*.so\n*.tmp\n\n# IDE / OS\n\n.cache/\n.ccls-cache/\n.direnv/\n.DS_Store\n.envrc\n.idea/\n.swiftpm\n.vs/\n.vscode/\nnppBackup\n\n# Models\nmodels/*\n\n# Python\n\n/.venv\n__pycache__/\n*/poetry.lock\npoetry.toml\n\nbuild/\nlogs/"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.1318359375,
          "content": "[submodule \"3rdparty/llama.cpp\"]\n\tpath = 3rdparty/llama.cpp\n\turl = https://github.com/Eddie-Wang1120/llama.cpp.git\n\tbranch = merge-dev\n"
        },
        {
          "name": "3rdparty",
          "type": "tree",
          "content": null
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 2.6025390625,
          "content": "cmake_minimum_required(VERSION 3.14)  # for add_link_options and implicit target directories.\nproject(\"bitnet.cpp\" C CXX)\ninclude(CheckIncludeFileCXX)\n\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n\nif (NOT XCODE AND NOT MSVC AND NOT CMAKE_BUILD_TYPE)\n    set(CMAKE_BUILD_TYPE Release CACHE STRING \"Build type\" FORCE)\n    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"MinSizeRel\" \"RelWithDebInfo\")\nendif()\n\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\n\n# option list\noption(BITNET_ARM_TL1    \"bitnet.cpp: use tl1 on arm platform\"    OFF)\noption(BITNET_X86_TL2    \"bitnet.cpp: use tl2 on x86 platform\"    OFF)\n\n\nset(CMAKE_CXX_STANDARD_REQUIRED true)\nset(CMAKE_C_STANDARD 11)\nset(CMAKE_C_STANDARD_REQUIRED true)\nset(THREADS_PREFER_PTHREAD_FLAG ON)\n\n# override ggml options\nset(GGML_BITNET_ARM_TL1    ${BITNET_ARM_TL1})\nset(GGML_BITNET_X86_TL2    ${BITNET_X86_TL2})\n\nif (GGML_BITNET_ARM_TL1)\n    add_compile_definitions(GGML_BITNET_ARM_TL1)\nendif()\nif (GGML_BITNET_X86_TL2)\n    add_compile_definitions(GGML_BITNET_X86_TL2)\nendif()\n\nif (CMAKE_C_COMPILER_ID STREQUAL \"GNU\" OR CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n    add_compile_options(-fpermissive)\nendif()\n\nfind_package(Threads REQUIRED)\n\nadd_subdirectory(src)\nadd_subdirectory(3rdparty/llama.cpp)\n\n# install\n\ninclude(GNUInstallDirs)\ninclude(CMakePackageConfigHelpers)\n\nset(LLAMA_INCLUDE_INSTALL_DIR ${CMAKE_INSTALL_INCLUDEDIR}\n    CACHE PATH \"Location of header files\")\nset(LLAMA_LIB_INSTALL_DIR ${CMAKE_INSTALL_LIBDIR}\n    CACHE PATH \"Location of library files\")\nset(LLAMA_BIN_INSTALL_DIR ${CMAKE_INSTALL_BINDIR}\n    CACHE PATH \"Location of binary files\")\nset(LLAMA_BUILD_NUMBER ${BUILD_NUMBER})\nset(LLAMA_BUILD_COMMIT ${BUILD_COMMIT})\nset(LLAMA_INSTALL_VERSION 0.0.${BUILD_NUMBER})\n\nget_target_property(GGML_DIRECTORY ggml SOURCE_DIR)\nget_directory_property(GGML_DIR_DEFINES DIRECTORY ${GGML_DIRECTORY} COMPILE_DEFINITIONS)\nget_target_property(GGML_TARGET_DEFINES ggml COMPILE_DEFINITIONS)\nset(GGML_TRANSIENT_DEFINES ${GGML_TARGET_DEFINES} ${GGML_DIR_DEFINES})\nget_target_property(GGML_LINK_LIBRARIES ggml LINK_LIBRARIES)\n\nget_directory_property(LLAMA_TRANSIENT_DEFINES COMPILE_DEFINITIONS)\n\nwrite_basic_package_version_file(\n        ${CMAKE_CURRENT_BINARY_DIR}/LlamaConfigVersion.cmake\n    VERSION ${LLAMA_INSTALL_VERSION}\n    COMPATIBILITY SameMajorVersion)\n\ninstall(FILES ${CMAKE_CURRENT_BINARY_DIR}/LlamaConfig.cmake\n              ${CMAKE_CURRENT_BINARY_DIR}/LlamaConfigVersion.cmake\n        DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/Llama)\n\nset_target_properties(llama PROPERTIES PUBLIC_HEADER ${CMAKE_CURRENT_SOURCE_DIR}/llama.h)\ninstall(TARGETS llama LIBRARY PUBLIC_HEADER)"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 0.43359375,
          "content": "# Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.1142578125,
          "content": "    MIT License\n\n    Copyright (c) Microsoft Corporation.\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy\n    of this software and associated documentation files (the \"Software\"), to deal\n    in the Software without restriction, including without limitation the rights\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the Software is\n    furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all\n    copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n    SOFTWARE\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.1279296875,
          "content": "# bitnet.cpp\n[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n![version](https://img.shields.io/badge/version-1.0-blue)\n\nbitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support **fast** and **lossless** inference of 1.58-bit models on CPU (with NPU and GPU support coming next).\n\nThe first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of **1.37x** to **5.07x** on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by **55.4%** to **70.0%**, further boosting overall efficiency. On x86 CPUs, speedups range from **2.37x** to **6.17x** with energy reductions between **71.9%** to **82.2%**. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the [technical report](https://arxiv.org/abs/2410.16144) for more details.\n\n<img src=\"./assets/m2_performance.jpg\" alt=\"m2_performance\" width=\"800\"/>\n<img src=\"./assets/intel_performance.jpg\" alt=\"m2_performance\" width=\"800\"/>\n\n>The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.\n\n## Demo\n\nA demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:\n\nhttps://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1\n\n## What's New:\n\n- 11/08/2024 [BitNet a4.8: 4-bit Activations for 1-bit LLMs](https://arxiv.org/abs/2411.04965) ![NEW](https://img.shields.io/badge/NEW-red) \n- 10/21/2024 [1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs](https://arxiv.org/abs/2410.16144)\n- 10/17/2024 bitnet.cpp 1.0 released.\n- 03/21/2024 [The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf)\n- 02/27/2024 [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)\n- 10/17/2023 [BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/abs/2310.11453)\n\n## Acknowledgements\n\nThis project is based on the [llama.cpp](https://github.com/ggerganov/llama.cpp) framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp's kernels are built on top of the Lookup Table methodologies pioneered in [T-MAC](https://github.com/microsoft/T-MAC/). For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.\n\n## Supported Models\n❗️**We use existing 1-bit LLMs available on [Hugging Face](https://huggingface.co/) to demonstrate the inference capabilities of bitnet.cpp. These models are neither trained nor released by Microsoft. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.**\n\n<table>\n    </tr>\n    <tr>\n        <th rowspan=\"2\">Model</th>\n        <th rowspan=\"2\">Parameters</th>\n        <th rowspan=\"2\">CPU</th>\n        <th colspan=\"3\">Kernel</th>\n    </tr>\n    <tr>\n        <th>I2_S</th>\n        <th>TL1</th>\n        <th>TL2</th>\n    </tr>\n    <tr>\n        <td rowspan=\"2\"><a href=\"https://huggingface.co/1bitLLM/bitnet_b1_58-large\">bitnet_b1_58-large</a></td>\n        <td rowspan=\"2\">0.7B</td>\n        <td>x86</td>\n        <td>&#9989;</td>\n        <td>&#10060;</td>\n        <td>&#9989;</td>\n    </tr>\n    <tr>\n        <td>ARM</td>\n        <td>&#9989;</td>\n        <td>&#9989;</td>\n        <td>&#10060;</td>\n    </tr>\n    <tr>\n        <td rowspan=\"2\"><a href=\"https://huggingface.co/1bitLLM/bitnet_b1_58-3B\">bitnet_b1_58-3B</a></td>\n        <td rowspan=\"2\">3.3B</td>\n        <td>x86</td>\n        <td>&#10060;</td>\n        <td>&#10060;</td>\n        <td>&#9989;</td>\n    </tr>\n    <tr>\n        <td>ARM</td>\n        <td>&#10060;</td>\n        <td>&#9989;</td>\n        <td>&#10060;</td>\n    </tr>\n    <tr>\n        <td rowspan=\"2\"><a href=\"https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens\">Llama3-8B-1.58-100B-tokens</a></td>\n        <td rowspan=\"2\">8.0B</td>\n        <td>x86</td>\n        <td>&#9989;</td>\n        <td>&#10060;</td>\n        <td>&#9989;</td>\n    </tr>\n    <tr>\n        <td>ARM</td>\n        <td>&#9989;</td>\n        <td>&#9989;</td>\n        <td>&#10060;</td>\n    </tr>\n    <tr>\n        <td rowspan=\"2\"><a href=\"https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026\">Falcon3 Family</a></td>\n        <td rowspan=\"2\">1B-10B</td>\n        <td>x86</td>\n        <td>&#9989;</td>\n        <td>&#10060;</td>\n        <td>&#9989;</td>\n    </tr>\n    <tr>\n        <td>ARM</td>\n        <td>&#9989;</td>\n        <td>&#9989;</td>\n        <td>&#10060;</td>\n    </tr>\n</table>\n\n\n\n## Installation\n\n### Requirements\n- python>=3.9\n- cmake>=3.22\n- clang>=18\n    - For Windows users, install [Visual Studio 2022](https://visualstudio.microsoft.com/downloads/). In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):\n        -  Desktop-development with C++\n        -  C++-CMake Tools for Windows\n        -  Git for Windows\n        -  C++-Clang Compiler for Windows\n        -  MS-Build Support for LLVM-Toolset (clang)\n    - For Debian/Ubuntu users, you can download with [Automatic installation script](https://apt.llvm.org/)\n\n        `bash -c \"$(wget -O - https://apt.llvm.org/llvm.sh)\"`\n- conda (highly recommend)\n\n### Build from source\n\n> [!IMPORTANT]\n> If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands\n\n1. Clone the repo\n```bash\ngit clone --recursive https://github.com/microsoft/BitNet.git\ncd BitNet\n```\n2. Install the dependencies\n```bash\n# (Recommended) Create a new conda environment\nconda create -n bitnet-cpp python=3.9\nconda activate bitnet-cpp\n\npip install -r requirements.txt\n```\n3. Build the project\n```bash\n# Download the model from Hugging Face, convert it to quantized gguf format, and build the project\npython setup_env.py --hf-repo tiiuae/Falcon3-7B-Instruct-1.58bit -q i2_s\n\n# Or you can manually download the model and run with local path\nhuggingface-cli download tiiuae/Falcon3-7B-Instruct-1.58bit --local-dir models/Falcon3-7B-Instruct-1.58bit\npython setup_env.py -md models/Falcon3-7B-Instruct-1.58bit -q i2_s\n```\n<pre>\nusage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]\n                    [--use-pretuned]\n\nSetup the environment for running inference\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}\n                        Model used for inference\n  --model-dir MODEL_DIR, -md MODEL_DIR\n                        Directory to save/load the model\n  --log-dir LOG_DIR, -ld LOG_DIR\n                        Directory to save the logging info\n  --quant-type {i2_s,tl1}, -q {i2_s,tl1}\n                        Quantization type\n  --quant-embd          Quantize the embeddings to f16\n  --use-pretuned, -p    Use the pretuned kernel parameters\n</pre>\n## Usage\n### Basic usage\n```bash\n# Run inference with the quantized model\npython run_inference.py -m models/Falcon3-7B-Instruct-1.58bit/ggml-model-i2_s.gguf -p \"You are a helpful assistant\" -cnv\n```\n<pre>\nusage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]\n\nRun inference\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -m MODEL, --model MODEL\n                        Path to model file\n  -n N_PREDICT, --n-predict N_PREDICT\n                        Number of tokens to predict when generating text\n  -p PROMPT, --prompt PROMPT\n                        Prompt to generate text from\n  -t THREADS, --threads THREADS\n                        Number of threads to use\n  -c CTX_SIZE, --ctx-size CTX_SIZE\n                        Size of the prompt context\n  -temp TEMPERATURE, --temperature TEMPERATURE\n                        Temperature, a hyperparameter that controls the randomness of the generated text\n  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)\n                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)\n</pre>\n\n### Benchmark\nWe provide scripts to run the inference benchmark providing a model.\n\n```  \nusage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  \n   \nSetup the environment for running the inference  \n   \nrequired arguments:  \n  -m MODEL, --model MODEL  \n                        Path to the model file. \n   \noptional arguments:  \n  -h, --help  \n                        Show this help message and exit. \n  -n N_TOKEN, --n-token N_TOKEN  \n                        Number of generated tokens. \n  -p N_PROMPT, --n-prompt N_PROMPT  \n                        Prompt to generate text from. \n  -t THREADS, --threads THREADS  \n                        Number of threads to use. \n```  \n   \nHere's a brief explanation of each argument:  \n   \n- `-m`, `--model`: The path to the model file. This is a required argument that must be provided when running the script.  \n- `-n`, `--n-token`: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.  \n- `-p`, `--n-prompt`: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.  \n- `-t`, `--threads`: The number of threads to use for running the inference. It is an optional argument with a default value of 2.  \n- `-h`, `--help`: Show the help message and exit. Use this argument to display usage information.  \n   \nFor example:  \n   \n```sh  \npython utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  \n```  \n   \nThis command would run the inference benchmark using the model located at `/path/to/model`, generating 200 tokens from a 256 token prompt, utilizing 4 threads.  \n\nFor the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:\n\n```bash\npython utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M\n\n# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate\npython utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128\n```\n\n\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 2.59375,
          "content": "<!-- BEGIN MICROSOFT SECURITY.MD V0.0.9 BLOCK -->\n\n## Security\n\nMicrosoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet) and [Xamarin](https://github.com/xamarin).\n\nIf you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://aka.ms/security.md/definition), please report it to us as described below.\n\n## Reporting Security Issues\n\n**Please do not report security vulnerabilities through public GitHub issues.**\n\nInstead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://aka.ms/security.md/msrc/create-report).\n\nIf you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://aka.ms/security.md/msrc/pgp).\n\nYou should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc). \n\nPlease include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:\n\n  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)\n  * Full paths of source file(s) related to the manifestation of the issue\n  * The location of the affected source code (tag/branch/commit or direct URL)\n  * Any special configuration required to reproduce the issue\n  * Step-by-step instructions to reproduce the issue\n  * Proof-of-concept or exploit code (if possible)\n  * Impact of the issue, including how an attacker might exploit the issue\n\nThis information will help us triage your report more quickly.\n\nIf you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://aka.ms/security.md/msrc/bounty) page for more details about our active programs.\n\n## Preferred Languages\n\nWe prefer all communications to be in English.\n\n## Policy\n\nMicrosoft follows the principle of [Coordinated Vulnerability Disclosure](https://aka.ms/security.md/cvd).\n\n<!-- END MICROSOFT SECURITY.MD BLOCK -->\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "include",
          "type": "tree",
          "content": null
        },
        {
          "name": "media",
          "type": "tree",
          "content": null
        },
        {
          "name": "preset_kernels",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.57421875,
          "content": "# These requirements include all dependencies for all top-level python scripts\n# for llama.cpp. Avoid adding packages here directly.\n#\n# Package versions must stay compatible across all top-level python scripts.\n#\n\n-r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt\n-r 3rdparty/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt\n-r 3rdparty/llama.cpp/requirements/requirements-convert_hf_to_gguf_update.txt\n-r 3rdparty/llama.cpp/requirements/requirements-convert_llama_ggml_to_gguf.txt\n-r 3rdparty/llama.cpp/requirements/requirements-convert_lora_to_gguf.txt"
        },
        {
          "name": "run_inference.py",
          "type": "blob",
          "size": 2.4189453125,
          "content": "import os\nimport sys\nimport signal\nimport platform\nimport argparse\nimport subprocess\n\ndef run_command(command, shell=False):\n    \"\"\"Run a system command and ensure it succeeds.\"\"\"\n    try:\n        subprocess.run(command, shell=shell, check=True)\n    except subprocess.CalledProcessError as e:\n        print(f\"Error occurred while running command: {e}\")\n        sys.exit(1)\n\ndef run_inference():\n    build_dir = \"build\"\n    if platform.system() == \"Windows\":\n        main_path = os.path.join(build_dir, \"bin\", \"Release\", \"llama-cli.exe\")\n        if not os.path.exists(main_path):\n            main_path = os.path.join(build_dir, \"bin\", \"llama-cli\")\n    else:\n        main_path = os.path.join(build_dir, \"bin\", \"llama-cli\")\n    command = [\n        f'{main_path}',\n        '-m', args.model,\n        '-n', str(args.n_predict),\n        '-t', str(args.threads),\n        '-p', args.prompt,\n        '-ngl', '0',\n        '-c', str(args.ctx_size),\n        '--temp', str(args.temperature),\n        \"-b\", \"1\",\n    ]\n    if args.conversation:\n        command.append(\"-cnv\")\n    run_command(command)\n\ndef signal_handler(sig, frame):\n    print(\"Ctrl+C pressed, exiting...\")\n    sys.exit(0)\n\nif __name__ == \"__main__\":\n    signal.signal(signal.SIGINT, signal_handler)\n    # Usage: python run_inference.py -p \"Microsoft Corporation is an American multinational corporation and technology company headquartered in Redmond, Washington.\"\n    parser = argparse.ArgumentParser(description='Run inference')\n    parser.add_argument(\"-m\", \"--model\", type=str, help=\"Path to model file\", required=False, default=\"models/bitnet_b1_58-3B/ggml-model-i2_s.gguf\")\n    parser.add_argument(\"-n\", \"--n-predict\", type=int, help=\"Number of tokens to predict when generating text\", required=False, default=128)\n    parser.add_argument(\"-p\", \"--prompt\", type=str, help=\"Prompt to generate text from\", required=True)\n    parser.add_argument(\"-t\", \"--threads\", type=int, help=\"Number of threads to use\", required=False, default=2)\n    parser.add_argument(\"-c\", \"--ctx-size\", type=int, help=\"Size of the prompt context\", required=False, default=2048)\n    parser.add_argument(\"-temp\", \"--temperature\", type=float, help=\"Temperature, a hyperparameter that controls the randomness of the generated text\", required=False, default=0.8)\n    parser.add_argument(\"-cnv\", \"--conversation\", action='store_true', help=\"Whether to enable chat mode or not (for instruct models.)\")\n\n    args = parser.parse_args()\n    run_inference()"
        },
        {
          "name": "setup_env.py",
          "type": "blob",
          "size": 10.169921875,
          "content": "import subprocess\nimport signal\nimport sys\nimport os\nimport platform\nimport argparse\nimport logging\nimport shutil\nfrom pathlib import Path\n\nlogger = logging.getLogger(\"setup_env\")\n\nSUPPORTED_HF_MODELS = {\n    \"1bitLLM/bitnet_b1_58-large\": {\n        \"model_name\": \"bitnet_b1_58-large\",\n    },\n    \"1bitLLM/bitnet_b1_58-3B\": {\n        \"model_name\": \"bitnet_b1_58-3B\",\n    },\n    \"HF1BitLLM/Llama3-8B-1.58-100B-tokens\": {\n        \"model_name\": \"Llama3-8B-1.58-100B-tokens\",\n    },\n    \"tiiuae/Falcon3-7B-Instruct-1.58bit\": {\n        \"model_name\": \"Falcon3-7B-Instruct-1.58bit\",\n    },\n    \"tiiuae/Falcon3-7B-1.58bit\": {\n        \"model_name\": \"Falcon3-7B-1.58bit\",\n    },\n    \"tiiuae/Falcon3-10B-Instruct-1.58bit\": {\n        \"model_name\": \"Falcon3-10B-Instruct-1.58bit\",\n    },\n    \"tiiuae/Falcon3-10B-1.58bit\": {\n        \"model_name\": \"Falcon3-10B-1.58bit\",\n    },\n    \"tiiuae/Falcon3-3B-Instruct-1.58bit\": {\n        \"model_name\": \"Falcon3-3B-Instruct-1.58bit\",\n    },\n    \"tiiuae/Falcon3-3B-1.58bit\": {\n        \"model_name\": \"Falcon3-3B-1.58bit\",\n    },\n    \"tiiuae/Falcon3-1B-Instruct-1.58bit\": {\n        \"model_name\": \"Falcon3-1B-Instruct-1.58bit\",\n    },\n}\n\nSUPPORTED_QUANT_TYPES = {\n    \"arm64\": [\"i2_s\", \"tl1\"],\n    \"x86_64\": [\"i2_s\", \"tl2\"]\n}\n\nCOMPILER_EXTRA_ARGS = {\n    \"arm64\": [\"-DBITNET_ARM_TL1=ON\"],\n    \"x86_64\": [\"-DBITNET_X86_TL2=ON\"]\n}\n\nOS_EXTRA_ARGS = {\n    \"Windows\":[\"-T\", \"ClangCL\"],\n}\n\nARCH_ALIAS = {\n    \"AMD64\": \"x86_64\",\n    \"x86\": \"x86_64\",\n    \"x86_64\": \"x86_64\",\n    \"aarch64\": \"arm64\",\n    \"arm64\": \"arm64\",\n    \"ARM64\": \"arm64\",\n}\n\ndef system_info():\n    return platform.system(), ARCH_ALIAS[platform.machine()]\n\ndef get_model_name():\n    if args.hf_repo:\n        return SUPPORTED_HF_MODELS[args.hf_repo][\"model_name\"]\n    return os.path.basename(os.path.normpath(args.model_dir))\n\ndef run_command(command, shell=False, log_step=None):\n    \"\"\"Run a system command and ensure it succeeds.\"\"\"\n    if log_step:\n        log_file = os.path.join(args.log_dir, log_step + \".log\")\n        with open(log_file, \"w\") as f:\n            try:\n                subprocess.run(command, shell=shell, check=True, stdout=f, stderr=f)\n            except subprocess.CalledProcessError as e:\n                logging.error(f\"Error occurred while running command: {e}, check details in {log_file}\")\n                sys.exit(1)\n    else:\n        try:\n            subprocess.run(command, shell=shell, check=True)\n        except subprocess.CalledProcessError as e:\n            logging.error(f\"Error occurred while running command: {e}\")\n        sys.exit(1)\n\ndef prepare_model():\n    _, arch = system_info()\n    hf_url = args.hf_repo\n    model_dir = args.model_dir\n    quant_type = args.quant_type\n    quant_embd = args.quant_embd\n    if hf_url is not None:\n        # download the model\n        model_dir = os.path.join(model_dir, SUPPORTED_HF_MODELS[hf_url][\"model_name\"])\n        Path(model_dir).mkdir(parents=True, exist_ok=True)\n        logging.info(f\"Downloading model {hf_url} from HuggingFace to {model_dir}...\")\n        run_command([\"huggingface-cli\", \"download\", hf_url, \"--local-dir\", model_dir], log_step=\"download_model\")\n    elif not os.path.exists(model_dir):\n        logging.error(f\"Model directory {model_dir} does not exist.\")\n        sys.exit(1)\n    else:\n        logging.info(f\"Loading model from directory {model_dir}.\")\n    gguf_path = os.path.join(model_dir, \"ggml-model-\" + quant_type + \".gguf\")\n    if not os.path.exists(gguf_path) or os.path.getsize(gguf_path) == 0:\n        logging.info(f\"Converting HF model to GGUF format...\")\n        if quant_type.startswith(\"tl\"):\n            run_command([sys.executable, \"utils/convert-hf-to-gguf-bitnet.py\", model_dir, \"--outtype\", quant_type, \"--quant-embd\"], log_step=\"convert_to_tl\")\n        else: # i2s\n            # convert to f32\n            run_command([sys.executable, \"utils/convert-hf-to-gguf-bitnet.py\", model_dir, \"--outtype\", \"f32\"], log_step=\"convert_to_f32_gguf\")\n            f32_model = os.path.join(model_dir, \"ggml-model-f32.gguf\")\n            i2s_model = os.path.join(model_dir, \"ggml-model-i2_s.gguf\")\n            # quantize to i2s\n            if platform.system() != \"Windows\":\n                if quant_embd:\n                    run_command([\"./build/bin/llama-quantize\", \"--token-embedding-type\", \"f16\", f32_model, i2s_model, \"I2_S\", \"1\", \"1\"], log_step=\"quantize_to_i2s\")\n                else:\n                    run_command([\"./build/bin/llama-quantize\", f32_model, i2s_model, \"I2_S\", \"1\"], log_step=\"quantize_to_i2s\")\n            else:\n                if quant_embd:\n                    run_command([\"./build/bin/Release/llama-quantize\", \"--token-embedding-type\", \"f16\", f32_model, i2s_model, \"I2_S\", \"1\", \"1\"], log_step=\"quantize_to_i2s\")\n                else:\n                    run_command([\"./build/bin/Release/llama-quantize\", f32_model, i2s_model, \"I2_S\", \"1\"], log_step=\"quantize_to_i2s\")\n\n        logging.info(f\"GGUF model saved at {gguf_path}\")\n    else:\n        logging.info(f\"GGUF model already exists at {gguf_path}\")\n\ndef setup_gguf():\n    # Install the pip package\n    run_command([sys.executable, \"-m\", \"pip\", \"install\", \"3rdparty/llama.cpp/gguf-py\"], log_step=\"install_gguf\")\n\ndef gen_code():\n    _, arch = system_info()\n    \n    llama3_f3_models = set([model['model_name'] for model in SUPPORTED_HF_MODELS.values() if model['model_name'].startswith(\"Falcon3\") or model['model_name'].startswith(\"Llama\")])\n\n    if arch == \"arm64\":\n        if args.use_pretuned:\n            pretuned_kernels = os.path.join(\"preset_kernels\", get_model_name())\n            if not os.path.exists(pretuned_kernels):\n                logging.error(f\"Pretuned kernels not found for model {args.hf_repo}\")\n                sys.exit(1)\n            if args.quant_type == \"tl1\":\n                shutil.copyfile(os.path.join(pretuned_kernels, \"bitnet-lut-kernels-tl1.h\"), \"include/bitnet-lut-kernels.h\")\n                shutil.copyfile(os.path.join(pretuned_kernels, \"kernel_config_tl1.ini\"), \"include/kernel_config.ini\")\n            elif args.quant_type == \"tl2\":\n                shutil.copyfile(os.path.join(pretuned_kernels, \"bitnet-lut-kernels-tl2.h\"), \"include/bitnet-lut-kernels.h\")\n                shutil.copyfile(os.path.join(pretuned_kernels, \"kernel_config_tl2.ini\"), \"include/kernel_config.ini\")\n        if get_model_name() == \"bitnet_b1_58-large\":\n            run_command([sys.executable, \"utils/codegen_tl1.py\", \"--model\", \"bitnet_b1_58-large\", \"--BM\", \"256,128,256\", \"--BK\", \"128,64,128\", \"--bm\", \"32,64,32\"], log_step=\"codegen\")\n        elif get_model_name() in llama3_f3_models:\n            run_command([sys.executable, \"utils/codegen_tl1.py\", \"--model\", \"Llama3-8B-1.58-100B-tokens\", \"--BM\", \"256,128,256,128\", \"--BK\", \"128,64,128,64\", \"--bm\", \"32,64,32,64\"], log_step=\"codegen\")\n        elif get_model_name() == \"bitnet_b1_58-3B\":\n            run_command([sys.executable, \"utils/codegen_tl1.py\", \"--model\", \"bitnet_b1_58-3B\", \"--BM\", \"160,320,320\", \"--BK\", \"64,128,64\", \"--bm\", \"32,64,32\"], log_step=\"codegen\")\n        else:\n            raise NotImplementedError()\n    else:\n        if args.use_pretuned:\n            # cp preset_kernels/model_name/bitnet-lut-kernels_tl1.h to include/bitnet-lut-kernels.h\n            pretuned_kernels = os.path.join(\"preset_kernels\", get_model_name())\n            if not os.path.exists(pretuned_kernels):\n                logging.error(f\"Pretuned kernels not found for model {args.hf_repo}\")\n                sys.exit(1)\n            shutil.copyfile(os.path.join(pretuned_kernels, \"bitnet-lut-kernels-tl2.h\"), \"include/bitnet-lut-kernels.h\")\n        if get_model_name() == \"bitnet_b1_58-large\":\n            run_command([sys.executable, \"utils/codegen_tl2.py\", \"--model\", \"bitnet_b1_58-large\", \"--BM\", \"256,128,256\", \"--BK\", \"96,192,96\", \"--bm\", \"32,32,32\"], log_step=\"codegen\")\n        elif get_model_name() in llama3_f3_models:\n            run_command([sys.executable, \"utils/codegen_tl2.py\", \"--model\", \"Llama3-8B-1.58-100B-tokens\", \"--BM\", \"256,128,256,128\", \"--BK\", \"96,96,96,96\", \"--bm\", \"32,32,32,32\"], log_step=\"codegen\")\n        elif get_model_name() == \"bitnet_b1_58-3B\":\n            run_command([sys.executable, \"utils/codegen_tl2.py\", \"--model\", \"bitnet_b1_58-3B\", \"--BM\", \"160,320,320\", \"--BK\", \"96,96,96\", \"--bm\", \"32,32,32\"], log_step=\"codegen\")\n        else:\n            raise NotImplementedError()\n\n\ndef compile():\n    # Check if cmake is installed\n    cmake_exists = subprocess.run([\"cmake\", \"--version\"], capture_output=True)\n    if cmake_exists.returncode != 0:\n        logging.error(\"Cmake is not available. Please install CMake and try again.\")\n        sys.exit(1)\n    _, arch = system_info()\n    if arch not in COMPILER_EXTRA_ARGS.keys():\n        logging.error(f\"Arch {arch} is not supported yet\")\n        exit(0)\n    logging.info(\"Compiling the code using CMake.\")\n    run_command([\"cmake\", \"-B\", \"build\", *COMPILER_EXTRA_ARGS[arch], *OS_EXTRA_ARGS.get(platform.system(), [])], log_step=\"generate_build_files\")\n    # run_command([\"cmake\", \"--build\", \"build\", \"--target\", \"llama-cli\", \"--config\", \"Release\"])\n    run_command([\"cmake\", \"--build\", \"build\", \"--config\", \"Release\"], log_step=\"compile\")\n\ndef main():\n    setup_gguf()\n    gen_code()\n    compile()\n    prepare_model()\n    \ndef parse_args():\n    _, arch = system_info()\n    parser = argparse.ArgumentParser(description='Setup the environment for running the inference')\n    parser.add_argument(\"--hf-repo\", \"-hr\", type=str, help=\"Model used for inference\", choices=SUPPORTED_HF_MODELS.keys())\n    parser.add_argument(\"--model-dir\", \"-md\", type=str, help=\"Directory to save/load the model\", default=\"models\")\n    parser.add_argument(\"--log-dir\", \"-ld\", type=str, help=\"Directory to save the logging info\", default=\"logs\")\n    parser.add_argument(\"--quant-type\", \"-q\", type=str, help=\"Quantization type\", choices=SUPPORTED_QUANT_TYPES[arch], default=\"i2_s\")\n    parser.add_argument(\"--quant-embd\", action=\"store_true\", help=\"Quantize the embeddings to f16\")\n    parser.add_argument(\"--use-pretuned\", \"-p\", action=\"store_true\", help=\"Use the pretuned kernel parameters\")\n    return parser.parse_args()\n\ndef signal_handler(sig, frame):\n    logging.info(\"Ctrl+C pressed, exiting...\")\n    sys.exit(0)\n\nif __name__ == \"__main__\":\n    signal.signal(signal.SIGINT, signal_handler)\n    args = parse_args()\n    Path(args.log_dir).mkdir(parents=True, exist_ok=True)\n    logging.basicConfig(level=logging.INFO)\n    main()\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}