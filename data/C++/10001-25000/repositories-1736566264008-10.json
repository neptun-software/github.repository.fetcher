{
  "metadata": {
    "timestamp": 1736566264008,
    "page": 10,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "PaddlePaddle/Paddle",
      "stars": 22396,
      "defaultBranch": "develop",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 0.8720703125,
          "content": "# This file is used by clang-format to autoformat paddle source code\n#\n# The clang-format is part of llvm toolchain.\n# It need to install llvm and clang to format source code style.\n#\n# The basic usage is,\n#   clang-format -i -style=file PATH/TO/SOURCE/CODE\n#\n# The -style=file implicit use \".clang-format\" file located in one of\n# parent directory.\n# The -i means inplace change.\n#\n# The document of clang-format is\n#   http://clang.llvm.org/docs/ClangFormat.html\n#   http://clang.llvm.org/docs/ClangFormatStyleOptions.html\n---\nLanguage:        Cpp\nBasedOnStyle:  Google\nIndentWidth:     2\nTabWidth:        2\nContinuationIndentWidth: 4\nAccessModifierOffset: -1  # The private/protected/public has no indent in class\nStandard:  Cpp11\nAllowAllParametersOfDeclarationOnNextLine: true\nBinPackParameters: false\nBinPackArguments: false\nIncludeBlocks: Preserve\nIncludeIsMainSourceRegex: (\\.cu)$\n...\n"
        },
        {
          "name": ".clang-tidy",
          "type": "blob",
          "size": 7.6552734375,
          "content": "---\nChecks: '\nbugprone-argument-comment,\n-bugprone-assert-side-effect,\n-bugprone-bad-signal-to-kill-thread,\n-bugprone-bool-pointer-implicit-conversion,\nbugprone-branch-clone,\nbugprone-copy-constructor-init,\n-bugprone-dangling-handle,\n-bugprone-dynamic-static-initializers,\nbugprone-exception-escape,\nbugprone-fold-init-type,\n-bugprone-forwarding-reference-overload,\nbugprone-inaccurate-erase,\nbugprone-incorrect-roundings,\nbugprone-infinite-loop,\nbugprone-integer-division,\n-bugprone-macro-repeated-side-effects,\n-bugprone-misplaced-operator-in-strlen-in-alloc,\nbugprone-misplaced-widening-cast,\n-bugprone-move-forwarding-reference,\n-bugprone-multiple-statement-macro,\nbugprone-narrowing-conversions,\n-bugprone-not-null-terminated-result,\n-bugprone-parent-virtual-call,\n-bugprone-posix-return,\nbugprone-signed-char-misuse,\n-bugprone-sizeof-container,\n-bugprone-sizeof-expression,\n-bugprone-string-constructor,\nbugprone-string-integer-assignment,\n-bugprone-string-literal-with-embedded-nul,\n-bugprone-suspicious-enum-usage,\n-bugprone-suspicious-memset-usage,\nbugprone-suspicious-missing-comma,\n-bugprone-suspicious-semicolon,\n-bugprone-suspicious-string-compare,\n-bugprone-terminating-continue,\n-bugprone-throw-keyword-missing,\n-bugprone-too-small-loop-variable,\n-bugprone-undefined-memory-manipulation,\n-bugprone-undelegated-constructor,\nbugprone-unhandled-self-assignment,\nbugprone-unused-raii,\nbugprone-unused-return-value,\nbugprone-use-after-move,\n-bugprone-virtual-near-miss,\n-clang-analyzer-apiModeling.StdCLibraryFunctions,\n-clang-analyzer-apiModeling.TrustNonnull,\n-clang-analyzer-apiModeling.google.GTest,\n-clang-analyzer-apiModeling.llvm.CastValue,\n-clang-analyzer-apiModeling.llvm.ReturnValue,\nclang-analyzer-core.CallAndMessage,\n-clang-analyzer-core.DivideZero,\n-clang-analyzer-core.DynamicTypePropagation,\nclang-analyzer-core.NonNullParamChecker,\n-clang-analyzer-core.NonnilStringConstants,\n-clang-analyzer-core.NullDereference,\n-clang-analyzer-core.StackAddrEscapeBase,\n-clang-analyzer-core.StackAddressEscape,\nclang-analyzer-core.UndefinedBinaryOperatorResult,\n-clang-analyzer-core.VLASize,\n-clang-analyzer-core.builtin.BuiltinFunctions,\n-clang-analyzer-core.builtin.NoReturnFunctions,\n-clang-analyzer-core.uninitialized.ArraySubscript,\nclang-analyzer-core.uninitialized.Assign,\n-clang-analyzer-core.uninitialized.Branch,\n-clang-analyzer-core.uninitialized.CapturedBlockVariable,\n-clang-analyzer-core.uninitialized.UndefReturn,\nclang-analyzer-cplusplus.InnerPointer,\n-clang-analyzer-cplusplus.Move,\n-clang-analyzer-cplusplus.NewDelete,\nclang-analyzer-cplusplus.NewDeleteLeaks,\n-clang-analyzer-cplusplus.PureVirtualCall,\n-clang-analyzer-cplusplus.SelfAssignment,\n-clang-analyzer-cplusplus.SmartPtr,\n-clang-analyzer-cplusplus.VirtualCallModeling,\nclang-analyzer-deadcode.DeadStores,\n-clang-analyzer-fuchsia.HandleChecker,\n-clang-analyzer-nullability.NullPassedToNonnull,\n-clang-analyzer-nullability.NullReturnedFromNonnull,\n-clang-analyzer-nullability.NullabilityBase,\n-clang-analyzer-nullability.NullableDereferenced,\n-clang-analyzer-nullability.NullablePassedToNonnull,\n-clang-analyzer-nullability.NullableReturnedFromNonnull,\nclang-analyzer-optin.cplusplus.UninitializedObject,\n-clang-analyzer-optin.cplusplus.VirtualCall,\n-clang-analyzer-optin.mpi.MPI-Checker,\n-clang-analyzer-optin.osx.OSObjectCStyleCast,\n-clang-analyzer-optin.osx.cocoa.localizability.EmptyLocalizationContextChecker,\n-clang-analyzer-optin.osx.cocoa.localizability.NonLocalizedStringChecker,\n-clang-analyzer-optin.performance.GCDAntipattern,\n-clang-analyzer-optin.performance.Padding,\nclang-analyzer-optin.portability.UnixAPI,\n-clang-analyzer-osx.API,\n-clang-analyzer-osx.MIG,\n-clang-analyzer-osx.NSOrCFErrorDerefChecker,\n-clang-analyzer-osx.NumberObjectConversion,\n-clang-analyzer-osx.OSObjectRetainCount,\n-clang-analyzer-osx.ObjCProperty,\n-clang-analyzer-osx.SecKeychainAPI,\n-clang-analyzer-osx.cocoa.AtSync,\n-clang-analyzer-osx.cocoa.AutoreleaseWrite,\n-clang-analyzer-osx.cocoa.ClassRelease,\n-clang-analyzer-osx.cocoa.Dealloc,\n-clang-analyzer-osx.cocoa.IncompatibleMethodTypes,\n-clang-analyzer-osx.cocoa.Loops,\n-clang-analyzer-osx.cocoa.MissingSuperCall,\n-clang-analyzer-osx.cocoa.NSAutoreleasePool,\n-clang-analyzer-osx.cocoa.NSError,\n-clang-analyzer-osx.cocoa.NilArg,\n-clang-analyzer-osx.cocoa.NonNilReturnValue,\n-clang-analyzer-osx.cocoa.ObjCGenerics,\n-clang-analyzer-osx.cocoa.RetainCount,\n-clang-analyzer-osx.cocoa.RetainCountBase,\n-clang-analyzer-osx.cocoa.RunLoopAutoreleaseLeak,\n-clang-analyzer-osx.cocoa.SelfInit,\n-clang-analyzer-osx.cocoa.SuperDealloc,\n-clang-analyzer-osx.cocoa.UnusedIvars,\n-clang-analyzer-osx.cocoa.VariadicMethodTypes,\n-clang-analyzer-osx.coreFoundation.CFError,\n-clang-analyzer-osx.coreFoundation.CFNumber,\n-clang-analyzer-osx.coreFoundation.CFRetainRelease,\n-clang-analyzer-osx.coreFoundation.containers.OutOfBounds,\n-clang-analyzer-osx.coreFoundation.containers.PointerSizedValues,\nclang-analyzer-security.FloatLoopCounter,\n-clang-analyzer-security.insecureAPI.DeprecatedOrUnsafeBufferHandling,\n-clang-analyzer-security.insecureAPI.SecuritySyntaxChecker,\n-clang-analyzer-security.insecureAPI.UncheckedReturn,\n-clang-analyzer-security.insecureAPI.bcmp,\n-clang-analyzer-security.insecureAPI.bcopy,\n-clang-analyzer-security.insecureAPI.bzero,\n-clang-analyzer-security.insecureAPI.decodeValueOfObjCType,\n-clang-analyzer-security.insecureAPI.getpw,\n-clang-analyzer-security.insecureAPI.gets,\n-clang-analyzer-security.insecureAPI.mkstemp,\n-clang-analyzer-security.insecureAPI.mktemp,\n-clang-analyzer-security.insecureAPI.rand,\n-clang-analyzer-security.insecureAPI.strcpy,\nclang-analyzer-security.insecureAPI.vfork,\n-clang-analyzer-unix.API,\n-clang-analyzer-unix.DynamicMemoryModeling,\nclang-analyzer-unix.Malloc,\n-clang-analyzer-unix.MallocSizeof,\n-clang-analyzer-unix.MismatchedDeallocator,\nclang-analyzer-unix.Vfork,\n-clang-analyzer-unix.cstring.BadSizeArg,\n-clang-analyzer-unix.cstring.CStringModeling,\n-clang-analyzer-unix.cstring.NullArg,\n-clang-analyzer-valist.CopyToSelf,\n-clang-analyzer-valist.Uninitialized,\n-clang-analyzer-valist.Unterminated,\n-clang-analyzer-valist.ValistBase,\ncppcoreguidelines-avoid-c-arrays,\n-cppcoreguidelines-avoid-goto,\ncppcoreguidelines-c-copy-assignment-signature,\ncppcoreguidelines-explicit-virtual-functions,\ncppcoreguidelines-init-variables,\ncppcoreguidelines-narrowing-conversions,\ncppcoreguidelines-no-malloc,\ncppcoreguidelines-pro-type-const-cast,\n-cppcoreguidelines-pro-type-member-init,\n-cppcoreguidelines-slicing,\n-hicpp-avoid-goto,\nhicpp-exception-baseclass,\nmisc-unused-alias-decls,\nmisc-unused-using-decls,\nmodernize-avoid-bind,\nmodernize-avoid-c-arrays,\nmodernize-deprecated-headers,\n-modernize-deprecated-ios-base-aliases,\nmodernize-loop-convert,\nmodernize-make-shared,\nmodernize-make-unique,\n-modernize-pass-by-value,\nmodernize-raw-string-literal,\nmodernize-redundant-void-arg,\n-modernize-replace-auto-ptr,\n-modernize-replace-random-shuffle,\n-modernize-shrink-to-fit,\n-modernize-unary-static-assert,\nmodernize-use-bool-literals,\nmodernize-use-emplace,\nmodernize-use-equals-default,\n-modernize-use-equals-delete,\n-modernize-use-noexcept,\nmodernize-use-nullptr,\nmodernize-use-override,\nmodernize-use-transparent-functors,\n-modernize-use-uncaught-exceptions,\nperformance-faster-string-find,\nperformance-for-range-copy,\n-performance-implicit-conversion-in-loop,\n-performance-inefficient-algorithm,\nperformance-inefficient-string-concatenation,\n-performance-inefficient-vector-operation,\nperformance-move-const-arg,\n-performance-move-constructor-init,\n-performance-no-automatic-move,\nperformance-noexcept-move-constructor,\nperformance-trivially-destructible,\n-performance-type-promotion-in-math-fn,\n-performance-unnecessary-copy-initialization,\nreadability-container-size-empty,\n'\nHeaderFilterRegex: '^(paddle/(?!cinn)).*$'\nAnalyzeTemporaryDtors: false\nWarningsAsErrors: '*'\n...\n"
        },
        {
          "name": ".cmake-format.py",
          "type": "blob",
          "size": 2.9619140625,
          "content": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# -----------------------------\n# Options affecting formatting.\n# -----------------------------\nwith section(\"format\"):\n    # How wide to allow formatted cmake files\n    line_width = 80\n\n# ------------------------------------------------\n# Options affecting comment reflow and formatting.\n# ------------------------------------------------\nwith section(\"markup\"):\n    # enable comment markup parsing and reflow\n    enable_markup = False\n\n    # If comment markup is enabled, don't reflow the first comment block in each\n    # listfile. Use this to preserve formatting of your copyright/license\n    # statements.\n    first_comment_is_literal = True\n\n# ----------------------------------\n# Options affecting listfile parsing\n# ----------------------------------\nwith section(\"parse\"):\n    # Additional FLAGS and KWARGS for custom commands\n    additional_commands = {\n        \"cc_library\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"nv_library\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"xpu_library\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"hip_library\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"go_library\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"copy\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DSTS\": '*',\n            }\n        },\n        \"cc_test\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"nv_test\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"hip_test\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"xpu_test\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"go_test\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"py_test\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n    }\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.126953125,
          "content": "*.DS_Store\nbuild/\n*.user\n.vscode\n.idea\n.project\n.cproject\n.pydevproject\nMakefile\n.test_env/\nthird_party/\n*~\nbazel-*\n\n!build/*.deb\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.470703125,
          "content": "# EditorConfig is a cross-editor configuration file\n# that helps to unify code styles for multiple\n# developers collaborative projects.\n# See more at https://editorconfig.org/\n\nroot = true\n\n[*]\nindent_style = space\nend_of_line = lf\ncharset = utf-8\ntrim_trailing_whitespace = true\ninsert_final_newline = true\n\n[*.{c,cc,cxx,cpp,cu,cuh,h,hpp,hxx,kps}]\nindent_size = 2\n\n[*.{py,pyi,java,r,toml}]\nindent_size = 4\n\n[Dockerfile.*]\nindent_size = 4\n\n[*.go]\nindent_style = tab\nindent_size = 4\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.4599609375,
          "content": "paddle/fluid/operators/distributed/send_recv.proto\npaddle/fluid/API.spec\npaddle/fluid/API_DEV.spec\npaddle/fluid/API_PR.spec\npaddle/fluid/eager/api/generated/*\npaddle/fluid/op_use_default_grad_maker_DEV.spec\npaddle/fluid/op_use_default_grad_maker_PR.spec\npaddle/fluid/operators/ops_extra_info.cc\npaddle/phi/api/backward/backward_api.h\npaddle/phi/api/backward/fused_backward_api.h\npaddle/phi/api/backward/sparse_bw_api.h\npaddle/phi/api/include/api.h\npaddle/phi/api/include/fused_api.h\npaddle/phi/api/include/operants_base.h\npaddle/phi/api/include/operants_manager.h\npaddle/phi/api/include/sparse_api.h\npaddle/phi/api/include/strings_api.h\npaddle/phi/api/include/tensor_operants.h\npaddle/phi/api/lib/api.cc\npaddle/phi/api/lib/fused_api.cc\npaddle/phi/api/lib/dygraph_api.*\npaddle/phi/api/lib/backward_api.cc\npaddle/phi/api/lib/fused_backward_api.cc\npaddle/phi/api/lib/operants_manager.cc\npaddle/phi/api/lib/sparse_api.cc\npaddle/phi/api/lib/strings_api.cc\npaddle/phi/api/lib/sparse_bw_api.cc\npaddle/phi/api/lib/tensor_api.cc\npaddle/phi/api/lib/tensor_operants.cc\npaddle/phi/extension.h\npaddle/phi/config.h\npaddle/phi/include/*\npaddle/phi/infermeta/generated.*\npaddle/fluid/prim/api/generated_prim/*.cc\npaddle/fluid/prim/api/generated_prim/*.h\npython/paddle/libs/bfloat16.h\npython/paddle/libs/cinn_cuda_runtime_source.cuh\npython/paddle/libs/float16.h\n*.DS_Store\n*.vs\nbuild/\ndist/\nbuild_doc/\n*.user\n*.tmp\n*.pyc\n*.log\n\n.vscode\n.idea\n.project\n.cproject\n.pydevproject\n.settings/\nCMakeSettings.json\nMakefile\n.test_env/\n.cache/\n.env\nthird_party/\n*~\nbazel-*\n\n\nbuild_*\n# clion workspace.\ncmake-build-*\npaddle/fluid/operators/distributed/send_recv.proto\nmodel_test\n\nTesting\ntools/__pycache__\ntools/nvcc_lazy\n\n# Ignore files generated from 'python setup.py develop'\n@PADDLE_BINARY_DIR@\n\n# This file is automatically generated.\n# TODO(zhiqiang) Move this file to build directory.\npaddle/fluid/pybind/eager_op_function.*\ntools/nvcc_lazy\npaddle/phi/kernels/sparse/gpu/cutlass_generator/all_gemm_operations.h\npaddle/phi/kernels/sparse/gpu/cutlass_generator/configurations.h\n\n#these files (directories) are generated before build system generation\npaddle/fluid/operators/generated_op*.cc\npaddle/fluid/operators/generated_sparse_op.cc\npaddle/fluid/operators/generated_static_op.cc\npaddle/fluid/operators/generated_fused_op.cc\npaddle/fluid/operators/ops_signature/generated_*.cc\npaddle/fluid/pybind/tmp_eager_op_function_impl.h\npaddle/fluid/pybind/eager_op_function_impl.h\npaddle/fluid/pybind/eager_op_function_impl.h\npaddle/fluid/pybind/op_function_impl.h\npaddle/fluid/pybind/*final_state_op_function_impl.h\npaddle/fluid/prim/api/generated/prim_api/*\npaddle/fluid/framework/__init__.py\npaddle/phi/api/profiler/__init__.py\npython/paddle/incubate/fleet/parameter_server/pslib/ps_pb2.py\npaddle/phi/kernels/fusion/cutlass/conv2d/generated/*\npaddle/phi/kernels/fusion/cutlass/conv2d/generated_tmp/*\npaddle/phi/ops/compat/generated_*\npython/paddle/base/incubate/fleet/parameter_server/pslib/ps_pb2.py\npaddle/fluid/ir_adaptor/translator/op_compat_info.cc\npaddle/phi/kernels/fusion/cutlass/cutlass_kernels/fpA_intB_gemm/autogen/*\npaddle/phi/kernels/fusion/cutlass/cutlass_kernels/fpA_intB_gemm/autogen_tmp/*\npaddle/fluid/pybind/static_op_function.*\npaddle/fluid/pybind/ops_api.cc\npython/paddle/tensor/tensor.pyi\npaddle/phi/kernels/fusion/cutlass/conv2d/build\npaddle/phi/kernels/fusion/cutlass/conv2d/cutlass\npaddle/phi/kernels/fusion/cutlass/gemm_epilogue/build\npaddle/phi/kernels/fusion/cutlass/gemm_epilogue/cutlass\npython/paddle/_typing/libs/**/*.pyi\nthird_party.tar.gz\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 4.0078125,
          "content": "[submodule \"third_party/protobuf\"]\n\tpath = third_party/protobuf\n\turl = https://github.com/protocolbuffers/protobuf.git\n\tignore = dirty\n[submodule \"third_party/pocketfft\"]\n\tpath = third_party/pocketfft\n\turl = https://gitlab.mpcdf.mpg.de/mtr/pocketfft.git\n\tignore = dirty\n[submodule \"third_party/gflags\"]\n\tpath = third_party/gflags\n\turl = https://github.com/gflags/gflags.git\n\tignore = dirty\n[submodule \"third_party/gloo\"]\n\tpath = third_party/gloo\n\turl = https://github.com/ziyoujiyi/gloo.git\n\tignore = dirty\n[submodule \"third_party/dlpack\"]\n\tpath = third_party/dlpack\n\turl = https://github.com/dmlc/dlpack.git\n\tignore = dirty\n[submodule \"third_party/utf8proc\"]\n\tpath = third_party/utf8proc\n\turl = https://github.com/JuliaStrings/utf8proc.git\n\tignore = dirty\n[submodule \"third_party/warpctc\"]\n\tpath = third_party/warpctc\n\turl = https://github.com/baidu-research/warp-ctc.git\n\tignore = dirty\n[submodule \"third_party/warprnnt\"]\n\tpath = third_party/warprnnt\n\turl = https://github.com/PaddlePaddle/warp-transducer.git\n\tignore = dirty\n[submodule \"third_party/xxhash\"]\n\tpath = third_party/xxhash\n\turl = https://github.com/Cyan4973/xxHash.git\n\tignore = dirty\n[submodule \"third_party/pybind\"]\n\tpath = third_party/pybind\n\turl = https://github.com/pybind/pybind11.git\n\tignore = dirty\n[submodule \"third_party/threadpool\"]\n\tpath = third_party/threadpool\n\turl = https://github.com/progschj/ThreadPool.git\n\tignore = dirty\n[submodule \"third_party/zlib\"]\n\tpath = third_party/zlib\n\turl = https://github.com/madler/zlib.git\n\tignore = dirty\n[submodule \"third_party/glog\"]\n\tpath = third_party/glog\n\turl = https://github.com/google/glog.git\n\tignore = dirty\n[submodule \"third_party/eigen3\"]\n\tpath = third_party/eigen3\n\turl = https://gitlab.com/libeigen/eigen.git\n\tignore = dirty\n[submodule \"third_party/snappy\"]\n\tpath = third_party/snappy\n\turl = https://github.com/google/snappy.git\n\tignore = dirty\n[submodule \"third_party/cub\"]\n\tpath = third_party/cub\n\turl = https://github.com/NVIDIA/cub.git\n\tignore = dirty\n[submodule \"third_party/cutlass\"]\n\tpath = third_party/cutlass\n\turl = https://github.com/NVIDIA/cutlass.git\n\tignore = dirty\n[submodule \"third_party/xbyak\"]\n\tpath = third_party/xbyak\n\turl = https://github.com/herumi/xbyak.git\n\tignore = dirty\n[submodule \"third_party/onednn\"]\n\tpath = third_party/onednn\n\turl = https://github.com/oneapi-src/oneDNN.git\n\tignore = dirty\n[submodule \"third_party/flashattn\"]\n\tpath = third_party/flashattn\n\turl = https://github.com/PaddlePaddle/flash-attention.git\n\tignore = dirty\n[submodule \"third_party/gtest\"]\n\tpath = third_party/gtest\n\turl = https://github.com/google/googletest.git\n\tignore = dirty\n[submodule \"third_party/openblas\"]\n\tpath = third_party/openblas\n\turl = https://github.com/xianyi/OpenBLAS.git\n\tignore = dirty\n[submodule \"third_party/leveldb\"]\n\tpath = third_party/leveldb\n\turl = https://github.com/google/leveldb.git\n\tignore = dirty\n[submodule \"third_party/brpc\"]\n\tpath = third_party/brpc\n\turl = https://github.com/apache/brpc.git\n\tignore = dirty\n[submodule \"third_party/rocksdb\"]\n\tpath = third_party/rocksdb\n\turl = https://github.com/Thunderbrook/rocksdb\n\tignore = dirty\n[submodule \"third_party/absl\"]\n\tpath = third_party/absl\n\turl = https://github.com/abseil/abseil-cpp.git\n\tignore = dirty\n[submodule \"third_party/jitify\"]\n\tpath = third_party/jitify\n\turl = https://github.com/NVIDIA/jitify.git\n\tignore = dirty\n[submodule \"third_party/cccl\"]\n\tpath = third_party/cccl\n\turl = https://github.com/NVIDIA/cccl.git\n\tignore = dirty\n[submodule \"third_party/cryptopp\"]\n\tpath = third_party/cryptopp\n\turl = https://github.com/weidai11/cryptopp.git\n\tignore = dirty\n[submodule \"third_party/cryptopp-cmake\"]\n\tpath = third_party/cryptopp-cmake\n\turl = https://github.com/noloader/cryptopp-cmake.git\n\tignore = dirty\n[submodule \"third_party/nlohmann_json\"]\n\tpath = third_party/nlohmann_json\n\turl = https://github.com/nlohmann/json.git\n\tignore = dirty\n[submodule \"third_party/yaml-cpp\"]\n\tpath = third_party/yaml-cpp\n\turl = https://github.com/jbeder/yaml-cpp\n[submodule \"third_party/openvino\"]\n\tpath = third_party/openvino\n\turl = https://github.com/openvinotoolkit/openvino.git\n\tignore = dirty\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 5.537109375,
          "content": "# Exclude all third-party libraries and auto-generated files globally\nexclude: |\n    (?x)^(\n        patches/.+|\n        paddle/fluid/framework/fleet/heter_ps/cudf/.+|\n        paddle/fluid/distributed/ps/thirdparty/round_robin.h|\n        python/paddle/utils/gast/.+|\n        third_party/.+\n    )$\nrepos:\n# Common hooks\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n    -   id: check-added-large-files\n    -   id: check-merge-conflict\n    -   id: check-symlinks\n    -   id: detect-private-key\n    -   id: end-of-file-fixer\n    -   id: sort-simple-yaml\n        files: (ops|backward|op_[a-z_]+)\\.yaml$\n    -   id: trailing-whitespace\n-   repo: https://github.com/Lucas-C/pre-commit-hooks.git\n    rev: v1.5.1\n    hooks:\n    -   id: remove-crlf\n    -   id: remove-tabs\n        name: Tabs remover (C++)\n        files: \\.(c|cc|cxx|cpp|cu|h|hpp|hxx|xpu|kps)$\n        args: [--whitespaces-count, '2']\n    -   id: remove-tabs\n        name: Tabs remover (Python)\n        files: (.*\\.(py|bzl)|BUILD|.*\\.BUILD|WORKSPACE)$\n        args: [--whitespaces-count, '4']\n        # Exclude some unit test files that require tabs.\n        exclude: |\n            (?x)^(\n                test/dygraph_to_static/test_error.py\n            )$\n-   repo: local\n    hooks:\n    -   id: copyright_checker\n        name: copyright_checker\n        entry: python ./tools/codestyle/copyright.py\n        language: system\n        files: \\.(c|cc|cxx|cpp|cu|h|hpp|hxx|proto|xpu|kps|py|pyi|sh)$\n        exclude: |\n            (?x)^(\n                paddle/utils/.*|\n                paddle/cinn/utils/registry.h\n            )$\n-   repo: https://github.com/PFCCLab/typos-pre-commit-mirror.git\n    rev: v1.27.3\n    hooks:\n    -   id: typos\n        args: [--force-exclude]\n# For Python files\n-   repo: https://github.com/psf/black-pre-commit-mirror\n    rev: 24.8.0\n    hooks:\n    -   id: black\n-   repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.8.3\n    hooks:\n    -   id: ruff\n        args: [--fix, --exit-non-zero-on-fix, --no-cache]\n# For C++ files\n-   repo: local\n    hooks:\n    -   id: clang-format\n        name: clang-format\n        description: Format files with ClangFormat.\n        entry: bash ./tools/codestyle/clang_format.sh -i\n        language: system\n        files: \\.(c|cc|cxx|cpp|cu|h|hpp|hxx|xpu|kps)$\n-   repo: local\n    hooks:\n    -   id: cpplint-cpp-source\n        name: cpplint\n        description: Check C++ code style using cpplint.py.\n        entry: bash ./tools/codestyle/cpplint_pre_commit.sh\n        language: system\n        files: \\.(cc|cxx|cpp|cu|h|hpp|hxx)$\n        args:\n            - --extensions=cc,cxx,cpp,cu,cuh,h,hpp,hxx,kps\n            - --filter=-readability/fn_size,-build/include_what_you_use,-build/c++11,-whitespace/parens\n            - --quiet\n        # Exclude third-party libraries\n        exclude:  |\n            (?x)^(\n                paddle/utils/flat_hash_map\\.h\n            )$\n-   repo: local\n    hooks:\n    -   id: clang-tidy\n        name: clang-tidy\n        description: Parallel clang-tidy runner.\n        entry: python ./tools/codestyle/clang-tidy.py\n        language: system\n        files: \\.(c|cc|cxx|cpp|h|hpp|hxx)$\n        args:\n            - -p=build/\n            - -extra-arg=-Wno-unknown-warning-option\n            - -extra-arg=-Wno-pessimizing-move\n            - -extra-arg=-Wno-braced-scalar-init\n            - -extra-arg=-Wno-dangling-gsl\n            - -extra-arg=-Wno-deprecated-copy\n            - -extra-arg=-Wno-final-dtor-non-final-class\n            - -extra-arg=-Wno-implicit-int-float-conversion\n            - -extra-arg=-Wno-inconsistent-missing-override\n            - -extra-arg=-Wno-infinite-recursion\n            - -extra-arg=-Wno-mismatched-tags\n            - -extra-arg=-Wno-self-assign\n            - -extra-arg=-Wno-sign-compare\n            - -extra-arg=-Wno-sometimes-uninitialized\n            - -extra-arg=-Wno-tautological-overlap-compare\n            - -extra-arg=-Wno-unused-const-variable\n            - -extra-arg=-Wno-unused-lambda-capture\n            - -extra-arg=-Wno-unused-private-field\n            - -extra-arg=-Wno-unused-value\n            - -extra-arg=-Wno-unused-variable\n            - -extra-arg=-Wno-overloaded-virtual\n            - -extra-arg=-Wno-defaulted-function-deleted\n            - -extra-arg=-Wno-delete-non-abstract-non-virtual-dtor\n            - -extra-arg=-Wno-return-type-c-linkage\n# For CMake files\n-   repo: local\n    hooks:\n    -   id: auto-generate-cmakelists\n        name: auto-generate-cmakelists\n        entry: bash ./tools/gen_ut_cmakelists.hook\n        language: system\n        files: testslist.csv$\n-   repo: https://github.com/cheshirekow/cmake-format-precommit\n    rev: v0.6.13\n    hooks:\n    -   id: cmake-format\n        # exclude paddle/fluid/operators/CMakeLists.txt, see the comment\n        # https://github.com/PaddlePaddle/Paddle/pull/43057#pullrequestreview-993471860\n        exclude: |\n            (?x)^(\n                paddle/fluid/operators/CMakeLists.txt\n            )$\n-   repo: https://github.com/PFCCLab/cmake-lint-paddle\n    rev: v1.5.1\n    hooks:\n    -   id: cmakelint\n        args: [--config=./tools/codestyle/.cmakelintrc]\n        # Exclude some files has false positive warnings\n        # Need to fix them in the future\n        exclude: |\n            (?x)^(\n                cmake/external/onnxruntime.cmake\n                )$\n# Others\n-   repo: local\n    hooks:\n    -   id: sort-txt-file\n        name: sort-txt-file\n        description: Sorts each line string in a text file\n        entry: python ./tools/codestyle/sort_txt_file.py\n        language: python\n        files: test/white_list/pir_op_test_white_list\n        args: []\n"
        },
        {
          "name": "AUTHORS.md",
          "type": "blob",
          "size": 5.3115234375,
          "content": "This is an incomplete list of authors of [Paddle](https://github.com/PaddlePaddle/Paddle/) codebase, to see a full list, please use the source control tool git. PaddlePaddle community encourages every Paddle codebase author include his/her GitHub account and fullname here.\n\n\n| Github account | name |\n|---|---|\n| abhinavarora | Abhinav Arora |\n| andreazanetti | Andrea Zanetti |\n| arlesniak | Artur Lesniak |\n| [arogowie-intel](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg) | Adam Osewski |\n| backyes | Yan-Fei Wang |\n| baiyfbupt | Yi-Fan Bai |\n| beckett1124 | Bin Qi |\n| ChengduoZH | Cheng-Duo Zhao|\n| chengxiaohua1105 | Xiao-Hua Cheng |\n| chenwhql | Wei-Hang Chen |\n| cxwangyi, yiwangbaidu, wangkuiyi | Yi Wang |\n| cxysteven | Xing-Yi Cheng |\n| ddokupil | Dariusz Dokupil |\n| dzhwinter | Zhi-Hong Dong |\n| dragonwarrior | Long Wang |\n| dyning | Yuning Du |\n| emailweixu | Wei Xu |\n| engineer1109 | Jia-Liang Wang |\n| gangliao | Gang Liao |\n| gongweibao | Wei-Bao Gong |\n| guru4elephant | Daxiang Dong |\n| Guo Sheng | Sheng Guo |\n| [grygielski](https://raw.githubusercontent.com/jczaja/Paddle/paddle-poland-team/doc/images/paddle_poland_team.jpg)| Adam Grygielski |\n| Haichao-Zhang | Hai-Chao Zhang |\n| HarperCy | Sicheng Hao |\n| hedaoyuan | Dao-Yuan He |\n| helinwang | He-Lin Wang |\n| heliqi | Li-Qi He |\n| houj04 | HOU Jue |\n| [runzhech](https://github.com/runzhech) | Runzhe Chen |\n| [dynamicheart](https://github.com/dynamicheart) | Jianbang Yang |\n| HulekJakub | Jakub Hulek |\n| jacquesqiao | Long-Fei Qiao |\n| [jakpiase](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg) | Jakub Piasecki |\n| [jczaja](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg) | Jacek Czaja |\n| jiahy0825 | Hongyu Jia |\n| JiayiFeng | Jia-Yi Feng |\n| kbinias | Krzysztof Binias |\n| kexinzhao | Ke-Xin Zhao |\n| kuke | Yi-Bing Liu |\n| [lidanqing](https://raw.githubusercontent.com/jczaja/Paddle/paddle-poland-team/doc/images/paddle_poland_team.jpg) | DanQing Li |\n| lcy-seso | Ying Cao |\n| cjld | Dun Liang |\n| lj970926 | Jin Li |\n| lipeng-unisound | Peng Li |\n| gavin1332 | Yi Liu |\n| cqulilujia | Lujia Li |\n| liuyuan | Yuan Liu |\n| livc | Zhao Li |\n| llxxxll | Yong-Feng Liu |\n| luotao01 | Tao Luo |\n| lzhao4ever | Liang Zhao |\n| [RuohengMa](https://github.com/RuohengMa) | Ruoheng Ma |\n| mozga-intel | Mateusz Ozga |\n| NHZlX | Zhao-Long Xing |\n| Noplz | Yuan Gao |\n| pakchoi | Chuan-Jiang Song |\n| panyx0718 | Xin Pan |\n| pengli09 | Peng Li |\n| [piotrekobiIntel](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg) | Piotr Paturej |\n| [pmajchrzak](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg) | Piotr Majchrzak |\n| pkuyym | Ya-Ming Yang |\n| pzelazko-intel | Pawel Zelazko |\n| [pawelpiotrowicz](https://raw.githubusercontent.com/jczaja/Paddle/paddle-poland-team/doc/images/paddle_poland_team.jpg)  | Pawel Piotrowicz |\n| QiJune | Jun Qi |\n| qingqing01 | Qing-Qing Dang |\n| reyoung | Yang Yu |\n| [Sand3r-](https://raw.githubusercontent.com/jczaja/Paddle/paddle-poland-team/doc/images/paddle_poland_team.jpg)| Michal Gallus |\n| [sfraczek](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg)| Sylwester Fraczek |\n| Silv3S | Slawomir Siwek |\n| sneaxiy | Jin-Le Zeng |\n| Superjom | Chun-Wei Yan |\n| tensor-tang | Jian Tang |\n| tianbingsz | Tian-Bing Xu |\n| tizhou86 | Ti Zhou |\n| tpatejko | Tomasz Patejko |\n| [tsocha](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg) | Tomasz Socha |\n| typhoonzero | Yi Wu |\n| velconia | Qi-Yang Min |\n| wanghaoshuang | Hao-Shuang Wang |\n| wangyang59 | Yang Wang |\n| wangzhen-nlp | Zhen Wang |\n| wen-bo-yang | Wen-Bo Yang |\n| wojtuss | Wojciech Uss |\n| [wozna](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg)| Joanna Wozna |\n| wwhu | Wei-Wei Hu |\n| xinghai-sun | Xing-Hai Sun |\n| Xreki | Yi-Qun Liu |\n| xujun05 | Jun Xu |\n| xushaoyong | Shao-Yong Xu |\n| Yancey1989 | Xu Yan |\n| zhaopu7 | Pu Zhao |\n| zhiqiu | Qiu-Liang Chen |\n| zhouxiao-coder | Xiao Zhou |\n| Zrachel | Rui-Qing Zhang |\n| jeng1220 | Bai-Cheng(Ryan) Jeng (NVIDIA) |\n| mingxu1067 | Ming Huang (NVIDIA) |\n| zlsh80826 | Reese Wang (NVIDIA) |\n| leo0519 | Leo Chen (NVIDIA) |\n| jzhang533 | Jun Zhang |\n| Ligoml | Meng-Liu Li |\n| jeff41404 | Xiang Gao |\n| zh794390558 | Hui Zhang |\n| limin2021 | Min Li |\n| zhouwei25 | Wei Zhou |\n| littletomatodonkey | Ruo-Yu Guo |\n| zhupengyang | Zhu Pengyang |\n| DesmonDay | Siming Dai |\n| thisjiang | jiangcheng |\n| yghstill | Guanghua Yu |\n| CtfGo | Tefeng Chen |\n| ZHUI | Hui Zhong|\n| LemonNoel | Huijuan Wang |\n| wawltor | Zeyang Fang |\n| FrostML | Zheng-Xi Liu |\n| jiangjiajun | jiangjiajun |\n| dingjiaweiww | dingjiawei |\n| gglin001 | Allen Guo (Graphcore) |\n| yaozhixin | Zhixin Yao (Graphcore) |\n| XBWGC | Xiaobing Wang (Graphcore) |\n| jianghaicheng | Haicheng Jiang (Graphcore) |\n| czr-gc | Zhaorui Chen (Graphcore) |\n| zhao-han | Han Zhao (Graphcore) |\n| yiakwy, yiakwy-xpu-ml-framework-team | Yi Wang (Graphcore) |\n| [Yulv-git](https://github.com/Yulv-git) | Shuangchi He |\n| [zrr1999](https://github.com/zrr1999) | Rongrui Zhan |\n| [will-jl944](https://github.com/will-jl944) | Jiafeng Lu |\n| [gouzil](https://github.com/gouzil) | Chuan Tian |\n| [skywalker2012](https://github.com/skywalker2012) | Yong Wei |\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 25.6337890625,
          "content": "# Copyright (c) 2016 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License\n\nif(APPLE AND WITH_ARM)\n  # cmake 3.19.2 version starts to support M1\n  cmake_minimum_required(VERSION 3.19.2)\n  cmake_policy(VERSION 3.19.2)\nelse()\n  cmake_minimum_required(VERSION 3.15)\n  cmake_policy(VERSION 3.10)\nendif()\n# use to get_property location of static lib\n# https://cmake.org/cmake/help/v3.0/policy/CMP0026.html?highlight=cmp0026\ncmake_policy(SET CMP0026 OLD)\ncmake_policy(SET CMP0079 NEW)\nset(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} \"${CMAKE_CURRENT_SOURCE_DIR}/cmake\")\nset(PADDLE_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR})\nset(PADDLE_BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR})\n\ninclude(system)\n\n# Note(zhouwei): Ninja Generator will set CMAKE_BUILD_TYPE to Debug\nif(NOT CMAKE_BUILD_TYPE)\n  set(CMAKE_BUILD_TYPE\n      \"Release\"\n      CACHE\n        STRING\n        \"Choose the type of build, options are: Debug Release RelWithDebInfo MinSizeRel\"\n        FORCE)\nendif()\n\nproject(paddle CXX C)\n\n# enable language CUDA\n# TODO(Shibo Tao): remove find_package(CUDA) completely.\nfind_package(CUDA QUIET)\nfind_package(MKL CONFIG QUIET)\noption(WITH_ONEMKL \"Compile PaddlePaddle with oneMKL\" OFF)\noption(WITH_GPU \"Compile PaddlePaddle with NVIDIA GPU\" ${CUDA_FOUND})\noption(WITH_MPI \"Compile PaddlePaddle with MPI\" OFF)\noption(WITH_TENSORRT \"Compile PaddlePaddle with NVIDIA TensorRT\" OFF)\noption(WITH_OPENVINO \"Compile PaddlePaddle with Intel OpenVINO\" OFF)\noption(WITH_XPU \"Compile PaddlePaddle with BAIDU KUNLUN XPU\" OFF)\noption(WITH_XPU_KP \"Compile PaddlePaddle with BAIDU XPU compiler \" OFF)\noption(WITH_XPU_XFT \"Compile PaddlePaddle with BAIDU XPU-XFT\" OFF)\noption(WITH_XPU_PLUGIN \"Compile PaddlePaddle with BAIDU XPU plugin\" OFF)\noption(WITH_XPU_XRE5 \"Compile PaddlePaddle with BAIDU XPU XRE 5\" OFF)\noption(WITH_WIN_DUMP_DBG \"Compile with windows core dump debug mode\" OFF)\noption(WITH_ROCM \"Compile PaddlePaddle with ROCM platform\" OFF)\noption(WITH_IPU \"Compile PaddlePaddle with Graphcore IPU\" OFF)\noption(WITH_ONNXRUNTIME \"Compile PaddlePaddle with ONNXRUNTIME\" OFF)\noption(WITH_CUSPARSELT \"Compile PaddlePaddle with CUSPARSELT\" OFF)\noption(WITH_SETUP_INSTALL \"Compile PaddlePaddle with setup.py\" OFF)\noption(WITH_SHARED_PHI \"Compile PaddlePaddle with SHARED LIB of PHI\" ON)\noption(CINN_WITH_CUDNN \"Compile CINN with CUDNN support\" ON)\noption(WITH_PIP_CUDA_LIBRARIES\n       \"Paddle uses the CUDA library provided by NVIDIA\" OFF)\noption(WITH_PIP_TENSORRT \"Paddle uses the tensorrt provided by NVIDIA\" OFF)\noption(WITH_NIGHTLY_BUILD\n       \"Compile nightly paddle whl package of the develop branch\" OFF)\noption(WITH_CPP_TEST \"Compile PaddlePaddle skip cpp test\" ON)\nfind_package(Git REQUIRED)\n\n# config GIT_URL with github mirrors to speed up dependent repos clone\noption(GIT_URL \"Git URL to clone dependent repos\" ${GIT_URL})\nif(NOT GIT_URL)\n  set(GIT_URL \"https://github.com\")\nendif()\n\n# Note(zhouwei): It use option above, so put here\ninclude(init)\ninclude(generic) # simplify cmake module\ninclude(experimental) # experimental build options\n\nif(WITH_GPU AND WITH_XPU)\n  message(FATAL_ERROR \"Error when compile GPU and XPU at the same time\")\nendif()\nif(WITH_GPU AND WITH_XPU_KP)\n  message(FATAL_ERROR \"Error when compile GPU and XPU2 at the same time\")\nendif()\nif(WITH_GPU AND WITH_XPU_XFT)\n  message(FATAL_ERROR \"Error when compile GPU and XPU-XFT at the same time\")\nendif()\nif(WITH_GPU AND WITH_XPU_XRE5)\n  message(FATAL_ERROR \"Error when compile GPU and XPU-XRE5 at the same time\")\nendif()\nif(WITH_GPU AND WITH_ROCM)\n  message(FATAL_ERROR \"Error when compile CUDA and ROCM at the same time\")\nendif()\n\nif(WITH_GPU AND NOT APPLE)\n  if(WITH_PIP_CUDA_LIBRARIES AND CMAKE_SYSTEM_NAME STREQUAL \"Windows\")\n    add_definitions(-DPADDLE_WITH_PIP_CUDA_LIBRARIES)\n  endif()\n  #(Note risemeup1): The cudart dynamic library libcudart.so is used by set CUDA_USE_STATIC_CUDA_RUNTIME and CMAKE_CUDA_FLAGS\n  if(CMAKE_SYSTEM_NAME STREQUAL \"Linux\" AND CMAKE_SYSTEM_PROCESSOR STREQUAL\n                                            \"x86_64\")\n    set(CUDA_USE_STATIC_CUDA_RUNTIME\n        OFF\n        CACHE BOOL \"\" FORCE)\n    set(CMAKE_CUDA_FLAGS \"--cudart shared\")\n    if(WITH_PIP_CUDA_LIBRARIES)\n      #(Note risemeup1): Flag 'PADDLE_WITH_PIP_CUDA_LIBRARIES' will be used in dynamic_loader.cc to search for CUDA-related .so files through the Python libraries provided by NVIDIA.\n      add_definitions(-DPADDLE_WITH_PIP_CUDA_LIBRARIES)\n    endif()\n  endif()\n  enable_language(CUDA)\n  message(STATUS \"CUDA compiler: ${CMAKE_CUDA_COMPILER}, version: \"\n                 \"${CMAKE_CUDA_COMPILER_ID} ${CMAKE_CUDA_COMPILER_VERSION}\")\nendif()\n\nmessage(STATUS \"CXX compiler: ${CMAKE_CXX_COMPILER}, version: \"\n               \"${CMAKE_CXX_COMPILER_ID} ${CMAKE_CXX_COMPILER_VERSION}\")\nmessage(STATUS \"C compiler: ${CMAKE_C_COMPILER}, version: \"\n               \"${CMAKE_C_COMPILER_ID} ${CMAKE_C_COMPILER_VERSION}\")\nmessage(STATUS \"AR tools: ${CMAKE_AR}\")\n\nif((CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\") AND CMAKE_CXX_COMPILER_VERSION\n                                              VERSION_GREATER 10.4)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-error=uninitialized\")\nendif()\n\n# MUSL build turn off warnings\n\nif(WITH_MUSL)\n  set(CMAKE_CXX_FLAGS\n      \"${CMAKE_CXX_FLAGS} -Wno-error=deprecated-declarations -Wno-deprecated-declarations -Wno-error=pessimizing-move -Wno-error=deprecated-copy\"\n  )\nendif()\n\nif(APPLE AND WITH_ARM)\n  set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -target arm64-apple-darwin\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_C_FLAGS} -target arm64-apple-darwin\")\nendif()\n\nif(WIN32)\n  option(MSVC_STATIC_CRT \"use static C Runtime library by default\" ON)\n  message(\"Build static library of PHI\")\n  # (Note xuxinyi04): If CMAKE_SUPPRESS_REGENERATION is OFF, which is default, then CMake adds a\n  # special target on which all other targets depend that checks the build system and optionally\n  # re-runs CMake to regenerate the build system when the target specification source changes.\n  set(CMAKE_SUPPRESS_REGENERATION OFF)\n  set(CMAKE_STATIC_LIBRARY_PREFIX lib)\n  set(WITH_SHARED_PHI\n      OFF\n      CACHE BOOL \"Disable WITH_SHARED_PHI when compiling PADDLE ON WIN32\" FORCE)\n\n  set(CMAKE_C_FLAGS_DEBUG \"${CMAKE_C_FLAGS_DEBUG} /bigobj\")\n  set(CMAKE_C_FLAGS_RELEASE \"${CMAKE_C_FLAGS_RELEASE} /bigobj\")\n  set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} /bigobj\")\n  set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} /bigobj\")\n\n  if(\"${CMAKE_GENERATOR}\" STREQUAL \"Ninja\")\n    set(CMAKE_C_FLAGS_DEBUG \"${CMAKE_C_FLAGS_DEBUG} /Zc:inline\")\n    set(CMAKE_C_FLAGS_RELEASE \"${CMAKE_C_FLAGS_RELEASE} /Zc:inline\")\n    set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} /Zc:inline\")\n    set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} /Zc:inline\")\n  endif()\n\n  if(MSVC_STATIC_CRT)\n    message(\n      STATUS\n        \"Use static C runtime time, refer to https://docs.microsoft.com/en-us/cpp/c-runtime-library/crt-library-features?view=vs-2019\"\n    )\n    foreach(\n      flag_var\n      CMAKE_CXX_FLAGS\n      CMAKE_CXX_FLAGS_DEBUG\n      CMAKE_CXX_FLAGS_RELEASE\n      CMAKE_CXX_FLAGS_MINSIZEREL\n      CMAKE_CXX_FLAGS_RELWITHDEBINFO\n      CMAKE_C_FLAGS\n      CMAKE_C_FLAGS_DEBUG\n      CMAKE_C_FLAGS_RELEASE\n      CMAKE_C_FLAGS_MINSIZEREL\n      CMAKE_C_FLAGS_RELWITHDEBINFO)\n      if(${flag_var} MATCHES \"/MD\")\n        string(REGEX REPLACE \"/MD\" \"/MT\" ${flag_var} \"${${flag_var}}\")\n      endif()\n    endforeach()\n  endif()\n\n  # msvc max/min macro conflict with std::min/max, define NOMINMAX globally\n  add_definitions(\"-DNOMINMAX\")\n\n  # 1. windows.h define 'small' cause CUDA11.6/11.7/11.8 's cub compile error,\n  # see https://github.com/microsoft/onnxruntime/issues/11227\n  # 2. WIN32_LEAN_AND_MEAN minimize the windows include files, avoid define 'small'\n  add_definitions(-DWIN32_LEAN_AND_MEAN)\n\n  # windows build turn off warnings, use parallel compiling.\n  foreach(\n    flag_var\n    CMAKE_CXX_FLAGS\n    CMAKE_CXX_FLAGS_DEBUG\n    CMAKE_CXX_FLAGS_RELEASE\n    CMAKE_CXX_FLAGS_MINSIZEREL\n    CMAKE_CXX_FLAGS_RELWITHDEBINFO\n    CMAKE_C_FLAGS\n    CMAKE_C_FLAGS_DEBUG\n    CMAKE_C_FLAGS_RELEASE\n    CMAKE_C_FLAGS_MINSIZEREL\n    CMAKE_C_FLAGS_RELWITHDEBINFO)\n    string(REGEX REPLACE \"/W[1-4]\" \" /W0 \" ${flag_var} \"${${flag_var}}\")\n\n    # NOTE(zhouwei25): GPU compile have too high memory utilization when parallel compiling,\n    # For Visual Studio generators, /MP should be added.\n    # For other generators like Ninja, it is not need to add /MP.\n    if(CMAKE_GENERATOR MATCHES \"Visual Studio\" AND NOT WITH_GPU)\n      math(EXPR PROCESS_MAX \"${CPU_CORES} * 2 / 3\")\n      set(${flag_var} \"${${flag_var}} /MP${PROCESS_MAX}\")\n    endif()\n  endforeach()\n  foreach(flag_var CMAKE_CXX_FLAGS CMAKE_C_FLAGS)\n    set(${flag_var} \"${${flag_var}} /w\")\n  endforeach()\n\n  # Windows Remove /Zi, /ZI for Release, MinSizeRel builds\n  foreach(flag_var\n          CMAKE_C_FLAGS CMAKE_C_FLAGS_RELEASE CMAKE_C_FLAGS_MINSIZEREL\n          CMAKE_CXX_FLAGS CMAKE_CXX_FLAGS_RELEASE CMAKE_CXX_FLAGS_MINSIZEREL)\n    if(${flag_var} MATCHES \"/Z[iI]\")\n      string(REGEX REPLACE \"/Z[iI]\" \"\" ${flag_var} \"${${flag_var}}\")\n    endif()\n  endforeach()\n\n  set(CMAKE_C_FLAGS\n      \"${CMAKE_C_FLAGS} /wd4068 /wd4129 /wd4244 /wd4267 /wd4297 /wd4530 /wd4577 /wd4819 /wd4838\"\n  )\n  set(CMAKE_CXX_FLAGS\n      \"${CMAKE_CXX_FLAGS} /wd4068 /wd4129 /wd4244 /wd4267 /wd4297 /wd4530 /wd4577 /wd4819 /wd4838\"\n  )\n\n  foreach(flag_var CMAKE_SHARED_LINKER_FLAGS CMAKE_STATIC_LINKER_FLAGS\n                   CMAKE_EXE_LINKER_FLAGS CMAKE_LINKER_FLAGS)\n    set(${flag_var}\n        \"${${flag_var}} /ignore:4049 /ignore:4217 /ignore:4006 /ignore:4221\")\n    if(MSVC_STATIC_CRT)\n      set(${flag_var} \"${${flag_var}} /NODEFAULTLIB:MSVCRT.LIB\")\n    else()\n      set(${flag_var} \"${${flag_var}} /NODEFAULTLIB:LIBCMT.LIB\")\n    endif()\n  endforeach()\n\n  if(WITH_WIN_DUMP_DBG)\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} /Zi\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /Zi\")\n\n    foreach(flag_var CMAKE_SHARED_LINKER_FLAGS CMAKE_STATIC_LINKER_FLAGS\n                     CMAKE_EXE_LINKER_FLAGS CMAKE_LINKER_FLAGS)\n      set(${flag_var} \"${${flag_var}} /DEBUG /OPT:REF /OPT:ICF\")\n    endforeach()\n\n    add_definitions(\"-DWITH_WIN_DUMP_DBG\")\n  endif()\n\nelse()\n  set(CMAKE_CXX_FLAGS\n      \"${CMAKE_CXX_FLAGS} -Wno-error=deprecated-declarations -Wno-deprecated-declarations\"\n  )\nendif()\n\nfind_package(Threads REQUIRED)\n\ninclude(simd)\n\n################################ Exposed Configurations #######################################\noption(WITH_AVX \"Compile PaddlePaddle with AVX intrinsics\" ${AVX_FOUND})\noption(WITH_PYTHON \"Compile PaddlePaddle with python interpreter\" ON)\noption(WITH_TESTING \"Compile PaddlePaddle with unit testing\" OFF)\noption(WITH_MULTINODE_TESTING \"Test multinode apis and ops\" OFF)\noption(WITH_MKL \"Compile PaddlePaddle with MKL support.\" ${AVX_FOUND})\noption(WITH_SYSTEM_BLAS \"Use system blas library\" OFF)\noption(WITH_DISTRIBUTE \"Compile with distributed support\" OFF)\noption(WITH_BRPC_RDMA \"Use brpc rdma as the rpc protocal\" OFF)\noption(ON_INFER \"Turn on inference optimization and inference-lib generation\"\n       ON)\noption(WITH_CPP_DIST \"Install PaddlePaddle C++ distribution\" OFF)\noption(WITH_GFLAGS \"Compile PaddlePaddle with gflags support\" OFF)\n################################ Internal Configurations #######################################\noption(WITH_NV_JETSON \"Compile PaddlePaddle with NV JETSON\" OFF)\noption(WITH_PROFILER \"Compile PaddlePaddle with GPU profiler and gperftools\"\n       OFF)\noption(WITH_COVERAGE \"Compile PaddlePaddle with code coverage\" OFF)\noption(WITH_INCREMENTAL_COVERAGE\n       \"Generate coverage reports only for incremental code\" OFF)\noption(WITH_LIBXSMM \"Compile with libxsmm\" OFF)\noption(COVERALLS_UPLOAD \"Package code coverage data to coveralls\" OFF)\noption(WITH_PSLIB \"Compile with pslib support\" OFF)\noption(WITH_BOX_PS \"Compile with box_ps support\" OFF)\noption(WITH_XBYAK \"Compile with xbyak support\" ON)\noption(WITH_PSCORE \"Compile with parameter server support\" ${WITH_DISTRIBUTE})\noption(WITH_HETERPS \"Compile with heterps\" OFF)\noption(WITH_INFERENCE_API_TEST\n       \"Test fluid inference C++ high-level api interface\" OFF)\noption(WITH_NVTX \"Paddle with nvtx for profiler\" OFF)\noption(PY_VERSION \"Compile PaddlePaddle with python3 support\" ${PY_VERSION})\noption(WITH_DGC \"Use DGC(Deep Gradient Compression) or not\" ${WITH_DISTRIBUTE})\noption(\n  SANITIZER_TYPE\n  \"Choose the type of sanitizer, options are: Address, Leak, Memory, Thread, Undefined\"\n  OFF)\noption(WITH_CINN \"Compile PaddlePaddle with CINN\" OFF)\noption(WITH_NCCL \"Compile PaddlePaddle with NCCL support\" ON)\noption(WITH_RCCL \"Compile PaddlePaddle with RCCL support\" ON)\noption(WITH_XPU_BKCL \"Compile PaddlePaddle with BAIDU KUNLUN XPU BKCL\" OFF)\noption(WITH_CRYPTO \"Compile PaddlePaddle with crypto support\" ON)\noption(WITH_ARM \"Compile PaddlePaddle with arm support\" OFF)\noption(WITH_SW \"Compile PaddlePaddle with sw support\" OFF)\noption(WITH_MIPS \"Compile PaddlePaddle with mips support\" OFF)\noption(WITH_LOONGARCH \"Compile PaddlePaddle with loongarch support\" OFF)\noption(WITH_MUSL \"Compile with musl libc instead of gblic\" OFF)\noption(WITH_UNITY_BUILD \"Compile with UnityBuild mode\" OFF)\noption(WITH_STRIP \"Strip so files of Whl packages\" OFF)\noption(NEW_RELEASE_PYPI\n       \"PaddlePaddle next-level release strategy for pypi cubin package\" OFF)\noption(NEW_RELEASE_ALL\n       \"PaddlePaddle next-level release strategy for all arches cubin package\"\n       OFF)\noption(NEW_RELEASE_JIT\n       \"PaddlePaddle next-level release strategy for backup jit package\" OFF)\noption(WITH_POCKETFFT \"Compile with pocketfft support\" ON)\noption(WITH_RECORD_BUILDTIME\n       \"Compile PaddlePaddle with record all targets build time\" OFF)\noption(WITH_CUSTOM_DEVICE \"Compile with custom device support\" OFF)\noption(WITH_ARM_BRPC \"Supprot Brpc in Arm\" OFF)\noption(WITH_FLPS \"FL PS mode\" OFF)\noption(WITH_RPC \"Compile with rpc support\" ${WITH_DISTRIBUTE})\noption(WITH_CUDNN_FRONTEND\n       \"Compile with CUDNN Frontend API support (experimental)\" OFF)\noption(WITH_SHARED_IR \"Compile PaddlePaddle with SHARED LIB of IR\" ON)\noption(WITH_NVCC_LAZY\n       \"Compile PaddlePaddle with nvcc lazy mode, used for CI-Inference only.\"\n       ON)\noption(BUILD_WHL_PACKAGE \"Build paddle whl package after compilation\" ON)\n\nif(WITH_RECORD_BUILDTIME)\n  set_property(\n    GLOBAL\n    PROPERTY\n      RULE_LAUNCH_COMPILE\n      \"${CMAKE_CURRENT_SOURCE_DIR}/tools/get_build_time.sh ${CMAKE_CURRENT_BINARY_DIR}\"\n  )\n  set_property(\n    GLOBAL\n    PROPERTY\n      RULE_LAUNCH_LINK\n      \"${CMAKE_CURRENT_SOURCE_DIR}/tools/get_build_time.sh ${CMAKE_CURRENT_BINARY_DIR}\"\n  )\nelse()\n  include(ccache\n  )# set ccache for compilation ; if WITH_RECORD_BUILDTIME=ON can't use ccache\nendif()\nunset(WITH_RECORD_BUILDTIME CACHE)\n\n# PY_VERSION\nif(NOT PY_VERSION)\n  set(PY_VERSION 3.8)\nelseif(${PY_VERSION} VERSION_LESS 3.8)\n  message(FATAL_ERROR \"Paddle only support Python version>=3.8 now\")\nendif()\nset(PYBIND11_PYTHON_VERSION ${PY_VERSION})\n\n# the type of sanitizer, options are: Address, Leak, Memory, Thread, Undefined. Default: OFF\nif(SANITIZER_TYPE AND NOT \"${SANITIZER_TYPE}\" MATCHES\n                      \"^(Address|Leak|Memory|Thread|Undefined)$\")\n  message(\"Choose the correct type of sanitizer\")\n  return()\nendif()\n\nif(LINUX\n   AND NOT WITH_CUSTOM_DEVICE\n   AND NOT WITH_GPU\n   AND NOT WITH_ROCM\n   AND NOT WITH_XPU\n   AND NOT WITH_XPU_KP\n   AND NOT WITH_XPU_XFT\n   AND WITH_PYTHON)\n  set(WITH_CUSTOM_DEVICE\n      ON\n      CACHE BOOL \"Enable Custom Device when compiling for Linux\" FORCE)\n  message(\n    \"Enable Custom Device when compiling for Linux. Force WITH_CUSTOM_DEVICE=ON.\"\n  )\nendif()\n\nif(WIN32)\n  if(WITH_DISTRIBUTE)\n    message(\n      WARNING\n        \"Disable DISTRIBUTE when compiling for Windows. Force WITH_DISTRIBUTE=OFF.\"\n    )\n    set(WITH_DISTRIBUTE\n        OFF\n        CACHE STRING \"Disable DISTRIBUTE when compiling for Windows\" FORCE)\n  endif()\n  if(WITH_NCCL)\n    message(\n      WARNING \"Disable NCCL when compiling for Windows. Force WITH_NCCL=OFF.\")\n    set(WITH_NCCL\n        OFF\n        CACHE STRING \"Disable NCCL when compiling for Windows\" FORCE)\n  endif()\nendif()\n\nif(NOT WITH_TESTING AND WITH_MULTINODE_TESTING)\n  message(\n    WARNING\n      \"Disable WITH_MULTINODE_TESTING when compiling without TESTING. Force WITH_MULTINODE_TESTING=OFF.\"\n  )\n  set(WITH_MULTINODE_TESTING\n      OFF\n      CACHE STRING\n            \"Disable WITH_MULTINODE_TESTING when compiling without TESTING\"\n            FORCE)\nendif()\n\nif(NOT WITH_GPU AND WITH_NCCL)\n  message(\n    WARNING \"Disable NCCL when compiling without GPU. Force WITH_NCCL=OFF.\")\n  set(WITH_NCCL\n      OFF\n      CACHE STRING \"Disable NCCL when compiling without GPU\" FORCE)\nendif()\n\n# force WITH_XPU on when WITH_XPU_KP\nif(WITH_XPU_KP AND NOT WITH_XPU)\n  message(\n    WARNING\n      \"Enable WITH_XPU when compiling with WITH_XPU_KP. Force WITH_XPU=ON.\")\n  set(WITH_XPU\n      ON\n      CACHE STRING \"Enable WITH_XPU when compiling with WITH_XPU_KP\" FORCE)\nendif()\n\nif(NOT WITH_XPU AND WITH_XPU_XFT)\n  message(\n    WARNING\n      \"Enable WITH_XPU when compiling with WITH_XPU_XFT. Force WITH_XPU=ON.\")\n  set(WITH_XPU\n      ON\n      CACHE STRING \"Enable WITH_XPU when compiling with WITH_XPU_XFT\" FORCE)\nendif()\n\nif(NOT WITH_XPU AND WITH_XPTI)\n  message(\n    WARNING \"Disable XPTI when compiling without XPU. Force WITH_XPTI=OFF.\")\n  set(WITH_XPTI\n      OFF\n      CACHE STRING \"Disable XPTI when compiling without XPU\" FORCE)\nendif()\n\nif(NOT WITH_XPU AND WITH_XPU_BKCL)\n  message(\n    WARNING \"Disable BKCL when compiling without XPU. Force WITH_XPU_BKCL=OFF.\")\n  set(WITH_XPU_BKCL\n      OFF\n      CACHE STRING \"Disable BKCL when compiling without XPU\" FORCE)\nendif()\n\nif(NOT WITH_XPU AND WITH_XPU_XRE5)\n  message((WARNING\n           \"Disable XRE5 when compiling without XPU. Force WITH_XPU_XRE5=OFF\"))\n  set(WITH_XPU_XRE5\n      OFF\n      CACHE STRING \"Disable XRE5 when compiling without XPU\" FORCE)\nendif()\n\nif(WITH_NCCL)\n  add_definitions(\"-DPADDLE_WITH_NCCL\")\n  include(nccl)\nelse()\n  if(WITH_GPU)\n    message(\n      WARNING\n        \"If the environment is multi-card, the WITH_NCCL option needs to be turned on, otherwise only a single card can be used.\"\n    )\n  endif()\nendif()\n\nif(WITH_BRPC_RDMA)\n  message(STATUS \"Use brpc with rdma.\")\n  if(NOT WITH_DISTRIBUTE)\n    message(FATAL_ERROR \"Can't use brpc rdma in no distribute env.\")\n  endif()\nendif()\n\nif(WITH_GPU)\n  include(cuda)\n  # lite subgraph compilation depends on CUDNN_ROOT,\n  # so include(cudnn) needs to be in front of include(third_party/lite)\n  include(cudnn) # set cudnn libraries, must before configure\n  include(tensorrt)\n  # there is no official support of nccl, cupti in windows\n  if(NOT WIN32)\n    include(cupti)\n  endif()\nendif()\n\nif(WITH_ROCM)\n  include(hip)\n  include(miopen) # set miopen libraries, must before configure\n  include(cupti)\nendif()\n\nif(WITH_XPU_KP)\n  include(xpu_kp)\nendif()\n\nif(NOT WITH_ROCM AND WITH_RCCL)\n  message(\n    WARNING \"Disable RCCL when compiling without ROCM. Force WITH_RCCL=OFF.\")\n  set(WITH_RCCL\n      OFF\n      CACHE STRING \"Disable RCCL when compiling without ROCM\" FORCE)\nendif()\n\nif(WITH_RCCL)\n  add_definitions(\"-DPADDLE_WITH_RCCL\")\n  include(rccl)\nelse()\n  if(WITH_ROCM)\n    message(\n      WARNING\n        \"If the environment is multi-card, the WITH_RCCL option needs to be turned on, otherwise only a single card can be used.\"\n    )\n  endif()\nendif()\n\nif(WITH_HETERPS AND WITH_PSLIB)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -D_GLIBCXX_USE_CXX11_ABI=0\")\nendif()\n\nif(WITH_DISTRIBUTE)\n  if(LINUX)\n    set(WITH_GLOO\n        ON\n        CACHE STRING \"Enable GLOO when compiling WITH_DISTRIBUTE=ON.\" FORCE)\n  endif()\n  if(WITH_ROCM AND HIP_VERSION LESS_EQUAL 40020496)\n    # TODO(qili93): third-party rocksdb throw Illegal instruction with HIP version 40020496\n    message(\n      WARNING\n        \"Disable WITH_PSCORE when HIP_VERSION is less than or equal 40020496. Force WITH_PSCORE=OFF.\"\n    )\n    set(WITH_PSCORE\n        OFF\n        CACHE\n          BOOL\n          \"Disable WITH_PSCORE when HIP_VERSION is less than or equal 40020496\"\n          FORCE)\n  endif()\nendif()\n\nif(WITH_RPC)\n  if(NOT LINUX)\n    message(\n      WARNING \"Disable WITH_RPC when not compiled on Linux. Force WITH_RPC=OFF.\"\n    )\n    set(WITH_RPC\n        OFF\n        CACHE BOOL \"Disable WITH_RPC when not compiled on Linux\" FORCE)\n  endif()\n  if(NOT WITH_DISTRIBUTE AND WITH_RPC)\n    message(\n      WARNING\n        \"Disable WITH_RPC when not compiled with distribute. Force WITH_RPC=OFF.\"\n    )\n    set(WITH_RPC\n        OFF\n        CACHE BOOL \"Disable WITH_RPC when not compiled with distribute\" FORCE)\n  endif()\n  if(WITH_ROCM AND WITH_RPC)\n    message(\n      WARNING \"Disable WITH_RPC when compiling with ROCM. Force WITH_RPC=OFF.\")\n    set(WITH_RPC\n        OFF\n        CACHE BOOL \"Disable WITH_RPC when compiling with ROCM\" FORCE)\n  endif()\n  if(WITH_XPU AND WITH_RPC)\n    message(\n      WARNING \"Disable WITH_RPC when compiling with XPU. Force WITH_RPC=OFF.\")\n    set(WITH_RPC\n        OFF\n        CACHE BOOL \"Disable WITH_RPC when compiling with XPU\" FORCE)\n  endif()\nendif()\n\nif(WITH_MPI)\n  include(mpi)\nendif()\n\ninclude(third_party\n)# download, build, install third_party, Contains about 20+ dependencies\n\ninclude(flags) # set paddle compile flags\ninclude(util) # set unittest and link libs\ninclude(version) # set PADDLE_VERSION\ninclude(coveralls) # set code coverage\ninclude(configure) # add paddle env configuration\n#------------- cinn cmake config start --------------\n\nif(WITH_CINN)\n  message(STATUS \"Compile Paddle with CINN.\")\n  # TODO(6clc): Use CINN_WITH_CUDNN to completely replace WITH_CUDNN in CINN.\n  #             Use WITH_GPU to completely replace WITH_CUDA in CINN.\n  set(WITH_MKL_CBLAS ${WITH_MKL})\n  if(WITH_GPU)\n    set(WITH_CUDA ${WITH_GPU})\n    add_definitions(-DCINN_WITH_CUDA)\n    set(WITH_CUDNN ${CINN_WITH_CUDNN})\n    if(WITH_CUDNN)\n      add_definitions(-DCINN_WITH_CUDNN)\n    endif()\n  endif()\n\n  include(cmake/cinn.cmake)\n  add_definitions(-DPADDLE_WITH_CINN)\nendif()\n\n#------------- cinn cmake config end --------------\n\nif(WITH_PROFILER)\n  find_package(Gperftools REQUIRED)\n  include_directories(${GPERFTOOLS_INCLUDE_DIR})\n  add_definitions(-DWITH_GPERFTOOLS)\nendif()\n\ninclude_directories(\"${PADDLE_SOURCE_DIR}\")\n\nif(WITH_NV_JETSON)\n  set(WITH_ARM\n      ON\n      CACHE STRING \"Set WITH_ARM=ON when compiling WITH_NV_JETSON=ON.\" FORCE)\nendif()\n\nif(WITH_ARM)\n  set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fPIC\")\n  set(WITH_XBYAK\n      OFF\n      CACHE STRING \"Disable XBYAK when compiling WITH_ARM=ON.\" FORCE)\n  set(WITH_MKL\n      OFF\n      CACHE STRING \"Disable MKL when compiling WITH_ARM=ON.\" FORCE)\n  set(WITH_AVX\n      OFF\n      CACHE STRING \"Disable AVX when compiling WITH_AVX=OFF.\" FORCE)\n  add_definitions(-DPADDLE_WITH_ARM)\nendif()\n\nif(WITH_SW)\n  # mieee flag solves floating-point exceptions under sw and ALPHA architectures\n  set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -mieee\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fPIC -mieee\")\n  set(WITH_XBYAK\n      OFF\n      CACHE STRING \"Disable XBYAK when compiling WITH_SW=ON\" FORCE)\n  set(WITH_MKL\n      OFF\n      CACHE STRING \"Disable MKL when compiling WITH_SW=ON.\" FORCE)\n  add_definitions(-DPADDLE_WITH_SW)\nendif()\n\nif(WITH_MIPS)\n  set(WITH_XBYAK\n      OFF\n      CACHE STRING \"Disable XBYAK when compiling WITH_MIPS=ON\" FORCE)\n  add_definitions(-DPADDLE_WITH_MIPS)\nendif()\n\nif(WITH_NVTX AND NOT WIN32)\n  add_definitions(-DPADDLE_WITH_NVTX)\nendif()\n\nif(WITH_LOONGARCH)\n  set(WITH_XBYAK\n      OFF\n      CACHE STRING \"Disable XBYAK when compiling WITH_LOONGARCH=ON\" FORCE)\n  set(WITH_MKL\n      OFF\n      CACHE STRING \"Disable MKL when compiling WITH_LOONGARCH=ON.\" FORCE)\n  add_definitions(-DPADDLE_WITH_LOONGARCH)\nendif()\n\nif(WITH_ONEMKL)\n  add_definitions(-DPADDLE_WITH_ONEMKL)\nendif()\n\nif(WITH_HETERPS)\n  if(CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 7.0)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -faligned-new\")\n  endif()\nendif()\nset(PADDLE_PYTHON_BUILD_DIR \"${CMAKE_CURRENT_BINARY_DIR}/python/build\")\n\nset(CMAKE_CXX_FLAGS_RELWITHDEBINFO \"-O3 -g -DNDEBUG\")\nset(CMAKE_C_FLAGS_RELWITHDEBINFO \"-O3 -g -DNDEBUG\")\n\nadd_definitions(-DPADDLE_DLL_EXPORT)\n\nif(ON_INFER)\n  # you can trun off the paddle fluid and inference lib by set ON_INFER=OFF\n  message(\n    STATUS \"On inference mode, will take place some specific optimization.\")\n  include(inference_lib)\n  add_definitions(-DPADDLE_ON_INFERENCE)\n  set(WITH_SHARED_IR\n      OFF\n      CACHE BOOL \"Only paddle_inference.so is allowed in inference.\" FORCE)\nelse()\n  #TODO(luotao), combine this warning with `make inference_lib_dist` command.\n  message(\n    WARNING\n      \"On inference mode, will take place some specific optimization. Turn on the ON_INFER flag when building inference_lib only.\"\n  )\nendif()\n\nif(NOT WITH_SHARED_IR)\n  add_definitions(-DSTATIC_IR)\nendif()\n\nif(WITH_STRIP)\n  find_program(STRIP_PATH strip)\n  if(NOT STRIP_PATH OR NOT LINUX)\n    set(WITH_STRIP\n        OFF\n        CACHE STRING \"Command strip is only used on Linux when it exists.\"\n              FORCE)\n  endif()\nendif()\n\nif(WITH_CPP_DIST)\n  # TODO(huangjiyi): Separate installing C++ distribution from python package\n  # installation and support for installing C++ distribution on more platforms.\n  if(NOT LINUX OR NOT WITH_PYTHON)\n    set(WITH_CPP_DIST\n        OFF\n        CACHE\n          STRING\n          \"Currently C++ Distribution Generation is only available on Linux and compiling WITH_PYTHON=ON.\"\n          FORCE)\n  else()\n    include(paddle_lib)\n  endif()\nendif()\n\nadd_subdirectory(paddle)\nif(WITH_PYTHON)\n  add_subdirectory(python)\nendif()\nadd_subdirectory(test)\n\nget_directory_property(all_inc_dirs INCLUDE_DIRECTORIES)\nlist(JOIN all_inc_dirs \"\\r\\n\" all_inc_dirs)\nfile(WRITE \"${CMAKE_CURRENT_BINARY_DIR}/includes.txt\" ${all_inc_dirs})\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.146484375,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment include:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at  ext_paddle_oss@baidu.com. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4, available at [http://contributor-covenant.org/version/1/4][version]\n\n[homepage]: http://contributor-covenant.org\n[version]: http://contributor-covenant.org/version/1/4/\n"
        },
        {
          "name": "CODE_OF_CONDUCT_cn.md",
          "type": "blob",
          "size": 2.5888671875,
          "content": "# 参与者公约\n\n## 我们的保证\n\n为了促进一个开放透明且友好的环境，我们作为贡献者和维护者保证：无论年龄、种族、民族、性别认同和表达（方式）、体型、身体健全与否、经验水平、国籍、个人表现、宗教或性别取向，参与者在我们项目和社区中都免于骚扰。\n\n## 我们的标准\n\n有助于创造正面环境的行为包括但不限于：\n* 使用友好和包容性语言\n* 尊重不同的观点和经历\n* 耐心地接受建设性批评\n* 关注对社区最有利的事情\n* 友善对待其他社区成员\n\n身为参与者不能接受的行为包括但不限于：\n* 使用与性有关的言语或是图像，以及不受欢迎的性骚扰\n* 捣乱/煽动/造谣的行为或进行侮辱/贬损的评论，人身攻击及政治攻击\n* 公开或私下的骚扰\n* 未经许可地发布他人的个人资料，例如住址或是电子地址\n* 其他可以被合理地认定为不恰当或者违反职业操守的行为\n\n## 我们的责任\n\n项目维护者有责任为「可接受的行为」标准做出诠释，以及对已发生的不被接受的行为采取恰当且公平的纠正措施。\n\n项目维护者有权利及责任去删除、编辑、拒绝与本行为标准有所违背的评论(comments)、提交(commits)、代码、wiki 编辑、问题(issues)和其他贡献，以及项目维护者可暂时或永久性的禁止任何他们认为有不适当、威胁、冒犯、有害行为的贡献者。\n\n## 使用范围\n\n当一个人代表该项目或是其社区时，本行为标准适用于其项目平台和公共平台。\n\n代表项目或是社区的情况，举例来说包括使用官方项目的电子邮件地址、通过官方的社区媒体账号发布或线上或线下事件中担任指定代表。\n\n该项目的呈现方式可由其项目维护者进行进一步的定义及解释。\n\n## 强制执行\n\n可以通过ext_paddle_oss@baidu.com，来联系项目团队来举报滥用、骚扰或其他不被接受的行为。\n\n任何维护团队认为有必要且适合的所有投诉都将进行审查及调查，并做出相对应的回应。项目小组有对事件回报者有保密的义务。具体执行的方针近一步细节可能会单独公布。\n\n没有切实地遵守或是执行本行为标准的项目维护人员，可能会因项目领导人或是其他成员的决定，暂时或是永久地取消其参与资格。\n\n## 来源\n\n本行为标准改编自[贡献者公约][主页]，版本 1.4\n可在此观看https://www.contributor-covenant.org/zh-cn/version/1/4/code-of-conduct.html\n\n[主页]: https://www.contributor-covenant.org\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 8.017578125,
          "content": "# Contribute Code\n\nYou are welcome to contribute to project PaddlePaddle. To contribute to PaddlePaddle, you have to agree with the\n[PaddlePaddle Contributor License Agreement](https://gist.github.com/XiaoguangHu01/75018ad8e11af13df97070dd18ae6808).\n\nWe sincerely appreciate your contribution.  This document explains our workflow and work style.\n\n## Workflow\n\nPaddlePaddle uses this [Git branching model](http://nvie.com/posts/a-successful-git-branching-model/).  The following steps guide usual contributions.\n\n1. Fork\n\n   Our development community has been growing fastly; it doesn't make sense for everyone to write into the official repo.  So, please file Pull Requests from your fork.  To make a fork,  just head over to the GitHub page and click the [\"Fork\" button](https://help.github.com/articles/fork-a-repo/).\n\n1. Clone\n\n   To make a copy of your fork to your local computers, please run\n\n   ```bash\n   git clone https://github.com/your-github-account/paddle\n   cd paddle\n   ```\n\n1. Create the local feature branch\n\n   For daily works like adding a new feature or fixing a bug, please open your feature branch before coding:\n\n   ```bash\n   git checkout -b my-cool-stuff\n   ```\n\n1. Commit\n\n   Before issuing your first `git commit` command, please install [`pre-commit`](http://pre-commit.com/) by running the following commands:\n\n   ```bash\n   pip install pre-commit\n   pre-commit install\n   ```\n\n   Our pre-commit configuration requires clang-format 3.8 for auto-formatting C/C++ code and yapf for Python.\n\n   Once installed, `pre-commit` checks the style of code and documentation in every commit.  We will see something like the following when you run `git commit`:\n\n   ```\n   ➜  git commit\n   CRLF end-lines remover...............................(no files to check)Skipped\n   yapf.................................................(no files to check)Skipped\n   Check for added large files..............................................Passed\n   Check for merge conflicts................................................Passed\n   Check for broken symlinks................................................Passed\n   Detect Private Key...................................(no files to check)Skipped\n   Fix End of Files.....................................(no files to check)Skipped\n   clang-format.........................................(no files to check)Skipped\n   [my-cool-stuff c703c041] add test file\n    1 file changed, 0 insertions(+), 0 deletions(-)\n    create mode 100644 233\n   ```\n\n\tNOTE: The `yapf` installed by `pip install pre-commit` and `conda install -c conda-forge pre-commit` is slightly different. Paddle developers use `pip install pre-commit`.\n\n1. Build and test\n\n   Users can build PaddlePaddle natively on Linux and Mac OS X.  But to unify the building environment and to make it easy for debugging, the recommended way is [using Docker](https://github.com/PaddlePaddle/Paddle/blob/develop/doc/howto/dev/build_en.md).\n\n1. Keep pulling\n\n   An experienced Git user pulls from the official repo often -- daily or even hourly, so they notice conflicts with others work early, and it's easier to resolve smaller conflicts.\n\n   ```bash\n   git remote add upstream https://github.com/PaddlePaddle/Paddle\n   git pull upstream develop\n   ```\n\n1. Push and file a pull request\n\n   You can \"push\" your local work into your forked repo:\n\n   ```bash\n   git push origin my-cool-stuff\n   ```\n\n   The push allows you to create a pull request, requesting owners of this [official repo](https://github.com/PaddlePaddle/Paddle) to pull your change into the official one.\n\n   To create a pull request, please follow [these steps](https://help.github.com/articles/creating-a-pull-request/).\n\n   If your change is for fixing an issue, please write [\"Fixes <issue-URL>\"](https://help.github.com/articles/closing-issues-using-keywords/) in the description section of your pull request.  Github would close the issue when the owners merge your pull request.\n\n   Please remember to specify some reviewers for your pull request.  If you don't know who are the right ones, please follow Github's recommendation.\n\n\n1. Delete local and remote branches\n\n   To keep your local workspace and your fork clean, you might want to remove merged branches:\n\n   ```bash\n   git push origin :my-cool-stuff\n   git checkout develop\n   git pull upstream develop\n   git branch -d my-cool-stuff\n   ```\n\n### Code Review\n\n-  Please feel free to ping your reviewers by sending them the URL of your pull request via IM or email.  Please do this after your pull request passes the CI.\n\n- Please answer reviewers' every comment.  If you are to follow the comment, please write \"Done\"; please give a reason otherwise.\n\n- If you don't want your reviewers to get overwhelmed by email notifications, you might reply their comments by [in a batch](https://help.github.com/articles/reviewing-proposed-changes-in-a-pull-request/).\n\n- Reduce the unnecessary commits.  Some developers commit often.  It is recommended to append a sequence of small changes into one commit by running `git commit --amend` instead of `git commit`.\n\n\n## Coding Standard\n\n### Code Style\n\nOur C/C++ code follows the [Google style guide](http://google.github.io/styleguide/cppguide.html).\n\nOur Python code follows the [PEP8 style guide](https://www.python.org/dev/peps/pep-0008/).\n\nOur build process helps to check the code style.  In [`build.sh`](https://github.com/PaddlePaddle/Paddle/blob/b84e8226514b8bb4405c3c28e54aa5077193d179/paddle/scripts/docker/build.sh#L42), the entry point of our [builder Docker image](https://github.com/PaddlePaddle/Paddle/blob/b84e8226514b8bb4405c3c28e54aa5077193d179/Dockerfile#L88), the CMake argument `WITH_STYLE_CHECK` is set to `ON` by default.  This flag is on\n\nPlease install pre-commit, which automatically reformat the changes to C/C++ and Python code whenever we run `git commit`.  To check the whole codebase, we can run the command `pre-commit run -a`, as in the [`check_style.sh` file](https://github.com/PaddlePaddle/Paddle/blob/b84e8226514b8bb4405c3c28e54aa5077193d179/paddle/scripts/travis/check_style.sh#L30), which is invoked by [our Travis CI configuration](https://github.com/PaddlePaddle/Paddle/blob/b84e8226514b8bb4405c3c28e54aa5077193d179/.travis.yml#L43).\n\n### Unit Tests\n\nPlease remember to add related unit tests.\n\n- For C/C++ code, please follow [`google-test` Primer](https://github.com/google/googletest/blob/master/googletest/docs/primer.md) .\n\n- For Python code, please use [Python's standard `unittest` package](http://pythontesting.net/framework/unittest/unittest-introduction/).\n\n\n### Writing Logs\n\nWe use [glog](https://github.com/google/glog) for logging in our C/C++ code.\n\nFor general information, please use `LOG`.  For debug information, please use [`VLOG`](http://htmlpreview.github.io/?https://github.com/google/glog/blob/master/doc/glog.html#verbose).  The reason is at [here](https://groups.google.com/a/chromium.org/d/msg/chromium-dev/3NDNd1KzXeY/AZKMMx37fdQJ).\n\n`VLOG` requires a *verbose level* parameter.  For example:\n\n```c++\nVLOG(3) << \"Operator FC is taking \" << num_inputs << \"inputs.\"\n```\n\nWhen we run a PaddlePaddle application or test, we can specify a verbose threshold.  For example:\n\n```bash\nGLOG_vmodule=buddy_allocator=2 \\\nGLOG_v=10 \\\npython \\\n../python/paddle/v2/framework/tests/test_recurrent_op.py\n```\n\nThis will enable VLOG messages generated by `buddy_allocator.{h,cc}` and in the verbose range of 0 to 3, so you will see above example VLOG message, which is in level 3.  This suggests that we output overall messages in lower verbose levels, so they display with higher probability.  When coding C++, please follow the verbose level convention as follows:\n\n- verbose level 1: [framework](https://github.com/PaddlePaddle/Paddle/tree/develop/paddle/fluid/framework)\n- verbose level 3: [operators](https://github.com/PaddlePaddle/Paddle/tree/develop/paddle/fluid/operators)\n- verbose level 5: [memory](https://github.com/PaddlePaddle/Paddle/tree/develop/paddle/fluid/memory), [platform](https://github.com/PaddlePaddle/Paddle/tree/develop/paddle/fluid/platform)\n- verbose level 7: [math](https://github.com/PaddlePaddle/Paddle/tree/develop/paddle/fluid/operators/math/)\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.169921875,
          "content": "Copyright (c) 2016 PaddlePaddle Authors. All Rights Reserved\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright (c) 2016 PaddlePaddle Authors. All Rights Reserved.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.57421875,
          "content": "<p align=\"center\">\n<img align=\"center\" src=\"doc/imgs/logo.png\", width=1600>\n<p>\n\n--------------------------------------------------------------------------------\n\nEnglish | [简体中文](./README_cn.md) | [日本語](./README_ja.md)\n\n[![Documentation Status](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n[![Documentation Status](https://img.shields.io/badge/中文文档-最新-brightgreen.svg)](https://paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html)\n[![Release](https://img.shields.io/github/release/PaddlePaddle/Paddle.svg)](https://github.com/PaddlePaddle/Paddle/releases)\n[![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](LICENSE)\n[![Twitter](https://img.shields.io/badge/Twitter-1ca0f1.svg?logo=twitter&logoColor=white)](https://twitter.com/PaddlePaddle)\n\nWelcome to the PaddlePaddle GitHub.\n\nPaddlePaddle, as the first independent R&D deep learning platform in China, has been officially open-sourced to professional communities since 2016. It is an industrial platform with advanced technologies and rich features that cover core deep learning frameworks, basic model libraries, end-to-end development kits, tools & components as well as service platforms.\nPaddlePaddle is originated from industrial practices with dedication and commitments to industrialization. It has been widely adopted by a wide range of sectors including manufacturing, agriculture, enterprise service, and so on while serving more than 10.7 million developers, 235,000 companies and generating 860,000 models. With such advantages, PaddlePaddle has helped an increasing number of partners commercialize AI.\n\n## Installation\n\n### Latest PaddlePaddle Release: [v2.6](https://github.com/PaddlePaddle/Paddle/tree/release/2.6)\n\nOur vision is to enable deep learning for everyone via PaddlePaddle.\nPlease refer to our [release announcement](https://github.com/PaddlePaddle/Paddle/releases) to track the latest features of PaddlePaddle.\n\n### Install Latest Stable Release\n\n``` sh\n# CPU\npip install paddlepaddle\n# GPU\npip install paddlepaddle-gpu\n```\n\nFor more information about installation, please view [Quick Install](https://www.paddlepaddle.org.cn/install/quick)\n\nNow our developers can acquire Tesla V100 online computing resources for free. If you create a program by AI Studio, you will obtain 8 hours to train models online per day. [Click here to start](https://aistudio.baidu.com/aistudio/index).\n\n## FOUR LEADING TECHNOLOGIES\n\n- **Agile Framework for Industrial Development of Deep Neural Networks**\n\n    The PaddlePaddle deep learning framework facilitates the development while lowering the technical burden, through leveraging a programmable scheme to architect the neural networks. It supports both declarative programming and imperative programming with both development flexibility and high runtime performance preserved.  The neural architectures could be automatically designed by algorithms with better performance than the ones designed by human experts.\n\n- **Support Ultra-Large-Scale Training of Deep Neural Networks**\n\n    PaddlePaddle has made breakthroughs in ultra-large-scale deep neural networks training. It launched the world's first large-scale open-source training platform that supports the training of deep networks with 100 billion features and trillions of parameters using data sources distributed over hundreds of nodes. PaddlePaddle overcomes the online deep learning challenges for ultra-large-scale deep learning models, and further achieved real-time model updating with more than 1 trillion parameters.\n     [Click here to learn more](https://github.com/PaddlePaddle/Fleet)\n\n- **High-Performance Inference Engines for Comprehensive Deployment Environments**\n\n   PaddlePaddle is not only compatible with models trained in 3rd party open-source frameworks , but also offers complete inference products for various production scenarios. Our inference product line includes [Paddle Inference](https://www.paddlepaddle.org.cn/inference/master/guides/introduction/index_intro.html): Native inference library for high-performance server and cloud inference; [FastDeploy](https://github.com/PaddlePaddle/FastDeploy): Easy-to-use and High Performance AI model deployment toolkit for Cloud, Mobile and Edge without-of-the-box and unified experience; [Paddle Lite](https://github.com/PaddlePaddle/Paddle-Lite): Ultra-Lightweight inference engine for mobile and IoT environments; [Paddle.js](https://www.paddlepaddle.org.cn/paddle/paddlejs): A frontend inference engine for browser and mini-apps. Furthermore, by great amounts of optimization with leading hardware in each scenario, Paddle inference engines outperform most of the other mainstream frameworks.\n\n- **Industry-Oriented Models and Libraries with Open Source Repositories**\n\n     PaddlePaddle includes and maintains more than 100 mainstream models that have been practiced and polished for a long time in the industry. Some of these models have won major prizes from key international competitions. In the meanwhile, PaddlePaddle has further more than 200 pre-training models (some of them with source codes) to facilitate the rapid development of industrial applications.\n     [Click here to learn more](https://github.com/PaddlePaddle/models)\n\n## Documentation\n\nWe provide [English](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html) and\n[Chinese](https://www.paddlepaddle.org.cn/documentation/docs/zh/guide/index_cn.html) documentation.\n\n- [Guides](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n\n  You might want to start from how to implement deep learning basics with PaddlePaddle.\n\n- [Practice](https://www.paddlepaddle.org.cn/documentation/docs/zh/tutorial/index_cn.html)\n\n  So far you have already been familiar with Fluid. And the next step should be building a more efficient model or inventing your original Operator.\n\n- [API Reference](https://www.paddlepaddle.org.cn/documentation/docs/en/api/index_en.html)\n\n   Our new API enables much shorter programs.\n\n- [How to Contribute](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/08_contribution/index_en.html)\n\n   We appreciate your contributions!\n\n## Open Source Community\n\n- [Github Issues](https://github.com/PaddlePaddle/Paddle/issues): bug reports, feature requests, install issues, usage issues, etc.\n\n- Open Source Contribution Activities:\n\n  - Beginner: Happy Open Source Activity（[Regular Season](https://github.com/PaddlePaddle/Paddle/issues/56689)、[Pre-Hackathon Camp](https://github.com/PaddlePaddle/Paddle/issues/58497)）\n  - Advanced: PaddlePaddle Hackathon（[Personal Challenge Competition](https://github.com/PaddlePaddle/Paddle/issues/57262)、[LLM Application Competition](https://github.com/PaddlePaddle/Paddle/issues/57585)、[Hackathon Code Camp](https://github.com/PaddlePaddle/Paddle/issues/57264)）\n\n- Community Organizations:\n  - Technical Organization: [Paddle Framework Contributor Club, PFCC](https://github.com/PaddlePaddle/community/tree/master/pfcc)\n  - Community Governance Organization: [PaddlePaddle OpenSource Development Working Group, PPOSDWG](https://github.com/PaddlePaddle/community/tree/master/pposdwg)\n\n- Community Blog: <https://pfcc.blog/>\n\n## Courses\n\n- [Server Deployments](https://aistudio.baidu.com/aistudio/course/introduce/19084): Courses introducing high performance server deployments via local and remote services.\n- [Edge Deployments](https://aistudio.baidu.com/aistudio/course/introduce/22690): Courses introducing edge deployments from mobile, IoT to web and applets.\n\n## Copyright and License\n\nPaddlePaddle is provided under the [Apache-2.0 license](LICENSE).\n"
        },
        {
          "name": "README_cn.md",
          "type": "blob",
          "size": 6.109375,
          "content": "\n<p align=\"center\">\n<img align=\"center\" src=\"doc/imgs/logo.png\", width=1600>\n<p>\n\n--------------------------------------------------------------------------------\n\n[English](./README.md) | 简体中文 | [日本語](./README_ja.md)\n\n[![Documentation Status](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n[![Documentation Status](https://img.shields.io/badge/中文文档-最新-brightgreen.svg)](https://paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html)\n[![Release](https://img.shields.io/github/release/PaddlePaddle/Paddle.svg)](https://github.com/PaddlePaddle/Paddle/releases)\n[![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](LICENSE)\n\n欢迎来到 PaddlePaddle GitHub\n\n飞桨(PaddlePaddle)以百度多年的深度学习技术研究和业务应用为基础，是中国首个自主研发、功能完备、 开源开放的产业级深度学习平台，集深度学习核心训练和推理框架、基础模型库、端到端开发套件和丰富的工具组件于一体。目前，飞桨累计开发者1070万，服务企业23.5万家，基于飞桨开源深度学习平台产生了86万个模型。飞桨助力开发者快速实现AI想法，快速上线AI业务。帮助越来越多的行业完成AI赋能，实现产业智能化升级。\n\n## 安装\n\n### PaddlePaddle 最新版本: [v2.6](https://github.com/PaddlePaddle/Paddle/tree/release/2.6)\n\n跟进 PaddlePaddle 最新特性请参考我们的[版本说明](https://github.com/PaddlePaddle/Paddle/releases)\n\n### 安装最新稳定版本\n\n``` sh\n# CPU\npip install paddlepaddle\n# GPU\npip install paddlepaddle-gpu\n```\n\n更多安装信息详见官网 [安装说明](https://www.paddlepaddle.org.cn/install/quick)\n\nPaddlePaddle用户可领取**免费Tesla V100在线算力资源**，训练模型更高效。**每日登陆即送8小时**，[前往使用免费算力](https://aistudio.baidu.com/aistudio/index)。\n\n## 四大领先技术\n\n- **开发便捷的产业级深度学习框架**\n\n    飞桨深度学习框架采用基于编程逻辑的组网范式，对于普通开发者而言更容易上手，符合他们的开发习惯。同时支持声明式和命令式编程，兼具开发的灵活性和高性能。网络结构自动设计，模型效果超越人类专家。\n\n- **支持超大规模深度学习模型的训练**\n\n    飞桨突破了超大规模深度学习模型训练技术，实现了支持千亿特征、万亿参数、数百节点的开源大规模训练平台，攻克了超大规模深度学习模型的在线学习难题，实现了万亿规模参数模型的实时更新。\n    [查看详情](https://github.com/PaddlePaddle/Fleet)\n\n- **支持多端多平台的高性能推理部署工具**\n\n    飞桨不仅广泛兼容第三方开源框架训练的模型部署，并且为不同的场景的生产环境提供了完备的推理引擎，包括适用于高性能服务器及云端推理的原生推理库 [Paddle Inference](https://www.paddlepaddle.org.cn/inference/master/guides/introduction/index_intro.html)，全场景、易用灵活、极致高效的AI推理部署工具，支持云边端部署工具 [FastDeploy](https://github.com/PaddlePaddle/FastDeploy)，针对于移动端、物联网场景的轻量化推理引擎 [Paddle Lite](https://github.com/PaddlePaddle/Paddle-Lite)，以及在浏览器、小程序等环境下使用的前端推理引擎 [Paddle.js](https://www.paddlepaddle.org.cn/paddle/paddlejs)。同时，透过与不同场景下的主流硬件高度适配优化及异构计算的支持, 飞桨的推理性能也领先绝大部分的主流实现。\n\n- **面向产业应用，开源开放覆盖多领域的工业级模型库。**\n\n    飞桨官方支持100多个经过产业实践长期打磨的主流模型，其中包括在国际竞赛中夺得冠军的模型；同时开源开放200多个预训练模型，助力快速的产业应用。\n    [查看详情](https://github.com/PaddlePaddle/models)\n\n## 文档\n\n我们提供 [英文](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html) 和 [中文](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html) 文档\n\n- [使用指南](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html)：或许你想从深度学习基础开始学习飞桨\n\n- [应用实践](https://www.paddlepaddle.org.cn/documentation/docs/zh/tutorial/index_cn.html)：使用飞桨搭建你的模型，更高效的完成深度学习任务\n\n- [API 文档](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/index_cn.html)：新的 API 支持代码更少更简洁的程序\n\n- [贡献方式](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/08_contribution/index_cn.html)：参与飞桨社区开源贡献的指南\n\n## 开源社区\n\n- [Github Issues](https://github.com/PaddlePaddle/Paddle/issues)：提交安装/使用问题、报告bug、建议新特性、沟通开发计划等\n- 社区活动：\n\n  - 入门：快乐开源活动（[热身打卡 + 常规赛](https://github.com/PaddlePaddle/Paddle/issues/56689)、[启航计划](https://github.com/PaddlePaddle/Paddle/issues/58497)）\n  - 进阶：飞桨黑客马拉松（[开源贡献个人挑战赛](https://github.com/PaddlePaddle/Paddle/issues/57262)、[大模型应用与创意赛](https://github.com/PaddlePaddle/Paddle/issues/57585)、[飞桨护航计划集训营](https://github.com/PaddlePaddle/Paddle/issues/57264)）\n\n- 社区组织：\n  - 技术交流组织：[飞桨核心框架贡献者俱乐部 PFCC](https://github.com/PaddlePaddle/community/tree/master/pfcc)\n  - 社区治理组织：[飞桨社区开源发展工作组 PPOSDWG](https://github.com/PaddlePaddle/community/tree/master/pposdwg)\n\n- 社区博客：<https://pfcc.blog/>\n\n## 课程\n\n- [服务器部署](https://aistudio.baidu.com/aistudio/course/introduce/19084)：详细介绍高性能服务器端部署实操，包含本地端及服务化Serving部署等\n- [端侧部署](https://aistudio.baidu.com/aistudio/course/introduce/22690)：详细介绍端侧多场景部署实操，从移动端设备、IoT、网页到小程序部署\n\n## 版权和许可证\n\nPaddlePaddle由[Apache-2.0 license](LICENSE)提供\n"
        },
        {
          "name": "README_ja.md",
          "type": "blob",
          "size": 8.29296875,
          "content": "<p align=\"center\">\n<img align=\"center\" src=\"doc/imgs/logo.png\", width=1600>\n<p>\n\n--------------------------------------------------------------------------------\n\n[English](./README.md) | [简体中文](./README_cn.md) | 日本語\n\n[![Documentation Status](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n[![Documentation Status](https://img.shields.io/badge/中文文档-最新-brightgreen.svg)](https://paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html)\n[![Release](https://img.shields.io/github/release/PaddlePaddle/Paddle.svg)](https://github.com/PaddlePaddle/Paddle/releases)\n[![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](LICENSE)\n[![Twitter](https://img.shields.io/badge/Twitter-1ca0f1.svg?logo=twitter&logoColor=white)](https://twitter.com/PaddlePaddle_)\n\nPaddlePaddle GitHub へようこそ。\n\nPaddlePaddle は中国初の独立系 R&D ディープラーニングプラットフォームとして、2016年からプロのコミュニティに正式にオープンソース化されました。コアとなる深層学習フレームワーク、基本モデルライブラリ、エンドツーエンドの開発キット、ツール＆コンポーネント、さらにサービスプラットフォームを網羅する、高度な技術と豊富な機能を備えた産業プラットフォームです。\nPaddlePaddle は、工業化に対するコミットメントを持つ工業的実践から生まれたものです。製造業、農業、企業サービスなど幅広い分野で採用され、1070万人以上の開発者、23.5万以上の企業、86万以上のモデルを生み出しています。それにより PaddlePaddle は、ますます多くのパートナーの AI 商用化を支援しています。\n\n## インストール\n\n### PaddlePaddle の最新リリース: [v2.6](https://github.com/PaddlePaddle/Paddle/tree/release/2.6)\n\n私たちのビジョンは、PaddlePaddle を通じて、誰もが深層学習を行えるようにすることです。\nPaddlePaddle の最新機能を追跡するために、私たちの[リリースのお知らせ](https://github.com/PaddlePaddle/Paddle/releases)を参照してください。\n\n### 最新の安定版リリースのインストール\n\n``` sh\n# CPU\npip install paddlepaddle\n# GPU\npip install paddlepaddle-gpu\n```\n\nインストール方法については、[クイックインストール](https://www.paddlepaddle.org.cn/install/quick)をご覧ください\n\nこの度、開発者の皆様が Tesla V100 のオンライン計算資源を無償で取得できるようになりました。AI Studio でプログラムを作成した場合、1日あたり8時間のオンライン学習が可能です。[スタートはこちら](https://aistudio.baidu.com/aistudio/index)。\n\n## 四大技術\n\n- **ディープニューラルネットワークの産業用開発のためのアジャイルフレームワーク**\n\n    PaddlePaddle ディープラーニングフレームワークは、ニューラルネットワークをアーキテクトするプログラマブルスキームを活用することで、技術的負担を軽減しながら開発を容易にする。宣言型プログラミングと命令型プログラミングの両方をサポートし、開発の柔軟性と高い実行性能を両立しています。 ニューラル・アーキテクチャは、アルゴリズムによって自動的に設計され、人間の専門家が設計したものよりも優れた性能を発揮する可能性があります。\n\n- **ディープニューラルネットワークの超大規模学習をサポート**\n\n    PaddlePaddle は、超大規模なディープニューラルネットワークのトレーニングでブレークスルーを起こしました。数百のノードに分散したデータソースを用いて、1000億の特徴量と数兆のパラメータを持つディープネットワークのトレーニングをサポートする、世界初の大規模オープンソース・トレーニング・プラットフォームを立ち上げたのです。PaddlePaddle は、超大規模ディープラーニングモデルのオンラインディープラーニングの課題を克服し、さらに1兆以上のパラメータでリアルタイムにモデル更新を実現しました。\n     [詳しくはこちら](https://github.com/PaddlePaddle/Fleet)\n\n- **総合的な展開環境に対応した高性能推論エンジン**\n\n   PaddlePaddle は、サードパーティのオープンソースフレームワークで学習されたモデルとの互換性があるだけでなく、様々な生産シナリオに対応した完全な推論エンジン、システム、スイートを提供しています。当社の推論エンジン、システム、スイートには、[Paddle Inference](https://www.paddlepaddle.org.cn/inference/master/guides/introduction/index_intro.html) があります：高性能なサーバーおよびクラウド推論用のネイティブ推論ライブラリ; [FastDeploy](https://github.com/PaddlePaddle/FastDeploy): オールシナリオで使いやすく、柔軟で非常に効率的なAI推論デプロイツールです; [Paddle Lite](https://github.com/PaddlePaddle/Paddle-Lite)： モバイルや IoT 環境向けの超軽量推論エンジン; [Paddle.js](https://www.paddlepaddle.org.cn/paddle/paddlejs)： ブラウザやミニアプリのためのフロントエンド推論エンジンです。さらに、各シナリオの主要なハードウェアに最適化することで、Paddle の推論エンジンは他の主流フレームワークのほとんどを凌駕しています。\n\n- **オープンソースリポジトリによる業界指向のモデルやライブラリ**\n\n     PaddlePaddle は、業界で長い間実践され、磨かれてきた100以上の主流モデルを含み、維持しています。これらのモデルの中には、主要な国際コンペティションで主要な賞を受賞したものもあります。一方、PaddlePaddle は、産業用アプリケーションの迅速な開発を促進するために、200以上のプレトレーニングモデル（そのうちのいくつかはソースコード付き）をさらに整備しています。\n     [詳しくはこちら](https://github.com/PaddlePaddle/models)\n\n## ドキュメント\n\n[英語](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)と\n[中国語](https://www.paddlepaddle.org.cn/documentation/docs/zh/guide/index_cn.html)のドキュメントを提供しています。\n\n- [ガイド](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n\n  PaddlePaddle でディープラーニングの基本を実装する方法から始めてみてはいかがでしょうか。\n\n- [プラクティス](https://www.paddlepaddle.org.cn/documentation/docs/zh/tutorial/index_cn.html)\n\n  Paddle を使ってモデルを構築し、ディープラーニングタスクをより効率的に実行しましょう。\n\n- [API リファレンス](https://www.paddlepaddle.org.cn/documentation/docs/en/api/index_en.html)\n\n   新しい API により、より短時間のプログラムが可能となりました。\n\n- [コントリビュート方法](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/08_contribution/index_en.html)\n\n   皆様のご投稿に感謝いたします！\n\n## コミュニケーション\n\n- [Github Issues](https://github.com/PaddlePaddle/Paddle/issues): バグレポート、機能リクエスト、インストールに関する問題、使用方法に関する問題など。\n- QQディスカッショングループ: 441226485 (PaddlePaddle)です。\n- [フォーラム](https://aistudio.baidu.com/paddle/forum): 実装や研究などについて話し合います。\n\n## コース\n\n- [Server Deployments](https://aistudio.baidu.com/aistudio/course/introduce/19084): ローカルサービスやリモートサービスを利用した高性能なサーバー展開を紹介するコースです。\n- [Edge Deployments](https://aistudio.baidu.com/aistudio/course/introduce/22690): モバイル、IoT から Web、アプレットまで、エッジの展開を紹介するコース。\n\n## Copyright とライセンス\n\nPaddlePaddle は [Apache-2.0 license](LICENSE) の下で提供されています。\n"
        },
        {
          "name": "RELEASE.md",
          "type": "blob",
          "size": 0.1025390625,
          "content": "# Release Note\n\nPlease turn to [here](https://github.com/PaddlePaddle/Paddle/releases) for release note.\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 4.8310546875,
          "content": "# Using PaddlePaddle Securely\n\nThis document describes model security and code security in PaddlePaddle. It also provides guidelines on how to report vulnerabilities in PaddlePaddle.\n\n## PaddlePaddle Model Security\n\nPaddlePaddle attaches great importance to security and privacy of model. This includes how to prevent the model from outputting wrong decision results under the interference when it is used in security-related and safety-critical scenarios, and how to avoid leaking data and privacy information from the model itself, the model gradient or the model inference results.\n\n\n\n[PaddleSleeve](https://github.com/PaddlePaddle/PaddleSleeve) provides a series of security and privacy tools, which can help model developers and users systematically evaluate and improve the model security and privacy in both development and deployment stages.\n\n\n\nThese tools include adversarial example evaluation test, pseudo-natural environment robustness evaluation test, model reversing evaluation test, member inference evaluation test, sample denoising, adversarial training, privacy enhancement optimizer, etc.\n\n### Running untrusted models\n\nAlways load and execute untrusted models inside a sandbox and be sure to know the security impacts.\nThere are several ways in which a model could become untrusted. PaddlePaddle has enough features to impact on the system. (e.g. `paddle.load` uses [pickle](https://docs.python.org/3/library/pickle.html) implicitly, which may cause malformed models to achieve arbitrary code execution). So we recommend when using the untrusted models, you need to carefully audit it and run PaddlePaddle inside a sandbox.\n\n## PaddlePaddle Code Security\n\nPaddlePaddle always take code security seriously. However, due to the complexity of the framework and its dependence on other thirdparty open source libraries, there may still be some security issues undetected. Therefore, we hope that more security researchers and PaddlePaddle developers can participate in the code security program. We encourage responsible disclosure of security issues, as well as contributing code to improve our vulnerability finding tools to make PaddlePaddle safer.\n\n### Code security tools\n\nPaddlePaddle security team attaches great importance to the security of the framework. In order to find and fix security issues as soon as possible, we are continuously conducting code security audit and developing automatic vunerability discovery tools. We have already open sourced some of them to the community, hoping this could encourage people to contribute and improve the safety and robustness of PaddlePaddle. [This tool](https://github.com/PaddlePaddle/PaddleSleeve/tree/main/CodeSecurity) includes two parts. The dynamic part includes some op fuzzer samples. And the static part includes some CodeQL samples. Both of them are aim to find vulnerabilities in PaddlePaddle framework codebase. By referring the samples, security researchers can write their own fuzzers or QLs to test more PaddlePaddle modules, and find more code security issues.\n\n### Reporting vulnerabilities\n\nWe encourage responsible disclosure of security issues to PaddlePaddle and please email reports about any security issues you find to paddle-security@baidu.com.\n\n\n\nAfter the security team receives your email, they will communicate with you in time. The security team will work to keep you informed of an issue fix.\n\n\n\nIn order to reproduce and identify the issue, please include the following information along with your email:\n\n- The details of the vulnerability including how to reproduce it. Try to attach a PoC.\n- The attack scenario and what an attacker might be able to achieve with this issue.\n- Whether this vulnerability has been made public. If it is, please attach details.\n- Your name and affiliation.\n\nWe will indicate the bug fix in the release of PaddlePaddle, and publish the vulnerability detail and the reporter in the security advisories (Your name will not be published if you choose to remain anonymous).\n\n### What is a vulnerability?\n\nIn the process of computation graphs in PaddlePaddle, models can perform arbitrary computations , including reading and writing files, communicating with the network, etc. It may cause memory exhaustion, deadlock, etc., which will lead to unexpected behavior of PaddlePaddle. We consider these behavior to be security vulnerabilities only if they are out of the intention of the operation involved.\n\n\n\nSome unexpected parameters and behaviors have been checked in PaddlePaddle by throwing exceptions in Python or return error states in C++. In these cases, denial of service is still possible, but the exit of the PaddlePaddle is clean. Since the error handling of PaddlePaddle is expected and correct, these cases are not security vulnerabilities.\n\n\n\nIf malicious input can trigger memory corruption or non-clean exit, such bug is considered a security problem.\n\n\n\n[security advisories](./security/README.md)\n"
        },
        {
          "name": "SECURITY_cn.md",
          "type": "blob",
          "size": 4.0859375,
          "content": "# 安全使用飞桨\n\n\n\n本文将对飞桨模型及代码安全进行介绍，并介绍如何向飞桨提报漏洞。\n\n## 飞桨模型安全\n\n飞桨关注模型的安全性和隐私性。其中包括当模型被用于安全攸关场景时，如何避免模型在干扰下输出错误的决策结果，以及如何避免从模型本身、模型梯度或模型推理结果中泄露数据和隐私信息。\n\n飞桨的安全和隐私套件[PaddleSleeve](https://github.com/PaddlePaddle/PaddleSleeve)提供了一系列工具，可帮助模型开发者及使用者在模型的开发或部署阶段，系统性地评估并提升模型的安全性和隐私性。这些工具包括对抗样本评估测试、拟自然环境鲁棒性评估测试、模型逆向评估测试、成员推断评估测试、样本去噪、对抗训练、隐私增强优化器等。\n\n### 运行非信任模型\n\n请永远在沙箱中加载和运行非信任模型并了解其可能造成的影响。\n有多种方式可能导致模型不受信任。飞桨的功能足以在加载不受信任的模型时对系统造成影响，如：`paddle.load` 使用了[pickle](https://docs.python.org/3/library/pickle.html)，这会导致恶意模型执行任意命令。所以在使用非信任模型时需要仔细地审计模型，并在沙箱中运行来确保安全。\n\n## 飞桨代码安全\n\n飞桨团队一向非常重视代码安全，但鉴于飞桨框架的实现非常复杂，并且依赖了多个第三方开源库，其中仍可能会存在未被发现的问题。因此，我们希望有更多安全研究人员、飞桨开发者能参与到飞桨代码安全保障项目中来，我们鼓励向飞桨负责任的披露(Responsible Disclosure)安全问题，也鼓励向飞桨贡献代码完善动静态漏洞挖掘工具，让飞桨变得更安全。\n\n### 安全工具\n\n飞桨安全团队对于飞桨框架自身的安全高度重视，为了尽快地发现和修复安全问题，我们内部在持续地进行代码安全审计和研发自动化漏洞挖掘工具。我们将一些工具和方法开源给社区，希望能抛砖引玉，大家一起来贡献提高飞桨的安全性和鲁棒性。工具开源见[CodeSecurity](https://github.com/PaddlePaddle/PaddleSleeve/tree/main/CodeSecurity)。该开源工具包含两部分内容，分别从动态（模糊测试）和静态（CodeQL）两个角度对飞桨代码进行安全审计和漏洞挖掘。通过参照和添加新的测试模块，可以帮助覆盖更多飞桨代码模块，发现更多的代码安全问题。\n\n### 报告安全问题\n\n我们鼓励向飞桨负责任地披露安全问题，请将所发现的安全问题发送电子邮件到 paddle-security@baidu.com。\n\n在安全团队收到邮件后将会及时与您沟通并反馈问题修复进度。\n\n为了更好地复现和认定问题情况，请在邮件中：\n\n- 详细描述漏洞细节，如何复现，并尽量附上PoC。\n- 描述攻击场景，介绍攻击者可能由此问题所能达到的效果。\n- 该问题是否已公开并描述情况。\n- 署名您的姓名和从属关系。\n\n我们会将漏洞修复情况注明在飞桨的发布当中，并在致谢公告中发布漏洞情况和提报人（如果您选择不公开署名将不会发布提报人信息）。\n\n### 安全问题认定说明\n\n飞桨在计算图的过程中，由于模型可以执行任何计算，操作文件，进行网络通信等功能，可能造成内存耗尽，死锁等情况发生，这将导致飞桨产生一些非预期的行为。我们认为只有当这些行为超出了所涉及的操作意图时才算作是安全问题。\n\n飞桨框架代码中对于一些非预期的参数和行为会进行检查，Python代码中以抛出异常为形式，C++代码中以返回错误状态为形式。这些情况下，飞桨代码的退出是干净的，但仍可能会因此造成拒绝服务，然而由于飞桨的处理是预期且正确的，所以造成这些情况并不算作是安全问题。\n\n如果输入非预期的参数后，对飞桨代码造成了内存破坏，或者非干净退出，这类行为被认定为存在安全问题。\n\n### [安全公告](./security/README_cn.md)\n"
        },
        {
          "name": "SECURITY_ja.md",
          "type": "blob",
          "size": 6.51171875,
          "content": "# PaddlePaddle を安全に使用する\n\nこのドキュメントでは、PaddlePaddle のモデルセキュリティとコードセキュリティについて説明します。また、PaddlePaddle の脆弱性を報告する方法のガイドラインも提供します。\n\n## PaddlePaddle モデルのセキュリティ\n\nPaddlePaddle はモデルのセキュリティとプライバシーを重要視しています。これには、モデルがセキュリティ関連やセーフティクリティカルなシナリオで使用される際に、干渉下で誤った判断結果を出力することを防ぐ方法や、モデル自身、モデルの勾配、モデルの推論結果からデータやプライバシー情報が漏れることを避ける方法が含まれます。\n\n\n\n[PaddleSleeve](https://github.com/PaddlePaddle/PaddleSleeve) は、一連のセキュリティとプライバシーのツールを提供します。これは、モデルの開発者とユーザが、開発と展開の両方の段階で、モデルのセキュリティとプライバシーを体系的に評価し、改善するのに役立ちます。\n\n\n\nこれらのツールには、敵対的事例評価テスト、擬似自然環境頑健性評価テスト、モデル反転評価テスト、メンバー推論評価テスト、サンプルノイズ除去、敵対的トレーニング、プライバシー強化オプティマイザなどが含まれます。\n\n### 信頼できないモデルの実行\n\n信頼されていないモデルを常にサンドボックスの中にロードして実行し、セキュリティへの影響を必ず知っておいてください。\nモデルが信頼されなくなる方法はいくつかあります。PaddlePaddle はシステムに影響を与えるのに十分な機能を持っています。(例えば、`paddle.load` は暗黙的に [pickle](https://docs.python.org/3/library/pickle.html) を使用します。これは、不正なモデルが任意のコードを実行する原因となるかもしれません)。そのため、信頼できないモデルを使用する場合は、慎重に監査し、サンドボックス内で PaddlePaddle を実行することを推奨します。\n\n## PaddlePaddle コードセキュリティ\n\nPaddlePaddle は常にコードセキュリティに真剣に取り組んでいます。しかし、フレームワークの複雑さと他のサードパーティのオープンソースライブラリへの依存のため、まだ発見されていないセキュリティ問題があるかもしれません。そのため、より多くのセキュリティ研究者と PaddlePaddle 開発者がコードセキュリティプログラムに参加することを望みます。PaddlePaddle をより安全なものにするために、セキュリティ問題の責任ある開示と、脆弱性発見ツールの改善のためのコード貢献を奨励します。\n\n### コードセキュリティツール\n\nPaddlePaddle セキュリティチームはフレームワークのセキュリティを非常に重視しています。できるだけ早くセキュリティ問題を発見し修正するために、我々は継続的にコードセキュリティ監査を行い、自動脆弱性発見ツールを開発しています。我々はすでにその一部をコミュニティにオープンソース化しており、これにより人々がコントリビュートし、PaddlePaddle の安全性と堅牢性が向上することを期待しています。[このツール](https://github.com/PaddlePaddle/PaddleSleeve/tree/main/CodeSecurity)には 2 つの部分があります。動的な部分には、いくつかのopファザーサンプルが含まれています。そして静的な部分には CodeQL のサンプルが含まれています。これらは両方とも、PaddlePaddle フレームワークのコードベースの脆弱性を発見することを目的としています。サンプルを参照することで、セキュリティ研究者は、より多くの PaddlePaddle モジュールをテストし、より多くのコードセキュリティ問題を発見するために、独自のファザーや QL を書くことができます。\n\n### 脆弱性の報告\n\n私たちは、PaddlePaddle にセキュリティ問題を責任を持って開示することを推奨します。あなたが見つけたセキュリティ問題については、paddle-security@baidu.com までメールで報告してください。\n\n\n\nセキュリティチームがメールを受信した後、時間内にご連絡を差し上げます。セキュリティーチームは、問題の修正についてお知らせするよう努めます。\n\n\n\n問題を再現して特定するために、以下の情報を電子メールに添付してください:\n\n- 再現方法を含む脆弱性の詳細。PoC を添付してください。\n- 攻撃シナリオと、攻撃者がこの問題で達成できるかもしれないこと。\n- この脆弱性が公表されているかどうか。公表されている場合は、その詳細を添付してください。\n- あなたの名前と所属。\n\nバグフィックスは PaddlePaddle のリリースに記載し、脆弱性の詳細と報告者をセキュリティ勧告で公表します（匿名を選択した場合、あなたの名前は公表されません）。\n\n### 脆弱性とは？\n\nPaddlePaddle の計算グラフの過程で、モデルはファイルの読み書き、ネットワークとの通信などを含む任意の計算を行うことができます。これはメモリ枯渇やデッドロックなどを引き起こす可能性があり、PaddlePaddle の予期せぬ動作につながります。私たちは、これらの振る舞いが、関係する操作の意図から外れている場合にのみ、セキュリティの脆弱性とみなします。\n\n\n\nいくつかの予期しないパラメータや動作は、Pythonでは例外を投げることで、C++ではエラー状態を返すことで、PaddlePaddle でチェックされています。このような場合でもサービス拒否の可能性はありますが、PaddlePaddle の終了はクリーンです。PaddlePaddle のエラー処理は予期されたものであり正しいので、これらのケースはセキュリティ脆弱性ではありません。\n\n\n\n悪意のある入力がメモリ破壊やクリーンでない終了の引き金となる場合、そのようなバグはセキュリティ上の問題とみなされます。\n\n\n\n[セキュリティ勧告](./security/README_ja.md)\n"
        },
        {
          "name": "_typos.toml",
          "type": "blob",
          "size": 7.5419921875,
          "content": "[files]\n# The following files will be excluded from spell check during commits\nextend-exclude = [\n    \"third_party\",\n    \"patches\",\n    \"build\",\n    # Skip `intermidiate` check in these files\n    \"test/cpp/eager/task_tests/CMakeLists.txt\",\n    \"test/cpp/eager/task_tests/hook_test_intermidiate.cc\",\n    # Skip `creater` check in these files\n    \"paddle/fluid/inference/tensorrt/convert/CMakeLists.txt\",\n    \"paddle/fluid/inference/tensorrt/convert/generic_and_custom_plugin_creater.cc\",\n    \"paddle/fluid/inference/tensorrt/convert/test_custom_plugin_creater.cc\",\n]\n\n[default]\n# Ignore 1-3 letter words, refer to https://github.com/crate-ci/typos/issues/1079\nextend-ignore-words-re = [\"^[a-zA-Z]{1,3}$\"]\n# refer to https://github.com/crate-ci/typos/blob/master/docs/reference.md#example-configurations\nextend-ignore-re = [\n    # Ignore lines by `# typos: disable-line`\n    \"(?Rm)^.*(#|//)\\\\s*typos:\\\\s*disable-line$\",\n    # Ignore block by `# typos: off` and `# typos: on`\n    \"(?s)(#|//)\\\\s*typos:\\\\s*off.*?\\\\n\\\\s*(#|//)\\\\s*typos:\\\\s*on\"\n]\n\n[default.extend-words]\n# PaddlePaddle specific words\narange = \"arange\"\nastroid = 'astroid'\ncacl = 'cacl'\nCANN = 'CANN'\nClas = 'Clas'\nclen = 'clen'\ndatas = 'datas'\ndota = 'dota'\ndout = \"dout\"\neles = 'eles'\nentrys = 'entrys'\nfeeded = 'feeded'\ngrad = \"grad\"\nHalfs = 'Halfs'\nkinf = 'kinf'\npash = 'pash'\nunpacket = \"unpacket\"\n\n# These words need to be fixed\nIndexs = 'Indexs'\nindexs = 'indexs'\nInfered = 'Infered'\ninfered = 'infered'\ninitilized = 'initilized'\ninitalized = 'initalized'\ninitalize = 'initalize'\nintialize = 'intialize'\ninital = 'inital'\nInouts = 'Inouts'\nintputs = 'intputs'\ninputed = 'inputed'\nintput = 'intput'\nIntput = 'Intput'\ninser = 'inser'\ninsid = 'insid'\ninsepection = 'insepection'\nintall = 'intall'\ninstanciate = 'instanciate'\nOperants = 'Operants'\noperants = 'operants'\noptin = 'optin'\nOptin = 'Optin'\nrder = 'rder'\noreder = 'oreder'\norignal = 'orignal'\norginal = 'orginal'\nonces = 'onces'\noutter = 'outter'\noutpus = 'outpus'\noutout = 'outout'\nouput = 'ouput'\noutpout = 'outpout'\nouptut = 'ouptut'\nOuput = 'Ouput'\noverriden = 'overriden'\nOveride = 'Overide'\noveride = 'overide'\noverrided = 'overrided'\nPackge = 'Packge'\npacakage = 'pacakage'\npadd = 'padd'\npayed = 'payed'\nparellel = 'parellel'\nparm = 'parm'\nParm = 'Parm'\nPARM = 'PARM'\nparamters = 'paramters'\nParamters = 'Paramters'\nparamter = 'paramter'\nParamater = 'Paramater'\nparamete = 'paramete'\nparmeter = 'parmeter'\nparemeter = 'paremeter'\nparrent = 'parrent'\nparital = 'parital'\npartitial = 'partitial'\nPartitial = 'Partitial'\nPartion = 'Partion'\npartion = 'partion'\npatition = 'patition'\npasss = 'passs'\nPasss = 'Passs'\npathes = 'pathes'\npatten = 'patten'\nPatten = 'Patten'\npattens = 'pattens'\nprecent = 'precent'\nperformace = 'performace'\nperfomed = 'perfomed'\nperfome = 'perfome'\npresistable = 'presistable'\npyhsical = 'pyhsical'\npipline = 'pipline'\npalce = 'palce'\nPlacment = 'Placment'\npleace = 'pleace'\nPoniter = 'Poniter'\npoped = 'poped'\npositon = 'positon'\nPOSTION = 'POSTION'\npostive = 'postive'\npossiable = 'possiable'\npotentialy = 'potentialy'\nprecending = 'precending'\nPrepration = 'Prepration'\nPrepar = 'Prepar'\npreprocesser = 'preprocesser'\npriorites = 'priorites'\nprobabalistic = 'probabalistic'\nprocesser = 'processer'\nproccess = 'proccess'\nproducted = 'producted'\nprogam = 'progam'\nprogrss = 'progrss'\npropogated = 'propogated'\nPropogation = 'Propogation'\nprotocal = 'protocal'\nPROTOCAL = 'PROTOCAL'\npyrhon = 'pyrhon'\npthon = 'pthon'\nRefered = 'Refered'\nrefered = 'refered'\nregisted = 'registed'\nregist = 'regist'\nRegist = 'Regist'\nRegiste = 'Registe'\nregiste = 'registe'\nREGIST = 'REGIST'\nRegiter = 'Regiter'\nreleated = 'releated'\nrealease = 'realease'\nrelase = 'relase'\nreomve = 'reomve'\nReoder = 'Reoder'\nrepeatly = 'repeatly'\nrepeate = 'repeate'\nrepalce = 'repalce'\nrepresention = 'represention'\nrequied = 'requied'\nReqiured = 'Reqiured'\nRequred = 'Requred'\nrequirments = 'requirments'\nReseting = 'Reseting'\nreseted = 'reseted'\nresouce = 'resouce'\nretore = 'retore'\nrewriten = 'rewriten'\nrewrited = 'rewrited'\nRuning = 'Runing'\nruning = 'runing'\nSMAE = 'SMAE'\nsatifies = 'satifies'\nsclar = 'sclar'\nsacle = 'sacle'\nschduler = 'schduler'\nsheduler = 'sheduler'\nshedule = 'shedule'\nscheule = 'scheule'\nserach = 'serach'\nseconde = 'seconde'\nSeleceted = 'Seleceted'\nsence = 'sence'\nseperately = 'seperately'\nseperate = 'seperate'\nsepearate = 'sepearate'\nseperating = 'seperating'\nseperator = 'seperator'\nsequnce = 'sequnce'\nseqence = 'seqence'\nsequece = 'sequece'\nsequnece = 'sequnece'\nsequentail = 'sequentail'\nserailize = 'serailize'\nsettting = 'settting'\nsetted = 'setted'\nshoule = 'shoule'\nshoud = 'shoud'\nSingal = 'Singal'\nSimiliar = 'Similiar'\nsimular = 'simular'\nSimle = 'Simle'\nsignle = 'signle'\nSkiped = 'Skiped'\nskiped = 'skiped'\nsmll = 'smll'\nsamll = 'samll'\nsomme = 'somme'\npatial = 'patial'\nPatial = 'Patial'\nspecificed = 'specificed'\nsplite = 'splite'\nspliter = 'spliter'\nspliting = 'spliting'\nSpliting = 'Spliting'\nsplited = 'splited'\nsplitted = 'splitted'\nSplited = 'Splited'\nsqaure = 'sqaure'\nsequeze = 'sequeze'\nstarup = 'starup'\nstatment = 'statment'\nstaticly = 'staticly'\nstaticaly = 'staticaly'\nStati = 'Stati'\nSTOPED = 'STOPED'\nStoped = 'Stoped'\nstoped = 'stoped'\nstoreage = 'storeage'\nsotring = 'sotring'\nstragety = 'stragety'\nstrem = 'strem'\nstructed = 'structed'\nsturcture = 'sturcture'\nsubsituted = 'subsituted'\nsubsitute = 'subsitute'\nsubstitude = 'substitude'\nsubstitue = 'substitue'\nSubsitute = 'Subsitute'\nSubstitude = 'Substitude'\nsubstract = 'substract'\nSubstract = 'Substract'\nsuccessed = 'successed'\nsucessfully = 'sucessfully'\nSucceess = 'Succeess'\nSuger = 'Suger'\nsupportted = 'supportted'\nsupoort = 'supoort'\nSupprot = 'Supprot'\nsuport = 'suport'\nsuppport = 'suppport'\nSWTICH = 'SWTICH'\nSwith = 'Swith'\nsysyem = 'sysyem'\ntenosr = 'tenosr'\niterm = 'iterm'\ntermiante = 'termiante'\nTheoritical = 'Theoritical'\nther = 'ther'\nthge = 'thge'\nthouse = 'thouse'\ntheads = 'theads'\nthrads = 'thrads'\nthre = 'thre'\nTHREAHOLD = 'THREAHOLD'\nTHORW = 'THORW'\ntimout = 'timout'\ntiemout = 'tiemout'\nTOOD = 'TOOD'\ntood = 'tood'\nTDOD = 'TDOD'\ntoghether = 'toghether'\ntrainning = 'trainning'\nTraning = 'Traning'\ntransforme = 'transforme'\ntransfered = 'transfered'\ntransfering = 'transfering'\ntranfers = 'tranfers'\ntranfer = 'tranfer'\nTranfer = 'Tranfer'\ntransfrom = 'transfrom'\ntranform = 'tranform'\nTranpose = 'Tranpose'\ntranpose = 'tranpose'\ntigger = 'tigger'\ntrimed = 'trimed'\ntrival = 'trival'\nTrye = 'Trye'\nTring = 'Tring'\ntring = 'tring'\ntunning = 'tunning'\nTYPLE = 'TYPLE'\ntrun = 'trun'\ntyep = 'tyep'\ntpye = 'tpye'\nunchangable = 'unchangable'\nUndefind = 'Undefind'\nunser = 'unser'\nUNEXPECT = 'UNEXPECT'\nUnifrom = 'Unifrom'\nuninitialzed = 'uninitialzed'\nUniqe = 'Uniqe'\nunqiue = 'unqiue'\nuniqe = 'uniqe'\nunkown = 'unkown'\nunkonwn = 'unkonwn'\nUnkown = 'Unkown'\nunsupport = 'unsupport'\nupsupported = 'upsupported'\nUnsupport = 'Unsupport'\nUNSUPPORT = 'UNSUPPORT'\nunziped = 'unziped'\nudpated = 'udpated'\nupgarde = 'upgarde'\nuptream = 'uptream'\nunsed = 'unsed'\nuesd = 'uesd'\nusefull = 'usefull'\nusless = 'usless'\nvaccum = 'vaccum'\nvalud = 'valud'\nVAILD = 'VAILD'\nvalus = 'valus'\nvaluse = 'valuse'\nVarible = 'Varible'\nvaraible = 'varaible'\nvecotr = 'vecotr'\nverson = 'verson'\nvesion = 'vesion'\nVetical = 'Vetical'\nvunerability = 'vunerability'\nvarn = 'varn'\nwarpped = 'warpped'\nwarpper = 'warpper'\nWarpper = 'Warpper'\nwheather = 'wheather'\nwether = 'wether'\nWether = 'Wether'\nwieghts = 'wieghts'\nwerid = 'werid'\nWheter = 'Wheter'\nwhther = 'whther'\nwhill = 'whill'\nwhos = 'whos'\nwiil = 'wiil'\nwitk = 'witk'\nworke = 'worke'\nworkround = 'workround'\nworksapce = 'worksapce'\nwrappered = 'wrappered'\nwraper = 'wraper'\nwraping = 'wraping'\nWritter = 'Writter'\nwrited = 'writed'\n"
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "paddle",
          "type": "tree",
          "content": null
        },
        {
          "name": "patches",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 4.212890625,
          "content": "[tool.black]\nline-length = 80\nskip-string-normalization = true\ntarget-version = [\"py38\", \"py39\", \"py310\", \"py311\", \"py312\"]\nextend-exclude = '''\n(\n    third_party/.+      # Exclude third_party directory\n    | build/.+          # Exclude build directory\n)\n'''\n\n[tool.ruff]\nexclude = [\n    \"./build\",\n    \"third_party\",\n    \"./python/paddle/utils/gast/**\",\n]\nline-length = 80\ntarget-version = \"py38\"\n\n[tool.ruff.format]\n# Prevent change to double quotes by some users use ruff format\nquote-style = \"preserve\"\n\n[tool.ruff.lint]\nselect = [\n    # Pycodestyle\n    \"E\",\n    \"W\",\n\n    # Pyflakes\n    \"F\",\n\n    # Isort\n    \"I\",\n\n    # Comprehensions\n    \"C4\",\n\n    # Debugger\n    \"T100\",\n\n    # Pyupgrade\n    \"UP\",\n\n    # Flake8-pyi\n    \"PYI\",\n\n    # NumPy-specific rules\n    \"NPY001\",\n    \"NPY003\",\n    \"NPY201\",\n\n    # Bugbear\n    \"B002\",\n    \"B003\",\n    \"B004\",\n    \"B009\",\n    \"B010\",\n    \"B011\",\n    \"B012\",\n    \"B013\",\n    \"B014\",\n    \"B015\",\n    \"B016\",\n    \"B017\",\n    \"B018\",\n    \"B019\",\n    \"B020\",\n    \"B021\",\n    \"B022\",\n    \"B025\",\n    \"B029\",\n    \"B032\",\n\n    # Pylint\n    \"PLE\",\n    \"PLC3002\",\n    \"PLR0206\",\n    \"PLR0402\",\n    \"PLR1711\",\n    \"PLR1722\",\n    \"PLW3301\",\n\n    # Flake8-simplify\n    \"SIM101\",\n\n    # Pygrep-hooks\n    \"PGH004\",\n\n    # Flake8-type-checking\n    \"TCH\",\n\n    # Ruff-specific rules\n    \"RUF005\",\n    \"RUF008\",\n    \"RUF009\",\n    \"RUF010\",\n    \"RUF013\",\n    \"RUF015\",\n    \"RUF016\",\n    \"RUF017\",\n    \"RUF018\",\n    \"RUF019\",\n    \"RUF020\",\n    \"RUF024\",\n    \"RUF026\",\n    \"RUF100\",\n\n    # Flake8-raise\n    \"RSE\",\n\n    # Flake8-quotes\n    \"Q003\",\n    \"Q004\",\n\n    # Refurb\n    \"FURB\",\n\n    # Flake8-future-annotations\n    \"FA\",\n]\nunfixable = [\n    \"NPY001\"\n]\nignore = [\n    # Whitespace before ‘,’, ‘;’, or ‘:’, it is not compatible with black\n    \"E203\",\n    # Module level import not at top of file\n    \"E402\",\n    # Line too long (82 > 79 characters)\n    \"E501\",\n    # Do not compare types, use `isinstance()`\n    \"E721\",\n    # Do not use bare except, specify exception instead\n    \"E722\",\n    # Do not assign a lambda expression, use a def\n    \"E731\",\n    # Do not use variables named ‘l’, ‘O’, or ‘I’\n    \"E741\",\n    # `name` may be undefined, or defined from star imports: `module`\n    \"F405\",\n    # Local variable name is assigned to but never used\n    \"F841\",\n    # It not met the \"Explicit is better than implicit\" rule\n    \"UP015\",\n    # It will cause the performance regression on python3.10\n    \"UP038\",\n    # collections.namedtuple can be quickly created a inlined class\n    \"PYI024\",\n    # `__all__.append` is a common pattern in Paddle\n    \"PYI056\",\n]\n\n[tool.ruff.lint.isort]\ncombine-as-imports = true\nknown-first-party = [\"paddle\"]\n\n[tool.ruff.lint.per-file-ignores]\n# Ignore for re-export in __init__ files\n\"__init__.py\" = [\"PLC0414\"]\n# Ignore compare with True in sot unittest\n\"test/sot/test_dup_top.py\" = [\"E712\"]\n# Ignore undefined variables in CMake config and some dygraph_to_static tests\n\".cmake-format.py\" = [\"F821\"]\n\"test/dygraph_to_static/test_closure_analysis.py\" = [\"F821\"]\n# Ignore version check in setup.py\n\"setup.py\" = [\"UP036\"]\n# Ignore unnecessary comprehension in dy2st unittest test_loop\n\"test/dygraph_to_static/test_loop.py\" = [\"C416\", \"F821\"]\n# Ignore unnecessary lambda in dy2st unittest test_lambda\n\"test/dygraph_to_static/test_lambda.py\" = [\"PLC3002\"]\n# Ignore docstring in tensor.pyi\n\"python/paddle/tensor/tensor.prototype.pyi\" = [\"PYI021\", \"PYI048\"]\n# Temproray ignore some dy2st test case because SOT bug\n# See https://github.com/PaddlePaddle/Paddle/pull/67344#discussion_r1714155671 for more details\n\"test/dygraph_to_static/seq2seq_dygraph_model.py\" = [\"RUF005\"]\n\n[tool.mypy]\npython_version = \"3.8\"\ncache_dir = \".mypy_cache\"\n# Miscellaneous strictness flags\nallow_redefinition = true\nlocal_partial_types = true\nstrict = false\n# Untyped definitions and calls\ncheck_untyped_defs = true\n# Import discovery\nfollow_imports = \"normal\"\n# Miscellaneous\nwarn_unused_configs = true\n# Disallow generic without type arguments\ndisallow_any_generics = true\n# Configuring warnings\nwarn_redundant_casts = true\nwarn_unused_ignores = true\nwarn_no_return = true\n# Configuring error messages\nshow_column_numbers = true\n\n[[tool.mypy.overrides]]\nmodule = [\n    \"astor\",\n    \"cv2\",\n    \"scipy\",\n    \"xlsxwriter\"\n]\nignore_missing_imports = true\n"
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "r",
          "type": "tree",
          "content": null
        },
        {
          "name": "security",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 83.6455078125,
          "content": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\nimport ctypes\nimport errno\nimport fnmatch\nimport glob\nimport multiprocessing\nimport os\nimport platform\nimport re\nimport shlex\nimport shutil\nimport subprocess\nimport sys\nimport time\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom subprocess import CalledProcessError\n\nfrom setuptools import Command, Extension, setup\nfrom setuptools.command.develop import develop as DevelopCommandBase\nfrom setuptools.command.egg_info import egg_info\nfrom setuptools.command.install import install as InstallCommandBase\nfrom setuptools.command.install_lib import install_lib\nfrom setuptools.dist import Distribution\n\npython_version = platform.python_version()\nversion_detail = sys.version_info\nversion = str(version_detail[0]) + '.' + str(version_detail[1])\nenv_version = os.getenv(\"PY_VERSION\", None)\n\nif version_detail < (3, 8):\n    raise RuntimeError(\n        f\"Paddle only supports Python version >= 3.8 now,\"\n        f\"you are using Python {python_version}\"\n    )\nelif env_version is None:\n    print(f\"export PY_VERSION = { version }\")\n    os.environ[\"PY_VERSION\"] = python_version\n\nelif env_version != version:\n    raise ValueError(\n        f\"You have set the PY_VERSION environment variable to {env_version}, but \"\n        f\"your current Python version is {version}, \"\n        f\"Please keep them consistent.\"\n    )\n\n\n# check cmake\nCMAKE = shutil.which('cmake3') or shutil.which('cmake')\nassert (\n    CMAKE\n), 'The \"cmake\" executable is not found. Please check if Cmake is installed.'\n\n\nTOP_DIR = os.path.dirname(os.path.realpath(__file__))\n\nIS_WINDOWS = os.name == 'nt'\n\n\ndef filter_setup_args(input_args):\n    cmake_and_build = True\n    only_cmake = False\n    rerun_cmake = False\n    filter_args_list = []\n    for arg in input_args:\n        if arg == 'rerun-cmake':\n            rerun_cmake = True  # delete CMakeCache.txt and rerun cmake\n            continue\n        if arg == 'only-cmake':\n            only_cmake = True  # only cmake and do not make, leave a chance for users to adjust build options\n            continue\n        if arg in ['clean', 'egg_info', 'sdist']:\n            cmake_and_build = False\n        filter_args_list.append(arg)\n    return cmake_and_build, only_cmake, rerun_cmake, filter_args_list\n\n\ncmake_and_build, only_cmake, rerun_cmake, filter_args_list = filter_setup_args(\n    sys.argv\n)\n\n\ndef parse_input_command(input_parameters):\n    dist = Distribution()\n    # get script name :setup.py\n    sys.argv = input_parameters\n    dist.script_name = os.path.basename(sys.argv[0])\n    # get args of setup.py\n    dist.script_args = sys.argv[1:]\n    print(\n        \"Start executing python {} {}\".format(\n            dist.script_name, \"\".join(dist.script_args)\n        )\n    )\n    try:\n        dist.parse_command_line()\n    except:\n        print(\n            f\"An error occurred while parsing\"\n            f\"the parameters, {dist.script_args}\"\n        )\n        sys.exit(1)\n\n\nclass BinaryDistribution(Distribution):\n    def has_ext_modules(foo):\n        return True\n\n\nRC = 0\next_suffix = (\n    '.dll'\n    if os.name == 'nt'\n    else ('.dylib' if sys.platform == 'darwin' else '.so')\n)\n\n\ndef get_header_install_dir(header):\n    if 'pb.h' in header:\n        install_dir = re.sub(\n            env_dict.get(\"PADDLE_BINARY_DIR\") + '/', '', header\n        )\n    elif 'third_party' not in header:\n        # paddle headers\n        install_dir = re.sub(\n            env_dict.get(\"PADDLE_SOURCE_DIR\") + '/', '', header\n        )\n        if 'fluid/jit' in install_dir:\n            install_dir = re.sub('fluid/jit', 'jit', install_dir)\n    else:\n        # third_party\n        install_dir = re.sub(\n            env_dict.get(\"THIRD_PARTY_PATH\"), 'third_party', header\n        )\n        patterns = [\n            'install/mkldnn/include/',\n            'pybind/src/extern_pybind/include/',\n            'third_party/xpu/src/extern_xpu/xpu/include/',\n        ]\n        for pattern in patterns:\n            install_dir = re.sub(pattern, '', install_dir)\n    return install_dir\n\n\nclass InstallHeaders(Command):\n    \"\"\"Override how headers are copied.\"\"\"\n\n    description = 'install C/C++ header files'\n\n    user_options = [\n        ('install-dir=', 'd', 'directory to install header files to'),\n        ('force', 'f', 'force installation (overwrite existing files)'),\n    ]\n\n    boolean_options = ['force']\n\n    def initialize_options(self):\n        self.install_dir = None\n        self.force = 0\n        self.outfiles = []\n\n    def finalize_options(self):\n        self.set_undefined_options(\n            'install', ('install_headers', 'install_dir'), ('force', 'force')\n        )\n\n    def run(self):\n        hdrs = self.distribution.headers\n        if not hdrs:\n            return\n        self.mkpath(self.install_dir)\n        for header in hdrs:\n            install_dir = get_header_install_dir(header)\n            install_dir = os.path.join(\n                self.install_dir, os.path.dirname(install_dir)\n            )\n            if not os.path.exists(install_dir):\n                self.mkpath(install_dir)\n            (out, _) = self.copy_file(header, install_dir)\n            self.outfiles.append(out)\n            # (out, _) = self.mkdir_and_copy_file(header)\n            # self.outfiles.append(out)\n\n    def get_inputs(self):\n        return self.distribution.headers or []\n\n    def get_outputs(self):\n        return self.outfiles\n\n\nclass InstallCommand(InstallCommandBase):\n    def finalize_options(self):\n        ret = InstallCommandBase.finalize_options(self)\n        self.install_lib = self.install_platlib\n\n        self.install_headers = os.path.join(\n            self.install_platlib, 'paddle', 'include'\n        )\n        return ret\n\n\nclass DevelopCommand(DevelopCommandBase):\n    def run(self):\n        # copy proto and .so to python_source_dir\n        fluid_proto_binary_path = (\n            paddle_binary_dir + '/python/paddle/base/proto/'\n        )\n        fluid_proto_source_path = (\n            paddle_source_dir + '/python/paddle/base/proto/'\n        )\n        distributed_proto_binary_path = (\n            paddle_binary_dir + '/python/paddle/distributed/fleet/proto/'\n        )\n        distributed_proto_source_path = (\n            paddle_source_dir + '/python/paddle/distributed/fleet/proto/'\n        )\n        os.system(f\"rm -rf {fluid_proto_source_path}\")\n        shutil.copytree(fluid_proto_binary_path, fluid_proto_source_path)\n        os.system(f\"rm -rf {distributed_proto_source_path}\")\n        shutil.copytree(\n            distributed_proto_binary_path, distributed_proto_source_path\n        )\n        shutil.copy(\n            paddle_binary_dir + '/python/paddle/base/libpaddle.so',\n            paddle_source_dir + '/python/paddle/base/',\n        )\n        dynamic_library_binary_path = paddle_binary_dir + '/python/paddle/libs/'\n        dynamic_library_source_path = paddle_source_dir + '/python/paddle/libs/'\n        for lib_so in os.listdir(dynamic_library_binary_path):\n            shutil.copy(\n                dynamic_library_binary_path + lib_so,\n                dynamic_library_source_path,\n            )\n        # write version.py and cuda_env_config_py to python_source_dir\n        write_version_py(\n            filename=f'{paddle_source_dir}/python/paddle/version/__init__.py'\n        )\n        write_cuda_env_config_py(\n            filename=f'{paddle_source_dir}/python/paddle/cuda_env.py'\n        )\n        write_parameter_server_version_py(\n            filename=f'{paddle_source_dir}/python/paddle/incubate/distributed/fleet/parameter_server/version.py'\n        )\n        DevelopCommandBase.run(self)\n\n\nclass EggInfo(egg_info):\n    \"\"\"Copy license file into `.dist-info` folder.\"\"\"\n\n    def run(self):\n        # don't duplicate license into `.dist-info` when building a distribution\n        if not self.distribution.have_run.get('install', True):\n            self.mkpath(self.egg_info)\n            self.copy_file(\n                env_dict.get(\"PADDLE_SOURCE_DIR\") + \"/LICENSE\", self.egg_info\n            )\n\n        egg_info.run(self)\n\n\n# class Installlib is rewritten to add header files to .egg/paddle\nclass InstallLib(install_lib):\n    def run(self):\n        self.build()\n        outfiles = self.install()\n        hrds = self.distribution.headers\n        if not hrds:\n            return\n        for header in hrds:\n            install_dir = get_header_install_dir(header)\n            install_dir = os.path.join(\n                self.install_dir, 'paddle/include', os.path.dirname(install_dir)\n            )\n            if not os.path.exists(install_dir):\n                self.mkpath(install_dir)\n            self.copy_file(header, install_dir)\n        if outfiles is not None:\n            # always compile, in case we have any extension stubs to deal with\n            self.byte_compile(outfiles)\n\n\ndef git_commit() -> str:\n    try:\n        cmd = ['git', 'rev-parse', 'HEAD']\n        git_commit = (\n            subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                cwd=env_dict.get(\"PADDLE_SOURCE_DIR\"),\n            )\n            .communicate()[0]\n            .strip()\n        )\n    except:\n        git_commit = 'Unknown'\n    git_commit = git_commit.decode('utf-8')\n    return str(git_commit)\n\n\ndef _get_version_detail(idx):\n    assert (\n        idx < 3\n    ), \"version info consists of %(major)d.%(minor)d.%(patch)d, \\\n        so detail index must less than 3\"\n    tag_version_regex = env_dict.get(\"TAG_VERSION_REGEX\")\n    paddle_version = env_dict.get(\"PADDLE_VERSION\")\n    if re.match(tag_version_regex, paddle_version):\n        version_details = paddle_version.split('.')\n        if len(version_details) >= 3:\n            return version_details[idx]\n    return 0\n\n\ndef _mkdir_p(dir_str):\n    try:\n        os.makedirs(dir_str)\n    except OSError as e:\n        raise RuntimeError(\"Failed to create build folder\")\n\n\ndef get_major() -> int:\n    return int(_get_version_detail(0))\n\n\ndef get_minor() -> int:\n    return int(_get_version_detail(1))\n\n\ndef get_patch() -> int:\n    return str(_get_version_detail(2))\n\n\ndef get_nccl_version() -> int:\n    if env_dict.get(\"WITH_NCCL\") == 'ON':\n        return int(env_dict.get(\"NCCL_VERSION\"))\n    return 0\n\n\ndef get_cuda_version() -> str:\n    with_gpu = env_dict.get(\"WITH_GPU\")\n    if with_gpu == 'ON':\n        return env_dict.get(\"CUDA_VERSION\")\n    else:\n        return 'False'\n\n\ndef get_cudnn_version() -> str:\n    with_gpu = env_dict.get(\"WITH_GPU\")\n    if with_gpu == 'ON':\n        temp_cudnn_version = ''\n        cudnn_major_version = env_dict.get(\"CUDNN_MAJOR_VERSION\")\n        if cudnn_major_version:\n            temp_cudnn_version += cudnn_major_version\n            cudnn_minor_version = env_dict.get(\"CUDNN_MINOR_VERSION\")\n            if cudnn_minor_version:\n                temp_cudnn_version = (\n                    temp_cudnn_version + '.' + cudnn_minor_version\n                )\n                cudnn_patchlevel_version = env_dict.get(\n                    \"CUDNN_PATCHLEVEL_VERSION\"\n                )\n                if cudnn_patchlevel_version:\n                    temp_cudnn_version = (\n                        temp_cudnn_version + '.' + cudnn_patchlevel_version\n                    )\n        return temp_cudnn_version\n    else:\n        return 'False'\n\n\ndef get_xpu_xre_version() -> str:\n    with_xpu = env_dict.get(\"WITH_XPU\")\n    if with_xpu == 'ON':\n        return env_dict.get(\"XPU_XRE_BASE_VERSION\")\n    else:\n        return 'False'\n\n\ndef get_xpu_xccl_version() -> str:\n    with_xpu_xccl = env_dict.get(\"WITH_XPU_BKCL\")\n    if with_xpu_xccl == 'ON':\n        return env_dict.get(\"XPU_XCCL_BASE_VERSION\")\n    else:\n        return 'False'\n\n\ndef get_xpu_xhpc_version() -> str:\n    with_xpu_xhpc = env_dict.get(\"WITH_XPU\")\n    if with_xpu_xhpc == 'ON':\n        return env_dict.get(\"XPU_XHPC_BASE_DATE\")\n    else:\n        return 'False'\n\n\ndef is_tagged() -> bool:\n    try:\n        cmd = [\n            'git',\n            'describe',\n            '--exact-match',\n            '--tags',\n            'HEAD',\n            '2>/dev/null',\n        ]\n        git_tag = (\n            subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                cwd=env_dict.get(\"PADDLE_SOURCE_DIR\"),\n            )\n            .communicate()[0]\n            .strip()\n        )\n        git_tag = git_tag.decode()\n    except:\n        return False\n    if str(git_tag).replace('v', '') == env_dict.get(\"PADDLE_VERSION\"):\n        return True\n    else:\n        return False\n\n\ndef get_cinn_version() -> str:\n    if env_dict.get(\"WITH_CINN\") != 'ON':\n        return \"False\"\n    return \"0.3.0\"\n\n\ndef get_cuda_archs() -> list[int]:\n    compiled_cuda_archs = env_dict.get(\"COMPILED_CUDA_ARCHS\")\n    if isinstance(compiled_cuda_archs, str):\n        compiled_cuda_archs = re.findall(r'\\d+', compiled_cuda_archs)\n        return [int(arch) for arch in compiled_cuda_archs]\n    else:\n        return []\n\n\ndef get_tensorrt_version() -> str:\n\n    def find_libnvinfer():\n        \"\"\"Search for libnvinfer.so file in LD_LIBRARY_PATH.\"\"\"\n\n        trt_infer_rt_path = env_dict.get(\"TR_INFER_RT\")\n        tensorrt_library_path = env_dict.get(\"TENSORRT_LIBRARY_DIR\")\n\n        libnvinfer_file = os.path.join(tensorrt_library_path, trt_infer_rt_path)\n\n        if os.path.exists(libnvinfer_file):\n            return libnvinfer_file\n        else:\n            print(f\"{libnvinfer_file} not found.\")\n        return None\n\n    try:\n        libnvinfer_path = find_libnvinfer()\n        if not libnvinfer_path:\n            return None\n\n        trt = ctypes.CDLL(libnvinfer_path)\n        get_version = trt.getInferLibVersion\n        get_version.restype = ctypes.c_int\n        version = get_version()\n        version_str = str(version)\n        major = version_str[:1] if len(version_str) > 1 else version_str\n        minor = version_str[1:2] if len(version_str) > 3 else version_str[1:]\n        patch = version_str[3:] if len(version_str) > 3 else ''\n\n        minor = minor if minor else '0'\n        patch = patch if patch else '0'\n        version_str = f\"{major}.{minor}.{patch}\"\n\n        return version_str\n\n    except Exception as e:\n        print(f\"Error while getting TensorRT version: {e}\")\n        return None\n\n\ndef write_version_py(filename='paddle/version/__init__.py'):\n    cnt = '''# THIS FILE IS GENERATED FROM PADDLEPADDLE SETUP.PY\n#\nfull_version     = '%(major)d.%(minor)d.%(patch)s'\nmajor            = '%(major)d'\nminor            = '%(minor)d'\npatch            = '%(patch)s'\nnccl_version     = '%(nccl)d'\nrc               = '%(rc)d'\ncuda_version     = '%(cuda)s'\ncudnn_version    = '%(cudnn)s'\nxpu_xre_version  = '%(xpu_xre)s'\nxpu_xccl_version = '%(xpu_xccl)s'\nxpu_xhpc_version = '%(xpu_xhpc)s'\nis_tagged        = %(is_tagged)s\ncommit           = '%(commit)s'\nwith_mkl         = '%(with_mkl)s'\ncinn_version     = '%(cinn)s'\ntensorrt_version = '%(tensorrt)s'\nwith_pip_cuda_libraries = '%(with_pip_cuda_libraries)s'\nwith_pip_tensorrt       = '%(with_pip_tensorrt)s'\ncompiled_cuda_archs     = %(compiled_cuda_archs)s\n\n__all__ = ['cuda', 'cudnn', 'nccl', 'show', 'xpu', 'xpu_xre', 'xpu_xccl', 'xpu_xhpc', 'tensorrt', 'cuda_archs']\n\ndef show() -> None:\n    \"\"\"Get the version of paddle if `paddle` package if tagged. Otherwise, output the corresponding commit id.\n\n    Returns:\n        If paddle package is not tagged, the commit-id of paddle will be output.\n        Otherwise, the following information will be output.\n\n        full_version: version of paddle\n\n        major: the major version of paddle\n\n        minor: the minor version of paddle\n\n        patch: the patch level version of paddle\n\n        rc: whether it's rc version\n\n        cuda: the cuda version of package. It will return `False` if CPU version paddle package is installed\n\n        cudnn: the cudnn version of package. It will return `False` if CPU version paddle package is installed\n\n        xpu_xre: the xpu xre version of package. It will return `False` if non-XPU version paddle package is installed\n\n        xpu_xccl: the xpu xccl version of package. It will return `False` if non-XPU version paddle package is installed\n\n        xpu_xhpc: the xpu xhpc version of package. It will return `False` if non-XPU version paddle package is installed\n\n        cinn: the cinn version of package. It will return `False` if paddle package is not compiled with CINN\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> # Case 1: paddle is tagged with 2.2.0\n            >>> paddle.version.show()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            full_version: 2.2.0\n            major: 2\n            minor: 2\n            patch: 0\n            rc: 0\n            cuda: '10.2'\n            cudnn: '7.6.5'\n            xpu_xre: '4.32.0.1'\n            xpu_xccl: '1.0.7'\n            xpu_xhpc: '20231208'\n            cinn: False\n            >>> # doctest: -SKIP\n\n            >>> # Case 2: paddle is not tagged\n            >>> paddle.version.show()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            commit: cfa357e984bfd2ffa16820e354020529df434f7d\n            cuda: '10.2'\n            cudnn: '7.6.5'\n            xpu_xre: '4.32.0.1'\n            xpu_xccl: '1.0.7'\n            xpu_xhpc: '20231208'\n            cinn: False\n            >>> # doctest: -SKIP\n    \"\"\"\n    if is_tagged:\n        print('full_version:', full_version)\n        print('major:', major)\n        print('minor:', minor)\n        print('patch:', patch)\n        print('rc:', rc)\n    else:\n        print('commit:', commit)\n    print('cuda:', cuda_version)\n    print('cudnn:', cudnn_version)\n    print('nccl:', nccl_version)\n    print('xpu_xre:', xpu_xre_version)\n    print('xpu_xccl:', xpu_xccl_version)\n    print('xpu_xhpc:', xpu_xhpc_version)\n    print('cinn:', cinn_version)\n    print('tensorrt_version:', tensorrt_version)\n    print('cuda_archs:', compiled_cuda_archs)\n\ndef mkl() -> str:\n    return with_mkl\n\ndef nccl() -> str:\n    \"\"\"Get nccl version of paddle package.\n\n    Returns:\n        string: Return the version information of cuda nccl. If paddle package is CPU version, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.nccl()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            '2804'\n\n    \"\"\"\n    return nccl_version\n\ndef cuda() -> str:\n    \"\"\"Get cuda version of paddle package.\n\n    Returns:\n        string: Return the version information of cuda. If paddle package is CPU version, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.cuda()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            '10.2'\n\n    \"\"\"\n    return cuda_version\n\ndef cudnn() -> str:\n    \"\"\"Get cudnn version of paddle package.\n\n    Returns:\n        string: Return the version information of cudnn. If paddle package is CPU version, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.cudnn()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            '7.6.5'\n\n    \"\"\"\n    return cudnn_version\n\ndef xpu() -> str:\n    \"\"\"Get xpu version of paddle package. The API is deprecated now, please use xpu_xhpc() instead.\n\n    Returns:\n        string: Return the version information of xpu. If paddle package is non-XPU version, it will return False.\n    Examples:\n        .. code-block:: python\n            >>> import paddle\n            >>> paddle.version.xpu()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            '20230114'\n    \"\"\"\n    return xpu_xhpc_version\n\ndef xpu_xre() -> str:\n    \"\"\"Get xpu xre version of paddle package.\n\n    Returns:\n        string: Return the version information of xpu. If paddle package is non-XPU version, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.xpu_xre()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            '4.32.0.1'\n\n    \"\"\"\n    return xpu_xre_version\n\ndef xpu_xccl() -> str:\n    \"\"\"Get xpu xccl version of paddle package.\n\n    Returns:\n        string: Return the version information of xpu xccl. If paddle package is non-XPU version, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.xpu_xccl()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            '1.0.7'\n\n    \"\"\"\n    return xpu_xccl_version\n\ndef xpu_xhpc() -> str:\n    \"\"\"Get xpu xhpc version of paddle package.\n\n    Returns:\n        string: Return the version information of xpu xhpc. If paddle package is non-XPU version, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.xpu_xhpc()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            '20231208'\n\n    \"\"\"\n    return xpu_xhpc_version\n\ndef cinn() -> str:\n    \"\"\"Get CINN version of paddle package.\n\n    Returns:\n        string: Return the version information of CINN. If paddle package is not compiled with CINN, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.cinn()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            False\n\n    \"\"\"\n    return cinn_version\n\ndef tensorrt() -> str:\n    \"\"\"Get TensorRT version of paddle package.\n\n    Returns:\n        string: Return the version information of TensorRT. If paddle package is not compiled with TensorRT, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.tensorrt()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            False\n\n    \"\"\"\n    return tensorrt_version\n\ndef cuda_archs():\n    \"\"\"Get compiled cuda archs of paddle package.\n\n    Returns:\n        list[int]: Return the compiled cuda archs if with gpu. If paddle package is not compiled with gpu, it will return \"\".\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.cuda_archs()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            [86]\n\n    \"\"\"\n    return compiled_cuda_archs\n'''\n    commit = git_commit()\n\n    dirname = os.path.dirname(filename)\n\n    try:\n        os.makedirs(dirname)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n\n    with open(filename, 'w') as f:\n        f.write(\n            cnt\n            % {\n                'major': get_major(),\n                'minor': get_minor(),\n                'patch': get_patch(),\n                'nccl': get_nccl_version(),\n                'rc': RC,\n                'version': env_dict.get(\"PADDLE_VERSION\"),\n                'cuda': get_cuda_version(),\n                'cudnn': get_cudnn_version(),\n                'xpu_xre': get_xpu_xre_version(),\n                'xpu_xccl': get_xpu_xccl_version(),\n                'xpu_xhpc': get_xpu_xhpc_version(),\n                'commit': commit,\n                'is_tagged': is_tagged(),\n                'with_mkl': env_dict.get(\"WITH_MKL\"),\n                'cinn': get_cinn_version(),\n                'tensorrt': get_tensorrt_version(),\n                'with_pip_cuda_libraries': env_dict.get(\n                    \"WITH_PIP_CUDA_LIBRARIES\"\n                ),\n                'with_pip_tensorrt': env_dict.get(\"WITH_PIP_TENSORRT\"),\n                'compiled_cuda_archs': get_cuda_archs(),\n            }\n        )\n\n\ndef write_cuda_env_config_py(filename='paddle/cuda_env.py'):\n    cnt = \"\"\n    if env_dict.get(\"JIT_RELEASE_WHL\") == 'ON':\n        cnt = '''# THIS FILE IS GENERATED FROM PADDLEPADDLE SETUP.PY\n#\nimport os\nos.environ['CUDA_CACHE_MAXSIZE'] = '805306368'\n'''\n\n    with open(filename, 'w') as f:\n        f.write(cnt)\n\n\ndef write_parameter_server_version_py(\n    filename='paddle/incubate/distributed/fleet/parameter_server/version.py',\n):\n    cnt = '''\n\n# THIS FILE IS GENERATED FROM PADDLEPADDLE SETUP.PY\n\nfrom paddle.incubate.distributed.fleet.base import Mode\n\nBUILD_MODE=Mode.%(mode)s\n\ndef is_transpiler():\n    return Mode.TRANSPILER == BUILD_MODE\n\n'''\n\n    dirname = os.path.dirname(filename)\n\n    try:\n        os.makedirs(dirname)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n    with open(filename, 'w') as f:\n        f.write(\n            cnt\n            % {\n                'mode': (\n                    'PSLIB'\n                    if env_dict.get(\"WITH_PSLIB\") == 'ON'\n                    else 'TRANSPILER'\n                )\n            }\n        )\n\n\ndef find_files(pattern, root, recursive=False):\n    for dirpath, _, files in os.walk(root):\n        for filename in fnmatch.filter(files, pattern):\n            yield os.path.join(dirpath, filename)\n        if not recursive:\n            break\n\n\n@contextmanager\ndef cd(path):\n    if not os.path.isabs(path):\n        raise RuntimeError(f'Can only cd to absolute path, got: {path}')\n    orig_path = os.getcwd()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(orig_path)\n\n\ndef options_process(args, build_options):\n    for key, value in sorted(build_options.items()):\n        if value is not None:\n            args.append(f\"-D{key}={value}\")\n\n\ndef get_cmake_generator():\n    if os.getenv(\"GENERATOR\"):\n        cmake_generator = os.getenv(\"GENERATOR\")\n        if os.system('ninja --version') == 0:\n            print(\"Ninja has been installed,use ninja to compile Paddle now.\")\n        else:\n            print(\"Ninja has not been installed,install it now.\")\n            os.system('python -m pip install ninja')\n    else:\n        cmake_generator = \"Unix Makefiles\"\n    return cmake_generator\n\n\ndef cmake_run(build_path):\n    args = []\n    env_var = os.environ.copy()  # get env variables\n    paddle_build_options = {}\n    other_options = {}\n    other_options.update(\n        {\n            option: option\n            for option in (\n                \"PYTHON_LIBRARY\",\n                \"INFERENCE_DEMO_INSTALL_DIR\",\n                \"ON_INFER\",\n                \"PYTHON_EXECUTABLE\",\n                \"TENSORRT_ROOT\",\n                \"CUDA_ARCH_NAME\",\n                \"CUDA_ARCH_BIN\",\n                \"PYTHON_INCLUDE_DIR\",\n                \"PYTHON_LIBRARIES\",\n                \"PY_VERSION\",\n                \"CUB_PATH\",\n                \"NEW_RELEASE_PYPI\",\n                \"CUDNN_ROOT\",\n                \"THIRD_PARTY_PATH\",\n                \"NOAVX_CORE_FILE\",\n                \"LITE_GIT_TAG\",\n                \"CUDA_TOOLKIT_ROOT_DIR\",\n                \"NEW_RELEASE_JIT\",\n                \"XPU_SDK_ROOT\",\n                \"MSVC_STATIC_CRT\",\n                \"NEW_RELEASE_ALL\",\n                \"GENERATOR\",\n            )\n        }\n    )\n    # if environment variables which start with \"WITH_\" or \"CMAKE_\",put it into build_options\n    for option_key, option_value in env_var.items():\n        if option_key.startswith((\"CMAKE_\", \"WITH_\")):\n            paddle_build_options[option_key] = option_value\n        if option_key in other_options:\n            if (\n                option_key == 'PYTHON_EXECUTABLE'\n                or option_key == 'PYTHON_LIBRARY'\n                or option_key == 'PYTHON_LIBRARIES'\n            ):\n                key = option_key + \":FILEPATH\"\n            elif option_key == 'PYTHON_INCLUDE_DIR':\n                key = option_key + ':PATH'\n            elif option_key == 'GENERATOR':\n                key = 'CMAKE_' + option_key\n            else:\n                key = other_options[option_key]\n            if key not in paddle_build_options:\n                paddle_build_options[key] = option_value\n\n    options_process(args, paddle_build_options)\n    with cd(build_path):\n        cmake_args = []\n        cmake_args.append(CMAKE)\n        cmake_args += args\n        cmake_args.append('-DWITH_SETUP_INSTALL=ON')\n        cmake_args.append(TOP_DIR)\n        subprocess.check_call(cmake_args)\n\n\ndef build_run(args, build_path, environ_var):\n    with cd(build_path):\n        build_args = []\n        build_args.append(CMAKE)\n        build_args += args\n        try:\n            subprocess.check_call(build_args, cwd=build_path, env=environ_var)\n        except (CalledProcessError, KeyboardInterrupt) as e:\n            sys.exit(1)\n\n\ndef run_cmake_build(build_path):\n    build_type = (\n        os.getenv(\"CMAKE_BUILD_TYPE\")\n        if os.getenv(\"CMAKE_BUILD_TYPE\") is not None\n        else \"release\"\n    )\n    build_args = [\"--build\", \".\", \"--target\", \"install\", \"--config\", build_type]\n    max_jobs = os.getenv(\"MAX_JOBS\")\n    if max_jobs is not None:\n        max_jobs = max_jobs or str(multiprocessing.cpu_count())\n\n        build_args += [\"--\"]\n        if IS_WINDOWS:\n            build_args += [f\"/p:CL_MPCount={max_jobs}\"]\n        else:\n            build_args += [\"-j\", max_jobs]\n    else:\n        build_args += [\"-j\", str(multiprocessing.cpu_count())]\n    environ_var = os.environ.copy()\n    build_run(build_args, build_path, environ_var)\n\n\ndef build_steps():\n    print('------- Building start ------')\n    build_dir = os.getenv(\"BUILD_DIR\")\n    if build_dir is not None:\n        build_dir = TOP_DIR + '/' + build_dir\n    else:\n        build_dir = TOP_DIR + '/build'\n    if not os.path.exists(build_dir):\n        _mkdir_p(build_dir)\n    build_path = build_dir\n    print(\"build_dir:\", build_dir)\n    # run cmake to generate native build files\n    cmake_cache_file_path = os.path.join(build_path, \"CMakeCache.txt\")\n    # if rerun_cmake is True,remove CMakeCache.txt and rerun cmake\n    if os.path.isfile(cmake_cache_file_path) and rerun_cmake is True:\n        os.remove(cmake_cache_file_path)\n\n    CMAKE_GENERATOR = get_cmake_generator()\n    bool_ninja = CMAKE_GENERATOR == \"Ninja\"\n    build_ninja_file_path = os.path.join(build_path, \"build.ninja\")\n    if os.path.exists(cmake_cache_file_path) and not (\n        bool_ninja and not os.path.exists(build_ninja_file_path)\n    ):\n        print(\"Do not need rerun cmake, everything is ready, run build now\")\n    else:\n        cmake_run(build_path)\n    # make\n    if only_cmake:\n        print(\n            \"You have finished running cmake, the program exited,run 'cmake build' to adjust build options and 'python setup.py install to build'\"\n        )\n        sys.exit()\n    run_cmake_build(build_path)\n\n\ndef get_setup_requires():\n    with open(\n        env_dict.get(\"PADDLE_SOURCE_DIR\") + '/python/requirements.txt'\n    ) as f:\n        setup_requires = (\n            f.read().splitlines()\n        )  # Specify the dependencies to install\n    if sys.version_info >= (3, 8):\n        setup_requires_tmp = []\n        for setup_requires_i in setup_requires:\n            if (\n                '<\"3.6\"' in setup_requires_i\n                or '<=\"3.6\"' in setup_requires_i\n                or '<\"3.5\"' in setup_requires_i\n                or '<=\"3.5\"' in setup_requires_i\n                or '<\"3.7\"' in setup_requires_i\n                or '<=\"3.7\"' in setup_requires_i\n                or '<\"3.8\"' in setup_requires_i\n                or setup_requires_i.strip().endswith('[build]')\n            ):\n                continue\n            setup_requires_tmp += [setup_requires_i]\n        setup_requires = setup_requires_tmp\n\n        return setup_requires\n    else:\n        raise RuntimeError(\n            \"please check your python version,Paddle only support Python version>=3.8 now\"\n        )\n\n\ndef get_paddle_extra_install_requirements():\n    paddle_cuda_requires = []\n    paddle_tensorrt_requires = []\n    # (Note risemeup1): Paddle will install the pypi cuda package provided by Nvidia, which includes the cuda runtime, cudnn, and cublas, thereby making the operation of 'pip install paddle' no longer dependent on the installation of cuda and cudnn.\n    if env_dict.get(\"WITH_PIP_CUDA_LIBRARIES\") == \"ON\":\n        if platform.system() == 'Linux':\n            PADDLE_CUDA_INSTALL_REQUIREMENTS = {\n                \"V11\": (\n                    \"nvidia-cuda-runtime-cu11==11.8.89; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cuda-cupti-cu11==11.8.87; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cudnn-cu11==8.9.6.50; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cublas-cu11==11.11.3.6; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cufft-cu11==10.9.0.58; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-curand-cu11==10.3.0.86; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cusolver-cu11==11.4.1.48; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cusparse-cu11==11.7.5.86; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-nccl-cu11==2.19.3; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-nvtx-cu11==11.8.86; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cuda-nvrtc-cu11==11.8.89; platform_system == 'Linux' and platform_machine == 'x86_64'\"\n                ),\n                \"V12\": (\n                    \"nvidia-cuda-runtime-cu12==12.3.101; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cuda-cupti-cu12==12.3.101; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cudnn-cu12==9.1.1.17; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cublas-cu12==12.3.4.1; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cufft-cu12==11.2.1.3; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-curand-cu12==10.3.5.147; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cusolver-cu12==11.6.1.9; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cusparse-cu12==12.3.1.170; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-nccl-cu12==2.19.3; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-nvtx-cu12==12.4.127; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cuda-nvrtc-cu12==12.3.107; platform_system == 'Linux' and platform_machine == 'x86_64'\"\n                ),\n            }\n        elif platform.system() == 'Windows':\n            PADDLE_CUDA_INSTALL_REQUIREMENTS = {\n                \"V11\": (\n                    \"nvidia-cuda-runtime-cu11==11.8.89 | \"\n                    \"nvidia-cudnn-cu11==8.9.4.19 | \"\n                    \"nvidia-cublas-cu11==11.11.3.6 | \"\n                    \"nvidia-cufft-cu11==10.9.0.58 | \"\n                    \"nvidia-curand-cu11==10.3.0.86 | \"\n                    \"nvidia-cusolver-cu11==11.4.1.48 | \"\n                    \"nvidia-cusparse-cu11==11.7.5.86 \"\n                ),\n                \"V12\": (\n                    \"nvidia-cuda-runtime-cu12==12.3.101 | \"\n                    \"nvidia-cudnn-cu12==9.1.1.17 | \"\n                    \"nvidia-cublas-cu12==12.3.4.1 | \"\n                    \"nvidia-cufft-cu12==11.2.1.3 | \"\n                    \"nvidia-curand-cu12==10.3.5.147 | \"\n                    \"nvidia-cusolver-cu12==11.6.1.9 | \"\n                    \"nvidia-cusparse-cu12==12.3.1.170 \"\n                ),\n            }\n        try:\n            output = subprocess.check_output(['nvcc', '--version']).decode(\n                'utf-8'\n            )\n            version_line = next(\n                line for line in output.split('\\n') if 'release' in line\n            )\n            version = version_line.split(' ')[-1].split(',')[0]\n            cuda_major_version = version.split('.')[0]\n        except Exception as e:\n            raise ValueError(\"CUDA not found\")\n\n        paddle_cuda_requires = PADDLE_CUDA_INSTALL_REQUIREMENTS[\n            cuda_major_version\n        ].split(\"|\")\n\n    if env_dict.get(\"WITH_PIP_TENSORRT\") == \"ON\":\n        version_str = get_tensorrt_version()\n        version_default = int(version_str.split(\".\")[0])\n        if platform.system() == 'Linux' or (\n            platform.system() == 'Windows' and version_default >= 10\n        ):\n\n            PADDLE_TENSORRT_INSTALL_REQUIREMENTS = [\n                \"tensorrt==8.5.3.1\",\n                \"tensorrt==8.6.0\",\n                \"tensorrt==8.6.1.post1\",\n            ]\n\n            if not version_str:\n                return paddle_cuda_requires, []\n\n            version_main = \".\".join(version_str.split(\".\")[:3])\n\n            matched_package = None\n            for (\n                paddle_tensorrt_requires\n            ) in PADDLE_TENSORRT_INSTALL_REQUIREMENTS:\n                paddle_tensorrt_version = paddle_tensorrt_requires.split(\"==\")[\n                    1\n                ]\n                paddle_tensorrt_main = \".\".join(\n                    paddle_tensorrt_version.split(\".\")[:3]\n                )\n\n                if version_main == paddle_tensorrt_main:\n                    matched_package = paddle_tensorrt_requires\n                    break\n\n            if matched_package:\n                paddle_tensorrt_requires = [matched_package]\n            else:\n                print(\n                    f\"No exact match found for TensorRT Version: {version_str}. We currently support TensorRT versions 8.5.3.1, 8.6.0, and 8.6.1.\"\n                )\n                return paddle_cuda_requires, []\n\n    return paddle_cuda_requires, paddle_tensorrt_requires\n\n\ndef get_cinn_config_jsons():\n    from pathlib import Path\n\n    src_cinn_config_path = (\n        env_dict.get(\"PADDLE_SOURCE_DIR\") + '/python/paddle/cinn_config'\n    )\n    prefix_len = len(src_cinn_config_path) + 1\n    p = Path(src_cinn_config_path)\n    json_list = list(p.glob('**/*.json'))\n    json_path_list = []\n    for json in json_list:\n        json = str(json)\n        json = json[prefix_len:]\n        json_path_list += [json]\n    return json_path_list\n\n\ndef get_typing_libs_packages(paddle_binary_dir):\n    \"\"\"get all libpaddle sub modules from 'python/paddle/_typing/libs/libpaddle'\n    e.g.\n        'paddle._typing.libs.libpaddle.cinn'\n        'paddle._typing.libs.libpaddle.pir'\n        'paddle._typing.libs.libpaddle.eager'\n        'paddle._typing.libs.libpaddle.eager.ops'\n    \"\"\"\n    base_dir = Path(paddle_binary_dir) / 'python'\n    libs_dir = base_dir / 'paddle' / '_typing' / 'libs' / 'libpaddle'\n    return [\n        '.'.join(str(Path(root).relative_to(base_dir)).split(os.sep))\n        for root, _, _ in os.walk(libs_dir)\n    ]\n\n\ndef extend_type_hints_package_data(packages, package_data, paddle_binary_dir):\n    typing_libs_packages = get_typing_libs_packages(paddle_binary_dir)\n\n    # update packages\n    packages += typing_libs_packages\n\n    # update package_data\n    type_hints_files = {\n        'paddle': ['py.typed', '*.pyi'],\n        'paddle.framework': ['*.pyi'],\n        'paddle.base': ['*.pyi'],\n        'paddle.tensor': ['tensor.pyi'],\n        'paddle._typing': ['*.pyi'],\n        'paddle._typing.libs': ['*.pyi', '*.md'],\n    }\n\n    for libpaddle_module in typing_libs_packages:\n        type_hints_files[libpaddle_module] = ['*.pyi']\n\n    for pkg, files in type_hints_files.items():\n        if pkg not in package_data:\n            package_data[pkg] = []\n        package_data[pkg] += files\n\n    return packages, package_data\n\n\ndef get_package_data_and_package_dir():\n    if os.name != 'nt':\n        package_data = {\n            'paddle.base': [env_dict.get(\"FLUID_CORE_NAME\") + '.so']\n        }\n    else:\n        package_data = {\n            'paddle.base': [\n                env_dict.get(\"FLUID_CORE_NAME\") + '.pyd',\n                env_dict.get(\"FLUID_CORE_NAME\") + '.lib',\n            ]\n        }\n    package_data['paddle.base'] += [\n        paddle_binary_dir + '/python/paddle/cost_model/static_op_benchmark.json'\n    ]\n\n    whl_cinn_config_path = paddle_binary_dir + '/python/paddle/cinn_config'\n    src_cinn_config_path = (\n        env_dict.get(\"PADDLE_SOURCE_DIR\") + '/python/paddle/cinn_config'\n    )\n    package_data['paddle.cinn_config'] = []\n    if os.path.exists(whl_cinn_config_path):\n        shutil.rmtree(whl_cinn_config_path)\n    shutil.copytree(src_cinn_config_path, whl_cinn_config_path)\n    json_path_list = get_cinn_config_jsons()\n    for json in json_path_list:\n        package_data['paddle.cinn_config'] += [json]\n\n    if 'develop' in sys.argv:\n        package_dir = {'': 'python'}\n    else:\n        package_dir = {\n            '': env_dict.get(\"PADDLE_BINARY_DIR\") + '/python',\n            'paddle.base.proto.profiler': env_dict.get(\"PADDLE_BINARY_DIR\")\n            + '/paddle/fluid/platform',\n            'paddle.base.proto': env_dict.get(\"PADDLE_BINARY_DIR\")\n            + '/paddle/fluid/framework',\n            'paddle.base': env_dict.get(\"PADDLE_BINARY_DIR\")\n            + '/python/paddle/base',\n        }\n    # put all thirdparty libraries in paddle.libs\n    libs_path = paddle_binary_dir + '/python/paddle/libs'\n    package_data['paddle.libs'] = []\n    if env_dict.get(\"WITH_SHARED_PHI\") == \"ON\":\n        package_data['paddle.libs'] += [\n            ('libphi' if os.name != 'nt' else 'phi') + ext_suffix\n        ]\n        shutil.copy(env_dict.get(\"PHI_LIB\"), libs_path)\n        package_data['paddle.libs'] += [\n            ('libphi_core' if os.name != 'nt' else 'phi_core') + ext_suffix\n        ]\n        shutil.copy(env_dict.get(\"PHI_CORE_LIB\"), libs_path)\n        if (\n            env_dict.get(\"WITH_GPU\") == \"ON\"\n            or env_dict.get(\"WITH_ROCM\") == \"ON\"\n        ):\n            package_data['paddle.libs'] += [\n                ('libphi_gpu' if os.name != 'nt' else 'phi_gpu') + ext_suffix\n            ]\n            shutil.copy(env_dict.get(\"PHI_GPU_LIB\"), libs_path)\n\n    if env_dict.get(\"WITH_SHARED_IR\") == \"ON\":\n        package_data['paddle.libs'] += [\n            ('libpir' if os.name != 'nt' else 'pir') + ext_suffix\n        ]\n        shutil.copy(env_dict.get(\"IR_LIB\"), libs_path)\n\n    package_data['paddle.libs'] += [\n        ('libwarpctc' if os.name != 'nt' else 'warpctc') + ext_suffix,\n        ('libwarprnnt' if os.name != 'nt' else 'warprnnt') + ext_suffix,\n    ]\n    package_data['paddle.libs'] += [\n        ('libcommon' if os.name != 'nt' else 'common') + ext_suffix,\n    ]\n    shutil.copy(env_dict.get(\"COMMON_LIB\"), libs_path)\n    shutil.copy(env_dict.get(\"WARPCTC_LIBRARIES\"), libs_path)\n    shutil.copy(env_dict.get(\"WARPRNNT_LIBRARIES\"), libs_path)\n    package_data['paddle.libs'] += [\n        os.path.basename(env_dict.get(\"LAPACK_LIB\")),\n        os.path.basename(env_dict.get(\"BLAS_LIB\")),\n        os.path.basename(env_dict.get(\"GFORTRAN_LIB\")),\n        os.path.basename(env_dict.get(\"GNU_RT_LIB_1\")),\n    ]\n    shutil.copy(env_dict.get(\"BLAS_LIB\"), libs_path)\n    shutil.copy(env_dict.get(\"LAPACK_LIB\"), libs_path)\n    shutil.copy(env_dict.get(\"GFORTRAN_LIB\"), libs_path)\n    shutil.copy(env_dict.get(\"GNU_RT_LIB_1\"), libs_path)\n\n    if not sys.platform.startswith(\"linux\"):\n        package_data['paddle.libs'] += [\n            os.path.basename(env_dict.get(\"GNU_RT_LIB_2\"))\n        ]\n        shutil.copy(env_dict.get(\"GNU_RT_LIB_2\"), libs_path)\n    if env_dict.get(\"WITH_MKL\") == 'ON':\n        shutil.copy(env_dict.get(\"MKLML_SHARED_LIB\"), libs_path)\n        shutil.copy(env_dict.get(\"MKLML_SHARED_IOMP_LIB\"), libs_path)\n        package_data['paddle.libs'] += [\n            ('libmklml_intel' if os.name != 'nt' else 'mklml') + ext_suffix,\n            ('libiomp5' if os.name != 'nt' else 'libiomp5md') + ext_suffix,\n        ]\n    else:\n        if os.name == 'nt':\n            # copy the openblas.dll\n            shutil.copy(env_dict.get(\"OPENBLAS_SHARED_LIB\"), libs_path)\n            package_data['paddle.libs'] += ['openblas' + ext_suffix]\n        elif (\n            os.name == 'posix'\n            and platform.machine() == 'aarch64'\n            and env_dict.get(\"OPENBLAS_LIB\").endswith('so')\n        ):\n            # copy the libopenblas.so on linux+aarch64\n            # special: libpaddle.so without avx depends on 'libopenblas.so.0', not 'libopenblas.so'\n            if os.path.exists(env_dict.get(\"OPENBLAS_LIB\") + '.0'):\n                shutil.copy(env_dict.get(\"OPENBLAS_LIB\") + '.0', libs_path)\n                package_data['paddle.libs'] += ['libopenblas.so.0']\n\n    if env_dict.get(\"WITH_GPU\") == 'ON' or env_dict.get(\"WITH_ROCM\") == 'ON':\n        if len(env_dict.get(\"FLASHATTN_LIBRARIES\", \"\")) > 1:\n            package_data['paddle.libs'] += [\n                os.path.basename(env_dict.get(\"FLASHATTN_LIBRARIES\"))\n            ]\n            shutil.copy(env_dict.get(\"FLASHATTN_LIBRARIES\"), libs_path)\n        if len(env_dict.get(\"FLASHATTN_V3_LIBRARIES\", \"\")) > 1:\n            package_data['paddle.libs'] += [\n                os.path.basename(env_dict.get(\"FLASHATTN_V3_LIBRARIES\"))\n            ]\n            shutil.copy(env_dict.get(\"FLASHATTN_V3_LIBRARIES\"), libs_path)\n    if env_dict.get(\"WITH_CINN\") == 'ON':\n        shutil.copy(\n            env_dict.get(\"CINN_LIB_LOCATION\")\n            + '/'\n            + env_dict.get(\"CINN_LIB_NAME\"),\n            libs_path,\n        )\n        shutil.copy(\n            env_dict.get(\"CINN_INCLUDE_DIR\")\n            + '/paddle/cinn/runtime/cuda/cinn_cuda_runtime_source.cuh',\n            libs_path,\n        )\n        shutil.copy(\n            env_dict.get(\"CINN_INCLUDE_DIR\")\n            + '/paddle/cinn/runtime/hip/cinn_hip_runtime_source.h',\n            libs_path,\n        )\n        package_data['paddle.libs'] += ['libcinnapi.so']\n        package_data['paddle.libs'] += ['cinn_cuda_runtime_source.cuh']\n        package_data['paddle.libs'] += ['cinn_hip_runtime_source.h']\n\n        cinn_fp16_file = (\n            env_dict.get(\"CINN_INCLUDE_DIR\")\n            + '/paddle/cinn/runtime/cuda/float16.h'\n        )\n        if os.path.exists(cinn_fp16_file):\n            shutil.copy(cinn_fp16_file, libs_path)\n            package_data['paddle.libs'] += ['float16.h']\n        cinn_bf16_file = (\n            env_dict.get(\"CINN_INCLUDE_DIR\")\n            + '/paddle/cinn/runtime/cuda/bfloat16.h'\n        )\n        if os.path.exists(cinn_bf16_file):\n            shutil.copy(cinn_bf16_file, libs_path)\n            package_data['paddle.libs'] += ['bfloat16.h']\n\n        if env_dict.get(\"CMAKE_BUILD_TYPE\") == 'Release' and os.name != 'nt':\n            command = (\n                f\"patchelf --set-rpath '$ORIGIN/../../nvidia/cuda_nvrtc/lib/:$ORIGIN/../../nvidia/cuda_runtime/lib/:$ORIGIN/../../nvidia/cublas/lib/:$ORIGIN/../../nvidia/cudnn/lib/:$ORIGIN/../../nvidia/curand/lib/:$ORIGIN/../../nvidia/cusolver/lib/:$ORIGIN/../../nvidia/nvtx/lib/:$ORIGIN/' {libs_path}/\"\n                + env_dict.get(\"CINN_LIB_NAME\")\n            )\n            if os.system(command) != 0:\n                raise Exception(\n                    'patch '\n                    + libs_path\n                    + '/'\n                    + env_dict.get(\"CINN_LIB_NAME\")\n                    + ' failed',\n                    f'command: {command}',\n                )\n    if env_dict.get(\"WITH_PSLIB\") == 'ON':\n        shutil.copy(env_dict.get(\"PSLIB_LIB\"), libs_path)\n        shutil.copy(env_dict.get(\"JVM_LIB\"), libs_path)\n        if os.path.exists(env_dict.get(\"PSLIB_VERSION_PY\")):\n            shutil.copy(\n                env_dict.get(\"PSLIB_VERSION_PY\"),\n                paddle_binary_dir\n                + '/python/paddle/incubate/distributed/fleet/parameter_server/pslib/',\n            )\n        package_data['paddle.libs'] += ['libps' + ext_suffix]\n        package_data['paddle.libs'] += ['libjvm' + ext_suffix]\n    if env_dict.get(\"WITH_ONEDNN\") == 'ON':\n        if env_dict.get(\"CMAKE_BUILD_TYPE\") == 'Release' and os.name != 'nt':\n            # only change rpath in Release mode.\n            # TODO(typhoonzero): use install_name_tool to patch mkl libs once\n            # we can support mkl on mac.\n            #\n            # change rpath of libdnnl.so.1, add $ORIGIN/ to it.\n            # The reason is that all thirdparty libraries in the same directory,\n            # thus, libdnnl.so.1 will find libmklml_intel.so and libiomp5.so.\n            command = \"patchelf --set-rpath '$ORIGIN/' \" + env_dict.get(\n                \"ONEDNN_SHARED_LIB\"\n            )\n            if os.system(command) != 0:\n                raise Exception(f\"patch libdnnl.so failed, command: {command}\")\n        shutil.copy(env_dict.get(\"ONEDNN_SHARED_LIB\"), libs_path)\n        if os.name != 'nt':\n            package_data['paddle.libs'] += ['libdnnl.so.3']\n        else:\n            package_data['paddle.libs'] += ['mkldnn.dll']\n\n    if env_dict.get(\"WITH_ONNXRUNTIME\") == 'ON':\n        shutil.copy(env_dict.get(\"ONNXRUNTIME_SHARED_LIB\"), libs_path)\n        shutil.copy(env_dict.get(\"PADDLE2ONNX_LIB\"), libs_path)\n        if os.name == 'nt':\n            package_data['paddle.libs'] += [\n                'paddle2onnx.dll',\n                'onnxruntime.dll',\n            ]\n        else:\n            package_data['paddle.libs'] += [\n                env_dict.get(\"PADDLE2ONNX_LIB_NAME\"),\n                env_dict.get(\"ONNXRUNTIME_LIB_NAME\"),\n            ]\n\n    if env_dict.get(\"WITH_OPENVINO\") == 'ON':\n        shutil.copy(env_dict.get(\"OPENVINO_LIB\"), libs_path)\n        shutil.copy(env_dict.get(\"TBB_LIB\"), libs_path)\n        shutil.copy(env_dict.get(\"OPENVINO_PADDLE_LIB\"), libs_path)\n        shutil.copy(env_dict.get(\"OPENVINO_CPU_PLUGIN_LIB\"), libs_path)\n        if os.name != 'nt':\n            package_data['paddle.libs'] += [\n                'libopenvino.so.2500',\n                'libtbb.so.12',\n                'libopenvino_paddle_frontend.so.2500',\n                'libopenvino_intel_cpu_plugin.so',\n            ]\n        else:\n            package_data['paddle.libs'] += [\n                'openvino.dll',\n                'tbb.dll',\n                'openvino_paddle_frontend.dll',\n                'openvino_intel_cpu_plugin.dll',\n            ]\n\n    if env_dict.get(\"WITH_XPU\") == 'ON':\n        shutil.copy(env_dict.get(\"XPU_API_LIB\"), libs_path)\n        package_data['paddle.libs'] += [env_dict.get(\"XPU_API_LIB_NAME\")]\n        xpu_rt_lib_list = glob.glob(env_dict.get(\"XPU_RT_LIB\") + '*')\n        for xpu_rt_lib_file in xpu_rt_lib_list:\n            shutil.copy(xpu_rt_lib_file, libs_path)\n            package_data['paddle.libs'] += [os.path.basename(xpu_rt_lib_file)]\n        xpu_cuda_lib_list = glob.glob(env_dict.get(\"XPU_CUDA_LIB\") + '*')\n        for xpu_cuda_lib_file in xpu_cuda_lib_list:\n            shutil.copy(xpu_cuda_lib_file, libs_path)\n            package_data['paddle.libs'] += [os.path.basename(xpu_cuda_lib_file)]\n        if env_dict.get(\"WITH_XPU_XRE5\") == 'ON':\n            xpu_cuda_rt_lib_list = glob.glob(\n                env_dict.get(\"XPU_CUDA_RT_LIB\") + '*'\n            )\n            for xpu_cuda_rt_lib_file in xpu_cuda_rt_lib_list:\n                shutil.copy(xpu_cuda_rt_lib_file, libs_path)\n                package_data['paddle.libs'] += [\n                    os.path.basename(xpu_cuda_rt_lib_file)\n                ]\n            xpu_ml_lib_list = glob.glob(env_dict.get(\"XPU_ML_LIB\") + '*')\n            for xpu_ml_lib_file in xpu_ml_lib_list:\n                shutil.copy(xpu_ml_lib_file, libs_path)\n                package_data['paddle.libs'] += [\n                    os.path.basename(xpu_ml_lib_file)\n                ]\n            shutil.copy(env_dict.get(\"XPU_XBLAS_LIB\"), libs_path)\n            package_data['paddle.libs'] += [env_dict.get(\"XPU_XBLAS_LIB_NAME\")]\n            shutil.copy(env_dict.get(\"XPU_XFA_LIB\"), libs_path)\n            package_data['paddle.libs'] += [env_dict.get(\"XPU_XFA_LIB_NAME\")]\n            shutil.copy(env_dict.get(\"XPU_XPUDNN_LIB\"), libs_path)\n            package_data['paddle.libs'] += [env_dict.get(\"XPU_XPUDNN_LIB_NAME\")]\n\n    if env_dict.get(\"WITH_XPU_BKCL\") == 'ON':\n        shutil.copy(env_dict.get(\"XPU_BKCL_LIB\"), libs_path)\n        package_data['paddle.libs'] += [env_dict.get(\"XPU_BKCL_LIB_NAME\")]\n\n    if env_dict.get(\"WITH_XPU_XFT\") == 'ON':\n        shutil.copy(env_dict.get(\"XPU_XFT_LIB\"), libs_path)\n        package_data['paddle.libs'] += [env_dict.get(\"XPU_XFT_LIB_NAME\")]\n\n    if env_dict.get(\"WITH_XPTI\") == 'ON':\n        shutil.copy(env_dict.get(\"XPU_XPTI_LIB\"), libs_path)\n        package_data['paddle.libs'] += [env_dict.get(\"XPU_XPTI_LIB_NAME\")]\n\n    # remove unused paddle/libs/__init__.py\n    if os.path.isfile(libs_path + '/__init__.py'):\n        os.remove(libs_path + '/__init__.py')\n    package_dir['paddle.libs'] = libs_path\n\n    # change rpath of ${FLUID_CORE_NAME}.ext, add $ORIGIN/../libs/ to it.\n    # The reason is that libwarpctc.ext, libwarprnnt.ext, libiomp5.ext etc are in paddle.libs, and\n    # ${FLUID_CORE_NAME}.ext is in paddle.base, thus paddle/fluid/../libs will pointer to above libraries.\n    # This operation will fix https://github.com/PaddlePaddle/Paddle/issues/3213\n    if env_dict.get(\"CMAKE_BUILD_TYPE\") == 'Release':\n        if os.name != 'nt':\n            # only change rpath in Release mode, since in Debug mode, ${FLUID_CORE_NAME}.xx is too large to be changed.\n            if env_dict.get(\"APPLE\") == \"1\":\n                commands = [\n                    \"install_name_tool -id '@loader_path/../libs/' \"\n                    + env_dict.get(\"PADDLE_BINARY_DIR\")\n                    + '/python/paddle/base/'\n                    + env_dict.get(\"FLUID_CORE_NAME\")\n                    + '.so'\n                ]\n                commands.append(\n                    \"install_name_tool -add_rpath '@loader_path/../libs/' \"\n                    + env_dict.get(\"PADDLE_BINARY_DIR\")\n                    + '/python/paddle/base/'\n                    + env_dict.get(\"FLUID_CORE_NAME\")\n                    + '.so'\n                )\n                commands.append(\n                    \"install_name_tool -add_rpath '@loader_path/../libs/' \"\n                    + env_dict.get(\"PADDLE_BINARY_DIR\")\n                    + '/python/paddle/libs/'\n                    + env_dict.get(\"COMMON_NAME\")\n                )\n                if env_dict.get(\"WITH_SHARED_PHI\") == \"ON\":\n                    commands.append(\n                        \"install_name_tool -add_rpath '@loader_path' \"\n                        + env_dict.get(\"PADDLE_BINARY_DIR\")\n                        + '/python/paddle/libs/'\n                        + env_dict.get(\"PHI_NAME\")\n                    )\n                    commands.append(\n                        \"install_name_tool -add_rpath '@loader_path' \"\n                        + env_dict.get(\"PADDLE_BINARY_DIR\")\n                        + '/python/paddle/libs/'\n                        + env_dict.get(\"PHI_CORE_NAME\")\n                    )\n                    if (\n                        env_dict.get(\"WITH_GPU\") == \"ON\"\n                        or env_dict.get(\"WITH_ROCM\") == \"ON\"\n                    ):\n                        commands.append(\n                            \"install_name_tool -add_rpath '@loader_path' \"\n                            + env_dict.get(\"PADDLE_BINARY_DIR\")\n                            + '/python/paddle/libs/'\n                            + env_dict.get(\"PHI_GPU_NAME\")\n                        )\n                if env_dict.get(\"WITH_SHARED_IR\") == \"ON\":\n                    commands.append(\n                        \"install_name_tool -add_rpath '@loader_path' \"\n                        + env_dict.get(\"PADDLE_BINARY_DIR\")\n                        + '/python/paddle/libs/'\n                        + env_dict.get(\"IR_NAME\")\n                    )\n            else:\n                commands = [\n                    \"patchelf --set-rpath '$ORIGIN/../../nvidia/cuda_runtime/lib:$ORIGIN/../../nvidia/cuda_nvrtc/lib:$ORIGIN/../../nvidia/cublas/lib:$ORIGIN/../../nvidia/cudnn/lib:$ORIGIN/../../nvidia/curand/lib:$ORIGIN/../../nvidia/cusparse/lib:$ORIGIN/../../nvidia/nvjitlink/lib:$ORIGIN/../../nvidia/cuda_cupti/lib:$ORIGIN/../../nvidia/cuda_runtime/lib:$ORIGIN/../../nvidia/cufft/lib:$ORIGIN/../../nvidia/cufft/lib:$ORIGIN/../../nvidia/cusolver/lib:$ORIGIN/../../nvidia/nccl/lib:$ORIGIN/../../nvidia/nvtx/lib:$ORIGIN/../libs/' \"\n                    + env_dict.get(\"PADDLE_BINARY_DIR\")\n                    + '/python/paddle/base/'\n                    + env_dict.get(\"FLUID_CORE_NAME\")\n                    + '.so'\n                ]\n                if env_dict.get(\"WITH_SHARED_PHI\") == \"ON\":\n                    commands.append(\n                        \"patchelf --set-rpath '$ORIGIN/../../nvidia/cuda_runtime/lib:$ORIGIN:$ORIGIN/../libs' \"\n                        + env_dict.get(\"PADDLE_BINARY_DIR\")\n                        + '/python/paddle/libs/'\n                        + env_dict.get(\"PHI_NAME\")\n                    )\n                    commands.append(\n                        \"patchelf --set-rpath '$ORIGIN/../../nvidia/cuda_runtime/lib:$ORIGIN:$ORIGIN/../libs' \"\n                        + env_dict.get(\"PADDLE_BINARY_DIR\")\n                        + '/python/paddle/libs/'\n                        + env_dict.get(\"PHI_CORE_NAME\")\n                    )\n                    if (\n                        env_dict.get(\"WITH_GPU\") == \"ON\"\n                        or env_dict.get(\"WITH_ROCM\") == \"ON\"\n                    ):\n                        commands.append(\n                            \"patchelf --set-rpath '$ORIGIN/../../nvidia/cuda_runtime/lib:$ORIGIN:$ORIGIN/../libs' \"\n                            + env_dict.get(\"PADDLE_BINARY_DIR\")\n                            + '/python/paddle/libs/'\n                            + env_dict.get(\"PHI_GPU_NAME\")\n                        )\n\n                if env_dict.get(\"WITH_SHARED_IR\") == \"ON\":\n                    commands.append(\n                        \"patchelf --set-rpath '$ORIGIN:$ORIGIN/../libs' \"\n                        + env_dict.get(\"PADDLE_BINARY_DIR\")\n                        + '/python/paddle/libs/'\n                        + env_dict.get(\"IR_NAME\")\n                    )\n            # The sw_64 not support patchelf, so we just disable that.\n            if platform.machine() != 'sw_64' and platform.machine() != 'mips64':\n                for command in commands:\n                    if os.system(command) != 0:\n                        raise Exception(\n                            'patch '\n                            + env_dict.get(\"FLUID_CORE_NAME\")\n                            + f'{ext_suffix} failed',\n                            f'command: {command}',\n                        )\n    # A list of extensions that specify c++ -written modules that compile source code into dynamically linked libraries\n    ext_modules = [Extension('_foo', [paddle_binary_dir + '/python/stub.cc'])]\n    if os.name == 'nt':\n        # fix the path separator under windows\n        fix_package_dir = {}\n        for k, v in package_dir.items():\n            fix_package_dir[k] = v.replace('/', '\\\\')\n        package_dir = fix_package_dir\n        ext_modules = []\n    elif sys.platform == 'darwin':\n        ext_modules = []\n\n    return package_data, package_dir, ext_modules\n\n\ndef get_headers():\n    headers = (\n        # paddle level api headers (high level api, for both training and inference)\n        list(find_files('*.h', paddle_source_dir + '/paddle'))\n        + list(find_files('*.h', paddle_source_dir + '/paddle/phi/api'))\n        + list(  # phi unify api header\n            find_files('*.h', paddle_source_dir + '/paddle/phi/api/ext')\n        )\n        + list(  # custom op api\n            find_files('*.h', paddle_source_dir + '/paddle/phi/api/include')\n        )\n        + list(  # phi api\n            find_files('*.h', paddle_source_dir + '/paddle/phi/common')\n        )\n        + list(  # common api\n            find_files('*.h', paddle_source_dir + '/paddle/common')\n        )\n        # phi level api headers (low level api, for training only)\n        + list(  # phi extension header\n            find_files('*.h', paddle_source_dir + '/paddle/phi')\n        )\n        + list(  # phi include header\n            find_files(\n                '*.h', paddle_source_dir + '/paddle/phi/include', recursive=True\n            )\n        )\n        + list(  # phi backends headers\n            find_files(\n                '*.h',\n                paddle_source_dir + '/paddle/phi/backends',\n                recursive=True,\n            )\n        )\n        + list(  # phi core headers\n            find_files(\n                '*.h', paddle_source_dir + '/paddle/phi/core', recursive=True\n            )\n        )\n        + list(  # phi infermeta headers\n            find_files(\n                '*.h',\n                paddle_source_dir + '/paddle/phi/infermeta',\n                recursive=True,\n            )\n        )\n        + list(  # phi kernel headers\n            find_files(\n                '*.h',\n                paddle_source_dir + '/paddle/phi/kernels',\n                recursive=True,\n            )\n        )\n        # phi capi headers\n        + list(\n            find_files(\n                '*.h', paddle_source_dir + '/paddle/phi/capi', recursive=True\n            )\n        )\n        + list(  # utils api headers\n            find_files(\n                '*.h', paddle_source_dir + '/paddle/utils', recursive=True\n            )\n        )\n        + list(  # phi profiler headers\n            find_files(\n                '*.h',\n                paddle_source_dir + '/paddle/phi/api/profiler',\n                recursive=True,\n            )\n        )\n        + list(  # phi init headers\n            find_files(\n                'init_phi.h',\n                paddle_source_dir + '/paddle/fluid/platform',\n                recursive=True,\n            )\n        )\n        + list(  # pir init headers\n            find_files(\n                '*.h',\n                paddle_source_dir + '/paddle/pir/include',\n                recursive=True,\n            )\n        )\n        + list(  # drr init headers\n            find_files(\n                '*.h',\n                paddle_source_dir + '/paddle/fluid/pir/drr/include',\n                recursive=True,\n            )\n        )\n        + list(  # drr init headers\n            find_files(\n                '*.h',\n                paddle_source_dir\n                + '/paddle/fluid/pir/dialect/operator/interface/infer_symbolic_shape',\n                recursive=True,\n            )\n        )\n        + list(  # operator init headers\n            find_files(\n                '*.h',\n                paddle_source_dir + '/paddle/fluid/pir/dialect/operator/ir',\n            )\n        )\n        + list(  # pass utils init headers\n            find_files(\n                'general_functions.h',\n                paddle_source_dir + '/paddle/fluid/pir/utils',\n            )\n        )\n        + list(  # serialize and deserialize interface headers\n            find_files(\n                'interface.h',\n                paddle_source_dir\n                + '/paddle/fluid/pir/serialize_deserialize/include',\n            )\n        )\n        + list(  # serialize and deserialize interface headers\n            find_files(\n                'dense_tensor.inl',\n                paddle_source_dir + '/paddle/phi/core',\n            )\n        )\n        + list(  # serialize and deserialize interface headers\n            find_files(\n                'op_yaml_info.h',\n                paddle_source_dir\n                + '/paddle/fluid/pir/dialect/operator/interface',\n            )\n        )\n        + list(  # serialize and deserialize interface headers\n            find_files(\n                'op_yaml_info_util.h',\n                paddle_source_dir + '/paddle/fluid/pir/dialect/operator/utils',\n            )\n        )\n        + list(  # op yaml parser interface headers\n            find_files(\n                'op_yaml_info_parser.h',\n                paddle_source_dir + '/paddle/fluid/pir/dialect/operator/utils',\n            )\n        )\n        + list(  # pir op utils interface headers\n            find_files(\n                'utils.h',\n                paddle_source_dir + '/paddle/fluid/pir/dialect/operator/utils',\n            )\n        )\n        + list(  # ir translator interface headers\n            find_files(\n                'op_compat_info.h',\n                paddle_source_dir + '/paddle/fluid/ir_adaptor/translator/',\n            )\n        )\n        + list(\n            find_files(\n                'infer_symbolic_shape.h',\n                paddle_source_dir\n                + '/paddle/fluid/pir/dialect/operator/interface/infer_symbolic_shape',\n            )\n        )\n        + list(\n            find_files(\n                'vjp.h',\n                paddle_source_dir\n                + '/paddle/fluid/pir/dialect/operator/interface',\n            )\n        )\n        + list(\n            find_files(\n                'lexer.h',\n                paddle_source_dir + '/paddle/pir/src/core/parser',\n            )\n        )\n        + list(\n            find_files(\n                'token.h',\n                paddle_source_dir + '/paddle/pir/src/core/parser',\n            )\n        )\n    )\n\n    jit_layer_headers = [\n        'layer.h',\n        'serializer.h',\n        'serializer_utils.h',\n        'all.h',\n        'function.h',\n    ]\n\n    for f in jit_layer_headers:\n        headers += list(\n            find_files(\n                f, paddle_source_dir + '/paddle/fluid/jit', recursive=True\n            )\n        )\n\n    if env_dict.get(\"WITH_ONEDNN\") == 'ON':\n        headers += list(\n            find_files('*', env_dict.get(\"ONEDNN_INSTALL_DIR\") + '/include')\n        )  # mkldnn\n\n    if env_dict.get(\"WITH_OPENVINO\") == 'ON':\n        headers += list(\n            find_files('*', env_dict.get(\"OPENVINO_INC_DIR\"))\n        )  # openvino\n        headers += list(\n            find_files('*', env_dict.get(\"TBB_INC_DIR\"))\n        )  # openvino\n\n    if env_dict.get(\"WITH_GPU\") == 'ON' or env_dict.get(\"WITH_ROCM\") == 'ON':\n        # externalErrorMsg.pb for External Error message\n        headers += list(\n            find_files('*.pb', env_dict.get(\"externalError_INCLUDE_DIR\"))\n        )\n\n    if env_dict.get(\"WITH_XPU\") == 'ON':\n        headers += list(\n            find_files(\n                '*.h',\n                paddle_binary_dir + '/third_party/xpu/src/extern_xpu/xpu',\n                recursive=True,\n            )\n        )  # xdnn api headers\n        headers += list(\n            find_files(\n                '*.hpp',\n                paddle_binary_dir + '/third_party/xpu/src/extern_xpu/xpu',\n                recursive=True,\n            )\n        )  # xre headers with .hpp extension\n\n    # pybind headers\n    headers += list(find_files('*.h', env_dict.get(\"PYBIND_INCLUDE_DIR\"), True))\n    return headers\n\n\ndef get_setup_parameters():\n    # get setup_requires\n    setup_requires = get_setup_requires()\n    if (\n        env_dict.get(\"WITH_GPU\") == 'ON'\n        and platform.system() in ('Linux', 'Windows')\n        and platform.machine()\n        in (\n            'x86_64',\n            'AMD64',\n        )\n    ):\n        paddle_cuda_requires, paddle_tensorrt_requires = (\n            get_paddle_extra_install_requirements()\n        )\n        setup_requires += paddle_cuda_requires\n        setup_requires += paddle_tensorrt_requires\n\n    packages = [\n        'paddle',\n        'paddle.libs',\n        'paddle.utils',\n        'paddle.utils.gast',\n        'paddle.utils.cpp_extension',\n        'paddle.dataset',\n        'paddle.reader',\n        'paddle.distributed',\n        'paddle.distributed.checkpoint',\n        'paddle.distributed.communication',\n        'paddle.distributed.communication.stream',\n        'paddle.distributed.metric',\n        'paddle.distributed.ps',\n        'paddle.distributed.ps.utils',\n        'paddle.incubate',\n        'paddle.incubate.autograd',\n        'paddle.incubate.optimizer',\n        'paddle.incubate.checkpoint',\n        'paddle.incubate.operators',\n        'paddle.incubate.tensor',\n        'paddle.incubate.multiprocessing',\n        'paddle.incubate.nn',\n        'paddle.incubate.jit',\n        'paddle.incubate.asp',\n        'paddle.incubate.passes',\n        'paddle.incubate.framework',\n        'paddle.distribution',\n        'paddle.distributed.utils',\n        'paddle.distributed.sharding',\n        'paddle.distributed.fleet',\n        'paddle.distributed.auto_tuner',\n        'paddle.distributed.launch',\n        'paddle.distributed.launch.context',\n        'paddle.distributed.launch.controllers',\n        'paddle.distributed.launch.job',\n        'paddle.distributed.launch.plugins',\n        'paddle.distributed.launch.utils',\n        'paddle.distributed.fleet.base',\n        'paddle.distributed.fleet.recompute',\n        'paddle.distributed.fleet.elastic',\n        'paddle.distributed.fleet.meta_optimizers',\n        'paddle.distributed.fleet.meta_optimizers.sharding',\n        'paddle.distributed.fleet.meta_optimizers.dygraph_optimizer',\n        'paddle.distributed.fleet.runtime',\n        'paddle.distributed.rpc',\n        'paddle.distributed.fleet.dataset',\n        'paddle.distributed.fleet.data_generator',\n        'paddle.distributed.fleet.metrics',\n        'paddle.distributed.fleet.proto',\n        'paddle.distributed.fleet.utils',\n        'paddle.distributed.fleet.layers',\n        'paddle.distributed.fleet.layers.mpu',\n        'paddle.distributed.fleet.meta_parallel',\n        'paddle.distributed.fleet.meta_parallel.pp_utils',\n        'paddle.distributed.fleet.meta_parallel.sharding',\n        'paddle.distributed.fleet.meta_parallel.parallel_layers',\n        'paddle.distributed.auto_parallel',\n        'paddle.distributed.auto_parallel.intermediate',\n        'paddle.distributed.auto_parallel.dygraph',\n        'paddle.distributed.auto_parallel.static',\n        'paddle.distributed.auto_parallel.static.operators',\n        'paddle.distributed.auto_parallel.static.tuner',\n        'paddle.distributed.auto_parallel.static.cost',\n        'paddle.distributed.auto_parallel.static.reshard_funcs',\n        'paddle.distributed.passes',\n        'paddle.distributed.passes.pipeline_scheduler_pass',\n        'paddle.distributed.models',\n        'paddle.distributed.models.moe',\n        'paddle.distributed.transpiler',\n        'paddle.distributed.transpiler.details',\n        'paddle.framework',\n        'paddle.jit',\n        'paddle.jit.dy2static',\n        'paddle.jit.dy2static.transformers',\n        'paddle.jit.pir_dy2static',\n        'paddle.jit.sot',\n        'paddle.jit.sot.opcode_translator',\n        'paddle.jit.sot.opcode_translator.executor',\n        'paddle.jit.sot.opcode_translator.executor.variables',\n        'paddle.jit.sot.opcode_translator.instruction_utils',\n        'paddle.jit.sot.profiler',\n        'paddle.jit.sot.symbolic',\n        'paddle.jit.sot.utils',\n        'paddle.inference',\n        'paddle.inference.contrib',\n        'paddle.inference.contrib.utils',\n        'paddle.base',\n        'paddle.base.dygraph',\n        'paddle.base.proto',\n        'paddle.base.proto.profiler',\n        'paddle.base.layers',\n        'paddle.base.incubate',\n        'paddle.incubate.distributed.fleet',\n        'paddle.base.incubate.checkpoint',\n        'paddle.amp',\n        'paddle.cost_model',\n        'paddle.cinn_config',\n        'paddle.hapi',\n        'paddle.vision',\n        'paddle.vision.models',\n        'paddle.vision.transforms',\n        'paddle.vision.datasets',\n        'paddle.audio',\n        'paddle.audio.functional',\n        'paddle.audio.features',\n        'paddle.audio.datasets',\n        'paddle.audio.backends',\n        'paddle.text',\n        'paddle.text.datasets',\n        'paddle.incubate',\n        'paddle.incubate.nn',\n        'paddle.incubate.jit',\n        'paddle.incubate.nn.functional',\n        'paddle.incubate.nn.layer',\n        'paddle.incubate.optimizer.functional',\n        'paddle.incubate.autograd',\n        'paddle.incubate.distributed',\n        'paddle.incubate.distributed.utils',\n        'paddle.incubate.distributed.utils.io',\n        'paddle.incubate.distributed.fleet',\n        'paddle.incubate.distributed.models',\n        'paddle.incubate.distributed.models.moe',\n        'paddle.incubate.distributed.models.moe.gate',\n        'paddle.incubate.distributed.fleet.parameter_server',\n        'paddle.incubate.distributed.fleet.parameter_server.distribute_transpiler',\n        'paddle.incubate.distributed.fleet.parameter_server.ir',\n        'paddle.incubate.distributed.fleet.parameter_server.pslib',\n        'paddle.incubate.layers',\n        'paddle.quantization',\n        'paddle.quantization.quanters',\n        'paddle.quantization.observers',\n        'paddle.sparse',\n        'paddle.sparse.nn',\n        'paddle.sparse.nn.layer',\n        'paddle.sparse.nn.functional',\n        'paddle.incubate.xpu',\n        'paddle.io',\n        'paddle.io.dataloader',\n        'paddle.optimizer',\n        'paddle.nn',\n        'paddle.nn.functional',\n        'paddle.nn.layer',\n        'paddle.nn.quant',\n        'paddle.nn.quant.qat',\n        'paddle.nn.initializer',\n        'paddle.nn.utils',\n        'paddle.metric',\n        'paddle.static',\n        'paddle.static.nn',\n        'paddle.static.amp',\n        'paddle.static.amp.bf16',\n        'paddle.static.quantization',\n        'paddle.quantization',\n        'paddle.quantization.imperative',\n        'paddle.tensor',\n        'paddle.onnx',\n        'paddle.autograd',\n        'paddle.device',\n        'paddle.device.cuda',\n        'paddle.device.xpu',\n        'paddle.version',\n        'paddle.profiler',\n        'paddle.geometric',\n        'paddle.geometric.message_passing',\n        'paddle.geometric.sampling',\n        'paddle.pir',\n        'paddle.decomposition',\n        'paddle._typing',\n        'paddle._typing.libs',\n        'paddle.tensorrt',\n    ]\n\n    paddle_bins = ''\n    if not env_dict.get(\"WIN32\"):\n        paddle_bins = [\n            env_dict.get(\"PADDLE_BINARY_DIR\") + '/paddle/scripts/paddle'\n        ]\n    package_data, package_dir, ext_modules = get_package_data_and_package_dir()\n    headers = get_headers()\n    return (\n        setup_requires,\n        packages,\n        paddle_bins,\n        package_data,\n        package_dir,\n        ext_modules,\n        headers,\n    )\n\n\ndef check_build_dependency():\n    missing_modules = '''Missing build dependency: {dependency}\nPlease run 'pip install -r python/requirements.txt' to make sure you have all the dependencies installed.\n'''.strip()\n\n    with open(TOP_DIR + '/python/requirements.txt') as f:\n        build_dependencies = (\n            f.read().splitlines()\n        )  # Specify the dependencies to install\n\n    python_dependencies_module = []\n    installed_packages = []\n\n    for dependency in build_dependencies:\n        python_dependencies_module.append(\n            re.sub(\"_|-\", '', re.sub(r\"==.*|>=.*|<=.*\", '', dependency))\n        )\n    reqs = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze'])\n\n    for r in reqs.split():\n        installed_packages.append(\n            re.sub(\"_|-\", '', r.decode().split('==')[0]).lower()\n        )\n\n    for dependency in python_dependencies_module:\n        if dependency.lower() not in installed_packages:\n            raise RuntimeError(missing_modules.format(dependency=dependency))\n\n\ndef install_cpp_dist_and_build_test(install_dir, lib_test_dir, headers, libs):\n    \"\"\"install cpp distribution and build test target\n\n    TODO(huangjiyi):\n    1. This function will be moved when separating C++ distribution\n    installation from python package installation.\n    2. Reduce the header and library files to be installed.\n    \"\"\"\n    if env_dict.get(\"CMAKE_BUILD_TYPE\") != 'Release':\n        return\n    os.makedirs(install_dir, exist_ok=True)\n    # install C++ header files\n    for header in headers:\n        header_install_dir = get_header_install_dir(header)\n        header_install_dir = os.path.join(\n            install_dir, 'include', os.path.dirname(header_install_dir)\n        )\n        os.makedirs(header_install_dir, exist_ok=True)\n        shutil.copy(header, header_install_dir)\n\n    # install C++ shared libraries\n    lib_install_dir = os.path.join(install_dir, 'lib')\n    os.makedirs(lib_install_dir, exist_ok=True)\n    # install libpaddle.ext\n    paddle_libs = glob.glob(\n        paddle_binary_dir\n        + '/paddle/fluid/pybind/'\n        + env_dict.get(\"FLUID_CORE_NAME\")\n        + '.*'\n    )\n    for lib in paddle_libs:\n        shutil.copy(lib, lib_install_dir)\n    # install dependent libraries\n    libs_path = paddle_binary_dir + '/python/paddle/libs'\n    for lib in libs:\n        lib_path = os.path.join(libs_path, lib)\n        shutil.copy(lib_path, lib_install_dir)\n\n    # build test target\n    cmake_args = [CMAKE, lib_test_dir, \"-B\", lib_test_dir]\n    if os.getenv(\"GENERATOR\") == \"Ninja\":\n        cmake_args.append(\"-GNinja\")\n    subprocess.check_call(cmake_args)\n    subprocess.check_call([CMAKE, \"--build\", lib_test_dir])\n\n\ndef check_submodules():\n    def get_submodule_folder():\n        git_submodules_path = os.path.join(TOP_DIR, \".gitmodules\")\n        with open(git_submodules_path) as f:\n            return [\n                os.path.join(TOP_DIR, line.split(\"=\", 1)[1].strip())\n                for line in f\n                if line.strip().startswith(\"path\")\n            ]\n\n    def submodules_not_exists_or_empty(folder):\n        return not os.path.exists(folder) or (\n            os.path.isdir(folder) and len(os.listdir(folder)) == 0\n        )\n\n    submodule_folders = get_submodule_folder()\n    # f none of the submodule folders exists, try to initialize them\n    if any(\n        submodules_not_exists_or_empty(folder) for folder in submodule_folders\n    ):\n        try:\n            print(' --- Trying to initialize submodules')\n            start = time.time()\n            subprocess.check_call(\n                [\"git\", \"submodule\", \"update\", \"--init\", \"--recursive\"],\n                cwd=TOP_DIR,\n            )\n            end = time.time()\n            print(f' --- Submodule initialization took {end - start:.2f} sec')\n        except Exception:\n            print(' --- Submodule initialization failed')\n            print('Please run:\\n\\tgit submodule update --init --recursive')\n            sys.exit(1)\n\n\ndef generate_stub_files(paddle_binary_dir, paddle_source_dir):\n    script_path = paddle_source_dir + '/tools/'\n    sys.path.append(script_path)\n\n    print('-' * 2, 'Generate stub file tensor.pyi ... ')\n    import gen_tensor_stub\n\n    gen_tensor_stub.generate_stub_file(\n        input_file=paddle_source_dir\n        + '/python/paddle/tensor/tensor.prototype.pyi',\n        output_file=paddle_binary_dir + '/python/paddle/tensor/tensor.pyi',\n    )\n\n    shutil.copy(\n        paddle_binary_dir + '/python/paddle/tensor/tensor.pyi',\n        paddle_source_dir + '/python/paddle/tensor/tensor.pyi',\n    )\n    print('-' * 2, 'End Generate stub file tensor.pyi ... ')\n\n    print('-' * 2, 'Generate stub file for python binding APIs ... ')\n    import gen_pybind11_stub\n\n    gen_pybind11_stub.generate_stub_file(\n        output_dir=str(Path(paddle_binary_dir) / 'python/paddle/_typing/libs/'),\n        module_name='paddle.base.libpaddle',\n        ignore_all_errors=True,\n        ops_yaml=[\n            paddle_source_dir\n            + \"/paddle/phi/ops/yaml/ops.yaml;paddle.base.libpaddle.eager.ops\",\n            paddle_source_dir\n            + \"/paddle/phi/ops/yaml/ops.yaml;paddle.base.libpaddle.pir.ops\",\n            paddle_source_dir\n            + \"/paddle/phi/ops/yaml/sparse_ops.yaml;paddle.base.libpaddle.eager.ops;sparse\",\n            paddle_source_dir\n            + \"/paddle/phi/ops/yaml/sparse_ops.yaml;paddle.base.libpaddle.pir.ops;sparse\",\n            paddle_source_dir\n            + \"/paddle/phi/ops/yaml/strings_ops.yaml;paddle.base.libpaddle.eager.ops;strings\",\n            paddle_source_dir\n            + \"/paddle/phi/ops/yaml/strings_ops.yaml;paddle.base.libpaddle.pir.ops;strings\",\n        ],\n    )\n\n    libpaddle_dst = paddle_source_dir + '/python/paddle/_typing/libs/libpaddle'\n    if Path(libpaddle_dst).exists():\n        shutil.rmtree(libpaddle_dst)\n\n    shutil.copytree(\n        paddle_binary_dir + '/python/paddle/_typing/libs/libpaddle',\n        libpaddle_dst,\n    )\n\n    print('-' * 2, 'End Generate stub for python binding APIs ... ')\n\n\ndef main():\n    # Parse the command line and check arguments before we proceed with building steps and setup\n    parse_input_command(filter_args_list)\n\n    # check build dependency\n    check_build_dependency()\n    check_submodules()\n    # Execute the build process,cmake and make\n    if cmake_and_build:\n        build_steps()\n\n    if os.getenv(\"WITH_PYTHON\") == \"OFF\":\n        print(\"only compile, not package\")\n        return\n\n    build_dir = os.getenv(\"BUILD_DIR\")\n    if build_dir is not None:\n        env_dict_path = TOP_DIR + '/' + build_dir + '/python'\n    else:\n        env_dict_path = TOP_DIR + \"/build/python/\"\n    sys.path.insert(1, env_dict_path)\n    from env_dict import env_dict\n\n    global env_dict  # noqa: F811\n    global paddle_binary_dir, paddle_source_dir\n\n    paddle_binary_dir = env_dict.get(\"PADDLE_BINARY_DIR\")\n    paddle_source_dir = env_dict.get(\"PADDLE_SOURCE_DIR\")\n\n    # preparing parameters for setup()\n    paddle_version = env_dict.get(\"PADDLE_VERSION\")\n    package_name = env_dict.get(\"PACKAGE_NAME\")\n\n    write_version_py(\n        filename=f'{paddle_binary_dir}/python/paddle/version/__init__.py'\n    )\n    write_cuda_env_config_py(\n        filename=f'{paddle_binary_dir}/python/paddle/cuda_env.py'\n    )\n    write_parameter_server_version_py(\n        filename=f'{paddle_binary_dir}/python/paddle/incubate/distributed/fleet/parameter_server/version.py'\n    )\n    (\n        setup_requires,\n        packages,\n        scripts,\n        package_data,\n        package_dir,\n        ext_modules,\n        headers,\n    ) = get_setup_parameters()\n\n    # Log for PYPI, get long_description of setup()\n    with open(\n        paddle_source_dir + '/python/paddle/README.md', \"r\", encoding='UTF-8'\n    ) as f:\n        long_description = f.read()\n\n    # strip *.so to reduce package size\n    if env_dict.get(\"WITH_STRIP\") == 'ON':\n        command = (\n            'find '\n            + shlex.quote(paddle_binary_dir)\n            + '/python/paddle -name \"*.so\" | xargs -i strip {}'\n        )\n        if os.system(command) != 0:\n            raise Exception(f\"strip *.so failed, command: {command}\")\n\n    # install cpp distribution\n    if env_dict.get(\"WITH_CPP_DIST\") == 'ON':\n        paddle_install_dir = env_dict.get(\"PADDLE_INSTALL_DIR\")\n        paddle_lib_test_dir = env_dict.get(\"PADDLE_LIB_TEST_DIR\")\n        install_cpp_dist_and_build_test(\n            paddle_install_dir,\n            paddle_lib_test_dir,\n            headers,\n            package_data['paddle.libs'],\n        )\n\n    # generate stub file `tensor.pyi`\n    if os.getenv(\"SKIP_STUB_GEN\", '').lower() not in [\n        'y',\n        'yes',\n        't',\n        'true',\n        'on',\n        '1',\n    ]:\n        generate_stub_files(paddle_binary_dir, paddle_source_dir)\n    # package stub files\n    packages, package_data = extend_type_hints_package_data(\n        packages, package_data, paddle_binary_dir\n    )\n\n    setup(\n        name=package_name,\n        version=paddle_version,\n        description='Parallel Distributed Deep Learning',\n        long_description=long_description,\n        long_description_content_type=\"text/markdown\",\n        author_email=\"Paddle-better@baidu.com\",\n        maintainer=\"PaddlePaddle\",\n        maintainer_email=\"Paddle-better@baidu.com\",\n        url='https://www.paddlepaddle.org.cn/',\n        download_url='https://github.com/paddlepaddle/paddle',\n        license='Apache Software License',\n        packages=packages,\n        install_requires=setup_requires,\n        ext_modules=ext_modules,\n        package_data=package_data,\n        package_dir=package_dir,\n        scripts=scripts,\n        distclass=BinaryDistribution,\n        headers=headers,\n        cmdclass={\n            'install_headers': InstallHeaders,\n            'install': InstallCommand,\n            'egg_info': EggInfo,\n            'install_lib': InstallLib,\n            'develop': DevelopCommand,\n        },\n        entry_points={\n            'console_scripts': [\n                'fleetrun = paddle.distributed.launch.main:launch'\n            ]\n        },\n        classifiers=[\n            'Development Status :: 5 - Production/Stable',\n            'Operating System :: OS Independent',\n            'Intended Audience :: Developers',\n            'Intended Audience :: Education',\n            'Intended Audience :: Science/Research',\n            'License :: OSI Approved :: Apache Software License',\n            'Programming Language :: C++',\n            'Programming Language :: Python :: 3.8',\n            'Programming Language :: Python :: 3.9',\n            'Programming Language :: Python :: 3.10',\n            'Programming Language :: Python :: 3.11',\n            'Programming Language :: Python :: 3.12',\n            'Programming Language :: Python :: 3.13',\n            'Typing :: Typed',\n        ],\n    )\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}