{
  "metadata": {
    "timestamp": 1736566264008,
    "page": 10,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "PaddlePaddle/Paddle",
      "stars": 22396,
      "defaultBranch": "develop",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 0.8720703125,
          "content": "# This file is used by clang-format to autoformat paddle source code\n#\n# The clang-format is part of llvm toolchain.\n# It need to install llvm and clang to format source code style.\n#\n# The basic usage is,\n#   clang-format -i -style=file PATH/TO/SOURCE/CODE\n#\n# The -style=file implicit use \".clang-format\" file located in one of\n# parent directory.\n# The -i means inplace change.\n#\n# The document of clang-format is\n#   http://clang.llvm.org/docs/ClangFormat.html\n#   http://clang.llvm.org/docs/ClangFormatStyleOptions.html\n---\nLanguage:        Cpp\nBasedOnStyle:  Google\nIndentWidth:     2\nTabWidth:        2\nContinuationIndentWidth: 4\nAccessModifierOffset: -1  # The private/protected/public has no indent in class\nStandard:  Cpp11\nAllowAllParametersOfDeclarationOnNextLine: true\nBinPackParameters: false\nBinPackArguments: false\nIncludeBlocks: Preserve\nIncludeIsMainSourceRegex: (\\.cu)$\n...\n"
        },
        {
          "name": ".clang-tidy",
          "type": "blob",
          "size": 7.6552734375,
          "content": "---\nChecks: '\nbugprone-argument-comment,\n-bugprone-assert-side-effect,\n-bugprone-bad-signal-to-kill-thread,\n-bugprone-bool-pointer-implicit-conversion,\nbugprone-branch-clone,\nbugprone-copy-constructor-init,\n-bugprone-dangling-handle,\n-bugprone-dynamic-static-initializers,\nbugprone-exception-escape,\nbugprone-fold-init-type,\n-bugprone-forwarding-reference-overload,\nbugprone-inaccurate-erase,\nbugprone-incorrect-roundings,\nbugprone-infinite-loop,\nbugprone-integer-division,\n-bugprone-macro-repeated-side-effects,\n-bugprone-misplaced-operator-in-strlen-in-alloc,\nbugprone-misplaced-widening-cast,\n-bugprone-move-forwarding-reference,\n-bugprone-multiple-statement-macro,\nbugprone-narrowing-conversions,\n-bugprone-not-null-terminated-result,\n-bugprone-parent-virtual-call,\n-bugprone-posix-return,\nbugprone-signed-char-misuse,\n-bugprone-sizeof-container,\n-bugprone-sizeof-expression,\n-bugprone-string-constructor,\nbugprone-string-integer-assignment,\n-bugprone-string-literal-with-embedded-nul,\n-bugprone-suspicious-enum-usage,\n-bugprone-suspicious-memset-usage,\nbugprone-suspicious-missing-comma,\n-bugprone-suspicious-semicolon,\n-bugprone-suspicious-string-compare,\n-bugprone-terminating-continue,\n-bugprone-throw-keyword-missing,\n-bugprone-too-small-loop-variable,\n-bugprone-undefined-memory-manipulation,\n-bugprone-undelegated-constructor,\nbugprone-unhandled-self-assignment,\nbugprone-unused-raii,\nbugprone-unused-return-value,\nbugprone-use-after-move,\n-bugprone-virtual-near-miss,\n-clang-analyzer-apiModeling.StdCLibraryFunctions,\n-clang-analyzer-apiModeling.TrustNonnull,\n-clang-analyzer-apiModeling.google.GTest,\n-clang-analyzer-apiModeling.llvm.CastValue,\n-clang-analyzer-apiModeling.llvm.ReturnValue,\nclang-analyzer-core.CallAndMessage,\n-clang-analyzer-core.DivideZero,\n-clang-analyzer-core.DynamicTypePropagation,\nclang-analyzer-core.NonNullParamChecker,\n-clang-analyzer-core.NonnilStringConstants,\n-clang-analyzer-core.NullDereference,\n-clang-analyzer-core.StackAddrEscapeBase,\n-clang-analyzer-core.StackAddressEscape,\nclang-analyzer-core.UndefinedBinaryOperatorResult,\n-clang-analyzer-core.VLASize,\n-clang-analyzer-core.builtin.BuiltinFunctions,\n-clang-analyzer-core.builtin.NoReturnFunctions,\n-clang-analyzer-core.uninitialized.ArraySubscript,\nclang-analyzer-core.uninitialized.Assign,\n-clang-analyzer-core.uninitialized.Branch,\n-clang-analyzer-core.uninitialized.CapturedBlockVariable,\n-clang-analyzer-core.uninitialized.UndefReturn,\nclang-analyzer-cplusplus.InnerPointer,\n-clang-analyzer-cplusplus.Move,\n-clang-analyzer-cplusplus.NewDelete,\nclang-analyzer-cplusplus.NewDeleteLeaks,\n-clang-analyzer-cplusplus.PureVirtualCall,\n-clang-analyzer-cplusplus.SelfAssignment,\n-clang-analyzer-cplusplus.SmartPtr,\n-clang-analyzer-cplusplus.VirtualCallModeling,\nclang-analyzer-deadcode.DeadStores,\n-clang-analyzer-fuchsia.HandleChecker,\n-clang-analyzer-nullability.NullPassedToNonnull,\n-clang-analyzer-nullability.NullReturnedFromNonnull,\n-clang-analyzer-nullability.NullabilityBase,\n-clang-analyzer-nullability.NullableDereferenced,\n-clang-analyzer-nullability.NullablePassedToNonnull,\n-clang-analyzer-nullability.NullableReturnedFromNonnull,\nclang-analyzer-optin.cplusplus.UninitializedObject,\n-clang-analyzer-optin.cplusplus.VirtualCall,\n-clang-analyzer-optin.mpi.MPI-Checker,\n-clang-analyzer-optin.osx.OSObjectCStyleCast,\n-clang-analyzer-optin.osx.cocoa.localizability.EmptyLocalizationContextChecker,\n-clang-analyzer-optin.osx.cocoa.localizability.NonLocalizedStringChecker,\n-clang-analyzer-optin.performance.GCDAntipattern,\n-clang-analyzer-optin.performance.Padding,\nclang-analyzer-optin.portability.UnixAPI,\n-clang-analyzer-osx.API,\n-clang-analyzer-osx.MIG,\n-clang-analyzer-osx.NSOrCFErrorDerefChecker,\n-clang-analyzer-osx.NumberObjectConversion,\n-clang-analyzer-osx.OSObjectRetainCount,\n-clang-analyzer-osx.ObjCProperty,\n-clang-analyzer-osx.SecKeychainAPI,\n-clang-analyzer-osx.cocoa.AtSync,\n-clang-analyzer-osx.cocoa.AutoreleaseWrite,\n-clang-analyzer-osx.cocoa.ClassRelease,\n-clang-analyzer-osx.cocoa.Dealloc,\n-clang-analyzer-osx.cocoa.IncompatibleMethodTypes,\n-clang-analyzer-osx.cocoa.Loops,\n-clang-analyzer-osx.cocoa.MissingSuperCall,\n-clang-analyzer-osx.cocoa.NSAutoreleasePool,\n-clang-analyzer-osx.cocoa.NSError,\n-clang-analyzer-osx.cocoa.NilArg,\n-clang-analyzer-osx.cocoa.NonNilReturnValue,\n-clang-analyzer-osx.cocoa.ObjCGenerics,\n-clang-analyzer-osx.cocoa.RetainCount,\n-clang-analyzer-osx.cocoa.RetainCountBase,\n-clang-analyzer-osx.cocoa.RunLoopAutoreleaseLeak,\n-clang-analyzer-osx.cocoa.SelfInit,\n-clang-analyzer-osx.cocoa.SuperDealloc,\n-clang-analyzer-osx.cocoa.UnusedIvars,\n-clang-analyzer-osx.cocoa.VariadicMethodTypes,\n-clang-analyzer-osx.coreFoundation.CFError,\n-clang-analyzer-osx.coreFoundation.CFNumber,\n-clang-analyzer-osx.coreFoundation.CFRetainRelease,\n-clang-analyzer-osx.coreFoundation.containers.OutOfBounds,\n-clang-analyzer-osx.coreFoundation.containers.PointerSizedValues,\nclang-analyzer-security.FloatLoopCounter,\n-clang-analyzer-security.insecureAPI.DeprecatedOrUnsafeBufferHandling,\n-clang-analyzer-security.insecureAPI.SecuritySyntaxChecker,\n-clang-analyzer-security.insecureAPI.UncheckedReturn,\n-clang-analyzer-security.insecureAPI.bcmp,\n-clang-analyzer-security.insecureAPI.bcopy,\n-clang-analyzer-security.insecureAPI.bzero,\n-clang-analyzer-security.insecureAPI.decodeValueOfObjCType,\n-clang-analyzer-security.insecureAPI.getpw,\n-clang-analyzer-security.insecureAPI.gets,\n-clang-analyzer-security.insecureAPI.mkstemp,\n-clang-analyzer-security.insecureAPI.mktemp,\n-clang-analyzer-security.insecureAPI.rand,\n-clang-analyzer-security.insecureAPI.strcpy,\nclang-analyzer-security.insecureAPI.vfork,\n-clang-analyzer-unix.API,\n-clang-analyzer-unix.DynamicMemoryModeling,\nclang-analyzer-unix.Malloc,\n-clang-analyzer-unix.MallocSizeof,\n-clang-analyzer-unix.MismatchedDeallocator,\nclang-analyzer-unix.Vfork,\n-clang-analyzer-unix.cstring.BadSizeArg,\n-clang-analyzer-unix.cstring.CStringModeling,\n-clang-analyzer-unix.cstring.NullArg,\n-clang-analyzer-valist.CopyToSelf,\n-clang-analyzer-valist.Uninitialized,\n-clang-analyzer-valist.Unterminated,\n-clang-analyzer-valist.ValistBase,\ncppcoreguidelines-avoid-c-arrays,\n-cppcoreguidelines-avoid-goto,\ncppcoreguidelines-c-copy-assignment-signature,\ncppcoreguidelines-explicit-virtual-functions,\ncppcoreguidelines-init-variables,\ncppcoreguidelines-narrowing-conversions,\ncppcoreguidelines-no-malloc,\ncppcoreguidelines-pro-type-const-cast,\n-cppcoreguidelines-pro-type-member-init,\n-cppcoreguidelines-slicing,\n-hicpp-avoid-goto,\nhicpp-exception-baseclass,\nmisc-unused-alias-decls,\nmisc-unused-using-decls,\nmodernize-avoid-bind,\nmodernize-avoid-c-arrays,\nmodernize-deprecated-headers,\n-modernize-deprecated-ios-base-aliases,\nmodernize-loop-convert,\nmodernize-make-shared,\nmodernize-make-unique,\n-modernize-pass-by-value,\nmodernize-raw-string-literal,\nmodernize-redundant-void-arg,\n-modernize-replace-auto-ptr,\n-modernize-replace-random-shuffle,\n-modernize-shrink-to-fit,\n-modernize-unary-static-assert,\nmodernize-use-bool-literals,\nmodernize-use-emplace,\nmodernize-use-equals-default,\n-modernize-use-equals-delete,\n-modernize-use-noexcept,\nmodernize-use-nullptr,\nmodernize-use-override,\nmodernize-use-transparent-functors,\n-modernize-use-uncaught-exceptions,\nperformance-faster-string-find,\nperformance-for-range-copy,\n-performance-implicit-conversion-in-loop,\n-performance-inefficient-algorithm,\nperformance-inefficient-string-concatenation,\n-performance-inefficient-vector-operation,\nperformance-move-const-arg,\n-performance-move-constructor-init,\n-performance-no-automatic-move,\nperformance-noexcept-move-constructor,\nperformance-trivially-destructible,\n-performance-type-promotion-in-math-fn,\n-performance-unnecessary-copy-initialization,\nreadability-container-size-empty,\n'\nHeaderFilterRegex: '^(paddle/(?!cinn)).*$'\nAnalyzeTemporaryDtors: false\nWarningsAsErrors: '*'\n...\n"
        },
        {
          "name": ".cmake-format.py",
          "type": "blob",
          "size": 2.9619140625,
          "content": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# -----------------------------\n# Options affecting formatting.\n# -----------------------------\nwith section(\"format\"):\n    # How wide to allow formatted cmake files\n    line_width = 80\n\n# ------------------------------------------------\n# Options affecting comment reflow and formatting.\n# ------------------------------------------------\nwith section(\"markup\"):\n    # enable comment markup parsing and reflow\n    enable_markup = False\n\n    # If comment markup is enabled, don't reflow the first comment block in each\n    # listfile. Use this to preserve formatting of your copyright/license\n    # statements.\n    first_comment_is_literal = True\n\n# ----------------------------------\n# Options affecting listfile parsing\n# ----------------------------------\nwith section(\"parse\"):\n    # Additional FLAGS and KWARGS for custom commands\n    additional_commands = {\n        \"cc_library\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"nv_library\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"xpu_library\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"hip_library\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"go_library\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"copy\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DSTS\": '*',\n            }\n        },\n        \"cc_test\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"nv_test\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"hip_test\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"xpu_test\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"go_test\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n        \"py_test\": {\n            \"kwargs\": {\n                \"SRCS\": '*',\n                \"DEPS\": '*',\n            }\n        },\n    }\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.126953125,
          "content": "*.DS_Store\nbuild/\n*.user\n.vscode\n.idea\n.project\n.cproject\n.pydevproject\nMakefile\n.test_env/\nthird_party/\n*~\nbazel-*\n\n!build/*.deb\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.470703125,
          "content": "# EditorConfig is a cross-editor configuration file\n# that helps to unify code styles for multiple\n# developers collaborative projects.\n# See more at https://editorconfig.org/\n\nroot = true\n\n[*]\nindent_style = space\nend_of_line = lf\ncharset = utf-8\ntrim_trailing_whitespace = true\ninsert_final_newline = true\n\n[*.{c,cc,cxx,cpp,cu,cuh,h,hpp,hxx,kps}]\nindent_size = 2\n\n[*.{py,pyi,java,r,toml}]\nindent_size = 4\n\n[Dockerfile.*]\nindent_size = 4\n\n[*.go]\nindent_style = tab\nindent_size = 4\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.4599609375,
          "content": "paddle/fluid/operators/distributed/send_recv.proto\npaddle/fluid/API.spec\npaddle/fluid/API_DEV.spec\npaddle/fluid/API_PR.spec\npaddle/fluid/eager/api/generated/*\npaddle/fluid/op_use_default_grad_maker_DEV.spec\npaddle/fluid/op_use_default_grad_maker_PR.spec\npaddle/fluid/operators/ops_extra_info.cc\npaddle/phi/api/backward/backward_api.h\npaddle/phi/api/backward/fused_backward_api.h\npaddle/phi/api/backward/sparse_bw_api.h\npaddle/phi/api/include/api.h\npaddle/phi/api/include/fused_api.h\npaddle/phi/api/include/operants_base.h\npaddle/phi/api/include/operants_manager.h\npaddle/phi/api/include/sparse_api.h\npaddle/phi/api/include/strings_api.h\npaddle/phi/api/include/tensor_operants.h\npaddle/phi/api/lib/api.cc\npaddle/phi/api/lib/fused_api.cc\npaddle/phi/api/lib/dygraph_api.*\npaddle/phi/api/lib/backward_api.cc\npaddle/phi/api/lib/fused_backward_api.cc\npaddle/phi/api/lib/operants_manager.cc\npaddle/phi/api/lib/sparse_api.cc\npaddle/phi/api/lib/strings_api.cc\npaddle/phi/api/lib/sparse_bw_api.cc\npaddle/phi/api/lib/tensor_api.cc\npaddle/phi/api/lib/tensor_operants.cc\npaddle/phi/extension.h\npaddle/phi/config.h\npaddle/phi/include/*\npaddle/phi/infermeta/generated.*\npaddle/fluid/prim/api/generated_prim/*.cc\npaddle/fluid/prim/api/generated_prim/*.h\npython/paddle/libs/bfloat16.h\npython/paddle/libs/cinn_cuda_runtime_source.cuh\npython/paddle/libs/float16.h\n*.DS_Store\n*.vs\nbuild/\ndist/\nbuild_doc/\n*.user\n*.tmp\n*.pyc\n*.log\n\n.vscode\n.idea\n.project\n.cproject\n.pydevproject\n.settings/\nCMakeSettings.json\nMakefile\n.test_env/\n.cache/\n.env\nthird_party/\n*~\nbazel-*\n\n\nbuild_*\n# clion workspace.\ncmake-build-*\npaddle/fluid/operators/distributed/send_recv.proto\nmodel_test\n\nTesting\ntools/__pycache__\ntools/nvcc_lazy\n\n# Ignore files generated from 'python setup.py develop'\n@PADDLE_BINARY_DIR@\n\n# This file is automatically generated.\n# TODO(zhiqiang) Move this file to build directory.\npaddle/fluid/pybind/eager_op_function.*\ntools/nvcc_lazy\npaddle/phi/kernels/sparse/gpu/cutlass_generator/all_gemm_operations.h\npaddle/phi/kernels/sparse/gpu/cutlass_generator/configurations.h\n\n#these files (directories) are generated before build system generation\npaddle/fluid/operators/generated_op*.cc\npaddle/fluid/operators/generated_sparse_op.cc\npaddle/fluid/operators/generated_static_op.cc\npaddle/fluid/operators/generated_fused_op.cc\npaddle/fluid/operators/ops_signature/generated_*.cc\npaddle/fluid/pybind/tmp_eager_op_function_impl.h\npaddle/fluid/pybind/eager_op_function_impl.h\npaddle/fluid/pybind/eager_op_function_impl.h\npaddle/fluid/pybind/op_function_impl.h\npaddle/fluid/pybind/*final_state_op_function_impl.h\npaddle/fluid/prim/api/generated/prim_api/*\npaddle/fluid/framework/__init__.py\npaddle/phi/api/profiler/__init__.py\npython/paddle/incubate/fleet/parameter_server/pslib/ps_pb2.py\npaddle/phi/kernels/fusion/cutlass/conv2d/generated/*\npaddle/phi/kernels/fusion/cutlass/conv2d/generated_tmp/*\npaddle/phi/ops/compat/generated_*\npython/paddle/base/incubate/fleet/parameter_server/pslib/ps_pb2.py\npaddle/fluid/ir_adaptor/translator/op_compat_info.cc\npaddle/phi/kernels/fusion/cutlass/cutlass_kernels/fpA_intB_gemm/autogen/*\npaddle/phi/kernels/fusion/cutlass/cutlass_kernels/fpA_intB_gemm/autogen_tmp/*\npaddle/fluid/pybind/static_op_function.*\npaddle/fluid/pybind/ops_api.cc\npython/paddle/tensor/tensor.pyi\npaddle/phi/kernels/fusion/cutlass/conv2d/build\npaddle/phi/kernels/fusion/cutlass/conv2d/cutlass\npaddle/phi/kernels/fusion/cutlass/gemm_epilogue/build\npaddle/phi/kernels/fusion/cutlass/gemm_epilogue/cutlass\npython/paddle/_typing/libs/**/*.pyi\nthird_party.tar.gz\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 4.0078125,
          "content": "[submodule \"third_party/protobuf\"]\n\tpath = third_party/protobuf\n\turl = https://github.com/protocolbuffers/protobuf.git\n\tignore = dirty\n[submodule \"third_party/pocketfft\"]\n\tpath = third_party/pocketfft\n\turl = https://gitlab.mpcdf.mpg.de/mtr/pocketfft.git\n\tignore = dirty\n[submodule \"third_party/gflags\"]\n\tpath = third_party/gflags\n\turl = https://github.com/gflags/gflags.git\n\tignore = dirty\n[submodule \"third_party/gloo\"]\n\tpath = third_party/gloo\n\turl = https://github.com/ziyoujiyi/gloo.git\n\tignore = dirty\n[submodule \"third_party/dlpack\"]\n\tpath = third_party/dlpack\n\turl = https://github.com/dmlc/dlpack.git\n\tignore = dirty\n[submodule \"third_party/utf8proc\"]\n\tpath = third_party/utf8proc\n\turl = https://github.com/JuliaStrings/utf8proc.git\n\tignore = dirty\n[submodule \"third_party/warpctc\"]\n\tpath = third_party/warpctc\n\turl = https://github.com/baidu-research/warp-ctc.git\n\tignore = dirty\n[submodule \"third_party/warprnnt\"]\n\tpath = third_party/warprnnt\n\turl = https://github.com/PaddlePaddle/warp-transducer.git\n\tignore = dirty\n[submodule \"third_party/xxhash\"]\n\tpath = third_party/xxhash\n\turl = https://github.com/Cyan4973/xxHash.git\n\tignore = dirty\n[submodule \"third_party/pybind\"]\n\tpath = third_party/pybind\n\turl = https://github.com/pybind/pybind11.git\n\tignore = dirty\n[submodule \"third_party/threadpool\"]\n\tpath = third_party/threadpool\n\turl = https://github.com/progschj/ThreadPool.git\n\tignore = dirty\n[submodule \"third_party/zlib\"]\n\tpath = third_party/zlib\n\turl = https://github.com/madler/zlib.git\n\tignore = dirty\n[submodule \"third_party/glog\"]\n\tpath = third_party/glog\n\turl = https://github.com/google/glog.git\n\tignore = dirty\n[submodule \"third_party/eigen3\"]\n\tpath = third_party/eigen3\n\turl = https://gitlab.com/libeigen/eigen.git\n\tignore = dirty\n[submodule \"third_party/snappy\"]\n\tpath = third_party/snappy\n\turl = https://github.com/google/snappy.git\n\tignore = dirty\n[submodule \"third_party/cub\"]\n\tpath = third_party/cub\n\turl = https://github.com/NVIDIA/cub.git\n\tignore = dirty\n[submodule \"third_party/cutlass\"]\n\tpath = third_party/cutlass\n\turl = https://github.com/NVIDIA/cutlass.git\n\tignore = dirty\n[submodule \"third_party/xbyak\"]\n\tpath = third_party/xbyak\n\turl = https://github.com/herumi/xbyak.git\n\tignore = dirty\n[submodule \"third_party/onednn\"]\n\tpath = third_party/onednn\n\turl = https://github.com/oneapi-src/oneDNN.git\n\tignore = dirty\n[submodule \"third_party/flashattn\"]\n\tpath = third_party/flashattn\n\turl = https://github.com/PaddlePaddle/flash-attention.git\n\tignore = dirty\n[submodule \"third_party/gtest\"]\n\tpath = third_party/gtest\n\turl = https://github.com/google/googletest.git\n\tignore = dirty\n[submodule \"third_party/openblas\"]\n\tpath = third_party/openblas\n\turl = https://github.com/xianyi/OpenBLAS.git\n\tignore = dirty\n[submodule \"third_party/leveldb\"]\n\tpath = third_party/leveldb\n\turl = https://github.com/google/leveldb.git\n\tignore = dirty\n[submodule \"third_party/brpc\"]\n\tpath = third_party/brpc\n\turl = https://github.com/apache/brpc.git\n\tignore = dirty\n[submodule \"third_party/rocksdb\"]\n\tpath = third_party/rocksdb\n\turl = https://github.com/Thunderbrook/rocksdb\n\tignore = dirty\n[submodule \"third_party/absl\"]\n\tpath = third_party/absl\n\turl = https://github.com/abseil/abseil-cpp.git\n\tignore = dirty\n[submodule \"third_party/jitify\"]\n\tpath = third_party/jitify\n\turl = https://github.com/NVIDIA/jitify.git\n\tignore = dirty\n[submodule \"third_party/cccl\"]\n\tpath = third_party/cccl\n\turl = https://github.com/NVIDIA/cccl.git\n\tignore = dirty\n[submodule \"third_party/cryptopp\"]\n\tpath = third_party/cryptopp\n\turl = https://github.com/weidai11/cryptopp.git\n\tignore = dirty\n[submodule \"third_party/cryptopp-cmake\"]\n\tpath = third_party/cryptopp-cmake\n\turl = https://github.com/noloader/cryptopp-cmake.git\n\tignore = dirty\n[submodule \"third_party/nlohmann_json\"]\n\tpath = third_party/nlohmann_json\n\turl = https://github.com/nlohmann/json.git\n\tignore = dirty\n[submodule \"third_party/yaml-cpp\"]\n\tpath = third_party/yaml-cpp\n\turl = https://github.com/jbeder/yaml-cpp\n[submodule \"third_party/openvino\"]\n\tpath = third_party/openvino\n\turl = https://github.com/openvinotoolkit/openvino.git\n\tignore = dirty\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 5.537109375,
          "content": "# Exclude all third-party libraries and auto-generated files globally\nexclude: |\n    (?x)^(\n        patches/.+|\n        paddle/fluid/framework/fleet/heter_ps/cudf/.+|\n        paddle/fluid/distributed/ps/thirdparty/round_robin.h|\n        python/paddle/utils/gast/.+|\n        third_party/.+\n    )$\nrepos:\n# Common hooks\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n    -   id: check-added-large-files\n    -   id: check-merge-conflict\n    -   id: check-symlinks\n    -   id: detect-private-key\n    -   id: end-of-file-fixer\n    -   id: sort-simple-yaml\n        files: (ops|backward|op_[a-z_]+)\\.yaml$\n    -   id: trailing-whitespace\n-   repo: https://github.com/Lucas-C/pre-commit-hooks.git\n    rev: v1.5.1\n    hooks:\n    -   id: remove-crlf\n    -   id: remove-tabs\n        name: Tabs remover (C++)\n        files: \\.(c|cc|cxx|cpp|cu|h|hpp|hxx|xpu|kps)$\n        args: [--whitespaces-count, '2']\n    -   id: remove-tabs\n        name: Tabs remover (Python)\n        files: (.*\\.(py|bzl)|BUILD|.*\\.BUILD|WORKSPACE)$\n        args: [--whitespaces-count, '4']\n        # Exclude some unit test files that require tabs.\n        exclude: |\n            (?x)^(\n                test/dygraph_to_static/test_error.py\n            )$\n-   repo: local\n    hooks:\n    -   id: copyright_checker\n        name: copyright_checker\n        entry: python ./tools/codestyle/copyright.py\n        language: system\n        files: \\.(c|cc|cxx|cpp|cu|h|hpp|hxx|proto|xpu|kps|py|pyi|sh)$\n        exclude: |\n            (?x)^(\n                paddle/utils/.*|\n                paddle/cinn/utils/registry.h\n            )$\n-   repo: https://github.com/PFCCLab/typos-pre-commit-mirror.git\n    rev: v1.27.3\n    hooks:\n    -   id: typos\n        args: [--force-exclude]\n# For Python files\n-   repo: https://github.com/psf/black-pre-commit-mirror\n    rev: 24.8.0\n    hooks:\n    -   id: black\n-   repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.8.3\n    hooks:\n    -   id: ruff\n        args: [--fix, --exit-non-zero-on-fix, --no-cache]\n# For C++ files\n-   repo: local\n    hooks:\n    -   id: clang-format\n        name: clang-format\n        description: Format files with ClangFormat.\n        entry: bash ./tools/codestyle/clang_format.sh -i\n        language: system\n        files: \\.(c|cc|cxx|cpp|cu|h|hpp|hxx|xpu|kps)$\n-   repo: local\n    hooks:\n    -   id: cpplint-cpp-source\n        name: cpplint\n        description: Check C++ code style using cpplint.py.\n        entry: bash ./tools/codestyle/cpplint_pre_commit.sh\n        language: system\n        files: \\.(cc|cxx|cpp|cu|h|hpp|hxx)$\n        args:\n            - --extensions=cc,cxx,cpp,cu,cuh,h,hpp,hxx,kps\n            - --filter=-readability/fn_size,-build/include_what_you_use,-build/c++11,-whitespace/parens\n            - --quiet\n        # Exclude third-party libraries\n        exclude:  |\n            (?x)^(\n                paddle/utils/flat_hash_map\\.h\n            )$\n-   repo: local\n    hooks:\n    -   id: clang-tidy\n        name: clang-tidy\n        description: Parallel clang-tidy runner.\n        entry: python ./tools/codestyle/clang-tidy.py\n        language: system\n        files: \\.(c|cc|cxx|cpp|h|hpp|hxx)$\n        args:\n            - -p=build/\n            - -extra-arg=-Wno-unknown-warning-option\n            - -extra-arg=-Wno-pessimizing-move\n            - -extra-arg=-Wno-braced-scalar-init\n            - -extra-arg=-Wno-dangling-gsl\n            - -extra-arg=-Wno-deprecated-copy\n            - -extra-arg=-Wno-final-dtor-non-final-class\n            - -extra-arg=-Wno-implicit-int-float-conversion\n            - -extra-arg=-Wno-inconsistent-missing-override\n            - -extra-arg=-Wno-infinite-recursion\n            - -extra-arg=-Wno-mismatched-tags\n            - -extra-arg=-Wno-self-assign\n            - -extra-arg=-Wno-sign-compare\n            - -extra-arg=-Wno-sometimes-uninitialized\n            - -extra-arg=-Wno-tautological-overlap-compare\n            - -extra-arg=-Wno-unused-const-variable\n            - -extra-arg=-Wno-unused-lambda-capture\n            - -extra-arg=-Wno-unused-private-field\n            - -extra-arg=-Wno-unused-value\n            - -extra-arg=-Wno-unused-variable\n            - -extra-arg=-Wno-overloaded-virtual\n            - -extra-arg=-Wno-defaulted-function-deleted\n            - -extra-arg=-Wno-delete-non-abstract-non-virtual-dtor\n            - -extra-arg=-Wno-return-type-c-linkage\n# For CMake files\n-   repo: local\n    hooks:\n    -   id: auto-generate-cmakelists\n        name: auto-generate-cmakelists\n        entry: bash ./tools/gen_ut_cmakelists.hook\n        language: system\n        files: testslist.csv$\n-   repo: https://github.com/cheshirekow/cmake-format-precommit\n    rev: v0.6.13\n    hooks:\n    -   id: cmake-format\n        # exclude paddle/fluid/operators/CMakeLists.txt, see the comment\n        # https://github.com/PaddlePaddle/Paddle/pull/43057#pullrequestreview-993471860\n        exclude: |\n            (?x)^(\n                paddle/fluid/operators/CMakeLists.txt\n            )$\n-   repo: https://github.com/PFCCLab/cmake-lint-paddle\n    rev: v1.5.1\n    hooks:\n    -   id: cmakelint\n        args: [--config=./tools/codestyle/.cmakelintrc]\n        # Exclude some files has false positive warnings\n        # Need to fix them in the future\n        exclude: |\n            (?x)^(\n                cmake/external/onnxruntime.cmake\n                )$\n# Others\n-   repo: local\n    hooks:\n    -   id: sort-txt-file\n        name: sort-txt-file\n        description: Sorts each line string in a text file\n        entry: python ./tools/codestyle/sort_txt_file.py\n        language: python\n        files: test/white_list/pir_op_test_white_list\n        args: []\n"
        },
        {
          "name": "AUTHORS.md",
          "type": "blob",
          "size": 5.3115234375,
          "content": "This is an incomplete list of authors of [Paddle](https://github.com/PaddlePaddle/Paddle/) codebase, to see a full list, please use the source control tool git. PaddlePaddle community encourages every Paddle codebase author include his/her GitHub account and fullname here.\n\n\n| Github account | name |\n|---|---|\n| abhinavarora | Abhinav Arora |\n| andreazanetti | Andrea Zanetti |\n| arlesniak | Artur Lesniak |\n| [arogowie-intel](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg) | Adam Osewski |\n| backyes | Yan-Fei Wang |\n| baiyfbupt | Yi-Fan Bai |\n| beckett1124 | Bin Qi |\n| ChengduoZH | Cheng-Duo Zhao|\n| chengxiaohua1105 | Xiao-Hua Cheng |\n| chenwhql | Wei-Hang Chen |\n| cxwangyi, yiwangbaidu, wangkuiyi | Yi Wang |\n| cxysteven | Xing-Yi Cheng |\n| ddokupil | Dariusz Dokupil |\n| dzhwinter | Zhi-Hong Dong |\n| dragonwarrior | Long Wang |\n| dyning | Yuning Du |\n| emailweixu | Wei Xu |\n| engineer1109 | Jia-Liang Wang |\n| gangliao | Gang Liao |\n| gongweibao | Wei-Bao Gong |\n| guru4elephant | Daxiang Dong |\n| Guo Sheng | Sheng Guo |\n| [grygielski](https://raw.githubusercontent.com/jczaja/Paddle/paddle-poland-team/doc/images/paddle_poland_team.jpg)| Adam Grygielski |\n| Haichao-Zhang | Hai-Chao Zhang |\n| HarperCy | Sicheng Hao |\n| hedaoyuan | Dao-Yuan He |\n| helinwang | He-Lin Wang |\n| heliqi | Li-Qi He |\n| houj04 | HOU Jue |\n| [runzhech](https://github.com/runzhech) | Runzhe Chen |\n| [dynamicheart](https://github.com/dynamicheart) | Jianbang Yang |\n| HulekJakub | Jakub Hulek |\n| jacquesqiao | Long-Fei Qiao |\n| [jakpiase](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg) | Jakub Piasecki |\n| [jczaja](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg) | Jacek Czaja |\n| jiahy0825 | Hongyu Jia |\n| JiayiFeng | Jia-Yi Feng |\n| kbinias | Krzysztof Binias |\n| kexinzhao | Ke-Xin Zhao |\n| kuke | Yi-Bing Liu |\n| [lidanqing](https://raw.githubusercontent.com/jczaja/Paddle/paddle-poland-team/doc/images/paddle_poland_team.jpg) | DanQing Li |\n| lcy-seso | Ying Cao |\n| cjld | Dun Liang |\n| lj970926 | Jin Li |\n| lipeng-unisound | Peng Li |\n| gavin1332 | Yi Liu |\n| cqulilujia | Lujia Li |\n| liuyuan | Yuan Liu |\n| livc | Zhao Li |\n| llxxxll | Yong-Feng Liu |\n| luotao01 | Tao Luo |\n| lzhao4ever | Liang Zhao |\n| [RuohengMa](https://github.com/RuohengMa) | Ruoheng Ma |\n| mozga-intel | Mateusz Ozga |\n| NHZlX | Zhao-Long Xing |\n| Noplz | Yuan Gao |\n| pakchoi | Chuan-Jiang Song |\n| panyx0718 | Xin Pan |\n| pengli09 | Peng Li |\n| [piotrekobiIntel](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg) | Piotr Paturej |\n| [pmajchrzak](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg) | Piotr Majchrzak |\n| pkuyym | Ya-Ming Yang |\n| pzelazko-intel | Pawel Zelazko |\n| [pawelpiotrowicz](https://raw.githubusercontent.com/jczaja/Paddle/paddle-poland-team/doc/images/paddle_poland_team.jpg)  | Pawel Piotrowicz |\n| QiJune | Jun Qi |\n| qingqing01 | Qing-Qing Dang |\n| reyoung | Yang Yu |\n| [Sand3r-](https://raw.githubusercontent.com/jczaja/Paddle/paddle-poland-team/doc/images/paddle_poland_team.jpg)| Michal Gallus |\n| [sfraczek](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg)| Sylwester Fraczek |\n| Silv3S | Slawomir Siwek |\n| sneaxiy | Jin-Le Zeng |\n| Superjom | Chun-Wei Yan |\n| tensor-tang | Jian Tang |\n| tianbingsz | Tian-Bing Xu |\n| tizhou86 | Ti Zhou |\n| tpatejko | Tomasz Patejko |\n| [tsocha](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg) | Tomasz Socha |\n| typhoonzero | Yi Wu |\n| velconia | Qi-Yang Min |\n| wanghaoshuang | Hao-Shuang Wang |\n| wangyang59 | Yang Wang |\n| wangzhen-nlp | Zhen Wang |\n| wen-bo-yang | Wen-Bo Yang |\n| wojtuss | Wojciech Uss |\n| [wozna](https://raw.githubusercontent.com/jakpiase/Paddle/new_paddle_intel_authors/img/img.jpg)| Joanna Wozna |\n| wwhu | Wei-Wei Hu |\n| xinghai-sun | Xing-Hai Sun |\n| Xreki | Yi-Qun Liu |\n| xujun05 | Jun Xu |\n| xushaoyong | Shao-Yong Xu |\n| Yancey1989 | Xu Yan |\n| zhaopu7 | Pu Zhao |\n| zhiqiu | Qiu-Liang Chen |\n| zhouxiao-coder | Xiao Zhou |\n| Zrachel | Rui-Qing Zhang |\n| jeng1220 | Bai-Cheng(Ryan) Jeng (NVIDIA) |\n| mingxu1067 | Ming Huang (NVIDIA) |\n| zlsh80826 | Reese Wang (NVIDIA) |\n| leo0519 | Leo Chen (NVIDIA) |\n| jzhang533 | Jun Zhang |\n| Ligoml | Meng-Liu Li |\n| jeff41404 | Xiang Gao |\n| zh794390558 | Hui Zhang |\n| limin2021 | Min Li |\n| zhouwei25 | Wei Zhou |\n| littletomatodonkey | Ruo-Yu Guo |\n| zhupengyang | Zhu Pengyang |\n| DesmonDay | Siming Dai |\n| thisjiang | jiangcheng |\n| yghstill | Guanghua Yu |\n| CtfGo | Tefeng Chen |\n| ZHUI | Hui Zhong|\n| LemonNoel | Huijuan Wang |\n| wawltor | Zeyang Fang |\n| FrostML | Zheng-Xi Liu |\n| jiangjiajun | jiangjiajun |\n| dingjiaweiww | dingjiawei |\n| gglin001 | Allen Guo (Graphcore) |\n| yaozhixin | Zhixin Yao (Graphcore) |\n| XBWGC | Xiaobing Wang (Graphcore) |\n| jianghaicheng | Haicheng Jiang (Graphcore) |\n| czr-gc | Zhaorui Chen (Graphcore) |\n| zhao-han | Han Zhao (Graphcore) |\n| yiakwy, yiakwy-xpu-ml-framework-team | Yi Wang (Graphcore) |\n| [Yulv-git](https://github.com/Yulv-git) | Shuangchi He |\n| [zrr1999](https://github.com/zrr1999) | Rongrui Zhan |\n| [will-jl944](https://github.com/will-jl944) | Jiafeng Lu |\n| [gouzil](https://github.com/gouzil) | Chuan Tian |\n| [skywalker2012](https://github.com/skywalker2012) | Yong Wei |\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 25.6337890625,
          "content": "# Copyright (c) 2016 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License\n\nif(APPLE AND WITH_ARM)\n  # cmake 3.19.2 version starts to support M1\n  cmake_minimum_required(VERSION 3.19.2)\n  cmake_policy(VERSION 3.19.2)\nelse()\n  cmake_minimum_required(VERSION 3.15)\n  cmake_policy(VERSION 3.10)\nendif()\n# use to get_property location of static lib\n# https://cmake.org/cmake/help/v3.0/policy/CMP0026.html?highlight=cmp0026\ncmake_policy(SET CMP0026 OLD)\ncmake_policy(SET CMP0079 NEW)\nset(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} \"${CMAKE_CURRENT_SOURCE_DIR}/cmake\")\nset(PADDLE_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR})\nset(PADDLE_BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR})\n\ninclude(system)\n\n# Note(zhouwei): Ninja Generator will set CMAKE_BUILD_TYPE to Debug\nif(NOT CMAKE_BUILD_TYPE)\n  set(CMAKE_BUILD_TYPE\n      \"Release\"\n      CACHE\n        STRING\n        \"Choose the type of build, options are: Debug Release RelWithDebInfo MinSizeRel\"\n        FORCE)\nendif()\n\nproject(paddle CXX C)\n\n# enable language CUDA\n# TODO(Shibo Tao): remove find_package(CUDA) completely.\nfind_package(CUDA QUIET)\nfind_package(MKL CONFIG QUIET)\noption(WITH_ONEMKL \"Compile PaddlePaddle with oneMKL\" OFF)\noption(WITH_GPU \"Compile PaddlePaddle with NVIDIA GPU\" ${CUDA_FOUND})\noption(WITH_MPI \"Compile PaddlePaddle with MPI\" OFF)\noption(WITH_TENSORRT \"Compile PaddlePaddle with NVIDIA TensorRT\" OFF)\noption(WITH_OPENVINO \"Compile PaddlePaddle with Intel OpenVINO\" OFF)\noption(WITH_XPU \"Compile PaddlePaddle with BAIDU KUNLUN XPU\" OFF)\noption(WITH_XPU_KP \"Compile PaddlePaddle with BAIDU XPU compiler \" OFF)\noption(WITH_XPU_XFT \"Compile PaddlePaddle with BAIDU XPU-XFT\" OFF)\noption(WITH_XPU_PLUGIN \"Compile PaddlePaddle with BAIDU XPU plugin\" OFF)\noption(WITH_XPU_XRE5 \"Compile PaddlePaddle with BAIDU XPU XRE 5\" OFF)\noption(WITH_WIN_DUMP_DBG \"Compile with windows core dump debug mode\" OFF)\noption(WITH_ROCM \"Compile PaddlePaddle with ROCM platform\" OFF)\noption(WITH_IPU \"Compile PaddlePaddle with Graphcore IPU\" OFF)\noption(WITH_ONNXRUNTIME \"Compile PaddlePaddle with ONNXRUNTIME\" OFF)\noption(WITH_CUSPARSELT \"Compile PaddlePaddle with CUSPARSELT\" OFF)\noption(WITH_SETUP_INSTALL \"Compile PaddlePaddle with setup.py\" OFF)\noption(WITH_SHARED_PHI \"Compile PaddlePaddle with SHARED LIB of PHI\" ON)\noption(CINN_WITH_CUDNN \"Compile CINN with CUDNN support\" ON)\noption(WITH_PIP_CUDA_LIBRARIES\n       \"Paddle uses the CUDA library provided by NVIDIA\" OFF)\noption(WITH_PIP_TENSORRT \"Paddle uses the tensorrt provided by NVIDIA\" OFF)\noption(WITH_NIGHTLY_BUILD\n       \"Compile nightly paddle whl package of the develop branch\" OFF)\noption(WITH_CPP_TEST \"Compile PaddlePaddle skip cpp test\" ON)\nfind_package(Git REQUIRED)\n\n# config GIT_URL with github mirrors to speed up dependent repos clone\noption(GIT_URL \"Git URL to clone dependent repos\" ${GIT_URL})\nif(NOT GIT_URL)\n  set(GIT_URL \"https://github.com\")\nendif()\n\n# Note(zhouwei): It use option above, so put here\ninclude(init)\ninclude(generic) # simplify cmake module\ninclude(experimental) # experimental build options\n\nif(WITH_GPU AND WITH_XPU)\n  message(FATAL_ERROR \"Error when compile GPU and XPU at the same time\")\nendif()\nif(WITH_GPU AND WITH_XPU_KP)\n  message(FATAL_ERROR \"Error when compile GPU and XPU2 at the same time\")\nendif()\nif(WITH_GPU AND WITH_XPU_XFT)\n  message(FATAL_ERROR \"Error when compile GPU and XPU-XFT at the same time\")\nendif()\nif(WITH_GPU AND WITH_XPU_XRE5)\n  message(FATAL_ERROR \"Error when compile GPU and XPU-XRE5 at the same time\")\nendif()\nif(WITH_GPU AND WITH_ROCM)\n  message(FATAL_ERROR \"Error when compile CUDA and ROCM at the same time\")\nendif()\n\nif(WITH_GPU AND NOT APPLE)\n  if(WITH_PIP_CUDA_LIBRARIES AND CMAKE_SYSTEM_NAME STREQUAL \"Windows\")\n    add_definitions(-DPADDLE_WITH_PIP_CUDA_LIBRARIES)\n  endif()\n  #(Note risemeup1): The cudart dynamic library libcudart.so is used by set CUDA_USE_STATIC_CUDA_RUNTIME and CMAKE_CUDA_FLAGS\n  if(CMAKE_SYSTEM_NAME STREQUAL \"Linux\" AND CMAKE_SYSTEM_PROCESSOR STREQUAL\n                                            \"x86_64\")\n    set(CUDA_USE_STATIC_CUDA_RUNTIME\n        OFF\n        CACHE BOOL \"\" FORCE)\n    set(CMAKE_CUDA_FLAGS \"--cudart shared\")\n    if(WITH_PIP_CUDA_LIBRARIES)\n      #(Note risemeup1): Flag 'PADDLE_WITH_PIP_CUDA_LIBRARIES' will be used in dynamic_loader.cc to search for CUDA-related .so files through the Python libraries provided by NVIDIA.\n      add_definitions(-DPADDLE_WITH_PIP_CUDA_LIBRARIES)\n    endif()\n  endif()\n  enable_language(CUDA)\n  message(STATUS \"CUDA compiler: ${CMAKE_CUDA_COMPILER}, version: \"\n                 \"${CMAKE_CUDA_COMPILER_ID} ${CMAKE_CUDA_COMPILER_VERSION}\")\nendif()\n\nmessage(STATUS \"CXX compiler: ${CMAKE_CXX_COMPILER}, version: \"\n               \"${CMAKE_CXX_COMPILER_ID} ${CMAKE_CXX_COMPILER_VERSION}\")\nmessage(STATUS \"C compiler: ${CMAKE_C_COMPILER}, version: \"\n               \"${CMAKE_C_COMPILER_ID} ${CMAKE_C_COMPILER_VERSION}\")\nmessage(STATUS \"AR tools: ${CMAKE_AR}\")\n\nif((CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\") AND CMAKE_CXX_COMPILER_VERSION\n                                              VERSION_GREATER 10.4)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-error=uninitialized\")\nendif()\n\n# MUSL build turn off warnings\n\nif(WITH_MUSL)\n  set(CMAKE_CXX_FLAGS\n      \"${CMAKE_CXX_FLAGS} -Wno-error=deprecated-declarations -Wno-deprecated-declarations -Wno-error=pessimizing-move -Wno-error=deprecated-copy\"\n  )\nendif()\n\nif(APPLE AND WITH_ARM)\n  set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -target arm64-apple-darwin\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_C_FLAGS} -target arm64-apple-darwin\")\nendif()\n\nif(WIN32)\n  option(MSVC_STATIC_CRT \"use static C Runtime library by default\" ON)\n  message(\"Build static library of PHI\")\n  # (Note xuxinyi04): If CMAKE_SUPPRESS_REGENERATION is OFF, which is default, then CMake adds a\n  # special target on which all other targets depend that checks the build system and optionally\n  # re-runs CMake to regenerate the build system when the target specification source changes.\n  set(CMAKE_SUPPRESS_REGENERATION OFF)\n  set(CMAKE_STATIC_LIBRARY_PREFIX lib)\n  set(WITH_SHARED_PHI\n      OFF\n      CACHE BOOL \"Disable WITH_SHARED_PHI when compiling PADDLE ON WIN32\" FORCE)\n\n  set(CMAKE_C_FLAGS_DEBUG \"${CMAKE_C_FLAGS_DEBUG} /bigobj\")\n  set(CMAKE_C_FLAGS_RELEASE \"${CMAKE_C_FLAGS_RELEASE} /bigobj\")\n  set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} /bigobj\")\n  set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} /bigobj\")\n\n  if(\"${CMAKE_GENERATOR}\" STREQUAL \"Ninja\")\n    set(CMAKE_C_FLAGS_DEBUG \"${CMAKE_C_FLAGS_DEBUG} /Zc:inline\")\n    set(CMAKE_C_FLAGS_RELEASE \"${CMAKE_C_FLAGS_RELEASE} /Zc:inline\")\n    set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} /Zc:inline\")\n    set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} /Zc:inline\")\n  endif()\n\n  if(MSVC_STATIC_CRT)\n    message(\n      STATUS\n        \"Use static C runtime time, refer to https://docs.microsoft.com/en-us/cpp/c-runtime-library/crt-library-features?view=vs-2019\"\n    )\n    foreach(\n      flag_var\n      CMAKE_CXX_FLAGS\n      CMAKE_CXX_FLAGS_DEBUG\n      CMAKE_CXX_FLAGS_RELEASE\n      CMAKE_CXX_FLAGS_MINSIZEREL\n      CMAKE_CXX_FLAGS_RELWITHDEBINFO\n      CMAKE_C_FLAGS\n      CMAKE_C_FLAGS_DEBUG\n      CMAKE_C_FLAGS_RELEASE\n      CMAKE_C_FLAGS_MINSIZEREL\n      CMAKE_C_FLAGS_RELWITHDEBINFO)\n      if(${flag_var} MATCHES \"/MD\")\n        string(REGEX REPLACE \"/MD\" \"/MT\" ${flag_var} \"${${flag_var}}\")\n      endif()\n    endforeach()\n  endif()\n\n  # msvc max/min macro conflict with std::min/max, define NOMINMAX globally\n  add_definitions(\"-DNOMINMAX\")\n\n  # 1. windows.h define 'small' cause CUDA11.6/11.7/11.8 's cub compile error,\n  # see https://github.com/microsoft/onnxruntime/issues/11227\n  # 2. WIN32_LEAN_AND_MEAN minimize the windows include files, avoid define 'small'\n  add_definitions(-DWIN32_LEAN_AND_MEAN)\n\n  # windows build turn off warnings, use parallel compiling.\n  foreach(\n    flag_var\n    CMAKE_CXX_FLAGS\n    CMAKE_CXX_FLAGS_DEBUG\n    CMAKE_CXX_FLAGS_RELEASE\n    CMAKE_CXX_FLAGS_MINSIZEREL\n    CMAKE_CXX_FLAGS_RELWITHDEBINFO\n    CMAKE_C_FLAGS\n    CMAKE_C_FLAGS_DEBUG\n    CMAKE_C_FLAGS_RELEASE\n    CMAKE_C_FLAGS_MINSIZEREL\n    CMAKE_C_FLAGS_RELWITHDEBINFO)\n    string(REGEX REPLACE \"/W[1-4]\" \" /W0 \" ${flag_var} \"${${flag_var}}\")\n\n    # NOTE(zhouwei25): GPU compile have too high memory utilization when parallel compiling,\n    # For Visual Studio generators, /MP should be added.\n    # For other generators like Ninja, it is not need to add /MP.\n    if(CMAKE_GENERATOR MATCHES \"Visual Studio\" AND NOT WITH_GPU)\n      math(EXPR PROCESS_MAX \"${CPU_CORES} * 2 / 3\")\n      set(${flag_var} \"${${flag_var}} /MP${PROCESS_MAX}\")\n    endif()\n  endforeach()\n  foreach(flag_var CMAKE_CXX_FLAGS CMAKE_C_FLAGS)\n    set(${flag_var} \"${${flag_var}} /w\")\n  endforeach()\n\n  # Windows Remove /Zi, /ZI for Release, MinSizeRel builds\n  foreach(flag_var\n          CMAKE_C_FLAGS CMAKE_C_FLAGS_RELEASE CMAKE_C_FLAGS_MINSIZEREL\n          CMAKE_CXX_FLAGS CMAKE_CXX_FLAGS_RELEASE CMAKE_CXX_FLAGS_MINSIZEREL)\n    if(${flag_var} MATCHES \"/Z[iI]\")\n      string(REGEX REPLACE \"/Z[iI]\" \"\" ${flag_var} \"${${flag_var}}\")\n    endif()\n  endforeach()\n\n  set(CMAKE_C_FLAGS\n      \"${CMAKE_C_FLAGS} /wd4068 /wd4129 /wd4244 /wd4267 /wd4297 /wd4530 /wd4577 /wd4819 /wd4838\"\n  )\n  set(CMAKE_CXX_FLAGS\n      \"${CMAKE_CXX_FLAGS} /wd4068 /wd4129 /wd4244 /wd4267 /wd4297 /wd4530 /wd4577 /wd4819 /wd4838\"\n  )\n\n  foreach(flag_var CMAKE_SHARED_LINKER_FLAGS CMAKE_STATIC_LINKER_FLAGS\n                   CMAKE_EXE_LINKER_FLAGS CMAKE_LINKER_FLAGS)\n    set(${flag_var}\n        \"${${flag_var}} /ignore:4049 /ignore:4217 /ignore:4006 /ignore:4221\")\n    if(MSVC_STATIC_CRT)\n      set(${flag_var} \"${${flag_var}} /NODEFAULTLIB:MSVCRT.LIB\")\n    else()\n      set(${flag_var} \"${${flag_var}} /NODEFAULTLIB:LIBCMT.LIB\")\n    endif()\n  endforeach()\n\n  if(WITH_WIN_DUMP_DBG)\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} /Zi\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /Zi\")\n\n    foreach(flag_var CMAKE_SHARED_LINKER_FLAGS CMAKE_STATIC_LINKER_FLAGS\n                     CMAKE_EXE_LINKER_FLAGS CMAKE_LINKER_FLAGS)\n      set(${flag_var} \"${${flag_var}} /DEBUG /OPT:REF /OPT:ICF\")\n    endforeach()\n\n    add_definitions(\"-DWITH_WIN_DUMP_DBG\")\n  endif()\n\nelse()\n  set(CMAKE_CXX_FLAGS\n      \"${CMAKE_CXX_FLAGS} -Wno-error=deprecated-declarations -Wno-deprecated-declarations\"\n  )\nendif()\n\nfind_package(Threads REQUIRED)\n\ninclude(simd)\n\n################################ Exposed Configurations #######################################\noption(WITH_AVX \"Compile PaddlePaddle with AVX intrinsics\" ${AVX_FOUND})\noption(WITH_PYTHON \"Compile PaddlePaddle with python interpreter\" ON)\noption(WITH_TESTING \"Compile PaddlePaddle with unit testing\" OFF)\noption(WITH_MULTINODE_TESTING \"Test multinode apis and ops\" OFF)\noption(WITH_MKL \"Compile PaddlePaddle with MKL support.\" ${AVX_FOUND})\noption(WITH_SYSTEM_BLAS \"Use system blas library\" OFF)\noption(WITH_DISTRIBUTE \"Compile with distributed support\" OFF)\noption(WITH_BRPC_RDMA \"Use brpc rdma as the rpc protocal\" OFF)\noption(ON_INFER \"Turn on inference optimization and inference-lib generation\"\n       ON)\noption(WITH_CPP_DIST \"Install PaddlePaddle C++ distribution\" OFF)\noption(WITH_GFLAGS \"Compile PaddlePaddle with gflags support\" OFF)\n################################ Internal Configurations #######################################\noption(WITH_NV_JETSON \"Compile PaddlePaddle with NV JETSON\" OFF)\noption(WITH_PROFILER \"Compile PaddlePaddle with GPU profiler and gperftools\"\n       OFF)\noption(WITH_COVERAGE \"Compile PaddlePaddle with code coverage\" OFF)\noption(WITH_INCREMENTAL_COVERAGE\n       \"Generate coverage reports only for incremental code\" OFF)\noption(WITH_LIBXSMM \"Compile with libxsmm\" OFF)\noption(COVERALLS_UPLOAD \"Package code coverage data to coveralls\" OFF)\noption(WITH_PSLIB \"Compile with pslib support\" OFF)\noption(WITH_BOX_PS \"Compile with box_ps support\" OFF)\noption(WITH_XBYAK \"Compile with xbyak support\" ON)\noption(WITH_PSCORE \"Compile with parameter server support\" ${WITH_DISTRIBUTE})\noption(WITH_HETERPS \"Compile with heterps\" OFF)\noption(WITH_INFERENCE_API_TEST\n       \"Test fluid inference C++ high-level api interface\" OFF)\noption(WITH_NVTX \"Paddle with nvtx for profiler\" OFF)\noption(PY_VERSION \"Compile PaddlePaddle with python3 support\" ${PY_VERSION})\noption(WITH_DGC \"Use DGC(Deep Gradient Compression) or not\" ${WITH_DISTRIBUTE})\noption(\n  SANITIZER_TYPE\n  \"Choose the type of sanitizer, options are: Address, Leak, Memory, Thread, Undefined\"\n  OFF)\noption(WITH_CINN \"Compile PaddlePaddle with CINN\" OFF)\noption(WITH_NCCL \"Compile PaddlePaddle with NCCL support\" ON)\noption(WITH_RCCL \"Compile PaddlePaddle with RCCL support\" ON)\noption(WITH_XPU_BKCL \"Compile PaddlePaddle with BAIDU KUNLUN XPU BKCL\" OFF)\noption(WITH_CRYPTO \"Compile PaddlePaddle with crypto support\" ON)\noption(WITH_ARM \"Compile PaddlePaddle with arm support\" OFF)\noption(WITH_SW \"Compile PaddlePaddle with sw support\" OFF)\noption(WITH_MIPS \"Compile PaddlePaddle with mips support\" OFF)\noption(WITH_LOONGARCH \"Compile PaddlePaddle with loongarch support\" OFF)\noption(WITH_MUSL \"Compile with musl libc instead of gblic\" OFF)\noption(WITH_UNITY_BUILD \"Compile with UnityBuild mode\" OFF)\noption(WITH_STRIP \"Strip so files of Whl packages\" OFF)\noption(NEW_RELEASE_PYPI\n       \"PaddlePaddle next-level release strategy for pypi cubin package\" OFF)\noption(NEW_RELEASE_ALL\n       \"PaddlePaddle next-level release strategy for all arches cubin package\"\n       OFF)\noption(NEW_RELEASE_JIT\n       \"PaddlePaddle next-level release strategy for backup jit package\" OFF)\noption(WITH_POCKETFFT \"Compile with pocketfft support\" ON)\noption(WITH_RECORD_BUILDTIME\n       \"Compile PaddlePaddle with record all targets build time\" OFF)\noption(WITH_CUSTOM_DEVICE \"Compile with custom device support\" OFF)\noption(WITH_ARM_BRPC \"Supprot Brpc in Arm\" OFF)\noption(WITH_FLPS \"FL PS mode\" OFF)\noption(WITH_RPC \"Compile with rpc support\" ${WITH_DISTRIBUTE})\noption(WITH_CUDNN_FRONTEND\n       \"Compile with CUDNN Frontend API support (experimental)\" OFF)\noption(WITH_SHARED_IR \"Compile PaddlePaddle with SHARED LIB of IR\" ON)\noption(WITH_NVCC_LAZY\n       \"Compile PaddlePaddle with nvcc lazy mode, used for CI-Inference only.\"\n       ON)\noption(BUILD_WHL_PACKAGE \"Build paddle whl package after compilation\" ON)\n\nif(WITH_RECORD_BUILDTIME)\n  set_property(\n    GLOBAL\n    PROPERTY\n      RULE_LAUNCH_COMPILE\n      \"${CMAKE_CURRENT_SOURCE_DIR}/tools/get_build_time.sh ${CMAKE_CURRENT_BINARY_DIR}\"\n  )\n  set_property(\n    GLOBAL\n    PROPERTY\n      RULE_LAUNCH_LINK\n      \"${CMAKE_CURRENT_SOURCE_DIR}/tools/get_build_time.sh ${CMAKE_CURRENT_BINARY_DIR}\"\n  )\nelse()\n  include(ccache\n  )# set ccache for compilation ; if WITH_RECORD_BUILDTIME=ON can't use ccache\nendif()\nunset(WITH_RECORD_BUILDTIME CACHE)\n\n# PY_VERSION\nif(NOT PY_VERSION)\n  set(PY_VERSION 3.8)\nelseif(${PY_VERSION} VERSION_LESS 3.8)\n  message(FATAL_ERROR \"Paddle only support Python version>=3.8 now\")\nendif()\nset(PYBIND11_PYTHON_VERSION ${PY_VERSION})\n\n# the type of sanitizer, options are: Address, Leak, Memory, Thread, Undefined. Default: OFF\nif(SANITIZER_TYPE AND NOT \"${SANITIZER_TYPE}\" MATCHES\n                      \"^(Address|Leak|Memory|Thread|Undefined)$\")\n  message(\"Choose the correct type of sanitizer\")\n  return()\nendif()\n\nif(LINUX\n   AND NOT WITH_CUSTOM_DEVICE\n   AND NOT WITH_GPU\n   AND NOT WITH_ROCM\n   AND NOT WITH_XPU\n   AND NOT WITH_XPU_KP\n   AND NOT WITH_XPU_XFT\n   AND WITH_PYTHON)\n  set(WITH_CUSTOM_DEVICE\n      ON\n      CACHE BOOL \"Enable Custom Device when compiling for Linux\" FORCE)\n  message(\n    \"Enable Custom Device when compiling for Linux. Force WITH_CUSTOM_DEVICE=ON.\"\n  )\nendif()\n\nif(WIN32)\n  if(WITH_DISTRIBUTE)\n    message(\n      WARNING\n        \"Disable DISTRIBUTE when compiling for Windows. Force WITH_DISTRIBUTE=OFF.\"\n    )\n    set(WITH_DISTRIBUTE\n        OFF\n        CACHE STRING \"Disable DISTRIBUTE when compiling for Windows\" FORCE)\n  endif()\n  if(WITH_NCCL)\n    message(\n      WARNING \"Disable NCCL when compiling for Windows. Force WITH_NCCL=OFF.\")\n    set(WITH_NCCL\n        OFF\n        CACHE STRING \"Disable NCCL when compiling for Windows\" FORCE)\n  endif()\nendif()\n\nif(NOT WITH_TESTING AND WITH_MULTINODE_TESTING)\n  message(\n    WARNING\n      \"Disable WITH_MULTINODE_TESTING when compiling without TESTING. Force WITH_MULTINODE_TESTING=OFF.\"\n  )\n  set(WITH_MULTINODE_TESTING\n      OFF\n      CACHE STRING\n            \"Disable WITH_MULTINODE_TESTING when compiling without TESTING\"\n            FORCE)\nendif()\n\nif(NOT WITH_GPU AND WITH_NCCL)\n  message(\n    WARNING \"Disable NCCL when compiling without GPU. Force WITH_NCCL=OFF.\")\n  set(WITH_NCCL\n      OFF\n      CACHE STRING \"Disable NCCL when compiling without GPU\" FORCE)\nendif()\n\n# force WITH_XPU on when WITH_XPU_KP\nif(WITH_XPU_KP AND NOT WITH_XPU)\n  message(\n    WARNING\n      \"Enable WITH_XPU when compiling with WITH_XPU_KP. Force WITH_XPU=ON.\")\n  set(WITH_XPU\n      ON\n      CACHE STRING \"Enable WITH_XPU when compiling with WITH_XPU_KP\" FORCE)\nendif()\n\nif(NOT WITH_XPU AND WITH_XPU_XFT)\n  message(\n    WARNING\n      \"Enable WITH_XPU when compiling with WITH_XPU_XFT. Force WITH_XPU=ON.\")\n  set(WITH_XPU\n      ON\n      CACHE STRING \"Enable WITH_XPU when compiling with WITH_XPU_XFT\" FORCE)\nendif()\n\nif(NOT WITH_XPU AND WITH_XPTI)\n  message(\n    WARNING \"Disable XPTI when compiling without XPU. Force WITH_XPTI=OFF.\")\n  set(WITH_XPTI\n      OFF\n      CACHE STRING \"Disable XPTI when compiling without XPU\" FORCE)\nendif()\n\nif(NOT WITH_XPU AND WITH_XPU_BKCL)\n  message(\n    WARNING \"Disable BKCL when compiling without XPU. Force WITH_XPU_BKCL=OFF.\")\n  set(WITH_XPU_BKCL\n      OFF\n      CACHE STRING \"Disable BKCL when compiling without XPU\" FORCE)\nendif()\n\nif(NOT WITH_XPU AND WITH_XPU_XRE5)\n  message((WARNING\n           \"Disable XRE5 when compiling without XPU. Force WITH_XPU_XRE5=OFF\"))\n  set(WITH_XPU_XRE5\n      OFF\n      CACHE STRING \"Disable XRE5 when compiling without XPU\" FORCE)\nendif()\n\nif(WITH_NCCL)\n  add_definitions(\"-DPADDLE_WITH_NCCL\")\n  include(nccl)\nelse()\n  if(WITH_GPU)\n    message(\n      WARNING\n        \"If the environment is multi-card, the WITH_NCCL option needs to be turned on, otherwise only a single card can be used.\"\n    )\n  endif()\nendif()\n\nif(WITH_BRPC_RDMA)\n  message(STATUS \"Use brpc with rdma.\")\n  if(NOT WITH_DISTRIBUTE)\n    message(FATAL_ERROR \"Can't use brpc rdma in no distribute env.\")\n  endif()\nendif()\n\nif(WITH_GPU)\n  include(cuda)\n  # lite subgraph compilation depends on CUDNN_ROOT,\n  # so include(cudnn) needs to be in front of include(third_party/lite)\n  include(cudnn) # set cudnn libraries, must before configure\n  include(tensorrt)\n  # there is no official support of nccl, cupti in windows\n  if(NOT WIN32)\n    include(cupti)\n  endif()\nendif()\n\nif(WITH_ROCM)\n  include(hip)\n  include(miopen) # set miopen libraries, must before configure\n  include(cupti)\nendif()\n\nif(WITH_XPU_KP)\n  include(xpu_kp)\nendif()\n\nif(NOT WITH_ROCM AND WITH_RCCL)\n  message(\n    WARNING \"Disable RCCL when compiling without ROCM. Force WITH_RCCL=OFF.\")\n  set(WITH_RCCL\n      OFF\n      CACHE STRING \"Disable RCCL when compiling without ROCM\" FORCE)\nendif()\n\nif(WITH_RCCL)\n  add_definitions(\"-DPADDLE_WITH_RCCL\")\n  include(rccl)\nelse()\n  if(WITH_ROCM)\n    message(\n      WARNING\n        \"If the environment is multi-card, the WITH_RCCL option needs to be turned on, otherwise only a single card can be used.\"\n    )\n  endif()\nendif()\n\nif(WITH_HETERPS AND WITH_PSLIB)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -D_GLIBCXX_USE_CXX11_ABI=0\")\nendif()\n\nif(WITH_DISTRIBUTE)\n  if(LINUX)\n    set(WITH_GLOO\n        ON\n        CACHE STRING \"Enable GLOO when compiling WITH_DISTRIBUTE=ON.\" FORCE)\n  endif()\n  if(WITH_ROCM AND HIP_VERSION LESS_EQUAL 40020496)\n    # TODO(qili93): third-party rocksdb throw Illegal instruction with HIP version 40020496\n    message(\n      WARNING\n        \"Disable WITH_PSCORE when HIP_VERSION is less than or equal 40020496. Force WITH_PSCORE=OFF.\"\n    )\n    set(WITH_PSCORE\n        OFF\n        CACHE\n          BOOL\n          \"Disable WITH_PSCORE when HIP_VERSION is less than or equal 40020496\"\n          FORCE)\n  endif()\nendif()\n\nif(WITH_RPC)\n  if(NOT LINUX)\n    message(\n      WARNING \"Disable WITH_RPC when not compiled on Linux. Force WITH_RPC=OFF.\"\n    )\n    set(WITH_RPC\n        OFF\n        CACHE BOOL \"Disable WITH_RPC when not compiled on Linux\" FORCE)\n  endif()\n  if(NOT WITH_DISTRIBUTE AND WITH_RPC)\n    message(\n      WARNING\n        \"Disable WITH_RPC when not compiled with distribute. Force WITH_RPC=OFF.\"\n    )\n    set(WITH_RPC\n        OFF\n        CACHE BOOL \"Disable WITH_RPC when not compiled with distribute\" FORCE)\n  endif()\n  if(WITH_ROCM AND WITH_RPC)\n    message(\n      WARNING \"Disable WITH_RPC when compiling with ROCM. Force WITH_RPC=OFF.\")\n    set(WITH_RPC\n        OFF\n        CACHE BOOL \"Disable WITH_RPC when compiling with ROCM\" FORCE)\n  endif()\n  if(WITH_XPU AND WITH_RPC)\n    message(\n      WARNING \"Disable WITH_RPC when compiling with XPU. Force WITH_RPC=OFF.\")\n    set(WITH_RPC\n        OFF\n        CACHE BOOL \"Disable WITH_RPC when compiling with XPU\" FORCE)\n  endif()\nendif()\n\nif(WITH_MPI)\n  include(mpi)\nendif()\n\ninclude(third_party\n)# download, build, install third_party, Contains about 20+ dependencies\n\ninclude(flags) # set paddle compile flags\ninclude(util) # set unittest and link libs\ninclude(version) # set PADDLE_VERSION\ninclude(coveralls) # set code coverage\ninclude(configure) # add paddle env configuration\n#------------- cinn cmake config start --------------\n\nif(WITH_CINN)\n  message(STATUS \"Compile Paddle with CINN.\")\n  # TODO(6clc): Use CINN_WITH_CUDNN to completely replace WITH_CUDNN in CINN.\n  #             Use WITH_GPU to completely replace WITH_CUDA in CINN.\n  set(WITH_MKL_CBLAS ${WITH_MKL})\n  if(WITH_GPU)\n    set(WITH_CUDA ${WITH_GPU})\n    add_definitions(-DCINN_WITH_CUDA)\n    set(WITH_CUDNN ${CINN_WITH_CUDNN})\n    if(WITH_CUDNN)\n      add_definitions(-DCINN_WITH_CUDNN)\n    endif()\n  endif()\n\n  include(cmake/cinn.cmake)\n  add_definitions(-DPADDLE_WITH_CINN)\nendif()\n\n#------------- cinn cmake config end --------------\n\nif(WITH_PROFILER)\n  find_package(Gperftools REQUIRED)\n  include_directories(${GPERFTOOLS_INCLUDE_DIR})\n  add_definitions(-DWITH_GPERFTOOLS)\nendif()\n\ninclude_directories(\"${PADDLE_SOURCE_DIR}\")\n\nif(WITH_NV_JETSON)\n  set(WITH_ARM\n      ON\n      CACHE STRING \"Set WITH_ARM=ON when compiling WITH_NV_JETSON=ON.\" FORCE)\nendif()\n\nif(WITH_ARM)\n  set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fPIC\")\n  set(WITH_XBYAK\n      OFF\n      CACHE STRING \"Disable XBYAK when compiling WITH_ARM=ON.\" FORCE)\n  set(WITH_MKL\n      OFF\n      CACHE STRING \"Disable MKL when compiling WITH_ARM=ON.\" FORCE)\n  set(WITH_AVX\n      OFF\n      CACHE STRING \"Disable AVX when compiling WITH_AVX=OFF.\" FORCE)\n  add_definitions(-DPADDLE_WITH_ARM)\nendif()\n\nif(WITH_SW)\n  # mieee flag solves floating-point exceptions under sw and ALPHA architectures\n  set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -mieee\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fPIC -mieee\")\n  set(WITH_XBYAK\n      OFF\n      CACHE STRING \"Disable XBYAK when compiling WITH_SW=ON\" FORCE)\n  set(WITH_MKL\n      OFF\n      CACHE STRING \"Disable MKL when compiling WITH_SW=ON.\" FORCE)\n  add_definitions(-DPADDLE_WITH_SW)\nendif()\n\nif(WITH_MIPS)\n  set(WITH_XBYAK\n      OFF\n      CACHE STRING \"Disable XBYAK when compiling WITH_MIPS=ON\" FORCE)\n  add_definitions(-DPADDLE_WITH_MIPS)\nendif()\n\nif(WITH_NVTX AND NOT WIN32)\n  add_definitions(-DPADDLE_WITH_NVTX)\nendif()\n\nif(WITH_LOONGARCH)\n  set(WITH_XBYAK\n      OFF\n      CACHE STRING \"Disable XBYAK when compiling WITH_LOONGARCH=ON\" FORCE)\n  set(WITH_MKL\n      OFF\n      CACHE STRING \"Disable MKL when compiling WITH_LOONGARCH=ON.\" FORCE)\n  add_definitions(-DPADDLE_WITH_LOONGARCH)\nendif()\n\nif(WITH_ONEMKL)\n  add_definitions(-DPADDLE_WITH_ONEMKL)\nendif()\n\nif(WITH_HETERPS)\n  if(CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 7.0)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -faligned-new\")\n  endif()\nendif()\nset(PADDLE_PYTHON_BUILD_DIR \"${CMAKE_CURRENT_BINARY_DIR}/python/build\")\n\nset(CMAKE_CXX_FLAGS_RELWITHDEBINFO \"-O3 -g -DNDEBUG\")\nset(CMAKE_C_FLAGS_RELWITHDEBINFO \"-O3 -g -DNDEBUG\")\n\nadd_definitions(-DPADDLE_DLL_EXPORT)\n\nif(ON_INFER)\n  # you can trun off the paddle fluid and inference lib by set ON_INFER=OFF\n  message(\n    STATUS \"On inference mode, will take place some specific optimization.\")\n  include(inference_lib)\n  add_definitions(-DPADDLE_ON_INFERENCE)\n  set(WITH_SHARED_IR\n      OFF\n      CACHE BOOL \"Only paddle_inference.so is allowed in inference.\" FORCE)\nelse()\n  #TODO(luotao), combine this warning with `make inference_lib_dist` command.\n  message(\n    WARNING\n      \"On inference mode, will take place some specific optimization. Turn on the ON_INFER flag when building inference_lib only.\"\n  )\nendif()\n\nif(NOT WITH_SHARED_IR)\n  add_definitions(-DSTATIC_IR)\nendif()\n\nif(WITH_STRIP)\n  find_program(STRIP_PATH strip)\n  if(NOT STRIP_PATH OR NOT LINUX)\n    set(WITH_STRIP\n        OFF\n        CACHE STRING \"Command strip is only used on Linux when it exists.\"\n              FORCE)\n  endif()\nendif()\n\nif(WITH_CPP_DIST)\n  # TODO(huangjiyi): Separate installing C++ distribution from python package\n  # installation and support for installing C++ distribution on more platforms.\n  if(NOT LINUX OR NOT WITH_PYTHON)\n    set(WITH_CPP_DIST\n        OFF\n        CACHE\n          STRING\n          \"Currently C++ Distribution Generation is only available on Linux and compiling WITH_PYTHON=ON.\"\n          FORCE)\n  else()\n    include(paddle_lib)\n  endif()\nendif()\n\nadd_subdirectory(paddle)\nif(WITH_PYTHON)\n  add_subdirectory(python)\nendif()\nadd_subdirectory(test)\n\nget_directory_property(all_inc_dirs INCLUDE_DIRECTORIES)\nlist(JOIN all_inc_dirs \"\\r\\n\" all_inc_dirs)\nfile(WRITE \"${CMAKE_CURRENT_BINARY_DIR}/includes.txt\" ${all_inc_dirs})\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.146484375,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment include:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at  ext_paddle_oss@baidu.com. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4, available at [http://contributor-covenant.org/version/1/4][version]\n\n[homepage]: http://contributor-covenant.org\n[version]: http://contributor-covenant.org/version/1/4/\n"
        },
        {
          "name": "CODE_OF_CONDUCT_cn.md",
          "type": "blob",
          "size": 2.5888671875,
          "content": "# \n\n## \n\n\n\n## \n\n\n* \n* \n* \n* \n* \n\n\n* \n* ///\n* \n* \n* \n\n## \n\n\n\n(comments)(commits)wiki (issues)\n\n## \n\n\n\n\n\n\n\n## \n\next_paddle_oss@baidu.com\n\n\n\n\n\n## \n\n[][] 1.4\nhttps://www.contributor-covenant.org/zh-cn/version/1/4/code-of-conduct.html\n\n[]: https://www.contributor-covenant.org\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 8.017578125,
          "content": "# Contribute Code\n\nYou are welcome to contribute to project PaddlePaddle. To contribute to PaddlePaddle, you have to agree with the\n[PaddlePaddle Contributor License Agreement](https://gist.github.com/XiaoguangHu01/75018ad8e11af13df97070dd18ae6808).\n\nWe sincerely appreciate your contribution.  This document explains our workflow and work style.\n\n## Workflow\n\nPaddlePaddle uses this [Git branching model](http://nvie.com/posts/a-successful-git-branching-model/).  The following steps guide usual contributions.\n\n1. Fork\n\n   Our development community has been growing fastly; it doesn't make sense for everyone to write into the official repo.  So, please file Pull Requests from your fork.  To make a fork,  just head over to the GitHub page and click the [\"Fork\" button](https://help.github.com/articles/fork-a-repo/).\n\n1. Clone\n\n   To make a copy of your fork to your local computers, please run\n\n   ```bash\n   git clone https://github.com/your-github-account/paddle\n   cd paddle\n   ```\n\n1. Create the local feature branch\n\n   For daily works like adding a new feature or fixing a bug, please open your feature branch before coding:\n\n   ```bash\n   git checkout -b my-cool-stuff\n   ```\n\n1. Commit\n\n   Before issuing your first `git commit` command, please install [`pre-commit`](http://pre-commit.com/) by running the following commands:\n\n   ```bash\n   pip install pre-commit\n   pre-commit install\n   ```\n\n   Our pre-commit configuration requires clang-format 3.8 for auto-formatting C/C++ code and yapf for Python.\n\n   Once installed, `pre-commit` checks the style of code and documentation in every commit.  We will see something like the following when you run `git commit`:\n\n   ```\n     git commit\n   CRLF end-lines remover...............................(no files to check)Skipped\n   yapf.................................................(no files to check)Skipped\n   Check for added large files..............................................Passed\n   Check for merge conflicts................................................Passed\n   Check for broken symlinks................................................Passed\n   Detect Private Key...................................(no files to check)Skipped\n   Fix End of Files.....................................(no files to check)Skipped\n   clang-format.........................................(no files to check)Skipped\n   [my-cool-stuff c703c041] add test file\n    1 file changed, 0 insertions(+), 0 deletions(-)\n    create mode 100644 233\n   ```\n\n\tNOTE: The `yapf` installed by `pip install pre-commit` and `conda install -c conda-forge pre-commit` is slightly different. Paddle developers use `pip install pre-commit`.\n\n1. Build and test\n\n   Users can build PaddlePaddle natively on Linux and Mac OS X.  But to unify the building environment and to make it easy for debugging, the recommended way is [using Docker](https://github.com/PaddlePaddle/Paddle/blob/develop/doc/howto/dev/build_en.md).\n\n1. Keep pulling\n\n   An experienced Git user pulls from the official repo often -- daily or even hourly, so they notice conflicts with others work early, and it's easier to resolve smaller conflicts.\n\n   ```bash\n   git remote add upstream https://github.com/PaddlePaddle/Paddle\n   git pull upstream develop\n   ```\n\n1. Push and file a pull request\n\n   You can \"push\" your local work into your forked repo:\n\n   ```bash\n   git push origin my-cool-stuff\n   ```\n\n   The push allows you to create a pull request, requesting owners of this [official repo](https://github.com/PaddlePaddle/Paddle) to pull your change into the official one.\n\n   To create a pull request, please follow [these steps](https://help.github.com/articles/creating-a-pull-request/).\n\n   If your change is for fixing an issue, please write [\"Fixes <issue-URL>\"](https://help.github.com/articles/closing-issues-using-keywords/) in the description section of your pull request.  Github would close the issue when the owners merge your pull request.\n\n   Please remember to specify some reviewers for your pull request.  If you don't know who are the right ones, please follow Github's recommendation.\n\n\n1. Delete local and remote branches\n\n   To keep your local workspace and your fork clean, you might want to remove merged branches:\n\n   ```bash\n   git push origin :my-cool-stuff\n   git checkout develop\n   git pull upstream develop\n   git branch -d my-cool-stuff\n   ```\n\n### Code Review\n\n-  Please feel free to ping your reviewers by sending them the URL of your pull request via IM or email.  Please do this after your pull request passes the CI.\n\n- Please answer reviewers' every comment.  If you are to follow the comment, please write \"Done\"; please give a reason otherwise.\n\n- If you don't want your reviewers to get overwhelmed by email notifications, you might reply their comments by [in a batch](https://help.github.com/articles/reviewing-proposed-changes-in-a-pull-request/).\n\n- Reduce the unnecessary commits.  Some developers commit often.  It is recommended to append a sequence of small changes into one commit by running `git commit --amend` instead of `git commit`.\n\n\n## Coding Standard\n\n### Code Style\n\nOur C/C++ code follows the [Google style guide](http://google.github.io/styleguide/cppguide.html).\n\nOur Python code follows the [PEP8 style guide](https://www.python.org/dev/peps/pep-0008/).\n\nOur build process helps to check the code style.  In [`build.sh`](https://github.com/PaddlePaddle/Paddle/blob/b84e8226514b8bb4405c3c28e54aa5077193d179/paddle/scripts/docker/build.sh#L42), the entry point of our [builder Docker image](https://github.com/PaddlePaddle/Paddle/blob/b84e8226514b8bb4405c3c28e54aa5077193d179/Dockerfile#L88), the CMake argument `WITH_STYLE_CHECK` is set to `ON` by default.  This flag is on\n\nPlease install pre-commit, which automatically reformat the changes to C/C++ and Python code whenever we run `git commit`.  To check the whole codebase, we can run the command `pre-commit run -a`, as in the [`check_style.sh` file](https://github.com/PaddlePaddle/Paddle/blob/b84e8226514b8bb4405c3c28e54aa5077193d179/paddle/scripts/travis/check_style.sh#L30), which is invoked by [our Travis CI configuration](https://github.com/PaddlePaddle/Paddle/blob/b84e8226514b8bb4405c3c28e54aa5077193d179/.travis.yml#L43).\n\n### Unit Tests\n\nPlease remember to add related unit tests.\n\n- For C/C++ code, please follow [`google-test` Primer](https://github.com/google/googletest/blob/master/googletest/docs/primer.md) .\n\n- For Python code, please use [Python's standard `unittest` package](http://pythontesting.net/framework/unittest/unittest-introduction/).\n\n\n### Writing Logs\n\nWe use [glog](https://github.com/google/glog) for logging in our C/C++ code.\n\nFor general information, please use `LOG`.  For debug information, please use [`VLOG`](http://htmlpreview.github.io/?https://github.com/google/glog/blob/master/doc/glog.html#verbose).  The reason is at [here](https://groups.google.com/a/chromium.org/d/msg/chromium-dev/3NDNd1KzXeY/AZKMMx37fdQJ).\n\n`VLOG` requires a *verbose level* parameter.  For example:\n\n```c++\nVLOG(3) << \"Operator FC is taking \" << num_inputs << \"inputs.\"\n```\n\nWhen we run a PaddlePaddle application or test, we can specify a verbose threshold.  For example:\n\n```bash\nGLOG_vmodule=buddy_allocator=2 \\\nGLOG_v=10 \\\npython \\\n../python/paddle/v2/framework/tests/test_recurrent_op.py\n```\n\nThis will enable VLOG messages generated by `buddy_allocator.{h,cc}` and in the verbose range of 0 to 3, so you will see above example VLOG message, which is in level 3.  This suggests that we output overall messages in lower verbose levels, so they display with higher probability.  When coding C++, please follow the verbose level convention as follows:\n\n- verbose level 1: [framework](https://github.com/PaddlePaddle/Paddle/tree/develop/paddle/fluid/framework)\n- verbose level 3: [operators](https://github.com/PaddlePaddle/Paddle/tree/develop/paddle/fluid/operators)\n- verbose level 5: [memory](https://github.com/PaddlePaddle/Paddle/tree/develop/paddle/fluid/memory), [platform](https://github.com/PaddlePaddle/Paddle/tree/develop/paddle/fluid/platform)\n- verbose level 7: [math](https://github.com/PaddlePaddle/Paddle/tree/develop/paddle/fluid/operators/math/)\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.169921875,
          "content": "Copyright (c) 2016 PaddlePaddle Authors. All Rights Reserved\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright (c) 2016 PaddlePaddle Authors. All Rights Reserved.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.57421875,
          "content": "<p align=\"center\">\n<img align=\"center\" src=\"doc/imgs/logo.png\", width=1600>\n<p>\n\n--------------------------------------------------------------------------------\n\nEnglish | [](./README_cn.md) | [](./README_ja.md)\n\n[![Documentation Status](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n[![Documentation Status](https://img.shields.io/badge/--brightgreen.svg)](https://paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html)\n[![Release](https://img.shields.io/github/release/PaddlePaddle/Paddle.svg)](https://github.com/PaddlePaddle/Paddle/releases)\n[![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](LICENSE)\n[![Twitter](https://img.shields.io/badge/Twitter-1ca0f1.svg?logo=twitter&logoColor=white)](https://twitter.com/PaddlePaddle)\n\nWelcome to the PaddlePaddle GitHub.\n\nPaddlePaddle, as the first independent R&D deep learning platform in China, has been officially open-sourced to professional communities since 2016. It is an industrial platform with advanced technologies and rich features that cover core deep learning frameworks, basic model libraries, end-to-end development kits, tools & components as well as service platforms.\nPaddlePaddle is originated from industrial practices with dedication and commitments to industrialization. It has been widely adopted by a wide range of sectors including manufacturing, agriculture, enterprise service, and so on while serving more than 10.7 million developers, 235,000 companies and generating 860,000 models. With such advantages, PaddlePaddle has helped an increasing number of partners commercialize AI.\n\n## Installation\n\n### Latest PaddlePaddle Release: [v2.6](https://github.com/PaddlePaddle/Paddle/tree/release/2.6)\n\nOur vision is to enable deep learning for everyone via PaddlePaddle.\nPlease refer to our [release announcement](https://github.com/PaddlePaddle/Paddle/releases) to track the latest features of PaddlePaddle.\n\n### Install Latest Stable Release\n\n``` sh\n# CPU\npip install paddlepaddle\n# GPU\npip install paddlepaddle-gpu\n```\n\nFor more information about installation, please view [Quick Install](https://www.paddlepaddle.org.cn/install/quick)\n\nNow our developers can acquire Tesla V100 online computing resources for free. If you create a program by AI Studio, you will obtain 8 hours to train models online per day. [Click here to start](https://aistudio.baidu.com/aistudio/index).\n\n## FOUR LEADING TECHNOLOGIES\n\n- **Agile Framework for Industrial Development of Deep Neural Networks**\n\n    The PaddlePaddle deep learning framework facilitates the development while lowering the technical burden, through leveraging a programmable scheme to architect the neural networks. It supports both declarative programming and imperative programming with both development flexibility and high runtime performance preserved.  The neural architectures could be automatically designed by algorithms with better performance than the ones designed by human experts.\n\n- **Support Ultra-Large-Scale Training of Deep Neural Networks**\n\n    PaddlePaddle has made breakthroughs in ultra-large-scale deep neural networks training. It launched the world's first large-scale open-source training platform that supports the training of deep networks with 100 billion features and trillions of parameters using data sources distributed over hundreds of nodes. PaddlePaddle overcomes the online deep learning challenges for ultra-large-scale deep learning models, and further achieved real-time model updating with more than 1 trillion parameters.\n     [Click here to learn more](https://github.com/PaddlePaddle/Fleet)\n\n- **High-Performance Inference Engines for Comprehensive Deployment Environments**\n\n   PaddlePaddle is not only compatible with models trained in 3rd party open-source frameworks , but also offers complete inference products for various production scenarios. Our inference product line includes [Paddle Inference](https://www.paddlepaddle.org.cn/inference/master/guides/introduction/index_intro.html): Native inference library for high-performance server and cloud inference; [FastDeploy](https://github.com/PaddlePaddle/FastDeploy): Easy-to-use and High Performance AI model deployment toolkit for Cloud, Mobile and Edge without-of-the-box and unified experience; [Paddle Lite](https://github.com/PaddlePaddle/Paddle-Lite): Ultra-Lightweight inference engine for mobile and IoT environments; [Paddle.js](https://www.paddlepaddle.org.cn/paddle/paddlejs): A frontend inference engine for browser and mini-apps. Furthermore, by great amounts of optimization with leading hardware in each scenario, Paddle inference engines outperform most of the other mainstream frameworks.\n\n- **Industry-Oriented Models and Libraries with Open Source Repositories**\n\n     PaddlePaddle includes and maintains more than 100 mainstream models that have been practiced and polished for a long time in the industry. Some of these models have won major prizes from key international competitions. In the meanwhile, PaddlePaddle has further more than 200 pre-training models (some of them with source codes) to facilitate the rapid development of industrial applications.\n     [Click here to learn more](https://github.com/PaddlePaddle/models)\n\n## Documentation\n\nWe provide [English](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html) and\n[Chinese](https://www.paddlepaddle.org.cn/documentation/docs/zh/guide/index_cn.html) documentation.\n\n- [Guides](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n\n  You might want to start from how to implement deep learning basics with PaddlePaddle.\n\n- [Practice](https://www.paddlepaddle.org.cn/documentation/docs/zh/tutorial/index_cn.html)\n\n  So far you have already been familiar with Fluid. And the next step should be building a more efficient model or inventing your original Operator.\n\n- [API Reference](https://www.paddlepaddle.org.cn/documentation/docs/en/api/index_en.html)\n\n   Our new API enables much shorter programs.\n\n- [How to Contribute](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/08_contribution/index_en.html)\n\n   We appreciate your contributions!\n\n## Open Source Community\n\n- [Github Issues](https://github.com/PaddlePaddle/Paddle/issues): bug reports, feature requests, install issues, usage issues, etc.\n\n- Open Source Contribution Activities:\n\n  - Beginner: Happy Open Source Activity[Regular Season](https://github.com/PaddlePaddle/Paddle/issues/56689)[Pre-Hackathon Camp](https://github.com/PaddlePaddle/Paddle/issues/58497)\n  - Advanced: PaddlePaddle Hackathon[Personal Challenge Competition](https://github.com/PaddlePaddle/Paddle/issues/57262)[LLM Application Competition](https://github.com/PaddlePaddle/Paddle/issues/57585)[Hackathon Code Camp](https://github.com/PaddlePaddle/Paddle/issues/57264)\n\n- Community Organizations:\n  - Technical Organization: [Paddle Framework Contributor Club, PFCC](https://github.com/PaddlePaddle/community/tree/master/pfcc)\n  - Community Governance Organization: [PaddlePaddle OpenSource Development Working Group, PPOSDWG](https://github.com/PaddlePaddle/community/tree/master/pposdwg)\n\n- Community Blog: <https://pfcc.blog/>\n\n## Courses\n\n- [Server Deployments](https://aistudio.baidu.com/aistudio/course/introduce/19084): Courses introducing high performance server deployments via local and remote services.\n- [Edge Deployments](https://aistudio.baidu.com/aistudio/course/introduce/22690): Courses introducing edge deployments from mobile, IoT to web and applets.\n\n## Copyright and License\n\nPaddlePaddle is provided under the [Apache-2.0 license](LICENSE).\n"
        },
        {
          "name": "README_cn.md",
          "type": "blob",
          "size": 6.109375,
          "content": "\n<p align=\"center\">\n<img align=\"center\" src=\"doc/imgs/logo.png\", width=1600>\n<p>\n\n--------------------------------------------------------------------------------\n\n[English](./README.md) |  | [](./README_ja.md)\n\n[![Documentation Status](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n[![Documentation Status](https://img.shields.io/badge/--brightgreen.svg)](https://paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html)\n[![Release](https://img.shields.io/github/release/PaddlePaddle/Paddle.svg)](https://github.com/PaddlePaddle/Paddle/releases)\n[![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](LICENSE)\n\n PaddlePaddle GitHub\n\n(PaddlePaddle) 107023.586AIAIAI\n\n## \n\n### PaddlePaddle : [v2.6](https://github.com/PaddlePaddle/Paddle/tree/release/2.6)\n\n PaddlePaddle [](https://github.com/PaddlePaddle/Paddle/releases)\n\n### \n\n``` sh\n# CPU\npip install paddlepaddle\n# GPU\npip install paddlepaddle-gpu\n```\n\n [](https://www.paddlepaddle.org.cn/install/quick)\n\nPaddlePaddle**Tesla V100****8**[](https://aistudio.baidu.com/aistudio/index)\n\n## \n\n- ****\n\n    \n\n- ****\n\n    \n    [](https://github.com/PaddlePaddle/Fleet)\n\n- ****\n\n     [Paddle Inference](https://www.paddlepaddle.org.cn/inference/master/guides/introduction/index_intro.html)AI [FastDeploy](https://github.com/PaddlePaddle/FastDeploy) [Paddle Lite](https://github.com/PaddlePaddle/Paddle-Lite) [Paddle.js](https://www.paddlepaddle.org.cn/paddle/paddlejs), \n\n- ****\n\n    100200\n    [](https://github.com/PaddlePaddle/models)\n\n## \n\n [](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)  [](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html) \n\n- [](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html)\n\n- [](https://www.paddlepaddle.org.cn/documentation/docs/zh/tutorial/index_cn.html)\n\n- [API ](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/index_cn.html) API \n\n- [](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/08_contribution/index_cn.html)\n\n## \n\n- [Github Issues](https://github.com/PaddlePaddle/Paddle/issues)/bug\n- \n\n  - [ + ](https://github.com/PaddlePaddle/Paddle/issues/56689)[](https://github.com/PaddlePaddle/Paddle/issues/58497)\n  - [](https://github.com/PaddlePaddle/Paddle/issues/57262)[](https://github.com/PaddlePaddle/Paddle/issues/57585)[](https://github.com/PaddlePaddle/Paddle/issues/57264)\n\n- \n  - [ PFCC](https://github.com/PaddlePaddle/community/tree/master/pfcc)\n  - [ PPOSDWG](https://github.com/PaddlePaddle/community/tree/master/pposdwg)\n\n- <https://pfcc.blog/>\n\n## \n\n- [](https://aistudio.baidu.com/aistudio/course/introduce/19084)Serving\n- [](https://aistudio.baidu.com/aistudio/course/introduce/22690)IoT\n\n## \n\nPaddlePaddle[Apache-2.0 license](LICENSE)\n"
        },
        {
          "name": "README_ja.md",
          "type": "blob",
          "size": 8.29296875,
          "content": "<p align=\"center\">\n<img align=\"center\" src=\"doc/imgs/logo.png\", width=1600>\n<p>\n\n--------------------------------------------------------------------------------\n\n[English](./README.md) | [](./README_cn.md) | \n\n[![Documentation Status](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n[![Documentation Status](https://img.shields.io/badge/--brightgreen.svg)](https://paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html)\n[![Release](https://img.shields.io/github/release/PaddlePaddle/Paddle.svg)](https://github.com/PaddlePaddle/Paddle/releases)\n[![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](LICENSE)\n[![Twitter](https://img.shields.io/badge/Twitter-1ca0f1.svg?logo=twitter&logoColor=white)](https://twitter.com/PaddlePaddle_)\n\nPaddlePaddle GitHub \n\nPaddlePaddle  R&D 2016\nPaddlePaddle 107023.586 PaddlePaddle  AI \n\n## \n\n### PaddlePaddle : [v2.6](https://github.com/PaddlePaddle/Paddle/tree/release/2.6)\n\nPaddlePaddle \nPaddlePaddle [](https://github.com/PaddlePaddle/Paddle/releases)\n\n### \n\n``` sh\n# CPU\npip install paddlepaddle\n# GPU\npip install paddlepaddle-gpu\n```\n\n[](https://www.paddlepaddle.org.cn/install/quick)\n\n Tesla V100 AI Studio 18[](https://aistudio.baidu.com/aistudio/index)\n\n## \n\n- ****\n\n    PaddlePaddle  \n\n- ****\n\n    PaddlePaddle 1000PaddlePaddle 1\n     [](https://github.com/PaddlePaddle/Fleet)\n\n- ****\n\n   PaddlePaddle [Paddle Inference](https://www.paddlepaddle.org.cn/inference/master/guides/introduction/index_intro.html) ; [FastDeploy](https://github.com/PaddlePaddle/FastDeploy): AI; [Paddle Lite](https://github.com/PaddlePaddle/Paddle-Lite)  IoT ; [Paddle.js](https://www.paddlepaddle.org.cn/paddle/paddlejs) Paddle \n\n- ****\n\n     PaddlePaddle 100PaddlePaddle 200\n     [](https://github.com/PaddlePaddle/models)\n\n## \n\n[](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n[](https://www.paddlepaddle.org.cn/documentation/docs/zh/guide/index_cn.html)\n\n- [](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n\n  PaddlePaddle \n\n- [](https://www.paddlepaddle.org.cn/documentation/docs/zh/tutorial/index_cn.html)\n\n  Paddle \n\n- [API ](https://www.paddlepaddle.org.cn/documentation/docs/en/api/index_en.html)\n\n    API \n\n- [](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/08_contribution/index_en.html)\n\n   \n\n## \n\n- [Github Issues](https://github.com/PaddlePaddle/Paddle/issues): \n- QQ: 441226485 (PaddlePaddle)\n- [](https://aistudio.baidu.com/paddle/forum): \n\n## \n\n- [Server Deployments](https://aistudio.baidu.com/aistudio/course/introduce/19084): \n- [Edge Deployments](https://aistudio.baidu.com/aistudio/course/introduce/22690): IoT  Web\n\n## Copyright \n\nPaddlePaddle  [Apache-2.0 license](LICENSE) \n"
        },
        {
          "name": "RELEASE.md",
          "type": "blob",
          "size": 0.1025390625,
          "content": "# Release Note\n\nPlease turn to [here](https://github.com/PaddlePaddle/Paddle/releases) for release note.\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 4.8310546875,
          "content": "# Using PaddlePaddle Securely\n\nThis document describes model security and code security in PaddlePaddle. It also provides guidelines on how to report vulnerabilities in PaddlePaddle.\n\n## PaddlePaddle Model Security\n\nPaddlePaddle attaches great importance to security and privacy of model. This includes how to prevent the model from outputting wrong decision results under the interference when it is used in security-related and safety-critical scenarios, and how to avoid leaking data and privacy information from the model itself, the model gradient or the model inference results.\n\n\n\n[PaddleSleeve](https://github.com/PaddlePaddle/PaddleSleeve) provides a series of security and privacy tools, which can help model developers and users systematically evaluate and improve the model security and privacy in both development and deployment stages.\n\n\n\nThese tools include adversarial example evaluation test, pseudo-natural environment robustness evaluation test, model reversing evaluation test, member inference evaluation test, sample denoising, adversarial training, privacy enhancement optimizer, etc.\n\n### Running untrusted models\n\nAlways load and execute untrusted models inside a sandbox and be sure to know the security impacts.\nThere are several ways in which a model could become untrusted. PaddlePaddle has enough features to impact on the system. (e.g. `paddle.load` uses [pickle](https://docs.python.org/3/library/pickle.html) implicitly, which may cause malformed models to achieve arbitrary code execution). So we recommend when using the untrusted models, you need to carefully audit it and run PaddlePaddle inside a sandbox.\n\n## PaddlePaddle Code Security\n\nPaddlePaddle always take code security seriously. However, due to the complexity of the framework and its dependence on other thirdparty open source libraries, there may still be some security issues undetected. Therefore, we hope that more security researchers and PaddlePaddle developers can participate in the code security program. We encourage responsible disclosure of security issues, as well as contributing code to improve our vulnerability finding tools to make PaddlePaddle safer.\n\n### Code security tools\n\nPaddlePaddle security team attaches great importance to the security of the framework. In order to find and fix security issues as soon as possible, we are continuously conducting code security audit and developing automatic vunerability discovery tools. We have already open sourced some of them to the community, hoping this could encourage people to contribute and improve the safety and robustness of PaddlePaddle. [This tool](https://github.com/PaddlePaddle/PaddleSleeve/tree/main/CodeSecurity) includes two parts. The dynamic part includes some op fuzzer samples. And the static part includes some CodeQL samples. Both of them are aim to find vulnerabilities in PaddlePaddle framework codebase. By referring the samples, security researchers can write their own fuzzers or QLs to test more PaddlePaddle modules, and find more code security issues.\n\n### Reporting vulnerabilities\n\nWe encourage responsible disclosure of security issues to PaddlePaddle and please email reports about any security issues you find to paddle-security@baidu.com.\n\n\n\nAfter the security team receives your email, they will communicate with you in time. The security team will work to keep you informed of an issue fix.\n\n\n\nIn order to reproduce and identify the issue, please include the following information along with your email:\n\n- The details of the vulnerability including how to reproduce it. Try to attach a PoC.\n- The attack scenario and what an attacker might be able to achieve with this issue.\n- Whether this vulnerability has been made public. If it is, please attach details.\n- Your name and affiliation.\n\nWe will indicate the bug fix in the release of PaddlePaddle, and publish the vulnerability detail and the reporter in the security advisories (Your name will not be published if you choose to remain anonymous).\n\n### What is a vulnerability?\n\nIn the process of computation graphs in PaddlePaddle, models can perform arbitrary computations , including reading and writing files, communicating with the network, etc. It may cause memory exhaustion, deadlock, etc., which will lead to unexpected behavior of PaddlePaddle. We consider these behavior to be security vulnerabilities only if they are out of the intention of the operation involved.\n\n\n\nSome unexpected parameters and behaviors have been checked in PaddlePaddle by throwing exceptions in Python or return error states in C++. In these cases, denial of service is still possible, but the exit of the PaddlePaddle is clean. Since the error handling of PaddlePaddle is expected and correct, these cases are not security vulnerabilities.\n\n\n\nIf malicious input can trigger memory corruption or non-clean exit, such bug is considered a security problem.\n\n\n\n[security advisories](./security/README.md)\n"
        },
        {
          "name": "SECURITY_cn.md",
          "type": "blob",
          "size": 4.0859375,
          "content": "# \n\n\n\n\n\n## \n\n\n\n[PaddleSleeve](https://github.com/PaddlePaddle/PaddleSleeve)\n\n### \n\n\n`paddle.load` [pickle](https://docs.python.org/3/library/pickle.html)\n\n## \n\n(Responsible Disclosure)\n\n### \n\n[CodeSecurity](https://github.com/PaddlePaddle/PaddleSleeve/tree/main/CodeSecurity)CodeQL\n\n### \n\n paddle-security@baidu.com\n\n\n\n\n\n- PoC\n- \n- \n- \n\n\n\n### \n\n\n\nPythonC++\n\n\n\n### [](./security/README_cn.md)\n"
        },
        {
          "name": "SECURITY_ja.md",
          "type": "blob",
          "size": 6.51171875,
          "content": "# PaddlePaddle \n\nPaddlePaddle PaddlePaddle \n\n## PaddlePaddle \n\nPaddlePaddle \n\n\n\n[PaddleSleeve](https://github.com/PaddlePaddle/PaddleSleeve) \n\n\n\n\n\n### \n\n\nPaddlePaddle (`paddle.load`  [pickle](https://docs.python.org/3/library/pickle.html) ) PaddlePaddle \n\n## PaddlePaddle \n\nPaddlePaddle  PaddlePaddle PaddlePaddle \n\n### \n\nPaddlePaddle PaddlePaddle [](https://github.com/PaddlePaddle/PaddleSleeve/tree/main/CodeSecurity) 2 op CodeQL PaddlePaddle  PaddlePaddle  QL \n\n### \n\nPaddlePaddle paddle-security@baidu.com \n\n\n\n\n\n\n\n:\n\n- PoC \n- \n- \n- \n\n PaddlePaddle \n\n### \n\nPaddlePaddle PaddlePaddle \n\n\n\nPythonC++PaddlePaddle PaddlePaddle PaddlePaddle \n\n\n\n\n\n\n\n[](./security/README_ja.md)\n"
        },
        {
          "name": "_typos.toml",
          "type": "blob",
          "size": 7.5419921875,
          "content": "[files]\n# The following files will be excluded from spell check during commits\nextend-exclude = [\n    \"third_party\",\n    \"patches\",\n    \"build\",\n    # Skip `intermidiate` check in these files\n    \"test/cpp/eager/task_tests/CMakeLists.txt\",\n    \"test/cpp/eager/task_tests/hook_test_intermidiate.cc\",\n    # Skip `creater` check in these files\n    \"paddle/fluid/inference/tensorrt/convert/CMakeLists.txt\",\n    \"paddle/fluid/inference/tensorrt/convert/generic_and_custom_plugin_creater.cc\",\n    \"paddle/fluid/inference/tensorrt/convert/test_custom_plugin_creater.cc\",\n]\n\n[default]\n# Ignore 1-3 letter words, refer to https://github.com/crate-ci/typos/issues/1079\nextend-ignore-words-re = [\"^[a-zA-Z]{1,3}$\"]\n# refer to https://github.com/crate-ci/typos/blob/master/docs/reference.md#example-configurations\nextend-ignore-re = [\n    # Ignore lines by `# typos: disable-line`\n    \"(?Rm)^.*(#|//)\\\\s*typos:\\\\s*disable-line$\",\n    # Ignore block by `# typos: off` and `# typos: on`\n    \"(?s)(#|//)\\\\s*typos:\\\\s*off.*?\\\\n\\\\s*(#|//)\\\\s*typos:\\\\s*on\"\n]\n\n[default.extend-words]\n# PaddlePaddle specific words\narange = \"arange\"\nastroid = 'astroid'\ncacl = 'cacl'\nCANN = 'CANN'\nClas = 'Clas'\nclen = 'clen'\ndatas = 'datas'\ndota = 'dota'\ndout = \"dout\"\neles = 'eles'\nentrys = 'entrys'\nfeeded = 'feeded'\ngrad = \"grad\"\nHalfs = 'Halfs'\nkinf = 'kinf'\npash = 'pash'\nunpacket = \"unpacket\"\n\n# These words need to be fixed\nIndexs = 'Indexs'\nindexs = 'indexs'\nInfered = 'Infered'\ninfered = 'infered'\ninitilized = 'initilized'\ninitalized = 'initalized'\ninitalize = 'initalize'\nintialize = 'intialize'\ninital = 'inital'\nInouts = 'Inouts'\nintputs = 'intputs'\ninputed = 'inputed'\nintput = 'intput'\nIntput = 'Intput'\ninser = 'inser'\ninsid = 'insid'\ninsepection = 'insepection'\nintall = 'intall'\ninstanciate = 'instanciate'\nOperants = 'Operants'\noperants = 'operants'\noptin = 'optin'\nOptin = 'Optin'\nrder = 'rder'\noreder = 'oreder'\norignal = 'orignal'\norginal = 'orginal'\nonces = 'onces'\noutter = 'outter'\noutpus = 'outpus'\noutout = 'outout'\nouput = 'ouput'\noutpout = 'outpout'\nouptut = 'ouptut'\nOuput = 'Ouput'\noverriden = 'overriden'\nOveride = 'Overide'\noveride = 'overide'\noverrided = 'overrided'\nPackge = 'Packge'\npacakage = 'pacakage'\npadd = 'padd'\npayed = 'payed'\nparellel = 'parellel'\nparm = 'parm'\nParm = 'Parm'\nPARM = 'PARM'\nparamters = 'paramters'\nParamters = 'Paramters'\nparamter = 'paramter'\nParamater = 'Paramater'\nparamete = 'paramete'\nparmeter = 'parmeter'\nparemeter = 'paremeter'\nparrent = 'parrent'\nparital = 'parital'\npartitial = 'partitial'\nPartitial = 'Partitial'\nPartion = 'Partion'\npartion = 'partion'\npatition = 'patition'\npasss = 'passs'\nPasss = 'Passs'\npathes = 'pathes'\npatten = 'patten'\nPatten = 'Patten'\npattens = 'pattens'\nprecent = 'precent'\nperformace = 'performace'\nperfomed = 'perfomed'\nperfome = 'perfome'\npresistable = 'presistable'\npyhsical = 'pyhsical'\npipline = 'pipline'\npalce = 'palce'\nPlacment = 'Placment'\npleace = 'pleace'\nPoniter = 'Poniter'\npoped = 'poped'\npositon = 'positon'\nPOSTION = 'POSTION'\npostive = 'postive'\npossiable = 'possiable'\npotentialy = 'potentialy'\nprecending = 'precending'\nPrepration = 'Prepration'\nPrepar = 'Prepar'\npreprocesser = 'preprocesser'\npriorites = 'priorites'\nprobabalistic = 'probabalistic'\nprocesser = 'processer'\nproccess = 'proccess'\nproducted = 'producted'\nprogam = 'progam'\nprogrss = 'progrss'\npropogated = 'propogated'\nPropogation = 'Propogation'\nprotocal = 'protocal'\nPROTOCAL = 'PROTOCAL'\npyrhon = 'pyrhon'\npthon = 'pthon'\nRefered = 'Refered'\nrefered = 'refered'\nregisted = 'registed'\nregist = 'regist'\nRegist = 'Regist'\nRegiste = 'Registe'\nregiste = 'registe'\nREGIST = 'REGIST'\nRegiter = 'Regiter'\nreleated = 'releated'\nrealease = 'realease'\nrelase = 'relase'\nreomve = 'reomve'\nReoder = 'Reoder'\nrepeatly = 'repeatly'\nrepeate = 'repeate'\nrepalce = 'repalce'\nrepresention = 'represention'\nrequied = 'requied'\nReqiured = 'Reqiured'\nRequred = 'Requred'\nrequirments = 'requirments'\nReseting = 'Reseting'\nreseted = 'reseted'\nresouce = 'resouce'\nretore = 'retore'\nrewriten = 'rewriten'\nrewrited = 'rewrited'\nRuning = 'Runing'\nruning = 'runing'\nSMAE = 'SMAE'\nsatifies = 'satifies'\nsclar = 'sclar'\nsacle = 'sacle'\nschduler = 'schduler'\nsheduler = 'sheduler'\nshedule = 'shedule'\nscheule = 'scheule'\nserach = 'serach'\nseconde = 'seconde'\nSeleceted = 'Seleceted'\nsence = 'sence'\nseperately = 'seperately'\nseperate = 'seperate'\nsepearate = 'sepearate'\nseperating = 'seperating'\nseperator = 'seperator'\nsequnce = 'sequnce'\nseqence = 'seqence'\nsequece = 'sequece'\nsequnece = 'sequnece'\nsequentail = 'sequentail'\nserailize = 'serailize'\nsettting = 'settting'\nsetted = 'setted'\nshoule = 'shoule'\nshoud = 'shoud'\nSingal = 'Singal'\nSimiliar = 'Similiar'\nsimular = 'simular'\nSimle = 'Simle'\nsignle = 'signle'\nSkiped = 'Skiped'\nskiped = 'skiped'\nsmll = 'smll'\nsamll = 'samll'\nsomme = 'somme'\npatial = 'patial'\nPatial = 'Patial'\nspecificed = 'specificed'\nsplite = 'splite'\nspliter = 'spliter'\nspliting = 'spliting'\nSpliting = 'Spliting'\nsplited = 'splited'\nsplitted = 'splitted'\nSplited = 'Splited'\nsqaure = 'sqaure'\nsequeze = 'sequeze'\nstarup = 'starup'\nstatment = 'statment'\nstaticly = 'staticly'\nstaticaly = 'staticaly'\nStati = 'Stati'\nSTOPED = 'STOPED'\nStoped = 'Stoped'\nstoped = 'stoped'\nstoreage = 'storeage'\nsotring = 'sotring'\nstragety = 'stragety'\nstrem = 'strem'\nstructed = 'structed'\nsturcture = 'sturcture'\nsubsituted = 'subsituted'\nsubsitute = 'subsitute'\nsubstitude = 'substitude'\nsubstitue = 'substitue'\nSubsitute = 'Subsitute'\nSubstitude = 'Substitude'\nsubstract = 'substract'\nSubstract = 'Substract'\nsuccessed = 'successed'\nsucessfully = 'sucessfully'\nSucceess = 'Succeess'\nSuger = 'Suger'\nsupportted = 'supportted'\nsupoort = 'supoort'\nSupprot = 'Supprot'\nsuport = 'suport'\nsuppport = 'suppport'\nSWTICH = 'SWTICH'\nSwith = 'Swith'\nsysyem = 'sysyem'\ntenosr = 'tenosr'\niterm = 'iterm'\ntermiante = 'termiante'\nTheoritical = 'Theoritical'\nther = 'ther'\nthge = 'thge'\nthouse = 'thouse'\ntheads = 'theads'\nthrads = 'thrads'\nthre = 'thre'\nTHREAHOLD = 'THREAHOLD'\nTHORW = 'THORW'\ntimout = 'timout'\ntiemout = 'tiemout'\nTOOD = 'TOOD'\ntood = 'tood'\nTDOD = 'TDOD'\ntoghether = 'toghether'\ntrainning = 'trainning'\nTraning = 'Traning'\ntransforme = 'transforme'\ntransfered = 'transfered'\ntransfering = 'transfering'\ntranfers = 'tranfers'\ntranfer = 'tranfer'\nTranfer = 'Tranfer'\ntransfrom = 'transfrom'\ntranform = 'tranform'\nTranpose = 'Tranpose'\ntranpose = 'tranpose'\ntigger = 'tigger'\ntrimed = 'trimed'\ntrival = 'trival'\nTrye = 'Trye'\nTring = 'Tring'\ntring = 'tring'\ntunning = 'tunning'\nTYPLE = 'TYPLE'\ntrun = 'trun'\ntyep = 'tyep'\ntpye = 'tpye'\nunchangable = 'unchangable'\nUndefind = 'Undefind'\nunser = 'unser'\nUNEXPECT = 'UNEXPECT'\nUnifrom = 'Unifrom'\nuninitialzed = 'uninitialzed'\nUniqe = 'Uniqe'\nunqiue = 'unqiue'\nuniqe = 'uniqe'\nunkown = 'unkown'\nunkonwn = 'unkonwn'\nUnkown = 'Unkown'\nunsupport = 'unsupport'\nupsupported = 'upsupported'\nUnsupport = 'Unsupport'\nUNSUPPORT = 'UNSUPPORT'\nunziped = 'unziped'\nudpated = 'udpated'\nupgarde = 'upgarde'\nuptream = 'uptream'\nunsed = 'unsed'\nuesd = 'uesd'\nusefull = 'usefull'\nusless = 'usless'\nvaccum = 'vaccum'\nvalud = 'valud'\nVAILD = 'VAILD'\nvalus = 'valus'\nvaluse = 'valuse'\nVarible = 'Varible'\nvaraible = 'varaible'\nvecotr = 'vecotr'\nverson = 'verson'\nvesion = 'vesion'\nVetical = 'Vetical'\nvunerability = 'vunerability'\nvarn = 'varn'\nwarpped = 'warpped'\nwarpper = 'warpper'\nWarpper = 'Warpper'\nwheather = 'wheather'\nwether = 'wether'\nWether = 'Wether'\nwieghts = 'wieghts'\nwerid = 'werid'\nWheter = 'Wheter'\nwhther = 'whther'\nwhill = 'whill'\nwhos = 'whos'\nwiil = 'wiil'\nwitk = 'witk'\nworke = 'worke'\nworkround = 'workround'\nworksapce = 'worksapce'\nwrappered = 'wrappered'\nwraper = 'wraper'\nwraping = 'wraping'\nWritter = 'Writter'\nwrited = 'writed'\n"
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "paddle",
          "type": "tree",
          "content": null
        },
        {
          "name": "patches",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 4.212890625,
          "content": "[tool.black]\nline-length = 80\nskip-string-normalization = true\ntarget-version = [\"py38\", \"py39\", \"py310\", \"py311\", \"py312\"]\nextend-exclude = '''\n(\n    third_party/.+      # Exclude third_party directory\n    | build/.+          # Exclude build directory\n)\n'''\n\n[tool.ruff]\nexclude = [\n    \"./build\",\n    \"third_party\",\n    \"./python/paddle/utils/gast/**\",\n]\nline-length = 80\ntarget-version = \"py38\"\n\n[tool.ruff.format]\n# Prevent change to double quotes by some users use ruff format\nquote-style = \"preserve\"\n\n[tool.ruff.lint]\nselect = [\n    # Pycodestyle\n    \"E\",\n    \"W\",\n\n    # Pyflakes\n    \"F\",\n\n    # Isort\n    \"I\",\n\n    # Comprehensions\n    \"C4\",\n\n    # Debugger\n    \"T100\",\n\n    # Pyupgrade\n    \"UP\",\n\n    # Flake8-pyi\n    \"PYI\",\n\n    # NumPy-specific rules\n    \"NPY001\",\n    \"NPY003\",\n    \"NPY201\",\n\n    # Bugbear\n    \"B002\",\n    \"B003\",\n    \"B004\",\n    \"B009\",\n    \"B010\",\n    \"B011\",\n    \"B012\",\n    \"B013\",\n    \"B014\",\n    \"B015\",\n    \"B016\",\n    \"B017\",\n    \"B018\",\n    \"B019\",\n    \"B020\",\n    \"B021\",\n    \"B022\",\n    \"B025\",\n    \"B029\",\n    \"B032\",\n\n    # Pylint\n    \"PLE\",\n    \"PLC3002\",\n    \"PLR0206\",\n    \"PLR0402\",\n    \"PLR1711\",\n    \"PLR1722\",\n    \"PLW3301\",\n\n    # Flake8-simplify\n    \"SIM101\",\n\n    # Pygrep-hooks\n    \"PGH004\",\n\n    # Flake8-type-checking\n    \"TCH\",\n\n    # Ruff-specific rules\n    \"RUF005\",\n    \"RUF008\",\n    \"RUF009\",\n    \"RUF010\",\n    \"RUF013\",\n    \"RUF015\",\n    \"RUF016\",\n    \"RUF017\",\n    \"RUF018\",\n    \"RUF019\",\n    \"RUF020\",\n    \"RUF024\",\n    \"RUF026\",\n    \"RUF100\",\n\n    # Flake8-raise\n    \"RSE\",\n\n    # Flake8-quotes\n    \"Q003\",\n    \"Q004\",\n\n    # Refurb\n    \"FURB\",\n\n    # Flake8-future-annotations\n    \"FA\",\n]\nunfixable = [\n    \"NPY001\"\n]\nignore = [\n    # Whitespace before ,, ;, or :, it is not compatible with black\n    \"E203\",\n    # Module level import not at top of file\n    \"E402\",\n    # Line too long (82 > 79 characters)\n    \"E501\",\n    # Do not compare types, use `isinstance()`\n    \"E721\",\n    # Do not use bare except, specify exception instead\n    \"E722\",\n    # Do not assign a lambda expression, use a def\n    \"E731\",\n    # Do not use variables named l, O, or I\n    \"E741\",\n    # `name` may be undefined, or defined from star imports: `module`\n    \"F405\",\n    # Local variable name is assigned to but never used\n    \"F841\",\n    # It not met the \"Explicit is better than implicit\" rule\n    \"UP015\",\n    # It will cause the performance regression on python3.10\n    \"UP038\",\n    # collections.namedtuple can be quickly created a inlined class\n    \"PYI024\",\n    # `__all__.append` is a common pattern in Paddle\n    \"PYI056\",\n]\n\n[tool.ruff.lint.isort]\ncombine-as-imports = true\nknown-first-party = [\"paddle\"]\n\n[tool.ruff.lint.per-file-ignores]\n# Ignore for re-export in __init__ files\n\"__init__.py\" = [\"PLC0414\"]\n# Ignore compare with True in sot unittest\n\"test/sot/test_dup_top.py\" = [\"E712\"]\n# Ignore undefined variables in CMake config and some dygraph_to_static tests\n\".cmake-format.py\" = [\"F821\"]\n\"test/dygraph_to_static/test_closure_analysis.py\" = [\"F821\"]\n# Ignore version check in setup.py\n\"setup.py\" = [\"UP036\"]\n# Ignore unnecessary comprehension in dy2st unittest test_loop\n\"test/dygraph_to_static/test_loop.py\" = [\"C416\", \"F821\"]\n# Ignore unnecessary lambda in dy2st unittest test_lambda\n\"test/dygraph_to_static/test_lambda.py\" = [\"PLC3002\"]\n# Ignore docstring in tensor.pyi\n\"python/paddle/tensor/tensor.prototype.pyi\" = [\"PYI021\", \"PYI048\"]\n# Temproray ignore some dy2st test case because SOT bug\n# See https://github.com/PaddlePaddle/Paddle/pull/67344#discussion_r1714155671 for more details\n\"test/dygraph_to_static/seq2seq_dygraph_model.py\" = [\"RUF005\"]\n\n[tool.mypy]\npython_version = \"3.8\"\ncache_dir = \".mypy_cache\"\n# Miscellaneous strictness flags\nallow_redefinition = true\nlocal_partial_types = true\nstrict = false\n# Untyped definitions and calls\ncheck_untyped_defs = true\n# Import discovery\nfollow_imports = \"normal\"\n# Miscellaneous\nwarn_unused_configs = true\n# Disallow generic without type arguments\ndisallow_any_generics = true\n# Configuring warnings\nwarn_redundant_casts = true\nwarn_unused_ignores = true\nwarn_no_return = true\n# Configuring error messages\nshow_column_numbers = true\n\n[[tool.mypy.overrides]]\nmodule = [\n    \"astor\",\n    \"cv2\",\n    \"scipy\",\n    \"xlsxwriter\"\n]\nignore_missing_imports = true\n"
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "r",
          "type": "tree",
          "content": null
        },
        {
          "name": "security",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 83.6455078125,
          "content": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\nimport ctypes\nimport errno\nimport fnmatch\nimport glob\nimport multiprocessing\nimport os\nimport platform\nimport re\nimport shlex\nimport shutil\nimport subprocess\nimport sys\nimport time\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom subprocess import CalledProcessError\n\nfrom setuptools import Command, Extension, setup\nfrom setuptools.command.develop import develop as DevelopCommandBase\nfrom setuptools.command.egg_info import egg_info\nfrom setuptools.command.install import install as InstallCommandBase\nfrom setuptools.command.install_lib import install_lib\nfrom setuptools.dist import Distribution\n\npython_version = platform.python_version()\nversion_detail = sys.version_info\nversion = str(version_detail[0]) + '.' + str(version_detail[1])\nenv_version = os.getenv(\"PY_VERSION\", None)\n\nif version_detail < (3, 8):\n    raise RuntimeError(\n        f\"Paddle only supports Python version >= 3.8 now,\"\n        f\"you are using Python {python_version}\"\n    )\nelif env_version is None:\n    print(f\"export PY_VERSION = { version }\")\n    os.environ[\"PY_VERSION\"] = python_version\n\nelif env_version != version:\n    raise ValueError(\n        f\"You have set the PY_VERSION environment variable to {env_version}, but \"\n        f\"your current Python version is {version}, \"\n        f\"Please keep them consistent.\"\n    )\n\n\n# check cmake\nCMAKE = shutil.which('cmake3') or shutil.which('cmake')\nassert (\n    CMAKE\n), 'The \"cmake\" executable is not found. Please check if Cmake is installed.'\n\n\nTOP_DIR = os.path.dirname(os.path.realpath(__file__))\n\nIS_WINDOWS = os.name == 'nt'\n\n\ndef filter_setup_args(input_args):\n    cmake_and_build = True\n    only_cmake = False\n    rerun_cmake = False\n    filter_args_list = []\n    for arg in input_args:\n        if arg == 'rerun-cmake':\n            rerun_cmake = True  # delete CMakeCache.txt and rerun cmake\n            continue\n        if arg == 'only-cmake':\n            only_cmake = True  # only cmake and do not make, leave a chance for users to adjust build options\n            continue\n        if arg in ['clean', 'egg_info', 'sdist']:\n            cmake_and_build = False\n        filter_args_list.append(arg)\n    return cmake_and_build, only_cmake, rerun_cmake, filter_args_list\n\n\ncmake_and_build, only_cmake, rerun_cmake, filter_args_list = filter_setup_args(\n    sys.argv\n)\n\n\ndef parse_input_command(input_parameters):\n    dist = Distribution()\n    # get script name :setup.py\n    sys.argv = input_parameters\n    dist.script_name = os.path.basename(sys.argv[0])\n    # get args of setup.py\n    dist.script_args = sys.argv[1:]\n    print(\n        \"Start executing python {} {}\".format(\n            dist.script_name, \"\".join(dist.script_args)\n        )\n    )\n    try:\n        dist.parse_command_line()\n    except:\n        print(\n            f\"An error occurred while parsing\"\n            f\"the parameters, {dist.script_args}\"\n        )\n        sys.exit(1)\n\n\nclass BinaryDistribution(Distribution):\n    def has_ext_modules(foo):\n        return True\n\n\nRC = 0\next_suffix = (\n    '.dll'\n    if os.name == 'nt'\n    else ('.dylib' if sys.platform == 'darwin' else '.so')\n)\n\n\ndef get_header_install_dir(header):\n    if 'pb.h' in header:\n        install_dir = re.sub(\n            env_dict.get(\"PADDLE_BINARY_DIR\") + '/', '', header\n        )\n    elif 'third_party' not in header:\n        # paddle headers\n        install_dir = re.sub(\n            env_dict.get(\"PADDLE_SOURCE_DIR\") + '/', '', header\n        )\n        if 'fluid/jit' in install_dir:\n            install_dir = re.sub('fluid/jit', 'jit', install_dir)\n    else:\n        # third_party\n        install_dir = re.sub(\n            env_dict.get(\"THIRD_PARTY_PATH\"), 'third_party', header\n        )\n        patterns = [\n            'install/mkldnn/include/',\n            'pybind/src/extern_pybind/include/',\n            'third_party/xpu/src/extern_xpu/xpu/include/',\n        ]\n        for pattern in patterns:\n            install_dir = re.sub(pattern, '', install_dir)\n    return install_dir\n\n\nclass InstallHeaders(Command):\n    \"\"\"Override how headers are copied.\"\"\"\n\n    description = 'install C/C++ header files'\n\n    user_options = [\n        ('install-dir=', 'd', 'directory to install header files to'),\n        ('force', 'f', 'force installation (overwrite existing files)'),\n    ]\n\n    boolean_options = ['force']\n\n    def initialize_options(self):\n        self.install_dir = None\n        self.force = 0\n        self.outfiles = []\n\n    def finalize_options(self):\n        self.set_undefined_options(\n            'install', ('install_headers', 'install_dir'), ('force', 'force')\n        )\n\n    def run(self):\n        hdrs = self.distribution.headers\n        if not hdrs:\n            return\n        self.mkpath(self.install_dir)\n        for header in hdrs:\n            install_dir = get_header_install_dir(header)\n            install_dir = os.path.join(\n                self.install_dir, os.path.dirname(install_dir)\n            )\n            if not os.path.exists(install_dir):\n                self.mkpath(install_dir)\n            (out, _) = self.copy_file(header, install_dir)\n            self.outfiles.append(out)\n            # (out, _) = self.mkdir_and_copy_file(header)\n            # self.outfiles.append(out)\n\n    def get_inputs(self):\n        return self.distribution.headers or []\n\n    def get_outputs(self):\n        return self.outfiles\n\n\nclass InstallCommand(InstallCommandBase):\n    def finalize_options(self):\n        ret = InstallCommandBase.finalize_options(self)\n        self.install_lib = self.install_platlib\n\n        self.install_headers = os.path.join(\n            self.install_platlib, 'paddle', 'include'\n        )\n        return ret\n\n\nclass DevelopCommand(DevelopCommandBase):\n    def run(self):\n        # copy proto and .so to python_source_dir\n        fluid_proto_binary_path = (\n            paddle_binary_dir + '/python/paddle/base/proto/'\n        )\n        fluid_proto_source_path = (\n            paddle_source_dir + '/python/paddle/base/proto/'\n        )\n        distributed_proto_binary_path = (\n            paddle_binary_dir + '/python/paddle/distributed/fleet/proto/'\n        )\n        distributed_proto_source_path = (\n            paddle_source_dir + '/python/paddle/distributed/fleet/proto/'\n        )\n        os.system(f\"rm -rf {fluid_proto_source_path}\")\n        shutil.copytree(fluid_proto_binary_path, fluid_proto_source_path)\n        os.system(f\"rm -rf {distributed_proto_source_path}\")\n        shutil.copytree(\n            distributed_proto_binary_path, distributed_proto_source_path\n        )\n        shutil.copy(\n            paddle_binary_dir + '/python/paddle/base/libpaddle.so',\n            paddle_source_dir + '/python/paddle/base/',\n        )\n        dynamic_library_binary_path = paddle_binary_dir + '/python/paddle/libs/'\n        dynamic_library_source_path = paddle_source_dir + '/python/paddle/libs/'\n        for lib_so in os.listdir(dynamic_library_binary_path):\n            shutil.copy(\n                dynamic_library_binary_path + lib_so,\n                dynamic_library_source_path,\n            )\n        # write version.py and cuda_env_config_py to python_source_dir\n        write_version_py(\n            filename=f'{paddle_source_dir}/python/paddle/version/__init__.py'\n        )\n        write_cuda_env_config_py(\n            filename=f'{paddle_source_dir}/python/paddle/cuda_env.py'\n        )\n        write_parameter_server_version_py(\n            filename=f'{paddle_source_dir}/python/paddle/incubate/distributed/fleet/parameter_server/version.py'\n        )\n        DevelopCommandBase.run(self)\n\n\nclass EggInfo(egg_info):\n    \"\"\"Copy license file into `.dist-info` folder.\"\"\"\n\n    def run(self):\n        # don't duplicate license into `.dist-info` when building a distribution\n        if not self.distribution.have_run.get('install', True):\n            self.mkpath(self.egg_info)\n            self.copy_file(\n                env_dict.get(\"PADDLE_SOURCE_DIR\") + \"/LICENSE\", self.egg_info\n            )\n\n        egg_info.run(self)\n\n\n# class Installlib is rewritten to add header files to .egg/paddle\nclass InstallLib(install_lib):\n    def run(self):\n        self.build()\n        outfiles = self.install()\n        hrds = self.distribution.headers\n        if not hrds:\n            return\n        for header in hrds:\n            install_dir = get_header_install_dir(header)\n            install_dir = os.path.join(\n                self.install_dir, 'paddle/include', os.path.dirname(install_dir)\n            )\n            if not os.path.exists(install_dir):\n                self.mkpath(install_dir)\n            self.copy_file(header, install_dir)\n        if outfiles is not None:\n            # always compile, in case we have any extension stubs to deal with\n            self.byte_compile(outfiles)\n\n\ndef git_commit() -> str:\n    try:\n        cmd = ['git', 'rev-parse', 'HEAD']\n        git_commit = (\n            subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                cwd=env_dict.get(\"PADDLE_SOURCE_DIR\"),\n            )\n            .communicate()[0]\n            .strip()\n        )\n    except:\n        git_commit = 'Unknown'\n    git_commit = git_commit.decode('utf-8')\n    return str(git_commit)\n\n\ndef _get_version_detail(idx):\n    assert (\n        idx < 3\n    ), \"version info consists of %(major)d.%(minor)d.%(patch)d, \\\n        so detail index must less than 3\"\n    tag_version_regex = env_dict.get(\"TAG_VERSION_REGEX\")\n    paddle_version = env_dict.get(\"PADDLE_VERSION\")\n    if re.match(tag_version_regex, paddle_version):\n        version_details = paddle_version.split('.')\n        if len(version_details) >= 3:\n            return version_details[idx]\n    return 0\n\n\ndef _mkdir_p(dir_str):\n    try:\n        os.makedirs(dir_str)\n    except OSError as e:\n        raise RuntimeError(\"Failed to create build folder\")\n\n\ndef get_major() -> int:\n    return int(_get_version_detail(0))\n\n\ndef get_minor() -> int:\n    return int(_get_version_detail(1))\n\n\ndef get_patch() -> int:\n    return str(_get_version_detail(2))\n\n\ndef get_nccl_version() -> int:\n    if env_dict.get(\"WITH_NCCL\") == 'ON':\n        return int(env_dict.get(\"NCCL_VERSION\"))\n    return 0\n\n\ndef get_cuda_version() -> str:\n    with_gpu = env_dict.get(\"WITH_GPU\")\n    if with_gpu == 'ON':\n        return env_dict.get(\"CUDA_VERSION\")\n    else:\n        return 'False'\n\n\ndef get_cudnn_version() -> str:\n    with_gpu = env_dict.get(\"WITH_GPU\")\n    if with_gpu == 'ON':\n        temp_cudnn_version = ''\n        cudnn_major_version = env_dict.get(\"CUDNN_MAJOR_VERSION\")\n        if cudnn_major_version:\n            temp_cudnn_version += cudnn_major_version\n            cudnn_minor_version = env_dict.get(\"CUDNN_MINOR_VERSION\")\n            if cudnn_minor_version:\n                temp_cudnn_version = (\n                    temp_cudnn_version + '.' + cudnn_minor_version\n                )\n                cudnn_patchlevel_version = env_dict.get(\n                    \"CUDNN_PATCHLEVEL_VERSION\"\n                )\n                if cudnn_patchlevel_version:\n                    temp_cudnn_version = (\n                        temp_cudnn_version + '.' + cudnn_patchlevel_version\n                    )\n        return temp_cudnn_version\n    else:\n        return 'False'\n\n\ndef get_xpu_xre_version() -> str:\n    with_xpu = env_dict.get(\"WITH_XPU\")\n    if with_xpu == 'ON':\n        return env_dict.get(\"XPU_XRE_BASE_VERSION\")\n    else:\n        return 'False'\n\n\ndef get_xpu_xccl_version() -> str:\n    with_xpu_xccl = env_dict.get(\"WITH_XPU_BKCL\")\n    if with_xpu_xccl == 'ON':\n        return env_dict.get(\"XPU_XCCL_BASE_VERSION\")\n    else:\n        return 'False'\n\n\ndef get_xpu_xhpc_version() -> str:\n    with_xpu_xhpc = env_dict.get(\"WITH_XPU\")\n    if with_xpu_xhpc == 'ON':\n        return env_dict.get(\"XPU_XHPC_BASE_DATE\")\n    else:\n        return 'False'\n\n\ndef is_tagged() -> bool:\n    try:\n        cmd = [\n            'git',\n            'describe',\n            '--exact-match',\n            '--tags',\n            'HEAD',\n            '2>/dev/null',\n        ]\n        git_tag = (\n            subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                cwd=env_dict.get(\"PADDLE_SOURCE_DIR\"),\n            )\n            .communicate()[0]\n            .strip()\n        )\n        git_tag = git_tag.decode()\n    except:\n        return False\n    if str(git_tag).replace('v', '') == env_dict.get(\"PADDLE_VERSION\"):\n        return True\n    else:\n        return False\n\n\ndef get_cinn_version() -> str:\n    if env_dict.get(\"WITH_CINN\") != 'ON':\n        return \"False\"\n    return \"0.3.0\"\n\n\ndef get_cuda_archs() -> list[int]:\n    compiled_cuda_archs = env_dict.get(\"COMPILED_CUDA_ARCHS\")\n    if isinstance(compiled_cuda_archs, str):\n        compiled_cuda_archs = re.findall(r'\\d+', compiled_cuda_archs)\n        return [int(arch) for arch in compiled_cuda_archs]\n    else:\n        return []\n\n\ndef get_tensorrt_version() -> str:\n\n    def find_libnvinfer():\n        \"\"\"Search for libnvinfer.so file in LD_LIBRARY_PATH.\"\"\"\n\n        trt_infer_rt_path = env_dict.get(\"TR_INFER_RT\")\n        tensorrt_library_path = env_dict.get(\"TENSORRT_LIBRARY_DIR\")\n\n        libnvinfer_file = os.path.join(tensorrt_library_path, trt_infer_rt_path)\n\n        if os.path.exists(libnvinfer_file):\n            return libnvinfer_file\n        else:\n            print(f\"{libnvinfer_file} not found.\")\n        return None\n\n    try:\n        libnvinfer_path = find_libnvinfer()\n        if not libnvinfer_path:\n            return None\n\n        trt = ctypes.CDLL(libnvinfer_path)\n        get_version = trt.getInferLibVersion\n        get_version.restype = ctypes.c_int\n        version = get_version()\n        version_str = str(version)\n        major = version_str[:1] if len(version_str) > 1 else version_str\n        minor = version_str[1:2] if len(version_str) > 3 else version_str[1:]\n        patch = version_str[3:] if len(version_str) > 3 else ''\n\n        minor = minor if minor else '0'\n        patch = patch if patch else '0'\n        version_str = f\"{major}.{minor}.{patch}\"\n\n        return version_str\n\n    except Exception as e:\n        print(f\"Error while getting TensorRT version: {e}\")\n        return None\n\n\ndef write_version_py(filename='paddle/version/__init__.py'):\n    cnt = '''# THIS FILE IS GENERATED FROM PADDLEPADDLE SETUP.PY\n#\nfull_version     = '%(major)d.%(minor)d.%(patch)s'\nmajor            = '%(major)d'\nminor            = '%(minor)d'\npatch            = '%(patch)s'\nnccl_version     = '%(nccl)d'\nrc               = '%(rc)d'\ncuda_version     = '%(cuda)s'\ncudnn_version    = '%(cudnn)s'\nxpu_xre_version  = '%(xpu_xre)s'\nxpu_xccl_version = '%(xpu_xccl)s'\nxpu_xhpc_version = '%(xpu_xhpc)s'\nis_tagged        = %(is_tagged)s\ncommit           = '%(commit)s'\nwith_mkl         = '%(with_mkl)s'\ncinn_version     = '%(cinn)s'\ntensorrt_version = '%(tensorrt)s'\nwith_pip_cuda_libraries = '%(with_pip_cuda_libraries)s'\nwith_pip_tensorrt       = '%(with_pip_tensorrt)s'\ncompiled_cuda_archs     = %(compiled_cuda_archs)s\n\n__all__ = ['cuda', 'cudnn', 'nccl', 'show', 'xpu', 'xpu_xre', 'xpu_xccl', 'xpu_xhpc', 'tensorrt', 'cuda_archs']\n\ndef show() -> None:\n    \"\"\"Get the version of paddle if `paddle` package if tagged. Otherwise, output the corresponding commit id.\n\n    Returns:\n        If paddle package is not tagged, the commit-id of paddle will be output.\n        Otherwise, the following information will be output.\n\n        full_version: version of paddle\n\n        major: the major version of paddle\n\n        minor: the minor version of paddle\n\n        patch: the patch level version of paddle\n\n        rc: whether it's rc version\n\n        cuda: the cuda version of package. It will return `False` if CPU version paddle package is installed\n\n        cudnn: the cudnn version of package. It will return `False` if CPU version paddle package is installed\n\n        xpu_xre: the xpu xre version of package. It will return `False` if non-XPU version paddle package is installed\n\n        xpu_xccl: the xpu xccl version of package. It will return `False` if non-XPU version paddle package is installed\n\n        xpu_xhpc: the xpu xhpc version of package. It will return `False` if non-XPU version paddle package is installed\n\n        cinn: the cinn version of package. It will return `False` if paddle package is not compiled with CINN\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> # Case 1: paddle is tagged with 2.2.0\n            >>> paddle.version.show()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            full_version: 2.2.0\n            major: 2\n            minor: 2\n            patch: 0\n            rc: 0\n            cuda: '10.2'\n            cudnn: '7.6.5'\n            xpu_xre: '4.32.0.1'\n            xpu_xccl: '1.0.7'\n            xpu_xhpc: '20231208'\n            cinn: False\n            >>> # doctest: -SKIP\n\n            >>> # Case 2: paddle is not tagged\n            >>> paddle.version.show()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            commit: cfa357e984bfd2ffa16820e354020529df434f7d\n            cuda: '10.2'\n            cudnn: '7.6.5'\n            xpu_xre: '4.32.0.1'\n            xpu_xccl: '1.0.7'\n            xpu_xhpc: '20231208'\n            cinn: False\n            >>> # doctest: -SKIP\n    \"\"\"\n    if is_tagged:\n        print('full_version:', full_version)\n        print('major:', major)\n        print('minor:', minor)\n        print('patch:', patch)\n        print('rc:', rc)\n    else:\n        print('commit:', commit)\n    print('cuda:', cuda_version)\n    print('cudnn:', cudnn_version)\n    print('nccl:', nccl_version)\n    print('xpu_xre:', xpu_xre_version)\n    print('xpu_xccl:', xpu_xccl_version)\n    print('xpu_xhpc:', xpu_xhpc_version)\n    print('cinn:', cinn_version)\n    print('tensorrt_version:', tensorrt_version)\n    print('cuda_archs:', compiled_cuda_archs)\n\ndef mkl() -> str:\n    return with_mkl\n\ndef nccl() -> str:\n    \"\"\"Get nccl version of paddle package.\n\n    Returns:\n        string: Return the version information of cuda nccl. If paddle package is CPU version, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.nccl()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            '2804'\n\n    \"\"\"\n    return nccl_version\n\ndef cuda() -> str:\n    \"\"\"Get cuda version of paddle package.\n\n    Returns:\n        string: Return the version information of cuda. If paddle package is CPU version, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.cuda()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            '10.2'\n\n    \"\"\"\n    return cuda_version\n\ndef cudnn() -> str:\n    \"\"\"Get cudnn version of paddle package.\n\n    Returns:\n        string: Return the version information of cudnn. If paddle package is CPU version, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.cudnn()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            '7.6.5'\n\n    \"\"\"\n    return cudnn_version\n\ndef xpu() -> str:\n    \"\"\"Get xpu version of paddle package. The API is deprecated now, please use xpu_xhpc() instead.\n\n    Returns:\n        string: Return the version information of xpu. If paddle package is non-XPU version, it will return False.\n    Examples:\n        .. code-block:: python\n            >>> import paddle\n            >>> paddle.version.xpu()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            '20230114'\n    \"\"\"\n    return xpu_xhpc_version\n\ndef xpu_xre() -> str:\n    \"\"\"Get xpu xre version of paddle package.\n\n    Returns:\n        string: Return the version information of xpu. If paddle package is non-XPU version, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.xpu_xre()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            '4.32.0.1'\n\n    \"\"\"\n    return xpu_xre_version\n\ndef xpu_xccl() -> str:\n    \"\"\"Get xpu xccl version of paddle package.\n\n    Returns:\n        string: Return the version information of xpu xccl. If paddle package is non-XPU version, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.xpu_xccl()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            '1.0.7'\n\n    \"\"\"\n    return xpu_xccl_version\n\ndef xpu_xhpc() -> str:\n    \"\"\"Get xpu xhpc version of paddle package.\n\n    Returns:\n        string: Return the version information of xpu xhpc. If paddle package is non-XPU version, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.xpu_xhpc()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            '20231208'\n\n    \"\"\"\n    return xpu_xhpc_version\n\ndef cinn() -> str:\n    \"\"\"Get CINN version of paddle package.\n\n    Returns:\n        string: Return the version information of CINN. If paddle package is not compiled with CINN, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.cinn()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            False\n\n    \"\"\"\n    return cinn_version\n\ndef tensorrt() -> str:\n    \"\"\"Get TensorRT version of paddle package.\n\n    Returns:\n        string: Return the version information of TensorRT. If paddle package is not compiled with TensorRT, it will return False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.tensorrt()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            False\n\n    \"\"\"\n    return tensorrt_version\n\ndef cuda_archs():\n    \"\"\"Get compiled cuda archs of paddle package.\n\n    Returns:\n        list[int]: Return the compiled cuda archs if with gpu. If paddle package is not compiled with gpu, it will return \"\".\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.version.cuda_archs()\n            >>> # doctest: +SKIP('Different environments yield different output.')\n            [86]\n\n    \"\"\"\n    return compiled_cuda_archs\n'''\n    commit = git_commit()\n\n    dirname = os.path.dirname(filename)\n\n    try:\n        os.makedirs(dirname)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n\n    with open(filename, 'w') as f:\n        f.write(\n            cnt\n            % {\n                'major': get_major(),\n                'minor': get_minor(),\n                'patch': get_patch(),\n                'nccl': get_nccl_version(),\n                'rc': RC,\n                'version': env_dict.get(\"PADDLE_VERSION\"),\n                'cuda': get_cuda_version(),\n                'cudnn': get_cudnn_version(),\n                'xpu_xre': get_xpu_xre_version(),\n                'xpu_xccl': get_xpu_xccl_version(),\n                'xpu_xhpc': get_xpu_xhpc_version(),\n                'commit': commit,\n                'is_tagged': is_tagged(),\n                'with_mkl': env_dict.get(\"WITH_MKL\"),\n                'cinn': get_cinn_version(),\n                'tensorrt': get_tensorrt_version(),\n                'with_pip_cuda_libraries': env_dict.get(\n                    \"WITH_PIP_CUDA_LIBRARIES\"\n                ),\n                'with_pip_tensorrt': env_dict.get(\"WITH_PIP_TENSORRT\"),\n                'compiled_cuda_archs': get_cuda_archs(),\n            }\n        )\n\n\ndef write_cuda_env_config_py(filename='paddle/cuda_env.py'):\n    cnt = \"\"\n    if env_dict.get(\"JIT_RELEASE_WHL\") == 'ON':\n        cnt = '''# THIS FILE IS GENERATED FROM PADDLEPADDLE SETUP.PY\n#\nimport os\nos.environ['CUDA_CACHE_MAXSIZE'] = '805306368'\n'''\n\n    with open(filename, 'w') as f:\n        f.write(cnt)\n\n\ndef write_parameter_server_version_py(\n    filename='paddle/incubate/distributed/fleet/parameter_server/version.py',\n):\n    cnt = '''\n\n# THIS FILE IS GENERATED FROM PADDLEPADDLE SETUP.PY\n\nfrom paddle.incubate.distributed.fleet.base import Mode\n\nBUILD_MODE=Mode.%(mode)s\n\ndef is_transpiler():\n    return Mode.TRANSPILER == BUILD_MODE\n\n'''\n\n    dirname = os.path.dirname(filename)\n\n    try:\n        os.makedirs(dirname)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n    with open(filename, 'w') as f:\n        f.write(\n            cnt\n            % {\n                'mode': (\n                    'PSLIB'\n                    if env_dict.get(\"WITH_PSLIB\") == 'ON'\n                    else 'TRANSPILER'\n                )\n            }\n        )\n\n\ndef find_files(pattern, root, recursive=False):\n    for dirpath, _, files in os.walk(root):\n        for filename in fnmatch.filter(files, pattern):\n            yield os.path.join(dirpath, filename)\n        if not recursive:\n            break\n\n\n@contextmanager\ndef cd(path):\n    if not os.path.isabs(path):\n        raise RuntimeError(f'Can only cd to absolute path, got: {path}')\n    orig_path = os.getcwd()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(orig_path)\n\n\ndef options_process(args, build_options):\n    for key, value in sorted(build_options.items()):\n        if value is not None:\n            args.append(f\"-D{key}={value}\")\n\n\ndef get_cmake_generator():\n    if os.getenv(\"GENERATOR\"):\n        cmake_generator = os.getenv(\"GENERATOR\")\n        if os.system('ninja --version') == 0:\n            print(\"Ninja has been installed,use ninja to compile Paddle now.\")\n        else:\n            print(\"Ninja has not been installed,install it now.\")\n            os.system('python -m pip install ninja')\n    else:\n        cmake_generator = \"Unix Makefiles\"\n    return cmake_generator\n\n\ndef cmake_run(build_path):\n    args = []\n    env_var = os.environ.copy()  # get env variables\n    paddle_build_options = {}\n    other_options = {}\n    other_options.update(\n        {\n            option: option\n            for option in (\n                \"PYTHON_LIBRARY\",\n                \"INFERENCE_DEMO_INSTALL_DIR\",\n                \"ON_INFER\",\n                \"PYTHON_EXECUTABLE\",\n                \"TENSORRT_ROOT\",\n                \"CUDA_ARCH_NAME\",\n                \"CUDA_ARCH_BIN\",\n                \"PYTHON_INCLUDE_DIR\",\n                \"PYTHON_LIBRARIES\",\n                \"PY_VERSION\",\n                \"CUB_PATH\",\n                \"NEW_RELEASE_PYPI\",\n                \"CUDNN_ROOT\",\n                \"THIRD_PARTY_PATH\",\n                \"NOAVX_CORE_FILE\",\n                \"LITE_GIT_TAG\",\n                \"CUDA_TOOLKIT_ROOT_DIR\",\n                \"NEW_RELEASE_JIT\",\n                \"XPU_SDK_ROOT\",\n                \"MSVC_STATIC_CRT\",\n                \"NEW_RELEASE_ALL\",\n                \"GENERATOR\",\n            )\n        }\n    )\n    # if environment variables which start with \"WITH_\" or \"CMAKE_\",put it into build_options\n    for option_key, option_value in env_var.items():\n        if option_key.startswith((\"CMAKE_\", \"WITH_\")):\n            paddle_build_options[option_key] = option_value\n        if option_key in other_options:\n            if (\n                option_key == 'PYTHON_EXECUTABLE'\n                or option_key == 'PYTHON_LIBRARY'\n                or option_key == 'PYTHON_LIBRARIES'\n            ):\n                key = option_key + \":FILEPATH\"\n            elif option_key == 'PYTHON_INCLUDE_DIR':\n                key = option_key + ':PATH'\n            elif option_key == 'GENERATOR':\n                key = 'CMAKE_' + option_key\n            else:\n                key = other_options[option_key]\n            if key not in paddle_build_options:\n                paddle_build_options[key] = option_value\n\n    options_process(args, paddle_build_options)\n    with cd(build_path):\n        cmake_args = []\n        cmake_args.append(CMAKE)\n        cmake_args += args\n        cmake_args.append('-DWITH_SETUP_INSTALL=ON')\n        cmake_args.append(TOP_DIR)\n        subprocess.check_call(cmake_args)\n\n\ndef build_run(args, build_path, environ_var):\n    with cd(build_path):\n        build_args = []\n        build_args.append(CMAKE)\n        build_args += args\n        try:\n            subprocess.check_call(build_args, cwd=build_path, env=environ_var)\n        except (CalledProcessError, KeyboardInterrupt) as e:\n            sys.exit(1)\n\n\ndef run_cmake_build(build_path):\n    build_type = (\n        os.getenv(\"CMAKE_BUILD_TYPE\")\n        if os.getenv(\"CMAKE_BUILD_TYPE\") is not None\n        else \"release\"\n    )\n    build_args = [\"--build\", \".\", \"--target\", \"install\", \"--config\", build_type]\n    max_jobs = os.getenv(\"MAX_JOBS\")\n    if max_jobs is not None:\n        max_jobs = max_jobs or str(multiprocessing.cpu_count())\n\n        build_args += [\"--\"]\n        if IS_WINDOWS:\n            build_args += [f\"/p:CL_MPCount={max_jobs}\"]\n        else:\n            build_args += [\"-j\", max_jobs]\n    else:\n        build_args += [\"-j\", str(multiprocessing.cpu_count())]\n    environ_var = os.environ.copy()\n    build_run(build_args, build_path, environ_var)\n\n\ndef build_steps():\n    print('------- Building start ------')\n    build_dir = os.getenv(\"BUILD_DIR\")\n    if build_dir is not None:\n        build_dir = TOP_DIR + '/' + build_dir\n    else:\n        build_dir = TOP_DIR + '/build'\n    if not os.path.exists(build_dir):\n        _mkdir_p(build_dir)\n    build_path = build_dir\n    print(\"build_dir:\", build_dir)\n    # run cmake to generate native build files\n    cmake_cache_file_path = os.path.join(build_path, \"CMakeCache.txt\")\n    # if rerun_cmake is True,remove CMakeCache.txt and rerun cmake\n    if os.path.isfile(cmake_cache_file_path) and rerun_cmake is True:\n        os.remove(cmake_cache_file_path)\n\n    CMAKE_GENERATOR = get_cmake_generator()\n    bool_ninja = CMAKE_GENERATOR == \"Ninja\"\n    build_ninja_file_path = os.path.join(build_path, \"build.ninja\")\n    if os.path.exists(cmake_cache_file_path) and not (\n        bool_ninja and not os.path.exists(build_ninja_file_path)\n    ):\n        print(\"Do not need rerun cmake, everything is ready, run build now\")\n    else:\n        cmake_run(build_path)\n    # make\n    if only_cmake:\n        print(\n            \"You have finished running cmake, the program exited,run 'cmake build' to adjust build options and 'python setup.py install to build'\"\n        )\n        sys.exit()\n    run_cmake_build(build_path)\n\n\ndef get_setup_requires():\n    with open(\n        env_dict.get(\"PADDLE_SOURCE_DIR\") + '/python/requirements.txt'\n    ) as f:\n        setup_requires = (\n            f.read().splitlines()\n        )  # Specify the dependencies to install\n    if sys.version_info >= (3, 8):\n        setup_requires_tmp = []\n        for setup_requires_i in setup_requires:\n            if (\n                '<\"3.6\"' in setup_requires_i\n                or '<=\"3.6\"' in setup_requires_i\n                or '<\"3.5\"' in setup_requires_i\n                or '<=\"3.5\"' in setup_requires_i\n                or '<\"3.7\"' in setup_requires_i\n                or '<=\"3.7\"' in setup_requires_i\n                or '<\"3.8\"' in setup_requires_i\n                or setup_requires_i.strip().endswith('[build]')\n            ):\n                continue\n            setup_requires_tmp += [setup_requires_i]\n        setup_requires = setup_requires_tmp\n\n        return setup_requires\n    else:\n        raise RuntimeError(\n            \"please check your python version,Paddle only support Python version>=3.8 now\"\n        )\n\n\ndef get_paddle_extra_install_requirements():\n    paddle_cuda_requires = []\n    paddle_tensorrt_requires = []\n    # (Note risemeup1): Paddle will install the pypi cuda package provided by Nvidia, which includes the cuda runtime, cudnn, and cublas, thereby making the operation of 'pip install paddle' no longer dependent on the installation of cuda and cudnn.\n    if env_dict.get(\"WITH_PIP_CUDA_LIBRARIES\") == \"ON\":\n        if platform.system() == 'Linux':\n            PADDLE_CUDA_INSTALL_REQUIREMENTS = {\n                \"V11\": (\n                    \"nvidia-cuda-runtime-cu11==11.8.89; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cuda-cupti-cu11==11.8.87; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cudnn-cu11==8.9.6.50; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cublas-cu11==11.11.3.6; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cufft-cu11==10.9.0.58; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-curand-cu11==10.3.0.86; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cusolver-cu11==11.4.1.48; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cusparse-cu11==11.7.5.86; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-nccl-cu11==2.19.3; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-nvtx-cu11==11.8.86; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cuda-nvrtc-cu11==11.8.89; platform_system == 'Linux' and platform_machine == 'x86_64'\"\n                ),\n                \"V12\": (\n                    \"nvidia-cuda-runtime-cu12==12.3.101; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cuda-cupti-cu12==12.3.101; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cudnn-cu12==9.1.1.17; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cublas-cu12==12.3.4.1; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cufft-cu12==11.2.1.3; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-curand-cu12==10.3.5.147; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cusolver-cu12==11.6.1.9; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cusparse-cu12==12.3.1.170; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-nccl-cu12==2.19.3; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-nvtx-cu12==12.4.127; platform_system == 'Linux' and platform_machine == 'x86_64' | \"\n                    \"nvidia-cuda-nvrtc-cu12==12.3.107; platform_system == 'Linux' and platform_machine == 'x86_64'\"\n                ),\n            }\n        elif platform.system() == 'Windows':\n            PADDLE_CUDA_INSTALL_REQUIREMENTS = {\n                \"V11\": (\n                    \"nvidia-cuda-runtime-cu11==11.8.89 | \"\n                    \"nvidia-cudnn-cu11==8.9.4.19 | \"\n                    \"nvidia-cublas-cu11==11.11.3.6 | \"\n                    \"nvidia-cufft-cu11==10.9.0.58 | \"\n                    \"nvidia-curand-cu11==10.3.0.86 | \"\n                    \"nvidia-cusolver-cu11==11.4.1.48 | \"\n                    \"nvidia-cusparse-cu11==11.7.5.86 \"\n                ),\n                \"V12\": (\n                    \"nvidia-cuda-runtime-cu12==12.3.101 | \"\n                    \"nvidia-cudnn-cu12==9.1.1.17 | \"\n                    \"nvidia-cublas-cu12==12.3.4.1 | \"\n                    \"nvidia-cufft-cu12==11.2.1.3 | \"\n                    \"nvidia-curand-cu12==10.3.5.147 | \"\n                    \"nvidia-cusolver-cu12==11.6.1.9 | \"\n                    \"nvidia-cusparse-cu12==12.3.1.170 \"\n                ),\n            }\n        try:\n            output = subprocess.check_output(['nvcc', '--version']).decode(\n                'utf-8'\n            )\n            version_line = next(\n                line for line in output.split('\\n') if 'release' in line\n            )\n            version = version_line.split(' ')[-1].split(',')[0]\n            cuda_major_version = version.split('.')[0]\n        except Exception as e:\n            raise ValueError(\"CUDA not found\")\n\n        paddle_cuda_requires = PADDLE_CUDA_INSTALL_REQUIREMENTS[\n            cuda_major_version\n        ].split(\"|\")\n\n    if env_dict.get(\"WITH_PIP_TENSORRT\") == \"ON\":\n        version_str = get_tensorrt_version()\n        version_default = int(version_str.split(\".\")[0])\n        if platform.system() == 'Linux' or (\n            platform.system() == 'Windows' and version_default >= 10\n        ):\n\n            PADDLE_TENSORRT_INSTALL_REQUIREMENTS = [\n                \"tensorrt==8.5.3.1\",\n                \"tensorrt==8.6.0\",\n                \"tensorrt==8.6.1.post1\",\n            ]\n\n            if not version_str:\n                return paddle_cuda_requires, []\n\n            version_main = \".\".join(version_str.split(\".\")[:3])\n\n            matched_package = None\n            for (\n                paddle_tensorrt_requires\n            ) in PADDLE_TENSORRT_INSTALL_REQUIREMENTS:\n                paddle_tensorrt_version = paddle_tensorrt_requires.split(\"==\")[\n                    1\n                ]\n                paddle_tensorrt_main = \".\".join(\n                    paddle_tensorrt_version.split(\".\")[:3]\n                )\n\n                if version_main == paddle_tensorrt_main:\n                    matched_package = paddle_tensorrt_requires\n                    break\n\n            if matched_package:\n                paddle_tensorrt_requires = [matched_package]\n            else:\n                print(\n                    f\"No exact match found for TensorRT Version: {version_str}. We currently support TensorRT versions 8.5.3.1, 8.6.0, and 8.6.1.\"\n                )\n                return paddle_cuda_requires, []\n\n    return paddle_cuda_requires, paddle_tensorrt_requires\n\n\ndef get_cinn_config_jsons():\n    from pathlib import Path\n\n    src_cinn_config_path = (\n        env_dict.get(\"PADDLE_SOURCE_DIR\") + '/python/paddle/cinn_config'\n    )\n    prefix_len = len(src_cinn_config_path) + 1\n    p = Path(src_cinn_config_path)\n    json_list = list(p.glob('**/*.json'))\n    json_path_list = []\n    for json in json_list:\n        json = str(json)\n        json = json[prefix_len:]\n        json_path_list += [json]\n    return json_path_list\n\n\ndef get_typing_libs_packages(paddle_binary_dir):\n    \"\"\"get all libpaddle sub modules from 'python/paddle/_typing/libs/libpaddle'\n    e.g.\n        'paddle._typing.libs.libpaddle.cinn'\n        'paddle._typing.libs.libpaddle.pir'\n        'paddle._typing.libs.libpaddle.eager'\n        'paddle._typing.libs.libpaddle.eager.ops'\n    \"\"\"\n    base_dir = Path(paddle_binary_dir) / 'python'\n    libs_dir = base_dir / 'paddle' / '_typing' / 'libs' / 'libpaddle'\n    return [\n        '.'.join(str(Path(root).relative_to(base_dir)).split(os.sep))\n        for root, _, _ in os.walk(libs_dir)\n    ]\n\n\ndef extend_type_hints_package_data(packages, package_data, paddle_binary_dir):\n    typing_libs_packages = get_typing_libs_packages(paddle_binary_dir)\n\n    # update packages\n    packages += typing_libs_packages\n\n    # update package_data\n    type_hints_files = {\n        'paddle': ['py.typed', '*.pyi'],\n        'paddle.framework': ['*.pyi'],\n        'paddle.base': ['*.pyi'],\n        'paddle.tensor': ['tensor.pyi'],\n        'paddle._typing': ['*.pyi'],\n        'paddle._typing.libs': ['*.pyi', '*.md'],\n    }\n\n    for libpaddle_module in typing_libs_packages:\n        type_hints_files[libpaddle_module] = ['*.pyi']\n\n    for pkg, files in type_hints_files.items():\n        if pkg not in package_data:\n            package_data[pkg] = []\n        package_data[pkg] += files\n\n    return packages, package_data\n\n\ndef get_package_data_and_package_dir():\n    if os.name != 'nt':\n        package_data = {\n            'paddle.base': [env_dict.get(\"FLUID_CORE_NAME\") + '.so']\n        }\n    else:\n        package_data = {\n            'paddle.base': [\n                env_dict.get(\"FLUID_CORE_NAME\") + '.pyd',\n                env_dict.get(\"FLUID_CORE_NAME\") + '.lib',\n            ]\n        }\n    package_data['paddle.base'] += [\n        paddle_binary_dir + '/python/paddle/cost_model/static_op_benchmark.json'\n    ]\n\n    whl_cinn_config_path = paddle_binary_dir + '/python/paddle/cinn_config'\n    src_cinn_config_path = (\n        env_dict.get(\"PADDLE_SOURCE_DIR\") + '/python/paddle/cinn_config'\n    )\n    package_data['paddle.cinn_config'] = []\n    if os.path.exists(whl_cinn_config_path):\n        shutil.rmtree(whl_cinn_config_path)\n    shutil.copytree(src_cinn_config_path, whl_cinn_config_path)\n    json_path_list = get_cinn_config_jsons()\n    for json in json_path_list:\n        package_data['paddle.cinn_config'] += [json]\n\n    if 'develop' in sys.argv:\n        package_dir = {'': 'python'}\n    else:\n        package_dir = {\n            '': env_dict.get(\"PADDLE_BINARY_DIR\") + '/python',\n            'paddle.base.proto.profiler': env_dict.get(\"PADDLE_BINARY_DIR\")\n            + '/paddle/fluid/platform',\n            'paddle.base.proto': env_dict.get(\"PADDLE_BINARY_DIR\")\n            + '/paddle/fluid/framework',\n            'paddle.base': env_dict.get(\"PADDLE_BINARY_DIR\")\n            + '/python/paddle/base',\n        }\n    # put all thirdparty libraries in paddle.libs\n    libs_path = paddle_binary_dir + '/python/paddle/libs'\n    package_data['paddle.libs'] = []\n    if env_dict.get(\"WITH_SHARED_PHI\") == \"ON\":\n        package_data['paddle.libs'] += [\n            ('libphi' if os.name != 'nt' else 'phi') + ext_suffix\n        ]\n        shutil.copy(env_dict.get(\"PHI_LIB\"), libs_path)\n        package_data['paddle.libs'] += [\n            ('libphi_core' if os.name != 'nt' else 'phi_core') + ext_suffix\n        ]\n        shutil.copy(env_dict.get(\"PHI_CORE_LIB\"), libs_path)\n        if (\n            env_dict.get(\"WITH_GPU\") == \"ON\"\n            or env_dict.get(\"WITH_ROCM\") == \"ON\"\n        ):\n            package_data['paddle.libs'] += [\n                ('libphi_gpu' if os.name != 'nt' else 'phi_gpu') + ext_suffix\n            ]\n            shutil.copy(env_dict.get(\"PHI_GPU_LIB\"), libs_path)\n\n    if env_dict.get(\"WITH_SHARED_IR\") == \"ON\":\n        package_data['paddle.libs'] += [\n            ('libpir' if os.name != 'nt' else 'pir') + ext_suffix\n        ]\n        shutil.copy(env_dict.get(\"IR_LIB\"), libs_path)\n\n    package_data['paddle.libs'] += [\n        ('libwarpctc' if os.name != 'nt' else 'warpctc') + ext_suffix,\n        ('libwarprnnt' if os.name != 'nt' else 'warprnnt') + ext_suffix,\n    ]\n    package_data['paddle.libs'] += [\n        ('libcommon' if os.name != 'nt' else 'common') + ext_suffix,\n    ]\n    shutil.copy(env_dict.get(\"COMMON_LIB\"), libs_path)\n    shutil.copy(env_dict.get(\"WARPCTC_LIBRARIES\"), libs_path)\n    shutil.copy(env_dict.get(\"WARPRNNT_LIBRARIES\"), libs_path)\n    package_data['paddle.libs'] += [\n        os.path.basename(env_dict.get(\"LAPACK_LIB\")),\n        os.path.basename(env_dict.get(\"BLAS_LIB\")),\n        os.path.basename(env_dict.get(\"GFORTRAN_LIB\")),\n        os.path.basename(env_dict.get(\"GNU_RT_LIB_1\")),\n    ]\n    shutil.copy(env_dict.get(\"BLAS_LIB\"), libs_path)\n    shutil.copy(env_dict.get(\"LAPACK_LIB\"), libs_path)\n    shutil.copy(env_dict.get(\"GFORTRAN_LIB\"), libs_path)\n    shutil.copy(env_dict.get(\"GNU_RT_LIB_1\"), libs_path)\n\n    if not sys.platform.startswith(\"linux\"):\n        package_data['paddle.libs'] += [\n            os.path.basename(env_dict.get(\"GNU_RT_LIB_2\"))\n        ]\n        shutil.copy(env_dict.get(\"GNU_RT_LIB_2\"), libs_path)\n    if env_dict.get(\"WITH_MKL\") == 'ON':\n        shutil.copy(env_dict.get(\"MKLML_SHARED_LIB\"), libs_path)\n        shutil.copy(env_dict.get(\"MKLML_SHARED_IOMP_LIB\"), libs_path)\n        package_data['paddle.libs'] += [\n            ('libmklml_intel' if os.name != 'nt' else 'mklml') + ext_suffix,\n            ('libiomp5' if os.name != 'nt' else 'libiomp5md') + ext_suffix,\n        ]\n    else:\n        if os.name == 'nt':\n            # copy the openblas.dll\n            shutil.copy(env_dict.get(\"OPENBLAS_SHARED_LIB\"), libs_path)\n            package_data['paddle.libs'] += ['openblas' + ext_suffix]\n        elif (\n            os.name == 'posix'\n            and platform.machine() == 'aarch64'\n            and env_dict.get(\"OPENBLAS_LIB\").endswith('so')\n        ):\n            # copy the libopenblas.so on linux+aarch64\n            # special: libpaddle.so without avx depends on 'libopenblas.so.0', not 'libopenblas.so'\n            if os.path.exists(env_dict.get(\"OPENBLAS_LIB\") + '.0'):\n                shutil.copy(env_dict.get(\"OPENBLAS_LIB\") + '.0', libs_path)\n                package_data['paddle.libs'] += ['libopenblas.so.0']\n\n    if env_dict.get(\"WITH_GPU\") == 'ON' or env_dict.get(\"WITH_ROCM\") == 'ON':\n        if len(env_dict.get(\"FLASHATTN_LIBRARIES\", \"\")) > 1:\n            package_data['paddle.libs'] += [\n                os.path.basename(env_dict.get(\"FLASHATTN_LIBRARIES\"))\n            ]\n            shutil.copy(env_dict.get(\"FLASHATTN_LIBRARIES\"), libs_path)\n        if len(env_dict.get(\"FLASHATTN_V3_LIBRARIES\", \"\")) > 1:\n            package_data['paddle.libs'] += [\n                os.path.basename(env_dict.get(\"FLASHATTN_V3_LIBRARIES\"))\n            ]\n            shutil.copy(env_dict.get(\"FLASHATTN_V3_LIBRARIES\"), libs_path)\n    if env_dict.get(\"WITH_CINN\") == 'ON':\n        shutil.copy(\n            env_dict.get(\"CINN_LIB_LOCATION\")\n            + '/'\n            + env_dict.get(\"CINN_LIB_NAME\"),\n            libs_path,\n        )\n        shutil.copy(\n            env_dict.get(\"CINN_INCLUDE_DIR\")\n            + '/paddle/cinn/runtime/cuda/cinn_cuda_runtime_source.cuh',\n            libs_path,\n        )\n        shutil.copy(\n            env_dict.get(\"CINN_INCLUDE_DIR\")\n            + '/paddle/cinn/runtime/hip/cinn_hip_runtime_source.h',\n            libs_path,\n        )\n        package_data['paddle.libs'] += ['libcinnapi.so']\n        package_data['paddle.libs'] += ['cinn_cuda_runtime_source.cuh']\n        package_data['paddle.libs'] += ['cinn_hip_runtime_source.h']\n\n        cinn_fp16_file = (\n            env_dict.get(\"CINN_INCLUDE_DIR\")\n            + '/paddle/cinn/runtime/cuda/float16.h'\n        )\n        if os.path.exists(cinn_fp16_file):\n            shutil.copy(cinn_fp16_file, libs_path)\n            package_data['paddle.libs'] += ['float16.h']\n        cinn_bf16_file = (\n            env_dict.get(\"CINN_INCLUDE_DIR\")\n            + '/paddle/cinn/runtime/cuda/bfloat16.h'\n        )\n        if os.path.exists(cinn_bf16_file):\n            shutil.copy(cinn_bf16_file, libs_path)\n            package_data['paddle.libs'] += ['bfloat16.h']\n\n        if env_dict.get(\"CMAKE_BUILD_TYPE\") == 'Release' and os.name != 'nt':\n            command = (\n                f\"patchelf --set-rpath '$ORIGIN/../../nvidia/cuda_nvrtc/lib/:$ORIGIN/../../nvidia/cuda_runtime/lib/:$ORIGIN/../../nvidia/cublas/lib/:$ORIGIN/../../nvidia/cudnn/lib/:$ORIGIN/../../nvidia/curand/lib/:$ORIGIN/../../nvidia/cusolver/lib/:$ORIGIN/../../nvidia/nvtx/lib/:$ORIGIN/' {libs_path}/\"\n                + env_dict.get(\"CINN_LIB_NAME\")\n            )\n            if os.system(command) != 0:\n                raise Exception(\n                    'patch '\n                    + libs_path\n                    + '/'\n                    + env_dict.get(\"CINN_LIB_NAME\")\n                    + ' failed',\n                    f'command: {command}',\n                )\n    if env_dict.get(\"WITH_PSLIB\") == 'ON':\n        shutil.copy(env_dict.get(\"PSLIB_LIB\"), libs_path)\n        shutil.copy(env_dict.get(\"JVM_LIB\"), libs_path)\n        if os.path.exists(env_dict.get(\"PSLIB_VERSION_PY\")):\n            shutil.copy(\n                env_dict.get(\"PSLIB_VERSION_PY\"),\n                paddle_binary_dir\n                + '/python/paddle/incubate/distributed/fleet/parameter_server/pslib/',\n            )\n        package_data['paddle.libs'] += ['libps' + ext_suffix]\n        package_data['paddle.libs'] += ['libjvm' + ext_suffix]\n    if env_dict.get(\"WITH_ONEDNN\") == 'ON':\n        if env_dict.get(\"CMAKE_BUILD_TYPE\") == 'Release' and os.name != 'nt':\n            # only change rpath in Release mode.\n            # TODO(typhoonzero): use install_name_tool to patch mkl libs once\n            # we can support mkl on mac.\n            #\n            # change rpath of libdnnl.so.1, add $ORIGIN/ to it.\n            # The reason is that all thirdparty libraries in the same directory,\n            # thus, libdnnl.so.1 will find libmklml_intel.so and libiomp5.so.\n            command = \"patchelf --set-rpath '$ORIGIN/' \" + env_dict.get(\n                \"ONEDNN_SHARED_LIB\"\n            )\n            if os.system(command) != 0:\n                raise Exception(f\"patch libdnnl.so failed, command: {command}\")\n        shutil.copy(env_dict.get(\"ONEDNN_SHARED_LIB\"), libs_path)\n        if os.name != 'nt':\n            package_data['paddle.libs'] += ['libdnnl.so.3']\n        else:\n            package_data['paddle.libs'] += ['mkldnn.dll']\n\n    if env_dict.get(\"WITH_ONNXRUNTIME\") == 'ON':\n        shutil.copy(env_dict.get(\"ONNXRUNTIME_SHARED_LIB\"), libs_path)\n        shutil.copy(env_dict.get(\"PADDLE2ONNX_LIB\"), libs_path)\n        if os.name == 'nt':\n            package_data['paddle.libs'] += [\n                'paddle2onnx.dll',\n                'onnxruntime.dll',\n            ]\n        else:\n            package_data['paddle.libs'] += [\n                env_dict.get(\"PADDLE2ONNX_LIB_NAME\"),\n                env_dict.get(\"ONNXRUNTIME_LIB_NAME\"),\n            ]\n\n    if env_dict.get(\"WITH_OPENVINO\") == 'ON':\n        shutil.copy(env_dict.get(\"OPENVINO_LIB\"), libs_path)\n        shutil.copy(env_dict.get(\"TBB_LIB\"), libs_path)\n        shutil.copy(env_dict.get(\"OPENVINO_PADDLE_LIB\"), libs_path)\n        shutil.copy(env_dict.get(\"OPENVINO_CPU_PLUGIN_LIB\"), libs_path)\n        if os.name != 'nt':\n            package_data['paddle.libs'] += [\n                'libopenvino.so.2500',\n                'libtbb.so.12',\n                'libopenvino_paddle_frontend.so.2500',\n                'libopenvino_intel_cpu_plugin.so',\n            ]\n        else:\n            package_data['paddle.libs'] += [\n                'openvino.dll',\n                'tbb.dll',\n                'openvino_paddle_frontend.dll',\n                'openvino_intel_cpu_plugin.dll',\n            ]\n\n    if env_dict.get(\"WITH_XPU\") == 'ON':\n        shutil.copy(env_dict.get(\"XPU_API_LIB\"), libs_path)\n        package_data['paddle.libs'] += [env_dict.get(\"XPU_API_LIB_NAME\")]\n        xpu_rt_lib_list = glob.glob(env_dict.get(\"XPU_RT_LIB\") + '*')\n        for xpu_rt_lib_file in xpu_rt_lib_list:\n            shutil.copy(xpu_rt_lib_file, libs_path)\n            package_data['paddle.libs'] += [os.path.basename(xpu_rt_lib_file)]\n        xpu_cuda_lib_list = glob.glob(env_dict.get(\"XPU_CUDA_LIB\") + '*')\n        for xpu_cuda_lib_file in xpu_cuda_lib_list:\n            shutil.copy(xpu_cuda_lib_file, libs_path)\n            package_data['paddle.libs'] += [os.path.basename(xpu_cuda_lib_file)]\n        if env_dict.get(\"WITH_XPU_XRE5\") == 'ON':\n            xpu_cuda_rt_lib_list = glob.glob(\n                env_dict.get(\"XPU_CUDA_RT_LIB\") + '*'\n            )\n            for xpu_cuda_rt_lib_file in xpu_cuda_rt_lib_list:\n                shutil.copy(xpu_cuda_rt_lib_file, libs_path)\n                package_data['paddle.libs'] += [\n                    os.path.basename(xpu_cuda_rt_lib_file)\n                ]\n            xpu_ml_lib_list = glob.glob(env_dict.get(\"XPU_ML_LIB\") + '*')\n            for xpu_ml_lib_file in xpu_ml_lib_list:\n                shutil.copy(xpu_ml_lib_file, libs_path)\n                package_data['paddle.libs'] += [\n                    os.path.basename(xpu_ml_lib_file)\n                ]\n            shutil.copy(env_dict.get(\"XPU_XBLAS_LIB\"), libs_path)\n            package_data['paddle.libs'] += [env_dict.get(\"XPU_XBLAS_LIB_NAME\")]\n            shutil.copy(env_dict.get(\"XPU_XFA_LIB\"), libs_path)\n            package_data['paddle.libs'] += [env_dict.get(\"XPU_XFA_LIB_NAME\")]\n            shutil.copy(env_dict.get(\"XPU_XPUDNN_LIB\"), libs_path)\n            package_data['paddle.libs'] += [env_dict.get(\"XPU_XPUDNN_LIB_NAME\")]\n\n    if env_dict.get(\"WITH_XPU_BKCL\") == 'ON':\n        shutil.copy(env_dict.get(\"XPU_BKCL_LIB\"), libs_path)\n        package_data['paddle.libs'] += [env_dict.get(\"XPU_BKCL_LIB_NAME\")]\n\n    if env_dict.get(\"WITH_XPU_XFT\") == 'ON':\n        shutil.copy(env_dict.get(\"XPU_XFT_LIB\"), libs_path)\n        package_data['paddle.libs'] += [env_dict.get(\"XPU_XFT_LIB_NAME\")]\n\n    if env_dict.get(\"WITH_XPTI\") == 'ON':\n        shutil.copy(env_dict.get(\"XPU_XPTI_LIB\"), libs_path)\n        package_data['paddle.libs'] += [env_dict.get(\"XPU_XPTI_LIB_NAME\")]\n\n    # remove unused paddle/libs/__init__.py\n    if os.path.isfile(libs_path + '/__init__.py'):\n        os.remove(libs_path + '/__init__.py')\n    package_dir['paddle.libs'] = libs_path\n\n    # change rpath of ${FLUID_CORE_NAME}.ext, add $ORIGIN/../libs/ to it.\n    # The reason is that libwarpctc.ext, libwarprnnt.ext, libiomp5.ext etc are in paddle.libs, and\n    # ${FLUID_CORE_NAME}.ext is in paddle.base, thus paddle/fluid/../libs will pointer to above libraries.\n    # This operation will fix https://github.com/PaddlePaddle/Paddle/issues/3213\n    if env_dict.get(\"CMAKE_BUILD_TYPE\") == 'Release':\n        if os.name != 'nt':\n            # only change rpath in Release mode, since in Debug mode, ${FLUID_CORE_NAME}.xx is too large to be changed.\n            if env_dict.get(\"APPLE\") == \"1\":\n                commands = [\n                    \"install_name_tool -id '@loader_path/../libs/' \"\n                    + env_dict.get(\"PADDLE_BINARY_DIR\")\n                    + '/python/paddle/base/'\n                    + env_dict.get(\"FLUID_CORE_NAME\")\n                    + '.so'\n                ]\n                commands.append(\n                    \"install_name_tool -add_rpath '@loader_path/../libs/' \"\n                    + env_dict.get(\"PADDLE_BINARY_DIR\")\n                    + '/python/paddle/base/'\n                    + env_dict.get(\"FLUID_CORE_NAME\")\n                    + '.so'\n                )\n                commands.append(\n                    \"install_name_tool -add_rpath '@loader_path/../libs/' \"\n                    + env_dict.get(\"PADDLE_BINARY_DIR\")\n                    + '/python/paddle/libs/'\n                    + env_dict.get(\"COMMON_NAME\")\n                )\n                if env_dict.get(\"WITH_SHARED_PHI\") == \"ON\":\n                    commands.append(\n                        \"install_name_tool -add_rpath '@loader_path' \"\n                        + env_dict.get(\"PADDLE_BINARY_DIR\")\n                        + '/python/paddle/libs/'\n                        + env_dict.get(\"PHI_NAME\")\n                    )\n                    commands.append(\n                        \"install_name_tool -add_rpath '@loader_path' \"\n                        + env_dict.get(\"PADDLE_BINARY_DIR\")\n                        + '/python/paddle/libs/'\n                        + env_dict.get(\"PHI_CORE_NAME\")\n                    )\n                    if (\n                        env_dict.get(\"WITH_GPU\") == \"ON\"\n                        or env_dict.get(\"WITH_ROCM\") == \"ON\"\n                    ):\n                        commands.append(\n                            \"install_name_tool -add_rpath '@loader_path' \"\n                            + env_dict.get(\"PADDLE_BINARY_DIR\")\n                            + '/python/paddle/libs/'\n                            + env_dict.get(\"PHI_GPU_NAME\")\n                        )\n                if env_dict.get(\"WITH_SHARED_IR\") == \"ON\":\n                    commands.append(\n                        \"install_name_tool -add_rpath '@loader_path' \"\n                        + env_dict.get(\"PADDLE_BINARY_DIR\")\n                        + '/python/paddle/libs/'\n                        + env_dict.get(\"IR_NAME\")\n                    )\n            else:\n                commands = [\n                    \"patchelf --set-rpath '$ORIGIN/../../nvidia/cuda_runtime/lib:$ORIGIN/../../nvidia/cuda_nvrtc/lib:$ORIGIN/../../nvidia/cublas/lib:$ORIGIN/../../nvidia/cudnn/lib:$ORIGIN/../../nvidia/curand/lib:$ORIGIN/../../nvidia/cusparse/lib:$ORIGIN/../../nvidia/nvjitlink/lib:$ORIGIN/../../nvidia/cuda_cupti/lib:$ORIGIN/../../nvidia/cuda_runtime/lib:$ORIGIN/../../nvidia/cufft/lib:$ORIGIN/../../nvidia/cufft/lib:$ORIGIN/../../nvidia/cusolver/lib:$ORIGIN/../../nvidia/nccl/lib:$ORIGIN/../../nvidia/nvtx/lib:$ORIGIN/../libs/' \"\n                    + env_dict.get(\"PADDLE_BINARY_DIR\")\n                    + '/python/paddle/base/'\n                    + env_dict.get(\"FLUID_CORE_NAME\")\n                    + '.so'\n                ]\n                if env_dict.get(\"WITH_SHARED_PHI\") == \"ON\":\n                    commands.append(\n                        \"patchelf --set-rpath '$ORIGIN/../../nvidia/cuda_runtime/lib:$ORIGIN:$ORIGIN/../libs' \"\n                        + env_dict.get(\"PADDLE_BINARY_DIR\")\n                        + '/python/paddle/libs/'\n                        + env_dict.get(\"PHI_NAME\")\n                    )\n                    commands.append(\n                        \"patchelf --set-rpath '$ORIGIN/../../nvidia/cuda_runtime/lib:$ORIGIN:$ORIGIN/../libs' \"\n                        + env_dict.get(\"PADDLE_BINARY_DIR\")\n                        + '/python/paddle/libs/'\n                        + env_dict.get(\"PHI_CORE_NAME\")\n                    )\n                    if (\n                        env_dict.get(\"WITH_GPU\") == \"ON\"\n                        or env_dict.get(\"WITH_ROCM\") == \"ON\"\n                    ):\n                        commands.append(\n                            \"patchelf --set-rpath '$ORIGIN/../../nvidia/cuda_runtime/lib:$ORIGIN:$ORIGIN/../libs' \"\n                            + env_dict.get(\"PADDLE_BINARY_DIR\")\n                            + '/python/paddle/libs/'\n                            + env_dict.get(\"PHI_GPU_NAME\")\n                        )\n\n                if env_dict.get(\"WITH_SHARED_IR\") == \"ON\":\n                    commands.append(\n                        \"patchelf --set-rpath '$ORIGIN:$ORIGIN/../libs' \"\n                        + env_dict.get(\"PADDLE_BINARY_DIR\")\n                        + '/python/paddle/libs/'\n                        + env_dict.get(\"IR_NAME\")\n                    )\n            # The sw_64 not support patchelf, so we just disable that.\n            if platform.machine() != 'sw_64' and platform.machine() != 'mips64':\n                for command in commands:\n                    if os.system(command) != 0:\n                        raise Exception(\n                            'patch '\n                            + env_dict.get(\"FLUID_CORE_NAME\")\n                            + f'{ext_suffix} failed',\n                            f'command: {command}',\n                        )\n    # A list of extensions that specify c++ -written modules that compile source code into dynamically linked libraries\n    ext_modules = [Extension('_foo', [paddle_binary_dir + '/python/stub.cc'])]\n    if os.name == 'nt':\n        # fix the path separator under windows\n        fix_package_dir = {}\n        for k, v in package_dir.items():\n            fix_package_dir[k] = v.replace('/', '\\\\')\n        package_dir = fix_package_dir\n        ext_modules = []\n    elif sys.platform == 'darwin':\n        ext_modules = []\n\n    return package_data, package_dir, ext_modules\n\n\ndef get_headers():\n    headers = (\n        # paddle level api headers (high level api, for both training and inference)\n        list(find_files('*.h', paddle_source_dir + '/paddle'))\n        + list(find_files('*.h', paddle_source_dir + '/paddle/phi/api'))\n        + list(  # phi unify api header\n            find_files('*.h', paddle_source_dir + '/paddle/phi/api/ext')\n        )\n        + list(  # custom op api\n            find_files('*.h', paddle_source_dir + '/paddle/phi/api/include')\n        )\n        + list(  # phi api\n            find_files('*.h', paddle_source_dir + '/paddle/phi/common')\n        )\n        + list(  # common api\n            find_files('*.h', paddle_source_dir + '/paddle/common')\n        )\n        # phi level api headers (low level api, for training only)\n        + list(  # phi extension header\n            find_files('*.h', paddle_source_dir + '/paddle/phi')\n        )\n        + list(  # phi include header\n            find_files(\n                '*.h', paddle_source_dir + '/paddle/phi/include', recursive=True\n            )\n        )\n        + list(  # phi backends headers\n            find_files(\n                '*.h',\n                paddle_source_dir + '/paddle/phi/backends',\n                recursive=True,\n            )\n        )\n        + list(  # phi core headers\n            find_files(\n                '*.h', paddle_source_dir + '/paddle/phi/core', recursive=True\n            )\n        )\n        + list(  # phi infermeta headers\n            find_files(\n                '*.h',\n                paddle_source_dir + '/paddle/phi/infermeta',\n                recursive=True,\n            )\n        )\n        + list(  # phi kernel headers\n            find_files(\n                '*.h',\n                paddle_source_dir + '/paddle/phi/kernels',\n                recursive=True,\n            )\n        )\n        # phi capi headers\n        + list(\n            find_files(\n                '*.h', paddle_source_dir + '/paddle/phi/capi', recursive=True\n            )\n        )\n        + list(  # utils api headers\n            find_files(\n                '*.h', paddle_source_dir + '/paddle/utils', recursive=True\n            )\n        )\n        + list(  # phi profiler headers\n            find_files(\n                '*.h',\n                paddle_source_dir + '/paddle/phi/api/profiler',\n                recursive=True,\n            )\n        )\n        + list(  # phi init headers\n            find_files(\n                'init_phi.h',\n                paddle_source_dir + '/paddle/fluid/platform',\n                recursive=True,\n            )\n        )\n        + list(  # pir init headers\n            find_files(\n                '*.h',\n                paddle_source_dir + '/paddle/pir/include',\n                recursive=True,\n            )\n        )\n        + list(  # drr init headers\n            find_files(\n                '*.h',\n                paddle_source_dir + '/paddle/fluid/pir/drr/include',\n                recursive=True,\n            )\n        )\n        + list(  # drr init headers\n            find_files(\n                '*.h',\n                paddle_source_dir\n                + '/paddle/fluid/pir/dialect/operator/interface/infer_symbolic_shape',\n                recursive=True,\n            )\n        )\n        + list(  # operator init headers\n            find_files(\n                '*.h',\n                paddle_source_dir + '/paddle/fluid/pir/dialect/operator/ir',\n            )\n        )\n        + list(  # pass utils init headers\n            find_files(\n                'general_functions.h',\n                paddle_source_dir + '/paddle/fluid/pir/utils',\n            )\n        )\n        + list(  # serialize and deserialize interface headers\n            find_files(\n                'interface.h',\n                paddle_source_dir\n                + '/paddle/fluid/pir/serialize_deserialize/include',\n            )\n        )\n        + list(  # serialize and deserialize interface headers\n            find_files(\n                'dense_tensor.inl',\n                paddle_source_dir + '/paddle/phi/core',\n            )\n        )\n        + list(  # serialize and deserialize interface headers\n            find_files(\n                'op_yaml_info.h',\n                paddle_source_dir\n                + '/paddle/fluid/pir/dialect/operator/interface',\n            )\n        )\n        + list(  # serialize and deserialize interface headers\n            find_files(\n                'op_yaml_info_util.h',\n                paddle_source_dir + '/paddle/fluid/pir/dialect/operator/utils',\n            )\n        )\n        + list(  # op yaml parser interface headers\n            find_files(\n                'op_yaml_info_parser.h',\n                paddle_source_dir + '/paddle/fluid/pir/dialect/operator/utils',\n            )\n        )\n        + list(  # pir op utils interface headers\n            find_files(\n                'utils.h',\n                paddle_source_dir + '/paddle/fluid/pir/dialect/operator/utils',\n            )\n        )\n        + list(  # ir translator interface headers\n            find_files(\n                'op_compat_info.h',\n                paddle_source_dir + '/paddle/fluid/ir_adaptor/translator/',\n            )\n        )\n        + list(\n            find_files(\n                'infer_symbolic_shape.h',\n                paddle_source_dir\n                + '/paddle/fluid/pir/dialect/operator/interface/infer_symbolic_shape',\n            )\n        )\n        + list(\n            find_files(\n                'vjp.h',\n                paddle_source_dir\n                + '/paddle/fluid/pir/dialect/operator/interface',\n            )\n        )\n        + list(\n            find_files(\n                'lexer.h',\n                paddle_source_dir + '/paddle/pir/src/core/parser',\n            )\n        )\n        + list(\n            find_files(\n                'token.h',\n                paddle_source_dir + '/paddle/pir/src/core/parser',\n            )\n        )\n    )\n\n    jit_layer_headers = [\n        'layer.h',\n        'serializer.h',\n        'serializer_utils.h',\n        'all.h',\n        'function.h',\n    ]\n\n    for f in jit_layer_headers:\n        headers += list(\n            find_files(\n                f, paddle_source_dir + '/paddle/fluid/jit', recursive=True\n            )\n        )\n\n    if env_dict.get(\"WITH_ONEDNN\") == 'ON':\n        headers += list(\n            find_files('*', env_dict.get(\"ONEDNN_INSTALL_DIR\") + '/include')\n        )  # mkldnn\n\n    if env_dict.get(\"WITH_OPENVINO\") == 'ON':\n        headers += list(\n            find_files('*', env_dict.get(\"OPENVINO_INC_DIR\"))\n        )  # openvino\n        headers += list(\n            find_files('*', env_dict.get(\"TBB_INC_DIR\"))\n        )  # openvino\n\n    if env_dict.get(\"WITH_GPU\") == 'ON' or env_dict.get(\"WITH_ROCM\") == 'ON':\n        # externalErrorMsg.pb for External Error message\n        headers += list(\n            find_files('*.pb', env_dict.get(\"externalError_INCLUDE_DIR\"))\n        )\n\n    if env_dict.get(\"WITH_XPU\") == 'ON':\n        headers += list(\n            find_files(\n                '*.h',\n                paddle_binary_dir + '/third_party/xpu/src/extern_xpu/xpu',\n                recursive=True,\n            )\n        )  # xdnn api headers\n        headers += list(\n            find_files(\n                '*.hpp',\n                paddle_binary_dir + '/third_party/xpu/src/extern_xpu/xpu',\n                recursive=True,\n            )\n        )  # xre headers with .hpp extension\n\n    # pybind headers\n    headers += list(find_files('*.h', env_dict.get(\"PYBIND_INCLUDE_DIR\"), True))\n    return headers\n\n\ndef get_setup_parameters():\n    # get setup_requires\n    setup_requires = get_setup_requires()\n    if (\n        env_dict.get(\"WITH_GPU\") == 'ON'\n        and platform.system() in ('Linux', 'Windows')\n        and platform.machine()\n        in (\n            'x86_64',\n            'AMD64',\n        )\n    ):\n        paddle_cuda_requires, paddle_tensorrt_requires = (\n            get_paddle_extra_install_requirements()\n        )\n        setup_requires += paddle_cuda_requires\n        setup_requires += paddle_tensorrt_requires\n\n    packages = [\n        'paddle',\n        'paddle.libs',\n        'paddle.utils',\n        'paddle.utils.gast',\n        'paddle.utils.cpp_extension',\n        'paddle.dataset',\n        'paddle.reader',\n        'paddle.distributed',\n        'paddle.distributed.checkpoint',\n        'paddle.distributed.communication',\n        'paddle.distributed.communication.stream',\n        'paddle.distributed.metric',\n        'paddle.distributed.ps',\n        'paddle.distributed.ps.utils',\n        'paddle.incubate',\n        'paddle.incubate.autograd',\n        'paddle.incubate.optimizer',\n        'paddle.incubate.checkpoint',\n        'paddle.incubate.operators',\n        'paddle.incubate.tensor',\n        'paddle.incubate.multiprocessing',\n        'paddle.incubate.nn',\n        'paddle.incubate.jit',\n        'paddle.incubate.asp',\n        'paddle.incubate.passes',\n        'paddle.incubate.framework',\n        'paddle.distribution',\n        'paddle.distributed.utils',\n        'paddle.distributed.sharding',\n        'paddle.distributed.fleet',\n        'paddle.distributed.auto_tuner',\n        'paddle.distributed.launch',\n        'paddle.distributed.launch.context',\n        'paddle.distributed.launch.controllers',\n        'paddle.distributed.launch.job',\n        'paddle.distributed.launch.plugins',\n        'paddle.distributed.launch.utils',\n        'paddle.distributed.fleet.base',\n        'paddle.distributed.fleet.recompute',\n        'paddle.distributed.fleet.elastic',\n        'paddle.distributed.fleet.meta_optimizers',\n        'paddle.distributed.fleet.meta_optimizers.sharding',\n        'paddle.distributed.fleet.meta_optimizers.dygraph_optimizer',\n        'paddle.distributed.fleet.runtime',\n        'paddle.distributed.rpc',\n        'paddle.distributed.fleet.dataset',\n        'paddle.distributed.fleet.data_generator',\n        'paddle.distributed.fleet.metrics',\n        'paddle.distributed.fleet.proto',\n        'paddle.distributed.fleet.utils',\n        'paddle.distributed.fleet.layers',\n        'paddle.distributed.fleet.layers.mpu',\n        'paddle.distributed.fleet.meta_parallel',\n        'paddle.distributed.fleet.meta_parallel.pp_utils',\n        'paddle.distributed.fleet.meta_parallel.sharding',\n        'paddle.distributed.fleet.meta_parallel.parallel_layers',\n        'paddle.distributed.auto_parallel',\n        'paddle.distributed.auto_parallel.intermediate',\n        'paddle.distributed.auto_parallel.dygraph',\n        'paddle.distributed.auto_parallel.static',\n        'paddle.distributed.auto_parallel.static.operators',\n        'paddle.distributed.auto_parallel.static.tuner',\n        'paddle.distributed.auto_parallel.static.cost',\n        'paddle.distributed.auto_parallel.static.reshard_funcs',\n        'paddle.distributed.passes',\n        'paddle.distributed.passes.pipeline_scheduler_pass',\n        'paddle.distributed.models',\n        'paddle.distributed.models.moe',\n        'paddle.distributed.transpiler',\n        'paddle.distributed.transpiler.details',\n        'paddle.framework',\n        'paddle.jit',\n        'paddle.jit.dy2static',\n        'paddle.jit.dy2static.transformers',\n        'paddle.jit.pir_dy2static',\n        'paddle.jit.sot',\n        'paddle.jit.sot.opcode_translator',\n        'paddle.jit.sot.opcode_translator.executor',\n        'paddle.jit.sot.opcode_translator.executor.variables',\n        'paddle.jit.sot.opcode_translator.instruction_utils',\n        'paddle.jit.sot.profiler',\n        'paddle.jit.sot.symbolic',\n        'paddle.jit.sot.utils',\n        'paddle.inference',\n        'paddle.inference.contrib',\n        'paddle.inference.contrib.utils',\n        'paddle.base',\n        'paddle.base.dygraph',\n        'paddle.base.proto',\n        'paddle.base.proto.profiler',\n        'paddle.base.layers',\n        'paddle.base.incubate',\n        'paddle.incubate.distributed.fleet',\n        'paddle.base.incubate.checkpoint',\n        'paddle.amp',\n        'paddle.cost_model',\n        'paddle.cinn_config',\n        'paddle.hapi',\n        'paddle.vision',\n        'paddle.vision.models',\n        'paddle.vision.transforms',\n        'paddle.vision.datasets',\n        'paddle.audio',\n        'paddle.audio.functional',\n        'paddle.audio.features',\n        'paddle.audio.datasets',\n        'paddle.audio.backends',\n        'paddle.text',\n        'paddle.text.datasets',\n        'paddle.incubate',\n        'paddle.incubate.nn',\n        'paddle.incubate.jit',\n        'paddle.incubate.nn.functional',\n        'paddle.incubate.nn.layer',\n        'paddle.incubate.optimizer.functional',\n        'paddle.incubate.autograd',\n        'paddle.incubate.distributed',\n        'paddle.incubate.distributed.utils',\n        'paddle.incubate.distributed.utils.io',\n        'paddle.incubate.distributed.fleet',\n        'paddle.incubate.distributed.models',\n        'paddle.incubate.distributed.models.moe',\n        'paddle.incubate.distributed.models.moe.gate',\n        'paddle.incubate.distributed.fleet.parameter_server',\n        'paddle.incubate.distributed.fleet.parameter_server.distribute_transpiler',\n        'paddle.incubate.distributed.fleet.parameter_server.ir',\n        'paddle.incubate.distributed.fleet.parameter_server.pslib',\n        'paddle.incubate.layers',\n        'paddle.quantization',\n        'paddle.quantization.quanters',\n        'paddle.quantization.observers',\n        'paddle.sparse',\n        'paddle.sparse.nn',\n        'paddle.sparse.nn.layer',\n        'paddle.sparse.nn.functional',\n        'paddle.incubate.xpu',\n        'paddle.io',\n        'paddle.io.dataloader',\n        'paddle.optimizer',\n        'paddle.nn',\n        'paddle.nn.functional',\n        'paddle.nn.layer',\n        'paddle.nn.quant',\n        'paddle.nn.quant.qat',\n        'paddle.nn.initializer',\n        'paddle.nn.utils',\n        'paddle.metric',\n        'paddle.static',\n        'paddle.static.nn',\n        'paddle.static.amp',\n        'paddle.static.amp.bf16',\n        'paddle.static.quantization',\n        'paddle.quantization',\n        'paddle.quantization.imperative',\n        'paddle.tensor',\n        'paddle.onnx',\n        'paddle.autograd',\n        'paddle.device',\n        'paddle.device.cuda',\n        'paddle.device.xpu',\n        'paddle.version',\n        'paddle.profiler',\n        'paddle.geometric',\n        'paddle.geometric.message_passing',\n        'paddle.geometric.sampling',\n        'paddle.pir',\n        'paddle.decomposition',\n        'paddle._typing',\n        'paddle._typing.libs',\n        'paddle.tensorrt',\n    ]\n\n    paddle_bins = ''\n    if not env_dict.get(\"WIN32\"):\n        paddle_bins = [\n            env_dict.get(\"PADDLE_BINARY_DIR\") + '/paddle/scripts/paddle'\n        ]\n    package_data, package_dir, ext_modules = get_package_data_and_package_dir()\n    headers = get_headers()\n    return (\n        setup_requires,\n        packages,\n        paddle_bins,\n        package_data,\n        package_dir,\n        ext_modules,\n        headers,\n    )\n\n\ndef check_build_dependency():\n    missing_modules = '''Missing build dependency: {dependency}\nPlease run 'pip install -r python/requirements.txt' to make sure you have all the dependencies installed.\n'''.strip()\n\n    with open(TOP_DIR + '/python/requirements.txt') as f:\n        build_dependencies = (\n            f.read().splitlines()\n        )  # Specify the dependencies to install\n\n    python_dependencies_module = []\n    installed_packages = []\n\n    for dependency in build_dependencies:\n        python_dependencies_module.append(\n            re.sub(\"_|-\", '', re.sub(r\"==.*|>=.*|<=.*\", '', dependency))\n        )\n    reqs = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze'])\n\n    for r in reqs.split():\n        installed_packages.append(\n            re.sub(\"_|-\", '', r.decode().split('==')[0]).lower()\n        )\n\n    for dependency in python_dependencies_module:\n        if dependency.lower() not in installed_packages:\n            raise RuntimeError(missing_modules.format(dependency=dependency))\n\n\ndef install_cpp_dist_and_build_test(install_dir, lib_test_dir, headers, libs):\n    \"\"\"install cpp distribution and build test target\n\n    TODO(huangjiyi):\n    1. This function will be moved when separating C++ distribution\n    installation from python package installation.\n    2. Reduce the header and library files to be installed.\n    \"\"\"\n    if env_dict.get(\"CMAKE_BUILD_TYPE\") != 'Release':\n        return\n    os.makedirs(install_dir, exist_ok=True)\n    # install C++ header files\n    for header in headers:\n        header_install_dir = get_header_install_dir(header)\n        header_install_dir = os.path.join(\n            install_dir, 'include', os.path.dirname(header_install_dir)\n        )\n        os.makedirs(header_install_dir, exist_ok=True)\n        shutil.copy(header, header_install_dir)\n\n    # install C++ shared libraries\n    lib_install_dir = os.path.join(install_dir, 'lib')\n    os.makedirs(lib_install_dir, exist_ok=True)\n    # install libpaddle.ext\n    paddle_libs = glob.glob(\n        paddle_binary_dir\n        + '/paddle/fluid/pybind/'\n        + env_dict.get(\"FLUID_CORE_NAME\")\n        + '.*'\n    )\n    for lib in paddle_libs:\n        shutil.copy(lib, lib_install_dir)\n    # install dependent libraries\n    libs_path = paddle_binary_dir + '/python/paddle/libs'\n    for lib in libs:\n        lib_path = os.path.join(libs_path, lib)\n        shutil.copy(lib_path, lib_install_dir)\n\n    # build test target\n    cmake_args = [CMAKE, lib_test_dir, \"-B\", lib_test_dir]\n    if os.getenv(\"GENERATOR\") == \"Ninja\":\n        cmake_args.append(\"-GNinja\")\n    subprocess.check_call(cmake_args)\n    subprocess.check_call([CMAKE, \"--build\", lib_test_dir])\n\n\ndef check_submodules():\n    def get_submodule_folder():\n        git_submodules_path = os.path.join(TOP_DIR, \".gitmodules\")\n        with open(git_submodules_path) as f:\n            return [\n                os.path.join(TOP_DIR, line.split(\"=\", 1)[1].strip())\n                for line in f\n                if line.strip().startswith(\"path\")\n            ]\n\n    def submodules_not_exists_or_empty(folder):\n        return not os.path.exists(folder) or (\n            os.path.isdir(folder) and len(os.listdir(folder)) == 0\n        )\n\n    submodule_folders = get_submodule_folder()\n    # f none of the submodule folders exists, try to initialize them\n    if any(\n        submodules_not_exists_or_empty(folder) for folder in submodule_folders\n    ):\n        try:\n            print(' --- Trying to initialize submodules')\n            start = time.time()\n            subprocess.check_call(\n                [\"git\", \"submodule\", \"update\", \"--init\", \"--recursive\"],\n                cwd=TOP_DIR,\n            )\n            end = time.time()\n            print(f' --- Submodule initialization took {end - start:.2f} sec')\n        except Exception:\n            print(' --- Submodule initialization failed')\n            print('Please run:\\n\\tgit submodule update --init --recursive')\n            sys.exit(1)\n\n\ndef generate_stub_files(paddle_binary_dir, paddle_source_dir):\n    script_path = paddle_source_dir + '/tools/'\n    sys.path.append(script_path)\n\n    print('-' * 2, 'Generate stub file tensor.pyi ... ')\n    import gen_tensor_stub\n\n    gen_tensor_stub.generate_stub_file(\n        input_file=paddle_source_dir\n        + '/python/paddle/tensor/tensor.prototype.pyi',\n        output_file=paddle_binary_dir + '/python/paddle/tensor/tensor.pyi',\n    )\n\n    shutil.copy(\n        paddle_binary_dir + '/python/paddle/tensor/tensor.pyi',\n        paddle_source_dir + '/python/paddle/tensor/tensor.pyi',\n    )\n    print('-' * 2, 'End Generate stub file tensor.pyi ... ')\n\n    print('-' * 2, 'Generate stub file for python binding APIs ... ')\n    import gen_pybind11_stub\n\n    gen_pybind11_stub.generate_stub_file(\n        output_dir=str(Path(paddle_binary_dir) / 'python/paddle/_typing/libs/'),\n        module_name='paddle.base.libpaddle',\n        ignore_all_errors=True,\n        ops_yaml=[\n            paddle_source_dir\n            + \"/paddle/phi/ops/yaml/ops.yaml;paddle.base.libpaddle.eager.ops\",\n            paddle_source_dir\n            + \"/paddle/phi/ops/yaml/ops.yaml;paddle.base.libpaddle.pir.ops\",\n            paddle_source_dir\n            + \"/paddle/phi/ops/yaml/sparse_ops.yaml;paddle.base.libpaddle.eager.ops;sparse\",\n            paddle_source_dir\n            + \"/paddle/phi/ops/yaml/sparse_ops.yaml;paddle.base.libpaddle.pir.ops;sparse\",\n            paddle_source_dir\n            + \"/paddle/phi/ops/yaml/strings_ops.yaml;paddle.base.libpaddle.eager.ops;strings\",\n            paddle_source_dir\n            + \"/paddle/phi/ops/yaml/strings_ops.yaml;paddle.base.libpaddle.pir.ops;strings\",\n        ],\n    )\n\n    libpaddle_dst = paddle_source_dir + '/python/paddle/_typing/libs/libpaddle'\n    if Path(libpaddle_dst).exists():\n        shutil.rmtree(libpaddle_dst)\n\n    shutil.copytree(\n        paddle_binary_dir + '/python/paddle/_typing/libs/libpaddle',\n        libpaddle_dst,\n    )\n\n    print('-' * 2, 'End Generate stub for python binding APIs ... ')\n\n\ndef main():\n    # Parse the command line and check arguments before we proceed with building steps and setup\n    parse_input_command(filter_args_list)\n\n    # check build dependency\n    check_build_dependency()\n    check_submodules()\n    # Execute the build process,cmake and make\n    if cmake_and_build:\n        build_steps()\n\n    if os.getenv(\"WITH_PYTHON\") == \"OFF\":\n        print(\"only compile, not package\")\n        return\n\n    build_dir = os.getenv(\"BUILD_DIR\")\n    if build_dir is not None:\n        env_dict_path = TOP_DIR + '/' + build_dir + '/python'\n    else:\n        env_dict_path = TOP_DIR + \"/build/python/\"\n    sys.path.insert(1, env_dict_path)\n    from env_dict import env_dict\n\n    global env_dict  # noqa: F811\n    global paddle_binary_dir, paddle_source_dir\n\n    paddle_binary_dir = env_dict.get(\"PADDLE_BINARY_DIR\")\n    paddle_source_dir = env_dict.get(\"PADDLE_SOURCE_DIR\")\n\n    # preparing parameters for setup()\n    paddle_version = env_dict.get(\"PADDLE_VERSION\")\n    package_name = env_dict.get(\"PACKAGE_NAME\")\n\n    write_version_py(\n        filename=f'{paddle_binary_dir}/python/paddle/version/__init__.py'\n    )\n    write_cuda_env_config_py(\n        filename=f'{paddle_binary_dir}/python/paddle/cuda_env.py'\n    )\n    write_parameter_server_version_py(\n        filename=f'{paddle_binary_dir}/python/paddle/incubate/distributed/fleet/parameter_server/version.py'\n    )\n    (\n        setup_requires,\n        packages,\n        scripts,\n        package_data,\n        package_dir,\n        ext_modules,\n        headers,\n    ) = get_setup_parameters()\n\n    # Log for PYPI, get long_description of setup()\n    with open(\n        paddle_source_dir + '/python/paddle/README.md', \"r\", encoding='UTF-8'\n    ) as f:\n        long_description = f.read()\n\n    # strip *.so to reduce package size\n    if env_dict.get(\"WITH_STRIP\") == 'ON':\n        command = (\n            'find '\n            + shlex.quote(paddle_binary_dir)\n            + '/python/paddle -name \"*.so\" | xargs -i strip {}'\n        )\n        if os.system(command) != 0:\n            raise Exception(f\"strip *.so failed, command: {command}\")\n\n    # install cpp distribution\n    if env_dict.get(\"WITH_CPP_DIST\") == 'ON':\n        paddle_install_dir = env_dict.get(\"PADDLE_INSTALL_DIR\")\n        paddle_lib_test_dir = env_dict.get(\"PADDLE_LIB_TEST_DIR\")\n        install_cpp_dist_and_build_test(\n            paddle_install_dir,\n            paddle_lib_test_dir,\n            headers,\n            package_data['paddle.libs'],\n        )\n\n    # generate stub file `tensor.pyi`\n    if os.getenv(\"SKIP_STUB_GEN\", '').lower() not in [\n        'y',\n        'yes',\n        't',\n        'true',\n        'on',\n        '1',\n    ]:\n        generate_stub_files(paddle_binary_dir, paddle_source_dir)\n    # package stub files\n    packages, package_data = extend_type_hints_package_data(\n        packages, package_data, paddle_binary_dir\n    )\n\n    setup(\n        name=package_name,\n        version=paddle_version,\n        description='Parallel Distributed Deep Learning',\n        long_description=long_description,\n        long_description_content_type=\"text/markdown\",\n        author_email=\"Paddle-better@baidu.com\",\n        maintainer=\"PaddlePaddle\",\n        maintainer_email=\"Paddle-better@baidu.com\",\n        url='https://www.paddlepaddle.org.cn/',\n        download_url='https://github.com/paddlepaddle/paddle',\n        license='Apache Software License',\n        packages=packages,\n        install_requires=setup_requires,\n        ext_modules=ext_modules,\n        package_data=package_data,\n        package_dir=package_dir,\n        scripts=scripts,\n        distclass=BinaryDistribution,\n        headers=headers,\n        cmdclass={\n            'install_headers': InstallHeaders,\n            'install': InstallCommand,\n            'egg_info': EggInfo,\n            'install_lib': InstallLib,\n            'develop': DevelopCommand,\n        },\n        entry_points={\n            'console_scripts': [\n                'fleetrun = paddle.distributed.launch.main:launch'\n            ]\n        },\n        classifiers=[\n            'Development Status :: 5 - Production/Stable',\n            'Operating System :: OS Independent',\n            'Intended Audience :: Developers',\n            'Intended Audience :: Education',\n            'Intended Audience :: Science/Research',\n            'License :: OSI Approved :: Apache Software License',\n            'Programming Language :: C++',\n            'Programming Language :: Python :: 3.8',\n            'Programming Language :: Python :: 3.9',\n            'Programming Language :: Python :: 3.10',\n            'Programming Language :: Python :: 3.11',\n            'Programming Language :: Python :: 3.12',\n            'Programming Language :: Python :: 3.13',\n            'Typing :: Typed',\n        ],\n    )\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}