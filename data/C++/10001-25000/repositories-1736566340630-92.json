{
  "metadata": {
    "timestamp": 1736566340630,
    "page": 92,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "arangodb/arangodb",
      "stars": 13641,
      "defaultBranch": "devel",
      "files": [
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 3.90625,
          "content": "---\nLanguage:        Cpp\nAccessModifierOffset: -1\nAlignAfterOpenBracket: Align\nAlignConsecutiveAssignments: false\nAlignConsecutiveDeclarations: false\nAlignEscapedNewlines: Left\nAlignOperands:   true\nAlignTrailingComments: true\nAllowAllParametersOfDeclarationOnNextLine: true\nAllowShortBlocksOnASingleLine: false\nAllowShortCaseLabelsOnASingleLine: false\nAllowShortFunctionsOnASingleLine: All\nAllowShortIfStatementsOnASingleLine: true\nAllowShortLoopsOnASingleLine: true\nAlwaysBreakAfterDefinitionReturnType: None\nAlwaysBreakAfterReturnType: None\nAlwaysBreakBeforeMultilineStrings: true\nAlwaysBreakTemplateDeclarations: Yes\nBinPackArguments: true\nBinPackParameters: true\nBraceWrapping:\n  AfterClass:      false\n  AfterControlStatement: false\n  AfterEnum:       false\n  AfterFunction:   false\n  AfterNamespace:  false\n  AfterObjCDeclaration: false\n  AfterStruct:     false\n  AfterUnion:      false\n  AfterExternBlock: false\n  BeforeCatch:     false\n  BeforeElse:      false\n  IndentBraces:    false\n  SplitEmptyFunction: true\n  SplitEmptyRecord: true\n  SplitEmptyNamespace: true\nBreakBeforeBinaryOperators: None\nBreakBeforeBraces: Attach\nBreakBeforeInheritanceComma: false\nBreakInheritanceList: BeforeColon\nBreakBeforeTernaryOperators: true\nBreakConstructorInitializersBeforeComma: false\nBreakConstructorInitializers: BeforeColon\nBreakAfterJavaFieldAnnotations: false\nBreakStringLiterals: true\nColumnLimit:     80\nCommentPragmas:  '^ IWYU pragma:'\nCompactNamespaces: false\nConstructorInitializerAllOnOneLineOrOnePerLine: true\nConstructorInitializerIndentWidth: 4\nContinuationIndentWidth: 4\nCpp11BracedListStyle: true\nDerivePointerAlignment: false\nDisableFormat:   false\nExperimentalAutoDetectBinPacking: false\nFixNamespaceComments: true\nForEachMacros:\n  - foreach\n  - Q_FOREACH\n  - BOOST_FOREACH\nIncludeBlocks:   Regroup\nIncludeCategories:\n  - Regex:           '^<ext/.*\\.h>'\n    Priority:        2\n  - Regex:           '^<.*\\.h>'\n    Priority:        1\n  - Regex:           '^<.*'\n    Priority:        2\n  - Regex:           '.*'\n    Priority:        3\nIncludeIsMainRegex: '([-_](test|unittest))?$'\nIndentCaseLabels: true\nIndentPPDirectives: None\nIndentWidth:     2\nIndentWrappedFunctionNames: false\nJavaScriptQuotes: Leave\nJavaScriptWrapImports: true\nKeepEmptyLinesAtTheStartOfBlocks: false\nMacroBlockBegin: ''\nMacroBlockEnd:   ''\nMaxEmptyLinesToKeep: 1\nNamespaceIndentation: None\nObjCBinPackProtocolList: Never\nObjCBlockIndentWidth: 2\nObjCSpaceAfterProperty: false\nObjCSpaceBeforeProtocolList: true\nPenaltyBreakAssignment: 2\nPenaltyBreakBeforeFirstCallParameter: 1\nPenaltyBreakComment: 300\nPenaltyBreakFirstLessLess: 120\nPenaltyBreakString: 1000\nPenaltyBreakTemplateDeclaration: 10\nPenaltyExcessCharacter: 1000000\nPenaltyReturnTypeOnItsOwnLine: 200\nPointerAlignment: Left\nRawStringFormats:\n  - Language:        Cpp\n    Delimiters:\n      - cc\n      - CC\n      - cpp\n      - Cpp\n      - CPP\n      - 'c++'\n      - 'C++'\n    CanonicalDelimiter: ''\n    BasedOnStyle:    google\n  - Language:        TextProto\n    Delimiters:\n      - pb\n      - PB\n      - proto\n      - PROTO\n    EnclosingFunctions:\n      - EqualsProto\n      - EquivToProto\n      - PARSE_PARTIAL_TEXT_PROTO\n      - PARSE_TEST_PROTO\n      - PARSE_TEXT_PROTO\n      - ParseTextOrDie\n      - ParseTextProtoOrDie\n    CanonicalDelimiter: ''\n    BasedOnStyle:    google\nReflowComments:  true\nSortIncludes:    false\nSortUsingDeclarations: true\nSpaceAfterCStyleCast: false\nSpaceAfterTemplateKeyword: false\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeCpp11BracedList: false\nSpaceBeforeCtorInitializerColon: true\nSpaceBeforeInheritanceColon: true\nSpaceBeforeParens: ControlStatements\nSpaceBeforeRangeBasedForLoopColon: true\nSpaceInEmptyParentheses: false\nSpacesBeforeTrailingComments: 2\nSpacesInAngles:  false\nSpacesInContainerLiterals: true\nSpacesInCStyleCastParentheses: false\nSpacesInParentheses: false\nSpacesInSquareBrackets: false\nStandard: Latest\nStatementMacros:\n  - Q_UNUSED\n  - QT_REQUIRE_VERSION\nTabWidth:        8\nUseTab:          Never\n"
        },
        {
          "name": ".ctags_exclude",
          "type": "blob",
          "size": 0.0693359375,
          "content": "Installation\nout\n3rdParty\nDocumentation\njs/node/node_modules\n.git\n.svn\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.7421875,
          "content": "# EditorConfig is awesome: http://EditorConfig.org\n\n# top-most EditorConfig file\nroot = true\n\n# Unix-style newlines with a newline ending every file\n[*]\nindent_size = 2\nend_of_line = lf\ninsert_final_newline = true\n\n[*.{c,cpp,h,hpp}]\ncharset = utf-8\nindent_style = space\nindent_size = 2\n\n# Matches multiple files with brace expansion notation\n# Set default charset\n[*.{js,py}]\ncharset = utf-8\n\n# 4 space indentation\n[*.py]\nindent_style = space\nindent_size = 4\n\n# Tab indentation (no size specified)\n[Makefile]\nindent_style = tab\n\n# Indentation override for all JS under lib directory\n[lib/**.js]\nindent_style = space\nindent_size = 2\n\n# Matches the exact files either package.json or .travis.yml\n[{package.json,.travis.yml}]\nindent_style = space\nindent_size = 2\n"
        },
        {
          "name": ".git-blame-ignore-revs",
          "type": "blob",
          "size": 0.4443359375,
          "content": "# This file contains a list of commits that are not likely what you\n# are looking for in a blame, such as mass reformatting or renaming.\n# You can set this file as a default ignore file for blame by running\n# the following command.\n#\n# $ git config blame.ignoreRevsFile .git-blame-ignore-revs\n\n# larger renaming PR for AQL functions\n736e2d519e2187cdf252654ff4518166ff1a7e2a\n\n# larger renaming PR for AQL executors\n80894efd1ff08b84c447fd571d6bd0878ba03b31\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.353515625,
          "content": "*.sh eol=lf \nVERSION eol=lf\nVERSIONS eol=lf\nscripts/unittest eol=lf\n*.csv binary\n*.json eol=lf\nVERSION merge=ours\nSTARTER_REV merge=ours\narangod/Aql/tokens.cpp merge=ours\narangod/Aql/grammar.cpp merge=ours\njs/apps/system/_admin/aardvark/APP/api-docs.json merge=ours\njs/node/node_modules/* linguist-vendored\ntests/* linguist-vendored\n3rdParty/* linguist-vendored\n"
        },
        {
          "name": ".githooks",
          "type": "tree",
          "content": null
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.126953125,
          "content": "tags\n./build\nbuild-presets\n/core.*\n*.vim\n.deps\n.dirstamp\n*.o\n*.a\n*~\n*.pyc\n*.sdf\n*.suo\n*.log\n\n*.iml\n*.orig\n*.rej\n*.gcno\n*.gcda\n\n*.deb\n*.rpm\n*.user\n\n.DS_Store\n*.swp\n*.diff\n*.patch\n!js/node/patches/*.patch\n!js/apps/system/_admin/aardvark/APP/react/patches/*.patch\n*.lnk\nThumbs.db\n\ncppcheck.xml\n\nenterprise\nupgrade-data-tests\ncompile_commands.json\ninstanceinfo.json\ntestresult.json\ntestsStarted\nsoc-pokec-*\n\nDocumentation/man\n\nbuild.sh\n/build*/\nBuild64/\nBuild32/\nDebug64/\nDebug32/\nRelease64/\nRelease32/\nWindowsLibraries/\n/cluster/**\n/cluster-init*/**\np/\n\n/core\n!3rdParty/iresearch/core\n!core.h\nTAGS\n\nCMakeUserPresets.json\nCPackConfig.cmake\nCPackSourceConfig.cmake\nCTestTestfile.cmake\nUnitTests/CTestTestfile.cmake\nUnitTests/cmake_install.cmake\narangod/cmake_install.cmake\nclient-tools/cmake_install.cmake\ncmake_install.cmake\nlib/cmake_install.cmake\n\narangod/Aql/grammar.output\n\n/bin\nlogs/\nout/\n\netc/arangodb/*.conf\n\nInstallation/MacOSX/Bundle/Info.plist\n\nnbproject/\n.idea\n.vs\nCMakeSettings.json\n.vscode\n*.sublime-workspace\n*.sublime-project\n\ntest.cpp.txt\n\n.sass-cache\n.sass-cache/\n\njs/apps/*\n!js/apps/system/\njs/apps/system/_admin/aardvark/APP/react/node_modules/\njs/apps/system/_admin/aardvark/APP/react/build/\njs/node/**/node_modules/.bin/\njs/node/node_modules/**/*.md\n!js/node/node_modules/**/LICENSE.md\njs/node/node_modules/**/test/\njs/node/node_modules/**/*.ts\njs/node/node_modules/**/*.yml\njs/node/node_modules/**/.babelrc\njs/node/node_modules/**/.editorconfig\njs/node/node_modules/**/.eslintrc\njs/node/node_modules/**/.eslintrc.json\njs/node/node_modules/**/.eslintignore\njs/node/node_modules/**/.github\njs/node/node_modules/**/.jscs.json\njs/node/node_modules/**/.jslintrc\njs/node/node_modules/**/.npmignore\njs/node/node_modules/**/.nycrc\njs/node/node_modules/**/.nyc_output\n\n.gdb-history\nnpm-debug.log\n\n/log-*\ndata-*\ndatabases\n!js/apps/system/_admin/aardvark/APP/react/src/views/databases\ncluster-init\n\ndatafile-*.db\n# by build process\narangodb-linux-amd64\nlast_compiled_version.sha\n\nscripts/perfanalysis\nperf*\ncallgrind*\n\ncmake-build*/\n.cache/\n\njs/node/**/*.map\njs/node/**/*.map.gz\njs/apps/**/*.map\njs/apps/**/*.map.gz\n\nswagger-ui-es-bundle*\nswagger-ui.js\nswagger-ui.js.map\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 1.220703125,
          "content": "[submodule \"iresearch\"]\n\tpath = 3rdParty/iresearch\n\turl = https://github.com/arangodb/iresearch.git\n[submodule \"velocypack\"]\n\tpath = 3rdParty/velocypack\n\turl = https://github.com/arangodb/velocypack.git\n[submodule \"3rdParty/fmt\"]\n\tpath = 3rdParty/fmt\n\turl = https://github.com/fmtlib/fmt\n[submodule \"3rdParty/rocksdb\"]\n\tpath = 3rdParty/rocksdb\n\turl = https://github.com/arangodb/rocksdb.git\n[submodule \"3rdParty/gtest\"]\n\tpath = 3rdParty/gtest\n\turl = https://github.com/arangodb/googletest.git\n[submodule \"3rdParty/abseil-cpp\"]\n\tpath = 3rdParty/abseil-cpp\n\turl = https://github.com/arangodb/abseil-cpp.git\n[submodule \"3rdParty/immer\"]\n\tpath = 3rdParty/immer\n\turl = https://github.com/arangodb/immer\n\tbranch = release-v0.8\n[submodule \"3rdParty/rta-makedata\"]\n\tpath = 3rdParty/rta-makedata\n\turl = https://github.com/arangodb/rta-makedata\n[submodule \"scripts/toolbox/feed\"]\n\tpath = scripts/toolbox/feed\n\turl = https://github.com/arangodb/feed.git\n[submodule \"3rdParty/v8-build\"]\n\tpath = 3rdParty/v8-build/v8\n\turl = https://github.com/arangodb/v8.git\n[submodule \"3rdParty/nghttp2\"]\n\tpath = 3rdParty/nghttp2-lib\n\turl = https://github.com/arangodb/nghttp2.git\n[submodule \"3rdParty/faiss\"]\n\tpath = 3rdParty/faiss\n\turl = https://github.com/arangodb/faiss.git\n"
        },
        {
          "name": ".jsbeautifyrc",
          "type": "blob",
          "size": 0.046875,
          "content": "{\n    \"indent_size\": 2,\n    \"indent_char\": \" \"\n}"
        },
        {
          "name": "3rdParty",
          "type": "tree",
          "content": null
        },
        {
          "name": "ARANGO-VERSION",
          "type": "blob",
          "size": 0.0126953125,
          "content": "3.12.4-devel\n"
        },
        {
          "name": "CHANGELOG",
          "type": "blob",
          "size": 1130.9287109375,
          "content": "devel\n-----\n\n* Add optional `factory` property to `params` in vector index definition. The\n  index factory enables to create composite vector indexes supported by FAISS.\n  The factory string defines the index structure, including preprocessing,\n  inverted files, and encoding components.\n  More details on how to compose them can be found on FAISS documentation\n  https://github.com/facebookresearch/faiss/wiki/The-index-factory.\n  When defining the index string the number of nLists from string must match the\n  `nLists` parameter. Here is an example of using this parameter:\n  ```\n    db.<collection>.ensureIndex({\n      type: \"vector\",\n      fields: [\"value1\"],\n      params: {\n        \"metric\": \"l2\",\n        \"dimension\": 300,\n        \"nLists\": 3800,\n        \"factory\": \"IVF3800_HNSW32,PQ480x8\"\n      }\n    });\n\n* Improve geo index performance in the cluster with multiple shards. This\n  fixes BTS-2046. An unnecessary and bad SORT node is removed from the\n  query plan in the case that a geo index is used.\n\n* Fail queries that use `APPROX_NEAR` functions but do not apply the vector\n  index.\n\n* Rebuilt included rclone v1.65.5 with go1.22.10 and non-vulnerable\n  dependencies.\n\n* Updated ArangoDB Starter to v0.19.8.\n\n* Introduce new level of index serialization `Maintenance` that excludes\n  trainedData in vector index.\n\n* Enable passing `nProbe` as additional search parameter for APPROX_NEAR\n  vector functions. `nProbe` defines how many neighboring lists to look into,\n  it is a trade off between more correct results and time it takes to fetch\n  results. The syntax looks like this:\n  ```aql\n  FOR doc IN col\n  LET distance = APPROX_NEAR_L2(doc.vector, @qp, {nProbe: 20})\n  SORT distance LIMIT 5\n  RETURN {key: doc._key, distance}\n  ````\n\n* Test a collection before it is leased out from the ConnectionCache.\n  This helps to detect stale TCP/IP or TLS connections residing in the \n  ConnectionCache and thus prevents network failures due to delays.\n\n* [ES-2294] Fix computation of the arangodb_vocbase_shards_read_only_by_write_concern metric.\n\n* Enable passing `nProbe` as additional search parameter for APPROX_NEAR\n  vector functions. `nProbe` defines how many neighboring lists to look into,\n  it is a trade off between more correct results and time it takes to fetch\n  results. The syntax looks like this:\n  ```aql\n  FOR doc IN col\n  LET distance = APPROX_NEAR_L2(doc.vector, @qp, {nProbe: 20})\n  SORT distance LIMIT 5\n  RETURN {key: doc._key, distance}\n  ````\n\n* Add a vector index based on faiss implementation. The current vector index \n  implementation supports the L2 and cosine metrics. To create a vector index,\n  the vector field(s) in the collection must already be populated with data. Here is an\n  example of a vector index creation:\n  ```\n    db.<collection>.ensureIndex({\n      type: \"vector\",\n      fields: [\"value1\"],\n      params: {\n        \"metric\": \"l2\",\n        \"dimension\": 300,\n        \"nLists\": 100\n      }\n    });\n  ````\n  Where the parameters are:\n  - `metric`: defines how similarity/distance is calculated, can be\n    l2 or cosine,\n  - `nLists`: number of lists to create,\n  - `dimension`: vector dimensions, ArangoDB will also check that all the vectors\n    have the correct number of components,\n  - `trainingIterations`: how many training iterations will be used, \n  The current limitation is that a vector index can be created only on a single field.\n  The vector index creation has two phases, training in which the centroids are being determined\n  from the subset of data, and insertion phase in which the lists are being populated.\n  To trigger the vector index an appropriate `APPROX_NEAR` function must be used\n  along with `SORT` and `LIMIT` clauses e.g.:\n  ```\n    FOR d IN docs\n    LET distance = APPROX_NEAR_L2(d.vector, @queryVector)\n    SORT distance\n    LIMIT 10\n    RETURN {d, distance}\n  ````\n  This returns the top 10 nearest neighbors to `@queryVector`.\n\n* Introduce expiry time for idle TCP/IP connections in the ConnectionCache\n  (for `SimpleHttpClient`) with a default of 120s. This is to prevent\n  errors in replication caused by cloud environments terminating\n  connections. Also add retries in a few places. Also increase the\n  timeout in initial sync to transfer up to 5000 documents from 25s to\n  900s. This addresses BTS-2011, BTS-2035 and BTS-2042.\n\n* Improve the use-index-for-sort optimizer rule: If there is a persistent\n  index which starts with the same fields as the sort fields, the index\n  can be used for sorting these first fields that are covered by the index.\n  Subsequent rows with the same value in these fields are then sorted by\n  the rest of the sorting fields using a new grouped sort executor.\n\n* If a coordinator cannot send a heartbeat to the agency, it will shut down\n  itself after 30 mins (configurable by the startup option\n  `--cluster.no-heartbeat-delay-before-shutdown` whose default is 1800.\n  This is to ensure that servers which are automatically cleaned up,\n  because they are away for too long cannot disturb hotbackups by running\n  transactions.\n\n* Reduce the time until an expired server is removed from the agency to 1h \n  (from 24h). This helps with BTS-2039 and thus helps hotbackups to go\n  through in more cases when coordinators have crashed. The time is\n  configurable with the `--agency.supervision-expired-servers-grace-time`\n  command line option.\n\n* Changed the priority of WebUI request to prevent endless loading if the\n  server is under load\n\n* Truncate error messages by rclone for TransferJobs to avoid excessive\n  memory use in the agency. Keep only 10 finished TransferJobs in agency.\n  This fixes BTS-2041.\n\n* Don't cleanup failed agency jobs if they are subjobs of pending jobs.\n  This avoids a bug in CleanOutServer jobs, where such a job could complete\n  seemingly successfully despite the fact that some MoveShard jobs had \n  actually failed. This fixes BTS-2022.\n\n* Disallow the KEEP keyword in conjunction with INTO var = expr when doing\n  COLLECT in an AQL query. The KEEP clause had no effect.\n\n* Do check for replication version 2 in a safe way. This fixes BTS-2040.\n\n* Put in more diagnostics and avoid a crash in follower on commit of a\n  transaction in case a collection is already gone (BTS-2033).\n\n* Fix delivery of planCacheKey attribute for batched and streaming cursors.\n\n* Fix view usage in cached query plans.\n\n* Optimized COLLECT queries by pushing collection of INTO values onto database\n  servers and aggregate on the coordinator. This can be disabled by setting\n  the option `aggregateIntoExpressionOnDBServers` to false.\n\n* Make it an error if usePlanCache is set to true but the query is not \n  eligible for query plan caching. This is better for debugging.\n\n* Upgraded OpenSSL to 3.4.0.\n\n* BTS-2018: enable count cache for virtual smart edge collections.\n\n* BTS-2017: fix CleanOutServer for satellite collections.\n\n* Add metric that counts the total number of lost transaction subordinate\n  states on the database servers.\n\n* FE-483: add support for usePlanCache in query editor UI.\n\n* ArangoDB now has an optional AQL query execution plan cache that can be used\n  to skip query planning and optimization when running the same queries \n  repeatedly. This feature is currently experimental.\n\n  The plan cache is fully optional and is only considered for AQL queries that\n  have the option `usePlanCache` set to `true`. If the option is not\n  set to `true`, the plan cache is bypassed. This is also the default.\n\n  If the option is set to `true`, the query's eligibility for being cached is\n  checked first. Any queries that have any of the following query options set\n  are not eligible for plan caching:\n  - `allPlans`\n  - `explainRegisters`\n  - `inaccessibleCollections`\n  - `optimizer.rules`\n  - `shardIds`\n\n  Additionally, a query is not eligible for plan caching if it uses attribute \n  name bind parameters, e.g. `FILTER doc.@attributeName == ...`.\n  Furthermore, queries will not be eligible for plan caching when using value\n  bind parameters in any of the following places:\n  - specifying the depths for traversals or path queries (e.g. \n    `FOR v, e IN @min..@max OUTBOUND ...`)\n  - referring to a named graph (e.g. `GRAPH @graphName`)\n  - referring to edge collections used in traversals or path queries \n    (e.g. `FOR v, e IN 1..2 OUTBOUND 'v/0' @ec...`)\n  - specifying the lookup value for UPSERT operations, e.g. `UPSERT @value...`)\n  If a query produces any warnings during parsing or query plan optimization, \n  it is also not eligible for plan caching.\n  Query plans are also not eligible for caching if they contain one of the\n  following execution node types:\n  - SingleRemoteOperationNode (cluster only)\n  - MultipleRemoteModificationNode (cluster only)\n  - UpsertNode, i.e. the AQL UPSERT functionality.\n\n  If a query is eligible for plan caching, the plan cache will be checked using\n  the exact same query string and set of collection bind parameter values.\n  A cached plan entry is only considered identical to the current query if the\n  query strings are bytewise identical and the set of collection bind \n  parameters is exactly the same (bind parameter names and bind parameter \n  values).\n  If no plan entry can be found in the plan cache, the query is planned and \n  optimized as usual, and the cached plan will be inserted into the plan cache.\n  Repeated executions of the same query (same query string and using the same \n  set of collection bind parameters) will then make use of the cached plan\n  entry, potentially with different value bind parameters.\n\n  The following query options will be ignored when a cached plan is used:\n  - `joinStrategyType`\n  - `maxDNFConditionMembers`\n  - `maxNodesPerCallstack`\n\n  Whenever a query uses a cached plan from the plan cache, the query\n  result will include an attribute `planCacheKey` on the top level when\n  executing, explaining or profiling a query. The explain and profiling\n  output will also display a \"plan cache key: ...\" indicator to show\n  that a cached query plan was used.\n\n  The query plan cache is organized per database. It gets invalidated at the\n  following events:\n  - an existing collection gets dropped or renamed, or the properties of an\n    existing collection are modified\n  - an index is added to an existing collection or an index is dropped\n  - an existing view gets dropped or renamed, or the properties of an existing\n    view are modified\n  - a named graph is added, or an existing named graph gets dropped\n\n  Memory usage of the query plan cache can be restricted using the following\n  startup options:\n  - `--query.plan-cache-max-entries`: maximum number of plans in the query plan\n    cache in each database. The default value is 128.\n  - `--query.plan-cache-max-memory-usage`: maximum total memory usage for the\n    query plan cache in each database. The default value is 8MB.\n  - `--query.plan-cache-max-entry-size`: maximum size of an individual entry\n    in the query plan cache in each database. The default value is 2MB.\n  Note that each database has its own query plan cache, and that these options\n  are used for each individual plan cache. Also note that in a cluster, each\n  coordinator will have its own query plan cache.\n\n  There are also new APIs to clear the contents of the query plan cache and to\n  retrieve the current plan cache entries. The following HTTP REST APIs exist:\n  - HTTP DELETE `/_api/query-plan-cache` to delete all entries in the query\n    plan cache for the current database. This requires write privileges for the\n    current database.\n  - HTTP GET `/_api/query-plan-cache` to retrieve all entries in the query\n    plan cache for the current database. This requires read privileges for the\n    current database. In addition, only those query plans will be returned for\n    which the current user has at least read permissions on all collections\n    and views included in the query.\n  In a cluster, these APIs are specific to the coordinator that they are run \n  on.\n\n  There is also a JavaScript module `@arangodb/query/plan-cache` that exposes\n  the same functionality:\n  - `require(\"@arangodb/aql/plan-cache`).clear()` to delete all entries in\n    the plan cache for the current database. This requires write privileges for\n    the current database.\n  - `require(\"@arangodb/aql/plan-cache`).toArray()` to retrieve all entries\n    in the plan cache for the current database. This requires read privileges\n    for the current database. In addition, only those query plans will be \n    returned for which the current user has at least read permissions on all\n    collections and views included in the query.\n  In a cluster, these APIs are specific to the coordinator that they are run \n  on.\n\n  The following metrics are exposed on a single server and each coordinator to\n  provide insights into how many query plans were served from the query plan\n  cache:\n  - `arangodb_aql_query_plan_cache_hits_total`: total number of plans looked\n    up and found in the query plan cache, across all database-specific plan\n    caches on the instance.\n  - `arangodb_aql_query_plan_cache_misses_total`: total number of plans looked\n    up and not found in the query plan cache, across all database-specific plan\n    caches on the instance.\n  - `arangodb_aql_query_plan_cache_memory_usage`: total current memory usage\n    of all query plan caches across all databases on this instance in bytes.\n\n* In replication use vpack as transport format throughout, so that even\n  large and strangely encoded numbers are transported faithfully.\n\n* BTS-2014: Fix delay if write hits early after a hotbackup restore.\n\n* Fix leader resignation race in coordinator, which lead to forgotten\n  collection read locks on dbservers, which in turn could lead to deadlocks\n  in the cluster.\n\n* Fix shard synchronisation race where after a shard move the new\n  leader informs followers before it updates Current in the Agency. In\n  some cases the old leader fell subsequently out of sync.\n\n* Fix a crash with async prefetch and user defined functions in AQL. This\n  fixes BTS-2003.\n\n* FE-466: use @arangodb/ui library.\n\n* Update tzdata as of 31.10.2024.\n\n* Bring unicode collation back to the exact 3.11 behaviour by shipping the \n  original 3.11 tables in a icudtl_legacy.dat file. This fixes corruption\n  bugs which could happen on upgrade to 3.12 because funny international\n  strings are indexed in VPack indexes.\n\n* Implements a faster encoding of doubles to a memcmp format. Makes it a cmake option.\n  Makes sure that it does not produce different values as the old/slower implementation.\n  Removes unneeded db._explain in a test\n\n* Improve the observability of asynchronous operations by saving all ongoing\n  asynchronous operations (associated with async<T> and Future<T>) in a registry.\n  A REST call gives information about all these operations via a list of all\n  ongoing stacktraces.\n\n\n3.12.3 (2024-10-25)\n-------------------\n\n* Add missing accounting for document lookups in JOIN nodes when in late\n  materialize mode.\n\n* Speed up shortest path further by using a neighbour cache in the\n  SingleServerProvider.\n\n* FE-446: add missing caching options for inverted indexes and arangosearch\n  views.\n\n* Speed up shortest path computations by choosing more diligently which side to\n  expand next. This brings back 3.10 performance.\n\n* Fixed a bug in (non-Yen) k-shortest-paths which could overlook some paths.\n\n* BTS_1792: Call `setLocale` earlier during startup to prevent crashes.\n\n* BTS-1975: Fix the output of dumped data in VPack format.\n\n* Add file descriptors to tele metrics\n\n* Bind variables for TTL deletions should not be moved when now reused.\n\n* Removed the deprecated Batch API (`/_api/batch`).\n  The arangobench `--batch-size` startup option is ignored now.\n\n* BTS-1950: geo index query stuck\n  The geo index iterator got stuck when multiple coverings were requested\n  multiple time. The corresponding flag was not resetted resulting in an endless\n  loop.\n\n* Allow to use `/_api/wal/tail` with VelocyPack response and change cluster\n  internal replication to use it. This has advantages in performance and can\n  replicate any VelocyPack value faithfully.\n\n* Change dumping of VPack to JSON for large double values with absolute value\n  between 2^53 and 2^64. This ensures that dumps to JSON followed by parsing of\n  the JSON back to VPack retain the right numerical values.\n  This fixes a problem in arangodump/arangorestore as well as in replication.\n  Furthermore, JSON output from the API now shows numbers in this area with\n  their correct numerical value.\n\n* FE-465: handle HiddenIndex type & \"fields\" object in collection index page.\n\n* Updated ArangoDB Starter to v0.19.6.\n\n* Upgraded OpenSSL to 3.3.2.\n\n* In replication use vpack as transport format throughout, so that even large\n  and strangely encoded numbers are transported faithfully.\n\n* The optimizer was very pessimistic about sparse mdi indexes and did not\n  properly react to queries that excluded null values.\n\n* Fix blockage in RestMetricsHandlers if there are resigned leaders. We use\n  getResponsibleServerNoDelay there to decide if metrics should be exposed.\n\n* Fix blockage in SynchronizeShard: We do some agency communication there, this\n  should use skipScheduler to avoid being blocked by all scheduler threads being\n  busy. This solves a problem found in RTA on upgrade with resignLeadership.\n\n* Fix bug in upstream rocksdb to not leak file descriptors when iouring is used\n  for RocksDB.\n\n* Fix CircleCI setup w.r.t. enterprise branch.\n\n* Adjust documentation and testing of --cluster.force-one-shard option to\n  reality.\n\n* Rebuilt included rclone v1.62.2 with go1.22.6 and non-vulnerable dependencies.\n\n* Add native support for strict ranges for MDI.\n\n* FE-463: Query UI view crashes for empty collections.\n\n* FE-462: saved queries view scrollbar is not visible.\n\n* Remove restriction that disabled async prefetching for all query nodes that\n  (indirectly) depended on a RemoteNode. This allows more query parts to run\n  concurrently.\n\n* Extend COLLECT optimizer rule to consider existing SORT nodes that cover all\n  group variables. Previously the following query would use a hash collect\n  instead of a sorted collect:\n  ```\n  FOR d IN docs\n    SORT d.value DESC\n    COLLECT v = d.value\n    RETURN v\n  ```\n  The reason is that previously the rule unconditionally created a SORT node\n  with all group variables sorted in ascending order, but because that resulted\n  in a plan with two different SORT nodes which obviously is more expensive than\n  the hash collect, which was thus preferred.\n  The sorted collect doesn't need the values to be sorted in a particular order,\n  it just needs the values to be grouped correctly. So the new version now\n  checks if there are existing SORT nodes that already cover all the group\n  values, and in that case does not create an additional SORT node. That way\n  we can also utilize descending SORTs as in the example above, and perhaps even\n  utilize an index if we have one.\n\n* FE-422: Permissions of users managed by ArangoGraph are now shown as\n  read-only.\n\n\n3.12.2 (2024-08-21)\n\n* FE-450: avoid scrollbars when license warning is visible.\n\n* Add API GET /_admin/cluster/vpackSortMigration/status to query the status of\n  the vpack sorting migration on dbservers, single servers and coordinators.\n\n* Use default language en_US (instead of en_US_POSIX as before in 3.12), if no\n  LANGUAGE file is found and none of the locale environment variables LANG,\n  LANGUAGE, LC_ALL and LC_COLLATE are set. This ensures more compatibility with\n  3.11. This fixes BTS-1941.\n\n* FE-461: fix JSON/table detection, disable CSV download when not a table.\n\n* FE-442: fix user avatar on user table.\n\n* FE-453: Upgrade arangojs version used in web UI.\n\n* FE-460: Query editor UI - fix buttons overlap when 'Options' tab is open.\n\n* Updated ArangoDB Starter to v0.19.4.\n\n* We already had the concept of individual log appenders (e.g., stdout, stderr,\n  files, syslog). but log levels could only be configured globally. That is,\n  all appenders always used the same log levels for all topics.\n\n  This PR introduces appender specific log levels. For example, it is now\n  possible to set the log levels for all topics to `error`, but leave the log\n  levels for the log file unchanged. This is particularly useful for test runs\n  because we avoid flooding the console with log messages, but still have all\n  the information in the log files in case of test failures.\n\n  The log level can be specified per appender via the command line by adding a\n  comma separated list of topics with their respective level after the output\n  definition, separated by a semicolon:\n    `--log.output file:///path/to/file;queries=trace,requests=info`\n    `--log.output -;all=error`\n\n  The `_admin/log/level` endpoint has also been extended to with an optional\n  parameter `withAppenders`. If that parameter is set to `true`, the response\n  has the following form:\n  ```\n  {\n    \"global\": {\n      <topic>: <level>,\n      ...\n    },\n    \"appenders\": {\n      <appender>: {\n        <topic>: <level>,\n        ...\n      },\n      ...\n    }\n  }\n  ```\n  A `PUT` request to `_admin/log/level` with `withAppenders` set to true\n  requires an input of the same format.\n\n  Previously, requests to the `_admin/log/level` endpoint where not fully\n  validated. A `PUT`' request that specified invalid topics or log levels\n  would silently ignore those. This has been improved, so that invalid input\n  is now always rejected with a proper error message - in that case _no_\n  changes are applied at all.\n\n  The global level for some topic is always identical to the highest log level\n  for that topic of all appenders. Thus, changing the log level for an appender\n  will implicitly also adjust the global level accordingly. When a global level\n  is set explicitly, this change is also applied to all appenders.\n\n* FE-459: fix failing foxx service download.\n\n* Fixed BTS-1742: `IN_RANGE` function now uses MDI index if possible.\n\n* FE-456: upgrade web UI to React 18.\n\n* FE-449: write concern is no longer set when creating a sattelite database\n\n* Add \"useCache\" option for graph nodes:\n  The \"useCache\" AQL option is now supported for the following node types in\n  addition:\n  - TraversalNode\n  - ShortestPathNode\n  - EnumeratePathsNode (K_SHORTEST_PATHS, K_PATHS, ALL_SHORTEST_PATHS)\n  The option controls whether the in-memory cache for edges is used for\n  the graph operation. It defaults to `true`. It can be set to `false` to\n  disable usage of the in-memory edge cache for the graph operation in\n  question.\n\n  Example:\n\n      FOR v, e, p IN 1..2 OUTBOUND 'vertex/123' edges OPTIONS {useCache:false}\n        ...\n\n* Fixed MDS-1225: reloading of AQL user-defined functions inside AQL queries \n  could cause trouble if the `_aqlfunctions` collection is located on different\n  DB server than the leader shards of other collections used in the same query.\n\n* Fix comparison of numeric values in AQL to bring it in line with the \n  now correct VPack numerical sorting.\n\n* Include LANGUAGE file in hotbackups. This is necessary to be able to detect\n  locale changes across a hotbackup create/restore process.\n\n* Fix sorting behaviour of VelocyPack values w.r.t. numbers. This has an\n  impact on indexes indexing VPackValues. Therefore, after an upgrade the \n  old sorting order will be retained to allow smooth upgrades. Newly started\n  instances with a fresh database directory will only use the new sorting\n  method. There is also a migration API under \n  GET /_admin/cluster/vpackSortMigration/check and \n  PUT /_admin/cluster/vpackSortMigration/migrate to check for problematic\n  indexes and - provided there are none - to migrate the instance to \n  the new sorting order.\n\n* Forcefully kill all running AQL queries on server shutdown.\n  This allows for a faster server shutdown even if there are some long-running\n  AQL queries ongoing.\n\n* Optional logging of AQL query metadata into `_queries` system collection.\n\n  There is now a functionality to optionally log metadata of finished AQL\n  queries into a system collection for later analysis. The query metadata\n  includes the following attributes:\n  - `id`: internal ID of the query\n  - `database`: name of the database the query ran in\n  - `user`: name of the user that executed the query\n  - `query`: query string\n  - `bindVars`: bind parameters values used by the query\n  - `dataSources`: array of collection names / view names that were used in the\n    query.\n  - `started`: date/time the query started executing\n  - `runTime`: total runtime of the query, in seconds\n  - `state`: the state of the query (will always be \"finished\")\n  - `stream`: whether or not the query was a streaming AQL query\n  - `modificationQuery`: whether or not the query was a modification query\n  - `warnings`: number of warnings issued by the query\n  - `exitCode`: exit code of the query (0 = success, any other exit code \n    indicates a specific error)\n\n  Note the following restrictions for the above attributes:\n  - `query` will only contain the query string if the startup option\n    `--query.tracking-with-querystring` is set to `true`. This is the default.\n    The query string will also be cut off after \n    `--query.max-artifact-log-length` characters.\n  - `bindVars` will only contain data if the startup option\n    `--query.tracking-with-bindvars` is set to `true`. This is the default.\n  - `dataSources` will only contain data if the startup option\n    `--query.tracking-with-datasources` is set to `true`. This is not the \n    default.\n  \n  Only finished queries will be logged to the `_queries` system collection.\n  Queries are considered \"finished\" in case they have executed completely\n  or have failed with an error. In-flight queries will not be logged, but\n  should eventually become finished queries.\n\n  Queries will only be logged to the `_queries` system collection if the\n  startup option `--query.collection-logger-enabled` is set to `true`.\n  Additionally, queries in the `_system` database are only logged if the\n  option `--query.collection-logger-include-system-database` is also set to\n  `true`.\n  The option `--query.collection-logger-probability` can be used to specify a\n  probability for logging queries to the system collection. If set to a value\n  of `100`, all executed queries will be logged. If set to a value less than\n  `100`, only the specified percentage of queries will be logged. This can be\n  used to draw a random sample from the total set of queries on very busy \n  systems on which tracking every individual query would have prohibitive\n  overhead. For example, using `--query.collection-logger-probability=1` will\n  log approximately every 100th query to the system collection, and ignore all\n  others. \n  Which exact queries are logged and which aren't is based on randomness.\n  The startup option `--query.collection-logger-all-slow-queries` can be used\n  to make all slow queries be logged, regardless of whether they were selected\n  for sampling or not. The slow query time threshold is configurable as usual\n  via the already existing startup options `--query.slow-threshold` and\n  `--query.slow-streaming-threshold`.\n\n  Query metadata is stored in the `_queries` collection in the `_system`\n  database. This collection will be created automatically when needed. The\n  collection is a normal system collection, so it can be queried by everyone\n  with at least read access to the `_system` database and the `_queries`\n  collection. Keep this in mind when enabling the query logging, as the data\n  may include sensitive information, such as query strings and/or user names.\n  Query metadata can be retrieved by running an AQL query on the `_queries`\n  collection with filters and sort clauses of choice, e.g.\n\n      FOR doc IN queries\n        FILTER doc.runTime >= 10.0\n        FILTER doc.user == @user\n        SORT doc.started\n        RETURN doc\n\n      FOR doc IN queries\n        FILTER doc.started >= @start\n        FILTER doc.started < @end\n        COLLECT db = doc.database, user = doc.user\n        WITH COUNT INTO count\n        RETURN {db, user, count}\n\n  When query logging is enabled, obsolete entries from the `_queries` \n  collection are purged with a configurable schedule. The startup option\n  `--query.collection-logger-retention-time` is used to determine how long\n  after a query's start time it will be approximately retained in the system\n  collection. For example, `--query.collection-logger-retention-time=86400`\n  will keep query information around for approximately 1 day (86400 seconds).\n  The default retention time is 28800 seconds, i.e. 8 hours.\n  The actual cleanup of the system collection is only executed with a \n  configurable interval as well, so that the cleanup process does not cause\n  much traffic. The cleanup interval can be controlled by setting the option\n  `--query.collection-logger-cleanup-interval`. The value is in milliseconds.\n\n  The option `--query.collection-logger-push-interval` can be used to set a\n  maximum wait time after which queries are logged to the system collection.\n  This is a performance optimization, which helps to reduce the overhead of\n  query logging. For example, `--query.collection-logger-push-interval=10000`\n  will buffer queries in memory for at most 10000 milliseconds before they\n  are actually written to the system collection. When additional queries\n  arrive within this interval, they will be batched together into a single\n  write operation to the system collection. This can amortize the cost of\n  writing the query metadata to the system collection across multiple user \n  queries.\n\n  The option `--query.collection-logger-max-buffered-queries` can be used to\n  limit the amount of query metadata buffered in memory before it is flushed\n  to the system collection. Once this limit has been reached, no further\n  query metadata will be buffered in memory and will be lost.\n  To make this relatively unlikely, a flush is triggered automatically once\n  a fourth of the maximum queries limit is reached. It is still possible\n  however that the single-thread flush operation cannot keep up with the\n  rate of incoming queries, so that the limit is reached and some query\n  metadata are not logged.\n\n  Any queries that are buffered in memory and are not yet flushed out to the\n  system collection will be lost upon an instance shutdown or crash.\n  This means that the query logging functionality should not be used for\n  auditing, but rather for debugging and troubleshooting query issues, such\n  as finding long-running queries, queries that produced warnings or errors,\n  users that overuse the database etc.\n\n* FE-457: fix query UI result table headers & table/geo detection.\n\n* Add optimizer rule `push-limit-into-index` that pushes a limit down into index\n  iteration and can optimize the query execution when compound index is used\n  along with SORT and LIMIT clauses.\n\n* FE-423: Add a progress bar to reflect an index creation process.\n\n* Remove unused \"mmap\" log topic.\n\n* Support array and object destructuring for FOR loops that refer to simple\n  expressions or collections.\n\n  The following is now possible to write in AQL:\n\n      FOR {value1, value2} IN collection\n        FILTER value1 == @value1 && value2 == @value2\n        RETURN value1\n\n  This destructures every single object from `collection` and extracts its\n  `value1` and `value2` attributes. It is equivalent to writing:\n\n      FOR doc IN collection\n        FILTER doc.value1 == @value1 && doc.value2 == @value2\n        RETURN doc.value1\n\n  It is also possible to destructure array inputs into separate variables.\n  For example, the following query\n\n      FOR doc IN collection \n        FOR [key, value] IN ENTRIES(doc) \n        RETURN {key, value}\n\n  can be used to destructure all members from the expression `ENTRIES(doc)` and\n  assign the first member to the variable `key`, and the second member to the\n  variable `value`.\n\n  This is equivalent to writing:\n\n      FOR doc IN collection \n        FOR entry IN ENTRIES(doc)\n          LET key = entry[0]\n          LET value = entry[1]\n          RETURN {key, value}\n\n  Accessing an undefined object attribute or an undefined array members results\n  in a value of `null` being produced. Applying destructuring on any\n  incompatible input types produces `null` as well.\n\n  Destructuring is not supported for view-based FOR loops, for traversal FOR\n  loops, SHORTEST_PATH FOR loops, K_PATHS FOR loops, K_SHORTEST_PATHS FOR loops\n  nor ALL_SHORTEST_PATHS FOR loops.\n\n* Make constrained heap sort in AQL report all filtered rows as \"filtered\" in\n  the query profile.\n\n* Adjust our usage of `_FORTIFY_SOURCE` in maintainer mode:\n  - use `_FORTIFY_SOURCE` compile definition also when building with clang.\n    Previously it was only used when compiling with gcc.\n  - use `_FORTIFY_SOURCE=3`, which adds runtime memory safety checks (and some\n    runtime overhead). Previously we used `_FORTIFY_SOURCE=2`.\n  This only affects maintainer mode builds. Non-maintainer-mode builds are not\n  affected, as `_FORTIFY_SOURCE` is not used on them.\n\n* Added array and object destructuring to AQL LET statements.\n\n  - Array Destructuring: Array destructuring is the assignment of array values\n    into one or multiple variables with a single `LET` assignment. This can be\n    achieved by putting the target assignment variables of the `LET` assignment\n    into angular brackets.\n\n    For example, the statement:\n\n        LET [y, z] = [1, 2]\n\n    will assign the value `1` to variable `y` and the value `2` to variable `z`.\n    The mapping of input array members to the target variables is positional.\n\n    It is also possible to skip over unneeded array values. The following\n    assignment will assign the value `2` to variable `y` and the value `3` to\n    variable `z`. The array member with value `1` will not be assigned to any\n    variable:\n\n        LET [, y, z] = [1, 2, 3]\n\n    When there are more variables assigned than there are array members, the\n    target variables that are mapped to non-existing array members are populated\n    with a value of `null`. The assigned target variables will also receive a\n    value of `null` when the destructuring is used on a non-array.\n\n    Destructuring can also be applied on nested arrays, e.g.\n\n        LET [[sub1], [sub2, sub3]] = [[\"foo\", \"bar\"], [1, 2, 3]]\n\n    In the above example, the value of variable `sub1` will be `foo`, the value\n    of variable `sub2` will be `1` and the value of variable `sub3` will be `2`.\n\n  - Object Destructuring: Object destructuring is the assignment of multiple\n    target variables from a source object value. This is achieved by using the\n    curly brackets after the `LET` assignment token:\n\n        LET { name, age } = { valid: true, age: 39, name: \"John Doe\" }\n\n    The mapping of input object members to the target variables is by name.\n    In the above example, the variable `name` will get a value of `John Doe`,\n    the variable `age` will get a value of `39`. The attribute `valid` from the\n    source object will be ignored.\n\n    Object destructuring also works with nested objects, e.g.\n\n        LET { name: {first, last} } =\n        { name: { first: \"John\", middle: \"J\", last: \"Doe\" } }\n\n    The above statement will assign the value `John` to the variable `first` and\n    the value `Doe` to the variable `last`. The attribute `middle` from the\n    source object is ignored.\n    Note that here only variables `first` and `last` will be populated, but\n    variable `name` is not.\n\n    It is also possible for the target variable to get a different name than in\n    the source object, e.g.\n\n        LET { name: {first: firstName, last: lastName} } =\n        { name: { first: \"John\", last: \"Doe\" } }\n\n    The above statement assigns the value `John` to the target variable\n    `firstName` and the value `Doe` to the target variable `lastName`. Note that\n    neither of these attributes exist in the source object.\n\n* Added `verifyCertificates` attribute option for the `requests` JavaScript\n  module. This option defaults to false, so that no certificates will be \n  verified in an HTTPS connection made with the `requests` module. If the\n  option is set to true, the server certificate of the remote server will be\n  verified using the default certificate store of the system.\n  There is also a `verifyDepth` attribute to limit the maximum length of the \n  certificate chain that counts as valid.\n\n\n3.12.1 (2024-07-21)\n-------------------\n\n* Fix an \"invalid document type\" exception when using the AQL ENTRIES function\n  on a database document or edge.\n\n* Fixed MDS-1216: restoring the previous value of the \"padded\" key generator\n  could lead to the key generator's sequence being set to a too low value after\n  recovery.\n\n* Updated ArangoDB Starter to v0.19.3.\n\n* Fix a nullptr access when aborting a move shard job for a already dropped\n  collection.\n\n* Use __nss_configure_lookup to opt out of /etc/nsswitch.conf.\n  Add the startup option --honor-nsswitch to cancel the opt-out.\n\n* Fix memory accounting in join executor.\n\n* Fix issues with late materialization being sometimes implicitly dependent on\n  the optimizer rule \"optimize-projections\" being enabled. Now disable some\n  late materialization optimizations in case this rule is not active.\n  Also fix a specific case in which no results were being materialized in case\n  the optimizer rule \"optimize-projections\" was disabled.\n\n* Fix serialization of multiple query plans using the `allPlans: true` hint\n  when run `explain` on a query.\n\n* Support indexHint AQL option for traversals.\n  The structure of traversal index hints is as follows:\n\n      { collection: { direction: { level: indexes } } }\n\n  with the following meanings:\n  - collection: the name of an edge collection for which the index hint should\n    be applied. Proper capitalization of the collection name is important.\n  - direction:  the direction for which to apply the index hint. Valid values\n    are `inbound` and `outbound`, in lowercase.\n  - level: the level/depth for which the index should be applied. Valid level\n    values are the string `base` and any stringified numeric values greater or\n    equal to zero.\n\n  Example:\n\n      FOR v, e, p IN 1..5 OUTBOUND startVertex edgeCollection\n      OPTIONS {\n        indexHint: {\n          \"edgeCollection\": {\n            \"outbound\": {\n              \"base\": [\"edge\"],\n              \"1\": \"myIndex1\",\n              \"2\": [\"myIndex2\", \"myIndex1\"],\n              \"3\": \"myIndex3\",\n            }\n          }\n        }\n      }\n      FILTER p.edges[1].foo == 'bar' &&\n             p.edges[2].foo == 'bar' &&\n             p.edges[2].baz == 'qux'\n\n  Because the collection names and levels/depths are used a object keys here, it\n  is useful to enclose them in quotes. Omitting the quotes around them may\n  result in parse errors for the query.\n\n  Note that index hints for levels other than `base` will only be considered\n  if the traversal actually uses a specific filter condition for the specified\n  level. In the above example, this is true for levels 1 and 2, but not for\n  level 3. Consequently, the index hint for level 3 will be ignored here.\n\n  Currently index hints cannot be forced to be used for traversals, i.e. the\n  `forceIndexHint` option is currently not supported for traversals.\n\n* Update timezone database as of 25.06.2024.\n\n* Rebuilt included rclone v1.65.2 with go1.22.4.\n\n* Swagger UI: Remove fragment identifiers from request URLs. They are merely\n  used to disambiguate polymorphic endpoints in the OpenAPI descriptions of the\n  HTTP API documentation.\n\n* BTS-1909, MDS-1232: Fixed a bug where COLLECT ... AGGREGATE x = UNIQUE(y)\n  could miss some results when multiple shards were aggregated in a cluster.\n\n* Fixed an issue where the request would hang when retrieving the result of an\n  asynchronous HEAD request. Now, the response body is cleared and the\n  Content-Length header is set to 0, ensuring compliance with the HTTP protocol\n  and preventing indefinite client waits.\n\n* Document modification operations (INSERT, UPDATE, REPLACE, REMOVE, and UPSERT)\n  during an AQL query on DBServers no longer block threads while waiting for\n  replication.\n\n* Fixed a bug where accessing a collection right after creation can sometimes\n  fail.\n\n* OASIS-25850: Fix incomplete replacing of projection variables in traversals.\n  This fixes a regression introduced in 3.12.0.\n\n* FE-454: Fix saving document in \"Tree\" mode.\n\n* FE-310: Query editor in web UI can now be resized vertically.\n\n* Fix a potential data corruption in a collection's Merkle tree, in case a write\n  operation in a streaming transaction ran into the transaction's maximum size\n  limit and failed. If the error was ignored and the transaction committed, the\n  leftovers of the failed operation were not removed from the collection's\n  in-memory Merkle tree buffer and thus be committed as well.\n  This could later cause issues with shards not getting properly in sync.\n\n* Fix the handling of View objects in AQL template strings of the JavaScript API\n  so that the behavior matches collection objects:\n\n      let view = db._view(\"myView\");\n      aql`FOR doc IN ${view} RETURN doc`\n\n* FE-451: update graph UI defaults for edge direction, edge/collection labels &\n  colors.\n\n* Fix possible crashes or UB during some document operations on single- or\n  DBServers.\n\n* Upgrade OpenSSL to 3.3.1 and used glibc build runtime to 2.39.0.\n\n* Updated ArangoDB Starter to v0.19.2.\n\n* Avoid dropping of followers in case a leader resigns and then comes back.\n\n* Fixed MDS-1229 (bad_function_call with batch materialization).\n\n* Make the `beginAsync()` operation for transactions fully async also in the\n  coordinator case, where it could previously block.\n\n* Optimize performance of in-memory cache lookups under contention.\n\n* Reduce the possibility of cache stampedes during collection count cache\n  refilling. Additionally, do not block or delay a foreground operation with\n  refilling the collection count cache value from the DB-Servers. Instead the\n  refilling is executed eventually via the scheduler in the background.\n\n* Move resolution of replication callbacks on leader DB-Servers to the\n  scheduler's HIGH prio lane. The HIGH lane is justified here because the\n  callback resolution must make progress, as it can unblock another thread\n  waiting in an AQL write query for the replication to return.\n\n* Skip the scheduler for AQL query shutdown on coordinators.\n  During query shutdown, we definitely want to skip the scheduler, as in some\n  cases the thread that orders the query shutdown can be blocked and needs to\n  wait synchronously until the shutdown requests have been responded to.\n\n* Fixed MDS-1230: added missing variable replacement for inline filters of\n  EnumerateListNodes.\n\n* Fixed BTS-1813: Fixed an AQL error that could lead to range-check exceptions\n  in certain queries with subqueries.\n\n* Move network retry requests to a dedicated thread.\n\n* Reduce frequency in which progress of hot backup upload / download progress is\n  reported to the agency. This limits traffic from DB servers to the agency\n  during hot backup uploads and downloads.\n\n* Pass all relevant compiler optimization flags down to relevant CMake sub\n  projects which are built using configure/make.\n\n* Added ENTRIES function, that takes an object and produces a list of key-value\n  pairs. The order of pairs is unspecified.\n\n* Prevent spurious rclone errors in S3.\n\n* Use posix_spawn instead of fork/exec for subprocesses. This solves a\n  performance issue during hotbackup upload.\n\n* Added new REST API endpoint HTTP DELETE `/_admin/log/level` to reset all log\n  levels to their startup values. The log levels will be reset to their factory\n  defaults unless they were overridden via a configuration file or command-line\n  options. In this case, the log levels will be reset to the value they were\n  configured to at startup.\n  All modifications to the log levels since the instance startup will be lost\n  when calling this API.\n\n  This API is useful for tools that temporarily change log levels but do not\n  want to fetch and remember the previous log levels settings. Such tools can\n  now simply call the new API to restore the original log levels.\n\n* Fixed OASIS-25823: when late materialization was used together with skipping\n  over documents or the `fullCount` operation, it was possible for AQL queries\n  to fail with errors such as `bad_function_call`.\n\n* Fixed the listDatabases API: Directly after adding a new empty DBServer, or\n  after restoring a Hotbackup the listDatabases() would return an empty list of\n  existing databases, which goes back to normal quickly. This bug only effected\n  the APIs exposing the list of database names, all databases in fact still\n  exist and are fully functional.\n\n* Fix a potential race in collectRebalanceInformation.\n\n* FE-447: fix query spotlight search, make it case-insensitive.\n\n* FE-443: Fix inconsistent tooltip padding in Create Index dialog.\n\n* Fix potentially hanging threads during index creation if starting one of the\n  parallel index creation threads returned an error.\n\n* Fix connection retry attempts for cluster-internal TLS connections that ran\n  into the 15 seconds timeout during the connection establishing attempt.\n  In this case, the low-level socket was repurposed, but not reset properly.\n  This could leave the connection in an improper state and lead to callbacks for\n  some requests to not being called as expected.\n  The connection timeout was also increased from 15 seconds to 60 seconds.\n\n* FE-448: auto-repair collection document JSON on save.\n\n* Improved the time required to create a new Collection in a database with\n  hundreds of collections. This also improves times for indexes and dropping of\n  collections.\n\n* Added startup option `--cache.max-cache-value-size` to limit the maximum size\n  of individual values stored in the in-memory cache used for\n\n  - edge indexes\n  - persistent indexes with caching enabled\n  - collections with document caches enabled\n\n  The startup option defaults to 4MB, meaning that no individual values larger\n  than 4MB will be stored in the in-memory cache in these areas.\n\n  Limiting the size of cache values helps to avoid storing very large values in\n  the cache for huge documents or for all the connections of super nodes, and\n  thus leaves more memory for other purposes.\n\n* BTS-1856: Fix a data-race inside the Query class during accesses on a private\n  property (`execStats`).\n\n* FE-432: allow row clicks on all UI tables.\n\n* FE-441: Fix collection unstable default sort in multi-shard collections UI.\n\n* Changed default value of `--log.max-queued-entries` from 10000 to 16384.\n\n* Added metric `arangodb_logger_messages_dropped_total` to count the number of\n  log messages that were discarded by the logger if its log queue was full.\n\n* Added option `skipFastLockRound` for transactions.\n  The option can be set in a streaming write transaction's options (i.e.\n  `{\"skipFastLockRound\":true}`) to disable the fast lock round inside the\n  transaction. The option defaults to `false, so the fast lock round is tried\n  where possible.\n  Disabling the fast lock round is normally not necessary, but it can make sense\n  when there are many concurrent streaming transactions queued that all try to\n  access the same collection and lock it exclusively. In this case, the fast\n  lock round actually makes things worse, because there will always be some\n  exclusive locks in the way. In this use case it will be better to skip the\n  fast lock round. Disabling the fast mode makes each actual locking operation\n  take longer than when in fast mode, but it guarantees a deterministic locking\n  order and thus avoids deadlocking and retrying that is possible with the fast\n  mode. Overall disabling the fast lock mode can be faster in this case.\n  Setting this option is irrelevant for read-only transactions and will degrade\n  performance when there are no concurrent transactions that use exclusive locks\n  on the same collection.\n\n* Fixed SEARCH-485 (ES-2010): Search files disappeared during creation of\n  HotBackup.\n\n* Bundle ICU library version 64 with the build, so that persistent indexes and\n  views relying on the old sort order can still be used without problems.\n  The prevent issues with RocksDB going into read-only mode after an upgrade\n  from 3.11, if it encounters persistent index values in an unexpected sort\n  order. RocksDB's error message in this case was \"Corruption: Compaction sees \n  out-of-order keys\".\n  The out-of-order keys were caused by a changed sort order for some Unicode\n  characters. The newer ICU 73 version that ArangoDB 3.12 bundles includes the\n  most recent changes to the Unicode standard that have been accepted until\n  recently.\n  This leads to problems, because old versions of ArangoDB were using ICU\n  version 64, which was based on an older version of the Unicode standard, and\n  ArangoDB versions from 3.12 onwards were using ICU 73, which implements a very\n  recent version of the Unicode standard.\n  For example, in the Unicode standard implemented by ICU 64, the ASCII\n  apostrophe character \\u0027 compares lower than the Unicode apostrophe\n  character \\u2019. In ICU 73 both characters compare equal.\n  The sort order for existing on-disk indexes however must not change, because\n  RocksDB relies on the index files being sorted according to a non-changing\n  sort order.\n  ArangoDB from 3.12.0.2 onwards will thus bundle ICU versions 64 and 73, so\n  that indexes in RocksDB and Arangosearch views will continue to use ICU\n  version 64 as they did in previous versions. The V8 engine relies on newer\n  versions of ICU and thus will use ICU version 73.\n\n* Make AQL query optimizer create less query execution plan permutations when a\n  query contains multiple occurrences of COLLECT statements under certain\n  conditions:\n  - COLLECT statements that are contained in a single loop that is driven by a\n    traversal or an enumeration over an array now prefer the hashed collect\n    variant.\n  - if the hashed collect variant is explicitly requested via the `OPTIONS`\n    clause of the COLLECT statement (i.e. COLLECT ... OPTIONS {method: \"hash\"})\n    then the hashed variant is now always used, even if the maximum number of\n    query execution plans is already reached. Previously only the sorted variant\n    would be used despite the explicit `OPTIONS` hint.\n\n* Fix an issue that can cause AQL related RestHandlers to wind up in a WAITING\n  state and never get woken up. This implies that the associated query snippet\n  and transaction cannot be released. If the query contains modification\n  operations, this also implies that the associated collection write lock is not\n  released and can therefore prevent other threads from acquiring the exclusive\n  lock (which is required by e.g., the replication).\n\n* Fix an issue that can cause some background load in parallel traversals while\n  waiting for data from upstream.\n  For full details see https://github.com/arangodb/arangodb/pull/20768.\n\n* BTS-1848: Issue with Collection Names Containing Non-ASCII Characters in Web\n  UI.\n\n* Upgrade bundled version of nghttp2 library to version 1.61.0.\n\n* Added internal APIs to dump the contents of the AQL query registry from\n  DB-Servers plus the contents of the transaction manager on coordinators and\n  DB-Servers.\n\n* Prioritize requests for commiting or aborting streaming transactions on\n  leaders and followers, because they can unblock other operations. Also\n  prioritize requests in already started streaming transactions and AQL queries\n  over new requests because it is assumed that already started streaming\n  transactions and AQL queries can block other operations from starting, so\n  these should be completed first.\n\n  The following request priorities have changed:\n  - cluster-internal requests for continuing already started AQL queries have\n    changed their priority from low to medium. This excludes requests that setup\n    AQL queries on DB servers, which still run with low priority.\n  - requests that include the transaction id header are now elevated to run with\n    medium priority if they originally ran with low priority. This excludes\n    requests that begin new transactions or AQL queries.\n  - requests to commit or abort an already running streaming transaction will be\n    elevated from low to medium priority.\n  - requests to HTTP GET `/_api/collection/<collection>/shards` and GET\n    `/_api/collection/<collection>/reponsibleShard` are now running with high\n    priority instead of low. Such requests are only used for inspection and have\n    no dependencies.\n  - requests to push further an existing query cursor via the `/_api/cursor` are\n    now running with medium priority instead of low priority. Requests to start\n    new queries still run with low priority.\n  - follower requests that acquire the lock on the leader while the follower\n    tries to get in sync are now running with low instead of medium priority.\n\n* Obsolete startup option `--server.max-number-detached-threads`.\n  This option is now obsolete because coroutines are used instead of blocking\n  function calls.\n\n* Log an info message if a request was queued in the scheduler queue for 30s or\n  longer.\n\n* Add upgrade task to drop `_pregel_queries` system collections.\n\n* Fixed an upgrade issue on Agents. When agents are started with\n  `--database.auto-upgrade true` and `--cluster.force-one-shard true`. The\n  upgrade procedure got into an incorrect state. Please note\n  `--cluster.force-one-shard` is not required on the Agent instances.\n\n* Fix potential deadlocks in \"fast lock round\" when starting transactions in\n  cluster. The \"fast lock round\" is used to send out transaction begin requests\n  to multiple leaders concurrently, without caring about the order in which\n  requests are send out. This can potentially lead to deadlock with other\n  out-of-order requests. Thus the intention of the \"fast lock round\" is to use\n  a very low timeout for these requests, so that deadlocks would be detected and\n  rolled back quickly. However, there was a code path that triggered the \"fast\n  lock round\" with long request timeouts, which could lead to long wait times\n  until deadlocks were detected and rolled back.\n\n* EE-Only bug-fix MDS-1190: When using arangorestore to restore a SmartGraph,\n  the restore process may generate new internal collection ids.\n  These generated ids were not always updated in the restored SmartGraph schema.\n  If the ids diverge, the restored SmartGraph still works as expected, all data\n  is there and can be queried and modified. Only the schema of the restored\n  SmartGraph could not be changed anymore, e.g. no collections could be added to\n  or removed from the graph.\n  The SmartGraph schema handling now does not consider collection ids anymore,\n  so that schemas can again be modified, even those of existing SmartGraphs that\n  were originally affected by the issue.\n\n* FE-316: Edge collection import example now correctly reflects the required\n  `from` and `to` attributes.\n\n* FE-437: reinitialize analyzer form when type changes.\n\n* Remove \"read lock\" acquisition calls from shard synchronization because they\n  are now unnecessary. This slightly simplifies the shard synchronization\n  protocol.\n\n* Repair shard rebalancer if some server has no leaders of a collection but\n  followers. Previously, this server was then not always considered for leader\n  movements.\n\n* Change the default log format. The option log.thread is now by default true.\n  The option log.time-format is now by default `utc-datestring-micros`.\n\n* Guarantee conditional-only evaluation of the operands of the AQL ternary\n  operator and the logical AND and OR operators, if any of the operands is a\n  subquery.\n\n  Given an AQL ternary condition such as\n\n      condition ? truePart : falsePart\n\n  the `truePart` expression is now executed only if `condition` is truthy, and\n  the `falsePart` is evaluated only if `condition` is falsy.\n  In previous versions of ArangoDB the `truePart` and `falsePart` operands of\n  the ternary operator were evaluated eagerly and out-of-line if the operands\n  were subqueries.\n\n  The ternary operator's condition is now pulled out and made available under a\n  separate temporary variable. For example, the query\n\n      RETURN condition ? truePart : falsePart\n\n  will internally be rewritten as\n\n      LET tmp1 = condition\n      RETURN tmp1 ? truePart : falsePart\n\n  In case the operands are subqueries and must be executed conditionally, the\n  query is evaluated as\n\n      LET tmp1 = condition\n      LET tmp2 = (\n        FILTER tmp1\n        truePart\n      )\n      LET tmp3 = (\n        FILTER ! tmp1\n        falsePart\n      )\n      RETURN tmp1 ? tmp2 : tmp3\n\n  This guarantees that the condition is only evaluated once, and that the\n  `truePart` and `falsePart` operands are only executed if `condition` was\n  truthy or false, respectively.\n\n  Also only evaluate the right-hand operands of logical AND and logical OR if\n  the left-hand operand evalutes to a truthy value (logical AND) or a falsy\n  value (logical OR).\n\n* Allow limiting the amount of async prefetching operations for AQL queries.\n  Since version 3.12, AQL queries can make use of async prefetching. While this\n  is normally beneficial for query performance, the async prefetch operations\n  may cause congestion in the ArangoDB scheduler and interfere with other\n  operations.\n  In order to control the maximum amount of prefetch operations, the following\n  startup options have been added:\n  \n  - `--query.max-total-async-prefetch-slots`: the maximum total number of slots\n    available for asynchronous prefetching, across all AQL queries.\n  - `--query.max-query-async-prefetch-slots`: the maximum per-query number of\n    slots available for asynchronous prefetching inside any AQL query.\n\n  That means the total number of concurrent prefetch operations across all AQL\n  queries can be limited using the first option, and the maximum number of\n  prefetch operations in every single AQL query can be capped with the second\n  option. These options prevent that running a lot of AQL queries with async\n  prefetching fully congests the scheduler queue, and also they prevent large\n  AQL queries to use up all async prefetching capacity on their own.\n\n* Fix BTS-1807: transactions are bound to the user who started it, and the\n  database they were started for. However, there was a code path that was missing\n  the database check, which allowed the transaction id to be used in a request\n  to a different database. The request would not return an error, but it would\n  actually be executed in the context of the original database, and not the one\n  specified in the request.\n  With this fix, such a request will now return an ERROR_TRANSACTION_NOT_FOUND.\n\n\n3.12.0 (2024-03-21)\n-------------------\n\n* Revert arangodump exclusion the following cluster collection properties when\n  writing collection parameters files in a single server dump:\n  - replicationFactor\n  - writeConcern\n  - numberOfShards\n  - minReplicationFactor\n  - shardKeys\n  - shards\n\n* Accept language settings found in the LANGUAGE file which previous versions\n  have detected for the \"C\" locale. This is to allow upgrades from 3.11 if the\n  database directory was created with the \"C\" locale.\n\n* FE-437: reinitialize analyzer form when type changes.\n\n\n3.12.0-rc.1 (2024-03-12)\n------------------------\n\n* Rebuilt included rclone v1.65.2 with go1.21.8.\n\n* Repair shard rebalancer if some server has no leaders of a collection but\n  followers. Previously, this server was then not always considered for leader\n  movements.\n\n* Updated ArangoDB Starter to v0.19.0.\n\n\n3.12.0-beta.1 (2024-03-07)\n--------------------------\n\n* Updated ArangoDB Starter to v0.19.0-preview-1.\n\n\n3.12.0-alpha.1 (2024-03-06)\n---------------------------\n\n* Updated ArangoDB Starter to v0.18.3.\n\n* FE-436: rename `multi_delimiter` analyzer property to `delimiters` on web UI.\n\n* FE-424: Adjusted Foxx service install button placement.\n\n* Add startup progress reporting for RocksDB .sst file checksum computation.\n\n* Upgrade OpenSSL to 3.2.1.\n\n* Remove arangosync binary.\n\n* Updated rclone to v1.65.2 (built with go1.21.6).\n\n* FE-426: Better error messages for Foxx installation failures.\n\n  When installing Foxx services from the web UI, the error messages for failures\n  or timeouts during the initial download of the service bundle now provide more\n  information about the underlying cause.\n\n  Additionally the timeout threshold for the initial download of Foxx service\n  bundles from remote sources has been increased from 10 seconds to 60 seconds\n  for both the web UI and the Foxx HTTP API.\n\n* Added startup options `--network.compression-method` and\n  `--network.compress-request-threshold` to optionally compress relevant\n  cluster-internal traffic.\n\n  If `--network.compression-method` is set to 'none', then no compression will\n  be performed. To enable compression for cluster-internal requests, set this\n  option to either 'deflate', 'gzip', 'lz4' or 'auto'.\n  The 'deflate' and 'gzip' compression methods are general purpose, but can have\n  significant CPU overhead for performing the compression work.\n  The 'lz4' compression method compresses slightly worse, but has a lot lower\n  CPU overhead for performing the compression.\n  The 'auto' compression method will use 'deflate' by default, and 'lz4' for\n  requests which have a size that is at least 3 times the configured threshold\n  size.\n  The compression method only matters if `--network.compress-request-threshold`\n  is set to value greater than zero. This option configures a threshold value\n  from which on the outgoing requests will be compressed. If the threshold is\n  set to value of 0, then no compression will be performed. If the threshold is\n  set to a value greater than 0, then the size of the request body will be\n  compared against the threshold value, and compression will happen if the\n  uncompressed request body size exceeds the threshold value.\n  The threshold can thus be used to avoid futile compression attempts for too\n  small requests.\n\n  Compression for all agency traffic is turned off regardless of the settings of\n  these options.\n  This is important so that the agent instances do not get overwhelmed by the\n  potential extra CPU utilization caused by compression/decompression.\n\n* Save some traffic in cluster-internal requests:\n\n  - Remove disk usage metrics from heartbeat messages (`freeDiskSpaceBytes` and\n    `freeDiskSpacePercent`), as these are not processed by the agency\n    supervision at all.\n  - Do not return `id` and `endpoint` attributes in responses to empty\n    appendEntries requests in the agency, as these attributes are not picked up\n    by the response message handler.\n  - Do not include `x-arango-source` HTTP header in any outgoing requests that\n    the agency makes to other servers.\n  - Do not include `x-arango-hlc` HTTP header in heartbeat messages to the\n    agency's transient store.\n  - Do not return the following HTTP response headers in cluster-internal\n    traffic:\n    - `x-arango-queue-time-seconds`\n    - `x-content-type-options`\n    - `content-security-policy`\n    - `cache-control`\n    - `pragma`\n    - `expires`\n    - `strict-transport-security`\n    - `server`\n\n    The following responses **do not count** as cluster internal traffic, so\n    that the mentioned HTTP response headers are fully retained for them:\n    - responses by single server instances.\n    - responses by coordinators if the `x-arango-source` HTTP header was not set\n      in the request.\n\n    The following responses count as cluster-internal traffic, so the HTTP\n    response headers will be removed:\n    - all responses by agents and DB servers, regardless from where the requests\n      were initiated.\n    - all responses from coordinators, if the `x-arango-source` HTTP header was\n      set in the request and is set to the id of a coordinator or DB server.\n\n* FE-409: add deletion success notifications in views on web UI.\n\n* FE-320: Database name maximum length validation during input.\n\n* BTS-1733: prevent dropping of collection if the collection is part of an\n  existing graph.\n  This PR prevents the unintentional dropping of collections that are part of\n  any existing graph, so that graph definitions remain intact even if the user\n  attempts to drop a member collection.\n  This introduces a change in behavior: while previously it was allowed to drop\n  collections that were part of an existing graph, trying to do so will now\n  result in error `ERROR_GRAPH_MUST_NOT_DROP_COLLECTION` (error code `1942`\n  and error message `must not drop collection while part of graph` with more\n  specifics following).\n  This may require a change in client application code that drops individual\n  collections from graphs in order to clean up. The recommended way to drop\n  these collections is to drop the graph instead. Dropping the graph can\n  optionally also delete all associated collections of the graph. Even if this\n  flag is not used, collections that are not part of a graph definition anymore\n  can later be dropped regularly.\n\n* Allow changing writeConcern of dependent collections.\n\n  Previously changing the `writeConcern` property of collections that also have\n  their `distributeShardsLike` property set was disallowed. This is now allowed.\n\n* FE-425: adds missing Analyzer types on web UI - \"multi_delimiter\" and\n  \"wildcard\".\n\n* BTS-1660: When updating (PATCH request) edges with API Gharial and only\n  specifying one of the two edge attributes `_from` or `_to` (not both), the\n  content of the edge was not fully validated. This has been fixed now.\n\n* BTS-1661: In addition to BTS-1259: For some operations where a collection is\n  being used, but not known by the graph definition itself, we've introduced a\n  new, more precise error, called:\n  `ERROR_GRAPH_COLLECTION_NOT_PART_OF_THE_GRAPH`.\n  This error will be reported in case a collection is being used, but not part\n  of the graph definition at all. Whereas the\n  `ERROR_GRAPH_REFERENCED_VERTEX_COLLECTION_NOT_USED` will be thrown in such\n  cases like a not known vertex is being used inside an edge documents `_from`\n  or `_to` attribute.\n\n* APM-665: RocksDB memory tracking\n  Changed the default value of the following command line options\n\n  - `--rocksdb.reserve-table-builder-memory`\n  - `--rocksdb.reserve-table-reader-memory`\n  - `--rocksdb.reserve-file-metadata-memory`\n\n  from `false` to `true` by default to account for the memory usage of the above\n  RocksDB components in the RocksDB block cache and thus the metric\n  `rocksdb_block_cache_usage`. This also allows to better limit the total memory\n  usage of RocksDB.\n\n* Index progress visibility for indexes, which are created in background.\n\n* Added support for simple external versioning to document operations.\n\n  UPDATE and REPLACE operations can now be given an optional `versionAttribute`\n  property. If set, the attribute with the name specified by the property is\n  looked up in the to-be-updated/to-be-replaced document. If no such attribute\n  exists, the UPDATE/REPLACE is performed as usual. If such attribute exists,\n  its content is read and compared numerically to the value of the versioning\n  attribute in the document that updates/replaces it. If the version number in\n  the new document is higher than in the document that exists in the database,\n  the UPDATE/REPLACE is performed normally. If the version number in the new\n  document is lower or equal to what exists in the database, the UPDATE/REPLACE\n  is not performed and behaves like a no-op. No error will be returned to the\n  user in this case.\n\n  This simple versioning can help to avoid overwriting existing data with older\n  versions in case data is transferred from an external system into ArangoDB and\n  the copies are currently not in sync.\n\n  The `versionAttribute` property can also be used for INSERT operations with\n  `overwriteMode: \"update\"` or `overwriteMode: \"replace\"`.\n  It can also be used inside AQL queries by specifying it in the `OPTIONS`\n  clause of an UPDATE/REPLACE/INSERT operation.\n\n  Examples:\n\n      // insert normally\n      db.collection.insert({_key: \"test\", value: 1, ver: 1});\n\n      // updates because version attribute value is higher in new document\n      db.collection.update(\"test\",\n                           {value: 2, ver: 2},\n                           {versionAttribute: \"ver\"});\n\n      // does not update because version attribute value is lower in new\n      //document\n      db.collection.update(\"test\",\n                           {value: 3, ver: 1},\n                           {versionAttribute: \"ver\"});\n\n      // updates because key already exists and version attribute value is\n      //higher in new document\n      db.collection.insert({_key: \"test\", value: 4, ver: 3},\n                           {overwriteMode: \"update\", versionAttribute: \"ver\"});\n\n      db._query(\"UPDATE 'test' WITH {value: 5, ver: 4} IN collection\n                OPTIONS {versionAttribute: 'ver'}\");\n\n  Note that versioning is opt-in, and that no version checking is performed for\n  operations for which the `versionAttribute` property was not set as part of\n  the update or replace operation or as an option in the AQL query.\n\n  Also note that version checking only kicks in if both the existing version of\n  the document in the database and the new document version contain the version\n  attribute and they contain numeric values between 0 and 18446744073709551615.\n  If either the existing document in the database or the new document version do\n  not contain the version attribute, or if the version attribute in any of the\n  two is not a number inside the valid range, the update/replace operation will\n  behave as if no version checking was requested.\n\n  Also note that document removal operations do not support versioning. Removal\n  operations are always carried out normally without checking the version\n  attribute, even if it is specified.\n\n* FE-349: show full error message on web UI when moving shards.\n\n* Upgraded bundled version of V8 to 12.1.165.\n\n* Upgraded bundled version of ICU to 73.\n\n* FE-296: Update shard distribution UI.\n\n* FE-367: ability to hide columns in table web UI.\n\n* Bug-Fix: Also fix the copy (mentioned directly below here) in the ArangoShell.\n\n* Bug-Fix: Fixes the unnecessary copy of the `node_modules` folder located in\n  the frontend directory. This will also improve the startup speed of the V8\n  feature.\n\n* No longer use getpwuid, getpwnam, getgrgid and getgrnam on Linux to avoid\n  crashes due to /etc/nsswitch.conf on older Linux distributions.\n\n* Escape control characters (e.g. newlines) in the query strings that are logged\n  in an instance's audit log.\n\n* FE-377: remove broken link from JSON editor.\n\n* Expose REST API endpoints for server options discovery:\n  - GET `/_admin/options-description`: returns a JSON description of all\n    available server options.\n  - GET `/_admin/options`: returns a JSON object with the currently set server\n    options.\n\n  As these API may reveal sensitive data about the deployment, they can only be\n  accessed from inside the `_system` database. In addition, there is a policy\n  control startup option `--server.options-api` that determines if and to whom\n  the APIs are made available. This option can have the following values:\n  - `disabled`: options APIs is disabled.\n  - `jwt`: options APIs can only be accessed via superuser JWT.\n  - `admin`: options APIs can be accessed by admin users in the `_system`\n    database only.\n  - `public`: everyone with access to `_system` database can access the options\n    APIs.\n\n* Only schedule a single compaction per RocksDB column family at any given time.\n  Scheduling mutliple concurrent compactions on the same column family can lead\n  to scheduler threads being stalled under some adverse conditions that lead to\n  a stall inside RocksDB.\n\n* Added REST API `/_api/key-generators` to query the available key generators\n  for collections.\n\n* Make AQL optimizer rule `move-filters-into-enumerate` also move filters into\n  EnumerateListNodes for early pruning. Previously it could only pull filters\n  into IndexNodes and EnumerateCollectionNodes.\n\n* Validate that smartgraph attribute exists and is not changed. Previously this\n  check was performed in the KeyGenerator and therefore only performed for\n  inserts, but not for replace or update operations. Therefore replace or update\n  operations could modify or even remove this attribute.\n  In addition, we now return the error code `ERROR_NO_SMART_GRAPH_ATTRIBUTE`\n  instead of `ERROR_KEY_MUST_BE_PREFIXED_WITH_SMART_GRAPH_ATTRIBUTE` if the\n  attribute is not set.\n\n* Fix BTS-1636: Under certain circumstances the path found by a weighted\n  shortest path search was not the path of smallest weight.\n\n* Added CMake option `USE_JEMALLOC_CHECKS` to toggle the usage of extra safety\n  checks in jemalloc. The option is currently enabled by default, but can be\n  turned off when there are performance concerns.\n\n* Fixed BTS-1703: Cannot chain multiple UPSERTS.\n\n* Added AQL function `TO_CHAR(number)`, which produces a character from the\n  given character number.\n\n* Added AQL function `REPEAT(string, count, separator)`, which repeats a string\n  for a configurable number of times, with an optional separator.\n  Note that the maximum size of the output string is limited to 16 MB to avoid\n  very large outputs that would consume lots of memory. Exceeding the maximum\n  output string length will make the function return `null` and issue a warning.\n\n* Fixed issue #17673: Adds timezone conversion as an optional parameter in all\n  relevant date functions.\n\n* Activate more checks in jemalloc:\n    --enable-opt-safety-checks\n    --enable-opt-size-checks\n  This is supposed to help find bugs w.r.t. memory manangement.\n\n* FE-406: update docs links on web UI. \n\n* Added an optimizer rule `optimize-enumerate-paths` that pulls filters of the\n  form\n   FILTER pathVar.vertices[* RETURN ...] ALL == value\n  or\n   FILTER pathVar.edges[* RETURN ...] ALL == value\n\n  inside path searches (ALL_SHORTEST_PATHS, K_SHORTEST_PATHS, K_PATHS) into the\n  path search itself. this should lead to better path searches if the filter is\n  selective enough.\n\n* BTS-1633: One Shard databases would incorrectly not be indicated as being One\n  Shard databases in the new database view in the frontend.\n\n* Added option `--http.handle-content-encoding-for-unauthenticated-requests` for\n  arangod. If set to `true`, the arangod server will automatically uncompress\n  incoming HTTP requests with Content-Encodings gzip and deflate even if the\n  request is not authenticated.\n  If the option is set to `false`, any unauthenticated request that has a\n  Content-Encoding header set will be rejected. This is the default setting.\n\n* Added options `--compress-transfer` and `--compress-request-threshold` to all\n  client tools.\n  These options can be used to enable transparent compression of the data that\n  is sent between the client tools and the ArangoDB server.\n\n  If the option `--compress-transfer` is set to `true`, the client-tools will\n  add an extra HTTP header `Accept-Encoding: deflate` to all requests they make\n  to the server. This allows the server to compress its responses before sending\n  them back to the client tools.\n\n  If the option is set to `true`, the client will also transparently compress\n  their own requests to the server if the size of the request body (in bytes)\n  is at least the value of the startup option `--compress-request-treshold`.\n  The default value for this option is `0`, which disables compression of the\n  request bodies in the client tools. To opt in to sending compressed data, the\n  option must be set to a value greater than zero.\n  Request body compression will be performed using the deflate compression\n  algorithm. The client tools will also add a `Content-Encoding: deflate` header\n  to the request when the request body was compressed.\n  As compression will use CPU cycles, it should be activated only in case the\n  network communication between the client tools and the server is slow and\n  there is enough CPU capacity left for the extra compression/decompression\n  work. Furthermore, requests should only be compressed when they exceed a\n  certain minimum size, e.g. 250 bytes.\n  Too small requests are typically not compressible very well.\n\n* Added startup option `--http.compress-response-threshold` to the ArangoDB\n  server. The value of this specifies the threshold value (in bytes) from which\n  on response bodies will be sent out compressed by the server.\n\n  The default value for this option is `0`, which disables sending out\n  compressed response bodies. To enable compression, the option should be set to\n  a value greater than 0. The selected value should be large enough to justify\n  the compression overhead. Too small response bodies are typically not well\n  compressible.\n\n  Regardless of the value of this option, response body compression will only\n  happen in case the client advertised that it expects a compressed response\n  body by sending an `Accept-Encoding: gzip` or `Accept-Encoding: deflate`\n  header with its request. If that header is missing, no response compression\n  will be performed by the server.\n  As compression will use CPU cycles, it should be activated only in case the\n  network communication between the server and clients is slow and there is\n  enough CPU capacity left for the extra compression/decompression work.\n  work. Furthermore, responses should only be compressed when they exceed a\n  certain minimum size, e.g. 250 bytes. \n\n* Fixed BTS-1698: Inverted index can be hinted/forced for UPSERT, which is\n  invalid.\n\n* Issue a warning when an UPSERT is used on a collection with multiple shards or\n  on a single-shard collection when a DistributeNode is used. In these cases,\n  the UPSERT cannot guarantee it has observed its own writes. The warning can be\n  opted-out from explicitly by setting the `readOwnWrites` option of the UPSERT\n  operation to `false`.\n\n* Added \"readOwnWrites\" option for AQL UPSERT operations.\n\n  If set to `true` (which is the default), an UPSERT operation will process its\n  inputs one by one. That way an UPSERT can observe its own writes and can\n  handle modifying the same target document multiple times in the same query.\n\n  When the \"readOwnWrite\" option is set to `false`, an UPSERT can process its\n  input in batches (a batch is normally 1000 inputs), so it can execute a lot\n  faster. However, when using batches, an UPSERT cannot observe its own writes.\n  So the \"readOwnWrite\" option should only be set to `false` if it can be\n  guaranteed that the input to the UPSERT will lead to disjoint documents being\n  inserted/updated/replaced.\n\n* Optimize equality lookups and prefix matches that use the AQL like function.\n  The optimization will turn `LIKE(doc.attribute, 'fixed string')` into an\n  equality lookup, and `LIKE(doc.attribute, 'prefix%')` into a range query.\n  That allows indexes to be used in case they are present on the underlying\n  attribute.\n  The optimization is applied only if the second parameter to the `LIKE`\n  function is a fixed string and if third parameter to the `LIKE` function is\n  either not set or set to a value of `false`.\n\n* FE-388: Update UI colors & fonts to new styles.\n\n* Change default value for arangoimport's `--type` file type option from \"json\"\n  to \"auto\".\n  The default value of \"auto\" will guess the type of the import file based on\n  the file extension. The following file extensions will be automatically\n  detected:\n  - .json\n  - .jsonl\n  - .csv\n  - .tsv\n  If the file extension cannot be mapped to any of the above types, the import\n  will assume that the input is JSON.\n\n* BTS-1700: arangoimport parser should abort after a reasonable amount of parse\n  errors.\n\n  This introduces an option `--max-errors` for arangoimport. This option can be\n  used to limit the amount of error messages displayed by arangoimport, and to\n  abort the import after at least this many errors have occurred.\n\n  The default value for the option is 20, so that imports will quickly stop in\n  case many errors occur.\n  This is a behavior change compared to previous versions, in which the import\n  would continue even when there were many errors.\n  To achive the same behavior with the new version, the value of the\n  arangoimport startup option `--max-errors` can be set to a high value.\n\n* Make AQL optimizer rule \"interchange-adjacent-enumerations\" generate less plan\n  permutations for the case the a non-collection FOR loop is followed by a\n  collection-based FOR loop. This change can reduce the number of plans in\n  complex AQL queries, and thus can save time needed for plan optimization.\n\n* FE-372: Truncate long saved query names in UI.\n\n* EE Only: When creating a disjoint SmartGraph or a SmartJoin collection where\n  the list of shard names would go from a k digit number to a k+1 digit number,\n  some AQL queries would mismatch the shard list of one collection with the\n  others.\n  The data is all in the correct place, this is an issue only in read\n  operations.\n\n* Added metric `arangodb_aql_cursors_memory_usage` to show the memory usage of\n  open, not yet fully consumed server-side query cursors.\n\n* Added AQL functions `PARSE_COLLECTION` and `PARSE_KEY` to quickly extract the\n  collection name and document key from an `_id` value without much overhead.\n\n* FE-286: refactor user permissions screen.\n\n* FE-396: fix JSON editor undo for inverted index addition.\n\n* Support alternative syntax for UPSERT queries, so that arbitrary filter\n  conditions can be used. The alternative syntax is:\n\n      UPSERT FILTER <expression> \n      UPDATE|REPLACE ...\n\n  The key change here is that the UPSERT will produce a new variable `$CURRENT`\n  with the to-be-found document. This variable can be used in the following\n  filter condition to apply arbitrary filter conditions and not just equality\n  matches.\n\n* Added AQL statistics for additional document lookups. Also, the JoinNode now\n  exposes additional statistics:\n    - `scannedIndex`\n    - `filtered`\n    - `seeks`\n    - `documentLookups`\n\n* Updated jemalloc to the most recent available version of the code.\n  There has not been a release in a while, and this code fixes compile problems\n  using jemalloc with newer versions of clang in ArangoDB's release build\n  environment.\n\n* Fix propagation of collection `writeConcern` changes to DB servers.\n\n* Change default value of arangodump startup option `--use-parallel-dump` from\n  `false` to `true`.\n  This enables the more parallel dump procedure by default, which allows dumps\n  to be produced more quickly.\n  The parallel dump protocol needs server-side support, and the first ArangoDB\n  version that completely supports it is 3.12.\n  In order to use a 3.12 arangodump to create dumps from servers that run an\n  older version of arangod, the option should be set to `false` when invoking\n  arangodump.\n  The parallel dump option also allows the usage of the `--split-files` option\n  in arangodump. When enabled, the data of each collection/shard can be dumped\n  into multiple independent files, which can be written in parallel, and that\n  can also be restored in parallel.\n  Please note that you will need an arangorestore from version 3.12 or higher to\n  be able to restore a dump that was taken with the `--split-files` option.\n\n* BTS-1602: Added new flag `--ignore-collection` to the arangodump binary.\n  Collections can now be excluded by using this flag. This flag can be specified\n  multiple times to exclude multiple collections.\n\n* Added `--write-concern` parameter similar to `--replication-factor` to\n  arangorestore. This enables the user to override the write concern for one or\n  multiple collections.\n\n* BTS-1259: Not all CRUD requests based on vertex and edge operations using\n  `_api/gharial` performed a check against the actual graph definition. This\n  behavior is now fixed. In case a collection is being used but not known by the\n  graph definition itself, this will now properly be reported by either\n  `ERROR_GRAPH_REFERENCED_VERTEX_COLLECTION_NOT_USED`, in case of a vertex, or\n  by `ERROR_GRAPH_EDGE_COLLECTION_NOT_USED`, in case of an edge.\n    \n* Added metric `arangodb_aql_cursors_active` to show the number of active AQL\n  query cursors.\n\n* Add AQL optimizer rule \"async-prefetch\", which allows certain AQL query parts\n  to asynchronously prefetch data while other parts of the query are still\n  operating.\n  Several parts of the same AQL query operating concurrently can lead to query\n  performance improvements in case there is still reserve (scheduler) capacity.\n\n  The following parts of an AQL query can be executed in parallel to other\n  parts:\n\n  - CalculationNode\n  - CollectNode\n  - EnumerateCollectionNode\n  - EnumerateViewNode\n  - FilterNode\n  - IndexNode\n  - SortNode\n\n  Asynchronous prefetching is disabled for the following parts of an AQL query\n\n  - LimitNode: asynchronously prefetching data for a LimitNode must be disabled\n    because otherwise we may overfetch data and the LIMIT values could be\n    violated.\n  - NoResultsNode: the NoResultsNode produces an empty result, and is always\n    quick to execute. There is no need to make it fetch its dependent data\n    asynchronously.\n  - ReturnNode: the ReturnNode normally only passes through existing values and\n    should be quick to execute.\n  - SingletonNode: the SingletonNode only produces a single value and is always\n    quick to execute. It also doesn't have any dependencies.\n  - any nodes inside a subquery\n\n  If the following nodes occur inside an AQL query, asynchronous prefetching is\n  disabled for the entire query:\n\n  - InsertNode, UpdateNode, ReplaceNode, RemoveNode, UpsertNode: prefetching is\n    currently not compatible with these node types.\n  - EnumeratePathsNode, ShortestPathNode, TraversalNode: graph nodes currently\n    do not support prefetching in all cases, thus the functionality is turned\n    off.\n\n  AQL queries in which some nodes are eligible for asynchronous prefetching will\n  be marked with the optimizer rule \"async-prefetch\" having been applied.\n\n  The explain output for queries is now extended with a new \"Par\" column, which\n  shows which nodes in the query are eligible for asynchronous prefetching.\n  All nodes denoted with \"✓\" will support asynchronous prefetching at runtime.\n\n  The profiling output for queries is now extended with a new \"Par\" column,\n  which shows the actual number of parallel calls during query execution. Note\n  that every call to a query node will be counted in \"Calls\" in addition.\n  \"Calls\" contains the total number of calls made to the query node, and \"Par\"\n  contains the number of successful asynchronous prefetch calls.\n\n  Note that asynchronous prefetching is done on a best effort basis, but there\n  is no guarantee that query parts actually execute concurrently. The reason for\n  this is that asynchronous prefetching tasks are posted to the server's\n  scheduler with normal priority, so they compete for scheduler threads in the\n  same way as other operations do. The more capacity the server's scheduler\n  still has, the more likely it is that query parts can actually run\n  concurrently.\n\n* FE-385: fix web UI query import stuck on loading for faulty JSON.\n\n* FE-378: fix fields containing spaces during index creation on web UI.\n\n* FE-288: Improve collection index UI.\n\n* FE-321: add validation for query name in web UI.\n\n* Change default value for `--database.extended-names` from `false` to `true`.\n  This will allow end users to use Unicode characters inside database names,\n  collection names, view names and index names by default, unless the\n  functionality is explicitly turned off.\n\n  Note: once a server in the deployment has been started with the flag set to\n  `true`, it will store that setting permanently. Switching the startup option\n  back to `false` will raise a warning about the option change at startup, but\n  not block the startup. Existing databases, collections, views and indexes with\n  extended names can still be used even with the option set back to `false`, but\n  no new database objects with extended names can be created with the option set\n  back to `false`. So this state is only meant to facilitate downgrading or\n  reverting the option change. When the option is supposed to be left at a value\n  of `false`, all database objects with extended names that were created in the\n  meantime should be removed manually.\n\n* Allow the usage of the `_id` attribute in the stored values of persistent\n  indexes.\n\n* Improve performance of AQL LimitExecutor for higher limit values.\n  The LimitExecutor now fast-forwards all input rows instead of working on the\n  individual input rows.\n\n* Remove special handling for backups taken with ArangoDB 3.5 in hot backup\n  restore.\n\n* Extend the internal state of ReadWriteLocks from 32-bit to 64-bit to prevent\n  potential overflows. Previously creating more than 64k streaming transactions\n  at the same time could produce an overflow that would effectively result in\n  undefined behavior.\n\n* Remove \"database\" attribute from the serialized version of AQL IndexNodes,\n  EnumerateCollectionNodes, EnumerateViewNodes, GraphNodes and RemoteNodes.\n  That attribute was never read back.\n\n  Also removed the \"reverse\" attribute from the serialized version of AQL\n  IndexNodes, because it was also not read back except from tests.\n\n* FE-376: Fix Query UI crash on null values.\n\n* FE-375: Adds document counts to collection info screen.\n\n* Make replication WAL tailing schedule prefetch requests via the global\n  scheduler, in the same way as the initial dump phase does. In the best case,\n  this can speed up the getting-in-sync phase when getting shards in sync.\n\n* Fixed BTS-1239: Restoring a single server dump with system collections into a\n  cluster caused some of those system collections to lose the\n  `distributeShardsLike` setting and also changed the `shardingStrategy` to\n  `enterprise-compat`.\n  Now system collections in the cluster are always restored with\n  `distributeShardsLike` and `shardingStrategy` set to the correct default\n  values.\n\n* Added the following server startup options that can be used to limit\n  the resource usage of parallel arangodump invocations:\n\n  - `--dump.max-memory-usage`: Maximum memory usage (in bytes) to be used by the\n    server-side parts of all ongoing arangodump invocations.\n    This option can be used to limit the amount of memory for prefetching and\n    keeping results on the server side when arangodump is invoked  with the\n    `--parallel-dump` option. It does not have an effect for arangodump\n    invocations that did not use the `--parallel-dump` option.\n    Note that the memory usage limit is not exact and that it can be slightly\n    exceeded in some situations to guarantee progress.\n  - `--dump.max-docs-per-batch`: Maximum number of documents per batch that can\n    be used in a dump. If an arangodump invocation requests higher values than\n    configured here, the value will automatically be capped to this value. Will\n    only be followed for arangodump invocations that use the `--parallel-dump`\n    option.\n  - `--dump.max-batch-size`: Maximum batch size value (in bytes) that can be\n    used in a dump. If an arangodump invocation requests larger batch sizes than\n    configured here, the actual batch sizes will be capped to this value. Will\n    only be followed for arangodump invocations that use the `--parallel-dump`\n    option.\n  - `--dump.max-parallelism`: Maximum parallelism (number of server-side\n    threads) that can be used in a dump. If an arangodump invocation requests\n    a higher number of prefetch threads than configured here, the actual number\n    of server-side prefetch threads will be capped to this value. Will only be\n    followed for arangodump invocations that use the `--parallel-dump` option.\n\n* Added the following metrics to observe the behavior of parallel\n  arangodump on the server:\n\n  - `arangodb_dump_memory_usage`: Current memory usage of all ongoing arangodump\n    operations on the server.\n  - `arangodb_dump_ongoing`: Number of currently ongoing arangodump operations\n    on the server.\n  - `arangodb_dump_threads_blocked_total`: Number of times a server-side dump\n    thread was blocked because it honored the server-side memory limit for\n    dumps.\n\n* When dropping a database, immediately also mark all collections of the\n  database as dropped. This has the advantage that ongoing operations on these\n  collections can be stopped earlier, and memory for the underlying collections\n  and indexes can be reclaimed sooner.\n\n  This changes the behavior for some operations, for example:\n  ```\n  db._createDatabase(\"testDatabase\");\n  db._useDatabase(\"testDatabase\");\n\n  let c = db._create(\"testCollection\");\n\n  db._useDatabase(\"_system\");\n  db._dropDatabase(\"testDatabase\");\n\n  // the following operation will now return \"collection not found\".\n  // in previous versions, it could still be executed.\n  print(c.count());\n  ```\n\n  The new behavior is more consistent with dropping collections, after which\n  operations on the dropped collection also fail immediately.\n\n* Fixed BTS-1489: Race condition in AsioSocket shutdown when using SSL.\n\n* Added startup option `--transaction.streaming-max-transaction-size` to make\n  the maximum size of streaming transactions configurable.\n  Previously the maximum size of streaming transactions was hard-coded to 128\n  MiB. This value is also used as default value for the option now.\n\n* Track memory usage for write operations in AQL queries. The memory usage of\n  write operations in AQL queries was previously not tracked and not accounted\n  for in AQL queries' own memory usage. The memory used for write operations\n  also was not accounted for when setting a memory limit for the query. The\n  memory used for write operations is now properly tracked, including in-memory\n  write batches and locked keys in RocksDB (for conflict checking).\n  With that change, the memory usage reported for AQL queries that perform write\n  operations will be significantly higher than before, but the values now will\n  reflect the reality a lot better. This has the consequence that previously\n  working AQL write queries can now fail because their actual memory hits the\n  configured per-query or global memory limit threshold.\n  In these cases it is useful to either revisit the per-query and/or global\n  memory limit settings, turn on intermediate commits for the query, or adjust\n  the query so that it uses less memory.\n\n* Added metrics for tracking the memory usage of write operations outside AQL:\n  * `arangodb_transactions_rest_memory_usage`: memory of user-initiated write\n    operations.\n  * `arangodb_transactions_internal_memory_usage`: memory usage of internal\n    operations, such as background statistics, TTL index maintenance,\n    replication etc.\n\n* Fixed BTS-1531: Async cancel Job returns different error messages.\n  Unify error messages for canceled jobs (error code 21) to \"request canceled\".\n  Previously, the server could return different messages, either \"request\n  canceled\", \"request has been canceled by user\" or \"handler canceled\".\n\n* Fixed BTS-1543:\n  - client tools (e.g. arangoexport) don't refuse to start up with the error\n    `output directory ... is not empty, use --overwrite true to overwrite the\n    data in it if the output directory *only* contains the `ENCRYPTION` file.\n    This is a special case, which can easily occur when invoking any of the\n    client tools with wrong arguments. In this case no harm is done when\n    allowing the tool to run anyway, even without the `--overwrite true` flag.\n    This will potentially clobber the `ENCRYPTION` file, but it is safe to do\n    because we restrict this to the situation when the directory *only* contains\n    the `ENCRYPTION` file.\n  - allow invoking client tools and arangod with `https://`- and `http://`-\n    prefixed endpoints.\n    This is more user-friendly than the previously supported `ssl://`- and\n    `tcp://`-prefixed endpoints. End users are familiar with using `https://`\n    and `http://` than our `ssl://` or `tcp://` endpoints.\n    Internally, `https://` will be converted to `ssl://`, and `http://` will\n    be converted to `tcp://`.\n\n* Improve Pregel garbage collection for dropped databases.\n\n* Refuse server startup with outdated little-endian on-disk format.\n  The little-endian on-disk format was used for deployments that were created\n  with either ArangoDB 3.2 or 3.3 when using the RocksDB storage engine.\n  Since ArangoDB 3.4, a big-endian on-disk format was used for the RocksDB\n  storage engine, which is more performant.\n  Deployments that were set up with the RocksDB storage engine using ArangoDB\n  3.2 or 3.3 and that have been upgraded since then will still use the old\n  format. This should not affect many users, because the default storage engine\n  in ArangoDB 3.2 and 3.3 was the MMFiles storage engine. Furthermore,\n  deployments that have been recreated from an arangodump since ArangoDB 3.4\n  will not be affected, because restoring an arangodump into a fresh deployment\n  will also make ArangoDB use the big-endian on-disk format.\n\n  When the outdated little-endian on-disk format is detected at startup, the\n  server will bail out with a descriptive error message. The recommend way to\n  migrate the database from the little-endian storage format to the big-endian\n  storage format is to\n\n  1. create a full logical backup of the database using arangodump\n  2. stop the database servers in the deployment\n  3. wipe the existing database directories\n  4. restart the servers in the deployment\n  5. restore the logical dump into the deployment\n\n  It will not be sufficient to take a hot backup of a little-endian dataset and\n  restore it, because when restoring a hot backup, the original database format\n  will be restored as it was at time of the backup.\n\n* BTS-1380: Added traversal order (dfs, bfs, weighted) to explain output.\n\n* arangodump and arangorestore will now try to bump up the limit for the number\n  of open file descriptors at startup, on Linux and macOS. This way the tools\n  are allowed to open more files concurrently, which can be useful when\n  dump/restore with many threads.\n\n* Added startup option `--descriptors-minimum` to arangodump and arangorestore.\n  This option can be used to set a lower bound value for the open file\n  descriptors limit that is required to invoke arangodump/arangorestore.\n  Setting the option to a value of 0 makes arangodump/arangorestore start\n  regardless of the actual limit for the number of open file descriptors.\n\n  This option is available on Linux and macOS only.\n\n* Renamed arangodump startup option `--use-experimental-dump` to\n  `--parallel-dump`.\n\n* Improved experimental dump functionality in arangodump to work in single\n  server mode and to support transport compression and dumping velocypack data.\n\n* Obsolete the following options of arangodump:\n  - `--envelope`: setting this option to true previously wrapped every dumped\n    document into a {data, type} envelope. This was useful for the MMFiles\n    storage engine, where dumps could also include document removals. With the\n    RocksDB storage engine, the envelope only caused overhead and increased the\n    size of the dumps. The default value of `--envelope` was changed to `false`\n    in ArangoDB 3.9 already, so by default all arangodump invocations since then\n    created non-envelope dumps. With the option being removed now, all\n    arangodump invocations will unconditionally create non-envelope dumps.\n  - `--tick-start`: setting this option allowed to restrict the dumped data to\n    some time range with the MMFiles storage engine. It had no effect for the\n    RocksDB storage engine and so it is removed now.\n  - `--tick-end`: setting this option allowed to restrict the dumped data to\n    some time range with the MMFiles storage engine. It had no effect for the\n    RocksDB storage engine and so it is removed now.\n\n* Added experimental options to arangodump:\n  - `--compress-transfer`: setting this option enables gzip-compression for the\n    server responses to dump requests. Enabling it can significantly reduce\n    traffic volume from the server back to arangodump, in exchange for some CPU\n    overhead for compression on the server and decompression in arangodump.\n    The default value for this option is false. Setting this option only affects\n    the transfer, but not the on-disk format of dumps.\n  - `--dump-vpack`: setting this option will dump data in raw velocypack format.\n    This will make arangodump operate faster and create smaller dumps.\n    The default value for this option is false, as enabling it would\n    incompatibly change the on-disk format of dumps.\n\n* FE-300: Implement new Views list UI.\n\n* Fix a timeout discrepancy between HeartbeatThread and DBServerAgencySync\n  during shutdown.\n  The HeartbeatThread on DB servers waited for at most 65 seconds for the\n  DBServerAgencySync to complete, but DBServerAgencySync could make requests to\n  the agency with a timeout of 120 seconds.\n  The timeouts have now been unified, so that the request timeout is 60 seconds\n  and that the HeartbeatThread waits for at most 65 seconds (as before).\n\n* BTS-842: make the error message for edge document validation slightly clearer.\n\n* FE-248: Improve collections list UI.\n\n* BTS-1124: wait for license to be applied locally before responding to the\n  request.\n\n* BTS-1566: remove redundant responsibility for creating local databases on\n  coordinators.\n\n* Prepare internal NetwortkFeature to transparently handle gzip-encoded\n  responses.\n\n* FE-309: Store created_at timestamp for queries in web UI.\n\n* Remove attribute `dfdb` from response of storage engine API (GET\n  `/_api/engine`).\n\n* The storage engine API (GET `/_api/engine`) now returns an `endianness`\n  attribute for the RocksDB storage engine on single servers and database\n  servers.\n\n* Print arangod executable's build-id in startup and crash messages.\n\n* Removed --jslint from arangosh and scripts/jslint.sh along with the outdated\n  copy of eslint; updated CONTRIBUTING.md to reflect the changes and hint at\n  scripts/eslint.sh which can be used\tto lint javascript code with a locally\n  installed version of eslint\n\n* Fix Pregel running on OneShard deployments.\n\n* FE-94: Support for query options and optimizer rules on web UI.\n\n* FE-308: Support for graph editing on web UI.\n\n* FE-313: Add support for minHash analyzers & offset field on web UI.\n\n* Improve crash handler by adding the state of the application server (e.g.\n  startup, running, shutting down) into the crash message.\n\n* Remove long-deprecated graph compatibility functions, which were implemented\n  as JavaScript user-defined AQL functions since ArangoDB 3.0:\n\n  - arangodb::GRAPH_EDGES(...)\n  - arangodb::GRAPH_VERTICES(...)\n  - arangodb::GRAPH_NEIGHBORS(...)\n  - arangodb::GRAPH_COMMON_NEIGHBORS(...)\n  - arangodb::GRAPH_COMMON_PROPERTIES(...)\n  - arangodb::GRAPH_PATHS(...)\n  - arangodb::GRAPH_SHORTEST_PATH(...)\n  - arangodb::GRAPH_DISTANCE_TO(...)\n  - arangodb::GRAPH_ABSOLUTE_ECCENTRICTIY(...)\n  - arangodb::GRAPH_ECCENTRICTIY(...)\n  - arangodb::GRAPH_ABSOLUTE_CLOSENESS(...)\n  - arangodb::GRAPH_CLOSENESS(...)\n  - arangodb::GRAPH_ABSOLUTE_BETWEENNESS(...)\n  - arangodb::GRAPH_BETWEENNESS(...)\n  - arangodb::GRAPH_RADIUS(...)\n  - arangodb::GRAPH_DIAMETER(...)\n\n  These functions were only available previously after explicitly calling the\n  `_registerCompatibilityFunctions()` function from any of the JavaScript graph\n  modules. \n  The `_registerCompatibilityFunctions()` exports have also been removed from\n  the JavaScript graph modules.\n\n* Removed the following long-deprecated features for the HTTP server:\n\n  - overriding the HTTP method by setting one of the HTTP headers \n    - `x-http-method`\n    - `x-http-method-override`\n    - `x-method-override`\n    This functionality could previously be enabled by starting the server with\n    the startup option `--http.allow-method-override`.\n    The functionality has now been removed and setting the startup option does\n    nothing.\n\n  - optionally hiding ArangoDB's `server` response header. This functionality\n    could optionally be enabled by starting the server with the startup option\n    `--http.hide-product-header`.\n    The functionality has now been removed and setting the startup option does\n    nothing.\n\n* Fixed BTS-1302: make \"silent\" option for document operations work in cluster.\n  When using the \"silent\" options for document operations in the cluster, the\n  coordinator will now only return the errors that happened, as it did in single\n  server for a long time.\n  This is a bugfix and feature alignment, but still a downwards-incompatible\n  change compared to previous versions, where the \"silent\" option in document\n  operations had no effect in the cluster.\n\n  This change also unifies the behavior of batch document operations in single\n  server and cluster with the behavior of document operations for SmartGraph\n  edge collections:\n  previously, batch document operations in SmartGraph edge collections stopped\n  upon the first error, whereas they were (intentionally) continued for the\n  remaining operations in the batch in single server and cluster.\n\n  This is a downwards-incompatible change compared to previous versions, but it\n  can be considered a bugfix.\n\n* FE-291: Refactor Query screen on web UI to React.\n\n* FE-313: Upgrade arangojs to v8.x on web UI.\n\n* Expose the \"high water multiplier\" for the in-memory cache subsystem via a\n  new startup option `--cache.high-water-multiplier`.\n  The high water multiplier is used to calculate the effective memory usage\n  limit for the in-memory cache subsystem. The cache's configured memory usage\n  limit (`--cache.size`) is multiplied by the high water multiplier, and the\n  resulting value is used as the effective memory limit. It defaults to 56% of\n  the configured cache size.\n\n* Wrap imports into SmartGraph edge collections via the REST import API (POST\n  `/_api/import`) into a managed transaction, so that edge data is either\n  completely inserted or not at all.\n\n* Changed default values for the following options related to the in-memory\n  cache subsystem:\n\n  - `--cache.ideal-lower-fill-ratio` changed from 0.04 to 0.08\n  - `--cache.ideal-upper-fill-ratio` changed from 0.25 to 0.33\n\n  This allows slightly better hash table utilization and earlier memory\n  reclamation.\n\n* Renamed memory usage metrics for better naming consistency:\n  - `arangodb_internal_index_estimates_memory` was renamed to\n    `arangodb_index_estimates_memory_usage`\n  - `arangodb_scheduler_queue_memory` was renamed to\n    `arangodb_scheduler_queue_memory_usage`\n  - `arangodb_scheduler_stack_memory` was renamed to\n    `arangodb_scheduler_stack_memory_usage`\n  These metrics were introduced in 3.12, so no previous versions are affected.\n\n* FE-301: Unify Users View on web UI.\n\n* Added deprecation message for parts of createCollection API. So far this API\n  ignored unknown options, and therefore did not warn about typos. Furthermore\n  some combinations of values that are not sensible are accepted. In some future\n  version of ArangoDB those will be rejected by the API. The documented part of\n  the API does not change. Watch out for an error with logid: ee638, this\n  indicates you call deprecated parts.\n\n* Remove TTL and Observe/Unobserve from the Agency API.\n\n* FE-290: Unify Graphs View on web UI.\n\n* FE-289: Improve database table UI.\n\n* BTS-744: Disable development mode for system services.\n\n* Lower default value of `--query.log-memory-usage-threshold` from 4GB to 1GB.\n  This leads to queries that consume 1GB or more memory to be logged so that\n  they can be investigated and improved.\n\n* Set threshold for RocksDB automatic WAL file deletion by age from 30 days to\n  30 years.\n  This should effectively not change any behavior, as we don't rely on RocksDB's\n  built-in WAL file deletion mechanism. Instead, we tell RocksDB explicitly to\n  delete which WAL files, depending on what other functionality (e.g.\n  replication, index creation) still needs them or not.\n\n* Deprecate the startup option `--rocksdb.wal-archive-size-limit` because it can\n  be unsafe to use in environments that use replication (cluster, active\n  failover, single-to-single replication). When using the option and limiting\n  the total size of the WAL files in the archive, this may lead to still-needed\n  WAL files being pruned too early. This can have side effects, such as aborting\n  ongoing replication or index creation attempts.\n\n* Don't show \"write query options\" for AQL write queries in the explain and\n  profile output. The write query options normally don't produce any valuable\n  insights and only take up screen real estate.\n\n* If a startup option has a dynamic default value (e.g. if the default value\n  depends on the amount of available RAM), display a \"dynamic default\" hint when\n  invoking `--help`.\n\n* FE-229: Improve shard rebalancing UI.\n\n* FE-262: Improve Analyzer table UI.\n\n* FE-223: refactor arangosearch, handle nested fields special characters.\n\n* FE-166: Handle a space separated list to display multiple node labels in the\n  Graph Viewer.\n\n* FE-15: Removed green checkmark next to the version number and made keyboard\n  shortcuts button easier to discover.\n\n* BTS-846: Relaxed setup cost for calculation of default sort cost. There was a\n  hardcoded value of 100 being added to the sort cost that would be dominant\n  when there was a small amount of documents in the collection. Even if an index\n  would be the best candidate, if it doesn't cover a sort node present in a\n  query, it might end up not being chosen because the default sort cost would be\n  minimum 100 because of the former hardcoded value.\n\n* BTS-1396: Restricted followers from attempting to send telemetrics data to the\n  endpoint on active failover mode.\n\n* APM-671: index selectivity estimates (cuckoo estimator) memory tracking.\n  This adds a new metric \"arangodb_internal_index_estimates_memory\" that reports\n  the total memory usage of all index selectivity estimates combined.\n  The metric includes memory used for buffering index write operations that\n  still have to applied.\n\n* APM-663: revision tree memory tracking.\n  Tracking memory used for buffered updates to all revision trees in a new\n  metric `arangodb_revision_tree_buffered_memory_usage`.\n\n* Optimize hibernation and resurrection of revision trees so that there are no\n  recurring hibernations and resurrections of the same tree quickly after each\n  other. This is achieved by tracking the time of the last hibernation attempt\n  for each revision tree and not hibernating it if the last attempt was less\n  than a minute ago.\n\n* Removed the long-deprecated JavaScript-based traversal functionality,\n  including the following APIs:\n\n  - REST API endpoint `/_api/traversal`\n  - JavaScript module `@arangodb/graph/traversal`\n\n  The functionality provided by these APIs was deprecated and unmaintained for\n  many years. JavaScript-based traversals were replaced with AQL traversals in\n  ArangoDB version 3.0. Additionally, the JavaScript-based traversals could not\n  handle larger amounts of data and were thus very limited.\n\n* Updated bundled version of libunwind to 1.7-rc2.\n\n* Remove function `getFishbowlStorage()` from bundled JavaScript module\n  `@arangodb/foxx/manager`. This function was publicly exposed in previous\n  versions, and returned a handle to the `_fishbowl` system collection.\n  Applications should not have depended on it, because that collection was\n  only used to temporarily store the list of Foxx applications stored in a\n  central Github repository maintained by ArangoDB.\n\n* BTS-486: fishbowl collection creation interferes with hot backup.\n\n  Do not dynamically create the `_fishbowl` system collections anymore to store\n  the list of Foxx applications stored from a central Github repository\n  maintained by ArangoDB. Instead, store the list in memory.\n  This has the advantage that the `_fishbowl` system collection does not need to\n  be created on the fly, which could have interfered with backups running at the\n  very same time.\n  Additionally the previous `_fishbowl` collection was database-specific, so one\n  instance of the collection could be created per database in a deployment.\n  Now, the list of applications stored in memory is shared between all databases\n  in a deployment.\n\n* Fixed GitHub issue #17291: Fixed a server crash which could occur in case an\n  AQL query using a PRUNE or FILTER statement, combined with UDFs (user defined\n  functions), got executed.\n\n\nv3.11.8 (2024-02-22)\n--------------------\n\n* Updated arangosync to v2.19.6.\n\n* Updated OpenSSL to 3.0.13 and OpenLDAP to 2.6.7.\n\n* Updated ArangoDB Starter to v0.18.2.\n\n* Rebuilt included rclone v1.62.2 with go1.21.6.\n\n* ES-1892: Fix hot restores missing user defined analyzers.\n\n* Fix: we cannot update a link, but have to drop and recreate it. Until now,\n  this new index had the same set of labels as the old one. However, on\n  followers (and with replication2 also on the leader), the DropIndex and\n  EnsureIndex actions could run concurrently. I.e., we could try to create the\n  new index before the old one was fully removed. In this case we could get a\n  duplicate metric exception, preventing the index from being created. Such\n  errors are not really handled ATM - they are simply logged and otherwise\n  ignored. That means the index will simply not be available on the affected\n  server, since we will also do not retry to create it at a later time.\n  To avoid this, we add a new `indexId` label to the metric to make it unique.\n\n\nv3.11.7 (2024-01-29)\n--------------------\n\n* BTS-1751: Strange error message when executing a query while creating an index\n  in the background.\n\n* Reduce default amount of per-collection document removals by TTL background\n  index creation thread from 1m to 100k in each iteration. This change gives\n  other collections a chance of being cleaned up as well.\n\n* Do not make AQL query runtime timeout affect TTL index background thread\n  removal queries.\n\n* New API to show progress in background index creation.\n\n* Updated ArangoDB Starter to v0.18.0.\n\n* BTS-1741: fix updates of values in unique persistent indexes with stored\n  values defined for them. When such an index value was updated, it was\n  possible that the stored value was not correctly updated, so that subsequent\n  reads of the index value would run into exceptions such as \n  `Expecting type Array or Object`.\n\n* APM-828: Per collection/database/user monitoring.\n\n  This adds optional metrics for tracking per-shard requests on DB-Servers.\n  The exported metrics are:\n  - `arangodb_collection_leader_reads_total`: number of read requests on\n    leaders, per shard, and optionally also split by user.\n  - `arangodb_collection_leader_writes_total`: number of write requests on\n    leaders, per shard, and optionally also split by user.\n\n  The new startup option `--server.export-shard-usage-metrics` can be used to\n  opt in to these metrics. It can be set to one of the following values on\n  DB-Servers:\n  - `disabled`: no shard usage metrics are recorded nor exported. This is the\n    default value.\n  - `enabled-per-shard`: this will make DB-Servers collect per-shard usage\n    metrics.\n  - `enabled-per-shard-per-user`: this will make DB-Servers collect per-shard\n    and per-user metrics. This is more granular than `enabled-per-shard` but\n    can produce a lot of metrics.\n  \n  If enabled, the metrics are only exposed on DB servers and not on \n  Coordinators or single servers.\n\n  Whenever a shard is accessed in read or write mode by one of the following\n  operations, the metrics are populated dynamically, either with a per-user\n  label or not, depending on the above setting.\n  The metrics are retained in memory on DB-Servers. Removing databases,\n  collections or users that are already included in the metrics won't remove the\n  metrics for these databases, collections or users until the DB-Server is\n  restarted.\n\n  The following operations increase the metrics:\n  - AQL queries: an AQL query will increase the read or write counters exactly\n    once for each involved shard. For shards that are accessed in read/write\n    mode, only the write counter will be increased.\n  - Single-document insert, update, replace, and remove operations: for each\n    such operation, the write counter will be increased once for the affected\n    shard.\n  - Multi-document insert, update, replace, and remove operations: for each such\n    operation, the write counter will be increased once for each shard that is\n    affected by the operation. Note that this includes collection truncate\n    operations.\n  - Single- and multi-document read operations: for each such operation, the\n    read counter will be increased once for each shard that is affected by the\n    operation.\n\n  The metrics are increased when any of the above operations start, and they\n  are not decreased should an operation abort or if an operation does not\n  lead to any actual reads or writes.\n\n  Note that internal operations, such as internal queries executed for \n  statistics gathering, internal garbage collection, and TTL index cleanup are\n  not counted in these metrics. Additionally, all requests that use the \n  superuser JWT for authentication and that do not have a specific user set,\n  are not counted.\n  Requests are also only counted if they have an ArangoDB user associated with\n  them, so that the cluster must also be running with authentication turned on.\n\n  As there can be many of these dynamic metrics based on the number of shards\n  and/or users in the deployment, these metrics are turned off by default (see\n  above), and if turned on, are only exposed only via a new HTTP REST API\n  endpoint GET `/_admin/usage-metrics`. They are not exposed via the existing\n  metrics API endpoint GET `/_admin/metrics`.\n\n Add additional metrics for tracking the number of bytes read or written per\n  shard on DB-Servers:\n\n  - `arangodb_collection_requests_bytes_read_total`:\n    This metric exposes the per-shard number of bytes read by read operation \n    requests on DB-Servers. \n    It is increased by AQL queries that read documents or edges and for single-\n    or multi-document read operations.\n    The metric is normally increased only on the leader, but it can also \n    increase on followers if \"reads from followers\" are enabled.\n\n    For every read operation, the metric will be increased by the approximate \n    number of bytes read to retrieve the underlying document or edge data. This\n    is also true if a document or edge is served from an in-memory cache.\n    If an operation reads multiple documents/edges, it will increase the \n    counter multiple times, each time with the approximate number of bytes read\n    for the particular document/edge.\n    \n    The numbers reported by this metric normally relate to the cumulated sizes\n    of documents/edges read.\n    The metric is also increased for transactions that are started but later\n    aborted.\n    Note that the metric is not increased for secondary index point lookups or\n    scans, or for scans in a collection that iterate over documents but do not\n    read them.\n\n  - `arangodb_collection_requests_bytes_written_total`:\n    This metric exposes the per-shard number of bytes written by write operation\n    requests on DB-Servers, on both leaders and followers.\n    It is increased by AQL queries and single-/multi-document write operations.\n    The metric is first increased only the leader, but for every replication \n    request to followers it is also increased on followers.\n    \n    For every write operation, the metric will be increased by the approximate \n    number of bytes written for the document or edge in question. \n    If an operation writes multiple documents/edges, it will increase the \n    counter multiple times, each time with the approximate number of bytes \n    written for the particular document/edge.\n\n    An AQL query will also increase the counter for every document or edge \n    written, each time with the approximate number of bytes written for \n    document/edge.\n    \n    The numbers reported by this metric normally relate to the cumulated sizes\n    of documents/edges written. For remove operations however only a fixed\n    number of bytes is counted per removed document/edge. For truncate \n    operations, the metrics will be affected differently depending on how the\n    truncate is executed internally.\n    For truncates on smaller shards, the truncate operation will be executed as\n    the removal of the individual documents in the shard. Thus the metric will\n    also be increased as if the documents were removed individually. Truncate\n    operations on larger shards however will be executed via a special \n    operation in the storage engine, which marks a whole range of documents as\n    removed, but defers the actual removal until much later (compaction \n    process). If a truncate is executed like this, the metric will not be \n    increased at all.\n    Writes into secondary indexes are not counted at all.\n\n    The metric is also increased for transactions that are started but later\n    aborted.\n  \n  These metrics are not exposed by default. It is only present if the startup \n  option `--server.export-shard-usage-metrics` is set to either \n  `enabled-per-shard` or `enabled-per-shard-per-user`. With the former setting,\n  the metric will have different labels for each shard that was read from. With\n  the latter setting, the metric will have different labels for each \n  combination of shard and user that accessed the shard.\n  \n  If enabled, the metrics are only exposed on DB servers and not on \n  Coordinators or single servers.\n  \n  Note that internal operations, such as internal queries executed for \n  statistics gathering, internal garbage collection, and TTL index cleanup are\n  not counted in these metrics. Additionally, all requests that use the \n  superuser JWT for authentication and that do not have a specific user set,\n  are not counted.\n  Requests are also only counted if they have an ArangoDB user associated with\n  them, so that the cluster must also be running with authentication turned on.\n\n* Validate that the attribute stored in the `smartGraphAttribute` of SmartGraph\n  vertex collections exists and is not changed afterwards by update or replace\n  operations. Previously the `smartGraphAttribute` value was checked only when\n  inserting documents into a SmartGraph vertex collection, but not for update or\n  replace operations, although the documentation always stated that the\n  `smartGraphAttribute` value must not be changed after the initial creation of\n  the document.\n  The missing checks on update/replace allowed users to retroactively modify the\n  value of the `smartGraphAttribute` for existing documents, which could have\n  led to problems when the data of such a SmartGraph vertex collection was\n  replicated to a new follower shard. On the new follower shard, the documents\n  went through the full validation, and led to documents with modified\n  `smartGraphAttribute` values being rejected on the follower. This could have\n  led to follower shards not getting in sync.\n  Now the value of the `smartGraphAttribute` will be fully validated with every\n  insert, update or replace operation, and every attempt to modify the value of\n  the `smartGraphAttribute` retroactively will fail with error 4003\n  (`ERROR_KEY_MUST_BE_PREFIXED_WITH_SMART_GRAPH_ATTRIBUTE`, error message \"in\n  smart vertex collections _key must be a string and prefixed with the value of\n  the smart graph attribute\").\n  Additionally, if upon insert the `smartGraphAttribute` is missing for a\n  SmartGraph vertex, the error code will be error 4001 (\n  `ERROR_NO_SMART_GRAPH_ATTRIBUTE`, error message \"smart graph attribute not\n  given\") instead of error 4003.\n  To retroactively repair the data in any of the affected collections, it is\n  possible to update every (affected) document with the correct value of the\n  `smartGraphAttribute` via an AQL query as follows:\n\n      FOR doc IN @@collection\n        LET expected = SUBSTRING(doc._key, 0, FIND_FIRST(doc._key, ':'))\n        LET actual = doc.@attr\n        FILTER expected != actual\n        UPDATE doc WITH {@attr: expected} IN @@collection\n        COLLECT WITH COUNT INTO updated\n        RETURN updated\n\n  This will update all documents with the correct (expected) value of the\n  `smartGraphAttribute` if it deviates from the expected value. The query will\n  return the number of updated documents as well.\n  The bind parameters necessary to run this query are:\n  - `@@collection`: name of a SmartGraph vertex collection to be updated\n  - `@attr`: attribute name of the `smartGraphAttribute` of the collection\n\n* FE-385: fix query import.\n\n* BTS-1731: protect streaming transaction garbage-collection from deletion of\n  the transaction's underlying database.\n\n* BTS-1727: Return proper EXIT_UPGRADE_REQUIRED in cluster mode.\n\n* Removal artificial upper bound value of `128` for the startup option\n  `--rocksdb.max-background-jobs`.\n\n* Added stored values support for ZKD indexes.\n\n* Fixed a crash during recursive AstNode creation when an exception was thrown.\n  This can happen on DB servers when a query plan snippet is created from\n  VelocyPack, and the query plan snippet would use more memory than is allowed\n  by the query memory limit.\n\n* Fix cmake setup for jemalloc library for the case of memory profiling.\n\n* BTS-1714: Within writing AQL queries the lock timeout was accidentally set to\n  2 seconds for all write operations on followers. This could lead to dropped\n  followers when an index of a large shard was finalized on an follower.\n\n* Fixed BTS-1701: Assertion triggered in AQL Traversal on edge PRUNE.\n\n* Fix an issue when forced index hints were used in a query, but the optimizer\n  selected a query execution plan that would not use the selected index.\n  Previously, the query failed with a \"Could not serve index hint\" error\n  message. With the fix, the optimizer will select the next best plan(s) until\n  all index hints are either satisfied, or there are no further plans to select\n  from. In the latter case, the query still aborts with said error.\n\n\nv3.11.6 (2023-11-29)\n--------------------\n\n* FE-403: Fix loader instantiation.\n\n* Fix display of \"unique\" column in indexes overview of collections.\n\n* Updated OpenSSL to 3.0.12.\n\n* Fixed MDS-1170: Significant performance degradation when queries are executed\n  within transactions that involve edits.\n\n* Solve a potential blockage during hotbackup by not stopping read-only\n  transactions from committing during hotbackup. Furthermore, improve behavior\n  of authentication in the case that the user cache is outdated.\n\n* FE-395: Fix query editor map not loading.\n\n* Stabilize detaching of threads test.\n\n* Rebuilt included rclone v1.62.2 with go1.21.4.\n\n* Updated ArangoDB Starter to v0.17.2 and arangosync to v2.19.5.\n\n* Track memory usage of internal connection statistics and request statistics:\n  - `arangodb_connection_statistics_memory_usage`\n  - `arangodb_requests_statistics_memory_usage`\n  These metrics will remain at 0 if the server is started with the option\n  `--server.statistics false`. Otherwise they will contain the memory usage used\n  by connection and request statistics. Memory usage should remain pretty\n  constant over time, unless there are bursts of new connections and/or\n  requests.\n\n* Avoid memory leak in case an arangod instance is started with the option\n  `--server.statistics false`. Previously, with that setting the request and\n  connection statistics were built up in memory, but were never released because\n  the statistics background thread was not running.\n\n* Avoid memory leak in case an arangod instance is started with the option \n  `--server.statistics false`. Previously, with that setting the request\n  and connection statistics were built up in memory, but were never released\n  because the statistics background thread was not running.\n\n* Remove version check on startup of arangosh. This can speed up the startup of\n  the arangosh considerably because it won't do a network request to\n  www.arangodb.com.\n\n\nv3.11.5 (2023-11-09)\n--------------------\n\n* Fixed a problem in ReadWriteLock which could prevent waiting readers from\n  being woken up, when a write lock acquire timed out.\n\n* Allow a scheduler thread to detach itself from the scheduler if it sees that\n  it has to perform a potentially long running task like waiting for a lock.\n  This allows a new scheduler thread to be started and avoids that it can happen\n  that all threads are blocked waiting for a lock, which has in the past led to\n  deadlock situations. The number of detached threads is limited by a\n  configurable option. Currently, only threads waiting for more than 1 second on\n  a collection lock will detach themselves.\n \n* MDS-1164: Added Publisher and Company to the Windows binaries.\n\n* Silence TSAN for shutdown for access to the SchedulerFeature::SCHEDULER\n  pointer using atomic references.\n\n* Fixed a race in controlled leader change which could lead to a situation in\n  which a shard follower is dropped when the first write operation happens. This\n  fixes BTS-1647.\n\n* Fixed BTS-1273: While queueing an async server log message, we could be\n  blocked by IO on the log-writer thread. This could slow down the main\n  path. In case the log-writer is configured to use slow device\n  (e.g. using syslog) this could have significant impact.\n\n* Introduced an upper bound of queued async log messages. If we would log\n  more messages then the background logger thread can actually process, we\n  start to write log messages synchronously. This is to prevent the queue\n  to grow indefinitely. The upper bound is configurable via the startup\n  option `--log.max-queued-entries`. The default value is 10000.\n* Fixed an unnecessary follower drop in controlled leader change, which will\n  speed up leader changes. This fixes BTS-1658.\n\n* Fixed BTS-1669 Transaction Manager returns Error if we Abort an Expired\n  Transaction.\n  There was a small time window of around 2 seconds in which aborting expired\n  transactions would return \"transaction aborted\" instead of returning success.\n  The time window was between when a transaction expired (according to its TTL\n  value) and when the transaction manager's garbage collection aborted the\n  transaction. The issue only happened for transactions which outlived their TTL\n  value and for which an abort operation was attempted in that time window.\n\n* Backport multiple fixes for the scheduler behavior during ArangoDB shutdown.\n\n* FE-374: Query UI - fix switching from table to JSON.\n\n* Make AQL query cursor garbage-collection clean up more expired cursors in a\n  single run than before. This change can help to reclaim memory of expired\n  cursors quicker than in previous versions.\n\n* Updated OpenSSL to 3.0.11.\n\n* Reduce number of atomic shared_ptr copies in in-memory cache subsystem.\n\n* Fixed BTS-1610: In certain situations with at least three levels of nested\n  subqueries, of which some of the outer iterations don't return any results,\n  results of later iterations could be lost.\n\n* Updated arangosync to v2.19.4.\n\n* Add cluster-internal connectivity checks.\n  This makes Coordinators and DB-Servers in a cluster periodically send check\n  requests to each other, in order to check if all servers can connect to each\n  other.\n  If a cluster-internal connection to another Coordinator or DB-Server cannot be\n  established within 10 seconds, a warning will be logged.\n\n  The new startup option `--cluster.connectivity-check-interval` can be used to\n  control the frequency of the connectivity check, in seconds.\n  If set to a value greater than zero, the initial connectivity check is\n  performed approximately 15 seconds after the instance start, and subsequent\n  connectivity checks are executed with the specified frequency.\n  If set to a value of zero, connectivity checks are disabled.\n\n  This change also adds the metrics\n  `arangodb_network_connectivity_failures_coordinators` and\n  `arangodb_network_connectivity_failures_dbservers`, which can be monitored for\n  detecting temporary or permanent connectivity issues.\n\n* Fix ineffective startup option `--rocksdb.partition-files-for-documents`.\n  Setting this option had no effect unless RocksDB version 8 was used.\n\n\nv3.11.4 (2023-10-04)\n--------------------\n\n* Fix updating of collection properties (schema) when the collection has a view\n  on top.\n\n* FE-323: allow 'nested' property in view JSON UI.\n\n* Slightly extended hot backup release lock timeout on coordinators from\n  `timeout + 5` seconds to `timeout + 30` seconds, to prevent premature release\n  of hot backup commit locks on coordinators.\n\n* Improve logging in case hot backup locks cannot be taken on coordinators.\n\n* BTS-1618: Preventing the problem from ever occurring on 3.11.\n\n* Speed up incremental hotbackup upload by parallelization of remote-to-remote\n  copies. Tolerate deletion of old backups during incremental upload.\n\n* Added the following metrics to improve observability of the in-memory cache\n  subsystem:\n  - `rocksdb_cache_free_memory_tasks_total`: total number of `freeMemory` tasks\n    scheduled\n  - `rocksdb_cache_migrate_tasks_total`: total number of `migrate` tasks\n    scheduled\n  - `rocksdb_cache_free_memory_tasks_duration_total`: total time (microseconds)\n    spent in `freeMemory` tasks\n  - `rocksdb_cache_migrate_tasks_duration_total`: total time (microseconds)\n    spent in `migrate` tasks\n\n* Improve performance of the in-memory cache's memory reclamation procedure.\n  The previous implementation acquired too many locks, which could drive system\n  CPU time up.\n\n* Fix MDS-1157: taking hot backups with --allow-inconsistent=false option always\n  reported a warning that the backup was potentially inconsistent, although\n  taking the backup actually succeeded.\n\n  The warning message was `Failed to get write lock before proceeding with\n  backup. Backup may contain some inconsistencies.`, but it was a false positive\n  and the backups taken were actually consistent.\n\n* Fixed issue with `keepNull=false` updates not being properly replicated to\n  followers.\n\n* Updated JavaScript dependencies:\n\n  qs: 6.11.0 -> 6.11.2\n  semver: 7.3.8 -> 7.5.4\n\n  This addresses CVE-2022-25883 in the semver module.\n\n* FE-355: fix ctrl + A for search box.\n\n* FE-365: fix query import breaking due to extra fields.\n\n* Fixed BTS-1613: Fixed processing analyzers imported from Cluster dump to the\n  Single server database.\n\n* Removed DocuBlocks and obsolete documentation tooling.\n  The HTTP API descriptions are now in the `arangodb/docs-hugo` repository.\n\n* Added metric `rocksdb_cache_edge_empty_inserts_total` to count the number\n  of inserts into the edge cache for non-connected edges.\n\n* Renamed two edge-cache related metrics to improve naming consistency:\n  - `rocksdb_cache_edge_effective_entries_size` was renamed to \n    `rocksdb_cache_edge_inserts_effective_entries_size_total` and was changed\n    from a gauge to a counter.\n  - `rocksdb_cache_edge_uncompressed_entries_size` was renamed to \n    `rocksdb_cache_edge_inserts_uncompressed_entries_size_total` and is now\n    also a counter instead of a gauge.\n\n* Do not auto-reload entries into in-memory edge cache for edge index entries\n  that were previously not contained in the in-memory edge cache. This change\n  will help to keep the hot set in memory rather than evicting it in favor of\n  \"random\" other index entries.\n\n* Properly track memory usage for allocated objects in the in-memory cache\n  (e.g. the edge cache or the in-memory cache for other persistent indexes).\n  Previously the memory used for the underlying hash tables was accounted for\n  correctly, but the sizes of the cache payloads (keys and values) were not\n  accounted for under all circumstances (at least for the initial entries in\n  the caches).\n  This change leads to more accurate memory usage tracking and reporting by the \n  in-memory cache subsystem, and to the cache subsystem not exceeding its \n  configured memory usage limit. \n  The cache subsystem was also changed so that it can use as much memory as\n  configured by the global cache memory limit (configurable via startup options\n  `--cache.size` and `--cache.high-water-multiplier`). Previously the cache \n  subsystem was freeing memory as soon as it hit 56% of the configured limit.\n  Overall, the effective memory usage of the cache subsystem can be different \n  to the cache memory usage in previous versions. In previous versions the\n  configured memory usage limit could be temporarily exceeded, but in most\n  cases the cache used considerably less memory than allowed by the limit.\n  Effectively the memory usage was capped at 56% of the configured limit.\n  Now the cache will try to use up to as much memory as allowed by the\n  configured memory usage limit (i.e. `--cache.size` multiplied by the high\n  water multiplier `--cache.high-water-multiplier`). The default value for\n  the high water multiplier is set to 56% in this version to keep compatibility\n  with previous versions.\n\n* Expose the \"high water multiplier\" for the in-memory cache subsystem. The\n  high water multiplier is used to calculate the effective memory usage limit\n  for the in-memory cache subsystem. The cache's configured memory usage limit\n  (`--cache.size`) is multiplied by the high water multiplier, and the \n  resulting value is used as the effective memory limit. It defaults to 56%\n  to ensure compatibility with previous versions, in which the threshold was\n  effectively hard-coded to the same value.\n\n* Reduce memory usage for empty in-memory edge caches by ~40%. This is achieved\n  by allocating each cache's statistics objects only lazily, when actually \n  needed.\n\n* Added the following metrics for the in-memory edge cache:\n\n  - `rocksdb_cache_edge_inserts_total`: total number of insertions into the\n    in-memory edge cache.\n  - `rocksdb_cache_edge_compressed_inserts_total`: total number of insertions\n    into the in-memory edge cache that used compression.\n\n* Added the startup option `--cache.max-spare-memory-usage` to control memory\n  usage for spare, unused hash tables in the in-memory caching subsystem. This\n  option can be used to cap the memory used by spare tables. It can be set to\n  a value of 0 to not use any memory except for active hash tables. \n\n* Fixed BTS-1556: Potential shutdown race, when the server is shutting down and\n  still AgencyCommunication was going on it could use the scheduler which was\n  already deleted, causing a crash right before a normal shutdown.\n\n* Fixed BTS-1556: Potential shutdown race, when the server is shutting down\n  and still AgencyCommunication was going on it could use the scheduler\n  which was already deleted, causing a crash right before a normal shutdown.\n\n  This should not have any negative effect, as all state preserving operations\n  are done by then, it just wasn't a clean exit.\n\n* Updated ArangoDB Starter to 0.17.1.\n\n* Fixed BTS-1541: Old legacy little endian key format for RocksDB database\n  (created in ArangoDB 3.2 and 3.3) showed wrong behavior on newer versions.\n  This fixes a persistent index corruption bug with the old format.\n\n* Fixed a loophole in COLLECT variable name validation: in COLLECT INTO\n  expressions it was possible to refer to variables that the COLLECT just\n  introduced. This was undefined behavior and not caught by the previous version\n  of COLLECT's variable checking.\n\n* BTS-1598: fix race in agency.\n\n* ES-1727: Fix `UPDATE`, `REPLACE`, and `UPSERT ... UPDATE/REPLACE` failing with\n  \"conflict, _rev values do not match\" for non-local edges in Smart- and\n  EnterpriseGraphs, when `OPTIONS { ignoreRevs: false }` is supplied.\n\n* Upgraded Swagger-UI to v5.4.1.\n\n* Fixed BTS-1590: Fixed potentially undefined behavior in NetworkFeature, when\n  it was referring to an options object that could have been destroyed already.\n\n* Allow compression content-negotation in metrics API, so that responses from\n  the metrics API at `/_admin/metrics` can be sent compressed if the client\n  supports it.\n\n\nv3.11.3 (2023-08-17)\n--------------------\n\n* Fixed BTS-1553: Fixed a rare occuring issue during AQL queries where the inner\n  amount of a limit in the LimitExecutor is set to a wrong value which lead to\n  some data rows not being returned to the client.\n\n* BTS-1544: The _system database now properly reports its sharding value.\n\n* Fixed BTS-1554: wrong aggregation count when an \"in\" or \"or\" condition\n  was executed through an index lookup.\n\n* BTS-1549: adjust permission handling in experimental dump to the same behavior\n  as in non-experimental dump.\n\n* Fixed AQL WINDOW statement for OneShard databases: Whenever WINDOW is used in\n  the row based variant like (e.g. WINDOW { preceding: 1, following: 1 }) it\n  errored with:  mandatory variable \"inVariable\" not found. This variable is now\n  correctly treated as optional.\n\n* Fixed AQL WINDOW statement for OneShard databases: Whenever WINDOW is used on\n  a OneShardDatabase, or on data from a collection that only has one shard, the\n  preceding and following clauses were flipped.\n\n* Updated arangosync to v2.19.3.\n\n* Harden HTTP/2 internal callback functions against exceptions.\n\n  These callback functions are called from C code which cannot handle exceptions\n  in any way. Instead, we now turn any exception into the return code\n  `HPE_INTERNAL` to signal that an error occurred.\n\n* Added better diagnostic messages in case documents<->primarx index corruption\n  occurs.\n\n* Prevent potential buffer overflow in the crash handler.\n\n* Updated OpenSSL to 3.0.10 and OpenLDAP to 2.6.6.\n\n* Updated ArangoDB Starter to 0.17.0.\n\n* ES-1566: instead of potentially accessing a nullptr inside graph traversal\n  setup, throw an exception. This will be handled properly and returned with\n  a proper error message.\n\n* BTS-1511: AQL: Fixed access of integers in the ranges\n  [-36028797018963968, -281474976710657] and\n  [281474976710656, 36028797018963968], i.e. those whose representation require\n  7 bytes. These values were misinterpreted as different integers.\n  Simple passing of values (i.e. writing to or reading from documents) was not\n  affected: Only accesses of those values as numbers by AQL was. E.g. arithmetic\n  (addition, multiplication, ...), certain AQL functions, comparing/sorting -\n  the latter only if the numbers are compared directly, but not as part of a\n  value. For example, `SORT x` lead to an unexpected order if x was such a\n  number. `SORT [x]` however worked as expected.\n\n* Added experimental startup options for RocksDB .sst file partitioning:\n\n  - `--rocksdb.partition-files-for-documents`\n  - `--rocksdb.partition-files-for-primary-index`\n  - `--rocksdb.partition-files-for-edge-index`\n  - `--rocksdb.partition-files-for-persistent-index`\n\n  Enabling any of these options will make RocksDB's compaction write the data\n  for different collections/shards/indexes into different .sst files.\n  Otherwise the document data from different collections/shards/indexes can be\n  mixed and written into the same .sst files.\n\n  Enabling these options usually has the benefit of making the RocksDB\n  compaction more efficient when a lot of different collections/shards/indexes\n  are written to in parallel.\n  The disavantage of enabling this option is that there can be more .sst files\n  than when the option is turned off, and the disk space used by these .sst\n  files can be higher than if there are fewer .sst files (this is because there\n  is some per-.sst file overhead).\n  In particular on deployments with many collections/shards/indexes this can\n  lead to a very high number of .sst files, with the potential of outgrowing the\n  maximum number of file descriptors the ArangoDB process can open.\n  Thus the option should only be enabled on deployments with a limited number of\n  collections/shards/indexes.\n\n* Fixed potential deadlock() in cache::Manager::unprepareTask() in case a\n  MigrateTask could not be scheduled successfully (in case of a full scheduler\n  queue on DB servers).\n\n\nv3.11.2 (2023-07-21)\n--------------------\n\n* Fix SEARCH-465: Truncate on ArangoSearch index could release a WAL if commit\n  passed.\n\n* Updated ArangoDB Starter to 0.16.0.\n\n* Avoid recursive lock during agency startup.\n\n* Fixed issue with lock starvation when an AQL insert operation with multiple\n  static documents was executed as part of a streaming transaction.\n\n* Added startup option `--database.max-databases` to limit the maximum number of\n  databases that can exist in parallel on a deployment. This option can be used\n  to limit resources used by database objects. If the option is used and there\n  already exist as many databases as configured by this option, any attempt to\n  create an additional database will fail with error 32\n  (`ERROR_RESOURCE_LIMIT`). Additional databases can then only be created if\n  other databases are dropped first.\n  The default value for this option is unlimited, so technically an arbitrary\n  amount of databases can be created (although effectively the number of\n  databases is limited by memory and processing resources).\n\n* FE-304, FE-305: use navigator.onLine for checking internet connection, correct\n  path for running/slow queries.\n\n* Enforce that server always returns an empty body if the return code is 204.\n\n* Arangodump retries dump requests in more cases: read, write and connection\n  errors.\n\n* Whenever there is a query request timeout in AQL (e.g., a server died) which\n  causes the query to fail, the DBServers will now all directly kill their\n  potentially ongoing parts of the query and not wait for garbage collection.\n\n* Use libunwind in jemalloc profiling to make it available with libmusl and\n  static binaries.\n\n* BTS-1331: Reduced the amount of required network calls when using traversals\n  combined with FILTER and/or PRUNE statements in a GeneralGraph in combination\n  with a clustered environment.\n\n* Added transparent LZ4 compression for values in the in-memory edge cache if\n  their size exceeds a configurable threshold. This is configurable as an opt-in\n  functionality.\n\n  LZ4 compression of edge index cache values allows to store more data in main\n  memory than without compression, so the available memory can be used more\n  efficiently. The compression is transparent and does not require any change to\n  queries or applications.\n  The compression can add CPU overhead for compressing values when storing them\n  in the cache, and for decompressing values when fetching them from the cache.\n  \n  The new startup option `--cache.min-value-size-for-edge-compression` can be\n  used to set a threshold value size for compression edge index cache payload\n  values. The default value is `1GB`, which will effectively turn compression\n  off. Setting the option to a lower value (e.g. `100`) will turn on the\n  compression for any payloads whose size exceeds this value.\n  \n  The new startup option `--cache.acceleration-factor-for-edge-compression` can\n  be used to fine-tune the compression. It controls the LZ4-internal\n  \"acceleration\" factor used for the compression. The default value is `1`.\n  Higher values typically mean less compression but faster speeds.\n\n  The following new metrics can be used to determine the usefulness of\n  compression:\n\n  - `rocksdb_cache_edge_effective_entries_size`: will return the total number of\n    bytes of all entries that were stored in the in-memory edge cache, after\n    compression was attempted/applied. This metric will be populated regardless\n    of whether compression is used or not.\n  - `rocksdb_cache_edge_uncompressed_entries_size`: will return the total number\n    of bytes of all entries that were ever stored in the in-memory edge cache,\n    before compression was applied. This metric will be populated regardless of\n    whether compression is used or not.\n  - `rocksdb_cache_edge_compression_ratio`: will return the effective\n    compression ratio for all edge cache entries ever stored in the cache.\n\n  Note that these metrics will be increased upon every insertion into the edge\n  cache, but not decreased when data gets evicted from the cache.\n\n* Optimize runtime performance of exclusive locks.\n\n* Added startup option `--replication.active-failover-leader-grace-period`.\n  This startup option can be used to set the amount of time (in seconds) for\n  which the current leader in an active failover setup will continue to assume\n  its leadership even if it lost connection to the agency.\n\n  In case the leader cannot contact the agency anymore, the agency will elect a\n  new leader after the supervision grace period has elapsed.\n  In order to avoid a split-brain situation with multiple servers assuming\n  leadership, this option can be used to make a disconnected leader refuse any\n  incoming write operations after the grace period controled by this option has\n  elapsed.\n  Ideally the startup option should be given a value greater than the value of\n  the supervision grace period, in order to avoid a temporarily disconnected\n  leader giving up leadership too early and unnecessarily.\n\n  The default value is 120 seconds. Setting the option to a value of 0 will keep\n  the existing behavior, in which a disconnected leader will not refuse incoming\n  write operations.\n\n* Added new startup options `--cache.ideal-lower-fill-ratio` and\n  `--cache.ideal-upper-fill-ratio` to control the minimum and maximum fill\n  ratios for cache tables that trigger shrinking and growing of the table by the\n  cache rebalancer. The default values are:\n\n  - `0.04` (i.e. 4%) for the lower bound that triggers shrinking\n  - `0.25` (i.e. 25%) for the upper bound that triggers growing\n\n  These values were hard-coded in previous versions of ArangoDB.\n\n* Remove temporary `CREATING_{number}` directories from hot backup in case a hot\n  backup runs into an error.\n\n* BTS-1490: Allow performing AQL updates locally without using DISTRIBUTE in\n  case the update AQL is of the pattern\n\n      FOR doc IN collection\n        UPDATE <doc> IN collection\n\n  Previously the optimization was only possible if the update AQL was of the\n  pattern\n\n      FOR doc IN collection\n        UPDATE <key> WITH <doc> IN collection\n\n  Both <key> and <doc> refer to data from the collection enumeration variable\n  `doc` here.\n\n  Also fix the optimization in case a shard key attribute is updated with a\n  value from a different attribute, e.g.\n\n     FOR doc IN collection\n       UPDATE { _key: doc.abc, abc: doc._key } IN collection\n\n  In this case the optimization was previously applied although it shouldn't.\n\n* BTS-1490: query can use a lot more memory when using COLLECT WITH COUNT.\n\n* Attempt to avoid busy looping when locking collections with a timeout.\n\n* Add a `description` field to OptimizerRule and dump the explanations via the\n  `GET /_api/query/rules` endpoint.\n\n* FE-20: fix UI placeholder format for locale when creating analyzers.\n\n* FE-240: disable JSON editor when viewing inverted index.\n\n* FE-287: fix number validation for replicationFactor and writeConcern.\n\n* FE-285: Fix query download - use post request for query.\n\n* Fixed Github issue #19175.\n  This fixes a problem in traversal query optimization that was introduced in\n  3.11 and that can lead to traversal queries being aborted with an error `AQL:\n  cannot and-combine normalized condition`.\n\n* Fixed two possible deadlocks which could occur if all medium priority threads\n  are busy. One is that in this case the AgencyCache could no longer receive\n  updates from the agency and another that queries could no longer be finished.\n  This fixes BTS-1475 and BTS-1486.\n\n\nv3.11.1 (2023-06-12)\n--------------------\n\n* Updated arangosync to v2.18.1.\n\n* SEARCH-480: Speedup ArangoSearch recovery.\n\n* SEARCH-476: Fix bug in fst builder.\n\n* BTS-1325: AQL: Fixed a possible deadlock with multiple parallel traversals.\n\n* Updated OpenSSL to 3.0.9.\n\n* BTS-1435: fixed invalid AQL optimization and added a safeguard.\n\n* APM-766, SEARCH-479: Reduce memory overhead for ArangoSearch removes.\n\n* Improve precision for ArangoSearch GEO_IN_RANGE function.\n\n* Updated ArangoDB Starter to 0.15.8.\n\n* OASIS-25262: Fixed undefined behavior in IN lookup in unique indexes when the\n  lookup array had to be rebuilt in memory.\n\n* Invalid keys are now reported as individual errors for batch insert operations\n  and no longer abort the whole batch.\n\n* BTS-1255: Fix sporadic memory usage accounting underflows in in-memory cache\n  subsystem.\n  Also turn the algorithm for freeing memory from a cache's buckets from a\n  non-deterministic one that did not guarantee progress into a bounded algorithm\n  with guaranteed progress.\n\n* ECONNABORTED is treated as a ConnectionClosed error in fuerte.\n\n* Database drop operation no longer fails if we cannot remove the corresponding\n  permissions from the _users collection.\n\n* Added startup option `--query.max-collections-per-query` to adjust the limit\n  for the maximum number of collections/shards per query. The option defaults to\n  `2048`, which is equivalent to the previous hardcoded value.\n\n* BTS-1261 For some named graphs in cluster, when creating a debugDump for a\n  traversal query that would use the graph's name, the graph wouldn't be able to\n  be recreated because the info gathered in the process of creating a debugDump\n  was broken. It uses the result form an aql explain to get the graph info and\n  use in the debugDump, but this wouldn't be available because, instead of\n  having the name of the graph as key in the graph object of the explain, there\n  would be an array with edge collection names. It was changed for the case when\n  there's a named graph, but maintained when there's no access to the graph's\n  name.\n\n* When trying to read multiple documents, the coordinator will now handle empty\n  lists gracefully and return an empty result set instead of an error.\n\n* Added metric \"rocksdb_total_sst_files\" to count the number of sst files,\n  aggregated over all levels of the LSM tree.\n\n* Increase too short timeouts for requests made from coordinators to DB-servers\n  when retrieving the number of documents or the index selectivity estimates for\n  SmartGraph edge collection parts. These parts were treated like system\n  collections because of their naming convention, and the requests were run with\n  a timeout of only 5s.\n\n* In batched query results, when executing requests for\n  `/_api/cursor/<cursorId>/<batchId>`, removed the restriction that the user\n  would only be able to fetch the next batch using the next batch id in\n  <batchId> if the query option `allowRetry` was set to true, but maintained the\n  restriction that the user can only retrieve the latest batch if the query\n  option `allowRetry` is set to true.\n\n* Add a startup parameter `--rclone.argument` which can be utilised to enable\n  debugging with logfiles in hot backup RClone upload operacions:\n  `--rclone.argument=--log-level=DEBUG`\n  `--rclone.argument=-log-file=/tmp/rclone.log`\n\n* Fix issue #18982: query editor null/undefined check filters out bindParams\n  with value 0.\n\n\nv3.11.0 (2023-05-23)\n--------------------\n\n* Convert v3.11.0-rc.2 into v3.11.0.\n\n* SEARCH-477: stabilize ArangoSearch (stage of BTS-1416).\n\n\nv3.11.0-rc.2 (2023-05-17)\n-------------------------\n\n* Internal bug-fixes and stabilization improvements.\n\n\nv3.11.0-rc.1 (2023-05-13)\n-------------------------\n\n* FE-211: Allow admins to edit gravatar email\n\n* Fixed BTS-1398: GEO_DISTANCE() for ArangoSearch geo_s2 analyzer.\n\n* Improved error reporting for system calls on Windows and fixed a\n  Windows-specific issue of the IOHeartbeatThread.\n\n* FE-256: Minor Graph Viewer improvements:\n  - re-order right click menu\n  - Make Right click menu and toolbar work in full screen\n  - Remove editable attributes in edit and delete (nodes and edges) modals\n  - Add \"tree\"-mode to editor in edit (nodes and edges) modals\n\n* FE-266: Use the method 'fromGeoJson()' instead of 'geometry.coordinates'\n  for GeoJSON.\n\n* Fixed issue #18942: arangorestore ignores the HTTP status code.\n\n* FE-267: fix custom analyzer & features not showing up in inverted index view,\n  and support for basic fields definition.\n\n* FE-265: Fix user permissions radio button shadow.\n\n* FE-268: remove 'switch to new graph' when no defined graph is present.\n\n* FE-258: allow specifying optimizeTopK during arangosearch view creation.\n\n* FE-257: allow additional properties in inverted index creation.\n\n* UI Fix - allow empty keys in document key validation.\n\n* BTS-1181: fix a data race on the collection list in TransactionState.\n  This race could happen in the cluster when a collection is sharded by a\n  different attribute than `_key` and a document lookup by _key is performed.\n  This requires the lookup to be sent to each shard, since we cannot deduce the\n  shard based on the `_key` value. If this lookup is done as part of a stream\n  transaction, and if the collection has not been declared in the transaction's\n  `read` list, then the collection is added lazily. Previously this would result\n  in a race if multiple shards are located on the same server.\n  For transactions with `allowDirtyReads` set to true read collections are\n  always added lazily, which made this race more likely.\n\n* BTS-1340: Fixed telemetrics api making shell hang when logging into it and\n  leaving it too quickly so the telemetrics API doesn't have time to send the\n  data to the warehouse. The thread could be hanging on SSL_connect() until the\n  connection timeout if the socket is blocking, so the socket was made\n  non-blocking for the thread to leave SSL_connect() after the connection is\n  interrupted when leaving the shell. Also created startup parameter\n  `--client.failure-points`for arangosh which enables failure points whose names\n  are provided in an array of strings, just like in `--server.failure-point` for\n  arangod.\n\n* BTS-1350: Fixed imprecision in index info in telemetrics object for cluster.\n  When the collection also had an arangosearch view, telemetrics showed\n  imprecise index values because the view was being considered as an index of\n  type arangosearch, and its object wouldn't contain expected fields that would\n  be found in other indexes' objects.\n\n* Add metric `arangodb_file_descriptors_current` to expose the number of file\n  descriptors currently opened by the arangod process. This metric is available\n  on Linux only.\n  As counting the number of open file descriptors can be expensive, this metric\n  is only updated in a configurable interval. The new startup option\n  `--server.descriptors-count-interval` can be used to specify the update\n  interval (in milliseconds). It defaults to 60,000 (i.e. once per minute).\n  The startup option can be set to a value of `0` to disable the counting of\n  open file descriptors for performance reasons.\n  If counting is turned off, the metric will report a value of `0`.\n\n* ES-1566: fix an issue when trying to restrict traversals to non-existing\n  collections with `edgeCollections` traversal option.\n\n  If a non-existing collection is specified in the `edgeCollections` or\n  `vertexCollections` options of an AQL traversal, the query will now fail \n  with a `collection or view not found` error. Also, if wrong collection\n  types are used (e.g. a document collection or a view for `edgeCollections`), \n  then an error is raised about an invalid collection type being used.\n\n  This is a behavior change compared to previous versions, which ignored \n  specifying non-existing vertex collections and had undefined behavior when \n  specifying non-existing edge collections or using a vertex collection\n  instead of an edge collection.\n\n* Added metric `rocksdb_cache_peak_allocated` to store the peak memory\n  allocation value for in-memory caches.\n\n* Remove leftover in-memory cache tables after dropping collections that\n  had their `cacheEnabled` flag set to `true`. Previously some memory \n  could remain allocated in the in-memory cache even after such collections\n  were dropped.\n\n* BTS-1315: Fixed spurious occurring failures during Foxx Application\n  installation in case a Load Balancer is being used in front of the\n  coordinators.\n\n* Updated arangosync to v2.17.0.\n\n* BTS-1343: Removed an error message of endpoint being invalid when attempting\n  fetch telemetrics from the servers by having the invalid endpoint as a\n  starting point. As telemetrics is transparent to the end user, the error\n  message interferes with the user experience if keeps appearing.\n\n* FE-255: add validation for collection, document, graph, view and database\n  names.\n\n* Add metric `arangodb_file_descriptors_limit` to expose the system limit for\n  the number of open files for the arangod process.\n\n* Fix an issue that causes followers to be dropped due to premature transaction\n  abortions as a result of query failures.\n  When we have a query that results in a failure this will cause the leaders to\n  abort the transaction on the followers. However, if the followers have\n  transactions that are shared with leaders of other shards, and if one of those\n  leaders has not yet seen the error, then it will happily continue to replicate\n  to that follower. But if the follower has already aborted the transaction,\n  then it will reject the replication request. Previously this would cause the\n  follower to be dropped, but now this should be handled gracefully.\n\n\nv3.11.0-beta.1 (2023-05-07)\n---------------------------\n\n* Updated arangosync to v2.17.0-preview-1.\n\n* FE-243: add support for geo_s2 analyzer.\n\n* Reduce memory usage for incremental sync replication.\n\n  The incremental sync protocol that was used for collections created with 3.8\n  or higher had memory usage issues in case the follower already had some local\n  data and its dataset was much larger than the leader's. For example, this can\n  happen if a follower gets disconnected, then a lot of document removals happen\n  on the leader and afterwards the follower tries to get back into sync.\n\n  In this case, the follower buffered the ids of documents it had to remove\n  locally in a vector, which could grow arbitrarily large. The removal of the\n  documents contained in this vector would only happen at the end, potentially\n  even without performing intermediate commits.\n\n  The change in this PR is to trigger the document removal earlier, once the\n  vector has reached some size threshold, and also to use intermediate commits\n  during the removals.\n\n\nv3.11.0-alpha.1 (2023-05-03)\n----------------------------\n\n* FE-22: Don't display collection content by default when clicking on a\n  collection in the Web UI.\n\n* Fixed a race in a test for aborting long running operations.\n\n* FE-251: bugfix - prohibit multiple expansions of the same node in the graph\n  viewer.\n\n* FE-162: Fix display of geodesic lines.\n\n* FE-263: Improve forceAtlas layout in the Graph Viewer.\n\n* Fixed a race in the Maintenance, impacting leaders that have just been\n  resigned. During a MoveShard job, the old leader could sabotage the whole\n  operation by removing the newly added follower from Current. The solution\n  is to update Current using local collection information, instead of the\n  potentially outdated velocypack slice.\n\n* Fix unstable test setting the desired number of dbservers.\n\n* FE-253: bugfix - validate JSON, show errors & disable save when error.\n\n* FE-254: bugfix - filter out empty storedValues for persistent index.\n\n* FE-252: bugfix - canvas image screenshot breaks graph colors.\n\n* Fix incompatibility between 3.9 and 3.10 w.r.t. to serialization of AQL array\n  filters (i.e. `[* FILTER ...]`). The array filters were serialized in a\n  different way in 3.9 than they are serialized in 3.10. 3.10 also expected the\n  new serialization format when unserializing a plan.\n  The fix now enables support for both formats.\n\n* Fixed issue #18769: Input validation allows invalid UTF-8 code points.\n\n  This change enforces the validation of UTF-8 surrogate pairs in incoming JSON\n  data. Previously, the following loopholes existed when validating UTF-8\n  surrogate pair data:\n  - a high surrogate, followed by something other than a low surrogate (or the\n    end of the string)\n  - a low surrogate, not preceeded by a high surrogate\n  These loopholes are now closed, which means that any JSON inputs with invalid\n  surrogate pair data will be rejected by the server.\n\n  Note that the extended validation for surrogates can be turned off along with\n  other UTF-8 string validation by setting the server startup option\n  `--server.validate-utf8-strings` to `false`. This is not recommended though,\n  but should only be used in situations when a database is known to contain\n  invalid data and must continue supporting it.\n\n* Updated rclone to v1.62.2 custom build with go1.20.3.\n\n* Changed return code of APIs that create databases from previously 1229\n  (`ERROR_ARANGO_DATABASE_NAME_INVALID`) to 1208 (`ERROR_ARANGO_ILLEGAL_NAME`)\n  in case an invalid database name is used.\n  This is a downwards-incompatible change, but unifies the behavior for\n  database creation with the behavior of collection and view creation,\n  which also return error 1208 in case the specified name is invalid.\n\n* FE-236: bugfix - remove unused files, use new tooltip in views UI.\n\n* FE-238: Added auto-login support in core web UI - disabled logout when\n  auto-login is enabled, set sessionStorage \"jwtUser\" value when login is\n  skipped.\n\n* FE-233: bugfix - fix query spotlight search not working.\n\n* FE-349: bugfix - filter out empty primarySort field in UI.\n\n* FE-247: bugfix - missing storedValues field in persistent index form.\n\n* FE-242, FE-244: bugfix - add support for cache fields, fix inverted index name\n  undefined.\n\n* FE-241: bugfix - filter predefined queries based on search term.\n\n* FE-216: bugfix - make view patches async in the UI.\n\n* FE-212: bugfix: links not getting removed when copying from another view in\n  UI.\n\n* FE-222: Fix - Allow additional properties in arangosearch, allow no fields in\n  inverted index when 'includeAllFIelds' is true.\n\n* APM-183: Support UTF-8 on UI (collection/view/index names).\n\n* FE-199: Remove URL handling of fields on view screen.\n\n* Changed the behavior of the following JavaScript functions in arangosh and\n  arangod (e.g. when used from a Foxx service):\n\n  - `db.<collection>.dropIndex(id)`: this function now throws if no index exists\n    with the specified id. Previously the function only returned the value\n    `false`.\n  - `db._dropIndex(id)`: this function now throws if no index exists with the\n    specified id. Previously the function only returned the value `false`.\n\n  These changes are not downwards-compatible, but they can be easily worked\n  around by wrapping dropIndex calls into a try ... catch.\n\n  The HTTP API for dropping indexes is not affected by these changes, as it\n  previously returned HTTP 404 already when the specified index could not be\n  found.\n\n* Added `--dump-views` option to arangodump, to control whether arangosearch\n  view definitions should be stored as part of the dump. The option defaults to\n  `true`.\n\n* APM-183: optionally allow special characters and Unicode characters in\n  collection names, view names and index names.\n\n  This feature allows toggling the naming convention for collection names, view\n  names and index names from the previous strict mode, which only allowed\n  selected ASCII characters, to an extended, more relaxed mode. The extended\n  mode allows additional ASCII characters as well as non-ASCII UTF-8 characters\n  in database names, collection names, index names and view names.\n  The extended mode can be enabled by setting the new startup option\n  - `--database.extended-names`\n  to true. It is turned off by default and requires an explicit opt-in, simply\n  because some drivers and client applications may not be ready for it yet.\n  The arangod server, the ArangoDB web interface and the following bundled\n  client tools are prepared and ready for using the extended names:\n  - arangobench\n  - arangodump\n  - arangoexport\n  - arangoimport\n  - arangorestore\n  - arangosh\n  More tools and the drivers shipped by ArangoDB may be added to the list in the\n  future.\n\n  Please note that the extended names should not be turned on during upgrades\n  from previous versions, but only once the upgrade has been completed\n  successfully. In addition, the extended names should not be used in\n  environments that require extracting data into a previous version of ArangoDB,\n  or when database dumps may be restored into a previous version of ArangoDB.\n  This is because older versions will not be able to handle the extended names.\n  Finally, it should not be turned on in environments in which drivers are in\n  use that haven't been prepared to work with the extended naming convention.\n\n  Warning: turning on the `--database.extended-names` option for a deployment\n  requires it to stay enabled permanently, i.e. it can be changed\n  from `false` to `true` but not back. When enabling it, it is also required\n  to do this consistently on all coordinators and DB servers.\n\n  The extended names for databases, collections, views and indexes will be\n  enabled by default in one of the future releases of ArangoDB, once enough\n  drivers and other client tools have had the chance to adapt.\n\n* FE-200: Adds smart & enterprise graph support in the UI.\n\n* Forward the `ttl` cursor option for AQL queries in the JavaScript API\n  from the `db._query()` and the `db._createStatement()` methods to the server.\n\n* APM-407: add an optimization for inserting multiple documents at the same time\n  via an AQL INSERT query.\n\n  There is an optimizer rule `optimize-cluster-multiple-document-operations`,\n  which fires in case an AQL query has one of the patterns\n  - `FOR doc IN @docs INSERT doc INTO ...` (where `@docs` is a bind parameter\n    with an array of documents to be inserted),\n  - `FOR doc IN [...] INSERT doc INTO ...` (where the FOR loop iterates over an\n    array of input documents known at query compile time),\n  - `LET docs = [...] FOR doc IN docs INSERT doc INTO ...` (where the documents\n    set up by the LET are some static documents known at query compile time)\n\n  If a query has such pattern, and all the following restrictions are met, then\n  the optimization is triggered:\n\n  - there are no following RETURN nodes (including any RETURN OLD, RETURN NEW)\n  - the FOR loop is not contained in another outer FOR loop or subquery\n  - there are no other nodes (e.g. LET, FILTER) between the FOR and the INSERT\n  - the INSERT is not used on a SmartGraph edge collection\n  - the FOR loop is iterating over a constant, deterministic expression\n\n  The optimization will then add a `MultipleRemoteExecutionNode` to the query\n  execution plan, which will care about inserting all documents into the\n  collection in one go. Further optimizer rules are skipped if the optimization\n  is triggered.\n\n  Future versions of ArangoDB may lift some of the restrictions for the query\n  pattern, so that the optimization may be triggered in more cases in the\n  future.\n\n* FE-200: Add canvas interactions to Graph Viewer.\n\n* FE-218: Updated WebUI dependencies.\n\n* Make REST API `/_admin/shutdown` sleep for only half a second until it\n  initiates the server shutdown. Previously it slept for 2 seconds, but half a\n  second should already be enough to send the server's response out.\n\n* MDS-1001: Performance improvement in AQL. If you are using a traversal like\n  `FOR v, e, p IN <....>` and later in the query access the last vertex on the\n  path e.g.:\n  `FILTER p.vertices[-1].name == \"ArangoDB\"` it will now be transformed to\n  `FILTER v.name == \"ArangoDB\"` which is an equivalent statement. The latter\n  however is cheaper to compute, as we do not need to create an in-memory\n  representation of the path. Furthermore we can apply additional optimizations\n  on `v` which are not possible on `p`. The same optimization holds true for\n  `p.edges[-1]` which is equivalent to `e`. The optimization rule for this is\n  called \"optimize-traversal-last-element-access\".\n\n* FE-142: Updates indices view list & index addition to React.\n\n* A Pregel execution now stores its state during and after execution into a\n  system collection. To read or delete entries the new API\n  `/_api/control_pregel/history[/<id>]` has been added. Additionally, the Pregel\n  JavaScript module has been extended to support access as well.\n  Read history `<PregelModule>.history(<pid>)`.\n  Remove history `<PregelModule>.removeHistory(<pid>)`.\n\n* Marked all memory-mapping options for Pregel as obsolete.\n  The memory mapping code was removed as it did not provide any advantages over\n  spilling into system-provided swap space.\n\n* FE-139 adds new search view type (search-alias).\n\n* Ran automated migrations on all .scss files to remove deprecated division\n  operator usage.\n\n* SEARCH-279: Fix consistency during update/replace operations for arangosearch\n  links and inverted indexes.\n\n* APM-294: Added telemetrics API that gathers anonymous feature usage statistics\n  from a deployment. The API is accessible via the endpoint\n  `/_admin/telemetrics`. The API is enabled by default in release builds, but\n  disabled by default in maintainer mode. It can be explicitly turned on/off\n  with the server startup parameter `--server.telemetrics-api`.\n  The required access privileges to access the telemetrics API can be configured\n  via the server startup option `--server.support-info-api`.\n  The telemetrics API is used by the arangosh: every time the arangosh is\n  started, it will send a request to the connected server to gather telemetrics\n  from the `/_admin/telemetrics` endpoint. The telemetrics data are then sent to\n  an aggregation service that is run by ArangoDB.\n\n* APM-283: Use parallel gather in almost all queries. The only case where we\n  cannot use parallel gather is when using traversals, although there are some\n  exceptions for disjoint SmartGraphs where the traversal can run completely on\n  the local DB-server. All other queries should now be able to parallelize the\n  gather node. This can not only speed up queries quite significantly, but also\n  overcomes issues with the previous serial processing within gather nodes,\n  which could lead to high memory usage on coordinators caused by buffering of\n  documents other shards, and timeouts on some DB-Servers because query parts\n  were idle for too long.\n\n* Changed path were test scripts locate configuration files from `etc/relative`\n  to `etc/testing`. These paths contain `arangosh.conf`, which we were reading\n  from `etc/relative` in test environment.\n\n* Made the return code configurable that is delivered if a write fails because\n  the write concern is not fulfilled (not enough in-sync replicas available).\n  Previously (and now by default), a code of HTTP 403 is returned and the\n  request returns immediately. If the command line option\n    --cluster.failed-write-concern-status-code=503\n  is set, then HTTP 503 is returned. Note that no cluster-internal retry is\n  happening, such that a client is informed right away about the problem.\n  Retry loops have to be organized in the client program.\n\n* Added support for sending gzip-compressed responses from the server.\n  Previously only deflated responses were supported.\n\n* FE-135: Add new Graph Viewer with vis.js and change the UI.\n\n* FE-19: Updated ArangoDB logo in web interface.\n\n* Make the hashed variant of AQL COLLECT support INTO clauses too.\n  Previously only the sorted variant of AQL COLLECT supported INTO clauses.\n\n* Upgraded OpenSSL to 3.0.8.\n\n* FE-174: Change ViewsUI layout to single-page instead of tabs.\n\n* Add peak memory usage to the query object details for queries in the slow\n  query history and in the list of currently running queries. The peak memory\n  usage is also returned via REST APIs as `peakMemoryUsage`.\n\n* Provide options for configuring and enabling RocksDB's blob storage (BlobDB)\n  for large documents in the documents column family.\n  This is currently an experimental feature.\n\n  The following experimental options are available:\n\n  - `--rocksdb.enable-blob-files`: Enable the usage of blob files for the\n    documents column family. This option defaults to `false`. All following\n    options are only relevant if this option is set to `true`.\n  - `--rocksdb.min-blob-size`: Size threshold for storing large documents in\n    blob files (in bytes, 0 = store all documents in blob files).\n  - `--rocksdb.blob-file-size`: Size limit for blob files in the documents\n    column family (in bytes).\n  - `--rocksdb.blob-compression-type`: Compression algorithm to use for blob\n    data in the documents column family.\n  - `--rocksdb.enable-blob-garbage-collection`: Enable blob garbage collection\n    during compaction in the documents column family.\n  - `--rocksdb.blob-garbage-collection-age-cutoff`: Age cutoff for garbage\n    collecting blob files in the documents column family (percentage value from\n    0 to 1 determines how many blob files are garbage collected during\n    compaction).\n  - `--rocksdb.blob-garbage-collection-force-threshold`: Garbage ratio threshold\n    for scheduling targeted compactions for the oldest blob files in the\n    documents column family.\n\n* FE-132: Added query sorting (in web UI) by modified date, option to sort\n  order.\n\n* Partial fix for PRESUPP-539: account for memory used during AQL condition\n  transformation to disjunctive normal form (DNF). This transformation can use\n  a lot of memory for complex filter conditions, which was previously not\n  accounted for. Now, if the transformation uses a lot of memory and hits the\n  configured query memory limit, the query will rather be aborted with a proper\n  error message than overuse memory.\n  For very complex conditions that would use massive amounts of memory when\n  transformed into DNF, the DNF conversion is also aborted at some threshold\n  complexity value. If the threshold is hit, the query continues with a\n  simplified representation of the condition, which will not be usable in index\n  lookups. However, this should still be better than overusing memory or taking\n  a very long time to compute the DNF version.\n  The complexity threshold value can be configured per query by setting the new\n  `maxDNFConditionMembers` query option. There is also a new startup option\n  `--query.max-dnf-condition-members` for coordinators and single servers to\n  adjust the threshold value globally.\n\n* The internal Graph code is completely converted to the new graph engine.\n  Last algorithms added to that lists are: ShortestPath, WeightedShortestPath,\n  KShortestPaths and WeightedKShortestPaths.\n\n* FE-131: Added search input for query page.\n\n* FE-133: Alphabetical sorting for collections on user permissions page.\n\n* Removed CMake variable `ARANGODB_BITS`, which was only used in one place.\n\n* Fixed the issue that the collection view search did not support selecting\n  everything using Ctrl + A.\n\n* APM-592: In batched query results, when executing requests for `/_api/cursor`,\n  there might be a connection error and the user might not be able to retrieve\n  the latest batch from the cursor. For that, a query option flag `allowRetry`\n  was added. When set to `true`, if the latest batch response object wasn't\n  successfully received, the user can send a retry request to receive it with a\n  POST request to `/_api/cursor/<cursorId>/<batchId>`. Only the latest batch is\n  cached, meaning former batches cannot be retrieved again later.\n\n* Use more compact and efficient representation for arrays and objects during\n  AQL AST serialization and deserialization. This can help to reduce the size\n  of messages exchanged between coordinator and database servers during query\n  setup, and also reduce the time needed for parsing these messages. This\n  especially helps when there are large bind parameter values that are arrays or\n  objects.\n  The more efficient format is used also inside an AQL query's \"explain\" and\n  \"profile\" methods, and thus any callers that process the return values of\n  explain and profile operations may now receive the new format. All callers\n  inside the ArangoDB code have been adjusted, but any external callers that\n  process the JSON response values of AQL query explain or profile operations\n  may need to be adjusted to handle the new format.\n\n* Added new stage \"instantiating executors\" to the query profiling output.\n  The time spent in \"instantiating executors\" is the time needed to create the\n  query executors from the final query execution time. In cluster mode, this\n  stage also includes the time needed for physically distributing the query\n  snippets to the participating database servers.\n  Previously, the time spent for instantiating executors and the physical\n  distribution was contained in the \"optimizing plan\" stage, which was\n  misleading.\n\n* Removed constant values for query variables from query plan serialization in\n  cases they were not needed. Previously, constant values of query variables\n  were always serialized for all occurrences of a variable in a query plan.\n  If the constant values were large, this contributed to higher serialization\n  and thus query setup times. Now the constant values are only serialized for\n  relevant parts of query execution plans.\n\n* BTS-199: remove check for environment variable `GLIBCXX_FORCE_NEW` from server\n  start, and remove setting this variable from startup scripts.\n  The reason is that the environment variable only controls the behavior of\n  programs linked against glibc, but our release builds are linked to libmusl.\n\n* Acquire a snapshot of the (list of) indexes when starting document insert,\n  update/replace and remove operations, and use that snapshot throughout the\n  operation. Previously, the list of indexes was acquired multiple times during\n  a write operation, and it was (at least in theory) possible that the list of\n  indexes changed between the individual acquisitions.\n  The PR also contains an optimization to not fetch the full document from the\n  storage engine for remove and replace operations in case the full document is\n  not needed to process the operation. This is the case when the collection does\n  not contain any secondary indexes and `returnOld` is not used.\n\n* Added experimental startup option `--rocksdb.block-cache-jemalloc-allocator`.\n  This option defaults to `false`. When set to `true`, a jemalloc-based memory\n  allocator will be used to allocate memory for the RocksDB block cache.\n  This allocator will also mark the memory of the block cache to be excluded\n  from coredumps, potentially reducing coredump size a lot.\n\n* Remove async mode from pregel.\n\n* Print the pid of the process which sent a SIGABRT or other fatal signal that\n  shuts down ArangoDB ungracefully.\n\n* Avoid write-write conflicts for single document operations performed via the\n  document REST API (i.e., no AQL, but also no streaming transactions). This is\n  achieved by locking the key of each document before performing the actual\n  modification. This lock acquisition effectively serializes all operations on\n  the same document. To avoid starvation, the lock acquisition is limited to\n  one second. This lock timeout value is currently hardcoded but will be made\n  configurable in the future. If the lock cannot be acquired within this time,\n  the operation fails with a write-write conflict error as before.\n\n  Performing changes to a unique index entry also requires us to lock that index\n  entry to ensure uniqueness. This lock acquisition is subject to the same lock\n  timeout as locking the document key.\n\n  We are planning to generalize this for multi-document operations as well as\n  AQL and streaming transactions in the future.\n\n  In case we cannot acquire the lock on the key of the document we want to\n  insert/modify, the error message will be\n  `Timeout waiting to lock key - in index primary of type primary over '_key';\n  conflicting key: <key>` where `<key>` corresponds to the key of the document\n  we tried to modify.\n  In addition, the error object will contain `_key`, `_id` and `_rev` fields.\n  The `_key` and `_id` correspond to the document we tried to insert/modify, and\n  `_rev` will correspond to the current revision of the document from the DB if\n  available, and otherwise empty.\n\n  In case we cannot acquire the lock on a unique index entry, the error message\n  will be `Timeout waiting to lock key - in index <indexName> of type persistent\n  over '<fields>'; document key: <key>; indexed values: [<values>]` where\n  `<indexName>` is the name of the index in which we tried to lock the entry,\n  `<fields>` is the list of fields included in that index, `<key>` corresponds\n  to the key of the document we tried to insert/modify, and `<values>`\n  corresponds to the indexed values from our document.\n  In addition, the error object will contain `_key`, `_id` and `_rev` fields.\n  The `_key` and `_id` correspond to the document we tried to insert/modify, and\n  `_rev` will correspond to the current revision of the document from the DB if\n  available, and otherwise empty.\n\n  This addresses GitHub issue #9702 and APM-522.\n\n* Fixed BTS-418: Suboptimal index range calculation with redundant conditions.\n\n* Added new per-operation option `refillIndexCaches` to write operations,\n  namely:\n\n  - AQL INSERT/UPDATE/REPLACE/REMOVE modification operations\n  - single-document insert, update, replace and remove operations\n  - multi-document insert, update, replace and remove operations\n\n  If the option is set to `true` every currently running transaction will keep\n  track of which in-memory index cache entries were invalidated by the\n  transaction, and will try to (re-)fill them later.\n  Currently edge indexes and velocypack-based indexes (persistent, hash,\n  skiplist index) are supported. For velocypack-based indexes, the refilling\n  will only happen if the index was set up with an in-memory cache (i.e. the\n  `cacheEnabled` flag was set during index creation).\n\n  Example usages:\n  - `db.<collection>.insert({ _from: ..., _to: ..., ... },\n                            { refillIndexCaches: true });`\n  - `db.<collection>.update(key, { _from: ..., _to: ..., ... },\n                                 { refillIndexCaches: true });`\n  - `db.<collection>.replace(key, { _from: ..., _to: ..., ... },\n                                  { refillIndexCaches: true });`\n  - `db.<collection>.remove(key, { refillIndexCaches: true });`\n  - `INSERT { ... } INTO <collection> OPTIONS { refillIndexCaches: true }`\n  - `UPDATE { ... } WITH { ... } INTO <collection> OPTIONS\n                                                   { refillIndexCaches: true }`\n  - `REPLACE { ... } WITH { ... } INTO <collection> OPTIONS\n                                                    { refillIndexCaches: true }`\n  - `REMOVE { ... } IN <collection> OPTIONS { refillIndexCaches: true }`\n\n  The refilling of the in-memory caches for indexes is performed by a background\n  thread, so that the foreground write operation shouldn't be slowed down a lot.\n  The background thread may however cause additional I/O for looking up the data\n  in RocksDB and for repopulating the caches.\n\n  The background refilling is done in a best-effort way and is not guaranteed to\n  always succeed, e.g. if there is no memory available for the cache subsystem,\n  or when an in-memory cache table is currently in a migration phase\n  (grow/shrink operation).\n\n  There is a new startup option `--rocksdb.auto-refill-index-caches-on-modify`\n  for DB-Servers and single servers, which currently defaults to `false`. If it\n  is set to `true`, the cache refilling will be turned on automatically for all\n  insert/update/replace/remove operations, so that it doesn't need to be\n  specified on the per-operation/per-query level.\n\n  The new startup option `--rocksdb.auto-refill-index-caches-queue-capacity` can\n  be used to limit the number of index cache entries that the background thread\n  will queue. This is a safeguard to keep the memory usage at bay in case the\n  background thread is slower than concurrent threads that perform ingestions.\n\n  There are also new startup options to control whether or not the in-memory\n  caches should automatically be seeded upon server restart.\n  The option `--rocksdb.auto-fill-index-caches-on-startup` for DB-Servers and\n  single servers enables this functionality. It currently defaults to `false`.\n  If it is set to `true`, the in-memory caches of all eligible indexes will be\n  automatically pre-seeded after the server startup. Note that this may cause\n  additional CPU and I/O load.\n  The option `--rocksdb.max-concurrent-index-fill-tasks` is available to limit\n  the impact of the automatic index filling at startup. It controls how many\n  full index filling operations can execute concurrently. The lower this number\n  is, the lower the impact of cache filling, but the longer it will take.\n  The default value for this option depends on the number of available cores,\n  and is at least `1`. A value of `0` cannot be used.\n  This option is only relevant if `--rocksdb.auto-fill-index-caches-on-startup`\n  is set to `true`.\n\n  The PR also adds the following metrics:\n  - `rocksdb_cache_auto_refill_loaded_total`: Total number of queued items for\n    in-memory index caches refilling. It will always report a value of zero on\n    coordinators.\n  - `rocksdb_cache_auto_refill_dropped_total`: Total number of dropped items for\n    in-memory index caches refilling (because number of queued items would\n    exceed the value of `--rocksdb.auto-refill-index-caches-queue-capacity`).\n    It will always report a value of zero on coordinators.\n  - `rocksdb_cache_full_index_refills_total`: Total number of in-memory index\n    caches refill operations for entire indexes. The counter gets increased for\n    every index automatically loaded (because startup option\n    `--rocksdb.auto-fill-index-caches-on-startup` is set to `true`) or when full\n    indexes are loaded into memory manually.\n    In cluster deployments the counter will be increased once per eligible index\n    per shard. It will always report a value of zero on coordinators.\n\n* BTS-128: Fixed http request not working when content-type is velocypack.\n\n* Deleted customizable Pregel (AIR) and Greenspun library.\n\n* Add support for terabyte units (t, tb, T, TB, tib, TiB, TIB) in startup\n  options.\n\n* Make the deprecated `--server.disable-authentication-unix-sockets` and\n  `--server.disable-authentication` startup options obsolete. They were\n  deprecated in v3.0 and mapped to `--server.authentication` and\n  `--server.authentication-unix-sockets`, which made them do the opposite of\n  what their names suggest.\n\n* Log startup warnings for any experimental, deprecated, obsolete or renamed\n  options at startup of arangod or any of the client tools.\n\n* Added option to exclude system collection from rebalance shards plan.\n\n* Improve performance and memory usage of IN list lookups for hash, skiplist and\n  persistent indexes.\n\n* Improve memory usage tracking for IN list lookups and other RocksDB-based\n  lookups.\n\n* Remove inactive query plan cache code (was only a stub and never enabled\n  before).\n\n* Fixed BTS-441: Honor read only mode with disabled authentication\n\n* Obsolete startup option `--database.force-sync-properties`. This option was\n  useful with the MMFiles storage engine, but didn't have any useful effect when\n  used with the RocksDB engine.\n\n* BTS-483: Added restriction for usage of query cache for streaming and JS\n  transactions when they are not read-only.\n\n* Remove map and map.gz files from repository and add them to gitignore.\n  These files are only used for debugging and therefore should not be included\n  in any release. This also reduces the size of release packages.\n\n* Improved help texts for the collection type and satellite collection options\n  in the web UI.\n\n* Deprecate the startup option `--agency.pool-size`. This option was never\n  properly supported for any values other than the value of `--agency.size`.\n  Now any value set for `--agency.pool-size` other than the value set for\n  `--agency.size` will now produce a fatal error on startup.\n\n* BTS-1082: Updating properties of a satellite collection breaks\n  replicationFactor.\n\n* BTS-209: Fixed requests to `_admin/execute` treating every payload as plain\n  text when they're in JSON or velocypack format, but will only treat the\n  payload as velocypack if specified in the header's `content-type`.\n\n* Fixed issue #17394: Unnecessary document-lookup instead of Index-Only query.\n  This change improves projection handling so that more projections can be\n  served from indexes.\n\n* Change default output format of arangoexport from `json` to `jsonl`.\n\n* BTS-941: The HTTP API now delivers the correct list of the collection's shards\n  in case a collection from an EnterpriseGraph, SmartGraph, Disjoint\n  EnterpriseGraph, Disjoint SmartGraph or SatelliteGraph is being used.\n\n* BTS-465: Added tests for RandomGenerator and warning that other options for\n  creating random values that are not Mersenne are deprecated.\n\n* BTS-977: Added an error message for when an unauthorized user makes an HTTP\n  GET request to current database from a database name that exists which the\n  user can't access and from a database name that doesn't exist, so both\n  requests have the same error message (`_db/<dbName>/_api/database/current`).\n\n* Added new AQL function SHA256(value).\n\n* Added index cleanup in Supervision. If an index was not created successfully\n  and the coordinator which initiated the creation was rebooted or is dead, then\n  the agency Supervision will drop the index again. If it was created\n  successfully, the agency Supervision will finalize it.\n\n* BTS-742: Added restriction for, when in smart graph, not accepting satellites\n  in invalid format when storing a graph (like `{satellites: null}`).\n\n* BTS-477: added integration tests for covering log parameters.\n\n* Moved the handling of escaping control and unicode chars in the log to the\n  Logger instead of LogAppenderFile.\n\n* Added authenticate header to the HTTP response when status code is 401 for\n  HTTP/2.\n\n* Best quality spam pushed down to DEBUG.\n\n* Fixed log with json format not respecting the value of parameter\n  `--log.shorten-filenames`.\n\n* Added \"intermediateCommits\" statistics return value for AQL queries, to relay\n  the number of intermediate commits back that a write query performed.\n\n* Added message on the UI view of Logs when the user has restricted access,\n  either because cannot access `_system`, or because is currently in another\n  database.\n\n* Fix for the Pregel's HITS algorithm using a fixed value instead of the passed\n  \"threshold\" parameter. The same applied to the new HITSKleinberg.\n\n* Now the Pregel API returns `{... algorithm: \"pagerank\", ...}` instead of\n  `{... algorithm: \"PageRank\", ...}` when the Page Rank algorithm is run (in\n  accordance to the documentation).\n\n* Added integration tests for `--log.escape-control-chars` and\n  `--log.escape-unicode-chars`.\n\n* A new Pregel algorithm: the version of Hypertext-Induced Topic Search (HITS)\n  as described in the original paper.\n\n* BTS-428: Added function DATE_ISOWEEKYEAR that retrieves the number of the week\n  counting from when the year started in ISO calendar and also the year it's in.\n\n* Added handling of requests with Transfer-Encoding chunked, which is not\n  implemented, so returns code HTTP code 501.\n\n* Disallowed index creation that covers fields in which the field's name starts\n  or ends with `:` for single server or cluster when the instance is a\n  coordinator or single server. This validation only happens for index creation,\n  so already existing indexes that might use such field names will remain as\n  they are.\n\n\nv3.10.6 (2023-04-27)\n--------------------\n\n* Fixed BTS-1292: Added automatic cleanup of dangling ArangoSearch links.\n\n* Automatically repair revision trees after several failed shard synchronization\n  attempts. This can help to get permanently out-of-sync shards back into sync.\n\n  The functionality can be turned off by setting the startup option\n  `--replication.auto-repair-revision-trees` to `false` on DB-Servers.\n\n* SEARCH-466 Fix leaking into individual link definition inherited properties\n  from view.\n\n* Fix race condition in invalidation of token cache on coordinators.\n\n* Adjusted timeouts for cluster internal commit and abort requests to withstand\n  network delays better. This fixes some problems when the networking\n  infrastructure delays requests.\n\n* Added sent time accounting and some metrics to fuerte and the NetworkFeature.\n  This can detect delays in the network infrastructure.\n\n* Added startup option `--server.ensure-whitespace-metrics-format`, which\n  controls whether additional whitespace is used in the metrics output format.\n  If set to `true`, then whitespace is emitted between the exported metric value\n  and the preceeding token (metric name or labels).\n  Using whitespace may be required to make the metrics output compatible with\n  some processing tools, although Prometheus itself doesn't need it.\n\n  The option defaults to `true`, which adds additional whitespace by default.\n\n* SEARCH-461: Added option \"--arangosearch.columns-cache-only-leader\". Used only\n  on EE DBServers. Default is false.\n  If set to true only leader shards have ArangoSearch caches enabled - this will\n  reduce RAM usage. In case of failover happens - in background caches are\n  populated for the new leader. Some queries that run at during a failover may\n  still run without caches.\n\n* BTS-1148: Fix a race when aborting/finishing a currently active query on a\n  DB-Server. This race could cause the query to remain in the server's query\n  registry longer than intended, potentially holding some locks. Such queries\n  were garbage collected eventually, but this could take a while, depending on\n  the specified TTL (10min per default).\n  This has now been fixed so that aborted/finished queries are cleaned up in a\n  timely manner.\n\n* MDS-1098: In 3.10 we have introduced an optimization on Traversals to pull\n  post-filter conditions into the traversal-statements, like the following:\n\n      FOR v,e,p IN 10 OUTBOUND @start GRAPH \"myGraph\"\n        FILTER v.isRelevant == true\n        RETURN p\n\n  If the comparison side contains a variable and the same variable is used as\n  the start vertex e.g. like this:\n\n  FOR candidate IN [\"vertices/1\", \"vertices/2\"]\n    FOR v,e,p IN 1 OUTBOUND candidate GRAPH \"myGraph\"\n      FILTER e.MostLikedNeighbor == candidate\n      RETURN v\n\n  There is a chance that we prematurely discarded this variable (candidate in\n  the example) if it is not used later. This has lead to incorrect results.\n\n* Fixed statistics values for writes executed and writes ignored when a query is\n  executed using the rule `optimize-cluster-single-document-operations`.\n  It was always increasing the amount of writes executed, even if the operation\n  wasn't successful, and also never increasing the amount of writes ignored when\n  needed.\n\n* Updated arangosync to v2.16.1.\n\n* Fix potential thread starvation in in-memory edge cache.\n\n* SEARCH-300: Fixed a rare case when arangosearch data folders might be left on\n  disk after database is dropped.\n\n* Fixed SEARCH-459 Fixed reporting ArangoSearch inverted index properties from\n  ensureIndex request.\n\n* Changed path where test scripts locate configuration files from `etc/relative`\n  to `etc/testing`. These paths contain `arangosh.conf`, which we were reading\n  from `etc/relative` in test environment.\n\n* Fix issues with deferred database creation:\n\n  When a database has made it into the Plan part of the agency with some\n  settings, e.g. `replicationFactor` that would violate the current settings for\n  databases (e.g. `--cluster.min-replication-factor` and\n  `--cluster.max-replication-factor`), and then a new DB server is added, it\n  will try to create the database locally with the settings from the Plan.\n  As these settings however violate the min/max replication factor values, the\n  database is not created on the new DB server and an error is written into\n  Current instead.\n  This can cause follow-up errors and the PlanSyncer complaining about missing\n  databases for analyzers etc.\n\n* Fixed ES-1508: (EE only) when deleting edges in a SmartGraph via\n  DELETE /_api/document/{collection} using _key or _id values as document\n  selectors, the INBOUND and OUTBOUND entries of the SmartEdges could diverge.\n  Using a document like {_key: \"xxxx\"} as a selector was always correct. Now\n  _key and _id variants are supported as intended.\n\n* Fixed single-to-single replication that used HTTP authentication to\n  authenticate requests on the leader. This could be broken if the collections\n  on the leader were created with 3.8 or later, and thus used the Merkle tree\n  protocol to exchange different document revisions.\n  When using HTTP authentication, the prefetching code for document revisions\n  did not pass on the authentication credentials, so the leader could reject\n  requests with HTTP 401 or HTTP 403, and replication failed.\n  Replication in the cluster and replication using JWT authentication were not\n  affected.\n\n* Added the following metrics for WAL file tracking:\n  - `rocksdb_live_wal_files_size`: cumulated size of alive WAL files (not\n    archived)\n  - `rocksdb_archived_wal_files_size`: cumulated size of archive WAL files\n\n* By default, start pruning of archived WAL files 60 seconds after server\n  start. Previously, pruning of WAL files started 180 seconds after server\n  startup.\n\n* Set default threshold value for automatic column flushing to 20 live WAL\n  files (previously: 10 files), and retry flushing every 30 minutes (previous\n  interval: every 60 minutes).\n\n* BTS-1272: Fixed metric `arangodb_connection_pool_connections_current`. In some\n  cases where multiple connections to a server are canceled the metric could\n  miss-count, as for now it only counted individually closed connections.\n  The wrong counted situations are: other server crashes, restore of a\n  HotBackup, rotation of JWT secret.\n\n* Added support to log response bodies as well as HTTP headers (incoming\n  and outgoing), when the requests log topic is set to TRACE.\n\n\nv3.10.5 (2023-03-16)\n--------------------\n\n* Stabilized resilience tests. The assumption that an AQL query can run\n  without error directly after a leader has been stopped, is wrong.\n\n* Auto-flush RocksDB WAL files and in-memory column family data if the number of\n  live WAL files exceeds a certain threshold. This is to make sure that WAL\n  files are moved to the archive when there are a lot of live WAL files present\n  (e.g. after a restart; in this case RocksDB does not count any previously\n  existing WAL files when calculating the size of WAL files and comparing it\n  `max_total_wal_size`.\n  The feature can be configured via the following startup options:\n  - `--rocksdb.auto-flush-min-live-wal-files`: minimum number of live WAL files\n    that triggers an auto-flush. Defaults to `10`.\n  - `--rocksdb.auto-flush-check-interval`: interval (in seconds) in which\n    auto-flushes are executed. Defaults to `3600`.\n  Note that an auto-flush is only executed if the number of live WAL files\n  exceeds the configured threshold and the last auto-flush is longer ago than\n  the configured auto-flush check interval. That way too frequent auto-flushes\n  can be avoided.\n\n* Fix potential memory under-accounting on cache shutdown for in-memory caches\n  for edge indexes.\n\n* Added the following metrics for WAL file tracking:\n  - `rocksdb_live_wal_files`: number of alive WAL files (not archived)\n  - `rocksdb_wal_released_tick_flush`: lower bound sequence number from which\n    onwards WAL files will be kept (i.e. not deleted from the archive) because\n    of external flushing needs. Candidates for these are arangosearch links and\n    background index creation.\n  - `rocksdb_wal_released_tick_replication`: lower bound sequence number from\n    which onwards WAL files will be kept because they may be needed by the\n    replication.\n  - `arangodb_flush_subscriptions`: number of currently active flush\n    subscriptions.\n\n* Updated internal JavaScript dependencies:\n\n  - @xmldom/xmldom: 0.8.0 -> 0.8.6\n  - accepts: 1.3.7 -> 1.3.8\n  - ajv: 8.10.0 -> 8.12.0\n  - ansi_up: 5.0.1 -> 5.1.0\n  - content-disposition: 0.5.3 -> 0.5.4\n  - content-type: 1.0.4 -> 1.0.5\n  - error-stack-parser: 2.0.6 -> 2.1.4\n  - mime-types: 2.1.31 -> 2.1.35\n  - semver: 7.3.5 -> 7.3.8\n\n* Updated transitive JS dependency hoek to @hapi/hoek@8.5.1 to resolve\n  CVE-2020-36604 in joi.\n\n* Updated JS dependency minimatch to 3.1.2 to resolve CVE-2022-3517.\n\n* Updated JS dependency qs to 6.11.0 to resolve CVE-2022-24999.\n\n* Updated arangosync to v2.15.0.\n\n* Allow usage of projections and covering indexes in more cases.\n  Previously, projections were not used if there were complex filter conditions\n  on the index attribute(s) that contained the `[*]` expansion operator with\n  inline FILTERs or RETURNs, e.g. `FILTER doc.addrs[* FILTER CURRENT.country ==\n  'US'].zip`.\n\n* PRESUPP-546: make AQL optimizer rule `simplify-conditions` correctly report\n  that it was triggered. Previously that rule never reported that it was\n  triggered although even though it actually was.\n\n* Added startup option `--rocksdb.auto-refill-index-caches-on-followers` to\n  control whether automatic refilling of in-memory caches should happen on\n  followers or just leaders. The default value is `true`, i.e. refilling happens\n  on followers too.\n\n* Added new geo_s2 ArangoSearch analyzer (Enterprise Only).\n\n* GORDO-1554: Fixes invalid document insertion with invalid user-specified keys\n  (e.g. numeric values) into EnterpriseGraph related vertices.\n\n* Added metric `arangodb_replication_clients` showing the number of currently\n  active/connected replication clients for a server.\n\n* BTS-1249: Add startup option `--foxx.enable`.\n  This startup option determines whether access to user-defined Foxx services is\n  possible for the instance. It defaults to `true`.\n  If the option is set to `false`, access to Foxx services is forbidden and will\n  be responded with an HTTP 403 Forbidden error. Access to ArangoDB's built-in\n  web interface, which is also a Foxx service, is still possible even with the\n  option set to `false`.\n  When setting the option to `false`, access to the management APIs for Foxx\n  services will also be disabled. This is the same as manually setting the\n  option `--foxx.api false`.\n\n* Fixed a bug in the API used by `arangorestore`: On restore, a new _rev value\n  is generated for each imported document to avoid clashes with previously\n  present data. This must be created on the shard leader rather than the\n  coordinator. The bug happened, when two coordinators were creating the same\n  _rev value for two different documents concurrently.\n\n* ES-1428: make the maximum number of V8 contexts depend on the maximum number\n  of server threads, if `--javascript.v8-contexts` is not set explicitly.\n  Previously the maximum number of V8 contexts was hard-coded to 16 when the\n  option `--javascript.v8-contexts` option was not set explicitly.\n  Now the maximum number defaults to 7/8 of the value of the startup option\n  `--server.maximal-threads`, regardless of if it is explicitly configured or\n  the default value is used. Only 7/8 are used to leave some headroom for other\n  important maintenance tasks.\n  A server with default configuration should now not block waiting for V8\n  contexts to become available, but it may use more memory for the additional V8\n  contexts if there are many concurrent requests that invoke JavaScript actions\n  (e.g. requests using the web UI or Foxx).\n\n* Improve memory usage of in-memory edge index cache if most of the edges in an\n  index refer to a single or mostly the same collection.\n  Previously the full edge ids, consisting of the the referred-to collection\n  name and the referred-to key of the edge were stored in full. Now, the first\n  edge inserted into an edge index' in-memory cache will determine the\n  collection name for which all corresponding edges can be prefix-compressed.\n  For example, when inserting an edge pointing to `the-collection/abc` into the\n  empty cache, the collection name `the-collection` will be noted for that cache\n  as a prefix. The edge will be stored in memory as only `/abc`. Further edges\n  that are inserted into the cache and that point to the same collection will\n  also be stored prefix-compressed.\n  The prefix compression is transparent and does not require configuration or\n  setup. Compression is done separately for each cache, i.e. a separate prefix\n  can be used for each individual edge index, and separately for the `_from` and\n  `_to` parts. Lookups from the in-memory edge cache will not return compressed\n  values but the full-length edge ids. The compressed values will also be used\n  in memory only and will not be persisted on disk.\n\n\nv3.10.4 (2023-02-19)\n--------------------\n\n* Updated ArangoDB Starter to 0.15.7.\n\n* Updated OpenSSL to 1.1.1t and OpenLDAP to 2.6.4.\n\n* BTS-1184: Fixed index hint with `forceIndexHint` set to true not being used on\n  query when geo index was present, because it would override the choice of the\n  index hint with optimizations related to it.\n\n* Fixed EE: Concurrent batch insert/update CRUD operations into\n  SmartEdgeCollections on conflicting edge keys could get the smart edge caching\n  out-of-sync, which would yield different results for OUTBOUND/INBOUND search\n  over edges. This is now fixed, however there is now a slightly higher chance\n  to get a CONFLICT response back on those queries.\n\n* Return peak memory usage and execution time as part of query explain result.\n  This helps finding queries that use a lot of memory to build the execution\n  plan.\n\n* Made all transactions used by the gharial API on coordinators and a few others\n  marked \"globally managed\". This fixes an issue where transaction conflicts\n  could lead to a silent out of sync situation between a leader shard and its\n  followers.\n\n* BTS-1219: Fix cost estimation for geo index usage and for collection\n  enumeration with included filtering. This fixes a regression from 3.9 where a\n  geo index was no longer used because of an optimizer rule, which gained new\n  powers, and wrong cost estimations for execution plans.\n\n* Allow usage of document projections and traversal projections in slightly more\n  cases, specifically when the document's or traversal's output variables were\n  used in subqueries. Previously the usage of the document or traversal output\n  variables in subqueries could lead to projections being disabled.\n\n* Improved optimization of functions to be covered by Traversals. Now more\n  functions should be optimized into the traversal, and some that are not valid\n  should not be optimized anymore. Fixes #16589.\n\n* BTS-1193: Fix for schema update. When removing a field and then inserting a\n  new field into the schema, previously, both old and new schema would be\n  merged, meaning it would maintain the old field and add the new one.\n\n* Fixed issue #18053: Computed Values become null when Schema is modified.\n\n* Set the cache_oblivious option of jemalloc to `false` by default. This helps\n  to save 4096 bytes of RAM for every allocation which is at least 16384 bytes\n  large. This is particularly beneficial for the RocksDB buffer cache.\n\n* Added startup option `--javascript.user-defined-functions`.\n  This option controls whether JavaScript user-defined functions (UDFs) can be\n  used in AQL queries. The option defaults to `true`. The option can be set to\n  `false` to disallow using JavaScript UDFs from inside AQL queries.\n  In that case, a parse error will be thrown when trying to run a query that\n  invokes a UDF.\n\n* Allowing enabling/disabling supervision maintenance mode also via followers in\n  active failover mode. Previously the supervision maintenance mode could only\n  be enabled/disabled by making a call to the active failover leader.\n\n* BTS-266: When starting up a cluster without `--cluster.force-one-shard`,\n  creating a database and then restarting the cluster with the startup option\n  `--cluster.force-one-shard` set to true, when the formerly created database\n  has more than one shard, but the flag is set to true, this could lead to\n  arangosearch's analyzers to use optimizations that should not be used if not\n  in a single shard mode. For this not to happen, the verification of the\n  parameter being true as a condition to run optimizations was removed.\n\n* Activate RDB_CoveringIterator and use it for some geo index queries.\n  This speeds up and simplifies geo queries with geo index which do not use\n  GEO_DISTANCE.\n\n\nv3.10.3 (2023-01-23)\n--------------------\n\n* Log information about follower state/apply progress in supervision job that\n  organizes failover in active failover mode.\n\n* Updated arangosync to v2.14.0.\n\n* Updated ArangoDB Starter to 0.15.6.\n\n* Fix bug in hotbackup download/restore to make sure no data is mixed up between\n  servers. This fixes a bug introduced in 3.10. Note that previous 3.10 versions\n  may not correctly restore a hotbackup which was uploaded from one cluster and\n  downloaded into another.\n\n* ES-1396: under some rare circumstances it was possible that background index\n  creation missed some documents in case the documents were inserted after\n  background index creation started and the corresponding WAL files with the\n  inserts were already removed before background indexing caught up.\n\n* Web UI [FE-48]: Additional fix to the previously introduced license\n  information usability improvement. In case the server is being started with\n  the additional parameter `--server.harden`, the previous fix did not handle\n  that specific edge case.\n\n* BTS-413: Added more explanatory messages for when the user cannot see the\n  statistics for a node in the UI when in cluster mode.\n\n* Fix coordinator segfault in AQL queries in which the query is invoked from\n  within a JavaScript context (e.g. from Foxx or from the server's console mode)\n  **and** the query has multiple coordinator snippets of which except the\n  outermost one invokes a JavaScript function.\n  Instead of crashing, coordinators will now respond with the exception \"no v8\n  context available to enter for current transaction context\".\n  For AQL queries that called one of the AQL functions `CALL` or `APPLY` with a\n  fixed function name, e.g. `APPLY('CONCAT', ...)`, it is now also assumed\n  correctly that no JavaScript is needed, except if the fixed function name is\n  the name of a user-defined function.\n  This fixes an issue described in OASIS-24962.\n\n* BTS-1192: fix a potential race during hot backup creation, which could result\n  in error messages such as `{backup} Source file engine_rocksdb/002516.sst does\n  not have a hash file.` during hot backup creation. However, despite the error\n  message being logged, the hot backup was still complete.\n\n* Prevent agency configuration confusion by an agent which comes back without\n  its data directory and thus without its UUID.\n\n* Change the request lane for replication catchup requests that leaders in\n  active failover receive from their followers from medium to high. This will\n  give catchup requests from followers highest priority, so that the leader will\n  preferrably execute them compared to regular requests.\n\n* Allow cluster database servers to start even when there are existing databases\n  that would violate the settings `--cluster.min-replication-factor` or\n  `--cluster.max-replication-factor`.\n  This allows upgrading from older versions in which the replication factor\n  validation for databases was not yet present.\n\n* Remove constant values for query variables from query plan serialization in\n  cases they were not needed. Previously, constant values of query variables\n  were always serialized for all occurrences of a variable in a query plan.\n  If the constant values were large, this contributed to higher serialization\n  and thus query setup times. Now the constant values are only serialized for\n  relevant parts of query execution plans.\n\n* Added startup option `--rocksdb.bloom-filter-bits-per-key` to configure the\n  average number of bits to use per key in a Bloom filter.\n\n* Make the cache_oblivious option of jemalloc configurable from the environment.\n  This helps to save 4096 bytes of RAM for every allocation which is at least\n  16384 bytes large. This is particularly beneficial for the RocksDB buffer\n  cache.\n\n* Improve performance of RocksDB's transaction lock manager by using different\n  container types for the locked keys maps.\n  This can improve performance of write-heavy operations that are not I/O-bound\n  by up to 10%.\n\n\nv3.10.2 (2022-12-16)\n--------------------\n\n* Added experimental per-operation option `refillIndexCaches` to write\n  operations, namely:\n\n  - AQL INSERT/UPDATE/REPLACE/REMOVE modification operations\n  - single-document insert, update, replace and remove operations\n  - multi-document insert, update, replace and remove operations\n\n  If the option is set to `true` every currently running transaction will keep\n  track of which in-memory edge index cache entries were invalidated by the\n  transaction, and will try to (re-)fill them later.\n  Currently only edge indexes are supported. ArangoDB 3.11 will add support for\n  velocypack-based indexes (persistent, hash, sklipist index).\n\n  Example usages:\n  - `db.<collection>.insert({ _from: ..., _to: ..., ... },\n    { refillIndexCaches: true });`\n  - `db.<collection>.update(key, { _from: ..., _to: ..., ... },\n    { refillIndexCaches: true });`\n  - `db.<collection>.replace(key, { _from: ..., _to: ..., ... },\n    { refillIndexCaches: true });`\n  - `db.<collection>.remove(key, { refillIndexCaches: true });`\n  - `INSERT { ... } INTO <collection> OPTIONS { refillIndexCaches: true }`\n  - `UPDATE { ... } WITH { ... } INTO <collection> OPTIONS { refillIndexCaches:\n    true }`\n  - `REPLACE { ... } WITH { ... } INTO <collection> OPTIONS { refillIndexCaches:\n    true }`\n  - `REMOVE { ... } IN <collection> OPTIONS { refillIndexCaches: true }`\n\n  The refilling of the in-memory caches for indexes is performed by a background\n  thread, so that the foreground write operation shouldn't be slowed down a lot.\n  The background thread may however cause additional I/O for looking up the data\n  in RocksDB and for repopulating the caches.\n\n  The background refilling is done in a best-effort way and is not guaranteed to\n  always succeed, e.g. if there is no memory available for the cache subsystem,\n  or when an in-memory cache table is currently in a migration phase(grow/shrink\n  operation).\n\n  There is a new experimental startup option\n  `--rocksdb.auto-refill-index-caches-on-modify` for DB-Servers and single\n  servers, which currently defaults to `false`. If it is set to `true`, the\n  cache refilling will be turned on automatically for all\n  insert/update/replace/remove operations, so that it doesn't need to be\n  specified on the per-operation/per-query level.\n\n  The experimental option `--rocksdb.auto-refill-index-caches-queue-capacity`\n  can be used to limit the number of index cache entries that the background\n  thread will queue. This is a safeguard to keep the memory usage at bay in case\n  the background thread is slower than concurrent threads that perform\n  ingestions.\n\n  There are also new experimental startup options to control whether or not the\n  in-memory caches should automatically be seeded upon server restart.\n  The option `--rocksdb.auto-fill-index-caches-on-startup` for DB-Servers and\n  single servers enables this functionality. It currently defaults to `false`.\n  If it is set to `true`, the in-memory caches of all eligible indexes will be\n  automatically pre-seeded after the server startup. Note that this may cause\n  additional CPU and I/O load.\n  The option `--rocksdb.max-concurrent-index-fill-tasks` is available to limit\n  the impact of the automatic index filling at startup. It controls how many\n  full index filling operations can execute concurrently. The lower this number\n  is, the lower the impact of cache filling, but the longer it will take.\n  The default value for this option depends on the number of available cores,\n  and is at least `1`. A value of `0` cannot be used.\n  This option is only relevant if `--rocksdb.auto-fill-index-caches-on-startup`\n  is set to `true`.\n\n  The PR also adds the following metrics:\n  - `rocksdb_cache_auto_refill_loaded_total`: Total number of queued items for\n    in-memory index caches refilling. It will always report a value of zero on\n    coordinators.\n  - `rocksdb_cache_auto_refill_dropped_total`: Total number of dropped items for\n    in-memory index caches refilling (because number of queued items would\n    exceed the value of `--rocksdb.auto-refill-index-caches-queue-capacity`).\n    It will always report a value of zero on coordinators.\n  - `rocksdb_cache_full_index_refills_total`: Total number of in-memory index\n    caches refill operations for entire indexes. The counter gets increased for\n    every index automatically loaded (because startup option\n    `--rocksdb.auto-fill-index-caches-on-startup` is set to `true`) or when full\n    indexes are loaded into memory manually.\n    In cluster deployments the counter will be increased once per eligible index\n    per shard. It will always report a value of zero on coordinators.\n\n* Use intermediate commits in old shard synchronization protocol. This avoids\n  overly large RocksDB transactions when syncing large shards, which is a remedy\n  for OOM kills during restarts.\n\n* Added a configuration option (for the agency):\n    --agency.supervision-failed-leader-adds-follower\n  with a default of `true` (behavior as before). If set to `false`, a\n  `FailedLeader` job does not automatically configure a new shard follower,\n  thereby preventing unnecessary network traffic, CPU and IO load for the case\n  that the server comes back quickly. If the server is permanently failed, an\n  `AddFollower` job will be created anyway eventually.\n\n* Max value of minhash_value was set to 2^53 - 1 (9007199254740991) to stay in\n  safe integer limits for javascript.\n\n* SEARCH-433 Fix Inverted index fields presence checks for IN clauses.\n\n* BTS-1141: Changed the default value of startup option\n  `--rocksdb.enforce-block-cache-size-limit` from `true` to `false`.\n  This change prevents RocksDB from going into read-only mode when an internal\n  operation tries to insert some value into the block cache, but can't do so\n  because the block cache's capacity limit is reached.\n\n* Don't log Boost ASIO warnings such as `asio IO error: 'stream truncated'` when\n  a peer closes an SSL/TLS connection without performing a proper connection\n  shutdown.\n\n* Disallow creating new databases with a `replicationFactor` value set to a\n  value lower than `--cluster.min-replication-factor` or higher than\n  `--cluster.max-replication-factor`. Previously the `replicationFactor`\n  settings for new databases were not bounds-checked, only for new collections.\n\n* Fixed Github issue #16451: In certain situations, a LIMIT inside a subquery\n  could erroneously reduce the number of results of the containing (sub)query.\n\n* Added a feature to the ResignLeadership job. By default, it will now\n  undo the leader changes automatically after the server is restarted,\n  unless the option `undoMoves` is set to `false`. This will help to\n  make rolling upgrades and restarts less troublesome, since the shard\n  leaderships will not get unbalanced.\n\n* Add missing metrics for user traffic: Histograms:\n    `arangodb_client_user_connection_statistics_bytes_received`\n    `arangodb_client_user_connection_statistics_bytes_sent`\n  These numbers were so far only published via the statistics API.\n  This is needed for Oasis traffic accounting.\n\n* Added agency options\n    --agency.supervision-delay-add-follower\n  and\n    --agency.supervision-delay-failed-follower\n  to delay supervision actions for a configurable amount of seconds. This is\n  desirable in case a DBServer fails and comes back quickly, because it gives\n  the cluster a chance to get in sync and fully resilient without deploying\n  additional shard replicas and thus without causing any data imbalance.\n\n* Enable \"collect-in-cluster\" optimizer rule for SmartGraph edge collections.\n\n* Fixed SEARCH-408 Added \"cache\" columns feature for ArangoSearch.\n\n* Fixed SEARCH-427 Fixed Inverted index usage of field with trackListPositions\n  enabled.\n\n* Fix HTTP/VST traffic accounting in internal statistics / metrics.\n\n* Add serverId parameter to _admin/log/level. Allows to forward the request to\n  other servers.\n\n* Delay a MoveShard operation for leader change, until the old leader has\n  actually assumed its leadership and until the new leader is actually in sync.\n  This fixes a bug which could block a shard under certain circumstances. This\n  fixes BTS-1110.\n\n* Updated arangosync to v2.13.0.\n\n* Fixed issue #17291: Server crash on error in the PRUNE expression.\n  Traversal PRUNE expressions containing JavaScript user-defined functions\n  (UDFs) are now properly rejected in single server and cluster mode.\n  PRUNE expressions that use UDFs require a V8 context for execution, which is\n  not available on DB-servers in a cluster, and also isn't necessarily available\n  for regular queries on single servers (a V8 context is only available if a\n  query was executed inside Foxx or from inside a JS transaction, but not\n  otherwise).\n\n* Removed more assertions from cluster rebalance js test that obligated the\n  rebalance plan to always have moves, but there were cases in which all there\n  are none.\n\n* Fix setting query memory limit to 0 for certain queries if a global memory\n  limit is set, but overriding the memory limit is allowed.\n\n* Do not query vertex data in K_PATHS queries if vertex data is not needed.\n\n* BTS-1075: AQL: RETURN DOCUMENT (\"\") inconsistent - single server vs cluster.\n\n* Repair \"load indexes into memory\" function in the web UI.\n\n* Fixed issue #17367: FILTER fails when using negation (!) on variable whose\n  name starts with \"in\". Add trailing context to NOT IN token.\n\n* Fix disk space metrics rocksdb_free_disk_space and rocksdb_total_disk_space\n  on macOS. Previously, they seem to have reported wrong values.\n\n* Show number of HTTP requests in cluster query profiles.\n\n* Removed assertions from cluster rebalance js test that obligated the rebalance\n  plan to always have moves, but there were cases in which all there are none.\n\n* Improved the syntax highlighter for AQL queries in the web interface\n  with support for multi-line strings, multi-line identifiers in forward\n  and backticks, colorization of escape sequences, separate tokens for\n  pseudo-keywords and pseudo-variables, an updated regex for numbers, and\n  the addition of the AT LEAST and WITH COUNT INTO constructs.\n\n\nv3.10.1 (2022-11-04)\n--------------------\n\n* Added detailed explanations for some startup options.\n  They are only exposed via `--dump-options` under the `longDescription` key.\n\n* Updated OpenSSL to 1.1.1s.\n\n* Solve a case of excessive memory consumption in certain AQL queries with IN\n  filters with very long lists. Free sub-iterators as soon as they are\n  exhausted.\n\n* BTS-1070: Fixed query explain not dealing with an aggregate function without\n  arguments and the WINDOW node not being defined as an Ast node type name.\n\n* Updating properties of a satellite collection breaks replicationFactor.\n\n* Log the documents counts on leader and follower shards at the end of each\n  successful shard synchronization.\n\n* Remove superfluous dash character from startup option name\n  `--temp.-intermediate-results-encryption-hardware-acceleration`.\n\n* FE-159: When creating a database in cluster mode, there are several parameters\n  required. However they are invisible (nothing shown) if I open DB settings\n  after creation. Those settings should be visible in readonly mode (grey out).\n\n* Added startup option `--query.log-failed` to optionally log all failed AQL\n  queries to the server log. The option is turned off by default.\n\n* Added startup option `--query.log-memory-usage-threshold` to optionally log\n  all AQL queries that have a peak memory usage larger than the configured\n  value. The default value is 4GB.\n\n* Added startup option `--query.max-artifact-log-length` to control the maximum\n  length of logged query strings and bind parameter values.\n  This allows truncating overly long query strings and bind parameter values to\n  a reasonable length. Previously the cutoff length was hard-coded.\n\n* Improve cardinality estimate for AQL EnumerateCollectionNode in case a `SORT\n  RAND() LIMIT 1` is used. Here, the estimated number of items is at most 1.\n\n* Improved shard distribution during collection creation.\n\n* Changed the encoding of revision ids returned by the following REST APIs:\n  - GET /_api/collection/<collection-name>/revision: the revision id was\n    previously returned as numeric value, and now it will be returned as\n    a string value with either numeric encoding or HLC-encoding inside.\n  - GET /_api/collection/<collection-name>/checksum: the revision id in the\n    \"revision\" attribute was previously encoded as a numeric value in single\n    server, and as a string in cluster. This is now unified so that the\n    \"revision\" attribute always contains a string value with either numeric\n    encoding or HLC-encoding inside.\n\n* Fixed handling of empty URL parameters in HTTP request handling.\n\n* Fixed diffing of completely non-overlapping revision trees, which could lead\n  to out-of-bounds reads at the right end of the first (smaller) tree.\n\n* Fixed aborting the server process if an exception was thrown in C++ code that\n  was invoked from the llhttp C code dispatcher. That dispatcher code couldn't\n  handle C++ exceptions properly.\n\n* Fixed BTS-1073: Fix encoding and decoding of revision ids in replication\n  incremental sync protocol. Previously, the encoding of revision ids could be\n  ambiguous under some circumstances, which could prevent shards from getting\n  into sync.\n\n* ES-1312: fix handling of reaching the WAL archive capacity limit.\n\n* Log better diagnosis information in case multiple servers in a cluster are\n  configured to use the same endpoint.\n\n* BTS-908: Fixed WebUI GraphViewer not being able to create a new edge relation\n  between two nodes in cases where only one edge definition has been defined\n  inside the graph definition.\n\n* MDS-1019: Make user search case-insensitive and allow search by name.\n\n* MDS-1016: When creating a new collection the fields \"Number of Shards\" and\n  \"Replication factor\" are greyed out now when the field \"Distribute shards\n  like\" is not empty.\n\n* Fixed BTS-850: Fixed the removal of already deleted orphan collections out of\n  a graph definition. The removal of an already deleted orphan collection out of\n  a graph definition failed and has been rejected in case the collection got\n  dropped already.\n\n* BTS-1008: Update react-autocomplete-input to fix single letter collection bug\n  when creating a link in the views in the WebUI.\n\n* BTS-1061: ARM was not recognized on Apple M1.\n\n* BTS-325: Changed the HTTP status code from `400` to `404` of the ArangoDB\n  error code `ERROR_GRAPH_REFERENCED_VERTEX_COLLECTION_NOT_USED` to handle this\n  error in accordance to our edge errors.\n\n* Adjust permissions for \"search-alias\" views.\n\n  Previously, \"search-alias\" views were visible to users that didn't have read\n  permissions on the underlying referenced collections. This was inconsistent,\n  because \"arangosearch\" views weren't shown to users that didn't have read\n  permissions on the underlying links.\n  Now, the behavior for \"search-alias\" views is the same as for \"arangosearch\"\n  views, i.e. \"search-alias\" views are not shown and are not accessible for\n  users that don't have at least read permissions on the underlying collections.\n\n* BTS-969: Added restriction for HTTP request `/cluster/rebalance`not to\n  consider servers that have failed status as a possible target for rebalancing\n  shards in its execution plan.\n\n* Fix an issue with replication of arangosearch view change entries in single\n  server replication and active failover. Previously, when changing the\n  properties of existing views, the changes were not properly picked up by\n  followers in these setups. Cluster setups were not affected.\n\n\nv3.10.0 (2022-09-29)\n--------------------\n\n* Convert v3.10.0-rc.1 into v3.10.0.\n\n\nv3.10.0-rc.1 (2022-09-25)\n-------------------------\n\n* Temporary fix for BTS-1006 (hides new view types).\n\n* Fixed SEARCH-399 Rule `restrict-to-single-shard` now works properly with\n  inverted index.\n\n* Fixed SEARCH-393 fixed analyzer setting for nested fields.\n\n* APM-517: Add tooltips with values of the displayed properties after clicking a\n  node or an edge in the graph viewer.\n\n* Updated arangosync to v2.12.0.\n\n* Improve upload and download speed of hotbackup by changing the way we use\n  rclone. Empty hash files are now uploaded or downloaded by pattern, and\n  all other files are done in batches without remote directory listing,\n  which allows rclone to parallelize and avoid a lot of unnecessary network\n  traffic. The format of hotbackups does not change at all.\n\n* Fixed SEARCH-392 Fixed field features propagation.\n\n* Fixed SEARCH-388 Fixed handling nested subfields.\n\n* Fixed SEARCH-379 Transaction is properly set to index during optimization.\n\n* Fixed issue BTS-1018: Improve logging of binary velocypack request data.\n\n* Updated ArangoDB Starter to 0.15.5.\n\n* Fixed BTS-1017: Fixed a graph search issue, where subqueries lead to incorrect\n  results when they have been pushed down fully onto a DBServer when they are in\n  a Hybrid Disjoint SmartGraph context and SatelliteCollections were part of it.\n\n* Fixed issue BTS-1023:\n  Added Linux-specific startup option `--use-splice-syscall` to control whether\n  the Linux-specific splice() syscall should be used for copying file contents.\n  While the syscall is generally available since Linux 2.6.x, it is also\n  required that the underlying filesystem supports the splice operation. This is\n  not true for some encrypted filesystems, on which splice() calls thus fail.\n  By setting the startup option `--use-splice-syscall` to `false`, a less\n  efficient, but more portable user-space file copying method will be used\n  instead, which should work on all filesystems.\n  The startup option is not available on other operating systems than Linux.\n\n* Fixed SEARCH-386 Fixed disjunction coverage in the inverted index with\n  non-default analyzers.\n\n* Fixed SEARCH-373  Fixed indexing same field as nested and as non-nested.\n\n\nv3.10.0-beta.1 (2022-09-14)\n---------------------------\n\n* Display progress during Arangosearch link and inverted index recovery.\n\n* Fixed SEARCH-374 and SEARCH-358 Fixed a rare case of wrong seek operation in\n  the sparse_bitset during query ArangoSearch execution.\n\n* Updated arangosync to v2.12.0-preview-12.\n\n* Updated ArangoDB Starter to 0.15.5-preview-3.\n\n* Fixed a rare occurring issue where paths inside a DisjointSmart traversal\n  containing only satellite relevant nodes were not returned properly (ES-1265).\n\n* Implement prefetch for revision trees, in case a batch is created with a\n  distinguished collection as for `SynchronizeShard`. This ensures that the\n  revision tree for the batch will be available when needed, even though the\n  revision tree for the collection might already have advanced beyond the\n  sequence number of the snapshot in the batch. This ensures that shards can get\n  in sync more reliably and more quickly.\n\n* Added startup option `--rocksdb.periodic-compaction-ttl`.\n  This option controls the TTL (in seconds) for periodic compaction of .sst\n  files in RocksDB, based on the .sst file age. The default value from RocksDB\n  is ~30 days. To avoid periodic auto-compaction, the option can be set to 0.\n\n* Fixed SEARCH-366: Fixed extracting sub-attributes using projections for the\n  `_id` attribute (e.g. `RETURN doc.sub._id`).\n\n* Fixed SEARCH-368: Fixed handling of array comparison for inverted index.\n\n* Fix get snapshot on single server for search-alias view when index was removed.\n\n* Fix waitForSync options for SEARCH in non-maintainer build.\n\n* Fix SEARCH-340 and SEARCH-341: add stats and metrics for inverted index.\n\n* Fixed SEARCH-334 Added searchField option for inverted index.\n\n* Fixed SEARCH-376 Assertion fixed for non-array value in array comparison.\n\n* Fixed BTS-926: UI showing the \"create index\" form to non-admin users.\n\n* Added startup option `--arangosearch.skip-recovery` to skip the recovery of\n  arangosearch view links or inverted indexes.\n  The startup option can be specified multiple times and is expected to either\n  contain the string `all` (will skip the recovery for all view links and\n  inverted indexes) or a collection name + link id/name pair (e.g.\n  `testCollection/123456`, where `123456` is a link/index id or an index name).\n  This new startup option is an emergency means to speed up lengthy recovery\n  procedures when there is a large WAL backlog to replay. The normal recovery\n  will still take place even with the option set, but recovery data for\n  links/indexes can be skipped. This can improve the recovery speed and reduce\n  memory usage during the recovery process.\n  All links or inverted indexes that are marked as to-be-skipped via the option,\n  but for which there is recovery data, will be marked as \"out of sync\" at the\n  end of the recovery.\n  The recovery procedure will also print a list of links/indexes which it has\n  marked as out-of-sync.\n  Additionally, if committing data for a link/index fails for whatever reason,\n  the link/index is also marked as being out-of-sync.\n\n  If an out-of-sync link or index can be used in queries depends on another new\n  startup option `--arangosearch.fail-queries-on-out-of-sync`. It defaults to\n  `false`, meaning that out-of-sync links/indexes can still be queried. If the\n  option is set to `true`, queries on such links/indexes will fail with error\n  \"collection/view is out of sync\" (error code 1481).\n\n  Links/indexes that are marked out-of-sync will keep the out-of-sync flag until\n  they are dropped. To get rid of an out-of-sync link/index, it is recommended\n  to manually drop and recreate it. As recreating a link/index may cause high\n  load, this is not done automatically but requires explicit user opt-in.\n\n  The number of out-of-sync links/indexes is also observable via a new metric\n  `arangodb_search_num_out_of_sync_links`.\n\n* Moved extensive log message down to DEBUG level.\n\n* Updated Views UI with all changes necessary for the 3.10.0 launch.\n\n* Fixed SEARCH-343 Fixed iterating all documents with nested fields.\n\n* Fixed SEARCH-322 Fixed executing empty nested condition.\n\n* Do not drop follower shard after too many failed shard synchronization\n  attempts.\n\n* Disable optimization rule to avoid crash (BTS-951).\n\n* Fixed SEARCH-369: add a compatibility check to the search-alias view for\n  indexes from the same collection.\n\n* Fixed BTS-959: no one call shutdown and delete datastore for the inverted\n  index in some C++ tests.\n\n* Fix SEARCH-350: Crash during consolidation.\n\n* When using `SHORTEST_PATH`, `K_SHORTEST_PATHS`, `ALL_SHORTEST_PATHS`, or\n  `K_PATHS` in an AQL Query and the query itself produced warnings during\n  execution, the type has been wrongly reported. It reported always with\n  `SHORTEST_PATH` and not the specific used one.\n\n* Fixed SEARCH-368 Fixed handling of array comparison for inverted index.\n\n* SEARCH-357: Added SUBSTRING_BYTES function.\n\n* Fixed SEARCH-347 Cycle variable reference in nested search query is properly\n  detected and rejected.\n\n* Fixed SEARCH-364 Fixed index fields match check for inverted index.\n\n* Web UI: Reduce size and initial render height of a modal (fixes BTS-940).\n\n* Fix comparison of JSON schemas on DB servers after there was a schema change\n  via a coordinator: the schema comparison previously did not take into account\n  that some ArangoDB versions store an internal `{\"type\":\"json\"}` attribute in\n  the schema, and some don't. Thus two identical schemas could compare\n  differently.\n  The correct schema version was always applied and used, and validation of\n  documents against the schema was also not affected. However, because two\n  schemas could compare unequal, this could have caused unnecessary repeated\n  work for background maintenance threads.\n\n* Removed transitive node dependencies is-wsl and media-typer.\n\n* Web UI: Now correctly handles the server error response when an error occurred\n  during the modification of a document or an edge (BTS-934).\n\n* Fixed SEARCH-328 fixed cookie key.\n\n* Make graph search case-insensitive (fixes BTS-882).\n\n* Fixed SEARCH-329 fixed removes with nested documents.\n\n* Fixed parsing on NOT operator in nested query filter.\n\n* Fixed SEARCH-346. If index creation is aborted due to existing same index new\n  index is properly dropped if it was already instantiated.\n\n* Add progress reporting to RocksDB WAL recovery, in case there are many WAL\n  files to recover.\n\n* Updated arangosync to v2.12.0-preview-9.\n\n\nv3.10.0-alpha.1 (2022-08-17)\n----------------------------\n\n* Updated arangosync to v2.12.0-preview-6.\n\n* Updated warning messages raised for non accepted query OPTIONS, distinguishing\n  between when the OPTIONS attribute is correct, but the value is in incorrect\n  format, and when the OPTIONS attribute itself is incorrect.\n\n* Since ArangoDB 3.8 there was a loophole for creating duplicate keys in the\n  same collection. The requirements were:\n  - cluster deployment\n  - needs at least two collections (source and target), and the target\n    collection must have more than one shard and must use a custom shard key.\n  - inserting documents into the target collection must have happened via an AQL\n    query like `FOR doc IN source INSERT doc INTO target`.\n  In this particular combination, the document keys (`_key` attribute) from the\n  source collection were used as-is for insertion into the target collection.\n  However, as the target collection is not sharded by `_key` and uses a custom\n  shard key, it is actually not allowed to specify user-defined values for\n  `_key`. That check was missing since 3.8 in this particular combination and\n  has now been added back. AQL queries attempting to insert documents into a\n  collection like this will now fail with the error \"must not specify _key for\n  this collection\", as they used to do before 3.8.\n\n* Updated ArangoDB Starter to 0.15.5-preview-1.\n\n* Improve error handling for passing wrong transaction ids / cursor ids / pregel\n  job ids to request forwarding. Also prevent the error \"transaction id not\n  found\" in cases when request forwarding was tried to a coordinator that was\n  recently restarted.\n\n* Fixed an invalid attribute access in AQL query optimization.\n  Without the fix, a query such as\n\n      LET data = {\n        \"a\": [\n          ...\n        ],\n      }\n      FOR d IN data[\"a\"]\n        RETURN d\n\n  could fail with error \"invalid operand to FOR loop, expecting Array\".\n\n* Added support for AT LEAST quantifier for SEARCH.\n\n* Fixed BTS-335 Ranges parsing fixed for nested queries.\n\n* BTS-907: Fixed some rare SortNode related optimizer issues, when at least two\n  or more SortNodes appeared in the AQL execution plan.\n\n* Added new AQL function `VALUE` capable of accessing object attribute by a\n  specified path.\n\n* Fixed BTS-918 (incorrectly navigating back 1 level in history when a\n  modal-dialog element is present).\n\n* Added OFFSET_INFO function (Enterprise Edition only) to support search results\n  highlighting.\n\n* Updated Rclone to v1.59.0.\n\n* Fixed BTS-902 (clicking on the search icon in the analyzers filter input used\n  to take the user to the collections view).\n\n* Fixed BTS-852 (user's saved queries used to disappear after updating user\n  profile).\n\n* ArangoSearch nested search feature (Enterprise Edition): Added ability to\n  index and search nested documents with ArangoSearch views.\n\n* Updated OpenSSL to 1.1.1q and OpenLDAP to 2.6.3.\n\n* Fixed handling of illegal edges in Enterprise Graphs. Adding an edge to a\n  SmartGraph vertex collection through document API caused incorrect sharding of\n  the edge. Now this edge is rejected as invalid. (BTS-906)\n\n* Added CSP recommended headers to Aardvaark app for better security.\n\n* Added more specific process exit codes for arangod and all client tools, and\n  changed the executables' exit code for the following situations:\n\n  - an unknown startup option name is used: previously the exit code was 1. Now\n    the exit code when using an invalid option is 3 (symbolic exit code name\n    EXIT_INVALID_OPTION_NAME).\n  - an invalid value is used for a startup option (e.g. a number that is outside\n    the allowed range for the option's underlying value type, or a string value\n    is used for a numeric option): previously the exit code was 1. Now the exit\n    code for these case is 4 (symbolic exit code name\n    EXIT_INVALID_OPTION_VALUE).\n  - a config file is specified that does not exist: previously the exit code was\n    either 1 or 6 (symbolic exit code name EXIT_CONFIG_NOT_FOUND). Now the exit\n    code in this case is always 6 (EXIT_CONFIG_NOT_FOUND).\n  - a structurally invalid config file is used, e.g. the config file contains a\n    line that cannot be parsed: previously the exit code in this situation was\n    1. Now it is always 6 (symbolic exit code name EXIT_CONFIG_NOT_FOUND).\n\n  Note that this change can affect any custom scripts that check for startup\n  failures using the specific exit code 1. These scripts should be adjusted so\n  that they check for a non-zero exit code. They can opt-in to more specific\n  error handling using the additional exit codes mentioned above, in order to\n  distinguish between different kinds of startup errors.\n\n* BTS-913: check for proper timezone setup of the system on startup.\n  This will then log errors that else would only occur in AQL-Functions at\n  runtime.\n\n* Added ALL_SHORTEST_PATHS functionality to find all shortest paths between two\n  given documents.\n\n* Fixed a potential deadlock in RocksDB compaction.\n  For details see https://github.com/facebook/rocksdb/pull/10355.\n\n* Changed rocksdb default compression type from snappy to lz4.\n\n* arangoimport now supports the option --remove-attribute on type JSON as well.\n  Before it was restricted to TSV and CSV only.\n\n* Fixed BTS-851: \"Could not fetch the applier state of: undefined\".\n\n* Removed internal JavaScript dependencies \"expect.js\", \"media-typer\" and\n  \"underscore\". We recommend always bundling your own copy of third-party\n  modules as all previously included third-party modules are now considered\n  deprecated and may be removed in future versions of ArangoD\n\n* APM-84: Added option to spill intermediate AQL query results from RAM to disk\n  when their size exceeds certain thresholds. Currently the only AQL operation\n  that can make use of this is the SortExecutor (AQL SORT operation without\n  using a LIMIT). Further AQL executor types will be supported in future\n  releases.\n\n  Spilling over query results from RAM to disk is off by default and currently\n  in an experimental stage. In order to opt-in to the feature, it is required to\n  set the following startup option `--temp.intermediate-results-path`.\n  The directory specified here must not be located underneath the instance's\n  database directory.\n  When this startup option is specified, ArangoDB assumes ownership of that\n  directory and will wipe its contents on startup and shutdown. The directory\n  can be placed on ephemeral storage, as the data stored inside it is there only\n  temporarily, while the instance is running. It does not need to be persisted\n  across instance restarts and does not need to be backed up.\n\n  When a directory is specified via the startup option, the following additional\n  configuration options can be used to control the threshold values for spilling\n  over data:\n\n  * `--temp.intermediate-results-capacity`: maximum on-disk size (in bytes) for\n    intermediate results. If set to 0, it means that the on-disk size is not\n    constrained. It can be set to a value other than 0 to restrict the size of\n    the temporary directory. Once the cumulated on-disk size of intermediate\n    results reaches the configured maximum capacity, the query will be aborted\n    with failure \"disk capacity limit for intermediate results exceeded\".\n  * `--temp.intermediate-results-spillover-threshold-num-rows`: number of result\n    rows from which on a spillover from RAM to disk will happen.\n  * `--temp.intermediate-results-spillover-threshold-memory-usage`: memory usage\n    (in bytes) after which a spillover from RAM to disk will happen.\n  * `--temp.intermediate-results-encryption`: whether or not the on-disk data\n    should be encrypted. This option is only available in the Enterprise\n    Edition.\n  * `--temp.-intermediate-results-encryption-hardware-acceleration`: whether or\n    not to use hardware acceleration for the on-disk encryption. This option is\n    only available in the Enterprise Edition.\n\n  Please note that the feature is currently still experimental and may slightly\n  change in future releases. As mentioned, the only Executor that can make use\n  of spilling data to disk is the SortExecutor (SORT without LIMIT).\n  Also note that the query results will still be built up entirely in RAM on\n  coordinators and single servers for non-streaming queries. In order to avoid\n  the buildup of the entire query result in RAM, a streaming query should be\n  used.\n\n* Enterprise only: Added `MINHASH`, `MINHASH_MATCH`, `MINHASH_ERROR`,\n  `MINHASH_COUNT` AQL functions.\n\n* Enterprise only: Added `minhash` analyzer.\n\n* BugFix in Pregel's status: When loading the graph into memory, Pregel's state\n  is now 'loading' instead of 'running'. When loading is finished, Pregel's\n  state changes to the 'running' state.\n\n* arangoimport now supports an additional option\n  \"--overwrite-collection-prefix\".\n  This option will only help while importing edge collections, and if it is used\n  together with \"--to-collection-prefix\" or \"--from-collection-prefix\". If there\n  are vertex collection prefixes in the file you want to import (e.g. you just\n  exported an edge collection from ArangoDB) you allow arangoimport to overwrite\n  those with the commandline prefixes. If the option is false (default value)\n  only _from and _to values without a prefix will be prefixed by the handed in\n  values.\n\n* Added startup option `--rocksdb.compaction-style` to configure the compaction\n  style which is used to pick the next file(s) to be compacted.\n\n* BugFix in Pregel's Label Propagation: the union of three undirected cliques of\n  size at least three connected by an undirected triangle now returns three\n  communities (each clique is a community) instead of two.\n\n* Pregel now reports correct and ongoing runtimes for loading, running, and\n  storing as well as runtimes for the separate global supersteps.\n\n* Fixed parsing of K_SHORTEST_PATHS queries to not allow ranges anymore.\n\n* Add log.time-format utc-datestring-micros to make debugging of concurrency\n  bugs easier.\n\n* Renamed KShortestPathsNode to EnumeratePathsNote; this is visible in explain\n  outputs for AQL queries.\n\n* Pregel SSSP now supports `resultField` as well as `_resultField` as parameter\n  name to specify the field into which results are stored. The name\n  `_resultField` will be deprecated in future.\n\n* Update Windows CI compiler to Visual Studio 2022.\n\n* Web UI: Fixes a GraphViewer issue related to display issues with node and edge\n  labels. Boolean node or edge values could not be used as label values\n  (ES-1084).\n\n* Made the SortExecutor receive its input incrementally, instead of receiving a\n  whole matrix containing all input at once.\n\n* Optimization for index post-filtering (early pruning): in case an index is\n  used for lookups, and the index covers the IndexNode's post-filter condition,\n  then loading the full document from the storage engine is now deferred until\n  the filter condition is evaluated and it is established that the document\n  matches the filter condition.\n\n* Added a fully functional UI for Views that lets users view, modify mutable\n  properties and delete views from the web UI.\n\n* Fix thread ids and thread names in log output for threads that are not started\n  directly by ArangoDB code, but indirectly via library code.\n  Previously, the ids of these threads were always reported as \"1\", and the\n  thread name was \"main\". Now return proper thread ids and names.\n\n* Changed default Linux CI compiler to gcc-11.\n\n* Add \"AT LEAST\" quantifier for array filters in AQL:\n\n      `RETURN [1,2,3][? AT LEAST (3) FILTER CURRENT > 42]`\n      `RETURN [1,2,3] AT LEAST (2) IN [1,2,3,4,5]`\n\n* Changed default macOS CI compiler to LLVM clang-14.\n\n* Added an automatic cluster rebalance api. Use `GET _admin/cluster/rebalance`\n  to receive an analysis of how imbalanced the cluster is. Calling it with\n  `POST _admin/cluster/rebalance` computes a plan of move shard operations to\n  rebalance the cluster. Options are passed via the request body. After\n  reviewing the plan, one can use `POST _admin/cluster/rebalance/execute` to put\n  that plan into action.\n\n* Introduce reading from followers in clusters. This works by offering an\n  additional HTTP header \"x-arango-allow-dirty-read\" for certain read-only APIs.\n  This header has already been used for active failover deployments to allow\n  reading from followers. Using this header leads to the fact that coordinators\n  are allowed to read from follower shards instead only from leader shards. This\n  can help to spread the read load better across the cluster. Obviously, using\n  this header can result in \"dirty reads\", which are read results returning\n  stale data or even not-yet-officially committed data. Use at your own risk if\n  performance is more important than correctness or if you know that data does\n  not change.\n  The responses which can contain dirty reads will have set the HTTP header\n  \"x-arango-potential-dirty-read\" set to \"true\".\n  There are the following new metrics showing the use of this feature:\n    - `arangodb_dirty_read_transactions_total`\n    - `arangodb_potentially_dirty_document_reads_total`\n    - `arangodb_dirty_read_queries_total`\n\n* Changed HTTP response code for error number 1521 from 500 to 400.\n\n  Error 1521 (query collection lock failed) is nowadays only emitted by\n  traversals, when a collection is accessed during the traversal that has not\n  been specified in the WITH statement of the query.\n  Thus returning HTTP 500 is not a good idea, as it is clearly a user error that\n  triggered the problem.\n\n* Renamed the `--frontend.*` startup options to `--web-interface.*`:\n\n  - `--frontend.proxy-request.check` -> `--web-interface.proxy-request.check`\n  - `--frontend.trusted-proxy` -> `--web-interface.trusted-proxy`\n  - `--frontend.version-check` -> `--web-interface.version-check`\n\n  The former startup options are still supported.\n\n* Added Enterprise Graph feature to enterprise version of ArangoDB.\n  The enterprise graph is another graph sharding model that we introduced, it is\n  less strict, and therefore easier to start with, then SmartGraphs, as it does\n  not require a smartGraphAttribute, and allows free choice of vertex _key\n  values. But still maintains performance gains as compared to general-graphs.\n  For more details please check documentation.\n\n* APM-135: Added multithreading to assigning non-unique indexes to documents, in\n  foreground or background mode. The number of index creation threads is\n  hardcoded to 2 for now. Improvements for higher parallelism are expected for\n  future versions.\n\n* Issue 15592: Permit `MERGE_RECURSIVE()` to be called with a single argument.\n\n* Fixed issue 16337: arangoimport with `--headers-file` and `--merge-attributes`\n  merges column names instead of row values on the first line of a CSV file.\n\n  Additionally, floating-point numbers are now merged using their standard\n  string representation instead of with a fixed precision of 6 decimal places.\n\n* Now supporting projections on traversals. In AQL Traversal statements like\n  FOR v,e,p IN 1..3 OUTBOUND @start GRAPH @graph RETURN v.name\n  we will now detect attribute accesses on the data, in above example \"v.name\"\n  and use it to optimize data-loading, e.g. we will only extract the \"name\"\n  attribute.\n  This optimization will help if you have large document sizes, but only access\n  small parts of the documents. By default we will only project up to 5\n  attributes on each vertex, and edge. This limit can be modified by adding\n  OPTIONS {maxProjections: 42}.\n  To identify if your query is using projections the explain output will now\n  contain a hint like `/* vertex (projections: `name`) */`\n  For now only attribute accesses are detected, functions like `KEEP` will not\n  be projected.\n\n* Change default `format_version` for RocksDB .sst files from 3 to 5.\n\n* Added support for creating autoincrement keys on cluster mode, but only for\n  single sharded collections.\n\n* Add support for LZ4 and LZ4HC compression support for RocksDB.\n\n* Allow parallel access to the shards of smart edge collections in AQL via\n  parallel GatherNodes.\n\n* Update RocksDB internal table checksum type to xxHash64.\n\n* Added several startup option to configure parallelism for individual Pregel\n  jobs:\n\n  - `--pregel.min-parallelism`: minimum parallelism usable in Pregel jobs.\n  - `--pregel.max-parallelism`: maximum parallelism usable in Pregel jobs.\n  - `--pregel.parallelism`: default parallelism to use in Pregel jobs.\n\n  These parallelism options can be used by administrators to set concurrency\n  defaults and bounds for Pregel jobs. Each individual Pregel job can set its\n  own parallelism value using the job's `parallelism` option, but the job's\n  parallelism value will be clamped to the bounds defined by\n  `--pregel.min-parallelism` and `--pregel.max-parallelism`. If a job does not\n  set its `parallelism` value, it will default to the parallelism value\n  configured via `--pregel.parallelism`.\n\n* Added startup options to configure the usage of memory-mapped files for Pregel\n  temporary data:\n\n  - `--pregel.memory-mapped-files`: if set to `true`, Pregel jobs will by\n    default store their temporary data in disk-backed memory-mapped files.\n    If set to `false`, the temporary data of Pregel jobs will be buffered in\n    RAM. The default value is `true`, meaning that memory-mapped files will be\n    used. The option can be overridden for each Pregel job by setting the\n    `useMemoryMaps` option of the job.\n\n  - `--pregel.memory-mapped-files-location-type`: location for memory-mapped\n    files written by Pregel. This option is only meaningful if memory-mapped\n    files are actually used. The option can have one of the following values:\n    - `temp-directory`: store memory-mapped files in the temporary directory, as\n      configured via `--temp.path`. If `--temp.path` is not set, the system's\n      temporary directory will be used.\n    - `database-directory`: store memory-mapped files in a separate directory\n      underneath the database directory.\n    - `custom`: use a custom directory location for memory-mapped files. The\n      exact location must be set via the configuration parameter\n      `--pregel.memory-mapped-files-custom-path`.\n\n    The default value for this option is `temp-directory`.\n\n  - `--pregel.memory-mapped-files-custom-path`: custom directory location for\n    Pregel's memory-mapped files. This setting can only be used if the option\n    `--pregel.memory-mapped-files-location-type` is set to `custom`.\n\n  The default location for Pregel's memory-mapped files is the temporary\n  directory (`temp-directory`), which may not provide enough capacity for larger\n  Pregel jobs.\n  It may be more sensible to configure a custom directory for memory-mapped\n  files and provide the necessary disk space there (`custom`). Such custom\n  directory can be mounted on ephemeral storage, as the files are only needed\n  temporarily.\n  There is also the option to use a subdirectory of the database directory as\n  the storage location for the memory-mapped files (`database-directory`).\n  The database directory often provides a lot of disk space capacity, but when\n  it is used for both the regular database data and Pregel's memory-mapped\n  files, it has to provide enough capacity to store both.\n\n* Pregel status now reports whether memory mapped files are used in a job.\n\n* Change default value of `--rocksdb.block-cache-shard-bits` to an automatic\n  default value that allows data blocks of at least 128MiB to be stored in each\n  cache shard if the block cache's strict capacity limit is used. The strict\n  capacity limit for the block cache is enabled by default in 3.10, but can be\n  turned off by setting the option `--rocksdb.enforce-block-cache-size-limit` to\n  `false`. Also log a startup warning if the resulting cache shard size would be\n  smaller than is potentially safe when the strict capacity limit is set.\n  Enforcing the block cache's capacity limit has the consequence that data reads\n  by RocksDB must fit into the block cache or the read operation will fail with\n  an \"Incomplete\" error.\n\n* The API `/_admin/status` now returns a progress attribute that shows the\n  server's current state (starting, stopping, etc.), with details about which\n  feature is currently started, stopped etc. During recovery, the current WAL\n  recovery sequence number is also reported in a sub-attribute of the `progress`\n  attribute. Clients can query this attribute to track the progress of the WAL\n  recovery.\n  The additional progress attribute returned by `/_admin/status` is most useful\n  when using the `--server.early-connections true` setting. With that setting,\n  the server will respond to incoming requests to a limited set of APIs already\n  during server startup. When the setting is not used, the REST interface will\n  be opened relatively late during the startup sequence, so that the progress\n  attribute will likely not be very useful anymore.\n\n* Optionally start up HTTP interface of servers earlier, so that ping probes\n  from tools can already be responded to when the server is not fully started.\n  By default, the HTTP interface is opened at the same point during the startup\n  sequence as before, but it can optionally be opened earlier by setting the new\n  startup option `--server.early-connections` to `true`. This will open the HTTP\n  interface early in the startup, so that the server can respond to a limited\n  set of REST APIs even during recovery. This can be useful because the recovery\n  procedure can take time proportional to the amount of data to recover.\n  When the `--server.early-connections` option is set to `true`, the server will\n  respond to requests to the following APIs during the startup already:\n  - `/_api/version`\n  - `/_admin/version`\n  - `/_admin/status`\n  All other APIs will be responded to with an HTTP response code 503, so that\n  callers can see that the server is not fully ready.\n  If authentication is used, then only JWT authentication can be used during the\n  early startup phase. Incoming requests relying on other authentication\n  mechanisms that require access to the database data will also be responded to\n  with HTTP 503 errors, even if correct credentials are used.\n\n* Upgraded bundled version of RocksDB to 7.2.\n\n* Added `[?]` array operator to AQL, which works as follows:\n  - `nonArray[?]`: returns `false`\n  - `nonArray[? FILTER CURRENT ...]`: returns `false`\n  - `array[?]`: returns `false` if array is empty, `true` otherwise\n  - `array[? FILTER CURRENT ...]`: returns `false` if no array member\n     satisfies the filter condition, returns `true` if at least one member\n     satisfies it.\n\n* Upgrade jemalloc to version 5.3.0.\n\n* Set \"useRevisionsAsDocumentIds\" to true when restoring collection data via\n  arangorestore in case it is not set in the collection structure input data.\n  This allows using revision trees for restored collections.\n\n* Added new optimization rule \"arangosearch-constrained-sort\" to perform sorting\n  & limiting inside ArangoSearch View enumeration node in case of using just\n  scoring for sort.\n\n* Updated lz4 to version 1.9.3.\n\n* Added option `--custom-query-file` to arangoexport, so that a custom query\n  string can also be read from an input file.\n\n* FE-46: UI improvement on the view UI pages as well as adding tooltips to\n  options where necessary. The affected pages are mostly the Info and\n  Consolidation Policy pages.\n\n* FE-44: Moved the Info page to before JSON, making the settings page the\n  default page in the view web UI.\n\n* Refactor internal code paths responsible for `_key` generation. For\n  collections with only a single shard, we can now always let the leader DB\n  server generate the keys locally. For collections with multiple shards, the\n  coordinators are now always responsible for key generation.\n  Previously the responsibility was mixed and depended on the type of operation\n  executed (document insert API vs. AQL query, single operation vs. batch).\n\n* Make web UI show the following information for collections:\n  - key generator type\n  - whether or not the document and primary index cache is enabled\n  - if cache is enabled, show cache usage and allocation size in figures\n  The `cacheEnabled` property of collections is now also changeable via the UI\n  for existing collections.\n\n* FE-45: Added tooltips with helpful information to the options on the View UI\n  settings page.\n\n* FE-43: Simplify the workflow on the web view UI (Links page): allow for users\n  to view a single link or field with their properties at a time.\n\n* Fixed BTS-811 in which there was an incongruence between data being\n  checksummed and data being written to `.sst` files, because checksumming\n  should have been made after the encryption of the data, not before it.\n\n* Added command line option to arangobench to disable implicit collection\n  creation. This allows one to run tests against a manually created and\n  configured collection.\n\n* Fix deadlocked shard synchronizations when planned shard leader has not yet\n  taken over leadership.\n\n* Unify the creation of normal and SmartGraph collections.\n\n  This unifies the code paths for creating collections for normal collections\n  and SmartGraph collections, so that the functionality is centralized in one\n  place. SmartGraph-specific code for validation and collection creation has\n  been moved to enterprise as well.\n\n* Auto-regenerate exit code and error code files in non-maintainer mode, too.\n\n* Only show slowest optimizer rules in explain output for optimizer rules that\n  took a considerable amount of time (>= 0.0002 seconds). Previously the slowest\n  5 optimizer rules were shown, regardless of how long they took to execute and\n  even if they executed sufficiently fast.\n\n* Make the StatisticsFeature start after the NetworkFeature, so that any network\n  request issues by cluster statistics gathering can rely on the networking\n  functionality being available until shutdown.\n\n* Rework internal queues for connection and request statistics. The previous\n  implementation allocated a lot of memory at program start for initializing\n  fixed-sized queues for the statistics objects.\n  The problem with using fixed-sized queues is that they will mostly require too\n  much memory for almost all cases, but still do not protect from the queues\n  becoming full and not being able to hold more items.\n  Now we go with a variable length queue instead, which only requires a small\n  amount of memory initially, and allocate more memory only when needed.\n  Freelists for reusing statistics items are still present to avoid lots of\n  reallocations.\n  The change also reduces the size of the executable's .bss section by more than\n  10MB.\n\n* Always open a new, working connection before HTTP request-fuzzing during\n  testing. Otherwise the fuzzing results are not 100% comparable from run to\n  run.\n\n* Updated bundled version of zlib library to 1.2.12.\n\n* Added AQL hint \"useCache\" for FOR loops, to explicitly disable the usage of\n  in-memory caches for lookups.\n\n* When profiling an AQL query via `db._profileQuery(...)` or via the web UI, the\n  query profile will now contain the number of index entries read from in-memory\n  caches (usable for edge indexes and indexes of type \"persistent\", \"hash\" or\n  \"skiplist\") plus the number of cache misses.\n\n* The caching subsystem now provides the following 3 additional metrics:\n  - `rocksdb_cache_active_tables`: total number of active hash tables used for\n    caching index values. There should be 1 table per shard per index for which\n    the in-memory cache is enabled. The number also includes temporary tables\n    that are built when migrating existing tables to larger equivalents.\n  - `rocksdb_cache_unused_memory`: total amount of memory used for inactive hash\n    tables used for caching index values. Some inactive tables can be kept\n    around after use, so they can be recycled quickly. The overall amount of\n    inactive tables is limited, so not much memory will be used here.\n  - `rocksdb_cache_unused_tables`: total number of inactive hash tables used for\n    caching index values. Some inactive tables are kept around after use, so\n    they can be recycled quickly. The overall amount of inactive tables is\n    limited, so not much memory will be used here.\n\n* Added optional in-memory caching for index entries when doing point lookups in\n  indexes of type \"persistent\", \"hash\" or \"skiplist\".\n  The caching is turned off by default, but can be enabled when creating an\n  index of type \"persistent\", \"hash\" or \"skiplist\" by setting the \"cacheEnabled\"\n  flag for the index upon index creation.\n  The cache will be initially empty, but will be populated lazily upon querying\n  data from the index using equality lookups on all index attributes.\n  As the cache is hash-based and unsorted, it cannot be used for full or partial\n  range scans, for sorting, or for lookups that do not include all index\n  attributes.\n  The maximum size of index entries that can be stored is currently 4 MB, i.e.\n  the cumulated size of all index entries for any index lookup value must be\n  less than 4 MB. This limitation is there to avoid storing the index entries of\n  \"super nodes\" in the cache.\n\n  The maximum combined memory usage of all in-memory caches can be controlled\n  via the existing `--cache.size` startup option, which now not only contains\n  the maximum memory usage for edge caches, but also for index caches added\n  here.\n\n* Added new AQL function `KEEP_RECURSIVE` to recursively keep attributes from\n  objects/documents, as a counterpart to `UNSET_RECURSIVE`.\n\n* Added an HTTP fuzzer to arangosh that can send fuzzed requests to the server.\n  The amount of requests sent is provided by one of the parameters of the new\n  arangosh function `fuzzRequests()`.\n  The optional parameters that can be supplied are:\n  `fuzzRequests(<numRequests>, <numIterations>, <seed>)`\n  The parameter numIterations is the amount of times the fuzzer is going to\n  perform its random actions on the header, and seed is for the seed that is\n  used for randomizing.\n  The fuzzer is available only when building with failure points.\n\n* Escape each key in attribute paths of nested attributes in the query explain\n  output for SEARCH queries that utilize the primary sort order.\n\n* Turn off sending \"Server\" HTTP response header on DB servers if not explicitly\n  requested. This saves a tiny bit of traffic on each response from a DB server.\n\n* Enable range deletions in the WAL for truncate operations in the cluster, too.\n  This can speed up truncate operations for large collections/shards.\n\n* Set max recursion depth for VelocyPack, JSON and JavaScript arrays and objects\n  to about 200.\n\n* Updated snowball to version 2.2.0\n\n* Fixed: Deadlock created by high load and a follower trying to get into sync.\n  In the final synchronization phase the follower needs to temporarily block\n  writes on the leader so we have a reliable point in time where we can prove\n  that the data is consistent.\n  If the leader at this point is flooded with write requests to that shard there\n  is a chance that all worker threads only pick up those writes, which cannot\n  make any progress until the lock is cleared. However, the process to clear the\n  lock was on the same priority as those writes.\n  Hence this lock clear operations could not bypass the writes. Now we moved\n  every follow up request after the lock to HIGH lanes, which will allow them to\n  bypass all non-internal operations.\n\n* arangosh now uses the same header the UI uses to gain higher priority on\n  initial connection.\n  This will increase the chance for an arangosh to connect to a server under\n  very high load.\n\n* Bugfix: DC2DC Disjoint-SmartGraphs and Hybrid-SmartGraphs are now replicated\n  to the follower data-center keeping their sharding intact.\n\n* Added a log message that appears upon starting arangod that shows the number\n  of the parent process id and, if able to acknowledge it, the name of the\n  parent process.\n\n* Parallelize applying of revision tree changes with fetching next revision tree\n  range in incremental collection replication for collections created with\n  ArangoDB 3.8 and higher.\n\n* Support JSON schema objects for documenting Foxx endpoints.\n\n* Internal refactoring of IndexIterator APIs.\n\n* Sorted out various geo problems:\n\n  - No more special detection of \"latitude-longitude rectangles\" is done, since\n    this is in conflict with the definition of polygon boundaries to be\n    geodesics.\n  - Linear rings in polygons are no longer automatically \"normalized\", so now it\n    is possible to have polygons which cover more than half of the Earth.\n  - Rules for polygons and multigons have been clarified and are now properly\n    enforced for the `GEO_POLYGON` and `GEO_MULTIPOLYGON` AQL functions.\n  - Introduced `legacyPolygon` flag for geo indexes to continue to support the\n    old behavior in existing geo indexes.\n  - Added lots of additional tests, thereby fixing several bugs in geo index\n    lookup.\n  - Use a faster algorithm for pure `GEO_CONTAINS` and `GEO_INTERSECTS` queries.\n\n* Added back the optimization for empty document update operations (i.e. update\n  requests in which no attributes were specified to be updated), handling them\n  in a special way without performing any writes, also excluding such special\n  cases of operation from replication to followers.\n\n* Changed Foxx service generator output to use static variable names.\n\n* Allow early pruning (moving a FILTER condition into an IndexNode or\n  EnumerateCollectionNode) in more cases than before. Previously, early pruning\n  was only possible if the FILTER condition referred to exactly one variable,\n  which had to be the FOR loop's own variable. Now, early pruning is possible\n  with arbitrary variables that are accessible by the FOR loop.\n\n* In an attempt to make the performance of the RocksDB throttle much more\n  consistent and predictable the default compaction slow down trigger is lowered\n  to 128kB.\n\n* The multi-dimensional index type `zkd` now supports an optional index hint for\n  tweaking performance by prefetching documents:\n\n  ```\n  FOR app IN appointments OPTIONS { lookahead: 32 }\n    FILTER @to <= app.to\n    FILTER app.from <= @from\n    RETURN app\n  ```\n\n  Specifying a lookahead value greater than zero makes the index fetch more\n  documents that are no longer in the search box, before seeking to the next\n  lookup position.\n  Because the seek operation is computationally expensive, probing more\n  documents before seeking may reduce the number of seeks, if matching documents\n  are found.\n  Please keep in mind that it might also affect performance negatively if\n  documents are fetched unnecessarily.\n\n* Enabled new internal graph refactored code for depth-first, breadth-first and\n  weighted traversals by default.\n\n* Improved performance of inner joins with dynamic lookup conditions being\n  injected from an outer loop, for indexes of type \"persistent\", \"hash\" and\n  \"skiplist\". Performance improvements can be expected if the inner join is\n  invoked a lot of times with many different values fed in by the outer loop.\n  The performance improvements are due to some improved handling of index\n  lookup conditions in the internals of the VelocyPack-based index.\n\n* Improve usefulness of `storedValues` together with late materialization.\n\n* Limited module resolution in arangosh to the path from which arangosh is\n  invoked.\n\n* Changed default value of startup option\n  `--rocksdb.cache-index-and-filter-blocks` from `false` to `true`.\n  This makes RocksDB track all loaded index and filter blocks in the block\n  cache, so they are accounted for in RocksDB's block cache. Also the default\n  value for the startup option `--rocksdb.enforce-block-cache-size-limit` was\n  flipped from `false` to `true` to make the RocksDB block cache not\n  temporarily exceed the configured memory limit (`--rocksdb.block-cache-size`).\n\n  These default value changes will make RocksDB adhere much better to the\n  configured memory limit. This is a trade-off between memory usage stability\n  and performance. These change may have a small negative impact on performance\n  because if the block cache is not large enough to hold the data plus the index\n  and filter blocks, additional disk I/O may be performed compared to previous\n  versions. In case there is still unused RAM capacity available, it may be\n  sensible to increase the total size of the RocksDB block cache.\n\n* Add \"filtered\" column to AQL query profiling output.\n  This column shows how many documents were filtered by the node and thus\n  provides insights into if additional indexes could help.\n\n* Reuse ExecutorExpressionContext inside IndexExecutor, so that repeated setup\n  and teardown of expression contexts can be avoided.\n\n* Adjust internal RocksDB setting `optimize_filters_for_hits` for Documents\n  column family, setting it from `false` to `true`. This should reduce memory\n  and disk space requirements for the bottom-most .sst files of the documents\n  column family.\n\n* Upgrade VelocyPack library to latest version.\n\n* Slightly improve the explain output of SingleRemoteOperationNodes.\n\n* Added more detail to the log messages that display the total time consumption\n  and total amount of data parsed for the client tools arangodump and\n  arangorestore.\n\n* Upgraded boost to 1.78.0.\n\n* Fixed issue #15501: Regression when using \"exclusive\" query option?\n  This fixes a regression in AQL query execution when exclusive locks are used\n  for a query and the query also uses the DOCUMENT() AQL function. In this\n  case, when there were more concurrent requests to the underlying collection\n  than available scheduler threads for the low priority queue, the query\n  start successfully acquired the exclusive lock, but could not get its\n  follow-up requests through and starve while holding the exclusive lock.\n\n* Improve performance of `db._explain()` for very large query execution plans.\n  Higher performance is achieved by not serializing some internal data\n  structures when serializing execution plans. Serializing internal data is now\n  opt-in and turned off if not needed. Apart from performance, there should be\n  no end user visible changes.\n\n* APM-24: Log messages can be displayed together with some other useful\n  parameters, e.g., the name of the database, username, query id, and so on.\n  There are some predefined parameters that we consider displaying, but, for the\n  moment, only database, username and url are being displayed.\n  The usage upon starting the server is, for example:\n  `arangod --log.structured-param database --log.structured-param username`\n\n* Add optional \"storedValues\" attribute for persistent indexes.\n\n  This will add the specified extra fields to the index, so that they can be\n  used for projections, but not for lookups or sorting.\n\n  Example:\n\n      db.<collection>.ensureIndex({\n        type: \"persistent\",\n        fields: [\"value1\"],\n        storedValues: [\"value2\"]\n      });\n\n  This will index `value1` in the traditional sense, so the index can be used\n  for looking up by `value1` or for sorting by `value1`. The index also supports\n  projections on `value1` as usual.\n  In addition, due to `storedValues` being used here, the index can now also\n  supply the values for the `value2` attribute for projections.\n\n  This allows covering index scans in more cases and helps to avoid making extra\n  document lookups in the documents column family. This can have a great\n  positive effect on index scan performance if the number of scanned index\n  entries is large.\n\n  The maximum number of attributes to store in `storedValues` is 32.\n\n* Added agency push-queue operation.\n\n* Make per-server values \"numberOfCores\" and \"physicalMemory\" available to\n  agency to improve quality of potential future shard rebalancing algorithms.\n\n* Unify the result structure of `db._version(true)` calls for arangosh and\n  server console. Previously such a call in the server console would return a\n  different structure that only consisted of the `details` subobject.\n  This is now unified so that the result structure in the server console is\n  consistent with arangosh, but strictly speaking this is a breaking change.\n\n* Added new option `--custom-query-bindvars` to arangoexport, so queries given\n  via option `--custom-query` can have bind variables in them. Also changed the\n  flag names `--query` to `--custom-query` and `--query-max-runtime` to\n  `--custom-query-max-runtime` to be like in the other client-tools.\n\n* Improve some RocksDB-related error messages during server startup.\n\n* Raised minimal macOS supported version to 10.15 (Catalina).\n\n* Remove background thread `RocksDBShaThread` for background SHA256 checksum\n  calculation for .sst files in the Enterprise Edition. The checksums are now\n  calculated incrementally while writing into the .sst files, and the checksum\n  files will be stored on disk as soon as an .sst file is made durable by\n  RocksDB. There is no more need to periodically scan the database directory and\n  look for any additional .sst files.\n\n* Fix BTS-580: Trimmed the password field from the payload in the client\n  requests when displaying error messages in arangorestore, because they\n  displayed the password as plain text.\n\n* Refactored unit tests with the `grey` keyword, which is for skipping certain\n  tests. A test file that did not perform any test, but only had a function to\n  sleep, was removed. Two test files were renamed so they would not be skipped.\n\n* Defer intermediate commits in the middle of a multi-document (array)\n  operation. This is to ensure that the RocksDB key locks for all participating\n  document keys are still held while the operations are replicating via the\n  synchronous replication.\n\n* Changed arangobench concurrency flag name from `--concurrency` to `--threads`.\n\n* APM-217: deprecate the usage of fulltext indexes.\n\n* Changed \"arangosh\" directory name to \"client-tools\", because the directory\n  contains the code for all client tools and not just arangosh.\n\n* Updated immer to version 0.7.0.\n\n* Made the `--version` and `--version-json` commands usable in arangobackup\n  when no positional argument (operation type) was specified. Previously,\n  arangobackup insisted on specifying the operation type alongside the\n  `--version` or `--version-json` commands.\n\n* Removed the following deprecated arangobench testcases:\n  - aqltrx\n  - aqlv8\n  - counttrx\n  - deadlocktrx\n  - multi-collection\n  - multitrx\n  - random-shapes\n  - shapes\n  - shapes-append\n  - skiplist\n  - stream-cursor\n\n* Renamed arangobench testcase \"hash\" to \"persistent-index\".\n\n* Add option to content-transfer encode gzip Foxx replies.\n\n* Simplify internal request compression/decompression handling code.\n\n* Added Enterprise Sharded Graphs Simulation: Now it is possible to test\n  SmartGraphs and SatelliteGraphs on a single server instance and then to port\n  them to a cluster with multiple servers. All existing types of SmartGraphs are\n  eligible to this procedure: SmartGraphs themselves, Disjoint SmartGraphs,\n  Hybrid SmartGraphs and Hybrid Disjoint SmartGraphs. One can create a graph of\n  any of those types in the usual way, e.g., using `arangosh`, but on a single\n  server, then dump it, start a cluster (with multiple servers) and restore the\n  graph in the cluster. The graph and the collections will keep all properties\n  that are kept when the graph is already created in a cluster. This feature is\n  only available in the Enterprise Edition.\n\n* Remove unsupported `--server.default-api-compatibility` startup option.\n\n\nv3.9.2 (2022-06-07)\n-------------------\n\n* Enterprise only: Restricted behavior of Hybrid Disjoint Smart Graphs. Within a\n  single traversal or path query we now restrict that you can only switch\n  between Smart and Satellite sharding once, all queries where more than one\n  switch is (in theory) possible will be rejected. e.g:\n  ```\n    FOR v IN 2 OUTBOUND @start smartToSatEdges, satToSmartEdges\n  ```\n  will be rejected (we can go smart -> sat -> smart, so two switches)\n  ```\n  FOR v1 IN 1 OUTBOUND @start smartToSatEdges\n    FOR v2 IN 1 OUTBOUND v1 satToSmartEdges\n  ```\n  will still be allowed, as each statement only switches once.\n  We have decided to take these restrictions as especially for ShortestPath\n  queries the results are not well-defined. If you have a use case where this\n  restriction hits you, please contact us.\n\n* Fixed issue BTS-875.\n\n* Updated arangosync to v2.10.0.\n\n* Make all requests which are needed for shard resync at least medium priority\n  to improve getting-in-sync under load.\n\n* Fix behavior when accessing a view instead of a collection by name in a REST\n  document operation. Now return a proper error.\n\n* Fix documentation of collection's `cacheEnabled` property default.\n\n* Added startup option `--cluster.shard-synchronization-attempt-timeout` to\n  limit the amount of time to spend in shard synchronization attempts. The\n  default timeout value is 20 minutes.\n  Running into the timeout will not lead to a synchronization failure, but will\n  continue the synchronization shortly after. Setting a timeout can help to\n  split the synchronization of large shards into smaller chunks and release\n  snapshots and archived WAL files on the leader earlier.\n  This change also introduces a new metric `arangodb_sync_timeouts_total` that\n  counts the number of timed-out shard synchronization attempts.\n\n* Make followers respond to synchronous replication requests with less data.\n  Specifically, followers will not build detailed results with _id, _key and\n  _rev for the inserted/modified/removed documents, which would be ignored by\n  the leader anyway.\n\n* Very verbose warning from failing to parse GEO JSON in search. Has lead to\n  billions of log lines on deployed services.\n\n* Fixed Github issue #16279: assertion failure/crash in AQL query optimizer when\n  permuting adjacent FOR loops that depended on each other.\n\n* Fixed a potential hang on shutdown, when there were still document operations\n  queued.\n\n* No good reason to fatal error in agency state, when local database entries\n  lack local timestamp (legacy). In that situation, we will record epoch begin\n  as local time.\n\n* Fixed BTS-860. Changed ArangoSearch index recovery procedure to remove\n  necessity to always fully recreate index if IndexCreation marker encountered.\n\n* Removed separate FlushThread (for views syncing) and merged it with the\n  RocksDBBackgroundThread.\n\n* Fix some issues with WAL recovery for views. Previously it was possible that\n  changes to a view/link were already recovered and persisted, but that the\n  lower bound WAL tick was not moved forward. This could lead to already fully\n  recovered views/links being recovered again on the next restart.\n\n* Put hotbackup requests on the HIGH priority queue to make hotbackups work\n  under high load (BTS-865).\n\n* Allow starting with option value `--cache.size 0`, to turn off in-memory\n  caches for indexes entirely (for performance testing or limiting memory\n  usage).\n\n* Updated OpenSSL to 1.1.1o and OpenLDAP to 2.6.2.\n\n* Added option `--enable-revision-trees` to arangorestore, which will add the\n  attributes `syncByRevision` and `usesRevisionsAsDocumentIds` to the\n  collection structure if they are missing. As a consequence, these collections\n  created by arangorestore will be able to use revision trees and a faster\n  getting-in-sync procedure after a restart. The option defaults to `true`,\n  meaning the attributes will be added if they are missing. If the option is\n  set to `false`, the attributes will not be added to the collection structure.\n  If the attributes are already present in the dump data, they will not be\n  modified by arangorestore irrespective of the setting of this option.\n\n* Fix agency inception when one of the gossip peers responds with HTTP 503 or\n  another unexpected error.\n\n* Make sure that newly created TTL indexes do not use index estimates, which\n  wouldn't be used for TTL indexes anyway.\n\n* Improve log output for WAL recovery, by providing more information and making\n  the wording more clear.\n\n* Fix: Highly unlikely race in cluster maintenance. For every shard only one\n  operation (change attribute, change leadership) should be performed at the\n  same time. However if two changes are detected in the same hearbeat  it could\n  lead to both operations to be executed in parallel. In most cases this is also\n  fine, but could lead to races on the same attribute, however the race will be\n  sorted out in the next heartbeat interval.\n\n* Fix: for the Windows build, the new Snappy version, which was introduced in\n  3.9, generated code that contained BMI2 instructions which where introduced\n  with the Intel Haswell architecture. However, our target architecture for 3.9\n  is actually Sandy Bridge, which predates Haswell. Running the build on these\n  older CPUs thus resulted in illegal instruction exceptions.\n\n* Increase internal transaction lock timeout on followers during cluster write\n  operations. Although writes to the same keys on followers should be serialized\n  by the key locks held on the leader, it is still possible that the global\n  transaction lock striped mutex is a source of contention and that concurrent\n  write operations time out while waiting to acquire this global mutex. The lock\n  timeout on followers is now significantly increased to make this very\n  unlikely.\n\n* Improve validation for variables used in the `KEEP` part of AQL COLLECT\n  operations. Previously referring to a variable that was introduced by the\n  COLLECT itself from out of the KEEP part triggered an internal error. The case\n  is detected properly now and handled with a descriptive error message.\n\n* Added startup option `--rocksdb.transaction-lock-stripes` to configure the\n  number of lock stripes to be used by RocksDB transactions. The option defaults\n  to the number of available cores, but is bumped to a value of 16 if the number\n  of cores is lower.\n\n* Fix deadlocked shard synchronisations when planned shard leader has not yet\n  taken over leadership.\n\n* Added an IO heartbeat which checks that the underlying volume is writable with\n  reasonable performance. The test is done every 15 seconds and can be\n  explicitly switched off. New metrics to give visibility if the test fails:\n    - `arangodb_ioheartbeat_delays_total`: total number of delayed io heartbeats\n    - `arangodb_ioheartbeat_duration`: histogram of execution times [us]\n    - `arangodb_ioheartbeat_failures_total`: total number of failures\n  These metrics are only populated, if `--database.io-heartbeat` is set to\n  `true` (which is currently the default).\n\n* Fix lock order in Agent::advanceCommitIndex for State's _logLock and Agent's\n  _waitForCV.\n\n* BugFix (enterprise-only): (BTS-787) In a hybrid disjoint SmartGraph, having\n  more than one relation, if you add a new vertex collection to a Smart -> Smart\n  edge relation this vertex collection was rejected with \"has to be satellite\"\n  error.\n  Now the collection is created as a SmartVertexCollection as desired.\n\n* Resync follower shard after a follower restart immediately and not lazily.\n\n\nv3.9.1 (2022-04-04)\n-------------------\n\n* Updated ArangoDB Starter to 0.15.4.\n\n* Improve parallelism in arangorestore in case new data format is used.\n\n* Remove error handling fetching license information to improve user\n  experience. To display the license informationn in the UI is only\n  informational. It disturbs the user experience to know somthing went wrong\n  and doesn't provide any important information for the user.\n\n* Added new server option: --icu-language. Used instead of --default-language to\n  set pure ICU collator.\n  For example, in Sweden language(\"sv\") lowercase letters should precede\n  uppercase ones. You can achieve it using following options when server starts:\n\n    `--icu-language sv`\n\n* No longer put document writes from replication into the audit log by default.\n  Same with low priority authentication like internal UI requests to .html files\n  for the UI. This solves a performance problem for shards getting in sync with\n  audit log switched on.\n\n* Updated OpenSSL to 1.1.1n and OpenLDAP to 2.6.1.\n\n* Fixed an assertion failure which could occur when there was an error in the\n  HTTP header, so that the message body was not actually read.\n\n* Fixed a crash which could occur when there was an error in the HTTP header\n  parsing.\n\n* Bug-Fix: Resolve BTS-673/Issue #15107, a spliced subquery could return too few\n  results.\n\n* Speed up initial sync (in case there is already data present) by prefetching\n  data from leader.\n\n* Fixed ES-1078: The REST API endpoint for handling `/_api/user/${user}/config`\n  did not work properly. The supplied data by sending a PUT request has not been\n  stored to the correct location. The Web UI uses this endpoint to store its\n  graph properties for storing the visualization properties. As this endpoint\n  did not work as expected, the graph visualization properties did not get\n  persisted as well. This is now resolved.\n\n* Fix counts and file size sum in hotbackup META files. Do no longer count\n  directories.\n\n* Optimize further RocksDB throttle to allow for no change on any given\n  calculation cycle.\n\n* Fix UI to only fetch license info as admin user.\n\n* Added `disableIndex` index hint for AQL FOR loops. This index hint disables\n  the usage of any index (except geo or full text indexes) and will cause a full\n  scan over the collection.\n  In some circumstances a full scan can be more efficient than an index scan,\n  for example if the index scan produces many matches (close to the number of\n  documents in the collection) and the index is not fully covering the query.\n  The `disableIndex` hint can be given per FOR loop in the query, e.g.:\n\n      FOR doc IN collection OPTIONS { disableIndex: true }\n        RETURN doc.value\n\n  The default value of `disableIndex` is `false`.\n  In case a different index hint is provided, `disableIndex: true` takes\n  precendence and produces a warning about the ambiguous settings.\n\n* Added `maxProjections` hint for AQL FOR loops. This hint can be used to set\n  the maximum number of document attributes that are taken into account for\n  using projections.\n\n  For example, in the following query, no projections will be used because the\n  number of potential projection attributes (`value1`, value2`, `value3`) is\n  higher than the maximum number of projection attributes set via the\n  `maxProjections` option:\n\n      FOR doc IN collection OPTIONS { maxProjections: 2 }\n        RETURN [ doc.value1, doc.value2, doc.value3 ]\n\n  The default value for `maxProjections` is `5`, which is compatible with the\n  previous hard-coded default value.\n\n* Avoid a deadlock in agency, when Inception::gossip() acquired a mutex, then\n  could call Agent under the mutex, and Agent could finally could call\n  Inception::signalConditionVar(), which would try to acquire the mutex again.\n\n* Avoid multiple parallel SIGHUP requests to be handled at the same time.\n  Now collapse multiple incoming SIGHUP requests into a single one, which can be\n  executed race-free.\n\n* Upgraded JavaScript \"i\" module from 0.3.6 to 0.3.7.\n\n* Removed internal JavaScript dependencies \"mocha\" and \"chalk\". We recommend\n  always bundling your own copy of third-party modules, even ones listed as\n  public.\n\n* Reduce memory usage of inner joins if they were performed by the IndexExecutor\n  with dynamic index lookup expressions that needed to be recomputed for input\n  from the outer loop.\n\n  For example, in the query\n  ```\n  FOR i IN 1..1000\n    FOR doc IN collection\n      FILTER doc.indexAttribute == i\n      RETURN doc\n  ```\n  the inner loop will be executed 1000 times. The IndexExecutor in the inner\n  loop needed to rebuild the index lookup attribute from the value of `i` 1000\n  times as well. The memory for index lookup attributes came from the Ast's\n  memory allocator and was not freed until the end of the query. In this query,\n  it would mean that up to 1000 lookup values were held in memory. With larger\n  inputs even more memory would be used.\n\n  Now the memory for index lookup values is freed when a new lookup value is\n  computed, i.e. only a single lookup value is held in memory.\n  This drastically reduces peak memory usage for queries that use index lookups\n  in inner loops and that get lots of different inputs from outer loops.\n\n* Harden validator for binary VelocyPack against additional types of malicious\n  inputs.\n\n* Bugfix: DC-2-DC Disjoint-SmartGraphs and Hybrid-SmartGraphs are now replicated\n  to the follower data-center keeping their sharding intact.\n\n* As we are now in constant stall regime, stall onset and warnings are demoted\n  to DEBUG.\n\n* Shorten the license grace period to the documented 3 hours.\n\n* Replaced internal JS dependency xmldom with @xmldom/xmldom.\n\n* Fixed BTS-750: Fixed the issue restricted to cluster mode in which queries\n  containing the keywords UPDATE or REPLACE together with the keyword WITH and\n  the same key value would result in an error. For example:\n  `UPDATE 'key1' WITH {_key: 'key1'} IN Collection`\n  because the same key used to update was provided in the object to update the\n  document with.\n\n* Replaced internal JS dependency ansi-html with ansi-html-community.\n\n* In an attempt to make the performance of the RocksDB throttle much more\n  consistent and predictable the default compaction slow down trigger is lowered\n  to 128kB.\n\n* Bug-Fix: AQL WINDOW statement if applied within a subquery could accidentally\n  skip over some subquery results. This did only show up if the subquery fills\n  exactly one internal batch before it is completed, so it is rather unlikely.\n\n* Fix null pointer access when using WINDOW operation with a COUNT/LENGTH\n  aggregate function without any arguments.\n\n* Reintroduce shard synchronization cancellation check that was disabled before.\n\n* Fixed BTS-621 Fixed rare case of segfault in cluster during database recovery\n  if DBServer is in upgrade mode in the same time.\n\n* Fixed PRESUPP-445: Foxx queues: Some Jobs are never run in case of multiple\n  Coordinators.\n\n* Fixed a race detected with chaos tests, where a db server could have\n  momentarily lost leadership, just when it was about to drop a follower to a\n  shard.\n\n* Fixed issue #15501: Regression when using \"exclusive\" query option?\n  This fixes a regression in AQL query execution when exclusive locks are used\n  for a query and the query also uses the DOCUMENT() AQL function. In this case,\n  when there were more concurrent requests to the underlying collection than\n  available scheduler threads for the low priority queue, the query start\n  successfully acquired the exclusive lock, but could not get its follow-up\n  requests through and starve while holding the exclusive lock.\n\n* Disable optimizer rule \"optimize-cluster-single-document-operations\" when a\n  collection is accessed in exclusive mode, because the optimized query would\n  use a slightly different mode of locking then.\n\n\nv3.9.0 (2022-02-07)\n-------------------\n\n* Convert v3.9.0-rc.1 into v3.9.0 (GA).\n\n\nv3.9.0-rc.1 (2022-02-03)\n------------------------\n\n* Fix potential access to dangling reference in cancellation of shard\n  synchronization.\n\n* Fixed BTS-740 (no released version infected) fixed Smart<->Sat SmartEdgeCollections\n  determining the shard in SingleRemoteModification Nodes was incorrect. E.g. this could\n  be triggered, by viewing the details of an edge in the UI. Only alpha/beta of 3.9.0\n  contained this bug\n\n* Fixed BTS-729 (no released version infected): Some conditions in a Hybrid\n  Smart Graph could led to wrong shard location calculation and therefore to\n  wrong graph query results. Only alpha/beta of 3.9.0 contained this bug.\n\n* Fixed minDepth handling of weighted Traversals. When using a minDepth of 3, also paths of length\n  2 have been returned, on all locally executed variants (SingleServer, OneShard, DisjointSmart).\n\n* Fixed BTS-728 (no released version infected) fixed: for DisjointSmartGraphs, that include\n  a satellite vertex collection, valid disjoint path were not always followed, if one of the\n  satellites has a connection to two (or more) vertices that have different shardValues that\n  by chance are routed to the same shard.\n\n* Fix creation of satellite graphs with a `numberOfShards` value != 1.\n\n* Fixed BTS-712: Collation analyzer now always produces valid UTF-8 sequence.\n\n* Updated Enterprise license behavior: now there will be a 48 hour period for a\n  new deployment and upgrade decision to provide the license. After that period,\n  the read-only mode will be enforced.\n  Upgrade procedure for a deployment without license will not take upgrade\n  period into account for the read-only mode will enforcement.\n\n* Fixed a bug that hotbackup upload could miss files (fixes BTS-734).\n\n* BTS-590: When creating a new database in Web UI the value of the write concern\n  has to be smaller or equal to the replication factor. Otherwise an error\n  message will be displayed and no database will be created.\n\n* Fixed potentially undefined behavior for exit code of arangodump.\n\n* Ignore signals such as SIGPIPE in client tools.\n\n* Validate that selected `writeConcern` value is less or equal to the selected\n  `replicationFactor` value when creating new databases.\n\n* Fixed issue #15476: FATAL {crash} occurs on a simple query.\n\n\nv3.9.0-beta.1 (2022-01-06)\n--------------------------\n\n* Fixed ES-1025: fixed a performance regression caused by different hash\n  calculation for primitive types like `uint64_t` and new the `Identifier`\n  wrapper and derived types.\n\n* BTS-707: rename \"hardened\" option value for `--server.support-info-api`\n  startup option to \"admin\".\n\n* APM-292: Added new AQL function SHARD_ID.\n\n* Extend timeouts for caching collection counts and index selectivity estimates\n  on coordinators from 15s/90s to 180s. This change will cause less requests to\n  be made from coordinators to DB servers to refresh info about collection\n  counts and index estimates as part of AQL queries. The cached info is used in\n  cluster query execution plans only and is not required to be fully up-to-date.\n\n* Improved performance in replication dump protocol by inserting arrays of\n  documents instead of one document at a time and also not retrieving the\n  document revision field when not needed.\n\n* APM-78: Added startup security option `--foxx.allow-install-from-remote` to\n  allow installing Foxx apps from remote URLs other than Github. The option is\n  turned off by default.\n\n* Fixed BTS-693: Sort-limit rule now always ensures proper LIMIT node placement\n  to avoid possible invalid results in the fullCount data\n\n* Updated OpenSSL to 1.1.1m and OpenLDAP to 2.6.0.\n\n* Updated arangosync to 2.7.0.\n\n* Fixed PRESUPP-439: In arangoimport, for CSV and TSV files, it could happen\n  that a buffer containing only the header would be sent to the server, and also\n  batches would contain the documents equivalent to the csv rows in them, but\n  not the header, which should be sent together with the documents.\n\n* Changed various default values for RocksDB to tune operations for different\n  typical scenarios like gp2 type volumes and gp3 type volumes and locally\n  attached SSDs with RAID0:\n  - `--rocksdb.level0-slowdown-trigger` has been decreased from 20 to 16\n  - `--rocksdb.level0-stop-trigger` has been increased from 36 to 256\n  - `--rocksdb.max-background-jobs` has been increased to the number of cores\n    and is no longer limited to 8\n  - `--rocksdb.enabled-pipelined-write` is now `true` by default instead of\n    `false`\n  - `--rocksdb.throttle-frequency` has been decreased from 60000ms down to\n    1000ms per iteration, which makes the RocksDB throttle react much quicker\n  - `--rocksdb.pending-compactions-slowdown-trigger` has been decreased from 64\n    GB down to 8 GB\n  - `--rocksdb.pending-compactions-stop-trigger` has been decreased from 256 GB\n    down to 16 GB\n  - `--rocksdb.throttle-slots` has been increased from 63 to 120\n  - `--rocksdb.encryption-hardware-acceleration` is now `true` by default,\n    which helps performance and should not create any problems, since we\n    require sandybridge anyway.\n  Combined, these changes help ArangoDB/RocksDB to react quicker to a backlog of\n  background jobs and thus to prevent catastrophic stops which abort data\n  ingestion or lead to cluster internal timeouts.\n\n* Adjust default value for startup option `--rocksdb.max-subcompactions` from 1\n  to 2. This allows compactions jobs to be broken up into disjoint ranges which\n  can be processed in parallel.\n\n* Added startup options to adjust previously hard-coded parameters for RocksDB's\n  behavior:\n\n  - `--rocksdb.pending-compactions-bytes-slowdown-trigger` controls RocksDB's\n    setting `soft_pending_compaction_bytes_limit`, which controls how many\n    pending compaction bytes RocksDB tolerates before it slows down writes.\n  - `--rocksdb.pending-compactions-bytes-stop-trigger` controls RocksDB's\n    setting `hard_pending_compaction_bytes_limit`, which controls how many\n    pending compaction bytes RocksDB tolerates before it stops writes entirely.\n  - `--rocksdb.throttle-lower-bound-bps`, which controls a lower bound for the\n    bandwidth restriction on RocksDB writes the throttle imposes.\n\n* Allow initial, full dump shard synchronization to abort prematurely if it\n  turns out that the follower was removed from the plan as a follower (e.g. if\n  there are enough other in-sync followers).\n\n* Set the limit for ArangoSearch segment size to 256MB during recovery to avoid\n  OOM kill in rare cases.\n\n* Cancel ongoing RocksDB compactions on server shutdown.\n\n* Updated Enterprise license behavior: now there will be a one hour period for\n  a new deployment to provide the license. After that period, the read-only mode\n  will be enforced.\n\n* Added startup options to adjust previously hard-coded parameters for the\n  RocksDB throttle:\n  - `--rocksdb.throttle-frequency`: frequency for write-throttle calculations\n    (in milliseconds, default value is 60000, i.e. 60 seconds).\n  - `--rocksdb.throttle-slots`: number of historic measures to use for throttle\n    value calculation (default value is 63).\n  - `--rocksdb.throttle-scaling-factor`: adaptiveness scaling factor for write-\n    throttle calculations (default value is 17). There is normally no need to\n    change this value.\n  - `--rocksdb.throttle-max-write-rate`: maximum write rate enforced by the\n    throttle (in bytes per second, default value is 0, meaning \"unlimited\").\n    The actual write rate will be the minimum of this value and the value the\n    throttle calculation produces.\n  - `--rocksdb.throttle-slow-down-writes-trigger`: number of level 0 files whose\n    payload is not considered in throttle calculations when penalizing the\n    presence of L0 files. There is normally no need to change this value.\n\n  All these options will only have an effect if `--rocksdb.throttle` is enabled\n  (which is the default). The configuration options introduced here use the\n  previously hard-coded settings as their default values, so there should not be\n  a change in behavior if the options are not adjusted.\n\n* Improve visibility in case of potential data corruption between primary index\n  and actual document store in documents column family.\n\n* Fixed BTS-611: In some cases AQL queries, in particular in a cluster, reported\n  the wrong fullCount when the optimizer rule(s)`late-document-materialization`\n  and/or `sort-limit` were active.\n\n* Fix BTS-535: TakeoverShardLeadership waits properly for Current data in\n  ClusterInfo. This avoids a fake warning and fake test failure.\n\n* Fix potential read inconsistency for single document operations.\n  When reading a single document that is concurrently being updated or replaced,\n  the read operation could erroneously return a \"document not found\" error\n  although the document actually existed. This only happened for single document\n  operations, i.e., no transactions or AQL queries.\n\n* APM-256: make arangoexport escape potential formulae in CSV exports.\n  This addresses a potential security issue when exporting specially crafted\n  documents to CSV, opening the CSV file in MS Excel or OpenOffice and then\n  clicking links in any of the tainted cells.\n\n  This change also adds a new option `--escape-csv-formulae` to toggle the\n  escaping behavior for potential formulae values. The option is turned on by\n  default.\n\n* Second step of hotbackup transfer job cleanup. Now locks are also cleaned up\n  as well as old, seemingly unfinished jobs.\n\n* Fix GitHub issue #15084. Fixed a potential use-after-free on Windows for\n  queries that used the NeighborsEnumerator (though other PathEnumerators\n  might have been affected as well).\n\n* BTS-624: The `move-calculations-up` optimization rule is now also applied to\n  subqueries, when they don't have dependencies on the outer nodes, don't have\n  modification nodes and don't read their own writes. This fixed the execution\n  of a query without the splicing-subqueries option being faster than the\n  execution of a query with this option (after version 3.8, this option cannot\n  be switched off).\n\n* Added the following metrics for revision trees:\n  - `arangodb_revision_tree_hibernations_total`: number of times a revision tree\n    was compressed in RAM and set to hibernation\n  - `arangodb_revision_tree_resurrections_total`: number of times a revision\n    tree was resurrected from hibernation and uncompressed in RAM\n  - `arangodb_revision_tree_memory_usage`: total memory usage (in bytes) by all\n    active revision trees\n\n* Added metric `rocksdb_wal_sequence` to track the current tip of the WAL's\n  sequence number.\n\n\nv3.9.0-alpha.1 (2021-11-30)\n---------------------------\n\n* Cleanup and properly fail hotbackup upload and download jobs if a dbserver\n  fails or is restarted during the transfer. This gets rid of upload and\n  download blockages in these circumstances.\n\n* Make `db.<collection>.figures(true)` operate on the same snapshot when\n  counting the number of documents in the documents column family and the\n  indexes. This ensures consistency for the results of a single figures result.\n\n* Upgraded bundled version of RocksDB to 6.27.\n\n* Improved sync protocol to commit after each chunk and get rid of potentially\n  dangerous NO_INDEXING optimization.\n\n* Removed an invalid assertion that could be triggered during chaos testing in\n  maintainer mode.\n\n* Simplify the tagging of EnumerateCollectionNodes and IndexNodes with the\n  \"read-own-writes\" flag. Previously the tagging only happened after all query\n  optimizations were completed, making the tag unavailable to the optimizer.\n  Now the tag is set early on, so it is accessible by the query optimizer.\n\n* APM-187: The \"Rebalance Shards\" button now is displayed in a new tab, and it\n  is displayed for any database in cluster mode. There is also a new flag for\n  arangod, `--cluster.max-number-of-move-shards` (default = 10), which limits\n  the amount of move shards operations each time the button is clicked to\n  rebalance shards. When the button is clicked, the number of move shards\n  operations scheduled is shown, or that no operation was scheduled if the flag\n  `--cluster.max-number-of-move-shards` has a value of 0.\n\n* Make the `--version` and `--version-json` commands usable in arangobackup when\n  no positional argument (operation type) was specified. Previously,\n  arangobackup insisted on specifying the operation type alongside the\n  `--version` or `--version-json` commands.\n\n* Fixed an issue in old incremental sync protocol with document keys that\n  contained special characters (`%`). These keys could be send unencoded in the\n  incremental sync protocol, leading to wrong key ranges being transferred\n  between leader and follower, and thus causing follow-up errors and preventing\n  getting in sync.\n\n* APM-209: Histogram displaying is now switched off by default. For displaying\n  it, the new flag `histogram.generate` must be set to true. Its default value\n  is false for compatibility with other versions and also for complying with the\n  histogram not being displayed by default. If this flag is not set to true, but\n  other histogram flags are addressed, e.g. `--histogram.interval-size 500`,\n  everything will still run normally, but a warning message will be displayed\n  saying that the histogram is switched off and setting this flag would not be\n  of use. When the flag is set to true, the histogram is displayed before the\n  summary in the output.\n\n* In the shards overview the list of servers to move the leader shard to, now\n  also contains the current followers. This means that from now on also active\n  follower servers can be nominated as the leading server for that specific\n  shard.\n\n* Extend Windows minidumps with memory regions referenced from CPU registers or\n  the stack to provide more contextual information in case of crashes.\n\n* Fix issues during rolling upgrades from 3.8.0 to 3.8.x (x >= 1) and from 3.7.x\n  (x <= 12) to 3.8.3. The problem was that older versions did not handle\n  following term ids that are sent from newer versions during synchronous\n  replication operations.\n\n* Increase default stack size on Windows from 1MB to 4MB. This should allow\n  execution of larger queries without overflowing the stack.\n\n* Make background calculation of SHA hashes for RocksDB .sst files less\n  intrusive. The previous implementation frequently iterated over all files in\n  the database directory to check if it needed to ad-hoc calculate the SHA\n  hashes for .sst files it previously missed. The procedure it used was to\n  iterate over all files in the database directory and check if there were\n  matching pairs of .sst files and .sha files. This was expensive, because a\n  full directory iteration was performed and a lot of temporary strings were\n  created for filenames and used in comparisons. This was especially expensive\n  for larger deployments with lots of .sst files.\n  The expensive iteration of files in the directory is now happening less\n  frequently, and will not be as expensive as before if it runs.\n\n* Close a potential gap during shard synchronization when moving from the\n  initial sync step to the WAL tailing step. In this small gap the leader could\n  purge some of the WAL files that would be required by the following WAL\n  tailing step. This was possible because at the end of the initial sync step,\n  the snapshot on the leader is released, and there is a small window of time\n  before the follower will issue its first WAL tailing request.\n\n* Improve Shards overview in web UI: the number of currently syncing shards is\n  now displayed per collection. Additionally, shards on failed servers are now\n  displayed in a different color.\n\n* Fixed BTS-637: Slow SynchronizeShard jobs which need to copy data could block\n  quick SynchronizeShard jobs which have the data and only need to resync.\n\n* DEVSUP-899: Fixed Subquery execution in a very rare case a subquery, nested in\n  another subquery, was not executed, which is fixed now.\n  Technical details:\n  If we have two subqueries: `Outer` and `Nested` the Outer will define the\n  input for Nested. And Outer has the pattern: 1 input, subqueryDone, 1 input,\n  subqueryDone [...] and our internal batching did cut a batch like this:\n  [<...>, input (A)] | [subqueryDone, input (B), subqueryDone, <...>] than\n  Nested on input (B) was not executed. As soon as we have more than 1 input per\n  Outer, or a different cutting position, all was good.\n\n* When enabling the cluster supervision maintenance mode via the web UI, there\n  is now the possibility to select a duration for the maintenance mode.\n  Previous versions of ArangoDB always enabled the maintenance mode for one\n  hour, without allowing any choice here.\n\n* Stop calling unnecessary `/_api/wal/open-transactions` REST API before\n  starting the continuous synchronization in active fail and single server\n  replication. This request is unnecessary with the RocksDB storage engine.\n\n* Fixed potential undefined behavior in edge cache during cache migration tasks.\n  There was a short window of time in which an already freed Table could be used\n  by concurrently running edge lookups.\n\n* BTS-623: The audit log messages, when written, were not showing the log level\n  of the message, as in the example:\n    `2021-10-21T02:28:42Z | hostname | audit-authentication | n/a | _system |\n     127.0.0.1:52490 | n/a | credentials missing | /_admin/aardvark/favicon.ico`\n  With the new flag `--audit.display-log-level`, the level of the audit log\n  message can be displayed in the log text.  When set to true, this behavior is\n  expected, as in the example:\n    `2021-10-21T02:28:42Z | DEBUG | hostname | audit-authentication | n/a |\n     _system | 127.0.0.1:52490 | n/a | credentials missing |\n     /_admin/aardvark/favicon.ico`\n  The default value for the flag is false for compatibility with former\n  versions. When this flag is not used, it is considered to have the default\n  behavior (that is, set to false).\n\n* Fixed SEARCH-261: Fix possible race between file creation and directory\n  cleaner (ArangoSearch).\n\n* Fixed SEARCH-260: Fix invalid sorting order of stored features in presence of\n  primary sort (ArangoSearch).\n\n* Change error message for queries that use too much memory from \"resource limit\n  exceeded\" to \"query would use more memory than allowed\".\n\n* When using Indexes within traversals (e.g. [_from, date]) and filter based on\n  a function (e.g. FILTER path.edges[0].date <= DATE_ADD(@now, 5, \"day\")) this\n  function was passed through to the index. The index cannot evaluate this\n  function and returned incorrect results. Now all functions are evaluted before\n  looking into the index. (Fixes BTS-407)\n\n* Old license mechanism for docker containers removed.\n\n* arangorestore: Fix the order (regarding distributeShardsLike) in which\n  collections are being created during restore, which could result in an error\n  and make manual intervention necessary.\n\n* Single server license output checking fixed.\n\n* Updated ArangoDB Starter to 0.15.3.\n\n* Fix caching of collection counts and index selectivity estimates in cluster.\n  The cache values expired too early in previous versions, making the cache\n  ineffective.\n\n* Add better error message for replication request failures in case requests are\n  retried.\n\n* Make background statistics gathering more efficient by avoiding one AQL query\n  every 10 seconds that fetched the most recent stats entry. Instead, buffer the\n  entry in value after we have written it. Also spread out the statistics calls\n  by different servers more randomly, so that request spikes are avoided for\n  cluster with many coordinators that used to run their statistics queries at\n  about the same time when the instances were started simultaneously.\n\n* Fix a potential overwhelm situation on DB servers that can lead to no further\n  tasks being pulled from a DB servers queue even though there would still be\n  processing capacity and idle threads available.\n\n* Fixed compilation and linking when using glibc 2.34.\n\n* Fuerte: don't fall back to identity encoding in case of unknown encoding.\n\n* Fixed ES-881: Fixed LDAP global options. This needs to use the first active\n  provider, not just the first provider and it should be globally disabled.\n\n* Web UI: Fixes the loading of map tiles which are being used to display the\n  query output based on a world map when using SSL encryption. This lead to not\n  displaying some world map tiles correctly (OASIS-590).\n\n* Web UI - Added missing HTML escaping inside the file upload plugin used in the\n  section of deploying a new Foxx application when uploading a zip file.\n\n* Now, arangoimport supports merging of attributes. When importing data from a\n  file into a collection, a document attribute can be comprised of merging\n  attributes from the file into it, with separators and other literal strings.\n  The new document attribute will result in the concatenation of the literal\n  strings, the values of the attributes and the separators, as in the example:\n\n      arangoimport --merge-attributes fullName=[firstName]:[lastName]\n\n* Do not use an edge index for range queries, i.e. with the comparison operators\n  `>`, `>=`, `<` or `<=`, but only for equality lookups using the `==` and `IN`\n  comparison operators.\n  The edge index is not fully ordered, so while using it for range queries may\n  produce _some_ documents, it is possible that other documents from the range\n  would be skipped.\n\n* Do not rename the arangod process to \"arangod [shutting down]\" during the\n  server shutdown. The renaming can cause issues with tools that look for\n  the exact process name \"arangod\".\n\n* Remove internal AQL query option `readCompleteInput` that controled if all\n  input for a modification operation (UPDATE / REPLACE / REMOVE) are read into\n  memory first. This was a necessity with the MMFiles storage engine in cases\n  when a query read from a collection and wrote into it in the same query\n  afterwards. With the RocksDB engine and its snapshots, we never need to read\n  the entire input into memory first.\n\n* Fix windows installer PATH manipulation issue by replacing the NSIS plugin\n  (BTS-176).\n\n* Fixed counting of all read transaction as aborted. Added a new metric to count\n  read transactions.\n\n* Fixed potential issues with revision trees and document counters getting out\n  of sync with the underlying collection data.\n\n* Fix race in RocksDB throttle listener, when it was getting started lazily\n  during server shutdown.\n\n* Extended the Views web UI by letting it capture View properties that are\n  immutable once created.\n\n* Fixed BTS-602 by not starting license feature is upgrade mode.\n\n* APM-173: Now, arangobench, arangodump and arangorestore support multiple\n  coordinators, so the flag `--server.endpoint` can be used multiple times, as\n  in the example below:\n\n  arangobench \\\n    --server.endpoint tcp://[::1]::8529 \\\n    --server.endpoint tcp://[::1]::8530 \\\n    --server.endpoint tcp://[::1]::8531\n\n  This does not compromise the use of the other client tools, which preserve the\n  behavior of having one coordinator.\n\n* The server now has two flags to control the escaping control and Unicode\n  characters in the log. The flag `--log.escape` is now deprecated and, instead,\n  the new flags `--log.escape-control-chars` and `--log.escape-unicode-chars`\n  should be used.\n\n  - `--log.escape-control-chars`: this flag applies to the control characters,\n    which have hex code below `\\x20`, and also the character DEL, with hex code\n    of `\\x7f`. When its value is set to false, the control character will be\n    retained, and its actual value will be displayed when it is a visible\n    character, or a space ` ` character will be displayed if it is not a visible\n    character. The same will happen to `DEL` character (code `\\xF7`), even\n    though it is not a control character, because it is not visible. For\n    example, control character `\\n` is visible, so a `\\n` will be displayed in\n    the log, and control character `BEL` is not visible, so a space ` ` would be\n    displayed. When its value is set to true, the hex code for the character is\n    displayed, for example, `BEL` character would be displayed as its hex code,\n    `\\x07`.\n    The default value for this flag is `true` for compatibility with previous\n    versions.\n\n  - `--log.escape-unicode-chars`: when its value is set to false, the unicode\n    character will be retained, and its actual value will be displayed. For\n    example, `犬` will be displayed as `犬`. When its value is set to true, the\n    character is escaped, and the hex code for the character is displayed. For\n    example, `犬` would be displayed as its hex code, `\\u72AC`.\n    The default value for this flag is `false` for compatibility with previous\n    versions.\n\n* Fixed BTS-582: ArangoDB client EXE package for Windows has incorrect metadata.\n\n* Fixed BTS-575: Windows EXE installer doesn't replace service during upgrade in\n  silent (non-UI) mode.\n\n* APM-121: allow the UPSERT query to have indexHint as an extra parameter for\n  OPTIONS. It will be used as a hint by the inner FOR loop that is performed as\n  part of the UPSERT query, and would help in cases such as UPSERT not picking\n  the best index automatically for lookup.\n\n* Fix issue #14819: Query: AQL: missing variable # for node #... location\n  RestCursorHandler.cpp.\n\n* Added enterprise licensing support including (only for Enterprise version):\n    - additional API endpoint `_admin/license(GET/PUT)?force=true <text>`\n    - arangosh functions: `setLicense(<text>)`, `getLicense()`\n    - new error codes and metrics support\n\n* Fix issue #14807: Fix crash during optimization of certain AQL queries during\n  the remove-collect-variables optimizer rule, when a COLLECT node without\n  output variables (this includes RETURN DISTINCT) occurred in the plan.\n\n* Update iresearch library to the upstream. Fixed TSan/ASan detected issues.\n\n* Added new ArangoSearch analyzer type 'collation'.\n\n* Add basic overload control to arangod.\n  This change adds the `x-arango-queue-time-seconds` header to all responses\n  sent by arangod. This header contains the most recent request dequeuing time\n  (in seconds) as tracked by the scheduler. This value can be used by client\n  applications and drivers to detect server overload and react on it.\n  The new startup option `--http.return-queue-time-header` can be set to `false`\n  to suppress these headers in responses sent by arangod.\n\n  In addition, client applications and drivers can optionally augment their\n  requests sent to arangod with a header of the same name. If set, the value of\n  the header should contain the maximum queuing time (in seconds) that the\n  client is willing to accept. If the header is set in an incoming request,\n  arangod will compare the current dequeuing time from its scheduler with the\n  maximum queue time value contained in the request. If the current dequeuing\n  time exceeds the value set in the header, arangod will reject the request and\n  return HTTP 412 (precondition failed) with the new error code 21004 (queue\n  time violated).\n\n  There is also a new metric `arangodb_scheduler_queue_time_violations_total`\n  that is increased whenever a request is dropped because of the requested queue\n  time not satisfiable.\n\n* Fixed a bug for array indexes on update of documents (BTS-548).\n\n* Prevent some possible deadlocks under high load regarding transactions and\n  document operations, and also improve performance slightly.\n\n* Hide help text fragment about VST connection strings in client tools that do\n  not support VST.\n\n* Added REST API endpoint `/_admin/debug/failat/all` to retrieve the list of\n  currently enabled failure points. This API is available only if failure\n  testing is enabled, but not in production.\n\n* APM-60: optionally allow special characters and Unicode characters in database\n  names.\n\n  This feature allows toggling the naming convention for database names from the\n  previous strict mode, which only allowed selected ASCII characters in database\n  names, to an extended, more relaxed mode. The extended mode allows additional\n  ASCII characters in database names as well as non-ASCII UTF-8 characters.\n  The extended mode can be enabled by setting the new startup option\n  `--database.extended-names-databases` to true. It is turned off by default and\n  requires an explicit opt-in, simply because some drivers and client\n  applications may not be ready for it yet. The arangod server, the ArangoDB web\n  interface and the following bundled client tools are prepared and ready for\n  using the extended database names:\n  - arangobench\n  - arangodump\n  - arangoexport\n  - arangoimport\n  - arangorestore\n  - arangosh\n  More tools and the drivers shipped by ArangoDB will be added to the list in\n  the future.\n\n  Please note that the extended names for databases should not be turned on\n  during upgrades from previous versions, but only once the upgrade has been\n  completed successfully. In addition, the extended names should not be used in\n  environments that require extracting data into a previous version of ArangoDB,\n  or when database dumps may be restored into a previous version of ArangoDB.\n  This is because older versions may not be able to handle the extended database\n  names. Finally, it should not be turned on in environments in which drivers\n  are in use that haven't been prepared to work with the extended naming\n  convention.\n\n  Warning: turning on the `--database.extended-names-databases` option for a\n  deployment requires it to stay enabled permanently, i.e. it can be changed\n  from `false` to `true` but not back. When enabling it, it is also required to\n  do this consistently on all coordinators and DB servers.\n\n  The extended names for databases will be enabled by default in one of the\n  future releases of ArangoDB, once enough drivers and other client tools have\n  had the chance to adapt.\n\n  Naming conventions for collections, views, analyzers, and document keys\n  (`_key` values) are not affected by this feature and will remain as in\n  previous versions of ArangoDB.\n\n* Prevent stealing of values from AQL const value registers. This fixes an issue\n  for queries that produce constant results (known at query compile time) when\n  he queries are executed directly on a DB server in a cluster (which is not\n  supported, but may happen for troubleshooting).\n\n* Fixed BTS-562: reduce-extraction-to-projection optimization returns null for\n  one attribute if nested attributes are named the same.\n\n* Add `--datatype` startup option to arangoimport, in order to hard-code the\n  datatype (null/boolean/number/string) for certain attributes in the CSV/TSV\n  import.\n  For example, given the following input file:\n\n      key,price,weight,fk\n      123456,200,5,585852\n      864924,120,10,9998242\n      9949,70,11.5,499494\n      6939926,2130,5,96962612\n\n  When invoking arangoimport with the startup options\n\n      --datatype key=string\n      --datatype price=number\n      --datatype weight=number\n      --datatype fk=string\n\n  it will turn the numeric-looking values in \"key\" into strings (so that they\n  can be used in the `_key` attribute), but treat the attributes \"price\" and\n  \"weight\" as numbers. The values in attribute \"fk\" finally will be treated as\n  strings again (potentially because they are used for linking to other \"_key\"\n  values).\n\n* Avoid the acquisition of a recursive read lock on server shutdown, which could\n  in theory lead to shutdown hangs at least if a concurrent thread is trying to\n  modify the list of collections (very unlikely and never observed until now).\n\n* Fixed display of unicode characters in Windows console.\n\n* Fixed issue BTS-531 \"Error happens during EXE package installation if\n  non-ASCII characters are present in target path\".\n\n* Fix active failover, so that the new host actually has working Foxx services\n  (BTS-558).\n\n* Fixed issue #14720: Bulk import ignores onDuplicate in 3.8.0.\n  The \"onDuplicate\" attribute was ignored by the `/_api/import` REST API when\n  not specifying the \"type\" URL parameter.\n\n* Updated OpenSSL to 1.1.1l and OpenLDAP to 2.4.59.\n\n* APM-70: allow PRUNE condition to be stored in a variable.\n\n  This feature allows the PRUNE condition to be stored in a variable, and this\n  variable can be used as a condition for some other statement, such as FILTER.\n\n* Allow startup of arangod with an existing database directory that was missing\n  the ZkdIndex column family.\n\n* Truncate must not trigger intermediate commits while in a streaming\n  transaction, because that would be against the assumption that streaming\n  transactions never do intermediate commits.\n\n* Added ArangoSearch condition optimization: STARTS_WITH is merged with\n  LEVENSHTEIN_MATCH if used in the same AND node and field name and prefix\n  matches.\n\n* Hybrid (Disjoint) SmartGraphs (Enterprise Edition):\n  SmartGraphs have been extended with a new option to create Hybrid SmartGraphs.\n  Hybrid SmartGraphs are capable of using SatelliteCollections within their\n  graph definition. You can now select some VertexCollections to be satellites,\n  and therefore available on all DBServers. The SmartGraph can make use of those\n  to collections to increase the traversal performance by larger local\n  components.\n\n* Added multidimensional indexes which can be used to efficiently intersect\n  multiple range queries. They are currently limited to IEEE-754 double values.\n  Given documents of the form {x: 12.9, y: -284.0, z: 0.02} one can define a\n  multidimensional index using the new type 'zkd' on the fields [\"x\", \"y\", \"z\"].\n\n  The AQL optimizer will then consider this index when doing queries on multiple\n  ranges, for example:\n\n    FOR p IN points\n        FILTER x0 <= p.x && p.x <= x1\n        FILTER y0 <= p.y && p.y <= y1\n        FILTER z0 <= p.z && p.z <= z1\n        RETURN p\n\n  The index implements the relation <=, == and >= natively. Strict relations are\n  emulated using post filtering. Ranges can be unbounded on one or both sides.\n\n* No runtime limits for shard move and server cleanout jobs, instead possibility\n  to cancel them.\n\n* Fix cluster-internal network protocol to HTTP/1 for now. Any other protocol\n  selected via the startup option `--network.protocol` will automatically be\n  switched to HTTP/1. The startup option `--network.protocol` is now deprecated\n  and hidden by default. It will be removed in a future version of arangod.\n  The rationale for this change is to move towards a single protocol for\n  cluster-internal communication instead of 3 different ones.\n\n* Disable RTTI when compiling Snappy. RTTI used to be disabled previously, up\n  until some Merkle tree improvement PR was merged about one month ago, which\n  turned on RTTI for compiling Snappy.\n\n* (EE only) Bug-fix: If you created a ArangoSearch view on satellite collections\n  only and then join with a collection only having a single shard the\n  cluster-one-shard-rule was falsely applied and could lead to empty view\n  results. The Rule will now detect the situation properly, and not trigger.\n\n* (EE only) If you have a query using only satellite collections, now the\n  cluster-one-shard-rule can be applied to improve query performance.\n\n* (Enterprise Edition only): added query option `forceOneShardAttributeValue`\n  to explicitly set a shard key value that will be used during query snippet\n  distribution to limit the query to a specific server in the cluster.\n\n  This query option can be used in complex queries in case the query optimizer\n  cannot automatically detect that the query can be limited to only a single\n  server (e.g. in a disjoint smart graph case).\n  When the option is set to the correct shard key value, the query will be\n  limited to the target server determined by the shard key value. It thus\n  requires that all collections in the query use the same distribution (i.e.\n  `distributeShardsLike` attribute via disjoint SmartGraphs).\n\n  Limiting the query to a single DB server is a performance optimization and may\n  make complex queries run a lot faster because of the reduced setup and\n  teardown costs and the reduced cluster-internal traffic during query\n  execution.\n\n  If the option is set incorrectly, i.e. to a wrong shard key value, then the\n  query may be shipped to a wrong DB server and may not return results (i.e.\n  empty result set). It is thus the caller's responsibility to set the\n  `forceOneShardAttributeValue` correctly or not use it.\n\n  The `forceOneShardAttributeValue` option will only honor string values.\n  All other values as well as the empty string will be ignored and treated as if\n  the option is not set.\n\n  If the option is set and the query satisfies the requirements for using the\n  option, the query's execution plan will contain the \"cluster-one-shard\"\n  optimizer rule.\n\n* SEARCH-238: Improved SortNodes placement optimization in cluster so\n  late materialization could cover more cases\n\n* Fix some memory leaks after adding optimization rule for AqlAnalyzer.\n\n* Fix internal iterator states after intermediate commits in write transactions.\n  Iterators could point to invalid data after an intermediate commit, producing\n  undefined behavior.\n\n* Fix read-own-write behavior in different scenarios:\n    - in some cases writes performed by an AQL query could be observed within\n      the same query. This was not intended and is fixed now.\n    - AQL queries in streaming transactions could observe their own writes in\n      even more cases, which could potentially result in an endless loop when\n      the query iterates over the same collection that it is inserting documents\n      into.\n    - UPSERT did not find documents inserted by a previous iteration if the\n      subquery relied on a non-unique secondary index.\n    - disabled intermediate commits for queries with UPSERTs, because\n      intermediate commits can invalidate the internal read-own-write iterator\n      required by UPSERT. Previously, UPSERTs that triggered intermediate\n      commits could have produced unexpected results (e.g., previous inserts\n      that have been committed might not be visible) or even crashes.\n  To achieve the correct read-own-write behavior in streaming transactions, we\n  sometimes have to copy the internal WriteBatch from the underlying RocksDB\n  transaction. In particular, the copy is created whenever an AQL query with\n  modification operations (INSERT/REMOVE/UPDATE/UPSERT/REPLACE) is executed in\n  the streaming transaction. If there have not been any other modifications so\n  far (queries/document operations), then the WriteBatch is empty and creating\n  the copy is essentially a no-op. However, if the transaction already contains\n  a lot of modifications, creating the WriteBatch copy might incur some\n  overhead that can now lead to decreased performance.\n\n* Fix rare case of invalid data that could be inserted into the ArangoSearch\n  index if several clients concurrently insert data and use custom analyzer\n  with non-string return type.\n\n* Fix a rare shutdown race in RocksDBShaCalculatorThread.\n\n* Added \"Analyzers\" view to web UI to let manage ArangoSearch analyzers\n  creation.\n\n* Add pseudo log topic \"all\" to set the log levels for all log topics at once.\n  For example, this can be used when starting a server with trace or debug\n  logging enabled for all log topics, e.g.\n\n  `--log.level all=debug`\n  `--log.level all=trace`\n\n  This is very coarse and should only be used for such use cases.\n\n* Change the default value for the `--threads` startup parameter of the\n  following client tools from previously 2 to the maximum of 2 and the number of\n  available CPU cores:\n  - arangodump\n  - arangoimport\n  - arangorestore\n\n* Remove old fixPrototypeChain agency migration, which was introduced in 3.2 and\n  is no longer necessary. This will make it impossible to upgrade directly from\n  a version < 3.2 to a version >= 3.9, provided one has a chain of\n  `distributeShardsLike` collections.\n\n* Added metrics for the number of errors and warnings logged:\n  - `arangodb_logger_warnings_total`: total number of warnings (WARN messages)\n    logged since server start\n  - `arangodb_logger_errors_total`: total number of errors (ERR messages)\n    logged since server start\n\n* Added REST API `/_admin/support-info` to retrieve deployment information.\n  As this API may reveal sensitive data about the deployment, it can only be\n  accessed from inside the system database. In addition, there is a policy\n  control startup option `--server.support-info-api` that determines if and to\n  whom the API is made available. This option can have the following values:\n  - `disabled`: support info API is disabled.\n  - `jwt`: support info API can only be accessed via superuser JWT.\n  - `hardened`: if `--server.harden` is set, the support info API can only be\n    accessed via superuser JWT. Otherwise it can be accessed by admin users\n    only.\n  - `public`: everyone with access to `_system` database can access the support\n    info API.\n\n* Send a keystroke to arangod's stdin when a shutdown command is received via\n  the REST API `/_admin/shutdown` and the server is started with the `--console`\n  argument. The keystroke will exit the blocking read loop that is waiting on\n  console input and that otherwise blocks the shutdown.\n  The implementation is based on ioctl and is thus only present on Linux and\n  macOS.\n\n* Some AQL queries erroneously reported the \"access after data-modification\"\n  error for queries in which there was a read attempt from a collection _before_\n  a data-modification operation. Such access is legal and should not trigger\n  said error anymore. Accessing a collection _after_ in a query a\n  data-modification in the same query is still disallowed.\n\n* Make AQL modification operations in a cluster asynchronous. This allows to\n  free the thread for other work until both the write and synchronous\n  replication are complete.\n\n* Fixed: /_api/transaction/begin called on edge collections of disjoint\n  SmartGraphs falsely returned CollectionNotFound errors.\n\n* Bugfix: In more complex queries there was a code-path where a (Disjoint-)Smart\n  graph access was not properly optimized.\n\n* Add ReplicatedLogs column family.\n\n* Add optimization rule for AqlAnalyzer.\n\n* Change optimization level for debug builds back to `-O0` (from `-Og`) because\n  `-Og` seems to cause debuggability issues in some environments.\n\n* Automatically extend web UI sessions while they are still active.\n  The web UI can now call a backend route to renew its JWT, so there will not be\n  any rude logouts in the middle of an active session.\n\n  Active web UI sessions (here: sessions with user activity within the last 90\n  minutes) will automatically renew their JWT if they get close to the JWT\n  expiry date.\n\n* Reduce memory usage for in-memory revision trees. Previously, a revision tree\n  instance for a non-empty collection/shard was using 4 MB of memory when\n  uncompressed. Trees that were unused for a while were compressed on the fly to\n  use less memory, and later uncompressed again when needed.\n  Now the uncompressed in-memory version of the revision tree will dynamically\n  allocate memory as needed. This allows the initial version of the trees to get\n  away with just 64 KB of memory. Memory usage will grow lazily when more parts\n  of the trees get populated. The compression of unused in-memory tree data is\n  still in place.\n\n* Refactored arangobench:\n  - Updated testcases to show description of them when beginning execution\n  - Fixed testcase histogram with time measures when batch size > 0\n  - Integrated testcases with Velocypack for simplification\n  - Deprecated some testcases\n  - Internal changes for performance optimization\n\n* Add 3 AQL functions: COSINE_SIMILARITY, L1_DISTANCE and L2_DISTANCE.\n\n* Honor the value of startup option `--rocksdb.sync-interval` on Windows, too.\n  Previously, the value was ignored and WAL syncing on Windows was using a\n  different code paths than on the other supported platforms. Now syncing is\n  unified across all platforms, and they all call RocksDB's `SyncWAL()`.\n\n* APM-132: Clean up collection statuses.\n  Removes collection statuses \"new born\", \"loading\", \"unloading\" and \"unloaded\".\n  These statuses were last relevant with the MMFiles storage engine, when it was\n  important to differentiate which collections are present in main memory and\n  which aren't. With the RocksDB storage engine, all that was automatically\n  handled anyway, and the statuses were not important anymore.\n\n  The change removes the \"Load\" and \"Unload\" buttons for collections from the\n  web interface. All collections in the web interface will be marked as \"loaded\"\n  permanently.\n\n  This change also obsoletes the `load()` and `unload()` calls for collections\n  as well as their HTTP API equivalents. The APIs will remain in place for now\n  but are changed to no-ops. They will removed eventually in a future version of\n  ArangoDB. This will be announced separately.\n\n* Reduce default value for max-nodes-per-callstack to 200 for macOS, because on\n  macOS worker threads have a stack size of only 512kb.\n\n* Slightly increase internal AQL query and transaction timeout on DB servers\n  from 3 to 5 minutes.\n  Previously, queries and transactions on DB servers could expire quicker, which\n  led to spurious \"query ID not found\" or \"transaction ID not found\" errors on\n  DB servers for multi-server queries/transactions with unbalanced access\n  patterns for the different participating DB servers.\n  The timeouts on coordinators remain unchanged, so any queries/transactions\n  that are abandoned will be aborted there, which will also be propagated to DB\n  servers. In addition, if a participating server in an AQL query becomes\n  unavailable, the coordinator is now notified of that and will terminate the\n  query more eagerly.\n\n* Add hard-coded complexity limits for AQL queries, in order to prevent\n  programmatically generated large queries from causing trouble (too deep\n  recursion, enormous memory usage, long query optimization and distribution\n  passes etc.).\n  This change introduces 2 limits:\n  - a recursion limit for AQL query expressions. An expression can now be up to\n    500 levels deep. An example expression is `1 + 2 + 3 + 4`, which is 3 levels\n    deep `1 + (2 + (3 + 4))`.\n    The expression recursion is limited to 500 levels.\n  - a limit for the number of execution nodes in the initial query execution\n  plan.\n    The number of execution nodes is limited to 4,000.\n\n* Remove _msg/please-upgrade handler.\n\n* Adapt various places related to handling of execution plans non-recursive in\n  order to avoid stack overflows. This allows us now to execute much larger\n  queries.\n\n* Fix locking of AQL queries write queries on DB servers.\n\n* APM-112: invalid use of OPTIONS in AQL queries will now raise a warning in the\n  query.\n  The feature is useful to detect misspelled attribute names in OPTIONS, e.g.\n\n      INSERT ... INTO collection\n        OPTIONS { overwrightMode: 'ignore' } /* should be 'overwriteMode' */\n\n  It is also useful to detect the usage of valid OPTIONS attribute names that\n  are used for a wrong query part, e.g.\n\n      FOR doc IN collection\n        FILTER doc.value == 1234\n        INSERT doc INTO other\n          OPTIONS { indexHint: 'myIndex' } /* should be used above for FOR */\n\n  In case a wrong option attribute is used, a warning with code 1575 will be\n  raised.\n  By default, warnings are reported but do not lead to the query being aborted.\n  This can be toggled by the startup option `--query.fail-on-warnings` or the\n  per-query runtime option `failOnWarnings`.\n\n* Added new command line-option `--version-json`. This will return the version\n  information as json object.\n\n* Fix ArangoAgency::version(), which always returned an empty string instead of\n  the agency's correctly reported version. This also fixes the agency version in\n  the startup log messages of the cluster.\n\n* Fix potential memleak in Pregel conductor garbage collection.\n\n* Added garbage collection for finished and failed Pregel conductors.\n  Previously, Pregel executions that finished successfully or unsuccessfully\n  remained in memory until being explicitly canceled. This prevented a cleanup\n  of abandoned jobs. Such jobs are now automatically cleaned about 10 minutes\n  after finalization. The time-to-live values can be overriden per Pregel job by\n  passing a \"ttl\" value.\n\n* Revive startup parameter `--server.session-timeout` to control the timeout for\n  web interface sessions and other sessions that are based on JWTs created by\n  the `/_open/auth` API.\n\n  This PR also changes the default session timeout for web interface sessions to\n  one hour. Older versions of ArangoDB had longer session timeouts.\n\n* Removed redirects from /_admin/cluster* to /_admin/cluster/*. Adjusted\n  internal requests to use the new url.\n\n* Fix potential stack overflow when executing large queries. This is achieved by\n  splitting the callstack and moving part of the execution to a separate thread.\n  The number of execution nodes after which such a callstack split should be\n  performed can be configured via the query option `maxNodesPerCallstack` and\n  the command line option `--query.max-nodes-per-callstack`; the default is 250.\n\n* Fixed invalid shard synchronization for documents not added via INSERT with\n  `overwriteMode` set to `ignore`. In this case, if a document with the given\n  key already exists, it is not changed on the leader (i.e. no write happens on\n  the leader). However, a write was replicated to the follower, which was wrong.\n  This write is now suppressed, which can only make such insert operations\n  faster.\n\n* Web UI: Disables the hover tooltip within the statistics view of the memory\n  consumption chart.\n\n* Add 3 AQL functions: DECAY_GAUSS, DECAY_EXP and DECAY_LINEAR.\n\n* Implemented an optimization for Traversals. If you apply a POST filter on the\n  vertex and/or edge result this filter will now be applied during the traversal\n  to avoid generating the full output for AQL. This will have positive effect if\n  you filter on the vertex/edge but return the path, this way the system does\n  only need to produce a path that is allowed to be passed through. E.g.\n\n    FOR v,e,p IN 10 OUTBOUND @start GRAPH \"myGraph\"\n      FILTER v.isRelevant == true\n      RETURN p\n\n  can now be optimized, and the traversal statement will only produce paths\n  where the last vertex has `isRelevant == true`.\n\n* Fix BTS-446: When finding a not yet fully initialized agency, do not\n  immediately fatal exit. Keep trying for (very generous) 5 minutes.\n\n* Reduced the agency store public members, for simpler support long-term.\n\n* Added a number of tests for the Agency Store public members.\n\n* Updated bundled version of Snappy library to 1.1.9.\n\n* Introduce a new internal error code for cases where a call cannot succeed\n  because the server startup phase is still in progress. This error will be\n  mapped to the HTTP status code 503 (service unavailable).\n  One example where this can happen is when trying to authenticate a request,\n  but the _users collection is not yet available in the cluster.\n\n* Fix DEVSUP-749: Fix potential deadlock when executing concurrent view/link\n  DDL operations and index DDL operations on the same collection.\n\n* Fixed issue #14122: when the optimizer rule \"inline-subqueries\" is applied,\n  it may rename some variables in the query. The variable renaming was however\n  not carried out for traversal PRUNE conditions, so the PRUNE conditions\n  could still refer to obsolete variables, which would make the query fail with\n  errors such as\n\n      Query: AQL: missing variable ... for node ... while planning registers\n\n* Fixed the error response if the HTTP version is not 1.0 or 1.1 and if\n  the Content-Length is too large (> 1 GB).\n\n* Add a connection cache for internal replication requests.\n\n* Improve legibility of size values (by adding KB, MB, GB, TB suffixes) to\n  output generated by client tools.\n\n* Timely updates of rebootId / cluster membership of DB servers and\n  coordinators in ClusterInfo. Fixes BTS-368 detected in chaos tests.\n\n* Guarded access only to ActionBase::_result.\n\n* Fixed proper return value in sendRequestRetry if server is shutting down.\n\n* Fixed internal issue #798: In  rare case when remove request\n  completely cleans just consolidated segment commit could be cancelled\n  and documents removed from collection may be left dangling in the ArangoSearch index.\n  Also fixes ES-810 and  BTS-279.\n\n* Retry if an ex-leader can no longer drop a follower because it is no longer\n  leading.\n\n* Fixed a small problem in fuerte which could lead to an assertion failure.\n\n* Upgrade jemalloc version to latest stable dev.\n\n* Fixed issue BTS-373: ASan detected possible heap-buffer-overflow at\n  arangodb::transaction::V8Context::exitV8Context().\n\n* Allow to specify a fail-over LDAP server. Instead of \"--ldap.OPTION\" you need\n  to specify \"--ldap2.OPTION\". Authentication / Authorization will first check\n  the primary LDAP server. If this server cannot authenticate a user, it will\n  try the secondary one. It is possible to specify a file containing all users\n  that the primary (or secondary) LDAP server is handling by specifying the\n  option \"--ldap.responsible-for\". This file must contain the usernames\n  line-by-line.\n\n* Make the time-to-live (TTL) value of a streaming cursor only count after\n  the response has been sent to the client.\n\n* Improve performance of batch CRUD operations (insert, update, replace,\n  remove) if some of the documents in the batch run into write-write conflicts.\n  Rolling back partial operations in case of a failure is very expensive\n  because it requires rebuilding RocksDB write batches for the transaction\n  from scratch. Rebuilding write batches takes time proportional to the number\n  of operations in the batch, and for larger batches the cost can be\n  prohibitive.\n  Now we are not rolling back write batches in some situations when this is\n  not required, so that in many cases running into a conflict does not have\n  that high overhead. There can still be issues when conflicts happen for index\n  entries, but a lot of previously problematic cases should now work better.\n\n* Allow AQL variable names starting with an underscore, as stated in the docs.\n\n* Fix crashes during arangorestore operations due to usage of wrong pointer\n  value for updating user permissions.\n\n* Added option `--query-max-runtime` to arangoexport, in order to control\n  maximum query runtime.\n\n* Fix BTS-340: AQL expressions similar to `x < 3 || x` are no longer erroneously\n  be reduced to `x < 3` by the optimizer rule remove-redundant-or.\n\n* Changed default value of arangodump's `--envelope` option from `true` to\n  `false`. This allows using higher parallelism in arangorestore when\n  restoring large collection dumps. As a side-effect, this will also decrease\n  the size of dumps taken with arangodump, and should slightly improve dump\n  speed.\n\n* Improve parallelism capabilities of arangorestore.\n\n  arangorestore can now dispatch restoring data chunks of a collection to idle\n  background threads, so that multiple restore requests can be in flight for\n  the same collection concurrently.\n\n  This can improve restore speed in situations when there are idle threads\n  left (number of threads can be configured via arangorestore's `--threads`\n  option) and the dump file for the collection is large.\n\n  The improved parallelism is only used when restoring dumps that are in the\n  non-enveloped format. This format has been introduced with ArangoDB 3.8.\n  The reason is that dumps in the non-enveloped format only contain the raw\n  documents, which can be restored independent of each other, i.e. in any\n  order. However, the enveloped format may contain documents and remove\n  operations, which need to be restored in the original order.\n\n* Fix BTS-374: thread race between ArangoSearch link unloading and storage\n  engine WAL flushing.\n\n* Fix thread race between ArangoSearch link unloading and storage engine\n  WAL flushing.\n\n* Add value of `_key` to more insert/update/replace/remove error messages so it\n  is easier to figure out which document caused unique constraint violations\n  and/or write-write conflict during a multi-document write operation.\n\n* Don't display obsoleted startup options and sections in `--help` and\n  `--help-.` commands. Also rename \"global\" to \"general\" options.\n\n* Removed assertion for success of a RocksDB function. Throw a proper exception\n  instead.\n\n* Experimentally switch to wyhash (from xxhash) for velocypack. This is an\n  experiment in devel to check if it produces any observable speedups.\n\n* Remove deprecated HTTP REST API `/_api/export`. This API was deprecated in a\n  previous version because it was not supported in clusters and was also covered\n  completely by streaming AQL queries for the RocksDB storage engine.\n\n* Added enterprise-build-repository and oskar-build-repository to `--version` as\n  `enterprise-build-repository` and `oskar-build-repository`.\n\n* Clean up replication code and remove a 3.2-compatibility mode that was only\n  useful when replicating from a leader < ArangoDB version 3.3.\n\n* Obsolete option `--database.old-system-collections`. This option has no\n  meaning in ArangoDB 3.9, as old system collections will not be created anymore\n  in this version. The option was deprecated in 3.8 and announced to be\n  obsoleted.\n\n* Upgrade velocypack to latest, C++17-only version.\n\n* Make arangovpack more powerful, by supporting different input and output\n  formats (json and vpack, plain or hex-encoded).\n  The arangovpack options `--json` and `--pretty` have been removed and have\n  been replaced with separate options for specifying the input and output types:\n    - `--input-type` (\"json\", \"json-hex\", \"vpack\", \"vpack-hex\")\n    - `--output-type` (\"json\", \"json-pretty\", \"vpack\", \"vpack-hex\")\n  The previous option `--print-non-json` has been replaced with the option\n  `--fail-on-non-json` which makes arangovpack fail when trying to emit non-JSON\n  types to JSON output.\n\n* Remove obsolete API endpoint /_admin/repair/distributeShardsLike`. This API\n  was intended to correct some bad state introduced before 3.2.12 or 3.3.4,\n  respectively. It had to be invoked manually by callers and there was never any\n  driver support for it.\n\n* Remove now-unused SubqueryExecutor. This is an internal change only and should\n  not have any effect on queries, as from 3.8 onwards only spliced subqueries\n  should be used in query execution plans and during query execution.\n\n* Switched to GCC 10 as the default compiler and use Sandy Bridge as the default\n  required architecture (Linux, macOS binaries).\n\n* Removed obsolete metrics in new v2 metric API. Those metrics' values were\n  identical to the sum value of histograms.\n\n* Fix potentially undefined behavior when creating a\n  CalculationTransactionContext for an ArangoSearch analyzer. An uninitialized\n  struct member was passed as an argument to its base class. This potentially\n  had no observable effects, but should be fixed.\n\n\nv3.8.1 (2021-08-27)\n-------------------\n\n* Reduce internal priority of AQL execution. This prevents possible deadlocks\n  with modification operations in a cluster and replicationFactor >= 2, and can\n  also improve responsiveness under high load of AQL queries.\n\n* Updated arangosync to 2.6.0.\n\n* Added protocol specific metrics: histogram about request body size, total\n  number of HTTP/2 connections and total number of VST connections.\n\n* Fix a potential multi-threading issue in index creation on coordinators, when\n  an agency callback was triggered at the same time the method\n  `ensureIndexCoordinatorInner` was left.\n\n* Append physical compaction of log collection to every Raft log compaction\n  (BTS-542).\n\n* Preselect \"create index in background\" option when creating indexes in the web\n  UI. The \"create index in background\" option can be less intrusive because it\n  allows other write operations on the collection to proceed.\n\n* Do not block a scheduler thread on the coordinator while an index is being\n  created. Instead, start a background thread for the actual index fill-up work.\n  The original thread can then be relinquished until the index is completely\n  filled or index creation has failed.\n  The default index creation timeout on coordinators has also been extended from\n  1 hour to 4 days, but it is still configurable via the startup parameter\n  `--cluster.index-create-timeout` in case this is necessary.\n\n* Fixed: getResponsibleShard call on disjoint Smart Graphs if you asked for the\n  responsible shard on a disjoint edge collection where the _from and _to differ\n  (invalid), the server would respond with \"DATASOURCE_NOT_FOUND\". This is now\n  fixed to \"BAD_PARAMETER\" to emphasize that the collection is fine but the\n  input is invalid.\n\n* Fixed: _api/transaction/begin called on edge collections of disjoint\n  SmartGraphs falsely returned CollectionNotFound errors.\n\n* Bug-Fix: In more complex queries there was a code-path where a (Disjoint-)\n  Smart graph access was not properly optimized.\n\n* Fix wrong assertion in fuerte and move it to where the TLA+ model says i\n  should be. This fixes a unit test failure occurring on newer Macs with a\n  certain clang version.\n\n* When creating Pregel memory-mapped files, create them with O_TMPFILE attribute\n  on Linux so that files are guaranteed to vanish even if a process dies.\n\n* Improve log messages for Pregel runs by giving them more context.\n\n* Fixed issue BTS-536 \"Upgrading without rest-server is aborted by error\".\n  Now stating `--server.rest-server false` does not require the additional\n  `--console` argument for upgrading a server.\n\n* Fixed issue #14592:  IS_NULL(@x) isn't recognized as a constant expression.\n\n* Fixed issue BTS-539 \"Unsynchronized query kill while it's being finalized in\n  another thread was uncovered through `test-kill.js` of `communication_ssl`\n  suite\". Fixed possible (but unlikely) crash when killing an AQL query.\n\n* Fixed various problems in GEO_INTERSECTS: wrong results, not implemented cases\n  and numerically unstable behaviour. In particular, the case of the\n  intersection of two polygons in which one is an S2LngLatRect is fixed\n  (BTS-475).\n\n* Fixed ES-867 and ES-922: removed eslint from NPM packages descriptions and\n  updated netmask package to non-vulnerable version.\n\n* Web UI: Fixes the loading of map tiles which are being used to display the\n  query output based on a world map when using SSL encryption. This lead to not\n  displaying some world map tiles correctly (OASIS-590).\n\n* Timely update of database server list on health check fixes BTS-505.\n\n* Updated JavaScript dependencies, including breaking changes to non-public\n  modules. We recommend always bundling your own copy of third-party modules,\n  even ones listed as public.\n\n  - accepts: 1.3.5 -> 1.3.7\n  - ansi_up: 4.0.3 -> 5.0.1\n  - content-type: (added) -> 1.0.4\n  - error-stack-parser: 2.0.2 -> 2.0.6\n  - highlight.js: 9.15.6 -> 10.7.3\n  - http-errors: 1.7.2 -> 1.8.0\n  - iconv-lite: 0.4.24 -> 0.6.3\n  - js-yaml: 3.13.1 -> 3.14.1\n  - lodash: 4.17.13 -> 4.17.21\n  - marked: 0.6.2 -> removed\n  - mime-types: 2.1.22 -> 2.1.31\n  - mocha: 6.1.3 -> 6.2.3\n  - netmask: 1.0.6 -> 2.0.2\n  - qs: 6.7.0 -> 6.10.1\n  - range-parser: 1.2.0 -> 1.2.1\n  - semver: 6.0.0 -> 7.3.5\n  - sinon: 1.17.6 -> 1.17.7\n  - timezone: 1.0.22 -> 1.0.23\n  - type-is: 1.6.16 -> 1.6.18\n  - underscore: 1.9.1 -> 1.13.1\n  - xmldom: 0.1.27 -> 0.6.0\n\n* Updated ArangoDB Starter to 0.15.1.\n\n* Fix BTS-453: Download of a HotBackup from remote source doesn't work on macOS.\n\n* Web UI: Fixes a logical error which occured after re-visiting the logs view\n  which lead to not displaying the logs view and its entries correctly\n  (BTS-507).\n\n* Raised the versions of the node modules `node-sass` and `sass-loader` to be\n  able to build the Web UI with Node v16+.\n\n* Suppress repeated warnings when settings LDAP options which turn out to be\n  unsupported on the target system. This avoids logging the same warnings\n  repeatedly.\n\n* Make `--javascript.copy-installation` also copy the `node_modules` sub\n  directory. This is required so we have a full copy of the JavaScript\n  dependencies and not one that excludes some infrequently changed modules.\n  In addition, file copying now intentionally excludes .map files as they are\n  not needed.\n\n* Fixed a bug, where Coordinators handled plan changes for databases in\n  heartbeat thread in wrong order. Databases could be listed, but not used.\n\n* Include K_SHORTEST_PATHS and SHORTEST_PATH execution nodes in AQL query memory\n  usage accounting. The memory used by these execution node types was previously\n  not tracked against the configured query memory limit (BTS-411).\n\n* Lower log level to warning, when take over shard leadership finds an agency\n  Current entry is missing the server taking over.\n\n* Fixed BTS-408: treat positive or negative signed numbers as constants\n  immediately during AQL query parsing.\n  Previously, a value of `-1` was parsed initially as `unary minus(value(1))`,\n  which was not treated in the same way as a constant value `value(-1)`.\n  The value was later optimized to just `value(-1)`, but this only happened\n  during constant-folding after parsing. Any operations that referred to the\n  unfolded values during parsing thus did not treat such values as constants.\n\n* Fix startup issues with encryption-at-rest enabled when there were empty (0\n  byte) RocksDB WAL files present. Such empty files caused RocksDB to abort the\n  startup, reporting corruption. However, empty WAL files are possible in case\n  of server crashes etc. Now, if a WAL file is completely empty, there will be\n  no attempt to read the encryption meta data from it, so the startup succeeds\n\t(BTS-392).\n\n* Fixes a bug in the maintenance's error-handling code. A shard error would\n  result in log messages like\n    ```\n    WARNING [ceb1a] {maintenance} caught exception in Maintenance shards error\n    reporting: Expecting Object\n    ERROR [c9a75] {maintenance} Error reporting in current: Expecting Object\n    ```\n  and also prevent the maintenance from reporting the current state to the\n  agency, which in turn can prevent cluster-wide progress of various actions.\n\n* APM-107: Added metric \"rocksdb_read_only\" to determine whether RocksDB is\n  currently in read-only mode due to a background error. The metric will have a\n  value of \"1\" if RocksDB is in read-only mode and \"0\" if RocksDB is in normal\n  operations mode. If the metric value is \"1\" it means all writes into RocksDB\n  will fail, so inspecting the logfiles and acting on the actual error situation\n  is required.\n\n* Fix numeric overflow in AQL WINDOW node cost estimation if the number of\n  preceding rows was set to `unbounded`.\n\n* Added a retry loop for arangorestore during the initial connection phase. The\n  number of retries defaults to 3 and can be configured using\n  --initial-connect-retries (BTS-491).\n\n* Add following term ids, which prevents old synchronous replication requests\n  to be accepted after a follower was dropped and has gotten in sync again.\n  This makes the chaos tests which delay synchronous replication requests more\n  reliable and prevent inconsistent shard replicas under bad network conditions.\n\n* Enable process metrics on agent instances by default. Previously, some metrics\n  (including the metrics starting with `arangodb_process` prefix) were not\n  returned by agent instances.\n\n* Add prefix parameter to LEVENSHTEIN_MATCH function in ArangoSearch\n  (DEVSUPP-753).\n\n* Bug-fix: Pregel WCC algorithm could yield incorrect results if a part of the\n  connected component was only attached via OUTBOUND edges.\n  The underlying algorithm is now modified to properly retain INBOUND edges for\n  the runtime of the execution. This uses more RAM for the algorithm but\n  guarantees correctness.\n\n* Fix serialization of query shutdown error code when sending it to DB servers.\n  The error code is numeric, but it was sent to the `/_api/aql/finish` API as a\n  string. This led to the DB servers always assuming the default error code\n  TRI_ERROR_INTERAL (error code 4). This was not a problem for normal query\n  operations, but it could have led to warnings being logged stating \"please\n  contact ArangoDB support\". Now the actual error code is using during query\n  shutdown.\n\n* Fix display of running and slow queries in web UI when there are multiple\n  coordinators. Previously, the display order of queries was undefined, which\n  could lead to queries from one coordinator being display on top once and then\n  the queries from another. That made using this UI harder than necessary.\n\n  Now queries are sorted for display, according to their query IDs.\n\n* Fixed an issue in index selection, when the selectivty estimate of another\n  prefix index was used without checking if the other index covered the FILTER\n  condition.\n\n  For example, given the following indexes:\n\n  - index 1: [\"e\", \"a\", \"b\", \"c\"]\n  - index 2: [\"e\", \"a\", \"b\"]\n  - index 3: [\"d\", \"e\", \"f\", \"g\"]\n\n  and the FILTER condition `d == 1 && e == 2 && f == 3`, then the best index to\n  pick would be index 3. However, the optimizer may have picked index 1 here.\n  All indexes are valid candidates for this FILTER condition, but none of the\n  indexes covered all attributes of the FILTER condition. So the index\n  selectivity estimates were (correctly) not used directly to determine the best\n  index.\n  The actual bug happened when comparing the usefulness of the candidate\n  indexes, when figuring out that even though the selectivity estimate for index\n  1 could not be used, but that there existed a prefix index of index 1 (index\n  2). The selecivity estimate of this index was taken _without_ checking that\n  prefix index actually satisfied the FILTER condition fully.\n  The prefix index' selectivity estimate must only be used if it fully satisfies\n  the FILTER condition, which was not the case here.\n\n* Fixed DEVSUP-799: unique vertex getter may point to invalid memory after being\n  resetted, resulting in undefined behavior for traversals returning unique\n  vertices from inner FOR loops.\n\n* Improve usability of hidden options: `--help` mentions that these exist and\n  how to display them.\n\n* Fixed ES-863: reloading of users within the Cluster.\n  If a Coordinator is asked to reload its users (e.g. by the UserManager in\n  Foxx, it is also possible to do via API, but this is internal and on purpose\n  not documented, so unlikely that it is used), in concurrency with user\n  management updates there is a chance that the reload is not correctly\n  performed on this coordinator. It may have missed the last update locally,\n  causing one user to have an older state. It will be fixed on the next\n  modification of any other users/permissions. Unfortunately this bug can\n  cascade and when hit again, the coordinator can now be off by two updates.\n  In DC2DC this situation is more likely to happen on the target datacenter,\n  causing this datacenter to have other users/permissions than the source one.\n\n* Fix BTS-446: When finding a not yet fully initialized agency, do not\n  immediately fatal exit. Keep trying for (very generous) 5 minutes.\n\n* Only build actually used subattributes of traversal paths, i.e. \"vertices\",\n  \"edges\" or \"weights\". If any of the paths subcomponents is not used, the\n  optimizer will try to save these components from being built for each result\n  item.\n\n* Backport bugfix from upstream rocksdb repository for calculating the free disk\n  space for the database directory. Before the bugfix, rocksdb could\n  overestimate the amount of free space when the arangod process was run as\n  non-privileged users.\n\n* Fixed a problem with active failover, where a failover could take 5 mins\n  because the follower was caught in a bad state during replication. This fixes\n  BTS-425.\n\n* Add soft coordinator shutdown: This is a new option `soft=true` for the\n  DELETE /_admin/shutdown API. Has only meaning for coordinators, otherwise\n  ignored. A number of things are allowed to finish but no new things are\n  allowed when in soft coordinator shutdown:\n    - AQL cursors\n    - transactions\n    - asynchronous operations\n    - Pregel runs\n  Once all of the ongoing operations of these have finished and all requests on\n  the low priority queue have been executed, the coordinator shuts down the\n  normal way. This is supposed to make a coordinator restart less intrusive for\n  clients.\n\n* Fix BTS-398: Cannot force index hint for primary index if FILTER has multiple\n  OR conditions that require different indexes.\n\n* Bug-fix (macOs): in macOs there is an upper bound for descriptors defined by\n  the system, which is independend of the settings in `ulimit -n`. If the hard\n  limit is set above this upper bound value ArangoDB tries to raise the soft\n  limit to the hard limit on boot. This will fail due to the system limit. This\n  could cause ArangoDB to not start, asking you to lower the minimum of required\n  file descriptors. The system set upper bound is now honored and the soft limit\n  will be set to either hard limit or system limit whichever is lower.\n\n* Implemented APM-86: add query option `fillBlockCache` to control population of\n  RocksDB block cache with data read by the query. The default value for this\n  per-query option is `true`, which mimics the previous behavior.\n  Setting the option to off allows not storing data in RocksDB's block cache for\n  queries that are known to read only semi-relevant or unimportant data.\n\n\nv3.8.0 (2021-07-14)\n-------------------\n\n* Always remove blocker object for revision trees in case of replication\n  failures.\n\n* Fix invalid assertion for insert/removal buffers positioning and internals of\n  `hasBlockerUpTo` function.\n\n* Updated ArangoDB Starter to 0.15.0-1.\n\n* Updated arangosync to 2.4.0.\n\n* For cluster AQL queries, let the coordinator determine the query id to be used\n  on DB servers. This allows the coordinator to roll back AQL query setup\n  requests via the query id. Previously, the DB servers each generated a local\n  query id and returned it to the coordinator, who would then keep track of them\n  for later use. The problem with this was that if an AQL query setup request\n  timed out, the coordinator had no way to roll it back.\n\n  In addition, if setting up a query takes long on a DB server so that the\n  coordinator sends a rollback request, there are some measures in place for the\n  unlikely case in which the rollback request overtakes the setup request. In\n  this case, the rollback request will not find a query yet, but will register a\n  tombstone for it. Once the query gets registered by the delayed request, it\n  will (correctly) fail because of the tombstone.\n\n* Removed a special case for empty document update operations (i.e. update\n  requests in which no attributes were specified to be updated) were handled in\n  a special way without performing any writes. The problem was that such updates\n  did not update the local state, but could have been replicated to followers.\n  This special empty update case is now removed and update operations that do\n  not update any attributes are treated as normal write operations both locally\n  and in the replication.\n\n* Fix partial cleanup of internal write batches for multi-document operations of\n  which one or multiple failed. The previous implementation had an unreleased\n  performance optimization that wouldn't clean up the write batch completely.\n  That could have led to a wrong sequence of events being accumulated in the\n  write batch, which may have confused the WAL tailing API later. This bug was\n  only present in 3.8 RCs.\n\n* Fix some occurrences in which Merkle trees could silently apply the same\n  change multiple times, which led to data drift between the Merkle tree and the\n  underlying collection's data. This bug was only present in 3.8 but no earlier\n  versions.\n\n* On a failure during synchronous replication, do not remove the failed follower\n  from the list of known servers in the transaction.\n  If we do, we would not be able to send the commit/abort to the follower later.\n  However, we still need to send the commit/abort to the follower at transaction\n  end, because the follower may be responsible  for _other_ shards as well.\n\n  This change also removes dangling transactions that could stay around on\n  followers until they expired after the transaction idle timeout (180 seconds),\n  and that could prevent a follower from getting back in sync during this\n  period.\n\n* Added more context to \"dropping follower\" messages, so it is easier to analyze\n  what exactly went wrong.\n\n* Fixed invalid shard synchronization for documents not added via INSERT with\n  `overwriteMode` set to `ignore`. In this case, if a document with the given\n  key already exists, it is not changed on the leader (i.e. no write happens on\n  the leader). However, a write was replicated to the follower, which was wrong.\n  This write is now suppressed, which can only make such insert operations\n  faster.\n\n* Fix DEVSUP-753: now it is safe to call visit on exhausted disjunction\n  iterator.\n\n* Slightly improve specific warning messages for better readability.\n\n* Fix URL request parsing in case data is handed in in small chunks.\n  Previously the URL could be cut off if the chunk size was smaller than the URL\n  size.\n\n* Fix BTS-430: Added missing explain output about indexes for SHORTEST_PATH,\n  K_SHORTEST_PATHS and K_PATHS.\n\n* Added check to utils/generateAllMetricsDocumentation.py to check that the file\n  name and the value of the name attribute are the same in the metrics\n  documentation snippets. Correct a few such names.\n\nv3.8.0-rc.2 (2021-06-07)\n------------------------\n\n* Updated arangosync to 2.3.0.\n\n* Fix BTS-456, BTS-457: Make geo intersection between point and rectangle\n  symmetrical.\n\n* Fix BTS-450: RandomGenerator caught assertion during a value generation within\n  `dump_maskings` testsuite. Ensure correct conversion between 64 and 32bit.\n\n* Added check for data type compatibility between members of pipeline\n  ArangoSearch analyzer.\n\n\nv3.8.0-rc.1 (2021-05-26)\n------------------------\n\n* Fix BTS-442: a query with fullCount on a sharded collection hangs indefinitely\n  when LIMIT is less then number of available documents.\n\n* Removed unused documentation snippets (non-Rest DocuBlocks) as well as the\n  documentation about the long deprecated features Simple Queries and\n  JavaScript-based graph traversal. Also removed the descriptions of the JS API\n  methods `collection.range()`, `collection.closedRange()`,\n  `cursor.setBatchSize()` and `cursor.getBatchSize()`. All the functionality is\n  superseded by AQL.\n\n* Fixed ES-881: ensure that LDAP options for async, referrals and restart set\n  the off value correctly. Otherwise, this can result in an \"operations error\".\n\n* Improve Merkle tree memory usage and allow left-growth of trees, too. This can\n  help with insertions of arbitrarily old data.\n\n* Added metric `arangodb_sync_rebuilds_total` to track the full rebuild of a\n  shard follower after too many subsequent shard synchronization failures. This\n  metric should always have a value of 0. Everything else indicates a serious\n  problem.\n\n* Fixed BTS-422: SingleRemoteModification in AQL behaves different.\n\n  This disables the optimizer rule `optimize-cluster-single-document-operations`\n  for array inputs, e.g.\n\n      INSERT [...] INTO collection\n      REMOVE [...] IN collection\n\n  For the cases, the optimization is not pulled off, and the normal insert/\n  update/replace/remove behavior is executed, which will fail because of an\n  array being used as input.\n\n* Fix BTS-409: return error 1948 when a negative edge was detected during or was\n  used as default weight in a SHORTEST_PATH or a K_SHOTRTEST_PAHS traversal.\n\n* Fixed issue BTS-424: fix invalid input row handling in WINDOW execution.\n\n* Added 2 options to allow HTTP redirection customization for root (\"/\") call of\n  HTTP API:\n\n  `--http.permanently-redirect-root`: if true (default), use a permanent\n  redirection (use HTTP 301 code), if false fall back to temporary redirection\n  (use HTTP 302 code);\n  `--http.redirect-root-to`: redirect of root URL to a specified path (redirects\n  to \"/_admin/aardvark/index.html\" if not set (default)).\n\n* Fixed DEVSUP-764 (SEARCH-7): inconsistent BM25 scoring for LEVENSHTEIN_MATCH\n  function.\n\n* Rename two metrics with previously Prometheus-incompatible names:\n  - `arangodb_aql_global_query_memory_limit_reached` was renamed to\n    `arangodb_aql_global_query_memory_limit_reached_total`\n  - `arangodb_aql_local_query_memory_limit_reached` was renamed to\n    `arangodb_aql_local_query_memory_limit_reached_total`\n\n  These metrics were introduced in 3.8 so there is no migration for these\n  metrics.\n\n* Return error 1948 when a negative edge was detected during a weighted\n  traversal or was used as default weight.\n\n* Fixes BTS-417. In some cases an index did not consider both bounds (lower and\n  upper) for a close range scan if both bounds are expressed using the same\n  operator, e.g., `FILTER doc.beginDate >= lb AND ub >= doc.beginDate`.\n\n* Fix various issues related to the new WINDOW operation (see BTS-402)\n  - Improved explain output for ISO 8601 duration strings and fixed missing week\n    component.\n  - Improved validation of input data and error messages.\n  - Prevent FILTERs from being moved beyond a WINDOW.\n\n* Fixes BTS-416. During shutdown, a Shard leader wrongly reported that it could\n  not drop a shard follower instead of correctly indicating the shutdown as the\n  reason.\n\n* Fixes pregel lifetime management. Previously shutting down the server while a\n  pregel job was still running could result in a segfault or a shutdown hanger.\n\n* Improve error reporting for Merkle tree operations and improve memory usage\n  for unused trees by hibernating them. In addition, add some backoff to shard\n  synchronization in case there are repeated sync failures for the same shard.\n\n* Fixed BTS-403: Hot restores must also clear relevant `Current` keys. The\n  overriding of the `Plan` entries needs to be reflected in `Current` to avoid\n  conflicts in maintenance jobs.\n\n* Log a proper message if an unexpected state is encountered when taking over\n  shard leadership. In addition, make the change to the internal followerinfo\n  state atomic so that it cannot be semi-changed.\n\n* Fixed two bugs in fuerte with HTTP/2 and VST connections.\n  One could lead to ordered timeouts not being honoured. The other could lead to\n  an ordered callback be called multiple times.\n\n* Improve \"Shards\" view in web UI so that the shards of individual collections\n  can be expanded and collapsed without affecting the display of any other\n  shards. Also added a \"Toggle all\" button the web UI to expand/collapse the\n  shards for all collections.\n\n* Improve exception safety for maintenance thread and shard unlock operations.\n\n* Fixed issue #14122: when the optimizer rule \"inline-subqueries\" is applied, it\n  may rename some variables in the query. The variable renaming was however not\n  carried out for traversal PRUNE conditions, so the PRUNE conditions could\n  still refer to obsolete variables, which would make the query fail with errors\n  such as\n\n    Query: AQL: missing variable ... for node ... while planning registers\n\n* Improve performance of batch CRUD operations (insert, update, replace, remove)\n  if some of the documents in the batch run into write-write conflicts.\n  Rolling back partial operations in case of a failure is very expensive because\n  it requires rebuilding RocksDB write batches for the transaction from scratch.\n  Rebuilding write batches takes time proportional to the number of operations\n  in the batch, and for larger batches the cost can be prohibitive.\n  Now we are not rolling back write batches in some situations when this is not\n  required, so that in many cases running into a conflict does not have that\n  high overhead. There can still be issues when conflicts happen for index\n  entries, but a lot of previously problematic cases should now work better.\n\n  This change also reduces the RocksDB-internal lock timeout for writing to keys\n  locked by another transaction from 1s to 1ms. This will mean that operations\n  that ran into a write-write conflict may fail quicker than before, and not\n  wait and retry to acquire the locked key(s).\n\n* Fix response when isBuilding could not be removed from newly created\n  collection, when agency precondition fails. This can happen, when own rebootId\n  increment has triggered plan entry to be removed.\n\n* Fixed issue BTS-354: Assertion related to getCollection.\n\n* Fix DEVSUP-749: Fix potential deadlock when executing concurrent view/link DDL\n  operations and index DDL operations on the same collection.\n\n* When writing to starting shard leader respond with specific 503.\n  Fixes BTS-390.\n\n* Fixed a use after free bug in the connection pool.\n\n* Show peak memory usage in AQL query profiling output.\n\n* Fixed various issues (mainly data races) reported by ThreadSanitizer.\n\n* Fixed bug in error reporting when a database create did not work, which lead\n  to a busy loop reporting this error to the agency.\n\n* Fixed the error response if the HTTP version is not 1.0 or 1.1 and if the\n  Content-Length is too large (> 1 GB).\n\n* Guarded access only to ActionBase::_result.\n\n* Updated arangosync to 2.2.0.\n\n* Fixed proper return value in sendRequestRetry if server is shutting down.\n\n* Fixed internal issue #798: In  rare case when remove request completely cleans\n  just consolidated segment commit could be cancelled and documents removed from\n  collection may be left dangling in the ArangoSearch index.\n  Also fixes ES-810 and BTS-279.\n\n* Fixed a small problem in fuerte which could lead to an assertion failure.\n\n* Retry if an ex-leader can no longer drop a follower because it is no longer\n  leading.\n\n* Fixed issue BTS-373: ASan detected possible heap-buffer-overflow at\n  arangodb::transaction::V8Context::exitV8Context().\n\n* Fix a potential buffer overflow in RestReplicationHandler.\n\n* Make the time-to-live (TTL) value of a streaming cursor only count after the\n  response has been sent to the client.\n\n* Added option `--query-max-runtime` to arangoexport, in order to control\n  maximum query runtime.\n\n* Fix BTS-340: AQL expressions similar to `x < 3 || x` are no longer erroneously\n  be reduced to `x < 3` by the optimizer rule remove-redundant-or.\n\n* Change arangosh client behavior:\n  - *_RAW methods will never add a `body` to HEAD responses\n  - *_RAW methods will now always return velocypack-typed responses in Buffers\n  - `--server.force-json` will now be applied as default, overrideable\n    by user code\n\n\nv3.8.0-beta.1 (2021-04-20)\n--------------------------\n\n* Fix BTS-374: thread race between ArangoSearch link unloading and storage\n  engine WAL flushing.\n\n* Improve parallelism capabilities of arangorestore.\n\n  arangorestore can now dispatch restoring data chunks of a collection to idle\n  background threads, so that multiple restore requests can be in flight for the\n  same collection concurrently.\n\n  This can improve restore speed in situations when there are idle threads left\n  (number of threads can be configured via arangorestore's `--threads` option)\n  and the dump file for the collection is large.\n\n  The improved parallelism is only used when restoring dumps that are in the\n  non-enveloped format. This format has been introduced with ArangoDB 3.8.\n  The reason is that dumps in the non-enveloped format only contain the raw\n  documents, which can be restored independent of each other, i.e. in any order.\n  However, the enveloped format may contain documents and remove operations,\n  which need to be restored in the original order.\n\n* Fix crashes during arangorestore operations due to usage of wrong pointer\n  value for updating user permissions.\n\n* Fixed BTS-360 and ES-826: sporadic ArangoSearch error `Invalid RL encoding in\n  'dense_fixed_offset_column_key'`.\n\n* Add HTTP REST API endpoint POST `/_api/cursor/<cursor-id>` as a drop-in\n  replacement for PUT `/_api/cursor/<cursor-id>`. The POST API is functionally\n  equivalent to the existing PUT API. The benefit of using the POST API is that\n  HTTP POST requests will not be considered as idempotent, so proxies may not\n  retry them if they fail. This was the case with the existing PUT API, as HTTP\n  PUT requests can be considered idempotent according to the HTTP specification.\n\n  The POST API is not used internally by ArangoDB's own requests in this\n  version. This means that compatibility to older versions of ArangoDB that do\n  not provide the new API is ensured.\n\n* Timely updates of rebootId / cluster membership of DB servers and coordinators\n  in ClusterInfo. Fixes BTS-368 detected in chaos tests.\n\n* Fix cluster internal retry behavior for network communications. In particular\n  retry on 421 (leader refuses operation). This leads to the cluster letting\n  less internal errors out to clients.\n\n* Fixed CPPCHECK warning or added suppression.\n\n* Web UI - Added missing HTML escaping inside the file upload plugin used in the\n  section of deploying a new Foxx application when uploading a zip file.\n\n* Allow to specify a fail-over LDAP server. Instead of \"--ldap.OPTION\" you need\n  to specify \"--ldap2.OPTION\". Authentication / Authorization will first check\n  the primary LDAP server. If this server cannot authenticate a user, it will\n  try the secondary one. It is possible to specify a file containing all users\n  that the primary (or secondary) LDAP server is handling by specifying the\n  option \"--ldap.responsible-for\". This file must contain the usernames\n  line-by-line.\n\n* Fix BTS-352: removed assertion for success of a RocksDB function and throw a\n  proper exception instead.\n\n* Added option `--query.require-with` to make AQL in single server mode also\n  require `WITH` clauses where the cluster would need them.\n  The option is turned off by default, but can be turned on in single servers to\n  remove this behavior difference between single servers and clusters, making\n  later a transition from single server to cluster easier.\n\n* Fixed a problem in document batch operations, where errors from one shard were\n  reported multiple times, if the shard is completely off line.\n\n* Fixed issue #13169: arangoimport tsv conversion of bools and null, although\n  switched off by `--convert false`.\n\n  Importing unquoted `null`, `false` and `true` literals from delimited files\n  get imported as strings now if `convert` is explicitly turned off. It\n  previously affected unquoted numbers only.\n\n* Web UI: Highlight binary and hexadecimal integer literals in AQL queries.\n\n* Prevent arangod from terminating with \"terminate called without an active\n  exception\" (SIGABRT) in case an out-of-memory exception occurs during creating\n  an ASIO socket connection.\n\n* Micro improvements for Pregel job API and documentation:\n  - Added a few useful attributes to Pregel HTTP API docs.\n  - Added \"parallelism\" attribute to the result of Pregel job status responses,\n    so that the effective parallelism is reported back.\n  - Make sure \"computationTime\" in Pregel job status response does not underflow\n    in case of errors.\n\n* Fix BTS-350, BTS-358: Fixed potential startup errors due to global replication\n  applier being started before end of database recovery procedure.\n  Also fixed potential shutdown errors due to global replication applier being\n  shut down in parallel to a concurrent shut down attempt.\n\n* Fix BTS-357: Fix processing of analyzer with return type by TOKENS function.\n\n* Fix BTS-346: Improved handling of AQL query kill command in unlikely places,\n  before the query starts to execute and after the query is done but the result\n  is still being written. Now the cleanup of queries works more reliably. This\n  unreliable kill time windows were very short and unlikely to hit, although if\n  one was hit transactions were not aborted, and collection locks could be\n  lingering until query timeout.\n\n* Updated ArangoDB Starter to 0.15.0.\n\n* Fix BTS-357: Fix processing of analyzer with return type by TOKENS function.\n\n* Added error handling for figures command in cluster. Previously errors\n  returned by shards were ignored when aggregating the individual responses.\n\n* Remove CMake control variable `UNCONDITIONALLY_BUILD_LOG_MESSAGES`.\n\n* Fix undefined behavior in dynarray constructor when running into\n  an out-of-memory exception during construction. In arangod, this can only\n  happen during metrics objects construction at program start.\n\n* Added option `--headers-file` to arangoimport, to optionally read CSV/TSV\n  headers from a separate file.\n\n* Fixed issue BTS-353: memleak when running into an out-of-memory situation\n  while repurposing an existing AqlItemBlock.\n\n* Change metrics' internal `low()` and `high()` methods so that they  return by\n  value, not by reference.\n\n* Fix logging of urls when using `--log.level requests=debug`. There was an\n  issue since v3.7.7 with the wrong URL being logged in request logging if\n  multiple requests were sent over the same connection. In this case, the\n  request logging only reported the first URL requested in the connection, even\n  for all subsequent requests.\n\n* Added startup option `--query.allow-collections-in-expressions` to control\n  whether collection names can be used in arbitrary places in AQL expressions,\n  e.g. `collection + 1`. This was allowed before, as a collection can be seen as\n  an array of documents. However, referring to a collection like this in a query\n  would materialize all the collection's documents in RAM, making such\n  constructs prohibitively expensive for medium-size to large-size collections.\n\n  The option can now be set to `false` to prohibit accidental usage of\n  collection names in AQL expressions. With that setting, using a collection\n  inside an arbitrary expression will trigger the error `collection used as\n  expression operand` and make the query fail.\n  Even with the option being set to `false`, it is still possible to use\n  collection names in AQL queries where they are expected, e.g. `FOR doc IN\n  collection RETURN doc`.\n\n  The option `--query.allow-collections-in-expressions` is introduced with a\n  default value of `true` in 3.8 to ensure downwards-compatibility, but the\n  default value will change to `false` in 3.9. Furthermore, the option will be\n  deprecated in 3.9 and removed in later versions, in addition to making\n  unintended usage of collection names always an error.\n\n* Deprecate option `--rocksdb.exclusive-writes`, which was meant to serve only\n  as a stopgap measure while porting applications from the MMFiles storage\n  engine to RocksDB.\n\n* Fix errors caused by creating some log messages in log level DEBUG in log\n  topics PREGEL and GRAPHS. Setting the log level to DEBUG for these topics\n  could lead to errors when running some Pregel jobs or SmartGraph traversals.\n\n\nv3.8.0-alpha.1 (2021-03-29)\n---------------------------\n\n* Updated ArangoDB Starter to 0.15.0-preview-2.\n\n* Updated OpenSSL to 1.1.1k and OpenLDAP to 2.4.58.\n\n* Updated arangosync to 2.0.1.\n\n* Fix connectionTime statistic. This statistic should provide the distribution\n  of the connection lifetimes, but in previous versions the tracking was broken\n  and no values were reported.\n\n* When using connections to multiple endpoints and switching between them,\n  arangosh can now reuse existing connections by referring to an internal\n  connection cache. This helps for arangosh scripts that repeatedly connect to\n  multiple endpoints, and avoids wasting lots of ephemeral TCP ports remaining\n  in CLOSE_WAIT state.\n  This change is transparent to any arangosh scripts or commands that do not\n  reconnect to other endpoints than the one specified at arangosh start.\n\n* Add an option for locking down all endpoints in the `/_admin/cluster` REST API\n  for callers without a proper JWT set in the request. There is a new startup\n  option `--cluster.api-jwt-policy` that allows *additional* checks for a valid\n  JWT in requests to sub-routes of `/_admin/cluster`. The possible values for\n  the startup option are:\n\n  - \"jwt-all\": requires a valid JWT for all accesses to `/_admin/cluster` and\n    its sub-routes. If this configuration is used, the \"Cluster\" and \"Nodes\"\n    sections of the web interface will be disabled, as they are relying on the\n    ability to read data from several cluster APIs.\n  - \"jwt-write\": requires a valid JWT for write accesses (all HTTP methods\n    except HTTP GET) to `/_admin/cluster`. This setting can be used to allow\n    privileged users to read data from the cluster APIs, but not to do any\n    modifications. All existing permissions checks for the cluster API routes\n    are still in effect with this setting, meaning that read operations without\n    a valid JWT may still require dedicated other permissions (as in 3.7).\n  - \"jwt-compat\": no *additional* access checks are in place for the cluster\n    APIs. However, all existing permissions checks for the cluster API routes\n    are still in effect with this setting, meaning that all operations may still\n    require dedicated other permissions (as in 3.7).\n\n  The default value for the option is `jwt-compat`, which means this option will\n  not cause any extra JWT checks compared to 3.7.\n\n* UI builds are now using the yarn package manager instead of the previously\n  used node package manager.\n\n* Fix shortName labels in metrics, in particular for agents.\n\n* The old metrics api contains the following gauges which should actually be\n  counters:\n    - arangodb_scheduler_jobs_dequeued\n    - arangodb_scheduler_jobs_submitted\n    - arangodb_scheduler_jobs_done\n  Therefore the new v2 metric api adds the following counters:\n    - arangodb_scheduler_jobs_dequeued_total\n    - arangodb_scheduler_jobs_submitted_total\n    - arangodb_scheduler_jobs_done_total\n  These counters are only visible in the new v2 metric and replace the old\n  metrics which are suppressed for v2.\n\n* Fix implicit capture of views in a context of JS transaction.\n\n* Introduce metrics for AQL query memory limit violations:\n  - `arangodb_aql_global_query_memory_limit_reached`: Total number of times the\n    global query memory limit was violated.\n  - `arangodb_aql_local_query_memory_limit_reached`: Total number of times a\n    local query memory limit was violated.\n\n* Set the default value for `--query.global-memory-limit` to around 90% of RAM,\n  so that a global memory limit is now effective by default.\n\n  The default global memory limit value is calculated by a formula depending on\n  the amount of available RAM and will result in the following values for\n  common RAM sizes:\n\n  RAM:            0      (0MiB)  Limit:            0   unlimited, %mem:  n/a\n  RAM:    134217728    (128MiB)  Limit:     33554432     (32MiB), %mem: 25.0\n  RAM:    268435456    (256MiB)  Limit:     67108864     (64MiB), %mem: 25.0\n  RAM:    536870912    (512MiB)  Limit:    255013683    (243MiB), %mem: 47.5\n  RAM:    805306368    (768MiB)  Limit:    510027366    (486MiB), %mem: 63.3\n  RAM:   1073741824   (1024MiB)  Limit:    765041049    (729MiB), %mem: 71.2\n  RAM:   2147483648   (2048MiB)  Limit:   1785095782   (1702MiB), %mem: 83.1\n  RAM:   4294967296   (4096MiB)  Limit:   3825205248   (3648MiB), %mem: 89.0\n  RAM:   8589934592   (8192MiB)  Limit:   7752415969   (7393MiB), %mem: 90.2\n  RAM:  17179869184  (16384MiB)  Limit:  15504831938  (14786MiB), %mem: 90.2\n  RAM:  25769803776  (24576MiB)  Limit:  23257247908  (22179MiB), %mem: 90.2\n  RAM:  34359738368  (32768MiB)  Limit:  31009663877  (29573MiB), %mem: 90.2\n  RAM:  42949672960  (40960MiB)  Limit:  38762079846  (36966MiB), %mem: 90.2\n  RAM:  68719476736  (65536MiB)  Limit:  62019327755  (59146MiB), %mem: 90.2\n  RAM: 103079215104  (98304MiB)  Limit:  93028991631  (88719MiB), %mem: 90.2\n  RAM: 137438953472 (131072MiB)  Limit: 124038655509 (118292MiB), %mem: 90.2\n  RAM: 274877906944 (262144MiB)  Limit: 248077311017 (236584MiB), %mem: 90.2\n  RAM: 549755813888 (524288MiB)  Limit: 496154622034 (473169MiB), %mem: 90.2\n\n* Increase default idle timeout in streaming transactions from 10 seconds to\n  60 seconds, and make the timeout configurable via a startup parameter\n  `--transaction.streaming-idle-timeout`.\n\n* Use RebootTracker to abort cluster transactions on DB servers should the\n  originating coordinator die or be rebooted. The previous implementation left\n  the coordinator's transactions open on DB servers until they timed out there.\n  Now, the coordinator's unavailability or reboot will be detected as early as\n  it is reported by the agency, and all open transactions from that coordinator\n  will be auto-aborted on DB servers.\n\n* Update the Web UI's list of built-in AQL functions for proper syntax\n  highlighting in the query editor.\n\n* Fix a crash caused by returning a result produced by ANALYZER function.\n\n* Fix a race in LogAppender::haveAppenders.\n  `haveAppenders` is called as part of audit logging. It accesses internal maps\n  but previously did not hold a lock while doing so.\n\n* Bug-fix in the case of very rare network issues there was a chance that an AQL\n  query could get stuck during a cleanup and after a commit.\n  This would cause the client to receive a timeout, and the Coordinator blocking\n  a Scheduler thread. This situation is sorted out and the thread will not be\n  blocked anymore. We also added logs in case the query could not successfully\n  be cleaned up, which would leave locks on shards behind.\n\n* Fix an assertion failure that occurred when restoring view definitions from a\n  cluster into a single server.\n\n* Added new ArangoSearch analyzer type \"stopwords\".\n\n* Fix error message in case of index unique constraint violations. They were\n  lacking the actual error message (i.e. \"unique constraint violated\") and only\n  showed the index details. The issue was introduced only in devel in Feb.\n\n* Removed old metrics in new v2 metric api. Those metric endpoints were\n  identical to the sum value of histograms.\n\n* Allow process-specific logfile names.\n\n  This change allows replacing '$PID' with the current process id in the\n  `--log.output` and `--audit.output` startup parameters.\n  This way it is easier to write process-specific logfiles.\n\n* Backport a bugfix from upstream RocksDB for opening encrypted files with small\n  sizes. Without the bugfix, the server may run into assertion failures during\n  recovery.\n\n* Fix duplicate leaving of V8 contexts when returning streaming cursors.\n  The `exitContext` call done on query shutdown could previously try to exit the\n  V8 context multiple times, which would cause undefined behavior. Now we are\n  tracking if we already left the context to prevent duplicate invocation.\n\n* In a cluster, do not create the collections `_statistics`, `_statistics15` and\n  `statisticsRaw` on DB servers. These collections should only be created by the\n  coordinator, and should translate into 2 shards each on DB servers. But there\n  shouldn't be shards named `_statistics*` on DB servers.\n\n* Fixed two bogus messages about hotbackup restore:\n  - Coordinators unconditionally logged the message \"Got a hotbackup restore\n    event, getting new cluster-wide unique IDs...\" on shutdown. This was not\n    necessarily related to a hotbackup restore.\n  - DB servers unconditionally logged the message \"Strange, we could not\n    unregister the hotbackup restore callback.\" on shutdown, although this was\n    meaningless.\n\n* Rename \"save\" return attribute to \"dst\" in AQL functions `DATE_UTCTOLOCAL` and\n  `DATE_LOCALTOUTC`.\n\n* Fix potentially undefined behavior when creating a\n  CalculationTransactionContext for an arangosearch analyzer. An uninitialized\n  struct member was passed as an argument to its base class. This potentially\n  had no observable effects, but should be fixed.\n\n* Retry a cluster internal network request if the connection comes from the pool\n  and turns out to be stale (connection immediately closed). This fixes some\n  spurious errors after a hotbackup restore.\n\n* Fix progress reporting for arangoimport with large files on Windows.\n  Previously, progress was only reported for the first 2GB of data due to an int\n  overflow.\n\n* Log the actual signal instead of \"control-c\" and also include the process id\n  of the process that sent the signal.\n\n* Fixed GitHub issue #13665: Improve index selection when there are multiple\n  candidate indexes.\n\n* When dropping a collection or an index with a larger amount of documents, the\n  key range for the collection/index in RocksDB gets compacted. Previously, the\n  compaction was running in foreground and thus would block the deletion\n  operations.\n  Now, the compaction is running in background, so that the deletion operations\n  can return earlier.\n  The maximum number of compaction jobs that are executed in background can be\n  configured using the new startup parameter\n  `--rocksdb.max-parallel-compactions`, which defaults to 2.\n\n* Put Sync/LatestID into hotbackup and restore it on hotbackup restore if it is\n  in the backup. This helps with unique key generation after a hotbackup is\n  restored to a young cluster.\n\n* Fixed a bug in the index count optimization that doubled counted documents\n  when using array expansions in the fields definition.\n\n* Don't store selectivity estimate values for newly created system collections.\n\n  Not storing the estimates has a benefit especially for the `_statistics`\n  system collections, which are written to periodically even on otherwise idle\n  servers. In this particular case, the actual statistics data was way smaller\n  than the writes caused by the index estimate values, causing a disproportional\n  overhead just for maintaining the selectivity estimates.\n  The change now turns off the selectivity estimates for indexes in all newly\n  created system collections, and for new user-defined indexes of type\n  \"persistent\", \"hash\" or \"skiplist\", there is now an attribute \"estimates\"\n  which can be set to `false` to disable the selectivity estimates for the\n  index.\n  The attribute is optional. Not setting it will lead to the index being created\n  with selectivity estimates, so this is a downwards-compatible change for\n  user-defined indexes.\n\n* Added startup option `--query.global-memory-limit` to set a limit on the\n  combined estimated memory usage of all AQL queries (in bytes).\n  If this option has a value of `0`, then no memory limit is in place.\n  This is also the default value and the same behavior as in previous versions\n  of ArangoDB.\n  Setting the option to a value greater than zero will mean that the total\n  memory usage of all AQL queries will be limited approximately to the\n  configured value.\n  The limit is enforced by each server in a cluster independently, i.e. it can\n  be set separately for coordinators, DB servers etc. The memory usage of a\n  query that runs on multiple servers in parallel is not summed up, but tracked\n  separately on each server.\n  If a memory allocation in a query would lead to the violation of the\n  configured global memory limit, then the query is aborted with error code 32\n  (\"resource limit exceeded\").\n  The global memory limit is approximate, in the same fashion as the per-query\n  limit provided by the option `--query.memory-limit` is. Some operations,\n  namely calls to AQL functions and their intermediate results, are currently\n  not properly tracked.\n  If both `--query.global-memory-limit` and `--query.memory-limit` are set, the\n  former must be set at least as high as the latter.\n\n  To reduce the cost of globally tracking the memory usage of AQL queries, the\n  global memory usage counter is only updated in steps of 32 kb, making this\n  also the minimum granularity of the global memory usage figure.\n  In the same fashion, the granularity of the peak memory usage counter inside\n  each query was also adjusted to steps of 32 kb.\n\n* Added startup option `--query.memory-limit-override` to control whether\n  individual AQL queries can increase their memory limit via the `memoryLimit`\n  query option. This is the default, so a query that increases its memory limit\n  is allowed to use more memory.\n  The new option `--query.memory-limit-override` allows turning this behavior\n  off, so that individual queries can only lower their maximum allowed memory\n  usage.\n\n* Added metric `arangodb_aql_global_memory_usage` to expose the total amount of\n  memory (in steps of 32 kb) that is currently in use by all AQL queries.\n\n* Added metric `arangodb_aql_global_memory_limit` to expose the memory limit\n  from startup option `--query.global-memory-limit`.\n\n* Allow setting path to the timezone information via the `TZ_DATA` environment\n  variable, in the same fashion as the currently existing `ICU_DATA` environment\n  variable. The `TZ_DATA` variable is useful in environments` that start arangod\n  from some unusual locations, when it can't find its `tzdata` directory\n  automatically.\n\n* Fixed a bug in query cost estimation when a NoResults node occurred in a\n  spliced subquery. This could lead to a server crash.\n\n* Fix slower-than-necessary arangoimport behavior:\n  arangoimport has a built-in rate limiter, which can be useful for importing\n  data with a somewhat constant rate. However, it is enabled by default and\n  limits imports to 1MB per second. These settings are not useful.\n\n  This change turns the rate limiting off by default, and sets the default chunk\n  size to 8MB (up from 1MB) as well. This means that arangoimport will send\n  larger batches to the server by default. The already existing `--batch-size`\n  option can be used to control the maximum size of each batch.\n\n  The new parameter `--auto-rate-limit` can now be used to toggle rate limiting.\n  It defaults to off, whereas previously rate limiting was enabled by default\n  unless `--batch-size` was specified when arangoimport was invoked.\n\n* The cluster dashboard charts in the web UI are now more readable during the\n  initialization phase. Additionally, the amount of agents are now displayed\n  there as well. An agent failure will also appear here in case it exists.\n\n* Added more useful information during the SmartGraph creation in the web UI\n  in case the current database is a OneShard database.\n\n* Add support for building with Zen 3 CPU when optimizing for the local\n  architecture.\n\n* The web UI's node overview now displays also agent information (cluster only).\n\n* The statistics view in the web UI does now provide more system specific\n  information in case the Metrics API is enabled. Different statistics may be\n  visible depending on the operating system.\n\n* Added metrics documentation snippets and infrastructure for that.\n\n* Added a new cluster distribution view to the web UI. The view includes general\n  details about cluster-wide distribution in general as well as more detailed\n  shard distribution specific information.\n\n* Reasonably harden MoveShard against invalid VelocyPack input.\n\n* Removed older reference to VelocyPackDumper.\n\n* Added `--documents-per-batch` option to arangoexport.\n  This option allows to control the number of documents to be returned by each\n  server-side batch. It can be used to limit the number of documents per batch\n  when exporting collections with large documents.\n\n* Added a new metrics view to the web UI. This view can be used in a clustered\n  environment as well as in a single instance. Metrics are displayed either in\n  a tabular format or as plain text (Prometheus Text-based format).\n  Additionally, the metrics can be downloaded there.\n\n* Added a new maintenance mode tab to the web UI in cluster mode.\n  The new tab shows the current state of the cluster supervision maintenance and\n  allows to enable/disable the maintenance mode from there. The tab will only be\n  visible in the `_system` database. The required privileges for displaying the\n  maintenance mode status and/or changing it are the as for using the REST APIs\n  for the maintenance mode.\n\n* Added ability to display Coordinator and DBServer logs from inside the Web UI\n  in a clustered environment when privileges are sufficient.\n  Additionally, displayed log entries can now be downloaded from the web UI in\n  single server and in cluster mode.\n\n* The Web UI's info view of a collection now displays additional properties and\n  statistics (e.g. RocksDB related figures, sharding information and more).\n\n* Improve progress reporting for shard synchronization in the web UI.\n  The UI will now show how many shards are actively syncing data, and will\n  provide a better progress indicator, especially if there is more than one\n  follower for a shard.\n\n* Added `--shard` option to arangodump, so that dumps can be restricted to one\n  or multiple shards only.\n\n* Add optional hostname logging to log messages.\n  Whether or not the hostname is added to each log message can be controlled via\n  the new startup option `--log.hostname`. Its default value is the empty\n  string, meaning no hostname will be added to log messages.\n  Setting the option to an arbitrary string value will make this string be\n  logged in front of each regular log message, and inside the `hostname`\n  attribute in case of JSON-based logging. Setting the option to a value of\n  `auto` will use the hostname as returned by `gethostbyname`.\n\n* Added list-repeat AIR primitive that creates a list containing n copies of the\n  input value.\n\n* Prevent arangosh from trying to connect after every executed command.\n  This fixes the case when arangosh is started with default options, but no\n  server is running on localhost:8529. In this particular case, arangosh will\n  try to connect on startup and after every executed shell command. The connect\n  attempts all fail and time out after 300ms.\n  In this case we now don't try to reconnect after every command.\n\n* Added 'custom-query' testcase to arangobench to allow execution of custom\n  queries.\n  This also adds the options `--custom-query` and `--custom-query-file` for\n  arangobench.\n\n* Addition to the internal Refactoring of K_PATHS feature: K_PATHS queries are\n  now being executed on the new refactored graph engine in a clustered\n  environment. This change should not have any visible effect on users.\n\n* Reduce memory footprint of agency Store in Node class.\n\n* On Windows create a minidump in case of an unhandled SEH exception for\n  post-mortem debugging.\n\n* Add JWT secret support for arangodump and arangorestore, i.e. they now also\n  provide the command-line options `--server.ask-jwt-secret` and\n  `--server.jwt-secret-keyfile` with the same meanings as in arangosh.\n\n* Add optional hyperlink to program option sections for information purposes,\n  and add optional sub-headlines to program options for better grouping.\n  These changes will be visible only when using `--help`.\n\n* For Windows builds, remove the defines\n  `_SILENCE_ALL_CXX17_DEPRECATION_WARNINGS` and `_ENABLE_ATOMIC_ALIGNMENT_FIX`\n  that were needed to build Boost components with MSVC in older versions of\n  Boost and MSVC.\n  Both of these defines are obsolete nowadays.\n\n* Database initial sync considers document count on leader for estimating\n  timeouts when over 1 million docs on leader.\n\n* Fixed issue #13117: Aardvark: Weird cursor offsets in query editor.\n\n  Disabled font ligatures for Ace editor in Web UI to avoid rare display issue.\n\n* Make all AQL cursors return compact result arrays.\n\n  As a side-effect of this change, this makes profiling (i.e. using\n  `db._profileQuery(...)` work for streaming queries as well. Previously,\n  profiling a streaming query could have led to some internal errors, and even\n  query results being returned, even though profiling a query should not return\n  any query results.\n\n* Try to raise file descriptors limit in local start scripts (in `scripts/`\n  directory - used for development only).\n\n* Fixed replication bug in MerkleTree sync protocol, which could lead to data\n  corruption. The visible effect was that shards could no longer get in sync\n  since the counts would not match after sync, even after a recount.\n  This corruption only happened if there were large amounts of differences (at\n  least 65537) and the destination side had newer revisions for some keys than\n  the source side.\n\n* Simplify the DistributeExecutor and avoid implicit modification of its input\n  variable. Previously the DistributeExecutor could update the input variable\n  in-place, leading to unexpected results (see #13509).\n  The modification logic has now been moved into three new _internal_ AQL\n  functions (MAKE_DISTRIBUTE_INPUT, MAKE_DISTRIBUTE_INPUT_WITH_KEY_CREATION, and\n  MAKE_DISTRIBUTE_GRAPH_INPUT) and an additional calculation node with an\n  according function call will be introduced if we need to prepare the input\n  data for the distribute node.\n\n* Added new REST APIs for retrieving the sharding distribution:\n\n  - GET `/_api/database/shardDistribution` will return the number of\n    collections, shards, leaders and followers for the database it is run\n    inside. The request can optionally be restricted to include data from only a\n    single DB server, by passing the `DBserver` URL parameter.\n\n    This API can only be used on coordinators.\n\n  - GET `/_admin/cluster/shardDistribution` will return global statistics on the\n    current shard distribution, showing the total number of databases,\n    collections, shards, leaders and followers for the entire cluster.\n    The results can optionally be restricted to include data from only a single\n    DB server, by passing the `DBserver` URL parameter.\n    By setting the `details` URL parameter, the response will not contain\n    aggregates, but instead one entry per available database will be returned.\n\n    This API can only be used in the `_system` database of coordinators, and\n    requires admin user privileges.\n\n* Decrease the size of serialized index estimates, by introducing a compressed\n  serialization format. The compressed format uses the previous uncompressed\n  format internally, compresses it, and stores the compressed data instead. This\n  makes serialized index estimates a lot smaller, which in turn decreases the\n  size of I/O operations for index maintenance.\n\n* More improvements for logging:\n\n  - Added new REST API endpoint GET `/_admin/log/entries` to return log entries\n    in a more intuitive format, putting each log entry with all its properties\n    into an object. The API response is an array with all log message objects\n    that match the search criteria.\n    This is an extension to the already existing API endpoint GET `/_admin/log`,\n    which returned log messages fragmented into 5 separate arrays.\n\n    The already existing API endpoint GET `/_admin/log` for retrieving log\n    messages is now deprecated, although it will stay available for some time.\n\n  - Truncation of log messages now takes JSON format into account, so that the\n    truncation of oversized JSON log messages still keeps a valid JSON structure\n    even after the truncation.\n\n  - The maximum size of in-memory log messages was doubled from 256 to 512\n    chars, so that longer parts of each log message can be preserved now.\n\n* Fix `/_admin/cluster/removeServer` API.\n  This often returned HTTP 500 with an error message \"Need open Array\" due to an\n  internal error when setting up agency preconditions.\n\n* Remove logging startup options `--log.api-enabled` and `--log.keep-logrotate`\n  for all client tools (arangosh, arangodump, arangorestore etc.), as these\n  options are only meaningful for arangod.\n\n* Extend the \"move-calculations-up\" optimizer rule so that it can move\n  calculations out of subqueries into the outer query.\n\n* Don't allocate ahead-of-time memory for striped PRNG array in arangod, but\n  instead use thread-local PRNG instances. Not only does this save a few\n  megabytes of memory, but it also avoids potential (but unlikely) sharing of\n  the same PRNG instance by multiple threads.\n\n* Remove undocumented CMake variable `USE_BACKTRACE`, and remove define\n  `ARANGODB_ENABLE_BACKTRACE`. Both were turned off by default before, and when\n  turned on allow to produce backtraces from within the executable in case debug\n  symbols were available, working and the build was also compiled with\n  `USE_MAINTAINER_MODE=On`. Some code in this context was obviously unreachable,\n  so now it has all been removed.\n  To log a backtrace from within arangod, it is now possible to call\n  `CrashHandler::logBacktrace()`, which will log a backtrace of the calling\n  thread to the arangod log. This is restricted to Linux builds only.\n\n* Fix warnings about suggest-override which can break builds when warnings aret\n  reated as errors.\n\n* Turn off option `--server.export-read-write-metrics` for now, until there is\n  certainty about the runtime overhead it introduces.\n\n* Remove unsafe query option `inspectSimplePlans`. This option previously\n  defaulted to `true`, and turning it off could make particular queries fail.\n  The option was ignored in the cluster previously, and turning it off only had\n  an effect in single server, there making very simple queries (queries not\n  containing any FOR loops) not going through the optimizer's complete pipeline\n  as a performance optimization. However, the optimization was only possible for\n  a very small number of queries and even had adverse effects, so it is now\n  removed entirely.\n\n* On Linux and MacOS, require at least 8192 usable file descriptors at startup.\n  If less file descriptors are available to the arangod process, then the\n  startup is automatically aborted.\n\n  Even the chosen minimum value of 8192 will often not be high enough to store\n  considerable amounts of data. However, no higher value was chosen in order to\n  not make too many existing small installations fail at startup after\n  upgrading.\n\n  The required number of file descriptors can be configured using the startup\n  option `--server.descriptors-minimum`. It defaults to 8192, but it can be\n  increased to ensure that arangod can make use of a sufficiently high number of\n  files. Setting `--server.descriptors-minimum` to a value of `0` will make the\n  startup require only an absolute minimum limit of 1024 file descriptors,\n  effectively disabling the change.\n  Such low values should only be used to bypass the file descriptors check in\n  case of an emergency, but this is not recommended for production.\n\n* Added metric `arangodb_transactions_expired` to track the total number of\n  expired and then garbage-collected transactions.\n\n* Allow toggling the document read/write counters and histograms via the new\n  startup option `--server.export-read-write-metrics false`. This option\n  defaults to `true`, so these metrics will be exposed by default.\n\n* Upgraded bundled version of libunwind to v1.5.\n\n* Added startup option `--javascript.tasks` to allow turning off JavaScript\n  tasks if not needed. The default value for this option is `true`, meaning\n  JavaScript tasks are available as before.\n  However, with this option they can be turned off by admins to limit the amount\n  of JavaScript user code that is executed.\n\n* Only instantiate a striped PRNG instance for the arangod server, but not for\n  any of the client tools (e.g. arangosh, arangodump, arangorestore).\n  The client tools do not use the striped PRNG, so we can save a few MBs of\n  memory for allocating the striped PRNG instance there, plus some CPU time for\n  initializing it.\n\n* Improve shard synchronization protocol by only transferring the required parts\n  of the inventory from leader to follower. Previously, for each shard the\n  entire inventory was exchanged, which included all shards of the respective\n  database with all their details.\n  In addition, save 3 cluster-internal requests per shard in the initial shard\n  synchronization protocol by reusing already existing information in the\n  different steps of the replication process.\n\n* Added metric `arangodb_scheduler_low_prio_queue_last_dequeue_time` that\n  provides the time (in milliseconds) it took for the most recent low priority\n  scheduler queue item to bubble up to the queue's head. This metric can be used\n  to estimate the queuing time for incoming requests.\n  The metric will be updated probabilistically when a request is pulled from the\n  scheduler queue, and may remain at its previous value for a while if only few\n  requests are coming in or remain permanently at its previous value if no\n  further requests are incoming at all.\n\n* Allow {USER} placeholder string also in `--ldap.search-filter`.\n\n* Fixed some wrong behavior in single document updates. If the option\n  ignoreRevs=false was given and the precondition _rev was given in the body but\n  the _key was given in the URL path, then the rev was wrongly taken as 0,\n  rather than using the one from the document body.\n\n* Improved logging for error 1489 (\"a shard leader refuses to perform a\n  replication operation\"). The log message will now provide the database and\n  shard name plus the differing information about the shard leader.\n\n* Add shard-parallelism to arangodump when dumping collections with multiple\n  shards.\n  Previously, arangodump could execute a dump concurrently on different\n  collections, but it did not parallelize the dump for multiple shards of the\n  same collection.\n  This change should speed up dumping of collections with multiple shards.\n  When dumping multiple shards of the same collection concurrently, parallelism\n  is still limited by all these threads needing to serialize their chunks into\n  the same (shared) output file.\n\n* Add option `--envelope` for arangodump, to control if each dumped document\n  should be wrapped into a small JSON envelope (e.g.\n  `{\"type\":2300,\"data\":{...}}`). This JSON envelope is not necessary anymore\n  since ArangoDB 3.8, so omitting it can produce smaller (and slightly faster)\n  dumps.\n  Restoring a dump without these JSON envelopers is handled automatically by\n  ArangoDB 3.8 and higher. Restoring a dump without these JSON envelopes into\n  previous versions (pre 3.8) however is not supported. Thus the option should\n  only be used if the client tools (arangodump, arangorestore) and the arangod\n  server are all using v3.8 or higher, and the dumps will never be stored into\n  earlier versions.\n  The default value for this option is `true`, meaning the JSON wrappers will be\n  stored as part of the dump. This is compatible with all previous versions.\n\n* Make AQL optimizer rule \"splice-subqueries\" mandatory, in the sense that it\n  cannot be disabled anymore. As a side effect of this change, there will no\n  query execution plans created by 3.8 that contain execution nodes of type\n  `SubqueryNode`. `SubqueryNode`s will only be used during query planning and\n  optimization, but at the end of the query optimization phase will all have\n  been replaced with nodes of types `SubqueryStartNode` and `SubqueryEndNode`.\n  The code to execute non-spliced subqueries remains in place so that 3.8 can\n  still execute queries planned on a 3.7 instance with the \"splice-subqueries\"\n  optimizer rule intentionally turned off. The code for executing non-spliced\n  subqueries can be removed in 3.9.\n\n* AQL query execution plan register usage optimization.\n\n  This is a performance optimization that may positively affect some AQL queries\n  that use a lot of variables that are only needed in certain parts of the\n  query.\n  The positive effect will come from saving registers, which directly translates\n  to saving columns in AqlItemBlocks.\n\n  Previously, the number of registers that were planned for each depth level of\n  the query never decreased when going from one level to the next. Even though\n  unused registers were recycled since 3.7, this did not lead to unused\n  registers being completely dismantled.\n\n  Now there is an extra step at the end of the register planning that keeps\n  track of the actually used registers on each depth, and that will shrink the\n  number of registers for the depth to the id of the maximum register. This is\n  done for each depth separately.\n  Unneeded registers on the right hand side of the maximum used register are now\n  discarded. Unused registers on the left hand side of the maximum used register\n  id are not discarded, because we still need to guarantee that registers from\n  depths above stay in the same slot when starting a new depth.\n\n* Added metric `arangodb_aql_current_query` to track the number of currently\n  executing AQL queries.\n\n* Internal refactoring of K_PATH feature, with the goal to have all graph\n  algorithms on the same framework. This change should not have any visible\n  effect on users.\n\n* Removed server-side JavaScript object `ArangoClusterComm`, so it cannot be\n  used from inside JavaScript operations or Foxx.\n  The `ArangoClusterComm` object was previously used inside a few internal\n  JavaScript operations, but was not part of the public APIs.\n\n* Restrict access to functions inside JavaScript objects `ArangoAgency` and\n  `ArangoAgent` to JavaScript code that is running in privileged mode, i.e. via\n  the server's emergency console, the `/_admin/execute` API (if turned on) or\n  internal bootstrap scripts.\n\n* Added startup option `--javascript.transactions` to allow turning off\n  JavaScript transactions if not needed. The default value for this option is\n  `true`, meaning JavaScript transactions are available as before.\n  However, with this option they can be turned off by admins to limit the amount\n  of JavaScript user code that is executed.\n\n* Introduce a default memory limit for AQL queries, to prevent rogue queries\n  from consuming the entire memory available to an arangod instance.\n\n  The limit is introduced via changing the default value of the option\n  `--query.memory-limit` from previously `0` (meaning: no limit) to a\n  dynamically calculated value.\n  The per-query memory limits defaults are now:\n\n    Available memory:            0      (0MiB)  Limit:            0   unlimited, %mem:  n/a\n    Available memory:    134217728    (128MiB)  Limit:     33554432     (32MiB), %mem: 25.0\n    Available memory:    268435456    (256MiB)  Limit:     67108864     (64MiB), %mem: 25.0\n    Available memory:    536870912    (512MiB)  Limit:    201326592    (192MiB), %mem: 37.5\n    Available memory:    805306368    (768MiB)  Limit:    402653184    (384MiB), %mem: 50.0\n    Available memory:   1073741824   (1024MiB)  Limit:    603979776    (576MiB), %mem: 56.2\n    Available memory:   2147483648   (2048MiB)  Limit:   1288490189   (1228MiB), %mem: 60.0\n    Available memory:   4294967296   (4096MiB)  Limit:   2576980377   (2457MiB), %mem: 60.0\n    Available memory:   8589934592   (8192MiB)  Limit:   5153960755   (4915MiB), %mem: 60.0\n    Available memory:  17179869184  (16384MiB)  Limit:  10307921511   (9830MiB), %mem: 60.0\n    Available memory:  25769803776  (24576MiB)  Limit:  15461882265  (14745MiB), %mem: 60.0\n    Available memory:  34359738368  (32768MiB)  Limit:  20615843021  (19660MiB), %mem: 60.0\n    Available memory:  42949672960  (40960MiB)  Limit:  25769803776  (24576MiB), %mem: 60.0\n    Available memory:  68719476736  (65536MiB)  Limit:  41231686041  (39321MiB), %mem: 60.0\n    Available memory: 103079215104  (98304MiB)  Limit:  61847529063  (58982MiB), %mem: 60.0\n    Available memory: 137438953472 (131072MiB)  Limit:  82463372083  (78643MiB), %mem: 60.0\n    Available memory: 274877906944 (262144MiB)  Limit: 164926744167 (157286MiB), %mem: 60.0\n    Available memory: 549755813888 (524288MiB)  Limit: 329853488333 (314572MiB), %mem: 60.0\n\n  As previously, a memory limit value of `0` means no limitation.\n  The limit values are per AQL query, so they may still be too high in case\n  queries run in parallel. The defaults are intentionally high in order to not\n  stop any valid, previously working queries from succeeding.\n\n* Added startup option `--audit.queue` to control audit logging queuing behavior\n  (Enterprise Edition only):\n\n  The option controls whether audit log messages are submitted to a queue and\n  written to disk in batches or if they should be written to disk directly\n  without being queued.\n  Queueing audit log entries may be beneficial for latency, but can lead to\n  unqueued messages being lost in case of a power loss or crash. Setting this\n  option to `false` mimics the behavior from 3.7 and before, where audit log\n  messages were not queued but written in a blocking fashion.\n\n* Added metric `arangodb_server_statistics_cpu_cores` to provide the number of\n  CPU cores visible to the arangod process. This is the number of CPU cores\n  reported by the operating system to the process.\n  If the environment variable `ARANGODB_OVERRIDE_DETECTED_NUMBER_OF_CORES` is\n  set to a positive value at instance startup, this value will be returned\n  instead.\n\n* `COLLECT WITH COUNT INTO x` and `COLLECT var = expr WITH COUNT INTO x` are now\n  internally transformed into `COLLECT AGGREGATE x = LENGTH()` and\n  `COLLECT var = expr AGGREGATE x = LENGTH()` respectively. In addition, any\n  argument passed to the `COUNT`/`LENGTH` aggregator functions are now optimized\n  away. This not only simplified the code, but also allows more query\n  optimizations:\n    - If the variable in `COLLECT WITH COUNT INTO var` is not used, the implicit\n      aggregator is now removed.\n    - All queries of the form `COLLECT AGGREGATE x = LENGTH()` are now executed\n      using the count executor, which can result in significantly improved\n      performance.\n\n* Added AQL timezone functions `DATE_TIMEZONE` and `DATE_TIMEZONES`.\n\n* Make DB servers report storage engine health to the agency, via a new \"health\"\n  attribute in requests sent to Sync/ServerStates/<id>.\n  The supervision can in the future check this attribute if it is posted, and\n  mark servers as BAD or FAILED in case an unhealthy status is reported.\n  DB server health is currently determined by whether or not the storage engine\n  (RocksDB) has reported a background error, and by whether or not the free disk\n  space has reached a critical low amount. The current threshold for free disk\n  space is set at 1% of the disk capacity (only the disk is considered that\n  contains the RocksDB database directory).\n  The minimum required free disk space percentage can be configured using the\n  new startup option `--rocksdb.minimum-disk-free-percent`, which needs to be\n  between 0 and 1 (including). A value of 0 disables the check.\n  The minimum required free disk space can also be configured in bytes using the\n  new startup option `--rocksdb.minimum-disk-free-bytes`. A value of 0 disables\n  this check, too.\n\n* Failed servers are now reported consistently in the web interface, at\n  approximately the same time in the navigation bar and in the nodes view.\n  Previously these two places had their own, independent poll mechanism for the\n  nodes' health, and they were updated independently, which could cause an\n  inconsistent view of the nodes' availability.\n  Using only one poll mechanism instead also saves some period background\n  requests for the second availability check.\n\n* Stabilize a Foxx cleanup test.\n\n* Drop a pair of braces {} in /_admin/metrics in case of empty labels, which\n  makes the API adhere better to the official Prometheus syntax.\n\n* Add some more metrics to the ConnectionPool.\n\n* Reduce overhead of audit logging functionality if audit logging is turned off.\n\n* Add several more attributes to audit-logged queries, namely query execution\n  time and exit code (0 = no error, other values correspond to general ArangoDB\n  error codes).\n\n* Fixed a bug in maintainer mode sorting followerinfo lists the wrong way.\n\n* Limit value of `--rocksdb.block-cache-size` to 1 GB for agent instances to\n  reduce agency RAM usage, unless configured otherwise. In addition, limit the\n  value of `--rocksdb.total-write-buffer-size` to 512 MB on agent instances for\n  the same reason.\n\n* Added new `rocksdb_write_stalls` and `rocksdb_write_stops` counter metrics,\n  which should be more accurate than existing metrics related to the underlying\n  conditions.\n\n* Increased the default value of `--rocksdb.min-write-buffer-number-to-merge` in\n  some cases when we have allocated a sufficient amount of memory to the write\n  buffers for this to make sense. The increased value should help prevent\n  compaction-induced write stalls/stops, and should only be enabled when under\n  conditions such that it shouldn't greatly increase the chance of flush-induced\n  write stalls/stops.\n\n* Changed the default values for `--rocksdb.cache-index-and-filter-blocks` and\n  `--rocksdb.cache-index-and-filter-blocks-with-high-priority` to true to\n  improve control over memory usage.\n\n* Lowered the minimum allowed value for `--rocksdb.max-write-buffer-number` from\n  9 to 4 to allow more fine-grained memory usage control.\n\n* Added new ArangoSearch view option 'countApproximate' for customizing view\n  count strategy.\n\n* Views on SmartGraph Edge collections do not contain some documents twice.\n\n* Fixed issue #12248: Web UI - Added missing HTML escaping in the setup script\n  section of a Foxx app.\n\n* The scheduler will now run a minimum of 4 threads at all times, and the\n  default and minimal value for `--server.maximal-threads` has been lowered from\n  64 to the greater of 32 and twice the number of detected cores.\n\n* Throttle work coming from low priority queue, according to a constant and to\n  an estimate taking into account fanout for multi-shard operations.\n\n* Move to 4 priority levels \"low\", \"medium\", \"high\" and \"maintenance\" in\n  scheduler to ensure that maintenance work and diagnostics is always possible,\n  even in the case of RocksDB throttles. Do not allow any RocksDB work on\n  \"maintenance\".\n\n* Commit replications on high priority queue.\n\n* Essentially get rid of timeout in replication to drop followers. This is now\n  entirely handled via reboot and failure tracking. The timeout has now a\n  default minimum of 15 minutes but can still be configured via options.\n\n* Additional metrics for all queue lengths and low prio ongoing work.\n\n* New metric for number and total time of replication operations.\n\n* New metrics for number of internal requests in flight, internal request\n  duration, and internal request timeouts\n\n* Fix `Gauge` assignment operators.\n\n* Add cluster support for collection.checksum() method to calculate CRC\n  checksums for collections.\n\n* Make all Pregel HTTP and JavaScript APIs also accept stringified execution\n  number values, in addition to numeric ones.\n\n  This allows passing larger execution numbers as strings, so that any data loss\n  due to numeric data type conversion (uint32 => double) can be avoided.\n\n  The change also makes the Pregel HTTP and JavaScript APIs for starting a run\n  return a stringified execution number, e.g. \"12345\" instead of 12345.\n\n* Turn off `StatisticsWorker` thread on DB servers.\n  This thread was previously only running queries on the local RocksDB instance,\n  but using the cluster-wide collection names. So effectively it did nothing\n  except use a bit of background CPU. In this case it is better to turn off the\n  background thread entirely on the DB servers.\n\n* Avoid the usage of std::regex when constructing date/time string values for\n  log messages. This is a performance optimization only.\n\n* Increase background garbage-collection interval for cluster transactions from\n  1 second to 2 seconds. This change should reduce the amount of background task\n  activity a tiny bit (though hardly measurable on an otherwise idle server).\n\n* Make the audit log honor the configured logging date/time output format (i.e.\n  `--log.time-format` option). Previously the audit logging always created a\n  time value in the server's local time, and logged it in format\n  YYYY-MM-DDTHH:MM:SS.\n\n  From 3.8 onwards, the audit logger will honor the date/time format specified\n  via the `--log.time-format` option, which defaults to `utc-datestring`. This\n  means the audit logging will by default log all dates/times in UTC time. To\n  restore the pre-3.8 behavior, please set the option `--log.time-format` to\n  `local-datestring`, which will make the audit logger (and all other server log\n  messages) use the server's local time.\n\n* Added metrics for the system CPU usage:\n  - `arangodb_server_statistics_user_percent`: Percentage of time that the\n    system CPUs have spent in user mode\n  - `arangodb_server_statistics_system_percent`: Percentage of time that the\n    system CPUs have spent in kernel mode\n  - `arangodb_server_statistics_idle_percent`: Percentage of time that the\n    system CPUs have been idle\n  - `arangodb_server_statistics_iowait_percent`: Percentage of time that the\n    system CPUs have been waiting for I/O\n\n  These metrics resemble the overall CPU usage metrics in `top`. They are\n  available on Linux only.\n\n* Fix log topic of general shutdown message from \"cluster\" to general.\n\n* Automatically add \"www-authenticate\" headers to server HTTP 401 responses, as\n  required by the HTTP specification.\n\n* Enable HTTP request statistics and provide metrics even in case\n  `--server.statistics-history` is set to `false` (this option will set itself\n  to off automatically on agency instances on startup if not explicitly set).\n  This change provides more metrics on all server instances, without the need to\n  persist them in the instance's RocksDB storage engine.\n\n* Remove extra CMake option `DEBUG_SYNC_REPLICATION` and use the already\n  existing `USE_FAILURE_TESTS` options for its purpose.\n\n* Updated bundled version of Snappy compression/decompression library to 1.1.8.\n\n* Added support of `GEO_DISTANCE`, `GEO_CONTAINS`, `GEO_INTERSECTS`,\n  `GEO_IN_RANGE` to ArangoSearch.\n\n* Added new `GeoJSON` ArangoSearch analyzer.\n\n* Added new `GeoPoint` ArangoSearch analyzer.\n\n* Added new `GEO_IN_RANGE` AQL function.\n\n* Added new 'aql' type for ArangoSearch analyzers.\n\n* Obsoleted the startup options `--database.throw-collection-not-loaded-error`\n  and `--ttl.only-loaded-collection`.\n\n  These options were meaningful for the MMFiles storage engine only, but for the\n  RocksDB storage engine they did not make any difference. Using these startup\n  options is still possible, but will have no effect other than generating a\n  warning at server startup.\n\n* Added CMake option `USE_MINIMAL_DEBUGINFO`.\n  This option is turned off by default. If turned on, the created binaries\n  will contain only a minimum amount of debug symbols, reducing the size of the\n  executables. If turned off (which is the default), the binaries will contain\n  full debug information, which will make them bigger in size unless the debug\n  information is later stripped again.\n\n* Modified the returned error code for calling the `shards()` function on a\n  collection in single-server from \"internal error\" (error number 4) to \"shards\n  API is only available in cluster\" and error number 9, HTTP status code 501.\n\n* Added WINDOW keyword to AQL to allow aggregations on related rows.\n\n* Added new graph method K_PATHS to AQL. This will enumerate all paths between a\n  source and a target vertex that match the given length.\n  For example, the query\n  ```\n    FOR path IN 2..4 OUTBOUND K_PATHS \"v/source\" TO \"v/target\" GRAPH \"g\"\n      RETURN path\n  ```\n  will yield all paths in format\n    {\n      vertices: [v/source, ... , v/target],\n      edges: [v/source -> v/1, ...,  v/n -> v/target\n    }\n  that have length exactly 2 or 3 or 4, start at v/source and end at v/target.\n  The order of those paths in the result set is not guaranteed.\n\n* Fixed issue BTS-195: AQL update queries using the `keepNull` option set to\n  false had an inconsistent behavior. For example, given a collection `test`\n  with an empty document with just key `testDoc`, the following query would\n  return different results when running for the first time or the second time:\n\n    UPDATE 'testDoc'\n        WITH {test: {sub1: true, sub2: null}} IN test\n    OPTIONS { keepNull: false, mergeObjects: true }\n\n  For its first run, the query would return\n\n      {\n        \"_key\": \"testDoc\",\n        \"test\": {\n          \"sub1\": true,\n          \"sub2\": null\n        }\n      }\n\n  (with the `null` attribute value not being removed). For all subsequent runs,\n  the same query would return\n\n      {\n        \"_key\": \"testDoc\",\n        \"test\": {\n          \"sub1\": true,\n        }\n      }\n\n  (with the `null` value removed as requested).\n\n  This inconsistency was due to how the `keepNull` attribute was handled if the\n  attribute already existed in the to-be-updated document or not. The behavior\n  is now consistent, so `null` values are now properly removed from\n  sub-attributes even if in the to-be-updated document the target attribute did\n  not yet exist. This makes such updates idempotent again.\n\n  This a behavior change compared previous versions, but it will only have\n  effect when `keepNull` is set to `false` (the default value is `true`\n  however), and only when just-inserted object sub-attributes contained `null`\n  values.\n\n* Optimization of empty append entries.\n\n* Remove any special handling for obsoleted collection attributes\n  `indexBuckets`, `journalSize`, `doCompact` and `isVolatile`. These attributes\n  were meaningful only with the MMFiles storage engine and have no meaning with\n  the RocksDB storage engine. Thus any special handling for these attributes can\n  be removed in the internal code.\n  Client applications and tests that rely on the behavior that setting any of\n  these attributes produces an error when using the RocksDB engine may need\n  adjustment now.\n\n* Added a --continue option to arangorestore. arangorestore now keeps track of\n  the progress and can continue the restore operation when some error occured.\n\n* Don't respond with misleading error in smart vertex collections.\n\n  When inserting a document with a non-conforming key pattern into a smart\n  vertex collection, the response error code and message are 1466\n  (ERROR_CLUSTER_MUST_NOT_SPECIFY_KEY) and \"must not specify _key for this\n  collection\".\n  This is misleading, because it is actually allowed to specify a key value for\n  documents in such collection. However, there are some restrictions for valid\n  key values (e.g. the key must be a string and contain the smart graph\n  attribute value at the front, followed by a colon.\n  If any of these restrictions are not met, the server currently responds with\n  \"must not specify key for this collection\", which is misleading. This change\n  rectifies it so that the server responds with error 4003\n  (ERROR_KEY_MUST_BE_PREFIXED_WITH_SMART_GRAPH_ATTRIBUTE) and message \"in smart\n  vertex collections _key must be a string and prefixed with the value of the\n  smart graph attribute\". This should make it a lot easier to understand what\n  the actual problem is.\n\n* Fix an issue in arangoimport improperly handling filenames with less than 3\n  characters. The specified input filename was checked for a potential \".gz\"\n  ending, but the check required the filename to have at least 3 characters.\n  This is now fixed.\n\n* Fix for BTS-191: Made transaction API database-aware.\n\n* Minor clean up of and less verbosity in agent callbacks.\n\n* Speed up initial replication of collections/shards data by not wrapping each\n  document in a separate `{\"type\":2300,\"data\":...}` envelope. In addition, the\n  follower side of the replication will request data from leaders in VelocyPack\n  format if the leader is running at least version 3.8.\n  Stripping the envelopes and using VelocyPack for transfer allows for smaller\n  data sizes when exchanging the documents and faster processing, and thus can\n  lead to time savings in document packing and unpacking as well as reduce the\n  number of required HTTP requests.\n\n* Added metric `arangodb_agency_callback_registered counter` for tracking the\n  total number of agency callbacks that were registered.\n\n* Added weighted traversal. Use `mode: \"weighted\"` as option to enumerate paths\n  by increasing weights. The cost of an edge can be read from an attribute which\n  can be specified using `weightAttribute` option.\n\n* Fixed issue ES-696: SEARCH vs FILTER lookup performance.\n  Consolidation functionality for ArangoSearch view links was able to hit non-\n  mergable enormous amount of segments due to improper scheduling logic.\n\n* Make scheduler react and start new threads slightly faster in case a lot of\n  new work arrives.\n\n* Added new ArangoSearch \"pipeline\" analyzer type.\n\n* Added replication metrics `arangodb_replication_initial_sync_bytes_received`\n  for the number of bytes received during replication initial sync operations\n  and `arangodb_replication_tailing_bytes_received` for the number of bytes\n  received for replication tailing requests.\n  Also added `arangodb_replication_failed_connects` to track the number of\n  connection failures or non-OK response during replication.\n\n* Added metrics `rocksdb_free_inodes` and `rocksdb_total_inodes` to track the\n  number of free inodes and the total/maximum number of inodes for the file\n  system the RocksDB database directory is located in. These metrics will always\n  be 0 on Windows.\n\n* Fixed slightly wrong log level for authentication and also added login event\n  to the standard log.\n\n* Added new metrics for the total and the free disk space for the mount used for\n  the RocksDB database directory:\n\n  - `arangodb_rocksdb_free_disk_space`: provides the free disk space for the\n    mount, in bytes\n  - `arangodb_rocksdb_total_disk_space`: provides the total disk space of the\n    mount, in bytes\n\n* Apply user-defined idle connection timeouts for HTTP/2 and VST connections.\n  The timeout value for idle HTTP/2 and VST connections can now be configured\n  via the configuration option `--http.keep-alive-timeout` in the same way as\n  for HTTP/1 connections.\n  HTTP/2 and VST connections that are sending data back to the client are now\n  closed after 300 seconds or the configured idle timeout (the higher of both\n  values is used here).\n  Before this change, the timeouts for HTTP/2 and VST connections were\n  hardcoded to 120 seconds, and even non-idle connections were closed after this\n  timeout.\n\n* Added new metrics for replication:\n  - `arangodb_replication_dump_requests`: number of replication dump requests\n    made.\n  - `arangodb_replication_dump_bytes_received`: number of bytes received in\n    replication dump requests.\n  - `arangodb_replication_dump_documents`: number of documents received in\n    replication dump requests.\n  - `arangodb_replication_dump_request_time`: wait time for replication dump\n    requests.\n  - `arangodb_replication_dump_apply_time`: time required for applying data from\n    replication dump responses.\n  - `arangodb_replication_initial_sync_keys_requests`: number of replication\n    initial sync keys requests made.\n  - `arangodb_replication_initial_sync_docs_requests`: number of replication\n    initial sync docs requests made.\n  - `arangodb_replication_initial_sync_docs_requested`: number of documents\n    requested via replication initial sync requests.\n  - `arangodb_replication_initial_sync_docs_inserted`: number of documents\n    inserted by replication initial sync.\n  - `arangodb_replication_initial_sync_docs_removed`: number of documents\n    inserted by replication initial sync.\n  - `arangodb_replication_initial_chunks_requests_time`: wait time histogram for\n    replication key chunks determination requests.\n  - `arangodb_replication_initial_keys_requests_time`: wait time for replication\n    keys requests.\n  - `arangodb_replication_initial_docs_requests_time`: time needed to apply\n    replication docs data.\n  - `arangodb_replication_initial_insert_apply_time`: time needed to apply\n    replication initial sync insertions.\n  - `arangodb_replication_initial_remove_apply_time`: time needed to apply\n    replication initial sync removals.\n  - `arangodb_replication_initial_lookup_time`: time needed for replication\n    initial sync key lookups.\n  - `arangodb_replication_tailing_requests`: number of replication tailing\n    requests.\n  - `arangodb_replication_tailing_follow_tick_failures`: number of replication\n    tailing failures due to missing tick on leader.\n  - `arangodb_replication_tailing_markers`: number of replication tailing\n    markers processed.\n  - `arangodb_replication_tailing_documents`: number of replication tailing\n    document inserts/replaces processed.\n  - `arangodb_replication_tailing_removals`: number of replication tailing\n    document removals processed.\n  - `arangodb_replication_tailing_bytes_received`: number of bytes received for\n    replication tailing requests.\n  - `arangodb_replication_tailing_request_time`: wait time for replication\n    tailing requests.\n  - `arangodb_replication_tailing_apply_time`: time needed to apply replication\n    tailing markers.\n\n* Allow calling of REST APIs `/_api/engine/stats`, GET `/_api/collection`, GET\n  `/_api/database/current` and GET `/_admin/metrics` on followers in active\n  failover deployments. This can help debugging and inspecting the follower.\n\n* Support projections on sub-attributes (e.g. `a.b.c`).\n\n  In previous versions of ArangoDB, projections were only supported on top-level\n  attributes. For example, in the query\n\n      FOR doc IN collection\n        RETURN doc.a.b\n\n  the projection that was used was just `a`. Now the projection will be `a.b`,\n  which can help reduce the amount of data to be extracted from documents, when\n  only some sub-attributes are accessed.\n\n  In addition, indexes can now be used to extract the data of sub-attributes for\n  projections. If for the above example query an index on `a.b` exists, it will\n  be used now. Previously, no index could be used for this projection.\n\n  Projections now can also be fed by any attribute in a combined index. For\n  example, in the query\n\n    FOR doc IN collection\n      RETURN doc.b\n\n  the projection can be satisfied by a single-attribute index on attribute `b`,\n  but now also by a combined index on attributes `a` and `b` (or `b` and `a`).\n\n* Remove some JavaScript files containing testsuites and test utilities from our\n  official release packages.\n\n* Show optimizer rules with highest execution times in explain output.\n\n* Renamed \"master\" to \"leader\" and \"slave\" to \"follower\" in replication\n  messages.\n  This will change the contents of replication log messages as well the string\n  contents of replication-related error messages.\n\n  The messages of the error codes 1402, 1403 and 1404 were also changed\n  accordingly, as well as the identifiers:\n  - `TRI_ERROR_REPLICATION_MASTER_ERROR` renamed to\n    `TRI_ERROR_REPLICATION_LEADER_ERROR`\n  - `TRI_ERROR_REPLICATION_MASTER_INCOMPATIBLE` renamed to\n    `TRI_ERROR_REPLICATION_LEADER_INCOMPATIBLE`\n  - `TRI_ERROR_REPLICATION_MASTER_CHANGE` renamed to\n    `TRI_ERROR_REPLICATION_LEADER_CHANGE`\n\n  This change also renames the API endpoint `/_api/replication/make-slave` to\n  `/_api/replication/make-follower`. The API is still available under the old\n  name, but using it is deprecated.\n\n* Make optimizer rule \"remove-filters-covered-by-index\" remove FILTERs that were\n  referring to aliases of the collection variable, e.g.\n\n      FOR doc IN collection\n        LET value = doc.indexedAttribute\n        FILTER value == ...\n\n  Previously, FILTERs that were using aliases were not removed by that optimizer\n  rule.\n  In addition, the optimizer rule \"remove-unnecessary-calculations\" will now run\n  again in case it successfully removed variables. This can unlock further\n  removal of unused variables in sequences such as\n\n      FOR doc IN collection\n        LET value = doc.indexedAttribute\n        LET tmp1 = value > ...\n        LET tmp2 = value < ...\n\n  when the removal of `tmp1` and `tmp2` makes it possible to also remove the\n  calculation of `value`.\n\n* Fixed issue BTS-168: Fixed undefined behavior that did trigger segfaults on\n  cluster startups. It is only witnessed for macOS based builds. The issue could\n  be triggered by any network connection.\n  This behavior is not part of any released version.\n\n* Hard-code returned \"planVersion\" attribute of collections to a value of 1.\n  Before 3.7, the most recent Plan version from the agency was returned inside\n  \"planVersion\".\n  In 3.7, the attribute contained the Plan version that was in use when the\n  in-memory LogicalCollection object was last constructed. The object was always\n  reconstructed in case the underlying Plan data for the collection changed or\n  when a collection contained links to arangosearch views.\n  This made the attribute relatively useless for any real-world use cases, and\n  so we are now hard-coding it to simplify the internal code. Using the\n  attribute in client applications is also deprecated.\n\n* Don't prevent concurrent synchronization of different shards from the same\n  database. Previously only one shard was synchronized at a time per database.\n\n* Wait until restore task queue is idle before shutting down.\n\n* Fix a race problem in the unit tests w.r.t. PlanSyncer.\n\n* Errors with error code 1200 (Arango conflict) will now get the HTTP response\n  code 409 (Conflict) instead of 412 (Precondition failed), unless \"if-match\"\n  header was used in `_api/document` or `_api/gharial`.\n\n* Keep the list of last-acknowledged entries in Agency more consistent.\n  During leadership take-over it was possible to get into a situation that the\n  new leader does not successfully report the agency config, which was\n  eventually fixed by the Agent itself. Now this situation is impossible.\n\n* Added support `db._engineStats()` API in coordinator. Previously calling this\n  API always produced an empty result. Now it will return the engine statistics\n  as an object, with an entry for each individual DB-Server.\n\n* Added option `--log.use-json-format` to switch log output to JSON format.\n  Each log message then produces a seperate line with JSON-encoded log data,\n  which can be consumed by applications.\n\n* Added option `--log.process` to toggle the logging of the process id (pid) in\n  log messages. Logging the process id is useless when running arangod in Docker\n  containers, as the pid will always be 1. So one may as well turn it off in\n  these contexts.\n\n* Added option `--log.in-memory` to toggle storing log messages in memory, from\n  which they can be consumed via the `/_admin/log` and by the web UI. By\n  default, this option is turned on, so log messages are consumable via API and\n  the web UI. Turning this option off will disable that functionality and save a\n  tiny bit of memory for the in-memory log buffers.\n\n\nv3.7.10 (2021-03-14)\n--------------------\n\n* Reasonably harden MoveShard against unexpected VelocyPack input.\n\n* Follower DB servers will now respond with error code\n  `TRI_ERROR_CLUSTER_SHARD_FOLLOWER_REFUSES_OPERATION`\n  to any read request. This fixes inadequate HTTP 404 responses from followers,\n  e.g. during chaos tests.\n\n* Fixed Github issue #13632: Query Fails on Upsert with Replace_nth.\n\n* Updated arangosync to 1.2.3.\n\n* Backported AQL sort performance improvements from devel.\n  This change can improve the performance of local sorts operations, e.g.\n\n  Baseline (3.7.9):\n\n      Query String (94 chars, cacheable: false):\n       FOR i IN 1..500000 LET value = CONCAT('testvalue-to-be-sorted', i) SORT value ASC RETURN value\n\n      Execution plan:\n       Id   NodeType            Calls    Items   Runtime [s]   Comment\n        1   SingletonNode           1        1       0.00003   * ROOT\n        2   CalculationNode         1        1       0.00003     - LET #2 = 1 .. 500000   /* range */   /* simple expression */\n        3   EnumerateListNode     500   500000       0.08725     - FOR i IN #2   /* list iteration */\n        4   CalculationNode       500   500000       0.22722       - LET value = CONCAT(\"testvalue-to-be-sorted\", i)   /* simple expression */\n        5   SortNode              500   500000       2.05180       - SORT value ASC   /* sorting strategy: standard */\n        6   ReturnNode            500   500000       0.02911       - RETURN value\n\n      Query Statistics:\n       Writes Exec   Writes Ign   Scan Full   Scan Index   Filtered   Exec Time [s]\n                 0            0           0            0          0         2.39644\n\n  With sort optimization (3.7.10):\n\n      Query String (94 chars, cacheable: false):\n       FOR i IN 1..500000 LET value = CONCAT('testvalue-to-be-sorted', i) SORT value ASC RETURN value\n\n      Execution plan:\n       Id   NodeType            Calls    Items   Runtime [s]   Comment\n        1   SingletonNode           1        1       0.00002   * ROOT\n        2   CalculationNode         1        1       0.00003     - LET #2 = 1 .. 500000   /* range */   /* simple expression */\n        3   EnumerateListNode     500   500000       0.08755     - FOR i IN #2   /* list iteration */\n        4   CalculationNode       500   500000       0.26161       - LET value = CONCAT(\"testvalue-to-be-sorted\", i)   /* simple expression */\n        5   SortNode              500   500000       1.36070       - SORT value ASC   /* sorting strategy: standard */\n        6   ReturnNode            500   500000       0.02864       - RETURN value\n\n      Query Statistics:\n       Writes Exec   Writes Ign   Scan Full   Scan Index   Filtered   Exec Time [s]\n                 0            0           0            0          0         1.73940\n\n* Fixed a problem that coordinators would vanish from the UI and the Health API\n  if one switched the agency Supervision into maintenance mode and kept left\n  that maintenance mode on for more than 24h.\n\n* Fixed a bug in the web interface that displayed the error \"Not authorized to\n  execute this request\" when trying to create an index in the web interface in a\n  database other than `_system` with a user that does not have any access\n  permissions for the `_system` database.\n  The error message previously displayed error actually came from an internal\n  request made by the web interface, but it did not affect the actual index\n  creation.\n\n* Fixed issue BTS-309: The Graph API (Gharial) did not respond with the correct\n  HTTP status code when validating edges. It now responds with 400 (Bad Request)\n  as documented and a new, more precise error code (1947) and message if a\n  vertex collection referenced in the _from or _to attribute is not part of the\n  graph.\n\n\nv3.7.9 (2021-03-01)\n-------------------\n\n* Fix issue #13476: The Java driver v6.9.0 (and older) has bad performance when\n  iterating over AQL cursor results in certain cases. This works around this.\n  This workaround will no longer be available in 3.8.\n\n* Enable statistics in web UI in non-`_system` databases in cluster mode.\n  In cluster mode, the web UI dashboard did not display statistics properly when\n  not being logged in to the `_system` database. For all other databases than\n  `_system`, no statistics were displayed but just some \"No data...\"\n  placeholders.\n  Statistics for non-`_system` databases were not properly displayed since\n  3.7.6 due to an internal change in the statistics processing.\n\n  In addition, a new startup option `--server.statistics-all-databases` controls\n  whether cluster statistics are displayed in the web interface for all\n  databases (if the option is set to `true`) or just for the system database (if\n  the option is set to `false`).\n  The default value for the option is `true`, meaning statistics will be\n  displayed in the web interface for all databases.\n\n* Updated OpenSSL to 1.1.1j and OpenLDAP to 2.4.57.\n\n* Cleanup old HotBackup transfer jobs in agency.\n\n* Added logging of elapsed time of ArangoSearch commit/consolidation/cleanup\n  jobs.\n\n* Fix too early stop of replication, when waiting for keys in large\n  collections/shards.\n\n* Fixed issue BTS-268: fix a flaky Foxx self-heal procedure.\n\n* Fixed issue DEVSUP-720: Within an AQL query, the \"COLLECT WITH COUNT INTO\"\n  statement could lead to a wrong count output when used in combination with an\n  index which has been created with an array index attribute.\n\n* Fix profiling of AQL queries with the `silent` and `stream` options sets in\n  combination. Using the `silent` option makes a query execute, but discard all\n  its results instantly. This led to some confusion in streaming queries, which\n  can return the first query results once they are available, but don't\n  necessarily execute the full query.\n  Now, `silent` correctly discards all results even in streaming queries, but\n  this has the effect that a streaming query will likely be executed completely\n  when the `silent` option is set. This is not the default however, and the\n  `silent` option is normally not set. There is no change for streaming queries\n  if the `silent` option is not set.\n\n  As a side-effect of this change, this makes profiling (i.e. using\n  `db._profileQuery(...)` work for streaming queries as well. Previously,\n  profiling a streaming query could have led to some internal errors, and even\n  query results being returned, even though profiling a query should not return\n  any query results.\n\n* Improved the wording for sharding options displayed in the web interface.\n\n  Instead of offering `flexible` and `single`, now use the more intuitive\n  `Sharded` and `OneShard` options, and update the help text for them.\n\n* EE only bugfix: On DisjointSmartGraphs that are used in anonymous way, there\n  was a chance that the query could fail, if non-disjoint collections were part\n  of the query. Named DisjointSmartGraphs have been save to this bug.\n  Example:\n  DisjointSmartGraph (graph) on vertices -edges-> vertices\n  Query:\n\n  WITH vertices, unrelated\n  FOR out IN 1 OUTBOUND \"v/1:1\" edges\n    FOR u IN unrelated\n    RETURN [out, u]\n\n  The \"unrelated\" collection was pulled into the DisjointSmartGraph, causing the\n  AQL setup to create erroneous state.\n  This is now fixed and the above query works.\n  This query:\n\n  WITH vertices, unrelated\n  FOR out IN 1 OUTBOUND \"v/1:1\" GRAPH \"graph\"\n    FOR u IN unrelated\n    RETURN [out, u]\n\n  was not affected by this bug.\n\n* Avoid a potential deadlock when dropping indexes.\n\n  A deadlock could theoretically happen for a thread that is attempting to drop\n  an index in case there was another thread that tried to create or drop an\n  index in the very same collection at the very same time. We haven't managed to\n  trigger the deadlock with concurrency tests, so it may have been a theoretical\n  issue only. The underlying code was changed anyway to make sure this will not\n  cause problems in reality.\n\n* Make dropping of indexes in cluster retry in case of precondition failed.\n\n  When dropping an indexes of a collection in the cluster, the operation could\n  fail with a \"precondition failed\" error in case there were simultaneous index\n  creation or drop actions running for the same collection. The error was\n  returned properly internally, but got lost at the point when\n  `<collection>.dropIndex()` simply converted any error to just `false`.\n  We can't make `dropIndex()` throw an exception for any error, because that\n  would affect downwards-compatibility. But in case there is a simultaneous\n  change to the collection indexes, we can just retry our own operation and\n  check if it succeeds then. This is what `dropIndex()` will do now.\n\n* Improve incremental sync replication for single server and cluster to cope\n  with multiple secondary index unique constraint violations (before this was\n  limited to a failure in a single unique secondary index). This allows\n  replicating the leader state to the follower in basically any order, as any\n  *other* conflicting documents in unique secondary indexes will be  detected\n  and removed on the follower.\n\n* Fix potential undefined behavior when iterating over connected nodes in an\n  execution plan and calling callbacks for each of the nodes: if the callbacks\n  modified the list of connected nodes of the current that they were called\n  from, this could lead to potentially undefined behavior due to iterator\n  invalidation. The issue occurred when using a debug STL via `_GLIBCXX_DEBUG`.\n\n* Fixed a RocksDB bug which could lead to an assertion failure when compiling\n  with STL debug mode -D_GLIBCXX_DEBUG.\n\n* Fixed a rare internal buffer overflow around ridBuffers.\n\n* Issue #13141: The `move-filters-into-enumerate` optimization, when applied to\n  an EnumerateCollectionNode (i.e. full collection scan), did not do regular\n  checks for the query being killed during the filtering of documents, resulting\n  in the maxRuntime option and manual kill of a query not working timely.\n\n* Do not create index estimator objects for proxy collection objects on\n  coordinators and DB servers. Proxy objects are created on coordinators and DB\n  servers for all shards, and they also make index objects available. In order\n  to reduce the memory usage by these objects, we don't create any index\n  estimator objects for indexes in those proxy objects. Index estimators usually\n  take several KB of memory each, so not creating them will pay out for higher\n  numbers of collections/shards.\n\n* Improvements for logging. This adds the following startup options to arangod:\n\n  - `--log.max-entry-length`: controls the maximum line length for individual\n    log messages that are written into normal logfiles by arangod (note: this\n    does not include audit log messages).\n    Any log messages longer than the specified value will be truncated and the\n    suffix '...' will be added to them. The purpose of this parameter is to\n    shorten long log messages in case there is not a lot of space for logfiles,\n    and to keep rogue log messages from overusing resources.\n    The default value is 128 MB, which is very high and should effectively mean\n    downwards-compatiblity with previous arangod versions, which did not\n    restrict the maximum size of log messages.\n\n  - `--audit.max-entry-length`: controls the maximum line length for individual\n    audit log messages that are written into audit logs by arangod. Any audit\n    log messages longer than the specified value will be truncated and the\n    suffix '...' will be added to them.\n    The default value is 128 MB, which is very high and should effectively mean\n    downwards-compatiblity with previous arangod versions, which did not\n    restrict the maximum size of log messages.\n\n  - `--log.in-memory-level`: controls which log messages are preserved in\n    memory. The default value is `info`, meaning all log messages of types\n    `info`, `warning`, `error` and `fatal` will be stored by an instance in\n    memory (this was also the behavior in previous versions of ArangoDB).\n    By setting this option to `warning`, only `warning` log messages will be\n    preserved in memory, and by setting the option to `error` only error\n    messages will be kept.\n    This option is useful because the number of in-memory log messages is\n    limited to the latest 2048 messages, and these slots are by default shared\n    between informational, warning and error messages.\n\n* Honor the value of startup option `--log.api-enabled` when set to `false`.\n  The desired behavior in this case is to turn off the REST API for logging, but\n  was not implemented. The default value for the option is `true`, so the REST\n  API is enabled. This behavior did not change, and neither did the behavior\n  when setting the option to a value of `jwt` (meaning the REST API for logging\n  is only available for superusers with a valid JWT token).\n\n* Fix error reporting in the reloadTLS route.\n\n* Split the update operations for the _fishbowl system collection with Foxx apps\n  into separate insert/replace and remove operations. This makes the overall\n  update not atomic, but as removes are unlikely here, we can now get away with\n  a simple multi-document insert-replace operation instead of a truncate and an\n  exclusive transaction, which was used before.\n\n\nv3.7.8 (2021-02-16)\n-------------------\n\n* Fixed ES-784 regression related to encryption cipher propagation to\n  ArangoSearch data.\n\n\nv3.7.7 (2021-02-05)\n-------------------\n\n* Added metrics for document read and write operations:\n\n  - `arangodb_document_writes: Total number of document write operations\n    (successful and failed) not performed by synchronous replication.\n  - `arangodb_document_writes_replication`: Total number of document write\n    operations (successful and failed) by cluster synchronous replication.\n  - `arangodb_collection_truncates`: Total number of collection truncate\n    operations (successful and failed) not performed by cluster synchronous\n    replication.\n  - `arangodb_collection_truncates_replication`: Total number of collection\n    truncate operations (successful and failed) by synchronous replication.\n  - `arangodb_document_read_time`: Execution time histogram of all document\n    primary key read operations (successful and failed) [s]. Note: this does not\n    include secondary index lookups, range scans and full collection scans.\n  - `arangodb_document_insert_time`: Execution time histogram of all document\n    insert operations (successful and failed) [s].\n  - `arangodb_document_replace_time`: Execution time histogram of all document\n    replace operations (successful and failed) [s].\n  - `arangodb_document_remove_time`: Execution time histogram of all document\n    remove operations (successful and failed) [s].\n  - `arangodb_document_update_time`: Execution time histogram of all document\n    update operations (successful and failed) [s].\n  - `arangodb_collection_truncate_time`: Execution time histogram of all\n    collection truncate operations (successful and failed) [s].\n\n  The timer metrics are turned off by default, and can be enabled by setting the\n  startup option `--server.export-read-write-metrics true`.\n\n* Fixed issue #12543: Unused Foxx service config can not be discarded.\n\n* Fixed issue #12363: Foxx HTTP API upgrade/replace always enables development\n  mode.\n\n* Fixed BTS-284: upgrading from 3.6 to 3.7 in cluster enviroment.\n  Moved upgrade ArangoSearch links task to later step as it needs cluster\n  connection. Removed misleading error log records for failed ArangoSearch index\n  creation during upgrade phase.\n\n* Normalize user-provided input/output directory names in arangoimport,\n  arangoexport, arangodump and arangorestore before splitting them into path\n  components, in the sense that now both forward and backward slashes can be\n  used on Windows, even interchangingly.\n\n* Fixed some wrong behaviour in single document updates. If the option\n  ignoreRevs=false was given and the precondition _rev was given in the body but\n  the _key was given in the URL path, then the rev was wrongly taken as 0,\n  rather than using the one from the document body.\n\n* Allow {USER} paceholder string also in `--ldap.search-filter`.\n\n* Make `padded` and `autoincrement` key generators export their `lastValue`\n  values, so that they are available in dumps and can be restored elsewhere from\n  a dump.\n\n* Fix decoding of values in `padded` key generator when restoring from a dump.\n\n* Fixed error reporting for hotbackup restore from dbservers back to\n  coordinators. This could for example swallow out of disk errors during\n  hotbackup restore.\n\n* Fix decoding of values in `padded` key generator when restoring from a dump.\n\n* Fixed some situations of\n  [...]\n  SUBQUERY\n  FILTER\n  LIMIT\n  [...]\n  in AQL queries, yielding incorrect responses. A distributed state within the\n  subquery was not resetted correctly. This could also lead into \"shrink\" errors\n  of AQL item blocks, or much higher query runtimes.\n  Fixes:\n  - BTS-252\n  - ES-687\n  - github issue: #13099\n  - github issue: #13124\n  - github issue: #13147\n  - github issue: #13305\n  - DEVSUP-665\n\n* Fix a bug in the agency Supervision which could lead to removeFollower\n  jobs constantly being created and immediately stopped again.\n\n* Limit additional replicas in failover cases to +2.\n\n* Prepare register planning for rolling upgrades. Previously, changes in\n  register planning from 3.7 to a minor future version (i.e. 3.8) could cause\n  queries executed by a 3.7 coordinator in combination with a minor future\n  version (i.e. 3.8) DBServer to fail during a rolling upgrade.\n\n* Fixed rare objectId conflict for indexes.\n\n* Fix for OASIS-409: fixed indexing _id attribute at recovery.\n\n* Fix some issues with key generators not properly taking into account the\n  `allowUserKeys` attribute when in a cluster.\n\n* Added the following bit handling functions to AQL:\n\n  - BIT_AND(array): and-combined result\n  - BIT_OR(array): or-combined result\n  - BIT_XOR(array): xor-combined result\n  - BIT_NEGATE(value, bits): bitwise negation of `value`, with a mask of `bits`\n    length\n  - BIT_TEST(value, index): test if bit at position `index` is set in `value`\n    (indexes are 0-based)\n  - BIT_POPCOUNT(value): return number of bits set in `value`\n  - BIT_SHIFT_LEFT(value, shift, bits): bitwise shift-left of `value` by `shift`\n    bits, with a mask of `bits` length\n  - BIT_SHIFT_RIGHT(value, shift, bits): bitwise shift-right of `value` by\n    `shift` bits, with a mask of `bits` length\n  - BIT_CONSTRUCT(array): construct a number with bits set at the positions\n    given in the array\n  - BIT_DECONSTRUCT(value): deconstruct a number into an array of its individual\n    set bits\n  - BIT_TO_STRING(value): create a bitstring representation from numeric `value`\n  - BIT_FROM_STRING(value): parse a bitstring representation into a number\n\n  `BIT_AND`, `BIT_OR` and `BIT_XOR` are also available as aggregate functions\n  for usage inside COLLECT AGGREGATE.\n\n  All above bit operations support unsigned integer values with up to 32 bits.\n  Using values outside the supported range will make any of these bit functions\n  return `null` and register a warning.\n\n* Add binary (base 2) and hexadecimal (base 16) integer literals to AQL.\n  These literals can be used where regular (base 10) integer literal can used.\n  The prefix for binary integer literals is `0b`, e.g. `0b10101110`.\n  The prefix for hexadecimal integer literals i `0x`, e.g. `0xabcdef02`.\n\n  Binary and hexadecimal integer literals can only be used for unsigned\n  integers.\n  The maximum supported value is `(2 ^ 32) - 1`, i.e. `0xffffffff` (hexadecimal)\n  or `0b11111111111111111111111111111111` (binary).\n\n* Print a version mismatch (major/minor version difference) between the arangosh\n  version and the remote arangod version at arangosh startup.\n\n* Fix a potential shutdown deadlock in AgencyCache.\n\n* Updated arangosync to 1.2.2.\n\n* Minor and rare AQL performance improvement, in nested subqueries:\n  LET sq1 ([..] FILTER false == true LET sq2 = (<X>) [..])\n  where sq1 produces no data (e.g. by the above filter) for sq2, the part <X>\n  have been asked two times (second returns empty result), instead of one, if\n  and only if the mainquery executes sq1 exactly one time.\n  Now we get away with one call only.\n  In the case sq1 has data, or sq1 is executed more often, only one call was\n  needed (assuming the data fits in one batch).\n\n* Improve internal error reporting by cluster maintenance.\n\n* Bug-Fix: In one-shard-database setups that were created in 3.6.* and then\n  upgraded to 3.7.5 the DOCUMENT method in AQL will now return documents again.\n\n\nv3.7.6 (2021-01-04)\n-------------------\n\n* Updated OpenSSL to 1.1.1i and OpenLDAP to 2.4.56.\n\n* Added new metric: \"arangodb_collection_lock_sequential_mode\" this will count\n  how many times we need to do a sequential locking of collections. If this\n  metric increases this indicates lock contention in transaction setup.\n  Most likely this is caused by exlcusive locks used on collections with more\n  than one shard.\n\n* Fix for BTS-213\n  Changed the transaction locking mechanism in the cluster case.\n  For all installations that do not use \"exclusive\" collection locks this change\n  will not be noticable. In case of \"exclusive\" locks, and collections with more\n  than one shard, it is now less likely to get a LOCK_TIMEOUT (ErrorNum 18).\n  It is still possible to get into the LOCK_TIMEOUT case, especially if the\n  \"exclusive\" operation(s) are long-running.\n\n* Fixed an endless busy loop which could happen if a coordinator tries to roll\n  back a database creation, but the database has already been dropped by other\n  means.\n\n* Make internal ClusterInfo::getPlan() wait for initial plan load from agency.\n\n* Remove HTTP \"Connection\" header when forwarding requests in the cluster from\n  one coordinator to another, and let the internal network layer handle closing\n  of connections and keep-alive.\n\n* Prevent a write to RocksDB during recovery in the case that the database\n  already exists. The write at startup is potentially blocking, and will delay\n  the startup for servers that were shut down while in a write-stopped state.\n\n* Fix recovery of \"clientId\" values in Agency when restarting an agent from\n  persistence.\n\n* Added \"startupTime\", \"computationTime\" and \"storageTime\" to Pregel result\n  statistics.\n\n* Add query execution time and query id to audit log query messages.\n\n* Fixed issue #13238 Thread naming API on Windows are now used only if\n  available in KERNEL32.DLL\n\n* Fix for issue #772: Optimized document counting for ArangoSearch views.\n  Added new ArangoSearch view option 'countApproximate' for customizing view\n  count strategy.\n\n* Fix ordering of FollowerInfo lists in maintainer mode.\n\n* Fix AR-113. Disallow non-values in the AQL geo-index-optimizer rule.\n\n* Added SNI support for arangosh.\n\n* Fix agency restart with mismatching compation and log indexes.\n\n* Improve performance and memory efficiency of agency restart from persisted\n  database directory.\n\n* Added the following agency-related metrics:\n  - `arangodb_agency_client_lookup_table_size`: current number of entries in\n    agency client id lookup table. This gauge is available only on agent\n    instances.\n  - `arangodb_agency_cache_callback_count`: current number of entries in agency\n    cache callbacks table. This gauge will be effective on coordinators and DB\n    servers.\n  - `arangodb_agency_callback_count`: current number of agency callbacks\n    registered. This gauge will be effective on coordinators and DB servers.\n\n* Fix cluster-internal replication of documents with special keys (percent\n  character, which has a special meaning when used inside URLs).\n\n* Improvements for the Pregel distributed graph processing feature:\n  - during the loading/startup phase, the in-memory edge cache is now\n    intentionally bypassed. The reason for this is that any edges are looked up\n    exactly once, so caching them is not beneficial, but would only lead to\n    cache pollution.\n  - the loading/startup phase can now load multiple collections in parallel,\n    whereas previously it was only loading multiple shards of the same\n    collection in parallel. This change helps to reduce load times in case there\n    are many collections with few shards, and on single server.\n  - the loading and result storage phases code has been overhauled so that it\n    runs slightly faster.\n  - for Pregel runs that are based on named graphs (in contrast to explicit\n    naming of the to-be-used vertex and edge collections), only those edge\n    collections are considered that, according to the graph definition, can have\n    connections with the vertex. This change can reduce the loading time\n    substantially in case the graph contains many edge definitions.\n  - the number of executed rounds for the underlying Pregel algorithm now does\n    not vary for different `parallelism` values.\n\n* Reimplement coordshort request handler. The new implementation only runs two\n  DB queries without any additional requests to other coordinators, resulting in\n  reduced load on the cluster. Previously this involved requests to all\n  coordinators, where each of them ran two DB queries.\n\n* When querying the list of currently running or slow AQL queries, ignore\n  not-yet created databases on other coordinators.\n\n* Fix AQL cost estimate of spliced subqueries which could lead to overly large\n  numbers in the explain output of such queries.\n\n* Add an AQL query kill check during early pruning. Fixes issue #13141.\n\n* Fix Windows directory creation error handling.\n\n* Added new metrics for tracking AQL queries and slow queries:\n  * `arangodb_aql_query_time`: histogram with AQL query times distribution.\n  * `arangodb_aql_slow_query_time`: histogram with AQL slow query times\n    distribution.\n\n* Reduce the number of dropped followers when running larger (>= 128 MB) write\n  transactions.\n\n* Remove a case in which followers were dropped unnecessarily in streaming\n  transactions that replicated to the same follower.\n\n* Added metrics for collection locks:\n  - `arangodb_collection_lock_timeouts_exclusive`: Number of lock timeouts when\n    trying to acquire collection exclusive locks\n  - `arangodb_collection_lock_timeouts_write`: Number of lock timeouts when\n    trying to acquire collection write locks\n  - `arangodb_collection_lock_acquisition_micros`: Total amount of collection\n    lock acquisition time [μs]\n  - `arangodb_collection_lock_acquisition_time`: Total collection lock\n    acquisition time histogram [s]\n\n* Reduce lock timeout on followers to 15 seconds.\n  Rationale: we should not have any locking conflicts on followers, generally.\n  Any shard locking should be performed on leaders first, which will then,\n  eventually replicate changes to followers. replication to followers is only\n  done once the locks have been acquired on the leader(s).\n\n* Better tracking of memory used in AQL graph traversals, COLLECT and SORT\n  operations. From this version onwards, certain AQL queries can report a higher\n  memory usage than in previous versions of ArangoDB. This is not because the\n  queries use more memory than before, but because the memory usage tracking has\n  been improved.\n  A side effect of this change is that queries with a memory limit set may now\n  be aborted whereas in previous versions they ran through successfully (but\n  actually violated the limit). In this case it may be necessary to adjust (i.e.\n  raise) query memory limits accordingly.\n\n* Added startup option `--foxx.force-update-on-startup` to toggle waiting for\n  all Foxx services in all databases to be propagated to a coordinator before it\n  completes the boot sequence.\n  In case the option is set to `false` (i.e. no waiting), the coordinator will\n  complete the boot sequence faster, and the Foxx services will be propagated\n  lazily. Until the initialization procedure has completed for the local Foxx\n  apps, any request to a Foxx app will be responded to with an HTTP 503 error\n  and message\n\n    waiting for initialization of Foxx services in this database\n\n  This can cause an unavailability window for Foxx services on coordinator\n  startup for the initial requests to Foxx apps until the app propagation has\n  completed.\n\n  When not using Foxx, this option should be set to `false` to benefit from a\n  faster coordinator startup.\n  Deployments relying on Foxx apps being available as soon as a coordinator is\n  integrated or responding should set this option to `true` (which is the\n  default value).\n  The option only has an effect for cluster setups.\n  On single servers and in active failover mode, all Foxx apps will be available\n  from the very beginning.\n  Note: ArangoDB 3.6 and 3.7 introduce this option with a default value of\n  `true`. ArangoDB 3.8 changes the default value to `false`.\n\n* Changed the server-side implementation of the following internal JavaScript\n  APIs to no-ops:\n  * `internal.reloadAqlFunctions()`: this is a no-op function now\n  * `@arangodb/actions.buildRouting()`: this is a no-op function now\n  * `@arangodb/actions.routingTree`: will return an empty object\n  * `@arangodb/actions.routingList`: will return an empty object\n\n  All the above APIs were intended to be used for internal means only. These\n  APIs are deprecated now and will be removed in ArangoDB v3.9.\n\n* Fix HTTP/1.1 status response header in fuerte responses\n\n  This change makes fuerte return the full status header, including the numeric\n  status code and the status string in the `http/1.1` header of fuerte\n  responses.\n\n  Previously, the return header lacked the numeric status code, so it looked\n  like\n  ```\n  \"http/1.1\" : \"Ok\"\n  ```\n  Now, with the numeric status code, the response header will look like\n  ```\n  \"http/1.1\" : \"200 Ok\"\n  ```\n  This PR also adds a protocol() method for arango client connections in order\n  to check the protocol in use. The possible return values are\n  - \"http\" for HTTP/1.1 connections\n  - \"http2\" for HTTP/2 connections\n  - \"vst\" for VST connections\n  - \"unknown\" for everyhting else\n  This is needed during testing, but can also be used for other purposes.\n\n* Fixed bug in the connection pool which could prevent connection reusage under\n  high load and lead to lots of new connection creations, in particular with\n  TLS.\n\n* Added more metrics around connection pool.\n\n* Fix a potential nullptr access in AsyncAgencyComm in case there was a specific\n  error when sending an agency request.\n\n* Clean up agency change log, cluster info caches.\n\n\nv3.7.5 (2020-12-09)\n-------------------\n\n* Fixed ES-662 by introducing refactored thread pool to make more efficient\n  consolidation and commit routines for links of ArangoSearch views.\n\n  Added new command line options for fine-grained ArangoSearch maintenance\n  control:\n  - `--arangosearch.commit-threads` - max number of ArangoSearch commit\n    threads\n  - `--arangosearch.commit-threads-idle` - min number of ArangoSearch\n    commit threads\n  - `--arangosearch.consolidation-threads` - max number of ArangoSearch\n    consolidation threads\n  - `--arangosearch.consolidation-threads-idle` - min number of ArangoSearch\n    consolidation threads\n\n  Deprecated the following command line options:\n  - `--arangosearch.threads` - will be used in --arangosearch.commit-threads`\n    and `--arangosearch.consolidation-threads` as provided value divided by 2\n    if set to non-autodetect value > 0\n  - `--arangosearch.threads-limit`\n\n* Updated ArangoDB Starter to 0.14.15-1.\n\n* Fixed agency redirect in poll API.\n\n* Updated arangosync to 1.2.1.\n\n* Added support for fetching the list of currently running and slow AQL queries\n  from all databases at once, by adding an `all` parameter to the following\n  query APIs:\n\n  * `require(\"@arangodb/aql/queries\").current({ all: true })`: will return the\n    currently running queries from all databases, not just the currently\n    selected database.\n  * HTTP GET `/_api/query/current?all=true`: same, but for the HTTP REST API.\n  * `require(\"@arangodb/aql/queries\").slow({ all: true })`: will return the slow\n    query history from all databases, not just the currently selected database.\n  * HTTP GET `/_api/query/slow?all=true`: same, but for the HTTP REST API.\n  * `require(\"@arangodb/aql/queries\").clearSlow({ all: true })`: will clear the\n    slow query history for all databases, not just the currently selected\n    database.\n  * HTTP DELETE `/_api/query/slow?all=true`: same, but for the HTTP REST API.\n\n  Using the `all` parameter is only allowed when making the call inside the\n  `_system` database and with superuser privileges.\n\n* Fixed issue #12734: Accept HTTP headers into Foxx framework.\n\n* Fix Gauge class' assignment operators.\n\n* Clean up callback bin and empty promises in single-host-agency.\n\n* Fix an issue where a query would not return a result when the geo index was\n  used.\n\n* Fix the activation of the agency supervision maintenance via the REST API\n  `/_admin/cluster/maintenance`. This API stored a boolean value instead of an\n  (expected) maintenance period end date/time string.\n\n* Make the cancel operation safe for asynchronoulsly started JavaScript\n  transactions (via HTTP POST to `/_api/transaction` with the `x-arango-async`\n  header set).\n\n* Fixed initial population of local AgencyCache values after a server restart.\n  Previously the local cache was populated from the agency using a commit index\n  value of 1, whereas it should have been 0 to get the full agency snapshot.\n\n* Updated OpenSSL to 1.1.1h.\n\n* Make the number of network I/O threads properly configurable via the startup\n  option `--network.io-threads`. This option existed before, but its configured\n  value was effectively clamped to a value of `1`. ArangoDB 3.7.5 thus also uses\n  a default value of `1` for this option to remain compatible in terms of\n  default option values.\n\n* Fix internal issue #777: Fixed memory access while substituting stored values\n  for ArangoSearch view optimization.\n\n* Added new metric `arangodb_network_forwarded_requests` to track the number\n  of requests forwarded from one coordinator to another in a load-balancing\n  context.\n\n* Added new metric `arangodb_replication_cluster_inventory_requests` to track\n  the number of requests received for cluster inventories. The cluster\n  inventory API is called at the beginning of a dump process or by arangosync.\n\n* Added new AQL metrics:\n  - `arangodb_aql_total_query_time_msec\": Total execution time of all AQL\n    queries (ms)\n  - `arangodb_aql_all_query`: total number of all AQL queries\n\n* Added new metric `arangodb_aql_total_query_time_msec` to track the combined\n  runtime of AQL queries (slow queries and non-slow queries).\n\n* Added more scheduler metrics:\n\n  - `arangodb_scheduler_threads_started`: Total number of scheduler threads\n    started\n  - `arangodb_scheduler_threads_stopped`: Total number of scheduler threads\n    stopped\n  - `arangodb_scheduler_jobs_done`: Total number of scheduler queue jobs done\n  - `arangodb_scheduler_jobs_submitted`: Total number of jobs submitted to the\n    scheduler queue\n  - `arangodb_scheduler_jobs_dequeued`: Total number of jobs dequeued from the\n    scheduler queue\n  - `arangodb_scheduler_num_working_threads`: Number of currently working\n    scheduler threads\n\n* Added startup option `--server.unavailability-queue-fill-grade`. This option\n  has a consequence for the `/_admin/server/availability` API only, which is\n  often called by load-balancers and other availability probing systems.\n  The `/_admin/server/availability` API will now return HTTP 200 if the fill\n  grade of the scheduler's queue is below the configured value, or HTTP 503 if\n  the fill grade is above it. This can be used to flag a server as unavailable\n  in case it is already highly loaded.\n  The default value for this option is `1`, which will mean that the\n  availability API will start returning HTTP 503 responses in case the scheduler\n  queue is completely full. This is mostly compatible with previous versions of\n  ArangoDB.\n  Previously the availability API still returned HTTP 200 in this situation, but\n  this can be considered a bug, because the server was effectively totally\n  overloaded.\n  To restore 100% compatible behavior with previous version, it is possible to\n  set the option to a value of `0`, which is a special value indicating that the\n  queue fill grade will not be honored.\n\n  To prevent sending more traffic to an already overloaded server, it can be\n  sensible to reduce the default value to `0.75` or even `0.5`.\n  This would mean that instances with a queue longer than 75% (or 50%, resp.) of\n  their maximum queue capacity would return HTTP 503 instead of HTTP 200 when\n  their availability API is probed.\n\n  nb: the default value for the scheduler queue length is 4096.\n\n* Fixed bug with ArangoSearch views on SmartGraph edge collections which could\n  contain some documents twice.\n  This change removes `_to_*` local auxiliary link creation and existence within\n  a view linked with a SmartGraph edge collection.\n\n* Fixed an AQL bug that ignored PRUNE statements in OneShard setups.\n\n* Added arangobench options:\n  `--create-database` to create the test database on start\n  `--duration` to run test for a duration rather than a defined count\n\n* Fixed a deadlock between AQL write transactions and hotbackup, since in AQL\n  write transactions follower transactions did not know they are follower\n  transactions.\n\n* Make the DOCUMENT AQL function eligible for running on DB servers in OneShard\n  deployment mode. This allows pushing more query parts to DB servers for\n  execution.\n\n* Fix REST API endpoint PUT `/_api/collection/<collection>/recalculateCount` on\n  coordinators. Coordinators sent a wrong message body to DB servers here, so\n  the request could not be handled properly.\n\n* Fixed issue #12778: fails validation if additionalProperties: false.\n\n* Added missing exceptions catch clause for some parts of supervision and\n  heartbeat threads.\n\n* Fixed potential deadlock in cluster transactions if a transaction is returned\n  that was soft-aborted by transaction garbage collection before.\n  This deadlock should rarely ever occur in practice, as it can only be\n  triggered once during the server shutdown sequence.\n\n* Fix a memory leak because server internal connections were not cleaned up for\n  agency communication.\n\n* Added compile option USE_JEMALLOC_PROF to enable memory profiling.\n\n* Fixed BTS-233 issue: Fixed invalid IndexId comparator.\n\n* Fixed very spurious errors if the `holdReadLockCollection` replication API for\n  the getting-in-sync procedure of shards was called during server shutdown.\n  In this case that method could ask the transaction manager for a specific\n  transaction, but wasn't returning one due to the server shutdown.\n\n* Agency cache clears change history. This keeps the change history, introduced\n  in v3.7.4, from growing in size too much.\n\n* Bug-fix: Allow to unlink a view created on a SmartGraphEdge collection.\n\n* If a collection (or database) is dropped during the instantiation of an AQL\n  query, the setup code now aborts with an ERROR_QUERY_COLLECTION_LOCK_FAILED\n  and earlier.\n  Before the setup code could abort with TRI_ERROR_INTERNAL in the same case.\n\n* Bug-fix: Creating an additional index on the edge collection of a disjoint\n  SmartGraph could falsely result in an error:\n  `Could not find all smart collections ...`\n  This is now ruled out and indexes can be created as expected.\n\n* Fixed issue #12248: Web UI - Added missing HTML escaping in the setup script\n  section of a foxx app.\n\n* Add parameter so `db.collection.truncate({compact: false})` will stop\n  compaction from happening. Compaction may have performance impacts even if the\n  truncate was invoked on nearly empty collections.\n\n* Instead of failing to connect to INADDR_ANY refuse it as a parameter, with a\n  descriptive error message for novice users (issue #12871).\n\n* Fixed collection count which could be off after a server crash.\n\n\nv3.7.4 (2020-10-16)\n-------------------\n\n* Data definition reconciliation in cluster has been modified\n  extensively to greatly accelerate the creation of 1000s of\n  databases through following means:\n  - AgencyCache offers change sets API based on Raft index.\n  - ClusterInfo caches are only updated using change sets.\n  - Maintenance uses local as well as agency change sets to limit\n    the scope of every runtime to these change sets.\n\n\nv3.7.3 (2020-10-14)\n-------------------\n\n* Added the following metrics for synchronous replication in the cluster:\n\n  - `arangodb_refused_followers_count`: Number of times a shard leader received\n    a refusal answer from a follower during synchronous replication.\n  - `arangodb_sync_wrong_checksum`: Number of times a mismatching shard\n    checksum was detected when syncing shards. In case this happens, a resync\n    will be triggered for the shard.\n\n* Fixed handling of failedLeaderJob. In case of a plan modification, that\n  removes a server from the plan, e.g. reduce replication factor. Directly\n  followed by a failure of the current shard leader, would reinsert the just\n  removed server in the plan, which is undesired, we first need to have a full\n  \"desync\" cycle on this server to be reusable in the plan again.\n\n* Make sure the optimizer doesn't pick another index than the TTL index itself\n  while fulfilling the expiry of TTL.\n\n* Added optional verbose logging for agency write operations. This logging is\n  configurable by using the new log topic \"agencystore\".\n\n  The following log levels can be used for for the \"agencystore\" log topic to\n  log writes to the agency:\n  - DEBUG: will log all writes on the leader\n  - TRACE: will log all writes on both leaders and followers\n  The default log level for the \"agencystore\" log topic is WARN, meaning no\n  agency writes will be logged.\n  Turning on this logging can be used for auditing and debugging, but it is not\n  recommended in the general case, as it can lead to large amounts of data being\n  logged, which can have a performance impact and will lead to higher disk space\n  usage.\n\n* Print image base address and CPU context (if available) in crash handler\n  messages.\n\n* Added configuration option `--query.tracking-slow-queries` to decide whether\n  slow queries are tracked extra.\n\n* Added configuration option `--query.tracking-with-querystring` to decide\n  whether the query string is shown in the slow query log and the list of\n  currently running queries. The option is true by default.\n\n  When turned off, querystrings in the slow query log and the list of currently\n  running queries are just shown as \"<hidden>\".\n\n* Added configuration option `--query.tracking-with-datasources` to toggle\n  whether the names of data sources used by queries are shown in the slow query\n  log and the list of currently running queries. The option is false by default.\n  When turned on, the names of data sources used by the query will be shown in\n  the slow query log and the list of currently running queries.\n\n* Fixed handling of failoverCandidates. Sometimes, a server can still be a\n  failoverCandidate even though it has been taken out of the Plan. With this\n  fix, such a server is quickly taken out of failoverCandidates and it can never\n  be re-added to the Plan before this has happened.\n\n* Fix #12693: SORT inside a subquery could sometimes swallow part of its input\n  when it crossed boundaries of internal row batches.\n\n* Fixed issue BTS-212: Web UI doesn't let to make partial view update and\n  partial view update should be audited (also reported as ES-700).\n  Fixed link definition comparison logic: equality wasn`t properly detected and\n  led to link recreation.\n\n* Added configuration option `--rocksdb.sync-delay-threshold`.\n  This option can be used to track if any RocksDB WAL sync operation is\n  delayed by more than the configured value (in milliseconds). The intention\n  is to get aware of severely delayed WAL sync operations.\n\n* Add database, shard name and error information to several shard-related log\n  messages.\n\n* Display shard names of a collection in the web interface when in the details\n  view of the collection.\n\n* Added HTTP requests metrics for tracking the number of superuser and normal\n  user requests separately:\n\n  - `arangodb_http_request_statistics_superuser_requests`: Total number of HTTP\n    requests executed by superuser/JWT\n  - `arangodb_http_request_statistics_user_requests`: Total number of HTTP\n    requests executed by clients\n\n* Fixed a bug in handling of followers which refuse to replicate operations.\n  In the case that the follower has simply been dropped in the meantime, we now\n  avoid an error reported by the shard leader.\n\n* Fix a performance regression when a LIMIT is combined with a COLLECT WITH\n  COUNT INTO. Reported in ES-692.\n\n* Fix REST handler GET /_admin/status when called with URL parameter value\n  `overview=true`. For generating the `hash` attribute in the response, the\n  current Plan was retrieved and analyzed. Due to a change in the internal Plan\n  format the REST handler code failed to pick up the number of servers, which\n  resulted in the REST handler returning HTTP 500 in cluster mode.\n\n* Use rclone built from v1.51.0 source with go1.15.2 instead of prebuilt\n  v1.51.0 release.\n\n* Fixed a bug in AQL COLLECT with OPTIONS { \"hash\" } that led to a quadratic\n  runtime in the number of output rows.\n\n* Added startup option `--database.old-system-collections` to toggle automatic\n  creation of system collections `_modules` and `_fishbowl`, along with their\n  internal usage. These collections are useful only in very few cases, so it\n  is normally not worth to create them in all databases.\n  The `_modules` collection is only used to register custom JavaScript modules,\n  for which there exists no API, and `_fishbowl` is used to store the temporary\n  list of Foxx apps retrieved from the GitHub Foxx store.\n  If the option value is `false` (which is the default from v3.8 onwards, but\n  for v3.7 the default value is `true` for downwards-compatibility), the two\n  collections will not be created for any new database. The `_fishbowl`\n  collection will still be created dynamically when needed. If the option value\n  is `true` (the default value in v3.7), the collections will be created\n  regularly as before.\n  The default value for the option is going to change to `false` in v3.8,\n  meaning the collections will not be created anymore there by default.\n\n  Any functionality related to the `_modules` system collection is deprecated\n  and will be removed in ArangoDB v3.9.\n\n  Two side effects of turning this option off are:\n  * there will no be iteration over all databases at server startup just to\n    check the contents of all `_modules` collections.\n  * less collections/shards will be around for deployments that create a large\n    number of databases.\n  Already existing `_modules` and `_fishbowl` system collections will not be\n  modified by this PR, even though they will likely be empty and unused.\n\n* Don't iterate over all databases at server startup in order to initialize the\n  routing information. This is not necessary, as the routing information is\n  global and not tied to a specific database.\n\n* Fixed a possible crash during instantiation of an AQL graph traversal.\n  Reported in #12597.\n\n* Added safeguards against using V8 internally in environments that have\n  JavaScript turned off via the `--javascript.enabled false` option.\n\n* Make scheduler properly count down the number of working threads in case an\n  exception happens in a worker thread.\n\n* Turn off upgrade checks in arangod in alpha/beta/preview Enterprise builds,\n  too.\n  Previously it was already turned off in arangod for Enterprise builds already,\n  but only for stable releases and not preview releases.\n\n* Fixed and extended LDAP log messages.\n\n* Added LDAP_OFF if referrals and restart are false.\n\n* If LDAP search fails, also retry (update to given number of retries).\n\n* Fixed infinite reload of the login window after logout of an LDAP user.\n\n* Make the reboot tracker catch failed coordinators, too. Previously the reboot\n  tracker was invoked only when a DB server failed or was restarted, and when a\n  coordinator was restarted. Now it will also act if a coordinator just fails\n  (without restart).\n\n* Added scheduler thread creation/destruction metrics:\n\n  - `arangodb_scheduler_threads_started`: Number of scheduler threads started\n  - `arangodb_scheduler_threads_stopped`: Number of scheduler threads stopped\n\n* Added startup option `--query.max-runtime` to limit the maximum runtime of all\n  AQL queries to a specified threshold value (in seconds). By default, the\n  threshold is 0, meaning that the runtime of AQL queries is not limited.\n  Setting it to any positive value will restrict the runtime of all AQL queries\n  unless it is overwritten in the per-query \"maxRuntime\" query option.\n  Please note that setting this option will affect *all* queries in all\n  databases, and also queries issues for administration and database-internal\n  purposes.\n  If a query exceeds the configured runtime, it will be killed on the next\n  occasion when the query checks its own status. Killing is best effort, so it\n  is not guaranteed that a query will no longer than exactly the configured\n  amount of time.\n\n* Ensure that the argument to an AQL OPTIONS clause is always an object which\n  does not contain any dynamic (run-time) values. Previously, this was only\n  enforced for traversal options and options for data-modification queries. This\n  change extends the check to all occurrences of OPTIONS.\n\n* Added `details` option to figures command of a collection:\n  `collection.figures(details)`\n\n  Setting `details` to `true` will return extended storage engine-specific\n  details to the figures. The details are intended for debugging ArangoDB itself\n  and their format is subject to change. There is not much use in using the\n  details from a client application.\n  By default, `details` is set to `false`, so no details are returned and the\n  behavior is identical to previous versions of ArangoDB.\n\n* Implement RebootTracker usage for AQL queries in case of coordinator restarts\n  or failures. This will clean up the rest of an AQL query on dbservers more\n  quickly and in particular release locks faster.\n\n* Serialize maintenance actions for each shard. This addresses lost document\n  problems found in chaos testing.\n\n* Enforce a maximum result register usage limit in AQL queries. In an AQL query,\n  every user-defined or internal (unnamed) variable will need a register to\n  store results in.\n\n  AQL queries that use more result registers than allowed (currently 1000) will\n  now abort deterministically during the planning stage with error 32\n  (`resource limit exceeded`) and the error message\n  \"too many registers (1000) needed for AQL query\".\n\n  Before this fix, an AQL query that used more than 1000 result registers\n  crashed the server when assertions were turned on, and the behavior was\n  undefined when assertions were turned off.\n\n* Fixed some cases where subqueries in PRUNE did not result in a parse error,\n  but either in an incomprehensible error (in 3.7), or undefined behavior\n  during execution (pre 3.7).\n\n* Fixed an issue with audit logging misreporting some document requests as\n  internal instead of logging the proper request information.\n\n* Add attributes `database` and `user` when tracking current and slow AQL\n  queries.\n  `database` contains the name of the database the query is/was running in,\n  `user` contains the name of the user that started the query.\n  These attributes will be returned in addition when calling the APIs for\n  current and slow query inspection:\n  * GET `/_api/query/current` and `require(\"arangodb/aql/queries\").current()`\n  * GET `/_api/query/slow` and `require(\"arangodb/aql/queries\").slow()`\n\n  The \"slow query\" log message has also been augmented to contain the database\n  name and the user name.\n\n  The `user` attribute is now also displayed in the web interface in the\n  \"Running queries\" and \"Slow queries\" views.\n\n* Added metrics for V8 contexts usage:\n  * `arangodb_v8_context_alive`: number of V8 contexts currently alive.\n  * `arangodb_v8_context_busy`: number of V8 contexts currently busy.\n  * `arangodb_v8_context_dirty`: number of V8 contexts currently dirty.\n  * `arangodb_v8_context_free`: number of V8 contexts currently free.\n  * `arangodb_v8_context_max`: maximum number of concurrent V8 contexts.\n  * `arangodb_v8_context_min`: minimum number of concurrent V8 contexts.\n\n* Updated arangosync to 0.7.11.\n\n* Make followers in active failover run a compaction after they process a\n  truncate operation and the truncate removed more than 4k documents. This can\n  help to reclaim disk space on the follower earlier than without running the\n  truncate.\n\n* The REST API PUT `/_api/collection/<name>/truncate` will now also run a\n  compaction if the truncation affected more than 4k documents. This may add\n  extra latency to the truncate operation, but can help to reclaim disk space\n  earlier.\n\n* Added REST API PUT `/_admin/compact` for compacting the entire database data.\n  This endpoint can be used to reclaim disk space after substantial data\n  deletions have taken place. The command is also exposed via the JavaScript API\n  as `db._compact();`.\n\n  This command can cause a full rewrite of all data in all databases, which may\n  take very long for large databases. It should thus only be used with care and\n  only when additional I/O load can be tolerated for a prolonged time.\n\n  This command requires superuser access and is only available for the RocksDB\n  storage engine.\n\n* Don't allow creation of smart satellite graphs or collections (i.e. using\n  `\"isSmart\":true` together with `\"replicationFactor\":\"satellite\"` when creating\n  graphs or collections. This combina"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 40.4140625,
          "content": "# -*- mode: CMAKE; -*-\n\n# ------------------------------------------------------------------------------\n# General\n# ------------------------------------------------------------------------------\n\ncmake_minimum_required(VERSION 3.21)\nmessage(STATUS \"CMake version: ${CMAKE_MAJOR_VERSION}.${CMAKE_MINOR_VERSION}.${CMAKE_PATCH_VERSION}\")\nset(CMAKE_POLICY_DEFAULT_CMP0077 NEW)\n\nif (NOT CMAKE_BUILD_TYPE)\n  set(CMAKE_BUILD_TYPE Release\n      CACHE STRING\n      \"Choose the type of build, options are: None Debug Release RelWithDebInfo MinSizeRel.\"\n      FORCE\n  )\nendif ()\n\nif (NOT (CMAKE_BUILD_TYPE STREQUAL \"Debug\"\n      OR CMAKE_BUILD_TYPE STREQUAL \"Release\"\n      OR CMAKE_BUILD_TYPE STREQUAL \"RelWithDebInfo\"\n      OR CMAKE_BUILD_TYPE STREQUAL \"MinSizeRel\"\n      OR CMAKE_BUILD_TYPE STREQUAL \"None\"))\n\n  message(FATAL_ERROR \"expecting CMAKE_BUILD_TYPE: None Debug Release RelWithDebInfo MinSizeRel, got ${CMAKE_BUILD_TYPE}.\")\nendif ()\n\nstring(TOUPPER ${CMAKE_BUILD_TYPE} CMAKE_BUILD_TYPE_UPPER)\n\n# where to find CMAKE modules\nset(CMAKE_MODULE_PATH ${CMAKE_SOURCE_DIR}/cmake ${CMAKE_MODULE_PATH})\n\noption(SKIP_PACKAGING \"\" OFF)\n\n# enable frontend build\noption(USE_FRONTEND \"build the browser-based frontend (requires yarn and nodejs)\" ON)\n\n# be verbose about flags used\noption(VERBOSE \"be verbose about flags used\" OFF)\n\n# treat warnings as errors on some platforms\noption(USE_FAIL_ON_WARNINGS \"treat warnings as errors\" ON)\n\n# use strictly required OpenSSL version (from ./VERSIONS file)\noption(USE_STRICT_OPENSSL_VERSION \"use strictly required OpenSSL version (from ./VERSIONS file)\" OFF)\n\noption(USE_MINIMAL_DEBUGINFO \"use minimal debug symbols for builds containing debug information\" OFF)\n\nfind_package(Git 1.5.3 REQUIRED)\n\n# ------------------------------------------------------------------------------\n# VERSION information\n# ------------------------------------------------------------------------------\n\n# stable release:   MAJOR.MINOR.PATCH\n# hot fix:          MAJOR.MINOR.PATCH-FIXNUMBER\n# unstable release: MAJOR.MINOR.PATCH-TYPE.NUMBER\n# devel:            MAJOR.MINOR.0-devel\n#\n# These are mapped to the following variables:\n#\n# ARANGODB_VERSION_MAJOR = MAJOR\n# ARANGODB_VERSION_MINOR = MINOR\n# ARANGODB_VERSION_PATCH = PATCH\n#\n# for pre-releases, otherwise empty:\n#\n# ARANGODB_VERSION_PRELEASE_TYPE   = TYPE\n# ARANGODB_VERSION_PRELEASE_NUMBER = NUMBER\n#\nset(ARANGODB_VERSION_MAJOR \"3\")\nset(ARANGODB_VERSION_MINOR \"12\")\n\n# when building the nightly ARANGODB_VERSION_PATCH will be set\nif (NOT DEFINED ARANGODB_VERSION_PATCH)\n  set(ARANGODB_VERSION_PATCH \"4\")\n  set(ARANGODB_VERSION_RELEASE_TYPE \"devel\")\n  set(ARANGODB_VERSION_RELEASE_NUMBER \"\")\nelse()\n  unset (ARANGODB_VERSION_RELEASE_TYPE) # do not remove space\n  unset (ARANGODB_VERSION_RELEASE_NUMBER) # do not remove space\nendif()\n\n# unset TYPE and NUMBER in case they are empty\nif (DEFINED ARANGODB_VERSION_RELEASE_TYPE)\n  if (ARANGODB_VERSION_RELEASE_TYPE STREQUAL \"\")\n    unset (ARANGODB_VERSION_RELEASE_TYPE) # do not remove space\n    unset (ARANGODB_VERSION_RELEASE_NUMBER) # do not remove space\n  endif()\nelse()\n  unset (ARANGODB_VERSION_RELEASE_NUMBER) # do not remove space\nendif()\n\nif (DEFINED ARANGODB_VERSION_RELEASE_NUMBER)\n  if (ARANGODB_VERSION_RELEASE_NUMBER STREQUAL \"\")\n    unset (ARANGODB_VERSION_RELEASE_NUMBER) # do not remove space\n  endif()\nendif()\n\n# semantic version\nset(ARANGODB_PLAIN_VERSION \"${ARANGODB_VERSION_MAJOR}.${ARANGODB_VERSION_MINOR}.${ARANGODB_VERSION_PATCH}\")\n\nif (DEFINED ARANGODB_VERSION_RELEASE_TYPE)\n  if (DEFINED ARANGODB_VERSION_RELEASE_NUMBER)\n    set(ARANGODB_VERSION \"${ARANGODB_PLAIN_VERSION}-${ARANGODB_VERSION_RELEASE_TYPE}.${ARANGODB_VERSION_RELEASE_NUMBER}\")\n  else()\n    set(ARANGODB_VERSION \"${ARANGODB_PLAIN_VERSION}-${ARANGODB_VERSION_RELEASE_TYPE}\")\n  endif()\nelse()\n  set(ARANGODB_VERSION \"${ARANGODB_PLAIN_VERSION}\")\nendif()\nset(ARANGODB_JS_VERSION \"js\")\n\nmessage(STATUS \"ARANGODB PLAIN VERSION: ${ARANGODB_PLAIN_VERSION}\")\nmessage(STATUS \"ARANGODB VERSION: ${ARANGODB_VERSION}\")\nmessage(STATUS \"ARANGODB JS VERSION: ${ARANGODB_JS_VERSION}\")\n\n################################################################################\n# SNAP version\n################################################################################\n\nset(ARANGODB_SNAP_REVISION \"1\")\n\nmessage(STATUS \"SNAP REVISION: ${ARANGODB_SNAP_REVISION}\")\n\n# ------------------------------------------------------------------------------\n#\n# ------------------------------------------------------------------------------\n\nproject(arangodb3 LANGUAGES CXX C ASM VERSION ${ARANGODB_VERSION_MAJOR}.${ARANGODB_VERSION_MINOR})\n\n# Enable CMake's Testing facilities.\n#\n# This enables you to use the add_test cmake command to add a test which is\n# automatically run by executing ctest\nenable_testing()\n\nset(BUILD_SHARED_LIBS OFF)\n\n# required for clang completion in editors - must be set after creating project\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n\nif (USE_FAIL_ON_WARNINGS\n    AND CMAKE_GENERATOR MATCHES \"^Visual Studio\"\n    AND CMAKE_GENERATOR_TOOLSET STREQUAL \"ClangCL\")\n  message(WARNING [[\n    clang-cl is used with USE_FAIL_ON_WARNINGS (i.e. /WX).\n    The build will probably fail, because system header includes aren't passed\n    as system header includes to clang-cl through the VS toolchain.\n  ]])\nendif()\n\n# Static executables:\noption(STATIC_EXECUTABLES \"produce static executables\" OFF)\nif (STATIC_EXECUTABLES)\n  set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -static\")\n  set(CMAKE_FIND_LIBRARY_SUFFIXES \".a\")\nendif()\n\n# enable V8/JavaScript in executables. \n# note that V8 will always be built, regardless of the value of this option.\n# this is required because the V8 build step also builds ICU, which is needed\n# in all programs.\n# if set to OFF, V8 will not be linked to arangod and the client-tools, \n# and V8 integration code for arangod will not be compiled. \n# additionally, arangosh will not be built.\n# setting the option to OFF will further disable all JavaScript support in\n# arangod, rendering the web UI, JavaScript transactions, AQL user-defined\n# functions (UDFs), Foxx and some other smaller features unusable.\n# setting this option to OFF also renders many tests unusable as many tests\n# depend on JavaScript.\n# setting the option to OFF is highly experimental option and not well tested.\n# use at your own risk!\noption(USE_V8 \"build with V8/JavaScript support\" ON)\nif (USE_V8)\n  add_definitions(\"-DUSE_V8=1\")\nendif ()\n\n# enable dtrace\noption(USE_DTRACE \"enable dtrace probes\" OFF)\nif (USE_DTRACE)\n  add_definitions(\"-DUSE_DTRACE=1\")\nendif ()\n\n# enable code coverage\noption(USE_COVERAGE \"enable gcov code coverage\" OFF)\nif (USE_COVERAGE)\n  add_compile_options(-fno-stack-protector -fprofile-arcs -ftest-coverage)\n  add_link_options(--coverage)\n  add_definitions(\"-DUSE_COVERAGE=1\")\nendif ()\n\n# enable Enterprise Edition features\nset(ENTERPRISE_INCLUDE_DIR \"enterprise\")\noption(USE_ENTERPRISE \"enable enterprise build\" OFF)\n\n# we want the following definitions to be in effect for both rocksdb and arangodb\nadd_definitions(\"-DNROCKSDB_THREAD_STATUS\")\nadd_definitions(\"-DROCKSDB_SUPPORT_THREAD_LOCAL\")\n\n# for the packages\nset(ARANGODB_PACKAGE_VENDOR  \"ArangoDB GmbH\")\nset(ARANGODB_PACKAGE_CONTACT \"info@arangodb.com\")\nset(ARANGODB_DISPLAY_NAME    \"ArangoDB\")\nset(ARANGODB_URL_INFO_ABOUT  \"https://www.arangodb.com\")\nset(ARANGODB_HELP_LINK       \"https://docs.arangodb.com/${ARANGODB_VERSION_MAJOR}.${ARANGODB_VERSION_MINOR}/\")\nset(ARANGODB_CONTACT         \"hackers@arangodb.com\")\nset(ARANGODB_FRIENDLY_STRING \"ArangoDB - the native multi-model NoSQL database\")\n\n\n# binaries\nset(BIN_ARANGOBENCH   arangobench)\nset(BIN_ARANGOBACKUP  arangobackup)\nset(BIN_ARANGOD       arangod)\nset(BIN_ARANGODUMP    arangodump)\nset(BIN_ARANGOEXPORT  arangoexport)\nset(BIN_ARANGOIMPORT  arangoimport)\nset(BIN_ARANGORESTORE arangorestore)\nset(BIN_ARANGOSH      arangosh)\nset(BIN_ARANGOVPACK   arangovpack)\n\n# test binaries\nset(BIN_ARANGODB_TESTS arangodbtests)\nset(CLEAN_AUTOGENERATED_FILES)\nset(PACKAGES_LIST)\nset(COPY_PACKAGES_LIST)\nset(CLEAN_PACKAGES_LIST)\nset(INSTALL_CONFIGFILES_LIST)\n\n# ------------------------------------------------------------------------------\n# update files containing VERSION information\n# ------------------------------------------------------------------------------\n\nconfigure_file(\n  \"${CMAKE_CURRENT_SOURCE_DIR}/lib/Basics/build.h.in\"\n  \"${CMAKE_CURRENT_BINARY_DIR}/lib/Basics/build.h\"\n  NEWLINE_STYLE UNIX\n)\n\noption(ARANGODB_BUILD_DATE \"Specific build date set from the outside (leave empty to auto-generate)\" \"\")\nif (ARANGODB_BUILD_DATE STREQUAL \"\" OR ARANGODB_BUILD_DATE STREQUAL \"OFF\")\n  if (NOT EXISTS \"${CMAKE_CURRENT_BINARY_DIR}/lib/Basics/build-date.h\")\n    # auto-generate build date\n    string(TIMESTAMP ARANGODB_BUILD_DATE \"%Y-%m-%d %H:%M:%S\")\n    set(GENERATE_BUILD_DATE ON)\n  else ()\n    # build-date.h file already exists. whatever is in there will be kept\n    set(GENERATE_BUILD_DATE OFF)\n  endif ()\nelse ()\n  # forcefully recreate build-date.h file from provided date\n  set(GENERATE_BUILD_DATE ON)\nendif ()\n\nif (NOT DEFINED GENERATE_BUILD_DATE OR GENERATE_BUILD_DATE)\n  set(GENERATE_BUILD_DATE ON CACHE INTERNAL \"whether we should generate the build date\")\n  configure_file(\n    \"${CMAKE_CURRENT_SOURCE_DIR}/lib/Basics/build-date.h.in\"\n    \"${CMAKE_CURRENT_BINARY_DIR}/lib/Basics/build-date.h\"\n    NEWLINE_STYLE UNIX\n    )\nelse ()\n  set(GENERATE_BUILD_DATE OFF CACHE INTERNAL \"whether we should generate the build date\")\nendif ()\n\nconfigure_file(\n  \"${CMAKE_CURRENT_SOURCE_DIR}/lib/Basics/VERSION.in\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/ARANGO-VERSION\"\n  NEWLINE_STYLE UNIX\n)\n\n################################################################################\n## Find the git revision\n################################################################################\n\nfunction(determine_repository_version source_dir build_repository have_build_repository)\n  # Get commit hash \n  execute_process(\n    WORKING_DIRECTORY ${source_dir}\n    COMMAND ${GIT_EXE} rev-parse --short HEAD\n    OUTPUT_VARIABLE COMMIT_RAW\n  )\n  if (NOT COMMIT_RAW)\n    message(FATAL_ERROR \"Can't extract current commit with the command: 'git rev-parse --short HEAD'\")\n  endif()\n\n  string(STRIP ${COMMIT_RAW} COMMIT_SHORT)\n\n  if (NOT DEFINED BUILD_REPO_INFO OR BUILD_REPO_INFO STREQUAL \"default\")\n    execute_process(\n      WORKING_DIRECTORY ${source_dir}\n      COMMAND ${GIT_EXE} branch --show-current\n      OUTPUT_VARIABLE BRANCH_NAME_RAW)\n    if (NOT BRANCH_NAME_RAW)\n      # For example, in docker we do 'checkout'. Hence, it is impossible to detect branch\n      set(${build_repository} \"${COMMIT_SHORT}\" PARENT_SCOPE)\n      set(${have_build_repository} \"1\" PARENT_SCOPE)\n    else()\n      string(STRIP ${BRANCH_NAME_RAW} BRANCH_NAME)\n      set(${build_repository} \"refs/${BRANCH_NAME} ${COMMIT_SHORT}\" PARENT_SCOPE)\n      set(${have_build_repository} \"1\" PARENT_SCOPE)\n    endif()\n  elseif(BUILD_REPO_INFO STREQUAL \"release\")\n    if (\"${ARANGODB_VERSION_RELEASE_NUMBER}\" STREQUAL \"\" AND ARANGODB_VERSION_RELEASE_TYPE MATCHES \"^[1-9][0-9]*$\")\n      string(REPLACE \"-\" \".\" RELEASE_TAG ${ARANGODB_VERSION})\n    else()\n      set(RELEASE_TAG ${ARANGODB_VERSION})\n    endif()\n    set(RELEASE_TAG \"v${RELEASE_TAG}\")\n    execute_process(\n      WORKING_DIRECTORY ${source_dir}\n      COMMAND ${GIT_EXE} describe --all --tags --match ${RELEASE_TAG}\n      OUTPUT_VARIABLE TAG_RAW)\n    if (NOT TAG_RAW)\n      message(FATAL_ERROR \"Can't extract tag using the command: 'git describe --all --tags --match v${ARANGODB_PLAIN_VERSION}\")\n    else()\n      string(STRIP ${TAG_RAW} TAG)\n      set(${build_repository} \"refs/${TAG} ${COMMIT_SHORT}\" PARENT_SCOPE)\n      set(${have_build_repository} \"1\" PARENT_SCOPE)\n    endif()\n  elseif(BUILD_REPO_INFO STREQUAL \"nightly\")\n    set(${build_repository} \"refs/head/${ARANGODB_VERSION_MAJOR}.${ARANGODB_VERSION_MINOR} ${COMMIT_SHORT}\" PARENT_SCOPE)\n    set(${have_build_repository} \"1\" PARENT_SCOPE)\n  else ()\n    set(${build_repository} \"GIT FAILED TO RETRIEVE THE VERSION - UNSUPPORTED BUILD MODE\" PARENT_SCOPE)\n    set(${have_build_repository} \"1\" PARENT_SCOPE)\n  endif()\nendfunction()\n\nfind_program (GIT_EXE git)\nif (DEFINED GIT_EXE AND IS_DIRECTORY \"${CMAKE_SOURCE_DIR}/.git\")\n  determine_repository_version(${CMAKE_SOURCE_DIR} ARANGODB_BUILD_REPOSITORY HAVE_ARANGODB_BUILD_REPOSITORY)\nelse()\n  set(ARANGODB_BUILD_REPOSITORY \"\")\n  set(HAVE_ARANGODB_BUILD_REPOSITORY \"0\")\nendif()\n\nif (DEFINED GIT_EXE AND USE_ENTERPRISE AND IS_DIRECTORY \"${CMAKE_SOURCE_DIR}/enterprise/.git\")\n  determine_repository_version(${CMAKE_SOURCE_DIR}/enterprise ENTERPRISE_BUILD_REPOSITORY HAVE_ENTERPRISE_BUILD_REPOSITORY)\nelse ()\n  set(ENTERPRISE_BUILD_REPOSITORY \"\")\n  set(HAVE_ENTERPRISE_BUILD_REPOSITORY \"0\")\nendif()\n\nif (DEFINED OSKAR_BUILD_REPOSITITORY)\n  set(HAVE_OSKAR_BUILD_REPOSITORY \"1\")\nelse ()\n  set(OSKAR_BUILD_REPOSITORY \"\")\n  set(HAVE_OSKAR_BUILD_REPOSITORY \"0\")\nendif ()\n\nconfigure_file(\n  \"${CMAKE_CURRENT_SOURCE_DIR}/lib/Basics/build-repository.h.in\"\n  \"${CMAKE_CURRENT_BINARY_DIR}/lib/Basics/build-repository.h\"\n  NEWLINE_STYLE UNIX\n)\n\nif (VERBOSE)\n  message(STATUS \"ARANGODB_BUILD_REPOSITORY=\\\"${ARANGODB_BUILD_REPOSITORY}\\\"\")\n  message(STATUS \"ENTERPRISE_BUILD_REPOSITORY=\\\"${ENTERPRISE_BUILD_REPOSITORY}\\\"\")\nendif ()\n\n################################################################################\n## OPERATING SYSTEM\n################################################################################\n\nif(CMAKE_SYSTEM_NAME MATCHES \".*Linux\")\n  set(LINUX TRUE)\nelseif (CMAKE_SYSTEM_NAME MATCHES \"kFreeBSD.*\")\n  set(FREEBSD TRUE)\nelseif (CMAKE_SYSTEM_NAME MATCHES \"kNetBSD.*|NetBSD.*\")\n  set(NETBSD TRUE)\nelseif (CMAKE_SYSTEM_NAME MATCHES \"kOpenBSD.*|OpenBSD.*\")\n  set(OPENBSD TRUE)\nelseif (CMAKE_SYSTEM_NAME MATCHES \".*GNU.*\")\n  set(GNU TRUE)\nelseif (CMAKE_SYSTEM_NAME MATCHES \".*BSDI.*\")\n  set(BSDI TRUE)\nelseif (CMAKE_SYSTEM_NAME MATCHES \"DragonFly.*|FreeBSD\")\n  set(FREEBSD TRUE)\nelseif (CMAKE_SYSTEM_NAME MATCHES \"SYSV5.*\")\n  set(SYSV5 TRUE)\nendif ()\n\n# ------------------------------------------------------------------------------\n# user options\n# ------------------------------------------------------------------------------\n\noption(\n  USE_JEMALLOC\n  \"use jemalloc memory allocator\"\n  ON\n)\n\noption(\n  USE_LIBUNWIND\n  \"use libunwind for stack traces\"\n  ON\n)\n\noption(USE_BUILD_ID_READER \"Add code to read arangod's build-id from within arangod\" ON)\n\nif (USE_BUILD_ID_READER)\n  add_definitions(-DUSE_BUILD_ID_READER=true)\nelse ()\n  add_definitions(-DUSE_BUILD_ID_READER=false)\nendif ()\n\n# Guess whether we're using mold\nexecute_process(\n  COMMAND ${CMAKE_CXX_COMPILER} ${CMAKE_EXE_LINKER_FLAGS} -Wl,--version\n  OUTPUT_VARIABLE LINKER_VERSION_OUT)\nif (LINKER_VERSION_OUT MATCHES ^mold)\n  SET(LINKER_IS_MOLD TRUE)\nelse ()\n  SET(LINKER_IS_MOLD FALSE)\nendif ()\n\n# mold doesn't support the linker scripts currently used for the build id reader, and will result in the error\n#   mold: fatal: /home/tobias/Documents/ArangoDB/arangodb/arangodb/lib/BuildId/BuildId.ld:1: build_id_start = ADDR(.note.gnu.build-id);\n#                                                                                    ^ unknown linker script token\n# . Let's warn about this early, and how to work around it:\nif (LINKER_IS_MOLD AND USE_BUILD_ID_READER)\n  message(SEND_ERROR \"It looks like you're using mold as a linker. That doesn't work together with USE_BUILD_ID_READER. Either disable USE_BUILD_ID_READER, or use another linker.\")\nendif ()\n\n################################################################################\n## EXTERNAL PROGRAMS\n################################################################################\n\nset(MAKE make)\n\nfind_package(PythonInterp 3 EXACT REQUIRED)\nget_filename_component(PYTHON_EXECUTABLE \"${PYTHON_EXECUTABLE}\" REALPATH)\n\nset($ENV{PYTHON_EXECUTABLE} ${PYTHON_EXECUTABLE})\nexecute_process(\n    COMMAND ${PYTHON_EXECUTABLE} -c \"from shutil import which\"\n    RESULT_VARIABLE EXIT_CODE\n    OUTPUT_QUIET\n    )\nif (NOT \"${EXIT_CODE}\" EQUAL \"0\")\n  message(FATAL_ERROR \"python shutil.which package is required! \")\nendif()\n\n# FIXME the build containers seem to have a\n# /usr/bin/ch(mod|own) to prevent the search\n# to find those files the NO_DEFAULT_PATH\n# argument is passed\nfind_program (MAKE make gmake)\n\nfind_program(\n  CHMOD_EXECUTABLE chmod\n  PATHS \"/bin/\" \"/usr/bin/\"\n  NO_DEFAULT_PATH\n)\nmessage(STATUS \"chmod found in ${CHMOD_EXECUTABLE}\")\nfind_program(\n  CHOWN_EXECUTABLE chown\n  PATHS \"/bin\" \"/usr/bin\"\n  NO_DEFAULT_PATH\n)\nmessage(STATUS \"chown found in ${CHOWN_EXECUTABLE}\")\n\n################################################################################\n## ARCHITECTURE\n################################################################################\n\nmath(EXPR BITS \"8*${CMAKE_SIZEOF_VOID_P}\")\n\n################################################################################\n## COMPILER FEATURES\n################################################################################\n\nif (CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n  set(CMAKE_COMPILER_IS_CLANG 1)\nelseif (CMAKE_CXX_COMPILER_ID STREQUAL \"AppleClang\")\n  set(CMAKE_COMPILER_IS_CLANG 1)\nendif ()\n\nset(BASE_FLAGS     \"\"                                  CACHE STRING \"base flags\")\nset(BASE_C_FLAGS   \"${CMAKE_C_FLAGS}   $ENV{CFLAGS}\"   CACHE STRING \"base C flags\")\nset(BASE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} $ENV{CXXFLAGS}\" CACHE STRING \"base C++ flags\")\nset(BASE_LD_FLAGS                     \"$ENV{LDFLAGS}\"  CACHE STRING \"base linker flags\")\nset(BASE_LIBS                         \"$ENV{LIBS}\"     CACHE STRING \"base libraries\")\n\ninclude(CheckCompilerVersion)\n\nCheckCompilerVersion(\n  11.2 # GCC\n  16.0 # Clang\n)\n\nif (CMAKE_COMPILER_IS_CLANG) \n  if(CMAKE_CXX_COMPILER_VERSION VERSION_LESS \"16.0\")\n    message(WARNING \"ArangoDB requires clang 16.0 or newer, building with older compiler versions is unsupported\")\n  endif()\n  list(APPEND BASE_LIBS atomic)\nendif ()\n\n# need c++20\n# XXX this should really be set on a per target level using cmake compile_features capabilities\nset(CMAKE_CXX_STANDARD 20)\n# turn off compiler language extensions (e.g. don't use -std=gnu++20, but\n# -std=c++20).\nset(CMAKE_CXX_EXTENSIONS OFF)\n\n# need threads\nfind_package(Threads REQUIRED)\n\nset(CMAKE_EXE_LINKER_FLAGS\n  \"${CMAKE_EXE_LINKER_FLAGS} ${BASE_LD_FLAGS}\"\n)\n\n################################################################################\n## TARGET ARCHITECTURE\n################################################################################\n\ninclude(OptimizeForArchitecture)\nset(BASE_FLAGS \"${BASE_FLAGS} ${ARCHITECTURE_OPTIMIZATIONS}\")\n\n################################################################################\n## BACKTRACE\n################################################################################\n\n# iresearch uses backtrace, so we need to find and link libexecinfo\n# for the case that we are on libmusl and not on glibc\nfind_package(Backtrace)\n\nif (Backtrace_LIBRARY)\n  set(BT_LIBS ${Backtrace_LIBRARY} CACHE PATH \"Debug Helper libraries\")\nelse()\n  set(BT_LIBS \"\" CACHE PATH \"Debug Helper libraries\")\nendif()\n\n################################################################################\n## ASSEMBLER OPTIMIZATIONS\n################################################################################\n\n# Allow to prohibit assembler optimization code explicitly\nif (ARCH_AMD64)\n  SET(ASM_OPTIMIZATIONS_DEFAULT ON)\nelse (ARCH_AMD64)\n  SET(ASM_OPTIMIZATIONS_DEFAULT OFF)\nendif (ARCH_AMD64)\n\noption(ASM_OPTIMIZATIONS \"whether hand-optimized assembler code should be used\"\n  ${ASM_OPTIMIZATIONS_DEFAULT})\n\nif (ASM_OPTIMIZATIONS)\n  add_definitions(\"-DASM_OPTIMIZATIONS=1\")\nelse (ASM_OPTIMIZATIONS)\n  add_definitions(\"-DASM_OPTIMIZATIONS=0\")\nendif (ASM_OPTIMIZATIONS)\n\n################################################################################\n## MAINTAINER MODE\n################################################################################\n\noption(USE_MAINTAINER_MODE\n  \"whether we want to have assertions and other development features\"\n  OFF\n)\n\nif (USE_MAINTAINER_MODE)\n  add_definitions(\"-DIRESEARCH_DEBUG\")\n\n  add_definitions(\"-DARANGODB_ENABLE_MAINTAINER_MODE=1\")\n  if ((CMAKE_COMPILER_IS_GNUCC OR CMAKE_COMPILER_IS_CLANG) AND NOT CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n    add_definitions(\"-D_FORTIFY_SOURCE=3\")\n  endif()\n\n  find_package(FLEX)\n  find_package(BISON)\n  # these are required for generateREADME.sh\n  find_program(FGREP_EXECUTABLE fgrep)\n  find_program(SED_EXECUTABLE sed)\n  find_program(AWK_EXECUTABLE awk)\nendif ()\n\noption(USE_GOOGLE_TESTS \"Compile C++ unit tests\" ON)\nif (USE_GOOGLE_TESTS)\n  add_definitions(\"-DARANGODB_USE_GOOGLE_TESTS=1\")\nendif()\n\ninclude(debugInformation)\nfind_program(READELF_EXECUTABLE readelf)\ndetect_binary_id_type(CMAKE_DEBUG_FILENAMES_SHA_SUM)\n\n################################################################################\n## FAILURE TESTS\n################################################################################\n\noption(USE_FAILURE_TESTS\n  \"whether we want to have failure tests compiled in\"\n  OFF\n)\n\nif (USE_FAILURE_TESTS)\n  add_definitions(\"-DARANGODB_ENABLE_FAILURE_TESTS=1\")\nendif ()\n\n################################################################################\n## INTERPROCEDURAL OPTIMIZATION (LINK TIME OPTIMIZATION)\n################################################################################\n\nset(USE_IPO AUTO CACHE STRING \"Use interprocedural optimization: ON, OFF or AUTO\")\nset_property(CACHE USE_IPO PROPERTY STRINGS AUTO ON OFF)\n\nset(IPO_ENABLED False)\n\n# Determine value if IPO_ENABLED from USE_IPO and CMAKE_BUILD_TYPE\nif (USE_IPO STREQUAL \"AUTO\")\n  # When USE_IPO=AUTO, enable IPO for optimized / release builds.\n  # But to work around a g++ segfault triggered by using both -flto and\n  # -fno-devirtualize-functions, we disable IPO when using google tests, because\n  # this will set no-devirtualize. See\n  # https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91387 and\n  # https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91375.\n  # So this check may be removed later as soon as we use fixed gcc versions.\n  # - Tobias, 2019-08-08\n  if (NOT USE_GOOGLE_TESTS AND\n    (CMAKE_BUILD_TYPE STREQUAL \"Release\"\n    OR CMAKE_BUILD_TYPE STREQUAL \"RelWithDebInfo\"\n    OR CMAKE_BUILD_TYPE STREQUAL \"MinSizeRel\"))\n    set(IPO_ENABLED True)\n  else()\n    set(IPO_ENABLED False)\n  endif ()\nelseif (USE_IPO)\n  set(IPO_ENABLED True)\nelse()\n  set(IPO_ENABLED False)\nendif()\n\nmessage(STATUS \"IPO_ENABLED: ${IPO_ENABLED}\")\nset(CMAKE_INTERPROCEDURAL_OPTIMIZATION ${IPO_ENABLED})\n\nif (IPO_ENABLED)\n  add_definitions(\"-DARANGODB_USE_IPO=1\")\nendif()\n\n################################################################################\n## LIBRARY RESOLV\n################################################################################\n\nset(SYS_LIBS ${SYS_LIBS} resolv rt)\n\n# ------------------------------------------------------------------------------\n# IMPLICIT INCLUDES AND LIBRARY DIRECTORIES\n# ------------------------------------------------------------------------------\n\nfunction(CREATE_FLAGS OUTPUT GLUE)\n  set(_TMP_RESULT \"\")\n\n  foreach(arg ${ARGN})\n    set(_TMP_RESULT \"${_TMP_RESULT} ${GLUE}${arg}\")\n  endforeach()\n\n  set(${OUTPUT} \"${_TMP_RESULT}\" PARENT_SCOPE)\nendfunction()\n\n# ------------------------------------------------------------------------------\n# JEMALLOC\n# ------------------------------------------------------------------------------\n\noption(USE_JEMALLOC_PROF \"use jemalloc profiler\" ON)\noption(USE_JEMALLOC_CHECKS \"use jemalloc extended safety checks\" ON)\n\nif (USE_JEMALLOC)\n  add_definitions(\"-DARANGODB_HAVE_JEMALLOC=1\")\nelse ()\n  # Must not compile in profiling stuff if we are not using JEMALLOC\n  set(USE_JEMALLOC_PROF OFF)\nendif ()\n\nif (USE_JEMALLOC_PROF)\n  add_definitions(\"-DUSE_MEMORY_PROFILE=1\")\nendif ()\n\n# ------------------------------------------------------------------------------\n# LIBUNWIND\n# ------------------------------------------------------------------------------\n\nif (USE_LIBUNWIND)\n  add_definitions(\"-DARANGODB_HAVE_LIBUNWIND=1\")\nendif ()\n\n# ------------------------------------------------------------------------------\n# NDEBUG\n# ------------------------------------------------------------------------------\n\nadd_definitions(-DNDEBUG)\n\n################################################################################\n## FLAGS\n################################################################################\n\ninclude(CheckCCompilerFlag)\ninclude(CheckCXXCompilerFlag)\n\n# Tests whether an argument can be passed to the C compiler without diagnostics,\n# and appends it to the first variable if possible.\n# If called with multiple arguments, they are tested and possibly appended\n# independently.\nfunction(add_c_flags_if_supported var)\n  foreach(flag ${ARGN})\n    set(flag_var_name \"C_COMPILER_SUPPORTS_${flag}\")\n    # Note that this compiles a test file, also using CMAKE_C_FLAGS, which\n    # may thus affect the result.\n    # Also note that check_c_compiler_flag *internally caches* the result for\n    # per *var_name*, forcing us to use a unique variable for each call with the\n    # same option.\n    # An alternative (albeit more costly) would be to unset the variable each time.\n    check_c_compiler_flag(${flag} \"${flag_var_name}\")\n    set(is_supported ${${flag_var_name}})\n    if(is_supported)\n      set(${var} \"${${var}} ${flag}\")\n    endif()\n  endforeach()\n  set(${var} \"${${var}}\" PARENT_SCOPE)\nendfunction()\n\n# Tests whether an argument can be passed to the C++ compiler without diagnostics,\n# and appends it to the first variable if possible.\n# If called with multiple arguments, they are tested and possibly appended\n# independently.\nfunction(add_cxx_flags_if_supported var)\n  foreach(flag ${ARGN})\n    set(flag_var_name \"CXX_COMPILER_SUPPORTS_${flag}\")\n    # Note that this compiles a test file, also using CMAKE_CXX_FLAGS, which\n    # may thus affect the result.\n    # Also note that check_cxx_compiler_flag *internally caches* the result for\n    # per *var_name*, forcing us to use a unique variable for each call with the\n    # same option.\n    # An alternative (albeit more costly) would be to unset the variable each time.\n    check_cxx_compiler_flag(${flag} \"${flag_var_name}\")\n    set(is_supported ${${flag_var_name}})\n    if(is_supported)\n      set(${var} \"${${var}} ${flag}\")\n    endif()\n  endforeach()\n  set(${var} \"${${var}}\" PARENT_SCOPE)\nendfunction()\n\nfunction(add_compile_warnings_flags)\n  if (USE_FAIL_ON_WARNINGS)\n    add_compile_options(-Werror -Wno-error=deprecated-declarations)\n    if (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n      # This option has too many false positives\n      add_compile_options(-Wno-error=maybe-uninitialized)\n      # This prevents useful partial initializations\n      add_compile_options(-Wno-missing-field-initializers)\n    endif()\n  endif ()\nendfunction()\n\nif (VERBOSE)\n  message(STATUS)\nendif ()\n\n\n# compiler options\nset(EXTRA_C_FLAGS \"\")\nset(EXTRA_CXX_FLAGS \"\")\n\nif (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n  message(STATUS \"Compiler type GNU: ${CMAKE_CXX_COMPILER}\")\n  set(BASE_FLAGS \"-Wall -Wextra -Wno-unused-parameter -Wno-deprecated-declarations ${BASE_FLAGS}\")\n  set(EXTRA_CXX_FLAGS \"-Wsuggest-override -Wnon-virtual-dtor\")\n  if (CMAKE_CXX_COMPILER_VERSION VERSION_EQUAL \"11.1.0\")\n    set(EXTRA_CXX_FLAGS \"${EXTRA_CXX_FLAGS} -Wno-error=nonnull\")\n  endif()\n  if (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL \"13.0.0\")\n    # the following warning types need to be suppressed with newer\n    # versions of g++, as they produce a lot of false positives.\n    # if the no-error directives are removed, a lot 3rdParty libraries\n    # such as date, fmt, and immer cannot be compiled anymore.\n    # https://gcc.gnu.org/bugzilla/buglist.cgi?bug_status=__open__&content=stringop-overread&no_redirect=1&product=gcc\n    set(EXTRA_CXX_FLAGS \"${EXTRA_CXX_FLAGS} -Wno-error=dangling-reference\")\n    set(EXTRA_CXX_FLAGS \"${EXTRA_CXX_FLAGS} -Wno-error=array-bounds\")\n    set(EXTRA_CXX_FLAGS \"${EXTRA_CXX_FLAGS} -Wno-error=overloaded-virtual\")\n    set(EXTRA_CXX_FLAGS \"${EXTRA_CXX_FLAGS} -Wno-error=stringop-overflow\")\n    set(EXTRA_CXX_FLAGS \"${EXTRA_CXX_FLAGS} -Wno-error=stringop-overread\")\n  endif()\nelseif (CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n  message(STATUS \"Compiler type CLANG: ${CMAKE_CXX_COMPILER}\")\n  set(BASE_FLAGS \"-Wall -Wextra -Wno-unused-parameter -Wno-deprecated-declarations ${BASE_FLAGS}\")\n  set(EXTRA_CXX_FLAGS \"-Wnon-virtual-dtor\")\n  \n  if (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL \"11.0\")\n    # clang 11 and higher supports -Wsuggest-override. older versions don't\n    set(EXTRA_CXX_FLAGS \"-Wsuggest-override ${EXTRA_CXX_FLAGS}\")\n  endif ()\nelse ()\n  # unknown compiler\n  message(STATUS \"Compiler type UNKNOWN: ${CMAKE_CXX_COMPILER}\")\n  set(BASE_FLAGS \"-Wall ${BASE_FLAGS}\")\nendif ()\n\n# flags for builds without debug symbols\nset(NODEBUGINFO_FLAGS \"-g0\")\n\n# flags for builds with debug symbols\nif (USE_MINIMAL_DEBUGINFO)\n  # minimal debug symbols\n  set(DEBUGINFO_FLAGS \"-g1 -gno-column-info -gz\")\nelse ()\n  # full debug symbols\n  set(DEBUGINFO_FLAGS \"-g -gz\")\nendif ()\n\n# c\n# note: when building one of the build types, CMake will automatically combine\n# the base flags from CMAKE_C_FLAGS with build type-specific flags in \n# CMAKE_C_FLAGS_${CMAKE_BUILD_TYPE}.\n# there is no need to repeat the base flags in the build-type specific flags!\nset(CMAKE_C_FLAGS                  \"\"                                                CACHE INTERNAL \"default C compiler flags\")\nset(CMAKE_C_FLAGS_DEBUG            \"${DEBUGINFO_FLAGS} -O0 -D_DEBUG=1\"               CACHE INTERNAL \"C debug flags\")\nset(CMAKE_C_FLAGS_MINSIZEREL       \"${NODEBUGINFO_FLAGS} -Os\"                        CACHE INTERNAL \"C minimal size flags\")\nset(CMAKE_C_FLAGS_RELEASE          \"${NODEBUGINFO_FLAGS} -O3 -fomit-frame-pointer\"   CACHE INTERNAL \"C release flags\")\nset(CMAKE_C_FLAGS_RELWITHDEBINFO   \"${DEBUGINFO_FLAGS} -O3 -fno-omit-frame-pointer\"  CACHE INTERNAL \"C release with debug info flags\")\n\n# cxx\n# note: when building one of the build types, CMake will automatically combine\n# the base flags from CMAKE_CXX_FLAGS with build type-specific flags in \n# CMAKE_CXX_FLAGS_${CMAKE_BUILD_TYPE}.\n# there is no need to repeat the base flags in the build-type specific flags!\nset(CMAKE_CXX_FLAGS                \"\"                                                CACHE INTERNAL \"default C++ compiler flags\")\nset(CMAKE_CXX_FLAGS_DEBUG          \"${DEBUGINFO_FLAGS} -O0 -D_DEBUG=1\"               CACHE INTERNAL \"C++ debug flags\")\nset(CMAKE_CXX_FLAGS_MINSIZEREL     \"${NODEBUGINFO_FLAGS} -Os\"                        CACHE INTERNAL \"C++ minimal size flags\")\nset(CMAKE_CXX_FLAGS_RELEASE        \"${NODEBUGINFO_FLAGS} -O3 -fomit-frame-pointer\"   CACHE INTERNAL \"C++ release flags\")\nset(CMAKE_CXX_FLAGS_RELWITHDEBINFO \"${DEBUGINFO_FLAGS} -O3 -fno-omit-frame-pointer\"  CACHE INTERNAL \"C++ release with debug info flags\")\n\nif (CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n    # On Darwin -fstandalone-debug is enabled by default, so we don't need\n    # to add it for AppleClang.\n    set(CMAKE_CXX_FLAGS_DEBUG             \"${CMAKE_CXX_FLAGS_DEBUG} -fstandalone-debug\")\n    set(CMAKE_CXX_FLAGS_RELWITHDEBINFO    \"${CMAKE_CXX_FLAGS_RELWITHDEBINFO} -fstandalone-debug\")\nendif ()\n\n# put together the final flags\nset(CMAKE_C_FLAGS    \"${BASE_FLAGS} ${BASE_C_FLAGS} ${CMAKE_C_FLAGS} ${EXTRA_C_FLAGS}\")\nset(CMAKE_CXX_FLAGS  \"${BASE_FLAGS} ${BASE_CXX_FLAGS} ${CMAKE_CXX_FLAGS} ${EXTRA_CXX_FLAGS}\")\n\nif (VERBOSE)\n  message(STATUS \"Info BASE_FLAGS:     ${BASE_FLAGS}\")\n  message(STATUS \"Info BASE_C_FLAGS:   ${BASE_C_FLAGS}\")\n  message(STATUS \"Info BASE_CXX_FLAGS: ${BASE_CXX_FLAGS}\")\n  message(STATUS \"Info BASE_LD_FLAGS:  ${BASE_LD_FLAGS}\")\n  message(STATUS \"Info BASE_LIBS:      ${BASE_LIBS}\")\n  message(STATUS)\n\n  message(STATUS \"Info EXTRA_C_FLAGS:   ${EXTRA_C_FLAGS}\")\n  message(STATUS \"Info EXTRA_CXX_FLAGS: ${EXTRA_CXX_FLAGS}\")\n  message(STATUS)\n\n  if (NOT CMAKE_BUILD_TYPE STREQUAL \"None\")\n    message(STATUS \"Info CMAKE_C_FLAGS:   ${CMAKE_C_FLAGS}\")\n    message(STATUS \"Info CMAKE_CXX_FLAGS: ${CMAKE_CXX_FLAGS}\")\n  endif ()\n  message(STATUS \"Info CMAKE_C_FLAGS_${CMAKE_BUILD_TYPE_UPPER}:   ${CMAKE_C_FLAGS_${CMAKE_BUILD_TYPE_UPPER}}\")\n  message(STATUS \"Info CMAKE_CXX_FLAGS_${CMAKE_BUILD_TYPE_UPPER}: ${CMAKE_CXX_FLAGS_${CMAKE_BUILD_TYPE_UPPER}}\")\n\n  message(STATUS \"Info CMAKE_EXE_LINKER_FLAGS: ${CMAKE_EXE_LINKER_FLAGS}\")\n  message(STATUS)\nendif ()\n\nif(ARANGODB_DEBUG_CMAKE)\n  get_directory_property( DirDefs DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} COMPILE_DEFINITIONS )\n  foreach( d ${DirDefs} )\n    message( STATUS \"Found Define: \" ${d} )\n  endforeach()\n  message( STATUS \"DirDefs: ${DirDefs}\" )\nendif()\n\n################################################################################\n## OpenSSL\n################################################################################\n\nif (NOT EXISTS \"${CMAKE_CURRENT_LIST_DIR}/VERSIONS\")\n  message(FATAL_ERROR \"expecting ${CMAKE_CURRENT_LIST_DIR}/VERSIONS\")\nelse ()\n  file(READ \"${CMAKE_CURRENT_LIST_DIR}/VERSIONS\" ARANGODB_VERSIONS_CONTENT)\n  set (TARGET_OS \"LINUX\")\n\n  if (USE_STRICT_OPENSSL_VERSION)\n    set (OPENSSL_VERSION_PATTERN \".*OPENSSL_${TARGET_OS}[ ]+\\\"([^\\\"]*).*\")\n  else ()\n    set (OPENSSL_VERSION_PATTERN \".*OPENSSL_${TARGET_OS}[ ]+\\\"([0-9]\\.[0-9]\\.[^0-9\\\"]*).*\")\n  endif ()\n  string(REGEX MATCH\n            \"${OPENSSL_VERSION_PATTERN}\"\n            ARANGODB_REQUIRED_OPENSSL_VERSION\n            \"${ARANGODB_VERSIONS_CONTENT}\")\n  if (\"${CMAKE_MATCH_1}\" STREQUAL \"\")\n    message(FATAL_ERROR \"expecting OPENSSL_${TARGET_OS} in ${CMAKE_CURRENT_LIST_DIR}/VERSIONS\")\n  else ()\n    set (ARANGODB_REQUIRED_OPENSSL_VERSION \"${CMAKE_MATCH_1}\")\n    if (USE_STRICT_OPENSSL_VERSION)\n      set (MSG_ARANGODB_REQUIRED_OPENSSL_VERSION \"${ARANGODB_REQUIRED_OPENSSL_VERSION}\")\n    else ()\n      set (MSG_ARANGODB_REQUIRED_OPENSSL_VERSION \"${ARANGODB_REQUIRED_OPENSSL_VERSION}*\")\n    endif ()\n    message (\"Required OpenSSL version: ${MSG_ARANGODB_REQUIRED_OPENSSL_VERSION}\")\n  endif ()\nendif ()\n\n\nif (NOT DEFINED OPENSSL_ROOT_DIR OR \"${OPENSSL_ROOT_DIR}\" STREQUAL \"\")\n  if (DEFINED ENV{OPENSSL_ROOT_DIR} AND NOT \"$ENV{OPENSSL_ROOT_DIR}\" STREQUAL \"\")\n    set (OPENSSL_ROOT_DIR \"$ENV{OPENSSL_ROOT_DIR}\")\n  endif ()\nelse ()\n  set (ENV{OPENSSL_ROOT_DIR} \"${OPENSSL_ROOT_DIR}\")\nendif ()\n\nunset (OPENSSL_FOUND CACHE)\nunset (OPENSSL_INCLUDE_DIR CACHE)\nunset (OPENSSL_CRYPTO_LIBRARY CACHE)\nunset (OPENSSL_SSL_LIBRARY CACHE)\nunset (OPENSSL_LIBRARIES CACHE)\nunset (OPENSSL_VERSION CACHE)\n\nif (DEFINED OPENSSL_ROOT_DIR AND NOT \"${OPENSSL_ROOT_DIR}\" STREQUAL \"\")\n  message (\"Use OPENSSL_ROOT_DIR: ${OPENSSL_ROOT_DIR}\")\nendif ()\n\nfind_package(OpenSSL REQUIRED)\n\nif (OPENSSL_FOUND AND USE_STRICT_OPENSSL_VERSION)\n  if (NOT \"${OPENSSL_VERSION}\" MATCHES \"${ARANGODB_REQUIRED_OPENSSL_VERSION}\")\n    message (FATAL_ERROR \"Wrong OpenSSL version was found: ${OPENSSL_VERSION}! Required version: ${MSG_ARANGODB_REQUIRED_OPENSSL_VERSION}!\")\n  endif ()\nendif ()\n\nmessage(${OPENSSL_INCLUDE_DIR})\n\nadd_definitions(-DARANGODB_OPENSSL_VERSION=\\\"${OPENSSL_VERSION}\\\")\nadd_definitions(-DARANGODB_OPENSSL_VERSION_MAJOR=${OPENSSL_VERSION_MAJOR})\nadd_definitions(-DARANGODB_OPENSSL_VERSION_MINOR=${OPENSSL_VERSION_MINOR})\n\nif (OPENSSL_VERSION)\n  string(REPLACE \".\" \";\" OPENSSL_VERSION_LIST ${OPENSSL_VERSION})\n  list(GET OPENSSL_VERSION_LIST 0 OPENSSL_VERSION_MAJOR)\n  list(GET OPENSSL_VERSION_LIST 1 OPENSSL_VERSION_MINOR)\n\n  if (\"${OPENSSL_VERSION_MAJOR}\" GREATER 0 AND \"${OPENSSL_VERSION_MINOR}\" GREATER 0)\n    option(USE_OPENSSL_NO_SSL2\n      \"do not use OPENSSL_NO_SSL2\"\n      ON\n    )\n  else ()\n    option(USE_OPENSSL_NO_SSL2\n      \"do not use OPENSSL_NO_SSL2\"\n      OFF\n    )\n  endif ()\nendif ()\n\nif (USE_OPENSSL_NO_SSL2)\n  add_definitions(-DOPENSSL_NO_SSL2)\nendif ()\n\n################################################################################\n## 3RD PARTY\n################################################################################\n\nadd_definitions(-DBOOST_UUID_RANDOM_PROVIDER_FORCE_POSIX=1)  # fix random provider\nadd_definitions(-DBOOST_ALL_NO_LIB=1)  # disable boost autolink on windows\nadd_subdirectory(3rdParty EXCLUDE_FROM_ALL)\n\nadd_definitions(\"-DARANGODB_BOOST_VERSION=\\\"${BOOST_VERSION}\\\"\")\n\n# ------------------------------------------------------------------------------\n# RocksDB\n# ------------------------------------------------------------------------------\n\nadd_dependencies(rocksdb snappy)\n\nif (USE_JEMALLOC)\n  add_dependencies(rocksdb jemalloc_build)\n  link_directories(\"${JEMALLOC_HOME}/lib\")\nendif ()\n\n################################################################################\n## VELOCYPACK\n################################################################################\n\nadd_definitions(\"-DVELOCYPACK_XXHASH=1\")\n\nset(V8_LINK_DIRECTORIES \"${LINK_DIRECTORIES}\" CACHE INTERNAL \"\" FORCE)\n\n################################################################################\n## V8\n################################################################################\n\nadd_definitions(\"-DARANGODB_V8_VERSION=\\\"${V8_VERSION}\\\"\")\n\nforeach (LINK_DIR ${V8_LINK_DIRECTORIES})\n  link_directories(\"${LINK_DIR}\")\nendforeach()\n\n################################################################################\n## ZLIB\n################################################################################\n\nadd_definitions(\"-DARANGODB_ZLIB_VERSION=\\\"${ZLIB_VERSION}\\\"\")\nlink_directories(\"${PROJECT_BINARY_DIR}/bin\")\n\n################################################################################\n## PATHS, installation, packages, frontend\n################################################################################\n\nadd_subdirectory(Documentation)\ninclude(ArangoDBInstall)\nif(USE_FRONTEND)\n  include(frontend/aardvark)\nendif()\nif (NOT(SKIP_PACKAGING))\n  include(packages/packages)\nendif()\n\n################################################################################\n## ERRORS FILES\n################################################################################\n\n# If \"make clean\" removes these files, afterwards neither \"make\" nor \"cmake\" work any more.\nset_directory_properties(PROPERTIES CLEAN_NO_CUSTOM \"On\")\n\nset(ERROR_FILES\n  lib/Basics/voc-errors.h\n  lib/Basics/error-registry.h\n  js/common/bootstrap/errors.js\n)\n\nset(ERROR_FILES_GEN)\nset(ERRORS_DAT lib/Basics/errors.dat)\n\nforeach (m IN LISTS ERROR_FILES)\n  get_filename_component(GEN_BASENAME \"${m}\" NAME)\n  if (GEN_BASENAME STREQUAL \"errors.js\")\n    # generated errors.js is copied into the source dir\n    set(TARGET_FILENAME ${CMAKE_SOURCE_DIR}/${m})\n  else ()\n    # generated C++ header files are copied into the build dir\n    set(TARGET_FILENAME ${CMAKE_BINARY_DIR}/${m})\n  endif ()\n  add_custom_command(\n    OUTPUT ${CMAKE_BINARY_DIR}/${m}\n    COMMAND ${PYTHON_EXECUTABLE} ${CMAKE_SOURCE_DIR}/utils/generateErrorfile.py ${CMAKE_SOURCE_DIR}/${ERRORS_DAT} ${CMAKE_BINARY_DIR}/${GEN_BASENAME}.tmp\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different ${CMAKE_BINARY_DIR}/${GEN_BASENAME}.tmp ${TARGET_FILENAME}\n    COMMAND ${CMAKE_COMMAND} -E remove ${CMAKE_BINARY_DIR}/${GEN_BASENAME}.tmp\n    DEPENDS ${CMAKE_SOURCE_DIR}/${ERRORS_DAT} ${CMAKE_SOURCE_DIR}/utils/generateErrorfile.py\n    WORKING_DIRECTORY ${CMAKE_BINARY_DIR}\n    COMMENT  \"Building errors file ${m}\"\n    VERBATIM\n  )\n\n  list(APPEND ERROR_FILES_GEN ${CMAKE_BINARY_DIR}/${m})\nendforeach ()\n\nadd_custom_target(errorfiles ALL DEPENDS ${ERROR_FILES_GEN})\n  \nset(EXIT_CODE_FILES\n  lib/Basics/exitcodes.h\n  js/common/bootstrap/exitcodes.js\n  Installation/Windows/Plugins/exitcodes.nsh\n)\n\nset(EXIT_CODE_FILES_GEN)\nset(EXIT_CODES_DAT lib/Basics/exitcodes.dat)\n\nforeach (m IN LISTS EXIT_CODE_FILES)\n  get_filename_component(GEN_BASENAME \"${m}\" NAME)\n  add_custom_command(\n    OUTPUT ${CMAKE_BINARY_DIR}/${m}\n    COMMAND ${PYTHON_EXECUTABLE} ${CMAKE_SOURCE_DIR}/utils/generateExitCodesFiles.py ${CMAKE_SOURCE_DIR}/${EXIT_CODES_DAT} ${CMAKE_BINARY_DIR}/${GEN_BASENAME}.tmp\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different ${CMAKE_BINARY_DIR}/${GEN_BASENAME}.tmp ${CMAKE_BINARY_DIR}/${m}\n    COMMAND ${CMAKE_COMMAND} -E remove ${CMAKE_BINARY_DIR}/${GEN_BASENAME}.tmp\n    DEPENDS ${CMAKE_SOURCE_DIR}/${EXIT_CODES_DAT} ${CMAKE_SOURCE_DIR}/utils/generateExitCodesFiles.py\n    WORKING_DIRECTORY ${CMAKE_BINARY_DIR}\n    COMMENT  \"Building exitcode file ${m}\"\n    VERBATIM\n  )\n\n  list(APPEND EXIT_CODE_FILES_GEN ${CMAKE_BINARY_DIR}/${m})\nendforeach ()\n\nadd_custom_target(exitcodefiles ALL DEPENDS ${EXIT_CODE_FILES_GEN})\n\n################################################################################\n## SUB-PROJECTS\n################################################################################\n\nlist(INSERT SYSTEM_LIBRARIES 0\n  ${BT_LIBS}\n  ${ZLIB_LIBS}\n  ${ICU_LIBS}\n  ${ICU64_LIBS}\n  ${OPENSSL_LIBRARIES}\n  ${BASE_LIBS}\n  ${SYS_LIBS}\n  ${CMAKE_DL_LIBS}\n  ${CMAKE_THREAD_LIBS_INIT}\n)\n\nadd_subdirectory(lib)\nadd_subdirectory(client-tools)\nadd_subdirectory(arangod)\n\n\nif (USE_GOOGLE_TESTS)\n  add_subdirectory(tests)\nendif ()\n\nadd_custom_target(packages\n  DEPENDS ${PACKAGES_LIST}\n)\n\nadd_custom_target(copy_packages\n  DEPENDS ${COPY_PACKAGES_LIST}\n)\n\nadd_custom_target(clean_packages\n  DEPENDS ${CLEAN_PACKAGES_LIST}\n)\n\nadd_custom_target(clean_autogenerated_files\n  DEPENDS ${CLEAN_AUTOGENERATED_FILES}\n)\n\nmessage(STATUS \"building for git revision: ${ARANGODB_BUILD_REPOSITORY}\")\n\nif (USE_ENTERPRISE)\n  add_definitions(\"-DUSE_ENTERPRISE=1\")\n  add_subdirectory(enterprise)\nendif ()\n\nadd_custom_target(arangodb\n   DEPENDS arangod client-tools)\n\nadd_subdirectory(utils/gdb-pretty-printers/immer/test)\n"
        },
        {
          "name": "CMakePresets.json",
          "type": "blob",
          "size": 9.8173828125,
          "content": "{\n  \"version\": 3,\n  \"cmakeMinimumRequired\": {\n    \"major\": 3,\n    \"minor\": 20,\n    \"patch\": 0\n  },\n  \"configurePresets\": [\n    {\n      \"hidden\": true,\n      \"name\": \"pr-base\",\n      \"binaryDir\": \"${sourceDir}/build\",\n      \"cacheVariables\": {\n        \"CMAKE_BUILD_TYPE\": \"RelWithDebInfo\",\n        \"CMAKE_C_FLAGS\": \"-fno-stack-protector\",\n        \"CMAKE_CXX_FLAGS\": \"-fno-stack-protector\",\n        \"CMAKE_EXE_LINKER_FLAGS\": \"-Wl,--build-id=sha1\",\n        \"USE_IPO\": \"Off\",\n        \"USE_STRICT_OPENSSL_VERSION\": \"On\",\n        \"STATIC_EXECUTABLES\": \"On\"\n      }\n    },\n    {\n      \"hidden\": true,\n      \"name\": \"release-base\",\n      \"binaryDir\": \"${sourceDir}/build-presets/${presetName}\",\n      \"cacheVariables\": {\n        \"CMAKE_BUILD_TYPE\": \"Release\",\n        \"USE_ENTERPRISE\": \"Off\",\n        \"USE_JEMALLOC\": \"On\",\n        \"USE_IPO\": \"On\",\n        \"USE_GOOGLE_TESTS\": \"Off\",\n        \"USE_FAILURE_TESTS\": \"Off\",\n        \"STATIC_EXECUTABLES\": \"On\"\n      }\n    },\n    {\n      \"hidden\": true,\n      \"name\": \"x86_64\",\n      \"cacheVariables\": {\n        \"USE_LIBUNWIND\": \"On\",\n        \"TARGET_ARCHITECTURE\": \"sandy-bridge\"\n      }\n    },\n    {\n      \"hidden\": true,\n      \"name\": \"arm\",\n      \"cacheVariables\": {\n        \"USE_LIBUNWIND\": \"Off\"\n      }\n    },\n    {\n      \"hidden\": true,\n      \"name\": \"pr\",\n      \"inherits\": [\n        \"pr-base\",\n        \"x86_64\"\n      ]\n    },\n    {\n      \"hidden\": true,\n      \"name\": \"pr-arm\",\n      \"inherits\": [\n        \"pr-base\",\n        \"arm\"\n      ]\n    },\n    {\n      \"hidden\": true,\n      \"name\": \"maintainer\",\n      \"cacheVariables\": {\n        \"USE_MAINTAINER_MODE\": \"On\",\n        \"USE_GOOGLE_TESTS\": \"On\",\n        \"USE_FAILURE_TESTS\": \"On\"\n      }\n    },\n    {\n      \"hidden\": true,\n      \"name\": \"tsan\",\n      \"cacheVariables\": {\n        \"USE_JEMALLOC\": \"Off\",\n        \"STATIC_EXECUTABLES\": \"Off\",\n        \"CMAKE_CXX_FLAGS\": \"-fsanitize=thread -fsanitize-blacklist=${sourceDir}/tsan_blacklist.txt\",\n        \"CMAKE_C_FLAGS\": \"-fsanitize=thread\",\n        \"USE_MINIMAL_DEBUGINFO\": \"On\"\n      }\n    },\n    {\n      \"hidden\": true,\n      \"name\": \"alubsan\",\n      \"cacheVariables\": {\n        \"USE_JEMALLOC\": \"Off\",\n        \"STATIC_EXECUTABLES\": \"Off\",\n        \"CMAKE_CXX_FLAGS\": \"-fsanitize=address -fsanitize=leak -fsanitize=undefined -fsanitize=float-divide-by-zero -fsanitize-address-use-after-return=never -fno-sanitize=vptr -fno-sanitize=alignment\",\n        \"CMAKE_C_FLAGS\": \"-fsanitize=address -fsanitize=leak -fsanitize=undefined -fsanitize=float-divide-by-zero -fsanitize-address-use-after-return=never -fno-sanitize=alignment\",\n        \"USE_MINIMAL_DEBUGINFO\": \"On\"\n      }\n    },\n    {\n      \"hidden\": true,\n      \"name\": \"gcov\",\n      \"cacheVariables\": {\n        \"USE_COVERAGE\": \"On\",\n        \"USE_JEMALLOC\": \"On\"\n      }\n    },\n    {\n      \"hidden\": true,\n      \"name\": \"no-v8\",\n      \"cacheVariables\": {\n        \"USE_V8\": \"Off\"\n      }\n    },\n    {\n      \"name\": \"community\",\n      \"inherits\": \"release-base\",\n      \"displayName\": \"Build ArangoDB Community Edition (Release Build)\",\n      \"binaryDir\": \"${sourceDir}/build-presets/${presetName}\",\n      \"cacheVariables\": {}\n    },\n    {\n      \"name\": \"enterprise\",\n      \"inherits\": \"release-base\",\n      \"displayName\": \"Build ArangoDB Enterprise Edition (Release Build)\",\n      \"cacheVariables\": {\n        \"USE_ENTERPRISE\": \"On\"\n      }\n    },\n    {\n      \"name\": \"community-developer\",\n      \"inherits\": [\n        \"maintainer\",\n        \"community\"\n      ],\n      \"displayName\": \"Build ArangoDB Community Edition (Developer Build)\",\n      \"cacheVariables\": {\n        \"CMAKE_BUILD_TYPE\": \"Debug\",\n        \"USE_IPO\": \"Off\",\n        \"USE_FAIL_ON_WARNINGS\": \"On\",\n        \"STATIC_EXECUTABLES\": \"On\",\n        \"USE_STRICT_OPENSSL_VERSION\": \"Off\"\n      }\n    },\n    {\n      \"name\": \"community-pr-non-maintainer\",\n      \"inherits\": [\n        \"pr\",\n        \"community\"\n      ],\n      \"displayName\": \"PR Build ArangoDB Community Edition\"\n    },\n    {\n      \"name\": \"community-pr-no-v8\",\n      \"inherits\": [\n        \"pr\",\n        \"community\",\n        \"no-v8\"\n      ],\n      \"displayName\": \"PR Build ArangoDB Community Edition (without V8)\"\n    },\n    {\n      \"name\": \"community-pr\",\n      \"inherits\": [\n        \"maintainer\",\n        \"community-pr-non-maintainer\"\n      ],\n      \"displayName\": \"PR Build ArangoDB Community Edition Maintainer Mode\"\n    },\n    {\n      \"name\": \"enterprise-pr-no-v8\",\n      \"inherits\": [\n        \"pr\",\n        \"enterprise\",\n        \"no-v8\"\n      ],\n      \"displayName\": \"PR Build ArangoDB Enterprise Edition (without V8)\"\n    },\n    {\n      \"name\": \"enterprise-pr-non-maintainer\",\n      \"inherits\": [\n        \"pr\",\n        \"enterprise\"\n      ],\n      \"displayName\": \"PR Build ArangoDB Enterprise Edition\"\n    },\n    {\n      \"name\": \"enterprise-pr\",\n      \"inherits\": [\n        \"maintainer\",\n        \"enterprise-pr-non-maintainer\"\n      ],\n      \"displayName\": \"PR Build ArangoDB Enterprise Edition Maintainer Mode\"\n    },\n    {\n      \"name\": \"community-pr-arm\",\n      \"inherits\": [\n        \"pr-arm\",\n        \"maintainer\",\n        \"community\"\n      ],\n      \"displayName\": \"PR Build ArangoDB Community Edition (ARM)\"\n    },\n    {\n      \"name\": \"enterprise-developer\",\n      \"inherits\": [\n        \"maintainer\",\n        \"enterprise\"\n      ],\n      \"displayName\": \"Build ArangoDB Enterprise Edition (Developer Build)\",\n      \"cacheVariables\": {\n        \"CMAKE_BUILD_TYPE\": \"Debug\",\n        \"USE_IPO\": \"Off\",\n        \"USE_FAIL_ON_WARNINGS\": \"On\",\n        \"USE_STRICT_OPENSSL_VERSION\": \"Off\"\n      }\n    },\n    {\n      \"name\": \"community-developer-tsan\",\n      \"inherits\": [\n        \"tsan\",\n        \"community-developer\"\n      ],\n      \"displayName\": \"Build ArangoDB Community Edition (TSAN Build)\"\n    },\n    {\n      \"name\": \"community-pr-tsan\",\n      \"inherits\": [\n        \"tsan\",\n        \"community-pr\"\n      ],\n      \"displayName\": \"Build ArangoDB Community Edition (TSAN Build)\"\n    },\n    {\n      \"name\": \"enterprise-developer-tsan\",\n      \"inherits\": [\n        \"tsan\",\n        \"enterprise-developer\"\n      ],\n      \"displayName\": \"Build ArangoDB Enterprise Edition (TSAN Build)\"\n    },\n    {\n      \"name\": \"enterprise-pr-tsan\",\n      \"inherits\": [\n        \"tsan\",\n        \"enterprise-pr\"\n      ],\n      \"displayName\": \"Build ArangoDB Enterprise Edition (TSAN Build)\"\n    },\n    {\n      \"name\": \"community-developer-alubsan\",\n      \"inherits\": [\n        \"alubsan\",\n        \"community-developer\"\n      ],\n      \"displayName\": \"Build ArangoDB Community Edition (ASAN and UBSAN Build)\"\n    },\n    {\n      \"name\": \"community-pr-alubsan\",\n      \"inherits\": [\n        \"alubsan\",\n        \"community-pr\"\n      ],\n      \"displayName\": \"Build ArangoDB Community Edition (ASAN and UBSAN Build)\"\n    },\n    {\n      \"name\": \"enterprise-developer-alubsan\",\n      \"inherits\": [\n        \"alubsan\",\n        \"enterprise-developer\"\n      ],\n      \"displayName\": \"Build ArangoDB Enterprise Edition (ASAN and UBSAN Build)\"\n    },\n    {\n      \"name\": \"enterprise-pr-alubsan\",\n      \"inherits\": [\n        \"alubsan\",\n        \"enterprise-pr\"\n      ],\n      \"displayName\": \"Build ArangoDB Enterprise Edition (ASAN and UBSAN Build)\"\n    },\n    {\n      \"name\": \"community-developer-coverage\",\n      \"inherits\": [\n        \"gcov\",\n        \"community-developer\"\n      ],\n      \"displayName\": \"Build ArangoDB Community Edition (Coverage Build)\"\n    },\n    {\n      \"name\": \"enterprise-developer-coverage\",\n      \"inherits\": [\n        \"gcov\",\n        \"enterprise-developer\"\n      ],\n      \"displayName\": \"Build ArangoDB Enterprise Edition (Coverage Build)\"\n    }\n  ],\n  \"buildPresets\": [\n    {\n      \"name\": \"community\",\n      \"configurePreset\": \"community\"\n    },\n    {\n      \"name\": \"enterprise\",\n      \"configurePreset\": \"enterprise\"\n    },\n    {\n      \"name\": \"community-developer\",\n      \"configurePreset\": \"community-developer\"\n    },\n    {\n      \"name\": \"enterprise-developer\",\n      \"configurePreset\": \"enterprise-developer\"\n    },\n    {\n      \"name\": \"community-pr\",\n      \"configurePreset\": \"community-pr\"\n    },\n    {\n      \"name\": \"community-pr-no-v8\",\n      \"configurePreset\": \"community-pr-no-v8\"\n    },\n    {\n      \"name\": \"community-pr-non-maintainer\",\n      \"configurePreset\": \"community-pr-non-maintainer\"\n    },\n    {\n      \"name\": \"enterprise-pr\",\n      \"configurePreset\": \"enterprise-pr\"\n    },\n    {\n      \"name\": \"enterprise-pr-no-v8\",\n      \"configurePreset\": \"enterprise-pr-no-v8\"\n    },\n    {\n      \"name\": \"enterprise-pr-non-maintainer\",\n      \"configurePreset\": \"enterprise-pr-non-maintainer\"\n    },\n    {\n      \"name\": \"community-pr-arm\",\n      \"configurePreset\": \"community-pr-arm\"\n    },\n    {\n      \"name\": \"community-developer-tsan\",\n      \"configurePreset\": \"community-developer-tsan\"\n    },\n    {\n      \"name\": \"community-pr-tsan\",\n      \"configurePreset\": \"community-pr-tsan\"\n    },\n    {\n      \"name\": \"enterprise-developer-tsan\",\n      \"configurePreset\": \"enterprise-developer-tsan\"\n    },\n    {\n      \"name\": \"enterprise-pr-tsan\",\n      \"configurePreset\": \"enterprise-pr-tsan\"\n    },\n    {\n      \"name\": \"community-developer-alubsan\",\n      \"configurePreset\": \"community-developer-alubsan\"\n    },\n    {\n      \"name\": \"community-pr-alubsan\",\n      \"configurePreset\": \"community-pr-alubsan\"\n    },\n    {\n      \"name\": \"enterprise-developer-alubsan\",\n      \"configurePreset\": \"enterprise-developer-alubsan\"\n    },\n    {\n      \"name\": \"enterprise-pr-alubsan\",\n      \"configurePreset\": \"enterprise-pr-alubsan\"\n    },\n    {\n      \"name\": \"community-developer-coverage\",\n      \"configurePreset\": \"community-developer-coverage\"\n    },\n    {\n      \"name\": \"enterprise-developer-coverage\",\n      \"configurePreset\": \"enterprise-developer-coverage\"\n    }\n  ],\n  \"testPresets\": [\n    {\n      \"name\": \"enterprise-developer\",\n      \"configurePreset\": \"enterprise-developer\"\n    },\n    {\n      \"name\": \"community-developer\",\n      \"configurePreset\": \"community-developer\"\n    },\n    {\n      \"name\": \"community-pr\",\n      \"configurePreset\": \"community-pr\"\n    },\n    {\n      \"name\": \"enterprise-pr\",\n      \"configurePreset\": \"enterprise-pr\"\n    }\n  ]\n}"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 56.0947265625,
          "content": "# Contributing\n\nWe welcome bug fixes and patches from 3rd party contributors. Please\nsee the [Contributor Agreement](https://www.arangodb.com/community#contribute)\nfor details.\n\nPlease follow these guidelines if you want to contribute to ArangoDB:\n\n# Reporting Bugs\n\nWhen reporting bugs, please use our issue tracker on GitHub. Please make sure\nto include the version number of ArangoDB in your bug report, along with the\nplatform you are using (e.g. `Linux OpenSuSE x86_64`). Please also include the\nArangoDB startup mode (daemon, console, supervisor mode) plus any special\nconfiguration. This will help us reproducing and finding bugs.\n\nPlease also take the time to check there are no similar/identical issues open\nyet.\n\n# Contributing features, documentation, tests\n\n- Create a new branch in your fork, based on the **devel** branch\n\n- Develop and test your modifications there\n\n- Commit as you like, but preferably in logical chunks. Use meaningful commit\n  messages and make sure you do not commit unnecessary files (e.g. object\n  files). It is normally a good idea to reference the issue number from the\n  commit message so the issues will get updated automatically with comments.\n\n- If the modifications change any documented behavior or add new features,\n  document the changes. It should be written in American English.\n  The documentation can be found in [`docs-hugo` repository](https://github.com/arangodb/docs-hugo#readme).\n\n- When done, run the complete test suite and make sure all tests pass.\n\n- When finished, push the changes to your GitHub repository and send a pull\n  request from your fork to the ArangoDB repository. Please make sure to select\n  the appropriate branches there. This will most likely be **devel**.\n\n- You must use the Apache License for your changes and have signed our\n  [CLA](https://arangodb.com/community/#contribute). We cannot accept pull requests\n  from contributors who didn't sign the CLA.\n\n- Please let us know if you plan to work on a ticket. This way we can make sure\n  redundant work is avoided.\n\n# Main sections\n\n- [Source Code](#source-code)\n- [Building](#building)\n- [Running](#running)\n- [Debugging](#debugging)\n  - [Linux Core Dumps](#linux-core-dumps)\n- [Unittests](#unittests)\n  - [invoking driver tests](#driver-tests) via scripts/unittests\n  - [capturing test HTTP communication](#running-tcpdump--windump-for-the-sut) but\n    [forcing communication to use plain-text JSON](#forcing-downgrade-from-vpack-to-json)\n  - [Evaluating previous testruns](#evaluating-json-test-reports-from-previous-testruns)\n    sorting by setup time etc.\n- [What to test where and how](tests/README.md)\n\n## Source Code\n\n### Get the source code\n\nDownload the latest source code with Git, including submodules:\n\n    git clone --recurse-submodules --jobs 8 https://github.com/arangodb/arangodb\n\nThis automatically clones the **devel** branch.\n\nIf you only plan to compile a specific version of ArangoDB locally and do not\nwant to commit changes, you can speed up cloning substantially by using the\n`--depth` and `--shallow-submodules` parameters and specifying a branch, tag,\nor commit hash using the `--branch` parameter for the clone command as follows:\n\n    git clone --depth 1 --recurse-submodules --shallow-submodules --jobs 8 --branch v3.12.0 https://github.com/arangodb/arangodb\n\nOn Windows, Git's automatic conversion to CRLF line endings needs to be disabled.\nOtherwise, [compiling](#building-the-binaries) in the Linux build container fails.\nYou can use `git clone --config core.autocrlf=input ...` so that it only affects\nthis working copy.\n\n### Git Setup\n\nSetting up git for automatically merging certain automatically generated files\nin the ArangoDB source tree:\n\n    git config --global merge.ours.driver true\n\n### Style Guide\n\nWe use `clang-format` to enforce consistent code formatting. Check \n[STYLEGUIDE.md](STYLEGUIDE.md) for a comprehensive description of ArangoDB's \nCoding Guidelines.\n\n### Compiler support policy\n\nWe support only officially released compiler versions which major version is\nat least 9 month old.\nArangoDB does not guarantee successful compilation with arbitrary compilers, \nbut uses a certain compiler make and version for building ArangoDB. Both the\ncompiler make and version can change over time.\n\n### Unique Log Ids\n\nWe have unique log ids in order to allow for easy locating of code producing\nerrors.\n\n    LOG_TOPIC(\"2dead\", ....)\n\nTo ensure that the ids are unique we run the script `./utils/checkLogIds.py`\nduring CI runs. The script will fail with a non-zero status if id collisions are\nfound. You can use `openssl rand -hex 3 | sed 's/.//;s/\\(.*\\)/\"\\1\"/'` or\nanything that suits you to generate a **5 hex digit log** id.\n\n### ESLint\n\nUse:\n\n    ./utils/giteslint.sh\n\nto lint your modified files.\n\n    ./utils/eslint.sh\n\nto find out whether all of your files comply to eslint. This is required to\nmake continuous integration work smoothly.\n\nIf you want to add new files / patterns to this make target, edit the respective\nshell scripts.\n\nTo be safe from committing non-linted stuff add **.git/hooks/pre-commit** with:\n\n    ./utils/eslint.sh\n\n### Adding startup options\n\nStartup option example with explanations:\n\n```cpp\noptions->\n    addOption(\"--section.option-name\",\n              \"A brief description of the startup option, maybe over multiple \"\n              \"lines, with an upper-case first letter and ending with a .\",\n              // for numeric options, you may need to indicate the unit like \"timeout (in seconds)\"\n                new XyzParameter(&_xyzVariable),\n                arangodb::options::makeFlags(\n                    arangodb::options::Flags::DefaultNoComponents,\n                    arangodb::options::Flags::OnCoordinator, // in a cluster, it only has an effect on Coordinators\n                    arangodb::options::Flags::OnSingle,      // supported in single server mode, too\n                    arangodb::options::Flags::Enterprise,    // only available in the Enterprise Edition\n                    arangodb::options::Flags::Uncommon))     // don't show with --help but only --help-all or --help-<section>\n    .setIntroducedIn(30906) // format XYYZZ, X = major, YY = minor, ZZ = bugfix version\n    .setIntroducedIn(31002) // list all versions the feature is added in but exclude versions that are implied, e.g. 31100\n    .setLongDescription(R\"(You can optionally add details here. They are only\nshown in the online documentation (and --dump-options). \n\nThe _text_ is interpreted as **Markdown**, allowing formatting like\n`inline code`, fenced code blocks, and even tables.)\");\n```\n\nSee [`lib/ProgramOptions/Option.h`](lib/ProgramOptions/Option.h) for details.\n\nFor a feature that is added to v3.9.6, v3.10.2, and devel, you only need to set\n`.setIntroducedIn()` for 3.9 and 3.10 but not 3.11 (in devel) because all later\nversions (after v3.10.2) can reasonably be expected to include the feature, too.\nLeaving v3.10.2 out would be unclear, however, because users may assume to find\nthe feature in v3.10.0 and v3.10.1 if it was added to v3.9.6.\n\nIn the 3.9 branch, you should only set the versions up to 3.9 but not 3.10,\npretending no later version exists to avoid additional maintenance burdens.\n\nIn 3.9:\n\n```cpp\n    .setIntroducedIn(30906)\n```\n\nIn 3.10 and later:\n\n```cpp\n    .setIntroducedIn(30906)\n    .setIntroducedIn(31002)\n```\n\nThese version remarks can be removed again when all of the listed versions for\nan option are obsolete (and removed from the online documentation).\n\nNote that `.setDeprecatedIn()` should not be removed until the startup option is\nobsoleted or fully removed.\n\n### Adding metrics\n\nAs of 3.8 we have enforced documentation for metrics. This works as\nfollows. Every metric which is generated has a name. The metric must be\ndeclared by using one of the macros\n\n```\n  DECLARE_COUNTER(name, helpstring);\n  DECLARE_GAUGE(name, type, helpstring);\n  DECLARE_HISTOGRAM(name, scaletype, helpstring);\n```\n\nin some `.cpp` file (please put only one on a line). Then, when the\nmetric is actually requested in the source code, it gets a template\nadded to the metrics feature with the `add` method and its name.\nLabels can be added with the `withLabel` method. In this way, the\ncompiler ensures that the metric declaration is actually there if\nthe metric is used.\n\nThen there is a helper script `utils/generateAllMetricsDocumentation.py`\nwhich needs `python3` with the `yaml` module. It will check and do the\nfollowing things:\n\n- Every declared metric in some `.cpp` file in the source has a\n  corresponding documentation snippet in `Documentation/Metrics`\n  under the name of the metric with `.yaml` appended\n- Each such file is a YAML file of a certain format (see template\n  under `Documentation/Metrics/template.yaml`)\n- Many of the components are required, so please provide adequate\n  information about your metric\n- Make sure to set `introducedIn` to the version the metric is added to the\n  current branch's version (e.g. `\"3.9.6\"` in the 3.9 branch, `\"3.10.2\"` in\n  the 3.10 and devel branches)\n- The script can also assemble all these YAML documentation snippets\n  into a single file under `Documentation/Metrics/allMetrics.yaml`,\n  the format is again a structured YAML file which can easily be\n  processed by the documentation tools, this is only needed when\n  we update the documentation web pages.\n\nPlease, if you have added or modified a metric, make sure to declare\nthe metric as shown above and add a documentation YAML snippet in the\ncorrect format. Afterwards, run\n\n    utils/generateAllMetricsDocumentation.py\n\nbut do not include `Documentation/Metrics/allMetrics.yaml` in your PR\n(as was a previous policy). The file is only generated if you add\n`-d` as command line option to the script.\n\n---\n\n## Building\n\nNote: Make sure that your source path does not contain spaces. Otherwise, the\nbuild process will likely fail.\n\n### Building the binaries\n\nTo compile ArangoDB 3.12 or later, you can use a Docker build container that\nis based on Ubuntu and includes the necessary toolchain. You need Git on the\nhost system to [get the source code](#get-the-source-code) and mount it into the\ncontainer, however.\n\nCheck the [`.circleci/base_config.yml`](.circleci/base_config.yml) file for the\ndefault `build-docker-image` value to ensure it's the matching image tag for the\nversion you want to build.\n\nYou can start the build container as follows in a Bash-like terminal:\n\n    cd arangodb\n    docker run -it -v $(pwd):/root/project -p 3000:3000 arangodb/ubuntubuildarangodb-devel:3\n\nIn PowerShell, use `${pwd}` instead of `$(pwd)`.\n\nThe port mapping is for accessing the web interface in case you want to run a\ndevelopment server for working on the frontend.\n\nIn the fish shell of the container, run the following commands for a release-like\nbuild using the officially supported version of the Clang compiler\n(also see the [`VERSIONS`](VERSIONS)):\n\n    cd /root/project\n    cmake --preset community -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_EXE_LINKER_FLAGS=\"-fuse-ld=lld\" -DCMAKE_LIBRARY_PATH=$OPENSSL_ROOT_DIR/lib -DOPENSSL_ROOT_DIR=/opt\n    cmake --build --preset community --parallel (nproc)\n\nFor a debug build, you can use the `community-developer` preset instead.\n\nThe default target CPU architecture is `sandybridge` for the x86-64 platform and\n`graviton1` for 64-bit ARM chips. You can specify a different architecture to\nenable more optimizations or to limit the compiler to the available instructions\nfor a particular architecture:\n\n    cmake --preset community -DTARGET_ARCHITECTURE=native ...\n\nOn ARM, disable the `USE_LIBUNWIND` option:\n\n    cmake --preset community -DUSE_LIBUNWIND=Off ...\n\nTo build specific targets instead of all, you can specify them like shown below:\n\n    cmake --build --preset community --target arangod arangosh arangodump arangorestore arangoimport arangoexport arangobench arangovpack frontend\n\nYou can find the output files at `build-presets/<preset-name>/`,\nwith the compiled executables in the `bin/` subfolder.\n\nTo collect all files for packaging, you can run the following command (with\n`<preset-name>` substituted, e.g. with `community`):\n\n    cmake --install ./build-presets/<preset-name> --prefix install/usr\n\nIt copies the needed files to a folder called `install`.\nIt may fail if you only built a subset of the targets.\nThe folder is organized following the Linux directory structure, with the\nserver executable at `usr/sbin/arangod`, the tools in `usr/bin/`, the\nconfiguration files in `usr/etc/arangodb3/`, and so on.\n\nFor building the ArangoDB Starter (`arangodb` executable), see the\n[ArangoDB Starter repository](https://github.com/arangodb-helper/arangodb).\n\n### Building the Web Interface\n\nThe web interface is also known as the Web UI, frontend, or _Aardvark_.\nBuilding ArangoDB also builds the frontend unless `-DUSE_FRONTEND=Off` was\nspecified when configuring CMake or if the `frontend` target was left out for\nthe build.\n\nTo build the web interface via CMake, you can run the following commands:\n\n    cmake --preset community\n    cmake --build --preset community --target frontend\n\nTo remove all installed Node.js modules and start a clean installation, run:\n\n    cmake --build --preset community --target frontend_clean\n\nYou can also build the frontend manually without CMake.\n[Node.js](https://nodejs.org/) and [Yarn](https://yarnpkg.com/) need to be\ninstalled.\n\n    cd <SourceRoot>/js/apps/system/_admin/aardvark/APP/react\n    yarn install\n    yarn build\n\nTo run a development server, go to `js/apps/system/_admin/aardvark/APP/react`\nand run the following command:\n\n    yarn start\n\nIt should start your browser and open the web interface at `http://localhost:3000`.\nChanges to any frontend source files trigger an automatic re-build and reload\nthe browser tab. If you use the build container, make sure to start it with a\nport mapping for the development server.\n\n#### Cross Origin Policy (CORS) error\n\nOur front-end development server currently runs on port:`3000`, while the backend\nruns on port:`8529` respectively. This implies that when the front-end sends a\nrequest to the backend would result in Cross-Origin-Policy security checks which\nrecently got enforced by some browsers for security reasons. Until recently, we\nnever had reports of CORS errors when running both the backend and front-end dev\nservers independently, however, we recently confirmed that this error occurs in\n(Chrome version 98.0.4758.102 and Firefox version 96.0.1).\n\nIn case you run into CORS errors while running the development server, here is a quick fix:\n\nSimply stop your already running backend process with the command below:\n\n```bash\ncmd + c\n```\n\nThen restart `arangodb` server with the command below:\n\n```bash\n./build-presets/<preset-name>/bin/arangod ../Arango --http.trusted-origin '*'\n```\n\nNote:\n\na: `./build-presets/<preset-name>/bin/arangod`: represents the location of `arangodb` binaries in your machine.\n\nb: `../Arango`: is the database directory where the data will be stored.\n\nc: `--http.trusted-origin '*'` the prefix that allows cross-origin requests.\n\n#### NPM Dependencies\n\nTo add new NPM dependencies, switch into the `js/node` folder and install them\nwith npm using the following options:\n\n`npm install [<@scope>/]<name> --global-style --save --save-exact`\n\nor simply\n\n`npm install [<@scope>/]<name> --global-style -s -E`\n\nThe `save` and `save-exact` options are necessary to make sure the\n`package.json` file is updated correctly.\n\nThe `global-style` option prevents newer versions of npm from unrolling nested\ndependencies inside the `node_modules` folder. Omitting this option results in\nexposing _all_ dependencies of _all_ modules to ArangoDB users.\n\nFinally add the module's licensing information to\n`LICENSES-OTHER-COMPONENTS.md`.\n\nIf you need to make adjustments/modifications to dependencies or replace\ntransitive dependencies with mocks, make sure to run `npx patch-package $dependencyName`\nin the `js/node` folder and commit the resulting patch file in `js/node/patches`.\nThis will ensure the changes are persisted if the dependency is overwritten by `npm`\nin the future.\n\nFor example to commit a patch for the transitive dependency `is-wsl` of the dependency\n`node-netstat`, make your changes in `js/node/node_modules/node-netstat/node_modules/is-wsl`\nand then run `npx patch-package node-netstat/is-wsl` in `js/node` and commit the resulting\npatch file in `js/node/patches`.\n\n#### Build the HTTP API documentation for Swagger-UI\n\nThe REST HTTP API of the ArangoDB server is described using the OpenAPI\nspecification (formerly Swagger). The source code is in documentation repository\nat <https://github.com/arangodb/docs-hugo>.\n\nTo build the `api-docs.json` file for viewing the API documentation in the\nSwagger-UI of the web interface (**SUPPORT** section, **Rest API** tab), run\nthe following commands in a terminal:\n\n1. Get a working copy of the documentation content with Git:\n\n   `git clone https://github.com/arangodb/docs-hugo`\n\n2. Enter the `docs-hugo` folder:\n\n   `cd docs-hugo`\n\n3. Optional: Switch to a tag, branch, or commit if you want to build the\n   API documentation for a specific version of the docs:\n   \n   `git checkout <reference>`\n\n4. Enter the folder of the Docker toolchain, `amd64` on the x86-64 architecture\n   and `arm64` on ARM CPUs:\n\n   ```shell\n   cd toolchain/docker/amd64 # x86-64\n   cd toolchain/docker/arm64 # ARM 64-bit\n   ```\n\n5. Set the environment variable `ENV` to any value other than `local` to make\n   the documentation tooling not start a live server in watch mode but rather\n   create and static build and exit:\n\n   ```shell\n   export ENV=static  # Bash\n   set -xg ENV static # Fish\n   $Env:ENV='static'  # PowerShell\n   ```\n\n6. Run Docker Compose using the plain build configuration for the documentation:\n\n   `docker compose -f docker-compose.plain-build.yml up --abort-on-container-exit`\n\n7. When the docs building finishes successfully, you can find the `api-docs.json`\n   files in `site/data/<version>/`.\n\n8. Copy the respective `api-docs.json` file into the ArangoDB working copy or\n   installation folder under `js/apps/system/_admin/aardvark/APP/api-docs.json`\n   and refresh the web interface.\n\n---\n\n## Running\n\n### Temporary files and temp directories\n\nArangoDB tries to locate the temporary directory like this:\n\n- the environment variable `TMPDIR` is evaluated.\n- the startup option `--temp.path` overrules the above system provided settings.\n\nOur testing framework uses this path in the cluster test cases to set an\nenvironment variable `ARANGOTEST_ROOT_DIR` which is global to the running\ncluster, but specific to the current test suite. You can access this as \n`global.instanceManager.rootDir` in Javascript client tests and via the\nenvironment variable on the C++ level.\n\n### Local Cluster Startup\n\nThe scripts `scripts/startLocalCluster` helps you to quickly fire up a testing\ncluster on your local machine. `scripts/shutdownLocalCluster` stops it again.\n\n`scripts/startLocalCluster [numDBServers numCoordinators [mode]]`\n\nWithout arguments it starts 2 DB-Servers and 1 Coordinator in the background,\nrunning on ports 8629, 8630 and 8530 respectively. The Agency runs on port 4001.\n\nMode:\n\n- `C`: Starts the first Coordinator with `--console` in a separate window\n  (using an `xterm`).\n- `D`: Starts all DB-Servers in the GNU debugger in separate windows\n  (using `xterm`s). Hit _ENTER_ in the original terminal where the script\n  runs to continue once all processes have been started up in the debugger.\n\n---\n\n## Debugging\n\n### Runtime\n\n- start arangod with `--console` to get a debug console\n- Cheapen startup for valgrind: `--server.rest-server false`\n\n### Startup\n\nArangod has a startup rc file: `~/.arangod.rc`. It's evaled as JavaScript. A\nsample version to help working with the arangod rescue console may look like\nthat:\n\n    internal = require(\"internal\");\n    fs = require(\"fs\");\n    db = internal.db;\n    time = internal.time;\n    timed = function (cb) {\n      let s = time();\n      cb();\n      return time() - s;\n    };\n    print = internal.print;\n\n_Hint_: You shouldn't lean on these variables in your Foxx services.\n\n### Debugging AQL execution blocks\n\nTo debug AQL execution blocks, two steps are required:\n\n- turn on logging for queries using `--extraArgs:log.level queries=info`\n- divert this facilities log output into individual files: `--extraArgs --log.output queries file://@ARANGODB_SERVER_DIR@/arangod_queries.log`\n- send queries enabling block debugging: `db._query('RETURN 1', {}, { profile: 4 })`\n\nYou now will get log entries with the contents being passed between the blocks.\n\n### Crashes\n\nThe Linux builds of the arangod executable contain a built-in crash handler\n(introduced in v3.7.0).\nThe crash handler is supposed to log basic crash information to the ArangoDB logfile in\ncase the arangod process receives one of the signals SIGSEGV, SIGBUS, SIGILL, SIGFPE or\nSIGABRT. SIGKILL signals, which the operating system can send to a process in case of OOM\n(out of memory), are not interceptable and thus cannot be intercepted by the crash handler,\n\nIn case the crash handler receives one of the mentioned interceptable signals, it will\nwrite basic crash information to the logfile and a backtrace of the call site.\nThe backtrace can be provided to the ArangoDB support for further inspection. Note that\nbacktraces are only usable if debug symbols for ArangoDB have been installed as well.\n\nAfter logging the crash information, the crash handler will execute the default action for\nthe signal it has caught. If core dumps are enabled, the default action for these signals\nis to generate a core file. If core dumps are not enabled, the crash handler will simply\nterminate the program with a non-zero exit code.\n\nThe crash handler can be disabled at server start by setting the environment variable\n`ARANGODB_OVERRIDE_CRASH_HANDLER` to `0` or `off`.\n\nAn example log output from the crash handler looks like this:\n\n```\n2020-05-26T23:26:10Z [16657] FATAL [a7902] {crash} ArangoDB 3.7.1-devel enterprise [linux], thread 22 [Console] caught unexpected signal 11 (SIGSEGV) accessing address 0x0000000000000000: signal handler invoked\n2020-05-26T23:26:10Z [16657] INFO [308c3] {crash} frame 1 [0x00007f9124e93ece]: _ZN12_GLOBAL__N_112crashHandlerEiP9siginfo_tPv (+0x000000000000002e)\n2020-05-26T23:26:10Z [16657] INFO [308c3] {crash} frame 2 [0x00007f912687bfb2]: sigprocmask (+0x0000000000000021)\n2020-05-26T23:26:10Z [16657] INFO [308c3] {crash} frame 3 [0x00007f9123e08024]: _ZN8arangodb3aql10Expression23executeSimpleExpressionEPKNS0_7AstNodeEPNS_11transaction7MethodsERbb (+0x00000000000001c4)\n2020-05-26T23:26:10Z [16657] INFO [308c3] {crash} frame 4 [0x00007f9123e08314]: _ZN8arangodb3aql10Expression7executeEPNS0_17ExpressionContextERb (+0x0000000000000064)\n2020-05-26T23:26:10Z [16657] INFO [308c3] {crash} frame 5 [0x00007f9123feaab2]: _ZN8arangodb3aql19CalculationExecutorILNS0_15CalculationTypeE0EE12doEvaluationERNS0_15InputAqlItemRowERNS0_16OutputAqlItemRowE (+0x0000000000000062)\n2020-05-26T23:26:10Z [16657] INFO [308c3] {crash} frame 6 [0x00007f9123feae85]: _ZN8arangodb3aql19CalculationExecutorILNS0_15CalculationTypeE0EE11produceRowsERNS0_22AqlItemBlockInputRangeERNS0_16OutputAqlItemRowE (+0x00000000000000f5)\n...\n2020-05-26T23:26:10Z [16657] INFO [308c3] {crash} frame 31 [0x000018820ffc6d91]: *no symbol name available for this frame\n2020-05-26T23:26:10Z [16657] INFO [ded81] {crash} available physical memory: 41721995264, rss usage: 294256640, vsz usage: 1217839104, threads: 46\nSegmentation fault (core dumped)\n```\n\nThe first line of the crash output will contain the cause of the crash (SIGSEGV in\nthis case). The following lines contain information about the stack frames. The\nhexadecimal value presented for each frame is the instruction pointer, and if debug\nsymbols are installed, there will be name information about the called procedures (in\nmangled format) plus the offsets into the procedures. If no debug symbols are\ninstalled, symbol names and offsets cannot be shown for the stack frames.\n\n### Memory profiling\n\nStarting in 3.6 we have support for heap profiling when using jemalloc on Linux.\nHere is how it is used:\n\nUse this `cmake` option in addition to the normal options:\n\n```\n-DUSE_JEMALLOC_PROF\n```\n\nwhen building. Make sure you are using the builtin `jemalloc`. You need\ndebugging symbols to analyze the heap dumps later on, so compile with\n`-DCMAKE_BUILD_TYPE=Debug` or `RelWithDebInfo`. `Debug` is probably\nless confusing in the end.\n\nThen set the environment variable for each instance you want to profile:\n\n```\nexport MALLOC_CONF=\"prof:true\"\n```\n\nWhen this is done, `jemalloc` internally does some profiling. You can\nthen use this endpoint to get a heap dump:\n\n```\ncurl \"http://localhost:8529/_admin/status?memory=true\" > heap.dump\n```\n\nYou can analyze such a heap dump with the `jeprof` tool, either by\ninspecting a single dump like this:\n\n```\njeprof build/bin/arangod heap.dump\n```\n\nor compare two dumps which were done on the same process to analyze\nthe difference:\n\n```\njeprof build/bin/arangod --base=heap.dump heap.dump2\n```\n\nSo far, we have mostly used the `web` command to open a picture in a\nbrowser. It produces an `svg` file.\n\nThe tool isn't perfect, but can often point towards the place which\nproduces for example a memory leak or a lot of memory usage.\n\nThis is known to work under Linux and the `jeprof` tool comes with the\n`libjemalloc-dev` package on Ubuntu.\n\nUsing it with the integration tests is possible; snapshots will be taken before\nthe first and after each subsequent testcase executed.\n\n### Core Dumps\n\nA core dump consists of the recorded state of the working memory of a process at\na specific time. Such a file can be created on a program crash to analyze the\ncause of the unexpected termination in a debugger.\n\n#### Linux Core Dumps\n\n##### Linux Core Dump Generation\n\nGenerally core dumps have to be enabled using:\n\n     ulimit -c unlimited\n\nYou should then see:\n\n     ulimit -a\n     core file size          (blocks, -c) unlimited\n\nfor each shell and its subsequent processes.\n\n_Hint_: on Ubuntu the `apport` package may interfere with this; however you may\nuse the `systemd-coredump` package which automates much of the following:\n\nSo that the unit testing framework can autorun gdb it needs to reliably find the\ncorefiles. In Linux this is configured via the `/proc` filesystem, you can make\nthis reboot permanent by creating the file `/etc/sysctl.d/corepattern.conf` (or\nadd the following lines to `/etc/sysctl.conf`)\n\n    # We want core files to be located in a central location\n    # and know the PID plus the process name for later use.\n    kernel.core_uses_pid = 1\n    kernel.core_pattern =  /var/tmp/core-%e-%p-%t\n\nto reload the above settings most systems support:\n\n    sudo sysctl -p\n\nNote that the `proc` paths translate sub-directories to dots. The non permanent\nway of doing this in a running system is:\n\n    echo 1 > /proc/sys/kernel/core_uses_pid\n    echo '/var/tmp/core-%e-%p-%t' > /proc/sys/kernel/core_pattern\n\n(you may also inspect these files to validate the current settings)\n\nMore modern systems facilitate\n[`systemd-coredump`](https://www.freedesktop.org/software/systemd/man/systemd-coredump.html)\n(via a similar named package) to control core dumps. On most systems it will\nput compressed core dumps to `/var/lib/systemd/coredump`.\n\nIn order to use automatic core dump analysis with the unittests you need to\nconfigure `/etc/systemd/coredump.conf` and set `Compress=no` - so instant\nanalysis may take place.\n\nPlease note that we can't support\n[Ubuntu Apport](https://wiki.ubuntu.com/Apport). Please use `apport-unpack` to\nsend us the bare core dumps.\n\nIn order to get core dumps from binaries changing their UID the system needs to\nbe told that its allowed to write cores from them. Default ArangoDB\ninstallations will do exactly that, so the following is necessary to make the\nsystem produce core dumps from production ArangoDB instances:\n\nEdit `/etc/security/limits.conf` to contain:\n\n```\narangodb        -       core            infinity\n```\n\nEdit the systemd unit file `/lib/systemd/system/arangodb3.service` (USE infinity!!!):\n\n```\n## setting for core files\n# Any dir that is writable by the user running arangod\nWorkingDirectory=/var/lib/arangodb3\n# core limit - set this to infinity to enable cores\nLimitCORE=0\n```\n\nEnable new systemd settings:\n\n`systemctl daemon-reload && systemctl restart arangodb3.service`\n\nEnable suid process dumping:\n\n`echo 1 >/proc/sys/fs/suid_dumpable`\n\nMake the above change permanent:\n\n`echo \"sys.fs.suid_dumpable = 1\" >> /etc/sysctl.d/99-suid-coredump.conf`\n\n**Please note that at least GDB 9 is required.**\n\nYou can also generate core dumps from running processes without killing them by\nusing gdb:\n\n    # sleep 100000 &\n    [2] 6942\n    # gdb /bin/sleep 6942\n    ...\n    0x00007faaa7abd4e4 in __GI___nanosleep (requested_time=0x7ffd047c9940, remaining=0x0) at ../sysdeps/unix/sysv/linux/nanosleep.c:28\n    gdb> gcore\n    Saved corefile core.6942\n    gdb> quit\n    Detaching from program: /bin/sleep, process 6942\n    # ls -l core*\n    -rw-r--r--  1 me users  352664 Nov 27 10:48  core.6942\n\n##### Installing GDB 8 on RedHat7 or Centos7\n\nRedHat7 and Centos7 have a package called `devtoolset-7` which contains a\ncomplete set of relative modern development tools. It can be installed by running\n\n```\nsudo yum install devtoolset-7\n```\n\nThese will be installed under some path under `/opt`. To actually use these\ntools, run\n\n```\nscl enable devtoolset-7 bash\n```\n\nto start a shell in which these are used. For example, you can pull a core dump\nwith `gcore` using this version, even for ArangoDB >= 3.4.\n\n##### Analyzing Core Dumps on Linux\n\nWe offer debug packages containing the debug symbols for your binaries. Please\ninstall them if you didn't compile yourselves.\n\nGiven you saw in the log of the arangod with the PID `25216` that it died, you\nshould then find `/var/tmp/core-V8 WorkerThread-25216-1490887259` with this\ninformation. We may now start GDB and inspect whats going on:\n\n    gdb /usr/sbin/arangod /var/tmp/*25216*\n\nThese commands give useful information about the incident:\n\n    backtrace full\n    thread apply all bt\n\nThe first gives the full stacktrace including variables of the last active\nthread, the later one the stacktraces of all threads.\n\n## Unittests\n\n### Dependencies\n\n- _Google Test_ (compile time, shipped in the 3rdParty directory)\n\n### Folder Locations\n\nThere are several major places where unittests live:\n\n| Path / File                                                  | Description                                                                                                                        |\n| :----------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------- |\n| `tests/js/server/`                                           | JavaScript tests, runnable on the server                                                                                           |\n| `tests/js/common/`                                           | JavaScript tests, runnable on the server & via arangosh                                                                            |\n| `tests/js/common/test-data/`                                 | Mock data used for the JavaScript tests                                                                                            |\n| `tests/js/client/`                                           | JavaScript tests, runnable via arangosh                                                                                            |\n| `tests/` (remaining)                                         | Google Test unittests                                                                                                              |\n| implementation specific files                                |\n| `scripts/unittest`                                           | Entry wrapper script for `UnitTests/unittest.js`                                                                                   |\n\n                                          |\nsee [js/client/modules/@arangodb/README.md] about implementation details of the framework.\n\n### Filename conventions\n\nSpecial patterns in the test filenames are used to select tests to be executed\nor skipped depending on parameters:\n\n| Substring                | Description                                                                                                                                                                                                                                                                                                                                                                                   |\n| :----------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `-cluster`               | These tests will only run if clustering is tested (option 'cluster' needs to be true).                                                                                                                                                                                                                                                                                                        |\n| `-noncluster`            | These tests will only run if no cluster is used (option 'cluster' needs to be false)                                                                                                                                                                                                                                                                                                          |\n| `-noinstr_or_noncluster` | These tests will not be ran if instrumented binaries are used and we are running in cluster mode                                                                                                                                                                                                                                                                                              |\n| `-noasan`                | These tests will not be ran if *san instrumented binaries are used                                                                                                                                                                                                                                                                                                                            |\n|  `-noinstr`              | These tests will not be ran if instrumented binaries are used, be it *san or gcov                                                                                                                                                                                                                                                                                                             | \n|  `-nocov`                | These tests will not be ran if gcov instrumented binaries are used.                                                                                                                                                                                                                                                                                                                           | \n|  `-fp`                   | These tests will only be ran if failurepoints are enabled while building the binaries to be used in the tests                                                                                                                                                                                                                                                                                 |\n|  `-r2`                   | These tests will not be ran if --replicationVersion 1 (default) is specified for an ArangoDB deployment.                                                                                                                                                                                                                                                                                      |\n| `-timecritical`          | These tests are critical to execution time - and thus may fail if arangod is to slow. This may happen i.e. if you run the tests in valgrind, so you want to avoid them since they will fail anyways. To skip them, set the option `skipTimeCritical` to _true_.                                                                                                                               |\n| `-spec`                  | These tests are run using the mocha framework instead of jsunity.                                                                                                                                                                                                                                                                                                                             |\n| `-nightly`               | These tests produce a certain thread on infrastructure or the test system, and therefore should only be executed once per day.                                                                                                                                                                                                                                                                |\n| `-grey`                  | These tests are currently listed as \"grey\", which means that they are known to be unstable or broken. These tests will not be executed by the testing framework if the option `--skipGrey` is given. If `--onlyGrey` option is given then non-\"grey\" tests are skipped. See `tests/Greylist.txt` for up-to-date information about greylisted tests. Please help to keep this file up to date. |\n\n### JavaScript Framework\n\nSince several testing technologies are utilized, and different ArangoDB startup\noptions may be required (even different compilation options may be required) the\nframework is split into testsuites.\n\nGet a list of the available testsuites and options by invoking:\n\n    ./scripts/unittest\n\nTo locate the suite(s) associated with a specific test file use:\n\n    ./scripts/unittest find --test tests/js/client/shell/api/aqlfunction.js\n\nRun all suite(s) associated with a specific test file in single server mode:\n\n    ./scripts/unittest auto --test tests/js/client/shell/api/aqlfunction.js\n\nRun all suite(s) associated with a specific test file in cluster mode:\n\n    ./scripts/unittest auto --cluster true --test tests/js/client/shell/api/aqlfunction.js\n\nRun all C++ based Google Test (gtest) tests using the `arangodbtests` binary:\n\n    ./scripts/unittest gtest\n\nRun specific gtest tests:\n\n    ./scripts/unittest gtest --testCase \"IResearchDocumentTest.*:*ReturnExecutor*\"\n    # equivalent to:\n    ./build/bin/arangodbtests --gtest_filter=\"IResearchDocumentTest.*:*ReturnExecutor*\"\n\nControlling the place where the test-data is stored:\n\n    TMPDIR=/some/other/path ./scripts/unittest shell_client_aql\n\nNote that the `arangodbtests` executable is not compiled and shipped for\nproduction releases (`-DUSE_GOOGLE_TESTS=off`).\n\n`scripts/unittest` is only a wrapper for the most part, the backend\nfunctionality lives in `js/client/modules/@arangodb/testutils`.\nThe actual testsuites are located in the\n`js/client/modules/@arangodb/testsuites` folder.\n\n#### Passing Options\n\nThe first parameter chooses the facility to execute. Available choices include:\n\n- **shell_client**\n- **shell_api**\n- **shell_api_multi**\n- **shell_client_aql**\n- **shell_client_multi**\n\nDifferent facilities may take different options. The above mentioned usage\noutput contains the full detail.\n\nInstead of starting its own instance, `unittest` can also make use of a\npreviously started arangod instance. You can launch the instance as you want\nincluding via a debugger or `rr` and prepare it for what you want to test with\nit. You then launch the test on it like this (assuming the default endpoint):\n\n    ./scripts/unittest shell_client --server tcp://127.0.0.1:8529/\n\nA commandline for running a single test (-> with the facility 'single_server')\nusing valgrind could look like this. Options are passed as regular long values\nin the syntax --option value --sub:option value. Using Valgrind could look like\nthis:\n\n    ./scripts/unittest shell_client --test tests/js/server/aql/aql-escaping.js \\\n      --cluster true \\\n      --extraArgs:server.threads 1 \\\n      --extraArgs:scheduler.threads 1 \\\n      --extraArgs:javascript.gc-frequency 1000000 \\\n      --extraArgs:javascript.gc-interval 65536 \\\n      --extraArgs:agent.log.level trace \\\n      --extraArgs:log.level request=debug \\\n      --extraArgs:log.force-direct true \\\n      --javascript.v8-contexts 2 \\\n      --valgrind /usr/bin/valgrind \\\n      --valgrindargs:log-file /tmp/valgrindlog.%p\n\n- We specify the test to execute\n- We specify some arangod arguments via --extraArgs which increase the server performance\n- We specify to run using valgrind (this is supported by all facilities)\n- We specify some valgrind commandline arguments\n- We set the log levels for agents to `trace` (the Iinstance type can be specified by:\n  - `single`\n  - `agent`\n  - `dbserver`\n  - `coordinator`\n  )\n- We set the `requests` log level to debug on all instances\n- We force the logging not to happen asynchronous\n- Eventually you may still add temporary `console.log()` statements to tests you debug.\n\n#### Running a Single Unittest Suite\n\nTesting a single test with the framework directly from the arangosh:\n\n    scripts/unittest shell_client --test tests/js/client/shell/shell-client.js\n\nYou can also only execute a filtered test case in a jsunity/mocha/gtest test\nsuite (in this case `testTokens`):\n\n    scripts/unittest shell_client_aql --test tests/js/client/aql/aql-escaping.js --testCase testTokens\n\n    scripts/unittest shell_client --test shell-util-spec.js --testCase zip\n\n    scripts/unittest gtest --testCase \"IResearchDocumentTest.*\"\n\nRunning a test against a server you started (instead of letting the script start its own server):\n\n    scripts/unittest shell_api --test analyzer.js --server tcp://127.0.0.1:8529 --serverRoot /tmp/123\n\nRe-running previously failed tests:\n\n    scripts/unittest <args> --failed\n\nSpecifying a `--test ` filter containing `-cluster` will implicitly set `--cluster true` and launch a cluster test.\n\nThe `<args>` should be the same as in the previous run, only `--test`/`--testCase` can be omitted.\nThe information which tests failed is taken from the `UNITTEST_RESULT.json` in your test output folder.\nThis failed filter should work for all jsunity and mocha tests.\n\n#### Running several Suites in one go\n\nSeveral testsuites can be launched consequently in one run by specifying them as coma separated list.\nThey all share the specified commandline arguments. Individual arguments can be passed as a JSON array.\nThe JSON Array has to contain the same number of elements as testsuites specified. The specified individual\nparameters will overrule global and default values.\n\nRunning the same testsuite twice with different and shared parameters would look like this:\n\n    ./scripts/unittest  shell_client_multi,shell_client_multi --test shell-admin-status.js  --optionsJson '[{\"http2\":true,\"suffix\":\"http2\"},{\"http\":true,\"suffix\":\"http\"}]'\n\n\n#### Running Foxx Tests with a Fake Foxx Repo\n\nSince downloading Foxx apps from GitHub can be cumbersome with shaky DSL and\nDoS'ed GitHub, we can fake it like this:\n\n    export FOXX_BASE_URL=\"http://germany/fakegit/\"\n    ./scripts/unittest single_server --test 'tests/js/server/shell/shell-foxx-manager-spec.js'\n\n### Running jsUnity Tests\n\nAssume that you have a test file containing:\n\n```js\nfunction exampleTestSuite () {\n  return {\n    testSizeOfTestCollection : function () {\n    assertEqual(5, 5);\n  };\n}\n\njsUnity.run(aqlTestSuite);\n\nreturn jsunity.done();\n```\n\nThen you can run the test suite using `jsunity.runTest()`:\n\n```\narangosh> require(\"jsunity\").runTest(\"test.js\");\n2012-01-28T19:10:23Z [10671] INFO Running aqlTestSuite\n2012-01-28T19:10:23Z [10671] INFO 1 test found\n2012-01-28T19:10:23Z [10671] INFO [PASSED] testSizeOfTestCollection\n2012-01-28T19:10:23Z [10671] INFO 1 test passed\n2012-01-28T19:10:23Z [10671] INFO 0 tests failed\n2012-01-28T19:10:23Z [10671] INFO 1 millisecond elapsed\n```\n\n#### Running jsUnity Tests with arangosh client\n\nRun tests this way:\n\n    require(\"jsunity\").runTest(\"tests/js/client/shell/shell-client.js\");\n\nYou can only run tests which are intended to be ran via arangosh.\n\n### Mocha Tests\n\nAll tests with `-spec` in their names are using the\n[mochajs.org](https://mochajs.org) framework. To run those tests, e.g. in the\narangosh, use this:\n\n    require(\"@arangodb/mocha-runner\").runTest('tests/js/client/endpoint-spec.js', true)\n\n### Release tests\n\n[RTA](https://github.com/arangodb/release-test-automation) has some tests to verify production integration.\nTo aid their development, they can also be used from the ArangoDB source tree.\n\n#### MakeData / CheckData suite\n\nThe [makedata framework](https://github.com/arangodb/rta-makedata) as git submodule in [3rdParty/rta-makedata](3rdParty/rta-makedata/)\nis implemented in arangosh javascript.\nIt uses the respective interface to execute DDL and DML operations. \nIt facilitates a per database approach, and can be run multiple times in loops. \nIt has hooks, that are intended to create DDL/DML objects in a way their existence\ncan be revalidated later on by other script hooks. The check hooks must respect a\nflag whether they can create resources or whether they're talking to a read-only source.\nThus, the check-hooks may create data, but must remove it on success.\nThe checks should be considered not as time intense as the creation of the data.\n\nIt is used by RTA to:\n\n- revalidate data can be created\n- data survives hot backup / restore\n- data makes it across the replication\n- data is accessible with clusters with one failing DB-Server\n- data makes it across DC2DC replication\n\nWith this integration additional DDL/DML checks can be more easily be developed without the\nrather time and resource consuming and complex RTA framework.\n\nThe `rta_makedata` testsuite can be invoked with:\n\n- `--cluster false` - to be ran on a single server setup.\n- `--cluster true` to be ran on a 3 db-server node cluster; one run will check resilience with 2 remaining dbservers.\n\nThese combinations are also engaged via [test-definitions.txt](tests/test-definitions.txt).\n\nInvoke it like this:\n\n    ./scripts/unittest rta_makedata --cluster true\n\n(you can override the 3rdParty/rta-makedata with `--rtasource ../rta-makedata` ,if you want to work with a full git clone of RTA-makedata)\n\n### Driver tests\n\n#### Go driver\n\nPre-requisites:\n\n- have a go-driver checkout next to the ArangoDB source tree\n- have the go binary in the path\n\nOnce this is completed, you may run it like this:\n\n    ./scripts/unittest go_driver --gosource ../go-driver/ --testCase View --goOptions:timeout 180m --cluster true\n\nThis will invoke the test with a filter to only execute tests that have `View` in their name.\nAs an additional parameter we pass `-timeout 100m` to the driver test.\n\nThe driver integration also features JWT pass in. It will launch a cluster with 3 DB-Servers, as\nthe tests expect to have at least 3 DB-Servers.\n\n#### Java driver\n\nPre-requisites:\n\n- have a arangodb-java-driver checkout next to the ArangoDB source tree in the 'next' branch\n- have a maven binary in the path (mvn) configured to use JDK 11\n\nYou can check if maven is correctly configured to use JDK 11 executing: `mvn -version`.\nIn case you have multiple JVMs in your machine and JDK 11 is not the default one, you can set\nthe environment variable `JAVA_HOME` to the root path of JDK 11.\n\nOnce this is completed, you may run it like this:\n\n    ./scripts/unittest java_driver --javasource ../arangodb-java-driver/ \\\n        --javaOptions:failIfNoTests false \\\n        --testCase com.arangodb.next.api.collection.CollectionApiTest#countAndDropCollection \\\n        --cluster true\n\nFor possible `javaOptions` see\n[arangodb-java-driver/dev-README.md#test-provided-deployment](https://github.com/arangodb/arangodb-java-driver/blob/master/dev-README.md)\nin the java source, or the\n[surefire documentation](https://maven.apache.org/surefire/maven-surefire-plugin/examples/single-test.html)\n\n#### ArangoJS\n\nPre-requisites:\n\n- have a arangojs checkout next to the ArangoDB source tree\n- have a nodejs and yarn binaries installed and in the path\n- ran `yarn` once in the arangojs source tree\n\nOnce this is completed, you may run it like this:\n\n    ./scripts/unittest js_driver --jssource ../arangojs/ \\\n        --testCase 'kills the given query' \\\n        --cluster true\n\n#### arangodb-php\n\nPre-requisites:\n\n- have a arangodb-php checkout next to the ArangoDB source tree\n- have `php` and `phpunit` binaries installed and in the path\n\nAt the time being phpunit version 6.5 is supported. Install it like this:\n\n    wget \"https://phar.phpunit.de/phpunit-6.5.14.phar\"\n    mv phpunit-6.5.14.phar /usr/bin/phpunit\n    chmod a+x /usr/bin/phpunit\n\nOnce this is completed, you may run it like this:\n\n    ./scripts/unittest php_driver --phpsource ../arangodb-php/ \\\n        --testCase testSaveVerticesAndEdgeBetweenThemAndRemoveOneByOne \\\n        --cluster true \\\n        --phpkeepalive false\n\n(without connection keepalive)\n\n#### generic driver interface\n\nThe generic driver interface expects to find i.e. script inside the\ndriver source, that does all the plumbing to run the respective tests against\nthe provided arangodb instance.\nThe invoked script is expected to exit non-zero on failure.\nAll content of `stdout` will be forwarded to the testresults.\nAll required data is passed as parameters:\n\n- driversource - the source directory with the workingcopy of the driver\n- driverScript - the script to be executed. defaults to `run_tests.sh`\n- driverScriptInterpreter - since currently there is no shebang support,\n  this needs to be provided or defaults to `/bin/bash`.\n- driverOptions options to be passed on to the driver works in the form of\n  `--driverOptions.argname value` evaluating to `--argname` `value`\n- `--test testcase` evaluates to `--testsuite testcase`\n- `--testCase testcaseExp` evaluates to `--filter testcaseExp`\n\nStatically provided options (with sample values):\n\n- `--instanceUrl http://127.0.0.1:7327`\n- `--instanceEndpoint tcp://127.0.0.1:7327`\n- `--port 7327`\n- `--host 127.0.0.1`\n- `--auth false`\n- `--username root`\n- `--password ''`\n- `--[no-]enterprise`\n- `--deployment-mode [SINGLE_SERVER|CLUSTER]`\n\n### Debugging Tests\n\nThis is quick introduction only.\n\nRunning a single arangosh client test:\n\n    ./scripts/unittest shell_client --test api-import.js\n\nDebugging arangosh with gdb:\n\n    server> ./scripts/unittest shell_client --test api-import.js --server tcp://127.0.0.1:7777\n\n    client> gdb --args ./build/bin/arangod --server.endpoint http+tcp://127.0.0.1:6666 \\\n                                           --server.authentication false \\\n                                           --log.level communication=trace \\\n                                           ../arangodb-data-test\n\nDebugging a storage engine:\n\n    host> rm -fr ../arangodb-data-rocksdb/; \\\n       gdb --args ./build/bin/arangod \\\n           --console \\\n           --foxx.queues false \\\n           --server.statistics false \\\n           --server.endpoint http+tcp://0.0.0.0:7777 \\\n           ../arangodb-data-rocksdb\n    (gdb) catch throw\n    (gdb) r\n    arangod> require(\"jsunity\").runTest(\"tests/js/client/shell/shell-client.js\");\n\n### Forcing downgrade from VPack to JSON\n\nWhile velocypack is better for the machine to machine communication, JSON does a better job\nif you want to observe the communication using `tcpdump`.\nHence a downgrade of the communication to JSON can be made at start time:\n\n    arangosh --server.force-json true --server.endpoint ...\n\n### Running tcpdump / windump for the SUT\n\nDon't want to miss a beat of your test? If you want to invoke tcpdump with sudo,\nmake sure that your current shell has sudo enabled. Try like this:\n\n    sudo /bin/true; ./scripts/unittest http_server \\\n      --sniff sudo --cleanup false\n\nThe pcap file will end up in your tests temporary directory. You may need to\npress an additional `ctrl+c` to force stop the sudo'ed tcpdump.\n\n### Evaluating json test reports from previous testruns\n\nAll test results of testruns are dumped to a json file named\n`UNITTEST_RESULT.json` which can be used for later analyzing of timings etc.\n\nCurrently available Analyzers are:\n\n- unitTestPrettyPrintResults - Prints a pretty summary and writes an ASCII representation into `out/testfailures.txt` (if any errors)\n- saveToJunitXML - saves jUnit compatible XML files\n- locateLongRunning - searches the 10 longest running tests from a testsuite; may add comparison to `--otherFile` specified times\n- locateShortServerLife - whether the servers lifetime for the tests isn't at least 10 times as long as startup/shutdown\n- locateLongSetupTeardown - locate tests that may use a lot of time in setup/teardown\n- yaml - dumps the json file as a yaml file\n- unitTestTabularPrintResults - prints a table, add one (or more) of the following columns to print by adding it to `--tableColumns`:\n\n  - `duration` - the time spent in the complete testfile\n  - `status` - success/fail\n  - `failed` - fail?\n  - `total` - the time spent in the testcase\n  - `totalSetUp` - the time spent in setup summarized\n  - `totalTearDown` - the time spent in teardown summarized\n  - `processStats.sum_servers.minorPageFaults` - Delta run values from `/proc/<pid>/io` summarized over all instances\n  - `processStats.sum_servers.majorPageFaults` -\n  - `processStats.sum_servers.userTime` -\n  - `processStats.sum_servers.systemTime` -\n  - `processStats.sum_servers.numberOfThreads` -\n  - `processStats.sum_servers.residentSize` -\n  - `processStats.sum_servers.residentSizePercent` -\n  - `processStats.sum_servers.virtualSize` -\n  - `processStats.sum_servers.rchar` -\n  - `processStats.sum_servers.wchar` -\n  - `processStats.sum_servers.syscr` -\n  - `processStats.sum_servers.syscw` -\n  - `processStats.sum_servers.read_bytes` -\n  - `processStats.sum_servers.write_bytes` -\n  - `processStats.sum_servers.cancelled_write_bytes` -\n  - `processStats.sum_servers.sockstat_sockets_used` - Absolute values from `/proc/<pid>/net/sockstat` summarized over all instances\n  - `processStats.sum_servers.sockstat_TCP_inuse` -\n  - `processStats.sum_servers.sockstat_TCP_orphan` -\n  - `processStats.sum_servers.sockstat_TCP_tw` -\n  - `processStats.sum_servers.sockstat_TCP_alloc` -\n  - `processStats.sum_servers.sockstat_TCP_mem` -\n  - `processStats.sum_servers.sockstat_UDP_inuse` -\n  - `processStats.sum_servers.sockstat_UDP_mem` -\n  - `processStats.sum_servers.sockstat_UDPLITE_inuse` -\n  - `processStats.sum_servers.sockstat_RAW_inuse` -\n  - `processStats.sum_servers.sockstat_FRAG_inuse` -\n  - `processStats.sum_servers.sockstat_FRAG_memory` -\n\n  Process stats are kept by process.\n  So if your DB-Server had the PID `1721882`, you can dial its values by specifying\n  `processStats.1721882_dbserver.sockstat_TCP_tw`\n  into the generated table.\n\ni.e.\n\n    ./scripts/examine_results.js -- 'yaml,locateLongRunning' --readFile out/UNITTEST_RESULT.json\n\nor:\n\n    ./scripts/examine_results.js -- 'unitTestTabularPrintResults' \\\n       --readFile out/UNITTEST_RESULT.json \\\n       --tableColumns 'duration,processStats.sum_servers.sockstat_TCP_orphan,processStats.sum_servers.sockstat_TCP_tw\n\nrevalidating one testcase using jq:\n\n    jq '.shell_client.\"enterprise/tests/js/common/shell/smart-graph-enterprise-cluster.js\"' < \\\n      out/UNITTEST_RESULT.json |grep sockstat_TCP_tw\n\ngetting the PIDs of the server in the testrun using jq:\n\n    jq '.shell_client.\"enterprise/tests/js/common/shell/smart-graph-enterprise-cluster.js\"' < \\\n      out/UNITTEST_RESULT.json |grep '\"[0-9]*_[agent|dbserver|coordinator]'\n    \"1721674_agent\": {\n    \"1721675_agent\": {\n    \"1721676_agent\": {\n    \"1721882_dbserver\": {\n    \"1721883_dbserver\": {\n    \"1721884_dbserver\": {\n    \"1721885_coordinator\": {\n\n### Installing bash completion for `./scripts/unittest`\n\nCalling\n\n```bash\n./scripts/buildUnittestBashCompletion.bash > ~/arango_unittest_comp.bash\n```\n\ngenerates a bash completion script for `./scripts/unittest` which can be sourced\nin your `~/.bashrc` or `~/.bash_profile`:\n\n```bash\n. ~/arango_unittest_comp.bash\n```\n\nYou can also install completions directly by running\n\n```bash\n  eval \"$(./scripts/buildUnittestBashCompletion.bash)\"\n```\n\nin your shell, or `.bashrc` etc., but note that it has to be executed in the\narangodb directory.\n\n# Additional Resources\n\n- [ArangoDB website](https://www.arangodb.com/)\n\n- [ArangoDB on Twitter](https://twitter.com/arangodb)\n\n- [General GitHub documentation](https://help.github.com/)\n\n- [GitHub pull request documentation](https://docs.github.com/en/github/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork/)\n"
        },
        {
          "name": "Documentation",
          "type": "tree",
          "content": null
        },
        {
          "name": "ERROR_LEVELS.md",
          "type": "blob",
          "size": 5.30859375,
          "content": "# Error Levels\n\nIn this section we define and describe the meaning of the error levels\nin ArangoDB's log messages. The error levels are, from most to least severe:\n\n  - FATAL\n  - ERROR\n  - WARN\n  - INFO\n  - DEBUG\n  - TRACE\n\nFor each log topic one can configure the lowest level which is actually logged.\nFor example, if one sets the error level to `ERROR` for some log topic,\none only sees messages of level `ERROR` and above (`ERROR` and `FATAL`).\n\n## FATAL\n\n_Fatal_ errors are the most severe errors and only occur if a service or application\ncan not recover safely from an abnormal state, which forces it to shut down.\n\nTypically, a fatal error only occurs once in the process lifetime,\nso if the log file is tied to the process, this is typically\nthe last message in the log. There might be a few exceptions to this\nrule, where it makes more sense to keep the server running, for example\nto be able to diagnose the problem better.\n\nWe reserve this error type for the following events:\n\n- crucial files/folders are missing or inaccessible during startup\n- overall application or system failure with a serious danger of\n  data corruption or loss (the following shutdown is intended to prevent\n  possible or further data loss)\n\n**Recommendation**:\nFatal errors should be investigated immediately by a system administrator.\n\n**Note that**:\nAlerts will be raised and people will be called at night for this type of\nerror.\n\n## ERROR\n\nIf a problem is encountered which is fatal to some operation, but not for\nthe service or the application as a whole, then an _error_ is logged.\n\nReasons for log entries of this severity are for example:\n\n- missing data\n- a required file can't be opened\n- incorrect connection strings\n- missing services\n\nIf some operation is automatically retried and eventually succeeds,\nno error will be written to the log. Therefore, if an error is logged then\nit should be taken seriously as it may require user intervention to solve.\n\nNote that in any distributed system, temporary failures of network connections\nor certain servers or services can and will happen. Most systems will tolerate\nsuch failures and retry for some time, but will eventually run out of patience,\ngive up and fail the operation one level up.\n\n**Recommendation**:\nA system administrator should be notified automatically to investigate the error.\nBy filtering the log to look at errors (and other logged events above)\none can determine the error frequency and quickly identify the initial failure\nthat might have resulted in a cascade of additional errors.\n\n**Note that**:\nAlerts will be raised and people will be called at night for this type of\nerror.\n\n## WARN\n\nA _warning_ is triggered by anything that can potentially cause\napplication oddities, but from which the system recovers automatically.\n\nExamples of events which lead to warnings:\n\n- switching from a primary to backup server\n- retrying an operation\n- missing secondary data\n- things running inefficiently\n  (in particular slow queries and bad system settings)\n  \nCertain warnings are logged at startup time only, such as startup option\nvalues which lie outside the recommended range.\n\nThese _might_ be problems, or might not. For example, expected transient\nenvironmental conditions such as short loss of network or database\nconnectivity are logged as warnings, not errors. Viewing a log filtered\nto show only warnings and errors may give quick insight into early\nhints at the root cause of subsequent errors.\n\n**Recommendation**:\nCan mostly be ignored but can give hints for inefficiencies or\nfuture problems.\n\n## INFO\n\n_Info_ messages are generally useful information to log to better\nunderstand what state the system is in. One will usually want to\nhave info messages available but does usually not care about them\nunder normal circumstances.\n\nInformative messages are logged in events like:\n\n- successful initialization\n- services starting or stopping\n- successful completion of significant transactions\n- configuration assumptions\n\nViewing log entries of severity _info_ and above should give a quick overview\nof major state changes in the process providing top-level context for\nunderstanding any warnings or errors that also occur. Logging info level\nmessages and above will usually not spam anything beyond good readability.\n\n**Recommendation**:\nUsually good to have, but one does not have to look at _info_ level messages\nunder normal circumstances.\n\n## DEBUG\n\nInformation which is helpful to ArangoDB developers as well as to other\npeople like system administrators to diagnose an application or service\nis logged as _debug_ message.\n\nDebug messages make software much more maintainable but require some\ndiligence because the value of individual debug statements may change\nover time as programs evolve. The best way to achieve this is by getting\nthe development team in the habit of regularly reviewing logs as a standard\npart of troubleshooting reported issues. We encourage our teams to\nprune out messages that no longer provide useful context and to add\nmessages where needed to understand the context of subsequent messages.\n\n**Recommendation**:\n_Debug_ level messages are usually switched off, but one can switch them on\nto investigate problems.\n\n## TRACE\n\n_Trace_ messages produce a lot of output and are usually only needed by\nArangoDB developers to debug problems in the source code.\n\n**Recommendation**:\n_Trace_ level logging should generally stay disabled.\n"
        },
        {
          "name": "EXPERIMENTAL",
          "type": "blob",
          "size": 0.0283203125,
          "content": "THIS BRANCH IS EXPERIMENTAL \n"
        },
        {
          "name": "ISSUE_TEMPLATE.md",
          "type": "blob",
          "size": 1.7578125,
          "content": "## My Environment\n\n* __ArangoDB Version__:        <!-- e.g. 3.9.2 or self-compiled devel branch -->\n* __Deployment Mode__:       <!-- Single Server | Leader/Follower (\"Master/Slave\") | Active Failover | Cluster | DC2DC -->\n* __Deployment Strategy__:   <!-- Manual Start | Manual Start in Docker | ArangoDB Starter | ArangoDB Starter in Docker | Kubernetes -->\n* __Configuration__:               <!-- cluster setup details, notable server settings, etc. -->\n* __Infrastructure__:               <!-- AWS | Azure | ... | own -->\n* __Operating System__:        <!-- Ubuntu 20.04 | Windows 10 | MacOS 10.13.4 | ... -->\n* __Total RAM in your machine__:        <!-- e.g. 32Gb. If more machines are in use, provide info for each of your machines -->\n* __Disks in use__:        <!-- SSD | HDD. If more machines are in use, provide info for each of your machines -->\n* __Used Package__:              <!-- Debian or Ubuntu .deb | SUSE or RedHat .rpm | Docker - official Docker library | other -->\n\n## Component, Query & Data\n\n__Affected feature__:\n<!-- e.g. Installation | Foxx | AQL query using web interface | arangosh | with driver | ... -->\n\n\n__AQL query (if applicable)__:\n```\n\n\n```\n__AQL explain and/or profile (if applicable)__:\n<!-- output of  db._explain(\"<my aql query>\")  or  db._profileQuery(\"<my aql query>\") -->\n\n\n__Dataset__:\n<!-- description, or if possible, share an example dataset to reproduce the issue either as Gist with an arangodump, or an arangosh script with db.collection.save({my: \"values\"}) statements -->\n\n\n__Size of your Dataset on disk__:\n<!-- size of your dataset on disk-->\n\n\n__Replication Factor & Number of Shards (Cluster only)__:\n<!-- please list these settings for each collection in question -->\n\n\n## Steps to reproduce\n\n1. \n2. \n3. \n\n__Problem__:\n\n\n__Expected result__:\n\n"
        },
        {
          "name": "Installation",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 3.0517578125,
          "content": "\nBusiness Source License 1.1\n\nParameters\n\nLicensor:             ArangoDB, Inc.\n \nLicensed Work:        ArangoDB Self Managed (Release Date: March 2024)\n                      \nChange Date:          4 year anniversary of the Release Date\nChange License:       Apache License, Version 2.0\n\nAdditional Use Grant:  Subject to your compliance with the terms of this \nAgreement, including this clause, you may make use of the Licensed Work \ninternally in production, provided that you may not use the Licensed Work \nin a commercial offering that allows one or more third parties \n(other than your contractors) to access, create or manage databases \nincluding data that is controlled by any such third parties.\n\nNotice:\n\nThe Business Source License (this document, or the “License”) is not an Open\nSource license. However, the Licensed Work will eventually be made available \nunder an Open Source License, as stated in this License.\n\n-----------------------------------------------------------------------------\n\nBusiness Source License 1.1\n\nTerms\n\nThe Licensor hereby grants you the right to copy, modify, create derivative \nworks, redistribute, and make non-production use of the Licensed Work. The\nLicensor may make an Additional Use Grant, above, permitting limited \nproduction use.\n\nEffective on the Change Date, or the fourth anniversary of the first publicly\navailable distribution of a specific version of the Licensed Work under this\nLicense, whichever comes first, the Licensor hereby grants you rights under\nthe terms of the Change License, and the rights granted in the paragraph \nabove terminate.\n\nIf your use of the Licensed Work does not comply with the requirements\ncurrently in effect as described in this License, you must purchase a\ncommercial license from the Licensor, its affiliated entities, \nor authorized resellers, or you must refrain from using the Licensed Work.\n\nAll copies of the original and modified Licensed Work, and derivative works \nof the Licensed Work, are subject to this License. \nThis License applies separately for each version of the Licensed Work \nand the Change Date may vary for each version of the Licensed Work \nreleased by Licensor.\n\nYou must conspicuously display this License on each original or modified copy \nof the Licensed Work. If you receive the Licensed Work in original or\nmodified form from a third party, the terms and conditions set forth in this \nLicense apply to your use of that work.\n\nAny use of the Licensed Work in violation of this License will automatically\nterminate your rights under this License for the current and all other \nversions of the Licensed Work.\n\nThis License does not grant you any right in any trademark or logo of\nLicensor or its affiliates (provided that you may use a trademark or logo of\nLicensor as expressly required by this License).\n\nTO THE EXTENT PERMITTED BY APPLICABLE LAW, THE LICENSED WORK IS PROVIDED ON\nAN “AS IS” BASIS. LICENSOR HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS,\nEXPRESS OR IMPLIED, INCLUDING (WITHOUT LIMITATION) WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, AND\nTITLE.\n"
        },
        {
          "name": "LICENSES-OTHER-COMPONENTS.md",
          "type": "blob",
          "size": 70.9228515625,
          "content": "- [Other Licenses](#other-licenses)\n  * [C/C++ Libraries](#cc-libraries)\n  * [Libraries used by iresearch](#libraries-used-by-iresearch)\n  * [Programs](#programs)\n  * [Data files](#data-files)\n  * [JavaScript](#javascript)\n    + [Node core modules](#node-core-modules)\n    + [Bundled NPM modules](#bundled-npm-modules)\n    + [Backend libraries](#backend-libraries)\n    + [Frontend libraries](#frontend-libraries)\n\n# Other Licenses\n\n## C/C++ Libraries\n\n### Abseil - C++ Common Libraries\n\n* Name: abseil-cpp\n* Version: 13708db87b1ab69f4f2b3214f3f51e986546f282\n* Date: 2022-11-22\n* Project Home: https://abseil.io\n* License: https://raw.githubusercontent.com/abseil/abseil-cpp/master/LICENSE\n* License Name: Apache License 2.0\n* License Id: Apache-2.0\n\n### Boost\n\n* Name: Boost\n* Version: 1.78.0\n* Date: 2021-12-08 03:45:00\n* Project Home: http://www.boost.org/\n* License: http://www.boost.org/LICENSE_1_0.txt\n* License Name: Boost Software License 1.0\n* License Id: BSL-1.0\n\n### cmdline\n\n* Name: cmdline\n* Version: a68095a\n* Date: 2012-10-22 10:23:16Z\n* Project Home: https://github.com/tanakh/cmdline\n* License: https://raw.githubusercontent.com/tanakh/cmdline/master/LICENSE\n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n### CreditCardGenerator\n\n_Enterprise Edition only_\n\n* Name: CreditCardGenerator\n* Version: (none)\n* Date: 2016-06-21 21:40:21Z\n* Project Home: https://github.com/stormdark/CreditCardGenerator\n* License: https://raw.githubusercontent.com/stormdark/CreditCardGenerator/master/LICENSE\n* License Name: MIT License\n* License Id: MIT\n\n### date\n\n* Name: date\n* Version: 3.0.0\n* Date: 2020-06-03\n* Project Home: https://github.com/HowardHinnant/date\n* License: https://raw.githubusercontent.com/HowardHinnant/date/master/LICENSE.txt\n* License Name: MIT License\n* License Id: MIT\n\n### faiss\n\n* Name: faiss\n* Version: 1.9.0\n* Date: 2024-10-04\n* Project Home:https://github.com/facebookresearch/faiss/\n* License: https://github.com/facebookresearch/faiss/blob/main/LICENSE \n* License Name: MIT License\n* License Id: MIT\n\n### fasthash\n\n* Name: fasthash\n* Version: 1.0\n* Date: 2012-11-02\n* Project Home: https://code.google.com/p/fast-hash/\n* License: https://raw.githubusercontent.com/arangodb/arangodb/devel/lib/Basics/fasthash.cpp\n* License Name: MIT License\n* License Id: MIT\n\n### fakeit\n\n* Name: fakeit\n* Version: 2.0.3\n* Date: 2017-02-28 13:11:35Z & 2018-08-17 00:22:33Z\n* Project Home: https://github.com/eranpeer/FakeIt\n* License: https://raw.githubusercontent.com/eranpeer/FakeIt/master/LICENSE\n* License Name: MIT License\n* License Id: MIT\n\n### fastvalidate-utf8\n\n* Name: fastvalidate-utf8\n* Version: (none)\n* Date: 2017-02-28 13:11:35Z \n* Project Home: https://github.com/lemire/fastvalidate-utf-8\n* License: https://raw.githubusercontent.com/lemire/fastvalidate-utf-8/master/LICENSE-APACHE\n* License Name: Apache License 2.0\n* License Id: Apache-2.0\n\n### fpconv_dtoa\n\n* Name: fpconv\n* Version: (none)\n* Date: 2013-10-15 03:33:30Z\n* Project Home: https://github.com/night-shift/fpconv/\n* License: https://raw.githubusercontent.com/night-shift/fpconv/master/license\n* License Name: MIT License\n* License Id: MIT\n\n### function2\n\n* Name: function2\n* Version: 4.0.0\n* Date: 2019-01-04 11:55:29Z\n* Project Home: https://github.com/Naios/function2\n* License: https://raw.githubusercontent.com/Naios/function2/master/LICENSE.txt\n* License Name: Boost Software License 1.0\n* License Id: BSL-1.0\n\n### GeographicLib\n\n* Name: GeographicLib\n* Version: 1.49\n* Date: 2017-10-05 01:36:00Z\n* Project Home: https://geographiclib.sourceforge.io/\n* License: https://geographiclib.sourceforge.io/html/LICENSE.txt\n* License Name: MIT License\n* License Id: MIT\n\n#### glibc\n\n* Name: glibc\n* Version: 2.39.0\n* Date: 2024-01-31\n* Project Home: https://www.gnu.org/software/libc\n* License: https://www.gnu.org/licenses/lgpl-3.0.html\n* License Name: GNU Lesser General Public License\n* License Id: LGPL-3.0\n\n### Google V8\n\n* Name: V8\n* Version: 12.1.165\n* Date: 2023-11-20 02:40:45 -0800\n* Project Home: https://github.com/v8/v8\n* License: https://github.com/arangodb/v8/blob/980b6da33f7275a2ef369e83299163f6446dd81f/LICENSE.v8\n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n#### colorama\n\n* Name: colorama\n* Version: (see V8)\n* Date: (see V8)\n* Project Home: (bundled with V8)\n* License: https://github.com/arangodb/v8/tree/980b6da33f7275a2ef369e83299163f6446dd81f/third_party/colorama\n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n#### cpu_features\n\n* Name: cpu_features\n* Version: 0.8.0\n* Date: (see V8)\n* Project Home: (bundled with V8)\n* License: https://github.com/arangodb/v8/tree/980b6da33f7275a2ef369e83299163f6446dd81f/third_party/cpu_features\n* License Name: Apache License 2.0\n* License Id: Apache-2.0\n\n#### FDLIBM\n\n* Name: FDLIBM\n* Version: (see V8)\n* Date: (see V8)\n* Project Home: (bundled with V8)\n* License: https://github.com/arangodb/v8/blob/980b6da33f7275a2ef369e83299163f6446dd81f/LICENSE.fdlibm\n* License Comment: free as-is license (like ISC without liability clauses)\n\n#### glibc mathematical functions\n\n* Name: glibc\n* Version: (see V8)\n* Date: (see V8)\n* Project Home: (bundled with V8)\n* License: https://github.com/arangodb/v8/blob/980b6da33f7275a2ef369e83299163f6446dd81f/third_party/glibc/LICENSE\n* License Name: GNU Lesser General Public License\n* License Id: LGPL-2.1-only\n\n#### Google Benchmark\n\n* Name: Google benchmark\n* Version: (see V8)\n* Date: (see V8)\n* Project Home: (bundled with V8)\n* License: https://github.com/arangodb/v8/tree/980b6da33f7275a2ef369e83299163f6446dd81f/third_party/google_benchmark\n* License Name: Apache License 2.0\n* License Id: Apache-2.0\n\n#### googletest\n\n* Name: googletest\n* Version: (see V8)\n* Date: (see V8)\n* Project Home: (bundled with V8)\n* License: https://github.com/arangodb/v8/tree/980b6da33f7275a2ef369e83299163f6446dd81f/third_party/googletest\n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n#### gyp\n\n* Name: gyp\n* Version: 0.16.1\n* Date: 2023-10-25 08:29:23 +0200\n* Project Home: https://github.com/nodejs/gyp-next\n* License: https://github.com/arangodb/arangodb/blob/devel/3rdParty/v8-build/gyp/LICENSE\n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n#### ICU\n\n* Name: ICU (International Components for Unicode)\n* Version: 73.1\n* Date: (see V8)\n* Project Home: (bundled with V8)\n* License: https://github.com/arangodb/v8/blob/980b6da33f7275a2ef369e83299163f6446dd81f/third_party/icu/LICENSE\n* License Name: ICU license\n* License Comment: BSD-3-Clause + third-party license\n\n#### Inspector Protocol\n\n* Name: Chromium inspector (devtools) protocol\n* Version: (see V8)\n* Date: (see V8)\n* Project Home: (bundled with V8)\n* License: https://github.com/arangodb/v8/blob/980b6da33f7275a2ef369e83299163f6446dd81f/third_party/inspector_protocol/LICENSE\n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n#### Jinja2\n\n* Name: Jinja2 Python Template Engine\n* Version: (see V8)\n* Date: (see V8)\n* Project Home: (bundled with V8)\n* License: https://github.com/arangodb/v8/blob/980b6da33f7275a2ef369e83299163f6446dd81f/third_party/jinja2/LICENSE.rst\n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n#### jsoncpp\n\n* Name: jsoncpp JSON parser and builder\n* Version: (see V8)\n* Date: (see V8)\n* Project Home: (bundled with V8)\n* License: https://github.com/arangodb/v8/blob/980b6da33f7275a2ef369e83299163f6446dd81f/third_party/jsoncpp/LICENSE\n* License Name: MIT License\n* License Id: MIT\n\n#### markupsafe\n\n* Name: MarkupSafe Python Safe String Class\n* Version: (see V8)\n* Date: (see V8)\n* Project Home: (bundled with V8)\n* License: https://github.com/arangodb/v8/blob/980b6da33f7275a2ef369e83299163f6446dd81f/third_party/markupsafe/LICENSE\n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n#### Strongtalk\n\n* Name: Strongtalk\n* Version: (bundled with V8)\n* Date: (bundled with V8)\n* Project Home: https://github.com/talksmall/Strongtalk\n* License: https://github.com/arangodb/v8/blob/980b6da33f7275a2ef369e83299163f6446dd81f/LICENSE.strongtalk\n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n#### V8 builtins\n\n* Name: V8 builtins\n* Version: (see V8)\n* Date: (see V8)\n* Project Home: (bundled with V8)\n* License: https://github.com/arangodb/v8/blob/980b6da33f7275a2ef369e83299163f6446dd81f/third_party/v8/builtins/LICENSE\n* License Name: Python Software Foundation License Version 2\n* License Id: Python-2.0\n\n#### wasm-c-api\n\n* Name: Wasm C/C++ API\n* Version: (see V8)\n* Date: (see V8)\n* Project Home: (bundled with V8)\n* License: https://github.com/arangodb/v8/blob/980b6da33f7275a2ef369e83299163f6446dd81f/third_party/wasm-api/LICENSE\n* License Name: Apache License 2.0\n* License Id: Apache-2.0\n\n### wcwidth\n\n* Name: wcwidth\n* Version: commit 911263c514237997c9c590517672c6ea729388dc\n* Date: 2022-12-16T06:51:44+0000\n* Project Home: https://github.com/termux/wcwidth\n* License: https://github.com/termux/wcwidth/blob/master/LICENSE.txt\n* License Name: MIT License\n* License Id: MIT\n\n### Google Test\n\n* Name: Google Test (gtest)\n* Version: 1.12.1\n* Date: 2022-06-30 10:14:00Z\n* Project Home: https://github.com/google/googletest\n* License: https://raw.githubusercontent.com/google/googletest/release-1.12.1/LICENSE \n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n### immer\n\n* Name: immer\n* Version: 0.8.0\n* Date: 2022-12-06 11:30 UTC\n* Project Home: https://github.com/arximboldi/immer\n* License: https://github.com/arximboldi/immer/blob/v0.8.0/LICENSE\n* License Name: Boost Software License 1.0\n\n### jemalloc\n\n* Name: jemalloc\n* Version: 5.3.0, commit e4817c8d89a2a413e835c4adeab5c5c4412f9235\n* Date: 2023-10-24 13:51:14 -0700\n* Project Home: https://github.com/jemalloc/jemalloc\n* License: https://raw.githubusercontent.com/jemalloc/jemalloc/dev/COPYING\n* License Comment: free as-is license\n\n  The generated files config.guess and config.sub have the explicit\n  exception from the GPL license:\n\n      As a special exception to the GNU General Public License, if you\n      distribute this file as part of a program that contains a\n      configuration script generated by Autoconf, you may include it under\n      the same distribution terms that you use for the rest of that\n      program.  This Exception is an additional permission under section 7\n      of the GNU General Public License, version 3 (\"GPLv3\").\n\n### libunwind\n\n* Name: libunwind\n* Version: 1.7.2\n* Date: 2023-07-29 11:04:36 -0400\n* Project Home: https://github.com/libunwind/libunwind\n* License: https://raw.githubusercontent.com/libunwind/libunwind/master/LICENSE\n* License Name: MIT License\n* License Id: MIT\n\n### linenoise-ng\n\n* Name: linenoise-ng\n* Version: 1.0.1\n* Date: 2017-03-06 16:01:33Z\n* GitHub: https://github.com/arangodb/linenoise-ng\n* License: https://raw.githubusercontent.com/arangodb/linenoise-ng/master/LICENSE\n* License Name: BSD 2-clause \"Simplified\" License\n* License Id: BSD-2-Clause\n\n### llhttp\n\n* Name: llhttp\n* Version: 6.0.4\n* Date: 2021-06-26 22:27:35 -0700\n* Project Home: https://github.com/nodejs/llhttp\n* License: https://raw.githubusercontent.com/nodejs/llhttp/main/LICENSE-MIT\n* License Name: MIT License\n* License Id: MIT\n\n### nghttp2\n\n* Name: nghttp2\n* Version: 1.61.0\n* Date: 2024-04-04 17:16:56 +09:00\n* GitHub: https://github.com/nghttp2/nghttp2\n* License: https://raw.githubusercontent.com/nghttp2/nghttp2/master/COPYING\n* License Name: MIT License\n* License Id: MIT\n\n### OpenSSL\n\n* Name: OpenSSL\n* Version: 1.1.1\n* Date: 2021-08-24\n* License: https://spdx.org/licenses/OpenSSL.html\n* License Name: Apache Style License\n* License Id: OpenSSL\n\n### RocksDB\n\n* Name: RocksDB\n* Version: 7.2.0\n* Date: 2022-05-05\n* GitHub: https://github.com/arangodb/rocksdb\n* License: https://github.com/arangodb/rocksdb/blob/main/README.md\n* License Name: Apache License 2.0\n* License Id: Apache-2.0\n\n### S2 Geometry Library\n\n* Name: s2geometry\n* Version: ce14f45ae0b6c28e3f47cbcf6646d988d41046b5\n* Date: 2023-01-04\n* Project Home: https://s2geometry.io/\n* License: https://raw.githubusercontent.com/arangodb/arangodb/devel/3rdParty/s2geometry/dfefe0c/LICENSE\n* License Name: Apache License 2.0\n* License Id: Apache-2.0\n\n### short_alloc\n\n* Name: short_alloc\n* Version: (none)\n* Date: 2016-02-04 22:56:08Z\n* Project Home: https://howardhinnant.github.io/stack_alloc.html\n* License: https://howardhinnant.github.io/short_alloc.h\n* License Name: MIT License\n* License Id: MIT\n\n### snappy\n\n* Name: snappy\n* Version: 1.1.9\n* Date: 2021-05-04 22:28:33Z\n* Project Home: https://github.com/google/snappy\n* License: https://raw.githubusercontent.com/google/snappy/master/COPYING\n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n### taocpp::json\n\n* Name: taoJSON\n* Version: (none)\n* Date: 2020-03-14 19:09:08Z\n* Project Home: https://github.com/taocpp/json\n* License: https://raw.githubusercontent.com/taocpp/json/master/LICENSE\n* License Name: MIT License\n* License Id: MIT\n\n### wyhash\n\n* Name: wyhash\n* Version: commit 896d7c57f68d96938f718c0695c44866d14b48d6\n* Date: 2021-04-02 20:34:24 +08:00\n* GitHub: https://github.com/wangyi-fudan/wyhash/\n* License: https://github.com/wangyi-fudan/wyhash/blob/master/LICENSE\n* License Name: Unlicense\n* License Id: Unlicense\n\n### xxHash\n\n* Name: xxHash\n* Version: 0.8.0\n* Date: 2021-03-17 17:51:40 +01:00\n* GitHub: https://github.com/Cyan4973/xxHash/\n* License: https://raw.githubusercontent.com/Cyan4973/xxHash/master/LICENSE\n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n### zlib\n\n* Name: zlib\n* Version: 1.2.13\n* Date: 2022-10-13\n* Project Home: https://github.com/madler/zlib/\n* License: https://raw.githubusercontent.com/madler/zlib/master/README\n* License Comment: free as-is license\n\n## Libraries used by iresearch\n\nThe ArangoDB software makes uses of the \n[iresearch library](https://github.com/iresearch-toolkit/iresearch),\nwhich is mainly developed by ArangoDB, too.\nThe iresearch library itself makes use of several other components with\ndifferent licenses. The full list of components and their licenses\ncan be found [here](https://github.com/arangodb/arangodb/blob/devel/3rdParty/iresearch/THIRD_PARTY_README.md).\n\n## Libraries used by faiss\n\n### OpenMP implementation by LLVM\n\n* Name: OpenMP\n* Version: 16.0.6\n* Date: 2023-06-14\n* Project Home: https://github.com/llvm/llvm-project\n* License: https://github.com/llvm/llvm-project/blob/main/LICENSE.TXT\n* License Name: Apache License 2.0\n* License Id: Apache-2.0\n\n### LAPACK\n\n* Name: LAPACK\n* Version: 3.10.0\n* Date: 2021-06-28\n* Project Home: https://www.netlib.org/lapack/\n* License: https://www.netlib.org/lapack/LICENSE.txt\n* License Comment: The license used for the software is the modified BSD license\n\n### Fortran\n\n* Name: gfortran\n* Version: 11.4.0 \n* Date: 2023-05-29\n* Project Home: https://gcc.gnu.org/wiki/GFortran\n* License: https://moinmo.in/GPL\n* License Id: GPLv2\n\n## Programs\n\n### Bison\n\n* Name: bison\n* Version: 3.4.1\n* Date: 2019-05-22 05:28:15Z\n* Project Home: https://www.gnu.org/software/bison/\n* License: https://raw.githubusercontent.com/arangodb/arangodb/devel/arangod/Aql/grammar.cpp\n* License Comment: only used to generate code, not part of the distribution;\n  for details about using Bison in this way see\n  http://www.gnu.org/software/bison/manual/bison.html#Conditions\n\n  As a special exception, you may create a larger work that contains part or\n  all of the Bison parser skeleton and distribute that work under terms of your\n  choice, so long as that work isn't itself a parser generator using the\n  skeleton or a modified version thereof as a parser skeleton. Alternatively,\n  if you modify or redistribute the parser skeleton itself, you may (at your\n  option) remove this special exception, which will cause the skeleton and the\n  resulting Bison output files to be licensed under the\n  GNU General Public License without this special exception.\n\n### cmake\n\n* Name: cmake\n* Version: 3.8\n* Project Home: https://cmake.org/\n* License: https://gitlab.kitware.com/cmake/cmake/raw/master/Copyright.txt\n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n### Flex\n\n* Name: flex\n* Version: 2.5.35\n* Date: 2008-02-26\n* Project Home: https://github.com/westes/flex\n* License: https://raw.githubusercontent.com/westes/flex/master/COPYING\n* License Comment: free as-is license (BSD-2-Clause variant),\n  only used to generate code, not part of the distribution\n\n### fmt\n\n* Name: fmt\n* Version: 9.1.0\n* Date: 2022-08-27 08:57:10-0700\n* Project Home: https://fmt.dev/\n* License: https://github.com/fmtlib/fmt/blob/master/LICENSE.rst\n* License Name: MIT\n* License Id: MIT\n\n## Data files\n\n#### IANA Time Zone Database (tzdata)\n\nName: Time Zone Database\nVersion: 2020f\nDate: 2020-12-29\nProject https://www.iana.org/time-zones\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/3rdParty/tzdata/LICENSE\nLicense Name: Public Domain\nLicense Id: -\n\n#### Unicode CLDR Project (windowsZones.xml)\n\nName: Unicode CLDR Project\nVersion: 38.1\nDate: 2020-11-19\nProject Home: https://github.com/unicode-org/cldr\nLicense: https://raw.githubusercontent.com/unicode-org/cldr/master/ICU-LICENSE\nLicense Name: ICU License\nLicense Id: ICU\n\n#### The Project Gutenberg eBook, Children of the Frost, by Jack London\n\nName: The Project Gutenberg eBook, Children of the Frost, by Jack London\nVersion: N/A\nDate: N/A\nProject Home: https://www.gutenberg.org/ebooks/10736\nLicense: https://www.gutenberg.org/policy/license.html\nLicense Name: The Project Gutenberg License\nLicense Id: -\n\n#### The Project Gutenberg eBook of A Dream Of Red Mansions, by Xueqin Cao\n\nName: The Project Gutenberg eBook of A Dream Of Red Mansions, by Xueqin Cao\nVersion: N/A\nDate: N/A\nProject Home: https://www.gutenberg.org/ebooks/24264\nLicense: https://www.gutenberg.org/policy/license.html\nLicense Name: The Project Gutenberg License\nLicense Id: -\n\n## JavaScript\n\n### Node core modules\n\n#### node\n\n* Name: node\n* Version: 2017\n* Project Home: http://nodejs.org\n* GitHub: https://github.com/nodejs/node\n* License: https://raw.githubusercontent.com/nodejs/node/master/LICENSE\n* License Comment: for all files except punycode.js\n\n#### punycode\n\n* Name: punycode\n* Version: 2017\n* License: https://raw.githubusercontent.com/bestiejs/punycode.js/master/LICENSE-MIT.txt\n* License Name: MIT License\n* License Id: MIT\n\n### Backend libraries\n\n#### AsciiTable\n\n* Name ascii-table\n* Version: 0.0.8\n* GitHub: https://github.com/sorensen/ascii-table\n* License: https://github.com/sorensen/ascii-table/blob/master/license\n* License Name: MIT License\n* License Id: MIT\n\n#### JSUnity\n\n* Name: jsunity\n* Version: 0.6\n* GitHub: https://github.com/atesgoral/jsunity\n* License: https://raw.githubusercontent.com/atesgoral/jsunity/master/LICENSE.txt\n* License Name: MIT License\n* License Id: MIT\n\n### Frontend libraries\n\n#### Ace\n\n* Name: ace\n* Version: 2010\n* GitHub: https://github.com/ajaxorg/ace\n* License: https://raw.githubusercontent.com/ajaxorg/ace/master/LICENSE\n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n#### Animate.css\n\n* Name: animate.css\n* Version: 2015\n* GitHub: https://github.com/daneden/animate.css\n* License: https://raw.githubusercontent.com/daneden/animate.css/master/LICENSE\n* License Name: MIT License\n* License Id: MIT\n\n#### Backbone.js\n\n* Name: backbone.js\n* Version: 2016\n* Project Home: http://backbonejs.org\n* GitHub: https://github.com/jashkenas/backbone\n* License: https://raw.githubusercontent.com/jashkenas/backbone/master/LICENSE\n* License Comment: free as-is license\n\n#### Bootstrap\n\n* Name: bootstrap\n* Version: 2015\n* Project Home: http://getbootstrap.com\n* GitHub: https://github.com/twbs/bootstrap\n* License: https://raw.githubusercontent.com/twbs/bootstrap/master/LICENSE\n* License Name: MIT License\n* License Id: MIT\n\n#### D3\n\n* Name: D3.js\n* Version: 2016 \n* Project Home: http://d3js.org\n* GitHub: https://github.com/mbostock/d3\n* License: https://raw.githubusercontent.com/mbostock/d3/master/LICENSE\n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n#### dygraph\n\n* Name: dygraph\n* Version: 2018\n* Project Home: http://dygraphs.com\n* GitHub: https://github.com/danvk/dygraphs\n* License: https://raw.githubusercontent.com/danvk/dygraphs/master/LICENSE.txt\n* License Comment: free as-is license\n\n#### jQuery\n\n* Name: jquery\n* Version: 2.1.0\n* Project Home: http://jquery.com\n* GitHub: https://github.com/jquery/jquery\n* License: https://raw.githubusercontent.com/jquery/jquery/master/LICENSE.txt\n* License Comment: free as-is license\n\n#### jQuery Contextmenu\n\n* Name: jquery.contextmenu\n* Version: 2015\n* GitHub: https://github.com/swisnl/jQuery-contextMenu\n* License: https://raw.githubusercontent.com/swisnl/jQuery-contextMenu/master/README.md\n  under \"License\"\n* License Name: MIT License\n* License Id: MIT\n\n#### jQuery Form\n\n* Name: jquery.form\n* Version: 2015\n* GitHub: https://github.com/malsup/form/\n* License: https://raw.githubusercontent.com/malsup/form/master/README.md\n  under \"Copyright and License\"\n* License Name: MIT License\n* License Id: MIT\n* License Comment: This project is dual licensed under the LGPLv2.1 (or later) or MIT licenses\n\n#### jQuery Hotkeys\n\n* Name: jquery.hotkeys\n* Version: 2015\n* GitHub: https://github.com/jeresig/jquery.hotkeys\n* License: https://raw.githubusercontent.com/jeresig/jquery.hotkeys/master/jquery.hotkeys.js\n* License Name: MIT License\n* License Id: MIT\n* License Comment: Dual licensed under the MIT or GPL Version 2 licenses.\n\n#### jQuery Snippet\n\n* Name: jquery.snippet\n* Version: 2015\n* Archived Project Home: https://web.archive.org/web/20150908173503/http://www.steamdev.com/snippet/\n* License: see https://web.archive.org/web/20150817084109/http://steamdev.com/snippet/\n  in \"Notes, section 5\"\n* License Name: MIT License\n* License Id: MIT\n* License Line: 5.) This script is released under the MIT license and is completely open to modification and redistribution.\n\n#### jQuery Strftime\n\n* Name: strftime\n* Version: 2015\n* GitHub: https://github.com/samsonjs/strftime\n* License: https://raw.githubusercontent.com/samsonjs/strftime/master/Readme.md\n  under \"License\"\n* License Name: MIT License\n* License Id: MIT\n\n#### jQuery Textfill\n\n* Name: jquery.textfill\n* Version: 2015\n* GitHub: https://github.com/jquery-textfill/jquery-textfill\n* License: https://raw.githubusercontent.com/jquery-textfill/jquery-textfill/master/COPYING.md\n* License Name: MIT License\n* License Id: MIT\n\n#### jQuery UI\n\n* Name: jquery-ui\n* Version: 1.9.2\n* Project Home: http://jqueryui.com\n* GitHub: https://github.com/jquery/jquery-ui\n* License: https://raw.githubusercontent.com/jquery/jquery-ui/master/LICENSE.txt\n* License Comment: MIT-style License\n\n#### jQuery UploadFile\n\n* Name: jquery.uploadfile\n* Version: 2015\n* GitHub: https://github.com/hayageek/jquery-upload-file/\n* License: https://raw.githubusercontent.com/hayageek/jquery-upload-file/master/MIT-License.txt\n* License Name: MIT License\n* License Id: MIT\n\n#### jsoneditor.js\n\n* Name: jsoneditor\n* Version: 2018\n* GitHub: https://github.com/josdejong/jsoneditor/\n* License: https://raw.githubusercontent.com/josdejong/jsoneditor/master/package.json\n* License Name: Apache License 2.0\n* License Id: Apache-2.0\n\n#### Moment.js\n\n* Name: moment\n* Version: 2\n* GitHub: https://github.com/moment/moment/\n* License: https://raw.githubusercontent.com/moment/moment/develop/LICENSE\n* License Name: MIT License\n* License Id: MIT\n\n#### nvd3\n\n* Name: nvd3\n* Version: 2016\n* GitHub: https://github.com/novus/nvd3\n* License: https://raw.githubusercontent.com/novus/nvd3/master/package.json\n* License Name: Apache License 2.0\n* License Id: Apache-2.0\n\n#### Noty\n\n* Name: noty\n* Version: 2015\n* GitHub: https://github.com/needim/noty/\n* License: https://raw.githubusercontent.com/needim/noty/master/LICENSE.txt\n* License Name: MIT License\n* License Id: MIT\n\n#### Numeral.js\n\n* Name: numeral.js\n* Version: 1.5.3\n* GitHub: https://github.com/adamwdraper/Numeral-js\n* License: https://raw.githubusercontent.com/adamwdraper/Numeral-js/master/LICENSE\n* License Name: MIT License\n* License Id: MIT\n\n#### prettyBytes.js\n\n* Name: pretty-bytes.js\n* Version: 2016\n* GitHub: https://github.com/sindresorhus/pretty-bytes\n* License: https://raw.githubusercontent.com/sindresorhus/pretty-bytes/master/license\n* License Name: MIT License\n* License Id: MIT\n\n#### select2\n\n* Name: select2\n* Version: 3.4.5\n* GitHub: https://github.com/select2/select2\n* License: https://raw.githubusercontent.com/select2/select2/master/LICENSE.md\n* License Name: MIT License\n* License Id: MIT\n\n#### sigma.js\n\n* Name: sigma.js\n* Version: 2016\n* GitHub: https://github.com/jacomyal/sigma.js\n* License: https://raw.githubusercontent.com/jacomyal/sigma.js/master/LICENSE.txt\n* License Name: MIT License\n* License Id: MIT\n\n#### Swagger UI\n\n* Name: swagger-ui\n* Version: 5.4.1\n* Project Home: http://swagger.io\n* GitHub: https://github.com/swagger-api/swagger-ui\n* License: https://raw.githubusercontent.com/swagger-api/swagger-ui/master/LICENSE\n* License Name: Apache License 2.0\n* License Id: Apache-2.0\n\n#### tippyjs.js\n\n* Name: tippyjs\n* Version: 2017\n* GitHub: https://github.com/atomiks/tippyjs\n* License: https://raw.githubusercontent.com/atomiks/tippyjs/master/LICENSE\n* License Name: MIT License\n* License Id: MIT\n\n#### wheelnav.js\n\n* Name: wheelnav\n* Version: 2016\n* GitHub: https://github.com/softwaretailoring/wheelnav\n* License: https://raw.githubusercontent.com/softwaretailoring/wheelnav/master/LICENSE\n* License Name: MIT License\n* License Id: MIT\n\n#### jquery-csv\n\n* Name: jquery.csv\n* Version: 2012\n* GitHub: https://github.com/evanplaice/jquery-csv\n* License: https://raw.githubusercontent.com/evanplaice/jquery-csv/master/LICENSE\n* License Name: MIT License\n* License Id: MIT\n\n#### Leaflet.js\n\n* Name: leaflet\n* Version: 1.3.3\n* GitHub: https://github.com/Leaflet/Leaflet\n* License: https://raw.githubusercontent.com/Leaflet/Leaflet/master/LICENSE\n* License Name: BSD 2-clause \"Simplified\" License\n* License Id: BSD-2-Clause\n\n#### Stamen\n\n* Name: stamen\n* Version: 1.3.0\n* GitHub: https://github.com/stamen/maps.stamen.com\n* License: https://raw.githubusercontent.com/stamen/maps.stamen.com/master/LICENSE\n* License Name: BSD 3-clause \"New\" or \"Revised\" License\n* License Id: BSD-3-Clause\n\n#### randomColor\n\n* Name: randomColor\n* Version: 0.4.0\n* GitHub: https://github.com/davidmerfield/randomColor\n* License: https://raw.githubusercontent.com/davidmerfield/randomColor/0.4.0/LICENSE.md\n* License Name: MIT License\n* License Id: MIT\n\n### Bundled NPM modules\n\n#### @babel/code-frame\n\nName: @babel/code-frame\nVersion: 7.0.0\nProject Home: https://github.com/babel/babel/tree/master/packages/babel-code-frame\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/@babel/code-frame/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### @babel/highlight\n\nName: @babel/highlight\nVersion: 7.0.0\nProject Home: https://github.com/babel/babel/tree/master/packages/babel-highlight\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/@babel/highlight/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### accepts\n\nName: accepts\nVersion: 1.3.5\nProject Home: https://github.com/jshttp/accepts\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/accepts/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### acorn-jsx\n\nName: acorn-jsx\nVersion: 5.0.1\nProject Home: https://github.com/RReverser/acorn-jsx\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/acorn-jsx/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### acorn\n\nName: acorn\nVersion: 6.1.1\nProject Home: https://github.com/acornjs/acorn\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/acorn/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### ajv\n\nName: ajv\nVersion: 8.2.0\nProject Home: https://github.com/epoberezkin/ajv\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/ajv/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### ajv\n\nName: ajv\nVersion: 6.10.0\nProject Home: https://github.com/epoberezkin/ajv\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/ajv/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### ansi-escapes\n\nName: ansi-escapes\nVersion: 3.2.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/ansi-escapes/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### ansi-html-community\n\nName: ansi-html-community\nVersion: 0.0.8\nProject Home: https://github.com/mahdyar/ansi-html-community\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/ansi-html-community/LICENSE\nLicense Name: Apache License 2.0\nLicense Id: Apache-2.0\n\n#### ansi-regex\n\nName: ansi-regex\nVersion: 3.0.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/ansi-regex/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### ansi-regex\n\nName: ansi-regex\nVersion: 4.1.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/inquirer/node_modules/ansi-regex/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### ansi-styles\n\nName: ansi-styles\nVersion: 3.2.1\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/ansi-styles/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### ansi_up\n\nName: ansi_up\nVersion: 4.0.3\nProject Home: https://github.com/drudru/ansi_up\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/ansi_up/Readme.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### aqb\n\nName: aqb\nVersion: 2.1.0\nProject Home: https://github.com/arangodb/aqbjs\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/aqb/LICENSE\nLicense Name: Apache License 2.0\nLicense Id: Apache-2.0\n\n#### argparse\n\nName: argparse\nVersion: 1.0.10\nProject Home: https://github.com/nodeca/argparse\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/js-yaml/node_modules/argparse/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### assertion-error\n\nName: assertion-error\nVersion: 1.0.2\nProject Home: http://qualiancy.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/chai/node_modules/assertion-error/README.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### astral-regex\n\nName: astral-regex\nVersion: 1.0.0\nProject Home: github.com/kevva\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/astral-regex/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### babel-code-frame\n\nName: babel-code-frame\nVersion: 6.26.0\nProject Home: https://github.com/babel/babel/tree/master/packages/babel-code-frame\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/babel-code-frame/README.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### balanced-match\n\nName: balanced-match\nVersion: 1.0.0\nProject Home: http://juliangruber.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/minimatch/node_modules/balanced-match/LICENSE.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### brace-expansion\n\nName: brace-expansion\nVersion: 1.1.8\nProject Home: http://juliangruber.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/minimatch/node_modules/brace-expansion/README.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### callsites\n\nName: callsites\nVersion: 3.1.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/callsites/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### chai\n\nName: chai\nVersion: 3.5.0\nProject Home: https://github.com/chaijs/chai\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/chai/README.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### chalk\n\nName: chalk\nVersion: 2.4.2\nProject Home: https://github.com/chalk/chalk\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/chalk/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### chardet\n\nName: chardet\nVersion: 0.7.0\nProject Home: https://github.com/runk/node-chardet\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/chardet/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### cli-cursor\n\nName: cli-cursor\nVersion: 2.1.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/cli-cursor/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### cli-width\n\nName: cli-width\nVersion: 2.2.0\nProject Home: https://github.com/knownasilya/cli-width\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/cli-width/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### color-convert\n\nName: color-convert\nVersion: 1.9.3\nProject Home: https://github.com/Qix-/color-convert\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/color-convert/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### color-name\n\nName: color-name\nVersion: 1.1.3\nProject Home: https://github.com/dfcreative/color-name\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/color-name/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### concat-map\n\nName: concat-map\nVersion: 0.0.1\nProject Home: http://substack.net\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/minimatch/node_modules/concat-map/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### content-disposition\n\nName: content-disposition\nVersion: 0.5.3\nProject Home: https://github.com/jshttp/content-disposition\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/content-disposition/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### content-type\n\nName: content-type\nVersion: 1.0.4\nProject Home: https://github.com/jshttp/content-type\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/content-type/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### cross-spawn\n\nName: cross-spawn\nVersion: 6.0.5\nProject Home: https://github.com/moxystudio/node-cross-spawn\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/cross-spawn/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### debug\n\nName: debug\nVersion: 4.1.1\nProject Home: https://github.com/visionmedia/debug\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/debug/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### dedent\n\nName: dedent\nVersion: 0.7.0\nProject Home: http://desmondbrand.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/dedent/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### deep-eql\n\nName: deep-eql\nVersion: 0.1.3\nProject Home: https://github.com/chaijs/deep-eql\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/chai/node_modules/deep-eql/README.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### deep-is\n\nName: deep-is\nVersion: 0.1.3\nProject Home: http://thlorenz.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/deep-is/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### depd\n\nName: depd\nVersion: 1.1.2\nProject Home: https://github.com/dougwilson/nodejs-depd\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/http-errors/node_modules/depd/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### doctrine\n\nName: doctrine\nVersion: 3.0.0\nProject Home: https://github.com/eslint/doctrine\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/doctrine/LICENSE\nLicense Name: Apache License 2.0\nLicense Id: Apache-2.0\n\n#### emoji-regex\n\nName: emoji-regex\nVersion: 7.0.3\nProject Home: https://mathiasbynens.be/\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/emoji-regex/LICENSE-MIT.txt\nLicense Name: MIT License\nLicense Id: MIT\n\n#### error-stack-parser\n\nName: error-stack-parser\nVersion: 2.0.2\nProject Home: https://github.com/stacktracejs/error-stack-parser\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/error-stack-parser/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### eslint-scope\n\nName: eslint-scope\nVersion: 4.0.3\nProject Home: https://github.com/eslint/eslint-scope\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/eslint-scope/LICENSE\nLicense Name: BSD 2-clause \"Simplified\" License\nLicense Id: BSD-2-Clause\n\n#### eslint-utils\n\nName: eslint-utils\nVersion: 1.3.1\nProject Home: https://github.com/mysticatea/eslint-utils\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/eslint-utils/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### eslint-visitor-keys\n\nName: eslint-visitor-keys\nVersion: 1.0.0\nProject Home: https://github.com/mysticatea\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/eslint-visitor-keys/LICENSE\nLicense Name: Apache License 2.0\nLicense Id: Apache-2.0\n\n#### eslint\n\nName: eslint\nVersion: 5.16.0\nProject Home: https://github.com/eslint/eslint\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### espree\n\nName: espree\nVersion: 5.0.1\nProject Home: https://github.com/eslint/espree\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/espree/LICENSE\nLicense Name: BSD 2-clause \"Simplified\" License\nLicense Id: BSD-2-Clause\n\n#### esprima\n\nName: esprima\nVersion: 4.0.1\nProject Home: https://github.com/jquery/esprima\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/js-yaml/node_modules/esprima/LICENSE.BSD\nLicense Name: BSD 2-clause \"Simplified\" License\nLicense Id: BSD-2-Clause\n\n#### esquery\n\nName: esquery\nVersion: 1.0.1\nProject Home: https://github.com/jrfeenst/esquery\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/esquery/license.txt\nLicense Name: BSD 3-clause \"New\" or \"Revised\" License\nLicense Id: BSD-3-Clause\n\n#### esrecurse\n\nName: esrecurse\nVersion: 4.2.1\nProject Home: https://github.com/estools/esrecurse\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/esrecurse/README.md\nLicense Name: BSD 2-clause \"Simplified\" License\nLicense Id: BSD-2-Clause\n\n#### estraverse\n\nName: estraverse\nVersion: 4.2.0\nProject Home: https://github.com/estools/estraverse\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/estraverse/LICENSE.BSD\nLicense Name: BSD 2-clause \"Simplified\" License\nLicense Id: BSD-2-Clause\n\n#### esutils\n\nName: esutils\nVersion: 2.0.2\nProject Home: https://github.com/estools/esutils\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/babel-code-frame/node_modules/esutils/LICENSE.BSD\nLicense Name: BSD License\nLicense Id: BSD\n\n#### extendible\n\nName: extendible\nVersion: 0.1.1\nProject Home: https://github.com/bigpipe/extendible\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/extendible/README.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### external-editor\n\nName: external-editor\nVersion: 3.0.3\nProject Home: https://mrkmg.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/external-editor/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### fast-deep-equal\n\nName: fast-deep-equal\nVersion: 3.1.3\nProject Home: https://github.com/epoberezkin/fast-deep-equal\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/ajv/node_modules/fast-deep-equal/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### fast-deep-equal\n\nName: fast-deep-equal\nVersion: 2.0.1\nProject Home: https://github.com/epoberezkin/fast-deep-equal\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/fast-deep-equal/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### fast-json-stable-stringify\n\nName: fast-json-stable-stringify\nVersion: 2.0.0\nProject Home: http://substack.net\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/fast-json-stable-stringify/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### fast-levenshtein\n\nName: fast-levenshtein\nVersion: 2.0.6\nProject Home: http://www.hiddentao.com/\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/fast-levenshtein/LICENSE.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### figures\n\nName: figures\nVersion: 2.0.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/figures/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### file-entry-cache\n\nName: file-entry-cache\nVersion: 5.0.1\nProject Home: http://royriojas.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/file-entry-cache/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### flat-cache\n\nName: flat-cache\nVersion: 2.0.1\nProject Home: http://royriojas.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/flat-cache/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### flatted\n\nName: flatted\nVersion: 2.0.0\nProject Home: https://github.com/WebReflection/flatted\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/flatted/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### formatio\n\nName: formatio\nVersion: 1.1.1\nProject Home: https://github.com/busterjs/formatio\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/sinon/node_modules/formatio/LICENSE\nLicense Name: BSD License\nLicense Id: BSD\n\n#### fs.realpath\n\nName: fs.realpath\nVersion: 1.0.0\nProject Home: http://blog.izs.me/\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/fs.realpath/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### functional-red-black-tree\n\nName: functional-red-black-tree\nVersion: 1.0.1\nProject Home: https://github.com/mikolalysenko/functional-red-black-tree\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/functional-red-black-tree/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### glob\n\nName: glob\nVersion: 7.1.3\nProject Home: http://blog.izs.me/\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/glob/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### globals\n\nName: globals\nVersion: 11.11.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/globals/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### graphql-sync\n\nName: graphql-sync\nVersion: 0.6.2-sync\nProject Home: https://github.com/arangodb/graphql-sync\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/graphql-sync/LICENSE\nLicense Name: BSD 3-clause \"New\" or \"Revised\" License\nLicense Id: BSD-3-Clause\n\n#### graphql\n\nName: graphql\nVersion: 0.6.2\nProject Home: https://github.com/graphql/graphql-js\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/graphql-sync/node_modules/graphql/LICENSE\nLicense Name: BSD 3-clause \"New\" or \"Revised\" License\nLicense Id: BSD-3-Clause\n\n#### has-flag\n\nName: has-flag\nVersion: 3.0.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/has-flag/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### highlight.js\n\nName: highlight.js\nVersion: 10.7.3\nProject Home: https://github.com/highlightjs/highlight.js\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/highlight.js/LICENSE\nLicense Name: BSD 3-clause \"New\" or \"Revised\" License\nLicense Id: BSD-3-Clause\n\n#### hoek\n\nName: hoek\nVersion: 6.1.3\nProject Home: https://github.com/hapijs/hoek\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/joi/node_modules/hoek/LICENSE\nLicense Name: BSD 3-clause \"New\" or \"Revised\" License\nLicense Id: BSD-3-Clause\n\n#### http-errors\n\nName: http-errors\nVersion: 1.7.2\nProject Home: http://jongleberry.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/http-errors/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### i\n\nName: i\nVersion: 0.3.6\nProject Home: pksunkara.github.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/i/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### iconv-lite\n\nName: iconv-lite\nVersion: 0.6.3\nProject Home: https://github.com/ashtuchkin/iconv-lite\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/iconv-lite/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### ignore\n\nName: ignore\nVersion: 4.0.6\nProject Home: https://github.com/kaelzhang/node-ignore\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/ignore/LICENSE-MIT\nLicense Name: MIT License\nLicense Id: MIT\n\n#### import-fresh\n\nName: import-fresh\nVersion: 3.0.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/import-fresh/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### imurmurhash\n\nName: imurmurhash\nVersion: 0.1.4\nProject Home: https://github.com/homebrewing\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/imurmurhash/README.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### inflight\n\nName: inflight\nVersion: 1.0.6\nProject Home: http://blog.izs.me/\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/inflight/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### inherits\n\nName: inherits\nVersion: 2.0.1\nProject Home: https://github.com/isaacs/inherits\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/sinon/node_modules/inherits/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### inherits\n\nName: inherits\nVersion: 2.0.3\nProject Home: https://github.com/isaacs/inherits\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/inherits/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### inquirer\n\nName: inquirer\nVersion: 6.3.1\nProject Home: https://github.com/SBoudrias/Inquirer.js\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/inquirer/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### is-fullwidth-code-point\n\nName: is-fullwidth-code-point\nVersion: 2.0.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/is-fullwidth-code-point/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### is-promise\n\nName: is-promise\nVersion: 2.1.0\nProject Home: https://github.com/then/is-promise\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/is-promise/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### is-wsl\n\n* Name: is-wsl\n* Version: 2.2.0\n* Project Home: https://github.com/sindresorhus/is-wsl\n* License: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/node-netstat/node_modules/is-wsl/LICENSE\n* License Name: MIT License\n* License Id: MIT\n\n#### isemail\n\nName: isemail\nVersion: 3.2.0\nProject Home: https://github.com/hapijs/isemail\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/joi/node_modules/isemail/LICENSE\nLicense Name: BSD 3-clause \"New\" or \"Revised\" License\nLicense Id: BSD-3-Clause\n\n#### isexe\n\nName: isexe\nVersion: 2.0.0\nProject Home: http://blog.izs.me/\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/isexe/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### iterall\n\nName: iterall\nVersion: 1.0.2\nProject Home: http://leebyron.com/\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/graphql-sync/node_modules/iterall/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### joi-to-json-schema\n\nName: joi-to-json-schema\nVersion: 4.0.1\nProject Home: https://github.com/lightsofapollo/joi-to-json-schema\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/joi-to-json-schema/README.md\nLicense Name: Apache License 2.0\nLicense Id: Apache-2.0\n\n#### joi\n\nName: joi\nVersion: 14.3.1\nProject Home: https://github.com/hapijs/joi\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/joi/LICENSE\nLicense Name: BSD 3-clause \"New\" or \"Revised\" License\nLicense Id: BSD-3-Clause\n\n#### js-tokens\n\nName: js-tokens\nVersion: 3.0.2\nProject Home: https://github.com/lydell/js-tokens\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/babel-code-frame/node_modules/js-tokens/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### js-tokens\n\nName: js-tokens\nVersion: 4.0.0\nProject Home: https://github.com/lydell/js-tokens\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/js-tokens/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### js-yaml\n\nName: js-yaml\nVersion: 3.13.1\nProject Home: https://github.com/nodeca/js-yaml\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/js-yaml/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### json-schema-traverse\n\nName: json-schema-traverse\nVersion: 1.0.0\nProject Home: https://github.com/epoberezkin/json-schema-traverse\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/ajv/node_modules/json-schema-traverse/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### json-schema-traverse\n\nName: json-schema-traverse\nVersion: 0.4.1\nProject Home: https://github.com/epoberezkin/json-schema-traverse\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/json-schema-traverse/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### json-stable-stringify-without-jsonify\n\nName: json-stable-stringify-without-jsonify\nVersion: 1.0.1\nProject Home: http://substack.net\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/json-stable-stringify-without-jsonify/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### levn\n\nName: levn\nVersion: 0.3.0\nProject Home: https://github.com/gkz/levn\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/levn/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### lodash\n\nName: lodash\nVersion: 4.17.13\nProject Home: https://github.com/lodash/lodash\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/lodash/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### lolex\n\nName: lolex\nVersion: 1.3.2\nProject Home: https://github.com/sinonjs/lolex\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/sinon/node_modules/lolex/LICENSE\nLicense Name: BSD 3-clause \"New\" or \"Revised\" License\nLicense Id: BSD-3-Clause\n\n#### media-typer\n\n* Name: media-typer\n* Version: 0.3.0\n* Project Home: https://github.com/jshttp/media-typer\n* License: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/type-is/node_modules/media-typer/LICENSE\n* License Name: MIT License\n* License Id: MIT\n\n#### mime-db\n\nName: mime-db\nVersion: 1.38.0\nProject Home: https://github.com/jshttp/mime-db\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/mime-types/node_modules/mime-db/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### mime-types\n\nName: mime-types\nVersion: 2.1.31\nProject Home: https://github.com/jshttp/mime-types\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/mime-types/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### mimic-fn\n\nName: mimic-fn\nVersion: 1.2.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/mimic-fn/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### minimatch\n\nName: minimatch\nVersion: 3.0.4\nProject Home: http://blog.izs.me\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/minimatch/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### minimist\n\nName: minimist\nVersion: 0.0.8\nProject Home: http://substack.net\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/minimist/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### mkdirp\n\nName: mkdirp\nVersion: 0.5.1\nProject Home: http://substack.net\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/mkdirp/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### ms\n\nName: ms\nVersion: 2.1.3\nProject Home: https://github.com/zeit/ms\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/ms/license.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### mute-stream\n\nName: mute-stream\nVersion: 0.0.7\nProject Home: http://blog.izs.me/\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/mute-stream/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### natural-compare\n\nName: natural-compare\nVersion: 1.4.0\nProject Home: https://github.com/litejs/natural-compare-lite\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/natural-compare/README.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### negotiator\n\nName: negotiator\nVersion: 0.6.1\nProject Home: https://github.com/jshttp/negotiator\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/accepts/node_modules/negotiator/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### netmask\n\nName: netmask\nVersion: 1.0.6\nProject Home: https://github.com/rs/node-netmask\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/netmask/README.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### nice-try\n\nName: nice-try\nVersion: 1.0.5\nProject Home: https://github.com/electerious/nice-try\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/nice-try/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n\n#### node-netstat\n\nName: node-netstat\nVersion: 1.8.0 + adjustments for ArangoDB\nProject Home: https://github.com/danielkrainas/node-netstat#readme\nLicense: http://unlicense.org/UNLICENSE\nLicense Name: Unlicense / Public Domain\nLicense Id: -\n\n#### once\n\nName: once\nVersion: 1.4.0\nProject Home: http://blog.izs.me/\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/once/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### onetime\n\nName: onetime\nVersion: 2.0.1\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/onetime/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### optionator\n\nName: optionator\nVersion: 0.8.2\nProject Home: https://github.com/gkz/optionator\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/optionator/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### os-tmpdir\n\nName: os-tmpdir\nVersion: 1.0.2\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/os-tmpdir/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### parent-module\n\nName: parent-module\nVersion: 1.0.1\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/parent-module/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### path-is-absolute\n\nName: path-is-absolute\nVersion: 1.0.1\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/path-is-absolute/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### path-is-inside\n\nName: path-is-inside\nVersion: 1.0.2\nProject Home: https://domenic.me\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/path-is-inside/LICENSE.txt\nLicense Name: MIT License\nLicense Id: MIT\n\n#### path-key\n\nName: path-key\nVersion: 2.0.1\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/path-key/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### prelude-ls\n\nName: prelude-ls\nVersion: 1.1.2\nProject Home: https://github.com/gkz/prelude-ls\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/prelude-ls/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### progress\n\nName: progress\nVersion: 2.0.3\nProject Home: https://github.com/visionmedia/node-progress\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/progress/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### punycode\n\nName: punycode\nVersion: 2.1.1\nProject Home: https://mathiasbynens.be/\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/ajv/node_modules/punycode/LICENSE-MIT.txt\nLicense Name: MIT License\nLicense Id: MIT\n\n#### punycode\n\nName: punycode\nVersion: 2.1.1\nProject Home: https://mathiasbynens.be/\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/punycode/LICENSE-MIT.txt\nLicense Name: MIT License\nLicense Id: MIT\n\n#### qs\n\nName: qs\nVersion: 6.7.0\nProject Home: https://github.com/ljharb/qs\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/qs/LICENSE\nLicense Name: BSD 3-clause \"New\" or \"Revised\" License\nLicense Id: BSD-3-Clause\n\n#### range-parser\n\nName: range-parser\nVersion: 1.2.0\nProject Home: http://tjholowaychuk.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/range-parser/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### regexpp\n\nName: regexpp\nVersion: 2.0.1\nProject Home: https://github.com/mysticatea\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/regexpp/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### require-from-string\n\nName: require-from-string\nVersion: 2.0.2\nProject Home: https://github.com/floatdrop/require-from-string#readme\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/ajv/node_modules/require-from-string/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### resolve-from\n\nName: resolve-from\nVersion: 4.0.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/resolve-from/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### restore-cursor\n\nName: restore-cursor\nVersion: 2.0.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/restore-cursor/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### rimraf\n\nName: rimraf\nVersion: 2.6.3\nProject Home: http://blog.izs.me/\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/rimraf/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### run-async\n\nName: run-async\nVersion: 2.3.0\nProject Home: https://github.com/SBoudrias/run-async\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/run-async/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### rxjs\n\nName: rxjs\nVersion: 6.4.0\nProject Home: https://github.com/reactivex/rxjs\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/rxjs/LICENSE.txt\nLicense Name: Apache License 2.0\nLicense Id: Apache-2.0\n\n#### safe-buffer\n\nName: safe-buffer\nVersion: 5.1.2\nProject Home: http://feross.org\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/content-disposition/node_modules/safe-buffer/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### safer-buffer\n\nName: safer-buffer\nVersion: 2.1.2\nProject Home: https://github.com/ChALkeR\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/safer-buffer/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### samsam\n\nName: samsam\nVersion: 1.1.2\nProject Home: https://github.com/busterjs/samsam\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/sinon/node_modules/samsam/LICENSE\nLicense Name: BSD License\nLicense Id: BSD\n\n#### semver\n\nName: semver\nVersion: 5.7.0\nProject Home: https://github.com/npm/node-semver\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/semver/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### semver\n\nName: semver\nVersion: 6.0.0\nProject Home: https://github.com/npm/node-semver\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/semver/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### setprototypeof\n\nName: setprototypeof\nVersion: 1.1.1\nProject Home: https://github.com/wesleytodd/setprototypeof\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/http-errors/node_modules/setprototypeof/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### shebang-command\n\nName: shebang-command\nVersion: 1.2.0\nProject Home: github.com/kevva\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/shebang-command/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### shebang-regex\n\nName: shebang-regex\nVersion: 1.0.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/shebang-regex/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### signal-exit\n\nName: signal-exit\nVersion: 3.0.2\nProject Home: https://github.com/tapjs/signal-exit\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/signal-exit/LICENSE.txt\nLicense Name: ISC License\nLicense Id: ISC\n\n#### sinon\n\nName: sinon\nVersion: 1.17.7\nProject Home: https://github.com/cjohansen/Sinon.JS\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/sinon/LICENSE\nLicense Name: BSD 3-clause \"New\" or \"Revised\" License\nLicense Id: BSD-3-Clause\n\n#### slice-ansi\n\nName: slice-ansi\nVersion: 2.1.0\nProject Home: https://github.com/chalk/slice-ansi\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/slice-ansi/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### sprintf-js\n\nName: sprintf-js\nVersion: 1.0.3\nProject Home: http://alexei.ro/\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/js-yaml/node_modules/sprintf-js/LICENSE\nLicense Name: BSD 3-clause \"New\" or \"Revised\" License\nLicense Id: BSD-3-Clause\n\n#### stackframe\n\nName: stackframe\nVersion: 1.0.4\nProject Home: https://github.com/stacktracejs/stackframe\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/error-stack-parser/node_modules/stackframe/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### statuses\n\nName: statuses\nVersion: 1.5.0\nProject Home: https://github.com/jshttp/statuses\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/statuses/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### string-width\n\nName: string-width\nVersion: 2.1.1\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/string-width/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### string-width\n\nName: string-width\nVersion: 3.1.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/table/node_modules/string-width/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### strip-ansi\n\nName: strip-ansi\nVersion: 4.0.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/strip-ansi/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### strip-ansi\n\nName: strip-ansi\nVersion: 5.2.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/inquirer/node_modules/strip-ansi/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### strip-json-comments\n\nName: strip-json-comments\nVersion: 2.0.1\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/strip-json-comments/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### supports-color\n\nName: supports-color\nVersion: 5.5.0\nProject Home: sindresorhus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/supports-color/license\nLicense Name: MIT License\nLicense Id: MIT\n\n#### table\n\nName: table\nVersion: 5.2.3\nProject Home: http://gajus.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/table/LICENSE\nLicense Name: BSD 3-clause \"New\" or \"Revised\" License\nLicense Id: BSD-3-Clause\n\n#### text-table\n\nName: text-table\nVersion: 0.2.0\nProject Home: http://substack.net\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/text-table/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### through\n\nName: through\nVersion: 2.3.8\nProject Home: dominictarr.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/through/LICENSE.APACHE2\nLicense Name: MIT License\nLicense Id: MIT\n\n#### timezone\n\nName: timezone\nVersion: 1.0.22\nProject Home: https://github.com/bigeasy/timezone\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/timezone/README.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### tmp\n\nName: tmp\nVersion: 0.0.33\nProject Home: http://raszi.hu/\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/tmp/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### toidentifier\n\nName: toidentifier\nVersion: 1.0.0\nProject Home: https://github.com/component/toidentifier\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/http-errors/node_modules/toidentifier/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### topo\n\nName: topo\nVersion: 3.0.3\nProject Home: https://github.com/hapijs/topo\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/joi/node_modules/topo/LICENSE\nLicense Name: BSD 3-clause \"New\" or \"Revised\" License\nLicense Id: BSD-3-Clause\n\n#### tslib\n\nName: tslib\nVersion: 1.9.3\nProject Home: https://github.com/Microsoft/tslib\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/tslib/LICENSE.txt\nLicense Name: Apache License 2.0\nLicense Id: Apache-2.0\n\n#### type-check\n\nName: type-check\nVersion: 0.3.2\nProject Home: https://github.com/gkz/type-check\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/type-check/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### type-detect\n\nName: type-detect\nVersion: 0.1.1\nProject Home: http://alogicalparadox.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/chai/node_modules/deep-eql/node_modules/type-detect/README.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### type-detect\n\nName: type-detect\nVersion: 1.0.0\nProject Home: http://alogicalparadox.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/chai/node_modules/type-detect/README.md\nLicense Name: MIT License\nLicense Id: MIT\n\n#### type-is\n\nName: type-is\nVersion: 1.6.16\nProject Home: https://github.com/jshttp/type-is\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/type-is/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### uri-js\n\nName: uri-js\nVersion: 4.2.2\nProject Home: https://github.com/garycourt/uri-js\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/uri-js/README.md\nLicense Name: BSD 2-clause \"Simplified\" License\nLicense Id: BSD-2-Clause\n\n#### uri-js\n\nName: uri-js\nVersion: 4.2.2\nProject Home: https://github.com/garycourt/uri-js\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/ajv/node_modules/uri-js/README.md\nLicense Name: BSD 2-clause \"Simplified\" License\nLicense Id: BSD-2-Clause\n\n#### util\n\nName: util\nVersion: 0.10.3\nProject Home: http://www.joyent.com\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/sinon/node_modules/util/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### vary\n\nName: vary\nVersion: 1.1.2\nProject Home: https://github.com/jshttp/vary\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/vary/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### which\n\nName: which\nVersion: 1.3.1\nProject Home: http://blog.izs.me\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/which/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### wordwrap\n\nName: wordwrap\nVersion: 1.0.0\nProject Home: http://substack.net\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/wordwrap/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### wrappy\n\nName: wrappy\nVersion: 1.0.2\nProject Home: http://blog.izs.me/\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/wrappy/LICENSE\nLicense Name: ISC License\nLicense Id: ISC\n\n#### write\n\nName: write\nVersion: 1.0.3\nProject Home: https://github.com/jonschlinkert\nLicense: https://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/eslint/node_modules/write/LICENSE\nLicense Name: MIT License\nLicense Id: MIT\n\n#### @xmldom/xmldom\n\nName: @xmldom/xmldom\nVersion: 0.8.0\nProject Home: https://github.com/xmldom/xmldom\nLicense: httpsx://raw.githubusercontent.com/arangodb/arangodb/devel/js/node/node_modules/xmldom/LICENSE\nLicense Name: MIT (dual license)\nLicense Id: MIT\n"
        },
        {
          "name": "README",
          "type": "blob",
          "size": 5.150390625,
          "content": "ArangoDB\n========\n\nArangoDB is a scalable graph database system to drive value from connected data,\nfaster. Native graphs, an integrated search engine, and JSON support, via a\nsingle query language. ArangoDB runs on-prem, in the cloud – anywhere.\n\nArangoDB Cloud Service\n----------------------\n\nThe [ArangoGraph Insights Platform](https://cloud.arangodb.com/home) is the\nsimplest way to run ArangoDB. You can create deployments on all major cloud\nproviders in many regions with ease.\n\nGetting Started\n---------------\n\n- [ArangoDB University](https://university.arangodb.com/)\n- [Free Udemy Course](https://www.udemy.com/course/getting-started-with-arangodb)\n- [Training Center](https://www.arangodb.com/learn/)\n- [Documentation](https://docs.arangodb.com/)\n\nFor the impatient:\n\n- Test ArangoDB in the cloud with [ArangoGraph](https://cloud.arangodb.com/home) for free.\n\n- Alternatively, [download](https://www.arangodb.com/download) and install ArangoDB.\n  Start the server `arangod` if the installer did not do it for you.\n\n  Or start ArangoDB in a Docker container:\n\n      docker run -e ARANGO_ROOT_PASSWORD=test123 -p 8529:8529 -d arangodb\n\n  Then point your browser to `http://127.0.0.1:8529/`.\n\nKey Features of ArangoDB\n------------------------\n\n**Native Graph** - Store both data and relationships, for faster queries even\nwith multiple levels of joins and deeper insights that simply aren't possible\nwith traditional relational and document database systems.\n\n**Document Store** - Every node in your graph is a JSON document:\nflexible, extensible, and easily imported from your existing document database.\n\n**ArangoSearch** - Natively integrated cross-platform indexing, text-search and\nranking engine for information retrieval, optimized for speed and memory.\n\nArangoDB is available in a free and open-source **Community Edition**, as well\nas a commercial **Enterprise Edition** with additional features.\n\n### Community Edition features\n\n- **Horizontal scalability**: Seamlessly shard your data across multiple machines.\n- **High availability** and **resilience**: Replicate data to multiple cluster\n  nodes, with automatic failover.\n- **Flexible data modeling**: Model your data as combination of key-value pairs,\n  documents, and graphs as you see fit for your application.\n- Work **schema-free** or use **schema validation** for data consistency.\n  Store any type of data - date/time, geo-spatial, text, nested.\n- **Powerful query language** (_AQL_) to retrieve and modify data - from simple\n  CRUD operations, over complex filters and aggregations, all the way to joins,\n  graphs, and ranked full-text search.\n- **Transactions**: Run queries on multiple documents or collections with\n  optional transactional consistency and isolation.\n- **Data-centric microservices**: Unify your data storage logic, reduce network\n  overhead, and secure sensitive data with the _ArangoDB Foxx_ JavaScript framework.\n- **Fast access to your data**: Fine-tune your queries with a variety of index\n  types for optimal performance. ArangoDB is written in C++ and can handle even\n  very large datasets efficiently.\n- Easy to use **web interface** and **command-line tools** for interaction\n  with the server.\n\n### Enterprise Edition features\n\nFocus on solving enterprise-scale problems for mission critical workloads using\nsecure graph data. The Enterprise Edition has all the features of the\nCommunity Edition and offers additional features for performance, compliance,\nand security, as well as further query capabilities.\n\n- Smartly shard and replicate graphs and datasets with features like\n  **EnterpriseGraphs**, **SmartGraphs**, and **SmartJoins** for lightning fast\n  query execution.\n- Combine the performance of a single server with the resilience of a cluster\n  setup using **OneShard** deployments.\n- Increase fault tolerance with **Datacenter-to-Datacenter Replication** and\n  and create incremental **Hot Backups** without downtime.\n- Enable highly secure work with **Encryption 360**, enhanced **Data Masking**, \n  and detailed **Auditing**.\n- Perform **parallel graph traversals**.\n- Use ArangoSearch **search highlighting** and **nested search** for advanced\n  information retrieval.\n\nLatest Release\n--------------\n\nPackages for all supported platforms can be downloaded from\n<https://www.arangodb.com/download/>.\n\nFor what's new in ArangoDB, see the Release Notes in the\n[Documentation](https://docs.arangodb.com/).\n\nStay in Contact\n---------------\n\n- Please use GitHub for feature requests and bug reports:\n  [https://github.com/arangodb/arangodb/issues](https://github.com/arangodb/arangodb/issues)\n\n- Ask questions about AQL, usage scenarios, etc. on StackOverflow:\n  [https://stackoverflow.com/questions/tagged/arangodb](https://stackoverflow.com/questions/tagged/arangodb)\n\n- Chat with the community and the developers on Slack:\n  [https://arangodb-community.slack.com/](https://arangodb-community.slack.com/)\n\n- Learn more about ArangoDB with our YouTube channel: \n  [https://www.youtube.com/@ArangoDB](https://www.youtube.com/@ArangoDB)\n\n- Follow us on Twitter to stay up to date:\n  [https://twitter.com/arangodb](https://twitter.com/arangodb)\n\n- Find out more about our community: [https://www.arangodb.com/community](https://www.arangodb.com/community/)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.7216796875,
          "content": "<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://user-images.githubusercontent.com/7819991/218699214-264942f9-b020-4f50-b1a6-3363cdc0ddc9.svg\" width=\"638\" height=\"105\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/7819991/218699215-b9b4a465-45f8-4db9-b5a4-ba912541e017.svg\" width=\"638\" height=\"105\">\n  <img alt=\"Two stylized avocado halves and the product name.\" src=\"https://user-images.githubusercontent.com/7819991/218697980-26ffd7af-cf29-4365-8a5d-504b850fc6b1.png\" width=\"638\" height=\"105\">\n</picture>\n\nArangoDB\n========\n\nArangoDB is a scalable graph database system to drive value from connected data,\nfaster. Native graphs, an integrated search engine, and JSON support, via a\nsingle query language. ArangoDB runs on-prem, in the cloud – anywhere.\n\nArangoDB Cloud Service\n----------------------\n\nThe [ArangoGraph Insights Platform](https://cloud.arangodb.com/home) is the\nsimplest way to run ArangoDB. You can create deployments on all major cloud\nproviders in many regions with ease.\n\nGetting Started\n---------------\n\n- [ArangoDB University](https://university.arangodb.com/)\n- [Free Udemy Course](https://www.udemy.com/course/getting-started-with-arangodb)\n- [Training Center](https://www.arangodb.com/learn/)\n- [Documentation](https://docs.arangodb.com/)\n\nFor the impatient:\n\n- Test ArangoDB in the cloud with [ArangoGraph](https://cloud.arangodb.com/home) for free.\n\n- Alternatively, [download](https://www.arangodb.com/download) and install ArangoDB.\n  Start the server `arangod` if the installer did not do it for you.\n\n  Or start ArangoDB in a Docker container:\n\n      docker run -e ARANGO_ROOT_PASSWORD=test123 -p 8529:8529 -d arangodb\n\n  Then point your browser to `http://127.0.0.1:8529/`.\n\nKey Features of ArangoDB\n------------------------\n\n**Native Graph** - Store both data and relationships, for faster queries even\nwith multiple levels of joins and deeper insights that simply aren't possible\nwith traditional relational and document database systems.\n\n**Document Store** - Every node in your graph is a JSON document:\nflexible, extensible, and easily imported from your existing document database.\n\n**ArangoSearch** - Natively integrated cross-platform indexing, text-search and\nranking engine for information retrieval, optimized for speed and memory.\n\nArangoDB is available in a free and open-source **Community Edition**, as well\nas a commercial **Enterprise Edition** with additional features.\n\n### Community Edition features\n\n- **Horizontal scalability**: Seamlessly shard your data across multiple machines.\n- **High availability** and **resilience**: Replicate data to multiple cluster\n  nodes, with automatic failover.\n- **Flexible data modeling**: Model your data as combination of key-value pairs,\n  documents, and graphs as you see fit for your application.\n- Work **schema-free** or use **schema validation** for data consistency.\n  Store any type of data - date/time, geo-spatial, text, nested.\n- **Powerful query language** (_AQL_) to retrieve and modify data - from simple\n  CRUD operations, over complex filters and aggregations, all the way to joins,\n  graphs, and ranked full-text search.\n- **Transactions**: Run queries on multiple documents or collections with\n  optional transactional consistency and isolation.\n- **Data-centric microservices**: Unify your data storage logic, reduce network\n  overhead, and secure sensitive data with the _ArangoDB Foxx_ JavaScript framework.\n- **Fast access to your data**: Fine-tune your queries with a variety of index\n  types for optimal performance. ArangoDB is written in C++ and can handle even\n  very large datasets efficiently.\n- Easy to use **web interface** and **command-line tools** for interaction\n  with the server.\n\n### Enterprise Edition features\n\nFocus on solving enterprise-scale problems for mission critical workloads using\nsecure graph data. The Enterprise Edition has all the features of the\nCommunity Edition and offers additional features for performance, compliance,\nand security, as well as further query capabilities.\n\n- Smartly shard and replicate graphs and datasets with features like\n  **EnterpriseGraphs**, **SmartGraphs**, and **SmartJoins** for lightning fast\n  query execution.\n- Combine the performance of a single server with the resilience of a cluster\n  setup using **OneShard** deployments.\n- Increase fault tolerance with **Datacenter-to-Datacenter Replication** and\n  and create incremental **Hot Backups** without downtime.\n- Enable highly secure work with **Encryption 360**, enhanced **Data Masking**, \n  and detailed **Auditing**.\n- Perform **parallel graph traversals**.\n- Use ArangoSearch **search highlighting** and **nested search** for advanced\n  information retrieval.\n\nLatest Release\n--------------\n\nPackages for all supported platforms can be downloaded from\n<https://www.arangodb.com/download/>.\n\nFor what's new in ArangoDB, see the Release Notes in the\n[Documentation](https://docs.arangodb.com/).\n\nStay in Contact\n---------------\n\n- Please use GitHub for feature requests and bug reports:\n  [https://github.com/arangodb/arangodb/issues](https://github.com/arangodb/arangodb/issues)\n\n- Ask questions about AQL, usage scenarios, etc. on StackOverflow:\n  [https://stackoverflow.com/questions/tagged/arangodb](https://stackoverflow.com/questions/tagged/arangodb)\n\n- Chat with the community and the developers on Slack:\n  [https://arangodb-community.slack.com/](https://arangodb-community.slack.com/)\n\n- Learn more about ArangoDB with our YouTube channel: \n  [https://www.youtube.com/@ArangoDB](https://www.youtube.com/@ArangoDB)\n\n- Follow us on Twitter to stay up to date:\n  [https://twitter.com/arangodb](https://twitter.com/arangodb)\n\n- Find out more about our community: [https://www.arangodb.com/community](https://www.arangodb.com/community/)\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.45703125,
          "content": "# Security Policy\n\n## Supported Versions\n\nWe release patches for security vulnerabilities for [supported\nversions](https://www.arangodb.com/subscriptions/end-of-life-notice/).\n\n## Reporting a Vulnerability\n\nPlease report (suspected) security vulnerabilities to\n**[security@arangodb.com](mailto:security@arangodb.com)**. You will\nreceive a response from us within 72 hours. If the issue is confirmed,\nwe will release a patch as soon as possible depending on complexity."
        },
        {
          "name": "STYLEGUIDE.md",
          "type": "blob",
          "size": 15.8486328125,
          "content": "# ArangoDB Clang-Format\n\nArangoDB uses `clang-format` to lint the codebase of [arangodb/arangodb](https://github.com/arangodb/arangodb) & [arangodb/enterprise](https://github.com/arangodb/enterprise).\n\nYou are expected to format your local changes via clang-format. Here are your options:\n\n1. Lint with a **pre-commit git hook** (OS-dependent)\n    * pre-requisites\n        * Docker\n        * Git 2.9+\n        * On Linux: bash, with some common tools installed\n    * how-to (one-time setup)\n        * `cd arangodb`\n        * `cp .githooks/pre-commit .git/hooks/pre-commit`\n        * `git config core.hooksPath .git/hooks`\n        * `cd enterprise`\n        * `cp .githooks/pre-commit .git/hooks/pre-commit`\n        * `git config core.hooksPath .git/hooks`\n\n   The commit hook will automatically verify that all to-be-committed files are\n   correctly formatted, and will abort the commit process in case of a formatting\n   violation.\n\n   Note that using `git commit -a` invokes the pre-commit check **before** adding\n   the files, so that the pre-commit check will run on the wrong version of the\n   file. Thus using `git commit -a` is not supported.\n\n2. Reformat code with a **local shell script** (OS-dependent)\n    * pre-requisites\n        * Docker\n        * Git 2.9+\n        * On Linux: bash, with some common tools installed\n    * how-to (must run manually)\n        * `cd arangodb`\n        * `./scripts/clang-format.sh`\n\n3. Lint with your **custom clang-format setup**\n    * This would be up to you to configure, if you do not want to rely on Docker and/or already have a way of automatically linting your changes via clang-format. We still highly encourage that you use `clang-format 12` or higher.\n\nOnce your changes are pushed, a [GitHub Action CI](https://github.com/arangodb/clang-format-action) will be triggered to validate that your changes comply to the [established clang-format standard](https://github.com/arangodb/arangodb/blob/devel/.clang-format). Should you be missing any format specifications, your push’s CI will fail.\n\n\n# ArangoDB coding guidelines\n\nThis document is mostly derived from [Google C++ Style\nGuide](https://google.github.io/styleguide/cppguide.html#C++_Version) with some\nArangoDB specific adjustments.\n\n## Preamble\n\nThese coding guidelines represent the C++ code style that we want to follow at ArangoDB.\nAs the ArangoDB code base has evolved over a period of around 10 years, there still exists\nsome code which does not or not fully follow these guidelines. Older code that is not following\nthe guidelines described here will eventually be adjusted to follow these guidelines. This will\ntake time.\nAny _newly_ added code however should follow the guidelines described here. When in doubt,\nplease always follow the coding guidelines described here.\n\n## Naming\n\nThe most important consistency rules are those that govern naming. The style of\na name immediately informs us what sort of thing the named entity is: a type, a\nvariable, a function, a constant, a macro, etc., without requiring us to search\nfor the declaration of that entity. The pattern-matching engine in our brains\nrelies a great deal on these naming rules.\n\nNaming rules are pretty arbitrary, but we feel that consistency is more\nimportant than individual preferences in this area, so regardless of whether you\nfind them sensible or not, the rules are the rules.\n\n### General Naming Rules\n\nOptimize for readability using names that would be clear even to people on a\ndifferent team.\n\nUse names that describe the purpose or intent of the object. Do not worry about\nsaving horizontal space as it is far more important to make your code\nimmediately understandable by a new reader. Minimize the use of abbreviations\nthat would likely be unknown to someone outside your project (especially\nacronyms and initialisms). Do not abbreviate by deleting letters within a word.\nAs a rule of thumb, an abbreviation is probably OK if it's listed in Wikipedia.\nGenerally speaking, descriptiveness should be proportional to the name's scope\nof visibility. For example, n may be a fine name within a 5-line function, but\nwithin the scope of a class, it's likely too vague.\n\n```\nclass MyClass {\n public:\n  int countFooErrors(std::vector<Foo> const& foos) {\n    int n = 0;  // Clear meaning given limited scope and context\n    for (auto const& foo : foos) {\n      ...\n      ++n;\n    }\n    return n;\n  }\n  void doSomethingImportant() {\n    std::string fqdn = ...;  // Well-known abbreviation for Fully Qualified Domain Name\n  }\n private:\n  int const kMaxAllowedConnections = ...;  // Clear meaning within context\n};\n\nclass MyClass {\n public:\n  int countFooErrors(std::vector<Foo> const& foos) {\n    int totalNumberOfFooErrors = 0;  // Overly verbose given limited scope and context\n    for (size_t fooIndex = 0; fooIndex < foos.size(); ++fooIndex) {  // Use idiomatic `i`\n      ...\n      ++totalNumberOfFooErrors;\n    }\n    return totalNumberOfFooErrors;\n  }\n  void doSomethingImportant() {\n    int cstmrId = ...;  // Deletes internal letters\n  }\n private:\n  int const kNum = ...;  // Unclear meaning within broad scope\n};\n```\n\nNote that certain universally-known abbreviations are OK, such as i for an\niteration variable and T for a template parameter.\n\nFor the purposes of the naming rules below, a \"word\" is anything that you would\nwrite in English without internal spaces. This includes abbreviations, such as\nacronyms and initialisms. For names written in mixed case (also sometimes\nreferred to as \"[camel case](https://en.wikipedia.org/wiki/Camel_case)\" or\n\"[Pascal case](https://en.wiktionary.org/wiki/Pascal_case)\"), in which the first\nletter of each word except the first one is capitalized, prefer to capitalize\nabbreviations as single words, e.g., startRpc() rather than startRPC().\n\nTemplate parameters should follow the naming style for their category: type\ntemplate parameters should follow the rules for type names, and non-type\ntemplate parameters should follow the rules for variable names.\n\n### File Names\n\nFilenames start with a capital letter and have a capital letter for each new\nword, with no underscores: `MyUsefulClass.cpp`, `VeryUsefulClass.cpp`\n\nC++ files should end in `.cpp` and header files should end in `.h`. Files that\nrely on being textually included at specific points should end in `.inc` (see\nalso the section on self-contained headers). Do not use filenames that already\nexist in `/usr/include`, such as `db.h`. In general, make your filenames very\nspecific. For example, use `HttpServerLogs.h` rather than `Logs.h`. A very\ncommon case is to have a pair of files called, e.g., `FooBar.h` and\n`FooBar.cpp`, defining a class called `FooBar`.\n\n### Type Names\n\nType names start with a capital letter and have a capital letter for each new\nword, with no underscores: `MyExcitingClass`, `MyExcitingEnum`.\n\nThe names of all types — classes, structs, type aliases, enums, and type\ntemplate parameters — have the same naming convention. Type names should start\nwith a capital letter and have a capital letter for each new word. No\nunderscores. For example:\n\n```\n// classes and structs\nclass UrlTable { ...\nclass UrlTableTester { ...\nstruct UrlTableProperties { ...\n\n// typedefs\ntypedef hash_map<UrlTableProperties*, std::string> PropertiesMap;\n\n// using aliases\nusing PropertiesMap = hash_map<UrlTableProperties*, std::string>;\n\n// enums\nenum class UrlTableError { ...\n```\n\n### Variable Names\n\nThe names of variables (including function parameters) should follow \"[camel\ncase](https://en.wikipedia.org/wiki/Camel_case)\" practice with no underscores\nbetween words. Data members of classes (but not structs) additionally have\nleading underscores. For instance: `aLocalVariable`, `aStructDataMember`,\n`_aClassDataMember`.\n\n#### Common Variable names\n\nFor example:\n\n```\nstd::string table_name;  // BAD - lowercase with underscore.\n\nstd::string tableName;   // Ok - mixed case.\n```\n\n#### Class Data Members\n\nData members of classes, both static and non-static, are named like ordinary\nnonmember variables, but with a leading underscore.\n\n```\nclass TableInfo {\n  ...\n private:\n  std::string _tableName;  // OK - mixed case with leading underscore\n  static Pool<TableInfo>* _pool;  // OK.\n};\n```\n\n#### Struct Data Members\n\nData members of structs, both static and non-static, are named like ordinary\nnonmember variables. They do not have the leading underscores that data members\nin classes have.\n\n```\nstruct UrlTableProperties {\n  std::string name;\n  int numEntries;\n  static Pool<UrlTableProperties>* pool;\n};\n```\n\n#### Constant Names\n\nVariables declared constexpr or const, and whose value is fixed for the duration\nof the program, are named with a leading \"k\" followed by mixed case. Underscores\ncan be used as separators in the rare cases where capitalization cannot be used\nfor separation. For example:\n\n```\nint const kDaysInAWeek = 7;\nint const kAndroid8_0_0 = 24;  // Android 8.0.0\n```\n\nAll such variables with static storage duration (i.e., statics and globals, see\nStorage Duration for details) should be named this way. This convention is\noptional for variables of other storage classes, e.g., automatic variables,\notherwise the usual variable naming rules apply.\n\n#### Function Names\n\nRegular functions have mixed case; accessors and mutators may be named like\nvariables.\n\nOrdinarily, functions should start with a non-capital letter and have a capital\nletter for each new word.\n\n```\naddTableEntry()\ndeleteUrl()\nopenFileOrDie()\n```\n\n(The same naming rule applies to class- and namespace-scope constants that are\nexposed as part of an API and that are intended to look like functions, because\nthe fact that they're objects rather than functions is an unimportant\nimplementation detail.)\n\nAccessors and mutators (get and set functions) may be named like variables.\nThese often correspond to actual member variables, but this is not required. For\nexample, `int count()` and void `setCount(int count)`.\n\n#### Namespace Names\n\nNamespace names are all lower-case, with words separated by underscores.\nTop-level namespace names are based on the project name . Avoid collisions\nbetween nested namespaces and well-known top-level namespaces.\n\nThe code in that namespace should usually be in a directory whose basename\nmatches the namespace name (or in subdirectories thereof).\n\nAvoid nested namespaces that match well-known top-level namespaces. Collisions\nbetween namespace names can lead to surprising build breaks because of name\nlookup rules. In particular, do not create any nested std namespaces. Prefer\nunique project identifiers (`websearch::index`, `websearch::index_util`) over\ncollision-prone names like `websearch::util`. Also avoid overly deep nesting\nnamespaces ([TotW #130](https://abseil.io/tips/130)).\n\nFor `internal` namespaces, be wary of other code being added to the same `internal`\nnamespace causing a collision (internal helpers within a team tend to be related\nand may lead to collisions). In such a situation, using the filename to make a\nunique internal name is helpful (`websearch::index::frobber_internal` for use in\n`frobber.h`).\n\n#### Enumerator Names\n\nEnumerators (for both scoped and unscoped enums) should be named like constants,\nnot like macros. That is, use `kEnumName` not `ENUM_NAME`.\n\n```\nenum class UrlTableError {\n  kOk = 0,\n  kOutOfMemory,\n  kMalformedInput,\n};\n```\n```\n// BAD practice\nenum class AlternateUrlTableError {\n  OK = 0,\n  OUT_OF_MEMORY = 1,\n  MALFORMED_INPUT = 2,\n};\n```\n\n#### Macro Names\n\nYou're not really going to define a macro, are you? If you do, they're like this: \n`MY_MACRO_THAT_SCARES_SMALL_CHILDREN_AND_ADULTS_ALIKE`.\n\nPlease see the description of macros; in general macros should not be used.\nHowever, if they are absolutely needed, then they should be named with all\ncapitals and underscores.\n\n```\n#define ROUND(x) ...\n#define PI_ROUNDED 3.0\n```\n\n### Comments\n\nComments are absolutely vital to keeping our code readable. The following rules\ndescribe what you should comment and where. But remember: while comments are\nvery important, the best code is self-documenting. Giving sensible names to\ntypes and variables is much better than using obscure names that you must then\nexplain through comments.\n\nWhen writing your comments, write for your audience: the next contributor who\nwill need to understand your code. Be generous — the next one may be you!\n\n#### Comment Style\n\nUse either the `//` or `/* */` syntax, as long as you are consistent.\n\nYou can use either the `//` or the `/* */` syntax; however, `//` is much more\ncommon. Be consistent with how you comment and what style you use where.\n\n#### File Comments\n\nStart each file with license boilerplate. \n\nFile comments describe the contents of a file. If a file declares, implements,\nor tests exactly one abstraction that is documented by a comment at the point of\ndeclaration, file comments are not required. All other files must have file\ncomments.\n\n#### Legal Notice and Author Line\n\nEvery file should contain license boilerplate. If you make significant changes\nto a file with an author line, consider deleting the author line. Use\n[non-enterprise](#non-enterprise-template) template for public arangodb\ncodebase, [enterprise template](#enterprise-template) for the enterprise\npart.\n\n##### Non-enterprise template\n\n```\n////////////////////////////////////////////////////////////////////////////////\n/// DISCLAIMER\n///\n/// Copyright 2014-2021 ArangoDB GmbH, Cologne, Germany\n/// Copyright 2004-2014 triAGENS GmbH, Cologne, Germany\n///\n/// Licensed under the Apache License, Version 2.0 (the \"License\");\n/// you may not use this file except in compliance with the License.\n/// You may obtain a copy of the License at\n///\n///     http://www.apache.org/licenses/LICENSE-2.0\n///\n/// Unless required by applicable law or agreed to in writing, software\n/// distributed under the License is distributed on an \"AS IS\" BASIS,\n/// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n/// See the License for the specific language governing permissions and\n/// limitations under the License.\n///\n/// Copyright holder is ArangoDB GmbH, Cologne, Germany\n///\n/// @author Jan Steemann\n////////////////////////////////////////////////////////////////////////////////\n```\n\n##### Enterprise template\n```\n////////////////////////////////////////////////////////////////////////////////\n/// DISCLAIMER\n///\n/// Copyright 2021 ArangoDB GmbH, Cologne, Germany\n///\n/// The Programs (which include both the software and documentation) contain\n/// proprietary information of ArangoDB GmbH; they are provided under a license\n/// agreement containing restrictions on use and disclosure and are also\n/// protected by copyright, patent and other intellectual and industrial\n/// property laws. Reverse engineering, disassembly or decompilation of the\n/// Programs, except to the extent required to obtain interoperability with\n/// other independently created software or as specified by law, is prohibited.\n///\n/// It shall be the licensee's responsibility to take all appropriate fail-safe,\n/// backup, redundancy, and other measures to ensure the safe use of\n/// applications if the Programs are used for purposes such as nuclear,\n/// aviation, mass transit, medical, or other inherently dangerous applications,\n/// and ArangoDB GmbH disclaims liability for any damages caused by such use of\n/// the Programs.\n///\n/// This software is the confidential and proprietary information of ArangoDB\n/// GmbH. You shall not disclose such confidential and proprietary information\n/// and shall use it only in accordance with the terms of the license agreement\n/// you entered into with ArangoDB GmbH.\n///\n/// Copyright holder is ArangoDB GmbH, Cologne, Germany\n///\n/// @author Dr. Frank Celler\n////////////////////////////////////////////////////////////////////////////////\n```\n\n#### File Contents\n\nIf a `.h` declares multiple abstractions, the file-level comment should broadly\ndescribe the contents of the file, and how the abstractions are related. A 1 or\n2 sentence file-level comment may be sufficient. The detailed documentation\nabout individual abstractions belongs with those abstractions, not at the file\nlevel.\n\nDo not duplicate comments in both the .h and the .cpp. Duplicated comments\ndiverge.\n"
        },
        {
          "name": "VERSIONS",
          "type": "blob",
          "size": 0.2138671875,
          "content": "CXX_STANDARD \"20\"\nSTARTER_REV \"v0.19.8\"\nGCC_LINUX \"13.2.0\"\nCLANG_LINUX \"16.0.6\"\nOPENSSL_LINUX   \"3.4.0\"\nUSE_RCLONE \"true\"\nRCLONE_VERSION \"1.65.2\"\nMINIMAL_DEBUG_INFO \"On\"\nDEFAULT_ARCHITECTURE \"sandy-bridge\"\nUSE_ARM \"On\"\n"
        },
        {
          "name": "arangod",
          "type": "tree",
          "content": null
        },
        {
          "name": "client-tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "etc",
          "type": "tree",
          "content": null
        },
        {
          "name": "js",
          "type": "tree",
          "content": null
        },
        {
          "name": "lib",
          "type": "tree",
          "content": null
        },
        {
          "name": "lsan_arangodb_suppressions.txt",
          "type": "blob",
          "size": 1.146484375,
          "content": "leak:create_conn\nleak:CRYPTO_zalloc\n# TODO(MBkkt) Should be removed when we update snowball\nleak:snowball\n\n# TODO(MBkkt) Should be fixed, but now we don't know how\n# Also it probably only last Buffer, so we don't really care\n# Direct leak of 80 byte(s) in 1 object(s) allocated from:\n#  0 in operator new(unsigned long)\n#  1 in V8Buffer::New(v8::FunctionCallbackInfo<v8::Value> const&) /work/ArangoDB/lib/V8/v8-buffer.cpp:567\n#  2 in v8::internal::FunctionCallbackArguments::Call(v8::internal::CallHandlerInfo)\n# Indirect leak of 59 byte(s) in 1 object(s) allocated from:\n#  0 in operator new[](unsigned long)\n#  1 in V8Buffer::replace(v8::Isolate*, char*, unsigned long, void (*)(char*, void*), void*, bool) /work/ArangoDB/lib/V8/v8-buffer.cpp:735\n#  2 in V8Buffer /work/ArangoDB/lib/V8/v8-buffer.cpp:678\n#  3 in V8Buffer::New(v8::FunctionCallbackInfo<v8::Value> const&) /work/ArangoDB/lib/V8/v8-buffer.cpp:567\n#  4 in v8::internal::FunctionCallbackArguments::Call(v8::internal::CallHandlerInfo) \n# SUMMARY: AddressSanitizer: 139 byte(s) leaked in 2 allocation(s).\nleak:V8Buffer::New\n\n# Suppressed to see other issue\n# TODO(mpoeter) ?\nleak:LogContext::Current::appendEntry\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tsan_arangodb_suppressions.txt",
          "type": "blob",
          "size": 3.6806640625,
          "content": "race:boost/lockfree/queue.hpp\nrace:boost/lockfree/detail/freelist.hpp\nrace:boost::lockfree::queue\n\n# These operators contain an assertion that checks the refCount.\n# That read operation is racy, but since it is just an assert we don't care!\nrace:SharedAqlItemBlockPtr::operator*\nrace:SharedAqlItemBlockPtr::operator->\n\n# logCrashInfo calls LOG_TOPIC which in turn calls std::string::reserve\nsignal:lib/Basics/CrashHandler.cpp\nsignal:crashHandlerSignalHandler\n\nsignal:c_exit_handler\n\n# alloc in signal handers strack trace here:\nsignal:arangodb::application_features::ApplicationServer::wait\n\n    \n# A compiler optimization in DBImpl::ReleaseSnapshot() produces code where a\n# register is populated with different addresses based on some condition, and\n# this register is later read to populate the variable `oldest_snapshot`.\n# However, this generated read is a non-atomic read, which therefore results in\n# a false positive race warning. I have created an according GitHub issue:\n# https://github.com/google/sanitizers/issues/1398\nrace:VersionSet::SetLastSequence\n\n# The following scenario is flagged by TSAN: T1(M0 -> M1), T2(M1 -> M2), T3(M2 -> M0)\n# T1 establishes leadership\n#   - calls `LogLeader::executeAppendEntriesRequests` and acquires M0 (LogLeader::_guardedLeaderData)\n#   - calls `ReplicatedStateManager::leadershipEstablished` and acquires M1 (ReplicatedStateManager::_guarded)\n# T2 creates the shard\n#   - calls `ReplicatedStateManager::getLeader` and acquires M1 (ReplicatedStateManager::_guarded)\n#   - calls `LeaderStateManager::getStateMachine` and acquires M2 (LeaderStateManager::_guardedData)\n# T3 does recovery\n#   - calls `LeaderStateManager::recoverEntries` and acquires M2 (LeaderStateManager::_guardedData)\n#   - calls `LogLeader::triggerAsyncReplication` and acquires M0 (LogLeader::_guardedLeaderData)\n# It is a false positive because:\n# * T3 (recovery) is spawned due to T1 (leadership established) and it is guaranteed that T1 already holds M0 and\n#   M1 before T3 is started. T1 will definitely finish and release its locks, regardless of what other threads are\n#   doing.\n# * T2 (shard creation) may only do significant work if T3 (recovery) has already finished\n#   (see `LeaderStateManager<S>::getStateMachine()`). Therefore, if T2 acquires M2 before T3 has started,\n#   it will release its locks and try again later, because the leader state is unusable unless recovery is completed.\ndeadlock:replication2::replicated_log::LogLeader::triggerAsyncReplication\ndeadlock:replication2::replicated_state::ReplicatedStateManager\n\n# the reference counting of exception_ptr is part of the prebuilt glibc and is\n# therefore not instrumented by TSan, so TSan is not aware of the synchronization\n# that happens there. That means we have to ignore all races in destructors of\n# exceptions as a result of a __exception_ptr release that causes the refcnt to\n# drop to zero.\nrace:std::__exception_ptr\n\n# TODO - this should be removed once BTS-685 is fixed\nrace:AllowImplicitCollectionsSwitcher\nrace:graph::RefactoredTraverserCache::appendVertex\n\n# TODO Fix known thread leaks\nthread:ClusterFeature::startHeartbeatThread\nthread:CacheManagerFeature::start\nthread:DatabaseFeature::start\n\n# TODO Fix lock order inversion\ndeadlock:consensus::Agent::setPersistedState\n\n# TODO Fix data race in arangodbtests\nrace:DummyConnection::sendRequest\n\n# ATM we build V8 with sanitizers _disabled_, so the V8 code is not instrumented\n# which means the sanitizer cannot observe the synchronizations inside that code.\n# So when the V8 code calls back into instrumented code (e.g. the C runtime), TSan\n# might report potential data races because of these missing synchronizations.\n# For that reason we simply need to ignore _all_ data races coming from V8.\nrace:v8::*\n"
        },
        {
          "name": "tsan_blacklist.txt",
          "type": "blob",
          "size": 0.2294921875,
          "content": "# blacklist functions with known races to avoid instrumentation and the overhead involved\n# note that we have to used mangled names here!\n\n# boost::lockfree::* namespace\nfun:_ZN5boost8lockfree*\n\n# v8::* namespace\nfun:ZN2v8*\nfun:_ZN2v8*"
        },
        {
          "name": "ubsan_arangodb_suppressions.txt",
          "type": "blob",
          "size": 1.5048828125,
          "content": "null:arangodb::ExecContext\n\n# fix issues with RocksDB library\n# (potentially no issues in RocksDB, but issues with UBSan\n# failing to understand thread-local variables properly)\n# there is also a pending issue in upstream RocksDB:\n# https://github.com/facebook/rocksdb/issues/10205\n# we may get rid of our own suppressions once the upstream\n# issue is fixed.\nnull:3rdParty/rocksdb/db/memtable.cc\nnull:3rdParty/rocksdb/db/db_iter.cc\nnull:3rdParty/rocksdb/db/db_impl/db_impl.cc\nnull:3rdParty/rocksdb/env/fs_posix.cc\nnull:3rdParty/rocksdb/file/random_access_file_reader.cc\nnull:3rdParty/rocksdb/file/writable_file_writer.cc\nnull:3rdParty/rocksdb/monitoring/perf_step_timer.h\nnull:3rdParty/rocksdb/util/user_comparator_wrapper.h\nalignment:3rdParty/rocksdb/util/crc32c.cc\n\n# fix issues with S2 library\nvptr:region_coverer.cc\nvptr:s2cell.h\nvptr:s2cell.cc\n\n# fix some V8 false positive\n# TODO(MBkkt) I don't think it's false positive, but probably issue in our version of V8, my issue about arangodbtests\nvptr:v8::Platform\n# TODO(MBkkt) Some issue in our version of V8, only in arangodbtests\nvptr:v8::internal::Isolate::~Isolate\n\n# Applying zero offset to null pointer not UB at least in C++20\n# https://github.com/libjpeg-turbo/libjpeg-turbo/issues/470\npointer-overflow:3rdParty/nghttp2/lib/nghttp2_buf.c\npointer-overflow:3rdParty/lz4/lib/lz4.c\n\n# It's intentional perform operations with nullptr\n# TODO(MBkkt) I think it can be fixed with using uintptr_t instead of pointer\npointer-overflow:3rdParty/iresearch/core/search/bitset_doc_iterator.cpp\n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}