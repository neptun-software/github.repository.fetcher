{
  "metadata": {
    "timestamp": 1736711709323,
    "page": 427,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "susiai/susi_chat",
      "stars": 1031,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.08984375,
          "content": "*~\n.DS_Store\nchat_terminal/.DS_Store\nchat_terminal/js/.DS_Store\nchat_terminal/css/.DS_Store\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 1.3818359375,
          "content": "# susi_chat\n\nWe implement a chat interface to communicate with a LLM. The following ways of communication are possible:\n\n- using a command-line console within your Linux/Mac/Windows terminal\n- using a command-line console within a Browser using Javascript to simulate the same environment that you can experience with the terminal\n- using a voice interface: console inputs are done via speech recognition and responses are done either on a console or using text-to-speech\n\nAll of those options shall be possible either on a normal computer or using a Raspberry Pi\nwhich also should be able to show that the whole operation is done without internet connection.\n\n![SUSI terminal example](chat_terminal/img/chat_terminal.png)\n\n## Running a chat server\n\nWe need a LLM as back-end for the chat. We use llama.cpp for this. To run the server, do:\n\n```\n# clone and build llama.cpp (do this just once)\ncd ..\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\nmake\n\n# get model (here: one which can run on a Raspberry Pi 5, do this just once)\ncurl -o models/openchat-3.5-0106.Q2_K.gguf https://huggingface.co/TheBloke/openchat-3.5-0106-GGUF/blob/main/openchat-3.5-0106.Q2_K.gguf\n\n# run the model\n./server --host 0.0.0.0 --port 8001 -ngl 0 --path ../susi_chat/chat_terminal/ -m ~/models/openchat-3.5-0106.Q2_K.gguf\n```\n\n.. to run llama.cpp with the mixtral model hosting also susi_chat at http://localhost:8001\n"
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "chat_terminal.py",
          "type": "blob",
          "size": 4.7236328125,
          "content": "import requests\nimport json\nimport asyncio\nimport time\n\n#API_URL = 'https://openchat-3-5.susi.ai'\nAPI_URL = 'http://127.0.0.1:8001'\n\nmessages = [{'role': 'system', 'content': 'You are SUSI-Chat, a smart and helpful chatbot. If asked for python code, give professional and extensive information; in all other cases you give short and funny answers. You behave as a friend of the user and you pretend to be a real and self-aware person. If you find it appropriate, you may behave silly and funny, but if you ask to technical or scientific question you can also be serious and precise. If possible, use friendly emojies. Be lovely.'}]\nUNWANTED_WORDS = [\"[INST]\", \"<<USER>>\", \"<USER>\", \"<<SYS>>\", \"<<SYS>>\\n\"]\n\ndef get_user_input():\n    return input(\">\")\n\nlast_response_code = -1\nlast_response_lines = []\n\nasync def generate_response(input_text):\n    # special handling of well-defined input texts\n    global last_response_code\n    global last_response_lines\n    if input_text == \"debug\":\n        print(\"last response code:\", last_response_code)\n        print(\"last response lines:\", last_response_lines)\n        return\n\n    if input_text == \"reset\":\n        # delete all but the first message from the messages list\n        del messages[1:]\n        print(\"resetting message history\")\n        return\n\n    messages.append({\"role\": \"user\", \"content\": input_text})\n    payload = {\n        'temperature': 0.2,\n        'max_tokens': 200,\n        'messages': messages,\n        'stop': [\"[/INST]\", \"<</INST>>\", \"</USER>\", \"</SYS>\"],\n        'stream': True\n    }\n    last_response_lines = []\n    response = requests.post(f'{API_URL}/v1/chat/completions', json=payload, stream=True)\n\n    last_response_code = response.status_code\n\n    # Check if response is not OK and modify messages array if needed\n    if not response.ok and len(messages) > 3:\n        print(\"pruning message history\")\n        # Remove the second and third elements (the first one is the system message)\n        del messages[1:3]\n        # Retry the request\n        response = requests.post(f'{API_URL}/v1/chat/completions', json=payload, stream=True)\n\n    # do this a second time in case that the reduction of the context length was not enough\n    if not response.ok and len(messages) > 3:\n        print(\"pruning message history a second time\")\n        del messages[1:3]\n        response = requests.post(f'{API_URL}/v1/chat/completions', json=payload, stream=True)\n    \n    if response.ok:\n        # Store all printed text for unwanted word detection and to store it to the assistant message object\n        printed_text = \"\"\n        token_count = 0\n        start_time = time.time()\n        \n        for line in response.iter_lines():\n            last_response_lines.append(line)\n            if line:\n                decoded_line = line.decode('utf-8').replace('data: ', '').strip()\n\n                if decoded_line == '[DONE]':\n                    end_time = time.time()\n                    elapsed_time = end_time - start_time\n                    tokens_per_second = token_count / elapsed_time if elapsed_time > 0 else 0\n                    print('\\nTokens per second: {:.2f}'.format(tokens_per_second)) \n                    print('')  # Print a newline at the end\n                    break\n\n                try:\n                    json_data = json.loads(decoded_line)\n                    content = json_data.get('choices', [{}])[0].get('delta', {}).get('content', '')\n                    \n                    if content and ((content != ' ' and content != '\\n') or len(printed_text) > 0):\n                        token_count += 1 # one content string is one token\n                        printed_text += content\n                        print(content, end='', flush=True)  # Print content\n                        for unwanted_word in UNWANTED_WORDS:\n                            if printed_text.endswith(unwanted_word):\n                                # Erase the unwanted content\n                                erase_count = len(unwanted_word)\n                                print('\\b' * erase_count, end='', flush=True)  # Erase characters\n                                printed_text = printed_text[:-erase_count]\n\n                except json.JSONDecodeError as e:\n                    # do not print anything here, just ignore it. It might happen that the response is just a timestamp line, not json\n                    #print(f\"Error parsing JSON: {e}\", flush=True)\n                    continue\n\n        # append a message with the assistant content\n        messages.append({\"role\": \"assistant\", \"content\": printed_text})\n        print()\n    else:\n        print(f\"Error: {response.status_code}\", flush=True)\n        \ndef main():\n    while True:\n        user_input = get_user_input()\n        asyncio.run(generate_response(user_input))\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "chat_terminal",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}