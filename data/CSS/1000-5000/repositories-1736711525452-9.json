{
  "metadata": {
    "timestamp": 1736711525452,
    "page": 9,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ParisNeo/lollms-webui",
      "stars": 4436,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".devcontainer",
          "type": "tree",
          "content": null
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.7890625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\n!web/dist\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n!web/.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# Database\n*.db\n\n# Docker files\n/data\n\n# models\n*.ckpt\n*.safetensors\n*.bin\n*.model\n\n# Temporary files\ntmp/\ntemp/\n# configurations other than the default one\nconfigs/*\n!configs/config.yaml\n\n# personalities other than the default one\npersonalities/*\n!personalities/english/default/*\n\n# personalities other than the default one\ndatabases/*\n!databases/.keep\n\n# extensions\nextensions/\n!extensions/.keep\n\nweb/.env.build\nweb/.env.dev\nweb/.env.development\nnode_modules/\n\n# Google chrome files\n*.crdownload\n\n# outputs folder\noutputs\n# junk stuff\n./src\n\n# ignore installation files\n.installed\n\ntest.http\n\nshared/*\n!shared/.keep\n\n\nuploads\n*global_paths_cfg.yaml\n\n.vscode\n\n# Endpoint test files using REST client (VSCODE Extension)\n\ntests/end_point_tests/*_local.http\n\nuser_data\nsrc/\nsrc/taming-transformers\n\n# Remove nogpu\n.no_gpu\n\n# Temporary arguments to restart file\ntemp_args.txt\n\n# Hugging face offloading folder\noffload\n\n# Removing personal data\npersonal_data\noutputs_*\noutput_*\n\nmPLUG-Owl\n\nxtts_models\n\nmodels.txt\nlogs\n*.pkl\nlollms_apps_zoo"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.7421875,
          "content": "[submodule \"zoos/bindings_zoo\"]\n\tpath = zoos/bindings_zoo\n\turl = https://github.com/ParisNeo/lollms_bindings_zoo.git\n\tbranch = main\n[submodule \"zoos/personalities_zoo\"]\n\tpath = zoos/personalities_zoo\n\turl = https://github.com/ParisNeo/lollms_personalities_zoo.git\n\tbranch = main\n[submodule \"zoos/models_zoo\"]\n\tpath = zoos/models_zoo\n\turl = https://github.com/ParisNeo/models_zoo.git\n\tbranch = main\n[submodule \"lollms_core\"]\n\tpath = lollms_core\n\turl = https://github.com/ParisNeo/lollms.git\n\tbranch = main\n[submodule \"utilities/safe_store\"]\n\tpath = utilities/safe_store\n\turl = https://github.com/ParisNeo/safe_store.git\n\tbranch = main\n[submodule \"utilities/pipmaster\"]\n\tpath = utilities/pipmaster\n\turl = https://github.com/ParisNeo/pipmaster.git\n\tbranch = main\n"
        },
        {
          "name": ".hadolint.yaml",
          "type": "blob",
          "size": 0.01953125,
          "content": "ignored:\n  - SC1091\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.181640625,
          "content": "repos:\n-   repo: https://github.com/ParisNeo/parisneo-precommit-hooks\n    rev: v0.1.0  # Use the latest version\n    hooks:\n    -   id: parisneo-python-check\n    -   id: parisneo-js-check"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 0.419921875,
          "content": "# V 15:\nEnhanced theming system\nNew Smart routing system\n# V 7.0:\nAdded changelog\nSeparated images from text in file upload\nAdded support for multimodal models\nAdded Dalle and gpt4 vision\nenhanced interface\nUpgraded the code execution\nNow it is possible to execute lollms multiple times on the same PC. You can now have multiple instances with different port numbers and so a work in parallel.\nAdded placeholder for xAI\nNew models"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 2.6083984375,
          "content": "## Code of Conduct\n\nOur chatbot is an inclusive community that values respect, collaboration, and innovation. We welcome contributors of all backgrounds and skill levels to join us in building a positive and productive community. To ensure that our community remains safe, respectful, and welcoming, we ask all contributors to abide by this code of conduct.\n\n### 1. Respectful Communication\n\nWe value open communication and encourage contributors to express themselves in a respectful and constructive manner. We do not tolerate discrimination, harassment, or abuse of any kind, including but not limited to:\n\n- Offensive comments related to gender, gender identity and expression, sexual orientation, disability, mental illness, race, ethnicity, age, nationality, religion, or physical appearance\n- Threats, intimidation, or bullying\n- Inappropriate sexual advances or imagery\n\n### 2. Collaborative Development\n\nWe encourage collaboration and teamwork among contributors and ask that everyone work together in a constructive and positive manner. We do not tolerate disruptive behavior, including but not limited to:\n\n- Spamming, trolling, or flaming\n- Hijacking discussions or derailing conversations\n- Refusal to consider alternative viewpoints or approaches\n\n### 3. Innovative Contributions\n\nWe welcome contributions of all kinds and encourage innovation and experimentation. However, we do not tolerate the use of our chatbot for any form of misinformation, including but not limited to:\n\n- The dissemination of false information, rumors, or hoaxes\n- The promotion of conspiracy theories or fake news\n- The use of our chatbot for malicious purposes, including but not limited to fraud, scams, or phishing\n\n### 4. Consequences of Unacceptable Behavior\n\nWe take all reports of unacceptable behavior seriously and will investigate all incidents promptly and thoroughly. We reserve the right to take any action deemed necessary, including but not limited to:\n\n- Warning the individual responsible for the unacceptable behavior\n- Temporarily or permanently revoking their access to the chatbot\n- Banning them from future participation in the community\n\n### 5. Reporting Unacceptable Behavior\n\nIf you experience or witness behavior that violates this code of conduct, please report it immediately to the chatbot administrator. All reports will be kept confidential and will be investigated promptly and thoroughly.\n\n### 6. Acknowledgment of Code of Conduct\n\nBy contributing to our chatbot, you acknowledge that you have read and agree to abide by this code of conduct. You also acknowledge that you have the responsibility to report any violations of this code of conduct.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.22265625,
          "content": "# Contributing to lollms-webui\n\nThank you for your interest in contributing to [Your Repository Name]! We appreciate your efforts to help make our project better.\n\n## Contributing Guidelines\n\nBefore you start contributing, please take a moment to review our guidelines:\n\n1. Please do not include links directly to your own repositories or external websites. Instead, please use a code tag to display any code snippets or examples.\n\n2. Please ensure that your contributions are well-documented and include comments where necessary.\n\n3. Please follow our coding standards and best practices, as outlined in our [style guide](link-to-style-guide).\n\n4. If you have any questions or need assistance, please feel free to reach out to the project maintainers.\n\n## Submitting Contributions\n\nTo submit a contribution, please follow these steps:\n\n1. Fork the repository and create a new branch for your changes.\n\n2. Make your changes and ensure that all tests pass.\n\n3. Commit your changes and push them to your fork.\n\n4. Submit a pull request to the main repository.\n\n5. Wait for feedback from the project maintainers.\n\nOnce your pull request is approved, your changes will be merged into the main repository.\n\nThank you for your contributions to lollms-webui!\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.216796875,
          "content": "# Use an official Python runtime as a parent image\nFROM python:3.11-slim\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Miniconda\nRUN curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\n    && bash Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda \\\n    && rm Miniconda3-latest-Linux-x86_64.sh\n\n# Add Conda to PATH\nENV PATH /opt/conda/bin:$PATH\n\n# Create and activate Conda environment\nRUN conda create --name lollms_env python=3.11 git pip -y\nSHELL [\"conda\", \"run\", \"-n\", \"lollms_env\", \"/bin/bash\", \"-c\"]\n\n# Clone the repository\nRUN git clone --depth 1 --recurse-submodules https://github.com/ParisNeo/lollms-webui.git \\\n    && cd lollms-webui/lollms_core \\\n    && pip install -e . \\\n    && cd ../.. \\\n    && cd lollms-webui/utilities/pipmaster \\\n    && pip install -e . \\\n    && cd ../..\n\n# Install project dependencies\nWORKDIR /app/lollms-webui\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Copy the rest of the application code\nCOPY . .\n\n# Expose port 9600\nEXPOSE 9600\n\n# Set the default command to run the application\nCMD [\"python\", \"app.py\"]"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.095703125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 Saifeddine ALOUI (aka: ParisNeo)\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "ManualInstall.md",
          "type": "blob",
          "size": 4.3642578125,
          "content": "# LoLLMS WebUI Cross-Platform Installation Guide\n\n## Introduction\n\nLoLLMS (Lord of Large Language and Multimodal Systems) is a powerful tool for working with large language models. This guide will walk you through the installation process for the LoLLMS WebUI on Windows, macOS, and Linux, including the installation of Python 3.11.\n\n## Prerequisites\n\nBefore you begin, ensure you have:\n\n- Internet connection\n- Administrator/sudo privileges on your system\n\n## Installation Steps\n\n### 1. Install Git\n\n#### Windows\n- Download and install Git from https://git-scm.com/download/win\n\n#### macOS\n- Install Homebrew if not already installed:\n  ```\n  /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n  ```\n- Install Git:\n  ```\n  brew install git\n  ```\n\n#### Linux\n- Use your distribution's package manager. For example, on Ubuntu:\n  ```\n  sudo apt-get update\n  sudo apt-get install git\n  ```\n\n### 2. Install Python 3.11\n\n#### Windows\n1. Download the Python 3.11 installer from https://www.python.org/downloads/release/python-3110/\n2. Run the installer\n3. Check \"Add Python 3.11 to PATH\"\n4. Click \"Install Now\"\n\n#### macOS\n1. Install Homebrew if not already installed (see Git installation step)\n2. Install Python 3.11:\n   ```\n   brew install python@3.11\n   ```\n3. Add Python 3.11 to your PATH:\n   ```\n   echo 'export PATH=\"/usr/local/opt/python@3.11/bin:$PATH\"' >> ~/.zshrc\n   source ~/.zshrc\n   ```\n\n#### Linux (Ubuntu/Debian)\n1. Add the deadsnakes PPA:\n   ```\n   sudo add-apt-repository ppa:deadsnakes/ppa\n   sudo apt-get update\n   ```\n2. Install Python 3.11:\n   ```\n   sudo apt-get install python3.11 python3.11-venv python3.11-dev\n   ```\n\n### 3. Clone the LoLLMS WebUI Repository\n\nOpen a terminal (Command Prompt on Windows) and run:\n\n```\ngit clone --depth 1 --recurse-submodules https://github.com/ParisNeo/lollms-webui.git\ncd lollms-webui\ngit submodule update --init --recursive\n```\n\n### 4. Create and Activate a Virtual Environment\n\n#### Windows\n```\npython3.11 -m venv lollms_env\nlollms_env\\Scripts\\activate\n```\n\n#### macOS and Linux\n```\npython3.11 -m venv lollms_env\nsource lollms_env/bin/activate\n```\n\n### 5. Install Requirements\n\nWith the virtual environment activated, run:\n\n```\npip install --upgrade pip\npip install -r requirements.txt\npip install -e lollms_core\n```\n\n### 6. Select and Install a Binding\n\nChoose a binding based on your needs. Here are some options:\n\n- Local bindings: ollama, python_llama_cpp, bs_exllamav2\n- Remote bindings: groq, open_router, open_ai, mistral_ai, gemini, vllm, xAI, elf, remote_lollms\n\nTo install a binding, run:\n```\npython zoos/bindings_zoo/<binding_name>/__init__.py\n```\nReplace `<binding_name>` with your chosen binding.\n\n### 7. Create Launcher Scripts\n\n#### Windows\nCreate `lollms.bat` in the LoLLMS directory:\n```batch\n@echo off\ncall lollms_env\\Scripts\\activate\ncd lollms-webui\npython app.py %*\npause\n```\n\n#### macOS and Linux\nCreate `lollms.sh` in the LoLLMS directory:\n```bash\n#!/bin/bash\nsource lollms_env/bin/activate\ncd lollms-webui\npython app.py \"$@\"\n```\nMake it executable:\n```\nchmod +x lollms.sh\n```\n\n## Optional Steps\n\n### Install CUDA (for NVIDIA GPUs)\n\nIf you have an NVIDIA GPU and want to use it for local AI:\n\n#### Windows\n- Download and install CUDA from https://developer.nvidia.com/cuda-downloads\n\n#### macOS\n- CUDA is not supported on macOS with recent NVIDIA GPUs.\n\n#### Linux\n- Follow the CUDA installation guide for your specific Linux distribution from https://developer.nvidia.com/cuda-downloads\n\n### Install Visual Studio Code\n\nFor local AI development, you may want to install Visual Studio Code:\n\n- Download and install from: https://code.visualstudio.com/download\n\n## Running LoLLMS WebUI\n\nTo start the LoLLMS WebUI:\n\n#### Windows\nRun the `lollms.bat` file.\n\n#### macOS and Linux\nRun the `lollms.sh` script:\n```\n./lollms.sh\n```\n\n## Troubleshooting\n\nIf you encounter any issues during installation or running the WebUI, please check the following:\n\n1. Ensure Python 3.11 is correctly installed:\n   ```\n   python3.11 --version\n   ```\n2. Verify that your Python environment is activated before running any commands.\n3. Check that all required packages are installed correctly.\n4. Make sure you have selected and installed a compatible binding.\n5. For platform-specific issues, consult the documentation for your operating system.\n\nFor further assistance, please refer to the official LoLLMS documentation or seek help in the project's support channels."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.7392578125,
          "content": "# LoLLMs (Lord of Large Language Multimodal Systems) Web UI\n<div align=\"center\">\n  <img src=\"https://github.com/ParisNeo/lollms-webui/blob/main/assets/logo.png\" alt=\"Logo\" width=\"200\" height=\"200\">\n</div>\n\n![GitHub license](https://img.shields.io/github/license/ParisNeo/lollms-webui)\n![GitHub issues](https://img.shields.io/github/issues/ParisNeo/lollms-webui)\n![GitHub stars](https://img.shields.io/github/stars/ParisNeo/lollms-webui)\n![GitHub forks](https://img.shields.io/github/forks/ParisNeo/lollms-webui)\n[![Discord](https://img.shields.io/discord/1092918764925882418?color=7289da&label=Discord&logo=discord&logoColor=ffffff)](https://discord.gg/4rR282WJb6)\n[![Follow me on X](https://img.shields.io/twitter/follow/ParisNeo_AI?style=social)](https://twitter.com/ParisNeo_AI)\n[![Follow Me on YouTube](https://img.shields.io/badge/Follow%20Me%20on-YouTube-red?style=flat&logo=youtube)](https://www.youtube.com/user/Parisneo)\n\n## LoLLMs core library download statistics\n[![Downloads](https://static.pepy.tech/badge/lollms)](https://pepy.tech/project/lollms)\n[![Downloads](https://static.pepy.tech/badge/lollms/month)](https://pepy.tech/project/lollms)\n[![Downloads](https://static.pepy.tech/badge/lollms/week)](https://pepy.tech/project/lollms)\n\n## LoLLMs webui download statistics\n[![Downloads](https://img.shields.io/github/downloads/ParisNeo/lollms-webui/total?style=flat-square)](https://github.com/ParisNeo/lollms-webui/releases)\n[![Downloads](https://img.shields.io/github/downloads/ParisNeo/lollms-webui/latest/total?style=flat-square)](https://github.com/ParisNeo/lollms-webui/releases)\n\n\nWelcome to LoLLMS WebUI (Lord of Large Language Multimodal Systems: One tool to rule them all), the hub for LLM (Large Language Models) and multimodal intelligence systems. This project aims to provide a user-friendly interface to access and utilize various LLM and other AI models for a wide range of tasks. Whether you need help with writing, coding, organizing data, analyzing images, generating images, generating music or seeking answers to your questions, LoLLMS WebUI has got you covered.\n\nAs an all-encompassing tool with access to over 500 AI expert conditioning across diverse domains and more than 2500 fine tuned models over multiple domains, you now have an immediate resource for any problem. Whether your car needs repair or if you need coding assistance in Python, C++ or JavaScript; feeling down about life decisions that were made wrongly yet unable to see how? Ask Lollms. Need guidance on what lies ahead healthwise based on current symptoms presented, our medical assistance AI can help you get a potential diagnosis and guide you to seek the right medical care. If stuck with legal matters such contract interpretation feel free to reach out to Lawyer personality, to get some insight at hand -all without leaving comfort home. Not only does it aid students struggling through those lengthy lectors but provides them extra support during assessments too, so they are able grasp concepts properly rather then just reading along lines which could leave many confused afterward. Want some entertainment? Then engage Laughter Botand let yourself go enjoy hysterical laughs until tears roll from eyes while playing Dungeons&Dragonsor make up crazy stories together thanks to Creative Story Generator. Need illustration work done? No worries, Artbot got us covered there! And last but definitely not least LordOfMusic is here for music generation according to individual specifications. So essentially say goodbye boring nights alone because everything possible can be achieved within one single platform called Lollms...\n\n## Features\n\n- Choose your preferred binding, model, and personality for your tasks\n- Enhance your emails, essays, code debugging, thought organization, and more\n- Explore a wide range of functionalities, such as searching, data organization, image generation, and music generation\n- Easy-to-use UI with light and dark mode options\n- Integration with GitHub repository for easy access\n- Support for different personalities with predefined welcome messages\n- Thumb up/down rating for generated answers\n- Copy, edit, and remove messages\n- Local database storage for your discussions\n- Search, export, and delete multiple discussions\n- Support for image/video generation based on stable diffusion\n- Support for music generation based on musicgen\n- Support for multi generation peer to peer network through Lollms Nodes and Petals.\n- Support for Docker, conda, and manual virtual environment setups\n- Support for LM Studio as a backend\n- Support for Ollama as a backend\n- Support for vllm as a backend\n- Support for prompt Routing to various models depending on the complexity of the task\n\n## Star History\n\n<a href=\"https://star-history.com/#ParisNeo/lollms-webui&ParisNeo/lollms&ParisNeo/lollms_cpp_client&ParisNeo/lollms_bindings_zoo&ParisNeo/lollms_personalities_zoo&Date\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=ParisNeo/lollms-webui,ParisNeo/lollms,ParisNeo/lollms_cpp_client,ParisNeo/lollms_bindings_zoo,ParisNeo/lollms_personalities_zoo&type=Date&theme=dark\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=ParisNeo/lollms-webui,ParisNeo/lollms,ParisNeo/lollms_cpp_client,ParisNeo/lollms_bindings_zoo,ParisNeo/lollms_personalities_zoo&type=Date\" />\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=ParisNeo/lollms-webui,ParisNeo/lollms,ParisNeo/lollms_cpp_client,ParisNeo/lollms_bindings_zoo,ParisNeo/lollms_personalities_zoo&type=Date\" />\n  </picture>\n</a>\n\nThank you for all users who tested this tool and helped making it more user friendly.\n\n## Installation\n\n### Automatic installation (Console)\nDownload the installation script from scripts folder and run it.\nThe installation scripts are:\n- `lollms_installer.bat` for Windows.\n- `lollms_installer.sh`for Linux.\n- `lollms_installer_macos.sh`for Mac.\n\n### Manual install:\nSince v 10.14, manual installation is back:\n\n- Make sure you have python 3.11 is installed or to have a python 3.11 conda environment or other way.\n- clone the repo at: `https://github.com/ParisNeo/lollms-webui.git`\n- better create an environment  for lollms and activate it\n- in the repo folder, make sure you pull all submodules:  `git submodule update --init --recursive`\n- install lollms by going to lollms_core then do: `pip install -e .`\n- go back to the root of the lollms_webui folder\n- install all requirements: `pip install -r requirements.txt` \n\nnow you are ready to run lolmms: `python app.py` \n## Smart Routing: Optimizing for Money and Speed\n\nLollms' Smart Routing feature goes beyond just selecting the right model for accuracy. It empowers you to optimize your text generation process for two key factors: **money** and **speed**.\n\n**Optimizing for Money:**\n\nImagine you're using multiple text generation services with varying price points. Some models might be exceptionally powerful but come with a hefty price tag, while others offer a more budget-friendly option with slightly less capability. Smart Routing lets you leverage this price difference to your advantage:\n\n* **Cost-Effective Selection:** By defining a hierarchy of models based on their cost, Smart Routing can automatically choose the most economical model for your prompt. This ensures you're only paying for the power you need, minimizing unnecessary expenses.\n* **Dynamic Price Adjustment:**  As your prompt complexity changes, Smart Routing can dynamically switch between models, ensuring you're always using the most cost-effective option for the task at hand.\n\n**Optimizing for Speed:**\n\nSpeed is another critical factor in text generation, especially when dealing with large volumes of content or time-sensitive tasks. Smart Routing allows you to prioritize speed by:\n\n* **Prioritizing Smaller Models:** By placing faster, less resource-intensive models higher in the hierarchy, Smart Routing can prioritize speed for simple prompts. This ensures quick responses and efficient processing.\n* **Dynamic Speed Adjustment:**  For more complex prompts requiring the power of larger models, Smart Routing can seamlessly switch to those models while maintaining a balance between speed and accuracy.\n\n**Example Use Cases:**\n\n* **Content Marketing:**  Use Smart Routing to select the most cost-effective model for generating large volumes of blog posts or social media content.\n* **Customer Support:**  Prioritize speed by using smaller models for quick responses to frequently asked questions, while leveraging more powerful models for complex inquiries.\n* **Research and Development:**  Optimize for both money and speed by using a tiered model hierarchy, ensuring you can quickly generate initial drafts while using more powerful models for in-depth analysis.\n\n**Conclusion:**\n\nSmart Routing is a versatile tool that empowers you to optimize your text generation process for both cost and speed. By leveraging a hierarchy of models and dynamically adjusting your selection based on prompt complexity, you can achieve the perfect balance between efficiency, accuracy, and cost-effectiveness.\n\n\n# Code of conduct\n\nBy using this tool, users agree to follow these guidelines :\n- This tool is not meant to be used for building and spreading fakenews / misinformation.\n- You are responsible for what you generate by using this tool. The creators will take no responsibility for anything created via this lollms.\n- You can use lollms in your own project free of charge if you agree to respect the Apache 2.0 licenseterms. Please refer to https://www.apache.org/licenses/LICENSE-2.0 .\n- You are not allowed to use lollms to harm others directly or indirectly. This tool is meant for peaceful purposes and should be used for good never for bad.\n- Users must comply with local laws when accessing content provided by third parties like OpenAI API etc., including copyright restrictions where applicable.\n\n# ⚠️ Security Warning\n\nPlease be aware that LoLLMs WebUI does not have built-in user authentication and is primarily designed for local use. Exposing the WebUI to external access without proper security measures could lead to potential vulnerabilities.\n\nIf you require remote access to LoLLMs, it is strongly recommended to follow these security guidelines:\n\n1. **Activate Headless Mode**: Enabling headless mode will expose only the generation API while turning off other potentially vulnerable endpoints. This helps to minimize the attack surface.\n\n2. **Set Up a Secure Tunnel**: Establish a secure tunnel between the localhost running LoLLMs and the remote PC that needs access. This ensures that the communication between the two devices is encrypted and protected.\n\n3. **Modify Configuration Settings**: After setting up the secure tunnel, edit the `/configs/local_config.yaml` file and adjust the following settings:\n   ```yaml\n   host: 0.0.0.0  # Allow remote connections\n   port: 9600  # Change the port number if desired (default is 9600)\n   force_accept_remote_access: true  # Force accepting remote connections\n   headless_server_mode: true  # Set to true for API-only access, or false if the WebUI is needed\n   ```\n\nBy following these security practices, you can help protect your LoLLMs instance and its users from potential security risks when enabling remote access.\n\nRemember, it is crucial to prioritize security and take necessary precautions to safeguard your system and sensitive information. If you have any further questions or concerns regarding the security of LoLLMs, please consult the documentation or reach out to the community for assistance.\n\nStay safe and enjoy using LoLLMs responsibly!\n\n\n# Disclaimer\nLarge Language Models are amazing tools that can be used for diverse purposes. Lollms was built to harness this power to help the user enhance its productivity. But you need to keep in mind that these models have their limitations and should not replace human intelligence or creativity, but rather augment it by providing suggestions based on patterns found within large amounts of data. It is up to each individual how they choose to use them responsibly!\n\nThe performance of the system varies depending on the used model, its size and the dataset on whichit has been trained. The larger a language model's training set (the more examples), generally speaking - better results will follow when using such systems as opposed those with smaller ones. But there is still no guarantee that the output generated from any given prompt would always be perfect and it may contain errors due various reasons. So please make sure you do not use it for serious matters like choosing medications or making financial decisions without consulting an expert first hand! \n\n# license\nThis repository uses code under ApacheLicense Version 2.0 , see [license](https://github.com/ParisNeo/lollms-webui/blob/main/LICENSE) file for details about rights granted with respect to usage & distribution\n\n# Copyright:\nParisNeo 2023\n\n\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.6044921875,
          "content": "# Security Policy\n\n## Supported Versions\n\nUse this section to tell people about which versions of your project are\ncurrently being supported with security updates.\n\n| Version | Supported          |\n| ------- | ------------------ |\n| 5.1.x   | :white_check_mark: |\n| 5.0.x   | :x:                |\n| 4.0.x   | :white_check_mark: |\n| < 4.0   | :x:                |\n\n## Reporting a Vulnerability\n\nUse this section to tell people how to report a vulnerability.\n\nTell them where to go, how often they can expect to get an update on a\nreported vulnerability, what to expect if the vulnerability is accepted or\ndeclined, etc.\n"
        },
        {
          "name": "ai_ethics",
          "type": "tree",
          "content": null
        },
        {
          "name": "api",
          "type": "tree",
          "content": null
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 19.001953125,
          "content": "\"\"\"\nFile: lollms_web_ui.py\nAuthor: ParisNeo\nDescription: Singleton class for the LoLLMS web UI.\n\nThis file is the entry point to the webui.\n\"\"\"\n\nimport os\nimport sys\nimport threading\nimport time\nfrom typing import List, Tuple\n\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom lollms.utilities import PackageManager\n\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\nexpected_ascii_colors_version = \"0.4.2\"\nprint(\n    f\"Checking ascii_colors ({expected_ascii_colors_version}) ...\", end=\"\", flush=True\n)\nif not PackageManager.check_package_installed_with_version(\n    \"ascii_colors\", expected_ascii_colors_version\n):\n    PackageManager.install_or_update(\"ascii_colors\")\nfrom ascii_colors import ASCIIColors\n\nASCIIColors.success(\"OK\")\n\nexpected_pipmaster_version = \"0.3.2\"\nASCIIColors.yellow(\n    f\"Checking pipmaster ({expected_pipmaster_version}) ...\", end=\"\", flush=True\n)\nif not PackageManager.check_package_installed_with_version(\n    \"pipmaster\", expected_pipmaster_version\n):\n    PackageManager.install_or_update(\"pipmaster\")\nimport pipmaster as pm\n\nASCIIColors.success(\"OK\")\n\n\ndef animate(text: str, stop_event: threading.Event):\n    animation = \"⠋⠙⠹⠸⠼⠴⠦⠧⠇⠏\"\n    idx = 0\n    while not stop_event.is_set():\n        ASCIIColors.yellow(\n            f\"\\r{text} {animation[idx % len(animation)]}\", end=\"\", flush=True\n        )\n        idx += 1\n        time.sleep(0.1)\n    print(\"\\r\" + \" \" * 50, end=\"\\r\")  # Clear the line\n\n\ndef check_and_install_package(package: str, version: str):\n    stop_event = threading.Event()\n    animation_thread = threading.Thread(\n        target=animate, args=(f\"Checking {package} ({version})\", stop_event)\n    )\n    animation_thread.start()\n\n    try:\n        installed = PackageManager.check_package_installed_with_version(\n            package, version\n        )\n\n        if not installed:\n            stop_event.set()\n            animation_thread.join()\n            print(\"\\r\" + \" \" * 50, end=\"\\r\")  # Clear the line\n            PackageManager.install_or_update(package)\n\n        stop_event.set()\n        animation_thread.join()\n\n        print(\"\\r\" + \" \" * 50, end=\"\\r\")  # Clear the line\n        ASCIIColors.yellow(f\"Checking {package} ({version}) ...\", end=\"\")\n        ASCIIColors.success(\"OK\")\n\n    except Exception as e:\n        stop_event.set()\n        animation_thread.join()\n        print(\"\\r\" + \" \" * 50, end=\"\\r\")  # Clear the line\n        ASCIIColors.red(f\"Error checking/installing {package}: {str(e)}\")\n\n\npackages: List[Tuple[str, str]] = [\n    (\"freedom_search\", \"0.1.9\"),\n    (\"scrapemaster\", \"0.2.1\"),\n    (\"lollms_client\", \"0.7.7\"),\n    (\"lollmsvectordb\", \"1.3.6\"),\n]\n\nif not pm.is_installed(\"einops\"):\n    pm.install(\"einops\")\nif not pm.is_installed(\"datasets\"):\n    pm.install(\"datasets\")\n# einops datasets\n\n\ndef check_pn_libs():\n    ASCIIColors.cyan(\"Checking ParisNeo libraries installation\")\n    print()\n\n    for package, version in packages:\n        check_and_install_package(package, version)\n        print()  # Add a newline for better readability between package checks\n\n    ASCIIColors.green(\"All packages have been checked and are up to date!\")\n\n\nimport argparse\nimport os\nimport socket\nimport sys\nimport webbrowser\nfrom pathlib import Path\n\nimport psutil\nimport socketio\nimport uvicorn\nfrom ascii_colors import ASCIIColors\nfrom fastapi import FastAPI, Request\nfrom fastapi.encoders import jsonable_encoder\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom lollms.main_config import LOLLMSConfig\nfrom lollms.paths import LollmsPaths\nfrom lollms.security import sanitize_path\nfrom lollms.utilities import trace_exception\nfrom pydantic import ValidationError\nfrom socketio import ASGIApp\nfrom starlette.responses import FileResponse\n\nfrom lollms_webui import LOLLMSWebUI\n\n\ndef get_ip_addresses():\n    hostname = socket.gethostname()\n    ip_addresses = [socket.gethostbyname(hostname)]\n\n    for interface_name, interface_addresses in psutil.net_if_addrs().items():\n        for address in interface_addresses:\n            if str(address.family) == \"AddressFamily.AF_INET\":\n                ip_addresses.append(address.address)\n\n    return ip_addresses\n\n\napp = FastAPI(title=\"LoLLMS\", description=\"This is the LoLLMS-Webui API documentation\")\n\n\ntry:\n    from lollms.security import MultipartBoundaryCheck\n\n    # Add the MultipartBoundaryCheck middleware\n    app.add_middleware(MultipartBoundaryCheck)\nexcept:\n    print(\"Couldn't activate MultipartBoundaryCheck\")\n\n# app.mount(\"/socket.io\", StaticFiles(directory=\"path/to/socketio.js\"))\n\nif __name__ == \"__main__\":\n    desired_version = (3, 11)\n    if not sys.version_info >= desired_version:\n        ASCIIColors.error(\n            f\"Your Python version is {sys.version_info.major}.{sys.version_info.minor}, but version {desired_version[0]}.{desired_version[1]} or higher is required.\"\n        )\n        sys.exit(1)\n    # Parsong parameters\n    parser = argparse.ArgumentParser(description=\"Start the chatbot FastAPI app.\")\n\n    parser.add_argument(\n        \"--host\", type=str, default=None, help=\"the hostname to listen on\"\n    )\n    parser.add_argument(\"--port\", type=int, default=None, help=\"the port to listen on\")\n\n    args = parser.parse_args()\n    root_path = Path(__file__).parent\n    lollms_paths = LollmsPaths.find_paths(\n        force_local=True, custom_default_cfg_path=\"configs/config.yaml\"\n    )\n    config = LOLLMSConfig.autoload(lollms_paths)\n\n    if config.auto_update:\n        check_pn_libs()\n\n    if config.debug_log_file_path != \"\":\n        ASCIIColors.log_path = config.debug_log_file_path\n    if args.host:\n        config.host = args.host\n    if args.port:\n        config.port = args.port\n\n    # Define the path to your custom CA bundle file\n    ca_bundle_path = lollms_paths.personal_certificates / \"truststore.pem\"\n\n    if ca_bundle_path.exists():\n        # Set the environment variable\n        os.environ[\"REQUESTS_CA_BUNDLE\"] = str(ca_bundle_path)\n\n    cert_file_path = lollms_paths.personal_certificates / \"cert.pem\"\n    key_file_path = lollms_paths.personal_certificates / \"key.pem\"\n    if os.path.exists(cert_file_path) and os.path.exists(key_file_path):\n        is_https = True\n    else:\n        is_https = False\n\n    # Create a Socket.IO server\n    if config[\"host\"] != \"localhost\":\n        if config[\"host\"] != \"0.0.0.0\":\n            config.allowed_origins.append(\n                f\"https://{config['host']}:{config['port']}\"\n                if is_https\n                else f\"http://{config['host']}:{config['port']}\"\n            )\n        else:\n            config.allowed_origins += [\n                (\n                    f\"https://{ip}:{config['port']}\"\n                    if is_https\n                    else f\"http://{ip}:{config['port']}\"\n                )\n                for ip in get_ip_addresses()\n            ]\n    allowed_origins = config.allowed_origins + [\n        (\n            f\"https://localhost:{config['port']}\"\n            if is_https\n            else f\"http://localhost:{config['port']}\"\n        )\n    ]\n\n    # class EndpointSpecificCORSMiddleware(BaseHTTPMiddleware):\n    #     async def dispatch(self, request: Request, call_next):\n    #         if request.url.path == \"/v1/completions\":\n    #             # For /v1/completions, allow all origins\n    #             response = await call_next(request)\n    #             response.headers[\"Access-Control-Allow-Origin\"] = \"*\"\n    #             response.headers[\"Access-Control-Allow-Methods\"] = \"*\"\n    #             response.headers[\"Access-Control-Allow-Headers\"] = \"*\"\n    #             return response\n    #         else:\n    #             # For other endpoints, use the restricted CORS policy\n    #             origin = request.headers.get(\"origin\")\n    #             if origin in allowed_origins:\n    #                 response = await call_next(request)\n    #                 response.headers[\"Access-Control-Allow-Origin\"] = origin\n    #                 response.headers[\"Access-Control-Allow-Credentials\"] = \"true\"\n    #                 response.headers[\"Access-Control-Allow-Methods\"] = \"*\"\n    #                 response.headers[\"Access-Control-Allow-Headers\"] = \"*\"\n    #                 return response\n    #             else:\n    #                 return await call_next(request)\n\n    # # Add the custom middleware\n    # app.add_middleware(EndpointSpecificCORSMiddleware)\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=allowed_origins,\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    sio = socketio.AsyncServer(\n        async_mode=\"asgi\",\n        cors_allowed_origins=allowed_origins,\n        ping_timeout=1200,\n        ping_interval=30,\n    )  # Enable CORS for selected origins\n\n    # A simple fix for v 11.0 to 12 alpha\n    if config.rag_vectorizer == \"bert\":\n        config.rag_vectorizer = \"tfidf\"\n        config.save_config()\n\n    LOLLMSWebUI.build_instance(\n        config=config, lollms_paths=lollms_paths, args=args, sio=sio\n    )\n    lollmsElfServer: LOLLMSWebUI = LOLLMSWebUI.get_instance()\n    lollmsElfServer.verbose = True\n\n    # Import all endpoints\n    from lollms.server.endpoints.lollms_binding_files_server import \\\n        router as lollms_binding_files_server_router\n    from lollms.server.endpoints.lollms_binding_infos import \\\n        router as lollms_binding_infos_router\n    from lollms.server.endpoints.lollms_comfyui import \\\n        router as lollms_comfyui_router\n    from lollms.server.endpoints.lollms_configuration_infos import \\\n        router as lollms_configuration_infos_router\n    from lollms.server.endpoints.lollms_diffusers import \\\n        router as lollms_diffusers_router\n    from lollms.server.endpoints.lollms_discussion import \\\n        router as lollms_discussion_router\n    from lollms.server.endpoints.lollms_file_system import \\\n        router as lollms_file_system_router\n    from lollms.server.endpoints.lollms_generator import \\\n        router as lollms_generator_router\n    from lollms.server.endpoints.lollms_hardware_infos import \\\n        router as lollms_hardware_infos_router\n    from lollms.server.endpoints.lollms_infos import \\\n        router as lollms_infos_router\n    from lollms.server.endpoints.lollms_models_infos import \\\n        router as lollms_models_infos_router\n    from lollms.server.endpoints.lollms_motion_ctrl import \\\n        router as lollms_motion_ctrl_router\n    from lollms.server.endpoints.lollms_ollama import \\\n        router as lollms_ollama_router\n    from lollms.server.endpoints.lollms_personalities_infos import \\\n        router as lollms_personalities_infos_router\n    from lollms.server.endpoints.lollms_petals import \\\n        router as lollms_petals_router\n    from lollms.server.endpoints.lollms_rag import router as lollms_rag_router\n    from lollms.server.endpoints.lollms_sd import router as lollms_sd_router\n    from lollms.server.endpoints.lollms_skills_library import \\\n        router as lollms_skills_library_router\n    from lollms.server.endpoints.lollms_tti import router as lollms_tti_router\n    from lollms.server.endpoints.lollms_tts import \\\n        router as lollms_tts_add_router\n    from lollms.server.endpoints.lollms_user import \\\n        router as lollms_user_router\n    from lollms.server.endpoints.lollms_vllm import \\\n        router as lollms_vllm_router\n    from lollms.server.endpoints.lollms_whisper import router as lollms_whisper\n    from lollms.server.endpoints.lollms_xtts import \\\n        router as lollms_xtts_add_router\n    from lollms.server.events.lollms_files_events import \\\n        add_events as lollms_files_events_add\n    from lollms.server.events.lollms_generation_events import \\\n        add_events as lollms_generation_events_add\n    from lollms.server.events.lollms_model_events import \\\n        add_events as lollms_model_events_add\n    from lollms.server.events.lollms_personality_events import \\\n        add_events as lollms_personality_events_add\n\n    from endpoints.chat_bar import router as chat_bar_router\n    from endpoints.lollms_advanced import router as lollms_advanced_router\n    from endpoints.lollms_apps import router as lollms_apps_router\n    from endpoints.lollms_help import router as help_router\n    from endpoints.lollms_message import router as lollms_message_router\n    from endpoints.lollms_playground import router as lollms_playground_router\n    from endpoints.lollms_webui_infos import \\\n        router as lollms_webui_infos_router\n    from events.lollms_chatbox_events import \\\n        add_events as lollms_chatbox_events_add\n    from events.lollms_discussion_events import \\\n        add_events as lollms_webui_discussion_events_add\n    # from lollms.server.events.lollms_rag_events import add_events as lollms_rag_events_add\n    from events.lollms_generation_events import \\\n        add_events as lollms_webui_generation_events_add\n    from events.lollms_interactive_events import \\\n        add_events as lollms_interactive_events_add\n\n    # endpoints for remote access\n    app.include_router(lollms_generator_router)\n\n    # Endpoints reserved for local access\n    if (\n        not config.headless_server_mode\n    ) or config.force_accept_remote_access:  # Be aware that forcing force_accept_remote_access can expose the server to attacks\n        app.include_router(lollms_infos_router)\n        app.include_router(lollms_binding_files_server_router)\n        app.include_router(lollms_hardware_infos_router)\n        app.include_router(lollms_binding_infos_router)\n        app.include_router(lollms_models_infos_router)\n        app.include_router(lollms_personalities_infos_router)\n        app.include_router(lollms_skills_library_router)\n        app.include_router(lollms_tti_router)\n\n        app.include_router(lollms_webui_infos_router)\n        app.include_router(lollms_discussion_router)\n        app.include_router(lollms_message_router)\n        app.include_router(lollms_user_router)\n        app.include_router(lollms_advanced_router)\n        app.include_router(lollms_apps_router)\n\n        app.include_router(chat_bar_router)\n        app.include_router(help_router)\n\n        app.include_router(lollms_tts_add_router)\n        app.include_router(lollms_xtts_add_router)\n        app.include_router(lollms_whisper)\n\n        app.include_router(lollms_sd_router)\n        app.include_router(lollms_diffusers_router)\n        app.include_router(lollms_comfyui_router)\n\n        app.include_router(lollms_ollama_router)\n        app.include_router(lollms_petals_router)\n\n        app.include_router(lollms_rag_router)\n        app.include_router(lollms_vllm_router)\n        app.include_router(lollms_motion_ctrl_router)\n\n        app.include_router(lollms_file_system_router)\n\n        app.include_router(lollms_playground_router)\n        app.include_router(lollms_configuration_infos_router)\n\n    @sio.event\n    async def disconnect(sid):\n        ASCIIColors.yellow(f\"Disconnected: {sid}\")\n\n    @sio.event\n    async def message(sid, data):\n        ASCIIColors.yellow(f\"Message from {sid}: {data}\")\n        await sio.send(sid, \"Message received!\")\n\n    lollms_generation_events_add(sio)\n\n    if (\n        not config.headless_server_mode\n    ) or config.force_accept_remote_access:  # Be aware that forcing force_accept_remote_access can expose the server to attacks\n        lollms_personality_events_add(sio)\n        lollms_files_events_add(sio)\n        lollms_model_events_add(sio)\n        # lollms_rag_events_add(sio)\n\n        lollms_webui_generation_events_add(sio)\n        lollms_webui_discussion_events_add(sio)\n        lollms_chatbox_events_add(sio)\n        lollms_interactive_events_add(sio)\n\n    app.mount(\n        \"/extensions\",\n        StaticFiles(directory=Path(__file__).parent / \"web\" / \"dist\", html=True),\n        name=\"extensions\",\n    )\n    app.mount(\n        \"/playground\",\n        StaticFiles(directory=Path(__file__).parent / \"web\" / \"dist\", html=True),\n        name=\"playground\",\n    )\n    app.mount(\n        \"/settings\",\n        StaticFiles(directory=Path(__file__).parent / \"web\" / \"dist\", html=True),\n        name=\"settings\",\n    )\n\n    # Custom route to serve JavaScript files with the correct MIME type\n    @app.get(\"/{path:path}\")\n    async def serve_js(path: str):\n        sanitize_path(path)\n        if path == \"\":\n            return FileResponse(\n                Path(__file__).parent / \"web\" / \"dist\" / \"index.html\",\n                media_type=\"text/html\",\n            )\n        file_path = Path(__file__).parent / \"web\" / \"dist\" / path\n        if file_path.suffix == \".js\":\n            return FileResponse(file_path, media_type=\"application/javascript\")\n        return FileResponse(file_path)\n\n    app.mount(\n        \"/\",\n        StaticFiles(directory=Path(__file__).parent / \"web\" / \"dist\", html=True),\n        name=\"static\",\n    )\n\n    @app.exception_handler(ValidationError)\n    async def validation_exception_handler(request: Request, exc: ValidationError):\n        print(f\"Error: {exc.errors()}\")  # Print the validation error details\n        if hasattr(exc, \"body\"):\n            return JSONResponse(\n                status_code=422,\n                content=jsonable_encoder(\n                    {\"detail\": exc.errors(), \"body\": await exc.body}\n                ),  # Send the error details and the original request body\n            )\n        else:\n            return JSONResponse(\n                status_code=422,\n                content=jsonable_encoder(\n                    {\"detail\": exc.errors(), \"body\": \"\"}\n                ),  # Send the error details and the original request body\n            )\n\n    app = ASGIApp(socketio_server=sio, other_asgi_app=app)\n\n    lollmsElfServer.app = app\n\n    try:\n        sio.reboot = False\n        # if config.enable_lollms_service:\n        #     ASCIIColors.yellow(\"Starting Lollms service\")\n        #     #uvicorn.run(app, host=config.host, port=6523)\n        #     def run_lollms_server():\n        #         parts = config.lollms_base_url.split(\":\")\n        #         host = \":\".join(parts[0:2])\n        #         port = int(parts[2])\n        #         uvicorn.run(app, host=host, port=port)\n        # New thread\n        #     thread = threading.Thread(target=run_lollms_server)\n\n        # start thread\n        #   thread.start()\n\n        # if autoshow\n\n        if config.auto_show_browser and not config.headless_server_mode:\n            if config[\"host\"] == \"0.0.0.0\":\n                webbrowser.open(\n                    f\"https://localhost:{config['port']}\"\n                    if is_https\n                    else f\"http://localhost:{config['port']}\"\n                )\n                # webbrowser.open(f\"http://localhost:{6523}\") # needed for debug (to be removed in production)\n            else:\n                webbrowser.open(\n                    f\"https://{config['host']}:{config['port']}\"\n                    if is_https\n                    else f\"http://{config['host']}:{config['port']}\"\n                )\n                # webbrowser.open(f\"http://{config['host']}:{6523}\") # needed for debug (to be removed in production)\n\n        if is_https:\n            uvicorn.run(\n                app,\n                host=config.host,\n                port=config.port,\n                ssl_certfile=cert_file_path,\n                ssl_keyfile=key_file_path,\n            )\n        else:\n            uvicorn.run(app, host=config.host, port=config.port)\n\n    except Exception as ex:\n        trace_exception(ex)\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "books.sqlite",
          "type": "blob",
          "size": 64,
          "content": null
        },
        {
          "name": "code_templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "databases",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 0.2021484375,
          "content": "version: '3'\nservices:\n  lollms-webui:\n    build: .\n    ports:\n      - \"9600:9600\"\n    volumes:\n      - ./lollms-webui:/app/lollms-webui\n    environment:\n      - PYTHONUNBUFFERED=1\n    command: python app.py"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "endpoints",
          "type": "tree",
          "content": null
        },
        {
          "name": "events",
          "type": "tree",
          "content": null
        },
        {
          "name": "extensions",
          "type": "tree",
          "content": null
        },
        {
          "name": "help",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "installer.iss",
          "type": "blob",
          "size": 2.564453125,
          "content": "; Script generated by the Inno Setup Script Wizard.\n; SEE THE DOCUMENTATION FOR DETAILS ON CREATING INNO SETUP SCRIPT FILES!\n\n#define MyAppName \"LoLLMs_webUI\"\n#define MyAppVersion \"6.0\"\n#define MyAppPublisher \"ParisNeo\"\n#define MyAppURL \"https://parisneo.github.io/lollms-webui/\"\n#define MyAppExeName \"win_run.bat\"\n#define MyAppUpdateModelsName \"win_update_models.bat\"\n#define MyAppCondaSessionName \"win_conda_session.bat\"\n[Setup]\n; NOTE: The value of AppId uniquely identifies this application. Do not use the same AppId value in installers for other applications.\n; (To generate a new GUID, click Tools | Generate GUID inside the IDE.)\nAppId={{1A757D39-4C63-4E0C-AED3-BA6273BBC0CA}\nAppName={#MyAppName}\nAppVersion={#MyAppVersion}\n;AppVerName={#MyAppName} {#MyAppVersion}\nAppPublisher={#MyAppPublisher}\nAppPublisherURL={#MyAppURL}\nAppSupportURL={#MyAppURL}\nAppUpdatesURL={#MyAppURL}\nDefaultDirName={%USERPROFILE}\\{#MyAppName}\nDisableProgramGroupPage=yes\nLicenseFile=.\\lollms-webui\\LICENSE\nInfoBeforeFile=.\\lollms-webui\\CODE_OF_CONDUCT.md\n; Remove the following line to run in administrative install mode (install for all users.)\nPrivilegesRequired=lowest\nOutputBaseFilename=lollms_v6_cpu\nSetupIconFile=.\\logo.ico\nCompression=lzma\nSolidCompression=yes\nWizardStyle=modern\n\n[Languages]\nName: \"english\"; MessagesFile: \"compiler:Default.isl\"\n\n[Tasks]\nName: \"desktopicon\"; Description: \"{cm:CreateDesktopIcon}\"; GroupDescription: \"{cm:AdditionalIcons}\"; Flags: unchecked\n\n[Files]\nSource: \".\\{#MyAppExeName}\"; DestDir: \"{app}\"; Flags: ignoreversion\nSource: \".\\*\"; DestDir: \"{app}\"; Flags: ignoreversion recursesubdirs createallsubdirs\n; NOTE: Don't use \"Flags: ignoreversion\" on any shared system files\n\n[Icons]\nName: \"{autodesktop}\\{#MyAppName}\"; Filename: \"{app}\\{#MyAppExeName}\"; Tasks: desktopicon; IconFilename: \"{app}\\logo.ico\"\nName: \"{commondesktop}\\{#MyAppName}\"; Filename: \"{app}\\{#MyAppExeName}\"; IconFilename: \"{app}\\logo.ico\"\nName: \"{autodesktop}\\{#MyAppUpdateModelsName}\"; Filename: \"{app}\\{#MyAppUpdateModelsName}\"; Tasks: desktopicon; IconFilename: \"{app}\\logo.ico\"\nName: \"{commondesktop}\\{#MyAppUpdateModelsName}\"; Filename: \"{app}\\{#MyAppUpdateModelsName}\"; IconFilename: \"{app}\\logo.ico\"\nName: \"{autodesktop}\\{#MyAppCondaSessionName}\"; Filename: \"{app}\\{#MyAppCondaSessionName}\"; Tasks: desktopicon; IconFilename: \"{app}\\logo.ico\"\nName: \"{commondesktop}\\{#MyAppCondaSessionName}\"; Filename: \"{app}\\{#MyAppCondaSessionName}\"; IconFilename: \"{app}\\logo.ico\"\n\n[Run]\nFilename: \"{app}\\{#MyAppExeName}\"; Description: \"{cm:LaunchProgram,{#StringChange(MyAppName, '&', '&&')}}\"; Flags: shellexec postinstall skipifsilent\n\n"
        },
        {
          "name": "installer_cpu.iss",
          "type": "blob",
          "size": 2.13671875,
          "content": "; Script generated by the Inno Setup Script Wizard.\n; SEE THE DOCUMENTATION FOR DETAILS ON CREATING INNO SETUP SCRIPT FILES!\n\n#define MyAppName \"LoLLMs_webUI\"\n#define MyAppVersion \"6.0\"\n#define MyAppPublisher \"ParisNeo\"\n#define MyAppURL \"https://parisneo.github.io/lollms-webui/\"\n#define MyAppExeName \"win_run.bat\"\n\n[Setup]\n; NOTE: The value of AppId uniquely identifies this application. Do not use the same AppId value in installers for other applications.\n; (To generate a new GUID, click Tools | Generate GUID inside the IDE.)\nAppId={{1A757D39-4C63-4E0C-AED3-BA6273BBC0CA}\nAppName={#MyAppName}\nAppVersion={#MyAppVersion}\n;AppVerName={#MyAppName} {#MyAppVersion}\nAppPublisher={#MyAppPublisher}\nAppPublisherURL={#MyAppURL}\nAppSupportURL={#MyAppURL}\nAppUpdatesURL={#MyAppURL}\nDefaultDirName={%USERPROFILE}\\{#MyAppName}\nDisableProgramGroupPage=yes\nLicenseFile=C:\\Users\\aloui\\Documents\\ai\\lollms-webui_test\\lollms-webui\\LICENSE\nInfoBeforeFile=C:\\Users\\aloui\\Documents\\ai\\lollms-webui_test\\lollms-webui\\CODE_OF_CONDUCT.md\n; Remove the following line to run in administrative install mode (install for all users.)\nPrivilegesRequired=lowest\nOutputBaseFilename=lollms_v6_cpu\nSetupIconFile=C:\\Users\\aloui\\Documents\\ai\\lollms-webui_test\\logo.ico\nCompression=lzma\nSolidCompression=yes\nWizardStyle=modern\nDiskSpanning=yes\n\n[Languages]\nName: \"english\"; MessagesFile: \"compiler:Default.isl\"\n\n[Tasks]\nName: \"desktopicon\"; Description: \"{cm:CreateDesktopIcon}\"; GroupDescription: \"{cm:AdditionalIcons}\"; Flags: unchecked\n\n[Files]\nSource: \"C:\\Users\\aloui\\Documents\\ai\\lollms-webui_test\\{#MyAppExeName}\"; DestDir: \"{app}\"; Flags: ignoreversion\nSource: \"C:\\Users\\aloui\\Documents\\ai\\lollms-webui_test\\*\"; DestDir: \"{app}\"; Flags: ignoreversion recursesubdirs createallsubdirs\n; NOTE: Don't use \"Flags: ignoreversion\" on any shared system files\n\n[Icons]\nName: \"{autodesktop}\\{#MyAppName}\"; Filename: \"{app}\\{#MyAppExeName}\"; Tasks: desktopicon\nName: \"{commondesktop}\\{#MyAppName}\"; Filename: \"{app}\\{#MyAppExeName}\"; IconFilename: \"{app}\\logo.ico\"\n\n[Run]\nFilename: \"{app}\\{#MyAppExeName}\"; Description: \"{cm:LaunchProgram,{#StringChange(MyAppName, '&', '&&')}}\"; Flags: shellexec postinstall skipifsilent\n\n"
        },
        {
          "name": "lollms_core",
          "type": "commit",
          "content": null
        },
        {
          "name": "lollms_webui.py",
          "type": "blob",
          "size": 86.1787109375,
          "content": "\"\"\"\nFile: lollms_web_ui.py\nAuthor: ParisNeo\nDescription: Singleton class for the LoLLMS web UI.\n\nThis class provides a singleton instance of the LoLLMS web UI, allowing access to its functionality and data across multiple endpoints.\n\"\"\"\n\nimport asyncio\nimport ctypes\nimport gc\nimport json\nimport os\nimport re\nimport shutil\nimport string\nimport sys\nimport threading\nimport time\nimport traceback\nfrom datetime import datetime\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import Any, Callable, List, Tuple\n\nimport git\nimport numpy as np\nimport requests\nfrom lollms.app import LollmsApplication\nfrom lollms.binding import (BindingBuilder, BindingType, LLMBinding,\n                            LOLLMSConfig, ModelBuilder)\nfrom lollms.client_session import Client\nfrom lollms.com import LoLLMsCom, NotificationDisplayType, NotificationType\nfrom lollms.config import InstallOption\nfrom lollms.databases.discussions_database import Discussion, DiscussionsDB\nfrom lollms.generation import (RECEPTION_MANAGER, ROLE_CHANGE_DECISION,\n                               ROLE_CHANGE_OURTPUT)\nfrom lollms.helpers import ASCIIColors, trace_exception\nfrom lollms.paths import LollmsPaths\nfrom lollms.personality import AIPersonality, PersonalityBuilder\nfrom lollms.server.elf_server import LOLLMSElfServer\nfrom lollms.types import (CONTENT_OPERATION_TYPES, MSG_OPERATION_TYPE,\n                          MSG_TYPE, SENDER_TYPES)\nfrom lollms.utilities import (File64BitsManager, PackageManager,\n                              PromptReshaper, convert_language_name,\n                              find_first_available_file_index,\n                              is_asyncio_loop_running, process_ai_output,\n                              run_async, yes_or_no_input)\nfrom tqdm import tqdm\n\nif not PackageManager.check_package_installed(\"requests\"):\n    PackageManager.install_package(\"requests\")\nif not PackageManager.check_package_installed(\"bs4\"):\n    PackageManager.install_package(\"beautifulsoup4\")\nimport requests\n\n\ndef terminate_thread(thread):\n    if thread:\n        if not thread.is_alive():\n            ASCIIColors.yellow(\"Thread not alive\")\n            return\n\n        thread_id = thread.ident\n        exc = ctypes.py_object(SystemExit)\n        res = ctypes.pythonapi.PyThreadState_SetAsyncExc(thread_id, exc)\n        if res > 1:\n            ctypes.pythonapi.PyThreadState_SetAsyncExc(thread_id, None)\n            del thread\n            gc.collect()\n            raise SystemError(\"Failed to terminate the thread.\")\n        else:\n            ASCIIColors.yellow(\n                \"Canceled successfully\"\n            )  # The current version of the webui\n\n\nlollms_webui_version = \"v16 (codename Nexus 🌀)\"\n\n\nclass LOLLMSWebUI(LOLLMSElfServer):\n    __instance = None\n\n    @staticmethod\n    def build_instance(\n        config: LOLLMSConfig,\n        lollms_paths: LollmsPaths,\n        load_binding=True,\n        load_model=True,\n        load_voice_service=True,\n        load_sd_service=True,\n        try_select_binding=False,\n        try_select_model=False,\n        callback=None,\n        args=None,\n        sio=None,\n    ):\n        if LOLLMSWebUI.__instance is None:\n            LOLLMSWebUI(\n                config,\n                lollms_paths,\n                load_binding=load_binding,\n                load_model=load_model,\n                load_sd_service=load_sd_service,\n                load_voice_service=load_voice_service,\n                try_select_binding=try_select_binding,\n                try_select_model=try_select_model,\n                callback=callback,\n                args=args,\n                sio=sio,\n            )\n        return LOLLMSWebUI.__instance\n\n    def __init__(\n        self,\n        config: LOLLMSConfig,\n        lollms_paths: LollmsPaths,\n        load_binding=True,\n        load_model=True,\n        load_voice_service=True,\n        load_sd_service=True,\n        try_select_binding=False,\n        try_select_model=False,\n        callback=None,\n        args=None,\n        sio=None,\n    ) -> None:\n        super().__init__(\n            config,\n            lollms_paths,\n            load_binding=load_binding,\n            load_model=load_model,\n            try_select_binding=try_select_binding,\n            try_select_model=try_select_model,\n            callback=callback,\n            sio=sio,\n        )\n        self.app_name: str = \"LOLLMSWebUI\"\n        self.version: str = lollms_webui_version\n        self.args = args\n\n        self.busy = False\n        self.nb_received_tokens = 0\n\n        self.config_file_path = config.file_path\n        self.cancel_gen = False\n\n        if self.config.auto_update:\n            if self.check_update_():\n                ASCIIColors.info(\"New version found. Updating!\")\n                self.run_update_script()\n        # Keeping track of current discussion and message\n        self._current_user_message_id = 0\n        self._current_ai_message_id = 0\n        self._message_id = 0\n\n        # migrate old databases to new ones:\n        databases_path = self.lollms_paths.personal_path / \"databases\"\n        if (\n            databases_path.exists()\n            and len([f for f in databases_path.iterdir() if f.suffix == \".db\"]) > 0\n        ):\n            if yes_or_no_input(\n                \"Old databases have been spotted on your system. Do you want me to migrate them to the new format?\"\n            ):\n                databases_found = False\n                for database_path in databases_path.iterdir():\n                    if database_path.suffix == \".db\":\n                        ASCIIColors.red(\n                            f\"Found old discussion database format : {database_path}\"\n                        )\n                        ASCIIColors.red(f\"Migrating to new format... \", end=\"\")\n                        new_db_path = (\n                            self.lollms_paths.personal_discussions_path\n                            / database_path.stem\n                        )\n                        new_db_path.mkdir(exist_ok=True, parents=True)\n                        try:\n                            shutil.copy(database_path, new_db_path / \"database.db\")\n                            ASCIIColors.green(\"ok\")\n                            databases_found = True\n                        except Exception as ex:\n                            ASCIIColors.warning(ex)\n                if databases_found:\n                    ASCIIColors.green(\n                        f\"Databases are migrated from {databases_path} to the new {self.lollms_paths.personal_discussions_path} path\"\n                    )\n                    if yes_or_no_input(\n                        \"Databases are migrated to the new format. Do you want me to delete the previous version?\"\n                    ):\n                        for database_path in databases_path.iterdir():\n                            if database_path.suffix == \".db\":\n                                ASCIIColors.red(f\"Deleting {database_path}\")\n                                database_path.unlink()\n        if config[\"discussion_db_name\"].endswith(\".db\"):\n            config[\"discussion_db_name\"] = config[\"discussion_db_name\"].replace(\n                \".db\", \"\"\n            )\n            config.save_config()\n\n        self.discussion_db_name = config[\"discussion_db_name\"]\n\n        # Create database object\n        self.db = DiscussionsDB(self, self.lollms_paths, self.discussion_db_name)\n\n        # If the database is empty, populate it with tables\n        ASCIIColors.info(\"Checking discussions database... \", end=\"\")\n        self.db.create_tables()\n        self.db.add_missing_columns()\n        ASCIIColors.success(\"ok\")\n\n        # This is used to keep track of messages\n        self.download_infos = {}\n\n        # Define a WebSocket event handler\n        @sio.event\n        async def connect(sid, environ):\n            self.session.add_client(sid, sid, self.db.load_last_discussion(), self.db)\n            await self.sio.emit(\"connected\", to=sid)\n            ASCIIColors.success(f\"Client {sid} connected\")\n\n        @sio.event\n        def disconnect(sid):\n            try:\n                self.session.add_client(\n                    sid, sid, self.db.load_last_discussion(), self.db\n                )\n                if self.session.get_client(sid).processing:\n                    self.session.get_client(sid).schedule_for_deletion = True\n                else:\n                    # Clients are now kept forever\n                    pass  # self.session.remove_client(sid, sid)\n            except Exception as ex:\n                pass\n\n            ASCIIColors.error(f\"Client {sid} disconnected\")\n\n        # generation status\n        self.generating = False\n        ASCIIColors.blue(f\"Your personal data is stored here :\", end=\"\")\n        ASCIIColors.green(f\"{self.lollms_paths.personal_path}\")\n\n        self.start_servers()\n\n    def get_uploads_path(self, client_id):\n        return self.session.get_client(\n            client_id\n        ).discussion_path  # self.db.discussion_db_path/f'{[\"discussion\"].discussion_id}'\n\n    # Other methods and properties of the LoLLMSWebUI singleton class\n    def check_module_update_(self, repo_path, branch_name=\"main\"):\n        try:\n            # Open the repository\n            ASCIIColors.yellow(f\"Checking for updates from {repo_path}\")\n            repo = git.Repo(repo_path)\n\n            # Fetch updates from the remote for the specified branch\n            repo.remotes.origin.fetch(\n                refspec=f\"refs/heads/{branch_name}:refs/remotes/origin/{branch_name}\"\n            )\n\n            # Compare the local and remote commit IDs for the specified branch\n            local_commit = repo.head.commit\n            remote_commit = repo.remotes.origin.refs[branch_name].commit\n\n            # Check if the local branch is behind the remote branch\n            is_behind = (\n                repo.is_ancestor(local_commit, remote_commit)\n                and local_commit != remote_commit\n            )\n\n            ASCIIColors.yellow(f\"update availability: {is_behind}\")\n\n            # Return True if the local branch is behind the remote branch\n            return is_behind\n        except Exception as e:\n            # Handle any errors that may occur during the fetch process\n            # trace_exception(e)\n            return False\n\n    def check_update_(self, branch_name=\"main\"):\n        try:\n            # Open the repository\n            repo_path = str(Path(__file__).parent / \"lollms_core\")\n            if self.check_module_update_(repo_path, branch_name):\n                return True\n            repo_path = str(Path(__file__).parent)\n            if self.check_module_update_(repo_path, branch_name):\n                return True\n            return False\n        except Exception as e:\n            # Handle any errors that may occur during the fetch process\n            # trace_exception(e)\n            return False\n\n    def run_update_script(self, args=None):\n        # deactivate trust store for github and pip package install\n        if \"REQUESTS_CA_BUNDLE\" in os.environ:\n            del os.environ[\"REQUESTS_CA_BUNDLE\"]\n        update_script = Path(__file__).parent / \"update_script.py\"\n\n        # Convert Namespace object to a dictionary\n        if args:\n            args_dict = vars(args)\n        else:\n            args_dict = {}\n        # Filter out any key-value pairs where the value is None\n        valid_args = {\n            key: value for key, value in args_dict.items() if value is not None\n        }\n\n        # Save the arguments to a temporary file\n        temp_file = Path(__file__).parent / \"temp_args.txt\"\n        with open(temp_file, \"w\") as file:\n            # Convert the valid_args dictionary to a string in the format \"key1 value1 key2 value2 ...\"\n            arg_string = \" \".join(\n                [f\"--{key} {value}\" for key, value in valid_args.items()]\n            )\n            file.write(arg_string)\n\n        os.system(f\"python {update_script}\")\n        sys.exit(0)\n\n    def run_restart_script(self, args):\n        restart_script = Path(__file__).parent / \"restart_script.py\"\n\n        # Convert Namespace object to a dictionary\n        args_dict = vars(args)\n\n        # Filter out any key-value pairs where the value is None\n        valid_args = {\n            key: value for key, value in args_dict.items() if value is not None\n        }\n\n        # Save the arguments to a temporary file\n        temp_file = Path(__file__).parent / \"temp_args.txt\"\n        with open(temp_file, \"w\") as file:\n            # Convert the valid_args dictionary to a string in the format \"key1 value1 key2 value2 ...\"\n            arg_string = \" \".join(\n                [f\"--{key} {value}\" for key, value in valid_args.items()]\n            )\n            file.write(arg_string)\n\n        os.system(f\"python {restart_script}\")\n        sys.exit(0)\n\n    def audio_callback(self, text):\n\n        if self.summoned:\n            client_id = 0\n            self.cancel_gen = False\n            client = self.session.get_client(client_id)\n            client.generated_text = \"\"\n            client.cancel_generation = False\n            client.continuing = False\n            client.first_chunk = True\n\n            if not self.model:\n                ASCIIColors.error(\"Model not selected. Please select a model\")\n                self.error(\n                    \"Model not selected. Please select a model\", client_id=client_id\n                )\n                return\n\n            if not self.busy:\n                if client.discussion is None:\n                    if self.db.does_last_discussion_have_messages():\n                        client.discussion = self.db.create_discussion()\n                    else:\n                        client.discussion = self.db.load_last_discussion()\n\n                prompt = text\n                try:\n                    nb_tokens = len(self.model.tokenize(prompt))\n                except:\n                    nb_tokens = None\n                message = client.discussion.add_message(\n                    message_type=MSG_TYPE.MSG_TYPE_CONTENT.value,\n                    sender_type=SENDER_TYPES.SENDER_TYPES_USER.value,\n                    sender=(\n                        self.config.user_name.strip()\n                        if self.config.use_user_name_in_discussions\n                        else self.config.user_name\n                    ),\n                    content=prompt,\n                    metadata=None,\n                    parent_message_id=self.message_id,\n                    nb_tokens=nb_tokens,\n                )\n\n                ASCIIColors.green(\n                    \"Starting message generation by \" + self.personality.name\n                )\n                client.generation_thread = threading.Thread(\n                    target=self.start_message_generation,\n                    args=(message, message.id, client_id),\n                )\n                client.generation_thread.start()\n\n                self.sio.sleep(0.01)\n                ASCIIColors.info(\"Started generation task\")\n                self.busy = True\n                # tpe = threading.Thread(target=self.start_message_generation, args=(message, message_id, client_id))\n                # tpe.start()\n            else:\n                self.error(\"I am busy. Come back later.\", client_id=client_id)\n        else:\n            if \"lollms\" in text.lower():\n                self.summoned = True\n\n    # def scrape_and_save(self, url, file_path):\n    #     # Send a GET request to the URL\n    #     response = requests.get(url)\n\n    #     # Parse the HTML content using BeautifulSoup\n    #     soup = BeautifulSoup(response.content, 'html.parser')\n\n    #     # Find all the text content in the webpage\n    #     text_content = soup.get_text()\n\n    #     # Remove extra returns and spaces\n    #     text_content = ' '.join(text_content.split())\n\n    #     # Save the text content as a text file\n    #     with open(file_path, 'w', encoding=\"utf-8\") as file:\n    #         file.write(text_content)\n\n    #     self.info(f\"Webpage content saved to {file_path}\")\n\n    def rebuild_personalities(self, reload_all=False):\n        if reload_all:\n            self.mounted_personalities = []\n\n        loaded = self.mounted_personalities\n        loaded_names = [\n            f\"{p.category}/{p.personality_folder_name}\" for p in loaded if p is not None\n        ]\n        mounted_personalities = []\n        ASCIIColors.success(f\" ╔══════════════════════════════════════════════════╗ \")\n        ASCIIColors.success(f\" ║           Building mounted Personalities         ║ \")\n        ASCIIColors.success(f\" ╚══════════════════════════════════════════════════╝ \")\n        to_remove = []\n        for i, personality in enumerate(self.config[\"personalities\"]):\n            if i == self.config[\"active_personality_id\"]:\n                ASCIIColors.red(\"*\", end=\"\")\n                ASCIIColors.green(f\" {personality}\")\n            else:\n                ASCIIColors.yellow(f\" {personality}\")\n            if personality in loaded_names:\n                mounted_personalities.append(loaded[loaded_names.index(personality)])\n            else:\n                personality_path = f\"{personality}\"\n                try:\n                    personality = AIPersonality(\n                        personality_path,\n                        self.lollms_paths,\n                        self.config,\n                        model=self.model,\n                        app=self,\n                        selected_language=self.config.current_language,\n                        run_scripts=True,\n                    )\n\n                    mounted_personalities.append(personality)\n                    if self.config.auto_read and len(personality.audio_samples) > 0:\n                        try:\n                            from lollms.services.tts.xtts.lollms_xtts import \\\n                                LollmsXTTS\n\n                            if self.tts is None:\n                                voice = self.config.xtts_current_voice\n                                if voice != \"main_voice\":\n                                    voices_folder = self.lollms_paths.custom_voices_path\n                                else:\n                                    voices_folder = (\n                                        Path(__file__).parent.parent.parent\n                                        / \"services/xtts/voices\"\n                                    )\n\n                                self.tts = LollmsXTTS(\n                                    self,\n                                    voices_folders=[\n                                        voices_folder,\n                                        Path(__file__).parent.parent.parent\n                                        / \"services/xtts/voices\",\n                                    ],\n                                    freq=self.config.xtts_freq,\n                                )\n\n                        except Exception as ex:\n                            trace_exception(ex)\n                            self.warning(\n                                f\"Personality {personality.name} request using custom voice but couldn't load XTTS\"\n                            )\n                except Exception as ex:\n                    ASCIIColors.error(\n                        f\"Personality file not found or is corrupted ({personality_path}).\\nReturned the following exception:{ex}\\nPlease verify that the personality you have selected exists or select another personality. Some updates may lead to change in personality name or category, so check the personality selection in settings to be sure.\"\n                    )\n                    ASCIIColors.info(\"Trying to force reinstall\")\n                    if self.config[\"debug\"]:\n                        print(ex)\n                    try:\n                        personality = AIPersonality(\n                            personality_path,\n                            self.lollms_paths,\n                            self.config,\n                            self.model,\n                            app=self,\n                            run_scripts=True,\n                            selected_language=self.config.current_language,\n                            installation_option=InstallOption.FORCE_INSTALL,\n                        )\n                        mounted_personalities.append(personality)\n                        if personality.processor:\n                            personality.processor.mounted()\n                    except Exception as ex:\n                        ASCIIColors.error(\n                            f\"Couldn't load personality at {personality_path}\"\n                        )\n                        trace_exception(ex)\n                        ASCIIColors.info(f\"Unmounting personality\")\n                        to_remove.append(i)\n                        personality = AIPersonality(\n                            None,\n                            self.lollms_paths,\n                            self.config,\n                            self.model,\n                            app=self,\n                            run_scripts=True,\n                            installation_option=InstallOption.FORCE_INSTALL,\n                        )\n                        mounted_personalities.append(personality)\n                        if personality.processor:\n                            personality.processor.mounted()\n\n                        ASCIIColors.info(\"Reverted to default personality\")\n        if self.config[\"active_personality_id\"] >= 0 and self.config[\n            \"active_personality_id\"\n        ] < len(self.config[\"personalities\"]):\n            ASCIIColors.success(\n                f'selected model : {self.config[\"personalities\"][self.config[\"active_personality_id\"]]}'\n            )\n        else:\n            ASCIIColors.warning(\n                \"An error was encountered while trying to mount personality\"\n            )\n        ASCIIColors.success(f\" ╔══════════════════════════════════════════════════╗ \")\n        ASCIIColors.success(f\" ║                      Done                        ║ \")\n        ASCIIColors.success(f\" ╚══════════════════════════════════════════════════╝ \")\n        # Sort the indices in descending order to ensure correct removal\n        to_remove.sort(reverse=True)\n\n        # Remove elements from the list based on the indices\n        for index in to_remove:\n            if 0 <= index < len(mounted_personalities):\n                mounted_personalities.pop(index)\n                self.config[\"personalities\"].pop(index)\n                ASCIIColors.info(f\"removed personality {personality_path}\")\n\n        if self.config[\"active_personality_id\"] >= len(self.config[\"personalities\"]):\n            self.config[\"active_personality_id\"] = 0\n\n        return mounted_personalities\n\n    # ================================== LOLLMSApp\n\n    # properties\n    @property\n    def message_id(self):\n        return self._message_id\n\n    @message_id.setter\n    def message_id(self, id):\n        self._message_id = id\n\n    @property\n    def current_user_message_id(self):\n        return self._current_user_message_id\n\n    @current_user_message_id.setter\n    def current_user_message_id(self, id):\n        self._current_user_message_id = id\n        self._message_id = id\n\n    @property\n    def current_ai_message_id(self):\n        return self._current_ai_message_id\n\n    @current_ai_message_id.setter\n    def current_ai_message_id(self, id):\n        self._current_ai_message_id = id\n        self._message_id = id\n\n    def download_file(self, url, installation_path, callback=None):\n        \"\"\"\n        Downloads a file from a URL, reports the download progress using a callback function, and displays a progress bar.\n\n        Args:\n            url (str): The URL of the file to download.\n            installation_path (str): The path where the file should be saved.\n            callback (function, optional): A callback function to be called during the download\n                with the progress percentage as an argument. Defaults to None.\n        \"\"\"\n        try:\n            response = requests.get(url, stream=True)\n\n            # Get the file size from the response headers\n            total_size = int(response.headers.get(\"content-length\", 0))\n\n            with open(installation_path, \"wb\") as file:\n                downloaded_size = 0\n                with tqdm(\n                    total=total_size, unit=\"B\", unit_scale=True, ncols=80\n                ) as progress_bar:\n                    for chunk in response.iter_content(chunk_size=8192):\n                        if chunk:\n                            file.write(chunk)\n                            downloaded_size += len(chunk)\n                            if callback is not None:\n                                callback(downloaded_size, total_size)\n                            progress_bar.update(len(chunk))\n\n            if callback is not None:\n                callback(total_size, total_size)\n\n            print(\"File downloaded successfully\")\n        except Exception as e:\n            print(\"Couldn't download file:\", str(e))\n\n    def clean_string(self, input_string):\n        # Remove extra spaces by replacing multiple spaces with a single space\n        # cleaned_string = re.sub(r'\\s+', ' ', input_string)\n\n        # Remove extra line breaks by replacing multiple consecutive line breaks with a single line break\n        cleaned_string = re.sub(r\"\\n\\s*\\n\", \"\\n\", input_string)\n        # Create a string containing all punctuation characters\n        punctuation_chars = string.punctuation\n        # Define a regular expression pattern to match and remove non-alphanumeric characters\n        # pattern = f'[^a-zA-Z0-9\\s{re.escape(punctuation_chars)}]'  # This pattern matches any character that is not a letter, digit, space, or punctuation\n        pattern = f\"[^a-zA-Z0-9\\u00C0-\\u017F\\s{re.escape(punctuation_chars)}]\"\n        # Use re.sub to replace the matched characters with an empty string\n        cleaned_string = re.sub(pattern, \"\", cleaned_string)\n        return cleaned_string\n\n    def make_discussion_title(self, discussion, client_id=None):\n        \"\"\"\n        Builds a title for a discussion\n        \"\"\"\n\n        # Get the list of messages\n        messages = discussion.get_messages()\n        discussion_messages = f\"{self.start_header_id_template}instruction{self.end_header_id_template}Create a short title to this discussion\\nYour response should only contain the title without any comments.\\n\"\n        discussion_title = f\"\\n{self.start_header_id_template}Discussion title{self.end_header_id_template}\"\n\n        available_space = (\n            self.config.ctx_size\n            - 150\n            - len(self.model.tokenize(discussion_messages))\n            - len(self.model.tokenize(discussion_title))\n        )\n        # Initialize a list to store the full messages\n        full_message_list = []\n        # Accumulate messages until the cumulative number of tokens exceeds available_space\n        tokens_accumulated = 0\n        # Accumulate messages starting from message_index\n        for message in messages:\n            # Check if the message content is not empty and visible to the AI\n            if message.content != \"\" and (\n                message.message_type\n                <= MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT_INVISIBLE_TO_USER.value\n                and message.message_type\n                != MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT_INVISIBLE_TO_AI.value\n            ):\n\n                # Tokenize the message content\n                message_tokenized = self.model.tokenize(\n                    \"\\n\"\n                    + self.config.discussion_prompt_separator\n                    + message.sender\n                    + \": \"\n                    + message.content.strip()\n                )\n\n                # Check if adding the message will exceed the available space\n                if tokens_accumulated + len(message_tokenized) > available_space:\n                    break\n\n                # Add the tokenized message to the full_message_list\n                full_message_list.insert(0, message_tokenized)\n\n                # Update the cumulative number of tokens\n                tokens_accumulated += len(message_tokenized)\n\n        # Build the final discussion messages by detokenizing the full_message_list\n\n        for message_tokens in full_message_list:\n            discussion_messages += self.model.detokenize(message_tokens)\n        discussion_messages += discussion_title\n        title = [\"\"]\n\n        def receive(chunk: str, message_type: MSG_OPERATION_TYPE):\n            if chunk:\n                title[0] += chunk\n            antiprompt = self.personality.detect_antiprompt(title[0])\n            if antiprompt:\n                ASCIIColors.warning(f\"\\n{antiprompt} detected. Stopping generation\")\n                title[0] = self.remove_text_from_string(title[0], antiprompt)\n                return False\n            else:\n                return True\n\n        self._generate(discussion_messages, 150, client_id, receive)\n        ASCIIColors.info(title[0])\n        return title[0]\n\n    def get_discussion_to(self, client_id, message_id=-1):\n        messages = self.session.get_client(client_id).discussion.get_messages()\n        full_message_list = []\n        ump = f\"{self.start_header_id_template}{self.config.user_name.strip()if self.config.use_user_name_in_discussions else self.personality.user_message_prefix}{self.end_header_id_template}\"\n\n        for message in messages:\n            if message[\"id\"] <= message_id or message_id == -1:\n                if (\n                    message[\"type\"]\n                    != MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT_INVISIBLE_TO_USER\n                ):\n                    if message[\"sender\"] == self.personality.name:\n                        full_message_list.append(\n                            self.config.discussion_prompt_separator\n                            + self.personality.ai_message_prefix\n                            + message[\"content\"]\n                        )\n                    else:\n                        full_message_list.append(ump + message[\"content\"])\n\n        link_text = \"\\n\"  # self.personality.link_text\n\n        if len(full_message_list) > self.config[\"nb_messages_to_remember\"]:\n            discussion_messages = (\n                self.config.discussion_prompt_separator\n                + self.personality.personality_conditioning\n                + link_text.join(\n                    full_message_list[-self.config[\"nb_messages_to_remember\"] :]\n                )\n            )\n        else:\n            discussion_messages = (\n                self.config.discussion_prompt_separator\n                + self.personality.personality_conditioning\n                + link_text.join(full_message_list)\n            )\n\n        return discussion_messages  # Removes the last return\n\n    def set_message_content(\n        self,\n        full_text: str,\n        callback: (\n            Callable[[str | list | None, MSG_OPERATION_TYPE, str, Any | None], bool]\n            | None\n        ) = None,\n        client_id=0,\n    ):\n        \"\"\"This sends full text to front end\n\n        Args:\n            step_text (dict): The step text\n            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.\n        \"\"\"\n        if not callback:\n            callback = partial(self.process_data, client_id=client_id)\n\n        if callback:\n            callback(full_text, MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT)\n\n    def emit_socket_io_info(self, name, data, client_id):\n        run_async(partial(self.sio.emit, name, data, to=client_id))\n\n    def notify(\n        self,\n        content,\n        notification_type: NotificationType = NotificationType.NOTIF_SUCCESS,\n        duration: int = 4,\n        client_id=None,\n        display_type: NotificationDisplayType = NotificationDisplayType.TOAST,\n        verbose: bool | None = None,\n    ):\n        if verbose is None:\n            verbose = self.verbose\n\n        run_async(\n            partial(\n                self.sio.emit,\n                \"notification\",\n                {\n                    \"content\": content,\n                    \"notification_type\": notification_type.value,\n                    \"duration\": duration,\n                    \"display_type\": display_type.value,\n                },\n                to=client_id,\n            )\n        )\n        if verbose:\n            if notification_type == NotificationType.NOTIF_SUCCESS:\n                ASCIIColors.success(content)\n            elif notification_type == NotificationType.NOTIF_INFO:\n                ASCIIColors.info(content)\n            elif notification_type == NotificationType.NOTIF_WARNING:\n                ASCIIColors.warning(content)\n            else:\n                ASCIIColors.red(content)\n\n    def refresh_files(self, client_id=None):\n        run_async(partial(self.sio.emit, \"refresh_files\", to=client_id))\n\n    def new_message(\n        self,\n        client_id,\n        sender=None,\n        content=\"\",\n        message_type: MSG_OPERATION_TYPE = MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT,\n        sender_type: SENDER_TYPES = SENDER_TYPES.SENDER_TYPES_AI,\n        open=False,\n    ):\n        client = self.session.get_client(client_id)\n        # self.close_message(client_id)\n        if sender == None:\n            sender = self.personality.name\n        msg = client.discussion.add_message(\n            message_type=message_type.value,\n            sender_type=sender_type.value,\n            sender=sender,\n            content=content,\n            steps=[],\n            metadata=None,\n            ui=None,\n            rank=0,\n            parent_message_id=(\n                client.discussion.current_message.id\n                if client.discussion.current_message is not None\n                else 0\n            ),\n            binding=self.config[\"binding_name\"],\n            model=self.config[\"model_name\"],\n            personality=self.config[\"personalities\"][\n                self.config[\"active_personality_id\"]\n            ],\n            created_at=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        )  # first the content is empty, but we'll fill it at the end\n        run_async(\n            partial(\n                self.sio.emit,\n                \"new_message\",\n                {\n                    \"sender\": sender,\n                    \"message_type\": message_type.value,\n                    \"sender_type\": SENDER_TYPES.SENDER_TYPES_AI.value,\n                    \"content\": content,\n                    \"metadata\": None,\n                    \"ui\": None,\n                    \"id\": msg.id,\n                    \"parent_message_id\": msg.parent_message_id,\n                    \"binding\": self.binding.binding_folder_name,\n                    \"model\": self.model.model_name,\n                    \"personality\": self.personality.name,\n                    \"created_at\": client.discussion.current_message.created_at,\n                    \"started_generating_at\": client.discussion.current_message.started_generating_at,\n                    \"finished_generating_at\": client.discussion.current_message.finished_generating_at,\n                    \"nb_tokens\": client.discussion.current_message.nb_tokens,\n                    \"open\": open,\n                },\n                to=client_id,\n            )\n        )\n\n    def new_block(\n        self,\n        client_id,\n        sender=None,\n        content=\"\",\n        parameters=None,\n        metadata=None,\n        ui=None,\n        message_type: MSG_OPERATION_TYPE = MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT,\n        sender_type: SENDER_TYPES = SENDER_TYPES.SENDER_TYPES_AI,\n        open=False,\n    ):\n        # like new_message but without adding the information to the database\n        client = self.session.get_client(client_id)\n        run_async(\n            partial(\n                self.sio.emit,\n                \"new_message\",\n                {\n                    \"sender\": sender,\n                    \"message_type\": message_type.value,\n                    \"sender_type\": SENDER_TYPES.SENDER_TYPES_AI.value,\n                    \"content\": content,\n                    \"parameters\": parameters,\n                    \"metadata\": metadata,\n                    \"ui\": ui,\n                    \"id\": 0,\n                    \"parent_message_id\": 0,\n                    \"binding\": self.binding.binding_folder_name,\n                    \"model\": self.model.model_name,\n                    \"personality\": self.personality.name,\n                    \"created_at\": client.discussion.current_message.created_at,\n                    \"started_generating_at\": client.discussion.current_message.started_generating_at,\n                    \"finished_generating_at\": client.discussion.current_message.finished_generating_at,\n                    \"nb_tokens\": client.discussion.current_message.nb_tokens,\n                    \"open\": open,\n                },\n                to=client_id,\n            )\n        )\n\n    def send_refresh(self, client_id):\n        client = self.session.get_client(client_id)\n        run_async(\n            partial(\n                self.sio.emit,\n                \"update_message\",\n                {\n                    \"sender\": client.discussion.current_message.sender,\n                    \"id\": client.discussion.current_message.id,\n                    \"content\": client.discussion.current_message.content,\n                    \"discussion_id\": client.discussion.discussion_id,\n                    \"operation_type\": MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT.value,\n                    \"message_type\": client.discussion.current_message.message_type,\n                    \"created_at\": client.discussion.current_message.created_at,\n                    \"started_generating_at\": client.discussion.current_message.started_generating_at,\n                    \"finished_generating_at\": client.discussion.current_message.finished_generating_at,\n                    \"nb_tokens\": client.discussion.current_message.nb_tokens,\n                    \"binding\": self.binding.binding_folder_name,\n                    \"model\": self.model.model_name,\n                    \"personality\": self.personality.name,\n                },\n                to=client_id,\n            )\n        )\n\n    def update_message(\n        self,\n        client_id,\n        chunk,\n        parameters=None,\n        metadata=[],\n        ui=None,\n        operation_type: MSG_OPERATION_TYPE = None,\n    ):\n        client = self.session.get_client(client_id)\n        client.discussion.current_message.finished_generating_at = (\n            datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        )\n        client.discussion.current_message.nb_tokens = self.nb_received_tokens\n        mtdt = (\n            json.dumps(metadata, indent=4)\n            if metadata is not None and type(metadata) == list\n            else metadata\n        )\n\n        if self.nb_received_tokens == 1:\n            client.discussion.current_message.started_generating_at = (\n                datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            )\n            self.update_message_step(\n                client_id,\n                \"🔥 warming up ...\",\n                MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_END_SUCCESS,\n            )\n            self.update_message_step(\n                client_id,\n                \"✍ generating ...\",\n                MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_END_SUCCESS,\n            )\n\n        run_async(\n            partial(\n                self.sio.emit,\n                \"update_message\",\n                {\n                    \"sender\": self.personality.name,\n                    \"id\": client.discussion.current_message.id,\n                    \"content\": chunk,\n                    \"ui\": client.discussion.current_message.ui if ui is None else ui,\n                    \"discussion_id\": client.discussion.discussion_id,\n                    \"operation_type\": (\n                        operation_type.value\n                        if operation_type is not None\n                        else (\n                            MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_ADD_CHUNK.value\n                            if self.nb_received_tokens > 1\n                            else MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT.value\n                        )\n                    ),\n                    \"message_type\": MSG_TYPE.MSG_TYPE_CONTENT.value,\n                    \"created_at\": client.discussion.current_message.created_at,\n                    \"started_generating_at\": client.discussion.current_message.started_generating_at,\n                    \"finished_generating_at\": client.discussion.current_message.finished_generating_at,\n                    \"nb_tokens\": client.discussion.current_message.nb_tokens,\n                    \"parameters\": parameters,\n                    \"metadata\": metadata,\n                    \"binding\": self.binding.binding_folder_name,\n                    \"model\": self.model.model_name,\n                    \"personality\": self.personality.name,\n                },\n                to=client_id,\n            )\n        )\n        if (\n            operation_type\n            and operation_type.value < MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_INFO.value\n        ):\n            client.discussion.update_message(\n                client.generated_text,\n                new_metadata=mtdt,\n                new_ui=ui,\n                started_generating_at=client.discussion.current_message.started_generating_at,\n                nb_tokens=client.discussion.current_message.nb_tokens,\n            )\n\n    def update_message_content(\n        self,\n        client_id,\n        chunk,\n        operation_type: MSG_OPERATION_TYPE = MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT,\n        message_type: MSG_TYPE = None,\n    ):\n        client = self.session.get_client(client_id)\n        client.discussion.current_message.finished_generating_at = (\n            datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        )\n        client.discussion.current_message.nb_tokens = self.nb_received_tokens\n\n        if self.nb_received_tokens == 1:\n            client.discussion.current_message.started_generating_at = (\n                datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            )\n\n        run_async(\n            partial(\n                self.sio.emit,\n                \"update_message\",\n                {\n                    \"sender\": self.personality.name,\n                    \"id\": client.discussion.current_message.id,\n                    \"content\": chunk,\n                    \"discussion_id\": client.discussion.discussion_id,\n                    \"operation_type\": operation_type.value,\n                    \"message_type\": (\n                        client.discussion.current_message.message_type\n                        if message_type is None\n                        else message_type\n                    ),\n                    \"created_at\": client.discussion.current_message.created_at,\n                    \"started_generating_at\": client.discussion.current_message.started_generating_at,\n                    \"finished_generating_at\": client.discussion.current_message.finished_generating_at,\n                    \"nb_tokens\": client.discussion.current_message.nb_tokens,\n                    \"binding\": self.binding.binding_folder_name,\n                    \"model\": self.model.model_name,\n                    \"personality\": self.personality.name,\n                },\n                to=client_id,\n            )\n        )\n\n        client.discussion.update_message_content(\n            client.generated_text,\n            started_generating_at=client.discussion.current_message.started_generating_at,\n            nb_tokens=client.discussion.current_message.nb_tokens,\n        )\n\n    def update_message_step(\n        self, client_id, step_text, msg_operation_type: MSG_OPERATION_TYPE = None\n    ):\n        client = self.session.get_client(client_id)\n        if msg_operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP:\n            client.discussion.current_message.add_step(step_text, \"instant\", True, True)\n        elif msg_operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_START:\n            client.discussion.current_message.add_step(\n                step_text, \"start_end\", True, False\n            )\n        elif (\n            msg_operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_END_SUCCESS\n        ):\n            client.discussion.current_message.add_step(\n                step_text, \"start_end\", True, True\n            )\n        elif (\n            msg_operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_END_FAILURE\n        ):\n            client.discussion.current_message.add_step(\n                step_text, \"start_end\", False, True\n            )\n\n        run_async(\n            partial(\n                self.sio.emit,\n                \"update_message\",\n                {\n                    \"id\": client.discussion.current_message.id,\n                    \"discussion_id\": client.discussion.discussion_id,\n                    \"operation_type\": msg_operation_type.value,\n                    \"steps\": client.discussion.current_message.steps,\n                },\n                to=client_id,\n            )\n        )\n\n    def update_message_metadata(self, client_id, metadata):\n        client = self.session.get_client(client_id)\n        md = (\n            json.dumps(metadata)\n            if type(metadata) == dict or type(metadata) == list\n            else metadata\n        )\n        run_async(\n            partial(\n                self.sio.emit,\n                \"update_message\",\n                {\n                    \"sender\": self.personality.name,\n                    \"id\": client.discussion.current_message.id,\n                    \"metadata\": md,\n                    \"discussion_id\": client.discussion.discussion_id,\n                    \"operation_type\": MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_JSON_INFOS.value,\n                },\n                to=client_id,\n            )\n        )\n\n        client.discussion.update_message_metadata(metadata)\n\n    def update_message_ui(self, client_id, ui):\n        client = self.session.get_client(client_id)\n\n        run_async(\n            partial(\n                self.sio.emit,\n                \"update_message\",\n                {\n                    \"sender\": self.personality.name,\n                    \"id\": client.discussion.current_message.id,\n                    \"ui\": ui,\n                    \"discussion_id\": client.discussion.discussion_id,\n                    \"operation_type\": MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_UI.value,\n                },\n                to=client_id,\n            )\n        )\n\n        client.discussion.update_message_ui(ui)\n\n    def close_message(self, client_id):\n        client = self.session.get_client(client_id)\n        for msg in client.discussion.messages:\n            if msg.steps is not None:\n                for step in msg.steps:\n                    step[\"done\"] = True\n        if not client.discussion:\n            return\n        # fix halucination\n        if len(client.generated_text) > 0 and len(self.start_header_id_template) > 0:\n            client.generated_text = client.generated_text.split(\n                f\"{self.start_header_id_template}\"\n            )[0]\n        # Send final message\n        client.discussion.current_message.finished_generating_at = (\n            datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        )\n        try:\n            client.discussion.current_message.nb_tokens = len(\n                self.model.tokenize(client.generated_text)\n            )\n        except:\n            client.discussion.current_message.nb_tokens = None\n        run_async(\n            partial(\n                self.sio.emit,\n                \"close_message\",\n                {\n                    \"sender\": self.personality.name,\n                    \"id\": client.discussion.current_message.id,\n                    \"content\": client.generated_text,\n                    \"binding\": self.binding.binding_folder_name,\n                    \"model\": self.model.model_name,\n                    \"personality\": self.personality.name,\n                    \"created_at\": client.discussion.current_message.created_at,\n                    \"started_generating_at\": client.discussion.current_message.started_generating_at,\n                    \"finished_generating_at\": client.discussion.current_message.finished_generating_at,\n                    \"nb_tokens\": client.discussion.current_message.nb_tokens,\n                },\n                to=client_id,\n            )\n        )\n\n    def process_data(\n        self,\n        data: str | list | None,\n        operation_type: MSG_OPERATION_TYPE,\n        client_id: str = 0,\n        personality: AIPersonality = None,\n    ):\n        \"\"\"\n        Processes a data of generated text\n        \"\"\"\n        client = self.session.get_client(client_id)\n        if data is None and operation_type in [\n            MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT,\n            MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_ADD_CHUNK,\n        ]:\n            return\n\n        if data is not None:\n            if not client_id in list(self.session.clients.keys()):\n                self.error(\"Connection lost\", client_id=client_id)\n                return\n        if operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP:\n            ASCIIColors.info(\"--> Step:\" + data)\n            self.update_message_step(client_id, data, operation_type)\n        elif operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_START:\n            ASCIIColors.info(\"--> Step started:\" + data)\n            self.update_message_step(client_id, data, operation_type)\n        elif operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_END_SUCCESS:\n            ASCIIColors.success(\"--> Step ended:\" + data)\n            self.update_message_step(client_id, data, operation_type)\n        elif operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_END_FAILURE:\n            ASCIIColors.success(\"--> Step ended:\" + data)\n            self.update_message_step(client_id, data, operation_type)\n        elif operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_WARNING:\n            self.warning(data, client_id=client_id)\n            ASCIIColors.error(\"--> Exception from personality:\" + data)\n        elif operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_EXCEPTION:\n            self.error(data, client_id=client_id)\n            ASCIIColors.error(\"--> Exception from personality:\" + data)\n            return\n        elif operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_INFO:\n            self.info(data, client_id=client_id)\n            ASCIIColors.info(\"--> Info:\" + data)\n            return\n        elif operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_UI:\n            self.update_message_ui(client_id, data)\n            return\n        elif operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_JSON_INFOS:\n            self.update_message_metadata(client_id, data)\n            return\n        elif operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_NEW_MESSAGE:\n            self.nb_received_tokens = 0\n            self.start_time = datetime.now()\n            self.update_message_step(\n                client_id,\n                \"🔥 warming up ...\",\n                MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_END_SUCCESS,\n            )\n            self.update_message_step(\n                client_id,\n                \"✍ generating ...\",\n                MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_END_SUCCESS,\n            )\n            self.new_message(\n                client_id,\n                self.personality.name if personality is None else personality.name,\n                data,\n                message_type=MSG_TYPE.MSG_TYPE_CONTENT,\n            )\n            return\n        elif operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_FINISHED_MESSAGE:\n            self.close_message(client_id)\n            return\n        elif operation_type == MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_ADD_CHUNK:\n            if self.nb_received_tokens == 0:\n                self.start_time = datetime.now()\n                try:\n                    self.update_message_step(\n                        client_id,\n                        \"🔥 warming up ...\",\n                        MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_END_SUCCESS,\n                    )\n                    self.update_message_step(\n                        client_id,\n                        \"✍ generating ...\",\n                        MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_END_SUCCESS,\n                    )\n                except Exception as ex:\n                    trace_exception(ex)\n                    ASCIIColors.warning(\"Couldn't send status update to client\")\n            dt = (datetime.now() - self.start_time).seconds\n            if dt == 0:\n                dt = 1\n            spd = self.nb_received_tokens / dt\n            if self.config.debug_show_chunks:\n                print(data, end=\"\", flush=True)\n            # ASCIIColors.green(f\"Received {self.nb_received_tokens} tokens (speed: {spd:.2f}t/s)              \",end=\"\\r\",flush=True)\n            sys.stdout = sys.__stdout__\n            sys.stdout.flush()\n            if data:\n                client.generated_text += data\n            antiprompt = self.personality.detect_antiprompt(client.generated_text)\n            if antiprompt:\n                ASCIIColors.warning(f\"\\n{antiprompt} detected. Stopping generation\")\n                client.generated_text = self.remove_text_from_string(\n                    client.generated_text, antiprompt\n                )\n                self.update_message_content(\n                    client_id,\n                    client.generated_text,\n                    MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT,\n                )\n                return False\n            else:\n                self.nb_received_tokens += 1\n                if client.continuing and client.first_chunk:\n                    self.update_message_content(\n                        client_id,\n                        client.generated_text,\n                        MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT,\n                    )\n                else:\n                    self.update_message_content(\n                        client_id, data, MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_ADD_CHUNK\n                    )\n\n                client.first_chunk = False\n                # if stop generation is detected then stop\n                if not self.cancel_gen:\n                    return True\n                else:\n                    self.cancel_gen = False\n                    ASCIIColors.warning(\"Generation canceled\")\n                    return False\n        # Stream the generated text to the main process\n        elif operation_type in [\n            MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT,\n            MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT_INVISIBLE_TO_AI,\n            MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT_INVISIBLE_TO_USER,\n        ]:\n            if self.nb_received_tokens == 0:\n                self.start_time = datetime.now()\n                try:\n                    self.update_message_step(\n                        client_id,\n                        \"🔥 warming up ...\",\n                        MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_END_SUCCESS,\n                    )\n                    self.update_message_step(\n                        client_id,\n                        \"✍ generating ...\",\n                        MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_END_SUCCESS,\n                    )\n\n                except Exception as ex:\n                    ASCIIColors.warning(\"Couldn't send status update to client\")\n\n            client.generated_text = data\n            antiprompt = self.personality.detect_antiprompt(client.generated_text)\n            if antiprompt:\n                ASCIIColors.warning(f\"\\n{antiprompt} detected. Stopping generation\")\n                client.generated_text = self.remove_text_from_string(\n                    client.generated_text, antiprompt\n                )\n                self.update_message_content(\n                    client_id, client.generated_text, operation_type\n                )\n                return False\n\n            self.update_message_content(client_id, data, operation_type)\n            return True\n        # Stream the generated text to the frontend\n        else:\n            self.update_message_content(client_id, data, operation_type)\n        return True\n\n    def generate(self, context_details, is_continue, client_id, callback=None):\n        full_prompt, tokens = self.personality.build_context(\n            context_details, is_continue, True\n        )\n        n_predict = self.personality.compute_n_predict(tokens)\n        if self.config.debug and self.config.debug_show_final_full_prompt:\n            ASCIIColors.highlight(\n                full_prompt,\n                [\n                    r\n                    for r in [\n                        self.config.discussion_prompt_separator,\n                        self.config.start_header_id_template,\n                        self.config.end_header_id_template,\n                        self.config.separator_template,\n                        self.config.start_user_header_id_template,\n                        self.config.end_user_header_id_template,\n                        self.config.end_user_message_id_template,\n                        self.config.start_ai_header_id_template,\n                        self.config.end_ai_header_id_template,\n                        self.config.end_ai_message_id_template,\n                        self.config.system_message_template,\n                    ]\n                    if r != \"\" and r != \"\\n\"\n                ],\n            )\n\n        if self.config.use_smart_routing:\n            if (\n                self.config.smart_routing_router_model != \"\"\n                and len(self.config.smart_routing_models_description) >= 2\n            ):\n                ASCIIColors.yellow(\"Using smart routing\")\n                self.personality.step_start(\"Routing request\")\n                self.back_model = (\n                    f\"{self.binding.binding_folder_name}::{self.model.model_name}\"\n                )\n                try:\n                    if not hasattr(self, \"routing_model\") or self.routing_model is None:\n                        binding, model_name = self.model_path_to_binding_model(\n                            self.config.smart_routing_router_model\n                        )\n                        self.select_model(binding, model_name)\n                        self.routing_model = self.model\n                    else:\n                        self.set_active_model(self.routing_model)\n\n                    models = [\n                        f\"{k}\"\n                        for k, v in self.config.smart_routing_models_description.items()\n                    ]\n                    \n                    code = self.personality.generate_custom_code(\n                        \"\\n\".join([\n                            self.system_full_header,\n                        \"Given the following list of models:\"]+\n                        [\n                            f\"{k}: {v}\"\n                            for k, v in self.config.smart_routing_models_description.items()\n                        ]+[\n                        \"!@>prompt:\" + context_details[\"prompt\"],\n                        \"\"\"Given the prompt, which model among the previous list is the most suited and why?\n\nYou must answer with json code placed inside the markdown code tag like this:\n```json\n{\n    \"choice_index\": [an int representing the index of the choice made]\n    \"justification\": \"[Justify the choice]\",\n}\nMake sure you fill all fields and to use the exact same keys as the template.\nDon't forget encapsulate the code inside a markdown code tag. This is mandatory.\n\n!@>assistant:\"\"\"\n                        ]\n                    ))\n\n                    if code:\n                        output_id = code[\"choice_index\"]\n                        explanation = code[\"justification\"]\n                        binding, model_name = self.model_path_to_binding_model(\n                            models[output_id]\n                        )\n                        self.select_model(\n                            binding, model_name, destroy_previous_model=False\n                        )\n                        self.personality.step_end(\"Routing request\")\n                        self.personality.step(f\"Choice explanation: {explanation}\")\n                        self.personality.step(f\"Selected {models[output_id]}\")\n                    else:\n                        ASCIIColors.error(\n                            \"Model failed to find the most suited model for your request\"\n                        )\n                        self.info(\n                            \"Model failed to find the most suited model for your request\"\n                        )\n                        binding, model_name = self.model_path_to_binding_model(\n                            models[0]\n                        )\n                        self.select_model(\n                            binding, model_name, destroy_previous_model=False\n                        )\n                        self.personality.step_end(\"Routing request\")\n                        self.personality.step(f\"Complexity level: {output_id}\")\n                        self.personality.step(f\"Selected {models[output_id]}\")\n                except Exception as ex:\n                    self.error(\"Failed to route beceause of this error : \" + str(ex))\n                    self.personality.step_end(\"Routing request\", False)\n            else:\n                ASCIIColors.yellow(\n                    \"Warning! Smart routing is active but one of the following requirements are not met\"\n                )\n                ASCIIColors.yellow(\"- smart_routing_router_model must be set correctly\")\n                ASCIIColors.yellow(\n                    \"- smart_routing_models_description must contain at least one model\"\n                )\n\n        if self.personality.processor is not None:\n            ASCIIColors.info(\"Running workflow\")\n            try:\n                self.personality.callback = callback\n                client = self.session.get_client(client_id)\n                self.personality.vectorizer = client.discussion.vectorizer\n                self.personality.text_files = client.discussion.text_files\n                self.personality.image_files = client.discussion.image_files\n                self.personality.audio_files = client.discussion.audio_files\n                output = self.personality.processor.run_workflow(\n                    context_details, client, callback\n                )\n            except Exception as ex:\n                trace_exception(ex)\n                # Catch the exception and get the traceback as a list of strings\n                traceback_lines = traceback.format_exception(\n                    type(ex), ex, ex.__traceback__\n                )\n                # Join the traceback lines into a single string\n                traceback_text = \"\".join(traceback_lines)\n                ASCIIColors.error(f\"Workflow run failed.\\nError:{ex}\")\n                ASCIIColors.error(traceback_text)\n                if callback:\n                    callback(\n                        f\"Workflow run failed\\nError:{ex}\",\n                        MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_EXCEPTION,\n                    )\n                return\n            print(\"Finished executing the workflow\")\n            return output\n\n        txt = self._generate(full_prompt, n_predict, client_id, callback)\n        ASCIIColors.success(\"\\nFinished executing the generation\")\n\n        if (\n            self.config.use_smart_routing\n            and self.config.restore_model_after_smart_routing\n        ):\n            if (\n                self.config.smart_routing_router_model != \"\"\n                and len(self.config.smart_routing_models_description) >= 2\n            ):\n                ASCIIColors.yellow(\"Restoring model\")\n                self.personality.step_start(\"Restoring main model\")\n                binding, model_name = self.model_path_to_binding_model(self.back_model)\n                self.select_model(binding, model_name)\n                self.personality.step_end(\"Restoring main model\")\n\n        return txt\n\n    def _generate(self, prompt, n_predict, client_id, callback=None):\n        client = self.session.get_client(client_id)\n        if client is None:\n            return None\n        self.nb_received_tokens = 0\n        self.start_time = datetime.now()\n        if self.model is not None:\n            if (\n                self.model.binding_type == BindingType.TEXT_IMAGE\n                and len(client.discussion.image_files) > 0\n            ):\n                if self.config[\"override_personality_model_parameters\"]:\n                    output = self.model.generate_with_images(\n                        prompt,\n                        client.discussion.image_files,\n                        callback=callback,\n                        n_predict=n_predict,\n                        temperature=self.config[\"temperature\"],\n                        top_k=self.config[\"top_k\"],\n                        top_p=self.config[\"top_p\"],\n                        repeat_penalty=self.config[\"repeat_penalty\"],\n                        repeat_last_n=self.config[\"repeat_last_n\"],\n                        seed=self.config[\"seed\"],\n                        n_threads=self.config[\"n_threads\"],\n                    )\n                else:\n                    prompt = \"\\n\".join(\n                        [\n                            f\"{self.start_header_id_template}{self.system_message_template}{self.end_header_id_template}I am an AI assistant that can converse and analyze images. When asked to locate something in an image you send, I will reply with:\",\n                            \"boundingbox(image_index, label, left, top, width, height)\",\n                            \"Where:\",\n                            \"image_index: 0-based index of the image\",\n                            \"label: brief description of what is located\",\n                            \"left, top: x,y coordinates of top-left box corner (0-1 scale)\",\n                            \"width, height: box dimensions as fraction of image size\",\n                            \"Coordinates have origin (0,0) at top-left, (1,1) at bottom-right.\",\n                            \"For other queries, I will respond conversationally to the best of my abilities.\",\n                            prompt,\n                        ]\n                    )\n                    output = self.model.generate_with_images(\n                        prompt,\n                        client.discussion.image_files,\n                        callback=callback,\n                        n_predict=n_predict,\n                        temperature=self.personality.model_temperature,\n                        top_k=self.personality.model_top_k,\n                        top_p=self.personality.model_top_p,\n                        repeat_penalty=self.personality.model_repeat_penalty,\n                        repeat_last_n=self.personality.model_repeat_last_n,\n                        seed=self.config[\"seed\"],\n                        n_threads=self.config[\"n_threads\"],\n                    )\n                    try:\n                        post_processed_output = process_ai_output(\n                            output,\n                            client.discussion.image_files,\n                            client.discussion.discussion_folder,\n                        )\n                        if len(post_processed_output) != output:\n                            self.process_data(\n                                post_processed_output,\n                                MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT,\n                                client_id=client_id,\n                            )\n                    except Exception as ex:\n                        ASCIIColors.error(str(ex))\n            else:\n                if self.config[\"override_personality_model_parameters\"]:\n                    output = self.model.generate(\n                        prompt,\n                        callback=callback,\n                        n_predict=n_predict,\n                        temperature=self.config[\"temperature\"],\n                        top_k=self.config[\"top_k\"],\n                        top_p=self.config[\"top_p\"],\n                        repeat_penalty=self.config[\"repeat_penalty\"],\n                        repeat_last_n=self.config[\"repeat_last_n\"],\n                        seed=self.config[\"seed\"],\n                        n_threads=self.config[\"n_threads\"],\n                    )\n                else:\n                    output = self.model.generate(\n                        prompt,\n                        callback=callback,\n                        n_predict=n_predict,\n                        temperature=self.personality.model_temperature,\n                        top_k=self.personality.model_top_k,\n                        top_p=self.personality.model_top_p,\n                        repeat_penalty=self.personality.model_repeat_penalty,\n                        repeat_last_n=self.personality.model_repeat_last_n,\n                        seed=self.config[\"seed\"],\n                        n_threads=self.config[\"n_threads\"],\n                    )\n        else:\n            print(\n                \"No model is installed or selected. Please make sure to install a model and select it inside your configuration before attempting to communicate with the model.\"\n            )\n            print(\"To do this: Install the model to your models/<binding name> folder.\")\n            print(\n                \"Then set your model information in your local configuration file that you can find in configs/local_config.yaml\"\n            )\n            print(\"You can also use the ui to set your model in the settings page.\")\n            output = \"\"\n        return output\n\n    def start_message_generation(\n        self,\n        message,\n        message_id,\n        client_id,\n        is_continue=False,\n        generation_type=None,\n        force_using_internet=False,\n    ):\n        client = self.session.get_client(client_id)\n        if self.personality is None:\n            self.warning(\"Select a personality\")\n            return\n        ASCIIColors.info(f\"Text generation requested by client: {client_id}\")\n        # send the message to the bot\n        if client.discussion:\n            try:\n                ASCIIColors.info(\n                    f\"Received message : {message.content} ({len(self.model.tokenize(message.content))})\"\n                )\n                # First we need to send the new message ID to the client\n                if is_continue:\n                    client.discussion.load_message(message_id)\n                    client.generated_text = message.content\n                else:\n                    self.send_refresh(client_id)\n                    self.new_message(client_id, self.personality.name, \"\")\n                    self.update_message_step(\n                        client_id,\n                        \"🔥 warming up ...\",\n                        MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_START,\n                    )\n\n                # prepare query and reception\n                context_details = self.prepare_query(\n                    client_id,\n                    message_id,\n                    is_continue,\n                    n_tokens=self.config.min_n_predict,\n                    generation_type=generation_type,\n                    force_using_internet=force_using_internet,\n                    previous_chunk=client.generated_text if is_continue else \"\",\n                )\n\n                ASCIIColors.info(\n                    f\"prompt has {self.config.ctx_size-context_details['available_space']} tokens\"\n                )\n                ASCIIColors.info(\n                    f\"warmup for generating up to {min(context_details['available_space'],self.config.max_n_predict)} tokens\"\n                )\n                self.prepare_reception(client_id)\n                self.generating = True\n                client.processing = True\n                try:\n                    self.generate(\n                        context_details,\n                        client_id=client_id,\n                        is_continue=is_continue,\n                        callback=partial(self.process_data, client_id=client_id),\n                    )\n                    if (\n                        self.tts\n                        and self.config.auto_read\n                        and len(self.personality.audio_samples) > 0\n                    ):\n                        try:\n                            self.process_data(\n                                \"Generating voice output\",\n                                MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_START,\n                                client_id=client_id,\n                            )\n                            from lollms.services.tts.xtts.lollms_xtts import \\\n                                LollmsXTTS\n\n                            voice = self.config.xtts_current_voice\n                            if voice != \"main_voice\":\n                                voices_folder = self.lollms_paths.custom_voices_path\n                            else:\n                                voices_folder = (\n                                    Path(__file__).parent.parent.parent\n                                    / \"services/xtts/voices\"\n                                )\n\n                            if self.xtts.ready:\n                                language = convert_language_name(\n                                    self.personality.language\n                                )\n                                self.xtts.set_speaker_folder(\n                                    Path(self.personality.audio_samples[0]).parent\n                                )\n                                fn = (\n                                    self.personality.name.lower()\n                                    .replace(\" \", \"_\")\n                                    .replace(\".\", \"\")\n                                )\n                                fn = f\"{fn}_{message_id}.wav\"\n                                url = f\"audio/{fn}\"\n                                self.xtts.tts_file(\n                                    client.generated_text,\n                                    Path(self.personality.audio_samples[0]).name,\n                                    f\"{fn}\",\n                                    language=language,\n                                )\n                                fl = f\"\\n\".join(\n                                    [\n                                        f\"<audio controls>\",\n                                        f'    <source src=\"{url}\" type=\"audio/wav\">',\n                                        f\"    Your browser does not support the audio element.\",\n                                        f\"</audio>\",\n                                    ]\n                                )\n                                self.process_data(\n                                    \"Generating voice output\",\n                                    operation_type=MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_END_SUCCESS,\n                                    client_id=client_id,\n                                )\n                                self.process_data(\n                                    fl,\n                                    MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_UI,\n                                    client_id=client_id,\n                                )\n                            else:\n                                self.InfoMessage(\n                                    \"xtts is not up yet.\\nPlease wait for it to load then try again. This may take some time.\"\n                                )\n\n                        except Exception as ex:\n                            ASCIIColors.error(\"Couldn't read\")\n                            trace_exception(ex)\n                    print()\n                    ASCIIColors.success(\"## Done Generation ##\")\n                    print()\n                except Exception as ex:\n                    trace_exception(ex)\n                    print()\n                    ASCIIColors.error(\"## Generation Error ##\")\n                    print()\n\n                self.cancel_gen = False\n                sources_text = \"\"\n                if len(context_details[\"documentation_entries\"]) > 0:\n                    sources_text += '<div class=\"text-gray-400 mr-10px\">Sources:</div>'\n                    sources_text += '<div class=\"mt-4 flex flex-col items-start gap-x-2 gap-y-1.5 text-sm\" style=\"max-height: 500px; overflow-y: auto;\">'\n                    for source in context_details[\"documentation_entries\"]:\n                        title = source[\"document_title\"]\n                        path = source[\"document_path\"]\n                        content = source[\"chunk_content\"]\n                        size = source[\"chunk_size\"]\n                        similarity = source[\"similarity\"]\n                        sources_text += f\"\"\"\n                            <div class=\"source-item\">\n                                <button onclick=\"var details = document.getElementById('source-details-{title}-{message_id}'); details.style.display = details.style.display === 'none' ? 'block' : 'none';\" style=\"text-align: left; font-weight: bold;\"><strong>{title}</strong> - ({similarity*100:.2f}%)</button>\n                                <div id=\"source-details-{title}-{message_id}\" style=\"display:none;\">\n                                    <div style=\"max-height: 200px; overflow-y: auto;\">\n                                        <p><strong>Path:</strong> {path}</p>\n                                        <p><strong>Content:</strong> {content}</p>\n                                        <p><strong>Size:</strong> {size}</p>\n                                        <p><strong>Similarity:</strong> {similarity}</p>\n                                    </div>\n                                </div>\n                            </div>\n                        \"\"\"\n                    sources_text += \"</div>\"\n                    self.personality.ui(sources_text)\n                if len(context_details[\"skills\"]) > 0:\n                    sources_text += '<div class=\"text-gray-400 mr-10px\">Memories:</div>'\n                    sources_text += '<div class=\"mt-4 w-full flex flex-col items-start gap-x-2 gap-y-1.5 text-sm\" style=\"max-height: 500px; overflow-y: auto;\">'\n                    ind = 0\n                    for skill in context_details[\"skills\"]:\n                        sources_text += f\"\"\"\n                            <div class=\"source-item\">\n                                <button onclick=\"var details = document.getElementById('source-details-{ind}-{message_id}'); details.style.display = details.style.display === 'none' ? 'block' : 'none';\" style=\"text-align: left; font-weight: bold;\"><strong>Memory {ind}: {skill['title']}</strong> - ({skill['similarity']*100:.2f}%)</button>\n                                <div id=\"source-details-{ind}-{message_id}\" style=\"display:none;\">\n                                    <div class=\"w-full\" style=\"max-height: 200px; overflow-y: auto;\">\n                                        <pre>{skill['content']}</pre>\n                                    </div>    \n                                </div>\n                            </div>\n                        \"\"\"\n                        ind += 1\n                    sources_text += \"</div>\"\n                    self.personality.ui(sources_text)\n                # Send final message\n                if (\n                    self.config.activate_internet_search\n                    or force_using_internet\n                    or generation_type == \"full_context_with_internet\"\n                ):\n                    from lollms.internet import get_favicon_url, get_root_url\n\n                    sources_text += \"\"\"\n                    <div class=\"mt-4 text-sm\">\n                        <div class=\"text-gray-500 font-semibold mb-2\">Sources:</div>\n                        <div class=\"grid grid-cols-1 sm:grid-cols-2 md:grid-cols-3 lg:grid-cols-4 gap-4 h-64 overflow-y-auto scrollbar-thin scrollbar-thumb-gray-300 scrollbar-track-gray-100\">\n                    \"\"\"\n\n                    for source in context_details[\"internet_search_infos\"]:\n                        url = source[\"url\"]\n                        title = source[\"title\"]\n                        brief = source[\"brief\"]\n                        favicon_url = (\n                            get_favicon_url(url)\n                            or \"/personalities/generic/lollms/assets/logo.png\"\n                        )\n                        root_url = get_root_url(url)\n\n                        sources_text += f\"\"\"\n                        <div class=\"relative flex flex-col items-start gap-2 rounded-lg border border-gray-200 bg-white p-3 shadow-sm transition duration-200 ease-in-out transform hover:scale-105 hover:border-gray-300 hover:shadow-lg dark:border-gray-700 dark:bg-gray-800 dark:hover:border-gray-600 dark:hover:shadow-lg animate-fade-in\">\n                            <a class=\"flex items-center w-full\" target=\"_blank\" href=\"{url}\" title=\"{brief}\">\n                                <img class=\"h-8 w-8 rounded-full\" src=\"{favicon_url}\" alt=\"{title}\" onerror=\"this.onerror=null;this.src='/personalities/generic/lollms/assets/logo.png';\">\n                                <div class=\"ml-2\">\n                                    <div class=\"text-gray-700 dark:text-gray-300 font-semibold text-sm\">{title}</div>\n                                    <div class=\"text-gray-500 dark:text-gray-400 text-xs\">{root_url}</div>\n                                    <div class=\"text-gray-400 dark:text-gray-500 text-xs\">{brief}</div>\n                                </div>\n                            </a>\n                        </div>\n                        \"\"\"\n\n                    sources_text += \"\"\"\n                        </div>\n                    </div>\n                    \"\"\"\n\n                    # Add CSS for animations and scrollbar styles\n                    sources_text += \"\"\"\n                    <style>\n                    @keyframes fadeIn {\n                        from { opacity: 0; transform: translateY(10px); }\n                        to { opacity: 1; transform: translateY(0); }\n                    }\n                    .animate-fade-in {\n                        animation: fadeIn 0.5s ease-in-out;\n                    }\n                    .scrollbar-thin::-webkit-scrollbar {\n                        width: 8px;\n                    }\n                    .scrollbar-thin::-webkit-scrollbar-thumb {\n                        background-color: #cbd5e1; /* Tailwind gray-300 */\n                        border-radius: 10px;\n                    }\n                    .scrollbar-thin::-webkit-scrollbar-track {\n                        background: #f9fafb; /* Tailwind gray-100 */\n                    }\n                    </style>\n                    \"\"\"\n                    self.personality.ui(sources_text)\n\n            except Exception as ex:\n                trace_exception(ex)\n            try:\n                self.update_message_step(\n                    client_id,\n                    \"🔥 warming up ...\",\n                    MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_END_SUCCESS,\n                )\n                self.update_message_step(\n                    client_id,\n                    \"✍ generating ...\",\n                    MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_STEP_END_SUCCESS,\n                )\n            except Exception as ex:\n                ASCIIColors.warning(\"Couldn't send status update to client\")\n            self.close_message(client_id)\n\n            client.processing = False\n            # Clients are now kept forever\n            # if client.schedule_for_deletion:\n            #    self.session.remove_client(client.client_id, client.client_id)\n\n            ASCIIColors.success(\n                f\" ╔══════════════════════════════════════════════════╗ \"\n            )\n            ASCIIColors.success(\n                f\" ║                        Done                      ║ \"\n            )\n            ASCIIColors.success(\n                f\" ╚══════════════════════════════════════════════════╝ \"\n            )\n            if self.config.auto_title:\n                d = client.discussion\n                ttl = d.title()\n                if ttl is None or ttl == \"\" or ttl == \"untitled\":\n                    title = self.make_discussion_title(d, client_id=client_id)\n                    d.rename(title)\n                    asyncio.run(\n                        self.sio.emit(\n                            \"disucssion_renamed\",\n                            {\n                                \"status\": True,\n                                \"discussion_id\": d.discussion_id,\n                                \"title\": title,\n                            },\n                            to=client_id,\n                        )\n                    )\n            self.busy = False\n\n        else:\n            self.cancel_gen = False\n            # No discussion available\n            ASCIIColors.warning(\"No discussion selected!!!\")\n\n            self.error(\"No discussion selected!!!\", client_id=client_id)\n\n            print()\n            self.busy = False\n            return \"\"\n\n    def receive_and_generate(self, text, client: Client, callback=None):\n        prompt = text\n        try:\n            nb_tokens = len(self.model.tokenize(prompt))\n        except:\n            nb_tokens = None\n\n        message = client.discussion.add_message(\n            operation_type=MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT.value,\n            sender_type=SENDER_TYPES.SENDER_TYPES_USER.value,\n            sender=(\n                self.config.user_name.strip()\n                if self.config.use_user_name_in_discussions\n                else self.personality.user_message_prefix\n            ),\n            content=prompt,\n            metadata=None,\n            parent_message_id=self.message_id,\n            nb_tokens=nb_tokens,\n        )\n        context_details = self.prepare_query(\n            client.client_id,\n            client.discussion.current_message.id,\n            False,\n            n_tokens=self.config.min_n_predict,\n            force_using_internet=False,\n        )\n        self.new_message(\n            client.client_id,\n            self.personality.name,\n            operation_type=MSG_OPERATION_TYPE.MSG_OPERATION_TYPE_SET_CONTENT,\n            content=\"\",\n        )\n        client.generated_text = \"\"\n        ASCIIColors.info(\n            f\"prompt has {self.config.ctx_size-context_details['available_space']} tokens\"\n        )\n        ASCIIColors.info(\n            f\"warmup for generating up to {min(context_details['available_space'],self.config.max_n_predict if self.config.max_n_predict else self.config.ctx_size)} tokens\"\n        )\n        self.generate(\n            context_details,\n            client.client_id,\n            False,\n            (\n                callback\n                if callback\n                else partial(self.process_data, client_id=client.client_id)\n            ),\n        )\n        self.close_message(client.client_id)\n        return client.generated_text\n"
        },
        {
          "name": "models.yaml",
          "type": "blob",
          "size": 39.625,
          "content": "- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/openrouter/auto\n  name: openrouter/auto\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Auto (best for prompt)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/nousresearch/nous-capybara-7b:free\n  name: nousresearch/nous-capybara-7b:free\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Nous: Capybara 7B (free)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/mistralai/mistral-7b-instruct:free\n  name: mistralai/mistral-7b-instruct:free\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Mistral 7B Instruct (free)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/openchat/openchat-7b:free\n  name: openchat/openchat-7b:free\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: OpenChat 3.5 (free)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/gryphe/mythomist-7b:free\n  name: gryphe/mythomist-7b:free\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: MythoMist 7B (free)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/undi95/toppy-m-7b:free\n  name: undi95/toppy-m-7b:free\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Toppy M 7B (free)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/openrouter/cinematika-7b:free\n  name: openrouter/cinematika-7b:free\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Cinematika 7B (alpha) (free)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/google/gemma-7b-it:free\n  name: google/gemma-7b-it:free\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Google: Gemma 7B (free)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/koboldai/psyfighter-13b-2\n  name: koboldai/psyfighter-13b-2\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Psyfighter v2 13B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/intel/neural-chat-7b\n  name: intel/neural-chat-7b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Neural Chat 7B v3.1\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/gryphe/mythomax-l2-13b\n  name: gryphe/mythomax-l2-13b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: MythoMax 13B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/pygmalionai/mythalion-13b\n  name: pygmalionai/mythalion-13b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Pygmalion: Mythalion 13B'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/xwin-lm/xwin-lm-70b\n  name: xwin-lm/xwin-lm-70b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Xwin 70B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/alpindale/goliath-120b\n  name: alpindale/goliath-120b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Goliath 120B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/neversleep/noromaid-20b\n  name: neversleep/noromaid-20b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Noromaid 20B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/gryphe/mythomist-7b\n  name: gryphe/mythomist-7b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: MythoMist 7B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/sophosympatheia/midnight-rose-70b\n  name: sophosympatheia/midnight-rose-70b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Midnight Rose 70B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/sao10k/fimbulvetr-11b-v2\n  name: sao10k/fimbulvetr-11b-v2\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Fimbulvetr 11B v2\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/undi95/remm-slerp-l2-13b:extended\n  name: undi95/remm-slerp-l2-13b:extended\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: ReMM SLERP 13B (extended)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/gryphe/mythomax-l2-13b:extended\n  name: gryphe/mythomax-l2-13b:extended\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: MythoMax 13B (extended)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/meta-llama/llama-3-8b-instruct:extended\n  name: meta-llama/llama-3-8b-instruct:extended\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Meta: Llama 3 8B Instruct (extended)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/mancer/weaver\n  name: mancer/weaver\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Mancer: Weaver (alpha)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/nousresearch/nous-capybara-7b\n  name: nousresearch/nous-capybara-7b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Nous: Capybara 7B'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/meta-llama/codellama-34b-instruct\n  name: meta-llama/codellama-34b-instruct\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Meta: CodeLlama 34B Instruct'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/codellama/codellama-70b-instruct\n  name: codellama/codellama-70b-instruct\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Meta: CodeLlama 70B Instruct'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/phind/phind-codellama-34b\n  name: phind/phind-codellama-34b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Phind: CodeLlama 34B v2'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/teknium/openhermes-2-mistral-7b\n  name: teknium/openhermes-2-mistral-7b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: OpenHermes 2 Mistral 7B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/undi95/remm-slerp-l2-13b\n  name: undi95/remm-slerp-l2-13b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: ReMM SLERP 13B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/openrouter/cinematika-7b\n  name: openrouter/cinematika-7b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Cinematika 7B (alpha)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/01-ai/yi-34b-chat\n  name: 01-ai/yi-34b-chat\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Yi 34B Chat\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/01-ai/yi-34b\n  name: 01-ai/yi-34b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Yi 34B (base)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/01-ai/yi-6b\n  name: 01-ai/yi-6b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Yi 6B (base)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/togethercomputer/stripedhyena-nous-7b\n  name: togethercomputer/stripedhyena-nous-7b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: StripedHyena Nous 7B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/togethercomputer/stripedhyena-hessian-7b\n  name: togethercomputer/stripedhyena-hessian-7b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: StripedHyena Hessian 7B (base)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/mistralai/mixtral-8x7b\n  name: mistralai/mixtral-8x7b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Mixtral 8x7B (base)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/nousresearch/nous-hermes-yi-34b\n  name: nousresearch/nous-hermes-yi-34b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Nous: Hermes 2 Yi 34B'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/nousresearch/nous-hermes-2-mixtral-8x7b-sft\n  name: nousresearch/nous-hermes-2-mixtral-8x7b-sft\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Nous: Hermes 2 Mixtral 8x7B SFT'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/nousresearch/nous-hermes-2-mistral-7b-dpo\n  name: nousresearch/nous-hermes-2-mistral-7b-dpo\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Nous: Hermes 2 Mistral 7B DPO'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/meta-llama/llama-3-70b-instruct\n  name: meta-llama/llama-3-70b-instruct\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Meta: Llama 3 70B Instruct'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/mistralai/mixtral-8x7b-instruct:nitro\n  name: mistralai/mixtral-8x7b-instruct:nitro\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Mixtral 8x7B Instruct (nitro)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/open-orca/mistral-7b-openorca\n  name: open-orca/mistral-7b-openorca\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Mistral OpenOrca 7B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/huggingfaceh4/zephyr-7b-beta\n  name: huggingfaceh4/zephyr-7b-beta\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Hugging Face: Zephyr 7B'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/openai/gpt-3.5-turbo\n  name: openai/gpt-3.5-turbo\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'OpenAI: GPT-3.5 Turbo'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/openai/gpt-3.5-turbo-0125\n  name: openai/gpt-3.5-turbo-0125\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'OpenAI: GPT-3.5 Turbo 16k'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/openai/gpt-3.5-turbo-16k\n  name: openai/gpt-3.5-turbo-16k\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'OpenAI: GPT-3.5 Turbo 16k'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/openai/gpt-4-turbo\n  name: openai/gpt-4-turbo\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'OpenAI: GPT-4 Turbo'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/openai/gpt-4-turbo-preview\n  name: openai/gpt-4-turbo-preview\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'OpenAI: GPT-4 Turbo Preview'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/openai/gpt-4\n  name: openai/gpt-4\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'OpenAI: GPT-4'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/openai/gpt-4-32k\n  name: openai/gpt-4-32k\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'OpenAI: GPT-4 32k'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/openai/gpt-4-vision-preview\n  name: openai/gpt-4-vision-preview\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'OpenAI: GPT-4 Vision'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/openai/gpt-3.5-turbo-instruct\n  name: openai/gpt-3.5-turbo-instruct\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'OpenAI: GPT-3.5 Turbo Instruct'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/google/palm-2-chat-bison\n  name: google/palm-2-chat-bison\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Google: PaLM 2 Chat'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/google/palm-2-codechat-bison\n  name: google/palm-2-codechat-bison\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Google: PaLM 2 Code Chat'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/google/palm-2-chat-bison-32k\n  name: google/palm-2-chat-bison-32k\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Google: PaLM 2 Chat 32k'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/google/palm-2-codechat-bison-32k\n  name: google/palm-2-codechat-bison-32k\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Google: PaLM 2 Code Chat 32k'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/google/gemini-pro\n  name: google/gemini-pro\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Google: Gemini Pro 1.0'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/google/gemini-pro-vision\n  name: google/gemini-pro-vision\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Google: Gemini Pro Vision 1.0'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/google/gemini-pro-1.5\n  name: google/gemini-pro-1.5\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Google: Gemini Pro 1.5 (preview)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/perplexity/pplx-70b-online\n  name: perplexity/pplx-70b-online\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Perplexity: PPLX 70B Online'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/perplexity/pplx-7b-online\n  name: perplexity/pplx-7b-online\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Perplexity: PPLX 7B Online'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/perplexity/pplx-7b-chat\n  name: perplexity/pplx-7b-chat\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Perplexity: PPLX 7B Chat'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/perplexity/pplx-70b-chat\n  name: perplexity/pplx-70b-chat\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Perplexity: PPLX 70B Chat'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/perplexity/sonar-small-chat\n  name: perplexity/sonar-small-chat\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Perplexity: Sonar 7B'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/perplexity/sonar-medium-chat\n  name: perplexity/sonar-medium-chat\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Perplexity: Sonar 8x7B'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/perplexity/sonar-small-online\n  name: perplexity/sonar-small-online\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Perplexity: Sonar 7B Online'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/perplexity/sonar-medium-online\n  name: perplexity/sonar-medium-online\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Perplexity: Sonar 8x7B Online'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/fireworks/firellava-13b\n  name: fireworks/firellava-13b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: FireLLaVA 13B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/anthropic/claude-3-opus\n  name: anthropic/claude-3-opus\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Anthropic: Claude 3 Opus'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/anthropic/claude-3-sonnet\n  name: anthropic/claude-3-sonnet\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Anthropic: Claude 3 Sonnet'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/anthropic/claude-3-haiku\n  name: anthropic/claude-3-haiku\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Anthropic: Claude 3 Haiku'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/anthropic/claude-2\n  name: anthropic/claude-2\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Anthropic: Claude v2'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/anthropic/claude-2.1\n  name: anthropic/claude-2.1\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Anthropic: Claude v2.1'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/anthropic/claude-2.0\n  name: anthropic/claude-2.0\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Anthropic: Claude v2.0'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/anthropic/claude-instant-1\n  name: anthropic/claude-instant-1\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Anthropic: Claude Instant v1'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/anthropic/claude-instant-1.2\n  name: anthropic/claude-instant-1.2\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Anthropic: Claude Instant v1.2'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/anthropic/claude-3-opus:beta\n  name: anthropic/claude-3-opus:beta\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Anthropic: Claude 3 Opus (self-moderated)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/anthropic/claude-3-sonnet:beta\n  name: anthropic/claude-3-sonnet:beta\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Anthropic: Claude 3 Sonnet (self-moderated)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/anthropic/claude-3-haiku:beta\n  name: anthropic/claude-3-haiku:beta\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Anthropic: Claude 3 Haiku (self-moderated)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/anthropic/claude-2:beta\n  name: anthropic/claude-2:beta\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Anthropic: Claude v2 (self-moderated)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/anthropic/claude-2.1:beta\n  name: anthropic/claude-2.1:beta\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Anthropic: Claude v2.1 (self-moderated)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/anthropic/claude-2.0:beta\n  name: anthropic/claude-2.0:beta\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Anthropic: Claude v2.0 (self-moderated)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/anthropic/claude-instant-1:beta\n  name: anthropic/claude-instant-1:beta\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Anthropic: Claude Instant v1 (self-moderated)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/meta-llama/llama-2-13b-chat\n  name: meta-llama/llama-2-13b-chat\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Meta: Llama v2 13B Chat'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/meta-llama/llama-2-70b-chat\n  name: meta-llama/llama-2-70b-chat\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Meta: Llama v2 70B Chat'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/nousresearch/nous-hermes-llama2-13b\n  name: nousresearch/nous-hermes-llama2-13b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Nous: Hermes 13B'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/nousresearch/nous-capybara-34b\n  name: nousresearch/nous-capybara-34b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Nous: Capybara 34B'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/jondurbin/airoboros-l2-70b\n  name: jondurbin/airoboros-l2-70b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Airoboros 70B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/austism/chronos-hermes-13b\n  name: austism/chronos-hermes-13b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Chronos Hermes 13B v2\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/mistralai/mistral-7b-instruct\n  name: mistralai/mistral-7b-instruct\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Mistral 7B Instruct\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/teknium/openhermes-2.5-mistral-7b\n  name: teknium/openhermes-2.5-mistral-7b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: OpenHermes 2.5 Mistral 7B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/openchat/openchat-7b\n  name: openchat/openchat-7b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: OpenChat 3.5\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/undi95/toppy-m-7b\n  name: undi95/toppy-m-7b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Toppy M 7B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/lizpreciatior/lzlv-70b-fp16-hf\n  name: lizpreciatior/lzlv-70b-fp16-hf\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: lzlv 70B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/mistralai/mixtral-8x7b-instruct\n  name: mistralai/mixtral-8x7b-instruct\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Mixtral 8x7B Instruct\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/cognitivecomputations/dolphin-mixtral-8x7b\n  name: cognitivecomputations/dolphin-mixtral-8x7b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: \"Dolphin 2.6 Mixtral 8x7B \\U0001F42C\"\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/neversleep/noromaid-mixtral-8x7b-instruct\n  name: neversleep/noromaid-mixtral-8x7b-instruct\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Noromaid Mixtral 8x7B Instruct\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/nousresearch/nous-hermes-2-mixtral-8x7b-dpo\n  name: nousresearch/nous-hermes-2-mixtral-8x7b-dpo\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Nous: Hermes 2 Mixtral 8x7B DPO'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/rwkv/rwkv-5-world-3b\n  name: rwkv/rwkv-5-world-3b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: RWKV v5 World 3B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/recursal/rwkv-5-3b-ai-town\n  name: recursal/rwkv-5-3b-ai-town\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: RWKV v5 3B AI Town\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/recursal/eagle-7b\n  name: recursal/eagle-7b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'RWKV v5: Eagle 7B'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/google/gemma-7b-it\n  name: google/gemma-7b-it\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Google: Gemma 7B'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/databricks/dbrx-instruct\n  name: databricks/dbrx-instruct\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Databricks: DBRX 132B Instruct'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/huggingfaceh4/zephyr-orpo-141b-a35b\n  name: huggingfaceh4/zephyr-orpo-141b-a35b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Zephyr 141B-A35B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/meta-llama/llama-3-8b-instruct\n  name: meta-llama/llama-3-8b-instruct\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Meta: Llama 3 8B Instruct'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/microsoft/wizardlm-2-8x22b\n  name: microsoft/wizardlm-2-8x22b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: WizardLM-2 8x22B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/microsoft/wizardlm-2-7b\n  name: microsoft/wizardlm-2-7b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: WizardLM-2 7B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/mistralai/mixtral-8x22b\n  name: mistralai/mixtral-8x22b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Mistral: Mixtral 8x22B (base)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/mistralai/mixtral-8x22b-instruct\n  name: mistralai/mixtral-8x22b-instruct\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Mistral: Mixtral 8x22B Instruct'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/lynn/soliloquy-l3\n  name: lynn/soliloquy-l3\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Lynn: Llama 3 Soliloquy 8B'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/huggingfaceh4/zephyr-7b-beta:free\n  name: huggingfaceh4/zephyr-7b-beta:free\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Hugging Face: Zephyr 7B (free)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/meta-llama/llama-2-70b-chat:nitro\n  name: meta-llama/llama-2-70b-chat:nitro\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Meta: Llama v2 70B Chat (nitro)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/gryphe/mythomax-l2-13b:nitro\n  name: gryphe/mythomax-l2-13b:nitro\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: MythoMax 13B (nitro)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/mistralai/mistral-7b-instruct:nitro\n  name: mistralai/mistral-7b-instruct:nitro\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Mistral 7B Instruct (nitro)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/google/gemma-7b-it:nitro\n  name: google/gemma-7b-it:nitro\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Google: Gemma 7B (nitro)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/undi95/toppy-m-7b:nitro\n  name: undi95/toppy-m-7b:nitro\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Toppy M 7B (nitro)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/microsoft/wizardlm-2-8x22b:nitro\n  name: microsoft/wizardlm-2-8x22b:nitro\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: WizardLM-2 8x22B (nitro)\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/meta-llama/llama-3-8b-instruct:nitro\n  name: meta-llama/llama-3-8b-instruct:nitro\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Meta: Llama 3 8B Instruct (nitro)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/meta-llama/llama-3-70b-instruct:nitro\n  name: meta-llama/llama-3-70b-instruct:nitro\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Meta: Llama 3 70B Instruct (nitro)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/haotian-liu/llava-13b\n  name: haotian-liu/llava-13b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Llava 13B\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/nousresearch/nous-hermes-2-vision-7b\n  name: nousresearch/nous-hermes-2-vision-7b\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Nous: Hermes 2 Vision 7B (alpha)'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/mistralai/mistral-tiny\n  name: mistralai/mistral-tiny\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Mistral Tiny\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/mistralai/mistral-small\n  name: mistralai/mistral-small\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Mistral Small\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/mistralai/mistral-medium\n  name: mistralai/mistral-medium\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Mistral Medium\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/mistralai/mistral-large\n  name: mistralai/mistral-large\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: Mistral Large\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/cohere/command\n  name: cohere/command\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Cohere: Command'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/cohere/command-r\n  name: cohere/command-r\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Cohere: Command R'\n    size: Not so much\n- category: generic\n  datasets: unknown\n  icon: ''\n  last_commit_time: ''\n  license: commercial\n  model_creator: ''\n  model_creator_link: /models/cohere/command-r-plus\n  name: cohere/command-r-plus\n  quantizer: null\n  rank: 0.0\n  type: api\n  variants:\n  - name: 'Cohere: Command R+'\n    size: Not so much\n"
        },
        {
          "name": "notebooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "package-lock.json",
          "type": "blob",
          "size": 0.55859375,
          "content": "{\n  \"name\": \"lollms-webui\",\n  \"lockfileVersion\": 3,\n  \"requires\": true,\n  \"packages\": {\n    \"\": {\n      \"dependencies\": {\n        \"vue-draggable-resizable\": \"^2.3.0\"\n      }\n    },\n    \"node_modules/vue-draggable-resizable\": {\n      \"version\": \"2.3.0\",\n      \"resolved\": \"https://registry.npmjs.org/vue-draggable-resizable/-/vue-draggable-resizable-2.3.0.tgz\",\n      \"integrity\": \"sha512-77CLRj1TPwB30pwsjOf3pkd1UzYanCdKXbqhILJ0Oo5QQl50lvBfyQCXxMFzwWwTc3sbBbQH3FfWSV+BkoSElA==\",\n      \"engines\": {\n        \"node\": \">= 4.0.0\",\n        \"npm\": \">= 3.0.0\"\n      }\n    }\n  }\n}\n"
        },
        {
          "name": "package.json",
          "type": "blob",
          "size": 0.06640625,
          "content": "{\n  \"dependencies\": {\n    \"vue-draggable-resizable\": \"^2.3.0\"\n  }\n}\n"
        },
        {
          "name": "presets",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.4404296875,
          "content": "colorama\nnumpy==1.26.*\npandas\nPillow>=9.5.0\npyyaml\nrequests\nrich\nscipy\ntqdm\nsetuptools\nwheel\npsutil\npytest\nGitPython\nascii-colors>=0.4.2\nbeautifulsoup4\npackaging\n\nfastapi\nuvicorn\npython-multipart\npython-socketio\npython-socketio[client]\npython-socketio[asyncio_client]\n\npydantic\nselenium\ntiktoken\n\npipmaster>=0.1.7\n\nlollmsvectordb>=1.1.0\nfreedom-search>=0.1.9\nscrapemaster>=0.2.0\nfreedom_search\nlollms_client>=0.7.5\n\naiofiles\npython-multipart\nzipfile36"
        },
        {
          "name": "requirements_dev.txt",
          "type": "blob",
          "size": 0.380859375,
          "content": "colorama\ndatasets\neinops\njinja2==3.1.5\nnumpy>=1.26.0\npandas\nPillow>=9.5.0\npyyaml\nrequests\nrich\nsafetensors==0.4.1\nscipy\nsentencepiece\ntensorboard\ntransformers>=4.37.*\ntqdm\nsetuptools\ntqdm\npsutil\npytest\nGitPython\nascii_colors>=0.1.4\nbeautifulsoup4\npackaging\n\nfastapi\nuvicorn\npython-multipart\npython-socketio\npython-socketio[client]\npython-socketio[asyncio_client]\n\npydantic\nselenium\ntiktoken"
        },
        {
          "name": "restart_script.py",
          "type": "blob",
          "size": 0.5732421875,
          "content": "import os\nimport sys\n\n\ndef main():\n    if len(sys.argv) != 1:\n        print(\"Usage: python restart_script.py\")\n        sys.exit(1)\n\n    # Reload the main script with the original arguments\n    temp_file = \"temp_args.txt\"\n    if os.path.exists(temp_file):\n        with open(temp_file, \"r\") as file:\n            args = file.read().split()\n        main_script = \"app.py\"\n        os.system(f\"python {main_script} {' '.join(args)}\")\n        os.remove(temp_file)\n    else:\n        print(\"Error: Temporary arguments file not found.\")\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.0634765625,
          "content": "from pathlib import Path\nfrom typing import Union\n\nimport setuptools\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\n\ndef read_requirements(path: Union[str, Path]):\n    with open(path, \"r\") as file:\n        return file.read().splitlines()\n\n\nrequirements = read_requirements(\"requirements.txt\")\nrequirements_dev = read_requirements(\"requirements_dev.txt\")\n\nsetuptools.setup(\n    name=\"Lollms-webui\",\n    version=\"5.0.2\",\n    author=\"Saifeddine ALOUI\",\n    author_email=\"aloui.saifeddine@gmail.com\",\n    description=\"A web ui for running chat models with different bindings. Supports multiple personalities and extensions.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/ParisNeo/lollms-webui\",\n    packages=setuptools.find_packages(),\n    install_requires=requirements,\n    extras_require={\"dev\": requirements_dev},\n    classifiers=[\n        \"Programming Language :: Python :: 3.10\",\n        \"License :: OSI Approved :: Apache 2.0 License\",\n        \"Operating System :: OS Independent\",\n    ],\n)\n"
        },
        {
          "name": "tailwind.config.js",
          "type": "blob",
          "size": 0.2275390625,
          "content": "// tailwind.config.js\nmodule.exports = {\n    purge: [],\n    darkMode: 'media', // or 'media' or 'class'\n    theme: {\n      extend: {},\n    },\n    content: [\n        './templates/index.html',\n        './templates/main.html',\n    ]\n  }"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests_and_fun",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "train",
          "type": "tree",
          "content": null
        },
        {
          "name": "update_script.py",
          "type": "blob",
          "size": 7.3955078125,
          "content": "import argparse\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport git\nimport pipmaster as pm\nfrom ascii_colors import ASCIIColors, trace_exception\n\nif not pm.is_installed(\"PyQt5\"):\n    pm.install(\"PyQt5\")\n\nimport sys\n\nfrom PyQt5.QtWidgets import QApplication, QMessageBox\n\n\ndef show_error_dialog(message):\n    try:\n        app = QApplication(sys.argv)\n        msg = QMessageBox()\n        msg.setIcon(QMessageBox.Critical)\n        msg.setText(message)\n        msg.setWindowTitle(\"Error\")\n        msg.exec_()\n    except:\n        ASCIIColors.error(message)\n\n\ndef run_git_pull():\n    try:\n        ASCIIColors.info(\"----------------> Updating the code <-----------------------\")\n\n        repo = git.Repo(Path(__file__).parent)\n        origin = repo.remotes.origin\n\n        # Fetch the latest changes\n        origin.fetch()\n\n        # Check if there are any changes to pull\n        if repo.head.commit == origin.refs.main.commit:\n            ASCIIColors.success(\"Already up-to-date.\")\n\n        # Discard local changes and force update\n        try:\n            repo.git.reset(\"--hard\", \"origin/main\")\n            repo.git.clean(\"-fd\")\n            origin.pull()\n            ASCIIColors.success(\"Successfully updated the code.\")\n        except git.GitCommandError as e:\n            error_message = f\"Failed to update the code: {str(e)}\"\n            ASCIIColors.error(error_message)\n            # show_error_dialog(error_message)\n\n        print(\"Updating submodules\")\n        try:\n            repo.git.submodule(\"update\", \"--init\", \"--recursive\", \"--force\")\n        except Exception as ex:\n            error_message = f\"Couldn't update submodules: {str(ex)}\"\n            ASCIIColors.error(error_message)\n            # show_error_dialog(error_message)\n        try:\n            # Checkout the main branch on each submodule\n            for submodule in repo.submodules:\n                try:\n                    submodule_repo = submodule.module()\n                    submodule_repo.git.fetch(\"origin\")\n                    submodule_repo.git.reset(\"--hard\", \"origin/main\")\n                    submodule_repo.git.clean(\"-fd\")\n                    ASCIIColors.success(f\"Updated submodule: {submodule}.\")\n                except Exception as ex:\n                    print(\n                        f\"Couldn't update submodule {submodule}: {str(ex)}\\nPlease report the error to ParisNeo either on Discord or on github.\"\n                    )\n\n            execution_path = Path(os.getcwd())\n        except Exception as ex:\n            error_message = f\"Couldn't update submodules: {str(ex)}\\nPlease report the error to ParisNeo either on Discord or on github.\"\n            ASCIIColors.error(error_message)\n            # show_error_dialog(error_message)\n        try:\n            # Update lollms_core\n            ASCIIColors.info(\"Updating lollms_core\")\n            lollms_core_path = execution_path / \"lollms_core\"\n            if lollms_core_path.exists():\n                subprocess.run(\n                    [\"git\", \"-C\", str(lollms_core_path), \"fetch\", \"origin\"], check=True\n                )\n                subprocess.run(\n                    [\n                        \"git\",\n                        \"-C\",\n                        str(lollms_core_path),\n                        \"reset\",\n                        \"--hard\",\n                        \"origin/main\",\n                    ],\n                    check=True,\n                )\n                subprocess.run(\n                    [\"git\", \"-C\", str(lollms_core_path), \"clean\", \"-fd\"], check=True\n                )\n                ASCIIColors.success(\"Successfully updated lollms_core\")\n            else:\n                ASCIIColors.warning(\"lollms_core directory not found\")\n\n        except Exception as ex:\n            error_message = f\"Couldn't update submodules: {str(ex)}\"\n            ASCIIColors.error(error_message)\n            # show_error_dialog(error_message)\n\n        return True\n    except Exception as e:\n        error_message = f\"Error during git operations: {str(e)}\"\n        ASCIIColors.error(error_message)\n        # show_error_dialog(error_message)\n        return False\n\n\ndef get_valid_input():\n    while True:\n        update_prompt = (\n            \"New code is updated. Do you want to update the requirements? (y/n): \"\n        )\n        user_response = input(update_prompt).strip().lower()\n\n        if user_response in [\"y\", \"yes\"]:\n            return \"yes\"\n        elif user_response in [\"n\", \"no\"]:\n            return \"no\"\n        else:\n            print(\"Invalid input. Please respond with 'y' or 'n'.\")\n\n\ndef install_requirements():\n    try:\n        # Get valid input from the user\n        user_choice = get_valid_input()\n\n        # Enhance the text based on the user's response\n        if user_choice == \"yes\":\n            enhanced_text = \"Great choice! Updating requirements ensures your project stays up-to-date with the latest changes.\"\n            print(enhanced_text)\n            subprocess.check_call(\n                [\n                    sys.executable,\n                    \"-m\",\n                    \"pip\",\n                    \"install\",\n                    \"--upgrade\",\n                    \"-r\",\n                    \"requirements.txt\",\n                ]\n            )\n            subprocess.check_call(\n                [\n                    sys.executable,\n                    \"-m\",\n                    \"pip\",\n                    \"install\",\n                    \"--upgrade\",\n                    \"-e\",\n                    \"lollms_core\",\n                ]\n            )\n            ASCIIColors.success(\"Successfully installed requirements\")\n        else:  # user_choice == 'no'\n            enhanced_text = \"Understood. Skipping the requirements update. Make sure to update them later if needed.\"\n            print(enhanced_text)\n    except subprocess.CalledProcessError as e:\n        error_message = f\"Error during pip install: {str(e)}\"\n        ASCIIColors.error(error_message)\n        show_error_dialog(error_message)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--repo\",\n        type=str,\n        default=\"https://github.com/ParisNeo/lollms-webui.git\",\n        help=\"Path to the Git repository\",\n    )\n    args = parser.parse_args()\n\n    repo_path = args.repo\n\n    # Perform git pull to update the repository\n    if run_git_pull():\n        # Install the new requirements\n        install_requirements()\n\n        # Reload the main script with the original arguments\n        temp_file = \"temp_args.txt\"\n        if os.path.exists(temp_file):\n            with open(temp_file, \"r\") as file:\n                args = file.read().split()\n            main_script = \"app.py\"  # Replace with the actual name of your main script\n            os.system(f\"{sys.executable} {main_script} {' '.join(args)}\")\n            try:\n                os.remove(temp_file)\n            except Exception as e:\n                error_message = f\"Couldn't remove temp file. Try to remove it manually.\\nThe file is located here: {temp_file}\\nError: {str(e)}\"\n                ASCIIColors.warning(error_message)\n        else:\n            error_message = \"Error: Temporary arguments file not found.\"\n            ASCIIColors.error(error_message)\n            show_error_dialog(error_message)\n            sys.exit(1)\n    else:\n        error_message = (\n            \"Update process failed. Please check the console for more details.\"\n        )\n        ASCIIColors.error(error_message)\n        show_error_dialog(error_message)\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "utilities",
          "type": "tree",
          "content": null
        },
        {
          "name": "web",
          "type": "tree",
          "content": null
        },
        {
          "name": "zoos",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}