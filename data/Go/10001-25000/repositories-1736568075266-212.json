{
  "metadata": {
    "timestamp": 1736568075266,
    "page": 212,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "jackc/pgx",
      "stars": 11088,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.2734375,
          "content": "# Compiled Object files, Static and Dynamic libs (Shared Objects)\n*.o\n*.a\n*.so\n\n# Folders\n_obj\n_test\n\n# Architecture specific extensions/prefixes\n*.[568vq]\n[568vq].out\n\n*.cgo1.go\n*.cgo2.c\n_cgo_defun.c\n_cgo_gotypes.go\n_cgo_export.*\n\n_testmain.go\n\n*.exe\n\n.envrc\n/.testdb\n\n.DS_Store\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 22.1923828125,
          "content": "# 5.7.2 (December 21, 2024)\n\n* Fix prepared statement already exists on batch prepare failure\n* Add commit query to tx options (Lucas Hild)\n* Fix pgtype.Timestamp json unmarshal (Shean de Montigny-Desautels)\n* Add message body size limits in frontend and backend (zene)\n* Add xid8 type\n* Ensure planning encodes and scans cannot infinitely recurse\n* Implement pgtype.UUID.String() (Konstantin Grachev)\n* Switch from ExecParams to Exec in ValidateConnectTargetSessionAttrs functions (Alexander Rumyantsev)\n* Update golang.org/x/crypto\n\n# 5.7.1 (September 10, 2024)\n\n* Fix data race in tracelog.TraceLog\n* Update puddle to v2.2.2. This removes the import of nanotime via linkname.\n* Update golang.org/x/crypto and golang.org/x/text\n\n# 5.7.0 (September 7, 2024)\n\n* Add support for sslrootcert=system (Yann Soubeyrand)\n* Add LoadTypes to load multiple types in a single SQL query (Nick Farrell)\n* Add XMLCodec supports encoding + scanning XML column type like json (nickcruess-soda)\n* Add MultiTrace (Stepan Rabotkin)\n* Add TraceLogConfig with customizable TimeKey (stringintech)\n* pgx.ErrNoRows wraps sql.ErrNoRows to aid in database/sql compatibility with native pgx functions (merlin)\n* Support scanning binary formatted uint32 into string / TextScanner (jennifersp)\n* Fix interval encoding to allow 0s and avoid extra spaces (Carlos Pérez-Aradros Herce)\n* Update pgservicefile - fixes panic when parsing invalid file\n* Better error message when reading past end of batch\n* Don't print url when url.Parse returns an error (Kevin Biju)\n* Fix snake case name normalization collision in RowToStructByName with db tag (nolandseigler)\n* Fix: Scan and encode types with underlying types of arrays\n\n# 5.6.0 (May 25, 2024)\n\n* Add StrictNamedArgs (Tomas Zahradnicek)\n* Add support for macaddr8 type (Carlos Pérez-Aradros Herce)\n* Add SeverityUnlocalized field to PgError / Notice\n* Performance optimization of RowToStructByPos/Name (Zach Olstein)\n* Allow customizing context canceled behavior for pgconn\n* Add ScanLocation to pgtype.Timestamp[tz]Codec\n* Add custom data to pgconn.PgConn\n* Fix ResultReader.Read() to handle nil values\n* Do not encode interval microseconds when they are 0 (Carlos Pérez-Aradros Herce)\n* pgconn.SafeToRetry checks for wrapped errors (tjasko)\n* Failed connection attempts include all errors\n* Optimize LargeObject.Read (Mitar)\n* Add tracing for connection acquire and release from pool (ngavinsir)\n* Fix encode driver.Valuer not called when nil\n* Add support for custom JSON marshal and unmarshal (Mitar)\n* Use Go default keepalive for TCP connections (Hans-Joachim Kliemeck)\n\n# 5.5.5 (March 9, 2024)\n\nUse spaces instead of parentheses for SQL sanitization.\n\nThis still solves the problem of negative numbers creating a line comment, but this avoids breaking edge cases such as\n`set foo to $1` where the substitution is taking place in a location where an arbitrary expression is not allowed.\n\n# 5.5.4 (March 4, 2024)\n\nFix CVE-2024-27304\n\nSQL injection can occur if an attacker can cause a single query or bind message to exceed 4 GB in size. An integer\noverflow in the calculated message size can cause the one large message to be sent as multiple messages under the\nattacker's control.\n\nThanks to Paul Gerste for reporting this issue.\n\n* Fix behavior of CollectRows to return empty slice if Rows are empty (Felix)\n* Fix simple protocol encoding of json.RawMessage\n* Fix *Pipeline.getResults should close pipeline on error\n* Fix panic in TryFindUnderlyingTypeScanPlan (David Kurman)\n* Fix deallocation of invalidated cached statements in a transaction\n* Handle invalid sslkey file\n* Fix scan float4 into sql.Scanner\n* Fix pgtype.Bits not making copy of data from read buffer. This would cause the data to be corrupted by future reads.\n\n# 5.5.3 (February 3, 2024)\n\n* Fix: prepared statement already exists\n* Improve CopyFrom auto-conversion of text-ish values\n* Add ltree type support (Florent Viel)\n* Make some properties of Batch and QueuedQuery public (Pavlo Golub)\n* Add AppendRows function (Edoardo Spadolini)\n* Optimize convert UUID [16]byte to string (Kirill Malikov)\n* Fix: LargeObject Read and Write of more than ~1GB at a time (Mitar)\n\n# 5.5.2 (January 13, 2024)\n\n* Allow NamedArgs to start with underscore\n* pgproto3: Maximum message body length support (jeremy.spriet)\n* Upgrade golang.org/x/crypto to v0.17.0\n* Add snake_case support to RowToStructByName (Tikhon Fedulov)\n* Fix: update description cache after exec prepare (James Hartig)\n* Fix: pipeline checks if it is closed (James Hartig and Ryan Fowler)\n* Fix: normalize timeout / context errors during TLS startup (Samuel Stauffer)\n* Add OnPgError for easier centralized error handling (James Hartig)\n\n# 5.5.1 (December 9, 2023)\n\n* Add CopyFromFunc helper function. (robford)\n* Add PgConn.Deallocate method that uses PostgreSQL protocol Close message.\n* pgx uses new PgConn.Deallocate method. This allows deallocating statements to work in a failed transaction. This fixes a case where the prepared statement map could become invalid.\n* Fix: Prefer driver.Valuer over json.Marshaler for json fields. (Jacopo)\n* Fix: simple protocol SQL sanitizer previously panicked if an invalid $0 placeholder was used. This now returns an error instead. (maksymnevajdev)\n* Add pgtype.Numeric.ScanScientific (Eshton Robateau)\n\n# 5.5.0 (November 4, 2023)\n\n* Add CollectExactlyOneRow. (Julien GOTTELAND)\n* Add OpenDBFromPool to create *database/sql.DB from *pgxpool.Pool. (Lev Zakharov)\n* Prepare can automatically choose statement name based on sql. This makes it easier to explicitly manage prepared statements.\n* Statement cache now uses deterministic, stable statement names.\n* database/sql prepared statement names are deterministically generated.\n* Fix: SendBatch wasn't respecting context cancellation.\n* Fix: Timeout error from pipeline is now normalized.\n* Fix: database/sql encoding json.RawMessage to []byte.\n* CancelRequest: Wait for the cancel request to be acknowledged by the server. This should improve PgBouncer compatibility. (Anton Levakin)\n* stdlib: Use Ping instead of CheckConn in ResetSession\n* Add json.Marshaler and json.Unmarshaler for Float4, Float8 (Kirill Mironov)\n\n# 5.4.3 (August 5, 2023)\n\n* Fix: QCharArrayOID was defined with the wrong OID (Christoph Engelbert)\n* Fix: connect_timeout for sslmode=allow|prefer (smaher-edb)\n* Fix: pgxpool: background health check cannot overflow pool\n* Fix: Check for nil in defer when sending batch (recover properly from panic)\n* Fix: json scan of non-string pointer to pointer\n* Fix: zeronull.Timestamptz should use pgtype.Timestamptz\n* Fix: NewConnsCount was not correctly counting connections created by Acquire directly. (James Hartig)\n* RowTo(AddrOf)StructByPos ignores fields with \"-\" db tag\n* Optimization: improve text format numeric parsing (horpto)\n\n# 5.4.2 (July 11, 2023)\n\n* Fix: RowScanner errors are fatal to Rows\n* Fix: Enable failover efforts when pg_hba.conf disallows non-ssl connections (Brandon Kauffman)\n* Hstore text codec internal improvements (Evan Jones)\n* Fix: Stop timers for background reader when not in use. Fixes memory leak when closing connections (Adrian-Stefan Mares)\n* Fix: Stop background reader as soon as possible.\n* Add PgConn.SyncConn(). This combined with the above fix makes it safe to directly use the underlying net.Conn.\n\n# 5.4.1 (June 18, 2023)\n\n* Fix: concurrency bug with pgtypeDefaultMap and simple protocol (Lev Zakharov)\n* Add TxOptions.BeginQuery to allow overriding the default BEGIN query\n\n# 5.4.0 (June 14, 2023)\n\n* Replace platform specific syscalls for non-blocking IO with more traditional goroutines and deadlines. This returns to the v4 approach with some additional improvements and fixes. This restores the ability to use a pgx.Conn over an ssh.Conn as well as other non-TCP or Unix socket connections. In addition, it is a significantly simpler implementation that is less likely to have cross platform issues.\n* Optimization: The default type registrations are now shared among all connections. This saves about 100KB of memory per connection. `pgtype.Type` and `pgtype.Codec` values are now required to be immutable after registration. This was already necessary in most cases but wasn't documented until now. (Lev Zakharov)\n* Fix: Ensure pgxpool.Pool.QueryRow.Scan releases connection on panic\n* CancelRequest: don't try to read the reply (Nicola Murino)\n* Fix: correctly handle bool type aliases (Wichert Akkerman)\n* Fix: pgconn.CancelRequest: Fix unix sockets: don't use RemoteAddr()\n* Fix: pgx.Conn memory leak with prepared statement caching (Evan Jones)\n* Add BeforeClose to pgxpool.Pool (Evan Cordell)\n* Fix: various hstore fixes and optimizations (Evan Jones)\n* Fix: RowToStructByPos with embedded unexported struct\n* Support different bool string representations (Lev Zakharov)\n* Fix: error when using BatchResults.Exec on a select that returns an error after some rows.\n* Fix: pipelineBatchResults.Exec() not returning error from ResultReader\n* Fix: pipeline batch results not closing pipeline when error occurs while reading directly from results instead of using\n    a callback.\n* Fix: scanning a table type into a struct\n* Fix: scan array of record to pointer to slice of struct\n* Fix: handle null for json (Cemre Mengu)\n* Batch Query callback is called even when there is an error\n* Add RowTo(AddrOf)StructByNameLax (Audi P. Risa P)\n\n# 5.3.1 (February 27, 2023)\n\n* Fix: Support v4 and v5 stdlib in same program (Tomáš Procházka)\n* Fix: sql.Scanner not being used in certain cases\n* Add text format jsonpath support\n* Fix: fake non-blocking read adaptive wait time\n\n# 5.3.0 (February 11, 2023)\n\n* Fix: json values work with sql.Scanner\n* Fixed / improved error messages (Mark Chambers and Yevgeny Pats)\n* Fix: support scan into single dimensional arrays\n* Fix: MaxConnLifetimeJitter setting actually jitter (Ben Weintraub)\n* Fix: driver.Value representation of bytea should be []byte not string\n* Fix: better handling of unregistered OIDs\n* CopyFrom can use query cache to avoid extra round trip to get OIDs (Alejandro Do Nascimento Mora)\n* Fix: encode to json ignoring driver.Valuer\n* Support sql.Scanner on renamed base type\n* Fix: pgtype.Numeric text encoding of negative numbers (Mark Chambers)\n* Fix: connect with multiple hostnames when one can't be resolved\n* Upgrade puddle to remove dependency on uber/atomic and fix alignment issue on 32-bit platform\n* Fix: scanning json column into **string\n* Multiple reductions in memory allocations\n* Fake non-blocking read adapts its max wait time\n* Improve CopyFrom performance and reduce memory usage\n* Fix: encode []any to array\n* Fix: LoadType for composite with dropped attributes (Felix Röhrich)\n* Support v4 and v5 stdlib in same program\n* Fix: text format array decoding with string of \"NULL\"\n* Prefer binary format for arrays\n\n# 5.2.0 (December 5, 2022)\n\n* `tracelog.TraceLog` implements the pgx.PrepareTracer interface. (Vitalii Solodilov)\n* Optimize creating begin transaction SQL string (Petr Evdokimov and ksco)\n* `Conn.LoadType` supports range and multirange types (Vitalii Solodilov)\n* Fix scan `uint` and `uint64` `ScanNumeric`. This resolves a PostgreSQL `numeric` being incorrectly scanned into `uint` and `uint64`.\n\n# 5.1.1 (November 17, 2022)\n\n* Fix simple query sanitizer where query text contains a Unicode replacement character.\n* Remove erroneous `name` argument from `DeallocateAll()`. Technically, this is a breaking change, but given that method was only added 5 days ago this change was accepted. (Bodo Kaiser)\n\n# 5.1.0 (November 12, 2022)\n\n* Update puddle to v2.1.2. This resolves a race condition and a deadlock in pgxpool.\n* `QueryRewriter.RewriteQuery` now returns an error. Technically, this is a breaking change for any external implementers, but given the minimal likelihood that there are actually any external implementers this change was accepted.\n* Expose `GetSSLPassword` support to pgx.\n* Fix encode `ErrorResponse` unknown field handling. This would only affect pgproto3 being used directly as a proxy with a non-PostgreSQL server that included additional error fields.\n* Fix date text format encoding with 5 digit years.\n* Fix date values passed to a `sql.Scanner` as `string` instead of `time.Time`.\n* DateCodec.DecodeValue can return `pgtype.InfinityModifier` instead of `string` for infinite values. This now matches the behavior of the timestamp types.\n* Add domain type support to `Conn.LoadType()`.\n* Add `RowToStructByName` and `RowToAddrOfStructByName`. (Pavlo Golub)\n* Add `Conn.DeallocateAll()` to clear all prepared statements including the statement cache. (Bodo Kaiser)\n\n# 5.0.4 (October 24, 2022)\n\n* Fix: CollectOneRow prefers PostgreSQL error over pgx.ErrorNoRows\n* Fix: some reflect Kind checks to first check for nil\n* Bump golang.org/x/text dependency to placate snyk\n* Fix: RowToStructByPos on structs with multiple anonymous sub-structs (Baptiste Fontaine)\n* Fix: Exec checks if tx is closed\n\n# 5.0.3 (October 14, 2022)\n\n* Fix `driver.Valuer` handling edge cases that could cause infinite loop or crash\n\n# v5.0.2 (October 8, 2022)\n\n* Fix date encoding in text format to always use 2 digits for month and day\n* Prefer driver.Valuer over wrap plans when encoding\n* Fix scan to pointer to pointer to renamed type\n* Allow scanning NULL even if PG and Go types are incompatible\n\n# v5.0.1 (September 24, 2022)\n\n* Fix 32-bit atomic usage\n* Add MarshalJSON for Float8 (yogipristiawan)\n* Add `[` and `]` to text encoding of `Lseg`\n* Fix sqlScannerWrapper NULL handling\n\n# v5.0.0 (September 17, 2022)\n\n## Merged Packages\n\n`github.com/jackc/pgtype`, `github.com/jackc/pgconn`, and `github.com/jackc/pgproto3` are now included in the main\n`github.com/jackc/pgx` repository. Previously there was confusion as to where issues should be reported, additional\nrelease work due to releasing multiple packages, and less clear changelogs.\n\n## pgconn\n\n`CommandTag` is now an opaque type instead of directly exposing an underlying `[]byte`.\n\nThe return value `ResultReader.Values()` is no longer safe to retain a reference to after a subsequent call to `NextRow()` or `Close()`.\n\n`Trace()` method adds low level message tracing similar to the `PQtrace` function in `libpq`.\n\npgconn now uses non-blocking IO. This is a significant internal restructuring, but it should not cause any visible changes on its own. However, it is important in implementing other new features.\n\n`CheckConn()` checks a connection's liveness by doing a non-blocking read. This can be used to detect database restarts or network interruptions without executing a query or a ping.\n\npgconn now supports pipeline mode.\n\n`*PgConn.ReceiveResults` removed. Use pipeline mode instead.\n\n`Timeout()` no longer considers `context.Canceled` as a timeout error. `context.DeadlineExceeded` still is considered a timeout error.\n\n## pgxpool\n\n`Connect` and `ConnectConfig` have been renamed to `New` and `NewWithConfig` respectively. The `LazyConnect` option has been removed. Pools always lazily connect.\n\n## pgtype\n\nThe `pgtype` package has been significantly changed.\n\n### NULL Representation\n\nPreviously, types had a `Status` field that could be `Undefined`, `Null`, or `Present`. This has been changed to a\n`Valid` `bool` field to harmonize with how `database/sql` represents `NULL` and to make the zero value useable.\n\nPreviously, a type that implemented `driver.Valuer` would have the `Value` method called even on a nil pointer. All nils\nwhether typed or untyped now represent `NULL`.\n\n### Codec and Value Split\n\nPreviously, the type system combined decoding and encoding values with the value types. e.g. Type `Int8` both handled\nencoding and decoding the PostgreSQL representation and acted as a value object. This caused some difficulties when\nthere was not an exact 1 to 1 relationship between the Go types and the PostgreSQL types For example, scanning a\nPostgreSQL binary `numeric` into a Go `float64` was awkward (see https://github.com/jackc/pgtype/issues/147). This\nconcepts have been separated. A `Codec` only has responsibility for encoding and decoding values. Value types are\ngenerally defined by implementing an interface that a particular `Codec` understands (e.g. `PointScanner` and\n`PointValuer` for the PostgreSQL `point` type).\n\n### Array Types\n\nAll array types are now handled by `ArrayCodec` instead of using code generation for each new array type. This also\nmeans that less common array types such as `point[]` are now supported. `Array[T]` supports PostgreSQL multi-dimensional\narrays.\n\n### Composite Types\n\nComposite types must be registered before use. `CompositeFields` may still be used to construct and destruct composite\nvalues, but any type may now implement `CompositeIndexGetter` and `CompositeIndexScanner` to be used as a composite.\n\n### Range Types\n\nRange types are now handled with types `RangeCodec` and `Range[T]`. This allows additional user defined range types to\neasily be handled. Multirange types are handled similarly with `MultirangeCodec` and `Multirange[T]`.\n\n### pgxtype\n\n`LoadDataType` moved to `*Conn` as `LoadType`.\n\n### Bytea\n\nThe `Bytea` and `GenericBinary` types have been replaced. Use the following instead:\n\n* `[]byte` - For normal usage directly use `[]byte`.\n* `DriverBytes` - Uses driver memory only available until next database method call. Avoids a copy and an allocation.\n* `PreallocBytes` - Uses preallocated byte slice to avoid an allocation.\n* `UndecodedBytes` - Avoids any decoding. Allows working with raw bytes.\n\n### Dropped lib/pq Support\n\n`pgtype` previously supported and was tested against [lib/pq](https://github.com/lib/pq). While it will continue to work\nin most cases this is no longer supported.\n\n### database/sql Scan\n\nPreviously, most `Scan` implementations would convert `[]byte` to `string` automatically to decode a text value. Now\nonly `string` is handled. This is to allow the possibility of future binary support in `database/sql` mode by\nconsidering `[]byte` to be binary format and `string` text format. This change should have no effect for any use with\n`pgx`. The previous behavior was only necessary for `lib/pq` compatibility.\n\nAdded `*Map.SQLScanner` to create a `sql.Scanner` for types such as `[]int32` and `Range[T]` that do not implement\n`sql.Scanner` directly.\n\n### Number Type Fields Include Bit size\n\n`Int2`, `Int4`, `Int8`, `Float4`, `Float8`, and `Uint32` fields now include bit size. e.g. `Int` is renamed to `Int64`.\nThis matches the convention set by `database/sql`. In addition, for comparable types like `pgtype.Int8` and\n`sql.NullInt64` the structures are identical. This means they can be directly converted one to another.\n\n### 3rd Party Type Integrations\n\n* Extracted integrations with https://github.com/shopspring/decimal and https://github.com/gofrs/uuid to\n  https://github.com/jackc/pgx-shopspring-decimal and https://github.com/jackc/pgx-gofrs-uuid respectively. This trims\n  the pgx dependency tree.\n\n### Other Changes\n\n* `Bit` and `Varbit` are both replaced by the `Bits` type.\n* `CID`, `OID`, `OIDValue`, and `XID` are replaced by the `Uint32` type.\n* `Hstore` is now defined as `map[string]*string`.\n* `JSON` and `JSONB` types removed. Use `[]byte` or `string` directly.\n* `QChar` type removed. Use `rune` or `byte` directly.\n* `Inet` and `Cidr` types removed. Use `netip.Addr` and `netip.Prefix` directly. These types are more memory efficient than the previous `net.IPNet`.\n* `Macaddr` type removed. Use `net.HardwareAddr` directly.\n* Renamed `pgtype.ConnInfo` to `pgtype.Map`.\n* Renamed `pgtype.DataType` to `pgtype.Type`.\n* Renamed `pgtype.None` to `pgtype.Finite`.\n* `RegisterType` now accepts a `*Type` instead of `Type`.\n* Assorted array helper methods and types made private.\n\n## stdlib\n\n* Removed `AcquireConn` and `ReleaseConn` as that functionality has been built in since Go 1.13.\n\n## Reduced Memory Usage by Reusing Read Buffers\n\nPreviously, the connection read buffer would allocate large chunks of memory and never reuse them. This allowed\ntransferring ownership to anything such as scanned values without incurring an additional allocation and memory copy.\nHowever, this came at the cost of overall increased memory allocation size. But worse it was also possible to pin large\nchunks of memory by retaining a reference to a small value that originally came directly from the read buffer. Now\nownership remains with the read buffer and anything needing to retain a value must make a copy.\n\n## Query Execution Modes\n\nControl over automatic prepared statement caching and simple protocol use are now combined into query execution mode.\nSee documentation for `QueryExecMode`.\n\n## QueryRewriter Interface and NamedArgs\n\npgx now supports named arguments with the `NamedArgs` type. This is implemented via the new `QueryRewriter` interface which\nallows arbitrary rewriting of query SQL and arguments.\n\n## RowScanner Interface\n\nThe `RowScanner` interface allows a single argument to Rows.Scan to scan the entire row.\n\n## Rows Result Helpers\n\n* `CollectRows` and `RowTo*` functions simplify collecting results into a slice.\n* `CollectOneRow` collects one row using `RowTo*` functions.\n* `ForEachRow` simplifies scanning each row and executing code using the scanned values. `ForEachRow` replaces `QueryFunc`.\n\n## Tx Helpers\n\nRather than every type that implemented `Begin` or `BeginTx` methods also needing to implement `BeginFunc` and\n`BeginTxFunc` these methods have been converted to functions that take a db that implements `Begin` or `BeginTx`.\n\n## Improved Batch Query Ergonomics\n\nPreviously, the code for building a batch went in one place before the call to `SendBatch`, and the code for reading the\nresults went in one place after the call to `SendBatch`. This could make it difficult to match up the query and the code\nto handle the results. Now `Queue` returns a `QueuedQuery` which has methods `Query`, `QueryRow`, and `Exec` which can\nbe used to register a callback function that will handle the result. Callback functions are called automatically when\n`BatchResults.Close` is called.\n\n## SendBatch Uses Pipeline Mode When Appropriate\n\nPreviously, a batch with 10 unique parameterized statements executed 100 times would entail 11 network round trips. 1\nfor each prepare / describe and 1 for executing them all. Now pipeline mode is used to prepare / describe all statements\nin a single network round trip. So it would only take 2 round trips.\n\n## Tracing and Logging\n\nInternal logging support has been replaced with tracing hooks. This allows custom tracing integration with tools like OpenTelemetry. Package tracelog provides an adapter for pgx v4 loggers to act as a tracer.\n\nAll integrations with 3rd party loggers have been extracted to separate repositories. This trims the pgx dependency\ntree.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 4.6845703125,
          "content": "# Contributing\n\n## Discuss Significant Changes\n\nBefore you invest a significant amount of time on a change, please create a discussion or issue describing your\nproposal. This will help to ensure your proposed change has a reasonable chance of being merged.\n\n## Avoid Dependencies\n\nAdding a dependency is a big deal. While on occasion a new dependency may be accepted, the default answer to any change\nthat adds a dependency is no.\n\n## Development Environment Setup\n\npgx tests naturally require a PostgreSQL database. It will connect to the database specified in the `PGX_TEST_DATABASE`\nenvironment variable. The `PGX_TEST_DATABASE` environment variable can either be a URL or key-value pairs. In addition,\nthe standard `PG*` environment variables will be respected. Consider using [direnv](https://github.com/direnv/direnv) to\nsimplify environment variable handling.\n\n### Using an Existing PostgreSQL Cluster\n\nIf you already have a PostgreSQL development server this is the quickest way to start and run the majority of the pgx\ntest suite. Some tests will be skipped that require server configuration changes (e.g. those testing different\nauthentication methods).\n\nCreate and setup a test database:\n\n```\nexport PGDATABASE=pgx_test\ncreatedb\npsql -c 'create extension hstore;'\npsql -c 'create extension ltree;'\npsql -c 'create domain uint64 as numeric(20,0);'\n```\n\nEnsure a `postgres` user exists. This happens by default in normal PostgreSQL installs, but some installation methods\nsuch as Homebrew do not.\n\n```\ncreateuser -s postgres\n```\n\nEnsure your `PGX_TEST_DATABASE` environment variable points to the database you just created and run the tests.\n\n```\nexport PGX_TEST_DATABASE=\"host=/private/tmp database=pgx_test\"\ngo test ./...\n```\n\nThis will run the vast majority of the tests, but some tests will be skipped (e.g. those testing different connection methods).\n\n### Creating a New PostgreSQL Cluster Exclusively for Testing\n\nThe following environment variables need to be set both for initial setup and whenever the tests are run. (direnv is\nhighly recommended). Depending on your platform, you may need to change the host for `PGX_TEST_UNIX_SOCKET_CONN_STRING`.\n\n```\nexport PGPORT=5015\nexport PGUSER=postgres\nexport PGDATABASE=pgx_test\nexport POSTGRESQL_DATA_DIR=postgresql\n\nexport PGX_TEST_DATABASE=\"host=127.0.0.1 database=pgx_test user=pgx_md5 password=secret\"\nexport PGX_TEST_UNIX_SOCKET_CONN_STRING=\"host=/private/tmp database=pgx_test\"\nexport PGX_TEST_TCP_CONN_STRING=\"host=127.0.0.1 database=pgx_test user=pgx_md5 password=secret\"\nexport PGX_TEST_SCRAM_PASSWORD_CONN_STRING=\"host=127.0.0.1 user=pgx_scram password=secret database=pgx_test\"\nexport PGX_TEST_MD5_PASSWORD_CONN_STRING=\"host=127.0.0.1 database=pgx_test user=pgx_md5 password=secret\"\nexport PGX_TEST_PLAIN_PASSWORD_CONN_STRING=\"host=127.0.0.1 user=pgx_pw password=secret\"\nexport PGX_TEST_TLS_CONN_STRING=\"host=localhost user=pgx_ssl password=secret sslmode=verify-full sslrootcert=`pwd`/.testdb/ca.pem\"\nexport PGX_SSL_PASSWORD=certpw\nexport PGX_TEST_TLS_CLIENT_CONN_STRING=\"host=localhost user=pgx_sslcert sslmode=verify-full sslrootcert=`pwd`/.testdb/ca.pem database=pgx_test sslcert=`pwd`/.testdb/pgx_sslcert.crt sslkey=`pwd`/.testdb/pgx_sslcert.key\"\n```\n\nCreate a new database cluster.\n\n```\ninitdb --locale=en_US -E UTF-8 --username=postgres .testdb/$POSTGRESQL_DATA_DIR\n\necho \"listen_addresses = '127.0.0.1'\" >> .testdb/$POSTGRESQL_DATA_DIR/postgresql.conf\necho \"port = $PGPORT\" >> .testdb/$POSTGRESQL_DATA_DIR/postgresql.conf\ncat testsetup/postgresql_ssl.conf >> .testdb/$POSTGRESQL_DATA_DIR/postgresql.conf\ncp testsetup/pg_hba.conf .testdb/$POSTGRESQL_DATA_DIR/pg_hba.conf\n\ncd .testdb\n\n# Generate CA, server, and encrypted client certificates.\ngo run ../testsetup/generate_certs.go\n\n# Copy certificates to server directory and set permissions.\ncp ca.pem $POSTGRESQL_DATA_DIR/root.crt\ncp localhost.key $POSTGRESQL_DATA_DIR/server.key\nchmod 600 $POSTGRESQL_DATA_DIR/server.key\ncp localhost.crt $POSTGRESQL_DATA_DIR/server.crt\n\ncd ..\n```\n\n\nStart the new cluster. This will be necessary whenever you are running pgx tests.\n\n```\npostgres -D .testdb/$POSTGRESQL_DATA_DIR\n```\n\nSetup the test database in the new cluster.\n\n```\ncreatedb\npsql --no-psqlrc -f testsetup/postgresql_setup.sql\n```\n\n### PgBouncer\n\nThere are tests specific for PgBouncer that will be executed if `PGX_TEST_PGBOUNCER_CONN_STRING` is set.\n\n### Optional Tests\n\npgx supports multiple connection types and means of authentication. These tests are optional. They will only run if the\nappropriate environment variables are set. In addition, there may be tests specific to particular PostgreSQL versions,\nnon-PostgreSQL servers (e.g. CockroachDB), or connection poolers (e.g. PgBouncer). `go test ./... -v | grep SKIP` to see\nif any tests are being skipped.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.052734375,
          "content": "Copyright (c) 2013-2021 Jack Christensen\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.2255859375,
          "content": "[![Go Reference](https://pkg.go.dev/badge/github.com/jackc/pgx/v5.svg)](https://pkg.go.dev/github.com/jackc/pgx/v5)\n[![Build Status](https://github.com/jackc/pgx/actions/workflows/ci.yml/badge.svg)](https://github.com/jackc/pgx/actions/workflows/ci.yml)\n\n# pgx - PostgreSQL Driver and Toolkit\n\npgx is a pure Go driver and toolkit for PostgreSQL.\n\nThe pgx driver is a low-level, high performance interface that exposes PostgreSQL-specific features such as `LISTEN` /\n`NOTIFY` and `COPY`. It also includes an adapter for the standard `database/sql` interface.\n\nThe toolkit component is a related set of packages that implement PostgreSQL functionality such as parsing the wire protocol\nand type mapping between PostgreSQL and Go. These underlying packages can be used to implement alternative drivers,\nproxies, load balancers, logical replication clients, etc.\n\n## Example Usage\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\n\t\"github.com/jackc/pgx/v5\"\n)\n\nfunc main() {\n\t// urlExample := \"postgres://username:password@localhost:5432/database_name\"\n\tconn, err := pgx.Connect(context.Background(), os.Getenv(\"DATABASE_URL\"))\n\tif err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"Unable to connect to database: %v\\n\", err)\n\t\tos.Exit(1)\n\t}\n\tdefer conn.Close(context.Background())\n\n\tvar name string\n\tvar weight int64\n\terr = conn.QueryRow(context.Background(), \"select name, weight from widgets where id=$1\", 42).Scan(&name, &weight)\n\tif err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"QueryRow failed: %v\\n\", err)\n\t\tos.Exit(1)\n\t}\n\n\tfmt.Println(name, weight)\n}\n```\n\nSee the [getting started guide](https://github.com/jackc/pgx/wiki/Getting-started-with-pgx) for more information.\n\n## Features\n\n* Support for approximately 70 different PostgreSQL types\n* Automatic statement preparation and caching\n* Batch queries\n* Single-round trip query mode\n* Full TLS connection control\n* Binary format support for custom types (allows for much quicker encoding/decoding)\n* `COPY` protocol support for faster bulk data loads\n* Tracing and logging support\n* Connection pool with after-connect hook for arbitrary connection setup\n* `LISTEN` / `NOTIFY`\n* Conversion of PostgreSQL arrays to Go slice mappings for integers, floats, and strings\n* `hstore` support\n* `json` and `jsonb` support\n* Maps `inet` and `cidr` PostgreSQL types to `netip.Addr` and `netip.Prefix`\n* Large object support\n* NULL mapping to pointer to pointer\n* Supports `database/sql.Scanner` and `database/sql/driver.Valuer` interfaces for custom types\n* Notice response handling\n* Simulated nested transactions with savepoints\n\n## Choosing Between the pgx and database/sql Interfaces\n\nThe pgx interface is faster. Many PostgreSQL specific features such as `LISTEN` / `NOTIFY` and `COPY` are not available\nthrough the `database/sql` interface.\n\nThe pgx interface is recommended when:\n\n1. The application only targets PostgreSQL.\n2. No other libraries that require `database/sql` are in use.\n\nIt is also possible to use the `database/sql` interface and convert a connection to the lower-level pgx interface as needed.\n\n## Testing\n\nSee [CONTRIBUTING.md](./CONTRIBUTING.md) for setup instructions.\n\n## Architecture\n\nSee the presentation at Golang Estonia, [PGX Top to Bottom](https://www.youtube.com/watch?v=sXMSWhcHCf8) for a description of pgx architecture.\n\n## Supported Go and PostgreSQL Versions\n\npgx supports the same versions of Go and PostgreSQL that are supported by their respective teams. For [Go](https://golang.org/doc/devel/release.html#policy) that is the two most recent major releases and for [PostgreSQL](https://www.postgresql.org/support/versioning/) the major releases in the last 5 years. This means pgx supports Go 1.21 and higher and PostgreSQL 12 and higher. pgx also is tested against the latest version of [CockroachDB](https://www.cockroachlabs.com/product/).\n\n## Version Policy\n\npgx follows semantic versioning for the documented public API on stable releases. `v5` is the latest stable major version.\n\n## PGX Family Libraries\n\n### [github.com/jackc/pglogrepl](https://github.com/jackc/pglogrepl)\n\npglogrepl provides functionality to act as a client for PostgreSQL logical replication.\n\n### [github.com/jackc/pgmock](https://github.com/jackc/pgmock)\n\npgmock offers the ability to create a server that mocks the PostgreSQL wire protocol. This is used internally to test pgx by purposely inducing unusual errors. pgproto3 and pgmock together provide most of the foundational tooling required to implement a PostgreSQL proxy or MitM (such as for a custom connection pooler).\n\n### [github.com/jackc/tern](https://github.com/jackc/tern)\n\ntern is a stand-alone SQL migration system.\n\n### [github.com/jackc/pgerrcode](https://github.com/jackc/pgerrcode)\n\npgerrcode contains constants for the PostgreSQL error codes.\n\n## Adapters for 3rd Party Types\n\n* [github.com/jackc/pgx-gofrs-uuid](https://github.com/jackc/pgx-gofrs-uuid)\n* [github.com/jackc/pgx-shopspring-decimal](https://github.com/jackc/pgx-shopspring-decimal)\n* [github.com/twpayne/pgx-geos](https://github.com/twpayne/pgx-geos) ([PostGIS](https://postgis.net/) and [GEOS](https://libgeos.org/) via [go-geos](https://github.com/twpayne/go-geos))\n* [github.com/vgarvardt/pgx-google-uuid](https://github.com/vgarvardt/pgx-google-uuid)\n\n\n## Adapters for 3rd Party Tracers\n\n* [github.com/jackhopner/pgx-xray-tracer](https://github.com/jackhopner/pgx-xray-tracer)\n\n## Adapters for 3rd Party Loggers\n\nThese adapters can be used with the tracelog package.\n\n* [github.com/jackc/pgx-go-kit-log](https://github.com/jackc/pgx-go-kit-log)\n* [github.com/jackc/pgx-log15](https://github.com/jackc/pgx-log15)\n* [github.com/jackc/pgx-logrus](https://github.com/jackc/pgx-logrus)\n* [github.com/jackc/pgx-zap](https://github.com/jackc/pgx-zap)\n* [github.com/jackc/pgx-zerolog](https://github.com/jackc/pgx-zerolog)\n* [github.com/mcosta74/pgx-slog](https://github.com/mcosta74/pgx-slog)\n* [github.com/kataras/pgx-golog](https://github.com/kataras/pgx-golog)\n\n## 3rd Party Libraries with PGX Support\n\n### [github.com/pashagolub/pgxmock](https://github.com/pashagolub/pgxmock)\n\npgxmock is a mock library implementing pgx interfaces.\npgxmock has one and only purpose - to simulate pgx behavior in tests, without needing a real database connection.\n\n### [github.com/georgysavva/scany](https://github.com/georgysavva/scany)\n\nLibrary for scanning data from a database into Go structs and more.\n\n### [github.com/vingarcia/ksql](https://github.com/vingarcia/ksql)\n\nA carefully designed SQL client for making using SQL easier,\nmore productive, and less error-prone on Golang.\n\n### [github.com/otan/gopgkrb5](https://github.com/otan/gopgkrb5)\n\nAdds GSSAPI / Kerberos authentication support.\n\n### [github.com/wcamarao/pmx](https://github.com/wcamarao/pmx)\n\nExplicit data mapping and scanning library for Go structs and slices.\n\n### [github.com/stephenafamo/scan](https://github.com/stephenafamo/scan)\n\nType safe and flexible package for scanning database data into Go types.\nSupports, structs, maps, slices and custom mapping functions.\n\n### [github.com/z0ne-dev/mgx](https://github.com/z0ne-dev/mgx)\n\nCode first migration library for native pgx (no database/sql abstraction).\n\n### [github.com/amirsalarsafaei/sqlc-pgx-monitoring](https://github.com/amirsalarsafaei/sqlc-pgx-monitoring)\n\nA database monitoring/metrics library for pgx and sqlc. Trace, log and monitor your sqlc query performance using OpenTelemetry.\n"
        },
        {
          "name": "Rakefile",
          "type": "blob",
          "size": 0.4521484375,
          "content": "require \"erb\"\n\nrule '.go' => '.go.erb' do |task|\n  erb = ERB.new(File.read(task.source))\n  File.write(task.name, \"// Code generated from #{task.source}. DO NOT EDIT.\\n\\n\" + erb.result(binding))\n  sh \"goimports\", \"-w\", task.name\nend\n\ngenerated_code_files = [\n  \"pgtype/int.go\",\n  \"pgtype/int_test.go\",\n  \"pgtype/integration_benchmark_test.go\",\n  \"pgtype/zeronull/int.go\",\n  \"pgtype/zeronull/int_test.go\"\n]\n\ndesc \"Generate code\"\ntask generate: generated_code_files\n"
        },
        {
          "name": "batch.go",
          "type": "blob",
          "size": 11.328125,
          "content": "package pgx\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/jackc/pgx/v5/pgconn\"\n)\n\n// QueuedQuery is a query that has been queued for execution via a Batch.\ntype QueuedQuery struct {\n\tSQL       string\n\tArguments []any\n\tFn        batchItemFunc\n\tsd        *pgconn.StatementDescription\n}\n\ntype batchItemFunc func(br BatchResults) error\n\n// Query sets fn to be called when the response to qq is received.\nfunc (qq *QueuedQuery) Query(fn func(rows Rows) error) {\n\tqq.Fn = func(br BatchResults) error {\n\t\trows, _ := br.Query()\n\t\tdefer rows.Close()\n\n\t\terr := fn(rows)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\trows.Close()\n\n\t\treturn rows.Err()\n\t}\n}\n\n// Query sets fn to be called when the response to qq is received.\nfunc (qq *QueuedQuery) QueryRow(fn func(row Row) error) {\n\tqq.Fn = func(br BatchResults) error {\n\t\trow := br.QueryRow()\n\t\treturn fn(row)\n\t}\n}\n\n// Exec sets fn to be called when the response to qq is received.\nfunc (qq *QueuedQuery) Exec(fn func(ct pgconn.CommandTag) error) {\n\tqq.Fn = func(br BatchResults) error {\n\t\tct, err := br.Exec()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\treturn fn(ct)\n\t}\n}\n\n// Batch queries are a way of bundling multiple queries together to avoid\n// unnecessary network round trips. A Batch must only be sent once.\ntype Batch struct {\n\tQueuedQueries []*QueuedQuery\n}\n\n// Queue queues a query to batch b. query can be an SQL query or the name of a prepared statement. The only pgx option\n// argument that is supported is QueryRewriter. Queries are executed using the connection's DefaultQueryExecMode.\n//\n// While query can contain multiple statements if the connection's DefaultQueryExecMode is QueryModeSimple, this should\n// be avoided. QueuedQuery.Fn must not be set as it will only be called for the first query. That is, QueuedQuery.Query,\n// QueuedQuery.QueryRow, and QueuedQuery.Exec must not be called. In addition, any error messages or tracing that\n// include the current query may reference the wrong query.\nfunc (b *Batch) Queue(query string, arguments ...any) *QueuedQuery {\n\tqq := &QueuedQuery{\n\t\tSQL:       query,\n\t\tArguments: arguments,\n\t}\n\tb.QueuedQueries = append(b.QueuedQueries, qq)\n\treturn qq\n}\n\n// Len returns number of queries that have been queued so far.\nfunc (b *Batch) Len() int {\n\treturn len(b.QueuedQueries)\n}\n\ntype BatchResults interface {\n\t// Exec reads the results from the next query in the batch as if the query has been sent with Conn.Exec. Prefer\n\t// calling Exec on the QueuedQuery.\n\tExec() (pgconn.CommandTag, error)\n\n\t// Query reads the results from the next query in the batch as if the query has been sent with Conn.Query. Prefer\n\t// calling Query on the QueuedQuery.\n\tQuery() (Rows, error)\n\n\t// QueryRow reads the results from the next query in the batch as if the query has been sent with Conn.QueryRow.\n\t// Prefer calling QueryRow on the QueuedQuery.\n\tQueryRow() Row\n\n\t// Close closes the batch operation. All unread results are read and any callback functions registered with\n\t// QueuedQuery.Query, QueuedQuery.QueryRow, or QueuedQuery.Exec will be called. If a callback function returns an\n\t// error or the batch encounters an error subsequent callback functions will not be called.\n\t//\n\t// Close must be called before the underlying connection can be used again. Any error that occurred during a batch\n\t// operation may have made it impossible to resyncronize the connection with the server. In this case the underlying\n\t// connection will have been closed.\n\t//\n\t// Close is safe to call multiple times. If it returns an error subsequent calls will return the same error. Callback\n\t// functions will not be rerun.\n\tClose() error\n}\n\ntype batchResults struct {\n\tctx       context.Context\n\tconn      *Conn\n\tmrr       *pgconn.MultiResultReader\n\terr       error\n\tb         *Batch\n\tqqIdx     int\n\tclosed    bool\n\tendTraced bool\n}\n\n// Exec reads the results from the next query in the batch as if the query has been sent with Exec.\nfunc (br *batchResults) Exec() (pgconn.CommandTag, error) {\n\tif br.err != nil {\n\t\treturn pgconn.CommandTag{}, br.err\n\t}\n\tif br.closed {\n\t\treturn pgconn.CommandTag{}, fmt.Errorf(\"batch already closed\")\n\t}\n\n\tquery, arguments, _ := br.nextQueryAndArgs()\n\n\tif !br.mrr.NextResult() {\n\t\terr := br.mrr.Close()\n\t\tif err == nil {\n\t\t\terr = errors.New(\"no more results in batch\")\n\t\t}\n\t\tif br.conn.batchTracer != nil {\n\t\t\tbr.conn.batchTracer.TraceBatchQuery(br.ctx, br.conn, TraceBatchQueryData{\n\t\t\t\tSQL:  query,\n\t\t\t\tArgs: arguments,\n\t\t\t\tErr:  err,\n\t\t\t})\n\t\t}\n\t\treturn pgconn.CommandTag{}, err\n\t}\n\n\tcommandTag, err := br.mrr.ResultReader().Close()\n\tif err != nil {\n\t\tbr.err = err\n\t\tbr.mrr.Close()\n\t}\n\n\tif br.conn.batchTracer != nil {\n\t\tbr.conn.batchTracer.TraceBatchQuery(br.ctx, br.conn, TraceBatchQueryData{\n\t\t\tSQL:        query,\n\t\t\tArgs:       arguments,\n\t\t\tCommandTag: commandTag,\n\t\t\tErr:        br.err,\n\t\t})\n\t}\n\n\treturn commandTag, br.err\n}\n\n// Query reads the results from the next query in the batch as if the query has been sent with Query.\nfunc (br *batchResults) Query() (Rows, error) {\n\tquery, arguments, ok := br.nextQueryAndArgs()\n\tif !ok {\n\t\tquery = \"batch query\"\n\t}\n\n\tif br.err != nil {\n\t\treturn &baseRows{err: br.err, closed: true}, br.err\n\t}\n\n\tif br.closed {\n\t\talreadyClosedErr := fmt.Errorf(\"batch already closed\")\n\t\treturn &baseRows{err: alreadyClosedErr, closed: true}, alreadyClosedErr\n\t}\n\n\trows := br.conn.getRows(br.ctx, query, arguments)\n\trows.batchTracer = br.conn.batchTracer\n\n\tif !br.mrr.NextResult() {\n\t\trows.err = br.mrr.Close()\n\t\tif rows.err == nil {\n\t\t\trows.err = errors.New(\"no more results in batch\")\n\t\t}\n\t\trows.closed = true\n\n\t\tif br.conn.batchTracer != nil {\n\t\t\tbr.conn.batchTracer.TraceBatchQuery(br.ctx, br.conn, TraceBatchQueryData{\n\t\t\t\tSQL:  query,\n\t\t\t\tArgs: arguments,\n\t\t\t\tErr:  rows.err,\n\t\t\t})\n\t\t}\n\n\t\treturn rows, rows.err\n\t}\n\n\trows.resultReader = br.mrr.ResultReader()\n\treturn rows, nil\n}\n\n// QueryRow reads the results from the next query in the batch as if the query has been sent with QueryRow.\nfunc (br *batchResults) QueryRow() Row {\n\trows, _ := br.Query()\n\treturn (*connRow)(rows.(*baseRows))\n\n}\n\n// Close closes the batch operation. Any error that occurred during a batch operation may have made it impossible to\n// resyncronize the connection with the server. In this case the underlying connection will have been closed.\nfunc (br *batchResults) Close() error {\n\tdefer func() {\n\t\tif !br.endTraced {\n\t\t\tif br.conn != nil && br.conn.batchTracer != nil {\n\t\t\t\tbr.conn.batchTracer.TraceBatchEnd(br.ctx, br.conn, TraceBatchEndData{Err: br.err})\n\t\t\t}\n\t\t\tbr.endTraced = true\n\t\t}\n\t}()\n\n\tif br.err != nil {\n\t\treturn br.err\n\t}\n\n\tif br.closed {\n\t\treturn nil\n\t}\n\n\t// Read and run fn for all remaining items\n\tfor br.err == nil && !br.closed && br.b != nil && br.qqIdx < len(br.b.QueuedQueries) {\n\t\tif br.b.QueuedQueries[br.qqIdx].Fn != nil {\n\t\t\terr := br.b.QueuedQueries[br.qqIdx].Fn(br)\n\t\t\tif err != nil {\n\t\t\t\tbr.err = err\n\t\t\t}\n\t\t} else {\n\t\t\tbr.Exec()\n\t\t}\n\t}\n\n\tbr.closed = true\n\n\terr := br.mrr.Close()\n\tif br.err == nil {\n\t\tbr.err = err\n\t}\n\n\treturn br.err\n}\n\nfunc (br *batchResults) earlyError() error {\n\treturn br.err\n}\n\nfunc (br *batchResults) nextQueryAndArgs() (query string, args []any, ok bool) {\n\tif br.b != nil && br.qqIdx < len(br.b.QueuedQueries) {\n\t\tbi := br.b.QueuedQueries[br.qqIdx]\n\t\tquery = bi.SQL\n\t\targs = bi.Arguments\n\t\tok = true\n\t\tbr.qqIdx++\n\t}\n\treturn\n}\n\ntype pipelineBatchResults struct {\n\tctx       context.Context\n\tconn      *Conn\n\tpipeline  *pgconn.Pipeline\n\tlastRows  *baseRows\n\terr       error\n\tb         *Batch\n\tqqIdx     int\n\tclosed    bool\n\tendTraced bool\n}\n\n// Exec reads the results from the next query in the batch as if the query has been sent with Exec.\nfunc (br *pipelineBatchResults) Exec() (pgconn.CommandTag, error) {\n\tif br.err != nil {\n\t\treturn pgconn.CommandTag{}, br.err\n\t}\n\tif br.closed {\n\t\treturn pgconn.CommandTag{}, fmt.Errorf(\"batch already closed\")\n\t}\n\tif br.lastRows != nil && br.lastRows.err != nil {\n\t\treturn pgconn.CommandTag{}, br.err\n\t}\n\n\tquery, arguments, err := br.nextQueryAndArgs()\n\tif err != nil {\n\t\treturn pgconn.CommandTag{}, err\n\t}\n\n\tresults, err := br.pipeline.GetResults()\n\tif err != nil {\n\t\tbr.err = err\n\t\treturn pgconn.CommandTag{}, br.err\n\t}\n\tvar commandTag pgconn.CommandTag\n\tswitch results := results.(type) {\n\tcase *pgconn.ResultReader:\n\t\tcommandTag, br.err = results.Close()\n\tdefault:\n\t\treturn pgconn.CommandTag{}, fmt.Errorf(\"unexpected pipeline result: %T\", results)\n\t}\n\n\tif br.conn.batchTracer != nil {\n\t\tbr.conn.batchTracer.TraceBatchQuery(br.ctx, br.conn, TraceBatchQueryData{\n\t\t\tSQL:        query,\n\t\t\tArgs:       arguments,\n\t\t\tCommandTag: commandTag,\n\t\t\tErr:        br.err,\n\t\t})\n\t}\n\n\treturn commandTag, br.err\n}\n\n// Query reads the results from the next query in the batch as if the query has been sent with Query.\nfunc (br *pipelineBatchResults) Query() (Rows, error) {\n\tif br.err != nil {\n\t\treturn &baseRows{err: br.err, closed: true}, br.err\n\t}\n\n\tif br.closed {\n\t\talreadyClosedErr := fmt.Errorf(\"batch already closed\")\n\t\treturn &baseRows{err: alreadyClosedErr, closed: true}, alreadyClosedErr\n\t}\n\n\tif br.lastRows != nil && br.lastRows.err != nil {\n\t\tbr.err = br.lastRows.err\n\t\treturn &baseRows{err: br.err, closed: true}, br.err\n\t}\n\n\tquery, arguments, err := br.nextQueryAndArgs()\n\tif err != nil {\n\t\treturn &baseRows{err: err, closed: true}, err\n\t}\n\n\trows := br.conn.getRows(br.ctx, query, arguments)\n\trows.batchTracer = br.conn.batchTracer\n\tbr.lastRows = rows\n\n\tresults, err := br.pipeline.GetResults()\n\tif err != nil {\n\t\tbr.err = err\n\t\trows.err = err\n\t\trows.closed = true\n\n\t\tif br.conn.batchTracer != nil {\n\t\t\tbr.conn.batchTracer.TraceBatchQuery(br.ctx, br.conn, TraceBatchQueryData{\n\t\t\t\tSQL:  query,\n\t\t\t\tArgs: arguments,\n\t\t\t\tErr:  err,\n\t\t\t})\n\t\t}\n\t} else {\n\t\tswitch results := results.(type) {\n\t\tcase *pgconn.ResultReader:\n\t\t\trows.resultReader = results\n\t\tdefault:\n\t\t\terr = fmt.Errorf(\"unexpected pipeline result: %T\", results)\n\t\t\tbr.err = err\n\t\t\trows.err = err\n\t\t\trows.closed = true\n\t\t}\n\t}\n\n\treturn rows, rows.err\n}\n\n// QueryRow reads the results from the next query in the batch as if the query has been sent with QueryRow.\nfunc (br *pipelineBatchResults) QueryRow() Row {\n\trows, _ := br.Query()\n\treturn (*connRow)(rows.(*baseRows))\n\n}\n\n// Close closes the batch operation. Any error that occurred during a batch operation may have made it impossible to\n// resyncronize the connection with the server. In this case the underlying connection will have been closed.\nfunc (br *pipelineBatchResults) Close() error {\n\tdefer func() {\n\t\tif !br.endTraced {\n\t\t\tif br.conn.batchTracer != nil {\n\t\t\t\tbr.conn.batchTracer.TraceBatchEnd(br.ctx, br.conn, TraceBatchEndData{Err: br.err})\n\t\t\t}\n\t\t\tbr.endTraced = true\n\t\t}\n\t}()\n\n\tif br.err == nil && br.lastRows != nil && br.lastRows.err != nil {\n\t\tbr.err = br.lastRows.err\n\t\treturn br.err\n\t}\n\n\tif br.closed {\n\t\treturn br.err\n\t}\n\n\t// Read and run fn for all remaining items\n\tfor br.err == nil && !br.closed && br.b != nil && br.qqIdx < len(br.b.QueuedQueries) {\n\t\tif br.b.QueuedQueries[br.qqIdx].Fn != nil {\n\t\t\terr := br.b.QueuedQueries[br.qqIdx].Fn(br)\n\t\t\tif err != nil {\n\t\t\t\tbr.err = err\n\t\t\t}\n\t\t} else {\n\t\t\tbr.Exec()\n\t\t}\n\t}\n\n\tbr.closed = true\n\n\terr := br.pipeline.Close()\n\tif br.err == nil {\n\t\tbr.err = err\n\t}\n\n\treturn br.err\n}\n\nfunc (br *pipelineBatchResults) earlyError() error {\n\treturn br.err\n}\n\nfunc (br *pipelineBatchResults) nextQueryAndArgs() (query string, args []any, err error) {\n\tif br.b == nil {\n\t\treturn \"\", nil, errors.New(\"no reference to batch\")\n\t}\n\n\tif br.qqIdx >= len(br.b.QueuedQueries) {\n\t\treturn \"\", nil, errors.New(\"no more results in batch\")\n\t}\n\n\tbi := br.b.QueuedQueries[br.qqIdx]\n\tbr.qqIdx++\n\treturn bi.SQL, bi.Arguments, nil\n}\n"
        },
        {
          "name": "batch_test.go",
          "type": "blob",
          "size": 26.7392578125,
          "content": "package pgx_test\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgxtest\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestConnSendBatch(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tpgxtest.SkipCockroachDB(t, conn, \"Server serial type is incompatible with test\")\n\n\t\tsql := `create temporary table ledger(\n\t  id serial primary key,\n\t  description varchar not null,\n\t  amount int not null\n\t);`\n\t\tmustExec(t, conn, sql)\n\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(\"insert into ledger(description, amount) values($1, $2)\", \"q1\", 1)\n\t\tbatch.Queue(\"insert into ledger(description, amount) values($1, $2)\", \"q2\", 2)\n\t\tbatch.Queue(\"insert into ledger(description, amount) values($1, $2)\", \"q3\", 3)\n\t\tbatch.Queue(\"select id, description, amount from ledger order by id\")\n\t\tbatch.Queue(\"select id, description, amount from ledger order by id\")\n\t\tbatch.Queue(\"select * from ledger where false\")\n\t\tbatch.Queue(\"select sum(amount) from ledger\")\n\n\t\tbr := conn.SendBatch(ctx, batch)\n\n\t\tct, err := br.Exec()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tif ct.RowsAffected() != 1 {\n\t\t\tt.Errorf(\"ct.RowsAffected() => %v, want %v\", ct.RowsAffected(), 1)\n\t\t}\n\n\t\tct, err = br.Exec()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tif ct.RowsAffected() != 1 {\n\t\t\tt.Errorf(\"ct.RowsAffected() => %v, want %v\", ct.RowsAffected(), 1)\n\t\t}\n\n\t\tct, err = br.Exec()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tif ct.RowsAffected() != 1 {\n\t\t\tt.Errorf(\"ct.RowsAffected() => %v, want %v\", ct.RowsAffected(), 1)\n\t\t}\n\n\t\tselectFromLedgerExpectedRows := []struct {\n\t\t\tid          int32\n\t\t\tdescription string\n\t\t\tamount      int32\n\t\t}{\n\t\t\t{1, \"q1\", 1},\n\t\t\t{2, \"q2\", 2},\n\t\t\t{3, \"q3\", 3},\n\t\t}\n\n\t\trows, err := br.Query()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\n\t\tvar id int32\n\t\tvar description string\n\t\tvar amount int32\n\t\trowCount := 0\n\n\t\tfor rows.Next() {\n\t\t\tif rowCount >= len(selectFromLedgerExpectedRows) {\n\t\t\t\tt.Fatalf(\"got too many rows: %d\", rowCount)\n\t\t\t}\n\n\t\t\tif err := rows.Scan(&id, &description, &amount); err != nil {\n\t\t\t\tt.Fatalf(\"row %d: %v\", rowCount, err)\n\t\t\t}\n\n\t\t\tif id != selectFromLedgerExpectedRows[rowCount].id {\n\t\t\t\tt.Errorf(\"id => %v, want %v\", id, selectFromLedgerExpectedRows[rowCount].id)\n\t\t\t}\n\t\t\tif description != selectFromLedgerExpectedRows[rowCount].description {\n\t\t\t\tt.Errorf(\"description => %v, want %v\", description, selectFromLedgerExpectedRows[rowCount].description)\n\t\t\t}\n\t\t\tif amount != selectFromLedgerExpectedRows[rowCount].amount {\n\t\t\t\tt.Errorf(\"amount => %v, want %v\", amount, selectFromLedgerExpectedRows[rowCount].amount)\n\t\t\t}\n\n\t\t\trowCount++\n\t\t}\n\n\t\tif rows.Err() != nil {\n\t\t\tt.Fatal(rows.Err())\n\t\t}\n\n\t\trowCount = 0\n\t\trows, _ = br.Query()\n\t\t_, err = pgx.ForEachRow(rows, []any{&id, &description, &amount}, func() error {\n\t\t\tif id != selectFromLedgerExpectedRows[rowCount].id {\n\t\t\t\tt.Errorf(\"id => %v, want %v\", id, selectFromLedgerExpectedRows[rowCount].id)\n\t\t\t}\n\t\t\tif description != selectFromLedgerExpectedRows[rowCount].description {\n\t\t\t\tt.Errorf(\"description => %v, want %v\", description, selectFromLedgerExpectedRows[rowCount].description)\n\t\t\t}\n\t\t\tif amount != selectFromLedgerExpectedRows[rowCount].amount {\n\t\t\t\tt.Errorf(\"amount => %v, want %v\", amount, selectFromLedgerExpectedRows[rowCount].amount)\n\t\t\t}\n\n\t\t\trowCount++\n\n\t\t\treturn nil\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\n\t\terr = br.QueryRow().Scan(&id, &description, &amount)\n\t\tif !errors.Is(err, pgx.ErrNoRows) {\n\t\t\tt.Errorf(\"expected pgx.ErrNoRows but got: %v\", err)\n\t\t}\n\n\t\terr = br.QueryRow().Scan(&amount)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tif amount != 6 {\n\t\t\tt.Errorf(\"amount => %v, want %v\", amount, 6)\n\t\t}\n\n\t\terr = br.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n}\n\nfunc TestConnSendBatchQueuedQuery(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tpgxtest.SkipCockroachDB(t, conn, \"Server serial type is incompatible with test\")\n\n\t\tsql := `create temporary table ledger(\n\t  id serial primary key,\n\t  description varchar not null,\n\t  amount int not null\n\t);`\n\t\tmustExec(t, conn, sql)\n\n\t\tbatch := &pgx.Batch{}\n\n\t\tbatch.Queue(\"insert into ledger(description, amount) values($1, $2)\", \"q1\", 1).Exec(func(ct pgconn.CommandTag) error {\n\t\t\tassert.EqualValues(t, 1, ct.RowsAffected())\n\t\t\treturn nil\n\t\t})\n\n\t\tbatch.Queue(\"insert into ledger(description, amount) values($1, $2)\", \"q2\", 2).Exec(func(ct pgconn.CommandTag) error {\n\t\t\tassert.EqualValues(t, 1, ct.RowsAffected())\n\t\t\treturn nil\n\t\t})\n\n\t\tbatch.Queue(\"insert into ledger(description, amount) values($1, $2)\", \"q3\", 3).Exec(func(ct pgconn.CommandTag) error {\n\t\t\tassert.EqualValues(t, 1, ct.RowsAffected())\n\t\t\treturn nil\n\t\t})\n\n\t\tselectFromLedgerExpectedRows := []struct {\n\t\t\tid          int32\n\t\t\tdescription string\n\t\t\tamount      int32\n\t\t}{\n\t\t\t{1, \"q1\", 1},\n\t\t\t{2, \"q2\", 2},\n\t\t\t{3, \"q3\", 3},\n\t\t}\n\n\t\tbatch.Queue(\"select id, description, amount from ledger order by id\").Query(func(rows pgx.Rows) error {\n\t\t\trowCount := 0\n\t\t\tvar id int32\n\t\t\tvar description string\n\t\t\tvar amount int32\n\t\t\t_, err := pgx.ForEachRow(rows, []any{&id, &description, &amount}, func() error {\n\t\t\t\tassert.Equal(t, selectFromLedgerExpectedRows[rowCount].id, id)\n\t\t\t\tassert.Equal(t, selectFromLedgerExpectedRows[rowCount].description, description)\n\t\t\t\tassert.Equal(t, selectFromLedgerExpectedRows[rowCount].amount, amount)\n\t\t\t\trowCount++\n\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\tassert.NoError(t, err)\n\t\t\treturn nil\n\t\t})\n\n\t\tbatch.Queue(\"select id, description, amount from ledger order by id\").Query(func(rows pgx.Rows) error {\n\t\t\trowCount := 0\n\t\t\tvar id int32\n\t\t\tvar description string\n\t\t\tvar amount int32\n\t\t\t_, err := pgx.ForEachRow(rows, []any{&id, &description, &amount}, func() error {\n\t\t\t\tassert.Equal(t, selectFromLedgerExpectedRows[rowCount].id, id)\n\t\t\t\tassert.Equal(t, selectFromLedgerExpectedRows[rowCount].description, description)\n\t\t\t\tassert.Equal(t, selectFromLedgerExpectedRows[rowCount].amount, amount)\n\t\t\t\trowCount++\n\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\tassert.NoError(t, err)\n\t\t\treturn nil\n\t\t})\n\n\t\tbatch.Queue(\"select * from ledger where false\").QueryRow(func(row pgx.Row) error {\n\t\t\terr := row.Scan(nil, nil, nil)\n\t\t\tassert.ErrorIs(t, err, pgx.ErrNoRows)\n\t\t\treturn nil\n\t\t})\n\n\t\tbatch.Queue(\"select sum(amount) from ledger\").QueryRow(func(row pgx.Row) error {\n\t\t\tvar sumAmount int32\n\t\t\terr := row.Scan(&sumAmount)\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.EqualValues(t, 6, sumAmount)\n\t\t\treturn nil\n\t\t})\n\n\t\terr := conn.SendBatch(ctx, batch).Close()\n\t\tassert.NoError(t, err)\n\t})\n}\n\nfunc TestConnSendBatchMany(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tsql := `create temporary table ledger(\n\t  id serial primary key,\n\t  description varchar not null,\n\t  amount int not null\n\t);`\n\t\tmustExec(t, conn, sql)\n\n\t\tbatch := &pgx.Batch{}\n\n\t\tnumInserts := 1000\n\n\t\tfor i := 0; i < numInserts; i++ {\n\t\t\tbatch.Queue(\"insert into ledger(description, amount) values($1, $2)\", \"q1\", 1)\n\t\t}\n\t\tbatch.Queue(\"select count(*) from ledger\")\n\n\t\tbr := conn.SendBatch(ctx, batch)\n\n\t\tfor i := 0; i < numInserts; i++ {\n\t\t\tct, err := br.Exec()\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.EqualValues(t, 1, ct.RowsAffected())\n\t\t}\n\n\t\tvar actualInserts int\n\t\terr := br.QueryRow().Scan(&actualInserts)\n\t\tassert.NoError(t, err)\n\t\tassert.EqualValues(t, numInserts, actualInserts)\n\n\t\terr = br.Close()\n\t\trequire.NoError(t, err)\n\t})\n}\n\n// https://github.com/jackc/pgx/issues/1801#issuecomment-2203784178\nfunc TestConnSendBatchReadResultsWhenNothingQueued(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tbatch := &pgx.Batch{}\n\t\tbr := conn.SendBatch(ctx, batch)\n\t\tcommandTag, err := br.Exec()\n\t\trequire.Equal(t, \"\", commandTag.String())\n\t\trequire.EqualError(t, err, \"no more results in batch\")\n\t\terr = br.Close()\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestConnSendBatchReadMoreResultsThanQueriesSent(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(\"select 1\")\n\t\tbr := conn.SendBatch(ctx, batch)\n\t\tcommandTag, err := br.Exec()\n\t\trequire.Equal(t, \"SELECT 1\", commandTag.String())\n\t\trequire.NoError(t, err)\n\t\tcommandTag, err = br.Exec()\n\t\trequire.Equal(t, \"\", commandTag.String())\n\t\trequire.EqualError(t, err, \"no more results in batch\")\n\t\terr = br.Close()\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestConnSendBatchWithPreparedStatement(t *testing.T) {\n\tt.Parallel()\n\n\tmodes := []pgx.QueryExecMode{\n\t\tpgx.QueryExecModeCacheStatement,\n\t\tpgx.QueryExecModeCacheDescribe,\n\t\tpgx.QueryExecModeDescribeExec,\n\t\tpgx.QueryExecModeExec,\n\t\t// Don't test simple mode with prepared statements.\n\t}\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, modes, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tpgxtest.SkipCockroachDB(t, conn, \"Server issues incorrect ParameterDescription (https://github.com/cockroachdb/cockroach/issues/60907)\")\n\t\t_, err := conn.Prepare(ctx, \"ps1\", \"select n from generate_series(0,$1::int) n\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tbatch := &pgx.Batch{}\n\n\t\tqueryCount := 3\n\t\tfor i := 0; i < queryCount; i++ {\n\t\t\tbatch.Queue(\"ps1\", 5)\n\t\t}\n\n\t\tbr := conn.SendBatch(ctx, batch)\n\n\t\tfor i := 0; i < queryCount; i++ {\n\t\t\trows, err := br.Query()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tfor k := 0; rows.Next(); k++ {\n\t\t\t\tvar n int\n\t\t\t\tif err := rows.Scan(&n); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t\tif n != k {\n\t\t\t\t\tt.Fatalf(\"n => %v, want %v\", n, k)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif rows.Err() != nil {\n\t\t\t\tt.Fatal(rows.Err())\n\t\t\t}\n\t\t}\n\n\t\terr = br.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n}\n\nfunc TestConnSendBatchWithQueryRewriter(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(\"something to be replaced\", &testQueryRewriter{sql: \"select $1::int\", args: []any{1}})\n\t\tbatch.Queue(\"something else to be replaced\", &testQueryRewriter{sql: \"select $1::text\", args: []any{\"hello\"}})\n\t\tbatch.Queue(\"more to be replaced\", &testQueryRewriter{sql: \"select $1::int\", args: []any{3}})\n\n\t\tbr := conn.SendBatch(ctx, batch)\n\n\t\tvar n int32\n\t\terr := br.QueryRow().Scan(&n)\n\t\trequire.NoError(t, err)\n\t\trequire.EqualValues(t, 1, n)\n\n\t\tvar s string\n\t\terr = br.QueryRow().Scan(&s)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, \"hello\", s)\n\n\t\terr = br.QueryRow().Scan(&n)\n\t\trequire.NoError(t, err)\n\t\trequire.EqualValues(t, 3, n)\n\n\t\terr = br.Close()\n\t\trequire.NoError(t, err)\n\t})\n}\n\n// https://github.com/jackc/pgx/issues/856\nfunc TestConnSendBatchWithPreparedStatementAndStatementCacheDisabled(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconfig, err := pgx.ParseConfig(os.Getenv(\"PGX_TEST_DATABASE\"))\n\trequire.NoError(t, err)\n\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeDescribeExec\n\tconfig.StatementCacheCapacity = 0\n\tconfig.DescriptionCacheCapacity = 0\n\n\tconn := mustConnect(t, config)\n\tdefer closeConn(t, conn)\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server issues incorrect ParameterDescription (https://github.com/cockroachdb/cockroach/issues/60907)\")\n\n\t_, err = conn.Prepare(ctx, \"ps1\", \"select n from generate_series(0,$1::int) n\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tbatch := &pgx.Batch{}\n\n\tqueryCount := 3\n\tfor i := 0; i < queryCount; i++ {\n\t\tbatch.Queue(\"ps1\", 5)\n\t}\n\n\tbr := conn.SendBatch(ctx, batch)\n\n\tfor i := 0; i < queryCount; i++ {\n\t\trows, err := br.Query()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tfor k := 0; rows.Next(); k++ {\n\t\t\tvar n int\n\t\t\tif err := rows.Scan(&n); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif n != k {\n\t\t\t\tt.Fatalf(\"n => %v, want %v\", n, k)\n\t\t\t}\n\t\t}\n\n\t\tif rows.Err() != nil {\n\t\t\tt.Fatal(rows.Err())\n\t\t}\n\t}\n\n\terr = br.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnSendBatchCloseRowsPartiallyRead(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(\"select n from generate_series(0,5) n\")\n\t\tbatch.Queue(\"select n from generate_series(0,5) n\")\n\n\t\tbr := conn.SendBatch(ctx, batch)\n\n\t\trows, err := br.Query()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\n\t\tfor i := 0; i < 3; i++ {\n\t\t\tif !rows.Next() {\n\t\t\t\tt.Error(\"expected a row to be available\")\n\t\t\t}\n\n\t\t\tvar n int\n\t\t\tif err := rows.Scan(&n); err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t}\n\t\t\tif n != i {\n\t\t\t\tt.Errorf(\"n => %v, want %v\", n, i)\n\t\t\t}\n\t\t}\n\n\t\trows.Close()\n\n\t\trows, err = br.Query()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\n\t\tfor i := 0; rows.Next(); i++ {\n\t\t\tvar n int\n\t\t\tif err := rows.Scan(&n); err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t}\n\t\t\tif n != i {\n\t\t\t\tt.Errorf(\"n => %v, want %v\", n, i)\n\t\t\t}\n\t\t}\n\n\t\tif rows.Err() != nil {\n\t\t\tt.Error(rows.Err())\n\t\t}\n\n\t\terr = br.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t})\n}\n\nfunc TestConnSendBatchQueryError(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(\"select n from generate_series(0,5) n where 100/(5-n) > 0\")\n\t\tbatch.Queue(\"select n from generate_series(0,5) n\")\n\n\t\tbr := conn.SendBatch(ctx, batch)\n\n\t\trows, err := br.Query()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\n\t\tfor i := 0; rows.Next(); i++ {\n\t\t\tvar n int\n\t\t\tif err := rows.Scan(&n); err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t}\n\t\t\tif n != i {\n\t\t\t\tt.Errorf(\"n => %v, want %v\", n, i)\n\t\t\t}\n\t\t}\n\n\t\tif pgErr, ok := rows.Err().(*pgconn.PgError); !(ok && pgErr.Code == \"22012\") {\n\t\t\tt.Errorf(\"rows.Err() => %v, want error code %v\", rows.Err(), 22012)\n\t\t}\n\n\t\terr = br.Close()\n\t\tif pgErr, ok := err.(*pgconn.PgError); !(ok && pgErr.Code == \"22012\") {\n\t\t\tt.Errorf(\"br.Close() => %v, want error code %v\", err, 22012)\n\t\t}\n\n\t})\n}\n\nfunc TestConnSendBatchQuerySyntaxError(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(\"select 1 1\")\n\n\t\tbr := conn.SendBatch(ctx, batch)\n\n\t\tvar n int32\n\t\terr := br.QueryRow().Scan(&n)\n\t\tif pgErr, ok := err.(*pgconn.PgError); !(ok && pgErr.Code == \"42601\") {\n\t\t\tt.Errorf(\"rows.Err() => %v, want error code %v\", err, 42601)\n\t\t}\n\n\t\terr = br.Close()\n\t\tif err == nil {\n\t\t\tt.Error(\"Expected error\")\n\t\t}\n\n\t})\n}\n\nfunc TestConnSendBatchQueryRowInsert(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\n\t\tsql := `create temporary table ledger(\n\t  id serial primary key,\n\t  description varchar not null,\n\t  amount int not null\n\t);`\n\t\tmustExec(t, conn, sql)\n\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(\"select 1\")\n\t\tbatch.Queue(\"insert into ledger(description, amount) values($1, $2),($1, $2)\", \"q1\", 1)\n\n\t\tbr := conn.SendBatch(ctx, batch)\n\n\t\tvar value int\n\t\terr := br.QueryRow().Scan(&value)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\n\t\tct, err := br.Exec()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tif ct.RowsAffected() != 2 {\n\t\t\tt.Errorf(\"ct.RowsAffected() => %v, want %v\", ct.RowsAffected(), 2)\n\t\t}\n\n\t\tbr.Close()\n\n\t})\n}\n\nfunc TestConnSendBatchQueryPartialReadInsert(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\n\t\tsql := `create temporary table ledger(\n\t  id serial primary key,\n\t  description varchar not null,\n\t  amount int not null\n\t);`\n\t\tmustExec(t, conn, sql)\n\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(\"select 1 union all select 2 union all select 3\")\n\t\tbatch.Queue(\"insert into ledger(description, amount) values($1, $2),($1, $2)\", \"q1\", 1)\n\n\t\tbr := conn.SendBatch(ctx, batch)\n\n\t\trows, err := br.Query()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\trows.Close()\n\n\t\tct, err := br.Exec()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tif ct.RowsAffected() != 2 {\n\t\t\tt.Errorf(\"ct.RowsAffected() => %v, want %v\", ct.RowsAffected(), 2)\n\t\t}\n\n\t\tbr.Close()\n\n\t})\n}\n\nfunc TestTxSendBatch(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\n\t\tsql := `create temporary table ledger1(\n\t  id serial primary key,\n\t  description varchar not null\n\t);`\n\t\tmustExec(t, conn, sql)\n\n\t\tsql = `create temporary table ledger2(\n\t  id int primary key,\n\t  amount int not null\n\t);`\n\t\tmustExec(t, conn, sql)\n\n\t\ttx, _ := conn.Begin(ctx)\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(\"insert into ledger1(description) values($1) returning id\", \"q1\")\n\n\t\tbr := tx.SendBatch(context.Background(), batch)\n\n\t\tvar id int\n\t\terr := br.QueryRow().Scan(&id)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tbr.Close()\n\n\t\tbatch = &pgx.Batch{}\n\t\tbatch.Queue(\"insert into ledger2(id,amount) values($1, $2)\", id, 2)\n\t\tbatch.Queue(\"select amount from ledger2 where id = $1\", id)\n\n\t\tbr = tx.SendBatch(ctx, batch)\n\n\t\tct, err := br.Exec()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tif ct.RowsAffected() != 1 {\n\t\t\tt.Errorf(\"ct.RowsAffected() => %v, want %v\", ct.RowsAffected(), 1)\n\t\t}\n\n\t\tvar amount int\n\t\terr = br.QueryRow().Scan(&amount)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\n\t\tbr.Close()\n\t\ttx.Commit(ctx)\n\n\t\tvar count int\n\t\tconn.QueryRow(ctx, \"select count(1) from ledger1 where id = $1\", id).Scan(&count)\n\t\tif count != 1 {\n\t\t\tt.Errorf(\"count => %v, want %v\", count, 1)\n\t\t}\n\n\t\terr = br.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t})\n}\n\nfunc TestTxSendBatchRollback(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\n\t\tsql := `create temporary table ledger1(\n\t  id serial primary key,\n\t  description varchar not null\n\t);`\n\t\tmustExec(t, conn, sql)\n\n\t\ttx, _ := conn.Begin(ctx)\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(\"insert into ledger1(description) values($1) returning id\", \"q1\")\n\n\t\tbr := tx.SendBatch(ctx, batch)\n\n\t\tvar id int\n\t\terr := br.QueryRow().Scan(&id)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tbr.Close()\n\t\ttx.Rollback(ctx)\n\n\t\trow := conn.QueryRow(ctx, \"select count(1) from ledger1 where id = $1\", id)\n\t\tvar count int\n\t\trow.Scan(&count)\n\t\tif count != 0 {\n\t\t\tt.Errorf(\"count => %v, want %v\", count, 0)\n\t\t}\n\n\t})\n}\n\n// https://github.com/jackc/pgx/issues/1578\nfunc TestSendBatchErrorWhileReadingResultsWithoutCallback(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(\"select 4 / $1::int\", 0)\n\n\t\tbatchResult := conn.SendBatch(ctx, batch)\n\n\t\t_, execErr := batchResult.Exec()\n\t\trequire.Error(t, execErr)\n\n\t\tcloseErr := batchResult.Close()\n\t\trequire.Equal(t, execErr, closeErr)\n\n\t\t// Try to use the connection.\n\t\t_, err := conn.Exec(ctx, \"select 1\")\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestSendBatchErrorWhileReadingResultsWithExecWhereSomeRowsAreReturned(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(\"select 4 / n from generate_series(-2, 2) n\")\n\n\t\tbatchResult := conn.SendBatch(ctx, batch)\n\n\t\t_, execErr := batchResult.Exec()\n\t\trequire.Error(t, execErr)\n\n\t\tcloseErr := batchResult.Close()\n\t\trequire.Equal(t, execErr, closeErr)\n\n\t\t// Try to use the connection.\n\t\t_, err := conn.Exec(ctx, \"select 1\")\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestConnBeginBatchDeferredError(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\n\t\tpgxtest.SkipCockroachDB(t, conn, \"Server does not support deferred constraint (https://github.com/cockroachdb/cockroach/issues/31632)\")\n\n\t\tmustExec(t, conn, `create temporary table t (\n\t\tid text primary key,\n\t\tn int not null,\n\t\tunique (n) deferrable initially deferred\n\t);\n\n\tinsert into t (id, n) values ('a', 1), ('b', 2), ('c', 3);`)\n\n\t\tbatch := &pgx.Batch{}\n\n\t\tbatch.Queue(`update t set n=n+1 where id='b' returning *`)\n\n\t\tbr := conn.SendBatch(ctx, batch)\n\n\t\trows, err := br.Query()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\n\t\tfor rows.Next() {\n\t\t\tvar id string\n\t\t\tvar n int32\n\t\t\terr = rows.Scan(&id, &n)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\n\t\terr = br.Close()\n\t\tif err == nil {\n\t\t\tt.Fatal(\"expected error 23505 but got none\")\n\t\t}\n\n\t\tif err, ok := err.(*pgconn.PgError); !ok || err.Code != \"23505\" {\n\t\t\tt.Fatalf(\"expected error 23505, got %v\", err)\n\t\t}\n\n\t})\n}\n\nfunc TestConnSendBatchNoStatementCache(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconfig := mustParseConfig(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeDescribeExec\n\tconfig.StatementCacheCapacity = 0\n\tconfig.DescriptionCacheCapacity = 0\n\n\tconn := mustConnect(t, config)\n\tdefer closeConn(t, conn)\n\n\ttestConnSendBatch(t, ctx, conn, 3)\n}\n\nfunc TestConnSendBatchPrepareStatementCache(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconfig := mustParseConfig(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeCacheStatement\n\tconfig.StatementCacheCapacity = 32\n\n\tconn := mustConnect(t, config)\n\tdefer closeConn(t, conn)\n\n\ttestConnSendBatch(t, ctx, conn, 3)\n}\n\nfunc TestConnSendBatchDescribeStatementCache(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconfig := mustParseConfig(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeCacheDescribe\n\tconfig.DescriptionCacheCapacity = 32\n\n\tconn := mustConnect(t, config)\n\tdefer closeConn(t, conn)\n\n\ttestConnSendBatch(t, ctx, conn, 3)\n}\n\nfunc testConnSendBatch(t *testing.T, ctx context.Context, conn *pgx.Conn, queryCount int) {\n\tbatch := &pgx.Batch{}\n\tfor j := 0; j < queryCount; j++ {\n\t\tbatch.Queue(\"select n from generate_series(0,5) n\")\n\t}\n\n\tbr := conn.SendBatch(ctx, batch)\n\n\tfor j := 0; j < queryCount; j++ {\n\t\trows, err := br.Query()\n\t\trequire.NoError(t, err)\n\n\t\tfor k := 0; rows.Next(); k++ {\n\t\t\tvar n int\n\t\t\terr := rows.Scan(&n)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, k, n)\n\t\t}\n\n\t\trequire.NoError(t, rows.Err())\n\t}\n\n\terr := br.Close()\n\trequire.NoError(t, err)\n}\n\nfunc TestSendBatchSimpleProtocol(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconfig := mustParseConfig(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeSimpleProtocol\n\n\tconn := mustConnect(t, config)\n\tdefer closeConn(t, conn)\n\n\tvar batch pgx.Batch\n\tbatch.Queue(\"SELECT 1::int\")\n\tbatch.Queue(\"SELECT 2::int; SELECT $1::int\", 3)\n\tresults := conn.SendBatch(ctx, &batch)\n\trows, err := results.Query()\n\tassert.NoError(t, err)\n\tassert.True(t, rows.Next())\n\tvalues, err := rows.Values()\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 1, values[0])\n\tassert.False(t, rows.Next())\n\n\trows, err = results.Query()\n\tassert.NoError(t, err)\n\tassert.True(t, rows.Next())\n\tvalues, err = rows.Values()\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 2, values[0])\n\tassert.False(t, rows.Next())\n\n\trows, err = results.Query()\n\tassert.NoError(t, err)\n\tassert.True(t, rows.Next())\n\tvalues, err = rows.Values()\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 3, values[0])\n\tassert.False(t, rows.Next())\n}\n\n// https://github.com/jackc/pgx/issues/1847#issuecomment-2347858887\nfunc TestConnSendBatchErrorDoesNotLeaveOrphanedPreparedStatement(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tpgxtest.SkipCockroachDB(t, conn, \"Server serial type is incompatible with test\")\n\n\t\tmustExec(t, conn, `create temporary table foo(col1 text primary key);`)\n\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(\"select col1 from foo\")\n\t\tbatch.Queue(\"select col1 from baz\")\n\t\terr := conn.SendBatch(ctx, batch).Close()\n\t\trequire.EqualError(t, err, `ERROR: relation \"baz\" does not exist (SQLSTATE 42P01)`)\n\n\t\tmustExec(t, conn, `create temporary table baz(col1 text primary key);`)\n\n\t\t// Since table baz now exists, the batch should succeed.\n\n\t\tbatch = &pgx.Batch{}\n\t\tbatch.Queue(\"select col1 from foo\")\n\t\tbatch.Queue(\"select col1 from baz\")\n\t\terr = conn.SendBatch(ctx, batch).Close()\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc ExampleConn_SendBatch() {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn, err := pgx.Connect(ctx, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tif err != nil {\n\t\tfmt.Printf(\"Unable to establish connection: %v\", err)\n\t\treturn\n\t}\n\n\tbatch := &pgx.Batch{}\n\tbatch.Queue(\"select 1 + 1\").QueryRow(func(row pgx.Row) error {\n\t\tvar n int32\n\t\terr := row.Scan(&n)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tfmt.Println(n)\n\n\t\treturn err\n\t})\n\n\tbatch.Queue(\"select 1 + 2\").QueryRow(func(row pgx.Row) error {\n\t\tvar n int32\n\t\terr := row.Scan(&n)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tfmt.Println(n)\n\n\t\treturn err\n\t})\n\n\tbatch.Queue(\"select 2 + 3\").QueryRow(func(row pgx.Row) error {\n\t\tvar n int32\n\t\terr := row.Scan(&n)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tfmt.Println(n)\n\n\t\treturn err\n\t})\n\n\terr = conn.SendBatch(ctx, batch).Close()\n\tif err != nil {\n\t\tfmt.Printf(\"SendBatch error: %v\", err)\n\t\treturn\n\t}\n\n\t// Output:\n\t// 2\n\t// 3\n\t// 5\n}\n"
        },
        {
          "name": "bench_test.go",
          "type": "blob",
          "size": 33.9296875,
          "content": "package pgx_test\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"os\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgtype\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc BenchmarkConnectClose(b *testing.B) {\n\tfor i := 0; i < b.N; i++ {\n\t\tconn, err := pgx.Connect(context.Background(), os.Getenv(\"PGX_TEST_DATABASE\"))\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\n\t\terr = conn.Close(context.Background())\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkMinimalUnpreparedSelectWithoutStatementCache(b *testing.B) {\n\tconfig := mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeDescribeExec\n\tconfig.StatementCacheCapacity = 0\n\tconfig.DescriptionCacheCapacity = 0\n\n\tconn := mustConnect(b, config)\n\tdefer closeConn(b, conn)\n\n\tvar n int64\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\terr := conn.QueryRow(context.Background(), \"select $1::int8\", i).Scan(&n)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\n\t\tif n != int64(i) {\n\t\t\tb.Fatalf(\"expected %d, got %d\", i, n)\n\t\t}\n\t}\n}\n\nfunc BenchmarkMinimalUnpreparedSelectWithStatementCacheModeDescribe(b *testing.B) {\n\tconfig := mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeCacheDescribe\n\tconfig.StatementCacheCapacity = 0\n\tconfig.DescriptionCacheCapacity = 32\n\n\tconn := mustConnect(b, config)\n\tdefer closeConn(b, conn)\n\n\tvar n int64\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\terr := conn.QueryRow(context.Background(), \"select $1::int8\", i).Scan(&n)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\n\t\tif n != int64(i) {\n\t\t\tb.Fatalf(\"expected %d, got %d\", i, n)\n\t\t}\n\t}\n}\n\nfunc BenchmarkMinimalUnpreparedSelectWithStatementCacheModePrepare(b *testing.B) {\n\tconfig := mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeCacheStatement\n\tconfig.StatementCacheCapacity = 32\n\tconfig.DescriptionCacheCapacity = 0\n\n\tconn := mustConnect(b, config)\n\tdefer closeConn(b, conn)\n\n\tvar n int64\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\terr := conn.QueryRow(context.Background(), \"select $1::int8\", i).Scan(&n)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\n\t\tif n != int64(i) {\n\t\t\tb.Fatalf(\"expected %d, got %d\", i, n)\n\t\t}\n\t}\n}\n\nfunc BenchmarkMinimalPreparedSelect(b *testing.B) {\n\tconn := mustConnect(b, mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\")))\n\tdefer closeConn(b, conn)\n\n\t_, err := conn.Prepare(context.Background(), \"ps1\", \"select $1::int8\")\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tvar n int64\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\terr = conn.QueryRow(context.Background(), \"ps1\", i).Scan(&n)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\n\t\tif n != int64(i) {\n\t\t\tb.Fatalf(\"expected %d, got %d\", i, n)\n\t\t}\n\t}\n}\n\nfunc BenchmarkMinimalPgConnPreparedSelect(b *testing.B) {\n\tconn := mustConnect(b, mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\")))\n\tdefer closeConn(b, conn)\n\n\tpgConn := conn.PgConn()\n\n\t_, err := pgConn.Prepare(context.Background(), \"ps1\", \"select $1::int8\", nil)\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tencodedBytes := make([]byte, 8)\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\n\t\trr := pgConn.ExecPrepared(context.Background(), \"ps1\", [][]byte{encodedBytes}, []int16{1}, []int16{1})\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\n\t\tfor rr.NextRow() {\n\t\t\tfor i := range rr.Values() {\n\t\t\t\tif !bytes.Equal(rr.Values()[0], encodedBytes) {\n\t\t\t\t\tb.Fatalf(\"unexpected values: %s %s\", rr.Values()[i], encodedBytes)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t_, err = rr.Close()\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkPointerPointerWithNullValues(b *testing.B) {\n\tconn := mustConnect(b, mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\")))\n\tdefer closeConn(b, conn)\n\n\t_, err := conn.Prepare(context.Background(), \"selectNulls\", \"select 1::int4, 'johnsmith', null::text, null::text, null::text, null::date, null::timestamptz\")\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tvar record struct {\n\t\t\tid            int32\n\t\t\tuserName      string\n\t\t\temail         *string\n\t\t\tname          *string\n\t\t\tsex           *string\n\t\t\tbirthDate     *time.Time\n\t\t\tlastLoginTime *time.Time\n\t\t}\n\n\t\terr = conn.QueryRow(context.Background(), \"selectNulls\").Scan(\n\t\t\t&record.id,\n\t\t\t&record.userName,\n\t\t\t&record.email,\n\t\t\t&record.name,\n\t\t\t&record.sex,\n\t\t\t&record.birthDate,\n\t\t\t&record.lastLoginTime,\n\t\t)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\n\t\t// These checks both ensure that the correct data was returned\n\t\t// and provide a benchmark of accessing the returned values.\n\t\tif record.id != 1 {\n\t\t\tb.Fatalf(\"bad value for id: %v\", record.id)\n\t\t}\n\t\tif record.userName != \"johnsmith\" {\n\t\t\tb.Fatalf(\"bad value for userName: %v\", record.userName)\n\t\t}\n\t\tif record.email != nil {\n\t\t\tb.Fatalf(\"bad value for email: %v\", record.email)\n\t\t}\n\t\tif record.name != nil {\n\t\t\tb.Fatalf(\"bad value for name: %v\", record.name)\n\t\t}\n\t\tif record.sex != nil {\n\t\t\tb.Fatalf(\"bad value for sex: %v\", record.sex)\n\t\t}\n\t\tif record.birthDate != nil {\n\t\t\tb.Fatalf(\"bad value for birthDate: %v\", record.birthDate)\n\t\t}\n\t\tif record.lastLoginTime != nil {\n\t\t\tb.Fatalf(\"bad value for lastLoginTime: %v\", record.lastLoginTime)\n\t\t}\n\t}\n}\n\nfunc BenchmarkPointerPointerWithPresentValues(b *testing.B) {\n\tconn := mustConnect(b, mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\")))\n\tdefer closeConn(b, conn)\n\n\t_, err := conn.Prepare(context.Background(), \"selectNulls\", \"select 1::int4, 'johnsmith', 'johnsmith@example.com', 'John Smith', 'male', '1970-01-01'::date, '2015-01-01 00:00:00'::timestamptz\")\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tvar record struct {\n\t\t\tid            int32\n\t\t\tuserName      string\n\t\t\temail         *string\n\t\t\tname          *string\n\t\t\tsex           *string\n\t\t\tbirthDate     *time.Time\n\t\t\tlastLoginTime *time.Time\n\t\t}\n\n\t\terr = conn.QueryRow(context.Background(), \"selectNulls\").Scan(\n\t\t\t&record.id,\n\t\t\t&record.userName,\n\t\t\t&record.email,\n\t\t\t&record.name,\n\t\t\t&record.sex,\n\t\t\t&record.birthDate,\n\t\t\t&record.lastLoginTime,\n\t\t)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\n\t\t// These checks both ensure that the correct data was returned\n\t\t// and provide a benchmark of accessing the returned values.\n\t\tif record.id != 1 {\n\t\t\tb.Fatalf(\"bad value for id: %v\", record.id)\n\t\t}\n\t\tif record.userName != \"johnsmith\" {\n\t\t\tb.Fatalf(\"bad value for userName: %v\", record.userName)\n\t\t}\n\t\tif record.email == nil || *record.email != \"johnsmith@example.com\" {\n\t\t\tb.Fatalf(\"bad value for email: %v\", record.email)\n\t\t}\n\t\tif record.name == nil || *record.name != \"John Smith\" {\n\t\t\tb.Fatalf(\"bad value for name: %v\", record.name)\n\t\t}\n\t\tif record.sex == nil || *record.sex != \"male\" {\n\t\t\tb.Fatalf(\"bad value for sex: %v\", record.sex)\n\t\t}\n\t\tif record.birthDate == nil || *record.birthDate != time.Date(1970, 1, 1, 0, 0, 0, 0, time.UTC) {\n\t\t\tb.Fatalf(\"bad value for birthDate: %v\", record.birthDate)\n\t\t}\n\t\tif record.lastLoginTime == nil || *record.lastLoginTime != time.Date(2015, 1, 1, 0, 0, 0, 0, time.Local) {\n\t\t\tb.Fatalf(\"bad value for lastLoginTime: %v\", record.lastLoginTime)\n\t\t}\n\t}\n}\n\nconst benchmarkWriteTableCreateSQL = `drop table if exists t;\n\ncreate table t(\n\tvarchar_1 varchar not null,\n\tvarchar_2 varchar not null,\n\tvarchar_null_1 varchar,\n\tdate_1 date not null,\n\tdate_null_1 date,\n\tint4_1 int4 not null,\n\tint4_2 int4 not null,\n\tint4_null_1 int4,\n\ttstz_1 timestamptz not null,\n\ttstz_2 timestamptz,\n\tbool_1 bool not null,\n\tbool_2 bool not null,\n\tbool_3 bool not null\n);\n`\n\nconst benchmarkWriteTableInsertSQL = `insert into t(\n\tvarchar_1,\n\tvarchar_2,\n\tvarchar_null_1,\n\tdate_1,\n\tdate_null_1,\n\tint4_1,\n\tint4_2,\n\tint4_null_1,\n\ttstz_1,\n\ttstz_2,\n\tbool_1,\n\tbool_2,\n\tbool_3\n) values (\n\t$1::varchar,\n\t$2::varchar,\n\t$3::varchar,\n\t$4::date,\n\t$5::date,\n\t$6::int4,\n\t$7::int4,\n\t$8::int4,\n\t$9::timestamptz,\n\t$10::timestamptz,\n\t$11::bool,\n\t$12::bool,\n\t$13::bool\n)`\n\ntype benchmarkWriteTableCopyFromSrc struct {\n\tcount int\n\tidx   int\n\trow   []any\n}\n\nfunc (s *benchmarkWriteTableCopyFromSrc) Next() bool {\n\tnext := s.idx < s.count\n\ts.idx++\n\treturn next\n}\n\nfunc (s *benchmarkWriteTableCopyFromSrc) Values() ([]any, error) {\n\treturn s.row, nil\n}\n\nfunc (s *benchmarkWriteTableCopyFromSrc) Err() error {\n\treturn nil\n}\n\nfunc newBenchmarkWriteTableCopyFromSrc(count int) pgx.CopyFromSource {\n\treturn &benchmarkWriteTableCopyFromSrc{\n\t\tcount: count,\n\t\trow: []any{\n\t\t\t\"varchar_1\",\n\t\t\t\"varchar_2\",\n\t\t\t&pgtype.Text{},\n\t\t\ttime.Date(2000, 1, 1, 0, 0, 0, 0, time.Local),\n\t\t\t&pgtype.Date{},\n\t\t\t1,\n\t\t\t2,\n\t\t\t&pgtype.Int4{},\n\t\t\ttime.Date(2001, 1, 1, 0, 0, 0, 0, time.Local),\n\t\t\ttime.Date(2002, 1, 1, 0, 0, 0, 0, time.Local),\n\t\t\ttrue,\n\t\t\tfalse,\n\t\t\ttrue,\n\t\t},\n\t}\n}\n\nfunc benchmarkWriteNRowsViaInsert(b *testing.B, n int) {\n\tconn := mustConnect(b, mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\")))\n\tdefer closeConn(b, conn)\n\n\tmustExec(b, conn, benchmarkWriteTableCreateSQL)\n\t_, err := conn.Prepare(context.Background(), \"insert_t\", benchmarkWriteTableInsertSQL)\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tb.ResetTimer()\n\n\tfor i := 0; i < b.N; i++ {\n\t\tsrc := newBenchmarkWriteTableCopyFromSrc(n)\n\n\t\ttx, err := conn.Begin(context.Background())\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\n\t\tfor src.Next() {\n\t\t\tvalues, _ := src.Values()\n\t\t\tif _, err = tx.Exec(context.Background(), \"insert_t\", values...); err != nil {\n\t\t\t\tb.Fatalf(\"Exec unexpectedly failed with: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\terr = tx.Commit(context.Background())\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc benchmarkWriteNRowsViaBatchInsert(b *testing.B, n int) {\n\tconn := mustConnect(b, mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\")))\n\tdefer closeConn(b, conn)\n\n\tmustExec(b, conn, benchmarkWriteTableCreateSQL)\n\t_, err := conn.Prepare(context.Background(), \"insert_t\", benchmarkWriteTableInsertSQL)\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tb.ResetTimer()\n\n\tfor i := 0; i < b.N; i++ {\n\t\tsrc := newBenchmarkWriteTableCopyFromSrc(n)\n\n\t\tbatch := &pgx.Batch{}\n\t\tfor src.Next() {\n\t\t\tvalues, _ := src.Values()\n\t\t\tbatch.Queue(\"insert_t\", values...)\n\t\t}\n\n\t\terr = conn.SendBatch(context.Background(), batch).Close()\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\ntype queryArgs []any\n\nfunc (qa *queryArgs) Append(v any) string {\n\t*qa = append(*qa, v)\n\treturn \"$\" + strconv.Itoa(len(*qa))\n}\n\n// note this function is only used for benchmarks -- it doesn't escape tableName\n// or columnNames\nfunc multiInsert(conn *pgx.Conn, tableName string, columnNames []string, rowSrc pgx.CopyFromSource) (int, error) {\n\tmaxRowsPerInsert := 65535 / len(columnNames)\n\trowsThisInsert := 0\n\trowCount := 0\n\n\tsqlBuf := &bytes.Buffer{}\n\targs := make(queryArgs, 0)\n\n\tresetQuery := func() {\n\t\tsqlBuf.Reset()\n\t\tfmt.Fprintf(sqlBuf, \"insert into %s(%s) values\", tableName, strings.Join(columnNames, \", \"))\n\n\t\targs = args[0:0]\n\n\t\trowsThisInsert = 0\n\t}\n\tresetQuery()\n\n\ttx, err := conn.Begin(context.Background())\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tdefer tx.Rollback(context.Background())\n\n\tfor rowSrc.Next() {\n\t\tif rowsThisInsert > 0 {\n\t\t\tsqlBuf.WriteByte(',')\n\t\t}\n\n\t\tsqlBuf.WriteByte('(')\n\n\t\tvalues, err := rowSrc.Values()\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\n\t\tfor i, val := range values {\n\t\t\tif i > 0 {\n\t\t\t\tsqlBuf.WriteByte(',')\n\t\t\t}\n\t\t\tsqlBuf.WriteString(args.Append(val))\n\t\t}\n\n\t\tsqlBuf.WriteByte(')')\n\n\t\trowsThisInsert++\n\n\t\tif rowsThisInsert == maxRowsPerInsert {\n\t\t\t_, err := tx.Exec(context.Background(), sqlBuf.String(), args...)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\n\t\t\trowCount += rowsThisInsert\n\t\t\tresetQuery()\n\t\t}\n\t}\n\n\tif rowsThisInsert > 0 {\n\t\t_, err := tx.Exec(context.Background(), sqlBuf.String(), args...)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\n\t\trowCount += rowsThisInsert\n\t}\n\n\tif err := tx.Commit(context.Background()); err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn rowCount, nil\n\n}\n\nfunc benchmarkWriteNRowsViaMultiInsert(b *testing.B, n int) {\n\tconn := mustConnect(b, mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\")))\n\tdefer closeConn(b, conn)\n\n\tmustExec(b, conn, benchmarkWriteTableCreateSQL)\n\t_, err := conn.Prepare(context.Background(), \"insert_t\", benchmarkWriteTableInsertSQL)\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tb.ResetTimer()\n\n\tfor i := 0; i < b.N; i++ {\n\t\tsrc := newBenchmarkWriteTableCopyFromSrc(n)\n\n\t\t_, err := multiInsert(conn, \"t\",\n\t\t\t[]string{\"varchar_1\",\n\t\t\t\t\"varchar_2\",\n\t\t\t\t\"varchar_null_1\",\n\t\t\t\t\"date_1\",\n\t\t\t\t\"date_null_1\",\n\t\t\t\t\"int4_1\",\n\t\t\t\t\"int4_2\",\n\t\t\t\t\"int4_null_1\",\n\t\t\t\t\"tstz_1\",\n\t\t\t\t\"tstz_2\",\n\t\t\t\t\"bool_1\",\n\t\t\t\t\"bool_2\",\n\t\t\t\t\"bool_3\"},\n\t\t\tsrc)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc benchmarkWriteNRowsViaCopy(b *testing.B, n int) {\n\tconn := mustConnect(b, mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\")))\n\tdefer closeConn(b, conn)\n\n\tmustExec(b, conn, benchmarkWriteTableCreateSQL)\n\n\tb.ResetTimer()\n\n\tfor i := 0; i < b.N; i++ {\n\t\tsrc := newBenchmarkWriteTableCopyFromSrc(n)\n\n\t\t_, err := conn.CopyFrom(context.Background(),\n\t\t\tpgx.Identifier{\"t\"},\n\t\t\t[]string{\"varchar_1\",\n\t\t\t\t\"varchar_2\",\n\t\t\t\t\"varchar_null_1\",\n\t\t\t\t\"date_1\",\n\t\t\t\t\"date_null_1\",\n\t\t\t\t\"int4_1\",\n\t\t\t\t\"int4_2\",\n\t\t\t\t\"int4_null_1\",\n\t\t\t\t\"tstz_1\",\n\t\t\t\t\"tstz_2\",\n\t\t\t\t\"bool_1\",\n\t\t\t\t\"bool_2\",\n\t\t\t\t\"bool_3\"},\n\t\t\tsrc)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkWrite2RowsViaInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaInsert(b, 2)\n}\n\nfunc BenchmarkWrite2RowsViaMultiInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaMultiInsert(b, 2)\n}\n\nfunc BenchmarkWrite2RowsViaBatchInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaBatchInsert(b, 2)\n}\n\nfunc BenchmarkWrite2RowsViaCopy(b *testing.B) {\n\tbenchmarkWriteNRowsViaCopy(b, 2)\n}\n\nfunc BenchmarkWrite5RowsViaInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaInsert(b, 5)\n}\n\nfunc BenchmarkWrite5RowsViaMultiInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaMultiInsert(b, 5)\n}\nfunc BenchmarkWrite5RowsViaBatchInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaBatchInsert(b, 5)\n}\n\nfunc BenchmarkWrite5RowsViaCopy(b *testing.B) {\n\tbenchmarkWriteNRowsViaCopy(b, 5)\n}\n\nfunc BenchmarkWrite10RowsViaInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaInsert(b, 10)\n}\n\nfunc BenchmarkWrite10RowsViaMultiInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaMultiInsert(b, 10)\n}\nfunc BenchmarkWrite10RowsViaBatchInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaBatchInsert(b, 10)\n}\n\nfunc BenchmarkWrite10RowsViaCopy(b *testing.B) {\n\tbenchmarkWriteNRowsViaCopy(b, 10)\n}\n\nfunc BenchmarkWrite100RowsViaInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaInsert(b, 100)\n}\n\nfunc BenchmarkWrite100RowsViaMultiInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaMultiInsert(b, 100)\n}\nfunc BenchmarkWrite100RowsViaBatchInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaBatchInsert(b, 100)\n}\n\nfunc BenchmarkWrite100RowsViaCopy(b *testing.B) {\n\tbenchmarkWriteNRowsViaCopy(b, 100)\n}\n\nfunc BenchmarkWrite1000RowsViaInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaInsert(b, 1000)\n}\n\nfunc BenchmarkWrite1000RowsViaMultiInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaMultiInsert(b, 1000)\n}\n\nfunc BenchmarkWrite1000RowsViaBatchInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaBatchInsert(b, 1000)\n}\n\nfunc BenchmarkWrite1000RowsViaCopy(b *testing.B) {\n\tbenchmarkWriteNRowsViaCopy(b, 1000)\n}\n\nfunc BenchmarkWrite10000RowsViaInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaInsert(b, 10000)\n}\n\nfunc BenchmarkWrite10000RowsViaMultiInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaMultiInsert(b, 10000)\n}\nfunc BenchmarkWrite10000RowsViaBatchInsert(b *testing.B) {\n\tbenchmarkWriteNRowsViaBatchInsert(b, 10000)\n}\n\nfunc BenchmarkWrite10000RowsViaCopy(b *testing.B) {\n\tbenchmarkWriteNRowsViaCopy(b, 10000)\n}\n\nfunc BenchmarkMultipleQueriesNonBatchNoStatementCache(b *testing.B) {\n\tconfig := mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeDescribeExec\n\tconfig.StatementCacheCapacity = 0\n\tconfig.DescriptionCacheCapacity = 0\n\n\tconn := mustConnect(b, config)\n\tdefer closeConn(b, conn)\n\n\tbenchmarkMultipleQueriesNonBatch(b, conn, 3)\n}\n\nfunc BenchmarkMultipleQueriesNonBatchPrepareStatementCache(b *testing.B) {\n\tconfig := mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeCacheStatement\n\tconfig.StatementCacheCapacity = 32\n\tconfig.DescriptionCacheCapacity = 0\n\n\tconn := mustConnect(b, config)\n\tdefer closeConn(b, conn)\n\n\tbenchmarkMultipleQueriesNonBatch(b, conn, 3)\n}\n\nfunc BenchmarkMultipleQueriesNonBatchDescribeStatementCache(b *testing.B) {\n\tconfig := mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeCacheDescribe\n\tconfig.StatementCacheCapacity = 0\n\tconfig.DescriptionCacheCapacity = 32\n\n\tconn := mustConnect(b, config)\n\tdefer closeConn(b, conn)\n\n\tbenchmarkMultipleQueriesNonBatch(b, conn, 3)\n}\n\nfunc benchmarkMultipleQueriesNonBatch(b *testing.B, conn *pgx.Conn, queryCount int) {\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tfor j := 0; j < queryCount; j++ {\n\t\t\trows, err := conn.Query(context.Background(), \"select n from generate_series(0, 5) n\")\n\t\t\tif err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\n\t\t\tfor k := 0; rows.Next(); k++ {\n\t\t\t\tvar n int\n\t\t\t\tif err := rows.Scan(&n); err != nil {\n\t\t\t\t\tb.Fatal(err)\n\t\t\t\t}\n\t\t\t\tif n != k {\n\t\t\t\t\tb.Fatalf(\"n => %v, want %v\", n, k)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif rows.Err() != nil {\n\t\t\t\tb.Fatal(rows.Err())\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc BenchmarkMultipleQueriesBatchNoStatementCache(b *testing.B) {\n\tconfig := mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeDescribeExec\n\tconfig.StatementCacheCapacity = 0\n\tconfig.DescriptionCacheCapacity = 0\n\n\tconn := mustConnect(b, config)\n\tdefer closeConn(b, conn)\n\n\tbenchmarkMultipleQueriesBatch(b, conn, 3)\n}\n\nfunc BenchmarkMultipleQueriesBatchPrepareStatementCache(b *testing.B) {\n\tconfig := mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeCacheStatement\n\tconfig.StatementCacheCapacity = 32\n\tconfig.DescriptionCacheCapacity = 0\n\n\tconn := mustConnect(b, config)\n\tdefer closeConn(b, conn)\n\n\tbenchmarkMultipleQueriesBatch(b, conn, 3)\n}\n\nfunc BenchmarkMultipleQueriesBatchDescribeStatementCache(b *testing.B) {\n\tconfig := mustParseConfig(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeCacheDescribe\n\tconfig.StatementCacheCapacity = 0\n\tconfig.DescriptionCacheCapacity = 32\n\n\tconn := mustConnect(b, config)\n\tdefer closeConn(b, conn)\n\n\tbenchmarkMultipleQueriesBatch(b, conn, 3)\n}\n\nfunc benchmarkMultipleQueriesBatch(b *testing.B, conn *pgx.Conn, queryCount int) {\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tbatch := &pgx.Batch{}\n\t\tfor j := 0; j < queryCount; j++ {\n\t\t\tbatch.Queue(\"select n from generate_series(0,5) n\")\n\t\t}\n\n\t\tbr := conn.SendBatch(context.Background(), batch)\n\n\t\tfor j := 0; j < queryCount; j++ {\n\t\t\trows, err := br.Query()\n\t\t\tif err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\n\t\t\tfor k := 0; rows.Next(); k++ {\n\t\t\t\tvar n int\n\t\t\t\tif err := rows.Scan(&n); err != nil {\n\t\t\t\t\tb.Fatal(err)\n\t\t\t\t}\n\t\t\t\tif n != k {\n\t\t\t\t\tb.Fatalf(\"n => %v, want %v\", n, k)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif rows.Err() != nil {\n\t\t\t\tb.Fatal(rows.Err())\n\t\t\t}\n\t\t}\n\n\t\terr := br.Close()\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkSelectManyUnknownEnum(b *testing.B) {\n\tconn := mustConnectString(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(b, conn)\n\n\tctx := context.Background()\n\ttx, err := conn.Begin(ctx)\n\trequire.NoError(b, err)\n\tdefer tx.Rollback(ctx)\n\n\t_, err = tx.Exec(context.Background(), \"drop type if exists color;\")\n\trequire.NoError(b, err)\n\n\t_, err = tx.Exec(ctx, `create type color as enum ('blue', 'green', 'orange')`)\n\trequire.NoError(b, err)\n\n\tb.ResetTimer()\n\tvar x, y, z string\n\tfor i := 0; i < b.N; i++ {\n\t\trows, err := conn.Query(ctx, \"select 'blue'::color, 'green'::color, 'orange'::color from generate_series(1,10)\")\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\n\t\tfor rows.Next() {\n\t\t\terr = rows.Scan(&x, &y, &z)\n\t\t\tif err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\n\t\t\tif x != \"blue\" {\n\t\t\t\tb.Fatal(\"unexpected result\")\n\t\t\t}\n\t\t\tif y != \"green\" {\n\t\t\t\tb.Fatal(\"unexpected result\")\n\t\t\t}\n\t\t\tif z != \"orange\" {\n\t\t\t\tb.Fatal(\"unexpected result\")\n\t\t\t}\n\t\t}\n\n\t\tif rows.Err() != nil {\n\t\t\tb.Fatal(rows.Err())\n\t\t}\n\t}\n}\n\nfunc BenchmarkSelectManyRegisteredEnum(b *testing.B) {\n\tconn := mustConnectString(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(b, conn)\n\n\tctx := context.Background()\n\ttx, err := conn.Begin(ctx)\n\trequire.NoError(b, err)\n\tdefer tx.Rollback(ctx)\n\n\t_, err = tx.Exec(context.Background(), \"drop type if exists color;\")\n\trequire.NoError(b, err)\n\n\t_, err = tx.Exec(ctx, `create type color as enum ('blue', 'green', 'orange')`)\n\trequire.NoError(b, err)\n\n\tvar oid uint32\n\terr = conn.QueryRow(context.Background(), \"select oid from pg_type where typname=$1;\", \"color\").Scan(&oid)\n\trequire.NoError(b, err)\n\n\tconn.TypeMap().RegisterType(&pgtype.Type{Name: \"color\", OID: oid, Codec: &pgtype.EnumCodec{}})\n\n\tb.ResetTimer()\n\tvar x, y, z string\n\tfor i := 0; i < b.N; i++ {\n\t\trows, err := conn.Query(ctx, \"select 'blue'::color, 'green'::color, 'orange'::color from generate_series(1,10)\")\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\n\t\tfor rows.Next() {\n\t\t\terr = rows.Scan(&x, &y, &z)\n\t\t\tif err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\n\t\t\tif x != \"blue\" {\n\t\t\t\tb.Fatal(\"unexpected result\")\n\t\t\t}\n\t\t\tif y != \"green\" {\n\t\t\t\tb.Fatal(\"unexpected result\")\n\t\t\t}\n\t\t\tif z != \"orange\" {\n\t\t\t\tb.Fatal(\"unexpected result\")\n\t\t\t}\n\t\t}\n\n\t\tif rows.Err() != nil {\n\t\t\tb.Fatal(rows.Err())\n\t\t}\n\t}\n}\n\nfunc getSelectRowsCounts(b *testing.B) []int64 {\n\tvar rowCounts []int64\n\t{\n\t\ts := os.Getenv(\"PGX_BENCH_SELECT_ROWS_COUNTS\")\n\t\tif s != \"\" {\n\t\t\tfor _, p := range strings.Split(s, \" \") {\n\t\t\t\tn, err := strconv.ParseInt(p, 10, 64)\n\t\t\t\tif err != nil {\n\t\t\t\t\tb.Fatalf(\"Bad PGX_BENCH_SELECT_ROWS_COUNTS value: %v\", err)\n\t\t\t\t}\n\t\t\t\trowCounts = append(rowCounts, n)\n\t\t\t}\n\t\t}\n\t}\n\n\tif len(rowCounts) == 0 {\n\t\trowCounts = []int64{1, 10, 100, 1000}\n\t}\n\n\treturn rowCounts\n}\n\ntype BenchRowSimple struct {\n\tID         int32\n\tFirstName  string\n\tLastName   string\n\tSex        string\n\tBirthDate  time.Time\n\tWeight     int32\n\tHeight     int32\n\tUpdateTime time.Time\n}\n\nfunc BenchmarkSelectRowsScanSimple(b *testing.B) {\n\tconn := mustConnectString(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(b, conn)\n\n\trowCounts := getSelectRowsCounts(b)\n\n\tfor _, rowCount := range rowCounts {\n\t\tb.Run(fmt.Sprintf(\"%d rows\", rowCount), func(b *testing.B) {\n\t\t\tbr := &BenchRowSimple{}\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\trows, err := conn.Query(context.Background(), \"select n, 'Adam', 'Smith ' || n, 'male', '1952-06-16'::date, 258, 72, '2001-01-28 01:02:03-05'::timestamptz from generate_series(100001, 100000 + $1) n\", rowCount)\n\t\t\t\tif err != nil {\n\t\t\t\t\tb.Fatal(err)\n\t\t\t\t}\n\n\t\t\t\tfor rows.Next() {\n\t\t\t\t\trows.Scan(&br.ID, &br.FirstName, &br.LastName, &br.Sex, &br.BirthDate, &br.Weight, &br.Height, &br.UpdateTime)\n\t\t\t\t}\n\n\t\t\t\tif rows.Err() != nil {\n\t\t\t\t\tb.Fatal(rows.Err())\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\ntype BenchRowStringBytes struct {\n\tID         int32\n\tFirstName  []byte\n\tLastName   []byte\n\tSex        []byte\n\tBirthDate  time.Time\n\tWeight     int32\n\tHeight     int32\n\tUpdateTime time.Time\n}\n\nfunc BenchmarkSelectRowsScanStringBytes(b *testing.B) {\n\tconn := mustConnectString(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(b, conn)\n\n\trowCounts := getSelectRowsCounts(b)\n\n\tfor _, rowCount := range rowCounts {\n\t\tb.Run(fmt.Sprintf(\"%d rows\", rowCount), func(b *testing.B) {\n\t\t\tbr := &BenchRowStringBytes{}\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\trows, err := conn.Query(context.Background(), \"select n, 'Adam', 'Smith ' || n, 'male', '1952-06-16'::date, 258, 72, '2001-01-28 01:02:03-05'::timestamptz from generate_series(100001, 100000 + $1) n\", rowCount)\n\t\t\t\tif err != nil {\n\t\t\t\t\tb.Fatal(err)\n\t\t\t\t}\n\n\t\t\t\tfor rows.Next() {\n\t\t\t\t\trows.Scan(&br.ID, &br.FirstName, &br.LastName, &br.Sex, &br.BirthDate, &br.Weight, &br.Height, &br.UpdateTime)\n\t\t\t\t}\n\n\t\t\t\tif rows.Err() != nil {\n\t\t\t\t\tb.Fatal(rows.Err())\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\ntype BenchRowDecoder struct {\n\tID         pgtype.Int4\n\tFirstName  pgtype.Text\n\tLastName   pgtype.Text\n\tSex        pgtype.Text\n\tBirthDate  pgtype.Date\n\tWeight     pgtype.Int4\n\tHeight     pgtype.Int4\n\tUpdateTime pgtype.Timestamptz\n}\n\nfunc BenchmarkSelectRowsScanDecoder(b *testing.B) {\n\tconn := mustConnectString(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(b, conn)\n\n\trowCounts := getSelectRowsCounts(b)\n\n\tfor _, rowCount := range rowCounts {\n\t\tb.Run(fmt.Sprintf(\"%d rows\", rowCount), func(b *testing.B) {\n\t\t\tformats := []struct {\n\t\t\t\tname string\n\t\t\t\tcode int16\n\t\t\t}{\n\t\t\t\t{\"text\", pgx.TextFormatCode},\n\t\t\t\t{\"binary\", pgx.BinaryFormatCode},\n\t\t\t}\n\t\t\tfor _, format := range formats {\n\t\t\t\tb.Run(format.name, func(b *testing.B) {\n\n\t\t\t\t\tbr := &BenchRowDecoder{}\n\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\trows, err := conn.Query(\n\t\t\t\t\t\t\tcontext.Background(),\n\t\t\t\t\t\t\t\"select n, 'Adam', 'Smith ' || n, 'male', '1952-06-16'::date, 258, 72, '2001-01-28 01:02:03-05'::timestamptz from generate_series(100001, 100000 + $1) n\",\n\t\t\t\t\t\t\tpgx.QueryResultFormats{format.code},\n\t\t\t\t\t\t\trowCount,\n\t\t\t\t\t\t)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tb.Fatal(err)\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tfor rows.Next() {\n\t\t\t\t\t\t\trows.Scan(&br.ID, &br.FirstName, &br.LastName, &br.Sex, &br.BirthDate, &br.Weight, &br.Height, &br.UpdateTime)\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif rows.Err() != nil {\n\t\t\t\t\t\t\tb.Fatal(rows.Err())\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc BenchmarkSelectRowsPgConnExecText(b *testing.B) {\n\tconn := mustConnectString(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(b, conn)\n\n\trowCounts := getSelectRowsCounts(b)\n\n\tfor _, rowCount := range rowCounts {\n\t\tb.Run(fmt.Sprintf(\"%d rows\", rowCount), func(b *testing.B) {\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\tmrr := conn.PgConn().Exec(context.Background(), fmt.Sprintf(\"select n, 'Adam', 'Smith ' || n, 'male', '1952-06-16'::date, 258, 72, '2001-01-28 01:02:03-05'::timestamptz from generate_series(100001, 100000 + %d) n\", rowCount))\n\t\t\t\tfor mrr.NextResult() {\n\t\t\t\t\trr := mrr.ResultReader()\n\t\t\t\t\tfor rr.NextRow() {\n\t\t\t\t\t\trr.Values()\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\terr := mrr.Close()\n\t\t\t\tif err != nil {\n\t\t\t\t\tb.Fatal(err)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc BenchmarkSelectRowsPgConnExecParams(b *testing.B) {\n\tconn := mustConnectString(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(b, conn)\n\n\trowCounts := getSelectRowsCounts(b)\n\n\tfor _, rowCount := range rowCounts {\n\t\tb.Run(fmt.Sprintf(\"%d rows\", rowCount), func(b *testing.B) {\n\t\t\tformats := []struct {\n\t\t\t\tname string\n\t\t\t\tcode int16\n\t\t\t}{\n\t\t\t\t{\"text\", pgx.TextFormatCode},\n\t\t\t\t{\"binary - mostly\", pgx.BinaryFormatCode},\n\t\t\t}\n\t\t\tfor _, format := range formats {\n\t\t\t\tb.Run(format.name, func(b *testing.B) {\n\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\trr := conn.PgConn().ExecParams(\n\t\t\t\t\t\t\tcontext.Background(),\n\t\t\t\t\t\t\t\"select n, 'Adam', 'Smith ' || n, 'male', '1952-06-16'::date, 258, 72, '2001-01-28 01:02:03-05'::timestamptz from generate_series(100001, 100000 + $1) n\",\n\t\t\t\t\t\t\t[][]byte{[]byte(strconv.FormatInt(rowCount, 10))},\n\t\t\t\t\t\t\tnil,\n\t\t\t\t\t\t\tnil,\n\t\t\t\t\t\t\t[]int16{format.code, pgx.TextFormatCode, pgx.TextFormatCode, pgx.TextFormatCode, format.code, format.code, format.code, format.code},\n\t\t\t\t\t\t)\n\t\t\t\t\t\tfor rr.NextRow() {\n\t\t\t\t\t\t\trr.Values()\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t_, err := rr.Close()\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tb.Fatal(err)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc BenchmarkSelectRowsSimpleCollectRowsRowToStructByPos(b *testing.B) {\n\tconn := mustConnectString(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(b, conn)\n\n\trowCounts := getSelectRowsCounts(b)\n\n\tfor _, rowCount := range rowCounts {\n\t\tb.Run(fmt.Sprintf(\"%d rows\", rowCount), func(b *testing.B) {\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\trows, _ := conn.Query(context.Background(), \"select n, 'Adam', 'Smith ' || n, 'male', '1952-06-16'::date, 258, 72, '2001-01-28 01:02:03-05'::timestamptz from generate_series(100001, 100000 + $1) n\", rowCount)\n\t\t\t\tbenchRows, err := pgx.CollectRows(rows, pgx.RowToStructByPos[BenchRowSimple])\n\t\t\t\tif err != nil {\n\t\t\t\t\tb.Fatal(err)\n\t\t\t\t}\n\t\t\t\tif len(benchRows) != int(rowCount) {\n\t\t\t\t\tb.Fatalf(\"Expected %d rows, got %d\", rowCount, len(benchRows))\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc BenchmarkSelectRowsSimpleAppendRowsRowToStructByPos(b *testing.B) {\n\tconn := mustConnectString(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(b, conn)\n\n\trowCounts := getSelectRowsCounts(b)\n\n\tfor _, rowCount := range rowCounts {\n\t\tb.Run(fmt.Sprintf(\"%d rows\", rowCount), func(b *testing.B) {\n\t\t\tbenchRows := make([]BenchRowSimple, 0, rowCount)\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\tbenchRows = benchRows[:0]\n\t\t\t\trows, _ := conn.Query(context.Background(), \"select n, 'Adam', 'Smith ' || n, 'male', '1952-06-16'::date, 258, 72, '2001-01-28 01:02:03-05'::timestamptz from generate_series(100001, 100000 + $1) n\", rowCount)\n\t\t\t\tvar err error\n\t\t\t\tbenchRows, err = pgx.AppendRows(benchRows, rows, pgx.RowToStructByPos[BenchRowSimple])\n\t\t\t\tif err != nil {\n\t\t\t\t\tb.Fatal(err)\n\t\t\t\t}\n\t\t\t\tif len(benchRows) != int(rowCount) {\n\t\t\t\t\tb.Fatalf(\"Expected %d rows, got %d\", rowCount, len(benchRows))\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc BenchmarkSelectRowsSimpleCollectRowsRowToStructByName(b *testing.B) {\n\tconn := mustConnectString(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(b, conn)\n\n\trowCounts := getSelectRowsCounts(b)\n\n\tfor _, rowCount := range rowCounts {\n\t\tb.Run(fmt.Sprintf(\"%d rows\", rowCount), func(b *testing.B) {\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\trows, _ := conn.Query(context.Background(), \"select n as id, 'Adam' as first_name, 'Smith ' || n as last_name, 'male' as sex, '1952-06-16'::date as birth_date, 258 as weight, 72 as height, '2001-01-28 01:02:03-05'::timestamptz as update_time from generate_series(100001, 100000 + $1) n\", rowCount)\n\t\t\t\tbenchRows, err := pgx.CollectRows(rows, pgx.RowToStructByName[BenchRowSimple])\n\t\t\t\tif err != nil {\n\t\t\t\t\tb.Fatal(err)\n\t\t\t\t}\n\t\t\t\tif len(benchRows) != int(rowCount) {\n\t\t\t\t\tb.Fatalf(\"Expected %d rows, got %d\", rowCount, len(benchRows))\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc BenchmarkSelectRowsSimpleAppendRowsRowToStructByName(b *testing.B) {\n\tconn := mustConnectString(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(b, conn)\n\n\trowCounts := getSelectRowsCounts(b)\n\n\tfor _, rowCount := range rowCounts {\n\t\tb.Run(fmt.Sprintf(\"%d rows\", rowCount), func(b *testing.B) {\n\t\t\tbenchRows := make([]BenchRowSimple, 0, rowCount)\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\tbenchRows = benchRows[:0]\n\t\t\t\trows, _ := conn.Query(context.Background(), \"select n as id, 'Adam' as first_name, 'Smith ' || n as last_name, 'male' as sex, '1952-06-16'::date as birth_date, 258 as weight, 72 as height, '2001-01-28 01:02:03-05'::timestamptz as update_time from generate_series(100001, 100000 + $1) n\", rowCount)\n\t\t\t\tvar err error\n\t\t\t\tbenchRows, err = pgx.AppendRows(benchRows, rows, pgx.RowToStructByPos[BenchRowSimple])\n\t\t\t\tif err != nil {\n\t\t\t\t\tb.Fatal(err)\n\t\t\t\t}\n\t\t\t\tif len(benchRows) != int(rowCount) {\n\t\t\t\t\tb.Fatalf(\"Expected %d rows, got %d\", rowCount, len(benchRows))\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc BenchmarkSelectRowsPgConnExecPrepared(b *testing.B) {\n\tconn := mustConnectString(b, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(b, conn)\n\n\trowCounts := getSelectRowsCounts(b)\n\n\t_, err := conn.PgConn().Prepare(context.Background(), \"ps1\", \"select n, 'Adam', 'Smith ' || n, 'male', '1952-06-16'::date, 258, 72, '2001-01-28 01:02:03-05'::timestamptz from generate_series(100001, 100000 + $1) n\", nil)\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tfor _, rowCount := range rowCounts {\n\t\tb.Run(fmt.Sprintf(\"%d rows\", rowCount), func(b *testing.B) {\n\t\t\tformats := []struct {\n\t\t\t\tname string\n\t\t\t\tcode int16\n\t\t\t}{\n\t\t\t\t{\"text\", pgx.TextFormatCode},\n\t\t\t\t{\"binary - mostly\", pgx.BinaryFormatCode},\n\t\t\t}\n\t\t\tfor _, format := range formats {\n\t\t\t\tb.Run(format.name, func(b *testing.B) {\n\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\trr := conn.PgConn().ExecPrepared(\n\t\t\t\t\t\t\tcontext.Background(),\n\t\t\t\t\t\t\t\"ps1\",\n\t\t\t\t\t\t\t[][]byte{[]byte(strconv.FormatInt(rowCount, 10))},\n\t\t\t\t\t\t\tnil,\n\t\t\t\t\t\t\t[]int16{format.code, pgx.TextFormatCode, pgx.TextFormatCode, pgx.TextFormatCode, format.code, format.code, format.code, format.code},\n\t\t\t\t\t\t)\n\t\t\t\t\t\tfor rr.NextRow() {\n\t\t\t\t\t\t\trr.Values()\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t_, err := rr.Close()\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tb.Fatal(err)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n\ntype queryRecorder struct {\n\tconn      net.Conn\n\twriteBuf  []byte\n\treadCount int\n}\n\nfunc (qr *queryRecorder) Read(b []byte) (n int, err error) {\n\tn, err = qr.conn.Read(b)\n\tqr.readCount += n\n\treturn n, err\n}\n\nfunc (qr *queryRecorder) Write(b []byte) (n int, err error) {\n\tqr.writeBuf = append(qr.writeBuf, b...)\n\treturn qr.conn.Write(b)\n}\n\nfunc (qr *queryRecorder) Close() error {\n\treturn qr.conn.Close()\n}\n\nfunc (qr *queryRecorder) LocalAddr() net.Addr {\n\treturn qr.conn.LocalAddr()\n}\n\nfunc (qr *queryRecorder) RemoteAddr() net.Addr {\n\treturn qr.conn.RemoteAddr()\n}\n\nfunc (qr *queryRecorder) SetDeadline(t time.Time) error {\n\treturn qr.conn.SetDeadline(t)\n}\n\nfunc (qr *queryRecorder) SetReadDeadline(t time.Time) error {\n\treturn qr.conn.SetReadDeadline(t)\n}\n\nfunc (qr *queryRecorder) SetWriteDeadline(t time.Time) error {\n\treturn qr.conn.SetWriteDeadline(t)\n}\n\n// BenchmarkSelectRowsRawPrepared hijacks a pgconn connection and inserts a queryRecorder. It then executes the query\n// once. The benchmark is simply sending the exact query bytes over the wire to the server and reading the expected\n// number of bytes back. It does nothing else. This should be the theoretical maximum performance a Go application\n// could achieve.\nfunc BenchmarkSelectRowsRawPrepared(b *testing.B) {\n\trowCounts := getSelectRowsCounts(b)\n\n\tfor _, rowCount := range rowCounts {\n\t\tb.Run(fmt.Sprintf(\"%d rows\", rowCount), func(b *testing.B) {\n\t\t\tformats := []struct {\n\t\t\t\tname string\n\t\t\t\tcode int16\n\t\t\t}{\n\t\t\t\t{\"text\", pgx.TextFormatCode},\n\t\t\t\t{\"binary - mostly\", pgx.BinaryFormatCode},\n\t\t\t}\n\t\t\tfor _, format := range formats {\n\t\t\t\tb.Run(format.name, func(b *testing.B) {\n\t\t\t\t\tconn := mustConnectString(b, os.Getenv(\"PGX_TEST_DATABASE\")).PgConn()\n\t\t\t\t\tdefer conn.Close(context.Background())\n\n\t\t\t\t\t_, err := conn.Prepare(context.Background(), \"ps1\", \"select n, 'Adam', 'Smith ' || n, 'male', '1952-06-16'::date, 258, 72, '2001-01-28 01:02:03-05'::timestamptz from generate_series(100001, 100000 + $1) n\", nil)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tb.Fatal(err)\n\t\t\t\t\t}\n\n\t\t\t\t\thijackedConn, err := conn.Hijack()\n\t\t\t\t\trequire.NoError(b, err)\n\n\t\t\t\t\tqr := &queryRecorder{\n\t\t\t\t\t\tconn: hijackedConn.Conn,\n\t\t\t\t\t}\n\n\t\t\t\t\thijackedConn.Conn = qr\n\t\t\t\t\thijackedConn.Frontend = hijackedConn.Config.BuildFrontend(qr, qr)\n\t\t\t\t\tconn, err = pgconn.Construct(hijackedConn)\n\t\t\t\t\trequire.NoError(b, err)\n\n\t\t\t\t\t{\n\t\t\t\t\t\trr := conn.ExecPrepared(\n\t\t\t\t\t\t\tcontext.Background(),\n\t\t\t\t\t\t\t\"ps1\",\n\t\t\t\t\t\t\t[][]byte{[]byte(strconv.FormatInt(rowCount, 10))},\n\t\t\t\t\t\t\tnil,\n\t\t\t\t\t\t\t[]int16{format.code, pgx.TextFormatCode, pgx.TextFormatCode, pgx.TextFormatCode, format.code, format.code, format.code, format.code},\n\t\t\t\t\t\t)\n\t\t\t\t\t\t_, err := rr.Close()\n\t\t\t\t\t\trequire.NoError(b, err)\n\t\t\t\t\t}\n\n\t\t\t\t\tbuf := make([]byte, qr.readCount)\n\n\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\t_, err := qr.conn.Write(qr.writeBuf)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tb.Fatal(err)\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t_, err = io.ReadFull(qr.conn, buf)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tb.Fatal(err)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "ci",
          "type": "tree",
          "content": null
        },
        {
          "name": "conn.go",
          "type": "blob",
          "size": 45.5439453125,
          "content": "package pgx\n\nimport (\n\t\"context\"\n\t\"crypto/sha256\"\n\t\"database/sql\"\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5/internal/sanitize\"\n\t\"github.com/jackc/pgx/v5/internal/stmtcache\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgtype\"\n)\n\n// ConnConfig contains all the options used to establish a connection. It must be created by ParseConfig and\n// then it can be modified. A manually initialized ConnConfig will cause ConnectConfig to panic.\ntype ConnConfig struct {\n\tpgconn.Config\n\n\tTracer QueryTracer\n\n\t// Original connection string that was parsed into config.\n\tconnString string\n\n\t// StatementCacheCapacity is maximum size of the statement cache used when executing a query with \"cache_statement\"\n\t// query exec mode.\n\tStatementCacheCapacity int\n\n\t// DescriptionCacheCapacity is the maximum size of the description cache used when executing a query with\n\t// \"cache_describe\" query exec mode.\n\tDescriptionCacheCapacity int\n\n\t// DefaultQueryExecMode controls the default mode for executing queries. By default pgx uses the extended protocol\n\t// and automatically prepares and caches prepared statements. However, this may be incompatible with proxies such as\n\t// PGBouncer. In this case it may be preferable to use QueryExecModeExec or QueryExecModeSimpleProtocol. The same\n\t// functionality can be controlled on a per query basis by passing a QueryExecMode as the first query argument.\n\tDefaultQueryExecMode QueryExecMode\n\n\tcreatedByParseConfig bool // Used to enforce created by ParseConfig rule.\n}\n\n// ParseConfigOptions contains options that control how a config is built such as getsslpassword.\ntype ParseConfigOptions struct {\n\tpgconn.ParseConfigOptions\n}\n\n// Copy returns a deep copy of the config that is safe to use and modify.\n// The only exception is the tls.Config:\n// according to the tls.Config docs it must not be modified after creation.\nfunc (cc *ConnConfig) Copy() *ConnConfig {\n\tnewConfig := new(ConnConfig)\n\t*newConfig = *cc\n\tnewConfig.Config = *newConfig.Config.Copy()\n\treturn newConfig\n}\n\n// ConnString returns the connection string as parsed by pgx.ParseConfig into pgx.ConnConfig.\nfunc (cc *ConnConfig) ConnString() string { return cc.connString }\n\n// Conn is a PostgreSQL connection handle. It is not safe for concurrent usage. Use a connection pool to manage access\n// to multiple database connections from multiple goroutines.\ntype Conn struct {\n\tpgConn             *pgconn.PgConn\n\tconfig             *ConnConfig // config used when establishing this connection\n\tpreparedStatements map[string]*pgconn.StatementDescription\n\tstatementCache     stmtcache.Cache\n\tdescriptionCache   stmtcache.Cache\n\n\tqueryTracer    QueryTracer\n\tbatchTracer    BatchTracer\n\tcopyFromTracer CopyFromTracer\n\tprepareTracer  PrepareTracer\n\n\tnotifications []*pgconn.Notification\n\n\tdoneChan   chan struct{}\n\tclosedChan chan error\n\n\ttypeMap *pgtype.Map\n\n\twbuf []byte\n\teqb  ExtendedQueryBuilder\n}\n\n// Identifier a PostgreSQL identifier or name. Identifiers can be composed of\n// multiple parts such as [\"schema\", \"table\"] or [\"table\", \"column\"].\ntype Identifier []string\n\n// Sanitize returns a sanitized string safe for SQL interpolation.\nfunc (ident Identifier) Sanitize() string {\n\tparts := make([]string, len(ident))\n\tfor i := range ident {\n\t\ts := strings.ReplaceAll(ident[i], string([]byte{0}), \"\")\n\t\tparts[i] = `\"` + strings.ReplaceAll(s, `\"`, `\"\"`) + `\"`\n\t}\n\treturn strings.Join(parts, \".\")\n}\n\nvar (\n\t// ErrNoRows occurs when rows are expected but none are returned.\n\tErrNoRows = newProxyErr(sql.ErrNoRows, \"no rows in result set\")\n\t// ErrTooManyRows occurs when more rows than expected are returned.\n\tErrTooManyRows = errors.New(\"too many rows in result set\")\n)\n\nfunc newProxyErr(background error, msg string) error {\n\treturn &proxyError{\n\t\tmsg:        msg,\n\t\tbackground: background,\n\t}\n}\n\ntype proxyError struct {\n\tmsg        string\n\tbackground error\n}\n\nfunc (err *proxyError) Error() string { return err.msg }\n\nfunc (err *proxyError) Unwrap() error { return err.background }\n\nvar (\n\terrDisabledStatementCache   = fmt.Errorf(\"cannot use QueryExecModeCacheStatement with disabled statement cache\")\n\terrDisabledDescriptionCache = fmt.Errorf(\"cannot use QueryExecModeCacheDescribe with disabled description cache\")\n)\n\n// Connect establishes a connection with a PostgreSQL server with a connection string. See\n// pgconn.Connect for details.\nfunc Connect(ctx context.Context, connString string) (*Conn, error) {\n\tconnConfig, err := ParseConfig(connString)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn connect(ctx, connConfig)\n}\n\n// ConnectWithOptions behaves exactly like Connect with the addition of options. At the present options is only used to\n// provide a GetSSLPassword function.\nfunc ConnectWithOptions(ctx context.Context, connString string, options ParseConfigOptions) (*Conn, error) {\n\tconnConfig, err := ParseConfigWithOptions(connString, options)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn connect(ctx, connConfig)\n}\n\n// ConnectConfig establishes a connection with a PostgreSQL server with a configuration struct.\n// connConfig must have been created by ParseConfig.\nfunc ConnectConfig(ctx context.Context, connConfig *ConnConfig) (*Conn, error) {\n\t// In general this improves safety. In particular avoid the config.Config.OnNotification mutation from affecting other\n\t// connections with the same config. See https://github.com/jackc/pgx/issues/618.\n\tconnConfig = connConfig.Copy()\n\n\treturn connect(ctx, connConfig)\n}\n\n// ParseConfigWithOptions behaves exactly as ParseConfig does with the addition of options. At the present options is\n// only used to provide a GetSSLPassword function.\nfunc ParseConfigWithOptions(connString string, options ParseConfigOptions) (*ConnConfig, error) {\n\tconfig, err := pgconn.ParseConfigWithOptions(connString, options.ParseConfigOptions)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tstatementCacheCapacity := 512\n\tif s, ok := config.RuntimeParams[\"statement_cache_capacity\"]; ok {\n\t\tdelete(config.RuntimeParams, \"statement_cache_capacity\")\n\t\tn, err := strconv.ParseInt(s, 10, 32)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"cannot parse statement_cache_capacity: %w\", err)\n\t\t}\n\t\tstatementCacheCapacity = int(n)\n\t}\n\n\tdescriptionCacheCapacity := 512\n\tif s, ok := config.RuntimeParams[\"description_cache_capacity\"]; ok {\n\t\tdelete(config.RuntimeParams, \"description_cache_capacity\")\n\t\tn, err := strconv.ParseInt(s, 10, 32)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"cannot parse description_cache_capacity: %w\", err)\n\t\t}\n\t\tdescriptionCacheCapacity = int(n)\n\t}\n\n\tdefaultQueryExecMode := QueryExecModeCacheStatement\n\tif s, ok := config.RuntimeParams[\"default_query_exec_mode\"]; ok {\n\t\tdelete(config.RuntimeParams, \"default_query_exec_mode\")\n\t\tswitch s {\n\t\tcase \"cache_statement\":\n\t\t\tdefaultQueryExecMode = QueryExecModeCacheStatement\n\t\tcase \"cache_describe\":\n\t\t\tdefaultQueryExecMode = QueryExecModeCacheDescribe\n\t\tcase \"describe_exec\":\n\t\t\tdefaultQueryExecMode = QueryExecModeDescribeExec\n\t\tcase \"exec\":\n\t\t\tdefaultQueryExecMode = QueryExecModeExec\n\t\tcase \"simple_protocol\":\n\t\t\tdefaultQueryExecMode = QueryExecModeSimpleProtocol\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"invalid default_query_exec_mode: %s\", s)\n\t\t}\n\t}\n\n\tconnConfig := &ConnConfig{\n\t\tConfig:                   *config,\n\t\tcreatedByParseConfig:     true,\n\t\tStatementCacheCapacity:   statementCacheCapacity,\n\t\tDescriptionCacheCapacity: descriptionCacheCapacity,\n\t\tDefaultQueryExecMode:     defaultQueryExecMode,\n\t\tconnString:               connString,\n\t}\n\n\treturn connConfig, nil\n}\n\n// ParseConfig creates a ConnConfig from a connection string. ParseConfig handles all options that [pgconn.ParseConfig]\n// does. In addition, it accepts the following options:\n//\n//   - default_query_exec_mode.\n//     Possible values: \"cache_statement\", \"cache_describe\", \"describe_exec\", \"exec\", and \"simple_protocol\". See\n//     QueryExecMode constant documentation for the meaning of these values. Default: \"cache_statement\".\n//\n//   - statement_cache_capacity.\n//     The maximum size of the statement cache used when executing a query with \"cache_statement\" query exec mode.\n//     Default: 512.\n//\n//   - description_cache_capacity.\n//     The maximum size of the description cache used when executing a query with \"cache_describe\" query exec mode.\n//     Default: 512.\nfunc ParseConfig(connString string) (*ConnConfig, error) {\n\treturn ParseConfigWithOptions(connString, ParseConfigOptions{})\n}\n\n// connect connects to a database. connect takes ownership of config. The caller must not use or access it again.\nfunc connect(ctx context.Context, config *ConnConfig) (c *Conn, err error) {\n\tif connectTracer, ok := config.Tracer.(ConnectTracer); ok {\n\t\tctx = connectTracer.TraceConnectStart(ctx, TraceConnectStartData{ConnConfig: config})\n\t\tdefer func() {\n\t\t\tconnectTracer.TraceConnectEnd(ctx, TraceConnectEndData{Conn: c, Err: err})\n\t\t}()\n\t}\n\n\t// Default values are set in ParseConfig. Enforce initial creation by ParseConfig rather than setting defaults from\n\t// zero values.\n\tif !config.createdByParseConfig {\n\t\tpanic(\"config must be created by ParseConfig\")\n\t}\n\n\tc = &Conn{\n\t\tconfig:      config,\n\t\ttypeMap:     pgtype.NewMap(),\n\t\tqueryTracer: config.Tracer,\n\t}\n\n\tif t, ok := c.queryTracer.(BatchTracer); ok {\n\t\tc.batchTracer = t\n\t}\n\tif t, ok := c.queryTracer.(CopyFromTracer); ok {\n\t\tc.copyFromTracer = t\n\t}\n\tif t, ok := c.queryTracer.(PrepareTracer); ok {\n\t\tc.prepareTracer = t\n\t}\n\n\t// Only install pgx notification system if no other callback handler is present.\n\tif config.Config.OnNotification == nil {\n\t\tconfig.Config.OnNotification = c.bufferNotifications\n\t}\n\n\tc.pgConn, err = pgconn.ConnectConfig(ctx, &config.Config)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tc.preparedStatements = make(map[string]*pgconn.StatementDescription)\n\tc.doneChan = make(chan struct{})\n\tc.closedChan = make(chan error)\n\tc.wbuf = make([]byte, 0, 1024)\n\n\tif c.config.StatementCacheCapacity > 0 {\n\t\tc.statementCache = stmtcache.NewLRUCache(c.config.StatementCacheCapacity)\n\t}\n\n\tif c.config.DescriptionCacheCapacity > 0 {\n\t\tc.descriptionCache = stmtcache.NewLRUCache(c.config.DescriptionCacheCapacity)\n\t}\n\n\treturn c, nil\n}\n\n// Close closes a connection. It is safe to call Close on an already closed\n// connection.\nfunc (c *Conn) Close(ctx context.Context) error {\n\tif c.IsClosed() {\n\t\treturn nil\n\t}\n\n\terr := c.pgConn.Close(ctx)\n\treturn err\n}\n\n// Prepare creates a prepared statement with name and sql. sql can contain placeholders for bound parameters. These\n// placeholders are referenced positionally as $1, $2, etc. name can be used instead of sql with Query, QueryRow, and\n// Exec to execute the statement. It can also be used with Batch.Queue.\n//\n// The underlying PostgreSQL identifier for the prepared statement will be name if name != sql or a digest of sql if\n// name == sql.\n//\n// Prepare is idempotent; i.e. it is safe to call Prepare multiple times with the same name and sql arguments. This\n// allows a code path to Prepare and Query/Exec without concern for if the statement has already been prepared.\nfunc (c *Conn) Prepare(ctx context.Context, name, sql string) (sd *pgconn.StatementDescription, err error) {\n\tif c.prepareTracer != nil {\n\t\tctx = c.prepareTracer.TracePrepareStart(ctx, c, TracePrepareStartData{Name: name, SQL: sql})\n\t}\n\n\tif name != \"\" {\n\t\tvar ok bool\n\t\tif sd, ok = c.preparedStatements[name]; ok && sd.SQL == sql {\n\t\t\tif c.prepareTracer != nil {\n\t\t\t\tc.prepareTracer.TracePrepareEnd(ctx, c, TracePrepareEndData{AlreadyPrepared: true})\n\t\t\t}\n\t\t\treturn sd, nil\n\t\t}\n\t}\n\n\tif c.prepareTracer != nil {\n\t\tdefer func() {\n\t\t\tc.prepareTracer.TracePrepareEnd(ctx, c, TracePrepareEndData{Err: err})\n\t\t}()\n\t}\n\n\tvar psName, psKey string\n\tif name == sql {\n\t\tdigest := sha256.Sum256([]byte(sql))\n\t\tpsName = \"stmt_\" + hex.EncodeToString(digest[0:24])\n\t\tpsKey = sql\n\t} else {\n\t\tpsName = name\n\t\tpsKey = name\n\t}\n\n\tsd, err = c.pgConn.Prepare(ctx, psName, sql, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif psKey != \"\" {\n\t\tc.preparedStatements[psKey] = sd\n\t}\n\n\treturn sd, nil\n}\n\n// Deallocate releases a prepared statement. Calling Deallocate on a non-existent prepared statement will succeed.\nfunc (c *Conn) Deallocate(ctx context.Context, name string) error {\n\tvar psName string\n\tsd := c.preparedStatements[name]\n\tif sd != nil {\n\t\tpsName = sd.Name\n\t} else {\n\t\tpsName = name\n\t}\n\n\terr := c.pgConn.Deallocate(ctx, psName)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif sd != nil {\n\t\tdelete(c.preparedStatements, name)\n\t}\n\n\treturn nil\n}\n\n// DeallocateAll releases all previously prepared statements from the server and client, where it also resets the statement and description cache.\nfunc (c *Conn) DeallocateAll(ctx context.Context) error {\n\tc.preparedStatements = map[string]*pgconn.StatementDescription{}\n\tif c.config.StatementCacheCapacity > 0 {\n\t\tc.statementCache = stmtcache.NewLRUCache(c.config.StatementCacheCapacity)\n\t}\n\tif c.config.DescriptionCacheCapacity > 0 {\n\t\tc.descriptionCache = stmtcache.NewLRUCache(c.config.DescriptionCacheCapacity)\n\t}\n\t_, err := c.pgConn.Exec(ctx, \"deallocate all\").ReadAll()\n\treturn err\n}\n\nfunc (c *Conn) bufferNotifications(_ *pgconn.PgConn, n *pgconn.Notification) {\n\tc.notifications = append(c.notifications, n)\n}\n\n// WaitForNotification waits for a PostgreSQL notification. It wraps the underlying pgconn notification system in a\n// slightly more convenient form.\nfunc (c *Conn) WaitForNotification(ctx context.Context) (*pgconn.Notification, error) {\n\tvar n *pgconn.Notification\n\n\t// Return already received notification immediately\n\tif len(c.notifications) > 0 {\n\t\tn = c.notifications[0]\n\t\tc.notifications = c.notifications[1:]\n\t\treturn n, nil\n\t}\n\n\terr := c.pgConn.WaitForNotification(ctx)\n\tif len(c.notifications) > 0 {\n\t\tn = c.notifications[0]\n\t\tc.notifications = c.notifications[1:]\n\t}\n\treturn n, err\n}\n\n// IsClosed reports if the connection has been closed.\nfunc (c *Conn) IsClosed() bool {\n\treturn c.pgConn.IsClosed()\n}\n\nfunc (c *Conn) die() {\n\tif c.IsClosed() {\n\t\treturn\n\t}\n\n\tctx, cancel := context.WithCancel(context.Background())\n\tcancel() // force immediate hard cancel\n\tc.pgConn.Close(ctx)\n}\n\nfunc quoteIdentifier(s string) string {\n\treturn `\"` + strings.ReplaceAll(s, `\"`, `\"\"`) + `\"`\n}\n\n// Ping delegates to the underlying *pgconn.PgConn.Ping.\nfunc (c *Conn) Ping(ctx context.Context) error {\n\treturn c.pgConn.Ping(ctx)\n}\n\n// PgConn returns the underlying *pgconn.PgConn. This is an escape hatch method that allows lower level access to the\n// PostgreSQL connection than pgx exposes.\n//\n// It is strongly recommended that the connection be idle (no in-progress queries) before the underlying *pgconn.PgConn\n// is used and the connection must be returned to the same state before any *pgx.Conn methods are again used.\nfunc (c *Conn) PgConn() *pgconn.PgConn { return c.pgConn }\n\n// TypeMap returns the connection info used for this connection.\nfunc (c *Conn) TypeMap() *pgtype.Map { return c.typeMap }\n\n// Config returns a copy of config that was used to establish this connection.\nfunc (c *Conn) Config() *ConnConfig { return c.config.Copy() }\n\n// Exec executes sql. sql can be either a prepared statement name or an SQL string. arguments should be referenced\n// positionally from the sql string as $1, $2, etc.\nfunc (c *Conn) Exec(ctx context.Context, sql string, arguments ...any) (pgconn.CommandTag, error) {\n\tif c.queryTracer != nil {\n\t\tctx = c.queryTracer.TraceQueryStart(ctx, c, TraceQueryStartData{SQL: sql, Args: arguments})\n\t}\n\n\tif err := c.deallocateInvalidatedCachedStatements(ctx); err != nil {\n\t\treturn pgconn.CommandTag{}, err\n\t}\n\n\tcommandTag, err := c.exec(ctx, sql, arguments...)\n\n\tif c.queryTracer != nil {\n\t\tc.queryTracer.TraceQueryEnd(ctx, c, TraceQueryEndData{CommandTag: commandTag, Err: err})\n\t}\n\n\treturn commandTag, err\n}\n\nfunc (c *Conn) exec(ctx context.Context, sql string, arguments ...any) (commandTag pgconn.CommandTag, err error) {\n\tmode := c.config.DefaultQueryExecMode\n\tvar queryRewriter QueryRewriter\n\noptionLoop:\n\tfor len(arguments) > 0 {\n\t\tswitch arg := arguments[0].(type) {\n\t\tcase QueryExecMode:\n\t\t\tmode = arg\n\t\t\targuments = arguments[1:]\n\t\tcase QueryRewriter:\n\t\t\tqueryRewriter = arg\n\t\t\targuments = arguments[1:]\n\t\tdefault:\n\t\t\tbreak optionLoop\n\t\t}\n\t}\n\n\tif queryRewriter != nil {\n\t\tsql, arguments, err = queryRewriter.RewriteQuery(ctx, c, sql, arguments)\n\t\tif err != nil {\n\t\t\treturn pgconn.CommandTag{}, fmt.Errorf(\"rewrite query failed: %w\", err)\n\t\t}\n\t}\n\n\t// Always use simple protocol when there are no arguments.\n\tif len(arguments) == 0 {\n\t\tmode = QueryExecModeSimpleProtocol\n\t}\n\n\tif sd, ok := c.preparedStatements[sql]; ok {\n\t\treturn c.execPrepared(ctx, sd, arguments)\n\t}\n\n\tswitch mode {\n\tcase QueryExecModeCacheStatement:\n\t\tif c.statementCache == nil {\n\t\t\treturn pgconn.CommandTag{}, errDisabledStatementCache\n\t\t}\n\t\tsd := c.statementCache.Get(sql)\n\t\tif sd == nil {\n\t\t\tsd, err = c.Prepare(ctx, stmtcache.StatementName(sql), sql)\n\t\t\tif err != nil {\n\t\t\t\treturn pgconn.CommandTag{}, err\n\t\t\t}\n\t\t\tc.statementCache.Put(sd)\n\t\t}\n\n\t\treturn c.execPrepared(ctx, sd, arguments)\n\tcase QueryExecModeCacheDescribe:\n\t\tif c.descriptionCache == nil {\n\t\t\treturn pgconn.CommandTag{}, errDisabledDescriptionCache\n\t\t}\n\t\tsd := c.descriptionCache.Get(sql)\n\t\tif sd == nil {\n\t\t\tsd, err = c.Prepare(ctx, \"\", sql)\n\t\t\tif err != nil {\n\t\t\t\treturn pgconn.CommandTag{}, err\n\t\t\t}\n\t\t\tc.descriptionCache.Put(sd)\n\t\t}\n\n\t\treturn c.execParams(ctx, sd, arguments)\n\tcase QueryExecModeDescribeExec:\n\t\tsd, err := c.Prepare(ctx, \"\", sql)\n\t\tif err != nil {\n\t\t\treturn pgconn.CommandTag{}, err\n\t\t}\n\t\treturn c.execPrepared(ctx, sd, arguments)\n\tcase QueryExecModeExec:\n\t\treturn c.execSQLParams(ctx, sql, arguments)\n\tcase QueryExecModeSimpleProtocol:\n\t\treturn c.execSimpleProtocol(ctx, sql, arguments)\n\tdefault:\n\t\treturn pgconn.CommandTag{}, fmt.Errorf(\"unknown QueryExecMode: %v\", mode)\n\t}\n}\n\nfunc (c *Conn) execSimpleProtocol(ctx context.Context, sql string, arguments []any) (commandTag pgconn.CommandTag, err error) {\n\tif len(arguments) > 0 {\n\t\tsql, err = c.sanitizeForSimpleQuery(sql, arguments...)\n\t\tif err != nil {\n\t\t\treturn pgconn.CommandTag{}, err\n\t\t}\n\t}\n\n\tmrr := c.pgConn.Exec(ctx, sql)\n\tfor mrr.NextResult() {\n\t\tcommandTag, _ = mrr.ResultReader().Close()\n\t}\n\terr = mrr.Close()\n\treturn commandTag, err\n}\n\nfunc (c *Conn) execParams(ctx context.Context, sd *pgconn.StatementDescription, arguments []any) (pgconn.CommandTag, error) {\n\terr := c.eqb.Build(c.typeMap, sd, arguments)\n\tif err != nil {\n\t\treturn pgconn.CommandTag{}, err\n\t}\n\n\tresult := c.pgConn.ExecParams(ctx, sd.SQL, c.eqb.ParamValues, sd.ParamOIDs, c.eqb.ParamFormats, c.eqb.ResultFormats).Read()\n\tc.eqb.reset() // Allow c.eqb internal memory to be GC'ed as soon as possible.\n\treturn result.CommandTag, result.Err\n}\n\nfunc (c *Conn) execPrepared(ctx context.Context, sd *pgconn.StatementDescription, arguments []any) (pgconn.CommandTag, error) {\n\terr := c.eqb.Build(c.typeMap, sd, arguments)\n\tif err != nil {\n\t\treturn pgconn.CommandTag{}, err\n\t}\n\n\tresult := c.pgConn.ExecPrepared(ctx, sd.Name, c.eqb.ParamValues, c.eqb.ParamFormats, c.eqb.ResultFormats).Read()\n\tc.eqb.reset() // Allow c.eqb internal memory to be GC'ed as soon as possible.\n\treturn result.CommandTag, result.Err\n}\n\nfunc (c *Conn) execSQLParams(ctx context.Context, sql string, args []any) (pgconn.CommandTag, error) {\n\terr := c.eqb.Build(c.typeMap, nil, args)\n\tif err != nil {\n\t\treturn pgconn.CommandTag{}, err\n\t}\n\n\tresult := c.pgConn.ExecParams(ctx, sql, c.eqb.ParamValues, nil, c.eqb.ParamFormats, c.eqb.ResultFormats).Read()\n\tc.eqb.reset() // Allow c.eqb internal memory to be GC'ed as soon as possible.\n\treturn result.CommandTag, result.Err\n}\n\nfunc (c *Conn) getRows(ctx context.Context, sql string, args []any) *baseRows {\n\tr := &baseRows{}\n\n\tr.ctx = ctx\n\tr.queryTracer = c.queryTracer\n\tr.typeMap = c.typeMap\n\tr.startTime = time.Now()\n\tr.sql = sql\n\tr.args = args\n\tr.conn = c\n\n\treturn r\n}\n\ntype QueryExecMode int32\n\nconst (\n\t_ QueryExecMode = iota\n\n\t// Automatically prepare and cache statements. This uses the extended protocol. Queries are executed in a single round\n\t// trip after the statement is cached. This is the default. If the database schema is modified or the search_path is\n\t// changed after a statement is cached then the first execution of a previously cached query may fail. e.g. If the\n\t// number of columns returned by a \"SELECT *\" changes or the type of a column is changed.\n\tQueryExecModeCacheStatement\n\n\t// Cache statement descriptions (i.e. argument and result types) and assume they do not change. This uses the extended\n\t// protocol. Queries are executed in a single round trip after the description is cached. If the database schema is\n\t// modified or the search_path is changed after a statement is cached then the first execution of a previously cached\n\t// query may fail. e.g. If the number of columns returned by a \"SELECT *\" changes or the type of a column is changed.\n\tQueryExecModeCacheDescribe\n\n\t// Get the statement description on every execution. This uses the extended protocol. Queries require two round trips\n\t// to execute. It does not use named prepared statements. But it does use the unnamed prepared statement to get the\n\t// statement description on the first round trip and then uses it to execute the query on the second round trip. This\n\t// may cause problems with connection poolers that switch the underlying connection between round trips. It is safe\n\t// even when the database schema is modified concurrently.\n\tQueryExecModeDescribeExec\n\n\t// Assume the PostgreSQL query parameter types based on the Go type of the arguments. This uses the extended protocol\n\t// with text formatted parameters and results. Queries are executed in a single round trip. Type mappings can be\n\t// registered with pgtype.Map.RegisterDefaultPgType. Queries will be rejected that have arguments that are\n\t// unregistered or ambiguous. e.g. A map[string]string may have the PostgreSQL type json or hstore. Modes that know\n\t// the PostgreSQL type can use a map[string]string directly as an argument. This mode cannot.\n\t//\n\t// On rare occasions user defined types may behave differently when encoded in the text format instead of the binary\n\t// format. For example, this could happen if a \"type RomanNumeral int32\" implements fmt.Stringer to format integers as\n\t// Roman numerals (e.g. 7 is VII). The binary format would properly encode the integer 7 as the binary value for 7.\n\t// But the text format would encode the integer 7 as the string \"VII\". As QueryExecModeExec uses the text format, it\n\t// is possible that changing query mode from another mode to QueryExecModeExec could change the behavior of the query.\n\t// This should not occur with types pgx supports directly and can be avoided by registering the types with\n\t// pgtype.Map.RegisterDefaultPgType and implementing the appropriate type interfaces. In the cas of RomanNumeral, it\n\t// should implement pgtype.Int64Valuer.\n\tQueryExecModeExec\n\n\t// Use the simple protocol. Assume the PostgreSQL query parameter types based on the Go type of the arguments. Queries\n\t// are executed in a single round trip. Type mappings can be registered with pgtype.Map.RegisterDefaultPgType. Queries\n\t// will be rejected that have arguments that are unregistered or ambiguous. e.g. A map[string]string may have the\n\t// PostgreSQL type json or hstore. Modes that know the PostgreSQL type can use a map[string]string directly as an\n\t// argument. This mode cannot.\n\t//\n\t// QueryExecModeSimpleProtocol should have the user application visible behavior as QueryExecModeExec. This includes\n\t// the warning regarding differences in text format and binary format encoding with user defined types. There may be\n\t// other minor exceptions such as behavior when multiple result returning queries are erroneously sent in a single\n\t// string.\n\t//\n\t// QueryExecModeSimpleProtocol uses client side parameter interpolation. All values are quoted and escaped. Prefer\n\t// QueryExecModeExec over QueryExecModeSimpleProtocol whenever possible. In general QueryExecModeSimpleProtocol should\n\t// only be used if connecting to a proxy server, connection pool server, or non-PostgreSQL server that does not\n\t// support the extended protocol.\n\tQueryExecModeSimpleProtocol\n)\n\nfunc (m QueryExecMode) String() string {\n\tswitch m {\n\tcase QueryExecModeCacheStatement:\n\t\treturn \"cache statement\"\n\tcase QueryExecModeCacheDescribe:\n\t\treturn \"cache describe\"\n\tcase QueryExecModeDescribeExec:\n\t\treturn \"describe exec\"\n\tcase QueryExecModeExec:\n\t\treturn \"exec\"\n\tcase QueryExecModeSimpleProtocol:\n\t\treturn \"simple protocol\"\n\tdefault:\n\t\treturn \"invalid\"\n\t}\n}\n\n// QueryResultFormats controls the result format (text=0, binary=1) of a query by result column position.\ntype QueryResultFormats []int16\n\n// QueryResultFormatsByOID controls the result format (text=0, binary=1) of a query by the result column OID.\ntype QueryResultFormatsByOID map[uint32]int16\n\n// QueryRewriter rewrites a query when used as the first arguments to a query method.\ntype QueryRewriter interface {\n\tRewriteQuery(ctx context.Context, conn *Conn, sql string, args []any) (newSQL string, newArgs []any, err error)\n}\n\n// Query sends a query to the server and returns a Rows to read the results. Only errors encountered sending the query\n// and initializing Rows will be returned. Err() on the returned Rows must be checked after the Rows is closed to\n// determine if the query executed successfully.\n//\n// The returned Rows must be closed before the connection can be used again. It is safe to attempt to read from the\n// returned Rows even if an error is returned. The error will be the available in rows.Err() after rows are closed. It\n// is allowed to ignore the error returned from Query and handle it in Rows.\n//\n// It is possible for a call of FieldDescriptions on the returned Rows to return nil even if the Query call did not\n// return an error.\n//\n// It is possible for a query to return one or more rows before encountering an error. In most cases the rows should be\n// collected before processing rather than processed while receiving each row. This avoids the possibility of the\n// application processing rows from a query that the server rejected. The CollectRows function is useful here.\n//\n// An implementor of QueryRewriter may be passed as the first element of args. It can rewrite the sql and change or\n// replace args. For example, NamedArgs is QueryRewriter that implements named arguments.\n//\n// For extra control over how the query is executed, the types QueryExecMode, QueryResultFormats, and\n// QueryResultFormatsByOID may be used as the first args to control exactly how the query is executed. This is rarely\n// needed. See the documentation for those types for details.\nfunc (c *Conn) Query(ctx context.Context, sql string, args ...any) (Rows, error) {\n\tif c.queryTracer != nil {\n\t\tctx = c.queryTracer.TraceQueryStart(ctx, c, TraceQueryStartData{SQL: sql, Args: args})\n\t}\n\n\tif err := c.deallocateInvalidatedCachedStatements(ctx); err != nil {\n\t\tif c.queryTracer != nil {\n\t\t\tc.queryTracer.TraceQueryEnd(ctx, c, TraceQueryEndData{Err: err})\n\t\t}\n\t\treturn &baseRows{err: err, closed: true}, err\n\t}\n\n\tvar resultFormats QueryResultFormats\n\tvar resultFormatsByOID QueryResultFormatsByOID\n\tmode := c.config.DefaultQueryExecMode\n\tvar queryRewriter QueryRewriter\n\noptionLoop:\n\tfor len(args) > 0 {\n\t\tswitch arg := args[0].(type) {\n\t\tcase QueryResultFormats:\n\t\t\tresultFormats = arg\n\t\t\targs = args[1:]\n\t\tcase QueryResultFormatsByOID:\n\t\t\tresultFormatsByOID = arg\n\t\t\targs = args[1:]\n\t\tcase QueryExecMode:\n\t\t\tmode = arg\n\t\t\targs = args[1:]\n\t\tcase QueryRewriter:\n\t\t\tqueryRewriter = arg\n\t\t\targs = args[1:]\n\t\tdefault:\n\t\t\tbreak optionLoop\n\t\t}\n\t}\n\n\tif queryRewriter != nil {\n\t\tvar err error\n\t\toriginalSQL := sql\n\t\toriginalArgs := args\n\t\tsql, args, err = queryRewriter.RewriteQuery(ctx, c, sql, args)\n\t\tif err != nil {\n\t\t\trows := c.getRows(ctx, originalSQL, originalArgs)\n\t\t\terr = fmt.Errorf(\"rewrite query failed: %w\", err)\n\t\t\trows.fatal(err)\n\t\t\treturn rows, err\n\t\t}\n\t}\n\n\t// Bypass any statement caching.\n\tif sql == \"\" {\n\t\tmode = QueryExecModeSimpleProtocol\n\t}\n\n\tc.eqb.reset()\n\trows := c.getRows(ctx, sql, args)\n\n\tvar err error\n\tsd, explicitPreparedStatement := c.preparedStatements[sql]\n\tif sd != nil || mode == QueryExecModeCacheStatement || mode == QueryExecModeCacheDescribe || mode == QueryExecModeDescribeExec {\n\t\tif sd == nil {\n\t\t\tsd, err = c.getStatementDescription(ctx, mode, sql)\n\t\t\tif err != nil {\n\t\t\t\trows.fatal(err)\n\t\t\t\treturn rows, err\n\t\t\t}\n\t\t}\n\n\t\tif len(sd.ParamOIDs) != len(args) {\n\t\t\trows.fatal(fmt.Errorf(\"expected %d arguments, got %d\", len(sd.ParamOIDs), len(args)))\n\t\t\treturn rows, rows.err\n\t\t}\n\n\t\trows.sql = sd.SQL\n\n\t\terr = c.eqb.Build(c.typeMap, sd, args)\n\t\tif err != nil {\n\t\t\trows.fatal(err)\n\t\t\treturn rows, rows.err\n\t\t}\n\n\t\tif resultFormatsByOID != nil {\n\t\t\tresultFormats = make([]int16, len(sd.Fields))\n\t\t\tfor i := range resultFormats {\n\t\t\t\tresultFormats[i] = resultFormatsByOID[uint32(sd.Fields[i].DataTypeOID)]\n\t\t\t}\n\t\t}\n\n\t\tif resultFormats == nil {\n\t\t\tresultFormats = c.eqb.ResultFormats\n\t\t}\n\n\t\tif !explicitPreparedStatement && mode == QueryExecModeCacheDescribe {\n\t\t\trows.resultReader = c.pgConn.ExecParams(ctx, sql, c.eqb.ParamValues, sd.ParamOIDs, c.eqb.ParamFormats, resultFormats)\n\t\t} else {\n\t\t\trows.resultReader = c.pgConn.ExecPrepared(ctx, sd.Name, c.eqb.ParamValues, c.eqb.ParamFormats, resultFormats)\n\t\t}\n\t} else if mode == QueryExecModeExec {\n\t\terr := c.eqb.Build(c.typeMap, nil, args)\n\t\tif err != nil {\n\t\t\trows.fatal(err)\n\t\t\treturn rows, rows.err\n\t\t}\n\n\t\trows.resultReader = c.pgConn.ExecParams(ctx, sql, c.eqb.ParamValues, nil, c.eqb.ParamFormats, c.eqb.ResultFormats)\n\t} else if mode == QueryExecModeSimpleProtocol {\n\t\tsql, err = c.sanitizeForSimpleQuery(sql, args...)\n\t\tif err != nil {\n\t\t\trows.fatal(err)\n\t\t\treturn rows, err\n\t\t}\n\n\t\tmrr := c.pgConn.Exec(ctx, sql)\n\t\tif mrr.NextResult() {\n\t\t\trows.resultReader = mrr.ResultReader()\n\t\t\trows.multiResultReader = mrr\n\t\t} else {\n\t\t\terr = mrr.Close()\n\t\t\trows.fatal(err)\n\t\t\treturn rows, err\n\t\t}\n\n\t\treturn rows, nil\n\t} else {\n\t\terr = fmt.Errorf(\"unknown QueryExecMode: %v\", mode)\n\t\trows.fatal(err)\n\t\treturn rows, rows.err\n\t}\n\n\tc.eqb.reset() // Allow c.eqb internal memory to be GC'ed as soon as possible.\n\n\treturn rows, rows.err\n}\n\n// getStatementDescription returns the statement description of the sql query\n// according to the given mode.\n//\n// If the mode is one that doesn't require to know the param and result OIDs\n// then nil is returned without error.\nfunc (c *Conn) getStatementDescription(\n\tctx context.Context,\n\tmode QueryExecMode,\n\tsql string,\n) (sd *pgconn.StatementDescription, err error) {\n\tswitch mode {\n\tcase QueryExecModeCacheStatement:\n\t\tif c.statementCache == nil {\n\t\t\treturn nil, errDisabledStatementCache\n\t\t}\n\t\tsd = c.statementCache.Get(sql)\n\t\tif sd == nil {\n\t\t\tsd, err = c.Prepare(ctx, stmtcache.StatementName(sql), sql)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tc.statementCache.Put(sd)\n\t\t}\n\tcase QueryExecModeCacheDescribe:\n\t\tif c.descriptionCache == nil {\n\t\t\treturn nil, errDisabledDescriptionCache\n\t\t}\n\t\tsd = c.descriptionCache.Get(sql)\n\t\tif sd == nil {\n\t\t\tsd, err = c.Prepare(ctx, \"\", sql)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tc.descriptionCache.Put(sd)\n\t\t}\n\tcase QueryExecModeDescribeExec:\n\t\treturn c.Prepare(ctx, \"\", sql)\n\t}\n\treturn sd, err\n}\n\n// QueryRow is a convenience wrapper over Query. Any error that occurs while\n// querying is deferred until calling Scan on the returned Row. That Row will\n// error with ErrNoRows if no rows are returned.\nfunc (c *Conn) QueryRow(ctx context.Context, sql string, args ...any) Row {\n\trows, _ := c.Query(ctx, sql, args...)\n\treturn (*connRow)(rows.(*baseRows))\n}\n\n// SendBatch sends all queued queries to the server at once. All queries are run in an implicit transaction unless\n// explicit transaction control statements are executed. The returned BatchResults must be closed before the connection\n// is used again.\n//\n// Depending on the QueryExecMode, all queries may be prepared before any are executed. This means that creating a table\n// and using it in a subsequent query in the same batch can fail.\nfunc (c *Conn) SendBatch(ctx context.Context, b *Batch) (br BatchResults) {\n\tif c.batchTracer != nil {\n\t\tctx = c.batchTracer.TraceBatchStart(ctx, c, TraceBatchStartData{Batch: b})\n\t\tdefer func() {\n\t\t\terr := br.(interface{ earlyError() error }).earlyError()\n\t\t\tif err != nil {\n\t\t\t\tc.batchTracer.TraceBatchEnd(ctx, c, TraceBatchEndData{Err: err})\n\t\t\t}\n\t\t}()\n\t}\n\n\tif err := c.deallocateInvalidatedCachedStatements(ctx); err != nil {\n\t\treturn &batchResults{ctx: ctx, conn: c, err: err}\n\t}\n\n\tfor _, bi := range b.QueuedQueries {\n\t\tvar queryRewriter QueryRewriter\n\t\tsql := bi.SQL\n\t\targuments := bi.Arguments\n\n\toptionLoop:\n\t\tfor len(arguments) > 0 {\n\t\t\t// Update Batch.Queue function comment when additional options are implemented\n\t\t\tswitch arg := arguments[0].(type) {\n\t\t\tcase QueryRewriter:\n\t\t\t\tqueryRewriter = arg\n\t\t\t\targuments = arguments[1:]\n\t\t\tdefault:\n\t\t\t\tbreak optionLoop\n\t\t\t}\n\t\t}\n\n\t\tif queryRewriter != nil {\n\t\t\tvar err error\n\t\t\tsql, arguments, err = queryRewriter.RewriteQuery(ctx, c, sql, arguments)\n\t\t\tif err != nil {\n\t\t\t\treturn &batchResults{ctx: ctx, conn: c, err: fmt.Errorf(\"rewrite query failed: %w\", err)}\n\t\t\t}\n\t\t}\n\n\t\tbi.SQL = sql\n\t\tbi.Arguments = arguments\n\t}\n\n\t// TODO: changing mode per batch? Update Batch.Queue function comment when implemented\n\tmode := c.config.DefaultQueryExecMode\n\tif mode == QueryExecModeSimpleProtocol {\n\t\treturn c.sendBatchQueryExecModeSimpleProtocol(ctx, b)\n\t}\n\n\t// All other modes use extended protocol and thus can use prepared statements.\n\tfor _, bi := range b.QueuedQueries {\n\t\tif sd, ok := c.preparedStatements[bi.SQL]; ok {\n\t\t\tbi.sd = sd\n\t\t}\n\t}\n\n\tswitch mode {\n\tcase QueryExecModeExec:\n\t\treturn c.sendBatchQueryExecModeExec(ctx, b)\n\tcase QueryExecModeCacheStatement:\n\t\treturn c.sendBatchQueryExecModeCacheStatement(ctx, b)\n\tcase QueryExecModeCacheDescribe:\n\t\treturn c.sendBatchQueryExecModeCacheDescribe(ctx, b)\n\tcase QueryExecModeDescribeExec:\n\t\treturn c.sendBatchQueryExecModeDescribeExec(ctx, b)\n\tdefault:\n\t\tpanic(\"unknown QueryExecMode\")\n\t}\n}\n\nfunc (c *Conn) sendBatchQueryExecModeSimpleProtocol(ctx context.Context, b *Batch) *batchResults {\n\tvar sb strings.Builder\n\tfor i, bi := range b.QueuedQueries {\n\t\tif i > 0 {\n\t\t\tsb.WriteByte(';')\n\t\t}\n\t\tsql, err := c.sanitizeForSimpleQuery(bi.SQL, bi.Arguments...)\n\t\tif err != nil {\n\t\t\treturn &batchResults{ctx: ctx, conn: c, err: err}\n\t\t}\n\t\tsb.WriteString(sql)\n\t}\n\tmrr := c.pgConn.Exec(ctx, sb.String())\n\treturn &batchResults{\n\t\tctx:   ctx,\n\t\tconn:  c,\n\t\tmrr:   mrr,\n\t\tb:     b,\n\t\tqqIdx: 0,\n\t}\n}\n\nfunc (c *Conn) sendBatchQueryExecModeExec(ctx context.Context, b *Batch) *batchResults {\n\tbatch := &pgconn.Batch{}\n\n\tfor _, bi := range b.QueuedQueries {\n\t\tsd := bi.sd\n\t\tif sd != nil {\n\t\t\terr := c.eqb.Build(c.typeMap, sd, bi.Arguments)\n\t\t\tif err != nil {\n\t\t\t\treturn &batchResults{ctx: ctx, conn: c, err: err}\n\t\t\t}\n\n\t\t\tbatch.ExecPrepared(sd.Name, c.eqb.ParamValues, c.eqb.ParamFormats, c.eqb.ResultFormats)\n\t\t} else {\n\t\t\terr := c.eqb.Build(c.typeMap, nil, bi.Arguments)\n\t\t\tif err != nil {\n\t\t\t\treturn &batchResults{ctx: ctx, conn: c, err: err}\n\t\t\t}\n\t\t\tbatch.ExecParams(bi.SQL, c.eqb.ParamValues, nil, c.eqb.ParamFormats, c.eqb.ResultFormats)\n\t\t}\n\t}\n\n\tc.eqb.reset() // Allow c.eqb internal memory to be GC'ed as soon as possible.\n\n\tmrr := c.pgConn.ExecBatch(ctx, batch)\n\n\treturn &batchResults{\n\t\tctx:   ctx,\n\t\tconn:  c,\n\t\tmrr:   mrr,\n\t\tb:     b,\n\t\tqqIdx: 0,\n\t}\n}\n\nfunc (c *Conn) sendBatchQueryExecModeCacheStatement(ctx context.Context, b *Batch) (pbr *pipelineBatchResults) {\n\tif c.statementCache == nil {\n\t\treturn &pipelineBatchResults{ctx: ctx, conn: c, err: errDisabledStatementCache, closed: true}\n\t}\n\n\tdistinctNewQueries := []*pgconn.StatementDescription{}\n\tdistinctNewQueriesIdxMap := make(map[string]int)\n\n\tfor _, bi := range b.QueuedQueries {\n\t\tif bi.sd == nil {\n\t\t\tsd := c.statementCache.Get(bi.SQL)\n\t\t\tif sd != nil {\n\t\t\t\tbi.sd = sd\n\t\t\t} else {\n\t\t\t\tif idx, present := distinctNewQueriesIdxMap[bi.SQL]; present {\n\t\t\t\t\tbi.sd = distinctNewQueries[idx]\n\t\t\t\t} else {\n\t\t\t\t\tsd = &pgconn.StatementDescription{\n\t\t\t\t\t\tName: stmtcache.StatementName(bi.SQL),\n\t\t\t\t\t\tSQL:  bi.SQL,\n\t\t\t\t\t}\n\t\t\t\t\tdistinctNewQueriesIdxMap[sd.SQL] = len(distinctNewQueries)\n\t\t\t\t\tdistinctNewQueries = append(distinctNewQueries, sd)\n\t\t\t\t\tbi.sd = sd\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn c.sendBatchExtendedWithDescription(ctx, b, distinctNewQueries, c.statementCache)\n}\n\nfunc (c *Conn) sendBatchQueryExecModeCacheDescribe(ctx context.Context, b *Batch) (pbr *pipelineBatchResults) {\n\tif c.descriptionCache == nil {\n\t\treturn &pipelineBatchResults{ctx: ctx, conn: c, err: errDisabledDescriptionCache, closed: true}\n\t}\n\n\tdistinctNewQueries := []*pgconn.StatementDescription{}\n\tdistinctNewQueriesIdxMap := make(map[string]int)\n\n\tfor _, bi := range b.QueuedQueries {\n\t\tif bi.sd == nil {\n\t\t\tsd := c.descriptionCache.Get(bi.SQL)\n\t\t\tif sd != nil {\n\t\t\t\tbi.sd = sd\n\t\t\t} else {\n\t\t\t\tif idx, present := distinctNewQueriesIdxMap[bi.SQL]; present {\n\t\t\t\t\tbi.sd = distinctNewQueries[idx]\n\t\t\t\t} else {\n\t\t\t\t\tsd = &pgconn.StatementDescription{\n\t\t\t\t\t\tSQL: bi.SQL,\n\t\t\t\t\t}\n\t\t\t\t\tdistinctNewQueriesIdxMap[sd.SQL] = len(distinctNewQueries)\n\t\t\t\t\tdistinctNewQueries = append(distinctNewQueries, sd)\n\t\t\t\t\tbi.sd = sd\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn c.sendBatchExtendedWithDescription(ctx, b, distinctNewQueries, c.descriptionCache)\n}\n\nfunc (c *Conn) sendBatchQueryExecModeDescribeExec(ctx context.Context, b *Batch) (pbr *pipelineBatchResults) {\n\tdistinctNewQueries := []*pgconn.StatementDescription{}\n\tdistinctNewQueriesIdxMap := make(map[string]int)\n\n\tfor _, bi := range b.QueuedQueries {\n\t\tif bi.sd == nil {\n\t\t\tif idx, present := distinctNewQueriesIdxMap[bi.SQL]; present {\n\t\t\t\tbi.sd = distinctNewQueries[idx]\n\t\t\t} else {\n\t\t\t\tsd := &pgconn.StatementDescription{\n\t\t\t\t\tSQL: bi.SQL,\n\t\t\t\t}\n\t\t\t\tdistinctNewQueriesIdxMap[sd.SQL] = len(distinctNewQueries)\n\t\t\t\tdistinctNewQueries = append(distinctNewQueries, sd)\n\t\t\t\tbi.sd = sd\n\t\t\t}\n\t\t}\n\t}\n\n\treturn c.sendBatchExtendedWithDescription(ctx, b, distinctNewQueries, nil)\n}\n\nfunc (c *Conn) sendBatchExtendedWithDescription(ctx context.Context, b *Batch, distinctNewQueries []*pgconn.StatementDescription, sdCache stmtcache.Cache) (pbr *pipelineBatchResults) {\n\tpipeline := c.pgConn.StartPipeline(ctx)\n\tdefer func() {\n\t\tif pbr != nil && pbr.err != nil {\n\t\t\tpipeline.Close()\n\t\t}\n\t}()\n\n\t// Prepare any needed queries\n\tif len(distinctNewQueries) > 0 {\n\t\terr := func() (err error) {\n\t\t\tfor _, sd := range distinctNewQueries {\n\t\t\t\tpipeline.SendPrepare(sd.Name, sd.SQL, nil)\n\t\t\t}\n\n\t\t\t// Store all statements we are preparing into the cache. It's fine if it overflows because HandleInvalidated will\n\t\t\t// clean them up later.\n\t\t\tif sdCache != nil {\n\t\t\t\tfor _, sd := range distinctNewQueries {\n\t\t\t\t\tsdCache.Put(sd)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// If something goes wrong preparing the statements, we need to invalidate the cache entries we just added.\n\t\t\tdefer func() {\n\t\t\t\tif err != nil && sdCache != nil {\n\t\t\t\t\tfor _, sd := range distinctNewQueries {\n\t\t\t\t\t\tsdCache.Invalidate(sd.SQL)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\terr = pipeline.Sync()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tfor _, sd := range distinctNewQueries {\n\t\t\t\tresults, err := pipeline.GetResults()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\tresultSD, ok := results.(*pgconn.StatementDescription)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn fmt.Errorf(\"expected statement description, got %T\", results)\n\t\t\t\t}\n\n\t\t\t\t// Fill in the previously empty / pending statement descriptions.\n\t\t\t\tsd.ParamOIDs = resultSD.ParamOIDs\n\t\t\t\tsd.Fields = resultSD.Fields\n\t\t\t}\n\n\t\t\tresults, err := pipeline.GetResults()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t_, ok := results.(*pgconn.PipelineSync)\n\t\t\tif !ok {\n\t\t\t\treturn fmt.Errorf(\"expected sync, got %T\", results)\n\t\t\t}\n\n\t\t\treturn nil\n\t\t}()\n\t\tif err != nil {\n\t\t\treturn &pipelineBatchResults{ctx: ctx, conn: c, err: err, closed: true}\n\t\t}\n\t}\n\n\t// Queue the queries.\n\tfor _, bi := range b.QueuedQueries {\n\t\terr := c.eqb.Build(c.typeMap, bi.sd, bi.Arguments)\n\t\tif err != nil {\n\t\t\t// we wrap the error so we the user can understand which query failed inside the batch\n\t\t\terr = fmt.Errorf(\"error building query %s: %w\", bi.SQL, err)\n\t\t\treturn &pipelineBatchResults{ctx: ctx, conn: c, err: err, closed: true}\n\t\t}\n\n\t\tif bi.sd.Name == \"\" {\n\t\t\tpipeline.SendQueryParams(bi.sd.SQL, c.eqb.ParamValues, bi.sd.ParamOIDs, c.eqb.ParamFormats, c.eqb.ResultFormats)\n\t\t} else {\n\t\t\tpipeline.SendQueryPrepared(bi.sd.Name, c.eqb.ParamValues, c.eqb.ParamFormats, c.eqb.ResultFormats)\n\t\t}\n\t}\n\n\terr := pipeline.Sync()\n\tif err != nil {\n\t\treturn &pipelineBatchResults{ctx: ctx, conn: c, err: err, closed: true}\n\t}\n\n\treturn &pipelineBatchResults{\n\t\tctx:      ctx,\n\t\tconn:     c,\n\t\tpipeline: pipeline,\n\t\tb:        b,\n\t}\n}\n\nfunc (c *Conn) sanitizeForSimpleQuery(sql string, args ...any) (string, error) {\n\tif c.pgConn.ParameterStatus(\"standard_conforming_strings\") != \"on\" {\n\t\treturn \"\", errors.New(\"simple protocol queries must be run with standard_conforming_strings=on\")\n\t}\n\n\tif c.pgConn.ParameterStatus(\"client_encoding\") != \"UTF8\" {\n\t\treturn \"\", errors.New(\"simple protocol queries must be run with client_encoding=UTF8\")\n\t}\n\n\tvar err error\n\tvalueArgs := make([]any, len(args))\n\tfor i, a := range args {\n\t\tvalueArgs[i], err = convertSimpleArgument(c.typeMap, a)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t}\n\n\treturn sanitize.SanitizeSQL(sql, valueArgs...)\n}\n\n// LoadType inspects the database for typeName and produces a pgtype.Type suitable for registration. typeName must be\n// the name of a type where the underlying type(s) is already understood by pgx. It is for derived types. In particular,\n// typeName must be one of the following:\n//   - An array type name of a type that is already registered. e.g. \"_foo\" when \"foo\" is registered.\n//   - A composite type name where all field types are already registered.\n//   - A domain type name where the base type is already registered.\n//   - An enum type name.\n//   - A range type name where the element type is already registered.\n//   - A multirange type name where the element type is already registered.\nfunc (c *Conn) LoadType(ctx context.Context, typeName string) (*pgtype.Type, error) {\n\tvar oid uint32\n\n\terr := c.QueryRow(ctx, \"select $1::text::regtype::oid;\", typeName).Scan(&oid)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar typtype string\n\tvar typbasetype uint32\n\n\terr = c.QueryRow(ctx, \"select typtype::text, typbasetype from pg_type where oid=$1\", oid).Scan(&typtype, &typbasetype)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tswitch typtype {\n\tcase \"b\": // array\n\t\telementOID, err := c.getArrayElementOID(ctx, oid)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tdt, ok := c.TypeMap().TypeForOID(elementOID)\n\t\tif !ok {\n\t\t\treturn nil, errors.New(\"array element OID not registered\")\n\t\t}\n\n\t\treturn &pgtype.Type{Name: typeName, OID: oid, Codec: &pgtype.ArrayCodec{ElementType: dt}}, nil\n\tcase \"c\": // composite\n\t\tfields, err := c.getCompositeFields(ctx, oid)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\treturn &pgtype.Type{Name: typeName, OID: oid, Codec: &pgtype.CompositeCodec{Fields: fields}}, nil\n\tcase \"d\": // domain\n\t\tdt, ok := c.TypeMap().TypeForOID(typbasetype)\n\t\tif !ok {\n\t\t\treturn nil, errors.New(\"domain base type OID not registered\")\n\t\t}\n\n\t\treturn &pgtype.Type{Name: typeName, OID: oid, Codec: dt.Codec}, nil\n\tcase \"e\": // enum\n\t\treturn &pgtype.Type{Name: typeName, OID: oid, Codec: &pgtype.EnumCodec{}}, nil\n\tcase \"r\": // range\n\t\telementOID, err := c.getRangeElementOID(ctx, oid)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tdt, ok := c.TypeMap().TypeForOID(elementOID)\n\t\tif !ok {\n\t\t\treturn nil, errors.New(\"range element OID not registered\")\n\t\t}\n\n\t\treturn &pgtype.Type{Name: typeName, OID: oid, Codec: &pgtype.RangeCodec{ElementType: dt}}, nil\n\tcase \"m\": // multirange\n\t\telementOID, err := c.getMultiRangeElementOID(ctx, oid)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tdt, ok := c.TypeMap().TypeForOID(elementOID)\n\t\tif !ok {\n\t\t\treturn nil, errors.New(\"multirange element OID not registered\")\n\t\t}\n\n\t\treturn &pgtype.Type{Name: typeName, OID: oid, Codec: &pgtype.MultirangeCodec{ElementType: dt}}, nil\n\tdefault:\n\t\treturn &pgtype.Type{}, errors.New(\"unknown typtype\")\n\t}\n}\n\nfunc (c *Conn) getArrayElementOID(ctx context.Context, oid uint32) (uint32, error) {\n\tvar typelem uint32\n\n\terr := c.QueryRow(ctx, \"select typelem from pg_type where oid=$1\", oid).Scan(&typelem)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn typelem, nil\n}\n\nfunc (c *Conn) getRangeElementOID(ctx context.Context, oid uint32) (uint32, error) {\n\tvar typelem uint32\n\n\terr := c.QueryRow(ctx, \"select rngsubtype from pg_range where rngtypid=$1\", oid).Scan(&typelem)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn typelem, nil\n}\n\nfunc (c *Conn) getMultiRangeElementOID(ctx context.Context, oid uint32) (uint32, error) {\n\tvar typelem uint32\n\n\terr := c.QueryRow(ctx, \"select rngtypid from pg_range where rngmultitypid=$1\", oid).Scan(&typelem)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn typelem, nil\n}\n\nfunc (c *Conn) getCompositeFields(ctx context.Context, oid uint32) ([]pgtype.CompositeCodecField, error) {\n\tvar typrelid uint32\n\n\terr := c.QueryRow(ctx, \"select typrelid from pg_type where oid=$1\", oid).Scan(&typrelid)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar fields []pgtype.CompositeCodecField\n\tvar fieldName string\n\tvar fieldOID uint32\n\trows, _ := c.Query(ctx, `select attname, atttypid\nfrom pg_attribute\nwhere attrelid=$1\n\tand not attisdropped\n\tand attnum > 0\norder by attnum`,\n\t\ttyprelid,\n\t)\n\t_, err = ForEachRow(rows, []any{&fieldName, &fieldOID}, func() error {\n\t\tdt, ok := c.TypeMap().TypeForOID(fieldOID)\n\t\tif !ok {\n\t\t\treturn fmt.Errorf(\"unknown composite type field OID: %v\", fieldOID)\n\t\t}\n\t\tfields = append(fields, pgtype.CompositeCodecField{Name: fieldName, Type: dt})\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn fields, nil\n}\n\nfunc (c *Conn) deallocateInvalidatedCachedStatements(ctx context.Context) error {\n\tif txStatus := c.pgConn.TxStatus(); txStatus != 'I' && txStatus != 'T' {\n\t\treturn nil\n\t}\n\n\tif c.descriptionCache != nil {\n\t\tc.descriptionCache.RemoveInvalidated()\n\t}\n\n\tvar invalidatedStatements []*pgconn.StatementDescription\n\tif c.statementCache != nil {\n\t\tinvalidatedStatements = c.statementCache.GetInvalidated()\n\t}\n\n\tif len(invalidatedStatements) == 0 {\n\t\treturn nil\n\t}\n\n\tpipeline := c.pgConn.StartPipeline(ctx)\n\tdefer pipeline.Close()\n\n\tfor _, sd := range invalidatedStatements {\n\t\tpipeline.SendDeallocate(sd.Name)\n\t}\n\n\terr := pipeline.Sync()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to deallocate cached statement(s): %w\", err)\n\t}\n\n\terr = pipeline.Close()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to deallocate cached statement(s): %w\", err)\n\t}\n\n\tc.statementCache.RemoveInvalidated()\n\tfor _, sd := range invalidatedStatements {\n\t\tdelete(c.preparedStatements, sd.Name)\n\t}\n\n\treturn nil\n}\n"
        },
        {
          "name": "conn_internal_test.go",
          "type": "blob",
          "size": 1.4951171875,
          "content": "package pgx\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc mustParseConfig(t testing.TB, connString string) *ConnConfig {\n\tconfig, err := ParseConfig(connString)\n\trequire.Nil(t, err)\n\treturn config\n}\n\nfunc mustConnect(t testing.TB, config *ConnConfig) *Conn {\n\tconn, err := ConnectConfig(context.Background(), config)\n\tif err != nil {\n\t\tt.Fatalf(\"Unable to establish connection: %v\", err)\n\t}\n\treturn conn\n}\n\n// Ensures the connection limits the size of its cached objects.\n// This test examines the internals of *Conn so must be in the same package.\nfunc TestStmtCacheSizeLimit(t *testing.T) {\n\tconst cacheLimit = 16\n\n\tconnConfig := mustParseConfig(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconnConfig.StatementCacheCapacity = cacheLimit\n\tconn := mustConnect(t, connConfig)\n\tdefer func() {\n\t\terr := conn.Close(context.Background())\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t// run a set of unique queries that should overflow the cache\n\tctx := context.Background()\n\tfor i := 0; i < cacheLimit*2; i++ {\n\t\tuniqueString := fmt.Sprintf(\"unique %d\", i)\n\t\tuniqueSQL := fmt.Sprintf(\"select '%s'\", uniqueString)\n\t\tvar output string\n\t\terr := conn.QueryRow(ctx, uniqueSQL).Scan(&output)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, uniqueString, output)\n\t}\n\t// preparedStatements contains cacheLimit+1 because deallocation happens before the query\n\tassert.Len(t, conn.preparedStatements, cacheLimit+1)\n\tassert.Equal(t, cacheLimit, conn.statementCache.Len())\n}\n"
        },
        {
          "name": "conn_test.go",
          "type": "blob",
          "size": 42.37890625,
          "content": "package pgx_test\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"os\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgtype\"\n\t\"github.com/jackc/pgx/v5/pgxtest\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestCrateDBConnect(t *testing.T) {\n\tt.Parallel()\n\n\tconnString := os.Getenv(\"PGX_TEST_CRATEDB_CONN_STRING\")\n\tif connString == \"\" {\n\t\tt.Skipf(\"Skipping due to missing environment variable %v\", \"PGX_TEST_CRATEDB_CONN_STRING\")\n\t}\n\n\tconn, err := pgx.Connect(context.Background(), connString)\n\trequire.Nil(t, err)\n\tdefer closeConn(t, conn)\n\n\tassert.Equal(t, connString, conn.Config().ConnString())\n\n\tvar result int\n\terr = conn.QueryRow(context.Background(), \"select 1 +1\").Scan(&result)\n\tif err != nil {\n\t\tt.Fatalf(\"QueryRow Scan unexpectedly failed: %v\", err)\n\t}\n\tif result != 2 {\n\t\tt.Errorf(\"bad result: %d\", result)\n\t}\n}\n\nfunc TestConnect(t *testing.T) {\n\tt.Parallel()\n\n\tconnString := os.Getenv(\"PGX_TEST_DATABASE\")\n\tconfig := mustParseConfig(t, connString)\n\n\tconn, err := pgx.ConnectConfig(context.Background(), config)\n\tif err != nil {\n\t\tt.Fatalf(\"Unable to establish connection: %v\", err)\n\t}\n\n\tassertConfigsEqual(t, config, conn.Config(), \"Conn.Config() returns original config\")\n\n\tvar currentDB string\n\terr = conn.QueryRow(context.Background(), \"select current_database()\").Scan(&currentDB)\n\tif err != nil {\n\t\tt.Fatalf(\"QueryRow Scan unexpectedly failed: %v\", err)\n\t}\n\tif currentDB != config.Config.Database {\n\t\tt.Errorf(\"Did not connect to specified database (%v)\", config.Config.Database)\n\t}\n\n\tvar user string\n\terr = conn.QueryRow(context.Background(), \"select current_user\").Scan(&user)\n\tif err != nil {\n\t\tt.Fatalf(\"QueryRow Scan unexpectedly failed: %v\", err)\n\t}\n\tif user != config.Config.User {\n\t\tt.Errorf(\"Did not connect as specified user (%v)\", config.Config.User)\n\t}\n\n\terr = conn.Close(context.Background())\n\tif err != nil {\n\t\tt.Fatal(\"Unable to close connection\")\n\t}\n}\n\nfunc TestConnectWithPreferSimpleProtocol(t *testing.T) {\n\tt.Parallel()\n\n\tconnConfig := mustParseConfig(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconnConfig.DefaultQueryExecMode = pgx.QueryExecModeSimpleProtocol\n\n\tconn := mustConnect(t, connConfig)\n\tdefer closeConn(t, conn)\n\n\t// If simple protocol is used we should be able to correctly scan the result\n\t// into a pgtype.Text as the integer will have been encoded in text.\n\n\tvar s pgtype.Text\n\terr := conn.QueryRow(context.Background(), \"select $1::int4\", 42).Scan(&s)\n\trequire.NoError(t, err)\n\trequire.Equal(t, pgtype.Text{String: \"42\", Valid: true}, s)\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnectConfigRequiresConnConfigFromParseConfig(t *testing.T) {\n\tconfig := &pgx.ConnConfig{}\n\trequire.PanicsWithValue(t, \"config must be created by ParseConfig\", func() {\n\t\tpgx.ConnectConfig(context.Background(), config)\n\t})\n}\n\nfunc TestConfigContainsConnStr(t *testing.T) {\n\tconnStr := os.Getenv(\"PGX_TEST_DATABASE\")\n\tconfig, err := pgx.ParseConfig(connStr)\n\trequire.NoError(t, err)\n\tassert.Equal(t, connStr, config.ConnString())\n}\n\nfunc TestConfigCopyReturnsEqualConfig(t *testing.T) {\n\tconnString := \"postgres://jack:secret@localhost:5432/mydb?application_name=pgxtest&search_path=myschema&connect_timeout=5\"\n\toriginal, err := pgx.ParseConfig(connString)\n\trequire.NoError(t, err)\n\n\tcopied := original.Copy()\n\tassertConfigsEqual(t, original, copied, t.Name())\n}\n\nfunc TestConfigCopyCanBeUsedToConnect(t *testing.T) {\n\tconnString := os.Getenv(\"PGX_TEST_DATABASE\")\n\toriginal, err := pgx.ParseConfig(connString)\n\trequire.NoError(t, err)\n\n\tcopied := original.Copy()\n\tassert.NotPanics(t, func() {\n\t\t_, err = pgx.ConnectConfig(context.Background(), copied)\n\t})\n\tassert.NoError(t, err)\n}\n\nfunc TestParseConfigExtractsStatementCacheOptions(t *testing.T) {\n\tt.Parallel()\n\n\tconfig, err := pgx.ParseConfig(\"statement_cache_capacity=0\")\n\trequire.NoError(t, err)\n\trequire.EqualValues(t, 0, config.StatementCacheCapacity)\n\n\tconfig, err = pgx.ParseConfig(\"statement_cache_capacity=42\")\n\trequire.NoError(t, err)\n\trequire.EqualValues(t, 42, config.StatementCacheCapacity)\n\n\tconfig, err = pgx.ParseConfig(\"description_cache_capacity=0\")\n\trequire.NoError(t, err)\n\trequire.EqualValues(t, 0, config.DescriptionCacheCapacity)\n\n\tconfig, err = pgx.ParseConfig(\"description_cache_capacity=42\")\n\trequire.NoError(t, err)\n\trequire.EqualValues(t, 42, config.DescriptionCacheCapacity)\n\n\t//\tdefault_query_exec_mode\n\t//\t\tPossible values: \"cache_statement\", \"cache_describe\", \"describe_exec\", \"exec\", and \"simple_protocol\". See\n\n\tconfig, err = pgx.ParseConfig(\"default_query_exec_mode=cache_statement\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, pgx.QueryExecModeCacheStatement, config.DefaultQueryExecMode)\n\n\tconfig, err = pgx.ParseConfig(\"default_query_exec_mode=cache_describe\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, pgx.QueryExecModeCacheDescribe, config.DefaultQueryExecMode)\n\n\tconfig, err = pgx.ParseConfig(\"default_query_exec_mode=describe_exec\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, pgx.QueryExecModeDescribeExec, config.DefaultQueryExecMode)\n\n\tconfig, err = pgx.ParseConfig(\"default_query_exec_mode=exec\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, pgx.QueryExecModeExec, config.DefaultQueryExecMode)\n\n\tconfig, err = pgx.ParseConfig(\"default_query_exec_mode=simple_protocol\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, pgx.QueryExecModeSimpleProtocol, config.DefaultQueryExecMode)\n}\n\nfunc TestParseConfigExtractsDefaultQueryExecMode(t *testing.T) {\n\tt.Parallel()\n\n\tfor _, tt := range []struct {\n\t\tconnString           string\n\t\tdefaultQueryExecMode pgx.QueryExecMode\n\t}{\n\t\t{\"\", pgx.QueryExecModeCacheStatement},\n\t\t{\"default_query_exec_mode=cache_statement\", pgx.QueryExecModeCacheStatement},\n\t\t{\"default_query_exec_mode=cache_describe\", pgx.QueryExecModeCacheDescribe},\n\t\t{\"default_query_exec_mode=describe_exec\", pgx.QueryExecModeDescribeExec},\n\t\t{\"default_query_exec_mode=exec\", pgx.QueryExecModeExec},\n\t\t{\"default_query_exec_mode=simple_protocol\", pgx.QueryExecModeSimpleProtocol},\n\t} {\n\t\tconfig, err := pgx.ParseConfig(tt.connString)\n\t\trequire.NoError(t, err)\n\t\trequire.Equalf(t, tt.defaultQueryExecMode, config.DefaultQueryExecMode, \"connString: `%s`\", tt.connString)\n\t\trequire.Empty(t, config.RuntimeParams[\"default_query_exec_mode\"])\n\t}\n}\n\nfunc TestParseConfigErrors(t *testing.T) {\n\tt.Parallel()\n\n\tfor _, tt := range []struct {\n\t\tconnString           string\n\t\texpectedErrSubstring string\n\t}{\n\t\t{\"default_query_exec_mode=does_not_exist\", \"does_not_exist\"},\n\t} {\n\t\tconfig, err := pgx.ParseConfig(tt.connString)\n\t\trequire.Nil(t, config)\n\t\trequire.ErrorContains(t, err, tt.expectedErrSubstring)\n\t}\n}\n\nfunc TestExec(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tif results := mustExec(t, conn, \"create temporary table foo(id integer primary key);\"); results.String() != \"CREATE TABLE\" {\n\t\t\tt.Error(\"Unexpected results from Exec\")\n\t\t}\n\n\t\t// Accept parameters\n\t\tif results := mustExec(t, conn, \"insert into foo(id) values($1)\", 1); results.String() != \"INSERT 0 1\" {\n\t\t\tt.Errorf(\"Unexpected results from Exec: %v\", results)\n\t\t}\n\n\t\tif results := mustExec(t, conn, \"drop table foo;\"); results.String() != \"DROP TABLE\" {\n\t\t\tt.Error(\"Unexpected results from Exec\")\n\t\t}\n\n\t\t// Multiple statements can be executed -- last command tag is returned\n\t\tif results := mustExec(t, conn, \"create temporary table foo(id serial primary key); drop table foo;\"); results.String() != \"DROP TABLE\" {\n\t\t\tt.Error(\"Unexpected results from Exec\")\n\t\t}\n\n\t\t// Can execute longer SQL strings than sharedBufferSize\n\t\tif results := mustExec(t, conn, strings.Repeat(\"select 42; \", 1000)); results.String() != \"SELECT 1\" {\n\t\t\tt.Errorf(\"Unexpected results from Exec: %v\", results)\n\t\t}\n\n\t\t// Exec no-op which does not return a command tag\n\t\tif results := mustExec(t, conn, \"--;\"); results.String() != \"\" {\n\t\t\tt.Errorf(\"Unexpected results from Exec: %v\", results)\n\t\t}\n\t})\n}\n\ntype testQueryRewriter struct {\n\tsql  string\n\targs []any\n}\n\nfunc (qr *testQueryRewriter) RewriteQuery(ctx context.Context, conn *pgx.Conn, sql string, args []any) (newSQL string, newArgs []any, err error) {\n\treturn qr.sql, qr.args, nil\n}\n\nfunc TestExecWithQueryRewriter(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tqr := testQueryRewriter{sql: \"select $1::int\", args: []any{42}}\n\t\t_, err := conn.Exec(ctx, \"should be replaced\", &qr)\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestExecFailure(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tif _, err := conn.Exec(context.Background(), \"selct;\"); err == nil {\n\t\t\tt.Fatal(\"Expected SQL syntax error\")\n\t\t}\n\n\t\trows, _ := conn.Query(context.Background(), \"select 1\")\n\t\trows.Close()\n\t\tif rows.Err() != nil {\n\t\t\tt.Fatalf(\"Exec failure appears to have broken connection: %v\", rows.Err())\n\t\t}\n\t})\n}\n\nfunc TestExecFailureWithArguments(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\t_, err := conn.Exec(context.Background(), \"selct $1;\", 1)\n\t\tif err == nil {\n\t\t\tt.Fatal(\"Expected SQL syntax error\")\n\t\t}\n\t\tassert.False(t, pgconn.SafeToRetry(err))\n\n\t\t_, err = conn.Exec(context.Background(), \"select $1::varchar(1);\", \"1\", \"2\")\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestExecContextWithoutCancelation(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tctx, cancelFunc := context.WithCancel(ctx)\n\t\tdefer cancelFunc()\n\n\t\tcommandTag, err := conn.Exec(ctx, \"create temporary table foo(id integer primary key);\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif commandTag.String() != \"CREATE TABLE\" {\n\t\t\tt.Fatalf(\"Unexpected results from Exec: %v\", commandTag)\n\t\t}\n\t\tassert.False(t, pgconn.SafeToRetry(err))\n\t})\n}\n\nfunc TestExecContextFailureWithoutCancelation(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tctx, cancelFunc := context.WithCancel(ctx)\n\t\tdefer cancelFunc()\n\n\t\t_, err := conn.Exec(ctx, \"selct;\")\n\t\tif err == nil {\n\t\t\tt.Fatal(\"Expected SQL syntax error\")\n\t\t}\n\t\tassert.False(t, pgconn.SafeToRetry(err))\n\n\t\trows, _ := conn.Query(context.Background(), \"select 1\")\n\t\trows.Close()\n\t\tif rows.Err() != nil {\n\t\t\tt.Fatalf(\"ExecEx failure appears to have broken connection: %v\", rows.Err())\n\t\t}\n\t\tassert.False(t, pgconn.SafeToRetry(err))\n\t})\n}\n\nfunc TestExecContextFailureWithoutCancelationWithArguments(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tctx, cancelFunc := context.WithCancel(ctx)\n\t\tdefer cancelFunc()\n\n\t\t_, err := conn.Exec(ctx, \"selct $1;\", 1)\n\t\tif err == nil {\n\t\t\tt.Fatal(\"Expected SQL syntax error\")\n\t\t}\n\t\tassert.False(t, pgconn.SafeToRetry(err))\n\t})\n}\n\nfunc TestExecFailureCloseBefore(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tcloseConn(t, conn)\n\n\t_, err := conn.Exec(context.Background(), \"select 1\")\n\trequire.Error(t, err)\n\tassert.True(t, pgconn.SafeToRetry(err))\n}\n\nfunc TestExecPerQuerySimpleProtocol(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tctx, cancelFunc := context.WithCancel(context.Background())\n\tdefer cancelFunc()\n\n\tcommandTag, err := conn.Exec(ctx, \"create temporary table foo(name varchar primary key);\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif commandTag.String() != \"CREATE TABLE\" {\n\t\tt.Fatalf(\"Unexpected results from Exec: %v\", commandTag)\n\t}\n\n\tcommandTag, err = conn.Exec(ctx,\n\t\t\"insert into foo(name) values($1);\",\n\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\"bar'; drop table foo;--\",\n\t)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif commandTag.String() != \"INSERT 0 1\" {\n\t\tt.Fatalf(\"Unexpected results from Exec: %v\", commandTag)\n\t}\n\n}\n\nfunc TestPrepare(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\t_, err := conn.Prepare(context.Background(), \"test\", \"select $1::varchar\")\n\tif err != nil {\n\t\tt.Errorf(\"Unable to prepare statement: %v\", err)\n\t\treturn\n\t}\n\n\tvar s string\n\terr = conn.QueryRow(context.Background(), \"test\", \"hello\").Scan(&s)\n\tif err != nil {\n\t\tt.Errorf(\"Executing prepared statement failed: %v\", err)\n\t}\n\n\tif s != \"hello\" {\n\t\tt.Errorf(\"Prepared statement did not return expected value: %v\", s)\n\t}\n\n\terr = conn.Deallocate(context.Background(), \"test\")\n\tif err != nil {\n\t\tt.Errorf(\"conn.Deallocate failed: %v\", err)\n\t}\n\n\t// Create another prepared statement to ensure Deallocate left the connection\n\t// in a working state and that we can reuse the prepared statement name.\n\n\t_, err = conn.Prepare(context.Background(), \"test\", \"select $1::integer\")\n\tif err != nil {\n\t\tt.Errorf(\"Unable to prepare statement: %v\", err)\n\t\treturn\n\t}\n\n\tvar n int32\n\terr = conn.QueryRow(context.Background(), \"test\", int32(1)).Scan(&n)\n\tif err != nil {\n\t\tt.Errorf(\"Executing prepared statement failed: %v\", err)\n\t}\n\n\tif n != 1 {\n\t\tt.Errorf(\"Prepared statement did not return expected value: %v\", s)\n\t}\n\n\terr = conn.DeallocateAll(context.Background())\n\tif err != nil {\n\t\tt.Errorf(\"conn.Deallocate failed: %v\", err)\n\t}\n}\n\nfunc TestPrepareBadSQLFailure(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tif _, err := conn.Prepare(context.Background(), \"badSQL\", \"select foo\"); err == nil {\n\t\tt.Fatal(\"Prepare should have failed with syntax error\")\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestPrepareIdempotency(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tfor i := 0; i < 2; i++ {\n\t\t\t_, err := conn.Prepare(context.Background(), \"test\", \"select 42::integer\")\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"%d. Unable to prepare statement: %v\", i, err)\n\t\t\t}\n\n\t\t\tvar n int32\n\t\t\terr = conn.QueryRow(context.Background(), \"test\").Scan(&n)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"%d. Executing prepared statement failed: %v\", i, err)\n\t\t\t}\n\n\t\t\tif n != int32(42) {\n\t\t\t\tt.Errorf(\"%d. Prepared statement did not return expected value: %v\", i, n)\n\t\t\t}\n\t\t}\n\n\t\t_, err := conn.Prepare(context.Background(), \"test\", \"select 'fail'::varchar\")\n\t\tif err == nil {\n\t\t\tt.Fatalf(\"Prepare statement with same name but different SQL should have failed but it didn't\")\n\t\t\treturn\n\t\t}\n\t})\n}\n\nfunc TestPrepareStatementCacheModes(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\t_, err := conn.Prepare(context.Background(), \"test\", \"select $1::text\")\n\t\trequire.NoError(t, err)\n\n\t\tvar s string\n\t\terr = conn.QueryRow(context.Background(), \"test\", \"hello\").Scan(&s)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, \"hello\", s)\n\t})\n}\n\nfunc TestPrepareWithDigestedName(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tsql := \"select $1::text\"\n\t\tsd, err := conn.Prepare(ctx, sql, sql)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, \"stmt_2510cc7db17de3f42758a2a29c8b9ef8305d007b997ebdd6\", sd.Name)\n\n\t\tvar s string\n\t\terr = conn.QueryRow(ctx, sql, \"hello\").Scan(&s)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, \"hello\", s)\n\n\t\terr = conn.Deallocate(ctx, sql)\n\t\trequire.NoError(t, err)\n\t})\n}\n\n// https://github.com/jackc/pgx/pull/1795\nfunc TestDeallocateInAbortedTransaction(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttx, err := conn.Begin(ctx)\n\t\trequire.NoError(t, err)\n\n\t\tsql := \"select $1::text\"\n\t\tsd, err := tx.Prepare(ctx, sql, sql)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, \"stmt_2510cc7db17de3f42758a2a29c8b9ef8305d007b997ebdd6\", sd.Name)\n\n\t\tvar s string\n\t\terr = tx.QueryRow(ctx, sql, \"hello\").Scan(&s)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, \"hello\", s)\n\n\t\t_, err = tx.Exec(ctx, \"select 1/0\") // abort transaction with divide by zero error\n\t\trequire.Error(t, err)\n\n\t\terr = conn.Deallocate(ctx, sql)\n\t\trequire.NoError(t, err)\n\n\t\terr = tx.Rollback(ctx)\n\t\trequire.NoError(t, err)\n\n\t\tsd, err = conn.Prepare(ctx, sql, sql)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, \"stmt_2510cc7db17de3f42758a2a29c8b9ef8305d007b997ebdd6\", sd.Name)\n\t})\n}\n\nfunc TestDeallocateMissingPreparedStatementStillClearsFromPreparedStatementMap(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\t_, err := conn.Prepare(ctx, \"ps\", \"select $1::text\")\n\t\trequire.NoError(t, err)\n\n\t\t_, err = conn.Exec(ctx, \"deallocate ps\")\n\t\trequire.NoError(t, err)\n\n\t\terr = conn.Deallocate(ctx, \"ps\")\n\t\trequire.NoError(t, err)\n\n\t\t_, err = conn.Prepare(ctx, \"ps\", \"select $1::text, $2::text\")\n\t\trequire.NoError(t, err)\n\n\t\tvar s1, s2 string\n\t\terr = conn.QueryRow(ctx, \"ps\", \"hello\", \"world\").Scan(&s1, &s2)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, \"hello\", s1)\n\t\trequire.Equal(t, \"world\", s2)\n\t})\n}\n\nfunc TestListenNotify(t *testing.T) {\n\tt.Parallel()\n\n\tlistener := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, listener)\n\n\tif listener.PgConn().ParameterStatus(\"crdb_version\") != \"\" {\n\t\tt.Skip(\"Server does not support LISTEN / NOTIFY (https://github.com/cockroachdb/cockroach/issues/41522)\")\n\t}\n\n\tmustExec(t, listener, \"listen chat\")\n\n\tnotifier := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, notifier)\n\n\tmustExec(t, notifier, \"notify chat\")\n\n\t// when notification is waiting on the socket to be read\n\tnotification, err := listener.WaitForNotification(context.Background())\n\trequire.NoError(t, err)\n\tassert.Equal(t, \"chat\", notification.Channel)\n\n\t// when notification has already been read during previous query\n\tmustExec(t, notifier, \"notify chat\")\n\trows, _ := listener.Query(context.Background(), \"select 1\")\n\trows.Close()\n\trequire.NoError(t, rows.Err())\n\n\tctx, cancelFn := context.WithCancel(context.Background())\n\tcancelFn()\n\tnotification, err = listener.WaitForNotification(ctx)\n\trequire.NoError(t, err)\n\tassert.Equal(t, \"chat\", notification.Channel)\n\n\t// when timeout occurs\n\tctx, cancel := context.WithTimeout(context.Background(), time.Millisecond)\n\tdefer cancel()\n\tnotification, err = listener.WaitForNotification(ctx)\n\tassert.True(t, pgconn.Timeout(err))\n\tassert.Nil(t, notification)\n\n\t// listener can listen again after a timeout\n\tmustExec(t, notifier, \"notify chat\")\n\tnotification, err = listener.WaitForNotification(context.Background())\n\trequire.NoError(t, err)\n\tassert.Equal(t, \"chat\", notification.Channel)\n}\n\nfunc TestListenNotifyWhileBusyIsSafe(t *testing.T) {\n\tt.Parallel()\n\n\tfunc() {\n\t\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\t\tdefer closeConn(t, conn)\n\t\tpgxtest.SkipCockroachDB(t, conn, \"Server does not support LISTEN / NOTIFY (https://github.com/cockroachdb/cockroach/issues/41522)\")\n\t}()\n\n\tlistenerDone := make(chan bool)\n\tnotifierDone := make(chan bool)\n\tlistening := make(chan bool)\n\tgo func() {\n\t\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\t\tdefer closeConn(t, conn)\n\t\tdefer func() {\n\t\t\tlistenerDone <- true\n\t\t}()\n\n\t\tmustExec(t, conn, \"listen busysafe\")\n\t\tlistening <- true\n\n\t\tfor i := 0; i < 5000; i++ {\n\t\t\tvar sum int32\n\t\t\tvar rowCount int32\n\n\t\t\trows, err := conn.Query(context.Background(), \"select generate_series(1,$1)\", 100)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"conn.Query failed: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tfor rows.Next() {\n\t\t\t\tvar n int32\n\t\t\t\tif err := rows.Scan(&n); err != nil {\n\t\t\t\t\tt.Errorf(\"Row scan failed: %v\", err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tsum += n\n\t\t\t\trowCount++\n\t\t\t}\n\n\t\t\tif rows.Err() != nil {\n\t\t\t\tt.Errorf(\"conn.Query failed: %v\", rows.Err())\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif sum != 5050 {\n\t\t\t\tt.Errorf(\"Wrong rows sum: %v\", sum)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif rowCount != 100 {\n\t\t\t\tt.Errorf(\"Wrong number of rows: %v\", rowCount)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\tgo func() {\n\t\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\t\tdefer closeConn(t, conn)\n\t\tdefer func() {\n\t\t\tnotifierDone <- true\n\t\t}()\n\n\t\t<-listening\n\n\t\tfor i := 0; i < 100000; i++ {\n\t\t\tmustExec(t, conn, \"notify busysafe, 'hello'\")\n\t\t}\n\t}()\n\n\t<-listenerDone\n\t<-notifierDone\n}\n\nfunc TestListenNotifySelfNotification(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server does not support LISTEN / NOTIFY (https://github.com/cockroachdb/cockroach/issues/41522)\")\n\n\tmustExec(t, conn, \"listen self\")\n\n\t// Notify self and WaitForNotification immediately\n\tmustExec(t, conn, \"notify self\")\n\n\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\tdefer cancel()\n\tnotification, err := conn.WaitForNotification(ctx)\n\trequire.NoError(t, err)\n\tassert.Equal(t, \"self\", notification.Channel)\n\n\t// Notify self and do something else before WaitForNotification\n\tmustExec(t, conn, \"notify self\")\n\n\trows, _ := conn.Query(context.Background(), \"select 1\")\n\trows.Close()\n\tif rows.Err() != nil {\n\t\tt.Fatalf(\"Unexpected error on Query: %v\", rows.Err())\n\t}\n\n\tctx, cncl := context.WithTimeout(context.Background(), time.Second)\n\tdefer cncl()\n\tnotification, err = conn.WaitForNotification(ctx)\n\trequire.NoError(t, err)\n\tassert.Equal(t, \"self\", notification.Channel)\n}\n\nfunc TestFatalRxError(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server does not support pg_terminate_backend() (https://github.com/cockroachdb/cockroach/issues/35897)\")\n\n\tvar wg sync.WaitGroup\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tvar n int32\n\t\tvar s string\n\t\terr := conn.QueryRow(context.Background(), \"select 1::int4, pg_sleep(10)::varchar\").Scan(&n, &s)\n\t\tif pgErr, ok := err.(*pgconn.PgError); ok && pgErr.Severity == \"FATAL\" {\n\t\t} else {\n\t\t\tt.Errorf(\"Expected QueryRow Scan to return fatal PgError, but instead received %v\", err)\n\t\t\treturn\n\t\t}\n\t}()\n\n\totherConn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer otherConn.Close(context.Background())\n\n\tif _, err := otherConn.Exec(context.Background(), \"select pg_terminate_backend($1)\", conn.PgConn().PID()); err != nil {\n\t\tt.Fatalf(\"Unable to kill backend PostgreSQL process: %v\", err)\n\t}\n\n\twg.Wait()\n\n\tif !conn.IsClosed() {\n\t\tt.Fatal(\"Connection should be closed\")\n\t}\n}\n\nfunc TestFatalTxError(t *testing.T) {\n\tt.Parallel()\n\n\t// Run timing sensitive test many times\n\tfor i := 0; i < 50; i++ {\n\t\tfunc() {\n\t\t\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\t\t\tdefer closeConn(t, conn)\n\n\t\t\tpgxtest.SkipCockroachDB(t, conn, \"Server does not support pg_terminate_backend() (https://github.com/cockroachdb/cockroach/issues/35897)\")\n\n\t\t\totherConn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\t\t\tdefer otherConn.Close(context.Background())\n\n\t\t\t_, err := otherConn.Exec(context.Background(), \"select pg_terminate_backend($1)\", conn.PgConn().PID())\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Unable to kill backend PostgreSQL process: %v\", err)\n\t\t\t}\n\n\t\t\terr = conn.QueryRow(context.Background(), \"select 1\").Scan(nil)\n\t\t\tif err == nil {\n\t\t\t\tt.Fatal(\"Expected error but none occurred\")\n\t\t\t}\n\n\t\t\tif !conn.IsClosed() {\n\t\t\t\tt.Fatalf(\"Connection should be closed but isn't. Previous Query err: %v\", err)\n\t\t\t}\n\t\t}()\n\t}\n}\n\nfunc TestInsertBoolArray(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tif results := mustExec(t, conn, \"create temporary table foo(spice bool[]);\"); results.String() != \"CREATE TABLE\" {\n\t\t\tt.Error(\"Unexpected results from Exec\")\n\t\t}\n\n\t\t// Accept parameters\n\t\tif results := mustExec(t, conn, \"insert into foo(spice) values($1)\", []bool{true, false, true}); results.String() != \"INSERT 0 1\" {\n\t\t\tt.Errorf(\"Unexpected results from Exec: %v\", results)\n\t\t}\n\t})\n}\n\nfunc TestInsertTimestampArray(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tif results := mustExec(t, conn, \"create temporary table foo(spice timestamp[]);\"); results.String() != \"CREATE TABLE\" {\n\t\t\tt.Error(\"Unexpected results from Exec\")\n\t\t}\n\n\t\t// Accept parameters\n\t\tif results := mustExec(t, conn, \"insert into foo(spice) values($1)\", []time.Time{time.Unix(1419143667, 0), time.Unix(1419143672, 0)}); results.String() != \"INSERT 0 1\" {\n\t\t\tt.Errorf(\"Unexpected results from Exec: %v\", results)\n\t\t}\n\t})\n}\n\nfunc TestIdentifierSanitize(t *testing.T) {\n\tt.Parallel()\n\n\ttests := []struct {\n\t\tident    pgx.Identifier\n\t\texpected string\n\t}{\n\t\t{\n\t\t\tident:    pgx.Identifier{`foo`},\n\t\t\texpected: `\"foo\"`,\n\t\t},\n\t\t{\n\t\t\tident:    pgx.Identifier{`select`},\n\t\t\texpected: `\"select\"`,\n\t\t},\n\t\t{\n\t\t\tident:    pgx.Identifier{`foo`, `bar`},\n\t\t\texpected: `\"foo\".\"bar\"`,\n\t\t},\n\t\t{\n\t\t\tident:    pgx.Identifier{`you should \" not do this`},\n\t\t\texpected: `\"you should \"\" not do this\"`,\n\t\t},\n\t\t{\n\t\t\tident:    pgx.Identifier{`you should \" not do this`, `please don't`},\n\t\t\texpected: `\"you should \"\" not do this\".\"please don't\"`,\n\t\t},\n\t\t{\n\t\t\tident:    pgx.Identifier{`you should ` + string([]byte{0}) + `not do this`},\n\t\t\texpected: `\"you should not do this\"`,\n\t\t},\n\t}\n\n\tfor i, tt := range tests {\n\t\tqval := tt.ident.Sanitize()\n\t\tif qval != tt.expected {\n\t\t\tt.Errorf(\"%d. Expected Sanitize %v to return %v but it was %v\", i, tt.ident, tt.expected, qval)\n\t\t}\n\t}\n}\n\nfunc TestConnInitTypeMap(t *testing.T) {\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\t// spot check that the standard postgres type names aren't qualified\n\tnameOIDs := map[string]uint32{\n\t\t\"_int8\": pgtype.Int8ArrayOID,\n\t\t\"int8\":  pgtype.Int8OID,\n\t\t\"json\":  pgtype.JSONOID,\n\t\t\"text\":  pgtype.TextOID,\n\t}\n\tfor name, oid := range nameOIDs {\n\t\tdtByName, ok := conn.TypeMap().TypeForName(name)\n\t\tif !ok {\n\t\t\tt.Fatalf(\"Expected type named %v to be present\", name)\n\t\t}\n\t\tdtByOID, ok := conn.TypeMap().TypeForOID(oid)\n\t\tif !ok {\n\t\t\tt.Fatalf(\"Expected type OID %v to be present\", oid)\n\t\t}\n\t\tif dtByName != dtByOID {\n\t\t\tt.Fatalf(\"Expected type named %v to be the same as type OID %v\", name, oid)\n\t\t}\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestUnregisteredTypeUsableAsStringArgumentAndBaseResult(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tpgxtest.SkipCockroachDB(t, conn, \"Server does support domain types (https://github.com/cockroachdb/cockroach/issues/27796)\")\n\n\t\tvar n uint64\n\t\terr := conn.QueryRow(context.Background(), \"select $1::uint64\", \"42\").Scan(&n)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif n != 42 {\n\t\t\tt.Fatalf(\"Expected n to be 42, but was %v\", n)\n\t\t}\n\t})\n}\n\nfunc TestDomainType(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tpgxtest.SkipCockroachDB(t, conn, \"Server does support domain types (https://github.com/cockroachdb/cockroach/issues/27796)\")\n\n\t\t// Domain type uint64 is a PostgreSQL domain of underlying type numeric.\n\n\t\t// In the extended protocol preparing \"select $1::uint64\" appears to create a statement that expects a param OID of\n\t\t// uint64 but a result OID of the underlying numeric.\n\n\t\tvar s string\n\t\terr := conn.QueryRow(ctx, \"select $1::uint64\", \"24\").Scan(&s)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, \"24\", s)\n\n\t\t// Register type\n\t\tuint64Type, err := conn.LoadType(ctx, \"uint64\")\n\t\trequire.NoError(t, err)\n\t\tconn.TypeMap().RegisterType(uint64Type)\n\n\t\tvar n uint64\n\t\terr = conn.QueryRow(ctx, \"select $1::uint64\", uint64(24)).Scan(&n)\n\t\trequire.NoError(t, err)\n\n\t\t// String is still an acceptable argument after registration\n\t\terr = conn.QueryRow(ctx, \"select $1::uint64\", \"7\").Scan(&n)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif n != 7 {\n\t\t\tt.Fatalf(\"Expected n to be 7, but was %v\", n)\n\t\t}\n\t})\n}\n\nfunc TestLoadTypeSameNameInDifferentSchemas(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tpgxtest.SkipCockroachDB(t, conn, \"Server does support composite types (https://github.com/cockroachdb/cockroach/issues/27792)\")\n\n\t\ttx, err := conn.Begin(ctx)\n\t\trequire.NoError(t, err)\n\t\tdefer tx.Rollback(ctx)\n\n\t\t_, err = tx.Exec(ctx, `create schema pgx_a;\ncreate type pgx_a.point as (a text, b text);\ncreate schema pgx_b;\ncreate type pgx_b.point as (c text);\n`)\n\t\trequire.NoError(t, err)\n\n\t\t// Register types\n\t\tfor _, typename := range []string{\"pgx_a.point\", \"pgx_b.point\"} {\n\t\t\t// Obviously using conn while a tx is in use and registering a type after the connection has been established are\n\t\t\t// really bad practices, but for the sake of convenience we do it in the test here.\n\t\t\tdt, err := conn.LoadType(ctx, typename)\n\t\t\trequire.NoError(t, err)\n\t\t\tconn.TypeMap().RegisterType(dt)\n\t\t}\n\n\t\ttype aPoint struct {\n\t\t\tA string\n\t\t\tB string\n\t\t}\n\n\t\ttype bPoint struct {\n\t\t\tC string\n\t\t}\n\n\t\tvar a aPoint\n\t\tvar b bPoint\n\t\terr = tx.QueryRow(ctx, `select '(foo,bar)'::pgx_a.point, '(baz)'::pgx_b.point`).Scan(&a, &b)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, aPoint{\"foo\", \"bar\"}, a)\n\t\trequire.Equal(t, bPoint{\"baz\"}, b)\n\t})\n}\n\nfunc TestLoadCompositeType(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tpgxtest.SkipCockroachDB(t, conn, \"Server does support composite types (https://github.com/cockroachdb/cockroach/issues/27792)\")\n\n\t\ttx, err := conn.Begin(ctx)\n\t\trequire.NoError(t, err)\n\t\tdefer tx.Rollback(ctx)\n\n\t\t_, err = tx.Exec(ctx, \"create type compositetype as (attr1 int, attr2 int)\")\n\t\trequire.NoError(t, err)\n\n\t\t_, err = tx.Exec(ctx, \"alter type compositetype drop attribute attr1\")\n\t\trequire.NoError(t, err)\n\n\t\t_, err = conn.LoadType(ctx, \"compositetype\")\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestLoadRangeType(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tpgxtest.SkipCockroachDB(t, conn, \"Server does support range types\")\n\n\t\ttx, err := conn.Begin(ctx)\n\t\trequire.NoError(t, err)\n\t\tdefer tx.Rollback(ctx)\n\n\t\t_, err = tx.Exec(ctx, \"create type examplefloatrange as range (subtype=float8, subtype_diff=float8mi)\")\n\t\trequire.NoError(t, err)\n\n\t\t// Register types\n\t\tnewRangeType, err := conn.LoadType(ctx, \"examplefloatrange\")\n\t\trequire.NoError(t, err)\n\t\tconn.TypeMap().RegisterType(newRangeType)\n\t\tconn.TypeMap().RegisterDefaultPgType(pgtype.Range[float64]{}, \"examplefloatrange\")\n\n\t\tvar inputRangeType = pgtype.Range[float64]{\n\t\t\tLower:     1.0,\n\t\t\tUpper:     2.0,\n\t\t\tLowerType: pgtype.Inclusive,\n\t\t\tUpperType: pgtype.Inclusive,\n\t\t\tValid:     true,\n\t\t}\n\t\tvar outputRangeType pgtype.Range[float64]\n\t\terr = tx.QueryRow(ctx, \"SELECT $1::examplefloatrange\", inputRangeType).Scan(&outputRangeType)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, inputRangeType, outputRangeType)\n\t})\n}\n\nfunc TestLoadMultiRangeType(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tpgxtest.SkipCockroachDB(t, conn, \"Server does support range types\")\n\t\tpgxtest.SkipPostgreSQLVersionLessThan(t, conn, 14) // multirange data type was added in 14 postgresql\n\n\t\ttx, err := conn.Begin(ctx)\n\t\trequire.NoError(t, err)\n\t\tdefer tx.Rollback(ctx)\n\n\t\t_, err = tx.Exec(ctx, \"create type examplefloatrange as range (subtype=float8, subtype_diff=float8mi, multirange_type_name=examplefloatmultirange)\")\n\t\trequire.NoError(t, err)\n\n\t\t// Register types\n\t\tnewRangeType, err := conn.LoadType(ctx, \"examplefloatrange\")\n\t\trequire.NoError(t, err)\n\t\tconn.TypeMap().RegisterType(newRangeType)\n\t\tconn.TypeMap().RegisterDefaultPgType(pgtype.Range[float64]{}, \"examplefloatrange\")\n\n\t\tnewMultiRangeType, err := conn.LoadType(ctx, \"examplefloatmultirange\")\n\t\trequire.NoError(t, err)\n\t\tconn.TypeMap().RegisterType(newMultiRangeType)\n\t\tconn.TypeMap().RegisterDefaultPgType(pgtype.Multirange[pgtype.Range[float64]]{}, \"examplefloatmultirange\")\n\n\t\tvar inputMultiRangeType = pgtype.Multirange[pgtype.Range[float64]]{\n\t\t\t{\n\t\t\t\tLower:     1.0,\n\t\t\t\tUpper:     2.0,\n\t\t\t\tLowerType: pgtype.Inclusive,\n\t\t\t\tUpperType: pgtype.Inclusive,\n\t\t\t\tValid:     true,\n\t\t\t},\n\t\t\t{\n\t\t\t\tLower:     3.0,\n\t\t\t\tUpper:     4.0,\n\t\t\t\tLowerType: pgtype.Exclusive,\n\t\t\t\tUpperType: pgtype.Exclusive,\n\t\t\t\tValid:     true,\n\t\t\t},\n\t\t}\n\t\tvar outputMultiRangeType pgtype.Multirange[pgtype.Range[float64]]\n\t\terr = tx.QueryRow(ctx, \"SELECT $1::examplefloatmultirange\", inputMultiRangeType).Scan(&outputMultiRangeType)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, inputMultiRangeType, outputMultiRangeType)\n\t})\n}\n\nfunc TestStmtCacheInvalidationConn(t *testing.T) {\n\tctx := context.Background()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\t// create a table and fill it with some data\n\t_, err := conn.Exec(ctx, `\n        DROP TABLE IF EXISTS drop_cols;\n        CREATE TABLE drop_cols (\n            id SERIAL PRIMARY KEY NOT NULL,\n            f1 int NOT NULL,\n            f2 int NOT NULL\n        );\n    `)\n\trequire.NoError(t, err)\n\t_, err = conn.Exec(ctx, \"INSERT INTO drop_cols (f1, f2) VALUES (1, 2)\")\n\trequire.NoError(t, err)\n\n\tgetSQL := \"SELECT * FROM drop_cols WHERE id = $1\"\n\n\t// This query will populate the statement cache. We don't care about the result.\n\trows, err := conn.Query(ctx, getSQL, 1)\n\trequire.NoError(t, err)\n\trows.Close()\n\trequire.NoError(t, rows.Err())\n\n\t// Now, change the schema of the table out from under the statement, making it invalid.\n\t_, err = conn.Exec(ctx, \"ALTER TABLE drop_cols DROP COLUMN f1\")\n\trequire.NoError(t, err)\n\n\t// We must get an error the first time we try to re-execute a bad statement.\n\t// It is up to the application to determine if it wants to try again. We punt to\n\t// the application because there is no clear recovery path in the case of failed transactions\n\t// or batch operations and because automatic retry is tricky and we don't want to get\n\t// it wrong at such an importaint layer of the stack.\n\trows, err = conn.Query(ctx, getSQL, 1)\n\trequire.NoError(t, err)\n\trows.Next()\n\tnextErr := rows.Err()\n\trows.Close()\n\tfor _, err := range []error{nextErr, rows.Err()} {\n\t\tif err == nil {\n\t\t\tt.Fatal(`expected \"cached plan must not change result type\": no error`)\n\t\t}\n\t\tif !strings.Contains(err.Error(), \"cached plan must not change result type\") {\n\t\t\tt.Fatalf(`expected \"cached plan must not change result type\", got: \"%s\"`, err.Error())\n\t\t}\n\t}\n\n\t// On retry, the statement should have been flushed from the cache.\n\trows, err = conn.Query(ctx, getSQL, 1)\n\trequire.NoError(t, err)\n\trows.Next()\n\terr = rows.Err()\n\trequire.NoError(t, err)\n\trows.Close()\n\trequire.NoError(t, rows.Err())\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestStmtCacheInvalidationTx(t *testing.T) {\n\tctx := context.Background()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tif conn.PgConn().ParameterStatus(\"crdb_version\") != \"\" {\n\t\tt.Skip(\"Server has non-standard prepare in errored transaction behavior (https://github.com/cockroachdb/cockroach/issues/84140)\")\n\t}\n\n\t// create a table and fill it with some data\n\t_, err := conn.Exec(ctx, `\n        DROP TABLE IF EXISTS drop_cols;\n        CREATE TABLE drop_cols (\n            id SERIAL PRIMARY KEY NOT NULL,\n            f1 int NOT NULL,\n            f2 int NOT NULL\n        );\n    `)\n\trequire.NoError(t, err)\n\t_, err = conn.Exec(ctx, \"INSERT INTO drop_cols (f1, f2) VALUES (1, 2)\")\n\trequire.NoError(t, err)\n\n\ttx, err := conn.Begin(ctx)\n\trequire.NoError(t, err)\n\n\tgetSQL := \"SELECT * FROM drop_cols WHERE id = $1\"\n\n\t// This query will populate the statement cache. We don't care about the result.\n\trows, err := tx.Query(ctx, getSQL, 1)\n\trequire.NoError(t, err)\n\trows.Close()\n\trequire.NoError(t, rows.Err())\n\n\t// Now, change the schema of the table out from under the statement, making it invalid.\n\t_, err = tx.Exec(ctx, \"ALTER TABLE drop_cols DROP COLUMN f1\")\n\trequire.NoError(t, err)\n\n\t// We must get an error the first time we try to re-execute a bad statement.\n\t// It is up to the application to determine if it wants to try again. We punt to\n\t// the application because there is no clear recovery path in the case of failed transactions\n\t// or batch operations and because automatic retry is tricky and we don't want to get\n\t// it wrong at such an importaint layer of the stack.\n\trows, err = tx.Query(ctx, getSQL, 1)\n\trequire.NoError(t, err)\n\trows.Next()\n\tnextErr := rows.Err()\n\trows.Close()\n\tfor _, err := range []error{nextErr, rows.Err()} {\n\t\tif err == nil {\n\t\t\tt.Fatal(`expected \"cached plan must not change result type\": no error`)\n\t\t}\n\t\tif !strings.Contains(err.Error(), \"cached plan must not change result type\") {\n\t\t\tt.Fatalf(`expected \"cached plan must not change result type\", got: \"%s\"`, err.Error())\n\t\t}\n\t}\n\n\trows, _ = tx.Query(ctx, getSQL, 1)\n\trows.Close()\n\terr = rows.Err()\n\t// Retries within the same transaction are errors (really anything except a rollback\n\t// will be an error in this transaction).\n\trequire.Error(t, err)\n\trows.Close()\n\n\terr = tx.Rollback(ctx)\n\trequire.NoError(t, err)\n\n\t// once we've rolled back, retries will work\n\trows, err = conn.Query(ctx, getSQL, 1)\n\trequire.NoError(t, err)\n\trows.Next()\n\terr = rows.Err()\n\trequire.NoError(t, err)\n\trows.Close()\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestInsertDurationInterval(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\t_, err := conn.Exec(context.Background(), \"create temporary table t(duration INTERVAL(0) NOT NULL)\")\n\t\trequire.NoError(t, err)\n\n\t\tresult, err := conn.Exec(context.Background(), \"insert into t(duration) values($1)\", time.Minute)\n\t\trequire.NoError(t, err)\n\n\t\tn := result.RowsAffected()\n\t\trequire.EqualValues(t, 1, n)\n\t})\n}\n\nfunc TestRawValuesUnderlyingMemoryReused(t *testing.T) {\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tvar buf []byte\n\n\t\trows, err := conn.Query(ctx, `select 1::int`)\n\t\trequire.NoError(t, err)\n\n\t\tfor rows.Next() {\n\t\t\tbuf = rows.RawValues()[0]\n\t\t}\n\n\t\trequire.NoError(t, rows.Err())\n\n\t\toriginal := make([]byte, len(buf))\n\t\tcopy(original, buf)\n\n\t\tfor i := 0; i < 1_000_000; i++ {\n\t\t\trows, err := conn.Query(ctx, `select $1::int`, i)\n\t\t\trequire.NoError(t, err)\n\t\t\trows.Close()\n\t\t\trequire.NoError(t, rows.Err())\n\n\t\t\tif !bytes.Equal(original, buf) {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tt.Fatal(\"expected buffer from RawValues to be overwritten by subsequent queries but it was not\")\n\t})\n}\n\n// https://github.com/jackc/pgx/issues/1847\nfunc TestConnDeallocateInvalidatedCachedStatementsWhenCanceled(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tpgxtest.SkipCockroachDB(t, conn, \"CockroachDB returns decimal instead of integer for integer division\")\n\n\t\tvar n int32\n\t\terr := conn.QueryRow(ctx, \"select 1 / $1::int\", 1).Scan(&n)\n\t\trequire.NoError(t, err)\n\t\trequire.EqualValues(t, 1, n)\n\n\t\t// Divide by zero causes an error. baseRows.Close() calls Invalidate on the statement cache whenever an error was\n\t\t// encountered by the query. Use this to purposely invalidate the query. If we had access to private fields of conn\n\t\t// we could call conn.statementCache.InvalidateAll() instead.\n\t\terr = conn.QueryRow(ctx, \"select 1 / $1::int\", 0).Scan(&n)\n\t\trequire.Error(t, err)\n\n\t\tctx2, cancel2 := context.WithCancel(ctx)\n\t\tcancel2()\n\t\terr = conn.QueryRow(ctx2, \"select 1 / $1::int\", 1).Scan(&n)\n\t\trequire.Error(t, err)\n\t\trequire.ErrorIs(t, err, context.Canceled)\n\n\t\terr = conn.QueryRow(ctx, \"select 1 / $1::int\", 1).Scan(&n)\n\t\trequire.NoError(t, err)\n\t\trequire.EqualValues(t, 1, n)\n\t})\n}\n\n// https://github.com/jackc/pgx/issues/1847\nfunc TestConnDeallocateInvalidatedCachedStatementsInTransactionWithBatch(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\tconnString := os.Getenv(\"PGX_TEST_DATABASE\")\n\tconfig := mustParseConfig(t, connString)\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeCacheStatement\n\tconfig.StatementCacheCapacity = 2\n\n\tconn, err := pgx.ConnectConfig(ctx, config)\n\trequire.NoError(t, err)\n\n\ttx, err := conn.Begin(ctx)\n\trequire.NoError(t, err)\n\tdefer tx.Rollback(ctx)\n\n\t_, err = tx.Exec(ctx, \"select $1::int + 1\", 1)\n\trequire.NoError(t, err)\n\n\t_, err = tx.Exec(ctx, \"select $1::int + 2\", 1)\n\trequire.NoError(t, err)\n\n\t// This should invalidate the first cached statement.\n\t_, err = tx.Exec(ctx, \"select $1::int + 3\", 1)\n\trequire.NoError(t, err)\n\n\tbatch := &pgx.Batch{}\n\tbatch.Queue(\"select $1::int + 1\", 1)\n\terr = tx.SendBatch(ctx, batch).Close()\n\trequire.NoError(t, err)\n\n\terr = tx.Rollback(ctx)\n\trequire.NoError(t, err)\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestErrNoRows(t *testing.T) {\n\tt.Parallel()\n\n\t// ensure we preserve old error message\n\trequire.Equal(t, \"no rows in result set\", pgx.ErrNoRows.Error())\n\n\trequire.ErrorIs(t, pgx.ErrNoRows, sql.ErrNoRows, \"pgx.ErrNowRows must match sql.ErrNoRows\")\n}\n"
        },
        {
          "name": "copy_from.go",
          "type": "blob",
          "size": 7.0966796875,
          "content": "package pgx\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\n\t\"github.com/jackc/pgx/v5/internal/pgio\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n)\n\n// CopyFromRows returns a CopyFromSource interface over the provided rows slice\n// making it usable by *Conn.CopyFrom.\nfunc CopyFromRows(rows [][]any) CopyFromSource {\n\treturn &copyFromRows{rows: rows, idx: -1}\n}\n\ntype copyFromRows struct {\n\trows [][]any\n\tidx  int\n}\n\nfunc (ctr *copyFromRows) Next() bool {\n\tctr.idx++\n\treturn ctr.idx < len(ctr.rows)\n}\n\nfunc (ctr *copyFromRows) Values() ([]any, error) {\n\treturn ctr.rows[ctr.idx], nil\n}\n\nfunc (ctr *copyFromRows) Err() error {\n\treturn nil\n}\n\n// CopyFromSlice returns a CopyFromSource interface over a dynamic func\n// making it usable by *Conn.CopyFrom.\nfunc CopyFromSlice(length int, next func(int) ([]any, error)) CopyFromSource {\n\treturn &copyFromSlice{next: next, idx: -1, len: length}\n}\n\ntype copyFromSlice struct {\n\tnext func(int) ([]any, error)\n\tidx  int\n\tlen  int\n\terr  error\n}\n\nfunc (cts *copyFromSlice) Next() bool {\n\tcts.idx++\n\treturn cts.idx < cts.len\n}\n\nfunc (cts *copyFromSlice) Values() ([]any, error) {\n\tvalues, err := cts.next(cts.idx)\n\tif err != nil {\n\t\tcts.err = err\n\t}\n\treturn values, err\n}\n\nfunc (cts *copyFromSlice) Err() error {\n\treturn cts.err\n}\n\n// CopyFromFunc returns a CopyFromSource interface that relies on nxtf for values.\n// nxtf returns rows until it either signals an 'end of data' by returning row=nil and err=nil,\n// or it returns an error. If nxtf returns an error, the copy is aborted.\nfunc CopyFromFunc(nxtf func() (row []any, err error)) CopyFromSource {\n\treturn &copyFromFunc{next: nxtf}\n}\n\ntype copyFromFunc struct {\n\tnext     func() ([]any, error)\n\tvalueRow []any\n\terr      error\n}\n\nfunc (g *copyFromFunc) Next() bool {\n\tg.valueRow, g.err = g.next()\n\t// only return true if valueRow exists and no error\n\treturn g.valueRow != nil && g.err == nil\n}\n\nfunc (g *copyFromFunc) Values() ([]any, error) {\n\treturn g.valueRow, g.err\n}\n\nfunc (g *copyFromFunc) Err() error {\n\treturn g.err\n}\n\n// CopyFromSource is the interface used by *Conn.CopyFrom as the source for copy data.\ntype CopyFromSource interface {\n\t// Next returns true if there is another row and makes the next row data\n\t// available to Values(). When there are no more rows available or an error\n\t// has occurred it returns false.\n\tNext() bool\n\n\t// Values returns the values for the current row.\n\tValues() ([]any, error)\n\n\t// Err returns any error that has been encountered by the CopyFromSource. If\n\t// this is not nil *Conn.CopyFrom will abort the copy.\n\tErr() error\n}\n\ntype copyFrom struct {\n\tconn          *Conn\n\ttableName     Identifier\n\tcolumnNames   []string\n\trowSrc        CopyFromSource\n\treaderErrChan chan error\n\tmode          QueryExecMode\n}\n\nfunc (ct *copyFrom) run(ctx context.Context) (int64, error) {\n\tif ct.conn.copyFromTracer != nil {\n\t\tctx = ct.conn.copyFromTracer.TraceCopyFromStart(ctx, ct.conn, TraceCopyFromStartData{\n\t\t\tTableName:   ct.tableName,\n\t\t\tColumnNames: ct.columnNames,\n\t\t})\n\t}\n\n\tquotedTableName := ct.tableName.Sanitize()\n\tcbuf := &bytes.Buffer{}\n\tfor i, cn := range ct.columnNames {\n\t\tif i != 0 {\n\t\t\tcbuf.WriteString(\", \")\n\t\t}\n\t\tcbuf.WriteString(quoteIdentifier(cn))\n\t}\n\tquotedColumnNames := cbuf.String()\n\n\tvar sd *pgconn.StatementDescription\n\tswitch ct.mode {\n\tcase QueryExecModeExec, QueryExecModeSimpleProtocol:\n\t\t// These modes don't support the binary format. Before the inclusion of the\n\t\t// QueryExecModes, Conn.Prepare was called on every COPY operation to get\n\t\t// the OIDs. These prepared statements were not cached.\n\t\t//\n\t\t// Since that's the same behavior provided by QueryExecModeDescribeExec,\n\t\t// we'll default to that mode.\n\t\tct.mode = QueryExecModeDescribeExec\n\t\tfallthrough\n\tcase QueryExecModeCacheStatement, QueryExecModeCacheDescribe, QueryExecModeDescribeExec:\n\t\tvar err error\n\t\tsd, err = ct.conn.getStatementDescription(\n\t\t\tctx,\n\t\t\tct.mode,\n\t\t\tfmt.Sprintf(\"select %s from %s\", quotedColumnNames, quotedTableName),\n\t\t)\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"statement description failed: %w\", err)\n\t\t}\n\tdefault:\n\t\treturn 0, fmt.Errorf(\"unknown QueryExecMode: %v\", ct.mode)\n\t}\n\n\tr, w := io.Pipe()\n\tdoneChan := make(chan struct{})\n\n\tgo func() {\n\t\tdefer close(doneChan)\n\n\t\t// Purposely NOT using defer w.Close(). See https://github.com/golang/go/issues/24283.\n\t\tbuf := ct.conn.wbuf\n\n\t\tbuf = append(buf, \"PGCOPY\\n\\377\\r\\n\\000\"...)\n\t\tbuf = pgio.AppendInt32(buf, 0)\n\t\tbuf = pgio.AppendInt32(buf, 0)\n\n\t\tmoreRows := true\n\t\tfor moreRows {\n\t\t\tvar err error\n\t\t\tmoreRows, buf, err = ct.buildCopyBuf(buf, sd)\n\t\t\tif err != nil {\n\t\t\t\tw.CloseWithError(err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif ct.rowSrc.Err() != nil {\n\t\t\t\tw.CloseWithError(ct.rowSrc.Err())\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif len(buf) > 0 {\n\t\t\t\t_, err = w.Write(buf)\n\t\t\t\tif err != nil {\n\t\t\t\t\tw.Close()\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tbuf = buf[:0]\n\t\t}\n\n\t\tw.Close()\n\t}()\n\n\tcommandTag, err := ct.conn.pgConn.CopyFrom(ctx, r, fmt.Sprintf(\"copy %s ( %s ) from stdin binary;\", quotedTableName, quotedColumnNames))\n\n\tr.Close()\n\t<-doneChan\n\n\tif ct.conn.copyFromTracer != nil {\n\t\tct.conn.copyFromTracer.TraceCopyFromEnd(ctx, ct.conn, TraceCopyFromEndData{\n\t\t\tCommandTag: commandTag,\n\t\t\tErr:        err,\n\t\t})\n\t}\n\n\treturn commandTag.RowsAffected(), err\n}\n\nfunc (ct *copyFrom) buildCopyBuf(buf []byte, sd *pgconn.StatementDescription) (bool, []byte, error) {\n\tconst sendBufSize = 65536 - 5 // The packet has a 5-byte header\n\tlastBufLen := 0\n\tlargestRowLen := 0\n\n\tfor ct.rowSrc.Next() {\n\t\tlastBufLen = len(buf)\n\n\t\tvalues, err := ct.rowSrc.Values()\n\t\tif err != nil {\n\t\t\treturn false, nil, err\n\t\t}\n\t\tif len(values) != len(ct.columnNames) {\n\t\t\treturn false, nil, fmt.Errorf(\"expected %d values, got %d values\", len(ct.columnNames), len(values))\n\t\t}\n\n\t\tbuf = pgio.AppendInt16(buf, int16(len(ct.columnNames)))\n\t\tfor i, val := range values {\n\t\t\tbuf, err = encodeCopyValue(ct.conn.typeMap, buf, sd.Fields[i].DataTypeOID, val)\n\t\t\tif err != nil {\n\t\t\t\treturn false, nil, err\n\t\t\t}\n\t\t}\n\n\t\trowLen := len(buf) - lastBufLen\n\t\tif rowLen > largestRowLen {\n\t\t\tlargestRowLen = rowLen\n\t\t}\n\n\t\t// Try not to overflow size of the buffer PgConn.CopyFrom will be reading into. If that happens then the nature of\n\t\t// io.Pipe means that the next Read will be short. This can lead to pathological send sizes such as 65531, 13, 65531\n\t\t// 13, 65531, 13, 65531, 13.\n\t\tif len(buf) > sendBufSize-largestRowLen {\n\t\t\treturn true, buf, nil\n\t\t}\n\t}\n\n\treturn false, buf, nil\n}\n\n// CopyFrom uses the PostgreSQL copy protocol to perform bulk data insertion. It returns the number of rows copied and\n// an error.\n//\n// CopyFrom requires all values use the binary format. A pgtype.Type that supports the binary format must be registered\n// for the type of each column. Almost all types implemented by pgx support the binary format.\n//\n// Even though enum types appear to be strings they still must be registered to use with CopyFrom. This can be done with\n// Conn.LoadType and pgtype.Map.RegisterType.\nfunc (c *Conn) CopyFrom(ctx context.Context, tableName Identifier, columnNames []string, rowSrc CopyFromSource) (int64, error) {\n\tct := &copyFrom{\n\t\tconn:          c,\n\t\ttableName:     tableName,\n\t\tcolumnNames:   columnNames,\n\t\trowSrc:        rowSrc,\n\t\treaderErrChan: make(chan error),\n\t\tmode:          c.config.DefaultQueryExecMode,\n\t}\n\n\treturn ct.run(ctx)\n}\n"
        },
        {
          "name": "copy_from_test.go",
          "type": "blob",
          "size": 21.892578125,
          "content": "package pgx_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\t\"reflect\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgxtest\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestConnCopyWithAllQueryExecModes(t *testing.T) {\n\tfor _, mode := range pgxtest.AllQueryExecModes {\n\t\tt.Run(mode.String(), func(t *testing.T) {\n\t\t\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\t\t\tdefer cancel()\n\n\t\t\tcfg := mustParseConfig(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\t\t\tcfg.DefaultQueryExecMode = mode\n\t\t\tconn := mustConnect(t, cfg)\n\t\t\tdefer closeConn(t, conn)\n\n\t\t\tmustExec(t, conn, `create temporary table foo(\n\t\t\ta int2,\n\t\t\tb int4,\n\t\t\tc int8,\n\t\t\td text,\n\t\t\te timestamptz\n\t\t)`)\n\n\t\t\ttzedTime := time.Date(2010, 2, 3, 4, 5, 6, 0, time.Local)\n\n\t\t\tinputRows := [][]any{\n\t\t\t\t{int16(0), int32(1), int64(2), \"abc\", tzedTime},\n\t\t\t\t{nil, nil, nil, nil, nil},\n\t\t\t}\n\n\t\t\tcopyCount, err := conn.CopyFrom(ctx, pgx.Identifier{\"foo\"}, []string{\"a\", \"b\", \"c\", \"d\", \"e\"}, pgx.CopyFromRows(inputRows))\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"Unexpected error for CopyFrom: %v\", err)\n\t\t\t}\n\t\t\tif int(copyCount) != len(inputRows) {\n\t\t\t\tt.Errorf(\"Expected CopyFrom to return %d copied rows, but got %d\", len(inputRows), copyCount)\n\t\t\t}\n\n\t\t\trows, err := conn.Query(ctx, \"select * from foo\")\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"Unexpected error for Query: %v\", err)\n\t\t\t}\n\n\t\t\tvar outputRows [][]any\n\t\t\tfor rows.Next() {\n\t\t\t\trow, err := rows.Values()\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Errorf(\"Unexpected error for rows.Values(): %v\", err)\n\t\t\t\t}\n\t\t\t\toutputRows = append(outputRows, row)\n\t\t\t}\n\n\t\t\tif rows.Err() != nil {\n\t\t\t\tt.Errorf(\"Unexpected error for rows.Err(): %v\", rows.Err())\n\t\t\t}\n\n\t\t\tif !reflect.DeepEqual(inputRows, outputRows) {\n\t\t\t\tt.Errorf(\"Input rows and output rows do not equal: %v -> %v\", inputRows, outputRows)\n\t\t\t}\n\n\t\t\tensureConnValid(t, conn)\n\t\t})\n\t}\n}\n\nfunc TestConnCopyWithKnownOIDQueryExecModes(t *testing.T) {\n\n\tfor _, mode := range pgxtest.KnownOIDQueryExecModes {\n\t\tt.Run(mode.String(), func(t *testing.T) {\n\t\t\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\t\t\tdefer cancel()\n\n\t\t\tcfg := mustParseConfig(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\t\t\tcfg.DefaultQueryExecMode = mode\n\t\t\tconn := mustConnect(t, cfg)\n\t\t\tdefer closeConn(t, conn)\n\n\t\t\tmustExec(t, conn, `create temporary table foo(\n\t\t\ta int2,\n\t\t\tb int4,\n\t\t\tc int8,\n\t\t\td varchar,\n\t\t\te text,\n\t\t\tf date,\n\t\t\tg timestamptz\n\t\t)`)\n\n\t\t\ttzedTime := time.Date(2010, 2, 3, 4, 5, 6, 0, time.Local)\n\n\t\t\tinputRows := [][]any{\n\t\t\t\t{int16(0), int32(1), int64(2), \"abc\", \"efg\", time.Date(2000, 1, 1, 0, 0, 0, 0, time.UTC), tzedTime},\n\t\t\t\t{nil, nil, nil, nil, nil, nil, nil},\n\t\t\t}\n\n\t\t\tcopyCount, err := conn.CopyFrom(ctx, pgx.Identifier{\"foo\"}, []string{\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"}, pgx.CopyFromRows(inputRows))\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"Unexpected error for CopyFrom: %v\", err)\n\t\t\t}\n\t\t\tif int(copyCount) != len(inputRows) {\n\t\t\t\tt.Errorf(\"Expected CopyFrom to return %d copied rows, but got %d\", len(inputRows), copyCount)\n\t\t\t}\n\n\t\t\trows, err := conn.Query(ctx, \"select * from foo\")\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"Unexpected error for Query: %v\", err)\n\t\t\t}\n\n\t\t\tvar outputRows [][]any\n\t\t\tfor rows.Next() {\n\t\t\t\trow, err := rows.Values()\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Errorf(\"Unexpected error for rows.Values(): %v\", err)\n\t\t\t\t}\n\t\t\t\toutputRows = append(outputRows, row)\n\t\t\t}\n\n\t\t\tif rows.Err() != nil {\n\t\t\t\tt.Errorf(\"Unexpected error for rows.Err(): %v\", rows.Err())\n\t\t\t}\n\n\t\t\tif !reflect.DeepEqual(inputRows, outputRows) {\n\t\t\t\tt.Errorf(\"Input rows and output rows do not equal: %v -> %v\", inputRows, outputRows)\n\t\t\t}\n\n\t\t\tensureConnValid(t, conn)\n\t\t})\n\t}\n}\n\nfunc TestConnCopyFromSmall(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tmustExec(t, conn, `create temporary table foo(\n\t\ta int2,\n\t\tb int4,\n\t\tc int8,\n\t\td varchar,\n\t\te text,\n\t\tf date,\n\t\tg timestamptz\n\t)`)\n\n\ttzedTime := time.Date(2010, 2, 3, 4, 5, 6, 0, time.Local)\n\n\tinputRows := [][]any{\n\t\t{int16(0), int32(1), int64(2), \"abc\", \"efg\", time.Date(2000, 1, 1, 0, 0, 0, 0, time.UTC), tzedTime},\n\t\t{nil, nil, nil, nil, nil, nil, nil},\n\t}\n\n\tcopyCount, err := conn.CopyFrom(ctx, pgx.Identifier{\"foo\"}, []string{\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"}, pgx.CopyFromRows(inputRows))\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error for CopyFrom: %v\", err)\n\t}\n\tif int(copyCount) != len(inputRows) {\n\t\tt.Errorf(\"Expected CopyFrom to return %d copied rows, but got %d\", len(inputRows), copyCount)\n\t}\n\n\trows, err := conn.Query(ctx, \"select * from foo\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error for Query: %v\", err)\n\t}\n\n\tvar outputRows [][]any\n\tfor rows.Next() {\n\t\trow, err := rows.Values()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Unexpected error for rows.Values(): %v\", err)\n\t\t}\n\t\toutputRows = append(outputRows, row)\n\t}\n\n\tif rows.Err() != nil {\n\t\tt.Errorf(\"Unexpected error for rows.Err(): %v\", rows.Err())\n\t}\n\n\tif !reflect.DeepEqual(inputRows, outputRows) {\n\t\tt.Errorf(\"Input rows and output rows do not equal: %v -> %v\", inputRows, outputRows)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnCopyFromSliceSmall(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tmustExec(t, conn, `create temporary table foo(\n\t\ta int2,\n\t\tb int4,\n\t\tc int8,\n\t\td varchar,\n\t\te text,\n\t\tf date,\n\t\tg timestamptz\n\t)`)\n\n\ttzedTime := time.Date(2010, 2, 3, 4, 5, 6, 0, time.Local)\n\n\tinputRows := [][]any{\n\t\t{int16(0), int32(1), int64(2), \"abc\", \"efg\", time.Date(2000, 1, 1, 0, 0, 0, 0, time.UTC), tzedTime},\n\t\t{nil, nil, nil, nil, nil, nil, nil},\n\t}\n\n\tcopyCount, err := conn.CopyFrom(ctx, pgx.Identifier{\"foo\"}, []string{\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"},\n\t\tpgx.CopyFromSlice(len(inputRows), func(i int) ([]any, error) {\n\t\t\treturn inputRows[i], nil\n\t\t}))\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error for CopyFrom: %v\", err)\n\t}\n\tif int(copyCount) != len(inputRows) {\n\t\tt.Errorf(\"Expected CopyFrom to return %d copied rows, but got %d\", len(inputRows), copyCount)\n\t}\n\n\trows, err := conn.Query(ctx, \"select * from foo\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error for Query: %v\", err)\n\t}\n\n\tvar outputRows [][]any\n\tfor rows.Next() {\n\t\trow, err := rows.Values()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Unexpected error for rows.Values(): %v\", err)\n\t\t}\n\t\toutputRows = append(outputRows, row)\n\t}\n\n\tif rows.Err() != nil {\n\t\tt.Errorf(\"Unexpected error for rows.Err(): %v\", rows.Err())\n\t}\n\n\tif !reflect.DeepEqual(inputRows, outputRows) {\n\t\tt.Errorf(\"Input rows and output rows do not equal: %v -> %v\", inputRows, outputRows)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnCopyFromLarge(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tmustExec(t, conn, `create temporary table foo(\n\t\ta int2,\n\t\tb int4,\n\t\tc int8,\n\t\td varchar,\n\t\te text,\n\t\tf date,\n\t\tg timestamptz,\n\t\th bytea\n\t)`)\n\n\ttzedTime := time.Date(2010, 2, 3, 4, 5, 6, 0, time.Local)\n\n\tinputRows := [][]any{}\n\n\tfor i := 0; i < 10000; i++ {\n\t\tinputRows = append(inputRows, []any{int16(0), int32(1), int64(2), \"abc\", \"efg\", time.Date(2000, 1, 1, 0, 0, 0, 0, time.UTC), tzedTime, []byte{111, 111, 111, 111}})\n\t}\n\n\tcopyCount, err := conn.CopyFrom(ctx, pgx.Identifier{\"foo\"}, []string{\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"}, pgx.CopyFromRows(inputRows))\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error for CopyFrom: %v\", err)\n\t}\n\tif int(copyCount) != len(inputRows) {\n\t\tt.Errorf(\"Expected CopyFrom to return %d copied rows, but got %d\", len(inputRows), copyCount)\n\t}\n\n\trows, err := conn.Query(ctx, \"select * from foo\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error for Query: %v\", err)\n\t}\n\n\tvar outputRows [][]any\n\tfor rows.Next() {\n\t\trow, err := rows.Values()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Unexpected error for rows.Values(): %v\", err)\n\t\t}\n\t\toutputRows = append(outputRows, row)\n\t}\n\n\tif rows.Err() != nil {\n\t\tt.Errorf(\"Unexpected error for rows.Err(): %v\", rows.Err())\n\t}\n\n\tif !reflect.DeepEqual(inputRows, outputRows) {\n\t\tt.Errorf(\"Input rows and output rows do not equal\")\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnCopyFromEnum(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\ttx, err := conn.Begin(ctx)\n\trequire.NoError(t, err)\n\tdefer tx.Rollback(ctx)\n\n\t_, err = tx.Exec(ctx, `drop type if exists color`)\n\trequire.NoError(t, err)\n\n\t_, err = tx.Exec(ctx, `drop type if exists fruit`)\n\trequire.NoError(t, err)\n\n\t_, err = tx.Exec(ctx, `create type color as enum ('blue', 'green', 'orange')`)\n\trequire.NoError(t, err)\n\n\t_, err = tx.Exec(ctx, `create type fruit as enum ('apple', 'orange', 'grape')`)\n\trequire.NoError(t, err)\n\n\t// Obviously using conn while a tx is in use and registering a type after the connection has been established are\n\t// really bad practices, but for the sake of convenience we do it in the test here.\n\tfor _, name := range []string{\"fruit\", \"color\"} {\n\t\ttyp, err := conn.LoadType(ctx, name)\n\t\trequire.NoError(t, err)\n\t\tconn.TypeMap().RegisterType(typ)\n\t}\n\n\t_, err = tx.Exec(ctx, `create temporary table foo(\n\t\ta text,\n\t\tb color,\n\t\tc fruit,\n\t\td color,\n\t\te fruit,\n\t\tf text\n\t)`)\n\trequire.NoError(t, err)\n\n\tinputRows := [][]any{\n\t\t{\"abc\", \"blue\", \"grape\", \"orange\", \"orange\", \"def\"},\n\t\t{nil, nil, nil, nil, nil, nil},\n\t}\n\n\tcopyCount, err := tx.CopyFrom(ctx, pgx.Identifier{\"foo\"}, []string{\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"}, pgx.CopyFromRows(inputRows))\n\trequire.NoError(t, err)\n\trequire.EqualValues(t, len(inputRows), copyCount)\n\n\trows, err := tx.Query(ctx, \"select * from foo\")\n\trequire.NoError(t, err)\n\n\tvar outputRows [][]any\n\tfor rows.Next() {\n\t\trow, err := rows.Values()\n\t\trequire.NoError(t, err)\n\t\toutputRows = append(outputRows, row)\n\t}\n\n\trequire.NoError(t, rows.Err())\n\n\tif !reflect.DeepEqual(inputRows, outputRows) {\n\t\tt.Errorf(\"Input rows and output rows do not equal: %v -> %v\", inputRows, outputRows)\n\t}\n\n\terr = tx.Rollback(ctx)\n\trequire.NoError(t, err)\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnCopyFromJSON(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tfor _, typeName := range []string{\"json\", \"jsonb\"} {\n\t\tif _, ok := conn.TypeMap().TypeForName(typeName); !ok {\n\t\t\treturn // No JSON/JSONB type -- must be running against old PostgreSQL\n\t\t}\n\t}\n\n\tmustExec(t, conn, `create temporary table foo(\n\t\ta json,\n\t\tb jsonb\n\t)`)\n\n\tinputRows := [][]any{\n\t\t{map[string]any{\"foo\": \"bar\"}, map[string]any{\"bar\": \"quz\"}},\n\t\t{nil, nil},\n\t}\n\n\tcopyCount, err := conn.CopyFrom(ctx, pgx.Identifier{\"foo\"}, []string{\"a\", \"b\"}, pgx.CopyFromRows(inputRows))\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error for CopyFrom: %v\", err)\n\t}\n\tif int(copyCount) != len(inputRows) {\n\t\tt.Errorf(\"Expected CopyFrom to return %d copied rows, but got %d\", len(inputRows), copyCount)\n\t}\n\n\trows, err := conn.Query(ctx, \"select * from foo\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error for Query: %v\", err)\n\t}\n\n\tvar outputRows [][]any\n\tfor rows.Next() {\n\t\trow, err := rows.Values()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Unexpected error for rows.Values(): %v\", err)\n\t\t}\n\t\toutputRows = append(outputRows, row)\n\t}\n\n\tif rows.Err() != nil {\n\t\tt.Errorf(\"Unexpected error for rows.Err(): %v\", rows.Err())\n\t}\n\n\tif !reflect.DeepEqual(inputRows, outputRows) {\n\t\tt.Errorf(\"Input rows and output rows do not equal: %v -> %v\", inputRows, outputRows)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\ntype clientFailSource struct {\n\tcount int\n\terr   error\n}\n\nfunc (cfs *clientFailSource) Next() bool {\n\tcfs.count++\n\treturn cfs.count < 100\n}\n\nfunc (cfs *clientFailSource) Values() ([]any, error) {\n\tif cfs.count == 3 {\n\t\tcfs.err = fmt.Errorf(\"client error\")\n\t\treturn nil, cfs.err\n\t}\n\treturn []any{make([]byte, 100000)}, nil\n}\n\nfunc (cfs *clientFailSource) Err() error {\n\treturn cfs.err\n}\n\nfunc TestConnCopyFromFailServerSideMidway(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tmustExec(t, conn, `create temporary table foo(\n\t\ta int4,\n\t\tb varchar not null\n\t)`)\n\n\tinputRows := [][]any{\n\t\t{int32(1), \"abc\"},\n\t\t{int32(2), nil}, // this row should trigger a failure\n\t\t{int32(3), \"def\"},\n\t}\n\n\tcopyCount, err := conn.CopyFrom(ctx, pgx.Identifier{\"foo\"}, []string{\"a\", \"b\"}, pgx.CopyFromRows(inputRows))\n\tif err == nil {\n\t\tt.Errorf(\"Expected CopyFrom return error, but it did not\")\n\t}\n\tif _, ok := err.(*pgconn.PgError); !ok {\n\t\tt.Errorf(\"Expected CopyFrom return pgx.PgError, but instead it returned: %v\", err)\n\t}\n\tif copyCount != 0 {\n\t\tt.Errorf(\"Expected CopyFrom to return 0 copied rows, but got %d\", copyCount)\n\t}\n\n\trows, err := conn.Query(ctx, \"select * from foo\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error for Query: %v\", err)\n\t}\n\n\tvar outputRows [][]any\n\tfor rows.Next() {\n\t\trow, err := rows.Values()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Unexpected error for rows.Values(): %v\", err)\n\t\t}\n\t\toutputRows = append(outputRows, row)\n\t}\n\n\tif rows.Err() != nil {\n\t\tt.Errorf(\"Unexpected error for rows.Err(): %v\", rows.Err())\n\t}\n\n\tif len(outputRows) != 0 {\n\t\tt.Errorf(\"Expected 0 rows, but got %v\", outputRows)\n\t}\n\n\tmustExec(t, conn, \"truncate foo\")\n\n\tensureConnValid(t, conn)\n}\n\ntype failSource struct {\n\tcount int\n}\n\nfunc (fs *failSource) Next() bool {\n\ttime.Sleep(time.Millisecond * 100)\n\tfs.count++\n\treturn fs.count < 100\n}\n\nfunc (fs *failSource) Values() ([]any, error) {\n\tif fs.count == 3 {\n\t\treturn []any{nil}, nil\n\t}\n\treturn []any{make([]byte, 100000)}, nil\n}\n\nfunc (fs *failSource) Err() error {\n\treturn nil\n}\n\nfunc TestConnCopyFromFailServerSideMidwayAbortsWithoutWaiting(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server copy error does not fail fast\")\n\n\tmustExec(t, conn, `create temporary table foo(\n\t\ta bytea not null\n\t)`)\n\n\tstartTime := time.Now()\n\n\tcopyCount, err := conn.CopyFrom(ctx, pgx.Identifier{\"foo\"}, []string{\"a\"}, &failSource{})\n\tif err == nil {\n\t\tt.Errorf(\"Expected CopyFrom return error, but it did not\")\n\t}\n\tif _, ok := err.(*pgconn.PgError); !ok {\n\t\tt.Errorf(\"Expected CopyFrom return pgx.PgError, but instead it returned: %v\", err)\n\t}\n\tif copyCount != 0 {\n\t\tt.Errorf(\"Expected CopyFrom to return 0 copied rows, but got %d\", copyCount)\n\t}\n\n\tendTime := time.Now()\n\tcopyTime := endTime.Sub(startTime)\n\tif copyTime > time.Second {\n\t\tt.Errorf(\"Failing CopyFrom shouldn't have taken so long: %v\", copyTime)\n\t}\n\n\trows, err := conn.Query(ctx, \"select * from foo\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error for Query: %v\", err)\n\t}\n\n\tvar outputRows [][]any\n\tfor rows.Next() {\n\t\trow, err := rows.Values()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Unexpected error for rows.Values(): %v\", err)\n\t\t}\n\t\toutputRows = append(outputRows, row)\n\t}\n\n\tif rows.Err() != nil {\n\t\tt.Errorf(\"Unexpected error for rows.Err(): %v\", rows.Err())\n\t}\n\n\tif len(outputRows) != 0 {\n\t\tt.Errorf(\"Expected 0 rows, but got %v\", outputRows)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\ntype slowFailRaceSource struct {\n\tcount int\n}\n\nfunc (fs *slowFailRaceSource) Next() bool {\n\ttime.Sleep(time.Millisecond)\n\tfs.count++\n\treturn fs.count < 1000\n}\n\nfunc (fs *slowFailRaceSource) Values() ([]any, error) {\n\tif fs.count == 500 {\n\t\treturn []any{nil, nil}, nil\n\t}\n\treturn []any{1, make([]byte, 1000)}, nil\n}\n\nfunc (fs *slowFailRaceSource) Err() error {\n\treturn nil\n}\n\nfunc TestConnCopyFromSlowFailRace(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tmustExec(t, conn, `create temporary table foo(\n\t\ta int not null,\n\t\tb bytea not null\n\t)`)\n\n\tcopyCount, err := conn.CopyFrom(ctx, pgx.Identifier{\"foo\"}, []string{\"a\", \"b\"}, &slowFailRaceSource{})\n\tif err == nil {\n\t\tt.Errorf(\"Expected CopyFrom return error, but it did not\")\n\t}\n\tif _, ok := err.(*pgconn.PgError); !ok {\n\t\tt.Errorf(\"Expected CopyFrom return pgx.PgError, but instead it returned: %v\", err)\n\t}\n\tif copyCount != 0 {\n\t\tt.Errorf(\"Expected CopyFrom to return 0 copied rows, but got %d\", copyCount)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnCopyFromCopyFromSourceErrorMidway(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tmustExec(t, conn, `create temporary table foo(\n\t\ta bytea not null\n\t)`)\n\n\tcopyCount, err := conn.CopyFrom(ctx, pgx.Identifier{\"foo\"}, []string{\"a\"}, &clientFailSource{})\n\tif err == nil {\n\t\tt.Errorf(\"Expected CopyFrom return error, but it did not\")\n\t}\n\tif copyCount != 0 {\n\t\tt.Errorf(\"Expected CopyFrom to return 0 copied rows, but got %d\", copyCount)\n\t}\n\n\trows, err := conn.Query(ctx, \"select * from foo\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error for Query: %v\", err)\n\t}\n\n\tvar outputRows [][]any\n\tfor rows.Next() {\n\t\trow, err := rows.Values()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Unexpected error for rows.Values(): %v\", err)\n\t\t}\n\t\toutputRows = append(outputRows, row)\n\t}\n\n\tif rows.Err() != nil {\n\t\tt.Errorf(\"Unexpected error for rows.Err(): %v\", rows.Err())\n\t}\n\n\tif len(outputRows) != 0 {\n\t\tt.Errorf(\"Expected 0 rows, but got %v\", len(outputRows))\n\t}\n\n\tensureConnValid(t, conn)\n}\n\ntype clientFinalErrSource struct {\n\tcount int\n}\n\nfunc (cfs *clientFinalErrSource) Next() bool {\n\tcfs.count++\n\treturn cfs.count < 5\n}\n\nfunc (cfs *clientFinalErrSource) Values() ([]any, error) {\n\treturn []any{make([]byte, 100000)}, nil\n}\n\nfunc (cfs *clientFinalErrSource) Err() error {\n\treturn fmt.Errorf(\"final error\")\n}\n\nfunc TestConnCopyFromCopyFromSourceErrorEnd(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tmustExec(t, conn, `create temporary table foo(\n\t\ta bytea not null\n\t)`)\n\n\tcopyCount, err := conn.CopyFrom(ctx, pgx.Identifier{\"foo\"}, []string{\"a\"}, &clientFinalErrSource{})\n\tif err == nil {\n\t\tt.Errorf(\"Expected CopyFrom return error, but it did not\")\n\t}\n\tif copyCount != 0 {\n\t\tt.Errorf(\"Expected CopyFrom to return 0 copied rows, but got %d\", copyCount)\n\t}\n\n\trows, err := conn.Query(ctx, \"select * from foo\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error for Query: %v\", err)\n\t}\n\n\tvar outputRows [][]any\n\tfor rows.Next() {\n\t\trow, err := rows.Values()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Unexpected error for rows.Values(): %v\", err)\n\t\t}\n\t\toutputRows = append(outputRows, row)\n\t}\n\n\tif rows.Err() != nil {\n\t\tt.Errorf(\"Unexpected error for rows.Err(): %v\", rows.Err())\n\t}\n\n\tif len(outputRows) != 0 {\n\t\tt.Errorf(\"Expected 0 rows, but got %v\", outputRows)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnCopyFromAutomaticStringConversion(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tmustExec(t, conn, `create temporary table foo(\n\t\ta int8\n\t)`)\n\n\tinputRows := [][]interface{}{\n\t\t{\"42\"},\n\t\t{\"7\"},\n\t\t{8},\n\t}\n\n\tcopyCount, err := conn.CopyFrom(ctx, pgx.Identifier{\"foo\"}, []string{\"a\"}, pgx.CopyFromRows(inputRows))\n\trequire.NoError(t, err)\n\trequire.EqualValues(t, len(inputRows), copyCount)\n\n\trows, _ := conn.Query(ctx, \"select * from foo\")\n\tnums, err := pgx.CollectRows(rows, pgx.RowTo[int64])\n\trequire.NoError(t, err)\n\n\trequire.Equal(t, []int64{42, 7, 8}, nums)\n\n\tensureConnValid(t, conn)\n}\n\n// https://github.com/jackc/pgx/discussions/1891\nfunc TestConnCopyFromAutomaticStringConversionArray(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tmustExec(t, conn, `create temporary table foo(\n\t\ta numeric[]\n\t)`)\n\n\tinputRows := [][]interface{}{\n\t\t{[]string{\"42\"}},\n\t\t{[]string{\"7\"}},\n\t\t{[]string{\"8\", \"9\"}},\n\t\t{[][]string{{\"10\", \"11\"}, {\"12\", \"13\"}}},\n\t}\n\n\tcopyCount, err := conn.CopyFrom(ctx, pgx.Identifier{\"foo\"}, []string{\"a\"}, pgx.CopyFromRows(inputRows))\n\trequire.NoError(t, err)\n\trequire.EqualValues(t, len(inputRows), copyCount)\n\n\t// Test reads as int64 and flattened array for simplicity.\n\trows, _ := conn.Query(ctx, \"select * from foo\")\n\tnums, err := pgx.CollectRows(rows, pgx.RowTo[[]int64])\n\trequire.NoError(t, err)\n\trequire.Equal(t, [][]int64{{42}, {7}, {8, 9}, {10, 11, 12, 13}}, nums)\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestCopyFromFunc(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tmustExec(t, conn, `create temporary table foo(\n\t\ta int\n\t)`)\n\n\tdataCh := make(chan int, 1)\n\n\tconst channelItems = 10\n\tgo func() {\n\t\tfor i := 0; i < channelItems; i++ {\n\t\t\tdataCh <- i\n\t\t}\n\t\tclose(dataCh)\n\t}()\n\n\tcopyCount, err := conn.CopyFrom(context.Background(), pgx.Identifier{\"foo\"}, []string{\"a\"},\n\t\tpgx.CopyFromFunc(func() ([]any, error) {\n\t\t\tv, ok := <-dataCh\n\t\t\tif !ok {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\treturn []any{v}, nil\n\t\t}))\n\n\trequire.ErrorIs(t, err, nil)\n\trequire.EqualValues(t, channelItems, copyCount)\n\n\trows, err := conn.Query(context.Background(), \"select * from foo order by a\")\n\trequire.NoError(t, err)\n\tnums, err := pgx.CollectRows(rows, pgx.RowTo[int64])\n\trequire.NoError(t, err)\n\trequire.Equal(t, []int64{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, nums)\n\n\t// simulate a failure\n\tcopyCount, err = conn.CopyFrom(context.Background(), pgx.Identifier{\"foo\"}, []string{\"a\"},\n\t\tpgx.CopyFromFunc(func() func() ([]any, error) {\n\t\t\tx := 9\n\t\t\treturn func() ([]any, error) {\n\t\t\t\tx++\n\t\t\t\tif x > 100 {\n\t\t\t\t\treturn nil, fmt.Errorf(\"simulated error\")\n\t\t\t\t}\n\t\t\t\treturn []any{x}, nil\n\t\t\t}\n\t\t}()))\n\trequire.NotErrorIs(t, err, nil)\n\trequire.EqualValues(t, 0, copyCount) // no change, due to error\n\n\tensureConnValid(t, conn)\n}\n"
        },
        {
          "name": "derived_types.go",
          "type": "blob",
          "size": 9.50390625,
          "content": "package pgx\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/jackc/pgx/v5/pgtype\"\n)\n\n/*\nbuildLoadDerivedTypesSQL generates the correct query for retrieving type information.\n\n\tpgVersion: the major version of the PostgreSQL server\n\ttypeNames: the names of the types to load. If nil, load all types.\n*/\nfunc buildLoadDerivedTypesSQL(pgVersion int64, typeNames []string) string {\n\tsupportsMultirange := (pgVersion >= 14)\n\tvar typeNamesClause string\n\n\tif typeNames == nil {\n\t\t// This should not occur; this will not return any types\n\t\ttypeNamesClause = \"= ''\"\n\t} else {\n\t\ttypeNamesClause = \"= ANY($1)\"\n\t}\n\tparts := make([]string, 0, 10)\n\n\t// Each of the type names provided might be found in pg_class or pg_type.\n\t// Additionally, it may or may not include a schema portion.\n\tparts = append(parts, `\nWITH RECURSIVE\n-- find the OIDs in pg_class which match one of the provided type names\nselected_classes(oid,reltype) AS (\n    -- this query uses the namespace search path, so will match type names without a schema prefix\n    SELECT pg_class.oid, pg_class.reltype\n    FROM pg_catalog.pg_class\n        LEFT JOIN pg_catalog.pg_namespace n ON n.oid = pg_class.relnamespace\n    WHERE pg_catalog.pg_table_is_visible(pg_class.oid)\n      AND relname `, typeNamesClause, `\nUNION ALL\n    -- this query will only match type names which include the schema prefix\n    SELECT pg_class.oid, pg_class.reltype\n    FROM pg_class\n    INNER JOIN pg_namespace ON (pg_class.relnamespace = pg_namespace.oid)\n    WHERE nspname || '.' || relname `, typeNamesClause, `\n),\nselected_types(oid) AS (\n    -- collect the OIDs from pg_types which correspond to the selected classes\n    SELECT reltype AS oid\n    FROM selected_classes\nUNION ALL\n    -- as well as any other type names which match our criteria\n    SELECT pg_type.oid\n    FROM pg_type\n    LEFT OUTER JOIN pg_namespace ON (pg_type.typnamespace = pg_namespace.oid)\n    WHERE typname `, typeNamesClause, `\n        OR nspname || '.' || typname `, typeNamesClause, `\n),\n-- this builds a parent/child mapping of objects, allowing us to know\n-- all the child (ie: dependent) types that a parent (type) requires\n-- As can be seen, there are 3 ways this can occur (the last of which\n-- is due to being a composite class, where the composite fields are children)\npc(parent, child) AS (\n    SELECT parent.oid, parent.typelem\n    FROM pg_type parent\n    WHERE parent.typtype = 'b' AND parent.typelem != 0\nUNION ALL\n    SELECT parent.oid, parent.typbasetype\n    FROM pg_type parent\n    WHERE parent.typtypmod = -1 AND parent.typbasetype != 0\nUNION ALL\n    SELECT pg_type.oid, atttypid\n    FROM pg_attribute\n    INNER JOIN pg_class ON (pg_class.oid = pg_attribute.attrelid)\n    INNER JOIN pg_type ON (pg_type.oid = pg_class.reltype)\n    WHERE NOT attisdropped\n      AND attnum > 0\n),\n-- Now construct a recursive query which includes a 'depth' element.\n-- This is used to ensure that the \"youngest\" children are registered before\n-- their parents.\nrelationships(parent, child, depth) AS (\n    SELECT DISTINCT 0::OID, selected_types.oid, 0\n    FROM selected_types\nUNION ALL\n    SELECT pg_type.oid AS parent, pg_attribute.atttypid AS child, 1\n    FROM selected_classes c\n    inner join pg_type ON (c.reltype = pg_type.oid)\n    inner join pg_attribute on (c.oid = pg_attribute.attrelid)\nUNION ALL\n    SELECT pc.parent, pc.child, relationships.depth + 1\n    FROM pc\n    INNER JOIN relationships ON (pc.parent = relationships.child)\n),\n-- composite fields need to be encapsulated as a couple of arrays to provide the required information for registration\ncomposite AS (\n    SELECT pg_type.oid, ARRAY_AGG(attname ORDER BY attnum) AS attnames, ARRAY_AGG(atttypid ORDER BY ATTNUM) AS atttypids\n    FROM pg_attribute\n    INNER JOIN pg_class ON (pg_class.oid = pg_attribute.attrelid)\n    INNER JOIN pg_type ON (pg_type.oid = pg_class.reltype)\n    WHERE NOT attisdropped\n      AND attnum > 0\n    GROUP BY pg_type.oid\n)\n-- Bring together this information, showing all the information which might possibly be required\n-- to complete the registration, applying filters to only show the items which relate to the selected\n-- types/classes.\nSELECT typname,\n       pg_namespace.nspname,\n       typtype,\n       typbasetype,\n       typelem,\n       pg_type.oid,`)\n\tif supportsMultirange {\n\t\tparts = append(parts, `\n       COALESCE(multirange.rngtypid, 0) AS rngtypid,`)\n\t} else {\n\t\tparts = append(parts, `\n       0 AS rngtypid,`)\n\t}\n\tparts = append(parts, `\n       COALESCE(pg_range.rngsubtype, 0) AS rngsubtype,\n       attnames, atttypids\n    FROM relationships\n    INNER JOIN pg_type ON (pg_type.oid = relationships.child)\n    LEFT OUTER JOIN pg_range ON (pg_type.oid = pg_range.rngtypid)`)\n\tif supportsMultirange {\n\t\tparts = append(parts, `\n    LEFT OUTER JOIN pg_range multirange ON (pg_type.oid = multirange.rngmultitypid)`)\n\t}\n\n\tparts = append(parts, `\n    LEFT OUTER JOIN composite USING (oid)\n    LEFT OUTER JOIN pg_namespace ON (pg_type.typnamespace = pg_namespace.oid)\n    WHERE NOT (typtype = 'b' AND typelem = 0)`)\n\tparts = append(parts, `\n    GROUP BY typname, pg_namespace.nspname, typtype, typbasetype, typelem, pg_type.oid, pg_range.rngsubtype,`)\n\tif supportsMultirange {\n\t\tparts = append(parts, `\n        multirange.rngtypid,`)\n\t}\n\tparts = append(parts, `\n        attnames, atttypids\n    ORDER BY MAX(depth) desc, typname;`)\n\treturn strings.Join(parts, \"\")\n}\n\ntype derivedTypeInfo struct {\n\tOid, Typbasetype, Typelem, Rngsubtype, Rngtypid uint32\n\tTypeName, Typtype, NspName                      string\n\tAttnames                                        []string\n\tAtttypids                                       []uint32\n}\n\n// LoadTypes performs a single (complex) query, returning all the required\n// information to register the named types, as well as any other types directly\n// or indirectly required to complete the registration.\n// The result of this call can be passed into RegisterTypes to complete the process.\nfunc (c *Conn) LoadTypes(ctx context.Context, typeNames []string) ([]*pgtype.Type, error) {\n\tm := c.TypeMap()\n\tif len(typeNames) == 0 {\n\t\treturn nil, fmt.Errorf(\"No type names were supplied.\")\n\t}\n\n\t// Disregard server version errors. This will result in\n\t// the SQL not support recent structures such as multirange\n\tserverVersion, _ := serverVersion(c)\n\tsql := buildLoadDerivedTypesSQL(serverVersion, typeNames)\n\trows, err := c.Query(ctx, sql, QueryExecModeSimpleProtocol, typeNames)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"While generating load types query: %w\", err)\n\t}\n\tdefer rows.Close()\n\tresult := make([]*pgtype.Type, 0, 100)\n\tfor rows.Next() {\n\t\tti := derivedTypeInfo{}\n\t\terr = rows.Scan(&ti.TypeName, &ti.NspName, &ti.Typtype, &ti.Typbasetype, &ti.Typelem, &ti.Oid, &ti.Rngtypid, &ti.Rngsubtype, &ti.Attnames, &ti.Atttypids)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"While scanning type information: %w\", err)\n\t\t}\n\t\tvar type_ *pgtype.Type\n\t\tswitch ti.Typtype {\n\t\tcase \"b\": // array\n\t\t\tdt, ok := m.TypeForOID(ti.Typelem)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(\"Array element OID %v not registered while loading pgtype %q\", ti.Typelem, ti.TypeName)\n\t\t\t}\n\t\t\ttype_ = &pgtype.Type{Name: ti.TypeName, OID: ti.Oid, Codec: &pgtype.ArrayCodec{ElementType: dt}}\n\t\tcase \"c\": // composite\n\t\t\tvar fields []pgtype.CompositeCodecField\n\t\t\tfor i, fieldName := range ti.Attnames {\n\t\t\t\tdt, ok := m.TypeForOID(ti.Atttypids[i])\n\t\t\t\tif !ok {\n\t\t\t\t\treturn nil, fmt.Errorf(\"Unknown field for composite type %q:  field %q (OID %v) is not already registered.\", ti.TypeName, fieldName, ti.Atttypids[i])\n\t\t\t\t}\n\t\t\t\tfields = append(fields, pgtype.CompositeCodecField{Name: fieldName, Type: dt})\n\t\t\t}\n\n\t\t\ttype_ = &pgtype.Type{Name: ti.TypeName, OID: ti.Oid, Codec: &pgtype.CompositeCodec{Fields: fields}}\n\t\tcase \"d\": // domain\n\t\t\tdt, ok := m.TypeForOID(ti.Typbasetype)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(\"Domain base type OID %v was not already registered, needed for %q\", ti.Typbasetype, ti.TypeName)\n\t\t\t}\n\n\t\t\ttype_ = &pgtype.Type{Name: ti.TypeName, OID: ti.Oid, Codec: dt.Codec}\n\t\tcase \"e\": // enum\n\t\t\ttype_ = &pgtype.Type{Name: ti.TypeName, OID: ti.Oid, Codec: &pgtype.EnumCodec{}}\n\t\tcase \"r\": // range\n\t\t\tdt, ok := m.TypeForOID(ti.Rngsubtype)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(\"Range element OID %v was not already registered, needed for %q\", ti.Rngsubtype, ti.TypeName)\n\t\t\t}\n\n\t\t\ttype_ = &pgtype.Type{Name: ti.TypeName, OID: ti.Oid, Codec: &pgtype.RangeCodec{ElementType: dt}}\n\t\tcase \"m\": // multirange\n\t\t\tdt, ok := m.TypeForOID(ti.Rngtypid)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(\"Multirange element OID %v was not already registered, needed for %q\", ti.Rngtypid, ti.TypeName)\n\t\t\t}\n\n\t\t\ttype_ = &pgtype.Type{Name: ti.TypeName, OID: ti.Oid, Codec: &pgtype.MultirangeCodec{ElementType: dt}}\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"Unknown typtype %q was found while registering %q\", ti.Typtype, ti.TypeName)\n\t\t}\n\n\t\t// the type_ is imposible to be null\n\t\tm.RegisterType(type_)\n\t\tif ti.NspName != \"\" {\n\t\t\tnspType := &pgtype.Type{Name: ti.NspName + \".\" + type_.Name, OID: type_.OID, Codec: type_.Codec}\n\t\t\tm.RegisterType(nspType)\n\t\t\tresult = append(result, nspType)\n\t\t}\n\t\tresult = append(result, type_)\n\t}\n\treturn result, nil\n}\n\n// serverVersion returns the postgresql server version.\nfunc serverVersion(c *Conn) (int64, error) {\n\tserverVersionStr := c.PgConn().ParameterStatus(\"server_version\")\n\tserverVersionStr = regexp.MustCompile(`^[0-9]+`).FindString(serverVersionStr)\n\t// if not PostgreSQL do nothing\n\tif serverVersionStr == \"\" {\n\t\treturn 0, fmt.Errorf(\"Cannot identify server version in %q\", serverVersionStr)\n\t}\n\n\tversion, err := strconv.ParseInt(serverVersionStr, 10, 64)\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"postgres version parsing failed: %w\", err)\n\t}\n\treturn version, nil\n}\n"
        },
        {
          "name": "derived_types_test.go",
          "type": "blob",
          "size": 1.1953125,
          "content": "package pgx_test\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestCompositeCodecTranscodeWithLoadTypes(t *testing.T) {\n\tskipCockroachDB(t, \"Server does not support composite types (see https://github.com/cockroachdb/cockroach/issues/27792)\")\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\t_, err := conn.Exec(ctx, `\ndrop type if exists dtype_test;\ndrop domain if exists anotheruint64;\n\ncreate domain anotheruint64 as numeric(20,0);\ncreate type dtype_test as (\n  a text,\n  b int4,\n  c anotheruint64,\n  d anotheruint64[]\n);`)\n\t\trequire.NoError(t, err)\n\t\tdefer conn.Exec(ctx, \"drop type dtype_test\")\n\t\tdefer conn.Exec(ctx, \"drop domain anotheruint64\")\n\n\t\ttypes, err := conn.LoadTypes(ctx, []string{\"dtype_test\"})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, types, 6)\n\t\trequire.Equal(t, types[0].Name, \"public.anotheruint64\")\n\t\trequire.Equal(t, types[1].Name, \"anotheruint64\")\n\t\trequire.Equal(t, types[2].Name, \"public._anotheruint64\")\n\t\trequire.Equal(t, types[3].Name, \"_anotheruint64\")\n\t\trequire.Equal(t, types[4].Name, \"public.dtype_test\")\n\t\trequire.Equal(t, types[5].Name, \"dtype_test\")\n\t})\n}\n"
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 7.0107421875,
          "content": "// Package pgx is a PostgreSQL database driver.\n/*\npgx provides a native PostgreSQL driver and can act as a database/sql driver. The native PostgreSQL interface is similar\nto the database/sql interface while providing better speed and access to PostgreSQL specific features. Use\ngithub.com/jackc/pgx/v5/stdlib to use pgx as a database/sql compatible driver. See that package's documentation for\ndetails.\n\nEstablishing a Connection\n\nThe primary way of establishing a connection is with [pgx.Connect]:\n\n    conn, err := pgx.Connect(context.Background(), os.Getenv(\"DATABASE_URL\"))\n\nThe database connection string can be in URL or key/value format. Both PostgreSQL settings and pgx settings can be\nspecified here. In addition, a config struct can be created by [ParseConfig] and modified before establishing the\nconnection with [ConnectConfig] to configure settings such as tracing that cannot be configured with a connection\nstring.\n\nConnection Pool\n\n[*pgx.Conn] represents a single connection to the database and is not concurrency safe. Use package\ngithub.com/jackc/pgx/v5/pgxpool for a concurrency safe connection pool.\n\nQuery Interface\n\npgx implements Query in the familiar database/sql style. However, pgx provides generic functions such as CollectRows and\nForEachRow that are a simpler and safer way of processing rows than manually calling defer rows.Close(), rows.Next(),\nrows.Scan, and rows.Err().\n\nCollectRows can be used collect all returned rows into a slice.\n\n    rows, _ := conn.Query(context.Background(), \"select generate_series(1,$1)\", 5)\n    numbers, err := pgx.CollectRows(rows, pgx.RowTo[int32])\n    if err != nil {\n      return err\n    }\n    // numbers => [1 2 3 4 5]\n\nForEachRow can be used to execute a callback function for every row. This is often easier than iterating over rows\ndirectly.\n\n    var sum, n int32\n    rows, _ := conn.Query(context.Background(), \"select generate_series(1,$1)\", 10)\n    _, err := pgx.ForEachRow(rows, []any{&n}, func() error {\n      sum += n\n      return nil\n    })\n    if err != nil {\n      return err\n    }\n\npgx also implements QueryRow in the same style as database/sql.\n\n    var name string\n    var weight int64\n    err := conn.QueryRow(context.Background(), \"select name, weight from widgets where id=$1\", 42).Scan(&name, &weight)\n    if err != nil {\n        return err\n    }\n\nUse Exec to execute a query that does not return a result set.\n\n    commandTag, err := conn.Exec(context.Background(), \"delete from widgets where id=$1\", 42)\n    if err != nil {\n        return err\n    }\n    if commandTag.RowsAffected() != 1 {\n        return errors.New(\"No row found to delete\")\n    }\n\nPostgreSQL Data Types\n\npgx uses the pgtype package to converting Go values to and from PostgreSQL values. It supports many PostgreSQL types\ndirectly and is customizable and extendable. User defined data types such as enums, domains,  and composite types may\nrequire type registration. See that package's documentation for details.\n\nTransactions\n\nTransactions are started by calling Begin.\n\n    tx, err := conn.Begin(context.Background())\n    if err != nil {\n        return err\n    }\n    // Rollback is safe to call even if the tx is already closed, so if\n    // the tx commits successfully, this is a no-op\n    defer tx.Rollback(context.Background())\n\n    _, err = tx.Exec(context.Background(), \"insert into foo(id) values (1)\")\n    if err != nil {\n        return err\n    }\n\n    err = tx.Commit(context.Background())\n    if err != nil {\n        return err\n    }\n\nThe Tx returned from Begin also implements the Begin method. This can be used to implement pseudo nested transactions.\nThese are internally implemented with savepoints.\n\nUse BeginTx to control the transaction mode. BeginTx also can be used to ensure a new transaction is created instead of\na pseudo nested transaction.\n\nBeginFunc and BeginTxFunc are functions that begin a transaction, execute a function, and commit or rollback the\ntransaction depending on the return value of the function. These can be simpler and less error prone to use.\n\n    err = pgx.BeginFunc(context.Background(), conn, func(tx pgx.Tx) error {\n        _, err := tx.Exec(context.Background(), \"insert into foo(id) values (1)\")\n        return err\n    })\n    if err != nil {\n        return err\n    }\n\nPrepared Statements\n\nPrepared statements can be manually created with the Prepare method. However, this is rarely necessary because pgx\nincludes an automatic statement cache by default. Queries run through the normal Query, QueryRow, and Exec functions are\nautomatically prepared on first execution and the prepared statement is reused on subsequent executions. See ParseConfig\nfor information on how to customize or disable the statement cache.\n\nCopy Protocol\n\nUse CopyFrom to efficiently insert multiple rows at a time using the PostgreSQL copy protocol. CopyFrom accepts a\nCopyFromSource interface. If the data is already in a [][]any use CopyFromRows to wrap it in a CopyFromSource interface.\nOr implement CopyFromSource to avoid buffering the entire data set in memory.\n\n    rows := [][]any{\n        {\"John\", \"Smith\", int32(36)},\n        {\"Jane\", \"Doe\", int32(29)},\n    }\n\n    copyCount, err := conn.CopyFrom(\n        context.Background(),\n        pgx.Identifier{\"people\"},\n        []string{\"first_name\", \"last_name\", \"age\"},\n        pgx.CopyFromRows(rows),\n    )\n\nWhen you already have a typed array using CopyFromSlice can be more convenient.\n\n    rows := []User{\n        {\"John\", \"Smith\", 36},\n        {\"Jane\", \"Doe\", 29},\n    }\n\n    copyCount, err := conn.CopyFrom(\n        context.Background(),\n        pgx.Identifier{\"people\"},\n        []string{\"first_name\", \"last_name\", \"age\"},\n        pgx.CopyFromSlice(len(rows), func(i int) ([]any, error) {\n            return []any{rows[i].FirstName, rows[i].LastName, rows[i].Age}, nil\n        }),\n    )\n\nCopyFrom can be faster than an insert with as few as 5 rows.\n\nListen and Notify\n\npgx can listen to the PostgreSQL notification system with the `Conn.WaitForNotification` method. It blocks until a\nnotification is received or the context is canceled.\n\n    _, err := conn.Exec(context.Background(), \"listen channelname\")\n    if err != nil {\n        return err\n    }\n\n    notification, err := conn.WaitForNotification(context.Background())\n    if err != nil {\n        return err\n    }\n    // do something with notification\n\n\nTracing and Logging\n\npgx supports tracing by setting ConnConfig.Tracer. To combine several tracers you can use the multitracer.Tracer.\n\nIn addition, the tracelog package provides the TraceLog type which lets a traditional logger act as a Tracer.\n\nFor debug tracing of the actual PostgreSQL wire protocol messages see github.com/jackc/pgx/v5/pgproto3.\n\nLower Level PostgreSQL Functionality\n\ngithub.com/jackc/pgx/v5/pgconn contains a lower level PostgreSQL driver roughly at the level of libpq. pgx.Conn in\nimplemented on top of pgconn. The Conn.PgConn() method can be used to access this lower layer.\n\nPgBouncer\n\nBy default pgx automatically uses prepared statements. Prepared statements are incompatible with PgBouncer. This can be\ndisabled by setting a different QueryExecMode in ConnConfig.DefaultQueryExecMode.\n*/\npackage pgx\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "extended_query_builder.go",
          "type": "blob",
          "size": 3.8037109375,
          "content": "package pgx\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgtype\"\n)\n\n// ExtendedQueryBuilder is used to choose the parameter formats, to format the parameters and to choose the result\n// formats for an extended query.\ntype ExtendedQueryBuilder struct {\n\tParamValues     [][]byte\n\tparamValueBytes []byte\n\tParamFormats    []int16\n\tResultFormats   []int16\n}\n\n// Build sets ParamValues, ParamFormats, and ResultFormats for use with *PgConn.ExecParams or *PgConn.ExecPrepared. If\n// sd is nil then QueryExecModeExec behavior will be used.\nfunc (eqb *ExtendedQueryBuilder) Build(m *pgtype.Map, sd *pgconn.StatementDescription, args []any) error {\n\teqb.reset()\n\n\tif sd == nil {\n\t\tfor i := range args {\n\t\t\terr := eqb.appendParam(m, 0, pgtype.TextFormatCode, args[i])\n\t\t\tif err != nil {\n\t\t\t\terr = fmt.Errorf(\"failed to encode args[%d]: %w\", i, err)\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\n\tif len(sd.ParamOIDs) != len(args) {\n\t\treturn fmt.Errorf(\"mismatched param and argument count\")\n\t}\n\n\tfor i := range args {\n\t\terr := eqb.appendParam(m, sd.ParamOIDs[i], -1, args[i])\n\t\tif err != nil {\n\t\t\terr = fmt.Errorf(\"failed to encode args[%d]: %w\", i, err)\n\t\t\treturn err\n\t\t}\n\t}\n\n\tfor i := range sd.Fields {\n\t\teqb.appendResultFormat(m.FormatCodeForOID(sd.Fields[i].DataTypeOID))\n\t}\n\n\treturn nil\n}\n\n// appendParam appends a parameter to the query. format may be -1 to automatically choose the format. If arg is nil it\n// must be an untyped nil.\nfunc (eqb *ExtendedQueryBuilder) appendParam(m *pgtype.Map, oid uint32, format int16, arg any) error {\n\tif format == -1 {\n\t\tpreferredFormat := eqb.chooseParameterFormatCode(m, oid, arg)\n\t\tpreferredErr := eqb.appendParam(m, oid, preferredFormat, arg)\n\t\tif preferredErr == nil {\n\t\t\treturn nil\n\t\t}\n\n\t\tvar otherFormat int16\n\t\tif preferredFormat == TextFormatCode {\n\t\t\totherFormat = BinaryFormatCode\n\t\t} else {\n\t\t\totherFormat = TextFormatCode\n\t\t}\n\n\t\totherErr := eqb.appendParam(m, oid, otherFormat, arg)\n\t\tif otherErr == nil {\n\t\t\treturn nil\n\t\t}\n\n\t\treturn preferredErr // return the error from the preferred format\n\t}\n\n\tv, err := eqb.encodeExtendedParamValue(m, oid, format, arg)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\teqb.ParamFormats = append(eqb.ParamFormats, format)\n\teqb.ParamValues = append(eqb.ParamValues, v)\n\n\treturn nil\n}\n\n// appendResultFormat appends a result format to the query.\nfunc (eqb *ExtendedQueryBuilder) appendResultFormat(format int16) {\n\teqb.ResultFormats = append(eqb.ResultFormats, format)\n}\n\n// reset readies eqb to build another query.\nfunc (eqb *ExtendedQueryBuilder) reset() {\n\teqb.ParamValues = eqb.ParamValues[0:0]\n\teqb.paramValueBytes = eqb.paramValueBytes[0:0]\n\teqb.ParamFormats = eqb.ParamFormats[0:0]\n\teqb.ResultFormats = eqb.ResultFormats[0:0]\n\n\tif cap(eqb.ParamValues) > 64 {\n\t\teqb.ParamValues = make([][]byte, 0, 64)\n\t}\n\n\tif cap(eqb.paramValueBytes) > 256 {\n\t\teqb.paramValueBytes = make([]byte, 0, 256)\n\t}\n\n\tif cap(eqb.ParamFormats) > 64 {\n\t\teqb.ParamFormats = make([]int16, 0, 64)\n\t}\n\tif cap(eqb.ResultFormats) > 64 {\n\t\teqb.ResultFormats = make([]int16, 0, 64)\n\t}\n}\n\nfunc (eqb *ExtendedQueryBuilder) encodeExtendedParamValue(m *pgtype.Map, oid uint32, formatCode int16, arg any) ([]byte, error) {\n\tif eqb.paramValueBytes == nil {\n\t\teqb.paramValueBytes = make([]byte, 0, 128)\n\t}\n\n\tpos := len(eqb.paramValueBytes)\n\n\tbuf, err := m.Encode(oid, formatCode, arg, eqb.paramValueBytes)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif buf == nil {\n\t\treturn nil, nil\n\t}\n\teqb.paramValueBytes = buf\n\treturn eqb.paramValueBytes[pos:], nil\n}\n\n// chooseParameterFormatCode determines the correct format code for an\n// argument to a prepared statement. It defaults to TextFormatCode if no\n// determination can be made.\nfunc (eqb *ExtendedQueryBuilder) chooseParameterFormatCode(m *pgtype.Map, oid uint32, arg any) int16 {\n\tswitch arg.(type) {\n\tcase string, *string:\n\t\treturn TextFormatCode\n\t}\n\n\treturn m.FormatCodeForOID(oid)\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.55078125,
          "content": "module github.com/jackc/pgx/v5\n\ngo 1.21\n\nrequire (\n\tgithub.com/jackc/pgpassfile v1.0.0\n\tgithub.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761\n\tgithub.com/jackc/puddle/v2 v2.2.2\n\tgithub.com/stretchr/testify v1.8.1\n\tgolang.org/x/crypto v0.31.0\n\tgolang.org/x/sync v0.10.0\n\tgolang.org/x/text v0.21.0\n)\n\nrequire (\n\tgithub.com/davecgh/go-spew v1.1.1 // indirect\n\tgithub.com/kr/pretty v0.3.0 // indirect\n\tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n\tgopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c // indirect\n\tgopkg.in/yaml.v3 v3.0.1 // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 3.85546875,
          "content": "github.com/creack/pty v1.1.9/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/jackc/pgpassfile v1.0.0 h1:/6Hmqy13Ss2zCq62VdNG8tM1wchn8zjSGOBJ6icpsIM=\ngithub.com/jackc/pgpassfile v1.0.0/go.mod h1:CEx0iS5ambNFdcRtxPj5JhEz+xB6uRky5eyVu/W2HEg=\ngithub.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 h1:iCEnooe7UlwOQYpKFhBabPMi4aNAfoODPEFNiAnClxo=\ngithub.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761/go.mod h1:5TJZWKEWniPve33vlWYSoGYefn3gLQRzjfDlhSJ9ZKM=\ngithub.com/jackc/puddle/v2 v2.2.2 h1:PR8nw+E/1w0GLuRFSmiioY6UooMp6KJv0/61nB7icHo=\ngithub.com/jackc/puddle/v2 v2.2.2/go.mod h1:vriiEXHvEE654aYKXXjOvZM39qJ0q+azkZFrfEOc3H4=\ngithub.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\ngithub.com/kr/pretty v0.2.1/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=\ngithub.com/kr/pretty v0.3.0 h1:WgNl7dwNpEZ6jJ9k1snq4pZsg7DOEN8hP9Xw0Tsjwk0=\ngithub.com/kr/pretty v0.3.0/go.mod h1:640gp4NfQd8pI5XOwp5fnNeVWj67G7CFk/SaSQn7NBk=\ngithub.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\ngithub.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\ngithub.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=\ngithub.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/rogpeppe/go-internal v1.6.1 h1:/FiVV8dS/e+YqF2JvO3yXRFbBLTIuSDkuC7aBOAvL+k=\ngithub.com/rogpeppe/go-internal v1.6.1/go.mod h1:xXDCJY+GAPziupqXw64V24skbSoqbTEfhy4qGm1nDQc=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=\ngithub.com/stretchr/objx v0.5.0/go.mod h1:Yh+to48EsGEfYuaHDzXPcE3xhTkx73EhmCGUpEOglKo=\ngithub.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\ngithub.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=\ngithub.com/stretchr/testify v1.8.1 h1:w7B6lhMri9wdJUVmEZPGGhZzrYTPvgJArz7wNPgYKsk=\ngithub.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=\ngolang.org/x/crypto v0.31.0 h1:ihbySMvVjLAeSH1IbfcRTkD/iNscyz8rGzjF/E5hV6U=\ngolang.org/x/crypto v0.31.0/go.mod h1:kDsLvtWBEx7MV9tJOj9bnXsPbxwJQ6csT/x4KIN4Ssk=\ngolang.org/x/sync v0.10.0 h1:3NQrjDixjgGwUOCaF8w2+VYHv0Ve/vGYSbdkTa98gmQ=\ngolang.org/x/sync v0.10.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\ngolang.org/x/text v0.21.0 h1:zyQAAkrwaneQ066sspRyJaG9VNi/YJ1NfzcGB3hZ/qo=\ngolang.org/x/text v0.21.0/go.mod h1:4IBbMaMmOPCJ8SecivzSH54+73PCFmPWxNTLm+vZkEQ=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=\ngopkg.in/errgo.v2 v2.1.0/go.mod h1:hNsd1EY+bozCKY1Ytp96fpM3vjJbqLJn88ws8XvfDNI=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n"
        },
        {
          "name": "helper_test.go",
          "type": "blob",
          "size": 4.99609375,
          "content": "package pgx_test\n\nimport (\n\t\"context\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgxtest\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar defaultConnTestRunner pgxtest.ConnTestRunner\n\nfunc init() {\n\tdefaultConnTestRunner = pgxtest.DefaultConnTestRunner()\n\tdefaultConnTestRunner.CreateConfig = func(ctx context.Context, t testing.TB) *pgx.ConnConfig {\n\t\tconfig, err := pgx.ParseConfig(os.Getenv(\"PGX_TEST_DATABASE\"))\n\t\trequire.NoError(t, err)\n\t\treturn config\n\t}\n}\n\nfunc mustConnectString(t testing.TB, connString string) *pgx.Conn {\n\tconn, err := pgx.Connect(context.Background(), connString)\n\tif err != nil {\n\t\tt.Fatalf(\"Unable to establish connection: %v\", err)\n\t}\n\treturn conn\n}\n\nfunc mustParseConfig(t testing.TB, connString string) *pgx.ConnConfig {\n\tconfig, err := pgx.ParseConfig(connString)\n\trequire.Nil(t, err)\n\treturn config\n}\n\nfunc mustConnect(t testing.TB, config *pgx.ConnConfig) *pgx.Conn {\n\tconn, err := pgx.ConnectConfig(context.Background(), config)\n\tif err != nil {\n\t\tt.Fatalf(\"Unable to establish connection: %v\", err)\n\t}\n\treturn conn\n}\n\nfunc closeConn(t testing.TB, conn *pgx.Conn) {\n\terr := conn.Close(context.Background())\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Close unexpectedly failed: %v\", err)\n\t}\n}\n\nfunc mustExec(t testing.TB, conn *pgx.Conn, sql string, arguments ...any) (commandTag pgconn.CommandTag) {\n\tvar err error\n\tif commandTag, err = conn.Exec(context.Background(), sql, arguments...); err != nil {\n\t\tt.Fatalf(\"Exec unexpectedly failed with %v: %v\", sql, err)\n\t}\n\treturn\n}\n\n// Do a simple query to ensure the connection is still usable\nfunc ensureConnValid(t testing.TB, conn *pgx.Conn) {\n\tvar sum, rowCount int32\n\n\trows, err := conn.Query(context.Background(), \"select generate_series(1,$1)\", 10)\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Query failed: %v\", err)\n\t}\n\tdefer rows.Close()\n\n\tfor rows.Next() {\n\t\tvar n int32\n\t\trows.Scan(&n)\n\t\tsum += n\n\t\trowCount++\n\t}\n\n\tif rows.Err() != nil {\n\t\tt.Fatalf(\"conn.Query failed: %v\", rows.Err())\n\t}\n\n\tif rowCount != 10 {\n\t\tt.Error(\"Select called onDataRow wrong number of times\")\n\t}\n\tif sum != 55 {\n\t\tt.Error(\"Wrong values returned\")\n\t}\n}\n\nfunc assertConfigsEqual(t *testing.T, expected, actual *pgx.ConnConfig, testName string) {\n\tif !assert.NotNil(t, expected) {\n\t\treturn\n\t}\n\tif !assert.NotNil(t, actual) {\n\t\treturn\n\t}\n\n\tassert.Equalf(t, expected.Tracer, actual.Tracer, \"%s - Tracer\", testName)\n\tassert.Equalf(t, expected.ConnString(), actual.ConnString(), \"%s - ConnString\", testName)\n\tassert.Equalf(t, expected.StatementCacheCapacity, actual.StatementCacheCapacity, \"%s - StatementCacheCapacity\", testName)\n\tassert.Equalf(t, expected.DescriptionCacheCapacity, actual.DescriptionCacheCapacity, \"%s - DescriptionCacheCapacity\", testName)\n\tassert.Equalf(t, expected.DefaultQueryExecMode, actual.DefaultQueryExecMode, \"%s - DefaultQueryExecMode\", testName)\n\tassert.Equalf(t, expected.Host, actual.Host, \"%s - Host\", testName)\n\tassert.Equalf(t, expected.Database, actual.Database, \"%s - Database\", testName)\n\tassert.Equalf(t, expected.Port, actual.Port, \"%s - Port\", testName)\n\tassert.Equalf(t, expected.User, actual.User, \"%s - User\", testName)\n\tassert.Equalf(t, expected.Password, actual.Password, \"%s - Password\", testName)\n\tassert.Equalf(t, expected.ConnectTimeout, actual.ConnectTimeout, \"%s - ConnectTimeout\", testName)\n\tassert.Equalf(t, expected.RuntimeParams, actual.RuntimeParams, \"%s - RuntimeParams\", testName)\n\n\t// Can't test function equality, so just test that they are set or not.\n\tassert.Equalf(t, expected.ValidateConnect == nil, actual.ValidateConnect == nil, \"%s - ValidateConnect\", testName)\n\tassert.Equalf(t, expected.AfterConnect == nil, actual.AfterConnect == nil, \"%s - AfterConnect\", testName)\n\n\tif assert.Equalf(t, expected.TLSConfig == nil, actual.TLSConfig == nil, \"%s - TLSConfig\", testName) {\n\t\tif expected.TLSConfig != nil {\n\t\t\tassert.Equalf(t, expected.TLSConfig.InsecureSkipVerify, actual.TLSConfig.InsecureSkipVerify, \"%s - TLSConfig InsecureSkipVerify\", testName)\n\t\t\tassert.Equalf(t, expected.TLSConfig.ServerName, actual.TLSConfig.ServerName, \"%s - TLSConfig ServerName\", testName)\n\t\t}\n\t}\n\n\tif assert.Equalf(t, len(expected.Fallbacks), len(actual.Fallbacks), \"%s - Fallbacks\", testName) {\n\t\tfor i := range expected.Fallbacks {\n\t\t\tassert.Equalf(t, expected.Fallbacks[i].Host, actual.Fallbacks[i].Host, \"%s - Fallback %d - Host\", testName, i)\n\t\t\tassert.Equalf(t, expected.Fallbacks[i].Port, actual.Fallbacks[i].Port, \"%s - Fallback %d - Port\", testName, i)\n\n\t\t\tif assert.Equalf(t, expected.Fallbacks[i].TLSConfig == nil, actual.Fallbacks[i].TLSConfig == nil, \"%s - Fallback %d - TLSConfig\", testName, i) {\n\t\t\t\tif expected.Fallbacks[i].TLSConfig != nil {\n\t\t\t\t\tassert.Equalf(t, expected.Fallbacks[i].TLSConfig.InsecureSkipVerify, actual.Fallbacks[i].TLSConfig.InsecureSkipVerify, \"%s - Fallback %d - TLSConfig InsecureSkipVerify\", testName)\n\t\t\t\t\tassert.Equalf(t, expected.Fallbacks[i].TLSConfig.ServerName, actual.Fallbacks[i].TLSConfig.ServerName, \"%s - Fallback %d - TLSConfig ServerName\", testName)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "large_objects.go",
          "type": "blob",
          "size": 4.4970703125,
          "content": "package pgx\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"io\"\n\n\t\"github.com/jackc/pgx/v5/pgtype\"\n)\n\n// The PostgreSQL wire protocol has a limit of 1 GB - 1 per message. See definition of\n// PQ_LARGE_MESSAGE_LIMIT in the PostgreSQL source code. To allow for the other data\n// in the message,maxLargeObjectMessageLength should be no larger than 1 GB - 1 KB.\nvar maxLargeObjectMessageLength = 1024*1024*1024 - 1024\n\n// LargeObjects is a structure used to access the large objects API. It is only valid within the transaction where it\n// was created.\n//\n// For more details see: http://www.postgresql.org/docs/current/static/largeobjects.html\ntype LargeObjects struct {\n\ttx Tx\n}\n\ntype LargeObjectMode int32\n\nconst (\n\tLargeObjectModeWrite LargeObjectMode = 0x20000\n\tLargeObjectModeRead  LargeObjectMode = 0x40000\n)\n\n// Create creates a new large object. If oid is zero, the server assigns an unused OID.\nfunc (o *LargeObjects) Create(ctx context.Context, oid uint32) (uint32, error) {\n\terr := o.tx.QueryRow(ctx, \"select lo_create($1)\", oid).Scan(&oid)\n\treturn oid, err\n}\n\n// Open opens an existing large object with the given mode. ctx will also be used for all operations on the opened large\n// object.\nfunc (o *LargeObjects) Open(ctx context.Context, oid uint32, mode LargeObjectMode) (*LargeObject, error) {\n\tvar fd int32\n\terr := o.tx.QueryRow(ctx, \"select lo_open($1, $2)\", oid, mode).Scan(&fd)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &LargeObject{fd: fd, tx: o.tx, ctx: ctx}, nil\n}\n\n// Unlink removes a large object from the database.\nfunc (o *LargeObjects) Unlink(ctx context.Context, oid uint32) error {\n\tvar result int32\n\terr := o.tx.QueryRow(ctx, \"select lo_unlink($1)\", oid).Scan(&result)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif result != 1 {\n\t\treturn errors.New(\"failed to remove large object\")\n\t}\n\n\treturn nil\n}\n\n// A LargeObject is a large object stored on the server. It is only valid within the transaction that it was initialized\n// in. It uses the context it was initialized with for all operations. It implements these interfaces:\n//\n//\tio.Writer\n//\tio.Reader\n//\tio.Seeker\n//\tio.Closer\ntype LargeObject struct {\n\tctx context.Context\n\ttx  Tx\n\tfd  int32\n}\n\n// Write writes p to the large object and returns the number of bytes written and an error if not all of p was written.\nfunc (o *LargeObject) Write(p []byte) (int, error) {\n\tnTotal := 0\n\tfor {\n\t\texpected := len(p) - nTotal\n\t\tif expected == 0 {\n\t\t\tbreak\n\t\t} else if expected > maxLargeObjectMessageLength {\n\t\t\texpected = maxLargeObjectMessageLength\n\t\t}\n\n\t\tvar n int\n\t\terr := o.tx.QueryRow(o.ctx, \"select lowrite($1, $2)\", o.fd, p[nTotal:nTotal+expected]).Scan(&n)\n\t\tif err != nil {\n\t\t\treturn nTotal, err\n\t\t}\n\n\t\tif n < 0 {\n\t\t\treturn nTotal, errors.New(\"failed to write to large object\")\n\t\t}\n\n\t\tnTotal += n\n\n\t\tif n < expected {\n\t\t\treturn nTotal, errors.New(\"short write to large object\")\n\t\t} else if n > expected {\n\t\t\treturn nTotal, errors.New(\"invalid write to large object\")\n\t\t}\n\t}\n\n\treturn nTotal, nil\n}\n\n// Read reads up to len(p) bytes into p returning the number of bytes read.\nfunc (o *LargeObject) Read(p []byte) (int, error) {\n\tnTotal := 0\n\tfor {\n\t\texpected := len(p) - nTotal\n\t\tif expected == 0 {\n\t\t\tbreak\n\t\t} else if expected > maxLargeObjectMessageLength {\n\t\t\texpected = maxLargeObjectMessageLength\n\t\t}\n\n\t\tres := pgtype.PreallocBytes(p[nTotal:])\n\t\terr := o.tx.QueryRow(o.ctx, \"select loread($1, $2)\", o.fd, expected).Scan(&res)\n\t\t// We compute expected so that it always fits into p, so it should never happen\n\t\t// that PreallocBytes's ScanBytes had to allocate a new slice.\n\t\tnTotal += len(res)\n\t\tif err != nil {\n\t\t\treturn nTotal, err\n\t\t}\n\n\t\tif len(res) < expected {\n\t\t\treturn nTotal, io.EOF\n\t\t} else if len(res) > expected {\n\t\t\treturn nTotal, errors.New(\"invalid read of large object\")\n\t\t}\n\t}\n\n\treturn nTotal, nil\n}\n\n// Seek moves the current location pointer to the new location specified by offset.\nfunc (o *LargeObject) Seek(offset int64, whence int) (n int64, err error) {\n\terr = o.tx.QueryRow(o.ctx, \"select lo_lseek64($1, $2, $3)\", o.fd, offset, whence).Scan(&n)\n\treturn n, err\n}\n\n// Tell returns the current read or write location of the large object descriptor.\nfunc (o *LargeObject) Tell() (n int64, err error) {\n\terr = o.tx.QueryRow(o.ctx, \"select lo_tell64($1)\", o.fd).Scan(&n)\n\treturn n, err\n}\n\n// Truncate the large object to size.\nfunc (o *LargeObject) Truncate(size int64) (err error) {\n\t_, err = o.tx.Exec(o.ctx, \"select lo_truncate64($1, $2)\", o.fd, size)\n\treturn err\n}\n\n// Close the large object descriptor.\nfunc (o *LargeObject) Close() error {\n\t_, err := o.tx.Exec(o.ctx, \"select lo_close($1)\", o.fd)\n\treturn err\n}\n"
        },
        {
          "name": "large_objects_private_test.go",
          "type": "blob",
          "size": 0.4462890625,
          "content": "package pgx\n\nimport (\n\t\"testing\"\n)\n\n// SetMaxLargeObjectMessageLength sets internal maxLargeObjectMessageLength variable\n// to the given length for the duration of the test.\n//\n// Tests using this helper should not use t.Parallel().\nfunc SetMaxLargeObjectMessageLength(t *testing.T, length int) {\n\tt.Helper()\n\n\toriginal := maxLargeObjectMessageLength\n\tt.Cleanup(func() {\n\t\tmaxLargeObjectMessageLength = original\n\t})\n\n\tmaxLargeObjectMessageLength = length\n}\n"
        },
        {
          "name": "large_objects_test.go",
          "type": "blob",
          "size": 5.7607421875,
          "content": "package pgx_test\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgxtest\"\n)\n\nfunc TestLargeObjects(t *testing.T) {\n\t// We use a very short limit to test chunking logic.\n\tpgx.SetMaxLargeObjectMessageLength(t, 2)\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn, err := pgx.Connect(ctx, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server does support large objects\")\n\n\ttx, err := conn.Begin(ctx)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttestLargeObjects(t, ctx, tx)\n}\n\nfunc TestLargeObjectsSimpleProtocol(t *testing.T) {\n\t// We use a very short limit to test chunking logic.\n\tpgx.SetMaxLargeObjectMessageLength(t, 2)\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconfig, err := pgx.ParseConfig(os.Getenv(\"PGX_TEST_DATABASE\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeSimpleProtocol\n\n\tconn, err := pgx.ConnectConfig(ctx, config)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server does support large objects\")\n\n\ttx, err := conn.Begin(ctx)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttestLargeObjects(t, ctx, tx)\n}\n\nfunc testLargeObjects(t *testing.T, ctx context.Context, tx pgx.Tx) {\n\tlo := tx.LargeObjects()\n\n\tid, err := lo.Create(ctx, 0)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tobj, err := lo.Open(ctx, id, pgx.LargeObjectModeRead|pgx.LargeObjectModeWrite)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tn, err := obj.Write([]byte(\"testing\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif n != 7 {\n\t\tt.Errorf(\"Expected n to be 7, got %d\", n)\n\t}\n\n\tpos, err := obj.Seek(1, 0)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif pos != 1 {\n\t\tt.Errorf(\"Expected pos to be 1, got %d\", pos)\n\t}\n\n\tres := make([]byte, 6)\n\tn, err = obj.Read(res)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif string(res) != \"esting\" {\n\t\tt.Errorf(`Expected res to be \"esting\", got %q`, res)\n\t}\n\tif n != 6 {\n\t\tt.Errorf(\"Expected n to be 6, got %d\", n)\n\t}\n\n\tn, err = obj.Read(res)\n\tif err != io.EOF {\n\t\tt.Error(\"Expected io.EOF, go nil\")\n\t}\n\tif n != 0 {\n\t\tt.Errorf(\"Expected n to be 0, got %d\", n)\n\t}\n\n\tpos, err = obj.Tell()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif pos != 7 {\n\t\tt.Errorf(\"Expected pos to be 7, got %d\", pos)\n\t}\n\n\terr = obj.Truncate(1)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tpos, err = obj.Seek(-1, 2)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif pos != 0 {\n\t\tt.Errorf(\"Expected pos to be 0, got %d\", pos)\n\t}\n\n\tres = make([]byte, 2)\n\tn, err = obj.Read(res)\n\tif err != io.EOF {\n\t\tt.Errorf(\"Expected err to be io.EOF, got %v\", err)\n\t}\n\tif n != 1 {\n\t\tt.Errorf(\"Expected n to be 1, got %d\", n)\n\t}\n\tif res[0] != 't' {\n\t\tt.Errorf(\"Expected res[0] to be 't', got %v\", res[0])\n\t}\n\n\terr = obj.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = lo.Unlink(ctx, id)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t_, err = lo.Open(ctx, id, pgx.LargeObjectModeRead)\n\tif e, ok := err.(*pgconn.PgError); !ok || e.Code != \"42704\" {\n\t\tt.Errorf(\"Expected undefined_object error (42704), got %#v\", err)\n\t}\n}\n\nfunc TestLargeObjectsMultipleTransactions(t *testing.T) {\n\t// We use a very short limit to test chunking logic.\n\tpgx.SetMaxLargeObjectMessageLength(t, 2)\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn, err := pgx.Connect(ctx, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server does support large objects\")\n\n\ttx, err := conn.Begin(ctx)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tlo := tx.LargeObjects()\n\n\tid, err := lo.Create(ctx, 0)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tobj, err := lo.Open(ctx, id, pgx.LargeObjectModeWrite)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tn, err := obj.Write([]byte(\"testing\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif n != 7 {\n\t\tt.Errorf(\"Expected n to be 7, got %d\", n)\n\t}\n\n\t// Commit the first transaction\n\terr = tx.Commit(ctx)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// IMPORTANT: Use the same connection for another query\n\tquery := `select n from generate_series(1,10) n`\n\trows, err := conn.Query(ctx, query)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\trows.Close()\n\n\t// Start a new transaction\n\ttx2, err := conn.Begin(ctx)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tlo2 := tx2.LargeObjects()\n\n\t// Reopen the large object in the new transaction\n\tobj2, err := lo2.Open(ctx, id, pgx.LargeObjectModeRead|pgx.LargeObjectModeWrite)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tpos, err := obj2.Seek(1, 0)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif pos != 1 {\n\t\tt.Errorf(\"Expected pos to be 1, got %d\", pos)\n\t}\n\n\tres := make([]byte, 6)\n\tn, err = obj2.Read(res)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif string(res) != \"esting\" {\n\t\tt.Errorf(`Expected res to be \"esting\", got %q`, res)\n\t}\n\tif n != 6 {\n\t\tt.Errorf(\"Expected n to be 6, got %d\", n)\n\t}\n\n\tn, err = obj2.Read(res)\n\tif err != io.EOF {\n\t\tt.Error(\"Expected io.EOF, go nil\")\n\t}\n\tif n != 0 {\n\t\tt.Errorf(\"Expected n to be 0, got %d\", n)\n\t}\n\n\tpos, err = obj2.Tell()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif pos != 7 {\n\t\tt.Errorf(\"Expected pos to be 7, got %d\", pos)\n\t}\n\n\terr = obj2.Truncate(1)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tpos, err = obj2.Seek(-1, 2)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif pos != 0 {\n\t\tt.Errorf(\"Expected pos to be 0, got %d\", pos)\n\t}\n\n\tres = make([]byte, 2)\n\tn, err = obj2.Read(res)\n\tif err != io.EOF {\n\t\tt.Errorf(\"Expected err to be io.EOF, got %v\", err)\n\t}\n\tif n != 1 {\n\t\tt.Errorf(\"Expected n to be 1, got %d\", n)\n\t}\n\tif res[0] != 't' {\n\t\tt.Errorf(\"Expected res[0] to be 't', got %v\", res[0])\n\t}\n\n\terr = obj2.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = lo2.Unlink(ctx, id)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t_, err = lo2.Open(ctx, id, pgx.LargeObjectModeRead)\n\tif e, ok := err.(*pgconn.PgError); !ok || e.Code != \"42704\" {\n\t\tt.Errorf(\"Expected undefined_object error (42704), got %#v\", err)\n\t}\n}\n"
        },
        {
          "name": "log",
          "type": "tree",
          "content": null
        },
        {
          "name": "multitracer",
          "type": "tree",
          "content": null
        },
        {
          "name": "named_args.go",
          "type": "blob",
          "size": 6.7841796875,
          "content": "package pgx\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\t\"unicode/utf8\"\n)\n\n// NamedArgs can be used as the first argument to a query method. It will replace every '@' named placeholder with a '$'\n// ordinal placeholder and construct the appropriate arguments.\n//\n// For example, the following two queries are equivalent:\n//\n//\tconn.Query(ctx, \"select * from widgets where foo = @foo and bar = @bar\", pgx.NamedArgs{\"foo\": 1, \"bar\": 2})\n//\tconn.Query(ctx, \"select * from widgets where foo = $1 and bar = $2\", 1, 2)\n//\n// Named placeholders are case sensitive and must start with a letter or underscore. Subsequent characters can be\n// letters, numbers, or underscores.\ntype NamedArgs map[string]any\n\n// RewriteQuery implements the QueryRewriter interface.\nfunc (na NamedArgs) RewriteQuery(ctx context.Context, conn *Conn, sql string, args []any) (newSQL string, newArgs []any, err error) {\n\treturn rewriteQuery(na, sql, false)\n}\n\n// StrictNamedArgs can be used in the same way as NamedArgs, but provided arguments are also checked to include all\n// named arguments that the sql query uses, and no extra arguments.\ntype StrictNamedArgs map[string]any\n\n// RewriteQuery implements the QueryRewriter interface.\nfunc (sna StrictNamedArgs) RewriteQuery(ctx context.Context, conn *Conn, sql string, args []any) (newSQL string, newArgs []any, err error) {\n\treturn rewriteQuery(sna, sql, true)\n}\n\ntype namedArg string\n\ntype sqlLexer struct {\n\tsrc     string\n\tstart   int\n\tpos     int\n\tnested  int // multiline comment nesting level.\n\tstateFn stateFn\n\tparts   []any\n\n\tnameToOrdinal map[namedArg]int\n}\n\ntype stateFn func(*sqlLexer) stateFn\n\nfunc rewriteQuery(na map[string]any, sql string, isStrict bool) (newSQL string, newArgs []any, err error) {\n\tl := &sqlLexer{\n\t\tsrc:           sql,\n\t\tstateFn:       rawState,\n\t\tnameToOrdinal: make(map[namedArg]int, len(na)),\n\t}\n\n\tfor l.stateFn != nil {\n\t\tl.stateFn = l.stateFn(l)\n\t}\n\n\tsb := strings.Builder{}\n\tfor _, p := range l.parts {\n\t\tswitch p := p.(type) {\n\t\tcase string:\n\t\t\tsb.WriteString(p)\n\t\tcase namedArg:\n\t\t\tsb.WriteRune('$')\n\t\t\tsb.WriteString(strconv.Itoa(l.nameToOrdinal[p]))\n\t\t}\n\t}\n\n\tnewArgs = make([]any, len(l.nameToOrdinal))\n\tfor name, ordinal := range l.nameToOrdinal {\n\t\tvar found bool\n\t\tnewArgs[ordinal-1], found = na[string(name)]\n\t\tif isStrict && !found {\n\t\t\treturn \"\", nil, fmt.Errorf(\"argument %s found in sql query but not present in StrictNamedArgs\", name)\n\t\t}\n\t}\n\n\tif isStrict {\n\t\tfor name := range na {\n\t\t\tif _, found := l.nameToOrdinal[namedArg(name)]; !found {\n\t\t\t\treturn \"\", nil, fmt.Errorf(\"argument %s of StrictNamedArgs not found in sql query\", name)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn sb.String(), newArgs, nil\n}\n\nfunc rawState(l *sqlLexer) stateFn {\n\tfor {\n\t\tr, width := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\tl.pos += width\n\n\t\tswitch r {\n\t\tcase 'e', 'E':\n\t\t\tnextRune, width := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\t\tif nextRune == '\\'' {\n\t\t\t\tl.pos += width\n\t\t\t\treturn escapeStringState\n\t\t\t}\n\t\tcase '\\'':\n\t\t\treturn singleQuoteState\n\t\tcase '\"':\n\t\t\treturn doubleQuoteState\n\t\tcase '@':\n\t\t\tnextRune, _ := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\t\tif isLetter(nextRune) || nextRune == '_' {\n\t\t\t\tif l.pos-l.start > 0 {\n\t\t\t\t\tl.parts = append(l.parts, l.src[l.start:l.pos-width])\n\t\t\t\t}\n\t\t\t\tl.start = l.pos\n\t\t\t\treturn namedArgState\n\t\t\t}\n\t\tcase '-':\n\t\t\tnextRune, width := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\t\tif nextRune == '-' {\n\t\t\t\tl.pos += width\n\t\t\t\treturn oneLineCommentState\n\t\t\t}\n\t\tcase '/':\n\t\t\tnextRune, width := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\t\tif nextRune == '*' {\n\t\t\t\tl.pos += width\n\t\t\t\treturn multilineCommentState\n\t\t\t}\n\t\tcase utf8.RuneError:\n\t\t\tif l.pos-l.start > 0 {\n\t\t\t\tl.parts = append(l.parts, l.src[l.start:l.pos])\n\t\t\t\tl.start = l.pos\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\nfunc isLetter(r rune) bool {\n\treturn (r >= 'a' && r <= 'z') || (r >= 'A' && r <= 'Z')\n}\n\nfunc namedArgState(l *sqlLexer) stateFn {\n\tfor {\n\t\tr, width := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\tl.pos += width\n\n\t\tif r == utf8.RuneError {\n\t\t\tif l.pos-l.start > 0 {\n\t\t\t\tna := namedArg(l.src[l.start:l.pos])\n\t\t\t\tif _, found := l.nameToOrdinal[na]; !found {\n\t\t\t\t\tl.nameToOrdinal[na] = len(l.nameToOrdinal) + 1\n\t\t\t\t}\n\t\t\t\tl.parts = append(l.parts, na)\n\t\t\t\tl.start = l.pos\n\t\t\t}\n\t\t\treturn nil\n\t\t} else if !(isLetter(r) || (r >= '0' && r <= '9') || r == '_') {\n\t\t\tl.pos -= width\n\t\t\tna := namedArg(l.src[l.start:l.pos])\n\t\t\tif _, found := l.nameToOrdinal[na]; !found {\n\t\t\t\tl.nameToOrdinal[na] = len(l.nameToOrdinal) + 1\n\t\t\t}\n\t\t\tl.parts = append(l.parts, namedArg(na))\n\t\t\tl.start = l.pos\n\t\t\treturn rawState\n\t\t}\n\t}\n}\n\nfunc singleQuoteState(l *sqlLexer) stateFn {\n\tfor {\n\t\tr, width := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\tl.pos += width\n\n\t\tswitch r {\n\t\tcase '\\'':\n\t\t\tnextRune, width := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\t\tif nextRune != '\\'' {\n\t\t\t\treturn rawState\n\t\t\t}\n\t\t\tl.pos += width\n\t\tcase utf8.RuneError:\n\t\t\tif l.pos-l.start > 0 {\n\t\t\t\tl.parts = append(l.parts, l.src[l.start:l.pos])\n\t\t\t\tl.start = l.pos\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\nfunc doubleQuoteState(l *sqlLexer) stateFn {\n\tfor {\n\t\tr, width := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\tl.pos += width\n\n\t\tswitch r {\n\t\tcase '\"':\n\t\t\tnextRune, width := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\t\tif nextRune != '\"' {\n\t\t\t\treturn rawState\n\t\t\t}\n\t\t\tl.pos += width\n\t\tcase utf8.RuneError:\n\t\t\tif l.pos-l.start > 0 {\n\t\t\t\tl.parts = append(l.parts, l.src[l.start:l.pos])\n\t\t\t\tl.start = l.pos\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\nfunc escapeStringState(l *sqlLexer) stateFn {\n\tfor {\n\t\tr, width := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\tl.pos += width\n\n\t\tswitch r {\n\t\tcase '\\\\':\n\t\t\t_, width = utf8.DecodeRuneInString(l.src[l.pos:])\n\t\t\tl.pos += width\n\t\tcase '\\'':\n\t\t\tnextRune, width := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\t\tif nextRune != '\\'' {\n\t\t\t\treturn rawState\n\t\t\t}\n\t\t\tl.pos += width\n\t\tcase utf8.RuneError:\n\t\t\tif l.pos-l.start > 0 {\n\t\t\t\tl.parts = append(l.parts, l.src[l.start:l.pos])\n\t\t\t\tl.start = l.pos\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\nfunc oneLineCommentState(l *sqlLexer) stateFn {\n\tfor {\n\t\tr, width := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\tl.pos += width\n\n\t\tswitch r {\n\t\tcase '\\\\':\n\t\t\t_, width = utf8.DecodeRuneInString(l.src[l.pos:])\n\t\t\tl.pos += width\n\t\tcase '\\n', '\\r':\n\t\t\treturn rawState\n\t\tcase utf8.RuneError:\n\t\t\tif l.pos-l.start > 0 {\n\t\t\t\tl.parts = append(l.parts, l.src[l.start:l.pos])\n\t\t\t\tl.start = l.pos\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\nfunc multilineCommentState(l *sqlLexer) stateFn {\n\tfor {\n\t\tr, width := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\tl.pos += width\n\n\t\tswitch r {\n\t\tcase '/':\n\t\t\tnextRune, width := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\t\tif nextRune == '*' {\n\t\t\t\tl.pos += width\n\t\t\t\tl.nested++\n\t\t\t}\n\t\tcase '*':\n\t\t\tnextRune, width := utf8.DecodeRuneInString(l.src[l.pos:])\n\t\t\tif nextRune != '/' {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tl.pos += width\n\t\t\tif l.nested == 0 {\n\t\t\t\treturn rawState\n\t\t\t}\n\t\t\tl.nested--\n\n\t\tcase utf8.RuneError:\n\t\t\tif l.pos-l.start > 0 {\n\t\t\t\tl.parts = append(l.parts, l.src[l.start:l.pos])\n\t\t\t\tl.start = l.pos\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "named_args_test.go",
          "type": "blob",
          "size": 4.4267578125,
          "content": "package pgx_test\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestNamedArgsRewriteQuery(t *testing.T) {\n\tt.Parallel()\n\n\tfor i, tt := range []struct {\n\t\tsql          string\n\t\targs         []any\n\t\tnamedArgs    pgx.NamedArgs\n\t\texpectedSQL  string\n\t\texpectedArgs []any\n\t}{\n\t\t{\n\t\t\tsql:          \"select * from users where id = @id\",\n\t\t\tnamedArgs:    pgx.NamedArgs{\"id\": int32(42)},\n\t\t\texpectedSQL:  \"select * from users where id = $1\",\n\t\t\texpectedArgs: []any{int32(42)},\n\t\t},\n\t\t{\n\t\t\tsql:          \"select * from t where foo < @abc and baz = @def and bar < @abc\",\n\t\t\tnamedArgs:    pgx.NamedArgs{\"abc\": int32(42), \"def\": int32(1)},\n\t\t\texpectedSQL:  \"select * from t where foo < $1 and baz = $2 and bar < $1\",\n\t\t\texpectedArgs: []any{int32(42), int32(1)},\n\t\t},\n\t\t{\n\t\t\tsql:          \"select @a::int, @b::text\",\n\t\t\tnamedArgs:    pgx.NamedArgs{\"a\": int32(42), \"b\": \"foo\"},\n\t\t\texpectedSQL:  \"select $1::int, $2::text\",\n\t\t\texpectedArgs: []any{int32(42), \"foo\"},\n\t\t},\n\t\t{\n\t\t\tsql:          \"select @Abc::int, @b_4::text, @_c::int\",\n\t\t\tnamedArgs:    pgx.NamedArgs{\"Abc\": int32(42), \"b_4\": \"foo\", \"_c\": int32(1)},\n\t\t\texpectedSQL:  \"select $1::int, $2::text, $3::int\",\n\t\t\texpectedArgs: []any{int32(42), \"foo\", int32(1)},\n\t\t},\n\t\t{\n\t\t\tsql:          \"at end @\",\n\t\t\tnamedArgs:    pgx.NamedArgs{\"a\": int32(42), \"b\": \"foo\"},\n\t\t\texpectedSQL:  \"at end @\",\n\t\t\texpectedArgs: []any{},\n\t\t},\n\t\t{\n\t\t\tsql:          \"ignores without valid character after @ foo bar\",\n\t\t\tnamedArgs:    pgx.NamedArgs{\"a\": int32(42), \"b\": \"foo\"},\n\t\t\texpectedSQL:  \"ignores without valid character after @ foo bar\",\n\t\t\texpectedArgs: []any{},\n\t\t},\n\t\t{\n\t\t\tsql:          \"name cannot start with number @1 foo bar\",\n\t\t\tnamedArgs:    pgx.NamedArgs{\"a\": int32(42), \"b\": \"foo\"},\n\t\t\texpectedSQL:  \"name cannot start with number @1 foo bar\",\n\t\t\texpectedArgs: []any{},\n\t\t},\n\t\t{\n\t\t\tsql:          `select *, '@foo' as \"@bar\" from users where id = @id`,\n\t\t\tnamedArgs:    pgx.NamedArgs{\"id\": int32(42)},\n\t\t\texpectedSQL:  `select *, '@foo' as \"@bar\" from users where id = $1`,\n\t\t\texpectedArgs: []any{int32(42)},\n\t\t},\n\t\t{\n\t\t\tsql: `select * -- @foo\n\t\t\tfrom users -- @single line comments\n\t\t\twhere id = @id;`,\n\t\t\tnamedArgs: pgx.NamedArgs{\"id\": int32(42)},\n\t\t\texpectedSQL: `select * -- @foo\n\t\t\tfrom users -- @single line comments\n\t\t\twhere id = $1;`,\n\t\t\texpectedArgs: []any{int32(42)},\n\t\t},\n\t\t{\n\t\t\tsql: `select * /* @multi line\n\t\t\t@comment\n\t\t\t*/\n\t\t\t/* /* with @nesting */ */\n\t\t\tfrom users\n\t\t\twhere id = @id;`,\n\t\t\tnamedArgs: pgx.NamedArgs{\"id\": int32(42)},\n\t\t\texpectedSQL: `select * /* @multi line\n\t\t\t@comment\n\t\t\t*/\n\t\t\t/* /* with @nesting */ */\n\t\t\tfrom users\n\t\t\twhere id = $1;`,\n\t\t\texpectedArgs: []any{int32(42)},\n\t\t},\n\t\t{\n\t\t\tsql:          \"extra provided argument\",\n\t\t\tnamedArgs:    pgx.NamedArgs{\"extra\": int32(1)},\n\t\t\texpectedSQL:  \"extra provided argument\",\n\t\t\texpectedArgs: []any{},\n\t\t},\n\t\t{\n\t\t\tsql:          \"@missing argument\",\n\t\t\tnamedArgs:    pgx.NamedArgs{},\n\t\t\texpectedSQL:  \"$1 argument\",\n\t\t\texpectedArgs: []any{nil},\n\t\t},\n\n\t\t// test comments and quotes\n\t} {\n\t\tsql, args, err := tt.namedArgs.RewriteQuery(context.Background(), nil, tt.sql, tt.args)\n\t\trequire.NoError(t, err)\n\t\tassert.Equalf(t, tt.expectedSQL, sql, \"%d\", i)\n\t\tassert.Equalf(t, tt.expectedArgs, args, \"%d\", i)\n\t}\n}\n\nfunc TestStrictNamedArgsRewriteQuery(t *testing.T) {\n\tt.Parallel()\n\n\tfor i, tt := range []struct {\n\t\tsql             string\n\t\tnamedArgs       pgx.StrictNamedArgs\n\t\texpectedSQL     string\n\t\texpectedArgs    []any\n\t\tisExpectedError bool\n\t}{\n\t\t{\n\t\t\tsql:             \"no arguments\",\n\t\t\tnamedArgs:       pgx.StrictNamedArgs{},\n\t\t\texpectedSQL:     \"no arguments\",\n\t\t\texpectedArgs:    []any{},\n\t\t\tisExpectedError: false,\n\t\t},\n\t\t{\n\t\t\tsql:             \"@all @matches\",\n\t\t\tnamedArgs:       pgx.StrictNamedArgs{\"all\": int32(1), \"matches\": int32(2)},\n\t\t\texpectedSQL:     \"$1 $2\",\n\t\t\texpectedArgs:    []any{int32(1), int32(2)},\n\t\t\tisExpectedError: false,\n\t\t},\n\t\t{\n\t\t\tsql:             \"extra provided argument\",\n\t\t\tnamedArgs:       pgx.StrictNamedArgs{\"extra\": int32(1)},\n\t\t\tisExpectedError: true,\n\t\t},\n\t\t{\n\t\t\tsql:             \"@missing argument\",\n\t\t\tnamedArgs:       pgx.StrictNamedArgs{},\n\t\t\tisExpectedError: true,\n\t\t},\n\t} {\n\t\tsql, args, err := tt.namedArgs.RewriteQuery(context.Background(), nil, tt.sql, nil)\n\t\tif tt.isExpectedError {\n\t\t\tassert.Errorf(t, err, \"%d\", i)\n\t\t} else {\n\t\t\trequire.NoErrorf(t, err, \"%d\", i)\n\t\t\tassert.Equalf(t, tt.expectedSQL, sql, \"%d\", i)\n\t\t\tassert.Equalf(t, tt.expectedArgs, args, \"%d\", i)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "pgbouncer_test.go",
          "type": "blob",
          "size": 2.0634765625,
          "content": "package pgx_test\n\nimport (\n\t\"context\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestPgbouncerStatementCacheDescribe(t *testing.T) {\n\tconnString := os.Getenv(\"PGX_TEST_PGBOUNCER_CONN_STRING\")\n\tif connString == \"\" {\n\t\tt.Skipf(\"Skipping due to missing environment variable %v\", \"PGX_TEST_PGBOUNCER_CONN_STRING\")\n\t}\n\n\tconfig := mustParseConfig(t, connString)\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeCacheDescribe\n\tconfig.DescriptionCacheCapacity = 1024\n\n\ttestPgbouncer(t, config, 10, 100)\n}\n\nfunc TestPgbouncerSimpleProtocol(t *testing.T) {\n\tconnString := os.Getenv(\"PGX_TEST_PGBOUNCER_CONN_STRING\")\n\tif connString == \"\" {\n\t\tt.Skipf(\"Skipping due to missing environment variable %v\", \"PGX_TEST_PGBOUNCER_CONN_STRING\")\n\t}\n\n\tconfig := mustParseConfig(t, connString)\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeSimpleProtocol\n\n\ttestPgbouncer(t, config, 10, 100)\n}\n\nfunc testPgbouncer(t *testing.T, config *pgx.ConnConfig, workers, iterations int) {\n\tdoneChan := make(chan struct{})\n\n\tfor i := 0; i < workers; i++ {\n\t\tgo func() {\n\t\t\tdefer func() { doneChan <- struct{}{} }()\n\t\t\tconn, err := pgx.ConnectConfig(context.Background(), config)\n\t\t\trequire.Nil(t, err)\n\t\t\tdefer closeConn(t, conn)\n\n\t\t\tfor i := 0; i < iterations; i++ {\n\t\t\t\tvar i32 int32\n\t\t\t\tvar i64 int64\n\t\t\t\tvar f32 float32\n\t\t\t\tvar s string\n\t\t\t\tvar s2 string\n\t\t\t\terr = conn.QueryRow(context.Background(), \"select 1::int4, 2::int8, 3::float4, 'hi'::text\").Scan(&i32, &i64, &f32, &s)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tassert.Equal(t, int32(1), i32)\n\t\t\t\tassert.Equal(t, int64(2), i64)\n\t\t\t\tassert.Equal(t, float32(3), f32)\n\t\t\t\tassert.Equal(t, \"hi\", s)\n\n\t\t\t\terr = conn.QueryRow(context.Background(), \"select 1::int8, 2::float4, 'bye'::text, 4::int4, 'whatever'::text\").Scan(&i64, &f32, &s, &i32, &s2)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tassert.Equal(t, int64(1), i64)\n\t\t\t\tassert.Equal(t, float32(2), f32)\n\t\t\t\tassert.Equal(t, \"bye\", s)\n\t\t\t\tassert.Equal(t, int32(4), i32)\n\t\t\t\tassert.Equal(t, \"whatever\", s2)\n\t\t\t}\n\t\t}()\n\t}\n\n\tfor i := 0; i < workers; i++ {\n\t\t<-doneChan\n\t}\n\n}\n"
        },
        {
          "name": "pgconn",
          "type": "tree",
          "content": null
        },
        {
          "name": "pgproto3",
          "type": "tree",
          "content": null
        },
        {
          "name": "pgtype",
          "type": "tree",
          "content": null
        },
        {
          "name": "pgx_test.go",
          "type": "blob",
          "size": 0.39453125,
          "content": "package pgx_test\n\nimport (\n\t\"context\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t_ \"github.com/jackc/pgx/v5/stdlib\"\n)\n\nfunc skipCockroachDB(t testing.TB, msg string) {\n\tconn, err := pgx.Connect(context.Background(), os.Getenv(\"PGX_TEST_DATABASE\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer conn.Close(context.Background())\n\n\tif conn.PgConn().ParameterStatus(\"crdb_version\") != \"\" {\n\t\tt.Skip(msg)\n\t}\n}\n"
        },
        {
          "name": "pgxpool",
          "type": "tree",
          "content": null
        },
        {
          "name": "pgxtest",
          "type": "tree",
          "content": null
        },
        {
          "name": "pipeline_test.go",
          "type": "blob",
          "size": 2.044921875,
          "content": "package pgx_test\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestPipelineWithoutPreparedOrDescribedStatements(t *testing.T) {\n\tt.Parallel()\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tpipeline := conn.PgConn().StartPipeline(ctx)\n\n\t\teqb := pgx.ExtendedQueryBuilder{}\n\n\t\terr := eqb.Build(conn.TypeMap(), nil, []any{1, 2})\n\t\trequire.NoError(t, err)\n\t\tpipeline.SendQueryParams(`select $1::bigint + $2::bigint`, eqb.ParamValues, nil, eqb.ParamFormats, eqb.ResultFormats)\n\n\t\terr = eqb.Build(conn.TypeMap(), nil, []any{3, 4, 5})\n\t\trequire.NoError(t, err)\n\t\tpipeline.SendQueryParams(`select $1::bigint + $2::bigint + $3::bigint`, eqb.ParamValues, nil, eqb.ParamFormats, eqb.ResultFormats)\n\n\t\terr = pipeline.Sync()\n\t\trequire.NoError(t, err)\n\n\t\tresults, err := pipeline.GetResults()\n\t\trequire.NoError(t, err)\n\t\trr, ok := results.(*pgconn.ResultReader)\n\t\trequire.True(t, ok)\n\t\trows := pgx.RowsFromResultReader(conn.TypeMap(), rr)\n\n\t\trowCount := 0\n\t\tvar n int64\n\t\tfor rows.Next() {\n\t\t\terr = rows.Scan(&n)\n\t\t\trequire.NoError(t, err)\n\t\t\trowCount++\n\t\t}\n\t\trequire.NoError(t, rows.Err())\n\t\trequire.Equal(t, 1, rowCount)\n\t\trequire.Equal(t, \"SELECT 1\", rows.CommandTag().String())\n\t\trequire.EqualValues(t, 3, n)\n\n\t\tresults, err = pipeline.GetResults()\n\t\trequire.NoError(t, err)\n\t\trr, ok = results.(*pgconn.ResultReader)\n\t\trequire.True(t, ok)\n\t\trows = pgx.RowsFromResultReader(conn.TypeMap(), rr)\n\n\t\trowCount = 0\n\t\tn = 0\n\t\tfor rows.Next() {\n\t\t\terr = rows.Scan(&n)\n\t\t\trequire.NoError(t, err)\n\t\t\trowCount++\n\t\t}\n\t\trequire.NoError(t, rows.Err())\n\t\trequire.Equal(t, 1, rowCount)\n\t\trequire.Equal(t, \"SELECT 1\", rows.CommandTag().String())\n\t\trequire.EqualValues(t, 12, n)\n\n\t\tresults, err = pipeline.GetResults()\n\t\trequire.NoError(t, err)\n\t\t_, ok = results.(*pgconn.PipelineSync)\n\t\trequire.True(t, ok)\n\n\t\tresults, err = pipeline.GetResults()\n\t\trequire.NoError(t, err)\n\t\trequire.Nil(t, results)\n\n\t\terr = pipeline.Close()\n\t\trequire.NoError(t, err)\n\t})\n}\n"
        },
        {
          "name": "query_test.go",
          "type": "blob",
          "size": 58.697265625,
          "content": "package pgx_test\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"database/sql/driver\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgtype\"\n\t\"github.com/jackc/pgx/v5/pgxtest\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestConnQueryScan(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tvar sum, rowCount int32\n\n\trows, err := conn.Query(context.Background(), \"select generate_series(1,$1)\", 10)\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Query failed: %v\", err)\n\t}\n\tdefer rows.Close()\n\n\tfor rows.Next() {\n\t\tvar n int32\n\t\trows.Scan(&n)\n\t\tsum += n\n\t\trowCount++\n\t}\n\n\tif rows.Err() != nil {\n\t\tt.Fatalf(\"conn.Query failed: %v\", rows.Err())\n\t}\n\n\tassert.Equal(t, \"SELECT 10\", rows.CommandTag().String())\n\n\tif rowCount != 10 {\n\t\tt.Error(\"Select called onDataRow wrong number of times\")\n\t}\n\tif sum != 55 {\n\t\tt.Error(\"Wrong values returned\")\n\t}\n}\n\nfunc TestConnQueryRowsFieldDescriptionsBeforeNext(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\trows, err := conn.Query(context.Background(), \"select 'hello' as msg\")\n\trequire.NoError(t, err)\n\tdefer rows.Close()\n\n\trequire.Len(t, rows.FieldDescriptions(), 1)\n\tassert.Equal(t, \"msg\", rows.FieldDescriptions()[0].Name)\n}\n\nfunc TestConnQueryWithoutResultSetCommandTag(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\trows, err := conn.Query(context.Background(), \"create temporary table t (id serial);\")\n\tassert.NoError(t, err)\n\trows.Close()\n\tassert.NoError(t, rows.Err())\n\tassert.Equal(t, \"CREATE TABLE\", rows.CommandTag().String())\n}\n\nfunc TestConnQueryScanWithManyColumns(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tcolumnCount := 1000\n\tsql := \"select \"\n\tfor i := 0; i < columnCount; i++ {\n\t\tif i > 0 {\n\t\t\tsql += \",\"\n\t\t}\n\t\tsql += fmt.Sprintf(\" %d\", i)\n\t}\n\tsql += \" from generate_series(1,5)\"\n\n\tdest := make([]int, columnCount)\n\n\tvar rowCount int\n\n\trows, err := conn.Query(context.Background(), sql)\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Query failed: %v\", err)\n\t}\n\tdefer rows.Close()\n\n\tfor rows.Next() {\n\t\tdestPtrs := make([]any, columnCount)\n\t\tfor i := range destPtrs {\n\t\t\tdestPtrs[i] = &dest[i]\n\t\t}\n\t\tif err := rows.Scan(destPtrs...); err != nil {\n\t\t\tt.Fatalf(\"rows.Scan failed: %v\", err)\n\t\t}\n\t\trowCount++\n\n\t\tfor i := range dest {\n\t\t\tif dest[i] != i {\n\t\t\t\tt.Errorf(\"dest[%d] => %d, want %d\", i, dest[i], i)\n\t\t\t}\n\t\t}\n\t}\n\n\tif rows.Err() != nil {\n\t\tt.Fatalf(\"conn.Query failed: %v\", rows.Err())\n\t}\n\n\tif rowCount != 5 {\n\t\tt.Errorf(\"rowCount => %d, want %d\", rowCount, 5)\n\t}\n}\n\nfunc TestConnQueryValues(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tvar rowCount int32\n\n\trows, err := conn.Query(context.Background(), \"select 'foo'::text, 'bar'::varchar, n, null, n from generate_series(1,$1) n\", 10)\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Query failed: %v\", err)\n\t}\n\tdefer rows.Close()\n\n\tfor rows.Next() {\n\t\trowCount++\n\n\t\tvalues, err := rows.Values()\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, values, 5)\n\t\tassert.Equal(t, \"foo\", values[0])\n\t\tassert.Equal(t, \"bar\", values[1])\n\t\tassert.EqualValues(t, rowCount, values[2])\n\t\tassert.Nil(t, values[3])\n\t\tassert.EqualValues(t, rowCount, values[4])\n\t}\n\n\tif rows.Err() != nil {\n\t\tt.Fatalf(\"conn.Query failed: %v\", rows.Err())\n\t}\n\n\tif rowCount != 10 {\n\t\tt.Error(\"Select called onDataRow wrong number of times\")\n\t}\n}\n\n// https://github.com/jackc/pgx/issues/666\nfunc TestConnQueryValuesWhenUnableToDecode(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\t// Note that this relies on pgtype.Record not supporting the text protocol. This seems safe as it is impossible to\n\t// decode the text protocol because unlike the binary protocol there is no way to determine the OIDs of the elements.\n\trows, err := conn.Query(context.Background(), \"select (array[1::oid], null)\", pgx.QueryResultFormats{pgx.TextFormatCode})\n\trequire.NoError(t, err)\n\tdefer rows.Close()\n\n\trequire.True(t, rows.Next())\n\n\tvalues, err := rows.Values()\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"({1},)\", values[0])\n}\n\nfunc TestConnQueryValuesWithUnregisteredOID(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\ttx, err := conn.Begin(ctx)\n\trequire.NoError(t, err)\n\tdefer tx.Rollback(ctx)\n\n\t_, err = tx.Exec(ctx, \"create type fruit as enum('orange', 'apple', 'pear')\")\n\trequire.NoError(t, err)\n\n\trows, err := conn.Query(context.Background(), \"select 'orange'::fruit\")\n\trequire.NoError(t, err)\n\tdefer rows.Close()\n\n\trequire.True(t, rows.Next())\n\n\tvalues, err := rows.Values()\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"orange\", values[0])\n}\n\nfunc TestConnQueryArgsAndScanWithUnregisteredOID(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttx, err := conn.Begin(ctx)\n\t\trequire.NoError(t, err)\n\t\tdefer tx.Rollback(ctx)\n\n\t\t_, err = tx.Exec(ctx, \"create type fruit as enum('orange', 'apple', 'pear')\")\n\t\trequire.NoError(t, err)\n\n\t\tvar result string\n\t\terr = conn.QueryRow(ctx, \"select $1::fruit\", \"orange\").Scan(&result)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, \"orange\", result)\n\t})\n}\n\n// https://github.com/jackc/pgx/issues/478\nfunc TestConnQueryReadRowMultipleTimes(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tvar rowCount int32\n\n\trows, err := conn.Query(context.Background(), \"select 'foo'::text, 'bar'::varchar, n, null, n from generate_series(1,$1) n\", 10)\n\trequire.NoError(t, err)\n\tdefer rows.Close()\n\n\tfor rows.Next() {\n\t\trowCount++\n\n\t\tfor i := 0; i < 2; i++ {\n\t\t\tvalues, err := rows.Values()\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, values, 5)\n\t\t\trequire.Equal(t, \"foo\", values[0])\n\t\t\trequire.Equal(t, \"bar\", values[1])\n\t\t\trequire.EqualValues(t, rowCount, values[2])\n\t\t\trequire.Nil(t, values[3])\n\t\t\trequire.EqualValues(t, rowCount, values[4])\n\n\t\t\tvar a, b string\n\t\t\tvar c int32\n\t\t\tvar d pgtype.Text\n\t\t\tvar e int32\n\n\t\t\terr = rows.Scan(&a, &b, &c, &d, &e)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, \"foo\", a)\n\t\t\trequire.Equal(t, \"bar\", b)\n\t\t\trequire.Equal(t, rowCount, c)\n\t\t\trequire.False(t, d.Valid)\n\t\t\trequire.Equal(t, rowCount, e)\n\t\t}\n\t}\n\n\trequire.NoError(t, rows.Err())\n\trequire.Equal(t, int32(10), rowCount)\n}\n\n// https://github.com/jackc/pgx/issues/228\nfunc TestRowsScanDoesNotAllowScanningBinaryFormatValuesIntoString(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server does not support point type\")\n\n\tvar s string\n\n\terr := conn.QueryRow(context.Background(), \"select point(1,2)\").Scan(&s)\n\tif err == nil || !(strings.Contains(err.Error(), \"cannot scan point (OID 600) in binary format into *string\")) {\n\t\tt.Fatalf(\"Expected Scan to fail to scan binary value into string but: %v\", err)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnQueryRawValues(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tvar rowCount int32\n\n\trows, err := conn.Query(\n\t\tcontext.Background(),\n\t\t\"select 'foo'::text, 'bar'::varchar, n, null, n from generate_series(1,$1) n\",\n\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t10,\n\t)\n\trequire.NoError(t, err)\n\tdefer rows.Close()\n\n\tfor rows.Next() {\n\t\trowCount++\n\n\t\trawValues := rows.RawValues()\n\t\tassert.Len(t, rawValues, 5)\n\t\tassert.Equal(t, \"foo\", string(rawValues[0]))\n\t\tassert.Equal(t, \"bar\", string(rawValues[1]))\n\t\tassert.Equal(t, strconv.FormatInt(int64(rowCount), 10), string(rawValues[2]))\n\t\tassert.Nil(t, rawValues[3])\n\t\tassert.Equal(t, strconv.FormatInt(int64(rowCount), 10), string(rawValues[4]))\n\t}\n\n\trequire.NoError(t, rows.Err())\n\tassert.EqualValues(t, 10, rowCount)\n}\n\n// Test that a connection stays valid when query results are closed early\nfunc TestConnQueryCloseEarly(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\t// Immediately close query without reading any rows\n\trows, err := conn.Query(context.Background(), \"select generate_series(1,$1)\", 10)\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Query failed: %v\", err)\n\t}\n\trows.Close()\n\n\tensureConnValid(t, conn)\n\n\t// Read partial response then close\n\trows, err = conn.Query(context.Background(), \"select generate_series(1,$1)\", 10)\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Query failed: %v\", err)\n\t}\n\n\tok := rows.Next()\n\tif !ok {\n\t\tt.Fatal(\"rows.Next terminated early\")\n\t}\n\n\tvar n int32\n\trows.Scan(&n)\n\tif n != 1 {\n\t\tt.Fatalf(\"Expected 1 from first row, but got %v\", n)\n\t}\n\n\trows.Close()\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnQueryCloseEarlyWithErrorOnWire(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\trows, err := conn.Query(context.Background(), \"select 1/(10-n) from generate_series(1,10) n\")\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Query failed: %v\", err)\n\t}\n\tassert.False(t, pgconn.SafeToRetry(err))\n\trows.Close()\n\n\tensureConnValid(t, conn)\n}\n\n// Test that a connection stays valid when query results read incorrectly\nfunc TestConnQueryReadWrongTypeError(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\t// Read a single value incorrectly\n\trows, err := conn.Query(context.Background(), \"select n::int4 from generate_series(1,$1) n\", 10)\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Query failed: %v\", err)\n\t}\n\n\trowsRead := 0\n\n\tfor rows.Next() {\n\t\tvar t time.Time\n\t\trows.Scan(&t)\n\t\trowsRead++\n\t}\n\n\tif rowsRead != 1 {\n\t\tt.Fatalf(\"Expected error to cause only 1 row to be read, but %d were read\", rowsRead)\n\t}\n\n\tif rows.Err() == nil {\n\t\tt.Fatal(\"Expected Rows to have an error after an improper read but it didn't\")\n\t}\n\n\tif rows.Err().Error() != \"can't scan into dest[0]: cannot scan int4 (OID 23) in binary format into *time.Time\" {\n\t\tt.Fatalf(\"Expected different Rows.Err(): %v\", rows.Err())\n\t}\n\n\tensureConnValid(t, conn)\n}\n\n// Test that a connection stays valid when query results read incorrectly\nfunc TestConnQueryReadTooManyValues(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\t// Read too many values\n\trows, err := conn.Query(context.Background(), \"select generate_series(1,$1)\", 10)\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Query failed: %v\", err)\n\t}\n\n\trowsRead := 0\n\n\tfor rows.Next() {\n\t\tvar n, m int32\n\t\trows.Scan(&n, &m)\n\t\trowsRead++\n\t}\n\n\tif rowsRead != 1 {\n\t\tt.Fatalf(\"Expected error to cause only 1 row to be read, but %d were read\", rowsRead)\n\t}\n\n\tif rows.Err() == nil {\n\t\tt.Fatal(\"Expected Rows to have an error after an improper read but it didn't\")\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnQueryScanIgnoreColumn(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\trows, err := conn.Query(context.Background(), \"select 1::int8, 2::int8, 3::int8\")\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Query failed: %v\", err)\n\t}\n\n\tok := rows.Next()\n\tif !ok {\n\t\tt.Fatal(\"rows.Next terminated early\")\n\t}\n\n\tvar n, m int64\n\terr = rows.Scan(&n, nil, &m)\n\tif err != nil {\n\t\tt.Fatalf(\"rows.Scan failed: %v\", err)\n\t}\n\trows.Close()\n\n\tif n != 1 {\n\t\tt.Errorf(\"Expected n to equal 1, but it was %d\", n)\n\t}\n\n\tif m != 3 {\n\t\tt.Errorf(\"Expected n to equal 3, but it was %d\", m)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\n// https://github.com/jackc/pgx/issues/570\nfunc TestConnQueryDeferredError(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server does not support deferred constraint (https://github.com/cockroachdb/cockroach/issues/31632)\")\n\n\tmustExec(t, conn, `create temporary table t (\n\tid text primary key,\n\tn int not null,\n\tunique (n) deferrable initially deferred\n);\n\ninsert into t (id, n) values ('a', 1), ('b', 2), ('c', 3);`)\n\n\trows, err := conn.Query(context.Background(), `update t set n=n+1 where id='b' returning *`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer rows.Close()\n\n\tfor rows.Next() {\n\t\tvar id string\n\t\tvar n int32\n\t\terr = rows.Scan(&id, &n)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tif rows.Err() == nil {\n\t\tt.Fatal(\"expected error 23505 but got none\")\n\t}\n\n\tif err, ok := rows.Err().(*pgconn.PgError); !ok || err.Code != \"23505\" {\n\t\tt.Fatalf(\"expected error 23505, got %v\", err)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnQueryErrorWhileReturningRows(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server uses numeric instead of int\")\n\n\tfor i := 0; i < 100; i++ {\n\t\tfunc() {\n\t\t\tsql := `select 42 / (random() * 20)::integer from generate_series(1,100000)`\n\n\t\t\trows, err := conn.Query(context.Background(), sql)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tdefer rows.Close()\n\n\t\t\tfor rows.Next() {\n\t\t\t\tvar n int32\n\t\t\t\tif err := rows.Scan(&n); err != nil {\n\t\t\t\t\tt.Fatalf(\"Row scan failed: %v\", err)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif _, ok := rows.Err().(*pgconn.PgError); !ok {\n\t\t\t\tt.Fatalf(\"Expected pgx.PgError, got %v\", rows.Err())\n\t\t\t}\n\n\t\t\tensureConnValid(t, conn)\n\t\t}()\n\t}\n\n}\n\nfunc TestQueryEncodeError(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\trows, err := conn.Query(context.Background(), \"select $1::integer\", \"wrong\")\n\tif err != nil {\n\t\tt.Errorf(\"conn.Query failure: %v\", err)\n\t}\n\tassert.False(t, pgconn.SafeToRetry(err))\n\tdefer rows.Close()\n\n\trows.Next()\n\n\tif rows.Err() == nil {\n\t\tt.Error(\"Expected rows.Err() to return error, but it didn't\")\n\t}\n\tif !strings.Contains(rows.Err().Error(), \"SQLSTATE 22P02\") {\n\t\tt.Error(\"Expected rows.Err() to return different error:\", rows.Err())\n\t}\n}\n\nfunc TestQueryRowCoreTypes(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\ttype allTypes struct {\n\t\ts   string\n\t\tf32 float32\n\t\tf64 float64\n\t\tb   bool\n\t\tt   time.Time\n\t\toid uint32\n\t}\n\n\tvar actual, zero allTypes\n\n\ttests := []struct {\n\t\tsql       string\n\t\tqueryArgs []any\n\t\tscanArgs  []any\n\t\texpected  allTypes\n\t}{\n\t\t{\"select $1::text\", []any{\"Jack\"}, []any{&actual.s}, allTypes{s: \"Jack\"}},\n\t\t{\"select $1::float4\", []any{float32(1.23)}, []any{&actual.f32}, allTypes{f32: 1.23}},\n\t\t{\"select $1::float8\", []any{float64(1.23)}, []any{&actual.f64}, allTypes{f64: 1.23}},\n\t\t{\"select $1::bool\", []any{true}, []any{&actual.b}, allTypes{b: true}},\n\t\t{\"select $1::timestamptz\", []any{time.Unix(123, 5000)}, []any{&actual.t}, allTypes{t: time.Unix(123, 5000)}},\n\t\t{\"select $1::timestamp\", []any{time.Date(2010, 1, 2, 3, 4, 5, 0, time.UTC)}, []any{&actual.t}, allTypes{t: time.Date(2010, 1, 2, 3, 4, 5, 0, time.UTC)}},\n\t\t{\"select $1::date\", []any{time.Date(1987, 1, 2, 0, 0, 0, 0, time.UTC)}, []any{&actual.t}, allTypes{t: time.Date(1987, 1, 2, 0, 0, 0, 0, time.UTC)}},\n\t\t{\"select $1::oid\", []any{uint32(42)}, []any{&actual.oid}, allTypes{oid: 42}},\n\t}\n\n\tfor i, tt := range tests {\n\t\tactual = zero\n\n\t\terr := conn.QueryRow(context.Background(), tt.sql, tt.queryArgs...).Scan(tt.scanArgs...)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%d. Unexpected failure: %v (sql -> %v, queryArgs -> %v)\", i, err, tt.sql, tt.queryArgs)\n\t\t}\n\n\t\tif actual.s != tt.expected.s || actual.f32 != tt.expected.f32 || actual.b != tt.expected.b || !actual.t.Equal(tt.expected.t) || actual.oid != tt.expected.oid {\n\t\t\tt.Errorf(\"%d. Expected %v, got %v (sql -> %v, queryArgs -> %v)\", i, tt.expected, actual, tt.sql, tt.queryArgs)\n\t\t}\n\n\t\tensureConnValid(t, conn)\n\n\t\t// Check that Scan errors when a core type is null\n\t\terr = conn.QueryRow(context.Background(), tt.sql, nil).Scan(tt.scanArgs...)\n\t\tif err == nil {\n\t\t\tt.Errorf(\"%d. Expected null to cause error, but it didn't (sql -> %v)\", i, tt.sql)\n\t\t}\n\n\t\tensureConnValid(t, conn)\n\t}\n}\n\nfunc TestQueryRowCoreIntegerEncoding(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\ttype allTypes struct {\n\t\ti16 int16\n\t\ti32 int32\n\t\ti64 int64\n\t}\n\n\tvar actual, zero allTypes\n\n\tsuccessfulEncodeTests := []struct {\n\t\tsql      string\n\t\tqueryArg any\n\t\tscanArg  any\n\t\texpected allTypes\n\t}{\n\t\t// Check any integer type where value is within int2 range can be encoded\n\t\t{\"select $1::int2\", int(42), &actual.i16, allTypes{i16: 42}},\n\t\t{\"select $1::int2\", int8(42), &actual.i16, allTypes{i16: 42}},\n\t\t{\"select $1::int2\", int16(42), &actual.i16, allTypes{i16: 42}},\n\t\t{\"select $1::int2\", int32(42), &actual.i16, allTypes{i16: 42}},\n\t\t{\"select $1::int2\", int64(42), &actual.i16, allTypes{i16: 42}},\n\t\t{\"select $1::int2\", uint(42), &actual.i16, allTypes{i16: 42}},\n\t\t{\"select $1::int2\", uint8(42), &actual.i16, allTypes{i16: 42}},\n\t\t{\"select $1::int2\", uint16(42), &actual.i16, allTypes{i16: 42}},\n\t\t{\"select $1::int2\", uint32(42), &actual.i16, allTypes{i16: 42}},\n\t\t{\"select $1::int2\", uint64(42), &actual.i16, allTypes{i16: 42}},\n\n\t\t// Check any integer type where value is within int4 range can be encoded\n\t\t{\"select $1::int4\", int(42), &actual.i32, allTypes{i32: 42}},\n\t\t{\"select $1::int4\", int8(42), &actual.i32, allTypes{i32: 42}},\n\t\t{\"select $1::int4\", int16(42), &actual.i32, allTypes{i32: 42}},\n\t\t{\"select $1::int4\", int32(42), &actual.i32, allTypes{i32: 42}},\n\t\t{\"select $1::int4\", int64(42), &actual.i32, allTypes{i32: 42}},\n\t\t{\"select $1::int4\", uint(42), &actual.i32, allTypes{i32: 42}},\n\t\t{\"select $1::int4\", uint8(42), &actual.i32, allTypes{i32: 42}},\n\t\t{\"select $1::int4\", uint16(42), &actual.i32, allTypes{i32: 42}},\n\t\t{\"select $1::int4\", uint32(42), &actual.i32, allTypes{i32: 42}},\n\t\t{\"select $1::int4\", uint64(42), &actual.i32, allTypes{i32: 42}},\n\n\t\t// Check any integer type where value is within int8 range can be encoded\n\t\t{\"select $1::int8\", int(42), &actual.i64, allTypes{i64: 42}},\n\t\t{\"select $1::int8\", int8(42), &actual.i64, allTypes{i64: 42}},\n\t\t{\"select $1::int8\", int16(42), &actual.i64, allTypes{i64: 42}},\n\t\t{\"select $1::int8\", int32(42), &actual.i64, allTypes{i64: 42}},\n\t\t{\"select $1::int8\", int64(42), &actual.i64, allTypes{i64: 42}},\n\t\t{\"select $1::int8\", uint(42), &actual.i64, allTypes{i64: 42}},\n\t\t{\"select $1::int8\", uint8(42), &actual.i64, allTypes{i64: 42}},\n\t\t{\"select $1::int8\", uint16(42), &actual.i64, allTypes{i64: 42}},\n\t\t{\"select $1::int8\", uint32(42), &actual.i64, allTypes{i64: 42}},\n\t\t{\"select $1::int8\", uint64(42), &actual.i64, allTypes{i64: 42}},\n\t}\n\n\tfor i, tt := range successfulEncodeTests {\n\t\tactual = zero\n\n\t\terr := conn.QueryRow(context.Background(), tt.sql, tt.queryArg).Scan(tt.scanArg)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%d. Unexpected failure: %v (sql -> %v, queryArg -> %v)\", i, err, tt.sql, tt.queryArg)\n\t\t\tcontinue\n\t\t}\n\n\t\tif actual != tt.expected {\n\t\t\tt.Errorf(\"%d. Expected %v, got %v (sql -> %v, queryArg -> %v)\", i, tt.expected, actual, tt.sql, tt.queryArg)\n\t\t}\n\n\t\tensureConnValid(t, conn)\n\t}\n\n\tfailedEncodeTests := []struct {\n\t\tsql      string\n\t\tqueryArg any\n\t}{\n\t\t// Check any integer type where value is outside pg:int2 range cannot be encoded\n\t\t{\"select $1::int2\", int(32769)},\n\t\t{\"select $1::int2\", int32(32769)},\n\t\t{\"select $1::int2\", int32(32769)},\n\t\t{\"select $1::int2\", int64(32769)},\n\t\t{\"select $1::int2\", uint(32769)},\n\t\t{\"select $1::int2\", uint16(32769)},\n\t\t{\"select $1::int2\", uint32(32769)},\n\t\t{\"select $1::int2\", uint64(32769)},\n\n\t\t// Check any integer type where value is outside pg:int4 range cannot be encoded\n\t\t{\"select $1::int4\", int64(2147483649)},\n\t\t{\"select $1::int4\", uint32(2147483649)},\n\t\t{\"select $1::int4\", uint64(2147483649)},\n\n\t\t// Check any integer type where value is outside pg:int8 range cannot be encoded\n\t\t{\"select $1::int8\", uint64(9223372036854775809)},\n\t}\n\n\tfor i, tt := range failedEncodeTests {\n\t\terr := conn.QueryRow(context.Background(), tt.sql, tt.queryArg).Scan(nil)\n\t\tif err == nil {\n\t\t\tt.Errorf(\"%d. Expected failure to encode, but unexpectedly succeeded: %v (sql -> %v, queryArg -> %v)\", i, err, tt.sql, tt.queryArg)\n\t\t} else if !strings.Contains(err.Error(), \"is greater than\") {\n\t\t\tt.Errorf(\"%d. Expected failure to encode, but got: %v (sql -> %v, queryArg -> %v)\", i, err, tt.sql, tt.queryArg)\n\t\t}\n\n\t\tensureConnValid(t, conn)\n\t}\n}\n\nfunc TestQueryRowCoreIntegerDecoding(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\ttype allTypes struct {\n\t\tui   uint\n\t\tui8  uint8\n\t\tui16 uint16\n\t\tui32 uint32\n\t\tui64 uint64\n\t\ti    int\n\t\ti8   int8\n\t\ti16  int16\n\t\ti32  int32\n\t\ti64  int64\n\t}\n\n\tvar actual, zero allTypes\n\n\tsuccessfulDecodeTests := []struct {\n\t\tsql      string\n\t\tscanArg  any\n\t\texpected allTypes\n\t}{\n\t\t// Check any integer type where value is within Go:int range can be decoded\n\t\t{\"select 42::int2\", &actual.i, allTypes{i: 42}},\n\t\t{\"select 42::int4\", &actual.i, allTypes{i: 42}},\n\t\t{\"select 42::int8\", &actual.i, allTypes{i: 42}},\n\t\t{\"select -42::int2\", &actual.i, allTypes{i: -42}},\n\t\t{\"select -42::int4\", &actual.i, allTypes{i: -42}},\n\t\t{\"select -42::int8\", &actual.i, allTypes{i: -42}},\n\n\t\t// Check any integer type where value is within Go:int8 range can be decoded\n\t\t{\"select 42::int2\", &actual.i8, allTypes{i8: 42}},\n\t\t{\"select 42::int4\", &actual.i8, allTypes{i8: 42}},\n\t\t{\"select 42::int8\", &actual.i8, allTypes{i8: 42}},\n\t\t{\"select -42::int2\", &actual.i8, allTypes{i8: -42}},\n\t\t{\"select -42::int4\", &actual.i8, allTypes{i8: -42}},\n\t\t{\"select -42::int8\", &actual.i8, allTypes{i8: -42}},\n\n\t\t// Check any integer type where value is within Go:int16 range can be decoded\n\t\t{\"select 42::int2\", &actual.i16, allTypes{i16: 42}},\n\t\t{\"select 42::int4\", &actual.i16, allTypes{i16: 42}},\n\t\t{\"select 42::int8\", &actual.i16, allTypes{i16: 42}},\n\t\t{\"select -42::int2\", &actual.i16, allTypes{i16: -42}},\n\t\t{\"select -42::int4\", &actual.i16, allTypes{i16: -42}},\n\t\t{\"select -42::int8\", &actual.i16, allTypes{i16: -42}},\n\n\t\t// Check any integer type where value is within Go:int32 range can be decoded\n\t\t{\"select 42::int2\", &actual.i32, allTypes{i32: 42}},\n\t\t{\"select 42::int4\", &actual.i32, allTypes{i32: 42}},\n\t\t{\"select 42::int8\", &actual.i32, allTypes{i32: 42}},\n\t\t{\"select -42::int2\", &actual.i32, allTypes{i32: -42}},\n\t\t{\"select -42::int4\", &actual.i32, allTypes{i32: -42}},\n\t\t{\"select -42::int8\", &actual.i32, allTypes{i32: -42}},\n\n\t\t// Check any integer type where value is within Go:int64 range can be decoded\n\t\t{\"select 42::int2\", &actual.i64, allTypes{i64: 42}},\n\t\t{\"select 42::int4\", &actual.i64, allTypes{i64: 42}},\n\t\t{\"select 42::int8\", &actual.i64, allTypes{i64: 42}},\n\t\t{\"select -42::int2\", &actual.i64, allTypes{i64: -42}},\n\t\t{\"select -42::int4\", &actual.i64, allTypes{i64: -42}},\n\t\t{\"select -42::int8\", &actual.i64, allTypes{i64: -42}},\n\n\t\t// Check any integer type where value is within Go:uint range can be decoded\n\t\t{\"select 128::int2\", &actual.ui, allTypes{ui: 128}},\n\t\t{\"select 128::int4\", &actual.ui, allTypes{ui: 128}},\n\t\t{\"select 128::int8\", &actual.ui, allTypes{ui: 128}},\n\n\t\t// Check any integer type where value is within Go:uint8 range can be decoded\n\t\t{\"select 128::int2\", &actual.ui8, allTypes{ui8: 128}},\n\t\t{\"select 128::int4\", &actual.ui8, allTypes{ui8: 128}},\n\t\t{\"select 128::int8\", &actual.ui8, allTypes{ui8: 128}},\n\n\t\t// Check any integer type where value is within Go:uint16 range can be decoded\n\t\t{\"select 42::int2\", &actual.ui16, allTypes{ui16: 42}},\n\t\t{\"select 32768::int4\", &actual.ui16, allTypes{ui16: 32768}},\n\t\t{\"select 32768::int8\", &actual.ui16, allTypes{ui16: 32768}},\n\n\t\t// Check any integer type where value is within Go:uint32 range can be decoded\n\t\t{\"select 42::int2\", &actual.ui32, allTypes{ui32: 42}},\n\t\t{\"select 42::int4\", &actual.ui32, allTypes{ui32: 42}},\n\t\t{\"select 2147483648::int8\", &actual.ui32, allTypes{ui32: 2147483648}},\n\n\t\t// Check any integer type where value is within Go:uint64 range can be decoded\n\t\t{\"select 42::int2\", &actual.ui64, allTypes{ui64: 42}},\n\t\t{\"select 42::int4\", &actual.ui64, allTypes{ui64: 42}},\n\t\t{\"select 42::int8\", &actual.ui64, allTypes{ui64: 42}},\n\t}\n\n\tfor i, tt := range successfulDecodeTests {\n\t\tactual = zero\n\n\t\terr := conn.QueryRow(context.Background(), tt.sql).Scan(tt.scanArg)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%d. Unexpected failure: %v (sql -> %v)\", i, err, tt.sql)\n\t\t\tcontinue\n\t\t}\n\n\t\tif actual != tt.expected {\n\t\t\tt.Errorf(\"%d. Expected %v, got %v (sql -> %v)\", i, tt.expected, actual, tt.sql)\n\t\t}\n\n\t\tensureConnValid(t, conn)\n\t}\n\n\tfailedDecodeTests := []struct {\n\t\tsql     string\n\t\tscanArg any\n\t}{\n\t\t// Check any integer type where value is outside Go:int8 range cannot be decoded\n\t\t{\"select 128::int2\", &actual.i8},\n\t\t{\"select 128::int4\", &actual.i8},\n\t\t{\"select 128::int8\", &actual.i8},\n\t\t{\"select -129::int2\", &actual.i8},\n\t\t{\"select -129::int4\", &actual.i8},\n\t\t{\"select -129::int8\", &actual.i8},\n\n\t\t// Check any integer type where value is outside Go:int16 range cannot be decoded\n\t\t{\"select 32768::int4\", &actual.i16},\n\t\t{\"select 32768::int8\", &actual.i16},\n\t\t{\"select -32769::int4\", &actual.i16},\n\t\t{\"select -32769::int8\", &actual.i16},\n\n\t\t// Check any integer type where value is outside Go:int32 range cannot be decoded\n\t\t{\"select 2147483648::int8\", &actual.i32},\n\t\t{\"select -2147483649::int8\", &actual.i32},\n\n\t\t// Check any integer type where value is outside Go:uint range cannot be decoded\n\t\t{\"select -1::int2\", &actual.ui},\n\t\t{\"select -1::int4\", &actual.ui},\n\t\t{\"select -1::int8\", &actual.ui},\n\n\t\t// Check any integer type where value is outside Go:uint8 range cannot be decoded\n\t\t{\"select 256::int2\", &actual.ui8},\n\t\t{\"select 256::int4\", &actual.ui8},\n\t\t{\"select 256::int8\", &actual.ui8},\n\t\t{\"select -1::int2\", &actual.ui8},\n\t\t{\"select -1::int4\", &actual.ui8},\n\t\t{\"select -1::int8\", &actual.ui8},\n\n\t\t// Check any integer type where value is outside Go:uint16 cannot be decoded\n\t\t{\"select 65536::int4\", &actual.ui16},\n\t\t{\"select 65536::int8\", &actual.ui16},\n\t\t{\"select -1::int2\", &actual.ui16},\n\t\t{\"select -1::int4\", &actual.ui16},\n\t\t{\"select -1::int8\", &actual.ui16},\n\n\t\t// Check any integer type where value is outside Go:uint32 range cannot be decoded\n\t\t{\"select 4294967296::int8\", &actual.ui32},\n\t\t{\"select -1::int2\", &actual.ui32},\n\t\t{\"select -1::int4\", &actual.ui32},\n\t\t{\"select -1::int8\", &actual.ui32},\n\n\t\t// Check any integer type where value is outside Go:uint64 range cannot be decoded\n\t\t{\"select -1::int2\", &actual.ui64},\n\t\t{\"select -1::int4\", &actual.ui64},\n\t\t{\"select -1::int8\", &actual.ui64},\n\t}\n\n\tfor i, tt := range failedDecodeTests {\n\t\terr := conn.QueryRow(context.Background(), tt.sql).Scan(tt.scanArg)\n\t\tif err == nil {\n\t\t\tt.Errorf(\"%d. Expected failure to decode, but unexpectedly succeeded: %v (sql -> %v)\", i, err, tt.sql)\n\t\t} else if !strings.Contains(err.Error(), \"can't scan\") {\n\t\t\tt.Errorf(\"%d. Expected failure to decode, but got: %v (sql -> %v)\", i, err, tt.sql)\n\t\t}\n\n\t\tensureConnValid(t, conn)\n\t}\n}\n\nfunc TestQueryRowCoreByteSlice(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\ttests := []struct {\n\t\tsql      string\n\t\tqueryArg any\n\t\texpected []byte\n\t}{\n\t\t{\"select $1::text\", \"Jack\", []byte(\"Jack\")},\n\t\t{\"select $1::text\", []byte(\"Jack\"), []byte(\"Jack\")},\n\t\t{\"select $1::varchar\", []byte(\"Jack\"), []byte(\"Jack\")},\n\t\t{\"select $1::bytea\", []byte{0, 15, 255, 17}, []byte{0, 15, 255, 17}},\n\t}\n\n\tfor i, tt := range tests {\n\t\tvar actual []byte\n\n\t\terr := conn.QueryRow(context.Background(), tt.sql, tt.queryArg).Scan(&actual)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%d. Unexpected failure: %v (sql -> %v)\", i, err, tt.sql)\n\t\t}\n\n\t\tif !bytes.Equal(actual, tt.expected) {\n\t\t\tt.Errorf(\"%d. Expected %v, got %v (sql -> %v)\", i, tt.expected, actual, tt.sql)\n\t\t}\n\n\t\tensureConnValid(t, conn)\n\t}\n}\n\nfunc TestQueryRowErrors(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tif conn.PgConn().ParameterStatus(\"crdb_version\") != \"\" {\n\t\tt.Skip(\"Skipping due to known server missing point type\")\n\t}\n\n\ttype allTypes struct {\n\t\ti16 int16\n\t\ts   string\n\t}\n\n\tvar actual, zero allTypes\n\n\ttests := []struct {\n\t\tsql       string\n\t\tqueryArgs []any\n\t\tscanArgs  []any\n\t\terr       string\n\t}{\n\t\t{\"select $1::badtype\", []any{\"Jack\"}, []any{&actual.i16}, `type \"badtype\" does not exist`},\n\t\t{\"SYNTAX ERROR\", []any{}, []any{&actual.i16}, \"SQLSTATE 42601\"},\n\t\t{\"select $1::text\", []any{\"Jack\"}, []any{&actual.i16}, \"cannot scan text (OID 25) in text format into *int16\"},\n\t\t{\"select $1::point\", []any{int(705)}, []any{&actual.s}, \"unable to encode 705 into binary format for point (OID 600)\"},\n\t}\n\n\tfor i, tt := range tests {\n\t\tactual = zero\n\n\t\terr := conn.QueryRow(context.Background(), tt.sql, tt.queryArgs...).Scan(tt.scanArgs...)\n\t\tif err == nil {\n\t\t\tt.Errorf(\"%d. Unexpected success (sql -> %v, queryArgs -> %v)\", i, tt.sql, tt.queryArgs)\n\t\t}\n\t\tif err != nil && !strings.Contains(err.Error(), tt.err) {\n\t\t\tt.Errorf(\"%d. Expected error to contain %s, but got %v (sql -> %v, queryArgs -> %v)\", i, tt.err, err, tt.sql, tt.queryArgs)\n\t\t}\n\n\t\tensureConnValid(t, conn)\n\t}\n}\n\nfunc TestQueryRowNoResults(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tvar n int32\n\terr := conn.QueryRow(context.Background(), \"select 1 where 1=0\").Scan(&n)\n\tif err != pgx.ErrNoRows {\n\t\tt.Errorf(\"Expected pgx.ErrNoRows, got %v\", err)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestQueryRowEmptyQuery(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tvar n int32\n\terr := conn.QueryRow(ctx, \"\").Scan(&n)\n\trequire.Error(t, err)\n\trequire.False(t, pgconn.Timeout(err))\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestReadingValueAfterEmptyArray(t *testing.T) {\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tvar a []string\n\tvar b int32\n\terr := conn.QueryRow(context.Background(), \"select '{}'::text[], 42::integer\").Scan(&a, &b)\n\tif err != nil {\n\t\tt.Fatalf(\"conn.QueryRow failed: %v\", err)\n\t}\n\n\tif len(a) != 0 {\n\t\tt.Errorf(\"Expected 'a' to have length 0, but it was: %d\", len(a))\n\t}\n\n\tif b != 42 {\n\t\tt.Errorf(\"Expected 'b' to 42, but it was: %d\", b)\n\t}\n}\n\nfunc TestReadingNullByteArray(t *testing.T) {\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tvar a []byte\n\terr := conn.QueryRow(context.Background(), \"select null::text\").Scan(&a)\n\tif err != nil {\n\t\tt.Fatalf(\"conn.QueryRow failed: %v\", err)\n\t}\n\n\tif a != nil {\n\t\tt.Errorf(\"Expected 'a' to be nil, but it was: %v\", a)\n\t}\n}\n\nfunc TestReadingNullByteArrays(t *testing.T) {\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\trows, err := conn.Query(context.Background(), \"select null::text union all select null::text\")\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Query failed: %v\", err)\n\t}\n\n\tcount := 0\n\tfor rows.Next() {\n\t\tcount++\n\t\tvar a []byte\n\t\tif err := rows.Scan(&a); err != nil {\n\t\t\tt.Fatalf(\"failed to scan row: %v\", err)\n\t\t}\n\t\tif a != nil {\n\t\t\tt.Errorf(\"Expected 'a' to be nil, but it was: %v\", a)\n\t\t}\n\t}\n\tif count != 2 {\n\t\tt.Errorf(\"Expected to read 2 rows, read: %d\", count)\n\t}\n}\n\nfunc TestQueryNullSliceIsSet(t *testing.T) {\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\ta := []int32{1, 2, 3}\n\terr := conn.QueryRow(context.Background(), \"select null::int[]\").Scan(&a)\n\tif err != nil {\n\t\tt.Fatalf(\"conn.QueryRow failed: %v\", err)\n\t}\n\n\tif a != nil {\n\t\tt.Errorf(\"Expected 'a' to be nil, but it was: %v\", a)\n\t}\n}\n\nfunc TestConnQueryDatabaseSQLScanner(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tvar num sql.NullFloat64\n\n\terr := conn.QueryRow(context.Background(), \"select '1234.567'::float8\").Scan(&num)\n\tif err != nil {\n\t\tt.Fatalf(\"Scan failed: %v\", err)\n\t}\n\n\trequire.True(t, num.Valid)\n\trequire.Equal(t, 1234.567, num.Float64)\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnQueryDatabaseSQLDriverValuer(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\texpected := sql.NullFloat64{Float64: 1234.567, Valid: true}\n\tvar actual sql.NullFloat64\n\n\terr := conn.QueryRow(context.Background(), \"select $1::float8\", &expected).Scan(&actual)\n\trequire.NoError(t, err)\n\trequire.Equal(t, expected, actual)\n\n\tensureConnValid(t, conn)\n}\n\n// https://github.com/jackc/pgx/issues/339\nfunc TestConnQueryDatabaseSQLDriverValuerWithAutoGeneratedPointerReceiver(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tmustExec(t, conn, \"create temporary table t(n numeric)\")\n\n\tvar d *sql.NullInt64\n\tcommandTag, err := conn.Exec(context.Background(), `insert into t(n) values($1)`, d)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif commandTag.String() != \"INSERT 0 1\" {\n\t\tt.Fatalf(\"want %s, got %s\", \"INSERT 0 1\", commandTag)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\ntype nilPointerAsEmptyJSONObject struct {\n\tID   string\n\tName string\n}\n\nfunc (v *nilPointerAsEmptyJSONObject) Value() (driver.Value, error) {\n\tif v == nil {\n\t\treturn \"{}\", nil\n\t}\n\n\treturn json.Marshal(v)\n}\n\n// https://github.com/jackc/pgx/issues/1566\nfunc TestConnQueryDatabaseSQLDriverValuerCalledOnNilPointerImplementers(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tmustExec(t, conn, \"create temporary table t(v json not null)\")\n\n\tvar v *nilPointerAsEmptyJSONObject\n\tcommandTag, err := conn.Exec(context.Background(), `insert into t(v) values($1)`, v)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"INSERT 0 1\", commandTag.String())\n\n\tvar s string\n\terr = conn.QueryRow(context.Background(), \"select v from t\").Scan(&s)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"{}\", s)\n\n\t_, err = conn.Exec(context.Background(), `delete from t`)\n\trequire.NoError(t, err)\n\n\tv = &nilPointerAsEmptyJSONObject{ID: \"1\", Name: \"foo\"}\n\tcommandTag, err = conn.Exec(context.Background(), `insert into t(v) values($1)`, v)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"INSERT 0 1\", commandTag.String())\n\n\tvar v2 *nilPointerAsEmptyJSONObject\n\terr = conn.QueryRow(context.Background(), \"select v from t\").Scan(&v2)\n\trequire.NoError(t, err)\n\trequire.Equal(t, v, v2)\n\n\tensureConnValid(t, conn)\n}\n\ntype nilSliceAsEmptySlice []byte\n\nfunc (j nilSliceAsEmptySlice) Value() (driver.Value, error) {\n\tif len(j) == 0 {\n\t\treturn []byte(\"[]\"), nil\n\t}\n\n\treturn []byte(j), nil\n}\n\nfunc (j *nilSliceAsEmptySlice) UnmarshalJSON(data []byte) error {\n\t*j = bytes.Clone(data)\n\treturn nil\n}\n\n// https://github.com/jackc/pgx/issues/1860\nfunc TestConnQueryDatabaseSQLDriverValuerCalledOnNilSliceImplementers(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tmustExec(t, conn, \"create temporary table t(v json not null)\")\n\n\tvar v nilSliceAsEmptySlice\n\tcommandTag, err := conn.Exec(context.Background(), `insert into t(v) values($1)`, v)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"INSERT 0 1\", commandTag.String())\n\n\tvar s string\n\terr = conn.QueryRow(context.Background(), \"select v from t\").Scan(&s)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"[]\", s)\n\n\t_, err = conn.Exec(context.Background(), `delete from t`)\n\trequire.NoError(t, err)\n\n\tv = nilSliceAsEmptySlice(`{\"name\": \"foo\"}`)\n\tcommandTag, err = conn.Exec(context.Background(), `insert into t(v) values($1)`, v)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"INSERT 0 1\", commandTag.String())\n\n\tvar v2 nilSliceAsEmptySlice\n\terr = conn.QueryRow(context.Background(), \"select v from t\").Scan(&v2)\n\trequire.NoError(t, err)\n\trequire.Equal(t, v, v2)\n\n\tensureConnValid(t, conn)\n}\n\ntype nilMapAsEmptyObject map[string]any\n\nfunc (j nilMapAsEmptyObject) Value() (driver.Value, error) {\n\tif j == nil {\n\t\treturn []byte(\"{}\"), nil\n\t}\n\n\treturn json.Marshal(j)\n}\n\nfunc (j *nilMapAsEmptyObject) UnmarshalJSON(data []byte) error {\n\tvar m map[string]any\n\terr := json.Unmarshal(data, &m)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t*j = m\n\n\treturn nil\n}\n\n// https://github.com/jackc/pgx/pull/2019#discussion_r1605806751\nfunc TestConnQueryDatabaseSQLDriverValuerCalledOnNilMapImplementers(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tmustExec(t, conn, \"create temporary table t(v json not null)\")\n\n\tvar v nilMapAsEmptyObject\n\tcommandTag, err := conn.Exec(context.Background(), `insert into t(v) values($1)`, v)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"INSERT 0 1\", commandTag.String())\n\n\tvar s string\n\terr = conn.QueryRow(context.Background(), \"select v from t\").Scan(&s)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"{}\", s)\n\n\t_, err = conn.Exec(context.Background(), `delete from t`)\n\trequire.NoError(t, err)\n\n\tv = nilMapAsEmptyObject{\"name\": \"foo\"}\n\tcommandTag, err = conn.Exec(context.Background(), `insert into t(v) values($1)`, v)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"INSERT 0 1\", commandTag.String())\n\n\tvar v2 nilMapAsEmptyObject\n\terr = conn.QueryRow(context.Background(), \"select v from t\").Scan(&v2)\n\trequire.NoError(t, err)\n\trequire.Equal(t, v, v2)\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnQueryDatabaseSQLDriverScannerWithBinaryPgTypeThatAcceptsSameType(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tvar actual sql.NullString\n\terr := conn.QueryRow(context.Background(), \"select '6ba7b810-9dad-11d1-80b4-00c04fd430c8'::uuid\").Scan(&actual)\n\trequire.NoError(t, err)\n\n\trequire.True(t, actual.Valid)\n\trequire.Equal(t, \"6ba7b810-9dad-11d1-80b4-00c04fd430c8\", actual.String)\n\n\tensureConnValid(t, conn)\n}\n\n// https://github.com/jackc/pgx/issues/1273#issuecomment-1221672175\nfunc TestConnQueryDatabaseSQLDriverValuerTextWhenBinaryIsPreferred(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\targ := sql.NullString{String: \"1.234\", Valid: true}\n\tvar result pgtype.Numeric\n\terr := conn.QueryRow(context.Background(), \"select $1::numeric\", arg).Scan(&result)\n\trequire.NoError(t, err)\n\n\trequire.True(t, result.Valid)\n\tf64, err := result.Float64Value()\n\trequire.NoError(t, err)\n\trequire.Equal(t, pgtype.Float8{Float64: 1.234, Valid: true}, f64)\n\n\tensureConnValid(t, conn)\n}\n\n// https://github.com/jackc/pgx/issues/1426\nfunc TestConnQueryDatabaseSQLNullFloat64NegativeZeroPointZero(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\ttests := []float64{\n\t\t-0.01,\n\t\t-0.001,\n\t\t-0.0001,\n\t}\n\n\tfor _, val := range tests {\n\t\tvar result sql.NullFloat64\n\t\terr := conn.QueryRow(context.Background(), \"select $1::numeric\", val).Scan(&result)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, sql.NullFloat64{Float64: val, Valid: true}, result)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnQueryDatabaseSQLNullX(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\ttype row struct {\n\t\tboolValid    sql.NullBool\n\t\tboolNull     sql.NullBool\n\t\tint64Valid   sql.NullInt64\n\t\tint64Null    sql.NullInt64\n\t\tfloat64Valid sql.NullFloat64\n\t\tfloat64Null  sql.NullFloat64\n\t\tstringValid  sql.NullString\n\t\tstringNull   sql.NullString\n\t}\n\n\texpected := row{\n\t\tboolValid:    sql.NullBool{Bool: true, Valid: true},\n\t\tint64Valid:   sql.NullInt64{Int64: 123, Valid: true},\n\t\tfloat64Valid: sql.NullFloat64{Float64: 3.14, Valid: true},\n\t\tstringValid:  sql.NullString{String: \"pgx\", Valid: true},\n\t}\n\n\tvar actual row\n\n\terr := conn.QueryRow(\n\t\tcontext.Background(),\n\t\t\"select $1::bool, $2::bool, $3::int8, $4::int8, $5::float8, $6::float8, $7::text, $8::text\",\n\t\texpected.boolValid,\n\t\texpected.boolNull,\n\t\texpected.int64Valid,\n\t\texpected.int64Null,\n\t\texpected.float64Valid,\n\t\texpected.float64Null,\n\t\texpected.stringValid,\n\t\texpected.stringNull,\n\t).Scan(\n\t\t&actual.boolValid,\n\t\t&actual.boolNull,\n\t\t&actual.int64Valid,\n\t\t&actual.int64Null,\n\t\t&actual.float64Valid,\n\t\t&actual.float64Null,\n\t\t&actual.stringValid,\n\t\t&actual.stringNull,\n\t)\n\tif err != nil {\n\t\tt.Fatalf(\"Scan failed: %v\", err)\n\t}\n\n\tif expected != actual {\n\t\tt.Errorf(\"Expected %v, but got %v\", expected, actual)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestQueryContextSuccess(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tctx, cancelFunc := context.WithCancel(context.Background())\n\tdefer cancelFunc()\n\n\trows, err := conn.Query(ctx, \"select 42::integer\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar result, rowCount int\n\tfor rows.Next() {\n\t\terr = rows.Scan(&result)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\trowCount++\n\t}\n\n\tif rows.Err() != nil {\n\t\tt.Fatal(rows.Err())\n\t}\n\n\tif rowCount != 1 {\n\t\tt.Fatalf(\"Expected 1 row, got %d\", rowCount)\n\t}\n\tif result != 42 {\n\t\tt.Fatalf(\"Expected result 42, got %d\", result)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestQueryContextErrorWhileReceivingRows(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server uses numeric instead of int\")\n\n\tctx, cancelFunc := context.WithCancel(context.Background())\n\tdefer cancelFunc()\n\n\trows, err := conn.Query(ctx, \"select 10/(10-n) from generate_series(1, 100) n\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar result, rowCount int\n\tfor rows.Next() {\n\t\terr = rows.Scan(&result)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\trowCount++\n\t}\n\n\tif rows.Err() == nil || rows.Err().Error() != \"ERROR: division by zero (SQLSTATE 22012)\" {\n\t\tt.Fatalf(\"Expected division by zero error, but got %v\", rows.Err())\n\t}\n\n\tif rowCount != 9 {\n\t\tt.Fatalf(\"Expected 9 rows, got %d\", rowCount)\n\t}\n\tif result != 10 {\n\t\tt.Fatalf(\"Expected result 10, got %d\", result)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestQueryRowContextSuccess(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tctx, cancelFunc := context.WithCancel(context.Background())\n\tdefer cancelFunc()\n\n\tvar result int\n\terr := conn.QueryRow(ctx, \"select 42::integer\").Scan(&result)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif result != 42 {\n\t\tt.Fatalf(\"Expected result 42, got %d\", result)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestQueryRowContextErrorWhileReceivingRow(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tctx, cancelFunc := context.WithCancel(context.Background())\n\tdefer cancelFunc()\n\n\tvar result int\n\terr := conn.QueryRow(ctx, \"select 10/0\").Scan(&result)\n\tif err == nil || err.Error() != \"ERROR: division by zero (SQLSTATE 22012)\" {\n\t\tt.Fatalf(\"Expected division by zero error, but got %v\", err)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestQueryCloseBefore(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tcloseConn(t, conn)\n\n\t_, err := conn.Query(context.Background(), \"select 1\")\n\trequire.Error(t, err)\n\tassert.True(t, pgconn.SafeToRetry(err))\n}\n\nfunc TestScanRow(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tresultReader := conn.PgConn().ExecParams(context.Background(), \"select generate_series(1,$1)\", [][]byte{[]byte(\"10\")}, nil, nil, nil)\n\n\tvar sum, rowCount int32\n\n\tfor resultReader.NextRow() {\n\t\tvar n int32\n\t\terr := pgx.ScanRow(conn.TypeMap(), resultReader.FieldDescriptions(), resultReader.Values(), &n)\n\t\tassert.NoError(t, err)\n\t\tsum += n\n\t\trowCount++\n\t}\n\n\t_, err := resultReader.Close()\n\n\trequire.NoError(t, err)\n\tassert.EqualValues(t, 10, rowCount)\n\tassert.EqualValues(t, 55, sum)\n}\n\nfunc TestConnSimpleProtocol(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\t// Test all supported low-level types\n\n\t{\n\t\texpected := int64(42)\n\t\tvar actual int64\n\t\terr := conn.QueryRow(\n\t\t\tcontext.Background(),\n\t\t\t\"select $1::int8\",\n\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\texpected,\n\t\t).Scan(&actual)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tif expected != actual {\n\t\t\tt.Errorf(\"expected %v got %v\", expected, actual)\n\t\t}\n\t}\n\n\t{\n\t\texpected := float64(1.23)\n\t\tvar actual float64\n\t\terr := conn.QueryRow(\n\t\t\tcontext.Background(),\n\t\t\t\"select $1::float8\",\n\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\texpected,\n\t\t).Scan(&actual)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tif expected != actual {\n\t\t\tt.Errorf(\"expected %v got %v\", expected, actual)\n\t\t}\n\t}\n\n\t{\n\t\texpected := true\n\t\tvar actual bool\n\t\terr := conn.QueryRow(\n\t\t\tcontext.Background(),\n\t\t\t\"select $1::boolean\",\n\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\texpected,\n\t\t).Scan(&actual)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tif expected != actual {\n\t\t\tt.Errorf(\"expected %v got %v\", expected, actual)\n\t\t}\n\t}\n\n\t{\n\t\texpected := []byte{0, 1, 20, 35, 64, 80, 120, 3, 255, 240, 128, 95}\n\t\tvar actual []byte\n\t\terr := conn.QueryRow(\n\t\t\tcontext.Background(),\n\t\t\t\"select $1::bytea\",\n\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\texpected,\n\t\t).Scan(&actual)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tif !bytes.Equal(actual, expected) {\n\t\t\tt.Errorf(\"expected %v got %v\", expected, actual)\n\t\t}\n\t}\n\n\t{\n\t\texpected := \"test\"\n\t\tvar actual string\n\t\terr := conn.QueryRow(\n\t\t\tcontext.Background(),\n\t\t\t\"select $1::text\",\n\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\texpected,\n\t\t).Scan(&actual)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tif expected != actual {\n\t\t\tt.Errorf(\"expected %v got %v\", expected, actual)\n\t\t}\n\t}\n\n\t{\n\t\ttests := []struct {\n\t\t\texpected []string\n\t\t}{\n\t\t\t{[]string(nil)},\n\t\t\t{[]string{}},\n\t\t\t{[]string{\"test\", \"foo\", \"bar\"}},\n\t\t\t{[]string{`foo'bar\"\\baz;quz`, `foo'bar\"\\baz;quz`}},\n\t\t}\n\t\tfor i, tt := range tests {\n\t\t\tvar actual []string\n\t\t\terr := conn.QueryRow(\n\t\t\t\tcontext.Background(),\n\t\t\t\t\"select $1::text[]\",\n\t\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\t\ttt.expected,\n\t\t\t).Scan(&actual)\n\t\t\tassert.NoErrorf(t, err, \"%d\", i)\n\t\t\tassert.Equalf(t, tt.expected, actual, \"%d\", i)\n\t\t}\n\t}\n\n\t{\n\t\ttests := []struct {\n\t\t\texpected []int16\n\t\t}{\n\t\t\t{[]int16(nil)},\n\t\t\t{[]int16{}},\n\t\t\t{[]int16{1, 2, 3}},\n\t\t}\n\t\tfor i, tt := range tests {\n\t\t\tvar actual []int16\n\t\t\terr := conn.QueryRow(\n\t\t\t\tcontext.Background(),\n\t\t\t\t\"select $1::smallint[]\",\n\t\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\t\ttt.expected,\n\t\t\t).Scan(&actual)\n\t\t\tassert.NoErrorf(t, err, \"%d\", i)\n\t\t\tassert.Equalf(t, tt.expected, actual, \"%d\", i)\n\t\t}\n\t}\n\n\t{\n\t\ttests := []struct {\n\t\t\texpected []int32\n\t\t}{\n\t\t\t{[]int32(nil)},\n\t\t\t{[]int32{}},\n\t\t\t{[]int32{1, 2, 3}},\n\t\t}\n\t\tfor i, tt := range tests {\n\t\t\tvar actual []int32\n\t\t\terr := conn.QueryRow(\n\t\t\t\tcontext.Background(),\n\t\t\t\t\"select $1::int[]\",\n\t\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\t\ttt.expected,\n\t\t\t).Scan(&actual)\n\t\t\tassert.NoErrorf(t, err, \"%d\", i)\n\t\t\tassert.Equalf(t, tt.expected, actual, \"%d\", i)\n\t\t}\n\t}\n\n\t{\n\t\ttests := []struct {\n\t\t\texpected []int64\n\t\t}{\n\t\t\t{[]int64(nil)},\n\t\t\t{[]int64{}},\n\t\t\t{[]int64{1, 2, 3}},\n\t\t}\n\t\tfor i, tt := range tests {\n\t\t\tvar actual []int64\n\t\t\terr := conn.QueryRow(\n\t\t\t\tcontext.Background(),\n\t\t\t\t\"select $1::bigint[]\",\n\t\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\t\ttt.expected,\n\t\t\t).Scan(&actual)\n\t\t\tassert.NoErrorf(t, err, \"%d\", i)\n\t\t\tassert.Equalf(t, tt.expected, actual, \"%d\", i)\n\t\t}\n\t}\n\n\t{\n\t\ttests := []struct {\n\t\t\texpected []int\n\t\t}{\n\t\t\t{[]int(nil)},\n\t\t\t{[]int{}},\n\t\t\t{[]int{1, 2, 3}},\n\t\t}\n\t\tfor i, tt := range tests {\n\t\t\tvar actual []int\n\t\t\terr := conn.QueryRow(\n\t\t\t\tcontext.Background(),\n\t\t\t\t\"select $1::bigint[]\",\n\t\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\t\ttt.expected,\n\t\t\t).Scan(&actual)\n\t\t\tassert.NoErrorf(t, err, \"%d\", i)\n\t\t\tassert.Equalf(t, tt.expected, actual, \"%d\", i)\n\t\t}\n\t}\n\n\t{\n\t\ttests := []struct {\n\t\t\texpected []uint16\n\t\t}{\n\t\t\t{[]uint16(nil)},\n\t\t\t{[]uint16{}},\n\t\t\t{[]uint16{1, 2, 3}},\n\t\t}\n\t\tfor i, tt := range tests {\n\t\t\tvar actual []uint16\n\t\t\terr := conn.QueryRow(\n\t\t\t\tcontext.Background(),\n\t\t\t\t\"select $1::smallint[]\",\n\t\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\t\ttt.expected,\n\t\t\t).Scan(&actual)\n\t\t\tassert.NoErrorf(t, err, \"%d\", i)\n\t\t\tassert.Equalf(t, tt.expected, actual, \"%d\", i)\n\t\t}\n\t}\n\n\t{\n\t\ttests := []struct {\n\t\t\texpected []uint32\n\t\t}{\n\t\t\t{[]uint32(nil)},\n\t\t\t{[]uint32{}},\n\t\t\t{[]uint32{1, 2, 3}},\n\t\t}\n\t\tfor i, tt := range tests {\n\t\t\tvar actual []uint32\n\t\t\terr := conn.QueryRow(\n\t\t\t\tcontext.Background(),\n\t\t\t\t\"select $1::bigint[]\",\n\t\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\t\ttt.expected,\n\t\t\t).Scan(&actual)\n\t\t\tassert.NoErrorf(t, err, \"%d\", i)\n\t\t\tassert.Equalf(t, tt.expected, actual, \"%d\", i)\n\t\t}\n\t}\n\n\t{\n\t\ttests := []struct {\n\t\t\texpected []uint64\n\t\t}{\n\t\t\t{[]uint64(nil)},\n\t\t\t{[]uint64{}},\n\t\t\t{[]uint64{1, 2, 3}},\n\t\t}\n\t\tfor i, tt := range tests {\n\t\t\tvar actual []uint64\n\t\t\terr := conn.QueryRow(\n\t\t\t\tcontext.Background(),\n\t\t\t\t\"select $1::bigint[]\",\n\t\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\t\ttt.expected,\n\t\t\t).Scan(&actual)\n\t\t\tassert.NoErrorf(t, err, \"%d\", i)\n\t\t\tassert.Equalf(t, tt.expected, actual, \"%d\", i)\n\t\t}\n\t}\n\n\t{\n\t\ttests := []struct {\n\t\t\texpected []uint\n\t\t}{\n\t\t\t{[]uint(nil)},\n\t\t\t{[]uint{}},\n\t\t\t{[]uint{1, 2, 3}},\n\t\t}\n\t\tfor i, tt := range tests {\n\t\t\tvar actual []uint\n\t\t\terr := conn.QueryRow(\n\t\t\t\tcontext.Background(),\n\t\t\t\t\"select $1::bigint[]\",\n\t\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\t\ttt.expected,\n\t\t\t).Scan(&actual)\n\t\t\tassert.NoErrorf(t, err, \"%d\", i)\n\t\t\tassert.Equalf(t, tt.expected, actual, \"%d\", i)\n\t\t}\n\t}\n\n\t{\n\t\ttests := []struct {\n\t\t\texpected []float32\n\t\t}{\n\t\t\t{[]float32(nil)},\n\t\t\t{[]float32{}},\n\t\t\t{[]float32{1, 2, 3}},\n\t\t}\n\t\tfor i, tt := range tests {\n\t\t\tvar actual []float32\n\t\t\terr := conn.QueryRow(\n\t\t\t\tcontext.Background(),\n\t\t\t\t\"select $1::float4[]\",\n\t\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\t\ttt.expected,\n\t\t\t).Scan(&actual)\n\t\t\tassert.NoErrorf(t, err, \"%d\", i)\n\t\t\tassert.Equalf(t, tt.expected, actual, \"%d\", i)\n\t\t}\n\t}\n\n\t{\n\t\ttests := []struct {\n\t\t\texpected []float64\n\t\t}{\n\t\t\t{[]float64(nil)},\n\t\t\t{[]float64{}},\n\t\t\t{[]float64{1, 2, 3}},\n\t\t}\n\t\tfor i, tt := range tests {\n\t\t\tvar actual []float64\n\t\t\terr := conn.QueryRow(\n\t\t\t\tcontext.Background(),\n\t\t\t\t\"select $1::float8[]\",\n\t\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\t\ttt.expected,\n\t\t\t).Scan(&actual)\n\t\t\tassert.NoErrorf(t, err, \"%d\", i)\n\t\t\tassert.Equalf(t, tt.expected, actual, \"%d\", i)\n\t\t}\n\t}\n\n\t// Test high-level type\n\n\t{\n\t\tif conn.PgConn().ParameterStatus(\"crdb_version\") == \"\" {\n\t\t\t// CockroachDB doesn't support circle type.\n\t\t\texpected := pgtype.Circle{P: pgtype.Vec2{X: 1, Y: 2}, R: 1.5, Valid: true}\n\t\t\tactual := expected\n\t\t\terr := conn.QueryRow(\n\t\t\t\tcontext.Background(),\n\t\t\t\t\"select $1::circle\",\n\t\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\t\t&expected,\n\t\t\t).Scan(&actual)\n\t\t\tif err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t}\n\t\t\tif expected != actual {\n\t\t\t\tt.Errorf(\"expected %v got %v\", expected, actual)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Test multiple args in single query\n\n\t{\n\t\texpectedInt64 := int64(234423)\n\t\texpectedFloat64 := float64(-0.2312)\n\t\texpectedBool := true\n\t\texpectedBytes := []byte{255, 0, 23, 16, 87, 45, 9, 23, 45, 223}\n\t\texpectedString := \"test\"\n\t\tvar actualInt64 int64\n\t\tvar actualFloat64 float64\n\t\tvar actualBool bool\n\t\tvar actualBytes []byte\n\t\tvar actualString string\n\t\terr := conn.QueryRow(\n\t\t\tcontext.Background(),\n\t\t\t\"select $1::int8, $2::float8, $3::boolean, $4::bytea, $5::text\",\n\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\texpectedInt64, expectedFloat64, expectedBool, expectedBytes, expectedString,\n\t\t).Scan(&actualInt64, &actualFloat64, &actualBool, &actualBytes, &actualString)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tif expectedInt64 != actualInt64 {\n\t\t\tt.Errorf(\"expected %v got %v\", expectedInt64, actualInt64)\n\t\t}\n\t\tif expectedFloat64 != actualFloat64 {\n\t\t\tt.Errorf(\"expected %v got %v\", expectedFloat64, actualFloat64)\n\t\t}\n\t\tif expectedBool != actualBool {\n\t\t\tt.Errorf(\"expected %v got %v\", expectedBool, actualBool)\n\t\t}\n\t\tif !bytes.Equal(expectedBytes, actualBytes) {\n\t\t\tt.Errorf(\"expected %v got %v\", expectedBytes, actualBytes)\n\t\t}\n\t\tif expectedString != actualString {\n\t\t\tt.Errorf(\"expected %v got %v\", expectedString, actualString)\n\t\t}\n\t}\n\n\t// Test dangerous cases\n\n\t{\n\t\texpected := \"foo';drop table users;\"\n\t\tvar actual string\n\t\terr := conn.QueryRow(\n\t\t\tcontext.Background(),\n\t\t\t\"select $1\",\n\t\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\texpected,\n\t\t).Scan(&actual)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\tif expected != actual {\n\t\t\tt.Errorf(\"expected %v got %v\", expected, actual)\n\t\t}\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnSimpleProtocolRefusesNonUTF8ClientEncoding(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server does not support changing client_encoding (https://www.cockroachlabs.com/docs/stable/set-vars.html)\")\n\n\tmustExec(t, conn, \"set client_encoding to 'SQL_ASCII'\")\n\n\tvar expected string\n\terr := conn.QueryRow(\n\t\tcontext.Background(),\n\t\t\"select $1\",\n\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t\"test\",\n\t).Scan(&expected)\n\tif err == nil {\n\t\tt.Error(\"expected error when client_encoding not UTF8, but no error occurred\")\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnSimpleProtocolRefusesNonStandardConformingStrings(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server does not support standard_conforming_strings = off (https://github.com/cockroachdb/cockroach/issues/36215)\")\n\n\tmustExec(t, conn, \"set standard_conforming_strings to off\")\n\n\tvar expected string\n\terr := conn.QueryRow(\n\t\tcontext.Background(),\n\t\t\"select $1\",\n\t\tpgx.QueryExecModeSimpleProtocol,\n\t\t`\\'; drop table users; --`,\n\t).Scan(&expected)\n\tif err == nil {\n\t\tt.Error(\"expected error when standard_conforming_strings is off, but no error occurred\")\n\t}\n\n\tensureConnValid(t, conn)\n}\n\n// https://github.com/jackc/pgx/issues/895\nfunc TestQueryErrorWithDisabledStatementCache(t *testing.T) {\n\tt.Parallel()\n\n\tconfig := mustParseConfig(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tconfig.DefaultQueryExecMode = pgx.QueryExecModeDescribeExec\n\tconfig.StatementCacheCapacity = 0\n\tconfig.DescriptionCacheCapacity = 0\n\n\tconn := mustConnect(t, config)\n\tdefer closeConn(t, conn)\n\n\t_, err := conn.Exec(context.Background(), \"create temporary table t_unq(id text primary key);\")\n\trequire.NoError(t, err)\n\n\t_, err = conn.Exec(context.Background(), \"insert into t_unq (id) values ($1)\", \"abc\")\n\trequire.NoError(t, err)\n\n\trows, err := conn.Query(context.Background(), \"insert into t_unq (id) values ($1)\", \"abc\")\n\trequire.NoError(t, err)\n\trows.Close()\n\terr = rows.Err()\n\trequire.Error(t, err)\n\tvar pgErr *pgconn.PgError\n\tif errors.As(err, &pgErr) {\n\t\tassert.Equal(t, \"23505\", pgErr.Code)\n\t} else {\n\t\tt.Errorf(\"err is not a *pgconn.PgError: %T\", err)\n\t}\n\n\tensureConnValid(t, conn)\n}\n\nfunc TestConnQueryQueryExecModeCacheDescribeSafeEvenWhenTypesChange(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server does not support alter column type from int to float4\")\n\n\t_, err := conn.Exec(ctx, `create temporary table to_change (\n\tname text primary key,\n\tage int\n);\n\ninsert into to_change (name, age) values ('John', 42);`)\n\trequire.NoError(t, err)\n\n\tvar name string\n\tvar ageInt32 int32\n\terr = conn.QueryRow(ctx, \"select * from to_change where age = $1\", pgx.QueryExecModeCacheDescribe, int32(42)).Scan(&name, &ageInt32)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"John\", name)\n\trequire.Equal(t, int32(42), ageInt32)\n\n\t_, err = conn.Exec(ctx, `alter table to_change alter column age type float4;`)\n\trequire.NoError(t, err)\n\n\terr = conn.QueryRow(ctx, \"select * from to_change where age = $1\", pgx.QueryExecModeCacheDescribe, int32(42)).Scan(&name, &ageInt32)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"John\", name)\n\trequire.Equal(t, int32(42), ageInt32)\n\n\tvar ageFloat32 float32\n\terr = conn.QueryRow(ctx, \"select * from to_change where age = $1\", pgx.QueryExecModeCacheDescribe, int32(42)).Scan(&name, &ageFloat32)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"John\", name)\n\trequire.Equal(t, float32(42), ageFloat32)\n\n\t_, err = conn.Exec(ctx, `alter table to_change drop column name;`)\n\trequire.NoError(t, err)\n\n\t// Number of result columns has changed, so just like with a prepared statement, this will fail the first time.\n\terr = conn.QueryRow(ctx, \"select * from to_change where age = $1\", pgx.QueryExecModeCacheDescribe, int32(42)).Scan(&ageFloat32)\n\trequire.EqualError(t, err, \"ERROR: bind message has 2 result formats but query has 1 columns (SQLSTATE 08P01)\")\n\n\t// But it will work the second time after the cache is invalidated.\n\terr = conn.QueryRow(ctx, \"select * from to_change where age = $1\", pgx.QueryExecModeCacheDescribe, int32(42)).Scan(&ageFloat32)\n\trequire.NoError(t, err)\n\trequire.Equal(t, float32(42), ageFloat32)\n\n\t_, err = conn.Exec(ctx, `alter table to_change alter column age type numeric;`)\n\trequire.NoError(t, err)\n\n\terr = conn.QueryRow(ctx, \"select * from to_change where age = $1\", pgx.QueryExecModeCacheDescribe, int32(42)).Scan(&ageFloat32)\n\trequire.NoError(t, err)\n\trequire.Equal(t, float32(42), ageFloat32)\n}\n\nfunc TestQueryWithQueryRewriter(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tqr := testQueryRewriter{sql: \"select $1::int\", args: []any{42}}\n\t\trows, err := conn.Query(ctx, \"should be replaced\", &qr)\n\t\trequire.NoError(t, err)\n\n\t\tvar n int32\n\t\tvar rowCount int\n\t\tfor rows.Next() {\n\t\t\trowCount++\n\t\t\terr = rows.Scan(&n)\n\t\t\trequire.NoError(t, err)\n\t\t}\n\n\t\trequire.NoError(t, rows.Err())\n\t})\n}\n\n// This example uses Query without using any helpers to read the results. Normally CollectRows, ForEachRow, or another\n// helper function should be used.\nfunc ExampleConn_Query() {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn, err := pgx.Connect(ctx, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tif err != nil {\n\t\tfmt.Printf(\"Unable to establish connection: %v\", err)\n\t\treturn\n\t}\n\n\tif conn.PgConn().ParameterStatus(\"crdb_version\") != \"\" {\n\t\t// Skip test / example when running on CockroachDB. Since an example can't be skipped fake success instead.\n\t\tfmt.Println(`Cheeseburger: $10\nFries: $5\nSoft Drink: $3`)\n\t\treturn\n\t}\n\n\t// Setup example schema and data.\n\t_, err = conn.Exec(ctx, `\ncreate temporary table products (\n\tid int primary key generated by default as identity,\n\tname varchar(100) not null,\n\tprice int not null\n);\n\ninsert into products (name, price) values\n\t('Cheeseburger', 10),\n\t('Double Cheeseburger', 14),\n\t('Fries', 5),\n\t('Soft Drink', 3);\n`)\n\tif err != nil {\n\t\tfmt.Printf(\"Unable to setup example schema and data: %v\", err)\n\t\treturn\n\t}\n\n\trows, err := conn.Query(ctx, \"select name, price from products where price < $1 order by price desc\", 12)\n\n\t// It is unnecessary to check err. If an error occurred it will be returned by rows.Err() later. But in rare\n\t// cases it may be useful to detect the error as early as possible.\n\tif err != nil {\n\t\tfmt.Printf(\"Query error: %v\", err)\n\t\treturn\n\t}\n\n\t// Ensure rows is closed. It is safe to close rows multiple times.\n\tdefer rows.Close()\n\n\t// Iterate through the result set\n\tfor rows.Next() {\n\t\tvar name string\n\t\tvar price int32\n\n\t\terr = rows.Scan(&name, &price)\n\t\tif err != nil {\n\t\t\tfmt.Printf(\"Scan error: %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tfmt.Printf(\"%s: $%d\\n\", name, price)\n\t}\n\n\t// rows is closed automatically when rows.Next() returns false so it is not necessary to manually close rows.\n\n\t// The first error encountered by the original Query call, rows.Next or rows.Scan will be returned here.\n\tif rows.Err() != nil {\n\t\tfmt.Printf(\"rows error: %v\", rows.Err())\n\t\treturn\n\t}\n\n\t// Output:\n\t// Cheeseburger: $10\n\t// Fries: $5\n\t// Soft Drink: $3\n}\n"
        },
        {
          "name": "rows.go",
          "type": "blob",
          "size": 23.7392578125,
          "content": "package pgx\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgtype\"\n)\n\n// Rows is the result set returned from *Conn.Query. Rows must be closed before\n// the *Conn can be used again. Rows are closed by explicitly calling Close(),\n// calling Next() until it returns false, or when a fatal error occurs.\n//\n// Once a Rows is closed the only methods that may be called are Close(), Err(),\n// and CommandTag().\n//\n// Rows is an interface instead of a struct to allow tests to mock Query. However,\n// adding a method to an interface is technically a breaking change. Because of this\n// the Rows interface is partially excluded from semantic version requirements.\n// Methods will not be removed or changed, but new methods may be added.\ntype Rows interface {\n\t// Close closes the rows, making the connection ready for use again. It is safe\n\t// to call Close after rows is already closed.\n\tClose()\n\n\t// Err returns any error that occurred while reading. Err must only be called after the Rows is closed (either by\n\t// calling Close or by Next returning false). If it is called early it may return nil even if there was an error\n\t// executing the query.\n\tErr() error\n\n\t// CommandTag returns the command tag from this query. It is only available after Rows is closed.\n\tCommandTag() pgconn.CommandTag\n\n\t// FieldDescriptions returns the field descriptions of the columns. It may return nil. In particular this can occur\n\t// when there was an error executing the query.\n\tFieldDescriptions() []pgconn.FieldDescription\n\n\t// Next prepares the next row for reading. It returns true if there is another\n\t// row and false if no more rows are available or a fatal error has occurred.\n\t// It automatically closes rows when all rows are read.\n\t//\n\t// Callers should check rows.Err() after rows.Next() returns false to detect\n\t// whether result-set reading ended prematurely due to an error. See\n\t// Conn.Query for details.\n\t//\n\t// For simpler error handling, consider using the higher-level pgx v5\n\t// CollectRows() and ForEachRow() helpers instead.\n\tNext() bool\n\n\t// Scan reads the values from the current row into dest values positionally.\n\t// dest can include pointers to core types, values implementing the Scanner\n\t// interface, and nil. nil will skip the value entirely. It is an error to\n\t// call Scan without first calling Next() and checking that it returned true.\n\tScan(dest ...any) error\n\n\t// Values returns the decoded row values. As with Scan(), it is an error to\n\t// call Values without first calling Next() and checking that it returned\n\t// true.\n\tValues() ([]any, error)\n\n\t// RawValues returns the unparsed bytes of the row values. The returned data is only valid until the next Next\n\t// call or the Rows is closed.\n\tRawValues() [][]byte\n\n\t// Conn returns the underlying *Conn on which the query was executed. This may return nil if Rows did not come from a\n\t// *Conn (e.g. if it was created by RowsFromResultReader)\n\tConn() *Conn\n}\n\n// Row is a convenience wrapper over Rows that is returned by QueryRow.\n//\n// Row is an interface instead of a struct to allow tests to mock QueryRow. However,\n// adding a method to an interface is technically a breaking change. Because of this\n// the Row interface is partially excluded from semantic version requirements.\n// Methods will not be removed or changed, but new methods may be added.\ntype Row interface {\n\t// Scan works the same as Rows. with the following exceptions. If no\n\t// rows were found it returns ErrNoRows. If multiple rows are returned it\n\t// ignores all but the first.\n\tScan(dest ...any) error\n}\n\n// RowScanner scans an entire row at a time into the RowScanner.\ntype RowScanner interface {\n\t// ScanRows scans the row.\n\tScanRow(rows Rows) error\n}\n\n// connRow implements the Row interface for Conn.QueryRow.\ntype connRow baseRows\n\nfunc (r *connRow) Scan(dest ...any) (err error) {\n\trows := (*baseRows)(r)\n\n\tif rows.Err() != nil {\n\t\treturn rows.Err()\n\t}\n\n\tfor _, d := range dest {\n\t\tif _, ok := d.(*pgtype.DriverBytes); ok {\n\t\t\trows.Close()\n\t\t\treturn fmt.Errorf(\"cannot scan into *pgtype.DriverBytes from QueryRow\")\n\t\t}\n\t}\n\n\tif !rows.Next() {\n\t\tif rows.Err() == nil {\n\t\t\treturn ErrNoRows\n\t\t}\n\t\treturn rows.Err()\n\t}\n\n\trows.Scan(dest...)\n\trows.Close()\n\treturn rows.Err()\n}\n\n// baseRows implements the Rows interface for Conn.Query.\ntype baseRows struct {\n\ttypeMap      *pgtype.Map\n\tresultReader *pgconn.ResultReader\n\n\tvalues [][]byte\n\n\tcommandTag pgconn.CommandTag\n\terr        error\n\tclosed     bool\n\n\tscanPlans []pgtype.ScanPlan\n\tscanTypes []reflect.Type\n\n\tconn              *Conn\n\tmultiResultReader *pgconn.MultiResultReader\n\n\tqueryTracer QueryTracer\n\tbatchTracer BatchTracer\n\tctx         context.Context\n\tstartTime   time.Time\n\tsql         string\n\targs        []any\n\trowCount    int\n}\n\nfunc (rows *baseRows) FieldDescriptions() []pgconn.FieldDescription {\n\treturn rows.resultReader.FieldDescriptions()\n}\n\nfunc (rows *baseRows) Close() {\n\tif rows.closed {\n\t\treturn\n\t}\n\n\trows.closed = true\n\n\tif rows.resultReader != nil {\n\t\tvar closeErr error\n\t\trows.commandTag, closeErr = rows.resultReader.Close()\n\t\tif rows.err == nil {\n\t\t\trows.err = closeErr\n\t\t}\n\t}\n\n\tif rows.multiResultReader != nil {\n\t\tcloseErr := rows.multiResultReader.Close()\n\t\tif rows.err == nil {\n\t\t\trows.err = closeErr\n\t\t}\n\t}\n\n\tif rows.err != nil && rows.conn != nil && rows.sql != \"\" {\n\t\tif sc := rows.conn.statementCache; sc != nil {\n\t\t\tsc.Invalidate(rows.sql)\n\t\t}\n\n\t\tif sc := rows.conn.descriptionCache; sc != nil {\n\t\t\tsc.Invalidate(rows.sql)\n\t\t}\n\t}\n\n\tif rows.batchTracer != nil {\n\t\trows.batchTracer.TraceBatchQuery(rows.ctx, rows.conn, TraceBatchQueryData{SQL: rows.sql, Args: rows.args, CommandTag: rows.commandTag, Err: rows.err})\n\t} else if rows.queryTracer != nil {\n\t\trows.queryTracer.TraceQueryEnd(rows.ctx, rows.conn, TraceQueryEndData{rows.commandTag, rows.err})\n\t}\n}\n\nfunc (rows *baseRows) CommandTag() pgconn.CommandTag {\n\treturn rows.commandTag\n}\n\nfunc (rows *baseRows) Err() error {\n\treturn rows.err\n}\n\n// fatal signals an error occurred after the query was sent to the server. It\n// closes the rows automatically.\nfunc (rows *baseRows) fatal(err error) {\n\tif rows.err != nil {\n\t\treturn\n\t}\n\n\trows.err = err\n\trows.Close()\n}\n\nfunc (rows *baseRows) Next() bool {\n\tif rows.closed {\n\t\treturn false\n\t}\n\n\tif rows.resultReader.NextRow() {\n\t\trows.rowCount++\n\t\trows.values = rows.resultReader.Values()\n\t\treturn true\n\t} else {\n\t\trows.Close()\n\t\treturn false\n\t}\n}\n\nfunc (rows *baseRows) Scan(dest ...any) error {\n\tm := rows.typeMap\n\tfieldDescriptions := rows.FieldDescriptions()\n\tvalues := rows.values\n\n\tif len(fieldDescriptions) != len(values) {\n\t\terr := fmt.Errorf(\"number of field descriptions must equal number of values, got %d and %d\", len(fieldDescriptions), len(values))\n\t\trows.fatal(err)\n\t\treturn err\n\t}\n\n\tif len(dest) == 1 {\n\t\tif rc, ok := dest[0].(RowScanner); ok {\n\t\t\terr := rc.ScanRow(rows)\n\t\t\tif err != nil {\n\t\t\t\trows.fatal(err)\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif len(fieldDescriptions) != len(dest) {\n\t\terr := fmt.Errorf(\"number of field descriptions must equal number of destinations, got %d and %d\", len(fieldDescriptions), len(dest))\n\t\trows.fatal(err)\n\t\treturn err\n\t}\n\n\tif rows.scanPlans == nil {\n\t\trows.scanPlans = make([]pgtype.ScanPlan, len(values))\n\t\trows.scanTypes = make([]reflect.Type, len(values))\n\t\tfor i := range dest {\n\t\t\trows.scanPlans[i] = m.PlanScan(fieldDescriptions[i].DataTypeOID, fieldDescriptions[i].Format, dest[i])\n\t\t\trows.scanTypes[i] = reflect.TypeOf(dest[i])\n\t\t}\n\t}\n\n\tfor i, dst := range dest {\n\t\tif dst == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tif rows.scanTypes[i] != reflect.TypeOf(dst) {\n\t\t\trows.scanPlans[i] = m.PlanScan(fieldDescriptions[i].DataTypeOID, fieldDescriptions[i].Format, dest[i])\n\t\t\trows.scanTypes[i] = reflect.TypeOf(dest[i])\n\t\t}\n\n\t\terr := rows.scanPlans[i].Scan(values[i], dst)\n\t\tif err != nil {\n\t\t\terr = ScanArgError{ColumnIndex: i, Err: err}\n\t\t\trows.fatal(err)\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (rows *baseRows) Values() ([]any, error) {\n\tif rows.closed {\n\t\treturn nil, errors.New(\"rows is closed\")\n\t}\n\n\tvalues := make([]any, 0, len(rows.FieldDescriptions()))\n\n\tfor i := range rows.FieldDescriptions() {\n\t\tbuf := rows.values[i]\n\t\tfd := &rows.FieldDescriptions()[i]\n\n\t\tif buf == nil {\n\t\t\tvalues = append(values, nil)\n\t\t\tcontinue\n\t\t}\n\n\t\tif dt, ok := rows.typeMap.TypeForOID(fd.DataTypeOID); ok {\n\t\t\tvalue, err := dt.Codec.DecodeValue(rows.typeMap, fd.DataTypeOID, fd.Format, buf)\n\t\t\tif err != nil {\n\t\t\t\trows.fatal(err)\n\t\t\t}\n\t\t\tvalues = append(values, value)\n\t\t} else {\n\t\t\tswitch fd.Format {\n\t\t\tcase TextFormatCode:\n\t\t\t\tvalues = append(values, string(buf))\n\t\t\tcase BinaryFormatCode:\n\t\t\t\tnewBuf := make([]byte, len(buf))\n\t\t\t\tcopy(newBuf, buf)\n\t\t\t\tvalues = append(values, newBuf)\n\t\t\tdefault:\n\t\t\t\trows.fatal(errors.New(\"unknown format code\"))\n\t\t\t}\n\t\t}\n\n\t\tif rows.Err() != nil {\n\t\t\treturn nil, rows.Err()\n\t\t}\n\t}\n\n\treturn values, rows.Err()\n}\n\nfunc (rows *baseRows) RawValues() [][]byte {\n\treturn rows.values\n}\n\nfunc (rows *baseRows) Conn() *Conn {\n\treturn rows.conn\n}\n\ntype ScanArgError struct {\n\tColumnIndex int\n\tErr         error\n}\n\nfunc (e ScanArgError) Error() string {\n\treturn fmt.Sprintf(\"can't scan into dest[%d]: %v\", e.ColumnIndex, e.Err)\n}\n\nfunc (e ScanArgError) Unwrap() error {\n\treturn e.Err\n}\n\n// ScanRow decodes raw row data into dest. It can be used to scan rows read from the lower level pgconn interface.\n//\n// typeMap - OID to Go type mapping.\n// fieldDescriptions - OID and format of values\n// values - the raw data as returned from the PostgreSQL server\n// dest - the destination that values will be decoded into\nfunc ScanRow(typeMap *pgtype.Map, fieldDescriptions []pgconn.FieldDescription, values [][]byte, dest ...any) error {\n\tif len(fieldDescriptions) != len(values) {\n\t\treturn fmt.Errorf(\"number of field descriptions must equal number of values, got %d and %d\", len(fieldDescriptions), len(values))\n\t}\n\tif len(fieldDescriptions) != len(dest) {\n\t\treturn fmt.Errorf(\"number of field descriptions must equal number of destinations, got %d and %d\", len(fieldDescriptions), len(dest))\n\t}\n\n\tfor i, d := range dest {\n\t\tif d == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\terr := typeMap.Scan(fieldDescriptions[i].DataTypeOID, fieldDescriptions[i].Format, values[i], d)\n\t\tif err != nil {\n\t\t\treturn ScanArgError{ColumnIndex: i, Err: err}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// RowsFromResultReader returns a Rows that will read from values resultReader and decode with typeMap. It can be used\n// to read from the lower level pgconn interface.\nfunc RowsFromResultReader(typeMap *pgtype.Map, resultReader *pgconn.ResultReader) Rows {\n\treturn &baseRows{\n\t\ttypeMap:      typeMap,\n\t\tresultReader: resultReader,\n\t}\n}\n\n// ForEachRow iterates through rows. For each row it scans into the elements of scans and calls fn. If any row\n// fails to scan or fn returns an error the query will be aborted and the error will be returned. Rows will be closed\n// when ForEachRow returns.\nfunc ForEachRow(rows Rows, scans []any, fn func() error) (pgconn.CommandTag, error) {\n\tdefer rows.Close()\n\n\tfor rows.Next() {\n\t\terr := rows.Scan(scans...)\n\t\tif err != nil {\n\t\t\treturn pgconn.CommandTag{}, err\n\t\t}\n\n\t\terr = fn()\n\t\tif err != nil {\n\t\t\treturn pgconn.CommandTag{}, err\n\t\t}\n\t}\n\n\tif err := rows.Err(); err != nil {\n\t\treturn pgconn.CommandTag{}, err\n\t}\n\n\treturn rows.CommandTag(), nil\n}\n\n// CollectableRow is the subset of Rows methods that a RowToFunc is allowed to call.\ntype CollectableRow interface {\n\tFieldDescriptions() []pgconn.FieldDescription\n\tScan(dest ...any) error\n\tValues() ([]any, error)\n\tRawValues() [][]byte\n}\n\n// RowToFunc is a function that scans or otherwise converts row to a T.\ntype RowToFunc[T any] func(row CollectableRow) (T, error)\n\n// AppendRows iterates through rows, calling fn for each row, and appending the results into a slice of T.\n//\n// This function closes the rows automatically on return.\nfunc AppendRows[T any, S ~[]T](slice S, rows Rows, fn RowToFunc[T]) (S, error) {\n\tdefer rows.Close()\n\n\tfor rows.Next() {\n\t\tvalue, err := fn(rows)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tslice = append(slice, value)\n\t}\n\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn slice, nil\n}\n\n// CollectRows iterates through rows, calling fn for each row, and collecting the results into a slice of T.\n//\n// This function closes the rows automatically on return.\nfunc CollectRows[T any](rows Rows, fn RowToFunc[T]) ([]T, error) {\n\treturn AppendRows([]T{}, rows, fn)\n}\n\n// CollectOneRow calls fn for the first row in rows and returns the result. If no rows are found returns an error where errors.Is(ErrNoRows) is true.\n// CollectOneRow is to CollectRows as QueryRow is to Query.\n//\n// This function closes the rows automatically on return.\nfunc CollectOneRow[T any](rows Rows, fn RowToFunc[T]) (T, error) {\n\tdefer rows.Close()\n\n\tvar value T\n\tvar err error\n\n\tif !rows.Next() {\n\t\tif err = rows.Err(); err != nil {\n\t\t\treturn value, err\n\t\t}\n\t\treturn value, ErrNoRows\n\t}\n\n\tvalue, err = fn(rows)\n\tif err != nil {\n\t\treturn value, err\n\t}\n\n\trows.Close()\n\treturn value, rows.Err()\n}\n\n// CollectExactlyOneRow calls fn for the first row in rows and returns the result.\n//   - If no rows are found returns an error where errors.Is(ErrNoRows) is true.\n//   - If more than 1 row is found returns an error where errors.Is(ErrTooManyRows) is true.\n//\n// This function closes the rows automatically on return.\nfunc CollectExactlyOneRow[T any](rows Rows, fn RowToFunc[T]) (T, error) {\n\tdefer rows.Close()\n\n\tvar (\n\t\terr   error\n\t\tvalue T\n\t)\n\n\tif !rows.Next() {\n\t\tif err = rows.Err(); err != nil {\n\t\t\treturn value, err\n\t\t}\n\n\t\treturn value, ErrNoRows\n\t}\n\n\tvalue, err = fn(rows)\n\tif err != nil {\n\t\treturn value, err\n\t}\n\n\tif rows.Next() {\n\t\tvar zero T\n\n\t\treturn zero, ErrTooManyRows\n\t}\n\n\treturn value, rows.Err()\n}\n\n// RowTo returns a T scanned from row.\nfunc RowTo[T any](row CollectableRow) (T, error) {\n\tvar value T\n\terr := row.Scan(&value)\n\treturn value, err\n}\n\n// RowTo returns a the address of a T scanned from row.\nfunc RowToAddrOf[T any](row CollectableRow) (*T, error) {\n\tvar value T\n\terr := row.Scan(&value)\n\treturn &value, err\n}\n\n// RowToMap returns a map scanned from row.\nfunc RowToMap(row CollectableRow) (map[string]any, error) {\n\tvar value map[string]any\n\terr := row.Scan((*mapRowScanner)(&value))\n\treturn value, err\n}\n\ntype mapRowScanner map[string]any\n\nfunc (rs *mapRowScanner) ScanRow(rows Rows) error {\n\tvalues, err := rows.Values()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t*rs = make(mapRowScanner, len(values))\n\n\tfor i := range values {\n\t\t(*rs)[string(rows.FieldDescriptions()[i].Name)] = values[i]\n\t}\n\n\treturn nil\n}\n\n// RowToStructByPos returns a T scanned from row. T must be a struct. T must have the same number a public fields as row\n// has fields. The row and T fields will be matched by position. If the \"db\" struct tag is \"-\" then the field will be\n// ignored.\nfunc RowToStructByPos[T any](row CollectableRow) (T, error) {\n\tvar value T\n\terr := (&positionalStructRowScanner{ptrToStruct: &value}).ScanRow(row)\n\treturn value, err\n}\n\n// RowToAddrOfStructByPos returns the address of a T scanned from row. T must be a struct. T must have the same number a\n// public fields as row has fields. The row and T fields will be matched by position. If the \"db\" struct tag is \"-\" then\n// the field will be ignored.\nfunc RowToAddrOfStructByPos[T any](row CollectableRow) (*T, error) {\n\tvar value T\n\terr := (&positionalStructRowScanner{ptrToStruct: &value}).ScanRow(row)\n\treturn &value, err\n}\n\ntype positionalStructRowScanner struct {\n\tptrToStruct any\n}\n\nfunc (rs *positionalStructRowScanner) ScanRow(rows CollectableRow) error {\n\ttyp := reflect.TypeOf(rs.ptrToStruct).Elem()\n\tfields := lookupStructFields(typ)\n\tif len(rows.RawValues()) > len(fields) {\n\t\treturn fmt.Errorf(\n\t\t\t\"got %d values, but dst struct has only %d fields\",\n\t\t\tlen(rows.RawValues()),\n\t\t\tlen(fields),\n\t\t)\n\t}\n\tscanTargets := setupStructScanTargets(rs.ptrToStruct, fields)\n\treturn rows.Scan(scanTargets...)\n}\n\n// Map from reflect.Type -> []structRowField\nvar positionalStructFieldMap sync.Map\n\nfunc lookupStructFields(t reflect.Type) []structRowField {\n\tif cached, ok := positionalStructFieldMap.Load(t); ok {\n\t\treturn cached.([]structRowField)\n\t}\n\n\tfieldStack := make([]int, 0, 1)\n\tfields := computeStructFields(t, make([]structRowField, 0, t.NumField()), &fieldStack)\n\tfieldsIface, _ := positionalStructFieldMap.LoadOrStore(t, fields)\n\treturn fieldsIface.([]structRowField)\n}\n\nfunc computeStructFields(\n\tt reflect.Type,\n\tfields []structRowField,\n\tfieldStack *[]int,\n) []structRowField {\n\ttail := len(*fieldStack)\n\t*fieldStack = append(*fieldStack, 0)\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tsf := t.Field(i)\n\t\t(*fieldStack)[tail] = i\n\t\t// Handle anonymous struct embedding, but do not try to handle embedded pointers.\n\t\tif sf.Anonymous && sf.Type.Kind() == reflect.Struct {\n\t\t\tfields = computeStructFields(sf.Type, fields, fieldStack)\n\t\t} else if sf.PkgPath == \"\" {\n\t\t\tdbTag, _ := sf.Tag.Lookup(structTagKey)\n\t\t\tif dbTag == \"-\" {\n\t\t\t\t// Field is ignored, skip it.\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfields = append(fields, structRowField{\n\t\t\t\tpath: append([]int(nil), *fieldStack...),\n\t\t\t})\n\t\t}\n\t}\n\t*fieldStack = (*fieldStack)[:tail]\n\treturn fields\n}\n\n// RowToStructByName returns a T scanned from row. T must be a struct. T must have the same number of named public\n// fields as row has fields. The row and T fields will be matched by name. The match is case-insensitive. The database\n// column name can be overridden with a \"db\" struct tag. If the \"db\" struct tag is \"-\" then the field will be ignored.\nfunc RowToStructByName[T any](row CollectableRow) (T, error) {\n\tvar value T\n\terr := (&namedStructRowScanner{ptrToStruct: &value}).ScanRow(row)\n\treturn value, err\n}\n\n// RowToAddrOfStructByName returns the address of a T scanned from row. T must be a struct. T must have the same number\n// of named public fields as row has fields. The row and T fields will be matched by name. The match is\n// case-insensitive. The database column name can be overridden with a \"db\" struct tag. If the \"db\" struct tag is \"-\"\n// then the field will be ignored.\nfunc RowToAddrOfStructByName[T any](row CollectableRow) (*T, error) {\n\tvar value T\n\terr := (&namedStructRowScanner{ptrToStruct: &value}).ScanRow(row)\n\treturn &value, err\n}\n\n// RowToStructByNameLax returns a T scanned from row. T must be a struct. T must have greater than or equal number of named public\n// fields as row has fields. The row and T fields will be matched by name. The match is case-insensitive. The database\n// column name can be overridden with a \"db\" struct tag. If the \"db\" struct tag is \"-\" then the field will be ignored.\nfunc RowToStructByNameLax[T any](row CollectableRow) (T, error) {\n\tvar value T\n\terr := (&namedStructRowScanner{ptrToStruct: &value, lax: true}).ScanRow(row)\n\treturn value, err\n}\n\n// RowToAddrOfStructByNameLax returns the address of a T scanned from row. T must be a struct. T must have greater than or\n// equal number of named public fields as row has fields. The row and T fields will be matched by name. The match is\n// case-insensitive. The database column name can be overridden with a \"db\" struct tag. If the \"db\" struct tag is \"-\"\n// then the field will be ignored.\nfunc RowToAddrOfStructByNameLax[T any](row CollectableRow) (*T, error) {\n\tvar value T\n\terr := (&namedStructRowScanner{ptrToStruct: &value, lax: true}).ScanRow(row)\n\treturn &value, err\n}\n\ntype namedStructRowScanner struct {\n\tptrToStruct any\n\tlax         bool\n}\n\nfunc (rs *namedStructRowScanner) ScanRow(rows CollectableRow) error {\n\ttyp := reflect.TypeOf(rs.ptrToStruct).Elem()\n\tfldDescs := rows.FieldDescriptions()\n\tnamedStructFields, err := lookupNamedStructFields(typ, fldDescs)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !rs.lax && namedStructFields.missingField != \"\" {\n\t\treturn fmt.Errorf(\"cannot find field %s in returned row\", namedStructFields.missingField)\n\t}\n\tfields := namedStructFields.fields\n\tscanTargets := setupStructScanTargets(rs.ptrToStruct, fields)\n\treturn rows.Scan(scanTargets...)\n}\n\n// Map from namedStructFieldMap -> *namedStructFields\nvar namedStructFieldMap sync.Map\n\ntype namedStructFieldsKey struct {\n\tt        reflect.Type\n\tcolNames string\n}\n\ntype namedStructFields struct {\n\tfields []structRowField\n\t// missingField is the first field from the struct without a corresponding row field.\n\t// This is used to construct the correct error message for non-lax queries.\n\tmissingField string\n}\n\nfunc lookupNamedStructFields(\n\tt reflect.Type,\n\tfldDescs []pgconn.FieldDescription,\n) (*namedStructFields, error) {\n\tkey := namedStructFieldsKey{\n\t\tt:        t,\n\t\tcolNames: joinFieldNames(fldDescs),\n\t}\n\tif cached, ok := namedStructFieldMap.Load(key); ok {\n\t\treturn cached.(*namedStructFields), nil\n\t}\n\n\t// We could probably do two-levels of caching, where we compute the key -> fields mapping\n\t// for a type only once, cache it by type, then use that to compute the column -> fields\n\t// mapping for a given set of columns.\n\tfieldStack := make([]int, 0, 1)\n\tfields, missingField := computeNamedStructFields(\n\t\tfldDescs,\n\t\tt,\n\t\tmake([]structRowField, len(fldDescs)),\n\t\t&fieldStack,\n\t)\n\tfor i, f := range fields {\n\t\tif f.path == nil {\n\t\t\treturn nil, fmt.Errorf(\n\t\t\t\t\"struct doesn't have corresponding row field %s\",\n\t\t\t\tfldDescs[i].Name,\n\t\t\t)\n\t\t}\n\t}\n\n\tfieldsIface, _ := namedStructFieldMap.LoadOrStore(\n\t\tkey,\n\t\t&namedStructFields{fields: fields, missingField: missingField},\n\t)\n\treturn fieldsIface.(*namedStructFields), nil\n}\n\nfunc joinFieldNames(fldDescs []pgconn.FieldDescription) string {\n\tswitch len(fldDescs) {\n\tcase 0:\n\t\treturn \"\"\n\tcase 1:\n\t\treturn fldDescs[0].Name\n\t}\n\n\ttotalSize := len(fldDescs) - 1 // Space for separator bytes.\n\tfor _, d := range fldDescs {\n\t\ttotalSize += len(d.Name)\n\t}\n\tvar b strings.Builder\n\tb.Grow(totalSize)\n\tb.WriteString(fldDescs[0].Name)\n\tfor _, d := range fldDescs[1:] {\n\t\tb.WriteByte(0) // Join with NUL byte as it's (presumably) not a valid column character.\n\t\tb.WriteString(d.Name)\n\t}\n\treturn b.String()\n}\n\nfunc computeNamedStructFields(\n\tfldDescs []pgconn.FieldDescription,\n\tt reflect.Type,\n\tfields []structRowField,\n\tfieldStack *[]int,\n) ([]structRowField, string) {\n\tvar missingField string\n\ttail := len(*fieldStack)\n\t*fieldStack = append(*fieldStack, 0)\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tsf := t.Field(i)\n\t\t(*fieldStack)[tail] = i\n\t\tif sf.PkgPath != \"\" && !sf.Anonymous {\n\t\t\t// Field is unexported, skip it.\n\t\t\tcontinue\n\t\t}\n\t\t// Handle anonymous struct embedding, but do not try to handle embedded pointers.\n\t\tif sf.Anonymous && sf.Type.Kind() == reflect.Struct {\n\t\t\tvar missingSubField string\n\t\t\tfields, missingSubField = computeNamedStructFields(\n\t\t\t\tfldDescs,\n\t\t\t\tsf.Type,\n\t\t\t\tfields,\n\t\t\t\tfieldStack,\n\t\t\t)\n\t\t\tif missingField == \"\" {\n\t\t\t\tmissingField = missingSubField\n\t\t\t}\n\t\t} else {\n\t\t\tdbTag, dbTagPresent := sf.Tag.Lookup(structTagKey)\n\t\t\tif dbTagPresent {\n\t\t\t\tdbTag, _, _ = strings.Cut(dbTag, \",\")\n\t\t\t}\n\t\t\tif dbTag == \"-\" {\n\t\t\t\t// Field is ignored, skip it.\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcolName := dbTag\n\t\t\tif !dbTagPresent {\n\t\t\t\tcolName = sf.Name\n\t\t\t}\n\t\t\tfpos := fieldPosByName(fldDescs, colName, !dbTagPresent)\n\t\t\tif fpos == -1 {\n\t\t\t\tif missingField == \"\" {\n\t\t\t\t\tmissingField = colName\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfields[fpos] = structRowField{\n\t\t\t\tpath: append([]int(nil), *fieldStack...),\n\t\t\t}\n\t\t}\n\t}\n\t*fieldStack = (*fieldStack)[:tail]\n\n\treturn fields, missingField\n}\n\nconst structTagKey = \"db\"\n\nfunc fieldPosByName(fldDescs []pgconn.FieldDescription, field string, normalize bool) (i int) {\n\ti = -1\n\n\tif normalize {\n\t\tfield = strings.ReplaceAll(field, \"_\", \"\")\n\t}\n\tfor i, desc := range fldDescs {\n\t\tif normalize {\n\t\t\tif strings.EqualFold(strings.ReplaceAll(desc.Name, \"_\", \"\"), field) {\n\t\t\t\treturn i\n\t\t\t}\n\t\t} else {\n\t\t\tif desc.Name == field {\n\t\t\t\treturn i\n\t\t\t}\n\t\t}\n\t}\n\treturn\n}\n\n// structRowField describes a field of a struct.\n//\n// TODO: It would be a bit more efficient to track the path using the pointer\n// offset within the (outermost) struct and use unsafe.Pointer arithmetic to\n// construct references when scanning rows. However, it's not clear it's worth\n// using unsafe for this.\ntype structRowField struct {\n\tpath []int\n}\n\nfunc setupStructScanTargets(receiver any, fields []structRowField) []any {\n\tscanTargets := make([]any, len(fields))\n\tv := reflect.ValueOf(receiver).Elem()\n\tfor i, f := range fields {\n\t\tscanTargets[i] = v.FieldByIndex(f.path).Addr().Interface()\n\t}\n\treturn scanTargets\n}\n"
        },
        {
          "name": "rows_test.go",
          "type": "blob",
          "size": 29.4130859375,
          "content": "package pgx_test\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgxtest\"\n)\n\ntype testRowScanner struct {\n\tname string\n\tage  int32\n}\n\nfunc (rs *testRowScanner) ScanRow(rows pgx.Rows) error {\n\treturn rows.Scan(&rs.name, &rs.age)\n}\n\nfunc TestRowScanner(t *testing.T) {\n\tt.Parallel()\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tvar s testRowScanner\n\t\terr := conn.QueryRow(ctx, \"select 'Adam' as name, 72 as height\").Scan(&s)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, \"Adam\", s.name)\n\t\trequire.Equal(t, int32(72), s.age)\n\t})\n}\n\ntype testErrRowScanner string\n\nfunc (ers *testErrRowScanner) ScanRow(rows pgx.Rows) error {\n\treturn errors.New(string(*ers))\n}\n\n// https://github.com/jackc/pgx/issues/1654\nfunc TestRowScannerErrorIsFatalToRows(t *testing.T) {\n\tt.Parallel()\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ts := testErrRowScanner(\"foo\")\n\t\terr := conn.QueryRow(ctx, \"select 'Adam' as name, 72 as height\").Scan(&s)\n\t\trequire.EqualError(t, err, \"foo\")\n\t})\n}\n\nfunc TestForEachRow(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tvar actualResults []any\n\n\t\trows, _ := conn.Query(\n\t\t\tcontext.Background(),\n\t\t\t\"select n, n * 2 from generate_series(1, $1) n\",\n\t\t\t3,\n\t\t)\n\t\tvar a, b int\n\t\tct, err := pgx.ForEachRow(rows, []any{&a, &b}, func() error {\n\t\t\tactualResults = append(actualResults, []any{a, b})\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\texpectedResults := []any{\n\t\t\t[]any{1, 2},\n\t\t\t[]any{2, 4},\n\t\t\t[]any{3, 6},\n\t\t}\n\t\trequire.Equal(t, expectedResults, actualResults)\n\t\trequire.EqualValues(t, 3, ct.RowsAffected())\n\t})\n}\n\nfunc TestForEachRowScanError(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tvar actualResults []any\n\n\t\trows, _ := conn.Query(\n\t\t\tcontext.Background(),\n\t\t\t\"select 'foo', 'bar' from generate_series(1, $1) n\",\n\t\t\t3,\n\t\t)\n\t\tvar a, b int\n\t\tct, err := pgx.ForEachRow(rows, []any{&a, &b}, func() error {\n\t\t\tactualResults = append(actualResults, []any{a, b})\n\t\t\treturn nil\n\t\t})\n\t\trequire.EqualError(t, err, \"can't scan into dest[0]: cannot scan text (OID 25) in text format into *int\")\n\t\trequire.Equal(t, pgconn.CommandTag{}, ct)\n\t})\n}\n\nfunc TestForEachRowAbort(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(\n\t\t\tcontext.Background(),\n\t\t\t\"select n, n * 2 from generate_series(1, $1) n\",\n\t\t\t3,\n\t\t)\n\t\tvar a, b int\n\t\tct, err := pgx.ForEachRow(rows, []any{&a, &b}, func() error {\n\t\t\treturn errors.New(\"abort\")\n\t\t})\n\t\trequire.EqualError(t, err, \"abort\")\n\t\trequire.Equal(t, pgconn.CommandTag{}, ct)\n\t})\n}\n\nfunc ExampleForEachRow() {\n\tconn, err := pgx.Connect(context.Background(), os.Getenv(\"PGX_TEST_DATABASE\"))\n\tif err != nil {\n\t\tfmt.Printf(\"Unable to establish connection: %v\", err)\n\t\treturn\n\t}\n\n\trows, _ := conn.Query(\n\t\tcontext.Background(),\n\t\t\"select n, n * 2 from generate_series(1, $1) n\",\n\t\t3,\n\t)\n\tvar a, b int\n\t_, err = pgx.ForEachRow(rows, []any{&a, &b}, func() error {\n\t\tfmt.Printf(\"%v, %v\\n\", a, b)\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\tfmt.Printf(\"ForEachRow error: %v\", err)\n\t\treturn\n\t}\n\n\t// Output:\n\t// 1, 2\n\t// 2, 4\n\t// 3, 6\n}\n\nfunc TestCollectRows(t *testing.T) {\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select n from generate_series(0, 99) n`)\n\t\tnumbers, err := pgx.CollectRows(rows, func(row pgx.CollectableRow) (int32, error) {\n\t\t\tvar n int32\n\t\t\terr := row.Scan(&n)\n\t\t\treturn n, err\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tassert.Len(t, numbers, 100)\n\t\tfor i := range numbers {\n\t\t\tassert.Equal(t, int32(i), numbers[i])\n\t\t}\n\t})\n}\n\nfunc TestCollectRowsEmpty(t *testing.T) {\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select n from generate_series(1, 0) n`)\n\t\tnumbers, err := pgx.CollectRows(rows, func(row pgx.CollectableRow) (int32, error) {\n\t\t\tvar n int32\n\t\t\terr := row.Scan(&n)\n\t\t\treturn n, err\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.NotNil(t, numbers)\n\n\t\tassert.Empty(t, numbers)\n\t})\n}\n\n// This example uses CollectRows with a manually written collector function. In most cases RowTo, RowToAddrOf,\n// RowToStructByPos, RowToAddrOfStructByPos, or another generic function would be used.\nfunc ExampleCollectRows() {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn, err := pgx.Connect(ctx, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tif err != nil {\n\t\tfmt.Printf(\"Unable to establish connection: %v\", err)\n\t\treturn\n\t}\n\n\trows, _ := conn.Query(ctx, `select n from generate_series(1, 5) n`)\n\tnumbers, err := pgx.CollectRows(rows, func(row pgx.CollectableRow) (int32, error) {\n\t\tvar n int32\n\t\terr := row.Scan(&n)\n\t\treturn n, err\n\t})\n\tif err != nil {\n\t\tfmt.Printf(\"CollectRows error: %v\", err)\n\t\treturn\n\t}\n\n\tfmt.Println(numbers)\n\n\t// Output:\n\t// [1 2 3 4 5]\n}\n\nfunc TestCollectOneRow(t *testing.T) {\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 42`)\n\t\tn, err := pgx.CollectOneRow(rows, func(row pgx.CollectableRow) (int32, error) {\n\t\t\tvar n int32\n\t\t\terr := row.Scan(&n)\n\t\t\treturn n, err\n\t\t})\n\t\tassert.NoError(t, err)\n\t\tassert.Equal(t, int32(42), n)\n\t})\n}\n\nfunc TestCollectOneRowNotFound(t *testing.T) {\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 42 where false`)\n\t\tn, err := pgx.CollectOneRow(rows, func(row pgx.CollectableRow) (int32, error) {\n\t\t\tvar n int32\n\t\t\terr := row.Scan(&n)\n\t\t\treturn n, err\n\t\t})\n\t\tassert.ErrorIs(t, err, pgx.ErrNoRows)\n\t\tassert.Equal(t, int32(0), n)\n\t})\n}\n\nfunc TestCollectOneRowIgnoresExtraRows(t *testing.T) {\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select n from generate_series(42, 99) n`)\n\t\tn, err := pgx.CollectOneRow(rows, func(row pgx.CollectableRow) (int32, error) {\n\t\t\tvar n int32\n\t\t\terr := row.Scan(&n)\n\t\t\treturn n, err\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tassert.NoError(t, err)\n\t\tassert.Equal(t, int32(42), n)\n\t})\n}\n\n// https://github.com/jackc/pgx/issues/1334\nfunc TestCollectOneRowPrefersPostgreSQLErrorOverErrNoRows(t *testing.T) {\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\t_, err := conn.Exec(ctx, `create temporary table t (name text not null unique)`)\n\t\trequire.NoError(t, err)\n\n\t\tvar name string\n\t\trows, _ := conn.Query(ctx, `insert into t (name) values ('foo') returning name`)\n\t\tname, err = pgx.CollectOneRow(rows, func(row pgx.CollectableRow) (string, error) {\n\t\t\tvar n string\n\t\t\terr := row.Scan(&n)\n\t\t\treturn n, err\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, \"foo\", name)\n\n\t\trows, _ = conn.Query(ctx, `insert into t (name) values ('foo') returning name`)\n\t\tname, err = pgx.CollectOneRow(rows, func(row pgx.CollectableRow) (string, error) {\n\t\t\tvar n string\n\t\t\terr := row.Scan(&n)\n\t\t\treturn n, err\n\t\t})\n\t\trequire.Error(t, err)\n\t\tvar pgErr *pgconn.PgError\n\t\trequire.ErrorAs(t, err, &pgErr)\n\t\trequire.Equal(t, \"23505\", pgErr.Code)\n\t\trequire.Equal(t, \"\", name)\n\t})\n}\n\nfunc TestCollectExactlyOneRow(t *testing.T) {\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 42`)\n\t\tn, err := pgx.CollectExactlyOneRow(rows, func(row pgx.CollectableRow) (int32, error) {\n\t\t\tvar n int32\n\t\t\terr := row.Scan(&n)\n\t\t\treturn n, err\n\t\t})\n\t\tassert.NoError(t, err)\n\t\tassert.Equal(t, int32(42), n)\n\t})\n}\n\nfunc TestCollectExactlyOneRowNotFound(t *testing.T) {\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 42 where false`)\n\t\tn, err := pgx.CollectExactlyOneRow(rows, func(row pgx.CollectableRow) (int32, error) {\n\t\t\tvar n int32\n\t\t\terr := row.Scan(&n)\n\t\t\treturn n, err\n\t\t})\n\t\tassert.ErrorIs(t, err, pgx.ErrNoRows)\n\t\tassert.Equal(t, int32(0), n)\n\t})\n}\n\nfunc TestCollectExactlyOneRowExtraRows(t *testing.T) {\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select n from generate_series(42, 99) n`)\n\t\tn, err := pgx.CollectExactlyOneRow(rows, func(row pgx.CollectableRow) (int32, error) {\n\t\t\tvar n int32\n\t\t\terr := row.Scan(&n)\n\t\t\treturn n, err\n\t\t})\n\t\tassert.ErrorIs(t, err, pgx.ErrTooManyRows)\n\t\tassert.Equal(t, int32(0), n)\n\t})\n}\n\nfunc TestRowTo(t *testing.T) {\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select n from generate_series(0, 99) n`)\n\t\tnumbers, err := pgx.CollectRows(rows, pgx.RowTo[int32])\n\t\trequire.NoError(t, err)\n\n\t\tassert.Len(t, numbers, 100)\n\t\tfor i := range numbers {\n\t\t\tassert.Equal(t, int32(i), numbers[i])\n\t\t}\n\t})\n}\n\nfunc ExampleRowTo() {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn, err := pgx.Connect(ctx, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tif err != nil {\n\t\tfmt.Printf(\"Unable to establish connection: %v\", err)\n\t\treturn\n\t}\n\n\trows, _ := conn.Query(ctx, `select n from generate_series(1, 5) n`)\n\tnumbers, err := pgx.CollectRows(rows, pgx.RowTo[int32])\n\tif err != nil {\n\t\tfmt.Printf(\"CollectRows error: %v\", err)\n\t\treturn\n\t}\n\n\tfmt.Println(numbers)\n\n\t// Output:\n\t// [1 2 3 4 5]\n}\n\nfunc TestRowToAddrOf(t *testing.T) {\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select n from generate_series(0, 99) n`)\n\t\tnumbers, err := pgx.CollectRows(rows, pgx.RowToAddrOf[int32])\n\t\trequire.NoError(t, err)\n\n\t\tassert.Len(t, numbers, 100)\n\t\tfor i := range numbers {\n\t\t\tassert.Equal(t, int32(i), *numbers[i])\n\t\t}\n\t})\n}\n\nfunc ExampleRowToAddrOf() {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn, err := pgx.Connect(ctx, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tif err != nil {\n\t\tfmt.Printf(\"Unable to establish connection: %v\", err)\n\t\treturn\n\t}\n\n\trows, _ := conn.Query(ctx, `select n from generate_series(1, 5) n`)\n\tpNumbers, err := pgx.CollectRows(rows, pgx.RowToAddrOf[int32])\n\tif err != nil {\n\t\tfmt.Printf(\"CollectRows error: %v\", err)\n\t\treturn\n\t}\n\n\tfor _, p := range pNumbers {\n\t\tfmt.Println(*p)\n\t}\n\n\t// Output:\n\t// 1\n\t// 2\n\t// 3\n\t// 4\n\t// 5\n}\n\nfunc TestRowToMap(t *testing.T) {\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 'Joe' as name, n as age from generate_series(0, 9) n`)\n\t\tslice, err := pgx.CollectRows(rows, pgx.RowToMap)\n\t\trequire.NoError(t, err)\n\n\t\tassert.Len(t, slice, 10)\n\t\tfor i := range slice {\n\t\t\tassert.Equal(t, \"Joe\", slice[i][\"name\"])\n\t\t\tassert.EqualValues(t, i, slice[i][\"age\"])\n\t\t}\n\t})\n}\n\nfunc TestRowToStructByPos(t *testing.T) {\n\ttype person struct {\n\t\tName string\n\t\tAge  int32\n\t}\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 'Joe' as name, n as age from generate_series(0, 9) n`)\n\t\tslice, err := pgx.CollectRows(rows, pgx.RowToStructByPos[person])\n\t\trequire.NoError(t, err)\n\n\t\tassert.Len(t, slice, 10)\n\t\tfor i := range slice {\n\t\t\tassert.Equal(t, \"Joe\", slice[i].Name)\n\t\t\tassert.EqualValues(t, i, slice[i].Age)\n\t\t}\n\t})\n}\n\nfunc TestRowToStructByPosIgnoredField(t *testing.T) {\n\ttype person struct {\n\t\tName string\n\t\tAge  int32 `db:\"-\"`\n\t}\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 'Joe' as name from generate_series(0, 9) n`)\n\t\tslice, err := pgx.CollectRows(rows, pgx.RowToStructByPos[person])\n\t\trequire.NoError(t, err)\n\n\t\tassert.Len(t, slice, 10)\n\t\tfor i := range slice {\n\t\t\tassert.Equal(t, \"Joe\", slice[i].Name)\n\t\t}\n\t})\n}\n\nfunc TestRowToStructByPosEmbeddedStruct(t *testing.T) {\n\ttype Name struct {\n\t\tFirst string\n\t\tLast  string\n\t}\n\n\ttype person struct {\n\t\tName\n\t\tAge int32\n\t}\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 'John' as first_name, 'Smith' as last_name, n as age from generate_series(0, 9) n`)\n\t\tslice, err := pgx.CollectRows(rows, pgx.RowToStructByPos[person])\n\t\trequire.NoError(t, err)\n\n\t\tassert.Len(t, slice, 10)\n\t\tfor i := range slice {\n\t\t\tassert.Equal(t, \"John\", slice[i].Name.First)\n\t\t\tassert.Equal(t, \"Smith\", slice[i].Name.Last)\n\t\t\tassert.EqualValues(t, i, slice[i].Age)\n\t\t}\n\t})\n}\n\nfunc TestRowToStructByPosMultipleEmbeddedStruct(t *testing.T) {\n\ttype Sandwich struct {\n\t\tBread string\n\t\tSalad string\n\t}\n\ttype Drink struct {\n\t\tMl int\n\t}\n\n\ttype meal struct {\n\t\tSandwich\n\t\tDrink\n\t}\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 'Baguette' as bread, 'Lettuce' as salad, drink_ml from generate_series(0, 9) drink_ml`)\n\t\tslice, err := pgx.CollectRows(rows, pgx.RowToStructByPos[meal])\n\t\trequire.NoError(t, err)\n\n\t\tassert.Len(t, slice, 10)\n\t\tfor i := range slice {\n\t\t\tassert.Equal(t, \"Baguette\", slice[i].Sandwich.Bread)\n\t\t\tassert.Equal(t, \"Lettuce\", slice[i].Sandwich.Salad)\n\t\t\tassert.EqualValues(t, i, slice[i].Drink.Ml)\n\t\t}\n\t})\n}\n\nfunc TestRowToStructByPosEmbeddedUnexportedStruct(t *testing.T) {\n\ttype name struct {\n\t\tFirst string\n\t\tLast  string\n\t}\n\n\ttype person struct {\n\t\tname\n\t\tAge int32\n\t}\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 'John' as first_name, 'Smith' as last_name, n as age from generate_series(0, 9) n`)\n\t\tslice, err := pgx.CollectRows(rows, pgx.RowToStructByPos[person])\n\t\trequire.NoError(t, err)\n\n\t\tassert.Len(t, slice, 10)\n\t\tfor i := range slice {\n\t\t\tassert.Equal(t, \"John\", slice[i].name.First)\n\t\t\tassert.Equal(t, \"Smith\", slice[i].name.Last)\n\t\t\tassert.EqualValues(t, i, slice[i].Age)\n\t\t}\n\t})\n}\n\n// Pointer to struct is not supported. But check that we don't panic.\nfunc TestRowToStructByPosEmbeddedPointerToStruct(t *testing.T) {\n\ttype Name struct {\n\t\tFirst string\n\t\tLast  string\n\t}\n\n\ttype person struct {\n\t\t*Name\n\t\tAge int32\n\t}\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 'John' as first_name, 'Smith' as last_name, n as age from generate_series(0, 9) n`)\n\t\t_, err := pgx.CollectRows(rows, pgx.RowToStructByPos[person])\n\t\trequire.EqualError(t, err, \"got 3 values, but dst struct has only 2 fields\")\n\t})\n}\n\nfunc ExampleRowToStructByPos() {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn, err := pgx.Connect(ctx, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tif err != nil {\n\t\tfmt.Printf(\"Unable to establish connection: %v\", err)\n\t\treturn\n\t}\n\n\tif conn.PgConn().ParameterStatus(\"crdb_version\") != \"\" {\n\t\t// Skip test / example when running on CockroachDB. Since an example can't be skipped fake success instead.\n\t\tfmt.Println(`Cheeseburger: $10\nFries: $5\nSoft Drink: $3`)\n\t\treturn\n\t}\n\n\t// Setup example schema and data.\n\t_, err = conn.Exec(ctx, `\ncreate temporary table products (\n\tid int primary key generated by default as identity,\n\tname varchar(100) not null,\n\tprice int not null\n);\n\ninsert into products (name, price) values\n\t('Cheeseburger', 10),\n\t('Double Cheeseburger', 14),\n\t('Fries', 5),\n\t('Soft Drink', 3);\n`)\n\tif err != nil {\n\t\tfmt.Printf(\"Unable to setup example schema and data: %v\", err)\n\t\treturn\n\t}\n\n\ttype product struct {\n\t\tID    int32\n\t\tName  string\n\t\tPrice int32\n\t}\n\n\trows, _ := conn.Query(ctx, \"select * from products where price < $1 order by price desc\", 12)\n\tproducts, err := pgx.CollectRows(rows, pgx.RowToStructByPos[product])\n\tif err != nil {\n\t\tfmt.Printf(\"CollectRows error: %v\", err)\n\t\treturn\n\t}\n\n\tfor _, p := range products {\n\t\tfmt.Printf(\"%s: $%d\\n\", p.Name, p.Price)\n\t}\n\n\t// Output:\n\t// Cheeseburger: $10\n\t// Fries: $5\n\t// Soft Drink: $3\n}\n\nfunc TestRowToAddrOfStructPos(t *testing.T) {\n\ttype person struct {\n\t\tName string\n\t\tAge  int32\n\t}\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 'Joe' as name, n as age from generate_series(0, 9) n`)\n\t\tslice, err := pgx.CollectRows(rows, pgx.RowToAddrOfStructByPos[person])\n\t\trequire.NoError(t, err)\n\n\t\tassert.Len(t, slice, 10)\n\t\tfor i := range slice {\n\t\t\tassert.Equal(t, \"Joe\", slice[i].Name)\n\t\t\tassert.EqualValues(t, i, slice[i].Age)\n\t\t}\n\t})\n}\n\nfunc TestRowToStructByName(t *testing.T) {\n\ttype person struct {\n\t\tLast      string\n\t\tFirst     string\n\t\tAge       int32\n\t\tAccountID string\n\t}\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 'John' as first, 'Smith' as last, n as age, 'd5e49d3f' as account_id from generate_series(0, 9) n`)\n\t\tslice, err := pgx.CollectRows(rows, pgx.RowToStructByName[person])\n\t\tassert.NoError(t, err)\n\n\t\tassert.Len(t, slice, 10)\n\t\tfor i := range slice {\n\t\t\tassert.Equal(t, \"Smith\", slice[i].Last)\n\t\t\tassert.Equal(t, \"John\", slice[i].First)\n\t\t\tassert.EqualValues(t, i, slice[i].Age)\n\t\t\tassert.Equal(t, \"d5e49d3f\", slice[i].AccountID)\n\t\t}\n\n\t\t// check missing fields in a returned row\n\t\trows, _ = conn.Query(ctx, `select 'Smith' as last, n as age from generate_series(0, 9) n`)\n\t\t_, err = pgx.CollectRows(rows, pgx.RowToStructByName[person])\n\t\tassert.ErrorContains(t, err, \"cannot find field First in returned row\")\n\n\t\t// check missing field in a destination struct\n\t\trows, _ = conn.Query(ctx, `select 'John' as first, 'Smith' as last, n as age, 'd5e49d3f' as account_id, null as ignore from generate_series(0, 9) n`)\n\t\t_, err = pgx.CollectRows(rows, pgx.RowToAddrOfStructByName[person])\n\t\tassert.ErrorContains(t, err, \"struct doesn't have corresponding row field ignore\")\n\t})\n}\n\nfunc TestRowToStructByNameDbTags(t *testing.T) {\n\ttype person struct {\n\t\tLast             string `db:\"last_name\"`\n\t\tFirst            string `db:\"first_name\"`\n\t\tAge              int32  `db:\"age\"`\n\t\tAccountID        string `db:\"account_id\"`\n\t\tAnotherAccountID string `db:\"account__id\"`\n\t}\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 'John' as first_name, 'Smith' as last_name, n as age, 'd5e49d3f' as account_id, '5e49d321' as account__id from generate_series(0, 9) n`)\n\t\tslice, err := pgx.CollectRows(rows, pgx.RowToStructByName[person])\n\t\tassert.NoError(t, err)\n\n\t\tassert.Len(t, slice, 10)\n\t\tfor i := range slice {\n\t\t\tassert.Equal(t, \"Smith\", slice[i].Last)\n\t\t\tassert.Equal(t, \"John\", slice[i].First)\n\t\t\tassert.EqualValues(t, i, slice[i].Age)\n\t\t\tassert.Equal(t, \"d5e49d3f\", slice[i].AccountID)\n\t\t\tassert.Equal(t, \"5e49d321\", slice[i].AnotherAccountID)\n\t\t}\n\n\t\t// check missing fields in a returned row\n\t\trows, _ = conn.Query(ctx, `select 'Smith' as last_name, n as age from generate_series(0, 9) n`)\n\t\t_, err = pgx.CollectRows(rows, pgx.RowToStructByName[person])\n\t\tassert.ErrorContains(t, err, \"cannot find field first_name in returned row\")\n\n\t\t// check missing field in a destination struct\n\t\trows, _ = conn.Query(ctx, `select 'John' as first_name, 'Smith' as last_name, n as age, 'd5e49d3f' as account_id, '5e49d321' as account__id, null as ignore from generate_series(0, 9) n`)\n\t\t_, err = pgx.CollectRows(rows, pgx.RowToAddrOfStructByName[person])\n\t\tassert.ErrorContains(t, err, \"struct doesn't have corresponding row field ignore\")\n\t})\n}\n\nfunc TestRowToStructByNameEmbeddedStruct(t *testing.T) {\n\ttype Name struct {\n\t\tLast  string `db:\"last_name\"`\n\t\tFirst string `db:\"first_name\"`\n\t}\n\n\ttype person struct {\n\t\tIgnore bool `db:\"-\"`\n\t\tName\n\t\tAge int32\n\t}\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 'John' as first_name, 'Smith' as last_name, n as age from generate_series(0, 9) n`)\n\t\tslice, err := pgx.CollectRows(rows, pgx.RowToStructByName[person])\n\t\tassert.NoError(t, err)\n\n\t\tassert.Len(t, slice, 10)\n\t\tfor i := range slice {\n\t\t\tassert.Equal(t, \"Smith\", slice[i].Name.Last)\n\t\t\tassert.Equal(t, \"John\", slice[i].Name.First)\n\t\t\tassert.EqualValues(t, i, slice[i].Age)\n\t\t}\n\n\t\t// check missing fields in a returned row\n\t\trows, _ = conn.Query(ctx, `select 'Smith' as last_name, n as age from generate_series(0, 9) n`)\n\t\t_, err = pgx.CollectRows(rows, pgx.RowToStructByName[person])\n\t\tassert.ErrorContains(t, err, \"cannot find field first_name in returned row\")\n\n\t\t// check missing field in a destination struct\n\t\trows, _ = conn.Query(ctx, `select 'John' as first_name, 'Smith' as last_name, n as age, null as ignore from generate_series(0, 9) n`)\n\t\t_, err = pgx.CollectRows(rows, pgx.RowToAddrOfStructByName[person])\n\t\tassert.ErrorContains(t, err, \"struct doesn't have corresponding row field ignore\")\n\t})\n}\n\nfunc ExampleRowToStructByName() {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn, err := pgx.Connect(ctx, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tif err != nil {\n\t\tfmt.Printf(\"Unable to establish connection: %v\", err)\n\t\treturn\n\t}\n\n\tif conn.PgConn().ParameterStatus(\"crdb_version\") != \"\" {\n\t\t// Skip test / example when running on CockroachDB. Since an example can't be skipped fake success instead.\n\t\tfmt.Println(`Cheeseburger: $10\nFries: $5\nSoft Drink: $3`)\n\t\treturn\n\t}\n\n\t// Setup example schema and data.\n\t_, err = conn.Exec(ctx, `\ncreate temporary table products (\n\tid int primary key generated by default as identity,\n\tname varchar(100) not null,\n\tprice int not null\n);\n\ninsert into products (name, price) values\n\t('Cheeseburger', 10),\n\t('Double Cheeseburger', 14),\n\t('Fries', 5),\n\t('Soft Drink', 3);\n`)\n\tif err != nil {\n\t\tfmt.Printf(\"Unable to setup example schema and data: %v\", err)\n\t\treturn\n\t}\n\n\ttype product struct {\n\t\tID    int32\n\t\tName  string\n\t\tPrice int32\n\t}\n\n\trows, _ := conn.Query(ctx, \"select * from products where price < $1 order by price desc\", 12)\n\tproducts, err := pgx.CollectRows(rows, pgx.RowToStructByName[product])\n\tif err != nil {\n\t\tfmt.Printf(\"CollectRows error: %v\", err)\n\t\treturn\n\t}\n\n\tfor _, p := range products {\n\t\tfmt.Printf(\"%s: $%d\\n\", p.Name, p.Price)\n\t}\n\n\t// Output:\n\t// Cheeseburger: $10\n\t// Fries: $5\n\t// Soft Drink: $3\n}\n\nfunc TestRowToStructByNameLax(t *testing.T) {\n\ttype person struct {\n\t\tLast   string\n\t\tFirst  string\n\t\tAge    int32\n\t\tIgnore bool `db:\"-\"`\n\t}\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 'John' as first, 'Smith' as last, n as age from generate_series(0, 9) n`)\n\t\tslice, err := pgx.CollectRows(rows, pgx.RowToStructByNameLax[person])\n\t\tassert.NoError(t, err)\n\n\t\tassert.Len(t, slice, 10)\n\t\tfor i := range slice {\n\t\t\tassert.Equal(t, \"Smith\", slice[i].Last)\n\t\t\tassert.Equal(t, \"John\", slice[i].First)\n\t\t\tassert.EqualValues(t, i, slice[i].Age)\n\t\t}\n\n\t\t// check missing fields in a returned row\n\t\trows, _ = conn.Query(ctx, `select 'John' as first, n as age from generate_series(0, 9) n`)\n\t\tslice, err = pgx.CollectRows(rows, pgx.RowToStructByNameLax[person])\n\t\tassert.NoError(t, err)\n\n\t\tassert.Len(t, slice, 10)\n\t\tfor i := range slice {\n\t\t\tassert.Equal(t, \"John\", slice[i].First)\n\t\t\tassert.EqualValues(t, i, slice[i].Age)\n\t\t}\n\n\t\t// check extra fields in a returned row\n\t\trows, _ = conn.Query(ctx, `select 'John' as first, 'Smith' as last, n as age, null as ignore from generate_series(0, 9) n`)\n\t\t_, err = pgx.CollectRows(rows, pgx.RowToAddrOfStructByNameLax[person])\n\t\tassert.ErrorContains(t, err, \"struct doesn't have corresponding row field ignore\")\n\n\t\t// check missing fields in a destination struct\n\t\trows, _ = conn.Query(ctx, `select 'Smith' as last, 'D.' as middle, n as age from generate_series(0, 9) n`)\n\t\t_, err = pgx.CollectRows(rows, pgx.RowToAddrOfStructByNameLax[person])\n\t\tassert.ErrorContains(t, err, \"struct doesn't have corresponding row field middle\")\n\n\t\t// check ignored fields in a destination struct\n\t\trows, _ = conn.Query(ctx, `select 'Smith' as last, n as age, null as ignore from generate_series(0, 9) n`)\n\t\t_, err = pgx.CollectRows(rows, pgx.RowToAddrOfStructByNameLax[person])\n\t\tassert.ErrorContains(t, err, \"struct doesn't have corresponding row field ignore\")\n\t})\n}\n\nfunc TestRowToStructByNameLaxEmbeddedStruct(t *testing.T) {\n\ttype Name struct {\n\t\tLast  string `db:\"last_name\"`\n\t\tFirst string `db:\"first_name\"`\n\t}\n\n\ttype person struct {\n\t\tIgnore bool `db:\"-\"`\n\t\tName\n\t\tAge int32\n\t}\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\trows, _ := conn.Query(ctx, `select 'John' as first_name, 'Smith' as last_name, n as age from generate_series(0, 9) n`)\n\t\tslice, err := pgx.CollectRows(rows, pgx.RowToStructByNameLax[person])\n\t\tassert.NoError(t, err)\n\n\t\tassert.Len(t, slice, 10)\n\t\tfor i := range slice {\n\t\t\tassert.Equal(t, \"Smith\", slice[i].Name.Last)\n\t\t\tassert.Equal(t, \"John\", slice[i].Name.First)\n\t\t\tassert.EqualValues(t, i, slice[i].Age)\n\t\t}\n\n\t\t// check missing fields in a returned row\n\t\trows, _ = conn.Query(ctx, `select 'John' as first_name, n as age from generate_series(0, 9) n`)\n\t\tslice, err = pgx.CollectRows(rows, pgx.RowToStructByNameLax[person])\n\t\tassert.NoError(t, err)\n\n\t\tassert.Len(t, slice, 10)\n\t\tfor i := range slice {\n\t\t\tassert.Equal(t, \"John\", slice[i].Name.First)\n\t\t\tassert.EqualValues(t, i, slice[i].Age)\n\t\t}\n\n\t\t// check extra fields in a returned row\n\t\trows, _ = conn.Query(ctx, `select 'John' as first_name, 'Smith' as last_name, n as age, null as ignore from generate_series(0, 9) n`)\n\t\t_, err = pgx.CollectRows(rows, pgx.RowToAddrOfStructByNameLax[person])\n\t\tassert.ErrorContains(t, err, \"struct doesn't have corresponding row field ignore\")\n\n\t\t// check missing fields in a destination struct\n\t\trows, _ = conn.Query(ctx, `select 'Smith' as last_name, 'D.' as middle_name, n as age from generate_series(0, 9) n`)\n\t\t_, err = pgx.CollectRows(rows, pgx.RowToAddrOfStructByNameLax[person])\n\t\tassert.ErrorContains(t, err, \"struct doesn't have corresponding row field middle_name\")\n\n\t\t// check ignored fields in a destination struct\n\t\trows, _ = conn.Query(ctx, `select 'Smith' as last_name, n as age, null as ignore from generate_series(0, 9) n`)\n\t\t_, err = pgx.CollectRows(rows, pgx.RowToAddrOfStructByNameLax[person])\n\t\tassert.ErrorContains(t, err, \"struct doesn't have corresponding row field ignore\")\n\t})\n}\n\nfunc TestRowToStructByNameLaxRowValue(t *testing.T) {\n\ttype AnotherTable struct{}\n\ttype User struct {\n\t\tUserID int    `json:\"userId\" db:\"user_id\"`\n\t\tName   string `json:\"name\" db:\"name\"`\n\t}\n\ttype UserAPIKey struct {\n\t\tUserAPIKeyID int `json:\"userApiKeyId\" db:\"user_api_key_id\"`\n\t\tUserID       int `json:\"userId\" db:\"user_id\"`\n\n\t\tUser         *User         `json:\"user\" db:\"user\"`\n\t\tAnotherTable *AnotherTable `json:\"anotherTable\" db:\"another_table\"`\n\t}\n\n\tdefaultConnTestRunner.RunTest(context.Background(), t, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tpgxtest.SkipCockroachDB(t, conn, \"\")\n\n\t\trows, _ := conn.Query(ctx, `\n\t\tWITH user_api_keys AS (\n\t\t\tSELECT 1 AS user_id, 101 AS user_api_key_id, 'abc123' AS api_key\n\t\t), users AS (\n\t\t\tSELECT 1 AS user_id, 'John Doe' AS name\n\t\t)\n\t\tSELECT user_api_keys.user_api_key_id, user_api_keys.user_id, row(users.*) AS user\n\t\tFROM user_api_keys\n\t\tLEFT JOIN users ON users.user_id = user_api_keys.user_id\n\t\tWHERE user_api_keys.api_key = 'abc123';\n\t\t`)\n\t\tslice, err := pgx.CollectRows(rows, pgx.RowToStructByNameLax[UserAPIKey])\n\n\t\tassert.NoError(t, err)\n\t\tassert.ElementsMatch(t, slice, []UserAPIKey{{UserAPIKeyID: 101, UserID: 1, User: &User{UserID: 1, Name: \"John Doe\"}, AnotherTable: nil}})\n\t})\n}\n\nfunc ExampleRowToStructByNameLax() {\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tconn, err := pgx.Connect(ctx, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tif err != nil {\n\t\tfmt.Printf(\"Unable to establish connection: %v\", err)\n\t\treturn\n\t}\n\n\tif conn.PgConn().ParameterStatus(\"crdb_version\") != \"\" {\n\t\t// Skip test / example when running on CockroachDB. Since an example can't be skipped fake success instead.\n\t\tfmt.Println(`Cheeseburger: $10\nFries: $5\nSoft Drink: $3`)\n\t\treturn\n\t}\n\n\t// Setup example schema and data.\n\t_, err = conn.Exec(ctx, `\ncreate temporary table products (\n\tid int primary key generated by default as identity,\n\tname varchar(100) not null,\n\tprice int not null\n);\n\ninsert into products (name, price) values\n\t('Cheeseburger', 10),\n\t('Double Cheeseburger', 14),\n\t('Fries', 5),\n\t('Soft Drink', 3);\n`)\n\tif err != nil {\n\t\tfmt.Printf(\"Unable to setup example schema and data: %v\", err)\n\t\treturn\n\t}\n\n\ttype product struct {\n\t\tID    int32\n\t\tName  string\n\t\tType  string\n\t\tPrice int32\n\t}\n\n\trows, _ := conn.Query(ctx, \"select * from products where price < $1 order by price desc\", 12)\n\tproducts, err := pgx.CollectRows(rows, pgx.RowToStructByNameLax[product])\n\tif err != nil {\n\t\tfmt.Printf(\"CollectRows error: %v\", err)\n\t\treturn\n\t}\n\n\tfor _, p := range products {\n\t\tfmt.Printf(\"%s: $%d\\n\", p.Name, p.Price)\n\t}\n\n\t// Output:\n\t// Cheeseburger: $10\n\t// Fries: $5\n\t// Soft Drink: $3\n}\n"
        },
        {
          "name": "stdlib",
          "type": "tree",
          "content": null
        },
        {
          "name": "testsetup",
          "type": "tree",
          "content": null
        },
        {
          "name": "tracelog",
          "type": "tree",
          "content": null
        },
        {
          "name": "tracer.go",
          "type": "blob",
          "size": 2.9521484375,
          "content": "package pgx\n\nimport (\n\t\"context\"\n\n\t\"github.com/jackc/pgx/v5/pgconn\"\n)\n\n// QueryTracer traces Query, QueryRow, and Exec.\ntype QueryTracer interface {\n\t// TraceQueryStart is called at the beginning of Query, QueryRow, and Exec calls. The returned context is used for the\n\t// rest of the call and will be passed to TraceQueryEnd.\n\tTraceQueryStart(ctx context.Context, conn *Conn, data TraceQueryStartData) context.Context\n\n\tTraceQueryEnd(ctx context.Context, conn *Conn, data TraceQueryEndData)\n}\n\ntype TraceQueryStartData struct {\n\tSQL  string\n\tArgs []any\n}\n\ntype TraceQueryEndData struct {\n\tCommandTag pgconn.CommandTag\n\tErr        error\n}\n\n// BatchTracer traces SendBatch.\ntype BatchTracer interface {\n\t// TraceBatchStart is called at the beginning of SendBatch calls. The returned context is used for the\n\t// rest of the call and will be passed to TraceBatchQuery and TraceBatchEnd.\n\tTraceBatchStart(ctx context.Context, conn *Conn, data TraceBatchStartData) context.Context\n\n\tTraceBatchQuery(ctx context.Context, conn *Conn, data TraceBatchQueryData)\n\tTraceBatchEnd(ctx context.Context, conn *Conn, data TraceBatchEndData)\n}\n\ntype TraceBatchStartData struct {\n\tBatch *Batch\n}\n\ntype TraceBatchQueryData struct {\n\tSQL        string\n\tArgs       []any\n\tCommandTag pgconn.CommandTag\n\tErr        error\n}\n\ntype TraceBatchEndData struct {\n\tErr error\n}\n\n// CopyFromTracer traces CopyFrom.\ntype CopyFromTracer interface {\n\t// TraceCopyFromStart is called at the beginning of CopyFrom calls. The returned context is used for the\n\t// rest of the call and will be passed to TraceCopyFromEnd.\n\tTraceCopyFromStart(ctx context.Context, conn *Conn, data TraceCopyFromStartData) context.Context\n\n\tTraceCopyFromEnd(ctx context.Context, conn *Conn, data TraceCopyFromEndData)\n}\n\ntype TraceCopyFromStartData struct {\n\tTableName   Identifier\n\tColumnNames []string\n}\n\ntype TraceCopyFromEndData struct {\n\tCommandTag pgconn.CommandTag\n\tErr        error\n}\n\n// PrepareTracer traces Prepare.\ntype PrepareTracer interface {\n\t// TracePrepareStart is called at the beginning of Prepare calls. The returned context is used for the\n\t// rest of the call and will be passed to TracePrepareEnd.\n\tTracePrepareStart(ctx context.Context, conn *Conn, data TracePrepareStartData) context.Context\n\n\tTracePrepareEnd(ctx context.Context, conn *Conn, data TracePrepareEndData)\n}\n\ntype TracePrepareStartData struct {\n\tName string\n\tSQL  string\n}\n\ntype TracePrepareEndData struct {\n\tAlreadyPrepared bool\n\tErr             error\n}\n\n// ConnectTracer traces Connect and ConnectConfig.\ntype ConnectTracer interface {\n\t// TraceConnectStart is called at the beginning of Connect and ConnectConfig calls. The returned context is used for\n\t// the rest of the call and will be passed to TraceConnectEnd.\n\tTraceConnectStart(ctx context.Context, data TraceConnectStartData) context.Context\n\n\tTraceConnectEnd(ctx context.Context, data TraceConnectEndData)\n}\n\ntype TraceConnectStartData struct {\n\tConnConfig *ConnConfig\n}\n\ntype TraceConnectEndData struct {\n\tConn *Conn\n\tErr  error\n}\n"
        },
        {
          "name": "tracer_test.go",
          "type": "blob",
          "size": 18.306640625,
          "content": "package pgx_test\n\nimport (\n\t\"context\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgxtest\"\n\t\"github.com/stretchr/testify/require\"\n)\n\ntype testTracer struct {\n\ttraceQueryStart    func(ctx context.Context, conn *pgx.Conn, data pgx.TraceQueryStartData) context.Context\n\ttraceQueryEnd      func(ctx context.Context, conn *pgx.Conn, data pgx.TraceQueryEndData)\n\ttraceBatchStart    func(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchStartData) context.Context\n\ttraceBatchQuery    func(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchQueryData)\n\ttraceBatchEnd      func(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchEndData)\n\ttraceCopyFromStart func(ctx context.Context, conn *pgx.Conn, data pgx.TraceCopyFromStartData) context.Context\n\ttraceCopyFromEnd   func(ctx context.Context, conn *pgx.Conn, data pgx.TraceCopyFromEndData)\n\ttracePrepareStart  func(ctx context.Context, conn *pgx.Conn, data pgx.TracePrepareStartData) context.Context\n\ttracePrepareEnd    func(ctx context.Context, conn *pgx.Conn, data pgx.TracePrepareEndData)\n\ttraceConnectStart  func(ctx context.Context, data pgx.TraceConnectStartData) context.Context\n\ttraceConnectEnd    func(ctx context.Context, data pgx.TraceConnectEndData)\n}\n\ntype ctxKey string\n\nfunc (tt *testTracer) TraceQueryStart(ctx context.Context, conn *pgx.Conn, data pgx.TraceQueryStartData) context.Context {\n\tif tt.traceQueryStart != nil {\n\t\treturn tt.traceQueryStart(ctx, conn, data)\n\t}\n\treturn ctx\n}\n\nfunc (tt *testTracer) TraceQueryEnd(ctx context.Context, conn *pgx.Conn, data pgx.TraceQueryEndData) {\n\tif tt.traceQueryEnd != nil {\n\t\ttt.traceQueryEnd(ctx, conn, data)\n\t}\n}\n\nfunc (tt *testTracer) TraceBatchStart(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchStartData) context.Context {\n\tif tt.traceBatchStart != nil {\n\t\treturn tt.traceBatchStart(ctx, conn, data)\n\t}\n\treturn ctx\n}\n\nfunc (tt *testTracer) TraceBatchQuery(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchQueryData) {\n\tif tt.traceBatchQuery != nil {\n\t\ttt.traceBatchQuery(ctx, conn, data)\n\t}\n}\n\nfunc (tt *testTracer) TraceBatchEnd(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchEndData) {\n\tif tt.traceBatchEnd != nil {\n\t\ttt.traceBatchEnd(ctx, conn, data)\n\t}\n}\n\nfunc (tt *testTracer) TraceCopyFromStart(ctx context.Context, conn *pgx.Conn, data pgx.TraceCopyFromStartData) context.Context {\n\tif tt.traceCopyFromStart != nil {\n\t\treturn tt.traceCopyFromStart(ctx, conn, data)\n\t}\n\treturn ctx\n}\n\nfunc (tt *testTracer) TraceCopyFromEnd(ctx context.Context, conn *pgx.Conn, data pgx.TraceCopyFromEndData) {\n\tif tt.traceCopyFromEnd != nil {\n\t\ttt.traceCopyFromEnd(ctx, conn, data)\n\t}\n}\n\nfunc (tt *testTracer) TracePrepareStart(ctx context.Context, conn *pgx.Conn, data pgx.TracePrepareStartData) context.Context {\n\tif tt.tracePrepareStart != nil {\n\t\treturn tt.tracePrepareStart(ctx, conn, data)\n\t}\n\treturn ctx\n}\n\nfunc (tt *testTracer) TracePrepareEnd(ctx context.Context, conn *pgx.Conn, data pgx.TracePrepareEndData) {\n\tif tt.tracePrepareEnd != nil {\n\t\ttt.tracePrepareEnd(ctx, conn, data)\n\t}\n}\n\nfunc (tt *testTracer) TraceConnectStart(ctx context.Context, data pgx.TraceConnectStartData) context.Context {\n\tif tt.traceConnectStart != nil {\n\t\treturn tt.traceConnectStart(ctx, data)\n\t}\n\treturn ctx\n}\n\nfunc (tt *testTracer) TraceConnectEnd(ctx context.Context, data pgx.TraceConnectEndData) {\n\tif tt.traceConnectEnd != nil {\n\t\ttt.traceConnectEnd(ctx, data)\n\t}\n}\n\nfunc TestTraceExec(t *testing.T) {\n\tt.Parallel()\n\n\ttracer := &testTracer{}\n\n\tctr := defaultConnTestRunner\n\tctr.CreateConfig = func(ctx context.Context, t testing.TB) *pgx.ConnConfig {\n\t\tconfig := defaultConnTestRunner.CreateConfig(ctx, t)\n\t\tconfig.Tracer = tracer\n\t\treturn config\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, ctr, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttraceQueryStartCalled := false\n\t\ttracer.traceQueryStart = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceQueryStartData) context.Context {\n\t\t\ttraceQueryStartCalled = true\n\t\t\trequire.Equal(t, `select $1::text`, data.SQL)\n\t\t\trequire.Len(t, data.Args, 1)\n\t\t\trequire.Equal(t, `testing`, data.Args[0])\n\t\t\treturn context.WithValue(ctx, ctxKey(ctxKey(\"fromTraceQueryStart\")), \"foo\")\n\t\t}\n\n\t\ttraceQueryEndCalled := false\n\t\ttracer.traceQueryEnd = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceQueryEndData) {\n\t\t\ttraceQueryEndCalled = true\n\t\t\trequire.Equal(t, \"foo\", ctx.Value(ctxKey(ctxKey(\"fromTraceQueryStart\"))))\n\t\t\trequire.Equal(t, `SELECT 1`, data.CommandTag.String())\n\t\t\trequire.NoError(t, data.Err)\n\t\t}\n\n\t\t_, err := conn.Exec(ctx, `select $1::text`, \"testing\")\n\t\trequire.NoError(t, err)\n\t\trequire.True(t, traceQueryStartCalled)\n\t\trequire.True(t, traceQueryEndCalled)\n\t})\n}\n\nfunc TestTraceQuery(t *testing.T) {\n\tt.Parallel()\n\n\ttracer := &testTracer{}\n\n\tctr := defaultConnTestRunner\n\tctr.CreateConfig = func(ctx context.Context, t testing.TB) *pgx.ConnConfig {\n\t\tconfig := defaultConnTestRunner.CreateConfig(ctx, t)\n\t\tconfig.Tracer = tracer\n\t\treturn config\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, ctr, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttraceQueryStartCalled := false\n\t\ttracer.traceQueryStart = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceQueryStartData) context.Context {\n\t\t\ttraceQueryStartCalled = true\n\t\t\trequire.Equal(t, `select $1::text`, data.SQL)\n\t\t\trequire.Len(t, data.Args, 1)\n\t\t\trequire.Equal(t, `testing`, data.Args[0])\n\t\t\treturn context.WithValue(ctx, ctxKey(\"fromTraceQueryStart\"), \"foo\")\n\t\t}\n\n\t\ttraceQueryEndCalled := false\n\t\ttracer.traceQueryEnd = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceQueryEndData) {\n\t\t\ttraceQueryEndCalled = true\n\t\t\trequire.Equal(t, \"foo\", ctx.Value(ctxKey(\"fromTraceQueryStart\")))\n\t\t\trequire.Equal(t, `SELECT 1`, data.CommandTag.String())\n\t\t\trequire.NoError(t, data.Err)\n\t\t}\n\n\t\tvar s string\n\t\terr := conn.QueryRow(ctx, `select $1::text`, \"testing\").Scan(&s)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, \"testing\", s)\n\t\trequire.True(t, traceQueryStartCalled)\n\t\trequire.True(t, traceQueryEndCalled)\n\t})\n}\n\nfunc TestTraceBatchNormal(t *testing.T) {\n\tt.Parallel()\n\n\ttracer := &testTracer{}\n\n\tctr := defaultConnTestRunner\n\tctr.CreateConfig = func(ctx context.Context, t testing.TB) *pgx.ConnConfig {\n\t\tconfig := defaultConnTestRunner.CreateConfig(ctx, t)\n\t\tconfig.Tracer = tracer\n\t\treturn config\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, ctr, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttraceBatchStartCalled := false\n\t\ttracer.traceBatchStart = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchStartData) context.Context {\n\t\t\ttraceBatchStartCalled = true\n\t\t\trequire.NotNil(t, data.Batch)\n\t\t\trequire.Equal(t, 2, data.Batch.Len())\n\t\t\treturn context.WithValue(ctx, ctxKey(\"fromTraceBatchStart\"), \"foo\")\n\t\t}\n\n\t\ttraceBatchQueryCalledCount := 0\n\t\ttracer.traceBatchQuery = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchQueryData) {\n\t\t\ttraceBatchQueryCalledCount++\n\t\t\trequire.Equal(t, \"foo\", ctx.Value(ctxKey(\"fromTraceBatchStart\")))\n\t\t\trequire.NoError(t, data.Err)\n\t\t}\n\n\t\ttraceBatchEndCalled := false\n\t\ttracer.traceBatchEnd = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchEndData) {\n\t\t\ttraceBatchEndCalled = true\n\t\t\trequire.Equal(t, \"foo\", ctx.Value(ctxKey(\"fromTraceBatchStart\")))\n\t\t\trequire.NoError(t, data.Err)\n\t\t}\n\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(`select 1`)\n\t\tbatch.Queue(`select 2`)\n\n\t\tbr := conn.SendBatch(context.Background(), batch)\n\t\trequire.True(t, traceBatchStartCalled)\n\n\t\tvar n int32\n\t\terr := br.QueryRow().Scan(&n)\n\t\trequire.NoError(t, err)\n\t\trequire.EqualValues(t, 1, n)\n\t\trequire.EqualValues(t, 1, traceBatchQueryCalledCount)\n\n\t\terr = br.QueryRow().Scan(&n)\n\t\trequire.NoError(t, err)\n\t\trequire.EqualValues(t, 2, n)\n\t\trequire.EqualValues(t, 2, traceBatchQueryCalledCount)\n\n\t\terr = br.Close()\n\t\trequire.NoError(t, err)\n\n\t\trequire.True(t, traceBatchEndCalled)\n\t})\n}\n\nfunc TestTraceBatchClose(t *testing.T) {\n\tt.Parallel()\n\n\ttracer := &testTracer{}\n\n\tctr := defaultConnTestRunner\n\tctr.CreateConfig = func(ctx context.Context, t testing.TB) *pgx.ConnConfig {\n\t\tconfig := defaultConnTestRunner.CreateConfig(ctx, t)\n\t\tconfig.Tracer = tracer\n\t\treturn config\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, ctr, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttraceBatchStartCalled := false\n\t\ttracer.traceBatchStart = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchStartData) context.Context {\n\t\t\ttraceBatchStartCalled = true\n\t\t\trequire.NotNil(t, data.Batch)\n\t\t\trequire.Equal(t, 2, data.Batch.Len())\n\t\t\treturn context.WithValue(ctx, ctxKey(\"fromTraceBatchStart\"), \"foo\")\n\t\t}\n\n\t\ttraceBatchQueryCalledCount := 0\n\t\ttracer.traceBatchQuery = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchQueryData) {\n\t\t\ttraceBatchQueryCalledCount++\n\t\t\trequire.Equal(t, \"foo\", ctx.Value(ctxKey(\"fromTraceBatchStart\")))\n\t\t\trequire.NoError(t, data.Err)\n\t\t}\n\n\t\ttraceBatchEndCalled := false\n\t\ttracer.traceBatchEnd = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchEndData) {\n\t\t\ttraceBatchEndCalled = true\n\t\t\trequire.Equal(t, \"foo\", ctx.Value(ctxKey(\"fromTraceBatchStart\")))\n\t\t\trequire.NoError(t, data.Err)\n\t\t}\n\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(`select 1`)\n\t\tbatch.Queue(`select 2`)\n\n\t\tbr := conn.SendBatch(context.Background(), batch)\n\t\trequire.True(t, traceBatchStartCalled)\n\t\terr := br.Close()\n\t\trequire.NoError(t, err)\n\t\trequire.EqualValues(t, 2, traceBatchQueryCalledCount)\n\t\trequire.True(t, traceBatchEndCalled)\n\t})\n}\n\nfunc TestTraceBatchErrorWhileReadingResults(t *testing.T) {\n\tt.Parallel()\n\n\ttracer := &testTracer{}\n\n\tctr := defaultConnTestRunner\n\tctr.CreateConfig = func(ctx context.Context, t testing.TB) *pgx.ConnConfig {\n\t\tconfig := defaultConnTestRunner.CreateConfig(ctx, t)\n\t\tconfig.Tracer = tracer\n\t\treturn config\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, ctr, []pgx.QueryExecMode{pgx.QueryExecModeSimpleProtocol}, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttraceBatchStartCalled := false\n\t\ttracer.traceBatchStart = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchStartData) context.Context {\n\t\t\ttraceBatchStartCalled = true\n\t\t\trequire.NotNil(t, data.Batch)\n\t\t\trequire.Equal(t, 3, data.Batch.Len())\n\t\t\treturn context.WithValue(ctx, ctxKey(\"fromTraceBatchStart\"), \"foo\")\n\t\t}\n\n\t\ttraceBatchQueryCalledCount := 0\n\t\ttracer.traceBatchQuery = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchQueryData) {\n\t\t\ttraceBatchQueryCalledCount++\n\t\t\trequire.Equal(t, \"foo\", ctx.Value(ctxKey(\"fromTraceBatchStart\")))\n\t\t\tif traceBatchQueryCalledCount == 2 {\n\t\t\t\trequire.Error(t, data.Err)\n\t\t\t} else {\n\t\t\t\trequire.NoError(t, data.Err)\n\t\t\t}\n\t\t}\n\n\t\ttraceBatchEndCalled := false\n\t\ttracer.traceBatchEnd = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchEndData) {\n\t\t\ttraceBatchEndCalled = true\n\t\t\trequire.Equal(t, \"foo\", ctx.Value(ctxKey(\"fromTraceBatchStart\")))\n\t\t\trequire.Error(t, data.Err)\n\t\t}\n\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(`select 1`)\n\t\tbatch.Queue(`select 2/n-2 from generate_series(0,10) n`)\n\t\tbatch.Queue(`select 3`)\n\n\t\tbr := conn.SendBatch(context.Background(), batch)\n\t\trequire.True(t, traceBatchStartCalled)\n\n\t\tcommandTag, err := br.Exec()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, \"SELECT 1\", commandTag.String())\n\n\t\tcommandTag, err = br.Exec()\n\t\trequire.Error(t, err)\n\t\trequire.Equal(t, \"\", commandTag.String())\n\n\t\tcommandTag, err = br.Exec()\n\t\trequire.Error(t, err)\n\t\trequire.Equal(t, \"\", commandTag.String())\n\n\t\terr = br.Close()\n\t\trequire.Error(t, err)\n\t\trequire.EqualValues(t, 2, traceBatchQueryCalledCount)\n\t\trequire.True(t, traceBatchEndCalled)\n\t})\n}\n\nfunc TestTraceBatchErrorWhileReadingResultsWhileClosing(t *testing.T) {\n\tt.Parallel()\n\n\ttracer := &testTracer{}\n\n\tctr := defaultConnTestRunner\n\tctr.CreateConfig = func(ctx context.Context, t testing.TB) *pgx.ConnConfig {\n\t\tconfig := defaultConnTestRunner.CreateConfig(ctx, t)\n\t\tconfig.Tracer = tracer\n\t\treturn config\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, ctr, []pgx.QueryExecMode{pgx.QueryExecModeSimpleProtocol}, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttraceBatchStartCalled := false\n\t\ttracer.traceBatchStart = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchStartData) context.Context {\n\t\t\ttraceBatchStartCalled = true\n\t\t\trequire.NotNil(t, data.Batch)\n\t\t\trequire.Equal(t, 3, data.Batch.Len())\n\t\t\treturn context.WithValue(ctx, ctxKey(\"fromTraceBatchStart\"), \"foo\")\n\t\t}\n\n\t\ttraceBatchQueryCalledCount := 0\n\t\ttracer.traceBatchQuery = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchQueryData) {\n\t\t\ttraceBatchQueryCalledCount++\n\t\t\trequire.Equal(t, \"foo\", ctx.Value(ctxKey(\"fromTraceBatchStart\")))\n\t\t\tif traceBatchQueryCalledCount == 2 {\n\t\t\t\trequire.Error(t, data.Err)\n\t\t\t} else {\n\t\t\t\trequire.NoError(t, data.Err)\n\t\t\t}\n\t\t}\n\n\t\ttraceBatchEndCalled := false\n\t\ttracer.traceBatchEnd = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceBatchEndData) {\n\t\t\ttraceBatchEndCalled = true\n\t\t\trequire.Equal(t, \"foo\", ctx.Value(ctxKey(\"fromTraceBatchStart\")))\n\t\t\trequire.Error(t, data.Err)\n\t\t}\n\n\t\tbatch := &pgx.Batch{}\n\t\tbatch.Queue(`select 1`)\n\t\tbatch.Queue(`select 2/n-2 from generate_series(0,10) n`)\n\t\tbatch.Queue(`select 3`)\n\n\t\tbr := conn.SendBatch(context.Background(), batch)\n\t\trequire.True(t, traceBatchStartCalled)\n\t\terr := br.Close()\n\t\trequire.Error(t, err)\n\t\trequire.EqualValues(t, 2, traceBatchQueryCalledCount)\n\t\trequire.True(t, traceBatchEndCalled)\n\t})\n}\n\nfunc TestTraceCopyFrom(t *testing.T) {\n\tt.Parallel()\n\n\ttracer := &testTracer{}\n\n\tctr := defaultConnTestRunner\n\tctr.CreateConfig = func(ctx context.Context, t testing.TB) *pgx.ConnConfig {\n\t\tconfig := defaultConnTestRunner.CreateConfig(ctx, t)\n\t\tconfig.Tracer = tracer\n\t\treturn config\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, ctr, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tctx, cancel := context.WithTimeout(ctx, 30*time.Second)\n\t\tdefer cancel()\n\n\t\ttraceCopyFromStartCalled := false\n\t\ttracer.traceCopyFromStart = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceCopyFromStartData) context.Context {\n\t\t\ttraceCopyFromStartCalled = true\n\t\t\trequire.Equal(t, pgx.Identifier{\"foo\"}, data.TableName)\n\t\t\trequire.Equal(t, []string{\"a\"}, data.ColumnNames)\n\t\t\treturn context.WithValue(ctx, ctxKey(\"fromTraceCopyFromStart\"), \"foo\")\n\t\t}\n\n\t\ttraceCopyFromEndCalled := false\n\t\ttracer.traceCopyFromEnd = func(ctx context.Context, conn *pgx.Conn, data pgx.TraceCopyFromEndData) {\n\t\t\ttraceCopyFromEndCalled = true\n\t\t\trequire.Equal(t, \"foo\", ctx.Value(ctxKey(\"fromTraceCopyFromStart\")))\n\t\t\trequire.Equal(t, `COPY 2`, data.CommandTag.String())\n\t\t\trequire.NoError(t, data.Err)\n\t\t}\n\n\t\t_, err := conn.Exec(ctx, `create temporary table foo(a int4)`)\n\t\trequire.NoError(t, err)\n\n\t\tinputRows := [][]any{\n\t\t\t{int32(1)},\n\t\t\t{nil},\n\t\t}\n\n\t\tcopyCount, err := conn.CopyFrom(ctx, pgx.Identifier{\"foo\"}, []string{\"a\"}, pgx.CopyFromRows(inputRows))\n\t\trequire.NoError(t, err)\n\t\trequire.EqualValues(t, len(inputRows), copyCount)\n\t\trequire.True(t, traceCopyFromStartCalled)\n\t\trequire.True(t, traceCopyFromEndCalled)\n\t})\n}\n\nfunc TestTracePrepare(t *testing.T) {\n\tt.Parallel()\n\n\ttracer := &testTracer{}\n\n\tctr := defaultConnTestRunner\n\tctr.CreateConfig = func(ctx context.Context, t testing.TB) *pgx.ConnConfig {\n\t\tconfig := defaultConnTestRunner.CreateConfig(ctx, t)\n\t\tconfig.Tracer = tracer\n\t\treturn config\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, ctr, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttracePrepareStartCalled := false\n\t\ttracer.tracePrepareStart = func(ctx context.Context, conn *pgx.Conn, data pgx.TracePrepareStartData) context.Context {\n\t\t\ttracePrepareStartCalled = true\n\t\t\trequire.Equal(t, `ps`, data.Name)\n\t\t\trequire.Equal(t, `select $1::text`, data.SQL)\n\t\t\treturn context.WithValue(ctx, ctxKey(\"fromTracePrepareStart\"), \"foo\")\n\t\t}\n\n\t\ttracePrepareEndCalled := false\n\t\ttracer.tracePrepareEnd = func(ctx context.Context, conn *pgx.Conn, data pgx.TracePrepareEndData) {\n\t\t\ttracePrepareEndCalled = true\n\t\t\trequire.False(t, data.AlreadyPrepared)\n\t\t\trequire.NoError(t, data.Err)\n\t\t}\n\n\t\t_, err := conn.Prepare(ctx, \"ps\", `select $1::text`)\n\t\trequire.NoError(t, err)\n\t\trequire.True(t, tracePrepareStartCalled)\n\t\trequire.True(t, tracePrepareEndCalled)\n\n\t\ttracePrepareStartCalled = false\n\t\ttracePrepareEndCalled = false\n\t\ttracer.tracePrepareEnd = func(ctx context.Context, conn *pgx.Conn, data pgx.TracePrepareEndData) {\n\t\t\ttracePrepareEndCalled = true\n\t\t\trequire.True(t, data.AlreadyPrepared)\n\t\t\trequire.NoError(t, data.Err)\n\t\t}\n\n\t\t_, err = conn.Prepare(ctx, \"ps\", `select $1::text`)\n\t\trequire.NoError(t, err)\n\t\trequire.True(t, tracePrepareStartCalled)\n\t\trequire.True(t, tracePrepareEndCalled)\n\t})\n}\n\nfunc TestTraceConnect(t *testing.T) {\n\tt.Parallel()\n\n\ttracer := &testTracer{}\n\n\tconfig := defaultConnTestRunner.CreateConfig(context.Background(), t)\n\tconfig.Tracer = tracer\n\n\ttraceConnectStartCalled := false\n\ttracer.traceConnectStart = func(ctx context.Context, data pgx.TraceConnectStartData) context.Context {\n\t\ttraceConnectStartCalled = true\n\t\trequire.NotNil(t, data.ConnConfig)\n\t\treturn context.WithValue(ctx, ctxKey(\"fromTraceConnectStart\"), \"foo\")\n\t}\n\n\ttraceConnectEndCalled := false\n\ttracer.traceConnectEnd = func(ctx context.Context, data pgx.TraceConnectEndData) {\n\t\ttraceConnectEndCalled = true\n\t\trequire.NotNil(t, data.Conn)\n\t\trequire.NoError(t, data.Err)\n\t}\n\n\tconn1, err := pgx.ConnectConfig(context.Background(), config)\n\trequire.NoError(t, err)\n\tdefer conn1.Close(context.Background())\n\trequire.True(t, traceConnectStartCalled)\n\trequire.True(t, traceConnectEndCalled)\n\n\tconfig, err = pgx.ParseConfig(\"host=/invalid\")\n\trequire.NoError(t, err)\n\tconfig.Tracer = tracer\n\n\ttraceConnectStartCalled = false\n\ttraceConnectEndCalled = false\n\ttracer.traceConnectEnd = func(ctx context.Context, data pgx.TraceConnectEndData) {\n\t\ttraceConnectEndCalled = true\n\t\trequire.Nil(t, data.Conn)\n\t\trequire.Error(t, data.Err)\n\t}\n\n\tconn2, err := pgx.ConnectConfig(context.Background(), config)\n\trequire.Nil(t, conn2)\n\trequire.Error(t, err)\n\trequire.True(t, traceConnectStartCalled)\n\trequire.True(t, traceConnectEndCalled)\n}\n"
        },
        {
          "name": "tx.go",
          "type": "blob",
          "size": 13.4326171875,
          "content": "package pgx\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/jackc/pgx/v5/pgconn\"\n)\n\n// TxIsoLevel is the transaction isolation level (serializable, repeatable read, read committed or read uncommitted)\ntype TxIsoLevel string\n\n// Transaction isolation levels\nconst (\n\tSerializable    TxIsoLevel = \"serializable\"\n\tRepeatableRead  TxIsoLevel = \"repeatable read\"\n\tReadCommitted   TxIsoLevel = \"read committed\"\n\tReadUncommitted TxIsoLevel = \"read uncommitted\"\n)\n\n// TxAccessMode is the transaction access mode (read write or read only)\ntype TxAccessMode string\n\n// Transaction access modes\nconst (\n\tReadWrite TxAccessMode = \"read write\"\n\tReadOnly  TxAccessMode = \"read only\"\n)\n\n// TxDeferrableMode is the transaction deferrable mode (deferrable or not deferrable)\ntype TxDeferrableMode string\n\n// Transaction deferrable modes\nconst (\n\tDeferrable    TxDeferrableMode = \"deferrable\"\n\tNotDeferrable TxDeferrableMode = \"not deferrable\"\n)\n\n// TxOptions are transaction modes within a transaction block\ntype TxOptions struct {\n\tIsoLevel       TxIsoLevel\n\tAccessMode     TxAccessMode\n\tDeferrableMode TxDeferrableMode\n\n\t// BeginQuery is the SQL query that will be executed to begin the transaction. This allows using non-standard syntax\n\t// such as BEGIN PRIORITY HIGH with CockroachDB. If set this will override the other settings.\n\tBeginQuery string\n\t// CommitQuery is the SQL query that will be executed to commit the transaction.\n\tCommitQuery string\n}\n\nvar emptyTxOptions TxOptions\n\nfunc (txOptions TxOptions) beginSQL() string {\n\tif txOptions == emptyTxOptions {\n\t\treturn \"begin\"\n\t}\n\n\tif txOptions.BeginQuery != \"\" {\n\t\treturn txOptions.BeginQuery\n\t}\n\n\tvar buf strings.Builder\n\tbuf.Grow(64) // 64 - maximum length of string with available options\n\tbuf.WriteString(\"begin\")\n\n\tif txOptions.IsoLevel != \"\" {\n\t\tbuf.WriteString(\" isolation level \")\n\t\tbuf.WriteString(string(txOptions.IsoLevel))\n\t}\n\tif txOptions.AccessMode != \"\" {\n\t\tbuf.WriteByte(' ')\n\t\tbuf.WriteString(string(txOptions.AccessMode))\n\t}\n\tif txOptions.DeferrableMode != \"\" {\n\t\tbuf.WriteByte(' ')\n\t\tbuf.WriteString(string(txOptions.DeferrableMode))\n\t}\n\n\treturn buf.String()\n}\n\nvar ErrTxClosed = errors.New(\"tx is closed\")\n\n// ErrTxCommitRollback occurs when an error has occurred in a transaction and\n// Commit() is called. PostgreSQL accepts COMMIT on aborted transactions, but\n// it is treated as ROLLBACK.\nvar ErrTxCommitRollback = errors.New(\"commit unexpectedly resulted in rollback\")\n\n// Begin starts a transaction. Unlike database/sql, the context only affects the begin command. i.e. there is no\n// auto-rollback on context cancellation.\nfunc (c *Conn) Begin(ctx context.Context) (Tx, error) {\n\treturn c.BeginTx(ctx, TxOptions{})\n}\n\n// BeginTx starts a transaction with txOptions determining the transaction mode. Unlike database/sql, the context only\n// affects the begin command. i.e. there is no auto-rollback on context cancellation.\nfunc (c *Conn) BeginTx(ctx context.Context, txOptions TxOptions) (Tx, error) {\n\t_, err := c.Exec(ctx, txOptions.beginSQL())\n\tif err != nil {\n\t\t// begin should never fail unless there is an underlying connection issue or\n\t\t// a context timeout. In either case, the connection is possibly broken.\n\t\tc.die()\n\t\treturn nil, err\n\t}\n\n\treturn &dbTx{\n\t\tconn:        c,\n\t\tcommitQuery: txOptions.CommitQuery,\n\t}, nil\n}\n\n// Tx represents a database transaction.\n//\n// Tx is an interface instead of a struct to enable connection pools to be implemented without relying on internal pgx\n// state, to support pseudo-nested transactions with savepoints, and to allow tests to mock transactions. However,\n// adding a method to an interface is technically a breaking change. If new methods are added to Conn it may be\n// desirable to add them to Tx as well. Because of this the Tx interface is partially excluded from semantic version\n// requirements. Methods will not be removed or changed, but new methods may be added.\ntype Tx interface {\n\t// Begin starts a pseudo nested transaction.\n\tBegin(ctx context.Context) (Tx, error)\n\n\t// Commit commits the transaction if this is a real transaction or releases the savepoint if this is a pseudo nested\n\t// transaction. Commit will return an error where errors.Is(ErrTxClosed) is true if the Tx is already closed, but is\n\t// otherwise safe to call multiple times. If the commit fails with a rollback status (e.g. the transaction was already\n\t// in a broken state) then an error where errors.Is(ErrTxCommitRollback) is true will be returned.\n\tCommit(ctx context.Context) error\n\n\t// Rollback rolls back the transaction if this is a real transaction or rolls back to the savepoint if this is a\n\t// pseudo nested transaction. Rollback will return an error where errors.Is(ErrTxClosed) is true if the Tx is already\n\t// closed, but is otherwise safe to call multiple times. Hence, a defer tx.Rollback() is safe even if tx.Commit() will\n\t// be called first in a non-error condition. Any other failure of a real transaction will result in the connection\n\t// being closed.\n\tRollback(ctx context.Context) error\n\n\tCopyFrom(ctx context.Context, tableName Identifier, columnNames []string, rowSrc CopyFromSource) (int64, error)\n\tSendBatch(ctx context.Context, b *Batch) BatchResults\n\tLargeObjects() LargeObjects\n\n\tPrepare(ctx context.Context, name, sql string) (*pgconn.StatementDescription, error)\n\n\tExec(ctx context.Context, sql string, arguments ...any) (commandTag pgconn.CommandTag, err error)\n\tQuery(ctx context.Context, sql string, args ...any) (Rows, error)\n\tQueryRow(ctx context.Context, sql string, args ...any) Row\n\n\t// Conn returns the underlying *Conn that on which this transaction is executing.\n\tConn() *Conn\n}\n\n// dbTx represents a database transaction.\n//\n// All dbTx methods return ErrTxClosed if Commit or Rollback has already been\n// called on the dbTx.\ntype dbTx struct {\n\tconn         *Conn\n\tsavepointNum int64\n\tclosed       bool\n\tcommitQuery  string\n}\n\n// Begin starts a pseudo nested transaction implemented with a savepoint.\nfunc (tx *dbTx) Begin(ctx context.Context) (Tx, error) {\n\tif tx.closed {\n\t\treturn nil, ErrTxClosed\n\t}\n\n\ttx.savepointNum++\n\t_, err := tx.conn.Exec(ctx, \"savepoint sp_\"+strconv.FormatInt(tx.savepointNum, 10))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &dbSimulatedNestedTx{tx: tx, savepointNum: tx.savepointNum}, nil\n}\n\n// Commit commits the transaction.\nfunc (tx *dbTx) Commit(ctx context.Context) error {\n\tif tx.closed {\n\t\treturn ErrTxClosed\n\t}\n\n\tcommandSQL := \"commit\"\n\tif tx.commitQuery != \"\" {\n\t\tcommandSQL = tx.commitQuery\n\t}\n\n\tcommandTag, err := tx.conn.Exec(ctx, commandSQL)\n\ttx.closed = true\n\tif err != nil {\n\t\tif tx.conn.PgConn().TxStatus() != 'I' {\n\t\t\t_ = tx.conn.Close(ctx) // already have error to return\n\t\t}\n\t\treturn err\n\t}\n\tif commandTag.String() == \"ROLLBACK\" {\n\t\treturn ErrTxCommitRollback\n\t}\n\n\treturn nil\n}\n\n// Rollback rolls back the transaction. Rollback will return ErrTxClosed if the\n// Tx is already closed, but is otherwise safe to call multiple times. Hence, a\n// defer tx.Rollback() is safe even if tx.Commit() will be called first in a\n// non-error condition.\nfunc (tx *dbTx) Rollback(ctx context.Context) error {\n\tif tx.closed {\n\t\treturn ErrTxClosed\n\t}\n\n\t_, err := tx.conn.Exec(ctx, \"rollback\")\n\ttx.closed = true\n\tif err != nil {\n\t\t// A rollback failure leaves the connection in an undefined state\n\t\ttx.conn.die()\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// Exec delegates to the underlying *Conn\nfunc (tx *dbTx) Exec(ctx context.Context, sql string, arguments ...any) (commandTag pgconn.CommandTag, err error) {\n\tif tx.closed {\n\t\treturn pgconn.CommandTag{}, ErrTxClosed\n\t}\n\n\treturn tx.conn.Exec(ctx, sql, arguments...)\n}\n\n// Prepare delegates to the underlying *Conn\nfunc (tx *dbTx) Prepare(ctx context.Context, name, sql string) (*pgconn.StatementDescription, error) {\n\tif tx.closed {\n\t\treturn nil, ErrTxClosed\n\t}\n\n\treturn tx.conn.Prepare(ctx, name, sql)\n}\n\n// Query delegates to the underlying *Conn\nfunc (tx *dbTx) Query(ctx context.Context, sql string, args ...any) (Rows, error) {\n\tif tx.closed {\n\t\t// Because checking for errors can be deferred to the *Rows, build one with the error\n\t\terr := ErrTxClosed\n\t\treturn &baseRows{closed: true, err: err}, err\n\t}\n\n\treturn tx.conn.Query(ctx, sql, args...)\n}\n\n// QueryRow delegates to the underlying *Conn\nfunc (tx *dbTx) QueryRow(ctx context.Context, sql string, args ...any) Row {\n\trows, _ := tx.Query(ctx, sql, args...)\n\treturn (*connRow)(rows.(*baseRows))\n}\n\n// CopyFrom delegates to the underlying *Conn\nfunc (tx *dbTx) CopyFrom(ctx context.Context, tableName Identifier, columnNames []string, rowSrc CopyFromSource) (int64, error) {\n\tif tx.closed {\n\t\treturn 0, ErrTxClosed\n\t}\n\n\treturn tx.conn.CopyFrom(ctx, tableName, columnNames, rowSrc)\n}\n\n// SendBatch delegates to the underlying *Conn\nfunc (tx *dbTx) SendBatch(ctx context.Context, b *Batch) BatchResults {\n\tif tx.closed {\n\t\treturn &batchResults{err: ErrTxClosed}\n\t}\n\n\treturn tx.conn.SendBatch(ctx, b)\n}\n\n// LargeObjects returns a LargeObjects instance for the transaction.\nfunc (tx *dbTx) LargeObjects() LargeObjects {\n\treturn LargeObjects{tx: tx}\n}\n\nfunc (tx *dbTx) Conn() *Conn {\n\treturn tx.conn\n}\n\n// dbSimulatedNestedTx represents a simulated nested transaction implemented by a savepoint.\ntype dbSimulatedNestedTx struct {\n\ttx           Tx\n\tsavepointNum int64\n\tclosed       bool\n}\n\n// Begin starts a pseudo nested transaction implemented with a savepoint.\nfunc (sp *dbSimulatedNestedTx) Begin(ctx context.Context) (Tx, error) {\n\tif sp.closed {\n\t\treturn nil, ErrTxClosed\n\t}\n\n\treturn sp.tx.Begin(ctx)\n}\n\n// Commit releases the savepoint essentially committing the pseudo nested transaction.\nfunc (sp *dbSimulatedNestedTx) Commit(ctx context.Context) error {\n\tif sp.closed {\n\t\treturn ErrTxClosed\n\t}\n\n\t_, err := sp.Exec(ctx, \"release savepoint sp_\"+strconv.FormatInt(sp.savepointNum, 10))\n\tsp.closed = true\n\treturn err\n}\n\n// Rollback rolls back to the savepoint essentially rolling back the pseudo nested transaction. Rollback will return\n// ErrTxClosed if the dbSavepoint is already closed, but is otherwise safe to call multiple times. Hence, a defer sp.Rollback()\n// is safe even if sp.Commit() will be called first in a non-error condition.\nfunc (sp *dbSimulatedNestedTx) Rollback(ctx context.Context) error {\n\tif sp.closed {\n\t\treturn ErrTxClosed\n\t}\n\n\t_, err := sp.Exec(ctx, \"rollback to savepoint sp_\"+strconv.FormatInt(sp.savepointNum, 10))\n\tsp.closed = true\n\treturn err\n}\n\n// Exec delegates to the underlying Tx\nfunc (sp *dbSimulatedNestedTx) Exec(ctx context.Context, sql string, arguments ...any) (commandTag pgconn.CommandTag, err error) {\n\tif sp.closed {\n\t\treturn pgconn.CommandTag{}, ErrTxClosed\n\t}\n\n\treturn sp.tx.Exec(ctx, sql, arguments...)\n}\n\n// Prepare delegates to the underlying Tx\nfunc (sp *dbSimulatedNestedTx) Prepare(ctx context.Context, name, sql string) (*pgconn.StatementDescription, error) {\n\tif sp.closed {\n\t\treturn nil, ErrTxClosed\n\t}\n\n\treturn sp.tx.Prepare(ctx, name, sql)\n}\n\n// Query delegates to the underlying Tx\nfunc (sp *dbSimulatedNestedTx) Query(ctx context.Context, sql string, args ...any) (Rows, error) {\n\tif sp.closed {\n\t\t// Because checking for errors can be deferred to the *Rows, build one with the error\n\t\terr := ErrTxClosed\n\t\treturn &baseRows{closed: true, err: err}, err\n\t}\n\n\treturn sp.tx.Query(ctx, sql, args...)\n}\n\n// QueryRow delegates to the underlying Tx\nfunc (sp *dbSimulatedNestedTx) QueryRow(ctx context.Context, sql string, args ...any) Row {\n\trows, _ := sp.Query(ctx, sql, args...)\n\treturn (*connRow)(rows.(*baseRows))\n}\n\n// CopyFrom delegates to the underlying *Conn\nfunc (sp *dbSimulatedNestedTx) CopyFrom(ctx context.Context, tableName Identifier, columnNames []string, rowSrc CopyFromSource) (int64, error) {\n\tif sp.closed {\n\t\treturn 0, ErrTxClosed\n\t}\n\n\treturn sp.tx.CopyFrom(ctx, tableName, columnNames, rowSrc)\n}\n\n// SendBatch delegates to the underlying *Conn\nfunc (sp *dbSimulatedNestedTx) SendBatch(ctx context.Context, b *Batch) BatchResults {\n\tif sp.closed {\n\t\treturn &batchResults{err: ErrTxClosed}\n\t}\n\n\treturn sp.tx.SendBatch(ctx, b)\n}\n\nfunc (sp *dbSimulatedNestedTx) LargeObjects() LargeObjects {\n\treturn LargeObjects{tx: sp}\n}\n\nfunc (sp *dbSimulatedNestedTx) Conn() *Conn {\n\treturn sp.tx.Conn()\n}\n\n// BeginFunc calls Begin on db and then calls fn. If fn does not return an error then it calls Commit on db. If fn\n// returns an error it calls Rollback on db. The context will be used when executing the transaction control statements\n// (BEGIN, ROLLBACK, and COMMIT) but does not otherwise affect the execution of fn.\nfunc BeginFunc(\n\tctx context.Context,\n\tdb interface {\n\t\tBegin(ctx context.Context) (Tx, error)\n\t},\n\tfn func(Tx) error,\n) (err error) {\n\tvar tx Tx\n\ttx, err = db.Begin(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn beginFuncExec(ctx, tx, fn)\n}\n\n// BeginTxFunc calls BeginTx on db and then calls fn. If fn does not return an error then it calls Commit on db. If fn\n// returns an error it calls Rollback on db. The context will be used when executing the transaction control statements\n// (BEGIN, ROLLBACK, and COMMIT) but does not otherwise affect the execution of fn.\nfunc BeginTxFunc(\n\tctx context.Context,\n\tdb interface {\n\t\tBeginTx(ctx context.Context, txOptions TxOptions) (Tx, error)\n\t},\n\ttxOptions TxOptions,\n\tfn func(Tx) error,\n) (err error) {\n\tvar tx Tx\n\ttx, err = db.BeginTx(ctx, txOptions)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn beginFuncExec(ctx, tx, fn)\n}\n\nfunc beginFuncExec(ctx context.Context, tx Tx, fn func(Tx) error) (err error) {\n\tdefer func() {\n\t\trollbackErr := tx.Rollback(ctx)\n\t\tif rollbackErr != nil && !errors.Is(rollbackErr, ErrTxClosed) {\n\t\t\terr = rollbackErr\n\t\t}\n\t}()\n\n\tfErr := fn(tx)\n\tif fErr != nil {\n\t\t_ = tx.Rollback(ctx) // ignore rollback error as there is already an error to return\n\t\treturn fErr\n\t}\n\n\treturn tx.Commit(ctx)\n}\n"
        },
        {
          "name": "tx_test.go",
          "type": "blob",
          "size": 15.697265625,
          "content": "package pgx_test\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgxtest\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestTransactionSuccessfulCommit(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tcreateSql := `\n    create temporary table foo(\n      id integer,\n      unique (id)\n    );\n  `\n\n\tif _, err := conn.Exec(context.Background(), createSql); err != nil {\n\t\tt.Fatalf(\"Failed to create table: %v\", err)\n\t}\n\n\ttx, err := conn.Begin(context.Background())\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Begin failed: %v\", err)\n\t}\n\n\t_, err = tx.Exec(context.Background(), \"insert into foo(id) values (1)\")\n\tif err != nil {\n\t\tt.Fatalf(\"tx.Exec failed: %v\", err)\n\t}\n\n\terr = tx.Commit(context.Background())\n\tif err != nil {\n\t\tt.Fatalf(\"tx.Commit failed: %v\", err)\n\t}\n\n\tvar n int64\n\terr = conn.QueryRow(context.Background(), \"select count(*) from foo\").Scan(&n)\n\tif err != nil {\n\t\tt.Fatalf(\"QueryRow Scan failed: %v\", err)\n\t}\n\tif n != 1 {\n\t\tt.Fatalf(\"Did not receive correct number of rows: %v\", n)\n\t}\n}\n\nfunc TestTxCommitWhenTxBroken(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tcreateSql := `\n    create temporary table foo(\n      id integer,\n      unique (id)\n    );\n  `\n\n\tif _, err := conn.Exec(context.Background(), createSql); err != nil {\n\t\tt.Fatalf(\"Failed to create table: %v\", err)\n\t}\n\n\ttx, err := conn.Begin(context.Background())\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Begin failed: %v\", err)\n\t}\n\n\tif _, err := tx.Exec(context.Background(), \"insert into foo(id) values (1)\"); err != nil {\n\t\tt.Fatalf(\"tx.Exec failed: %v\", err)\n\t}\n\n\t// Purposely break transaction\n\tif _, err := tx.Exec(context.Background(), \"syntax error\"); err == nil {\n\t\tt.Fatal(\"Unexpected success\")\n\t}\n\n\terr = tx.Commit(context.Background())\n\tif err != pgx.ErrTxCommitRollback {\n\t\tt.Fatalf(\"Expected error %v, got %v\", pgx.ErrTxCommitRollback, err)\n\t}\n\n\tvar n int64\n\terr = conn.QueryRow(context.Background(), \"select count(*) from foo\").Scan(&n)\n\tif err != nil {\n\t\tt.Fatalf(\"QueryRow Scan failed: %v\", err)\n\t}\n\tif n != 0 {\n\t\tt.Fatalf(\"Did not receive correct number of rows: %v\", n)\n\t}\n}\n\nfunc TestTxCommitWhenDeferredConstraintFailure(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server does not support deferred constraint (https://github.com/cockroachdb/cockroach/issues/31632)\")\n\n\tcreateSql := `\n    create temporary table foo(\n      id integer,\n      unique (id) initially deferred\n    );\n  `\n\n\tif _, err := conn.Exec(context.Background(), createSql); err != nil {\n\t\tt.Fatalf(\"Failed to create table: %v\", err)\n\t}\n\n\ttx, err := conn.Begin(context.Background())\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Begin failed: %v\", err)\n\t}\n\n\tif _, err := tx.Exec(context.Background(), \"insert into foo(id) values (1)\"); err != nil {\n\t\tt.Fatalf(\"tx.Exec failed: %v\", err)\n\t}\n\n\tif _, err := tx.Exec(context.Background(), \"insert into foo(id) values (1)\"); err != nil {\n\t\tt.Fatalf(\"tx.Exec failed: %v\", err)\n\t}\n\n\terr = tx.Commit(context.Background())\n\tif pgErr, ok := err.(*pgconn.PgError); !ok || pgErr.Code != \"23505\" {\n\t\tt.Fatalf(\"Expected unique constraint violation 23505, got %#v\", err)\n\t}\n\n\tvar n int64\n\terr = conn.QueryRow(context.Background(), \"select count(*) from foo\").Scan(&n)\n\tif err != nil {\n\t\tt.Fatalf(\"QueryRow Scan failed: %v\", err)\n\t}\n\tif n != 0 {\n\t\tt.Fatalf(\"Did not receive correct number of rows: %v\", n)\n\t}\n}\n\nfunc TestTxCommitSerializationFailure(t *testing.T) {\n\tt.Parallel()\n\n\tc1 := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, c1)\n\n\tif c1.PgConn().ParameterStatus(\"crdb_version\") != \"\" {\n\t\tt.Skip(\"Skipping due to known server issue: (https://github.com/cockroachdb/cockroach/issues/60754)\")\n\t}\n\n\tc2 := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, c2)\n\n\tctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n\tdefer cancel()\n\n\tc1.Exec(ctx, `drop table if exists tx_serializable_sums`)\n\t_, err := c1.Exec(ctx, `create table tx_serializable_sums(num integer);`)\n\tif err != nil {\n\t\tt.Fatalf(\"Unable to create temporary table: %v\", err)\n\t}\n\tdefer c1.Exec(ctx, `drop table tx_serializable_sums`)\n\n\ttx1, err := c1.BeginTx(ctx, pgx.TxOptions{IsoLevel: pgx.Serializable})\n\tif err != nil {\n\t\tt.Fatalf(\"Begin failed: %v\", err)\n\t}\n\tdefer tx1.Rollback(ctx)\n\n\ttx2, err := c2.BeginTx(ctx, pgx.TxOptions{IsoLevel: pgx.Serializable})\n\tif err != nil {\n\t\tt.Fatalf(\"Begin failed: %v\", err)\n\t}\n\tdefer tx2.Rollback(ctx)\n\n\t_, err = tx1.Exec(ctx, `insert into tx_serializable_sums(num) select sum(num)::int from tx_serializable_sums`)\n\tif err != nil {\n\t\tt.Fatalf(\"Exec failed: %v\", err)\n\t}\n\n\t_, err = tx2.Exec(ctx, `insert into tx_serializable_sums(num) select sum(num)::int from tx_serializable_sums`)\n\tif err != nil {\n\t\tt.Fatalf(\"Exec failed: %v\", err)\n\t}\n\n\terr = tx1.Commit(ctx)\n\tif err != nil {\n\t\tt.Fatalf(\"Commit failed: %v\", err)\n\t}\n\n\terr = tx2.Commit(ctx)\n\tif pgErr, ok := err.(*pgconn.PgError); !ok || pgErr.Code != \"40001\" {\n\t\tt.Fatalf(\"Expected serialization error 40001, got %#v\", err)\n\t}\n\n\tensureConnValid(t, c1)\n\tensureConnValid(t, c2)\n}\n\nfunc TestTransactionSuccessfulRollback(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tcreateSql := `\n    create temporary table foo(\n      id integer,\n      unique (id)\n    );\n  `\n\n\tif _, err := conn.Exec(context.Background(), createSql); err != nil {\n\t\tt.Fatalf(\"Failed to create table: %v\", err)\n\t}\n\n\ttx, err := conn.Begin(context.Background())\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Begin failed: %v\", err)\n\t}\n\n\t_, err = tx.Exec(context.Background(), \"insert into foo(id) values (1)\")\n\tif err != nil {\n\t\tt.Fatalf(\"tx.Exec failed: %v\", err)\n\t}\n\n\terr = tx.Rollback(context.Background())\n\tif err != nil {\n\t\tt.Fatalf(\"tx.Rollback failed: %v\", err)\n\t}\n\n\tvar n int64\n\terr = conn.QueryRow(context.Background(), \"select count(*) from foo\").Scan(&n)\n\tif err != nil {\n\t\tt.Fatalf(\"QueryRow Scan failed: %v\", err)\n\t}\n\tif n != 0 {\n\t\tt.Fatalf(\"Did not receive correct number of rows: %v\", n)\n\t}\n}\n\nfunc TestTransactionRollbackFailsClosesConnection(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tctx, cancel := context.WithCancel(context.Background())\n\n\ttx, err := conn.Begin(ctx)\n\trequire.NoError(t, err)\n\n\tcancel()\n\n\terr = tx.Rollback(ctx)\n\trequire.Error(t, err)\n\n\trequire.True(t, conn.IsClosed())\n}\n\nfunc TestBeginIsoLevels(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tpgxtest.SkipCockroachDB(t, conn, \"Server always uses SERIALIZABLE isolation (https://www.cockroachlabs.com/docs/stable/demo-serializable.html)\")\n\n\tisoLevels := []pgx.TxIsoLevel{pgx.Serializable, pgx.RepeatableRead, pgx.ReadCommitted, pgx.ReadUncommitted}\n\tfor _, iso := range isoLevels {\n\t\ttx, err := conn.BeginTx(context.Background(), pgx.TxOptions{IsoLevel: iso})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"conn.Begin failed: %v\", err)\n\t\t}\n\n\t\tvar level pgx.TxIsoLevel\n\t\tconn.QueryRow(context.Background(), \"select current_setting('transaction_isolation')\").Scan(&level)\n\t\tif level != iso {\n\t\t\tt.Errorf(\"Expected to be in isolation level %v but was %v\", iso, level)\n\t\t}\n\n\t\terr = tx.Rollback(context.Background())\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"tx.Rollback failed: %v\", err)\n\t\t}\n\t}\n}\n\nfunc TestBeginFunc(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tcreateSql := `\n    create temporary table foo(\n      id integer,\n      unique (id)\n    );\n  `\n\n\t_, err := conn.Exec(context.Background(), createSql)\n\trequire.NoError(t, err)\n\n\terr = pgx.BeginFunc(context.Background(), conn, func(tx pgx.Tx) error {\n\t\t_, err := tx.Exec(context.Background(), \"insert into foo(id) values (1)\")\n\t\trequire.NoError(t, err)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n\n\tvar n int64\n\terr = conn.QueryRow(context.Background(), \"select count(*) from foo\").Scan(&n)\n\trequire.NoError(t, err)\n\trequire.EqualValues(t, 1, n)\n}\n\nfunc TestBeginFuncRollbackOnError(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tcreateSql := `\n    create temporary table foo(\n      id integer,\n      unique (id)\n    );\n  `\n\n\t_, err := conn.Exec(context.Background(), createSql)\n\trequire.NoError(t, err)\n\n\terr = pgx.BeginFunc(context.Background(), conn, func(tx pgx.Tx) error {\n\t\t_, err := tx.Exec(context.Background(), \"insert into foo(id) values (1)\")\n\t\trequire.NoError(t, err)\n\t\treturn errors.New(\"some error\")\n\t})\n\trequire.EqualError(t, err, \"some error\")\n\n\tvar n int64\n\terr = conn.QueryRow(context.Background(), \"select count(*) from foo\").Scan(&n)\n\trequire.NoError(t, err)\n\trequire.EqualValues(t, 0, n)\n}\n\nfunc TestBeginReadOnly(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\ttx, err := conn.BeginTx(context.Background(), pgx.TxOptions{AccessMode: pgx.ReadOnly})\n\tif err != nil {\n\t\tt.Fatalf(\"conn.Begin failed: %v\", err)\n\t}\n\tdefer tx.Rollback(context.Background())\n\n\t_, err = conn.Exec(context.Background(), \"create table foo(id serial primary key)\")\n\tif pgErr, ok := err.(*pgconn.PgError); !ok || pgErr.Code != \"25006\" {\n\t\tt.Errorf(\"Expected error SQLSTATE 25006, but got %#v\", err)\n\t}\n}\n\nfunc TestBeginTxBeginQuery(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttx, err := conn.BeginTx(ctx, pgx.TxOptions{BeginQuery: \"begin read only\"})\n\t\trequire.NoError(t, err)\n\t\tdefer tx.Rollback(ctx)\n\n\t\tvar readOnly bool\n\t\tconn.QueryRow(ctx, \"select current_setting('transaction_read_only')::bool\").Scan(&readOnly)\n\t\trequire.True(t, readOnly)\n\n\t\terr = tx.Rollback(ctx)\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestTxNestedTransactionCommit(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tcreateSql := `\n    create temporary table foo(\n      id integer,\n      unique (id)\n    );\n  `\n\n\tif _, err := conn.Exec(context.Background(), createSql); err != nil {\n\t\tt.Fatalf(\"Failed to create table: %v\", err)\n\t}\n\n\ttx, err := conn.Begin(context.Background())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t_, err = tx.Exec(context.Background(), \"insert into foo(id) values (1)\")\n\tif err != nil {\n\t\tt.Fatalf(\"tx.Exec failed: %v\", err)\n\t}\n\n\tnestedTx, err := tx.Begin(context.Background())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t_, err = nestedTx.Exec(context.Background(), \"insert into foo(id) values (2)\")\n\tif err != nil {\n\t\tt.Fatalf(\"nestedTx.Exec failed: %v\", err)\n\t}\n\n\tdoubleNestedTx, err := nestedTx.Begin(context.Background())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t_, err = doubleNestedTx.Exec(context.Background(), \"insert into foo(id) values (3)\")\n\tif err != nil {\n\t\tt.Fatalf(\"doubleNestedTx.Exec failed: %v\", err)\n\t}\n\n\terr = doubleNestedTx.Commit(context.Background())\n\tif err != nil {\n\t\tt.Fatalf(\"doubleNestedTx.Commit failed: %v\", err)\n\t}\n\n\terr = nestedTx.Commit(context.Background())\n\tif err != nil {\n\t\tt.Fatalf(\"nestedTx.Commit failed: %v\", err)\n\t}\n\n\terr = tx.Commit(context.Background())\n\tif err != nil {\n\t\tt.Fatalf(\"tx.Commit failed: %v\", err)\n\t}\n\n\tvar n int64\n\terr = conn.QueryRow(context.Background(), \"select count(*) from foo\").Scan(&n)\n\tif err != nil {\n\t\tt.Fatalf(\"QueryRow Scan failed: %v\", err)\n\t}\n\tif n != 3 {\n\t\tt.Fatalf(\"Did not receive correct number of rows: %v\", n)\n\t}\n}\n\nfunc TestTxNestedTransactionRollback(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tcreateSql := `\n    create temporary table foo(\n      id integer,\n      unique (id)\n    );\n  `\n\n\tif _, err := conn.Exec(context.Background(), createSql); err != nil {\n\t\tt.Fatalf(\"Failed to create table: %v\", err)\n\t}\n\n\ttx, err := conn.Begin(context.Background())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t_, err = tx.Exec(context.Background(), \"insert into foo(id) values (1)\")\n\tif err != nil {\n\t\tt.Fatalf(\"tx.Exec failed: %v\", err)\n\t}\n\n\tnestedTx, err := tx.Begin(context.Background())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t_, err = nestedTx.Exec(context.Background(), \"insert into foo(id) values (2)\")\n\tif err != nil {\n\t\tt.Fatalf(\"nestedTx.Exec failed: %v\", err)\n\t}\n\n\terr = nestedTx.Rollback(context.Background())\n\tif err != nil {\n\t\tt.Fatalf(\"nestedTx.Rollback failed: %v\", err)\n\t}\n\n\t_, err = tx.Exec(context.Background(), \"insert into foo(id) values (3)\")\n\tif err != nil {\n\t\tt.Fatalf(\"tx.Exec failed: %v\", err)\n\t}\n\n\terr = tx.Commit(context.Background())\n\tif err != nil {\n\t\tt.Fatalf(\"tx.Commit failed: %v\", err)\n\t}\n\n\tvar n int64\n\terr = conn.QueryRow(context.Background(), \"select count(*) from foo\").Scan(&n)\n\tif err != nil {\n\t\tt.Fatalf(\"QueryRow Scan failed: %v\", err)\n\t}\n\tif n != 2 {\n\t\tt.Fatalf(\"Did not receive correct number of rows: %v\", n)\n\t}\n}\n\nfunc TestTxBeginFuncNestedTransactionCommit(t *testing.T) {\n\tt.Parallel()\n\n\tdb := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, db)\n\n\tcreateSql := `\n    create temporary table foo(\n      id integer,\n      unique (id)\n    );\n  `\n\n\t_, err := db.Exec(context.Background(), createSql)\n\trequire.NoError(t, err)\n\n\terr = pgx.BeginFunc(context.Background(), db, func(db pgx.Tx) error {\n\t\t_, err := db.Exec(context.Background(), \"insert into foo(id) values (1)\")\n\t\trequire.NoError(t, err)\n\n\t\terr = pgx.BeginFunc(context.Background(), db, func(db pgx.Tx) error {\n\t\t\t_, err := db.Exec(context.Background(), \"insert into foo(id) values (2)\")\n\t\t\trequire.NoError(t, err)\n\n\t\t\terr = pgx.BeginFunc(context.Background(), db, func(db pgx.Tx) error {\n\t\t\t\t_, err := db.Exec(context.Background(), \"insert into foo(id) values (3)\")\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n\n\tvar n int64\n\terr = db.QueryRow(context.Background(), \"select count(*) from foo\").Scan(&n)\n\trequire.NoError(t, err)\n\trequire.EqualValues(t, 3, n)\n}\n\nfunc TestTxBeginFuncNestedTransactionRollback(t *testing.T) {\n\tt.Parallel()\n\n\tdb := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, db)\n\n\tcreateSql := `\n    create temporary table foo(\n      id integer,\n      unique (id)\n    );\n  `\n\n\t_, err := db.Exec(context.Background(), createSql)\n\trequire.NoError(t, err)\n\n\terr = pgx.BeginFunc(context.Background(), db, func(db pgx.Tx) error {\n\t\t_, err := db.Exec(context.Background(), \"insert into foo(id) values (1)\")\n\t\trequire.NoError(t, err)\n\n\t\terr = pgx.BeginFunc(context.Background(), db, func(db pgx.Tx) error {\n\t\t\t_, err := db.Exec(context.Background(), \"insert into foo(id) values (2)\")\n\t\t\trequire.NoError(t, err)\n\t\t\treturn errors.New(\"do a rollback\")\n\t\t})\n\t\trequire.EqualError(t, err, \"do a rollback\")\n\n\t\t_, err = db.Exec(context.Background(), \"insert into foo(id) values (3)\")\n\t\trequire.NoError(t, err)\n\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n\n\tvar n int64\n\terr = db.QueryRow(context.Background(), \"select count(*) from foo\").Scan(&n)\n\trequire.NoError(t, err)\n\trequire.EqualValues(t, 2, n)\n}\n\nfunc TestTxSendBatchClosed(t *testing.T) {\n\tt.Parallel()\n\n\tdb := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, db)\n\n\ttx, err := db.Begin(context.Background())\n\trequire.NoError(t, err)\n\tdefer tx.Rollback(context.Background())\n\n\terr = tx.Commit(context.Background())\n\trequire.NoError(t, err)\n\n\tbatch := &pgx.Batch{}\n\tbatch.Queue(\"select 1\")\n\tbatch.Queue(\"select 2\")\n\tbatch.Queue(\"select 3\")\n\n\tbr := tx.SendBatch(context.Background(), batch)\n\tdefer br.Close()\n\n\tvar n int\n\n\t_, err = br.Exec()\n\trequire.Error(t, err)\n\n\terr = br.QueryRow().Scan(&n)\n\trequire.Error(t, err)\n\n\t_, err = br.Query()\n\trequire.Error(t, err)\n}\n"
        },
        {
          "name": "values.go",
          "type": "blob",
          "size": 1.318359375,
          "content": "package pgx\n\nimport (\n\t\"errors\"\n\n\t\"github.com/jackc/pgx/v5/internal/pgio\"\n\t\"github.com/jackc/pgx/v5/pgtype\"\n)\n\n// PostgreSQL format codes\nconst (\n\tTextFormatCode   = 0\n\tBinaryFormatCode = 1\n)\n\nfunc convertSimpleArgument(m *pgtype.Map, arg any) (any, error) {\n\tbuf, err := m.Encode(0, TextFormatCode, arg, []byte{})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif buf == nil {\n\t\treturn nil, nil\n\t}\n\treturn string(buf), nil\n}\n\nfunc encodeCopyValue(m *pgtype.Map, buf []byte, oid uint32, arg any) ([]byte, error) {\n\tsp := len(buf)\n\tbuf = pgio.AppendInt32(buf, -1)\n\targBuf, err := m.Encode(oid, BinaryFormatCode, arg, buf)\n\tif err != nil {\n\t\tif argBuf2, err2 := tryScanStringCopyValueThenEncode(m, buf, oid, arg); err2 == nil {\n\t\t\targBuf = argBuf2\n\t\t} else {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif argBuf != nil {\n\t\tbuf = argBuf\n\t\tpgio.SetInt32(buf[sp:], int32(len(buf[sp:])-4))\n\t}\n\treturn buf, nil\n}\n\nfunc tryScanStringCopyValueThenEncode(m *pgtype.Map, buf []byte, oid uint32, arg any) ([]byte, error) {\n\ts, ok := arg.(string)\n\tif !ok {\n\t\ttextBuf, err := m.Encode(oid, TextFormatCode, arg, nil)\n\t\tif err != nil {\n\t\t\treturn nil, errors.New(\"not a string and cannot be encoded as text\")\n\t\t}\n\t\ts = string(textBuf)\n\t}\n\n\tvar v any\n\terr := m.Scan(oid, TextFormatCode, []byte(s), &v)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn m.Encode(oid, BinaryFormatCode, v, buf)\n}\n"
        },
        {
          "name": "values_test.go",
          "type": "blob",
          "size": 31.8828125,
          "content": "package pgx_test\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"net\"\n\t\"os\"\n\t\"reflect\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgxtest\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestDateTranscode(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tdates := []time.Time{\n\t\t\ttime.Date(1, 1, 1, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(1000, 1, 1, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(1600, 1, 1, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(1700, 1, 1, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(1800, 1, 1, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(1900, 1, 1, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(1990, 1, 1, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(1999, 12, 31, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2000, 1, 1, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2001, 1, 2, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2004, 2, 29, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2013, 7, 4, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2013, 12, 25, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2029, 1, 1, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2081, 1, 1, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2096, 2, 29, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2550, 1, 1, 0, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(9999, 12, 31, 0, 0, 0, 0, time.UTC),\n\t\t}\n\n\t\tfor _, actualDate := range dates {\n\t\t\tvar d time.Time\n\n\t\t\terr := conn.QueryRow(context.Background(), \"select $1::date\", actualDate).Scan(&d)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Unexpected failure on QueryRow Scan: %v\", err)\n\t\t\t}\n\t\t\tif !actualDate.Equal(d) {\n\t\t\t\tt.Errorf(\"Did not transcode date successfully: %v is not %v\", d, actualDate)\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestTimestampTzTranscode(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tinputTime := time.Date(2013, 1, 2, 3, 4, 5, 6000, time.Local)\n\n\t\tvar outputTime time.Time\n\n\t\terr := conn.QueryRow(context.Background(), \"select $1::timestamptz\", inputTime).Scan(&outputTime)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"QueryRow Scan failed: %v\", err)\n\t\t}\n\t\tif !inputTime.Equal(outputTime) {\n\t\t\tt.Errorf(\"Did not transcode time successfully: %v is not %v\", outputTime, inputTime)\n\t\t}\n\t})\n}\n\n// TODO - move these tests to pgtype\n\nfunc TestJSONAndJSONBTranscode(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tfor _, typename := range []string{\"json\", \"jsonb\"} {\n\t\t\tif _, ok := conn.TypeMap().TypeForName(typename); !ok {\n\t\t\t\tcontinue // No JSON/JSONB type -- must be running against old PostgreSQL\n\t\t\t}\n\n\t\t\ttestJSONString(t, conn, typename)\n\t\t\ttestJSONStringPointer(t, conn, typename)\n\t\t}\n\t})\n}\n\nfunc TestJSONAndJSONBTranscodeExtendedOnly(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\n\tfor _, typename := range []string{\"json\", \"jsonb\"} {\n\t\tif _, ok := conn.TypeMap().TypeForName(typename); !ok {\n\t\t\tcontinue // No JSON/JSONB type -- must be running against old PostgreSQL\n\t\t}\n\t\ttestJSONSingleLevelStringMap(t, conn, typename)\n\t\ttestJSONNestedMap(t, conn, typename)\n\t\ttestJSONStringArray(t, conn, typename)\n\t\ttestJSONInt64Array(t, conn, typename)\n\t\ttestJSONInt16ArrayFailureDueToOverflow(t, conn, typename)\n\t\ttestJSONStruct(t, conn, typename)\n\t}\n\n}\n\nfunc testJSONString(t testing.TB, conn *pgx.Conn, typename string) {\n\tinput := `{\"key\": \"value\"}`\n\texpectedOutput := map[string]string{\"key\": \"value\"}\n\tvar output map[string]string\n\terr := conn.QueryRow(context.Background(), \"select $1::\"+typename, input).Scan(&output)\n\tif err != nil {\n\t\tt.Errorf(\"%s: QueryRow Scan failed: %v\", typename, err)\n\t\treturn\n\t}\n\n\tif !reflect.DeepEqual(expectedOutput, output) {\n\t\tt.Errorf(\"%s: Did not transcode map[string]string successfully: %v is not %v\", typename, expectedOutput, output)\n\t\treturn\n\t}\n}\n\nfunc testJSONStringPointer(t testing.TB, conn *pgx.Conn, typename string) {\n\tinput := `{\"key\": \"value\"}`\n\texpectedOutput := map[string]string{\"key\": \"value\"}\n\tvar output map[string]string\n\terr := conn.QueryRow(context.Background(), \"select $1::\"+typename, &input).Scan(&output)\n\tif err != nil {\n\t\tt.Errorf(\"%s: QueryRow Scan failed: %v\", typename, err)\n\t\treturn\n\t}\n\n\tif !reflect.DeepEqual(expectedOutput, output) {\n\t\tt.Errorf(\"%s: Did not transcode map[string]string successfully: %v is not %v\", typename, expectedOutput, output)\n\t\treturn\n\t}\n}\n\nfunc testJSONSingleLevelStringMap(t *testing.T, conn *pgx.Conn, typename string) {\n\tinput := map[string]string{\"key\": \"value\"}\n\tvar output map[string]string\n\terr := conn.QueryRow(context.Background(), \"select $1::\"+typename, input).Scan(&output)\n\tif err != nil {\n\t\tt.Errorf(\"%s: QueryRow Scan failed: %v\", typename, err)\n\t\treturn\n\t}\n\n\tif !reflect.DeepEqual(input, output) {\n\t\tt.Errorf(\"%s: Did not transcode map[string]string successfully: %v is not %v\", typename, input, output)\n\t\treturn\n\t}\n}\n\nfunc testJSONNestedMap(t *testing.T, conn *pgx.Conn, typename string) {\n\tinput := map[string]any{\n\t\t\"name\":      \"Uncanny\",\n\t\t\"stats\":     map[string]any{\"hp\": float64(107), \"maxhp\": float64(150)},\n\t\t\"inventory\": []any{\"phone\", \"key\"},\n\t}\n\tvar output map[string]any\n\terr := conn.QueryRow(context.Background(), \"select $1::\"+typename, input).Scan(&output)\n\tif err != nil {\n\t\tt.Errorf(\"%s: QueryRow Scan failed: %v\", typename, err)\n\t\treturn\n\t}\n\n\tif !reflect.DeepEqual(input, output) {\n\t\tt.Errorf(\"%s: Did not transcode map[string]any successfully: %v is not %v\", typename, input, output)\n\t\treturn\n\t}\n}\n\nfunc testJSONStringArray(t *testing.T, conn *pgx.Conn, typename string) {\n\tinput := []string{\"foo\", \"bar\", \"baz\"}\n\tvar output []string\n\terr := conn.QueryRow(context.Background(), \"select $1::\"+typename, input).Scan(&output)\n\tif err != nil {\n\t\tt.Errorf(\"%s: QueryRow Scan failed: %v\", typename, err)\n\t}\n\n\tif !reflect.DeepEqual(input, output) {\n\t\tt.Errorf(\"%s: Did not transcode []string successfully: %v is not %v\", typename, input, output)\n\t}\n}\n\nfunc testJSONInt64Array(t *testing.T, conn *pgx.Conn, typename string) {\n\tinput := []int64{1, 2, 234432}\n\tvar output []int64\n\terr := conn.QueryRow(context.Background(), \"select $1::\"+typename, input).Scan(&output)\n\tif err != nil {\n\t\tt.Errorf(\"%s: QueryRow Scan failed: %v\", typename, err)\n\t}\n\n\tif !reflect.DeepEqual(input, output) {\n\t\tt.Errorf(\"%s: Did not transcode []int64 successfully: %v is not %v\", typename, input, output)\n\t}\n}\n\nfunc testJSONInt16ArrayFailureDueToOverflow(t *testing.T, conn *pgx.Conn, typename string) {\n\tinput := []int{1, 2, 234432}\n\tvar output []int16\n\terr := conn.QueryRow(context.Background(), \"select $1::\"+typename, input).Scan(&output)\n\tif err == nil || err.Error() != \"can't scan into dest[0]: json: cannot unmarshal number 234432 into Go value of type int16\" {\n\t\tt.Errorf(\"%s: Expected *json.UnmarshalTypeError, but got %v\", typename, err)\n\t}\n}\n\nfunc testJSONStruct(t *testing.T, conn *pgx.Conn, typename string) {\n\ttype person struct {\n\t\tName string `json:\"name\"`\n\t\tAge  int    `json:\"age\"`\n\t}\n\n\tinput := person{\n\t\tName: \"John\",\n\t\tAge:  42,\n\t}\n\n\tvar output person\n\n\terr := conn.QueryRow(context.Background(), \"select $1::\"+typename, input).Scan(&output)\n\tif err != nil {\n\t\tt.Errorf(\"%s: QueryRow Scan failed: %v\", typename, err)\n\t}\n\n\tif !reflect.DeepEqual(input, output) {\n\t\tt.Errorf(\"%s: Did not transcode struct successfully: %v is not %v\", typename, input, output)\n\t}\n}\n\nfunc mustParseCIDR(t testing.TB, s string) *net.IPNet {\n\t_, ipnet, err := net.ParseCIDR(s)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\treturn ipnet\n}\n\nfunc TestInetCIDRTranscodeIPNet(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttests := []struct {\n\t\t\tsql   string\n\t\t\tvalue *net.IPNet\n\t\t}{\n\t\t\t{\"select $1::inet\", mustParseCIDR(t, \"0.0.0.0/32\")},\n\t\t\t{\"select $1::inet\", mustParseCIDR(t, \"127.0.0.1/32\")},\n\t\t\t{\"select $1::inet\", mustParseCIDR(t, \"12.34.56.0/32\")},\n\t\t\t{\"select $1::inet\", mustParseCIDR(t, \"192.168.1.0/24\")},\n\t\t\t{\"select $1::inet\", mustParseCIDR(t, \"255.0.0.0/8\")},\n\t\t\t{\"select $1::inet\", mustParseCIDR(t, \"255.255.255.255/32\")},\n\t\t\t{\"select $1::inet\", mustParseCIDR(t, \"::/128\")},\n\t\t\t{\"select $1::inet\", mustParseCIDR(t, \"::/0\")},\n\t\t\t{\"select $1::inet\", mustParseCIDR(t, \"::1/128\")},\n\t\t\t{\"select $1::inet\", mustParseCIDR(t, \"2607:f8b0:4009:80b::200e/128\")},\n\t\t\t{\"select $1::cidr\", mustParseCIDR(t, \"0.0.0.0/32\")},\n\t\t\t{\"select $1::cidr\", mustParseCIDR(t, \"127.0.0.1/32\")},\n\t\t\t{\"select $1::cidr\", mustParseCIDR(t, \"12.34.56.0/32\")},\n\t\t\t{\"select $1::cidr\", mustParseCIDR(t, \"192.168.1.0/24\")},\n\t\t\t{\"select $1::cidr\", mustParseCIDR(t, \"255.0.0.0/8\")},\n\t\t\t{\"select $1::cidr\", mustParseCIDR(t, \"255.255.255.255/32\")},\n\t\t\t{\"select $1::cidr\", mustParseCIDR(t, \"::/128\")},\n\t\t\t{\"select $1::cidr\", mustParseCIDR(t, \"::/0\")},\n\t\t\t{\"select $1::cidr\", mustParseCIDR(t, \"::1/128\")},\n\t\t\t{\"select $1::cidr\", mustParseCIDR(t, \"2607:f8b0:4009:80b::200e/128\")},\n\t\t}\n\n\t\tfor i, tt := range tests {\n\t\t\tif conn.PgConn().ParameterStatus(\"crdb_version\") != \"\" && strings.Contains(tt.sql, \"cidr\") {\n\t\t\t\tt.Log(\"Server does not support cidr type (https://github.com/cockroachdb/cockroach/issues/18846)\")\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tvar actual net.IPNet\n\n\t\t\terr := conn.QueryRow(context.Background(), tt.sql, tt.value).Scan(&actual)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"%d. Unexpected failure: %v (sql -> %v, value -> %v)\", i, err, tt.sql, tt.value)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif actual.String() != tt.value.String() {\n\t\t\t\tt.Errorf(\"%d. Expected %v, got %v (sql -> %v)\", i, tt.value, actual, tt.sql)\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestInetCIDRTranscodeIP(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttests := []struct {\n\t\t\tsql   string\n\t\t\tvalue net.IP\n\t\t}{\n\t\t\t{\"select $1::inet\", net.ParseIP(\"0.0.0.0\")},\n\t\t\t{\"select $1::inet\", net.ParseIP(\"127.0.0.1\")},\n\t\t\t{\"select $1::inet\", net.ParseIP(\"12.34.56.0\")},\n\t\t\t{\"select $1::inet\", net.ParseIP(\"255.255.255.255\")},\n\t\t\t{\"select $1::inet\", net.ParseIP(\"::1\")},\n\t\t\t{\"select $1::inet\", net.ParseIP(\"2607:f8b0:4009:80b::200e\")},\n\t\t\t{\"select $1::cidr\", net.ParseIP(\"0.0.0.0\")},\n\t\t\t{\"select $1::cidr\", net.ParseIP(\"127.0.0.1\")},\n\t\t\t{\"select $1::cidr\", net.ParseIP(\"12.34.56.0\")},\n\t\t\t{\"select $1::cidr\", net.ParseIP(\"255.255.255.255\")},\n\t\t\t{\"select $1::cidr\", net.ParseIP(\"::1\")},\n\t\t\t{\"select $1::cidr\", net.ParseIP(\"2607:f8b0:4009:80b::200e\")},\n\t\t}\n\n\t\tfor i, tt := range tests {\n\t\t\tif conn.PgConn().ParameterStatus(\"crdb_version\") != \"\" && strings.Contains(tt.sql, \"cidr\") {\n\t\t\t\tt.Log(\"Server does not support cidr type (https://github.com/cockroachdb/cockroach/issues/18846)\")\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tvar actual net.IP\n\n\t\t\terr := conn.QueryRow(context.Background(), tt.sql, tt.value).Scan(&actual)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"%d. Unexpected failure: %v (sql -> %v, value -> %v)\", i, err, tt.sql, tt.value)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif !actual.Equal(tt.value) {\n\t\t\t\tt.Errorf(\"%d. Expected %v, got %v (sql -> %v)\", i, tt.value, actual, tt.sql)\n\t\t\t}\n\n\t\t\tensureConnValid(t, conn)\n\t\t}\n\n\t\tfailTests := []struct {\n\t\t\tsql   string\n\t\t\tvalue *net.IPNet\n\t\t}{\n\t\t\t{\"select $1::inet\", mustParseCIDR(t, \"192.168.1.0/24\")},\n\t\t\t{\"select $1::cidr\", mustParseCIDR(t, \"192.168.1.0/24\")},\n\t\t}\n\t\tfor i, tt := range failTests {\n\t\t\tvar actual net.IP\n\n\t\t\terr := conn.QueryRow(context.Background(), tt.sql, tt.value).Scan(&actual)\n\t\t\tif err == nil {\n\t\t\t\tt.Errorf(\"%d. Expected failure but got none\", i)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tensureConnValid(t, conn)\n\t\t}\n\t})\n}\n\nfunc TestInetCIDRArrayTranscodeIPNet(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttests := []struct {\n\t\t\tsql   string\n\t\t\tvalue []*net.IPNet\n\t\t}{\n\t\t\t{\n\t\t\t\t\"select $1::inet[]\",\n\t\t\t\t[]*net.IPNet{\n\t\t\t\t\tmustParseCIDR(t, \"0.0.0.0/32\"),\n\t\t\t\t\tmustParseCIDR(t, \"127.0.0.1/32\"),\n\t\t\t\t\tmustParseCIDR(t, \"12.34.56.0/32\"),\n\t\t\t\t\tmustParseCIDR(t, \"192.168.1.0/24\"),\n\t\t\t\t\tmustParseCIDR(t, \"255.0.0.0/8\"),\n\t\t\t\t\tmustParseCIDR(t, \"255.255.255.255/32\"),\n\t\t\t\t\tmustParseCIDR(t, \"::/128\"),\n\t\t\t\t\tmustParseCIDR(t, \"::/0\"),\n\t\t\t\t\tmustParseCIDR(t, \"::1/128\"),\n\t\t\t\t\tmustParseCIDR(t, \"2607:f8b0:4009:80b::200e/128\"),\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"select $1::cidr[]\",\n\t\t\t\t[]*net.IPNet{\n\t\t\t\t\tmustParseCIDR(t, \"0.0.0.0/32\"),\n\t\t\t\t\tmustParseCIDR(t, \"127.0.0.1/32\"),\n\t\t\t\t\tmustParseCIDR(t, \"12.34.56.0/32\"),\n\t\t\t\t\tmustParseCIDR(t, \"192.168.1.0/24\"),\n\t\t\t\t\tmustParseCIDR(t, \"255.0.0.0/8\"),\n\t\t\t\t\tmustParseCIDR(t, \"255.255.255.255/32\"),\n\t\t\t\t\tmustParseCIDR(t, \"::/128\"),\n\t\t\t\t\tmustParseCIDR(t, \"::/0\"),\n\t\t\t\t\tmustParseCIDR(t, \"::1/128\"),\n\t\t\t\t\tmustParseCIDR(t, \"2607:f8b0:4009:80b::200e/128\"),\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\n\t\tfor i, tt := range tests {\n\t\t\tif conn.PgConn().ParameterStatus(\"crdb_version\") != \"\" && strings.Contains(tt.sql, \"cidr\") {\n\t\t\t\tt.Log(\"Server does not support cidr type (https://github.com/cockroachdb/cockroach/issues/18846)\")\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tvar actual []*net.IPNet\n\n\t\t\terr := conn.QueryRow(context.Background(), tt.sql, tt.value).Scan(&actual)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"%d. Unexpected failure: %v (sql -> %v, value -> %v)\", i, err, tt.sql, tt.value)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif !reflect.DeepEqual(actual, tt.value) {\n\t\t\t\tt.Errorf(\"%d. Expected %v, got %v (sql -> %v)\", i, tt.value, actual, tt.sql)\n\t\t\t}\n\n\t\t\tensureConnValid(t, conn)\n\t\t}\n\t})\n}\n\nfunc TestInetCIDRArrayTranscodeIP(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttests := []struct {\n\t\t\tsql   string\n\t\t\tvalue []net.IP\n\t\t}{\n\t\t\t{\n\t\t\t\t\"select $1::inet[]\",\n\t\t\t\t[]net.IP{\n\t\t\t\t\tnet.ParseIP(\"0.0.0.0\"),\n\t\t\t\t\tnet.ParseIP(\"127.0.0.1\"),\n\t\t\t\t\tnet.ParseIP(\"12.34.56.0\"),\n\t\t\t\t\tnet.ParseIP(\"255.255.255.255\"),\n\t\t\t\t\tnet.ParseIP(\"2607:f8b0:4009:80b::200e\"),\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"select $1::cidr[]\",\n\t\t\t\t[]net.IP{\n\t\t\t\t\tnet.ParseIP(\"0.0.0.0\"),\n\t\t\t\t\tnet.ParseIP(\"127.0.0.1\"),\n\t\t\t\t\tnet.ParseIP(\"12.34.56.0\"),\n\t\t\t\t\tnet.ParseIP(\"255.255.255.255\"),\n\t\t\t\t\tnet.ParseIP(\"2607:f8b0:4009:80b::200e\"),\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\n\t\tfor i, tt := range tests {\n\t\t\tif conn.PgConn().ParameterStatus(\"crdb_version\") != \"\" && strings.Contains(tt.sql, \"cidr\") {\n\t\t\t\tt.Log(\"Server does not support cidr type (https://github.com/cockroachdb/cockroach/issues/18846)\")\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tvar actual []net.IP\n\n\t\t\terr := conn.QueryRow(context.Background(), tt.sql, tt.value).Scan(&actual)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"%d. Unexpected failure: %v (sql -> %v, value -> %v)\", i, err, tt.sql, tt.value)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tassert.Equal(t, len(tt.value), len(actual), \"%d\", i)\n\t\t\tfor j := range actual {\n\t\t\t\tassert.True(t, actual[j].Equal(tt.value[j]), \"%d\", i)\n\t\t\t}\n\n\t\t\tensureConnValid(t, conn)\n\t\t}\n\n\t\tfailTests := []struct {\n\t\t\tsql   string\n\t\t\tvalue []*net.IPNet\n\t\t}{\n\t\t\t{\n\t\t\t\t\"select $1::inet[]\",\n\t\t\t\t[]*net.IPNet{\n\t\t\t\t\tmustParseCIDR(t, \"12.34.56.0/32\"),\n\t\t\t\t\tmustParseCIDR(t, \"192.168.1.0/24\"),\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"select $1::cidr[]\",\n\t\t\t\t[]*net.IPNet{\n\t\t\t\t\tmustParseCIDR(t, \"12.34.56.0/32\"),\n\t\t\t\t\tmustParseCIDR(t, \"192.168.1.0/24\"),\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\n\t\tfor i, tt := range failTests {\n\t\t\tvar actual []net.IP\n\n\t\t\terr := conn.QueryRow(context.Background(), tt.sql, tt.value).Scan(&actual)\n\t\t\tif err == nil {\n\t\t\t\tt.Errorf(\"%d. Expected failure but got none\", i)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tensureConnValid(t, conn)\n\t\t}\n\t})\n}\n\nfunc TestInetCIDRTranscodeWithJustIP(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttests := []struct {\n\t\t\tsql   string\n\t\t\tvalue string\n\t\t}{\n\t\t\t{\"select $1::inet\", \"0.0.0.0/32\"},\n\t\t\t{\"select $1::inet\", \"127.0.0.1/32\"},\n\t\t\t{\"select $1::inet\", \"12.34.56.0/32\"},\n\t\t\t{\"select $1::inet\", \"255.255.255.255/32\"},\n\t\t\t{\"select $1::inet\", \"::/128\"},\n\t\t\t{\"select $1::inet\", \"2607:f8b0:4009:80b::200e/128\"},\n\t\t\t{\"select $1::cidr\", \"0.0.0.0/32\"},\n\t\t\t{\"select $1::cidr\", \"127.0.0.1/32\"},\n\t\t\t{\"select $1::cidr\", \"12.34.56.0/32\"},\n\t\t\t{\"select $1::cidr\", \"255.255.255.255/32\"},\n\t\t\t{\"select $1::cidr\", \"::/128\"},\n\t\t\t{\"select $1::cidr\", \"2607:f8b0:4009:80b::200e/128\"},\n\t\t}\n\n\t\tfor i, tt := range tests {\n\t\t\tif conn.PgConn().ParameterStatus(\"crdb_version\") != \"\" && strings.Contains(tt.sql, \"cidr\") {\n\t\t\t\tt.Log(\"Server does not support cidr type (https://github.com/cockroachdb/cockroach/issues/18846)\")\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\texpected := mustParseCIDR(t, tt.value)\n\t\t\tvar actual net.IPNet\n\n\t\t\terr := conn.QueryRow(context.Background(), tt.sql, expected.IP).Scan(&actual)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"%d. Unexpected failure: %v (sql -> %v, value -> %v)\", i, err, tt.sql, tt.value)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif actual.String() != expected.String() {\n\t\t\t\tt.Errorf(\"%d. Expected %v, got %v (sql -> %v)\", i, tt.value, actual, tt.sql)\n\t\t\t}\n\n\t\t\tensureConnValid(t, conn)\n\t\t}\n\t})\n}\n\nfunc TestArrayDecoding(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttests := []struct {\n\t\t\tsql    string\n\t\t\tquery  any\n\t\t\tscan   any\n\t\t\tassert func(testing.TB, any, any)\n\t\t}{\n\t\t\t{\n\t\t\t\t\"select $1::bool[]\", []bool{true, false, true}, &[]bool{},\n\t\t\t\tfunc(t testing.TB, query, scan any) {\n\t\t\t\t\tif !reflect.DeepEqual(query, *(scan.(*[]bool))) {\n\t\t\t\t\t\tt.Errorf(\"failed to encode bool[]\")\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"select $1::smallint[]\", []int16{2, 4, 484, 32767}, &[]int16{},\n\t\t\t\tfunc(t testing.TB, query, scan any) {\n\t\t\t\t\tif !reflect.DeepEqual(query, *(scan.(*[]int16))) {\n\t\t\t\t\t\tt.Errorf(\"failed to encode smallint[]\")\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"select $1::smallint[]\", []uint16{2, 4, 484, 32767}, &[]uint16{},\n\t\t\t\tfunc(t testing.TB, query, scan any) {\n\t\t\t\t\tif !reflect.DeepEqual(query, *(scan.(*[]uint16))) {\n\t\t\t\t\t\tt.Errorf(\"failed to encode smallint[]\")\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"select $1::int[]\", []int32{2, 4, 484}, &[]int32{},\n\t\t\t\tfunc(t testing.TB, query, scan any) {\n\t\t\t\t\tif !reflect.DeepEqual(query, *(scan.(*[]int32))) {\n\t\t\t\t\t\tt.Errorf(\"failed to encode int[]\")\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"select $1::int[]\", []uint32{2, 4, 484, 2147483647}, &[]uint32{},\n\t\t\t\tfunc(t testing.TB, query, scan any) {\n\t\t\t\t\tif !reflect.DeepEqual(query, *(scan.(*[]uint32))) {\n\t\t\t\t\t\tt.Errorf(\"failed to encode int[]\")\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"select $1::bigint[]\", []int64{2, 4, 484, 9223372036854775807}, &[]int64{},\n\t\t\t\tfunc(t testing.TB, query, scan any) {\n\t\t\t\t\tif !reflect.DeepEqual(query, *(scan.(*[]int64))) {\n\t\t\t\t\t\tt.Errorf(\"failed to encode bigint[]\")\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"select $1::bigint[]\", []uint64{2, 4, 484, 9223372036854775807}, &[]uint64{},\n\t\t\t\tfunc(t testing.TB, query, scan any) {\n\t\t\t\t\tif !reflect.DeepEqual(query, *(scan.(*[]uint64))) {\n\t\t\t\t\t\tt.Errorf(\"failed to encode bigint[]\")\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"select $1::text[]\", []string{\"it's\", \"over\", \"9000!\"}, &[]string{},\n\t\t\t\tfunc(t testing.TB, query, scan any) {\n\t\t\t\t\tif !reflect.DeepEqual(query, *(scan.(*[]string))) {\n\t\t\t\t\t\tt.Errorf(\"failed to encode text[]\")\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"select $1::timestamptz[]\", []time.Time{time.Unix(323232, 0), time.Unix(3239949334, 00)}, &[]time.Time{},\n\t\t\t\tfunc(t testing.TB, query, scan any) {\n\t\t\t\t\tqueryTimeSlice := query.([]time.Time)\n\t\t\t\t\tscanTimeSlice := *(scan.(*[]time.Time))\n\t\t\t\t\trequire.Equal(t, len(queryTimeSlice), len(scanTimeSlice))\n\t\t\t\t\tfor i := range queryTimeSlice {\n\t\t\t\t\t\tassert.Truef(t, queryTimeSlice[i].Equal(scanTimeSlice[i]), \"%d\", i)\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"select $1::bytea[]\", [][]byte{{0, 1, 2, 3}, {4, 5, 6, 7}}, &[][]byte{},\n\t\t\t\tfunc(t testing.TB, query, scan any) {\n\t\t\t\t\tqueryBytesSliceSlice := query.([][]byte)\n\t\t\t\t\tscanBytesSliceSlice := *(scan.(*[][]byte))\n\t\t\t\t\tif len(queryBytesSliceSlice) != len(scanBytesSliceSlice) {\n\t\t\t\t\t\tt.Errorf(\"failed to encode byte[][] to bytea[]: expected %d to equal %d\", len(queryBytesSliceSlice), len(scanBytesSliceSlice))\n\t\t\t\t\t}\n\t\t\t\t\tfor i := range queryBytesSliceSlice {\n\t\t\t\t\t\tqb := queryBytesSliceSlice[i]\n\t\t\t\t\t\tsb := scanBytesSliceSlice[i]\n\t\t\t\t\t\tif !bytes.Equal(qb, sb) {\n\t\t\t\t\t\t\tt.Errorf(\"failed to encode byte[][] to bytea[]: expected %v to equal %v\", qb, sb)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\n\t\tfor i, tt := range tests {\n\t\t\terr := conn.QueryRow(context.Background(), tt.sql, tt.query).Scan(tt.scan)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(`%d. error reading array: %v`, i, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttt.assert(t, tt.query, tt.scan)\n\t\t\tensureConnValid(t, conn)\n\t\t}\n\t})\n}\n\nfunc TestEmptyArrayDecoding(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tvar val []string\n\n\t\terr := conn.QueryRow(context.Background(), \"select array[]::text[]\").Scan(&val)\n\t\tif err != nil {\n\t\t\tt.Errorf(`error reading array: %v`, err)\n\t\t}\n\t\tif len(val) != 0 {\n\t\t\tt.Errorf(\"Expected 0 values, got %d\", len(val))\n\t\t}\n\n\t\tvar n, m int32\n\n\t\terr = conn.QueryRow(context.Background(), \"select 1::integer, array[]::text[], 42::integer\").Scan(&n, &val, &m)\n\t\tif err != nil {\n\t\t\tt.Errorf(`error reading array: %v`, err)\n\t\t}\n\t\tif len(val) != 0 {\n\t\t\tt.Errorf(\"Expected 0 values, got %d\", len(val))\n\t\t}\n\t\tif n != 1 {\n\t\t\tt.Errorf(\"Expected n to be 1, but it was %d\", n)\n\t\t}\n\t\tif m != 42 {\n\t\t\tt.Errorf(\"Expected n to be 42, but it was %d\", n)\n\t\t}\n\n\t\trows, err := conn.Query(context.Background(), \"select 1::integer, array['test']::text[] union select 2::integer, array[]::text[] union select 3::integer, array['test']::text[]\")\n\t\tif err != nil {\n\t\t\tt.Errorf(`error retrieving rows with array: %v`, err)\n\t\t}\n\t\tdefer rows.Close()\n\n\t\tfor rows.Next() {\n\t\t\terr = rows.Scan(&n, &val)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(`error reading array: %v`, err)\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestPointerPointer(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tpgxtest.SkipCockroachDB(t, conn, \"Server auto converts ints to bigint and test relies on exact types\")\n\n\t\ttype allTypes struct {\n\t\t\ts   *string\n\t\t\ti16 *int16\n\t\t\ti32 *int32\n\t\t\ti64 *int64\n\t\t\tf32 *float32\n\t\t\tf64 *float64\n\t\t\tb   *bool\n\t\t\tt   *time.Time\n\t\t}\n\n\t\tvar actual, zero, expected allTypes\n\n\t\t{\n\t\t\ts := \"foo\"\n\t\t\texpected.s = &s\n\t\t\ti16 := int16(1)\n\t\t\texpected.i16 = &i16\n\t\t\ti32 := int32(1)\n\t\t\texpected.i32 = &i32\n\t\t\ti64 := int64(1)\n\t\t\texpected.i64 = &i64\n\t\t\tf32 := float32(1.23)\n\t\t\texpected.f32 = &f32\n\t\t\tf64 := float64(1.23)\n\t\t\texpected.f64 = &f64\n\t\t\tb := true\n\t\t\texpected.b = &b\n\t\t\tt := time.Unix(123, 5000)\n\t\t\texpected.t = &t\n\t\t}\n\n\t\ttests := []struct {\n\t\t\tsql       string\n\t\t\tqueryArgs []any\n\t\t\tscanArgs  []any\n\t\t\texpected  allTypes\n\t\t}{\n\t\t\t{\"select $1::text\", []any{expected.s}, []any{&actual.s}, allTypes{s: expected.s}},\n\t\t\t{\"select $1::text\", []any{zero.s}, []any{&actual.s}, allTypes{}},\n\t\t\t{\"select $1::int2\", []any{expected.i16}, []any{&actual.i16}, allTypes{i16: expected.i16}},\n\t\t\t{\"select $1::int2\", []any{zero.i16}, []any{&actual.i16}, allTypes{}},\n\t\t\t{\"select $1::int4\", []any{expected.i32}, []any{&actual.i32}, allTypes{i32: expected.i32}},\n\t\t\t{\"select $1::int4\", []any{zero.i32}, []any{&actual.i32}, allTypes{}},\n\t\t\t{\"select $1::int8\", []any{expected.i64}, []any{&actual.i64}, allTypes{i64: expected.i64}},\n\t\t\t{\"select $1::int8\", []any{zero.i64}, []any{&actual.i64}, allTypes{}},\n\t\t\t{\"select $1::float4\", []any{expected.f32}, []any{&actual.f32}, allTypes{f32: expected.f32}},\n\t\t\t{\"select $1::float4\", []any{zero.f32}, []any{&actual.f32}, allTypes{}},\n\t\t\t{\"select $1::float8\", []any{expected.f64}, []any{&actual.f64}, allTypes{f64: expected.f64}},\n\t\t\t{\"select $1::float8\", []any{zero.f64}, []any{&actual.f64}, allTypes{}},\n\t\t\t{\"select $1::bool\", []any{expected.b}, []any{&actual.b}, allTypes{b: expected.b}},\n\t\t\t{\"select $1::bool\", []any{zero.b}, []any{&actual.b}, allTypes{}},\n\t\t\t{\"select $1::timestamptz\", []any{expected.t}, []any{&actual.t}, allTypes{t: expected.t}},\n\t\t\t{\"select $1::timestamptz\", []any{zero.t}, []any{&actual.t}, allTypes{}},\n\t\t}\n\n\t\tfor i, tt := range tests {\n\t\t\tactual = zero\n\n\t\t\terr := conn.QueryRow(context.Background(), tt.sql, tt.queryArgs...).Scan(tt.scanArgs...)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"%d. Unexpected failure: %v (sql -> %v, queryArgs -> %v)\", i, err, tt.sql, tt.queryArgs)\n\t\t\t}\n\n\t\t\tassert.Equal(t, tt.expected.s, actual.s)\n\t\t\tassert.Equal(t, tt.expected.i16, actual.i16)\n\t\t\tassert.Equal(t, tt.expected.i32, actual.i32)\n\t\t\tassert.Equal(t, tt.expected.i64, actual.i64)\n\t\t\tassert.Equal(t, tt.expected.f32, actual.f32)\n\t\t\tassert.Equal(t, tt.expected.f64, actual.f64)\n\t\t\tassert.Equal(t, tt.expected.b, actual.b)\n\t\t\tif tt.expected.t != nil || actual.t != nil {\n\t\t\t\tassert.True(t, tt.expected.t.Equal(*actual.t))\n\t\t\t}\n\n\t\t\tensureConnValid(t, conn)\n\t\t}\n\t})\n}\n\nfunc TestPointerPointerNonZero(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tf := \"foo\"\n\t\tdest := &f\n\n\t\terr := conn.QueryRow(context.Background(), \"select $1::text\", nil).Scan(&dest)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Unexpected failure scanning: %v\", err)\n\t\t}\n\t\tif dest != nil {\n\t\t\tt.Errorf(\"Expected dest to be nil, got %#v\", dest)\n\t\t}\n\t})\n}\n\nfunc TestEncodeTypeRename(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\ttype _int int\n\t\tinInt := _int(1)\n\t\tvar outInt _int\n\n\t\ttype _int8 int8\n\t\tinInt8 := _int8(2)\n\t\tvar outInt8 _int8\n\n\t\ttype _int16 int16\n\t\tinInt16 := _int16(3)\n\t\tvar outInt16 _int16\n\n\t\ttype _int32 int32\n\t\tinInt32 := _int32(4)\n\t\tvar outInt32 _int32\n\n\t\ttype _int64 int64\n\t\tinInt64 := _int64(5)\n\t\tvar outInt64 _int64\n\n\t\ttype _uint uint\n\t\tinUint := _uint(6)\n\t\tvar outUint _uint\n\n\t\ttype _uint8 uint8\n\t\tinUint8 := _uint8(7)\n\t\tvar outUint8 _uint8\n\n\t\ttype _uint16 uint16\n\t\tinUint16 := _uint16(8)\n\t\tvar outUint16 _uint16\n\n\t\ttype _uint32 uint32\n\t\tinUint32 := _uint32(9)\n\t\tvar outUint32 _uint32\n\n\t\ttype _uint64 uint64\n\t\tinUint64 := _uint64(10)\n\t\tvar outUint64 _uint64\n\n\t\ttype _string string\n\t\tinString := _string(\"foo\")\n\t\tvar outString _string\n\n\t\ttype _bool bool\n\t\tinBool := _bool(true)\n\t\tvar outBool _bool\n\n\t\t// pgx.QueryExecModeExec requires all types to be registered.\n\t\tconn.TypeMap().RegisterDefaultPgType(inInt, \"int8\")\n\t\tconn.TypeMap().RegisterDefaultPgType(inInt8, \"int8\")\n\t\tconn.TypeMap().RegisterDefaultPgType(inInt16, \"int8\")\n\t\tconn.TypeMap().RegisterDefaultPgType(inInt32, \"int8\")\n\t\tconn.TypeMap().RegisterDefaultPgType(inInt64, \"int8\")\n\t\tconn.TypeMap().RegisterDefaultPgType(inUint, \"int8\")\n\t\tconn.TypeMap().RegisterDefaultPgType(inUint8, \"int8\")\n\t\tconn.TypeMap().RegisterDefaultPgType(inUint16, \"int8\")\n\t\tconn.TypeMap().RegisterDefaultPgType(inUint32, \"int8\")\n\t\tconn.TypeMap().RegisterDefaultPgType(inUint64, \"int8\")\n\t\tconn.TypeMap().RegisterDefaultPgType(inString, \"text\")\n\t\tconn.TypeMap().RegisterDefaultPgType(inBool, \"bool\")\n\n\t\terr := conn.QueryRow(context.Background(), \"select $1::int, $2::int, $3::int2, $4::int4, $5::int8, $6::int, $7::int, $8::int, $9::int, $10::int, $11::text, $12::bool\",\n\t\t\tinInt, inInt8, inInt16, inInt32, inInt64, inUint, inUint8, inUint16, inUint32, inUint64, inString, inBool,\n\t\t).Scan(&outInt, &outInt8, &outInt16, &outInt32, &outInt64, &outUint, &outUint8, &outUint16, &outUint32, &outUint64, &outString, &outBool)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"Failed with type rename: %v\", err)\n\t\t}\n\n\t\tif inInt != outInt {\n\t\t\tt.Errorf(\"int rename: expected %v, got %v\", inInt, outInt)\n\t\t}\n\n\t\tif inInt8 != outInt8 {\n\t\t\tt.Errorf(\"int8 rename: expected %v, got %v\", inInt8, outInt8)\n\t\t}\n\n\t\tif inInt16 != outInt16 {\n\t\t\tt.Errorf(\"int16 rename: expected %v, got %v\", inInt16, outInt16)\n\t\t}\n\n\t\tif inInt32 != outInt32 {\n\t\t\tt.Errorf(\"int32 rename: expected %v, got %v\", inInt32, outInt32)\n\t\t}\n\n\t\tif inInt64 != outInt64 {\n\t\t\tt.Errorf(\"int64 rename: expected %v, got %v\", inInt64, outInt64)\n\t\t}\n\n\t\tif inUint != outUint {\n\t\t\tt.Errorf(\"uint rename: expected %v, got %v\", inUint, outUint)\n\t\t}\n\n\t\tif inUint8 != outUint8 {\n\t\t\tt.Errorf(\"uint8 rename: expected %v, got %v\", inUint8, outUint8)\n\t\t}\n\n\t\tif inUint16 != outUint16 {\n\t\t\tt.Errorf(\"uint16 rename: expected %v, got %v\", inUint16, outUint16)\n\t\t}\n\n\t\tif inUint32 != outUint32 {\n\t\t\tt.Errorf(\"uint32 rename: expected %v, got %v\", inUint32, outUint32)\n\t\t}\n\n\t\tif inUint64 != outUint64 {\n\t\t\tt.Errorf(\"uint64 rename: expected %v, got %v\", inUint64, outUint64)\n\t\t}\n\n\t\tif inString != outString {\n\t\t\tt.Errorf(\"string rename: expected %v, got %v\", inString, outString)\n\t\t}\n\n\t\tif inBool != outBool {\n\t\t\tt.Errorf(\"bool rename: expected %v, got %v\", inBool, outBool)\n\t\t}\n\t})\n}\n\n// func TestRowDecodeBinary(t *testing.T) {\n// \tt.Parallel()\n\n// \tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n// \tdefer closeConn(t, conn)\n\n// \ttests := []struct {\n// \t\tsql      string\n// \t\texpected []any\n// \t}{\n// \t\t{\n// \t\t\t\"select row(1, 'cat', '2015-01-01 08:12:42-00'::timestamptz)\",\n// \t\t\t[]any{\n// \t\t\t\tint32(1),\n// \t\t\t\t\"cat\",\n// \t\t\t\ttime.Date(2015, 1, 1, 8, 12, 42, 0, time.UTC).Local(),\n// \t\t\t},\n// \t\t},\n// \t\t{\n// \t\t\t\"select row(100.0::float, 1.09::float)\",\n// \t\t\t[]any{\n// \t\t\t\tfloat64(100),\n// \t\t\t\tfloat64(1.09),\n// \t\t\t},\n// \t\t},\n// \t}\n\n// \tfor i, tt := range tests {\n// \t\tvar actual []any\n\n// \t\terr := conn.QueryRow(context.Background(), tt.sql).Scan(&actual)\n// \t\tif err != nil {\n// \t\t\tt.Errorf(\"%d. Unexpected failure: %v (sql -> %v)\", i, err, tt.sql)\n// \t\t\tcontinue\n// \t\t}\n\n// \t\tfor j := range tt.expected {\n// \t\t\tassert.EqualValuesf(t, tt.expected[j], actual[j], \"%d. [%d]\", i, j)\n\n// \t\t}\n\n// \t\tensureConnValid(t, conn)\n// \t}\n// }\n\n// https://github.com/jackc/pgx/issues/810\nfunc TestRowsScanNilThenScanValue(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\n\tpgxtest.RunWithQueryExecModes(ctx, t, defaultConnTestRunner, nil, func(ctx context.Context, t testing.TB, conn *pgx.Conn) {\n\t\tsql := `select null as a, null as b\nunion\nselect 1, 2\norder by a nulls first\n`\n\t\trows, err := conn.Query(context.Background(), sql)\n\t\trequire.NoError(t, err)\n\n\t\trequire.True(t, rows.Next())\n\n\t\terr = rows.Scan(nil, nil)\n\t\trequire.NoError(t, err)\n\n\t\trequire.True(t, rows.Next())\n\n\t\tvar a int\n\t\tvar b int\n\t\terr = rows.Scan(&a, &b)\n\t\trequire.NoError(t, err)\n\n\t\trequire.EqualValues(t, 1, a)\n\t\trequire.EqualValues(t, 2, b)\n\n\t\trows.Close()\n\t\trequire.NoError(t, rows.Err())\n\t})\n}\n\nfunc TestScanIntoByteSlice(t *testing.T) {\n\tt.Parallel()\n\n\tconn := mustConnectString(t, os.Getenv(\"PGX_TEST_DATABASE\"))\n\tdefer closeConn(t, conn)\n\t// Success cases\n\tfor _, tt := range []struct {\n\t\tname             string\n\t\tsql              string\n\t\tresultFormatCode int16\n\t\toutput           []byte\n\t}{\n\t\t{\"int - text\", \"select 42\", pgx.TextFormatCode, []byte(\"42\")},\n\t\t{\"int - binary\", \"select 42\", pgx.BinaryFormatCode, []byte(\"42\")},\n\t\t{\"text - text\", \"select 'hi'\", pgx.TextFormatCode, []byte(\"hi\")},\n\t\t{\"text - binary\", \"select 'hi'\", pgx.BinaryFormatCode, []byte(\"hi\")},\n\t\t{\"json - text\", \"select '{}'::json\", pgx.TextFormatCode, []byte(\"{}\")},\n\t\t{\"json - binary\", \"select '{}'::json\", pgx.BinaryFormatCode, []byte(\"{}\")},\n\t\t{\"jsonb - text\", \"select '{}'::jsonb\", pgx.TextFormatCode, []byte(\"{}\")},\n\t\t{\"jsonb - binary\", \"select '{}'::jsonb\", pgx.BinaryFormatCode, []byte(\"{}\")},\n\t} {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tvar buf []byte\n\t\t\terr := conn.QueryRow(context.Background(), tt.sql, pgx.QueryResultFormats{tt.resultFormatCode}).Scan(&buf)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, tt.output, buf)\n\t\t})\n\t}\n}\n"
        }
      ]
    }
  ]
}