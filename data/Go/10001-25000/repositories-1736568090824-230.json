{
  "metadata": {
    "timestamp": 1736568090824,
    "page": 230,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "hibiken/asynq",
      "stars": 10445,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3916015625,
          "content": "vendor\n# Binaries for programs and plugins\n*.exe\n*.exe~\n*.dll\n*.so\n*.dylib\n\n# Test binary, built with `go test -c`\n*.test\n\n# Output of the go coverage tool, specifically when used with LiteIDE\n*.out\n\n# Ignore examples for now\n/examples\n\n# Ignore tool binaries\n/tools/asynq/asynq\n/tools/metrics_exporter/metrics_exporter\n\n# Ignore asynq config file\n.asynq.*\n\n# Ignore editor config files\n.vscode\n.idea\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 20.0859375,
          "content": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [\"Keep a Changelog\"](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n## [0.25.1] - 2024-12-11\n\n### Upgrades\n\n* Some packages\n\n### Added\n\n* Add `HeartbeatInterval` option to the scheduler (PR: https://github.com/hibiken/asynq/pull/956)\n* Add `RedisUniversalClient` support to periodic task manager (PR: https://github.com/hibiken/asynq/pull/958)\n* Add `--insecure` flag to CLI dash command (PR: https://github.com/hibiken/asynq/pull/980)\n* Add logging for registration errors (PR: https://github.com/hibiken/asynq/pull/657)\n\n### Fixes\n- Perf: Use string concat inplace of fmt.Sprintf in hotpath (PR: https://github.com/hibiken/asynq/pull/962)\n- Perf: Init map with size (PR: https://github.com/hibiken/asynq/pull/673)\n- Fix: `Scheduler` and `PeriodicTaskManager` graceful shutdown (PR: https://github.com/hibiken/asynq/pull/977)\n- Fix: `Server` graceful shutdown on UNIX systems (PR: https://github.com/hibiken/asynq/pull/982)\n\n## [0.25.0] - 2024-10-29\n\n### Upgrades\n- Minumum go version is set to 1.22 (PR: https://github.com/hibiken/asynq/pull/925)\n- Internal protobuf package is upgraded to address security advisories (PR: https://github.com/hibiken/asynq/pull/925)\n- Most packages are upgraded\n- CI/CD spec upgraded\n\n### Added\n- `IsPanicError` function is introduced to support catching of panic errors when processing tasks (PR: https://github.com/hibiken/asynq/pull/491)\n- `JanitorInterval` and `JanitorBatchSize` are added as Server options (PR: https://github.com/hibiken/asynq/pull/715)\n- `NewClientFromRedisClient` is introduced to allow reusing an existing redis client (PR: https://github.com/hibiken/asynq/pull/742)\n- `TaskCheckInterval` config option is added to specify the interval between checks for new tasks to process when all queues are empty (PR: https://github.com/hibiken/asynq/pull/694)\n- `Ping` method is added to Client, Server and Scheduler ((PR: https://github.com/hibiken/asynq/pull/585))\n- `RevokeTask` error type is introduced to prevent a task from being retried or archived (PR: https://github.com/hibiken/asynq/pull/882)\n- `SentinelUsername` is added as a redis config option (PR: https://github.com/hibiken/asynq/pull/924)\n- Some jitter is introduced to improve latency when fetching jobs in the processor (PR: https://github.com/hibiken/asynq/pull/868)\n- Add task enqueue command to the CLI (PR: https://github.com/hibiken/asynq/pull/918)\n- Add a map cache (concurrent safe) to keep track of queues that ultimately reduces redis load when enqueuing tasks (PR: https://github.com/hibiken/asynq/pull/946)\n\n### Fixes\n- Archived tasks that are trimmed should now be deleted (PR: https://github.com/hibiken/asynq/pull/743)\n- Fix lua script when listing task messages with an expired lease (PR: https://github.com/hibiken/asynq/pull/709)\n- Fix potential context leaks due to cancellation not being called (PR: https://github.com/hibiken/asynq/pull/926)\n- Misc documentation fixes\n- Misc test fixes\n\n\n## [0.24.1] - 2023-05-01\n\n### Changed\n- Updated package version dependency for go-redis \n\n## [0.24.0] - 2023-01-02\n\n### Added\n- `PreEnqueueFunc`, `PostEnqueueFunc` is added in `Scheduler` and deprecated `EnqueueErrorHandler` (PR: https://github.com/hibiken/asynq/pull/476)\n\n### Changed\n- Removed error log when `Scheduler` failed to enqueue a task. Use `PostEnqueueFunc` to check for errors and task actions if needed.\n- Changed log level from ERROR to WARNINING when `Scheduler` failed to record `SchedulerEnqueueEvent`.\n\n## [0.23.0] - 2022-04-11\n\n### Added\n\n- `Group` option is introduced to enqueue task in a group.\n- `GroupAggregator` and related types are introduced for task aggregation feature.\n- `GroupGracePeriod`, `GroupMaxSize`, `GroupMaxDelay`, and `GroupAggregator` fields are added to `Config`.\n- `Inspector` has new methods related to \"aggregating tasks\".\n- `Group` field is added to `TaskInfo`.\n- (CLI): `group ls` command is added\n- (CLI): `task ls` supports listing aggregating tasks via `--state=aggregating --group=<GROUP>` flags\n- Enable rediss url parsing support\n\n### Fixed\n\n- Fixed overflow issue with 32-bit systems (For details, see https://github.com/hibiken/asynq/pull/426)\n\n## [0.22.1] - 2022-02-20\n\n### Fixed\n\n- Fixed Redis version compatibility: Keep support for redis v4.0+\n\n## [0.22.0] - 2022-02-19\n\n### Added\n\n- `BaseContext` is introduced in `Config` to specify callback hook to provide a base `context` from which `Handler` `context` is derived\n- `IsOrphaned` field is added to `TaskInfo` to describe a task left in active state with no worker processing it.\n\n### Changed\n\n- `Server` now recovers tasks with an expired lease. Recovered tasks are retried/archived with `ErrLeaseExpired` error.\n\n## [0.21.0] - 2022-01-22\n\n### Added\n\n- `PeriodicTaskManager` is added. Prefer using this over `Scheduler` as it has better support for dynamic periodic tasks.\n- The `asynq stats` command now supports a `--json` option, making its output a JSON object\n- Introduced new configuration for `DelayedTaskCheckInterval`. See [godoc](https://godoc.org/github.com/hibiken/asynq) for more details.\n\n## [0.20.0] - 2021-12-19\n\n### Added\n\n- Package `x/metrics` is added.\n- Tool `tools/metrics_exporter` binary is added.\n- `ProcessedTotal` and `FailedTotal` fields were added to `QueueInfo` struct.\n\n## [0.19.1] - 2021-12-12\n\n### Added\n\n- `Latency` field is added to `QueueInfo`.\n- `EnqueueContext` method is added to `Client`.\n\n### Fixed\n\n- Fixed an error when user pass a duration less than 1s to `Unique` option\n\n## [0.19.0] - 2021-11-06\n\n### Changed\n\n- `NewTask` takes `Option` as variadic argument\n- Bumped minimum supported go version to 1.14 (i.e. go1.14 or higher is required).\n\n### Added\n\n- `Retention` option is added to allow user to specify task retention duration after completion.\n- `TaskID` option is added to allow user to specify task ID.\n- `ErrTaskIDConflict` sentinel error value is added.\n- `ResultWriter` type is added and provided through `Task.ResultWriter` method.\n- `TaskInfo` has new fields `CompletedAt`, `Result` and `Retention`.\n\n### Removed\n\n- `Client.SetDefaultOptions` is removed. Use `NewTask` instead to pass default options for tasks.\n\n## [0.18.6] - 2021-10-03\n\n### Changed\n\n- Updated `github.com/go-redis/redis` package to v8\n\n## [0.18.5] - 2021-09-01\n\n### Added\n\n- `IsFailure` config option is added to determine whether error returned from Handler counts as a failure.\n\n## [0.18.4] - 2021-08-17\n\n### Fixed\n\n- Scheduler methods are now thread-safe. It's now safe to call `Register` and `Unregister` concurrently.\n\n## [0.18.3] - 2021-08-09\n\n### Changed\n\n- `Client.Enqueue` no longer enqueues tasks with empty typename; Error message is returned.\n\n## [0.18.2] - 2021-07-15\n\n### Changed\n\n- Changed `Queue` function to not to convert the provided queue name to lowercase. Queue names are now case-sensitive.\n- `QueueInfo.MemoryUsage` is now an approximate usage value.\n\n### Fixed\n\n- Fixed latency issue around memory usage (see https://github.com/hibiken/asynq/issues/309).\n\n## [0.18.1] - 2021-07-04\n\n### Changed\n\n- Changed to execute task recovering logic when server starts up; Previously it needed to wait for a minute for task recovering logic to exeucte.\n\n### Fixed\n\n- Fixed task recovering logic to execute every minute\n\n## [0.18.0] - 2021-06-29\n\n### Changed\n\n- NewTask function now takes array of bytes as payload.\n- Task `Type` and `Payload` should be accessed by a method call.\n- `Server` API has changed. Renamed `Quiet` to `Stop`. Renamed `Stop` to `Shutdown`. _Note:_ As a result of this renaming, the behavior of `Stop` has changed. Please update the exising code to call `Shutdown` where it used to call `Stop`.\n- `Scheduler` API has changed. Renamed `Stop` to `Shutdown`.\n- Requires redis v4.0+ for multiple field/value pair support\n- `Client.Enqueue` now returns `TaskInfo`\n- `Inspector.RunTaskByKey` is replaced with `Inspector.RunTask`\n- `Inspector.DeleteTaskByKey` is replaced with `Inspector.DeleteTask`\n- `Inspector.ArchiveTaskByKey` is replaced with `Inspector.ArchiveTask`\n- `inspeq` package is removed. All types and functions from the package is moved to `asynq` package.\n- `WorkerInfo` field names have changed.\n- `Inspector.CancelActiveTask` is renamed to `Inspector.CancelProcessing`\n\n## [0.17.2] - 2021-06-06\n\n### Fixed\n\n- Free unique lock when task is deleted (https://github.com/hibiken/asynq/issues/275).\n\n## [0.17.1] - 2021-04-04\n\n### Fixed\n\n- Fix bug in internal `RDB.memoryUsage` method.\n\n## [0.17.0] - 2021-03-24\n\n### Added\n\n- `DialTimeout`, `ReadTimeout`, and `WriteTimeout` options are added to `RedisConnOpt`.\n\n## [0.16.1] - 2021-03-20\n\n### Fixed\n\n- Replace `KEYS` command with `SCAN` as recommended by [redis doc](https://redis.io/commands/KEYS).\n\n## [0.16.0] - 2021-03-10\n\n### Added\n\n- `Unregister` method is added to `Scheduler` to remove a registered entry.\n\n## [0.15.0] - 2021-01-31\n\n**IMPORTATNT**: All `Inspector` related code are moved to subpackage \"github.com/hibiken/asynq/inspeq\"\n\n### Changed\n\n- `Inspector` related code are moved to subpackage \"github.com/hibken/asynq/inspeq\".\n- `RedisConnOpt` interface has changed slightly. If you have been passing `RedisClientOpt`, `RedisFailoverClientOpt`, or `RedisClusterClientOpt` as a pointer,\n  update your code to pass as a value.\n- `ErrorMsg` field in `RetryTask` and `ArchivedTask` was renamed to `LastError`.\n\n### Added\n\n- `MaxRetry`, `Retried`, `LastError` fields were added to all task types returned from `Inspector`.\n- `MemoryUsage` field was added to `QueueStats`.\n- `DeleteAllPendingTasks`, `ArchiveAllPendingTasks` were added to `Inspector`\n- `DeleteTaskByKey` and `ArchiveTaskByKey` now supports deleting/archiving `PendingTask`.\n- asynq CLI now supports deleting/archiving pending tasks.\n\n## [0.14.1] - 2021-01-19\n\n### Fixed\n\n- `go.mod` file for CLI\n\n## [0.14.0] - 2021-01-14\n\n**IMPORTATNT**: Please run `asynq migrate` command to migrate from the previous versions.\n\n### Changed\n\n- Renamed `DeadTask` to `ArchivedTask`.\n- Renamed the operation `Kill` to `Archive` in `Inpsector`.\n- Print stack trace when Handler panics.\n- Include a file name and a line number in the error message when recovering from a panic.\n\n### Added\n\n- `DefaultRetryDelayFunc` is now a public API, which can be used in the custom `RetryDelayFunc`.\n- `SkipRetry` error is added to be used as a return value from `Handler`.\n- `Servers` method is added to `Inspector`\n- `CancelActiveTask` method is added to `Inspector`.\n- `ListSchedulerEnqueueEvents` method is added to `Inspector`.\n- `SchedulerEntries` method is added to `Inspector`.\n- `DeleteQueue` method is added to `Inspector`.\n\n## [0.13.1] - 2020-11-22\n\n### Fixed\n\n- Fixed processor to wait for specified time duration before forcefully shutdown workers.\n\n## [0.13.0] - 2020-10-13\n\n### Added\n\n- `Scheduler` type is added to enable periodic tasks. See the godoc for its APIs and [wiki](https://github.com/hibiken/asynq/wiki/Periodic-Tasks) for the getting-started guide.\n\n### Changed\n\n- interface `Option` has changed. See the godoc for the new interface.\n  This change would have no impact as long as you are using exported functions (e.g. `MaxRetry`, `Queue`, etc)\n  to create `Option`s.\n\n### Added\n\n- `Payload.String() string` method is added\n- `Payload.MarshalJSON() ([]byte, error)` method is added\n\n## [0.12.0] - 2020-09-12\n\n**IMPORTANT**: If you are upgrading from a previous version, please install the latest version of the CLI `go get -u github.com/hibiken/asynq/tools/asynq` and run `asynq migrate` command. No process should be writing to Redis while you run the migration command.\n\n## The semantics of queue have changed\n\nPreviously, we called tasks that are ready to be processed _\"Enqueued tasks\"_, and other tasks that are scheduled to be processed in the future _\"Scheduled tasks\"_, etc.\nWe changed the semantics of _\"Enqueue\"_ slightly; All tasks that client pushes to Redis are _Enqueued_ to a queue. Within a queue, tasks will transition from one state to another.\nPossible task states are:\n\n- `Pending`: task is ready to be processed (previously called \"Enqueued\")\n- `Active`: tasks is currently being processed (previously called \"InProgress\")\n- `Scheduled`: task is scheduled to be processed in the future\n- `Retry`: task failed to be processed and will be retried again in the future\n- `Dead`: task has exhausted all of its retries and stored for manual inspection purpose\n\n**These semantics change is reflected in the new `Inspector` API and CLI commands.**\n\n---\n\n### Changed\n\n#### `Client`\n\nUse `ProcessIn` or `ProcessAt` option to schedule a task instead of `EnqueueIn` or `EnqueueAt`.\n\n| Previously                  | v0.12.0                                    |\n| --------------------------- | ------------------------------------------ |\n| `client.EnqueueAt(t, task)` | `client.Enqueue(task, asynq.ProcessAt(t))` |\n| `client.EnqueueIn(d, task)` | `client.Enqueue(task, asynq.ProcessIn(d))` |\n\n#### `Inspector`\n\nAll Inspector methods are scoped to a queue, and the methods take `qname (string)` as the first argument.\n`EnqueuedTask` is renamed to `PendingTask` and its corresponding methods.\n`InProgressTask` is renamed to `ActiveTask` and its corresponding methods.\nCommand \"Enqueue\" is replaced by the verb \"Run\" (e.g. `EnqueueAllScheduledTasks` --> `RunAllScheduledTasks`)\n\n#### `CLI`\n\nCLI commands are restructured to use subcommands. Commands are organized into a few management commands:\nTo view details on any command, use `asynq help <command> <subcommand>`.\n\n- `asynq stats`\n- `asynq queue [ls inspect history rm pause unpause]`\n- `asynq task [ls cancel delete kill run delete-all kill-all run-all]`\n- `asynq server [ls]`\n\n### Added\n\n#### `RedisConnOpt`\n\n- `RedisClusterClientOpt` is added to connect to Redis Cluster.\n- `Username` field is added to all `RedisConnOpt` types in order to authenticate connection when Redis ACLs are used.\n\n#### `Client`\n\n- `ProcessIn(d time.Duration) Option` and `ProcessAt(t time.Time) Option` are added to replace `EnqueueIn` and `EnqueueAt` functionality.\n\n#### `Inspector`\n\n- `Queues() ([]string, error)` method is added to get all queue names.\n- `ClusterKeySlot(qname string) (int64, error)` method is added to get queue's hash slot in Redis cluster.\n- `ClusterNodes(qname string) ([]ClusterNode, error)` method is added to get a list of Redis cluster nodes for the given queue.\n- `Close() error` method is added to close connection with redis.\n\n### `Handler`\n\n- `GetQueueName(ctx context.Context) (string, bool)` helper is added to extract queue name from a context.\n\n## [0.11.0] - 2020-07-28\n\n### Added\n\n- `Inspector` type was added to monitor and mutate state of queues and tasks.\n- `HealthCheckFunc` and `HealthCheckInterval` fields were added to `Config` to allow user to specify a callback\n  function to check for broker connection.\n\n## [0.10.0] - 2020-07-06\n\n### Changed\n\n- All tasks now requires timeout or deadline. By default, timeout is set to 30 mins.\n- Tasks that exceed its deadline are automatically retried.\n- Encoding schema for task message has changed. Please install the latest CLI and run `migrate` command if\n  you have tasks enqueued with the previous version of asynq.\n- API of `(*Client).Enqueue`, `(*Client).EnqueueIn`, and `(*Client).EnqueueAt` has changed to return a `*Result`.\n- API of `ErrorHandler` has changed. It now takes context as the first argument and removed `retried`, `maxRetry` from the argument list.\n  Use `GetRetryCount` and/or `GetMaxRetry` to get the count values.\n\n## [0.9.4] - 2020-06-13\n\n### Fixed\n\n- Fixes issue of same tasks processed by more than one worker (https://github.com/hibiken/asynq/issues/90).\n\n## [0.9.3] - 2020-06-12\n\n### Fixed\n\n- Fixes the JSON number overflow issue (https://github.com/hibiken/asynq/issues/166).\n\n## [0.9.2] - 2020-06-08\n\n### Added\n\n- The `pause` and `unpause` commands were added to the CLI. See README for the CLI for details.\n\n## [0.9.1] - 2020-05-29\n\n### Added\n\n- `GetTaskID`, `GetRetryCount`, and `GetMaxRetry` functions were added to extract task metadata from context.\n\n## [0.9.0] - 2020-05-16\n\n### Changed\n\n- `Logger` interface has changed. Please see the godoc for the new interface.\n\n### Added\n\n- `LogLevel` type is added. Server's log level can be specified through `LogLevel` field in `Config`.\n\n## [0.8.3] - 2020-05-08\n\n### Added\n\n- `Close` method is added to `Client`.\n\n## [0.8.2] - 2020-05-03\n\n### Fixed\n\n- [Fixed cancelfunc leak](https://github.com/hibiken/asynq/pull/145)\n\n## [0.8.1] - 2020-04-27\n\n### Added\n\n- `ParseRedisURI` helper function is added to create a `RedisConnOpt` from a URI string.\n- `SetDefaultOptions` method is added to `Client`.\n\n## [0.8.0] - 2020-04-19\n\n### Changed\n\n- `Background` type is renamed to `Server`.\n- To upgrade from the previous version, Update `NewBackground` to `NewServer` and pass `Config` by value.\n- CLI is renamed to `asynq`.\n- To upgrade the CLI to the latest version run `go get -u github.com/hibiken/tools/asynq`\n- The `ps` command in CLI is renamed to `servers`\n- `Concurrency` defaults to the number of CPUs when unset or set to a negative value.\n\n### Added\n\n- `ShutdownTimeout` field is added to `Config` to speicfy timeout duration used during graceful shutdown.\n- New `Server` type exposes `Start`, `Stop`, and `Quiet` as well as `Run`.\n\n## [0.7.1] - 2020-04-05\n\n### Fixed\n\n- Fixed signal handling for windows.\n\n## [0.7.0] - 2020-03-22\n\n### Changed\n\n- Support Go v1.13+, dropped support for go v1.12\n\n### Added\n\n- `Unique` option was added to allow client to enqueue a task only if it's unique within a certain time period.\n\n## [0.6.2] - 2020-03-15\n\n### Added\n\n- `Use` method was added to `ServeMux` to apply middlewares to all handlers.\n\n## [0.6.1] - 2020-03-12\n\n### Added\n\n- `Client` can optionally schedule task with `asynq.Deadline(time)` to specify deadline for task's context. Default is no deadline.\n- `Logger` option was added to config, which allows user to specify the logger used by the background instance.\n\n## [0.6.0] - 2020-03-01\n\n### Added\n\n- Added `ServeMux` type to make it easy for users to implement Handler interface.\n- `ErrorHandler` type was added. Allow users to specify error handling function (e.g. Report error to error reporting service such as Honeybadger, Bugsnag, etc)\n\n## [0.5.0] - 2020-02-23\n\n### Changed\n\n- `Client` API has changed. Use `Enqueue`, `EnqueueAt` and `EnqueueIn` to enqueue and schedule tasks.\n\n### Added\n\n- `asynqmon workers` was added to list all running workers information\n\n## [0.4.0] - 2020-02-13\n\n### Changed\n\n- `Handler` interface has changed. `ProcessTask` method takes two arguments `context.Context` and `*asynq.Task`\n- `Queues` field in `Config` has change from `map[string]uint` to `map[string]int`\n\n### Added\n\n- `Client` can optionally schedule task with `asynq.Timeout(duration)` to specify timeout duration for task. Default is no timeout.\n- `asynqmon cancel [task id]` will send a cancelation signal to the goroutine processing the speicified task.\n\n## [0.3.0] - 2020-02-04\n\n### Added\n\n- `asynqmon ps` was added to list all background worker processes\n\n## [0.2.2] - 2020-01-26\n\n### Fixed\n\n- Fixed restoring unfinished tasks back to correct queues.\n\n### Changed\n\n- `asynqmon ls` command is now paginated (default 30 tasks from first page)\n- `asynqmon ls enqueued:[queue name]` requires queue name to be specified\n\n## [0.2.1] - 2020-01-22\n\n### Fixed\n\n- More structured log messages\n- Prevent spamming logs with a bunch of errors when Redis connection is lost\n- Fixed and updated README doc\n\n## [0.2.0] - 2020-01-19\n\n### Added\n\n- NewTask constructor\n- `Queues` option in `Config` to specify mutiple queues with priority level\n- `Client` can schedule a task with `asynq.Queue(name)` to specify which queue to use\n- `StrictPriority` option in `Config` to specify whether the priority should be followed strictly\n- `RedisConnOpt` to abstract away redis client implementation\n- [CLI] `asynqmon rmq` command to remove queue\n\n### Changed\n\n- `Client` and `Background` constructors take `RedisConnOpt` as their first argument.\n- `asynqmon stats` now shows the total of all enqueued tasks under \"Enqueued\"\n- `asynqmon stats` now shows each queue's task count\n- `asynqmon history` now doesn't take any arguments and shows data from the last 10 days by default (use `--days` flag to change the number of days)\n- Task type is now immutable (i.e., Payload is read-only)\n\n## [0.1.0] - 2020-01-04\n\n### Added\n\n- Initial version of asynq package\n- Initial version of asynqmon CLI\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.1005859375,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\nken.hibino7@gmail.com.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior,  harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.994140625,
          "content": "# Contributing\n\nThanks for your interest in contributing to Asynq!\nWe are open to, and grateful for, any contributions made by the community.\n\n## Reporting Bugs\n\nHave a look at our [issue tracker](https://github.com/hibiken/asynq/issues). If you can't find an issue (open or closed)\ndescribing your problem (or a very similar one) there, please open a new issue with\nthe following details:\n\n- Which versions of Go and Redis are you using?\n- What are you trying to accomplish?\n- What is the full error you are seeing?\n- How can we reproduce this?\n  - Please quote as much of your code as needed to reproduce (best link to a\n    public repository or Gist)\n\n## Getting Help\n\nWe run a [Gitter\nchannel](https://gitter.im/go-asynq/community) where you can ask questions and\nget help. Feel free to ask there before opening a GitHub issue.\n\n## Submitting Feature Requests\n\nIf you can't find an issue (open or closed) describing your idea on our [issue\ntracker](https://github.com/hibiken/asynq/issues), open an issue. Adding answers to the following\nquestions in your description is +1:\n\n- What do you want to do, and how do you expect Asynq to support you with that?\n- How might this be added to Asynq?\n- What are possible alternatives?\n- Are there any disadvantages?\n\nThank you! We'll try to respond as quickly as possible.\n\n## Contributing Code\n\n1. Fork this repo\n2. Download your fork `git clone git@github.com:your-username/asynq.git && cd asynq`\n3. Create your branch `git checkout -b your-branch-name`\n4. Make and commit your changes\n5. Push the branch `git push origin your-branch-name`\n6. Create a new pull request\n\nPlease try to keep your pull request focused in scope and avoid including unrelated commits.\nPlease run tests against redis cluster locally with `--redis_cluster` flag to ensure that code works for Redis cluster. TODO: Run tests using Redis cluster on CI.\n\nAfter you have submitted your pull request, we'll try to get back to you as soon as possible. We may suggest some changes or improvements.\n\nThank you for contributing!\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0458984375,
          "content": "MIT License\n\nCopyright (c) 2019 Kentaro Hibino\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.3251953125,
          "content": "ROOT_DIR:=$(shell dirname $(realpath $(firstword $(MAKEFILE_LIST))))\n\nproto: internal/proto/asynq.proto\n\tprotoc -I=$(ROOT_DIR)/internal/proto \\\n\t\t\t\t --go_out=$(ROOT_DIR)/internal/proto \\\n\t\t\t\t --go_opt=module=github.com/hibiken/asynq/internal/proto \\\n\t\t\t\t $(ROOT_DIR)/internal/proto/asynq.proto\n\n.PHONY: lint\nlint:\n\tgolangci-lint run\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.6796875,
          "content": "<img src=\"https://user-images.githubusercontent.com/11155743/114697792-ffbfa580-9d26-11eb-8e5b-33bef69476dc.png\" alt=\"Asynq logo\" width=\"360px\" />\n\n# Simple, reliable & efficient distributed task queue in Go\n\n[![GoDoc](https://godoc.org/github.com/hibiken/asynq?status.svg)](https://godoc.org/github.com/hibiken/asynq)\n[![Go Report Card](https://goreportcard.com/badge/github.com/hibiken/asynq)](https://goreportcard.com/report/github.com/hibiken/asynq)\n![Build Status](https://github.com/hibiken/asynq/workflows/build/badge.svg)\n[![License: MIT](https://img.shields.io/badge/license-MIT-green.svg)](https://opensource.org/licenses/MIT)\n[![Gitter chat](https://badges.gitter.im/go-asynq/gitter.svg)](https://gitter.im/go-asynq/community)\n\nAsynq is a Go library for queueing tasks and processing them asynchronously with workers. It's backed by [Redis](https://redis.io/) and is designed to be scalable yet easy to get started.\n\nHighlevel overview of how Asynq works:\n\n- Client puts tasks on a queue\n- Server pulls tasks off queues and starts a worker goroutine for each task\n- Tasks are processed concurrently by multiple workers\n\nTask queues are used as a mechanism to distribute work across multiple machines. A system can consist of multiple worker servers and brokers, giving way to high availability and horizontal scaling.\n\n**Example use case**\n\n![Task Queue Diagram](https://user-images.githubusercontent.com/11155743/116358505-656f5f80-a806-11eb-9c16-94e49dab0f99.jpg)\n\n## Features\n\n- Guaranteed [at least one execution](https://www.cloudcomputingpatterns.org/at_least_once_delivery/) of a task\n- Scheduling of tasks\n- [Retries](https://github.com/hibiken/asynq/wiki/Task-Retry) of failed tasks\n- Automatic recovery of tasks in the event of a worker crash\n- [Weighted priority queues](https://github.com/hibiken/asynq/wiki/Queue-Priority#weighted-priority)\n- [Strict priority queues](https://github.com/hibiken/asynq/wiki/Queue-Priority#strict-priority)\n- Low latency to add a task since writes are fast in Redis\n- De-duplication of tasks using [unique option](https://github.com/hibiken/asynq/wiki/Unique-Tasks)\n- Allow [timeout and deadline per task](https://github.com/hibiken/asynq/wiki/Task-Timeout-and-Cancelation)\n- Allow [aggregating group of tasks](https://github.com/hibiken/asynq/wiki/Task-aggregation) to batch multiple successive operations\n- [Flexible handler interface with support for middlewares](https://github.com/hibiken/asynq/wiki/Handler-Deep-Dive)\n- [Ability to pause queue](/tools/asynq/README.md#pause) to stop processing tasks from the queue\n- [Periodic Tasks](https://github.com/hibiken/asynq/wiki/Periodic-Tasks)\n- [Support Redis Sentinels](https://github.com/hibiken/asynq/wiki/Automatic-Failover) for high availability\n- Integration with [Prometheus](https://prometheus.io/) to collect and visualize queue metrics\n- [Web UI](#web-ui) to inspect and remote-control queues and tasks\n- [CLI](#command-line-tool) to inspect and remote-control queues and tasks\n\n## Stability and Compatibility\n\n**Status**: The library relatively stable and is currently undergoing **moderate development** with less frequent breaking API changes.\n\n> ☝️ **Important Note**: Current major version is zero (`v0.x.x`) to accommodate rapid development and fast iteration while getting early feedback from users (_feedback on APIs are appreciated!_). The public API could change without a major version update before `v1.0.0` release.\n\n### Redis Cluster Compatibility\n\nSome of the lua scripts in this library may not be compatible with Redis Cluster.\n\n## Sponsoring\nIf you are using this package in production, **please consider sponsoring the project to show your support!**\n\n## Quickstart\nMake sure you have Go installed ([download](https://golang.org/dl/)). The **last two** Go versions are supported (See https://go.dev/dl).\n\nInitialize your project by creating a folder and then running `go mod init github.com/your/repo` ([learn more](https://blog.golang.org/using-go-modules)) inside the folder. Then install Asynq library with the [`go get`](https://golang.org/cmd/go/#hdr-Add_dependencies_to_current_module_and_install_them) command:\n\n```sh\ngo get -u github.com/hibiken/asynq\n```\n\nMake sure you're running a Redis server locally or from a [Docker](https://hub.docker.com/_/redis) container. Version `4.0` or higher is required.\n\nNext, write a package that encapsulates task creation and task handling.\n\n```go\npackage tasks\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \"log\"\n    \"time\"\n    \"github.com/hibiken/asynq\"\n)\n\n// A list of task types.\nconst (\n    TypeEmailDelivery   = \"email:deliver\"\n    TypeImageResize     = \"image:resize\"\n)\n\ntype EmailDeliveryPayload struct {\n    UserID     int\n    TemplateID string\n}\n\ntype ImageResizePayload struct {\n    SourceURL string\n}\n\n//----------------------------------------------\n// Write a function NewXXXTask to create a task.\n// A task consists of a type and a payload.\n//----------------------------------------------\n\nfunc NewEmailDeliveryTask(userID int, tmplID string) (*asynq.Task, error) {\n    payload, err := json.Marshal(EmailDeliveryPayload{UserID: userID, TemplateID: tmplID})\n    if err != nil {\n        return nil, err\n    }\n    return asynq.NewTask(TypeEmailDelivery, payload), nil\n}\n\nfunc NewImageResizeTask(src string) (*asynq.Task, error) {\n    payload, err := json.Marshal(ImageResizePayload{SourceURL: src})\n    if err != nil {\n        return nil, err\n    }\n    // task options can be passed to NewTask, which can be overridden at enqueue time.\n    return asynq.NewTask(TypeImageResize, payload, asynq.MaxRetry(5), asynq.Timeout(20 * time.Minute)), nil\n}\n\n//---------------------------------------------------------------\n// Write a function HandleXXXTask to handle the input task.\n// Note that it satisfies the asynq.HandlerFunc interface.\n//\n// Handler doesn't need to be a function. You can define a type\n// that satisfies asynq.Handler interface. See examples below.\n//---------------------------------------------------------------\n\nfunc HandleEmailDeliveryTask(ctx context.Context, t *asynq.Task) error {\n    var p EmailDeliveryPayload\n    if err := json.Unmarshal(t.Payload(), &p); err != nil {\n        return fmt.Errorf(\"json.Unmarshal failed: %v: %w\", err, asynq.SkipRetry)\n    }\n    log.Printf(\"Sending Email to User: user_id=%d, template_id=%s\", p.UserID, p.TemplateID)\n    // Email delivery code ...\n    return nil\n}\n\n// ImageProcessor implements asynq.Handler interface.\ntype ImageProcessor struct {\n    // ... fields for struct\n}\n\nfunc (processor *ImageProcessor) ProcessTask(ctx context.Context, t *asynq.Task) error {\n    var p ImageResizePayload\n    if err := json.Unmarshal(t.Payload(), &p); err != nil {\n        return fmt.Errorf(\"json.Unmarshal failed: %v: %w\", err, asynq.SkipRetry)\n    }\n    log.Printf(\"Resizing image: src=%s\", p.SourceURL)\n    // Image resizing code ...\n    return nil\n}\n\nfunc NewImageProcessor() *ImageProcessor {\n\treturn &ImageProcessor{}\n}\n```\n\nIn your application code, import the above package and use [`Client`](https://pkg.go.dev/github.com/hibiken/asynq?tab=doc#Client) to put tasks on queues.\n\n```go\npackage main\n\nimport (\n    \"log\"\n    \"time\"\n\n    \"github.com/hibiken/asynq\"\n    \"your/app/package/tasks\"\n)\n\nconst redisAddr = \"127.0.0.1:6379\"\n\nfunc main() {\n    client := asynq.NewClient(asynq.RedisClientOpt{Addr: redisAddr})\n    defer client.Close()\n\n    // ------------------------------------------------------\n    // Example 1: Enqueue task to be processed immediately.\n    //            Use (*Client).Enqueue method.\n    // ------------------------------------------------------\n\n    task, err := tasks.NewEmailDeliveryTask(42, \"some:template:id\")\n    if err != nil {\n        log.Fatalf(\"could not create task: %v\", err)\n    }\n    info, err := client.Enqueue(task)\n    if err != nil {\n        log.Fatalf(\"could not enqueue task: %v\", err)\n    }\n    log.Printf(\"enqueued task: id=%s queue=%s\", info.ID, info.Queue)\n\n\n    // ------------------------------------------------------------\n    // Example 2: Schedule task to be processed in the future.\n    //            Use ProcessIn or ProcessAt option.\n    // ------------------------------------------------------------\n\n    info, err = client.Enqueue(task, asynq.ProcessIn(24*time.Hour))\n    if err != nil {\n        log.Fatalf(\"could not schedule task: %v\", err)\n    }\n    log.Printf(\"enqueued task: id=%s queue=%s\", info.ID, info.Queue)\n\n\n    // ----------------------------------------------------------------------------\n    // Example 3: Set other options to tune task processing behavior.\n    //            Options include MaxRetry, Queue, Timeout, Deadline, Unique etc.\n    // ----------------------------------------------------------------------------\n\n    task, err = tasks.NewImageResizeTask(\"https://example.com/myassets/image.jpg\")\n    if err != nil {\n        log.Fatalf(\"could not create task: %v\", err)\n    }\n    info, err = client.Enqueue(task, asynq.MaxRetry(10), asynq.Timeout(3 * time.Minute))\n    if err != nil {\n        log.Fatalf(\"could not enqueue task: %v\", err)\n    }\n    log.Printf(\"enqueued task: id=%s queue=%s\", info.ID, info.Queue)\n}\n```\n\nNext, start a worker server to process these tasks in the background. To start the background workers, use [`Server`](https://pkg.go.dev/github.com/hibiken/asynq?tab=doc#Server) and provide your [`Handler`](https://pkg.go.dev/github.com/hibiken/asynq?tab=doc#Handler) to process the tasks.\n\nYou can optionally use [`ServeMux`](https://pkg.go.dev/github.com/hibiken/asynq?tab=doc#ServeMux) to create a handler, just as you would with [`net/http`](https://golang.org/pkg/net/http/) Handler.\n\n```go\npackage main\n\nimport (\n    \"log\"\n\n    \"github.com/hibiken/asynq\"\n    \"your/app/package/tasks\"\n)\n\nconst redisAddr = \"127.0.0.1:6379\"\n\nfunc main() {\n    srv := asynq.NewServer(\n        asynq.RedisClientOpt{Addr: redisAddr},\n        asynq.Config{\n            // Specify how many concurrent workers to use\n            Concurrency: 10,\n            // Optionally specify multiple queues with different priority.\n            Queues: map[string]int{\n                \"critical\": 6,\n                \"default\":  3,\n                \"low\":      1,\n            },\n            // See the godoc for other configuration options\n        },\n    )\n\n    // mux maps a type to a handler\n    mux := asynq.NewServeMux()\n    mux.HandleFunc(tasks.TypeEmailDelivery, tasks.HandleEmailDeliveryTask)\n    mux.Handle(tasks.TypeImageResize, tasks.NewImageProcessor())\n    // ...register other handlers...\n\n    if err := srv.Run(mux); err != nil {\n        log.Fatalf(\"could not run server: %v\", err)\n    }\n}\n```\n\nFor a more detailed walk-through of the library, see our [Getting Started](https://github.com/hibiken/asynq/wiki/Getting-Started) guide.\n\nTo learn more about `asynq` features and APIs, see the package [godoc](https://godoc.org/github.com/hibiken/asynq).\n\n## Web UI\n\n[Asynqmon](https://github.com/hibiken/asynqmon) is a web based tool for monitoring and administrating Asynq queues and tasks.\n\nHere's a few screenshots of the Web UI:\n\n**Queues view**\n\n![Web UI Queues View](https://user-images.githubusercontent.com/11155743/114697016-07327f00-9d26-11eb-808c-0ac841dc888e.png)\n\n**Tasks view**\n\n![Web UI TasksView](https://user-images.githubusercontent.com/11155743/114697070-1f0a0300-9d26-11eb-855c-d3ec263865b7.png)\n\n**Metrics view**\n<img width=\"1532\" alt=\"Screen Shot 2021-12-19 at 4 37 19 PM\" src=\"https://user-images.githubusercontent.com/10953044/146777420-cae6c476-bac6-469c-acce-b2f6584e8707.png\">\n\n**Settings and adaptive dark mode**\n\n![Web UI Settings and adaptive dark mode](https://user-images.githubusercontent.com/11155743/114697149-3517c380-9d26-11eb-9f7a-ae2dd00aad5b.png)\n\nFor details on how to use the tool, refer to the tool's [README](https://github.com/hibiken/asynqmon#readme).\n\n## Command Line Tool\n\nAsynq ships with a command line tool to inspect the state of queues and tasks.\n\nTo install the CLI tool, run the following command:\n\n```sh\ngo install github.com/hibiken/asynq/tools/asynq@latest\n```\n\nHere's an example of running the `asynq dash` command:\n\n![Gif](/docs/assets/dash.gif)\n\nFor details on how to use the tool, refer to the tool's [README](/tools/asynq/README.md).\n\n## Contributing\n\nWe are open to, and grateful for, any contributions (GitHub issues/PRs, feedback on [Gitter channel](https://gitter.im/go-asynq/community), etc) made by the community.\n\nPlease see the [Contribution Guide](/CONTRIBUTING.md) before contributing.\n\n## License\n\nCopyright (c) 2019-present [Ken Hibino](https://github.com/hibiken) and [Contributors](https://github.com/hibiken/asynq/graphs/contributors). `Asynq` is free and open-source software licensed under the [MIT License](https://github.com/hibiken/asynq/blob/master/LICENSE). Official logo was created by [Vic Shóstak](https://github.com/koddr) and distributed under [Creative Commons](https://creativecommons.org/publicdomain/zero/1.0/) license (CC0 1.0 Universal).\n"
        },
        {
          "name": "aggregator.go",
          "type": "blob",
          "size": 4.765625,
          "content": "// Copyright 2022 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/log\"\n)\n\n// An aggregator is responsible for checking groups and aggregate into one task\n// if any of the grouping condition is met.\ntype aggregator struct {\n\tlogger *log.Logger\n\tbroker base.Broker\n\tclient *Client\n\n\t// channel to communicate back to the long running \"aggregator\" goroutine.\n\tdone chan struct{}\n\n\t// list of queue names to check and aggregate.\n\tqueues []string\n\n\t// Group configurations\n\tgracePeriod time.Duration\n\tmaxDelay    time.Duration\n\tmaxSize     int\n\n\t// User provided group aggregator.\n\tga GroupAggregator\n\n\t// interval used to check for aggregation\n\tinterval time.Duration\n\n\t// sema is a counting semaphore to ensure the number of active aggregating function\n\t// does not exceed the limit.\n\tsema chan struct{}\n}\n\ntype aggregatorParams struct {\n\tlogger          *log.Logger\n\tbroker          base.Broker\n\tqueues          []string\n\tgracePeriod     time.Duration\n\tmaxDelay        time.Duration\n\tmaxSize         int\n\tgroupAggregator GroupAggregator\n}\n\nconst (\n\t// Maximum number of aggregation checks in flight concurrently.\n\tmaxConcurrentAggregationChecks = 3\n\n\t// Default interval used for aggregation checks. If the provided gracePeriod is less than\n\t// the default, use the gracePeriod.\n\tdefaultAggregationCheckInterval = 7 * time.Second\n)\n\nfunc newAggregator(params aggregatorParams) *aggregator {\n\tinterval := defaultAggregationCheckInterval\n\tif params.gracePeriod < interval {\n\t\tinterval = params.gracePeriod\n\t}\n\treturn &aggregator{\n\t\tlogger:      params.logger,\n\t\tbroker:      params.broker,\n\t\tclient:      &Client{broker: params.broker},\n\t\tdone:        make(chan struct{}),\n\t\tqueues:      params.queues,\n\t\tgracePeriod: params.gracePeriod,\n\t\tmaxDelay:    params.maxDelay,\n\t\tmaxSize:     params.maxSize,\n\t\tga:          params.groupAggregator,\n\t\tsema:        make(chan struct{}, maxConcurrentAggregationChecks),\n\t\tinterval:    interval,\n\t}\n}\n\nfunc (a *aggregator) shutdown() {\n\tif a.ga == nil {\n\t\treturn\n\t}\n\ta.logger.Debug(\"Aggregator shutting down...\")\n\t// Signal the aggregator goroutine to stop.\n\ta.done <- struct{}{}\n}\n\nfunc (a *aggregator) start(wg *sync.WaitGroup) {\n\tif a.ga == nil {\n\t\treturn\n\t}\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tticker := time.NewTicker(a.interval)\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-a.done:\n\t\t\t\ta.logger.Debug(\"Waiting for all aggregation checks to finish...\")\n\t\t\t\t// block until all aggregation checks released the token\n\t\t\t\tfor i := 0; i < cap(a.sema); i++ {\n\t\t\t\t\ta.sema <- struct{}{}\n\t\t\t\t}\n\t\t\t\ta.logger.Debug(\"Aggregator done\")\n\t\t\t\tticker.Stop()\n\t\t\t\treturn\n\t\t\tcase t := <-ticker.C:\n\t\t\t\ta.exec(t)\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc (a *aggregator) exec(t time.Time) {\n\tselect {\n\tcase a.sema <- struct{}{}: // acquire token\n\t\tgo a.aggregate(t)\n\tdefault:\n\t\t// If the semaphore blocks, then we are currently running max number of\n\t\t// aggregation checks. Skip this round and log warning.\n\t\ta.logger.Warnf(\"Max number of aggregation checks in flight. Skipping\")\n\t}\n}\n\nfunc (a *aggregator) aggregate(t time.Time) {\n\tdefer func() { <-a.sema /* release token */ }()\n\tfor _, qname := range a.queues {\n\t\tgroups, err := a.broker.ListGroups(qname)\n\t\tif err != nil {\n\t\t\ta.logger.Errorf(\"Failed to list groups in queue: %q\", qname)\n\t\t\tcontinue\n\t\t}\n\t\tfor _, gname := range groups {\n\t\t\taggregationSetID, err := a.broker.AggregationCheck(\n\t\t\t\tqname, gname, t, a.gracePeriod, a.maxDelay, a.maxSize)\n\t\t\tif err != nil {\n\t\t\t\ta.logger.Errorf(\"Failed to run aggregation check: queue=%q group=%q\", qname, gname)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif aggregationSetID == \"\" {\n\t\t\t\ta.logger.Debugf(\"No aggregation needed at this time: queue=%q group=%q\", qname, gname)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Aggregate and enqueue.\n\t\t\tmsgs, deadline, err := a.broker.ReadAggregationSet(qname, gname, aggregationSetID)\n\t\t\tif err != nil {\n\t\t\t\ta.logger.Errorf(\"Failed to read aggregation set: queue=%q, group=%q, setID=%q\",\n\t\t\t\t\tqname, gname, aggregationSetID)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttasks := make([]*Task, len(msgs))\n\t\t\tfor i, m := range msgs {\n\t\t\t\ttasks[i] = NewTask(m.Type, m.Payload)\n\t\t\t}\n\t\t\taggregatedTask := a.ga.Aggregate(gname, tasks)\n\t\t\tctx, cancel := context.WithDeadline(context.Background(), deadline)\n\t\t\tif _, err := a.client.EnqueueContext(ctx, aggregatedTask, Queue(qname)); err != nil {\n\t\t\t\ta.logger.Errorf(\"Failed to enqueue aggregated task (queue=%q, group=%q, setID=%q): %v\",\n\t\t\t\t\tqname, gname, aggregationSetID, err)\n\t\t\t\tcancel()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err := a.broker.DeleteAggregationSet(ctx, qname, gname, aggregationSetID); err != nil {\n\t\t\t\ta.logger.Warnf(\"Failed to delete aggregation set: queue=%q, group=%q, setID=%q\",\n\t\t\t\t\tqname, gname, aggregationSetID)\n\t\t\t}\n\t\t\tcancel()\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "aggregator_test.go",
          "type": "blob",
          "size": 4.939453125,
          "content": "// Copyright 2022 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/rdb\"\n\th \"github.com/hibiken/asynq/internal/testutil\"\n)\n\nfunc TestAggregator(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\trdbClient := rdb.NewRDB(r)\n\tclient := Client{broker: rdbClient}\n\n\ttests := []struct {\n\t\tdesc             string\n\t\tgracePeriod      time.Duration\n\t\tmaxDelay         time.Duration\n\t\tmaxSize          int\n\t\taggregateFunc    func(gname string, tasks []*Task) *Task\n\t\ttasks            []*Task       // tasks to enqueue\n\t\tenqueueFrequency time.Duration // time between one enqueue event to another\n\t\twaitTime         time.Duration // time to wait\n\t\twantGroups       map[string]map[string][]base.Z\n\t\twantPending      map[string][]*base.TaskMessage\n\t}{\n\t\t{\n\t\t\tdesc:        \"group older than the grace period should be aggregated\",\n\t\t\tgracePeriod: 1 * time.Second,\n\t\t\tmaxDelay:    0, // no maxdelay limit\n\t\t\tmaxSize:     0, // no maxsize limit\n\t\t\taggregateFunc: func(gname string, tasks []*Task) *Task {\n\t\t\t\treturn NewTask(gname, nil, MaxRetry(len(tasks))) // use max retry to see how many tasks were aggregated\n\t\t\t},\n\t\t\ttasks: []*Task{\n\t\t\t\tNewTask(\"task1\", nil, Group(\"mygroup\")),\n\t\t\t\tNewTask(\"task2\", nil, Group(\"mygroup\")),\n\t\t\t\tNewTask(\"task3\", nil, Group(\"mygroup\")),\n\t\t\t},\n\t\t\tenqueueFrequency: 300 * time.Millisecond,\n\t\t\twaitTime:         3 * time.Second,\n\t\t\twantGroups: map[string]map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t\"mygroup\": {},\n\t\t\t\t},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {\n\t\t\t\t\th.NewTaskMessageBuilder().SetType(\"mygroup\").SetRetry(3).Build(),\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc:        \"group older than the max-delay should be aggregated\",\n\t\t\tgracePeriod: 2 * time.Second,\n\t\t\tmaxDelay:    4 * time.Second,\n\t\t\tmaxSize:     0, // no maxsize limit\n\t\t\taggregateFunc: func(gname string, tasks []*Task) *Task {\n\t\t\t\treturn NewTask(gname, nil, MaxRetry(len(tasks))) // use max retry to see how many tasks were aggregated\n\t\t\t},\n\t\t\ttasks: []*Task{\n\t\t\t\tNewTask(\"task1\", nil, Group(\"mygroup\")), // time 0\n\t\t\t\tNewTask(\"task2\", nil, Group(\"mygroup\")), // time 1s\n\t\t\t\tNewTask(\"task3\", nil, Group(\"mygroup\")), // time 2s\n\t\t\t\tNewTask(\"task4\", nil, Group(\"mygroup\")), // time 3s\n\t\t\t},\n\t\t\tenqueueFrequency: 1 * time.Second,\n\t\t\twaitTime:         4 * time.Second,\n\t\t\twantGroups: map[string]map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t\"mygroup\": {},\n\t\t\t\t},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {\n\t\t\t\t\th.NewTaskMessageBuilder().SetType(\"mygroup\").SetRetry(4).Build(),\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc:        \"group reached the max-size should be aggregated\",\n\t\t\tgracePeriod: 1 * time.Minute,\n\t\t\tmaxDelay:    0, // no maxdelay limit\n\t\t\tmaxSize:     5,\n\t\t\taggregateFunc: func(gname string, tasks []*Task) *Task {\n\t\t\t\treturn NewTask(gname, nil, MaxRetry(len(tasks))) // use max retry to see how many tasks were aggregated\n\t\t\t},\n\t\t\ttasks: []*Task{\n\t\t\t\tNewTask(\"task1\", nil, Group(\"mygroup\")),\n\t\t\t\tNewTask(\"task2\", nil, Group(\"mygroup\")),\n\t\t\t\tNewTask(\"task3\", nil, Group(\"mygroup\")),\n\t\t\t\tNewTask(\"task4\", nil, Group(\"mygroup\")),\n\t\t\t\tNewTask(\"task5\", nil, Group(\"mygroup\")),\n\t\t\t},\n\t\t\tenqueueFrequency: 300 * time.Millisecond,\n\t\t\twaitTime:         defaultAggregationCheckInterval * 2,\n\t\t\twantGroups: map[string]map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t\"mygroup\": {},\n\t\t\t\t},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {\n\t\t\t\t\th.NewTaskMessageBuilder().SetType(\"mygroup\").SetRetry(5).Build(),\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\n\t\taggregator := newAggregator(aggregatorParams{\n\t\t\tlogger:          testLogger,\n\t\t\tbroker:          rdbClient,\n\t\t\tqueues:          []string{\"default\"},\n\t\t\tgracePeriod:     tc.gracePeriod,\n\t\t\tmaxDelay:        tc.maxDelay,\n\t\t\tmaxSize:         tc.maxSize,\n\t\t\tgroupAggregator: GroupAggregatorFunc(tc.aggregateFunc),\n\t\t})\n\n\t\tvar wg sync.WaitGroup\n\t\taggregator.start(&wg)\n\n\t\tfor _, task := range tc.tasks {\n\t\t\tif _, err := client.Enqueue(task); err != nil {\n\t\t\t\tt.Errorf(\"%s: Client Enqueue failed: %v\", tc.desc, err)\n\t\t\t\taggregator.shutdown()\n\t\t\t\twg.Wait()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttime.Sleep(tc.enqueueFrequency)\n\t\t}\n\n\t\ttime.Sleep(tc.waitTime)\n\n\t\tfor qname, groups := range tc.wantGroups {\n\t\t\tfor gname, want := range groups {\n\t\t\t\tgotGroup := h.GetGroupEntries(t, r, qname, gname)\n\t\t\t\tif diff := cmp.Diff(want, gotGroup, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\t\tt.Errorf(\"%s: mismatch found in %q; (-want,+got)\\n%s\", tc.desc, base.GroupKey(qname, gname), diff)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgotPending := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotPending, h.SortMsgOpt, h.IgnoreIDOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"%s: mismatch found in %q; (-want,+got)\\n%s\", tc.desc, base.PendingKey(qname), diff)\n\t\t\t}\n\t\t}\n\t\taggregator.shutdown()\n\t\twg.Wait()\n\t}\n}\n"
        },
        {
          "name": "asynq.go",
          "type": "blob",
          "size": 15.9765625,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\t\"crypto/tls\"\n\t\"fmt\"\n\t\"net\"\n\t\"net/url\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/hibiken/asynq/internal/base\"\n)\n\n// Task represents a unit of work to be performed.\ntype Task struct {\n\t// typename indicates the type of task to be performed.\n\ttypename string\n\n\t// payload holds data needed to perform the task.\n\tpayload []byte\n\n\t// opts holds options for the task.\n\topts []Option\n\n\t// w is the ResultWriter for the task.\n\tw *ResultWriter\n}\n\nfunc (t *Task) Type() string    { return t.typename }\nfunc (t *Task) Payload() []byte { return t.payload }\n\n// ResultWriter returns a pointer to the ResultWriter associated with the task.\n//\n// Nil pointer is returned if called on a newly created task (i.e. task created by calling NewTask).\n// Only the tasks passed to Handler.ProcessTask have a valid ResultWriter pointer.\nfunc (t *Task) ResultWriter() *ResultWriter { return t.w }\n\n// NewTask returns a new Task given a type name and payload data.\n// Options can be passed to configure task processing behavior.\nfunc NewTask(typename string, payload []byte, opts ...Option) *Task {\n\treturn &Task{\n\t\ttypename: typename,\n\t\tpayload:  payload,\n\t\topts:     opts,\n\t}\n}\n\n// newTask creates a task with the given typename, payload and ResultWriter.\nfunc newTask(typename string, payload []byte, w *ResultWriter) *Task {\n\treturn &Task{\n\t\ttypename: typename,\n\t\tpayload:  payload,\n\t\tw:        w,\n\t}\n}\n\n// A TaskInfo describes a task and its metadata.\ntype TaskInfo struct {\n\t// ID is the identifier of the task.\n\tID string\n\n\t// Queue is the name of the queue in which the task belongs.\n\tQueue string\n\n\t// Type is the type name of the task.\n\tType string\n\n\t// Payload is the payload data of the task.\n\tPayload []byte\n\n\t// State indicates the task state.\n\tState TaskState\n\n\t// MaxRetry is the maximum number of times the task can be retried.\n\tMaxRetry int\n\n\t// Retried is the number of times the task has retried so far.\n\tRetried int\n\n\t// LastErr is the error message from the last failure.\n\tLastErr string\n\n\t// LastFailedAt is the time time of the last failure if any.\n\t// If the task has no failures, LastFailedAt is zero time (i.e. time.Time{}).\n\tLastFailedAt time.Time\n\n\t// Timeout is the duration the task can be processed by Handler before being retried,\n\t// zero if not specified\n\tTimeout time.Duration\n\n\t// Deadline is the deadline for the task, zero value if not specified.\n\tDeadline time.Time\n\n\t// Group is the name of the group in which the task belongs.\n\t//\n\t// Tasks in the same queue can be grouped together by Group name and will be aggregated into one task\n\t// by a Server processing the queue.\n\t//\n\t// Empty string (default) indicates task does not belong to any groups, and no aggregation will be applied to the task.\n\tGroup string\n\n\t// NextProcessAt is the time the task is scheduled to be processed,\n\t// zero if not applicable.\n\tNextProcessAt time.Time\n\n\t// IsOrphaned describes whether the task is left in active state with no worker processing it.\n\t// An orphaned task indicates that the worker has crashed or experienced network failures and was not able to\n\t// extend its lease on the task.\n\t//\n\t// This task will be recovered by running a server against the queue the task is in.\n\t// This field is only applicable to tasks with TaskStateActive.\n\tIsOrphaned bool\n\n\t// Retention is duration of the retention period after the task is successfully processed.\n\tRetention time.Duration\n\n\t// CompletedAt is the time when the task is processed successfully.\n\t// Zero value (i.e. time.Time{}) indicates no value.\n\tCompletedAt time.Time\n\n\t// Result holds the result data associated with the task.\n\t// Use ResultWriter to write result data from the Handler.\n\tResult []byte\n}\n\n// If t is non-zero, returns time converted from t as unix time in seconds.\n// If t is zero, returns zero value of time.Time.\nfunc fromUnixTimeOrZero(t int64) time.Time {\n\tif t == 0 {\n\t\treturn time.Time{}\n\t}\n\treturn time.Unix(t, 0)\n}\n\nfunc newTaskInfo(msg *base.TaskMessage, state base.TaskState, nextProcessAt time.Time, result []byte) *TaskInfo {\n\tinfo := TaskInfo{\n\t\tID:            msg.ID,\n\t\tQueue:         msg.Queue,\n\t\tType:          msg.Type,\n\t\tPayload:       msg.Payload, // Do we need to make a copy?\n\t\tMaxRetry:      msg.Retry,\n\t\tRetried:       msg.Retried,\n\t\tLastErr:       msg.ErrorMsg,\n\t\tGroup:         msg.GroupKey,\n\t\tTimeout:       time.Duration(msg.Timeout) * time.Second,\n\t\tDeadline:      fromUnixTimeOrZero(msg.Deadline),\n\t\tRetention:     time.Duration(msg.Retention) * time.Second,\n\t\tNextProcessAt: nextProcessAt,\n\t\tLastFailedAt:  fromUnixTimeOrZero(msg.LastFailedAt),\n\t\tCompletedAt:   fromUnixTimeOrZero(msg.CompletedAt),\n\t\tResult:        result,\n\t}\n\n\tswitch state {\n\tcase base.TaskStateActive:\n\t\tinfo.State = TaskStateActive\n\tcase base.TaskStatePending:\n\t\tinfo.State = TaskStatePending\n\tcase base.TaskStateScheduled:\n\t\tinfo.State = TaskStateScheduled\n\tcase base.TaskStateRetry:\n\t\tinfo.State = TaskStateRetry\n\tcase base.TaskStateArchived:\n\t\tinfo.State = TaskStateArchived\n\tcase base.TaskStateCompleted:\n\t\tinfo.State = TaskStateCompleted\n\tcase base.TaskStateAggregating:\n\t\tinfo.State = TaskStateAggregating\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"internal error: unknown state: %d\", state))\n\t}\n\treturn &info\n}\n\n// TaskState denotes the state of a task.\ntype TaskState int\n\nconst (\n\t// Indicates that the task is currently being processed by Handler.\n\tTaskStateActive TaskState = iota + 1\n\n\t// Indicates that the task is ready to be processed by Handler.\n\tTaskStatePending\n\n\t// Indicates that the task is scheduled to be processed some time in the future.\n\tTaskStateScheduled\n\n\t// Indicates that the task has previously failed and scheduled to be processed some time in the future.\n\tTaskStateRetry\n\n\t// Indicates that the task is archived and stored for inspection purposes.\n\tTaskStateArchived\n\n\t// Indicates that the task is processed successfully and retained until the retention TTL expires.\n\tTaskStateCompleted\n\n\t// Indicates that the task is waiting in a group to be aggregated into one task.\n\tTaskStateAggregating\n)\n\nfunc (s TaskState) String() string {\n\tswitch s {\n\tcase TaskStateActive:\n\t\treturn \"active\"\n\tcase TaskStatePending:\n\t\treturn \"pending\"\n\tcase TaskStateScheduled:\n\t\treturn \"scheduled\"\n\tcase TaskStateRetry:\n\t\treturn \"retry\"\n\tcase TaskStateArchived:\n\t\treturn \"archived\"\n\tcase TaskStateCompleted:\n\t\treturn \"completed\"\n\tcase TaskStateAggregating:\n\t\treturn \"aggregating\"\n\t}\n\tpanic(\"asynq: unknown task state\")\n}\n\n// RedisConnOpt is a discriminated union of types that represent Redis connection configuration option.\n//\n// RedisConnOpt represents a sum of following types:\n//\n//   - RedisClientOpt\n//   - RedisFailoverClientOpt\n//   - RedisClusterClientOpt\ntype RedisConnOpt interface {\n\t// MakeRedisClient returns a new redis client instance.\n\t// Return value is intentionally opaque to hide the implementation detail of redis client.\n\tMakeRedisClient() interface{}\n}\n\n// RedisClientOpt is used to create a redis client that connects\n// to a redis server directly.\ntype RedisClientOpt struct {\n\t// Network type to use, either tcp or unix.\n\t// Default is tcp.\n\tNetwork string\n\n\t// Redis server address in \"host:port\" format.\n\tAddr string\n\n\t// Username to authenticate the current connection when Redis ACLs are used.\n\t// See: https://redis.io/commands/auth.\n\tUsername string\n\n\t// Password to authenticate the current connection.\n\t// See: https://redis.io/commands/auth.\n\tPassword string\n\n\t// Redis DB to select after connecting to a server.\n\t// See: https://redis.io/commands/select.\n\tDB int\n\n\t// Dial timeout for establishing new connections.\n\t// Default is 5 seconds.\n\tDialTimeout time.Duration\n\n\t// Timeout for socket reads.\n\t// If timeout is reached, read commands will fail with a timeout error\n\t// instead of blocking.\n\t//\n\t// Use value -1 for no timeout and 0 for default.\n\t// Default is 3 seconds.\n\tReadTimeout time.Duration\n\n\t// Timeout for socket writes.\n\t// If timeout is reached, write commands will fail with a timeout error\n\t// instead of blocking.\n\t//\n\t// Use value -1 for no timeout and 0 for default.\n\t// Default is ReadTimout.\n\tWriteTimeout time.Duration\n\n\t// Maximum number of socket connections.\n\t// Default is 10 connections per every CPU as reported by runtime.NumCPU.\n\tPoolSize int\n\n\t// TLS Config used to connect to a server.\n\t// TLS will be negotiated only if this field is set.\n\tTLSConfig *tls.Config\n}\n\nfunc (opt RedisClientOpt) MakeRedisClient() interface{} {\n\treturn redis.NewClient(&redis.Options{\n\t\tNetwork:      opt.Network,\n\t\tAddr:         opt.Addr,\n\t\tUsername:     opt.Username,\n\t\tPassword:     opt.Password,\n\t\tDB:           opt.DB,\n\t\tDialTimeout:  opt.DialTimeout,\n\t\tReadTimeout:  opt.ReadTimeout,\n\t\tWriteTimeout: opt.WriteTimeout,\n\t\tPoolSize:     opt.PoolSize,\n\t\tTLSConfig:    opt.TLSConfig,\n\t})\n}\n\n// RedisFailoverClientOpt is used to creates a redis client that talks\n// to redis sentinels for service discovery and has an automatic failover\n// capability.\ntype RedisFailoverClientOpt struct {\n\t// Redis master name that monitored by sentinels.\n\tMasterName string\n\n\t// Addresses of sentinels in \"host:port\" format.\n\t// Use at least three sentinels to avoid problems described in\n\t// https://redis.io/topics/sentinel.\n\tSentinelAddrs []string\n\n\t// Redis sentinel username.\n\tSentinelUsername string\n\n\t// Redis sentinel password.\n\tSentinelPassword string\n\n\t// Username to authenticate the current connection when Redis ACLs are used.\n\t// See: https://redis.io/commands/auth.\n\tUsername string\n\n\t// Password to authenticate the current connection.\n\t// See: https://redis.io/commands/auth.\n\tPassword string\n\n\t// Redis DB to select after connecting to a server.\n\t// See: https://redis.io/commands/select.\n\tDB int\n\n\t// Dial timeout for establishing new connections.\n\t// Default is 5 seconds.\n\tDialTimeout time.Duration\n\n\t// Timeout for socket reads.\n\t// If timeout is reached, read commands will fail with a timeout error\n\t// instead of blocking.\n\t//\n\t// Use value -1 for no timeout and 0 for default.\n\t// Default is 3 seconds.\n\tReadTimeout time.Duration\n\n\t// Timeout for socket writes.\n\t// If timeout is reached, write commands will fail with a timeout error\n\t// instead of blocking.\n\t//\n\t// Use value -1 for no timeout and 0 for default.\n\t// Default is ReadTimeout\n\tWriteTimeout time.Duration\n\n\t// Maximum number of socket connections.\n\t// Default is 10 connections per every CPU as reported by runtime.NumCPU.\n\tPoolSize int\n\n\t// TLS Config used to connect to a server.\n\t// TLS will be negotiated only if this field is set.\n\tTLSConfig *tls.Config\n}\n\nfunc (opt RedisFailoverClientOpt) MakeRedisClient() interface{} {\n\treturn redis.NewFailoverClient(&redis.FailoverOptions{\n\t\tMasterName:       opt.MasterName,\n\t\tSentinelAddrs:    opt.SentinelAddrs,\n\t\tSentinelUsername: opt.SentinelUsername,\n\t\tSentinelPassword: opt.SentinelPassword,\n\t\tUsername:         opt.Username,\n\t\tPassword:         opt.Password,\n\t\tDB:               opt.DB,\n\t\tDialTimeout:      opt.DialTimeout,\n\t\tReadTimeout:      opt.ReadTimeout,\n\t\tWriteTimeout:     opt.WriteTimeout,\n\t\tPoolSize:         opt.PoolSize,\n\t\tTLSConfig:        opt.TLSConfig,\n\t})\n}\n\n// RedisClusterClientOpt is used to creates a redis client that connects to\n// redis cluster.\ntype RedisClusterClientOpt struct {\n\t// A seed list of host:port addresses of cluster nodes.\n\tAddrs []string\n\n\t// The maximum number of retries before giving up.\n\t// Command is retried on network errors and MOVED/ASK redirects.\n\t// Default is 8 retries.\n\tMaxRedirects int\n\n\t// Username to authenticate the current connection when Redis ACLs are used.\n\t// See: https://redis.io/commands/auth.\n\tUsername string\n\n\t// Password to authenticate the current connection.\n\t// See: https://redis.io/commands/auth.\n\tPassword string\n\n\t// Dial timeout for establishing new connections.\n\t// Default is 5 seconds.\n\tDialTimeout time.Duration\n\n\t// Timeout for socket reads.\n\t// If timeout is reached, read commands will fail with a timeout error\n\t// instead of blocking.\n\t//\n\t// Use value -1 for no timeout and 0 for default.\n\t// Default is 3 seconds.\n\tReadTimeout time.Duration\n\n\t// Timeout for socket writes.\n\t// If timeout is reached, write commands will fail with a timeout error\n\t// instead of blocking.\n\t//\n\t// Use value -1 for no timeout and 0 for default.\n\t// Default is ReadTimeout.\n\tWriteTimeout time.Duration\n\n\t// TLS Config used to connect to a server.\n\t// TLS will be negotiated only if this field is set.\n\tTLSConfig *tls.Config\n}\n\nfunc (opt RedisClusterClientOpt) MakeRedisClient() interface{} {\n\treturn redis.NewClusterClient(&redis.ClusterOptions{\n\t\tAddrs:        opt.Addrs,\n\t\tMaxRedirects: opt.MaxRedirects,\n\t\tUsername:     opt.Username,\n\t\tPassword:     opt.Password,\n\t\tDialTimeout:  opt.DialTimeout,\n\t\tReadTimeout:  opt.ReadTimeout,\n\t\tWriteTimeout: opt.WriteTimeout,\n\t\tTLSConfig:    opt.TLSConfig,\n\t})\n}\n\n// ParseRedisURI parses redis uri string and returns RedisConnOpt if uri is valid.\n// It returns a non-nil error if uri cannot be parsed.\n//\n// Three URI schemes are supported, which are redis:, rediss:, redis-socket:, and redis-sentinel:.\n// Supported formats are:\n//     redis://[:password@]host[:port][/dbnumber]\n//     rediss://[:password@]host[:port][/dbnumber]\n//     redis-socket://[:password@]path[?db=dbnumber]\n//     redis-sentinel://[:password@]host1[:port][,host2:[:port]][,hostN:[:port]][?master=masterName]\nfunc ParseRedisURI(uri string) (RedisConnOpt, error) {\n\tu, err := url.Parse(uri)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"asynq: could not parse redis uri: %v\", err)\n\t}\n\tswitch u.Scheme {\n\tcase \"redis\", \"rediss\":\n\t\treturn parseRedisURI(u)\n\tcase \"redis-socket\":\n\t\treturn parseRedisSocketURI(u)\n\tcase \"redis-sentinel\":\n\t\treturn parseRedisSentinelURI(u)\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"asynq: unsupported uri scheme: %q\", u.Scheme)\n\t}\n}\n\nfunc parseRedisURI(u *url.URL) (RedisConnOpt, error) {\n\tvar db int\n\tvar err error\n\tvar redisConnOpt RedisClientOpt\n\n\tif len(u.Path) > 0 {\n\t\txs := strings.Split(strings.Trim(u.Path, \"/\"), \"/\")\n\t\tdb, err = strconv.Atoi(xs[0])\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"asynq: could not parse redis uri: database number should be the first segment of the path\")\n\t\t}\n\t}\n\tvar password string\n\tif v, ok := u.User.Password(); ok {\n\t\tpassword = v\n\t}\n\n\tif u.Scheme == \"rediss\" {\n\t\th, _, err := net.SplitHostPort(u.Host)\n\t\tif err != nil {\n\t\t\th = u.Host\n\t\t}\n\t\tredisConnOpt.TLSConfig = &tls.Config{ServerName: h}\n\t}\n\n\tredisConnOpt.Addr = u.Host\n\tredisConnOpt.Password = password\n\tredisConnOpt.DB = db\n\n\treturn redisConnOpt, nil\n}\n\nfunc parseRedisSocketURI(u *url.URL) (RedisConnOpt, error) {\n\tconst errPrefix = \"asynq: could not parse redis socket uri\"\n\tif len(u.Path) == 0 {\n\t\treturn nil, fmt.Errorf(\"%s: path does not exist\", errPrefix)\n\t}\n\tq := u.Query()\n\tvar db int\n\tvar err error\n\tif n := q.Get(\"db\"); n != \"\" {\n\t\tdb, err = strconv.Atoi(n)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"%s: query param `db` should be a number\", errPrefix)\n\t\t}\n\t}\n\tvar password string\n\tif v, ok := u.User.Password(); ok {\n\t\tpassword = v\n\t}\n\treturn RedisClientOpt{Network: \"unix\", Addr: u.Path, DB: db, Password: password}, nil\n}\n\nfunc parseRedisSentinelURI(u *url.URL) (RedisConnOpt, error) {\n\taddrs := strings.Split(u.Host, \",\")\n\tmaster := u.Query().Get(\"master\")\n\tvar password string\n\tif v, ok := u.User.Password(); ok {\n\t\tpassword = v\n\t}\n\treturn RedisFailoverClientOpt{MasterName: master, SentinelAddrs: addrs, SentinelPassword: password}, nil\n}\n\n// ResultWriter is a client interface to write result data for a task.\n// It writes the data to the redis instance the server is connected to.\ntype ResultWriter struct {\n\tid     string // task ID this writer is responsible for\n\tqname  string // queue name the task belongs to\n\tbroker base.Broker\n\tctx    context.Context // context associated with the task\n}\n\n// Write writes the given data as a result of the task the ResultWriter is associated with.\nfunc (w *ResultWriter) Write(data []byte) (n int, err error) {\n\tselect {\n\tcase <-w.ctx.Done():\n\t\treturn 0, fmt.Errorf(\"failed to result task result: %v\", w.ctx.Err())\n\tdefault:\n\t}\n\treturn w.broker.WriteResult(w.qname, w.id, data)\n}\n\n// TaskID returns the ID of the task the ResultWriter is associated with.\nfunc (w *ResultWriter) TaskID() string {\n\treturn w.id\n}\n"
        },
        {
          "name": "asynq_test.go",
          "type": "blob",
          "size": 5.310546875,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"crypto/tls\"\n\t\"flag\"\n\t\"sort\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/google/go-cmp/cmp/cmpopts\"\n\t\"github.com/hibiken/asynq/internal/log\"\n\th \"github.com/hibiken/asynq/internal/testutil\"\n)\n\n//============================================================================\n// This file defines helper functions and variables used in other test files.\n//============================================================================\n\n// variables used for package testing.\nvar (\n\tredisAddr string\n\tredisDB   int\n\n\tuseRedisCluster   bool\n\tredisClusterAddrs string // comma-separated list of host:port\n\n\ttestLogLevel = FatalLevel\n)\n\nvar testLogger *log.Logger\n\nfunc init() {\n\tflag.StringVar(&redisAddr, \"redis_addr\", \"localhost:6379\", \"redis address to use in testing\")\n\tflag.IntVar(&redisDB, \"redis_db\", 14, \"redis db number to use in testing\")\n\tflag.BoolVar(&useRedisCluster, \"redis_cluster\", false, \"use redis cluster as a broker in testing\")\n\tflag.StringVar(&redisClusterAddrs, \"redis_cluster_addrs\", \"localhost:7000,localhost:7001,localhost:7002\", \"comma separated list of redis server addresses\")\n\tflag.Var(&testLogLevel, \"loglevel\", \"log level to use in testing\")\n\n\ttestLogger = log.NewLogger(nil)\n\ttestLogger.SetLevel(toInternalLogLevel(testLogLevel))\n}\n\nfunc setup(tb testing.TB) (r redis.UniversalClient) {\n\ttb.Helper()\n\tif useRedisCluster {\n\t\taddrs := strings.Split(redisClusterAddrs, \",\")\n\t\tif len(addrs) == 0 {\n\t\t\ttb.Fatal(\"No redis cluster addresses provided. Please set addresses using --redis_cluster_addrs flag.\")\n\t\t}\n\t\tr = redis.NewClusterClient(&redis.ClusterOptions{\n\t\t\tAddrs: addrs,\n\t\t})\n\t} else {\n\t\tr = redis.NewClient(&redis.Options{\n\t\t\tAddr: redisAddr,\n\t\t\tDB:   redisDB,\n\t\t})\n\t}\n\t// Start each test with a clean slate.\n\th.FlushDB(tb, r)\n\treturn r\n}\n\nfunc getRedisConnOpt(tb testing.TB) RedisConnOpt {\n\ttb.Helper()\n\tif useRedisCluster {\n\t\taddrs := strings.Split(redisClusterAddrs, \",\")\n\t\tif len(addrs) == 0 {\n\t\t\ttb.Fatal(\"No redis cluster addresses provided. Please set addresses using --redis_cluster_addrs flag.\")\n\t\t}\n\t\treturn RedisClusterClientOpt{\n\t\t\tAddrs: addrs,\n\t\t}\n\t}\n\treturn RedisClientOpt{\n\t\tAddr: redisAddr,\n\t\tDB:   redisDB,\n\t}\n}\n\nvar sortTaskOpt = cmp.Transformer(\"SortMsg\", func(in []*Task) []*Task {\n\tout := append([]*Task(nil), in...) // Copy input to avoid mutating it\n\tsort.Slice(out, func(i, j int) bool {\n\t\treturn out[i].Type() < out[j].Type()\n\t})\n\treturn out\n})\n\nfunc TestParseRedisURI(t *testing.T) {\n\ttests := []struct {\n\t\turi  string\n\t\twant RedisConnOpt\n\t}{\n\t\t{\n\t\t\t\"redis://localhost:6379\",\n\t\t\tRedisClientOpt{Addr: \"localhost:6379\"},\n\t\t},\n\t\t{\n\t\t\t\"rediss://localhost:6379\",\n\t\t\tRedisClientOpt{Addr: \"localhost:6379\", TLSConfig: &tls.Config{ServerName: \"localhost\"}},\n\t\t},\n\t\t{\n\t\t\t\"redis://localhost:6379/3\",\n\t\t\tRedisClientOpt{Addr: \"localhost:6379\", DB: 3},\n\t\t},\n\t\t{\n\t\t\t\"redis://:mypassword@localhost:6379\",\n\t\t\tRedisClientOpt{Addr: \"localhost:6379\", Password: \"mypassword\"},\n\t\t},\n\t\t{\n\t\t\t\"redis://:mypassword@127.0.0.1:6379/11\",\n\t\t\tRedisClientOpt{Addr: \"127.0.0.1:6379\", Password: \"mypassword\", DB: 11},\n\t\t},\n\t\t{\n\t\t\t\"redis-socket:///var/run/redis/redis.sock\",\n\t\t\tRedisClientOpt{Network: \"unix\", Addr: \"/var/run/redis/redis.sock\"},\n\t\t},\n\t\t{\n\t\t\t\"redis-socket://:mypassword@/var/run/redis/redis.sock\",\n\t\t\tRedisClientOpt{Network: \"unix\", Addr: \"/var/run/redis/redis.sock\", Password: \"mypassword\"},\n\t\t},\n\t\t{\n\t\t\t\"redis-socket:///var/run/redis/redis.sock?db=7\",\n\t\t\tRedisClientOpt{Network: \"unix\", Addr: \"/var/run/redis/redis.sock\", DB: 7},\n\t\t},\n\t\t{\n\t\t\t\"redis-socket://:mypassword@/var/run/redis/redis.sock?db=12\",\n\t\t\tRedisClientOpt{Network: \"unix\", Addr: \"/var/run/redis/redis.sock\", Password: \"mypassword\", DB: 12},\n\t\t},\n\t\t{\n\t\t\t\"redis-sentinel://localhost:5000,localhost:5001,localhost:5002?master=mymaster\",\n\t\t\tRedisFailoverClientOpt{\n\t\t\t\tMasterName:    \"mymaster\",\n\t\t\t\tSentinelAddrs: []string{\"localhost:5000\", \"localhost:5001\", \"localhost:5002\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"redis-sentinel://:mypassword@localhost:5000,localhost:5001,localhost:5002?master=mymaster\",\n\t\t\tRedisFailoverClientOpt{\n\t\t\t\tMasterName:       \"mymaster\",\n\t\t\t\tSentinelAddrs:    []string{\"localhost:5000\", \"localhost:5001\", \"localhost:5002\"},\n\t\t\t\tSentinelPassword: \"mypassword\",\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\tgot, err := ParseRedisURI(tc.uri)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"ParseRedisURI(%q) returned an error: %v\", tc.uri, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif diff := cmp.Diff(tc.want, got, cmpopts.IgnoreUnexported(tls.Config{})); diff != \"\" {\n\t\t\tt.Errorf(\"ParseRedisURI(%q) = %+v, want %+v\\n(-want,+got)\\n%s\", tc.uri, got, tc.want, diff)\n\t\t}\n\t}\n}\n\nfunc TestParseRedisURIErrors(t *testing.T) {\n\ttests := []struct {\n\t\tdesc string\n\t\turi  string\n\t}{\n\t\t{\n\t\t\t\"unsupported scheme\",\n\t\t\t\"rdb://localhost:6379\",\n\t\t},\n\t\t{\n\t\t\t\"missing scheme\",\n\t\t\t\"localhost:6379\",\n\t\t},\n\t\t{\n\t\t\t\"multiple db numbers\",\n\t\t\t\"redis://localhost:6379/1,2,3\",\n\t\t},\n\t\t{\n\t\t\t\"missing path for socket connection\",\n\t\t\t\"redis-socket://?db=one\",\n\t\t},\n\t\t{\n\t\t\t\"non integer for db numbers for socket\",\n\t\t\t\"redis-socket:///some/path/to/redis?db=one\",\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\t_, err := ParseRedisURI(tc.uri)\n\t\tif err == nil {\n\t\t\tt.Errorf(\"%s: ParseRedisURI(%q) succeeded for malformed input, want error\",\n\t\t\t\ttc.desc, tc.uri)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "benchmark_test.go",
          "type": "blob",
          "size": 5.806640625,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\th \"github.com/hibiken/asynq/internal/testutil\"\n)\n\n// Creates a new task of type \"task<n>\" with payload {\"data\": n}.\nfunc makeTask(n int) *Task {\n\tb, err := json.Marshal(map[string]int{\"data\": n})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn NewTask(fmt.Sprintf(\"task%d\", n), b)\n}\n\n// Simple E2E Benchmark testing with no scheduled tasks and retries.\nfunc BenchmarkEndToEndSimple(b *testing.B) {\n\tconst count = 100000\n\tfor n := 0; n < b.N; n++ {\n\t\tb.StopTimer() // begin setup\n\t\tsetup(b)\n\t\tredis := getRedisConnOpt(b)\n\t\tclient := NewClient(redis)\n\t\tsrv := NewServer(redis, Config{\n\t\t\tConcurrency: 10,\n\t\t\tRetryDelayFunc: func(n int, err error, t *Task) time.Duration {\n\t\t\t\treturn time.Second\n\t\t\t},\n\t\t\tLogLevel: testLogLevel,\n\t\t})\n\t\t// Create a bunch of tasks\n\t\tfor i := 0; i < count; i++ {\n\t\t\tif _, err := client.Enqueue(makeTask(i)); err != nil {\n\t\t\t\tb.Fatalf(\"could not enqueue a task: %v\", err)\n\t\t\t}\n\t\t}\n\t\tclient.Close()\n\n\t\tvar wg sync.WaitGroup\n\t\twg.Add(count)\n\t\thandler := func(ctx context.Context, t *Task) error {\n\t\t\twg.Done()\n\t\t\treturn nil\n\t\t}\n\t\tb.StartTimer() // end setup\n\n\t\t_ = srv.Start(HandlerFunc(handler))\n\t\twg.Wait()\n\n\t\tb.StopTimer() // begin teardown\n\t\tsrv.Stop()\n\t\tb.StartTimer() // end teardown\n\t}\n}\n\n// E2E benchmark with scheduled tasks and retries.\nfunc BenchmarkEndToEnd(b *testing.B) {\n\tconst count = 100000\n\tfor n := 0; n < b.N; n++ {\n\t\tb.StopTimer() // begin setup\n\t\tsetup(b)\n\t\tredis := getRedisConnOpt(b)\n\t\tclient := NewClient(redis)\n\t\tsrv := NewServer(redis, Config{\n\t\t\tConcurrency: 10,\n\t\t\tRetryDelayFunc: func(n int, err error, t *Task) time.Duration {\n\t\t\t\treturn time.Second\n\t\t\t},\n\t\t\tLogLevel: testLogLevel,\n\t\t})\n\t\t// Create a bunch of tasks\n\t\tfor i := 0; i < count; i++ {\n\t\t\tif _, err := client.Enqueue(makeTask(i)); err != nil {\n\t\t\t\tb.Fatalf(\"could not enqueue a task: %v\", err)\n\t\t\t}\n\t\t}\n\t\tfor i := 0; i < count; i++ {\n\t\t\tif _, err := client.Enqueue(makeTask(i), ProcessIn(1*time.Second)); err != nil {\n\t\t\t\tb.Fatalf(\"could not enqueue a task: %v\", err)\n\t\t\t}\n\t\t}\n\t\tclient.Close()\n\n\t\tvar wg sync.WaitGroup\n\t\twg.Add(count * 2)\n\t\thandler := func(ctx context.Context, t *Task) error {\n\t\t\tvar p map[string]int\n\t\t\tif err := json.Unmarshal(t.Payload(), &p); err != nil {\n\t\t\t\tb.Logf(\"internal error: %v\", err)\n\t\t\t}\n\t\t\tn, ok := p[\"data\"]\n\t\t\tif !ok {\n\t\t\t\tn = 1\n\t\t\t\tb.Logf(\"internal error: could not get data from payload\")\n\t\t\t}\n\t\t\tretried, ok := GetRetryCount(ctx)\n\t\t\tif !ok {\n\t\t\t\tb.Logf(\"internal error: could not get retry count from context\")\n\t\t\t}\n\t\t\t// Fail 1% of tasks for the first attempt.\n\t\t\tif retried == 0 && n%100 == 0 {\n\t\t\t\treturn fmt.Errorf(\":(\")\n\t\t\t}\n\t\t\twg.Done()\n\t\t\treturn nil\n\t\t}\n\t\tb.StartTimer() // end setup\n\n\t\t_ = srv.Start(HandlerFunc(handler))\n\t\twg.Wait()\n\n\t\tb.StopTimer() // begin teardown\n\t\tsrv.Stop()\n\t\tb.StartTimer() // end teardown\n\t}\n}\n\n// Simple E2E Benchmark testing with no scheduled tasks and retries with multiple queues.\nfunc BenchmarkEndToEndMultipleQueues(b *testing.B) {\n\t// number of tasks to create for each queue\n\tconst (\n\t\thighCount    = 20000\n\t\tdefaultCount = 20000\n\t\tlowCount     = 20000\n\t)\n\tfor n := 0; n < b.N; n++ {\n\t\tb.StopTimer() // begin setup\n\t\tsetup(b)\n\t\tredis := getRedisConnOpt(b)\n\t\tclient := NewClient(redis)\n\t\tsrv := NewServer(redis, Config{\n\t\t\tConcurrency: 10,\n\t\t\tQueues: map[string]int{\n\t\t\t\t\"high\":    6,\n\t\t\t\t\"default\": 3,\n\t\t\t\t\"low\":     1,\n\t\t\t},\n\t\t\tLogLevel: testLogLevel,\n\t\t})\n\t\t// Create a bunch of tasks\n\t\tfor i := 0; i < highCount; i++ {\n\t\t\tif _, err := client.Enqueue(makeTask(i), Queue(\"high\")); err != nil {\n\t\t\t\tb.Fatalf(\"could not enqueue a task: %v\", err)\n\t\t\t}\n\t\t}\n\t\tfor i := 0; i < defaultCount; i++ {\n\t\t\tif _, err := client.Enqueue(makeTask(i)); err != nil {\n\t\t\t\tb.Fatalf(\"could not enqueue a task: %v\", err)\n\t\t\t}\n\t\t}\n\t\tfor i := 0; i < lowCount; i++ {\n\t\t\tif _, err := client.Enqueue(makeTask(i), Queue(\"low\")); err != nil {\n\t\t\t\tb.Fatalf(\"could not enqueue a task: %v\", err)\n\t\t\t}\n\t\t}\n\t\tclient.Close()\n\n\t\tvar wg sync.WaitGroup\n\t\twg.Add(highCount + defaultCount + lowCount)\n\t\thandler := func(ctx context.Context, t *Task) error {\n\t\t\twg.Done()\n\t\t\treturn nil\n\t\t}\n\t\tb.StartTimer() // end setup\n\n\t\t_ = srv.Start(HandlerFunc(handler))\n\t\twg.Wait()\n\n\t\tb.StopTimer() // begin teardown\n\t\tsrv.Stop()\n\t\tb.StartTimer() // end teardown\n\t}\n}\n\n// E2E benchmark to check client enqueue operation performs correctly,\n// while server is busy processing tasks.\nfunc BenchmarkClientWhileServerRunning(b *testing.B) {\n\tconst count = 10000\n\tfor n := 0; n < b.N; n++ {\n\t\tb.StopTimer() // begin setup\n\t\tsetup(b)\n\t\tredis := getRedisConnOpt(b)\n\t\tclient := NewClient(redis)\n\t\tsrv := NewServer(redis, Config{\n\t\t\tConcurrency: 10,\n\t\t\tRetryDelayFunc: func(n int, err error, t *Task) time.Duration {\n\t\t\t\treturn time.Second\n\t\t\t},\n\t\t\tLogLevel: testLogLevel,\n\t\t})\n\t\t// Enqueue 10,000 tasks.\n\t\tfor i := 0; i < count; i++ {\n\t\t\tif _, err := client.Enqueue(makeTask(i)); err != nil {\n\t\t\t\tb.Fatalf(\"could not enqueue a task: %v\", err)\n\t\t\t}\n\t\t}\n\t\t// Schedule 10,000 tasks.\n\t\tfor i := 0; i < count; i++ {\n\t\t\tif _, err := client.Enqueue(makeTask(i), ProcessIn(1*time.Second)); err != nil {\n\t\t\t\tb.Fatalf(\"could not enqueue a task: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\thandler := func(ctx context.Context, t *Task) error {\n\t\t\treturn nil\n\t\t}\n\t\t_ = srv.Start(HandlerFunc(handler))\n\n\t\tb.StartTimer() // end setup\n\n\t\tb.Log(\"Starting enqueueing\")\n\t\tenqueued := 0\n\t\tfor enqueued < 100000 {\n\t\t\tt := NewTask(fmt.Sprintf(\"enqueued%d\", enqueued), h.JSON(map[string]interface{}{\"data\": enqueued}))\n\t\t\tif _, err := client.Enqueue(t); err != nil {\n\t\t\t\tb.Logf(\"could not enqueue task %d: %v\", enqueued, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tenqueued++\n\t\t}\n\t\tb.Logf(\"Finished enqueueing %d tasks\", enqueued)\n\n\t\tb.StopTimer() // begin teardown\n\t\tsrv.Stop()\n\t\tclient.Close()\n\t\tb.StartTimer() // end teardown\n\t}\n}\n"
        },
        {
          "name": "client.go",
          "type": "blob",
          "size": 14.5859375,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/errors\"\n\t\"github.com/hibiken/asynq/internal/rdb\"\n\t\"github.com/redis/go-redis/v9\"\n)\n\n// A Client is responsible for scheduling tasks.\n//\n// A Client is used to register tasks that should be processed\n// immediately or some time in the future.\n//\n// Clients are safe for concurrent use by multiple goroutines.\ntype Client struct {\n\tbroker base.Broker\n\t// When a Client has been created with an existing Redis connection, we do\n\t// not want to close it.\n\tsharedConnection bool\n}\n\n// NewClient returns a new Client instance given a redis connection option.\nfunc NewClient(r RedisConnOpt) *Client {\n\tredisClient, ok := r.MakeRedisClient().(redis.UniversalClient)\n\tif !ok {\n\t\tpanic(fmt.Sprintf(\"asynq: unsupported RedisConnOpt type %T\", r))\n\t}\n\tclient := NewClientFromRedisClient(redisClient)\n\tclient.sharedConnection = false\n\treturn client\n}\n\n// NewClientFromRedisClient returns a new instance of Client given a redis.UniversalClient\n// Warning: The underlying redis connection pool will not be closed by Asynq, you are responsible for closing it.\nfunc NewClientFromRedisClient(c redis.UniversalClient) *Client {\n\treturn &Client{broker: rdb.NewRDB(c), sharedConnection: true}\n}\n\ntype OptionType int\n\nconst (\n\tMaxRetryOpt OptionType = iota\n\tQueueOpt\n\tTimeoutOpt\n\tDeadlineOpt\n\tUniqueOpt\n\tProcessAtOpt\n\tProcessInOpt\n\tTaskIDOpt\n\tRetentionOpt\n\tGroupOpt\n)\n\n// Option specifies the task processing behavior.\ntype Option interface {\n\t// String returns a string representation of the option.\n\tString() string\n\n\t// Type describes the type of the option.\n\tType() OptionType\n\n\t// Value returns a value used to create this option.\n\tValue() interface{}\n}\n\n// Internal option representations.\ntype (\n\tretryOption     int\n\tqueueOption     string\n\ttaskIDOption    string\n\ttimeoutOption   time.Duration\n\tdeadlineOption  time.Time\n\tuniqueOption    time.Duration\n\tprocessAtOption time.Time\n\tprocessInOption time.Duration\n\tretentionOption time.Duration\n\tgroupOption     string\n)\n\n// MaxRetry returns an option to specify the max number of times\n// the task will be retried.\n//\n// Negative retry count is treated as zero retry.\nfunc MaxRetry(n int) Option {\n\tif n < 0 {\n\t\tn = 0\n\t}\n\treturn retryOption(n)\n}\n\nfunc (n retryOption) String() string     { return fmt.Sprintf(\"MaxRetry(%d)\", int(n)) }\nfunc (n retryOption) Type() OptionType   { return MaxRetryOpt }\nfunc (n retryOption) Value() interface{} { return int(n) }\n\n// Queue returns an option to specify the queue to enqueue the task into.\nfunc Queue(name string) Option {\n\treturn queueOption(name)\n}\n\nfunc (name queueOption) String() string     { return fmt.Sprintf(\"Queue(%q)\", string(name)) }\nfunc (name queueOption) Type() OptionType   { return QueueOpt }\nfunc (name queueOption) Value() interface{} { return string(name) }\n\n// TaskID returns an option to specify the task ID.\nfunc TaskID(id string) Option {\n\treturn taskIDOption(id)\n}\n\nfunc (id taskIDOption) String() string     { return fmt.Sprintf(\"TaskID(%q)\", string(id)) }\nfunc (id taskIDOption) Type() OptionType   { return TaskIDOpt }\nfunc (id taskIDOption) Value() interface{} { return string(id) }\n\n// Timeout returns an option to specify how long a task may run.\n// If the timeout elapses before the Handler returns, then the task\n// will be retried.\n//\n// Zero duration means no limit.\n//\n// If there's a conflicting Deadline option, whichever comes earliest\n// will be used.\nfunc Timeout(d time.Duration) Option {\n\treturn timeoutOption(d)\n}\n\nfunc (d timeoutOption) String() string     { return fmt.Sprintf(\"Timeout(%v)\", time.Duration(d)) }\nfunc (d timeoutOption) Type() OptionType   { return TimeoutOpt }\nfunc (d timeoutOption) Value() interface{} { return time.Duration(d) }\n\n// Deadline returns an option to specify the deadline for the given task.\n// If it reaches the deadline before the Handler returns, then the task\n// will be retried.\n//\n// If there's a conflicting Timeout option, whichever comes earliest\n// will be used.\nfunc Deadline(t time.Time) Option {\n\treturn deadlineOption(t)\n}\n\nfunc (t deadlineOption) String() string {\n\treturn fmt.Sprintf(\"Deadline(%v)\", time.Time(t).Format(time.UnixDate))\n}\nfunc (t deadlineOption) Type() OptionType   { return DeadlineOpt }\nfunc (t deadlineOption) Value() interface{} { return time.Time(t) }\n\n// Unique returns an option to enqueue a task only if the given task is unique.\n// Task enqueued with this option is guaranteed to be unique within the given ttl.\n// Once the task gets processed successfully or once the TTL has expired,\n// another task with the same uniqueness may be enqueued.\n// ErrDuplicateTask error is returned when enqueueing a duplicate task.\n// TTL duration must be greater than or equal to 1 second.\n//\n// Uniqueness of a task is based on the following properties:\n//   - Task Type\n//   - Task Payload\n//   - Queue Name\nfunc Unique(ttl time.Duration) Option {\n\treturn uniqueOption(ttl)\n}\n\nfunc (ttl uniqueOption) String() string     { return fmt.Sprintf(\"Unique(%v)\", time.Duration(ttl)) }\nfunc (ttl uniqueOption) Type() OptionType   { return UniqueOpt }\nfunc (ttl uniqueOption) Value() interface{} { return time.Duration(ttl) }\n\n// ProcessAt returns an option to specify when to process the given task.\n//\n// If there's a conflicting ProcessIn option, the last option passed to Enqueue overrides the others.\nfunc ProcessAt(t time.Time) Option {\n\treturn processAtOption(t)\n}\n\nfunc (t processAtOption) String() string {\n\treturn fmt.Sprintf(\"ProcessAt(%v)\", time.Time(t).Format(time.UnixDate))\n}\nfunc (t processAtOption) Type() OptionType   { return ProcessAtOpt }\nfunc (t processAtOption) Value() interface{} { return time.Time(t) }\n\n// ProcessIn returns an option to specify when to process the given task relative to the current time.\n//\n// If there's a conflicting ProcessAt option, the last option passed to Enqueue overrides the others.\nfunc ProcessIn(d time.Duration) Option {\n\treturn processInOption(d)\n}\n\nfunc (d processInOption) String() string     { return fmt.Sprintf(\"ProcessIn(%v)\", time.Duration(d)) }\nfunc (d processInOption) Type() OptionType   { return ProcessInOpt }\nfunc (d processInOption) Value() interface{} { return time.Duration(d) }\n\n// Retention returns an option to specify the duration of retention period for the task.\n// If this option is provided, the task will be stored as a completed task after successful processing.\n// A completed task will be deleted after the specified duration elapses.\nfunc Retention(d time.Duration) Option {\n\treturn retentionOption(d)\n}\n\nfunc (ttl retentionOption) String() string     { return fmt.Sprintf(\"Retention(%v)\", time.Duration(ttl)) }\nfunc (ttl retentionOption) Type() OptionType   { return RetentionOpt }\nfunc (ttl retentionOption) Value() interface{} { return time.Duration(ttl) }\n\n// Group returns an option to specify the group used for the task.\n// Tasks in a given queue with the same group will be aggregated into one task before passed to Handler.\nfunc Group(name string) Option {\n\treturn groupOption(name)\n}\n\nfunc (name groupOption) String() string     { return fmt.Sprintf(\"Group(%q)\", string(name)) }\nfunc (name groupOption) Type() OptionType   { return GroupOpt }\nfunc (name groupOption) Value() interface{} { return string(name) }\n\n// ErrDuplicateTask indicates that the given task could not be enqueued since it's a duplicate of another task.\n//\n// ErrDuplicateTask error only applies to tasks enqueued with a Unique option.\nvar ErrDuplicateTask = errors.New(\"task already exists\")\n\n// ErrTaskIDConflict indicates that the given task could not be enqueued since its task ID already exists.\n//\n// ErrTaskIDConflict error only applies to tasks enqueued with a TaskID option.\nvar ErrTaskIDConflict = errors.New(\"task ID conflicts with another task\")\n\ntype option struct {\n\tretry     int\n\tqueue     string\n\ttaskID    string\n\ttimeout   time.Duration\n\tdeadline  time.Time\n\tuniqueTTL time.Duration\n\tprocessAt time.Time\n\tretention time.Duration\n\tgroup     string\n}\n\n// composeOptions merges user provided options into the default options\n// and returns the composed option.\n// It also validates the user provided options and returns an error if any of\n// the user provided options fail the validations.\nfunc composeOptions(opts ...Option) (option, error) {\n\tres := option{\n\t\tretry:     defaultMaxRetry,\n\t\tqueue:     base.DefaultQueueName,\n\t\ttaskID:    uuid.NewString(),\n\t\ttimeout:   0, // do not set to defaultTimeout here\n\t\tdeadline:  time.Time{},\n\t\tprocessAt: time.Now(),\n\t}\n\tfor _, opt := range opts {\n\t\tswitch opt := opt.(type) {\n\t\tcase retryOption:\n\t\t\tres.retry = int(opt)\n\t\tcase queueOption:\n\t\t\tqname := string(opt)\n\t\t\tif err := base.ValidateQueueName(qname); err != nil {\n\t\t\t\treturn option{}, err\n\t\t\t}\n\t\t\tres.queue = qname\n\t\tcase taskIDOption:\n\t\t\tid := string(opt)\n\t\t\tif isBlank(id) {\n\t\t\t\treturn option{}, errors.New(\"task ID cannot be empty\")\n\t\t\t}\n\t\t\tres.taskID = id\n\t\tcase timeoutOption:\n\t\t\tres.timeout = time.Duration(opt)\n\t\tcase deadlineOption:\n\t\t\tres.deadline = time.Time(opt)\n\t\tcase uniqueOption:\n\t\t\tttl := time.Duration(opt)\n\t\t\tif ttl < 1*time.Second {\n\t\t\t\treturn option{}, errors.New(\"Unique TTL cannot be less than 1s\")\n\t\t\t}\n\t\t\tres.uniqueTTL = ttl\n\t\tcase processAtOption:\n\t\t\tres.processAt = time.Time(opt)\n\t\tcase processInOption:\n\t\t\tres.processAt = time.Now().Add(time.Duration(opt))\n\t\tcase retentionOption:\n\t\t\tres.retention = time.Duration(opt)\n\t\tcase groupOption:\n\t\t\tkey := string(opt)\n\t\t\tif isBlank(key) {\n\t\t\t\treturn option{}, errors.New(\"group key cannot be empty\")\n\t\t\t}\n\t\t\tres.group = key\n\t\tdefault:\n\t\t\t// ignore unexpected option\n\t\t}\n\t}\n\treturn res, nil\n}\n\n// isBlank returns true if the given s is empty or consist of all whitespaces.\nfunc isBlank(s string) bool {\n\treturn strings.TrimSpace(s) == \"\"\n}\n\nconst (\n\t// Default max retry count used if nothing is specified.\n\tdefaultMaxRetry = 25\n\n\t// Default timeout used if both timeout and deadline are not specified.\n\tdefaultTimeout = 30 * time.Minute\n)\n\n// Value zero indicates no timeout and no deadline.\nvar (\n\tnoTimeout  time.Duration = 0\n\tnoDeadline time.Time     = time.Unix(0, 0)\n)\n\n// Close closes the connection with redis.\nfunc (c *Client) Close() error {\n\tif c.sharedConnection {\n\t\treturn fmt.Errorf(\"redis connection is shared so the Client can't be closed through asynq\")\n\t}\n\treturn c.broker.Close()\n}\n\n// Enqueue enqueues the given task to a queue.\n//\n// Enqueue returns TaskInfo and nil error if the task is enqueued successfully, otherwise returns a non-nil error.\n//\n// The argument opts specifies the behavior of task processing.\n// If there are conflicting Option values the last one overrides others.\n// Any options provided to NewTask can be overridden by options passed to Enqueue.\n// By default, max retry is set to 25 and timeout is set to 30 minutes.\n//\n// If no ProcessAt or ProcessIn options are provided, the task will be pending immediately.\n//\n// Enqueue uses context.Background internally; to specify the context, use EnqueueContext.\nfunc (c *Client) Enqueue(task *Task, opts ...Option) (*TaskInfo, error) {\n\treturn c.EnqueueContext(context.Background(), task, opts...)\n}\n\n// EnqueueContext enqueues the given task to a queue.\n//\n// EnqueueContext returns TaskInfo and nil error if the task is enqueued successfully, otherwise returns a non-nil error.\n//\n// The argument opts specifies the behavior of task processing.\n// If there are conflicting Option values the last one overrides others.\n// Any options provided to NewTask can be overridden by options passed to Enqueue.\n// By default, max retry is set to 25 and timeout is set to 30 minutes.\n//\n// If no ProcessAt or ProcessIn options are provided, the task will be pending immediately.\n//\n// The first argument context applies to the enqueue operation. To specify task timeout and deadline, use Timeout and Deadline option instead.\nfunc (c *Client) EnqueueContext(ctx context.Context, task *Task, opts ...Option) (*TaskInfo, error) {\n\tif task == nil {\n\t\treturn nil, fmt.Errorf(\"task cannot be nil\")\n\t}\n\tif strings.TrimSpace(task.Type()) == \"\" {\n\t\treturn nil, fmt.Errorf(\"task typename cannot be empty\")\n\t}\n\t// merge task options with the options provided at enqueue time.\n\topts = append(task.opts, opts...)\n\topt, err := composeOptions(opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdeadline := noDeadline\n\tif !opt.deadline.IsZero() {\n\t\tdeadline = opt.deadline\n\t}\n\ttimeout := noTimeout\n\tif opt.timeout != 0 {\n\t\ttimeout = opt.timeout\n\t}\n\tif deadline.Equal(noDeadline) && timeout == noTimeout {\n\t\t// If neither deadline nor timeout are set, use default timeout.\n\t\ttimeout = defaultTimeout\n\t}\n\tvar uniqueKey string\n\tif opt.uniqueTTL > 0 {\n\t\tuniqueKey = base.UniqueKey(opt.queue, task.Type(), task.Payload())\n\t}\n\tmsg := &base.TaskMessage{\n\t\tID:        opt.taskID,\n\t\tType:      task.Type(),\n\t\tPayload:   task.Payload(),\n\t\tQueue:     opt.queue,\n\t\tRetry:     opt.retry,\n\t\tDeadline:  deadline.Unix(),\n\t\tTimeout:   int64(timeout.Seconds()),\n\t\tUniqueKey: uniqueKey,\n\t\tGroupKey:  opt.group,\n\t\tRetention: int64(opt.retention.Seconds()),\n\t}\n\tnow := time.Now()\n\tvar state base.TaskState\n\tif opt.processAt.After(now) {\n\t\terr = c.schedule(ctx, msg, opt.processAt, opt.uniqueTTL)\n\t\tstate = base.TaskStateScheduled\n\t} else if opt.group != \"\" {\n\t\t// Use zero value for processAt since we don't know when the task will be aggregated and processed.\n\t\topt.processAt = time.Time{}\n\t\terr = c.addToGroup(ctx, msg, opt.group, opt.uniqueTTL)\n\t\tstate = base.TaskStateAggregating\n\t} else {\n\t\topt.processAt = now\n\t\terr = c.enqueue(ctx, msg, opt.uniqueTTL)\n\t\tstate = base.TaskStatePending\n\t}\n\tswitch {\n\tcase errors.Is(err, errors.ErrDuplicateTask):\n\t\treturn nil, fmt.Errorf(\"%w\", ErrDuplicateTask)\n\tcase errors.Is(err, errors.ErrTaskIdConflict):\n\t\treturn nil, fmt.Errorf(\"%w\", ErrTaskIDConflict)\n\tcase err != nil:\n\t\treturn nil, err\n\t}\n\treturn newTaskInfo(msg, state, opt.processAt, nil), nil\n}\n\n// Ping performs a ping against the redis connection.\nfunc (c *Client) Ping() error {\n\treturn c.broker.Ping()\n}\n\nfunc (c *Client) enqueue(ctx context.Context, msg *base.TaskMessage, uniqueTTL time.Duration) error {\n\tif uniqueTTL > 0 {\n\t\treturn c.broker.EnqueueUnique(ctx, msg, uniqueTTL)\n\t}\n\treturn c.broker.Enqueue(ctx, msg)\n}\n\nfunc (c *Client) schedule(ctx context.Context, msg *base.TaskMessage, t time.Time, uniqueTTL time.Duration) error {\n\tif uniqueTTL > 0 {\n\t\tttl := time.Until(t.Add(uniqueTTL))\n\t\treturn c.broker.ScheduleUnique(ctx, msg, t, ttl)\n\t}\n\treturn c.broker.Schedule(ctx, msg, t)\n}\n\nfunc (c *Client) addToGroup(ctx context.Context, msg *base.TaskMessage, group string, uniqueTTL time.Duration) error {\n\tif uniqueTTL > 0 {\n\t\treturn c.broker.AddToGroupUnique(ctx, msg, group, uniqueTTL)\n\t}\n\treturn c.broker.AddToGroup(ctx, msg, group)\n}\n"
        },
        {
          "name": "client_test.go",
          "type": "blob",
          "size": 31.1123046875,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/google/go-cmp/cmp/cmpopts\"\n\t\"github.com/hibiken/asynq/internal/base\"\n\th \"github.com/hibiken/asynq/internal/testutil\"\n\t\"github.com/redis/go-redis/v9\"\n)\n\nfunc TestClientEnqueueWithProcessAtOption(t *testing.T) {\n\tr := setup(t)\n\tclient := NewClient(getRedisConnOpt(t))\n\tdefer client.Close()\n\n\ttask := NewTask(\"send_email\", h.JSON(map[string]interface{}{\"to\": \"customer@gmail.com\", \"from\": \"merchant@example.com\"}))\n\n\tvar (\n\t\tnow          = time.Now()\n\t\toneHourLater = now.Add(time.Hour)\n\t)\n\n\ttests := []struct {\n\t\tdesc          string\n\t\ttask          *Task\n\t\tprocessAt     time.Time // value for ProcessAt option\n\t\topts          []Option  // other options\n\t\twantInfo      *TaskInfo\n\t\twantPending   map[string][]*base.TaskMessage\n\t\twantScheduled map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tdesc:      \"Process task immediately\",\n\t\t\ttask:      task,\n\t\t\tprocessAt: now,\n\t\t\topts:      []Option{},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"default\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStatePending,\n\t\t\t\tMaxRetry:      defaultMaxRetry,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: now,\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tType:     task.Type(),\n\t\t\t\t\t\tPayload:  task.Payload(),\n\t\t\t\t\t\tRetry:    defaultMaxRetry,\n\t\t\t\t\t\tQueue:    \"default\",\n\t\t\t\t\t\tTimeout:  int64(defaultTimeout.Seconds()),\n\t\t\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc:      \"Schedule task to be processed in the future\",\n\t\t\ttask:      task,\n\t\t\tprocessAt: oneHourLater,\n\t\t\topts:      []Option{},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"default\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStateScheduled,\n\t\t\t\tMaxRetry:      defaultMaxRetry,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: oneHourLater,\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tMessage: &base.TaskMessage{\n\t\t\t\t\t\t\tType:     task.Type(),\n\t\t\t\t\t\t\tPayload:  task.Payload(),\n\t\t\t\t\t\t\tRetry:    defaultMaxRetry,\n\t\t\t\t\t\t\tQueue:    \"default\",\n\t\t\t\t\t\t\tTimeout:  int64(defaultTimeout.Seconds()),\n\t\t\t\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t\t\t\t},\n\t\t\t\t\t\tScore: oneHourLater.Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r) // clean up db before each test case.\n\n\t\topts := append(tc.opts, ProcessAt(tc.processAt))\n\t\tgotInfo, err := client.Enqueue(tc.task, opts...)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\tcontinue\n\t\t}\n\t\tcmpOptions := []cmp.Option{\n\t\t\tcmpopts.IgnoreFields(TaskInfo{}, \"ID\"),\n\t\t\tcmpopts.EquateApproxTime(500 * time.Millisecond),\n\t\t}\n\t\tif diff := cmp.Diff(tc.wantInfo, gotInfo, cmpOptions...); diff != \"\" {\n\t\t\tt.Errorf(\"%s;\\nEnqueue(task, ProcessAt(%v)) returned %v, want %v; (-want,+got)\\n%s\",\n\t\t\t\ttc.desc, tc.processAt, gotInfo, tc.wantInfo, diff)\n\t\t}\n\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgotPending := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotPending, h.IgnoreIDOpt, cmpopts.EquateEmpty()); diff != \"\" {\n\t\t\t\tt.Errorf(\"%s;\\nmismatch found in %q; (-want,+got)\\n%s\", tc.desc, base.PendingKey(qname), diff)\n\t\t\t}\n\t\t}\n\t\tfor qname, want := range tc.wantScheduled {\n\t\t\tgotScheduled := h.GetScheduledEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotScheduled, h.IgnoreIDOpt, cmpopts.EquateEmpty()); diff != \"\" {\n\t\t\t\tt.Errorf(\"%s;\\nmismatch found in %q; (-want,+got)\\n%s\", tc.desc, base.ScheduledKey(qname), diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc testClientEnqueue(t *testing.T, client *Client, r redis.UniversalClient) {\n\ttask := NewTask(\"send_email\", h.JSON(map[string]interface{}{\"to\": \"customer@gmail.com\", \"from\": \"merchant@example.com\"}))\n\tnow := time.Now()\n\n\ttests := []struct {\n\t\tdesc        string\n\t\ttask        *Task\n\t\topts        []Option\n\t\twantInfo    *TaskInfo\n\t\twantPending map[string][]*base.TaskMessage\n\t}{\n\t\t{\n\t\t\tdesc: \"Process task immediately with a custom retry count\",\n\t\t\ttask: task,\n\t\t\topts: []Option{\n\t\t\t\tMaxRetry(3),\n\t\t\t},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"default\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStatePending,\n\t\t\t\tMaxRetry:      3,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: now,\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tType:     task.Type(),\n\t\t\t\t\t\tPayload:  task.Payload(),\n\t\t\t\t\t\tRetry:    3,\n\t\t\t\t\t\tQueue:    \"default\",\n\t\t\t\t\t\tTimeout:  int64(defaultTimeout.Seconds()),\n\t\t\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"Negative retry count\",\n\t\t\ttask: task,\n\t\t\topts: []Option{\n\t\t\t\tMaxRetry(-2),\n\t\t\t},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"default\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStatePending,\n\t\t\t\tMaxRetry:      0, // Retry count should be set to zero\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: now,\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tType:     task.Type(),\n\t\t\t\t\t\tPayload:  task.Payload(),\n\t\t\t\t\t\tRetry:    0, // Retry count should be set to zero\n\t\t\t\t\t\tQueue:    \"default\",\n\t\t\t\t\t\tTimeout:  int64(defaultTimeout.Seconds()),\n\t\t\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"Conflicting options\",\n\t\t\ttask: task,\n\t\t\topts: []Option{\n\t\t\t\tMaxRetry(2),\n\t\t\t\tMaxRetry(10),\n\t\t\t},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"default\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStatePending,\n\t\t\t\tMaxRetry:      10, // Last option takes precedence\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: now,\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tType:     task.Type(),\n\t\t\t\t\t\tPayload:  task.Payload(),\n\t\t\t\t\t\tRetry:    10, // Last option takes precedence\n\t\t\t\t\t\tQueue:    \"default\",\n\t\t\t\t\t\tTimeout:  int64(defaultTimeout.Seconds()),\n\t\t\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"With queue option\",\n\t\t\ttask: task,\n\t\t\topts: []Option{\n\t\t\t\tQueue(\"custom\"),\n\t\t\t},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"custom\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStatePending,\n\t\t\t\tMaxRetry:      defaultMaxRetry,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: now,\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"custom\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tType:     task.Type(),\n\t\t\t\t\t\tPayload:  task.Payload(),\n\t\t\t\t\t\tRetry:    defaultMaxRetry,\n\t\t\t\t\t\tQueue:    \"custom\",\n\t\t\t\t\t\tTimeout:  int64(defaultTimeout.Seconds()),\n\t\t\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"Queue option should be case sensitive\",\n\t\t\ttask: task,\n\t\t\topts: []Option{\n\t\t\t\tQueue(\"MyQueue\"),\n\t\t\t},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"MyQueue\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStatePending,\n\t\t\t\tMaxRetry:      defaultMaxRetry,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: now,\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"MyQueue\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tType:     task.Type(),\n\t\t\t\t\t\tPayload:  task.Payload(),\n\t\t\t\t\t\tRetry:    defaultMaxRetry,\n\t\t\t\t\t\tQueue:    \"MyQueue\",\n\t\t\t\t\t\tTimeout:  int64(defaultTimeout.Seconds()),\n\t\t\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"With timeout option\",\n\t\t\ttask: task,\n\t\t\topts: []Option{\n\t\t\t\tTimeout(20 * time.Second),\n\t\t\t},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"default\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStatePending,\n\t\t\t\tMaxRetry:      defaultMaxRetry,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       20 * time.Second,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: now,\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tType:     task.Type(),\n\t\t\t\t\t\tPayload:  task.Payload(),\n\t\t\t\t\t\tRetry:    defaultMaxRetry,\n\t\t\t\t\t\tQueue:    \"default\",\n\t\t\t\t\t\tTimeout:  20,\n\t\t\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"With deadline option\",\n\t\t\ttask: task,\n\t\t\topts: []Option{\n\t\t\t\tDeadline(time.Date(2020, time.June, 24, 0, 0, 0, 0, time.UTC)),\n\t\t\t},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"default\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStatePending,\n\t\t\t\tMaxRetry:      defaultMaxRetry,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       noTimeout,\n\t\t\t\tDeadline:      time.Date(2020, time.June, 24, 0, 0, 0, 0, time.UTC),\n\t\t\t\tNextProcessAt: now,\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tType:     task.Type(),\n\t\t\t\t\t\tPayload:  task.Payload(),\n\t\t\t\t\t\tRetry:    defaultMaxRetry,\n\t\t\t\t\t\tQueue:    \"default\",\n\t\t\t\t\t\tTimeout:  int64(noTimeout.Seconds()),\n\t\t\t\t\t\tDeadline: time.Date(2020, time.June, 24, 0, 0, 0, 0, time.UTC).Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"With both deadline and timeout options\",\n\t\t\ttask: task,\n\t\t\topts: []Option{\n\t\t\t\tTimeout(20 * time.Second),\n\t\t\t\tDeadline(time.Date(2020, time.June, 24, 0, 0, 0, 0, time.UTC)),\n\t\t\t},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"default\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStatePending,\n\t\t\t\tMaxRetry:      defaultMaxRetry,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       20 * time.Second,\n\t\t\t\tDeadline:      time.Date(2020, time.June, 24, 0, 0, 0, 0, time.UTC),\n\t\t\t\tNextProcessAt: now,\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tType:     task.Type(),\n\t\t\t\t\t\tPayload:  task.Payload(),\n\t\t\t\t\t\tRetry:    defaultMaxRetry,\n\t\t\t\t\t\tQueue:    \"default\",\n\t\t\t\t\t\tTimeout:  20,\n\t\t\t\t\t\tDeadline: time.Date(2020, time.June, 24, 0, 0, 0, 0, time.UTC).Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"With Retention option\",\n\t\t\ttask: task,\n\t\t\topts: []Option{\n\t\t\t\tRetention(24 * time.Hour),\n\t\t\t},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"default\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStatePending,\n\t\t\t\tMaxRetry:      defaultMaxRetry,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: now,\n\t\t\t\tRetention:     24 * time.Hour,\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tType:      task.Type(),\n\t\t\t\t\t\tPayload:   task.Payload(),\n\t\t\t\t\t\tRetry:     defaultMaxRetry,\n\t\t\t\t\t\tQueue:     \"default\",\n\t\t\t\t\t\tTimeout:   int64(defaultTimeout.Seconds()),\n\t\t\t\t\t\tDeadline:  noDeadline.Unix(),\n\t\t\t\t\t\tRetention: int64((24 * time.Hour).Seconds()),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r) // clean up db before each test case.\n\n\t\tgotInfo, err := client.Enqueue(tc.task, tc.opts...)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\tcontinue\n\t\t}\n\t\tcmpOptions := []cmp.Option{\n\t\t\tcmpopts.IgnoreFields(TaskInfo{}, \"ID\"),\n\t\t\tcmpopts.EquateApproxTime(500 * time.Millisecond),\n\t\t}\n\t\tif diff := cmp.Diff(tc.wantInfo, gotInfo, cmpOptions...); diff != \"\" {\n\t\t\tt.Errorf(\"%s;\\nEnqueue(task) returned %v, want %v; (-want,+got)\\n%s\",\n\t\t\t\ttc.desc, gotInfo, tc.wantInfo, diff)\n\t\t}\n\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgot := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, got, h.IgnoreIDOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"%s;\\nmismatch found in %q; (-want,+got)\\n%s\", tc.desc, base.PendingKey(qname), diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestClientEnqueue(t *testing.T) {\n\tr := setup(t)\n\tclient := NewClient(getRedisConnOpt(t))\n\tdefer client.Close()\n\ttestClientEnqueue(t, client, r)\n}\n\nfunc TestClientFromRedisClientEnqueue(t *testing.T) {\n\tr := setup(t)\n\tredisClient := getRedisConnOpt(t).MakeRedisClient().(redis.UniversalClient)\n\tclient := NewClientFromRedisClient(redisClient)\n\ttestClientEnqueue(t, client, r)\n\terr := client.Close()\n\tif err == nil {\n\t\tt.Error(\"client.Close() should have failed because of a shared client but it didn't\")\n\t}\n}\n\nfunc TestClientEnqueueWithGroupOption(t *testing.T) {\n\tr := setup(t)\n\tclient := NewClient(getRedisConnOpt(t))\n\tdefer client.Close()\n\n\ttask := NewTask(\"mytask\", []byte(\"foo\"))\n\tnow := time.Now()\n\n\ttests := []struct {\n\t\tdesc          string\n\t\ttask          *Task\n\t\topts          []Option\n\t\twantInfo      *TaskInfo\n\t\twantPending   map[string][]*base.TaskMessage\n\t\twantGroups    map[string]map[string][]base.Z // map queue name to a set of groups\n\t\twantScheduled map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tdesc: \"With only Group option\",\n\t\t\ttask: task,\n\t\t\topts: []Option{\n\t\t\t\tGroup(\"mygroup\"),\n\t\t\t},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"default\",\n\t\t\t\tGroup:         \"mygroup\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStateAggregating,\n\t\t\t\tMaxRetry:      defaultMaxRetry,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: time.Time{},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {}, // should not be pending\n\t\t\t},\n\t\t\twantGroups: map[string]map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t\"mygroup\": {\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tMessage: &base.TaskMessage{\n\t\t\t\t\t\t\t\tType:     task.Type(),\n\t\t\t\t\t\t\t\tPayload:  task.Payload(),\n\t\t\t\t\t\t\t\tRetry:    defaultMaxRetry,\n\t\t\t\t\t\t\t\tQueue:    \"default\",\n\t\t\t\t\t\t\t\tTimeout:  int64(defaultTimeout.Seconds()),\n\t\t\t\t\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t\t\t\t\t\tGroupKey: \"mygroup\",\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tScore: now.Unix(),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"With Group and ProcessAt options\",\n\t\t\ttask: task,\n\t\t\topts: []Option{\n\t\t\t\tGroup(\"mygroup\"),\n\t\t\t\tProcessAt(now.Add(30 * time.Minute)),\n\t\t\t},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"default\",\n\t\t\t\tGroup:         \"mygroup\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStateScheduled,\n\t\t\t\tMaxRetry:      defaultMaxRetry,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: now.Add(30 * time.Minute),\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {}, // should not be pending\n\t\t\t},\n\t\t\twantGroups: map[string]map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t\"mygroup\": {}, // should not be added to the group yet\n\t\t\t\t},\n\t\t\t},\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tMessage: &base.TaskMessage{\n\t\t\t\t\t\t\tType:     task.Type(),\n\t\t\t\t\t\t\tPayload:  task.Payload(),\n\t\t\t\t\t\t\tRetry:    defaultMaxRetry,\n\t\t\t\t\t\t\tQueue:    \"default\",\n\t\t\t\t\t\t\tTimeout:  int64(defaultTimeout.Seconds()),\n\t\t\t\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t\t\t\t\tGroupKey: \"mygroup\",\n\t\t\t\t\t\t},\n\t\t\t\t\t\tScore: now.Add(30 * time.Minute).Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r) // clean up db before each test case.\n\n\t\tgotInfo, err := client.Enqueue(tc.task, tc.opts...)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\tcontinue\n\t\t}\n\t\tcmpOptions := []cmp.Option{\n\t\t\tcmpopts.IgnoreFields(TaskInfo{}, \"ID\"),\n\t\t\tcmpopts.EquateApproxTime(500 * time.Millisecond),\n\t\t}\n\t\tif diff := cmp.Diff(tc.wantInfo, gotInfo, cmpOptions...); diff != \"\" {\n\t\t\tt.Errorf(\"%s;\\nEnqueue(task) returned %v, want %v; (-want,+got)\\n%s\",\n\t\t\t\ttc.desc, gotInfo, tc.wantInfo, diff)\n\t\t}\n\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgot := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, got, h.IgnoreIDOpt, cmpopts.EquateEmpty()); diff != \"\" {\n\t\t\t\tt.Errorf(\"%s;\\nmismatch found in %q; (-want,+got)\\n%s\", tc.desc, base.PendingKey(qname), diff)\n\t\t\t}\n\t\t}\n\n\t\tfor qname, groups := range tc.wantGroups {\n\t\t\tfor groupKey, want := range groups {\n\t\t\t\tgot := h.GetGroupEntries(t, r, qname, groupKey)\n\t\t\t\tif diff := cmp.Diff(want, got, h.IgnoreIDOpt, cmpopts.EquateEmpty()); diff != \"\" {\n\t\t\t\t\tt.Errorf(\"%s;\\nmismatch found in %q; (-want,+got)\\n%s\", tc.desc, base.GroupKey(qname, groupKey), diff)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tfor qname, want := range tc.wantScheduled {\n\t\t\tgotScheduled := h.GetScheduledEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotScheduled, h.IgnoreIDOpt, cmpopts.EquateEmpty()); diff != \"\" {\n\t\t\t\tt.Errorf(\"%s;\\nmismatch found in %q; (-want,+got)\\n%s\", tc.desc, base.ScheduledKey(qname), diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestClientEnqueueWithTaskIDOption(t *testing.T) {\n\tr := setup(t)\n\tclient := NewClient(getRedisConnOpt(t))\n\tdefer client.Close()\n\n\ttask := NewTask(\"send_email\", nil)\n\tnow := time.Now()\n\n\ttests := []struct {\n\t\tdesc        string\n\t\ttask        *Task\n\t\topts        []Option\n\t\twantInfo    *TaskInfo\n\t\twantPending map[string][]*base.TaskMessage\n\t}{\n\t\t{\n\t\t\tdesc: \"With a valid TaskID option\",\n\t\t\ttask: task,\n\t\t\topts: []Option{\n\t\t\t\tTaskID(\"custom_id\"),\n\t\t\t},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tID:            \"custom_id\",\n\t\t\t\tQueue:         \"default\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStatePending,\n\t\t\t\tMaxRetry:      defaultMaxRetry,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: now,\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tID:       \"custom_id\",\n\t\t\t\t\t\tType:     task.Type(),\n\t\t\t\t\t\tPayload:  task.Payload(),\n\t\t\t\t\t\tRetry:    defaultMaxRetry,\n\t\t\t\t\t\tQueue:    \"default\",\n\t\t\t\t\t\tTimeout:  int64(defaultTimeout.Seconds()),\n\t\t\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r) // clean up db before each test case.\n\n\t\tgotInfo, err := client.Enqueue(tc.task, tc.opts...)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"got non-nil error %v, want nil\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\tcmpOptions := []cmp.Option{\n\t\t\tcmpopts.EquateApproxTime(500 * time.Millisecond),\n\t\t}\n\t\tif diff := cmp.Diff(tc.wantInfo, gotInfo, cmpOptions...); diff != \"\" {\n\t\t\tt.Errorf(\"%s;\\nEnqueue(task) returned %v, want %v; (-want,+got)\\n%s\",\n\t\t\t\ttc.desc, gotInfo, tc.wantInfo, diff)\n\t\t}\n\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgot := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, got); diff != \"\" {\n\t\t\t\tt.Errorf(\"%s;\\nmismatch found in %q; (-want,+got)\\n%s\", tc.desc, base.PendingKey(qname), diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestClientEnqueueWithConflictingTaskID(t *testing.T) {\n\tsetup(t)\n\tclient := NewClient(getRedisConnOpt(t))\n\tdefer client.Close()\n\n\tconst taskID = \"custom_id\"\n\ttask := NewTask(\"foo\", nil)\n\n\tif _, err := client.Enqueue(task, TaskID(taskID)); err != nil {\n\t\tt.Fatalf(\"First task: Enqueue failed: %v\", err)\n\t}\n\t_, err := client.Enqueue(task, TaskID(taskID))\n\tif !errors.Is(err, ErrTaskIDConflict) {\n\t\tt.Errorf(\"Second task: Enqueue returned %v, want %v\", err, ErrTaskIDConflict)\n\t}\n}\n\nfunc TestClientEnqueueWithProcessInOption(t *testing.T) {\n\tr := setup(t)\n\tclient := NewClient(getRedisConnOpt(t))\n\tdefer client.Close()\n\n\ttask := NewTask(\"send_email\", h.JSON(map[string]interface{}{\"to\": \"customer@gmail.com\", \"from\": \"merchant@example.com\"}))\n\tnow := time.Now()\n\n\ttests := []struct {\n\t\tdesc          string\n\t\ttask          *Task\n\t\tdelay         time.Duration // value for ProcessIn option\n\t\topts          []Option      // other options\n\t\twantInfo      *TaskInfo\n\t\twantPending   map[string][]*base.TaskMessage\n\t\twantScheduled map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tdesc:  \"schedule a task to be processed in one hour\",\n\t\t\ttask:  task,\n\t\t\tdelay: 1 * time.Hour,\n\t\t\topts:  []Option{},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"default\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStateScheduled,\n\t\t\t\tMaxRetry:      defaultMaxRetry,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: time.Now().Add(1 * time.Hour),\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tMessage: &base.TaskMessage{\n\t\t\t\t\t\t\tType:     task.Type(),\n\t\t\t\t\t\t\tPayload:  task.Payload(),\n\t\t\t\t\t\t\tRetry:    defaultMaxRetry,\n\t\t\t\t\t\t\tQueue:    \"default\",\n\t\t\t\t\t\t\tTimeout:  int64(defaultTimeout.Seconds()),\n\t\t\t\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t\t\t\t},\n\t\t\t\t\t\tScore: time.Now().Add(time.Hour).Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc:  \"Zero delay\",\n\t\t\ttask:  task,\n\t\t\tdelay: 0,\n\t\t\topts:  []Option{},\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"default\",\n\t\t\t\tType:          task.Type(),\n\t\t\t\tPayload:       task.Payload(),\n\t\t\t\tState:         TaskStatePending,\n\t\t\t\tMaxRetry:      defaultMaxRetry,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: now,\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tType:     task.Type(),\n\t\t\t\t\t\tPayload:  task.Payload(),\n\t\t\t\t\t\tRetry:    defaultMaxRetry,\n\t\t\t\t\t\tQueue:    \"default\",\n\t\t\t\t\t\tTimeout:  int64(defaultTimeout.Seconds()),\n\t\t\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r) // clean up db before each test case.\n\n\t\topts := append(tc.opts, ProcessIn(tc.delay))\n\t\tgotInfo, err := client.Enqueue(tc.task, opts...)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\tcontinue\n\t\t}\n\t\tcmpOptions := []cmp.Option{\n\t\t\tcmpopts.IgnoreFields(TaskInfo{}, \"ID\"),\n\t\t\tcmpopts.EquateApproxTime(500 * time.Millisecond),\n\t\t}\n\t\tif diff := cmp.Diff(tc.wantInfo, gotInfo, cmpOptions...); diff != \"\" {\n\t\t\tt.Errorf(\"%s;\\nEnqueue(task, ProcessIn(%v)) returned %v, want %v; (-want,+got)\\n%s\",\n\t\t\t\ttc.desc, tc.delay, gotInfo, tc.wantInfo, diff)\n\t\t}\n\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgotPending := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotPending, h.IgnoreIDOpt, cmpopts.EquateEmpty()); diff != \"\" {\n\t\t\t\tt.Errorf(\"%s;\\nmismatch found in %q; (-want,+got)\\n%s\", tc.desc, base.PendingKey(qname), diff)\n\t\t\t}\n\t\t}\n\t\tfor qname, want := range tc.wantScheduled {\n\t\t\tgotScheduled := h.GetScheduledEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotScheduled, h.IgnoreIDOpt, cmpopts.EquateEmpty()); diff != \"\" {\n\t\t\t\tt.Errorf(\"%s;\\nmismatch found in %q; (-want,+got)\\n%s\", tc.desc, base.ScheduledKey(qname), diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestClientEnqueueError(t *testing.T) {\n\tr := setup(t)\n\tclient := NewClient(getRedisConnOpt(t))\n\tdefer client.Close()\n\n\ttask := NewTask(\"send_email\", h.JSON(map[string]interface{}{\"to\": \"customer@gmail.com\", \"from\": \"merchant@example.com\"}))\n\n\ttests := []struct {\n\t\tdesc string\n\t\ttask *Task\n\t\topts []Option\n\t}{\n\t\t{\n\t\t\tdesc: \"With nil task\",\n\t\t\ttask: nil,\n\t\t\topts: []Option{},\n\t\t},\n\t\t{\n\t\t\tdesc: \"With empty queue name\",\n\t\t\ttask: task,\n\t\t\topts: []Option{\n\t\t\t\tQueue(\"\"),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"With empty task typename\",\n\t\t\ttask: NewTask(\"\", h.JSON(map[string]interface{}{})),\n\t\t\topts: []Option{},\n\t\t},\n\t\t{\n\t\t\tdesc: \"With blank task typename\",\n\t\t\ttask: NewTask(\"    \", h.JSON(map[string]interface{}{})),\n\t\t\topts: []Option{},\n\t\t},\n\t\t{\n\t\t\tdesc: \"With empty task ID\",\n\t\t\ttask: NewTask(\"foo\", nil),\n\t\t\topts: []Option{TaskID(\"\")},\n\t\t},\n\t\t{\n\t\t\tdesc: \"With blank task ID\",\n\t\t\ttask: NewTask(\"foo\", nil),\n\t\t\topts: []Option{TaskID(\"  \")},\n\t\t},\n\t\t{\n\t\t\tdesc: \"With unique option less than 1s\",\n\t\t\ttask: NewTask(\"foo\", nil),\n\t\t\topts: []Option{Unique(300 * time.Millisecond)},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\n\t\t_, err := client.Enqueue(tc.task, tc.opts...)\n\t\tif err == nil {\n\t\t\tt.Errorf(\"%s; client.Enqueue(task, opts...) did not return non-nil error\", tc.desc)\n\t\t}\n\t}\n}\n\nfunc TestClientWithDefaultOptions(t *testing.T) {\n\tr := setup(t)\n\n\tnow := time.Now()\n\n\ttests := []struct {\n\t\tdesc        string\n\t\tdefaultOpts []Option // options set at task initialization time\n\t\topts        []Option // options used at enqueue time.\n\t\ttasktype    string\n\t\tpayload     []byte\n\t\twantInfo    *TaskInfo\n\t\tqueue       string // queue that the message should go into.\n\t\twant        *base.TaskMessage\n\t}{\n\t\t{\n\t\t\tdesc:        \"With queue routing option\",\n\t\t\tdefaultOpts: []Option{Queue(\"feed\")},\n\t\t\topts:        []Option{},\n\t\t\ttasktype:    \"feed:import\",\n\t\t\tpayload:     nil,\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"feed\",\n\t\t\t\tType:          \"feed:import\",\n\t\t\t\tPayload:       nil,\n\t\t\t\tState:         TaskStatePending,\n\t\t\t\tMaxRetry:      defaultMaxRetry,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: now,\n\t\t\t},\n\t\t\tqueue: \"feed\",\n\t\t\twant: &base.TaskMessage{\n\t\t\t\tType:     \"feed:import\",\n\t\t\t\tPayload:  nil,\n\t\t\t\tRetry:    defaultMaxRetry,\n\t\t\t\tQueue:    \"feed\",\n\t\t\t\tTimeout:  int64(defaultTimeout.Seconds()),\n\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc:        \"With multiple options\",\n\t\t\tdefaultOpts: []Option{Queue(\"feed\"), MaxRetry(5)},\n\t\t\topts:        []Option{},\n\t\t\ttasktype:    \"feed:import\",\n\t\t\tpayload:     nil,\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"feed\",\n\t\t\t\tType:          \"feed:import\",\n\t\t\t\tPayload:       nil,\n\t\t\t\tState:         TaskStatePending,\n\t\t\t\tMaxRetry:      5,\n\t\t\t\tRetried:       0,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: now,\n\t\t\t},\n\t\t\tqueue: \"feed\",\n\t\t\twant: &base.TaskMessage{\n\t\t\t\tType:     \"feed:import\",\n\t\t\t\tPayload:  nil,\n\t\t\t\tRetry:    5,\n\t\t\t\tQueue:    \"feed\",\n\t\t\t\tTimeout:  int64(defaultTimeout.Seconds()),\n\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc:        \"With overriding options at enqueue time\",\n\t\t\tdefaultOpts: []Option{Queue(\"feed\"), MaxRetry(5)},\n\t\t\topts:        []Option{Queue(\"critical\")},\n\t\t\ttasktype:    \"feed:import\",\n\t\t\tpayload:     nil,\n\t\t\twantInfo: &TaskInfo{\n\t\t\t\tQueue:         \"critical\",\n\t\t\t\tType:          \"feed:import\",\n\t\t\t\tPayload:       nil,\n\t\t\t\tState:         TaskStatePending,\n\t\t\t\tMaxRetry:      5,\n\t\t\t\tLastErr:       \"\",\n\t\t\t\tLastFailedAt:  time.Time{},\n\t\t\t\tTimeout:       defaultTimeout,\n\t\t\t\tDeadline:      time.Time{},\n\t\t\t\tNextProcessAt: now,\n\t\t\t},\n\t\t\tqueue: \"critical\",\n\t\t\twant: &base.TaskMessage{\n\t\t\t\tType:     \"feed:import\",\n\t\t\t\tPayload:  nil,\n\t\t\t\tRetry:    5,\n\t\t\t\tQueue:    \"critical\",\n\t\t\t\tTimeout:  int64(defaultTimeout.Seconds()),\n\t\t\t\tDeadline: noDeadline.Unix(),\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\tc := NewClient(getRedisConnOpt(t))\n\t\tdefer c.Close()\n\t\ttask := NewTask(tc.tasktype, tc.payload, tc.defaultOpts...)\n\t\tgotInfo, err := c.Enqueue(task, tc.opts...)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tcmpOptions := []cmp.Option{\n\t\t\tcmpopts.IgnoreFields(TaskInfo{}, \"ID\"),\n\t\t\tcmpopts.EquateApproxTime(500 * time.Millisecond),\n\t\t}\n\t\tif diff := cmp.Diff(tc.wantInfo, gotInfo, cmpOptions...); diff != \"\" {\n\t\t\tt.Errorf(\"%s;\\nEnqueue(task, opts...) returned %v, want %v; (-want,+got)\\n%s\",\n\t\t\t\ttc.desc, gotInfo, tc.wantInfo, diff)\n\t\t}\n\t\tpending := h.GetPendingMessages(t, r, tc.queue)\n\t\tif len(pending) != 1 {\n\t\t\tt.Errorf(\"%s;\\nexpected queue %q to have one message; got %d messages in the queue.\",\n\t\t\t\ttc.desc, tc.queue, len(pending))\n\t\t\tcontinue\n\t\t}\n\t\tgot := pending[0]\n\t\tif diff := cmp.Diff(tc.want, got, h.IgnoreIDOpt); diff != \"\" {\n\t\t\tt.Errorf(\"%s;\\nmismatch found in pending task message; (-want,+got)\\n%s\",\n\t\t\t\ttc.desc, diff)\n\t\t}\n\t}\n}\n\nfunc TestClientEnqueueUnique(t *testing.T) {\n\tr := setup(t)\n\tc := NewClient(getRedisConnOpt(t))\n\tdefer c.Close()\n\n\ttests := []struct {\n\t\ttask *Task\n\t\tttl  time.Duration\n\t}{\n\t\t{\n\t\t\tNewTask(\"email\", h.JSON(map[string]interface{}{\"user_id\": 123})),\n\t\t\ttime.Hour,\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r) // clean up db before each test case.\n\n\t\t// Enqueue the task first. It should succeed.\n\t\t_, err := c.Enqueue(tc.task, Unique(tc.ttl))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tgotTTL := r.TTL(context.Background(), base.UniqueKey(base.DefaultQueueName, tc.task.Type(), tc.task.Payload())).Val()\n\t\tif !cmp.Equal(tc.ttl.Seconds(), gotTTL.Seconds(), cmpopts.EquateApprox(0, 1)) {\n\t\t\tt.Errorf(\"TTL = %v, want %v\", gotTTL, tc.ttl)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Enqueue the task again. It should fail.\n\t\t_, err = c.Enqueue(tc.task, Unique(tc.ttl))\n\t\tif err == nil {\n\t\t\tt.Errorf(\"Enqueueing %+v did not return an error\", tc.task)\n\t\t\tcontinue\n\t\t}\n\t\tif !errors.Is(err, ErrDuplicateTask) {\n\t\t\tt.Errorf(\"Enqueueing %+v returned an error that is not ErrDuplicateTask\", tc.task)\n\t\t\tcontinue\n\t\t}\n\t}\n}\n\nfunc TestClientEnqueueUniqueWithProcessInOption(t *testing.T) {\n\tr := setup(t)\n\tc := NewClient(getRedisConnOpt(t))\n\tdefer c.Close()\n\n\ttests := []struct {\n\t\ttask *Task\n\t\td    time.Duration\n\t\tttl  time.Duration\n\t}{\n\t\t{\n\t\t\tNewTask(\"reindex\", nil),\n\t\t\ttime.Hour,\n\t\t\t10 * time.Minute,\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r) // clean up db before each test case.\n\n\t\t// Enqueue the task first. It should succeed.\n\t\t_, err := c.Enqueue(tc.task, ProcessIn(tc.d), Unique(tc.ttl))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tgotTTL := r.TTL(context.Background(), base.UniqueKey(base.DefaultQueueName, tc.task.Type(), tc.task.Payload())).Val()\n\t\twantTTL := time.Duration(tc.ttl.Seconds()+tc.d.Seconds()) * time.Second\n\t\tif !cmp.Equal(wantTTL.Seconds(), gotTTL.Seconds(), cmpopts.EquateApprox(0, 1)) {\n\t\t\tt.Errorf(\"TTL = %v, want %v\", gotTTL, wantTTL)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Enqueue the task again. It should fail.\n\t\t_, err = c.Enqueue(tc.task, ProcessIn(tc.d), Unique(tc.ttl))\n\t\tif err == nil {\n\t\t\tt.Errorf(\"Enqueueing %+v did not return an error\", tc.task)\n\t\t\tcontinue\n\t\t}\n\t\tif !errors.Is(err, ErrDuplicateTask) {\n\t\t\tt.Errorf(\"Enqueueing %+v returned an error that is not ErrDuplicateTask\", tc.task)\n\t\t\tcontinue\n\t\t}\n\t}\n}\n\nfunc TestClientEnqueueUniqueWithProcessAtOption(t *testing.T) {\n\tr := setup(t)\n\tc := NewClient(getRedisConnOpt(t))\n\tdefer c.Close()\n\n\ttests := []struct {\n\t\ttask *Task\n\t\tat   time.Time\n\t\tttl  time.Duration\n\t}{\n\t\t{\n\t\t\tNewTask(\"reindex\", nil),\n\t\t\ttime.Now().Add(time.Hour),\n\t\t\t10 * time.Minute,\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r) // clean up db before each test case.\n\n\t\t// Enqueue the task first. It should succeed.\n\t\t_, err := c.Enqueue(tc.task, ProcessAt(tc.at), Unique(tc.ttl))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tgotTTL := r.TTL(context.Background(), base.UniqueKey(base.DefaultQueueName, tc.task.Type(), tc.task.Payload())).Val()\n\t\twantTTL := time.Until(tc.at.Add(tc.ttl))\n\t\tif !cmp.Equal(wantTTL.Seconds(), gotTTL.Seconds(), cmpopts.EquateApprox(0, 1)) {\n\t\t\tt.Errorf(\"TTL = %v, want %v\", gotTTL, wantTTL)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Enqueue the task again. It should fail.\n\t\t_, err = c.Enqueue(tc.task, ProcessAt(tc.at), Unique(tc.ttl))\n\t\tif err == nil {\n\t\t\tt.Errorf(\"Enqueueing %+v did not return an error\", tc.task)\n\t\t\tcontinue\n\t\t}\n\t\tif !errors.Is(err, ErrDuplicateTask) {\n\t\t\tt.Errorf(\"Enqueueing %+v returned an error that is not ErrDuplicateTask\", tc.task)\n\t\t\tcontinue\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "context.go",
          "type": "blob",
          "size": 1.27734375,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\n\tasynqcontext \"github.com/hibiken/asynq/internal/context\"\n)\n\n// GetTaskID extracts a task ID from a context, if any.\n//\n// ID of a task is guaranteed to be unique.\n// ID of a task doesn't change if the task is being retried.\nfunc GetTaskID(ctx context.Context) (id string, ok bool) {\n\treturn asynqcontext.GetTaskID(ctx)\n}\n\n// GetRetryCount extracts retry count from a context, if any.\n//\n// Return value n indicates the number of times associated task has been\n// retried so far.\nfunc GetRetryCount(ctx context.Context) (n int, ok bool) {\n\treturn asynqcontext.GetRetryCount(ctx)\n}\n\n// GetMaxRetry extracts maximum retry from a context, if any.\n//\n// Return value n indicates the maximum number of times the associated task\n// can be retried if ProcessTask returns a non-nil error.\nfunc GetMaxRetry(ctx context.Context) (n int, ok bool) {\n\treturn asynqcontext.GetMaxRetry(ctx)\n}\n\n// GetQueueName extracts queue name from a context, if any.\n//\n// Return value queue indicates which queue the task was pulled from.\nfunc GetQueueName(ctx context.Context) (queue string, ok bool) {\n\treturn asynqcontext.GetQueueName(ctx)\n}\n"
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 2.1259765625,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\n/*\nPackage asynq provides a framework for Redis based distrubted task queue.\n\nAsynq uses Redis as a message broker. To connect to redis,\nspecify the connection using one of RedisConnOpt types.\n\n    redisConnOpt = asynq.RedisClientOpt{\n        Addr:     \"127.0.0.1:6379\",\n        Password: \"xxxxx\",\n        DB:       2,\n    }\n\nThe Client is used to enqueue a task.\n\n\n    client := asynq.NewClient(redisConnOpt)\n\n    // Task is created with two parameters: its type and payload.\n    // Payload data is simply an array of bytes. It can be encoded in JSON, Protocol Buffer, Gob, etc.\n    b, err := json.Marshal(ExamplePayload{UserID: 42})\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    task := asynq.NewTask(\"example\", b)\n\n    // Enqueue the task to be processed immediately.\n    info, err := client.Enqueue(task)\n\n    // Schedule the task to be processed after one minute.\n    info, err = client.Enqueue(t, asynq.ProcessIn(1*time.Minute))\n\nThe Server is used to run the task processing workers with a given\nhandler.\n    srv := asynq.NewServer(redisConnOpt, asynq.Config{\n        Concurrency: 10,\n    })\n\n    if err := srv.Run(handler); err != nil {\n        log.Fatal(err)\n    }\n\nHandler is an interface type with a method which\ntakes a task and returns an error. Handler should return nil if\nthe processing is successful, otherwise return a non-nil error.\nIf handler panics or returns a non-nil error, the task will be retried in the future.\n\nExample of a type that implements the Handler interface.\n    type TaskHandler struct {\n        // ...\n    }\n\n    func (h *TaskHandler) ProcessTask(ctx context.Context, task *asynq.Task) error {\n        switch task.Type {\n        case \"example\":\n            var data ExamplePayload\n            if err := json.Unmarshal(task.Payload(), &data); err != nil {\n                return err\n            }\n            // perform task with the data\n\n        default:\n            return fmt.Errorf(\"unexpected task type %q\", task.Type)\n        }\n        return nil\n    }\n*/\npackage asynq\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "example_test.go",
          "type": "blob",
          "size": 2.732421875,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\t\"os/signal\"\n\t\"time\"\n\n\t\"github.com/hibiken/asynq\"\n\t\"golang.org/x/sys/unix\"\n)\n\nfunc ExampleServer_Run() {\n\tsrv := asynq.NewServer(\n\t\tasynq.RedisClientOpt{Addr: \":6379\"},\n\t\tasynq.Config{Concurrency: 20},\n\t)\n\n\th := asynq.NewServeMux()\n\t// ... Register handlers\n\n\t// Run blocks and waits for os signal to terminate the program.\n\tif err := srv.Run(h); err != nil {\n\t\tlog.Fatal(err)\n\t}\n}\n\nfunc ExampleServer_Shutdown() {\n\tsrv := asynq.NewServer(\n\t\tasynq.RedisClientOpt{Addr: \":6379\"},\n\t\tasynq.Config{Concurrency: 20},\n\t)\n\n\th := asynq.NewServeMux()\n\t// ... Register handlers\n\n\tif err := srv.Start(h); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tsigs := make(chan os.Signal, 1)\n\tsignal.Notify(sigs, unix.SIGTERM, unix.SIGINT)\n\t<-sigs // wait for termination signal\n\n\tsrv.Shutdown()\n}\n\nfunc ExampleServer_Stop() {\n\tsrv := asynq.NewServer(\n\t\tasynq.RedisClientOpt{Addr: \":6379\"},\n\t\tasynq.Config{Concurrency: 20},\n\t)\n\n\th := asynq.NewServeMux()\n\t// ... Register handlers\n\n\tif err := srv.Start(h); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tsigs := make(chan os.Signal, 1)\n\tsignal.Notify(sigs, unix.SIGTERM, unix.SIGINT, unix.SIGTSTP)\n\t// Handle SIGTERM, SIGINT to exit the program.\n\t// Handle SIGTSTP to stop processing new tasks.\n\tfor {\n\t\ts := <-sigs\n\t\tif s == unix.SIGTSTP {\n\t\t\tsrv.Stop() // stop processing new tasks\n\t\t\tcontinue\n\t\t}\n\t\tbreak // received SIGTERM or SIGINT signal\n\t}\n\n\tsrv.Shutdown()\n}\n\nfunc ExampleScheduler() {\n\tscheduler := asynq.NewScheduler(\n\t\tasynq.RedisClientOpt{Addr: \":6379\"},\n\t\t&asynq.SchedulerOpts{Location: time.Local},\n\t)\n\n\tif _, err := scheduler.Register(\"* * * * *\", asynq.NewTask(\"task1\", nil)); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tif _, err := scheduler.Register(\"@every 30s\", asynq.NewTask(\"task2\", nil)); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Run blocks and waits for os signal to terminate the program.\n\tif err := scheduler.Run(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n}\n\nfunc ExampleParseRedisURI() {\n\trconn, err := asynq.ParseRedisURI(\"redis://localhost:6379/10\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tr, ok := rconn.(asynq.RedisClientOpt)\n\tif !ok {\n\t\tlog.Fatal(\"unexpected type\")\n\t}\n\tfmt.Println(r.Addr)\n\tfmt.Println(r.DB)\n\t// Output:\n\t// localhost:6379\n\t// 10\n}\n\nfunc ExampleResultWriter() {\n\t// ResultWriter is only accessible in Handler.\n\th := func(ctx context.Context, task *asynq.Task) error {\n\t\t// .. do task processing work\n\n\t\tres := []byte(\"task result data\")\n\t\tn, err := task.ResultWriter().Write(res) // implements io.Writer\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to write task result: %v\", err)\n\t\t}\n\t\tlog.Printf(\" %d bytes written\", n)\n\t\treturn nil\n\t}\n\n\t_ = h\n}\n"
        },
        {
          "name": "forwarder.go",
          "type": "blob",
          "size": 1.673828125,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/log\"\n)\n\n// A forwarder is responsible for moving scheduled and retry tasks to pending state\n// so that the tasks get processed by the workers.\ntype forwarder struct {\n\tlogger *log.Logger\n\tbroker base.Broker\n\n\t// channel to communicate back to the long running \"forwarder\" goroutine.\n\tdone chan struct{}\n\n\t// list of queue names to check and enqueue.\n\tqueues []string\n\n\t// poll interval on average\n\tavgInterval time.Duration\n}\n\ntype forwarderParams struct {\n\tlogger   *log.Logger\n\tbroker   base.Broker\n\tqueues   []string\n\tinterval time.Duration\n}\n\nfunc newForwarder(params forwarderParams) *forwarder {\n\treturn &forwarder{\n\t\tlogger:      params.logger,\n\t\tbroker:      params.broker,\n\t\tdone:        make(chan struct{}),\n\t\tqueues:      params.queues,\n\t\tavgInterval: params.interval,\n\t}\n}\n\nfunc (f *forwarder) shutdown() {\n\tf.logger.Debug(\"Forwarder shutting down...\")\n\t// Signal the forwarder goroutine to stop polling.\n\tf.done <- struct{}{}\n}\n\n// start starts the \"forwarder\" goroutine.\nfunc (f *forwarder) start(wg *sync.WaitGroup) {\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\ttimer := time.NewTimer(f.avgInterval)\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-f.done:\n\t\t\t\tf.logger.Debug(\"Forwarder done\")\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t\tf.exec()\n\t\t\t\ttimer.Reset(f.avgInterval)\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc (f *forwarder) exec() {\n\tif err := f.broker.ForwardIfReady(f.queues...); err != nil {\n\t\tf.logger.Errorf(\"Failed to forward scheduled tasks: %v\", err)\n\t}\n}\n"
        },
        {
          "name": "forwarder_test.go",
          "type": "blob",
          "size": 4.2275390625,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/rdb\"\n\th \"github.com/hibiken/asynq/internal/testutil\"\n)\n\nfunc TestForwarder(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\trdbClient := rdb.NewRDB(r)\n\tconst pollInterval = time.Second\n\ts := newForwarder(forwarderParams{\n\t\tlogger:   testLogger,\n\t\tbroker:   rdbClient,\n\t\tqueues:   []string{\"default\", \"critical\"},\n\t\tinterval: pollInterval,\n\t})\n\tt1 := h.NewTaskMessageWithQueue(\"gen_thumbnail\", nil, \"default\")\n\tt2 := h.NewTaskMessageWithQueue(\"send_email\", nil, \"critical\")\n\tt3 := h.NewTaskMessageWithQueue(\"reindex\", nil, \"default\")\n\tt4 := h.NewTaskMessageWithQueue(\"sync\", nil, \"critical\")\n\tnow := time.Now()\n\n\ttests := []struct {\n\t\tinitScheduled map[string][]base.Z            // scheduled queue initial state\n\t\tinitRetry     map[string][]base.Z            // retry queue initial state\n\t\tinitPending   map[string][]*base.TaskMessage // default queue initial state\n\t\twait          time.Duration                  // wait duration before checking for final state\n\t\twantScheduled map[string][]*base.TaskMessage // schedule queue final state\n\t\twantRetry     map[string][]*base.TaskMessage // retry queue final state\n\t\twantPending   map[string][]*base.TaskMessage // default queue final state\n\t}{\n\t\t{\n\t\t\tinitScheduled: map[string][]base.Z{\n\t\t\t\t\"default\":  {{Message: t1, Score: now.Add(time.Hour).Unix()}},\n\t\t\t\t\"critical\": {{Message: t2, Score: now.Add(-2 * time.Second).Unix()}},\n\t\t\t},\n\t\t\tinitRetry: map[string][]base.Z{\n\t\t\t\t\"default\":  {{Message: t3, Score: time.Now().Add(-500 * time.Millisecond).Unix()}},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\tinitPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {t4},\n\t\t\t},\n\t\t\twait: pollInterval * 2,\n\t\t\twantScheduled: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {t1},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\twantRetry: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {t3},\n\t\t\t\t\"critical\": {t2, t4},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinitScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{Message: t1, Score: now.Unix()},\n\t\t\t\t\t{Message: t3, Score: now.Add(-500 * time.Millisecond).Unix()},\n\t\t\t\t},\n\t\t\t\t\"critical\": {\n\t\t\t\t\t{Message: t2, Score: now.Add(-2 * time.Second).Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t\tinitRetry: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\tinitPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {t4},\n\t\t\t},\n\t\t\twait: pollInterval * 2,\n\t\t\twantScheduled: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\twantRetry: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {t1, t3},\n\t\t\t\t\"critical\": {t2, t4},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)                                  // clean up db before each test case.\n\t\th.SeedAllScheduledQueues(t, r, tc.initScheduled) // initialize scheduled queue\n\t\th.SeedAllRetryQueues(t, r, tc.initRetry)         // initialize retry queue\n\t\th.SeedAllPendingQueues(t, r, tc.initPending)     // initialize default queue\n\n\t\tvar wg sync.WaitGroup\n\t\ts.start(&wg)\n\t\ttime.Sleep(tc.wait)\n\t\ts.shutdown()\n\n\t\tfor qname, want := range tc.wantScheduled {\n\t\t\tgotScheduled := h.GetScheduledMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotScheduled, h.SortMsgOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"mismatch found in %q after running forwarder: (-want, +got)\\n%s\", base.ScheduledKey(qname), diff)\n\t\t\t}\n\t\t}\n\n\t\tfor qname, want := range tc.wantRetry {\n\t\t\tgotRetry := h.GetRetryMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotRetry, h.SortMsgOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"mismatch found in %q after running forwarder: (-want, +got)\\n%s\", base.RetryKey(qname), diff)\n\t\t\t}\n\t\t}\n\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgotPending := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotPending, h.SortMsgOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"mismatch found in %q after running forwarder: (-want, +got)\\n%s\", base.PendingKey(qname), diff)\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.4658203125,
          "content": "module github.com/hibiken/asynq\n\ngo 1.22\n\nrequire (\n\tgithub.com/google/go-cmp v0.6.0\n\tgithub.com/google/uuid v1.6.0\n\tgithub.com/redis/go-redis/v9 v9.7.0\n\tgithub.com/robfig/cron/v3 v3.0.1\n\tgithub.com/spf13/cast v1.7.0\n\tgo.uber.org/goleak v1.3.0\n\tgolang.org/x/sys v0.27.0\n\tgolang.org/x/time v0.8.0\n\tgoogle.golang.org/protobuf v1.35.2\n)\n\nrequire (\n\tgithub.com/cespare/xxhash/v2 v2.2.0 // indirect\n\tgithub.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 3.4794921875,
          "content": "github.com/bsm/ginkgo/v2 v2.12.0 h1:Ny8MWAHyOepLGlLKYmXG4IEkioBysk6GpaRTLC8zwWs=\ngithub.com/bsm/ginkgo/v2 v2.12.0/go.mod h1:SwYbGRRDovPVboqFv0tPTcG1sN61LM1Z4ARdbAV9g4c=\ngithub.com/bsm/gomega v1.27.10 h1:yeMWxP2pV2fG3FgAODIY8EiRE3dy0aeFYt4l7wh6yKA=\ngithub.com/bsm/gomega v1.27.10/go.mod h1:JyEr/xRbxbtgWNi8tIEVPUYZ5Dzef52k01W3YH0H+O0=\ngithub.com/cespare/xxhash/v2 v2.2.0 h1:DC2CZ1Ep5Y4k3ZQ899DldepgrayRUGE6BBZ/cd9Cj44=\ngithub.com/cespare/xxhash/v2 v2.2.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f h1:lO4WD4F/rVNCu3HqELle0jiPLLBs70cWOduZpkS1E78=\ngithub.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f/go.mod h1:cuUVRXasLTGF7a8hSLbxyZXjz+1KgoB3wDUb6vlszIc=\ngithub.com/frankban/quicktest v1.14.6 h1:7Xjx+VpznH+oBnejlPUj8oUpdxnVs4f8XU8WnHkI4W8=\ngithub.com/frankban/quicktest v1.14.6/go.mod h1:4ptaffx2x8+WTWXmUCuVU6aPUX1/Mz7zb5vbUoiM6w0=\ngithub.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=\ngithub.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\ngithub.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=\ngithub.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=\ngithub.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=\ngithub.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=\ngithub.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/redis/go-redis/v9 v9.7.0 h1:HhLSs+B6O021gwzl+locl0zEDnyNkxMtf/Z3NNBMa9E=\ngithub.com/redis/go-redis/v9 v9.7.0/go.mod h1:f6zhXITC7JUJIlPEiBOTXxJgPLdZcA93GewI7inzyWw=\ngithub.com/robfig/cron/v3 v3.0.1 h1:WdRxkvbJztn8LMz/QEvLN5sBU+xKpSqwwUO1Pjr4qDs=\ngithub.com/robfig/cron/v3 v3.0.1/go.mod h1:eQICP3HwyT7UooqI/z+Ov+PtYAWygg1TEWWzGIFLtro=\ngithub.com/rogpeppe/go-internal v1.9.0 h1:73kH8U+JUqXU8lRuOHeVHaa/SZPifC7BkcraZVejAe8=\ngithub.com/rogpeppe/go-internal v1.9.0/go.mod h1:WtVeX8xhTBvf0smdhujwtBcq4Qrzq/fJaraNFVN+nFs=\ngithub.com/spf13/cast v1.7.0 h1:ntdiHjuueXFgm5nzDRdOS4yfT43P5Fnud6DH50rz/7w=\ngithub.com/spf13/cast v1.7.0/go.mod h1:ancEpBxwJDODSW/UG4rDrAqiKolqNNh2DX3mk86cAdo=\ngithub.com/stretchr/testify v1.8.0 h1:pSgiaMZlXftHpm5L7V1+rVB+AZJydKsMxsQBIJw4PKk=\ngithub.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=\ngo.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=\ngo.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=\ngolang.org/x/sys v0.27.0 h1:wBqf8DvsY9Y/2P8gAfPDEYNuS30J4lPHJxXSb/nJZ+s=\ngolang.org/x/sys v0.27.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngolang.org/x/time v0.8.0 h1:9i3RxcPv3PZnitoVGMPDKZSq1xW1gK1Xy3ArNOGZfEg=\ngolang.org/x/time v0.8.0/go.mod h1:3BpzKBy/shNhVucY/MWOyx10tF3SFh9QdLuxbVysPQM=\ngoogle.golang.org/protobuf v1.35.2 h1:8Ar7bF+apOIoThw1EdZl0p1oWvMqTHmpA2fRTyZO8io=\ngoogle.golang.org/protobuf v1.35.2/go.mod h1:9fA7Ob0pmnwhb644+1+CVWFRbNajQ6iRojtC/QF5bRE=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n"
        },
        {
          "name": "healthcheck.go",
          "type": "blob",
          "size": 1.716796875,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/log\"\n)\n\n// healthchecker is responsible for pinging broker periodically\n// and call user provided HeathCheckFunc with the ping result.\ntype healthchecker struct {\n\tlogger *log.Logger\n\tbroker base.Broker\n\n\t// channel to communicate back to the long running \"healthchecker\" goroutine.\n\tdone chan struct{}\n\n\t// interval between healthchecks.\n\tinterval time.Duration\n\n\t// function to call periodically.\n\thealthcheckFunc func(error)\n}\n\ntype healthcheckerParams struct {\n\tlogger          *log.Logger\n\tbroker          base.Broker\n\tinterval        time.Duration\n\thealthcheckFunc func(error)\n}\n\nfunc newHealthChecker(params healthcheckerParams) *healthchecker {\n\treturn &healthchecker{\n\t\tlogger:          params.logger,\n\t\tbroker:          params.broker,\n\t\tdone:            make(chan struct{}),\n\t\tinterval:        params.interval,\n\t\thealthcheckFunc: params.healthcheckFunc,\n\t}\n}\n\nfunc (hc *healthchecker) shutdown() {\n\tif hc.healthcheckFunc == nil {\n\t\treturn\n\t}\n\n\thc.logger.Debug(\"Healthchecker shutting down...\")\n\t// Signal the healthchecker goroutine to stop.\n\thc.done <- struct{}{}\n}\n\nfunc (hc *healthchecker) start(wg *sync.WaitGroup) {\n\tif hc.healthcheckFunc == nil {\n\t\treturn\n\t}\n\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\ttimer := time.NewTimer(hc.interval)\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-hc.done:\n\t\t\t\thc.logger.Debug(\"Healthchecker done\")\n\t\t\t\ttimer.Stop()\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t\terr := hc.broker.Ping()\n\t\t\t\thc.healthcheckFunc(err)\n\t\t\t\ttimer.Reset(hc.interval)\n\t\t\t}\n\t\t}\n\t}()\n}\n"
        },
        {
          "name": "healthcheck_test.go",
          "type": "blob",
          "size": 1.974609375,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/hibiken/asynq/internal/rdb\"\n\t\"github.com/hibiken/asynq/internal/testbroker\"\n)\n\nfunc TestHealthChecker(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\trdbClient := rdb.NewRDB(r)\n\n\tvar (\n\t\t// mu guards called and e variables.\n\t\tmu     sync.Mutex\n\t\tcalled int\n\t\te      error\n\t)\n\tcheckFn := func(err error) {\n\t\tmu.Lock()\n\t\tdefer mu.Unlock()\n\t\tcalled++\n\t\te = err\n\t}\n\n\thc := newHealthChecker(healthcheckerParams{\n\t\tlogger:          testLogger,\n\t\tbroker:          rdbClient,\n\t\tinterval:        1 * time.Second,\n\t\thealthcheckFunc: checkFn,\n\t})\n\n\thc.start(&sync.WaitGroup{})\n\n\ttime.Sleep(2 * time.Second)\n\n\tmu.Lock()\n\tif called == 0 {\n\t\tt.Errorf(\"Healthchecker did not call the provided HealthCheckFunc\")\n\t}\n\tif e != nil {\n\t\tt.Errorf(\"HealthCheckFunc was called with non-nil error: %v\", e)\n\t}\n\tmu.Unlock()\n\n\thc.shutdown()\n}\n\nfunc TestHealthCheckerWhenRedisDown(t *testing.T) {\n\t// Make sure that healthchecker goroutine doesn't panic\n\t// if it cannot connect to redis.\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tt.Errorf(\"panic occurred: %v\", r)\n\t\t}\n\t}()\n\tr := rdb.NewRDB(setup(t))\n\tdefer r.Close()\n\ttestBroker := testbroker.NewTestBroker(r)\n\tvar (\n\t\t// mu guards called and e variables.\n\t\tmu     sync.Mutex\n\t\tcalled int\n\t\te      error\n\t)\n\tcheckFn := func(err error) {\n\t\tmu.Lock()\n\t\tdefer mu.Unlock()\n\t\tcalled++\n\t\te = err\n\t}\n\n\thc := newHealthChecker(healthcheckerParams{\n\t\tlogger:          testLogger,\n\t\tbroker:          testBroker,\n\t\tinterval:        1 * time.Second,\n\t\thealthcheckFunc: checkFn,\n\t})\n\n\ttestBroker.Sleep()\n\thc.start(&sync.WaitGroup{})\n\n\ttime.Sleep(2 * time.Second)\n\n\tmu.Lock()\n\tif called == 0 {\n\t\tt.Errorf(\"Healthchecker did not call the provided HealthCheckFunc\")\n\t}\n\tif e == nil {\n\t\tt.Errorf(\"HealthCheckFunc was called with nil; want non-nil error\")\n\t}\n\tmu.Unlock()\n\n\thc.shutdown()\n}\n"
        },
        {
          "name": "heartbeat.go",
          "type": "blob",
          "size": 5.1953125,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"os\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/log\"\n\t\"github.com/hibiken/asynq/internal/timeutil\"\n)\n\n// heartbeater is responsible for writing process info to redis periodically to\n// indicate that the background worker process is up.\ntype heartbeater struct {\n\tlogger *log.Logger\n\tbroker base.Broker\n\tclock  timeutil.Clock\n\n\t// channel to communicate back to the long running \"heartbeater\" goroutine.\n\tdone chan struct{}\n\n\t// interval between heartbeats.\n\tinterval time.Duration\n\n\t// following fields are initialized at construction time and are immutable.\n\thost           string\n\tpid            int\n\tserverID       string\n\tconcurrency    int\n\tqueues         map[string]int\n\tstrictPriority bool\n\n\t// following fields are mutable and should be accessed only by the\n\t// heartbeater goroutine. In other words, confine these variables\n\t// to this goroutine only.\n\tstarted time.Time\n\tworkers map[string]*workerInfo\n\n\t// state is shared with other goroutine but is concurrency safe.\n\tstate *serverState\n\n\t// channels to receive updates on active workers.\n\tstarting <-chan *workerInfo\n\tfinished <-chan *base.TaskMessage\n}\n\ntype heartbeaterParams struct {\n\tlogger         *log.Logger\n\tbroker         base.Broker\n\tinterval       time.Duration\n\tconcurrency    int\n\tqueues         map[string]int\n\tstrictPriority bool\n\tstate          *serverState\n\tstarting       <-chan *workerInfo\n\tfinished       <-chan *base.TaskMessage\n}\n\nfunc newHeartbeater(params heartbeaterParams) *heartbeater {\n\thost, err := os.Hostname()\n\tif err != nil {\n\t\thost = \"unknown-host\"\n\t}\n\n\treturn &heartbeater{\n\t\tlogger:   params.logger,\n\t\tbroker:   params.broker,\n\t\tclock:    timeutil.NewRealClock(),\n\t\tdone:     make(chan struct{}),\n\t\tinterval: params.interval,\n\n\t\thost:           host,\n\t\tpid:            os.Getpid(),\n\t\tserverID:       uuid.New().String(),\n\t\tconcurrency:    params.concurrency,\n\t\tqueues:         params.queues,\n\t\tstrictPriority: params.strictPriority,\n\n\t\tstate:    params.state,\n\t\tworkers:  make(map[string]*workerInfo),\n\t\tstarting: params.starting,\n\t\tfinished: params.finished,\n\t}\n}\n\nfunc (h *heartbeater) shutdown() {\n\th.logger.Debug(\"Heartbeater shutting down...\")\n\t// Signal the heartbeater goroutine to stop.\n\th.done <- struct{}{}\n}\n\n// A workerInfo holds an active worker information.\ntype workerInfo struct {\n\t// the task message the worker is processing.\n\tmsg *base.TaskMessage\n\t// the time the worker has started processing the message.\n\tstarted time.Time\n\t// deadline the worker has to finish processing the task by.\n\tdeadline time.Time\n\t// lease the worker holds for the task.\n\tlease *base.Lease\n}\n\nfunc (h *heartbeater) start(wg *sync.WaitGroup) {\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\n\t\th.started = h.clock.Now()\n\n\t\th.beat()\n\n\t\ttimer := time.NewTimer(h.interval)\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-h.done:\n\t\t\t\tif err := h.broker.ClearServerState(h.host, h.pid, h.serverID); err != nil {\n\t\t\t\t\th.logger.Errorf(\"Failed to clear server state: %v\", err)\n\t\t\t\t}\n\t\t\t\th.logger.Debug(\"Heartbeater done\")\n\t\t\t\ttimer.Stop()\n\t\t\t\treturn\n\n\t\t\tcase <-timer.C:\n\t\t\t\th.beat()\n\t\t\t\ttimer.Reset(h.interval)\n\n\t\t\tcase w := <-h.starting:\n\t\t\t\th.workers[w.msg.ID] = w\n\n\t\t\tcase msg := <-h.finished:\n\t\t\t\tdelete(h.workers, msg.ID)\n\t\t\t}\n\t\t}\n\t}()\n}\n\n// beat extends lease for workers and writes server/worker info to redis.\nfunc (h *heartbeater) beat() {\n\th.state.mu.Lock()\n\tsrvStatus := h.state.value.String()\n\th.state.mu.Unlock()\n\n\tinfo := base.ServerInfo{\n\t\tHost:              h.host,\n\t\tPID:               h.pid,\n\t\tServerID:          h.serverID,\n\t\tConcurrency:       h.concurrency,\n\t\tQueues:            h.queues,\n\t\tStrictPriority:    h.strictPriority,\n\t\tStatus:            srvStatus,\n\t\tStarted:           h.started,\n\t\tActiveWorkerCount: len(h.workers),\n\t}\n\n\tvar ws []*base.WorkerInfo\n\tidsByQueue := make(map[string][]string)\n\tfor id, w := range h.workers {\n\t\tws = append(ws, &base.WorkerInfo{\n\t\t\tHost:     h.host,\n\t\t\tPID:      h.pid,\n\t\t\tServerID: h.serverID,\n\t\t\tID:       id,\n\t\t\tType:     w.msg.Type,\n\t\t\tQueue:    w.msg.Queue,\n\t\t\tPayload:  w.msg.Payload,\n\t\t\tStarted:  w.started,\n\t\t\tDeadline: w.deadline,\n\t\t})\n\t\t// Check lease before adding to the set to make sure not to extend the lease if the lease is already expired.\n\t\tif w.lease.IsValid() {\n\t\t\tidsByQueue[w.msg.Queue] = append(idsByQueue[w.msg.Queue], id)\n\t\t} else {\n\t\t\tw.lease.NotifyExpiration() // notify processor if the lease is expired\n\t\t}\n\t}\n\n\t// Note: Set TTL to be long enough so that it won't expire before we write again\n\t// and short enough to expire quickly once the process is shut down or killed.\n\tif err := h.broker.WriteServerState(&info, ws, h.interval*2); err != nil {\n\t\th.logger.Errorf(\"Failed to write server state data: %v\", err)\n\t}\n\n\tfor qname, ids := range idsByQueue {\n\t\texpirationTime, err := h.broker.ExtendLease(qname, ids...)\n\t\tif err != nil {\n\t\t\th.logger.Errorf(\"Failed to extend lease for tasks %v: %v\", ids, err)\n\t\t\tcontinue\n\t\t}\n\t\tfor _, id := range ids {\n\t\t\tif l := h.workers[id].lease; !l.Reset(expirationTime) {\n\t\t\t\th.logger.Warnf(\"Lease reset failed for %s; lease deadline: %v\", id, l.Deadline())\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "heartbeat_test.go",
          "type": "blob",
          "size": 10.556640625,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/google/go-cmp/cmp/cmpopts\"\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/rdb\"\n\t\"github.com/hibiken/asynq/internal/testbroker\"\n\th \"github.com/hibiken/asynq/internal/testutil\"\n\t\"github.com/hibiken/asynq/internal/timeutil\"\n)\n\n// Test goes through a few phases.\n//\n// Phase1: Simulate Server startup; Simulate starting tasks listed in startedWorkers\n// Phase2: Simulate finishing tasks listed in finishedTasks\n// Phase3: Simulate Server shutdown;\nfunc TestHeartbeater(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\trdbClient := rdb.NewRDB(r)\n\n\tnow := time.Now()\n\tconst elapsedTime = 10 * time.Second // simulated time elapsed between phase1 and phase2\n\n\tclock := timeutil.NewSimulatedClock(time.Time{}) // time will be set in each test\n\n\tt1 := h.NewTaskMessageWithQueue(\"task1\", nil, \"default\")\n\tt2 := h.NewTaskMessageWithQueue(\"task2\", nil, \"default\")\n\tt3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"default\")\n\tt4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"custom\")\n\tt5 := h.NewTaskMessageWithQueue(\"task5\", nil, \"custom\")\n\tt6 := h.NewTaskMessageWithQueue(\"task6\", nil, \"default\")\n\n\t// Note: intentionally set to time less than now.Add(rdb.LeaseDuration) to test lease extension is working.\n\tlease1 := h.NewLeaseWithClock(now.Add(10*time.Second), clock)\n\tlease2 := h.NewLeaseWithClock(now.Add(10*time.Second), clock)\n\tlease3 := h.NewLeaseWithClock(now.Add(10*time.Second), clock)\n\tlease4 := h.NewLeaseWithClock(now.Add(10*time.Second), clock)\n\tlease5 := h.NewLeaseWithClock(now.Add(10*time.Second), clock)\n\tlease6 := h.NewLeaseWithClock(now.Add(10*time.Second), clock)\n\n\ttests := []struct {\n\t\tdesc string\n\n\t\t// Interval between heartbeats.\n\t\tinterval time.Duration\n\n\t\t// Server info.\n\t\thost        string\n\t\tpid         int\n\t\tqueues      map[string]int\n\t\tconcurrency int\n\n\t\tactive         map[string][]*base.TaskMessage // initial active set state\n\t\tlease          map[string][]base.Z            // initial lease set state\n\t\twantLease1     map[string][]base.Z            // expected lease set state after starting all startedWorkers\n\t\twantLease2     map[string][]base.Z            // expected lease set state after finishing all finishedTasks\n\t\tstartedWorkers []*workerInfo                  // workerInfo to send via the started channel\n\t\tfinishedTasks  []*base.TaskMessage            // tasks to send via the finished channel\n\n\t\tstartTime   time.Time     // simulated start time\n\t\telapsedTime time.Duration // simulated time elapsed between starting and finishing processing tasks\n\t}{\n\t\t{\n\t\t\tdesc:        \"With single queue\",\n\t\t\tinterval:    2 * time.Second,\n\t\t\thost:        \"localhost\",\n\t\t\tpid:         45678,\n\t\t\tqueues:      map[string]int{\"default\": 1},\n\t\t\tconcurrency: 10,\n\t\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {t1, t2, t3},\n\t\t\t},\n\t\t\tlease: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{Message: t1, Score: now.Add(10 * time.Second).Unix()},\n\t\t\t\t\t{Message: t2, Score: now.Add(10 * time.Second).Unix()},\n\t\t\t\t\t{Message: t3, Score: now.Add(10 * time.Second).Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t\tstartedWorkers: []*workerInfo{\n\t\t\t\t{msg: t1, started: now, deadline: now.Add(2 * time.Minute), lease: lease1},\n\t\t\t\t{msg: t2, started: now, deadline: now.Add(2 * time.Minute), lease: lease2},\n\t\t\t\t{msg: t3, started: now, deadline: now.Add(2 * time.Minute), lease: lease3},\n\t\t\t},\n\t\t\tfinishedTasks: []*base.TaskMessage{t1, t2},\n\t\t\twantLease1: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{Message: t1, Score: now.Add(rdb.LeaseDuration).Unix()},\n\t\t\t\t\t{Message: t2, Score: now.Add(rdb.LeaseDuration).Unix()},\n\t\t\t\t\t{Message: t3, Score: now.Add(rdb.LeaseDuration).Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t\twantLease2: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{Message: t3, Score: now.Add(elapsedTime).Add(rdb.LeaseDuration).Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t\tstartTime:   now,\n\t\t\telapsedTime: elapsedTime,\n\t\t},\n\t\t{\n\t\t\tdesc:        \"With multiple queue\",\n\t\t\tinterval:    2 * time.Second,\n\t\t\thost:        \"localhost\",\n\t\t\tpid:         45678,\n\t\t\tqueues:      map[string]int{\"default\": 1, \"custom\": 2},\n\t\t\tconcurrency: 10,\n\t\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {t6},\n\t\t\t\t\"custom\":  {t4, t5},\n\t\t\t},\n\t\t\tlease: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{Message: t6, Score: now.Add(10 * time.Second).Unix()},\n\t\t\t\t},\n\t\t\t\t\"custom\": {\n\t\t\t\t\t{Message: t4, Score: now.Add(10 * time.Second).Unix()},\n\t\t\t\t\t{Message: t5, Score: now.Add(10 * time.Second).Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t\tstartedWorkers: []*workerInfo{\n\t\t\t\t{msg: t6, started: now, deadline: now.Add(2 * time.Minute), lease: lease6},\n\t\t\t\t{msg: t4, started: now, deadline: now.Add(2 * time.Minute), lease: lease4},\n\t\t\t\t{msg: t5, started: now, deadline: now.Add(2 * time.Minute), lease: lease5},\n\t\t\t},\n\t\t\tfinishedTasks: []*base.TaskMessage{t6, t5},\n\t\t\twantLease1: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{Message: t6, Score: now.Add(rdb.LeaseDuration).Unix()},\n\t\t\t\t},\n\t\t\t\t\"custom\": {\n\t\t\t\t\t{Message: t4, Score: now.Add(rdb.LeaseDuration).Unix()},\n\t\t\t\t\t{Message: t5, Score: now.Add(rdb.LeaseDuration).Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t\twantLease2: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\": {\n\t\t\t\t\t{Message: t4, Score: now.Add(elapsedTime).Add(rdb.LeaseDuration).Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t\tstartTime:   now,\n\t\t\telapsedTime: elapsedTime,\n\t\t},\n\t}\n\n\ttimeCmpOpt := cmpopts.EquateApproxTime(10 * time.Millisecond)\n\tignoreOpt := cmpopts.IgnoreUnexported(base.ServerInfo{})\n\tignoreFieldOpt := cmpopts.IgnoreFields(base.ServerInfo{}, \"ServerID\")\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllActiveQueues(t, r, tc.active)\n\t\th.SeedAllLease(t, r, tc.lease)\n\n\t\tclock.SetTime(tc.startTime)\n\t\trdbClient.SetClock(clock)\n\n\t\tsrvState := &serverState{}\n\t\tstartingCh := make(chan *workerInfo)\n\t\tfinishedCh := make(chan *base.TaskMessage)\n\t\thb := newHeartbeater(heartbeaterParams{\n\t\t\tlogger:         testLogger,\n\t\t\tbroker:         rdbClient,\n\t\t\tinterval:       tc.interval,\n\t\t\tconcurrency:    tc.concurrency,\n\t\t\tqueues:         tc.queues,\n\t\t\tstrictPriority: false,\n\t\t\tstate:          srvState,\n\t\t\tstarting:       startingCh,\n\t\t\tfinished:       finishedCh,\n\t\t})\n\t\thb.clock = clock\n\n\t\t// Change host and pid fields for testing purpose.\n\t\thb.host = tc.host\n\t\thb.pid = tc.pid\n\n\t\t//===================\n\t\t// Start Phase1\n\t\t//===================\n\n\t\tsrvState.mu.Lock()\n\t\tsrvState.value = srvStateActive // simulating Server.Start\n\t\tsrvState.mu.Unlock()\n\n\t\tvar wg sync.WaitGroup\n\t\thb.start(&wg)\n\n\t\t// Simulate processor starting to work on tasks.\n\t\tfor _, w := range tc.startedWorkers {\n\t\t\tstartingCh <- w\n\t\t}\n\n\t\t// Wait for heartbeater to write to redis\n\t\ttime.Sleep(tc.interval * 2)\n\n\t\tss, err := rdbClient.ListServers()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%s: could not read server info from redis: %v\", tc.desc, err)\n\t\t\thb.shutdown()\n\t\t\tcontinue\n\t\t}\n\n\t\tif len(ss) != 1 {\n\t\t\tt.Errorf(\"%s: (*RDB).ListServers returned %d server info, want 1\", tc.desc, len(ss))\n\t\t\thb.shutdown()\n\t\t\tcontinue\n\t\t}\n\n\t\twantInfo := &base.ServerInfo{\n\t\t\tHost:              tc.host,\n\t\t\tPID:               tc.pid,\n\t\t\tQueues:            tc.queues,\n\t\t\tConcurrency:       tc.concurrency,\n\t\t\tStarted:           now,\n\t\t\tStatus:            \"active\",\n\t\t\tActiveWorkerCount: len(tc.startedWorkers),\n\t\t}\n\t\tif diff := cmp.Diff(wantInfo, ss[0], timeCmpOpt, ignoreOpt, ignoreFieldOpt); diff != \"\" {\n\t\t\tt.Errorf(\"%s: redis stored server status %+v, want %+v; (-want, +got)\\n%s\", tc.desc, ss[0], wantInfo, diff)\n\t\t\thb.shutdown()\n\t\t\tcontinue\n\t\t}\n\n\t\tfor qname, wantLease := range tc.wantLease1 {\n\t\t\tgotLease := h.GetLeaseEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(wantLease, gotLease, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"%s: mismatch found in %q: (-want,+got):\\n%s\", tc.desc, base.LeaseKey(qname), diff)\n\t\t\t}\n\t\t}\n\n\t\tfor _, w := range tc.startedWorkers {\n\t\t\tif want := now.Add(rdb.LeaseDuration); w.lease.Deadline() != want {\n\t\t\t\tt.Errorf(\"%s: lease deadline for %v is set to %v, want %v\", tc.desc, w.msg, w.lease.Deadline(), want)\n\t\t\t}\n\t\t}\n\n\t\t//===================\n\t\t// Start Phase2\n\t\t//===================\n\n\t\tclock.AdvanceTime(tc.elapsedTime)\n\t\t// Simulate processor finished processing tasks.\n\t\tfor _, msg := range tc.finishedTasks {\n\t\t\tif err := rdbClient.Done(context.Background(), msg); err != nil {\n\t\t\t\tt.Fatalf(\"RDB.Done failed: %v\", err)\n\t\t\t}\n\t\t\tfinishedCh <- msg\n\t\t}\n\t\t// Wait for heartbeater to write to redis\n\t\ttime.Sleep(tc.interval * 2)\n\n\t\tfor qname, wantLease := range tc.wantLease2 {\n\t\t\tgotLease := h.GetLeaseEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(wantLease, gotLease, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"%s: mismatch found in %q: (-want,+got):\\n%s\", tc.desc, base.LeaseKey(qname), diff)\n\t\t\t}\n\t\t}\n\n\t\t//===================\n\t\t// Start Phase3\n\t\t//===================\n\n\t\t// Server state change; simulating Server.Shutdown\n\t\tsrvState.mu.Lock()\n\t\tsrvState.value = srvStateClosed\n\t\tsrvState.mu.Unlock()\n\n\t\t// Wait for heartbeater to write to redis\n\t\ttime.Sleep(tc.interval * 2)\n\n\t\twantInfo = &base.ServerInfo{\n\t\t\tHost:              tc.host,\n\t\t\tPID:               tc.pid,\n\t\t\tQueues:            tc.queues,\n\t\t\tConcurrency:       tc.concurrency,\n\t\t\tStarted:           now,\n\t\t\tStatus:            \"closed\",\n\t\t\tActiveWorkerCount: len(tc.startedWorkers) - len(tc.finishedTasks),\n\t\t}\n\t\tss, err = rdbClient.ListServers()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%s: could not read server status from redis: %v\", tc.desc, err)\n\t\t\thb.shutdown()\n\t\t\tcontinue\n\t\t}\n\n\t\tif len(ss) != 1 {\n\t\t\tt.Errorf(\"%s: (*RDB).ListServers returned %d server info, want 1\", tc.desc, len(ss))\n\t\t\thb.shutdown()\n\t\t\tcontinue\n\t\t}\n\n\t\tif diff := cmp.Diff(wantInfo, ss[0], timeCmpOpt, ignoreOpt, ignoreFieldOpt); diff != \"\" {\n\t\t\tt.Errorf(\"%s: redis stored process status %+v, want %+v; (-want, +got)\\n%s\", tc.desc, ss[0], wantInfo, diff)\n\t\t\thb.shutdown()\n\t\t\tcontinue\n\t\t}\n\n\t\thb.shutdown()\n\t}\n}\n\nfunc TestHeartbeaterWithRedisDown(t *testing.T) {\n\t// Make sure that heartbeater goroutine doesn't panic\n\t// if it cannot connect to redis.\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tt.Errorf(\"panic occurred: %v\", r)\n\t\t}\n\t}()\n\tr := rdb.NewRDB(setup(t))\n\tdefer r.Close()\n\ttestBroker := testbroker.NewTestBroker(r)\n\tstate := &serverState{value: srvStateActive}\n\thb := newHeartbeater(heartbeaterParams{\n\t\tlogger:         testLogger,\n\t\tbroker:         testBroker,\n\t\tinterval:       time.Second,\n\t\tconcurrency:    10,\n\t\tqueues:         map[string]int{\"default\": 1},\n\t\tstrictPriority: false,\n\t\tstate:          state,\n\t\tstarting:       make(chan *workerInfo),\n\t\tfinished:       make(chan *base.TaskMessage),\n\t})\n\n\ttestBroker.Sleep()\n\tvar wg sync.WaitGroup\n\thb.start(&wg)\n\n\t// wait for heartbeater to try writing data to redis\n\ttime.Sleep(2 * time.Second)\n\n\thb.shutdown()\n}\n"
        },
        {
          "name": "inspector.go",
          "type": "blob",
          "size": 29.3193359375,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/errors\"\n\t\"github.com/hibiken/asynq/internal/rdb\"\n\t\"github.com/redis/go-redis/v9\"\n)\n\n// Inspector is a client interface to inspect and mutate the state of\n// queues and tasks.\ntype Inspector struct {\n\trdb *rdb.RDB\n\t// When an Inspector has been created with an existing Redis connection, we do\n\t// not want to close it.\n\tsharedConnection bool\n}\n\n// New returns a new instance of Inspector.\nfunc NewInspector(r RedisConnOpt) *Inspector {\n\tc, ok := r.MakeRedisClient().(redis.UniversalClient)\n\tif !ok {\n\t\tpanic(fmt.Sprintf(\"inspeq: unsupported RedisConnOpt type %T\", r))\n\t}\n\tinspector := NewInspectorFromRedisClient(c)\n\tinspector.sharedConnection = false\n\treturn inspector\n}\n\n// NewInspectorFromRedisClient returns a new instance of Inspector given a redis.UniversalClient\n// Warning: The underlying redis connection pool will not be closed by Asynq, you are responsible for closing it.\nfunc NewInspectorFromRedisClient(c redis.UniversalClient) *Inspector {\n\treturn &Inspector{\n\t\trdb:              rdb.NewRDB(c),\n\t\tsharedConnection: true,\n\t}\n}\n\n// Close closes the connection with redis.\nfunc (i *Inspector) Close() error {\n\tif i.sharedConnection {\n\t\treturn fmt.Errorf(\"redis connection is shared so the Inspector can't be closed through asynq\")\n\t}\n\treturn i.rdb.Close()\n}\n\n// Queues returns a list of all queue names.\nfunc (i *Inspector) Queues() ([]string, error) {\n\treturn i.rdb.AllQueues()\n}\n\n// Groups returns a list of all groups within the given queue.\nfunc (i *Inspector) Groups(queue string) ([]*GroupInfo, error) {\n\tstats, err := i.rdb.GroupStats(queue)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar res []*GroupInfo\n\tfor _, s := range stats {\n\t\tres = append(res, &GroupInfo{\n\t\t\tGroup: s.Group,\n\t\t\tSize:  s.Size,\n\t\t})\n\t}\n\treturn res, nil\n}\n\n// GroupInfo represents a state of a group at a certain time.\ntype GroupInfo struct {\n\t// Name of the group.\n\tGroup string\n\n\t// Size is the total number of tasks in the group.\n\tSize int\n}\n\n// QueueInfo represents a state of a queue at a certain time.\ntype QueueInfo struct {\n\t// Name of the queue.\n\tQueue string\n\n\t// Total number of bytes that the queue and its tasks require to be stored in redis.\n\t// It is an approximate memory usage value in bytes since the value is computed by sampling.\n\tMemoryUsage int64\n\n\t// Latency of the queue, measured by the oldest pending task in the queue.\n\tLatency time.Duration\n\n\t// Size is the total number of tasks in the queue.\n\t// The value is the sum of Pending, Active, Scheduled, Retry, Aggregating and Archived.\n\tSize int\n\n\t// Groups is the total number of groups in the queue.\n\tGroups int\n\n\t// Number of pending tasks.\n\tPending int\n\t// Number of active tasks.\n\tActive int\n\t// Number of scheduled tasks.\n\tScheduled int\n\t// Number of retry tasks.\n\tRetry int\n\t// Number of archived tasks.\n\tArchived int\n\t// Number of stored completed tasks.\n\tCompleted int\n\t// Number of aggregating tasks.\n\tAggregating int\n\n\t// Total number of tasks being processed within the given date (counter resets daily).\n\t// The number includes both succeeded and failed tasks.\n\tProcessed int\n\t// Total number of tasks failed to be processed within the given date (counter resets daily).\n\tFailed int\n\n\t// Total number of tasks processed (cumulative).\n\tProcessedTotal int\n\t// Total number of tasks failed (cumulative).\n\tFailedTotal int\n\n\t// Paused indicates whether the queue is paused.\n\t// If true, tasks in the queue will not be processed.\n\tPaused bool\n\n\t// Time when this queue info snapshot was taken.\n\tTimestamp time.Time\n}\n\n// GetQueueInfo returns current information of the given queue.\nfunc (i *Inspector) GetQueueInfo(queue string) (*QueueInfo, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn nil, err\n\t}\n\tstats, err := i.rdb.CurrentStats(queue)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &QueueInfo{\n\t\tQueue:          stats.Queue,\n\t\tMemoryUsage:    stats.MemoryUsage,\n\t\tLatency:        stats.Latency,\n\t\tSize:           stats.Size,\n\t\tGroups:         stats.Groups,\n\t\tPending:        stats.Pending,\n\t\tActive:         stats.Active,\n\t\tScheduled:      stats.Scheduled,\n\t\tRetry:          stats.Retry,\n\t\tArchived:       stats.Archived,\n\t\tCompleted:      stats.Completed,\n\t\tAggregating:    stats.Aggregating,\n\t\tProcessed:      stats.Processed,\n\t\tFailed:         stats.Failed,\n\t\tProcessedTotal: stats.ProcessedTotal,\n\t\tFailedTotal:    stats.FailedTotal,\n\t\tPaused:         stats.Paused,\n\t\tTimestamp:      stats.Timestamp,\n\t}, nil\n}\n\n// DailyStats holds aggregate data for a given day for a given queue.\ntype DailyStats struct {\n\t// Name of the queue.\n\tQueue string\n\t// Total number of tasks being processed during the given date.\n\t// The number includes both succeeded and failed tasks.\n\tProcessed int\n\t// Total number of tasks failed to be processed during the given date.\n\tFailed int\n\t// Date this stats was taken.\n\tDate time.Time\n}\n\n// History returns a list of stats from the last n days.\nfunc (i *Inspector) History(queue string, n int) ([]*DailyStats, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn nil, err\n\t}\n\tstats, err := i.rdb.HistoricalStats(queue, n)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar res []*DailyStats\n\tfor _, s := range stats {\n\t\tres = append(res, &DailyStats{\n\t\t\tQueue:     s.Queue,\n\t\t\tProcessed: s.Processed,\n\t\t\tFailed:    s.Failed,\n\t\t\tDate:      s.Time,\n\t\t})\n\t}\n\treturn res, nil\n}\n\nvar (\n\t// ErrQueueNotFound indicates that the specified queue does not exist.\n\tErrQueueNotFound = errors.New(\"queue not found\")\n\n\t// ErrQueueNotEmpty indicates that the specified queue is not empty.\n\tErrQueueNotEmpty = errors.New(\"queue is not empty\")\n\n\t// ErrTaskNotFound indicates that the specified task cannot be found in the queue.\n\tErrTaskNotFound = errors.New(\"task not found\")\n)\n\n// DeleteQueue removes the specified queue.\n//\n// If force is set to true, DeleteQueue will remove the queue regardless of\n// the queue size as long as no tasks are active in the queue.\n// If force is set to false, DeleteQueue will remove the queue only if\n// the queue is empty.\n//\n// If the specified queue does not exist, DeleteQueue returns ErrQueueNotFound.\n// If force is set to false and the specified queue is not empty, DeleteQueue\n// returns ErrQueueNotEmpty.\nfunc (i *Inspector) DeleteQueue(queue string, force bool) error {\n\terr := i.rdb.RemoveQueue(queue, force)\n\tif errors.IsQueueNotFound(err) {\n\t\treturn fmt.Errorf(\"%w: queue=%q\", ErrQueueNotFound, queue)\n\t}\n\tif errors.IsQueueNotEmpty(err) {\n\t\treturn fmt.Errorf(\"%w: queue=%q\", ErrQueueNotEmpty, queue)\n\t}\n\treturn err\n}\n\n// GetTaskInfo retrieves task information given a task id and queue name.\n//\n// Returns an error wrapping ErrQueueNotFound if a queue with the given name doesn't exist.\n// Returns an error wrapping ErrTaskNotFound if a task with the given id doesn't exist in the queue.\nfunc (i *Inspector) GetTaskInfo(queue, id string) (*TaskInfo, error) {\n\tinfo, err := i.rdb.GetTaskInfo(queue, id)\n\tswitch {\n\tcase errors.IsQueueNotFound(err):\n\t\treturn nil, fmt.Errorf(\"asynq: %w\", ErrQueueNotFound)\n\tcase errors.IsTaskNotFound(err):\n\t\treturn nil, fmt.Errorf(\"asynq: %w\", ErrTaskNotFound)\n\tcase err != nil:\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\treturn newTaskInfo(info.Message, info.State, info.NextProcessAt, info.Result), nil\n}\n\n// ListOption specifies behavior of list operation.\ntype ListOption interface{}\n\n// Internal list option representations.\ntype (\n\tpageSizeOpt int\n\tpageNumOpt  int\n)\n\ntype listOption struct {\n\tpageSize int\n\tpageNum  int\n}\n\nconst (\n\t// Page size used by default in list operation.\n\tdefaultPageSize = 30\n\n\t// Page number used by default in list operation.\n\tdefaultPageNum = 1\n)\n\nfunc composeListOptions(opts ...ListOption) listOption {\n\tres := listOption{\n\t\tpageSize: defaultPageSize,\n\t\tpageNum:  defaultPageNum,\n\t}\n\tfor _, opt := range opts {\n\t\tswitch opt := opt.(type) {\n\t\tcase pageSizeOpt:\n\t\t\tres.pageSize = int(opt)\n\t\tcase pageNumOpt:\n\t\t\tres.pageNum = int(opt)\n\t\tdefault:\n\t\t\t// ignore unexpected option\n\t\t}\n\t}\n\treturn res\n}\n\n// PageSize returns an option to specify the page size for list operation.\n//\n// Negative page size is treated as zero.\nfunc PageSize(n int) ListOption {\n\tif n < 0 {\n\t\tn = 0\n\t}\n\treturn pageSizeOpt(n)\n}\n\n// Page returns an option to specify the page number for list operation.\n// The value 1 fetches the first page.\n//\n// Negative page number is treated as one.\nfunc Page(n int) ListOption {\n\tif n < 0 {\n\t\tn = 1\n\t}\n\treturn pageNumOpt(n)\n}\n\n// ListPendingTasks retrieves pending tasks from the specified queue.\n//\n// By default, it retrieves the first 30 tasks.\nfunc (i *Inspector) ListPendingTasks(queue string, opts ...ListOption) ([]*TaskInfo, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\topt := composeListOptions(opts...)\n\tpgn := rdb.Pagination{Size: opt.pageSize, Page: opt.pageNum - 1}\n\tinfos, err := i.rdb.ListPending(queue, pgn)\n\tswitch {\n\tcase errors.IsQueueNotFound(err):\n\t\treturn nil, fmt.Errorf(\"asynq: %w\", ErrQueueNotFound)\n\tcase err != nil:\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\tvar tasks []*TaskInfo\n\tfor _, i := range infos {\n\t\ttasks = append(tasks, newTaskInfo(\n\t\t\ti.Message,\n\t\t\ti.State,\n\t\t\ti.NextProcessAt,\n\t\t\ti.Result,\n\t\t))\n\t}\n\treturn tasks, err\n}\n\n// ListActiveTasks retrieves active tasks from the specified queue.\n//\n// By default, it retrieves the first 30 tasks.\nfunc (i *Inspector) ListActiveTasks(queue string, opts ...ListOption) ([]*TaskInfo, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\topt := composeListOptions(opts...)\n\tpgn := rdb.Pagination{Size: opt.pageSize, Page: opt.pageNum - 1}\n\tinfos, err := i.rdb.ListActive(queue, pgn)\n\tswitch {\n\tcase errors.IsQueueNotFound(err):\n\t\treturn nil, fmt.Errorf(\"asynq: %w\", ErrQueueNotFound)\n\tcase err != nil:\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\texpired, err := i.rdb.ListLeaseExpired(time.Now(), queue)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\texpiredSet := make(map[string]struct{}) // set of expired message IDs\n\tfor _, msg := range expired {\n\t\texpiredSet[msg.ID] = struct{}{}\n\t}\n\tvar tasks []*TaskInfo\n\tfor _, i := range infos {\n\t\tt := newTaskInfo(\n\t\t\ti.Message,\n\t\t\ti.State,\n\t\t\ti.NextProcessAt,\n\t\t\ti.Result,\n\t\t)\n\t\tif _, ok := expiredSet[i.Message.ID]; ok {\n\t\t\tt.IsOrphaned = true\n\t\t}\n\t\ttasks = append(tasks, t)\n\t}\n\treturn tasks, nil\n}\n\n// ListAggregatingTasks retrieves scheduled tasks from the specified group.\n//\n// By default, it retrieves the first 30 tasks.\nfunc (i *Inspector) ListAggregatingTasks(queue, group string, opts ...ListOption) ([]*TaskInfo, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\topt := composeListOptions(opts...)\n\tpgn := rdb.Pagination{Size: opt.pageSize, Page: opt.pageNum - 1}\n\tinfos, err := i.rdb.ListAggregating(queue, group, pgn)\n\tswitch {\n\tcase errors.IsQueueNotFound(err):\n\t\treturn nil, fmt.Errorf(\"asynq: %w\", ErrQueueNotFound)\n\tcase err != nil:\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\tvar tasks []*TaskInfo\n\tfor _, i := range infos {\n\t\ttasks = append(tasks, newTaskInfo(\n\t\t\ti.Message,\n\t\t\ti.State,\n\t\t\ti.NextProcessAt,\n\t\t\ti.Result,\n\t\t))\n\t}\n\treturn tasks, nil\n}\n\n// ListScheduledTasks retrieves scheduled tasks from the specified queue.\n// Tasks are sorted by NextProcessAt in ascending order.\n//\n// By default, it retrieves the first 30 tasks.\nfunc (i *Inspector) ListScheduledTasks(queue string, opts ...ListOption) ([]*TaskInfo, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\topt := composeListOptions(opts...)\n\tpgn := rdb.Pagination{Size: opt.pageSize, Page: opt.pageNum - 1}\n\tinfos, err := i.rdb.ListScheduled(queue, pgn)\n\tswitch {\n\tcase errors.IsQueueNotFound(err):\n\t\treturn nil, fmt.Errorf(\"asynq: %w\", ErrQueueNotFound)\n\tcase err != nil:\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\tvar tasks []*TaskInfo\n\tfor _, i := range infos {\n\t\ttasks = append(tasks, newTaskInfo(\n\t\t\ti.Message,\n\t\t\ti.State,\n\t\t\ti.NextProcessAt,\n\t\t\ti.Result,\n\t\t))\n\t}\n\treturn tasks, nil\n}\n\n// ListRetryTasks retrieves retry tasks from the specified queue.\n// Tasks are sorted by NextProcessAt in ascending order.\n//\n// By default, it retrieves the first 30 tasks.\nfunc (i *Inspector) ListRetryTasks(queue string, opts ...ListOption) ([]*TaskInfo, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\topt := composeListOptions(opts...)\n\tpgn := rdb.Pagination{Size: opt.pageSize, Page: opt.pageNum - 1}\n\tinfos, err := i.rdb.ListRetry(queue, pgn)\n\tswitch {\n\tcase errors.IsQueueNotFound(err):\n\t\treturn nil, fmt.Errorf(\"asynq: %w\", ErrQueueNotFound)\n\tcase err != nil:\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\tvar tasks []*TaskInfo\n\tfor _, i := range infos {\n\t\ttasks = append(tasks, newTaskInfo(\n\t\t\ti.Message,\n\t\t\ti.State,\n\t\t\ti.NextProcessAt,\n\t\t\ti.Result,\n\t\t))\n\t}\n\treturn tasks, nil\n}\n\n// ListArchivedTasks retrieves archived tasks from the specified queue.\n// Tasks are sorted by LastFailedAt in descending order.\n//\n// By default, it retrieves the first 30 tasks.\nfunc (i *Inspector) ListArchivedTasks(queue string, opts ...ListOption) ([]*TaskInfo, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\topt := composeListOptions(opts...)\n\tpgn := rdb.Pagination{Size: opt.pageSize, Page: opt.pageNum - 1}\n\tinfos, err := i.rdb.ListArchived(queue, pgn)\n\tswitch {\n\tcase errors.IsQueueNotFound(err):\n\t\treturn nil, fmt.Errorf(\"asynq: %w\", ErrQueueNotFound)\n\tcase err != nil:\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\tvar tasks []*TaskInfo\n\tfor _, i := range infos {\n\t\ttasks = append(tasks, newTaskInfo(\n\t\t\ti.Message,\n\t\t\ti.State,\n\t\t\ti.NextProcessAt,\n\t\t\ti.Result,\n\t\t))\n\t}\n\treturn tasks, nil\n}\n\n// ListCompletedTasks retrieves completed tasks from the specified queue.\n// Tasks are sorted by expiration time (i.e. CompletedAt + Retention) in descending order.\n//\n// By default, it retrieves the first 30 tasks.\nfunc (i *Inspector) ListCompletedTasks(queue string, opts ...ListOption) ([]*TaskInfo, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\topt := composeListOptions(opts...)\n\tpgn := rdb.Pagination{Size: opt.pageSize, Page: opt.pageNum - 1}\n\tinfos, err := i.rdb.ListCompleted(queue, pgn)\n\tswitch {\n\tcase errors.IsQueueNotFound(err):\n\t\treturn nil, fmt.Errorf(\"asynq: %w\", ErrQueueNotFound)\n\tcase err != nil:\n\t\treturn nil, fmt.Errorf(\"asynq: %v\", err)\n\t}\n\tvar tasks []*TaskInfo\n\tfor _, i := range infos {\n\t\ttasks = append(tasks, newTaskInfo(\n\t\t\ti.Message,\n\t\t\ti.State,\n\t\t\ti.NextProcessAt,\n\t\t\ti.Result,\n\t\t))\n\t}\n\treturn tasks, nil\n}\n\n// DeleteAllPendingTasks deletes all pending tasks from the specified queue,\n// and reports the number tasks deleted.\nfunc (i *Inspector) DeleteAllPendingTasks(queue string) (int, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn 0, err\n\t}\n\tn, err := i.rdb.DeleteAllPendingTasks(queue)\n\treturn int(n), err\n}\n\n// DeleteAllScheduledTasks deletes all scheduled tasks from the specified queue,\n// and reports the number tasks deleted.\nfunc (i *Inspector) DeleteAllScheduledTasks(queue string) (int, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn 0, err\n\t}\n\tn, err := i.rdb.DeleteAllScheduledTasks(queue)\n\treturn int(n), err\n}\n\n// DeleteAllRetryTasks deletes all retry tasks from the specified queue,\n// and reports the number tasks deleted.\nfunc (i *Inspector) DeleteAllRetryTasks(queue string) (int, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn 0, err\n\t}\n\tn, err := i.rdb.DeleteAllRetryTasks(queue)\n\treturn int(n), err\n}\n\n// DeleteAllArchivedTasks deletes all archived tasks from the specified queue,\n// and reports the number tasks deleted.\nfunc (i *Inspector) DeleteAllArchivedTasks(queue string) (int, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn 0, err\n\t}\n\tn, err := i.rdb.DeleteAllArchivedTasks(queue)\n\treturn int(n), err\n}\n\n// DeleteAllCompletedTasks deletes all completed tasks from the specified queue,\n// and reports the number tasks deleted.\nfunc (i *Inspector) DeleteAllCompletedTasks(queue string) (int, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn 0, err\n\t}\n\tn, err := i.rdb.DeleteAllCompletedTasks(queue)\n\treturn int(n), err\n}\n\n// DeleteAllAggregatingTasks deletes all tasks from the specified group,\n// and reports the number of tasks deleted.\nfunc (i *Inspector) DeleteAllAggregatingTasks(queue, group string) (int, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn 0, err\n\t}\n\tn, err := i.rdb.DeleteAllAggregatingTasks(queue, group)\n\treturn int(n), err\n}\n\n// DeleteTask deletes a task with the given id from the given queue.\n// The task needs to be in pending, scheduled, retry, or archived state,\n// otherwise DeleteTask will return an error.\n//\n// If a queue with the given name doesn't exist, it returns an error wrapping ErrQueueNotFound.\n// If a task with the given id doesn't exist in the queue, it returns an error wrapping ErrTaskNotFound.\n// If the task is in active state, it returns a non-nil error.\nfunc (i *Inspector) DeleteTask(queue, id string) error {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn fmt.Errorf(\"asynq: %v\", err)\n\t}\n\terr := i.rdb.DeleteTask(queue, id)\n\tswitch {\n\tcase errors.IsQueueNotFound(err):\n\t\treturn fmt.Errorf(\"asynq: %w\", ErrQueueNotFound)\n\tcase errors.IsTaskNotFound(err):\n\t\treturn fmt.Errorf(\"asynq: %w\", ErrTaskNotFound)\n\tcase err != nil:\n\t\treturn fmt.Errorf(\"asynq: %v\", err)\n\t}\n\treturn nil\n\n}\n\n// RunAllScheduledTasks schedules all scheduled tasks from the given queue to run,\n// and reports the number of tasks scheduled to run.\nfunc (i *Inspector) RunAllScheduledTasks(queue string) (int, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn 0, err\n\t}\n\tn, err := i.rdb.RunAllScheduledTasks(queue)\n\treturn int(n), err\n}\n\n// RunAllRetryTasks schedules all retry tasks from the given queue to run,\n// and reports the number of tasks scheduled to run.\nfunc (i *Inspector) RunAllRetryTasks(queue string) (int, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn 0, err\n\t}\n\tn, err := i.rdb.RunAllRetryTasks(queue)\n\treturn int(n), err\n}\n\n// RunAllArchivedTasks schedules all archived tasks from the given queue to run,\n// and reports the number of tasks scheduled to run.\nfunc (i *Inspector) RunAllArchivedTasks(queue string) (int, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn 0, err\n\t}\n\tn, err := i.rdb.RunAllArchivedTasks(queue)\n\treturn int(n), err\n}\n\n// RunAllAggregatingTasks schedules all tasks from the given grou to run.\n// and reports the number of tasks scheduled to run.\nfunc (i *Inspector) RunAllAggregatingTasks(queue, group string) (int, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn 0, err\n\t}\n\tn, err := i.rdb.RunAllAggregatingTasks(queue, group)\n\treturn int(n), err\n}\n\n// RunTask updates the task to pending state given a queue name and task id.\n// The task needs to be in scheduled, retry, or archived state, otherwise RunTask\n// will return an error.\n//\n// If a queue with the given name doesn't exist, it returns an error wrapping ErrQueueNotFound.\n// If a task with the given id doesn't exist in the queue, it returns an error wrapping ErrTaskNotFound.\n// If the task is in pending or active state, it returns a non-nil error.\nfunc (i *Inspector) RunTask(queue, id string) error {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn fmt.Errorf(\"asynq: %v\", err)\n\t}\n\terr := i.rdb.RunTask(queue, id)\n\tswitch {\n\tcase errors.IsQueueNotFound(err):\n\t\treturn fmt.Errorf(\"asynq: %w\", ErrQueueNotFound)\n\tcase errors.IsTaskNotFound(err):\n\t\treturn fmt.Errorf(\"asynq: %w\", ErrTaskNotFound)\n\tcase err != nil:\n\t\treturn fmt.Errorf(\"asynq: %v\", err)\n\t}\n\treturn nil\n}\n\n// ArchiveAllPendingTasks archives all pending tasks from the given queue,\n// and reports the number of tasks archived.\nfunc (i *Inspector) ArchiveAllPendingTasks(queue string) (int, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn 0, err\n\t}\n\tn, err := i.rdb.ArchiveAllPendingTasks(queue)\n\treturn int(n), err\n}\n\n// ArchiveAllScheduledTasks archives all scheduled tasks from the given queue,\n// and reports the number of tasks archiveed.\nfunc (i *Inspector) ArchiveAllScheduledTasks(queue string) (int, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn 0, err\n\t}\n\tn, err := i.rdb.ArchiveAllScheduledTasks(queue)\n\treturn int(n), err\n}\n\n// ArchiveAllRetryTasks archives all retry tasks from the given queue,\n// and reports the number of tasks archiveed.\nfunc (i *Inspector) ArchiveAllRetryTasks(queue string) (int, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn 0, err\n\t}\n\tn, err := i.rdb.ArchiveAllRetryTasks(queue)\n\treturn int(n), err\n}\n\n// ArchiveAllAggregatingTasks archives all tasks from the given group,\n// and reports the number of tasks archived.\nfunc (i *Inspector) ArchiveAllAggregatingTasks(queue, group string) (int, error) {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn 0, err\n\t}\n\tn, err := i.rdb.ArchiveAllAggregatingTasks(queue, group)\n\treturn int(n), err\n}\n\n// ArchiveTask archives a task with the given id in the given queue.\n// The task needs to be in pending, scheduled, or retry state, otherwise ArchiveTask\n// will return an error.\n//\n// If a queue with the given name doesn't exist, it returns an error wrapping ErrQueueNotFound.\n// If a task with the given id doesn't exist in the queue, it returns an error wrapping ErrTaskNotFound.\n// If the task is in already archived, it returns a non-nil error.\nfunc (i *Inspector) ArchiveTask(queue, id string) error {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn fmt.Errorf(\"asynq: err\")\n\t}\n\terr := i.rdb.ArchiveTask(queue, id)\n\tswitch {\n\tcase errors.IsQueueNotFound(err):\n\t\treturn fmt.Errorf(\"asynq: %w\", ErrQueueNotFound)\n\tcase errors.IsTaskNotFound(err):\n\t\treturn fmt.Errorf(\"asynq: %w\", ErrTaskNotFound)\n\tcase err != nil:\n\t\treturn fmt.Errorf(\"asynq: %v\", err)\n\t}\n\treturn nil\n}\n\n// CancelProcessing sends a signal to cancel processing of the task\n// given a task id. CancelProcessing is best-effort, which means that it does not\n// guarantee that the task with the given id will be canceled. The return\n// value only indicates whether the cancelation signal has been sent.\nfunc (i *Inspector) CancelProcessing(id string) error {\n\treturn i.rdb.PublishCancelation(id)\n}\n\n// PauseQueue pauses task processing on the specified queue.\n// If the queue is already paused, it will return a non-nil error.\nfunc (i *Inspector) PauseQueue(queue string) error {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn err\n\t}\n\treturn i.rdb.Pause(queue)\n}\n\n// UnpauseQueue resumes task processing on the specified queue.\n// If the queue is not paused, it will return a non-nil error.\nfunc (i *Inspector) UnpauseQueue(queue string) error {\n\tif err := base.ValidateQueueName(queue); err != nil {\n\t\treturn err\n\t}\n\treturn i.rdb.Unpause(queue)\n}\n\n// Servers return a list of running servers' information.\nfunc (i *Inspector) Servers() ([]*ServerInfo, error) {\n\tservers, err := i.rdb.ListServers()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tworkers, err := i.rdb.ListWorkers()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tm := make(map[string]*ServerInfo) // ServerInfo keyed by serverID\n\tfor _, s := range servers {\n\t\tm[s.ServerID] = &ServerInfo{\n\t\t\tID:             s.ServerID,\n\t\t\tHost:           s.Host,\n\t\t\tPID:            s.PID,\n\t\t\tConcurrency:    s.Concurrency,\n\t\t\tQueues:         s.Queues,\n\t\t\tStrictPriority: s.StrictPriority,\n\t\t\tStarted:        s.Started,\n\t\t\tStatus:         s.Status,\n\t\t\tActiveWorkers:  make([]*WorkerInfo, 0),\n\t\t}\n\t}\n\tfor _, w := range workers {\n\t\tsrvInfo, ok := m[w.ServerID]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\twrkInfo := &WorkerInfo{\n\t\t\tTaskID:      w.ID,\n\t\t\tTaskType:    w.Type,\n\t\t\tTaskPayload: w.Payload,\n\t\t\tQueue:       w.Queue,\n\t\t\tStarted:     w.Started,\n\t\t\tDeadline:    w.Deadline,\n\t\t}\n\t\tsrvInfo.ActiveWorkers = append(srvInfo.ActiveWorkers, wrkInfo)\n\t}\n\tvar out []*ServerInfo\n\tfor _, srvInfo := range m {\n\t\tout = append(out, srvInfo)\n\t}\n\treturn out, nil\n}\n\n// ServerInfo describes a running Server instance.\ntype ServerInfo struct {\n\t// Unique Identifier for the server.\n\tID string\n\t// Host machine on which the server is running.\n\tHost string\n\t// PID of the process in which the server is running.\n\tPID int\n\n\t// Server configuration details.\n\t// See Config doc for field descriptions.\n\tConcurrency    int\n\tQueues         map[string]int\n\tStrictPriority bool\n\n\t// Time the server started.\n\tStarted time.Time\n\t// Status indicates the status of the server.\n\t// TODO: Update comment with more details.\n\tStatus string\n\t// A List of active workers currently processing tasks.\n\tActiveWorkers []*WorkerInfo\n}\n\n// WorkerInfo describes a running worker processing a task.\ntype WorkerInfo struct {\n\t// ID of the task the worker is processing.\n\tTaskID string\n\t// Type of the task the worker is processing.\n\tTaskType string\n\t// Payload of the task the worker is processing.\n\tTaskPayload []byte\n\t// Queue from which the worker got its task.\n\tQueue string\n\t// Time the worker started processing the task.\n\tStarted time.Time\n\t// Time the worker needs to finish processing the task by.\n\tDeadline time.Time\n}\n\n// ClusterKeySlot returns an integer identifying the hash slot the given queue hashes to.\nfunc (i *Inspector) ClusterKeySlot(queue string) (int64, error) {\n\treturn i.rdb.ClusterKeySlot(queue)\n}\n\n// ClusterNode describes a node in redis cluster.\ntype ClusterNode struct {\n\t// Node ID in the cluster.\n\tID string\n\n\t// Address of the node.\n\tAddr string\n}\n\n// ClusterNodes returns a list of nodes the given queue belongs to.\n//\n// Only relevant if task queues are stored in redis cluster.\nfunc (i *Inspector) ClusterNodes(queue string) ([]*ClusterNode, error) {\n\tnodes, err := i.rdb.ClusterNodes(queue)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar res []*ClusterNode\n\tfor _, node := range nodes {\n\t\tres = append(res, &ClusterNode{ID: node.ID, Addr: node.Addr})\n\t}\n\treturn res, nil\n}\n\n// SchedulerEntry holds information about a periodic task registered with a scheduler.\ntype SchedulerEntry struct {\n\t// Identifier of this entry.\n\tID string\n\n\t// Spec describes the schedule of this entry.\n\tSpec string\n\n\t// Periodic Task registered for this entry.\n\tTask *Task\n\n\t// Opts is the options for the periodic task.\n\tOpts []Option\n\n\t// Next shows the next time the task will be enqueued.\n\tNext time.Time\n\n\t// Prev shows the last time the task was enqueued.\n\t// Zero time if task was never enqueued.\n\tPrev time.Time\n}\n\n// SchedulerEntries returns a list of all entries registered with\n// currently running schedulers.\nfunc (i *Inspector) SchedulerEntries() ([]*SchedulerEntry, error) {\n\tvar entries []*SchedulerEntry\n\tres, err := i.rdb.ListSchedulerEntries()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, e := range res {\n\t\ttask := NewTask(e.Type, e.Payload)\n\t\tvar opts []Option\n\t\tfor _, s := range e.Opts {\n\t\t\tif o, err := parseOption(s); err == nil {\n\t\t\t\t// ignore bad data\n\t\t\t\topts = append(opts, o)\n\t\t\t}\n\t\t}\n\t\tentries = append(entries, &SchedulerEntry{\n\t\t\tID:   e.ID,\n\t\t\tSpec: e.Spec,\n\t\t\tTask: task,\n\t\t\tOpts: opts,\n\t\t\tNext: e.Next,\n\t\t\tPrev: e.Prev,\n\t\t})\n\t}\n\treturn entries, nil\n}\n\n// parseOption interprets a string s as an Option and returns the Option if parsing is successful,\n// otherwise returns non-nil error.\nfunc parseOption(s string) (Option, error) {\n\tfn, arg := parseOptionFunc(s), parseOptionArg(s)\n\tswitch fn {\n\tcase \"Queue\":\n\t\tqueue, err := strconv.Unquote(arg)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn Queue(queue), nil\n\tcase \"MaxRetry\":\n\t\tn, err := strconv.Atoi(arg)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn MaxRetry(n), nil\n\tcase \"Timeout\":\n\t\td, err := time.ParseDuration(arg)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn Timeout(d), nil\n\tcase \"Deadline\":\n\t\tt, err := time.Parse(time.UnixDate, arg)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn Deadline(t), nil\n\tcase \"Unique\":\n\t\td, err := time.ParseDuration(arg)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn Unique(d), nil\n\tcase \"ProcessAt\":\n\t\tt, err := time.Parse(time.UnixDate, arg)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn ProcessAt(t), nil\n\tcase \"ProcessIn\":\n\t\td, err := time.ParseDuration(arg)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn ProcessIn(d), nil\n\tcase \"Retention\":\n\t\td, err := time.ParseDuration(arg)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn Retention(d), nil\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"cannot not parse option string %q\", s)\n\t}\n}\n\nfunc parseOptionFunc(s string) string {\n\ti := strings.Index(s, \"(\")\n\treturn s[:i]\n}\n\nfunc parseOptionArg(s string) string {\n\ti := strings.Index(s, \"(\")\n\tif i >= 0 {\n\t\tj := strings.Index(s, \")\")\n\t\tif j > i {\n\t\t\treturn s[i+1 : j]\n\t\t}\n\t}\n\treturn \"\"\n}\n\n// SchedulerEnqueueEvent holds information about an enqueue event by a scheduler.\ntype SchedulerEnqueueEvent struct {\n\t// ID of the task that was enqueued.\n\tTaskID string\n\n\t// Time the task was enqueued.\n\tEnqueuedAt time.Time\n}\n\n// ListSchedulerEnqueueEvents retrieves a list of enqueue events from the specified scheduler entry.\n//\n// By default, it retrieves the first 30 tasks.\nfunc (i *Inspector) ListSchedulerEnqueueEvents(entryID string, opts ...ListOption) ([]*SchedulerEnqueueEvent, error) {\n\topt := composeListOptions(opts...)\n\tpgn := rdb.Pagination{Size: opt.pageSize, Page: opt.pageNum - 1}\n\tdata, err := i.rdb.ListSchedulerEnqueueEvents(entryID, pgn)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar events []*SchedulerEnqueueEvent\n\tfor _, e := range data {\n\t\tevents = append(events, &SchedulerEnqueueEvent{TaskID: e.TaskID, EnqueuedAt: e.EnqueuedAt})\n\t}\n\treturn events, nil\n}\n"
        },
        {
          "name": "inspector_test.go",
          "type": "blob",
          "size": 93.0234375,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"sort\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/google/go-cmp/cmp/cmpopts\"\n\t\"github.com/google/uuid\"\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/rdb\"\n\th \"github.com/hibiken/asynq/internal/testutil\"\n\t\"github.com/hibiken/asynq/internal/timeutil\"\n\t\"github.com/redis/go-redis/v9\"\n)\n\nfunc testInspectorQueues(t *testing.T, inspector *Inspector, r redis.UniversalClient) {\n\ttests := []struct {\n\t\tqueues []string\n\t}{\n\t\t{queues: []string{\"default\"}},\n\t\t{queues: []string{\"custom1\", \"custom2\"}},\n\t\t{queues: []string{\"default\", \"custom1\", \"custom2\"}},\n\t\t{queues: []string{}},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\tfor _, qname := range tc.queues {\n\t\t\tif err := r.SAdd(context.Background(), base.AllQueues, qname).Err(); err != nil {\n\t\t\t\tt.Fatalf(\"could not initialize all queue set: %v\", err)\n\t\t\t}\n\t\t}\n\t\tgot, err := inspector.Queues()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Queues() returned an error: %v\", err)\n\t\t\tcontinue\n\t\t}\n\t\tif diff := cmp.Diff(tc.queues, got, h.SortStringSliceOpt); diff != \"\" {\n\t\t\tt.Errorf(\"Queues() = %v, want %v; (-want, +got)\\n%s\", got, tc.queues, diff)\n\t\t}\n\t}\n}\n\nfunc TestInspectorQueues(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tinspector := NewInspector(getRedisConnOpt(t))\n\ttestInspectorQueues(t, inspector, r)\n}\n\nfunc TestInspectorFromRedisClientQueues(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tredisClient := getRedisConnOpt(t).MakeRedisClient().(redis.UniversalClient)\n\tinspector := NewInspectorFromRedisClient(redisClient)\n\ttestInspectorQueues(t, inspector, r)\n}\n\nfunc TestInspectorDeleteQueue(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tinspector := NewInspector(getRedisConnOpt(t))\n\tdefer inspector.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tm4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"custom\")\n\n\ttests := []struct {\n\t\tpending   map[string][]*base.TaskMessage\n\t\tactive    map[string][]*base.TaskMessage\n\t\tscheduled map[string][]base.Z\n\t\tretry     map[string][]base.Z\n\t\tarchived  map[string][]base.Z\n\t\tqname     string // queue to remove\n\t\tforce     bool\n\t}{\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m2},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tqname: \"custom\",\n\t\t\tforce: false,\n\t\t},\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m2},\n\t\t\t\t\"custom\":  {m3},\n\t\t\t},\n\t\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {{Message: m4, Score: time.Now().Unix()}},\n\t\t\t},\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tqname: \"custom\",\n\t\t\tforce: true, // allow removing non-empty queue\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\t\th.SeedAllActiveQueues(t, r, tc.active)\n\t\th.SeedAllScheduledQueues(t, r, tc.scheduled)\n\t\th.SeedAllRetryQueues(t, r, tc.retry)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\n\t\terr := inspector.DeleteQueue(tc.qname, tc.force)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"DeleteQueue(%q, %t) = %v, want nil\",\n\t\t\t\ttc.qname, tc.force, err)\n\t\t\tcontinue\n\t\t}\n\t\tif r.SIsMember(context.Background(), base.AllQueues, tc.qname).Val() {\n\t\t\tt.Errorf(\"%q is a member of %q\", tc.qname, base.AllQueues)\n\t\t}\n\t}\n}\n\nfunc TestInspectorDeleteQueueErrorQueueNotEmpty(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tinspector := NewInspector(getRedisConnOpt(t))\n\tdefer inspector.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tm4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"custom\")\n\n\ttests := []struct {\n\t\tpending   map[string][]*base.TaskMessage\n\t\tactive    map[string][]*base.TaskMessage\n\t\tscheduled map[string][]base.Z\n\t\tretry     map[string][]base.Z\n\t\tarchived  map[string][]base.Z\n\t\tqname     string // queue to remove\n\t\tforce     bool\n\t}{\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m2},\n\t\t\t},\n\t\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m3, m4},\n\t\t\t},\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\tforce: false,\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\t\th.SeedAllActiveQueues(t, r, tc.active)\n\t\th.SeedAllScheduledQueues(t, r, tc.scheduled)\n\t\th.SeedAllRetryQueues(t, r, tc.retry)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\n\t\terr := inspector.DeleteQueue(tc.qname, tc.force)\n\t\tif !errors.Is(err, ErrQueueNotEmpty) {\n\t\t\tt.Errorf(\"DeleteQueue(%v, %t) did not return ErrQueueNotEmpty\",\n\t\t\t\ttc.qname, tc.force)\n\t\t}\n\t}\n}\n\nfunc TestInspectorDeleteQueueErrorQueueNotFound(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tinspector := NewInspector(getRedisConnOpt(t))\n\tdefer inspector.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tm4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"custom\")\n\n\ttests := []struct {\n\t\tpending   map[string][]*base.TaskMessage\n\t\tactive    map[string][]*base.TaskMessage\n\t\tscheduled map[string][]base.Z\n\t\tretry     map[string][]base.Z\n\t\tarchived  map[string][]base.Z\n\t\tqname     string // queue to remove\n\t\tforce     bool\n\t}{\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m2},\n\t\t\t},\n\t\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m3, m4},\n\t\t\t},\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tqname: \"nonexistent\",\n\t\t\tforce: false,\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\t\th.SeedAllActiveQueues(t, r, tc.active)\n\t\th.SeedAllScheduledQueues(t, r, tc.scheduled)\n\t\th.SeedAllRetryQueues(t, r, tc.retry)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\n\t\terr := inspector.DeleteQueue(tc.qname, tc.force)\n\t\tif !errors.Is(err, ErrQueueNotFound) {\n\t\t\tt.Errorf(\"DeleteQueue(%v, %t) did not return ErrQueueNotFound\",\n\t\t\t\ttc.qname, tc.force)\n\t\t}\n\t}\n}\n\nfunc TestInspectorGetQueueInfo(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessage(\"task3\", nil)\n\tm4 := h.NewTaskMessage(\"task4\", nil)\n\tm5 := h.NewTaskMessageWithQueue(\"task5\", nil, \"critical\")\n\tm6 := h.NewTaskMessageWithQueue(\"task6\", nil, \"low\")\n\tnow := time.Now()\n\ttimeCmpOpt := cmpopts.EquateApproxTime(time.Second)\n\tignoreMemUsg := cmpopts.IgnoreFields(QueueInfo{}, \"MemoryUsage\")\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\tinspector.rdb.SetClock(timeutil.NewSimulatedClock(now))\n\n\ttests := []struct {\n\t\tpending                         map[string][]*base.TaskMessage\n\t\tactive                          map[string][]*base.TaskMessage\n\t\tscheduled                       map[string][]base.Z\n\t\tretry                           map[string][]base.Z\n\t\tarchived                        map[string][]base.Z\n\t\tcompleted                       map[string][]base.Z\n\t\tprocessed                       map[string]int\n\t\tfailed                          map[string]int\n\t\tprocessedTotal                  map[string]int\n\t\tfailedTotal                     map[string]int\n\t\toldestPendingMessageEnqueueTime map[string]time.Time\n\t\tqname                           string\n\t\twant                            *QueueInfo\n\t}{\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {m1},\n\t\t\t\t\"critical\": {m5},\n\t\t\t\t\"low\":      {m6},\n\t\t\t},\n\t\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {m2},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{Message: m3, Score: now.Add(time.Hour).Unix()},\n\t\t\t\t\t{Message: m4, Score: now.Unix()},\n\t\t\t\t},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t\tcompleted: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t\tprocessed: map[string]int{\n\t\t\t\t\"default\":  120,\n\t\t\t\t\"critical\": 100,\n\t\t\t\t\"low\":      42,\n\t\t\t},\n\t\t\tfailed: map[string]int{\n\t\t\t\t\"default\":  2,\n\t\t\t\t\"critical\": 0,\n\t\t\t\t\"low\":      5,\n\t\t\t},\n\t\t\tprocessedTotal: map[string]int{\n\t\t\t\t\"default\":  11111,\n\t\t\t\t\"critical\": 22222,\n\t\t\t\t\"low\":      33333,\n\t\t\t},\n\t\t\tfailedTotal: map[string]int{\n\t\t\t\t\"default\":  111,\n\t\t\t\t\"critical\": 222,\n\t\t\t\t\"low\":      333,\n\t\t\t},\n\t\t\toldestPendingMessageEnqueueTime: map[string]time.Time{\n\t\t\t\t\"default\":  now.Add(-15 * time.Second),\n\t\t\t\t\"critical\": now.Add(-200 * time.Millisecond),\n\t\t\t\t\"low\":      now.Add(-30 * time.Second),\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant: &QueueInfo{\n\t\t\t\tQueue:          \"default\",\n\t\t\t\tLatency:        15 * time.Second,\n\t\t\t\tSize:           4,\n\t\t\t\tPending:        1,\n\t\t\t\tActive:         1,\n\t\t\t\tScheduled:      2,\n\t\t\t\tRetry:          0,\n\t\t\t\tArchived:       0,\n\t\t\t\tCompleted:      0,\n\t\t\t\tProcessed:      120,\n\t\t\t\tFailed:         2,\n\t\t\t\tProcessedTotal: 11111,\n\t\t\t\tFailedTotal:    111,\n\t\t\t\tPaused:         false,\n\t\t\t\tTimestamp:      now,\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\t\th.SeedAllActiveQueues(t, r, tc.active)\n\t\th.SeedAllScheduledQueues(t, r, tc.scheduled)\n\t\th.SeedAllRetryQueues(t, r, tc.retry)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\t\th.SeedAllCompletedQueues(t, r, tc.completed)\n\t\tctx := context.Background()\n\t\tfor qname, n := range tc.processed {\n\t\t\tr.Set(ctx, base.ProcessedKey(qname, now), n, 0)\n\t\t}\n\t\tfor qname, n := range tc.failed {\n\t\t\tr.Set(ctx, base.FailedKey(qname, now), n, 0)\n\t\t}\n\t\tfor qname, n := range tc.processedTotal {\n\t\t\tr.Set(ctx, base.ProcessedTotalKey(qname), n, 0)\n\t\t}\n\t\tfor qname, n := range tc.failedTotal {\n\t\t\tr.Set(ctx, base.FailedTotalKey(qname), n, 0)\n\t\t}\n\t\tfor qname, enqueueTime := range tc.oldestPendingMessageEnqueueTime {\n\t\t\tif enqueueTime.IsZero() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\toldestPendingMessageID := r.LRange(ctx, base.PendingKey(qname), -1, -1).Val()[0] // get the right most msg in the list\n\t\t\tr.HSet(ctx, base.TaskKey(qname, oldestPendingMessageID), \"pending_since\", enqueueTime.UnixNano())\n\t\t}\n\n\t\tgot, err := inspector.GetQueueInfo(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"r.GetQueueInfo(%q) = %v, %v, want %v, nil\",\n\t\t\t\ttc.qname, got, err, tc.want)\n\t\t\tcontinue\n\t\t}\n\t\tif diff := cmp.Diff(tc.want, got, timeCmpOpt, ignoreMemUsg); diff != \"\" {\n\t\t\tt.Errorf(\"r.GetQueueInfo(%q) = %v, %v, want %v, nil; (-want, +got)\\n%s\",\n\t\t\t\ttc.qname, got, err, tc.want, diff)\n\t\t\tcontinue\n\t\t}\n\t}\n\n}\n\nfunc TestInspectorHistory(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tnow := time.Now().UTC()\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tqname string // queue of interest\n\t\tn     int    // number of days\n\t}{\n\t\t{\"default\", 90},\n\t\t{\"custom\", 7},\n\t\t{\"default\", 1},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\n\t\tr.SAdd(context.Background(), base.AllQueues, tc.qname)\n\t\t// populate last n days data\n\t\tfor i := 0; i < tc.n; i++ {\n\t\t\tts := now.Add(-time.Duration(i) * 24 * time.Hour)\n\t\t\tprocessedKey := base.ProcessedKey(tc.qname, ts)\n\t\t\tfailedKey := base.FailedKey(tc.qname, ts)\n\t\t\tr.Set(context.Background(), processedKey, (i+1)*1000, 0)\n\t\t\tr.Set(context.Background(), failedKey, (i+1)*10, 0)\n\t\t}\n\n\t\tgot, err := inspector.History(tc.qname, tc.n)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Inspector.History(%q, %d) returned error: %v\", tc.qname, tc.n, err)\n\t\t\tcontinue\n\t\t}\n\t\tif len(got) != tc.n {\n\t\t\tt.Errorf(\"Inspector.History(%q, %d) returned %d daily stats, want %d\",\n\t\t\t\ttc.qname, tc.n, len(got), tc.n)\n\t\t\tcontinue\n\t\t}\n\t\tfor i := 0; i < tc.n; i++ {\n\t\t\twant := &DailyStats{\n\t\t\t\tQueue:     tc.qname,\n\t\t\t\tProcessed: (i + 1) * 1000,\n\t\t\t\tFailed:    (i + 1) * 10,\n\t\t\t\tDate:      now.Add(-time.Duration(i) * 24 * time.Hour),\n\t\t\t}\n\t\t\t// Allow 2 seconds difference in timestamp.\n\t\t\ttimeCmpOpt := cmpopts.EquateApproxTime(2 * time.Second)\n\t\t\tif diff := cmp.Diff(want, got[i], timeCmpOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"Inspector.History %d days ago data; got %+v, want %+v; (-want,+got):\\n%s\",\n\t\t\t\t\ti, got[i], want, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc createPendingTask(msg *base.TaskMessage) *TaskInfo {\n\treturn newTaskInfo(msg, base.TaskStatePending, time.Now(), nil)\n}\n\nfunc TestInspectorGetTaskInfo(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\n\tm1 := h.NewTaskMessageWithQueue(\"task1\", nil, \"default\")\n\tm2 := h.NewTaskMessageWithQueue(\"task2\", nil, \"default\")\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tm4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"custom\")\n\tm5 := h.NewTaskMessageWithQueue(\"task5\", nil, \"custom\")\n\n\tnow := time.Now()\n\tfiveMinsFromNow := now.Add(5 * time.Minute)\n\toneHourFromNow := now.Add(1 * time.Hour)\n\ttwoHoursAgo := now.Add(-2 * time.Hour)\n\n\tfixtures := struct {\n\t\tactive    map[string][]*base.TaskMessage\n\t\tpending   map[string][]*base.TaskMessage\n\t\tscheduled map[string][]base.Z\n\t\tretry     map[string][]base.Z\n\t\tarchived  map[string][]base.Z\n\t}{\n\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\"default\": {m1},\n\t\t\t\"custom\":  {},\n\t\t},\n\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\"default\": {},\n\t\t\t\"custom\":  {m5},\n\t\t},\n\t\tscheduled: map[string][]base.Z{\n\t\t\t\"default\": {{Message: m2, Score: fiveMinsFromNow.Unix()}},\n\t\t\t\"custom\":  {},\n\t\t},\n\t\tretry: map[string][]base.Z{\n\t\t\t\"default\": {},\n\t\t\t\"custom\":  {{Message: m3, Score: oneHourFromNow.Unix()}},\n\t\t},\n\t\tarchived: map[string][]base.Z{\n\t\t\t\"default\": {},\n\t\t\t\"custom\":  {{Message: m4, Score: twoHoursAgo.Unix()}},\n\t\t},\n\t}\n\n\th.SeedAllActiveQueues(t, r, fixtures.active)\n\th.SeedAllPendingQueues(t, r, fixtures.pending)\n\th.SeedAllScheduledQueues(t, r, fixtures.scheduled)\n\th.SeedAllRetryQueues(t, r, fixtures.retry)\n\th.SeedAllArchivedQueues(t, r, fixtures.archived)\n\n\ttests := []struct {\n\t\tqname string\n\t\tid    string\n\t\twant  *TaskInfo\n\t}{\n\t\t{\n\t\t\tqname: \"default\",\n\t\t\tid:    m1.ID,\n\t\t\twant: newTaskInfo(\n\t\t\t\tm1,\n\t\t\t\tbase.TaskStateActive,\n\t\t\t\ttime.Time{}, // zero value for n/a\n\t\t\t\tnil,\n\t\t\t),\n\t\t},\n\t\t{\n\t\t\tqname: \"default\",\n\t\t\tid:    m2.ID,\n\t\t\twant: newTaskInfo(\n\t\t\t\tm2,\n\t\t\t\tbase.TaskStateScheduled,\n\t\t\t\tfiveMinsFromNow,\n\t\t\t\tnil,\n\t\t\t),\n\t\t},\n\t\t{\n\t\t\tqname: \"custom\",\n\t\t\tid:    m3.ID,\n\t\t\twant: newTaskInfo(\n\t\t\t\tm3,\n\t\t\t\tbase.TaskStateRetry,\n\t\t\t\toneHourFromNow,\n\t\t\t\tnil,\n\t\t\t),\n\t\t},\n\t\t{\n\t\t\tqname: \"custom\",\n\t\t\tid:    m4.ID,\n\t\t\twant: newTaskInfo(\n\t\t\t\tm4,\n\t\t\t\tbase.TaskStateArchived,\n\t\t\t\ttime.Time{}, // zero value for n/a\n\t\t\t\tnil,\n\t\t\t),\n\t\t},\n\t\t{\n\t\t\tqname: \"custom\",\n\t\t\tid:    m5.ID,\n\t\t\twant: newTaskInfo(\n\t\t\t\tm5,\n\t\t\t\tbase.TaskStatePending,\n\t\t\t\tnow,\n\t\t\t\tnil,\n\t\t\t),\n\t\t},\n\t}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\tfor _, tc := range tests {\n\t\tgot, err := inspector.GetTaskInfo(tc.qname, tc.id)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"GetTaskInfo(%q, %q) returned error: %v\", tc.qname, tc.id, err)\n\t\t\tcontinue\n\t\t}\n\t\tcmpOpts := []cmp.Option{\n\t\t\tcmp.AllowUnexported(TaskInfo{}),\n\t\t\tcmpopts.EquateApproxTime(2 * time.Second),\n\t\t}\n\t\tif diff := cmp.Diff(tc.want, got, cmpOpts...); diff != \"\" {\n\t\t\tt.Errorf(\"GetTaskInfo(%q, %q) = %v, want %v; (-want, +got)\\n%s\", tc.qname, tc.id, got, tc.want, diff)\n\t\t}\n\t}\n}\n\nfunc TestInspectorGetTaskInfoError(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\n\tm1 := h.NewTaskMessageWithQueue(\"task1\", nil, \"default\")\n\tm2 := h.NewTaskMessageWithQueue(\"task2\", nil, \"default\")\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tm4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"custom\")\n\tm5 := h.NewTaskMessageWithQueue(\"task5\", nil, \"custom\")\n\n\tnow := time.Now()\n\tfiveMinsFromNow := now.Add(5 * time.Minute)\n\toneHourFromNow := now.Add(1 * time.Hour)\n\ttwoHoursAgo := now.Add(-2 * time.Hour)\n\n\tfixtures := struct {\n\t\tactive    map[string][]*base.TaskMessage\n\t\tpending   map[string][]*base.TaskMessage\n\t\tscheduled map[string][]base.Z\n\t\tretry     map[string][]base.Z\n\t\tarchived  map[string][]base.Z\n\t}{\n\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\"default\": {m1},\n\t\t\t\"custom\":  {},\n\t\t},\n\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\"default\": {},\n\t\t\t\"custom\":  {m5},\n\t\t},\n\t\tscheduled: map[string][]base.Z{\n\t\t\t\"default\": {{Message: m2, Score: fiveMinsFromNow.Unix()}},\n\t\t\t\"custom\":  {},\n\t\t},\n\t\tretry: map[string][]base.Z{\n\t\t\t\"default\": {},\n\t\t\t\"custom\":  {{Message: m3, Score: oneHourFromNow.Unix()}},\n\t\t},\n\t\tarchived: map[string][]base.Z{\n\t\t\t\"default\": {},\n\t\t\t\"custom\":  {{Message: m4, Score: twoHoursAgo.Unix()}},\n\t\t},\n\t}\n\n\th.SeedAllActiveQueues(t, r, fixtures.active)\n\th.SeedAllPendingQueues(t, r, fixtures.pending)\n\th.SeedAllScheduledQueues(t, r, fixtures.scheduled)\n\th.SeedAllRetryQueues(t, r, fixtures.retry)\n\th.SeedAllArchivedQueues(t, r, fixtures.archived)\n\n\ttests := []struct {\n\t\tqname   string\n\t\tid      string\n\t\twantErr error\n\t}{\n\t\t{\n\t\t\tqname:   \"nonexistent\",\n\t\t\tid:      m1.ID,\n\t\t\twantErr: ErrQueueNotFound,\n\t\t},\n\t\t{\n\t\t\tqname:   \"default\",\n\t\t\tid:      uuid.NewString(),\n\t\t\twantErr: ErrTaskNotFound,\n\t\t},\n\t}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\tfor _, tc := range tests {\n\t\tinfo, err := inspector.GetTaskInfo(tc.qname, tc.id)\n\t\tif info != nil {\n\t\t\tt.Errorf(\"GetTaskInfo(%q, %q) returned info: %v\", tc.qname, tc.id, info)\n\t\t}\n\t\tif !errors.Is(err, tc.wantErr) {\n\t\t\tt.Errorf(\"GetTaskInfo(%q, %q) returned unexpected error: %v, want %v\", tc.qname, tc.id, err, tc.wantErr)\n\t\t}\n\t}\n}\n\nfunc TestInspectorListPendingTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"critical\")\n\tm4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"low\")\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tdesc    string\n\t\tpending map[string][]*base.TaskMessage\n\t\tqname   string\n\t\twant    []*TaskInfo\n\t}{\n\t\t{\n\t\t\tdesc: \"with default queue\",\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m2},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant: []*TaskInfo{\n\t\t\t\tcreatePendingTask(m1),\n\t\t\t\tcreatePendingTask(m2),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"with named queue\",\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {m1, m2},\n\t\t\t\t\"critical\": {m3},\n\t\t\t\t\"low\":      {m4},\n\t\t\t},\n\t\t\tqname: \"critical\",\n\t\t\twant: []*TaskInfo{\n\t\t\t\tcreatePendingTask(m3),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"with empty queue\",\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  []*TaskInfo(nil),\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\tfor q, msgs := range tc.pending {\n\t\t\th.SeedPendingQueue(t, r, msgs, q)\n\t\t}\n\n\t\tgot, err := inspector.ListPendingTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%s; ListPendingTasks(%q) returned error: %v\",\n\t\t\t\ttc.desc, tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tcmpOpts := []cmp.Option{\n\t\t\tcmpopts.EquateApproxTime(2 * time.Second),\n\t\t\tcmp.AllowUnexported(TaskInfo{}),\n\t\t}\n\t\tif diff := cmp.Diff(tc.want, got, cmpOpts...); diff != \"\" {\n\t\t\tt.Errorf(\"%s; ListPendingTasks(%q) = %v, want %v; (-want,+got)\\n%s\",\n\t\t\t\ttc.desc, tc.qname, got, tc.want, diff)\n\t\t}\n\t}\n}\n\nfunc newOrphanedTaskInfo(msg *base.TaskMessage) *TaskInfo {\n\tinfo := newTaskInfo(msg, base.TaskStateActive, time.Time{}, nil)\n\tinfo.IsOrphaned = true\n\treturn info\n}\n\nfunc TestInspectorListActiveTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tm4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"custom\")\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\tnow := time.Now()\n\n\ttests := []struct {\n\t\tdesc   string\n\t\tactive map[string][]*base.TaskMessage\n\t\tlease  map[string][]base.Z\n\t\tqname  string\n\t\twant   []*TaskInfo\n\t}{\n\t\t{\n\t\t\tdesc: \"with a few active tasks\",\n\t\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m2},\n\t\t\t\t\"custom\":  {m3, m4},\n\t\t\t},\n\t\t\tlease: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{Message: m1, Score: now.Add(20 * time.Second).Unix()},\n\t\t\t\t\t{Message: m2, Score: now.Add(20 * time.Second).Unix()},\n\t\t\t\t},\n\t\t\t\t\"custom\": {\n\t\t\t\t\t{Message: m3, Score: now.Add(20 * time.Second).Unix()},\n\t\t\t\t\t{Message: m4, Score: now.Add(20 * time.Second).Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t\tqname: \"custom\",\n\t\t\twant: []*TaskInfo{\n\t\t\t\tnewTaskInfo(m3, base.TaskStateActive, time.Time{}, nil),\n\t\t\t\tnewTaskInfo(m4, base.TaskStateActive, time.Time{}, nil),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"with an orphaned task\",\n\t\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m2},\n\t\t\t\t\"custom\":  {m3, m4},\n\t\t\t},\n\t\t\tlease: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{Message: m1, Score: now.Add(20 * time.Second).Unix()},\n\t\t\t\t\t{Message: m2, Score: now.Add(-10 * time.Second).Unix()}, // orphaned task\n\t\t\t\t},\n\t\t\t\t\"custom\": {\n\t\t\t\t\t{Message: m3, Score: now.Add(20 * time.Second).Unix()},\n\t\t\t\t\t{Message: m4, Score: now.Add(20 * time.Second).Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant: []*TaskInfo{\n\t\t\t\tnewTaskInfo(m1, base.TaskStateActive, time.Time{}, nil),\n\t\t\t\tnewOrphanedTaskInfo(m2),\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllActiveQueues(t, r, tc.active)\n\t\th.SeedAllLease(t, r, tc.lease)\n\n\t\tgot, err := inspector.ListActiveTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%s; ListActiveTasks(%q) returned error: %v\", tc.qname, tc.desc, err)\n\t\t\tcontinue\n\t\t}\n\t\tif diff := cmp.Diff(tc.want, got, cmp.AllowUnexported(TaskInfo{})); diff != \"\" {\n\t\t\tt.Errorf(\"%s; ListActiveTask(%q) = %v, want %v; (-want,+got)\\n%s\",\n\t\t\t\ttc.desc, tc.qname, got, tc.want, diff)\n\t\t}\n\t}\n}\n\nfunc createScheduledTask(z base.Z) *TaskInfo {\n\treturn newTaskInfo(\n\t\tz.Message,\n\t\tbase.TaskStateScheduled,\n\t\ttime.Unix(z.Score, 0),\n\t\tnil,\n\t)\n}\n\nfunc TestInspectorListScheduledTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessage(\"task3\", nil)\n\tm4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\tz4 := base.Z{Message: m4, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tdesc      string\n\t\tscheduled map[string][]base.Z\n\t\tqname     string\n\t\twant      []*TaskInfo\n\t}{\n\t\t{\n\t\t\tdesc: \"with a few scheduled tasks\",\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2, z3},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\t// Should be sorted by NextProcessAt.\n\t\t\twant: []*TaskInfo{\n\t\t\t\tcreateScheduledTask(z3),\n\t\t\t\tcreateScheduledTask(z1),\n\t\t\t\tcreateScheduledTask(z2),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"with empty scheduled queue\",\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  []*TaskInfo(nil),\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllScheduledQueues(t, r, tc.scheduled)\n\n\t\tgot, err := inspector.ListScheduledTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%s; ListScheduledTasks(%q) returned error: %v\", tc.desc, tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tif diff := cmp.Diff(tc.want, got, cmp.AllowUnexported(TaskInfo{})); diff != \"\" {\n\t\t\tt.Errorf(\"%s; ListScheduledTask(%q) = %v, want %v; (-want,+got)\\n%s\",\n\t\t\t\ttc.desc, tc.qname, got, tc.want, diff)\n\t\t}\n\t}\n}\n\nfunc createRetryTask(z base.Z) *TaskInfo {\n\treturn newTaskInfo(\n\t\tz.Message,\n\t\tbase.TaskStateRetry,\n\t\ttime.Unix(z.Score, 0),\n\t\tnil,\n\t)\n}\n\nfunc TestInspectorListRetryTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessage(\"task3\", nil)\n\tm4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\tz4 := base.Z{Message: m4, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tdesc  string\n\t\tretry map[string][]base.Z\n\t\tqname string\n\t\twant  []*TaskInfo\n\t}{\n\t\t{\n\t\t\tdesc: \"with a few retry tasks\",\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2, z3},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\t// Should be sorted by NextProcessAt.\n\t\t\twant: []*TaskInfo{\n\t\t\t\tcreateRetryTask(z3),\n\t\t\t\tcreateRetryTask(z1),\n\t\t\t\tcreateRetryTask(z2),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"with empty retry queue\",\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  []*TaskInfo(nil),\n\t\t},\n\t\t// TODO(hibiken): ErrQueueNotFound when queue doesn't exist\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllRetryQueues(t, r, tc.retry)\n\n\t\tgot, err := inspector.ListRetryTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%s; ListRetryTasks(%q) returned error: %v\", tc.desc, tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tif diff := cmp.Diff(tc.want, got, cmp.AllowUnexported(TaskInfo{})); diff != \"\" {\n\t\t\tt.Errorf(\"%s; ListRetryTask(%q) = %v, want %v; (-want,+got)\\n%s\",\n\t\t\t\ttc.desc, tc.qname, got, tc.want, diff)\n\t\t}\n\t}\n}\n\nfunc createArchivedTask(z base.Z) *TaskInfo {\n\treturn newTaskInfo(\n\t\tz.Message,\n\t\tbase.TaskStateArchived,\n\t\ttime.Time{}, // zero value for n/a\n\t\tnil,\n\t)\n}\n\nfunc TestInspectorListArchivedTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessage(\"task3\", nil)\n\tm4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(-5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(-15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(-2 * time.Minute).Unix()}\n\tz4 := base.Z{Message: m4, Score: now.Add(-2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tdesc     string\n\t\tarchived map[string][]base.Z\n\t\tqname    string\n\t\twant     []*TaskInfo\n\t}{\n\t\t{\n\t\t\tdesc: \"with a few archived tasks\",\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2, z3},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\t// Should be sorted by LastFailedAt.\n\t\t\twant: []*TaskInfo{\n\t\t\t\tcreateArchivedTask(z2),\n\t\t\t\tcreateArchivedTask(z1),\n\t\t\t\tcreateArchivedTask(z3),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"with empty archived queue\",\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  []*TaskInfo(nil),\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\n\t\tgot, err := inspector.ListArchivedTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%s; ListArchivedTasks(%q) returned error: %v\", tc.desc, tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tif diff := cmp.Diff(tc.want, got, cmp.AllowUnexported(TaskInfo{})); diff != \"\" {\n\t\t\tt.Errorf(\"%s; ListArchivedTask(%q) = %v, want %v; (-want,+got)\\n%s\",\n\t\t\t\ttc.desc, tc.qname, got, tc.want, diff)\n\t\t}\n\t}\n}\n\nfunc newCompletedTaskMessage(typename, qname string, retention time.Duration, completedAt time.Time) *base.TaskMessage {\n\tmsg := h.NewTaskMessageWithQueue(typename, nil, qname)\n\tmsg.Retention = int64(retention.Seconds())\n\tmsg.CompletedAt = completedAt.Unix()\n\treturn msg\n}\n\nfunc createCompletedTask(z base.Z) *TaskInfo {\n\treturn newTaskInfo(\n\t\tz.Message,\n\t\tbase.TaskStateCompleted,\n\t\ttime.Time{}, // zero value for n/a\n\t\tnil,         // TODO: Test with result data\n\t)\n}\n\nfunc TestInspectorListCompletedTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tnow := time.Now()\n\tm1 := newCompletedTaskMessage(\"task1\", \"default\", 1*time.Hour, now.Add(-3*time.Minute))     // Expires in 57 mins\n\tm2 := newCompletedTaskMessage(\"task2\", \"default\", 30*time.Minute, now.Add(-10*time.Minute)) // Expires in 20 mins\n\tm3 := newCompletedTaskMessage(\"task3\", \"default\", 2*time.Hour, now.Add(-30*time.Minute))    // Expires in 90 mins\n\tm4 := newCompletedTaskMessage(\"task4\", \"custom\", 15*time.Minute, now.Add(-2*time.Minute))   // Expires in 13 mins\n\tz1 := base.Z{Message: m1, Score: m1.CompletedAt + m1.Retention}\n\tz2 := base.Z{Message: m2, Score: m2.CompletedAt + m2.Retention}\n\tz3 := base.Z{Message: m3, Score: m3.CompletedAt + m3.Retention}\n\tz4 := base.Z{Message: m4, Score: m4.CompletedAt + m4.Retention}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tdesc      string\n\t\tcompleted map[string][]base.Z\n\t\tqname     string\n\t\twant      []*TaskInfo\n\t}{\n\t\t{\n\t\t\tdesc: \"with a few completed tasks\",\n\t\t\tcompleted: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2, z3},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\t// Should be sorted by expiration time (CompletedAt + Retention).\n\t\t\twant: []*TaskInfo{\n\t\t\t\tcreateCompletedTask(z2),\n\t\t\t\tcreateCompletedTask(z1),\n\t\t\t\tcreateCompletedTask(z3),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"with empty completed queue\",\n\t\t\tcompleted: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  []*TaskInfo(nil),\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllCompletedQueues(t, r, tc.completed)\n\n\t\tgot, err := inspector.ListCompletedTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%s; ListCompletedTasks(%q) returned error: %v\", tc.desc, tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tif diff := cmp.Diff(tc.want, got, cmp.AllowUnexported(TaskInfo{})); diff != \"\" {\n\t\t\tt.Errorf(\"%s; ListCompletedTasks(%q) = %v, want %v; (-want,+got)\\n%s\",\n\t\t\t\ttc.desc, tc.qname, got, tc.want, diff)\n\t\t}\n\t}\n}\n\nfunc TestInspectorListAggregatingTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tnow := time.Now()\n\n\tm1 := h.NewTaskMessageBuilder().SetType(\"task1\").SetQueue(\"default\").SetGroup(\"group1\").Build()\n\tm2 := h.NewTaskMessageBuilder().SetType(\"task2\").SetQueue(\"default\").SetGroup(\"group1\").Build()\n\tm3 := h.NewTaskMessageBuilder().SetType(\"task3\").SetQueue(\"default\").SetGroup(\"group1\").Build()\n\tm4 := h.NewTaskMessageBuilder().SetType(\"task4\").SetQueue(\"default\").SetGroup(\"group2\").Build()\n\tm5 := h.NewTaskMessageBuilder().SetType(\"task5\").SetQueue(\"custom\").SetGroup(\"group1\").Build()\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\tfxt := struct {\n\t\ttasks     []*h.TaskSeedData\n\t\tallQueues []string\n\t\tallGroups map[string][]string\n\t\tgroups    map[string][]redis.Z\n\t}{\n\t\ttasks: []*h.TaskSeedData{\n\t\t\t{Msg: m1, State: base.TaskStateAggregating},\n\t\t\t{Msg: m2, State: base.TaskStateAggregating},\n\t\t\t{Msg: m3, State: base.TaskStateAggregating},\n\t\t\t{Msg: m4, State: base.TaskStateAggregating},\n\t\t\t{Msg: m5, State: base.TaskStateAggregating},\n\t\t},\n\t\tallQueues: []string{\"default\", \"custom\"},\n\t\tallGroups: map[string][]string{\n\t\t\tbase.AllGroups(\"default\"): {\"group1\", \"group2\"},\n\t\t\tbase.AllGroups(\"custom\"):  {\"group1\"},\n\t\t},\n\t\tgroups: map[string][]redis.Z{\n\t\t\tbase.GroupKey(\"default\", \"group1\"): {\n\t\t\t\t{Member: m1.ID, Score: float64(now.Add(-30 * time.Second).Unix())},\n\t\t\t\t{Member: m2.ID, Score: float64(now.Add(-20 * time.Second).Unix())},\n\t\t\t\t{Member: m3.ID, Score: float64(now.Add(-10 * time.Second).Unix())},\n\t\t\t},\n\t\t\tbase.GroupKey(\"default\", \"group2\"): {\n\t\t\t\t{Member: m4.ID, Score: float64(now.Add(-30 * time.Second).Unix())},\n\t\t\t},\n\t\t\tbase.GroupKey(\"custom\", \"group1\"): {\n\t\t\t\t{Member: m5.ID, Score: float64(now.Add(-30 * time.Second).Unix())},\n\t\t\t},\n\t\t},\n\t}\n\n\ttests := []struct {\n\t\tdesc  string\n\t\tqname string\n\t\tgname string\n\t\twant  []*TaskInfo\n\t}{\n\t\t{\n\t\t\tdesc:  \"default queue group1\",\n\t\t\tqname: \"default\",\n\t\t\tgname: \"group1\",\n\t\t\twant: []*TaskInfo{\n\t\t\t\tcreateAggregatingTaskInfo(m1),\n\t\t\t\tcreateAggregatingTaskInfo(m2),\n\t\t\t\tcreateAggregatingTaskInfo(m3),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc:  \"custom queue group1\",\n\t\t\tqname: \"custom\",\n\t\t\tgname: \"group1\",\n\t\t\twant: []*TaskInfo{\n\t\t\t\tcreateAggregatingTaskInfo(m5),\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedTasks(t, r, fxt.tasks)\n\t\th.SeedRedisSet(t, r, base.AllQueues, fxt.allQueues)\n\t\th.SeedRedisSets(t, r, fxt.allGroups)\n\t\th.SeedRedisZSets(t, r, fxt.groups)\n\n\t\tt.Run(tc.desc, func(t *testing.T) {\n\t\t\tgot, err := inspector.ListAggregatingTasks(tc.qname, tc.gname)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"ListAggregatingTasks returned error: %v\", err)\n\t\t\t}\n\n\t\t\tcmpOpts := []cmp.Option{\n\t\t\t\tcmpopts.EquateApproxTime(2 * time.Second),\n\t\t\t\tcmp.AllowUnexported(TaskInfo{}),\n\t\t\t}\n\t\t\tif diff := cmp.Diff(tc.want, got, cmpOpts...); diff != \"\" {\n\t\t\t\tt.Errorf(\"ListAggregatingTasks = %v, want = %v; (-want,+got)\\n%s\", got, tc.want, diff)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc createAggregatingTaskInfo(msg *base.TaskMessage) *TaskInfo {\n\treturn newTaskInfo(msg, base.TaskStateAggregating, time.Time{}, nil)\n}\n\nfunc TestInspectorListPagination(t *testing.T) {\n\t// Create 100 tasks.\n\tvar msgs []*base.TaskMessage\n\tfor i := 0; i <= 99; i++ {\n\t\tmsgs = append(msgs,\n\t\t\th.NewTaskMessage(fmt.Sprintf(\"task%d\", i), nil))\n\t}\n\tr := setup(t)\n\tdefer r.Close()\n\th.SeedPendingQueue(t, r, msgs, base.DefaultQueueName)\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tpage     int\n\t\tpageSize int\n\t\twant     []*TaskInfo\n\t}{\n\t\t{\n\t\t\tpage:     1,\n\t\t\tpageSize: 5,\n\t\t\twant: []*TaskInfo{\n\t\t\t\tcreatePendingTask(msgs[0]),\n\t\t\t\tcreatePendingTask(msgs[1]),\n\t\t\t\tcreatePendingTask(msgs[2]),\n\t\t\t\tcreatePendingTask(msgs[3]),\n\t\t\t\tcreatePendingTask(msgs[4]),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tpage:     3,\n\t\t\tpageSize: 10,\n\t\t\twant: []*TaskInfo{\n\t\t\t\tcreatePendingTask(msgs[20]),\n\t\t\t\tcreatePendingTask(msgs[21]),\n\t\t\t\tcreatePendingTask(msgs[22]),\n\t\t\t\tcreatePendingTask(msgs[23]),\n\t\t\t\tcreatePendingTask(msgs[24]),\n\t\t\t\tcreatePendingTask(msgs[25]),\n\t\t\t\tcreatePendingTask(msgs[26]),\n\t\t\t\tcreatePendingTask(msgs[27]),\n\t\t\t\tcreatePendingTask(msgs[28]),\n\t\t\t\tcreatePendingTask(msgs[29]),\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\tgot, err := inspector.ListPendingTasks(\"default\", Page(tc.page), PageSize(tc.pageSize))\n\t\tif err != nil {\n\t\t\tt.Errorf(\"ListPendingTask('default') returned error: %v\", err)\n\t\t\tcontinue\n\t\t}\n\t\tcmpOpts := []cmp.Option{\n\t\t\tcmpopts.EquateApproxTime(2 * time.Second),\n\t\t\tcmp.AllowUnexported(TaskInfo{}),\n\t\t}\n\t\tif diff := cmp.Diff(tc.want, got, cmpOpts...); diff != \"\" {\n\t\t\tt.Errorf(\"ListPendingTask('default') = %v, want %v; (-want,+got)\\n%s\",\n\t\t\t\tgot, tc.want, diff)\n\t\t}\n\t}\n}\n\nfunc TestInspectorListTasksQueueNotFoundError(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tqname   string\n\t\twantErr error\n\t}{\n\t\t{\n\t\t\tqname:   \"nonexistent\",\n\t\t\twantErr: ErrQueueNotFound,\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\n\t\tif _, err := inspector.ListActiveTasks(tc.qname); !errors.Is(err, tc.wantErr) {\n\t\t\tt.Errorf(\"ListActiveTasks(%q) returned error %v, want %v\", tc.qname, err, tc.wantErr)\n\t\t}\n\t\tif _, err := inspector.ListPendingTasks(tc.qname); !errors.Is(err, tc.wantErr) {\n\t\t\tt.Errorf(\"ListPendingTasks(%q) returned error %v, want %v\", tc.qname, err, tc.wantErr)\n\t\t}\n\t\tif _, err := inspector.ListScheduledTasks(tc.qname); !errors.Is(err, tc.wantErr) {\n\t\t\tt.Errorf(\"ListScheduledTasks(%q) returned error %v, want %v\", tc.qname, err, tc.wantErr)\n\t\t}\n\t\tif _, err := inspector.ListRetryTasks(tc.qname); !errors.Is(err, tc.wantErr) {\n\t\t\tt.Errorf(\"ListRetryTasks(%q) returned error %v, want %v\", tc.qname, err, tc.wantErr)\n\t\t}\n\t\tif _, err := inspector.ListArchivedTasks(tc.qname); !errors.Is(err, tc.wantErr) {\n\t\t\tt.Errorf(\"ListArchivedTasks(%q) returned error %v, want %v\", tc.qname, err, tc.wantErr)\n\t\t}\n\t\tif _, err := inspector.ListCompletedTasks(tc.qname); !errors.Is(err, tc.wantErr) {\n\t\t\tt.Errorf(\"ListCompletedTasks(%q) returned error %v, want %v\", tc.qname, err, tc.wantErr)\n\t\t}\n\t\tif _, err := inspector.ListAggregatingTasks(tc.qname, \"mygroup\"); !errors.Is(err, tc.wantErr) {\n\t\t\tt.Errorf(\"ListAggregatingTasks(%q, \\\"mygroup\\\") returned error %v, want %v\", tc.qname, err, tc.wantErr)\n\t\t}\n\t}\n}\n\nfunc TestInspectorDeleteAllPendingTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessage(\"task3\", nil)\n\tm4 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tpending     map[string][]*base.TaskMessage\n\t\tqname       string\n\t\twant        int\n\t\twantPending map[string][]*base.TaskMessage\n\t}{\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m2, m3},\n\t\t\t\t\"custom\":  {m4},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  3,\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {m4},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m2, m3},\n\t\t\t\t\"custom\":  {m4},\n\t\t\t},\n\t\t\tqname: \"custom\",\n\t\t\twant:  1,\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m2, m3},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\n\t\tgot, err := inspector.DeleteAllPendingTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"DeleteAllPendingTasks(%q) returned error: %v\", tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tif got != tc.want {\n\t\t\tt.Errorf(\"DeleteAllPendingTasks(%q) = %d, want %d\", tc.qname, got, tc.want)\n\t\t}\n\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgotPending := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotPending, h.SortMsgOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected pending tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorDeleteAllScheduledTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessage(\"task3\", nil)\n\tm4 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\tz4 := base.Z{Message: m4, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tscheduled     map[string][]base.Z\n\t\tqname         string\n\t\twant          int\n\t\twantScheduled map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2, z3},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  3,\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  0,\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllScheduledQueues(t, r, tc.scheduled)\n\n\t\tgot, err := inspector.DeleteAllScheduledTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"DeleteAllScheduledTasks(%q) returned error: %v\", tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tif got != tc.want {\n\t\t\tt.Errorf(\"DeleteAllScheduledTasks(%q) = %d, want %d\", tc.qname, got, tc.want)\n\t\t}\n\t\tfor qname, want := range tc.wantScheduled {\n\t\t\tgotScheduled := h.GetScheduledEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotScheduled, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected scheduled tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorDeleteAllRetryTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessage(\"task3\", nil)\n\tm4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\tz4 := base.Z{Message: m4, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tretry     map[string][]base.Z\n\t\tqname     string\n\t\twant      int\n\t\twantRetry map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2, z3},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  3,\n\t\t\twantRetry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  0,\n\t\t\twantRetry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllRetryQueues(t, r, tc.retry)\n\n\t\tgot, err := inspector.DeleteAllRetryTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"DeleteAllRetryTasks(%q) returned error: %v\", tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tif got != tc.want {\n\t\t\tt.Errorf(\"DeleteAllRetryTasks(%q) = %d, want %d\", tc.qname, got, tc.want)\n\t\t}\n\t\tfor qname, want := range tc.wantRetry {\n\t\t\tgotRetry := h.GetRetryEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotRetry, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected retry tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorDeleteAllArchivedTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessage(\"task3\", nil)\n\tm4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\tz4 := base.Z{Message: m4, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tarchived     map[string][]base.Z\n\t\tqname        string\n\t\twant         int\n\t\twantArchived map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2, z3},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  3,\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  0,\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\n\t\tgot, err := inspector.DeleteAllArchivedTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"DeleteAllArchivedTasks(%q) returned error: %v\", tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tif got != tc.want {\n\t\t\tt.Errorf(\"DeleteAllArchivedTasks(%q) = %d, want %d\", tc.qname, got, tc.want)\n\t\t}\n\t\tfor qname, want := range tc.wantArchived {\n\t\t\tgotArchived := h.GetArchivedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotArchived, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected archived tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorDeleteAllCompletedTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tnow := time.Now()\n\tm1 := newCompletedTaskMessage(\"task1\", \"default\", 30*time.Minute, now.Add(-2*time.Minute))\n\tm2 := newCompletedTaskMessage(\"task2\", \"default\", 30*time.Minute, now.Add(-5*time.Minute))\n\tm3 := newCompletedTaskMessage(\"task3\", \"default\", 30*time.Minute, now.Add(-10*time.Minute))\n\tm4 := newCompletedTaskMessage(\"task4\", \"custom\", 30*time.Minute, now.Add(-3*time.Minute))\n\tz1 := base.Z{Message: m1, Score: m1.CompletedAt + m1.Retention}\n\tz2 := base.Z{Message: m2, Score: m2.CompletedAt + m2.Retention}\n\tz3 := base.Z{Message: m3, Score: m3.CompletedAt + m3.Retention}\n\tz4 := base.Z{Message: m4, Score: m4.CompletedAt + m4.Retention}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tcompleted     map[string][]base.Z\n\t\tqname         string\n\t\twant          int\n\t\twantCompleted map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tcompleted: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2, z3},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  3,\n\t\t\twantCompleted: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tcompleted: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  0,\n\t\t\twantCompleted: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllCompletedQueues(t, r, tc.completed)\n\n\t\tgot, err := inspector.DeleteAllCompletedTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"DeleteAllCompletedTasks(%q) returned error: %v\", tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tif got != tc.want {\n\t\t\tt.Errorf(\"DeleteAllCompletedTasks(%q) = %d, want %d\", tc.qname, got, tc.want)\n\t\t}\n\t\tfor qname, want := range tc.wantCompleted {\n\t\t\tgotCompleted := h.GetCompletedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotCompleted, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected completed tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorArchiveAllPendingTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessage(\"task3\", nil)\n\tm4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tinspector := NewInspector(getRedisConnOpt(t))\n\tinspector.rdb.SetClock(timeutil.NewSimulatedClock(now))\n\n\ttests := []struct {\n\t\tpending      map[string][]*base.TaskMessage\n\t\tarchived     map[string][]base.Z\n\t\tqname        string\n\t\twant         int\n\t\twantPending  map[string][]*base.TaskMessage\n\t\twantArchived map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m2, m3},\n\t\t\t\t\"custom\":  {m4},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  3,\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {m4},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\tbase.Z{Message: m1, Score: now.Unix()},\n\t\t\t\t\tbase.Z{Message: m2, Score: now.Unix()},\n\t\t\t\t\tbase.Z{Message: m3, Score: now.Unix()},\n\t\t\t\t},\n\t\t\t\t\"custom\": {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  0,\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m3},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  1,\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\tz1,\n\t\t\t\t\tz2,\n\t\t\t\t\tbase.Z{Message: m3, Score: now.Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\n\t\tgot, err := inspector.ArchiveAllPendingTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"ArchiveAllPendingTasks(%q) returned error: %v\", tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tif got != tc.want {\n\t\t\tt.Errorf(\"ArchiveAllPendingTasks(%q) = %d, want %d\", tc.qname, got, tc.want)\n\t\t}\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgotPending := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotPending, h.SortMsgOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected pending tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t\tfor qname, want := range tc.wantArchived {\n\t\t\tgotArchived := h.GetArchivedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotArchived, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected archived tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorArchiveAllScheduledTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessage(\"task3\", nil)\n\tm4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\tz4 := base.Z{Message: m4, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\tinspector.rdb.SetClock(timeutil.NewSimulatedClock(now))\n\n\ttests := []struct {\n\t\tscheduled     map[string][]base.Z\n\t\tarchived      map[string][]base.Z\n\t\tqname         string\n\t\twant          int\n\t\twantScheduled map[string][]base.Z\n\t\twantArchived  map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2, z3},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  3,\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\tbase.Z{Message: m1, Score: now.Unix()},\n\t\t\t\t\tbase.Z{Message: m2, Score: now.Unix()},\n\t\t\t\t\tbase.Z{Message: m3, Score: now.Unix()},\n\t\t\t\t},\n\t\t\t\t\"custom\": {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {z3},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  2,\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\tz3,\n\t\t\t\t\tbase.Z{Message: m1, Score: now.Unix()},\n\t\t\t\t\tbase.Z{Message: m2, Score: now.Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  0,\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  0,\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllScheduledQueues(t, r, tc.scheduled)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\n\t\tgot, err := inspector.ArchiveAllScheduledTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"ArchiveAllScheduledTasks(%q) returned error: %v\", tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tif got != tc.want {\n\t\t\tt.Errorf(\"ArchiveAllScheduledTasks(%q) = %d, want %d\", tc.qname, got, tc.want)\n\t\t}\n\t\tfor qname, want := range tc.wantScheduled {\n\t\t\tgotScheduled := h.GetScheduledEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotScheduled, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected scheduled tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t\tfor qname, want := range tc.wantArchived {\n\t\t\tgotArchived := h.GetArchivedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotArchived, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected archived tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorArchiveAllRetryTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessage(\"task3\", nil)\n\tm4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\tz4 := base.Z{Message: m4, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\tinspector.rdb.SetClock(timeutil.NewSimulatedClock(now))\n\n\ttests := []struct {\n\t\tretry        map[string][]base.Z\n\t\tarchived     map[string][]base.Z\n\t\tqname        string\n\t\twant         int\n\t\twantRetry    map[string][]base.Z\n\t\twantArchived map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2, z3},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  3,\n\t\t\twantRetry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {z4},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\tbase.Z{Message: m1, Score: now.Unix()},\n\t\t\t\t\tbase.Z{Message: m2, Score: now.Unix()},\n\t\t\t\t\tbase.Z{Message: m3, Score: now.Unix()},\n\t\t\t\t},\n\t\t\t\t\"custom\": {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {z3},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  2,\n\t\t\twantRetry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\tz3,\n\t\t\t\t\tbase.Z{Message: m1, Score: now.Unix()},\n\t\t\t\t\tbase.Z{Message: m2, Score: now.Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  0,\n\t\t\twantRetry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllRetryQueues(t, r, tc.retry)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\n\t\tgot, err := inspector.ArchiveAllRetryTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"ArchiveAllRetryTasks(%q) returned error: %v\", tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tif got != tc.want {\n\t\t\tt.Errorf(\"ArchiveAllRetryTasks(%q) = %d, want %d\", tc.qname, got, tc.want)\n\t\t}\n\t\tfor qname, want := range tc.wantRetry {\n\t\t\tgotRetry := h.GetRetryEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotRetry, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected retry tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t\tfor qname, want := range tc.wantArchived {\n\t\t\twantArchived := h.GetArchivedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, wantArchived, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected archived tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorRunAllScheduledTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessageWithQueue(\"task2\", nil, \"critical\")\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"low\")\n\tm4 := h.NewTaskMessage(\"task4\", nil)\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\tz4 := base.Z{Message: m4, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tscheduled     map[string][]base.Z\n\t\tpending       map[string][]*base.TaskMessage\n\t\tqname         string\n\t\twant          int\n\t\twantScheduled map[string][]base.Z\n\t\twantPending   map[string][]*base.TaskMessage\n\t}{\n\t\t{\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\":  {z1, z4},\n\t\t\t\t\"critical\": {z2},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  2,\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {z2},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {m1, m4},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\":  {z1},\n\t\t\t\t\"critical\": {z2},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {m4},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  1,\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {z2},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {m4, m1},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m4},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  0,\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m4},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllScheduledQueues(t, r, tc.scheduled)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\n\t\tgot, err := inspector.RunAllScheduledTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"RunAllScheduledTasks(%q) returned error: %v\", tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tif got != tc.want {\n\t\t\tt.Errorf(\"RunAllScheduledTasks(%q) = %d, want %d\", tc.qname, got, tc.want)\n\t\t}\n\t\tfor qname, want := range tc.wantScheduled {\n\t\t\tgotScheduled := h.GetScheduledEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotScheduled, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected scheduled tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgotPending := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotPending, h.SortMsgOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected pending tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorRunAllRetryTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessageWithQueue(\"task2\", nil, \"critical\")\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"low\")\n\tm4 := h.NewTaskMessage(\"task2\", nil)\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\tz4 := base.Z{Message: m4, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tretry       map[string][]base.Z\n\t\tpending     map[string][]*base.TaskMessage\n\t\tqname       string\n\t\twant        int\n\t\twantRetry   map[string][]base.Z\n\t\twantPending map[string][]*base.TaskMessage\n\t}{\n\t\t{\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\":  {z1, z4},\n\t\t\t\t\"critical\": {z2},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  2,\n\t\t\twantRetry: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {z2},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {m1, m4},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\":  {z1},\n\t\t\t\t\"critical\": {z2},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {m4},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  1,\n\t\t\twantRetry: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {z2},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {m4, m1},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m4},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  0,\n\t\t\twantRetry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m4},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllRetryQueues(t, r, tc.retry)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\n\t\tgot, err := inspector.RunAllRetryTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"RunAllRetryTasks(%q) returned error: %v\", tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tif got != tc.want {\n\t\t\tt.Errorf(\"RunAllRetryTasks(%q) = %d, want %d\", tc.qname, got, tc.want)\n\t\t}\n\t\tfor qname, want := range tc.wantRetry {\n\t\t\tgotRetry := h.GetRetryEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotRetry, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected retry tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgotPending := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotPending, h.SortMsgOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected pending tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorRunAllArchivedTasks(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessageWithQueue(\"task2\", nil, \"critical\")\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"low\")\n\tm4 := h.NewTaskMessage(\"task2\", nil)\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(-5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(-15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(-2 * time.Minute).Unix()}\n\tz4 := base.Z{Message: m4, Score: now.Add(-2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tarchived     map[string][]base.Z\n\t\tpending      map[string][]*base.TaskMessage\n\t\tqname        string\n\t\twant         int\n\t\twantArchived map[string][]base.Z\n\t\twantPending  map[string][]*base.TaskMessage\n\t}{\n\t\t{\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\":  {z1, z4},\n\t\t\t\t\"critical\": {z2},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  2,\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {z2},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {m1, m4},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\":  {z1},\n\t\t\t\t\"critical\": {z2},\n\t\t\t},\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {m4},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  1,\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {z2},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {m4, m1},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m4},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\twant:  0,\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m4},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\n\t\tgot, err := inspector.RunAllArchivedTasks(tc.qname)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"RunAllArchivedTasks(%q) returned error: %v\", tc.qname, err)\n\t\t\tcontinue\n\t\t}\n\t\tif got != tc.want {\n\t\t\tt.Errorf(\"RunAllArchivedTasks(%q) = %d, want %d\", tc.qname, got, tc.want)\n\t\t}\n\t\tfor qname, want := range tc.wantArchived {\n\t\t\twantArchived := h.GetArchivedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, wantArchived, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected archived tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\n\t\t}\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgotPending := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotPending, h.SortMsgOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected pending tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorDeleteTaskDeletesPendingTask(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tpending     map[string][]*base.TaskMessage\n\t\tqname       string\n\t\tid          string\n\t\twantPending map[string][]*base.TaskMessage\n\t}{\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m2},\n\t\t\t\t\"custom\":  {m3},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\tid:    createPendingTask(m2).ID,\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1},\n\t\t\t\t\"custom\":  {m3},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m2},\n\t\t\t\t\"custom\":  {m3},\n\t\t\t},\n\t\t\tqname: \"custom\",\n\t\t\tid:    createPendingTask(m3).ID,\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m2},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\n\t\tif err := inspector.DeleteTask(tc.qname, tc.id); err != nil {\n\t\t\tt.Errorf(\"DeleteTask(%q, %q) returned error: %v\", tc.qname, tc.id, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgot := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, got, h.SortMsgOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unspected pending tasks in queue %q: (-want,+got):\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorDeleteTaskDeletesScheduledTask(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tscheduled     map[string][]base.Z\n\t\tqname         string\n\t\tid            string\n\t\twantScheduled map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2},\n\t\t\t\t\"custom\":  {z3},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\tid:    createScheduledTask(z2).ID,\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {z1},\n\t\t\t\t\"custom\":  {z3},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllScheduledQueues(t, r, tc.scheduled)\n\n\t\tif err := inspector.DeleteTask(tc.qname, tc.id); err != nil {\n\t\t\tt.Errorf(\"DeleteTask(%q, %q) returned error: %v\", tc.qname, tc.id, err)\n\t\t}\n\t\tfor qname, want := range tc.wantScheduled {\n\t\t\tgotScheduled := h.GetScheduledEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotScheduled, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected scheduled tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\n\t\t}\n\t}\n}\n\nfunc TestInspectorDeleteTaskDeletesRetryTask(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tretry     map[string][]base.Z\n\t\tqname     string\n\t\tid        string\n\t\twantRetry map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2},\n\t\t\t\t\"custom\":  {z3},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\tid:    createRetryTask(z2).ID,\n\t\t\twantRetry: map[string][]base.Z{\n\t\t\t\t\"default\": {z1},\n\t\t\t\t\"custom\":  {z3},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllRetryQueues(t, r, tc.retry)\n\n\t\tif err := inspector.DeleteTask(tc.qname, tc.id); err != nil {\n\t\t\tt.Errorf(\"DeleteTask(%q, %q) returned error: %v\", tc.qname, tc.id, err)\n\t\t\tcontinue\n\t\t}\n\t\tfor qname, want := range tc.wantRetry {\n\t\t\tgotRetry := h.GetRetryEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotRetry, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected retry tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorDeleteTaskDeletesArchivedTask(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(-5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(-15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(-2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tarchived     map[string][]base.Z\n\t\tqname        string\n\t\tid           string\n\t\twantArchived map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2},\n\t\t\t\t\"custom\":  {z3},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\tid:    createArchivedTask(z2).ID,\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {z1},\n\t\t\t\t\"custom\":  {z3},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\n\t\tif err := inspector.DeleteTask(tc.qname, tc.id); err != nil {\n\t\t\tt.Errorf(\"DeleteTask(%q, %q) returned error: %v\", tc.qname, tc.id, err)\n\t\t\tcontinue\n\t\t}\n\t\tfor qname, want := range tc.wantArchived {\n\t\t\twantArchived := h.GetArchivedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, wantArchived, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected archived tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorDeleteTaskError(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(-5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(-15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(-2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tarchived     map[string][]base.Z\n\t\tqname        string\n\t\tid           string\n\t\twantErr      error\n\t\twantArchived map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2},\n\t\t\t\t\"custom\":  {z3},\n\t\t\t},\n\t\t\tqname:   \"nonexistent\",\n\t\t\tid:      createArchivedTask(z2).ID,\n\t\t\twantErr: ErrQueueNotFound,\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2},\n\t\t\t\t\"custom\":  {z3},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2},\n\t\t\t\t\"custom\":  {z3},\n\t\t\t},\n\t\t\tqname:   \"default\",\n\t\t\tid:      uuid.NewString(),\n\t\t\twantErr: ErrTaskNotFound,\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2},\n\t\t\t\t\"custom\":  {z3},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\n\t\tif err := inspector.DeleteTask(tc.qname, tc.id); !errors.Is(err, tc.wantErr) {\n\t\t\tt.Errorf(\"DeleteTask(%q, %q) = %v, want %v\", tc.qname, tc.id, err, tc.wantErr)\n\t\t\tcontinue\n\t\t}\n\t\tfor qname, want := range tc.wantArchived {\n\t\t\twantArchived := h.GetArchivedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, wantArchived, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected archived tasks in queue %q: (-want, +got)\\n%s\", qname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorRunTaskRunsScheduledTask(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tscheduled     map[string][]base.Z\n\t\tpending       map[string][]*base.TaskMessage\n\t\tqname         string\n\t\tid            string\n\t\twantScheduled map[string][]base.Z\n\t\twantPending   map[string][]*base.TaskMessage\n\t}{\n\t\t{\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {z1, z2},\n\t\t\t\t\"custom\":  {z3},\n\t\t\t},\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\tid:    createScheduledTask(z2).ID,\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {z1},\n\t\t\t\t\"custom\":  {z3},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m2},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllScheduledQueues(t, r, tc.scheduled)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\n\t\tif err := inspector.RunTask(tc.qname, tc.id); err != nil {\n\t\t\tt.Errorf(\"RunTask(%q, %q) returned error: %v\", tc.qname, tc.id, err)\n\t\t\tcontinue\n\t\t}\n\t\tfor qname, want := range tc.wantScheduled {\n\t\t\tgotScheduled := h.GetScheduledEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotScheduled, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected scheduled tasks in queue %q: (-want, +got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\n\t\t}\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgotPending := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotPending, h.SortMsgOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected pending tasks in queue %q: (-want, +got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorRunTaskRunsRetryTask(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessageWithQueue(\"task2\", nil, \"custom\")\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tretry       map[string][]base.Z\n\t\tpending     map[string][]*base.TaskMessage\n\t\tqname       string\n\t\tid          string\n\t\twantRetry   map[string][]base.Z\n\t\twantPending map[string][]*base.TaskMessage\n\t}{\n\t\t{\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {z1},\n\t\t\t\t\"custom\":  {z2, z3},\n\t\t\t},\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tqname: \"custom\",\n\t\t\tid:    createRetryTask(z2).ID,\n\t\t\twantRetry: map[string][]base.Z{\n\t\t\t\t\"default\": {z1},\n\t\t\t\t\"custom\":  {z3},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {m2},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllRetryQueues(t, r, tc.retry)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\n\t\tif err := inspector.RunTask(tc.qname, tc.id); err != nil {\n\t\t\tt.Errorf(\"RunTaskBy(%q, %q) returned error: %v\", tc.qname, tc.id, err)\n\t\t\tcontinue\n\t\t}\n\t\tfor qname, want := range tc.wantRetry {\n\t\t\tgotRetry := h.GetRetryEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotRetry, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected retry tasks in queue %q: (-want, +got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\t\t}\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgotPending := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotPending, h.SortMsgOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected pending tasks in queue %q: (-want, +got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorRunTaskRunsArchivedTask(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessageWithQueue(\"task2\", nil, \"critical\")\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"low\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(-5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(-15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(-2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tarchived     map[string][]base.Z\n\t\tpending      map[string][]*base.TaskMessage\n\t\tqname        string\n\t\tid           string\n\t\twantArchived map[string][]base.Z\n\t\twantPending  map[string][]*base.TaskMessage\n\t}{\n\t\t{\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\":  {z1},\n\t\t\t\t\"critical\": {z2},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t\tqname: \"critical\",\n\t\t\tid:    createArchivedTask(z2).ID,\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\":  {z1},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {m2},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\n\t\tif err := inspector.RunTask(tc.qname, tc.id); err != nil {\n\t\t\tt.Errorf(\"RunTask(%q, %q) returned error: %v\", tc.qname, tc.id, err)\n\t\t\tcontinue\n\t\t}\n\t\tfor qname, want := range tc.wantArchived {\n\t\t\twantArchived := h.GetArchivedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, wantArchived, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected archived tasks in queue %q: (-want, +got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\t\t}\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgotPending := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotPending, h.SortMsgOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected pending tasks in queue %q: (-want, +got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorRunTaskError(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessageWithQueue(\"task2\", nil, \"critical\")\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"low\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(-5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(-15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(-2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\ttests := []struct {\n\t\tarchived     map[string][]base.Z\n\t\tpending      map[string][]*base.TaskMessage\n\t\tqname        string\n\t\tid           string\n\t\twantErr      error\n\t\twantArchived map[string][]base.Z\n\t\twantPending  map[string][]*base.TaskMessage\n\t}{\n\t\t{\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\":  {z1},\n\t\t\t\t\"critical\": {z2},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t\tqname:   \"nonexistent\",\n\t\t\tid:      createArchivedTask(z2).ID,\n\t\t\twantErr: ErrQueueNotFound,\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\":  {z1},\n\t\t\t\t\"critical\": {z2},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\":  {z1},\n\t\t\t\t\"critical\": {z2},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t\tqname:   \"default\",\n\t\t\tid:      uuid.NewString(),\n\t\t\twantErr: ErrTaskNotFound,\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\":  {z1},\n\t\t\t\t\"critical\": {z2},\n\t\t\t\t\"low\":      {z3},\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t\t\"low\":      {},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\n\t\tif err := inspector.RunTask(tc.qname, tc.id); !errors.Is(err, tc.wantErr) {\n\t\t\tt.Errorf(\"RunTask(%q, %q) = %v, want %v\", tc.qname, tc.id, err, tc.wantErr)\n\t\t\tcontinue\n\t\t}\n\t\tfor qname, want := range tc.wantArchived {\n\t\t\twantArchived := h.GetArchivedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, wantArchived, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected archived tasks in queue %q: (-want, +got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\t\t}\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgotPending := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotPending, h.SortMsgOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected pending tasks in queue %q: (-want, +got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorArchiveTaskArchivesPendingTask(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessageWithQueue(\"task2\", nil, \"custom\")\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tnow := time.Now()\n\tinspector := NewInspector(getRedisConnOpt(t))\n\tinspector.rdb.SetClock(timeutil.NewSimulatedClock(now))\n\n\ttests := []struct {\n\t\tpending      map[string][]*base.TaskMessage\n\t\tarchived     map[string][]base.Z\n\t\tqname        string\n\t\tid           string\n\t\twantPending  map[string][]*base.TaskMessage\n\t\twantArchived map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1},\n\t\t\t\t\"custom\":  {m2, m3},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tqname: \"default\",\n\t\t\tid:    createPendingTask(m1).ID,\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {m2, m3},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{Message: m1, Score: now.Unix()},\n\t\t\t\t},\n\t\t\t\t\"custom\": {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1},\n\t\t\t\t\"custom\":  {m2, m3},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tqname: \"custom\",\n\t\t\tid:    createPendingTask(m2).ID,\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1},\n\t\t\t\t\"custom\":  {m3},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\": {\n\t\t\t\t\t{Message: m2, Score: now.Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\n\t\tif err := inspector.ArchiveTask(tc.qname, tc.id); err != nil {\n\t\t\tt.Errorf(\"ArchiveTask(%q, %q) returned error: %v\", tc.qname, tc.id, err)\n\t\t\tcontinue\n\t\t}\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgotPending := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotPending, h.SortMsgOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected pending tasks in queue %q: (-want,+got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\n\t\t}\n\t\tfor qname, want := range tc.wantArchived {\n\t\t\twantArchived := h.GetArchivedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, wantArchived, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected archived tasks in queue %q: (-want,+got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorArchiveTaskArchivesScheduledTask(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessageWithQueue(\"task2\", nil, \"custom\")\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\tinspector.rdb.SetClock(timeutil.NewSimulatedClock(now))\n\n\ttests := []struct {\n\t\tscheduled     map[string][]base.Z\n\t\tarchived      map[string][]base.Z\n\t\tqname         string\n\t\tid            string\n\t\twant          string\n\t\twantScheduled map[string][]base.Z\n\t\twantArchived  map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tscheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {z1},\n\t\t\t\t\"custom\":  {z2, z3},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tqname: \"custom\",\n\t\t\tid:    createScheduledTask(z2).ID,\n\t\t\twantScheduled: map[string][]base.Z{\n\t\t\t\t\"default\": {z1},\n\t\t\t\t\"custom\":  {z3},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tMessage: m2,\n\t\t\t\t\t\tScore:   now.Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllScheduledQueues(t, r, tc.scheduled)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\n\t\tif err := inspector.ArchiveTask(tc.qname, tc.id); err != nil {\n\t\t\tt.Errorf(\"ArchiveTask(%q, %q) returned error: %v\", tc.qname, tc.id, err)\n\t\t\tcontinue\n\t\t}\n\t\tfor qname, want := range tc.wantScheduled {\n\t\t\tgotScheduled := h.GetScheduledEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotScheduled, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected scheduled tasks in queue %q: (-want, +got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\n\t\t}\n\t\tfor qname, want := range tc.wantArchived {\n\t\t\twantArchived := h.GetArchivedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, wantArchived, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected archived tasks in queue %q: (-want, +got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorArchiveTaskArchivesRetryTask(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessageWithQueue(\"task2\", nil, \"custom\")\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\tinspector.rdb.SetClock(timeutil.NewSimulatedClock(now))\n\n\ttests := []struct {\n\t\tretry        map[string][]base.Z\n\t\tarchived     map[string][]base.Z\n\t\tqname        string\n\t\tid           string\n\t\twantRetry    map[string][]base.Z\n\t\twantArchived map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {z1},\n\t\t\t\t\"custom\":  {z2, z3},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tqname: \"custom\",\n\t\t\tid:    createRetryTask(z2).ID,\n\t\t\twantRetry: map[string][]base.Z{\n\t\t\t\t\"default\": {z1},\n\t\t\t\t\"custom\":  {z3},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\": {\n\t\t\t\t\t{\n\t\t\t\t\t\tMessage: m2,\n\t\t\t\t\t\tScore:   now.Unix(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllRetryQueues(t, r, tc.retry)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\n\t\tif err := inspector.ArchiveTask(tc.qname, tc.id); err != nil {\n\t\t\tt.Errorf(\"ArchiveTask(%q, %q) returned error: %v\", tc.qname, tc.id, err)\n\t\t\tcontinue\n\t\t}\n\t\tfor qname, want := range tc.wantRetry {\n\t\t\tgotRetry := h.GetRetryEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotRetry, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected retry tasks in queue %q: (-want, +got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\t\t}\n\t\tfor qname, want := range tc.wantArchived {\n\t\t\twantArchived := h.GetArchivedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, wantArchived, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected archived tasks in queue %q: (-want, +got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInspectorArchiveTaskError(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessageWithQueue(\"task2\", nil, \"custom\")\n\tm3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"custom\")\n\tnow := time.Now()\n\tz1 := base.Z{Message: m1, Score: now.Add(5 * time.Minute).Unix()}\n\tz2 := base.Z{Message: m2, Score: now.Add(15 * time.Minute).Unix()}\n\tz3 := base.Z{Message: m3, Score: now.Add(2 * time.Minute).Unix()}\n\n\tinspector := NewInspector(getRedisConnOpt(t))\n\tinspector.rdb.SetClock(timeutil.NewSimulatedClock(now))\n\n\ttests := []struct {\n\t\tretry        map[string][]base.Z\n\t\tarchived     map[string][]base.Z\n\t\tqname        string\n\t\tid           string\n\t\twantErr      error\n\t\twantRetry    map[string][]base.Z\n\t\twantArchived map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {z1},\n\t\t\t\t\"custom\":  {z2, z3},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tqname:   \"nonexistent\",\n\t\t\tid:      createRetryTask(z2).ID,\n\t\t\twantErr: ErrQueueNotFound,\n\t\t\twantRetry: map[string][]base.Z{\n\t\t\t\t\"default\": {z1},\n\t\t\t\t\"custom\":  {z2, z3},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {z1},\n\t\t\t\t\"custom\":  {z2, z3},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tqname:   \"custom\",\n\t\t\tid:      uuid.NewString(),\n\t\t\twantErr: ErrTaskNotFound,\n\t\t\twantRetry: map[string][]base.Z{\n\t\t\t\t\"default\": {z1},\n\t\t\t\t\"custom\":  {z2, z3},\n\t\t\t},\n\t\t\twantArchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllRetryQueues(t, r, tc.retry)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\n\t\tif err := inspector.ArchiveTask(tc.qname, tc.id); !errors.Is(err, tc.wantErr) {\n\t\t\tt.Errorf(\"ArchiveTask(%q, %q) = %v, want %v\", tc.qname, tc.id, err, tc.wantErr)\n\t\t\tcontinue\n\t\t}\n\t\tfor qname, want := range tc.wantRetry {\n\t\t\tgotRetry := h.GetRetryEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotRetry, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected retry tasks in queue %q: (-want, +got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\t\t}\n\t\tfor qname, want := range tc.wantArchived {\n\t\t\twantArchived := h.GetArchivedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, wantArchived, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"unexpected archived tasks in queue %q: (-want, +got)\\n%s\",\n\t\t\t\t\tqname, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\nvar sortSchedulerEntry = cmp.Transformer(\"SortSchedulerEntry\", func(in []*SchedulerEntry) []*SchedulerEntry {\n\tout := append([]*SchedulerEntry(nil), in...)\n\tsort.Slice(out, func(i, j int) bool {\n\t\treturn out[i].Spec < out[j].Spec\n\t})\n\treturn out\n})\n\nfunc TestInspectorSchedulerEntries(t *testing.T) {\n\tr := setup(t)\n\trdbClient := rdb.NewRDB(r)\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\tnow := time.Now().UTC()\n\tschedulerID := \"127.0.0.1:9876:abc123\"\n\n\ttests := []struct {\n\t\tdata []*base.SchedulerEntry // data to seed redis\n\t\twant []*SchedulerEntry\n\t}{\n\t\t{\n\t\t\tdata: []*base.SchedulerEntry{\n\t\t\t\t{\n\t\t\t\t\tSpec:    \"* * * * *\",\n\t\t\t\t\tType:    \"foo\",\n\t\t\t\t\tPayload: nil,\n\t\t\t\t\tOpts:    nil,\n\t\t\t\t\tNext:    now.Add(5 * time.Hour),\n\t\t\t\t\tPrev:    now.Add(-2 * time.Hour),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tSpec:    \"@every 20m\",\n\t\t\t\t\tType:    \"bar\",\n\t\t\t\t\tPayload: h.JSON(map[string]interface{}{\"fiz\": \"baz\"}),\n\t\t\t\t\tOpts:    []string{`Queue(\"bar\")`, `MaxRetry(20)`},\n\t\t\t\t\tNext:    now.Add(1 * time.Minute),\n\t\t\t\t\tPrev:    now.Add(-19 * time.Minute),\n\t\t\t\t},\n\t\t\t},\n\t\t\twant: []*SchedulerEntry{\n\t\t\t\t{\n\t\t\t\t\tSpec: \"* * * * *\",\n\t\t\t\t\tTask: NewTask(\"foo\", nil),\n\t\t\t\t\tOpts: nil,\n\t\t\t\t\tNext: now.Add(5 * time.Hour),\n\t\t\t\t\tPrev: now.Add(-2 * time.Hour),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tSpec: \"@every 20m\",\n\t\t\t\t\tTask: NewTask(\"bar\", h.JSON(map[string]interface{}{\"fiz\": \"baz\"})),\n\t\t\t\t\tOpts: []Option{Queue(\"bar\"), MaxRetry(20)},\n\t\t\t\t\tNext: now.Add(1 * time.Minute),\n\t\t\t\t\tPrev: now.Add(-19 * time.Minute),\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\terr := rdbClient.WriteSchedulerEntries(schedulerID, tc.data, time.Minute)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"could not write data: %v\", err)\n\t\t}\n\t\tgot, err := inspector.SchedulerEntries()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"SchedulerEntries() returned error: %v\", err)\n\t\t\tcontinue\n\t\t}\n\t\tignoreOpt := cmpopts.IgnoreUnexported(Task{})\n\t\tif diff := cmp.Diff(tc.want, got, sortSchedulerEntry, ignoreOpt); diff != \"\" {\n\t\t\tt.Errorf(\"SchedulerEntries() = %v, want %v; (-want,+got)\\n%s\",\n\t\t\t\tgot, tc.want, diff)\n\t\t}\n\t}\n}\n\nfunc TestParseOption(t *testing.T) {\n\toneHourFromNow := time.Now().Add(1 * time.Hour)\n\ttests := []struct {\n\t\ts        string\n\t\twantType OptionType\n\t\twantVal  interface{}\n\t}{\n\t\t{`MaxRetry(10)`, MaxRetryOpt, 10},\n\t\t{`Queue(\"email\")`, QueueOpt, \"email\"},\n\t\t{`Timeout(3m)`, TimeoutOpt, 3 * time.Minute},\n\t\t{Deadline(oneHourFromNow).String(), DeadlineOpt, oneHourFromNow},\n\t\t{`Unique(1h)`, UniqueOpt, 1 * time.Hour},\n\t\t{ProcessAt(oneHourFromNow).String(), ProcessAtOpt, oneHourFromNow},\n\t\t{`ProcessIn(10m)`, ProcessInOpt, 10 * time.Minute},\n\t\t{`Retention(24h)`, RetentionOpt, 24 * time.Hour},\n\t}\n\n\tfor _, tc := range tests {\n\t\tt.Run(tc.s, func(t *testing.T) {\n\t\t\tgot, err := parseOption(tc.s)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"returned error: %v\", err)\n\t\t\t}\n\t\t\tif got == nil {\n\t\t\t\tt.Fatal(\"returned nil\")\n\t\t\t}\n\t\t\tif got.Type() != tc.wantType {\n\t\t\t\tt.Fatalf(\"got type %v, want type %v \", got.Type(), tc.wantType)\n\t\t\t}\n\t\t\tswitch tc.wantType {\n\t\t\tcase QueueOpt:\n\t\t\t\tgotVal, ok := got.Value().(string)\n\t\t\t\tif !ok {\n\t\t\t\t\tt.Fatal(\"returned Option with non-string value\")\n\t\t\t\t}\n\t\t\t\tif gotVal != tc.wantVal.(string) {\n\t\t\t\t\tt.Fatalf(\"got value %v, want %v\", gotVal, tc.wantVal)\n\t\t\t\t}\n\t\t\tcase MaxRetryOpt:\n\t\t\t\tgotVal, ok := got.Value().(int)\n\t\t\t\tif !ok {\n\t\t\t\t\tt.Fatal(\"returned Option with non-int value\")\n\t\t\t\t}\n\t\t\t\tif gotVal != tc.wantVal.(int) {\n\t\t\t\t\tt.Fatalf(\"got value %v, want %v\", gotVal, tc.wantVal)\n\t\t\t\t}\n\t\t\tcase TimeoutOpt, UniqueOpt, ProcessInOpt, RetentionOpt:\n\t\t\t\tgotVal, ok := got.Value().(time.Duration)\n\t\t\t\tif !ok {\n\t\t\t\t\tt.Fatal(\"returned Option with non duration value\")\n\t\t\t\t}\n\t\t\t\tif gotVal != tc.wantVal.(time.Duration) {\n\t\t\t\t\tt.Fatalf(\"got value %v, want %v\", gotVal, tc.wantVal)\n\t\t\t\t}\n\t\t\tcase DeadlineOpt, ProcessAtOpt:\n\t\t\t\tgotVal, ok := got.Value().(time.Time)\n\t\t\t\tif !ok {\n\t\t\t\t\tt.Fatal(\"returned Option with non time value\")\n\t\t\t\t}\n\t\t\t\tif cmp.Equal(gotVal, tc.wantVal.(time.Time)) {\n\t\t\t\t\tt.Fatalf(\"got value %v, want %v\", gotVal, tc.wantVal)\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tt.Fatalf(\"returned Option with unexpected type: %v\", got.Type())\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestInspectorGroups(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\tinspector := NewInspector(getRedisConnOpt(t))\n\n\tm1 := h.NewTaskMessageBuilder().SetGroup(\"group1\").Build()\n\tm2 := h.NewTaskMessageBuilder().SetGroup(\"group1\").Build()\n\tm3 := h.NewTaskMessageBuilder().SetGroup(\"group1\").Build()\n\tm4 := h.NewTaskMessageBuilder().SetGroup(\"group2\").Build()\n\tm5 := h.NewTaskMessageBuilder().SetQueue(\"custom\").SetGroup(\"group1\").Build()\n\tm6 := h.NewTaskMessageBuilder().SetQueue(\"custom\").SetGroup(\"group1\").Build()\n\n\tnow := time.Now()\n\n\tfixtures := struct {\n\t\ttasks     []*h.TaskSeedData\n\t\tallGroups map[string][]string\n\t\tgroups    map[string][]redis.Z\n\t}{\n\t\ttasks: []*h.TaskSeedData{\n\t\t\t{Msg: m1, State: base.TaskStateAggregating},\n\t\t\t{Msg: m2, State: base.TaskStateAggregating},\n\t\t\t{Msg: m3, State: base.TaskStateAggregating},\n\t\t\t{Msg: m4, State: base.TaskStateAggregating},\n\t\t\t{Msg: m5, State: base.TaskStateAggregating},\n\t\t},\n\t\tallGroups: map[string][]string{\n\t\t\tbase.AllGroups(\"default\"): {\"group1\", \"group2\"},\n\t\t\tbase.AllGroups(\"custom\"):  {\"group1\"},\n\t\t},\n\t\tgroups: map[string][]redis.Z{\n\t\t\tbase.GroupKey(\"default\", \"group1\"): {\n\t\t\t\t{Member: m1.ID, Score: float64(now.Add(-10 * time.Second).Unix())},\n\t\t\t\t{Member: m2.ID, Score: float64(now.Add(-20 * time.Second).Unix())},\n\t\t\t\t{Member: m3.ID, Score: float64(now.Add(-30 * time.Second).Unix())},\n\t\t\t},\n\t\t\tbase.GroupKey(\"default\", \"group2\"): {\n\t\t\t\t{Member: m4.ID, Score: float64(now.Add(-20 * time.Second).Unix())},\n\t\t\t},\n\t\t\tbase.GroupKey(\"custom\", \"group1\"): {\n\t\t\t\t{Member: m5.ID, Score: float64(now.Add(-10 * time.Second).Unix())},\n\t\t\t\t{Member: m6.ID, Score: float64(now.Add(-20 * time.Second).Unix())},\n\t\t\t},\n\t\t},\n\t}\n\n\ttests := []struct {\n\t\tdesc  string\n\t\tqname string\n\t\twant  []*GroupInfo\n\t}{\n\t\t{\n\t\t\tdesc:  \"default queue groups\",\n\t\t\tqname: \"default\",\n\t\t\twant: []*GroupInfo{\n\t\t\t\t{Group: \"group1\", Size: 3},\n\t\t\t\t{Group: \"group2\", Size: 1},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc:  \"custom queue groups\",\n\t\t\tqname: \"custom\",\n\t\t\twant: []*GroupInfo{\n\t\t\t\t{Group: \"group1\", Size: 2},\n\t\t\t},\n\t\t},\n\t}\n\n\tvar sortGroupInfosOpt = cmp.Transformer(\n\t\t\"SortGroupInfos\",\n\t\tfunc(in []*GroupInfo) []*GroupInfo {\n\t\t\tout := append([]*GroupInfo(nil), in...)\n\t\t\tsort.Slice(out, func(i, j int) bool {\n\t\t\t\treturn out[i].Group < out[j].Group\n\t\t\t})\n\t\t\treturn out\n\t\t})\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedTasks(t, r, fixtures.tasks)\n\t\th.SeedRedisSets(t, r, fixtures.allGroups)\n\t\th.SeedRedisZSets(t, r, fixtures.groups)\n\n\t\tt.Run(tc.desc, func(t *testing.T) {\n\t\t\tgot, err := inspector.Groups(tc.qname)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Groups returned error: %v\", err)\n\t\t\t}\n\t\t\tif diff := cmp.Diff(tc.want, got, sortGroupInfosOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"Groups = %v, want %v; (-want,+got)\\n%s\", got, tc.want, diff)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "janitor.go",
          "type": "blob",
          "size": 1.9697265625,
          "content": "// Copyright 2021 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/log\"\n)\n\n// A janitor is responsible for deleting expired completed tasks from the specified\n// queues. It periodically checks for any expired tasks in the completed set, and\n// deletes them.\ntype janitor struct {\n\tlogger *log.Logger\n\tbroker base.Broker\n\n\t// channel to communicate back to the long running \"janitor\" goroutine.\n\tdone chan struct{}\n\n\t// list of queue names to check.\n\tqueues []string\n\n\t// average interval between checks.\n\tavgInterval time.Duration\n\n\t// number of tasks to be deleted when janitor runs to delete the expired completed tasks.\n\tbatchSize int\n}\n\ntype janitorParams struct {\n\tlogger    *log.Logger\n\tbroker    base.Broker\n\tqueues    []string\n\tinterval  time.Duration\n\tbatchSize int\n}\n\nfunc newJanitor(params janitorParams) *janitor {\n\treturn &janitor{\n\t\tlogger:      params.logger,\n\t\tbroker:      params.broker,\n\t\tdone:        make(chan struct{}),\n\t\tqueues:      params.queues,\n\t\tavgInterval: params.interval,\n\t\tbatchSize:   params.batchSize,\n\t}\n}\n\nfunc (j *janitor) shutdown() {\n\tj.logger.Debug(\"Janitor shutting down...\")\n\t// Signal the janitor goroutine to stop.\n\tj.done <- struct{}{}\n}\n\n// start starts the \"janitor\" goroutine.\nfunc (j *janitor) start(wg *sync.WaitGroup) {\n\twg.Add(1)\n\ttimer := time.NewTimer(j.avgInterval) // randomize this interval with margin of 1s\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-j.done:\n\t\t\t\tj.logger.Debug(\"Janitor done\")\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t\tj.exec()\n\t\t\t\ttimer.Reset(j.avgInterval)\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc (j *janitor) exec() {\n\tfor _, qname := range j.queues {\n\t\tif err := j.broker.DeleteExpiredCompletedTasks(qname, j.batchSize); err != nil {\n\t\t\tj.logger.Errorf(\"Failed to delete expired completed tasks from queue %q: %v\",\n\t\t\t\tqname, err)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "janitor_test.go",
          "type": "blob",
          "size": 2.5185546875,
          "content": "// Copyright 2021 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/rdb\"\n\th \"github.com/hibiken/asynq/internal/testutil\"\n)\n\nfunc newCompletedTask(qname, tasktype string, payload []byte, completedAt time.Time) *base.TaskMessage {\n\tmsg := h.NewTaskMessageWithQueue(tasktype, payload, qname)\n\tmsg.CompletedAt = completedAt.Unix()\n\treturn msg\n}\n\nfunc TestJanitor(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\trdbClient := rdb.NewRDB(r)\n\tconst interval = 1 * time.Second\n\tconst batchSize = 100\n\tjanitor := newJanitor(janitorParams{\n\t\tlogger:    testLogger,\n\t\tbroker:    rdbClient,\n\t\tqueues:    []string{\"default\", \"custom\"},\n\t\tinterval:  interval,\n\t\tbatchSize: batchSize,\n\t})\n\n\tnow := time.Now()\n\thourAgo := now.Add(-1 * time.Hour)\n\tminuteAgo := now.Add(-1 * time.Minute)\n\thalfHourAgo := now.Add(-30 * time.Minute)\n\thalfHourFromNow := now.Add(30 * time.Minute)\n\tfiveMinFromNow := now.Add(5 * time.Minute)\n\tmsg1 := newCompletedTask(\"default\", \"task1\", nil, hourAgo)\n\tmsg2 := newCompletedTask(\"default\", \"task2\", nil, minuteAgo)\n\tmsg3 := newCompletedTask(\"custom\", \"task3\", nil, hourAgo)\n\tmsg4 := newCompletedTask(\"custom\", \"task4\", nil, minuteAgo)\n\n\ttests := []struct {\n\t\tcompleted     map[string][]base.Z // initial completed sets\n\t\twantCompleted map[string][]base.Z // expected completed sets after janitor runs\n\t}{\n\t\t{\n\t\t\tcompleted: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{Message: msg1, Score: halfHourAgo.Unix()},\n\t\t\t\t\t{Message: msg2, Score: fiveMinFromNow.Unix()},\n\t\t\t\t},\n\t\t\t\t\"custom\": {\n\t\t\t\t\t{Message: msg3, Score: halfHourFromNow.Unix()},\n\t\t\t\t\t{Message: msg4, Score: minuteAgo.Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t\twantCompleted: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{Message: msg2, Score: fiveMinFromNow.Unix()},\n\t\t\t\t},\n\t\t\t\t\"custom\": {\n\t\t\t\t\t{Message: msg3, Score: halfHourFromNow.Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllCompletedQueues(t, r, tc.completed)\n\n\t\tvar wg sync.WaitGroup\n\t\tjanitor.start(&wg)\n\t\ttime.Sleep(2 * interval) // make sure to let janitor run at least one time\n\t\tjanitor.shutdown()\n\n\t\tfor qname, want := range tc.wantCompleted {\n\t\t\tgot := h.GetCompletedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, got, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"diff found in %q after running janitor: (-want, +got)\\n%s\", base.CompletedKey(qname), diff)\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "periodic_task_manager.go",
          "type": "blob",
          "size": 7.388671875,
          "content": "// Copyright 2022 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/redis/go-redis/v9\"\n)\n\n// PeriodicTaskManager manages scheduling of periodic tasks.\n// It syncs scheduler's entries by calling the config provider periodically.\ntype PeriodicTaskManager struct {\n\ts            *Scheduler\n\tp            PeriodicTaskConfigProvider\n\tsyncInterval time.Duration\n\tdone         chan (struct{})\n\twg           sync.WaitGroup\n\tm            map[string]string // map[hash]entryID\n}\n\ntype PeriodicTaskManagerOpts struct {\n\t// Required: must be non nil\n\tPeriodicTaskConfigProvider PeriodicTaskConfigProvider\n\n\t// Optional: if RedisUniversalClient is nil must be non nil\n\tRedisConnOpt RedisConnOpt\n\n\t// Optional: if RedisUniversalClient is non nil, RedisConnOpt is ignored.\n\tRedisUniversalClient redis.UniversalClient\n\n\t// Optional: scheduler options\n\t*SchedulerOpts\n\n\t// Optional: default is 3m\n\tSyncInterval time.Duration\n}\n\nconst defaultSyncInterval = 3 * time.Minute\n\n// NewPeriodicTaskManager returns a new PeriodicTaskManager instance.\n// The given opts should specify the RedisConnOp and PeriodicTaskConfigProvider at minimum.\nfunc NewPeriodicTaskManager(opts PeriodicTaskManagerOpts) (*PeriodicTaskManager, error) {\n\tif opts.PeriodicTaskConfigProvider == nil {\n\t\treturn nil, fmt.Errorf(\"PeriodicTaskConfigProvider cannot be nil\")\n\t}\n\tif opts.RedisConnOpt == nil && opts.RedisUniversalClient == nil {\n\t\treturn nil, fmt.Errorf(\"RedisConnOpt/RedisUniversalClient cannot be nil\")\n\t}\n\tvar scheduler *Scheduler\n\tif opts.RedisUniversalClient != nil {\n\t\tscheduler = NewSchedulerFromRedisClient(opts.RedisUniversalClient, opts.SchedulerOpts)\n\t} else {\n\t\tscheduler = NewScheduler(opts.RedisConnOpt, opts.SchedulerOpts)\n\t}\n\n\tsyncInterval := opts.SyncInterval\n\tif syncInterval == 0 {\n\t\tsyncInterval = defaultSyncInterval\n\t}\n\treturn &PeriodicTaskManager{\n\t\ts:            scheduler,\n\t\tp:            opts.PeriodicTaskConfigProvider,\n\t\tsyncInterval: syncInterval,\n\t\tdone:         make(chan struct{}),\n\t\tm:            make(map[string]string),\n\t}, nil\n}\n\n// PeriodicTaskConfigProvider provides configs for periodic tasks.\n// GetConfigs will be called by a PeriodicTaskManager periodically to\n// sync the scheduler's entries with the configs returned by the provider.\ntype PeriodicTaskConfigProvider interface {\n\tGetConfigs() ([]*PeriodicTaskConfig, error)\n}\n\n// PeriodicTaskConfig specifies the details of a periodic task.\ntype PeriodicTaskConfig struct {\n\tCronspec string   // required: must be non empty string\n\tTask     *Task    // required: must be non nil\n\tOpts     []Option // optional: can be nil\n}\n\nfunc (c *PeriodicTaskConfig) hash() string {\n\th := sha256.New()\n\t_, _ = h.Write([]byte(c.Cronspec))\n\t_, _ = h.Write([]byte(c.Task.Type()))\n\th.Write(c.Task.Payload())\n\topts := stringifyOptions(c.Opts)\n\tsort.Strings(opts)\n\tfor _, opt := range opts {\n\t\t_, _ = h.Write([]byte(opt))\n\t}\n\treturn fmt.Sprintf(\"%x\", h.Sum(nil))\n}\n\nfunc validatePeriodicTaskConfig(c *PeriodicTaskConfig) error {\n\tif c == nil {\n\t\treturn fmt.Errorf(\"PeriodicTaskConfig cannot be nil\")\n\t}\n\tif c.Task == nil {\n\t\treturn fmt.Errorf(\"PeriodicTaskConfig.Task cannot be nil\")\n\t}\n\tif c.Cronspec == \"\" {\n\t\treturn fmt.Errorf(\"PeriodicTaskConfig.Cronspec cannot be empty\")\n\t}\n\treturn nil\n}\n\n// Start starts a scheduler and background goroutine to sync the scheduler with the configs\n// returned by the provider.\n//\n// Start returns any error encountered at start up time.\nfunc (mgr *PeriodicTaskManager) Start() error {\n\tif mgr.s == nil || mgr.p == nil {\n\t\tpanic(\"asynq: cannot start uninitialized PeriodicTaskManager; use NewPeriodicTaskManager to initialize\")\n\t}\n\tif err := mgr.initialSync(); err != nil {\n\t\treturn fmt.Errorf(\"asynq: %v\", err)\n\t}\n\tif err := mgr.s.Start(); err != nil {\n\t\treturn fmt.Errorf(\"asynq: %v\", err)\n\t}\n\tmgr.wg.Add(1)\n\tgo func() {\n\t\tdefer mgr.wg.Done()\n\t\tticker := time.NewTicker(mgr.syncInterval)\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-mgr.done:\n\t\t\t\tmgr.s.logger.Debugf(\"Stopping syncer goroutine\")\n\t\t\t\tticker.Stop()\n\t\t\t\treturn\n\t\t\tcase <-ticker.C:\n\t\t\t\tmgr.sync()\n\t\t\t}\n\t\t}\n\t}()\n\treturn nil\n}\n\n// Shutdown gracefully shuts down the manager.\n// It notifies a background syncer goroutine to stop and stops scheduler.\nfunc (mgr *PeriodicTaskManager) Shutdown() {\n\tclose(mgr.done)\n\tmgr.wg.Wait()\n\tmgr.s.Shutdown()\n}\n\n// Run starts the manager and blocks until an os signal to exit the program is received.\n// Once it receives a signal, it gracefully shuts down the manager.\nfunc (mgr *PeriodicTaskManager) Run() error {\n\tif err := mgr.Start(); err != nil {\n\t\treturn err\n\t}\n\tmgr.s.waitForSignals()\n\tmgr.Shutdown()\n\tmgr.s.logger.Debugf(\"PeriodicTaskManager exiting\")\n\treturn nil\n}\n\nfunc (mgr *PeriodicTaskManager) initialSync() error {\n\tconfigs, err := mgr.p.GetConfigs()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"initial call to GetConfigs failed: %v\", err)\n\t}\n\tfor _, c := range configs {\n\t\tif err := validatePeriodicTaskConfig(c); err != nil {\n\t\t\treturn fmt.Errorf(\"initial call to GetConfigs contained an invalid config: %v\", err)\n\t\t}\n\t}\n\tmgr.add(configs)\n\treturn nil\n}\n\nfunc (mgr *PeriodicTaskManager) add(configs []*PeriodicTaskConfig) {\n\tfor _, c := range configs {\n\t\tentryID, err := mgr.s.Register(c.Cronspec, c.Task, c.Opts...)\n\t\tif err != nil {\n\t\t\tmgr.s.logger.Errorf(\"Failed to register periodic task: cronspec=%q task=%q err=%v\",\n\t\t\t\tc.Cronspec, c.Task.Type(), err)\n\t\t\tcontinue\n\t\t}\n\t\tmgr.m[c.hash()] = entryID\n\t\tmgr.s.logger.Infof(\"Successfully registered periodic task: cronspec=%q task=%q, entryID=%s\",\n\t\t\tc.Cronspec, c.Task.Type(), entryID)\n\t}\n}\n\nfunc (mgr *PeriodicTaskManager) remove(removed map[string]string) {\n\tfor hash, entryID := range removed {\n\t\tif err := mgr.s.Unregister(entryID); err != nil {\n\t\t\tmgr.s.logger.Errorf(\"Failed to unregister periodic task: %v\", err)\n\t\t\tcontinue\n\t\t}\n\t\tdelete(mgr.m, hash)\n\t\tmgr.s.logger.Infof(\"Successfully unregistered periodic task: entryID=%s\", entryID)\n\t}\n}\n\nfunc (mgr *PeriodicTaskManager) sync() {\n\tconfigs, err := mgr.p.GetConfigs()\n\tif err != nil {\n\t\tmgr.s.logger.Errorf(\"Failed to get periodic task configs: %v\", err)\n\t\treturn\n\t}\n\tfor _, c := range configs {\n\t\tif err := validatePeriodicTaskConfig(c); err != nil {\n\t\t\tmgr.s.logger.Errorf(\"Failed to sync: GetConfigs returned an invalid config: %v\", err)\n\t\t\treturn\n\t\t}\n\t}\n\t// Diff and only register/unregister the newly added/removed entries.\n\tremoved := mgr.diffRemoved(configs)\n\tadded := mgr.diffAdded(configs)\n\tmgr.remove(removed)\n\tmgr.add(added)\n}\n\n// diffRemoved diffs the incoming configs with the registered config and returns\n// a map containing hash and entryID of each config that was removed.\nfunc (mgr *PeriodicTaskManager) diffRemoved(configs []*PeriodicTaskConfig) map[string]string {\n\tnewConfigs := make(map[string]string)\n\tfor _, c := range configs {\n\t\tnewConfigs[c.hash()] = \"\" // empty value since we don't have entryID yet\n\t}\n\tremoved := make(map[string]string)\n\tfor k, v := range mgr.m {\n\t\t// test whether existing config is present in the incoming configs\n\t\tif _, found := newConfigs[k]; !found {\n\t\t\tremoved[k] = v\n\t\t}\n\t}\n\treturn removed\n}\n\n// diffAdded diffs the incoming configs with the registered configs and returns\n// a list of configs that were added.\nfunc (mgr *PeriodicTaskManager) diffAdded(configs []*PeriodicTaskConfig) []*PeriodicTaskConfig {\n\tvar added []*PeriodicTaskConfig\n\tfor _, c := range configs {\n\t\tif _, found := mgr.m[c.hash()]; !found {\n\t\t\tadded = append(added, c)\n\t\t}\n\t}\n\treturn added\n}\n"
        },
        {
          "name": "periodic_task_manager_test.go",
          "type": "blob",
          "size": 8.5078125,
          "content": "// Copyright 2022 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"sort\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/go-cmp/cmp\"\n)\n\n// Trivial implementation of PeriodicTaskConfigProvider for testing purpose.\ntype FakeConfigProvider struct {\n\tmu   sync.Mutex\n\tcfgs []*PeriodicTaskConfig\n}\n\nfunc (p *FakeConfigProvider) SetConfigs(cfgs []*PeriodicTaskConfig) {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\tp.cfgs = cfgs\n}\n\nfunc (p *FakeConfigProvider) GetConfigs() ([]*PeriodicTaskConfig, error) {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\treturn p.cfgs, nil\n}\n\nfunc TestNewPeriodicTaskManager(t *testing.T) {\n\tredisConnOpt := getRedisConnOpt(t)\n\tcfgs := []*PeriodicTaskConfig{\n\t\t{Cronspec: \"* * * * *\", Task: NewTask(\"foo\", nil)},\n\t\t{Cronspec: \"* * * * *\", Task: NewTask(\"bar\", nil)},\n\t}\n\ttests := []struct {\n\t\tdesc string\n\t\topts PeriodicTaskManagerOpts\n\t}{\n\t\t{\n\t\t\tdesc: \"with provider and redisConnOpt\",\n\t\t\topts: PeriodicTaskManagerOpts{\n\t\t\t\tRedisConnOpt:               redisConnOpt,\n\t\t\t\tPeriodicTaskConfigProvider: &FakeConfigProvider{cfgs: cfgs},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"with sync option\",\n\t\t\topts: PeriodicTaskManagerOpts{\n\t\t\t\tRedisConnOpt:               redisConnOpt,\n\t\t\t\tPeriodicTaskConfigProvider: &FakeConfigProvider{cfgs: cfgs},\n\t\t\t\tSyncInterval:               5 * time.Minute,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"with scheduler option\",\n\t\t\topts: PeriodicTaskManagerOpts{\n\t\t\t\tRedisConnOpt:               redisConnOpt,\n\t\t\t\tPeriodicTaskConfigProvider: &FakeConfigProvider{cfgs: cfgs},\n\t\t\t\tSyncInterval:               5 * time.Minute,\n\t\t\t\tSchedulerOpts: &SchedulerOpts{\n\t\t\t\t\tLogLevel: DebugLevel,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\t_, err := NewPeriodicTaskManager(tc.opts)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%s; NewPeriodicTaskManager returned error: %v\", tc.desc, err)\n\t\t}\n\t}\n\n\tt.Run(\"error\", func(t *testing.T) {\n\t\ttests := []struct {\n\t\t\tdesc string\n\t\t\topts PeriodicTaskManagerOpts\n\t\t}{\n\t\t\t{\n\t\t\t\tdesc: \"without provider\",\n\t\t\t\topts: PeriodicTaskManagerOpts{\n\t\t\t\t\tRedisConnOpt: redisConnOpt,\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tdesc: \"without redisConOpt\",\n\t\t\t\topts: PeriodicTaskManagerOpts{\n\t\t\t\t\tPeriodicTaskConfigProvider: &FakeConfigProvider{cfgs: cfgs},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\n\t\tfor _, tc := range tests {\n\t\t\t_, err := NewPeriodicTaskManager(tc.opts)\n\t\t\tif err == nil {\n\t\t\t\tt.Errorf(\"%s; NewPeriodicTaskManager did not return error\", tc.desc)\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestPeriodicTaskConfigHash(t *testing.T) {\n\ttests := []struct {\n\t\tdesc   string\n\t\ta      *PeriodicTaskConfig\n\t\tb      *PeriodicTaskConfig\n\t\tisSame bool\n\t}{\n\t\t{\n\t\t\tdesc: \"basic identity test\",\n\t\t\ta: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", nil),\n\t\t\t},\n\t\t\tb: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", nil),\n\t\t\t},\n\t\t\tisSame: true,\n\t\t},\n\t\t{\n\t\t\tdesc: \"with a option\",\n\t\t\ta: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", nil),\n\t\t\t\tOpts:     []Option{Queue(\"myqueue\")},\n\t\t\t},\n\t\t\tb: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", nil),\n\t\t\t\tOpts:     []Option{Queue(\"myqueue\")},\n\t\t\t},\n\t\t\tisSame: true,\n\t\t},\n\t\t{\n\t\t\tdesc: \"with multiple options (different order)\",\n\t\t\ta: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", nil),\n\t\t\t\tOpts:     []Option{Unique(5 * time.Minute), Queue(\"myqueue\")},\n\t\t\t},\n\t\t\tb: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", nil),\n\t\t\t\tOpts:     []Option{Queue(\"myqueue\"), Unique(5 * time.Minute)},\n\t\t\t},\n\t\t\tisSame: true,\n\t\t},\n\t\t{\n\t\t\tdesc: \"with payload\",\n\t\t\ta: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", []byte(\"hello world!\")),\n\t\t\t\tOpts:     []Option{Queue(\"myqueue\")},\n\t\t\t},\n\t\t\tb: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", []byte(\"hello world!\")),\n\t\t\t\tOpts:     []Option{Queue(\"myqueue\")},\n\t\t\t},\n\t\t\tisSame: true,\n\t\t},\n\t\t{\n\t\t\tdesc: \"with different cronspecs\",\n\t\t\ta: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", nil),\n\t\t\t},\n\t\t\tb: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"5 * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", nil),\n\t\t\t},\n\t\t\tisSame: false,\n\t\t},\n\t\t{\n\t\t\tdesc: \"with different task type\",\n\t\t\ta: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", nil),\n\t\t\t},\n\t\t\tb: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"bar\", nil),\n\t\t\t},\n\t\t\tisSame: false,\n\t\t},\n\t\t{\n\t\t\tdesc: \"with different options\",\n\t\t\ta: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", nil),\n\t\t\t\tOpts:     []Option{Queue(\"myqueue\")},\n\t\t\t},\n\t\t\tb: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", nil),\n\t\t\t\tOpts:     []Option{Unique(10 * time.Minute)},\n\t\t\t},\n\t\t\tisSame: false,\n\t\t},\n\t\t{\n\t\t\tdesc: \"with different options (one is subset of the other)\",\n\t\t\ta: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", nil),\n\t\t\t\tOpts:     []Option{Queue(\"myqueue\")},\n\t\t\t},\n\t\t\tb: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", nil),\n\t\t\t\tOpts:     []Option{Queue(\"myqueue\"), Unique(10 * time.Minute)},\n\t\t\t},\n\t\t\tisSame: false,\n\t\t},\n\t\t{\n\t\t\tdesc: \"with different payload\",\n\t\t\ta: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", []byte(\"hello!\")),\n\t\t\t\tOpts:     []Option{Queue(\"myqueue\")},\n\t\t\t},\n\t\t\tb: &PeriodicTaskConfig{\n\t\t\t\tCronspec: \"* * * * *\",\n\t\t\t\tTask:     NewTask(\"foo\", []byte(\"HELLO!\")),\n\t\t\t\tOpts:     []Option{Queue(\"myqueue\"), Unique(10 * time.Minute)},\n\t\t\t},\n\t\t\tisSame: false,\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\tif tc.isSame && tc.a.hash() != tc.b.hash() {\n\t\t\tt.Errorf(\"%s: a.hash=%s b.hash=%s expected to be equal\",\n\t\t\t\ttc.desc, tc.a.hash(), tc.b.hash())\n\t\t}\n\t\tif !tc.isSame && tc.a.hash() == tc.b.hash() {\n\t\t\tt.Errorf(\"%s: a.hash=%s b.hash=%s expected to be not equal\",\n\t\t\t\ttc.desc, tc.a.hash(), tc.b.hash())\n\t\t}\n\t}\n}\n\n// Things to test.\n// - Run the manager\n// - Change provider to return new configs\n// - Verify that the scheduler synced with the new config\nfunc TestPeriodicTaskManager(t *testing.T) {\n\t// Note: In this test, we'll use task type as an ID for each config.\n\tcfgs := []*PeriodicTaskConfig{\n\t\t{Task: NewTask(\"task1\", nil), Cronspec: \"* * * * 1\"},\n\t\t{Task: NewTask(\"task2\", nil), Cronspec: \"* * * * 2\"},\n\t}\n\tconst syncInterval = 3 * time.Second\n\tprovider := &FakeConfigProvider{cfgs: cfgs}\n\tmgr, err := NewPeriodicTaskManager(PeriodicTaskManagerOpts{\n\t\tRedisConnOpt:               getRedisConnOpt(t),\n\t\tPeriodicTaskConfigProvider: provider,\n\t\tSyncInterval:               syncInterval,\n\t})\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to initialize PeriodicTaskManager: %v\", err)\n\t}\n\n\tif err := mgr.Start(); err != nil {\n\t\tt.Fatalf(\"Failed to start PeriodicTaskManager: %v\", err)\n\t}\n\tdefer mgr.Shutdown()\n\n\tgot := extractCronEntries(mgr.s)\n\twant := []*cronEntry{\n\t\t{Cronspec: \"* * * * 1\", TaskType: \"task1\"},\n\t\t{Cronspec: \"* * * * 2\", TaskType: \"task2\"},\n\t}\n\tif diff := cmp.Diff(want, got, sortCronEntry); diff != \"\" {\n\t\tt.Errorf(\"Diff found in scheduler's registered entries: %s\", diff)\n\t}\n\n\t// Change the underlying configs\n\t// - task2 removed\n\t// - task3 added\n\tprovider.SetConfigs([]*PeriodicTaskConfig{\n\t\t{Task: NewTask(\"task1\", nil), Cronspec: \"* * * * 1\"},\n\t\t{Task: NewTask(\"task3\", nil), Cronspec: \"* * * * 3\"},\n\t})\n\n\t// Wait for the next sync\n\ttime.Sleep(syncInterval * 2)\n\n\t// Verify the entries are synced\n\tgot = extractCronEntries(mgr.s)\n\twant = []*cronEntry{\n\t\t{Cronspec: \"* * * * 1\", TaskType: \"task1\"},\n\t\t{Cronspec: \"* * * * 3\", TaskType: \"task3\"},\n\t}\n\tif diff := cmp.Diff(want, got, sortCronEntry); diff != \"\" {\n\t\tt.Errorf(\"Diff found in scheduler's registered entries: %s\", diff)\n\t}\n\n\t// Change the underlying configs\n\t// All configs removed, empty set.\n\tprovider.SetConfigs([]*PeriodicTaskConfig{})\n\n\t// Wait for the next sync\n\ttime.Sleep(syncInterval * 2)\n\n\t// Verify the entries are synced\n\tgot = extractCronEntries(mgr.s)\n\twant = []*cronEntry{}\n\tif diff := cmp.Diff(want, got, sortCronEntry); diff != \"\" {\n\t\tt.Errorf(\"Diff found in scheduler's registered entries: %s\", diff)\n\t}\n}\n\nfunc extractCronEntries(s *Scheduler) []*cronEntry {\n\tvar out []*cronEntry\n\tfor _, e := range s.cron.Entries() {\n\t\tjob := e.Job.(*enqueueJob)\n\t\tout = append(out, &cronEntry{Cronspec: job.cronspec, TaskType: job.task.Type()})\n\t}\n\treturn out\n}\n\nvar sortCronEntry = cmp.Transformer(\"sortCronEntry\", func(in []*cronEntry) []*cronEntry {\n\tout := append([]*cronEntry(nil), in...)\n\tsort.Slice(out, func(i, j int) bool {\n\t\treturn out[i].TaskType < out[j].TaskType\n\t})\n\treturn out\n})\n\n// A simple struct to allow for simpler comparison in test.\ntype cronEntry struct {\n\tCronspec string\n\tTaskType string\n}\n"
        },
        {
          "name": "processor.go",
          "type": "blob",
          "size": 15.392578125,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand/v2\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/hibiken/asynq/internal/base\"\n\tasynqcontext \"github.com/hibiken/asynq/internal/context\"\n\t\"github.com/hibiken/asynq/internal/errors\"\n\t\"github.com/hibiken/asynq/internal/log\"\n\t\"github.com/hibiken/asynq/internal/timeutil\"\n\t\"golang.org/x/time/rate\"\n)\n\ntype processor struct {\n\tlogger *log.Logger\n\tbroker base.Broker\n\tclock  timeutil.Clock\n\n\thandler   Handler\n\tbaseCtxFn func() context.Context\n\n\tqueueConfig map[string]int\n\n\t// orderedQueues is set only in strict-priority mode.\n\torderedQueues []string\n\n\ttaskCheckInterval time.Duration\n\tretryDelayFunc    RetryDelayFunc\n\tisFailureFunc     func(error) bool\n\n\terrHandler      ErrorHandler\n\tshutdownTimeout time.Duration\n\n\t// channel via which to send sync requests to syncer.\n\tsyncRequestCh chan<- *syncRequest\n\n\t// rate limiter to prevent spamming logs with a bunch of errors.\n\terrLogLimiter *rate.Limiter\n\n\t// sema is a counting semaphore to ensure the number of active workers\n\t// does not exceed the limit.\n\tsema chan struct{}\n\n\t// channel to communicate back to the long running \"processor\" goroutine.\n\t// once is used to send value to the channel only once.\n\tdone chan struct{}\n\tonce sync.Once\n\n\t// quit channel is closed when the shutdown of the \"processor\" goroutine starts.\n\tquit chan struct{}\n\n\t// abort channel communicates to the in-flight worker goroutines to stop.\n\tabort chan struct{}\n\n\t// cancelations is a set of cancel functions for all active tasks.\n\tcancelations *base.Cancelations\n\n\tstarting chan<- *workerInfo\n\tfinished chan<- *base.TaskMessage\n}\n\ntype processorParams struct {\n\tlogger            *log.Logger\n\tbroker            base.Broker\n\tbaseCtxFn         func() context.Context\n\tretryDelayFunc    RetryDelayFunc\n\ttaskCheckInterval time.Duration\n\tisFailureFunc     func(error) bool\n\tsyncCh            chan<- *syncRequest\n\tcancelations      *base.Cancelations\n\tconcurrency       int\n\tqueues            map[string]int\n\tstrictPriority    bool\n\terrHandler        ErrorHandler\n\tshutdownTimeout   time.Duration\n\tstarting          chan<- *workerInfo\n\tfinished          chan<- *base.TaskMessage\n}\n\n// newProcessor constructs a new processor.\nfunc newProcessor(params processorParams) *processor {\n\tqueues := normalizeQueues(params.queues)\n\torderedQueues := []string(nil)\n\tif params.strictPriority {\n\t\torderedQueues = sortByPriority(queues)\n\t}\n\treturn &processor{\n\t\tlogger:            params.logger,\n\t\tbroker:            params.broker,\n\t\tbaseCtxFn:         params.baseCtxFn,\n\t\tclock:             timeutil.NewRealClock(),\n\t\tqueueConfig:       queues,\n\t\torderedQueues:     orderedQueues,\n\t\ttaskCheckInterval: params.taskCheckInterval,\n\t\tretryDelayFunc:    params.retryDelayFunc,\n\t\tisFailureFunc:     params.isFailureFunc,\n\t\tsyncRequestCh:     params.syncCh,\n\t\tcancelations:      params.cancelations,\n\t\terrLogLimiter:     rate.NewLimiter(rate.Every(3*time.Second), 1),\n\t\tsema:              make(chan struct{}, params.concurrency),\n\t\tdone:              make(chan struct{}),\n\t\tquit:              make(chan struct{}),\n\t\tabort:             make(chan struct{}),\n\t\terrHandler:        params.errHandler,\n\t\thandler:           HandlerFunc(func(ctx context.Context, t *Task) error { return fmt.Errorf(\"handler not set\") }),\n\t\tshutdownTimeout:   params.shutdownTimeout,\n\t\tstarting:          params.starting,\n\t\tfinished:          params.finished,\n\t}\n}\n\n// Note: stops only the \"processor\" goroutine, does not stop workers.\n// It's safe to call this method multiple times.\nfunc (p *processor) stop() {\n\tp.once.Do(func() {\n\t\tp.logger.Debug(\"Processor shutting down...\")\n\t\t// Unblock if processor is waiting for sema token.\n\t\tclose(p.quit)\n\t\t// Signal the processor goroutine to stop processing tasks\n\t\t// from the queue.\n\t\tp.done <- struct{}{}\n\t})\n}\n\n// NOTE: once shutdown, processor cannot be re-started.\nfunc (p *processor) shutdown() {\n\tp.stop()\n\n\ttime.AfterFunc(p.shutdownTimeout, func() { close(p.abort) })\n\n\tp.logger.Info(\"Waiting for all workers to finish...\")\n\t// block until all workers have released the token\n\tfor i := 0; i < cap(p.sema); i++ {\n\t\tp.sema <- struct{}{}\n\t}\n\tp.logger.Info(\"All workers have finished\")\n}\n\nfunc (p *processor) start(wg *sync.WaitGroup) {\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-p.done:\n\t\t\t\tp.logger.Debug(\"Processor done\")\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t\tp.exec()\n\t\t\t}\n\t\t}\n\t}()\n}\n\n// exec pulls a task out of the queue and starts a worker goroutine to\n// process the task.\nfunc (p *processor) exec() {\n\tselect {\n\tcase <-p.quit:\n\t\treturn\n\tcase p.sema <- struct{}{}: // acquire token\n\t\tqnames := p.queues()\n\t\tmsg, leaseExpirationTime, err := p.broker.Dequeue(qnames...)\n\t\tswitch {\n\t\tcase errors.Is(err, errors.ErrNoProcessableTask):\n\t\t\tp.logger.Debug(\"All queues are empty\")\n\t\t\t// Queues are empty, this is a normal behavior.\n\t\t\t// Sleep to avoid slamming redis and let scheduler move tasks into queues.\n\t\t\t// Note: We are not using blocking pop operation and polling queues instead.\n\t\t\t// This adds significant load to redis.\n\t\t\tjitter := rand.N(p.taskCheckInterval)\n\t\t\ttime.Sleep(p.taskCheckInterval/2 + jitter)\n\t\t\t<-p.sema // release token\n\t\t\treturn\n\t\tcase err != nil:\n\t\t\tif p.errLogLimiter.Allow() {\n\t\t\t\tp.logger.Errorf(\"Dequeue error: %v\", err)\n\t\t\t}\n\t\t\t<-p.sema // release token\n\t\t\treturn\n\t\t}\n\n\t\tlease := base.NewLease(leaseExpirationTime)\n\t\tdeadline := p.computeDeadline(msg)\n\t\tp.starting <- &workerInfo{msg, time.Now(), deadline, lease}\n\t\tgo func() {\n\t\t\tdefer func() {\n\t\t\t\tp.finished <- msg\n\t\t\t\t<-p.sema // release token\n\t\t\t}()\n\n\t\t\tctx, cancel := asynqcontext.New(p.baseCtxFn(), msg, deadline)\n\t\t\tp.cancelations.Add(msg.ID, cancel)\n\t\t\tdefer func() {\n\t\t\t\tcancel()\n\t\t\t\tp.cancelations.Delete(msg.ID)\n\t\t\t}()\n\n\t\t\t// check context before starting a worker goroutine.\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\t// already canceled (e.g. deadline exceeded).\n\t\t\t\tp.handleFailedMessage(ctx, lease, msg, ctx.Err())\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t}\n\n\t\t\tresCh := make(chan error, 1)\n\t\t\tgo func() {\n\t\t\t\ttask := newTask(\n\t\t\t\t\tmsg.Type,\n\t\t\t\t\tmsg.Payload,\n\t\t\t\t\t&ResultWriter{\n\t\t\t\t\t\tid:     msg.ID,\n\t\t\t\t\t\tqname:  msg.Queue,\n\t\t\t\t\t\tbroker: p.broker,\n\t\t\t\t\t\tctx:    ctx,\n\t\t\t\t\t},\n\t\t\t\t)\n\t\t\t\tresCh <- p.perform(ctx, task)\n\t\t\t}()\n\n\t\t\tselect {\n\t\t\tcase <-p.abort:\n\t\t\t\t// time is up, push the message back to queue and quit this worker goroutine.\n\t\t\t\tp.logger.Warnf(\"Quitting worker. task id=%s\", msg.ID)\n\t\t\t\tp.requeue(lease, msg)\n\t\t\t\treturn\n\t\t\tcase <-lease.Done():\n\t\t\t\tcancel()\n\t\t\t\tp.handleFailedMessage(ctx, lease, msg, ErrLeaseExpired)\n\t\t\t\treturn\n\t\t\tcase <-ctx.Done():\n\t\t\t\tp.handleFailedMessage(ctx, lease, msg, ctx.Err())\n\t\t\t\treturn\n\t\t\tcase resErr := <-resCh:\n\t\t\t\tif resErr != nil {\n\t\t\t\t\tp.handleFailedMessage(ctx, lease, msg, resErr)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tp.handleSucceededMessage(lease, msg)\n\t\t\t}\n\t\t}()\n\t}\n}\n\nfunc (p *processor) requeue(l *base.Lease, msg *base.TaskMessage) {\n\tif !l.IsValid() {\n\t\t// If lease is not valid, do not write to redis; Let recoverer take care of it.\n\t\treturn\n\t}\n\tctx, cancel := context.WithDeadline(context.Background(), l.Deadline())\n\tdefer cancel()\n\terr := p.broker.Requeue(ctx, msg)\n\tif err != nil {\n\t\tp.logger.Errorf(\"Could not push task id=%s back to queue: %v\", msg.ID, err)\n\t} else {\n\t\tp.logger.Infof(\"Pushed task id=%s back to queue\", msg.ID)\n\t}\n}\n\nfunc (p *processor) handleSucceededMessage(l *base.Lease, msg *base.TaskMessage) {\n\tif msg.Retention > 0 {\n\t\tp.markAsComplete(l, msg)\n\t} else {\n\t\tp.markAsDone(l, msg)\n\t}\n}\n\nfunc (p *processor) markAsComplete(l *base.Lease, msg *base.TaskMessage) {\n\tif !l.IsValid() {\n\t\t// If lease is not valid, do not write to redis; Let recoverer take care of it.\n\t\treturn\n\t}\n\tctx, cancel := context.WithDeadline(context.Background(), l.Deadline())\n\tdefer cancel()\n\terr := p.broker.MarkAsComplete(ctx, msg)\n\tif err != nil {\n\t\terrMsg := fmt.Sprintf(\"Could not move task id=%s type=%q from %q to %q:  %+v\",\n\t\t\tmsg.ID, msg.Type, base.ActiveKey(msg.Queue), base.CompletedKey(msg.Queue), err)\n\t\tp.logger.Warnf(\"%s; Will retry syncing\", errMsg)\n\t\tp.syncRequestCh <- &syncRequest{\n\t\t\tfn: func() error {\n\t\t\t\treturn p.broker.MarkAsComplete(ctx, msg)\n\t\t\t},\n\t\t\terrMsg:   errMsg,\n\t\t\tdeadline: l.Deadline(),\n\t\t}\n\t}\n}\n\nfunc (p *processor) markAsDone(l *base.Lease, msg *base.TaskMessage) {\n\tif !l.IsValid() {\n\t\t// If lease is not valid, do not write to redis; Let recoverer take care of it.\n\t\treturn\n\t}\n\tctx, cancel := context.WithDeadline(context.Background(), l.Deadline())\n\tdefer cancel()\n\terr := p.broker.Done(ctx, msg)\n\tif err != nil {\n\t\terrMsg := fmt.Sprintf(\"Could not remove task id=%s type=%q from %q err: %+v\", msg.ID, msg.Type, base.ActiveKey(msg.Queue), err)\n\t\tp.logger.Warnf(\"%s; Will retry syncing\", errMsg)\n\t\tp.syncRequestCh <- &syncRequest{\n\t\t\tfn: func() error {\n\t\t\t\treturn p.broker.Done(ctx, msg)\n\t\t\t},\n\t\t\terrMsg:   errMsg,\n\t\t\tdeadline: l.Deadline(),\n\t\t}\n\t}\n}\n\n// SkipRetry is used as a return value from Handler.ProcessTask to indicate that\n// the task should not be retried and should be archived instead.\nvar SkipRetry = errors.New(\"skip retry for the task\")\n\n// RevokeTask is used as a return value from Handler.ProcessTask to indicate that\n// the task should not be retried or archived.\nvar RevokeTask = errors.New(\"revoke task\")\n\nfunc (p *processor) handleFailedMessage(ctx context.Context, l *base.Lease, msg *base.TaskMessage, err error) {\n\tif p.errHandler != nil {\n\t\tp.errHandler.HandleError(ctx, NewTask(msg.Type, msg.Payload), err)\n\t}\n\tswitch {\n\tcase errors.Is(err, RevokeTask):\n\t\tp.logger.Warnf(\"revoke task id=%s\", msg.ID)\n\t\tp.markAsDone(l, msg)\n\tcase msg.Retried >= msg.Retry || errors.Is(err, SkipRetry):\n\t\tp.logger.Warnf(\"Retry exhausted for task id=%s\", msg.ID)\n\t\tp.archive(l, msg, err)\n\tdefault:\n\t\tp.retry(l, msg, err, p.isFailureFunc(err))\n\t}\n}\n\nfunc (p *processor) retry(l *base.Lease, msg *base.TaskMessage, e error, isFailure bool) {\n\tif !l.IsValid() {\n\t\t// If lease is not valid, do not write to redis; Let recoverer take care of it.\n\t\treturn\n\t}\n\tctx, cancel := context.WithDeadline(context.Background(), l.Deadline())\n\tdefer cancel()\n\td := p.retryDelayFunc(msg.Retried, e, NewTask(msg.Type, msg.Payload))\n\tretryAt := time.Now().Add(d)\n\terr := p.broker.Retry(ctx, msg, retryAt, e.Error(), isFailure)\n\tif err != nil {\n\t\terrMsg := fmt.Sprintf(\"Could not move task id=%s from %q to %q\", msg.ID, base.ActiveKey(msg.Queue), base.RetryKey(msg.Queue))\n\t\tp.logger.Warnf(\"%s; Will retry syncing\", errMsg)\n\t\tp.syncRequestCh <- &syncRequest{\n\t\t\tfn: func() error {\n\t\t\t\treturn p.broker.Retry(ctx, msg, retryAt, e.Error(), isFailure)\n\t\t\t},\n\t\t\terrMsg:   errMsg,\n\t\t\tdeadline: l.Deadline(),\n\t\t}\n\t}\n}\n\nfunc (p *processor) archive(l *base.Lease, msg *base.TaskMessage, e error) {\n\tif !l.IsValid() {\n\t\t// If lease is not valid, do not write to redis; Let recoverer take care of it.\n\t\treturn\n\t}\n\tctx, cancel := context.WithDeadline(context.Background(), l.Deadline())\n\tdefer cancel()\n\terr := p.broker.Archive(ctx, msg, e.Error())\n\tif err != nil {\n\t\terrMsg := fmt.Sprintf(\"Could not move task id=%s from %q to %q\", msg.ID, base.ActiveKey(msg.Queue), base.ArchivedKey(msg.Queue))\n\t\tp.logger.Warnf(\"%s; Will retry syncing\", errMsg)\n\t\tp.syncRequestCh <- &syncRequest{\n\t\t\tfn: func() error {\n\t\t\t\treturn p.broker.Archive(ctx, msg, e.Error())\n\t\t\t},\n\t\t\terrMsg:   errMsg,\n\t\t\tdeadline: l.Deadline(),\n\t\t}\n\t}\n}\n\n// queues returns a list of queues to query.\n// Order of the queue names is based on the priority of each queue.\n// Queue names is sorted by their priority level if strict-priority is true.\n// If strict-priority is false, then the order of queue names are roughly based on\n// the priority level but randomized in order to avoid starving low priority queues.\nfunc (p *processor) queues() []string {\n\t// skip the overhead of generating a list of queue names\n\t// if we are processing one queue.\n\tif len(p.queueConfig) == 1 {\n\t\tfor qname := range p.queueConfig {\n\t\t\treturn []string{qname}\n\t\t}\n\t}\n\tif p.orderedQueues != nil {\n\t\treturn p.orderedQueues\n\t}\n\tvar names []string\n\tfor qname, priority := range p.queueConfig {\n\t\tfor i := 0; i < priority; i++ {\n\t\t\tnames = append(names, qname)\n\t\t}\n\t}\n\trand.Shuffle(len(names), func(i, j int) { names[i], names[j] = names[j], names[i] })\n\treturn uniq(names, len(p.queueConfig))\n}\n\n// perform calls the handler with the given task.\n// If the call returns without panic, it simply returns the value,\n// otherwise, it recovers from panic and returns an error.\nfunc (p *processor) perform(ctx context.Context, task *Task) (err error) {\n\tdefer func() {\n\t\tif x := recover(); x != nil {\n\t\t\tp.logger.Errorf(\"recovering from panic. See the stack trace below for details:\\n%s\", string(debug.Stack()))\n\t\t\t_, file, line, ok := runtime.Caller(1) // skip the first frame (panic itself)\n\t\t\tif ok && strings.Contains(file, \"runtime/\") {\n\t\t\t\t// The panic came from the runtime, most likely due to incorrect\n\t\t\t\t// map/slice usage. The parent frame should have the real trigger.\n\t\t\t\t_, file, line, ok = runtime.Caller(2)\n\t\t\t}\n\t\t\tvar errMsg string\n\t\t\t// Include the file and line number info in the error, if runtime.Caller returned ok.\n\t\t\tif ok {\n\t\t\t\terrMsg = fmt.Sprintf(\"panic [%s:%d]: %v\", file, line, x)\n\t\t\t} else {\n\t\t\t\terrMsg = fmt.Sprintf(\"panic: %v\", x)\n\t\t\t}\n\t\t\terr = &errors.PanicError{\n\t\t\t\tErrMsg: errMsg,\n\t\t\t}\n\t\t}\n\t}()\n\treturn p.handler.ProcessTask(ctx, task)\n}\n\n// uniq dedupes elements and returns a slice of unique names of length l.\n// Order of the output slice is based on the input list.\nfunc uniq(names []string, l int) []string {\n\tvar res []string\n\tseen := make(map[string]struct{})\n\tfor _, s := range names {\n\t\tif _, ok := seen[s]; !ok {\n\t\t\tseen[s] = struct{}{}\n\t\t\tres = append(res, s)\n\t\t}\n\t\tif len(res) == l {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn res\n}\n\n// sortByPriority returns a list of queue names sorted by\n// their priority level in descending order.\nfunc sortByPriority(qcfg map[string]int) []string {\n\tvar queues []*queue\n\tfor qname, n := range qcfg {\n\t\tqueues = append(queues, &queue{qname, n})\n\t}\n\tsort.Sort(sort.Reverse(byPriority(queues)))\n\tvar res []string\n\tfor _, q := range queues {\n\t\tres = append(res, q.name)\n\t}\n\treturn res\n}\n\ntype queue struct {\n\tname     string\n\tpriority int\n}\n\ntype byPriority []*queue\n\nfunc (x byPriority) Len() int           { return len(x) }\nfunc (x byPriority) Less(i, j int) bool { return x[i].priority < x[j].priority }\nfunc (x byPriority) Swap(i, j int)      { x[i], x[j] = x[j], x[i] }\n\n// normalizeQueues divides priority numbers by their greatest common divisor.\nfunc normalizeQueues(queues map[string]int) map[string]int {\n\tvar xs []int\n\tfor _, x := range queues {\n\t\txs = append(xs, x)\n\t}\n\td := gcd(xs...)\n\tres := make(map[string]int)\n\tfor q, x := range queues {\n\t\tres[q] = x / d\n\t}\n\treturn res\n}\n\nfunc gcd(xs ...int) int {\n\tfn := func(x, y int) int {\n\t\tfor y > 0 {\n\t\t\tx, y = y, x%y\n\t\t}\n\t\treturn x\n\t}\n\tres := xs[0]\n\tfor i := 0; i < len(xs); i++ {\n\t\tres = fn(xs[i], res)\n\t\tif res == 1 {\n\t\t\treturn 1\n\t\t}\n\t}\n\treturn res\n}\n\n// computeDeadline returns the given task's deadline,\nfunc (p *processor) computeDeadline(msg *base.TaskMessage) time.Time {\n\tif msg.Timeout == 0 && msg.Deadline == 0 {\n\t\tp.logger.Errorf(\"asynq: internal error: both timeout and deadline are not set for the task message: %s\", msg.ID)\n\t\treturn p.clock.Now().Add(defaultTimeout)\n\t}\n\tif msg.Timeout != 0 && msg.Deadline != 0 {\n\t\tdeadlineUnix := math.Min(float64(p.clock.Now().Unix()+msg.Timeout), float64(msg.Deadline))\n\t\treturn time.Unix(int64(deadlineUnix), 0)\n\t}\n\tif msg.Timeout != 0 {\n\t\treturn p.clock.Now().Add(time.Duration(msg.Timeout) * time.Second)\n\t}\n\treturn time.Unix(msg.Deadline, 0)\n}\n\nfunc IsPanicError(err error) bool {\n\treturn errors.IsPanicError(err)\n}\n"
        },
        {
          "name": "processor_test.go",
          "type": "blob",
          "size": 28.615234375,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/google/go-cmp/cmp/cmpopts\"\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/errors\"\n\t\"github.com/hibiken/asynq/internal/log\"\n\t\"github.com/hibiken/asynq/internal/rdb\"\n\th \"github.com/hibiken/asynq/internal/testutil\"\n\t\"github.com/hibiken/asynq/internal/timeutil\"\n)\n\nvar taskCmpOpts = []cmp.Option{\n\tsortTaskOpt,                               // sort the tasks\n\tcmp.AllowUnexported(Task{}),               // allow typename, payload fields to be compared\n\tcmpopts.IgnoreFields(Task{}, \"opts\", \"w\"), // ignore opts, w fields\n}\n\n// fakeHeartbeater receives from starting and finished channels and do nothing.\nfunc fakeHeartbeater(starting <-chan *workerInfo, finished <-chan *base.TaskMessage, done <-chan struct{}) {\n\tfor {\n\t\tselect {\n\t\tcase <-starting:\n\t\tcase <-finished:\n\t\tcase <-done:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// fakeSyncer receives from sync channel and do nothing.\nfunc fakeSyncer(syncCh <-chan *syncRequest, done <-chan struct{}) {\n\tfor {\n\t\tselect {\n\t\tcase <-syncCh:\n\t\tcase <-done:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Returns a processor instance configured for testing purpose.\nfunc newProcessorForTest(t *testing.T, r *rdb.RDB, h Handler) *processor {\n\tstarting := make(chan *workerInfo)\n\tfinished := make(chan *base.TaskMessage)\n\tsyncCh := make(chan *syncRequest)\n\tdone := make(chan struct{})\n\tt.Cleanup(func() { close(done) })\n\tgo fakeHeartbeater(starting, finished, done)\n\tgo fakeSyncer(syncCh, done)\n\tp := newProcessor(processorParams{\n\t\tlogger:            testLogger,\n\t\tbroker:            r,\n\t\tbaseCtxFn:         context.Background,\n\t\tretryDelayFunc:    DefaultRetryDelayFunc,\n\t\ttaskCheckInterval: defaultTaskCheckInterval,\n\t\tisFailureFunc:     defaultIsFailureFunc,\n\t\tsyncCh:            syncCh,\n\t\tcancelations:      base.NewCancelations(),\n\t\tconcurrency:       10,\n\t\tqueues:            defaultQueueConfig,\n\t\tstrictPriority:    false,\n\t\terrHandler:        nil,\n\t\tshutdownTimeout:   defaultShutdownTimeout,\n\t\tstarting:          starting,\n\t\tfinished:          finished,\n\t})\n\tp.handler = h\n\treturn p\n}\n\nfunc TestProcessorSuccessWithSingleQueue(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\trdbClient := rdb.NewRDB(r)\n\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\tm2 := h.NewTaskMessage(\"task2\", nil)\n\tm3 := h.NewTaskMessage(\"task3\", nil)\n\tm4 := h.NewTaskMessage(\"task4\", nil)\n\n\tt1 := NewTask(m1.Type, m1.Payload)\n\tt2 := NewTask(m2.Type, m2.Payload)\n\tt3 := NewTask(m3.Type, m3.Payload)\n\tt4 := NewTask(m4.Type, m4.Payload)\n\n\ttests := []struct {\n\t\tpending       []*base.TaskMessage // initial default queue state\n\t\tincoming      []*base.TaskMessage // tasks to be enqueued during run\n\t\twantProcessed []*Task             // tasks to be processed at the end\n\t}{\n\t\t{\n\t\t\tpending:       []*base.TaskMessage{m1},\n\t\t\tincoming:      []*base.TaskMessage{m2, m3, m4},\n\t\t\twantProcessed: []*Task{t1, t2, t3, t4},\n\t\t},\n\t\t{\n\t\t\tpending:       []*base.TaskMessage{},\n\t\t\tincoming:      []*base.TaskMessage{m1},\n\t\t\twantProcessed: []*Task{t1},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)                                             // clean up db before each test case.\n\t\th.SeedPendingQueue(t, r, tc.pending, base.DefaultQueueName) // initialize default queue.\n\n\t\t// instantiate a new processor\n\t\tvar mu sync.Mutex\n\t\tvar processed []*Task\n\t\thandler := func(ctx context.Context, task *Task) error {\n\t\t\tmu.Lock()\n\t\t\tdefer mu.Unlock()\n\t\t\tprocessed = append(processed, task)\n\t\t\treturn nil\n\t\t}\n\t\tp := newProcessorForTest(t, rdbClient, HandlerFunc(handler))\n\n\t\tp.start(&sync.WaitGroup{})\n\t\tfor _, msg := range tc.incoming {\n\t\t\terr := rdbClient.Enqueue(context.Background(), msg)\n\t\t\tif err != nil {\n\t\t\t\tp.shutdown()\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\t\ttime.Sleep(2 * time.Second) // wait for two second to allow all pending tasks to be processed.\n\t\tif l := r.LLen(context.Background(), base.ActiveKey(base.DefaultQueueName)).Val(); l != 0 {\n\t\t\tt.Errorf(\"%q has %d tasks, want 0\", base.ActiveKey(base.DefaultQueueName), l)\n\t\t}\n\t\tp.shutdown()\n\n\t\tmu.Lock()\n\t\tif diff := cmp.Diff(tc.wantProcessed, processed, taskCmpOpts...); diff != \"\" {\n\t\t\tt.Errorf(\"mismatch found in processed tasks; (-want, +got)\\n%s\", diff)\n\t\t}\n\t\tmu.Unlock()\n\t}\n}\n\nfunc TestProcessorSuccessWithMultipleQueues(t *testing.T) {\n\tvar (\n\t\tr         = setup(t)\n\t\trdbClient = rdb.NewRDB(r)\n\n\t\tm1 = h.NewTaskMessage(\"task1\", nil)\n\t\tm2 = h.NewTaskMessage(\"task2\", nil)\n\t\tm3 = h.NewTaskMessageWithQueue(\"task3\", nil, \"high\")\n\t\tm4 = h.NewTaskMessageWithQueue(\"task4\", nil, \"low\")\n\n\t\tt1 = NewTask(m1.Type, m1.Payload)\n\t\tt2 = NewTask(m2.Type, m2.Payload)\n\t\tt3 = NewTask(m3.Type, m3.Payload)\n\t\tt4 = NewTask(m4.Type, m4.Payload)\n\t)\n\tdefer r.Close()\n\n\ttests := []struct {\n\t\tpending       map[string][]*base.TaskMessage\n\t\tqueues        []string // list of queues to consume the tasks from\n\t\twantProcessed []*Task  // tasks to be processed at the end\n\t}{\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {m1, m2},\n\t\t\t\t\"high\":    {m3},\n\t\t\t\t\"low\":     {m4},\n\t\t\t},\n\t\t\tqueues:        []string{\"default\", \"high\", \"low\"},\n\t\t\twantProcessed: []*Task{t1, t2, t3, t4},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\t// Set up test case.\n\t\th.FlushDB(t, r)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\n\t\t// Instantiate a new processor.\n\t\tvar mu sync.Mutex\n\t\tvar processed []*Task\n\t\thandler := func(ctx context.Context, task *Task) error {\n\t\t\tmu.Lock()\n\t\t\tdefer mu.Unlock()\n\t\t\tprocessed = append(processed, task)\n\t\t\treturn nil\n\t\t}\n\t\tp := newProcessorForTest(t, rdbClient, HandlerFunc(handler))\n\t\tp.queueConfig = map[string]int{\n\t\t\t\"default\": 2,\n\t\t\t\"high\":    3,\n\t\t\t\"low\":     1,\n\t\t}\n\n\t\tp.start(&sync.WaitGroup{})\n\t\t// Wait for two second to allow all pending tasks to be processed.\n\t\ttime.Sleep(2 * time.Second)\n\t\t// Make sure no messages are stuck in active list.\n\t\tfor _, qname := range tc.queues {\n\t\t\tif l := r.LLen(context.Background(), base.ActiveKey(qname)).Val(); l != 0 {\n\t\t\t\tt.Errorf(\"%q has %d tasks, want 0\", base.ActiveKey(qname), l)\n\t\t\t}\n\t\t}\n\t\tp.shutdown()\n\n\t\tmu.Lock()\n\t\tif diff := cmp.Diff(tc.wantProcessed, processed, taskCmpOpts...); diff != \"\" {\n\t\t\tt.Errorf(\"mismatch found in processed tasks; (-want, +got)\\n%s\", diff)\n\t\t}\n\t\tmu.Unlock()\n\t}\n}\n\n// https://github.com/hibiken/asynq/issues/166\nfunc TestProcessTasksWithLargeNumberInPayload(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\trdbClient := rdb.NewRDB(r)\n\n\tm1 := h.NewTaskMessage(\"large_number\", h.JSON(map[string]interface{}{\"data\": 111111111111111111}))\n\tt1 := NewTask(m1.Type, m1.Payload)\n\n\ttests := []struct {\n\t\tpending       []*base.TaskMessage // initial default queue state\n\t\twantProcessed []*Task             // tasks to be processed at the end\n\t}{\n\t\t{\n\t\t\tpending:       []*base.TaskMessage{m1},\n\t\t\twantProcessed: []*Task{t1},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)                                             // clean up db before each test case.\n\t\th.SeedPendingQueue(t, r, tc.pending, base.DefaultQueueName) // initialize default queue.\n\n\t\tvar mu sync.Mutex\n\t\tvar processed []*Task\n\t\thandler := func(ctx context.Context, task *Task) error {\n\t\t\tmu.Lock()\n\t\t\tdefer mu.Unlock()\n\t\t\tvar payload map[string]int\n\t\t\tif err := json.Unmarshal(task.Payload(), &payload); err != nil {\n\t\t\t\tt.Errorf(\"coult not decode payload: %v\", err)\n\t\t\t}\n\t\t\tif data, ok := payload[\"data\"]; ok {\n\t\t\t\tt.Logf(\"data == %d\", data)\n\t\t\t} else {\n\t\t\t\tt.Errorf(\"could not get data from payload\")\n\t\t\t}\n\t\t\tprocessed = append(processed, task)\n\t\t\treturn nil\n\t\t}\n\t\tp := newProcessorForTest(t, rdbClient, HandlerFunc(handler))\n\n\t\tp.start(&sync.WaitGroup{})\n\t\ttime.Sleep(2 * time.Second) // wait for two second to allow all pending tasks to be processed.\n\t\tif l := r.LLen(context.Background(), base.ActiveKey(base.DefaultQueueName)).Val(); l != 0 {\n\t\t\tt.Errorf(\"%q has %d tasks, want 0\", base.ActiveKey(base.DefaultQueueName), l)\n\t\t}\n\t\tp.shutdown()\n\n\t\tmu.Lock()\n\t\tif diff := cmp.Diff(tc.wantProcessed, processed, taskCmpOpts...); diff != \"\" {\n\t\t\tt.Errorf(\"mismatch found in processed tasks; (-want, +got)\\n%s\", diff)\n\t\t}\n\t\tmu.Unlock()\n\t}\n}\n\nfunc TestProcessorRetry(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\trdbClient := rdb.NewRDB(r)\n\n\tm1 := h.NewTaskMessage(\"send_email\", nil)\n\tm1.Retried = m1.Retry // m1 has reached its max retry count\n\tm2 := h.NewTaskMessage(\"gen_thumbnail\", nil)\n\tm3 := h.NewTaskMessage(\"reindex\", nil)\n\tm4 := h.NewTaskMessage(\"sync\", nil)\n\n\terrMsg := \"something went wrong\"\n\twrappedSkipRetry := fmt.Errorf(\"%s:%w\", errMsg, SkipRetry)\n\twrappedRevokeTask := fmt.Errorf(\"%s:%w\", errMsg, RevokeTask)\n\n\ttests := []struct {\n\t\tdesc         string              // test description\n\t\tpending      []*base.TaskMessage // initial default queue state\n\t\tdelay        time.Duration       // retry delay duration\n\t\thandler      Handler             // task handler\n\t\twait         time.Duration       // wait duration between starting and stopping processor for this test case\n\t\twantErrMsg   string              // error message the task should record\n\t\twantRetry    []*base.TaskMessage // tasks in retry queue at the end\n\t\twantArchived []*base.TaskMessage // tasks in archived queue at the end\n\t\twantErrCount int                 // number of times error handler should be called\n\t}{\n\t\t{\n\t\t\tdesc:    \"Should automatically retry errored tasks\",\n\t\t\tpending: []*base.TaskMessage{m1, m2, m3, m4},\n\t\t\tdelay:   time.Minute,\n\t\t\thandler: HandlerFunc(func(ctx context.Context, task *Task) error {\n\t\t\t\treturn errors.New(errMsg)\n\t\t\t}),\n\t\t\twait:         2 * time.Second,\n\t\t\twantErrMsg:   errMsg,\n\t\t\twantRetry:    []*base.TaskMessage{m2, m3, m4},\n\t\t\twantArchived: []*base.TaskMessage{m1},\n\t\t\twantErrCount: 4,\n\t\t},\n\t\t{\n\t\t\tdesc:    \"Should skip retry errored tasks\",\n\t\t\tpending: []*base.TaskMessage{m1, m2},\n\t\t\tdelay:   time.Minute,\n\t\t\thandler: HandlerFunc(func(ctx context.Context, task *Task) error {\n\t\t\t\treturn SkipRetry // return SkipRetry without wrapping\n\t\t\t}),\n\t\t\twait:         2 * time.Second,\n\t\t\twantErrMsg:   SkipRetry.Error(),\n\t\t\twantRetry:    []*base.TaskMessage{},\n\t\t\twantArchived: []*base.TaskMessage{m1, m2},\n\t\t\twantErrCount: 2, // ErrorHandler should still be called with SkipRetry error\n\t\t},\n\t\t{\n\t\t\tdesc:    \"Should skip retry errored tasks (with error wrapping)\",\n\t\t\tpending: []*base.TaskMessage{m1, m2},\n\t\t\tdelay:   time.Minute,\n\t\t\thandler: HandlerFunc(func(ctx context.Context, task *Task) error {\n\t\t\t\treturn wrappedSkipRetry\n\t\t\t}),\n\t\t\twait:         2 * time.Second,\n\t\t\twantErrMsg:   wrappedSkipRetry.Error(),\n\t\t\twantRetry:    []*base.TaskMessage{},\n\t\t\twantArchived: []*base.TaskMessage{m1, m2},\n\t\t\twantErrCount: 2, // ErrorHandler should still be called with SkipRetry error\n\t\t},\n\t\t{\n\t\t\tdesc:    \"Should revoke task\",\n\t\t\tpending: []*base.TaskMessage{m1, m2},\n\t\t\tdelay:   time.Minute,\n\t\t\thandler: HandlerFunc(func(ctx context.Context, task *Task) error {\n\t\t\t\treturn RevokeTask // return RevokeTask without wrapping\n\t\t\t}),\n\t\t\twait:         2 * time.Second,\n\t\t\twantErrMsg:   RevokeTask.Error(),\n\t\t\twantRetry:    []*base.TaskMessage{},\n\t\t\twantArchived: []*base.TaskMessage{},\n\t\t\twantErrCount: 2, // ErrorHandler should still be called with RevokeTask error\n\t\t},\n\t\t{\n\t\t\tdesc:    \"Should revoke task (with error wrapping)\",\n\t\t\tpending: []*base.TaskMessage{m1, m2},\n\t\t\tdelay:   time.Minute,\n\t\t\thandler: HandlerFunc(func(ctx context.Context, task *Task) error {\n\t\t\t\treturn wrappedRevokeTask\n\t\t\t}),\n\t\t\twait:         2 * time.Second,\n\t\t\twantErrMsg:   wrappedRevokeTask.Error(),\n\t\t\twantRetry:    []*base.TaskMessage{},\n\t\t\twantArchived: []*base.TaskMessage{},\n\t\t\twantErrCount: 2, // ErrorHandler should still be called with RevokeTask error\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)                                             // clean up db before each test case.\n\t\th.SeedPendingQueue(t, r, tc.pending, base.DefaultQueueName) // initialize default queue.\n\n\t\t// instantiate a new processor\n\t\tdelayFunc := func(n int, e error, t *Task) time.Duration {\n\t\t\treturn tc.delay\n\t\t}\n\t\tvar (\n\t\t\tmu sync.Mutex // guards n\n\t\t\tn  int        // number of times error handler is called\n\t\t)\n\t\terrHandler := func(ctx context.Context, t *Task, err error) {\n\t\t\tmu.Lock()\n\t\t\tdefer mu.Unlock()\n\t\t\tn++\n\t\t}\n\t\tp := newProcessorForTest(t, rdbClient, tc.handler)\n\t\tp.errHandler = ErrorHandlerFunc(errHandler)\n\t\tp.retryDelayFunc = delayFunc\n\n\t\tp.start(&sync.WaitGroup{})\n\t\trunTime := time.Now() // time when processor is running\n\t\ttime.Sleep(tc.wait)   // FIXME: This makes test flaky.\n\t\tp.shutdown()\n\n\t\tcmpOpt := h.EquateInt64Approx(int64(tc.wait.Seconds())) // allow up to a wait-second difference in zset score\n\t\tgotRetry := h.GetRetryEntries(t, r, base.DefaultQueueName)\n\t\tvar wantRetry []base.Z // Note: construct wantRetry here since `LastFailedAt` and ZSCORE is relative to each test run.\n\t\tfor _, msg := range tc.wantRetry {\n\t\t\twantRetry = append(wantRetry,\n\t\t\t\tbase.Z{\n\t\t\t\t\tMessage: h.TaskMessageAfterRetry(*msg, tc.wantErrMsg, runTime),\n\t\t\t\t\tScore:   runTime.Add(tc.delay).Unix(),\n\t\t\t\t})\n\t\t}\n\t\tif diff := cmp.Diff(wantRetry, gotRetry, h.SortZSetEntryOpt, cmpOpt); diff != \"\" {\n\t\t\tt.Errorf(\"%s: mismatch found in %q after running processor; (-want, +got)\\n%s\", tc.desc, base.RetryKey(base.DefaultQueueName), diff)\n\t\t}\n\n\t\tgotArchived := h.GetArchivedEntries(t, r, base.DefaultQueueName)\n\t\tvar wantArchived []base.Z // Note: construct wantArchived here since `LastFailedAt` and ZSCORE is relative to each test run.\n\t\tfor _, msg := range tc.wantArchived {\n\t\t\twantArchived = append(wantArchived,\n\t\t\t\tbase.Z{\n\t\t\t\t\tMessage: h.TaskMessageWithError(*msg, tc.wantErrMsg, runTime),\n\t\t\t\t\tScore:   runTime.Unix(),\n\t\t\t\t})\n\t\t}\n\t\tif diff := cmp.Diff(wantArchived, gotArchived, h.SortZSetEntryOpt, cmpOpt); diff != \"\" {\n\t\t\tt.Errorf(\"%s: mismatch found in %q after running processor; (-want, +got)\\n%s\", tc.desc, base.ArchivedKey(base.DefaultQueueName), diff)\n\t\t}\n\n\t\tif l := r.LLen(context.Background(), base.ActiveKey(base.DefaultQueueName)).Val(); l != 0 {\n\t\t\tt.Errorf(\"%s: %q has %d tasks, want 0\", base.ActiveKey(base.DefaultQueueName), tc.desc, l)\n\t\t}\n\n\t\tif n != tc.wantErrCount {\n\t\t\tt.Errorf(\"error handler was called %d times, want %d\", n, tc.wantErrCount)\n\t\t}\n\t}\n}\n\nfunc TestProcessorMarkAsComplete(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\trdbClient := rdb.NewRDB(r)\n\n\tmsg1 := h.NewTaskMessage(\"one\", nil)\n\tmsg2 := h.NewTaskMessage(\"two\", nil)\n\tmsg3 := h.NewTaskMessageWithQueue(\"three\", nil, \"custom\")\n\tmsg1.Retention = 3600\n\tmsg3.Retention = 7200\n\n\thandler := func(ctx context.Context, task *Task) error { return nil }\n\n\ttests := []struct {\n\t\tpending       map[string][]*base.TaskMessage\n\t\tcompleted     map[string][]base.Z\n\t\tqueueCfg      map[string]int\n\t\twantPending   map[string][]*base.TaskMessage\n\t\twantCompleted func(completedAt time.Time) map[string][]base.Z\n\t}{\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {msg1, msg2},\n\t\t\t\t\"custom\":  {msg3},\n\t\t\t},\n\t\t\tcompleted: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\tqueueCfg: map[string]int{\n\t\t\t\t\"default\": 1,\n\t\t\t\t\"custom\":  1,\n\t\t\t},\n\t\t\twantPending: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"custom\":  {},\n\t\t\t},\n\t\t\twantCompleted: func(completedAt time.Time) map[string][]base.Z {\n\t\t\t\treturn map[string][]base.Z{\n\t\t\t\t\t\"default\": {{Message: h.TaskMessageWithCompletedAt(*msg1, completedAt), Score: completedAt.Unix() + msg1.Retention}},\n\t\t\t\t\t\"custom\":  {{Message: h.TaskMessageWithCompletedAt(*msg3, completedAt), Score: completedAt.Unix() + msg3.Retention}},\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllPendingQueues(t, r, tc.pending)\n\t\th.SeedAllCompletedQueues(t, r, tc.completed)\n\n\t\tp := newProcessorForTest(t, rdbClient, HandlerFunc(handler))\n\t\tp.queueConfig = tc.queueCfg\n\n\t\tp.start(&sync.WaitGroup{})\n\t\trunTime := time.Now() // time when processor is running\n\t\ttime.Sleep(2 * time.Second)\n\t\tp.shutdown()\n\n\t\tfor qname, want := range tc.wantPending {\n\t\t\tgotPending := h.GetPendingMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotPending, cmpopts.EquateEmpty()); diff != \"\" {\n\t\t\t\tt.Errorf(\"diff found in %q pending set; want=%v, got=%v\\n%s\", qname, want, gotPending, diff)\n\t\t\t}\n\t\t}\n\n\t\tfor qname, want := range tc.wantCompleted(runTime) {\n\t\t\tgotCompleted := h.GetCompletedEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotCompleted, cmpopts.EquateEmpty()); diff != \"\" {\n\t\t\t\tt.Errorf(\"diff found in %q completed set; want=%v, got=%v\\n%s\", qname, want, gotCompleted, diff)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Test a scenario where the worker server cannot communicate with redis due to a network failure\n// and the lease expires\nfunc TestProcessorWithExpiredLease(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\trdbClient := rdb.NewRDB(r)\n\n\tm1 := h.NewTaskMessage(\"task1\", nil)\n\n\ttests := []struct {\n\t\tpending      []*base.TaskMessage\n\t\thandler      Handler\n\t\twantErrCount int\n\t}{\n\t\t{\n\t\t\tpending: []*base.TaskMessage{m1},\n\t\t\thandler: HandlerFunc(func(ctx context.Context, task *Task) error {\n\t\t\t\t// make sure the task processing time exceeds lease duration\n\t\t\t\t// to test expired lease.\n\t\t\t\ttime.Sleep(rdb.LeaseDuration + 10*time.Second)\n\t\t\t\treturn nil\n\t\t\t}),\n\t\t\twantErrCount: 1, // ErrorHandler should still be called with ErrLeaseExpired\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedPendingQueue(t, r, tc.pending, base.DefaultQueueName)\n\n\t\tstarting := make(chan *workerInfo)\n\t\tfinished := make(chan *base.TaskMessage)\n\t\tsyncCh := make(chan *syncRequest)\n\t\tdone := make(chan struct{})\n\t\tt.Cleanup(func() { close(done) })\n\t\t// fake heartbeater which notifies lease expiration\n\t\tgo func() {\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase w := <-starting:\n\t\t\t\t\t// simulate expiration by resetting to some time in the past\n\t\t\t\t\tw.lease.Reset(time.Now().Add(-5 * time.Second))\n\t\t\t\t\tif !w.lease.NotifyExpiration() {\n\t\t\t\t\t\tpanic(\"Failed to notifiy lease expiration\")\n\t\t\t\t\t}\n\t\t\t\tcase <-finished:\n\t\t\t\t\t// do nothing\n\t\t\t\tcase <-done:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t\tgo fakeSyncer(syncCh, done)\n\t\tp := newProcessor(processorParams{\n\t\t\tlogger:            testLogger,\n\t\t\tbroker:            rdbClient,\n\t\t\tbaseCtxFn:         context.Background,\n\t\t\ttaskCheckInterval: defaultTaskCheckInterval,\n\t\t\tretryDelayFunc:    DefaultRetryDelayFunc,\n\t\t\tisFailureFunc:     defaultIsFailureFunc,\n\t\t\tsyncCh:            syncCh,\n\t\t\tcancelations:      base.NewCancelations(),\n\t\t\tconcurrency:       10,\n\t\t\tqueues:            defaultQueueConfig,\n\t\t\tstrictPriority:    false,\n\t\t\terrHandler:        nil,\n\t\t\tshutdownTimeout:   defaultShutdownTimeout,\n\t\t\tstarting:          starting,\n\t\t\tfinished:          finished,\n\t\t})\n\t\tp.handler = tc.handler\n\t\tvar (\n\t\t\tmu   sync.Mutex // guards n and errs\n\t\t\tn    int        // number of times error handler is called\n\t\t\terrs []error    // error passed to error handler\n\t\t)\n\t\tp.errHandler = ErrorHandlerFunc(func(ctx context.Context, t *Task, err error) {\n\t\t\tmu.Lock()\n\t\t\tdefer mu.Unlock()\n\t\t\tn++\n\t\t\terrs = append(errs, err)\n\t\t})\n\n\t\tp.start(&sync.WaitGroup{})\n\t\ttime.Sleep(4 * time.Second)\n\t\tp.shutdown()\n\n\t\tif n != tc.wantErrCount {\n\t\t\tt.Errorf(\"Unexpected number of error count: got %d, want %d\", n, tc.wantErrCount)\n\t\t\tcontinue\n\t\t}\n\t\tfor i := 0; i < tc.wantErrCount; i++ {\n\t\t\tif !errors.Is(errs[i], ErrLeaseExpired) {\n\t\t\t\tt.Errorf(\"Unexpected error was passed to ErrorHandler: got %v want %v\", errs[i], ErrLeaseExpired)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestProcessorQueues(t *testing.T) {\n\tsortOpt := cmp.Transformer(\"SortStrings\", func(in []string) []string {\n\t\tout := append([]string(nil), in...) // Copy input to avoid mutating it\n\t\tsort.Strings(out)\n\t\treturn out\n\t})\n\n\ttests := []struct {\n\t\tqueueCfg map[string]int\n\t\twant     []string\n\t}{\n\t\t{\n\t\t\tqueueCfg: map[string]int{\n\t\t\t\t\"high\":    6,\n\t\t\t\t\"default\": 3,\n\t\t\t\t\"low\":     1,\n\t\t\t},\n\t\t\twant: []string{\"high\", \"default\", \"low\"},\n\t\t},\n\t\t{\n\t\t\tqueueCfg: map[string]int{\n\t\t\t\t\"default\": 1,\n\t\t\t},\n\t\t\twant: []string{\"default\"},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\t// Note: rdb and handler not needed for this test.\n\t\tp := newProcessorForTest(t, nil, nil)\n\t\tp.queueConfig = tc.queueCfg\n\n\t\tgot := p.queues()\n\t\tif diff := cmp.Diff(tc.want, got, sortOpt); diff != \"\" {\n\t\t\tt.Errorf(\"with queue config: %v\\n(*processor).queues() = %v, want %v\\n(-want,+got):\\n%s\",\n\t\t\t\ttc.queueCfg, got, tc.want, diff)\n\t\t}\n\t}\n}\n\nfunc TestProcessorWithStrictPriority(t *testing.T) {\n\tvar (\n\t\tr = setup(t)\n\n\t\trdbClient = rdb.NewRDB(r)\n\n\t\tm1 = h.NewTaskMessageWithQueue(\"task1\", nil, \"critical\")\n\t\tm2 = h.NewTaskMessageWithQueue(\"task2\", nil, \"critical\")\n\t\tm3 = h.NewTaskMessageWithQueue(\"task3\", nil, \"critical\")\n\t\tm4 = h.NewTaskMessageWithQueue(\"task4\", nil, base.DefaultQueueName)\n\t\tm5 = h.NewTaskMessageWithQueue(\"task5\", nil, base.DefaultQueueName)\n\t\tm6 = h.NewTaskMessageWithQueue(\"task6\", nil, \"low\")\n\t\tm7 = h.NewTaskMessageWithQueue(\"task7\", nil, \"low\")\n\n\t\tt1 = NewTask(m1.Type, m1.Payload)\n\t\tt2 = NewTask(m2.Type, m2.Payload)\n\t\tt3 = NewTask(m3.Type, m3.Payload)\n\t\tt4 = NewTask(m4.Type, m4.Payload)\n\t\tt5 = NewTask(m5.Type, m5.Payload)\n\t\tt6 = NewTask(m6.Type, m6.Payload)\n\t\tt7 = NewTask(m7.Type, m7.Payload)\n\t)\n\tdefer r.Close()\n\n\ttests := []struct {\n\t\tpending       map[string][]*base.TaskMessage // initial queues state\n\t\tqueues        []string                       // list of queues to consume tasks from\n\t\twait          time.Duration                  // wait duration between starting and stopping processor for this test case\n\t\twantProcessed []*Task                        // tasks to be processed at the end\n\t}{\n\t\t{\n\t\t\tpending: map[string][]*base.TaskMessage{\n\t\t\t\tbase.DefaultQueueName: {m4, m5},\n\t\t\t\t\"critical\":            {m1, m2, m3},\n\t\t\t\t\"low\":                 {m6, m7},\n\t\t\t},\n\t\t\tqueues:        []string{base.DefaultQueueName, \"critical\", \"low\"},\n\t\t\twait:          time.Second,\n\t\t\twantProcessed: []*Task{t1, t2, t3, t4, t5, t6, t7},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r) // clean up db before each test case.\n\t\tfor qname, msgs := range tc.pending {\n\t\t\th.SeedPendingQueue(t, r, msgs, qname)\n\t\t}\n\n\t\t// instantiate a new processor\n\t\tvar mu sync.Mutex\n\t\tvar processed []*Task\n\t\thandler := func(ctx context.Context, task *Task) error {\n\t\t\tmu.Lock()\n\t\t\tdefer mu.Unlock()\n\t\t\tprocessed = append(processed, task)\n\t\t\treturn nil\n\t\t}\n\t\tqueueCfg := map[string]int{\n\t\t\tbase.DefaultQueueName: 2,\n\t\t\t\"critical\":            3,\n\t\t\t\"low\":                 1,\n\t\t}\n\t\tstarting := make(chan *workerInfo)\n\t\tfinished := make(chan *base.TaskMessage)\n\t\tsyncCh := make(chan *syncRequest)\n\t\tdone := make(chan struct{})\n\t\tdefer func() { close(done) }()\n\t\tgo fakeHeartbeater(starting, finished, done)\n\t\tgo fakeSyncer(syncCh, done)\n\t\tp := newProcessor(processorParams{\n\t\t\tlogger:            testLogger,\n\t\t\tbroker:            rdbClient,\n\t\t\tbaseCtxFn:         context.Background,\n\t\t\ttaskCheckInterval: defaultTaskCheckInterval,\n\t\t\tretryDelayFunc:    DefaultRetryDelayFunc,\n\t\t\tisFailureFunc:     defaultIsFailureFunc,\n\t\t\tsyncCh:            syncCh,\n\t\t\tcancelations:      base.NewCancelations(),\n\t\t\tconcurrency:       1, // Set concurrency to 1 to make sure tasks are processed one at a time.\n\t\t\tqueues:            queueCfg,\n\t\t\tstrictPriority:    true,\n\t\t\terrHandler:        nil,\n\t\t\tshutdownTimeout:   defaultShutdownTimeout,\n\t\t\tstarting:          starting,\n\t\t\tfinished:          finished,\n\t\t})\n\t\tp.handler = HandlerFunc(handler)\n\n\t\tp.start(&sync.WaitGroup{})\n\t\ttime.Sleep(tc.wait)\n\t\t// Make sure no tasks are stuck in active list.\n\t\tfor _, qname := range tc.queues {\n\t\t\tif l := r.LLen(context.Background(), base.ActiveKey(qname)).Val(); l != 0 {\n\t\t\t\tt.Errorf(\"%q has %d tasks, want 0\", base.ActiveKey(qname), l)\n\t\t\t}\n\t\t}\n\t\tp.shutdown()\n\n\t\tif diff := cmp.Diff(tc.wantProcessed, processed, taskCmpOpts...); diff != \"\" {\n\t\t\tt.Errorf(\"mismatch found in processed tasks; (-want, +got)\\n%s\", diff)\n\t\t}\n\n\t}\n}\n\nfunc TestProcessorPerform(t *testing.T) {\n\ttests := []struct {\n\t\tdesc    string\n\t\thandler HandlerFunc\n\t\ttask    *Task\n\t\twantErr bool\n\t}{\n\t\t{\n\t\t\tdesc: \"handler returns nil\",\n\t\t\thandler: func(ctx context.Context, t *Task) error {\n\t\t\t\treturn nil\n\t\t\t},\n\t\t\ttask:    NewTask(\"gen_thumbnail\", h.JSON(map[string]interface{}{\"src\": \"some/img/path\"})),\n\t\t\twantErr: false,\n\t\t},\n\t\t{\n\t\t\tdesc: \"handler returns error\",\n\t\t\thandler: func(ctx context.Context, t *Task) error {\n\t\t\t\treturn fmt.Errorf(\"something went wrong\")\n\t\t\t},\n\t\t\ttask:    NewTask(\"gen_thumbnail\", h.JSON(map[string]interface{}{\"src\": \"some/img/path\"})),\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tdesc: \"handler panics\",\n\t\t\thandler: func(ctx context.Context, t *Task) error {\n\t\t\t\tpanic(\"something went terribly wrong\")\n\t\t\t},\n\t\t\ttask:    NewTask(\"gen_thumbnail\", h.JSON(map[string]interface{}{\"src\": \"some/img/path\"})),\n\t\t\twantErr: true,\n\t\t},\n\t}\n\t// Note: We don't need to fully initialized the processor since we are only testing\n\t// perform method.\n\tp := newProcessorForTest(t, nil, nil)\n\n\tfor _, tc := range tests {\n\t\tp.handler = tc.handler\n\t\tgot := p.perform(context.Background(), tc.task)\n\t\tif !tc.wantErr && got != nil {\n\t\t\tt.Errorf(\"%s: perform() = %v, want nil\", tc.desc, got)\n\t\t\tcontinue\n\t\t}\n\t\tif tc.wantErr && got == nil {\n\t\t\tt.Errorf(\"%s: perform() = nil, want non-nil error\", tc.desc)\n\t\t\tcontinue\n\t\t}\n\t}\n}\n\nfunc TestGCD(t *testing.T) {\n\ttests := []struct {\n\t\tinput []int\n\t\twant  int\n\t}{\n\t\t{[]int{6, 2, 12}, 2},\n\t\t{[]int{3, 3, 3}, 3},\n\t\t{[]int{6, 3, 1}, 1},\n\t\t{[]int{1}, 1},\n\t\t{[]int{1, 0, 2}, 1},\n\t\t{[]int{8, 0, 4}, 4},\n\t\t{[]int{9, 12, 18, 30}, 3},\n\t}\n\n\tfor _, tc := range tests {\n\t\tgot := gcd(tc.input...)\n\t\tif got != tc.want {\n\t\t\tt.Errorf(\"gcd(%v) = %d, want %d\", tc.input, got, tc.want)\n\t\t}\n\t}\n}\n\nfunc TestNormalizeQueues(t *testing.T) {\n\ttests := []struct {\n\t\tinput map[string]int\n\t\twant  map[string]int\n\t}{\n\t\t{\n\t\t\tinput: map[string]int{\n\t\t\t\t\"high\":    100,\n\t\t\t\t\"default\": 20,\n\t\t\t\t\"low\":     5,\n\t\t\t},\n\t\t\twant: map[string]int{\n\t\t\t\t\"high\":    20,\n\t\t\t\t\"default\": 4,\n\t\t\t\t\"low\":     1,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: map[string]int{\n\t\t\t\t\"default\": 10,\n\t\t\t},\n\t\t\twant: map[string]int{\n\t\t\t\t\"default\": 1,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: map[string]int{\n\t\t\t\t\"critical\": 5,\n\t\t\t\t\"default\":  1,\n\t\t\t},\n\t\t\twant: map[string]int{\n\t\t\t\t\"critical\": 5,\n\t\t\t\t\"default\":  1,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: map[string]int{\n\t\t\t\t\"critical\": 6,\n\t\t\t\t\"default\":  3,\n\t\t\t\t\"low\":      0,\n\t\t\t},\n\t\t\twant: map[string]int{\n\t\t\t\t\"critical\": 2,\n\t\t\t\t\"default\":  1,\n\t\t\t\t\"low\":      0,\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\tgot := normalizeQueues(tc.input)\n\t\tif diff := cmp.Diff(tc.want, got); diff != \"\" {\n\t\t\tt.Errorf(\"normalizeQueues(%v) = %v, want %v; (-want, +got):\\n%s\",\n\t\t\t\ttc.input, got, tc.want, diff)\n\t\t}\n\t}\n}\n\nfunc TestProcessorComputeDeadline(t *testing.T) {\n\tnow := time.Now()\n\tp := processor{\n\t\tlogger: log.NewLogger(nil),\n\t\tclock:  timeutil.NewSimulatedClock(now),\n\t}\n\n\ttests := []struct {\n\t\tdesc string\n\t\tmsg  *base.TaskMessage\n\t\twant time.Time\n\t}{\n\t\t{\n\t\t\tdesc: \"message with only timeout specified\",\n\t\t\tmsg: &base.TaskMessage{\n\t\t\t\tTimeout: int64((30 * time.Minute).Seconds()),\n\t\t\t},\n\t\t\twant: now.Add(30 * time.Minute),\n\t\t},\n\t\t{\n\t\t\tdesc: \"message with only deadline specified\",\n\t\t\tmsg: &base.TaskMessage{\n\t\t\t\tDeadline: now.Add(24 * time.Hour).Unix(),\n\t\t\t},\n\t\t\twant: now.Add(24 * time.Hour),\n\t\t},\n\t\t{\n\t\t\tdesc: \"message with both timeout and deadline set (now+timeout < deadline)\",\n\t\t\tmsg: &base.TaskMessage{\n\t\t\t\tDeadline: now.Add(24 * time.Hour).Unix(),\n\t\t\t\tTimeout:  int64((30 * time.Minute).Seconds()),\n\t\t\t},\n\t\t\twant: now.Add(30 * time.Minute),\n\t\t},\n\t\t{\n\t\t\tdesc: \"message with both timeout and deadline set (now+timeout > deadline)\",\n\t\t\tmsg: &base.TaskMessage{\n\t\t\t\tDeadline: now.Add(10 * time.Minute).Unix(),\n\t\t\t\tTimeout:  int64((30 * time.Minute).Seconds()),\n\t\t\t},\n\t\t\twant: now.Add(10 * time.Minute),\n\t\t},\n\t\t{\n\t\t\tdesc: \"message with both timeout and deadline set (now+timeout == deadline)\",\n\t\t\tmsg: &base.TaskMessage{\n\t\t\t\tDeadline: now.Add(30 * time.Minute).Unix(),\n\t\t\t\tTimeout:  int64((30 * time.Minute).Seconds()),\n\t\t\t},\n\t\t\twant: now.Add(30 * time.Minute),\n\t\t},\n\t\t{\n\t\t\tdesc: \"message without timeout and deadline\",\n\t\t\tmsg:  &base.TaskMessage{},\n\t\t\twant: now.Add(defaultTimeout),\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\tgot := p.computeDeadline(tc.msg)\n\t\t// Compare the Unix epoch with seconds granularity\n\t\tif got.Unix() != tc.want.Unix() {\n\t\t\tt.Errorf(\"%s: got=%v, want=%v\", tc.desc, got.Unix(), tc.want.Unix())\n\t\t}\n\t}\n}\n\nfunc TestReturnPanicError(t *testing.T) {\n\n\ttask := NewTask(\"gen_thumbnail\", h.JSON(map[string]interface{}{\"src\": \"some/img/path\"}))\n\n\ttests := []struct {\n\t\tname         string\n\t\thandler      HandlerFunc\n\t\tIsPanicError bool\n\t}{\n\t\t{\n\t\t\tname: \"should return panic error when occurred panic recovery\",\n\t\t\thandler: func(ctx context.Context, t *Task) error {\n\t\t\t\tpanic(\"something went terribly wrong\")\n\t\t\t},\n\t\t\tIsPanicError: true,\n\t\t},\n\t\t{\n\t\t\tname: \"should return normal error when don't occur panic recovery\",\n\t\t\thandler: func(ctx context.Context, t *Task) error {\n\t\t\t\treturn fmt.Errorf(\"something went terribly wrong\")\n\t\t\t},\n\t\t\tIsPanicError: false,\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tp := processor{\n\t\t\t\tlogger:  log.NewLogger(nil),\n\t\t\t\thandler: tc.handler,\n\t\t\t}\n\t\t\tgot := p.perform(context.Background(), task)\n\t\t\tif tc.IsPanicError != IsPanicError(got) {\n\t\t\t\tt.Errorf(\"%s: got=%t, want=%t\", tc.name, IsPanicError(got), tc.IsPanicError)\n\t\t\t}\n\t\t\tif tc.IsPanicError && !strings.HasPrefix(got.Error(), \"panic error cause by:\") {\n\t\t\t\tt.Error(\"wrong text msg for panic error\")\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "recoverer.go",
          "type": "blob",
          "size": 3.3681640625,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/errors\"\n\t\"github.com/hibiken/asynq/internal/log\"\n)\n\ntype recoverer struct {\n\tlogger         *log.Logger\n\tbroker         base.Broker\n\tretryDelayFunc RetryDelayFunc\n\tisFailureFunc  func(error) bool\n\n\t// channel to communicate back to the long running \"recoverer\" goroutine.\n\tdone chan struct{}\n\n\t// list of queues to check for deadline.\n\tqueues []string\n\n\t// poll interval.\n\tinterval time.Duration\n}\n\ntype recovererParams struct {\n\tlogger         *log.Logger\n\tbroker         base.Broker\n\tqueues         []string\n\tinterval       time.Duration\n\tretryDelayFunc RetryDelayFunc\n\tisFailureFunc  func(error) bool\n}\n\nfunc newRecoverer(params recovererParams) *recoverer {\n\treturn &recoverer{\n\t\tlogger:         params.logger,\n\t\tbroker:         params.broker,\n\t\tdone:           make(chan struct{}),\n\t\tqueues:         params.queues,\n\t\tinterval:       params.interval,\n\t\tretryDelayFunc: params.retryDelayFunc,\n\t\tisFailureFunc:  params.isFailureFunc,\n\t}\n}\n\nfunc (r *recoverer) shutdown() {\n\tr.logger.Debug(\"Recoverer shutting down...\")\n\t// Signal the recoverer goroutine to stop polling.\n\tr.done <- struct{}{}\n}\n\nfunc (r *recoverer) start(wg *sync.WaitGroup) {\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tr.recover()\n\t\ttimer := time.NewTimer(r.interval)\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-r.done:\n\t\t\t\tr.logger.Debug(\"Recoverer done\")\n\t\t\t\ttimer.Stop()\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t\tr.recover()\n\t\t\t\ttimer.Reset(r.interval)\n\t\t\t}\n\t\t}\n\t}()\n}\n\n// ErrLeaseExpired error indicates that the task failed because the worker working on the task\n// could not extend its lease due to missing heartbeats. The worker may have crashed or got cutoff from the network.\nvar ErrLeaseExpired = errors.New(\"asynq: task lease expired\")\n\nfunc (r *recoverer) recover() {\n\tr.recoverLeaseExpiredTasks()\n\tr.recoverStaleAggregationSets()\n}\n\nfunc (r *recoverer) recoverLeaseExpiredTasks() {\n\t// Get all tasks which have expired 30 seconds ago or earlier to accommodate certain amount of clock skew.\n\tcutoff := time.Now().Add(-30 * time.Second)\n\tmsgs, err := r.broker.ListLeaseExpired(cutoff, r.queues...)\n\tif err != nil {\n\t\tr.logger.Warnf(\"recoverer: could not list lease expired tasks: %v\", err)\n\t\treturn\n\t}\n\tfor _, msg := range msgs {\n\t\tif msg.Retried >= msg.Retry {\n\t\t\tr.archive(msg, ErrLeaseExpired)\n\t\t} else {\n\t\t\tr.retry(msg, ErrLeaseExpired)\n\t\t}\n\t}\n}\n\nfunc (r *recoverer) recoverStaleAggregationSets() {\n\tfor _, qname := range r.queues {\n\t\tif err := r.broker.ReclaimStaleAggregationSets(qname); err != nil {\n\t\t\tr.logger.Warnf(\"recoverer: could not reclaim stale aggregation sets in queue %q: %v\", qname, err)\n\t\t}\n\t}\n}\n\nfunc (r *recoverer) retry(msg *base.TaskMessage, err error) {\n\tdelay := r.retryDelayFunc(msg.Retried, err, NewTask(msg.Type, msg.Payload))\n\tretryAt := time.Now().Add(delay)\n\tif err := r.broker.Retry(context.Background(), msg, retryAt, err.Error(), r.isFailureFunc(err)); err != nil {\n\t\tr.logger.Warnf(\"recoverer: could not retry lease expired task: %v\", err)\n\t}\n}\n\nfunc (r *recoverer) archive(msg *base.TaskMessage, err error) {\n\tif err := r.broker.Archive(context.Background(), msg, err.Error()); err != nil {\n\t\tr.logger.Warnf(\"recoverer: could not move task to archive: %v\", err)\n\t}\n}\n"
        },
        {
          "name": "recoverer_test.go",
          "type": "blob",
          "size": 7.4892578125,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/rdb\"\n\th \"github.com/hibiken/asynq/internal/testutil\"\n)\n\nfunc TestRecoverer(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\trdbClient := rdb.NewRDB(r)\n\n\tt1 := h.NewTaskMessageWithQueue(\"task1\", nil, \"default\")\n\tt2 := h.NewTaskMessageWithQueue(\"task2\", nil, \"default\")\n\tt3 := h.NewTaskMessageWithQueue(\"task3\", nil, \"critical\")\n\tt4 := h.NewTaskMessageWithQueue(\"task4\", nil, \"default\")\n\tt4.Retried = t4.Retry // t4 has reached its max retry count\n\n\tnow := time.Now()\n\n\ttests := []struct {\n\t\tdesc         string\n\t\tactive       map[string][]*base.TaskMessage\n\t\tlease        map[string][]base.Z\n\t\tretry        map[string][]base.Z\n\t\tarchived     map[string][]base.Z\n\t\twantActive   map[string][]*base.TaskMessage\n\t\twantLease    map[string][]base.Z\n\t\twantRetry    map[string][]*base.TaskMessage\n\t\twantArchived map[string][]*base.TaskMessage\n\t}{\n\t\t{\n\t\t\tdesc: \"with one active task\",\n\t\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {t1},\n\t\t\t},\n\t\t\tlease: map[string][]base.Z{\n\t\t\t\t\"default\": {{Message: t1, Score: now.Add(-1 * time.Minute).Unix()}},\n\t\t\t},\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\twantActive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\twantLease: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t\twantRetry: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {t1},\n\t\t\t},\n\t\t\twantArchived: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\": {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"with a task with max-retry reached\",\n\t\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {t4},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\tlease: map[string][]base.Z{\n\t\t\t\t\"default\":  {{Message: t4, Score: now.Add(-40 * time.Second).Unix()}},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\twantActive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\twantLease: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\twantRetry: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\twantArchived: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {t4},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"with multiple active tasks, and one expired\",\n\t\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {t1, t2},\n\t\t\t\t\"critical\": {t3},\n\t\t\t},\n\t\t\tlease: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{Message: t1, Score: now.Add(-2 * time.Minute).Unix()},\n\t\t\t\t\t{Message: t2, Score: now.Add(20 * time.Second).Unix()},\n\t\t\t\t},\n\t\t\t\t\"critical\": {\n\t\t\t\t\t{Message: t3, Score: now.Add(20 * time.Second).Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\twantActive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {t2},\n\t\t\t\t\"critical\": {t3},\n\t\t\t},\n\t\t\twantLease: map[string][]base.Z{\n\t\t\t\t\"default\":  {{Message: t2, Score: now.Add(20 * time.Second).Unix()}},\n\t\t\t\t\"critical\": {{Message: t3, Score: now.Add(20 * time.Second).Unix()}},\n\t\t\t},\n\t\t\twantRetry: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {t1},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\twantArchived: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"with multiple expired active tasks\",\n\t\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {t1, t2},\n\t\t\t\t\"critical\": {t3},\n\t\t\t},\n\t\t\tlease: map[string][]base.Z{\n\t\t\t\t\"default\": {\n\t\t\t\t\t{Message: t1, Score: now.Add(-1 * time.Minute).Unix()},\n\t\t\t\t\t{Message: t2, Score: now.Add(10 * time.Second).Unix()},\n\t\t\t\t},\n\t\t\t\t\"critical\": {\n\t\t\t\t\t{Message: t3, Score: now.Add(-1 * time.Minute).Unix()},\n\t\t\t\t},\n\t\t\t},\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"cricial\": {},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\": {},\n\t\t\t\t\"cricial\": {},\n\t\t\t},\n\t\t\twantActive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {t2},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\twantLease: map[string][]base.Z{\n\t\t\t\t\"default\": {{Message: t2, Score: now.Add(10 * time.Second).Unix()}},\n\t\t\t},\n\t\t\twantRetry: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {t1},\n\t\t\t\t\"critical\": {t3},\n\t\t\t},\n\t\t\twantArchived: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"with empty active queue\",\n\t\t\tactive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\tlease: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\tretry: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\tarchived: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\twantActive: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\twantLease: map[string][]base.Z{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\twantRetry: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t\twantArchived: map[string][]*base.TaskMessage{\n\t\t\t\t\"default\":  {},\n\t\t\t\t\"critical\": {},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\th.FlushDB(t, r)\n\t\th.SeedAllActiveQueues(t, r, tc.active)\n\t\th.SeedAllLease(t, r, tc.lease)\n\t\th.SeedAllRetryQueues(t, r, tc.retry)\n\t\th.SeedAllArchivedQueues(t, r, tc.archived)\n\n\t\trecoverer := newRecoverer(recovererParams{\n\t\t\tlogger:         testLogger,\n\t\t\tbroker:         rdbClient,\n\t\t\tqueues:         []string{\"default\", \"critical\"},\n\t\t\tinterval:       1 * time.Second,\n\t\t\tretryDelayFunc: func(n int, err error, task *Task) time.Duration { return 30 * time.Second },\n\t\t\tisFailureFunc:  defaultIsFailureFunc,\n\t\t})\n\n\t\tvar wg sync.WaitGroup\n\t\trecoverer.start(&wg)\n\t\trunTime := time.Now() // time when recoverer is running\n\t\ttime.Sleep(2 * time.Second)\n\t\trecoverer.shutdown()\n\n\t\tfor qname, want := range tc.wantActive {\n\t\t\tgotActive := h.GetActiveMessages(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotActive, h.SortMsgOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"%s; mismatch found in %q; (-want,+got)\\n%s\", tc.desc, base.ActiveKey(qname), diff)\n\t\t\t}\n\t\t}\n\t\tfor qname, want := range tc.wantLease {\n\t\t\tgotLease := h.GetLeaseEntries(t, r, qname)\n\t\t\tif diff := cmp.Diff(want, gotLease, h.SortZSetEntryOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"%s; mismatch found in %q; (-want,+got)\\n%s\", tc.desc, base.LeaseKey(qname), diff)\n\t\t\t}\n\t\t}\n\t\tcmpOpt := h.EquateInt64Approx(2) // allow up to two-second difference in `LastFailedAt`\n\t\tfor qname, msgs := range tc.wantRetry {\n\t\t\tgotRetry := h.GetRetryMessages(t, r, qname)\n\t\t\tvar wantRetry []*base.TaskMessage // Note: construct message here since `LastFailedAt` is relative to each test run\n\t\t\tfor _, msg := range msgs {\n\t\t\t\twantRetry = append(wantRetry, h.TaskMessageAfterRetry(*msg, ErrLeaseExpired.Error(), runTime))\n\t\t\t}\n\t\t\tif diff := cmp.Diff(wantRetry, gotRetry, h.SortMsgOpt, cmpOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"%s; mismatch found in %q: (-want, +got)\\n%s\", tc.desc, base.RetryKey(qname), diff)\n\t\t\t}\n\t\t}\n\t\tfor qname, msgs := range tc.wantArchived {\n\t\t\tgotArchived := h.GetArchivedMessages(t, r, qname)\n\t\t\tvar wantArchived []*base.TaskMessage\n\t\t\tfor _, msg := range msgs {\n\t\t\t\twantArchived = append(wantArchived, h.TaskMessageWithError(*msg, ErrLeaseExpired.Error(), runTime))\n\t\t\t}\n\t\t\tif diff := cmp.Diff(wantArchived, gotArchived, h.SortMsgOpt, cmpOpt); diff != \"\" {\n\t\t\t\tt.Errorf(\"%s; mismatch found in %q: (-want, +got)\\n%s\", tc.desc, base.ArchivedKey(qname), diff)\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "scheduler.go",
          "type": "blob",
          "size": 10.384765625,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/log\"\n\t\"github.com/hibiken/asynq/internal/rdb\"\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// A Scheduler kicks off tasks at regular intervals based on the user defined schedule.\n//\n// Schedulers are safe for concurrent use by multiple goroutines.\ntype Scheduler struct {\n\tid string\n\n\tstate *serverState\n\n\theartbeatInterval time.Duration\n\tlogger            *log.Logger\n\tclient            *Client\n\trdb               *rdb.RDB\n\tcron              *cron.Cron\n\tlocation          *time.Location\n\tdone              chan struct{}\n\twg                sync.WaitGroup\n\tpreEnqueueFunc    func(task *Task, opts []Option)\n\tpostEnqueueFunc   func(info *TaskInfo, err error)\n\terrHandler        func(task *Task, opts []Option, err error)\n\n\t// guards idmap\n\tmu sync.Mutex\n\t// idmap maps Scheduler's entry ID to cron.EntryID\n\t// to avoid using cron.EntryID as the public API of\n\t// the Scheduler.\n\tidmap map[string]cron.EntryID\n}\n\nconst defaultHeartbeatInterval = 10 * time.Second\n\n// NewScheduler returns a new Scheduler instance given the redis connection option.\n// The parameter opts is optional, defaults will be used if opts is set to nil\nfunc NewScheduler(r RedisConnOpt, opts *SchedulerOpts) *Scheduler {\n\tscheduler := newScheduler(opts)\n\n\tredisClient, ok := r.MakeRedisClient().(redis.UniversalClient)\n\tif !ok {\n\t\tpanic(fmt.Sprintf(\"asynq: unsupported RedisConnOpt type %T\", r))\n\t}\n\n\trdb := rdb.NewRDB(redisClient)\n\n\tscheduler.rdb = rdb\n\tscheduler.client = &Client{broker: rdb, sharedConnection: false}\n\n\treturn scheduler\n}\n\n// NewSchedulerFromRedisClient returns a new instance of Scheduler given a redis.UniversalClient\n// The parameter opts is optional, defaults will be used if opts is set to nil.\n// Warning: The underlying redis connection pool will not be closed by Asynq, you are responsible for closing it.\nfunc NewSchedulerFromRedisClient(c redis.UniversalClient, opts *SchedulerOpts) *Scheduler {\n\tscheduler := newScheduler(opts)\n\n\tscheduler.rdb = rdb.NewRDB(c)\n\tscheduler.client = NewClientFromRedisClient(c)\n\n\treturn scheduler\n}\n\nfunc newScheduler(opts *SchedulerOpts) *Scheduler {\n\tif opts == nil {\n\t\topts = &SchedulerOpts{}\n\t}\n\n\theartbeatInterval := opts.HeartbeatInterval\n\tif heartbeatInterval <= 0 {\n\t\theartbeatInterval = defaultHeartbeatInterval\n\t}\n\n\tlogger := log.NewLogger(opts.Logger)\n\tloglevel := opts.LogLevel\n\tif loglevel == level_unspecified {\n\t\tloglevel = InfoLevel\n\t}\n\tlogger.SetLevel(toInternalLogLevel(loglevel))\n\n\tloc := opts.Location\n\tif loc == nil {\n\t\tloc = time.UTC\n\t}\n\n\treturn &Scheduler{\n\t\tid:                generateSchedulerID(),\n\t\tstate:             &serverState{value: srvStateNew},\n\t\theartbeatInterval: heartbeatInterval,\n\t\tlogger:            logger,\n\t\tcron:              cron.New(cron.WithLocation(loc)),\n\t\tlocation:          loc,\n\t\tdone:              make(chan struct{}),\n\t\tpreEnqueueFunc:    opts.PreEnqueueFunc,\n\t\tpostEnqueueFunc:   opts.PostEnqueueFunc,\n\t\terrHandler:        opts.EnqueueErrorHandler,\n\t\tidmap:             make(map[string]cron.EntryID),\n\t}\n}\n\nfunc generateSchedulerID() string {\n\thost, err := os.Hostname()\n\tif err != nil {\n\t\thost = \"unknown-host\"\n\t}\n\treturn fmt.Sprintf(\"%s:%d:%v\", host, os.Getpid(), uuid.New())\n}\n\n// SchedulerOpts specifies scheduler options.\ntype SchedulerOpts struct {\n\t// HeartbeatInterval specifies the interval between scheduler heartbeats.\n\t//\n\t// If unset, zero or a negative value, the interval is set to 10 second.\n\t//\n\t// Note: Setting this value too low may add significant load to redis.\n\t//\n\t// By default, HeartbeatInterval is set to 10 seconds.\n\tHeartbeatInterval time.Duration\n\n\t// Logger specifies the logger used by the scheduler instance.\n\t//\n\t// If unset, the default logger is used.\n\tLogger Logger\n\n\t// LogLevel specifies the minimum log level to enable.\n\t//\n\t// If unset, InfoLevel is used by default.\n\tLogLevel LogLevel\n\n\t// Location specifies the time zone location.\n\t//\n\t// If unset, the UTC time zone (time.UTC) is used.\n\tLocation *time.Location\n\n\t// PreEnqueueFunc, if provided, is called before a task gets enqueued by Scheduler.\n\t// The callback function should return quickly to not block the current thread.\n\tPreEnqueueFunc func(task *Task, opts []Option)\n\n\t// PostEnqueueFunc, if provided, is called after a task gets enqueued by Scheduler.\n\t// The callback function should return quickly to not block the current thread.\n\tPostEnqueueFunc func(info *TaskInfo, err error)\n\n\t// Deprecated: Use PostEnqueueFunc instead\n\t// EnqueueErrorHandler gets called when scheduler cannot enqueue a registered task\n\t// due to an error.\n\tEnqueueErrorHandler func(task *Task, opts []Option, err error)\n}\n\n// enqueueJob encapsulates the job of enqueuing a task and recording the event.\ntype enqueueJob struct {\n\tid              uuid.UUID\n\tcronspec        string\n\ttask            *Task\n\topts            []Option\n\tlocation        *time.Location\n\tlogger          *log.Logger\n\tclient          *Client\n\trdb             *rdb.RDB\n\tpreEnqueueFunc  func(task *Task, opts []Option)\n\tpostEnqueueFunc func(info *TaskInfo, err error)\n\terrHandler      func(task *Task, opts []Option, err error)\n}\n\nfunc (j *enqueueJob) Run() {\n\tif j.preEnqueueFunc != nil {\n\t\tj.preEnqueueFunc(j.task, j.opts)\n\t}\n\tinfo, err := j.client.Enqueue(j.task, j.opts...)\n\tif j.postEnqueueFunc != nil {\n\t\tj.postEnqueueFunc(info, err)\n\t}\n\tif err != nil {\n\t\tif j.errHandler != nil {\n\t\t\tj.errHandler(j.task, j.opts, err)\n\t\t}\n\t\treturn\n\t}\n\tj.logger.Debugf(\"scheduler enqueued a task: %+v\", info)\n\tevent := &base.SchedulerEnqueueEvent{\n\t\tTaskID:     info.ID,\n\t\tEnqueuedAt: time.Now().In(j.location),\n\t}\n\terr = j.rdb.RecordSchedulerEnqueueEvent(j.id.String(), event)\n\tif err != nil {\n\t\tj.logger.Warnf(\"scheduler could not record enqueue event of enqueued task %s: %v\", info.ID, err)\n\t}\n}\n\n// Register registers a task to be enqueued on the given schedule specified by the cronspec.\n// It returns an ID of the newly registered entry.\nfunc (s *Scheduler) Register(cronspec string, task *Task, opts ...Option) (entryID string, err error) {\n\tjob := &enqueueJob{\n\t\tid:              uuid.New(),\n\t\tcronspec:        cronspec,\n\t\ttask:            task,\n\t\topts:            opts,\n\t\tlocation:        s.location,\n\t\tclient:          s.client,\n\t\trdb:             s.rdb,\n\t\tlogger:          s.logger,\n\t\tpreEnqueueFunc:  s.preEnqueueFunc,\n\t\tpostEnqueueFunc: s.postEnqueueFunc,\n\t\terrHandler:      s.errHandler,\n\t}\n\tcronID, err := s.cron.AddJob(cronspec, job)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\ts.mu.Lock()\n\ts.idmap[job.id.String()] = cronID\n\ts.mu.Unlock()\n\treturn job.id.String(), nil\n}\n\n// Unregister removes a registered entry by entry ID.\n// Unregister returns a non-nil error if no entries were found for the given entryID.\nfunc (s *Scheduler) Unregister(entryID string) error {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tcronID, ok := s.idmap[entryID]\n\tif !ok {\n\t\treturn fmt.Errorf(\"asynq: no scheduler entry found\")\n\t}\n\tdelete(s.idmap, entryID)\n\ts.cron.Remove(cronID)\n\treturn nil\n}\n\n// Run starts the scheduler until an os signal to exit the program is received.\n// It returns an error if scheduler is already running or has been shutdown.\nfunc (s *Scheduler) Run() error {\n\tif err := s.Start(); err != nil {\n\t\treturn err\n\t}\n\ts.waitForSignals()\n\ts.Shutdown()\n\treturn nil\n}\n\n// Start starts the scheduler.\n// It returns an error if the scheduler is already running or has been shutdown.\nfunc (s *Scheduler) Start() error {\n\tif err := s.start(); err != nil {\n\t\treturn err\n\t}\n\ts.logger.Info(\"Scheduler starting\")\n\ts.logger.Infof(\"Scheduler timezone is set to %v\", s.location)\n\ts.cron.Start()\n\ts.wg.Add(1)\n\tgo s.runHeartbeater()\n\treturn nil\n}\n\n// Checks server state and returns an error if pre-condition is not met.\n// Otherwise it sets the server state to active.\nfunc (s *Scheduler) start() error {\n\ts.state.mu.Lock()\n\tdefer s.state.mu.Unlock()\n\tswitch s.state.value {\n\tcase srvStateActive:\n\t\treturn fmt.Errorf(\"asynq: the scheduler is already running\")\n\tcase srvStateClosed:\n\t\treturn fmt.Errorf(\"asynq: the scheduler has already been stopped\")\n\t}\n\ts.state.value = srvStateActive\n\treturn nil\n}\n\n// Shutdown stops and shuts down the scheduler.\nfunc (s *Scheduler) Shutdown() {\n\ts.state.mu.Lock()\n\tif s.state.value == srvStateNew || s.state.value == srvStateClosed {\n\t\t// scheduler is not running, do nothing and return.\n\t\ts.state.mu.Unlock()\n\t\treturn\n\t}\n\ts.state.value = srvStateClosed\n\ts.state.mu.Unlock()\n\n\ts.logger.Info(\"Scheduler shutting down\")\n\tclose(s.done) // signal heartbeater to stop\n\tctx := s.cron.Stop()\n\t<-ctx.Done()\n\ts.wg.Wait()\n\n\ts.clearHistory()\n\tif err := s.client.Close(); err != nil {\n\t\ts.logger.Errorf(\"Failed to close redis client connection: %v\", err)\n\t}\n\ts.logger.Info(\"Scheduler stopped\")\n}\n\nfunc (s *Scheduler) runHeartbeater() {\n\tdefer s.wg.Done()\n\tticker := time.NewTicker(s.heartbeatInterval)\n\tfor {\n\t\tselect {\n\t\tcase <-s.done:\n\t\t\ts.logger.Debugf(\"Scheduler heatbeater shutting down\")\n\t\t\tif err := s.rdb.ClearSchedulerEntries(s.id); err != nil {\n\t\t\t\ts.logger.Errorf(\"Failed to clear the scheduler entries: %v\", err)\n\t\t\t}\n\t\t\tticker.Stop()\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\ts.beat()\n\t\t}\n\t}\n}\n\n// beat writes a snapshot of entries to redis.\nfunc (s *Scheduler) beat() {\n\tvar entries []*base.SchedulerEntry\n\tfor _, entry := range s.cron.Entries() {\n\t\tjob := entry.Job.(*enqueueJob)\n\t\te := &base.SchedulerEntry{\n\t\t\tID:      job.id.String(),\n\t\t\tSpec:    job.cronspec,\n\t\t\tType:    job.task.Type(),\n\t\t\tPayload: job.task.Payload(),\n\t\t\tOpts:    stringifyOptions(job.opts),\n\t\t\tNext:    entry.Next,\n\t\t\tPrev:    entry.Prev,\n\t\t}\n\t\tentries = append(entries, e)\n\t}\n\tif err := s.rdb.WriteSchedulerEntries(s.id, entries, s.heartbeatInterval*2); err != nil {\n\t\ts.logger.Warnf(\"Scheduler could not write heartbeat data: %v\", err)\n\t}\n}\n\nfunc stringifyOptions(opts []Option) []string {\n\tvar res []string\n\tfor _, opt := range opts {\n\t\tres = append(res, opt.String())\n\t}\n\treturn res\n}\n\nfunc (s *Scheduler) clearHistory() {\n\tfor _, entry := range s.cron.Entries() {\n\t\tjob := entry.Job.(*enqueueJob)\n\t\tif err := s.rdb.ClearSchedulerHistory(job.id.String()); err != nil {\n\t\t\ts.logger.Warnf(\"Could not clear scheduler history for entry %q: %v\", job.id.String(), err)\n\t\t}\n\t}\n}\n\n// Ping performs a ping against the redis connection.\nfunc (s *Scheduler) Ping() error {\n\ts.state.mu.Lock()\n\tdefer s.state.mu.Unlock()\n\tif s.state.value == srvStateClosed {\n\t\treturn nil\n\t}\n\n\treturn s.rdb.Ping()\n}\n"
        },
        {
          "name": "scheduler_test.go",
          "type": "blob",
          "size": 5.1435546875,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/redis/go-redis/v9\"\n\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/testutil\"\n)\n\nfunc TestSchedulerRegister(t *testing.T) {\n\ttests := []struct {\n\t\tcronspec string\n\t\ttask     *Task\n\t\topts     []Option\n\t\twait     time.Duration\n\t\tqueue    string\n\t\twant     []*base.TaskMessage\n\t}{\n\t\t{\n\t\t\tcronspec: \"@every 3s\",\n\t\t\ttask:     NewTask(\"task1\", nil),\n\t\t\topts:     []Option{MaxRetry(10)},\n\t\t\twait:     10 * time.Second,\n\t\t\tqueue:    \"default\",\n\t\t\twant: []*base.TaskMessage{\n\t\t\t\t{\n\t\t\t\t\tType:    \"task1\",\n\t\t\t\t\tPayload: nil,\n\t\t\t\t\tRetry:   10,\n\t\t\t\t\tTimeout: int64(defaultTimeout.Seconds()),\n\t\t\t\t\tQueue:   \"default\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tType:    \"task1\",\n\t\t\t\t\tPayload: nil,\n\t\t\t\t\tRetry:   10,\n\t\t\t\t\tTimeout: int64(defaultTimeout.Seconds()),\n\t\t\t\t\tQueue:   \"default\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tType:    \"task1\",\n\t\t\t\t\tPayload: nil,\n\t\t\t\t\tRetry:   10,\n\t\t\t\t\tTimeout: int64(defaultTimeout.Seconds()),\n\t\t\t\t\tQueue:   \"default\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tr := setup(t)\n\n\t// Tests for new redis connection.\n\tfor _, tc := range tests {\n\t\tscheduler := NewScheduler(getRedisConnOpt(t), nil)\n\t\tif _, err := scheduler.Register(tc.cronspec, tc.task, tc.opts...); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif err := scheduler.Start(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\ttime.Sleep(tc.wait)\n\t\tscheduler.Shutdown()\n\n\t\tgot := testutil.GetPendingMessages(t, r, tc.queue)\n\t\tif diff := cmp.Diff(tc.want, got, testutil.IgnoreIDOpt); diff != \"\" {\n\t\t\tt.Errorf(\"mismatch found in queue %q: (-want,+got)\\n%s\", tc.queue, diff)\n\t\t}\n\t}\n\n\tr = setup(t)\n\n\t// Tests for existing redis connection.\n\tfor _, tc := range tests {\n\t\tredisClient := getRedisConnOpt(t).MakeRedisClient().(redis.UniversalClient)\n\t\tscheduler := NewSchedulerFromRedisClient(redisClient, nil)\n\t\tif _, err := scheduler.Register(tc.cronspec, tc.task, tc.opts...); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif err := scheduler.Start(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\ttime.Sleep(tc.wait)\n\t\tscheduler.Shutdown()\n\n\t\tgot := testutil.GetPendingMessages(t, r, tc.queue)\n\t\tif diff := cmp.Diff(tc.want, got, testutil.IgnoreIDOpt); diff != \"\" {\n\t\t\tt.Errorf(\"mismatch found in queue %q: (-want,+got)\\n%s\", tc.queue, diff)\n\t\t}\n\t}\n}\n\nfunc TestSchedulerWhenRedisDown(t *testing.T) {\n\tvar (\n\t\tmu      sync.Mutex\n\t\tcounter int\n\t)\n\terrorHandler := func(task *Task, opts []Option, err error) {\n\t\tmu.Lock()\n\t\tcounter++\n\t\tmu.Unlock()\n\t}\n\n\t// Connect to non-existent redis instance to simulate a redis server being down.\n\tscheduler := NewScheduler(\n\t\tRedisClientOpt{Addr: \":9876\"}, // no Redis listening to this port.\n\t\t&SchedulerOpts{EnqueueErrorHandler: errorHandler},\n\t)\n\n\ttask := NewTask(\"test\", nil)\n\n\tif _, err := scheduler.Register(\"@every 3s\", task); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := scheduler.Start(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// Scheduler should attempt to enqueue the task three times (every 3s).\n\ttime.Sleep(10 * time.Second)\n\tscheduler.Shutdown()\n\n\tmu.Lock()\n\tif counter != 3 {\n\t\tt.Errorf(\"EnqueueErrorHandler was called %d times, want 3\", counter)\n\t}\n\tmu.Unlock()\n}\n\nfunc TestSchedulerUnregister(t *testing.T) {\n\ttests := []struct {\n\t\tcronspec string\n\t\ttask     *Task\n\t\topts     []Option\n\t\twait     time.Duration\n\t\tqueue    string\n\t}{\n\t\t{\n\t\t\tcronspec: \"@every 3s\",\n\t\t\ttask:     NewTask(\"task1\", nil),\n\t\t\topts:     []Option{MaxRetry(10)},\n\t\t\twait:     10 * time.Second,\n\t\t\tqueue:    \"default\",\n\t\t},\n\t}\n\n\tr := setup(t)\n\n\tfor _, tc := range tests {\n\t\tscheduler := NewScheduler(getRedisConnOpt(t), nil)\n\t\tentryID, err := scheduler.Register(tc.cronspec, tc.task, tc.opts...)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif err := scheduler.Unregister(entryID); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif err := scheduler.Start(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\ttime.Sleep(tc.wait)\n\t\tscheduler.Shutdown()\n\n\t\tgot := testutil.GetPendingMessages(t, r, tc.queue)\n\t\tif len(got) != 0 {\n\t\t\tt.Errorf(\"%d tasks were enqueued, want zero\", len(got))\n\t\t}\n\t}\n}\n\nfunc TestSchedulerPostAndPreEnqueueHandler(t *testing.T) {\n\tvar (\n\t\tpreMu       sync.Mutex\n\t\tpreCounter  int\n\t\tpostMu      sync.Mutex\n\t\tpostCounter int\n\t)\n\tpreHandler := func(task *Task, opts []Option) {\n\t\tpreMu.Lock()\n\t\tpreCounter++\n\t\tpreMu.Unlock()\n\t}\n\tpostHandler := func(info *TaskInfo, err error) {\n\t\tpostMu.Lock()\n\t\tpostCounter++\n\t\tpostMu.Unlock()\n\t}\n\n\t// Connect to non-existent redis instance to simulate a redis server being down.\n\tscheduler := NewScheduler(\n\t\tgetRedisConnOpt(t),\n\t\t&SchedulerOpts{\n\t\t\tPreEnqueueFunc:  preHandler,\n\t\t\tPostEnqueueFunc: postHandler,\n\t\t},\n\t)\n\n\ttask := NewTask(\"test\", nil)\n\n\tif _, err := scheduler.Register(\"@every 3s\", task); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := scheduler.Start(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// Scheduler should attempt to enqueue the task three times (every 3s).\n\ttime.Sleep(10 * time.Second)\n\tscheduler.Shutdown()\n\n\tpreMu.Lock()\n\tif preCounter != 3 {\n\t\tt.Errorf(\"PreEnqueueFunc was called %d times, want 3\", preCounter)\n\t}\n\tpreMu.Unlock()\n\n\tpostMu.Lock()\n\tif postCounter != 3 {\n\t\tt.Errorf(\"PostEnqueueFunc was called %d times, want 3\", postCounter)\n\t}\n\tpostMu.Unlock()\n}\n"
        },
        {
          "name": "servemux.go",
          "type": "blob",
          "size": 4.53515625,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n)\n\n// ServeMux is a multiplexer for asynchronous tasks.\n// It matches the type of each task against a list of registered patterns\n// and calls the handler for the pattern that most closely matches the\n// task's type name.\n//\n// Longer patterns take precedence over shorter ones, so that if there are\n// handlers registered for both \"images\" and \"images:thumbnails\",\n// the latter handler will be called for tasks with a type name beginning with\n// \"images:thumbnails\" and the former will receive tasks with type name beginning\n// with \"images\".\ntype ServeMux struct {\n\tmu  sync.RWMutex\n\tm   map[string]muxEntry\n\tes  []muxEntry // slice of entries sorted from longest to shortest.\n\tmws []MiddlewareFunc\n}\n\ntype muxEntry struct {\n\th       Handler\n\tpattern string\n}\n\n// MiddlewareFunc is a function which receives an asynq.Handler and returns another asynq.Handler.\n// Typically, the returned handler is a closure which does something with the context and task passed\n// to it, and then calls the handler passed as parameter to the MiddlewareFunc.\ntype MiddlewareFunc func(Handler) Handler\n\n// NewServeMux allocates and returns a new ServeMux.\nfunc NewServeMux() *ServeMux {\n\treturn new(ServeMux)\n}\n\n// ProcessTask dispatches the task to the handler whose\n// pattern most closely matches the task type.\nfunc (mux *ServeMux) ProcessTask(ctx context.Context, task *Task) error {\n\th, _ := mux.Handler(task)\n\treturn h.ProcessTask(ctx, task)\n}\n\n// Handler returns the handler to use for the given task.\n// It always return a non-nil handler.\n//\n// Handler also returns the registered pattern that matches the task.\n//\n// If there is no registered handler that applies to the task,\n// handler returns a 'not found' handler which returns an error.\nfunc (mux *ServeMux) Handler(t *Task) (h Handler, pattern string) {\n\tmux.mu.RLock()\n\tdefer mux.mu.RUnlock()\n\n\th, pattern = mux.match(t.Type())\n\tif h == nil {\n\t\th, pattern = NotFoundHandler(), \"\"\n\t}\n\tfor i := len(mux.mws) - 1; i >= 0; i-- {\n\t\th = mux.mws[i](h)\n\t}\n\treturn h, pattern\n}\n\n// Find a handler on a handler map given a typename string.\n// Most-specific (longest) pattern wins.\nfunc (mux *ServeMux) match(typename string) (h Handler, pattern string) {\n\t// Check for exact match first.\n\tv, ok := mux.m[typename]\n\tif ok {\n\t\treturn v.h, v.pattern\n\t}\n\n\t// Check for longest valid match.\n\t// mux.es contains all patterns from longest to shortest.\n\tfor _, e := range mux.es {\n\t\tif strings.HasPrefix(typename, e.pattern) {\n\t\t\treturn e.h, e.pattern\n\t\t}\n\t}\n\treturn nil, \"\"\n\n}\n\n// Handle registers the handler for the given pattern.\n// If a handler already exists for pattern, Handle panics.\nfunc (mux *ServeMux) Handle(pattern string, handler Handler) {\n\tmux.mu.Lock()\n\tdefer mux.mu.Unlock()\n\n\tif strings.TrimSpace(pattern) == \"\" {\n\t\tpanic(\"asynq: invalid pattern\")\n\t}\n\tif handler == nil {\n\t\tpanic(\"asynq: nil handler\")\n\t}\n\tif _, exist := mux.m[pattern]; exist {\n\t\tpanic(\"asynq: multiple registrations for \" + pattern)\n\t}\n\n\tif mux.m == nil {\n\t\tmux.m = make(map[string]muxEntry)\n\t}\n\te := muxEntry{h: handler, pattern: pattern}\n\tmux.m[pattern] = e\n\tmux.es = appendSorted(mux.es, e)\n}\n\nfunc appendSorted(es []muxEntry, e muxEntry) []muxEntry {\n\tn := len(es)\n\ti := sort.Search(n, func(i int) bool {\n\t\treturn len(es[i].pattern) < len(e.pattern)\n\t})\n\tif i == n {\n\t\treturn append(es, e)\n\t}\n\t// we now know that i points at where we want to insert.\n\tes = append(es, muxEntry{}) // try to grow the slice in place, any entry works.\n\tcopy(es[i+1:], es[i:])      // shift shorter entries down.\n\tes[i] = e\n\treturn es\n}\n\n// HandleFunc registers the handler function for the given pattern.\nfunc (mux *ServeMux) HandleFunc(pattern string, handler func(context.Context, *Task) error) {\n\tif handler == nil {\n\t\tpanic(\"asynq: nil handler\")\n\t}\n\tmux.Handle(pattern, HandlerFunc(handler))\n}\n\n// Use appends a MiddlewareFunc to the chain.\n// Middlewares are executed in the order that they are applied to the ServeMux.\nfunc (mux *ServeMux) Use(mws ...MiddlewareFunc) {\n\tmux.mu.Lock()\n\tdefer mux.mu.Unlock()\n\tmux.mws = append(mux.mws, mws...)\n}\n\n// NotFound returns an error indicating that the handler was not found for the given task.\nfunc NotFound(ctx context.Context, task *Task) error {\n\treturn fmt.Errorf(\"handler not found for task %q\", task.Type())\n}\n\n// NotFoundHandler returns a simple task handler that returns a ``not found`` error.\nfunc NotFoundHandler() Handler { return HandlerFunc(NotFound) }\n"
        },
        {
          "name": "servemux_test.go",
          "type": "blob",
          "size": 4.5234375,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\t\"github.com/google/go-cmp/cmp\"\n)\n\nvar called string    // identity of the handler that was called.\nvar invoked []string // list of middlewares in the order they were invoked.\n\n// makeFakeHandler returns a handler that updates the global called variable\n// to the given identity.\nfunc makeFakeHandler(identity string) Handler {\n\treturn HandlerFunc(func(ctx context.Context, t *Task) error {\n\t\tcalled = identity\n\t\treturn nil\n\t})\n}\n\n// makeFakeMiddleware returns a middleware function that appends the given identity\n//to the global invoked slice.\nfunc makeFakeMiddleware(identity string) MiddlewareFunc {\n\treturn func(next Handler) Handler {\n\t\treturn HandlerFunc(func(ctx context.Context, t *Task) error {\n\t\t\tinvoked = append(invoked, identity)\n\t\t\treturn next.ProcessTask(ctx, t)\n\t\t})\n\t}\n}\n\n// A list of pattern, handler pair that is registered with mux.\nvar serveMuxRegister = []struct {\n\tpattern string\n\th       Handler\n}{\n\t{\"email:\", makeFakeHandler(\"default email handler\")},\n\t{\"email:signup\", makeFakeHandler(\"signup email handler\")},\n\t{\"csv:export\", makeFakeHandler(\"csv export handler\")},\n}\n\nvar serveMuxTests = []struct {\n\ttypename string // task's type name\n\twant     string // identifier of the handler that should be called\n}{\n\t{\"email:signup\", \"signup email handler\"},\n\t{\"csv:export\", \"csv export handler\"},\n\t{\"email:daily\", \"default email handler\"},\n}\n\nfunc TestServeMux(t *testing.T) {\n\tmux := NewServeMux()\n\tfor _, e := range serveMuxRegister {\n\t\tmux.Handle(e.pattern, e.h)\n\t}\n\n\tfor _, tc := range serveMuxTests {\n\t\tcalled = \"\" // reset to zero value\n\n\t\ttask := NewTask(tc.typename, nil)\n\t\tif err := mux.ProcessTask(context.Background(), task); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif called != tc.want {\n\t\t\tt.Errorf(\"%q handler was called for task %q, want %q to be called\", called, task.Type(), tc.want)\n\t\t}\n\t}\n}\n\nfunc TestServeMuxRegisterNilHandler(t *testing.T) {\n\tdefer func() {\n\t\tif err := recover(); err == nil {\n\t\t\tt.Error(\"expected call to mux.HandleFunc to panic\")\n\t\t}\n\t}()\n\n\tmux := NewServeMux()\n\tmux.HandleFunc(\"email:signup\", nil)\n}\n\nfunc TestServeMuxRegisterEmptyPattern(t *testing.T) {\n\tdefer func() {\n\t\tif err := recover(); err == nil {\n\t\t\tt.Error(\"expected call to mux.HandleFunc to panic\")\n\t\t}\n\t}()\n\n\tmux := NewServeMux()\n\tmux.Handle(\"\", makeFakeHandler(\"email\"))\n}\n\nfunc TestServeMuxRegisterDuplicatePattern(t *testing.T) {\n\tdefer func() {\n\t\tif err := recover(); err == nil {\n\t\t\tt.Error(\"expected call to mux.HandleFunc to panic\")\n\t\t}\n\t}()\n\n\tmux := NewServeMux()\n\tmux.Handle(\"email\", makeFakeHandler(\"email\"))\n\tmux.Handle(\"email\", makeFakeHandler(\"email:default\"))\n}\n\nvar notFoundTests = []struct {\n\ttypename string // task's type name\n}{\n\t{\"image:minimize\"},\n\t{\"csv:\"}, // registered patterns match the task's type prefix, not the other way around.\n}\n\nfunc TestServeMuxNotFound(t *testing.T) {\n\tmux := NewServeMux()\n\tfor _, e := range serveMuxRegister {\n\t\tmux.Handle(e.pattern, e.h)\n\t}\n\n\tfor _, tc := range notFoundTests {\n\t\ttask := NewTask(tc.typename, nil)\n\t\terr := mux.ProcessTask(context.Background(), task)\n\t\tif err == nil {\n\t\t\tt.Errorf(\"ProcessTask did not return error for task %q, should return 'not found' error\", task.Type())\n\t\t}\n\t}\n}\n\nvar middlewareTests = []struct {\n\ttypename    string   // task's type name\n\tmiddlewares []string // middlewares to use. They should be called in this order.\n\twant        string   // identifier of the handler that should be called\n}{\n\t{\"email:signup\", []string{\"logging\", \"expiration\"}, \"signup email handler\"},\n\t{\"csv:export\", []string{}, \"csv export handler\"},\n\t{\"email:daily\", []string{\"expiration\", \"logging\"}, \"default email handler\"},\n}\n\nfunc TestServeMuxMiddlewares(t *testing.T) {\n\tfor _, tc := range middlewareTests {\n\t\tmux := NewServeMux()\n\t\tfor _, e := range serveMuxRegister {\n\t\t\tmux.Handle(e.pattern, e.h)\n\t\t}\n\t\tvar mws []MiddlewareFunc\n\t\tfor _, s := range tc.middlewares {\n\t\t\tmws = append(mws, makeFakeMiddleware(s))\n\t\t}\n\t\tmux.Use(mws...)\n\n\t\tinvoked = []string{} // reset to empty slice\n\t\tcalled = \"\"          // reset to zero value\n\n\t\ttask := NewTask(tc.typename, nil)\n\t\tif err := mux.ProcessTask(context.Background(), task); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif diff := cmp.Diff(invoked, tc.middlewares); diff != \"\" {\n\t\t\tt.Errorf(\"invoked middlewares were %v, want %v\", invoked, tc.middlewares)\n\t\t}\n\n\t\tif called != tc.want {\n\t\t\tt.Errorf(\"%q handler was called for task %q, want %q to be called\", called, task.Type(), tc.want)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "server.go",
          "type": "blob",
          "size": 24.0654296875,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand/v2\"\n\t\"runtime\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/log\"\n\t\"github.com/hibiken/asynq/internal/rdb\"\n\t\"github.com/redis/go-redis/v9\"\n)\n\n// Server is responsible for task processing and task lifecycle management.\n//\n// Server pulls tasks off queues and processes them.\n// If the processing of a task is unsuccessful, server will schedule it for a retry.\n//\n// A task will be retried until either the task gets processed successfully\n// or until it reaches its max retry count.\n//\n// If a task exhausts its retries, it will be moved to the archive and\n// will be kept in the archive set.\n// Note that the archive size is finite and once it reaches its max size,\n// oldest tasks in the archive will be deleted.\ntype Server struct {\n\tlogger *log.Logger\n\n\tbroker base.Broker\n\t// When a Server has been created with an existing Redis connection, we do\n\t// not want to close it.\n\tsharedConnection bool\n\n\tstate *serverState\n\n\t// wait group to wait for all goroutines to finish.\n\twg            sync.WaitGroup\n\tforwarder     *forwarder\n\tprocessor     *processor\n\tsyncer        *syncer\n\theartbeater   *heartbeater\n\tsubscriber    *subscriber\n\trecoverer     *recoverer\n\thealthchecker *healthchecker\n\tjanitor       *janitor\n\taggregator    *aggregator\n}\n\ntype serverState struct {\n\tmu    sync.Mutex\n\tvalue serverStateValue\n}\n\ntype serverStateValue int\n\nconst (\n\t// StateNew represents a new server. Server begins in\n\t// this state and then transition to StatusActive when\n\t// Start or Run is callled.\n\tsrvStateNew serverStateValue = iota\n\n\t// StateActive indicates the server is up and active.\n\tsrvStateActive\n\n\t// StateStopped indicates the server is up but no longer processing new tasks.\n\tsrvStateStopped\n\n\t// StateClosed indicates the server has been shutdown.\n\tsrvStateClosed\n)\n\nvar serverStates = []string{\n\t\"new\",\n\t\"active\",\n\t\"stopped\",\n\t\"closed\",\n}\n\nfunc (s serverStateValue) String() string {\n\tif srvStateNew <= s && s <= srvStateClosed {\n\t\treturn serverStates[s]\n\t}\n\treturn \"unknown status\"\n}\n\n// Config specifies the server's background-task processing behavior.\ntype Config struct {\n\t// Maximum number of concurrent processing of tasks.\n\t//\n\t// If set to a zero or negative value, NewServer will overwrite the value\n\t// to the number of CPUs usable by the current process.\n\tConcurrency int\n\n\t// BaseContext optionally specifies a function that returns the base context for Handler invocations on this server.\n\t//\n\t// If BaseContext is nil, the default is context.Background().\n\t// If this is defined, then it MUST return a non-nil context\n\tBaseContext func() context.Context\n\n\t// TaskCheckInterval specifies the interval between checks for new tasks to process when all queues are empty.\n\t//\n\t// If unset, zero or a negative value, the interval is set to 1 second.\n\t//\n\t// Note: Setting this value too low may add significant load to redis.\n\t//\n\t// By default, TaskCheckInterval is set to 1 seconds.\n\tTaskCheckInterval time.Duration\n\n\t// Function to calculate retry delay for a failed task.\n\t//\n\t// By default, it uses exponential backoff algorithm to calculate the delay.\n\tRetryDelayFunc RetryDelayFunc\n\n\t// Predicate function to determine whether the error returned from Handler is a failure.\n\t// If the function returns false, Server will not increment the retried counter for the task,\n\t// and Server won't record the queue stats (processed and failed stats) to avoid skewing the error\n\t// rate of the queue.\n\t//\n\t// By default, if the given error is non-nil the function returns true.\n\tIsFailure func(error) bool\n\n\t// List of queues to process with given priority value. Keys are the names of the\n\t// queues and values are associated priority value.\n\t//\n\t// If set to nil or not specified, the server will process only the \"default\" queue.\n\t//\n\t// Priority is treated as follows to avoid starving low priority queues.\n\t//\n\t// Example:\n\t//\n\t//     Queues: map[string]int{\n\t//         \"critical\": 6,\n\t//         \"default\":  3,\n\t//         \"low\":      1,\n\t//     }\n\t//\n\t// With the above config and given that all queues are not empty, the tasks\n\t// in \"critical\", \"default\", \"low\" should be processed 60%, 30%, 10% of\n\t// the time respectively.\n\t//\n\t// If a queue has a zero or negative priority value, the queue will be ignored.\n\tQueues map[string]int\n\n\t// StrictPriority indicates whether the queue priority should be treated strictly.\n\t//\n\t// If set to true, tasks in the queue with the highest priority is processed first.\n\t// The tasks in lower priority queues are processed only when those queues with\n\t// higher priorities are empty.\n\tStrictPriority bool\n\n\t// ErrorHandler handles errors returned by the task handler.\n\t//\n\t// HandleError is invoked only if the task handler returns a non-nil error.\n\t//\n\t// Example:\n\t//\n\t//     func reportError(ctx context, task *asynq.Task, err error) {\n\t//         retried, _ := asynq.GetRetryCount(ctx)\n\t//         maxRetry, _ := asynq.GetMaxRetry(ctx)\n\t//     \t   if retried >= maxRetry {\n\t//             err = fmt.Errorf(\"retry exhausted for task %s: %w\", task.Type, err)\n\t//     \t   }\n\t//         errorReportingService.Notify(err)\n\t//     })\n\t//\n\t//     ErrorHandler: asynq.ErrorHandlerFunc(reportError)\n\n\t//    we can also handle panic error like:\n\t//     func reportError(ctx context, task *asynq.Task, err error) {\n\t//         if asynq.IsPanic(err) {\n\t//\t          errorReportingService.Notify(err)\n\t// \t       }\n\t//     })\n\t//\n\t//     ErrorHandler: asynq.ErrorHandlerFunc(reportError)\n\n\tErrorHandler ErrorHandler\n\n\t// Logger specifies the logger used by the server instance.\n\t//\n\t// If unset, default logger is used.\n\tLogger Logger\n\n\t// LogLevel specifies the minimum log level to enable.\n\t//\n\t// If unset, InfoLevel is used by default.\n\tLogLevel LogLevel\n\n\t// ShutdownTimeout specifies the duration to wait to let workers finish their tasks\n\t// before forcing them to abort when stopping the server.\n\t//\n\t// If unset or zero, default timeout of 8 seconds is used.\n\tShutdownTimeout time.Duration\n\n\t// HealthCheckFunc is called periodically with any errors encountered during ping to the\n\t// connected redis server.\n\tHealthCheckFunc func(error)\n\n\t// HealthCheckInterval specifies the interval between healthchecks.\n\t//\n\t// If unset or zero, the interval is set to 15 seconds.\n\tHealthCheckInterval time.Duration\n\n\t// DelayedTaskCheckInterval specifies the interval between checks run on 'scheduled' and 'retry'\n\t// tasks, and forwarding them to 'pending' state if they are ready to be processed.\n\t//\n\t// If unset or zero, the interval is set to 5 seconds.\n\tDelayedTaskCheckInterval time.Duration\n\n\t// GroupGracePeriod specifies the amount of time the server will wait for an incoming task before aggregating\n\t// the tasks in a group. If an incoming task is received within this period, the server will wait for another\n\t// period of the same length, up to GroupMaxDelay if specified.\n\t//\n\t// If unset or zero, the grace period is set to 1 minute.\n\t// Minimum duration for GroupGracePeriod is 1 second. If value specified is less than a second, the call to\n\t// NewServer will panic.\n\tGroupGracePeriod time.Duration\n\n\t// GroupMaxDelay specifies the maximum amount of time the server will wait for incoming tasks before aggregating\n\t// the tasks in a group.\n\t//\n\t// If unset or zero, no delay limit is used.\n\tGroupMaxDelay time.Duration\n\n\t// GroupMaxSize specifies the maximum number of tasks that can be aggregated into a single task within a group.\n\t// If GroupMaxSize is reached, the server will aggregate the tasks into one immediately.\n\t//\n\t// If unset or zero, no size limit is used.\n\tGroupMaxSize int\n\n\t// GroupAggregator specifies the aggregation function used to aggregate multiple tasks in a group into one task.\n\t//\n\t// If unset or nil, the group aggregation feature will be disabled on the server.\n\tGroupAggregator GroupAggregator\n\n\t// JanitorInterval specifies the average interval of janitor checks for expired completed tasks.\n\t//\n\t// If unset or zero, default interval of 8 seconds is used.\n\tJanitorInterval time.Duration\n\n\t// JanitorBatchSize specifies the number of expired completed tasks to be deleted in one run.\n\t//\n\t// If unset or zero, default batch size of 100 is used.\n\t// Make sure to not put a big number as the batch size to prevent a long-running script.\n\tJanitorBatchSize int\n}\n\n// GroupAggregator aggregates a group of tasks into one before the tasks are passed to the Handler.\ntype GroupAggregator interface {\n\t// Aggregate aggregates the given tasks in a group with the given group name,\n\t// and returns a new task which is the aggregation of those tasks.\n\t//\n\t// Use NewTask(typename, payload, opts...) to set any options for the aggregated task.\n\t// The Queue option, if provided, will be ignored and the aggregated task will always be enqueued\n\t// to the same queue the group belonged.\n\tAggregate(group string, tasks []*Task) *Task\n}\n\n// The GroupAggregatorFunc type is an adapter to allow the use of  ordinary functions as a GroupAggregator.\n// If f is a function with the appropriate signature, GroupAggregatorFunc(f) is a GroupAggregator that calls f.\ntype GroupAggregatorFunc func(group string, tasks []*Task) *Task\n\n// Aggregate calls fn(group, tasks)\nfunc (fn GroupAggregatorFunc) Aggregate(group string, tasks []*Task) *Task {\n\treturn fn(group, tasks)\n}\n\n// An ErrorHandler handles an error occurred during task processing.\ntype ErrorHandler interface {\n\tHandleError(ctx context.Context, task *Task, err error)\n}\n\n// The ErrorHandlerFunc type is an adapter to allow the use of  ordinary functions as a ErrorHandler.\n// If f is a function with the appropriate signature, ErrorHandlerFunc(f) is a ErrorHandler that calls f.\ntype ErrorHandlerFunc func(ctx context.Context, task *Task, err error)\n\n// HandleError calls fn(ctx, task, err)\nfunc (fn ErrorHandlerFunc) HandleError(ctx context.Context, task *Task, err error) {\n\tfn(ctx, task, err)\n}\n\n// RetryDelayFunc calculates the retry delay duration for a failed task given\n// the retry count, error, and the task.\n//\n// n is the number of times the task has been retried.\n// e is the error returned by the task handler.\n// t is the task in question.\ntype RetryDelayFunc func(n int, e error, t *Task) time.Duration\n\n// Logger supports logging at various log levels.\ntype Logger interface {\n\t// Debug logs a message at Debug level.\n\tDebug(args ...interface{})\n\n\t// Info logs a message at Info level.\n\tInfo(args ...interface{})\n\n\t// Warn logs a message at Warning level.\n\tWarn(args ...interface{})\n\n\t// Error logs a message at Error level.\n\tError(args ...interface{})\n\n\t// Fatal logs a message at Fatal level\n\t// and process will exit with status set to 1.\n\tFatal(args ...interface{})\n}\n\n// LogLevel represents logging level.\n//\n// It satisfies flag.Value interface.\ntype LogLevel int32\n\nconst (\n\t// Note: reserving value zero to differentiate unspecified case.\n\tlevel_unspecified LogLevel = iota\n\n\t// DebugLevel is the lowest level of logging.\n\t// Debug logs are intended for debugging and development purposes.\n\tDebugLevel\n\n\t// InfoLevel is used for general informational log messages.\n\tInfoLevel\n\n\t// WarnLevel is used for undesired but relatively expected events,\n\t// which may indicate a problem.\n\tWarnLevel\n\n\t// ErrorLevel is used for undesired and unexpected events that\n\t// the program can recover from.\n\tErrorLevel\n\n\t// FatalLevel is used for undesired and unexpected events that\n\t// the program cannot recover from.\n\tFatalLevel\n)\n\n// String is part of the flag.Value interface.\nfunc (l *LogLevel) String() string {\n\tswitch *l {\n\tcase DebugLevel:\n\t\treturn \"debug\"\n\tcase InfoLevel:\n\t\treturn \"info\"\n\tcase WarnLevel:\n\t\treturn \"warn\"\n\tcase ErrorLevel:\n\t\treturn \"error\"\n\tcase FatalLevel:\n\t\treturn \"fatal\"\n\t}\n\tpanic(fmt.Sprintf(\"asynq: unexpected log level: %v\", *l))\n}\n\n// Set is part of the flag.Value interface.\nfunc (l *LogLevel) Set(val string) error {\n\tswitch strings.ToLower(val) {\n\tcase \"debug\":\n\t\t*l = DebugLevel\n\tcase \"info\":\n\t\t*l = InfoLevel\n\tcase \"warn\", \"warning\":\n\t\t*l = WarnLevel\n\tcase \"error\":\n\t\t*l = ErrorLevel\n\tcase \"fatal\":\n\t\t*l = FatalLevel\n\tdefault:\n\t\treturn fmt.Errorf(\"asynq: unsupported log level %q\", val)\n\t}\n\treturn nil\n}\n\nfunc toInternalLogLevel(l LogLevel) log.Level {\n\tswitch l {\n\tcase DebugLevel:\n\t\treturn log.DebugLevel\n\tcase InfoLevel:\n\t\treturn log.InfoLevel\n\tcase WarnLevel:\n\t\treturn log.WarnLevel\n\tcase ErrorLevel:\n\t\treturn log.ErrorLevel\n\tcase FatalLevel:\n\t\treturn log.FatalLevel\n\t}\n\tpanic(fmt.Sprintf(\"asynq: unexpected log level: %v\", l))\n}\n\n// DefaultRetryDelayFunc is the default RetryDelayFunc used if one is not specified in Config.\n// It uses exponential back-off strategy to calculate the retry delay.\nfunc DefaultRetryDelayFunc(n int, e error, t *Task) time.Duration {\n\t// Formula taken from https://github.com/mperham/sidekiq.\n\ts := int(math.Pow(float64(n), 4)) + 15 + (rand.IntN(30) * (n + 1))\n\treturn time.Duration(s) * time.Second\n}\n\nfunc defaultIsFailureFunc(err error) bool { return err != nil }\n\nvar defaultQueueConfig = map[string]int{\n\tbase.DefaultQueueName: 1,\n}\n\nconst (\n\tdefaultTaskCheckInterval = 1 * time.Second\n\n\tdefaultShutdownTimeout = 8 * time.Second\n\n\tdefaultHealthCheckInterval = 15 * time.Second\n\n\tdefaultDelayedTaskCheckInterval = 5 * time.Second\n\n\tdefaultGroupGracePeriod = 1 * time.Minute\n\n\tdefaultJanitorInterval = 8 * time.Second\n\n\tdefaultJanitorBatchSize = 100\n)\n\n// NewServer returns a new Server given a redis connection option\n// and server configuration.\nfunc NewServer(r RedisConnOpt, cfg Config) *Server {\n\tredisClient, ok := r.MakeRedisClient().(redis.UniversalClient)\n\tif !ok {\n\t\tpanic(fmt.Sprintf(\"asynq: unsupported RedisConnOpt type %T\", r))\n\t}\n\tserver := NewServerFromRedisClient(redisClient, cfg)\n\tserver.sharedConnection = false\n\treturn server\n}\n\n// NewServerFromRedisClient returns a new instance of Server given a redis.UniversalClient\n// and server configuration\n// Warning: The underlying redis connection pool will not be closed by Asynq, you are responsible for closing it.\nfunc NewServerFromRedisClient(c redis.UniversalClient, cfg Config) *Server {\n\tbaseCtxFn := cfg.BaseContext\n\tif baseCtxFn == nil {\n\t\tbaseCtxFn = context.Background\n\t}\n\tn := cfg.Concurrency\n\tif n < 1 {\n\t\tn = runtime.NumCPU()\n\t}\n\n\ttaskCheckInterval := cfg.TaskCheckInterval\n\tif taskCheckInterval <= 0 {\n\t\ttaskCheckInterval = defaultTaskCheckInterval\n\t}\n\n\tdelayFunc := cfg.RetryDelayFunc\n\tif delayFunc == nil {\n\t\tdelayFunc = DefaultRetryDelayFunc\n\t}\n\tisFailureFunc := cfg.IsFailure\n\tif isFailureFunc == nil {\n\t\tisFailureFunc = defaultIsFailureFunc\n\t}\n\tqueues := make(map[string]int)\n\tfor qname, p := range cfg.Queues {\n\t\tif err := base.ValidateQueueName(qname); err != nil {\n\t\t\tcontinue // ignore invalid queue names\n\t\t}\n\t\tif p > 0 {\n\t\t\tqueues[qname] = p\n\t\t}\n\t}\n\tif len(queues) == 0 {\n\t\tqueues = defaultQueueConfig\n\t}\n\tvar qnames []string\n\tfor q := range queues {\n\t\tqnames = append(qnames, q)\n\t}\n\tshutdownTimeout := cfg.ShutdownTimeout\n\tif shutdownTimeout == 0 {\n\t\tshutdownTimeout = defaultShutdownTimeout\n\t}\n\thealthcheckInterval := cfg.HealthCheckInterval\n\tif healthcheckInterval == 0 {\n\t\thealthcheckInterval = defaultHealthCheckInterval\n\t}\n\t// TODO: Create a helper to check for zero value and fall back to default (e.g. getDurationOrDefault())\n\tgroupGracePeriod := cfg.GroupGracePeriod\n\tif groupGracePeriod == 0 {\n\t\tgroupGracePeriod = defaultGroupGracePeriod\n\t}\n\tif groupGracePeriod < time.Second {\n\t\tpanic(\"GroupGracePeriod cannot be less than a second\")\n\t}\n\tlogger := log.NewLogger(cfg.Logger)\n\tloglevel := cfg.LogLevel\n\tif loglevel == level_unspecified {\n\t\tloglevel = InfoLevel\n\t}\n\tlogger.SetLevel(toInternalLogLevel(loglevel))\n\n\trdb := rdb.NewRDB(c)\n\tstarting := make(chan *workerInfo)\n\tfinished := make(chan *base.TaskMessage)\n\tsyncCh := make(chan *syncRequest)\n\tsrvState := &serverState{value: srvStateNew}\n\tcancels := base.NewCancelations()\n\n\tsyncer := newSyncer(syncerParams{\n\t\tlogger:     logger,\n\t\trequestsCh: syncCh,\n\t\tinterval:   5 * time.Second,\n\t})\n\theartbeater := newHeartbeater(heartbeaterParams{\n\t\tlogger:         logger,\n\t\tbroker:         rdb,\n\t\tinterval:       5 * time.Second,\n\t\tconcurrency:    n,\n\t\tqueues:         queues,\n\t\tstrictPriority: cfg.StrictPriority,\n\t\tstate:          srvState,\n\t\tstarting:       starting,\n\t\tfinished:       finished,\n\t})\n\tdelayedTaskCheckInterval := cfg.DelayedTaskCheckInterval\n\tif delayedTaskCheckInterval == 0 {\n\t\tdelayedTaskCheckInterval = defaultDelayedTaskCheckInterval\n\t}\n\tforwarder := newForwarder(forwarderParams{\n\t\tlogger:   logger,\n\t\tbroker:   rdb,\n\t\tqueues:   qnames,\n\t\tinterval: delayedTaskCheckInterval,\n\t})\n\tsubscriber := newSubscriber(subscriberParams{\n\t\tlogger:       logger,\n\t\tbroker:       rdb,\n\t\tcancelations: cancels,\n\t})\n\tprocessor := newProcessor(processorParams{\n\t\tlogger:            logger,\n\t\tbroker:            rdb,\n\t\tretryDelayFunc:    delayFunc,\n\t\ttaskCheckInterval: taskCheckInterval,\n\t\tbaseCtxFn:         baseCtxFn,\n\t\tisFailureFunc:     isFailureFunc,\n\t\tsyncCh:            syncCh,\n\t\tcancelations:      cancels,\n\t\tconcurrency:       n,\n\t\tqueues:            queues,\n\t\tstrictPriority:    cfg.StrictPriority,\n\t\terrHandler:        cfg.ErrorHandler,\n\t\tshutdownTimeout:   shutdownTimeout,\n\t\tstarting:          starting,\n\t\tfinished:          finished,\n\t})\n\trecoverer := newRecoverer(recovererParams{\n\t\tlogger:         logger,\n\t\tbroker:         rdb,\n\t\tretryDelayFunc: delayFunc,\n\t\tisFailureFunc:  isFailureFunc,\n\t\tqueues:         qnames,\n\t\tinterval:       1 * time.Minute,\n\t})\n\thealthchecker := newHealthChecker(healthcheckerParams{\n\t\tlogger:          logger,\n\t\tbroker:          rdb,\n\t\tinterval:        healthcheckInterval,\n\t\thealthcheckFunc: cfg.HealthCheckFunc,\n\t})\n\n\tjanitorInterval := cfg.JanitorInterval\n\tif janitorInterval == 0 {\n\t\tjanitorInterval = defaultJanitorInterval\n\t}\n\n\tjanitorBatchSize := cfg.JanitorBatchSize\n\tif janitorBatchSize == 0 {\n\t\tjanitorBatchSize = defaultJanitorBatchSize\n\t}\n\tif janitorBatchSize > defaultJanitorBatchSize {\n\t\tlogger.Warnf(\"Janitor batch size of %d is greater than the recommended batch size of %d. \"+\n\t\t\t\"This might cause a long-running script\", janitorBatchSize, defaultJanitorBatchSize)\n\t}\n\tjanitor := newJanitor(janitorParams{\n\t\tlogger:    logger,\n\t\tbroker:    rdb,\n\t\tqueues:    qnames,\n\t\tinterval:  janitorInterval,\n\t\tbatchSize: janitorBatchSize,\n\t})\n\taggregator := newAggregator(aggregatorParams{\n\t\tlogger:          logger,\n\t\tbroker:          rdb,\n\t\tqueues:          qnames,\n\t\tgracePeriod:     groupGracePeriod,\n\t\tmaxDelay:        cfg.GroupMaxDelay,\n\t\tmaxSize:         cfg.GroupMaxSize,\n\t\tgroupAggregator: cfg.GroupAggregator,\n\t})\n\treturn &Server{\n\t\tlogger:           logger,\n\t\tbroker:           rdb,\n\t\tsharedConnection: true,\n\t\tstate:            srvState,\n\t\tforwarder:        forwarder,\n\t\tprocessor:        processor,\n\t\tsyncer:           syncer,\n\t\theartbeater:      heartbeater,\n\t\tsubscriber:       subscriber,\n\t\trecoverer:        recoverer,\n\t\thealthchecker:    healthchecker,\n\t\tjanitor:          janitor,\n\t\taggregator:       aggregator,\n\t}\n}\n\n// A Handler processes tasks.\n//\n// ProcessTask should return nil if the processing of a task\n// is successful.\n//\n// If ProcessTask returns a non-nil error or panics, the task\n// will be retried after delay if retry-count is remaining,\n// otherwise the task will be archived.\n//\n// One exception to this rule is when ProcessTask returns a SkipRetry error.\n// If the returned error is SkipRetry or an error wraps SkipRetry, retry is\n// skipped and the task will be immediately archived instead.\n//\n// Another exception to this rule is when ProcessTask returns a RevokeTask error.\n// If the returned error is RevokeTask or an error wraps RevokeTask, the task\n// will not be retried or archived.\ntype Handler interface {\n\tProcessTask(context.Context, *Task) error\n}\n\n// The HandlerFunc type is an adapter to allow the use of\n// ordinary functions as a Handler. If f is a function\n// with the appropriate signature, HandlerFunc(f) is a\n// Handler that calls f.\ntype HandlerFunc func(context.Context, *Task) error\n\n// ProcessTask calls fn(ctx, task)\nfunc (fn HandlerFunc) ProcessTask(ctx context.Context, task *Task) error {\n\treturn fn(ctx, task)\n}\n\n// ErrServerClosed indicates that the operation is now illegal because of the server has been shutdown.\nvar ErrServerClosed = errors.New(\"asynq: Server closed\")\n\n// Run starts the task processing and blocks until\n// an os signal to exit the program is received. Once it receives\n// a signal, it gracefully shuts down all active workers and other\n// goroutines to process the tasks.\n//\n// Run returns any error encountered at server startup time.\n// If the server has already been shutdown, ErrServerClosed is returned.\nfunc (srv *Server) Run(handler Handler) error {\n\tif err := srv.Start(handler); err != nil {\n\t\treturn err\n\t}\n\tsrv.waitForSignals()\n\tsrv.Shutdown()\n\treturn nil\n}\n\n// Start starts the worker server. Once the server has started,\n// it pulls tasks off queues and starts a worker goroutine for each task\n// and then call Handler to process it.\n// Tasks are processed concurrently by the workers up to the number of\n// concurrency specified in Config.Concurrency.\n//\n// Start returns any error encountered at server startup time.\n// If the server has already been shutdown, ErrServerClosed is returned.\nfunc (srv *Server) Start(handler Handler) error {\n\tif handler == nil {\n\t\treturn fmt.Errorf(\"asynq: server cannot run with nil handler\")\n\t}\n\tsrv.processor.handler = handler\n\n\tif err := srv.start(); err != nil {\n\t\treturn err\n\t}\n\tsrv.logger.Info(\"Starting processing\")\n\n\tsrv.heartbeater.start(&srv.wg)\n\tsrv.healthchecker.start(&srv.wg)\n\tsrv.subscriber.start(&srv.wg)\n\tsrv.syncer.start(&srv.wg)\n\tsrv.recoverer.start(&srv.wg)\n\tsrv.forwarder.start(&srv.wg)\n\tsrv.processor.start(&srv.wg)\n\tsrv.janitor.start(&srv.wg)\n\tsrv.aggregator.start(&srv.wg)\n\treturn nil\n}\n\n// Checks server state and returns an error if pre-condition is not met.\n// Otherwise it sets the server state to active.\nfunc (srv *Server) start() error {\n\tsrv.state.mu.Lock()\n\tdefer srv.state.mu.Unlock()\n\tswitch srv.state.value {\n\tcase srvStateActive:\n\t\treturn fmt.Errorf(\"asynq: the server is already running\")\n\tcase srvStateStopped:\n\t\treturn fmt.Errorf(\"asynq: the server is in the stopped state. Waiting for shutdown.\")\n\tcase srvStateClosed:\n\t\treturn ErrServerClosed\n\t}\n\tsrv.state.value = srvStateActive\n\treturn nil\n}\n\n// Shutdown gracefully shuts down the server.\n// It gracefully closes all active workers. The server will wait for\n// active workers to finish processing tasks for duration specified in Config.ShutdownTimeout.\n// If worker didn't finish processing a task during the timeout, the task will be pushed back to Redis.\nfunc (srv *Server) Shutdown() {\n\tsrv.state.mu.Lock()\n\tif srv.state.value == srvStateNew || srv.state.value == srvStateClosed {\n\t\tsrv.state.mu.Unlock()\n\t\t// server is not running, do nothing and return.\n\t\treturn\n\t}\n\tsrv.state.value = srvStateClosed\n\tsrv.state.mu.Unlock()\n\n\tsrv.logger.Info(\"Starting graceful shutdown\")\n\t// Note: The order of shutdown is important.\n\t// Sender goroutines should be terminated before the receiver goroutines.\n\t// processor -> syncer (via syncCh)\n\t// processor -> heartbeater (via starting, finished channels)\n\tsrv.forwarder.shutdown()\n\tsrv.processor.shutdown()\n\tsrv.recoverer.shutdown()\n\tsrv.syncer.shutdown()\n\tsrv.subscriber.shutdown()\n\tsrv.janitor.shutdown()\n\tsrv.aggregator.shutdown()\n\tsrv.healthchecker.shutdown()\n\tsrv.heartbeater.shutdown()\n\tsrv.wg.Wait()\n\n\tif !srv.sharedConnection {\n\t\tsrv.broker.Close()\n\t}\n\tsrv.logger.Info(\"Exiting\")\n}\n\n// Stop signals the server to stop pulling new tasks off queues.\n// Stop can be used before shutting down the server to ensure that all\n// currently active tasks are processed before server shutdown.\n//\n// Stop does not shutdown the server, make sure to call Shutdown before exit.\nfunc (srv *Server) Stop() {\n\tsrv.state.mu.Lock()\n\tif srv.state.value != srvStateActive {\n\t\t// Invalid call to Stop, server can only go from Active state to Stopped state.\n\t\tsrv.state.mu.Unlock()\n\t\treturn\n\t}\n\tsrv.state.value = srvStateStopped\n\tsrv.state.mu.Unlock()\n\n\tsrv.logger.Info(\"Stopping processor\")\n\tsrv.processor.stop()\n\tsrv.logger.Info(\"Processor stopped\")\n}\n\n// Ping performs a ping against the redis connection.\n//\n// This is an alternative to the HealthCheckFunc available in the Config object.\nfunc (srv *Server) Ping() error {\n\tsrv.state.mu.Lock()\n\tdefer srv.state.mu.Unlock()\n\tif srv.state.value == srvStateClosed {\n\t\treturn nil\n\t}\n\n\treturn srv.broker.Ping()\n}\n"
        },
        {
          "name": "server_test.go",
          "type": "blob",
          "size": 6.4970703125,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"syscall\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/hibiken/asynq/internal/rdb\"\n\t\"github.com/hibiken/asynq/internal/testbroker\"\n\t\"github.com/hibiken/asynq/internal/testutil\"\n\n\t\"github.com/redis/go-redis/v9\"\n\t\"go.uber.org/goleak\"\n)\n\nfunc testServer(t *testing.T, c *Client, srv *Server) {\n\t// no-op handler\n\th := func(ctx context.Context, task *Task) error {\n\t\treturn nil\n\t}\n\n\terr := srv.Start(HandlerFunc(h))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t_, err = c.Enqueue(NewTask(\"send_email\", testutil.JSON(map[string]interface{}{\"recipient_id\": 123})))\n\tif err != nil {\n\t\tt.Errorf(\"could not enqueue a task: %v\", err)\n\t}\n\n\t_, err = c.Enqueue(NewTask(\"send_email\", testutil.JSON(map[string]interface{}{\"recipient_id\": 456})), ProcessIn(1*time.Hour))\n\tif err != nil {\n\t\tt.Errorf(\"could not enqueue a task: %v\", err)\n\t}\n\n\tsrv.Shutdown()\n}\n\nfunc TestServer(t *testing.T) {\n\t// https://github.com/go-redis/redis/issues/1029\n\tignoreOpt := goleak.IgnoreTopFunction(\"github.com/redis/go-redis/v9/internal/pool.(*ConnPool).reaper\")\n\tdefer goleak.VerifyNone(t, ignoreOpt)\n\n\tredisConnOpt := getRedisConnOpt(t)\n\tc := NewClient(redisConnOpt)\n\tdefer c.Close()\n\tsrv := NewServer(redisConnOpt, Config{\n\t\tConcurrency: 10,\n\t\tLogLevel:    testLogLevel,\n\t})\n\n\ttestServer(t, c, srv)\n}\n\nfunc TestServerFromRedisClient(t *testing.T) {\n\t// https://github.com/go-redis/redis/issues/1029\n\tignoreOpt := goleak.IgnoreTopFunction(\"github.com/redis/go-redis/v9/internal/pool.(*ConnPool).reaper\")\n\tdefer goleak.VerifyNone(t, ignoreOpt)\n\n\tredisConnOpt := getRedisConnOpt(t)\n\tredisClient := redisConnOpt.MakeRedisClient().(redis.UniversalClient)\n\tc := NewClientFromRedisClient(redisClient)\n\tsrv := NewServerFromRedisClient(redisClient, Config{\n\t\tConcurrency: 10,\n\t\tLogLevel:    testLogLevel,\n\t})\n\n\ttestServer(t, c, srv)\n\n\terr := c.Close()\n\tif err == nil {\n\t\tt.Error(\"client.Close() should have failed because of a shared client but it didn't\")\n\t}\n}\n\nfunc TestServerRun(t *testing.T) {\n\t// https://github.com/go-redis/redis/issues/1029\n\tignoreOpt := goleak.IgnoreTopFunction(\"github.com/redis/go-redis/v9/internal/pool.(*ConnPool).reaper\")\n\tdefer goleak.VerifyNone(t, ignoreOpt)\n\n\tsrv := NewServer(getRedisConnOpt(t), Config{LogLevel: testLogLevel})\n\n\tdone := make(chan struct{})\n\t// Make sure server exits when receiving TERM signal.\n\tgo func() {\n\t\ttime.Sleep(2 * time.Second)\n\t\t_ = syscall.Kill(syscall.Getpid(), syscall.SIGTERM)\n\t\tdone <- struct{}{}\n\t}()\n\n\tgo func() {\n\t\tselect {\n\t\tcase <-time.After(10 * time.Second):\n\t\t\tpanic(\"server did not stop after receiving TERM signal\")\n\t\tcase <-done:\n\t\t}\n\t}()\n\n\tmux := NewServeMux()\n\tif err := srv.Run(mux); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestServerErrServerClosed(t *testing.T) {\n\tsrv := NewServer(getRedisConnOpt(t), Config{LogLevel: testLogLevel})\n\thandler := NewServeMux()\n\tif err := srv.Start(handler); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tsrv.Shutdown()\n\terr := srv.Start(handler)\n\tif err != ErrServerClosed {\n\t\tt.Errorf(\"Restarting server: (*Server).Start(handler) = %v, want ErrServerClosed error\", err)\n\t}\n}\n\nfunc TestServerErrNilHandler(t *testing.T) {\n\tsrv := NewServer(getRedisConnOpt(t), Config{LogLevel: testLogLevel})\n\terr := srv.Start(nil)\n\tif err == nil {\n\t\tt.Error(\"Starting server with nil handler: (*Server).Start(nil) did not return error\")\n\t\tsrv.Shutdown()\n\t}\n}\n\nfunc TestServerErrServerRunning(t *testing.T) {\n\tsrv := NewServer(getRedisConnOpt(t), Config{LogLevel: testLogLevel})\n\thandler := NewServeMux()\n\tif err := srv.Start(handler); err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr := srv.Start(handler)\n\tif err == nil {\n\t\tt.Error(\"Calling (*Server).Start(handler) on already running server did not return error\")\n\t}\n\tsrv.Shutdown()\n}\n\nfunc TestServerWithRedisDown(t *testing.T) {\n\t// Make sure that server does not panic and exit if redis is down.\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tt.Errorf(\"panic occurred: %v\", r)\n\t\t}\n\t}()\n\tr := rdb.NewRDB(setup(t))\n\ttestBroker := testbroker.NewTestBroker(r)\n\tsrv := NewServer(getRedisConnOpt(t), Config{LogLevel: testLogLevel})\n\tsrv.broker = testBroker\n\tsrv.forwarder.broker = testBroker\n\tsrv.heartbeater.broker = testBroker\n\tsrv.processor.broker = testBroker\n\tsrv.subscriber.broker = testBroker\n\ttestBroker.Sleep()\n\n\t// no-op handler\n\th := func(ctx context.Context, task *Task) error {\n\t\treturn nil\n\t}\n\n\terr := srv.Start(HandlerFunc(h))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttime.Sleep(3 * time.Second)\n\n\tsrv.Shutdown()\n}\n\nfunc TestServerWithFlakyBroker(t *testing.T) {\n\t// Make sure that server does not panic and exit if redis is down.\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tt.Errorf(\"panic occurred: %v\", r)\n\t\t}\n\t}()\n\tr := rdb.NewRDB(setup(t))\n\ttestBroker := testbroker.NewTestBroker(r)\n\tredisConnOpt := getRedisConnOpt(t)\n\tsrv := NewServer(redisConnOpt, Config{LogLevel: testLogLevel})\n\tsrv.broker = testBroker\n\tsrv.forwarder.broker = testBroker\n\tsrv.heartbeater.broker = testBroker\n\tsrv.processor.broker = testBroker\n\tsrv.subscriber.broker = testBroker\n\n\tc := NewClient(redisConnOpt)\n\n\th := func(ctx context.Context, task *Task) error {\n\t\t// force task retry.\n\t\tif task.Type() == \"bad_task\" {\n\t\t\treturn fmt.Errorf(\"could not process %q\", task.Type())\n\t\t}\n\t\ttime.Sleep(2 * time.Second)\n\t\treturn nil\n\t}\n\n\terr := srv.Start(HandlerFunc(h))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tfor i := 0; i < 10; i++ {\n\t\t_, err := c.Enqueue(NewTask(\"enqueued\", nil), MaxRetry(i))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\t_, err = c.Enqueue(NewTask(\"bad_task\", nil))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\t_, err = c.Enqueue(NewTask(\"scheduled\", nil), ProcessIn(time.Duration(i)*time.Second))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\t// simulate redis going down.\n\ttestBroker.Sleep()\n\n\ttime.Sleep(3 * time.Second)\n\n\t// simulate redis comes back online.\n\ttestBroker.Wakeup()\n\n\ttime.Sleep(3 * time.Second)\n\n\tsrv.Shutdown()\n}\n\nfunc TestLogLevel(t *testing.T) {\n\ttests := []struct {\n\t\tflagVal string\n\t\twant    LogLevel\n\t\twantStr string\n\t}{\n\t\t{\"debug\", DebugLevel, \"debug\"},\n\t\t{\"Info\", InfoLevel, \"info\"},\n\t\t{\"WARN\", WarnLevel, \"warn\"},\n\t\t{\"warning\", WarnLevel, \"warn\"},\n\t\t{\"Error\", ErrorLevel, \"error\"},\n\t\t{\"fatal\", FatalLevel, \"fatal\"},\n\t}\n\n\tfor _, tc := range tests {\n\t\tlevel := new(LogLevel)\n\t\tif err := level.Set(tc.flagVal); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif *level != tc.want {\n\t\t\tt.Errorf(\"Set(%q): got %v, want %v\", tc.flagVal, level, &tc.want)\n\t\t\tcontinue\n\t\t}\n\t\tif got := level.String(); got != tc.wantStr {\n\t\t\tt.Errorf(\"String() returned %q, want %q\", got, tc.wantStr)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "signals_unix.go",
          "type": "blob",
          "size": 0.923828125,
          "content": "//go:build linux || dragonfly || freebsd || netbsd || openbsd || darwin\n\npackage asynq\n\nimport (\n\t\"os\"\n\t\"os/signal\"\n\n\t\"golang.org/x/sys/unix\"\n)\n\n// waitForSignals waits for signals and handles them.\n// It handles SIGTERM, SIGINT, and SIGTSTP.\n// SIGTERM and SIGINT will signal the process to exit.\n// SIGTSTP will signal the process to stop processing new tasks.\nfunc (srv *Server) waitForSignals() {\n\tsrv.logger.Info(\"Send signal TSTP to stop processing new tasks\")\n\tsrv.logger.Info(\"Send signal TERM or INT to terminate the process\")\n\n\tsigs := make(chan os.Signal, 1)\n\tsignal.Notify(sigs, unix.SIGTERM, unix.SIGINT, unix.SIGTSTP)\n\tfor {\n\t\tsig := <-sigs\n\t\tif sig == unix.SIGTSTP {\n\t\t\tsrv.Stop()\n\t\t\tcontinue\n\t\t} else {\n\t\t\tsrv.Stop()\n\t\t\tbreak\n\t\t}\n\t}\n}\n\nfunc (s *Scheduler) waitForSignals() {\n\ts.logger.Info(\"Send signal TERM or INT to stop the scheduler\")\n\tsigs := make(chan os.Signal, 1)\n\tsignal.Notify(sigs, unix.SIGTERM, unix.SIGINT)\n\t<-sigs\n}\n"
        },
        {
          "name": "signals_windows.go",
          "type": "blob",
          "size": 0.6923828125,
          "content": "//go:build windows\n\npackage asynq\n\nimport (\n\t\"os\"\n\t\"os/signal\"\n\n\t\"golang.org/x/sys/windows\"\n)\n\n// waitForSignals waits for signals and handles them.\n// It handles SIGTERM and SIGINT.\n// SIGTERM and SIGINT will signal the process to exit.\n//\n// Note: Currently SIGTSTP is not supported for windows build.\nfunc (srv *Server) waitForSignals() {\n\tsrv.logger.Info(\"Send signal TERM or INT to terminate the process\")\n\tsigs := make(chan os.Signal, 1)\n\tsignal.Notify(sigs, windows.SIGTERM, windows.SIGINT)\n\t<-sigs\n}\n\nfunc (s *Scheduler) waitForSignals() {\n\ts.logger.Info(\"Send signal TERM or INT to stop the scheduler\")\n\tsigs := make(chan os.Signal, 1)\n\tsignal.Notify(sigs, windows.SIGTERM, windows.SIGINT)\n\t<-sigs\n}\n"
        },
        {
          "name": "subscriber.go",
          "type": "blob",
          "size": 1.890625,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/log\"\n)\n\ntype subscriber struct {\n\tlogger *log.Logger\n\tbroker base.Broker\n\n\t// channel to communicate back to the long running \"subscriber\" goroutine.\n\tdone chan struct{}\n\n\t// cancelations hold cancel functions for all active tasks.\n\tcancelations *base.Cancelations\n\n\t// time to wait before retrying to connect to redis.\n\tretryTimeout time.Duration\n}\n\ntype subscriberParams struct {\n\tlogger       *log.Logger\n\tbroker       base.Broker\n\tcancelations *base.Cancelations\n}\n\nfunc newSubscriber(params subscriberParams) *subscriber {\n\treturn &subscriber{\n\t\tlogger:       params.logger,\n\t\tbroker:       params.broker,\n\t\tdone:         make(chan struct{}),\n\t\tcancelations: params.cancelations,\n\t\tretryTimeout: 5 * time.Second,\n\t}\n}\n\nfunc (s *subscriber) shutdown() {\n\ts.logger.Debug(\"Subscriber shutting down...\")\n\t// Signal the subscriber goroutine to stop.\n\ts.done <- struct{}{}\n}\n\nfunc (s *subscriber) start(wg *sync.WaitGroup) {\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tvar (\n\t\t\tpubsub *redis.PubSub\n\t\t\terr    error\n\t\t)\n\t\t// Try until successfully connect to Redis.\n\t\tfor {\n\t\t\tpubsub, err = s.broker.CancelationPubSub()\n\t\t\tif err != nil {\n\t\t\t\ts.logger.Errorf(\"cannot subscribe to cancelation channel: %v\", err)\n\t\t\t\tselect {\n\t\t\t\tcase <-time.After(s.retryTimeout):\n\t\t\t\t\tcontinue\n\t\t\t\tcase <-s.done:\n\t\t\t\t\ts.logger.Debug(\"Subscriber done\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\tcancelCh := pubsub.Channel()\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-s.done:\n\t\t\t\tpubsub.Close()\n\t\t\t\ts.logger.Debug(\"Subscriber done\")\n\t\t\t\treturn\n\t\t\tcase msg := <-cancelCh:\n\t\t\t\tcancel, ok := s.cancelations.Get(msg.Payload)\n\t\t\t\tif ok {\n\t\t\t\t\tcancel()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n}\n"
        },
        {
          "name": "subscriber_test.go",
          "type": "blob",
          "size": 2.97265625,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/rdb\"\n\t\"github.com/hibiken/asynq/internal/testbroker\"\n)\n\nfunc TestSubscriber(t *testing.T) {\n\tr := setup(t)\n\tdefer r.Close()\n\trdbClient := rdb.NewRDB(r)\n\n\ttests := []struct {\n\t\tregisteredID string // ID for which cancel func is registered\n\t\tpublishID    string // ID to be published\n\t\twantCalled   bool   // whether cancel func should be called\n\t}{\n\t\t{\"abc123\", \"abc123\", true},\n\t\t{\"abc456\", \"abc123\", false},\n\t}\n\n\tfor _, tc := range tests {\n\t\tvar mu sync.Mutex\n\t\tcalled := false\n\t\tfakeCancelFunc := func() {\n\t\t\tmu.Lock()\n\t\t\tdefer mu.Unlock()\n\t\t\tcalled = true\n\t\t}\n\t\tcancelations := base.NewCancelations()\n\t\tcancelations.Add(tc.registeredID, fakeCancelFunc)\n\n\t\tsubscriber := newSubscriber(subscriberParams{\n\t\t\tlogger:       testLogger,\n\t\t\tbroker:       rdbClient,\n\t\t\tcancelations: cancelations,\n\t\t})\n\t\tvar wg sync.WaitGroup\n\t\tsubscriber.start(&wg)\n\t\tdefer subscriber.shutdown()\n\n\t\t// wait for subscriber to establish connection to pubsub channel\n\t\ttime.Sleep(time.Second)\n\n\t\tif err := rdbClient.PublishCancelation(tc.publishID); err != nil {\n\t\t\tt.Fatalf(\"could not publish cancelation message: %v\", err)\n\t\t}\n\n\t\t// wait for redis to publish message\n\t\ttime.Sleep(time.Second)\n\n\t\tmu.Lock()\n\t\tif called != tc.wantCalled {\n\t\t\tif tc.wantCalled {\n\t\t\t\tt.Errorf(\"fakeCancelFunc was not called, want the function to be called\")\n\t\t\t} else {\n\t\t\t\tt.Errorf(\"fakeCancelFunc was called, want the function to not be called\")\n\t\t\t}\n\t\t}\n\t\tmu.Unlock()\n\t}\n}\n\nfunc TestSubscriberWithRedisDown(t *testing.T) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tt.Errorf(\"panic occurred: %v\", r)\n\t\t}\n\t}()\n\tr := rdb.NewRDB(setup(t))\n\tdefer r.Close()\n\ttestBroker := testbroker.NewTestBroker(r)\n\n\tcancelations := base.NewCancelations()\n\tsubscriber := newSubscriber(subscriberParams{\n\t\tlogger:       testLogger,\n\t\tbroker:       testBroker,\n\t\tcancelations: cancelations,\n\t})\n\tsubscriber.retryTimeout = 1 * time.Second // set shorter retry timeout for testing purpose.\n\n\ttestBroker.Sleep() // simulate a situation where subscriber cannot connect to redis.\n\tvar wg sync.WaitGroup\n\tsubscriber.start(&wg)\n\tdefer subscriber.shutdown()\n\n\ttime.Sleep(2 * time.Second) // subscriber should wait and retry connecting to redis.\n\n\ttestBroker.Wakeup() // simulate a situation where redis server is back online.\n\n\ttime.Sleep(2 * time.Second) // allow subscriber to establish pubsub channel.\n\n\tconst id = \"test\"\n\tvar (\n\t\tmu     sync.Mutex\n\t\tcalled bool\n\t)\n\tcancelations.Add(id, func() {\n\t\tmu.Lock()\n\t\tdefer mu.Unlock()\n\t\tcalled = true\n\t})\n\n\tif err := r.PublishCancelation(id); err != nil {\n\t\tt.Fatalf(\"could not publish cancelation message: %v\", err)\n\t}\n\n\ttime.Sleep(time.Second) // wait for redis to publish message.\n\n\tmu.Lock()\n\tif !called {\n\t\tt.Errorf(\"cancel function was not called\")\n\t}\n\tmu.Unlock()\n}\n"
        },
        {
          "name": "syncer.go",
          "type": "blob",
          "size": 1.9541015625,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/hibiken/asynq/internal/log\"\n)\n\n// syncer is responsible for queuing up failed requests to redis and retry\n// those requests to sync state between the background process and redis.\ntype syncer struct {\n\tlogger *log.Logger\n\n\trequestsCh <-chan *syncRequest\n\n\t// channel to communicate back to the long running \"syncer\" goroutine.\n\tdone chan struct{}\n\n\t// interval between sync operations.\n\tinterval time.Duration\n}\n\ntype syncRequest struct {\n\tfn       func() error // sync operation\n\terrMsg   string       // error message\n\tdeadline time.Time    // request should be dropped if deadline has been exceeded\n}\n\ntype syncerParams struct {\n\tlogger     *log.Logger\n\trequestsCh <-chan *syncRequest\n\tinterval   time.Duration\n}\n\nfunc newSyncer(params syncerParams) *syncer {\n\treturn &syncer{\n\t\tlogger:     params.logger,\n\t\trequestsCh: params.requestsCh,\n\t\tdone:       make(chan struct{}),\n\t\tinterval:   params.interval,\n\t}\n}\n\nfunc (s *syncer) shutdown() {\n\ts.logger.Debug(\"Syncer shutting down...\")\n\t// Signal the syncer goroutine to stop.\n\ts.done <- struct{}{}\n}\n\nfunc (s *syncer) start(wg *sync.WaitGroup) {\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tvar requests []*syncRequest\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-s.done:\n\t\t\t\t// Try sync one last time before shutting down.\n\t\t\t\tfor _, req := range requests {\n\t\t\t\t\tif err := req.fn(); err != nil {\n\t\t\t\t\t\ts.logger.Error(req.errMsg)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\ts.logger.Debug(\"Syncer done\")\n\t\t\t\treturn\n\t\t\tcase req := <-s.requestsCh:\n\t\t\t\trequests = append(requests, req)\n\t\t\tcase <-time.After(s.interval):\n\t\t\t\tvar temp []*syncRequest\n\t\t\t\tfor _, req := range requests {\n\t\t\t\t\tif req.deadline.Before(time.Now()) {\n\t\t\t\t\t\tcontinue // drop stale request\n\t\t\t\t\t}\n\t\t\t\t\tif err := req.fn(); err != nil {\n\t\t\t\t\t\ttemp = append(temp, req)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trequests = temp\n\t\t\t}\n\t\t}\n\t}()\n}\n"
        },
        {
          "name": "syncer_test.go",
          "type": "blob",
          "size": 2.9833984375,
          "content": "// Copyright 2020 Kentaro Hibino. All rights reserved.\n// Use of this source code is governed by a MIT license\n// that can be found in the LICENSE file.\n\npackage asynq\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/hibiken/asynq/internal/base\"\n\t\"github.com/hibiken/asynq/internal/rdb\"\n\th \"github.com/hibiken/asynq/internal/testutil\"\n)\n\nfunc TestSyncer(t *testing.T) {\n\tinProgress := []*base.TaskMessage{\n\t\th.NewTaskMessage(\"send_email\", nil),\n\t\th.NewTaskMessage(\"reindex\", nil),\n\t\th.NewTaskMessage(\"gen_thumbnail\", nil),\n\t}\n\tr := setup(t)\n\tdefer r.Close()\n\trdbClient := rdb.NewRDB(r)\n\th.SeedActiveQueue(t, r, inProgress, base.DefaultQueueName)\n\n\tconst interval = time.Second\n\tsyncRequestCh := make(chan *syncRequest)\n\tsyncer := newSyncer(syncerParams{\n\t\tlogger:     testLogger,\n\t\trequestsCh: syncRequestCh,\n\t\tinterval:   interval,\n\t})\n\tvar wg sync.WaitGroup\n\tsyncer.start(&wg)\n\tdefer syncer.shutdown()\n\n\tfor _, msg := range inProgress {\n\t\tm := msg\n\t\tsyncRequestCh <- &syncRequest{\n\t\t\tfn: func() error {\n\t\t\t\treturn rdbClient.Done(context.Background(), m)\n\t\t\t},\n\t\t\tdeadline: time.Now().Add(5 * time.Minute),\n\t\t}\n\t}\n\n\ttime.Sleep(2 * interval) // ensure that syncer runs at least once\n\n\tgotActive := h.GetActiveMessages(t, r, base.DefaultQueueName)\n\tif l := len(gotActive); l != 0 {\n\t\tt.Errorf(\"%q has length %d; want 0\", base.ActiveKey(base.DefaultQueueName), l)\n\t}\n}\n\nfunc TestSyncerRetry(t *testing.T) {\n\tconst interval = time.Second\n\tsyncRequestCh := make(chan *syncRequest)\n\tsyncer := newSyncer(syncerParams{\n\t\tlogger:     testLogger,\n\t\trequestsCh: syncRequestCh,\n\t\tinterval:   interval,\n\t})\n\n\tvar wg sync.WaitGroup\n\tsyncer.start(&wg)\n\tdefer syncer.shutdown()\n\n\tvar (\n\t\tmu      sync.Mutex\n\t\tcounter int\n\t)\n\n\t// Increment the counter for each call.\n\t// Initial call will fail and second call will succeed.\n\trequestFunc := func() error {\n\t\tmu.Lock()\n\t\tdefer mu.Unlock()\n\t\tif counter == 0 {\n\t\t\tcounter++\n\t\t\treturn fmt.Errorf(\"zero\")\n\t\t}\n\t\tcounter++\n\t\treturn nil\n\t}\n\n\tsyncRequestCh <- &syncRequest{\n\t\tfn:       requestFunc,\n\t\terrMsg:   \"error\",\n\t\tdeadline: time.Now().Add(5 * time.Minute),\n\t}\n\n\t// allow syncer to retry\n\ttime.Sleep(3 * interval)\n\n\tmu.Lock()\n\tif counter != 2 {\n\t\tt.Errorf(\"counter = %d, want 2\", counter)\n\t}\n\tmu.Unlock()\n}\n\nfunc TestSyncerDropsStaleRequests(t *testing.T) {\n\tconst interval = time.Second\n\tsyncRequestCh := make(chan *syncRequest)\n\tsyncer := newSyncer(syncerParams{\n\t\tlogger:     testLogger,\n\t\trequestsCh: syncRequestCh,\n\t\tinterval:   interval,\n\t})\n\tvar wg sync.WaitGroup\n\tsyncer.start(&wg)\n\n\tvar (\n\t\tmu sync.Mutex\n\t\tn  int // number of times request has been processed\n\t)\n\n\tfor i := 0; i < 10; i++ {\n\t\tsyncRequestCh <- &syncRequest{\n\t\t\tfn: func() error {\n\t\t\t\tmu.Lock()\n\t\t\t\tn++\n\t\t\t\tmu.Unlock()\n\t\t\t\treturn nil\n\t\t\t},\n\t\t\tdeadline: time.Now().Add(time.Duration(-i) * time.Second), // already exceeded deadline\n\t\t}\n\t}\n\n\ttime.Sleep(2 * interval) // ensure that syncer runs at least once\n\tsyncer.shutdown()\n\n\tmu.Lock()\n\tif n != 0 {\n\t\tt.Errorf(\"requests has been processed %d times, want 0\", n)\n\t}\n\tmu.Unlock()\n}\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "x",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}