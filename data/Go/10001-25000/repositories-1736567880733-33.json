{
  "metadata": {
    "timestamp": 1736567880733,
    "page": 33,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "grpc/grpc-go",
      "stars": 21281,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": "AUTHORS",
          "type": "blob",
          "size": 0.01171875,
          "content": "Google Inc.\n"
        },
        {
          "name": "CODE-OF-CONDUCT.md",
          "type": "blob",
          "size": 0.134765625,
          "content": "## Community Code of Conduct\n\ngRPC follows the [CNCF Code of Conduct](https://github.com/cncf/foundation/blob/master/code-of-conduct.md).\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 3.744140625,
          "content": "# How to contribute\n\nWe definitely welcome your patches and contributions to gRPC! Please read the gRPC\norganization's [governance rules](https://github.com/grpc/grpc-community/blob/master/governance.md)\nand [contribution guidelines](https://github.com/grpc/grpc-community/blob/master/CONTRIBUTING.md) before proceeding.\n\nIf you are new to GitHub, please start by reading [Pull Request howto](https://help.github.com/articles/about-pull-requests/)\n\n## Legal requirements\n\nIn order to protect both you and ourselves, you will need to sign the\n[Contributor License Agreement](https://identity.linuxfoundation.org/projects/cncf).\n\n## Guidelines for Pull Requests\nHow to get your contributions merged smoothly and quickly.\n\n- Create **small PRs** that are narrowly focused on **addressing a single\n  concern**. We often times receive PRs that are trying to fix several things at\n  a time, but only one fix is considered acceptable, nothing gets merged and\n  both author's & review's time is wasted. Create more PRs to address different\n  concerns and everyone will be happy.\n\n- If you are searching for features to work on, issues labeled [Status: Help\n  Wanted](https://github.com/grpc/grpc-go/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3A%22Status%3A+Help+Wanted%22)\n  is a great place to start. These issues are well-documented and usually can be\n  resolved with a single pull request.\n\n- If you are adding a new file, make sure it has the copyright message template\n  at the top as a comment. You can copy over the message from an existing file\n  and update the year.\n\n- The grpc package should only depend on standard Go packages and a small number\n  of exceptions. If your contribution introduces new dependencies which are NOT\n  in the [list](https://godoc.org/google.golang.org/grpc?imports), you need a\n  discussion with gRPC-Go authors and consultants.\n\n- For speculative changes, consider opening an issue and discussing it first. If\n  you are suggesting a behavioral or API change, consider starting with a [gRFC\n  proposal](https://github.com/grpc/proposal).\n\n- Provide a good **PR description** as a record of **what** change is being made\n  and **why** it was made. Link to a GitHub issue if it exists.\n\n- If you want to fix formatting or style, consider whether your changes are an\n  obvious improvement or might be considered a personal preference. If a style\n  change is based on preference, it likely will not be accepted. If it corrects\n  widely agreed-upon anti-patterns, then please do create a PR and explain the\n  benefits of the change.\n\n- Unless your PR is trivial, you should expect there will be reviewer comments\n  that you'll need to address before merging. We'll mark it as `Status: Requires\n  Reporter Clarification` if we expect you to respond to these comments in a\n  timely manner. If the PR remains inactive for 6 days, it will be marked as\n  `stale` and automatically close 7 days after that if we don't hear back from\n  you.\n\n- Maintain **clean commit history** and use **meaningful commit messages**. PRs\n  with messy commit history are difficult to review and won't be merged. Use\n  `rebase -i upstream/master` to curate your commit history and/or to bring in\n  latest changes from master (but avoid rebasing in the middle of a code\n  review).\n\n- Keep your PR up to date with upstream/master (if there are merge conflicts, we\n  can't really merge your change).\n\n- **All tests need to be passing** before your change can be merged. We\n  recommend you **run tests locally** before creating your PR to catch breakages\n  early on.\n  - `./scripts/vet.sh` to catch vet errors\n  - `go test -cpu 1,4 -timeout 7m ./...` to run the tests\n  - `go test -race -cpu 1,4 -timeout 7m ./...` to run tests in race mode\n\n- Exceptions to the rules can be made if there's a compelling reason for doing so.\n"
        },
        {
          "name": "Documentation",
          "type": "tree",
          "content": null
        },
        {
          "name": "GOVERNANCE.md",
          "type": "blob",
          "size": 0.1376953125,
          "content": "This repository is governed by the gRPC organization's [governance rules](https://github.com/grpc/grpc-community/blob/master/governance.md).\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MAINTAINERS.md",
          "type": "blob",
          "size": 1.58203125,
          "content": "This page lists all active maintainers of this repository. If you were a\nmaintainer and would like to add your name to the Emeritus list, please send us a\nPR.\n\nSee [GOVERNANCE.md](https://github.com/grpc/grpc-community/blob/master/governance.md)\nfor governance guidelines and how to become a maintainer.\nSee [CONTRIBUTING.md](https://github.com/grpc/grpc-community/blob/master/CONTRIBUTING.md)\nfor general contribution guidelines.\n\n## Maintainers (in alphabetical order)\n\n- [aranjans](https://github.com/aranjans), Google LLC\n- [arjan-bal](https://github.com/arjan-bal), Google LLC\n- [arvindbr8](https://github.com/arvindbr8), Google LLC\n- [atollena](https://github.com/atollena), Datadog, Inc.\n- [dfawley](https://github.com/dfawley), Google LLC\n- [easwars](https://github.com/easwars), Google LLC\n- [erm-g](https://github.com/erm-g), Google LLC\n- [gtcooke94](https://github.com/gtcooke94), Google LLC\n- [purnesh42h](https://github.com/purnesh42h), Google LLC\n- [zasweq](https://github.com/zasweq), Google LLC\n\n## Emeritus Maintainers (in alphabetical order)\n- [adelez](https://github.com/adelez)\n- [canguler](https://github.com/canguler)\n- [cesarghali](https://github.com/cesarghali)\n- [iamqizhao](https://github.com/iamqizhao)\n- [jeanbza](https://github.com/jeanbza)\n- [jtattermusch](https://github.com/jtattermusch)\n- [lyuxuan](https://github.com/lyuxuan)\n- [makmukhi](https://github.com/makmukhi)\n- [matt-kwong](https://github.com/matt-kwong)\n- [menghanl](https://github.com/menghanl)\n- [nicolasnoble](https://github.com/nicolasnoble)\n- [srini100](https://github.com/srini100)\n- [yongni](https://github.com/yongni)\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.9306640625,
          "content": "all: vet test testrace\n\nbuild:\n\tgo build google.golang.org/grpc/...\n\nclean:\n\tgo clean -i google.golang.org/grpc/...\n\ndeps:\n\tGO111MODULE=on go get -d -v google.golang.org/grpc/...\n\nproto:\n\t@ if ! which protoc > /dev/null; then \\\n\t\techo \"error: protoc not installed\" >&2; \\\n\t\texit 1; \\\n\tfi\n\tgo generate google.golang.org/grpc/...\n\ntest:\n\tgo test -cpu 1,4 -timeout 7m google.golang.org/grpc/...\n\ntestsubmodule:\n\tcd security/advancedtls && go test -cpu 1,4 -timeout 7m google.golang.org/grpc/security/advancedtls/...\n\tcd security/authorization && go test -cpu 1,4 -timeout 7m google.golang.org/grpc/security/authorization/...\n\ntestrace:\n\tgo test -race -cpu 1,4 -timeout 7m google.golang.org/grpc/...\n\ntestdeps:\n\tGO111MODULE=on go get -d -v -t google.golang.org/grpc/...\n\nvet: vetdeps\n\t./scripts/vet.sh\n\nvetdeps:\n\t./scripts/vet.sh -install\n\n.PHONY: \\\n\tall \\\n\tbuild \\\n\tclean \\\n\tdeps \\\n\tproto \\\n\ttest \\\n\ttestsubmodule \\\n\ttestrace \\\n\ttestdeps \\\n\tvet \\\n\tvetdeps\n"
        },
        {
          "name": "NOTICE.txt",
          "type": "blob",
          "size": 0.541015625,
          "content": "Copyright 2014 gRPC authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.8515625,
          "content": "# gRPC-Go\n\n[![GoDoc](https://pkg.go.dev/badge/google.golang.org/grpc)][API]\n[![GoReportCard](https://goreportcard.com/badge/grpc/grpc-go)](https://goreportcard.com/report/github.com/grpc/grpc-go)\n[![codecov](https://codecov.io/gh/grpc/grpc-go/graph/badge.svg)](https://codecov.io/gh/grpc/grpc-go)\n\nThe [Go][] implementation of [gRPC][]: A high performance, open source, general\nRPC framework that puts mobile and HTTP/2 first. For more information see the\n[Go gRPC docs][], or jump directly into the [quick start][].\n\n## Prerequisites\n\n- **[Go][]**: any one of the **two latest major** [releases][go-releases].\n\n## Installation\n\nSimply add the following import to your code, and then `go [build|run|test]`\nwill automatically fetch the necessary dependencies:\n\n\n```go\nimport \"google.golang.org/grpc\"\n```\n\n> **Note:** If you are trying to access `grpc-go` from **China**, see the\n> [FAQ](#FAQ) below.\n\n## Learn more\n\n- [Go gRPC docs][], which include a [quick start][] and [API\n  reference][API] among other resources\n- [Low-level technical docs](Documentation) from this repository\n- [Performance benchmark][]\n- [Examples](examples)\n\n## FAQ\n\n### I/O Timeout Errors\n\nThe `golang.org` domain may be blocked from some countries. `go get` usually\nproduces an error like the following when this happens:\n\n```console\n$ go get -u google.golang.org/grpc\npackage google.golang.org/grpc: unrecognized import path \"google.golang.org/grpc\" (https fetch: Get https://google.golang.org/grpc?go-get=1: dial tcp 216.239.37.1:443: i/o timeout)\n```\n\nTo build Go code, there are several options:\n\n- Set up a VPN and access google.golang.org through that.\n\n- With Go module support: it is possible to use the `replace` feature of `go\n  mod` to create aliases for golang.org packages.  In your project's directory:\n\n  ```sh\n  go mod edit -replace=google.golang.org/grpc=github.com/grpc/grpc-go@latest\n  go mod tidy\n  go mod vendor\n  go build -mod=vendor\n  ```\n\n  Again, this will need to be done for all transitive dependencies hosted on\n  golang.org as well. For details, refer to [golang/go issue\n  #28652](https://github.com/golang/go/issues/28652).\n\n### Compiling error, undefined: grpc.SupportPackageIsVersion\n\nPlease update to the latest version of gRPC-Go using\n`go get google.golang.org/grpc`.\n\n### How to turn on logging\n\nThe default logger is controlled by environment variables. Turn everything on\nlike this:\n\n```console\n$ export GRPC_GO_LOG_VERBOSITY_LEVEL=99\n$ export GRPC_GO_LOG_SEVERITY_LEVEL=info\n```\n\n### The RPC failed with error `\"code = Unavailable desc = transport is closing\"`\n\nThis error means the connection the RPC is using was closed, and there are many\npossible reasons, including:\n 1. mis-configured transport credentials, connection failed on handshaking\n 1. bytes disrupted, possibly by a proxy in between\n 1. server shutdown\n 1. Keepalive parameters caused connection shutdown, for example if you have\n    configured your server to terminate connections regularly to [trigger DNS\n    lookups](https://github.com/grpc/grpc-go/issues/3170#issuecomment-552517779).\n    If this is the case, you may want to increase your\n    [MaxConnectionAgeGrace](https://pkg.go.dev/google.golang.org/grpc/keepalive?tab=doc#ServerParameters),\n    to allow longer RPC calls to finish.\n\nIt can be tricky to debug this because the error happens on the client side but\nthe root cause of the connection being closed is on the server side. Turn on\nlogging on __both client and server__, and see if there are any transport\nerrors.\n\n[API]: https://pkg.go.dev/google.golang.org/grpc\n[Go]: https://golang.org\n[Go module]: https://github.com/golang/go/wiki/Modules\n[gRPC]: https://grpc.io\n[Go gRPC docs]: https://grpc.io/docs/languages/go\n[Performance benchmark]: https://performance-dot-grpc-testing.appspot.com/explore?dashboard=5180705743044608\n[quick start]: https://grpc.io/docs/languages/go/quickstart\n[go-releases]: https://golang.org/doc/devel/release.html\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.1953125,
          "content": "# Security Policy\n\nFor information on gRPC Security Policy and reporting potential security issues, please see [gRPC CVE Process](https://github.com/grpc/proposal/blob/master/P4-grpc-cve-process.md).\n"
        },
        {
          "name": "admin",
          "type": "tree",
          "content": null
        },
        {
          "name": "attributes",
          "type": "tree",
          "content": null
        },
        {
          "name": "authz",
          "type": "tree",
          "content": null
        },
        {
          "name": "backoff.go",
          "type": "blob",
          "size": 1.94921875,
          "content": "/*\n *\n * Copyright 2017 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\n// See internal/backoff package for the backoff implementation. This file is\n// kept for the exported types and API backward compatibility.\n\npackage grpc\n\nimport (\n\t\"time\"\n\n\t\"google.golang.org/grpc/backoff\"\n)\n\n// DefaultBackoffConfig uses values specified for backoff in\n// https://github.com/grpc/grpc/blob/master/doc/connection-backoff.md.\n//\n// Deprecated: use ConnectParams instead. Will be supported throughout 1.x.\nvar DefaultBackoffConfig = BackoffConfig{\n\tMaxDelay: 120 * time.Second,\n}\n\n// BackoffConfig defines the parameters for the default gRPC backoff strategy.\n//\n// Deprecated: use ConnectParams instead. Will be supported throughout 1.x.\ntype BackoffConfig struct {\n\t// MaxDelay is the upper bound of backoff delay.\n\tMaxDelay time.Duration\n}\n\n// ConnectParams defines the parameters for connecting and retrying. Users are\n// encouraged to use this instead of the BackoffConfig type defined above. See\n// here for more details:\n// https://github.com/grpc/grpc/blob/master/doc/connection-backoff.md.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype ConnectParams struct {\n\t// Backoff specifies the configuration options for connection backoff.\n\tBackoff backoff.Config\n\t// MinConnectTimeout is the minimum amount of time we are willing to give a\n\t// connection to complete.\n\tMinConnectTimeout time.Duration\n}\n"
        },
        {
          "name": "backoff",
          "type": "tree",
          "content": null
        },
        {
          "name": "balancer",
          "type": "tree",
          "content": null
        },
        {
          "name": "balancer_wrapper.go",
          "type": "blob",
          "size": 17.4892578125,
          "content": "/*\n *\n * Copyright 2017 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\n\t\"google.golang.org/grpc/balancer\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/connectivity\"\n\t\"google.golang.org/grpc/internal\"\n\t\"google.golang.org/grpc/internal/balancer/gracefulswitch\"\n\t\"google.golang.org/grpc/internal/channelz\"\n\t\"google.golang.org/grpc/internal/grpcsync\"\n\t\"google.golang.org/grpc/resolver\"\n\t\"google.golang.org/grpc/status\"\n)\n\nvar (\n\tsetConnectedAddress = internal.SetConnectedAddress.(func(*balancer.SubConnState, resolver.Address))\n\t// noOpRegisterHealthListenerFn is used when client side health checking is\n\t// disabled. It sends a single READY update on the registered listener.\n\tnoOpRegisterHealthListenerFn = func(_ context.Context, listener func(balancer.SubConnState)) func() {\n\t\tlistener(balancer.SubConnState{ConnectivityState: connectivity.Ready})\n\t\treturn func() {}\n\t}\n)\n\n// ccBalancerWrapper sits between the ClientConn and the Balancer.\n//\n// ccBalancerWrapper implements methods corresponding to the ones on the\n// balancer.Balancer interface. The ClientConn is free to call these methods\n// concurrently and the ccBalancerWrapper ensures that calls from the ClientConn\n// to the Balancer happen in order by performing them in the serializer, without\n// any mutexes held.\n//\n// ccBalancerWrapper also implements the balancer.ClientConn interface and is\n// passed to the Balancer implementations. It invokes unexported methods on the\n// ClientConn to handle these calls from the Balancer.\n//\n// It uses the gracefulswitch.Balancer internally to ensure that balancer\n// switches happen in a graceful manner.\ntype ccBalancerWrapper struct {\n\t// The following fields are initialized when the wrapper is created and are\n\t// read-only afterwards, and therefore can be accessed without a mutex.\n\tcc               *ClientConn\n\topts             balancer.BuildOptions\n\tserializer       *grpcsync.CallbackSerializer\n\tserializerCancel context.CancelFunc\n\n\t// The following fields are only accessed within the serializer or during\n\t// initialization.\n\tcurBalancerName string\n\tbalancer        *gracefulswitch.Balancer\n\n\t// The following field is protected by mu.  Caller must take cc.mu before\n\t// taking mu.\n\tmu     sync.Mutex\n\tclosed bool\n}\n\n// newCCBalancerWrapper creates a new balancer wrapper in idle state. The\n// underlying balancer is not created until the updateClientConnState() method\n// is invoked.\nfunc newCCBalancerWrapper(cc *ClientConn) *ccBalancerWrapper {\n\tctx, cancel := context.WithCancel(cc.ctx)\n\tccb := &ccBalancerWrapper{\n\t\tcc: cc,\n\t\topts: balancer.BuildOptions{\n\t\t\tDialCreds:       cc.dopts.copts.TransportCredentials,\n\t\t\tCredsBundle:     cc.dopts.copts.CredsBundle,\n\t\t\tDialer:          cc.dopts.copts.Dialer,\n\t\t\tAuthority:       cc.authority,\n\t\t\tCustomUserAgent: cc.dopts.copts.UserAgent,\n\t\t\tChannelzParent:  cc.channelz,\n\t\t\tTarget:          cc.parsedTarget,\n\t\t\tMetricsRecorder: cc.metricsRecorderList,\n\t\t},\n\t\tserializer:       grpcsync.NewCallbackSerializer(ctx),\n\t\tserializerCancel: cancel,\n\t}\n\tccb.balancer = gracefulswitch.NewBalancer(ccb, ccb.opts)\n\treturn ccb\n}\n\n// updateClientConnState is invoked by grpc to push a ClientConnState update to\n// the underlying balancer.  This is always executed from the serializer, so\n// it is safe to call into the balancer here.\nfunc (ccb *ccBalancerWrapper) updateClientConnState(ccs *balancer.ClientConnState) error {\n\terrCh := make(chan error)\n\tuccs := func(ctx context.Context) {\n\t\tdefer close(errCh)\n\t\tif ctx.Err() != nil || ccb.balancer == nil {\n\t\t\treturn\n\t\t}\n\t\tname := gracefulswitch.ChildName(ccs.BalancerConfig)\n\t\tif ccb.curBalancerName != name {\n\t\t\tccb.curBalancerName = name\n\t\t\tchannelz.Infof(logger, ccb.cc.channelz, \"Channel switches to new LB policy %q\", name)\n\t\t}\n\t\terr := ccb.balancer.UpdateClientConnState(*ccs)\n\t\tif logger.V(2) && err != nil {\n\t\t\tlogger.Infof(\"error from balancer.UpdateClientConnState: %v\", err)\n\t\t}\n\t\terrCh <- err\n\t}\n\tonFailure := func() { close(errCh) }\n\n\t// UpdateClientConnState can race with Close, and when the latter wins, the\n\t// serializer is closed, and the attempt to schedule the callback will fail.\n\t// It is acceptable to ignore this failure. But since we want to handle the\n\t// state update in a blocking fashion (when we successfully schedule the\n\t// callback), we have to use the ScheduleOr method and not the MaybeSchedule\n\t// method on the serializer.\n\tccb.serializer.ScheduleOr(uccs, onFailure)\n\treturn <-errCh\n}\n\n// resolverError is invoked by grpc to push a resolver error to the underlying\n// balancer.  The call to the balancer is executed from the serializer.\nfunc (ccb *ccBalancerWrapper) resolverError(err error) {\n\tccb.serializer.TrySchedule(func(ctx context.Context) {\n\t\tif ctx.Err() != nil || ccb.balancer == nil {\n\t\t\treturn\n\t\t}\n\t\tccb.balancer.ResolverError(err)\n\t})\n}\n\n// close initiates async shutdown of the wrapper.  cc.mu must be held when\n// calling this function.  To determine the wrapper has finished shutting down,\n// the channel should block on ccb.serializer.Done() without cc.mu held.\nfunc (ccb *ccBalancerWrapper) close() {\n\tccb.mu.Lock()\n\tccb.closed = true\n\tccb.mu.Unlock()\n\tchannelz.Info(logger, ccb.cc.channelz, \"ccBalancerWrapper: closing\")\n\tccb.serializer.TrySchedule(func(context.Context) {\n\t\tif ccb.balancer == nil {\n\t\t\treturn\n\t\t}\n\t\tccb.balancer.Close()\n\t\tccb.balancer = nil\n\t})\n\tccb.serializerCancel()\n}\n\n// exitIdle invokes the balancer's exitIdle method in the serializer.\nfunc (ccb *ccBalancerWrapper) exitIdle() {\n\tccb.serializer.TrySchedule(func(ctx context.Context) {\n\t\tif ctx.Err() != nil || ccb.balancer == nil {\n\t\t\treturn\n\t\t}\n\t\tccb.balancer.ExitIdle()\n\t})\n}\n\nfunc (ccb *ccBalancerWrapper) NewSubConn(addrs []resolver.Address, opts balancer.NewSubConnOptions) (balancer.SubConn, error) {\n\tccb.cc.mu.Lock()\n\tdefer ccb.cc.mu.Unlock()\n\n\tccb.mu.Lock()\n\tif ccb.closed {\n\t\tccb.mu.Unlock()\n\t\treturn nil, fmt.Errorf(\"balancer is being closed; no new SubConns allowed\")\n\t}\n\tccb.mu.Unlock()\n\n\tif len(addrs) == 0 {\n\t\treturn nil, fmt.Errorf(\"grpc: cannot create SubConn with empty address list\")\n\t}\n\tac, err := ccb.cc.newAddrConnLocked(addrs, opts)\n\tif err != nil {\n\t\tchannelz.Warningf(logger, ccb.cc.channelz, \"acBalancerWrapper: NewSubConn: failed to newAddrConn: %v\", err)\n\t\treturn nil, err\n\t}\n\tacbw := &acBalancerWrapper{\n\t\tccb:           ccb,\n\t\tac:            ac,\n\t\tproducers:     make(map[balancer.ProducerBuilder]*refCountedProducer),\n\t\tstateListener: opts.StateListener,\n\t\thealthData:    newHealthData(connectivity.Idle),\n\t}\n\tac.acbw = acbw\n\treturn acbw, nil\n}\n\nfunc (ccb *ccBalancerWrapper) RemoveSubConn(balancer.SubConn) {\n\t// The graceful switch balancer will never call this.\n\tlogger.Errorf(\"ccb RemoveSubConn(%v) called unexpectedly, sc\")\n}\n\nfunc (ccb *ccBalancerWrapper) UpdateAddresses(sc balancer.SubConn, addrs []resolver.Address) {\n\tacbw, ok := sc.(*acBalancerWrapper)\n\tif !ok {\n\t\treturn\n\t}\n\tacbw.UpdateAddresses(addrs)\n}\n\nfunc (ccb *ccBalancerWrapper) UpdateState(s balancer.State) {\n\tccb.cc.mu.Lock()\n\tdefer ccb.cc.mu.Unlock()\n\tif ccb.cc.conns == nil {\n\t\t// The CC has been closed; ignore this update.\n\t\treturn\n\t}\n\n\tccb.mu.Lock()\n\tif ccb.closed {\n\t\tccb.mu.Unlock()\n\t\treturn\n\t}\n\tccb.mu.Unlock()\n\t// Update picker before updating state.  Even though the ordering here does\n\t// not matter, it can lead to multiple calls of Pick in the common start-up\n\t// case where we wait for ready and then perform an RPC.  If the picker is\n\t// updated later, we could call the \"connecting\" picker when the state is\n\t// updated, and then call the \"ready\" picker after the picker gets updated.\n\n\t// Note that there is no need to check if the balancer wrapper was closed,\n\t// as we know the graceful switch LB policy will not call cc if it has been\n\t// closed.\n\tccb.cc.pickerWrapper.updatePicker(s.Picker)\n\tccb.cc.csMgr.updateState(s.ConnectivityState)\n}\n\nfunc (ccb *ccBalancerWrapper) ResolveNow(o resolver.ResolveNowOptions) {\n\tccb.cc.mu.RLock()\n\tdefer ccb.cc.mu.RUnlock()\n\n\tccb.mu.Lock()\n\tif ccb.closed {\n\t\tccb.mu.Unlock()\n\t\treturn\n\t}\n\tccb.mu.Unlock()\n\tccb.cc.resolveNowLocked(o)\n}\n\nfunc (ccb *ccBalancerWrapper) Target() string {\n\treturn ccb.cc.target\n}\n\n// acBalancerWrapper is a wrapper on top of ac for balancers.\n// It implements balancer.SubConn interface.\ntype acBalancerWrapper struct {\n\tinternal.EnforceSubConnEmbedding\n\tac            *addrConn          // read-only\n\tccb           *ccBalancerWrapper // read-only\n\tstateListener func(balancer.SubConnState)\n\n\tproducersMu sync.Mutex\n\tproducers   map[balancer.ProducerBuilder]*refCountedProducer\n\n\t// Access to healthData is protected by healthMu.\n\thealthMu sync.Mutex\n\t// healthData is stored as a pointer to detect when the health listener is\n\t// dropped or updated. This is required as closures can't be compared for\n\t// equality.\n\thealthData *healthData\n}\n\n// healthData holds data related to health state reporting.\ntype healthData struct {\n\t// connectivityState stores the most recent connectivity state delivered\n\t// to the LB policy. This is stored to avoid sending updates when the\n\t// SubConn has already exited connectivity state READY.\n\tconnectivityState connectivity.State\n\t// closeHealthProducer stores function to close the ref counted health\n\t// producer. The health producer is automatically closed when the SubConn\n\t// state changes.\n\tcloseHealthProducer func()\n}\n\nfunc newHealthData(s connectivity.State) *healthData {\n\treturn &healthData{\n\t\tconnectivityState:   s,\n\t\tcloseHealthProducer: func() {},\n\t}\n}\n\n// updateState is invoked by grpc to push a subConn state update to the\n// underlying balancer.\nfunc (acbw *acBalancerWrapper) updateState(s connectivity.State, curAddr resolver.Address, err error) {\n\tacbw.ccb.serializer.TrySchedule(func(ctx context.Context) {\n\t\tif ctx.Err() != nil || acbw.ccb.balancer == nil {\n\t\t\treturn\n\t\t}\n\t\t// Invalidate all producers on any state change.\n\t\tacbw.closeProducers()\n\n\t\t// Even though it is optional for balancers, gracefulswitch ensures\n\t\t// opts.StateListener is set, so this cannot ever be nil.\n\t\t// TODO: delete this comment when UpdateSubConnState is removed.\n\t\tscs := balancer.SubConnState{ConnectivityState: s, ConnectionError: err}\n\t\tif s == connectivity.Ready {\n\t\t\tsetConnectedAddress(&scs, curAddr)\n\t\t}\n\t\t// Invalidate the health listener by updating the healthData.\n\t\tacbw.healthMu.Lock()\n\t\t// A race may occur if a health listener is registered soon after the\n\t\t// connectivity state is set but before the stateListener is called.\n\t\t// Two cases may arise:\n\t\t// 1. The new state is not READY: RegisterHealthListener has checks to\n\t\t//    ensure no updates are sent when the connectivity state is not\n\t\t//    READY.\n\t\t// 2. The new state is READY: This means that the old state wasn't Ready.\n\t\t//    The RegisterHealthListener API mentions that a health listener\n\t\t//    must not be registered when a SubConn is not ready to avoid such\n\t\t//    races. When this happens, the LB policy would get health updates\n\t\t//    on the old listener. When the LB policy registers a new listener\n\t\t//    on receiving the connectivity update, the health updates will be\n\t\t//    sent to the new health listener.\n\t\tacbw.healthData = newHealthData(scs.ConnectivityState)\n\t\tacbw.healthMu.Unlock()\n\n\t\tacbw.stateListener(scs)\n\t})\n}\n\nfunc (acbw *acBalancerWrapper) String() string {\n\treturn fmt.Sprintf(\"SubConn(id:%d)\", acbw.ac.channelz.ID)\n}\n\nfunc (acbw *acBalancerWrapper) UpdateAddresses(addrs []resolver.Address) {\n\tacbw.ac.updateAddrs(addrs)\n}\n\nfunc (acbw *acBalancerWrapper) Connect() {\n\tgo acbw.ac.connect()\n}\n\nfunc (acbw *acBalancerWrapper) Shutdown() {\n\tacbw.closeProducers()\n\tacbw.ccb.cc.removeAddrConn(acbw.ac, errConnDrain)\n}\n\n// NewStream begins a streaming RPC on the addrConn.  If the addrConn is not\n// ready, blocks until it is or ctx expires.  Returns an error when the context\n// expires or the addrConn is shut down.\nfunc (acbw *acBalancerWrapper) NewStream(ctx context.Context, desc *StreamDesc, method string, opts ...CallOption) (ClientStream, error) {\n\ttransport := acbw.ac.getReadyTransport()\n\tif transport == nil {\n\t\treturn nil, status.Errorf(codes.Unavailable, \"SubConn state is not Ready\")\n\n\t}\n\treturn newNonRetryClientStream(ctx, desc, method, transport, acbw.ac, opts...)\n}\n\n// Invoke performs a unary RPC.  If the addrConn is not ready, returns\n// errSubConnNotReady.\nfunc (acbw *acBalancerWrapper) Invoke(ctx context.Context, method string, args any, reply any, opts ...CallOption) error {\n\tcs, err := acbw.NewStream(ctx, unaryStreamDesc, method, opts...)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := cs.SendMsg(args); err != nil {\n\t\treturn err\n\t}\n\treturn cs.RecvMsg(reply)\n}\n\ntype refCountedProducer struct {\n\tproducer balancer.Producer\n\trefs     int    // number of current refs to the producer\n\tclose    func() // underlying producer's close function\n}\n\nfunc (acbw *acBalancerWrapper) GetOrBuildProducer(pb balancer.ProducerBuilder) (balancer.Producer, func()) {\n\tacbw.producersMu.Lock()\n\tdefer acbw.producersMu.Unlock()\n\n\t// Look up existing producer from this builder.\n\tpData := acbw.producers[pb]\n\tif pData == nil {\n\t\t// Not found; create a new one and add it to the producers map.\n\t\tp, closeFn := pb.Build(acbw)\n\t\tpData = &refCountedProducer{producer: p, close: closeFn}\n\t\tacbw.producers[pb] = pData\n\t}\n\t// Account for this new reference.\n\tpData.refs++\n\n\t// Return a cleanup function wrapped in a OnceFunc to remove this reference\n\t// and delete the refCountedProducer from the map if the total reference\n\t// count goes to zero.\n\tunref := func() {\n\t\tacbw.producersMu.Lock()\n\t\t// If closeProducers has already closed this producer instance, refs is\n\t\t// set to 0, so the check after decrementing will never pass, and the\n\t\t// producer will not be double-closed.\n\t\tpData.refs--\n\t\tif pData.refs == 0 {\n\t\t\tdefer pData.close() // Run outside the acbw mutex\n\t\t\tdelete(acbw.producers, pb)\n\t\t}\n\t\tacbw.producersMu.Unlock()\n\t}\n\treturn pData.producer, grpcsync.OnceFunc(unref)\n}\n\nfunc (acbw *acBalancerWrapper) closeProducers() {\n\tacbw.producersMu.Lock()\n\tdefer acbw.producersMu.Unlock()\n\tfor pb, pData := range acbw.producers {\n\t\tpData.refs = 0\n\t\tpData.close()\n\t\tdelete(acbw.producers, pb)\n\t}\n}\n\n// healthProducerRegisterFn is a type alias for the health producer's function\n// for registering listeners.\ntype healthProducerRegisterFn = func(context.Context, balancer.SubConn, string, func(balancer.SubConnState)) func()\n\n// healthListenerRegFn returns a function to register a listener for health\n// updates. If client side health checks are disabled, the registered listener\n// will get a single READY (raw connectivity state) update.\n//\n// Client side health checking is enabled when all the following\n// conditions are satisfied:\n// 1. Health checking is not disabled using the dial option.\n// 2. The health package is imported.\n// 3. The health check config is present in the service config.\nfunc (acbw *acBalancerWrapper) healthListenerRegFn() func(context.Context, func(balancer.SubConnState)) func() {\n\tif acbw.ccb.cc.dopts.disableHealthCheck {\n\t\treturn noOpRegisterHealthListenerFn\n\t}\n\tregHealthLisFn := internal.RegisterClientHealthCheckListener\n\tif regHealthLisFn == nil {\n\t\t// The health package is not imported.\n\t\treturn noOpRegisterHealthListenerFn\n\t}\n\tcfg := acbw.ac.cc.healthCheckConfig()\n\tif cfg == nil {\n\t\treturn noOpRegisterHealthListenerFn\n\t}\n\treturn func(ctx context.Context, listener func(balancer.SubConnState)) func() {\n\t\treturn regHealthLisFn.(healthProducerRegisterFn)(ctx, acbw, cfg.ServiceName, listener)\n\t}\n}\n\n// RegisterHealthListener accepts a health listener from the LB policy. It sends\n// updates to the health listener as long as the SubConn's connectivity state\n// doesn't change and a new health listener is not registered. To invalidate\n// the currently registered health listener, acbw updates the healthData. If a\n// nil listener is registered, the active health listener is dropped.\nfunc (acbw *acBalancerWrapper) RegisterHealthListener(listener func(balancer.SubConnState)) {\n\tacbw.healthMu.Lock()\n\tdefer acbw.healthMu.Unlock()\n\tacbw.healthData.closeHealthProducer()\n\t// listeners should not be registered when the connectivity state\n\t// isn't Ready. This may happen when the balancer registers a listener\n\t// after the connectivityState is updated, but before it is notified\n\t// of the update.\n\tif acbw.healthData.connectivityState != connectivity.Ready {\n\t\treturn\n\t}\n\t// Replace the health data to stop sending updates to any previously\n\t// registered health listeners.\n\thd := newHealthData(connectivity.Ready)\n\tacbw.healthData = hd\n\tif listener == nil {\n\t\treturn\n\t}\n\n\tregisterFn := acbw.healthListenerRegFn()\n\tacbw.ccb.serializer.TrySchedule(func(ctx context.Context) {\n\t\tif ctx.Err() != nil || acbw.ccb.balancer == nil {\n\t\t\treturn\n\t\t}\n\t\t// Don't send updates if a new listener is registered.\n\t\tacbw.healthMu.Lock()\n\t\tdefer acbw.healthMu.Unlock()\n\t\tif acbw.healthData != hd {\n\t\t\treturn\n\t\t}\n\t\t// Serialize the health updates from the health producer with\n\t\t// other calls into the LB policy.\n\t\tlistenerWrapper := func(scs balancer.SubConnState) {\n\t\t\tacbw.ccb.serializer.TrySchedule(func(ctx context.Context) {\n\t\t\t\tif ctx.Err() != nil || acbw.ccb.balancer == nil {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tacbw.healthMu.Lock()\n\t\t\t\tdefer acbw.healthMu.Unlock()\n\t\t\t\tif acbw.healthData != hd {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tlistener(scs)\n\t\t\t})\n\t\t}\n\n\t\thd.closeHealthProducer = registerFn(ctx, listenerWrapper)\n\t})\n}\n"
        },
        {
          "name": "balancer_wrapper_test.go",
          "type": "blob",
          "size": 2.62109375,
          "content": "/*\n *\n * Copyright 2023 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\n\t\"google.golang.org/grpc/balancer\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n\t\"google.golang.org/grpc/internal/balancer/stub\"\n\t\"google.golang.org/grpc/internal/grpcsync\"\n)\n\n// TestBalancer_StateListenerBeforeConnect tries to stimulate a race between\n// NewSubConn and ClientConn.Close.  In no cases should the SubConn's\n// StateListener be invoked, because Connect was never called.\nfunc (s) TestBalancer_StateListenerBeforeConnect(t *testing.T) {\n\t// started is fired after cc is set so cc can be used in the balancer.\n\tstarted := grpcsync.NewEvent()\n\tvar cc *ClientConn\n\n\twg := sync.WaitGroup{}\n\twg.Add(2)\n\n\t// Create a balancer that calls NewSubConn and cc.Close at approximately the\n\t// same time.\n\tbf := stub.BalancerFuncs{\n\t\tUpdateClientConnState: func(bd *stub.BalancerData, ccs balancer.ClientConnState) error {\n\t\t\tgo func() {\n\t\t\t\t// Wait for cc to be valid after the channel is created.\n\t\t\t\t<-started.Done()\n\t\t\t\t// In a goroutine, create the subconn.\n\t\t\t\tgo func() {\n\t\t\t\t\t_, err := bd.ClientConn.NewSubConn(ccs.ResolverState.Addresses, balancer.NewSubConnOptions{\n\t\t\t\t\t\tStateListener: func(scs balancer.SubConnState) {\n\t\t\t\t\t\t\tt.Error(\"Unexpected call to StateListener with:\", scs)\n\t\t\t\t\t\t},\n\t\t\t\t\t})\n\t\t\t\t\tif err != nil && !strings.Contains(err.Error(), \"connection is closing\") && !strings.Contains(err.Error(), \"is deleted\") && !strings.Contains(err.Error(), \"is closed or idle\") && !strings.Contains(err.Error(), \"balancer is being closed\") {\n\t\t\t\t\t\tt.Error(\"Unexpected error creating subconn:\", err)\n\t\t\t\t\t}\n\t\t\t\t\twg.Done()\n\t\t\t\t}()\n\t\t\t\t// At approximately the same time, close the channel.\n\t\t\t\tcc.Close()\n\t\t\t\twg.Done()\n\t\t\t}()\n\t\t\treturn nil\n\t\t},\n\t}\n\tstub.Register(t.Name(), bf)\n\tsvcCfg := fmt.Sprintf(`{ \"loadBalancingConfig\": [{%q: {}}] }`, t.Name())\n\n\tcc, err := Dial(\"fake\", WithTransportCredentials(insecure.NewCredentials()), WithDefaultServiceConfig(svcCfg))\n\tif err != nil {\n\t\tt.Fatal(\"Error dialing:\", err)\n\t}\n\tstarted.Fire()\n\n\t// Wait for the LB policy to call NewSubConn and cc.Close.\n\twg.Wait()\n}\n"
        },
        {
          "name": "benchmark",
          "type": "tree",
          "content": null
        },
        {
          "name": "binarylog",
          "type": "tree",
          "content": null
        },
        {
          "name": "call.go",
          "type": "blob",
          "size": 2.4052734375,
          "content": "/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n)\n\n// Invoke sends the RPC request on the wire and returns after response is\n// received.  This is typically called by generated code.\n//\n// All errors returned by Invoke are compatible with the status package.\nfunc (cc *ClientConn) Invoke(ctx context.Context, method string, args, reply any, opts ...CallOption) error {\n\t// allow interceptor to see all applicable call options, which means those\n\t// configured as defaults from dial option as well as per-call options\n\topts = combine(cc.dopts.callOptions, opts)\n\n\tif cc.dopts.unaryInt != nil {\n\t\treturn cc.dopts.unaryInt(ctx, method, args, reply, cc, invoke, opts...)\n\t}\n\treturn invoke(ctx, method, args, reply, cc, opts...)\n}\n\nfunc combine(o1 []CallOption, o2 []CallOption) []CallOption {\n\t// we don't use append because o1 could have extra capacity whose\n\t// elements would be overwritten, which could cause inadvertent\n\t// sharing (and race conditions) between concurrent calls\n\tif len(o1) == 0 {\n\t\treturn o2\n\t} else if len(o2) == 0 {\n\t\treturn o1\n\t}\n\tret := make([]CallOption, len(o1)+len(o2))\n\tcopy(ret, o1)\n\tcopy(ret[len(o1):], o2)\n\treturn ret\n}\n\n// Invoke sends the RPC request on the wire and returns after response is\n// received.  This is typically called by generated code.\n//\n// DEPRECATED: Use ClientConn.Invoke instead.\nfunc Invoke(ctx context.Context, method string, args, reply any, cc *ClientConn, opts ...CallOption) error {\n\treturn cc.Invoke(ctx, method, args, reply, opts...)\n}\n\nvar unaryStreamDesc = &StreamDesc{ServerStreams: false, ClientStreams: false}\n\nfunc invoke(ctx context.Context, method string, req, reply any, cc *ClientConn, opts ...CallOption) error {\n\tcs, err := newClientStream(ctx, unaryStreamDesc, cc, method, opts...)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := cs.SendMsg(req); err != nil {\n\t\treturn err\n\t}\n\treturn cs.RecvMsg(reply)\n}\n"
        },
        {
          "name": "channelz",
          "type": "tree",
          "content": null
        },
        {
          "name": "clientconn.go",
          "type": "blob",
          "size": 59.20703125,
          "content": "/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"net/url\"\n\t\"slices\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"google.golang.org/grpc/balancer\"\n\t\"google.golang.org/grpc/balancer/base\"\n\t\"google.golang.org/grpc/balancer/pickfirst\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/connectivity\"\n\t\"google.golang.org/grpc/internal\"\n\t\"google.golang.org/grpc/internal/channelz\"\n\t\"google.golang.org/grpc/internal/grpcsync\"\n\t\"google.golang.org/grpc/internal/idle\"\n\tiresolver \"google.golang.org/grpc/internal/resolver\"\n\t\"google.golang.org/grpc/internal/stats\"\n\t\"google.golang.org/grpc/internal/transport\"\n\t\"google.golang.org/grpc/keepalive\"\n\t\"google.golang.org/grpc/resolver\"\n\t\"google.golang.org/grpc/serviceconfig\"\n\t\"google.golang.org/grpc/status\"\n\n\t_ \"google.golang.org/grpc/balancer/roundrobin\"           // To register roundrobin.\n\t_ \"google.golang.org/grpc/internal/resolver/passthrough\" // To register passthrough resolver.\n\t_ \"google.golang.org/grpc/internal/resolver/unix\"        // To register unix resolver.\n\t_ \"google.golang.org/grpc/resolver/dns\"                  // To register dns resolver.\n)\n\nconst (\n\t// minimum time to give a connection to complete\n\tminConnectTimeout = 20 * time.Second\n)\n\nvar (\n\t// ErrClientConnClosing indicates that the operation is illegal because\n\t// the ClientConn is closing.\n\t//\n\t// Deprecated: this error should not be relied upon by users; use the status\n\t// code of Canceled instead.\n\tErrClientConnClosing = status.Error(codes.Canceled, \"grpc: the client connection is closing\")\n\t// errConnDrain indicates that the connection starts to be drained and does not accept any new RPCs.\n\terrConnDrain = errors.New(\"grpc: the connection is drained\")\n\t// errConnClosing indicates that the connection is closing.\n\terrConnClosing = errors.New(\"grpc: the connection is closing\")\n\t// errConnIdling indicates the connection is being closed as the channel\n\t// is moving to an idle mode due to inactivity.\n\terrConnIdling = errors.New(\"grpc: the connection is closing due to channel idleness\")\n\t// invalidDefaultServiceConfigErrPrefix is used to prefix the json parsing error for the default\n\t// service config.\n\tinvalidDefaultServiceConfigErrPrefix = \"grpc: the provided default service config is invalid\"\n\t// PickFirstBalancerName is the name of the pick_first balancer.\n\tPickFirstBalancerName = pickfirst.Name\n)\n\n// The following errors are returned from Dial and DialContext\nvar (\n\t// errNoTransportSecurity indicates that there is no transport security\n\t// being set for ClientConn. Users should either set one or explicitly\n\t// call WithInsecure DialOption to disable security.\n\terrNoTransportSecurity = errors.New(\"grpc: no transport security set (use grpc.WithTransportCredentials(insecure.NewCredentials()) explicitly or set credentials)\")\n\t// errTransportCredsAndBundle indicates that creds bundle is used together\n\t// with other individual Transport Credentials.\n\terrTransportCredsAndBundle = errors.New(\"grpc: credentials.Bundle may not be used with individual TransportCredentials\")\n\t// errNoTransportCredsInBundle indicated that the configured creds bundle\n\t// returned a transport credentials which was nil.\n\terrNoTransportCredsInBundle = errors.New(\"grpc: credentials.Bundle must return non-nil transport credentials\")\n\t// errTransportCredentialsMissing indicates that users want to transmit\n\t// security information (e.g., OAuth2 token) which requires secure\n\t// connection on an insecure connection.\n\terrTransportCredentialsMissing = errors.New(\"grpc: the credentials require transport level security (use grpc.WithTransportCredentials() to set)\")\n)\n\nconst (\n\tdefaultClientMaxReceiveMessageSize = 1024 * 1024 * 4\n\tdefaultClientMaxSendMessageSize    = math.MaxInt32\n\t// http2IOBufSize specifies the buffer size for sending frames.\n\tdefaultWriteBufSize = 32 * 1024\n\tdefaultReadBufSize  = 32 * 1024\n)\n\ntype defaultConfigSelector struct {\n\tsc *ServiceConfig\n}\n\nfunc (dcs *defaultConfigSelector) SelectConfig(rpcInfo iresolver.RPCInfo) (*iresolver.RPCConfig, error) {\n\treturn &iresolver.RPCConfig{\n\t\tContext:      rpcInfo.Context,\n\t\tMethodConfig: getMethodConfig(dcs.sc, rpcInfo.Method),\n\t}, nil\n}\n\n// NewClient creates a new gRPC \"channel\" for the target URI provided.  No I/O\n// is performed.  Use of the ClientConn for RPCs will automatically cause it to\n// connect.  Connect may be used to manually create a connection, but for most\n// users this is unnecessary.\n//\n// The target name syntax is defined in\n// https://github.com/grpc/grpc/blob/master/doc/naming.md.  e.g. to use dns\n// resolver, a \"dns:///\" prefix should be applied to the target.\n//\n// The DialOptions returned by WithBlock, WithTimeout,\n// WithReturnConnectionError, and FailOnNonTempDialError are ignored by this\n// function.\nfunc NewClient(target string, opts ...DialOption) (conn *ClientConn, err error) {\n\tcc := &ClientConn{\n\t\ttarget: target,\n\t\tconns:  make(map[*addrConn]struct{}),\n\t\tdopts:  defaultDialOptions(),\n\t}\n\n\tcc.retryThrottler.Store((*retryThrottler)(nil))\n\tcc.safeConfigSelector.UpdateConfigSelector(&defaultConfigSelector{nil})\n\tcc.ctx, cc.cancel = context.WithCancel(context.Background())\n\n\t// Apply dial options.\n\tdisableGlobalOpts := false\n\tfor _, opt := range opts {\n\t\tif _, ok := opt.(*disableGlobalDialOptions); ok {\n\t\t\tdisableGlobalOpts = true\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif !disableGlobalOpts {\n\t\tfor _, opt := range globalDialOptions {\n\t\t\topt.apply(&cc.dopts)\n\t\t}\n\t}\n\n\tfor _, opt := range opts {\n\t\topt.apply(&cc.dopts)\n\t}\n\n\t// Determine the resolver to use.\n\tif err := cc.initParsedTargetAndResolverBuilder(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor _, opt := range globalPerTargetDialOptions {\n\t\topt.DialOptionForTarget(cc.parsedTarget.URL).apply(&cc.dopts)\n\t}\n\n\tchainUnaryClientInterceptors(cc)\n\tchainStreamClientInterceptors(cc)\n\n\tif err := cc.validateTransportCredentials(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif cc.dopts.defaultServiceConfigRawJSON != nil {\n\t\tscpr := parseServiceConfig(*cc.dopts.defaultServiceConfigRawJSON, cc.dopts.maxCallAttempts)\n\t\tif scpr.Err != nil {\n\t\t\treturn nil, fmt.Errorf(\"%s: %v\", invalidDefaultServiceConfigErrPrefix, scpr.Err)\n\t\t}\n\t\tcc.dopts.defaultServiceConfig, _ = scpr.Config.(*ServiceConfig)\n\t}\n\tcc.mkp = cc.dopts.copts.KeepaliveParams\n\n\tif err = cc.initAuthority(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Register ClientConn with channelz. Note that this is only done after\n\t// channel creation cannot fail.\n\tcc.channelzRegistration(target)\n\tchannelz.Infof(logger, cc.channelz, \"parsed dial target is: %#v\", cc.parsedTarget)\n\tchannelz.Infof(logger, cc.channelz, \"Channel authority set to %q\", cc.authority)\n\n\tcc.csMgr = newConnectivityStateManager(cc.ctx, cc.channelz)\n\tcc.pickerWrapper = newPickerWrapper(cc.dopts.copts.StatsHandlers)\n\n\tcc.metricsRecorderList = stats.NewMetricsRecorderList(cc.dopts.copts.StatsHandlers)\n\n\tcc.initIdleStateLocked() // Safe to call without the lock, since nothing else has a reference to cc.\n\tcc.idlenessMgr = idle.NewManager((*idler)(cc), cc.dopts.idleTimeout)\n\n\treturn cc, nil\n}\n\n// Dial calls DialContext(context.Background(), target, opts...).\n//\n// Deprecated: use NewClient instead.  Will be supported throughout 1.x.\nfunc Dial(target string, opts ...DialOption) (*ClientConn, error) {\n\treturn DialContext(context.Background(), target, opts...)\n}\n\n// DialContext calls NewClient and then exits idle mode.  If WithBlock(true) is\n// used, it calls Connect and WaitForStateChange until either the context\n// expires or the state of the ClientConn is Ready.\n//\n// One subtle difference between NewClient and Dial and DialContext is that the\n// former uses \"dns\" as the default name resolver, while the latter use\n// \"passthrough\" for backward compatibility.  This distinction should not matter\n// to most users, but could matter to legacy users that specify a custom dialer\n// and expect it to receive the target string directly.\n//\n// Deprecated: use NewClient instead.  Will be supported throughout 1.x.\nfunc DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) {\n\t// At the end of this method, we kick the channel out of idle, rather than\n\t// waiting for the first rpc.\n\topts = append([]DialOption{withDefaultScheme(\"passthrough\")}, opts...)\n\tcc, err := NewClient(target, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// We start the channel off in idle mode, but kick it out of idle now,\n\t// instead of waiting for the first RPC.  This is the legacy behavior of\n\t// Dial.\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tcc.Close()\n\t\t}\n\t}()\n\n\t// This creates the name resolver, load balancer, etc.\n\tif err := cc.idlenessMgr.ExitIdleMode(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Return now for non-blocking dials.\n\tif !cc.dopts.block {\n\t\treturn cc, nil\n\t}\n\n\tif cc.dopts.timeout > 0 {\n\t\tvar cancel context.CancelFunc\n\t\tctx, cancel = context.WithTimeout(ctx, cc.dopts.timeout)\n\t\tdefer cancel()\n\t}\n\tdefer func() {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tswitch {\n\t\t\tcase ctx.Err() == err:\n\t\t\t\tconn = nil\n\t\t\tcase err == nil || !cc.dopts.returnLastError:\n\t\t\t\tconn, err = nil, ctx.Err()\n\t\t\tdefault:\n\t\t\t\tconn, err = nil, fmt.Errorf(\"%v: %v\", ctx.Err(), err)\n\t\t\t}\n\t\tdefault:\n\t\t}\n\t}()\n\n\t// A blocking dial blocks until the clientConn is ready.\n\tfor {\n\t\ts := cc.GetState()\n\t\tif s == connectivity.Idle {\n\t\t\tcc.Connect()\n\t\t}\n\t\tif s == connectivity.Ready {\n\t\t\treturn cc, nil\n\t\t} else if cc.dopts.copts.FailOnNonTempDialError && s == connectivity.TransientFailure {\n\t\t\tif err = cc.connectionError(); err != nil {\n\t\t\t\tterr, ok := err.(interface {\n\t\t\t\t\tTemporary() bool\n\t\t\t\t})\n\t\t\t\tif ok && !terr.Temporary() {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !cc.WaitForStateChange(ctx, s) {\n\t\t\t// ctx got timeout or canceled.\n\t\t\tif err = cc.connectionError(); err != nil && cc.dopts.returnLastError {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn nil, ctx.Err()\n\t\t}\n\t}\n}\n\n// addTraceEvent is a helper method to add a trace event on the channel. If the\n// channel is a nested one, the same event is also added on the parent channel.\nfunc (cc *ClientConn) addTraceEvent(msg string) {\n\tted := &channelz.TraceEvent{\n\t\tDesc:     fmt.Sprintf(\"Channel %s\", msg),\n\t\tSeverity: channelz.CtInfo,\n\t}\n\tif cc.dopts.channelzParent != nil {\n\t\tted.Parent = &channelz.TraceEvent{\n\t\t\tDesc:     fmt.Sprintf(\"Nested channel(id:%d) %s\", cc.channelz.ID, msg),\n\t\t\tSeverity: channelz.CtInfo,\n\t\t}\n\t}\n\tchannelz.AddTraceEvent(logger, cc.channelz, 0, ted)\n}\n\ntype idler ClientConn\n\nfunc (i *idler) EnterIdleMode() {\n\t(*ClientConn)(i).enterIdleMode()\n}\n\nfunc (i *idler) ExitIdleMode() error {\n\treturn (*ClientConn)(i).exitIdleMode()\n}\n\n// exitIdleMode moves the channel out of idle mode by recreating the name\n// resolver and load balancer.  This should never be called directly; use\n// cc.idlenessMgr.ExitIdleMode instead.\nfunc (cc *ClientConn) exitIdleMode() (err error) {\n\tcc.mu.Lock()\n\tif cc.conns == nil {\n\t\tcc.mu.Unlock()\n\t\treturn errConnClosing\n\t}\n\tcc.mu.Unlock()\n\n\t// This needs to be called without cc.mu because this builds a new resolver\n\t// which might update state or report error inline, which would then need to\n\t// acquire cc.mu.\n\tif err := cc.resolverWrapper.start(); err != nil {\n\t\treturn err\n\t}\n\n\tcc.addTraceEvent(\"exiting idle mode\")\n\treturn nil\n}\n\n// initIdleStateLocked initializes common state to how it should be while idle.\nfunc (cc *ClientConn) initIdleStateLocked() {\n\tcc.resolverWrapper = newCCResolverWrapper(cc)\n\tcc.balancerWrapper = newCCBalancerWrapper(cc)\n\tcc.firstResolveEvent = grpcsync.NewEvent()\n\t// cc.conns == nil is a proxy for the ClientConn being closed. So, instead\n\t// of setting it to nil here, we recreate the map. This also means that we\n\t// don't have to do this when exiting idle mode.\n\tcc.conns = make(map[*addrConn]struct{})\n}\n\n// enterIdleMode puts the channel in idle mode, and as part of it shuts down the\n// name resolver, load balancer, and any subchannels.  This should never be\n// called directly; use cc.idlenessMgr.EnterIdleMode instead.\nfunc (cc *ClientConn) enterIdleMode() {\n\tcc.mu.Lock()\n\n\tif cc.conns == nil {\n\t\tcc.mu.Unlock()\n\t\treturn\n\t}\n\n\tconns := cc.conns\n\n\trWrapper := cc.resolverWrapper\n\trWrapper.close()\n\tcc.pickerWrapper.reset()\n\tbWrapper := cc.balancerWrapper\n\tbWrapper.close()\n\tcc.csMgr.updateState(connectivity.Idle)\n\tcc.addTraceEvent(\"entering idle mode\")\n\n\tcc.initIdleStateLocked()\n\n\tcc.mu.Unlock()\n\n\t// Block until the name resolver and LB policy are closed.\n\t<-rWrapper.serializer.Done()\n\t<-bWrapper.serializer.Done()\n\n\t// Close all subchannels after the LB policy is closed.\n\tfor ac := range conns {\n\t\tac.tearDown(errConnIdling)\n\t}\n}\n\n// validateTransportCredentials performs a series of checks on the configured\n// transport credentials. It returns a non-nil error if any of these conditions\n// are met:\n//   - no transport creds and no creds bundle is configured\n//   - both transport creds and creds bundle are configured\n//   - creds bundle is configured, but it lacks a transport credentials\n//   - insecure transport creds configured alongside call creds that require\n//     transport level security\n//\n// If none of the above conditions are met, the configured credentials are\n// deemed valid and a nil error is returned.\nfunc (cc *ClientConn) validateTransportCredentials() error {\n\tif cc.dopts.copts.TransportCredentials == nil && cc.dopts.copts.CredsBundle == nil {\n\t\treturn errNoTransportSecurity\n\t}\n\tif cc.dopts.copts.TransportCredentials != nil && cc.dopts.copts.CredsBundle != nil {\n\t\treturn errTransportCredsAndBundle\n\t}\n\tif cc.dopts.copts.CredsBundle != nil && cc.dopts.copts.CredsBundle.TransportCredentials() == nil {\n\t\treturn errNoTransportCredsInBundle\n\t}\n\ttransportCreds := cc.dopts.copts.TransportCredentials\n\tif transportCreds == nil {\n\t\ttransportCreds = cc.dopts.copts.CredsBundle.TransportCredentials()\n\t}\n\tif transportCreds.Info().SecurityProtocol == \"insecure\" {\n\t\tfor _, cd := range cc.dopts.copts.PerRPCCredentials {\n\t\t\tif cd.RequireTransportSecurity() {\n\t\t\t\treturn errTransportCredentialsMissing\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// channelzRegistration registers the newly created ClientConn with channelz and\n// stores the returned identifier in `cc.channelz`.  A channelz trace event is\n// emitted for ClientConn creation. If the newly created ClientConn is a nested\n// one, i.e a valid parent ClientConn ID is specified via a dial option, the\n// trace event is also added to the parent.\n//\n// Doesn't grab cc.mu as this method is expected to be called only at Dial time.\nfunc (cc *ClientConn) channelzRegistration(target string) {\n\tparentChannel, _ := cc.dopts.channelzParent.(*channelz.Channel)\n\tcc.channelz = channelz.RegisterChannel(parentChannel, target)\n\tcc.addTraceEvent(\"created\")\n}\n\n// chainUnaryClientInterceptors chains all unary client interceptors into one.\nfunc chainUnaryClientInterceptors(cc *ClientConn) {\n\tinterceptors := cc.dopts.chainUnaryInts\n\t// Prepend dopts.unaryInt to the chaining interceptors if it exists, since unaryInt will\n\t// be executed before any other chained interceptors.\n\tif cc.dopts.unaryInt != nil {\n\t\tinterceptors = append([]UnaryClientInterceptor{cc.dopts.unaryInt}, interceptors...)\n\t}\n\tvar chainedInt UnaryClientInterceptor\n\tif len(interceptors) == 0 {\n\t\tchainedInt = nil\n\t} else if len(interceptors) == 1 {\n\t\tchainedInt = interceptors[0]\n\t} else {\n\t\tchainedInt = func(ctx context.Context, method string, req, reply any, cc *ClientConn, invoker UnaryInvoker, opts ...CallOption) error {\n\t\t\treturn interceptors[0](ctx, method, req, reply, cc, getChainUnaryInvoker(interceptors, 0, invoker), opts...)\n\t\t}\n\t}\n\tcc.dopts.unaryInt = chainedInt\n}\n\n// getChainUnaryInvoker recursively generate the chained unary invoker.\nfunc getChainUnaryInvoker(interceptors []UnaryClientInterceptor, curr int, finalInvoker UnaryInvoker) UnaryInvoker {\n\tif curr == len(interceptors)-1 {\n\t\treturn finalInvoker\n\t}\n\treturn func(ctx context.Context, method string, req, reply any, cc *ClientConn, opts ...CallOption) error {\n\t\treturn interceptors[curr+1](ctx, method, req, reply, cc, getChainUnaryInvoker(interceptors, curr+1, finalInvoker), opts...)\n\t}\n}\n\n// chainStreamClientInterceptors chains all stream client interceptors into one.\nfunc chainStreamClientInterceptors(cc *ClientConn) {\n\tinterceptors := cc.dopts.chainStreamInts\n\t// Prepend dopts.streamInt to the chaining interceptors if it exists, since streamInt will\n\t// be executed before any other chained interceptors.\n\tif cc.dopts.streamInt != nil {\n\t\tinterceptors = append([]StreamClientInterceptor{cc.dopts.streamInt}, interceptors...)\n\t}\n\tvar chainedInt StreamClientInterceptor\n\tif len(interceptors) == 0 {\n\t\tchainedInt = nil\n\t} else if len(interceptors) == 1 {\n\t\tchainedInt = interceptors[0]\n\t} else {\n\t\tchainedInt = func(ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, streamer Streamer, opts ...CallOption) (ClientStream, error) {\n\t\t\treturn interceptors[0](ctx, desc, cc, method, getChainStreamer(interceptors, 0, streamer), opts...)\n\t\t}\n\t}\n\tcc.dopts.streamInt = chainedInt\n}\n\n// getChainStreamer recursively generate the chained client stream constructor.\nfunc getChainStreamer(interceptors []StreamClientInterceptor, curr int, finalStreamer Streamer) Streamer {\n\tif curr == len(interceptors)-1 {\n\t\treturn finalStreamer\n\t}\n\treturn func(ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, opts ...CallOption) (ClientStream, error) {\n\t\treturn interceptors[curr+1](ctx, desc, cc, method, getChainStreamer(interceptors, curr+1, finalStreamer), opts...)\n\t}\n}\n\n// newConnectivityStateManager creates an connectivityStateManager with\n// the specified channel.\nfunc newConnectivityStateManager(ctx context.Context, channel *channelz.Channel) *connectivityStateManager {\n\treturn &connectivityStateManager{\n\t\tchannelz: channel,\n\t\tpubSub:   grpcsync.NewPubSub(ctx),\n\t}\n}\n\n// connectivityStateManager keeps the connectivity.State of ClientConn.\n// This struct will eventually be exported so the balancers can access it.\n//\n// TODO: If possible, get rid of the `connectivityStateManager` type, and\n// provide this functionality using the `PubSub`, to avoid keeping track of\n// the connectivity state at two places.\ntype connectivityStateManager struct {\n\tmu         sync.Mutex\n\tstate      connectivity.State\n\tnotifyChan chan struct{}\n\tchannelz   *channelz.Channel\n\tpubSub     *grpcsync.PubSub\n}\n\n// updateState updates the connectivity.State of ClientConn.\n// If there's a change it notifies goroutines waiting on state change to\n// happen.\nfunc (csm *connectivityStateManager) updateState(state connectivity.State) {\n\tcsm.mu.Lock()\n\tdefer csm.mu.Unlock()\n\tif csm.state == connectivity.Shutdown {\n\t\treturn\n\t}\n\tif csm.state == state {\n\t\treturn\n\t}\n\tcsm.state = state\n\tcsm.channelz.ChannelMetrics.State.Store(&state)\n\tcsm.pubSub.Publish(state)\n\n\tchannelz.Infof(logger, csm.channelz, \"Channel Connectivity change to %v\", state)\n\tif csm.notifyChan != nil {\n\t\t// There are other goroutines waiting on this channel.\n\t\tclose(csm.notifyChan)\n\t\tcsm.notifyChan = nil\n\t}\n}\n\nfunc (csm *connectivityStateManager) getState() connectivity.State {\n\tcsm.mu.Lock()\n\tdefer csm.mu.Unlock()\n\treturn csm.state\n}\n\nfunc (csm *connectivityStateManager) getNotifyChan() <-chan struct{} {\n\tcsm.mu.Lock()\n\tdefer csm.mu.Unlock()\n\tif csm.notifyChan == nil {\n\t\tcsm.notifyChan = make(chan struct{})\n\t}\n\treturn csm.notifyChan\n}\n\n// ClientConnInterface defines the functions clients need to perform unary and\n// streaming RPCs.  It is implemented by *ClientConn, and is only intended to\n// be referenced by generated code.\ntype ClientConnInterface interface {\n\t// Invoke performs a unary RPC and returns after the response is received\n\t// into reply.\n\tInvoke(ctx context.Context, method string, args any, reply any, opts ...CallOption) error\n\t// NewStream begins a streaming RPC.\n\tNewStream(ctx context.Context, desc *StreamDesc, method string, opts ...CallOption) (ClientStream, error)\n}\n\n// Assert *ClientConn implements ClientConnInterface.\nvar _ ClientConnInterface = (*ClientConn)(nil)\n\n// ClientConn represents a virtual connection to a conceptual endpoint, to\n// perform RPCs.\n//\n// A ClientConn is free to have zero or more actual connections to the endpoint\n// based on configuration, load, etc. It is also free to determine which actual\n// endpoints to use and may change it every RPC, permitting client-side load\n// balancing.\n//\n// A ClientConn encapsulates a range of functionality including name\n// resolution, TCP connection establishment (with retries and backoff) and TLS\n// handshakes. It also handles errors on established connections by\n// re-resolving the name and reconnecting.\ntype ClientConn struct {\n\tctx    context.Context    // Initialized using the background context at dial time.\n\tcancel context.CancelFunc // Cancelled on close.\n\n\t// The following are initialized at dial time, and are read-only after that.\n\ttarget              string            // User's dial target.\n\tparsedTarget        resolver.Target   // See initParsedTargetAndResolverBuilder().\n\tauthority           string            // See initAuthority().\n\tdopts               dialOptions       // Default and user specified dial options.\n\tchannelz            *channelz.Channel // Channelz object.\n\tresolverBuilder     resolver.Builder  // See initParsedTargetAndResolverBuilder().\n\tidlenessMgr         *idle.Manager\n\tmetricsRecorderList *stats.MetricsRecorderList\n\n\t// The following provide their own synchronization, and therefore don't\n\t// require cc.mu to be held to access them.\n\tcsMgr              *connectivityStateManager\n\tpickerWrapper      *pickerWrapper\n\tsafeConfigSelector iresolver.SafeConfigSelector\n\tretryThrottler     atomic.Value // Updated from service config.\n\n\t// mu protects the following fields.\n\t// TODO: split mu so the same mutex isn't used for everything.\n\tmu              sync.RWMutex\n\tresolverWrapper *ccResolverWrapper         // Always recreated whenever entering idle to simplify Close.\n\tbalancerWrapper *ccBalancerWrapper         // Always recreated whenever entering idle to simplify Close.\n\tsc              *ServiceConfig             // Latest service config received from the resolver.\n\tconns           map[*addrConn]struct{}     // Set to nil on close.\n\tmkp             keepalive.ClientParameters // May be updated upon receipt of a GoAway.\n\t// firstResolveEvent is used to track whether the name resolver sent us at\n\t// least one update. RPCs block on this event.  May be accessed without mu\n\t// if we know we cannot be asked to enter idle mode while accessing it (e.g.\n\t// when the idle manager has already been closed, or if we are already\n\t// entering idle mode).\n\tfirstResolveEvent *grpcsync.Event\n\n\tlceMu               sync.Mutex // protects lastConnectionError\n\tlastConnectionError error\n}\n\n// WaitForStateChange waits until the connectivity.State of ClientConn changes from sourceState or\n// ctx expires. A true value is returned in former case and false in latter.\nfunc (cc *ClientConn) WaitForStateChange(ctx context.Context, sourceState connectivity.State) bool {\n\tch := cc.csMgr.getNotifyChan()\n\tif cc.csMgr.getState() != sourceState {\n\t\treturn true\n\t}\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn false\n\tcase <-ch:\n\t\treturn true\n\t}\n}\n\n// GetState returns the connectivity.State of ClientConn.\nfunc (cc *ClientConn) GetState() connectivity.State {\n\treturn cc.csMgr.getState()\n}\n\n// Connect causes all subchannels in the ClientConn to attempt to connect if\n// the channel is idle.  Does not wait for the connection attempts to begin\n// before returning.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a later\n// release.\nfunc (cc *ClientConn) Connect() {\n\tif err := cc.idlenessMgr.ExitIdleMode(); err != nil {\n\t\tcc.addTraceEvent(err.Error())\n\t\treturn\n\t}\n\t// If the ClientConn was not in idle mode, we need to call ExitIdle on the\n\t// LB policy so that connections can be created.\n\tcc.mu.Lock()\n\tcc.balancerWrapper.exitIdle()\n\tcc.mu.Unlock()\n}\n\n// waitForResolvedAddrs blocks until the resolver has provided addresses or the\n// context expires.  Returns nil unless the context expires first; otherwise\n// returns a status error based on the context.\nfunc (cc *ClientConn) waitForResolvedAddrs(ctx context.Context) error {\n\t// This is on the RPC path, so we use a fast path to avoid the\n\t// more-expensive \"select\" below after the resolver has returned once.\n\tif cc.firstResolveEvent.HasFired() {\n\t\treturn nil\n\t}\n\tselect {\n\tcase <-cc.firstResolveEvent.Done():\n\t\treturn nil\n\tcase <-ctx.Done():\n\t\treturn status.FromContextError(ctx.Err()).Err()\n\tcase <-cc.ctx.Done():\n\t\treturn ErrClientConnClosing\n\t}\n}\n\nvar emptyServiceConfig *ServiceConfig\n\nfunc init() {\n\tcfg := parseServiceConfig(\"{}\", defaultMaxCallAttempts)\n\tif cfg.Err != nil {\n\t\tpanic(fmt.Sprintf(\"impossible error parsing empty service config: %v\", cfg.Err))\n\t}\n\temptyServiceConfig = cfg.Config.(*ServiceConfig)\n\n\tinternal.SubscribeToConnectivityStateChanges = func(cc *ClientConn, s grpcsync.Subscriber) func() {\n\t\treturn cc.csMgr.pubSub.Subscribe(s)\n\t}\n\tinternal.EnterIdleModeForTesting = func(cc *ClientConn) {\n\t\tcc.idlenessMgr.EnterIdleModeForTesting()\n\t}\n\tinternal.ExitIdleModeForTesting = func(cc *ClientConn) error {\n\t\treturn cc.idlenessMgr.ExitIdleMode()\n\t}\n}\n\nfunc (cc *ClientConn) maybeApplyDefaultServiceConfig() {\n\tif cc.sc != nil {\n\t\tcc.applyServiceConfigAndBalancer(cc.sc, nil)\n\t\treturn\n\t}\n\tif cc.dopts.defaultServiceConfig != nil {\n\t\tcc.applyServiceConfigAndBalancer(cc.dopts.defaultServiceConfig, &defaultConfigSelector{cc.dopts.defaultServiceConfig})\n\t} else {\n\t\tcc.applyServiceConfigAndBalancer(emptyServiceConfig, &defaultConfigSelector{emptyServiceConfig})\n\t}\n}\n\nfunc (cc *ClientConn) updateResolverStateAndUnlock(s resolver.State, err error) error {\n\tdefer cc.firstResolveEvent.Fire()\n\t// Check if the ClientConn is already closed. Some fields (e.g.\n\t// balancerWrapper) are set to nil when closing the ClientConn, and could\n\t// cause nil pointer panic if we don't have this check.\n\tif cc.conns == nil {\n\t\tcc.mu.Unlock()\n\t\treturn nil\n\t}\n\n\tif err != nil {\n\t\t// May need to apply the initial service config in case the resolver\n\t\t// doesn't support service configs, or doesn't provide a service config\n\t\t// with the new addresses.\n\t\tcc.maybeApplyDefaultServiceConfig()\n\n\t\tcc.balancerWrapper.resolverError(err)\n\n\t\t// No addresses are valid with err set; return early.\n\t\tcc.mu.Unlock()\n\t\treturn balancer.ErrBadResolverState\n\t}\n\n\tvar ret error\n\tif cc.dopts.disableServiceConfig {\n\t\tchannelz.Infof(logger, cc.channelz, \"ignoring service config from resolver (%v) and applying the default because service config is disabled\", s.ServiceConfig)\n\t\tcc.maybeApplyDefaultServiceConfig()\n\t} else if s.ServiceConfig == nil {\n\t\tcc.maybeApplyDefaultServiceConfig()\n\t\t// TODO: do we need to apply a failing LB policy if there is no\n\t\t// default, per the error handling design?\n\t} else {\n\t\tif sc, ok := s.ServiceConfig.Config.(*ServiceConfig); s.ServiceConfig.Err == nil && ok {\n\t\t\tconfigSelector := iresolver.GetConfigSelector(s)\n\t\t\tif configSelector != nil {\n\t\t\t\tif len(s.ServiceConfig.Config.(*ServiceConfig).Methods) != 0 {\n\t\t\t\t\tchannelz.Infof(logger, cc.channelz, \"method configs in service config will be ignored due to presence of config selector\")\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tconfigSelector = &defaultConfigSelector{sc}\n\t\t\t}\n\t\t\tcc.applyServiceConfigAndBalancer(sc, configSelector)\n\t\t} else {\n\t\t\tret = balancer.ErrBadResolverState\n\t\t\tif cc.sc == nil {\n\t\t\t\t// Apply the failing LB only if we haven't received valid service config\n\t\t\t\t// from the name resolver in the past.\n\t\t\t\tcc.applyFailingLBLocked(s.ServiceConfig)\n\t\t\t\tcc.mu.Unlock()\n\t\t\t\treturn ret\n\t\t\t}\n\t\t}\n\t}\n\n\tbalCfg := cc.sc.lbConfig\n\tbw := cc.balancerWrapper\n\tcc.mu.Unlock()\n\n\tuccsErr := bw.updateClientConnState(&balancer.ClientConnState{ResolverState: s, BalancerConfig: balCfg})\n\tif ret == nil {\n\t\tret = uccsErr // prefer ErrBadResolver state since any other error is\n\t\t// currently meaningless to the caller.\n\t}\n\treturn ret\n}\n\n// applyFailingLBLocked is akin to configuring an LB policy on the channel which\n// always fails RPCs. Here, an actual LB policy is not configured, but an always\n// erroring picker is configured, which returns errors with information about\n// what was invalid in the received service config. A config selector with no\n// service config is configured, and the connectivity state of the channel is\n// set to TransientFailure.\nfunc (cc *ClientConn) applyFailingLBLocked(sc *serviceconfig.ParseResult) {\n\tvar err error\n\tif sc.Err != nil {\n\t\terr = status.Errorf(codes.Unavailable, \"error parsing service config: %v\", sc.Err)\n\t} else {\n\t\terr = status.Errorf(codes.Unavailable, \"illegal service config type: %T\", sc.Config)\n\t}\n\tcc.safeConfigSelector.UpdateConfigSelector(&defaultConfigSelector{nil})\n\tcc.pickerWrapper.updatePicker(base.NewErrPicker(err))\n\tcc.csMgr.updateState(connectivity.TransientFailure)\n}\n\n// Makes a copy of the input addresses slice. Addresses are passed during\n// subconn creation and address update operations.\nfunc copyAddresses(in []resolver.Address) []resolver.Address {\n\tout := make([]resolver.Address, len(in))\n\tcopy(out, in)\n\treturn out\n}\n\n// newAddrConnLocked creates an addrConn for addrs and adds it to cc.conns.\n//\n// Caller needs to make sure len(addrs) > 0.\nfunc (cc *ClientConn) newAddrConnLocked(addrs []resolver.Address, opts balancer.NewSubConnOptions) (*addrConn, error) {\n\tif cc.conns == nil {\n\t\treturn nil, ErrClientConnClosing\n\t}\n\n\tac := &addrConn{\n\t\tstate:        connectivity.Idle,\n\t\tcc:           cc,\n\t\taddrs:        copyAddresses(addrs),\n\t\tscopts:       opts,\n\t\tdopts:        cc.dopts,\n\t\tchannelz:     channelz.RegisterSubChannel(cc.channelz, \"\"),\n\t\tresetBackoff: make(chan struct{}),\n\t}\n\tac.ctx, ac.cancel = context.WithCancel(cc.ctx)\n\t// Start with our address set to the first address; this may be updated if\n\t// we connect to different addresses.\n\tac.channelz.ChannelMetrics.Target.Store(&addrs[0].Addr)\n\n\tchannelz.AddTraceEvent(logger, ac.channelz, 0, &channelz.TraceEvent{\n\t\tDesc:     \"Subchannel created\",\n\t\tSeverity: channelz.CtInfo,\n\t\tParent: &channelz.TraceEvent{\n\t\t\tDesc:     fmt.Sprintf(\"Subchannel(id:%d) created\", ac.channelz.ID),\n\t\t\tSeverity: channelz.CtInfo,\n\t\t},\n\t})\n\n\t// Track ac in cc. This needs to be done before any getTransport(...) is called.\n\tcc.conns[ac] = struct{}{}\n\treturn ac, nil\n}\n\n// removeAddrConn removes the addrConn in the subConn from clientConn.\n// It also tears down the ac with the given error.\nfunc (cc *ClientConn) removeAddrConn(ac *addrConn, err error) {\n\tcc.mu.Lock()\n\tif cc.conns == nil {\n\t\tcc.mu.Unlock()\n\t\treturn\n\t}\n\tdelete(cc.conns, ac)\n\tcc.mu.Unlock()\n\tac.tearDown(err)\n}\n\n// Target returns the target string of the ClientConn.\nfunc (cc *ClientConn) Target() string {\n\treturn cc.target\n}\n\n// CanonicalTarget returns the canonical target string of the ClientConn.\nfunc (cc *ClientConn) CanonicalTarget() string {\n\treturn cc.parsedTarget.String()\n}\n\nfunc (cc *ClientConn) incrCallsStarted() {\n\tcc.channelz.ChannelMetrics.CallsStarted.Add(1)\n\tcc.channelz.ChannelMetrics.LastCallStartedTimestamp.Store(time.Now().UnixNano())\n}\n\nfunc (cc *ClientConn) incrCallsSucceeded() {\n\tcc.channelz.ChannelMetrics.CallsSucceeded.Add(1)\n}\n\nfunc (cc *ClientConn) incrCallsFailed() {\n\tcc.channelz.ChannelMetrics.CallsFailed.Add(1)\n}\n\n// connect starts creating a transport.\n// It does nothing if the ac is not IDLE.\n// TODO(bar) Move this to the addrConn section.\nfunc (ac *addrConn) connect() error {\n\tac.mu.Lock()\n\tif ac.state == connectivity.Shutdown {\n\t\tif logger.V(2) {\n\t\t\tlogger.Infof(\"connect called on shutdown addrConn; ignoring.\")\n\t\t}\n\t\tac.mu.Unlock()\n\t\treturn errConnClosing\n\t}\n\tif ac.state != connectivity.Idle {\n\t\tif logger.V(2) {\n\t\t\tlogger.Infof(\"connect called on addrConn in non-idle state (%v); ignoring.\", ac.state)\n\t\t}\n\t\tac.mu.Unlock()\n\t\treturn nil\n\t}\n\n\tac.resetTransportAndUnlock()\n\treturn nil\n}\n\n// equalAddressIgnoringBalAttributes returns true is a and b are considered equal.\n// This is different from the Equal method on the resolver.Address type which\n// considers all fields to determine equality. Here, we only consider fields\n// that are meaningful to the subConn.\nfunc equalAddressIgnoringBalAttributes(a, b *resolver.Address) bool {\n\treturn a.Addr == b.Addr && a.ServerName == b.ServerName &&\n\t\ta.Attributes.Equal(b.Attributes) &&\n\t\ta.Metadata == b.Metadata\n}\n\nfunc equalAddressesIgnoringBalAttributes(a, b []resolver.Address) bool {\n\treturn slices.EqualFunc(a, b, func(a, b resolver.Address) bool { return equalAddressIgnoringBalAttributes(&a, &b) })\n}\n\n// updateAddrs updates ac.addrs with the new addresses list and handles active\n// connections or connection attempts.\nfunc (ac *addrConn) updateAddrs(addrs []resolver.Address) {\n\taddrs = copyAddresses(addrs)\n\tlimit := len(addrs)\n\tif limit > 5 {\n\t\tlimit = 5\n\t}\n\tchannelz.Infof(logger, ac.channelz, \"addrConn: updateAddrs addrs (%d of %d): %v\", limit, len(addrs), addrs[:limit])\n\n\tac.mu.Lock()\n\tif equalAddressesIgnoringBalAttributes(ac.addrs, addrs) {\n\t\tac.mu.Unlock()\n\t\treturn\n\t}\n\n\tac.addrs = addrs\n\n\tif ac.state == connectivity.Shutdown ||\n\t\tac.state == connectivity.TransientFailure ||\n\t\tac.state == connectivity.Idle {\n\t\t// We were not connecting, so do nothing but update the addresses.\n\t\tac.mu.Unlock()\n\t\treturn\n\t}\n\n\tif ac.state == connectivity.Ready {\n\t\t// Try to find the connected address.\n\t\tfor _, a := range addrs {\n\t\t\ta.ServerName = ac.cc.getServerName(a)\n\t\t\tif equalAddressIgnoringBalAttributes(&a, &ac.curAddr) {\n\t\t\t\t// We are connected to a valid address, so do nothing but\n\t\t\t\t// update the addresses.\n\t\t\t\tac.mu.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\t// We are either connected to the wrong address or currently connecting.\n\t// Stop the current iteration and restart.\n\n\tac.cancel()\n\tac.ctx, ac.cancel = context.WithCancel(ac.cc.ctx)\n\n\t// We have to defer here because GracefulClose => onClose, which requires\n\t// locking ac.mu.\n\tif ac.transport != nil {\n\t\tdefer ac.transport.GracefulClose()\n\t\tac.transport = nil\n\t}\n\n\tif len(addrs) == 0 {\n\t\tac.updateConnectivityState(connectivity.Idle, nil)\n\t}\n\n\t// Since we were connecting/connected, we should start a new connection\n\t// attempt.\n\tgo ac.resetTransportAndUnlock()\n}\n\n// getServerName determines the serverName to be used in the connection\n// handshake. The default value for the serverName is the authority on the\n// ClientConn, which either comes from the user's dial target or through an\n// authority override specified using the WithAuthority dial option. Name\n// resolvers can specify a per-address override for the serverName through the\n// resolver.Address.ServerName field which is used only if the WithAuthority\n// dial option was not used. The rationale is that per-address authority\n// overrides specified by the name resolver can represent a security risk, while\n// an override specified by the user is more dependable since they probably know\n// what they are doing.\nfunc (cc *ClientConn) getServerName(addr resolver.Address) string {\n\tif cc.dopts.authority != \"\" {\n\t\treturn cc.dopts.authority\n\t}\n\tif addr.ServerName != \"\" {\n\t\treturn addr.ServerName\n\t}\n\treturn cc.authority\n}\n\nfunc getMethodConfig(sc *ServiceConfig, method string) MethodConfig {\n\tif sc == nil {\n\t\treturn MethodConfig{}\n\t}\n\tif m, ok := sc.Methods[method]; ok {\n\t\treturn m\n\t}\n\ti := strings.LastIndex(method, \"/\")\n\tif m, ok := sc.Methods[method[:i+1]]; ok {\n\t\treturn m\n\t}\n\treturn sc.Methods[\"\"]\n}\n\n// GetMethodConfig gets the method config of the input method.\n// If there's an exact match for input method (i.e. /service/method), we return\n// the corresponding MethodConfig.\n// If there isn't an exact match for the input method, we look for the service's default\n// config under the service (i.e /service/) and then for the default for all services (empty string).\n//\n// If there is a default MethodConfig for the service, we return it.\n// Otherwise, we return an empty MethodConfig.\nfunc (cc *ClientConn) GetMethodConfig(method string) MethodConfig {\n\t// TODO: Avoid the locking here.\n\tcc.mu.RLock()\n\tdefer cc.mu.RUnlock()\n\treturn getMethodConfig(cc.sc, method)\n}\n\nfunc (cc *ClientConn) healthCheckConfig() *healthCheckConfig {\n\tcc.mu.RLock()\n\tdefer cc.mu.RUnlock()\n\tif cc.sc == nil {\n\t\treturn nil\n\t}\n\treturn cc.sc.healthCheckConfig\n}\n\nfunc (cc *ClientConn) getTransport(ctx context.Context, failfast bool, method string) (transport.ClientTransport, balancer.PickResult, error) {\n\treturn cc.pickerWrapper.pick(ctx, failfast, balancer.PickInfo{\n\t\tCtx:            ctx,\n\t\tFullMethodName: method,\n\t})\n}\n\nfunc (cc *ClientConn) applyServiceConfigAndBalancer(sc *ServiceConfig, configSelector iresolver.ConfigSelector) {\n\tif sc == nil {\n\t\t// should never reach here.\n\t\treturn\n\t}\n\tcc.sc = sc\n\tif configSelector != nil {\n\t\tcc.safeConfigSelector.UpdateConfigSelector(configSelector)\n\t}\n\n\tif cc.sc.retryThrottling != nil {\n\t\tnewThrottler := &retryThrottler{\n\t\t\ttokens: cc.sc.retryThrottling.MaxTokens,\n\t\t\tmax:    cc.sc.retryThrottling.MaxTokens,\n\t\t\tthresh: cc.sc.retryThrottling.MaxTokens / 2,\n\t\t\tratio:  cc.sc.retryThrottling.TokenRatio,\n\t\t}\n\t\tcc.retryThrottler.Store(newThrottler)\n\t} else {\n\t\tcc.retryThrottler.Store((*retryThrottler)(nil))\n\t}\n}\n\nfunc (cc *ClientConn) resolveNow(o resolver.ResolveNowOptions) {\n\tcc.mu.RLock()\n\tcc.resolverWrapper.resolveNow(o)\n\tcc.mu.RUnlock()\n}\n\nfunc (cc *ClientConn) resolveNowLocked(o resolver.ResolveNowOptions) {\n\tcc.resolverWrapper.resolveNow(o)\n}\n\n// ResetConnectBackoff wakes up all subchannels in transient failure and causes\n// them to attempt another connection immediately.  It also resets the backoff\n// times used for subsequent attempts regardless of the current state.\n//\n// In general, this function should not be used.  Typical service or network\n// outages result in a reasonable client reconnection strategy by default.\n// However, if a previously unavailable network becomes available, this may be\n// used to trigger an immediate reconnect.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc (cc *ClientConn) ResetConnectBackoff() {\n\tcc.mu.Lock()\n\tconns := cc.conns\n\tcc.mu.Unlock()\n\tfor ac := range conns {\n\t\tac.resetConnectBackoff()\n\t}\n}\n\n// Close tears down the ClientConn and all underlying connections.\nfunc (cc *ClientConn) Close() error {\n\tdefer func() {\n\t\tcc.cancel()\n\t\t<-cc.csMgr.pubSub.Done()\n\t}()\n\n\t// Prevent calls to enter/exit idle immediately, and ensure we are not\n\t// currently entering/exiting idle mode.\n\tcc.idlenessMgr.Close()\n\n\tcc.mu.Lock()\n\tif cc.conns == nil {\n\t\tcc.mu.Unlock()\n\t\treturn ErrClientConnClosing\n\t}\n\n\tconns := cc.conns\n\tcc.conns = nil\n\tcc.csMgr.updateState(connectivity.Shutdown)\n\n\t// We can safely unlock and continue to access all fields now as\n\t// cc.conns==nil, preventing any further operations on cc.\n\tcc.mu.Unlock()\n\n\tcc.resolverWrapper.close()\n\t// The order of closing matters here since the balancer wrapper assumes the\n\t// picker is closed before it is closed.\n\tcc.pickerWrapper.close()\n\tcc.balancerWrapper.close()\n\n\t<-cc.resolverWrapper.serializer.Done()\n\t<-cc.balancerWrapper.serializer.Done()\n\tvar wg sync.WaitGroup\n\tfor ac := range conns {\n\t\twg.Add(1)\n\t\tgo func(ac *addrConn) {\n\t\t\tdefer wg.Done()\n\t\t\tac.tearDown(ErrClientConnClosing)\n\t\t}(ac)\n\t}\n\twg.Wait()\n\tcc.addTraceEvent(\"deleted\")\n\t// TraceEvent needs to be called before RemoveEntry, as TraceEvent may add\n\t// trace reference to the entity being deleted, and thus prevent it from being\n\t// deleted right away.\n\tchannelz.RemoveEntry(cc.channelz.ID)\n\n\treturn nil\n}\n\n// addrConn is a network connection to a given address.\ntype addrConn struct {\n\tctx    context.Context\n\tcancel context.CancelFunc\n\n\tcc     *ClientConn\n\tdopts  dialOptions\n\tacbw   *acBalancerWrapper\n\tscopts balancer.NewSubConnOptions\n\n\t// transport is set when there's a viable transport (note: ac state may not be READY as LB channel\n\t// health checking may require server to report healthy to set ac to READY), and is reset\n\t// to nil when the current transport should no longer be used to create a stream (e.g. after GoAway\n\t// is received, transport is closed, ac has been torn down).\n\ttransport transport.ClientTransport // The current transport.\n\n\t// This mutex is used on the RPC path, so its usage should be minimized as\n\t// much as possible.\n\t// TODO: Find a lock-free way to retrieve the transport and state from the\n\t// addrConn.\n\tmu      sync.Mutex\n\tcurAddr resolver.Address   // The current address.\n\taddrs   []resolver.Address // All addresses that the resolver resolved to.\n\n\t// Use updateConnectivityState for updating addrConn's connectivity state.\n\tstate connectivity.State\n\n\tbackoffIdx   int // Needs to be stateful for resetConnectBackoff.\n\tresetBackoff chan struct{}\n\n\tchannelz *channelz.SubChannel\n}\n\n// Note: this requires a lock on ac.mu.\nfunc (ac *addrConn) updateConnectivityState(s connectivity.State, lastErr error) {\n\tif ac.state == s {\n\t\treturn\n\t}\n\tac.state = s\n\tac.channelz.ChannelMetrics.State.Store(&s)\n\tif lastErr == nil {\n\t\tchannelz.Infof(logger, ac.channelz, \"Subchannel Connectivity change to %v\", s)\n\t} else {\n\t\tchannelz.Infof(logger, ac.channelz, \"Subchannel Connectivity change to %v, last error: %s\", s, lastErr)\n\t}\n\tac.acbw.updateState(s, ac.curAddr, lastErr)\n}\n\n// adjustParams updates parameters used to create transports upon\n// receiving a GoAway.\nfunc (ac *addrConn) adjustParams(r transport.GoAwayReason) {\n\tswitch r {\n\tcase transport.GoAwayTooManyPings:\n\t\tv := 2 * ac.dopts.copts.KeepaliveParams.Time\n\t\tac.cc.mu.Lock()\n\t\tif v > ac.cc.mkp.Time {\n\t\t\tac.cc.mkp.Time = v\n\t\t}\n\t\tac.cc.mu.Unlock()\n\t}\n}\n\n// resetTransportAndUnlock unconditionally connects the addrConn.\n//\n// ac.mu must be held by the caller, and this function will guarantee it is released.\nfunc (ac *addrConn) resetTransportAndUnlock() {\n\tacCtx := ac.ctx\n\tif acCtx.Err() != nil {\n\t\tac.mu.Unlock()\n\t\treturn\n\t}\n\n\taddrs := ac.addrs\n\tbackoffFor := ac.dopts.bs.Backoff(ac.backoffIdx)\n\t// This will be the duration that dial gets to finish.\n\tdialDuration := minConnectTimeout\n\tif ac.dopts.minConnectTimeout != nil {\n\t\tdialDuration = ac.dopts.minConnectTimeout()\n\t}\n\n\tif dialDuration < backoffFor {\n\t\t// Give dial more time as we keep failing to connect.\n\t\tdialDuration = backoffFor\n\t}\n\t// We can potentially spend all the time trying the first address, and\n\t// if the server accepts the connection and then hangs, the following\n\t// addresses will never be tried.\n\t//\n\t// The spec doesn't mention what should be done for multiple addresses.\n\t// https://github.com/grpc/grpc/blob/master/doc/connection-backoff.md#proposed-backoff-algorithm\n\tconnectDeadline := time.Now().Add(dialDuration)\n\n\tac.updateConnectivityState(connectivity.Connecting, nil)\n\tac.mu.Unlock()\n\n\tif err := ac.tryAllAddrs(acCtx, addrs, connectDeadline); err != nil {\n\t\t// TODO: #7534 - Move re-resolution requests into the pick_first LB policy\n\t\t// to ensure one resolution request per pass instead of per subconn failure.\n\t\tac.cc.resolveNow(resolver.ResolveNowOptions{})\n\t\tac.mu.Lock()\n\t\tif acCtx.Err() != nil {\n\t\t\t// addrConn was torn down.\n\t\t\tac.mu.Unlock()\n\t\t\treturn\n\t\t}\n\t\t// After exhausting all addresses, the addrConn enters\n\t\t// TRANSIENT_FAILURE.\n\t\tac.updateConnectivityState(connectivity.TransientFailure, err)\n\n\t\t// Backoff.\n\t\tb := ac.resetBackoff\n\t\tac.mu.Unlock()\n\n\t\ttimer := time.NewTimer(backoffFor)\n\t\tselect {\n\t\tcase <-timer.C:\n\t\t\tac.mu.Lock()\n\t\t\tac.backoffIdx++\n\t\t\tac.mu.Unlock()\n\t\tcase <-b:\n\t\t\ttimer.Stop()\n\t\tcase <-acCtx.Done():\n\t\t\ttimer.Stop()\n\t\t\treturn\n\t\t}\n\n\t\tac.mu.Lock()\n\t\tif acCtx.Err() == nil {\n\t\t\tac.updateConnectivityState(connectivity.Idle, err)\n\t\t}\n\t\tac.mu.Unlock()\n\t\treturn\n\t}\n\t// Success; reset backoff.\n\tac.mu.Lock()\n\tac.backoffIdx = 0\n\tac.mu.Unlock()\n}\n\n// tryAllAddrs tries to create a connection to the addresses, and stop when at\n// the first successful one. It returns an error if no address was successfully\n// connected, or updates ac appropriately with the new transport.\nfunc (ac *addrConn) tryAllAddrs(ctx context.Context, addrs []resolver.Address, connectDeadline time.Time) error {\n\tvar firstConnErr error\n\tfor _, addr := range addrs {\n\t\tac.channelz.ChannelMetrics.Target.Store(&addr.Addr)\n\t\tif ctx.Err() != nil {\n\t\t\treturn errConnClosing\n\t\t}\n\t\tac.mu.Lock()\n\n\t\tac.cc.mu.RLock()\n\t\tac.dopts.copts.KeepaliveParams = ac.cc.mkp\n\t\tac.cc.mu.RUnlock()\n\n\t\tcopts := ac.dopts.copts\n\t\tif ac.scopts.CredsBundle != nil {\n\t\t\tcopts.CredsBundle = ac.scopts.CredsBundle\n\t\t}\n\t\tac.mu.Unlock()\n\n\t\tchannelz.Infof(logger, ac.channelz, \"Subchannel picks a new address %q to connect\", addr.Addr)\n\n\t\terr := ac.createTransport(ctx, addr, copts, connectDeadline)\n\t\tif err == nil {\n\t\t\treturn nil\n\t\t}\n\t\tif firstConnErr == nil {\n\t\t\tfirstConnErr = err\n\t\t}\n\t\tac.cc.updateConnectionError(err)\n\t}\n\n\t// Couldn't connect to any address.\n\treturn firstConnErr\n}\n\n// createTransport creates a connection to addr. It returns an error if the\n// address was not successfully connected, or updates ac appropriately with the\n// new transport.\nfunc (ac *addrConn) createTransport(ctx context.Context, addr resolver.Address, copts transport.ConnectOptions, connectDeadline time.Time) error {\n\taddr.ServerName = ac.cc.getServerName(addr)\n\thctx, hcancel := context.WithCancel(ctx)\n\n\tonClose := func(r transport.GoAwayReason) {\n\t\tac.mu.Lock()\n\t\tdefer ac.mu.Unlock()\n\t\t// adjust params based on GoAwayReason\n\t\tac.adjustParams(r)\n\t\tif ctx.Err() != nil {\n\t\t\t// Already shut down or connection attempt canceled.  tearDown() or\n\t\t\t// updateAddrs() already cleared the transport and canceled hctx\n\t\t\t// via ac.ctx, and we expected this connection to be closed, so do\n\t\t\t// nothing here.\n\t\t\treturn\n\t\t}\n\t\thcancel()\n\t\tif ac.transport == nil {\n\t\t\t// We're still connecting to this address, which could error.  Do\n\t\t\t// not update the connectivity state or resolve; these will happen\n\t\t\t// at the end of the tryAllAddrs connection loop in the event of an\n\t\t\t// error.\n\t\t\treturn\n\t\t}\n\t\tac.transport = nil\n\t\t// Refresh the name resolver on any connection loss.\n\t\tac.cc.resolveNow(resolver.ResolveNowOptions{})\n\t\t// Always go idle and wait for the LB policy to initiate a new\n\t\t// connection attempt.\n\t\tac.updateConnectivityState(connectivity.Idle, nil)\n\t}\n\n\tconnectCtx, cancel := context.WithDeadline(ctx, connectDeadline)\n\tdefer cancel()\n\tcopts.ChannelzParent = ac.channelz\n\n\tnewTr, err := transport.NewHTTP2Client(connectCtx, ac.cc.ctx, addr, copts, onClose)\n\tif err != nil {\n\t\tif logger.V(2) {\n\t\t\tlogger.Infof(\"Creating new client transport to %q: %v\", addr, err)\n\t\t}\n\t\t// newTr is either nil, or closed.\n\t\thcancel()\n\t\tchannelz.Warningf(logger, ac.channelz, \"grpc: addrConn.createTransport failed to connect to %s. Err: %v\", addr, err)\n\t\treturn err\n\t}\n\n\tac.mu.Lock()\n\tdefer ac.mu.Unlock()\n\tif ctx.Err() != nil {\n\t\t// This can happen if the subConn was removed while in `Connecting`\n\t\t// state. tearDown() would have set the state to `Shutdown`, but\n\t\t// would not have closed the transport since ac.transport would not\n\t\t// have been set at that point.\n\t\t//\n\t\t// We run this in a goroutine because newTr.Close() calls onClose()\n\t\t// inline, which requires locking ac.mu.\n\t\t//\n\t\t// The error we pass to Close() is immaterial since there are no open\n\t\t// streams at this point, so no trailers with error details will be sent\n\t\t// out. We just need to pass a non-nil error.\n\t\t//\n\t\t// This can also happen when updateAddrs is called during a connection\n\t\t// attempt.\n\t\tgo newTr.Close(transport.ErrConnClosing)\n\t\treturn nil\n\t}\n\tif hctx.Err() != nil {\n\t\t// onClose was already called for this connection, but the connection\n\t\t// was successfully established first.  Consider it a success and set\n\t\t// the new state to Idle.\n\t\tac.updateConnectivityState(connectivity.Idle, nil)\n\t\treturn nil\n\t}\n\tac.curAddr = addr\n\tac.transport = newTr\n\tac.startHealthCheck(hctx) // Will set state to READY if appropriate.\n\treturn nil\n}\n\n// startHealthCheck starts the health checking stream (RPC) to watch the health\n// stats of this connection if health checking is requested and configured.\n//\n// LB channel health checking is enabled when all requirements below are met:\n// 1. it is not disabled by the user with the WithDisableHealthCheck DialOption\n// 2. internal.HealthCheckFunc is set by importing the grpc/health package\n// 3. a service config with non-empty healthCheckConfig field is provided\n// 4. the load balancer requests it\n//\n// It sets addrConn to READY if the health checking stream is not started.\n//\n// Caller must hold ac.mu.\nfunc (ac *addrConn) startHealthCheck(ctx context.Context) {\n\tvar healthcheckManagingState bool\n\tdefer func() {\n\t\tif !healthcheckManagingState {\n\t\t\tac.updateConnectivityState(connectivity.Ready, nil)\n\t\t}\n\t}()\n\n\tif ac.cc.dopts.disableHealthCheck {\n\t\treturn\n\t}\n\thealthCheckConfig := ac.cc.healthCheckConfig()\n\tif healthCheckConfig == nil {\n\t\treturn\n\t}\n\tif !ac.scopts.HealthCheckEnabled {\n\t\treturn\n\t}\n\thealthCheckFunc := internal.HealthCheckFunc\n\tif healthCheckFunc == nil {\n\t\t// The health package is not imported to set health check function.\n\t\t//\n\t\t// TODO: add a link to the health check doc in the error message.\n\t\tchannelz.Error(logger, ac.channelz, \"Health check is requested but health check function is not set.\")\n\t\treturn\n\t}\n\n\thealthcheckManagingState = true\n\n\t// Set up the health check helper functions.\n\tcurrentTr := ac.transport\n\tnewStream := func(method string) (any, error) {\n\t\tac.mu.Lock()\n\t\tif ac.transport != currentTr {\n\t\t\tac.mu.Unlock()\n\t\t\treturn nil, status.Error(codes.Canceled, \"the provided transport is no longer valid to use\")\n\t\t}\n\t\tac.mu.Unlock()\n\t\treturn newNonRetryClientStream(ctx, &StreamDesc{ServerStreams: true}, method, currentTr, ac)\n\t}\n\tsetConnectivityState := func(s connectivity.State, lastErr error) {\n\t\tac.mu.Lock()\n\t\tdefer ac.mu.Unlock()\n\t\tif ac.transport != currentTr {\n\t\t\treturn\n\t\t}\n\t\tac.updateConnectivityState(s, lastErr)\n\t}\n\t// Start the health checking stream.\n\tgo func() {\n\t\terr := healthCheckFunc(ctx, newStream, setConnectivityState, healthCheckConfig.ServiceName)\n\t\tif err != nil {\n\t\t\tif status.Code(err) == codes.Unimplemented {\n\t\t\t\tchannelz.Error(logger, ac.channelz, \"Subchannel health check is unimplemented at server side, thus health check is disabled\")\n\t\t\t} else {\n\t\t\t\tchannelz.Errorf(logger, ac.channelz, \"Health checking failed: %v\", err)\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc (ac *addrConn) resetConnectBackoff() {\n\tac.mu.Lock()\n\tclose(ac.resetBackoff)\n\tac.backoffIdx = 0\n\tac.resetBackoff = make(chan struct{})\n\tac.mu.Unlock()\n}\n\n// getReadyTransport returns the transport if ac's state is READY or nil if not.\nfunc (ac *addrConn) getReadyTransport() transport.ClientTransport {\n\tac.mu.Lock()\n\tdefer ac.mu.Unlock()\n\tif ac.state == connectivity.Ready {\n\t\treturn ac.transport\n\t}\n\treturn nil\n}\n\n// tearDown starts to tear down the addrConn.\n//\n// Note that tearDown doesn't remove ac from ac.cc.conns, so the addrConn struct\n// will leak. In most cases, call cc.removeAddrConn() instead.\nfunc (ac *addrConn) tearDown(err error) {\n\tac.mu.Lock()\n\tif ac.state == connectivity.Shutdown {\n\t\tac.mu.Unlock()\n\t\treturn\n\t}\n\tcurTr := ac.transport\n\tac.transport = nil\n\t// We have to set the state to Shutdown before anything else to prevent races\n\t// between setting the state and logic that waits on context cancellation / etc.\n\tac.updateConnectivityState(connectivity.Shutdown, nil)\n\tac.cancel()\n\tac.curAddr = resolver.Address{}\n\n\tchannelz.AddTraceEvent(logger, ac.channelz, 0, &channelz.TraceEvent{\n\t\tDesc:     \"Subchannel deleted\",\n\t\tSeverity: channelz.CtInfo,\n\t\tParent: &channelz.TraceEvent{\n\t\t\tDesc:     fmt.Sprintf(\"Subchannel(id:%d) deleted\", ac.channelz.ID),\n\t\t\tSeverity: channelz.CtInfo,\n\t\t},\n\t})\n\t// TraceEvent needs to be called before RemoveEntry, as TraceEvent may add\n\t// trace reference to the entity being deleted, and thus prevent it from\n\t// being deleted right away.\n\tchannelz.RemoveEntry(ac.channelz.ID)\n\tac.mu.Unlock()\n\n\t// We have to release the lock before the call to GracefulClose/Close here\n\t// because both of them call onClose(), which requires locking ac.mu.\n\tif curTr != nil {\n\t\tif err == errConnDrain {\n\t\t\t// Close the transport gracefully when the subConn is being shutdown.\n\t\t\t//\n\t\t\t// GracefulClose() may be executed multiple times if:\n\t\t\t// - multiple GoAway frames are received from the server\n\t\t\t// - there are concurrent name resolver or balancer triggered\n\t\t\t//   address removal and GoAway\n\t\t\tcurTr.GracefulClose()\n\t\t} else {\n\t\t\t// Hard close the transport when the channel is entering idle or is\n\t\t\t// being shutdown. In the case where the channel is being shutdown,\n\t\t\t// closing of transports is also taken care of by cancellation of cc.ctx.\n\t\t\t// But in the case where the channel is entering idle, we need to\n\t\t\t// explicitly close the transports here. Instead of distinguishing\n\t\t\t// between these two cases, it is simpler to close the transport\n\t\t\t// unconditionally here.\n\t\t\tcurTr.Close(err)\n\t\t}\n\t}\n}\n\ntype retryThrottler struct {\n\tmax    float64\n\tthresh float64\n\tratio  float64\n\n\tmu     sync.Mutex\n\ttokens float64 // TODO(dfawley): replace with atomic and remove lock.\n}\n\n// throttle subtracts a retry token from the pool and returns whether a retry\n// should be throttled (disallowed) based upon the retry throttling policy in\n// the service config.\nfunc (rt *retryThrottler) throttle() bool {\n\tif rt == nil {\n\t\treturn false\n\t}\n\trt.mu.Lock()\n\tdefer rt.mu.Unlock()\n\trt.tokens--\n\tif rt.tokens < 0 {\n\t\trt.tokens = 0\n\t}\n\treturn rt.tokens <= rt.thresh\n}\n\nfunc (rt *retryThrottler) successfulRPC() {\n\tif rt == nil {\n\t\treturn\n\t}\n\trt.mu.Lock()\n\tdefer rt.mu.Unlock()\n\trt.tokens += rt.ratio\n\tif rt.tokens > rt.max {\n\t\trt.tokens = rt.max\n\t}\n}\n\nfunc (ac *addrConn) incrCallsStarted() {\n\tac.channelz.ChannelMetrics.CallsStarted.Add(1)\n\tac.channelz.ChannelMetrics.LastCallStartedTimestamp.Store(time.Now().UnixNano())\n}\n\nfunc (ac *addrConn) incrCallsSucceeded() {\n\tac.channelz.ChannelMetrics.CallsSucceeded.Add(1)\n}\n\nfunc (ac *addrConn) incrCallsFailed() {\n\tac.channelz.ChannelMetrics.CallsFailed.Add(1)\n}\n\n// ErrClientConnTimeout indicates that the ClientConn cannot establish the\n// underlying connections within the specified timeout.\n//\n// Deprecated: This error is never returned by grpc and should not be\n// referenced by users.\nvar ErrClientConnTimeout = errors.New(\"grpc: timed out when dialing\")\n\n// getResolver finds the scheme in the cc's resolvers or the global registry.\n// scheme should always be lowercase (typically by virtue of url.Parse()\n// performing proper RFC3986 behavior).\nfunc (cc *ClientConn) getResolver(scheme string) resolver.Builder {\n\tfor _, rb := range cc.dopts.resolvers {\n\t\tif scheme == rb.Scheme() {\n\t\t\treturn rb\n\t\t}\n\t}\n\treturn resolver.Get(scheme)\n}\n\nfunc (cc *ClientConn) updateConnectionError(err error) {\n\tcc.lceMu.Lock()\n\tcc.lastConnectionError = err\n\tcc.lceMu.Unlock()\n}\n\nfunc (cc *ClientConn) connectionError() error {\n\tcc.lceMu.Lock()\n\tdefer cc.lceMu.Unlock()\n\treturn cc.lastConnectionError\n}\n\n// initParsedTargetAndResolverBuilder parses the user's dial target and stores\n// the parsed target in `cc.parsedTarget`.\n//\n// The resolver to use is determined based on the scheme in the parsed target\n// and the same is stored in `cc.resolverBuilder`.\n//\n// Doesn't grab cc.mu as this method is expected to be called only at Dial time.\nfunc (cc *ClientConn) initParsedTargetAndResolverBuilder() error {\n\tlogger.Infof(\"original dial target is: %q\", cc.target)\n\n\tvar rb resolver.Builder\n\tparsedTarget, err := parseTarget(cc.target)\n\tif err == nil {\n\t\trb = cc.getResolver(parsedTarget.URL.Scheme)\n\t\tif rb != nil {\n\t\t\tcc.parsedTarget = parsedTarget\n\t\t\tcc.resolverBuilder = rb\n\t\t\treturn nil\n\t\t}\n\t}\n\n\t// We are here because the user's dial target did not contain a scheme or\n\t// specified an unregistered scheme. We should fallback to the default\n\t// scheme, except when a custom dialer is specified in which case, we should\n\t// always use passthrough scheme. For either case, we need to respect any overridden\n\t// global defaults set by the user.\n\tdefScheme := cc.dopts.defaultScheme\n\tif internal.UserSetDefaultScheme {\n\t\tdefScheme = resolver.GetDefaultScheme()\n\t}\n\n\tcanonicalTarget := defScheme + \":///\" + cc.target\n\n\tparsedTarget, err = parseTarget(canonicalTarget)\n\tif err != nil {\n\t\treturn err\n\t}\n\trb = cc.getResolver(parsedTarget.URL.Scheme)\n\tif rb == nil {\n\t\treturn fmt.Errorf(\"could not get resolver for default scheme: %q\", parsedTarget.URL.Scheme)\n\t}\n\tcc.parsedTarget = parsedTarget\n\tcc.resolverBuilder = rb\n\treturn nil\n}\n\n// parseTarget uses RFC 3986 semantics to parse the given target into a\n// resolver.Target struct containing url. Query params are stripped from the\n// endpoint.\nfunc parseTarget(target string) (resolver.Target, error) {\n\tu, err := url.Parse(target)\n\tif err != nil {\n\t\treturn resolver.Target{}, err\n\t}\n\n\treturn resolver.Target{URL: *u}, nil\n}\n\n// encodeAuthority escapes the authority string based on valid chars defined in\n// https://datatracker.ietf.org/doc/html/rfc3986#section-3.2.\nfunc encodeAuthority(authority string) string {\n\tconst upperhex = \"0123456789ABCDEF\"\n\n\t// Return for characters that must be escaped as per\n\t// Valid chars are mentioned here:\n\t// https://datatracker.ietf.org/doc/html/rfc3986#section-3.2\n\tshouldEscape := func(c byte) bool {\n\t\t// Alphanum are always allowed.\n\t\tif 'a' <= c && c <= 'z' || 'A' <= c && c <= 'Z' || '0' <= c && c <= '9' {\n\t\t\treturn false\n\t\t}\n\t\tswitch c {\n\t\tcase '-', '_', '.', '~': // Unreserved characters\n\t\t\treturn false\n\t\tcase '!', '$', '&', '\\'', '(', ')', '*', '+', ',', ';', '=': // Subdelim characters\n\t\t\treturn false\n\t\tcase ':', '[', ']', '@': // Authority related delimiters\n\t\t\treturn false\n\t\t}\n\t\t// Everything else must be escaped.\n\t\treturn true\n\t}\n\n\thexCount := 0\n\tfor i := 0; i < len(authority); i++ {\n\t\tc := authority[i]\n\t\tif shouldEscape(c) {\n\t\t\thexCount++\n\t\t}\n\t}\n\n\tif hexCount == 0 {\n\t\treturn authority\n\t}\n\n\trequired := len(authority) + 2*hexCount\n\tt := make([]byte, required)\n\n\tj := 0\n\t// This logic is a barebones version of escape in the go net/url library.\n\tfor i := 0; i < len(authority); i++ {\n\t\tswitch c := authority[i]; {\n\t\tcase shouldEscape(c):\n\t\t\tt[j] = '%'\n\t\t\tt[j+1] = upperhex[c>>4]\n\t\t\tt[j+2] = upperhex[c&15]\n\t\t\tj += 3\n\t\tdefault:\n\t\t\tt[j] = authority[i]\n\t\t\tj++\n\t\t}\n\t}\n\treturn string(t)\n}\n\n// Determine channel authority. The order of precedence is as follows:\n// - user specified authority override using `WithAuthority` dial option\n// - creds' notion of server name for the authentication handshake\n// - endpoint from dial target of the form \"scheme://[authority]/endpoint\"\n//\n// Stores the determined authority in `cc.authority`.\n//\n// Returns a non-nil error if the authority returned by the transport\n// credentials do not match the authority configured through the dial option.\n//\n// Doesn't grab cc.mu as this method is expected to be called only at Dial time.\nfunc (cc *ClientConn) initAuthority() error {\n\tdopts := cc.dopts\n\t// Historically, we had two options for users to specify the serverName or\n\t// authority for a channel. One was through the transport credentials\n\t// (either in its constructor, or through the OverrideServerName() method).\n\t// The other option (for cases where WithInsecure() dial option was used)\n\t// was to use the WithAuthority() dial option.\n\t//\n\t// A few things have changed since:\n\t// - `insecure` package with an implementation of the `TransportCredentials`\n\t//   interface for the insecure case\n\t// - WithAuthority() dial option support for secure credentials\n\tauthorityFromCreds := \"\"\n\tif creds := dopts.copts.TransportCredentials; creds != nil && creds.Info().ServerName != \"\" {\n\t\tauthorityFromCreds = creds.Info().ServerName\n\t}\n\tauthorityFromDialOption := dopts.authority\n\tif (authorityFromCreds != \"\" && authorityFromDialOption != \"\") && authorityFromCreds != authorityFromDialOption {\n\t\treturn fmt.Errorf(\"ClientConn's authority from transport creds %q and dial option %q don't match\", authorityFromCreds, authorityFromDialOption)\n\t}\n\n\tendpoint := cc.parsedTarget.Endpoint()\n\tif authorityFromDialOption != \"\" {\n\t\tcc.authority = authorityFromDialOption\n\t} else if authorityFromCreds != \"\" {\n\t\tcc.authority = authorityFromCreds\n\t} else if auth, ok := cc.resolverBuilder.(resolver.AuthorityOverrider); ok {\n\t\tcc.authority = auth.OverrideAuthority(cc.parsedTarget)\n\t} else if strings.HasPrefix(endpoint, \":\") {\n\t\tcc.authority = \"localhost\" + endpoint\n\t} else {\n\t\tcc.authority = encodeAuthority(endpoint)\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "clientconn_authority_test.go",
          "type": "blob",
          "size": 4.4462890625,
          "content": "/*\n *\n * Copyright 2021 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n\t\"net\"\n\t\"testing\"\n\n\t\"google.golang.org/grpc/credentials\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n\t\"google.golang.org/grpc/testdata\"\n)\n\nfunc (s) TestClientConnAuthority(t *testing.T) {\n\tserverNameOverride := \"over.write.server.name\"\n\tcreds, err := credentials.NewClientTLSFromFile(testdata.Path(\"x509/server_ca_cert.pem\"), serverNameOverride)\n\tif err != nil {\n\t\tt.Fatalf(\"credentials.NewClientTLSFromFile(_, %q) failed: %v\", err, serverNameOverride)\n\t}\n\n\ttests := []struct {\n\t\tname          string\n\t\ttarget        string\n\t\topts          []DialOption\n\t\twantAuthority string\n\t}{\n\t\t{\n\t\t\tname:          \"default\",\n\t\t\ttarget:        \"Non-Existent.Server:8080\",\n\t\t\topts:          []DialOption{WithTransportCredentials(insecure.NewCredentials())},\n\t\t\twantAuthority: \"Non-Existent.Server:8080\",\n\t\t},\n\t\t{\n\t\t\tname:          \"override-via-creds\",\n\t\t\ttarget:        \"Non-Existent.Server:8080\",\n\t\t\topts:          []DialOption{WithTransportCredentials(creds)},\n\t\t\twantAuthority: serverNameOverride,\n\t\t},\n\t\t{\n\t\t\tname:          \"override-via-WithAuthority\",\n\t\t\ttarget:        \"Non-Existent.Server:8080\",\n\t\t\topts:          []DialOption{WithTransportCredentials(insecure.NewCredentials()), WithAuthority(\"authority-override\")},\n\t\t\twantAuthority: \"authority-override\",\n\t\t},\n\t\t{\n\t\t\tname:          \"override-via-creds-and-WithAuthority\",\n\t\t\ttarget:        \"Non-Existent.Server:8080\",\n\t\t\topts:          []DialOption{WithTransportCredentials(creds), WithAuthority(serverNameOverride)},\n\t\t\twantAuthority: serverNameOverride,\n\t\t},\n\t\t{\n\t\t\tname:          \"unix relative\",\n\t\t\ttarget:        \"unix:sock.sock\",\n\t\t\topts:          []DialOption{WithTransportCredentials(insecure.NewCredentials())},\n\t\t\twantAuthority: \"localhost\",\n\t\t},\n\t\t{\n\t\t\tname:   \"unix relative with custom dialer\",\n\t\t\ttarget: \"unix:sock.sock\",\n\t\t\topts: []DialOption{WithTransportCredentials(insecure.NewCredentials()), WithContextDialer(func(ctx context.Context, addr string) (net.Conn, error) {\n\t\t\t\treturn (&net.Dialer{}).DialContext(ctx, \"\", addr)\n\t\t\t})},\n\t\t\twantAuthority: \"localhost\",\n\t\t},\n\t\t{\n\t\t\tname:          \"unix absolute\",\n\t\t\ttarget:        \"unix:/sock.sock\",\n\t\t\topts:          []DialOption{WithTransportCredentials(insecure.NewCredentials())},\n\t\t\twantAuthority: \"localhost\",\n\t\t},\n\t\t{\n\t\t\tname:   \"unix absolute with custom dialer\",\n\t\t\ttarget: \"unix:///sock.sock\",\n\t\t\topts: []DialOption{WithTransportCredentials(insecure.NewCredentials()), WithContextDialer(func(ctx context.Context, addr string) (net.Conn, error) {\n\t\t\t\treturn (&net.Dialer{}).DialContext(ctx, \"\", addr)\n\t\t\t})},\n\t\t\twantAuthority: \"localhost\",\n\t\t},\n\t\t{\n\t\t\tname:          \"localhost colon port\",\n\t\t\ttarget:        \"localhost:50051\",\n\t\t\topts:          []DialOption{WithTransportCredentials(insecure.NewCredentials())},\n\t\t\twantAuthority: \"localhost:50051\",\n\t\t},\n\t\t{\n\t\t\tname:          \"colon port\",\n\t\t\ttarget:        \":50051\",\n\t\t\topts:          []DialOption{WithTransportCredentials(insecure.NewCredentials())},\n\t\t\twantAuthority: \"localhost:50051\",\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tcc, err := Dial(test.target, test.opts...)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Dial(%q) failed: %v\", test.target, err)\n\t\t\t}\n\t\t\tdefer cc.Close()\n\t\t\tif cc.authority != test.wantAuthority {\n\t\t\t\tt.Fatalf(\"cc.authority = %q, want %q\", cc.authority, test.wantAuthority)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc (s) TestClientConnAuthority_CredsAndDialOptionMismatch(t *testing.T) {\n\tserverNameOverride := \"over.write.server.name\"\n\tcreds, err := credentials.NewClientTLSFromFile(testdata.Path(\"x509/server_ca_cert.pem\"), serverNameOverride)\n\tif err != nil {\n\t\tt.Fatalf(\"credentials.NewClientTLSFromFile(_, %q) failed: %v\", err, serverNameOverride)\n\t}\n\topts := []DialOption{WithTransportCredentials(creds), WithAuthority(\"authority-override\")}\n\tif cc, err := NewClient(\"Non-Existent.Server:8000\", opts...); err == nil {\n\t\tcc.Close()\n\t\tt.Fatal(\"grpc.NewClient() succeeded when expected to fail\")\n\t}\n}\n"
        },
        {
          "name": "clientconn_parsed_target_test.go",
          "type": "blob",
          "size": 9.7978515625,
          "content": "/*\n *\n * Copyright 2021 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/go-cmp/cmp\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n\t\"google.golang.org/grpc/internal\"\n\t\"google.golang.org/grpc/internal/testutils\"\n\t\"google.golang.org/grpc/resolver\"\n)\n\nfunc generateTarget(target string) resolver.Target {\n\treturn resolver.Target{URL: *testutils.MustParseURL(target)}\n}\n\n// Resets the default scheme as though it was never set by the user.\nfunc resetInitialResolverState() {\n\tresolver.SetDefaultScheme(\"passthrough\")\n\tinternal.UserSetDefaultScheme = false\n}\n\ntype testResolverForParser struct {\n\tresolver.Resolver\n}\n\nfunc (testResolverForParser) Build(resolver.Target, resolver.ClientConn, resolver.BuildOptions) (resolver.Resolver, error) {\n\treturn testResolverForParser{}, nil\n}\n\nfunc (testResolverForParser) Close() {}\n\nfunc (testResolverForParser) Scheme() string {\n\treturn \"testresolverforparser\"\n}\n\nfunc init() { resolver.Register(testResolverForParser{}) }\n\nfunc (s) TestParsedTarget_Success_WithoutCustomDialer(t *testing.T) {\n\ttests := []struct {\n\t\ttarget             string\n\t\twantDialParse      resolver.Target\n\t\twantNewClientParse resolver.Target\n\t\twantCustomParse    resolver.Target\n\t}{\n\t\t// No scheme is specified.\n\t\t{\n\t\t\ttarget:             \"://a/b\",\n\t\t\twantDialParse:      generateTarget(\"passthrough:///://a/b\"),\n\t\t\twantNewClientParse: generateTarget(\"dns:///://a/b\"),\n\t\t\twantCustomParse:    generateTarget(\"testresolverforparser:///://a/b\"),\n\t\t},\n\t\t{\n\t\t\ttarget:             \"a//b\",\n\t\t\twantDialParse:      generateTarget(\"passthrough:///a//b\"),\n\t\t\twantNewClientParse: generateTarget(\"dns:///a//b\"),\n\t\t\twantCustomParse:    generateTarget(\"testresolverforparser:///a//b\"),\n\t\t},\n\n\t\t// An unregistered scheme is specified.\n\t\t{\n\t\t\ttarget:             \"a:///\",\n\t\t\twantDialParse:      generateTarget(\"passthrough:///a:///\"),\n\t\t\twantNewClientParse: generateTarget(\"dns:///a:///\"),\n\t\t\twantCustomParse:    generateTarget(\"testresolverforparser:///a:///\"),\n\t\t},\n\t\t{\n\t\t\ttarget:             \"a:b\",\n\t\t\twantDialParse:      generateTarget(\"passthrough:///a:b\"),\n\t\t\twantNewClientParse: generateTarget(\"dns:///a:b\"),\n\t\t\twantCustomParse:    generateTarget(\"testresolverforparser:///a:b\"),\n\t\t},\n\n\t\t// A registered scheme is specified.\n\t\t{\n\t\t\ttarget:             \"dns://a.server.com/google.com\",\n\t\t\twantDialParse:      generateTarget(\"dns://a.server.com/google.com\"),\n\t\t\twantNewClientParse: generateTarget(\"dns://a.server.com/google.com\"),\n\t\t\twantCustomParse:    generateTarget(\"dns://a.server.com/google.com\"),\n\t\t},\n\t\t{\n\t\t\ttarget:             \"unix-abstract:/ a///://::!@#$%25^&*()b\",\n\t\t\twantDialParse:      generateTarget(\"unix-abstract:/ a///://::!@#$%25^&*()b\"),\n\t\t\twantNewClientParse: generateTarget(\"unix-abstract:/ a///://::!@#$%25^&*()b\"),\n\t\t\twantCustomParse:    generateTarget(\"unix-abstract:/ a///://::!@#$%25^&*()b\"),\n\t\t},\n\t\t{\n\t\t\ttarget:             \"unix-abstract:passthrough:abc\",\n\t\t\twantDialParse:      generateTarget(\"unix-abstract:passthrough:abc\"),\n\t\t\twantNewClientParse: generateTarget(\"unix-abstract:passthrough:abc\"),\n\t\t\twantCustomParse:    generateTarget(\"unix-abstract:passthrough:abc\"),\n\t\t},\n\t\t{\n\t\t\ttarget:             \"passthrough:///unix:///a/b/c\",\n\t\t\twantDialParse:      generateTarget(\"passthrough:///unix:///a/b/c\"),\n\t\t\twantNewClientParse: generateTarget(\"passthrough:///unix:///a/b/c\"),\n\t\t\twantCustomParse:    generateTarget(\"passthrough:///unix:///a/b/c\"),\n\t\t},\n\n\t\t// Cases for `scheme:absolute-path`.\n\t\t{\n\t\t\ttarget:             \"dns:/a/b/c\",\n\t\t\twantDialParse:      generateTarget(\"dns:/a/b/c\"),\n\t\t\twantNewClientParse: generateTarget(\"dns:/a/b/c\"),\n\t\t\twantCustomParse:    generateTarget(\"dns:/a/b/c\"),\n\t\t},\n\t\t{\n\t\t\ttarget:             \"unregistered:/a/b/c\",\n\t\t\twantDialParse:      generateTarget(\"passthrough:///unregistered:/a/b/c\"),\n\t\t\twantNewClientParse: generateTarget(\"dns:///unregistered:/a/b/c\"),\n\t\t\twantCustomParse:    generateTarget(\"testresolverforparser:///unregistered:/a/b/c\"),\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.target, func(t *testing.T) {\n\t\t\tresetInitialResolverState()\n\t\t\tcc, err := Dial(test.target, WithTransportCredentials(insecure.NewCredentials()))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Dial(%q) failed: %v\", test.target, err)\n\t\t\t}\n\t\t\tcc.Close()\n\n\t\t\tif !cmp.Equal(cc.parsedTarget, test.wantDialParse) {\n\t\t\t\tt.Errorf(\"cc.parsedTarget for dial target %q = %+v, want %+v\", test.target, cc.parsedTarget, test.wantDialParse)\n\t\t\t}\n\n\t\t\tcc, err = NewClient(test.target, WithTransportCredentials(insecure.NewCredentials()))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"NewClient(%q) failed: %v\", test.target, err)\n\t\t\t}\n\t\t\tcc.Close()\n\n\t\t\tif !cmp.Equal(cc.parsedTarget, test.wantNewClientParse) {\n\t\t\t\tt.Errorf(\"cc.parsedTarget for newClient target %q = %+v, want %+v\", test.target, cc.parsedTarget, test.wantNewClientParse)\n\t\t\t}\n\n\t\t\tresolver.SetDefaultScheme(\"testresolverforparser\")\n\t\t\tcc, err = Dial(test.target, WithTransportCredentials(insecure.NewCredentials()))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Dial(%q) failed: %v\", test.target, err)\n\t\t\t}\n\t\t\tcc.Close()\n\n\t\t\tif !cmp.Equal(cc.parsedTarget, test.wantCustomParse) {\n\t\t\t\tt.Errorf(\"cc.parsedTarget for dial target %q = %+v, want %+v\", test.target, cc.parsedTarget, test.wantDialParse)\n\t\t\t}\n\n\t\t\tcc, err = NewClient(test.target, WithTransportCredentials(insecure.NewCredentials()))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"NewClient(%q) failed: %v\", test.target, err)\n\t\t\t}\n\t\t\tcc.Close()\n\n\t\t\tif !cmp.Equal(cc.parsedTarget, test.wantCustomParse) {\n\t\t\t\tt.Errorf(\"cc.parsedTarget for newClient target %q = %+v, want %+v\", test.target, cc.parsedTarget, test.wantNewClientParse)\n\t\t\t}\n\n\t\t})\n\t}\n\tresetInitialResolverState()\n}\n\nfunc (s) TestParsedTarget_Failure_WithoutCustomDialer(t *testing.T) {\n\ttargets := []string{\n\t\t\"\",\n\t\t\"unix://a/b/c\",\n\t\t\"unix://authority\",\n\t\t\"unix-abstract://authority/a/b/c\",\n\t\t\"unix-abstract://authority\",\n\t}\n\n\tfor _, target := range targets {\n\t\tt.Run(target, func(t *testing.T) {\n\t\t\tif cc, err := Dial(target, WithTransportCredentials(insecure.NewCredentials())); err == nil {\n\t\t\t\tdefer cc.Close()\n\t\t\t\tt.Fatalf(\"Dial(%q) succeeded cc.parsedTarget = %+v, expected to fail\", target, cc.parsedTarget)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc (s) TestParsedTarget_WithCustomDialer(t *testing.T) {\n\tresetInitialResolverState()\n\tdefScheme := resolver.GetDefaultScheme()\n\ttests := []struct {\n\t\ttarget            string\n\t\twantParsed        resolver.Target\n\t\twantDialerAddress string\n\t}{\n\t\t// unix:[local_path], unix:[/absolute], and unix://[/absolute] have\n\t\t// different behaviors with a custom dialer.\n\t\t{\n\t\t\ttarget:            \"unix:a/b/c\",\n\t\t\twantParsed:        resolver.Target{URL: *testutils.MustParseURL(\"unix:a/b/c\")},\n\t\t\twantDialerAddress: \"unix:a/b/c\",\n\t\t},\n\t\t{\n\t\t\ttarget:            \"unix:/a/b/c\",\n\t\t\twantParsed:        resolver.Target{URL: *testutils.MustParseURL(\"unix:/a/b/c\")},\n\t\t\twantDialerAddress: \"unix:///a/b/c\",\n\t\t},\n\t\t{\n\t\t\ttarget:            \"unix:///a/b/c\",\n\t\t\twantParsed:        resolver.Target{URL: *testutils.MustParseURL(\"unix:///a/b/c\")},\n\t\t\twantDialerAddress: \"unix:///a/b/c\",\n\t\t},\n\t\t{\n\t\t\ttarget:            \"dns:///127.0.0.1:50051\",\n\t\t\twantParsed:        resolver.Target{URL: *testutils.MustParseURL(\"dns:///127.0.0.1:50051\")},\n\t\t\twantDialerAddress: \"127.0.0.1:50051\",\n\t\t},\n\t\t{\n\t\t\ttarget:            \":///127.0.0.1:50051\",\n\t\t\twantParsed:        resolver.Target{URL: *testutils.MustParseURL(fmt.Sprintf(\"%s:///%s\", defScheme, \":///127.0.0.1:50051\"))},\n\t\t\twantDialerAddress: \":///127.0.0.1:50051\",\n\t\t},\n\t\t{\n\t\t\ttarget:            \"dns://authority/127.0.0.1:50051\",\n\t\t\twantParsed:        resolver.Target{URL: *testutils.MustParseURL(\"dns://authority/127.0.0.1:50051\")},\n\t\t\twantDialerAddress: \"127.0.0.1:50051\",\n\t\t},\n\t\t{\n\t\t\ttarget:            \"://authority/127.0.0.1:50051\",\n\t\t\twantParsed:        resolver.Target{URL: *testutils.MustParseURL(fmt.Sprintf(\"%s:///%s\", defScheme, \"://authority/127.0.0.1:50051\"))},\n\t\t\twantDialerAddress: \"://authority/127.0.0.1:50051\",\n\t\t},\n\t\t{\n\t\t\ttarget:            \"/unix/socket/address\",\n\t\t\twantParsed:        resolver.Target{URL: *testutils.MustParseURL(fmt.Sprintf(\"%s:///%s\", defScheme, \"/unix/socket/address\"))},\n\t\t\twantDialerAddress: \"/unix/socket/address\",\n\t\t},\n\t\t{\n\t\t\ttarget:            \"\",\n\t\t\twantParsed:        resolver.Target{URL: *testutils.MustParseURL(fmt.Sprintf(\"%s:///%s\", defScheme, \"\"))},\n\t\t\twantDialerAddress: \"\",\n\t\t},\n\t\t{\n\t\t\ttarget:            \"passthrough://a.server.com/google.com\",\n\t\t\twantParsed:        resolver.Target{URL: *testutils.MustParseURL(\"passthrough://a.server.com/google.com\")},\n\t\t\twantDialerAddress: \"google.com\",\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.target, func(t *testing.T) {\n\t\t\taddrCh := make(chan string, 1)\n\t\t\tdialer := func(_ context.Context, address string) (net.Conn, error) {\n\t\t\t\taddrCh <- address\n\t\t\t\treturn nil, errors.New(\"dialer error\")\n\t\t\t}\n\n\t\t\tcc, err := Dial(test.target, WithTransportCredentials(insecure.NewCredentials()), WithContextDialer(dialer))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Dial(%q) failed: %v\", test.target, err)\n\t\t\t}\n\t\t\tdefer cc.Close()\n\n\t\t\tselect {\n\t\t\tcase addr := <-addrCh:\n\t\t\t\tif addr != test.wantDialerAddress {\n\t\t\t\t\tt.Fatalf(\"address in custom dialer is %q, want %q\", addr, test.wantDialerAddress)\n\t\t\t\t}\n\t\t\tcase <-time.After(time.Second):\n\t\t\t\tt.Fatal(\"timeout when waiting for custom dialer to be invoked\")\n\t\t\t}\n\t\t\tif !cmp.Equal(cc.parsedTarget, test.wantParsed) {\n\t\t\t\tt.Errorf(\"cc.parsedTarget for dial target %q = %+v, want %+v\", test.target, cc.parsedTarget, test.wantParsed)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "clientconn_test.go",
          "type": "blob",
          "size": 38.97265625,
          "content": "/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"golang.org/x/net/http2\"\n\t\"google.golang.org/grpc/backoff\"\n\t\"google.golang.org/grpc/balancer\"\n\t\"google.golang.org/grpc/connectivity\"\n\t\"google.golang.org/grpc/credentials\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n\tinternalbackoff \"google.golang.org/grpc/internal/backoff\"\n\t\"google.golang.org/grpc/internal/envconfig\"\n\t\"google.golang.org/grpc/internal/grpcsync\"\n\t\"google.golang.org/grpc/internal/grpctest\"\n\t\"google.golang.org/grpc/internal/transport\"\n\t\"google.golang.org/grpc/keepalive\"\n\t\"google.golang.org/grpc/resolver\"\n\t\"google.golang.org/grpc/resolver/manual\"\n\t\"google.golang.org/grpc/serviceconfig\"\n\t\"google.golang.org/grpc/testdata\"\n)\n\nconst (\n\tdefaultTestTimeout         = 10 * time.Second\n\tstateRecordingBalancerName = \"state_recording_balancer\"\n)\n\nvar testBalancerBuilder = newStateRecordingBalancerBuilder()\n\nfunc init() {\n\tbalancer.Register(testBalancerBuilder)\n}\n\nfunc parseCfg(r *manual.Resolver, s string) *serviceconfig.ParseResult {\n\tscpr := r.CC.ParseServiceConfig(s)\n\tif scpr.Err != nil {\n\t\tpanic(fmt.Sprintf(\"Error parsing config %q: %v\", s, scpr.Err))\n\t}\n\treturn scpr\n}\n\nfunc (s) TestDialWithTimeout(t *testing.T) {\n\tlis, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Error while listening. Err: %v\", err)\n\t}\n\tdefer lis.Close()\n\tlisAddr := resolver.Address{Addr: lis.Addr().String()}\n\tlisDone := make(chan struct{})\n\tdialDone := make(chan struct{})\n\t// 1st listener accepts the connection and then does nothing\n\tgo func() {\n\t\tdefer close(lisDone)\n\t\tconn, err := lis.Accept()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Error while accepting. Err: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tframer := http2.NewFramer(conn, conn)\n\t\tif err := framer.WriteSettings(http2.Setting{}); err != nil {\n\t\t\tt.Errorf(\"Error while writing settings. Err: %v\", err)\n\t\t\treturn\n\t\t}\n\t\t<-dialDone // Close conn only after dial returns.\n\t}()\n\n\tr := manual.NewBuilderWithScheme(\"whatever\")\n\tr.InitialState(resolver.State{Addresses: []resolver.Address{lisAddr}})\n\tclient, err := Dial(r.Scheme()+\":///test.server\", WithTransportCredentials(insecure.NewCredentials()), WithResolvers(r), WithTimeout(5*time.Second))\n\tclose(dialDone)\n\tif err != nil {\n\t\tt.Fatalf(\"Dial failed. Err: %v\", err)\n\t}\n\tdefer client.Close()\n\ttimeout := time.After(1 * time.Second)\n\tselect {\n\tcase <-timeout:\n\t\tt.Fatal(\"timed out waiting for server to finish\")\n\tcase <-lisDone:\n\t}\n}\n\nfunc (s) TestDialWithMultipleBackendsNotSendingServerPreface(t *testing.T) {\n\tlis1, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Error while listening. Err: %v\", err)\n\t}\n\tdefer lis1.Close()\n\tlis1Addr := resolver.Address{Addr: lis1.Addr().String()}\n\tlis1Done := make(chan struct{})\n\t// 1st listener accepts the connection and immediately closes it.\n\tgo func() {\n\t\tdefer close(lis1Done)\n\t\tconn, err := lis1.Accept()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Error while accepting. Err: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tconn.Close()\n\t}()\n\n\tlis2, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Error while listening. Err: %v\", err)\n\t}\n\tdefer lis2.Close()\n\tlis2Done := make(chan struct{})\n\tlis2Addr := resolver.Address{Addr: lis2.Addr().String()}\n\t// 2nd listener should get a connection attempt since the first one failed.\n\tgo func() {\n\t\tdefer close(lis2Done)\n\t\t_, err := lis2.Accept() // Closing the client will clean up this conn.\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Error while accepting. Err: %v\", err)\n\t\t\treturn\n\t\t}\n\t}()\n\n\tr := manual.NewBuilderWithScheme(\"whatever\")\n\tr.InitialState(resolver.State{Addresses: []resolver.Address{lis1Addr, lis2Addr}})\n\tclient, err := Dial(r.Scheme()+\":///test.server\", WithTransportCredentials(insecure.NewCredentials()), WithResolvers(r))\n\tif err != nil {\n\t\tt.Fatalf(\"Dial failed. Err: %v\", err)\n\t}\n\tdefer client.Close()\n\ttimeout := time.After(5 * time.Second)\n\tselect {\n\tcase <-timeout:\n\t\tt.Fatal(\"timed out waiting for server 1 to finish\")\n\tcase <-lis1Done:\n\t}\n\tselect {\n\tcase <-timeout:\n\t\tt.Fatal(\"timed out waiting for server 2 to finish\")\n\tcase <-lis2Done:\n\t}\n}\n\nfunc (s) TestDialWaitsForServerSettings(t *testing.T) {\n\tlis, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Error while listening. Err: %v\", err)\n\t}\n\tdefer lis.Close()\n\tdone := make(chan struct{})\n\tsent := make(chan struct{})\n\tdialDone := make(chan struct{})\n\tgo func() { // Launch the server.\n\t\tdefer func() {\n\t\t\tclose(done)\n\t\t}()\n\t\tconn, err := lis.Accept()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Error while accepting. Err: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer conn.Close()\n\t\t// Sleep for a little bit to make sure that Dial on client\n\t\t// side blocks until settings are received.\n\t\ttime.Sleep(100 * time.Millisecond)\n\t\tframer := http2.NewFramer(conn, conn)\n\t\tclose(sent)\n\t\tif err := framer.WriteSettings(http2.Setting{}); err != nil {\n\t\t\tt.Errorf(\"Error while writing settings. Err: %v\", err)\n\t\t\treturn\n\t\t}\n\t\t<-dialDone // Close conn only after dial returns.\n\t}()\n\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n\tdefer cancel()\n\tclient, err := DialContext(ctx, lis.Addr().String(), WithTransportCredentials(insecure.NewCredentials()), WithBlock())\n\tclose(dialDone)\n\tif err != nil {\n\t\tt.Fatalf(\"Error while dialing. Err: %v\", err)\n\t}\n\tdefer client.Close()\n\tselect {\n\tcase <-sent:\n\tdefault:\n\t\tt.Fatalf(\"Dial returned before server settings were sent\")\n\t}\n\t<-done\n}\n\nfunc (s) TestDialWaitsForServerSettingsAndFails(t *testing.T) {\n\tlis, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Error while listening. Err: %v\", err)\n\t}\n\tdone := make(chan struct{})\n\tnumConns := 0\n\tgo func() { // Launch the server.\n\t\tdefer func() {\n\t\t\tclose(done)\n\t\t}()\n\t\tfor {\n\t\t\tconn, err := lis.Accept()\n\t\t\tif err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tnumConns++\n\t\t\tdefer conn.Close()\n\t\t}\n\t}()\n\tctx, cancel := context.WithTimeout(context.Background(), 1*time.Second)\n\tdefer cancel()\n\tclient, err := DialContext(ctx,\n\t\tlis.Addr().String(),\n\t\tWithTransportCredentials(insecure.NewCredentials()),\n\t\tWithReturnConnectionError(),\n\t\tWithConnectParams(ConnectParams{\n\t\t\tBackoff:           backoff.Config{},\n\t\t\tMinConnectTimeout: 250 * time.Millisecond,\n\t\t}))\n\tlis.Close()\n\tif err == nil {\n\t\tclient.Close()\n\t\tt.Fatalf(\"Unexpected success (err=nil) while dialing\")\n\t}\n\texpectedMsg := \"server preface\"\n\tif !strings.Contains(err.Error(), context.DeadlineExceeded.Error()) || !strings.Contains(err.Error(), expectedMsg) {\n\t\tt.Fatalf(\"DialContext(_) = %v; want a message that includes both %q and %q\", err, context.DeadlineExceeded.Error(), expectedMsg)\n\t}\n\t<-done\n\tif numConns < 2 {\n\t\tt.Fatalf(\"dial attempts: %v; want > 1\", numConns)\n\t}\n}\n\n// 1. Client connects to a server that doesn't send preface.\n// 2. After minConnectTimeout(500 ms here), client disconnects and retries.\n// 3. The new server sends its preface.\n// 4. Client doesn't kill the connection this time.\nfunc (s) TestCloseConnectionWhenServerPrefaceNotReceived(t *testing.T) {\n\tlis, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Error while listening. Err: %v\", err)\n\t}\n\tvar (\n\t\tconn2 net.Conn\n\t\tover  uint32\n\t)\n\tdefer func() {\n\t\tlis.Close()\n\t\t// conn2 shouldn't be closed until the client has\n\t\t// observed a successful test.\n\t\tif conn2 != nil {\n\t\t\tconn2.Close()\n\t\t}\n\t}()\n\tdone := make(chan struct{})\n\taccepted := make(chan struct{})\n\tgo func() { // Launch the server.\n\t\tdefer close(done)\n\t\tconn1, err := lis.Accept()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Error while accepting. Err: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer conn1.Close()\n\t\t// Don't send server settings and the client should close the connection and try again.\n\t\tconn2, err = lis.Accept() // Accept a reconnection request from client.\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Error while accepting. Err: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tclose(accepted)\n\t\tframer := http2.NewFramer(conn2, conn2)\n\t\tif err = framer.WriteSettings(http2.Setting{}); err != nil {\n\t\t\tt.Errorf(\"Error while writing settings. Err: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tb := make([]byte, 8)\n\t\tfor {\n\t\t\t_, err = conn2.Read(b)\n\t\t\tif err == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif atomic.LoadUint32(&over) == 1 {\n\t\t\t\t// The connection stayed alive for the timer.\n\t\t\t\t// Success.\n\t\t\t\treturn\n\t\t\t}\n\t\t\tt.Errorf(\"Unexpected error while reading. Err: %v, want timeout error\", err)\n\t\t\tbreak\n\t\t}\n\t}()\n\tclient, err := Dial(lis.Addr().String(), WithTransportCredentials(insecure.NewCredentials()), withMinConnectDeadline(func() time.Duration { return time.Millisecond * 500 }))\n\tif err != nil {\n\t\tt.Fatalf(\"Error while dialing. Err: %v\", err)\n\t}\n\n\tgo stayConnected(client)\n\n\t// wait for connection to be accepted on the server.\n\ttimer := time.NewTimer(time.Second * 10)\n\tselect {\n\tcase <-accepted:\n\tcase <-timer.C:\n\t\tt.Fatalf(\"Client didn't make another connection request in time.\")\n\t}\n\t// Make sure the connection stays alive for some time.\n\ttime.Sleep(time.Second)\n\tatomic.StoreUint32(&over, 1)\n\tclient.Close()\n\t<-done\n}\n\nfunc (s) TestBackoffWhenNoServerPrefaceReceived(t *testing.T) {\n\tlis, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error from net.Listen(%q, %q): %v\", \"tcp\", \"localhost:0\", err)\n\t}\n\tdefer lis.Close()\n\tdone := make(chan struct{})\n\tgo func() { // Launch the server.\n\t\tdefer close(done)\n\t\tconn, err := lis.Accept() // Accept the connection only to close it immediately.\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Error while accepting. Err: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tprevAt := time.Now()\n\t\tconn.Close()\n\t\tvar prevDuration time.Duration\n\t\t// Make sure the retry attempts are backed off properly.\n\t\tfor i := 0; i < 3; i++ {\n\t\t\tconn, err := lis.Accept()\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"Error while accepting. Err: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tmeow := time.Now()\n\t\t\tconn.Close()\n\t\t\tdr := meow.Sub(prevAt)\n\t\t\tif dr <= prevDuration {\n\t\t\t\tt.Errorf(\"Client backoff did not increase with retries. Previous duration: %v, current duration: %v\", prevDuration, dr)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tprevDuration = dr\n\t\t\tprevAt = meow\n\t\t}\n\t}()\n\tbc := backoff.Config{\n\t\tBaseDelay:  200 * time.Millisecond,\n\t\tMultiplier: 2.0,\n\t\tJitter:     0,\n\t\tMaxDelay:   120 * time.Second,\n\t}\n\tcp := ConnectParams{\n\t\tBackoff:           bc,\n\t\tMinConnectTimeout: 1 * time.Second,\n\t}\n\tcc, err := Dial(lis.Addr().String(), WithTransportCredentials(insecure.NewCredentials()), WithConnectParams(cp))\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error from Dial(%v) = %v\", lis.Addr(), err)\n\t}\n\tdefer cc.Close()\n\tgo stayConnected(cc)\n\t<-done\n}\n\nfunc (s) TestWithTimeout(t *testing.T) {\n\tconn, err := Dial(\"passthrough:///Non-Existent.Server:80\",\n\t\tWithTimeout(time.Millisecond),\n\t\tWithBlock(),\n\t\tWithTransportCredentials(insecure.NewCredentials()))\n\tif err == nil {\n\t\tconn.Close()\n\t}\n\tif err != context.DeadlineExceeded {\n\t\tt.Fatalf(\"Dial(_, _) = %v, %v, want %v\", conn, err, context.DeadlineExceeded)\n\t}\n}\n\nfunc (s) TestWithTransportCredentialsTLS(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), time.Millisecond)\n\tdefer cancel()\n\tcreds, err := credentials.NewClientTLSFromFile(testdata.Path(\"x509/server_ca_cert.pem\"), \"x.test.example.com\")\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to create credentials %v\", err)\n\t}\n\tconn, err := DialContext(ctx, \"passthrough:///Non-Existent.Server:80\", WithTransportCredentials(creds), WithBlock())\n\tif err == nil {\n\t\tconn.Close()\n\t}\n\tif err != context.DeadlineExceeded {\n\t\tt.Fatalf(\"Dial(_, _) = %v, %v, want %v\", conn, err, context.DeadlineExceeded)\n\t}\n}\n\n// When creating a transport configured with n addresses, only calculate the\n// backoff once per \"round\" of attempts instead of once per address (n times\n// per \"round\" of attempts) for old pickfirst and once per address for new pickfirst.\nfunc (s) TestDial_BackoffCountPerRetryGroup(t *testing.T) {\n\tvar attempts uint32\n\twantBackoffs := uint32(1)\n\tif envconfig.NewPickFirstEnabled {\n\t\twantBackoffs = 2\n\t}\n\tgetMinConnectTimeout := func() time.Duration {\n\t\tif atomic.AddUint32(&attempts, 1) <= wantBackoffs {\n\t\t\t// Once all addresses are exhausted, hang around and wait for the\n\t\t\t// client.Close to happen rather than re-starting a new round of\n\t\t\t// attempts.\n\t\t\treturn time.Hour\n\t\t}\n\t\tt.Errorf(\"only %d attempt backoff calculation, but got more\", wantBackoffs)\n\t\treturn 0\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)\n\tdefer cancel()\n\n\tlis1, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Error while listening. Err: %v\", err)\n\t}\n\tdefer lis1.Close()\n\n\tlis2, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Error while listening. Err: %v\", err)\n\t}\n\tdefer lis2.Close()\n\n\tserver1Done := make(chan struct{})\n\tserver2Done := make(chan struct{})\n\n\t// Launch server 1.\n\tgo func() {\n\t\tconn, err := lis1.Accept()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\treturn\n\t\t}\n\n\t\tconn.Close()\n\t\tclose(server1Done)\n\t}()\n\t// Launch server 2.\n\tgo func() {\n\t\tconn, err := lis2.Accept()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\treturn\n\t\t}\n\t\tconn.Close()\n\t\tclose(server2Done)\n\t}()\n\n\trb := manual.NewBuilderWithScheme(\"whatever\")\n\trb.InitialState(resolver.State{Addresses: []resolver.Address{\n\t\t{Addr: lis1.Addr().String()},\n\t\t{Addr: lis2.Addr().String()},\n\t}})\n\tclient, err := DialContext(ctx, \"whatever:///this-gets-overwritten\",\n\t\tWithTransportCredentials(insecure.NewCredentials()),\n\t\tWithResolvers(rb),\n\t\twithMinConnectDeadline(getMinConnectTimeout))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer client.Close()\n\n\ttimeout := time.After(15 * time.Second)\n\n\tselect {\n\tcase <-timeout:\n\t\tt.Fatal(\"timed out waiting for test to finish\")\n\tcase <-server1Done:\n\t}\n\n\tselect {\n\tcase <-timeout:\n\t\tt.Fatal(\"timed out waiting for test to finish\")\n\tcase <-server2Done:\n\t}\n\n\tif got, want := atomic.LoadUint32(&attempts), wantBackoffs; got != want {\n\t\tt.Errorf(\"attempts = %d, want %d\", got, want)\n\t}\n}\n\nfunc (s) TestDialContextCancel(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\tcancel()\n\tif _, err := DialContext(ctx, \"Non-Existent.Server:80\", WithBlock(), WithTransportCredentials(insecure.NewCredentials())); err != context.Canceled {\n\t\tt.Fatalf(\"DialContext(%v, _) = _, %v, want _, %v\", ctx, err, context.Canceled)\n\t}\n}\n\ntype failFastError struct{}\n\nfunc (failFastError) Error() string   { return \"failfast\" }\nfunc (failFastError) Temporary() bool { return false }\n\nfunc (s) TestDialContextFailFast(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n\tdefer cancel()\n\tfailErr := failFastError{}\n\tdialer := func(string, time.Duration) (net.Conn, error) {\n\t\treturn nil, failErr\n\t}\n\n\t_, err := DialContext(ctx, \"Non-Existent.Server:80\", WithBlock(), WithTransportCredentials(insecure.NewCredentials()), WithDialer(dialer), FailOnNonTempDialError(true))\n\tif terr, ok := err.(transport.ConnectionError); !ok || terr.Origin() != failErr {\n\t\tt.Fatalf(\"DialContext() = _, %v, want _, %v\", err, failErr)\n\t}\n}\n\n// securePerRPCCredentials always requires transport security.\ntype securePerRPCCredentials struct {\n\tcredentials.PerRPCCredentials\n}\n\nfunc (c securePerRPCCredentials) RequireTransportSecurity() bool {\n\treturn true\n}\n\ntype fakeBundleCreds struct {\n\tcredentials.Bundle\n\ttransportCreds credentials.TransportCredentials\n}\n\nfunc (b *fakeBundleCreds) TransportCredentials() credentials.TransportCredentials {\n\treturn b.transportCreds\n}\n\nfunc (s) TestCredentialsMisuse(t *testing.T) {\n\t// Use of no transport creds and no creds bundle must fail.\n\tif _, err := Dial(\"passthrough:///Non-Existent.Server:80\"); err != errNoTransportSecurity {\n\t\tt.Fatalf(\"Dial(_, _) = _, %v, want _, %v\", err, errNoTransportSecurity)\n\t}\n\n\t// Use of both transport creds and creds bundle must fail.\n\tcreds, err := credentials.NewClientTLSFromFile(testdata.Path(\"x509/server_ca_cert.pem\"), \"x.test.example.com\")\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to create authenticator %v\", err)\n\t}\n\tdopts := []DialOption{\n\t\tWithTransportCredentials(creds),\n\t\tWithCredentialsBundle(&fakeBundleCreds{transportCreds: creds}),\n\t}\n\tif _, err := Dial(\"passthrough:///Non-Existent.Server:80\", dopts...); err != errTransportCredsAndBundle {\n\t\tt.Fatalf(\"Dial(_, _) = _, %v, want _, %v\", err, errTransportCredsAndBundle)\n\t}\n\n\t// Use of perRPC creds requiring transport security over an insecure\n\t// transport must fail.\n\tif _, err := Dial(\"passthrough:///Non-Existent.Server:80\", WithPerRPCCredentials(securePerRPCCredentials{}), WithTransportCredentials(insecure.NewCredentials())); err != errTransportCredentialsMissing {\n\t\tt.Fatalf(\"Dial(_, _) = _, %v, want _, %v\", err, errTransportCredentialsMissing)\n\t}\n\n\t// Use of a creds bundle with nil transport credentials must fail.\n\tif _, err := Dial(\"passthrough:///Non-Existent.Server:80\", WithCredentialsBundle(&fakeBundleCreds{})); err != errNoTransportCredsInBundle {\n\t\tt.Fatalf(\"Dial(_, _) = _, %v, want _, %v\", err, errTransportCredsAndBundle)\n\t}\n}\n\nfunc (s) TestWithBackoffConfigDefault(t *testing.T) {\n\ttestBackoffConfigSet(t, internalbackoff.DefaultExponential)\n}\n\nfunc (s) TestWithBackoffConfig(t *testing.T) {\n\tb := BackoffConfig{MaxDelay: DefaultBackoffConfig.MaxDelay / 2}\n\tbc := backoff.DefaultConfig\n\tbc.MaxDelay = b.MaxDelay\n\twantBackoff := internalbackoff.Exponential{Config: bc}\n\ttestBackoffConfigSet(t, wantBackoff, WithBackoffConfig(b))\n}\n\nfunc (s) TestWithBackoffMaxDelay(t *testing.T) {\n\tmd := DefaultBackoffConfig.MaxDelay / 2\n\tbc := backoff.DefaultConfig\n\tbc.MaxDelay = md\n\twantBackoff := internalbackoff.Exponential{Config: bc}\n\ttestBackoffConfigSet(t, wantBackoff, WithBackoffMaxDelay(md))\n}\n\nfunc (s) TestWithConnectParams(t *testing.T) {\n\tbd := 2 * time.Second\n\tmltpr := 2.0\n\tjitter := 0.0\n\tbc := backoff.Config{BaseDelay: bd, Multiplier: mltpr, Jitter: jitter}\n\n\tcrt := ConnectParams{Backoff: bc}\n\t// MaxDelay is not set in the ConnectParams. So it should not be set on\n\t// internalbackoff.Exponential as well.\n\twantBackoff := internalbackoff.Exponential{Config: bc}\n\ttestBackoffConfigSet(t, wantBackoff, WithConnectParams(crt))\n}\n\nfunc testBackoffConfigSet(t *testing.T, wantBackoff internalbackoff.Exponential, opts ...DialOption) {\n\topts = append(opts, WithTransportCredentials(insecure.NewCredentials()))\n\tconn, err := Dial(\"passthrough:///foo:80\", opts...)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected error dialing connection: %v\", err)\n\t}\n\tdefer conn.Close()\n\n\tif conn.dopts.bs == nil {\n\t\tt.Fatalf(\"backoff config not set\")\n\t}\n\n\tgotBackoff, ok := conn.dopts.bs.(internalbackoff.Exponential)\n\tif !ok {\n\t\tt.Fatalf(\"unexpected type of backoff config: %#v\", conn.dopts.bs)\n\t}\n\n\tif gotBackoff != wantBackoff {\n\t\tt.Fatalf(\"unexpected backoff config on connection: %v, want %v\", gotBackoff, wantBackoff)\n\t}\n}\n\nfunc (s) TestConnectParamsWithMinConnectTimeout(t *testing.T) {\n\t// Default value specified for minConnectTimeout in the spec is 20 seconds.\n\tmct := 1 * time.Minute\n\tconn, err := Dial(\"passthrough:///foo:80\", WithTransportCredentials(insecure.NewCredentials()), WithConnectParams(ConnectParams{MinConnectTimeout: mct}))\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected error dialing connection: %v\", err)\n\t}\n\tdefer conn.Close()\n\n\tif got := conn.dopts.minConnectTimeout(); got != mct {\n\t\tt.Errorf(\"unexpected minConnectTimeout on the connection: %v, want %v\", got, mct)\n\t}\n}\n\nfunc (s) TestResolverServiceConfigBeforeAddressNotPanic(t *testing.T) {\n\tr := manual.NewBuilderWithScheme(\"whatever\")\n\n\tcc, err := Dial(r.Scheme()+\":///test.server\", WithTransportCredentials(insecure.NewCredentials()), WithResolvers(r))\n\tif err != nil {\n\t\tt.Fatalf(\"failed to dial: %v\", err)\n\t}\n\tdefer cc.Close()\n\n\t// SwitchBalancer before NewAddress. There was no balancer created, this\n\t// makes sure we don't call close on nil balancerWrapper.\n\tr.UpdateState(resolver.State{ServiceConfig: parseCfg(r, `{\"loadBalancingPolicy\": \"round_robin\"}`)}) // This should not panic.\n\n\ttime.Sleep(time.Second) // Sleep to make sure the service config is handled by ClientConn.\n}\n\nfunc (s) TestResolverServiceConfigWhileClosingNotPanic(t *testing.T) {\n\tfor i := 0; i < 10; i++ { // Run this multiple times to make sure it doesn't panic.\n\t\tr := manual.NewBuilderWithScheme(fmt.Sprintf(\"whatever-%d\", i))\n\n\t\tcc, err := Dial(r.Scheme()+\":///test.server\", WithTransportCredentials(insecure.NewCredentials()), WithResolvers(r))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to dial: %v\", err)\n\t\t}\n\t\t// Send a new service config while closing the ClientConn.\n\t\tgo cc.Close()\n\t\tgo r.UpdateState(resolver.State{ServiceConfig: parseCfg(r, `{\"loadBalancingPolicy\": \"round_robin\"}`)}) // This should not panic.\n\t}\n}\n\nfunc (s) TestResolverEmptyUpdateNotPanic(t *testing.T) {\n\tr := manual.NewBuilderWithScheme(\"whatever\")\n\n\tcc, err := Dial(r.Scheme()+\":///test.server\", WithTransportCredentials(insecure.NewCredentials()), WithResolvers(r))\n\tif err != nil {\n\t\tt.Fatalf(\"failed to dial: %v\", err)\n\t}\n\tdefer cc.Close()\n\n\t// This make sure we don't create addrConn with empty address list.\n\tr.UpdateState(resolver.State{}) // This should not panic.\n\n\ttime.Sleep(time.Second) // Sleep to make sure the service config is handled by ClientConn.\n}\n\nfunc (s) TestClientUpdatesParamsAfterGoAway(t *testing.T) {\n\tgrpctest.TLogger.ExpectError(\"Client received GoAway with error code ENHANCE_YOUR_CALM and debug data equal to ASCII \\\"too_many_pings\\\"\")\n\n\tlis, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to listen. Err: %v\", err)\n\t}\n\tdefer lis.Close()\n\tconnected := grpcsync.NewEvent()\n\tdefer connected.Fire()\n\tgo func() {\n\t\tconn, err := lis.Accept()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"error accepting connection: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer conn.Close()\n\t\tf := http2.NewFramer(conn, conn)\n\t\t// Start a goroutine to read from the conn to prevent the client from\n\t\t// blocking after it writes its preface.\n\t\tgo func() {\n\t\t\tfor {\n\t\t\t\tif _, err := f.ReadFrame(); err != nil {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t\tif err := f.WriteSettings(http2.Setting{}); err != nil {\n\t\t\tt.Errorf(\"error writing settings: %v\", err)\n\t\t\treturn\n\t\t}\n\t\t<-connected.Done()\n\t\tif err := f.WriteGoAway(0, http2.ErrCodeEnhanceYourCalm, []byte(\"too_many_pings\")); err != nil {\n\t\t\tt.Errorf(\"error writing GOAWAY: %v\", err)\n\t\t\treturn\n\t\t}\n\t}()\n\taddr := lis.Addr().String()\n\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n\tdefer cancel()\n\tcc, err := DialContext(ctx, addr, WithBlock(), WithTransportCredentials(insecure.NewCredentials()), WithKeepaliveParams(keepalive.ClientParameters{\n\t\tTime:                10 * time.Second,\n\t\tTimeout:             100 * time.Millisecond,\n\t\tPermitWithoutStream: true,\n\t}))\n\tif err != nil {\n\t\tt.Fatalf(\"Dial(%s, _) = _, %v, want _, <nil>\", addr, err)\n\t}\n\tdefer cc.Close()\n\tconnected.Fire()\n\tfor {\n\t\ttime.Sleep(10 * time.Millisecond)\n\t\tcc.mu.RLock()\n\t\tv := cc.mkp.Time\n\t\tcc.mu.RUnlock()\n\t\tif v == 20*time.Second {\n\t\t\t// Success\n\t\t\treturn\n\t\t}\n\t\tif ctx.Err() != nil {\n\t\t\t// Timeout\n\t\t\tt.Fatalf(\"cc.dopts.copts.Keepalive.Time = %v , want 20s\", v)\n\t\t}\n\t}\n}\n\nfunc (s) TestDisableServiceConfigOption(t *testing.T) {\n\tr := manual.NewBuilderWithScheme(\"whatever\")\n\taddr := r.Scheme() + \":///non.existent\"\n\tcc, err := Dial(addr, WithTransportCredentials(insecure.NewCredentials()), WithResolvers(r), WithDisableServiceConfig())\n\tif err != nil {\n\t\tt.Fatalf(\"Dial(%s, _) = _, %v, want _, <nil>\", addr, err)\n\t}\n\tdefer cc.Close()\n\tr.UpdateState(resolver.State{ServiceConfig: parseCfg(r, `{\n    \"methodConfig\": [\n        {\n            \"name\": [\n                {\n                    \"service\": \"foo\",\n                    \"method\": \"Bar\"\n                }\n            ],\n            \"waitForReady\": true\n        }\n    ]\n}`)})\n\ttime.Sleep(1 * time.Second)\n\tm := cc.GetMethodConfig(\"/foo/Bar\")\n\tif m.WaitForReady != nil {\n\t\tt.Fatalf(\"want: method (\\\"/foo/bar/\\\") config to be empty, got: %+v\", m)\n\t}\n}\n\nfunc (s) TestMethodConfigDefaultService(t *testing.T) {\n\taddr := \"nonexist:///non.existent\"\n\tcc, err := Dial(addr, WithTransportCredentials(insecure.NewCredentials()), WithDefaultServiceConfig(`{\n  \"methodConfig\": [{\n    \"name\": [\n      {\n        \"service\": \"\"\n      }\n    ],\n    \"waitForReady\": true\n  }]\n}`))\n\tif err != nil {\n\t\tt.Fatalf(\"Dial(%s, _) = _, %v, want _, <nil>\", addr, err)\n\t}\n\tdefer cc.Close()\n\n\tm := cc.GetMethodConfig(\"/foo/Bar\")\n\tif m.WaitForReady == nil {\n\t\tt.Fatalf(\"want: method (%q) config to fallback to the default service\", \"/foo/Bar\")\n\t}\n}\n\nfunc (s) TestClientConnCanonicalTarget(t *testing.T) {\n\ttests := []struct {\n\t\tname                string\n\t\taddr                string\n\t\tcanonicalTargetWant string\n\t}{\n\t\t{\n\t\t\tname:                \"normal-case\",\n\t\t\taddr:                \"dns://a.server.com/google.com\",\n\t\t\tcanonicalTargetWant: \"dns://a.server.com/google.com\",\n\t\t},\n\t\t{\n\t\t\tname:                \"canonical-target-not-specified\",\n\t\t\taddr:                \"no.scheme\",\n\t\t\tcanonicalTargetWant: \"passthrough:///no.scheme\",\n\t\t},\n\t\t{\n\t\t\tname:                \"canonical-target-nonexistent\",\n\t\t\taddr:                \"nonexist:///non.existent\",\n\t\t\tcanonicalTargetWant: \"passthrough:///nonexist:///non.existent\",\n\t\t},\n\t\t{\n\t\t\tname:                \"canonical-target-add-colon-slash\",\n\t\t\taddr:                \"dns:hostname:port\",\n\t\t\tcanonicalTargetWant: \"dns:///hostname:port\",\n\t\t},\n\t}\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tcc, err := Dial(test.addr, WithTransportCredentials(insecure.NewCredentials()))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Dial(%s, _) = _, %v, want _, <nil>\", test.addr, err)\n\t\t\t}\n\t\t\tdefer cc.Close()\n\t\t\tif cc.Target() != test.addr {\n\t\t\t\tt.Fatalf(\"Target() = %s, want %s\", cc.Target(), test.addr)\n\t\t\t}\n\t\t\tif cc.CanonicalTarget() != test.canonicalTargetWant {\n\t\t\t\tt.Fatalf(\"CanonicalTarget() = %s, want %s\", cc.CanonicalTarget(), test.canonicalTargetWant)\n\t\t\t}\n\t\t})\n\t}\n}\n\ntype backoffForever struct{}\n\nfunc (b backoffForever) Backoff(int) time.Duration { return time.Duration(math.MaxInt64) }\n\nfunc (s) TestResetConnectBackoff(t *testing.T) {\n\tdials := make(chan struct{})\n\tdefer func() { // If we fail, let the http2client break out of dialing.\n\t\tselect {\n\t\tcase <-dials:\n\t\tdefault:\n\t\t}\n\t}()\n\tdialer := func(string, time.Duration) (net.Conn, error) {\n\t\tdials <- struct{}{}\n\t\treturn nil, errors.New(\"failed to fake dial\")\n\t}\n\tcc, err := Dial(\"any\", WithTransportCredentials(insecure.NewCredentials()), WithDialer(dialer), withBackoff(backoffForever{}))\n\tif err != nil {\n\t\tt.Fatalf(\"Dial() = _, %v; want _, nil\", err)\n\t}\n\tdefer cc.Close()\n\tgo stayConnected(cc)\n\tselect {\n\tcase <-dials:\n\tcase <-time.NewTimer(10 * time.Second).C:\n\t\tt.Fatal(\"Failed to call dial within 10s\")\n\t}\n\n\tselect {\n\tcase <-dials:\n\t\tt.Fatal(\"Dial called unexpectedly before resetting backoff\")\n\tcase <-time.NewTimer(100 * time.Millisecond).C:\n\t}\n\n\tcc.ResetConnectBackoff()\n\n\tselect {\n\tcase <-dials:\n\tcase <-time.NewTimer(10 * time.Second).C:\n\t\tt.Fatal(\"Failed to call dial within 10s after resetting backoff\")\n\t}\n}\n\nfunc (s) TestBackoffCancel(t *testing.T) {\n\tdialStrCh := make(chan string)\n\tcc, err := Dial(\"any\", WithTransportCredentials(insecure.NewCredentials()), WithDialer(func(t string, _ time.Duration) (net.Conn, error) {\n\t\tdialStrCh <- t\n\t\treturn nil, fmt.Errorf(\"test dialer, always error\")\n\t}))\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to create ClientConn: %v\", err)\n\t}\n\tdefer cc.Close()\n\n\tselect {\n\tcase <-time.After(defaultTestTimeout):\n\t\tt.Fatal(\"Timeout when waiting for custom dialer to be invoked during Dial\")\n\tcase <-dialStrCh:\n\t}\n}\n\n// TestUpdateAddresses_NoopIfCalledWithSameAddresses tests that UpdateAddresses\n// should be noop if UpdateAddresses is called with the same list of addresses,\n// even when the SubConn is in Connecting and doesn't have a current address.\nfunc (s) TestUpdateAddresses_NoopIfCalledWithSameAddresses(t *testing.T) {\n\tlis1, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Error while listening. Err: %v\", err)\n\t}\n\tdefer lis1.Close()\n\n\tlis2, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Error while listening. Err: %v\", err)\n\t}\n\tdefer lis2.Close()\n\n\tlis3, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Error while listening. Err: %v\", err)\n\t}\n\tdefer lis3.Close()\n\n\tcloseServer2 := make(chan struct{})\n\texitCh := make(chan struct{})\n\tserver1ContactedFirstTime := make(chan struct{})\n\tserver1ContactedSecondTime := make(chan struct{})\n\tserver2ContactedFirstTime := make(chan struct{})\n\tserver2ContactedSecondTime := make(chan struct{})\n\tserver3Contacted := make(chan struct{})\n\n\tdefer close(exitCh)\n\n\t// Launch server 1.\n\tgo func() {\n\t\t// First, let's allow the initial connection to go READY. We need to do\n\t\t// this because tryUpdateAddrs only works after there's some non-nil\n\t\t// address on the ac, and curAddress is only set after READY.\n\t\tconn1, err := lis1.Accept()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\treturn\n\t\t}\n\t\tgo keepReading(conn1)\n\n\t\tframer := http2.NewFramer(conn1, conn1)\n\t\tif err := framer.WriteSettings(http2.Setting{}); err != nil {\n\t\t\tt.Errorf(\"Error while writing settings frame. %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// nextStateNotifier() is updated after balancerBuilder.Build(), which is\n\t\t// called by grpc.Dial. It's safe to do it here because lis1.Accept blocks\n\t\t// until balancer is built to process the addresses.\n\t\tstateNotifications := testBalancerBuilder.nextStateNotifier()\n\t\t// Wait for the transport to become ready.\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase st := <-stateNotifications:\n\t\t\t\tif st == connectivity.Ready {\n\t\t\t\t\tgoto ready\n\t\t\t\t}\n\t\t\tcase <-exitCh:\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\tready:\n\t\t// Once it's ready, curAddress has been set. So let's close this\n\t\t// connection prompting the first reconnect cycle.\n\t\tconn1.Close()\n\n\t\t// Accept and immediately close, causing it to go to server2.\n\t\tconn2, err := lis1.Accept()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\treturn\n\t\t}\n\t\tclose(server1ContactedFirstTime)\n\t\tconn2.Close()\n\n\t\t// Hopefully it picks this server after tryUpdateAddrs.\n\t\tlis1.Accept()\n\t\tclose(server1ContactedSecondTime)\n\t}()\n\t// Launch server 2.\n\tgo func() {\n\t\t// Accept and then hang waiting for the test call tryUpdateAddrs and\n\t\t// then signal to this server to close. After this server closes, it\n\t\t// should start from the top instead of trying server2 or continuing\n\t\t// to server3.\n\t\tconn, err := lis2.Accept()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\treturn\n\t\t}\n\n\t\tclose(server2ContactedFirstTime)\n\t\t<-closeServer2\n\t\tconn.Close()\n\n\t\t// After tryUpdateAddrs, it should NOT try server2.\n\t\tlis2.Accept()\n\t\tclose(server2ContactedSecondTime)\n\t}()\n\t// Launch server 3.\n\tgo func() {\n\t\t// After tryUpdateAddrs, it should NOT try server3. (or any other time)\n\t\tlis3.Accept()\n\t\tclose(server3Contacted)\n\t}()\n\n\taddrsList := []resolver.Address{\n\t\t{Addr: lis1.Addr().String()},\n\t\t{Addr: lis2.Addr().String()},\n\t\t{Addr: lis3.Addr().String()},\n\t}\n\trb := manual.NewBuilderWithScheme(\"whatever\")\n\trb.InitialState(resolver.State{Addresses: addrsList})\n\n\tclient, err := Dial(\"whatever:///this-gets-overwritten\",\n\t\tWithTransportCredentials(insecure.NewCredentials()),\n\t\tWithResolvers(rb),\n\t\tWithConnectParams(ConnectParams{\n\t\t\tBackoff:           backoff.Config{},\n\t\t\tMinConnectTimeout: time.Hour,\n\t\t}),\n\t\tWithDefaultServiceConfig(fmt.Sprintf(`{\"loadBalancingConfig\": [{\"%s\":{}}]}`, stateRecordingBalancerName)))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer client.Close()\n\tgo stayConnected(client)\n\n\ttimeout := time.After(5 * time.Second)\n\n\t// Wait for server1 to be contacted (which will immediately fail), then\n\t// server2 (which will hang waiting for our signal).\n\tselect {\n\tcase <-server1ContactedFirstTime:\n\tcase <-timeout:\n\t\tt.Fatal(\"timed out waiting for server1 to be contacted\")\n\t}\n\tselect {\n\tcase <-server2ContactedFirstTime:\n\tcase <-timeout:\n\t\tt.Fatal(\"timed out waiting for server2 to be contacted\")\n\t}\n\n\t// Grab the addrConn and call tryUpdateAddrs.\n\tclient.mu.Lock()\n\tfor clientAC := range client.conns {\n\t\t// Call UpdateAddresses with the same list of addresses, it should be a noop\n\t\t// (even when the SubConn is Connecting, and doesn't have a curAddr).\n\t\tclientAC.acbw.UpdateAddresses(clientAC.addrs)\n\t}\n\tclient.mu.Unlock()\n\n\t// We've called tryUpdateAddrs - now let's make server2 close the\n\t// connection and check that it continues to server3.\n\tclose(closeServer2)\n\n\tselect {\n\tcase <-server1ContactedSecondTime:\n\t\tt.Fatal(\"server1 was contacted a second time, but it should have continued to server 3\")\n\tcase <-server2ContactedSecondTime:\n\t\tt.Fatal(\"server2 was contacted a second time, but it should have continued to server 3\")\n\tcase <-server3Contacted:\n\tcase <-timeout:\n\t\tt.Fatal(\"timed out waiting for any server to be contacted after tryUpdateAddrs\")\n\t}\n}\n\nfunc (s) TestDefaultServiceConfig(t *testing.T) {\n\tconst defaultSC = `\n{\n    \"methodConfig\": [\n        {\n            \"name\": [\n                {\n                    \"service\": \"foo\",\n                    \"method\": \"bar\"\n                }\n            ],\n            \"waitForReady\": true\n        }\n    ]\n}`\n\ttests := []struct {\n\t\tname  string\n\t\ttestF func(t *testing.T, r *manual.Resolver, addr, sc string)\n\t\tsc    string\n\t}{\n\t\t{\n\t\t\tname:  \"invalid-service-config\",\n\t\t\ttestF: testInvalidDefaultServiceConfig,\n\t\t\tsc:    \"\",\n\t\t},\n\t\t{\n\t\t\tname:  \"resolver-service-config-disabled\",\n\t\t\ttestF: testDefaultServiceConfigWhenResolverServiceConfigDisabled,\n\t\t\tsc:    defaultSC,\n\t\t},\n\t\t{\n\t\t\tname:  \"resolver-does-not-return-service-config\",\n\t\t\ttestF: testDefaultServiceConfigWhenResolverDoesNotReturnServiceConfig,\n\t\t\tsc:    defaultSC,\n\t\t},\n\t\t{\n\t\t\tname:  \"resolver-returns-invalid-service-config\",\n\t\t\ttestF: testDefaultServiceConfigWhenResolverReturnInvalidServiceConfig,\n\t\t\tsc:    defaultSC,\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tr := manual.NewBuilderWithScheme(test.name)\n\t\t\taddr := r.Scheme() + \":///non.existent\"\n\t\t\ttest.testF(t, r, addr, test.sc)\n\t\t})\n\t}\n}\n\nfunc verifyWaitForReadyEqualsTrue(cc *ClientConn) bool {\n\tvar i int\n\tfor i = 0; i < 10; i++ {\n\t\tmc := cc.GetMethodConfig(\"/foo/bar\")\n\t\tif mc.WaitForReady != nil && *mc.WaitForReady == true {\n\t\t\tbreak\n\t\t}\n\t\ttime.Sleep(100 * time.Millisecond)\n\t}\n\treturn i != 10\n}\n\nfunc testInvalidDefaultServiceConfig(t *testing.T, r *manual.Resolver, addr, sc string) {\n\t_, err := Dial(addr, WithTransportCredentials(insecure.NewCredentials()), WithResolvers(r), WithDefaultServiceConfig(sc))\n\tif !strings.Contains(err.Error(), invalidDefaultServiceConfigErrPrefix) {\n\t\tt.Fatalf(\"Dial got err: %v, want err contains: %v\", err, invalidDefaultServiceConfigErrPrefix)\n\t}\n}\n\nfunc testDefaultServiceConfigWhenResolverServiceConfigDisabled(t *testing.T, r *manual.Resolver, addr string, js string) {\n\tcc, err := Dial(addr, WithTransportCredentials(insecure.NewCredentials()), WithDisableServiceConfig(), WithResolvers(r), WithDefaultServiceConfig(js))\n\tif err != nil {\n\t\tt.Fatalf(\"Dial(%s, _) = _, %v, want _, <nil>\", addr, err)\n\t}\n\tdefer cc.Close()\n\t// Resolver service config gets ignored since resolver service config is disabled.\n\tr.UpdateState(resolver.State{\n\t\tAddresses:     []resolver.Address{{Addr: addr}},\n\t\tServiceConfig: parseCfg(r, \"{}\"),\n\t})\n\tif !verifyWaitForReadyEqualsTrue(cc) {\n\t\tt.Fatal(\"default service config failed to be applied after 1s\")\n\t}\n}\n\nfunc testDefaultServiceConfigWhenResolverDoesNotReturnServiceConfig(t *testing.T, r *manual.Resolver, addr string, js string) {\n\tcc, err := Dial(addr, WithTransportCredentials(insecure.NewCredentials()), WithResolvers(r), WithDefaultServiceConfig(js))\n\tif err != nil {\n\t\tt.Fatalf(\"Dial(%s, _) = _, %v, want _, <nil>\", addr, err)\n\t}\n\tdefer cc.Close()\n\tr.UpdateState(resolver.State{\n\t\tAddresses: []resolver.Address{{Addr: addr}},\n\t})\n\tif !verifyWaitForReadyEqualsTrue(cc) {\n\t\tt.Fatal(\"default service config failed to be applied after 1s\")\n\t}\n}\n\nfunc testDefaultServiceConfigWhenResolverReturnInvalidServiceConfig(t *testing.T, r *manual.Resolver, addr string, js string) {\n\tcc, err := Dial(addr, WithTransportCredentials(insecure.NewCredentials()), WithResolvers(r), WithDefaultServiceConfig(js))\n\tif err != nil {\n\t\tt.Fatalf(\"Dial(%s, _) = _, %v, want _, <nil>\", addr, err)\n\t}\n\tdefer cc.Close()\n\tr.UpdateState(resolver.State{\n\t\tAddresses: []resolver.Address{{Addr: addr}},\n\t})\n\tif !verifyWaitForReadyEqualsTrue(cc) {\n\t\tt.Fatal(\"default service config failed to be applied after 1s\")\n\t}\n}\n\ntype stateRecordingBalancer struct {\n\tbalancer.Balancer\n}\n\nfunc (b *stateRecordingBalancer) UpdateSubConnState(sc balancer.SubConn, s balancer.SubConnState) {\n\tpanic(fmt.Sprintf(\"UpdateSubConnState(%v, %+v) called unexpectedly\", sc, s))\n}\n\nfunc (b *stateRecordingBalancer) Close() {\n\tb.Balancer.Close()\n}\n\ntype stateRecordingBalancerBuilder struct {\n\tmu       sync.Mutex\n\tnotifier chan connectivity.State // The notifier used in the last Balancer.\n}\n\nfunc newStateRecordingBalancerBuilder() *stateRecordingBalancerBuilder {\n\treturn &stateRecordingBalancerBuilder{}\n}\n\nfunc (b *stateRecordingBalancerBuilder) Name() string {\n\treturn stateRecordingBalancerName\n}\n\nfunc (b *stateRecordingBalancerBuilder) Build(cc balancer.ClientConn, opts balancer.BuildOptions) balancer.Balancer {\n\tstateNotifications := make(chan connectivity.State, 10)\n\tb.mu.Lock()\n\tb.notifier = stateNotifications\n\tb.mu.Unlock()\n\treturn &stateRecordingBalancer{\n\t\tBalancer: balancer.Get(\"pick_first\").Build(&stateRecordingCCWrapper{cc, stateNotifications}, opts),\n\t}\n}\n\nfunc (b *stateRecordingBalancerBuilder) nextStateNotifier() <-chan connectivity.State {\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\tret := b.notifier\n\tb.notifier = nil\n\treturn ret\n}\n\ntype stateRecordingCCWrapper struct {\n\tbalancer.ClientConn\n\tnotifier chan<- connectivity.State\n}\n\nfunc (ccw *stateRecordingCCWrapper) NewSubConn(addrs []resolver.Address, opts balancer.NewSubConnOptions) (balancer.SubConn, error) {\n\toldListener := opts.StateListener\n\topts.StateListener = func(s balancer.SubConnState) {\n\t\tccw.notifier <- s.ConnectivityState\n\t\toldListener(s)\n\t}\n\treturn ccw.ClientConn.NewSubConn(addrs, opts)\n}\n\n// Keep reading until something causes the connection to die (EOF, server\n// closed, etc). Useful as a tool for mindlessly keeping the connection\n// healthy, since the client will error if things like client prefaces are not\n// accepted in a timely fashion.\nfunc keepReading(conn net.Conn) {\n\tbuf := make([]byte, 1024)\n\tfor _, err := conn.Read(buf); err == nil; _, err = conn.Read(buf) {\n\t}\n}\n\n// stayConnected makes cc stay connected by repeatedly calling cc.Connect()\n// until the state becomes Shutdown or until 10 seconds elapses.\nfunc stayConnected(cc *ClientConn) {\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\n\tfor {\n\t\tstate := cc.GetState()\n\t\tswitch state {\n\t\tcase connectivity.Idle:\n\t\t\tcc.Connect()\n\t\tcase connectivity.Shutdown:\n\t\t\treturn\n\t\t}\n\t\tif !cc.WaitForStateChange(ctx, state) {\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (s) TestURLAuthorityEscape(t *testing.T) {\n\ttests := []struct {\n\t\tname      string\n\t\tauthority string\n\t\twant      string\n\t}{\n\t\t{\n\t\t\tname:      \"ipv6_authority\",\n\t\t\tauthority: \"[::1]\",\n\t\t\twant:      \"[::1]\",\n\t\t},\n\t\t{\n\t\t\tname:      \"with_user_and_host\",\n\t\t\tauthority: \"userinfo@host:10001\",\n\t\t\twant:      \"userinfo@host:10001\",\n\t\t},\n\t\t{\n\t\t\tname:      \"with_multiple_slashes\",\n\t\t\tauthority: \"projects/123/network/abc/service\",\n\t\t\twant:      \"projects%2F123%2Fnetwork%2Fabc%2Fservice\",\n\t\t},\n\t\t{\n\t\t\tname:      \"all_possible_allowed_chars\",\n\t\t\tauthority: \"abc123-._~!$&'()*+,;=@:[]\",\n\t\t\twant:      \"abc123-._~!$&'()*+,;=@:[]\",\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tif got, want := encodeAuthority(test.authority), test.want; got != want {\n\t\t\t\tt.Errorf(\"encodeAuthority(%s) = %s, want %s\", test.authority, got, test.want)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "codec.go",
          "type": "blob",
          "size": 3.080078125,
          "content": "/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"google.golang.org/grpc/encoding\"\n\t_ \"google.golang.org/grpc/encoding/proto\" // to register the Codec for \"proto\"\n\t\"google.golang.org/grpc/mem\"\n)\n\n// baseCodec captures the new encoding.CodecV2 interface without the Name\n// function, allowing it to be implemented by older Codec and encoding.Codec\n// implementations. The omitted Name function is only needed for the register in\n// the encoding package and is not part of the core functionality.\ntype baseCodec interface {\n\tMarshal(v any) (mem.BufferSlice, error)\n\tUnmarshal(data mem.BufferSlice, v any) error\n}\n\n// getCodec returns an encoding.CodecV2 for the codec of the given name (if\n// registered). Initially checks the V2 registry with encoding.GetCodecV2 and\n// returns the V2 codec if it is registered. Otherwise, it checks the V1 registry\n// with encoding.GetCodec and if it is registered wraps it with newCodecV1Bridge\n// to turn it into an encoding.CodecV2. Returns nil otherwise.\nfunc getCodec(name string) encoding.CodecV2 {\n\tif codecV1 := encoding.GetCodec(name); codecV1 != nil {\n\t\treturn newCodecV1Bridge(codecV1)\n\t}\n\n\treturn encoding.GetCodecV2(name)\n}\n\nfunc newCodecV0Bridge(c Codec) baseCodec {\n\treturn codecV0Bridge{codec: c}\n}\n\nfunc newCodecV1Bridge(c encoding.Codec) encoding.CodecV2 {\n\treturn codecV1Bridge{\n\t\tcodecV0Bridge: codecV0Bridge{codec: c},\n\t\tname:          c.Name(),\n\t}\n}\n\nvar _ baseCodec = codecV0Bridge{}\n\ntype codecV0Bridge struct {\n\tcodec interface {\n\t\tMarshal(v any) ([]byte, error)\n\t\tUnmarshal(data []byte, v any) error\n\t}\n}\n\nfunc (c codecV0Bridge) Marshal(v any) (mem.BufferSlice, error) {\n\tdata, err := c.codec.Marshal(v)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn mem.BufferSlice{mem.SliceBuffer(data)}, nil\n}\n\nfunc (c codecV0Bridge) Unmarshal(data mem.BufferSlice, v any) (err error) {\n\treturn c.codec.Unmarshal(data.Materialize(), v)\n}\n\nvar _ encoding.CodecV2 = codecV1Bridge{}\n\ntype codecV1Bridge struct {\n\tcodecV0Bridge\n\tname string\n}\n\nfunc (c codecV1Bridge) Name() string {\n\treturn c.name\n}\n\n// Codec defines the interface gRPC uses to encode and decode messages.\n// Note that implementations of this interface must be thread safe;\n// a Codec's methods can be called from concurrent goroutines.\n//\n// Deprecated: use encoding.Codec instead.\ntype Codec interface {\n\t// Marshal returns the wire format of v.\n\tMarshal(v any) ([]byte, error)\n\t// Unmarshal parses the wire format into v.\n\tUnmarshal(data []byte, v any) error\n\t// String returns the name of the Codec implementation.  This is unused by\n\t// gRPC.\n\tString() string\n}\n"
        },
        {
          "name": "codec_test.go",
          "type": "blob",
          "size": 0.8759765625,
          "content": "/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"testing\"\n\n\t\"google.golang.org/grpc/encoding\"\n\t\"google.golang.org/grpc/encoding/proto\"\n)\n\nfunc (s) TestGetCodecForProtoIsNotNil(t *testing.T) {\n\tif encoding.GetCodecV2(proto.Name) == nil {\n\t\tt.Fatalf(\"encoding.GetCodec(%q) must not be nil by default\", proto.Name)\n\t}\n}\n"
        },
        {
          "name": "codes",
          "type": "tree",
          "content": null
        },
        {
          "name": "connectivity",
          "type": "tree",
          "content": null
        },
        {
          "name": "credentials",
          "type": "tree",
          "content": null
        },
        {
          "name": "default_dial_option_server_option_test.go",
          "type": "blob",
          "size": 7.236328125,
          "content": "/*\n *\n * Copyright 2022 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"fmt\"\n\t\"net/url\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"google.golang.org/grpc/credentials/insecure\"\n\t\"google.golang.org/grpc/internal\"\n)\n\nfunc (s) TestAddGlobalDialOptions(t *testing.T) {\n\t// Ensure the Dial fails without credentials\n\tif _, err := Dial(\"fake\"); err == nil {\n\t\tt.Fatalf(\"Dialing without a credential did not fail\")\n\t} else {\n\t\tif !strings.Contains(err.Error(), \"no transport security set\") {\n\t\t\tt.Fatalf(\"Dialing failed with unexpected error: %v\", err)\n\t\t}\n\t}\n\n\t// Set and check the DialOptions\n\topts := []DialOption{WithTransportCredentials(insecure.NewCredentials()), WithTransportCredentials(insecure.NewCredentials()), WithTransportCredentials(insecure.NewCredentials())}\n\tinternal.AddGlobalDialOptions.(func(opt ...DialOption))(opts...)\n\tdefer internal.ClearGlobalDialOptions()\n\tfor i, opt := range opts {\n\t\tif globalDialOptions[i] != opt {\n\t\t\tt.Fatalf(\"Unexpected global dial option at index %d: %v != %v\", i, globalDialOptions[i], opt)\n\t\t}\n\t}\n\n\t// Ensure the Dial passes with the extra dial options\n\tif cc, err := Dial(\"fake\"); err != nil {\n\t\tt.Fatalf(\"Dialing with insecure credential failed: %v\", err)\n\t} else {\n\t\tcc.Close()\n\t}\n\n\tinternal.ClearGlobalDialOptions()\n\tif len(globalDialOptions) != 0 {\n\t\tt.Fatalf(\"Unexpected len of globalDialOptions: %d != 0\", len(globalDialOptions))\n\t}\n}\n\n// TestDisableGlobalOptions tests dialing with the disableGlobalDialOptions dial\n// option. Dialing with this set should not pick up global options.\nfunc (s) TestDisableGlobalOptions(t *testing.T) {\n\t// Set transport credentials as a global option.\n\tinternal.AddGlobalDialOptions.(func(opt ...DialOption))(WithTransportCredentials(insecure.NewCredentials()))\n\tdefer internal.ClearGlobalDialOptions()\n\t// Dial with the disable global options dial option. This dial should fail\n\t// due to the global dial options with credentials not being picked up due\n\t// to global options being disabled.\n\tnoTSecStr := \"no transport security set\"\n\tif _, err := Dial(\"fake\", internal.DisableGlobalDialOptions.(func() DialOption)()); !strings.Contains(fmt.Sprint(err), noTSecStr) {\n\t\tt.Fatalf(\"Dialing received unexpected error: %v, want error containing \\\"%v\\\"\", err, noTSecStr)\n\t}\n}\n\ntype testPerTargetDialOption struct{}\n\nfunc (do *testPerTargetDialOption) DialOptionForTarget(parsedTarget url.URL) DialOption {\n\tif parsedTarget.Scheme == \"passthrough\" {\n\t\treturn WithTransportCredentials(insecure.NewCredentials()) // credentials provided, should pass NewClient.\n\t}\n\treturn EmptyDialOption{} // no credentials, should fail NewClient\n}\n\n// TestGlobalPerTargetDialOption configures a global per target dial option that\n// produces transport credentials for channels using \"passthrough\" scheme.\n// Channels that use the passthrough scheme should be successfully created due\n// to picking up transport credentials, whereas other channels should fail at\n// creation due to not having transport credentials.\nfunc (s) TestGlobalPerTargetDialOption(t *testing.T) {\n\tinternal.AddGlobalPerTargetDialOptions.(func(opt any))(&testPerTargetDialOption{})\n\tdefer internal.ClearGlobalPerTargetDialOptions()\n\tnoTSecStr := \"no transport security set\"\n\tif _, err := NewClient(\"dns:///fake\"); !strings.Contains(fmt.Sprint(err), noTSecStr) {\n\t\tt.Fatalf(\"Dialing received unexpected error: %v, want error containing \\\"%v\\\"\", err, noTSecStr)\n\t}\n\tcc, err := NewClient(\"passthrough:///nice\")\n\tif err != nil {\n\t\tt.Fatalf(\"Dialing with insecure credentials failed: %v\", err)\n\t}\n\tcc.Close()\n}\n\nfunc (s) TestAddGlobalServerOptions(t *testing.T) {\n\tconst maxRecvSize = 998765\n\t// Set and check the ServerOptions\n\topts := []ServerOption{Creds(insecure.NewCredentials()), MaxRecvMsgSize(maxRecvSize)}\n\tinternal.AddGlobalServerOptions.(func(opt ...ServerOption))(opts...)\n\tdefer internal.ClearGlobalServerOptions()\n\tfor i, opt := range opts {\n\t\tif globalServerOptions[i] != opt {\n\t\t\tt.Fatalf(\"Unexpected global server option at index %d: %v != %v\", i, globalServerOptions[i], opt)\n\t\t}\n\t}\n\n\t// Ensure the extra server options applies to new servers\n\ts := NewServer()\n\tif s.opts.maxReceiveMessageSize != maxRecvSize {\n\t\tt.Fatalf(\"Unexpected s.opts.maxReceiveMessageSize: %d != %d\", s.opts.maxReceiveMessageSize, maxRecvSize)\n\t}\n\n\tinternal.ClearGlobalServerOptions()\n\tif len(globalServerOptions) != 0 {\n\t\tt.Fatalf(\"Unexpected len of globalServerOptions: %d != 0\", len(globalServerOptions))\n\t}\n}\n\n// TestJoinDialOption tests the join dial option. It configures a joined dial\n// option with three individual dial options, and verifies that all three are\n// successfully applied.\nfunc (s) TestJoinDialOption(t *testing.T) {\n\tconst maxRecvSize = 998765\n\tconst initialWindowSize = 100\n\tjdo := newJoinDialOption(WithTransportCredentials(insecure.NewCredentials()), WithReadBufferSize(maxRecvSize), WithInitialWindowSize(initialWindowSize))\n\tcc, err := Dial(\"fake\", jdo)\n\tif err != nil {\n\t\tt.Fatalf(\"Dialing with insecure credentials failed: %v\", err)\n\t}\n\tdefer cc.Close()\n\tif cc.dopts.copts.ReadBufferSize != maxRecvSize {\n\t\tt.Fatalf(\"Unexpected cc.dopts.copts.ReadBufferSize: %d != %d\", cc.dopts.copts.ReadBufferSize, maxRecvSize)\n\t}\n\tif cc.dopts.copts.InitialWindowSize != initialWindowSize {\n\t\tt.Fatalf(\"Unexpected cc.dopts.copts.InitialWindowSize: %d != %d\", cc.dopts.copts.InitialWindowSize, initialWindowSize)\n\t}\n}\n\n// TestJoinServerOption tests the join server option. It configures a joined\n// server option with three individual server options, and verifies that all\n// three are successfully applied.\nfunc (s) TestJoinServerOption(t *testing.T) {\n\tconst maxRecvSize = 998765\n\tconst initialWindowSize = 100\n\tjso := newJoinServerOption(Creds(insecure.NewCredentials()), MaxRecvMsgSize(maxRecvSize), InitialWindowSize(initialWindowSize))\n\ts := NewServer(jso)\n\tif s.opts.maxReceiveMessageSize != maxRecvSize {\n\t\tt.Fatalf(\"Unexpected s.opts.maxReceiveMessageSize: %d != %d\", s.opts.maxReceiveMessageSize, maxRecvSize)\n\t}\n\tif s.opts.initialWindowSize != initialWindowSize {\n\t\tt.Fatalf(\"Unexpected s.opts.initialWindowSize: %d != %d\", s.opts.initialWindowSize, initialWindowSize)\n\t}\n}\n\n// funcTestHeaderListSizeDialOptionServerOption tests\nfunc (s) TestHeaderListSizeDialOptionServerOption(t *testing.T) {\n\tconst maxHeaderListSize uint32 = 998765\n\tclientHeaderListSize := WithMaxHeaderListSize(maxHeaderListSize)\n\tif clientHeaderListSize.(MaxHeaderListSizeDialOption).MaxHeaderListSize != maxHeaderListSize {\n\t\tt.Fatalf(\"Unexpected s.opts.MaxHeaderListSizeDialOption.MaxHeaderListSize: %d != %d\", clientHeaderListSize, maxHeaderListSize)\n\t}\n\tserverHeaderListSize := MaxHeaderListSize(maxHeaderListSize)\n\tif serverHeaderListSize.(MaxHeaderListSizeServerOption).MaxHeaderListSize != maxHeaderListSize {\n\t\tt.Fatalf(\"Unexpected s.opts.MaxHeaderListSizeDialOption.MaxHeaderListSize: %d != %d\", serverHeaderListSize, maxHeaderListSize)\n\t}\n}\n"
        },
        {
          "name": "dialoptions.go",
          "type": "blob",
          "size": 26.4130859375,
          "content": "/*\n *\n * Copyright 2018 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n\t\"net\"\n\t\"net/url\"\n\t\"time\"\n\n\t\"google.golang.org/grpc/backoff\"\n\t\"google.golang.org/grpc/channelz\"\n\t\"google.golang.org/grpc/credentials\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n\t\"google.golang.org/grpc/internal\"\n\tinternalbackoff \"google.golang.org/grpc/internal/backoff\"\n\t\"google.golang.org/grpc/internal/binarylog\"\n\t\"google.golang.org/grpc/internal/transport\"\n\t\"google.golang.org/grpc/keepalive\"\n\t\"google.golang.org/grpc/mem\"\n\t\"google.golang.org/grpc/resolver\"\n\t\"google.golang.org/grpc/stats\"\n)\n\nconst (\n\t// https://github.com/grpc/proposal/blob/master/A6-client-retries.md#limits-on-retries-and-hedges\n\tdefaultMaxCallAttempts = 5\n)\n\nfunc init() {\n\tinternal.AddGlobalDialOptions = func(opt ...DialOption) {\n\t\tglobalDialOptions = append(globalDialOptions, opt...)\n\t}\n\tinternal.ClearGlobalDialOptions = func() {\n\t\tglobalDialOptions = nil\n\t}\n\tinternal.AddGlobalPerTargetDialOptions = func(opt any) {\n\t\tif ptdo, ok := opt.(perTargetDialOption); ok {\n\t\t\tglobalPerTargetDialOptions = append(globalPerTargetDialOptions, ptdo)\n\t\t}\n\t}\n\tinternal.ClearGlobalPerTargetDialOptions = func() {\n\t\tglobalPerTargetDialOptions = nil\n\t}\n\tinternal.WithBinaryLogger = withBinaryLogger\n\tinternal.JoinDialOptions = newJoinDialOption\n\tinternal.DisableGlobalDialOptions = newDisableGlobalDialOptions\n\tinternal.WithBufferPool = withBufferPool\n}\n\n// dialOptions configure a Dial call. dialOptions are set by the DialOption\n// values passed to Dial.\ntype dialOptions struct {\n\tunaryInt  UnaryClientInterceptor\n\tstreamInt StreamClientInterceptor\n\n\tchainUnaryInts  []UnaryClientInterceptor\n\tchainStreamInts []StreamClientInterceptor\n\n\tcp                          Compressor\n\tdc                          Decompressor\n\tbs                          internalbackoff.Strategy\n\tblock                       bool\n\treturnLastError             bool\n\ttimeout                     time.Duration\n\tauthority                   string\n\tbinaryLogger                binarylog.Logger\n\tcopts                       transport.ConnectOptions\n\tcallOptions                 []CallOption\n\tchannelzParent              channelz.Identifier\n\tdisableServiceConfig        bool\n\tdisableRetry                bool\n\tdisableHealthCheck          bool\n\tminConnectTimeout           func() time.Duration\n\tdefaultServiceConfig        *ServiceConfig // defaultServiceConfig is parsed from defaultServiceConfigRawJSON.\n\tdefaultServiceConfigRawJSON *string\n\tresolvers                   []resolver.Builder\n\tidleTimeout                 time.Duration\n\tdefaultScheme               string\n\tmaxCallAttempts             int\n}\n\n// DialOption configures how we set up the connection.\ntype DialOption interface {\n\tapply(*dialOptions)\n}\n\nvar globalDialOptions []DialOption\n\n// perTargetDialOption takes a parsed target and returns a dial option to apply.\n//\n// This gets called after NewClient() parses the target, and allows per target\n// configuration set through a returned DialOption. The DialOption will not take\n// effect if specifies a resolver builder, as that Dial Option is factored in\n// while parsing target.\ntype perTargetDialOption interface {\n\t// DialOption returns a Dial Option to apply.\n\tDialOptionForTarget(parsedTarget url.URL) DialOption\n}\n\nvar globalPerTargetDialOptions []perTargetDialOption\n\n// EmptyDialOption does not alter the dial configuration. It can be embedded in\n// another structure to build custom dial options.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype EmptyDialOption struct{}\n\nfunc (EmptyDialOption) apply(*dialOptions) {}\n\ntype disableGlobalDialOptions struct{}\n\nfunc (disableGlobalDialOptions) apply(*dialOptions) {}\n\n// newDisableGlobalDialOptions returns a DialOption that prevents the ClientConn\n// from applying the global DialOptions (set via AddGlobalDialOptions).\nfunc newDisableGlobalDialOptions() DialOption {\n\treturn &disableGlobalDialOptions{}\n}\n\n// funcDialOption wraps a function that modifies dialOptions into an\n// implementation of the DialOption interface.\ntype funcDialOption struct {\n\tf func(*dialOptions)\n}\n\nfunc (fdo *funcDialOption) apply(do *dialOptions) {\n\tfdo.f(do)\n}\n\nfunc newFuncDialOption(f func(*dialOptions)) *funcDialOption {\n\treturn &funcDialOption{\n\t\tf: f,\n\t}\n}\n\ntype joinDialOption struct {\n\topts []DialOption\n}\n\nfunc (jdo *joinDialOption) apply(do *dialOptions) {\n\tfor _, opt := range jdo.opts {\n\t\topt.apply(do)\n\t}\n}\n\nfunc newJoinDialOption(opts ...DialOption) DialOption {\n\treturn &joinDialOption{opts: opts}\n}\n\n// WithSharedWriteBuffer allows reusing per-connection transport write buffer.\n// If this option is set to true every connection will release the buffer after\n// flushing the data on the wire.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc WithSharedWriteBuffer(val bool) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.copts.SharedWriteBuffer = val\n\t})\n}\n\n// WithWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The default value for this buffer is 32KB.\n//\n// Zero or negative values will disable the write buffer such that each write\n// will be on underlying connection. Note: A Send call may not directly\n// translate to a write.\nfunc WithWriteBufferSize(s int) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.copts.WriteBufferSize = s\n\t})\n}\n\n// WithReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero or negative values will\n// disable read buffer for a connection so data framer can access the\n// underlying conn directly.\nfunc WithReadBufferSize(s int) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.copts.ReadBufferSize = s\n\t})\n}\n\n// WithInitialWindowSize returns a DialOption which sets the value for initial\n// window size on a stream. The lower bound for window size is 64K and any value\n// smaller than that will be ignored.\nfunc WithInitialWindowSize(s int32) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.copts.InitialWindowSize = s\n\t})\n}\n\n// WithInitialConnWindowSize returns a DialOption which sets the value for\n// initial window size on a connection. The lower bound for window size is 64K\n// and any value smaller than that will be ignored.\nfunc WithInitialConnWindowSize(s int32) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.copts.InitialConnWindowSize = s\n\t})\n}\n\n// WithMaxMsgSize returns a DialOption which sets the maximum message size the\n// client can receive.\n//\n// Deprecated: use WithDefaultCallOptions(MaxCallRecvMsgSize(s)) instead.  Will\n// be supported throughout 1.x.\nfunc WithMaxMsgSize(s int) DialOption {\n\treturn WithDefaultCallOptions(MaxCallRecvMsgSize(s))\n}\n\n// WithDefaultCallOptions returns a DialOption which sets the default\n// CallOptions for calls over the connection.\nfunc WithDefaultCallOptions(cos ...CallOption) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.callOptions = append(o.callOptions, cos...)\n\t})\n}\n\n// WithCodec returns a DialOption which sets a codec for message marshaling and\n// unmarshaling.\n//\n// Deprecated: use WithDefaultCallOptions(ForceCodec(_)) instead.  Will be\n// supported throughout 1.x.\nfunc WithCodec(c Codec) DialOption {\n\treturn WithDefaultCallOptions(CallCustomCodec(c))\n}\n\n// WithCompressor returns a DialOption which sets a Compressor to use for\n// message compression. It has lower priority than the compressor set by the\n// UseCompressor CallOption.\n//\n// Deprecated: use UseCompressor instead.  Will be supported throughout 1.x.\nfunc WithCompressor(cp Compressor) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.cp = cp\n\t})\n}\n\n// WithDecompressor returns a DialOption which sets a Decompressor to use for\n// incoming message decompression.  If incoming response messages are encoded\n// using the decompressor's Type(), it will be used.  Otherwise, the message\n// encoding will be used to look up the compressor registered via\n// encoding.RegisterCompressor, which will then be used to decompress the\n// message.  If no compressor is registered for the encoding, an Unimplemented\n// status error will be returned.\n//\n// Deprecated: use encoding.RegisterCompressor instead.  Will be supported\n// throughout 1.x.\nfunc WithDecompressor(dc Decompressor) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.dc = dc\n\t})\n}\n\n// WithConnectParams configures the ClientConn to use the provided ConnectParams\n// for creating and maintaining connections to servers.\n//\n// The backoff configuration specified as part of the ConnectParams overrides\n// all defaults specified in\n// https://github.com/grpc/grpc/blob/master/doc/connection-backoff.md. Consider\n// using the backoff.DefaultConfig as a base, in cases where you want to\n// override only a subset of the backoff configuration.\nfunc WithConnectParams(p ConnectParams) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.bs = internalbackoff.Exponential{Config: p.Backoff}\n\t\to.minConnectTimeout = func() time.Duration {\n\t\t\treturn p.MinConnectTimeout\n\t\t}\n\t})\n}\n\n// WithBackoffMaxDelay configures the dialer to use the provided maximum delay\n// when backing off after failed connection attempts.\n//\n// Deprecated: use WithConnectParams instead. Will be supported throughout 1.x.\nfunc WithBackoffMaxDelay(md time.Duration) DialOption {\n\treturn WithBackoffConfig(BackoffConfig{MaxDelay: md})\n}\n\n// WithBackoffConfig configures the dialer to use the provided backoff\n// parameters after connection failures.\n//\n// Deprecated: use WithConnectParams instead. Will be supported throughout 1.x.\nfunc WithBackoffConfig(b BackoffConfig) DialOption {\n\tbc := backoff.DefaultConfig\n\tbc.MaxDelay = b.MaxDelay\n\treturn withBackoff(internalbackoff.Exponential{Config: bc})\n}\n\n// withBackoff sets the backoff strategy used for connectRetryNum after a failed\n// connection attempt.\n//\n// This can be exported if arbitrary backoff strategies are allowed by gRPC.\nfunc withBackoff(bs internalbackoff.Strategy) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.bs = bs\n\t})\n}\n\n// WithBlock returns a DialOption which makes callers of Dial block until the\n// underlying connection is up. Without this, Dial returns immediately and\n// connecting the server happens in background.\n//\n// Use of this feature is not recommended.  For more information, please see:\n// https://github.com/grpc/grpc-go/blob/master/Documentation/anti-patterns.md\n//\n// Deprecated: this DialOption is not supported by NewClient.\n// Will be supported throughout 1.x.\nfunc WithBlock() DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.block = true\n\t})\n}\n\n// WithReturnConnectionError returns a DialOption which makes the client connection\n// return a string containing both the last connection error that occurred and\n// the context.DeadlineExceeded error.\n// Implies WithBlock()\n//\n// Use of this feature is not recommended.  For more information, please see:\n// https://github.com/grpc/grpc-go/blob/master/Documentation/anti-patterns.md\n//\n// Deprecated: this DialOption is not supported by NewClient.\n// Will be supported throughout 1.x.\nfunc WithReturnConnectionError() DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.block = true\n\t\to.returnLastError = true\n\t})\n}\n\n// WithInsecure returns a DialOption which disables transport security for this\n// ClientConn. Under the hood, it uses insecure.NewCredentials().\n//\n// Note that using this DialOption with per-RPC credentials (through\n// WithCredentialsBundle or WithPerRPCCredentials) which require transport\n// security is incompatible and will cause grpc.Dial() to fail.\n//\n// Deprecated: use WithTransportCredentials and insecure.NewCredentials()\n// instead. Will be supported throughout 1.x.\nfunc WithInsecure() DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.copts.TransportCredentials = insecure.NewCredentials()\n\t})\n}\n\n// WithNoProxy returns a DialOption which disables the use of proxies for this\n// ClientConn. This is ignored if WithDialer or WithContextDialer are used.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc WithNoProxy() DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.copts.UseProxy = false\n\t})\n}\n\n// WithTransportCredentials returns a DialOption which configures a connection\n// level security credentials (e.g., TLS/SSL). This should not be used together\n// with WithCredentialsBundle.\nfunc WithTransportCredentials(creds credentials.TransportCredentials) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.copts.TransportCredentials = creds\n\t})\n}\n\n// WithPerRPCCredentials returns a DialOption which sets credentials and places\n// auth state on each outbound RPC.\nfunc WithPerRPCCredentials(creds credentials.PerRPCCredentials) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.copts.PerRPCCredentials = append(o.copts.PerRPCCredentials, creds)\n\t})\n}\n\n// WithCredentialsBundle returns a DialOption to set a credentials bundle for\n// the ClientConn.WithCreds. This should not be used together with\n// WithTransportCredentials.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc WithCredentialsBundle(b credentials.Bundle) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.copts.CredsBundle = b\n\t})\n}\n\n// WithTimeout returns a DialOption that configures a timeout for dialing a\n// ClientConn initially. This is valid if and only if WithBlock() is present.\n//\n// Deprecated: this DialOption is not supported by NewClient.\n// Will be supported throughout 1.x.\nfunc WithTimeout(d time.Duration) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.timeout = d\n\t})\n}\n\n// WithContextDialer returns a DialOption that sets a dialer to create\n// connections. If FailOnNonTempDialError() is set to true, and an error is\n// returned by f, gRPC checks the error's Temporary() method to decide if it\n// should try to reconnect to the network address.\n//\n// Note that gRPC by default performs name resolution on the target passed to\n// NewClient. To bypass name resolution and cause the target string to be\n// passed directly to the dialer here instead, use the \"passthrough\" resolver\n// by specifying it in the target string, e.g. \"passthrough:target\".\n//\n// Note: All supported releases of Go (as of December 2023) override the OS\n// defaults for TCP keepalive time and interval to 15s. To enable TCP keepalive\n// with OS defaults for keepalive time and interval, use a net.Dialer that sets\n// the KeepAlive field to a negative value, and sets the SO_KEEPALIVE socket\n// option to true from the Control field. For a concrete example of how to do\n// this, see internal.NetDialerWithTCPKeepalive().\n//\n// For more information, please see [issue 23459] in the Go GitHub repo.\n//\n// [issue 23459]: https://github.com/golang/go/issues/23459\nfunc WithContextDialer(f func(context.Context, string) (net.Conn, error)) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.copts.Dialer = f\n\t})\n}\n\n// WithDialer returns a DialOption that specifies a function to use for dialing\n// network addresses. If FailOnNonTempDialError() is set to true, and an error\n// is returned by f, gRPC checks the error's Temporary() method to decide if it\n// should try to reconnect to the network address.\n//\n// Deprecated: use WithContextDialer instead.  Will be supported throughout\n// 1.x.\nfunc WithDialer(f func(string, time.Duration) (net.Conn, error)) DialOption {\n\treturn WithContextDialer(\n\t\tfunc(ctx context.Context, addr string) (net.Conn, error) {\n\t\t\tif deadline, ok := ctx.Deadline(); ok {\n\t\t\t\treturn f(addr, time.Until(deadline))\n\t\t\t}\n\t\t\treturn f(addr, 0)\n\t\t})\n}\n\n// WithStatsHandler returns a DialOption that specifies the stats handler for\n// all the RPCs and underlying network connections in this ClientConn.\nfunc WithStatsHandler(h stats.Handler) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\tif h == nil {\n\t\t\tlogger.Error(\"ignoring nil parameter in grpc.WithStatsHandler ClientOption\")\n\t\t\t// Do not allow a nil stats handler, which would otherwise cause\n\t\t\t// panics.\n\t\t\treturn\n\t\t}\n\t\to.copts.StatsHandlers = append(o.copts.StatsHandlers, h)\n\t})\n}\n\n// withBinaryLogger returns a DialOption that specifies the binary logger for\n// this ClientConn.\nfunc withBinaryLogger(bl binarylog.Logger) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.binaryLogger = bl\n\t})\n}\n\n// FailOnNonTempDialError returns a DialOption that specifies if gRPC fails on\n// non-temporary dial errors. If f is true, and dialer returns a non-temporary\n// error, gRPC will fail the connection to the network address and won't try to\n// reconnect. The default value of FailOnNonTempDialError is false.\n//\n// FailOnNonTempDialError only affects the initial dial, and does not do\n// anything useful unless you are also using WithBlock().\n//\n// Use of this feature is not recommended.  For more information, please see:\n// https://github.com/grpc/grpc-go/blob/master/Documentation/anti-patterns.md\n//\n// Deprecated: this DialOption is not supported by NewClient.\n// This API may be changed or removed in a\n// later release.\nfunc FailOnNonTempDialError(f bool) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.copts.FailOnNonTempDialError = f\n\t})\n}\n\n// WithUserAgent returns a DialOption that specifies a user agent string for all\n// the RPCs.\nfunc WithUserAgent(s string) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.copts.UserAgent = s + \" \" + grpcUA\n\t})\n}\n\n// WithKeepaliveParams returns a DialOption that specifies keepalive parameters\n// for the client transport.\n//\n// Keepalive is disabled by default.\nfunc WithKeepaliveParams(kp keepalive.ClientParameters) DialOption {\n\tif kp.Time < internal.KeepaliveMinPingTime {\n\t\tlogger.Warningf(\"Adjusting keepalive ping interval to minimum period of %v\", internal.KeepaliveMinPingTime)\n\t\tkp.Time = internal.KeepaliveMinPingTime\n\t}\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.copts.KeepaliveParams = kp\n\t})\n}\n\n// WithUnaryInterceptor returns a DialOption that specifies the interceptor for\n// unary RPCs.\nfunc WithUnaryInterceptor(f UnaryClientInterceptor) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.unaryInt = f\n\t})\n}\n\n// WithChainUnaryInterceptor returns a DialOption that specifies the chained\n// interceptor for unary RPCs. The first interceptor will be the outer most,\n// while the last interceptor will be the inner most wrapper around the real call.\n// All interceptors added by this method will be chained, and the interceptor\n// defined by WithUnaryInterceptor will always be prepended to the chain.\nfunc WithChainUnaryInterceptor(interceptors ...UnaryClientInterceptor) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.chainUnaryInts = append(o.chainUnaryInts, interceptors...)\n\t})\n}\n\n// WithStreamInterceptor returns a DialOption that specifies the interceptor for\n// streaming RPCs.\nfunc WithStreamInterceptor(f StreamClientInterceptor) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.streamInt = f\n\t})\n}\n\n// WithChainStreamInterceptor returns a DialOption that specifies the chained\n// interceptor for streaming RPCs. The first interceptor will be the outer most,\n// while the last interceptor will be the inner most wrapper around the real call.\n// All interceptors added by this method will be chained, and the interceptor\n// defined by WithStreamInterceptor will always be prepended to the chain.\nfunc WithChainStreamInterceptor(interceptors ...StreamClientInterceptor) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.chainStreamInts = append(o.chainStreamInts, interceptors...)\n\t})\n}\n\n// WithAuthority returns a DialOption that specifies the value to be used as the\n// :authority pseudo-header and as the server name in authentication handshake.\nfunc WithAuthority(a string) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.authority = a\n\t})\n}\n\n// WithChannelzParentID returns a DialOption that specifies the channelz ID of\n// current ClientConn's parent. This function is used in nested channel creation\n// (e.g. grpclb dial).\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc WithChannelzParentID(c channelz.Identifier) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.channelzParent = c\n\t})\n}\n\n// WithDisableServiceConfig returns a DialOption that causes gRPC to ignore any\n// service config provided by the resolver and provides a hint to the resolver\n// to not fetch service configs.\n//\n// Note that this dial option only disables service config from resolver. If\n// default service config is provided, gRPC will use the default service config.\nfunc WithDisableServiceConfig() DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.disableServiceConfig = true\n\t})\n}\n\n// WithDefaultServiceConfig returns a DialOption that configures the default\n// service config, which will be used in cases where:\n//\n// 1. WithDisableServiceConfig is also used, or\n//\n// 2. The name resolver does not provide a service config or provides an\n// invalid service config.\n//\n// The parameter s is the JSON representation of the default service config.\n// For more information about service configs, see:\n// https://github.com/grpc/grpc/blob/master/doc/service_config.md\n// For a simple example of usage, see:\n// examples/features/load_balancing/client/main.go\nfunc WithDefaultServiceConfig(s string) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.defaultServiceConfigRawJSON = &s\n\t})\n}\n\n// WithDisableRetry returns a DialOption that disables retries, even if the\n// service config enables them.  This does not impact transparent retries, which\n// will happen automatically if no data is written to the wire or if the RPC is\n// unprocessed by the remote server.\nfunc WithDisableRetry() DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.disableRetry = true\n\t})\n}\n\n// MaxHeaderListSizeDialOption is a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\ntype MaxHeaderListSizeDialOption struct {\n\tMaxHeaderListSize uint32\n}\n\nfunc (o MaxHeaderListSizeDialOption) apply(do *dialOptions) {\n\tdo.copts.MaxHeaderListSize = &o.MaxHeaderListSize\n}\n\n// WithMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\nfunc WithMaxHeaderListSize(s uint32) DialOption {\n\treturn MaxHeaderListSizeDialOption{\n\t\tMaxHeaderListSize: s,\n\t}\n}\n\n// WithDisableHealthCheck disables the LB channel health checking for all\n// SubConns of this ClientConn.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc WithDisableHealthCheck() DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.disableHealthCheck = true\n\t})\n}\n\nfunc defaultDialOptions() dialOptions {\n\treturn dialOptions{\n\t\tcopts: transport.ConnectOptions{\n\t\t\tReadBufferSize:  defaultReadBufSize,\n\t\t\tWriteBufferSize: defaultWriteBufSize,\n\t\t\tUseProxy:        true,\n\t\t\tUserAgent:       grpcUA,\n\t\t\tBufferPool:      mem.DefaultBufferPool(),\n\t\t},\n\t\tbs:              internalbackoff.DefaultExponential,\n\t\tidleTimeout:     30 * time.Minute,\n\t\tdefaultScheme:   \"dns\",\n\t\tmaxCallAttempts: defaultMaxCallAttempts,\n\t}\n}\n\n// withMinConnectDeadline specifies the function that clientconn uses to\n// get minConnectDeadline. This can be used to make connection attempts happen\n// faster/slower.\n//\n// For testing purpose only.\nfunc withMinConnectDeadline(f func() time.Duration) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.minConnectTimeout = f\n\t})\n}\n\n// withDefaultScheme is used to allow Dial to use \"passthrough\" as the default\n// name resolver, while NewClient uses \"dns\" otherwise.\nfunc withDefaultScheme(s string) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.defaultScheme = s\n\t})\n}\n\n// WithResolvers allows a list of resolver implementations to be registered\n// locally with the ClientConn without needing to be globally registered via\n// resolver.Register.  They will be matched against the scheme used for the\n// current Dial only, and will take precedence over the global registry.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc WithResolvers(rs ...resolver.Builder) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.resolvers = append(o.resolvers, rs...)\n\t})\n}\n\n// WithIdleTimeout returns a DialOption that configures an idle timeout for the\n// channel. If the channel is idle for the configured timeout, i.e there are no\n// ongoing RPCs and no new RPCs are initiated, the channel will enter idle mode\n// and as a result the name resolver and load balancer will be shut down. The\n// channel will exit idle mode when the Connect() method is called or when an\n// RPC is initiated.\n//\n// A default timeout of 30 minutes will be used if this dial option is not set\n// at dial time and idleness can be disabled by passing a timeout of zero.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc WithIdleTimeout(d time.Duration) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.idleTimeout = d\n\t})\n}\n\n// WithMaxCallAttempts returns a DialOption that configures the maximum number\n// of attempts per call (including retries and hedging) using the channel.\n// Service owners may specify a higher value for these parameters, but higher\n// values will be treated as equal to the maximum value by the client\n// implementation. This mitigates security concerns related to the service\n// config being transferred to the client via DNS.\n//\n// A value of 5 will be used if this dial option is not set or n < 2.\nfunc WithMaxCallAttempts(n int) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\tif n < 2 {\n\t\t\tn = defaultMaxCallAttempts\n\t\t}\n\t\to.maxCallAttempts = n\n\t})\n}\n\nfunc withBufferPool(bufferPool mem.BufferPool) DialOption {\n\treturn newFuncDialOption(func(o *dialOptions) {\n\t\to.copts.BufferPool = bufferPool\n\t})\n}\n"
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 0.775390625,
          "content": "/*\n *\n * Copyright 2015 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\n//go:generate ./scripts/regenerate.sh\n\n/*\nPackage grpc implements an RPC system called gRPC.\n\nSee grpc.io for more information about gRPC.\n*/\npackage grpc // import \"google.golang.org/grpc\"\n"
        },
        {
          "name": "encoding",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "experimental",
          "type": "tree",
          "content": null
        },
        {
          "name": "gcp",
          "type": "tree",
          "content": null
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 1.4462890625,
          "content": "module google.golang.org/grpc\n\ngo 1.22\n\nrequire (\n\tgithub.com/cespare/xxhash/v2 v2.3.0\n\tgithub.com/cncf/xds/go v0.0.0-20240905190251-b4127c9b8d78\n\tgithub.com/envoyproxy/go-control-plane v0.13.4\n\tgithub.com/envoyproxy/go-control-plane/envoy v1.32.3\n\tgithub.com/golang/glog v1.2.3\n\tgithub.com/golang/protobuf v1.5.4\n\tgithub.com/google/go-cmp v0.6.0\n\tgithub.com/google/uuid v1.6.0\n\tgo.opentelemetry.io/contrib/detectors/gcp v1.32.0\n\tgo.opentelemetry.io/otel v1.32.0\n\tgo.opentelemetry.io/otel/metric v1.32.0\n\tgo.opentelemetry.io/otel/sdk v1.32.0\n\tgo.opentelemetry.io/otel/sdk/metric v1.32.0\n\tgo.opentelemetry.io/otel/trace v1.32.0\n\tgolang.org/x/net v0.32.0\n\tgolang.org/x/oauth2 v0.24.0\n\tgolang.org/x/sync v0.10.0\n\tgolang.org/x/sys v0.28.0\n\tgoogle.golang.org/genproto/googleapis/rpc v0.0.0-20241202173237-19429a94021a\n\tgoogle.golang.org/protobuf v1.35.2\n)\n\nrequire (\n\tcel.dev/expr v0.19.0 // indirect\n\tcloud.google.com/go/compute/metadata v0.5.2 // indirect\n\tgithub.com/GoogleCloudPlatform/opentelemetry-operations-go/detectors/gcp v1.25.0 // indirect\n\tgithub.com/envoyproxy/go-control-plane/ratelimit v0.1.0 // indirect\n\tgithub.com/envoyproxy/protoc-gen-validate v1.1.0 // indirect\n\tgithub.com/go-logr/logr v1.4.2 // indirect\n\tgithub.com/go-logr/stdr v1.2.2 // indirect\n\tgithub.com/planetscale/vtprotobuf v0.6.1-0.20240319094008-0393e58bdf10 // indirect\n\tgolang.org/x/text v0.21.0 // indirect\n\tgoogle.golang.org/genproto/googleapis/api v0.0.0-20241202173237-19429a94021a // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 6.3837890625,
          "content": "cel.dev/expr v0.19.0 h1:lXuo+nDhpyJSpWxpPVi5cPUwzKb+dsdOiw6IreM5yt0=\ncel.dev/expr v0.19.0/go.mod h1:MrpN08Q+lEBs+bGYdLxxHkZoUSsCp0nSKTs0nTymJgw=\ncloud.google.com/go/compute/metadata v0.5.2 h1:UxK4uu/Tn+I3p2dYWTfiX4wva7aYlKixAHn3fyqngqo=\ncloud.google.com/go/compute/metadata v0.5.2/go.mod h1:C66sj2AluDcIqakBq/M8lw8/ybHgOZqin2obFxa/E5k=\ngithub.com/GoogleCloudPlatform/opentelemetry-operations-go/detectors/gcp v1.25.0 h1:3c8yed4lgqTt+oTQ+JNMDo+F4xprBf+O/il4ZC0nRLw=\ngithub.com/GoogleCloudPlatform/opentelemetry-operations-go/detectors/gcp v1.25.0/go.mod h1:obipzmGjfSjam60XLwGfqUkJsfiheAl+TUjG+4yzyPM=\ngithub.com/cespare/xxhash/v2 v2.3.0 h1:UL815xU9SqsFlibzuggzjXhog7bL6oX9BbNZnL2UFvs=\ngithub.com/cespare/xxhash/v2 v2.3.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/cncf/xds/go v0.0.0-20240905190251-b4127c9b8d78 h1:QVw89YDxXxEe+l8gU8ETbOasdwEV+avkR75ZzsVV9WI=\ngithub.com/cncf/xds/go v0.0.0-20240905190251-b4127c9b8d78/go.mod h1:W+zGtBO5Y1IgJhy4+A9GOqVhqLpfZi+vwmdNXUehLA8=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/envoyproxy/go-control-plane v0.13.4 h1:zEqyPVyku6IvWCFwux4x9RxkLOMUL+1vC9xUFv5l2/M=\ngithub.com/envoyproxy/go-control-plane v0.13.4/go.mod h1:kDfuBlDVsSj2MjrLEtRWtHlsWIFcGyB2RMO44Dc5GZA=\ngithub.com/envoyproxy/go-control-plane/envoy v1.32.3 h1:hVEaommgvzTjTd4xCaFd+kEQ2iYBtGxP6luyLrx6uOk=\ngithub.com/envoyproxy/go-control-plane/envoy v1.32.3/go.mod h1:F6hWupPfh75TBXGKA++MCT/CZHFq5r9/uwt/kQYkZfE=\ngithub.com/envoyproxy/go-control-plane/ratelimit v0.1.0 h1:/G9QYbddjL25KvtKTv3an9lx6VBE2cnb8wp1vEGNYGI=\ngithub.com/envoyproxy/go-control-plane/ratelimit v0.1.0/go.mod h1:Wk+tMFAFbCXaJPzVVHnPgRKdUdwW/KdbRt94AzgRee4=\ngithub.com/envoyproxy/protoc-gen-validate v1.1.0 h1:tntQDh69XqOCOZsDz0lVJQez/2L6Uu2PdjCQwWCJ3bM=\ngithub.com/envoyproxy/protoc-gen-validate v1.1.0/go.mod h1:sXRDRVmzEbkM7CVcM06s9shE/m23dg3wzjl0UWqJ2q4=\ngithub.com/go-logr/logr v1.2.2/go.mod h1:jdQByPbusPIv2/zmleS9BjJVeZ6kBagPoEUsqbVz/1A=\ngithub.com/go-logr/logr v1.4.2 h1:6pFjapn8bFcIbiKo3XT4j/BhANplGihG6tvd+8rYgrY=\ngithub.com/go-logr/logr v1.4.2/go.mod h1:9T104GzyrTigFIr8wt5mBrctHMim0Nb2HLGrmQ40KvY=\ngithub.com/go-logr/stdr v1.2.2 h1:hSWxHoqTgW2S2qGc0LTAI563KZ5YKYRhT3MFKZMbjag=\ngithub.com/go-logr/stdr v1.2.2/go.mod h1:mMo/vtBO5dYbehREoey6XUKy/eSumjCCveDpRre4VKE=\ngithub.com/golang/glog v1.2.3 h1:oDTdz9f5VGVVNGu/Q7UXKWYsD0873HXLHdJUNBsSEKM=\ngithub.com/golang/glog v1.2.3/go.mod h1:6AhwSGph0fcJtXVM/PEHPqZlFeoLxhs7/t5UDAwmO+w=\ngithub.com/golang/protobuf v1.5.4 h1:i7eJL8qZTpSEXOPTxNKhASYpMn+8e5Q6AdndVa1dWek=\ngithub.com/golang/protobuf v1.5.4/go.mod h1:lnTiLA8Wa4RWRcIUkrtSVa5nRhsEGBg48fD6rSs7xps=\ngithub.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=\ngithub.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\ngithub.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=\ngithub.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/planetscale/vtprotobuf v0.6.1-0.20240319094008-0393e58bdf10 h1:GFCKgmp0tecUJ0sJuv4pzYCqS9+RGSn52M3FUwPs+uo=\ngithub.com/planetscale/vtprotobuf v0.6.1-0.20240319094008-0393e58bdf10/go.mod h1:t/avpk3KcrXxUnYOhZhMXJlSEyie6gQbtLq5NM3loB8=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/stretchr/testify v1.10.0 h1:Xv5erBjTwe/5IxqUQTdXv5kgmIvbHo3QQyRwhJsOfJA=\ngithub.com/stretchr/testify v1.10.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=\ngo.opentelemetry.io/contrib/detectors/gcp v1.32.0 h1:P78qWqkLSShicHmAzfECaTgvslqHxblNE9j62Ws1NK8=\ngo.opentelemetry.io/contrib/detectors/gcp v1.32.0/go.mod h1:TVqo0Sda4Cv8gCIixd7LuLwW4EylumVWfhjZJjDD4DU=\ngo.opentelemetry.io/otel v1.32.0 h1:WnBN+Xjcteh0zdk01SVqV55d/m62NJLJdIyb4y/WO5U=\ngo.opentelemetry.io/otel v1.32.0/go.mod h1:00DCVSB0RQcnzlwyTfqtxSm+DRr9hpYrHjNGiBHVQIg=\ngo.opentelemetry.io/otel/metric v1.32.0 h1:xV2umtmNcThh2/a/aCP+h64Xx5wsj8qqnkYZktzNa0M=\ngo.opentelemetry.io/otel/metric v1.32.0/go.mod h1:jH7CIbbK6SH2V2wE16W05BHCtIDzauciCRLoc/SyMv8=\ngo.opentelemetry.io/otel/sdk v1.32.0 h1:RNxepc9vK59A8XsgZQouW8ue8Gkb4jpWtJm9ge5lEG4=\ngo.opentelemetry.io/otel/sdk v1.32.0/go.mod h1:LqgegDBjKMmb2GC6/PrTnteJG39I8/vJCAP9LlJXEjU=\ngo.opentelemetry.io/otel/sdk/metric v1.32.0 h1:rZvFnvmvawYb0alrYkjraqJq0Z4ZUJAiyYCU9snn1CU=\ngo.opentelemetry.io/otel/sdk/metric v1.32.0/go.mod h1:PWeZlq0zt9YkYAp3gjKZ0eicRYvOh1Gd+X99x6GHpCQ=\ngo.opentelemetry.io/otel/trace v1.32.0 h1:WIC9mYrXf8TmY/EXuULKc8hR17vE+Hjv2cssQDe03fM=\ngo.opentelemetry.io/otel/trace v1.32.0/go.mod h1:+i4rkvCraA+tG6AzwloGaCtkx53Fa+L+V8e9a7YvhT8=\ngo.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=\ngo.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=\ngolang.org/x/net v0.32.0 h1:ZqPmj8Kzc+Y6e0+skZsuACbx+wzMgo5MQsJh9Qd6aYI=\ngolang.org/x/net v0.32.0/go.mod h1:CwU0IoeOlnQQWJ6ioyFrfRuomB8GKF6KbYXZVyeXNfs=\ngolang.org/x/oauth2 v0.24.0 h1:KTBBxWqUa0ykRPLtV69rRto9TLXcqYkeswu48x/gvNE=\ngolang.org/x/oauth2 v0.24.0/go.mod h1:XYTD2NtWslqkgxebSiOHnXEap4TF09sJSc7H1sXbhtI=\ngolang.org/x/sync v0.10.0 h1:3NQrjDixjgGwUOCaF8w2+VYHv0Ve/vGYSbdkTa98gmQ=\ngolang.org/x/sync v0.10.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\ngolang.org/x/sys v0.28.0 h1:Fksou7UEQUWlKvIdsqzJmUmCX3cZuD2+P3XyyzwMhlA=\ngolang.org/x/sys v0.28.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngolang.org/x/text v0.21.0 h1:zyQAAkrwaneQ066sspRyJaG9VNi/YJ1NfzcGB3hZ/qo=\ngolang.org/x/text v0.21.0/go.mod h1:4IBbMaMmOPCJ8SecivzSH54+73PCFmPWxNTLm+vZkEQ=\ngoogle.golang.org/genproto/googleapis/api v0.0.0-20241202173237-19429a94021a h1:OAiGFfOiA0v9MRYsSidp3ubZaBnteRUyn3xB2ZQ5G/E=\ngoogle.golang.org/genproto/googleapis/api v0.0.0-20241202173237-19429a94021a/go.mod h1:jehYqy3+AhJU9ve55aNOaSml7wUXjF9x6z2LcCfpAhY=\ngoogle.golang.org/genproto/googleapis/rpc v0.0.0-20241202173237-19429a94021a h1:hgh8P4EuoxpsuKMXX/To36nOFD7vixReXgn8lPGnt+o=\ngoogle.golang.org/genproto/googleapis/rpc v0.0.0-20241202173237-19429a94021a/go.mod h1:5uTbfoYQed2U9p3KIj2/Zzm02PYhndfdmML0qC3q3FU=\ngoogle.golang.org/protobuf v1.35.2 h1:8Ar7bF+apOIoThw1EdZl0p1oWvMqTHmpA2fRTyZO8io=\ngoogle.golang.org/protobuf v1.35.2/go.mod h1:9fA7Ob0pmnwhb644+1+CVWFRbNajQ6iRojtC/QF5bRE=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n"
        },
        {
          "name": "grpc_test.go",
          "type": "blob",
          "size": 0.76171875,
          "content": "/*\n *\n * Copyright 2018 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"testing\"\n\n\t\"google.golang.org/grpc/internal/grpctest\"\n)\n\ntype s struct {\n\tgrpctest.Tester\n}\n\nfunc Test(t *testing.T) {\n\tgrpctest.RunSubTests(t, s{})\n}\n"
        },
        {
          "name": "grpclog",
          "type": "tree",
          "content": null
        },
        {
          "name": "health",
          "type": "tree",
          "content": null
        },
        {
          "name": "interceptor.go",
          "type": "blob",
          "size": 5.1962890625,
          "content": "/*\n *\n * Copyright 2016 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n)\n\n// UnaryInvoker is called by UnaryClientInterceptor to complete RPCs.\ntype UnaryInvoker func(ctx context.Context, method string, req, reply any, cc *ClientConn, opts ...CallOption) error\n\n// UnaryClientInterceptor intercepts the execution of a unary RPC on the client.\n// Unary interceptors can be specified as a DialOption, using\n// WithUnaryInterceptor() or WithChainUnaryInterceptor(), when creating a\n// ClientConn. When a unary interceptor(s) is set on a ClientConn, gRPC\n// delegates all unary RPC invocations to the interceptor, and it is the\n// responsibility of the interceptor to call invoker to complete the processing\n// of the RPC.\n//\n// method is the RPC name. req and reply are the corresponding request and\n// response messages. cc is the ClientConn on which the RPC was invoked. invoker\n// is the handler to complete the RPC and it is the responsibility of the\n// interceptor to call it. opts contain all applicable call options, including\n// defaults from the ClientConn as well as per-call options.\n//\n// The returned error must be compatible with the status package.\ntype UnaryClientInterceptor func(ctx context.Context, method string, req, reply any, cc *ClientConn, invoker UnaryInvoker, opts ...CallOption) error\n\n// Streamer is called by StreamClientInterceptor to create a ClientStream.\ntype Streamer func(ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, opts ...CallOption) (ClientStream, error)\n\n// StreamClientInterceptor intercepts the creation of a ClientStream. Stream\n// interceptors can be specified as a DialOption, using WithStreamInterceptor()\n// or WithChainStreamInterceptor(), when creating a ClientConn. When a stream\n// interceptor(s) is set on the ClientConn, gRPC delegates all stream creations\n// to the interceptor, and it is the responsibility of the interceptor to call\n// streamer.\n//\n// desc contains a description of the stream. cc is the ClientConn on which the\n// RPC was invoked. streamer is the handler to create a ClientStream and it is\n// the responsibility of the interceptor to call it. opts contain all applicable\n// call options, including defaults from the ClientConn as well as per-call\n// options.\n//\n// StreamClientInterceptor may return a custom ClientStream to intercept all I/O\n// operations. The returned error must be compatible with the status package.\ntype StreamClientInterceptor func(ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, streamer Streamer, opts ...CallOption) (ClientStream, error)\n\n// UnaryServerInfo consists of various information about a unary RPC on\n// server side. All per-rpc information may be mutated by the interceptor.\ntype UnaryServerInfo struct {\n\t// Server is the service implementation the user provides. This is read-only.\n\tServer any\n\t// FullMethod is the full RPC method string, i.e., /package.service/method.\n\tFullMethod string\n}\n\n// UnaryHandler defines the handler invoked by UnaryServerInterceptor to complete the normal\n// execution of a unary RPC.\n//\n// If a UnaryHandler returns an error, it should either be produced by the\n// status package, or be one of the context errors. Otherwise, gRPC will use\n// codes.Unknown as the status code and err.Error() as the status message of the\n// RPC.\ntype UnaryHandler func(ctx context.Context, req any) (any, error)\n\n// UnaryServerInterceptor provides a hook to intercept the execution of a unary RPC on the server. info\n// contains all the information of this RPC the interceptor can operate on. And handler is the wrapper\n// of the service method implementation. It is the responsibility of the interceptor to invoke handler\n// to complete the RPC.\ntype UnaryServerInterceptor func(ctx context.Context, req any, info *UnaryServerInfo, handler UnaryHandler) (resp any, err error)\n\n// StreamServerInfo consists of various information about a streaming RPC on\n// server side. All per-rpc information may be mutated by the interceptor.\ntype StreamServerInfo struct {\n\t// FullMethod is the full RPC method string, i.e., /package.service/method.\n\tFullMethod string\n\t// IsClientStream indicates whether the RPC is a client streaming RPC.\n\tIsClientStream bool\n\t// IsServerStream indicates whether the RPC is a server streaming RPC.\n\tIsServerStream bool\n}\n\n// StreamServerInterceptor provides a hook to intercept the execution of a streaming RPC on the server.\n// info contains all the information of this RPC the interceptor can operate on. And handler is the\n// service method implementation. It is the responsibility of the interceptor to invoke handler to\n// complete the RPC.\ntype StreamServerInterceptor func(srv any, ss ServerStream, info *StreamServerInfo, handler StreamHandler) error\n"
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "interop",
          "type": "tree",
          "content": null
        },
        {
          "name": "keepalive",
          "type": "tree",
          "content": null
        },
        {
          "name": "mem",
          "type": "tree",
          "content": null
        },
        {
          "name": "metadata",
          "type": "tree",
          "content": null
        },
        {
          "name": "orca",
          "type": "tree",
          "content": null
        },
        {
          "name": "peer",
          "type": "tree",
          "content": null
        },
        {
          "name": "picker_wrapper.go",
          "type": "blob",
          "size": 6.9482421875,
          "content": "/*\n *\n * Copyright 2017 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"sync/atomic\"\n\n\t\"google.golang.org/grpc/balancer\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/internal/channelz\"\n\tistatus \"google.golang.org/grpc/internal/status\"\n\t\"google.golang.org/grpc/internal/transport\"\n\t\"google.golang.org/grpc/stats\"\n\t\"google.golang.org/grpc/status\"\n)\n\n// pickerGeneration stores a picker and a channel used to signal that a picker\n// newer than this one is available.\ntype pickerGeneration struct {\n\t// picker is the picker produced by the LB policy.  May be nil if a picker\n\t// has never been produced.\n\tpicker balancer.Picker\n\t// blockingCh is closed when the picker has been invalidated because there\n\t// is a new one available.\n\tblockingCh chan struct{}\n}\n\n// pickerWrapper is a wrapper of balancer.Picker. It blocks on certain pick\n// actions and unblock when there's a picker update.\ntype pickerWrapper struct {\n\t// If pickerGen holds a nil pointer, the pickerWrapper is closed.\n\tpickerGen     atomic.Pointer[pickerGeneration]\n\tstatsHandlers []stats.Handler // to record blocking picker calls\n}\n\nfunc newPickerWrapper(statsHandlers []stats.Handler) *pickerWrapper {\n\tpw := &pickerWrapper{\n\t\tstatsHandlers: statsHandlers,\n\t}\n\tpw.pickerGen.Store(&pickerGeneration{\n\t\tblockingCh: make(chan struct{}),\n\t})\n\treturn pw\n}\n\n// updatePicker is called by UpdateState calls from the LB policy. It\n// unblocks all blocked pick.\nfunc (pw *pickerWrapper) updatePicker(p balancer.Picker) {\n\told := pw.pickerGen.Swap(&pickerGeneration{\n\t\tpicker:     p,\n\t\tblockingCh: make(chan struct{}),\n\t})\n\tclose(old.blockingCh)\n}\n\n// doneChannelzWrapper performs the following:\n//   - increments the calls started channelz counter\n//   - wraps the done function in the passed in result to increment the calls\n//     failed or calls succeeded channelz counter before invoking the actual\n//     done function.\nfunc doneChannelzWrapper(acbw *acBalancerWrapper, result *balancer.PickResult) {\n\tac := acbw.ac\n\tac.incrCallsStarted()\n\tdone := result.Done\n\tresult.Done = func(b balancer.DoneInfo) {\n\t\tif b.Err != nil && b.Err != io.EOF {\n\t\t\tac.incrCallsFailed()\n\t\t} else {\n\t\t\tac.incrCallsSucceeded()\n\t\t}\n\t\tif done != nil {\n\t\t\tdone(b)\n\t\t}\n\t}\n}\n\n// pick returns the transport that will be used for the RPC.\n// It may block in the following cases:\n// - there's no picker\n// - the current picker returns ErrNoSubConnAvailable\n// - the current picker returns other errors and failfast is false.\n// - the subConn returned by the current picker is not READY\n// When one of these situations happens, pick blocks until the picker gets updated.\nfunc (pw *pickerWrapper) pick(ctx context.Context, failfast bool, info balancer.PickInfo) (transport.ClientTransport, balancer.PickResult, error) {\n\tvar ch chan struct{}\n\n\tvar lastPickErr error\n\n\tfor {\n\t\tpg := pw.pickerGen.Load()\n\t\tif pg == nil {\n\t\t\treturn nil, balancer.PickResult{}, ErrClientConnClosing\n\t\t}\n\t\tif pg.picker == nil {\n\t\t\tch = pg.blockingCh\n\t\t}\n\t\tif ch == pg.blockingCh {\n\t\t\t// This could happen when either:\n\t\t\t// - pw.picker is nil (the previous if condition), or\n\t\t\t// - we have already called pick on the current picker.\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tvar errStr string\n\t\t\t\tif lastPickErr != nil {\n\t\t\t\t\terrStr = \"latest balancer error: \" + lastPickErr.Error()\n\t\t\t\t} else {\n\t\t\t\t\terrStr = fmt.Sprintf(\"received context error while waiting for new LB policy update: %s\", ctx.Err().Error())\n\t\t\t\t}\n\t\t\t\tswitch ctx.Err() {\n\t\t\t\tcase context.DeadlineExceeded:\n\t\t\t\t\treturn nil, balancer.PickResult{}, status.Error(codes.DeadlineExceeded, errStr)\n\t\t\t\tcase context.Canceled:\n\t\t\t\t\treturn nil, balancer.PickResult{}, status.Error(codes.Canceled, errStr)\n\t\t\t\t}\n\t\t\tcase <-ch:\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// If the channel is set, it means that the pick call had to wait for a\n\t\t// new picker at some point. Either it's the first iteration and this\n\t\t// function received the first picker, or a picker errored with\n\t\t// ErrNoSubConnAvailable or errored with failfast set to false, which\n\t\t// will trigger a continue to the next iteration. In the first case this\n\t\t// conditional will hit if this call had to block (the channel is set).\n\t\t// In the second case, the only way it will get to this conditional is\n\t\t// if there is a new picker.\n\t\tif ch != nil {\n\t\t\tfor _, sh := range pw.statsHandlers {\n\t\t\t\tsh.HandleRPC(ctx, &stats.PickerUpdated{})\n\t\t\t}\n\t\t}\n\n\t\tch = pg.blockingCh\n\t\tp := pg.picker\n\n\t\tpickResult, err := p.Pick(info)\n\t\tif err != nil {\n\t\t\tif err == balancer.ErrNoSubConnAvailable {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif st, ok := status.FromError(err); ok {\n\t\t\t\t// Status error: end the RPC unconditionally with this status.\n\t\t\t\t// First restrict the code to the list allowed by gRFC A54.\n\t\t\t\tif istatus.IsRestrictedControlPlaneCode(st) {\n\t\t\t\t\terr = status.Errorf(codes.Internal, \"received picker error with illegal status: %v\", err)\n\t\t\t\t}\n\t\t\t\treturn nil, balancer.PickResult{}, dropError{error: err}\n\t\t\t}\n\t\t\t// For all other errors, wait for ready RPCs should block and other\n\t\t\t// RPCs should fail with unavailable.\n\t\t\tif !failfast {\n\t\t\t\tlastPickErr = err\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn nil, balancer.PickResult{}, status.Error(codes.Unavailable, err.Error())\n\t\t}\n\n\t\tacbw, ok := pickResult.SubConn.(*acBalancerWrapper)\n\t\tif !ok {\n\t\t\tlogger.Errorf(\"subconn returned from pick is type %T, not *acBalancerWrapper\", pickResult.SubConn)\n\t\t\tcontinue\n\t\t}\n\t\tif t := acbw.ac.getReadyTransport(); t != nil {\n\t\t\tif channelz.IsOn() {\n\t\t\t\tdoneChannelzWrapper(acbw, &pickResult)\n\t\t\t\treturn t, pickResult, nil\n\t\t\t}\n\t\t\treturn t, pickResult, nil\n\t\t}\n\t\tif pickResult.Done != nil {\n\t\t\t// Calling done with nil error, no bytes sent and no bytes received.\n\t\t\t// DoneInfo with default value works.\n\t\t\tpickResult.Done(balancer.DoneInfo{})\n\t\t}\n\t\tlogger.Infof(\"blockingPicker: the picked transport is not ready, loop back to repick\")\n\t\t// If ok == false, ac.state is not READY.\n\t\t// A valid picker always returns READY subConn. This means the state of ac\n\t\t// just changed, and picker will be updated shortly.\n\t\t// continue back to the beginning of the for loop to repick.\n\t}\n}\n\nfunc (pw *pickerWrapper) close() {\n\told := pw.pickerGen.Swap(nil)\n\tclose(old.blockingCh)\n}\n\n// reset clears the pickerWrapper and prepares it for being used again when idle\n// mode is exited.\nfunc (pw *pickerWrapper) reset() {\n\told := pw.pickerGen.Swap(&pickerGeneration{blockingCh: make(chan struct{})})\n\tclose(old.blockingCh)\n}\n\n// dropError is a wrapper error that indicates the LB policy wishes to drop the\n// RPC and not retry it.\ntype dropError struct {\n\terror\n}\n"
        },
        {
          "name": "picker_wrapper_test.go",
          "type": "blob",
          "size": 5.8427734375,
          "content": "/*\n *\n * Copyright 2017 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"google.golang.org/grpc/balancer\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/connectivity\"\n\t\"google.golang.org/grpc/internal/transport\"\n\t\"google.golang.org/grpc/status\"\n)\n\nconst goroutineCount = 5\n\nvar (\n\ttestT  = &testTransport{}\n\ttestSC = &acBalancerWrapper{ac: &addrConn{\n\t\tstate:     connectivity.Ready,\n\t\ttransport: testT,\n\t}}\n\ttestSCNotReady = &acBalancerWrapper{ac: &addrConn{\n\t\tstate: connectivity.TransientFailure,\n\t}}\n)\n\ntype testTransport struct {\n\ttransport.ClientTransport\n}\n\ntype testingPicker struct {\n\terr       error\n\tsc        balancer.SubConn\n\tmaxCalled int64\n}\n\nfunc (p *testingPicker) Pick(balancer.PickInfo) (balancer.PickResult, error) {\n\tif atomic.AddInt64(&p.maxCalled, -1) < 0 {\n\t\treturn balancer.PickResult{}, fmt.Errorf(\"pick called to many times (> goroutineCount)\")\n\t}\n\tif p.err != nil {\n\t\treturn balancer.PickResult{}, p.err\n\t}\n\treturn balancer.PickResult{SubConn: p.sc}, nil\n}\n\nfunc (s) TestBlockingPickTimeout(t *testing.T) {\n\tbp := newPickerWrapper(nil)\n\tctx, cancel := context.WithTimeout(context.Background(), time.Millisecond)\n\tdefer cancel()\n\tif _, _, err := bp.pick(ctx, true, balancer.PickInfo{}); status.Code(err) != codes.DeadlineExceeded {\n\t\tt.Errorf(\"bp.pick returned error %v, want DeadlineExceeded\", err)\n\t}\n}\n\nfunc (s) TestBlockingPick(t *testing.T) {\n\tbp := newPickerWrapper(nil)\n\t// All goroutines should block because picker is nil in bp.\n\tvar finishedCount uint64\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\twg := sync.WaitGroup{}\n\twg.Add(goroutineCount)\n\tfor i := goroutineCount; i > 0; i-- {\n\t\tgo func() {\n\t\t\tif tr, _, err := bp.pick(ctx, true, balancer.PickInfo{}); err != nil || tr != testT {\n\t\t\t\tt.Errorf(\"bp.pick returned transport: %v, error: %v, want transport: %v, error: nil\", tr, err, testT)\n\t\t\t}\n\t\t\tatomic.AddUint64(&finishedCount, 1)\n\t\t\twg.Done()\n\t\t}()\n\t}\n\ttime.Sleep(50 * time.Millisecond)\n\tif c := atomic.LoadUint64(&finishedCount); c != 0 {\n\t\tt.Errorf(\"finished goroutines count: %v, want 0\", c)\n\t}\n\tbp.updatePicker(&testingPicker{sc: testSC, maxCalled: goroutineCount})\n\t// Wait for all pickers to finish before the context is cancelled.\n\twg.Wait()\n}\n\nfunc (s) TestBlockingPickNoSubAvailable(t *testing.T) {\n\tbp := newPickerWrapper(nil)\n\tvar finishedCount uint64\n\tbp.updatePicker(&testingPicker{err: balancer.ErrNoSubConnAvailable, maxCalled: goroutineCount})\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\t// All goroutines should block because picker returns no subConn available.\n\twg := sync.WaitGroup{}\n\twg.Add(goroutineCount)\n\tfor i := goroutineCount; i > 0; i-- {\n\t\tgo func() {\n\t\t\tif tr, _, err := bp.pick(ctx, true, balancer.PickInfo{}); err != nil || tr != testT {\n\t\t\t\tt.Errorf(\"bp.pick returned transport: %v, error: %v, want transport: %v, error: nil\", tr, err, testT)\n\t\t\t}\n\t\t\tatomic.AddUint64(&finishedCount, 1)\n\t\t\twg.Done()\n\t\t}()\n\t}\n\ttime.Sleep(50 * time.Millisecond)\n\tif c := atomic.LoadUint64(&finishedCount); c != 0 {\n\t\tt.Errorf(\"finished goroutines count: %v, want 0\", c)\n\t}\n\tbp.updatePicker(&testingPicker{sc: testSC, maxCalled: goroutineCount})\n\t// Wait for all pickers to finish before the context is cancelled.\n\twg.Wait()\n}\n\nfunc (s) TestBlockingPickTransientWaitforready(t *testing.T) {\n\tbp := newPickerWrapper(nil)\n\tbp.updatePicker(&testingPicker{err: balancer.ErrTransientFailure, maxCalled: goroutineCount})\n\tvar finishedCount uint64\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\t// All goroutines should block because picker returns transientFailure and\n\t// picks are not failfast.\n\twg := sync.WaitGroup{}\n\twg.Add(goroutineCount)\n\tfor i := goroutineCount; i > 0; i-- {\n\t\tgo func() {\n\t\t\tif tr, _, err := bp.pick(ctx, false, balancer.PickInfo{}); err != nil || tr != testT {\n\t\t\t\tt.Errorf(\"bp.pick returned transport: %v, error: %v, want transport: %v, error: nil\", tr, err, testT)\n\t\t\t}\n\t\t\tatomic.AddUint64(&finishedCount, 1)\n\t\t\twg.Done()\n\t\t}()\n\t}\n\ttime.Sleep(time.Millisecond)\n\tif c := atomic.LoadUint64(&finishedCount); c != 0 {\n\t\tt.Errorf(\"finished goroutines count: %v, want 0\", c)\n\t}\n\tbp.updatePicker(&testingPicker{sc: testSC, maxCalled: goroutineCount})\n\t// Wait for all pickers to finish before the context is cancelled.\n\twg.Wait()\n}\n\nfunc (s) TestBlockingPickSCNotReady(t *testing.T) {\n\tbp := newPickerWrapper(nil)\n\tbp.updatePicker(&testingPicker{sc: testSCNotReady, maxCalled: goroutineCount})\n\tvar finishedCount uint64\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\t// All goroutines should block because subConn is not ready.\n\twg := sync.WaitGroup{}\n\twg.Add(goroutineCount)\n\tfor i := goroutineCount; i > 0; i-- {\n\t\tgo func() {\n\t\t\tif tr, _, err := bp.pick(ctx, true, balancer.PickInfo{}); err != nil || tr != testT {\n\t\t\t\tt.Errorf(\"bp.pick returned transport: %v, error: %v, want transport: %v, error: nil\", tr, err, testT)\n\t\t\t}\n\t\t\tatomic.AddUint64(&finishedCount, 1)\n\t\t\twg.Done()\n\t\t}()\n\t}\n\ttime.Sleep(time.Millisecond)\n\tif c := atomic.LoadUint64(&finishedCount); c != 0 {\n\t\tt.Errorf(\"finished goroutines count: %v, want 0\", c)\n\t}\n\tbp.updatePicker(&testingPicker{sc: testSC, maxCalled: goroutineCount})\n\t// Wait for all pickers to finish before the context is cancelled.\n\twg.Wait()\n}\n"
        },
        {
          "name": "preloader.go",
          "type": "blob",
          "size": 2.515625,
          "content": "/*\n *\n * Copyright 2019 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/mem\"\n\t\"google.golang.org/grpc/status\"\n)\n\n// PreparedMsg is responsible for creating a Marshalled and Compressed object.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype PreparedMsg struct {\n\t// Struct for preparing msg before sending them\n\tencodedData mem.BufferSlice\n\thdr         []byte\n\tpayload     mem.BufferSlice\n\tpf          payloadFormat\n}\n\n// Encode marshalls and compresses the message using the codec and compressor for the stream.\nfunc (p *PreparedMsg) Encode(s Stream, msg any) error {\n\tctx := s.Context()\n\trpcInfo, ok := rpcInfoFromContext(ctx)\n\tif !ok {\n\t\treturn status.Errorf(codes.Internal, \"grpc: unable to get rpcInfo\")\n\t}\n\n\t// check if the context has the relevant information to prepareMsg\n\tif rpcInfo.preloaderInfo == nil {\n\t\treturn status.Errorf(codes.Internal, \"grpc: rpcInfo.preloaderInfo is nil\")\n\t}\n\tif rpcInfo.preloaderInfo.codec == nil {\n\t\treturn status.Errorf(codes.Internal, \"grpc: rpcInfo.preloaderInfo.codec is nil\")\n\t}\n\n\t// prepare the msg\n\tdata, err := encode(rpcInfo.preloaderInfo.codec, msg)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tmaterializedData := data.Materialize()\n\tdata.Free()\n\tp.encodedData = mem.BufferSlice{mem.SliceBuffer(materializedData)}\n\n\t// TODO: it should be possible to grab the bufferPool from the underlying\n\t//  stream implementation with a type cast to its actual type (such as\n\t//  addrConnStream) and accessing the buffer pool directly.\n\tvar compData mem.BufferSlice\n\tcompData, p.pf, err = compress(p.encodedData, rpcInfo.preloaderInfo.cp, rpcInfo.preloaderInfo.comp, mem.DefaultBufferPool())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.pf.isCompressed() {\n\t\tmaterializedCompData := compData.Materialize()\n\t\tcompData.Free()\n\t\tcompData = mem.BufferSlice{mem.SliceBuffer(materializedCompData)}\n\t}\n\n\tp.hdr, p.payload = msgHeader(p.encodedData, compData, p.pf)\n\n\treturn nil\n}\n"
        },
        {
          "name": "producer_ext_test.go",
          "type": "blob",
          "size": 3.4541015625,
          "content": "/*\n *\n * Copyright 2024 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc_test\n\nimport (\n\t\"context\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"testing\"\n\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/balancer\"\n\t\"google.golang.org/grpc/connectivity\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n\t\"google.golang.org/grpc/internal/balancer/stub\"\n\t\"google.golang.org/grpc/internal/stubserver\"\n\t\"google.golang.org/grpc/internal/testutils\"\n\ttestgrpc \"google.golang.org/grpc/interop/grpc_testing\"\n)\n\n// TestProducerStopsBeforeStateChange confirms that producers are stopped before\n// any state change notification is delivered to the LB policy.\nfunc (s) TestProducerStopsBeforeStateChange(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\n\tname := strings.ReplaceAll(strings.ToLower(t.Name()), \"/\", \"\")\n\tvar lastProducer *testProducer\n\tbf := stub.BalancerFuncs{\n\t\tUpdateClientConnState: func(bd *stub.BalancerData, ccs balancer.ClientConnState) error {\n\t\t\tvar sc balancer.SubConn\n\t\t\tsc, err := bd.ClientConn.NewSubConn(ccs.ResolverState.Addresses, balancer.NewSubConnOptions{\n\t\t\t\tStateListener: func(scs balancer.SubConnState) {\n\t\t\t\t\tbd.ClientConn.UpdateState(balancer.State{\n\t\t\t\t\t\tConnectivityState: scs.ConnectivityState,\n\t\t\t\t\t\t// We do not pass a picker, but since we don't perform\n\t\t\t\t\t\t// RPCs, that's okay.\n\t\t\t\t\t})\n\t\t\t\t\tif !lastProducer.stopped.Load() {\n\t\t\t\t\t\tt.Errorf(\"lastProducer not stopped before state change notification\")\n\t\t\t\t\t}\n\t\t\t\t\tt.Logf(\"State is now %v; recreating producer\", scs.ConnectivityState)\n\t\t\t\t\tp, _ := sc.GetOrBuildProducer(producerBuilderSingleton)\n\t\t\t\t\tlastProducer = p.(*testProducer)\n\t\t\t\t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tp, _ := sc.GetOrBuildProducer(producerBuilderSingleton)\n\t\t\tlastProducer = p.(*testProducer)\n\t\t\tsc.Connect()\n\t\t\treturn nil\n\t\t},\n\t}\n\tstub.Register(name, bf)\n\n\tss := stubserver.StubServer{\n\t\tFullDuplexCallF: func(stream testgrpc.TestService_FullDuplexCallServer) error {\n\t\t\treturn nil\n\t\t},\n\t}\n\tif err := ss.StartServer(); err != nil {\n\t\tt.Fatal(\"Error starting server:\", err)\n\t}\n\tdefer ss.Stop()\n\n\tcc, err := grpc.NewClient(\"dns:///\"+ss.Address,\n\t\tgrpc.WithDefaultServiceConfig(`{\"loadBalancingConfig\": [{\"`+name+`\":{}}]}`),\n\t\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\n\t)\n\tif err != nil {\n\t\tt.Fatalf(\"Error creating client: %v\", err)\n\t}\n\tdefer cc.Close()\n\n\tgo cc.Connect()\n\ttestutils.AwaitState(ctx, t, cc, connectivity.Ready)\n\n\tcc.Close()\n\ttestutils.AwaitState(ctx, t, cc, connectivity.Shutdown)\n}\n\ntype producerBuilder struct{}\n\ntype testProducer struct {\n\t// There should be no race accessing this field, but use an atomic since\n\t// the race checker probably can't detect that.\n\tstopped atomic.Bool\n}\n\n// Build constructs and returns a producer and its cleanup function\nfunc (*producerBuilder) Build(cci any) (balancer.Producer, func()) {\n\tp := &testProducer{}\n\treturn p, func() {\n\t\tp.stopped.Store(true)\n\t}\n}\n\nvar producerBuilderSingleton = &producerBuilder{}\n"
        },
        {
          "name": "profiling",
          "type": "tree",
          "content": null
        },
        {
          "name": "reflection",
          "type": "tree",
          "content": null
        },
        {
          "name": "resolver",
          "type": "tree",
          "content": null
        },
        {
          "name": "resolver_balancer_ext_test.go",
          "type": "blob",
          "size": 8.4091796875,
          "content": "/*\n *\n * Copyright 2023 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc_test\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"runtime\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/balancer\"\n\t\"google.golang.org/grpc/connectivity\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n\t\"google.golang.org/grpc/internal\"\n\t\"google.golang.org/grpc/internal/balancer/stub\"\n\t\"google.golang.org/grpc/internal/channelz\"\n\t\"google.golang.org/grpc/resolver\"\n\t\"google.golang.org/grpc/resolver/manual\"\n)\n\n// TestResolverBalancerInteraction tests:\n// 1. resolver.Builder.Build() ->\n// 2. resolver.ClientConn.UpdateState() ->\n// 3. balancer.Balancer.UpdateClientConnState() ->\n// 4. balancer.ClientConn.ResolveNow() ->\n// 5. resolver.Resolver.ResolveNow() ->\nfunc (s) TestResolverBalancerInteraction(t *testing.T) {\n\tname := strings.ReplaceAll(strings.ToLower(t.Name()), \"/\", \"\")\n\tbf := stub.BalancerFuncs{\n\t\tUpdateClientConnState: func(bd *stub.BalancerData, ccs balancer.ClientConnState) error {\n\t\t\tbd.ClientConn.ResolveNow(resolver.ResolveNowOptions{})\n\t\t\treturn nil\n\t\t},\n\t}\n\tstub.Register(name, bf)\n\n\trb := manual.NewBuilderWithScheme(name)\n\trb.BuildCallback = func(_ resolver.Target, cc resolver.ClientConn, _ resolver.BuildOptions) {\n\t\tsc := cc.ParseServiceConfig(`{\"loadBalancingConfig\": [{\"` + name + `\":{}}]}`)\n\t\tcc.UpdateState(resolver.State{\n\t\t\tAddresses:     []resolver.Address{{Addr: \"test\"}},\n\t\t\tServiceConfig: sc,\n\t\t})\n\t}\n\trnCh := make(chan struct{})\n\trb.ResolveNowCallback = func(resolver.ResolveNowOptions) { close(rnCh) }\n\tresolver.Register(rb)\n\n\tcc, err := grpc.NewClient(name+\":///\", grpc.WithTransportCredentials(insecure.NewCredentials()))\n\tif err != nil {\n\t\tt.Fatalf(\"grpc.NewClient error: %v\", err)\n\t}\n\tdefer cc.Close()\n\tcc.Connect()\n\tselect {\n\tcase <-rnCh:\n\tcase <-time.After(defaultTestTimeout):\n\t\tt.Fatalf(\"timed out waiting for resolver.ResolveNow\")\n\t}\n}\n\ntype resolverBuilderWithErr struct {\n\tresolver.Resolver\n\terrCh  <-chan error\n\tscheme string\n}\n\nfunc (b *resolverBuilderWithErr) Build(_ resolver.Target, cc resolver.ClientConn, _ resolver.BuildOptions) (resolver.Resolver, error) {\n\tif err := <-b.errCh; err != nil {\n\t\treturn nil, err\n\t}\n\treturn b, nil\n}\n\nfunc (b *resolverBuilderWithErr) Scheme() string {\n\treturn b.scheme\n}\n\nfunc (b *resolverBuilderWithErr) Close() {}\n\n// TestResolverBuildFailure tests:\n// 1. resolver.Builder.Build() passes.\n// 2. Channel enters idle mode.\n// 3. An RPC happens.\n// 4. resolver.Builder.Build() fails.\nfunc (s) TestResolverBuildFailure(t *testing.T) {\n\tenterIdle := internal.EnterIdleModeForTesting.(func(*grpc.ClientConn))\n\tname := strings.ReplaceAll(strings.ToLower(t.Name()), \"/\", \"\")\n\tresErrCh := make(chan error, 1)\n\tresolver.Register(&resolverBuilderWithErr{errCh: resErrCh, scheme: name})\n\n\tresErrCh <- nil\n\tcc, err := grpc.NewClient(name+\":///\", grpc.WithTransportCredentials(insecure.NewCredentials()))\n\tif err != nil {\n\t\tt.Fatalf(\"grpc.NewClient error: %v\", err)\n\t}\n\tdefer cc.Close()\n\tcc.Connect()\n\tenterIdle(cc)\n\tconst errStr = \"test error from resolver builder\"\n\tt.Log(\"pushing res err\")\n\tresErrCh <- errors.New(errStr)\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\tif err := cc.Invoke(ctx, \"/a/b\", nil, nil); err == nil || !strings.Contains(err.Error(), errStr) {\n\t\tt.Fatalf(\"Invoke = %v; want %v\", err, errStr)\n\t}\n}\n\n// TestEnterIdleDuringResolverUpdateState tests a scenario that used to deadlock\n// while calling UpdateState at the same time as the resolver being closed while\n// the channel enters idle mode.\nfunc (s) TestEnterIdleDuringResolverUpdateState(t *testing.T) {\n\tenterIdle := internal.EnterIdleModeForTesting.(func(*grpc.ClientConn))\n\tname := strings.ReplaceAll(strings.ToLower(t.Name()), \"/\", \"\")\n\n\t// Create a manual resolver that spams UpdateState calls until it is closed.\n\trb := manual.NewBuilderWithScheme(name)\n\tvar cancel context.CancelFunc\n\trb.BuildCallback = func(_ resolver.Target, cc resolver.ClientConn, _ resolver.BuildOptions) {\n\t\tvar ctx context.Context\n\t\tctx, cancel = context.WithCancel(context.Background())\n\t\tgo func() {\n\t\t\tfor ctx.Err() == nil {\n\t\t\t\tcc.UpdateState(resolver.State{Addresses: []resolver.Address{{Addr: \"test\"}}})\n\t\t\t}\n\t\t}()\n\t}\n\trb.CloseCallback = func() {\n\t\tcancel()\n\t}\n\tresolver.Register(rb)\n\n\tcc, err := grpc.NewClient(name+\":///\", grpc.WithTransportCredentials(insecure.NewCredentials()))\n\tif err != nil {\n\t\tt.Fatalf(\"grpc.NewClient error: %v\", err)\n\t}\n\tdefer cc.Close()\n\n\t// Enter/exit idle mode repeatedly.\n\tfor i := 0; i < 2000; i++ {\n\t\t// Start a timer so we panic out of the deadlock and can see all the\n\t\t// stack traces to debug the problem.\n\t\tp := time.AfterFunc(time.Second, func() {\n\t\t\tbuf := make([]byte, 8192)\n\t\t\tbuf = buf[0:runtime.Stack(buf, true)]\n\t\t\tt.Error(\"Timed out waiting for enterIdle\")\n\t\t\tpanic(fmt.Sprint(\"Stack trace:\\n\", string(buf)))\n\t\t})\n\t\tenterIdle(cc)\n\t\tp.Stop()\n\t\tcc.Connect()\n\t}\n}\n\n// TestEnterIdleDuringBalancerUpdateState tests calling UpdateState at the same\n// time as the balancer being closed while the channel enters idle mode.\nfunc (s) TestEnterIdleDuringBalancerUpdateState(t *testing.T) {\n\tenterIdle := internal.EnterIdleModeForTesting.(func(*grpc.ClientConn))\n\tname := strings.ReplaceAll(strings.ToLower(t.Name()), \"/\", \"\")\n\n\t// Create a balancer that calls UpdateState once asynchronously, attempting\n\t// to make the channel appear ready even after entering idle.\n\tbf := stub.BalancerFuncs{\n\t\tUpdateClientConnState: func(bd *stub.BalancerData, ccs balancer.ClientConnState) error {\n\t\t\tgo func() {\n\t\t\t\tbd.ClientConn.UpdateState(balancer.State{ConnectivityState: connectivity.Ready})\n\t\t\t}()\n\t\t\treturn nil\n\t\t},\n\t}\n\tstub.Register(name, bf)\n\n\trb := manual.NewBuilderWithScheme(name)\n\trb.BuildCallback = func(_ resolver.Target, cc resolver.ClientConn, _ resolver.BuildOptions) {\n\t\tcc.UpdateState(resolver.State{Addresses: []resolver.Address{{Addr: \"test\"}}})\n\t}\n\tresolver.Register(rb)\n\n\tcc, err := grpc.NewClient(\n\t\tname+\":///\",\n\t\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\n\t\tgrpc.WithDefaultServiceConfig(`{\"loadBalancingConfig\": [{\"`+name+`\":{}}]}`))\n\tif err != nil {\n\t\tt.Fatalf(\"grpc.NewClient error: %v\", err)\n\t}\n\tdefer cc.Close()\n\n\t// Enter/exit idle mode repeatedly.\n\tfor i := 0; i < 2000; i++ {\n\t\tenterIdle(cc)\n\t\tif got, want := cc.GetState(), connectivity.Idle; got != want {\n\t\t\tt.Fatalf(\"cc state = %v; want %v\", got, want)\n\t\t}\n\t\tcc.Connect()\n\t}\n}\n\n// TestEnterIdleDuringBalancerNewSubConn tests calling NewSubConn at the same\n// time as the balancer being closed while the channel enters idle mode.\nfunc (s) TestEnterIdleDuringBalancerNewSubConn(t *testing.T) {\n\tchannelz.TurnOn()\n\tdefer internal.ChannelzTurnOffForTesting()\n\tenterIdle := internal.EnterIdleModeForTesting.(func(*grpc.ClientConn))\n\tname := strings.ReplaceAll(strings.ToLower(t.Name()), \"/\", \"\")\n\n\t// Create a balancer that calls NewSubConn once asynchronously, attempting\n\t// to create a subchannel after going idle.\n\tbf := stub.BalancerFuncs{\n\t\tUpdateClientConnState: func(bd *stub.BalancerData, ccs balancer.ClientConnState) error {\n\t\t\tgo func() {\n\t\t\t\tbd.ClientConn.NewSubConn([]resolver.Address{{Addr: \"test\"}}, balancer.NewSubConnOptions{})\n\t\t\t}()\n\t\t\treturn nil\n\t\t},\n\t}\n\tstub.Register(name, bf)\n\n\trb := manual.NewBuilderWithScheme(name)\n\trb.BuildCallback = func(_ resolver.Target, cc resolver.ClientConn, _ resolver.BuildOptions) {\n\t\tcc.UpdateState(resolver.State{Addresses: []resolver.Address{{Addr: \"test\"}}})\n\t}\n\tresolver.Register(rb)\n\n\tcc, err := grpc.NewClient(\n\t\tname+\":///\",\n\t\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\n\t\tgrpc.WithDefaultServiceConfig(`{\"loadBalancingConfig\": [{\"`+name+`\":{}}]}`))\n\tif err != nil {\n\t\tt.Fatalf(\"grpc.NewClient error: %v\", err)\n\t}\n\tdefer cc.Close()\n\n\t// Enter/exit idle mode repeatedly.\n\tfor i := 0; i < 2000; i++ {\n\t\tenterIdle(cc)\n\t\ttcs, _ := channelz.GetTopChannels(0, 0)\n\t\tif len(tcs) != 1 {\n\t\t\tt.Fatalf(\"Found channels: %v; expected 1 entry\", tcs)\n\t\t}\n\t\tif got := tcs[0].SubChans(); len(got) != 0 {\n\t\t\tt.Fatalf(\"Found subchannels: %v; expected 0 entries\", got)\n\t\t}\n\t\tcc.Connect()\n\t}\n}\n"
        },
        {
          "name": "resolver_test.go",
          "type": "blob",
          "size": 5.1806640625,
          "content": "/*\n *\n * Copyright 2023 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"testing\"\n\n\t\"github.com/google/go-cmp/cmp\"\n\t\"google.golang.org/grpc/attributes\"\n\t\"google.golang.org/grpc/balancer\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n\t\"google.golang.org/grpc/internal/balancer/stub\"\n\t\"google.golang.org/grpc/resolver\"\n\t\"google.golang.org/grpc/resolver/manual\"\n)\n\ntype wrapResolverBuilder struct {\n\tresolver.Builder\n\tscheme string\n}\n\nfunc (w *wrapResolverBuilder) Scheme() string {\n\treturn w.scheme\n}\n\nfunc init() {\n\tresolver.Register(&wrapResolverBuilder{Builder: resolver.Get(\"passthrough\"), scheme: \"casetest\"})\n\tresolver.Register(&wrapResolverBuilder{Builder: resolver.Get(\"dns\"), scheme: \"caseTest\"})\n}\n\nfunc (s) TestResolverCaseSensitivity(t *testing.T) {\n\t// This should find the \"casetest\" resolver instead of the \"caseTest\"\n\t// resolver, even though the latter was registered later.  \"casetest\" is\n\t// \"passthrough\" and \"caseTest\" is \"dns\".  With \"passthrough\" the dialer\n\t// should see the target's address directly, but \"dns\" would be converted\n\t// into a loopback IP (v4 or v6) address.\n\ttarget := \"caseTest:///localhost:1234\"\n\taddrCh := make(chan string, 1)\n\tcustomDialer := func(_ context.Context, addr string) (net.Conn, error) {\n\t\tselect {\n\t\tcase addrCh <- addr:\n\t\tdefault:\n\t\t}\n\t\treturn nil, fmt.Errorf(\"not dialing with custom dialer\")\n\t}\n\n\tcc, err := Dial(target, WithContextDialer(customDialer), WithTransportCredentials(insecure.NewCredentials()))\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected Dial(%q) error: %v\", target, err)\n\t}\n\tcc.Connect()\n\tif got, want := <-addrCh, \"localhost:1234\"; got != want {\n\t\tcc.Close()\n\t\tt.Fatalf(\"Dialer got address %q; wanted %q\", got, want)\n\t}\n\tcc.Close()\n\n\t// Clear addrCh for future use.\n\tselect {\n\tcase <-addrCh:\n\tdefault:\n\t}\n\n\tres := &wrapResolverBuilder{Builder: resolver.Get(\"dns\"), scheme: \"caseTest2\"}\n\t// This should not find the injected resolver due to the case not matching.\n\t// This results in \"passthrough\" being used with the address as the whole\n\t// target.\n\ttarget = \"caseTest2:///localhost:1234\"\n\tcc, err = Dial(target, WithContextDialer(customDialer), WithResolvers(res), WithTransportCredentials(insecure.NewCredentials()))\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected Dial(%q) error: %v\", target, err)\n\t}\n\tcc.Connect()\n\tif got, want := <-addrCh, target; got != want {\n\t\tcc.Close()\n\t\tt.Fatalf(\"Dialer got address %q; wanted %q\", got, want)\n\t}\n\tcc.Close()\n}\n\n// TestResolverAddressesToEndpoints ensures one Endpoint is created for each\n// entry in resolver.State.Addresses automatically.\nfunc (s) TestResolverAddressesToEndpoints(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\n\tconst scheme = \"testresolveraddressestoendpoints\"\n\tr := manual.NewBuilderWithScheme(scheme)\n\n\tstateCh := make(chan balancer.ClientConnState, 1)\n\tbf := stub.BalancerFuncs{\n\t\tUpdateClientConnState: func(_ *stub.BalancerData, ccs balancer.ClientConnState) error {\n\t\t\tstateCh <- ccs\n\t\t\treturn nil\n\t\t},\n\t}\n\tbalancerName := \"stub-balancer-\" + scheme\n\tstub.Register(balancerName, bf)\n\n\ta1 := attributes.New(\"x\", \"y\")\n\ta2 := attributes.New(\"a\", \"b\")\n\tr.InitialState(resolver.State{Addresses: []resolver.Address{{Addr: \"addr1\", BalancerAttributes: a1}, {Addr: \"addr2\", BalancerAttributes: a2}}})\n\n\tcc, err := Dial(r.Scheme()+\":///\",\n\t\tWithTransportCredentials(insecure.NewCredentials()),\n\t\tWithResolvers(r),\n\t\tWithDefaultServiceConfig(fmt.Sprintf(`{\"loadBalancingConfig\": [{\"%s\":{}}]}`, balancerName)))\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error dialing: %v\", err)\n\t}\n\tdefer cc.Close()\n\n\tselect {\n\tcase got := <-stateCh:\n\t\twant := []resolver.Endpoint{\n\t\t\t{Addresses: []resolver.Address{{Addr: \"addr1\"}}, Attributes: a1},\n\t\t\t{Addresses: []resolver.Address{{Addr: \"addr2\"}}, Attributes: a2},\n\t\t}\n\t\tif diff := cmp.Diff(got.ResolverState.Endpoints, want); diff != \"\" {\n\t\t\tt.Errorf(\"Did not receive expected endpoints.  Diff (-got +want):\\n%v\", diff)\n\t\t}\n\tcase <-ctx.Done():\n\t\tt.Fatalf(\"timed out waiting for endpoints\")\n\t}\n}\n\n// Test ensures that there is no panic if the attributes within\n// resolver.State.Addresses contains a typed-nil value.\nfunc (s) TestResolverAddressesWithTypedNilAttribute(t *testing.T) {\n\tr := manual.NewBuilderWithScheme(t.Name())\n\tresolver.Register(r)\n\n\taddrAttr := attributes.New(\"typed_nil\", (*stringerVal)(nil))\n\tr.InitialState(resolver.State{Addresses: []resolver.Address{{Addr: \"addr1\", Attributes: addrAttr}}})\n\n\tcc, err := Dial(r.Scheme()+\":///\", WithTransportCredentials(insecure.NewCredentials()), WithResolvers(r))\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error dialing: %v\", err)\n\t}\n\tdefer cc.Close()\n}\n\ntype stringerVal struct{ s string }\n\nfunc (s stringerVal) String() string { return s.s }\n"
        },
        {
          "name": "resolver_wrapper.go",
          "type": "blob",
          "size": 6.3837890625,
          "content": "/*\n *\n * Copyright 2017 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"google.golang.org/grpc/internal/channelz\"\n\t\"google.golang.org/grpc/internal/grpcsync\"\n\t\"google.golang.org/grpc/internal/pretty\"\n\t\"google.golang.org/grpc/resolver\"\n\t\"google.golang.org/grpc/serviceconfig\"\n)\n\n// ccResolverWrapper is a wrapper on top of cc for resolvers.\n// It implements resolver.ClientConn interface.\ntype ccResolverWrapper struct {\n\t// The following fields are initialized when the wrapper is created and are\n\t// read-only afterwards, and therefore can be accessed without a mutex.\n\tcc                  *ClientConn\n\tignoreServiceConfig bool\n\tserializer          *grpcsync.CallbackSerializer\n\tserializerCancel    context.CancelFunc\n\n\tresolver resolver.Resolver // only accessed within the serializer\n\n\t// The following fields are protected by mu.  Caller must take cc.mu before\n\t// taking mu.\n\tmu       sync.Mutex\n\tcurState resolver.State\n\tclosed   bool\n}\n\n// newCCResolverWrapper initializes the ccResolverWrapper.  It can only be used\n// after calling start, which builds the resolver.\nfunc newCCResolverWrapper(cc *ClientConn) *ccResolverWrapper {\n\tctx, cancel := context.WithCancel(cc.ctx)\n\treturn &ccResolverWrapper{\n\t\tcc:                  cc,\n\t\tignoreServiceConfig: cc.dopts.disableServiceConfig,\n\t\tserializer:          grpcsync.NewCallbackSerializer(ctx),\n\t\tserializerCancel:    cancel,\n\t}\n}\n\n// start builds the name resolver using the resolver.Builder in cc and returns\n// any error encountered.  It must always be the first operation performed on\n// any newly created ccResolverWrapper, except that close may be called instead.\nfunc (ccr *ccResolverWrapper) start() error {\n\terrCh := make(chan error)\n\tccr.serializer.TrySchedule(func(ctx context.Context) {\n\t\tif ctx.Err() != nil {\n\t\t\treturn\n\t\t}\n\t\topts := resolver.BuildOptions{\n\t\t\tDisableServiceConfig: ccr.cc.dopts.disableServiceConfig,\n\t\t\tDialCreds:            ccr.cc.dopts.copts.TransportCredentials,\n\t\t\tCredsBundle:          ccr.cc.dopts.copts.CredsBundle,\n\t\t\tDialer:               ccr.cc.dopts.copts.Dialer,\n\t\t\tAuthority:            ccr.cc.authority,\n\t\t}\n\t\tvar err error\n\t\tccr.resolver, err = ccr.cc.resolverBuilder.Build(ccr.cc.parsedTarget, ccr, opts)\n\t\terrCh <- err\n\t})\n\treturn <-errCh\n}\n\nfunc (ccr *ccResolverWrapper) resolveNow(o resolver.ResolveNowOptions) {\n\tccr.serializer.TrySchedule(func(ctx context.Context) {\n\t\tif ctx.Err() != nil || ccr.resolver == nil {\n\t\t\treturn\n\t\t}\n\t\tccr.resolver.ResolveNow(o)\n\t})\n}\n\n// close initiates async shutdown of the wrapper.  To determine the wrapper has\n// finished shutting down, the channel should block on ccr.serializer.Done()\n// without cc.mu held.\nfunc (ccr *ccResolverWrapper) close() {\n\tchannelz.Info(logger, ccr.cc.channelz, \"Closing the name resolver\")\n\tccr.mu.Lock()\n\tccr.closed = true\n\tccr.mu.Unlock()\n\n\tccr.serializer.TrySchedule(func(context.Context) {\n\t\tif ccr.resolver == nil {\n\t\t\treturn\n\t\t}\n\t\tccr.resolver.Close()\n\t\tccr.resolver = nil\n\t})\n\tccr.serializerCancel()\n}\n\n// UpdateState is called by resolver implementations to report new state to gRPC\n// which includes addresses and service config.\nfunc (ccr *ccResolverWrapper) UpdateState(s resolver.State) error {\n\tccr.cc.mu.Lock()\n\tccr.mu.Lock()\n\tif ccr.closed {\n\t\tccr.mu.Unlock()\n\t\tccr.cc.mu.Unlock()\n\t\treturn nil\n\t}\n\tif s.Endpoints == nil {\n\t\ts.Endpoints = make([]resolver.Endpoint, 0, len(s.Addresses))\n\t\tfor _, a := range s.Addresses {\n\t\t\tep := resolver.Endpoint{Addresses: []resolver.Address{a}, Attributes: a.BalancerAttributes}\n\t\t\tep.Addresses[0].BalancerAttributes = nil\n\t\t\ts.Endpoints = append(s.Endpoints, ep)\n\t\t}\n\t}\n\tccr.addChannelzTraceEvent(s)\n\tccr.curState = s\n\tccr.mu.Unlock()\n\treturn ccr.cc.updateResolverStateAndUnlock(s, nil)\n}\n\n// ReportError is called by resolver implementations to report errors\n// encountered during name resolution to gRPC.\nfunc (ccr *ccResolverWrapper) ReportError(err error) {\n\tccr.cc.mu.Lock()\n\tccr.mu.Lock()\n\tif ccr.closed {\n\t\tccr.mu.Unlock()\n\t\tccr.cc.mu.Unlock()\n\t\treturn\n\t}\n\tccr.mu.Unlock()\n\tchannelz.Warningf(logger, ccr.cc.channelz, \"ccResolverWrapper: reporting error to cc: %v\", err)\n\tccr.cc.updateResolverStateAndUnlock(resolver.State{}, err)\n}\n\n// NewAddress is called by the resolver implementation to send addresses to\n// gRPC.\nfunc (ccr *ccResolverWrapper) NewAddress(addrs []resolver.Address) {\n\tccr.cc.mu.Lock()\n\tccr.mu.Lock()\n\tif ccr.closed {\n\t\tccr.mu.Unlock()\n\t\tccr.cc.mu.Unlock()\n\t\treturn\n\t}\n\ts := resolver.State{Addresses: addrs, ServiceConfig: ccr.curState.ServiceConfig}\n\tccr.addChannelzTraceEvent(s)\n\tccr.curState = s\n\tccr.mu.Unlock()\n\tccr.cc.updateResolverStateAndUnlock(s, nil)\n}\n\n// ParseServiceConfig is called by resolver implementations to parse a JSON\n// representation of the service config.\nfunc (ccr *ccResolverWrapper) ParseServiceConfig(scJSON string) *serviceconfig.ParseResult {\n\treturn parseServiceConfig(scJSON, ccr.cc.dopts.maxCallAttempts)\n}\n\n// addChannelzTraceEvent adds a channelz trace event containing the new\n// state received from resolver implementations.\nfunc (ccr *ccResolverWrapper) addChannelzTraceEvent(s resolver.State) {\n\tif !logger.V(0) && !channelz.IsOn() {\n\t\treturn\n\t}\n\tvar updates []string\n\tvar oldSC, newSC *ServiceConfig\n\tvar oldOK, newOK bool\n\tif ccr.curState.ServiceConfig != nil {\n\t\toldSC, oldOK = ccr.curState.ServiceConfig.Config.(*ServiceConfig)\n\t}\n\tif s.ServiceConfig != nil {\n\t\tnewSC, newOK = s.ServiceConfig.Config.(*ServiceConfig)\n\t}\n\tif oldOK != newOK || (oldOK && newOK && oldSC.rawJSONString != newSC.rawJSONString) {\n\t\tupdates = append(updates, \"service config updated\")\n\t}\n\tif len(ccr.curState.Addresses) > 0 && len(s.Addresses) == 0 {\n\t\tupdates = append(updates, \"resolver returned an empty address list\")\n\t} else if len(ccr.curState.Addresses) == 0 && len(s.Addresses) > 0 {\n\t\tupdates = append(updates, \"resolver returned new addresses\")\n\t}\n\tchannelz.Infof(logger, ccr.cc.channelz, \"Resolver state updated: %s (%v)\", pretty.ToJSON(s), strings.Join(updates, \"; \"))\n}\n"
        },
        {
          "name": "rpc_util.go",
          "type": "blob",
          "size": 31.4033203125,
          "content": "/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/credentials\"\n\t\"google.golang.org/grpc/encoding\"\n\t\"google.golang.org/grpc/encoding/proto\"\n\t\"google.golang.org/grpc/internal/transport\"\n\t\"google.golang.org/grpc/mem\"\n\t\"google.golang.org/grpc/metadata\"\n\t\"google.golang.org/grpc/peer\"\n\t\"google.golang.org/grpc/stats\"\n\t\"google.golang.org/grpc/status\"\n)\n\n// Compressor defines the interface gRPC uses to compress a message.\n//\n// Deprecated: use package encoding.\ntype Compressor interface {\n\t// Do compresses p into w.\n\tDo(w io.Writer, p []byte) error\n\t// Type returns the compression algorithm the Compressor uses.\n\tType() string\n}\n\ntype gzipCompressor struct {\n\tpool sync.Pool\n}\n\n// NewGZIPCompressor creates a Compressor based on GZIP.\n//\n// Deprecated: use package encoding/gzip.\nfunc NewGZIPCompressor() Compressor {\n\tc, _ := NewGZIPCompressorWithLevel(gzip.DefaultCompression)\n\treturn c\n}\n\n// NewGZIPCompressorWithLevel is like NewGZIPCompressor but specifies the gzip compression level instead\n// of assuming DefaultCompression.\n//\n// The error returned will be nil if the level is valid.\n//\n// Deprecated: use package encoding/gzip.\nfunc NewGZIPCompressorWithLevel(level int) (Compressor, error) {\n\tif level < gzip.DefaultCompression || level > gzip.BestCompression {\n\t\treturn nil, fmt.Errorf(\"grpc: invalid compression level: %d\", level)\n\t}\n\treturn &gzipCompressor{\n\t\tpool: sync.Pool{\n\t\t\tNew: func() any {\n\t\t\t\tw, err := gzip.NewWriterLevel(io.Discard, level)\n\t\t\t\tif err != nil {\n\t\t\t\t\tpanic(err)\n\t\t\t\t}\n\t\t\t\treturn w\n\t\t\t},\n\t\t},\n\t}, nil\n}\n\nfunc (c *gzipCompressor) Do(w io.Writer, p []byte) error {\n\tz := c.pool.Get().(*gzip.Writer)\n\tdefer c.pool.Put(z)\n\tz.Reset(w)\n\tif _, err := z.Write(p); err != nil {\n\t\treturn err\n\t}\n\treturn z.Close()\n}\n\nfunc (c *gzipCompressor) Type() string {\n\treturn \"gzip\"\n}\n\n// Decompressor defines the interface gRPC uses to decompress a message.\n//\n// Deprecated: use package encoding.\ntype Decompressor interface {\n\t// Do reads the data from r and uncompress them.\n\tDo(r io.Reader) ([]byte, error)\n\t// Type returns the compression algorithm the Decompressor uses.\n\tType() string\n}\n\ntype gzipDecompressor struct {\n\tpool sync.Pool\n}\n\n// NewGZIPDecompressor creates a Decompressor based on GZIP.\n//\n// Deprecated: use package encoding/gzip.\nfunc NewGZIPDecompressor() Decompressor {\n\treturn &gzipDecompressor{}\n}\n\nfunc (d *gzipDecompressor) Do(r io.Reader) ([]byte, error) {\n\tvar z *gzip.Reader\n\tswitch maybeZ := d.pool.Get().(type) {\n\tcase nil:\n\t\tnewZ, err := gzip.NewReader(r)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tz = newZ\n\tcase *gzip.Reader:\n\t\tz = maybeZ\n\t\tif err := z.Reset(r); err != nil {\n\t\t\td.pool.Put(z)\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tdefer func() {\n\t\tz.Close()\n\t\td.pool.Put(z)\n\t}()\n\treturn io.ReadAll(z)\n}\n\nfunc (d *gzipDecompressor) Type() string {\n\treturn \"gzip\"\n}\n\n// callInfo contains all related configuration and information about an RPC.\ntype callInfo struct {\n\tcompressorType        string\n\tfailFast              bool\n\tmaxReceiveMessageSize *int\n\tmaxSendMessageSize    *int\n\tcreds                 credentials.PerRPCCredentials\n\tcontentSubtype        string\n\tcodec                 baseCodec\n\tmaxRetryRPCBufferSize int\n\tonFinish              []func(err error)\n}\n\nfunc defaultCallInfo() *callInfo {\n\treturn &callInfo{\n\t\tfailFast:              true,\n\t\tmaxRetryRPCBufferSize: 256 * 1024, // 256KB\n\t}\n}\n\n// CallOption configures a Call before it starts or extracts information from\n// a Call after it completes.\ntype CallOption interface {\n\t// before is called before the call is sent to any server.  If before\n\t// returns a non-nil error, the RPC fails with that error.\n\tbefore(*callInfo) error\n\n\t// after is called after the call has completed.  after cannot return an\n\t// error, so any failures should be reported via output parameters.\n\tafter(*callInfo, *csAttempt)\n}\n\n// EmptyCallOption does not alter the Call configuration.\n// It can be embedded in another structure to carry satellite data for use\n// by interceptors.\ntype EmptyCallOption struct{}\n\nfunc (EmptyCallOption) before(*callInfo) error      { return nil }\nfunc (EmptyCallOption) after(*callInfo, *csAttempt) {}\n\n// StaticMethod returns a CallOption which specifies that a call is being made\n// to a method that is static, which means the method is known at compile time\n// and doesn't change at runtime. This can be used as a signal to stats plugins\n// that this method is safe to include as a key to a measurement.\nfunc StaticMethod() CallOption {\n\treturn StaticMethodCallOption{}\n}\n\n// StaticMethodCallOption is a CallOption that specifies that a call comes\n// from a static method.\ntype StaticMethodCallOption struct {\n\tEmptyCallOption\n}\n\n// Header returns a CallOptions that retrieves the header metadata\n// for a unary RPC.\nfunc Header(md *metadata.MD) CallOption {\n\treturn HeaderCallOption{HeaderAddr: md}\n}\n\n// HeaderCallOption is a CallOption for collecting response header metadata.\n// The metadata field will be populated *after* the RPC completes.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype HeaderCallOption struct {\n\tHeaderAddr *metadata.MD\n}\n\nfunc (o HeaderCallOption) before(*callInfo) error { return nil }\nfunc (o HeaderCallOption) after(_ *callInfo, attempt *csAttempt) {\n\t*o.HeaderAddr, _ = attempt.s.Header()\n}\n\n// Trailer returns a CallOptions that retrieves the trailer metadata\n// for a unary RPC.\nfunc Trailer(md *metadata.MD) CallOption {\n\treturn TrailerCallOption{TrailerAddr: md}\n}\n\n// TrailerCallOption is a CallOption for collecting response trailer metadata.\n// The metadata field will be populated *after* the RPC completes.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype TrailerCallOption struct {\n\tTrailerAddr *metadata.MD\n}\n\nfunc (o TrailerCallOption) before(*callInfo) error { return nil }\nfunc (o TrailerCallOption) after(_ *callInfo, attempt *csAttempt) {\n\t*o.TrailerAddr = attempt.s.Trailer()\n}\n\n// Peer returns a CallOption that retrieves peer information for a unary RPC.\n// The peer field will be populated *after* the RPC completes.\nfunc Peer(p *peer.Peer) CallOption {\n\treturn PeerCallOption{PeerAddr: p}\n}\n\n// PeerCallOption is a CallOption for collecting the identity of the remote\n// peer. The peer field will be populated *after* the RPC completes.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype PeerCallOption struct {\n\tPeerAddr *peer.Peer\n}\n\nfunc (o PeerCallOption) before(*callInfo) error { return nil }\nfunc (o PeerCallOption) after(_ *callInfo, attempt *csAttempt) {\n\tif x, ok := peer.FromContext(attempt.s.Context()); ok {\n\t\t*o.PeerAddr = *x\n\t}\n}\n\n// WaitForReady configures the RPC's behavior when the client is in\n// TRANSIENT_FAILURE, which occurs when all addresses fail to connect.  If\n// waitForReady is false, the RPC will fail immediately.  Otherwise, the client\n// will wait until a connection becomes available or the RPC's deadline is\n// reached.\n//\n// By default, RPCs do not \"wait for ready\".\nfunc WaitForReady(waitForReady bool) CallOption {\n\treturn FailFastCallOption{FailFast: !waitForReady}\n}\n\n// FailFast is the opposite of WaitForReady.\n//\n// Deprecated: use WaitForReady.\nfunc FailFast(failFast bool) CallOption {\n\treturn FailFastCallOption{FailFast: failFast}\n}\n\n// FailFastCallOption is a CallOption for indicating whether an RPC should fail\n// fast or not.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype FailFastCallOption struct {\n\tFailFast bool\n}\n\nfunc (o FailFastCallOption) before(c *callInfo) error {\n\tc.failFast = o.FailFast\n\treturn nil\n}\nfunc (o FailFastCallOption) after(*callInfo, *csAttempt) {}\n\n// OnFinish returns a CallOption that configures a callback to be called when\n// the call completes. The error passed to the callback is the status of the\n// RPC, and may be nil. The onFinish callback provided will only be called once\n// by gRPC. This is mainly used to be used by streaming interceptors, to be\n// notified when the RPC completes along with information about the status of\n// the RPC.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc OnFinish(onFinish func(err error)) CallOption {\n\treturn OnFinishCallOption{\n\t\tOnFinish: onFinish,\n\t}\n}\n\n// OnFinishCallOption is CallOption that indicates a callback to be called when\n// the call completes.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype OnFinishCallOption struct {\n\tOnFinish func(error)\n}\n\nfunc (o OnFinishCallOption) before(c *callInfo) error {\n\tc.onFinish = append(c.onFinish, o.OnFinish)\n\treturn nil\n}\n\nfunc (o OnFinishCallOption) after(*callInfo, *csAttempt) {}\n\n// MaxCallRecvMsgSize returns a CallOption which sets the maximum message size\n// in bytes the client can receive. If this is not set, gRPC uses the default\n// 4MB.\nfunc MaxCallRecvMsgSize(bytes int) CallOption {\n\treturn MaxRecvMsgSizeCallOption{MaxRecvMsgSize: bytes}\n}\n\n// MaxRecvMsgSizeCallOption is a CallOption that indicates the maximum message\n// size in bytes the client can receive.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype MaxRecvMsgSizeCallOption struct {\n\tMaxRecvMsgSize int\n}\n\nfunc (o MaxRecvMsgSizeCallOption) before(c *callInfo) error {\n\tc.maxReceiveMessageSize = &o.MaxRecvMsgSize\n\treturn nil\n}\nfunc (o MaxRecvMsgSizeCallOption) after(*callInfo, *csAttempt) {}\n\n// MaxCallSendMsgSize returns a CallOption which sets the maximum message size\n// in bytes the client can send. If this is not set, gRPC uses the default\n// `math.MaxInt32`.\nfunc MaxCallSendMsgSize(bytes int) CallOption {\n\treturn MaxSendMsgSizeCallOption{MaxSendMsgSize: bytes}\n}\n\n// MaxSendMsgSizeCallOption is a CallOption that indicates the maximum message\n// size in bytes the client can send.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype MaxSendMsgSizeCallOption struct {\n\tMaxSendMsgSize int\n}\n\nfunc (o MaxSendMsgSizeCallOption) before(c *callInfo) error {\n\tc.maxSendMessageSize = &o.MaxSendMsgSize\n\treturn nil\n}\nfunc (o MaxSendMsgSizeCallOption) after(*callInfo, *csAttempt) {}\n\n// PerRPCCredentials returns a CallOption that sets credentials.PerRPCCredentials\n// for a call.\nfunc PerRPCCredentials(creds credentials.PerRPCCredentials) CallOption {\n\treturn PerRPCCredsCallOption{Creds: creds}\n}\n\n// PerRPCCredsCallOption is a CallOption that indicates the per-RPC\n// credentials to use for the call.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype PerRPCCredsCallOption struct {\n\tCreds credentials.PerRPCCredentials\n}\n\nfunc (o PerRPCCredsCallOption) before(c *callInfo) error {\n\tc.creds = o.Creds\n\treturn nil\n}\nfunc (o PerRPCCredsCallOption) after(*callInfo, *csAttempt) {}\n\n// UseCompressor returns a CallOption which sets the compressor used when\n// sending the request.  If WithCompressor is also set, UseCompressor has\n// higher priority.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc UseCompressor(name string) CallOption {\n\treturn CompressorCallOption{CompressorType: name}\n}\n\n// CompressorCallOption is a CallOption that indicates the compressor to use.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype CompressorCallOption struct {\n\tCompressorType string\n}\n\nfunc (o CompressorCallOption) before(c *callInfo) error {\n\tc.compressorType = o.CompressorType\n\treturn nil\n}\nfunc (o CompressorCallOption) after(*callInfo, *csAttempt) {}\n\n// CallContentSubtype returns a CallOption that will set the content-subtype\n// for a call. For example, if content-subtype is \"json\", the Content-Type over\n// the wire will be \"application/grpc+json\". The content-subtype is converted\n// to lowercase before being included in Content-Type. See Content-Type on\n// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests for\n// more details.\n//\n// If ForceCodec is not also used, the content-subtype will be used to look up\n// the Codec to use in the registry controlled by RegisterCodec. See the\n// documentation on RegisterCodec for details on registration. The lookup of\n// content-subtype is case-insensitive. If no such Codec is found, the call\n// will result in an error with code codes.Internal.\n//\n// If ForceCodec is also used, that Codec will be used for all request and\n// response messages, with the content-subtype set to the given contentSubtype\n// here for requests.\nfunc CallContentSubtype(contentSubtype string) CallOption {\n\treturn ContentSubtypeCallOption{ContentSubtype: strings.ToLower(contentSubtype)}\n}\n\n// ContentSubtypeCallOption is a CallOption that indicates the content-subtype\n// used for marshaling messages.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype ContentSubtypeCallOption struct {\n\tContentSubtype string\n}\n\nfunc (o ContentSubtypeCallOption) before(c *callInfo) error {\n\tc.contentSubtype = o.ContentSubtype\n\treturn nil\n}\nfunc (o ContentSubtypeCallOption) after(*callInfo, *csAttempt) {}\n\n// ForceCodec returns a CallOption that will set codec to be used for all\n// request and response messages for a call. The result of calling Name() will\n// be used as the content-subtype after converting to lowercase, unless\n// CallContentSubtype is also used.\n//\n// See Content-Type on\n// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests for\n// more details. Also see the documentation on RegisterCodec and\n// CallContentSubtype for more details on the interaction between Codec and\n// content-subtype.\n//\n// This function is provided for advanced users; prefer to use only\n// CallContentSubtype to select a registered codec instead.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc ForceCodec(codec encoding.Codec) CallOption {\n\treturn ForceCodecCallOption{Codec: codec}\n}\n\n// ForceCodecCallOption is a CallOption that indicates the codec used for\n// marshaling messages.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype ForceCodecCallOption struct {\n\tCodec encoding.Codec\n}\n\nfunc (o ForceCodecCallOption) before(c *callInfo) error {\n\tc.codec = newCodecV1Bridge(o.Codec)\n\treturn nil\n}\nfunc (o ForceCodecCallOption) after(*callInfo, *csAttempt) {}\n\n// ForceCodecV2 returns a CallOption that will set codec to be used for all\n// request and response messages for a call. The result of calling Name() will\n// be used as the content-subtype after converting to lowercase, unless\n// CallContentSubtype is also used.\n//\n// See Content-Type on\n// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests for\n// more details. Also see the documentation on RegisterCodec and\n// CallContentSubtype for more details on the interaction between Codec and\n// content-subtype.\n//\n// This function is provided for advanced users; prefer to use only\n// CallContentSubtype to select a registered codec instead.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc ForceCodecV2(codec encoding.CodecV2) CallOption {\n\treturn ForceCodecV2CallOption{CodecV2: codec}\n}\n\n// ForceCodecV2CallOption is a CallOption that indicates the codec used for\n// marshaling messages.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype ForceCodecV2CallOption struct {\n\tCodecV2 encoding.CodecV2\n}\n\nfunc (o ForceCodecV2CallOption) before(c *callInfo) error {\n\tc.codec = o.CodecV2\n\treturn nil\n}\n\nfunc (o ForceCodecV2CallOption) after(*callInfo, *csAttempt) {}\n\n// CallCustomCodec behaves like ForceCodec, but accepts a grpc.Codec instead of\n// an encoding.Codec.\n//\n// Deprecated: use ForceCodec instead.\nfunc CallCustomCodec(codec Codec) CallOption {\n\treturn CustomCodecCallOption{Codec: codec}\n}\n\n// CustomCodecCallOption is a CallOption that indicates the codec used for\n// marshaling messages.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype CustomCodecCallOption struct {\n\tCodec Codec\n}\n\nfunc (o CustomCodecCallOption) before(c *callInfo) error {\n\tc.codec = newCodecV0Bridge(o.Codec)\n\treturn nil\n}\nfunc (o CustomCodecCallOption) after(*callInfo, *csAttempt) {}\n\n// MaxRetryRPCBufferSize returns a CallOption that limits the amount of memory\n// used for buffering this RPC's requests for retry purposes.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc MaxRetryRPCBufferSize(bytes int) CallOption {\n\treturn MaxRetryRPCBufferSizeCallOption{bytes}\n}\n\n// MaxRetryRPCBufferSizeCallOption is a CallOption indicating the amount of\n// memory to be used for caching this RPC for retry purposes.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype MaxRetryRPCBufferSizeCallOption struct {\n\tMaxRetryRPCBufferSize int\n}\n\nfunc (o MaxRetryRPCBufferSizeCallOption) before(c *callInfo) error {\n\tc.maxRetryRPCBufferSize = o.MaxRetryRPCBufferSize\n\treturn nil\n}\nfunc (o MaxRetryRPCBufferSizeCallOption) after(*callInfo, *csAttempt) {}\n\n// The format of the payload: compressed or not?\ntype payloadFormat uint8\n\nconst (\n\tcompressionNone payloadFormat = 0 // no compression\n\tcompressionMade payloadFormat = 1 // compressed\n)\n\nfunc (pf payloadFormat) isCompressed() bool {\n\treturn pf == compressionMade\n}\n\ntype streamReader interface {\n\tReadMessageHeader(header []byte) error\n\tRead(n int) (mem.BufferSlice, error)\n}\n\n// parser reads complete gRPC messages from the underlying reader.\ntype parser struct {\n\t// r is the underlying reader.\n\t// See the comment on recvMsg for the permissible\n\t// error types.\n\tr streamReader\n\n\t// The header of a gRPC message. Find more detail at\n\t// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md\n\theader [5]byte\n\n\t// bufferPool is the pool of shared receive buffers.\n\tbufferPool mem.BufferPool\n}\n\n// recvMsg reads a complete gRPC message from the stream.\n//\n// It returns the message and its payload (compression/encoding)\n// format. The caller owns the returned msg memory.\n//\n// If there is an error, possible values are:\n//   - io.EOF, when no messages remain\n//   - io.ErrUnexpectedEOF\n//   - of type transport.ConnectionError\n//   - an error from the status package\n//\n// No other error values or types must be returned, which also means\n// that the underlying streamReader must not return an incompatible\n// error.\nfunc (p *parser) recvMsg(maxReceiveMessageSize int) (payloadFormat, mem.BufferSlice, error) {\n\terr := p.r.ReadMessageHeader(p.header[:])\n\tif err != nil {\n\t\treturn 0, nil, err\n\t}\n\n\tpf := payloadFormat(p.header[0])\n\tlength := binary.BigEndian.Uint32(p.header[1:])\n\n\tif int64(length) > int64(maxInt) {\n\t\treturn 0, nil, status.Errorf(codes.ResourceExhausted, \"grpc: received message larger than max length allowed on current machine (%d vs. %d)\", length, maxInt)\n\t}\n\tif int(length) > maxReceiveMessageSize {\n\t\treturn 0, nil, status.Errorf(codes.ResourceExhausted, \"grpc: received message larger than max (%d vs. %d)\", length, maxReceiveMessageSize)\n\t}\n\n\tdata, err := p.r.Read(int(length))\n\tif err != nil {\n\t\tif err == io.EOF {\n\t\t\terr = io.ErrUnexpectedEOF\n\t\t}\n\t\treturn 0, nil, err\n\t}\n\treturn pf, data, nil\n}\n\n// encode serializes msg and returns a buffer containing the message, or an\n// error if it is too large to be transmitted by grpc.  If msg is nil, it\n// generates an empty message.\nfunc encode(c baseCodec, msg any) (mem.BufferSlice, error) {\n\tif msg == nil { // NOTE: typed nils will not be caught by this check\n\t\treturn nil, nil\n\t}\n\tb, err := c.Marshal(msg)\n\tif err != nil {\n\t\treturn nil, status.Errorf(codes.Internal, \"grpc: error while marshaling: %v\", err.Error())\n\t}\n\tif uint(b.Len()) > math.MaxUint32 {\n\t\tb.Free()\n\t\treturn nil, status.Errorf(codes.ResourceExhausted, \"grpc: message too large (%d bytes)\", len(b))\n\t}\n\treturn b, nil\n}\n\n// compress returns the input bytes compressed by compressor or cp.\n// If both compressors are nil, or if the message has zero length, returns nil,\n// indicating no compression was done.\n//\n// TODO(dfawley): eliminate cp parameter by wrapping Compressor in an encoding.Compressor.\nfunc compress(in mem.BufferSlice, cp Compressor, compressor encoding.Compressor, pool mem.BufferPool) (mem.BufferSlice, payloadFormat, error) {\n\tif (compressor == nil && cp == nil) || in.Len() == 0 {\n\t\treturn nil, compressionNone, nil\n\t}\n\tvar out mem.BufferSlice\n\tw := mem.NewWriter(&out, pool)\n\twrapErr := func(err error) error {\n\t\tout.Free()\n\t\treturn status.Errorf(codes.Internal, \"grpc: error while compressing: %v\", err.Error())\n\t}\n\tif compressor != nil {\n\t\tz, err := compressor.Compress(w)\n\t\tif err != nil {\n\t\t\treturn nil, 0, wrapErr(err)\n\t\t}\n\t\tfor _, b := range in {\n\t\t\tif _, err := z.Write(b.ReadOnlyData()); err != nil {\n\t\t\t\treturn nil, 0, wrapErr(err)\n\t\t\t}\n\t\t}\n\t\tif err := z.Close(); err != nil {\n\t\t\treturn nil, 0, wrapErr(err)\n\t\t}\n\t} else {\n\t\t// This is obviously really inefficient since it fully materializes the data, but\n\t\t// there is no way around this with the old Compressor API. At least it attempts\n\t\t// to return the buffer to the provider, in the hopes it can be reused (maybe\n\t\t// even by a subsequent call to this very function).\n\t\tbuf := in.MaterializeToBuffer(pool)\n\t\tdefer buf.Free()\n\t\tif err := cp.Do(w, buf.ReadOnlyData()); err != nil {\n\t\t\treturn nil, 0, wrapErr(err)\n\t\t}\n\t}\n\treturn out, compressionMade, nil\n}\n\nconst (\n\tpayloadLen = 1\n\tsizeLen    = 4\n\theaderLen  = payloadLen + sizeLen\n)\n\n// msgHeader returns a 5-byte header for the message being transmitted and the\n// payload, which is compData if non-nil or data otherwise.\nfunc msgHeader(data, compData mem.BufferSlice, pf payloadFormat) (hdr []byte, payload mem.BufferSlice) {\n\thdr = make([]byte, headerLen)\n\thdr[0] = byte(pf)\n\n\tvar length uint32\n\tif pf.isCompressed() {\n\t\tlength = uint32(compData.Len())\n\t\tpayload = compData\n\t} else {\n\t\tlength = uint32(data.Len())\n\t\tpayload = data\n\t}\n\n\t// Write length of payload into buf\n\tbinary.BigEndian.PutUint32(hdr[payloadLen:], length)\n\treturn hdr, payload\n}\n\nfunc outPayload(client bool, msg any, dataLength, payloadLength int, t time.Time) *stats.OutPayload {\n\treturn &stats.OutPayload{\n\t\tClient:           client,\n\t\tPayload:          msg,\n\t\tLength:           dataLength,\n\t\tWireLength:       payloadLength + headerLen,\n\t\tCompressedLength: payloadLength,\n\t\tSentTime:         t,\n\t}\n}\n\nfunc checkRecvPayload(pf payloadFormat, recvCompress string, haveCompressor bool, isServer bool) *status.Status {\n\tswitch pf {\n\tcase compressionNone:\n\tcase compressionMade:\n\t\tif recvCompress == \"\" || recvCompress == encoding.Identity {\n\t\t\treturn status.New(codes.Internal, \"grpc: compressed flag set with identity or empty encoding\")\n\t\t}\n\t\tif !haveCompressor {\n\t\t\tif isServer {\n\t\t\t\treturn status.Newf(codes.Unimplemented, \"grpc: Decompressor is not installed for grpc-encoding %q\", recvCompress)\n\t\t\t}\n\t\t\treturn status.Newf(codes.Internal, \"grpc: Decompressor is not installed for grpc-encoding %q\", recvCompress)\n\t\t}\n\tdefault:\n\t\treturn status.Newf(codes.Internal, \"grpc: received unexpected payload format %d\", pf)\n\t}\n\treturn nil\n}\n\ntype payloadInfo struct {\n\tcompressedLength  int // The compressed length got from wire.\n\tuncompressedBytes mem.BufferSlice\n}\n\nfunc (p *payloadInfo) free() {\n\tif p != nil && p.uncompressedBytes != nil {\n\t\tp.uncompressedBytes.Free()\n\t}\n}\n\n// recvAndDecompress reads a message from the stream, decompressing it if necessary.\n//\n// Cancelling the returned cancel function releases the buffer back to the pool. So the caller should cancel as soon as\n// the buffer is no longer needed.\n// TODO: Refactor this function to reduce the number of arguments.\n// See: https://google.github.io/styleguide/go/best-practices.html#function-argument-lists\nfunc recvAndDecompress(p *parser, s recvCompressor, dc Decompressor, maxReceiveMessageSize int, payInfo *payloadInfo, compressor encoding.Compressor, isServer bool,\n) (out mem.BufferSlice, err error) {\n\tpf, compressed, err := p.recvMsg(maxReceiveMessageSize)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcompressedLength := compressed.Len()\n\n\tif st := checkRecvPayload(pf, s.RecvCompress(), compressor != nil || dc != nil, isServer); st != nil {\n\t\tcompressed.Free()\n\t\treturn nil, st.Err()\n\t}\n\n\tvar size int\n\tif pf.isCompressed() {\n\t\tdefer compressed.Free()\n\n\t\t// To match legacy behavior, if the decompressor is set by WithDecompressor or RPCDecompressor,\n\t\t// use this decompressor as the default.\n\t\tif dc != nil {\n\t\t\tvar uncompressedBuf []byte\n\t\t\tuncompressedBuf, err = dc.Do(compressed.Reader())\n\t\t\tif err == nil {\n\t\t\t\tout = mem.BufferSlice{mem.SliceBuffer(uncompressedBuf)}\n\t\t\t}\n\t\t\tsize = len(uncompressedBuf)\n\t\t} else {\n\t\t\tout, size, err = decompress(compressor, compressed, maxReceiveMessageSize, p.bufferPool)\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, status.Errorf(codes.Internal, \"grpc: failed to decompress the received message: %v\", err)\n\t\t}\n\t\tif size > maxReceiveMessageSize {\n\t\t\tout.Free()\n\t\t\t// TODO: Revisit the error code. Currently keep it consistent with java\n\t\t\t// implementation.\n\t\t\treturn nil, status.Errorf(codes.ResourceExhausted, \"grpc: received message after decompression larger than max (%d vs. %d)\", size, maxReceiveMessageSize)\n\t\t}\n\t} else {\n\t\tout = compressed\n\t}\n\n\tif payInfo != nil {\n\t\tpayInfo.compressedLength = compressedLength\n\t\tout.Ref()\n\t\tpayInfo.uncompressedBytes = out\n\t}\n\n\treturn out, nil\n}\n\n// Using compressor, decompress d, returning data and size.\n// Optionally, if data will be over maxReceiveMessageSize, just return the size.\nfunc decompress(compressor encoding.Compressor, d mem.BufferSlice, maxReceiveMessageSize int, pool mem.BufferPool) (mem.BufferSlice, int, error) {\n\tdcReader, err := compressor.Decompress(d.Reader())\n\tif err != nil {\n\t\treturn nil, 0, err\n\t}\n\n\tout, err := mem.ReadAll(io.LimitReader(dcReader, int64(maxReceiveMessageSize)+1), pool)\n\tif err != nil {\n\t\tout.Free()\n\t\treturn nil, 0, err\n\t}\n\treturn out, out.Len(), nil\n}\n\ntype recvCompressor interface {\n\tRecvCompress() string\n}\n\n// For the two compressor parameters, both should not be set, but if they are,\n// dc takes precedence over compressor.\n// TODO(dfawley): wrap the old compressor/decompressor using the new API?\nfunc recv(p *parser, c baseCodec, s recvCompressor, dc Decompressor, m any, maxReceiveMessageSize int, payInfo *payloadInfo, compressor encoding.Compressor, isServer bool) error {\n\tdata, err := recvAndDecompress(p, s, dc, maxReceiveMessageSize, payInfo, compressor, isServer)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// If the codec wants its own reference to the data, it can get it. Otherwise, always\n\t// free the buffers.\n\tdefer data.Free()\n\n\tif err := c.Unmarshal(data, m); err != nil {\n\t\treturn status.Errorf(codes.Internal, \"grpc: failed to unmarshal the received message: %v\", err)\n\t}\n\n\treturn nil\n}\n\n// Information about RPC\ntype rpcInfo struct {\n\tfailfast      bool\n\tpreloaderInfo *compressorInfo\n}\n\n// Information about Preloader\n// Responsible for storing codec, and compressors\n// If stream (s) has  context s.Context which stores rpcInfo that has non nil\n// pointers to codec, and compressors, then we can use preparedMsg for Async message prep\n// and reuse marshalled bytes\ntype compressorInfo struct {\n\tcodec baseCodec\n\tcp    Compressor\n\tcomp  encoding.Compressor\n}\n\ntype rpcInfoContextKey struct{}\n\nfunc newContextWithRPCInfo(ctx context.Context, failfast bool, codec baseCodec, cp Compressor, comp encoding.Compressor) context.Context {\n\treturn context.WithValue(ctx, rpcInfoContextKey{}, &rpcInfo{\n\t\tfailfast: failfast,\n\t\tpreloaderInfo: &compressorInfo{\n\t\t\tcodec: codec,\n\t\t\tcp:    cp,\n\t\t\tcomp:  comp,\n\t\t},\n\t})\n}\n\nfunc rpcInfoFromContext(ctx context.Context) (s *rpcInfo, ok bool) {\n\ts, ok = ctx.Value(rpcInfoContextKey{}).(*rpcInfo)\n\treturn\n}\n\n// Code returns the error code for err if it was produced by the rpc system.\n// Otherwise, it returns codes.Unknown.\n//\n// Deprecated: use status.Code instead.\nfunc Code(err error) codes.Code {\n\treturn status.Code(err)\n}\n\n// ErrorDesc returns the error description of err if it was produced by the rpc system.\n// Otherwise, it returns err.Error() or empty string when err is nil.\n//\n// Deprecated: use status.Convert and Message method instead.\nfunc ErrorDesc(err error) string {\n\treturn status.Convert(err).Message()\n}\n\n// Errorf returns an error containing an error code and a description;\n// Errorf returns nil if c is OK.\n//\n// Deprecated: use status.Errorf instead.\nfunc Errorf(c codes.Code, format string, a ...any) error {\n\treturn status.Errorf(c, format, a...)\n}\n\nvar errContextCanceled = status.Error(codes.Canceled, context.Canceled.Error())\nvar errContextDeadline = status.Error(codes.DeadlineExceeded, context.DeadlineExceeded.Error())\n\n// toRPCErr converts an error into an error from the status package.\nfunc toRPCErr(err error) error {\n\tswitch err {\n\tcase nil, io.EOF:\n\t\treturn err\n\tcase context.DeadlineExceeded:\n\t\treturn errContextDeadline\n\tcase context.Canceled:\n\t\treturn errContextCanceled\n\tcase io.ErrUnexpectedEOF:\n\t\treturn status.Error(codes.Internal, err.Error())\n\t}\n\n\tswitch e := err.(type) {\n\tcase transport.ConnectionError:\n\t\treturn status.Error(codes.Unavailable, e.Desc)\n\tcase *transport.NewStreamError:\n\t\treturn toRPCErr(e.Err)\n\t}\n\n\tif _, ok := status.FromError(err); ok {\n\t\treturn err\n\t}\n\n\treturn status.Error(codes.Unknown, err.Error())\n}\n\n// setCallInfoCodec should only be called after CallOptions have been applied.\nfunc setCallInfoCodec(c *callInfo) error {\n\tif c.codec != nil {\n\t\t// codec was already set by a CallOption; use it, but set the content\n\t\t// subtype if it is not set.\n\t\tif c.contentSubtype == \"\" {\n\t\t\t// c.codec is a baseCodec to hide the difference between grpc.Codec and\n\t\t\t// encoding.Codec (Name vs. String method name).  We only support\n\t\t\t// setting content subtype from encoding.Codec to avoid a behavior\n\t\t\t// change with the deprecated version.\n\t\t\tif ec, ok := c.codec.(encoding.CodecV2); ok {\n\t\t\t\tc.contentSubtype = strings.ToLower(ec.Name())\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\n\tif c.contentSubtype == \"\" {\n\t\t// No codec specified in CallOptions; use proto by default.\n\t\tc.codec = getCodec(proto.Name)\n\t\treturn nil\n\t}\n\n\t// c.contentSubtype is already lowercased in CallContentSubtype\n\tc.codec = getCodec(c.contentSubtype)\n\tif c.codec == nil {\n\t\treturn status.Errorf(codes.Internal, \"no codec registered for content-subtype %s\", c.contentSubtype)\n\t}\n\treturn nil\n}\n\n// The SupportPackageIsVersion variables are referenced from generated protocol\n// buffer files to ensure compatibility with the gRPC version used.  The latest\n// support package version is 9.\n//\n// Older versions are kept for compatibility.\n//\n// These constants should not be referenced from any other code.\nconst (\n\tSupportPackageIsVersion3 = true\n\tSupportPackageIsVersion4 = true\n\tSupportPackageIsVersion5 = true\n\tSupportPackageIsVersion6 = true\n\tSupportPackageIsVersion7 = true\n\tSupportPackageIsVersion8 = true\n\tSupportPackageIsVersion9 = true\n)\n\nconst grpcUA = \"grpc-go/\" + Version\n"
        },
        {
          "name": "rpc_util_test.go",
          "type": "blob",
          "size": 7.9501953125,
          "content": "/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"bytes\"\n\t\"compress/gzip\"\n\t\"io\"\n\t\"math\"\n\t\"reflect\"\n\t\"testing\"\n\n\t\"google.golang.org/grpc/codes\"\n\tprotoenc \"google.golang.org/grpc/encoding/proto\"\n\t\"google.golang.org/grpc/internal/testutils\"\n\t\"google.golang.org/grpc/internal/transport\"\n\t\"google.golang.org/grpc/mem\"\n\t\"google.golang.org/grpc/status\"\n\tperfpb \"google.golang.org/grpc/test/codec_perf\"\n\t\"google.golang.org/protobuf/proto\"\n)\n\ntype fullReader struct {\n\tdata []byte\n}\n\nfunc (f *fullReader) ReadMessageHeader(header []byte) error {\n\tbuf, err := f.Read(len(header))\n\tdefer buf.Free()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tbuf.CopyTo(header)\n\treturn nil\n}\n\nfunc (f *fullReader) Read(n int) (mem.BufferSlice, error) {\n\tif n == 0 {\n\t\treturn nil, nil\n\t}\n\n\tif len(f.data) == 0 {\n\t\treturn nil, io.EOF\n\t}\n\n\tif len(f.data) < n {\n\t\tdata := f.data\n\t\tf.data = nil\n\t\treturn mem.BufferSlice{mem.SliceBuffer(data)}, io.ErrUnexpectedEOF\n\t}\n\n\tbuf := f.data[:n]\n\tf.data = f.data[n:]\n\n\treturn mem.BufferSlice{mem.SliceBuffer(buf)}, nil\n}\n\nvar _ CallOption = EmptyCallOption{} // ensure EmptyCallOption implements the interface\n\nfunc (s) TestSimpleParsing(t *testing.T) {\n\tbigMsg := bytes.Repeat([]byte{'x'}, 1<<24)\n\tfor _, test := range []struct {\n\t\t// input\n\t\tp []byte\n\t\t// outputs\n\t\terr error\n\t\tb   []byte\n\t\tpt  payloadFormat\n\t}{\n\t\t{nil, io.EOF, nil, compressionNone},\n\t\t{[]byte{0, 0, 0, 0, 0}, nil, nil, compressionNone},\n\t\t{[]byte{0, 0, 0, 0, 1, 'a'}, nil, []byte{'a'}, compressionNone},\n\t\t{[]byte{1, 0}, io.ErrUnexpectedEOF, nil, compressionNone},\n\t\t{[]byte{0, 0, 0, 0, 10, 'a'}, io.ErrUnexpectedEOF, nil, compressionNone},\n\t\t// Check that messages with length >= 2^24 are parsed.\n\t\t{append([]byte{0, 1, 0, 0, 0}, bigMsg...), nil, bigMsg, compressionNone},\n\t} {\n\t\tbuf := &fullReader{test.p}\n\t\tparser := &parser{r: buf, bufferPool: mem.DefaultBufferPool()}\n\t\tpt, b, err := parser.recvMsg(math.MaxInt32)\n\t\tif err != test.err || !bytes.Equal(b.Materialize(), test.b) || pt != test.pt {\n\t\t\tt.Fatalf(\"parser{%v}.recvMsg(_) = %v, %v, %v\\nwant %v, %v, %v\", test.p, pt, b, err, test.pt, test.b, test.err)\n\t\t}\n\t}\n}\n\nfunc (s) TestMultipleParsing(t *testing.T) {\n\t// Set a byte stream consists of 3 messages with their headers.\n\tp := []byte{0, 0, 0, 0, 1, 'a', 0, 0, 0, 0, 2, 'b', 'c', 0, 0, 0, 0, 1, 'd'}\n\tb := &fullReader{p}\n\tparser := &parser{r: b, bufferPool: mem.DefaultBufferPool()}\n\n\twantRecvs := []struct {\n\t\tpt   payloadFormat\n\t\tdata []byte\n\t}{\n\t\t{compressionNone, []byte(\"a\")},\n\t\t{compressionNone, []byte(\"bc\")},\n\t\t{compressionNone, []byte(\"d\")},\n\t}\n\tfor i, want := range wantRecvs {\n\t\tpt, data, err := parser.recvMsg(math.MaxInt32)\n\t\tif err != nil || pt != want.pt || !reflect.DeepEqual(data.Materialize(), want.data) {\n\t\t\tt.Fatalf(\"after %d calls, parser{%v}.recvMsg(_) = %v, %v, %v\\nwant %v, %v, <nil>\",\n\t\t\t\ti, p, pt, data, err, want.pt, want.data)\n\t\t}\n\t}\n\n\tpt, data, err := parser.recvMsg(math.MaxInt32)\n\tif err != io.EOF {\n\t\tt.Fatalf(\"after %d recvMsgs calls, parser{%v}.recvMsg(_) = %v, %v, %v\\nwant _, _, %v\",\n\t\t\tlen(wantRecvs), p, pt, data, err, io.EOF)\n\t}\n}\n\nfunc (s) TestEncode(t *testing.T) {\n\tfor _, test := range []struct {\n\t\t// input\n\t\tmsg proto.Message\n\t\t// outputs\n\t\thdr  []byte\n\t\tdata []byte\n\t\terr  error\n\t}{\n\t\t{nil, []byte{0, 0, 0, 0, 0}, []byte{}, nil},\n\t} {\n\t\tdata, err := encode(getCodec(protoenc.Name), test.msg)\n\t\tif err != test.err || !bytes.Equal(data.Materialize(), test.data) {\n\t\t\tt.Errorf(\"encode(_, %v) = %v, %v; want %v, %v\", test.msg, data, err, test.data, test.err)\n\t\t\tcontinue\n\t\t}\n\t\tif hdr, _ := msgHeader(data, nil, compressionNone); !bytes.Equal(hdr, test.hdr) {\n\t\t\tt.Errorf(\"msgHeader(%v, false) = %v; want %v\", data, hdr, test.hdr)\n\t\t}\n\t}\n}\n\nfunc (s) TestCompress(t *testing.T) {\n\tbestCompressor, err := NewGZIPCompressorWithLevel(gzip.BestCompression)\n\tif err != nil {\n\t\tt.Fatalf(\"Could not initialize gzip compressor with best compression.\")\n\t}\n\tbestSpeedCompressor, err := NewGZIPCompressorWithLevel(gzip.BestSpeed)\n\tif err != nil {\n\t\tt.Fatalf(\"Could not initialize gzip compressor with best speed compression.\")\n\t}\n\n\tdefaultCompressor, err := NewGZIPCompressorWithLevel(gzip.BestSpeed)\n\tif err != nil {\n\t\tt.Fatalf(\"Could not initialize gzip compressor with default compression.\")\n\t}\n\n\tlevel5, err := NewGZIPCompressorWithLevel(5)\n\tif err != nil {\n\t\tt.Fatalf(\"Could not initialize gzip compressor with level 5 compression.\")\n\t}\n\n\tfor _, test := range []struct {\n\t\t// input\n\t\tdata []byte\n\t\tcp   Compressor\n\t\tdc   Decompressor\n\t\t// outputs\n\t\terr error\n\t}{\n\t\t{make([]byte, 1024), NewGZIPCompressor(), NewGZIPDecompressor(), nil},\n\t\t{make([]byte, 1024), bestCompressor, NewGZIPDecompressor(), nil},\n\t\t{make([]byte, 1024), bestSpeedCompressor, NewGZIPDecompressor(), nil},\n\t\t{make([]byte, 1024), defaultCompressor, NewGZIPDecompressor(), nil},\n\t\t{make([]byte, 1024), level5, NewGZIPDecompressor(), nil},\n\t} {\n\t\tb := new(bytes.Buffer)\n\t\tif err := test.cp.Do(b, test.data); err != test.err {\n\t\t\tt.Fatalf(\"Compressor.Do(_, %v) = %v, want %v\", test.data, err, test.err)\n\t\t}\n\t\tif b.Len() >= len(test.data) {\n\t\t\tt.Fatalf(\"The compressor fails to compress data.\")\n\t\t}\n\t\tif p, err := test.dc.Do(b); err != nil || !bytes.Equal(test.data, p) {\n\t\t\tt.Fatalf(\"Decompressor.Do(%v) = %v, %v, want %v, <nil>\", b, p, err, test.data)\n\t\t}\n\t}\n}\n\nfunc (s) TestToRPCErr(t *testing.T) {\n\tfor _, test := range []struct {\n\t\t// input\n\t\terrIn error\n\t\t// outputs\n\t\terrOut error\n\t}{\n\t\t{transport.ErrConnClosing, status.Error(codes.Unavailable, transport.ErrConnClosing.Desc)},\n\t\t{io.ErrUnexpectedEOF, status.Error(codes.Internal, io.ErrUnexpectedEOF.Error())},\n\t} {\n\t\terr := toRPCErr(test.errIn)\n\t\tif _, ok := status.FromError(err); !ok {\n\t\t\tt.Errorf(\"toRPCErr{%v} returned type %T, want %T\", test.errIn, err, status.Error)\n\t\t}\n\t\tif !testutils.StatusErrEqual(err, test.errOut) {\n\t\t\tt.Errorf(\"toRPCErr{%v} = %v \\nwant %v\", test.errIn, err, test.errOut)\n\t\t}\n\t}\n}\n\n// bmEncode benchmarks encoding a Protocol Buffer message containing mSize\n// bytes.\nfunc bmEncode(b *testing.B, mSize int) {\n\tcdc := getCodec(protoenc.Name)\n\tmsg := &perfpb.Buffer{Body: make([]byte, mSize)}\n\tencodeData, _ := encode(cdc, msg)\n\tencodedSz := int64(len(encodeData))\n\tb.ReportAllocs()\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tencode(cdc, msg)\n\t}\n\tb.SetBytes(encodedSz)\n}\n\nfunc BenchmarkEncode1B(b *testing.B) {\n\tbmEncode(b, 1)\n}\n\nfunc BenchmarkEncode1KiB(b *testing.B) {\n\tbmEncode(b, 1024)\n}\n\nfunc BenchmarkEncode8KiB(b *testing.B) {\n\tbmEncode(b, 8*1024)\n}\n\nfunc BenchmarkEncode64KiB(b *testing.B) {\n\tbmEncode(b, 64*1024)\n}\n\nfunc BenchmarkEncode512KiB(b *testing.B) {\n\tbmEncode(b, 512*1024)\n}\n\nfunc BenchmarkEncode1MiB(b *testing.B) {\n\tbmEncode(b, 1024*1024)\n}\n\n// bmCompressor benchmarks a compressor of a Protocol Buffer message containing\n// mSize bytes.\nfunc bmCompressor(b *testing.B, mSize int, cp Compressor) {\n\tpayload := make([]byte, mSize)\n\tcBuf := bytes.NewBuffer(make([]byte, mSize))\n\tb.ReportAllocs()\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tcp.Do(cBuf, payload)\n\t\tcBuf.Reset()\n\t}\n}\n\nfunc BenchmarkGZIPCompressor1B(b *testing.B) {\n\tbmCompressor(b, 1, NewGZIPCompressor())\n}\n\nfunc BenchmarkGZIPCompressor1KiB(b *testing.B) {\n\tbmCompressor(b, 1024, NewGZIPCompressor())\n}\n\nfunc BenchmarkGZIPCompressor8KiB(b *testing.B) {\n\tbmCompressor(b, 8*1024, NewGZIPCompressor())\n}\n\nfunc BenchmarkGZIPCompressor64KiB(b *testing.B) {\n\tbmCompressor(b, 64*1024, NewGZIPCompressor())\n}\n\nfunc BenchmarkGZIPCompressor512KiB(b *testing.B) {\n\tbmCompressor(b, 512*1024, NewGZIPCompressor())\n}\n\nfunc BenchmarkGZIPCompressor1MiB(b *testing.B) {\n\tbmCompressor(b, 1024*1024, NewGZIPCompressor())\n}\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "security",
          "type": "tree",
          "content": null
        },
        {
          "name": "server.go",
          "type": "blob",
          "size": 68.544921875,
          "content": "/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"net\"\n\t\"net/http\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/credentials\"\n\t\"google.golang.org/grpc/encoding\"\n\t\"google.golang.org/grpc/encoding/proto\"\n\t\"google.golang.org/grpc/grpclog\"\n\t\"google.golang.org/grpc/internal\"\n\t\"google.golang.org/grpc/internal/binarylog\"\n\t\"google.golang.org/grpc/internal/channelz\"\n\t\"google.golang.org/grpc/internal/grpcsync\"\n\t\"google.golang.org/grpc/internal/grpcutil\"\n\t\"google.golang.org/grpc/internal/transport\"\n\t\"google.golang.org/grpc/keepalive\"\n\t\"google.golang.org/grpc/mem\"\n\t\"google.golang.org/grpc/metadata\"\n\t\"google.golang.org/grpc/peer\"\n\t\"google.golang.org/grpc/stats\"\n\t\"google.golang.org/grpc/status\"\n\t\"google.golang.org/grpc/tap\"\n)\n\nconst (\n\tdefaultServerMaxReceiveMessageSize = 1024 * 1024 * 4\n\tdefaultServerMaxSendMessageSize    = math.MaxInt32\n\n\t// Server transports are tracked in a map which is keyed on listener\n\t// address. For regular gRPC traffic, connections are accepted in Serve()\n\t// through a call to Accept(), and we use the actual listener address as key\n\t// when we add it to the map. But for connections received through\n\t// ServeHTTP(), we do not have a listener and hence use this dummy value.\n\tlistenerAddressForServeHTTP = \"listenerAddressForServeHTTP\"\n)\n\nfunc init() {\n\tinternal.GetServerCredentials = func(srv *Server) credentials.TransportCredentials {\n\t\treturn srv.opts.creds\n\t}\n\tinternal.IsRegisteredMethod = func(srv *Server, method string) bool {\n\t\treturn srv.isRegisteredMethod(method)\n\t}\n\tinternal.ServerFromContext = serverFromContext\n\tinternal.AddGlobalServerOptions = func(opt ...ServerOption) {\n\t\tglobalServerOptions = append(globalServerOptions, opt...)\n\t}\n\tinternal.ClearGlobalServerOptions = func() {\n\t\tglobalServerOptions = nil\n\t}\n\tinternal.BinaryLogger = binaryLogger\n\tinternal.JoinServerOptions = newJoinServerOption\n\tinternal.BufferPool = bufferPool\n}\n\nvar statusOK = status.New(codes.OK, \"\")\nvar logger = grpclog.Component(\"core\")\n\n// MethodHandler is a function type that processes a unary RPC method call.\ntype MethodHandler func(srv any, ctx context.Context, dec func(any) error, interceptor UnaryServerInterceptor) (any, error)\n\n// MethodDesc represents an RPC service's method specification.\ntype MethodDesc struct {\n\tMethodName string\n\tHandler    MethodHandler\n}\n\n// ServiceDesc represents an RPC service's specification.\ntype ServiceDesc struct {\n\tServiceName string\n\t// The pointer to the service interface. Used to check whether the user\n\t// provided implementation satisfies the interface requirements.\n\tHandlerType any\n\tMethods     []MethodDesc\n\tStreams     []StreamDesc\n\tMetadata    any\n}\n\n// serviceInfo wraps information about a service. It is very similar to\n// ServiceDesc and is constructed from it for internal purposes.\ntype serviceInfo struct {\n\t// Contains the implementation for the methods in this service.\n\tserviceImpl any\n\tmethods     map[string]*MethodDesc\n\tstreams     map[string]*StreamDesc\n\tmdata       any\n}\n\n// Server is a gRPC server to serve RPC requests.\ntype Server struct {\n\topts serverOptions\n\n\tmu  sync.Mutex // guards following\n\tlis map[net.Listener]bool\n\t// conns contains all active server transports. It is a map keyed on a\n\t// listener address with the value being the set of active transports\n\t// belonging to that listener.\n\tconns    map[string]map[transport.ServerTransport]bool\n\tserve    bool\n\tdrain    bool\n\tcv       *sync.Cond              // signaled when connections close for GracefulStop\n\tservices map[string]*serviceInfo // service name -> service info\n\tevents   traceEventLog\n\n\tquit               *grpcsync.Event\n\tdone               *grpcsync.Event\n\tchannelzRemoveOnce sync.Once\n\tserveWG            sync.WaitGroup // counts active Serve goroutines for Stop/GracefulStop\n\thandlersWG         sync.WaitGroup // counts active method handler goroutines\n\n\tchannelz *channelz.Server\n\n\tserverWorkerChannel      chan func()\n\tserverWorkerChannelClose func()\n}\n\ntype serverOptions struct {\n\tcreds                 credentials.TransportCredentials\n\tcodec                 baseCodec\n\tcp                    Compressor\n\tdc                    Decompressor\n\tunaryInt              UnaryServerInterceptor\n\tstreamInt             StreamServerInterceptor\n\tchainUnaryInts        []UnaryServerInterceptor\n\tchainStreamInts       []StreamServerInterceptor\n\tbinaryLogger          binarylog.Logger\n\tinTapHandle           tap.ServerInHandle\n\tstatsHandlers         []stats.Handler\n\tmaxConcurrentStreams  uint32\n\tmaxReceiveMessageSize int\n\tmaxSendMessageSize    int\n\tunknownStreamDesc     *StreamDesc\n\tkeepaliveParams       keepalive.ServerParameters\n\tkeepalivePolicy       keepalive.EnforcementPolicy\n\tinitialWindowSize     int32\n\tinitialConnWindowSize int32\n\twriteBufferSize       int\n\treadBufferSize        int\n\tsharedWriteBuffer     bool\n\tconnectionTimeout     time.Duration\n\tmaxHeaderListSize     *uint32\n\theaderTableSize       *uint32\n\tnumServerWorkers      uint32\n\tbufferPool            mem.BufferPool\n\twaitForHandlers       bool\n}\n\nvar defaultServerOptions = serverOptions{\n\tmaxConcurrentStreams:  math.MaxUint32,\n\tmaxReceiveMessageSize: defaultServerMaxReceiveMessageSize,\n\tmaxSendMessageSize:    defaultServerMaxSendMessageSize,\n\tconnectionTimeout:     120 * time.Second,\n\twriteBufferSize:       defaultWriteBufSize,\n\treadBufferSize:        defaultReadBufSize,\n\tbufferPool:            mem.DefaultBufferPool(),\n}\nvar globalServerOptions []ServerOption\n\n// A ServerOption sets options such as credentials, codec and keepalive parameters, etc.\ntype ServerOption interface {\n\tapply(*serverOptions)\n}\n\n// EmptyServerOption does not alter the server configuration. It can be embedded\n// in another structure to build custom server options.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype EmptyServerOption struct{}\n\nfunc (EmptyServerOption) apply(*serverOptions) {}\n\n// funcServerOption wraps a function that modifies serverOptions into an\n// implementation of the ServerOption interface.\ntype funcServerOption struct {\n\tf func(*serverOptions)\n}\n\nfunc (fdo *funcServerOption) apply(do *serverOptions) {\n\tfdo.f(do)\n}\n\nfunc newFuncServerOption(f func(*serverOptions)) *funcServerOption {\n\treturn &funcServerOption{\n\t\tf: f,\n\t}\n}\n\n// joinServerOption provides a way to combine arbitrary number of server\n// options into one.\ntype joinServerOption struct {\n\topts []ServerOption\n}\n\nfunc (mdo *joinServerOption) apply(do *serverOptions) {\n\tfor _, opt := range mdo.opts {\n\t\topt.apply(do)\n\t}\n}\n\nfunc newJoinServerOption(opts ...ServerOption) ServerOption {\n\treturn &joinServerOption{opts: opts}\n}\n\n// SharedWriteBuffer allows reusing per-connection transport write buffer.\n// If this option is set to true every connection will release the buffer after\n// flushing the data on the wire.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc SharedWriteBuffer(val bool) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.sharedWriteBuffer = val\n\t})\n}\n\n// WriteBufferSize determines how much data can be batched before doing a write\n// on the wire. The default value for this buffer is 32KB. Zero or negative\n// values will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\nfunc WriteBufferSize(s int) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.writeBufferSize = s\n\t})\n}\n\n// ReadBufferSize lets you set the size of read buffer, this determines how much\n// data can be read at most for one read syscall. The default value for this\n// buffer is 32KB. Zero or negative values will disable read buffer for a\n// connection so data framer can access the underlying conn directly.\nfunc ReadBufferSize(s int) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.readBufferSize = s\n\t})\n}\n\n// InitialWindowSize returns a ServerOption that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\nfunc InitialWindowSize(s int32) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.initialWindowSize = s\n\t})\n}\n\n// InitialConnWindowSize returns a ServerOption that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\nfunc InitialConnWindowSize(s int32) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.initialConnWindowSize = s\n\t})\n}\n\n// KeepaliveParams returns a ServerOption that sets keepalive and max-age parameters for the server.\nfunc KeepaliveParams(kp keepalive.ServerParameters) ServerOption {\n\tif kp.Time > 0 && kp.Time < internal.KeepaliveMinServerPingTime {\n\t\tlogger.Warning(\"Adjusting keepalive ping interval to minimum period of 1s\")\n\t\tkp.Time = internal.KeepaliveMinServerPingTime\n\t}\n\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.keepaliveParams = kp\n\t})\n}\n\n// KeepaliveEnforcementPolicy returns a ServerOption that sets keepalive enforcement policy for the server.\nfunc KeepaliveEnforcementPolicy(kep keepalive.EnforcementPolicy) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.keepalivePolicy = kep\n\t})\n}\n\n// CustomCodec returns a ServerOption that sets a codec for message marshaling and unmarshaling.\n//\n// This will override any lookups by content-subtype for Codecs registered with RegisterCodec.\n//\n// Deprecated: register codecs using encoding.RegisterCodec. The server will\n// automatically use registered codecs based on the incoming requests' headers.\n// See also\n// https://github.com/grpc/grpc-go/blob/master/Documentation/encoding.md#using-a-codec.\n// Will be supported throughout 1.x.\nfunc CustomCodec(codec Codec) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.codec = newCodecV0Bridge(codec)\n\t})\n}\n\n// ForceServerCodec returns a ServerOption that sets a codec for message\n// marshaling and unmarshaling.\n//\n// This will override any lookups by content-subtype for Codecs registered\n// with RegisterCodec.\n//\n// See Content-Type on\n// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests for\n// more details. Also see the documentation on RegisterCodec and\n// CallContentSubtype for more details on the interaction between encoding.Codec\n// and content-subtype.\n//\n// This function is provided for advanced users; prefer to register codecs\n// using encoding.RegisterCodec.\n// The server will automatically use registered codecs based on the incoming\n// requests' headers. See also\n// https://github.com/grpc/grpc-go/blob/master/Documentation/encoding.md#using-a-codec.\n// Will be supported throughout 1.x.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc ForceServerCodec(codec encoding.Codec) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.codec = newCodecV1Bridge(codec)\n\t})\n}\n\n// ForceServerCodecV2 is the equivalent of ForceServerCodec, but for the new\n// CodecV2 interface.\n//\n// Will be supported throughout 1.x.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc ForceServerCodecV2(codecV2 encoding.CodecV2) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.codec = codecV2\n\t})\n}\n\n// RPCCompressor returns a ServerOption that sets a compressor for outbound\n// messages.  For backward compatibility, all outbound messages will be sent\n// using this compressor, regardless of incoming message compression.  By\n// default, server messages will be sent using the same compressor with which\n// request messages were sent.\n//\n// Deprecated: use encoding.RegisterCompressor instead. Will be supported\n// throughout 1.x.\nfunc RPCCompressor(cp Compressor) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.cp = cp\n\t})\n}\n\n// RPCDecompressor returns a ServerOption that sets a decompressor for inbound\n// messages.  It has higher priority than decompressors registered via\n// encoding.RegisterCompressor.\n//\n// Deprecated: use encoding.RegisterCompressor instead. Will be supported\n// throughout 1.x.\nfunc RPCDecompressor(dc Decompressor) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.dc = dc\n\t})\n}\n\n// MaxMsgSize returns a ServerOption to set the max message size in bytes the server can receive.\n// If this is not set, gRPC uses the default limit.\n//\n// Deprecated: use MaxRecvMsgSize instead. Will be supported throughout 1.x.\nfunc MaxMsgSize(m int) ServerOption {\n\treturn MaxRecvMsgSize(m)\n}\n\n// MaxRecvMsgSize returns a ServerOption to set the max message size in bytes the server can receive.\n// If this is not set, gRPC uses the default 4MB.\nfunc MaxRecvMsgSize(m int) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.maxReceiveMessageSize = m\n\t})\n}\n\n// MaxSendMsgSize returns a ServerOption to set the max message size in bytes the server can send.\n// If this is not set, gRPC uses the default `math.MaxInt32`.\nfunc MaxSendMsgSize(m int) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.maxSendMessageSize = m\n\t})\n}\n\n// MaxConcurrentStreams returns a ServerOption that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\nfunc MaxConcurrentStreams(n uint32) ServerOption {\n\tif n == 0 {\n\t\tn = math.MaxUint32\n\t}\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.maxConcurrentStreams = n\n\t})\n}\n\n// Creds returns a ServerOption that sets credentials for server connections.\nfunc Creds(c credentials.TransportCredentials) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.creds = c\n\t})\n}\n\n// UnaryInterceptor returns a ServerOption that sets the UnaryServerInterceptor for the\n// server. Only one unary interceptor can be installed. The construction of multiple\n// interceptors (e.g., chaining) can be implemented at the caller.\nfunc UnaryInterceptor(i UnaryServerInterceptor) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\tif o.unaryInt != nil {\n\t\t\tpanic(\"The unary server interceptor was already set and may not be reset.\")\n\t\t}\n\t\to.unaryInt = i\n\t})\n}\n\n// ChainUnaryInterceptor returns a ServerOption that specifies the chained interceptor\n// for unary RPCs. The first interceptor will be the outer most,\n// while the last interceptor will be the inner most wrapper around the real call.\n// All unary interceptors added by this method will be chained.\nfunc ChainUnaryInterceptor(interceptors ...UnaryServerInterceptor) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.chainUnaryInts = append(o.chainUnaryInts, interceptors...)\n\t})\n}\n\n// StreamInterceptor returns a ServerOption that sets the StreamServerInterceptor for the\n// server. Only one stream interceptor can be installed.\nfunc StreamInterceptor(i StreamServerInterceptor) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\tif o.streamInt != nil {\n\t\t\tpanic(\"The stream server interceptor was already set and may not be reset.\")\n\t\t}\n\t\to.streamInt = i\n\t})\n}\n\n// ChainStreamInterceptor returns a ServerOption that specifies the chained interceptor\n// for streaming RPCs. The first interceptor will be the outer most,\n// while the last interceptor will be the inner most wrapper around the real call.\n// All stream interceptors added by this method will be chained.\nfunc ChainStreamInterceptor(interceptors ...StreamServerInterceptor) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.chainStreamInts = append(o.chainStreamInts, interceptors...)\n\t})\n}\n\n// InTapHandle returns a ServerOption that sets the tap handle for all the server\n// transport to be created. Only one can be installed.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc InTapHandle(h tap.ServerInHandle) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\tif o.inTapHandle != nil {\n\t\t\tpanic(\"The tap handle was already set and may not be reset.\")\n\t\t}\n\t\to.inTapHandle = h\n\t})\n}\n\n// StatsHandler returns a ServerOption that sets the stats handler for the server.\nfunc StatsHandler(h stats.Handler) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\tif h == nil {\n\t\t\tlogger.Error(\"ignoring nil parameter in grpc.StatsHandler ServerOption\")\n\t\t\t// Do not allow a nil stats handler, which would otherwise cause\n\t\t\t// panics.\n\t\t\treturn\n\t\t}\n\t\to.statsHandlers = append(o.statsHandlers, h)\n\t})\n}\n\n// binaryLogger returns a ServerOption that can set the binary logger for the\n// server.\nfunc binaryLogger(bl binarylog.Logger) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.binaryLogger = bl\n\t})\n}\n\n// UnknownServiceHandler returns a ServerOption that allows for adding a custom\n// unknown service handler. The provided method is a bidi-streaming RPC service\n// handler that will be invoked instead of returning the \"unimplemented\" gRPC\n// error whenever a request is received for an unregistered service or method.\n// The handling function and stream interceptor (if set) have full access to\n// the ServerStream, including its Context.\nfunc UnknownServiceHandler(streamHandler StreamHandler) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.unknownStreamDesc = &StreamDesc{\n\t\t\tStreamName: \"unknown_service_handler\",\n\t\t\tHandler:    streamHandler,\n\t\t\t// We need to assume that the users of the streamHandler will want to use both.\n\t\t\tClientStreams: true,\n\t\t\tServerStreams: true,\n\t\t}\n\t})\n}\n\n// ConnectionTimeout returns a ServerOption that sets the timeout for\n// connection establishment (up to and including HTTP/2 handshaking) for all\n// new connections.  If this is not set, the default is 120 seconds.  A zero or\n// negative value will result in an immediate timeout.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc ConnectionTimeout(d time.Duration) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.connectionTimeout = d\n\t})\n}\n\n// MaxHeaderListSizeServerOption is a ServerOption that sets the max\n// (uncompressed) size of header list that the server is prepared to accept.\ntype MaxHeaderListSizeServerOption struct {\n\tMaxHeaderListSize uint32\n}\n\nfunc (o MaxHeaderListSizeServerOption) apply(so *serverOptions) {\n\tso.maxHeaderListSize = &o.MaxHeaderListSize\n}\n\n// MaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\nfunc MaxHeaderListSize(s uint32) ServerOption {\n\treturn MaxHeaderListSizeServerOption{\n\t\tMaxHeaderListSize: s,\n\t}\n}\n\n// HeaderTableSize returns a ServerOption that sets the size of dynamic\n// header table for stream.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc HeaderTableSize(s uint32) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.headerTableSize = &s\n\t})\n}\n\n// NumStreamWorkers returns a ServerOption that sets the number of worker\n// goroutines that should be used to process incoming streams. Setting this to\n// zero (default) will disable workers and spawn a new goroutine for each\n// stream.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc NumStreamWorkers(numServerWorkers uint32) ServerOption {\n\t// TODO: If/when this API gets stabilized (i.e. stream workers become the\n\t// only way streams are processed), change the behavior of the zero value to\n\t// a sane default. Preliminary experiments suggest that a value equal to the\n\t// number of CPUs available is most performant; requires thorough testing.\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.numServerWorkers = numServerWorkers\n\t})\n}\n\n// WaitForHandlers cause Stop to wait until all outstanding method handlers have\n// exited before returning.  If false, Stop will return as soon as all\n// connections have closed, but method handlers may still be running. By\n// default, Stop does not wait for method handlers to return.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc WaitForHandlers(w bool) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.waitForHandlers = w\n\t})\n}\n\nfunc bufferPool(bufferPool mem.BufferPool) ServerOption {\n\treturn newFuncServerOption(func(o *serverOptions) {\n\t\to.bufferPool = bufferPool\n\t})\n}\n\n// serverWorkerResetThreshold defines how often the stack must be reset. Every\n// N requests, by spawning a new goroutine in its place, a worker can reset its\n// stack so that large stacks don't live in memory forever. 2^16 should allow\n// each goroutine stack to live for at least a few seconds in a typical\n// workload (assuming a QPS of a few thousand requests/sec).\nconst serverWorkerResetThreshold = 1 << 16\n\n// serverWorker blocks on a *transport.ServerStream channel forever and waits\n// for data to be fed by serveStreams. This allows multiple requests to be\n// processed by the same goroutine, removing the need for expensive stack\n// re-allocations (see the runtime.morestack problem [1]).\n//\n// [1] https://github.com/golang/go/issues/18138\nfunc (s *Server) serverWorker() {\n\tfor completed := 0; completed < serverWorkerResetThreshold; completed++ {\n\t\tf, ok := <-s.serverWorkerChannel\n\t\tif !ok {\n\t\t\treturn\n\t\t}\n\t\tf()\n\t}\n\tgo s.serverWorker()\n}\n\n// initServerWorkers creates worker goroutines and a channel to process incoming\n// connections to reduce the time spent overall on runtime.morestack.\nfunc (s *Server) initServerWorkers() {\n\ts.serverWorkerChannel = make(chan func())\n\ts.serverWorkerChannelClose = grpcsync.OnceFunc(func() {\n\t\tclose(s.serverWorkerChannel)\n\t})\n\tfor i := uint32(0); i < s.opts.numServerWorkers; i++ {\n\t\tgo s.serverWorker()\n\t}\n}\n\n// NewServer creates a gRPC server which has no service registered and has not\n// started to accept requests yet.\nfunc NewServer(opt ...ServerOption) *Server {\n\topts := defaultServerOptions\n\tfor _, o := range globalServerOptions {\n\t\to.apply(&opts)\n\t}\n\tfor _, o := range opt {\n\t\to.apply(&opts)\n\t}\n\ts := &Server{\n\t\tlis:      make(map[net.Listener]bool),\n\t\topts:     opts,\n\t\tconns:    make(map[string]map[transport.ServerTransport]bool),\n\t\tservices: make(map[string]*serviceInfo),\n\t\tquit:     grpcsync.NewEvent(),\n\t\tdone:     grpcsync.NewEvent(),\n\t\tchannelz: channelz.RegisterServer(\"\"),\n\t}\n\tchainUnaryServerInterceptors(s)\n\tchainStreamServerInterceptors(s)\n\ts.cv = sync.NewCond(&s.mu)\n\tif EnableTracing {\n\t\t_, file, line, _ := runtime.Caller(1)\n\t\ts.events = newTraceEventLog(\"grpc.Server\", fmt.Sprintf(\"%s:%d\", file, line))\n\t}\n\n\tif s.opts.numServerWorkers > 0 {\n\t\ts.initServerWorkers()\n\t}\n\n\tchannelz.Info(logger, s.channelz, \"Server created\")\n\treturn s\n}\n\n// printf records an event in s's event log, unless s has been stopped.\n// REQUIRES s.mu is held.\nfunc (s *Server) printf(format string, a ...any) {\n\tif s.events != nil {\n\t\ts.events.Printf(format, a...)\n\t}\n}\n\n// errorf records an error in s's event log, unless s has been stopped.\n// REQUIRES s.mu is held.\nfunc (s *Server) errorf(format string, a ...any) {\n\tif s.events != nil {\n\t\ts.events.Errorf(format, a...)\n\t}\n}\n\n// ServiceRegistrar wraps a single method that supports service registration. It\n// enables users to pass concrete types other than grpc.Server to the service\n// registration methods exported by the IDL generated code.\ntype ServiceRegistrar interface {\n\t// RegisterService registers a service and its implementation to the\n\t// concrete type implementing this interface.  It may not be called\n\t// once the server has started serving.\n\t// desc describes the service and its methods and handlers. impl is the\n\t// service implementation which is passed to the method handlers.\n\tRegisterService(desc *ServiceDesc, impl any)\n}\n\n// RegisterService registers a service and its implementation to the gRPC\n// server. It is called from the IDL generated code. This must be called before\n// invoking Serve. If ss is non-nil (for legacy code), its type is checked to\n// ensure it implements sd.HandlerType.\nfunc (s *Server) RegisterService(sd *ServiceDesc, ss any) {\n\tif ss != nil {\n\t\tht := reflect.TypeOf(sd.HandlerType).Elem()\n\t\tst := reflect.TypeOf(ss)\n\t\tif !st.Implements(ht) {\n\t\t\tlogger.Fatalf(\"grpc: Server.RegisterService found the handler of type %v that does not satisfy %v\", st, ht)\n\t\t}\n\t}\n\ts.register(sd, ss)\n}\n\nfunc (s *Server) register(sd *ServiceDesc, ss any) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\ts.printf(\"RegisterService(%q)\", sd.ServiceName)\n\tif s.serve {\n\t\tlogger.Fatalf(\"grpc: Server.RegisterService after Server.Serve for %q\", sd.ServiceName)\n\t}\n\tif _, ok := s.services[sd.ServiceName]; ok {\n\t\tlogger.Fatalf(\"grpc: Server.RegisterService found duplicate service registration for %q\", sd.ServiceName)\n\t}\n\tinfo := &serviceInfo{\n\t\tserviceImpl: ss,\n\t\tmethods:     make(map[string]*MethodDesc),\n\t\tstreams:     make(map[string]*StreamDesc),\n\t\tmdata:       sd.Metadata,\n\t}\n\tfor i := range sd.Methods {\n\t\td := &sd.Methods[i]\n\t\tinfo.methods[d.MethodName] = d\n\t}\n\tfor i := range sd.Streams {\n\t\td := &sd.Streams[i]\n\t\tinfo.streams[d.StreamName] = d\n\t}\n\ts.services[sd.ServiceName] = info\n}\n\n// MethodInfo contains the information of an RPC including its method name and type.\ntype MethodInfo struct {\n\t// Name is the method name only, without the service name or package name.\n\tName string\n\t// IsClientStream indicates whether the RPC is a client streaming RPC.\n\tIsClientStream bool\n\t// IsServerStream indicates whether the RPC is a server streaming RPC.\n\tIsServerStream bool\n}\n\n// ServiceInfo contains unary RPC method info, streaming RPC method info and metadata for a service.\ntype ServiceInfo struct {\n\tMethods []MethodInfo\n\t// Metadata is the metadata specified in ServiceDesc when registering service.\n\tMetadata any\n}\n\n// GetServiceInfo returns a map from service names to ServiceInfo.\n// Service names include the package names, in the form of <package>.<service>.\nfunc (s *Server) GetServiceInfo() map[string]ServiceInfo {\n\tret := make(map[string]ServiceInfo)\n\tfor n, srv := range s.services {\n\t\tmethods := make([]MethodInfo, 0, len(srv.methods)+len(srv.streams))\n\t\tfor m := range srv.methods {\n\t\t\tmethods = append(methods, MethodInfo{\n\t\t\t\tName:           m,\n\t\t\t\tIsClientStream: false,\n\t\t\t\tIsServerStream: false,\n\t\t\t})\n\t\t}\n\t\tfor m, d := range srv.streams {\n\t\t\tmethods = append(methods, MethodInfo{\n\t\t\t\tName:           m,\n\t\t\t\tIsClientStream: d.ClientStreams,\n\t\t\t\tIsServerStream: d.ServerStreams,\n\t\t\t})\n\t\t}\n\n\t\tret[n] = ServiceInfo{\n\t\t\tMethods:  methods,\n\t\t\tMetadata: srv.mdata,\n\t\t}\n\t}\n\treturn ret\n}\n\n// ErrServerStopped indicates that the operation is now illegal because of\n// the server being stopped.\nvar ErrServerStopped = errors.New(\"grpc: the server has been stopped\")\n\ntype listenSocket struct {\n\tnet.Listener\n\tchannelz *channelz.Socket\n}\n\nfunc (l *listenSocket) Close() error {\n\terr := l.Listener.Close()\n\tchannelz.RemoveEntry(l.channelz.ID)\n\tchannelz.Info(logger, l.channelz, \"ListenSocket deleted\")\n\treturn err\n}\n\n// Serve accepts incoming connections on the listener lis, creating a new\n// ServerTransport and service goroutine for each. The service goroutines\n// read gRPC requests and then call the registered handlers to reply to them.\n// Serve returns when lis.Accept fails with fatal errors.  lis will be closed when\n// this method returns.\n// Serve will return a non-nil error unless Stop or GracefulStop is called.\n//\n// Note: All supported releases of Go (as of December 2023) override the OS\n// defaults for TCP keepalive time and interval to 15s. To enable TCP keepalive\n// with OS defaults for keepalive time and interval, callers need to do the\n// following two things:\n//   - pass a net.Listener created by calling the Listen method on a\n//     net.ListenConfig with the `KeepAlive` field set to a negative value. This\n//     will result in the Go standard library not overriding OS defaults for TCP\n//     keepalive interval and time. But this will also result in the Go standard\n//     library not enabling TCP keepalives by default.\n//   - override the Accept method on the passed in net.Listener and set the\n//     SO_KEEPALIVE socket option to enable TCP keepalives, with OS defaults.\nfunc (s *Server) Serve(lis net.Listener) error {\n\ts.mu.Lock()\n\ts.printf(\"serving\")\n\ts.serve = true\n\tif s.lis == nil {\n\t\t// Serve called after Stop or GracefulStop.\n\t\ts.mu.Unlock()\n\t\tlis.Close()\n\t\treturn ErrServerStopped\n\t}\n\n\ts.serveWG.Add(1)\n\tdefer func() {\n\t\ts.serveWG.Done()\n\t\tif s.quit.HasFired() {\n\t\t\t// Stop or GracefulStop called; block until done and return nil.\n\t\t\t<-s.done.Done()\n\t\t}\n\t}()\n\n\tls := &listenSocket{\n\t\tListener: lis,\n\t\tchannelz: channelz.RegisterSocket(&channelz.Socket{\n\t\t\tSocketType:    channelz.SocketTypeListen,\n\t\t\tParent:        s.channelz,\n\t\t\tRefName:       lis.Addr().String(),\n\t\t\tLocalAddr:     lis.Addr(),\n\t\t\tSocketOptions: channelz.GetSocketOption(lis)},\n\t\t),\n\t}\n\ts.lis[ls] = true\n\n\tdefer func() {\n\t\ts.mu.Lock()\n\t\tif s.lis != nil && s.lis[ls] {\n\t\t\tls.Close()\n\t\t\tdelete(s.lis, ls)\n\t\t}\n\t\ts.mu.Unlock()\n\t}()\n\n\ts.mu.Unlock()\n\tchannelz.Info(logger, ls.channelz, \"ListenSocket created\")\n\n\tvar tempDelay time.Duration // how long to sleep on accept failure\n\tfor {\n\t\trawConn, err := lis.Accept()\n\t\tif err != nil {\n\t\t\tif ne, ok := err.(interface {\n\t\t\t\tTemporary() bool\n\t\t\t}); ok && ne.Temporary() {\n\t\t\t\tif tempDelay == 0 {\n\t\t\t\t\ttempDelay = 5 * time.Millisecond\n\t\t\t\t} else {\n\t\t\t\t\ttempDelay *= 2\n\t\t\t\t}\n\t\t\t\tif max := 1 * time.Second; tempDelay > max {\n\t\t\t\t\ttempDelay = max\n\t\t\t\t}\n\t\t\t\ts.mu.Lock()\n\t\t\t\ts.printf(\"Accept error: %v; retrying in %v\", err, tempDelay)\n\t\t\t\ts.mu.Unlock()\n\t\t\t\ttimer := time.NewTimer(tempDelay)\n\t\t\t\tselect {\n\t\t\t\tcase <-timer.C:\n\t\t\t\tcase <-s.quit.Done():\n\t\t\t\t\ttimer.Stop()\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ts.mu.Lock()\n\t\t\ts.printf(\"done serving; Accept = %v\", err)\n\t\t\ts.mu.Unlock()\n\n\t\t\tif s.quit.HasFired() {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\ttempDelay = 0\n\t\t// Start a new goroutine to deal with rawConn so we don't stall this Accept\n\t\t// loop goroutine.\n\t\t//\n\t\t// Make sure we account for the goroutine so GracefulStop doesn't nil out\n\t\t// s.conns before this conn can be added.\n\t\ts.serveWG.Add(1)\n\t\tgo func() {\n\t\t\ts.handleRawConn(lis.Addr().String(), rawConn)\n\t\t\ts.serveWG.Done()\n\t\t}()\n\t}\n}\n\n// handleRawConn forks a goroutine to handle a just-accepted connection that\n// has not had any I/O performed on it yet.\nfunc (s *Server) handleRawConn(lisAddr string, rawConn net.Conn) {\n\tif s.quit.HasFired() {\n\t\trawConn.Close()\n\t\treturn\n\t}\n\trawConn.SetDeadline(time.Now().Add(s.opts.connectionTimeout))\n\n\t// Finish handshaking (HTTP2)\n\tst := s.newHTTP2Transport(rawConn)\n\trawConn.SetDeadline(time.Time{})\n\tif st == nil {\n\t\treturn\n\t}\n\n\tif cc, ok := rawConn.(interface {\n\t\tPassServerTransport(transport.ServerTransport)\n\t}); ok {\n\t\tcc.PassServerTransport(st)\n\t}\n\n\tif !s.addConn(lisAddr, st) {\n\t\treturn\n\t}\n\tgo func() {\n\t\ts.serveStreams(context.Background(), st, rawConn)\n\t\ts.removeConn(lisAddr, st)\n\t}()\n}\n\n// newHTTP2Transport sets up a http/2 transport (using the\n// gRPC http2 server transport in transport/http2_server.go).\nfunc (s *Server) newHTTP2Transport(c net.Conn) transport.ServerTransport {\n\tconfig := &transport.ServerConfig{\n\t\tMaxStreams:            s.opts.maxConcurrentStreams,\n\t\tConnectionTimeout:     s.opts.connectionTimeout,\n\t\tCredentials:           s.opts.creds,\n\t\tInTapHandle:           s.opts.inTapHandle,\n\t\tStatsHandlers:         s.opts.statsHandlers,\n\t\tKeepaliveParams:       s.opts.keepaliveParams,\n\t\tKeepalivePolicy:       s.opts.keepalivePolicy,\n\t\tInitialWindowSize:     s.opts.initialWindowSize,\n\t\tInitialConnWindowSize: s.opts.initialConnWindowSize,\n\t\tWriteBufferSize:       s.opts.writeBufferSize,\n\t\tReadBufferSize:        s.opts.readBufferSize,\n\t\tSharedWriteBuffer:     s.opts.sharedWriteBuffer,\n\t\tChannelzParent:        s.channelz,\n\t\tMaxHeaderListSize:     s.opts.maxHeaderListSize,\n\t\tHeaderTableSize:       s.opts.headerTableSize,\n\t\tBufferPool:            s.opts.bufferPool,\n\t}\n\tst, err := transport.NewServerTransport(c, config)\n\tif err != nil {\n\t\ts.mu.Lock()\n\t\ts.errorf(\"NewServerTransport(%q) failed: %v\", c.RemoteAddr(), err)\n\t\ts.mu.Unlock()\n\t\t// ErrConnDispatched means that the connection was dispatched away from\n\t\t// gRPC; those connections should be left open.\n\t\tif err != credentials.ErrConnDispatched {\n\t\t\t// Don't log on ErrConnDispatched and io.EOF to prevent log spam.\n\t\t\tif err != io.EOF {\n\t\t\t\tchannelz.Info(logger, s.channelz, \"grpc: Server.Serve failed to create ServerTransport: \", err)\n\t\t\t}\n\t\t\tc.Close()\n\t\t}\n\t\treturn nil\n\t}\n\n\treturn st\n}\n\nfunc (s *Server) serveStreams(ctx context.Context, st transport.ServerTransport, rawConn net.Conn) {\n\tctx = transport.SetConnection(ctx, rawConn)\n\tctx = peer.NewContext(ctx, st.Peer())\n\tfor _, sh := range s.opts.statsHandlers {\n\t\tctx = sh.TagConn(ctx, &stats.ConnTagInfo{\n\t\t\tRemoteAddr: st.Peer().Addr,\n\t\t\tLocalAddr:  st.Peer().LocalAddr,\n\t\t})\n\t\tsh.HandleConn(ctx, &stats.ConnBegin{})\n\t}\n\n\tdefer func() {\n\t\tst.Close(errors.New(\"finished serving streams for the server transport\"))\n\t\tfor _, sh := range s.opts.statsHandlers {\n\t\t\tsh.HandleConn(ctx, &stats.ConnEnd{})\n\t\t}\n\t}()\n\n\tstreamQuota := newHandlerQuota(s.opts.maxConcurrentStreams)\n\tst.HandleStreams(ctx, func(stream *transport.ServerStream) {\n\t\ts.handlersWG.Add(1)\n\t\tstreamQuota.acquire()\n\t\tf := func() {\n\t\t\tdefer streamQuota.release()\n\t\t\tdefer s.handlersWG.Done()\n\t\t\ts.handleStream(st, stream)\n\t\t}\n\n\t\tif s.opts.numServerWorkers > 0 {\n\t\t\tselect {\n\t\t\tcase s.serverWorkerChannel <- f:\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t\t// If all stream workers are busy, fallback to the default code path.\n\t\t\t}\n\t\t}\n\t\tgo f()\n\t})\n}\n\nvar _ http.Handler = (*Server)(nil)\n\n// ServeHTTP implements the Go standard library's http.Handler\n// interface by responding to the gRPC request r, by looking up\n// the requested gRPC method in the gRPC server s.\n//\n// The provided HTTP request must have arrived on an HTTP/2\n// connection. When using the Go standard library's server,\n// practically this means that the Request must also have arrived\n// over TLS.\n//\n// To share one port (such as 443 for https) between gRPC and an\n// existing http.Handler, use a root http.Handler such as:\n//\n//\tif r.ProtoMajor == 2 && strings.HasPrefix(\n//\t\tr.Header.Get(\"Content-Type\"), \"application/grpc\") {\n//\t\tgrpcServer.ServeHTTP(w, r)\n//\t} else {\n//\t\tyourMux.ServeHTTP(w, r)\n//\t}\n//\n// Note that ServeHTTP uses Go's HTTP/2 server implementation which is totally\n// separate from grpc-go's HTTP/2 server. Performance and features may vary\n// between the two paths. ServeHTTP does not support some gRPC features\n// available through grpc-go's HTTP/2 server.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc (s *Server) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n\tst, err := transport.NewServerHandlerTransport(w, r, s.opts.statsHandlers, s.opts.bufferPool)\n\tif err != nil {\n\t\t// Errors returned from transport.NewServerHandlerTransport have\n\t\t// already been written to w.\n\t\treturn\n\t}\n\tif !s.addConn(listenerAddressForServeHTTP, st) {\n\t\treturn\n\t}\n\tdefer s.removeConn(listenerAddressForServeHTTP, st)\n\ts.serveStreams(r.Context(), st, nil)\n}\n\nfunc (s *Server) addConn(addr string, st transport.ServerTransport) bool {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif s.conns == nil {\n\t\tst.Close(errors.New(\"Server.addConn called when server has already been stopped\"))\n\t\treturn false\n\t}\n\tif s.drain {\n\t\t// Transport added after we drained our existing conns: drain it\n\t\t// immediately.\n\t\tst.Drain(\"\")\n\t}\n\n\tif s.conns[addr] == nil {\n\t\t// Create a map entry if this is the first connection on this listener.\n\t\ts.conns[addr] = make(map[transport.ServerTransport]bool)\n\t}\n\ts.conns[addr][st] = true\n\treturn true\n}\n\nfunc (s *Server) removeConn(addr string, st transport.ServerTransport) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tconns := s.conns[addr]\n\tif conns != nil {\n\t\tdelete(conns, st)\n\t\tif len(conns) == 0 {\n\t\t\t// If the last connection for this address is being removed, also\n\t\t\t// remove the map entry corresponding to the address. This is used\n\t\t\t// in GracefulStop() when waiting for all connections to be closed.\n\t\t\tdelete(s.conns, addr)\n\t\t}\n\t\ts.cv.Broadcast()\n\t}\n}\n\nfunc (s *Server) incrCallsStarted() {\n\ts.channelz.ServerMetrics.CallsStarted.Add(1)\n\ts.channelz.ServerMetrics.LastCallStartedTimestamp.Store(time.Now().UnixNano())\n}\n\nfunc (s *Server) incrCallsSucceeded() {\n\ts.channelz.ServerMetrics.CallsSucceeded.Add(1)\n}\n\nfunc (s *Server) incrCallsFailed() {\n\ts.channelz.ServerMetrics.CallsFailed.Add(1)\n}\n\nfunc (s *Server) sendResponse(ctx context.Context, stream *transport.ServerStream, msg any, cp Compressor, opts *transport.WriteOptions, comp encoding.Compressor) error {\n\tdata, err := encode(s.getCodec(stream.ContentSubtype()), msg)\n\tif err != nil {\n\t\tchannelz.Error(logger, s.channelz, \"grpc: server failed to encode response: \", err)\n\t\treturn err\n\t}\n\n\tcompData, pf, err := compress(data, cp, comp, s.opts.bufferPool)\n\tif err != nil {\n\t\tdata.Free()\n\t\tchannelz.Error(logger, s.channelz, \"grpc: server failed to compress response: \", err)\n\t\treturn err\n\t}\n\n\thdr, payload := msgHeader(data, compData, pf)\n\n\tdefer func() {\n\t\tcompData.Free()\n\t\tdata.Free()\n\t\t// payload does not need to be freed here, it is either data or compData, both of\n\t\t// which are already freed.\n\t}()\n\n\tdataLen := data.Len()\n\tpayloadLen := payload.Len()\n\t// TODO(dfawley): should we be checking len(data) instead?\n\tif payloadLen > s.opts.maxSendMessageSize {\n\t\treturn status.Errorf(codes.ResourceExhausted, \"grpc: trying to send message larger than max (%d vs. %d)\", payloadLen, s.opts.maxSendMessageSize)\n\t}\n\terr = stream.Write(hdr, payload, opts)\n\tif err == nil {\n\t\tif len(s.opts.statsHandlers) != 0 {\n\t\t\tfor _, sh := range s.opts.statsHandlers {\n\t\t\t\tsh.HandleRPC(ctx, outPayload(false, msg, dataLen, payloadLen, time.Now()))\n\t\t\t}\n\t\t}\n\t}\n\treturn err\n}\n\n// chainUnaryServerInterceptors chains all unary server interceptors into one.\nfunc chainUnaryServerInterceptors(s *Server) {\n\t// Prepend opts.unaryInt to the chaining interceptors if it exists, since unaryInt will\n\t// be executed before any other chained interceptors.\n\tinterceptors := s.opts.chainUnaryInts\n\tif s.opts.unaryInt != nil {\n\t\tinterceptors = append([]UnaryServerInterceptor{s.opts.unaryInt}, s.opts.chainUnaryInts...)\n\t}\n\n\tvar chainedInt UnaryServerInterceptor\n\tif len(interceptors) == 0 {\n\t\tchainedInt = nil\n\t} else if len(interceptors) == 1 {\n\t\tchainedInt = interceptors[0]\n\t} else {\n\t\tchainedInt = chainUnaryInterceptors(interceptors)\n\t}\n\n\ts.opts.unaryInt = chainedInt\n}\n\nfunc chainUnaryInterceptors(interceptors []UnaryServerInterceptor) UnaryServerInterceptor {\n\treturn func(ctx context.Context, req any, info *UnaryServerInfo, handler UnaryHandler) (any, error) {\n\t\treturn interceptors[0](ctx, req, info, getChainUnaryHandler(interceptors, 0, info, handler))\n\t}\n}\n\nfunc getChainUnaryHandler(interceptors []UnaryServerInterceptor, curr int, info *UnaryServerInfo, finalHandler UnaryHandler) UnaryHandler {\n\tif curr == len(interceptors)-1 {\n\t\treturn finalHandler\n\t}\n\treturn func(ctx context.Context, req any) (any, error) {\n\t\treturn interceptors[curr+1](ctx, req, info, getChainUnaryHandler(interceptors, curr+1, info, finalHandler))\n\t}\n}\n\nfunc (s *Server) processUnaryRPC(ctx context.Context, stream *transport.ServerStream, info *serviceInfo, md *MethodDesc, trInfo *traceInfo) (err error) {\n\tshs := s.opts.statsHandlers\n\tif len(shs) != 0 || trInfo != nil || channelz.IsOn() {\n\t\tif channelz.IsOn() {\n\t\t\ts.incrCallsStarted()\n\t\t}\n\t\tvar statsBegin *stats.Begin\n\t\tfor _, sh := range shs {\n\t\t\tbeginTime := time.Now()\n\t\t\tstatsBegin = &stats.Begin{\n\t\t\t\tBeginTime:      beginTime,\n\t\t\t\tIsClientStream: false,\n\t\t\t\tIsServerStream: false,\n\t\t\t}\n\t\t\tsh.HandleRPC(ctx, statsBegin)\n\t\t}\n\t\tif trInfo != nil {\n\t\t\ttrInfo.tr.LazyLog(&trInfo.firstLine, false)\n\t\t}\n\t\t// The deferred error handling for tracing, stats handler and channelz are\n\t\t// combined into one function to reduce stack usage -- a defer takes ~56-64\n\t\t// bytes on the stack, so overflowing the stack will require a stack\n\t\t// re-allocation, which is expensive.\n\t\t//\n\t\t// To maintain behavior similar to separate deferred statements, statements\n\t\t// should be executed in the reverse order. That is, tracing first, stats\n\t\t// handler second, and channelz last. Note that panics *within* defers will\n\t\t// lead to different behavior, but that's an acceptable compromise; that\n\t\t// would be undefined behavior territory anyway.\n\t\tdefer func() {\n\t\t\tif trInfo != nil {\n\t\t\t\tif err != nil && err != io.EOF {\n\t\t\t\t\ttrInfo.tr.LazyLog(&fmtStringer{\"%v\", []any{err}}, true)\n\t\t\t\t\ttrInfo.tr.SetError()\n\t\t\t\t}\n\t\t\t\ttrInfo.tr.Finish()\n\t\t\t}\n\n\t\t\tfor _, sh := range shs {\n\t\t\t\tend := &stats.End{\n\t\t\t\t\tBeginTime: statsBegin.BeginTime,\n\t\t\t\t\tEndTime:   time.Now(),\n\t\t\t\t}\n\t\t\t\tif err != nil && err != io.EOF {\n\t\t\t\t\tend.Error = toRPCErr(err)\n\t\t\t\t}\n\t\t\t\tsh.HandleRPC(ctx, end)\n\t\t\t}\n\n\t\t\tif channelz.IsOn() {\n\t\t\t\tif err != nil && err != io.EOF {\n\t\t\t\t\ts.incrCallsFailed()\n\t\t\t\t} else {\n\t\t\t\t\ts.incrCallsSucceeded()\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n\tvar binlogs []binarylog.MethodLogger\n\tif ml := binarylog.GetMethodLogger(stream.Method()); ml != nil {\n\t\tbinlogs = append(binlogs, ml)\n\t}\n\tif s.opts.binaryLogger != nil {\n\t\tif ml := s.opts.binaryLogger.GetMethodLogger(stream.Method()); ml != nil {\n\t\t\tbinlogs = append(binlogs, ml)\n\t\t}\n\t}\n\tif len(binlogs) != 0 {\n\t\tmd, _ := metadata.FromIncomingContext(ctx)\n\t\tlogEntry := &binarylog.ClientHeader{\n\t\t\tHeader:     md,\n\t\t\tMethodName: stream.Method(),\n\t\t\tPeerAddr:   nil,\n\t\t}\n\t\tif deadline, ok := ctx.Deadline(); ok {\n\t\t\tlogEntry.Timeout = time.Until(deadline)\n\t\t\tif logEntry.Timeout < 0 {\n\t\t\t\tlogEntry.Timeout = 0\n\t\t\t}\n\t\t}\n\t\tif a := md[\":authority\"]; len(a) > 0 {\n\t\t\tlogEntry.Authority = a[0]\n\t\t}\n\t\tif peer, ok := peer.FromContext(ctx); ok {\n\t\t\tlogEntry.PeerAddr = peer.Addr\n\t\t}\n\t\tfor _, binlog := range binlogs {\n\t\t\tbinlog.Log(ctx, logEntry)\n\t\t}\n\t}\n\n\t// comp and cp are used for compression.  decomp and dc are used for\n\t// decompression.  If comp and decomp are both set, they are the same;\n\t// however they are kept separate to ensure that at most one of the\n\t// compressor/decompressor variable pairs are set for use later.\n\tvar comp, decomp encoding.Compressor\n\tvar cp Compressor\n\tvar dc Decompressor\n\tvar sendCompressorName string\n\n\t// If dc is set and matches the stream's compression, use it.  Otherwise, try\n\t// to find a matching registered compressor for decomp.\n\tif rc := stream.RecvCompress(); s.opts.dc != nil && s.opts.dc.Type() == rc {\n\t\tdc = s.opts.dc\n\t} else if rc != \"\" && rc != encoding.Identity {\n\t\tdecomp = encoding.GetCompressor(rc)\n\t\tif decomp == nil {\n\t\t\tst := status.Newf(codes.Unimplemented, \"grpc: Decompressor is not installed for grpc-encoding %q\", rc)\n\t\t\tstream.WriteStatus(st)\n\t\t\treturn st.Err()\n\t\t}\n\t}\n\n\t// If cp is set, use it.  Otherwise, attempt to compress the response using\n\t// the incoming message compression method.\n\t//\n\t// NOTE: this needs to be ahead of all handling, https://github.com/grpc/grpc-go/issues/686.\n\tif s.opts.cp != nil {\n\t\tcp = s.opts.cp\n\t\tsendCompressorName = cp.Type()\n\t} else if rc := stream.RecvCompress(); rc != \"\" && rc != encoding.Identity {\n\t\t// Legacy compressor not specified; attempt to respond with same encoding.\n\t\tcomp = encoding.GetCompressor(rc)\n\t\tif comp != nil {\n\t\t\tsendCompressorName = comp.Name()\n\t\t}\n\t}\n\n\tif sendCompressorName != \"\" {\n\t\tif err := stream.SetSendCompress(sendCompressorName); err != nil {\n\t\t\treturn status.Errorf(codes.Internal, \"grpc: failed to set send compressor: %v\", err)\n\t\t}\n\t}\n\n\tvar payInfo *payloadInfo\n\tif len(shs) != 0 || len(binlogs) != 0 {\n\t\tpayInfo = &payloadInfo{}\n\t\tdefer payInfo.free()\n\t}\n\n\td, err := recvAndDecompress(&parser{r: stream, bufferPool: s.opts.bufferPool}, stream, dc, s.opts.maxReceiveMessageSize, payInfo, decomp, true)\n\tif err != nil {\n\t\tif e := stream.WriteStatus(status.Convert(err)); e != nil {\n\t\t\tchannelz.Warningf(logger, s.channelz, \"grpc: Server.processUnaryRPC failed to write status: %v\", e)\n\t\t}\n\t\treturn err\n\t}\n\tdefer d.Free()\n\tdf := func(v any) error {\n\t\tif err := s.getCodec(stream.ContentSubtype()).Unmarshal(d, v); err != nil {\n\t\t\treturn status.Errorf(codes.Internal, \"grpc: error unmarshalling request: %v\", err)\n\t\t}\n\n\t\tfor _, sh := range shs {\n\t\t\tsh.HandleRPC(ctx, &stats.InPayload{\n\t\t\t\tRecvTime:         time.Now(),\n\t\t\t\tPayload:          v,\n\t\t\t\tLength:           d.Len(),\n\t\t\t\tWireLength:       payInfo.compressedLength + headerLen,\n\t\t\t\tCompressedLength: payInfo.compressedLength,\n\t\t\t})\n\t\t}\n\t\tif len(binlogs) != 0 {\n\t\t\tcm := &binarylog.ClientMessage{\n\t\t\t\tMessage: d.Materialize(),\n\t\t\t}\n\t\t\tfor _, binlog := range binlogs {\n\t\t\t\tbinlog.Log(ctx, cm)\n\t\t\t}\n\t\t}\n\t\tif trInfo != nil {\n\t\t\ttrInfo.tr.LazyLog(&payload{sent: false, msg: v}, true)\n\t\t}\n\t\treturn nil\n\t}\n\tctx = NewContextWithServerTransportStream(ctx, stream)\n\treply, appErr := md.Handler(info.serviceImpl, ctx, df, s.opts.unaryInt)\n\tif appErr != nil {\n\t\tappStatus, ok := status.FromError(appErr)\n\t\tif !ok {\n\t\t\t// Convert non-status application error to a status error with code\n\t\t\t// Unknown, but handle context errors specifically.\n\t\t\tappStatus = status.FromContextError(appErr)\n\t\t\tappErr = appStatus.Err()\n\t\t}\n\t\tif trInfo != nil {\n\t\t\ttrInfo.tr.LazyLog(stringer(appStatus.Message()), true)\n\t\t\ttrInfo.tr.SetError()\n\t\t}\n\t\tif e := stream.WriteStatus(appStatus); e != nil {\n\t\t\tchannelz.Warningf(logger, s.channelz, \"grpc: Server.processUnaryRPC failed to write status: %v\", e)\n\t\t}\n\t\tif len(binlogs) != 0 {\n\t\t\tif h, _ := stream.Header(); h.Len() > 0 {\n\t\t\t\t// Only log serverHeader if there was header. Otherwise it can\n\t\t\t\t// be trailer only.\n\t\t\t\tsh := &binarylog.ServerHeader{\n\t\t\t\t\tHeader: h,\n\t\t\t\t}\n\t\t\t\tfor _, binlog := range binlogs {\n\t\t\t\t\tbinlog.Log(ctx, sh)\n\t\t\t\t}\n\t\t\t}\n\t\t\tst := &binarylog.ServerTrailer{\n\t\t\t\tTrailer: stream.Trailer(),\n\t\t\t\tErr:     appErr,\n\t\t\t}\n\t\t\tfor _, binlog := range binlogs {\n\t\t\t\tbinlog.Log(ctx, st)\n\t\t\t}\n\t\t}\n\t\treturn appErr\n\t}\n\tif trInfo != nil {\n\t\ttrInfo.tr.LazyLog(stringer(\"OK\"), false)\n\t}\n\topts := &transport.WriteOptions{Last: true}\n\n\t// Server handler could have set new compressor by calling SetSendCompressor.\n\t// In case it is set, we need to use it for compressing outbound message.\n\tif stream.SendCompress() != sendCompressorName {\n\t\tcomp = encoding.GetCompressor(stream.SendCompress())\n\t}\n\tif err := s.sendResponse(ctx, stream, reply, cp, opts, comp); err != nil {\n\t\tif err == io.EOF {\n\t\t\t// The entire stream is done (for unary RPC only).\n\t\t\treturn err\n\t\t}\n\t\tif sts, ok := status.FromError(err); ok {\n\t\t\tif e := stream.WriteStatus(sts); e != nil {\n\t\t\t\tchannelz.Warningf(logger, s.channelz, \"grpc: Server.processUnaryRPC failed to write status: %v\", e)\n\t\t\t}\n\t\t} else {\n\t\t\tswitch st := err.(type) {\n\t\t\tcase transport.ConnectionError:\n\t\t\t\t// Nothing to do here.\n\t\t\tdefault:\n\t\t\t\tpanic(fmt.Sprintf(\"grpc: Unexpected error (%T) from sendResponse: %v\", st, st))\n\t\t\t}\n\t\t}\n\t\tif len(binlogs) != 0 {\n\t\t\th, _ := stream.Header()\n\t\t\tsh := &binarylog.ServerHeader{\n\t\t\t\tHeader: h,\n\t\t\t}\n\t\t\tst := &binarylog.ServerTrailer{\n\t\t\t\tTrailer: stream.Trailer(),\n\t\t\t\tErr:     appErr,\n\t\t\t}\n\t\t\tfor _, binlog := range binlogs {\n\t\t\t\tbinlog.Log(ctx, sh)\n\t\t\t\tbinlog.Log(ctx, st)\n\t\t\t}\n\t\t}\n\t\treturn err\n\t}\n\tif len(binlogs) != 0 {\n\t\th, _ := stream.Header()\n\t\tsh := &binarylog.ServerHeader{\n\t\t\tHeader: h,\n\t\t}\n\t\tsm := &binarylog.ServerMessage{\n\t\t\tMessage: reply,\n\t\t}\n\t\tfor _, binlog := range binlogs {\n\t\t\tbinlog.Log(ctx, sh)\n\t\t\tbinlog.Log(ctx, sm)\n\t\t}\n\t}\n\tif trInfo != nil {\n\t\ttrInfo.tr.LazyLog(&payload{sent: true, msg: reply}, true)\n\t}\n\t// TODO: Should we be logging if writing status failed here, like above?\n\t// Should the logging be in WriteStatus?  Should we ignore the WriteStatus\n\t// error or allow the stats handler to see it?\n\tif len(binlogs) != 0 {\n\t\tst := &binarylog.ServerTrailer{\n\t\t\tTrailer: stream.Trailer(),\n\t\t\tErr:     appErr,\n\t\t}\n\t\tfor _, binlog := range binlogs {\n\t\t\tbinlog.Log(ctx, st)\n\t\t}\n\t}\n\treturn stream.WriteStatus(statusOK)\n}\n\n// chainStreamServerInterceptors chains all stream server interceptors into one.\nfunc chainStreamServerInterceptors(s *Server) {\n\t// Prepend opts.streamInt to the chaining interceptors if it exists, since streamInt will\n\t// be executed before any other chained interceptors.\n\tinterceptors := s.opts.chainStreamInts\n\tif s.opts.streamInt != nil {\n\t\tinterceptors = append([]StreamServerInterceptor{s.opts.streamInt}, s.opts.chainStreamInts...)\n\t}\n\n\tvar chainedInt StreamServerInterceptor\n\tif len(interceptors) == 0 {\n\t\tchainedInt = nil\n\t} else if len(interceptors) == 1 {\n\t\tchainedInt = interceptors[0]\n\t} else {\n\t\tchainedInt = chainStreamInterceptors(interceptors)\n\t}\n\n\ts.opts.streamInt = chainedInt\n}\n\nfunc chainStreamInterceptors(interceptors []StreamServerInterceptor) StreamServerInterceptor {\n\treturn func(srv any, ss ServerStream, info *StreamServerInfo, handler StreamHandler) error {\n\t\treturn interceptors[0](srv, ss, info, getChainStreamHandler(interceptors, 0, info, handler))\n\t}\n}\n\nfunc getChainStreamHandler(interceptors []StreamServerInterceptor, curr int, info *StreamServerInfo, finalHandler StreamHandler) StreamHandler {\n\tif curr == len(interceptors)-1 {\n\t\treturn finalHandler\n\t}\n\treturn func(srv any, stream ServerStream) error {\n\t\treturn interceptors[curr+1](srv, stream, info, getChainStreamHandler(interceptors, curr+1, info, finalHandler))\n\t}\n}\n\nfunc (s *Server) processStreamingRPC(ctx context.Context, stream *transport.ServerStream, info *serviceInfo, sd *StreamDesc, trInfo *traceInfo) (err error) {\n\tif channelz.IsOn() {\n\t\ts.incrCallsStarted()\n\t}\n\tshs := s.opts.statsHandlers\n\tvar statsBegin *stats.Begin\n\tif len(shs) != 0 {\n\t\tbeginTime := time.Now()\n\t\tstatsBegin = &stats.Begin{\n\t\t\tBeginTime:      beginTime,\n\t\t\tIsClientStream: sd.ClientStreams,\n\t\t\tIsServerStream: sd.ServerStreams,\n\t\t}\n\t\tfor _, sh := range shs {\n\t\t\tsh.HandleRPC(ctx, statsBegin)\n\t\t}\n\t}\n\tctx = NewContextWithServerTransportStream(ctx, stream)\n\tss := &serverStream{\n\t\tctx:                   ctx,\n\t\ts:                     stream,\n\t\tp:                     &parser{r: stream, bufferPool: s.opts.bufferPool},\n\t\tcodec:                 s.getCodec(stream.ContentSubtype()),\n\t\tmaxReceiveMessageSize: s.opts.maxReceiveMessageSize,\n\t\tmaxSendMessageSize:    s.opts.maxSendMessageSize,\n\t\ttrInfo:                trInfo,\n\t\tstatsHandler:          shs,\n\t}\n\n\tif len(shs) != 0 || trInfo != nil || channelz.IsOn() {\n\t\t// See comment in processUnaryRPC on defers.\n\t\tdefer func() {\n\t\t\tif trInfo != nil {\n\t\t\t\tss.mu.Lock()\n\t\t\t\tif err != nil && err != io.EOF {\n\t\t\t\t\tss.trInfo.tr.LazyLog(&fmtStringer{\"%v\", []any{err}}, true)\n\t\t\t\t\tss.trInfo.tr.SetError()\n\t\t\t\t}\n\t\t\t\tss.trInfo.tr.Finish()\n\t\t\t\tss.trInfo.tr = nil\n\t\t\t\tss.mu.Unlock()\n\t\t\t}\n\n\t\t\tif len(shs) != 0 {\n\t\t\t\tend := &stats.End{\n\t\t\t\t\tBeginTime: statsBegin.BeginTime,\n\t\t\t\t\tEndTime:   time.Now(),\n\t\t\t\t}\n\t\t\t\tif err != nil && err != io.EOF {\n\t\t\t\t\tend.Error = toRPCErr(err)\n\t\t\t\t}\n\t\t\t\tfor _, sh := range shs {\n\t\t\t\t\tsh.HandleRPC(ctx, end)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif channelz.IsOn() {\n\t\t\t\tif err != nil && err != io.EOF {\n\t\t\t\t\ts.incrCallsFailed()\n\t\t\t\t} else {\n\t\t\t\t\ts.incrCallsSucceeded()\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n\n\tif ml := binarylog.GetMethodLogger(stream.Method()); ml != nil {\n\t\tss.binlogs = append(ss.binlogs, ml)\n\t}\n\tif s.opts.binaryLogger != nil {\n\t\tif ml := s.opts.binaryLogger.GetMethodLogger(stream.Method()); ml != nil {\n\t\t\tss.binlogs = append(ss.binlogs, ml)\n\t\t}\n\t}\n\tif len(ss.binlogs) != 0 {\n\t\tmd, _ := metadata.FromIncomingContext(ctx)\n\t\tlogEntry := &binarylog.ClientHeader{\n\t\t\tHeader:     md,\n\t\t\tMethodName: stream.Method(),\n\t\t\tPeerAddr:   nil,\n\t\t}\n\t\tif deadline, ok := ctx.Deadline(); ok {\n\t\t\tlogEntry.Timeout = time.Until(deadline)\n\t\t\tif logEntry.Timeout < 0 {\n\t\t\t\tlogEntry.Timeout = 0\n\t\t\t}\n\t\t}\n\t\tif a := md[\":authority\"]; len(a) > 0 {\n\t\t\tlogEntry.Authority = a[0]\n\t\t}\n\t\tif peer, ok := peer.FromContext(ss.Context()); ok {\n\t\t\tlogEntry.PeerAddr = peer.Addr\n\t\t}\n\t\tfor _, binlog := range ss.binlogs {\n\t\t\tbinlog.Log(ctx, logEntry)\n\t\t}\n\t}\n\n\t// If dc is set and matches the stream's compression, use it.  Otherwise, try\n\t// to find a matching registered compressor for decomp.\n\tif rc := stream.RecvCompress(); s.opts.dc != nil && s.opts.dc.Type() == rc {\n\t\tss.dc = s.opts.dc\n\t} else if rc != \"\" && rc != encoding.Identity {\n\t\tss.decomp = encoding.GetCompressor(rc)\n\t\tif ss.decomp == nil {\n\t\t\tst := status.Newf(codes.Unimplemented, \"grpc: Decompressor is not installed for grpc-encoding %q\", rc)\n\t\t\tss.s.WriteStatus(st)\n\t\t\treturn st.Err()\n\t\t}\n\t}\n\n\t// If cp is set, use it.  Otherwise, attempt to compress the response using\n\t// the incoming message compression method.\n\t//\n\t// NOTE: this needs to be ahead of all handling, https://github.com/grpc/grpc-go/issues/686.\n\tif s.opts.cp != nil {\n\t\tss.cp = s.opts.cp\n\t\tss.sendCompressorName = s.opts.cp.Type()\n\t} else if rc := stream.RecvCompress(); rc != \"\" && rc != encoding.Identity {\n\t\t// Legacy compressor not specified; attempt to respond with same encoding.\n\t\tss.comp = encoding.GetCompressor(rc)\n\t\tif ss.comp != nil {\n\t\t\tss.sendCompressorName = rc\n\t\t}\n\t}\n\n\tif ss.sendCompressorName != \"\" {\n\t\tif err := stream.SetSendCompress(ss.sendCompressorName); err != nil {\n\t\t\treturn status.Errorf(codes.Internal, \"grpc: failed to set send compressor: %v\", err)\n\t\t}\n\t}\n\n\tss.ctx = newContextWithRPCInfo(ss.ctx, false, ss.codec, ss.cp, ss.comp)\n\n\tif trInfo != nil {\n\t\ttrInfo.tr.LazyLog(&trInfo.firstLine, false)\n\t}\n\tvar appErr error\n\tvar server any\n\tif info != nil {\n\t\tserver = info.serviceImpl\n\t}\n\tif s.opts.streamInt == nil {\n\t\tappErr = sd.Handler(server, ss)\n\t} else {\n\t\tinfo := &StreamServerInfo{\n\t\t\tFullMethod:     stream.Method(),\n\t\t\tIsClientStream: sd.ClientStreams,\n\t\t\tIsServerStream: sd.ServerStreams,\n\t\t}\n\t\tappErr = s.opts.streamInt(server, ss, info, sd.Handler)\n\t}\n\tif appErr != nil {\n\t\tappStatus, ok := status.FromError(appErr)\n\t\tif !ok {\n\t\t\t// Convert non-status application error to a status error with code\n\t\t\t// Unknown, but handle context errors specifically.\n\t\t\tappStatus = status.FromContextError(appErr)\n\t\t\tappErr = appStatus.Err()\n\t\t}\n\t\tif trInfo != nil {\n\t\t\tss.mu.Lock()\n\t\t\tss.trInfo.tr.LazyLog(stringer(appStatus.Message()), true)\n\t\t\tss.trInfo.tr.SetError()\n\t\t\tss.mu.Unlock()\n\t\t}\n\t\tif len(ss.binlogs) != 0 {\n\t\t\tst := &binarylog.ServerTrailer{\n\t\t\t\tTrailer: ss.s.Trailer(),\n\t\t\t\tErr:     appErr,\n\t\t\t}\n\t\t\tfor _, binlog := range ss.binlogs {\n\t\t\t\tbinlog.Log(ctx, st)\n\t\t\t}\n\t\t}\n\t\tss.s.WriteStatus(appStatus)\n\t\t// TODO: Should we log an error from WriteStatus here and below?\n\t\treturn appErr\n\t}\n\tif trInfo != nil {\n\t\tss.mu.Lock()\n\t\tss.trInfo.tr.LazyLog(stringer(\"OK\"), false)\n\t\tss.mu.Unlock()\n\t}\n\tif len(ss.binlogs) != 0 {\n\t\tst := &binarylog.ServerTrailer{\n\t\t\tTrailer: ss.s.Trailer(),\n\t\t\tErr:     appErr,\n\t\t}\n\t\tfor _, binlog := range ss.binlogs {\n\t\t\tbinlog.Log(ctx, st)\n\t\t}\n\t}\n\treturn ss.s.WriteStatus(statusOK)\n}\n\nfunc (s *Server) handleStream(t transport.ServerTransport, stream *transport.ServerStream) {\n\tctx := stream.Context()\n\tctx = contextWithServer(ctx, s)\n\tvar ti *traceInfo\n\tif EnableTracing {\n\t\ttr := newTrace(\"grpc.Recv.\"+methodFamily(stream.Method()), stream.Method())\n\t\tctx = newTraceContext(ctx, tr)\n\t\tti = &traceInfo{\n\t\t\ttr: tr,\n\t\t\tfirstLine: firstLine{\n\t\t\t\tclient:     false,\n\t\t\t\tremoteAddr: t.Peer().Addr,\n\t\t\t},\n\t\t}\n\t\tif dl, ok := ctx.Deadline(); ok {\n\t\t\tti.firstLine.deadline = time.Until(dl)\n\t\t}\n\t}\n\n\tsm := stream.Method()\n\tif sm != \"\" && sm[0] == '/' {\n\t\tsm = sm[1:]\n\t}\n\tpos := strings.LastIndex(sm, \"/\")\n\tif pos == -1 {\n\t\tif ti != nil {\n\t\t\tti.tr.LazyLog(&fmtStringer{\"Malformed method name %q\", []any{sm}}, true)\n\t\t\tti.tr.SetError()\n\t\t}\n\t\terrDesc := fmt.Sprintf(\"malformed method name: %q\", stream.Method())\n\t\tif err := stream.WriteStatus(status.New(codes.Unimplemented, errDesc)); err != nil {\n\t\t\tif ti != nil {\n\t\t\t\tti.tr.LazyLog(&fmtStringer{\"%v\", []any{err}}, true)\n\t\t\t\tti.tr.SetError()\n\t\t\t}\n\t\t\tchannelz.Warningf(logger, s.channelz, \"grpc: Server.handleStream failed to write status: %v\", err)\n\t\t}\n\t\tif ti != nil {\n\t\t\tti.tr.Finish()\n\t\t}\n\t\treturn\n\t}\n\tservice := sm[:pos]\n\tmethod := sm[pos+1:]\n\n\t// FromIncomingContext is expensive: skip if there are no statsHandlers\n\tif len(s.opts.statsHandlers) > 0 {\n\t\tmd, _ := metadata.FromIncomingContext(ctx)\n\t\tfor _, sh := range s.opts.statsHandlers {\n\t\t\tctx = sh.TagRPC(ctx, &stats.RPCTagInfo{FullMethodName: stream.Method()})\n\t\t\tsh.HandleRPC(ctx, &stats.InHeader{\n\t\t\t\tFullMethod:  stream.Method(),\n\t\t\t\tRemoteAddr:  t.Peer().Addr,\n\t\t\t\tLocalAddr:   t.Peer().LocalAddr,\n\t\t\t\tCompression: stream.RecvCompress(),\n\t\t\t\tWireLength:  stream.HeaderWireLength(),\n\t\t\t\tHeader:      md,\n\t\t\t})\n\t\t}\n\t}\n\t// To have calls in stream callouts work. Will delete once all stats handler\n\t// calls come from the gRPC layer.\n\tstream.SetContext(ctx)\n\n\tsrv, knownService := s.services[service]\n\tif knownService {\n\t\tif md, ok := srv.methods[method]; ok {\n\t\t\ts.processUnaryRPC(ctx, stream, srv, md, ti)\n\t\t\treturn\n\t\t}\n\t\tif sd, ok := srv.streams[method]; ok {\n\t\t\ts.processStreamingRPC(ctx, stream, srv, sd, ti)\n\t\t\treturn\n\t\t}\n\t}\n\t// Unknown service, or known server unknown method.\n\tif unknownDesc := s.opts.unknownStreamDesc; unknownDesc != nil {\n\t\ts.processStreamingRPC(ctx, stream, nil, unknownDesc, ti)\n\t\treturn\n\t}\n\tvar errDesc string\n\tif !knownService {\n\t\terrDesc = fmt.Sprintf(\"unknown service %v\", service)\n\t} else {\n\t\terrDesc = fmt.Sprintf(\"unknown method %v for service %v\", method, service)\n\t}\n\tif ti != nil {\n\t\tti.tr.LazyPrintf(\"%s\", errDesc)\n\t\tti.tr.SetError()\n\t}\n\tif err := stream.WriteStatus(status.New(codes.Unimplemented, errDesc)); err != nil {\n\t\tif ti != nil {\n\t\t\tti.tr.LazyLog(&fmtStringer{\"%v\", []any{err}}, true)\n\t\t\tti.tr.SetError()\n\t\t}\n\t\tchannelz.Warningf(logger, s.channelz, \"grpc: Server.handleStream failed to write status: %v\", err)\n\t}\n\tif ti != nil {\n\t\tti.tr.Finish()\n\t}\n}\n\n// The key to save ServerTransportStream in the context.\ntype streamKey struct{}\n\n// NewContextWithServerTransportStream creates a new context from ctx and\n// attaches stream to it.\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc NewContextWithServerTransportStream(ctx context.Context, stream ServerTransportStream) context.Context {\n\treturn context.WithValue(ctx, streamKey{}, stream)\n}\n\n// ServerTransportStream is a minimal interface that a transport stream must\n// implement. This can be used to mock an actual transport stream for tests of\n// handler code that use, for example, grpc.SetHeader (which requires some\n// stream to be in context).\n//\n// See also NewContextWithServerTransportStream.\n//\n// # Experimental\n//\n// Notice: This type is EXPERIMENTAL and may be changed or removed in a\n// later release.\ntype ServerTransportStream interface {\n\tMethod() string\n\tSetHeader(md metadata.MD) error\n\tSendHeader(md metadata.MD) error\n\tSetTrailer(md metadata.MD) error\n}\n\n// ServerTransportStreamFromContext returns the ServerTransportStream saved in\n// ctx. Returns nil if the given context has no stream associated with it\n// (which implies it is not an RPC invocation context).\n//\n// # Experimental\n//\n// Notice: This API is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc ServerTransportStreamFromContext(ctx context.Context) ServerTransportStream {\n\ts, _ := ctx.Value(streamKey{}).(ServerTransportStream)\n\treturn s\n}\n\n// Stop stops the gRPC server. It immediately closes all open\n// connections and listeners.\n// It cancels all active RPCs on the server side and the corresponding\n// pending RPCs on the client side will get notified by connection\n// errors.\nfunc (s *Server) Stop() {\n\ts.stop(false)\n}\n\n// GracefulStop stops the gRPC server gracefully. It stops the server from\n// accepting new connections and RPCs and blocks until all the pending RPCs are\n// finished.\nfunc (s *Server) GracefulStop() {\n\ts.stop(true)\n}\n\nfunc (s *Server) stop(graceful bool) {\n\ts.quit.Fire()\n\tdefer s.done.Fire()\n\n\ts.channelzRemoveOnce.Do(func() { channelz.RemoveEntry(s.channelz.ID) })\n\ts.mu.Lock()\n\ts.closeListenersLocked()\n\t// Wait for serving threads to be ready to exit.  Only then can we be sure no\n\t// new conns will be created.\n\ts.mu.Unlock()\n\ts.serveWG.Wait()\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tif graceful {\n\t\ts.drainAllServerTransportsLocked()\n\t} else {\n\t\ts.closeServerTransportsLocked()\n\t}\n\n\tfor len(s.conns) != 0 {\n\t\ts.cv.Wait()\n\t}\n\ts.conns = nil\n\n\tif s.opts.numServerWorkers > 0 {\n\t\t// Closing the channel (only once, via grpcsync.OnceFunc) after all the\n\t\t// connections have been closed above ensures that there are no\n\t\t// goroutines executing the callback passed to st.HandleStreams (where\n\t\t// the channel is written to).\n\t\ts.serverWorkerChannelClose()\n\t}\n\n\tif graceful || s.opts.waitForHandlers {\n\t\ts.handlersWG.Wait()\n\t}\n\n\tif s.events != nil {\n\t\ts.events.Finish()\n\t\ts.events = nil\n\t}\n}\n\n// s.mu must be held by the caller.\nfunc (s *Server) closeServerTransportsLocked() {\n\tfor _, conns := range s.conns {\n\t\tfor st := range conns {\n\t\t\tst.Close(errors.New(\"Server.Stop called\"))\n\t\t}\n\t}\n}\n\n// s.mu must be held by the caller.\nfunc (s *Server) drainAllServerTransportsLocked() {\n\tif !s.drain {\n\t\tfor _, conns := range s.conns {\n\t\t\tfor st := range conns {\n\t\t\t\tst.Drain(\"graceful_stop\")\n\t\t\t}\n\t\t}\n\t\ts.drain = true\n\t}\n}\n\n// s.mu must be held by the caller.\nfunc (s *Server) closeListenersLocked() {\n\tfor lis := range s.lis {\n\t\tlis.Close()\n\t}\n\ts.lis = nil\n}\n\n// contentSubtype must be lowercase\n// cannot return nil\nfunc (s *Server) getCodec(contentSubtype string) baseCodec {\n\tif s.opts.codec != nil {\n\t\treturn s.opts.codec\n\t}\n\tif contentSubtype == \"\" {\n\t\treturn getCodec(proto.Name)\n\t}\n\tcodec := getCodec(contentSubtype)\n\tif codec == nil {\n\t\tlogger.Warningf(\"Unsupported codec %q. Defaulting to %q for now. This will start to fail in future releases.\", contentSubtype, proto.Name)\n\t\treturn getCodec(proto.Name)\n\t}\n\treturn codec\n}\n\ntype serverKey struct{}\n\n// serverFromContext gets the Server from the context.\nfunc serverFromContext(ctx context.Context) *Server {\n\ts, _ := ctx.Value(serverKey{}).(*Server)\n\treturn s\n}\n\n// contextWithServer sets the Server in the context.\nfunc contextWithServer(ctx context.Context, server *Server) context.Context {\n\treturn context.WithValue(ctx, serverKey{}, server)\n}\n\n// isRegisteredMethod returns whether the passed in method is registered as a\n// method on the server. /service/method and service/method will match if the\n// service and method are registered on the server.\nfunc (s *Server) isRegisteredMethod(serviceMethod string) bool {\n\tif serviceMethod != \"\" && serviceMethod[0] == '/' {\n\t\tserviceMethod = serviceMethod[1:]\n\t}\n\tpos := strings.LastIndex(serviceMethod, \"/\")\n\tif pos == -1 { // Invalid method name syntax.\n\t\treturn false\n\t}\n\tservice := serviceMethod[:pos]\n\tmethod := serviceMethod[pos+1:]\n\tsrv, knownService := s.services[service]\n\tif knownService {\n\t\tif _, ok := srv.methods[method]; ok {\n\t\t\treturn true\n\t\t}\n\t\tif _, ok := srv.streams[method]; ok {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// SetHeader sets the header metadata to be sent from the server to the client.\n// The context provided must be the context passed to the server's handler.\n//\n// Streaming RPCs should prefer the SetHeader method of the ServerStream.\n//\n// When called multiple times, all the provided metadata will be merged.  All\n// the metadata will be sent out when one of the following happens:\n//\n//   - grpc.SendHeader is called, or for streaming handlers, stream.SendHeader.\n//   - The first response message is sent.  For unary handlers, this occurs when\n//     the handler returns; for streaming handlers, this can happen when stream's\n//     SendMsg method is called.\n//   - An RPC status is sent out (error or success).  This occurs when the handler\n//     returns.\n//\n// SetHeader will fail if called after any of the events above.\n//\n// The error returned is compatible with the status package.  However, the\n// status code will often not match the RPC status as seen by the client\n// application, and therefore, should not be relied upon for this purpose.\nfunc SetHeader(ctx context.Context, md metadata.MD) error {\n\tif md.Len() == 0 {\n\t\treturn nil\n\t}\n\tstream := ServerTransportStreamFromContext(ctx)\n\tif stream == nil {\n\t\treturn status.Errorf(codes.Internal, \"grpc: failed to fetch the stream from the context %v\", ctx)\n\t}\n\treturn stream.SetHeader(md)\n}\n\n// SendHeader sends header metadata. It may be called at most once, and may not\n// be called after any event that causes headers to be sent (see SetHeader for\n// a complete list).  The provided md and headers set by SetHeader() will be\n// sent.\n//\n// The error returned is compatible with the status package.  However, the\n// status code will often not match the RPC status as seen by the client\n// application, and therefore, should not be relied upon for this purpose.\nfunc SendHeader(ctx context.Context, md metadata.MD) error {\n\tstream := ServerTransportStreamFromContext(ctx)\n\tif stream == nil {\n\t\treturn status.Errorf(codes.Internal, \"grpc: failed to fetch the stream from the context %v\", ctx)\n\t}\n\tif err := stream.SendHeader(md); err != nil {\n\t\treturn toRPCErr(err)\n\t}\n\treturn nil\n}\n\n// SetSendCompressor sets a compressor for outbound messages from the server.\n// It must not be called after any event that causes headers to be sent\n// (see ServerStream.SetHeader for the complete list). Provided compressor is\n// used when below conditions are met:\n//\n//   - compressor is registered via encoding.RegisterCompressor\n//   - compressor name must exist in the client advertised compressor names\n//     sent in grpc-accept-encoding header. Use ClientSupportedCompressors to\n//     get client supported compressor names.\n//\n// The context provided must be the context passed to the server's handler.\n// It must be noted that compressor name encoding.Identity disables the\n// outbound compression.\n// By default, server messages will be sent using the same compressor with\n// which request messages were sent.\n//\n// It is not safe to call SetSendCompressor concurrently with SendHeader and\n// SendMsg.\n//\n// # Experimental\n//\n// Notice: This function is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc SetSendCompressor(ctx context.Context, name string) error {\n\tstream, ok := ServerTransportStreamFromContext(ctx).(*transport.ServerStream)\n\tif !ok || stream == nil {\n\t\treturn fmt.Errorf(\"failed to fetch the stream from the given context\")\n\t}\n\n\tif err := validateSendCompressor(name, stream.ClientAdvertisedCompressors()); err != nil {\n\t\treturn fmt.Errorf(\"unable to set send compressor: %w\", err)\n\t}\n\n\treturn stream.SetSendCompress(name)\n}\n\n// ClientSupportedCompressors returns compressor names advertised by the client\n// via grpc-accept-encoding header.\n//\n// The context provided must be the context passed to the server's handler.\n//\n// # Experimental\n//\n// Notice: This function is EXPERIMENTAL and may be changed or removed in a\n// later release.\nfunc ClientSupportedCompressors(ctx context.Context) ([]string, error) {\n\tstream, ok := ServerTransportStreamFromContext(ctx).(*transport.ServerStream)\n\tif !ok || stream == nil {\n\t\treturn nil, fmt.Errorf(\"failed to fetch the stream from the given context %v\", ctx)\n\t}\n\n\treturn stream.ClientAdvertisedCompressors(), nil\n}\n\n// SetTrailer sets the trailer metadata that will be sent when an RPC returns.\n// When called more than once, all the provided metadata will be merged.\n//\n// The error returned is compatible with the status package.  However, the\n// status code will often not match the RPC status as seen by the client\n// application, and therefore, should not be relied upon for this purpose.\nfunc SetTrailer(ctx context.Context, md metadata.MD) error {\n\tif md.Len() == 0 {\n\t\treturn nil\n\t}\n\tstream := ServerTransportStreamFromContext(ctx)\n\tif stream == nil {\n\t\treturn status.Errorf(codes.Internal, \"grpc: failed to fetch the stream from the context %v\", ctx)\n\t}\n\treturn stream.SetTrailer(md)\n}\n\n// Method returns the method string for the server context.  The returned\n// string is in the format of \"/service/method\".\nfunc Method(ctx context.Context) (string, bool) {\n\ts := ServerTransportStreamFromContext(ctx)\n\tif s == nil {\n\t\treturn \"\", false\n\t}\n\treturn s.Method(), true\n}\n\n// validateSendCompressor returns an error when given compressor name cannot be\n// handled by the server or the client based on the advertised compressors.\nfunc validateSendCompressor(name string, clientCompressors []string) error {\n\tif name == encoding.Identity {\n\t\treturn nil\n\t}\n\n\tif !grpcutil.IsCompressorNameRegistered(name) {\n\t\treturn fmt.Errorf(\"compressor not registered %q\", name)\n\t}\n\n\tfor _, c := range clientCompressors {\n\t\tif c == name {\n\t\t\treturn nil // found match\n\t\t}\n\t}\n\treturn fmt.Errorf(\"client does not support compressor %q\", name)\n}\n\n// atomicSemaphore implements a blocking, counting semaphore. acquire should be\n// called synchronously; release may be called asynchronously.\ntype atomicSemaphore struct {\n\tn    atomic.Int64\n\twait chan struct{}\n}\n\nfunc (q *atomicSemaphore) acquire() {\n\tif q.n.Add(-1) < 0 {\n\t\t// We ran out of quota.  Block until a release happens.\n\t\t<-q.wait\n\t}\n}\n\nfunc (q *atomicSemaphore) release() {\n\t// N.B. the \"<= 0\" check below should allow for this to work with multiple\n\t// concurrent calls to acquire, but also note that with synchronous calls to\n\t// acquire, as our system does, n will never be less than -1.  There are\n\t// fairness issues (queuing) to consider if this was to be generalized.\n\tif q.n.Add(1) <= 0 {\n\t\t// An acquire was waiting on us.  Unblock it.\n\t\tq.wait <- struct{}{}\n\t}\n}\n\nfunc newHandlerQuota(n uint32) *atomicSemaphore {\n\ta := &atomicSemaphore{wait: make(chan struct{}, 1)}\n\ta.n.Store(int64(n))\n\treturn a\n}\n"
        },
        {
          "name": "server_ext_test.go",
          "type": "blob",
          "size": 9.7255859375,
          "content": "/*\n *\n * Copyright 2023 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc_test\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"runtime\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n\t\"google.golang.org/grpc/internal/grpcsync\"\n\t\"google.golang.org/grpc/internal/stubserver\"\n\t\"google.golang.org/grpc/status\"\n\n\ttestgrpc \"google.golang.org/grpc/interop/grpc_testing\"\n\ttestpb \"google.golang.org/grpc/interop/grpc_testing\"\n)\n\n// TestServer_MaxHandlers ensures that no more than MaxConcurrentStreams server\n// handlers are active at one time.\nfunc (s) TestServer_MaxHandlers(t *testing.T) {\n\tstarted := make(chan struct{})\n\tblockCalls := grpcsync.NewEvent()\n\n\t// This stub server does not properly respect the stream context, so it will\n\t// not exit when the context is canceled.\n\tss := stubserver.StubServer{\n\t\tFullDuplexCallF: func(stream testgrpc.TestService_FullDuplexCallServer) error {\n\t\t\tstarted <- struct{}{}\n\t\t\t<-blockCalls.Done()\n\t\t\treturn nil\n\t\t},\n\t}\n\tif err := ss.Start([]grpc.ServerOption{grpc.MaxConcurrentStreams(1)}); err != nil {\n\t\tt.Fatal(\"Error starting server:\", err)\n\t}\n\tdefer ss.Stop()\n\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\n\t// Start one RPC to the server.\n\tctx1, cancel1 := context.WithCancel(ctx)\n\t_, err := ss.Client.FullDuplexCall(ctx1)\n\tif err != nil {\n\t\tt.Fatal(\"Error staring call:\", err)\n\t}\n\n\t// Wait for the handler to be invoked.\n\tselect {\n\tcase <-started:\n\tcase <-ctx.Done():\n\t\tt.Fatalf(\"Timed out waiting for RPC to start on server.\")\n\t}\n\n\t// Cancel it on the client.  The server handler will still be running.\n\tcancel1()\n\n\tctx2, cancel2 := context.WithCancel(ctx)\n\tdefer cancel2()\n\ts, err := ss.Client.FullDuplexCall(ctx2)\n\tif err != nil {\n\t\tt.Fatal(\"Error staring call:\", err)\n\t}\n\n\t// After 100ms, allow the first call to unblock.  That should allow the\n\t// second RPC to run and finish.\n\tselect {\n\tcase <-started:\n\t\tblockCalls.Fire()\n\t\tt.Fatalf(\"RPC started unexpectedly.\")\n\tcase <-time.After(100 * time.Millisecond):\n\t\tblockCalls.Fire()\n\t}\n\n\tselect {\n\tcase <-started:\n\tcase <-ctx.Done():\n\t\tt.Fatalf(\"Timed out waiting for second RPC to start on server.\")\n\t}\n\tif _, err := s.Recv(); err != io.EOF {\n\t\tt.Fatal(\"Received unexpected RPC error:\", err)\n\t}\n}\n\n// Tests the case where the stream worker goroutine option is enabled, and a\n// number of RPCs are initiated around the same time that Stop() is called. This\n// used to result in a write to a closed channel. This test verifies that there\n// is no panic.\nfunc (s) TestStreamWorkers_RPCsAndStop(t *testing.T) {\n\tss := stubserver.StartTestService(t, nil, grpc.NumStreamWorkers(uint32(runtime.NumCPU())))\n\t// This deferred stop takes care of stopping the server when one of the\n\t// below grpc.Dials fail, and the test exits early.\n\tdefer ss.Stop()\n\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\tconst numChannels = 20\n\tconst numRPCLoops = 20\n\n\t// Create a bunch of clientconns and ensure that they are READY by making an\n\t// RPC on them.\n\tccs := make([]*grpc.ClientConn, numChannels)\n\tfor i := 0; i < numChannels; i++ {\n\t\tvar err error\n\t\tccs[i], err = grpc.NewClient(ss.Address, grpc.WithTransportCredentials(insecure.NewCredentials()))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"[iteration: %d] grpc.NewClient(%s) failed: %v\", i, ss.Address, err)\n\t\t}\n\t\tdefer ccs[i].Close()\n\t\tclient := testgrpc.NewTestServiceClient(ccs[i])\n\t\tif _, err := client.EmptyCall(ctx, &testpb.Empty{}); err != nil {\n\t\t\tt.Fatalf(\"EmptyCall() failed: %v\", err)\n\t\t}\n\t}\n\n\t// Make a bunch of concurrent RPCs on the above clientconns. These will\n\t// eventually race with Stop(), and will start to fail.\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < numChannels; i++ {\n\t\tclient := testgrpc.NewTestServiceClient(ccs[i])\n\t\tfor j := 0; j < numRPCLoops; j++ {\n\t\t\twg.Add(1)\n\t\t\tgo func(client testgrpc.TestServiceClient) {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tfor {\n\t\t\t\t\t_, err := client.EmptyCall(ctx, &testpb.Empty{})\n\t\t\t\t\tif err == nil {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tif code := status.Code(err); code == codes.Unavailable {\n\t\t\t\t\t\t// Once Stop() has been called on the server, we expect\n\t\t\t\t\t\t// subsequent calls to fail with Unavailable.\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tt.Errorf(\"EmptyCall() failed: %v\", err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}(client)\n\t\t}\n\t}\n\n\t// Call Stop() concurrently with the above RPC attempts.\n\tss.Stop()\n\twg.Wait()\n}\n\n// Tests the case where the stream worker goroutine option is enabled, and both\n// Stop() and GracefulStop() care called. This used to result in a close of a\n// closed channel. This test verifies that there is no panic.\nfunc (s) TestStreamWorkers_GracefulStopAndStop(t *testing.T) {\n\tss := stubserver.StartTestService(t, nil, grpc.NumStreamWorkers(uint32(runtime.NumCPU())))\n\tdefer ss.Stop()\n\n\tif err := ss.StartClient(grpc.WithTransportCredentials(insecure.NewCredentials())); err != nil {\n\t\tt.Fatalf(\"Failed to create client to stub server: %v\", err)\n\t}\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\tclient := testgrpc.NewTestServiceClient(ss.CC)\n\tif _, err := client.EmptyCall(ctx, &testpb.Empty{}); err != nil {\n\t\tt.Fatalf(\"EmptyCall() failed: %v\", err)\n\t}\n\n\tss.S.GracefulStop()\n}\n\n// Tests the WaitForHandlers ServerOption by leaving an RPC running while Stop\n// is called, and ensures Stop doesn't return until the handler returns.\nfunc (s) TestServer_WaitForHandlers(t *testing.T) {\n\tstarted := grpcsync.NewEvent()\n\tblockCalls := grpcsync.NewEvent()\n\n\t// This stub server does not properly respect the stream context, so it will\n\t// not exit when the context is canceled.\n\tss := stubserver.StubServer{\n\t\tFullDuplexCallF: func(stream testgrpc.TestService_FullDuplexCallServer) error {\n\t\t\tstarted.Fire()\n\t\t\t<-blockCalls.Done()\n\t\t\treturn nil\n\t\t},\n\t}\n\tif err := ss.Start([]grpc.ServerOption{grpc.WaitForHandlers(true)}); err != nil {\n\t\tt.Fatal(\"Error starting server:\", err)\n\t}\n\tdefer ss.Stop()\n\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\n\t// Start one RPC to the server.\n\tctx1, cancel1 := context.WithCancel(ctx)\n\t_, err := ss.Client.FullDuplexCall(ctx1)\n\tif err != nil {\n\t\tt.Fatal(\"Error staring call:\", err)\n\t}\n\n\t// Wait for the handler to be invoked.\n\tselect {\n\tcase <-started.Done():\n\tcase <-ctx.Done():\n\t\tt.Fatalf(\"Timed out waiting for RPC to start on server.\")\n\t}\n\n\t// Cancel it on the client.  The server handler will still be running.\n\tcancel1()\n\n\t// Close the connection.  This might be sufficient to allow the server to\n\t// return if it doesn't properly wait for outstanding method handlers to\n\t// return.\n\tss.CC.Close()\n\n\t// Try to Stop() the server, which should block indefinitely (until\n\t// blockCalls is fired).\n\tstopped := grpcsync.NewEvent()\n\tgo func() {\n\t\tss.S.Stop()\n\t\tstopped.Fire()\n\t}()\n\n\t// Wait 100ms and ensure stopped does not fire.\n\tselect {\n\tcase <-stopped.Done():\n\t\ttrace := make([]byte, 4096)\n\t\ttrace = trace[0:runtime.Stack(trace, true)]\n\t\tblockCalls.Fire()\n\t\tt.Fatalf(\"Server returned from Stop() illegally.  Stack trace:\\n%v\", string(trace))\n\tcase <-time.After(100 * time.Millisecond):\n\t\t// Success; unblock the call and wait for stopped.\n\t\tblockCalls.Fire()\n\t}\n\n\tselect {\n\tcase <-stopped.Done():\n\tcase <-ctx.Done():\n\t\tt.Fatalf(\"Timed out waiting for second RPC to start on server.\")\n\t}\n}\n\n// Tests that GracefulStop will wait for all method handlers to return by\n// blocking a handler and ensuring GracefulStop doesn't return until after it is\n// unblocked.\nfunc (s) TestServer_GracefulStopWaits(t *testing.T) {\n\tstarted := grpcsync.NewEvent()\n\tblockCalls := grpcsync.NewEvent()\n\n\t// This stub server does not properly respect the stream context, so it will\n\t// not exit when the context is canceled.\n\tss := stubserver.StubServer{\n\t\tFullDuplexCallF: func(stream testgrpc.TestService_FullDuplexCallServer) error {\n\t\t\tstarted.Fire()\n\t\t\t<-blockCalls.Done()\n\t\t\treturn nil\n\t\t},\n\t}\n\tif err := ss.Start(nil); err != nil {\n\t\tt.Fatal(\"Error starting server:\", err)\n\t}\n\tdefer ss.Stop()\n\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\n\t// Start one RPC to the server.\n\tctx1, cancel1 := context.WithCancel(ctx)\n\t_, err := ss.Client.FullDuplexCall(ctx1)\n\tif err != nil {\n\t\tt.Fatal(\"Error staring call:\", err)\n\t}\n\n\t// Wait for the handler to be invoked.\n\tselect {\n\tcase <-started.Done():\n\tcase <-ctx.Done():\n\t\tt.Fatalf(\"Timed out waiting for RPC to start on server.\")\n\t}\n\n\t// Cancel it on the client.  The server handler will still be running.\n\tcancel1()\n\n\t// Close the connection.  This might be sufficient to allow the server to\n\t// return if it doesn't properly wait for outstanding method handlers to\n\t// return.\n\tss.CC.Close()\n\n\t// Try to Stop() the server, which should block indefinitely (until\n\t// blockCalls is fired).\n\tstopped := grpcsync.NewEvent()\n\tgo func() {\n\t\tss.S.GracefulStop()\n\t\tstopped.Fire()\n\t}()\n\n\t// Wait 100ms and ensure stopped does not fire.\n\tselect {\n\tcase <-stopped.Done():\n\t\ttrace := make([]byte, 4096)\n\t\ttrace = trace[0:runtime.Stack(trace, true)]\n\t\tblockCalls.Fire()\n\t\tt.Fatalf(\"Server returned from Stop() illegally.  Stack trace:\\n%v\", string(trace))\n\tcase <-time.After(100 * time.Millisecond):\n\t\t// Success; unblock the call and wait for stopped.\n\t\tblockCalls.Fire()\n\t}\n\n\tselect {\n\tcase <-stopped.Done():\n\tcase <-ctx.Done():\n\t\tt.Fatalf(\"Timed out waiting for second RPC to start on server.\")\n\t}\n}\n"
        },
        {
          "name": "server_test.go",
          "type": "blob",
          "size": 5.8564453125,
          "content": "/*\n *\n * Copyright 2016 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n\t\"net\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/go-cmp/cmp\"\n\t\"google.golang.org/grpc/internal/transport\"\n\t\"google.golang.org/grpc/status\"\n)\n\ntype emptyServiceServer any\n\ntype testServer struct{}\n\nfunc errorDesc(err error) string {\n\tif s, ok := status.FromError(err); ok {\n\t\treturn s.Message()\n\t}\n\treturn err.Error()\n}\n\nfunc (s) TestStopBeforeServe(t *testing.T) {\n\tlis, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to create listener: %v\", err)\n\t}\n\n\tserver := NewServer()\n\tserver.Stop()\n\terr = server.Serve(lis)\n\tif err != ErrServerStopped {\n\t\tt.Fatalf(\"server.Serve() error = %v, want %v\", err, ErrServerStopped)\n\t}\n\n\t// server.Serve is responsible for closing the listener, even if the\n\t// server was already stopped.\n\terr = lis.Close()\n\tif got, want := errorDesc(err), \"use of closed\"; !strings.Contains(got, want) {\n\t\tt.Errorf(\"Close() error = %q, want %q\", got, want)\n\t}\n}\n\nfunc (s) TestGracefulStop(t *testing.T) {\n\n\tlis, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to create listener: %v\", err)\n\t}\n\n\tserver := NewServer()\n\tgo func() {\n\t\t// make sure Serve() is called\n\t\ttime.Sleep(time.Millisecond * 500)\n\t\tserver.GracefulStop()\n\t}()\n\n\terr = server.Serve(lis)\n\tif err != nil {\n\t\tt.Fatalf(\"Serve() returned non-nil error on GracefulStop: %v\", err)\n\t}\n}\n\nfunc (s) TestGetServiceInfo(t *testing.T) {\n\ttestSd := ServiceDesc{\n\t\tServiceName: \"grpc.testing.EmptyService\",\n\t\tHandlerType: (*emptyServiceServer)(nil),\n\t\tMethods: []MethodDesc{\n\t\t\t{\n\t\t\t\tMethodName: \"EmptyCall\",\n\t\t\t\tHandler:    nil,\n\t\t\t},\n\t\t},\n\t\tStreams: []StreamDesc{\n\t\t\t{\n\t\t\t\tStreamName:    \"EmptyStream\",\n\t\t\t\tHandler:       nil,\n\t\t\t\tServerStreams: false,\n\t\t\t\tClientStreams: true,\n\t\t\t},\n\t\t},\n\t\tMetadata: []int{0, 2, 1, 3},\n\t}\n\n\tserver := NewServer()\n\tserver.RegisterService(&testSd, &testServer{})\n\n\tinfo := server.GetServiceInfo()\n\twant := map[string]ServiceInfo{\n\t\t\"grpc.testing.EmptyService\": {\n\t\t\tMethods: []MethodInfo{\n\t\t\t\t{\n\t\t\t\t\tName:           \"EmptyCall\",\n\t\t\t\t\tIsClientStream: false,\n\t\t\t\t\tIsServerStream: false,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tName:           \"EmptyStream\",\n\t\t\t\t\tIsClientStream: true,\n\t\t\t\t\tIsServerStream: false,\n\t\t\t\t}},\n\t\t\tMetadata: []int{0, 2, 1, 3},\n\t\t},\n\t}\n\n\tif !reflect.DeepEqual(info, want) {\n\t\tt.Errorf(\"GetServiceInfo() = %+v, want %+v\", info, want)\n\t}\n}\n\nfunc (s) TestRetryChainedInterceptor(t *testing.T) {\n\tvar records []int\n\ti1 := func(ctx context.Context, req any, _ *UnaryServerInfo, handler UnaryHandler) (resp any, err error) {\n\t\trecords = append(records, 1)\n\t\t// call handler twice to simulate a retry here.\n\t\thandler(ctx, req)\n\t\treturn handler(ctx, req)\n\t}\n\ti2 := func(ctx context.Context, req any, _ *UnaryServerInfo, handler UnaryHandler) (resp any, err error) {\n\t\trecords = append(records, 2)\n\t\treturn handler(ctx, req)\n\t}\n\ti3 := func(ctx context.Context, req any, _ *UnaryServerInfo, handler UnaryHandler) (resp any, err error) {\n\t\trecords = append(records, 3)\n\t\treturn handler(ctx, req)\n\t}\n\n\tii := chainUnaryInterceptors([]UnaryServerInterceptor{i1, i2, i3})\n\n\thandler := func(context.Context, any) (any, error) {\n\t\treturn nil, nil\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\n\tii(ctx, nil, nil, handler)\n\tif !cmp.Equal(records, []int{1, 2, 3, 2, 3}) {\n\t\tt.Fatalf(\"retry failed on chained interceptors: %v\", records)\n\t}\n}\n\nfunc (s) TestStreamContext(t *testing.T) {\n\texpectedStream := &transport.ServerStream{}\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\tctx = NewContextWithServerTransportStream(ctx, expectedStream)\n\n\ts := ServerTransportStreamFromContext(ctx)\n\tstream, ok := s.(*transport.ServerStream)\n\tif !ok || expectedStream != stream {\n\t\tt.Fatalf(\"GetStreamFromContext(%v) = %v, %t, want: %v, true\", ctx, stream, ok, expectedStream)\n\t}\n}\n\nfunc BenchmarkChainUnaryInterceptor(b *testing.B) {\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\tfor _, n := range []int{1, 3, 5, 10} {\n\t\tn := n\n\t\tb.Run(strconv.Itoa(n), func(b *testing.B) {\n\t\t\tinterceptors := make([]UnaryServerInterceptor, 0, n)\n\t\t\tfor i := 0; i < n; i++ {\n\t\t\t\tinterceptors = append(interceptors, func(\n\t\t\t\t\tctx context.Context, req any, _ *UnaryServerInfo, handler UnaryHandler,\n\t\t\t\t) (any, error) {\n\t\t\t\t\treturn handler(ctx, req)\n\t\t\t\t})\n\t\t\t}\n\n\t\t\ts := NewServer(ChainUnaryInterceptor(interceptors...))\n\t\t\tb.ReportAllocs()\n\t\t\tb.ResetTimer()\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\tif _, err := s.opts.unaryInt(ctx, nil, nil,\n\t\t\t\t\tfunc(context.Context, any) (any, error) {\n\t\t\t\t\t\treturn nil, nil\n\t\t\t\t\t},\n\t\t\t\t); err != nil {\n\t\t\t\t\tb.Fatal(err)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc BenchmarkChainStreamInterceptor(b *testing.B) {\n\tfor _, n := range []int{1, 3, 5, 10} {\n\t\tn := n\n\t\tb.Run(strconv.Itoa(n), func(b *testing.B) {\n\t\t\tinterceptors := make([]StreamServerInterceptor, 0, n)\n\t\t\tfor i := 0; i < n; i++ {\n\t\t\t\tinterceptors = append(interceptors, func(\n\t\t\t\t\tsrv any, ss ServerStream, _ *StreamServerInfo, handler StreamHandler,\n\t\t\t\t) error {\n\t\t\t\t\treturn handler(srv, ss)\n\t\t\t\t})\n\t\t\t}\n\n\t\t\ts := NewServer(ChainStreamInterceptor(interceptors...))\n\t\t\tb.ReportAllocs()\n\t\t\tb.ResetTimer()\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\tif err := s.opts.streamInt(nil, nil, nil, func(any, ServerStream) error {\n\t\t\t\t\treturn nil\n\t\t\t\t}); err != nil {\n\t\t\t\t\tb.Fatal(err)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "service_config.go",
          "type": "blob",
          "size": 10.8291015625,
          "content": "/*\n *\n * Copyright 2017 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"time\"\n\n\t\"google.golang.org/grpc/balancer\"\n\t\"google.golang.org/grpc/balancer/pickfirst\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/internal\"\n\t\"google.golang.org/grpc/internal/balancer/gracefulswitch\"\n\tinternalserviceconfig \"google.golang.org/grpc/internal/serviceconfig\"\n\t\"google.golang.org/grpc/serviceconfig\"\n)\n\nconst maxInt = int(^uint(0) >> 1)\n\n// MethodConfig defines the configuration recommended by the service providers for a\n// particular method.\n//\n// Deprecated: Users should not use this struct. Service config should be received\n// through name resolver, as specified here\n// https://github.com/grpc/grpc/blob/master/doc/service_config.md\ntype MethodConfig = internalserviceconfig.MethodConfig\n\n// ServiceConfig is provided by the service provider and contains parameters for how\n// clients that connect to the service should behave.\n//\n// Deprecated: Users should not use this struct. Service config should be received\n// through name resolver, as specified here\n// https://github.com/grpc/grpc/blob/master/doc/service_config.md\ntype ServiceConfig struct {\n\tserviceconfig.Config\n\n\t// lbConfig is the service config's load balancing configuration.  If\n\t// lbConfig and LB are both present, lbConfig will be used.\n\tlbConfig serviceconfig.LoadBalancingConfig\n\n\t// Methods contains a map for the methods in this service.  If there is an\n\t// exact match for a method (i.e. /service/method) in the map, use the\n\t// corresponding MethodConfig.  If there's no exact match, look for the\n\t// default config for the service (/service/) and use the corresponding\n\t// MethodConfig if it exists.  Otherwise, the method has no MethodConfig to\n\t// use.\n\tMethods map[string]MethodConfig\n\n\t// If a retryThrottlingPolicy is provided, gRPC will automatically throttle\n\t// retry attempts and hedged RPCs when the client’s ratio of failures to\n\t// successes exceeds a threshold.\n\t//\n\t// For each server name, the gRPC client will maintain a token_count which is\n\t// initially set to maxTokens, and can take values between 0 and maxTokens.\n\t//\n\t// Every outgoing RPC (regardless of service or method invoked) will change\n\t// token_count as follows:\n\t//\n\t//   - Every failed RPC will decrement the token_count by 1.\n\t//   - Every successful RPC will increment the token_count by tokenRatio.\n\t//\n\t// If token_count is less than or equal to maxTokens / 2, then RPCs will not\n\t// be retried and hedged RPCs will not be sent.\n\tretryThrottling *retryThrottlingPolicy\n\t// healthCheckConfig must be set as one of the requirement to enable LB channel\n\t// health check.\n\thealthCheckConfig *healthCheckConfig\n\t// rawJSONString stores service config json string that get parsed into\n\t// this service config struct.\n\trawJSONString string\n}\n\n// healthCheckConfig defines the go-native version of the LB channel health check config.\ntype healthCheckConfig struct {\n\t// serviceName is the service name to use in the health-checking request.\n\tServiceName string\n}\n\ntype jsonRetryPolicy struct {\n\tMaxAttempts          int\n\tInitialBackoff       internalserviceconfig.Duration\n\tMaxBackoff           internalserviceconfig.Duration\n\tBackoffMultiplier    float64\n\tRetryableStatusCodes []codes.Code\n}\n\n// retryThrottlingPolicy defines the go-native version of the retry throttling\n// policy defined by the service config here:\n// https://github.com/grpc/proposal/blob/master/A6-client-retries.md#integration-with-service-config\ntype retryThrottlingPolicy struct {\n\t// The number of tokens starts at maxTokens. The token_count will always be\n\t// between 0 and maxTokens.\n\t//\n\t// This field is required and must be greater than zero.\n\tMaxTokens float64\n\t// The amount of tokens to add on each successful RPC. Typically this will\n\t// be some number between 0 and 1, e.g., 0.1.\n\t//\n\t// This field is required and must be greater than zero. Up to 3 decimal\n\t// places are supported.\n\tTokenRatio float64\n}\n\ntype jsonName struct {\n\tService string\n\tMethod  string\n}\n\nvar (\n\terrDuplicatedName             = errors.New(\"duplicated name\")\n\terrEmptyServiceNonEmptyMethod = errors.New(\"cannot combine empty 'service' and non-empty 'method'\")\n)\n\nfunc (j jsonName) generatePath() (string, error) {\n\tif j.Service == \"\" {\n\t\tif j.Method != \"\" {\n\t\t\treturn \"\", errEmptyServiceNonEmptyMethod\n\t\t}\n\t\treturn \"\", nil\n\t}\n\tres := \"/\" + j.Service + \"/\"\n\tif j.Method != \"\" {\n\t\tres += j.Method\n\t}\n\treturn res, nil\n}\n\n// TODO(lyuxuan): delete this struct after cleaning up old service config implementation.\ntype jsonMC struct {\n\tName                    *[]jsonName\n\tWaitForReady            *bool\n\tTimeout                 *internalserviceconfig.Duration\n\tMaxRequestMessageBytes  *int64\n\tMaxResponseMessageBytes *int64\n\tRetryPolicy             *jsonRetryPolicy\n}\n\n// TODO(lyuxuan): delete this struct after cleaning up old service config implementation.\ntype jsonSC struct {\n\tLoadBalancingPolicy *string\n\tLoadBalancingConfig *json.RawMessage\n\tMethodConfig        *[]jsonMC\n\tRetryThrottling     *retryThrottlingPolicy\n\tHealthCheckConfig   *healthCheckConfig\n}\n\nfunc init() {\n\tinternal.ParseServiceConfig = func(js string) *serviceconfig.ParseResult {\n\t\treturn parseServiceConfig(js, defaultMaxCallAttempts)\n\t}\n}\n\nfunc parseServiceConfig(js string, maxAttempts int) *serviceconfig.ParseResult {\n\tif len(js) == 0 {\n\t\treturn &serviceconfig.ParseResult{Err: fmt.Errorf(\"no JSON service config provided\")}\n\t}\n\tvar rsc jsonSC\n\terr := json.Unmarshal([]byte(js), &rsc)\n\tif err != nil {\n\t\tlogger.Warningf(\"grpc: unmarshalling service config %s: %v\", js, err)\n\t\treturn &serviceconfig.ParseResult{Err: err}\n\t}\n\tsc := ServiceConfig{\n\t\tMethods:           make(map[string]MethodConfig),\n\t\tretryThrottling:   rsc.RetryThrottling,\n\t\thealthCheckConfig: rsc.HealthCheckConfig,\n\t\trawJSONString:     js,\n\t}\n\tc := rsc.LoadBalancingConfig\n\tif c == nil {\n\t\tname := pickfirst.Name\n\t\tif rsc.LoadBalancingPolicy != nil {\n\t\t\tname = *rsc.LoadBalancingPolicy\n\t\t}\n\t\tif balancer.Get(name) == nil {\n\t\t\tname = pickfirst.Name\n\t\t}\n\t\tcfg := []map[string]any{{name: struct{}{}}}\n\t\tstrCfg, err := json.Marshal(cfg)\n\t\tif err != nil {\n\t\t\treturn &serviceconfig.ParseResult{Err: fmt.Errorf(\"unexpected error marshaling simple LB config: %w\", err)}\n\t\t}\n\t\tr := json.RawMessage(strCfg)\n\t\tc = &r\n\t}\n\tcfg, err := gracefulswitch.ParseConfig(*c)\n\tif err != nil {\n\t\treturn &serviceconfig.ParseResult{Err: err}\n\t}\n\tsc.lbConfig = cfg\n\n\tif rsc.MethodConfig == nil {\n\t\treturn &serviceconfig.ParseResult{Config: &sc}\n\t}\n\n\tpaths := map[string]struct{}{}\n\tfor _, m := range *rsc.MethodConfig {\n\t\tif m.Name == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tmc := MethodConfig{\n\t\t\tWaitForReady: m.WaitForReady,\n\t\t\tTimeout:      (*time.Duration)(m.Timeout),\n\t\t}\n\t\tif mc.RetryPolicy, err = convertRetryPolicy(m.RetryPolicy, maxAttempts); err != nil {\n\t\t\tlogger.Warningf(\"grpc: unmarshalling service config %s: %v\", js, err)\n\t\t\treturn &serviceconfig.ParseResult{Err: err}\n\t\t}\n\t\tif m.MaxRequestMessageBytes != nil {\n\t\t\tif *m.MaxRequestMessageBytes > int64(maxInt) {\n\t\t\t\tmc.MaxReqSize = newInt(maxInt)\n\t\t\t} else {\n\t\t\t\tmc.MaxReqSize = newInt(int(*m.MaxRequestMessageBytes))\n\t\t\t}\n\t\t}\n\t\tif m.MaxResponseMessageBytes != nil {\n\t\t\tif *m.MaxResponseMessageBytes > int64(maxInt) {\n\t\t\t\tmc.MaxRespSize = newInt(maxInt)\n\t\t\t} else {\n\t\t\t\tmc.MaxRespSize = newInt(int(*m.MaxResponseMessageBytes))\n\t\t\t}\n\t\t}\n\t\tfor i, n := range *m.Name {\n\t\t\tpath, err := n.generatePath()\n\t\t\tif err != nil {\n\t\t\t\tlogger.Warningf(\"grpc: error unmarshalling service config %s due to methodConfig[%d]: %v\", js, i, err)\n\t\t\t\treturn &serviceconfig.ParseResult{Err: err}\n\t\t\t}\n\n\t\t\tif _, ok := paths[path]; ok {\n\t\t\t\terr = errDuplicatedName\n\t\t\t\tlogger.Warningf(\"grpc: error unmarshalling service config %s due to methodConfig[%d]: %v\", js, i, err)\n\t\t\t\treturn &serviceconfig.ParseResult{Err: err}\n\t\t\t}\n\t\t\tpaths[path] = struct{}{}\n\t\t\tsc.Methods[path] = mc\n\t\t}\n\t}\n\n\tif sc.retryThrottling != nil {\n\t\tif mt := sc.retryThrottling.MaxTokens; mt <= 0 || mt > 1000 {\n\t\t\treturn &serviceconfig.ParseResult{Err: fmt.Errorf(\"invalid retry throttling config: maxTokens (%v) out of range (0, 1000]\", mt)}\n\t\t}\n\t\tif tr := sc.retryThrottling.TokenRatio; tr <= 0 {\n\t\t\treturn &serviceconfig.ParseResult{Err: fmt.Errorf(\"invalid retry throttling config: tokenRatio (%v) may not be negative\", tr)}\n\t\t}\n\t}\n\treturn &serviceconfig.ParseResult{Config: &sc}\n}\n\nfunc isValidRetryPolicy(jrp *jsonRetryPolicy) bool {\n\treturn jrp.MaxAttempts > 1 &&\n\t\tjrp.InitialBackoff > 0 &&\n\t\tjrp.MaxBackoff > 0 &&\n\t\tjrp.BackoffMultiplier > 0 &&\n\t\tlen(jrp.RetryableStatusCodes) > 0\n}\n\nfunc convertRetryPolicy(jrp *jsonRetryPolicy, maxAttempts int) (p *internalserviceconfig.RetryPolicy, err error) {\n\tif jrp == nil {\n\t\treturn nil, nil\n\t}\n\n\tif !isValidRetryPolicy(jrp) {\n\t\treturn nil, fmt.Errorf(\"invalid retry policy (%+v): \", jrp)\n\t}\n\n\tif jrp.MaxAttempts < maxAttempts {\n\t\tmaxAttempts = jrp.MaxAttempts\n\t}\n\trp := &internalserviceconfig.RetryPolicy{\n\t\tMaxAttempts:          maxAttempts,\n\t\tInitialBackoff:       time.Duration(jrp.InitialBackoff),\n\t\tMaxBackoff:           time.Duration(jrp.MaxBackoff),\n\t\tBackoffMultiplier:    jrp.BackoffMultiplier,\n\t\tRetryableStatusCodes: make(map[codes.Code]bool),\n\t}\n\tfor _, code := range jrp.RetryableStatusCodes {\n\t\trp.RetryableStatusCodes[code] = true\n\t}\n\treturn rp, nil\n}\n\nfunc minPointers(a, b *int) *int {\n\tif *a < *b {\n\t\treturn a\n\t}\n\treturn b\n}\n\nfunc getMaxSize(mcMax, doptMax *int, defaultVal int) *int {\n\tif mcMax == nil && doptMax == nil {\n\t\treturn &defaultVal\n\t}\n\tif mcMax != nil && doptMax != nil {\n\t\treturn minPointers(mcMax, doptMax)\n\t}\n\tif mcMax != nil {\n\t\treturn mcMax\n\t}\n\treturn doptMax\n}\n\nfunc newInt(b int) *int {\n\treturn &b\n}\n\nfunc init() {\n\tinternal.EqualServiceConfigForTesting = equalServiceConfig\n}\n\n// equalServiceConfig compares two configs. The rawJSONString field is ignored,\n// because they may diff in white spaces.\n//\n// If any of them is NOT *ServiceConfig, return false.\nfunc equalServiceConfig(a, b serviceconfig.Config) bool {\n\tif a == nil && b == nil {\n\t\treturn true\n\t}\n\taa, ok := a.(*ServiceConfig)\n\tif !ok {\n\t\treturn false\n\t}\n\tbb, ok := b.(*ServiceConfig)\n\tif !ok {\n\t\treturn false\n\t}\n\taaRaw := aa.rawJSONString\n\taa.rawJSONString = \"\"\n\tbbRaw := bb.rawJSONString\n\tbb.rawJSONString = \"\"\n\tdefer func() {\n\t\taa.rawJSONString = aaRaw\n\t\tbb.rawJSONString = bbRaw\n\t}()\n\t// Using reflect.DeepEqual instead of cmp.Equal because many balancer\n\t// configs are unexported, and cmp.Equal cannot compare unexported fields\n\t// from unexported structs.\n\treturn reflect.DeepEqual(aa, bb)\n}\n"
        },
        {
          "name": "service_config_test.go",
          "type": "blob",
          "size": 12.1220703125,
          "content": "/*\n *\n * Copyright 2017 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"testing\"\n\t\"time\"\n\n\t\"google.golang.org/grpc/balancer\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/internal/balancer/gracefulswitch\"\n\t\"google.golang.org/grpc/serviceconfig\"\n\n\tinternalserviceconfig \"google.golang.org/grpc/internal/serviceconfig\"\n)\n\ntype parseTestCase struct {\n\tname    string\n\tscjs    string\n\twantSC  *ServiceConfig\n\twantErr bool\n}\n\nfunc lbConfigFor(t *testing.T, name string, cfg serviceconfig.LoadBalancingConfig) serviceconfig.LoadBalancingConfig {\n\tif name == \"\" {\n\t\tname = \"pick_first\"\n\t\tcfg = struct {\n\t\t\tserviceconfig.LoadBalancingConfig\n\t\t}{}\n\t}\n\td := []map[string]any{{name: cfg}}\n\tstrCfg, err := json.Marshal(d)\n\tt.Logf(\"strCfg = %v\", string(strCfg))\n\tif err != nil {\n\t\tt.Fatalf(\"Error parsing config: %v\", err)\n\t}\n\tparsedCfg, err := gracefulswitch.ParseConfig(strCfg)\n\tif err != nil {\n\t\tt.Fatalf(\"Error parsing config: %v\", err)\n\t}\n\treturn parsedCfg\n}\n\nfunc runParseTests(t *testing.T, testCases []parseTestCase) {\n\tt.Helper()\n\tfor i, c := range testCases {\n\t\tname := c.name\n\t\tif name == \"\" {\n\t\t\tname = fmt.Sprint(i)\n\t\t}\n\t\tt.Run(name, func(t *testing.T) {\n\t\t\tscpr := parseServiceConfig(c.scjs, defaultMaxCallAttempts)\n\t\t\tvar sc *ServiceConfig\n\t\t\tsc, _ = scpr.Config.(*ServiceConfig)\n\t\t\tif !c.wantErr {\n\t\t\t\tc.wantSC.rawJSONString = c.scjs\n\t\t\t}\n\t\t\tif c.wantErr != (scpr.Err != nil) || !reflect.DeepEqual(sc, c.wantSC) {\n\t\t\t\tt.Fatalf(\"parseServiceConfig(%s) = %+v, %v, want %+v, %v\", c.scjs, sc, scpr.Err, c.wantSC, c.wantErr)\n\t\t\t}\n\t\t})\n\t}\n}\n\ntype pbbData struct {\n\tserviceconfig.LoadBalancingConfig\n\tFoo string\n\tBar int\n}\n\ntype parseBalancerBuilder struct{}\n\nfunc (parseBalancerBuilder) Name() string {\n\treturn \"pbb\"\n}\n\nfunc (parseBalancerBuilder) ParseConfig(c json.RawMessage) (serviceconfig.LoadBalancingConfig, error) {\n\td := pbbData{}\n\tif err := json.Unmarshal(c, &d); err != nil {\n\t\treturn nil, err\n\t}\n\treturn d, nil\n}\n\nfunc (parseBalancerBuilder) Build(balancer.ClientConn, balancer.BuildOptions) balancer.Balancer {\n\tpanic(\"unimplemented\")\n}\n\nfunc init() {\n\tbalancer.Register(parseBalancerBuilder{})\n}\n\nfunc (s) TestParseLBConfig(t *testing.T) {\n\ttestcases := []parseTestCase{\n\t\t{\n\t\t\tscjs: `{\n    \"loadBalancingConfig\": [{\"pbb\": { \"foo\": \"hi\" } }]\n}`,\n\t\t\twantSC: &ServiceConfig{\n\t\t\t\tMethods:  make(map[string]MethodConfig),\n\t\t\t\tlbConfig: lbConfigFor(t, \"pbb\", pbbData{Foo: \"hi\"}),\n\t\t\t},\n\t\t\twantErr: false,\n\t\t},\n\t}\n\trunParseTests(t, testcases)\n}\n\nfunc (s) TestParseNoLBConfigSupported(t *testing.T) {\n\t// We have a loadBalancingConfig field but will not encounter a supported\n\t// policy.  The config will be considered invalid in this case.\n\ttestcases := []parseTestCase{\n\t\t{\n\t\t\tscjs: `{\n    \"loadBalancingConfig\": [{\"not_a_balancer1\": {} }, {\"not_a_balancer2\": {}}]\n}`,\n\t\t\twantErr: true,\n\t\t}, {\n\t\t\tscjs:    `{\"loadBalancingConfig\": []}`,\n\t\t\twantErr: true,\n\t\t},\n\t}\n\trunParseTests(t, testcases)\n}\n\nfunc (s) TestParseLoadBalancer(t *testing.T) {\n\ttestcases := []parseTestCase{\n\t\t{\n\t\t\tscjs: `{\n    \"loadBalancingPolicy\": \"round_robin\",\n    \"methodConfig\": [\n        {\n            \"name\": [\n                {\n                    \"service\": \"foo\",\n                    \"method\": \"Bar\"\n                }\n            ],\n            \"waitForReady\": true\n        }\n    ]\n}`,\n\t\t\twantSC: &ServiceConfig{\n\t\t\t\tMethods: map[string]MethodConfig{\n\t\t\t\t\t\"/foo/Bar\": {\n\t\t\t\t\t\tWaitForReady: newBool(true),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tlbConfig: lbConfigFor(t, \"round_robin\", nil),\n\t\t\t},\n\t\t\twantErr: false,\n\t\t},\n\t\t{\n\t\t\tscjs: `{\n    \"loadBalancingPolicy\": 1,\n    \"methodConfig\": [\n        {\n            \"name\": [\n                {\n                    \"service\": \"foo\",\n                    \"method\": \"Bar\"\n                }\n            ],\n            \"waitForReady\": false\n        }\n    ]\n}`,\n\t\t\twantErr: true,\n\t\t},\n\t}\n\trunParseTests(t, testcases)\n}\n\nfunc (s) TestParseWaitForReady(t *testing.T) {\n\ttestcases := []parseTestCase{\n\t\t{\n\t\t\tscjs: `{\n    \"methodConfig\": [\n        {\n            \"name\": [\n                {\n                    \"service\": \"foo\",\n                    \"method\": \"Bar\"\n                }\n            ],\n            \"waitForReady\": true\n        }\n    ]\n}`,\n\t\t\twantSC: &ServiceConfig{\n\t\t\t\tMethods: map[string]MethodConfig{\n\t\t\t\t\t\"/foo/Bar\": {\n\t\t\t\t\t\tWaitForReady: newBool(true),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tlbConfig: lbConfigFor(t, \"\", nil),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tscjs: `{\n    \"methodConfig\": [\n        {\n            \"name\": [\n                {\n                    \"service\": \"foo\",\n                    \"method\": \"Bar\"\n                }\n            ],\n            \"waitForReady\": false\n        }\n    ]\n}`,\n\t\t\twantSC: &ServiceConfig{\n\t\t\t\tMethods: map[string]MethodConfig{\n\t\t\t\t\t\"/foo/Bar\": {\n\t\t\t\t\t\tWaitForReady: newBool(false),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tlbConfig: lbConfigFor(t, \"\", nil),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tscjs: `{\n    \"methodConfig\": [\n        {\n            \"name\": [\n                {\n                    \"service\": \"foo\",\n                    \"method\": \"Bar\"\n                }\n            ],\n            \"waitForReady\": fall\n        },\n        {\n            \"name\": [\n                {\n                    \"service\": \"foo\",\n                    \"method\": \"Bar\"\n                }\n            ],\n            \"waitForReady\": true\n        }\n    ]\n}`,\n\t\t\twantErr: true,\n\t\t},\n\t}\n\n\trunParseTests(t, testcases)\n}\n\nfunc (s) TestParseTimeOut(t *testing.T) {\n\ttestcases := []parseTestCase{\n\t\t{\n\t\t\tscjs: `{\n    \"methodConfig\": [\n        {\n            \"name\": [\n                {\n                    \"service\": \"foo\",\n                    \"method\": \"Bar\"\n                }\n            ],\n            \"timeout\": \"1s\"\n        }\n    ]\n}`,\n\t\t\twantSC: &ServiceConfig{\n\t\t\t\tMethods: map[string]MethodConfig{\n\t\t\t\t\t\"/foo/Bar\": {\n\t\t\t\t\t\tTimeout: newDuration(time.Second),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tlbConfig: lbConfigFor(t, \"\", nil),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tscjs: `{\n    \"methodConfig\": [\n        {\n            \"name\": [\n                {\n                    \"service\": \"foo\",\n                    \"method\": \"Bar\"\n                }\n            ],\n            \"timeout\": \"3c\"\n        }\n    ]\n}`,\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tscjs: `{\n    \"methodConfig\": [\n        {\n            \"name\": [\n                {\n                    \"service\": \"foo\",\n                    \"method\": \"Bar\"\n                }\n            ],\n            \"timeout\": \"3c\"\n        },\n        {\n            \"name\": [\n                {\n                    \"service\": \"foo\",\n                    \"method\": \"Bar\"\n                }\n            ],\n            \"timeout\": \"1s\"\n        }\n    ]\n}`,\n\t\t\twantErr: true,\n\t\t},\n\t}\n\n\trunParseTests(t, testcases)\n}\n\nfunc (s) TestParseMsgSize(t *testing.T) {\n\ttestcases := []parseTestCase{\n\t\t{\n\t\t\tscjs: `{\n    \"methodConfig\": [\n        {\n            \"name\": [\n                {\n                    \"service\": \"foo\",\n                    \"method\": \"Bar\"\n                }\n            ],\n            \"maxRequestMessageBytes\": 1024,\n            \"maxResponseMessageBytes\": 2048\n        }\n    ]\n}`,\n\t\t\twantSC: &ServiceConfig{\n\t\t\t\tMethods: map[string]MethodConfig{\n\t\t\t\t\t\"/foo/Bar\": {\n\t\t\t\t\t\tMaxReqSize:  newInt(1024),\n\t\t\t\t\t\tMaxRespSize: newInt(2048),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tlbConfig: lbConfigFor(t, \"\", nil),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tscjs: `{\n    \"methodConfig\": [\n        {\n            \"name\": [\n                {\n                    \"service\": \"foo\",\n                    \"method\": \"Bar\"\n                }\n            ],\n            \"maxRequestMessageBytes\": \"1024\",\n            \"maxResponseMessageBytes\": \"2048\"\n        },\n        {\n            \"name\": [\n                {\n                    \"service\": \"foo\",\n                    \"method\": \"Bar\"\n                }\n            ],\n            \"maxRequestMessageBytes\": 1024,\n            \"maxResponseMessageBytes\": 2048\n        }\n    ]\n}`,\n\t\t\twantErr: true,\n\t\t},\n\t}\n\n\trunParseTests(t, testcases)\n}\nfunc (s) TestParseDefaultMethodConfig(t *testing.T) {\n\tdc := &ServiceConfig{\n\t\tMethods: map[string]MethodConfig{\n\t\t\t\"\": {WaitForReady: newBool(true)},\n\t\t},\n\t\tlbConfig: lbConfigFor(t, \"\", nil),\n\t}\n\n\trunParseTests(t, []parseTestCase{\n\t\t{\n\t\t\tscjs: `{\n  \"methodConfig\": [{\n    \"name\": [{}],\n    \"waitForReady\": true\n  }]\n}`,\n\t\t\twantSC: dc,\n\t\t},\n\t\t{\n\t\t\tscjs: `{\n  \"methodConfig\": [{\n    \"name\": [{\"service\": null}],\n    \"waitForReady\": true\n  }]\n}`,\n\t\t\twantSC: dc,\n\t\t},\n\t\t{\n\t\t\tscjs: `{\n  \"methodConfig\": [{\n    \"name\": [{\"service\": \"\"}],\n    \"waitForReady\": true\n  }]\n}`,\n\t\t\twantSC: dc,\n\t\t},\n\t\t{\n\t\t\tscjs: `{\n  \"methodConfig\": [{\n    \"name\": [{\"method\": \"Bar\"}],\n    \"waitForReady\": true\n  }]\n}`,\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tscjs: `{\n  \"methodConfig\": [{\n    \"name\": [{\"service\": \"\", \"method\": \"Bar\"}],\n    \"waitForReady\": true\n  }]\n}`,\n\t\t\twantErr: true,\n\t\t},\n\t})\n}\n\nfunc (s) TestParseMethodConfigDuplicatedName(t *testing.T) {\n\trunParseTests(t, []parseTestCase{\n\t\t{\n\t\t\tscjs: `{\n  \"methodConfig\": [{\n    \"name\": [\n      {\"service\": \"foo\"},\n      {\"service\": \"foo\"}\n    ],\n    \"waitForReady\": true\n  }]\n}`,\n\t\t\twantErr: true,\n\t\t},\n\t})\n}\n\nfunc (s) TestParseRetryPolicy(t *testing.T) {\n\trunParseTests(t, []parseTestCase{\n\t\t{\n\t\t\tname: \"valid\",\n\t\t\tscjs: `{\n\t\t\t\t\"methodConfig\": [{\n\t\t\t\t  \"name\": [{\"service\": \"foo\"}],\n\t\t\t\t  \"retryPolicy\": {\n\t\t\t\t\t\"maxAttempts\": 2,\n\t\t\t\t\t\"initialBackoff\": \"2s\",\n\t\t\t\t\t\"maxBackoff\": \"10s\",\n\t\t\t\t\t\"backoffMultiplier\": 2,\n\t\t\t\t\t\"retryableStatusCodes\": [\"UNAVAILABLE\"]\n\t\t\t\t  }\n\t\t\t\t}]\n\t\t\t  }`,\n\t\t\twantSC: &ServiceConfig{\n\t\t\t\tMethods: map[string]MethodConfig{\n\t\t\t\t\t\"/foo/\": {\n\t\t\t\t\t\tRetryPolicy: &internalserviceconfig.RetryPolicy{\n\t\t\t\t\t\t\tMaxAttempts:          2,\n\t\t\t\t\t\t\tInitialBackoff:       2 * time.Second,\n\t\t\t\t\t\t\tMaxBackoff:           10 * time.Second,\n\t\t\t\t\t\t\tBackoffMultiplier:    2,\n\t\t\t\t\t\t\tRetryableStatusCodes: map[codes.Code]bool{codes.Unavailable: true},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tlbConfig: lbConfigFor(t, \"\", nil),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"negative maxAttempts\",\n\t\t\tscjs: `{\n\t\t\t\t\"methodConfig\": [{\n\t\t\t\t  \"name\": [{\"service\": \"foo\"}],\n\t\t\t\t  \"retryPolicy\": {\n\t\t\t\t\t  \"maxAttempts\": -1,\n\t\t\t\t\t  \"initialBackoff\": \"2s\",\n\t\t\t\t\t  \"maxBackoff\": \"10s\",\n\t\t\t\t\t  \"backoffMultiplier\": 2,\n\t\t\t\t\t  \"retryableStatusCodes\": [\"UNAVAILABLE\"]\n\t\t\t\t  }\n\t\t\t\t}]\n\t\t\t  }`,\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tname: \"missing maxAttempts\",\n\t\t\tscjs: `{\n\t\t\t\t\"methodConfig\": [{\n\t\t\t\t  \"name\": [{\"service\": \"foo\"}],\n\t\t\t\t  \"retryPolicy\": {\n\t\t\t\t\t  \"initialBackoff\": \"2s\",\n\t\t\t\t\t  \"maxBackoff\": \"10s\",\n\t\t\t\t\t  \"backoffMultiplier\": 2,\n\t\t\t\t\t  \"retryableStatusCodes\": [\"UNAVAILABLE\"]\n\t\t\t\t  }\n\t\t\t\t}]\n\t\t\t  }`,\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tname: \"zero initialBackoff\",\n\t\t\tscjs: `{\n\t\t\t\t\"methodConfig\": [{\n\t\t\t\t  \"name\": [{\"service\": \"foo\"}],\n\t\t\t\t  \"retryPolicy\": {\n\t\t\t\t\t  \"maxAttempts\": 2,\n\t\t\t\t\t  \"initialBackoff\": \"0s\",\n\t\t\t\t\t  \"maxBackoff\": \"10s\",\n\t\t\t\t\t  \"backoffMultiplier\": 2,\n\t\t\t\t\t  \"retryableStatusCodes\": [\"UNAVAILABLE\"]\n\t\t\t\t  }\n\t\t\t\t}]\n\t\t\t  }`,\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tname: \"zero maxBackoff\",\n\t\t\tscjs: `{\n\t\t\t\t\"methodConfig\": [{\n\t\t\t\t  \"name\": [{\"service\": \"foo\"}],\n\t\t\t\t  \"retryPolicy\": {\n\t\t\t\t\t  \"maxAttempts\": 2,\n\t\t\t\t\t  \"initialBackoff\": \"2s\",\n\t\t\t\t\t  \"maxBackoff\": \"0s\",\n\t\t\t\t\t  \"backoffMultiplier\": 2,\n\t\t\t\t\t  \"retryableStatusCodes\": [\"UNAVAILABLE\"]\n\t\t\t\t  }\n\t\t\t\t}]\n\t\t\t  }`,\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tname: \"zero backoffMultiplier\",\n\t\t\tscjs: `{\n\t\t\t\t\"methodConfig\": [{\n\t\t\t\t  \"name\": [{\"service\": \"foo\"}],\n\t\t\t\t  \"retryPolicy\": {\n\t\t\t\t\t  \"maxAttempts\": 2,\n\t\t\t\t\t  \"initialBackoff\": \"2s\",\n\t\t\t\t\t  \"maxBackoff\": \"10s\",\n\t\t\t\t\t  \"backoffMultiplier\": 0,\n\t\t\t\t\t  \"retryableStatusCodes\": [\"UNAVAILABLE\"]\n\t\t\t\t  }\n\t\t\t\t}]\n\t\t\t  }`,\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tname: \"no retryable codes\",\n\t\t\tscjs: `{\n\t\t\t\t\"methodConfig\": [{\n\t\t\t\t  \"name\": [{\"service\": \"foo\"}],\n\t\t\t\t  \"retryPolicy\": {\n\t\t\t\t\t  \"maxAttempts\": 2,\n\t\t\t\t\t  \"initialBackoff\": \"2s\",\n\t\t\t\t\t  \"maxBackoff\": \"10s\",\n\t\t\t\t\t  \"backoffMultiplier\": 2,\n\t\t\t\t\t  \"retryableStatusCodes\": []\n\t\t\t\t  }\n\t\t\t\t}]\n\t\t\t  }`,\n\t\t\twantErr: true,\n\t\t},\n\t})\n}\n\nfunc newBool(b bool) *bool {\n\treturn &b\n}\n\nfunc newDuration(b time.Duration) *time.Duration {\n\treturn &b\n}\n"
        },
        {
          "name": "serviceconfig",
          "type": "tree",
          "content": null
        },
        {
          "name": "stats",
          "type": "tree",
          "content": null
        },
        {
          "name": "status",
          "type": "tree",
          "content": null
        },
        {
          "name": "stream.go",
          "type": "blob",
          "size": 55.1416015625,
          "content": "/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"io\"\n\t\"math\"\n\trand \"math/rand/v2\"\n\t\"strconv\"\n\t\"sync\"\n\t\"time\"\n\n\t\"google.golang.org/grpc/balancer\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/encoding\"\n\t\"google.golang.org/grpc/internal\"\n\t\"google.golang.org/grpc/internal/balancerload\"\n\t\"google.golang.org/grpc/internal/binarylog\"\n\t\"google.golang.org/grpc/internal/channelz\"\n\t\"google.golang.org/grpc/internal/grpcutil\"\n\timetadata \"google.golang.org/grpc/internal/metadata\"\n\tiresolver \"google.golang.org/grpc/internal/resolver\"\n\t\"google.golang.org/grpc/internal/serviceconfig\"\n\tistatus \"google.golang.org/grpc/internal/status\"\n\t\"google.golang.org/grpc/internal/transport\"\n\t\"google.golang.org/grpc/mem\"\n\t\"google.golang.org/grpc/metadata\"\n\t\"google.golang.org/grpc/peer\"\n\t\"google.golang.org/grpc/stats\"\n\t\"google.golang.org/grpc/status\"\n)\n\nvar metadataFromOutgoingContextRaw = internal.FromOutgoingContextRaw.(func(context.Context) (metadata.MD, [][]string, bool))\n\n// StreamHandler defines the handler called by gRPC server to complete the\n// execution of a streaming RPC.\n//\n// If a StreamHandler returns an error, it should either be produced by the\n// status package, or be one of the context errors. Otherwise, gRPC will use\n// codes.Unknown as the status code and err.Error() as the status message of the\n// RPC.\ntype StreamHandler func(srv any, stream ServerStream) error\n\n// StreamDesc represents a streaming RPC service's method specification.  Used\n// on the server when registering services and on the client when initiating\n// new streams.\ntype StreamDesc struct {\n\t// StreamName and Handler are only used when registering handlers on a\n\t// server.\n\tStreamName string        // the name of the method excluding the service\n\tHandler    StreamHandler // the handler called for the method\n\n\t// ServerStreams and ClientStreams are used for registering handlers on a\n\t// server as well as defining RPC behavior when passed to NewClientStream\n\t// and ClientConn.NewStream.  At least one must be true.\n\tServerStreams bool // indicates the server can perform streaming sends\n\tClientStreams bool // indicates the client can perform streaming sends\n}\n\n// Stream defines the common interface a client or server stream has to satisfy.\n//\n// Deprecated: See ClientStream and ServerStream documentation instead.\ntype Stream interface {\n\t// Deprecated: See ClientStream and ServerStream documentation instead.\n\tContext() context.Context\n\t// Deprecated: See ClientStream and ServerStream documentation instead.\n\tSendMsg(m any) error\n\t// Deprecated: See ClientStream and ServerStream documentation instead.\n\tRecvMsg(m any) error\n}\n\n// ClientStream defines the client-side behavior of a streaming RPC.\n//\n// All errors returned from ClientStream methods are compatible with the\n// status package.\ntype ClientStream interface {\n\t// Header returns the header metadata received from the server if there\n\t// is any. It blocks if the metadata is not ready to read.  If the metadata\n\t// is nil and the error is also nil, then the stream was terminated without\n\t// headers, and the status can be discovered by calling RecvMsg.\n\tHeader() (metadata.MD, error)\n\t// Trailer returns the trailer metadata from the server, if there is any.\n\t// It must only be called after stream.CloseAndRecv has returned, or\n\t// stream.Recv has returned a non-nil error (including io.EOF).\n\tTrailer() metadata.MD\n\t// CloseSend closes the send direction of the stream. It closes the stream\n\t// when non-nil error is met. It is also not safe to call CloseSend\n\t// concurrently with SendMsg.\n\tCloseSend() error\n\t// Context returns the context for this stream.\n\t//\n\t// It should not be called until after Header or RecvMsg has returned. Once\n\t// called, subsequent client-side retries are disabled.\n\tContext() context.Context\n\t// SendMsg is generally called by generated code. On error, SendMsg aborts\n\t// the stream. If the error was generated by the client, the status is\n\t// returned directly; otherwise, io.EOF is returned and the status of\n\t// the stream may be discovered using RecvMsg. For unary or server-streaming\n\t// RPCs (StreamDesc.ClientStreams is false), a nil error is returned\n\t// unconditionally.\n\t//\n\t// SendMsg blocks until:\n\t//   - There is sufficient flow control to schedule m with the transport, or\n\t//   - The stream is done, or\n\t//   - The stream breaks.\n\t//\n\t// SendMsg does not wait until the message is received by the server. An\n\t// untimely stream closure may result in lost messages. To ensure delivery,\n\t// users should ensure the RPC completed successfully using RecvMsg.\n\t//\n\t// It is safe to have a goroutine calling SendMsg and another goroutine\n\t// calling RecvMsg on the same stream at the same time, but it is not safe\n\t// to call SendMsg on the same stream in different goroutines. It is also\n\t// not safe to call CloseSend concurrently with SendMsg.\n\t//\n\t// It is not safe to modify the message after calling SendMsg. Tracing\n\t// libraries and stats handlers may use the message lazily.\n\tSendMsg(m any) error\n\t// RecvMsg blocks until it receives a message into m or the stream is\n\t// done. It returns io.EOF when the stream completes successfully. On\n\t// any other error, the stream is aborted and the error contains the RPC\n\t// status.\n\t//\n\t// It is safe to have a goroutine calling SendMsg and another goroutine\n\t// calling RecvMsg on the same stream at the same time, but it is not\n\t// safe to call RecvMsg on the same stream in different goroutines.\n\tRecvMsg(m any) error\n}\n\n// NewStream creates a new Stream for the client side. This is typically\n// called by generated code. ctx is used for the lifetime of the stream.\n//\n// To ensure resources are not leaked due to the stream returned, one of the following\n// actions must be performed:\n//\n//  1. Call Close on the ClientConn.\n//  2. Cancel the context provided.\n//  3. Call RecvMsg until a non-nil error is returned. A protobuf-generated\n//     client-streaming RPC, for instance, might use the helper function\n//     CloseAndRecv (note that CloseSend does not Recv, therefore is not\n//     guaranteed to release all resources).\n//  4. Receive a non-nil, non-io.EOF error from Header or SendMsg.\n//\n// If none of the above happen, a goroutine and a context will be leaked, and grpc\n// will not call the optionally-configured stats handler with a stats.End message.\nfunc (cc *ClientConn) NewStream(ctx context.Context, desc *StreamDesc, method string, opts ...CallOption) (ClientStream, error) {\n\t// allow interceptor to see all applicable call options, which means those\n\t// configured as defaults from dial option as well as per-call options\n\topts = combine(cc.dopts.callOptions, opts)\n\n\tif cc.dopts.streamInt != nil {\n\t\treturn cc.dopts.streamInt(ctx, desc, cc, method, newClientStream, opts...)\n\t}\n\treturn newClientStream(ctx, desc, cc, method, opts...)\n}\n\n// NewClientStream is a wrapper for ClientConn.NewStream.\nfunc NewClientStream(ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, opts ...CallOption) (ClientStream, error) {\n\treturn cc.NewStream(ctx, desc, method, opts...)\n}\n\nfunc newClientStream(ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, opts ...CallOption) (_ ClientStream, err error) {\n\t// Start tracking the RPC for idleness purposes. This is where a stream is\n\t// created for both streaming and unary RPCs, and hence is a good place to\n\t// track active RPC count.\n\tif err := cc.idlenessMgr.OnCallBegin(); err != nil {\n\t\treturn nil, err\n\t}\n\t// Add a calloption, to decrement the active call count, that gets executed\n\t// when the RPC completes.\n\topts = append([]CallOption{OnFinish(func(error) { cc.idlenessMgr.OnCallEnd() })}, opts...)\n\n\tif md, added, ok := metadataFromOutgoingContextRaw(ctx); ok {\n\t\t// validate md\n\t\tif err := imetadata.Validate(md); err != nil {\n\t\t\treturn nil, status.Error(codes.Internal, err.Error())\n\t\t}\n\t\t// validate added\n\t\tfor _, kvs := range added {\n\t\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\t\tif err := imetadata.ValidatePair(kvs[i], kvs[i+1]); err != nil {\n\t\t\t\t\treturn nil, status.Error(codes.Internal, err.Error())\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif channelz.IsOn() {\n\t\tcc.incrCallsStarted()\n\t\tdefer func() {\n\t\t\tif err != nil {\n\t\t\t\tcc.incrCallsFailed()\n\t\t\t}\n\t\t}()\n\t}\n\t// Provide an opportunity for the first RPC to see the first service config\n\t// provided by the resolver.\n\tif err := cc.waitForResolvedAddrs(ctx); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar mc serviceconfig.MethodConfig\n\tvar onCommit func()\n\tnewStream := func(ctx context.Context, done func()) (iresolver.ClientStream, error) {\n\t\treturn newClientStreamWithParams(ctx, desc, cc, method, mc, onCommit, done, opts...)\n\t}\n\n\trpcInfo := iresolver.RPCInfo{Context: ctx, Method: method}\n\trpcConfig, err := cc.safeConfigSelector.SelectConfig(rpcInfo)\n\tif err != nil {\n\t\tif st, ok := status.FromError(err); ok {\n\t\t\t// Restrict the code to the list allowed by gRFC A54.\n\t\t\tif istatus.IsRestrictedControlPlaneCode(st) {\n\t\t\t\terr = status.Errorf(codes.Internal, \"config selector returned illegal status: %v\", err)\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\treturn nil, toRPCErr(err)\n\t}\n\n\tif rpcConfig != nil {\n\t\tif rpcConfig.Context != nil {\n\t\t\tctx = rpcConfig.Context\n\t\t}\n\t\tmc = rpcConfig.MethodConfig\n\t\tonCommit = rpcConfig.OnCommitted\n\t\tif rpcConfig.Interceptor != nil {\n\t\t\trpcInfo.Context = nil\n\t\t\tns := newStream\n\t\t\tnewStream = func(ctx context.Context, done func()) (iresolver.ClientStream, error) {\n\t\t\t\tcs, err := rpcConfig.Interceptor.NewStream(ctx, rpcInfo, done, ns)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, toRPCErr(err)\n\t\t\t\t}\n\t\t\t\treturn cs, nil\n\t\t\t}\n\t\t}\n\t}\n\n\treturn newStream(ctx, func() {})\n}\n\nfunc newClientStreamWithParams(ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, mc serviceconfig.MethodConfig, onCommit, doneFunc func(), opts ...CallOption) (_ iresolver.ClientStream, err error) {\n\tc := defaultCallInfo()\n\tif mc.WaitForReady != nil {\n\t\tc.failFast = !*mc.WaitForReady\n\t}\n\n\t// Possible context leak:\n\t// The cancel function for the child context we create will only be called\n\t// when RecvMsg returns a non-nil error, if the ClientConn is closed, or if\n\t// an error is generated by SendMsg.\n\t// https://github.com/grpc/grpc-go/issues/1818.\n\tvar cancel context.CancelFunc\n\tif mc.Timeout != nil && *mc.Timeout >= 0 {\n\t\tctx, cancel = context.WithTimeout(ctx, *mc.Timeout)\n\t} else {\n\t\tctx, cancel = context.WithCancel(ctx)\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tcancel()\n\t\t}\n\t}()\n\n\tfor _, o := range opts {\n\t\tif err := o.before(c); err != nil {\n\t\t\treturn nil, toRPCErr(err)\n\t\t}\n\t}\n\tc.maxSendMessageSize = getMaxSize(mc.MaxReqSize, c.maxSendMessageSize, defaultClientMaxSendMessageSize)\n\tc.maxReceiveMessageSize = getMaxSize(mc.MaxRespSize, c.maxReceiveMessageSize, defaultClientMaxReceiveMessageSize)\n\tif err := setCallInfoCodec(c); err != nil {\n\t\treturn nil, err\n\t}\n\n\tcallHdr := &transport.CallHdr{\n\t\tHost:           cc.authority,\n\t\tMethod:         method,\n\t\tContentSubtype: c.contentSubtype,\n\t\tDoneFunc:       doneFunc,\n\t}\n\n\t// Set our outgoing compression according to the UseCompressor CallOption, if\n\t// set.  In that case, also find the compressor from the encoding package.\n\t// Otherwise, use the compressor configured by the WithCompressor DialOption,\n\t// if set.\n\tvar cp Compressor\n\tvar comp encoding.Compressor\n\tif ct := c.compressorType; ct != \"\" {\n\t\tcallHdr.SendCompress = ct\n\t\tif ct != encoding.Identity {\n\t\t\tcomp = encoding.GetCompressor(ct)\n\t\t\tif comp == nil {\n\t\t\t\treturn nil, status.Errorf(codes.Internal, \"grpc: Compressor is not installed for requested grpc-encoding %q\", ct)\n\t\t\t}\n\t\t}\n\t} else if cc.dopts.cp != nil {\n\t\tcallHdr.SendCompress = cc.dopts.cp.Type()\n\t\tcp = cc.dopts.cp\n\t}\n\tif c.creds != nil {\n\t\tcallHdr.Creds = c.creds\n\t}\n\n\tcs := &clientStream{\n\t\tcallHdr:      callHdr,\n\t\tctx:          ctx,\n\t\tmethodConfig: &mc,\n\t\topts:         opts,\n\t\tcallInfo:     c,\n\t\tcc:           cc,\n\t\tdesc:         desc,\n\t\tcodec:        c.codec,\n\t\tcp:           cp,\n\t\tcomp:         comp,\n\t\tcancel:       cancel,\n\t\tfirstAttempt: true,\n\t\tonCommit:     onCommit,\n\t}\n\tif !cc.dopts.disableRetry {\n\t\tcs.retryThrottler = cc.retryThrottler.Load().(*retryThrottler)\n\t}\n\tif ml := binarylog.GetMethodLogger(method); ml != nil {\n\t\tcs.binlogs = append(cs.binlogs, ml)\n\t}\n\tif cc.dopts.binaryLogger != nil {\n\t\tif ml := cc.dopts.binaryLogger.GetMethodLogger(method); ml != nil {\n\t\t\tcs.binlogs = append(cs.binlogs, ml)\n\t\t}\n\t}\n\n\t// Pick the transport to use and create a new stream on the transport.\n\t// Assign cs.attempt upon success.\n\top := func(a *csAttempt) error {\n\t\tif err := a.getTransport(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := a.newStream(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// Because this operation is always called either here (while creating\n\t\t// the clientStream) or by the retry code while locked when replaying\n\t\t// the operation, it is safe to access cs.attempt directly.\n\t\tcs.attempt = a\n\t\treturn nil\n\t}\n\tif err := cs.withRetry(op, func() { cs.bufferForRetryLocked(0, op, nil) }); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif len(cs.binlogs) != 0 {\n\t\tmd, _ := metadata.FromOutgoingContext(ctx)\n\t\tlogEntry := &binarylog.ClientHeader{\n\t\t\tOnClientSide: true,\n\t\t\tHeader:       md,\n\t\t\tMethodName:   method,\n\t\t\tAuthority:    cs.cc.authority,\n\t\t}\n\t\tif deadline, ok := ctx.Deadline(); ok {\n\t\t\tlogEntry.Timeout = time.Until(deadline)\n\t\t\tif logEntry.Timeout < 0 {\n\t\t\t\tlogEntry.Timeout = 0\n\t\t\t}\n\t\t}\n\t\tfor _, binlog := range cs.binlogs {\n\t\t\tbinlog.Log(cs.ctx, logEntry)\n\t\t}\n\t}\n\n\tif desc != unaryStreamDesc {\n\t\t// Listen on cc and stream contexts to cleanup when the user closes the\n\t\t// ClientConn or cancels the stream context.  In all other cases, an error\n\t\t// should already be injected into the recv buffer by the transport, which\n\t\t// the client will eventually receive, and then we will cancel the stream's\n\t\t// context in clientStream.finish.\n\t\tgo func() {\n\t\t\tselect {\n\t\t\tcase <-cc.ctx.Done():\n\t\t\t\tcs.finish(ErrClientConnClosing)\n\t\t\tcase <-ctx.Done():\n\t\t\t\tcs.finish(toRPCErr(ctx.Err()))\n\t\t\t}\n\t\t}()\n\t}\n\treturn cs, nil\n}\n\n// newAttemptLocked creates a new csAttempt without a transport or stream.\nfunc (cs *clientStream) newAttemptLocked(isTransparent bool) (*csAttempt, error) {\n\tif err := cs.ctx.Err(); err != nil {\n\t\treturn nil, toRPCErr(err)\n\t}\n\tif err := cs.cc.ctx.Err(); err != nil {\n\t\treturn nil, ErrClientConnClosing\n\t}\n\n\tctx := newContextWithRPCInfo(cs.ctx, cs.callInfo.failFast, cs.callInfo.codec, cs.cp, cs.comp)\n\tmethod := cs.callHdr.Method\n\tvar beginTime time.Time\n\tshs := cs.cc.dopts.copts.StatsHandlers\n\tfor _, sh := range shs {\n\t\tctx = sh.TagRPC(ctx, &stats.RPCTagInfo{FullMethodName: method, FailFast: cs.callInfo.failFast})\n\t\tbeginTime = time.Now()\n\t\tbegin := &stats.Begin{\n\t\t\tClient:                    true,\n\t\t\tBeginTime:                 beginTime,\n\t\t\tFailFast:                  cs.callInfo.failFast,\n\t\t\tIsClientStream:            cs.desc.ClientStreams,\n\t\t\tIsServerStream:            cs.desc.ServerStreams,\n\t\t\tIsTransparentRetryAttempt: isTransparent,\n\t\t}\n\t\tsh.HandleRPC(ctx, begin)\n\t}\n\n\tvar trInfo *traceInfo\n\tif EnableTracing {\n\t\ttrInfo = &traceInfo{\n\t\t\ttr: newTrace(\"grpc.Sent.\"+methodFamily(method), method),\n\t\t\tfirstLine: firstLine{\n\t\t\t\tclient: true,\n\t\t\t},\n\t\t}\n\t\tif deadline, ok := ctx.Deadline(); ok {\n\t\t\ttrInfo.firstLine.deadline = time.Until(deadline)\n\t\t}\n\t\ttrInfo.tr.LazyLog(&trInfo.firstLine, false)\n\t\tctx = newTraceContext(ctx, trInfo.tr)\n\t}\n\n\tif cs.cc.parsedTarget.URL.Scheme == internal.GRPCResolverSchemeExtraMetadata {\n\t\t// Add extra metadata (metadata that will be added by transport) to context\n\t\t// so the balancer can see them.\n\t\tctx = grpcutil.WithExtraMetadata(ctx, metadata.Pairs(\n\t\t\t\"content-type\", grpcutil.ContentType(cs.callHdr.ContentSubtype),\n\t\t))\n\t}\n\n\treturn &csAttempt{\n\t\tctx:           ctx,\n\t\tbeginTime:     beginTime,\n\t\tcs:            cs,\n\t\tdc:            cs.cc.dopts.dc,\n\t\tstatsHandlers: shs,\n\t\ttrInfo:        trInfo,\n\t}, nil\n}\n\nfunc (a *csAttempt) getTransport() error {\n\tcs := a.cs\n\n\tvar err error\n\ta.t, a.pickResult, err = cs.cc.getTransport(a.ctx, cs.callInfo.failFast, cs.callHdr.Method)\n\tif err != nil {\n\t\tif de, ok := err.(dropError); ok {\n\t\t\terr = de.error\n\t\t\ta.drop = true\n\t\t}\n\t\treturn err\n\t}\n\tif a.trInfo != nil {\n\t\ta.trInfo.firstLine.SetRemoteAddr(a.t.RemoteAddr())\n\t}\n\treturn nil\n}\n\nfunc (a *csAttempt) newStream() error {\n\tcs := a.cs\n\tcs.callHdr.PreviousAttempts = cs.numRetries\n\n\t// Merge metadata stored in PickResult, if any, with existing call metadata.\n\t// It is safe to overwrite the csAttempt's context here, since all state\n\t// maintained in it are local to the attempt. When the attempt has to be\n\t// retried, a new instance of csAttempt will be created.\n\tif a.pickResult.Metadata != nil {\n\t\t// We currently do not have a function it the metadata package which\n\t\t// merges given metadata with existing metadata in a context. Existing\n\t\t// function `AppendToOutgoingContext()` takes a variadic argument of key\n\t\t// value pairs.\n\t\t//\n\t\t// TODO: Make it possible to retrieve key value pairs from metadata.MD\n\t\t// in a form passable to AppendToOutgoingContext(), or create a version\n\t\t// of AppendToOutgoingContext() that accepts a metadata.MD.\n\t\tmd, _ := metadata.FromOutgoingContext(a.ctx)\n\t\tmd = metadata.Join(md, a.pickResult.Metadata)\n\t\ta.ctx = metadata.NewOutgoingContext(a.ctx, md)\n\t}\n\n\ts, err := a.t.NewStream(a.ctx, cs.callHdr)\n\tif err != nil {\n\t\tnse, ok := err.(*transport.NewStreamError)\n\t\tif !ok {\n\t\t\t// Unexpected.\n\t\t\treturn err\n\t\t}\n\n\t\tif nse.AllowTransparentRetry {\n\t\t\ta.allowTransparentRetry = true\n\t\t}\n\n\t\t// Unwrap and convert error.\n\t\treturn toRPCErr(nse.Err)\n\t}\n\ta.s = s\n\ta.ctx = s.Context()\n\ta.p = &parser{r: s, bufferPool: a.cs.cc.dopts.copts.BufferPool}\n\treturn nil\n}\n\n// clientStream implements a client side Stream.\ntype clientStream struct {\n\tcallHdr  *transport.CallHdr\n\topts     []CallOption\n\tcallInfo *callInfo\n\tcc       *ClientConn\n\tdesc     *StreamDesc\n\n\tcodec baseCodec\n\tcp    Compressor\n\tcomp  encoding.Compressor\n\n\tcancel context.CancelFunc // cancels all attempts\n\n\tsentLast bool // sent an end stream\n\n\tmethodConfig *MethodConfig\n\n\tctx context.Context // the application's context, wrapped by stats/tracing\n\n\tretryThrottler *retryThrottler // The throttler active when the RPC began.\n\n\tbinlogs []binarylog.MethodLogger\n\t// serverHeaderBinlogged is a boolean for whether server header has been\n\t// logged. Server header will be logged when the first time one of those\n\t// happens: stream.Header(), stream.Recv().\n\t//\n\t// It's only read and used by Recv() and Header(), so it doesn't need to be\n\t// synchronized.\n\tserverHeaderBinlogged bool\n\n\tmu                      sync.Mutex\n\tfirstAttempt            bool // if true, transparent retry is valid\n\tnumRetries              int  // exclusive of transparent retry attempt(s)\n\tnumRetriesSincePushback int  // retries since pushback; to reset backoff\n\tfinished                bool // TODO: replace with atomic cmpxchg or sync.Once?\n\t// attempt is the active client stream attempt.\n\t// The only place where it is written is the newAttemptLocked method and this method never writes nil.\n\t// So, attempt can be nil only inside newClientStream function when clientStream is first created.\n\t// One of the first things done after clientStream's creation, is to call newAttemptLocked which either\n\t// assigns a non nil value to the attempt or returns an error. If an error is returned from newAttemptLocked,\n\t// then newClientStream calls finish on the clientStream and returns. So, finish method is the only\n\t// place where we need to check if the attempt is nil.\n\tattempt *csAttempt\n\t// TODO(hedging): hedging will have multiple attempts simultaneously.\n\tcommitted        bool // active attempt committed for retry?\n\tonCommit         func()\n\treplayBuffer     []replayOp // operations to replay on retry\n\treplayBufferSize int        // current size of replayBuffer\n}\n\ntype replayOp struct {\n\top      func(a *csAttempt) error\n\tcleanup func()\n}\n\n// csAttempt implements a single transport stream attempt within a\n// clientStream.\ntype csAttempt struct {\n\tctx        context.Context\n\tcs         *clientStream\n\tt          transport.ClientTransport\n\ts          *transport.ClientStream\n\tp          *parser\n\tpickResult balancer.PickResult\n\n\tfinished  bool\n\tdc        Decompressor\n\tdecomp    encoding.Compressor\n\tdecompSet bool\n\n\tmu sync.Mutex // guards trInfo.tr\n\t// trInfo may be nil (if EnableTracing is false).\n\t// trInfo.tr is set when created (if EnableTracing is true),\n\t// and cleared when the finish method is called.\n\ttrInfo *traceInfo\n\n\tstatsHandlers []stats.Handler\n\tbeginTime     time.Time\n\n\t// set for newStream errors that may be transparently retried\n\tallowTransparentRetry bool\n\t// set for pick errors that are returned as a status\n\tdrop bool\n}\n\nfunc (cs *clientStream) commitAttemptLocked() {\n\tif !cs.committed && cs.onCommit != nil {\n\t\tcs.onCommit()\n\t}\n\tcs.committed = true\n\tfor _, op := range cs.replayBuffer {\n\t\tif op.cleanup != nil {\n\t\t\top.cleanup()\n\t\t}\n\t}\n\tcs.replayBuffer = nil\n}\n\nfunc (cs *clientStream) commitAttempt() {\n\tcs.mu.Lock()\n\tcs.commitAttemptLocked()\n\tcs.mu.Unlock()\n}\n\n// shouldRetry returns nil if the RPC should be retried; otherwise it returns\n// the error that should be returned by the operation.  If the RPC should be\n// retried, the bool indicates whether it is being retried transparently.\nfunc (a *csAttempt) shouldRetry(err error) (bool, error) {\n\tcs := a.cs\n\n\tif cs.finished || cs.committed || a.drop {\n\t\t// RPC is finished or committed or was dropped by the picker; cannot retry.\n\t\treturn false, err\n\t}\n\tif a.s == nil && a.allowTransparentRetry {\n\t\treturn true, nil\n\t}\n\t// Wait for the trailers.\n\tunprocessed := false\n\tif a.s != nil {\n\t\t<-a.s.Done()\n\t\tunprocessed = a.s.Unprocessed()\n\t}\n\tif cs.firstAttempt && unprocessed {\n\t\t// First attempt, stream unprocessed: transparently retry.\n\t\treturn true, nil\n\t}\n\tif cs.cc.dopts.disableRetry {\n\t\treturn false, err\n\t}\n\n\tpushback := 0\n\thasPushback := false\n\tif a.s != nil {\n\t\tif !a.s.TrailersOnly() {\n\t\t\treturn false, err\n\t\t}\n\n\t\t// TODO(retry): Move down if the spec changes to not check server pushback\n\t\t// before considering this a failure for throttling.\n\t\tsps := a.s.Trailer()[\"grpc-retry-pushback-ms\"]\n\t\tif len(sps) == 1 {\n\t\t\tvar e error\n\t\t\tif pushback, e = strconv.Atoi(sps[0]); e != nil || pushback < 0 {\n\t\t\t\tchannelz.Infof(logger, cs.cc.channelz, \"Server retry pushback specified to abort (%q).\", sps[0])\n\t\t\t\tcs.retryThrottler.throttle() // This counts as a failure for throttling.\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t\thasPushback = true\n\t\t} else if len(sps) > 1 {\n\t\t\tchannelz.Warningf(logger, cs.cc.channelz, \"Server retry pushback specified multiple values (%q); not retrying.\", sps)\n\t\t\tcs.retryThrottler.throttle() // This counts as a failure for throttling.\n\t\t\treturn false, err\n\t\t}\n\t}\n\n\tvar code codes.Code\n\tif a.s != nil {\n\t\tcode = a.s.Status().Code()\n\t} else {\n\t\tcode = status.Code(err)\n\t}\n\n\trp := cs.methodConfig.RetryPolicy\n\tif rp == nil || !rp.RetryableStatusCodes[code] {\n\t\treturn false, err\n\t}\n\n\t// Note: the ordering here is important; we count this as a failure\n\t// only if the code matched a retryable code.\n\tif cs.retryThrottler.throttle() {\n\t\treturn false, err\n\t}\n\tif cs.numRetries+1 >= rp.MaxAttempts {\n\t\treturn false, err\n\t}\n\n\tvar dur time.Duration\n\tif hasPushback {\n\t\tdur = time.Millisecond * time.Duration(pushback)\n\t\tcs.numRetriesSincePushback = 0\n\t} else {\n\t\tfact := math.Pow(rp.BackoffMultiplier, float64(cs.numRetriesSincePushback))\n\t\tcur := min(float64(rp.InitialBackoff)*fact, float64(rp.MaxBackoff))\n\t\t// Apply jitter by multiplying with a random factor between 0.8 and 1.2\n\t\tcur *= 0.8 + 0.4*rand.Float64()\n\t\tdur = time.Duration(int64(cur))\n\t\tcs.numRetriesSincePushback++\n\t}\n\n\t// TODO(dfawley): we could eagerly fail here if dur puts us past the\n\t// deadline, but unsure if it is worth doing.\n\tt := time.NewTimer(dur)\n\tselect {\n\tcase <-t.C:\n\t\tcs.numRetries++\n\t\treturn false, nil\n\tcase <-cs.ctx.Done():\n\t\tt.Stop()\n\t\treturn false, status.FromContextError(cs.ctx.Err()).Err()\n\t}\n}\n\n// Returns nil if a retry was performed and succeeded; error otherwise.\nfunc (cs *clientStream) retryLocked(attempt *csAttempt, lastErr error) error {\n\tfor {\n\t\tattempt.finish(toRPCErr(lastErr))\n\t\tisTransparent, err := attempt.shouldRetry(lastErr)\n\t\tif err != nil {\n\t\t\tcs.commitAttemptLocked()\n\t\t\treturn err\n\t\t}\n\t\tcs.firstAttempt = false\n\t\tattempt, err = cs.newAttemptLocked(isTransparent)\n\t\tif err != nil {\n\t\t\t// Only returns error if the clientconn is closed or the context of\n\t\t\t// the stream is canceled.\n\t\t\treturn err\n\t\t}\n\t\t// Note that the first op in replayBuffer always sets cs.attempt\n\t\t// if it is able to pick a transport and create a stream.\n\t\tif lastErr = cs.replayBufferLocked(attempt); lastErr == nil {\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\nfunc (cs *clientStream) Context() context.Context {\n\tcs.commitAttempt()\n\t// No need to lock before using attempt, since we know it is committed and\n\t// cannot change.\n\tif cs.attempt.s != nil {\n\t\treturn cs.attempt.s.Context()\n\t}\n\treturn cs.ctx\n}\n\nfunc (cs *clientStream) withRetry(op func(a *csAttempt) error, onSuccess func()) error {\n\tcs.mu.Lock()\n\tfor {\n\t\tif cs.committed {\n\t\t\tcs.mu.Unlock()\n\t\t\t// toRPCErr is used in case the error from the attempt comes from\n\t\t\t// NewClientStream, which intentionally doesn't return a status\n\t\t\t// error to allow for further inspection; all other errors should\n\t\t\t// already be status errors.\n\t\t\treturn toRPCErr(op(cs.attempt))\n\t\t}\n\t\tif len(cs.replayBuffer) == 0 {\n\t\t\t// For the first op, which controls creation of the stream and\n\t\t\t// assigns cs.attempt, we need to create a new attempt inline\n\t\t\t// before executing the first op.  On subsequent ops, the attempt\n\t\t\t// is created immediately before replaying the ops.\n\t\t\tvar err error\n\t\t\tif cs.attempt, err = cs.newAttemptLocked(false /* isTransparent */); err != nil {\n\t\t\t\tcs.mu.Unlock()\n\t\t\t\tcs.finish(err)\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\ta := cs.attempt\n\t\tcs.mu.Unlock()\n\t\terr := op(a)\n\t\tcs.mu.Lock()\n\t\tif a != cs.attempt {\n\t\t\t// We started another attempt already.\n\t\t\tcontinue\n\t\t}\n\t\tif err == io.EOF {\n\t\t\t<-a.s.Done()\n\t\t}\n\t\tif err == nil || (err == io.EOF && a.s.Status().Code() == codes.OK) {\n\t\t\tonSuccess()\n\t\t\tcs.mu.Unlock()\n\t\t\treturn err\n\t\t}\n\t\tif err := cs.retryLocked(a, err); err != nil {\n\t\t\tcs.mu.Unlock()\n\t\t\treturn err\n\t\t}\n\t}\n}\n\nfunc (cs *clientStream) Header() (metadata.MD, error) {\n\tvar m metadata.MD\n\terr := cs.withRetry(func(a *csAttempt) error {\n\t\tvar err error\n\t\tm, err = a.s.Header()\n\t\treturn toRPCErr(err)\n\t}, cs.commitAttemptLocked)\n\n\tif m == nil && err == nil {\n\t\t// The stream ended with success.  Finish the clientStream.\n\t\terr = io.EOF\n\t}\n\n\tif err != nil {\n\t\tcs.finish(err)\n\t\t// Do not return the error.  The user should get it by calling Recv().\n\t\treturn nil, nil\n\t}\n\n\tif len(cs.binlogs) != 0 && !cs.serverHeaderBinlogged && m != nil {\n\t\t// Only log if binary log is on and header has not been logged, and\n\t\t// there is actually headers to log.\n\t\tlogEntry := &binarylog.ServerHeader{\n\t\t\tOnClientSide: true,\n\t\t\tHeader:       m,\n\t\t\tPeerAddr:     nil,\n\t\t}\n\t\tif peer, ok := peer.FromContext(cs.Context()); ok {\n\t\t\tlogEntry.PeerAddr = peer.Addr\n\t\t}\n\t\tcs.serverHeaderBinlogged = true\n\t\tfor _, binlog := range cs.binlogs {\n\t\t\tbinlog.Log(cs.ctx, logEntry)\n\t\t}\n\t}\n\n\treturn m, nil\n}\n\nfunc (cs *clientStream) Trailer() metadata.MD {\n\t// On RPC failure, we never need to retry, because usage requires that\n\t// RecvMsg() returned a non-nil error before calling this function is valid.\n\t// We would have retried earlier if necessary.\n\t//\n\t// Commit the attempt anyway, just in case users are not following those\n\t// directions -- it will prevent races and should not meaningfully impact\n\t// performance.\n\tcs.commitAttempt()\n\tif cs.attempt.s == nil {\n\t\treturn nil\n\t}\n\treturn cs.attempt.s.Trailer()\n}\n\nfunc (cs *clientStream) replayBufferLocked(attempt *csAttempt) error {\n\tfor _, f := range cs.replayBuffer {\n\t\tif err := f.op(attempt); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (cs *clientStream) bufferForRetryLocked(sz int, op func(a *csAttempt) error, cleanup func()) {\n\t// Note: we still will buffer if retry is disabled (for transparent retries).\n\tif cs.committed {\n\t\treturn\n\t}\n\tcs.replayBufferSize += sz\n\tif cs.replayBufferSize > cs.callInfo.maxRetryRPCBufferSize {\n\t\tcs.commitAttemptLocked()\n\t\tcleanup()\n\t\treturn\n\t}\n\tcs.replayBuffer = append(cs.replayBuffer, replayOp{op: op, cleanup: cleanup})\n}\n\nfunc (cs *clientStream) SendMsg(m any) (err error) {\n\tdefer func() {\n\t\tif err != nil && err != io.EOF {\n\t\t\t// Call finish on the client stream for errors generated by this SendMsg\n\t\t\t// call, as these indicate problems created by this client.  (Transport\n\t\t\t// errors are converted to an io.EOF error in csAttempt.sendMsg; the real\n\t\t\t// error will be returned from RecvMsg eventually in that case, or be\n\t\t\t// retried.)\n\t\t\tcs.finish(err)\n\t\t}\n\t}()\n\tif cs.sentLast {\n\t\treturn status.Errorf(codes.Internal, \"SendMsg called after CloseSend\")\n\t}\n\tif !cs.desc.ClientStreams {\n\t\tcs.sentLast = true\n\t}\n\n\t// load hdr, payload, data\n\thdr, data, payload, pf, err := prepareMsg(m, cs.codec, cs.cp, cs.comp, cs.cc.dopts.copts.BufferPool)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdefer func() {\n\t\tdata.Free()\n\t\t// only free payload if compression was made, and therefore it is a different set\n\t\t// of buffers from data.\n\t\tif pf.isCompressed() {\n\t\t\tpayload.Free()\n\t\t}\n\t}()\n\n\tdataLen := data.Len()\n\tpayloadLen := payload.Len()\n\t// TODO(dfawley): should we be checking len(data) instead?\n\tif payloadLen > *cs.callInfo.maxSendMessageSize {\n\t\treturn status.Errorf(codes.ResourceExhausted, \"trying to send message larger than max (%d vs. %d)\", payloadLen, *cs.callInfo.maxSendMessageSize)\n\t}\n\n\t// always take an extra ref in case data == payload (i.e. when the data isn't\n\t// compressed). The original ref will always be freed by the deferred free above.\n\tpayload.Ref()\n\top := func(a *csAttempt) error {\n\t\treturn a.sendMsg(m, hdr, payload, dataLen, payloadLen)\n\t}\n\n\t// onSuccess is invoked when the op is captured for a subsequent retry. If the\n\t// stream was established by a previous message and therefore retries are\n\t// disabled, onSuccess will not be invoked, and payloadRef can be freed\n\t// immediately.\n\tonSuccessCalled := false\n\terr = cs.withRetry(op, func() {\n\t\tcs.bufferForRetryLocked(len(hdr)+payloadLen, op, payload.Free)\n\t\tonSuccessCalled = true\n\t})\n\tif !onSuccessCalled {\n\t\tpayload.Free()\n\t}\n\tif len(cs.binlogs) != 0 && err == nil {\n\t\tcm := &binarylog.ClientMessage{\n\t\t\tOnClientSide: true,\n\t\t\tMessage:      data.Materialize(),\n\t\t}\n\t\tfor _, binlog := range cs.binlogs {\n\t\t\tbinlog.Log(cs.ctx, cm)\n\t\t}\n\t}\n\treturn err\n}\n\nfunc (cs *clientStream) RecvMsg(m any) error {\n\tif len(cs.binlogs) != 0 && !cs.serverHeaderBinlogged {\n\t\t// Call Header() to binary log header if it's not already logged.\n\t\tcs.Header()\n\t}\n\tvar recvInfo *payloadInfo\n\tif len(cs.binlogs) != 0 {\n\t\trecvInfo = &payloadInfo{}\n\t\tdefer recvInfo.free()\n\t}\n\terr := cs.withRetry(func(a *csAttempt) error {\n\t\treturn a.recvMsg(m, recvInfo)\n\t}, cs.commitAttemptLocked)\n\tif len(cs.binlogs) != 0 && err == nil {\n\t\tsm := &binarylog.ServerMessage{\n\t\t\tOnClientSide: true,\n\t\t\tMessage:      recvInfo.uncompressedBytes.Materialize(),\n\t\t}\n\t\tfor _, binlog := range cs.binlogs {\n\t\t\tbinlog.Log(cs.ctx, sm)\n\t\t}\n\t}\n\tif err != nil || !cs.desc.ServerStreams {\n\t\t// err != nil or non-server-streaming indicates end of stream.\n\t\tcs.finish(err)\n\t}\n\treturn err\n}\n\nfunc (cs *clientStream) CloseSend() error {\n\tif cs.sentLast {\n\t\t// TODO: return an error and finish the stream instead, due to API misuse?\n\t\treturn nil\n\t}\n\tcs.sentLast = true\n\top := func(a *csAttempt) error {\n\t\ta.s.Write(nil, nil, &transport.WriteOptions{Last: true})\n\t\t// Always return nil; io.EOF is the only error that might make sense\n\t\t// instead, but there is no need to signal the client to call RecvMsg\n\t\t// as the only use left for the stream after CloseSend is to call\n\t\t// RecvMsg.  This also matches historical behavior.\n\t\treturn nil\n\t}\n\tcs.withRetry(op, func() { cs.bufferForRetryLocked(0, op, nil) })\n\tif len(cs.binlogs) != 0 {\n\t\tchc := &binarylog.ClientHalfClose{\n\t\t\tOnClientSide: true,\n\t\t}\n\t\tfor _, binlog := range cs.binlogs {\n\t\t\tbinlog.Log(cs.ctx, chc)\n\t\t}\n\t}\n\t// We never returned an error here for reasons.\n\treturn nil\n}\n\nfunc (cs *clientStream) finish(err error) {\n\tif err == io.EOF {\n\t\t// Ending a stream with EOF indicates a success.\n\t\terr = nil\n\t}\n\tcs.mu.Lock()\n\tif cs.finished {\n\t\tcs.mu.Unlock()\n\t\treturn\n\t}\n\tcs.finished = true\n\tfor _, onFinish := range cs.callInfo.onFinish {\n\t\tonFinish(err)\n\t}\n\tcs.commitAttemptLocked()\n\tif cs.attempt != nil {\n\t\tcs.attempt.finish(err)\n\t\t// after functions all rely upon having a stream.\n\t\tif cs.attempt.s != nil {\n\t\t\tfor _, o := range cs.opts {\n\t\t\t\to.after(cs.callInfo, cs.attempt)\n\t\t\t}\n\t\t}\n\t}\n\n\tcs.mu.Unlock()\n\t// Only one of cancel or trailer needs to be logged.\n\tif len(cs.binlogs) != 0 {\n\t\tswitch err {\n\t\tcase errContextCanceled, errContextDeadline, ErrClientConnClosing:\n\t\t\tc := &binarylog.Cancel{\n\t\t\t\tOnClientSide: true,\n\t\t\t}\n\t\t\tfor _, binlog := range cs.binlogs {\n\t\t\t\tbinlog.Log(cs.ctx, c)\n\t\t\t}\n\t\tdefault:\n\t\t\tlogEntry := &binarylog.ServerTrailer{\n\t\t\t\tOnClientSide: true,\n\t\t\t\tTrailer:      cs.Trailer(),\n\t\t\t\tErr:          err,\n\t\t\t}\n\t\t\tif peer, ok := peer.FromContext(cs.Context()); ok {\n\t\t\t\tlogEntry.PeerAddr = peer.Addr\n\t\t\t}\n\t\t\tfor _, binlog := range cs.binlogs {\n\t\t\t\tbinlog.Log(cs.ctx, logEntry)\n\t\t\t}\n\t\t}\n\t}\n\tif err == nil {\n\t\tcs.retryThrottler.successfulRPC()\n\t}\n\tif channelz.IsOn() {\n\t\tif err != nil {\n\t\t\tcs.cc.incrCallsFailed()\n\t\t} else {\n\t\t\tcs.cc.incrCallsSucceeded()\n\t\t}\n\t}\n\tcs.cancel()\n}\n\nfunc (a *csAttempt) sendMsg(m any, hdr []byte, payld mem.BufferSlice, dataLength, payloadLength int) error {\n\tcs := a.cs\n\tif a.trInfo != nil {\n\t\ta.mu.Lock()\n\t\tif a.trInfo.tr != nil {\n\t\t\ta.trInfo.tr.LazyLog(&payload{sent: true, msg: m}, true)\n\t\t}\n\t\ta.mu.Unlock()\n\t}\n\tif err := a.s.Write(hdr, payld, &transport.WriteOptions{Last: !cs.desc.ClientStreams}); err != nil {\n\t\tif !cs.desc.ClientStreams {\n\t\t\t// For non-client-streaming RPCs, we return nil instead of EOF on error\n\t\t\t// because the generated code requires it.  finish is not called; RecvMsg()\n\t\t\t// will call it with the stream's status independently.\n\t\t\treturn nil\n\t\t}\n\t\treturn io.EOF\n\t}\n\tif len(a.statsHandlers) != 0 {\n\t\tfor _, sh := range a.statsHandlers {\n\t\t\tsh.HandleRPC(a.ctx, outPayload(true, m, dataLength, payloadLength, time.Now()))\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (a *csAttempt) recvMsg(m any, payInfo *payloadInfo) (err error) {\n\tcs := a.cs\n\tif len(a.statsHandlers) != 0 && payInfo == nil {\n\t\tpayInfo = &payloadInfo{}\n\t\tdefer payInfo.free()\n\t}\n\n\tif !a.decompSet {\n\t\t// Block until we receive headers containing received message encoding.\n\t\tif ct := a.s.RecvCompress(); ct != \"\" && ct != encoding.Identity {\n\t\t\tif a.dc == nil || a.dc.Type() != ct {\n\t\t\t\t// No configured decompressor, or it does not match the incoming\n\t\t\t\t// message encoding; attempt to find a registered compressor that does.\n\t\t\t\ta.dc = nil\n\t\t\t\ta.decomp = encoding.GetCompressor(ct)\n\t\t\t}\n\t\t} else {\n\t\t\t// No compression is used; disable our decompressor.\n\t\t\ta.dc = nil\n\t\t}\n\t\t// Only initialize this state once per stream.\n\t\ta.decompSet = true\n\t}\n\tif err := recv(a.p, cs.codec, a.s, a.dc, m, *cs.callInfo.maxReceiveMessageSize, payInfo, a.decomp, false); err != nil {\n\t\tif err == io.EOF {\n\t\t\tif statusErr := a.s.Status().Err(); statusErr != nil {\n\t\t\t\treturn statusErr\n\t\t\t}\n\t\t\treturn io.EOF // indicates successful end of stream.\n\t\t}\n\n\t\treturn toRPCErr(err)\n\t}\n\tif a.trInfo != nil {\n\t\ta.mu.Lock()\n\t\tif a.trInfo.tr != nil {\n\t\t\ta.trInfo.tr.LazyLog(&payload{sent: false, msg: m}, true)\n\t\t}\n\t\ta.mu.Unlock()\n\t}\n\tfor _, sh := range a.statsHandlers {\n\t\tsh.HandleRPC(a.ctx, &stats.InPayload{\n\t\t\tClient:           true,\n\t\t\tRecvTime:         time.Now(),\n\t\t\tPayload:          m,\n\t\t\tWireLength:       payInfo.compressedLength + headerLen,\n\t\t\tCompressedLength: payInfo.compressedLength,\n\t\t\tLength:           payInfo.uncompressedBytes.Len(),\n\t\t})\n\t}\n\tif cs.desc.ServerStreams {\n\t\t// Subsequent messages should be received by subsequent RecvMsg calls.\n\t\treturn nil\n\t}\n\t// Special handling for non-server-stream rpcs.\n\t// This recv expects EOF or errors, so we don't collect inPayload.\n\tif err := recv(a.p, cs.codec, a.s, a.dc, m, *cs.callInfo.maxReceiveMessageSize, nil, a.decomp, false); err == io.EOF {\n\t\treturn a.s.Status().Err() // non-server streaming Recv returns nil on success\n\t} else if err != nil {\n\t\treturn toRPCErr(err)\n\t}\n\treturn toRPCErr(errors.New(\"grpc: client streaming protocol violation: get <nil>, want <EOF>\"))\n}\n\nfunc (a *csAttempt) finish(err error) {\n\ta.mu.Lock()\n\tif a.finished {\n\t\ta.mu.Unlock()\n\t\treturn\n\t}\n\ta.finished = true\n\tif err == io.EOF {\n\t\t// Ending a stream with EOF indicates a success.\n\t\terr = nil\n\t}\n\tvar tr metadata.MD\n\tif a.s != nil {\n\t\ta.s.Close(err)\n\t\ttr = a.s.Trailer()\n\t}\n\n\tif a.pickResult.Done != nil {\n\t\tbr := false\n\t\tif a.s != nil {\n\t\t\tbr = a.s.BytesReceived()\n\t\t}\n\t\ta.pickResult.Done(balancer.DoneInfo{\n\t\t\tErr:           err,\n\t\t\tTrailer:       tr,\n\t\t\tBytesSent:     a.s != nil,\n\t\t\tBytesReceived: br,\n\t\t\tServerLoad:    balancerload.Parse(tr),\n\t\t})\n\t}\n\tfor _, sh := range a.statsHandlers {\n\t\tend := &stats.End{\n\t\t\tClient:    true,\n\t\t\tBeginTime: a.beginTime,\n\t\t\tEndTime:   time.Now(),\n\t\t\tTrailer:   tr,\n\t\t\tError:     err,\n\t\t}\n\t\tsh.HandleRPC(a.ctx, end)\n\t}\n\tif a.trInfo != nil && a.trInfo.tr != nil {\n\t\tif err == nil {\n\t\t\ta.trInfo.tr.LazyPrintf(\"RPC: [OK]\")\n\t\t} else {\n\t\t\ta.trInfo.tr.LazyPrintf(\"RPC: [%v]\", err)\n\t\t\ta.trInfo.tr.SetError()\n\t\t}\n\t\ta.trInfo.tr.Finish()\n\t\ta.trInfo.tr = nil\n\t}\n\ta.mu.Unlock()\n}\n\n// newNonRetryClientStream creates a ClientStream with the specified transport, on the\n// given addrConn.\n//\n// It's expected that the given transport is either the same one in addrConn, or\n// is already closed. To avoid race, transport is specified separately, instead\n// of using ac.transport.\n//\n// Main difference between this and ClientConn.NewStream:\n// - no retry\n// - no service config (or wait for service config)\n// - no tracing or stats\nfunc newNonRetryClientStream(ctx context.Context, desc *StreamDesc, method string, t transport.ClientTransport, ac *addrConn, opts ...CallOption) (_ ClientStream, err error) {\n\tif t == nil {\n\t\t// TODO: return RPC error here?\n\t\treturn nil, errors.New(\"transport provided is nil\")\n\t}\n\t// defaultCallInfo contains unnecessary info(i.e. failfast, maxRetryRPCBufferSize), so we just initialize an empty struct.\n\tc := &callInfo{}\n\n\t// Possible context leak:\n\t// The cancel function for the child context we create will only be called\n\t// when RecvMsg returns a non-nil error, if the ClientConn is closed, or if\n\t// an error is generated by SendMsg.\n\t// https://github.com/grpc/grpc-go/issues/1818.\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tcancel()\n\t\t}\n\t}()\n\n\tfor _, o := range opts {\n\t\tif err := o.before(c); err != nil {\n\t\t\treturn nil, toRPCErr(err)\n\t\t}\n\t}\n\tc.maxReceiveMessageSize = getMaxSize(nil, c.maxReceiveMessageSize, defaultClientMaxReceiveMessageSize)\n\tc.maxSendMessageSize = getMaxSize(nil, c.maxSendMessageSize, defaultServerMaxSendMessageSize)\n\tif err := setCallInfoCodec(c); err != nil {\n\t\treturn nil, err\n\t}\n\n\tcallHdr := &transport.CallHdr{\n\t\tHost:           ac.cc.authority,\n\t\tMethod:         method,\n\t\tContentSubtype: c.contentSubtype,\n\t}\n\n\t// Set our outgoing compression according to the UseCompressor CallOption, if\n\t// set.  In that case, also find the compressor from the encoding package.\n\t// Otherwise, use the compressor configured by the WithCompressor DialOption,\n\t// if set.\n\tvar cp Compressor\n\tvar comp encoding.Compressor\n\tif ct := c.compressorType; ct != \"\" {\n\t\tcallHdr.SendCompress = ct\n\t\tif ct != encoding.Identity {\n\t\t\tcomp = encoding.GetCompressor(ct)\n\t\t\tif comp == nil {\n\t\t\t\treturn nil, status.Errorf(codes.Internal, \"grpc: Compressor is not installed for requested grpc-encoding %q\", ct)\n\t\t\t}\n\t\t}\n\t} else if ac.cc.dopts.cp != nil {\n\t\tcallHdr.SendCompress = ac.cc.dopts.cp.Type()\n\t\tcp = ac.cc.dopts.cp\n\t}\n\tif c.creds != nil {\n\t\tcallHdr.Creds = c.creds\n\t}\n\n\t// Use a special addrConnStream to avoid retry.\n\tas := &addrConnStream{\n\t\tcallHdr:  callHdr,\n\t\tac:       ac,\n\t\tctx:      ctx,\n\t\tcancel:   cancel,\n\t\topts:     opts,\n\t\tcallInfo: c,\n\t\tdesc:     desc,\n\t\tcodec:    c.codec,\n\t\tcp:       cp,\n\t\tcomp:     comp,\n\t\tt:        t,\n\t}\n\n\ts, err := as.t.NewStream(as.ctx, as.callHdr)\n\tif err != nil {\n\t\terr = toRPCErr(err)\n\t\treturn nil, err\n\t}\n\tas.s = s\n\tas.p = &parser{r: s, bufferPool: ac.dopts.copts.BufferPool}\n\tac.incrCallsStarted()\n\tif desc != unaryStreamDesc {\n\t\t// Listen on stream context to cleanup when the stream context is\n\t\t// canceled.  Also listen for the addrConn's context in case the\n\t\t// addrConn is closed or reconnects to a different address.  In all\n\t\t// other cases, an error should already be injected into the recv\n\t\t// buffer by the transport, which the client will eventually receive,\n\t\t// and then we will cancel the stream's context in\n\t\t// addrConnStream.finish.\n\t\tgo func() {\n\t\t\tac.mu.Lock()\n\t\t\tacCtx := ac.ctx\n\t\t\tac.mu.Unlock()\n\t\t\tselect {\n\t\t\tcase <-acCtx.Done():\n\t\t\t\tas.finish(status.Error(codes.Canceled, \"grpc: the SubConn is closing\"))\n\t\t\tcase <-ctx.Done():\n\t\t\t\tas.finish(toRPCErr(ctx.Err()))\n\t\t\t}\n\t\t}()\n\t}\n\treturn as, nil\n}\n\ntype addrConnStream struct {\n\ts         *transport.ClientStream\n\tac        *addrConn\n\tcallHdr   *transport.CallHdr\n\tcancel    context.CancelFunc\n\topts      []CallOption\n\tcallInfo  *callInfo\n\tt         transport.ClientTransport\n\tctx       context.Context\n\tsentLast  bool\n\tdesc      *StreamDesc\n\tcodec     baseCodec\n\tcp        Compressor\n\tcomp      encoding.Compressor\n\tdecompSet bool\n\tdc        Decompressor\n\tdecomp    encoding.Compressor\n\tp         *parser\n\tmu        sync.Mutex\n\tfinished  bool\n}\n\nfunc (as *addrConnStream) Header() (metadata.MD, error) {\n\tm, err := as.s.Header()\n\tif err != nil {\n\t\tas.finish(toRPCErr(err))\n\t}\n\treturn m, err\n}\n\nfunc (as *addrConnStream) Trailer() metadata.MD {\n\treturn as.s.Trailer()\n}\n\nfunc (as *addrConnStream) CloseSend() error {\n\tif as.sentLast {\n\t\t// TODO: return an error and finish the stream instead, due to API misuse?\n\t\treturn nil\n\t}\n\tas.sentLast = true\n\n\tas.s.Write(nil, nil, &transport.WriteOptions{Last: true})\n\t// Always return nil; io.EOF is the only error that might make sense\n\t// instead, but there is no need to signal the client to call RecvMsg\n\t// as the only use left for the stream after CloseSend is to call\n\t// RecvMsg.  This also matches historical behavior.\n\treturn nil\n}\n\nfunc (as *addrConnStream) Context() context.Context {\n\treturn as.s.Context()\n}\n\nfunc (as *addrConnStream) SendMsg(m any) (err error) {\n\tdefer func() {\n\t\tif err != nil && err != io.EOF {\n\t\t\t// Call finish on the client stream for errors generated by this SendMsg\n\t\t\t// call, as these indicate problems created by this client.  (Transport\n\t\t\t// errors are converted to an io.EOF error in csAttempt.sendMsg; the real\n\t\t\t// error will be returned from RecvMsg eventually in that case, or be\n\t\t\t// retried.)\n\t\t\tas.finish(err)\n\t\t}\n\t}()\n\tif as.sentLast {\n\t\treturn status.Errorf(codes.Internal, \"SendMsg called after CloseSend\")\n\t}\n\tif !as.desc.ClientStreams {\n\t\tas.sentLast = true\n\t}\n\n\t// load hdr, payload, data\n\thdr, data, payload, pf, err := prepareMsg(m, as.codec, as.cp, as.comp, as.ac.dopts.copts.BufferPool)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdefer func() {\n\t\tdata.Free()\n\t\t// only free payload if compression was made, and therefore it is a different set\n\t\t// of buffers from data.\n\t\tif pf.isCompressed() {\n\t\t\tpayload.Free()\n\t\t}\n\t}()\n\n\t// TODO(dfawley): should we be checking len(data) instead?\n\tif payload.Len() > *as.callInfo.maxSendMessageSize {\n\t\treturn status.Errorf(codes.ResourceExhausted, \"trying to send message larger than max (%d vs. %d)\", payload.Len(), *as.callInfo.maxSendMessageSize)\n\t}\n\n\tif err := as.s.Write(hdr, payload, &transport.WriteOptions{Last: !as.desc.ClientStreams}); err != nil {\n\t\tif !as.desc.ClientStreams {\n\t\t\t// For non-client-streaming RPCs, we return nil instead of EOF on error\n\t\t\t// because the generated code requires it.  finish is not called; RecvMsg()\n\t\t\t// will call it with the stream's status independently.\n\t\t\treturn nil\n\t\t}\n\t\treturn io.EOF\n\t}\n\n\treturn nil\n}\n\nfunc (as *addrConnStream) RecvMsg(m any) (err error) {\n\tdefer func() {\n\t\tif err != nil || !as.desc.ServerStreams {\n\t\t\t// err != nil or non-server-streaming indicates end of stream.\n\t\t\tas.finish(err)\n\t\t}\n\t}()\n\n\tif !as.decompSet {\n\t\t// Block until we receive headers containing received message encoding.\n\t\tif ct := as.s.RecvCompress(); ct != \"\" && ct != encoding.Identity {\n\t\t\tif as.dc == nil || as.dc.Type() != ct {\n\t\t\t\t// No configured decompressor, or it does not match the incoming\n\t\t\t\t// message encoding; attempt to find a registered compressor that does.\n\t\t\t\tas.dc = nil\n\t\t\t\tas.decomp = encoding.GetCompressor(ct)\n\t\t\t}\n\t\t} else {\n\t\t\t// No compression is used; disable our decompressor.\n\t\t\tas.dc = nil\n\t\t}\n\t\t// Only initialize this state once per stream.\n\t\tas.decompSet = true\n\t}\n\tif err := recv(as.p, as.codec, as.s, as.dc, m, *as.callInfo.maxReceiveMessageSize, nil, as.decomp, false); err != nil {\n\t\tif err == io.EOF {\n\t\t\tif statusErr := as.s.Status().Err(); statusErr != nil {\n\t\t\t\treturn statusErr\n\t\t\t}\n\t\t\treturn io.EOF // indicates successful end of stream.\n\t\t}\n\t\treturn toRPCErr(err)\n\t}\n\n\tif as.desc.ServerStreams {\n\t\t// Subsequent messages should be received by subsequent RecvMsg calls.\n\t\treturn nil\n\t}\n\n\t// Special handling for non-server-stream rpcs.\n\t// This recv expects EOF or errors, so we don't collect inPayload.\n\tif err := recv(as.p, as.codec, as.s, as.dc, m, *as.callInfo.maxReceiveMessageSize, nil, as.decomp, false); err == io.EOF {\n\t\treturn as.s.Status().Err() // non-server streaming Recv returns nil on success\n\t} else if err != nil {\n\t\treturn toRPCErr(err)\n\t}\n\treturn toRPCErr(errors.New(\"grpc: client streaming protocol violation: get <nil>, want <EOF>\"))\n}\n\nfunc (as *addrConnStream) finish(err error) {\n\tas.mu.Lock()\n\tif as.finished {\n\t\tas.mu.Unlock()\n\t\treturn\n\t}\n\tas.finished = true\n\tif err == io.EOF {\n\t\t// Ending a stream with EOF indicates a success.\n\t\terr = nil\n\t}\n\tif as.s != nil {\n\t\tas.s.Close(err)\n\t}\n\n\tif err != nil {\n\t\tas.ac.incrCallsFailed()\n\t} else {\n\t\tas.ac.incrCallsSucceeded()\n\t}\n\tas.cancel()\n\tas.mu.Unlock()\n}\n\n// ServerStream defines the server-side behavior of a streaming RPC.\n//\n// Errors returned from ServerStream methods are compatible with the status\n// package.  However, the status code will often not match the RPC status as\n// seen by the client application, and therefore, should not be relied upon for\n// this purpose.\ntype ServerStream interface {\n\t// SetHeader sets the header metadata. It may be called multiple times.\n\t// When call multiple times, all the provided metadata will be merged.\n\t// All the metadata will be sent out when one of the following happens:\n\t//  - ServerStream.SendHeader() is called;\n\t//  - The first response is sent out;\n\t//  - An RPC status is sent out (error or success).\n\tSetHeader(metadata.MD) error\n\t// SendHeader sends the header metadata.\n\t// The provided md and headers set by SetHeader() will be sent.\n\t// It fails if called multiple times.\n\tSendHeader(metadata.MD) error\n\t// SetTrailer sets the trailer metadata which will be sent with the RPC status.\n\t// When called more than once, all the provided metadata will be merged.\n\tSetTrailer(metadata.MD)\n\t// Context returns the context for this stream.\n\tContext() context.Context\n\t// SendMsg sends a message. On error, SendMsg aborts the stream and the\n\t// error is returned directly.\n\t//\n\t// SendMsg blocks until:\n\t//   - There is sufficient flow control to schedule m with the transport, or\n\t//   - The stream is done, or\n\t//   - The stream breaks.\n\t//\n\t// SendMsg does not wait until the message is received by the client. An\n\t// untimely stream closure may result in lost messages.\n\t//\n\t// It is safe to have a goroutine calling SendMsg and another goroutine\n\t// calling RecvMsg on the same stream at the same time, but it is not safe\n\t// to call SendMsg on the same stream in different goroutines.\n\t//\n\t// It is not safe to modify the message after calling SendMsg. Tracing\n\t// libraries and stats handlers may use the message lazily.\n\tSendMsg(m any) error\n\t// RecvMsg blocks until it receives a message into m or the stream is\n\t// done. It returns io.EOF when the client has performed a CloseSend. On\n\t// any non-EOF error, the stream is aborted and the error contains the\n\t// RPC status.\n\t//\n\t// It is safe to have a goroutine calling SendMsg and another goroutine\n\t// calling RecvMsg on the same stream at the same time, but it is not\n\t// safe to call RecvMsg on the same stream in different goroutines.\n\tRecvMsg(m any) error\n}\n\n// serverStream implements a server side Stream.\ntype serverStream struct {\n\tctx   context.Context\n\ts     *transport.ServerStream\n\tp     *parser\n\tcodec baseCodec\n\n\tcp     Compressor\n\tdc     Decompressor\n\tcomp   encoding.Compressor\n\tdecomp encoding.Compressor\n\n\tsendCompressorName string\n\n\tmaxReceiveMessageSize int\n\tmaxSendMessageSize    int\n\ttrInfo                *traceInfo\n\n\tstatsHandler []stats.Handler\n\n\tbinlogs []binarylog.MethodLogger\n\t// serverHeaderBinlogged indicates whether server header has been logged. It\n\t// will happen when one of the following two happens: stream.SendHeader(),\n\t// stream.Send().\n\t//\n\t// It's only checked in send and sendHeader, doesn't need to be\n\t// synchronized.\n\tserverHeaderBinlogged bool\n\n\tmu sync.Mutex // protects trInfo.tr after the service handler runs.\n}\n\nfunc (ss *serverStream) Context() context.Context {\n\treturn ss.ctx\n}\n\nfunc (ss *serverStream) SetHeader(md metadata.MD) error {\n\tif md.Len() == 0 {\n\t\treturn nil\n\t}\n\terr := imetadata.Validate(md)\n\tif err != nil {\n\t\treturn status.Error(codes.Internal, err.Error())\n\t}\n\treturn ss.s.SetHeader(md)\n}\n\nfunc (ss *serverStream) SendHeader(md metadata.MD) error {\n\terr := imetadata.Validate(md)\n\tif err != nil {\n\t\treturn status.Error(codes.Internal, err.Error())\n\t}\n\n\terr = ss.s.SendHeader(md)\n\tif len(ss.binlogs) != 0 && !ss.serverHeaderBinlogged {\n\t\th, _ := ss.s.Header()\n\t\tsh := &binarylog.ServerHeader{\n\t\t\tHeader: h,\n\t\t}\n\t\tss.serverHeaderBinlogged = true\n\t\tfor _, binlog := range ss.binlogs {\n\t\t\tbinlog.Log(ss.ctx, sh)\n\t\t}\n\t}\n\treturn err\n}\n\nfunc (ss *serverStream) SetTrailer(md metadata.MD) {\n\tif md.Len() == 0 {\n\t\treturn\n\t}\n\tif err := imetadata.Validate(md); err != nil {\n\t\tlogger.Errorf(\"stream: failed to validate md when setting trailer, err: %v\", err)\n\t}\n\tss.s.SetTrailer(md)\n}\n\nfunc (ss *serverStream) SendMsg(m any) (err error) {\n\tdefer func() {\n\t\tif ss.trInfo != nil {\n\t\t\tss.mu.Lock()\n\t\t\tif ss.trInfo.tr != nil {\n\t\t\t\tif err == nil {\n\t\t\t\t\tss.trInfo.tr.LazyLog(&payload{sent: true, msg: m}, true)\n\t\t\t\t} else {\n\t\t\t\t\tss.trInfo.tr.LazyLog(&fmtStringer{\"%v\", []any{err}}, true)\n\t\t\t\t\tss.trInfo.tr.SetError()\n\t\t\t\t}\n\t\t\t}\n\t\t\tss.mu.Unlock()\n\t\t}\n\t\tif err != nil && err != io.EOF {\n\t\t\tst, _ := status.FromError(toRPCErr(err))\n\t\t\tss.s.WriteStatus(st)\n\t\t\t// Non-user specified status was sent out. This should be an error\n\t\t\t// case (as a server side Cancel maybe).\n\t\t\t//\n\t\t\t// This is not handled specifically now. User will return a final\n\t\t\t// status from the service handler, we will log that error instead.\n\t\t\t// This behavior is similar to an interceptor.\n\t\t}\n\t}()\n\n\t// Server handler could have set new compressor by calling SetSendCompressor.\n\t// In case it is set, we need to use it for compressing outbound message.\n\tif sendCompressorsName := ss.s.SendCompress(); sendCompressorsName != ss.sendCompressorName {\n\t\tss.comp = encoding.GetCompressor(sendCompressorsName)\n\t\tss.sendCompressorName = sendCompressorsName\n\t}\n\n\t// load hdr, payload, data\n\thdr, data, payload, pf, err := prepareMsg(m, ss.codec, ss.cp, ss.comp, ss.p.bufferPool)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdefer func() {\n\t\tdata.Free()\n\t\t// only free payload if compression was made, and therefore it is a different set\n\t\t// of buffers from data.\n\t\tif pf.isCompressed() {\n\t\t\tpayload.Free()\n\t\t}\n\t}()\n\n\tdataLen := data.Len()\n\tpayloadLen := payload.Len()\n\n\t// TODO(dfawley): should we be checking len(data) instead?\n\tif payloadLen > ss.maxSendMessageSize {\n\t\treturn status.Errorf(codes.ResourceExhausted, \"trying to send message larger than max (%d vs. %d)\", payloadLen, ss.maxSendMessageSize)\n\t}\n\tif err := ss.s.Write(hdr, payload, &transport.WriteOptions{Last: false}); err != nil {\n\t\treturn toRPCErr(err)\n\t}\n\n\tif len(ss.binlogs) != 0 {\n\t\tif !ss.serverHeaderBinlogged {\n\t\t\th, _ := ss.s.Header()\n\t\t\tsh := &binarylog.ServerHeader{\n\t\t\t\tHeader: h,\n\t\t\t}\n\t\t\tss.serverHeaderBinlogged = true\n\t\t\tfor _, binlog := range ss.binlogs {\n\t\t\t\tbinlog.Log(ss.ctx, sh)\n\t\t\t}\n\t\t}\n\t\tsm := &binarylog.ServerMessage{\n\t\t\tMessage: data.Materialize(),\n\t\t}\n\t\tfor _, binlog := range ss.binlogs {\n\t\t\tbinlog.Log(ss.ctx, sm)\n\t\t}\n\t}\n\tif len(ss.statsHandler) != 0 {\n\t\tfor _, sh := range ss.statsHandler {\n\t\t\tsh.HandleRPC(ss.s.Context(), outPayload(false, m, dataLen, payloadLen, time.Now()))\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (ss *serverStream) RecvMsg(m any) (err error) {\n\tdefer func() {\n\t\tif ss.trInfo != nil {\n\t\t\tss.mu.Lock()\n\t\t\tif ss.trInfo.tr != nil {\n\t\t\t\tif err == nil {\n\t\t\t\t\tss.trInfo.tr.LazyLog(&payload{sent: false, msg: m}, true)\n\t\t\t\t} else if err != io.EOF {\n\t\t\t\t\tss.trInfo.tr.LazyLog(&fmtStringer{\"%v\", []any{err}}, true)\n\t\t\t\t\tss.trInfo.tr.SetError()\n\t\t\t\t}\n\t\t\t}\n\t\t\tss.mu.Unlock()\n\t\t}\n\t\tif err != nil && err != io.EOF {\n\t\t\tst, _ := status.FromError(toRPCErr(err))\n\t\t\tss.s.WriteStatus(st)\n\t\t\t// Non-user specified status was sent out. This should be an error\n\t\t\t// case (as a server side Cancel maybe).\n\t\t\t//\n\t\t\t// This is not handled specifically now. User will return a final\n\t\t\t// status from the service handler, we will log that error instead.\n\t\t\t// This behavior is similar to an interceptor.\n\t\t}\n\t}()\n\tvar payInfo *payloadInfo\n\tif len(ss.statsHandler) != 0 || len(ss.binlogs) != 0 {\n\t\tpayInfo = &payloadInfo{}\n\t\tdefer payInfo.free()\n\t}\n\tif err := recv(ss.p, ss.codec, ss.s, ss.dc, m, ss.maxReceiveMessageSize, payInfo, ss.decomp, true); err != nil {\n\t\tif err == io.EOF {\n\t\t\tif len(ss.binlogs) != 0 {\n\t\t\t\tchc := &binarylog.ClientHalfClose{}\n\t\t\t\tfor _, binlog := range ss.binlogs {\n\t\t\t\t\tbinlog.Log(ss.ctx, chc)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\tif err == io.ErrUnexpectedEOF {\n\t\t\terr = status.Error(codes.Internal, io.ErrUnexpectedEOF.Error())\n\t\t}\n\t\treturn toRPCErr(err)\n\t}\n\tif len(ss.statsHandler) != 0 {\n\t\tfor _, sh := range ss.statsHandler {\n\t\t\tsh.HandleRPC(ss.s.Context(), &stats.InPayload{\n\t\t\t\tRecvTime:         time.Now(),\n\t\t\t\tPayload:          m,\n\t\t\t\tLength:           payInfo.uncompressedBytes.Len(),\n\t\t\t\tWireLength:       payInfo.compressedLength + headerLen,\n\t\t\t\tCompressedLength: payInfo.compressedLength,\n\t\t\t})\n\t\t}\n\t}\n\tif len(ss.binlogs) != 0 {\n\t\tcm := &binarylog.ClientMessage{\n\t\t\tMessage: payInfo.uncompressedBytes.Materialize(),\n\t\t}\n\t\tfor _, binlog := range ss.binlogs {\n\t\t\tbinlog.Log(ss.ctx, cm)\n\t\t}\n\t}\n\treturn nil\n}\n\n// MethodFromServerStream returns the method string for the input stream.\n// The returned string is in the format of \"/service/method\".\nfunc MethodFromServerStream(stream ServerStream) (string, bool) {\n\treturn Method(stream.Context())\n}\n\n// prepareMsg returns the hdr, payload and data using the compressors passed or\n// using the passed preparedmsg. The returned boolean indicates whether\n// compression was made and therefore whether the payload needs to be freed in\n// addition to the returned data. Freeing the payload if the returned boolean is\n// false can lead to undefined behavior.\nfunc prepareMsg(m any, codec baseCodec, cp Compressor, comp encoding.Compressor, pool mem.BufferPool) (hdr []byte, data, payload mem.BufferSlice, pf payloadFormat, err error) {\n\tif preparedMsg, ok := m.(*PreparedMsg); ok {\n\t\treturn preparedMsg.hdr, preparedMsg.encodedData, preparedMsg.payload, preparedMsg.pf, nil\n\t}\n\t// The input interface is not a prepared msg.\n\t// Marshal and Compress the data at this point\n\tdata, err = encode(codec, m)\n\tif err != nil {\n\t\treturn nil, nil, nil, 0, err\n\t}\n\tcompData, pf, err := compress(data, cp, comp, pool)\n\tif err != nil {\n\t\tdata.Free()\n\t\treturn nil, nil, nil, 0, err\n\t}\n\thdr, payload = msgHeader(data, compData, pf)\n\treturn hdr, data, payload, pf, nil\n}\n"
        },
        {
          "name": "stream_interfaces.go",
          "type": "blob",
          "size": 10.177734375,
          "content": "/*\n *\n * Copyright 2024 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\n// ServerStreamingClient represents the client side of a server-streaming (one\n// request, many responses) RPC. It is generic over the type of the response\n// message. It is used in generated code.\ntype ServerStreamingClient[Res any] interface {\n\t// Recv receives the next response message from the server. The client may\n\t// repeatedly call Recv to read messages from the response stream.  If\n\t// io.EOF is returned, the stream has terminated with an OK status.  Any\n\t// other error is compatible with the status package and indicates the\n\t// RPC's status code and message.\n\tRecv() (*Res, error)\n\n\t// ClientStream is embedded to provide Context, Header, and Trailer\n\t// functionality.  No other methods in the ClientStream should be called\n\t// directly.\n\tClientStream\n}\n\n// ServerStreamingServer represents the server side of a server-streaming (one\n// request, many responses) RPC. It is generic over the type of the response\n// message. It is used in generated code.\n//\n// To terminate the response stream, return from the handler method and return\n// an error from the status package, or use nil to indicate an OK status code.\ntype ServerStreamingServer[Res any] interface {\n\t// Send sends a response message to the client.  The server handler may\n\t// call Send multiple times to send multiple messages to the client.  An\n\t// error is returned if the stream was terminated unexpectedly, and the\n\t// handler method should return, as the stream is no longer usable.\n\tSend(*Res) error\n\n\t// ServerStream is embedded to provide Context, SetHeader, SendHeader, and\n\t// SetTrailer functionality.  No other methods in the ServerStream should\n\t// be called directly.\n\tServerStream\n}\n\n// ClientStreamingClient represents the client side of a client-streaming (many\n// requests, one response) RPC. It is generic over both the type of the request\n// message stream and the type of the unary response message. It is used in\n// generated code.\ntype ClientStreamingClient[Req any, Res any] interface {\n\t// Send sends a request message to the server.  The client may call Send\n\t// multiple times to send multiple messages to the server.  On error, Send\n\t// aborts the stream.  If the error was generated by the client, the status\n\t// is returned directly.  Otherwise, io.EOF is returned, and the status of\n\t// the stream may be discovered using CloseAndRecv().\n\tSend(*Req) error\n\n\t// CloseAndRecv closes the request stream and waits for the server's\n\t// response.  This method must be called once and only once after sending\n\t// all request messages.  Any error returned is implemented by the status\n\t// package.\n\tCloseAndRecv() (*Res, error)\n\n\t// ClientStream is embedded to provide Context, Header, and Trailer\n\t// functionality.  No other methods in the ClientStream should be called\n\t// directly.\n\tClientStream\n}\n\n// ClientStreamingServer represents the server side of a client-streaming (many\n// requests, one response) RPC. It is generic over both the type of the request\n// message stream and the type of the unary response message. It is used in\n// generated code.\n//\n// To terminate the RPC, call SendAndClose and return nil from the method\n// handler or do not call SendAndClose and return an error from the status\n// package.\ntype ClientStreamingServer[Req any, Res any] interface {\n\t// Recv receives the next request message from the client.  The server may\n\t// repeatedly call Recv to read messages from the request stream.  If\n\t// io.EOF is returned, it indicates the client called CloseAndRecv on its\n\t// ClientStreamingClient.  Any other error indicates the stream was\n\t// terminated unexpectedly, and the handler method should return, as the\n\t// stream is no longer usable.\n\tRecv() (*Req, error)\n\n\t// SendAndClose sends a single response message to the client and closes\n\t// the stream.  This method must be called once and only once after all\n\t// request messages have been processed.  Recv should not be called after\n\t// calling SendAndClose.\n\tSendAndClose(*Res) error\n\n\t// ServerStream is embedded to provide Context, SetHeader, SendHeader, and\n\t// SetTrailer functionality.  No other methods in the ServerStream should\n\t// be called directly.\n\tServerStream\n}\n\n// BidiStreamingClient represents the client side of a bidirectional-streaming\n// (many requests, many responses) RPC. It is generic over both the type of the\n// request message stream and the type of the response message stream. It is\n// used in generated code.\ntype BidiStreamingClient[Req any, Res any] interface {\n\t// Send sends a request message to the server.  The client may call Send\n\t// multiple times to send multiple messages to the server.  On error, Send\n\t// aborts the stream.  If the error was generated by the client, the status\n\t// is returned directly.  Otherwise, io.EOF is returned, and the status of\n\t// the stream may be discovered using Recv().\n\tSend(*Req) error\n\n\t// Recv receives the next response message from the server. The client may\n\t// repeatedly call Recv to read messages from the response stream.  If\n\t// io.EOF is returned, the stream has terminated with an OK status.  Any\n\t// other error is compatible with the status package and indicates the\n\t// RPC's status code and message.\n\tRecv() (*Res, error)\n\n\t// ClientStream is embedded to provide Context, Header, Trailer, and\n\t// CloseSend functionality.  No other methods in the ClientStream should be\n\t// called directly.\n\tClientStream\n}\n\n// BidiStreamingServer represents the server side of a bidirectional-streaming\n// (many requests, many responses) RPC. It is generic over both the type of the\n// request message stream and the type of the response message stream. It is\n// used in generated code.\n//\n// To terminate the stream, return from the handler method and return\n// an error from the status package, or use nil to indicate an OK status code.\ntype BidiStreamingServer[Req any, Res any] interface {\n\t// Recv receives the next request message from the client.  The server may\n\t// repeatedly call Recv to read messages from the request stream.  If\n\t// io.EOF is returned, it indicates the client called CloseSend on its\n\t// BidiStreamingClient.  Any other error indicates the stream was\n\t// terminated unexpectedly, and the handler method should return, as the\n\t// stream is no longer usable.\n\tRecv() (*Req, error)\n\n\t// Send sends a response message to the client.  The server handler may\n\t// call Send multiple times to send multiple messages to the client.  An\n\t// error is returned if the stream was terminated unexpectedly, and the\n\t// handler method should return, as the stream is no longer usable.\n\tSend(*Res) error\n\n\t// ServerStream is embedded to provide Context, SetHeader, SendHeader, and\n\t// SetTrailer functionality.  No other methods in the ServerStream should\n\t// be called directly.\n\tServerStream\n}\n\n// GenericClientStream implements the ServerStreamingClient, ClientStreamingClient,\n// and BidiStreamingClient interfaces. It is used in generated code.\ntype GenericClientStream[Req any, Res any] struct {\n\tClientStream\n}\n\nvar _ ServerStreamingClient[string] = (*GenericClientStream[int, string])(nil)\nvar _ ClientStreamingClient[int, string] = (*GenericClientStream[int, string])(nil)\nvar _ BidiStreamingClient[int, string] = (*GenericClientStream[int, string])(nil)\n\n// Send pushes one message into the stream of requests to be consumed by the\n// server. The type of message which can be sent is determined by the Req type\n// parameter of the GenericClientStream receiver.\nfunc (x *GenericClientStream[Req, Res]) Send(m *Req) error {\n\treturn x.ClientStream.SendMsg(m)\n}\n\n// Recv reads one message from the stream of responses generated by the server.\n// The type of the message returned is determined by the Res type parameter\n// of the GenericClientStream receiver.\nfunc (x *GenericClientStream[Req, Res]) Recv() (*Res, error) {\n\tm := new(Res)\n\tif err := x.ClientStream.RecvMsg(m); err != nil {\n\t\treturn nil, err\n\t}\n\treturn m, nil\n}\n\n// CloseAndRecv closes the sending side of the stream, then receives the unary\n// response from the server. The type of message which it returns is determined\n// by the Res type parameter of the GenericClientStream receiver.\nfunc (x *GenericClientStream[Req, Res]) CloseAndRecv() (*Res, error) {\n\tif err := x.ClientStream.CloseSend(); err != nil {\n\t\treturn nil, err\n\t}\n\tm := new(Res)\n\tif err := x.ClientStream.RecvMsg(m); err != nil {\n\t\treturn nil, err\n\t}\n\treturn m, nil\n}\n\n// GenericServerStream implements the ServerStreamingServer, ClientStreamingServer,\n// and BidiStreamingServer interfaces. It is used in generated code.\ntype GenericServerStream[Req any, Res any] struct {\n\tServerStream\n}\n\nvar _ ServerStreamingServer[string] = (*GenericServerStream[int, string])(nil)\nvar _ ClientStreamingServer[int, string] = (*GenericServerStream[int, string])(nil)\nvar _ BidiStreamingServer[int, string] = (*GenericServerStream[int, string])(nil)\n\n// Send pushes one message into the stream of responses to be consumed by the\n// client. The type of message which can be sent is determined by the Res\n// type parameter of the serverStreamServer receiver.\nfunc (x *GenericServerStream[Req, Res]) Send(m *Res) error {\n\treturn x.ServerStream.SendMsg(m)\n}\n\n// SendAndClose pushes the unary response to the client. The type of message\n// which can be sent is determined by the Res type parameter of the\n// clientStreamServer receiver.\nfunc (x *GenericServerStream[Req, Res]) SendAndClose(m *Res) error {\n\treturn x.ServerStream.SendMsg(m)\n}\n\n// Recv reads one message from the stream of requests generated by the client.\n// The type of the message returned is determined by the Req type parameter\n// of the clientStreamServer receiver.\nfunc (x *GenericServerStream[Req, Res]) Recv() (*Req, error) {\n\tm := new(Req)\n\tif err := x.ServerStream.RecvMsg(m); err != nil {\n\t\treturn nil, err\n\t}\n\treturn m, nil\n}\n"
        },
        {
          "name": "stream_test.go",
          "type": "blob",
          "size": 1.7529296875,
          "content": "/*\n *\n * Copyright 2023 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc_test\n\nimport (\n\t\"context\"\n\t\"testing\"\n\t\"time\"\n\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/internal/grpctest\"\n\t\"google.golang.org/grpc/internal/stubserver\"\n\t\"google.golang.org/grpc/status\"\n\n\ttestgrpc \"google.golang.org/grpc/interop/grpc_testing\"\n)\n\nconst defaultTestTimeout = 10 * time.Second\n\ntype s struct {\n\tgrpctest.Tester\n}\n\nfunc Test(t *testing.T) {\n\tgrpctest.RunSubTests(t, s{})\n}\n\nfunc (s) TestStream_Header_TrailersOnly(t *testing.T) {\n\tss := stubserver.StubServer{\n\t\tFullDuplexCallF: func(stream testgrpc.TestService_FullDuplexCallServer) error {\n\t\t\treturn status.Errorf(codes.NotFound, \"a test error\")\n\t\t},\n\t}\n\tif err := ss.Start(nil); err != nil {\n\t\tt.Fatal(\"Error starting server:\", err)\n\t}\n\tdefer ss.Stop()\n\n\tctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)\n\tdefer cancel()\n\n\ts, err := ss.Client.FullDuplexCall(ctx)\n\tif err != nil {\n\t\tt.Fatal(\"Error staring call\", err)\n\t}\n\tif md, err := s.Header(); md != nil || err != nil {\n\t\tt.Fatalf(\"s.Header() = %v, %v; want nil, nil\", md, err)\n\t}\n\tif _, err := s.Recv(); status.Code(err) != codes.NotFound {\n\t\tt.Fatalf(\"s.Recv() = _, %v; want _, err.Code()=codes.NotFound\", err)\n\t}\n}\n"
        },
        {
          "name": "tap",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "testdata",
          "type": "tree",
          "content": null
        },
        {
          "name": "trace.go",
          "type": "blob",
          "size": 3.412109375,
          "content": "/*\n *\n * Copyright 2015 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n)\n\n// EnableTracing controls whether to trace RPCs using the golang.org/x/net/trace package.\n// This should only be set before any RPCs are sent or received by this program.\nvar EnableTracing bool\n\n// methodFamily returns the trace family for the given method.\n// It turns \"/pkg.Service/GetFoo\" into \"pkg.Service\".\nfunc methodFamily(m string) string {\n\tm = strings.TrimPrefix(m, \"/\") // remove leading slash\n\tif i := strings.Index(m, \"/\"); i >= 0 {\n\t\tm = m[:i] // remove everything from second slash\n\t}\n\treturn m\n}\n\n// traceEventLog mirrors golang.org/x/net/trace.EventLog.\n//\n// It exists in order to avoid importing x/net/trace on grpcnotrace builds.\ntype traceEventLog interface {\n\tPrintf(format string, a ...any)\n\tErrorf(format string, a ...any)\n\tFinish()\n}\n\n// traceLog mirrors golang.org/x/net/trace.Trace.\n//\n// It exists in order to avoid importing x/net/trace on grpcnotrace builds.\ntype traceLog interface {\n\tLazyLog(x fmt.Stringer, sensitive bool)\n\tLazyPrintf(format string, a ...any)\n\tSetError()\n\tSetRecycler(f func(any))\n\tSetTraceInfo(traceID, spanID uint64)\n\tSetMaxEvents(m int)\n\tFinish()\n}\n\n// traceInfo contains tracing information for an RPC.\ntype traceInfo struct {\n\ttr        traceLog\n\tfirstLine firstLine\n}\n\n// firstLine is the first line of an RPC trace.\n// It may be mutated after construction; remoteAddr specifically may change\n// during client-side use.\ntype firstLine struct {\n\tmu         sync.Mutex\n\tclient     bool // whether this is a client (outgoing) RPC\n\tremoteAddr net.Addr\n\tdeadline   time.Duration // may be zero\n}\n\nfunc (f *firstLine) SetRemoteAddr(addr net.Addr) {\n\tf.mu.Lock()\n\tf.remoteAddr = addr\n\tf.mu.Unlock()\n}\n\nfunc (f *firstLine) String() string {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\n\tvar line bytes.Buffer\n\tio.WriteString(&line, \"RPC: \")\n\tif f.client {\n\t\tio.WriteString(&line, \"to\")\n\t} else {\n\t\tio.WriteString(&line, \"from\")\n\t}\n\tfmt.Fprintf(&line, \" %v deadline:\", f.remoteAddr)\n\tif f.deadline != 0 {\n\t\tfmt.Fprint(&line, f.deadline)\n\t} else {\n\t\tio.WriteString(&line, \"none\")\n\t}\n\treturn line.String()\n}\n\nconst truncateSize = 100\n\nfunc truncate(x string, l int) string {\n\tif l > len(x) {\n\t\treturn x\n\t}\n\treturn x[:l]\n}\n\n// payload represents an RPC request or response payload.\ntype payload struct {\n\tsent bool // whether this is an outgoing payload\n\tmsg  any  // e.g. a proto.Message\n\t// TODO(dsymonds): add stringifying info to codec, and limit how much we hold here?\n}\n\nfunc (p payload) String() string {\n\tif p.sent {\n\t\treturn truncate(fmt.Sprintf(\"sent: %v\", p.msg), truncateSize)\n\t}\n\treturn truncate(fmt.Sprintf(\"recv: %v\", p.msg), truncateSize)\n}\n\ntype fmtStringer struct {\n\tformat string\n\ta      []any\n}\n\nfunc (f *fmtStringer) String() string {\n\treturn fmt.Sprintf(f.format, f.a...)\n}\n\ntype stringer string\n\nfunc (s stringer) String() string { return string(s) }\n"
        },
        {
          "name": "trace_notrace.go",
          "type": "blob",
          "size": 1.5234375,
          "content": "//go:build grpcnotrace\n\n/*\n *\n * Copyright 2024 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\n// grpcnotrace can be used to avoid importing golang.org/x/net/trace, which in\n// turn enables binaries using gRPC-Go for dead code elimination, which can\n// yield 10-15% improvements in binary size when tracing is not needed.\n\nimport (\n\t\"context\"\n\t\"fmt\"\n)\n\ntype notrace struct{}\n\nfunc (notrace) LazyLog(x fmt.Stringer, sensitive bool) {}\nfunc (notrace) LazyPrintf(format string, a ...any)     {}\nfunc (notrace) SetError()                              {}\nfunc (notrace) SetRecycler(f func(any))                {}\nfunc (notrace) SetTraceInfo(traceID, spanID uint64)    {}\nfunc (notrace) SetMaxEvents(m int)                     {}\nfunc (notrace) Finish()                                {}\n\nfunc newTrace(family, title string) traceLog {\n\treturn notrace{}\n}\n\nfunc newTraceContext(ctx context.Context, tr traceLog) context.Context {\n\treturn ctx\n}\n\nfunc newTraceEventLog(family, title string) traceEventLog {\n\treturn nil\n}\n"
        },
        {
          "name": "trace_test.go",
          "type": "blob",
          "size": 1.248046875,
          "content": "/*\n *\n * Copyright 2019 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"testing\"\n)\n\nfunc (s) TestMethodFamily(t *testing.T) {\n\tcases := []struct {\n\t\tdesc             string\n\t\tmethod           string\n\t\twantMethodFamily string\n\t}{\n\t\t{\n\t\t\tdesc:             \"No leading slash\",\n\t\t\tmethod:           \"pkg.service/method\",\n\t\t\twantMethodFamily: \"pkg.service\",\n\t\t},\n\t\t{\n\t\t\tdesc:             \"Leading slash\",\n\t\t\tmethod:           \"/pkg.service/method\",\n\t\t\twantMethodFamily: \"pkg.service\",\n\t\t},\n\t}\n\n\tfor _, ut := range cases {\n\t\tt.Run(ut.desc, func(t *testing.T) {\n\t\t\tif got := methodFamily(ut.method); got != ut.wantMethodFamily {\n\t\t\t\tt.Fatalf(\"methodFamily(%s) = %s, want %s\", ut.method, got, ut.wantMethodFamily)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "trace_withtrace.go",
          "type": "blob",
          "size": 0.9560546875,
          "content": "//go:build !grpcnotrace\n\n/*\n *\n * Copyright 2024 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\nimport (\n\t\"context\"\n\n\tt \"golang.org/x/net/trace\"\n)\n\nfunc newTrace(family, title string) traceLog {\n\treturn t.New(family, title)\n}\n\nfunc newTraceContext(ctx context.Context, tr traceLog) context.Context {\n\treturn t.NewContext(ctx, tr)\n}\n\nfunc newTraceEventLog(family, title string) traceEventLog {\n\treturn t.NewEventLog(family, title)\n}\n"
        },
        {
          "name": "version.go",
          "type": "blob",
          "size": 0.6708984375,
          "content": "/*\n *\n * Copyright 2018 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage grpc\n\n// Version is the current grpc version.\nconst Version = \"1.71.0-dev\"\n"
        },
        {
          "name": "xds",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}