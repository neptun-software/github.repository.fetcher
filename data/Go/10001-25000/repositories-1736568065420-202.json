{
  "metadata": {
    "timestamp": 1736568065420,
    "page": 202,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "benbjohnson/litestream",
      "stars": 11318,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.015625,
          "content": ".DS_Store\n/dist\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.5166015625,
          "content": "repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.1.0\n    hooks:\n      - id: trailing-whitespace\n        exclude_types: [markdown]\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n\n  - repo: https://github.com/tekwizely/pre-commit-golang\n    rev: v1.0.0-beta.5\n    hooks:\n      - id: go-imports-repo\n        args:\n          - \"-local\"\n          - \"github.com/benbjohnson/litestrem\"\n          - \"-w\"\n      - id: go-vet-repo-mod\n      - id: go-staticcheck-repo-mod\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.4951171875,
          "content": "FROM golang:1.23 as builder\n\nWORKDIR /src/litestream\nCOPY . .\n\nARG LITESTREAM_VERSION=latest\n\nRUN --mount=type=cache,target=/root/.cache/go-build \\\n\t--mount=type=cache,target=/go/pkg \\\n\tgo build -ldflags \"-s -w -X 'main.Version=${LITESTREAM_VERSION}' -extldflags '-static'\" -tags osusergo,netgo,sqlite_omit_load_extension -o /usr/local/bin/litestream ./cmd/litestream\n\n\nFROM alpine:3.20\nCOPY --from=builder /usr/local/bin/litestream /usr/local/bin/litestream\nENTRYPOINT [\"/usr/local/bin/litestream\"]\nCMD []\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.68359375,
          "content": "default:\n\ndocker:\n\tdocker build -t litestream .\n\ndist-linux:\n\tmkdir -p dist\n\tcp etc/litestream.yml dist/litestream.yml\n\tdocker run --rm -v \"${PWD}\":/usr/src/litestream -w /usr/src/litestream -e GOOS=linux -e GOARCH=amd64 golang:1.16 go build -v -ldflags \"-s -w\" -o dist/litestream ./cmd/litestream\n\ttar -cz -f dist/litestream-linux-amd64.tar.gz -C dist litestream\n\ndist-linux-arm:\n\tdocker run --rm -v \"${PWD}\":/usr/src/litestream -w /usr/src/litestream -e CGO_ENABLED=1 -e CC=arm-linux-gnueabihf-gcc -e GOOS=linux -e GOARCH=arm golang-xc:1.16 go build -v -o dist/litestream-linux-arm ./cmd/litestream\n\ndist-linux-arm64:\n\tdocker run --rm -v \"${PWD}\":/usr/src/litestream -w /usr/src/litestream -e CGO_ENABLED=1 -e CC=aarch64-linux-gnu-gcc -e GOOS=linux -e GOARCH=arm64 golang-xc:1.16 go build -v -o dist/litestream-linux-arm64 ./cmd/litestream\n\ndist-macos:\nifndef LITESTREAM_VERSION\n\t$(error LITESTREAM_VERSION is undefined)\nendif\n\tmkdir -p dist\n\n\tGOOS=darwin GOARCH=amd64 CC=\"gcc -target amd64-apple-macos11\" CGO_ENABLED=1 go build -v -ldflags \"-s -w -X 'main.Version=${LITESTREAM_VERSION}'\"  -o dist/litestream ./cmd/litestream\n\tgon etc/gon.hcl\n\tmv dist/litestream.zip dist/litestream-${LITESTREAM_VERSION}-darwin-amd64.zip\n\topenssl dgst -sha256 dist/litestream-${LITESTREAM_VERSION}-darwin-amd64.zip\n\n\tGOOS=darwin GOARCH=arm64 CC=\"gcc -target arm64-apple-macos11\" CGO_ENABLED=1 go build -v -ldflags \"-s -w -X 'main.Version=${LITESTREAM_VERSION}'\"  -o dist/litestream ./cmd/litestream\n\tgon etc/gon.hcl\n\tmv dist/litestream.zip dist/litestream-${LITESTREAM_VERSION}-darwin-arm64.zip\n\topenssl dgst -sha256 dist/litestream-${LITESTREAM_VERSION}-darwin-arm64.zip\n\nclean:\n\trm -rf dist\n\n.PHONY: default dist-linux dist-macos clean\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.447265625,
          "content": "Litestream\n![GitHub release (latest by date)](https://img.shields.io/github/v/release/benbjohnson/litestream)\n![Status](https://img.shields.io/badge/status-beta-blue)\n![GitHub](https://img.shields.io/github/license/benbjohnson/litestream)\n[![Docker Pulls](https://img.shields.io/docker/pulls/litestream/litestream.svg?maxAge=604800)](https://hub.docker.com/r/litestream/litestream/)\n![test](https://github.com/benbjohnson/litestream/workflows/test/badge.svg)\n==========\n\nLitestream is a standalone disaster recovery tool for SQLite. It runs as a\nbackground process and safely replicates changes incrementally to another file\nor S3. Litestream only communicates with SQLite through the SQLite API so it\nwill not corrupt your database.\n\nIf you need support or have ideas for improving Litestream, please join the\n[Litestream Slack][slack] or visit the [GitHub Discussions](https://github.com/benbjohnson/litestream/discussions).\nPlease visit the [Litestream web site](https://litestream.io) for installation\ninstructions and documentation.\n\nIf you find this project interesting, please consider starring the project on\nGitHub.\n\n[slack]: https://join.slack.com/t/litestream/shared_invite/zt-n0j4s3ci-lx1JziR3bV6L2NMF723H3Q\n\n\n## Acknowledgements\n\nWhile the Litestream project does not accept external code patches, many\nof the most valuable contributions are in the forms of testing, feedback, and\ndocumentation. These help harden software and streamline usage for other users.\n\nI want to give special thanks to individuals who invest much of their time and \nenergy into the project to help make it better:\n\n- Thanks to [Cory LaNou](https://twitter.com/corylanou) for giving early feedback and testing when Litestream was still pre-release.\n- Thanks to [Michael Lynch](https://github.com/mtlynch) for digging into issues and contributing to the documentation.\n- Thanks to [Kurt Mackey](https://twitter.com/mrkurt) for feedback and testing.\n- Thanks to [Sam Weston](https://twitter.com/cablespaghetti) for figuring out how to run Litestream on Kubernetes and writing up the docs for it.\n- Thanks to [Rafael](https://github.com/netstx) & [Jungle Boogie](https://github.com/jungle-boogie) for helping to get OpenBSD release builds working.\n- Thanks to [Simon Gottschlag](https://github.com/simongottschlag), [Marin](https://github.com/supermarin),[Victor BjÃ¶rklund](https://github.com/victorbjorklund), [Jonathan Beri](https://twitter.com/beriberikix) [Yuri](https://github.com/yurivish), [Nathan Probst](https://github.com/nprbst), [Yann Coleu](https://github.com/yanc0), and [Nicholas Grilly](https://twitter.com/ngrilly) for frequent feedback, testing, & support.\n\nHuge thanks to fly.io for their support and for contributing credits for testing and development!\n\n\n## Contribution Policy\n\nInitially, Litestream was closed to outside contributions. The goal was to\nreduce burnout by limiting the maintenance overhead of reviewing and validating\nthird-party code. However, this policy is overly broad and has prevented small,\neasily testable patches from being contributed.\n\nLitestream is now open to code contributions for bug fixes only. Features carry\na long-term maintenance burden so they will not be accepted at this time.\nPlease [submit an issue][new-issue] if you have a feature you'd like to\nrequest.\n\nIf you find mistakes in the documentation, please submit a fix to the\n[documentation repository][docs].\n\n[new-issue]: https://github.com/benbjohnson/litestream/issues/new\n[docs]: https://github.com/benbjohnson/litestream.io\n"
        },
        {
          "name": "abs",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "db.go",
          "type": "blob",
          "size": 49.404296875,
          "content": "package litestream\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/binary\"\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash/crc64\"\n\t\"io\"\n\t\"log/slog\"\n\t\"math\"\n\t\"math/rand\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/benbjohnson/litestream/internal\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n)\n\n// Default DB settings.\nconst (\n\tDefaultMonitorInterval    = 1 * time.Second\n\tDefaultCheckpointInterval = 1 * time.Minute\n\tDefaultBusyTimeout        = 1 * time.Second\n\tDefaultMinCheckpointPageN = 1000\n\tDefaultMaxCheckpointPageN = 10000\n\tDefaultTruncatePageN      = 500000\n)\n\n// MaxIndex is the maximum possible WAL index.\n// If this index is reached then a new generation will be started.\nconst MaxIndex = 0x7FFFFFFF\n\n// DB represents a managed instance of a SQLite database in the file system.\ntype DB struct {\n\tmu       sync.RWMutex\n\tpath     string        // part to database\n\tmetaPath string        // Path to the database metadata.\n\tdb       *sql.DB       // target database\n\tf        *os.File      // long-running db file descriptor\n\trtx      *sql.Tx       // long running read transaction\n\tpageSize int           // page size, in bytes\n\tnotify   chan struct{} // closes on WAL change\n\tchkMu    sync.Mutex    // checkpoint lock\n\n\tfileInfo os.FileInfo // db info cached during init\n\tdirInfo  os.FileInfo // parent dir info cached during init\n\n\tctx    context.Context\n\tcancel func()\n\twg     sync.WaitGroup\n\n\t// Metrics\n\tdbSizeGauge                 prometheus.Gauge\n\twalSizeGauge                prometheus.Gauge\n\ttotalWALBytesCounter        prometheus.Counter\n\tshadowWALIndexGauge         prometheus.Gauge\n\tshadowWALSizeGauge          prometheus.Gauge\n\tsyncNCounter                prometheus.Counter\n\tsyncErrorNCounter           prometheus.Counter\n\tsyncSecondsCounter          prometheus.Counter\n\tcheckpointNCounterVec       *prometheus.CounterVec\n\tcheckpointErrorNCounterVec  *prometheus.CounterVec\n\tcheckpointSecondsCounterVec *prometheus.CounterVec\n\n\t// Minimum threshold of WAL size, in pages, before a passive checkpoint.\n\t// A passive checkpoint will attempt a checkpoint but fail if there are\n\t// active transactions occurring at the same time.\n\tMinCheckpointPageN int\n\n\t// Maximum threshold of WAL size, in pages, before a forced checkpoint.\n\t// A forced checkpoint will block new transactions and wait for existing\n\t// transactions to finish before issuing a checkpoint and resetting the WAL.\n\t//\n\t// If zero, no checkpoints are forced. This can cause the WAL to grow\n\t// unbounded if there are always read transactions occurring.\n\tMaxCheckpointPageN int\n\n\t// Threshold of WAL size, in pages, before a forced truncation checkpoint.\n\t// A forced truncation checkpoint will block new transactions and wait for\n\t// existing transactions to finish before issuing a checkpoint and\n\t// truncating the WAL.\n\t//\n\t// If zero, no truncates are forced. This can cause the WAL to grow\n\t// unbounded if there's a sudden spike of changes between other\n\t// checkpoints.\n\tTruncatePageN int\n\n\t// Time between automatic checkpoints in the WAL. This is done to allow\n\t// more fine-grained WAL files so that restores can be performed with\n\t// better precision.\n\tCheckpointInterval time.Duration\n\n\t// Frequency at which to perform db sync.\n\tMonitorInterval time.Duration\n\n\t// The timeout to wait for EBUSY from SQLite.\n\tBusyTimeout time.Duration\n\n\t// List of replicas for the database.\n\t// Must be set before calling Open().\n\tReplicas []*Replica\n\n\t// Where to send log messages, defaults to global slog with databas epath.\n\tLogger *slog.Logger\n}\n\n// NewDB returns a new instance of DB for a given path.\nfunc NewDB(path string) *DB {\n\tdir, file := filepath.Split(path)\n\n\tdb := &DB{\n\t\tpath:     path,\n\t\tmetaPath: filepath.Join(dir, \".\"+file+MetaDirSuffix),\n\t\tnotify:   make(chan struct{}),\n\n\t\tMinCheckpointPageN: DefaultMinCheckpointPageN,\n\t\tMaxCheckpointPageN: DefaultMaxCheckpointPageN,\n\t\tTruncatePageN:      DefaultTruncatePageN,\n\t\tCheckpointInterval: DefaultCheckpointInterval,\n\t\tMonitorInterval:    DefaultMonitorInterval,\n\t\tBusyTimeout:        DefaultBusyTimeout,\n\t\tLogger:             slog.With(\"db\", path),\n\t}\n\n\tdb.dbSizeGauge = dbSizeGaugeVec.WithLabelValues(db.path)\n\tdb.walSizeGauge = walSizeGaugeVec.WithLabelValues(db.path)\n\tdb.totalWALBytesCounter = totalWALBytesCounterVec.WithLabelValues(db.path)\n\tdb.shadowWALIndexGauge = shadowWALIndexGaugeVec.WithLabelValues(db.path)\n\tdb.shadowWALSizeGauge = shadowWALSizeGaugeVec.WithLabelValues(db.path)\n\tdb.syncNCounter = syncNCounterVec.WithLabelValues(db.path)\n\tdb.syncErrorNCounter = syncErrorNCounterVec.WithLabelValues(db.path)\n\tdb.syncSecondsCounter = syncSecondsCounterVec.WithLabelValues(db.path)\n\tdb.checkpointNCounterVec = checkpointNCounterVec.MustCurryWith(prometheus.Labels{\"db\": db.path})\n\tdb.checkpointErrorNCounterVec = checkpointErrorNCounterVec.MustCurryWith(prometheus.Labels{\"db\": db.path})\n\tdb.checkpointSecondsCounterVec = checkpointSecondsCounterVec.MustCurryWith(prometheus.Labels{\"db\": db.path})\n\n\tdb.ctx, db.cancel = context.WithCancel(context.Background())\n\n\treturn db\n}\n\n// SQLDB returns a reference to the underlying sql.DB connection.\nfunc (db *DB) SQLDB() *sql.DB {\n\treturn db.db\n}\n\n// Path returns the path to the database.\nfunc (db *DB) Path() string {\n\treturn db.path\n}\n\n// WALPath returns the path to the database's WAL file.\nfunc (db *DB) WALPath() string {\n\treturn db.path + \"-wal\"\n}\n\n// MetaPath returns the path to the database metadata.\nfunc (db *DB) MetaPath() string {\n\treturn db.metaPath\n}\n\n// SetMetaPath sets the path to database metadata.\nfunc (db *DB) SetMetaPath(mp string) {\n\tdb.metaPath = mp\n}\n\n// GenerationNamePath returns the path of the name of the current generation.\nfunc (db *DB) GenerationNamePath() string {\n\treturn filepath.Join(db.metaPath, \"generation\")\n}\n\n// GenerationPath returns the path of a single generation.\n// Panics if generation is blank.\nfunc (db *DB) GenerationPath(generation string) string {\n\tassert(generation != \"\", \"generation name required\")\n\treturn filepath.Join(db.metaPath, \"generations\", generation)\n}\n\n// ShadowWALDir returns the path of the shadow wal directory.\n// Panics if generation is blank.\nfunc (db *DB) ShadowWALDir(generation string) string {\n\treturn filepath.Join(db.GenerationPath(generation), \"wal\")\n}\n\n// ShadowWALPath returns the path of a single shadow WAL file.\n// Panics if generation is blank or index is negative.\nfunc (db *DB) ShadowWALPath(generation string, index int) string {\n\tassert(index >= 0, \"shadow wal index cannot be negative\")\n\treturn filepath.Join(db.ShadowWALDir(generation), FormatWALPath(index))\n}\n\n// CurrentShadowWALPath returns the path to the last shadow WAL in a generation.\nfunc (db *DB) CurrentShadowWALPath(generation string) (string, error) {\n\tindex, _, err := db.CurrentShadowWALIndex(generation)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn db.ShadowWALPath(generation, index), nil\n}\n\n// CurrentShadowWALIndex returns the current WAL index & total size.\nfunc (db *DB) CurrentShadowWALIndex(generation string) (index int, size int64, err error) {\n\tdes, err := os.ReadDir(filepath.Join(db.GenerationPath(generation), \"wal\"))\n\tif os.IsNotExist(err) {\n\t\treturn 0, 0, nil // no wal files written for generation\n\t} else if err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\t// Find highest wal index.\n\tfor _, de := range des {\n\t\tfi, err := de.Info()\n\t\tif os.IsNotExist(err) {\n\t\t\tcontinue // file was deleted after os.ReadDir returned\n\t\t} else if err != nil {\n\t\t\treturn 0, 0, err\n\t\t}\n\n\t\tif v, err := ParseWALPath(fi.Name()); err != nil {\n\t\t\tcontinue // invalid wal filename\n\t\t} else if v > index {\n\t\t\tindex = v\n\t\t}\n\n\t\tsize += fi.Size()\n\t}\n\treturn index, size, nil\n}\n\n// FileInfo returns the cached file stats for the database file when it was initialized.\nfunc (db *DB) FileInfo() os.FileInfo {\n\treturn db.fileInfo\n}\n\n// DirInfo returns the cached file stats for the parent directory of the database file when it was initialized.\nfunc (db *DB) DirInfo() os.FileInfo {\n\treturn db.dirInfo\n}\n\n// Replica returns a replica by name.\nfunc (db *DB) Replica(name string) *Replica {\n\tfor _, r := range db.Replicas {\n\t\tif r.Name() == name {\n\t\t\treturn r\n\t\t}\n\t}\n\treturn nil\n}\n\n// Pos returns the current position of the database.\nfunc (db *DB) Pos() (Pos, error) {\n\tgeneration, err := db.CurrentGeneration()\n\tif err != nil {\n\t\treturn Pos{}, err\n\t} else if generation == \"\" {\n\t\treturn Pos{}, nil\n\t}\n\n\tindex, _, err := db.CurrentShadowWALIndex(generation)\n\tif err != nil {\n\t\treturn Pos{}, err\n\t}\n\n\tfi, err := os.Stat(db.ShadowWALPath(generation, index))\n\tif os.IsNotExist(err) {\n\t\treturn Pos{Generation: generation, Index: index}, nil\n\t} else if err != nil {\n\t\treturn Pos{}, err\n\t}\n\n\treturn Pos{Generation: generation, Index: index, Offset: frameAlign(fi.Size(), db.pageSize)}, nil\n}\n\n// Notify returns a channel that closes when the shadow WAL changes.\nfunc (db *DB) Notify() <-chan struct{} {\n\tdb.mu.RLock()\n\tdefer db.mu.RUnlock()\n\treturn db.notify\n}\n\n// PageSize returns the page size of the underlying database.\n// Only valid after database exists & Init() has successfully run.\nfunc (db *DB) PageSize() int {\n\tdb.mu.RLock()\n\tdefer db.mu.RUnlock()\n\treturn db.pageSize\n}\n\n// Open initializes the background monitoring goroutine.\nfunc (db *DB) Open() (err error) {\n\t// Validate fields on database.\n\tif db.MinCheckpointPageN <= 0 {\n\t\treturn fmt.Errorf(\"minimum checkpoint page count required\")\n\t}\n\n\t// Validate that all replica names are unique.\n\tm := make(map[string]struct{})\n\tfor _, r := range db.Replicas {\n\t\tif _, ok := m[r.Name()]; ok {\n\t\t\treturn fmt.Errorf(\"duplicate replica name: %q\", r.Name())\n\t\t}\n\t\tm[r.Name()] = struct{}{}\n\t}\n\n\t// Clear old temporary files that my have been left from a crash.\n\tif err := removeTmpFiles(db.metaPath); err != nil {\n\t\treturn fmt.Errorf(\"cannot remove tmp files: %w\", err)\n\t}\n\n\t// Start monitoring SQLite database in a separate goroutine.\n\tif db.MonitorInterval > 0 {\n\t\tdb.wg.Add(1)\n\t\tgo func() { defer db.wg.Done(); db.monitor() }()\n\t}\n\n\treturn nil\n}\n\n// Close flushes outstanding WAL writes to replicas, releases the read lock,\n// and closes the database. Takes a context for final sync.\nfunc (db *DB) Close(ctx context.Context) (err error) {\n\tdb.cancel()\n\tdb.wg.Wait()\n\n\t// Perform a final db sync, if initialized.\n\tif db.db != nil {\n\t\tif e := db.Sync(ctx); e != nil && err == nil {\n\t\t\terr = e\n\t\t}\n\t}\n\n\t// Ensure replicas perform a final sync and stop replicating.\n\tfor _, r := range db.Replicas {\n\t\tif db.db != nil {\n\t\t\tif e := r.Sync(ctx); e != nil && err == nil {\n\t\t\t\terr = e\n\t\t\t}\n\t\t}\n\t\tr.Stop(true)\n\t}\n\n\t// Release the read lock to allow other applications to handle checkpointing.\n\tif db.rtx != nil {\n\t\tif e := db.releaseReadLock(); e != nil && err == nil {\n\t\t\terr = e\n\t\t}\n\t}\n\n\tif db.db != nil {\n\t\tif e := db.db.Close(); e != nil && err == nil {\n\t\t\terr = e\n\t\t}\n\t}\n\n\tif db.f != nil {\n\t\tif e := db.f.Close(); e != nil && err == nil {\n\t\t\terr = e\n\t\t}\n\t}\n\n\treturn err\n}\n\n// UpdatedAt returns the last modified time of the database or WAL file.\nfunc (db *DB) UpdatedAt() (time.Time, error) {\n\t// Determine database modified time.\n\tfi, err := os.Stat(db.Path())\n\tif err != nil {\n\t\treturn time.Time{}, err\n\t}\n\tt := fi.ModTime().UTC()\n\n\t// Use WAL modified time, if available & later.\n\tif fi, err := os.Stat(db.WALPath()); os.IsNotExist(err) {\n\t\treturn t, nil\n\t} else if err != nil {\n\t\treturn t, err\n\t} else if fi.ModTime().After(t) {\n\t\tt = fi.ModTime().UTC()\n\t}\n\treturn t, nil\n}\n\n// init initializes the connection to the database.\n// Skipped if already initialized or if the database file does not exist.\nfunc (db *DB) init() (err error) {\n\t// Exit if already initialized.\n\tif db.db != nil {\n\t\treturn nil\n\t}\n\n\t// Exit if no database file exists.\n\tfi, err := os.Stat(db.path)\n\tif os.IsNotExist(err) {\n\t\treturn nil\n\t} else if err != nil {\n\t\treturn err\n\t}\n\tdb.fileInfo = fi\n\n\t// Obtain permissions for parent directory.\n\tif fi, err = os.Stat(filepath.Dir(db.path)); err != nil {\n\t\treturn err\n\t}\n\tdb.dirInfo = fi\n\n\tdsn := db.path\n\tdsn += fmt.Sprintf(\"?_busy_timeout=%d\", db.BusyTimeout.Milliseconds())\n\n\t// Connect to SQLite database. Use the driver registered with a hook to\n\t// prevent WAL files from being removed.\n\tif db.db, err = sql.Open(\"litestream-sqlite3\", dsn); err != nil {\n\t\treturn err\n\t}\n\n\t// Open long-running database file descriptor. Required for non-OFD locks.\n\tif db.f, err = os.Open(db.path); err != nil {\n\t\treturn fmt.Errorf(\"open db file descriptor: %w\", err)\n\t}\n\n\t// Ensure database is closed if init fails.\n\t// Initialization can retry on next sync.\n\tdefer func() {\n\t\tif err != nil {\n\t\t\t_ = db.releaseReadLock()\n\t\t\tdb.db.Close()\n\t\t\tdb.f.Close()\n\t\t\tdb.db, db.f = nil, nil\n\t\t}\n\t}()\n\n\t// Enable WAL and ensure it is set. New mode should be returned on success:\n\t// https://www.sqlite.org/pragma.html#pragma_journal_mode\n\tvar mode string\n\tif err := db.db.QueryRow(`PRAGMA journal_mode = wal;`).Scan(&mode); err != nil {\n\t\treturn err\n\t} else if mode != \"wal\" {\n\t\treturn fmt.Errorf(\"enable wal failed, mode=%q\", mode)\n\t}\n\n\t// Disable autocheckpoint for litestream's connection.\n\tif _, err := db.db.ExecContext(db.ctx, `PRAGMA wal_autocheckpoint = 0;`); err != nil {\n\t\treturn fmt.Errorf(\"disable autocheckpoint: %w\", err)\n\t}\n\n\t// Create a table to force writes to the WAL when empty.\n\t// There should only ever be one row with id=1.\n\tif _, err := db.db.Exec(`CREATE TABLE IF NOT EXISTS _litestream_seq (id INTEGER PRIMARY KEY, seq INTEGER);`); err != nil {\n\t\treturn fmt.Errorf(\"create _litestream_seq table: %w\", err)\n\t}\n\n\t// Create a lock table to force write locks during sync.\n\t// The sync write transaction always rolls back so no data should be in this table.\n\tif _, err := db.db.Exec(`CREATE TABLE IF NOT EXISTS _litestream_lock (id INTEGER);`); err != nil {\n\t\treturn fmt.Errorf(\"create _litestream_lock table: %w\", err)\n\t}\n\n\t// Start a long-running read transaction to prevent other transactions\n\t// from checkpointing.\n\tif err := db.acquireReadLock(); err != nil {\n\t\treturn fmt.Errorf(\"acquire read lock: %w\", err)\n\t}\n\n\t// Read page size.\n\tif err := db.db.QueryRow(`PRAGMA page_size;`).Scan(&db.pageSize); err != nil {\n\t\treturn fmt.Errorf(\"read page size: %w\", err)\n\t} else if db.pageSize <= 0 {\n\t\treturn fmt.Errorf(\"invalid db page size: %d\", db.pageSize)\n\t}\n\n\t// Ensure meta directory structure exists.\n\tif err := internal.MkdirAll(db.metaPath, db.dirInfo); err != nil {\n\t\treturn err\n\t}\n\n\t// If we have an existing shadow WAL, ensure the headers match.\n\tif err := db.verifyHeadersMatch(); err != nil {\n\t\tdb.Logger.Warn(\"init: cannot determine last wal position, clearing generation\", \"error\", err)\n\t\tif err := os.Remove(db.GenerationNamePath()); err != nil && !os.IsNotExist(err) {\n\t\t\treturn fmt.Errorf(\"remove generation name: %w\", err)\n\t\t}\n\t}\n\n\t// Clean up previous generations.\n\tif err := db.clean(); err != nil {\n\t\treturn fmt.Errorf(\"clean: %w\", err)\n\t}\n\n\t// Start replication.\n\tfor _, r := range db.Replicas {\n\t\tr.Start(db.ctx)\n\t}\n\n\treturn nil\n}\n\n// verifyHeadersMatch returns true if the primary WAL and last shadow WAL header match.\nfunc (db *DB) verifyHeadersMatch() error {\n\t// Determine current generation.\n\tgeneration, err := db.CurrentGeneration()\n\tif err != nil {\n\t\treturn err\n\t} else if generation == \"\" {\n\t\treturn nil\n\t}\n\n\t// Find current generation & latest shadow WAL.\n\tshadowWALPath, err := db.CurrentShadowWALPath(generation)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"cannot determine current shadow wal path: %w\", err)\n\t}\n\n\thdr0, err := readWALHeader(db.WALPath())\n\tif os.IsNotExist(err) {\n\t\treturn fmt.Errorf(\"no primary wal: %w\", err)\n\t} else if err != nil {\n\t\treturn fmt.Errorf(\"primary wal header: %w\", err)\n\t}\n\n\thdr1, err := readWALHeader(shadowWALPath)\n\tif os.IsNotExist(err) {\n\t\treturn fmt.Errorf(\"no shadow wal\")\n\t} else if err != nil {\n\t\treturn fmt.Errorf(\"shadow wal header: %w\", err)\n\t}\n\n\tif !bytes.Equal(hdr0, hdr1) {\n\t\treturn fmt.Errorf(\"wal header mismatch %x <> %x on %s\", hdr0, hdr1, shadowWALPath)\n\t}\n\treturn nil\n}\n\n// clean removes old generations & WAL files.\nfunc (db *DB) clean() error {\n\tif err := db.cleanGenerations(); err != nil {\n\t\treturn err\n\t}\n\treturn db.cleanWAL()\n}\n\n// cleanGenerations removes old generations.\nfunc (db *DB) cleanGenerations() error {\n\tgeneration, err := db.CurrentGeneration()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdir := filepath.Join(db.metaPath, \"generations\")\n\tfis, err := os.ReadDir(dir)\n\tif os.IsNotExist(err) {\n\t\treturn nil\n\t} else if err != nil {\n\t\treturn err\n\t}\n\tfor _, fi := range fis {\n\t\t// Skip the current generation.\n\t\tif filepath.Base(fi.Name()) == generation {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Delete all other generations.\n\t\tif err := os.RemoveAll(filepath.Join(dir, fi.Name())); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// cleanWAL removes WAL files that have been replicated.\nfunc (db *DB) cleanWAL() error {\n\tgeneration, err := db.CurrentGeneration()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Determine lowest index that's been replicated to all replicas.\n\tmin := -1\n\tfor _, r := range db.Replicas {\n\t\tpos := r.Pos()\n\t\tif pos.Generation != generation {\n\t\t\tpos = Pos{} // different generation, reset index to zero\n\t\t}\n\t\tif min == -1 || pos.Index < min {\n\t\t\tmin = pos.Index\n\t\t}\n\t}\n\n\t// Skip if our lowest index is too small.\n\tif min <= 0 {\n\t\treturn nil\n\t}\n\tmin-- // Keep an extra WAL file.\n\n\t// Remove all WAL files for the generation before the lowest index.\n\tdir := db.ShadowWALDir(generation)\n\tfis, err := os.ReadDir(dir)\n\tif os.IsNotExist(err) {\n\t\treturn nil\n\t} else if err != nil {\n\t\treturn err\n\t}\n\tfor _, fi := range fis {\n\t\tif idx, err := ParseWALPath(fi.Name()); err != nil || idx >= min {\n\t\t\tcontinue\n\t\t}\n\t\tif err := os.Remove(filepath.Join(dir, fi.Name())); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// acquireReadLock begins a read transaction on the database to prevent checkpointing.\nfunc (db *DB) acquireReadLock() error {\n\tif db.rtx != nil {\n\t\treturn nil\n\t}\n\n\t// Start long running read-transaction to prevent checkpoints.\n\ttx, err := db.db.Begin()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Execute read query to obtain read lock.\n\tif _, err := tx.ExecContext(db.ctx, `SELECT COUNT(1) FROM _litestream_seq;`); err != nil {\n\t\t_ = tx.Rollback()\n\t\treturn err\n\t}\n\n\t// Track transaction so we can release it later before checkpoint.\n\tdb.rtx = tx\n\treturn nil\n}\n\n// releaseReadLock rolls back the long-running read transaction.\nfunc (db *DB) releaseReadLock() error {\n\t// Ignore if we do not have a read lock.\n\tif db.rtx == nil {\n\t\treturn nil\n\t}\n\n\t// Rollback & clear read transaction.\n\terr := db.rtx.Rollback()\n\tdb.rtx = nil\n\treturn err\n}\n\n// CurrentGeneration returns the name of the generation saved to the \"generation\"\n// file in the meta data directory. Returns empty string if none exists.\nfunc (db *DB) CurrentGeneration() (string, error) {\n\tbuf, err := os.ReadFile(db.GenerationNamePath())\n\tif os.IsNotExist(err) {\n\t\treturn \"\", nil\n\t} else if err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// TODO: Verify if generation directory exists. If not, delete name file.\n\n\tgeneration := strings.TrimSpace(string(buf))\n\tif len(generation) != GenerationNameLen {\n\t\treturn \"\", nil\n\t}\n\treturn generation, nil\n}\n\n// createGeneration starts a new generation by creating the generation\n// directory, snapshotting to each replica, and updating the current\n// generation name.\nfunc (db *DB) createGeneration() (string, error) {\n\t// Generate random generation hex name.\n\tbuf := make([]byte, GenerationNameLen/2)\n\t_, _ = rand.New(rand.NewSource(time.Now().UnixNano())).Read(buf)\n\tgeneration := hex.EncodeToString(buf)\n\n\t// Generate new directory.\n\tdir := filepath.Join(db.metaPath, \"generations\", generation)\n\tif err := internal.MkdirAll(dir, db.dirInfo); err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Initialize shadow WAL with copy of header.\n\tif _, err := db.initShadowWALFile(db.ShadowWALPath(generation, 0)); err != nil {\n\t\treturn \"\", fmt.Errorf(\"initialize shadow wal: %w\", err)\n\t}\n\n\t// Atomically write generation name as current generation.\n\tgenerationNamePath := db.GenerationNamePath()\n\tmode := os.FileMode(0600)\n\tif db.fileInfo != nil {\n\t\tmode = db.fileInfo.Mode()\n\t}\n\tif err := os.WriteFile(generationNamePath+\".tmp\", []byte(generation+\"\\n\"), mode); err != nil {\n\t\treturn \"\", fmt.Errorf(\"write generation temp file: %w\", err)\n\t}\n\tuid, gid := internal.Fileinfo(db.fileInfo)\n\t_ = os.Chown(generationNamePath+\".tmp\", uid, gid)\n\tif err := os.Rename(generationNamePath+\".tmp\", generationNamePath); err != nil {\n\t\treturn \"\", fmt.Errorf(\"rename generation file: %w\", err)\n\t}\n\n\t// Remove old generations.\n\tif err := db.clean(); err != nil {\n\t\treturn \"\", err\n\t}\n\n\treturn generation, nil\n}\n\n// Sync copies pending data from the WAL to the shadow WAL.\nfunc (db *DB) Sync(ctx context.Context) (err error) {\n\tdb.mu.Lock()\n\tdefer db.mu.Unlock()\n\n\t// Initialize database, if necessary. Exit if no DB exists.\n\tif err := db.init(); err != nil {\n\t\treturn err\n\t} else if db.db == nil {\n\t\tdb.Logger.Debug(\"sync: no database found\")\n\t\treturn nil\n\t}\n\n\t// Track total sync metrics.\n\tt := time.Now()\n\tdefer func() {\n\t\tdb.syncNCounter.Inc()\n\t\tif err != nil {\n\t\t\tdb.syncErrorNCounter.Inc()\n\t\t}\n\t\tdb.syncSecondsCounter.Add(float64(time.Since(t).Seconds()))\n\t}()\n\n\t// Ensure WAL has at least one frame in it.\n\tif err := db.ensureWALExists(); err != nil {\n\t\treturn fmt.Errorf(\"ensure wal exists: %w\", err)\n\t}\n\n\t// Verify our last sync matches the current state of the WAL.\n\t// This ensures that we have an existing generation & that the last sync\n\t// position of the real WAL hasn't been overwritten by another process.\n\tinfo, err := db.verify()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"cannot verify wal state: %w\", err)\n\t}\n\tdb.Logger.Debug(\"sync\", \"info\", &info)\n\n\t// Track if anything in the shadow WAL changes and then notify at the end.\n\tchanged := info.walSize != info.shadowWALSize || info.restart || info.reason != \"\"\n\n\t// If we are unable to verify the WAL state then we start a new generation.\n\tif info.reason != \"\" {\n\t\t// Start new generation & notify user via log message.\n\t\tif info.generation, err = db.createGeneration(); err != nil {\n\t\t\treturn fmt.Errorf(\"create generation: %w\", err)\n\t\t}\n\t\tdb.Logger.Info(\"sync: new generation\", \"generation\", info.generation, \"reason\", info.reason)\n\n\t\t// Clear shadow wal info.\n\t\tinfo.shadowWALPath = db.ShadowWALPath(info.generation, 0)\n\t\tinfo.shadowWALSize = WALHeaderSize\n\t\tinfo.restart = false\n\t\tinfo.reason = \"\"\n\n\t}\n\n\t// Synchronize real WAL with current shadow WAL.\n\torigWALSize, newWALSize, err := db.syncWAL(info)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"sync wal: %w\", err)\n\t}\n\n\t// If WAL size is great than max threshold, force checkpoint.\n\t// If WAL size is greater than min threshold, attempt checkpoint.\n\tvar checkpoint bool\n\tcheckpointMode := CheckpointModePassive\n\tif db.TruncatePageN > 0 && origWALSize >= calcWALSize(db.pageSize, db.TruncatePageN) {\n\t\tcheckpoint, checkpointMode = true, CheckpointModeTruncate\n\t} else if db.MaxCheckpointPageN > 0 && newWALSize >= calcWALSize(db.pageSize, db.MaxCheckpointPageN) {\n\t\tcheckpoint, checkpointMode = true, CheckpointModeRestart\n\t} else if newWALSize >= calcWALSize(db.pageSize, db.MinCheckpointPageN) {\n\t\tcheckpoint = true\n\t} else if db.CheckpointInterval > 0 && !info.dbModTime.IsZero() && time.Since(info.dbModTime) > db.CheckpointInterval && newWALSize > calcWALSize(db.pageSize, 1) {\n\t\tcheckpoint = true\n\t}\n\n\t// Issue the checkpoint.\n\tif checkpoint {\n\t\tchanged = true\n\n\t\tif err := db.checkpoint(ctx, info.generation, checkpointMode); err != nil {\n\t\t\treturn fmt.Errorf(\"checkpoint: mode=%v err=%w\", checkpointMode, err)\n\t\t}\n\t}\n\n\t// Clean up any old files.\n\tif err := db.clean(); err != nil {\n\t\treturn fmt.Errorf(\"cannot clean: %w\", err)\n\t}\n\n\t// Compute current index and total shadow WAL size.\n\t// This is only for metrics so we ignore any errors that occur.\n\tindex, size, _ := db.CurrentShadowWALIndex(info.generation)\n\tdb.shadowWALIndexGauge.Set(float64(index))\n\tdb.shadowWALSizeGauge.Set(float64(size))\n\n\t// Notify replicas of WAL changes.\n\tif changed {\n\t\tclose(db.notify)\n\t\tdb.notify = make(chan struct{})\n\t}\n\n\tdb.Logger.Debug(\"sync: ok\")\n\n\treturn nil\n}\n\n// ensureWALExists checks that the real WAL exists and has a header.\nfunc (db *DB) ensureWALExists() (err error) {\n\t// Exit early if WAL header exists.\n\tif fi, err := os.Stat(db.WALPath()); err == nil && fi.Size() >= WALHeaderSize {\n\t\treturn nil\n\t}\n\n\t// Otherwise create transaction that updates the internal litestream table.\n\t_, err = db.db.Exec(`INSERT INTO _litestream_seq (id, seq) VALUES (1, 1) ON CONFLICT (id) DO UPDATE SET seq = seq + 1`)\n\treturn err\n}\n\n// verify ensures the current shadow WAL state matches where it left off from\n// the real WAL. Returns generation & WAL sync information. If info.reason is\n// not blank, verification failed and a new generation should be started.\nfunc (db *DB) verify() (info syncInfo, err error) {\n\t// Look up existing generation.\n\tgeneration, err := db.CurrentGeneration()\n\tif err != nil {\n\t\treturn info, fmt.Errorf(\"cannot find current generation: %w\", err)\n\t} else if generation == \"\" {\n\t\tinfo.reason = \"no generation exists\"\n\t\treturn info, nil\n\t}\n\tinfo.generation = generation\n\n\t// Determine total bytes of real DB for metrics.\n\tfi, err := os.Stat(db.Path())\n\tif err != nil {\n\t\treturn info, err\n\t}\n\tinfo.dbModTime = fi.ModTime()\n\tdb.dbSizeGauge.Set(float64(fi.Size()))\n\n\t// Determine total bytes of real WAL.\n\tfi, err = os.Stat(db.WALPath())\n\tif err != nil {\n\t\treturn info, err\n\t}\n\tinfo.walSize = frameAlign(fi.Size(), db.pageSize)\n\tinfo.walModTime = fi.ModTime()\n\tdb.walSizeGauge.Set(float64(fi.Size()))\n\n\t// Open shadow WAL to copy append to.\n\tindex, _, err := db.CurrentShadowWALIndex(info.generation)\n\tif err != nil {\n\t\treturn info, fmt.Errorf(\"cannot determine shadow WAL index: %w\", err)\n\t} else if index >= MaxIndex {\n\t\tinfo.reason = \"max index exceeded\"\n\t\treturn info, nil\n\t}\n\tinfo.shadowWALPath = db.ShadowWALPath(generation, index)\n\n\t// Determine shadow WAL current size.\n\tfi, err = os.Stat(info.shadowWALPath)\n\tif os.IsNotExist(err) {\n\t\tinfo.reason = \"no shadow wal\"\n\t\treturn info, nil\n\t} else if err != nil {\n\t\treturn info, err\n\t}\n\tinfo.shadowWALSize = frameAlign(fi.Size(), db.pageSize)\n\n\t// Exit if shadow WAL does not contain a full header.\n\tif info.shadowWALSize < WALHeaderSize {\n\t\tinfo.reason = \"short shadow wal\"\n\t\treturn info, nil\n\t}\n\n\t// If shadow WAL is larger than real WAL then the WAL has been truncated\n\t// so we cannot determine our last state.\n\tif info.shadowWALSize > info.walSize {\n\t\tinfo.reason = \"wal truncated by another process\"\n\t\treturn info, nil\n\t}\n\n\t// Compare WAL headers. Start a new shadow WAL if they are mismatched.\n\tif hdr0, err := readWALHeader(db.WALPath()); err != nil {\n\t\treturn info, fmt.Errorf(\"cannot read wal header: %w\", err)\n\t} else if hdr1, err := readWALHeader(info.shadowWALPath); err != nil {\n\t\treturn info, fmt.Errorf(\"cannot read shadow wal header: %w\", err)\n\t} else if !bytes.Equal(hdr0, hdr1) {\n\t\tinfo.restart = !bytes.Equal(hdr0, hdr1)\n\t}\n\n\t// If we only have a header then ensure header matches.\n\t// Otherwise we need to start a new generation.\n\tif info.shadowWALSize == WALHeaderSize && info.restart {\n\t\tinfo.reason = \"wal header only, mismatched\"\n\t\treturn info, nil\n\t}\n\n\t// Verify last page synced still matches.\n\tif info.shadowWALSize > WALHeaderSize {\n\t\toffset := info.shadowWALSize - int64(db.pageSize+WALFrameHeaderSize)\n\t\tif buf0, err := readWALFileAt(db.WALPath(), offset, int64(db.pageSize+WALFrameHeaderSize)); err != nil {\n\t\t\treturn info, fmt.Errorf(\"cannot read last synced wal page: %w\", err)\n\t\t} else if buf1, err := readWALFileAt(info.shadowWALPath, offset, int64(db.pageSize+WALFrameHeaderSize)); err != nil {\n\t\t\treturn info, fmt.Errorf(\"cannot read last synced shadow wal page: %w\", err)\n\t\t} else if !bytes.Equal(buf0, buf1) {\n\t\t\tinfo.reason = \"wal overwritten by another process\"\n\t\t\treturn info, nil\n\t\t}\n\t}\n\n\treturn info, nil\n}\n\ntype syncInfo struct {\n\tgeneration    string    // generation name\n\tdbModTime     time.Time // last modified date of real DB file\n\twalSize       int64     // size of real WAL file\n\twalModTime    time.Time // last modified date of real WAL file\n\tshadowWALPath string    // name of last shadow WAL file\n\tshadowWALSize int64     // size of last shadow WAL file\n\trestart       bool      // if true, real WAL header does not match shadow WAL\n\treason        string    // if non-blank, reason for sync failure\n}\n\n// syncWAL copies pending bytes from the real WAL to the shadow WAL.\nfunc (db *DB) syncWAL(info syncInfo) (origSize int64, newSize int64, err error) {\n\t// Copy WAL starting from end of shadow WAL. Exit if no new shadow WAL needed.\n\torigSize, newSize, err = db.copyToShadowWAL(info.shadowWALPath)\n\tif err != nil {\n\t\treturn origSize, newSize, fmt.Errorf(\"cannot copy to shadow wal: %w\", err)\n\t} else if !info.restart {\n\t\treturn origSize, newSize, nil // If no restart required, exit.\n\t}\n\n\t// Parse index of current shadow WAL file.\n\tdir, base := filepath.Split(info.shadowWALPath)\n\tindex, err := ParseWALPath(base)\n\tif err != nil {\n\t\treturn 0, 0, fmt.Errorf(\"cannot parse shadow wal filename: %s\", base)\n\t}\n\n\t// Start a new shadow WAL file with next index.\n\tnewShadowWALPath := filepath.Join(dir, FormatWALPath(index+1))\n\tnewSize, err = db.initShadowWALFile(newShadowWALPath)\n\tif err != nil {\n\t\treturn 0, 0, fmt.Errorf(\"cannot init shadow wal file: name=%s err=%w\", newShadowWALPath, err)\n\t}\n\treturn origSize, newSize, nil\n}\n\nfunc (db *DB) initShadowWALFile(filename string) (int64, error) {\n\thdr, err := readWALHeader(db.WALPath())\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"read header: %w\", err)\n\t}\n\n\t// Determine byte order for checksumming from header magic.\n\tbo, err := headerByteOrder(hdr)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\t// Verify checksum.\n\ts0 := binary.BigEndian.Uint32(hdr[24:])\n\ts1 := binary.BigEndian.Uint32(hdr[28:])\n\tif v0, v1 := Checksum(bo, 0, 0, hdr[:24]); v0 != s0 || v1 != s1 {\n\t\treturn 0, fmt.Errorf(\"invalid header checksum: (%x,%x) != (%x,%x)\", v0, v1, s0, s1)\n\t}\n\n\t// Write header to new WAL shadow file.\n\tmode := os.FileMode(0600)\n\tif fi := db.fileInfo; fi != nil {\n\t\tmode = fi.Mode()\n\t}\n\tif err := internal.MkdirAll(filepath.Dir(filename), db.dirInfo); err != nil {\n\t\treturn 0, err\n\t} else if err := os.WriteFile(filename, hdr, mode); err != nil {\n\t\treturn 0, err\n\t}\n\tuid, gid := internal.Fileinfo(db.fileInfo)\n\t_ = os.Chown(filename, uid, gid)\n\n\t// Copy as much shadow WAL as available.\n\t_, newSize, err := db.copyToShadowWAL(filename)\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"cannot copy to new shadow wal: %w\", err)\n\t}\n\treturn newSize, nil\n}\n\nfunc (db *DB) copyToShadowWAL(filename string) (origWalSize int64, newSize int64, err error) {\n\tlogger := db.Logger.With(\"filename\", filename)\n\tlogger.Debug(\"copy-shadow\")\n\n\tr, err := os.Open(db.WALPath())\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\tdefer r.Close()\n\n\tfi, err := r.Stat()\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\torigWalSize = frameAlign(fi.Size(), db.pageSize)\n\n\tw, err := os.OpenFile(filename, os.O_RDWR, 0666)\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\tdefer w.Close()\n\n\tfi, err = w.Stat()\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\torigSize := frameAlign(fi.Size(), db.pageSize)\n\n\t// Read shadow WAL header to determine byte order for checksum & salt.\n\thdr := make([]byte, WALHeaderSize)\n\tif _, err := io.ReadFull(w, hdr); err != nil {\n\t\treturn 0, 0, fmt.Errorf(\"read header: %w\", err)\n\t}\n\thsalt0 := binary.BigEndian.Uint32(hdr[16:])\n\thsalt1 := binary.BigEndian.Uint32(hdr[20:])\n\n\tbo, err := headerByteOrder(hdr)\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\t// Read previous checksum.\n\tchksum0, chksum1, err := readLastChecksumFrom(w, db.pageSize)\n\tif err != nil {\n\t\treturn 0, 0, fmt.Errorf(\"last checksum: %w\", err)\n\t}\n\n\t// Write to a temporary shadow file.\n\ttempFilename := filename + \".tmp\"\n\tdefer os.Remove(tempFilename)\n\n\tf, err := internal.CreateFile(tempFilename, db.fileInfo)\n\tif err != nil {\n\t\treturn 0, 0, fmt.Errorf(\"create temp file: %w\", err)\n\t}\n\tdefer f.Close()\n\n\t// Seek to correct position on real wal.\n\tif _, err := r.Seek(origSize, io.SeekStart); err != nil {\n\t\treturn 0, 0, fmt.Errorf(\"real wal seek: %w\", err)\n\t} else if _, err := w.Seek(origSize, io.SeekStart); err != nil {\n\t\treturn 0, 0, fmt.Errorf(\"shadow wal seek: %w\", err)\n\t}\n\n\t// Read through WAL from last position to find the page of the last\n\t// committed transaction.\n\tframe := make([]byte, db.pageSize+WALFrameHeaderSize)\n\toffset := origSize\n\tlastCommitSize := origSize\n\tfor {\n\t\t// Read next page from WAL file.\n\t\tif _, err := io.ReadFull(r, frame); err == io.EOF || err == io.ErrUnexpectedEOF {\n\t\t\tlogger.Debug(\"copy-shadow: break\", \"offset\", offset, \"error\", err)\n\t\t\tbreak // end of file or partial page\n\t\t} else if err != nil {\n\t\t\treturn 0, 0, fmt.Errorf(\"read wal: %w\", err)\n\t\t}\n\n\t\t// Read frame salt & compare to header salt. Stop reading on mismatch.\n\t\tsalt0 := binary.BigEndian.Uint32(frame[8:])\n\t\tsalt1 := binary.BigEndian.Uint32(frame[12:])\n\t\tif salt0 != hsalt0 || salt1 != hsalt1 {\n\t\t\tlogger.Debug(\"copy-shadow: break: salt mismatch\")\n\t\t\tbreak\n\t\t}\n\n\t\t// Verify checksum of page is valid.\n\t\tfchksum0 := binary.BigEndian.Uint32(frame[16:])\n\t\tfchksum1 := binary.BigEndian.Uint32(frame[20:])\n\t\tchksum0, chksum1 = Checksum(bo, chksum0, chksum1, frame[:8])  // frame header\n\t\tchksum0, chksum1 = Checksum(bo, chksum0, chksum1, frame[24:]) // frame data\n\t\tif chksum0 != fchksum0 || chksum1 != fchksum1 {\n\t\t\tlogger.Debug(\"copy shadow: checksum mismatch, skipping\", \"offset\", offset, \"check\", fmt.Sprintf(\"(%x,%x) != (%x,%x)\", chksum0, chksum1, fchksum0, fchksum1))\n\t\t\tbreak\n\t\t}\n\n\t\t// Write page to temporary WAL file.\n\t\tif _, err := f.Write(frame); err != nil {\n\t\t\treturn 0, 0, fmt.Errorf(\"write temp shadow wal: %w\", err)\n\t\t}\n\n\t\tlogger.Debug(\"copy-shadow: ok\", \"offset\", offset, \"salt\", fmt.Sprintf(\"%x %x\", salt0, salt1))\n\t\toffset += int64(len(frame))\n\n\t\t// Update new size if written frame was a commit record.\n\t\tnewDBSize := binary.BigEndian.Uint32(frame[4:])\n\t\tif newDBSize != 0 {\n\t\t\tlastCommitSize = offset\n\t\t}\n\t}\n\n\t// If no WAL writes found, exit.\n\tif origSize == lastCommitSize {\n\t\treturn origSize, lastCommitSize, nil\n\t}\n\n\twalByteN := lastCommitSize - origSize\n\n\t// Move to beginning of temporary file.\n\tif _, err := f.Seek(0, io.SeekStart); err != nil {\n\t\treturn 0, 0, fmt.Errorf(\"temp file seek: %w\", err)\n\t}\n\n\t// Copy from temporary file to shadow WAL.\n\tif _, err := io.Copy(w, &io.LimitedReader{R: f, N: walByteN}); err != nil {\n\t\treturn 0, 0, fmt.Errorf(\"write shadow file: %w\", err)\n\t}\n\n\t// Close & remove temporary file.\n\tif err := f.Close(); err != nil {\n\t\treturn 0, 0, err\n\t} else if err := os.Remove(tempFilename); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\t// Sync & close shadow WAL.\n\tif err := w.Sync(); err != nil {\n\t\treturn 0, 0, err\n\t} else if err := w.Close(); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\t// Track total number of bytes written to WAL.\n\tdb.totalWALBytesCounter.Add(float64(walByteN))\n\n\treturn origWalSize, lastCommitSize, nil\n}\n\n// ShadowWALReader opens a reader for a shadow WAL file at a given position.\n// If the reader is at the end of the file, it attempts to return the next file.\n//\n// The caller should check Pos() & Size() on the returned reader to check offset.\nfunc (db *DB) ShadowWALReader(pos Pos) (r *ShadowWALReader, err error) {\n\t// Fetch reader for the requested position. Return if it has data.\n\tr, err = db.shadowWALReader(pos)\n\tif err != nil {\n\t\treturn nil, err\n\t} else if r.N() > 0 {\n\t\treturn r, nil\n\t} else if err := r.Close(); err != nil { // no data, close, try next\n\t\treturn nil, err\n\t}\n\n\t// Otherwise attempt to read the start of the next WAL file.\n\tpos.Index, pos.Offset = pos.Index+1, 0\n\n\tr, err = db.shadowWALReader(pos)\n\tif os.IsNotExist(err) {\n\t\treturn nil, io.EOF\n\t}\n\treturn r, err\n}\n\n// shadowWALReader opens a file reader for a shadow WAL file at a given position.\nfunc (db *DB) shadowWALReader(pos Pos) (r *ShadowWALReader, err error) {\n\tfilename := db.ShadowWALPath(pos.Generation, pos.Index)\n\n\tf, err := os.Open(filename)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Ensure file is closed if any error occurs.\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tf.Close()\n\t\t}\n\t}()\n\n\t// Fetch frame-aligned file size and ensure requested offset is not past EOF.\n\tfi, err := f.Stat()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfileSize := frameAlign(fi.Size(), db.pageSize)\n\tif pos.Offset > fileSize {\n\t\treturn nil, fmt.Errorf(\"wal reader offset too high: %d > %d\", pos.Offset, fi.Size())\n\t}\n\n\t// Move file handle to offset position.\n\tif _, err := f.Seek(pos.Offset, io.SeekStart); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &ShadowWALReader{\n\t\tf:   f,\n\t\tn:   fileSize - pos.Offset,\n\t\tpos: pos,\n\t}, nil\n}\n\n// frameAlign returns a frame-aligned offset.\n// Returns zero if offset is less than the WAL header size.\nfunc frameAlign(offset int64, pageSize int) int64 {\n\tassert(offset >= 0, \"frameAlign(): offset must be non-negative\")\n\tassert(pageSize >= 0, \"frameAlign(): page size must be non-negative\")\n\n\tif offset < WALHeaderSize {\n\t\treturn 0\n\t}\n\n\tframeSize := WALFrameHeaderSize + int64(pageSize)\n\tframeN := (offset - WALHeaderSize) / frameSize\n\treturn (frameN * frameSize) + WALHeaderSize\n}\n\n// ShadowWALReader represents a reader for a shadow WAL file that tracks WAL position.\ntype ShadowWALReader struct {\n\tf   *os.File\n\tn   int64\n\tpos Pos\n}\n\n// Name returns the filename of the underlying file.\nfunc (r *ShadowWALReader) Name() string { return r.f.Name() }\n\n// Close closes the underlying WAL file handle.\nfunc (r *ShadowWALReader) Close() error { return r.f.Close() }\n\n// N returns the remaining bytes in the reader.\nfunc (r *ShadowWALReader) N() int64 { return r.n }\n\n// Pos returns the current WAL position.\nfunc (r *ShadowWALReader) Pos() Pos { return r.pos }\n\n// Read reads bytes into p, updates the position, and returns the bytes read.\n// Returns io.EOF at the end of the available section of the WAL.\nfunc (r *ShadowWALReader) Read(p []byte) (n int, err error) {\n\tif r.n <= 0 {\n\t\treturn 0, io.EOF\n\t}\n\tif int64(len(p)) > r.n {\n\t\tp = p[0:r.n]\n\t}\n\tn, err = r.f.Read(p)\n\tr.n -= int64(n)\n\tr.pos.Offset += int64(n)\n\treturn n, err\n}\n\n// SQLite WAL constants\nconst (\n\tWALHeaderChecksumOffset      = 24\n\tWALFrameHeaderChecksumOffset = 16\n)\n\nfunc readLastChecksumFrom(f *os.File, pageSize int) (uint32, uint32, error) {\n\t// Determine the byte offset of the checksum for the header (if no pages\n\t// exist) or for the last page (if at least one page exists).\n\toffset := int64(WALHeaderChecksumOffset)\n\tif fi, err := f.Stat(); err != nil {\n\t\treturn 0, 0, err\n\t} else if sz := frameAlign(fi.Size(), pageSize); fi.Size() > WALHeaderSize {\n\t\toffset = sz - int64(pageSize) - WALFrameHeaderSize + WALFrameHeaderChecksumOffset\n\t}\n\n\t// Read big endian checksum.\n\tb := make([]byte, 8)\n\tif n, err := f.ReadAt(b, offset); err != nil {\n\t\treturn 0, 0, err\n\t} else if n != len(b) {\n\t\treturn 0, 0, io.ErrUnexpectedEOF\n\t}\n\treturn binary.BigEndian.Uint32(b[0:]), binary.BigEndian.Uint32(b[4:]), nil\n}\n\n// Checkpoint performs a checkpoint on the WAL file.\nfunc (db *DB) Checkpoint(ctx context.Context, mode string) (err error) {\n\tdb.mu.Lock()\n\tdefer db.mu.Unlock()\n\n\tgeneration, err := db.CurrentGeneration()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"cannot determine generation: %w\", err)\n\t}\n\treturn db.checkpoint(ctx, generation, mode)\n}\n\n// checkpoint performs a checkpoint on the WAL file and initializes a\n// new shadow WAL file.\nfunc (db *DB) checkpoint(ctx context.Context, generation, mode string) error {\n\t// Try getting a checkpoint lock, will fail during snapshots.\n\tif !db.chkMu.TryLock() {\n\t\treturn nil\n\t}\n\tdefer db.chkMu.Unlock()\n\n\tshadowWALPath, err := db.CurrentShadowWALPath(generation)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Read WAL header before checkpoint to check if it has been restarted.\n\thdr, err := readWALHeader(db.WALPath())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Copy shadow WAL before checkpoint to copy as much as possible.\n\tif _, _, err := db.copyToShadowWAL(shadowWALPath); err != nil {\n\t\treturn fmt.Errorf(\"cannot copy to end of shadow wal before checkpoint: %w\", err)\n\t}\n\n\t// Execute checkpoint and immediately issue a write to the WAL to ensure\n\t// a new page is written.\n\tif err := db.execCheckpoint(mode); err != nil {\n\t\treturn err\n\t} else if _, err = db.db.Exec(`INSERT INTO _litestream_seq (id, seq) VALUES (1, 1) ON CONFLICT (id) DO UPDATE SET seq = seq + 1`); err != nil {\n\t\treturn err\n\t}\n\n\t// If WAL hasn't been restarted, exit.\n\tif other, err := readWALHeader(db.WALPath()); err != nil {\n\t\treturn err\n\t} else if bytes.Equal(hdr, other) {\n\t\treturn nil\n\t}\n\n\t// Start a transaction. This will be promoted immediately after.\n\ttx, err := db.db.Begin()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"begin: %w\", err)\n\t}\n\tdefer func() { _ = rollback(tx) }()\n\n\t// Insert into the lock table to promote to a write tx. The lock table\n\t// insert will never actually occur because our tx will be rolled back,\n\t// however, it will ensure our tx grabs the write lock. Unfortunately,\n\t// we can't call \"BEGIN IMMEDIATE\" as we are already in a transaction.\n\tif _, err := tx.ExecContext(ctx, `INSERT INTO _litestream_lock (id) VALUES (1);`); err != nil {\n\t\treturn fmt.Errorf(\"_litestream_lock: %w\", err)\n\t}\n\n\t// Copy the end of the previous WAL before starting a new shadow WAL.\n\tif _, _, err := db.copyToShadowWAL(shadowWALPath); err != nil {\n\t\treturn fmt.Errorf(\"cannot copy to end of shadow wal: %w\", err)\n\t}\n\n\t// Parse index of current shadow WAL file.\n\tindex, err := ParseWALPath(shadowWALPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"cannot parse shadow wal filename: %s\", shadowWALPath)\n\t}\n\n\t// Start a new shadow WAL file with next index.\n\tnewShadowWALPath := filepath.Join(filepath.Dir(shadowWALPath), FormatWALPath(index+1))\n\tif _, err := db.initShadowWALFile(newShadowWALPath); err != nil {\n\t\treturn fmt.Errorf(\"cannot init shadow wal file: name=%s err=%w\", newShadowWALPath, err)\n\t}\n\n\t// Release write lock before checkpointing & exiting.\n\tif err := tx.Rollback(); err != nil {\n\t\treturn fmt.Errorf(\"rollback post-checkpoint tx: %w\", err)\n\t}\n\treturn nil\n}\n\nfunc (db *DB) execCheckpoint(mode string) (err error) {\n\t// Ignore if there is no underlying database.\n\tif db.db == nil {\n\t\treturn nil\n\t}\n\n\t// Track checkpoint metrics.\n\tt := time.Now()\n\tdefer func() {\n\t\tlabels := prometheus.Labels{\"mode\": mode}\n\t\tdb.checkpointNCounterVec.With(labels).Inc()\n\t\tif err != nil {\n\t\t\tdb.checkpointErrorNCounterVec.With(labels).Inc()\n\t\t}\n\t\tdb.checkpointSecondsCounterVec.With(labels).Add(float64(time.Since(t).Seconds()))\n\t}()\n\n\t// Ensure the read lock has been removed before issuing a checkpoint.\n\t// We defer the re-acquire to ensure it occurs even on an early return.\n\tif err := db.releaseReadLock(); err != nil {\n\t\treturn fmt.Errorf(\"release read lock: %w\", err)\n\t}\n\tdefer func() { _ = db.acquireReadLock() }()\n\n\t// A non-forced checkpoint is issued as \"PASSIVE\". This will only checkpoint\n\t// if there are not pending transactions. A forced checkpoint (\"RESTART\")\n\t// will wait for pending transactions to end & block new transactions before\n\t// forcing the checkpoint and restarting the WAL.\n\t//\n\t// See: https://www.sqlite.org/pragma.html#pragma_wal_checkpoint\n\trawsql := `PRAGMA wal_checkpoint(` + mode + `);`\n\n\tvar row [3]int\n\tif err := db.db.QueryRow(rawsql).Scan(&row[0], &row[1], &row[2]); err != nil {\n\t\treturn err\n\t}\n\tdb.Logger.Debug(\"checkpoint\", \"mode\", mode, \"result\", fmt.Sprintf(\"%d,%d,%d\", row[0], row[1], row[2]))\n\n\t// Reacquire the read lock immediately after the checkpoint.\n\tif err := db.acquireReadLock(); err != nil {\n\t\treturn fmt.Errorf(\"release read lock: %w\", err)\n\t}\n\n\treturn nil\n}\n\n// monitor runs in a separate goroutine and monitors the database & WAL.\nfunc (db *DB) monitor() {\n\tticker := time.NewTicker(db.MonitorInterval)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\t// Wait for ticker or context close.\n\t\tselect {\n\t\tcase <-db.ctx.Done():\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t}\n\n\t\t// Sync the database to the shadow WAL.\n\t\tif err := db.Sync(db.ctx); err != nil && !errors.Is(err, context.Canceled) {\n\t\t\tdb.Logger.Error(\"sync error\", \"error\", err)\n\t\t}\n\t}\n}\n\n// CalcRestoreTarget returns a replica & generation to restore from based on opt criteria.\nfunc (db *DB) CalcRestoreTarget(ctx context.Context, opt RestoreOptions) (*Replica, string, error) {\n\tvar target struct {\n\t\treplica    *Replica\n\t\tgeneration string\n\t\tupdatedAt  time.Time\n\t}\n\n\tfor _, r := range db.Replicas {\n\t\t// Skip replica if it does not match filter.\n\t\tif opt.ReplicaName != \"\" && r.Name() != opt.ReplicaName {\n\t\t\tcontinue\n\t\t}\n\n\t\tgeneration, updatedAt, err := r.CalcRestoreTarget(ctx, opt)\n\t\tif err != nil {\n\t\t\treturn nil, \"\", err\n\t\t}\n\n\t\t// Use the latest replica if we have multiple candidates.\n\t\tif !updatedAt.After(target.updatedAt) {\n\t\t\tcontinue\n\t\t}\n\n\t\ttarget.replica, target.generation, target.updatedAt = r, generation, updatedAt\n\t}\n\treturn target.replica, target.generation, nil\n}\n\n// applyWAL performs a truncating checkpoint on the given database.\nfunc applyWAL(ctx context.Context, index int, dbPath string) error {\n\t// Copy WAL file from it's staging path to the correct \"-wal\" location.\n\tif err := os.Rename(fmt.Sprintf(\"%s-%08x-wal\", dbPath, index), dbPath+\"-wal\"); err != nil {\n\t\treturn err\n\t}\n\n\t// Open SQLite database and force a truncating checkpoint.\n\td, err := sql.Open(\"sqlite3\", dbPath)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer d.Close()\n\n\tvar row [3]int\n\tif err := d.QueryRow(`PRAGMA wal_checkpoint(TRUNCATE);`).Scan(&row[0], &row[1], &row[2]); err != nil {\n\t\treturn err\n\t} else if row[0] != 0 {\n\t\treturn fmt.Errorf(\"truncation checkpoint failed during restore (%d,%d,%d)\", row[0], row[1], row[2])\n\t}\n\treturn d.Close()\n}\n\n// CRC64 returns a CRC-64 ISO checksum of the database and its current position.\n//\n// This function obtains a read lock so it prevents syncs from occurring until\n// the operation is complete. The database will still be usable but it will be\n// unable to checkpoint during this time.\n//\n// If dst is set, the database file is copied to that location before checksum.\nfunc (db *DB) CRC64(ctx context.Context) (uint64, Pos, error) {\n\tdb.mu.Lock()\n\tdefer db.mu.Unlock()\n\n\tif err := db.init(); err != nil {\n\t\treturn 0, Pos{}, err\n\t} else if db.db == nil {\n\t\treturn 0, Pos{}, os.ErrNotExist\n\t}\n\n\tgeneration, err := db.CurrentGeneration()\n\tif err != nil {\n\t\treturn 0, Pos{}, fmt.Errorf(\"cannot find current generation: %w\", err)\n\t} else if generation == \"\" {\n\t\treturn 0, Pos{}, fmt.Errorf(\"no current generation\")\n\t}\n\n\t// Force a RESTART checkpoint to ensure the database is at the start of the WAL.\n\tif err := db.checkpoint(ctx, generation, CheckpointModeRestart); err != nil {\n\t\treturn 0, Pos{}, err\n\t}\n\n\t// Obtain current position. Clear the offset since we are only reading the\n\t// DB and not applying the current WAL.\n\tpos, err := db.Pos()\n\tif err != nil {\n\t\treturn 0, pos, err\n\t}\n\tpos.Offset = 0\n\n\t// Seek to the beginning of the db file descriptor and checksum whole file.\n\th := crc64.New(crc64.MakeTable(crc64.ISO))\n\tif _, err := db.f.Seek(0, io.SeekStart); err != nil {\n\t\treturn 0, pos, err\n\t} else if _, err := io.Copy(h, db.f); err != nil {\n\t\treturn 0, pos, err\n\t}\n\treturn h.Sum64(), pos, nil\n}\n\n// BeginSnapshot takes an internal snapshot lock preventing checkpoints.\n//\n// When calling this the caller must also call EndSnapshot() once the snapshot\n// is finished.\nfunc (db *DB) BeginSnapshot() {\n\tdb.chkMu.Lock()\n}\n\n// EndSnapshot releases the internal snapshot lock that prevents checkpoints.\nfunc (db *DB) EndSnapshot() {\n\tdb.chkMu.Unlock()\n}\n\n// DefaultRestoreParallelism is the default parallelism when downloading WAL files.\nconst DefaultRestoreParallelism = 8\n\n// RestoreOptions represents options for DB.Restore().\ntype RestoreOptions struct {\n\t// Target path to restore into.\n\t// If blank, the original DB path is used.\n\tOutputPath string\n\n\t// Specific replica to restore from.\n\t// If blank, all replicas are considered.\n\tReplicaName string\n\n\t// Specific generation to restore from.\n\t// If blank, all generations considered.\n\tGeneration string\n\n\t// Specific index to restore from.\n\t// Set to math.MaxInt32 to ignore index.\n\tIndex int\n\n\t// Point-in-time to restore database.\n\t// If zero, database restore to most recent state available.\n\tTimestamp time.Time\n\n\t// Specifies how many WAL files are downloaded in parallel during restore.\n\tParallelism int\n}\n\n// NewRestoreOptions returns a new instance of RestoreOptions with defaults.\nfunc NewRestoreOptions() RestoreOptions {\n\treturn RestoreOptions{\n\t\tIndex:       math.MaxInt32,\n\t\tParallelism: DefaultRestoreParallelism,\n\t}\n}\n\n// Database metrics.\nvar (\n\tdbSizeGaugeVec = promauto.NewGaugeVec(prometheus.GaugeOpts{\n\t\tName: \"litestream_db_size\",\n\t\tHelp: \"The current size of the real DB\",\n\t}, []string{\"db\"})\n\n\twalSizeGaugeVec = promauto.NewGaugeVec(prometheus.GaugeOpts{\n\t\tName: \"litestream_wal_size\",\n\t\tHelp: \"The current size of the real WAL\",\n\t}, []string{\"db\"})\n\n\ttotalWALBytesCounterVec = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: \"litestream_total_wal_bytes\",\n\t\tHelp: \"Total number of bytes written to shadow WAL\",\n\t}, []string{\"db\"})\n\n\tshadowWALIndexGaugeVec = promauto.NewGaugeVec(prometheus.GaugeOpts{\n\t\tName: \"litestream_shadow_wal_index\",\n\t\tHelp: \"The current index of the shadow WAL\",\n\t}, []string{\"db\"})\n\n\tshadowWALSizeGaugeVec = promauto.NewGaugeVec(prometheus.GaugeOpts{\n\t\tName: \"litestream_shadow_wal_size\",\n\t\tHelp: \"Current size of shadow WAL, in bytes\",\n\t}, []string{\"db\"})\n\n\tsyncNCounterVec = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: \"litestream_sync_count\",\n\t\tHelp: \"Number of sync operations performed\",\n\t}, []string{\"db\"})\n\n\tsyncErrorNCounterVec = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: \"litestream_sync_error_count\",\n\t\tHelp: \"Number of sync errors that have occurred\",\n\t}, []string{\"db\"})\n\n\tsyncSecondsCounterVec = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: \"litestream_sync_seconds\",\n\t\tHelp: \"Time spent syncing shadow WAL, in seconds\",\n\t}, []string{\"db\"})\n\n\tcheckpointNCounterVec = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: \"litestream_checkpoint_count\",\n\t\tHelp: \"Number of checkpoint operations performed\",\n\t}, []string{\"db\", \"mode\"})\n\n\tcheckpointErrorNCounterVec = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: \"litestream_checkpoint_error_count\",\n\t\tHelp: \"Number of checkpoint errors that have occurred\",\n\t}, []string{\"db\", \"mode\"})\n\n\tcheckpointSecondsCounterVec = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: \"litestream_checkpoint_seconds\",\n\t\tHelp: \"Time spent checkpointing WAL, in seconds\",\n\t}, []string{\"db\", \"mode\"})\n)\n\nfunc headerByteOrder(hdr []byte) (binary.ByteOrder, error) {\n\tmagic := binary.BigEndian.Uint32(hdr[0:])\n\tswitch magic {\n\tcase 0x377f0682:\n\t\treturn binary.LittleEndian, nil\n\tcase 0x377f0683:\n\t\treturn binary.BigEndian, nil\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"invalid wal header magic: %x\", magic)\n\t}\n}\n"
        },
        {
          "name": "db_test.go",
          "type": "blob",
          "size": 18.0087890625,
          "content": "package litestream_test\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/benbjohnson/litestream\"\n)\n\nfunc TestDB_Path(t *testing.T) {\n\tdb := litestream.NewDB(\"/tmp/db\")\n\tif got, want := db.Path(), `/tmp/db`; got != want {\n\t\tt.Fatalf(\"Path()=%v, want %v\", got, want)\n\t}\n}\n\nfunc TestDB_WALPath(t *testing.T) {\n\tdb := litestream.NewDB(\"/tmp/db\")\n\tif got, want := db.WALPath(), `/tmp/db-wal`; got != want {\n\t\tt.Fatalf(\"WALPath()=%v, want %v\", got, want)\n\t}\n}\n\nfunc TestDB_MetaPath(t *testing.T) {\n\tt.Run(\"Absolute\", func(t *testing.T) {\n\t\tdb := litestream.NewDB(\"/tmp/db\")\n\t\tif got, want := db.MetaPath(), `/tmp/.db-litestream`; got != want {\n\t\t\tt.Fatalf(\"MetaPath()=%v, want %v\", got, want)\n\t\t}\n\t})\n\tt.Run(\"Relative\", func(t *testing.T) {\n\t\tdb := litestream.NewDB(\"db\")\n\t\tif got, want := db.MetaPath(), `.db-litestream`; got != want {\n\t\t\tt.Fatalf(\"MetaPath()=%v, want %v\", got, want)\n\t\t}\n\t})\n}\n\nfunc TestDB_GenerationNamePath(t *testing.T) {\n\tdb := litestream.NewDB(\"/tmp/db\")\n\tif got, want := db.GenerationNamePath(), `/tmp/.db-litestream/generation`; got != want {\n\t\tt.Fatalf(\"GenerationNamePath()=%v, want %v\", got, want)\n\t}\n}\n\nfunc TestDB_GenerationPath(t *testing.T) {\n\tdb := litestream.NewDB(\"/tmp/db\")\n\tif got, want := db.GenerationPath(\"xxxx\"), `/tmp/.db-litestream/generations/xxxx`; got != want {\n\t\tt.Fatalf(\"GenerationPath()=%v, want %v\", got, want)\n\t}\n}\n\nfunc TestDB_ShadowWALDir(t *testing.T) {\n\tdb := litestream.NewDB(\"/tmp/db\")\n\tif got, want := db.ShadowWALDir(\"xxxx\"), `/tmp/.db-litestream/generations/xxxx/wal`; got != want {\n\t\tt.Fatalf(\"ShadowWALDir()=%v, want %v\", got, want)\n\t}\n}\n\nfunc TestDB_ShadowWALPath(t *testing.T) {\n\tdb := litestream.NewDB(\"/tmp/db\")\n\tif got, want := db.ShadowWALPath(\"xxxx\", 1000), `/tmp/.db-litestream/generations/xxxx/wal/000003e8.wal`; got != want {\n\t\tt.Fatalf(\"ShadowWALPath()=%v, want %v\", got, want)\n\t}\n}\n\n// Ensure we can check the last modified time of the real database and its WAL.\nfunc TestDB_UpdatedAt(t *testing.T) {\n\tt.Run(\"ErrNotExist\", func(t *testing.T) {\n\t\tdb := MustOpenDB(t)\n\t\tdefer MustCloseDB(t, db)\n\t\tif _, err := db.UpdatedAt(); !os.IsNotExist(err) {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n\n\tt.Run(\"DB\", func(t *testing.T) {\n\t\tdb, sqldb := MustOpenDBs(t)\n\t\tdefer MustCloseDBs(t, db, sqldb)\n\n\t\tif t0, err := db.UpdatedAt(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if time.Since(t0) > 10*time.Second {\n\t\t\tt.Fatalf(\"unexpected updated at time: %s\", t0)\n\t\t}\n\t})\n\n\tt.Run(\"WAL\", func(t *testing.T) {\n\t\tdb, sqldb := MustOpenDBs(t)\n\t\tdefer MustCloseDBs(t, db, sqldb)\n\n\t\tt0, err := db.UpdatedAt()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tsleepTime := 100 * time.Millisecond\n\t\tif os.Getenv(\"CI\") != \"\" {\n\t\t\tsleepTime = 1 * time.Second\n\t\t}\n\t\ttime.Sleep(sleepTime)\n\n\t\tif _, err := sqldb.Exec(`CREATE TABLE t (id INT);`); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif t1, err := db.UpdatedAt(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !t1.After(t0) {\n\t\t\tt.Fatalf(\"expected newer updated at time: %s > %s\", t1, t0)\n\t\t}\n\t})\n}\n\n// Ensure we can compute a checksum on the real database.\nfunc TestDB_CRC64(t *testing.T) {\n\tt.Run(\"ErrNotExist\", func(t *testing.T) {\n\t\tdb := MustOpenDB(t)\n\t\tdefer MustCloseDB(t, db)\n\t\tif _, _, err := db.CRC64(context.Background()); !os.IsNotExist(err) {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n\n\tt.Run(\"DB\", func(t *testing.T) {\n\t\tdb, sqldb := MustOpenDBs(t)\n\t\tdefer MustCloseDBs(t, db, sqldb)\n\n\t\tif err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tchksum0, _, err := db.CRC64(context.Background())\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Issue change that is applied to the WAL. Checksum should not change.\n\t\tif _, err := sqldb.Exec(`CREATE TABLE t (id INT);`); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if chksum1, _, err := db.CRC64(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if chksum0 == chksum1 {\n\t\t\tt.Fatal(\"expected different checksum event after WAL change\")\n\t\t}\n\n\t\t// Checkpoint change into database. Checksum should change.\n\t\tif err := db.Checkpoint(context.Background(), litestream.CheckpointModeTruncate); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif chksum2, _, err := db.CRC64(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if chksum0 == chksum2 {\n\t\t\tt.Fatal(\"expected different checksums after checkpoint\")\n\t\t}\n\t})\n}\n\n// Ensure we can sync the real WAL to the shadow WAL.\nfunc TestDB_Sync(t *testing.T) {\n\t// Ensure sync is skipped if no database exists.\n\tt.Run(\"NoDB\", func(t *testing.T) {\n\t\tdb := MustOpenDB(t)\n\t\tdefer MustCloseDB(t, db)\n\t\tif err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n\n\t// Ensure sync can successfully run on the initial sync.\n\tt.Run(\"Initial\", func(t *testing.T) {\n\t\tdb, sqldb := MustOpenDBs(t)\n\t\tdefer MustCloseDBs(t, db, sqldb)\n\n\t\tif err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Verify page size if now available.\n\t\tif db.PageSize() == 0 {\n\t\t\tt.Fatal(\"expected page size after initial sync\")\n\t\t}\n\n\t\t// Obtain real WAL size.\n\t\tfi, err := os.Stat(db.WALPath())\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Ensure position now available.\n\t\tif pos, err := db.Pos(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if pos.Generation == \"\" {\n\t\t\tt.Fatal(\"expected generation\")\n\t\t} else if got, want := pos.Index, 0; got != want {\n\t\t\tt.Fatalf(\"pos.Index=%v, want %v\", got, want)\n\t\t} else if got, want := pos.Offset, fi.Size(); got != want {\n\t\t\tt.Fatalf(\"pos.Offset=%v, want %v\", got, want)\n\t\t}\n\t})\n\n\t// Ensure DB can keep in sync across multiple Sync() invocations.\n\tt.Run(\"MultiSync\", func(t *testing.T) {\n\t\tdb, sqldb := MustOpenDBs(t)\n\t\tdefer MustCloseDBs(t, db, sqldb)\n\n\t\t// Execute a query to force a write to the WAL.\n\t\tif _, err := sqldb.Exec(`CREATE TABLE foo (bar TEXT);`); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Perform initial sync & grab initial position.\n\t\tif err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tpos0, err := db.Pos()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Insert into table.\n\t\tif _, err := sqldb.Exec(`INSERT INTO foo (bar) VALUES ('baz');`); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Sync to ensure position moves forward one page.\n\t\tif err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if pos1, err := db.Pos(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if pos0.Generation != pos1.Generation {\n\t\t\tt.Fatal(\"expected the same generation\")\n\t\t} else if got, want := pos1.Index, pos0.Index; got != want {\n\t\t\tt.Fatalf(\"Index=%v, want %v\", got, want)\n\t\t} else if got, want := pos1.Offset, pos0.Offset+4096+litestream.WALFrameHeaderSize; got != want {\n\t\t\tt.Fatalf(\"Offset=%v, want %v\", got, want)\n\t\t}\n\t})\n\n\t// Ensure a WAL file is created if one does not already exist.\n\tt.Run(\"NoWAL\", func(t *testing.T) {\n\t\tdb, sqldb := MustOpenDBs(t)\n\t\tdefer MustCloseDBs(t, db, sqldb)\n\n\t\t// Issue initial sync and truncate WAL.\n\t\tif err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Obtain initial position.\n\t\tpos0, err := db.Pos()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Checkpoint & fully close which should close WAL file.\n\t\tif err := db.Checkpoint(context.Background(), litestream.CheckpointModeTruncate); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := db.Close(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := sqldb.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Remove WAL file.\n\t\tif err := os.Remove(db.WALPath()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Reopen the managed database.\n\t\tdb = MustOpenDBAt(t, db.Path())\n\t\tdefer MustCloseDB(t, db)\n\n\t\t// Re-sync and ensure new generation has been created.\n\t\tif err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Obtain initial position.\n\t\tif pos1, err := db.Pos(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if pos0.Generation == pos1.Generation {\n\t\t\tt.Fatal(\"expected new generation after truncation\")\n\t\t}\n\t})\n\n\t// Ensure DB can start new generation if it detects it cannot verify last position.\n\tt.Run(\"OverwritePrevPosition\", func(t *testing.T) {\n\t\tdb, sqldb := MustOpenDBs(t)\n\t\tdefer MustCloseDBs(t, db, sqldb)\n\n\t\t// Execute a query to force a write to the WAL.\n\t\tif _, err := sqldb.Exec(`CREATE TABLE foo (bar TEXT);`); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Issue initial sync and truncate WAL.\n\t\tif err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Obtain initial position.\n\t\tpos0, err := db.Pos()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Fully close which should close WAL file.\n\t\tif err := db.Close(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := sqldb.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Verify WAL does not exist.\n\t\tif _, err := os.Stat(db.WALPath()); !os.IsNotExist(err) {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Insert into table multiple times to move past old offset\n\t\tsqldb = MustOpenSQLDB(t, db.Path())\n\t\tdefer MustCloseSQLDB(t, sqldb)\n\t\tfor i := 0; i < 100; i++ {\n\t\t\tif _, err := sqldb.Exec(`INSERT INTO foo (bar) VALUES ('baz');`); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\n\t\t// Reopen the managed database.\n\t\tdb = MustOpenDBAt(t, db.Path())\n\t\tdefer MustCloseDB(t, db)\n\n\t\t// Re-sync and ensure new generation has been created.\n\t\tif err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Obtain initial position.\n\t\tif pos1, err := db.Pos(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if pos0.Generation == pos1.Generation {\n\t\t\tt.Fatal(\"expected new generation after truncation\")\n\t\t}\n\t})\n\n\t// Ensure DB can handle a mismatched header-only and start new generation.\n\tt.Run(\"WALHeaderMismatch\", func(t *testing.T) {\n\t\tdb, sqldb := MustOpenDBs(t)\n\t\tdefer MustCloseDBs(t, db, sqldb)\n\n\t\t// Execute a query to force a write to the WAL and then sync.\n\t\tif _, err := sqldb.Exec(`CREATE TABLE foo (bar TEXT);`); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Grab initial position & close.\n\t\tpos0, err := db.Pos()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := db.Close(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Read existing file, update header checksum, and write back only header\n\t\t// to simulate a header with a mismatched checksum.\n\t\tshadowWALPath := db.ShadowWALPath(pos0.Generation, pos0.Index)\n\t\tif buf, err := os.ReadFile(shadowWALPath); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := os.WriteFile(shadowWALPath, append(buf[:litestream.WALHeaderSize-8], 0, 0, 0, 0, 0, 0, 0, 0), 0600); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Reopen managed database & ensure sync will still work.\n\t\tdb = MustOpenDBAt(t, db.Path())\n\t\tdefer MustCloseDB(t, db)\n\t\tif err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Verify a new generation was started.\n\t\tif pos1, err := db.Pos(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if pos0.Generation == pos1.Generation {\n\t\t\tt.Fatal(\"expected new generation\")\n\t\t}\n\t})\n\n\t// Ensure DB can handle partial shadow WAL header write.\n\tt.Run(\"PartialShadowWALHeader\", func(t *testing.T) {\n\t\tdb, sqldb := MustOpenDBs(t)\n\t\tdefer MustCloseDBs(t, db, sqldb)\n\n\t\t// Execute a query to force a write to the WAL and then sync.\n\t\tif _, err := sqldb.Exec(`CREATE TABLE foo (bar TEXT);`); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tpos0, err := db.Pos()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Close & truncate shadow WAL to simulate a partial header write.\n\t\tif err := db.Close(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := os.Truncate(db.ShadowWALPath(pos0.Generation, pos0.Index), litestream.WALHeaderSize-1); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Reopen managed database & ensure sync will still work.\n\t\tdb = MustOpenDBAt(t, db.Path())\n\t\tdefer MustCloseDB(t, db)\n\t\tif err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Verify a new generation was started.\n\t\tif pos1, err := db.Pos(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if pos0.Generation == pos1.Generation {\n\t\t\tt.Fatal(\"expected new generation\")\n\t\t}\n\t})\n\n\t// Ensure DB can handle partial shadow WAL writes.\n\tt.Run(\"PartialShadowWALFrame\", func(t *testing.T) {\n\t\tdb, sqldb := MustOpenDBs(t)\n\t\tdefer MustCloseDBs(t, db, sqldb)\n\n\t\t// Execute a query to force a write to the WAL and then sync.\n\t\tif _, err := sqldb.Exec(`CREATE TABLE foo (bar TEXT);`); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tpos0, err := db.Pos()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Obtain current shadow WAL size.\n\t\tfi, err := os.Stat(db.ShadowWALPath(pos0.Generation, pos0.Index))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Close & truncate shadow WAL to simulate a partial frame write.\n\t\tif err := db.Close(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := os.Truncate(db.ShadowWALPath(pos0.Generation, pos0.Index), fi.Size()-1); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Reopen managed database & ensure sync will still work.\n\t\tdb = MustOpenDBAt(t, db.Path())\n\t\tdefer MustCloseDB(t, db)\n\t\tif err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Verify same generation is kept.\n\t\tif pos1, err := db.Pos(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := pos1, pos0; got != want {\n\t\t\tt.Fatalf(\"Pos()=%s want %s\", got, want)\n\t\t}\n\n\t\t// Ensure shadow WAL has recovered.\n\t\tif fi0, err := os.Stat(db.ShadowWALPath(pos0.Generation, pos0.Index)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := fi0.Size(), fi.Size(); got != want {\n\t\t\tt.Fatalf(\"Size()=%v, want %v\", got, want)\n\t\t}\n\t})\n\n\t// Ensure DB can handle a generation directory with a missing shadow WAL.\n\tt.Run(\"NoShadowWAL\", func(t *testing.T) {\n\t\tdb, sqldb := MustOpenDBs(t)\n\t\tdefer MustCloseDBs(t, db, sqldb)\n\n\t\t// Execute a query to force a write to the WAL and then sync.\n\t\tif _, err := sqldb.Exec(`CREATE TABLE foo (bar TEXT);`); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tpos0, err := db.Pos()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Close & delete shadow WAL to simulate dir created but not WAL.\n\t\tif err := db.Close(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := os.Remove(db.ShadowWALPath(pos0.Generation, pos0.Index)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Reopen managed database & ensure sync will still work.\n\t\tdb = MustOpenDBAt(t, db.Path())\n\t\tdefer MustCloseDB(t, db)\n\t\tif err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Verify new generation created but index/offset the same.\n\t\tif pos1, err := db.Pos(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if pos0.Generation == pos1.Generation {\n\t\t\tt.Fatal(\"expected new generation\")\n\t\t} else if got, want := pos1.Index, pos0.Index; got != want {\n\t\t\tt.Fatalf(\"Index=%v want %v\", got, want)\n\t\t} else if got, want := pos1.Offset, pos0.Offset; got != want {\n\t\t\tt.Fatalf(\"Offset=%v want %v\", got, want)\n\t\t}\n\t})\n\n\t// Ensure DB checkpoints after minimum number of pages.\n\tt.Run(\"MinCheckpointPageN\", func(t *testing.T) {\n\t\tdb, sqldb := MustOpenDBs(t)\n\t\tdefer MustCloseDBs(t, db, sqldb)\n\n\t\t// Execute a query to force a write to the WAL and then sync.\n\t\tif _, err := sqldb.Exec(`CREATE TABLE foo (bar TEXT);`); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Write at least minimum number of pages to trigger rollover.\n\t\tfor i := 0; i < db.MinCheckpointPageN; i++ {\n\t\t\tif _, err := sqldb.Exec(`INSERT INTO foo (bar) VALUES ('baz');`); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\n\t\t// Sync to shadow WAL.\n\t\tif err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Ensure position is now on the second index.\n\t\tif pos, err := db.Pos(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := pos.Index, 1; got != want {\n\t\t\tt.Fatalf(\"Index=%v, want %v\", got, want)\n\t\t}\n\t})\n\n\t// Ensure DB checkpoints after interval.\n\tt.Run(\"CheckpointInterval\", func(t *testing.T) {\n\t\tdb, sqldb := MustOpenDBs(t)\n\t\tdefer MustCloseDBs(t, db, sqldb)\n\n\t\t// Execute a query to force a write to the WAL and then sync.\n\t\tif _, err := sqldb.Exec(`CREATE TABLE foo (bar TEXT);`); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Reduce checkpoint interval to ensure a rollover is triggered.\n\t\tdb.CheckpointInterval = 1 * time.Nanosecond\n\n\t\t// Write to WAL & sync.\n\t\tif _, err := sqldb.Exec(`INSERT INTO foo (bar) VALUES ('baz');`); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := db.Sync(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Ensure position is now on the second index.\n\t\tif pos, err := db.Pos(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := pos.Index, 1; got != want {\n\t\t\tt.Fatalf(\"Index=%v, want %v\", got, want)\n\t\t}\n\t})\n}\n\n// MustOpenDBs returns a new instance of a DB & associated SQL DB.\nfunc MustOpenDBs(tb testing.TB) (*litestream.DB, *sql.DB) {\n\ttb.Helper()\n\tdb := MustOpenDB(tb)\n\treturn db, MustOpenSQLDB(tb, db.Path())\n}\n\n// MustCloseDBs closes db & sqldb and removes the parent directory.\nfunc MustCloseDBs(tb testing.TB, db *litestream.DB, sqldb *sql.DB) {\n\ttb.Helper()\n\tMustCloseDB(tb, db)\n\tMustCloseSQLDB(tb, sqldb)\n}\n\n// MustOpenDB returns a new instance of a DB.\nfunc MustOpenDB(tb testing.TB) *litestream.DB {\n\tdir := tb.TempDir()\n\treturn MustOpenDBAt(tb, filepath.Join(dir, \"db\"))\n}\n\n// MustOpenDBAt returns a new instance of a DB for a given path.\nfunc MustOpenDBAt(tb testing.TB, path string) *litestream.DB {\n\ttb.Helper()\n\tdb := litestream.NewDB(path)\n\tdb.MonitorInterval = 0 // disable background goroutine\n\tif err := db.Open(); err != nil {\n\t\ttb.Fatal(err)\n\t}\n\treturn db\n}\n\n// MustCloseDB closes db and removes its parent directory.\nfunc MustCloseDB(tb testing.TB, db *litestream.DB) {\n\ttb.Helper()\n\tif err := db.Close(context.Background()); err != nil && !strings.Contains(err.Error(), `database is closed`) {\n\t\ttb.Fatal(err)\n\t} else if err := os.RemoveAll(filepath.Dir(db.Path())); err != nil {\n\t\ttb.Fatal(err)\n\t}\n}\n\n// MustOpenSQLDB returns a database/sql DB.\nfunc MustOpenSQLDB(tb testing.TB, path string) *sql.DB {\n\ttb.Helper()\n\td, err := sql.Open(\"sqlite3\", path)\n\tif err != nil {\n\t\ttb.Fatal(err)\n\t} else if _, err := d.Exec(`PRAGMA journal_mode = wal;`); err != nil {\n\t\ttb.Fatal(err)\n\t}\n\treturn d\n}\n\n// MustCloseSQLDB closes a database/sql DB.\nfunc MustCloseSQLDB(tb testing.TB, d *sql.DB) {\n\ttb.Helper()\n\tif err := d.Close(); err != nil {\n\t\ttb.Fatal(err)\n\t}\n}\n"
        },
        {
          "name": "etc",
          "type": "tree",
          "content": null
        },
        {
          "name": "file",
          "type": "tree",
          "content": null
        },
        {
          "name": "gcs",
          "type": "tree",
          "content": null
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 2.7587890625,
          "content": "module github.com/benbjohnson/litestream\n\ngo 1.21\n\nrequire (\n\tcloud.google.com/go/storage v1.36.0\n\tfilippo.io/age v1.1.1\n\tgithub.com/Azure/azure-storage-blob-go v0.15.0\n\tgithub.com/aws/aws-sdk-go v1.49.5\n\tgithub.com/mattn/go-shellwords v1.0.12\n\tgithub.com/mattn/go-sqlite3 v1.14.19\n\tgithub.com/pierrec/lz4/v4 v4.1.19\n\tgithub.com/pkg/sftp v1.13.6\n\tgithub.com/prometheus/client_golang v1.17.0\n\tgolang.org/x/crypto v0.17.0\n\tgolang.org/x/sync v0.5.0\n\tgolang.org/x/sys v0.15.0\n\tgoogle.golang.org/api v0.154.0\n\tgopkg.in/yaml.v2 v2.4.0\n)\n\nrequire (\n\tcloud.google.com/go v0.111.0 // indirect\n\tcloud.google.com/go/compute v1.23.3 // indirect\n\tcloud.google.com/go/compute/metadata v0.2.3 // indirect\n\tcloud.google.com/go/iam v1.1.5 // indirect\n\tgithub.com/Azure/azure-pipeline-go v0.2.3 // indirect\n\tgithub.com/beorn7/perks v1.0.1 // indirect\n\tgithub.com/cespare/xxhash/v2 v2.2.0 // indirect\n\tgithub.com/felixge/httpsnoop v1.0.4 // indirect\n\tgithub.com/go-logr/logr v1.3.0 // indirect\n\tgithub.com/go-logr/stdr v1.2.2 // indirect\n\tgithub.com/golang/groupcache v0.0.0-20210331224755-41bb18bfe9da // indirect\n\tgithub.com/golang/protobuf v1.5.3 // indirect\n\tgithub.com/google/go-cmp v0.6.0 // indirect\n\tgithub.com/google/s2a-go v0.1.7 // indirect\n\tgithub.com/google/uuid v1.5.0 // indirect\n\tgithub.com/googleapis/enterprise-certificate-proxy v0.3.2 // indirect\n\tgithub.com/googleapis/gax-go/v2 v2.12.0 // indirect\n\tgithub.com/jmespath/go-jmespath v0.4.0 // indirect\n\tgithub.com/kr/fs v0.1.0 // indirect\n\tgithub.com/mattn/go-ieproxy v0.0.11 // indirect\n\tgithub.com/matttproud/golang_protobuf_extensions v1.0.4 // indirect\n\tgithub.com/matttproud/golang_protobuf_extensions/v2 v2.0.0 // indirect\n\tgithub.com/prometheus/client_model v0.5.0 // indirect\n\tgithub.com/prometheus/common v0.45.0 // indirect\n\tgithub.com/prometheus/procfs v0.12.0 // indirect\n\tgo.opencensus.io v0.24.0 // indirect\n\tgo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc v0.46.1 // indirect\n\tgo.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.46.1 // indirect\n\tgo.opentelemetry.io/otel v1.21.0 // indirect\n\tgo.opentelemetry.io/otel/metric v1.21.0 // indirect\n\tgo.opentelemetry.io/otel/trace v1.21.0 // indirect\n\tgolang.org/x/net v0.19.0 // indirect\n\tgolang.org/x/oauth2 v0.15.0 // indirect\n\tgolang.org/x/text v0.14.0 // indirect\n\tgolang.org/x/time v0.5.0 // indirect\n\tgolang.org/x/xerrors v0.0.0-20231012003039-104605ab7028 // indirect\n\tgoogle.golang.org/appengine v1.6.8 // indirect\n\tgoogle.golang.org/genproto v0.0.0-20231212172506-995d672761c0 // indirect\n\tgoogle.golang.org/genproto/googleapis/api v0.0.0-20231212172506-995d672761c0 // indirect\n\tgoogle.golang.org/genproto/googleapis/rpc v0.0.0-20231212172506-995d672761c0 // indirect\n\tgoogle.golang.org/grpc v1.60.1 // indirect\n\tgoogle.golang.org/protobuf v1.31.0 // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 34.326171875,
          "content": "cloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ncloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ncloud.google.com/go v0.110.7 h1:rJyC7nWRg2jWGZ4wSJ5nY65GTdYJkg0cd/uXb+ACI6o=\ncloud.google.com/go v0.110.7/go.mod h1:+EYjdK8e5RME/VY/qLCAtuyALQ9q67dvuum8i+H5xsI=\ncloud.google.com/go v0.111.0 h1:YHLKNupSD1KqjDbQ3+LVdQ81h/UJbJyZG203cEfnQgM=\ncloud.google.com/go v0.111.0/go.mod h1:0mibmpKP1TyOOFYQY5izo0LnT+ecvOQ0Sg3OdmMiNRU=\ncloud.google.com/go/compute v1.23.0 h1:tP41Zoavr8ptEqaW6j+LQOnyBBhO7OkOMAGrgLopTwY=\ncloud.google.com/go/compute v1.23.0/go.mod h1:4tCnrn48xsqlwSAiLf1HXMQk8CONslYbdiEZc9FEIbM=\ncloud.google.com/go/compute v1.23.3 h1:6sVlXXBmbd7jNX0Ipq0trII3e4n1/MsADLK6a+aiVlk=\ncloud.google.com/go/compute v1.23.3/go.mod h1:VCgBUoMnIVIR0CscqQiPJLAG25E3ZRZMzcFZeQ+h8CI=\ncloud.google.com/go/compute/metadata v0.2.3 h1:mg4jlk7mCAj6xXp9UJ4fjI9VUI5rubuGBW5aJ7UnBMY=\ncloud.google.com/go/compute/metadata v0.2.3/go.mod h1:VAV5nSsACxMJvgaAuX6Pk2AawlZn8kiOGuCv6gTkwuA=\ncloud.google.com/go/iam v1.1.1 h1:lW7fzj15aVIXYHREOqjRBV9PsH0Z6u8Y46a1YGvQP4Y=\ncloud.google.com/go/iam v1.1.1/go.mod h1:A5avdyVL2tCppe4unb0951eI9jreack+RJ0/d+KUZOU=\ncloud.google.com/go/iam v1.1.5 h1:1jTsCu4bcsNsE4iiqNT5SHwrDRCfRmIaaaVFhRveTJI=\ncloud.google.com/go/iam v1.1.5/go.mod h1:rB6P/Ic3mykPbFio+vo7403drjlgvoWfYpJhMXEbzv8=\ncloud.google.com/go/storage v1.31.0 h1:+S3LjjEN2zZ+L5hOwj4+1OkGCsLVe0NzpXKQ1pSdTCI=\ncloud.google.com/go/storage v1.31.0/go.mod h1:81ams1PrhW16L4kF7qg+4mTq7SRs5HsbDTM0bWvrwJ0=\ncloud.google.com/go/storage v1.36.0 h1:P0mOkAcaJxhCTvAkMhxMfrTKiNcub4YmmPBtlhAyTr8=\ncloud.google.com/go/storage v1.36.0/go.mod h1:M6M/3V/D3KpzMTJyPOR/HU6n2Si5QdaXYEsng2xgOs8=\nfilippo.io/age v1.1.1 h1:pIpO7l151hCnQ4BdyBujnGP2YlUo0uj6sAVNHGBvXHg=\nfilippo.io/age v1.1.1/go.mod h1:l03SrzDUrBkdBx8+IILdnn2KZysqQdbEBUQ4p3sqEQE=\ngithub.com/Azure/azure-pipeline-go v0.2.3 h1:7U9HBg1JFK3jHl5qmo4CTZKFTVgMwdFHMVtCdfBE21U=\ngithub.com/Azure/azure-pipeline-go v0.2.3/go.mod h1:x841ezTBIMG6O3lAcl8ATHnsOPVl2bqk7S3ta6S6u4k=\ngithub.com/Azure/azure-storage-blob-go v0.15.0 h1:rXtgp8tN1p29GvpGgfJetavIG0V7OgcSXPpwp3tx6qk=\ngithub.com/Azure/azure-storage-blob-go v0.15.0/go.mod h1:vbjsVbX0dlxnRc4FFMPsS9BsJWPcne7GB7onqlPvz58=\ngithub.com/Azure/go-autorest v14.2.0+incompatible h1:V5VMDjClD3GiElqLWO7mz2MxNAK/vTfRHdAubSIPRgs=\ngithub.com/Azure/go-autorest v14.2.0+incompatible/go.mod h1:r+4oMnoxhatjLLJ6zxSWATqVooLgysK6ZNox3g/xq24=\ngithub.com/Azure/go-autorest/autorest/adal v0.9.13 h1:Mp5hbtOePIzM8pJVRa3YLrWWmZtoxRXqUEzCfJt3+/Q=\ngithub.com/Azure/go-autorest/autorest/adal v0.9.13/go.mod h1:W/MM4U6nLxnIskrw4UwWzlHfGjwUS50aOsc/I3yuU8M=\ngithub.com/Azure/go-autorest/autorest/date v0.3.0 h1:7gUk1U5M/CQbp9WoqinNzJar+8KY+LPI6wiWrP/myHw=\ngithub.com/Azure/go-autorest/autorest/date v0.3.0/go.mod h1:BI0uouVdmngYNUzGWeSYnokU+TrmwEsOqdt8Y6sso74=\ngithub.com/Azure/go-autorest/autorest/mocks v0.4.1/go.mod h1:LTp+uSrOhSkaKrUy935gNZuuIPPVsHlr9DSOxSayd+k=\ngithub.com/Azure/go-autorest/logger v0.2.1 h1:IG7i4p/mDa2Ce4TRyAO8IHnVhAVF3RFU+ZtXWSmf4Tg=\ngithub.com/Azure/go-autorest/logger v0.2.1/go.mod h1:T9E3cAhj2VqvPOtCYAvby9aBXkZmbF5NWuPV8+WeEW8=\ngithub.com/Azure/go-autorest/tracing v0.6.0 h1:TYi4+3m5t6K48TGI9AUdb+IzbnSxvnvUMfuitfgcfuo=\ngithub.com/Azure/go-autorest/tracing v0.6.0/go.mod h1:+vhtPC754Xsa23ID7GlGsrdKBpUA79WCAKPPZVC2DeU=\ngithub.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=\ngithub.com/antihax/optional v1.0.0/go.mod h1:uupD/76wgC+ih3iEmQUL+0Ugr19nfwCT1kdvxnR2qWY=\ngithub.com/aws/aws-sdk-go v1.44.318 h1:Yl66rpbQHFUbxe9JBKLcvOvRivhVgP6+zH0b9KzARX8=\ngithub.com/aws/aws-sdk-go v1.44.318/go.mod h1:aVsgQcEevwlmQ7qHE9I3h+dtQgpqhFB+i8Phjh7fkwI=\ngithub.com/aws/aws-sdk-go v1.49.5 h1:y2yfBlwjPDi3/sBVKeznYEdDy6wIhjA2L5NCBMLUIYA=\ngithub.com/aws/aws-sdk-go v1.49.5/go.mod h1:LF8svs817+Nz+DmiMQKTO3ubZ/6IaTpq3TjupRn3Eqk=\ngithub.com/beorn7/perks v1.0.1 h1:VlbKKnNfV8bJzeqoa4cOKqO6bYr3WgKZxO8Z16+hsOM=\ngithub.com/beorn7/perks v1.0.1/go.mod h1:G2ZrVWU2WbWT9wwq4/hrbKbnv/1ERSJQ0ibhJ6rlkpw=\ngithub.com/census-instrumentation/opencensus-proto v0.2.1/go.mod h1:f6KPmirojxKA12rnyqOA5BBL4O983OfeGPqjHWSTneU=\ngithub.com/cespare/xxhash/v2 v2.1.1/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/cespare/xxhash/v2 v2.2.0 h1:DC2CZ1Ep5Y4k3ZQ899DldepgrayRUGE6BBZ/cd9Cj44=\ngithub.com/cespare/xxhash/v2 v2.2.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/client9/misspell v0.3.4/go.mod h1:qj6jICC3Q7zFZvVWo7KLAzC3yx5G7kyvSDkc90ppPyw=\ngithub.com/cncf/udpa/go v0.0.0-20191209042840-269d4d468f6f/go.mod h1:M8M6+tZqaGXZJjfX53e64911xZQV5JYwmTeXPW+k8Sc=\ngithub.com/cncf/udpa/go v0.0.0-20201120205902-5459f2c99403/go.mod h1:WmhPx2Nbnhtbo57+VJT5O0JRkEi1Wbu0z5j0R8u5Hbk=\ngithub.com/cncf/udpa/go v0.0.0-20210930031921-04548b0d99d4/go.mod h1:6pvJx4me5XPnfI9Z40ddWsdw2W/uZgQLFXToKeRcDiI=\ngithub.com/cncf/xds/go v0.0.0-20210805033703-aa0b78936158/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\ngithub.com/cncf/xds/go v0.0.0-20210922020428-25de7278fc84/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\ngithub.com/cncf/xds/go v0.0.0-20211011173535-cb28da3451f1/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/envoyproxy/go-control-plane v0.9.0/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\ngithub.com/envoyproxy/go-control-plane v0.9.1-0.20191026205805-5f8ba28d4473/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\ngithub.com/envoyproxy/go-control-plane v0.9.4/go.mod h1:6rpuAdCZL397s3pYoYcLgu1mIlRU8Am5FuJP05cCM98=\ngithub.com/envoyproxy/go-control-plane v0.9.9-0.20201210154907-fd9021fe5dad/go.mod h1:cXg6YxExXjJnVBQHBLXeUAgxn2UodCpnH306RInaBQk=\ngithub.com/envoyproxy/go-control-plane v0.9.10-0.20210907150352-cf90f659a021/go.mod h1:AFq3mo9L8Lqqiid3OhADV3RfLJnjiw63cSpi+fDTRC0=\ngithub.com/envoyproxy/protoc-gen-validate v0.1.0/go.mod h1:iSmxcyjqTsJpI2R4NaDN7+kN2VEUnK/pcBlmesArF7c=\ngithub.com/felixge/httpsnoop v1.0.4 h1:NFTV2Zj1bL4mc9sqWACXbQFVBBg2W3GPvqp8/ESS2Wg=\ngithub.com/felixge/httpsnoop v1.0.4/go.mod h1:m8KPJKqk1gH5J9DgRY2ASl2lWCfGKXixSwevea8zH2U=\ngithub.com/form3tech-oss/jwt-go v3.2.2+incompatible h1:TcekIExNqud5crz4xD2pavyTgWiPvpYe4Xau31I0PRk=\ngithub.com/form3tech-oss/jwt-go v3.2.2+incompatible/go.mod h1:pbq4aXjuKjdthFRnoDwaVPLA+WlJuPGy+QneDUgJi2k=\ngithub.com/ghodss/yaml v1.0.0/go.mod h1:4dBDuWmgqj2HViK6kFavaiC9ZROes6MMH2rRYeMEF04=\ngithub.com/go-logr/logr v1.2.2/go.mod h1:jdQByPbusPIv2/zmleS9BjJVeZ6kBagPoEUsqbVz/1A=\ngithub.com/go-logr/logr v1.3.0 h1:2y3SDp0ZXuc6/cjLSZ+Q3ir+QB9T/iG5yYRXqsagWSY=\ngithub.com/go-logr/logr v1.3.0/go.mod h1:9T104GzyrTigFIr8wt5mBrctHMim0Nb2HLGrmQ40KvY=\ngithub.com/go-logr/stdr v1.2.2 h1:hSWxHoqTgW2S2qGc0LTAI563KZ5YKYRhT3MFKZMbjag=\ngithub.com/go-logr/stdr v1.2.2/go.mod h1:mMo/vtBO5dYbehREoey6XUKy/eSumjCCveDpRre4VKE=\ngithub.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=\ngithub.com/golang/groupcache v0.0.0-20200121045136-8c9f03a8e57e/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/groupcache v0.0.0-20210331224755-41bb18bfe9da h1:oI5xCqsCo564l8iNU+DwB5epxmsaqB+rhGL0m5jtYqE=\ngithub.com/golang/groupcache v0.0.0-20210331224755-41bb18bfe9da/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\ngithub.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.3/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=\ngithub.com/golang/protobuf v1.4.0-rc.1/go.mod h1:ceaxUfeHdC40wWswd/P6IGgMaK3YpKi5j83Wpe3EHw8=\ngithub.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208/go.mod h1:xKAWHe0F5eneWXFV3EuXVDTCmh+JuBKY0li0aMyXATA=\ngithub.com/golang/protobuf v1.4.0-rc.2/go.mod h1:LlEzMj4AhA7rCAGe4KMBDvJI+AwstrUpVNzEA03Pprs=\ngithub.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0/go.mod h1:WU3c8KckQ9AFe+yFwt9sWVRKCVIyN9cPHBJSNnbL67w=\ngithub.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=\ngithub.com/golang/protobuf v1.4.1/go.mod h1:U8fpvMrcmy5pZrNK1lt4xCsGvpyWQ/VVv6QDs8UjoX8=\ngithub.com/golang/protobuf v1.4.2/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/golang/protobuf v1.4.3/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=\ngithub.com/golang/protobuf v1.5.2/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=\ngithub.com/golang/protobuf v1.5.3 h1:KhyjKVUg7Usr/dYsdSqoFveMYd5ko72D+zANwlG1mmg=\ngithub.com/golang/protobuf v1.5.3/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=\ngithub.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=\ngithub.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.3/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.9 h1:O2Tfq5qg4qc4AmwVlvv0oLiVAGB7enBSJ2x2DqQFi38=\ngithub.com/google/go-cmp v0.5.9/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\ngithub.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=\ngithub.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\ngithub.com/google/martian/v3 v3.3.2 h1:IqNFLAmvJOgVlpdEBiQbDc2EwKW77amAycfTuWKdfvw=\ngithub.com/google/martian/v3 v3.3.2/go.mod h1:oBOf6HBosgwRXnUGWUB05QECsc6uvmMiJ3+6W4l/CUk=\ngithub.com/google/s2a-go v0.1.4 h1:1kZ/sQM3srePvKs3tXAvQzo66XfcReoqFpIpIccE7Oc=\ngithub.com/google/s2a-go v0.1.4/go.mod h1:Ej+mSEMGRnqRzjc7VtF+jdBwYG5fuJfiZ8ELkjEwM0A=\ngithub.com/google/s2a-go v0.1.7 h1:60BLSyTrOV4/haCDW4zb1guZItoSq8foHCXrAnjBo/o=\ngithub.com/google/s2a-go v0.1.7/go.mod h1:50CgR4k1jNlWBu4UfS4AcfhVe1r6pdZPygJ3R8F0Qdw=\ngithub.com/google/uuid v1.1.2/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/google/uuid v1.2.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/google/uuid v1.3.0 h1:t6JiXgmwXMjEs8VusXIJk2BXHsn+wx8BZdTaoZ5fu7I=\ngithub.com/google/uuid v1.3.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/google/uuid v1.5.0 h1:1p67kYwdtXjb0gL0BPiP1Av9wiZPo5A8z2cWkTZ+eyU=\ngithub.com/google/uuid v1.5.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/googleapis/enterprise-certificate-proxy v0.2.5 h1:UR4rDjcgpgEnqpIEvkiqTYKBCKLNmlge2eVjoZfySzM=\ngithub.com/googleapis/enterprise-certificate-proxy v0.2.5/go.mod h1:RxW0N9901Cko1VOCW3SXCpWP+mlIEkk2tP7jnHy9a3w=\ngithub.com/googleapis/enterprise-certificate-proxy v0.3.2 h1:Vie5ybvEvT75RniqhfFxPRy3Bf7vr3h0cechB90XaQs=\ngithub.com/googleapis/enterprise-certificate-proxy v0.3.2/go.mod h1:VLSiSSBs/ksPL8kq3OBOQ6WRI2QnaFynd1DCjZ62+V0=\ngithub.com/googleapis/gax-go/v2 v2.12.0 h1:A+gCJKdRfqXkr+BIRGtZLibNXf0m1f9E4HG56etFpas=\ngithub.com/googleapis/gax-go/v2 v2.12.0/go.mod h1:y+aIqrI5eb1YGMVJfuV3185Ts/D7qKpsEkdD5+I6QGU=\ngithub.com/grpc-ecosystem/grpc-gateway v1.16.0/go.mod h1:BDjrQk3hbvj6Nolgz8mAMFbcEtjT1g+wF4CSlocrBnw=\ngithub.com/jmespath/go-jmespath v0.4.0 h1:BEgLn5cpjn8UN1mAw4NjwDrS35OdebyEtFe+9YPoQUg=\ngithub.com/jmespath/go-jmespath v0.4.0/go.mod h1:T8mJZnbsbmF+m6zOOFylbeCJqk5+pHWvzYPziyZiYoo=\ngithub.com/jmespath/go-jmespath/internal/testify v1.5.1 h1:shLQSRRSCCPj3f2gpwzGwWFoC7ycTf1rcQZHOlsJ6N8=\ngithub.com/jmespath/go-jmespath/internal/testify v1.5.1/go.mod h1:L3OGu8Wl2/fWfCI6z80xFu9LTZmf1ZRjMHUOPmWr69U=\ngithub.com/kr/fs v0.1.0 h1:Jskdu9ieNAYnjxsi0LbQp1ulIKZV1LAFgK1tWhpZgl8=\ngithub.com/kr/fs v0.1.0/go.mod h1:FFnZGqtBN9Gxj7eW1uZ42v5BccTP0vu6NEaFoC2HwRg=\ngithub.com/kr/pretty v0.2.1/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=\ngithub.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=\ngithub.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=\ngithub.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\ngithub.com/kr/text v0.1.0 h1:45sCR5RtlFHMR4UwH9sdQ5TC8v0qDQCHnXt+kaKSTVE=\ngithub.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\ngithub.com/mattn/go-ieproxy v0.0.1/go.mod h1:pYabZ6IHcRpFh7vIaLfK7rdcWgFEb3SFJ6/gNWuh88E=\ngithub.com/mattn/go-ieproxy v0.0.11 h1:MQ/5BuGSgDAHZOJe6YY80IF2UVCfGkwfo6AeD7HtHYo=\ngithub.com/mattn/go-ieproxy v0.0.11/go.mod h1:/NsJd+kxZBmjMc5hrJCKMbP57B84rvq9BiDRbtO9AS0=\ngithub.com/mattn/go-shellwords v1.0.12 h1:M2zGm7EW6UQJvDeQxo4T51eKPurbeFbe8WtebGE2xrk=\ngithub.com/mattn/go-shellwords v1.0.12/go.mod h1:EZzvwXDESEeg03EKmM+RmDnNOPKG4lLtQsUlTZDWQ8Y=\ngithub.com/mattn/go-sqlite3 v1.14.17 h1:mCRHCLDUBXgpKAqIKsaAaAsrAlbkeomtRFKXh2L6YIM=\ngithub.com/mattn/go-sqlite3 v1.14.17/go.mod h1:2eHXhiwb8IkHr+BDWZGa96P6+rkvnG63S2DGjv9HUNg=\ngithub.com/mattn/go-sqlite3 v1.14.19 h1:fhGleo2h1p8tVChob4I9HpmVFIAkKGpiukdrgQbWfGI=\ngithub.com/mattn/go-sqlite3 v1.14.19/go.mod h1:2eHXhiwb8IkHr+BDWZGa96P6+rkvnG63S2DGjv9HUNg=\ngithub.com/matttproud/golang_protobuf_extensions v1.0.4 h1:mmDVorXM7PCGKw94cs5zkfA9PSy5pEvNWRP0ET0TIVo=\ngithub.com/matttproud/golang_protobuf_extensions v1.0.4/go.mod h1:BSXmuO+STAnVfrANrmjBb36TMTDstsz7MSK+HVaYKv4=\ngithub.com/matttproud/golang_protobuf_extensions/v2 v2.0.0 h1:jWpvCLoY8Z/e3VKvlsiIGKtc+UG6U5vzxaoagmhXfyg=\ngithub.com/matttproud/golang_protobuf_extensions/v2 v2.0.0/go.mod h1:QUyp042oQthUoa9bqDv0ER0wrtXnBruoNd7aNjkbP+k=\ngithub.com/pierrec/lz4/v4 v4.1.18 h1:xaKrnTkyoqfh1YItXl56+6KJNVYWlEEPuAQW9xsplYQ=\ngithub.com/pierrec/lz4/v4 v4.1.18/go.mod h1:gZWDp/Ze/IJXGXf23ltt2EXimqmTUXEy0GFuRQyBid4=\ngithub.com/pierrec/lz4/v4 v4.1.19 h1:tYLzDnjDXh9qIxSTKHwXwOYmm9d887Y7Y1ZkyXYHAN4=\ngithub.com/pierrec/lz4/v4 v4.1.19/go.mod h1:gZWDp/Ze/IJXGXf23ltt2EXimqmTUXEy0GFuRQyBid4=\ngithub.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\ngithub.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/sftp v1.13.5 h1:a3RLUqkyjYRtBTZJZ1VRrKbN3zhuPLlUc3sphVz81go=\ngithub.com/pkg/sftp v1.13.5/go.mod h1:wHDZ0IZX6JcBYRK1TH9bcVq8G7TLpVHYIGJRFnmPfxg=\ngithub.com/pkg/sftp v1.13.6 h1:JFZT4XbOU7l77xGSpOdW+pwIMqP044IyjXX6FGyEKFo=\ngithub.com/pkg/sftp v1.13.6/go.mod h1:tz1ryNURKu77RL+GuCzmoJYxQczL3wLNNpPWagdg4Qk=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/prometheus/client_golang v1.16.0 h1:yk/hx9hDbrGHovbci4BY+pRMfSuuat626eFsHb7tmT8=\ngithub.com/prometheus/client_golang v1.16.0/go.mod h1:Zsulrv/L9oM40tJ7T815tM89lFEugiJ9HzIqaAx4LKc=\ngithub.com/prometheus/client_golang v1.17.0 h1:rl2sfwZMtSthVU752MqfjQozy7blglC+1SOtjMAMh+Q=\ngithub.com/prometheus/client_golang v1.17.0/go.mod h1:VeL+gMmOAxkS2IqfCq0ZmHSL+LjWfWDUmp1mBz9JgUY=\ngithub.com/prometheus/client_model v0.0.0-20190812154241-14fe0d1b01d4/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/client_model v0.4.0 h1:5lQXD3cAg1OXBf4Wq03gTrXHeaV0TQvGfUooCfx1yqY=\ngithub.com/prometheus/client_model v0.4.0/go.mod h1:oMQmHW1/JoDwqLtg57MGgP/Fb1CJEYF2imWWhWtMkYU=\ngithub.com/prometheus/client_model v0.5.0 h1:VQw1hfvPvk3Uv6Qf29VrPF32JB6rtbgI6cYPYQjL0Qw=\ngithub.com/prometheus/client_model v0.5.0/go.mod h1:dTiFglRmd66nLR9Pv9f0mZi7B7fk5Pm3gvsjB5tr+kI=\ngithub.com/prometheus/common v0.44.0 h1:+5BrQJwiBB9xsMygAB3TNvpQKOwlkc25LbISbrdOOfY=\ngithub.com/prometheus/common v0.44.0/go.mod h1:ofAIvZbQ1e/nugmZGz4/qCb9Ap1VoSTIO7x0VV9VvuY=\ngithub.com/prometheus/common v0.45.0 h1:2BGz0eBc2hdMDLnO/8n0jeB3oPrt2D08CekT0lneoxM=\ngithub.com/prometheus/common v0.45.0/go.mod h1:YJmSTw9BoKxJplESWWxlbyttQR4uaEcGyv9MZjVOJsY=\ngithub.com/prometheus/procfs v0.11.1 h1:xRC8Iq1yyca5ypa9n1EZnWZkt7dwcoRPQwX/5gwaUuI=\ngithub.com/prometheus/procfs v0.11.1/go.mod h1:eesXgaPo1q7lBpVMoMy0ZOFTth9hBn4W/y0/p/ScXhY=\ngithub.com/prometheus/procfs v0.12.0 h1:jluTpSng7V9hY0O2R9DzzJHYb2xULk9VTR1V1R/k6Bo=\ngithub.com/prometheus/procfs v0.12.0/go.mod h1:pcuDEFsWDnvcgNzo4EEweacyhjeA9Zk3cnaOZAZEfOo=\ngithub.com/rogpeppe/fastuuid v1.2.0/go.mod h1:jVj6XXZzXRy/MSR5jhDC/2q6DgLz+nrA6LYCDYWNEvQ=\ngithub.com/rogpeppe/go-internal v1.10.0 h1:TMyTOH3F/DB16zRVcYyreMH6GnZZrwQVAoYjRBZyWFQ=\ngithub.com/rogpeppe/go-internal v1.10.0/go.mod h1:UQnix2H7Ngw/k4C5ijL5+65zddjncjaFoBhdsK/akog=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=\ngithub.com/stretchr/objx v0.5.0/go.mod h1:Yh+to48EsGEfYuaHDzXPcE3xhTkx73EhmCGUpEOglKo=\ngithub.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=\ngithub.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=\ngithub.com/stretchr/testify v1.8.1 h1:w7B6lhMri9wdJUVmEZPGGhZzrYTPvgJArz7wNPgYKsk=\ngithub.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=\ngithub.com/yuin/goldmark v1.4.13/go.mod h1:6yULJ656Px+3vBD8DxQVa3kxgyrAnzto9xy5taEt/CY=\ngo.opencensus.io v0.24.0 h1:y73uSU6J157QMP2kn2r30vwW1A2W2WFwSCGnAVxeaD0=\ngo.opencensus.io v0.24.0/go.mod h1:vNK8G9p7aAivkbmorf4v+7Hgx+Zs0yY+0fOtgBfjQKo=\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc v0.46.1 h1:SpGay3w+nEwMpfVnbqOLH5gY52/foP8RE8UzTZ1pdSE=\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc v0.46.1/go.mod h1:4UoMYEZOC0yN/sPGH76KPkkU7zgiEWYWL9vwmbnTJPE=\ngo.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.46.1 h1:aFJWCqJMNjENlcleuuOkGAPH82y0yULBScfXcIEdS24=\ngo.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.46.1/go.mod h1:sEGXWArGqc3tVa+ekntsN65DmVbVeW+7lTKTjZF3/Fo=\ngo.opentelemetry.io/otel v1.21.0 h1:hzLeKBZEL7Okw2mGzZ0cc4k/A7Fta0uoPgaJCr8fsFc=\ngo.opentelemetry.io/otel v1.21.0/go.mod h1:QZzNPQPm1zLX4gZK4cMi+71eaorMSGT3A4znnUvNNEo=\ngo.opentelemetry.io/otel/metric v1.21.0 h1:tlYWfeo+Bocx5kLEloTjbcDwBuELRrIFxwdQ36PlJu4=\ngo.opentelemetry.io/otel/metric v1.21.0/go.mod h1:o1p3CA8nNHW8j5yuQLdc1eeqEaPfzug24uvsyIEJRWM=\ngo.opentelemetry.io/otel/trace v1.21.0 h1:WD9i5gzvoUPuXIXH24ZNBudiarZDKuekPqi/E8fpfLc=\ngo.opentelemetry.io/otel/trace v1.21.0/go.mod h1:LGbsEB0f9LGjN+OZaQQ26sohbOmiMR+BaslueVtS/qQ=\ngo.opentelemetry.io/proto/otlp v0.7.0/go.mod h1:PqfVotwruBrMGOCsRd/89rSnXhoiJIqeYNgFYFoEGnI=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/crypto v0.0.0-20201002170205-7f63de1d35b0/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/crypto v0.0.0-20201016220609-9e8e0b390897/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/crypto v0.0.0-20210921155107-089bfa567519/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=\ngolang.org/x/crypto v0.0.0-20211215153901-e495a2d5b3d3/go.mod h1:IxCIyHEi3zRg3s0A5j5BB6A9Jmi73HwBIUl50j+osU4=\ngolang.org/x/crypto v0.0.0-20220314234659-1baeb1ce4c0b/go.mod h1:IxCIyHEi3zRg3s0A5j5BB6A9Jmi73HwBIUl50j+osU4=\ngolang.org/x/crypto v0.1.0/go.mod h1:RecgLatLF4+eUMCP1PoPZQb+cVrJcOPbHkTkbkB9sbw=\ngolang.org/x/crypto v0.12.0 h1:tFM/ta59kqch6LlvYnPa0yx5a83cL2nHflFhYKvv9Yk=\ngolang.org/x/crypto v0.12.0/go.mod h1:NF0Gs7EO5K4qLn+Ylc+fih8BSTeIjAP05siRnAh98yw=\ngolang.org/x/crypto v0.17.0 h1:r8bRNjWL3GshPW3gkd+RpvzWrZAwPS49OmTGZ/uhM4k=\ngolang.org/x/crypto v0.17.0/go.mod h1:gCAAfMLgwOJRpTjQ2zCCt2OcSfYMTeZVSRtQlPC7Nq4=\ngolang.org/x/exp v0.0.0-20190121172915-509febef88a4/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\ngolang.org/x/lint v0.0.0-20190227174305-5b3e6a55c961/go.mod h1:wehouNa3lNwaWXcvxsM5YxQ5yQlVC4a0KAMCusXpPoU=\ngolang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/mod v0.6.0-dev.0.20220419223038-86c51ed26bb4/go.mod h1:jJ57K6gSWd91VN4djpZkiMVwK6gcyfeH4XE8wZrZaV4=\ngolang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20180826012351-8a410e7b638d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190213061140-3a22650c66bd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190603091049-60506f45cf65/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=\ngolang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20191112182307-2180aed22343/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200822124328-c89045814202/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20201110031124-69a78807bb2b/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=\ngolang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\ngolang.org/x/net v0.0.0-20210610132358-84b48f89b13b/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\ngolang.org/x/net v0.0.0-20211112202133-69e39bad7dc2/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\ngolang.org/x/net v0.0.0-20220722155237-a158d28d115b/go.mod h1:XRhObCWvk6IyKnWLug+ECip1KBveYUHfp+8e9klMJ9c=\ngolang.org/x/net v0.1.0/go.mod h1:Cx3nUiGt4eDBEyega/BKRp+/AlGL8hYe7U9odMt2Cco=\ngolang.org/x/net v0.14.0 h1:BONx9s002vGdD9umnlX1Po8vOZmrgH34qlHcD1MfK14=\ngolang.org/x/net v0.14.0/go.mod h1:PpSgVXXLK0OxS0F31C1/tv6XNguvCrnXIDrFMspZIUI=\ngolang.org/x/net v0.19.0 h1:zTwKpTd2XuCqf8huc7Fo2iSy+4RHPd10s4KzeTnVr1c=\ngolang.org/x/net v0.19.0/go.mod h1:CfAk/cbD4CthTvqiEl8NpboMuiuOYsAr/7NOjZJtv1U=\ngolang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=\ngolang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.11.0 h1:vPL4xzxBM4niKCW6g9whtaWVXTJf1U5e4aZxxFx/gbU=\ngolang.org/x/oauth2 v0.11.0/go.mod h1:LdF7O/8bLR/qWK9DrpXmbHLTouvRHK0SgJl0GmDBchk=\ngolang.org/x/oauth2 v0.15.0 h1:s8pnnxNVzjWyrvYdFUQq5llS1PX2zhPXmccZv99h7uQ=\ngolang.org/x/oauth2 v0.15.0/go.mod h1:q48ptWNTY5XWf+JNten23lcvHpLJ0ZSxF5ttTHKVCAM=\ngolang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20220722155255-886fb9371eb4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.3.0 h1:ftCYgMx6zT/asHUrPw8BLLscYtGznsLAnjq5RH9P66E=\ngolang.org/x/sync v0.3.0/go.mod h1:FU7BRWz2tNW+3quACPkgCx/L+uEAv1htQ0V83Z9Rj+Y=\ngolang.org/x/sync v0.5.0 h1:60k92dhOjHxJkrqnwsfl8KuaHbn/5dl0lUPUklKo3qE=\ngolang.org/x/sync v0.5.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\ngolang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191112214154-59a1497f0cea/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210423082822-04245dca01da/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20211216021012-1d35b9e2eb4e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220520151302-bc2c85ada10a/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220722155257-8c9f86f7a55f/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.1.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.11.0 h1:eG7RXZHdqOJ1i+0lgLgCpSXAp6M3LYlAo6osgSi0xOM=\ngolang.org/x/sys v0.11.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.15.0 h1:h48lPFYpsTvQJZF4EKyI4aLHaev3CxivZmv7yZig9pc=\ngolang.org/x/sys v0.15.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngolang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=\ngolang.org/x/term v0.0.0-20210927222741-03fcf44c2211/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=\ngolang.org/x/term v0.1.0/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=\ngolang.org/x/term v0.11.0 h1:F9tnn/DA/Im8nCwm+fX+1/eBwi4qFjRT++MhtVC4ZX0=\ngolang.org/x/term v0.11.0/go.mod h1:zC9APTIj3jG3FdV/Ons+XE1riIZXG4aZ4GTHiPZJPIU=\ngolang.org/x/term v0.15.0 h1:y/Oo/a/q3IXu26lQgl04j/gjuBDOBlx7X6Om1j2CPW4=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\ngolang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.6/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=\ngolang.org/x/text v0.3.8/go.mod h1:E6s5w1FMmriuDzIBO73fBruAKo1PCIq6d2Q6DHfQ8WQ=\ngolang.org/x/text v0.4.0/go.mod h1:mrYo+phRRbMaCq/xk9113O4dZlRixOauAjOtrjsXDZ8=\ngolang.org/x/text v0.12.0 h1:k+n5B8goJNdU7hSvEtMUz3d1Q6D/XW4COJSJR6fN0mc=\ngolang.org/x/text v0.12.0/go.mod h1:TvPlkZtksWOMsz7fbANvkp4WM8x/WCo/om8BMLbz+aE=\ngolang.org/x/text v0.14.0 h1:ScX5w1eTa3QqT8oi6+ziP7dTV1S2+ALU0bI+0zXKWiQ=\ngolang.org/x/text v0.14.0/go.mod h1:18ZOQIKpY8NJVqYksKHtTdi31H5itFRjB5/qKTNYzSU=\ngolang.org/x/time v0.5.0 h1:o7cqy6amK/52YcAKIPlM3a+Fpj35zvRj2TP+e1xFSfk=\ngolang.org/x/time v0.5.0/go.mod h1:3BpzKBy/shNhVucY/MWOyx10tF3SFh9QdLuxbVysPQM=\ngolang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190114222345-bf090417da8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190226205152-f727befe758c/go.mod h1:9Yl7xja0Znq3iFh3HoIrodX9oNMXvdceNzlUR8zjMvY=\ngolang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190524140312-2c0ae7006135/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.1.12/go.mod h1:hNGJHUnrk76NpqgfD5Aqm5Crs+Hm0VOH/i9J2+nxYbc=\ngolang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20220907171357-04be3eba64a2 h1:H2TDz8ibqkAF6YGhCdN3jS9O0/s90v0rJh3X/OLHEUk=\ngolang.org/x/xerrors v0.0.0-20220907171357-04be3eba64a2/go.mod h1:K8+ghG5WaK9qNqU5K3HdILfMLy1f3aNYFI/wnl100a8=\ngolang.org/x/xerrors v0.0.0-20231012003039-104605ab7028 h1:+cNy6SZtPcJQH3LJVLOSmiC7MMxXNOb3PU/VUEz+EhU=\ngolang.org/x/xerrors v0.0.0-20231012003039-104605ab7028/go.mod h1:NDW/Ps6MPRej6fsCIbMTohpP40sJ/P/vI1MoTEGwX90=\ngoogle.golang.org/api v0.135.0 h1:6Vgfj6uPMXcyy66waYWBwmkeNB+9GmUlJDOzkukPQYQ=\ngoogle.golang.org/api v0.135.0/go.mod h1:Bp77uRFgwsSKI0BWH573F5Q6wSlznwI2NFayLOp/7mQ=\ngoogle.golang.org/api v0.154.0 h1:X7QkVKZBskztmpPKWQXgjJRPA2dJYrL6r+sYPRLj050=\ngoogle.golang.org/api v0.154.0/go.mod h1:qhSMkM85hgqiokIYsrRyKxrjfBeIhgl4Z2JmeRkYylc=\ngoogle.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=\ngoogle.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\ngoogle.golang.org/appengine v1.6.7 h1:FZR1q0exgwxzPzp/aF+VccGrSfxfPpkBqjIIEq3ru6c=\ngoogle.golang.org/appengine v1.6.7/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\ngoogle.golang.org/appengine v1.6.8 h1:IhEN5q69dyKagZPYMSdIjS2HqprW324FRQZJcGqPAsM=\ngoogle.golang.org/appengine v1.6.8/go.mod h1:1jJ3jBArFh5pcgW8gCtRJnepW8FzD1V44FJffLiz/Ds=\ngoogle.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=\ngoogle.golang.org/genproto v0.0.0-20190819201941-24fa4b261c55/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=\ngoogle.golang.org/genproto v0.0.0-20200513103714-09dca8ec2884/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200526211855-cb27e3aa2013/go.mod h1:NbSheEEYHJ7i3ixzK3sjbqSGDJWnxyFXZblF3eUsNvo=\ngoogle.golang.org/genproto v0.0.0-20230807174057-1744710a1577 h1:Tyk/35yqszRCvaragTn5NnkY6IiKk/XvHzEWepo71N0=\ngoogle.golang.org/genproto v0.0.0-20230807174057-1744710a1577/go.mod h1:yZTlhN0tQnXo3h00fuXNCxJdLdIdnVFVBaRJ5LWBbw4=\ngoogle.golang.org/genproto v0.0.0-20231212172506-995d672761c0 h1:YJ5pD9rF8o9Qtta0Cmy9rdBwkSjrTCT6XTiUQVOtIos=\ngoogle.golang.org/genproto v0.0.0-20231212172506-995d672761c0/go.mod h1:l/k7rMz0vFTBPy+tFSGvXEd3z+BcoG1k7EHbqm+YBsY=\ngoogle.golang.org/genproto/googleapis/api v0.0.0-20230807174057-1744710a1577 h1:xv8KoglAClYGkprUSmDTKaILtzfD8XzG9NYVXMprjKo=\ngoogle.golang.org/genproto/googleapis/api v0.0.0-20230807174057-1744710a1577/go.mod h1:KjSP20unUpOx5kyQUFa7k4OJg0qeJ7DEZflGDu2p6Bk=\ngoogle.golang.org/genproto/googleapis/api v0.0.0-20231212172506-995d672761c0 h1:s1w3X6gQxwrLEpxnLd/qXTVLgQE2yXwaOaoa6IlY/+o=\ngoogle.golang.org/genproto/googleapis/api v0.0.0-20231212172506-995d672761c0/go.mod h1:CAny0tYF+0/9rmDB9fahA9YLzX3+AEVl1qXbv5hhj6c=\ngoogle.golang.org/genproto/googleapis/rpc v0.0.0-20230807174057-1744710a1577 h1:wukfNtZmZUurLN/atp2hiIeTKn7QJWIQdHzqmsOnAOk=\ngoogle.golang.org/genproto/googleapis/rpc v0.0.0-20230807174057-1744710a1577/go.mod h1:+Bk1OCOj40wS2hwAMA+aCW9ypzm63QTBBHp6lQ3p+9M=\ngoogle.golang.org/genproto/googleapis/rpc v0.0.0-20231212172506-995d672761c0 h1:/jFB8jK5R3Sq3i/lmeZO0cATSzFfZaJq1J2Euan3XKU=\ngoogle.golang.org/genproto/googleapis/rpc v0.0.0-20231212172506-995d672761c0/go.mod h1:FUoWkonphQm3RhTS+kOEhF8h0iDpm4tdXolVCeZ9KKA=\ngoogle.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=\ngoogle.golang.org/grpc v1.23.0/go.mod h1:Y5yQAOtifL1yxbo5wqy6BxZv8vAUGQwXBOALyacEbxg=\ngoogle.golang.org/grpc v1.25.1/go.mod h1:c3i+UQWmh7LiEpx4sFZnkU36qjEYZ0imhYfXVyQciAY=\ngoogle.golang.org/grpc v1.27.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.33.1/go.mod h1:fr5YgcSWrqhRRxogOsw7RzIpsmvOZ6IcH4kBYTpR3n0=\ngoogle.golang.org/grpc v1.33.2/go.mod h1:JMHMWHQWaTccqQQlmk3MJZS+GWXOdAesneDmEnv2fbc=\ngoogle.golang.org/grpc v1.36.0/go.mod h1:qjiiYl8FncCW8feJPdyg3v6XW24KsRHe+dy9BAGRRjU=\ngoogle.golang.org/grpc v1.45.0/go.mod h1:lN7owxKUQEqMfSyQikvvk5tf/6zMPsrK+ONuO11+0rQ=\ngoogle.golang.org/grpc v1.57.0 h1:kfzNeI/klCGD2YPMUlaGNT3pxvYfga7smW3Vth8Zsiw=\ngoogle.golang.org/grpc v1.57.0/go.mod h1:Sd+9RMTACXwmub0zcNY2c4arhtrbBYD1AUHI/dt16Mo=\ngoogle.golang.org/grpc v1.60.1 h1:26+wFr+cNqSGFcOXcabYC0lUVJVRa2Sb2ortSK7VrEU=\ngoogle.golang.org/grpc v1.60.1/go.mod h1:OlCHIeLYqSSsLi6i49B5QGdzaMZK9+M7LXN2FKz4eGM=\ngoogle.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd/go.mod h1:DFci5gLYBciE7Vtevhsrf46CRTquxDuWsQurQQe4oz8=\ngoogle.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64/go.mod h1:kwYJMbMJ01Woi6D6+Kah6886xMZcty6N08ah7+eCXa0=\ngoogle.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60/go.mod h1:cfTl7dwQJ+fmap5saPgwCLgHXTUD7jkjRqWcaiX5VyM=\ngoogle.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967/go.mod h1:A+miEFZTKqfCUM6K7xSMQL9OKL/b6hQv+e19PK+JZNE=\ngoogle.golang.org/protobuf v1.21.0/go.mod h1:47Nbq4nVaFHyn7ilMalzfO3qCViNmqZ2kzikPIcrTAo=\ngoogle.golang.org/protobuf v1.22.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.23.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.23.1-0.20200526195155-81db48ad09cc/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.25.0/go.mod h1:9JNX74DMeImyA3h4bdi1ymwjUzf21/xIlbajtzgsN7c=\ngoogle.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=\ngoogle.golang.org/protobuf v1.26.0/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=\ngoogle.golang.org/protobuf v1.31.0 h1:g0LDEJHgrBl9N9r17Ru3sqWhkIx2NB67okBHPwC7hs8=\ngoogle.golang.org/protobuf v1.31.0/go.mod h1:HV8QOd/L58Z+nl8r43ehVNZIU/HEI6OcFqwMG9pJV4I=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=\ngopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.3/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.8/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=\ngopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\nhonnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190523083050-ea95bdfd59fc/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n"
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "litestream.go",
          "type": "blob",
          "size": 14.763671875,
          "content": "package litestream\n\nimport (\n\t\"database/sql\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mattn/go-sqlite3\"\n)\n\n// Naming constants.\nconst (\n\tMetaDirSuffix = \"-litestream\"\n\n\tWALDirName    = \"wal\"\n\tWALExt        = \".wal\"\n\tWALSegmentExt = \".wal.lz4\"\n\tSnapshotExt   = \".snapshot.lz4\"\n\n\tGenerationNameLen = 16\n)\n\n// SQLite checkpoint modes.\nconst (\n\tCheckpointModePassive  = \"PASSIVE\"\n\tCheckpointModeFull     = \"FULL\"\n\tCheckpointModeRestart  = \"RESTART\"\n\tCheckpointModeTruncate = \"TRUNCATE\"\n)\n\n// Litestream errors.\nvar (\n\tErrNoGeneration     = errors.New(\"no generation available\")\n\tErrNoSnapshots      = errors.New(\"no snapshots available\")\n\tErrChecksumMismatch = errors.New(\"invalid replica, checksum mismatch\")\n)\n\nvar (\n\t// LogWriter is the destination writer for all logging.\n\tLogWriter = os.Stdout\n\n\t// LogFlags are the flags passed to log.New().\n\tLogFlags = 0\n)\n\nfunc init() {\n\tsql.Register(\"litestream-sqlite3\", &sqlite3.SQLiteDriver{\n\t\tConnectHook: func(conn *sqlite3.SQLiteConn) error {\n\t\t\tif err := conn.SetFileControlInt(\"main\", sqlite3.SQLITE_FCNTL_PERSIST_WAL, 1); err != nil {\n\t\t\t\treturn fmt.Errorf(\"cannot set file control: %w\", err)\n\t\t\t}\n\t\t\treturn nil\n\t\t},\n\t})\n}\n\n// SnapshotIterator represents an iterator over a collection of snapshot metadata.\ntype SnapshotIterator interface {\n\tio.Closer\n\n\t// Prepares the the next snapshot for reading with the Snapshot() method.\n\t// Returns true if another snapshot is available. Returns false if no more\n\t// snapshots are available or if an error occured.\n\tNext() bool\n\n\t// Returns an error that occurred during iteration.\n\tErr() error\n\n\t// Returns metadata for the currently positioned snapshot.\n\tSnapshot() SnapshotInfo\n}\n\n// SliceSnapshotIterator returns all snapshots from an iterator as a slice.\nfunc SliceSnapshotIterator(itr SnapshotIterator) ([]SnapshotInfo, error) {\n\tvar a []SnapshotInfo\n\tfor itr.Next() {\n\t\ta = append(a, itr.Snapshot())\n\t}\n\treturn a, itr.Close()\n}\n\nvar _ SnapshotIterator = (*SnapshotInfoSliceIterator)(nil)\n\n// SnapshotInfoSliceIterator represents an iterator for iterating over a slice of snapshots.\ntype SnapshotInfoSliceIterator struct {\n\tinit bool\n\ta    []SnapshotInfo\n}\n\n// NewSnapshotInfoSliceIterator returns a new instance of SnapshotInfoSliceIterator.\nfunc NewSnapshotInfoSliceIterator(a []SnapshotInfo) *SnapshotInfoSliceIterator {\n\treturn &SnapshotInfoSliceIterator{a: a}\n}\n\n// Close always returns nil.\nfunc (itr *SnapshotInfoSliceIterator) Close() error { return nil }\n\n// Next moves to the next snapshot. Returns true if another snapshot is available.\nfunc (itr *SnapshotInfoSliceIterator) Next() bool {\n\tif !itr.init {\n\t\titr.init = true\n\t\treturn len(itr.a) > 0\n\t}\n\titr.a = itr.a[1:]\n\treturn len(itr.a) > 0\n}\n\n// Err always returns nil.\nfunc (itr *SnapshotInfoSliceIterator) Err() error { return nil }\n\n// Snapshot returns the metadata from the currently positioned snapshot.\nfunc (itr *SnapshotInfoSliceIterator) Snapshot() SnapshotInfo {\n\tif len(itr.a) == 0 {\n\t\treturn SnapshotInfo{}\n\t}\n\treturn itr.a[0]\n}\n\n// WALSegmentIterator represents an iterator over a collection of WAL segments.\ntype WALSegmentIterator interface {\n\tio.Closer\n\n\t// Prepares the the next WAL for reading with the WAL() method.\n\t// Returns true if another WAL is available. Returns false if no more\n\t// WAL files are available or if an error occured.\n\tNext() bool\n\n\t// Returns an error that occurred during iteration.\n\tErr() error\n\n\t// Returns metadata for the currently positioned WAL segment file.\n\tWALSegment() WALSegmentInfo\n}\n\n// SliceWALSegmentIterator returns all WAL segment files from an iterator as a slice.\nfunc SliceWALSegmentIterator(itr WALSegmentIterator) ([]WALSegmentInfo, error) {\n\tvar a []WALSegmentInfo\n\tfor itr.Next() {\n\t\ta = append(a, itr.WALSegment())\n\t}\n\treturn a, itr.Close()\n}\n\nvar _ WALSegmentIterator = (*WALSegmentInfoSliceIterator)(nil)\n\n// WALSegmentInfoSliceIterator represents an iterator for iterating over a slice of wal segments.\ntype WALSegmentInfoSliceIterator struct {\n\tinit bool\n\ta    []WALSegmentInfo\n}\n\n// NewWALSegmentInfoSliceIterator returns a new instance of WALSegmentInfoSliceIterator.\nfunc NewWALSegmentInfoSliceIterator(a []WALSegmentInfo) *WALSegmentInfoSliceIterator {\n\treturn &WALSegmentInfoSliceIterator{a: a}\n}\n\n// Close always returns nil.\nfunc (itr *WALSegmentInfoSliceIterator) Close() error { return nil }\n\n// Next moves to the next wal segment. Returns true if another segment is available.\nfunc (itr *WALSegmentInfoSliceIterator) Next() bool {\n\tif !itr.init {\n\t\titr.init = true\n\t\treturn len(itr.a) > 0\n\t}\n\titr.a = itr.a[1:]\n\treturn len(itr.a) > 0\n}\n\n// Err always returns nil.\nfunc (itr *WALSegmentInfoSliceIterator) Err() error { return nil }\n\n// WALSegment returns the metadata from the currently positioned wal segment.\nfunc (itr *WALSegmentInfoSliceIterator) WALSegment() WALSegmentInfo {\n\tif len(itr.a) == 0 {\n\t\treturn WALSegmentInfo{}\n\t}\n\treturn itr.a[0]\n}\n\n// SnapshotInfo represents file information about a snapshot.\ntype SnapshotInfo struct {\n\tGeneration string\n\tIndex      int\n\tSize       int64\n\tCreatedAt  time.Time\n}\n\n// Pos returns the WAL position when the snapshot was made.\nfunc (info *SnapshotInfo) Pos() Pos {\n\treturn Pos{Generation: info.Generation, Index: info.Index}\n}\n\n// SnapshotInfoSlice represents a slice of snapshot metadata.\ntype SnapshotInfoSlice []SnapshotInfo\n\nfunc (a SnapshotInfoSlice) Len() int { return len(a) }\n\nfunc (a SnapshotInfoSlice) Swap(i, j int) { a[i], a[j] = a[j], a[i] }\n\nfunc (a SnapshotInfoSlice) Less(i, j int) bool {\n\tif a[i].Generation != a[j].Generation {\n\t\treturn a[i].Generation < a[j].Generation\n\t}\n\treturn a[i].Index < a[j].Index\n}\n\n// FilterSnapshotsAfter returns all snapshots that were created on or after t.\nfunc FilterSnapshotsAfter(a []SnapshotInfo, t time.Time) []SnapshotInfo {\n\tother := make([]SnapshotInfo, 0, len(a))\n\tfor _, snapshot := range a {\n\t\tif !snapshot.CreatedAt.Before(t) {\n\t\t\tother = append(other, snapshot)\n\t\t}\n\t}\n\treturn other\n}\n\n// FindMinSnapshotByGeneration finds the snapshot with the lowest index in a generation.\nfunc FindMinSnapshotByGeneration(a []SnapshotInfo, generation string) *SnapshotInfo {\n\tvar min *SnapshotInfo\n\tfor i := range a {\n\t\tsnapshot := &a[i]\n\n\t\tif snapshot.Generation != generation {\n\t\t\tcontinue\n\t\t} else if min == nil || snapshot.Index < min.Index {\n\t\t\tmin = snapshot\n\t\t}\n\t}\n\treturn min\n}\n\n// WALInfo represents file information about a WAL file.\ntype WALInfo struct {\n\tGeneration string\n\tIndex      int\n\tCreatedAt  time.Time\n}\n\n// WALInfoSlice represents a slice of WAL metadata.\ntype WALInfoSlice []WALInfo\n\nfunc (a WALInfoSlice) Len() int { return len(a) }\n\nfunc (a WALInfoSlice) Swap(i, j int) { a[i], a[j] = a[j], a[i] }\n\nfunc (a WALInfoSlice) Less(i, j int) bool {\n\tif a[i].Generation != a[j].Generation {\n\t\treturn a[i].Generation < a[j].Generation\n\t}\n\treturn a[i].Index < a[j].Index\n}\n\n// WALSegmentInfo represents file information about a WAL segment file.\ntype WALSegmentInfo struct {\n\tGeneration string\n\tIndex      int\n\tOffset     int64\n\tSize       int64\n\tCreatedAt  time.Time\n}\n\n// Pos returns the WAL position when the segment was made.\nfunc (info *WALSegmentInfo) Pos() Pos {\n\treturn Pos{Generation: info.Generation, Index: info.Index, Offset: info.Offset}\n}\n\n// WALSegmentInfoSlice represents a slice of WAL segment metadata.\ntype WALSegmentInfoSlice []WALSegmentInfo\n\nfunc (a WALSegmentInfoSlice) Len() int { return len(a) }\n\nfunc (a WALSegmentInfoSlice) Swap(i, j int) { a[i], a[j] = a[j], a[i] }\n\nfunc (a WALSegmentInfoSlice) Less(i, j int) bool {\n\tif a[i].Generation != a[j].Generation {\n\t\treturn a[i].Generation < a[j].Generation\n\t} else if a[i].Index != a[j].Index {\n\t\treturn a[i].Index < a[j].Index\n\t}\n\treturn a[i].Offset < a[j].Offset\n}\n\n// Pos is a position in the WAL for a generation.\ntype Pos struct {\n\tGeneration string // generation name\n\tIndex      int    // wal file index\n\tOffset     int64  // offset within wal file\n}\n\n// String returns a string representation.\nfunc (p Pos) String() string {\n\tif p.IsZero() {\n\t\treturn \"\"\n\t}\n\treturn fmt.Sprintf(\"%s/%08x:%d\", p.Generation, p.Index, p.Offset)\n}\n\n// IsZero returns true if p is the zero value.\nfunc (p Pos) IsZero() bool {\n\treturn p == (Pos{})\n}\n\n// Truncate returns p with the offset truncated to zero.\nfunc (p Pos) Truncate() Pos {\n\treturn Pos{Generation: p.Generation, Index: p.Index}\n}\n\n// Checksum computes a running SQLite checksum over a byte slice.\nfunc Checksum(bo binary.ByteOrder, s0, s1 uint32, b []byte) (uint32, uint32) {\n\tassert(len(b)%8 == 0, \"misaligned checksum byte slice\")\n\n\t// Iterate over 8-byte units and compute checksum.\n\tfor i := 0; i < len(b); i += 8 {\n\t\ts0 += bo.Uint32(b[i:]) + s1\n\t\ts1 += bo.Uint32(b[i+4:]) + s0\n\t}\n\treturn s0, s1\n}\n\nconst (\n\t// WALHeaderSize is the size of the WAL header, in bytes.\n\tWALHeaderSize = 32\n\n\t// WALFrameHeaderSize is the size of the WAL frame header, in bytes.\n\tWALFrameHeaderSize = 24\n)\n\n// calcWALSize returns the size of the WAL, in bytes, for a given number of pages.\nfunc calcWALSize(pageSize int, n int) int64 {\n\treturn int64(WALHeaderSize + ((WALFrameHeaderSize + pageSize) * n))\n}\n\n// rollback rolls back tx. Ignores already-rolled-back errors.\nfunc rollback(tx *sql.Tx) error {\n\tif err := tx.Rollback(); err != nil && !strings.Contains(err.Error(), `transaction has already been committed or rolled back`) {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// readWALHeader returns the header read from a WAL file.\nfunc readWALHeader(filename string) ([]byte, error) {\n\tf, err := os.Open(filename)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\tbuf := make([]byte, WALHeaderSize)\n\tn, err := io.ReadFull(f, buf)\n\treturn buf[:n], err\n}\n\n// readWALFileAt reads a slice from a file. Do not use this with database files\n// as it causes problems with non-OFD locks.\nfunc readWALFileAt(filename string, offset, n int64) ([]byte, error) {\n\tf, err := os.Open(filename)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\tbuf := make([]byte, n)\n\tif n, err := f.ReadAt(buf, offset); err != nil {\n\t\treturn buf[:n], err\n\t} else if n < len(buf) {\n\t\treturn buf[:n], io.ErrUnexpectedEOF\n\t}\n\treturn buf, nil\n}\n\n// removeTmpFiles recursively finds and removes .tmp files.\nfunc removeTmpFiles(root string) error {\n\treturn filepath.Walk(root, func(path string, info os.FileInfo, err error) error {\n\t\tif err != nil {\n\t\t\treturn nil // skip errored files\n\t\t} else if info.IsDir() {\n\t\t\treturn nil // skip directories\n\t\t} else if !strings.HasSuffix(path, \".tmp\") {\n\t\t\treturn nil // skip non-temp files\n\t\t}\n\t\treturn os.Remove(path)\n\t})\n}\n\n// IsGenerationName returns true if s is the correct length and is only lowercase hex characters.\nfunc IsGenerationName(s string) bool {\n\tif len(s) != GenerationNameLen {\n\t\treturn false\n\t}\n\tfor _, ch := range s {\n\t\tif !isHexChar(ch) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// GenerationsPath returns the path to a generation root directory.\nfunc GenerationsPath(root string) string {\n\treturn path.Join(root, \"generations\")\n}\n\n// GenerationPath returns the path to a generation's root directory.\nfunc GenerationPath(root, generation string) (string, error) {\n\tdir := GenerationsPath(root)\n\tif generation == \"\" {\n\t\treturn \"\", fmt.Errorf(\"generation required\")\n\t}\n\treturn path.Join(dir, generation), nil\n}\n\n// SnapshotsPath returns the path to a generation's snapshot directory.\nfunc SnapshotsPath(root, generation string) (string, error) {\n\tdir, err := GenerationPath(root, generation)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn path.Join(dir, \"snapshots\"), nil\n}\n\n// SnapshotPath returns the path to an uncompressed snapshot file.\nfunc SnapshotPath(root, generation string, index int) (string, error) {\n\tdir, err := SnapshotsPath(root, generation)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn path.Join(dir, FormatSnapshotPath(index)), nil\n}\n\n// WALPath returns the path to a generation's WAL directory\nfunc WALPath(root, generation string) (string, error) {\n\tdir, err := GenerationPath(root, generation)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn path.Join(dir, \"wal\"), nil\n}\n\n// WALSegmentPath returns the path to a WAL segment file.\nfunc WALSegmentPath(root, generation string, index int, offset int64) (string, error) {\n\tdir, err := WALPath(root, generation)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn path.Join(dir, FormatWALSegmentPath(index, offset)), nil\n}\n\n// IsSnapshotPath returns true if s is a path to a snapshot file.\nfunc IsSnapshotPath(s string) bool {\n\treturn snapshotPathRegex.MatchString(s)\n}\n\n// ParseSnapshotPath returns the index for the snapshot.\n// Returns an error if the path is not a valid snapshot path.\nfunc ParseSnapshotPath(s string) (index int, err error) {\n\ts = filepath.Base(s)\n\n\ta := snapshotPathRegex.FindStringSubmatch(s)\n\tif a == nil {\n\t\treturn 0, fmt.Errorf(\"invalid snapshot path: %s\", s)\n\t}\n\n\ti64, _ := strconv.ParseUint(a[1], 16, 64)\n\treturn int(i64), nil\n}\n\n// FormatSnapshotPath formats a snapshot filename with a given index.\nfunc FormatSnapshotPath(index int) string {\n\tassert(index >= 0, \"snapshot index must be non-negative\")\n\treturn fmt.Sprintf(\"%08x%s\", index, SnapshotExt)\n}\n\nvar snapshotPathRegex = regexp.MustCompile(`^([0-9a-f]{8})\\.snapshot\\.lz4$`)\n\n// IsWALPath returns true if s is a path to a WAL file.\nfunc IsWALPath(s string) bool {\n\treturn walPathRegex.MatchString(s)\n}\n\n// ParseWALPath returns the index for the WAL file.\n// Returns an error if the path is not a valid WAL path.\nfunc ParseWALPath(s string) (index int, err error) {\n\ts = filepath.Base(s)\n\n\ta := walPathRegex.FindStringSubmatch(s)\n\tif a == nil {\n\t\treturn 0, fmt.Errorf(\"invalid wal path: %s\", s)\n\t}\n\n\ti64, _ := strconv.ParseUint(a[1], 16, 64)\n\treturn int(i64), nil\n}\n\n// FormatWALPath formats a WAL filename with a given index.\nfunc FormatWALPath(index int) string {\n\tassert(index >= 0, \"wal index must be non-negative\")\n\treturn fmt.Sprintf(\"%08x%s\", index, WALExt)\n}\n\nvar walPathRegex = regexp.MustCompile(`^([0-9a-f]{8})\\.wal$`)\n\n// ParseWALSegmentPath returns the index & offset for the WAL segment file.\n// Returns an error if the path is not a valid wal segment path.\nfunc ParseWALSegmentPath(s string) (index int, offset int64, err error) {\n\ts = filepath.Base(s)\n\n\ta := walSegmentPathRegex.FindStringSubmatch(s)\n\tif a == nil {\n\t\treturn 0, 0, fmt.Errorf(\"invalid wal segment path: %s\", s)\n\t}\n\n\ti64, _ := strconv.ParseUint(a[1], 16, 64)\n\toff64, _ := strconv.ParseUint(a[2], 16, 64)\n\treturn int(i64), int64(off64), nil\n}\n\n// FormatWALSegmentPath formats a WAL segment filename with a given index & offset.\nfunc FormatWALSegmentPath(index int, offset int64) string {\n\tassert(index >= 0, \"wal index must be non-negative\")\n\tassert(offset >= 0, \"wal offset must be non-negative\")\n\treturn fmt.Sprintf(\"%08x_%08x%s\", index, offset, WALSegmentExt)\n}\n\nvar walSegmentPathRegex = regexp.MustCompile(`^([0-9a-f]{8})(?:_([0-9a-f]{8}))\\.wal\\.lz4$`)\n\n// isHexChar returns true if ch is a lowercase hex character.\nfunc isHexChar(ch rune) bool {\n\treturn (ch >= '0' && ch <= '9') || (ch >= 'a' && ch <= 'f')\n}\n\nfunc assert(condition bool, message string) {\n\tif !condition {\n\t\tpanic(\"assertion failed: \" + message)\n\t}\n}\n"
        },
        {
          "name": "litestream_test.go",
          "type": "blob",
          "size": 20.89453125,
          "content": "package litestream_test\n\nimport (\n\t\"encoding/binary\"\n\t\"encoding/hex\"\n\t\"testing\"\n\n\t\"github.com/benbjohnson/litestream\"\n\t_ \"github.com/mattn/go-sqlite3\"\n)\n\nfunc TestChecksum(t *testing.T) {\n\t// Ensure a WAL header, frame header, & frame data can be checksummed in one pass.\n\tt.Run(\"OnePass\", func(t *testing.T) {\n\t\tinput, err := hex.DecodeString(\"377f0682002de218000010000000000052382eac857b1a4e00000002000000020d000000080fe0000ffc0ff80ff40ff00fec0fe80fe40fe000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000208020902070209020602090205020902040209020302090202020902010209\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\ts0, s1 := litestream.Checksum(binary.LittleEndian, 0, 0, input)\n\t\tif got, want := [2]uint32{s0, s1}, [2]uint32{0xdc2f3e84, 0x540488d3}; got != want {\n\t\t\tt.Fatalf(\"Checksum()=%x, want %x\", got, want)\n\t\t}\n\t})\n\n\t// Ensure we get the same result as OnePass even if we split up into multiple calls.\n\tt.Run(\"Incremental\", func(t *testing.T) {\n\t\t// Compute checksum for beginning of WAL header.\n\t\ts0, s1 := litestream.Checksum(binary.LittleEndian, 0, 0, MustDecodeHexString(\"377f0682002de218000010000000000052382eac857b1a4e\"))\n\t\tif got, want := [2]uint32{s0, s1}, [2]uint32{0x81153b65, 0x87178e8f}; got != want {\n\t\t\tt.Fatalf(\"Checksum()=%x, want %x\", got, want)\n\t\t}\n\n\t\t// Continue checksum with WAL frame header & frame contents.\n\t\ts0a, s1a := litestream.Checksum(binary.LittleEndian, s0, s1, MustDecodeHexString(\"0000000200000002\"))\n\t\ts0b, s1b := litestream.Checksum(binary.LittleEndian, s0a, s1a, MustDecodeHexString(`0d000000080fe0000ffc0ff80ff40ff00fec0fe80fe40fe000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000208020902070209020602090205020902040209020302090202020902010209`))\n\t\tif got, want := [2]uint32{s0b, s1b}, [2]uint32{0xdc2f3e84, 0x540488d3}; got != want {\n\t\t\tt.Fatalf(\"Checksum()=%x, want %x\", got, want)\n\t\t}\n\t})\n}\n\nfunc TestGenerationsPath(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tif got, want := litestream.GenerationsPath(\"foo\"), \"foo/generations\"; got != want {\n\t\t\tt.Fatalf(\"GenerationsPath()=%v, want %v\", got, want)\n\t\t}\n\t})\n\tt.Run(\"NoPath\", func(t *testing.T) {\n\t\tif got, want := litestream.GenerationsPath(\"\"), \"generations\"; got != want {\n\t\t\tt.Fatalf(\"GenerationsPath()=%v, want %v\", got, want)\n\t\t}\n\t})\n}\n\nfunc TestGenerationPath(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tif got, err := litestream.GenerationPath(\"foo\", \"0123456701234567\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if want := \"foo/generations/0123456701234567\"; got != want {\n\t\t\tt.Fatalf(\"GenerationPath()=%v, want %v\", got, want)\n\t\t}\n\t})\n\tt.Run(\"ErrNoGeneration\", func(t *testing.T) {\n\t\tif _, err := litestream.GenerationPath(\"foo\", \"\"); err == nil || err.Error() != `generation required` {\n\t\t\tt.Fatalf(\"expected error: %v\", err)\n\t\t}\n\t})\n}\n\nfunc TestSnapshotsPath(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tif got, err := litestream.SnapshotsPath(\"foo\", \"0123456701234567\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if want := \"foo/generations/0123456701234567/snapshots\"; got != want {\n\t\t\tt.Fatalf(\"SnapshotsPath()=%v, want %v\", got, want)\n\t\t}\n\t})\n\tt.Run(\"ErrNoGeneration\", func(t *testing.T) {\n\t\tif _, err := litestream.SnapshotsPath(\"foo\", \"\"); err == nil || err.Error() != `generation required` {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t})\n}\n\nfunc TestSnapshotPath(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tif got, err := litestream.SnapshotPath(\"foo\", \"0123456701234567\", 1000); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if want := \"foo/generations/0123456701234567/snapshots/000003e8.snapshot.lz4\"; got != want {\n\t\t\tt.Fatalf(\"SnapshotPath()=%v, want %v\", got, want)\n\t\t}\n\t})\n\tt.Run(\"ErrNoGeneration\", func(t *testing.T) {\n\t\tif _, err := litestream.SnapshotPath(\"foo\", \"\", 1000); err == nil || err.Error() != `generation required` {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t})\n}\n\nfunc TestWALPath(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tif got, err := litestream.WALPath(\"foo\", \"0123456701234567\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if want := \"foo/generations/0123456701234567/wal\"; got != want {\n\t\t\tt.Fatalf(\"WALPath()=%v, want %v\", got, want)\n\t\t}\n\t})\n\tt.Run(\"ErrNoGeneration\", func(t *testing.T) {\n\t\tif _, err := litestream.WALPath(\"foo\", \"\"); err == nil || err.Error() != `generation required` {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t})\n}\n\nfunc TestWALSegmentPath(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tif got, err := litestream.WALSegmentPath(\"foo\", \"0123456701234567\", 1000, 1001); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if want := \"foo/generations/0123456701234567/wal/000003e8_000003e9.wal.lz4\"; got != want {\n\t\t\tt.Fatalf(\"WALPath()=%v, want %v\", got, want)\n\t\t}\n\t})\n\tt.Run(\"ErrNoGeneration\", func(t *testing.T) {\n\t\tif _, err := litestream.WALSegmentPath(\"foo\", \"\", 1000, 0); err == nil || err.Error() != `generation required` {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t})\n}\n\nfunc TestFindMinSnapshotByGeneration(t *testing.T) {\n\tinfos := []litestream.SnapshotInfo{\n\t\t{Generation: \"29cf4bced74e92ab\", Index: 0},\n\t\t{Generation: \"5dfeb4aa03232553\", Index: 24},\n\t}\n\tif got, want := litestream.FindMinSnapshotByGeneration(infos, \"29cf4bced74e92ab\"), &infos[0]; got != want {\n\t\tt.Fatalf(\"info=%#v, want %#v\", got, want)\n\t}\n}\n\nfunc MustDecodeHexString(s string) []byte {\n\tb, err := hex.DecodeString(s)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn b\n}\n"
        },
        {
          "name": "mock",
          "type": "tree",
          "content": null
        },
        {
          "name": "replica.go",
          "type": "blob",
          "size": 38.75,
          "content": "package litestream\n\nimport (\n\t\"context\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"hash/crc64\"\n\t\"io\"\n\t\"log/slog\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"filippo.io/age\"\n\t\"github.com/benbjohnson/litestream/internal\"\n\t\"github.com/pierrec/lz4/v4\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\n// Default replica settings.\nconst (\n\tDefaultSyncInterval           = 1 * time.Second\n\tDefaultRetention              = 24 * time.Hour\n\tDefaultRetentionCheckInterval = 1 * time.Hour\n)\n\n// Replica connects a database to a replication destination via a ReplicaClient.\n// The replica manages periodic synchronization and maintaining the current\n// replica position.\ntype Replica struct {\n\tdb   *DB\n\tname string\n\n\tmu  sync.RWMutex\n\tpos Pos // current replicated position\n\n\tmuf sync.Mutex\n\tf   *os.File // long-running file descriptor to avoid non-OFD lock issues\n\n\twg     sync.WaitGroup\n\tcancel func()\n\n\t// Client used to connect to the remote replica.\n\tClient ReplicaClient\n\n\t// Time between syncs with the shadow WAL.\n\tSyncInterval time.Duration\n\n\t// Frequency to create new snapshots.\n\tSnapshotInterval time.Duration\n\n\t// Time to keep snapshots and related WAL files.\n\t// Database is snapshotted after interval, if needed, and older WAL files are discarded.\n\tRetention time.Duration\n\n\t// Time between checks for retention.\n\tRetentionCheckInterval time.Duration\n\n\t// Time between validation checks.\n\tValidationInterval time.Duration\n\n\t// If true, replica monitors database for changes automatically.\n\t// Set to false if replica is being used synchronously (such as in tests).\n\tMonitorEnabled bool\n\n\t// Encryption identities and recipients\n\tAgeIdentities []age.Identity\n\tAgeRecipients []age.Recipient\n}\n\nfunc NewReplica(db *DB, name string) *Replica {\n\tr := &Replica{\n\t\tdb:     db,\n\t\tname:   name,\n\t\tcancel: func() {},\n\n\t\tSyncInterval:           DefaultSyncInterval,\n\t\tRetention:              DefaultRetention,\n\t\tRetentionCheckInterval: DefaultRetentionCheckInterval,\n\t\tMonitorEnabled:         true,\n\t}\n\n\treturn r\n}\n\n// Name returns the name of the replica.\nfunc (r *Replica) Name() string {\n\tif r.name == \"\" && r.Client != nil {\n\t\treturn r.Client.Type()\n\t}\n\treturn r.name\n}\n\n// Logger returns the DB sub-logger for this replica.\nfunc (r *Replica) Logger() *slog.Logger {\n\tlogger := slog.Default()\n\tif r.db != nil {\n\t\tlogger = r.db.Logger\n\t}\n\treturn logger.With(\"replica\", r.Name())\n}\n\n// DB returns a reference to the database the replica is attached to, if any.\nfunc (r *Replica) DB() *DB { return r.db }\n\n// Starts replicating in a background goroutine.\nfunc (r *Replica) Start(ctx context.Context) error {\n\t// Ignore if replica is being used sychronously.\n\tif !r.MonitorEnabled {\n\t\treturn nil\n\t}\n\n\t// Stop previous replication.\n\tr.Stop(false)\n\n\t// Wrap context with cancelation.\n\tctx, r.cancel = context.WithCancel(ctx)\n\n\t// Start goroutine to replicate data.\n\tr.wg.Add(4)\n\tgo func() { defer r.wg.Done(); r.monitor(ctx) }()\n\tgo func() { defer r.wg.Done(); r.retainer(ctx) }()\n\tgo func() { defer r.wg.Done(); r.snapshotter(ctx) }()\n\tgo func() { defer r.wg.Done(); r.validator(ctx) }()\n\n\treturn nil\n}\n\n// Stop cancels any outstanding replication and blocks until finished.\n//\n// Performing a hard stop will close the DB file descriptor which could release\n// locks on per-process locks. Hard stops should only be performed when\n// stopping the entire process.\nfunc (r *Replica) Stop(hard bool) (err error) {\n\tr.cancel()\n\tr.wg.Wait()\n\n\tr.muf.Lock()\n\tdefer r.muf.Unlock()\n\tif hard && r.f != nil {\n\t\tif e := r.f.Close(); e != nil && err == nil {\n\t\t\terr = e\n\t\t}\n\t}\n\treturn err\n}\n\n// Sync copies new WAL frames from the shadow WAL to the replica client.\nfunc (r *Replica) Sync(ctx context.Context) (err error) {\n\t// Clear last position if if an error occurs during sync.\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tr.mu.Lock()\n\t\t\tr.pos = Pos{}\n\t\t\tr.mu.Unlock()\n\t\t}\n\t}()\n\n\t// Find current position of database.\n\tdpos, err := r.db.Pos()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"cannot determine current generation: %w\", err)\n\t} else if dpos.IsZero() {\n\t\treturn fmt.Errorf(\"no generation, waiting for data\")\n\t}\n\tgeneration := dpos.Generation\n\n\tr.Logger().Debug(\"replica sync\", \"position\", dpos.String())\n\n\t// Create a new snapshot and update the current replica position if\n\t// the generation on the database has changed.\n\tif r.Pos().Generation != generation {\n\t\t// Create snapshot if no snapshots exist for generation.\n\t\tsnapshotN, err := r.snapshotN(ctx, generation)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t} else if snapshotN == 0 {\n\t\t\tif info, err := r.Snapshot(ctx); err != nil {\n\t\t\t\treturn err\n\t\t\t} else if info.Generation != generation {\n\t\t\t\treturn fmt.Errorf(\"generation changed during snapshot, exiting sync\")\n\t\t\t}\n\t\t}\n\n\t\tpos, err := r.calcPos(ctx, generation)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"cannot determine replica position: %s\", err)\n\t\t}\n\n\t\tr.Logger().Debug(\"replica sync: calc new pos\", \"position\", pos.String())\n\t\tr.mu.Lock()\n\t\tr.pos = pos\n\t\tr.mu.Unlock()\n\t}\n\n\t// Read all WAL files since the last position.\n\tfor {\n\t\tif err = r.syncWAL(ctx); err == io.EOF {\n\t\t\tbreak\n\t\t} else if err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (r *Replica) syncWAL(ctx context.Context) (err error) {\n\trd, err := r.db.ShadowWALReader(r.Pos())\n\tif err == io.EOF {\n\t\treturn err\n\t} else if err != nil {\n\t\treturn fmt.Errorf(\"replica wal reader: %w\", err)\n\t}\n\tdefer rd.Close()\n\n\t// Copy shadow WAL to client write via io.Pipe().\n\tpr, pw := io.Pipe()\n\tdefer func() { _ = pw.CloseWithError(err) }()\n\n\t// Obtain initial position from shadow reader.\n\t// It may have moved to the next index if previous position was at the end.\n\tpos := rd.Pos()\n\tinitialPos := pos\n\tstartTime := time.Now()\n\tvar bytesWritten int\n\n\tlogger := r.Logger()\n\tlogger.Info(\"write wal segment\", \"position\", initialPos.String())\n\n\t// Copy through pipe into client from the starting position.\n\tvar g errgroup.Group\n\tg.Go(func() error {\n\t\t_, err := r.Client.WriteWALSegment(ctx, pos, pr)\n\n\t\t// Always close pipe reader to signal writers.\n\t\tif e := pr.CloseWithError(err); err == nil {\n\t\t\treturn e\n\t\t}\n\n\t\treturn err\n\t})\n\n\tvar ew io.WriteCloser = pw\n\n\t// Add encryption if we have recipients.\n\tif len(r.AgeRecipients) > 0 {\n\t\tvar err error\n\t\tew, err = age.Encrypt(pw, r.AgeRecipients...)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer ew.Close()\n\t}\n\n\t// Wrap writer to LZ4 compress.\n\tzw := lz4.NewWriter(ew)\n\n\t// Track total WAL bytes written to replica client.\n\twalBytesCounter := replicaWALBytesCounterVec.WithLabelValues(r.db.Path(), r.Name())\n\n\t// Copy header if at offset zero.\n\tvar psalt uint64 // previous salt value\n\tif pos := rd.Pos(); pos.Offset == 0 {\n\t\tbuf := make([]byte, WALHeaderSize)\n\t\tif _, err := io.ReadFull(rd, buf); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tpsalt = binary.BigEndian.Uint64(buf[16:24])\n\n\t\tn, err := zw.Write(buf)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\twalBytesCounter.Add(float64(n))\n\t\tbytesWritten += n\n\t}\n\n\t// Copy frames.\n\tfor {\n\t\tpos := rd.Pos()\n\t\tassert(pos.Offset == frameAlign(pos.Offset, r.db.pageSize), \"shadow wal reader not frame aligned\")\n\n\t\tbuf := make([]byte, WALFrameHeaderSize+r.db.pageSize)\n\t\tif _, err := io.ReadFull(rd, buf); err == io.EOF {\n\t\t\tbreak\n\t\t} else if err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Verify salt matches the previous frame/header read.\n\t\tsalt := binary.BigEndian.Uint64(buf[8:16])\n\t\tif psalt != 0 && psalt != salt {\n\t\t\treturn fmt.Errorf(\"replica salt mismatch: %s\", pos.String())\n\t\t}\n\t\tpsalt = salt\n\n\t\tn, err := zw.Write(buf)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\twalBytesCounter.Add(float64(n))\n\t\tbytesWritten += n\n\t}\n\n\t// Flush LZ4 writer, encryption writer and close pipe.\n\tif err := zw.Close(); err != nil {\n\t\treturn err\n\t} else if err := ew.Close(); err != nil {\n\t\treturn err\n\t} else if err := pw.Close(); err != nil {\n\t\treturn err\n\t}\n\n\t// Wait for client to finish write.\n\tif err := g.Wait(); err != nil {\n\t\treturn fmt.Errorf(\"client write: %w\", err)\n\t}\n\n\t// Save last replicated position.\n\tr.mu.Lock()\n\tr.pos = rd.Pos()\n\tr.mu.Unlock()\n\n\t// Track current position\n\treplicaWALIndexGaugeVec.WithLabelValues(r.db.Path(), r.Name()).Set(float64(rd.Pos().Index))\n\treplicaWALOffsetGaugeVec.WithLabelValues(r.db.Path(), r.Name()).Set(float64(rd.Pos().Offset))\n\n\tlogger.Info(\"wal segment written\", \"position\", initialPos.String(), \"elapsed\", time.Since(startTime).String(), \"sz\", bytesWritten)\n\treturn nil\n}\n\n// snapshotN returns the number of snapshots for a generation.\nfunc (r *Replica) snapshotN(ctx context.Context, generation string) (int, error) {\n\titr, err := r.Client.Snapshots(ctx, generation)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tdefer itr.Close()\n\n\tvar n int\n\tfor itr.Next() {\n\t\tn++\n\t}\n\treturn n, itr.Close()\n}\n\n// calcPos returns the last position for the given generation.\nfunc (r *Replica) calcPos(ctx context.Context, generation string) (pos Pos, err error) {\n\t// Fetch last snapshot. Return error if no snapshots exist.\n\tsnapshot, err := r.maxSnapshot(ctx, generation)\n\tif err != nil {\n\t\treturn pos, fmt.Errorf(\"max snapshot: %w\", err)\n\t} else if snapshot == nil {\n\t\treturn pos, fmt.Errorf(\"no snapshot available: generation=%s\", generation)\n\t}\n\n\t// Determine last WAL segment available. Use snapshot if none exist.\n\tsegment, err := r.maxWALSegment(ctx, generation)\n\tif err != nil {\n\t\treturn pos, fmt.Errorf(\"max wal segment: %w\", err)\n\t} else if segment == nil {\n\t\treturn Pos{Generation: snapshot.Generation, Index: snapshot.Index}, nil\n\t}\n\n\t// Read segment to determine size to add to offset.\n\trd, err := r.Client.WALSegmentReader(ctx, segment.Pos())\n\tif err != nil {\n\t\treturn pos, fmt.Errorf(\"wal segment reader: %w\", err)\n\t}\n\tdefer rd.Close()\n\n\tif len(r.AgeIdentities) > 0 {\n\t\tdrd, err := age.Decrypt(rd, r.AgeIdentities...)\n\t\tif err != nil {\n\t\t\treturn pos, err\n\t\t}\n\n\t\trd = io.NopCloser(drd)\n\t}\n\n\tn, err := io.Copy(io.Discard, lz4.NewReader(rd))\n\tif err != nil {\n\t\treturn pos, err\n\t}\n\n\t// Return the position at the end of the last WAL segment.\n\treturn Pos{\n\t\tGeneration: segment.Generation,\n\t\tIndex:      segment.Index,\n\t\tOffset:     segment.Offset + n,\n\t}, nil\n}\n\n// maxSnapshot returns the last snapshot in a generation.\nfunc (r *Replica) maxSnapshot(ctx context.Context, generation string) (*SnapshotInfo, error) {\n\titr, err := r.Client.Snapshots(ctx, generation)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer itr.Close()\n\n\tvar max *SnapshotInfo\n\tfor itr.Next() {\n\t\tif info := itr.Snapshot(); max == nil || info.Index > max.Index {\n\t\t\tmax = &info\n\t\t}\n\t}\n\treturn max, itr.Close()\n}\n\n// maxWALSegment returns the highest WAL segment in a generation.\nfunc (r *Replica) maxWALSegment(ctx context.Context, generation string) (*WALSegmentInfo, error) {\n\titr, err := r.Client.WALSegments(ctx, generation)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer itr.Close()\n\n\tvar max *WALSegmentInfo\n\tfor itr.Next() {\n\t\tif info := itr.WALSegment(); max == nil || info.Index > max.Index || (info.Index == max.Index && info.Offset > max.Offset) {\n\t\t\tmax = &info\n\t\t}\n\t}\n\treturn max, itr.Close()\n}\n\n// Pos returns the current replicated position.\n// Returns a zero value if the current position cannot be determined.\nfunc (r *Replica) Pos() Pos {\n\tr.mu.RLock()\n\tdefer r.mu.RUnlock()\n\treturn r.pos\n}\n\n// Snapshots returns a list of all snapshots across all generations.\nfunc (r *Replica) Snapshots(ctx context.Context) ([]SnapshotInfo, error) {\n\tgenerations, err := r.Client.Generations(ctx)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"cannot fetch generations: %w\", err)\n\t}\n\n\tvar a []SnapshotInfo\n\tfor _, generation := range generations {\n\t\tif err := func() error {\n\t\t\titr, err := r.Client.Snapshots(ctx, generation)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tdefer itr.Close()\n\n\t\t\tother, err := SliceSnapshotIterator(itr)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\ta = append(a, other...)\n\n\t\t\treturn itr.Close()\n\t\t}(); err != nil {\n\t\t\treturn a, err\n\t\t}\n\t}\n\n\tsort.Sort(SnapshotInfoSlice(a))\n\n\treturn a, nil\n}\n\n// Snapshot copies the entire database to the replica path.\nfunc (r *Replica) Snapshot(ctx context.Context) (info SnapshotInfo, err error) {\n\tif r.db == nil || r.db.db == nil {\n\t\treturn info, fmt.Errorf(\"no database available\")\n\t}\n\n\tr.muf.Lock()\n\tdefer r.muf.Unlock()\n\n\t// Issue a passive checkpoint to flush any pages to disk before snapshotting.\n\tif err := r.db.Checkpoint(ctx, CheckpointModePassive); err != nil {\n\t\treturn info, fmt.Errorf(\"pre-snapshot checkpoint: %w\", err)\n\t}\n\n\t// Prevent internal checkpoints during snapshot.\n\tr.db.BeginSnapshot()\n\tdefer r.db.EndSnapshot()\n\n\t// Acquire a read lock on the database during snapshot to prevent external checkpoints.\n\ttx, err := r.db.db.Begin()\n\tif err != nil {\n\t\treturn info, err\n\t} else if _, err := tx.ExecContext(ctx, `SELECT COUNT(1) FROM _litestream_seq;`); err != nil {\n\t\t_ = tx.Rollback()\n\t\treturn info, err\n\t}\n\tdefer func() { _ = tx.Rollback() }()\n\n\t// Obtain current position.\n\tpos, err := r.db.Pos()\n\tif err != nil {\n\t\treturn info, fmt.Errorf(\"cannot determine db position: %w\", err)\n\t} else if pos.IsZero() {\n\t\treturn info, ErrNoGeneration\n\t}\n\n\t// Open db file descriptor, if not already open, & position at beginning.\n\tif r.f == nil {\n\t\tif r.f, err = os.Open(r.db.Path()); err != nil {\n\t\t\treturn info, err\n\t\t}\n\t}\n\tif _, err := r.f.Seek(0, io.SeekStart); err != nil {\n\t\treturn info, err\n\t}\n\n\t// Use a pipe to convert the LZ4 writer to a reader.\n\tpr, pw := io.Pipe()\n\n\t// Copy the database file to the LZ4 writer in a separate goroutine.\n\tvar g errgroup.Group\n\tg.Go(func() error {\n\t\t// We need to ensure the pipe is closed.\n\t\tdefer pw.Close()\n\n\t\tvar wc io.WriteCloser = pw\n\n\t\t// Add encryption if we have recipients.\n\t\tif len(r.AgeRecipients) > 0 {\n\t\t\tvar err error\n\t\t\twc, err = age.Encrypt(pw, r.AgeRecipients...)\n\t\t\tif err != nil {\n\t\t\t\tpw.CloseWithError(err)\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tdefer wc.Close()\n\t\t}\n\n\t\tzr := lz4.NewWriter(wc)\n\t\tdefer zr.Close()\n\n\t\tif _, err := io.Copy(zr, r.f); err != nil {\n\t\t\tpw.CloseWithError(err)\n\t\t\treturn err\n\t\t} else if err := zr.Close(); err != nil {\n\t\t\tpw.CloseWithError(err)\n\t\t\treturn err\n\t\t}\n\t\treturn wc.Close()\n\t})\n\n\tlogger := r.Logger()\n\tlogger.Info(\"write snapshot\", \"position\", pos.String())\n\n\tstartTime := time.Now()\n\t// Delegate write to client & wait for writer goroutine to finish.\n\tif info, err = r.Client.WriteSnapshot(ctx, pos.Generation, pos.Index, pr); err != nil {\n\t\treturn info, err\n\t} else if err := g.Wait(); err != nil {\n\t\treturn info, err\n\t}\n\n\tlogger.Info(\"snapshot written\", \"position\", pos.String(), \"elapsed\", time.Since(startTime).String(), \"sz\", info.Size)\n\treturn info, nil\n}\n\n// EnforceRetention forces a new snapshot once the retention interval has passed.\n// Older snapshots and WAL files are then removed.\nfunc (r *Replica) EnforceRetention(ctx context.Context) (err error) {\n\t// Obtain list of snapshots that are within the retention period.\n\tsnapshots, err := r.Snapshots(ctx)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"snapshots: %w\", err)\n\t}\n\tretained := FilterSnapshotsAfter(snapshots, time.Now().Add(-r.Retention))\n\n\t// If no retained snapshots exist, create a new snapshot.\n\tif len(retained) == 0 {\n\t\tsnapshot, err := r.Snapshot(ctx)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"snapshot: %w\", err)\n\t\t}\n\t\tretained = append(retained, snapshot)\n\t}\n\n\t// Loop over generations and delete unretained snapshots & WAL files.\n\tgenerations, err := r.Client.Generations(ctx)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"generations: %w\", err)\n\t}\n\tfor _, generation := range generations {\n\t\t// Find earliest retained snapshot for this generation.\n\t\tsnapshot := FindMinSnapshotByGeneration(retained, generation)\n\n\t\t// Delete entire generation if no snapshots are being retained.\n\t\tif snapshot == nil {\n\t\t\tif err := r.Client.DeleteGeneration(ctx, generation); err != nil {\n\t\t\t\treturn fmt.Errorf(\"delete generation: %w\", err)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// Otherwise remove all earlier snapshots & WAL segments.\n\t\tif err := r.deleteSnapshotsBeforeIndex(ctx, generation, snapshot.Index); err != nil {\n\t\t\treturn fmt.Errorf(\"delete snapshots before index: %w\", err)\n\t\t} else if err := r.deleteWALSegmentsBeforeIndex(ctx, generation, snapshot.Index); err != nil {\n\t\t\treturn fmt.Errorf(\"delete wal segments before index: %w\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (r *Replica) deleteSnapshotsBeforeIndex(ctx context.Context, generation string, index int) error {\n\titr, err := r.Client.Snapshots(ctx, generation)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"fetch snapshots: %w\", err)\n\t}\n\tdefer itr.Close()\n\n\tfor itr.Next() {\n\t\tinfo := itr.Snapshot()\n\t\tif info.Index >= index {\n\t\t\tcontinue\n\t\t}\n\n\t\tif err := r.Client.DeleteSnapshot(ctx, info.Generation, info.Index); err != nil {\n\t\t\treturn fmt.Errorf(\"delete snapshot %s/%08x: %w\", info.Generation, info.Index, err)\n\t\t}\n\t\tr.Logger().Info(\"snapshot deleted\", \"generation\", generation, \"index\", index)\n\t}\n\n\treturn itr.Close()\n}\n\nfunc (r *Replica) deleteWALSegmentsBeforeIndex(ctx context.Context, generation string, index int) error {\n\titr, err := r.Client.WALSegments(ctx, generation)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"fetch wal segments: %w\", err)\n\t}\n\tdefer itr.Close()\n\n\tvar a []Pos\n\tfor itr.Next() {\n\t\tinfo := itr.WALSegment()\n\t\tif info.Index >= index {\n\t\t\tcontinue\n\t\t}\n\t\ta = append(a, info.Pos())\n\t}\n\tif err := itr.Close(); err != nil {\n\t\treturn err\n\t}\n\n\tif len(a) == 0 {\n\t\treturn nil\n\t}\n\n\tif err := r.Client.DeleteWALSegments(ctx, a); err != nil {\n\t\treturn fmt.Errorf(\"delete wal segments: %w\", err)\n\t}\n\n\tr.Logger().Info(\"wal segmented deleted before\", \"generation\", generation, \"index\", index, \"n\", len(a))\n\treturn nil\n}\n\n// monitor runs in a separate goroutine and continuously replicates the DB.\nfunc (r *Replica) monitor(ctx context.Context) {\n\tticker := time.NewTicker(r.SyncInterval)\n\tdefer ticker.Stop()\n\n\t// Continuously check for new data to replicate.\n\tch := make(chan struct{})\n\tclose(ch)\n\tvar notify <-chan struct{} = ch\n\n\tfor initial := true; ; initial = false {\n\t\t// Enforce a minimum time between synchronization.\n\t\tif !initial {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\tcase <-ticker.C:\n\t\t\t}\n\t\t}\n\n\t\t// Wait for changes to the database.\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase <-notify:\n\t\t}\n\n\t\t// Fetch new notify channel before replicating data.\n\t\tnotify = r.db.Notify()\n\n\t\t// Synchronize the shadow wal into the replication directory.\n\t\tif err := r.Sync(ctx); err != nil {\n\t\t\tr.Logger().Error(\"monitor error\", \"error\", err)\n\t\t\tcontinue\n\t\t}\n\t}\n}\n\n// retainer runs in a separate goroutine and handles retention.\nfunc (r *Replica) retainer(ctx context.Context) {\n\t// Disable retention enforcement if retention period is non-positive.\n\tif r.Retention <= 0 {\n\t\treturn\n\t}\n\n\t// Ensure check interval is not longer than retention period.\n\tcheckInterval := r.RetentionCheckInterval\n\tif checkInterval > r.Retention {\n\t\tcheckInterval = r.Retention\n\t}\n\n\tticker := time.NewTicker(checkInterval)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\tif err := r.EnforceRetention(ctx); err != nil {\n\t\t\t\tr.Logger().Error(\"retainer error\", \"error\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n}\n\n// snapshotter runs in a separate goroutine and handles snapshotting.\nfunc (r *Replica) snapshotter(ctx context.Context) {\n\tif r.SnapshotInterval <= 0 {\n\t\treturn\n\t}\n\n\tlogger := r.Logger()\n\tif pos, err := r.db.Pos(); err != nil {\n\t\tlogger.Error(\"snapshotter cannot determine generation\", \"error\", err)\n\t} else if !pos.IsZero() {\n\t\tif snapshot, err := r.maxSnapshot(ctx, pos.Generation); err != nil {\n\t\t\tlogger.Error(\"snapshotter cannot determine latest snapshot\", \"error\", err)\n\t\t} else if snapshot != nil {\n\t\t\tnextSnapshot := r.SnapshotInterval - time.Since(snapshot.CreatedAt)\n\t\t\tif nextSnapshot < 0 {\n\t\t\t\tnextSnapshot = 0\n\t\t\t}\n\n\t\t\tlogger.Info(\"snapshot interval adjusted\", \"previous\", snapshot.CreatedAt.Format(time.RFC3339), \"next\", nextSnapshot.String())\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\tcase <-time.After(nextSnapshot):\n\t\t\t\tif _, err := r.Snapshot(ctx); err != nil && err != ErrNoGeneration {\n\t\t\t\t\tlogger.Error(\"snapshotter error\", \"error\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tticker := time.NewTicker(r.SnapshotInterval)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\tif _, err := r.Snapshot(ctx); err != nil && err != ErrNoGeneration {\n\t\t\t\tr.Logger().Error(\"snapshotter error\", \"error\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n}\n\n// validator runs in a separate goroutine and handles periodic validation.\nfunc (r *Replica) validator(ctx context.Context) {\n\t// Initialize counters since validation occurs infrequently.\n\tfor _, status := range []string{\"ok\", \"error\"} {\n\t\treplicaValidationTotalCounterVec.WithLabelValues(r.db.Path(), r.Name(), status).Add(0)\n\t}\n\n\t// Exit validation if interval is not set.\n\tif r.ValidationInterval <= 0 {\n\t\treturn\n\t}\n\n\tticker := time.NewTicker(r.ValidationInterval)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\tif err := r.Validate(ctx); err != nil {\n\t\t\t\tr.Logger().Error(\"validation error\", \"error\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Validate restores the most recent data from a replica and validates\n// that the resulting database matches the current database.\nfunc (r *Replica) Validate(ctx context.Context) error {\n\tdb := r.DB()\n\n\t// Restore replica to a temporary directory.\n\ttmpdir, err := os.MkdirTemp(\"\", \"*-litestream\")\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer os.RemoveAll(tmpdir)\n\n\t// Compute checksum of primary database under lock. This prevents a\n\t// sync from occurring and the database will not be written.\n\tchksum0, pos, err := db.CRC64(ctx)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"cannot compute checksum: %w\", err)\n\t}\n\n\t// Wait until replica catches up to position.\n\tif err := r.waitForReplica(ctx, pos); err != nil {\n\t\treturn fmt.Errorf(\"cannot wait for replica: %w\", err)\n\t}\n\n\trestorePath := filepath.Join(tmpdir, \"replica\")\n\tif err := r.Restore(ctx, RestoreOptions{\n\t\tOutputPath:  restorePath,\n\t\tReplicaName: r.Name(),\n\t\tGeneration:  pos.Generation,\n\t\tIndex:       pos.Index - 1,\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"cannot restore: %w\", err)\n\t}\n\n\t// Open file handle for restored database.\n\t// NOTE: This open is ok as the restored database is not managed by litestream.\n\tf, err := os.Open(restorePath)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close()\n\n\t// Read entire file into checksum.\n\th := crc64.New(crc64.MakeTable(crc64.ISO))\n\tif _, err := io.Copy(h, f); err != nil {\n\t\treturn err\n\t}\n\tchksum1 := h.Sum64()\n\n\tstatus := \"ok\"\n\tmismatch := chksum0 != chksum1\n\tif mismatch {\n\t\tstatus = \"mismatch\"\n\t}\n\tr.Logger().Info(\"validator\", \"status\", status, \"db\", fmt.Sprintf(\"%016x\", chksum0), \"replica\", fmt.Sprintf(\"%016x\", chksum1), \"position\", pos.String())\n\n\t// Validate checksums match.\n\tif mismatch {\n\t\treplicaValidationTotalCounterVec.WithLabelValues(r.db.Path(), r.Name(), \"error\").Inc()\n\t\treturn ErrChecksumMismatch\n\t}\n\n\treplicaValidationTotalCounterVec.WithLabelValues(r.db.Path(), r.Name(), \"ok\").Inc()\n\n\tif err := os.RemoveAll(tmpdir); err != nil {\n\t\treturn fmt.Errorf(\"cannot remove temporary validation directory: %w\", err)\n\t}\n\treturn nil\n}\n\n// waitForReplica blocks until replica reaches at least the given position.\nfunc (r *Replica) waitForReplica(ctx context.Context, pos Pos) error {\n\tticker := time.NewTicker(500 * time.Millisecond)\n\tdefer ticker.Stop()\n\n\ttimer := time.NewTicker(10 * time.Second)\n\tdefer ticker.Stop()\n\n\tonce := make(chan struct{}, 1)\n\tonce <- struct{}{}\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tcase <-timer.C:\n\t\t\treturn fmt.Errorf(\"replica wait exceeded timeout\")\n\t\tcase <-ticker.C:\n\t\tcase <-once: // immediate on first check\n\t\t}\n\n\t\t// Obtain current position of replica, check if past target position.\n\t\tcurr := r.Pos()\n\t\tif curr.IsZero() {\n\t\t\tr.Logger().Info(\"validator: no replica position available\")\n\t\t\tcontinue\n\t\t}\n\n\t\t// Exit if the generation has changed while waiting as there will be\n\t\t// no further progress on the old generation.\n\t\tif curr.Generation != pos.Generation {\n\t\t\treturn fmt.Errorf(\"generation changed\")\n\t\t}\n\n\t\tready := true\n\t\tif curr.Index < pos.Index {\n\t\t\tready = false\n\t\t} else if curr.Index == pos.Index && curr.Offset < pos.Offset {\n\t\t\tready = false\n\t\t}\n\n\t\t// If not ready, restart loop.\n\t\tif !ready {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Current position at or after target position.\n\t\treturn nil\n\t}\n}\n\n// GenerationCreatedAt returns the earliest creation time of any snapshot.\n// Returns zero time if no snapshots exist.\nfunc (r *Replica) GenerationCreatedAt(ctx context.Context, generation string) (time.Time, error) {\n\tvar min time.Time\n\n\titr, err := r.Client.Snapshots(ctx, generation)\n\tif err != nil {\n\t\treturn min, err\n\t}\n\tdefer itr.Close()\n\n\tfor itr.Next() {\n\t\tif info := itr.Snapshot(); min.IsZero() || info.CreatedAt.Before(min) {\n\t\t\tmin = info.CreatedAt\n\t\t}\n\t}\n\treturn min, itr.Close()\n}\n\n// GenerationTimeBounds returns the creation time & last updated time of a generation.\n// Returns zero time if no snapshots or WAL segments exist.\nfunc (r *Replica) GenerationTimeBounds(ctx context.Context, generation string) (createdAt, updatedAt time.Time, err error) {\n\t// Iterate over snapshots.\n\tsitr, err := r.Client.Snapshots(ctx, generation)\n\tif err != nil {\n\t\treturn createdAt, updatedAt, err\n\t}\n\tdefer sitr.Close()\n\n\tminIndex, maxIndex := -1, -1\n\tfor sitr.Next() {\n\t\tinfo := sitr.Snapshot()\n\t\tif createdAt.IsZero() || info.CreatedAt.Before(createdAt) {\n\t\t\tcreatedAt = info.CreatedAt\n\t\t}\n\t\tif updatedAt.IsZero() || info.CreatedAt.After(updatedAt) {\n\t\t\tupdatedAt = info.CreatedAt\n\t\t}\n\t\tif minIndex == -1 || info.Index < minIndex {\n\t\t\tminIndex = info.Index\n\t\t}\n\t\tif info.Index > maxIndex {\n\t\t\tmaxIndex = info.Index\n\t\t}\n\t}\n\tif err := sitr.Close(); err != nil {\n\t\treturn createdAt, updatedAt, err\n\t}\n\n\t// Iterate over WAL segments.\n\twitr, err := r.Client.WALSegments(ctx, generation)\n\tif err != nil {\n\t\treturn createdAt, updatedAt, err\n\t}\n\tdefer witr.Close()\n\n\tfor witr.Next() {\n\t\tinfo := witr.WALSegment()\n\t\tif info.Index < minIndex || info.Index > maxIndex {\n\t\t\tcontinue\n\t\t}\n\t\tif createdAt.IsZero() || info.CreatedAt.Before(createdAt) {\n\t\t\tcreatedAt = info.CreatedAt\n\t\t}\n\t\tif updatedAt.IsZero() || info.CreatedAt.After(updatedAt) {\n\t\t\tupdatedAt = info.CreatedAt\n\t\t}\n\t}\n\tif err := witr.Close(); err != nil {\n\t\treturn createdAt, updatedAt, err\n\t}\n\n\treturn createdAt, updatedAt, nil\n}\n\n// CalcRestoreTarget returns a generation to restore from.\nfunc (r *Replica) CalcRestoreTarget(ctx context.Context, opt RestoreOptions) (generation string, updatedAt time.Time, err error) {\n\tvar target struct {\n\t\tgeneration string\n\t\tupdatedAt  time.Time\n\t}\n\n\tgenerations, err := r.Client.Generations(ctx)\n\tif err != nil {\n\t\treturn \"\", time.Time{}, fmt.Errorf(\"cannot fetch generations: %w\", err)\n\t}\n\n\t// Search generations for one that contains the requested timestamp.\n\tfor _, generation := range generations {\n\t\t// Skip generation if it does not match filter.\n\t\tif opt.Generation != \"\" && generation != opt.Generation {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Determine the time bounds for the generation.\n\t\tcreatedAt, updatedAt, err := r.GenerationTimeBounds(ctx, generation)\n\t\tif err != nil {\n\t\t\treturn \"\", time.Time{}, fmt.Errorf(\"generation created at: %w\", err)\n\t\t}\n\n\t\t// Skip if it does not contain timestamp.\n\t\tif !opt.Timestamp.IsZero() {\n\t\t\tif opt.Timestamp.Before(createdAt) || opt.Timestamp.After(updatedAt) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// Use the latest replica if we have multiple candidates.\n\t\tif !updatedAt.After(target.updatedAt) {\n\t\t\tcontinue\n\t\t}\n\n\t\ttarget.generation = generation\n\t\ttarget.updatedAt = updatedAt\n\t}\n\n\treturn target.generation, target.updatedAt, nil\n}\n\n// Replica restores the database from a replica based on the options given.\n// This method will restore into opt.OutputPath, if specified, or into the\n// DB's original database path. It can optionally restore from a specific\n// replica or generation or it will automatically choose the best one. Finally,\n// a timestamp can be specified to restore the database to a specific\n// point-in-time.\nfunc (r *Replica) Restore(ctx context.Context, opt RestoreOptions) (err error) {\n\t// Validate options.\n\tif opt.OutputPath == \"\" {\n\t\treturn fmt.Errorf(\"output path required\")\n\t} else if opt.Generation == \"\" && opt.Index != math.MaxInt32 {\n\t\treturn fmt.Errorf(\"must specify generation when restoring to index\")\n\t} else if opt.Index != math.MaxInt32 && !opt.Timestamp.IsZero() {\n\t\treturn fmt.Errorf(\"cannot specify index & timestamp to restore\")\n\t}\n\n\t// Ensure output path does not already exist.\n\tif _, err := os.Stat(opt.OutputPath); err == nil {\n\t\treturn fmt.Errorf(\"cannot restore, output path already exists: %s\", opt.OutputPath)\n\t} else if err != nil && !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\n\t// Find lastest snapshot that occurs before timestamp or index.\n\tvar minWALIndex int\n\tif opt.Index < math.MaxInt32 {\n\t\tif minWALIndex, err = r.SnapshotIndexByIndex(ctx, opt.Generation, opt.Index); err != nil {\n\t\t\treturn fmt.Errorf(\"cannot find snapshot index: %w\", err)\n\t\t}\n\t} else {\n\t\tif minWALIndex, err = r.SnapshotIndexAt(ctx, opt.Generation, opt.Timestamp); err != nil {\n\t\t\treturn fmt.Errorf(\"cannot find snapshot index by timestamp: %w\", err)\n\t\t}\n\t}\n\n\t// Compute list of offsets for each WAL index.\n\twalSegmentMap, err := r.walSegmentMap(ctx, opt.Generation, minWALIndex, opt.Index, opt.Timestamp)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"cannot find max wal index for restore: %w\", err)\n\t}\n\n\t// Find the maximum WAL index that occurs before timestamp.\n\tmaxWALIndex := -1\n\tfor index := range walSegmentMap {\n\t\tif index > maxWALIndex {\n\t\t\tmaxWALIndex = index\n\t\t}\n\t}\n\n\t// Ensure that we found the specific index, if one was specified.\n\tif opt.Index != math.MaxInt32 && opt.Index != maxWALIndex {\n\t\treturn fmt.Errorf(\"unable to locate index %d in generation %q, highest index was %d\", opt.Index, opt.Generation, maxWALIndex)\n\t}\n\n\t// If no WAL files were found, mark this as a snapshot-only restore.\n\tsnapshotOnly := maxWALIndex == -1\n\n\t// Initialize starting position.\n\tpos := Pos{Generation: opt.Generation, Index: minWALIndex}\n\ttmpPath := opt.OutputPath + \".tmp\"\n\n\t// Copy snapshot to output path.\n\tr.Logger().Info(\"restoring snapshot\", \"generation\", opt.Generation, \"index\", minWALIndex, \"path\", tmpPath)\n\tif err := r.restoreSnapshot(ctx, pos.Generation, pos.Index, tmpPath); err != nil {\n\t\treturn fmt.Errorf(\"cannot restore snapshot: %w\", err)\n\t}\n\n\t// If no WAL files available, move snapshot to final path & exit early.\n\tif snapshotOnly {\n\t\tr.Logger().Info(\"snapshot only, finalizing database\")\n\t\treturn os.Rename(tmpPath, opt.OutputPath)\n\t}\n\n\t// Begin processing WAL files.\n\tr.Logger().Info(\"restoring wal files\", \"generation\", opt.Generation, \"index_min\", minWALIndex, \"index_max\", maxWALIndex)\n\n\t// Fill input channel with all WAL indexes to be loaded in order.\n\t// Verify every index has at least one offset.\n\tch := make(chan int, maxWALIndex-minWALIndex+1)\n\tfor index := minWALIndex; index <= maxWALIndex; index++ {\n\t\tif len(walSegmentMap[index]) == 0 {\n\t\t\treturn fmt.Errorf(\"missing WAL index: %s/%08x\", opt.Generation, index)\n\t\t}\n\t\tch <- index\n\t}\n\tclose(ch)\n\n\t// Track load state for each WAL.\n\tvar mu sync.Mutex\n\tcond := sync.NewCond(&mu)\n\twalStates := make([]walRestoreState, maxWALIndex-minWALIndex+1)\n\n\tparallelism := opt.Parallelism\n\tif parallelism < 1 {\n\t\tparallelism = 1\n\t}\n\n\t// Download WAL files to disk in parallel.\n\tg, ctx := errgroup.WithContext(ctx)\n\tfor i := 0; i < parallelism; i++ {\n\t\tg.Go(func() error {\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\tcond.Broadcast()\n\t\t\t\t\treturn err\n\t\t\t\tcase index, ok := <-ch:\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\tcond.Broadcast()\n\t\t\t\t\t\treturn nil\n\t\t\t\t\t}\n\n\t\t\t\t\tstartTime := time.Now()\n\n\t\t\t\t\terr := r.downloadWAL(ctx, opt.Generation, index, walSegmentMap[index], tmpPath)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\terr = fmt.Errorf(\"cannot download wal %s/%08x: %w\", opt.Generation, index, err)\n\t\t\t\t\t}\n\n\t\t\t\t\t// Mark index as ready-to-apply and notify applying code.\n\t\t\t\t\tmu.Lock()\n\t\t\t\t\twalStates[index-minWALIndex] = walRestoreState{ready: true, err: err}\n\t\t\t\t\tmu.Unlock()\n\t\t\t\t\tcond.Broadcast()\n\n\t\t\t\t\t// Returning the error here will cancel the other goroutines.\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\n\t\t\t\t\tr.Logger().Info(\"downloaded wal\",\n\t\t\t\t\t\t\"generation\", opt.Generation, \"index\", index,\n\t\t\t\t\t\t\"elapsed\", time.Since(startTime).String(),\n\t\t\t\t\t)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\n\t// Apply WAL files in order as they are ready.\n\tfor index := minWALIndex; index <= maxWALIndex; index++ {\n\t\t// Wait until next WAL file is ready to apply.\n\t\tmu.Lock()\n\t\tfor !walStates[index-minWALIndex].ready {\n\t\t\tif err := ctx.Err(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tcond.Wait()\n\t\t}\n\t\tif err := walStates[index-minWALIndex].err; err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmu.Unlock()\n\n\t\t// Apply WAL to database file.\n\t\tstartTime := time.Now()\n\t\tif err = applyWAL(ctx, index, tmpPath); err != nil {\n\t\t\treturn fmt.Errorf(\"cannot apply wal: %w\", err)\n\t\t}\n\t\tr.Logger().Info(\"applied wal\", \"generation\", opt.Generation, \"index\", index, \"elapsed\", time.Since(startTime).String())\n\t}\n\n\t// Ensure all goroutines finish. All errors should have been handled during\n\t// the processing of WAL files but this ensures that all processing is done.\n\tif err := g.Wait(); err != nil {\n\t\treturn err\n\t}\n\n\t// Copy file to final location.\n\tr.Logger().Info(\"renaming database from temporary location\")\n\tif err := os.Rename(tmpPath, opt.OutputPath); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\ntype walRestoreState struct {\n\tready bool\n\terr   error\n}\n\n// SnapshotIndexAt returns the highest index for a snapshot within a generation\n// that occurs before timestamp. If timestamp is zero, returns the latest snapshot.\nfunc (r *Replica) SnapshotIndexAt(ctx context.Context, generation string, timestamp time.Time) (int, error) {\n\titr, err := r.Client.Snapshots(ctx, generation)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tdefer itr.Close()\n\n\tsnapshotIndex := -1\n\tvar max time.Time\n\tfor itr.Next() {\n\t\tsnapshot := itr.Snapshot()\n\t\tif !timestamp.IsZero() && snapshot.CreatedAt.After(timestamp) {\n\t\t\tcontinue // after timestamp, skip\n\t\t}\n\n\t\t// Use snapshot if it newer.\n\t\tif max.IsZero() || snapshot.CreatedAt.After(max) {\n\t\t\tsnapshotIndex, max = snapshot.Index, snapshot.CreatedAt\n\t\t}\n\t}\n\tif err := itr.Close(); err != nil {\n\t\treturn 0, err\n\t} else if snapshotIndex == -1 {\n\t\treturn 0, ErrNoSnapshots\n\t}\n\treturn snapshotIndex, nil\n}\n\n// SnapshotIndexbyIndex returns the highest index for a snapshot within a generation\n// that occurs before a given index. If index is MaxInt32, returns the latest snapshot.\nfunc (r *Replica) SnapshotIndexByIndex(ctx context.Context, generation string, index int) (int, error) {\n\titr, err := r.Client.Snapshots(ctx, generation)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tdefer itr.Close()\n\n\tsnapshotIndex := -1\n\tfor itr.Next() {\n\t\tsnapshot := itr.Snapshot()\n\n\t\tif index < math.MaxInt32 && snapshot.Index > index {\n\t\t\tcontinue // after index, skip\n\t\t}\n\n\t\t// Use snapshot if it newer.\n\t\tif snapshotIndex == -1 || snapshot.Index >= snapshotIndex {\n\t\t\tsnapshotIndex = snapshot.Index\n\t\t}\n\t}\n\tif err := itr.Close(); err != nil {\n\t\treturn 0, err\n\t} else if snapshotIndex == -1 {\n\t\treturn 0, ErrNoSnapshots\n\t}\n\treturn snapshotIndex, nil\n}\n\n// walSegmentMap returns a map of WAL indices to their segments.\n// Filters by a max timestamp or a max index.\nfunc (r *Replica) walSegmentMap(ctx context.Context, generation string, minIndex, maxIndex int, maxTimestamp time.Time) (map[int][]int64, error) {\n\titr, err := r.Client.WALSegments(ctx, generation)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer itr.Close()\n\n\ta := []WALSegmentInfo{}\n\tfor itr.Next() {\n\t\ta = append(a, itr.WALSegment())\n\t}\n\n\tsort.Sort(WALSegmentInfoSlice(a))\n\n\tm := make(map[int][]int64)\n\tfor _, info := range a {\n\t\t// Exit if we go past the max timestamp or index.\n\t\tif !maxTimestamp.IsZero() && info.CreatedAt.After(maxTimestamp) {\n\t\t\tbreak // after max timestamp, skip\n\t\t} else if info.Index > maxIndex {\n\t\t\tbreak // after max index, skip\n\t\t} else if info.Index < minIndex {\n\t\t\tcontinue // before min index, continue\n\t\t}\n\n\t\t// Verify offsets are added in order.\n\t\toffsets := m[info.Index]\n\t\tif len(offsets) == 0 && info.Offset != 0 {\n\t\t\treturn nil, fmt.Errorf(\"missing initial wal segment: generation=%s index=%08x offset=%d\", generation, info.Index, info.Offset)\n\t\t} else if len(offsets) > 0 && offsets[len(offsets)-1] >= info.Offset {\n\t\t\treturn nil, fmt.Errorf(\"wal segments out of order: generation=%s index=%08x offsets=(%d,%d)\", generation, info.Index, offsets[len(offsets)-1], info.Offset)\n\t\t}\n\n\t\t// Append to the end of the WAL file.\n\t\tm[info.Index] = append(offsets, info.Offset)\n\t}\n\treturn m, itr.Close()\n}\n\n// restoreSnapshot copies a snapshot from the replica to a file.\nfunc (r *Replica) restoreSnapshot(ctx context.Context, generation string, index int, filename string) error {\n\t// Determine the user/group & mode based on the DB, if available.\n\tvar fileInfo, dirInfo os.FileInfo\n\tif db := r.DB(); db != nil {\n\t\tfileInfo, dirInfo = db.fileInfo, db.dirInfo\n\t}\n\n\tif err := internal.MkdirAll(filepath.Dir(filename), dirInfo); err != nil {\n\t\treturn err\n\t}\n\n\tf, err := internal.CreateFile(filename, fileInfo)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close()\n\n\trd, err := r.Client.SnapshotReader(ctx, generation, index)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer rd.Close()\n\n\tif len(r.AgeIdentities) > 0 {\n\t\tdrd, err := age.Decrypt(rd, r.AgeIdentities...)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\trd = io.NopCloser(drd)\n\t}\n\n\tif _, err := io.Copy(f, lz4.NewReader(rd)); err != nil {\n\t\treturn err\n\t} else if err := f.Sync(); err != nil {\n\t\treturn err\n\t}\n\treturn f.Close()\n}\n\n// downloadWAL copies a WAL file from the replica to a local copy next to the DB.\n// The WAL is later applied by applyWAL(). This function can be run in parallel\n// to download multiple WAL files simultaneously.\nfunc (r *Replica) downloadWAL(ctx context.Context, generation string, index int, offsets []int64, dbPath string) (err error) {\n\t// Determine the user/group & mode based on the DB, if available.\n\tvar fileInfo os.FileInfo\n\tif db := r.DB(); db != nil {\n\t\tfileInfo = db.fileInfo\n\t}\n\n\t// Open readers for every segment in the WAL file, in order.\n\tvar readers []io.Reader\n\tfor _, offset := range offsets {\n\t\trd, err := r.Client.WALSegmentReader(ctx, Pos{Generation: generation, Index: index, Offset: offset})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer rd.Close()\n\n\t\tif len(r.AgeIdentities) > 0 {\n\t\t\tdrd, err := age.Decrypt(rd, r.AgeIdentities...)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\trd = io.NopCloser(drd)\n\t\t}\n\n\t\treaders = append(readers, lz4.NewReader(rd))\n\t}\n\n\t// Open handle to destination WAL path.\n\tf, err := internal.CreateFile(fmt.Sprintf(\"%s-%08x-wal\", dbPath, index), fileInfo)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close()\n\n\t// Combine segments together and copy WAL to target path.\n\tif _, err := io.Copy(f, io.MultiReader(readers...)); err != nil {\n\t\treturn err\n\t} else if err := f.Close(); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Replica metrics.\nvar (\n\treplicaWALBytesCounterVec = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tNamespace: \"litestream\",\n\t\tSubsystem: \"replica\",\n\t\tName:      \"wal_bytes\",\n\t\tHelp:      \"The number wal bytes written\",\n\t}, []string{\"db\", \"name\"})\n\n\treplicaWALIndexGaugeVec = promauto.NewGaugeVec(prometheus.GaugeOpts{\n\t\tNamespace: \"litestream\",\n\t\tSubsystem: \"replica\",\n\t\tName:      \"wal_index\",\n\t\tHelp:      \"The current WAL index\",\n\t}, []string{\"db\", \"name\"})\n\n\treplicaWALOffsetGaugeVec = promauto.NewGaugeVec(prometheus.GaugeOpts{\n\t\tNamespace: \"litestream\",\n\t\tSubsystem: \"replica\",\n\t\tName:      \"wal_offset\",\n\t\tHelp:      \"The current WAL offset\",\n\t}, []string{\"db\", \"name\"})\n\n\treplicaValidationTotalCounterVec = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tNamespace: \"litestream\",\n\t\tSubsystem: \"replica\",\n\t\tName:      \"validation_total\",\n\t\tHelp:      \"The number of validations performed\",\n\t}, []string{\"db\", \"name\", \"status\"})\n)\n"
        },
        {
          "name": "replica_client.go",
          "type": "blob",
          "size": 1.943359375,
          "content": "package litestream\n\nimport (\n\t\"context\"\n\t\"io\"\n)\n\n// ReplicaClient represents client to connect to a Replica.\ntype ReplicaClient interface {\n\t// Returns the type of client.\n\tType() string\n\n\t// Returns a list of available generations. Order is undefined.\n\tGenerations(ctx context.Context) ([]string, error)\n\n\t// Deletes all snapshots & WAL segments within a generation.\n\tDeleteGeneration(ctx context.Context, generation string) error\n\n\t// Returns an iterator of all snapshots within a generation on the replica. Order is undefined.\n\tSnapshots(ctx context.Context, generation string) (SnapshotIterator, error)\n\n\t// Writes LZ4 compressed snapshot data to the replica at a given index\n\t// within a generation. Returns metadata for the snapshot.\n\tWriteSnapshot(ctx context.Context, generation string, index int, r io.Reader) (SnapshotInfo, error)\n\n\t// Deletes a snapshot with the given generation & index.\n\tDeleteSnapshot(ctx context.Context, generation string, index int) error\n\n\t// Returns a reader that contains LZ4 compressed snapshot data for a\n\t// given index within a generation. Returns an os.ErrNotFound error if\n\t// the snapshot does not exist.\n\tSnapshotReader(ctx context.Context, generation string, index int) (io.ReadCloser, error)\n\n\t// Returns an iterator of all WAL segments within a generation on the replica. Order is undefined.\n\tWALSegments(ctx context.Context, generation string) (WALSegmentIterator, error)\n\n\t// Writes an LZ4 compressed WAL segment at a given position.\n\t// Returns metadata for the written segment.\n\tWriteWALSegment(ctx context.Context, pos Pos, r io.Reader) (WALSegmentInfo, error)\n\n\t// Deletes one or more WAL segments at the given positions.\n\tDeleteWALSegments(ctx context.Context, a []Pos) error\n\n\t// Returns a reader that contains an LZ4 compressed WAL segment at a given\n\t// index/offset within a generation. Returns an os.ErrNotFound error if the\n\t// WAL segment does not exist.\n\tWALSegmentReader(ctx context.Context, pos Pos) (io.ReadCloser, error)\n}\n"
        },
        {
          "name": "replica_client_test.go",
          "type": "blob",
          "size": 18.78515625,
          "content": "package litestream_test\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/rand\"\n\t\"os\"\n\t\"path\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/benbjohnson/litestream\"\n\t\"github.com/benbjohnson/litestream/abs\"\n\t\"github.com/benbjohnson/litestream/file\"\n\t\"github.com/benbjohnson/litestream/gcs\"\n\t\"github.com/benbjohnson/litestream/s3\"\n\t\"github.com/benbjohnson/litestream/sftp\"\n)\n\nvar (\n\t// Enables integration tests.\n\tintegration = flag.String(\"integration\", \"file\", \"\")\n)\n\n// S3 settings\nvar (\n\t// Replica client settings\n\ts3AccessKeyID     = flag.String(\"s3-access-key-id\", os.Getenv(\"LITESTREAM_S3_ACCESS_KEY_ID\"), \"\")\n\ts3SecretAccessKey = flag.String(\"s3-secret-access-key\", os.Getenv(\"LITESTREAM_S3_SECRET_ACCESS_KEY\"), \"\")\n\ts3Region          = flag.String(\"s3-region\", os.Getenv(\"LITESTREAM_S3_REGION\"), \"\")\n\ts3Bucket          = flag.String(\"s3-bucket\", os.Getenv(\"LITESTREAM_S3_BUCKET\"), \"\")\n\ts3Path            = flag.String(\"s3-path\", os.Getenv(\"LITESTREAM_S3_PATH\"), \"\")\n\ts3Endpoint        = flag.String(\"s3-endpoint\", os.Getenv(\"LITESTREAM_S3_ENDPOINT\"), \"\")\n\ts3ForcePathStyle  = flag.Bool(\"s3-force-path-style\", os.Getenv(\"LITESTREAM_S3_FORCE_PATH_STYLE\") == \"true\", \"\")\n\ts3SkipVerify      = flag.Bool(\"s3-skip-verify\", os.Getenv(\"LITESTREAM_S3_SKIP_VERIFY\") == \"true\", \"\")\n)\n\n// Google cloud storage settings\nvar (\n\tgcsBucket = flag.String(\"gcs-bucket\", os.Getenv(\"LITESTREAM_GCS_BUCKET\"), \"\")\n\tgcsPath   = flag.String(\"gcs-path\", os.Getenv(\"LITESTREAM_GCS_PATH\"), \"\")\n)\n\n// Azure blob storage settings\nvar (\n\tabsAccountName = flag.String(\"abs-account-name\", os.Getenv(\"LITESTREAM_ABS_ACCOUNT_NAME\"), \"\")\n\tabsAccountKey  = flag.String(\"abs-account-key\", os.Getenv(\"LITESTREAM_ABS_ACCOUNT_KEY\"), \"\")\n\tabsBucket      = flag.String(\"abs-bucket\", os.Getenv(\"LITESTREAM_ABS_BUCKET\"), \"\")\n\tabsPath        = flag.String(\"abs-path\", os.Getenv(\"LITESTREAM_ABS_PATH\"), \"\")\n)\n\n// SFTP settings\nvar (\n\tsftpHost     = flag.String(\"sftp-host\", os.Getenv(\"LITESTREAM_SFTP_HOST\"), \"\")\n\tsftpUser     = flag.String(\"sftp-user\", os.Getenv(\"LITESTREAM_SFTP_USER\"), \"\")\n\tsftpPassword = flag.String(\"sftp-password\", os.Getenv(\"LITESTREAM_SFTP_PASSWORD\"), \"\")\n\tsftpKeyPath  = flag.String(\"sftp-key-path\", os.Getenv(\"LITESTREAM_SFTP_KEY_PATH\"), \"\")\n\tsftpPath     = flag.String(\"sftp-path\", os.Getenv(\"LITESTREAM_SFTP_PATH\"), \"\")\n)\n\nfunc TestReplicaClient_Generations(t *testing.T) {\n\tRunWithReplicaClient(t, \"OK\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\t// Write snapshots.\n\t\tif _, err := c.WriteSnapshot(context.Background(), \"5efbd8d042012dca\", 0, strings.NewReader(`foo`)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.WriteSnapshot(context.Background(), \"b16ddcf5c697540f\", 0, strings.NewReader(`bar`)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.WriteSnapshot(context.Background(), \"155fe292f8333c72\", 0, strings.NewReader(`baz`)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Fetch and sort generations.\n\t\tgot, err := c.Generations(context.Background())\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tsort.Strings(got)\n\n\t\tif want := []string{\"155fe292f8333c72\", \"5efbd8d042012dca\", \"b16ddcf5c697540f\"}; !reflect.DeepEqual(got, want) {\n\t\t\tt.Fatalf(\"Generations()=%v, want %v\", got, want)\n\t\t}\n\t})\n\n\tRunWithReplicaClient(t, \"NoGenerationsDir\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\tif generations, err := c.Generations(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := len(generations), 0; got != want {\n\t\t\tt.Fatalf(\"len(Generations())=%v, want %v\", got, want)\n\t\t}\n\t})\n}\n\nfunc TestReplicaClient_Snapshots(t *testing.T) {\n\tRunWithReplicaClient(t, \"OK\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\t// Write snapshots.\n\t\tif _, err := c.WriteSnapshot(context.Background(), \"5efbd8d042012dca\", 1, strings.NewReader(``)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.WriteSnapshot(context.Background(), \"b16ddcf5c697540f\", 5, strings.NewReader(`x`)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.WriteSnapshot(context.Background(), \"b16ddcf5c697540f\", 10, strings.NewReader(`xyz`)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Fetch all snapshots by generation.\n\t\titr, err := c.Snapshots(context.Background(), \"b16ddcf5c697540f\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tdefer itr.Close()\n\n\t\t// Read all snapshots into a slice so they can be sorted.\n\t\ta, err := litestream.SliceSnapshotIterator(itr)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := len(a), 2; got != want {\n\t\t\tt.Fatalf(\"len=%v, want %v\", got, want)\n\t\t}\n\t\tsort.Sort(litestream.SnapshotInfoSlice(a))\n\n\t\t// Verify first snapshot metadata.\n\t\tif got, want := a[0].Generation, \"b16ddcf5c697540f\"; got != want {\n\t\t\tt.Fatalf(\"Generation=%v, want %v\", got, want)\n\t\t} else if got, want := a[0].Index, 5; got != want {\n\t\t\tt.Fatalf(\"Index=%v, want %v\", got, want)\n\t\t} else if got, want := a[0].Size, int64(1); got != want {\n\t\t\tt.Fatalf(\"Size=%v, want %v\", got, want)\n\t\t} else if a[0].CreatedAt.IsZero() {\n\t\t\tt.Fatalf(\"expected CreatedAt\")\n\t\t}\n\n\t\t// Verify second snapshot metadata.\n\t\tif got, want := a[1].Generation, \"b16ddcf5c697540f\"; got != want {\n\t\t\tt.Fatalf(\"Generation=%v, want %v\", got, want)\n\t\t} else if got, want := a[1].Index, 0xA; got != want {\n\t\t\tt.Fatalf(\"Index=%v, want %v\", got, want)\n\t\t} else if got, want := a[1].Size, int64(3); got != want {\n\t\t\tt.Fatalf(\"Size=%v, want %v\", got, want)\n\t\t} else if a[1].CreatedAt.IsZero() {\n\t\t\tt.Fatalf(\"expected CreatedAt\")\n\t\t}\n\n\t\t// Ensure close is clean.\n\t\tif err := itr.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n\n\tRunWithReplicaClient(t, \"NoGenerationDir\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\titr, err := c.Snapshots(context.Background(), \"5efbd8d042012dca\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tdefer itr.Close()\n\n\t\tif itr.Next() {\n\t\t\tt.Fatal(\"expected no snapshots\")\n\t\t}\n\t})\n\n\tRunWithReplicaClient(t, \"ErrNoGeneration\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\titr, err := c.Snapshots(context.Background(), \"\")\n\t\tif err == nil {\n\t\t\terr = itr.Close()\n\t\t}\n\t\tif err == nil || err.Error() != `cannot determine snapshots path: generation required` {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t})\n}\n\nfunc TestReplicaClient_WriteSnapshot(t *testing.T) {\n\tRunWithReplicaClient(t, \"OK\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\tif _, err := c.WriteSnapshot(context.Background(), \"b16ddcf5c697540f\", 1000, strings.NewReader(`foobar`)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif r, err := c.SnapshotReader(context.Background(), \"b16ddcf5c697540f\", 1000); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if buf, err := io.ReadAll(r); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := r.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := string(buf), `foobar`; got != want {\n\t\t\tt.Fatalf(\"data=%q, want %q\", got, want)\n\t\t}\n\t})\n\n\tRunWithReplicaClient(t, \"ErrNoGeneration\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\t\tif _, err := c.WriteSnapshot(context.Background(), \"\", 0, nil); err == nil || err.Error() != `cannot determine snapshot path: generation required` {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t})\n}\n\nfunc TestReplicaClient_SnapshotReader(t *testing.T) {\n\tRunWithReplicaClient(t, \"OK\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\tif _, err := c.WriteSnapshot(context.Background(), \"5efbd8d042012dca\", 10, strings.NewReader(`foo`)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tr, err := c.SnapshotReader(context.Background(), \"5efbd8d042012dca\", 10)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tdefer r.Close()\n\n\t\tif buf, err := io.ReadAll(r); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := string(buf), \"foo\"; got != want {\n\t\t\tt.Fatalf(\"ReadAll=%v, want %v\", got, want)\n\t\t}\n\t})\n\n\tRunWithReplicaClient(t, \"ErrNotFound\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\tif _, err := c.SnapshotReader(context.Background(), \"5efbd8d042012dca\", 1); !os.IsNotExist(err) {\n\t\t\tt.Fatalf(\"expected not exist, got %#v\", err)\n\t\t}\n\t})\n\n\tRunWithReplicaClient(t, \"ErrNoGeneration\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\tif _, err := c.SnapshotReader(context.Background(), \"\", 1); err == nil || err.Error() != `cannot determine snapshot path: generation required` {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t})\n}\n\nfunc TestReplicaClient_WALs(t *testing.T) {\n\tRunWithReplicaClient(t, \"OK\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\tif _, err := c.WriteWALSegment(context.Background(), litestream.Pos{Generation: \"5efbd8d042012dca\", Index: 1, Offset: 0}, strings.NewReader(``)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif _, err := c.WriteWALSegment(context.Background(), litestream.Pos{Generation: \"b16ddcf5c697540f\", Index: 2, Offset: 0}, strings.NewReader(`12345`)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.WriteWALSegment(context.Background(), litestream.Pos{Generation: \"b16ddcf5c697540f\", Index: 2, Offset: 5}, strings.NewReader(`67`)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.WriteWALSegment(context.Background(), litestream.Pos{Generation: \"b16ddcf5c697540f\", Index: 3, Offset: 0}, strings.NewReader(`xyz`)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\titr, err := c.WALSegments(context.Background(), \"b16ddcf5c697540f\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tdefer itr.Close()\n\n\t\t// Read all WAL segment files into a slice so they can be sorted.\n\t\ta, err := litestream.SliceWALSegmentIterator(itr)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := len(a), 3; got != want {\n\t\t\tt.Fatalf(\"len=%v, want %v\", got, want)\n\t\t}\n\t\tsort.Sort(litestream.WALSegmentInfoSlice(a))\n\n\t\t// Verify first WAL segment metadata.\n\t\tif got, want := a[0].Generation, \"b16ddcf5c697540f\"; got != want {\n\t\t\tt.Fatalf(\"Generation=%v, want %v\", got, want)\n\t\t} else if got, want := a[0].Index, 2; got != want {\n\t\t\tt.Fatalf(\"Index=%v, want %v\", got, want)\n\t\t} else if got, want := a[0].Offset, int64(0); got != want {\n\t\t\tt.Fatalf(\"Offset=%v, want %v\", got, want)\n\t\t} else if got, want := a[0].Size, int64(5); got != want {\n\t\t\tt.Fatalf(\"Size=%v, want %v\", got, want)\n\t\t} else if a[0].CreatedAt.IsZero() {\n\t\t\tt.Fatalf(\"expected CreatedAt\")\n\t\t}\n\n\t\t// Verify first WAL segment metadata.\n\t\tif got, want := a[1].Generation, \"b16ddcf5c697540f\"; got != want {\n\t\t\tt.Fatalf(\"Generation=%v, want %v\", got, want)\n\t\t} else if got, want := a[1].Index, 2; got != want {\n\t\t\tt.Fatalf(\"Index=%v, want %v\", got, want)\n\t\t} else if got, want := a[1].Offset, int64(5); got != want {\n\t\t\tt.Fatalf(\"Offset=%v, want %v\", got, want)\n\t\t} else if got, want := a[1].Size, int64(2); got != want {\n\t\t\tt.Fatalf(\"Size=%v, want %v\", got, want)\n\t\t} else if a[1].CreatedAt.IsZero() {\n\t\t\tt.Fatalf(\"expected CreatedAt\")\n\t\t}\n\n\t\t// Verify third WAL segment metadata.\n\t\tif got, want := a[2].Generation, \"b16ddcf5c697540f\"; got != want {\n\t\t\tt.Fatalf(\"Generation=%v, want %v\", got, want)\n\t\t} else if got, want := a[2].Index, 3; got != want {\n\t\t\tt.Fatalf(\"Index=%v, want %v\", got, want)\n\t\t} else if got, want := a[2].Offset, int64(0); got != want {\n\t\t\tt.Fatalf(\"Offset=%v, want %v\", got, want)\n\t\t} else if got, want := a[2].Size, int64(3); got != want {\n\t\t\tt.Fatalf(\"Size=%v, want %v\", got, want)\n\t\t} else if a[1].CreatedAt.IsZero() {\n\t\t\tt.Fatalf(\"expected CreatedAt\")\n\t\t}\n\n\t\t// Ensure close is clean.\n\t\tif err := itr.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n\n\tRunWithReplicaClient(t, \"NoGenerationDir\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\titr, err := c.WALSegments(context.Background(), \"5efbd8d042012dca\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tdefer itr.Close()\n\n\t\tif itr.Next() {\n\t\t\tt.Fatal(\"expected no wal files\")\n\t\t}\n\t})\n\n\tRunWithReplicaClient(t, \"NoWALs\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\tif _, err := c.WriteSnapshot(context.Background(), \"5efbd8d042012dca\", 0, strings.NewReader(`foo`)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\titr, err := c.WALSegments(context.Background(), \"5efbd8d042012dca\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tdefer itr.Close()\n\n\t\tif itr.Next() {\n\t\t\tt.Fatal(\"expected no wal files\")\n\t\t}\n\t})\n\n\tRunWithReplicaClient(t, \"ErrNoGeneration\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\titr, err := c.WALSegments(context.Background(), \"\")\n\t\tif err == nil {\n\t\t\terr = itr.Close()\n\t\t}\n\t\tif err == nil || err.Error() != `cannot determine wal path: generation required` {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t})\n}\n\nfunc TestReplicaClient_WriteWALSegment(t *testing.T) {\n\tRunWithReplicaClient(t, \"OK\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\tif _, err := c.WriteWALSegment(context.Background(), litestream.Pos{Generation: \"b16ddcf5c697540f\", Index: 1000, Offset: 2000}, strings.NewReader(`foobar`)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif r, err := c.WALSegmentReader(context.Background(), litestream.Pos{Generation: \"b16ddcf5c697540f\", Index: 1000, Offset: 2000}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if buf, err := io.ReadAll(r); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := r.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := string(buf), `foobar`; got != want {\n\t\t\tt.Fatalf(\"data=%q, want %q\", got, want)\n\t\t}\n\t})\n\n\tRunWithReplicaClient(t, \"ErrNoGeneration\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\t\tif _, err := c.WriteWALSegment(context.Background(), litestream.Pos{Generation: \"\", Index: 0, Offset: 0}, nil); err == nil || err.Error() != `cannot determine wal segment path: generation required` {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t})\n}\n\nfunc TestReplicaClient_WALReader(t *testing.T) {\n\n\tRunWithReplicaClient(t, \"OK\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\t\tif _, err := c.WriteWALSegment(context.Background(), litestream.Pos{Generation: \"5efbd8d042012dca\", Index: 10, Offset: 5}, strings.NewReader(`foobar`)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tr, err := c.WALSegmentReader(context.Background(), litestream.Pos{Generation: \"5efbd8d042012dca\", Index: 10, Offset: 5})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tdefer r.Close()\n\n\t\tif buf, err := io.ReadAll(r); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := string(buf), \"foobar\"; got != want {\n\t\t\tt.Fatalf(\"ReadAll=%v, want %v\", got, want)\n\t\t}\n\t})\n\n\tRunWithReplicaClient(t, \"ErrNotFound\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\tif _, err := c.WALSegmentReader(context.Background(), litestream.Pos{Generation: \"5efbd8d042012dca\", Index: 1, Offset: 0}); !os.IsNotExist(err) {\n\t\t\tt.Fatalf(\"expected not exist, got %#v\", err)\n\t\t}\n\t})\n}\n\nfunc TestReplicaClient_DeleteWALSegments(t *testing.T) {\n\tRunWithReplicaClient(t, \"OK\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\n\t\tif _, err := c.WriteWALSegment(context.Background(), litestream.Pos{Generation: \"b16ddcf5c697540f\", Index: 1, Offset: 2}, strings.NewReader(`foo`)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.WriteWALSegment(context.Background(), litestream.Pos{Generation: \"5efbd8d042012dca\", Index: 3, Offset: 4}, strings.NewReader(`bar`)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif err := c.DeleteWALSegments(context.Background(), []litestream.Pos{\n\t\t\t{Generation: \"b16ddcf5c697540f\", Index: 1, Offset: 2},\n\t\t\t{Generation: \"5efbd8d042012dca\", Index: 3, Offset: 4},\n\t\t}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := c.WALSegmentReader(context.Background(), litestream.Pos{Generation: \"b16ddcf5c697540f\", Index: 1, Offset: 2}); !os.IsNotExist(err) {\n\t\t\tt.Fatalf(\"expected not exist, got %#v\", err)\n\t\t} else if _, err := c.WALSegmentReader(context.Background(), litestream.Pos{Generation: \"5efbd8d042012dca\", Index: 3, Offset: 4}); !os.IsNotExist(err) {\n\t\t\tt.Fatalf(\"expected not exist, got %#v\", err)\n\t\t}\n\t})\n\n\tRunWithReplicaClient(t, \"ErrNoGeneration\", func(t *testing.T, c litestream.ReplicaClient) {\n\t\tt.Parallel()\n\t\tif err := c.DeleteWALSegments(context.Background(), []litestream.Pos{{}}); err == nil || err.Error() != `cannot determine wal segment path: generation required` {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t})\n}\n\n// RunWithReplicaClient executes fn with each replica specified by the -integration flag\nfunc RunWithReplicaClient(t *testing.T, name string, fn func(*testing.T, litestream.ReplicaClient)) {\n\tt.Run(name, func(t *testing.T) {\n\t\tfor _, typ := range strings.Split(*integration, \",\") {\n\t\t\tt.Run(typ, func(t *testing.T) {\n\t\t\t\tc := NewReplicaClient(t, typ)\n\t\t\t\tdefer MustDeleteAll(t, c)\n\n\t\t\t\tfn(t, c)\n\t\t\t})\n\t\t}\n\t})\n}\n\n// NewReplicaClient returns a new client for integration testing by type name.\nfunc NewReplicaClient(tb testing.TB, typ string) litestream.ReplicaClient {\n\ttb.Helper()\n\n\tswitch typ {\n\tcase file.ReplicaClientType:\n\t\treturn NewFileReplicaClient(tb)\n\tcase s3.ReplicaClientType:\n\t\treturn NewS3ReplicaClient(tb)\n\tcase gcs.ReplicaClientType:\n\t\treturn NewGCSReplicaClient(tb)\n\tcase abs.ReplicaClientType:\n\t\treturn NewABSReplicaClient(tb)\n\tcase sftp.ReplicaClientType:\n\t\treturn NewSFTPReplicaClient(tb)\n\tdefault:\n\t\ttb.Fatalf(\"invalid replica client type: %q\", typ)\n\t\treturn nil\n\t}\n}\n\n// NewFileReplicaClient returns a new client for integration testing.\nfunc NewFileReplicaClient(tb testing.TB) *file.ReplicaClient {\n\ttb.Helper()\n\treturn file.NewReplicaClient(tb.TempDir())\n}\n\n// NewS3ReplicaClient returns a new client for integration testing.\nfunc NewS3ReplicaClient(tb testing.TB) *s3.ReplicaClient {\n\ttb.Helper()\n\n\tc := s3.NewReplicaClient()\n\tc.AccessKeyID = *s3AccessKeyID\n\tc.SecretAccessKey = *s3SecretAccessKey\n\tc.Region = *s3Region\n\tc.Bucket = *s3Bucket\n\tc.Path = path.Join(*s3Path, fmt.Sprintf(\"%016x\", rand.Uint64()))\n\tc.Endpoint = *s3Endpoint\n\tc.ForcePathStyle = *s3ForcePathStyle\n\tc.SkipVerify = *s3SkipVerify\n\treturn c\n}\n\n// NewGCSReplicaClient returns a new client for integration testing.\nfunc NewGCSReplicaClient(tb testing.TB) *gcs.ReplicaClient {\n\ttb.Helper()\n\n\tc := gcs.NewReplicaClient()\n\tc.Bucket = *gcsBucket\n\tc.Path = path.Join(*gcsPath, fmt.Sprintf(\"%016x\", rand.Uint64()))\n\treturn c\n}\n\n// NewABSReplicaClient returns a new client for integration testing.\nfunc NewABSReplicaClient(tb testing.TB) *abs.ReplicaClient {\n\ttb.Helper()\n\n\tc := abs.NewReplicaClient()\n\tc.AccountName = *absAccountName\n\tc.AccountKey = *absAccountKey\n\tc.Bucket = *absBucket\n\tc.Path = path.Join(*absPath, fmt.Sprintf(\"%016x\", rand.Uint64()))\n\treturn c\n}\n\n// NewSFTPReplicaClient returns a new client for integration testing.\nfunc NewSFTPReplicaClient(tb testing.TB) *sftp.ReplicaClient {\n\ttb.Helper()\n\n\tc := sftp.NewReplicaClient()\n\tc.Host = *sftpHost\n\tc.User = *sftpUser\n\tc.Password = *sftpPassword\n\tc.KeyPath = *sftpKeyPath\n\tc.Path = path.Join(*sftpPath, fmt.Sprintf(\"%016x\", rand.Uint64()))\n\treturn c\n}\n\n// MustDeleteAll deletes all objects under the client's path.\nfunc MustDeleteAll(tb testing.TB, c litestream.ReplicaClient) {\n\ttb.Helper()\n\n\tgenerations, err := c.Generations(context.Background())\n\tif err != nil {\n\t\ttb.Fatalf(\"cannot list generations for deletion: %s\", err)\n\t}\n\n\tfor _, generation := range generations {\n\t\tif err := c.DeleteGeneration(context.Background(), generation); err != nil {\n\t\t\ttb.Fatalf(\"cannot delete generation: %s\", err)\n\t\t}\n\t}\n\n\tswitch c := c.(type) {\n\tcase *sftp.ReplicaClient:\n\t\tif err := c.Cleanup(context.Background()); err != nil {\n\t\t\ttb.Fatalf(\"cannot cleanup sftp: %s\", err)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "replica_test.go",
          "type": "blob",
          "size": 5.24609375,
          "content": "package litestream_test\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"io\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/benbjohnson/litestream\"\n\t\"github.com/benbjohnson/litestream/file\"\n\t\"github.com/benbjohnson/litestream/mock\"\n\t\"github.com/pierrec/lz4/v4\"\n)\n\nfunc nextIndex(pos litestream.Pos) litestream.Pos {\n\treturn litestream.Pos{\n\t\tGeneration: pos.Generation,\n\t\tIndex:      pos.Index + 1,\n\t}\n}\n\nfunc TestReplica_Name(t *testing.T) {\n\tt.Run(\"WithName\", func(t *testing.T) {\n\t\tif got, want := litestream.NewReplica(nil, \"NAME\").Name(), \"NAME\"; got != want {\n\t\t\tt.Fatalf(\"Name()=%v, want %v\", got, want)\n\t\t}\n\t})\n\tt.Run(\"WithoutName\", func(t *testing.T) {\n\t\tr := litestream.NewReplica(nil, \"\")\n\t\tr.Client = &mock.ReplicaClient{}\n\t\tif got, want := r.Name(), \"mock\"; got != want {\n\t\t\tt.Fatalf(\"Name()=%v, want %v\", got, want)\n\t\t}\n\t})\n}\n\nfunc TestReplica_Sync(t *testing.T) {\n\tdb, sqldb := MustOpenDBs(t)\n\tdefer MustCloseDBs(t, db, sqldb)\n\n\t// Issue initial database sync to setup generation.\n\tif err := db.Sync(context.Background()); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Fetch current database position.\n\tdpos, err := db.Pos()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tc := file.NewReplicaClient(t.TempDir())\n\tr := litestream.NewReplica(db, \"\")\n\tc.Replica, r.Client = r, c\n\n\tif err := r.Sync(context.Background()); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Verify client generation matches database.\n\tgenerations, err := c.Generations(context.Background())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if got, want := len(generations), 1; got != want {\n\t\tt.Fatalf(\"len(generations)=%v, want %v\", got, want)\n\t} else if got, want := generations[0], dpos.Generation; got != want {\n\t\tt.Fatalf(\"generations[0]=%v, want %v\", got, want)\n\t}\n\n\t// Verify we synced checkpoint page to WAL.\n\tif r, err := c.WALSegmentReader(context.Background(), nextIndex(dpos)); err != nil {\n\t\tt.Fatal(err)\n\t} else if b, err := io.ReadAll(lz4.NewReader(r)); err != nil {\n\t\tt.Fatal(err)\n\t} else if err := r.Close(); err != nil {\n\t\tt.Fatal(err)\n\t} else if len(b) == db.PageSize() {\n\t\tt.Fatalf(\"wal mismatch: len(%d), len(%d)\", len(b), db.PageSize())\n\t}\n\n\t// Reset WAL so the next write will only write out the segment we are checking.\n\tif err := db.Checkpoint(context.Background(), litestream.CheckpointModeTruncate); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Execute a query to write something into the truncated WAL.\n\tif _, err := sqldb.Exec(`CREATE TABLE foo (bar TEXT);`); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Sync database to catch up the shadow WAL.\n\tif err := db.Sync(context.Background()); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Save position after sync, it should be after our write.\n\tdpos, err = db.Pos()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Sync WAL segment out to replica.\n\tif err := r.Sync(context.Background()); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Verify WAL matches replica WAL.\n\tif b0, err := os.ReadFile(db.Path() + \"-wal\"); err != nil {\n\t\tt.Fatal(err)\n\t} else if r, err := c.WALSegmentReader(context.Background(), dpos.Truncate()); err != nil {\n\t\tt.Fatal(err)\n\t} else if b1, err := io.ReadAll(lz4.NewReader(r)); err != nil {\n\t\tt.Fatal(err)\n\t} else if err := r.Close(); err != nil {\n\t\tt.Fatal(err)\n\t} else if !bytes.Equal(b0, b1) {\n\t\tt.Fatalf(\"wal mismatch: len(%d), len(%d)\", len(b0), len(b1))\n\t}\n}\n\nfunc TestReplica_Snapshot(t *testing.T) {\n\tdb, sqldb := MustOpenDBs(t)\n\tdefer MustCloseDBs(t, db, sqldb)\n\n\tc := file.NewReplicaClient(t.TempDir())\n\tr := litestream.NewReplica(db, \"\")\n\tr.Client = c\n\n\t// Execute a query to force a write to the WAL.\n\tif _, err := sqldb.Exec(`CREATE TABLE foo (bar TEXT);`); err != nil {\n\t\tt.Fatal(err)\n\t} else if err := db.Sync(context.Background()); err != nil {\n\t\tt.Fatal(err)\n\t} else if err := r.Sync(context.Background()); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Fetch current database position & snapshot.\n\tpos0, err := db.Pos()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if info, err := r.Snapshot(context.Background()); err != nil {\n\t\tt.Fatal(err)\n\t} else if got, want := info.Pos(), nextIndex(pos0); got != want {\n\t\tt.Fatalf(\"pos=%s, want %s\", got, want)\n\t}\n\n\t// Sync database and then replica.\n\tif err := db.Sync(context.Background()); err != nil {\n\t\tt.Fatal(err)\n\t} else if err := r.Sync(context.Background()); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Execute a query to force a write to the WAL & truncate to start new index.\n\tif _, err := sqldb.Exec(`INSERT INTO foo (bar) VALUES ('baz');`); err != nil {\n\t\tt.Fatal(err)\n\t} else if err := db.Checkpoint(context.Background(), litestream.CheckpointModeTruncate); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Fetch current database position & snapshot.\n\tpos1, err := db.Pos()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if info, err := r.Snapshot(context.Background()); err != nil {\n\t\tt.Fatal(err)\n\t} else if got, want := info.Pos(), nextIndex(pos1); got != want {\n\t\tt.Fatalf(\"pos=%v, want %v\", got, want)\n\t}\n\n\t// Verify three snapshots exist.\n\tif infos, err := r.Snapshots(context.Background()); err != nil {\n\t\tt.Fatal(err)\n\t} else if got, want := len(infos), 3; got != want {\n\t\tt.Fatalf(\"len=%v, want %v\", got, want)\n\t} else if got, want := infos[0].Pos(), pos0.Truncate(); got != want {\n\t\tt.Fatalf(\"info[0]=%s, want %s\", got, want)\n\t} else if got, want := infos[1].Pos(), nextIndex(pos0); got != want {\n\t\tt.Fatalf(\"info[1]=%s, want %s\", got, want)\n\t} else if got, want := infos[2].Pos(), nextIndex(pos1); got != want {\n\t\tt.Fatalf(\"info[2]=%s, want %s\", got, want)\n\t}\n}\n"
        },
        {
          "name": "s3",
          "type": "tree",
          "content": null
        },
        {
          "name": "sftp",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}